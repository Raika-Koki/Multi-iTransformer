[32m[I 2025-01-06 02:45:34,312][0m A new study created in memory with name: no-name-1b2f1988-ebe2-4fbf-bd07-47904753a3b8[0m
[32m[I 2025-01-06 02:47:55,339][0m Trial 0 finished with value: 0.5594313144683838 and parameters: {'observation_period_num': 119, 'train_rates': 0.7999327972304093, 'learning_rate': 1.6561425657043493e-05, 'batch_size': 210, 'step_size': 6, 'gamma': 0.9776527926412917}. Best is trial 0 with value: 0.5594313144683838.[0m
[32m[I 2025-01-06 02:48:35,039][0m Trial 1 finished with value: 0.6524782512375112 and parameters: {'observation_period_num': 19, 'train_rates': 0.8003192696445433, 'learning_rate': 4.452760207326099e-06, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9474895049214762}. Best is trial 0 with value: 0.5594313144683838.[0m
[32m[I 2025-01-06 02:52:28,986][0m Trial 2 finished with value: 0.9025911302991616 and parameters: {'observation_period_num': 197, 'train_rates': 0.6726894007721304, 'learning_rate': 3.9942240771438456e-05, 'batch_size': 179, 'step_size': 3, 'gamma': 0.8848158102798946}. Best is trial 0 with value: 0.5594313144683838.[0m
[32m[I 2025-01-06 02:53:13,390][0m Trial 3 finished with value: 0.6957129074531851 and parameters: {'observation_period_num': 6, 'train_rates': 0.8252821368567229, 'learning_rate': 2.1512518297564374e-06, 'batch_size': 83, 'step_size': 14, 'gamma': 0.9378960140313243}. Best is trial 0 with value: 0.5594313144683838.[0m
[32m[I 2025-01-06 02:58:01,931][0m Trial 4 finished with value: 0.6622103430947798 and parameters: {'observation_period_num': 232, 'train_rates': 0.702952966301399, 'learning_rate': 0.0002026836345955978, 'batch_size': 206, 'step_size': 3, 'gamma': 0.8142228099000871}. Best is trial 0 with value: 0.5594313144683838.[0m
[32m[I 2025-01-06 03:04:04,978][0m Trial 5 finished with value: 1.0149221223428708 and parameters: {'observation_period_num': 230, 'train_rates': 0.9601404650400641, 'learning_rate': 0.000606758917815977, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9517425718018249}. Best is trial 0 with value: 0.5594313144683838.[0m
[32m[I 2025-01-06 03:06:37,975][0m Trial 6 finished with value: 0.17878621220588684 and parameters: {'observation_period_num': 108, 'train_rates': 0.9649924544268131, 'learning_rate': 6.0667880359987736e-05, 'batch_size': 60, 'step_size': 13, 'gamma': 0.8594971456393274}. Best is trial 6 with value: 0.17878621220588684.[0m
[32m[I 2025-01-06 03:10:26,641][0m Trial 7 finished with value: 0.7806095949012606 and parameters: {'observation_period_num': 178, 'train_rates': 0.7988091011642451, 'learning_rate': 4.942234553961266e-06, 'batch_size': 188, 'step_size': 9, 'gamma': 0.9857817221953079}. Best is trial 6 with value: 0.17878621220588684.[0m
[32m[I 2025-01-06 03:11:39,544][0m Trial 8 finished with value: 1.2242402817846774 and parameters: {'observation_period_num': 65, 'train_rates': 0.6825078343580768, 'learning_rate': 0.000877772348569362, 'batch_size': 162, 'step_size': 6, 'gamma': 0.9379770980986958}. Best is trial 6 with value: 0.17878621220588684.[0m
[32m[I 2025-01-06 03:17:11,598][0m Trial 9 finished with value: 0.3193886141979556 and parameters: {'observation_period_num': 239, 'train_rates': 0.8092776664187311, 'learning_rate': 7.623174706725608e-05, 'batch_size': 232, 'step_size': 5, 'gamma': 0.9564727935890862}. Best is trial 6 with value: 0.17878621220588684.[0m
[32m[I 2025-01-06 03:20:59,258][0m Trial 10 finished with value: 0.10544968154281378 and parameters: {'observation_period_num': 117, 'train_rates': 0.9860105673340258, 'learning_rate': 1.5686418913180134e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.7599345292351932}. Best is trial 10 with value: 0.10544968154281378.[0m
[32m[I 2025-01-06 03:25:17,276][0m Trial 11 finished with value: 0.10970705948971413 and parameters: {'observation_period_num': 112, 'train_rates': 0.9871804714269671, 'learning_rate': 1.5070832482480735e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.750124578064299}. Best is trial 10 with value: 0.10544968154281378.[0m
[32m[I 2025-01-06 03:29:40,116][0m Trial 12 finished with value: 0.1821720782630995 and parameters: {'observation_period_num': 156, 'train_rates': 0.8905140758121626, 'learning_rate': 1.2019611881242786e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7524049823107988}. Best is trial 10 with value: 0.10544968154281378.[0m
[32m[I 2025-01-06 03:33:19,987][0m Trial 13 finished with value: 0.18314617425783136 and parameters: {'observation_period_num': 81, 'train_rates': 0.9108256626263805, 'learning_rate': 1.3555008772681534e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7569318596886441}. Best is trial 10 with value: 0.10544968154281378.[0m
[32m[I 2025-01-06 03:36:44,835][0m Trial 14 finished with value: 0.5970447122073564 and parameters: {'observation_period_num': 148, 'train_rates': 0.8914228694761437, 'learning_rate': 5.1802710151453235e-06, 'batch_size': 125, 'step_size': 11, 'gamma': 0.7966009054528059}. Best is trial 10 with value: 0.10544968154281378.[0m
[32m[I 2025-01-06 03:38:41,786][0m Trial 15 finished with value: 0.08102421462535858 and parameters: {'observation_period_num': 76, 'train_rates': 0.9828511585931539, 'learning_rate': 9.903552939511636e-05, 'batch_size': 51, 'step_size': 15, 'gamma': 0.7921258202524439}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:40:11,481][0m Trial 16 finished with value: 0.13304552565450253 and parameters: {'observation_period_num': 60, 'train_rates': 0.9206747475867519, 'learning_rate': 0.00018053885556250926, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8037766900814103}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:41:06,169][0m Trial 17 finished with value: 0.24508059876305716 and parameters: {'observation_period_num': 41, 'train_rates': 0.8607667156049161, 'learning_rate': 0.00017499271182438896, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8435420594541242}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:43:00,751][0m Trial 18 finished with value: 0.34258590394838573 and parameters: {'observation_period_num': 86, 'train_rates': 0.7528900386460973, 'learning_rate': 9.176222985594353e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.7864811719894984}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:45:36,133][0m Trial 19 finished with value: 1.5810761391950516 and parameters: {'observation_period_num': 143, 'train_rates': 0.6167446232265799, 'learning_rate': 1.2350660423190014e-06, 'batch_size': 154, 'step_size': 15, 'gamma': 0.8910587167078624}. Best is trial 15 with value: 0.08102421462535858.[0m
Early stopping at epoch 65
[32m[I 2025-01-06 03:47:04,223][0m Trial 20 finished with value: 0.6383871797359351 and parameters: {'observation_period_num': 95, 'train_rates': 0.9424648092669328, 'learning_rate': 3.351736046798839e-05, 'batch_size': 83, 'step_size': 1, 'gamma': 0.8312109356364598}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:50:16,560][0m Trial 21 finished with value: 0.11537354318208473 and parameters: {'observation_period_num': 125, 'train_rates': 0.985044620655789, 'learning_rate': 2.0002545398681037e-05, 'batch_size': 36, 'step_size': 15, 'gamma': 0.774869493361611}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:54:37,134][0m Trial 22 finished with value: 0.12996046543121337 and parameters: {'observation_period_num': 53, 'train_rates': 0.98650226874399, 'learning_rate': 8.841159064405494e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.7708514804890415}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:56:59,788][0m Trial 23 finished with value: 0.2388182304924427 and parameters: {'observation_period_num': 103, 'train_rates': 0.9375191318170306, 'learning_rate': 2.1608366064286784e-05, 'batch_size': 71, 'step_size': 14, 'gamma': 0.7507170947995587}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 03:58:51,639][0m Trial 24 finished with value: 0.2203201153470427 and parameters: {'observation_period_num': 76, 'train_rates': 0.8644046808507915, 'learning_rate': 4.8196195901783725e-05, 'batch_size': 42, 'step_size': 12, 'gamma': 0.8207118999184893}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:03:32,006][0m Trial 25 finished with value: 0.10005193874239922 and parameters: {'observation_period_num': 177, 'train_rates': 0.9856592832016897, 'learning_rate': 0.00035032031161251443, 'batch_size': 37, 'step_size': 14, 'gamma': 0.7817589399200385}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:08:03,288][0m Trial 26 finished with value: 0.21363452054836132 and parameters: {'observation_period_num': 182, 'train_rates': 0.9513244501358427, 'learning_rate': 0.0003981436946054594, 'batch_size': 97, 'step_size': 14, 'gamma': 0.781377641153383}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:13:19,634][0m Trial 27 finished with value: 0.4516922342891056 and parameters: {'observation_period_num': 214, 'train_rates': 0.8471417316540886, 'learning_rate': 0.0004096370297135028, 'batch_size': 31, 'step_size': 12, 'gamma': 0.7972860868649203}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:16:55,098][0m Trial 28 finished with value: 0.41702282547681446 and parameters: {'observation_period_num': 165, 'train_rates': 0.7621768799380869, 'learning_rate': 0.0001150469095951377, 'batch_size': 55, 'step_size': 14, 'gamma': 0.8442187528872249}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:20:04,967][0m Trial 29 finished with value: 0.1846329935249828 and parameters: {'observation_period_num': 136, 'train_rates': 0.9108697685859135, 'learning_rate': 0.0003125164187706671, 'batch_size': 250, 'step_size': 11, 'gamma': 0.7720412645951437}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:22:47,838][0m Trial 30 finished with value: 0.2285982714798636 and parameters: {'observation_period_num': 121, 'train_rates': 0.8821102134462692, 'learning_rate': 0.00012107306425599573, 'batch_size': 78, 'step_size': 13, 'gamma': 0.9121015257697991}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:25:52,292][0m Trial 31 finished with value: 0.2034525430047667 and parameters: {'observation_period_num': 117, 'train_rates': 0.9729431798919063, 'learning_rate': 7.684482530917253e-06, 'batch_size': 30, 'step_size': 15, 'gamma': 0.7578439815652197}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:28:17,315][0m Trial 32 finished with value: 0.12905361643061042 and parameters: {'observation_period_num': 35, 'train_rates': 0.9891000666134662, 'learning_rate': 9.40823618361659e-06, 'batch_size': 29, 'step_size': 15, 'gamma': 0.7887919535429754}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:30:38,107][0m Trial 33 finished with value: 0.1923235666107487 and parameters: {'observation_period_num': 97, 'train_rates': 0.935506463264393, 'learning_rate': 2.859326207140826e-05, 'batch_size': 48, 'step_size': 14, 'gamma': 0.769414355576471}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:35:37,155][0m Trial 34 finished with value: 0.8161603212356567 and parameters: {'observation_period_num': 194, 'train_rates': 0.9618973124776171, 'learning_rate': 2.6479459333421185e-06, 'batch_size': 106, 'step_size': 15, 'gamma': 0.8144733258595422}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:39:40,121][0m Trial 35 finished with value: 0.2573343153692718 and parameters: {'observation_period_num': 170, 'train_rates': 0.9160920775276841, 'learning_rate': 2.8265234806182477e-05, 'batch_size': 94, 'step_size': 9, 'gamma': 0.7679532135847381}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:43:01,550][0m Trial 36 finished with value: 0.1934474324402602 and parameters: {'observation_period_num': 133, 'train_rates': 0.9673904939943564, 'learning_rate': 0.00027239436090984684, 'batch_size': 68, 'step_size': 14, 'gamma': 0.805461043045341}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:45:50,939][0m Trial 37 finished with value: 2.579088625154997 and parameters: {'observation_period_num': 112, 'train_rates': 0.9401444560384826, 'learning_rate': 0.0005883753472850485, 'batch_size': 30, 'step_size': 13, 'gamma': 0.7873329700532709}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:51:23,192][0m Trial 38 finished with value: 0.16688168904882797 and parameters: {'observation_period_num': 211, 'train_rates': 0.9659588354299385, 'learning_rate': 4.725427459445652e-05, 'batch_size': 57, 'step_size': 8, 'gamma': 0.8287008558153245}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:55:24,039][0m Trial 39 finished with value: 0.12050507217645645 and parameters: {'observation_period_num': 155, 'train_rates': 0.9875830772998019, 'learning_rate': 1.694954071840971e-05, 'batch_size': 42, 'step_size': 12, 'gamma': 0.8680041227234653}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:55:47,317][0m Trial 40 finished with value: 0.8727739761195253 and parameters: {'observation_period_num': 15, 'train_rates': 0.6092080384581221, 'learning_rate': 6.451283200055278e-05, 'batch_size': 143, 'step_size': 10, 'gamma': 0.7626474067418976}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 04:59:06,409][0m Trial 41 finished with value: 0.11085291568076971 and parameters: {'observation_period_num': 128, 'train_rates': 0.9885748906435399, 'learning_rate': 1.6859244041764653e-05, 'batch_size': 31, 'step_size': 15, 'gamma': 0.7780665060727452}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:02:01,720][0m Trial 42 finished with value: 0.2409144455962109 and parameters: {'observation_period_num': 70, 'train_rates': 0.9544017843244846, 'learning_rate': 6.303472221332175e-06, 'batch_size': 23, 'step_size': 14, 'gamma': 0.7819022958073637}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:06:08,778][0m Trial 43 finished with value: 0.2752898227423429 and parameters: {'observation_period_num': 92, 'train_rates': 0.9720977902525898, 'learning_rate': 2.979060167423885e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7505205053168253}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:08:41,334][0m Trial 44 finished with value: 0.42711132764816284 and parameters: {'observation_period_num': 110, 'train_rates': 0.9290304745878472, 'learning_rate': 1.2101300664865307e-05, 'batch_size': 59, 'step_size': 4, 'gamma': 0.7958619061035129}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:12:22,748][0m Trial 45 finished with value: 2.594220863448249 and parameters: {'observation_period_num': 144, 'train_rates': 0.9488796246521717, 'learning_rate': 0.0009052701486367386, 'batch_size': 38, 'step_size': 14, 'gamma': 0.7620704664127328}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:15:27,498][0m Trial 46 finished with value: 0.23204384659445007 and parameters: {'observation_period_num': 130, 'train_rates': 0.8943568597482041, 'learning_rate': 2.091363392908963e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.7774089021510254}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:20:59,222][0m Trial 47 finished with value: 0.9838962742662806 and parameters: {'observation_period_num': 248, 'train_rates': 0.7654639269689563, 'learning_rate': 3.636936104283849e-06, 'batch_size': 185, 'step_size': 13, 'gamma': 0.8050473095198052}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:24:00,068][0m Trial 48 finished with value: 0.15081255557015538 and parameters: {'observation_period_num': 82, 'train_rates': 0.9723961007752162, 'learning_rate': 1.5479339445354297e-05, 'batch_size': 25, 'step_size': 7, 'gamma': 0.9688662822250985}. Best is trial 15 with value: 0.08102421462535858.[0m
[32m[I 2025-01-06 05:26:05,968][0m Trial 49 finished with value: 0.5144715592477727 and parameters: {'observation_period_num': 103, 'train_rates': 0.7297205613667206, 'learning_rate': 3.89683464644349e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.8897293810002624}. Best is trial 15 with value: 0.08102421462535858.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.7456 | 0.7980
Epoch 2/300, Loss: 0.5619 | 0.5688
Epoch 3/300, Loss: 0.4133 | 0.5674
Epoch 4/300, Loss: 0.3630 | 0.4868
Epoch 5/300, Loss: 0.3594 | 0.4441
Epoch 6/300, Loss: 0.2834 | 0.4110
Epoch 7/300, Loss: 0.2754 | 0.3545
Epoch 8/300, Loss: 0.2761 | 0.3396
Epoch 9/300, Loss: 0.2398 | 0.3290
Epoch 10/300, Loss: 0.2216 | 0.2967
Epoch 11/300, Loss: 0.2119 | 0.2685
Epoch 12/300, Loss: 0.2119 | 0.2607
Epoch 13/300, Loss: 0.1890 | 0.2501
Epoch 14/300, Loss: 0.1877 | 0.2392
Epoch 15/300, Loss: 0.1872 | 0.2271
Epoch 16/300, Loss: 0.1818 | 0.2173
Epoch 17/300, Loss: 0.1713 | 0.2025
Epoch 18/300, Loss: 0.1680 | 0.2008
Epoch 19/300, Loss: 0.1686 | 0.2005
Epoch 20/300, Loss: 0.1691 | 0.1900
Epoch 21/300, Loss: 0.1747 | 0.2090
Epoch 22/300, Loss: 0.1829 | 0.2026
Epoch 23/300, Loss: 0.1948 | 0.2175
Epoch 24/300, Loss: 0.1877 | 0.1965
Epoch 25/300, Loss: 0.2031 | 0.1882
Epoch 26/300, Loss: 0.1824 | 0.1817
Epoch 27/300, Loss: 0.1666 | 0.1822
Epoch 28/300, Loss: 0.1613 | 0.1713
Epoch 29/300, Loss: 0.1528 | 0.1654
Epoch 30/300, Loss: 0.1468 | 0.1601
Epoch 31/300, Loss: 0.1456 | 0.1576
Epoch 32/300, Loss: 0.1423 | 0.1528
Epoch 33/300, Loss: 0.1405 | 0.1510
Epoch 34/300, Loss: 0.1399 | 0.1494
Epoch 35/300, Loss: 0.1371 | 0.1446
Epoch 36/300, Loss: 0.1375 | 0.1426
Epoch 37/300, Loss: 0.1349 | 0.1440
Epoch 38/300, Loss: 0.1343 | 0.1369
Epoch 39/300, Loss: 0.1342 | 0.1354
Epoch 40/300, Loss: 0.1317 | 0.1341
Epoch 41/300, Loss: 0.1308 | 0.1319
Epoch 42/300, Loss: 0.1289 | 0.1274
Epoch 43/300, Loss: 0.1270 | 0.1299
Epoch 44/300, Loss: 0.1271 | 0.1258
Epoch 45/300, Loss: 0.1267 | 0.1240
Epoch 46/300, Loss: 0.1248 | 0.1267
Epoch 47/300, Loss: 0.1249 | 0.1234
Epoch 48/300, Loss: 0.1239 | 0.1197
Epoch 49/300, Loss: 0.1226 | 0.1227
Epoch 50/300, Loss: 0.1251 | 0.1237
Epoch 51/300, Loss: 0.1227 | 0.1156
Epoch 52/300, Loss: 0.1242 | 0.1259
Epoch 53/300, Loss: 0.1231 | 0.1201
Epoch 54/300, Loss: 0.1227 | 0.1135
Epoch 55/300, Loss: 0.1216 | 0.1207
Epoch 56/300, Loss: 0.1193 | 0.1127
Epoch 57/300, Loss: 0.1190 | 0.1123
Epoch 58/300, Loss: 0.1187 | 0.1147
Epoch 59/300, Loss: 0.1158 | 0.1094
Epoch 60/300, Loss: 0.1160 | 0.1090
Epoch 61/300, Loss: 0.1155 | 0.1097
Epoch 62/300, Loss: 0.1143 | 0.1069
Epoch 63/300, Loss: 0.1127 | 0.1078
Epoch 64/300, Loss: 0.1130 | 0.1063
Epoch 65/300, Loss: 0.1116 | 0.1049
Epoch 66/300, Loss: 0.1123 | 0.1068
Epoch 67/300, Loss: 0.1117 | 0.1041
Epoch 68/300, Loss: 0.1113 | 0.1043
Epoch 69/300, Loss: 0.1109 | 0.1049
Epoch 70/300, Loss: 0.1096 | 0.1030
Epoch 71/300, Loss: 0.1101 | 0.1030
Epoch 72/300, Loss: 0.1087 | 0.1024
Epoch 73/300, Loss: 0.1097 | 0.1044
Epoch 74/300, Loss: 0.1102 | 0.1019
Epoch 75/300, Loss: 0.1090 | 0.1020
Epoch 76/300, Loss: 0.1085 | 0.1022
Epoch 77/300, Loss: 0.1077 | 0.1000
Epoch 78/300, Loss: 0.1074 | 0.1001
Epoch 79/300, Loss: 0.1070 | 0.0999
Epoch 80/300, Loss: 0.1059 | 0.0991
Epoch 81/300, Loss: 0.1059 | 0.0991
Epoch 82/300, Loss: 0.1055 | 0.0987
Epoch 83/300, Loss: 0.1053 | 0.0986
Epoch 84/300, Loss: 0.1048 | 0.0974
Epoch 85/300, Loss: 0.1046 | 0.0966
Epoch 86/300, Loss: 0.1046 | 0.0971
Epoch 87/300, Loss: 0.1039 | 0.0963
Epoch 88/300, Loss: 0.1037 | 0.0960
Epoch 89/300, Loss: 0.1041 | 0.0962
Epoch 90/300, Loss: 0.1029 | 0.0960
Epoch 91/300, Loss: 0.1029 | 0.0953
Epoch 92/300, Loss: 0.1025 | 0.0951
Epoch 93/300, Loss: 0.1023 | 0.0949
Epoch 94/300, Loss: 0.1026 | 0.0949
Epoch 95/300, Loss: 0.1021 | 0.0945
Epoch 96/300, Loss: 0.1018 | 0.0941
Epoch 97/300, Loss: 0.1008 | 0.0937
Epoch 98/300, Loss: 0.1012 | 0.0937
Epoch 99/300, Loss: 0.1011 | 0.0937
Epoch 100/300, Loss: 0.1007 | 0.0931
Epoch 101/300, Loss: 0.1009 | 0.0931
Epoch 102/300, Loss: 0.1005 | 0.0927
Epoch 103/300, Loss: 0.1009 | 0.0928
Epoch 104/300, Loss: 0.1000 | 0.0923
Epoch 105/300, Loss: 0.1000 | 0.0922
Epoch 106/300, Loss: 0.0998 | 0.0918
Epoch 107/300, Loss: 0.0998 | 0.0921
Epoch 108/300, Loss: 0.0986 | 0.0918
Epoch 109/300, Loss: 0.0992 | 0.0919
Epoch 110/300, Loss: 0.0993 | 0.0918
Epoch 111/300, Loss: 0.0991 | 0.0914
Epoch 112/300, Loss: 0.0994 | 0.0914
Epoch 113/300, Loss: 0.0994 | 0.0913
Epoch 114/300, Loss: 0.0992 | 0.0914
Epoch 115/300, Loss: 0.0985 | 0.0913
Epoch 116/300, Loss: 0.0990 | 0.0913
Epoch 117/300, Loss: 0.0983 | 0.0910
Epoch 118/300, Loss: 0.0985 | 0.0908
Epoch 119/300, Loss: 0.0990 | 0.0907
Epoch 120/300, Loss: 0.0977 | 0.0907
Epoch 121/300, Loss: 0.0983 | 0.0904
Epoch 122/300, Loss: 0.0977 | 0.0903
Epoch 123/300, Loss: 0.0986 | 0.0903
Epoch 124/300, Loss: 0.0978 | 0.0904
Epoch 125/300, Loss: 0.0985 | 0.0902
Epoch 126/300, Loss: 0.0983 | 0.0902
Epoch 127/300, Loss: 0.0984 | 0.0902
Epoch 128/300, Loss: 0.0981 | 0.0900
Epoch 129/300, Loss: 0.0972 | 0.0896
Epoch 130/300, Loss: 0.0977 | 0.0896
Epoch 131/300, Loss: 0.0975 | 0.0896
Epoch 132/300, Loss: 0.0972 | 0.0895
Epoch 133/300, Loss: 0.0975 | 0.0895
Epoch 134/300, Loss: 0.0967 | 0.0898
Epoch 135/300, Loss: 0.0969 | 0.0894
Epoch 136/300, Loss: 0.0969 | 0.0892
Epoch 137/300, Loss: 0.0968 | 0.0893
Epoch 138/300, Loss: 0.0967 | 0.0892
Epoch 139/300, Loss: 0.0967 | 0.0892
Epoch 140/300, Loss: 0.0964 | 0.0892
Epoch 141/300, Loss: 0.0966 | 0.0892
Epoch 142/300, Loss: 0.0970 | 0.0891
Epoch 143/300, Loss: 0.0963 | 0.0892
Epoch 144/300, Loss: 0.0961 | 0.0892
Epoch 145/300, Loss: 0.0963 | 0.0888
Epoch 146/300, Loss: 0.0967 | 0.0888
Epoch 147/300, Loss: 0.0966 | 0.0884
Epoch 148/300, Loss: 0.0962 | 0.0884
Epoch 149/300, Loss: 0.0965 | 0.0885
Epoch 150/300, Loss: 0.0965 | 0.0883
Epoch 151/300, Loss: 0.0963 | 0.0884
Epoch 152/300, Loss: 0.0959 | 0.0885
Epoch 153/300, Loss: 0.0961 | 0.0885
Epoch 154/300, Loss: 0.0956 | 0.0885
Epoch 155/300, Loss: 0.0967 | 0.0884
Epoch 156/300, Loss: 0.0960 | 0.0884
Epoch 157/300, Loss: 0.0961 | 0.0884
Epoch 158/300, Loss: 0.0959 | 0.0883
Epoch 159/300, Loss: 0.0962 | 0.0884
Epoch 160/300, Loss: 0.0961 | 0.0883
Epoch 161/300, Loss: 0.0961 | 0.0882
Epoch 162/300, Loss: 0.0957 | 0.0881
Epoch 163/300, Loss: 0.0961 | 0.0882
Epoch 164/300, Loss: 0.0963 | 0.0881
Epoch 165/300, Loss: 0.0960 | 0.0881
Epoch 166/300, Loss: 0.0959 | 0.0881
Epoch 167/300, Loss: 0.0959 | 0.0881
Epoch 168/300, Loss: 0.0955 | 0.0881
Epoch 169/300, Loss: 0.0954 | 0.0879
Epoch 170/300, Loss: 0.0960 | 0.0881
Epoch 171/300, Loss: 0.0953 | 0.0881
Epoch 172/300, Loss: 0.0952 | 0.0880
Epoch 173/300, Loss: 0.0958 | 0.0879
Epoch 174/300, Loss: 0.0953 | 0.0878
Epoch 175/300, Loss: 0.0955 | 0.0878
Epoch 176/300, Loss: 0.0960 | 0.0878
Epoch 177/300, Loss: 0.0959 | 0.0878
Epoch 178/300, Loss: 0.0960 | 0.0877
Epoch 179/300, Loss: 0.0955 | 0.0877
Epoch 180/300, Loss: 0.0956 | 0.0877
Epoch 181/300, Loss: 0.0960 | 0.0877
Epoch 182/300, Loss: 0.0951 | 0.0877
Epoch 183/300, Loss: 0.0962 | 0.0876
Epoch 184/300, Loss: 0.0955 | 0.0876
Epoch 185/300, Loss: 0.0954 | 0.0876
Epoch 186/300, Loss: 0.0959 | 0.0875
Epoch 187/300, Loss: 0.0958 | 0.0875
Epoch 188/300, Loss: 0.0956 | 0.0876
Epoch 189/300, Loss: 0.0959 | 0.0876
Epoch 190/300, Loss: 0.0951 | 0.0876
Epoch 191/300, Loss: 0.0963 | 0.0876
Epoch 192/300, Loss: 0.0957 | 0.0876
Epoch 193/300, Loss: 0.0956 | 0.0876
Epoch 194/300, Loss: 0.0956 | 0.0876
Epoch 195/300, Loss: 0.0951 | 0.0876
Epoch 196/300, Loss: 0.0960 | 0.0877
Epoch 197/300, Loss: 0.0953 | 0.0877
Epoch 198/300, Loss: 0.0953 | 0.0877
Epoch 199/300, Loss: 0.0959 | 0.0876
Epoch 200/300, Loss: 0.0956 | 0.0876
Epoch 201/300, Loss: 0.0947 | 0.0875
Epoch 202/300, Loss: 0.0950 | 0.0876
Epoch 203/300, Loss: 0.0956 | 0.0876
Epoch 204/300, Loss: 0.0956 | 0.0875
Epoch 205/300, Loss: 0.0945 | 0.0875
Epoch 206/300, Loss: 0.0956 | 0.0875
Epoch 207/300, Loss: 0.0951 | 0.0875
Epoch 208/300, Loss: 0.0952 | 0.0875
Epoch 209/300, Loss: 0.0951 | 0.0875
Epoch 210/300, Loss: 0.0951 | 0.0875
Epoch 211/300, Loss: 0.0950 | 0.0875
Epoch 212/300, Loss: 0.0954 | 0.0875
Epoch 213/300, Loss: 0.0944 | 0.0875
Epoch 214/300, Loss: 0.0946 | 0.0875
Epoch 215/300, Loss: 0.0955 | 0.0875
Epoch 216/300, Loss: 0.0950 | 0.0875
Epoch 217/300, Loss: 0.0955 | 0.0875
Epoch 218/300, Loss: 0.0952 | 0.0875
Epoch 219/300, Loss: 0.0955 | 0.0875
Epoch 220/300, Loss: 0.0951 | 0.0875
Epoch 221/300, Loss: 0.0954 | 0.0875
Epoch 222/300, Loss: 0.0958 | 0.0875
Epoch 223/300, Loss: 0.0956 | 0.0875
Epoch 224/300, Loss: 0.0955 | 0.0875
Epoch 225/300, Loss: 0.0950 | 0.0875
Epoch 226/300, Loss: 0.0952 | 0.0875
Epoch 227/300, Loss: 0.0950 | 0.0874
Epoch 228/300, Loss: 0.0950 | 0.0874
Epoch 229/300, Loss: 0.0945 | 0.0874
Epoch 230/300, Loss: 0.0954 | 0.0874
Epoch 231/300, Loss: 0.0951 | 0.0875
Epoch 232/300, Loss: 0.0954 | 0.0875
Epoch 233/300, Loss: 0.0950 | 0.0875
Epoch 234/300, Loss: 0.0952 | 0.0875
Epoch 235/300, Loss: 0.0946 | 0.0875
Epoch 236/300, Loss: 0.0948 | 0.0875
Epoch 237/300, Loss: 0.0953 | 0.0875
Epoch 238/300, Loss: 0.0954 | 0.0875
Epoch 239/300, Loss: 0.0948 | 0.0875
Epoch 240/300, Loss: 0.0961 | 0.0874
Epoch 241/300, Loss: 0.0949 | 0.0874
Epoch 242/300, Loss: 0.0954 | 0.0874
Epoch 243/300, Loss: 0.0953 | 0.0874
Epoch 244/300, Loss: 0.0953 | 0.0874
Epoch 245/300, Loss: 0.0955 | 0.0874
Epoch 246/300, Loss: 0.0956 | 0.0874
Epoch 247/300, Loss: 0.0952 | 0.0874
Epoch 248/300, Loss: 0.0949 | 0.0874
Epoch 249/300, Loss: 0.0958 | 0.0874
Epoch 250/300, Loss: 0.0954 | 0.0874
Epoch 251/300, Loss: 0.0952 | 0.0874
Epoch 252/300, Loss: 0.0955 | 0.0874
Epoch 253/300, Loss: 0.0947 | 0.0874
Epoch 254/300, Loss: 0.0956 | 0.0874
Epoch 255/300, Loss: 0.0951 | 0.0874
Epoch 256/300, Loss: 0.0950 | 0.0874
Epoch 257/300, Loss: 0.0958 | 0.0874
Epoch 258/300, Loss: 0.0954 | 0.0874
Epoch 259/300, Loss: 0.0948 | 0.0874
Epoch 260/300, Loss: 0.0953 | 0.0874
Epoch 261/300, Loss: 0.0950 | 0.0874
Epoch 262/300, Loss: 0.0946 | 0.0874
Epoch 263/300, Loss: 0.0952 | 0.0874
Epoch 264/300, Loss: 0.0943 | 0.0874
Epoch 265/300, Loss: 0.0948 | 0.0874
Epoch 266/300, Loss: 0.0947 | 0.0874
Epoch 267/300, Loss: 0.0950 | 0.0874
Early stopping
Runtime (seconds): 313.554899930954
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 205.39312386512756
RMSE: 14.33154296875
MAE: 14.33154296875
R-squared: nan
[220.59845]
