[32m[I 2025-01-07 15:50:44,877][0m A new study created in memory with name: no-name-f05458b7-bde1-4423-8aa3-b26f5b7ee18c[0m
[32m[I 2025-01-07 15:53:49,937][0m Trial 0 finished with value: 0.8420683046681198 and parameters: {'observation_period_num': 163, 'train_rates': 0.6450235043359535, 'learning_rate': 4.3211252600716437e-05, 'batch_size': 89, 'step_size': 2, 'gamma': 0.9542330328721086}. Best is trial 0 with value: 0.8420683046681198.[0m
[32m[I 2025-01-07 15:55:11,927][0m Trial 1 finished with value: 0.526836608652196 and parameters: {'observation_period_num': 70, 'train_rates': 0.6831821068470306, 'learning_rate': 9.45826368849282e-05, 'batch_size': 111, 'step_size': 7, 'gamma': 0.9518212555271072}. Best is trial 1 with value: 0.526836608652196.[0m
[32m[I 2025-01-07 16:00:24,311][0m Trial 2 finished with value: 0.7664790975814042 and parameters: {'observation_period_num': 244, 'train_rates': 0.68120045109695, 'learning_rate': 1.096535567849e-05, 'batch_size': 66, 'step_size': 2, 'gamma': 0.968710164177994}. Best is trial 1 with value: 0.526836608652196.[0m
[32m[I 2025-01-07 16:06:11,869][0m Trial 3 finished with value: 0.4224580632788794 and parameters: {'observation_period_num': 248, 'train_rates': 0.8146215019497511, 'learning_rate': 4.891664475938188e-05, 'batch_size': 159, 'step_size': 15, 'gamma': 0.7590191711704655}. Best is trial 3 with value: 0.4224580632788794.[0m
[32m[I 2025-01-07 16:09:57,027][0m Trial 4 finished with value: 0.6599145848032335 and parameters: {'observation_period_num': 177, 'train_rates': 0.7790237887805211, 'learning_rate': 0.0005493932714231769, 'batch_size': 229, 'step_size': 9, 'gamma': 0.968910040235716}. Best is trial 3 with value: 0.4224580632788794.[0m
[32m[I 2025-01-07 16:10:33,705][0m Trial 5 finished with value: 1.3723849092254672 and parameters: {'observation_period_num': 6, 'train_rates': 0.6046150194147557, 'learning_rate': 1.4398974277888613e-06, 'batch_size': 87, 'step_size': 15, 'gamma': 0.8650661504560679}. Best is trial 3 with value: 0.4224580632788794.[0m
[32m[I 2025-01-07 16:16:08,017][0m Trial 6 finished with value: 0.5139072937890887 and parameters: {'observation_period_num': 238, 'train_rates': 0.8125574921759916, 'learning_rate': 2.5551479323415956e-05, 'batch_size': 244, 'step_size': 15, 'gamma': 0.8370034513725606}. Best is trial 3 with value: 0.4224580632788794.[0m
[32m[I 2025-01-07 16:19:25,070][0m Trial 7 finished with value: 0.34070719078362705 and parameters: {'observation_period_num': 122, 'train_rates': 0.854400462073812, 'learning_rate': 5.455552046984537e-05, 'batch_size': 20, 'step_size': 2, 'gamma': 0.7671006031360893}. Best is trial 7 with value: 0.34070719078362705.[0m
[32m[I 2025-01-07 16:21:54,545][0m Trial 8 finished with value: 1.3674757674562699 and parameters: {'observation_period_num': 120, 'train_rates': 0.8380384451040013, 'learning_rate': 1.7448332800648304e-06, 'batch_size': 145, 'step_size': 10, 'gamma': 0.8208741007602459}. Best is trial 7 with value: 0.34070719078362705.[0m
[32m[I 2025-01-07 16:25:30,405][0m Trial 9 finished with value: 1.0019569253646226 and parameters: {'observation_period_num': 189, 'train_rates': 0.641565521501235, 'learning_rate': 0.0005173526436159896, 'batch_size': 211, 'step_size': 6, 'gamma': 0.8427380144234298}. Best is trial 7 with value: 0.34070719078362705.[0m
[32m[I 2025-01-07 16:30:12,561][0m Trial 10 finished with value: 0.35786161593028476 and parameters: {'observation_period_num': 88, 'train_rates': 0.987925374215386, 'learning_rate': 6.827468664863485e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7819109813090923}. Best is trial 7 with value: 0.34070719078362705.[0m
[32m[I 2025-01-07 16:34:42,994][0m Trial 11 finished with value: 0.3715189657769762 and parameters: {'observation_period_num': 94, 'train_rates': 0.9614755875863721, 'learning_rate': 6.0019025084158265e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.7628874992090534}. Best is trial 7 with value: 0.34070719078362705.[0m
Early stopping at epoch 61
[32m[I 2025-01-07 16:36:37,753][0m Trial 12 finished with value: 0.24568676555698568 and parameters: {'observation_period_num': 48, 'train_rates': 0.9625064359260672, 'learning_rate': 0.00014522460707969544, 'batch_size': 23, 'step_size': 1, 'gamma': 0.7942421637258656}. Best is trial 12 with value: 0.24568676555698568.[0m
Early stopping at epoch 86
[32m[I 2025-01-07 16:37:38,419][0m Trial 13 finished with value: 0.4022357708488973 and parameters: {'observation_period_num': 35, 'train_rates': 0.8812111205842532, 'learning_rate': 0.00019499918700951194, 'batch_size': 55, 'step_size': 1, 'gamma': 0.7949480198190786}. Best is trial 12 with value: 0.24568676555698568.[0m
[32m[I 2025-01-07 16:39:02,561][0m Trial 14 finished with value: 0.15529545282458398 and parameters: {'observation_period_num': 38, 'train_rates': 0.9243647791833988, 'learning_rate': 0.0001731453325083238, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8036978615285929}. Best is trial 14 with value: 0.15529545282458398.[0m
[32m[I 2025-01-07 16:40:19,563][0m Trial 15 finished with value: 0.13049182214520194 and parameters: {'observation_period_num': 45, 'train_rates': 0.924796849082184, 'learning_rate': 0.00018923022734832038, 'batch_size': 52, 'step_size': 5, 'gamma': 0.9080165172200554}. Best is trial 15 with value: 0.13049182214520194.[0m
[32m[I 2025-01-07 16:40:53,716][0m Trial 16 finished with value: 0.14835739403198928 and parameters: {'observation_period_num': 23, 'train_rates': 0.9075307572051176, 'learning_rate': 0.00028704111788050465, 'batch_size': 185, 'step_size': 5, 'gamma': 0.9051125227024082}. Best is trial 15 with value: 0.13049182214520194.[0m
[32m[I 2025-01-07 16:41:17,238][0m Trial 17 finished with value: 0.8254081640803517 and parameters: {'observation_period_num': 10, 'train_rates': 0.9050484500314638, 'learning_rate': 0.0009318071720024031, 'batch_size': 183, 'step_size': 12, 'gamma': 0.9125364887756892}. Best is trial 15 with value: 0.13049182214520194.[0m
[32m[I 2025-01-07 16:42:25,177][0m Trial 18 finished with value: 0.3662259861275002 and parameters: {'observation_period_num': 59, 'train_rates': 0.767951060741871, 'learning_rate': 0.0003816155141996958, 'batch_size': 185, 'step_size': 6, 'gamma': 0.9102856622297958}. Best is trial 15 with value: 0.13049182214520194.[0m
[32m[I 2025-01-07 16:44:22,371][0m Trial 19 finished with value: 0.19226923761109413 and parameters: {'observation_period_num': 88, 'train_rates': 0.9295628109645759, 'learning_rate': 0.00034753651477343516, 'batch_size': 118, 'step_size': 8, 'gamma': 0.913031595035541}. Best is trial 15 with value: 0.13049182214520194.[0m
[32m[I 2025-01-07 16:44:57,092][0m Trial 20 finished with value: 0.2011712380191859 and parameters: {'observation_period_num': 23, 'train_rates': 0.8847191592285895, 'learning_rate': 9.959777820451842e-05, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8865792531087096}. Best is trial 15 with value: 0.13049182214520194.[0m
[32m[I 2025-01-07 16:46:17,446][0m Trial 21 finished with value: 0.1286003781216485 and parameters: {'observation_period_num': 37, 'train_rates': 0.9284868263303376, 'learning_rate': 0.00021973013342002664, 'batch_size': 50, 'step_size': 4, 'gamma': 0.8790194050719106}. Best is trial 21 with value: 0.1286003781216485.[0m
[32m[I 2025-01-07 16:47:55,006][0m Trial 22 finished with value: 2.6623167113253943 and parameters: {'observation_period_num': 68, 'train_rates': 0.9477400592186694, 'learning_rate': 0.0009844910795102828, 'batch_size': 80, 'step_size': 5, 'gamma': 0.8863085566711071}. Best is trial 21 with value: 0.1286003781216485.[0m
[32m[I 2025-01-07 16:48:29,477][0m Trial 23 finished with value: 0.22634787262254155 and parameters: {'observation_period_num': 21, 'train_rates': 0.8683756759717399, 'learning_rate': 0.0003338318807376575, 'batch_size': 123, 'step_size': 3, 'gamma': 0.933329877434649}. Best is trial 21 with value: 0.1286003781216485.[0m
[32m[I 2025-01-07 16:50:05,713][0m Trial 24 finished with value: 0.08298528147861362 and parameters: {'observation_period_num': 47, 'train_rates': 0.9836719078346112, 'learning_rate': 0.00021393925737228108, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8656247967062154}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 16:52:45,108][0m Trial 25 finished with value: 0.11898966001558907 and parameters: {'observation_period_num': 105, 'train_rates': 0.9725634794629513, 'learning_rate': 8.735171661490309e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.8676522653869576}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 16:56:34,856][0m Trial 26 finished with value: 0.13756899185040417 and parameters: {'observation_period_num': 146, 'train_rates': 0.982169307995299, 'learning_rate': 2.318344308834755e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.8606376220184986}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 16:58:44,215][0m Trial 27 finished with value: 0.520125895600992 and parameters: {'observation_period_num': 108, 'train_rates': 0.7350096451908281, 'learning_rate': 8.911771109256144e-05, 'batch_size': 69, 'step_size': 13, 'gamma': 0.8816772017613776}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:02:26,768][0m Trial 28 finished with value: 0.12520283460617065 and parameters: {'observation_period_num': 146, 'train_rates': 0.9881238248992816, 'learning_rate': 8.807381534654112e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8431292870906172}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:06:09,975][0m Trial 29 finished with value: 0.14041763544082642 and parameters: {'observation_period_num': 148, 'train_rates': 0.9876187620734951, 'learning_rate': 7.520371069636813e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8368275715627691}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:11:28,571][0m Trial 30 finished with value: 0.2405461549758911 and parameters: {'observation_period_num': 208, 'train_rates': 0.9495469938978387, 'learning_rate': 3.8366477694080044e-05, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8483648147785835}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:15:10,295][0m Trial 31 finished with value: 0.13084526630965146 and parameters: {'observation_period_num': 141, 'train_rates': 0.9651084993397409, 'learning_rate': 0.00015751033927099945, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8222171360140309}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:16:58,496][0m Trial 32 finished with value: 0.1610617633908987 and parameters: {'observation_period_num': 75, 'train_rates': 0.9310542127299557, 'learning_rate': 0.00010631432234743363, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8704519780610713}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:19:28,393][0m Trial 33 finished with value: 0.20331374581968575 and parameters: {'observation_period_num': 107, 'train_rates': 0.900475346687205, 'learning_rate': 1.8209562602439575e-05, 'batch_size': 55, 'step_size': 13, 'gamma': 0.8568407396316059}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:23:41,724][0m Trial 34 finished with value: 0.14686238765716553 and parameters: {'observation_period_num': 168, 'train_rates': 0.9721123602593731, 'learning_rate': 5.537952687836401e-05, 'batch_size': 86, 'step_size': 9, 'gamma': 0.8208576050503054}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:26:54,151][0m Trial 35 finished with value: 0.21698810815811156 and parameters: {'observation_period_num': 133, 'train_rates': 0.9473421393471926, 'learning_rate': 0.000245259149118178, 'batch_size': 134, 'step_size': 7, 'gamma': 0.8781979533691741}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:29:16,643][0m Trial 36 finished with value: 0.1670469609754426 and parameters: {'observation_period_num': 58, 'train_rates': 0.9422752222139514, 'learning_rate': 0.00012222863765722511, 'batch_size': 33, 'step_size': 10, 'gamma': 0.8950937453667155}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:33:18,346][0m Trial 37 finished with value: 0.13590192794799805 and parameters: {'observation_period_num': 158, 'train_rates': 0.9861935610503101, 'learning_rate': 6.588196704441921e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9488858099686571}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:37:35,093][0m Trial 38 finished with value: 0.7922941509046053 and parameters: {'observation_period_num': 190, 'train_rates': 0.8360355680516897, 'learning_rate': 0.0005602155086380768, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9301316242499611}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:40:20,519][0m Trial 39 finished with value: 0.17262750991161854 and parameters: {'observation_period_num': 105, 'train_rates': 0.9121032881590926, 'learning_rate': 3.1841552424000684e-05, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9839670362415642}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:41:58,023][0m Trial 40 finished with value: 0.7961842034992419 and parameters: {'observation_period_num': 79, 'train_rates': 0.7369115316666526, 'learning_rate': 1.2109678839647725e-05, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8509688922661282}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:43:28,732][0m Trial 41 finished with value: 0.15469747597028402 and parameters: {'observation_period_num': 47, 'train_rates': 0.9587101724183056, 'learning_rate': 0.00021925239911340584, 'batch_size': 48, 'step_size': 3, 'gamma': 0.8713112703505899}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:44:48,709][0m Trial 42 finished with value: 0.1241072151604869 and parameters: {'observation_period_num': 37, 'train_rates': 0.9258226726180828, 'learning_rate': 0.00013178128696997833, 'batch_size': 53, 'step_size': 7, 'gamma': 0.892850264824334}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:46:14,141][0m Trial 43 finished with value: 0.1594820780058702 and parameters: {'observation_period_num': 59, 'train_rates': 0.9673028656726803, 'learning_rate': 4.4573921663242343e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8974768648505677}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:48:03,537][0m Trial 44 finished with value: 0.11380641293743099 and parameters: {'observation_period_num': 7, 'train_rates': 0.8890987589549221, 'learning_rate': 0.00012525634321245539, 'batch_size': 40, 'step_size': 9, 'gamma': 0.8324604290408373}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:50:31,540][0m Trial 45 finished with value: 0.11846528336667944 and parameters: {'observation_period_num': 7, 'train_rates': 0.8848647290587645, 'learning_rate': 0.00012878811083321377, 'batch_size': 29, 'step_size': 9, 'gamma': 0.8261727769828715}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:52:52,256][0m Trial 46 finished with value: 0.2093489677487593 and parameters: {'observation_period_num': 9, 'train_rates': 0.8502466374389892, 'learning_rate': 0.00013772045795532284, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8297699321247459}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:55:46,831][0m Trial 47 finished with value: 0.1414437481237076 and parameters: {'observation_period_num': 5, 'train_rates': 0.8831193207653714, 'learning_rate': 7.004486985234203e-05, 'batch_size': 26, 'step_size': 9, 'gamma': 0.8107719436314125}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:57:27,976][0m Trial 48 finished with value: 0.5654897463698554 and parameters: {'observation_period_num': 28, 'train_rates': 0.7955943931854871, 'learning_rate': 0.0004614611122061925, 'batch_size': 41, 'step_size': 7, 'gamma': 0.8610812690149048}. Best is trial 24 with value: 0.08298528147861362.[0m
[32m[I 2025-01-07 17:58:39,457][0m Trial 49 finished with value: 0.22134603682446152 and parameters: {'observation_period_num': 17, 'train_rates': 0.8268806194260994, 'learning_rate': 0.0001263388141242582, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8323454753080096}. Best is trial 24 with value: 0.08298528147861362.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_Transformer(nomstl).json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.5590 | 0.8650
Epoch 2/300, Loss: 0.5116 | 0.6502
Epoch 3/300, Loss: 0.4972 | 0.5074
Epoch 4/300, Loss: 0.5190 | 0.4916
Epoch 5/300, Loss: 0.3991 | 0.4592
Epoch 6/300, Loss: 0.3529 | 0.3789
Epoch 7/300, Loss: 0.3234 | 0.4256
Epoch 8/300, Loss: 0.3251 | 0.3423
Epoch 9/300, Loss: 0.3308 | 0.3452
Epoch 10/300, Loss: 0.2524 | 0.3321
Epoch 11/300, Loss: 0.2527 | 0.3548
Epoch 12/300, Loss: 0.2521 | 0.3151
Epoch 13/300, Loss: 0.2263 | 0.2728
Epoch 14/300, Loss: 0.2206 | 0.2673
Epoch 15/300, Loss: 0.2375 | 0.2403
Epoch 16/300, Loss: 0.2429 | 0.2507
Epoch 17/300, Loss: 0.2160 | 0.2420
Epoch 18/300, Loss: 0.2118 | 0.2312
Epoch 19/300, Loss: 0.2053 | 0.2142
Epoch 20/300, Loss: 0.1902 | 0.2136
Epoch 21/300, Loss: 0.1783 | 0.2074
Epoch 22/300, Loss: 0.1730 | 0.1974
Epoch 23/300, Loss: 0.1619 | 0.1989
Epoch 24/300, Loss: 0.1588 | 0.1951
Epoch 25/300, Loss: 0.1534 | 0.1810
Epoch 26/300, Loss: 0.1480 | 0.1770
Epoch 27/300, Loss: 0.1450 | 0.1726
Epoch 28/300, Loss: 0.1424 | 0.1651
Epoch 29/300, Loss: 0.1363 | 0.1653
Epoch 30/300, Loss: 0.1334 | 0.1600
Epoch 31/300, Loss: 0.1321 | 0.1518
Epoch 32/300, Loss: 0.1333 | 0.1522
Epoch 33/300, Loss: 0.1276 | 0.1466
Epoch 34/300, Loss: 0.1260 | 0.1439
Epoch 35/300, Loss: 0.1233 | 0.1455
Epoch 36/300, Loss: 0.1200 | 0.1368
Epoch 37/300, Loss: 0.1205 | 0.1394
Epoch 38/300, Loss: 0.1180 | 0.1356
Epoch 39/300, Loss: 0.1153 | 0.1304
Epoch 40/300, Loss: 0.1178 | 0.1339
Epoch 41/300, Loss: 0.1138 | 0.1301
Epoch 42/300, Loss: 0.1134 | 0.1290
Epoch 43/300, Loss: 0.1160 | 0.1370
Epoch 44/300, Loss: 0.1123 | 0.1272
Epoch 45/300, Loss: 0.1115 | 0.1251
Epoch 46/300, Loss: 0.1118 | 0.1276
Epoch 47/300, Loss: 0.1082 | 0.1228
Epoch 48/300, Loss: 0.1064 | 0.1225
Epoch 49/300, Loss: 0.1072 | 0.1258
Epoch 50/300, Loss: 0.1052 | 0.1194
Epoch 51/300, Loss: 0.1038 | 0.1185
Epoch 52/300, Loss: 0.1019 | 0.1191
Epoch 53/300, Loss: 0.1003 | 0.1168
Epoch 54/300, Loss: 0.0998 | 0.1144
Epoch 55/300, Loss: 0.0995 | 0.1153
Epoch 56/300, Loss: 0.0987 | 0.1126
Epoch 57/300, Loss: 0.0982 | 0.1128
Epoch 58/300, Loss: 0.0976 | 0.1117
Epoch 59/300, Loss: 0.0965 | 0.1105
Epoch 60/300, Loss: 0.0953 | 0.1108
Epoch 61/300, Loss: 0.0953 | 0.1099
Epoch 62/300, Loss: 0.0942 | 0.1091
Epoch 63/300, Loss: 0.0945 | 0.1077
Epoch 64/300, Loss: 0.0944 | 0.1082
Epoch 65/300, Loss: 0.0939 | 0.1076
Epoch 66/300, Loss: 0.0929 | 0.1072
Epoch 67/300, Loss: 0.0934 | 0.1075
Epoch 68/300, Loss: 0.0926 | 0.1060
Epoch 69/300, Loss: 0.0924 | 0.1061
Epoch 70/300, Loss: 0.0909 | 0.1062
Epoch 71/300, Loss: 0.0913 | 0.1057
Epoch 72/300, Loss: 0.0908 | 0.1043
Epoch 73/300, Loss: 0.0914 | 0.1047
Epoch 74/300, Loss: 0.0900 | 0.1042
Epoch 75/300, Loss: 0.0901 | 0.1039
Epoch 76/300, Loss: 0.0891 | 0.1042
Epoch 77/300, Loss: 0.0894 | 0.1044
Epoch 78/300, Loss: 0.0896 | 0.1042
Epoch 79/300, Loss: 0.0890 | 0.1036
Epoch 80/300, Loss: 0.0889 | 0.1028
Epoch 81/300, Loss: 0.0887 | 0.1033
Epoch 82/300, Loss: 0.0886 | 0.1024
Epoch 83/300, Loss: 0.0888 | 0.1028
Epoch 84/300, Loss: 0.0877 | 0.1022
Epoch 85/300, Loss: 0.0874 | 0.1025
Epoch 86/300, Loss: 0.0878 | 0.1021
Epoch 87/300, Loss: 0.0877 | 0.1020
Epoch 88/300, Loss: 0.0873 | 0.1017
Epoch 89/300, Loss: 0.0871 | 0.1024
Epoch 90/300, Loss: 0.0866 | 0.1018
Epoch 91/300, Loss: 0.0865 | 0.1017
Epoch 92/300, Loss: 0.0868 | 0.1016
Epoch 93/300, Loss: 0.0859 | 0.1014
Epoch 94/300, Loss: 0.0862 | 0.1010
Epoch 95/300, Loss: 0.0864 | 0.1008
Epoch 96/300, Loss: 0.0855 | 0.1005
Epoch 97/300, Loss: 0.0863 | 0.1008
Epoch 98/300, Loss: 0.0860 | 0.1007
Epoch 99/300, Loss: 0.0857 | 0.1005
Epoch 100/300, Loss: 0.0851 | 0.0998
Epoch 101/300, Loss: 0.0848 | 0.0997
Epoch 102/300, Loss: 0.0844 | 0.0995
Epoch 103/300, Loss: 0.0852 | 0.0995
Epoch 104/300, Loss: 0.0847 | 0.0996
Epoch 105/300, Loss: 0.0852 | 0.0993
Epoch 106/300, Loss: 0.0847 | 0.0989
Epoch 107/300, Loss: 0.0853 | 0.0993
Epoch 108/300, Loss: 0.0842 | 0.0994
Epoch 109/300, Loss: 0.0852 | 0.0990
Epoch 110/300, Loss: 0.0850 | 0.0992
Epoch 111/300, Loss: 0.0847 | 0.0996
Epoch 112/300, Loss: 0.0845 | 0.1001
Epoch 113/300, Loss: 0.0844 | 0.0996
Epoch 114/300, Loss: 0.0843 | 0.0998
Epoch 115/300, Loss: 0.0840 | 0.0996
Epoch 116/300, Loss: 0.0846 | 0.0993
Epoch 117/300, Loss: 0.0845 | 0.0991
Epoch 118/300, Loss: 0.0840 | 0.0989
Epoch 119/300, Loss: 0.0840 | 0.0990
Epoch 120/300, Loss: 0.0839 | 0.0991
Epoch 121/300, Loss: 0.0842 | 0.0991
Epoch 122/300, Loss: 0.0840 | 0.0992
Epoch 123/300, Loss: 0.0835 | 0.0992
Epoch 124/300, Loss: 0.0837 | 0.0991
Epoch 125/300, Loss: 0.0836 | 0.0989
Epoch 126/300, Loss: 0.0841 | 0.0989
Epoch 127/300, Loss: 0.0836 | 0.0988
Epoch 128/300, Loss: 0.0837 | 0.0990
Epoch 129/300, Loss: 0.0835 | 0.0990
Epoch 130/300, Loss: 0.0835 | 0.0990
Epoch 131/300, Loss: 0.0834 | 0.0989
Epoch 132/300, Loss: 0.0843 | 0.0989
Epoch 133/300, Loss: 0.0837 | 0.0988
Epoch 134/300, Loss: 0.0831 | 0.0988
Epoch 135/300, Loss: 0.0837 | 0.0987
Epoch 136/300, Loss: 0.0829 | 0.0986
Epoch 137/300, Loss: 0.0833 | 0.0985
Epoch 138/300, Loss: 0.0842 | 0.0985
Epoch 139/300, Loss: 0.0838 | 0.0984
Epoch 140/300, Loss: 0.0833 | 0.0982
Epoch 141/300, Loss: 0.0835 | 0.0984
Epoch 142/300, Loss: 0.0836 | 0.0982
Epoch 143/300, Loss: 0.0837 | 0.0981
Epoch 144/300, Loss: 0.0834 | 0.0980
Epoch 145/300, Loss: 0.0837 | 0.0980
Epoch 146/300, Loss: 0.0833 | 0.0983
Epoch 147/300, Loss: 0.0826 | 0.0983
Epoch 148/300, Loss: 0.0829 | 0.0983
Epoch 149/300, Loss: 0.0838 | 0.0983
Epoch 150/300, Loss: 0.0834 | 0.0983
Epoch 151/300, Loss: 0.0834 | 0.0983
Epoch 152/300, Loss: 0.0832 | 0.0983
Epoch 153/300, Loss: 0.0832 | 0.0983
Epoch 154/300, Loss: 0.0833 | 0.0984
Epoch 155/300, Loss: 0.0831 | 0.0983
Epoch 156/300, Loss: 0.0831 | 0.0983
Epoch 157/300, Loss: 0.0832 | 0.0983
Epoch 158/300, Loss: 0.0830 | 0.0982
Epoch 159/300, Loss: 0.0825 | 0.0982
Epoch 160/300, Loss: 0.0831 | 0.0982
Epoch 161/300, Loss: 0.0837 | 0.0981
Epoch 162/300, Loss: 0.0834 | 0.0982
Epoch 163/300, Loss: 0.0829 | 0.0982
Epoch 164/300, Loss: 0.0819 | 0.0981
Epoch 165/300, Loss: 0.0835 | 0.0981
Epoch 166/300, Loss: 0.0834 | 0.0981
Epoch 167/300, Loss: 0.0828 | 0.0982
Epoch 168/300, Loss: 0.0829 | 0.0982
Epoch 169/300, Loss: 0.0833 | 0.0982
Epoch 170/300, Loss: 0.0827 | 0.0982
Epoch 171/300, Loss: 0.0829 | 0.0982
Epoch 172/300, Loss: 0.0829 | 0.0982
Epoch 173/300, Loss: 0.0838 | 0.0981
Epoch 174/300, Loss: 0.0829 | 0.0981
Epoch 175/300, Loss: 0.0834 | 0.0981
Epoch 176/300, Loss: 0.0829 | 0.0980
Epoch 177/300, Loss: 0.0835 | 0.0980
Epoch 178/300, Loss: 0.0827 | 0.0981
Epoch 179/300, Loss: 0.0836 | 0.0981
Epoch 180/300, Loss: 0.0834 | 0.0981
Epoch 181/300, Loss: 0.0831 | 0.0981
Epoch 182/300, Loss: 0.0829 | 0.0981
Epoch 183/300, Loss: 0.0838 | 0.0981
Epoch 184/300, Loss: 0.0833 | 0.0981
Epoch 185/300, Loss: 0.0824 | 0.0981
Epoch 186/300, Loss: 0.0833 | 0.0981
Epoch 187/300, Loss: 0.0830 | 0.0981
Epoch 188/300, Loss: 0.0830 | 0.0981
Epoch 189/300, Loss: 0.0824 | 0.0981
Epoch 190/300, Loss: 0.0827 | 0.0981
Epoch 191/300, Loss: 0.0834 | 0.0981
Epoch 192/300, Loss: 0.0830 | 0.0981
Epoch 193/300, Loss: 0.0830 | 0.0981
Epoch 194/300, Loss: 0.0828 | 0.0981
Epoch 195/300, Loss: 0.0828 | 0.0981
Epoch 196/300, Loss: 0.0826 | 0.0980
Epoch 197/300, Loss: 0.0829 | 0.0980
Epoch 198/300, Loss: 0.0829 | 0.0980
Epoch 199/300, Loss: 0.0829 | 0.0980
Epoch 200/300, Loss: 0.0832 | 0.0980
Epoch 201/300, Loss: 0.0833 | 0.0980
Epoch 202/300, Loss: 0.0834 | 0.0980
Epoch 203/300, Loss: 0.0822 | 0.0980
Epoch 204/300, Loss: 0.0831 | 0.0980
Epoch 205/300, Loss: 0.0832 | 0.0980
Epoch 206/300, Loss: 0.0823 | 0.0980
Epoch 207/300, Loss: 0.0824 | 0.0980
Epoch 208/300, Loss: 0.0832 | 0.0980
Epoch 209/300, Loss: 0.0831 | 0.0980
Epoch 210/300, Loss: 0.0823 | 0.0980
Epoch 211/300, Loss: 0.0827 | 0.0980
Epoch 212/300, Loss: 0.0824 | 0.0980
Epoch 213/300, Loss: 0.0833 | 0.0980
Epoch 214/300, Loss: 0.0836 | 0.0980
Epoch 215/300, Loss: 0.0831 | 0.0980
Epoch 216/300, Loss: 0.0826 | 0.0980
Epoch 217/300, Loss: 0.0826 | 0.0980
Epoch 218/300, Loss: 0.0828 | 0.0980
Epoch 219/300, Loss: 0.0830 | 0.0980
Epoch 220/300, Loss: 0.0831 | 0.0980
Epoch 221/300, Loss: 0.0830 | 0.0980
Epoch 222/300, Loss: 0.0830 | 0.0980
Epoch 223/300, Loss: 0.0827 | 0.0980
Epoch 224/300, Loss: 0.0823 | 0.0980
Epoch 225/300, Loss: 0.0831 | 0.0980
Epoch 226/300, Loss: 0.0829 | 0.0980
Epoch 227/300, Loss: 0.0828 | 0.0980
Epoch 228/300, Loss: 0.0825 | 0.0980
Epoch 229/300, Loss: 0.0830 | 0.0980
Epoch 230/300, Loss: 0.0832 | 0.0980
Epoch 231/300, Loss: 0.0831 | 0.0980
Epoch 232/300, Loss: 0.0831 | 0.0980
Epoch 233/300, Loss: 0.0833 | 0.0980
Epoch 234/300, Loss: 0.0830 | 0.0980
Epoch 235/300, Loss: 0.0830 | 0.0980
Epoch 236/300, Loss: 0.0834 | 0.0980
Epoch 237/300, Loss: 0.0832 | 0.0980
Epoch 238/300, Loss: 0.0826 | 0.0980
Epoch 239/300, Loss: 0.0825 | 0.0980
Epoch 240/300, Loss: 0.0829 | 0.0980
Epoch 241/300, Loss: 0.0829 | 0.0980
Epoch 242/300, Loss: 0.0838 | 0.0980
Epoch 243/300, Loss: 0.0830 | 0.0980
Epoch 244/300, Loss: 0.0823 | 0.0980
Epoch 245/300, Loss: 0.0822 | 0.0980
Epoch 246/300, Loss: 0.0828 | 0.0980
Epoch 247/300, Loss: 0.0823 | 0.0980
Epoch 248/300, Loss: 0.0829 | 0.0980
Epoch 249/300, Loss: 0.0830 | 0.0980
Epoch 250/300, Loss: 0.0828 | 0.0980
Epoch 251/300, Loss: 0.0829 | 0.0980
Epoch 252/300, Loss: 0.0822 | 0.0980
Epoch 253/300, Loss: 0.0831 | 0.0980
Epoch 254/300, Loss: 0.0830 | 0.0980
Epoch 255/300, Loss: 0.0834 | 0.0980
Epoch 256/300, Loss: 0.0829 | 0.0980
Epoch 257/300, Loss: 0.0832 | 0.0980
Epoch 258/300, Loss: 0.0829 | 0.0980
Epoch 259/300, Loss: 0.0837 | 0.0980
Epoch 260/300, Loss: 0.0831 | 0.0980
Epoch 261/300, Loss: 0.0831 | 0.0980
Epoch 262/300, Loss: 0.0829 | 0.0980
Epoch 263/300, Loss: 0.0832 | 0.0980
Epoch 264/300, Loss: 0.0832 | 0.0980
Epoch 265/300, Loss: 0.0830 | 0.0980
Epoch 266/300, Loss: 0.0828 | 0.0980
Epoch 267/300, Loss: 0.0837 | 0.0980
Epoch 268/300, Loss: 0.0825 | 0.0980
Epoch 269/300, Loss: 0.0830 | 0.0980
Epoch 270/300, Loss: 0.0834 | 0.0980
Epoch 271/300, Loss: 0.0833 | 0.0980
Epoch 272/300, Loss: 0.0831 | 0.0980
Epoch 273/300, Loss: 0.0828 | 0.0980
Epoch 274/300, Loss: 0.0831 | 0.0980
Epoch 275/300, Loss: 0.0828 | 0.0980
Epoch 276/300, Loss: 0.0829 | 0.0980
Epoch 277/300, Loss: 0.0832 | 0.0980
Epoch 278/300, Loss: 0.0828 | 0.0980
Epoch 279/300, Loss: 0.0824 | 0.0980
Epoch 280/300, Loss: 0.0833 | 0.0980
Epoch 281/300, Loss: 0.0827 | 0.0980
Epoch 282/300, Loss: 0.0836 | 0.0980
Epoch 283/300, Loss: 0.0832 | 0.0980
Epoch 284/300, Loss: 0.0827 | 0.0980
Epoch 285/300, Loss: 0.0834 | 0.0980
Epoch 286/300, Loss: 0.0829 | 0.0980
Epoch 287/300, Loss: 0.0831 | 0.0980
Epoch 288/300, Loss: 0.0833 | 0.0980
Epoch 289/300, Loss: 0.0832 | 0.0980
Epoch 290/300, Loss: 0.0823 | 0.0980
Epoch 291/300, Loss: 0.0824 | 0.0980
Epoch 292/300, Loss: 0.0824 | 0.0980
Epoch 293/300, Loss: 0.0830 | 0.0980
Epoch 294/300, Loss: 0.0825 | 0.0980
Epoch 295/300, Loss: 0.0827 | 0.0980
Epoch 296/300, Loss: 0.0830 | 0.0980
Epoch 297/300, Loss: 0.0828 | 0.0980
Epoch 298/300, Loss: 0.0827 | 0.0980
Epoch 299/300, Loss: 0.0830 | 0.0980
Epoch 300/300, Loss: 0.0829 | 0.0980
Runtime (seconds): 336.84621000289917
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 355.8435279598925
RMSE: 18.863815307617188
MAE: 18.863815307617188
R-squared: nan
[234.61618]
