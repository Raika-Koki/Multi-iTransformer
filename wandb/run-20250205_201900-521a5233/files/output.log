[32m[I 2025-02-05 20:19:04,757][0m A new study created in memory with name: no-name-57f90e94-785b-43f6-9e9b-017a62194a33[0m
[32m[I 2025-02-05 20:19:39,622][0m Trial 0 finished with value: 0.24287216365337372 and parameters: {'observation_period_num': 99, 'train_rates': 0.9753019293439811, 'learning_rate': 8.14999227980377e-05, 'batch_size': 182, 'step_size': 7, 'gamma': 0.8021556158382731}. Best is trial 0 with value: 0.24287216365337372.[0m
[32m[I 2025-02-05 20:20:19,093][0m Trial 1 finished with value: 0.5030645739937586 and parameters: {'observation_period_num': 250, 'train_rates': 0.9075572226403712, 'learning_rate': 6.558204990836013e-06, 'batch_size': 139, 'step_size': 13, 'gamma': 0.8957214436384846}. Best is trial 0 with value: 0.24287216365337372.[0m
[32m[I 2025-02-05 20:20:45,634][0m Trial 2 finished with value: 0.3501738188204961 and parameters: {'observation_period_num': 130, 'train_rates': 0.6746564333497868, 'learning_rate': 0.00020825421155224504, 'batch_size': 187, 'step_size': 1, 'gamma': 0.9262016356228542}. Best is trial 0 with value: 0.24287216365337372.[0m
[32m[I 2025-02-05 20:21:19,418][0m Trial 3 finished with value: 0.0661741261496956 and parameters: {'observation_period_num': 51, 'train_rates': 0.8893007806240143, 'learning_rate': 0.0004928677925819287, 'batch_size': 176, 'step_size': 3, 'gamma': 0.885242003715463}. Best is trial 3 with value: 0.0661741261496956.[0m
[32m[I 2025-02-05 20:21:43,443][0m Trial 4 finished with value: 0.5235537873275244 and parameters: {'observation_period_num': 127, 'train_rates': 0.7647566997857209, 'learning_rate': 1.2825201990687163e-05, 'batch_size': 237, 'step_size': 11, 'gamma': 0.8074047719876914}. Best is trial 3 with value: 0.0661741261496956.[0m
[32m[I 2025-02-05 20:22:12,291][0m Trial 5 finished with value: 0.45615899562835693 and parameters: {'observation_period_num': 98, 'train_rates': 0.9400891345925783, 'learning_rate': 1.4019268552834099e-05, 'batch_size': 223, 'step_size': 7, 'gamma': 0.8824425651079373}. Best is trial 3 with value: 0.0661741261496956.[0m
[32m[I 2025-02-05 20:22:34,222][0m Trial 6 finished with value: 0.8295611275267277 and parameters: {'observation_period_num': 182, 'train_rates': 0.7628028157272579, 'learning_rate': 8.346749950873628e-06, 'batch_size': 249, 'step_size': 8, 'gamma': 0.7521339345965272}. Best is trial 3 with value: 0.0661741261496956.[0m
[32m[I 2025-02-05 20:23:01,484][0m Trial 7 finished with value: 0.4117046180166232 and parameters: {'observation_period_num': 185, 'train_rates': 0.665677571103832, 'learning_rate': 2.6810053061651077e-05, 'batch_size': 177, 'step_size': 7, 'gamma': 0.8381701865135689}. Best is trial 3 with value: 0.0661741261496956.[0m
[32m[I 2025-02-05 20:23:41,721][0m Trial 8 finished with value: 0.3434873879570322 and parameters: {'observation_period_num': 251, 'train_rates': 0.6440086734375288, 'learning_rate': 8.086977070694904e-05, 'batch_size': 112, 'step_size': 11, 'gamma': 0.8707226186498045}. Best is trial 3 with value: 0.0661741261496956.[0m
Early stopping at epoch 57
[32m[I 2025-02-05 20:23:55,287][0m Trial 9 finished with value: 2.961980183919271 and parameters: {'observation_period_num': 226, 'train_rates': 0.7162883493272876, 'learning_rate': 1.2084009111865873e-06, 'batch_size': 238, 'step_size': 1, 'gamma': 0.8381075900731195}. Best is trial 3 with value: 0.0661741261496956.[0m
[32m[I 2025-02-05 20:27:06,350][0m Trial 10 finished with value: 0.0293252124772168 and parameters: {'observation_period_num': 10, 'train_rates': 0.8622505262597637, 'learning_rate': 0.000981297277713436, 'batch_size': 29, 'step_size': 4, 'gamma': 0.9698654913177478}. Best is trial 10 with value: 0.0293252124772168.[0m
[32m[I 2025-02-05 20:29:17,167][0m Trial 11 finished with value: 0.029142338681865384 and parameters: {'observation_period_num': 5, 'train_rates': 0.8631006203091672, 'learning_rate': 0.0008026809289914145, 'batch_size': 44, 'step_size': 4, 'gamma': 0.9551776678692214}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:34:00,731][0m Trial 12 finished with value: 0.033086256932168226 and parameters: {'observation_period_num': 7, 'train_rates': 0.8446458106259255, 'learning_rate': 0.0008569827852455729, 'batch_size': 19, 'step_size': 4, 'gamma': 0.9825628443118677}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:39:12,728][0m Trial 13 finished with value: 0.05568569975804591 and parameters: {'observation_period_num': 7, 'train_rates': 0.8345985745004023, 'learning_rate': 0.0003133065886432874, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9846323708214784}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:40:37,693][0m Trial 14 finished with value: 0.08875355986851612 and parameters: {'observation_period_num': 46, 'train_rates': 0.833190449584633, 'learning_rate': 0.0009723256682719624, 'batch_size': 64, 'step_size': 5, 'gamma': 0.9489606219348944}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:42:09,952][0m Trial 15 finished with value: 0.05723199284710418 and parameters: {'observation_period_num': 52, 'train_rates': 0.8810751361306549, 'learning_rate': 0.00013528605905163577, 'batch_size': 61, 'step_size': 3, 'gamma': 0.946385214716332}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:43:36,333][0m Trial 16 finished with value: 0.09395193003975184 and parameters: {'observation_period_num': 35, 'train_rates': 0.8079131047537451, 'learning_rate': 0.00042230555819456044, 'batch_size': 62, 'step_size': 10, 'gamma': 0.9213686850656801}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:44:36,820][0m Trial 17 finished with value: 0.8296105103728212 and parameters: {'observation_period_num': 77, 'train_rates': 0.944406945654477, 'learning_rate': 2.065376144288435e-06, 'batch_size': 100, 'step_size': 5, 'gamma': 0.9575747387235636}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:46:19,351][0m Trial 18 finished with value: 0.1604767980617146 and parameters: {'observation_period_num': 21, 'train_rates': 0.6071047571250751, 'learning_rate': 4.969209855725656e-05, 'batch_size': 42, 'step_size': 15, 'gamma': 0.9138991888950465}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:47:16,919][0m Trial 19 finished with value: 0.1977336585924432 and parameters: {'observation_period_num': 68, 'train_rates': 0.7710159065130977, 'learning_rate': 0.0001729357701719675, 'batch_size': 93, 'step_size': 2, 'gamma': 0.9665602269528059}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:49:13,087][0m Trial 20 finished with value: 0.2819683685065797 and parameters: {'observation_period_num': 148, 'train_rates': 0.7152335405196826, 'learning_rate': 0.0005620432167262532, 'batch_size': 40, 'step_size': 6, 'gamma': 0.9888401531008801}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:53:48,223][0m Trial 21 finished with value: 0.03194987611370344 and parameters: {'observation_period_num': 5, 'train_rates': 0.8627696469768058, 'learning_rate': 0.0008429797500520093, 'batch_size': 20, 'step_size': 4, 'gamma': 0.9705268361554703}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:56:04,453][0m Trial 22 finished with value: 0.16489254401704645 and parameters: {'observation_period_num': 26, 'train_rates': 0.8680615026894225, 'learning_rate': 0.0008698553041592631, 'batch_size': 40, 'step_size': 4, 'gamma': 0.9279606916807385}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:57:18,151][0m Trial 23 finished with value: 0.033663833943697125 and parameters: {'observation_period_num': 7, 'train_rates': 0.9212896520485504, 'learning_rate': 0.0002778857540924884, 'batch_size': 80, 'step_size': 3, 'gamma': 0.9674568091728043}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 20:58:01,724][0m Trial 24 finished with value: 0.10483860282592178 and parameters: {'observation_period_num': 67, 'train_rates': 0.8016078438435676, 'learning_rate': 0.0005281993648989266, 'batch_size': 128, 'step_size': 9, 'gamma': 0.9418558731179028}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:03:37,799][0m Trial 25 finished with value: 0.04301946575085519 and parameters: {'observation_period_num': 29, 'train_rates': 0.8634264562500253, 'learning_rate': 0.0009622722602286314, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9677987576533489}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:05:49,889][0m Trial 26 finished with value: 0.12128878052084191 and parameters: {'observation_period_num': 95, 'train_rates': 0.9643460979494398, 'learning_rate': 0.00011538391853837774, 'batch_size': 45, 'step_size': 4, 'gamma': 0.8969877148779327}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:07:01,280][0m Trial 27 finished with value: 0.07827174106767462 and parameters: {'observation_period_num': 41, 'train_rates': 0.9056670247135792, 'learning_rate': 0.00029229732427139046, 'batch_size': 82, 'step_size': 6, 'gamma': 0.9356565628027549}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:10:03,598][0m Trial 28 finished with value: 0.04728249507641071 and parameters: {'observation_period_num': 15, 'train_rates': 0.8212799149715055, 'learning_rate': 0.0005393777079931815, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9094545680951849}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:11:46,606][0m Trial 29 finished with value: 0.14006216824054718 and parameters: {'observation_period_num': 86, 'train_rates': 0.9894619513695098, 'learning_rate': 6.386509710815577e-05, 'batch_size': 58, 'step_size': 6, 'gamma': 0.8456654366240606}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:12:58,114][0m Trial 30 finished with value: 0.1515583789851292 and parameters: {'observation_period_num': 119, 'train_rates': 0.8548964574789879, 'learning_rate': 0.00020716221440942193, 'batch_size': 76, 'step_size': 8, 'gamma': 0.9723001730683797}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:16:04,603][0m Trial 31 finished with value: 0.04432483776180842 and parameters: {'observation_period_num': 8, 'train_rates': 0.8441302032882685, 'learning_rate': 0.0006822055274043254, 'batch_size': 29, 'step_size': 4, 'gamma': 0.9895516947129728}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:20:30,031][0m Trial 32 finished with value: 0.04846506310770145 and parameters: {'observation_period_num': 24, 'train_rates': 0.8899899277345837, 'learning_rate': 0.00036708140774075143, 'batch_size': 21, 'step_size': 4, 'gamma': 0.975910695054227}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:22:09,092][0m Trial 33 finished with value: 0.04522897724795048 and parameters: {'observation_period_num': 5, 'train_rates': 0.794549752551482, 'learning_rate': 0.0006823637176369002, 'batch_size': 53, 'step_size': 5, 'gamma': 0.9483535642964551}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:22:48,888][0m Trial 34 finished with value: 0.06778133431064429 and parameters: {'observation_period_num': 58, 'train_rates': 0.9189582638147459, 'learning_rate': 0.0009015626299325876, 'batch_size': 152, 'step_size': 1, 'gamma': 0.9564589310211309}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:25:46,428][0m Trial 35 finished with value: 0.05395230918852144 and parameters: {'observation_period_num': 34, 'train_rates': 0.8783617751697279, 'learning_rate': 0.00039371335703585505, 'batch_size': 31, 'step_size': 2, 'gamma': 0.979496499181523}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:26:17,269][0m Trial 36 finished with value: 0.04863866234819094 and parameters: {'observation_period_num': 20, 'train_rates': 0.8987434858423956, 'learning_rate': 0.00024951838093848166, 'batch_size': 202, 'step_size': 3, 'gamma': 0.9580713073879343}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:26:52,634][0m Trial 37 finished with value: 0.21781382402952978 and parameters: {'observation_period_num': 49, 'train_rates': 0.7850138351744704, 'learning_rate': 0.0006530381949199912, 'batch_size': 157, 'step_size': 6, 'gamma': 0.9312743402769313}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:29:35,308][0m Trial 38 finished with value: 0.4886393269506754 and parameters: {'observation_period_num': 36, 'train_rates': 0.7424002859679901, 'learning_rate': 3.735909953739751e-06, 'batch_size': 30, 'step_size': 3, 'gamma': 0.7580777639272648}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:30:48,374][0m Trial 39 finished with value: 0.11252133737804838 and parameters: {'observation_period_num': 156, 'train_rates': 0.8188142847749014, 'learning_rate': 0.0001283287599917252, 'batch_size': 72, 'step_size': 4, 'gamma': 0.9070293119230329}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:32:34,509][0m Trial 40 finished with value: 0.129386136389297 and parameters: {'observation_period_num': 110, 'train_rates': 0.8557110707796891, 'learning_rate': 3.292124267134531e-05, 'batch_size': 51, 'step_size': 7, 'gamma': 0.778935067584334}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:35:07,688][0m Trial 41 finished with value: 0.040963126834541716 and parameters: {'observation_period_num': 14, 'train_rates': 0.9370785406515283, 'learning_rate': 0.00039615534708229035, 'batch_size': 38, 'step_size': 3, 'gamma': 0.9672515692776433}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:36:15,321][0m Trial 42 finished with value: 0.033294521105129087 and parameters: {'observation_period_num': 5, 'train_rates': 0.9249867134925784, 'learning_rate': 0.0007005651740039416, 'batch_size': 90, 'step_size': 1, 'gamma': 0.96334569225122}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:37:07,099][0m Trial 43 finished with value: 0.05006840452551842 and parameters: {'observation_period_num': 20, 'train_rates': 0.9623702622967603, 'learning_rate': 0.000706780715774131, 'batch_size': 123, 'step_size': 1, 'gamma': 0.9565516362686595}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:38:02,698][0m Trial 44 finished with value: 0.22656812470544269 and parameters: {'observation_period_num': 230, 'train_rates': 0.8427007797593937, 'learning_rate': 0.0004868873353576977, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9385424143746656}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:42:47,416][0m Trial 45 finished with value: 0.02967649767462444 and parameters: {'observation_period_num': 5, 'train_rates': 0.9149082935988742, 'learning_rate': 0.0009400825370119964, 'batch_size': 20, 'step_size': 2, 'gamma': 0.9791445133218787}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:48:24,151][0m Trial 46 finished with value: 0.10778023012586542 and parameters: {'observation_period_num': 58, 'train_rates': 0.8733192483720369, 'learning_rate': 0.000954143739556234, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9865975362138825}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:52:06,460][0m Trial 47 finished with value: 0.071780851110816 and parameters: {'observation_period_num': 41, 'train_rates': 0.8911646136836207, 'learning_rate': 1.2515557369124182e-05, 'batch_size': 25, 'step_size': 2, 'gamma': 0.9803411176301036}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:53:55,611][0m Trial 48 finished with value: 0.0508642566986382 and parameters: {'observation_period_num': 29, 'train_rates': 0.830368087403162, 'learning_rate': 0.0001889570226619186, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8659531364561004}. Best is trial 11 with value: 0.029142338681865384.[0m
[32m[I 2025-02-05 21:56:33,999][0m Trial 49 finished with value: 0.053348767661279245 and parameters: {'observation_period_num': 19, 'train_rates': 0.9056566220678599, 'learning_rate': 0.0003455794323947197, 'batch_size': 36, 'step_size': 4, 'gamma': 0.9774048956223731}. Best is trial 11 with value: 0.029142338681865384.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2128 | 0.1277
Epoch 2/300, Loss: 0.1227 | 0.0765
Epoch 3/300, Loss: 0.1074 | 0.0649
Epoch 4/300, Loss: 0.1021 | 0.0584
Epoch 5/300, Loss: 0.0969 | 0.0531
Epoch 6/300, Loss: 0.0935 | 0.0524
Epoch 7/300, Loss: 0.0962 | 0.0572
Epoch 8/300, Loss: 0.0965 | 0.0548
Epoch 9/300, Loss: 0.0910 | 0.0569
Epoch 10/300, Loss: 0.0920 | 0.0602
Epoch 11/300, Loss: 0.0912 | 0.0693
Epoch 12/300, Loss: 0.0847 | 0.0677
Epoch 13/300, Loss: 0.0824 | 0.0606
Epoch 14/300, Loss: 0.0813 | 0.0613
Epoch 15/300, Loss: 0.0771 | 0.0512
Epoch 16/300, Loss: 0.0763 | 0.0403
Epoch 17/300, Loss: 0.0737 | 0.0398
Epoch 18/300, Loss: 0.0728 | 0.0389
Epoch 19/300, Loss: 0.0722 | 0.0411
Epoch 20/300, Loss: 0.0710 | 0.0392
Epoch 21/300, Loss: 0.0702 | 0.0388
Epoch 22/300, Loss: 0.0689 | 0.0379
Epoch 23/300, Loss: 0.0685 | 0.0410
Epoch 24/300, Loss: 0.0687 | 0.0438
Epoch 25/300, Loss: 0.0688 | 0.0427
Epoch 26/300, Loss: 0.0684 | 0.0372
Epoch 27/300, Loss: 0.0670 | 0.0392
Epoch 28/300, Loss: 0.0675 | 0.0420
Epoch 29/300, Loss: 0.0658 | 0.0387
Epoch 30/300, Loss: 0.0655 | 0.0382
Epoch 31/300, Loss: 0.0646 | 0.0378
Epoch 32/300, Loss: 0.0645 | 0.0381
Epoch 33/300, Loss: 0.0648 | 0.0356
Epoch 34/300, Loss: 0.0631 | 0.0410
Epoch 35/300, Loss: 0.0624 | 0.0388
Epoch 36/300, Loss: 0.0618 | 0.0360
Epoch 37/300, Loss: 0.0609 | 0.0362
Epoch 38/300, Loss: 0.0602 | 0.0400
Epoch 39/300, Loss: 0.0600 | 0.0389
Epoch 40/300, Loss: 0.0599 | 0.0437
Epoch 41/300, Loss: 0.0594 | 0.0403
Epoch 42/300, Loss: 0.0591 | 0.0424
Epoch 43/300, Loss: 0.0583 | 0.0438
Epoch 44/300, Loss: 0.0585 | 0.0443
Epoch 45/300, Loss: 0.0576 | 0.0442
Epoch 46/300, Loss: 0.0578 | 0.0429
Epoch 47/300, Loss: 0.0573 | 0.0477
Epoch 48/300, Loss: 0.0573 | 0.0479
Epoch 49/300, Loss: 0.0579 | 0.0487
Epoch 50/300, Loss: 0.0617 | 0.0360
Epoch 51/300, Loss: 0.0617 | 0.0484
Epoch 52/300, Loss: 0.0582 | 0.0398
Epoch 53/300, Loss: 0.0599 | 0.0335
Epoch 54/300, Loss: 0.0563 | 0.0356
Epoch 55/300, Loss: 0.0555 | 0.0388
Epoch 56/300, Loss: 0.0581 | 0.0365
Epoch 57/300, Loss: 0.0540 | 0.0636
Epoch 58/300, Loss: 0.0603 | 0.0500
Epoch 59/300, Loss: 0.0573 | 0.0480
Epoch 60/300, Loss: 0.0567 | 0.0446
Epoch 61/300, Loss: 0.0554 | 0.0547
Epoch 62/300, Loss: 0.0551 | 0.0673
Epoch 63/300, Loss: 0.0531 | 0.0461
Epoch 64/300, Loss: 0.0580 | 0.0345
Epoch 65/300, Loss: 0.0552 | 0.0391
Epoch 66/300, Loss: 0.0543 | 0.0389
Epoch 67/300, Loss: 0.0541 | 0.0440
Epoch 68/300, Loss: 0.0539 | 0.0439
Epoch 69/300, Loss: 0.0539 | 0.0521
Epoch 70/300, Loss: 0.0542 | 0.0521
Epoch 71/300, Loss: 0.0543 | 0.0689
Epoch 72/300, Loss: 0.0548 | 0.0739
Epoch 73/300, Loss: 0.0546 | 0.0536
Epoch 74/300, Loss: 0.0542 | 0.0819
Epoch 75/300, Loss: 0.0541 | 0.0534
Epoch 76/300, Loss: 0.0527 | 0.0578
Epoch 77/300, Loss: 0.0518 | 0.1135
Epoch 78/300, Loss: 0.0531 | 0.0701
Epoch 79/300, Loss: 0.0564 | 0.0818
Epoch 80/300, Loss: 0.0521 | 0.1438
Epoch 81/300, Loss: 0.0492 | 0.0634
Epoch 82/300, Loss: 0.0534 | 0.0871
Epoch 83/300, Loss: 0.0513 | 0.1196
Epoch 84/300, Loss: 0.0484 | 0.0759
Epoch 85/300, Loss: 0.0547 | 0.0761
Epoch 86/300, Loss: 0.0536 | 0.0982
Epoch 87/300, Loss: 0.0520 | 0.1399
Epoch 88/300, Loss: 0.0488 | 0.1249
Epoch 89/300, Loss: 0.0524 | 0.1236
Epoch 90/300, Loss: 0.0487 | 0.1432
Epoch 91/300, Loss: 0.0533 | 0.1492
Epoch 92/300, Loss: 0.0513 | 0.1261
Epoch 93/300, Loss: 0.0567 | 0.1286
Epoch 94/300, Loss: 0.0532 | 0.1127
Epoch 95/300, Loss: 0.0572 | 0.0742
Epoch 96/300, Loss: 0.0544 | 0.0607
Epoch 97/300, Loss: 0.0566 | 0.0477
Epoch 98/300, Loss: 0.0515 | 0.0401
Epoch 99/300, Loss: 0.0486 | 0.0359
Epoch 100/300, Loss: 0.0462 | 0.0352
Epoch 101/300, Loss: 0.0462 | 0.0331
Epoch 102/300, Loss: 0.0449 | 0.0356
Epoch 103/300, Loss: 0.0451 | 0.0343
Epoch 104/300, Loss: 0.0442 | 0.0340
Epoch 105/300, Loss: 0.0456 | 0.0325
Epoch 106/300, Loss: 0.0449 | 0.0352
Epoch 107/300, Loss: 0.0477 | 0.0347
Epoch 108/300, Loss: 0.0468 | 0.0392
Epoch 109/300, Loss: 0.0489 | 0.0461
Epoch 110/300, Loss: 0.0473 | 0.0487
Epoch 111/300, Loss: 0.0467 | 0.0513
Epoch 112/300, Loss: 0.0447 | 0.0462
Epoch 113/300, Loss: 0.0436 | 0.0445
Epoch 114/300, Loss: 0.0426 | 0.0410
Epoch 115/300, Loss: 0.0422 | 0.0401
Epoch 116/300, Loss: 0.0417 | 0.0382
Epoch 117/300, Loss: 0.0416 | 0.0381
Epoch 118/300, Loss: 0.0414 | 0.0370
Epoch 119/300, Loss: 0.0413 | 0.0372
Epoch 120/300, Loss: 0.0411 | 0.0363
Epoch 121/300, Loss: 0.0411 | 0.0366
Epoch 122/300, Loss: 0.0409 | 0.0360
Epoch 123/300, Loss: 0.0409 | 0.0362
Epoch 124/300, Loss: 0.0408 | 0.0358
Epoch 125/300, Loss: 0.0407 | 0.0360
Epoch 126/300, Loss: 0.0406 | 0.0356
Epoch 127/300, Loss: 0.0405 | 0.0358
Epoch 128/300, Loss: 0.0404 | 0.0354
Epoch 129/300, Loss: 0.0403 | 0.0357
Epoch 130/300, Loss: 0.0402 | 0.0353
Epoch 131/300, Loss: 0.0401 | 0.0355
Epoch 132/300, Loss: 0.0400 | 0.0352
Epoch 133/300, Loss: 0.0399 | 0.0353
Epoch 134/300, Loss: 0.0398 | 0.0351
Epoch 135/300, Loss: 0.0397 | 0.0352
Epoch 136/300, Loss: 0.0396 | 0.0350
Epoch 137/300, Loss: 0.0395 | 0.0351
Epoch 138/300, Loss: 0.0394 | 0.0350
Epoch 139/300, Loss: 0.0393 | 0.0351
Epoch 140/300, Loss: 0.0393 | 0.0350
Epoch 141/300, Loss: 0.0392 | 0.0352
Epoch 142/300, Loss: 0.0392 | 0.0350
Epoch 143/300, Loss: 0.0392 | 0.0352
Epoch 144/300, Loss: 0.0392 | 0.0350
Epoch 145/300, Loss: 0.0392 | 0.0350
Epoch 146/300, Loss: 0.0392 | 0.0347
Epoch 147/300, Loss: 0.0391 | 0.0350
Epoch 148/300, Loss: 0.0390 | 0.0347
Epoch 149/300, Loss: 0.0389 | 0.0350
Epoch 150/300, Loss: 0.0388 | 0.0345
Epoch 151/300, Loss: 0.0387 | 0.0347
Epoch 152/300, Loss: 0.0386 | 0.0344
Epoch 153/300, Loss: 0.0386 | 0.0345
Epoch 154/300, Loss: 0.0385 | 0.0343
Epoch 155/300, Loss: 0.0385 | 0.0345
Epoch 156/300, Loss: 0.0384 | 0.0345
Epoch 157/300, Loss: 0.0384 | 0.0348
Epoch 158/300, Loss: 0.0383 | 0.0348
Epoch 159/300, Loss: 0.0382 | 0.0352
Epoch 160/300, Loss: 0.0382 | 0.0352
Epoch 161/300, Loss: 0.0381 | 0.0355
Epoch 162/300, Loss: 0.0380 | 0.0355
Epoch 163/300, Loss: 0.0379 | 0.0355
Epoch 164/300, Loss: 0.0378 | 0.0355
Epoch 165/300, Loss: 0.0376 | 0.0354
Epoch 166/300, Loss: 0.0376 | 0.0354
Epoch 167/300, Loss: 0.0375 | 0.0353
Epoch 168/300, Loss: 0.0374 | 0.0354
Epoch 169/300, Loss: 0.0374 | 0.0353
Epoch 170/300, Loss: 0.0373 | 0.0354
Epoch 171/300, Loss: 0.0373 | 0.0353
Epoch 172/300, Loss: 0.0373 | 0.0354
Epoch 173/300, Loss: 0.0372 | 0.0354
Epoch 174/300, Loss: 0.0372 | 0.0355
Epoch 175/300, Loss: 0.0371 | 0.0355
Epoch 176/300, Loss: 0.0371 | 0.0356
Epoch 177/300, Loss: 0.0371 | 0.0356
Epoch 178/300, Loss: 0.0370 | 0.0356
Epoch 179/300, Loss: 0.0370 | 0.0356
Epoch 180/300, Loss: 0.0370 | 0.0357
Epoch 181/300, Loss: 0.0369 | 0.0357
Epoch 182/300, Loss: 0.0369 | 0.0358
Epoch 183/300, Loss: 0.0369 | 0.0358
Epoch 184/300, Loss: 0.0368 | 0.0359
Epoch 185/300, Loss: 0.0368 | 0.0359
Epoch 186/300, Loss: 0.0368 | 0.0360
Epoch 187/300, Loss: 0.0367 | 0.0360
Epoch 188/300, Loss: 0.0367 | 0.0361
Epoch 189/300, Loss: 0.0366 | 0.0362
Epoch 190/300, Loss: 0.0366 | 0.0362
Epoch 191/300, Loss: 0.0365 | 0.0363
Epoch 192/300, Loss: 0.0365 | 0.0364
Epoch 193/300, Loss: 0.0365 | 0.0364
Epoch 194/300, Loss: 0.0364 | 0.0365
Epoch 195/300, Loss: 0.0364 | 0.0365
Epoch 196/300, Loss: 0.0364 | 0.0366
Epoch 197/300, Loss: 0.0363 | 0.0366
Epoch 198/300, Loss: 0.0363 | 0.0366
Epoch 199/300, Loss: 0.0363 | 0.0366
Epoch 200/300, Loss: 0.0362 | 0.0366
Epoch 201/300, Loss: 0.0362 | 0.0366
Epoch 202/300, Loss: 0.0362 | 0.0367
Epoch 203/300, Loss: 0.0361 | 0.0367
Epoch 204/300, Loss: 0.0361 | 0.0367
Epoch 205/300, Loss: 0.0361 | 0.0367
Epoch 206/300, Loss: 0.0361 | 0.0367
Epoch 207/300, Loss: 0.0360 | 0.0368
Epoch 208/300, Loss: 0.0360 | 0.0368
Epoch 209/300, Loss: 0.0360 | 0.0368
Epoch 210/300, Loss: 0.0360 | 0.0369
Epoch 211/300, Loss: 0.0359 | 0.0369
Epoch 212/300, Loss: 0.0359 | 0.0370
Epoch 213/300, Loss: 0.0359 | 0.0370
Epoch 214/300, Loss: 0.0359 | 0.0370
Epoch 215/300, Loss: 0.0359 | 0.0371
Epoch 216/300, Loss: 0.0358 | 0.0371
Epoch 217/300, Loss: 0.0358 | 0.0371
Epoch 218/300, Loss: 0.0358 | 0.0372
Epoch 219/300, Loss: 0.0358 | 0.0372
Epoch 220/300, Loss: 0.0358 | 0.0372
Epoch 221/300, Loss: 0.0357 | 0.0373
Epoch 222/300, Loss: 0.0357 | 0.0373
Epoch 223/300, Loss: 0.0357 | 0.0373
Epoch 224/300, Loss: 0.0357 | 0.0373
Epoch 225/300, Loss: 0.0357 | 0.0374
Epoch 226/300, Loss: 0.0357 | 0.0374
Epoch 227/300, Loss: 0.0356 | 0.0374
Epoch 228/300, Loss: 0.0356 | 0.0374
Epoch 229/300, Loss: 0.0356 | 0.0375
Epoch 230/300, Loss: 0.0356 | 0.0375
Epoch 231/300, Loss: 0.0356 | 0.0375
Epoch 232/300, Loss: 0.0356 | 0.0375
Epoch 233/300, Loss: 0.0356 | 0.0375
Epoch 234/300, Loss: 0.0355 | 0.0376
Epoch 235/300, Loss: 0.0355 | 0.0376
Epoch 236/300, Loss: 0.0355 | 0.0376
Epoch 237/300, Loss: 0.0355 | 0.0376
Epoch 238/300, Loss: 0.0355 | 0.0377
Epoch 239/300, Loss: 0.0355 | 0.0377
Epoch 240/300, Loss: 0.0355 | 0.0377
Epoch 241/300, Loss: 0.0355 | 0.0377
Epoch 242/300, Loss: 0.0354 | 0.0377
Epoch 243/300, Loss: 0.0354 | 0.0377
Epoch 244/300, Loss: 0.0354 | 0.0378
Epoch 245/300, Loss: 0.0354 | 0.0378
Epoch 246/300, Loss: 0.0354 | 0.0378
Epoch 247/300, Loss: 0.0354 | 0.0378
Epoch 248/300, Loss: 0.0354 | 0.0378
Epoch 249/300, Loss: 0.0354 | 0.0378
Epoch 250/300, Loss: 0.0354 | 0.0379
Epoch 251/300, Loss: 0.0353 | 0.0379
Epoch 252/300, Loss: 0.0353 | 0.0379
Epoch 253/300, Loss: 0.0353 | 0.0379
Epoch 254/300, Loss: 0.0353 | 0.0379
Epoch 255/300, Loss: 0.0353 | 0.0379
Epoch 256/300, Loss: 0.0353 | 0.0379
Epoch 257/300, Loss: 0.0353 | 0.0379
Epoch 258/300, Loss: 0.0353 | 0.0380
Epoch 259/300, Loss: 0.0353 | 0.0380
Epoch 260/300, Loss: 0.0353 | 0.0380
Epoch 261/300, Loss: 0.0353 | 0.0380
Epoch 262/300, Loss: 0.0353 | 0.0380
Epoch 263/300, Loss: 0.0352 | 0.0380
Epoch 264/300, Loss: 0.0352 | 0.0380
Epoch 265/300, Loss: 0.0352 | 0.0380
Epoch 266/300, Loss: 0.0352 | 0.0381
Epoch 267/300, Loss: 0.0352 | 0.0381
Epoch 268/300, Loss: 0.0352 | 0.0381
Epoch 269/300, Loss: 0.0352 | 0.0381
Epoch 270/300, Loss: 0.0352 | 0.0381
Epoch 271/300, Loss: 0.0352 | 0.0381
Epoch 272/300, Loss: 0.0352 | 0.0381
Epoch 273/300, Loss: 0.0352 | 0.0381
Epoch 274/300, Loss: 0.0352 | 0.0381
Epoch 275/300, Loss: 0.0352 | 0.0381
Epoch 276/300, Loss: 0.0352 | 0.0381
Epoch 277/300, Loss: 0.0352 | 0.0382
Epoch 278/300, Loss: 0.0352 | 0.0382
Epoch 279/300, Loss: 0.0352 | 0.0382
Epoch 280/300, Loss: 0.0351 | 0.0382
Epoch 281/300, Loss: 0.0351 | 0.0382
Epoch 282/300, Loss: 0.0351 | 0.0382
Epoch 283/300, Loss: 0.0351 | 0.0382
Epoch 284/300, Loss: 0.0351 | 0.0382
Epoch 285/300, Loss: 0.0351 | 0.0382
Epoch 286/300, Loss: 0.0351 | 0.0382
Epoch 287/300, Loss: 0.0351 | 0.0382
Epoch 288/300, Loss: 0.0351 | 0.0382
Epoch 289/300, Loss: 0.0351 | 0.0382
Epoch 290/300, Loss: 0.0351 | 0.0382
Epoch 291/300, Loss: 0.0351 | 0.0382
Epoch 292/300, Loss: 0.0351 | 0.0383
Epoch 293/300, Loss: 0.0351 | 0.0383
Epoch 294/300, Loss: 0.0351 | 0.0383
Epoch 295/300, Loss: 0.0351 | 0.0383
Epoch 296/300, Loss: 0.0351 | 0.0383
Epoch 297/300, Loss: 0.0351 | 0.0383
Epoch 298/300, Loss: 0.0351 | 0.0383
Epoch 299/300, Loss: 0.0351 | 0.0383
Epoch 300/300, Loss: 0.0351 | 0.0383
Runtime (seconds): 379.1103813648224
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 19.190993022406474
RMSE: 4.3807525634765625
MAE: 4.3807525634765625
R-squared: nan
[188.30925]
