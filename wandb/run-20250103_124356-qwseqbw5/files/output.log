ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 12:43:57,865][0m A new study created in memory with name: no-name-27265870-b066-445e-a81f-3aca8e565fb0[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-03 12:44:34,709][0m Trial 0 finished with value: 0.6829138994216919 and parameters: {'observation_period_num': 101, 'train_rates': 0.9888153261519644, 'learning_rate': 1.5664988971305888e-06, 'batch_size': 218, 'step_size': 10, 'gamma': 0.9014757296025944}. Best is trial 0 with value: 0.6829138994216919.[0m
[32m[I 2025-01-03 12:45:15,003][0m Trial 1 finished with value: 0.16962718841482383 and parameters: {'observation_period_num': 223, 'train_rates': 0.9332837917549358, 'learning_rate': 0.0005473597365549946, 'batch_size': 144, 'step_size': 1, 'gamma': 0.9007466899772888}. Best is trial 1 with value: 0.16962718841482383.[0m
[32m[I 2025-01-03 12:46:12,886][0m Trial 2 finished with value: 0.06851244934681636 and parameters: {'observation_period_num': 97, 'train_rates': 0.906321830779212, 'learning_rate': 3.557383498330668e-05, 'batch_size': 99, 'step_size': 10, 'gamma': 0.973462319168975}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:47:02,933][0m Trial 3 finished with value: 0.18983142599777833 and parameters: {'observation_period_num': 78, 'train_rates': 0.8352110249393168, 'learning_rate': 6.725364021530982e-06, 'batch_size': 112, 'step_size': 13, 'gamma': 0.8787294840435244}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:47:35,187][0m Trial 4 finished with value: 0.2274276652924262 and parameters: {'observation_period_num': 111, 'train_rates': 0.8674928001389945, 'learning_rate': 2.397336740539584e-05, 'batch_size': 189, 'step_size': 6, 'gamma': 0.7528288006942129}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:48:09,158][0m Trial 5 finished with value: 0.07392176985740662 and parameters: {'observation_period_num': 49, 'train_rates': 0.9603663095675787, 'learning_rate': 0.0003622438169625873, 'batch_size': 194, 'step_size': 7, 'gamma': 0.9818686960565576}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:48:47,388][0m Trial 6 finished with value: 0.31089828233378486 and parameters: {'observation_period_num': 118, 'train_rates': 0.7144609294897699, 'learning_rate': 2.727452346515464e-05, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9030061165019043}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:49:12,722][0m Trial 7 finished with value: 0.13450109958648682 and parameters: {'observation_period_num': 218, 'train_rates': 0.9221311995822419, 'learning_rate': 4.765178985614513e-05, 'batch_size': 230, 'step_size': 3, 'gamma': 0.9752833911603499}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:51:06,051][0m Trial 8 finished with value: 0.14275556037059198 and parameters: {'observation_period_num': 122, 'train_rates': 0.885126434075489, 'learning_rate': 5.620871119043928e-06, 'batch_size': 48, 'step_size': 9, 'gamma': 0.9381885720920606}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:53:16,267][0m Trial 9 finished with value: 0.08626256138086319 and parameters: {'observation_period_num': 93, 'train_rates': 0.986511220304731, 'learning_rate': 2.6905954255402437e-05, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9139194739185719}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:53:58,015][0m Trial 10 finished with value: 0.2910309938320439 and parameters: {'observation_period_num': 169, 'train_rates': 0.6037621823510384, 'learning_rate': 0.00012922189831555215, 'batch_size': 101, 'step_size': 11, 'gamma': 0.8111411442431359}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:54:30,124][0m Trial 11 finished with value: 0.16731259555466696 and parameters: {'observation_period_num': 15, 'train_rates': 0.7831848697909373, 'learning_rate': 0.0007770980537766475, 'batch_size': 176, 'step_size': 5, 'gamma': 0.9853118388161205}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:55:37,027][0m Trial 12 finished with value: 0.1875784336883999 and parameters: {'observation_period_num': 30, 'train_rates': 0.7838616523188194, 'learning_rate': 0.00020545295049478792, 'batch_size': 78, 'step_size': 15, 'gamma': 0.9656731684639128}. Best is trial 2 with value: 0.06851244934681636.[0m
[32m[I 2025-01-03 12:56:12,481][0m Trial 13 finished with value: 0.059311245348961913 and parameters: {'observation_period_num': 51, 'train_rates': 0.9299302607090477, 'learning_rate': 0.00016333708074990012, 'batch_size': 171, 'step_size': 8, 'gamma': 0.84261036907574}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 12:56:48,021][0m Trial 14 finished with value: 0.09571798518300056 and parameters: {'observation_period_num': 165, 'train_rates': 0.8882313112777508, 'learning_rate': 8.510862961515976e-05, 'batch_size': 156, 'step_size': 11, 'gamma': 0.8286271150443011}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 13:01:18,655][0m Trial 15 finished with value: 0.07528941032348895 and parameters: {'observation_period_num': 49, 'train_rates': 0.8245618435650605, 'learning_rate': 1.035114462411057e-05, 'batch_size': 19, 'step_size': 4, 'gamma': 0.8393379347861565}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 13:02:08,166][0m Trial 16 finished with value: 0.2396514337461604 and parameters: {'observation_period_num': 158, 'train_rates': 0.6960300162102898, 'learning_rate': 0.00010358652321490962, 'batch_size': 95, 'step_size': 8, 'gamma': 0.7856834704245697}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 13:02:33,934][0m Trial 17 finished with value: 0.062359560281038284 and parameters: {'observation_period_num': 70, 'train_rates': 0.9288018619751472, 'learning_rate': 0.00023717396483989876, 'batch_size': 247, 'step_size': 9, 'gamma': 0.8534399525578342}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 13:02:57,528][0m Trial 18 finished with value: 0.06082386005848035 and parameters: {'observation_period_num': 57, 'train_rates': 0.848376358653127, 'learning_rate': 0.0002785530506242255, 'batch_size': 250, 'step_size': 8, 'gamma': 0.8503728822848556}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 13:03:22,089][0m Trial 19 finished with value: 0.2137034758925438 and parameters: {'observation_period_num': 54, 'train_rates': 0.7392532344753535, 'learning_rate': 0.00035939195205639116, 'batch_size': 216, 'step_size': 2, 'gamma': 0.8672943034052979}. Best is trial 13 with value: 0.059311245348961913.[0m
[32m[I 2025-01-03 13:03:46,170][0m Trial 20 finished with value: 0.03641281419784675 and parameters: {'observation_period_num': 8, 'train_rates': 0.8443142540793855, 'learning_rate': 0.0009738613409362301, 'batch_size': 253, 'step_size': 5, 'gamma': 0.8080630822278653}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:04:12,245][0m Trial 21 finished with value: 0.04314783137680879 and parameters: {'observation_period_num': 9, 'train_rates': 0.8181234695356586, 'learning_rate': 0.000879512407008316, 'batch_size': 248, 'step_size': 5, 'gamma': 0.8015729889246244}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:04:40,337][0m Trial 22 finished with value: 0.0418840346224222 and parameters: {'observation_period_num': 7, 'train_rates': 0.8025712470375904, 'learning_rate': 0.0009318664639543172, 'batch_size': 231, 'step_size': 5, 'gamma': 0.7937184030560827}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:05:05,261][0m Trial 23 finished with value: 0.17640806443198434 and parameters: {'observation_period_num': 8, 'train_rates': 0.7569506248653644, 'learning_rate': 0.0009057549947045553, 'batch_size': 252, 'step_size': 5, 'gamma': 0.7761921072818959}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:05:35,561][0m Trial 24 finished with value: 0.05044918730330299 and parameters: {'observation_period_num': 26, 'train_rates': 0.8059500132211272, 'learning_rate': 0.0006373782294268603, 'batch_size': 228, 'step_size': 4, 'gamma': 0.8069638187958212}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:06:04,752][0m Trial 25 finished with value: 0.15325418883934616 and parameters: {'observation_period_num': 7, 'train_rates': 0.6605094837963206, 'learning_rate': 0.0008833315215142572, 'batch_size': 205, 'step_size': 3, 'gamma': 0.784897627090979}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:06:34,596][0m Trial 26 finished with value: 0.05016309525323125 and parameters: {'observation_period_num': 29, 'train_rates': 0.8067874976617327, 'learning_rate': 0.000564430361548597, 'batch_size': 256, 'step_size': 6, 'gamma': 0.7574909777245081}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:07:04,194][0m Trial 27 finished with value: 0.26632286697758767 and parameters: {'observation_period_num': 142, 'train_rates': 0.7602489657621878, 'learning_rate': 0.00045033383785635794, 'batch_size': 233, 'step_size': 5, 'gamma': 0.8151393149818755}. Best is trial 20 with value: 0.03641281419784675.[0m
Early stopping at epoch 50
[32m[I 2025-01-03 13:07:21,624][0m Trial 28 finished with value: 0.218427018516443 and parameters: {'observation_period_num': 29, 'train_rates': 0.8494990195753396, 'learning_rate': 6.46692662261625e-05, 'batch_size': 205, 'step_size': 1, 'gamma': 0.7733287597162624}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:07:45,934][0m Trial 29 finished with value: 1.0199798242219034 and parameters: {'observation_period_num': 203, 'train_rates': 0.8195085169728897, 'learning_rate': 1.3633102654657409e-06, 'batch_size': 235, 'step_size': 3, 'gamma': 0.7943345892273564}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:08:13,959][0m Trial 30 finished with value: 0.4484063782238409 and parameters: {'observation_period_num': 36, 'train_rates': 0.866786152834951, 'learning_rate': 2.6844039318671966e-06, 'batch_size': 219, 'step_size': 6, 'gamma': 0.8278312617688988}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:08:42,716][0m Trial 31 finished with value: 0.047792208882478565 and parameters: {'observation_period_num': 5, 'train_rates': 0.797302466249659, 'learning_rate': 0.0005259087075284944, 'batch_size': 252, 'step_size': 6, 'gamma': 0.7511685796691142}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:09:09,138][0m Trial 32 finished with value: 0.19328278598358045 and parameters: {'observation_period_num': 15, 'train_rates': 0.7835985958959646, 'learning_rate': 0.0004942486591577168, 'batch_size': 240, 'step_size': 4, 'gamma': 0.799817345860487}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:09:36,550][0m Trial 33 finished with value: 0.15740482680810805 and parameters: {'observation_period_num': 8, 'train_rates': 0.7398299951688452, 'learning_rate': 0.000990277594256777, 'batch_size': 215, 'step_size': 7, 'gamma': 0.7643334601674954}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:10:02,017][0m Trial 34 finished with value: 0.07365137479684931 and parameters: {'observation_period_num': 76, 'train_rates': 0.8438480726040715, 'learning_rate': 0.0003198286670107203, 'batch_size': 254, 'step_size': 5, 'gamma': 0.767870709476508}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:10:29,333][0m Trial 35 finished with value: 0.28635704627310277 and parameters: {'observation_period_num': 246, 'train_rates': 0.764548283780447, 'learning_rate': 0.0005803512109293431, 'batch_size': 199, 'step_size': 6, 'gamma': 0.8219443663378795}. Best is trial 20 with value: 0.03641281419784675.[0m
Early stopping at epoch 88
[32m[I 2025-01-03 13:10:57,846][0m Trial 36 finished with value: 0.11705317165064548 and parameters: {'observation_period_num': 39, 'train_rates': 0.8760171394493323, 'learning_rate': 0.00018432680070332752, 'batch_size': 183, 'step_size': 2, 'gamma': 0.7506116737053872}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:11:24,684][0m Trial 37 finished with value: 0.07120324134427522 and parameters: {'observation_period_num': 66, 'train_rates': 0.8062303096899059, 'learning_rate': 0.000434498898865351, 'batch_size': 221, 'step_size': 4, 'gamma': 0.7894688379498597}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:11:51,691][0m Trial 38 finished with value: 0.0374340574995455 and parameters: {'observation_period_num': 18, 'train_rates': 0.8291678831698766, 'learning_rate': 0.0007027369252297364, 'batch_size': 240, 'step_size': 7, 'gamma': 0.8755998217510323}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:12:18,018][0m Trial 39 finished with value: 0.0810373789215 and parameters: {'observation_period_num': 88, 'train_rates': 0.9056058937740156, 'learning_rate': 0.0007359482035286799, 'batch_size': 241, 'step_size': 7, 'gamma': 0.8772760195711234}. Best is trial 20 with value: 0.03641281419784675.[0m
[32m[I 2025-01-03 13:12:54,038][0m Trial 40 finished with value: 0.03630844631580391 and parameters: {'observation_period_num': 22, 'train_rates': 0.8311824066419422, 'learning_rate': 0.00025423964413042274, 'batch_size': 158, 'step_size': 9, 'gamma': 0.8919713428040859}. Best is trial 40 with value: 0.03630844631580391.[0m
[32m[I 2025-01-03 13:13:36,526][0m Trial 41 finished with value: 0.07714753668932688 and parameters: {'observation_period_num': 20, 'train_rates': 0.8282112806938206, 'learning_rate': 1.531683961366269e-05, 'batch_size': 132, 'step_size': 9, 'gamma': 0.8923205934694185}. Best is trial 40 with value: 0.03630844631580391.[0m
[32m[I 2025-01-03 13:14:04,216][0m Trial 42 finished with value: 0.06850764997160205 and parameters: {'observation_period_num': 44, 'train_rates': 0.85691387808748, 'learning_rate': 0.00031585538706695496, 'batch_size': 226, 'step_size': 10, 'gamma': 0.9202745914852744}. Best is trial 40 with value: 0.03630844631580391.[0m
[32m[I 2025-01-03 13:14:43,627][0m Trial 43 finished with value: 0.028349023644945452 and parameters: {'observation_period_num': 19, 'train_rates': 0.9047440379192491, 'learning_rate': 0.0006910777661350586, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8611674426915483}. Best is trial 43 with value: 0.028349023644945452.[0m
[32m[I 2025-01-03 13:15:24,360][0m Trial 44 finished with value: 0.04175740252368343 and parameters: {'observation_period_num': 41, 'train_rates': 0.8930027470038483, 'learning_rate': 0.0006619510945608016, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8667785566503066}. Best is trial 43 with value: 0.028349023644945452.[0m
[32m[I 2025-01-03 13:16:05,931][0m Trial 45 finished with value: 0.0333634652197361 and parameters: {'observation_period_num': 20, 'train_rates': 0.9534790440672777, 'learning_rate': 0.0003960210118975413, 'batch_size': 153, 'step_size': 11, 'gamma': 0.8954659333669056}. Best is trial 43 with value: 0.028349023644945452.[0m
[32m[I 2025-01-03 13:16:45,480][0m Trial 46 finished with value: 0.04567863792181015 and parameters: {'observation_period_num': 21, 'train_rates': 0.9601660027469099, 'learning_rate': 0.000150874603464204, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8921520600981276}. Best is trial 43 with value: 0.028349023644945452.[0m
[32m[I 2025-01-03 13:17:39,279][0m Trial 47 finished with value: 0.10621237009763718 and parameters: {'observation_period_num': 106, 'train_rates': 0.973405687907814, 'learning_rate': 0.00023061775584511866, 'batch_size': 114, 'step_size': 12, 'gamma': 0.9394073052090557}. Best is trial 43 with value: 0.028349023644945452.[0m
[32m[I 2025-01-03 13:18:22,970][0m Trial 48 finished with value: 0.07252690941095352 and parameters: {'observation_period_num': 64, 'train_rates': 0.9555438775736086, 'learning_rate': 0.0003993678331321311, 'batch_size': 142, 'step_size': 12, 'gamma': 0.9058692983956175}. Best is trial 43 with value: 0.028349023644945452.[0m
[32m[I 2025-01-03 13:19:11,571][0m Trial 49 finished with value: 0.05574892945010071 and parameters: {'observation_period_num': 86, 'train_rates': 0.9102715070043117, 'learning_rate': 0.00012081469863432564, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9296191873398056}. Best is trial 43 with value: 0.028349023644945452.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 13:19:11,581][0m A new study created in memory with name: no-name-0e5f27f6-be9a-4334-b82c-1d27def690c1[0m
[32m[I 2025-01-03 13:20:13,129][0m Trial 0 finished with value: 0.7551662220411572 and parameters: {'observation_period_num': 80, 'train_rates': 0.7678978324669324, 'learning_rate': 1.01702439845829e-06, 'batch_size': 83, 'step_size': 7, 'gamma': 0.8483186446303055}. Best is trial 0 with value: 0.7551662220411572.[0m
[32m[I 2025-01-03 13:20:49,741][0m Trial 1 finished with value: 0.1571764361299028 and parameters: {'observation_period_num': 248, 'train_rates': 0.8739520647468355, 'learning_rate': 3.762060797419437e-05, 'batch_size': 156, 'step_size': 11, 'gamma': 0.7858320695478173}. Best is trial 1 with value: 0.1571764361299028.[0m
[32m[I 2025-01-03 13:22:24,183][0m Trial 2 finished with value: 0.06497689669194207 and parameters: {'observation_period_num': 60, 'train_rates': 0.8870972782527047, 'learning_rate': 8.39091102708915e-05, 'batch_size': 59, 'step_size': 3, 'gamma': 0.8327742689242549}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:23:16,845][0m Trial 3 finished with value: 0.44407528135367247 and parameters: {'observation_period_num': 71, 'train_rates': 0.8728812071763101, 'learning_rate': 1.0683653677577197e-06, 'batch_size': 107, 'step_size': 10, 'gamma': 0.8463339214073531}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:23:40,473][0m Trial 4 finished with value: 0.15579330082982779 and parameters: {'observation_period_num': 238, 'train_rates': 0.8998523914486289, 'learning_rate': 4.581562179402431e-05, 'batch_size': 255, 'step_size': 9, 'gamma': 0.8336059753174262}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:24:46,004][0m Trial 5 finished with value: 0.3224898070646321 and parameters: {'observation_period_num': 132, 'train_rates': 0.8462911442546395, 'learning_rate': 1.908900106090504e-06, 'batch_size': 82, 'step_size': 13, 'gamma': 0.8831117531035146}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:25:20,485][0m Trial 6 finished with value: 0.13585491671514668 and parameters: {'observation_period_num': 5, 'train_rates': 0.795663330343791, 'learning_rate': 2.7526279085545983e-06, 'batch_size': 164, 'step_size': 13, 'gamma': 0.8019313484124058}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:25:48,754][0m Trial 7 finished with value: 0.40877959077410836 and parameters: {'observation_period_num': 161, 'train_rates': 0.756582192563844, 'learning_rate': 4.6225512697939285e-05, 'batch_size': 185, 'step_size': 5, 'gamma': 0.7560038424642792}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:26:23,221][0m Trial 8 finished with value: 0.5469367903157284 and parameters: {'observation_period_num': 224, 'train_rates': 0.825994826771103, 'learning_rate': 1.6222282454617608e-06, 'batch_size': 158, 'step_size': 13, 'gamma': 0.8203408229660734}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:28:48,778][0m Trial 9 finished with value: 0.3443167612886966 and parameters: {'observation_period_num': 172, 'train_rates': 0.6485344589819554, 'learning_rate': 9.292999054657172e-06, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8227960753385024}. Best is trial 2 with value: 0.06497689669194207.[0m
[32m[I 2025-01-03 13:34:16,198][0m Trial 10 finished with value: 0.03778455701639068 and parameters: {'observation_period_num': 5, 'train_rates': 0.9822483546766331, 'learning_rate': 0.00047937624603642723, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9397359970896697}. Best is trial 10 with value: 0.03778455701639068.[0m
[32m[I 2025-01-03 13:37:46,818][0m Trial 11 finished with value: 0.041042884811758995 and parameters: {'observation_period_num': 12, 'train_rates': 0.9660970812472749, 'learning_rate': 0.0006020058091739612, 'batch_size': 28, 'step_size': 1, 'gamma': 0.95180876237105}. Best is trial 10 with value: 0.03778455701639068.[0m
[32m[I 2025-01-03 13:43:34,403][0m Trial 12 finished with value: 0.03740004740308884 and parameters: {'observation_period_num': 18, 'train_rates': 0.985351484788381, 'learning_rate': 0.0007205370190948751, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9566703656917221}. Best is trial 12 with value: 0.03740004740308884.[0m
[32m[I 2025-01-03 13:49:42,059][0m Trial 13 finished with value: 0.02718198224902153 and parameters: {'observation_period_num': 38, 'train_rates': 0.9865478027156104, 'learning_rate': 0.0006073684298445799, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9673104092197924}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:51:28,266][0m Trial 14 finished with value: 0.06209881940340421 and parameters: {'observation_period_num': 47, 'train_rates': 0.9431709883674346, 'learning_rate': 0.00022035603219938587, 'batch_size': 55, 'step_size': 4, 'gamma': 0.9877979401601746}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:52:13,960][0m Trial 15 finished with value: 0.17572494042394435 and parameters: {'observation_period_num': 40, 'train_rates': 0.6895299372989325, 'learning_rate': 0.00021652257567085912, 'batch_size': 109, 'step_size': 3, 'gamma': 0.9024410043417491}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:52:43,050][0m Trial 16 finished with value: 0.056764550507068634 and parameters: {'observation_period_num': 104, 'train_rates': 0.933910132211776, 'learning_rate': 0.0009970349092424111, 'batch_size': 214, 'step_size': 1, 'gamma': 0.9820589576113143}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:54:19,273][0m Trial 17 finished with value: 0.08179309219121933 and parameters: {'observation_period_num': 109, 'train_rates': 0.9831959906109952, 'learning_rate': 0.00018273899347552046, 'batch_size': 61, 'step_size': 5, 'gamma': 0.92257542255285}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:55:06,993][0m Trial 18 finished with value: 0.11788413280973563 and parameters: {'observation_period_num': 35, 'train_rates': 0.924067849904932, 'learning_rate': 1.2860346882055456e-05, 'batch_size': 125, 'step_size': 3, 'gamma': 0.9633420473081229}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:56:36,776][0m Trial 19 finished with value: 0.19546963795121894 and parameters: {'observation_period_num': 29, 'train_rates': 0.6026667126650533, 'learning_rate': 0.00010609058378863206, 'batch_size': 48, 'step_size': 6, 'gamma': 0.9154735460000853}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 13:57:34,533][0m Trial 20 finished with value: 0.21059043307083541 and parameters: {'observation_period_num': 90, 'train_rates': 0.7180742099975566, 'learning_rate': 0.0004132262654590736, 'batch_size': 85, 'step_size': 2, 'gamma': 0.8737981076299983}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:03:24,057][0m Trial 21 finished with value: 0.03214011701162566 and parameters: {'observation_period_num': 13, 'train_rates': 0.9852056923690699, 'learning_rate': 0.0004585054412000515, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9433394680091686}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:08:37,369][0m Trial 22 finished with value: 0.043471344870466715 and parameters: {'observation_period_num': 53, 'train_rates': 0.9405321487628611, 'learning_rate': 0.0009439407264959349, 'batch_size': 18, 'step_size': 2, 'gamma': 0.963401256163912}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:10:55,223][0m Trial 23 finished with value: 0.04483638615491464 and parameters: {'observation_period_num': 28, 'train_rates': 0.9669183151811351, 'learning_rate': 0.0003484284692150853, 'batch_size': 43, 'step_size': 15, 'gamma': 0.9354040984262526}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:13:24,354][0m Trial 24 finished with value: 0.03467079669688687 and parameters: {'observation_period_num': 22, 'train_rates': 0.9099102427207859, 'learning_rate': 0.0001251857370876507, 'batch_size': 38, 'step_size': 4, 'gamma': 0.8967187325752874}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:15:59,785][0m Trial 25 finished with value: 0.06733988461051534 and parameters: {'observation_period_num': 61, 'train_rates': 0.9125317633408137, 'learning_rate': 0.00010385720490363771, 'batch_size': 36, 'step_size': 4, 'gamma': 0.9024204718731026}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:17:24,136][0m Trial 26 finished with value: 0.08285100667076256 and parameters: {'observation_period_num': 147, 'train_rates': 0.9535207487371871, 'learning_rate': 0.0003243038858549671, 'batch_size': 67, 'step_size': 2, 'gamma': 0.904363040970912}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:18:17,694][0m Trial 27 finished with value: 0.06108508855104446 and parameters: {'observation_period_num': 96, 'train_rates': 0.8530441852461341, 'learning_rate': 0.00016271319789813894, 'batch_size': 103, 'step_size': 4, 'gamma': 0.8866497303070858}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:19:36,547][0m Trial 28 finished with value: 0.060611683626969656 and parameters: {'observation_period_num': 25, 'train_rates': 0.9129947767282776, 'learning_rate': 2.064074475061844e-05, 'batch_size': 74, 'step_size': 5, 'gamma': 0.9259741411468333}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:22:00,644][0m Trial 29 finished with value: 0.09543850795021207 and parameters: {'observation_period_num': 80, 'train_rates': 0.9560214904593988, 'learning_rate': 0.00013115870569010724, 'batch_size': 40, 'step_size': 7, 'gamma': 0.9717452416279094}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:22:57,007][0m Trial 30 finished with value: 0.129386118958184 and parameters: {'observation_period_num': 204, 'train_rates': 0.8104984604499255, 'learning_rate': 6.720493598928433e-05, 'batch_size': 91, 'step_size': 2, 'gamma': 0.9491147376435318}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:26:16,824][0m Trial 31 finished with value: 0.036455544219775635 and parameters: {'observation_period_num': 20, 'train_rates': 0.9889805758215038, 'learning_rate': 0.0006427583756321379, 'batch_size': 30, 'step_size': 1, 'gamma': 0.952082533778778}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:28:58,693][0m Trial 32 finished with value: 0.04261747982214998 and parameters: {'observation_period_num': 43, 'train_rates': 0.9817304937451243, 'learning_rate': 0.00029790031186411063, 'batch_size': 37, 'step_size': 3, 'gamma': 0.9772029327578483}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:30:45,293][0m Trial 33 finished with value: 0.030903304820092452 and parameters: {'observation_period_num': 18, 'train_rates': 0.8783544452465009, 'learning_rate': 0.000553941951390159, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9483941697911242}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:32:29,664][0m Trial 34 finished with value: 0.05619421572554359 and parameters: {'observation_period_num': 63, 'train_rates': 0.8835099254593707, 'learning_rate': 0.0002570894971956168, 'batch_size': 54, 'step_size': 2, 'gamma': 0.9337286734560893}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:33:50,806][0m Trial 35 finished with value: 0.054297017166391015 and parameters: {'observation_period_num': 72, 'train_rates': 0.8667622504745144, 'learning_rate': 0.0004455628453980497, 'batch_size': 68, 'step_size': 3, 'gamma': 0.8537229448961403}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:39:34,415][0m Trial 36 finished with value: 0.05687329173088074 and parameters: {'observation_period_num': 49, 'train_rates': 0.9077759769566028, 'learning_rate': 7.126924625184141e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9146241965872132}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:41:20,959][0m Trial 37 finished with value: 0.047875686100975895 and parameters: {'observation_period_num': 34, 'train_rates': 0.8404282923167785, 'learning_rate': 0.0006157479027192762, 'batch_size': 51, 'step_size': 4, 'gamma': 0.8629887179579363}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:42:03,905][0m Trial 38 finished with value: 0.03651251290973864 and parameters: {'observation_period_num': 16, 'train_rates': 0.8965490716629091, 'learning_rate': 0.00016147792323366957, 'batch_size': 144, 'step_size': 10, 'gamma': 0.8905958743675022}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:42:27,939][0m Trial 39 finished with value: 0.22242244554217905 and parameters: {'observation_period_num': 5, 'train_rates': 0.7612664706925261, 'learning_rate': 2.314449233470863e-05, 'batch_size': 253, 'step_size': 6, 'gamma': 0.9414373663358105}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:43:39,795][0m Trial 40 finished with value: 0.20871692955493926 and parameters: {'observation_period_num': 72, 'train_rates': 0.8786966980116742, 'learning_rate': 4.231292088426877e-06, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9689801328006143}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:46:57,621][0m Trial 41 finished with value: 0.03835217749737963 and parameters: {'observation_period_num': 20, 'train_rates': 0.9332370798517859, 'learning_rate': 0.0007119280668694835, 'batch_size': 29, 'step_size': 1, 'gamma': 0.9477801740723291}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:50:14,088][0m Trial 42 finished with value: 0.042673587517918284 and parameters: {'observation_period_num': 24, 'train_rates': 0.9639035741720999, 'learning_rate': 0.0005073768603364817, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9608978093837671}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:52:19,712][0m Trial 43 finished with value: 0.07089402771421842 and parameters: {'observation_period_num': 56, 'train_rates': 0.9517801589421437, 'learning_rate': 0.00033737342111544104, 'batch_size': 46, 'step_size': 3, 'gamma': 0.9887204915892511}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:55:35,316][0m Trial 44 finished with value: 0.03195358057107244 and parameters: {'observation_period_num': 12, 'train_rates': 0.9882725236369533, 'learning_rate': 0.0008565034927293326, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9274110392631494}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 14:57:04,279][0m Trial 45 finished with value: 0.03111694120486413 and parameters: {'observation_period_num': 5, 'train_rates': 0.7947420951082436, 'learning_rate': 0.00088751128641865, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9268146306116353}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 15:00:28,415][0m Trial 46 finished with value: 0.1701823168091084 and parameters: {'observation_period_num': 5, 'train_rates': 0.7873258569016434, 'learning_rate': 0.000879892928590496, 'batch_size': 25, 'step_size': 2, 'gamma': 0.9252422267067336}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 15:01:48,606][0m Trial 47 finished with value: 0.17387775409832057 and parameters: {'observation_period_num': 39, 'train_rates': 0.7150576044029952, 'learning_rate': 0.0005384726723371321, 'batch_size': 61, 'step_size': 1, 'gamma': 0.9124282010900603}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 15:02:21,165][0m Trial 48 finished with value: 0.034378448960751914 and parameters: {'observation_period_num': 12, 'train_rates': 0.819596145605057, 'learning_rate': 0.0008326560824235056, 'batch_size': 175, 'step_size': 12, 'gamma': 0.7643042837285468}. Best is trial 13 with value: 0.02718198224902153.[0m
[32m[I 2025-01-03 15:03:13,698][0m Trial 49 finished with value: 0.09044954740425569 and parameters: {'observation_period_num': 125, 'train_rates': 0.7839047935276356, 'learning_rate': 0.0004079711653645497, 'batch_size': 97, 'step_size': 1, 'gamma': 0.9328188871649189}. Best is trial 13 with value: 0.02718198224902153.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 15:03:13,707][0m A new study created in memory with name: no-name-e9f70557-ebfa-4dd4-aa14-88250370a436[0m
[32m[I 2025-01-03 15:04:00,193][0m Trial 0 finished with value: 0.3904184927313314 and parameters: {'observation_period_num': 211, 'train_rates': 0.6215289381399414, 'learning_rate': 1.3534041955555012e-05, 'batch_size': 94, 'step_size': 8, 'gamma': 0.8805034386203578}. Best is trial 0 with value: 0.3904184927313314.[0m
[32m[I 2025-01-03 15:04:25,826][0m Trial 1 finished with value: 0.3952482342720032 and parameters: {'observation_period_num': 209, 'train_rates': 0.9219616703925849, 'learning_rate': 1.2179380206082172e-05, 'batch_size': 246, 'step_size': 6, 'gamma': 0.7827799331416393}. Best is trial 0 with value: 0.3904184927313314.[0m
[32m[I 2025-01-03 15:05:40,689][0m Trial 2 finished with value: 0.16405014383900646 and parameters: {'observation_period_num': 17, 'train_rates': 0.7552248567704704, 'learning_rate': 0.00024805510827334576, 'batch_size': 68, 'step_size': 15, 'gamma': 0.8419191924580538}. Best is trial 2 with value: 0.16405014383900646.[0m
[32m[I 2025-01-03 15:06:20,412][0m Trial 3 finished with value: 0.11817765777761285 and parameters: {'observation_period_num': 93, 'train_rates': 0.9114912199755727, 'learning_rate': 0.0003479049528456837, 'batch_size': 146, 'step_size': 1, 'gamma': 0.8781836758539682}. Best is trial 3 with value: 0.11817765777761285.[0m
[32m[I 2025-01-03 15:06:44,717][0m Trial 4 finished with value: 0.27416381911849075 and parameters: {'observation_period_num': 193, 'train_rates': 0.8078320233907847, 'learning_rate': 0.00015221582582854843, 'batch_size': 223, 'step_size': 2, 'gamma': 0.7825089154898935}. Best is trial 3 with value: 0.11817765777761285.[0m
[32m[I 2025-01-03 15:07:12,077][0m Trial 5 finished with value: 0.062607541680336 and parameters: {'observation_period_num': 8, 'train_rates': 0.9763232572017707, 'learning_rate': 6.769409739411119e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.8932914111734807}. Best is trial 5 with value: 0.062607541680336.[0m
[32m[I 2025-01-03 15:08:07,491][0m Trial 6 finished with value: 0.6939179632624299 and parameters: {'observation_period_num': 169, 'train_rates': 0.6307849348378407, 'learning_rate': 3.2639222779592144e-06, 'batch_size': 79, 'step_size': 9, 'gamma': 0.9076764531180216}. Best is trial 5 with value: 0.062607541680336.[0m
[32m[I 2025-01-03 15:09:11,272][0m Trial 7 finished with value: 0.12982347285555254 and parameters: {'observation_period_num': 148, 'train_rates': 0.9448281871231052, 'learning_rate': 0.0003699080718750065, 'batch_size': 89, 'step_size': 15, 'gamma': 0.9309482109348082}. Best is trial 5 with value: 0.062607541680336.[0m
[32m[I 2025-01-03 15:09:51,242][0m Trial 8 finished with value: 0.2848706723828065 and parameters: {'observation_period_num': 147, 'train_rates': 0.6547083691206004, 'learning_rate': 1.628845366463656e-05, 'batch_size': 118, 'step_size': 14, 'gamma': 0.930777834370575}. Best is trial 5 with value: 0.062607541680336.[0m
[32m[I 2025-01-03 15:11:34,185][0m Trial 9 finished with value: 0.24921854902505874 and parameters: {'observation_period_num': 106, 'train_rates': 0.7806326945144284, 'learning_rate': 2.6531739660022518e-05, 'batch_size': 49, 'step_size': 6, 'gamma': 0.8818987614394822}. Best is trial 5 with value: 0.062607541680336.[0m
[32m[I 2025-01-03 15:12:10,174][0m Trial 10 finished with value: 0.04210824891924858 and parameters: {'observation_period_num': 14, 'train_rates': 0.9832109309331304, 'learning_rate': 9.919534767760395e-05, 'batch_size': 188, 'step_size': 11, 'gamma': 0.9847510788404565}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:12:46,565][0m Trial 11 finished with value: 0.04927215725183487 and parameters: {'observation_period_num': 22, 'train_rates': 0.9884459376364301, 'learning_rate': 7.963662022660916e-05, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9771096148268658}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:13:23,534][0m Trial 12 finished with value: 0.0730406805067449 and parameters: {'observation_period_num': 56, 'train_rates': 0.8594248712184103, 'learning_rate': 7.908998965133411e-05, 'batch_size': 165, 'step_size': 12, 'gamma': 0.9899018206118848}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:14:00,867][0m Trial 13 finished with value: 0.05496896803379059 and parameters: {'observation_period_num': 52, 'train_rates': 0.9826139092227465, 'learning_rate': 0.0007734671119362139, 'batch_size': 192, 'step_size': 11, 'gamma': 0.9888965665282207}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:14:34,930][0m Trial 14 finished with value: 0.4717716909055685 and parameters: {'observation_period_num': 60, 'train_rates': 0.8697063358797363, 'learning_rate': 1.121027399448852e-06, 'batch_size': 191, 'step_size': 12, 'gamma': 0.9666364163749407}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:15:03,055][0m Trial 15 finished with value: 0.30444724402440076 and parameters: {'observation_period_num': 244, 'train_rates': 0.7162669384302289, 'learning_rate': 8.237299815462759e-05, 'batch_size': 188, 'step_size': 10, 'gamma': 0.9428982826959196}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:19:45,878][0m Trial 16 finished with value: 0.04223775526478807 and parameters: {'observation_period_num': 31, 'train_rates': 0.8720979106045922, 'learning_rate': 3.5274805634496426e-05, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8452185686229023}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:22:25,645][0m Trial 17 finished with value: 0.10821705110944234 and parameters: {'observation_period_num': 89, 'train_rates': 0.8639402469983657, 'learning_rate': 5.3370747550636724e-06, 'batch_size': 33, 'step_size': 13, 'gamma': 0.8367148563713624}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:26:15,190][0m Trial 18 finished with value: 0.04815905094146729 and parameters: {'observation_period_num': 39, 'train_rates': 0.8971520377309312, 'learning_rate': 3.693027971294504e-05, 'batch_size': 24, 'step_size': 6, 'gamma': 0.8359123626680542}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:26:58,177][0m Trial 19 finished with value: 0.08483903018151175 and parameters: {'observation_period_num': 80, 'train_rates': 0.8194144327781547, 'learning_rate': 3.2475151251607955e-05, 'batch_size': 130, 'step_size': 13, 'gamma': 0.8158193243125911}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:27:23,382][0m Trial 20 finished with value: 0.20825725905169015 and parameters: {'observation_period_num': 121, 'train_rates': 0.692859283806525, 'learning_rate': 0.0009666910044057521, 'batch_size': 215, 'step_size': 10, 'gamma': 0.7522487762858192}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:31:11,941][0m Trial 21 finished with value: 0.04794854028640609 and parameters: {'observation_period_num': 35, 'train_rates': 0.8957753352127873, 'learning_rate': 3.8764678558062356e-05, 'batch_size': 24, 'step_size': 5, 'gamma': 0.8465214366051839}. Best is trial 10 with value: 0.04210824891924858.[0m
[32m[I 2025-01-03 15:33:10,530][0m Trial 22 finished with value: 0.041415217646592575 and parameters: {'observation_period_num': 34, 'train_rates': 0.9412464837662746, 'learning_rate': 0.00016896707177029068, 'batch_size': 49, 'step_size': 4, 'gamma': 0.8558389963475755}. Best is trial 22 with value: 0.041415217646592575.[0m
[32m[I 2025-01-03 15:34:58,938][0m Trial 23 finished with value: 0.07423426686571194 and parameters: {'observation_period_num': 67, 'train_rates': 0.9550681829984363, 'learning_rate': 0.00019286496068785785, 'batch_size': 53, 'step_size': 3, 'gamma': 0.8101630789392081}. Best is trial 22 with value: 0.041415217646592575.[0m
[32m[I 2025-01-03 15:35:52,823][0m Trial 24 finished with value: 0.05957916473760407 and parameters: {'observation_period_num': 33, 'train_rates': 0.9420972332150905, 'learning_rate': 0.0001322657535492662, 'batch_size': 110, 'step_size': 3, 'gamma': 0.8564472582631119}. Best is trial 22 with value: 0.041415217646592575.[0m
[32m[I 2025-01-03 15:36:31,008][0m Trial 25 finished with value: 0.03092245298297438 and parameters: {'observation_period_num': 11, 'train_rates': 0.8510318374974182, 'learning_rate': 0.0005887544349275441, 'batch_size': 156, 'step_size': 8, 'gamma': 0.9035806570902402}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:37:07,330][0m Trial 26 finished with value: 0.036623293255130536 and parameters: {'observation_period_num': 6, 'train_rates': 0.8437137209396071, 'learning_rate': 0.0003352800710457268, 'batch_size': 158, 'step_size': 4, 'gamma': 0.9054628865097802}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:37:41,963][0m Trial 27 finished with value: 0.05058609362767667 and parameters: {'observation_period_num': 71, 'train_rates': 0.8298706951951998, 'learning_rate': 0.00046974315344371705, 'batch_size': 163, 'step_size': 4, 'gamma': 0.912761932541099}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:38:19,936][0m Trial 28 finished with value: 0.15845521965652662 and parameters: {'observation_period_num': 6, 'train_rates': 0.7621151401946624, 'learning_rate': 0.0006110224165128914, 'batch_size': 144, 'step_size': 7, 'gamma': 0.9037529568304336}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:39:13,325][0m Trial 29 finished with value: 0.04833943887356797 and parameters: {'observation_period_num': 43, 'train_rates': 0.8406587696684381, 'learning_rate': 0.0002871058252526921, 'batch_size': 105, 'step_size': 4, 'gamma': 0.8690649665339045}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:39:47,864][0m Trial 30 finished with value: 0.18157029783311554 and parameters: {'observation_period_num': 6, 'train_rates': 0.7876133308925173, 'learning_rate': 0.0005394323276899701, 'batch_size': 161, 'step_size': 1, 'gamma': 0.9421025617695958}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:40:23,276][0m Trial 31 finished with value: 0.045812226831912994 and parameters: {'observation_period_num': 22, 'train_rates': 0.9570590763609224, 'learning_rate': 0.00013140507887931705, 'batch_size': 177, 'step_size': 8, 'gamma': 0.9545633037727814}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:41:09,051][0m Trial 32 finished with value: 0.038219838749801095 and parameters: {'observation_period_num': 25, 'train_rates': 0.9240409561571467, 'learning_rate': 0.0002272304061431936, 'batch_size': 131, 'step_size': 7, 'gamma': 0.9231579750517204}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:41:54,810][0m Trial 33 finished with value: 0.0486206299790139 and parameters: {'observation_period_num': 51, 'train_rates': 0.9164585194871361, 'learning_rate': 0.0002173526834564449, 'batch_size': 128, 'step_size': 7, 'gamma': 0.917457490259708}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:42:35,036][0m Trial 34 finished with value: 0.03983334431594068 and parameters: {'observation_period_num': 29, 'train_rates': 0.8873330758861789, 'learning_rate': 0.00035380334339385467, 'batch_size': 147, 'step_size': 5, 'gamma': 0.8870220666789677}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:43:15,425][0m Trial 35 finished with value: 0.033289688321116576 and parameters: {'observation_period_num': 23, 'train_rates': 0.891060719954159, 'learning_rate': 0.00038167982090553653, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8917991156962811}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:43:43,666][0m Trial 36 finished with value: 0.0330464059495119 and parameters: {'observation_period_num': 20, 'train_rates': 0.8438802269507768, 'learning_rate': 0.0006511695021574279, 'batch_size': 215, 'step_size': 8, 'gamma': 0.8993786376716641}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:44:11,797][0m Trial 37 finished with value: 0.037973552516077536 and parameters: {'observation_period_num': 5, 'train_rates': 0.8399683227621558, 'learning_rate': 0.0008637706571792664, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8988340150660117}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:44:40,226][0m Trial 38 finished with value: 0.0671632184099605 and parameters: {'observation_period_num': 101, 'train_rates': 0.8163157382694114, 'learning_rate': 0.0004739978468256251, 'batch_size': 204, 'step_size': 8, 'gamma': 0.8710440509014343}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:45:02,567][0m Trial 39 finished with value: 0.1881062914819821 and parameters: {'observation_period_num': 45, 'train_rates': 0.7467580413840799, 'learning_rate': 0.0006191765868207296, 'batch_size': 248, 'step_size': 9, 'gamma': 0.8939539567296845}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:45:37,269][0m Trial 40 finished with value: 0.11275881153586156 and parameters: {'observation_period_num': 205, 'train_rates': 0.8019261706143753, 'learning_rate': 0.00029097937911739055, 'batch_size': 150, 'step_size': 6, 'gamma': 0.8760315212981138}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:46:02,900][0m Trial 41 finished with value: 0.03165242258045409 and parameters: {'observation_period_num': 15, 'train_rates': 0.8469178461622926, 'learning_rate': 0.000971764560679106, 'batch_size': 230, 'step_size': 9, 'gamma': 0.8989116375099545}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:46:28,939][0m Trial 42 finished with value: 0.03393293475538357 and parameters: {'observation_period_num': 22, 'train_rates': 0.849252197299529, 'learning_rate': 0.0009887155867060258, 'batch_size': 236, 'step_size': 8, 'gamma': 0.9013922513734098}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:46:54,960][0m Trial 43 finished with value: 0.032577398129635386 and parameters: {'observation_period_num': 18, 'train_rates': 0.8804897727968626, 'learning_rate': 0.0009428891935541122, 'batch_size': 237, 'step_size': 8, 'gamma': 0.8891002683457964}. Best is trial 25 with value: 0.03092245298297438.[0m
[32m[I 2025-01-03 15:47:22,989][0m Trial 44 finished with value: 0.030297965730473497 and parameters: {'observation_period_num': 19, 'train_rates': 0.8957163064017348, 'learning_rate': 0.0007417394998563828, 'batch_size': 229, 'step_size': 10, 'gamma': 0.9289464207510643}. Best is trial 44 with value: 0.030297965730473497.[0m
[32m[I 2025-01-03 15:47:51,499][0m Trial 45 finished with value: 0.0346365796207492 and parameters: {'observation_period_num': 17, 'train_rates': 0.9089849324119088, 'learning_rate': 0.0006595661351557939, 'batch_size': 234, 'step_size': 10, 'gamma': 0.9298183990835703}. Best is trial 44 with value: 0.030297965730473497.[0m
[32m[I 2025-01-03 15:48:17,528][0m Trial 46 finished with value: 0.0519049582766815 and parameters: {'observation_period_num': 71, 'train_rates': 0.876788563616299, 'learning_rate': 0.0007454471053771497, 'batch_size': 239, 'step_size': 9, 'gamma': 0.9375433257089882}. Best is trial 44 with value: 0.030297965730473497.[0m
[32m[I 2025-01-03 15:48:41,981][0m Trial 47 finished with value: 0.10816607900379878 and parameters: {'observation_period_num': 158, 'train_rates': 0.8589611754336471, 'learning_rate': 0.00044605664253324705, 'batch_size': 256, 'step_size': 9, 'gamma': 0.9595329194967096}. Best is trial 44 with value: 0.030297965730473497.[0m
[32m[I 2025-01-03 15:49:08,638][0m Trial 48 finished with value: 0.1122245703679157 and parameters: {'observation_period_num': 180, 'train_rates': 0.8190721220941778, 'learning_rate': 0.000744351477542132, 'batch_size': 212, 'step_size': 10, 'gamma': 0.9164152221917901}. Best is trial 44 with value: 0.030297965730473497.[0m
[32m[I 2025-01-03 15:49:37,159][0m Trial 49 finished with value: 0.223479328098597 and parameters: {'observation_period_num': 60, 'train_rates': 0.777373221702954, 'learning_rate': 5.487199280249458e-05, 'batch_size': 202, 'step_size': 11, 'gamma': 0.8832486539050618}. Best is trial 44 with value: 0.030297965730473497.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-03 15:49:37,169][0m A new study created in memory with name: no-name-cdc6659f-1122-4856-8111-ecfcf2ce3ec9[0m
[32m[I 2025-01-03 15:51:01,431][0m Trial 0 finished with value: 0.054570476285642405 and parameters: {'observation_period_num': 34, 'train_rates': 0.8158692840318058, 'learning_rate': 3.242723351961541e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7679647959741908}. Best is trial 0 with value: 0.054570476285642405.[0m
[32m[I 2025-01-03 15:56:02,803][0m Trial 1 finished with value: 0.20236299924757725 and parameters: {'observation_period_num': 114, 'train_rates': 0.7825941853400533, 'learning_rate': 0.00022948586533226984, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8656296096739619}. Best is trial 0 with value: 0.054570476285642405.[0m
[32m[I 2025-01-03 15:58:37,504][0m Trial 2 finished with value: 0.1481854468473518 and parameters: {'observation_period_num': 18, 'train_rates': 0.6553938110006726, 'learning_rate': 0.00020289539100097005, 'batch_size': 29, 'step_size': 9, 'gamma': 0.9481803066871918}. Best is trial 0 with value: 0.054570476285642405.[0m
[32m[I 2025-01-03 15:59:28,362][0m Trial 3 finished with value: 0.48483263923410785 and parameters: {'observation_period_num': 198, 'train_rates': 0.9173095577844463, 'learning_rate': 4.037006729750862e-06, 'batch_size': 112, 'step_size': 3, 'gamma': 0.837163693650669}. Best is trial 0 with value: 0.054570476285642405.[0m
[32m[I 2025-01-03 16:00:02,046][0m Trial 4 finished with value: 0.4694115359868322 and parameters: {'observation_period_num': 216, 'train_rates': 0.8979012839263231, 'learning_rate': 1.0994604128372484e-05, 'batch_size': 183, 'step_size': 3, 'gamma': 0.8535892380347657}. Best is trial 0 with value: 0.054570476285642405.[0m
Early stopping at epoch 78
[32m[I 2025-01-03 16:00:24,831][0m Trial 5 finished with value: 0.6052719354629517 and parameters: {'observation_period_num': 123, 'train_rates': 0.9704244838338933, 'learning_rate': 1.6913045873273403e-05, 'batch_size': 254, 'step_size': 1, 'gamma': 0.8680955193021413}. Best is trial 0 with value: 0.054570476285642405.[0m
[32m[I 2025-01-03 16:00:54,357][0m Trial 6 finished with value: 0.09899329394102097 and parameters: {'observation_period_num': 198, 'train_rates': 0.9463128687405168, 'learning_rate': 0.0001709004679645123, 'batch_size': 217, 'step_size': 10, 'gamma': 0.7699706799661507}. Best is trial 0 with value: 0.054570476285642405.[0m
[32m[I 2025-01-03 16:01:18,416][0m Trial 7 finished with value: 0.044789770485893374 and parameters: {'observation_period_num': 47, 'train_rates': 0.840188858429227, 'learning_rate': 0.0005348054924771879, 'batch_size': 249, 'step_size': 4, 'gamma': 0.9404884471599355}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:02:29,949][0m Trial 8 finished with value: 0.11820873216568649 and parameters: {'observation_period_num': 180, 'train_rates': 0.7779043854163507, 'learning_rate': 4.870820431180621e-05, 'batch_size': 69, 'step_size': 4, 'gamma': 0.9643811019492461}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:03:27,735][0m Trial 9 finished with value: 0.1592877953807487 and parameters: {'observation_period_num': 10, 'train_rates': 0.7394775275933811, 'learning_rate': 0.000996451046794103, 'batch_size': 90, 'step_size': 13, 'gamma': 0.8411880861272547}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:03:55,722][0m Trial 10 finished with value: 1.741436704827064 and parameters: {'observation_period_num': 74, 'train_rates': 0.6260659528978753, 'learning_rate': 1.0262855587400166e-06, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9202666084837265}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:04:35,890][0m Trial 11 finished with value: 0.07901722055326388 and parameters: {'observation_period_num': 64, 'train_rates': 0.8576181175714477, 'learning_rate': 5.7910678632411964e-05, 'batch_size': 142, 'step_size': 7, 'gamma': 0.7515981656568941}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:06:06,233][0m Trial 12 finished with value: 0.08199203678559486 and parameters: {'observation_period_num': 60, 'train_rates': 0.8502466575665149, 'learning_rate': 0.0007387509974973692, 'batch_size': 60, 'step_size': 15, 'gamma': 0.9093351939906271}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:06:28,834][0m Trial 13 finished with value: 0.6649565866159772 and parameters: {'observation_period_num': 41, 'train_rates': 0.7095883266457068, 'learning_rate': 4.380331080090687e-06, 'batch_size': 253, 'step_size': 11, 'gamma': 0.7957833218620904}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:06:56,987][0m Trial 14 finished with value: 0.07192438947419152 and parameters: {'observation_period_num': 104, 'train_rates': 0.8394038148504513, 'learning_rate': 9.11869663310293e-05, 'batch_size': 207, 'step_size': 6, 'gamma': 0.9840187085578841}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:07:40,165][0m Trial 15 finished with value: 0.09049473203191853 and parameters: {'observation_period_num': 159, 'train_rates': 0.8228002658829572, 'learning_rate': 0.0004098103038248205, 'batch_size': 124, 'step_size': 15, 'gamma': 0.9071508779069258}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:08:11,579][0m Trial 16 finished with value: 0.3668242027917671 and parameters: {'observation_period_num': 244, 'train_rates': 0.7160798472193264, 'learning_rate': 2.448180352195194e-05, 'batch_size': 154, 'step_size': 12, 'gamma': 0.8117678796590707}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:09:59,285][0m Trial 17 finished with value: 0.09405969158758092 and parameters: {'observation_period_num': 84, 'train_rates': 0.8898785642679724, 'learning_rate': 9.381206015664086e-06, 'batch_size': 52, 'step_size': 8, 'gamma': 0.8961202602247994}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:10:56,892][0m Trial 18 finished with value: 1.402818126261978 and parameters: {'observation_period_num': 36, 'train_rates': 0.8077710733908467, 'learning_rate': 1.1161399440291126e-06, 'batch_size': 94, 'step_size': 1, 'gamma': 0.9402954609379882}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:11:21,263][0m Trial 19 finished with value: 0.1906901011073247 and parameters: {'observation_period_num': 5, 'train_rates': 0.7402259419367365, 'learning_rate': 0.00010912549414768349, 'batch_size': 220, 'step_size': 5, 'gamma': 0.7946255755709456}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:11:56,787][0m Trial 20 finished with value: 0.0747450739145279 and parameters: {'observation_period_num': 41, 'train_rates': 0.9877694136510988, 'learning_rate': 0.00045127354874996287, 'batch_size': 180, 'step_size': 9, 'gamma': 0.989260342933828}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:12:23,017][0m Trial 21 finished with value: 0.1739164535973814 and parameters: {'observation_period_num': 108, 'train_rates': 0.8520786748193304, 'learning_rate': 7.662810319891209e-05, 'batch_size': 227, 'step_size': 6, 'gamma': 0.9877627669594483}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:12:51,346][0m Trial 22 finished with value: 0.13130063004791737 and parameters: {'observation_period_num': 150, 'train_rates': 0.8260866372323883, 'learning_rate': 3.680365416921692e-05, 'batch_size': 206, 'step_size': 4, 'gamma': 0.9625156219333197}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:13:15,858][0m Trial 23 finished with value: 0.2517198122779278 and parameters: {'observation_period_num': 89, 'train_rates': 0.778818502267414, 'learning_rate': 9.701913551590826e-05, 'batch_size': 236, 'step_size': 7, 'gamma': 0.886349362296586}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:13:45,883][0m Trial 24 finished with value: 0.056465286873392875 and parameters: {'observation_period_num': 94, 'train_rates': 0.8834899187551954, 'learning_rate': 0.0004094335015171221, 'batch_size': 197, 'step_size': 3, 'gamma': 0.939793969374271}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:14:17,763][0m Trial 25 finished with value: 0.055063985990354426 and parameters: {'observation_period_num': 54, 'train_rates': 0.8920490132339814, 'learning_rate': 0.000428791955963979, 'batch_size': 189, 'step_size': 2, 'gamma': 0.9225238715704074}. Best is trial 7 with value: 0.044789770485893374.[0m
[32m[I 2025-01-03 16:14:53,361][0m Trial 26 finished with value: 0.042474989570788484 and parameters: {'observation_period_num': 29, 'train_rates': 0.9312603566564485, 'learning_rate': 0.0002988818563128882, 'batch_size': 183, 'step_size': 2, 'gamma': 0.9284697527457854}. Best is trial 26 with value: 0.042474989570788484.[0m
[32m[I 2025-01-03 16:15:31,042][0m Trial 27 finished with value: 0.043325081505448684 and parameters: {'observation_period_num': 26, 'train_rates': 0.9329011966666192, 'learning_rate': 0.00016956680576065875, 'batch_size': 166, 'step_size': 4, 'gamma': 0.8851042498765179}. Best is trial 26 with value: 0.042474989570788484.[0m
[32m[I 2025-01-03 16:16:08,466][0m Trial 28 finished with value: 0.05529698729515076 and parameters: {'observation_period_num': 23, 'train_rates': 0.9452006366113308, 'learning_rate': 0.00024406040525057154, 'batch_size': 166, 'step_size': 2, 'gamma': 0.8803946835661194}. Best is trial 26 with value: 0.042474989570788484.[0m
[32m[I 2025-01-03 16:16:54,399][0m Trial 29 finished with value: 0.03323466275440584 and parameters: {'observation_period_num': 26, 'train_rates': 0.9312162027586053, 'learning_rate': 0.0006750268018635366, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9253998137743488}. Best is trial 29 with value: 0.03323466275440584.[0m
[32m[I 2025-01-03 16:17:40,442][0m Trial 30 finished with value: 0.0396056128758015 and parameters: {'observation_period_num': 25, 'train_rates': 0.9310155228974696, 'learning_rate': 0.0001357288072940581, 'batch_size': 132, 'step_size': 5, 'gamma': 0.9203241885389537}. Best is trial 29 with value: 0.03323466275440584.[0m
[32m[I 2025-01-03 16:18:26,730][0m Trial 31 finished with value: 0.04200645477186501 and parameters: {'observation_period_num': 26, 'train_rates': 0.9244674217362278, 'learning_rate': 0.0001333774408730699, 'batch_size': 134, 'step_size': 5, 'gamma': 0.9237806319460038}. Best is trial 29 with value: 0.03323466275440584.[0m
[32m[I 2025-01-03 16:19:14,346][0m Trial 32 finished with value: 0.047080568969249725 and parameters: {'observation_period_num': 33, 'train_rates': 0.9656142609565967, 'learning_rate': 0.00028339853920444425, 'batch_size': 131, 'step_size': 5, 'gamma': 0.9237599233854814}. Best is trial 29 with value: 0.03323466275440584.[0m
[32m[I 2025-01-03 16:20:11,036][0m Trial 33 finished with value: 0.03416343915331013 and parameters: {'observation_period_num': 17, 'train_rates': 0.9191667437803196, 'learning_rate': 0.00014961755682444224, 'batch_size': 105, 'step_size': 2, 'gamma': 0.963302326352901}. Best is trial 29 with value: 0.03323466275440584.[0m
[32m[I 2025-01-03 16:21:07,714][0m Trial 34 finished with value: 0.02984394490226241 and parameters: {'observation_period_num': 6, 'train_rates': 0.9179836880706952, 'learning_rate': 0.0001305876489155947, 'batch_size': 108, 'step_size': 5, 'gamma': 0.9640024953031814}. Best is trial 34 with value: 0.02984394490226241.[0m
[32m[I 2025-01-03 16:22:02,861][0m Trial 35 finished with value: 0.048113738918656355 and parameters: {'observation_period_num': 12, 'train_rates': 0.907813405785899, 'learning_rate': 5.7666181321892634e-05, 'batch_size': 108, 'step_size': 7, 'gamma': 0.9674619359576222}. Best is trial 34 with value: 0.02984394490226241.[0m
[32m[I 2025-01-03 16:22:55,111][0m Trial 36 finished with value: 0.0749937682311334 and parameters: {'observation_period_num': 72, 'train_rates': 0.8722318992674336, 'learning_rate': 0.0006958058300019278, 'batch_size': 113, 'step_size': 3, 'gamma': 0.9531273915153682}. Best is trial 34 with value: 0.02984394490226241.[0m
[32m[I 2025-01-03 16:24:07,408][0m Trial 37 finished with value: 0.035963524386142066 and parameters: {'observation_period_num': 8, 'train_rates': 0.9652698159055259, 'learning_rate': 0.00016634149807584795, 'batch_size': 84, 'step_size': 1, 'gamma': 0.9786418040777415}. Best is trial 34 with value: 0.02984394490226241.[0m
[32m[I 2025-01-03 16:25:24,271][0m Trial 38 finished with value: 0.03976840153336525 and parameters: {'observation_period_num': 7, 'train_rates': 0.9879571443931043, 'learning_rate': 0.00020614612379209337, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9747853336667986}. Best is trial 34 with value: 0.02984394490226241.[0m
[32m[I 2025-01-03 16:27:27,843][0m Trial 39 finished with value: 0.07549267416963211 and parameters: {'observation_period_num': 54, 'train_rates': 0.9552607800508013, 'learning_rate': 2.3932343923856104e-05, 'batch_size': 47, 'step_size': 2, 'gamma': 0.9530651161921687}. Best is trial 34 with value: 0.02984394490226241.[0m
[32m[I 2025-01-03 16:30:25,576][0m Trial 40 finished with value: 0.02807282583098712 and parameters: {'observation_period_num': 14, 'train_rates': 0.9138269142879545, 'learning_rate': 0.0009476912444006162, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9761858467843287}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:34:12,340][0m Trial 41 finished with value: 0.037754888062869064 and parameters: {'observation_period_num': 16, 'train_rates': 0.9126433121113624, 'learning_rate': 0.0008771913126916377, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9764059505424133}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:35:14,500][0m Trial 42 finished with value: 0.029386980459094048 and parameters: {'observation_period_num': 16, 'train_rates': 0.973503514226701, 'learning_rate': 0.0005386843388168986, 'batch_size': 100, 'step_size': 1, 'gamma': 0.9566927023626696}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:36:32,106][0m Trial 43 finished with value: 0.057630467849473156 and parameters: {'observation_period_num': 46, 'train_rates': 0.9092272947399064, 'learning_rate': 0.000639178073484257, 'batch_size': 74, 'step_size': 2, 'gamma': 0.9586753435335466}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:37:34,064][0m Trial 44 finished with value: 0.03749825246632099 and parameters: {'observation_period_num': 16, 'train_rates': 0.9510741309062085, 'learning_rate': 0.0009987914709511554, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9444585946775637}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:38:26,219][0m Trial 45 finished with value: 0.10158213973045349 and parameters: {'observation_period_num': 133, 'train_rates': 0.9764535275384582, 'learning_rate': 0.0003280395356270968, 'batch_size': 116, 'step_size': 3, 'gamma': 0.9687921543149461}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:39:23,635][0m Trial 46 finished with value: 0.045389281104093994 and parameters: {'observation_period_num': 38, 'train_rates': 0.880797395391859, 'learning_rate': 0.0005333469438696509, 'batch_size': 101, 'step_size': 1, 'gamma': 0.938395299942531}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:40:02,619][0m Trial 47 finished with value: 0.04622528935206755 and parameters: {'observation_period_num': 68, 'train_rates': 0.8637890830370438, 'learning_rate': 0.0005877657397407647, 'batch_size': 147, 'step_size': 2, 'gamma': 0.9517011545730978}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:40:53,102][0m Trial 48 finished with value: 0.057820718926497 and parameters: {'observation_period_num': 52, 'train_rates': 0.9463964000392974, 'learning_rate': 0.00035626312336928945, 'batch_size': 124, 'step_size': 3, 'gamma': 0.9695357198594534}. Best is trial 40 with value: 0.02807282583098712.[0m
[32m[I 2025-01-03 16:43:11,621][0m Trial 49 finished with value: 0.037348122493769245 and parameters: {'observation_period_num': 20, 'train_rates': 0.8975858671914545, 'learning_rate': 0.00021910918866514237, 'batch_size': 41, 'step_size': 4, 'gamma': 0.9309267805949987}. Best is trial 40 with value: 0.02807282583098712.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-03 16:43:11,631][0m A new study created in memory with name: no-name-73866d0d-21cd-4bbd-a401-472fb67078e3[0m
[32m[I 2025-01-03 16:44:29,911][0m Trial 0 finished with value: 0.06159941990771433 and parameters: {'observation_period_num': 52, 'train_rates': 0.9528416556202164, 'learning_rate': 0.00030037102240722247, 'batch_size': 76, 'step_size': 6, 'gamma': 0.9521100963923131}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:45:31,982][0m Trial 1 finished with value: 0.3726998416634349 and parameters: {'observation_period_num': 204, 'train_rates': 0.9440359683590753, 'learning_rate': 2.0633202800834104e-06, 'batch_size': 90, 'step_size': 5, 'gamma': 0.9875663221260976}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:46:36,475][0m Trial 2 finished with value: 0.6535856529157988 and parameters: {'observation_period_num': 27, 'train_rates': 0.7824110546121545, 'learning_rate': 1.5294566941412889e-06, 'batch_size': 81, 'step_size': 5, 'gamma': 0.8892915982585053}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:47:02,377][0m Trial 3 finished with value: 0.34495988825680074 and parameters: {'observation_period_num': 70, 'train_rates': 0.7011978229973752, 'learning_rate': 2.2109123946502565e-05, 'batch_size': 213, 'step_size': 8, 'gamma': 0.8160645954121697}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:48:47,233][0m Trial 4 finished with value: 0.3555937506752449 and parameters: {'observation_period_num': 165, 'train_rates': 0.7404077975365405, 'learning_rate': 5.890258353308886e-06, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9831613472629145}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:49:14,788][0m Trial 5 finished with value: 0.2916663668763179 and parameters: {'observation_period_num': 185, 'train_rates': 0.7181746504282209, 'learning_rate': 6.78183688935176e-05, 'batch_size': 181, 'step_size': 14, 'gamma': 0.776720978089833}. Best is trial 0 with value: 0.06159941990771433.[0m
Early stopping at epoch 90
[32m[I 2025-01-03 16:49:41,476][0m Trial 6 finished with value: 1.6083283424377441 and parameters: {'observation_period_num': 252, 'train_rates': 0.9441104916572767, 'learning_rate': 2.0103061988538404e-06, 'batch_size': 226, 'step_size': 2, 'gamma': 0.7952344084808478}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:50:09,762][0m Trial 7 finished with value: 0.17924979860317725 and parameters: {'observation_period_num': 27, 'train_rates': 0.7577724842305668, 'learning_rate': 0.00029601886305925844, 'batch_size': 220, 'step_size': 7, 'gamma': 0.8943069484131256}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:54:47,607][0m Trial 8 finished with value: 0.21789849859598318 and parameters: {'observation_period_num': 146, 'train_rates': 0.704755034286733, 'learning_rate': 7.936922640423293e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7876009407972259}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:56:28,687][0m Trial 9 finished with value: 0.18951137186707678 and parameters: {'observation_period_num': 217, 'train_rates': 0.7972367514493293, 'learning_rate': 6.014466929251343e-06, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9748952097324735}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:57:01,924][0m Trial 10 finished with value: 0.26310727999428074 and parameters: {'observation_period_num': 106, 'train_rates': 0.6076918467681899, 'learning_rate': 0.0009308810882733261, 'batch_size': 135, 'step_size': 1, 'gamma': 0.9389915023548596}. Best is trial 0 with value: 0.06159941990771433.[0m
[32m[I 2025-01-03 16:57:27,650][0m Trial 11 finished with value: 0.03659638990248953 and parameters: {'observation_period_num': 6, 'train_rates': 0.8813512128952945, 'learning_rate': 0.0005733457033994963, 'batch_size': 256, 'step_size': 5, 'gamma': 0.897063999346387}. Best is trial 11 with value: 0.03659638990248953.[0m
[32m[I 2025-01-03 16:58:08,485][0m Trial 12 finished with value: 0.028682274040035847 and parameters: {'observation_period_num': 5, 'train_rates': 0.8682057983440555, 'learning_rate': 0.0008762650879126588, 'batch_size': 147, 'step_size': 4, 'gamma': 0.922633925455207}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 16:58:34,096][0m Trial 13 finished with value: 0.038388046315914234 and parameters: {'observation_period_num': 9, 'train_rates': 0.8604298143097195, 'learning_rate': 0.0009066906132331459, 'batch_size': 253, 'step_size': 3, 'gamma': 0.8505911502928702}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 16:59:14,771][0m Trial 14 finished with value: 0.04845553531249364 and parameters: {'observation_period_num': 89, 'train_rates': 0.8691273439332953, 'learning_rate': 0.0003188700902406358, 'batch_size': 146, 'step_size': 4, 'gamma': 0.9229647887247525}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 16:59:53,463][0m Trial 15 finished with value: 0.03888937380785743 and parameters: {'observation_period_num': 6, 'train_rates': 0.8698349471473729, 'learning_rate': 0.00012009684075742734, 'batch_size': 152, 'step_size': 10, 'gamma': 0.858643064621896}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:00:25,871][0m Trial 16 finished with value: 0.06674192018486971 and parameters: {'observation_period_num': 52, 'train_rates': 0.8314904246513184, 'learning_rate': 0.0005313234413376217, 'batch_size': 182, 'step_size': 1, 'gamma': 0.9064634714965983}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:01:12,262][0m Trial 17 finished with value: 0.20237690283285154 and parameters: {'observation_period_num': 120, 'train_rates': 0.8990399006578522, 'learning_rate': 2.9046035057336675e-05, 'batch_size': 122, 'step_size': 3, 'gamma': 0.8391797368406477}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:01:48,045][0m Trial 18 finished with value: 0.08936363458633423 and parameters: {'observation_period_num': 75, 'train_rates': 0.9894996471506807, 'learning_rate': 0.00017226254999780247, 'batch_size': 184, 'step_size': 9, 'gamma': 0.8785763405821052}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:02:14,187][0m Trial 19 finished with value: 0.18157914324880087 and parameters: {'observation_period_num': 42, 'train_rates': 0.9069403560816476, 'learning_rate': 1.3804280162576186e-05, 'batch_size': 253, 'step_size': 4, 'gamma': 0.9299837843520321}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:03:05,872][0m Trial 20 finished with value: 0.10891845663195675 and parameters: {'observation_period_num': 101, 'train_rates': 0.8196121590168567, 'learning_rate': 0.0005305658697377612, 'batch_size': 106, 'step_size': 15, 'gamma': 0.9555557744657085}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:03:32,049][0m Trial 21 finished with value: 0.037977199469293864 and parameters: {'observation_period_num': 12, 'train_rates': 0.8572740601342184, 'learning_rate': 0.0009554823661204862, 'batch_size': 246, 'step_size': 3, 'gamma': 0.8505813595363465}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:03:59,426][0m Trial 22 finished with value: 0.0529488192789683 and parameters: {'observation_period_num': 26, 'train_rates': 0.8985459784171379, 'learning_rate': 0.0009819012876071539, 'batch_size': 232, 'step_size': 3, 'gamma': 0.7508873347861323}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:04:29,852][0m Trial 23 finished with value: 0.031435042758823924 and parameters: {'observation_period_num': 5, 'train_rates': 0.8364993719334425, 'learning_rate': 0.00048749118723326936, 'batch_size': 198, 'step_size': 5, 'gamma': 0.90991991271631}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:04:58,093][0m Trial 24 finished with value: 0.05227950956889914 and parameters: {'observation_period_num': 40, 'train_rates': 0.8177517787991655, 'learning_rate': 0.00016606300817068327, 'batch_size': 199, 'step_size': 6, 'gamma': 0.9111209263797023}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:05:33,815][0m Trial 25 finished with value: 0.04830668550729752 and parameters: {'observation_period_num': 67, 'train_rates': 0.9134511798831186, 'learning_rate': 0.0004896858558843746, 'batch_size': 166, 'step_size': 5, 'gamma': 0.8753632391368871}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:06:04,211][0m Trial 26 finished with value: 0.04639033643373599 and parameters: {'observation_period_num': 6, 'train_rates': 0.8371942966196897, 'learning_rate': 6.33468689326256e-05, 'batch_size': 199, 'step_size': 7, 'gamma': 0.9048128355321229}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:06:34,915][0m Trial 27 finished with value: 0.14790240414404002 and parameters: {'observation_period_num': 33, 'train_rates': 0.6517232274025685, 'learning_rate': 0.00021123329970399647, 'batch_size': 164, 'step_size': 6, 'gamma': 0.9358942311885615}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:07:01,565][0m Trial 28 finished with value: 0.2106098737305319 and parameters: {'observation_period_num': 54, 'train_rates': 0.7796969785295148, 'learning_rate': 0.0005538099102157608, 'batch_size': 207, 'step_size': 4, 'gamma': 0.9599796757582352}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:07:29,371][0m Trial 29 finished with value: 0.07129516452550888 and parameters: {'observation_period_num': 51, 'train_rates': 0.9827984077167964, 'learning_rate': 0.000315691523139881, 'batch_size': 238, 'step_size': 2, 'gamma': 0.920190386171583}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:08:19,570][0m Trial 30 finished with value: 0.04290355917989114 and parameters: {'observation_period_num': 22, 'train_rates': 0.9258714346676916, 'learning_rate': 0.00011339792499325185, 'batch_size': 123, 'step_size': 8, 'gamma': 0.8679746209841116}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:08:46,873][0m Trial 31 finished with value: 0.05379332159381875 and parameters: {'observation_period_num': 16, 'train_rates': 0.8500610393786922, 'learning_rate': 0.0006462832425517478, 'batch_size': 239, 'step_size': 2, 'gamma': 0.8389189148006734}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:09:14,446][0m Trial 32 finished with value: 0.03879635244866327 and parameters: {'observation_period_num': 5, 'train_rates': 0.8826906455816796, 'learning_rate': 0.0003384195935012303, 'batch_size': 252, 'step_size': 4, 'gamma': 0.8922105055338005}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:09:41,734][0m Trial 33 finished with value: 0.046866170058287485 and parameters: {'observation_period_num': 37, 'train_rates': 0.8013862871826508, 'learning_rate': 0.0007095869926171182, 'batch_size': 240, 'step_size': 5, 'gamma': 0.825738001993393}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:10:13,091][0m Trial 34 finished with value: 0.03221848386552836 and parameters: {'observation_period_num': 22, 'train_rates': 0.8480148115809746, 'learning_rate': 0.00041837474940374176, 'batch_size': 214, 'step_size': 5, 'gamma': 0.9469748045007369}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:10:43,658][0m Trial 35 finished with value: 0.055740999293524815 and parameters: {'observation_period_num': 64, 'train_rates': 0.8851764127960274, 'learning_rate': 0.00024213193937803652, 'batch_size': 213, 'step_size': 6, 'gamma': 0.9452905761757907}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:11:16,548][0m Trial 36 finished with value: 0.1788251184052375 and parameters: {'observation_period_num': 23, 'train_rates': 0.7766707315263726, 'learning_rate': 0.00040825422487545646, 'batch_size': 187, 'step_size': 5, 'gamma': 0.9672672328226001}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:11:52,802][0m Trial 37 finished with value: 0.0630389774361482 and parameters: {'observation_period_num': 88, 'train_rates': 0.9275316924026187, 'learning_rate': 0.00013440947544341423, 'batch_size': 168, 'step_size': 8, 'gamma': 0.914043028602818}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:12:55,973][0m Trial 38 finished with value: 0.05664930244286855 and parameters: {'observation_period_num': 43, 'train_rates': 0.9557900309028373, 'learning_rate': 0.00022120163723673546, 'batch_size': 98, 'step_size': 5, 'gamma': 0.949946491255198}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:13:23,060][0m Trial 39 finished with value: 1.1513879692300837 and parameters: {'observation_period_num': 147, 'train_rates': 0.7486551713531981, 'learning_rate': 1.00618483997827e-06, 'batch_size': 225, 'step_size': 6, 'gamma': 0.9887953741081806}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:13:54,545][0m Trial 40 finished with value: 0.05001931916519161 and parameters: {'observation_period_num': 29, 'train_rates': 0.8418920825738273, 'learning_rate': 8.294698364067952e-05, 'batch_size': 205, 'step_size': 7, 'gamma': 0.8966726658638678}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:14:24,402][0m Trial 41 finished with value: 0.03503777449711775 and parameters: {'observation_period_num': 16, 'train_rates': 0.814149866079579, 'learning_rate': 0.0007180491024571302, 'batch_size': 224, 'step_size': 3, 'gamma': 0.9284528454597218}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:14:52,770][0m Trial 42 finished with value: 0.048812699998436425 and parameters: {'observation_period_num': 19, 'train_rates': 0.8113577235201194, 'learning_rate': 0.0004171736198961878, 'batch_size': 221, 'step_size': 2, 'gamma': 0.9318847410453059}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:16:21,093][0m Trial 43 finished with value: 0.035200192279018654 and parameters: {'observation_period_num': 20, 'train_rates': 0.8817504124406462, 'learning_rate': 0.0006113157250385423, 'batch_size': 64, 'step_size': 4, 'gamma': 0.8875574582425138}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:17:32,202][0m Trial 44 finished with value: 0.3163270302460744 and parameters: {'observation_period_num': 210, 'train_rates': 0.7628624666832536, 'learning_rate': 0.0007310206435073226, 'batch_size': 67, 'step_size': 4, 'gamma': 0.9661343128103346}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:20:16,556][0m Trial 45 finished with value: 0.10657053703682558 and parameters: {'observation_period_num': 185, 'train_rates': 0.8208474084563505, 'learning_rate': 4.781980663586677e-05, 'batch_size': 30, 'step_size': 3, 'gamma': 0.886358970864499}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:21:31,187][0m Trial 46 finished with value: 0.23553259361099887 and parameters: {'observation_period_num': 19, 'train_rates': 0.842719829520333, 'learning_rate': 5.299467441473191e-06, 'batch_size': 74, 'step_size': 1, 'gamma': 0.9431415300126119}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:23:13,227][0m Trial 47 finished with value: 0.05872120143545051 and parameters: {'observation_period_num': 56, 'train_rates': 0.792434316103675, 'learning_rate': 0.0003951350904511093, 'batch_size': 50, 'step_size': 2, 'gamma': 0.9236524902439137}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:23:41,236][0m Trial 48 finished with value: 0.1655585771978197 and parameters: {'observation_period_num': 32, 'train_rates': 0.7237211335785398, 'learning_rate': 0.0007631512548847574, 'batch_size': 194, 'step_size': 4, 'gamma': 0.9763521531880609}. Best is trial 12 with value: 0.028682274040035847.[0m
[32m[I 2025-01-03 17:24:18,175][0m Trial 49 finished with value: 0.10692047235175703 and parameters: {'observation_period_num': 248, 'train_rates': 0.8751285139324075, 'learning_rate': 0.0002801303242111799, 'batch_size': 154, 'step_size': 13, 'gamma': 0.9038165033643886}. Best is trial 12 with value: 0.028682274040035847.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-03 17:24:18,185][0m A new study created in memory with name: no-name-362e09e1-c5cb-4463-8743-6b5e3dabf7fe[0m
[32m[I 2025-01-03 17:25:49,415][0m Trial 0 finished with value: 0.2582738050315159 and parameters: {'observation_period_num': 123, 'train_rates': 0.7331004092407587, 'learning_rate': 0.0007398036942856963, 'batch_size': 53, 'step_size': 11, 'gamma': 0.9223556905423002}. Best is trial 0 with value: 0.2582738050315159.[0m
[32m[I 2025-01-03 17:26:16,908][0m Trial 1 finished with value: 0.6006097744705418 and parameters: {'observation_period_num': 129, 'train_rates': 0.8015096410615766, 'learning_rate': 2.47681351265897e-06, 'batch_size': 206, 'step_size': 6, 'gamma': 0.7807204299461334}. Best is trial 0 with value: 0.2582738050315159.[0m
[32m[I 2025-01-03 17:27:03,344][0m Trial 2 finished with value: 0.04673277959227562 and parameters: {'observation_period_num': 28, 'train_rates': 0.9685598381339151, 'learning_rate': 0.00034643587005810583, 'batch_size': 137, 'step_size': 14, 'gamma': 0.8138863140664367}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:27:37,681][0m Trial 3 finished with value: 0.2835826212187113 and parameters: {'observation_period_num': 133, 'train_rates': 0.6726893466185061, 'learning_rate': 0.00030302894199767024, 'batch_size': 140, 'step_size': 12, 'gamma': 0.9561124732612138}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:28:12,885][0m Trial 4 finished with value: 0.18352142285398196 and parameters: {'observation_period_num': 53, 'train_rates': 0.7551919737019611, 'learning_rate': 0.0003778705031326836, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8511999790127409}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:28:41,976][0m Trial 5 finished with value: 0.6977986055030871 and parameters: {'observation_period_num': 213, 'train_rates': 0.7119741698821652, 'learning_rate': 1.3138375335291516e-06, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9754882619770499}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:29:31,139][0m Trial 6 finished with value: 0.12085355296552325 and parameters: {'observation_period_num': 92, 'train_rates': 0.8543874712648578, 'learning_rate': 1.553446665134848e-05, 'batch_size': 113, 'step_size': 12, 'gamma': 0.8695606019705427}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:30:09,472][0m Trial 7 finished with value: 0.3904471398616324 and parameters: {'observation_period_num': 106, 'train_rates': 0.8279506054135447, 'learning_rate': 3.1402095947522e-06, 'batch_size': 142, 'step_size': 2, 'gamma': 0.9714084546264592}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:30:48,661][0m Trial 8 finished with value: 0.6278567910194397 and parameters: {'observation_period_num': 240, 'train_rates': 0.967495054912719, 'learning_rate': 1.2083010691688633e-05, 'batch_size': 151, 'step_size': 3, 'gamma': 0.769906350591142}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:31:10,962][0m Trial 9 finished with value: 2.2410376503549774 and parameters: {'observation_period_num': 46, 'train_rates': 0.6010313329193009, 'learning_rate': 1.678446712897076e-06, 'batch_size': 228, 'step_size': 9, 'gamma': 0.8652287254715272}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:32:40,717][0m Trial 10 finished with value: 0.0767182831534709 and parameters: {'observation_period_num': 27, 'train_rates': 0.9703318965415861, 'learning_rate': 9.807356687163735e-05, 'batch_size': 67, 'step_size': 15, 'gamma': 0.8139037858200189}. Best is trial 2 with value: 0.04673277959227562.[0m
[32m[I 2025-01-03 17:34:02,015][0m Trial 11 finished with value: 0.040834784507751465 and parameters: {'observation_period_num': 7, 'train_rates': 0.9816345925968414, 'learning_rate': 0.00010189823982992868, 'batch_size': 75, 'step_size': 14, 'gamma': 0.8142126809901725}. Best is trial 11 with value: 0.040834784507751465.[0m
[32m[I 2025-01-03 17:38:39,890][0m Trial 12 finished with value: 0.026569809955495405 and parameters: {'observation_period_num': 8, 'train_rates': 0.9038443166091631, 'learning_rate': 8.59652971340502e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.8166806044090198}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 17:44:24,838][0m Trial 13 finished with value: 0.030241357584794362 and parameters: {'observation_period_num': 7, 'train_rates': 0.8982243509772287, 'learning_rate': 6.576549787879022e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8193531256565072}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 17:48:21,980][0m Trial 14 finished with value: 0.04598503164947033 and parameters: {'observation_period_num': 70, 'train_rates': 0.8961617438262062, 'learning_rate': 5.6514439214073566e-05, 'batch_size': 23, 'step_size': 8, 'gamma': 0.9077510057252637}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 17:51:32,825][0m Trial 15 finished with value: 0.08350810034646734 and parameters: {'observation_period_num': 189, 'train_rates': 0.9048557133520904, 'learning_rate': 2.564537718829255e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8311619572365563}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 17:52:33,516][0m Trial 16 finished with value: 0.324663662492997 and parameters: {'observation_period_num': 168, 'train_rates': 0.9079386434707742, 'learning_rate': 6.1867059549873175e-06, 'batch_size': 93, 'step_size': 6, 'gamma': 0.7782087909617347}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 17:58:04,087][0m Trial 17 finished with value: 0.0356191560827397 and parameters: {'observation_period_num': 15, 'train_rates': 0.8488372219301967, 'learning_rate': 0.0001268980884563937, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7550855047930174}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 18:00:10,327][0m Trial 18 finished with value: 0.11211209866819502 and parameters: {'observation_period_num': 74, 'train_rates': 0.9176873874798487, 'learning_rate': 4.428060444747834e-05, 'batch_size': 44, 'step_size': 1, 'gamma': 0.9038886928160168}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 18:01:05,319][0m Trial 19 finished with value: 0.046663701596881164 and parameters: {'observation_period_num': 47, 'train_rates': 0.8773364305559755, 'learning_rate': 0.000195788303812413, 'batch_size': 104, 'step_size': 7, 'gamma': 0.8465786070265511}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 18:02:46,842][0m Trial 20 finished with value: 0.2103076114061293 and parameters: {'observation_period_num': 5, 'train_rates': 0.7832328786438946, 'learning_rate': 2.6736235826816137e-05, 'batch_size': 51, 'step_size': 4, 'gamma': 0.7930764637115938}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 18:07:24,181][0m Trial 21 finished with value: 0.04199800024861875 and parameters: {'observation_period_num': 25, 'train_rates': 0.8509789001131725, 'learning_rate': 0.00011530310158613721, 'batch_size': 19, 'step_size': 10, 'gamma': 0.7529945703886154}. Best is trial 12 with value: 0.026569809955495405.[0m
[32m[I 2025-01-03 18:12:59,725][0m Trial 22 finished with value: 0.02551558617483917 and parameters: {'observation_period_num': 7, 'train_rates': 0.9317924053095814, 'learning_rate': 6.298769229180126e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.7571794478614324}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:14:16,461][0m Trial 23 finished with value: 0.06085806050047076 and parameters: {'observation_period_num': 66, 'train_rates': 0.9235211111734579, 'learning_rate': 5.012499901199688e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.7973724079699717}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:17:11,379][0m Trial 24 finished with value: 0.04737984743750024 and parameters: {'observation_period_num': 40, 'train_rates': 0.9427736247880553, 'learning_rate': 6.708396613987854e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8374041393623604}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:19:09,203][0m Trial 25 finished with value: 0.0725622306044182 and parameters: {'observation_period_num': 87, 'train_rates': 0.9380654062580616, 'learning_rate': 1.5340945509929044e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.8872546623266057}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:20:33,384][0m Trial 26 finished with value: 0.028645730115302647 and parameters: {'observation_period_num': 6, 'train_rates': 0.8666521441393455, 'learning_rate': 0.0001947792149012157, 'batch_size': 66, 'step_size': 11, 'gamma': 0.7989432297805693}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:21:56,868][0m Trial 27 finished with value: 0.04829253792646794 and parameters: {'observation_period_num': 33, 'train_rates': 0.8238762645702956, 'learning_rate': 0.0007312464725017502, 'batch_size': 64, 'step_size': 13, 'gamma': 0.7957888007204956}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:24:15,771][0m Trial 28 finished with value: 0.09980692726904968 and parameters: {'observation_period_num': 154, 'train_rates': 0.8716509358697533, 'learning_rate': 0.00020684663167378717, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7623385397150061}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:25:17,981][0m Trial 29 finished with value: 0.06175652428007707 and parameters: {'observation_period_num': 57, 'train_rates': 0.9435537587677733, 'learning_rate': 0.000989237521279274, 'batch_size': 96, 'step_size': 11, 'gamma': 0.7834316226491712}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:26:06,029][0m Trial 30 finished with value: 0.07597796363238046 and parameters: {'observation_period_num': 99, 'train_rates': 0.8816311848936643, 'learning_rate': 0.0005963647992570157, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8037802068384238}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:27:43,040][0m Trial 31 finished with value: 0.027995556537005993 and parameters: {'observation_period_num': 14, 'train_rates': 0.8867061770903062, 'learning_rate': 0.00019346312290237038, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8358289789441692}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:28:47,786][0m Trial 32 finished with value: 0.03895810210581766 and parameters: {'observation_period_num': 23, 'train_rates': 0.8140933737091017, 'learning_rate': 0.00018674593707527462, 'batch_size': 83, 'step_size': 10, 'gamma': 0.8363887344464483}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:30:16,425][0m Trial 33 finished with value: 0.03423692882302907 and parameters: {'observation_period_num': 18, 'train_rates': 0.7902345075028921, 'learning_rate': 0.00017233619530531466, 'batch_size': 58, 'step_size': 11, 'gamma': 0.7798685476284102}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:32:36,224][0m Trial 34 finished with value: 0.0576548811327666 and parameters: {'observation_period_num': 39, 'train_rates': 0.8627043442036859, 'learning_rate': 0.0004117968547947081, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8258306009417051}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:34:18,496][0m Trial 35 finished with value: 0.0424256426117876 and parameters: {'observation_period_num': 18, 'train_rates': 0.9422376146675008, 'learning_rate': 0.00027303297540753543, 'batch_size': 57, 'step_size': 12, 'gamma': 0.8558868653150249}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:36:56,904][0m Trial 36 finished with value: 0.07951137019470819 and parameters: {'observation_period_num': 58, 'train_rates': 0.8409827676903082, 'learning_rate': 0.0004880122562656412, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8055066246056308}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:37:19,564][0m Trial 37 finished with value: 0.29788928105886514 and parameters: {'observation_period_num': 123, 'train_rates': 0.7564154024631691, 'learning_rate': 3.5381189947017024e-05, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8852240619189231}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:38:08,352][0m Trial 38 finished with value: 0.04828059909657049 and parameters: {'observation_period_num': 37, 'train_rates': 0.8864172086454833, 'learning_rate': 8.838497318649313e-05, 'batch_size': 122, 'step_size': 13, 'gamma': 0.7724130172904218}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:38:43,968][0m Trial 39 finished with value: 0.03519310280680656 and parameters: {'observation_period_num': 5, 'train_rates': 0.9287354646843823, 'learning_rate': 0.0002800612619654534, 'batch_size': 182, 'step_size': 12, 'gamma': 0.7890581041350039}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:39:54,006][0m Trial 40 finished with value: 0.08701897843887932 and parameters: {'observation_period_num': 78, 'train_rates': 0.9603608977846085, 'learning_rate': 0.00013264443527369286, 'batch_size': 84, 'step_size': 10, 'gamma': 0.9544649357886524}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:45:40,521][0m Trial 41 finished with value: 0.02711046748870128 and parameters: {'observation_period_num': 5, 'train_rates': 0.8996832534985737, 'learning_rate': 6.378878179562919e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8474512555612913}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:48:36,274][0m Trial 42 finished with value: 0.04128490782473698 and parameters: {'observation_period_num': 31, 'train_rates': 0.9172682919138231, 'learning_rate': 7.290403479556487e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8478229845672263}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:50:26,648][0m Trial 43 finished with value: 0.15036232009726136 and parameters: {'observation_period_num': 17, 'train_rates': 0.6893574206634, 'learning_rate': 3.901252124460701e-05, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8597355166105685}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:51:56,529][0m Trial 44 finished with value: 0.05345941707491875 and parameters: {'observation_period_num': 51, 'train_rates': 0.985342468296148, 'learning_rate': 8.464042386808941e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.8767905145724191}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:55:06,724][0m Trial 45 finished with value: 0.12517336783062333 and parameters: {'observation_period_num': 14, 'train_rates': 0.6275305577885908, 'learning_rate': 0.0001504750717999872, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8236138173542334}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 18:56:53,534][0m Trial 46 finished with value: 0.11764756057943616 and parameters: {'observation_period_num': 238, 'train_rates': 0.9587968449590083, 'learning_rate': 2.4931100346501777e-05, 'batch_size': 52, 'step_size': 14, 'gamma': 0.7661967892425755}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 19:00:25,648][0m Trial 47 finished with value: 0.04550128242043408 and parameters: {'observation_period_num': 28, 'train_rates': 0.8951737036314977, 'learning_rate': 0.00025400343474843564, 'batch_size': 26, 'step_size': 9, 'gamma': 0.8093344730106411}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 19:00:58,379][0m Trial 48 finished with value: 0.25997585705236387 and parameters: {'observation_period_num': 111, 'train_rates': 0.8395816540111434, 'learning_rate': 9.325312130263852e-06, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8399569101279041}. Best is trial 22 with value: 0.02551558617483917.[0m
[32m[I 2025-01-03 19:02:16,676][0m Trial 49 finished with value: 0.059111659192073514 and parameters: {'observation_period_num': 5, 'train_rates': 0.9039067737221725, 'learning_rate': 2.0714586846307682e-05, 'batch_size': 74, 'step_size': 7, 'gamma': 0.8193601254162675}. Best is trial 22 with value: 0.02551558617483917.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 19, 'train_rates': 0.9047440379192491, 'learning_rate': 0.0006910777661350586, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8611674426915483}
Epoch 1/300, trend Loss: 0.4999 | 0.2626
Epoch 2/300, trend Loss: 0.1940 | 0.1434
Epoch 3/300, trend Loss: 0.1672 | 0.1089
Epoch 4/300, trend Loss: 0.1470 | 0.0850
Epoch 5/300, trend Loss: 0.1557 | 0.0760
Epoch 6/300, trend Loss: 0.1467 | 0.0852
Epoch 7/300, trend Loss: 0.1353 | 0.0747
Epoch 8/300, trend Loss: 0.1470 | 0.1045
Epoch 9/300, trend Loss: 0.1507 | 0.0764
Epoch 10/300, trend Loss: 0.2005 | 0.1411
Epoch 11/300, trend Loss: 0.2570 | 0.4285
Epoch 12/300, trend Loss: 0.2121 | 0.0967
Epoch 13/300, trend Loss: 0.1527 | 0.0819
Epoch 14/300, trend Loss: 0.1419 | 0.0736
Epoch 15/300, trend Loss: 0.1109 | 0.0755
Epoch 16/300, trend Loss: 0.1392 | 0.0735
Epoch 17/300, trend Loss: 0.1325 | 0.1001
Epoch 18/300, trend Loss: 0.1228 | 0.0663
Epoch 19/300, trend Loss: 0.1100 | 0.0704
Epoch 20/300, trend Loss: 0.1074 | 0.0612
Epoch 21/300, trend Loss: 0.1026 | 0.0572
Epoch 22/300, trend Loss: 0.1015 | 0.0634
Epoch 23/300, trend Loss: 0.0980 | 0.0645
Epoch 24/300, trend Loss: 0.0970 | 0.0542
Epoch 25/300, trend Loss: 0.0959 | 0.0748
Epoch 26/300, trend Loss: 0.0955 | 0.0910
Epoch 27/300, trend Loss: 0.0957 | 0.0546
Epoch 28/300, trend Loss: 0.0892 | 0.0522
Epoch 29/300, trend Loss: 0.0880 | 0.0525
Epoch 30/300, trend Loss: 0.0867 | 0.0526
Epoch 31/300, trend Loss: 0.0858 | 0.0525
Epoch 32/300, trend Loss: 0.0845 | 0.0490
Epoch 33/300, trend Loss: 0.0832 | 0.0471
Epoch 34/300, trend Loss: 0.0823 | 0.0467
Epoch 35/300, trend Loss: 0.0814 | 0.0475
Epoch 36/300, trend Loss: 0.0804 | 0.0459
Epoch 37/300, trend Loss: 0.0795 | 0.0433
Epoch 38/300, trend Loss: 0.0786 | 0.0423
Epoch 39/300, trend Loss: 0.0780 | 0.0424
Epoch 40/300, trend Loss: 0.0773 | 0.0431
Epoch 41/300, trend Loss: 0.0768 | 0.0409
Epoch 42/300, trend Loss: 0.0764 | 0.0399
Epoch 43/300, trend Loss: 0.0760 | 0.0406
Epoch 44/300, trend Loss: 0.0757 | 0.0406
Epoch 45/300, trend Loss: 0.0751 | 0.0395
Epoch 46/300, trend Loss: 0.0746 | 0.0389
Epoch 47/300, trend Loss: 0.0741 | 0.0385
Epoch 48/300, trend Loss: 0.0737 | 0.0379
Epoch 49/300, trend Loss: 0.0733 | 0.0374
Epoch 50/300, trend Loss: 0.0731 | 0.0374
Epoch 51/300, trend Loss: 0.0731 | 0.0374
Epoch 52/300, trend Loss: 0.0732 | 0.0376
Epoch 53/300, trend Loss: 0.0733 | 0.0376
Epoch 54/300, trend Loss: 0.0737 | 0.0382
Epoch 55/300, trend Loss: 0.0745 | 0.0388
Epoch 56/300, trend Loss: 0.0749 | 0.0375
Epoch 57/300, trend Loss: 0.0734 | 0.0377
Epoch 58/300, trend Loss: 0.0726 | 0.0372
Epoch 59/300, trend Loss: 0.0728 | 0.0366
Epoch 60/300, trend Loss: 0.0722 | 0.0363
Epoch 61/300, trend Loss: 0.0712 | 0.0360
Epoch 62/300, trend Loss: 0.0707 | 0.0358
Epoch 63/300, trend Loss: 0.0704 | 0.0358
Epoch 64/300, trend Loss: 0.0703 | 0.0355
Epoch 65/300, trend Loss: 0.0702 | 0.0354
Epoch 66/300, trend Loss: 0.0700 | 0.0353
Epoch 67/300, trend Loss: 0.0698 | 0.0353
Epoch 68/300, trend Loss: 0.0697 | 0.0350
Epoch 69/300, trend Loss: 0.0695 | 0.0350
Epoch 70/300, trend Loss: 0.0694 | 0.0349
Epoch 71/300, trend Loss: 0.0692 | 0.0347
Epoch 72/300, trend Loss: 0.0691 | 0.0346
Epoch 73/300, trend Loss: 0.0690 | 0.0346
Epoch 74/300, trend Loss: 0.0689 | 0.0345
Epoch 75/300, trend Loss: 0.0688 | 0.0344
Epoch 76/300, trend Loss: 0.0687 | 0.0344
Epoch 77/300, trend Loss: 0.0686 | 0.0343
Epoch 78/300, trend Loss: 0.0685 | 0.0342
Epoch 79/300, trend Loss: 0.0684 | 0.0342
Epoch 80/300, trend Loss: 0.0683 | 0.0341
Epoch 81/300, trend Loss: 0.0683 | 0.0341
Epoch 82/300, trend Loss: 0.0682 | 0.0340
Epoch 83/300, trend Loss: 0.0681 | 0.0340
Epoch 84/300, trend Loss: 0.0681 | 0.0339
Epoch 85/300, trend Loss: 0.0680 | 0.0339
Epoch 86/300, trend Loss: 0.0679 | 0.0339
Epoch 87/300, trend Loss: 0.0679 | 0.0338
Epoch 88/300, trend Loss: 0.0678 | 0.0338
Epoch 89/300, trend Loss: 0.0678 | 0.0337
Epoch 90/300, trend Loss: 0.0677 | 0.0337
Epoch 91/300, trend Loss: 0.0677 | 0.0336
Epoch 92/300, trend Loss: 0.0676 | 0.0337
Epoch 93/300, trend Loss: 0.0676 | 0.0336
Epoch 94/300, trend Loss: 0.0675 | 0.0336
Epoch 95/300, trend Loss: 0.0675 | 0.0335
Epoch 96/300, trend Loss: 0.0674 | 0.0335
Epoch 97/300, trend Loss: 0.0674 | 0.0335
Epoch 98/300, trend Loss: 0.0674 | 0.0335
Epoch 99/300, trend Loss: 0.0673 | 0.0334
Epoch 100/300, trend Loss: 0.0673 | 0.0334
Epoch 101/300, trend Loss: 0.0673 | 0.0334
Epoch 102/300, trend Loss: 0.0672 | 0.0334
Epoch 103/300, trend Loss: 0.0672 | 0.0334
Epoch 104/300, trend Loss: 0.0672 | 0.0334
Epoch 105/300, trend Loss: 0.0671 | 0.0333
Epoch 106/300, trend Loss: 0.0671 | 0.0333
Epoch 107/300, trend Loss: 0.0671 | 0.0333
Epoch 108/300, trend Loss: 0.0671 | 0.0333
Epoch 109/300, trend Loss: 0.0670 | 0.0333
Epoch 110/300, trend Loss: 0.0670 | 0.0333
Epoch 111/300, trend Loss: 0.0670 | 0.0332
Epoch 112/300, trend Loss: 0.0670 | 0.0332
Epoch 113/300, trend Loss: 0.0669 | 0.0332
Epoch 114/300, trend Loss: 0.0669 | 0.0332
Epoch 115/300, trend Loss: 0.0669 | 0.0332
Epoch 116/300, trend Loss: 0.0669 | 0.0332
Epoch 117/300, trend Loss: 0.0669 | 0.0332
Epoch 118/300, trend Loss: 0.0669 | 0.0332
Epoch 119/300, trend Loss: 0.0668 | 0.0331
Epoch 120/300, trend Loss: 0.0668 | 0.0331
Epoch 121/300, trend Loss: 0.0668 | 0.0331
Epoch 122/300, trend Loss: 0.0668 | 0.0331
Epoch 123/300, trend Loss: 0.0668 | 0.0331
Epoch 124/300, trend Loss: 0.0668 | 0.0331
Epoch 125/300, trend Loss: 0.0668 | 0.0331
Epoch 126/300, trend Loss: 0.0667 | 0.0331
Epoch 127/300, trend Loss: 0.0667 | 0.0331
Epoch 128/300, trend Loss: 0.0667 | 0.0331
Epoch 129/300, trend Loss: 0.0667 | 0.0331
Epoch 130/300, trend Loss: 0.0667 | 0.0331
Epoch 131/300, trend Loss: 0.0667 | 0.0330
Epoch 132/300, trend Loss: 0.0667 | 0.0330
Epoch 133/300, trend Loss: 0.0667 | 0.0330
Epoch 134/300, trend Loss: 0.0667 | 0.0330
Epoch 135/300, trend Loss: 0.0667 | 0.0330
Epoch 136/300, trend Loss: 0.0666 | 0.0330
Epoch 137/300, trend Loss: 0.0666 | 0.0330
Epoch 138/300, trend Loss: 0.0666 | 0.0330
Epoch 139/300, trend Loss: 0.0666 | 0.0330
Epoch 140/300, trend Loss: 0.0666 | 0.0330
Epoch 141/300, trend Loss: 0.0666 | 0.0330
Epoch 142/300, trend Loss: 0.0666 | 0.0330
Epoch 143/300, trend Loss: 0.0666 | 0.0330
Epoch 144/300, trend Loss: 0.0666 | 0.0330
Epoch 145/300, trend Loss: 0.0666 | 0.0330
Epoch 146/300, trend Loss: 0.0666 | 0.0330
Epoch 147/300, trend Loss: 0.0666 | 0.0330
Epoch 148/300, trend Loss: 0.0666 | 0.0330
Epoch 149/300, trend Loss: 0.0666 | 0.0330
Epoch 150/300, trend Loss: 0.0666 | 0.0330
Epoch 151/300, trend Loss: 0.0665 | 0.0330
Epoch 152/300, trend Loss: 0.0665 | 0.0330
Epoch 153/300, trend Loss: 0.0665 | 0.0329
Epoch 154/300, trend Loss: 0.0665 | 0.0329
Epoch 155/300, trend Loss: 0.0665 | 0.0329
Epoch 156/300, trend Loss: 0.0665 | 0.0329
Epoch 157/300, trend Loss: 0.0665 | 0.0329
Epoch 158/300, trend Loss: 0.0665 | 0.0329
Epoch 159/300, trend Loss: 0.0665 | 0.0329
Epoch 160/300, trend Loss: 0.0665 | 0.0329
Epoch 161/300, trend Loss: 0.0665 | 0.0329
Epoch 162/300, trend Loss: 0.0665 | 0.0329
Epoch 163/300, trend Loss: 0.0665 | 0.0329
Epoch 164/300, trend Loss: 0.0665 | 0.0329
Epoch 165/300, trend Loss: 0.0665 | 0.0329
Epoch 166/300, trend Loss: 0.0665 | 0.0329
Epoch 167/300, trend Loss: 0.0665 | 0.0329
Epoch 168/300, trend Loss: 0.0665 | 0.0329
Epoch 169/300, trend Loss: 0.0665 | 0.0329
Epoch 170/300, trend Loss: 0.0665 | 0.0329
Epoch 171/300, trend Loss: 0.0665 | 0.0329
Epoch 172/300, trend Loss: 0.0665 | 0.0329
Epoch 173/300, trend Loss: 0.0665 | 0.0329
Epoch 174/300, trend Loss: 0.0665 | 0.0329
Epoch 175/300, trend Loss: 0.0665 | 0.0329
Epoch 176/300, trend Loss: 0.0665 | 0.0329
Epoch 177/300, trend Loss: 0.0665 | 0.0329
Epoch 178/300, trend Loss: 0.0665 | 0.0329
Epoch 179/300, trend Loss: 0.0665 | 0.0329
Epoch 180/300, trend Loss: 0.0665 | 0.0329
Epoch 181/300, trend Loss: 0.0665 | 0.0329
Epoch 182/300, trend Loss: 0.0665 | 0.0329
Epoch 183/300, trend Loss: 0.0665 | 0.0329
Epoch 184/300, trend Loss: 0.0665 | 0.0329
Epoch 185/300, trend Loss: 0.0665 | 0.0329
Epoch 186/300, trend Loss: 0.0665 | 0.0329
Epoch 187/300, trend Loss: 0.0665 | 0.0329
Epoch 188/300, trend Loss: 0.0665 | 0.0329
Epoch 189/300, trend Loss: 0.0665 | 0.0329
Epoch 190/300, trend Loss: 0.0665 | 0.0329
Epoch 191/300, trend Loss: 0.0665 | 0.0329
Epoch 192/300, trend Loss: 0.0665 | 0.0329
Epoch 193/300, trend Loss: 0.0665 | 0.0329
Epoch 194/300, trend Loss: 0.0665 | 0.0329
Epoch 195/300, trend Loss: 0.0665 | 0.0329
Epoch 196/300, trend Loss: 0.0664 | 0.0329
Epoch 197/300, trend Loss: 0.0664 | 0.0329
Epoch 198/300, trend Loss: 0.0664 | 0.0329
Epoch 199/300, trend Loss: 0.0664 | 0.0329
Epoch 200/300, trend Loss: 0.0664 | 0.0329
Epoch 201/300, trend Loss: 0.0664 | 0.0329
Epoch 202/300, trend Loss: 0.0664 | 0.0329
Epoch 203/300, trend Loss: 0.0664 | 0.0329
Epoch 204/300, trend Loss: 0.0664 | 0.0329
Epoch 205/300, trend Loss: 0.0664 | 0.0329
Epoch 206/300, trend Loss: 0.0664 | 0.0329
Epoch 207/300, trend Loss: 0.0664 | 0.0329
Epoch 208/300, trend Loss: 0.0664 | 0.0329
Epoch 209/300, trend Loss: 0.0664 | 0.0329
Epoch 210/300, trend Loss: 0.0664 | 0.0329
Epoch 211/300, trend Loss: 0.0664 | 0.0329
Epoch 212/300, trend Loss: 0.0664 | 0.0329
Epoch 213/300, trend Loss: 0.0664 | 0.0329
Epoch 214/300, trend Loss: 0.0664 | 0.0329
Epoch 215/300, trend Loss: 0.0664 | 0.0329
Epoch 216/300, trend Loss: 0.0664 | 0.0329
Epoch 217/300, trend Loss: 0.0664 | 0.0329
Epoch 218/300, trend Loss: 0.0664 | 0.0329
Epoch 219/300, trend Loss: 0.0664 | 0.0329
Epoch 220/300, trend Loss: 0.0664 | 0.0329
Epoch 221/300, trend Loss: 0.0664 | 0.0329
Epoch 222/300, trend Loss: 0.0664 | 0.0329
Epoch 223/300, trend Loss: 0.0664 | 0.0329
Epoch 224/300, trend Loss: 0.0664 | 0.0329
Epoch 225/300, trend Loss: 0.0664 | 0.0329
Epoch 226/300, trend Loss: 0.0664 | 0.0329
Epoch 227/300, trend Loss: 0.0664 | 0.0329
Epoch 228/300, trend Loss: 0.0664 | 0.0329
Epoch 229/300, trend Loss: 0.0664 | 0.0329
Epoch 230/300, trend Loss: 0.0664 | 0.0329
Epoch 231/300, trend Loss: 0.0664 | 0.0329
Epoch 232/300, trend Loss: 0.0664 | 0.0329
Epoch 233/300, trend Loss: 0.0664 | 0.0329
Epoch 234/300, trend Loss: 0.0664 | 0.0329
Epoch 235/300, trend Loss: 0.0664 | 0.0329
Epoch 236/300, trend Loss: 0.0664 | 0.0329
Epoch 237/300, trend Loss: 0.0664 | 0.0329
Epoch 238/300, trend Loss: 0.0664 | 0.0329
Epoch 239/300, trend Loss: 0.0664 | 0.0329
Epoch 240/300, trend Loss: 0.0664 | 0.0329
Epoch 241/300, trend Loss: 0.0664 | 0.0329
Epoch 242/300, trend Loss: 0.0664 | 0.0329
Epoch 243/300, trend Loss: 0.0664 | 0.0329
Epoch 244/300, trend Loss: 0.0664 | 0.0329
Epoch 245/300, trend Loss: 0.0664 | 0.0329
Epoch 246/300, trend Loss: 0.0664 | 0.0329
Epoch 247/300, trend Loss: 0.0664 | 0.0329
Epoch 248/300, trend Loss: 0.0664 | 0.0329
Epoch 249/300, trend Loss: 0.0664 | 0.0329
Epoch 250/300, trend Loss: 0.0664 | 0.0329
Epoch 251/300, trend Loss: 0.0664 | 0.0329
Epoch 252/300, trend Loss: 0.0664 | 0.0329
Epoch 253/300, trend Loss: 0.0664 | 0.0329
Epoch 254/300, trend Loss: 0.0664 | 0.0329
Epoch 255/300, trend Loss: 0.0664 | 0.0329
Epoch 256/300, trend Loss: 0.0664 | 0.0329
Epoch 257/300, trend Loss: 0.0664 | 0.0329
Epoch 258/300, trend Loss: 0.0664 | 0.0329
Epoch 259/300, trend Loss: 0.0664 | 0.0329
Epoch 260/300, trend Loss: 0.0664 | 0.0329
Epoch 261/300, trend Loss: 0.0664 | 0.0329
Epoch 262/300, trend Loss: 0.0664 | 0.0329
Epoch 263/300, trend Loss: 0.0664 | 0.0329
Epoch 264/300, trend Loss: 0.0664 | 0.0329
Epoch 265/300, trend Loss: 0.0664 | 0.0329
Epoch 266/300, trend Loss: 0.0664 | 0.0329
Epoch 267/300, trend Loss: 0.0664 | 0.0329
Epoch 268/300, trend Loss: 0.0664 | 0.0329
Epoch 269/300, trend Loss: 0.0664 | 0.0329
Epoch 270/300, trend Loss: 0.0664 | 0.0329
Epoch 271/300, trend Loss: 0.0664 | 0.0329
Epoch 272/300, trend Loss: 0.0664 | 0.0329
Epoch 273/300, trend Loss: 0.0664 | 0.0329
Epoch 274/300, trend Loss: 0.0664 | 0.0329
Epoch 275/300, trend Loss: 0.0664 | 0.0329
Epoch 276/300, trend Loss: 0.0664 | 0.0329
Epoch 277/300, trend Loss: 0.0664 | 0.0329
Epoch 278/300, trend Loss: 0.0664 | 0.0329
Epoch 279/300, trend Loss: 0.0664 | 0.0329
Epoch 280/300, trend Loss: 0.0664 | 0.0329
Epoch 281/300, trend Loss: 0.0664 | 0.0329
Epoch 282/300, trend Loss: 0.0664 | 0.0329
Epoch 283/300, trend Loss: 0.0664 | 0.0329
Epoch 284/300, trend Loss: 0.0664 | 0.0329
Epoch 285/300, trend Loss: 0.0664 | 0.0329
Epoch 286/300, trend Loss: 0.0664 | 0.0329
Epoch 287/300, trend Loss: 0.0664 | 0.0329
Epoch 288/300, trend Loss: 0.0664 | 0.0329
Epoch 289/300, trend Loss: 0.0664 | 0.0329
Epoch 290/300, trend Loss: 0.0664 | 0.0329
Epoch 291/300, trend Loss: 0.0664 | 0.0329
Epoch 292/300, trend Loss: 0.0664 | 0.0329
Epoch 293/300, trend Loss: 0.0664 | 0.0329
Epoch 294/300, trend Loss: 0.0664 | 0.0329
Epoch 295/300, trend Loss: 0.0664 | 0.0329
Epoch 296/300, trend Loss: 0.0664 | 0.0329
Epoch 297/300, trend Loss: 0.0664 | 0.0329
Epoch 298/300, trend Loss: 0.0664 | 0.0329
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 38, 'train_rates': 0.9865478027156104, 'learning_rate': 0.0006073684298445799, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9673104092197924}
Epoch 1/300, seasonal_0 Loss: 0.1971 | 0.0881
Epoch 2/300, seasonal_0 Loss: 0.1174 | 0.0662
Epoch 3/300, seasonal_0 Loss: 0.1048 | 0.0672
Epoch 4/300, seasonal_0 Loss: 0.0965 | 0.0628
Epoch 5/300, seasonal_0 Loss: 0.0912 | 0.0557
Epoch 6/300, seasonal_0 Loss: 0.0853 | 0.0554
Epoch 7/300, seasonal_0 Loss: 0.0865 | 0.0482
Epoch 8/300, seasonal_0 Loss: 0.0865 | 0.0482
Epoch 9/300, seasonal_0 Loss: 0.0822 | 0.0476
Epoch 10/300, seasonal_0 Loss: 0.0757 | 0.0482
Epoch 11/300, seasonal_0 Loss: 0.0686 | 0.0455
Epoch 12/300, seasonal_0 Loss: 0.0657 | 0.0469
Epoch 13/300, seasonal_0 Loss: 0.0631 | 0.0444
Epoch 14/300, seasonal_0 Loss: 0.0619 | 0.0451
Epoch 15/300, seasonal_0 Loss: 0.0612 | 0.0427
Epoch 16/300, seasonal_0 Loss: 0.0590 | 0.0417
Epoch 17/300, seasonal_0 Loss: 0.0566 | 0.0365
Epoch 18/300, seasonal_0 Loss: 0.0541 | 0.0351
Epoch 19/300, seasonal_0 Loss: 0.0523 | 0.0316
Epoch 20/300, seasonal_0 Loss: 0.0504 | 0.0301
Epoch 21/300, seasonal_0 Loss: 0.0497 | 0.0293
Epoch 22/300, seasonal_0 Loss: 0.0481 | 0.0437
Epoch 23/300, seasonal_0 Loss: 0.0458 | 0.0398
Epoch 24/300, seasonal_0 Loss: 0.0470 | 0.0430
Epoch 25/300, seasonal_0 Loss: 0.0447 | 0.0398
Epoch 26/300, seasonal_0 Loss: 0.0448 | 0.0378
Epoch 27/300, seasonal_0 Loss: 0.0428 | 0.0325
Epoch 28/300, seasonal_0 Loss: 0.0389 | 0.0303
Epoch 29/300, seasonal_0 Loss: 0.0372 | 0.0287
Epoch 30/300, seasonal_0 Loss: 0.0361 | 0.0278
Epoch 31/300, seasonal_0 Loss: 0.0353 | 0.0275
Epoch 32/300, seasonal_0 Loss: 0.0346 | 0.0269
Epoch 33/300, seasonal_0 Loss: 0.0339 | 0.0264
Epoch 34/300, seasonal_0 Loss: 0.0334 | 0.0261
Epoch 35/300, seasonal_0 Loss: 0.0328 | 0.0259
Epoch 36/300, seasonal_0 Loss: 0.0325 | 0.0259
Epoch 37/300, seasonal_0 Loss: 0.0321 | 0.0257
Epoch 38/300, seasonal_0 Loss: 0.0317 | 0.0257
Epoch 39/300, seasonal_0 Loss: 0.0312 | 0.0257
Epoch 40/300, seasonal_0 Loss: 0.0309 | 0.0257
Epoch 41/300, seasonal_0 Loss: 0.0305 | 0.0257
Epoch 42/300, seasonal_0 Loss: 0.0302 | 0.0257
Epoch 43/300, seasonal_0 Loss: 0.0299 | 0.0258
Epoch 44/300, seasonal_0 Loss: 0.0296 | 0.0258
Epoch 45/300, seasonal_0 Loss: 0.0293 | 0.0259
Epoch 46/300, seasonal_0 Loss: 0.0290 | 0.0260
Epoch 47/300, seasonal_0 Loss: 0.0288 | 0.0261
Epoch 48/300, seasonal_0 Loss: 0.0286 | 0.0261
Epoch 49/300, seasonal_0 Loss: 0.0284 | 0.0260
Epoch 50/300, seasonal_0 Loss: 0.0282 | 0.0259
Epoch 51/300, seasonal_0 Loss: 0.0281 | 0.0257
Epoch 52/300, seasonal_0 Loss: 0.0279 | 0.0255
Epoch 53/300, seasonal_0 Loss: 0.0278 | 0.0253
Epoch 54/300, seasonal_0 Loss: 0.0277 | 0.0252
Epoch 55/300, seasonal_0 Loss: 0.0276 | 0.0250
Epoch 56/300, seasonal_0 Loss: 0.0274 | 0.0248
Epoch 57/300, seasonal_0 Loss: 0.0273 | 0.0247
Epoch 58/300, seasonal_0 Loss: 0.0272 | 0.0246
Epoch 59/300, seasonal_0 Loss: 0.0271 | 0.0245
Epoch 60/300, seasonal_0 Loss: 0.0271 | 0.0245
Epoch 61/300, seasonal_0 Loss: 0.0270 | 0.0245
Epoch 62/300, seasonal_0 Loss: 0.0269 | 0.0245
Epoch 63/300, seasonal_0 Loss: 0.0268 | 0.0245
Epoch 64/300, seasonal_0 Loss: 0.0268 | 0.0246
Epoch 65/300, seasonal_0 Loss: 0.0267 | 0.0247
Epoch 66/300, seasonal_0 Loss: 0.0266 | 0.0249
Epoch 67/300, seasonal_0 Loss: 0.0266 | 0.0250
Epoch 68/300, seasonal_0 Loss: 0.0265 | 0.0252
Epoch 69/300, seasonal_0 Loss: 0.0265 | 0.0253
Epoch 70/300, seasonal_0 Loss: 0.0264 | 0.0255
Epoch 71/300, seasonal_0 Loss: 0.0264 | 0.0256
Epoch 72/300, seasonal_0 Loss: 0.0263 | 0.0256
Epoch 73/300, seasonal_0 Loss: 0.0263 | 0.0257
Epoch 74/300, seasonal_0 Loss: 0.0262 | 0.0258
Epoch 75/300, seasonal_0 Loss: 0.0262 | 0.0258
Epoch 76/300, seasonal_0 Loss: 0.0261 | 0.0259
Epoch 77/300, seasonal_0 Loss: 0.0261 | 0.0259
Epoch 78/300, seasonal_0 Loss: 0.0261 | 0.0260
Epoch 79/300, seasonal_0 Loss: 0.0260 | 0.0260
Epoch 80/300, seasonal_0 Loss: 0.0260 | 0.0260
Epoch 81/300, seasonal_0 Loss: 0.0260 | 0.0259
Epoch 82/300, seasonal_0 Loss: 0.0260 | 0.0259
Epoch 83/300, seasonal_0 Loss: 0.0259 | 0.0259
Epoch 84/300, seasonal_0 Loss: 0.0259 | 0.0258
Epoch 85/300, seasonal_0 Loss: 0.0259 | 0.0258
Epoch 86/300, seasonal_0 Loss: 0.0259 | 0.0258
Epoch 87/300, seasonal_0 Loss: 0.0259 | 0.0257
Epoch 88/300, seasonal_0 Loss: 0.0258 | 0.0257
Epoch 89/300, seasonal_0 Loss: 0.0258 | 0.0258
Epoch 90/300, seasonal_0 Loss: 0.0258 | 0.0258
Epoch 91/300, seasonal_0 Loss: 0.0258 | 0.0258
Epoch 92/300, seasonal_0 Loss: 0.0258 | 0.0258
Epoch 93/300, seasonal_0 Loss: 0.0258 | 0.0259
Epoch 94/300, seasonal_0 Loss: 0.0258 | 0.0259
Epoch 95/300, seasonal_0 Loss: 0.0258 | 0.0259
Epoch 96/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 97/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 98/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 99/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 100/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 101/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 102/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 103/300, seasonal_0 Loss: 0.0257 | 0.0260
Epoch 104/300, seasonal_0 Loss: 0.0257 | 0.0259
Epoch 105/300, seasonal_0 Loss: 0.0257 | 0.0259
Epoch 106/300, seasonal_0 Loss: 0.0257 | 0.0259
Epoch 107/300, seasonal_0 Loss: 0.0257 | 0.0259
Epoch 108/300, seasonal_0 Loss: 0.0257 | 0.0258
Epoch 109/300, seasonal_0 Loss: 0.0257 | 0.0258
Epoch 110/300, seasonal_0 Loss: 0.0256 | 0.0258
Epoch 111/300, seasonal_0 Loss: 0.0256 | 0.0258
Epoch 112/300, seasonal_0 Loss: 0.0256 | 0.0258
Epoch 113/300, seasonal_0 Loss: 0.0256 | 0.0258
Epoch 114/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 115/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 116/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 117/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 118/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 119/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 120/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 121/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 122/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 123/300, seasonal_0 Loss: 0.0256 | 0.0257
Epoch 124/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 125/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 126/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 127/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 128/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 129/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 130/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 131/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 132/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 133/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 134/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 135/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 136/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 137/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 138/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 139/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 140/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 141/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 142/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 143/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 144/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 145/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 146/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 147/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 148/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 149/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 150/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 151/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 152/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 153/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 154/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 155/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 156/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 157/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 158/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 159/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 160/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 161/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 162/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 163/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 164/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 165/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 166/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 167/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 168/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 169/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 170/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 171/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 172/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 173/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 174/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 175/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 176/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 177/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 178/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 179/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 180/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 181/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 182/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 183/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 184/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 185/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 186/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 187/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 188/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 189/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 190/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 191/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 192/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 193/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 194/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 195/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 196/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 197/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 198/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 199/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 200/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 201/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 202/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 203/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 204/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 205/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 206/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 207/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 208/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 209/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 210/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 211/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 212/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 213/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 214/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 215/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 216/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 217/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 218/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 219/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 220/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 221/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 222/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 223/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 224/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 225/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 226/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 227/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 228/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 229/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 230/300, seasonal_0 Loss: 0.0256 | 0.0256
Epoch 231/300, seasonal_0 Loss: 0.0256 | 0.0256
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 19, 'train_rates': 0.8957163064017348, 'learning_rate': 0.0007417394998563828, 'batch_size': 229, 'step_size': 10, 'gamma': 0.9289464207510643}
Epoch 1/300, seasonal_1 Loss: 0.6539 | 0.2288
Epoch 2/300, seasonal_1 Loss: 0.2575 | 0.1630
Epoch 3/300, seasonal_1 Loss: 0.2176 | 0.2055
Epoch 4/300, seasonal_1 Loss: 0.2351 | 0.1798
Epoch 5/300, seasonal_1 Loss: 0.1556 | 0.1227
Epoch 6/300, seasonal_1 Loss: 0.1752 | 0.0834
Epoch 7/300, seasonal_1 Loss: 0.1475 | 0.0929
Epoch 8/300, seasonal_1 Loss: 0.1525 | 0.1036
Epoch 9/300, seasonal_1 Loss: 0.1359 | 0.0817
Epoch 10/300, seasonal_1 Loss: 0.1614 | 0.1476
Epoch 11/300, seasonal_1 Loss: 0.1568 | 0.3494
Epoch 12/300, seasonal_1 Loss: 0.1845 | 0.1348
Epoch 13/300, seasonal_1 Loss: 0.1713 | 0.1054
Epoch 14/300, seasonal_1 Loss: 0.1284 | 0.0774
Epoch 15/300, seasonal_1 Loss: 0.1281 | 0.1161
Epoch 16/300, seasonal_1 Loss: 0.1319 | 0.0715
Epoch 17/300, seasonal_1 Loss: 0.1155 | 0.0759
Epoch 18/300, seasonal_1 Loss: 0.1318 | 0.0846
Epoch 19/300, seasonal_1 Loss: 0.1192 | 0.0650
Epoch 20/300, seasonal_1 Loss: 0.1160 | 0.0611
Epoch 21/300, seasonal_1 Loss: 0.1038 | 0.0678
Epoch 22/300, seasonal_1 Loss: 0.1043 | 0.0639
Epoch 23/300, seasonal_1 Loss: 0.1017 | 0.0585
Epoch 24/300, seasonal_1 Loss: 0.0997 | 0.0556
Epoch 25/300, seasonal_1 Loss: 0.1038 | 0.0749
Epoch 26/300, seasonal_1 Loss: 0.1040 | 0.0723
Epoch 27/300, seasonal_1 Loss: 0.0990 | 0.0571
Epoch 28/300, seasonal_1 Loss: 0.1073 | 0.1455
Epoch 29/300, seasonal_1 Loss: 0.1181 | 0.0903
Epoch 30/300, seasonal_1 Loss: 0.1104 | 0.0543
Epoch 31/300, seasonal_1 Loss: 0.1110 | 0.0700
Epoch 32/300, seasonal_1 Loss: 0.1060 | 0.0555
Epoch 33/300, seasonal_1 Loss: 0.0987 | 0.1069
Epoch 34/300, seasonal_1 Loss: 0.1037 | 0.0509
Epoch 35/300, seasonal_1 Loss: 0.0922 | 0.0497
Epoch 36/300, seasonal_1 Loss: 0.1002 | 0.0499
Epoch 37/300, seasonal_1 Loss: 0.0887 | 0.0490
Epoch 38/300, seasonal_1 Loss: 0.0882 | 0.0582
Epoch 39/300, seasonal_1 Loss: 0.0856 | 0.0505
Epoch 40/300, seasonal_1 Loss: 0.0854 | 0.0441
Epoch 41/300, seasonal_1 Loss: 0.0835 | 0.0435
Epoch 42/300, seasonal_1 Loss: 0.0826 | 0.0486
Epoch 43/300, seasonal_1 Loss: 0.0817 | 0.0538
Epoch 44/300, seasonal_1 Loss: 0.0816 | 0.0446
Epoch 45/300, seasonal_1 Loss: 0.0810 | 0.0416
Epoch 46/300, seasonal_1 Loss: 0.0797 | 0.0434
Epoch 47/300, seasonal_1 Loss: 0.0793 | 0.0459
Epoch 48/300, seasonal_1 Loss: 0.0787 | 0.0452
Epoch 49/300, seasonal_1 Loss: 0.0779 | 0.0392
Epoch 50/300, seasonal_1 Loss: 0.0769 | 0.0412
Epoch 51/300, seasonal_1 Loss: 0.0766 | 0.0419
Epoch 52/300, seasonal_1 Loss: 0.0761 | 0.0429
Epoch 53/300, seasonal_1 Loss: 0.0753 | 0.0379
Epoch 54/300, seasonal_1 Loss: 0.0748 | 0.0402
Epoch 55/300, seasonal_1 Loss: 0.0741 | 0.0398
Epoch 56/300, seasonal_1 Loss: 0.0736 | 0.0391
Epoch 57/300, seasonal_1 Loss: 0.0729 | 0.0362
Epoch 58/300, seasonal_1 Loss: 0.0725 | 0.0402
Epoch 59/300, seasonal_1 Loss: 0.0718 | 0.0385
Epoch 60/300, seasonal_1 Loss: 0.0716 | 0.0369
Epoch 61/300, seasonal_1 Loss: 0.0710 | 0.0351
Epoch 62/300, seasonal_1 Loss: 0.0709 | 0.0383
Epoch 63/300, seasonal_1 Loss: 0.0701 | 0.0363
Epoch 64/300, seasonal_1 Loss: 0.0700 | 0.0366
Epoch 65/300, seasonal_1 Loss: 0.0696 | 0.0355
Epoch 66/300, seasonal_1 Loss: 0.0696 | 0.0367
Epoch 67/300, seasonal_1 Loss: 0.0692 | 0.0366
Epoch 68/300, seasonal_1 Loss: 0.0694 | 0.0389
Epoch 69/300, seasonal_1 Loss: 0.0696 | 0.0373
Epoch 70/300, seasonal_1 Loss: 0.0708 | 0.0370
Epoch 71/300, seasonal_1 Loss: 0.0712 | 0.0403
Epoch 72/300, seasonal_1 Loss: 0.0735 | 0.0378
Epoch 73/300, seasonal_1 Loss: 0.0709 | 0.0351
Epoch 74/300, seasonal_1 Loss: 0.0688 | 0.0336
Epoch 75/300, seasonal_1 Loss: 0.0713 | 0.0331
Epoch 76/300, seasonal_1 Loss: 0.0687 | 0.0348
Epoch 77/300, seasonal_1 Loss: 0.0683 | 0.0359
Epoch 78/300, seasonal_1 Loss: 0.0677 | 0.0364
Epoch 79/300, seasonal_1 Loss: 0.0669 | 0.0373
Epoch 80/300, seasonal_1 Loss: 0.0668 | 0.0349
Epoch 81/300, seasonal_1 Loss: 0.0668 | 0.0334
Epoch 82/300, seasonal_1 Loss: 0.0680 | 0.0368
Epoch 83/300, seasonal_1 Loss: 0.0723 | 0.0366
Epoch 84/300, seasonal_1 Loss: 0.0712 | 0.0402
Epoch 85/300, seasonal_1 Loss: 0.0673 | 0.0361
Epoch 86/300, seasonal_1 Loss: 0.0661 | 0.0328
Epoch 87/300, seasonal_1 Loss: 0.0649 | 0.0317
Epoch 88/300, seasonal_1 Loss: 0.0653 | 0.0330
Epoch 89/300, seasonal_1 Loss: 0.0645 | 0.0348
Epoch 90/300, seasonal_1 Loss: 0.0660 | 0.0349
Epoch 91/300, seasonal_1 Loss: 0.0660 | 0.0323
Epoch 92/300, seasonal_1 Loss: 0.0664 | 0.0326
Epoch 93/300, seasonal_1 Loss: 0.0648 | 0.0335
Epoch 94/300, seasonal_1 Loss: 0.0641 | 0.0314
Epoch 95/300, seasonal_1 Loss: 0.0635 | 0.0311
Epoch 96/300, seasonal_1 Loss: 0.0628 | 0.0340
Epoch 97/300, seasonal_1 Loss: 0.0633 | 0.0337
Epoch 98/300, seasonal_1 Loss: 0.0633 | 0.0313
Epoch 99/300, seasonal_1 Loss: 0.0626 | 0.0310
Epoch 100/300, seasonal_1 Loss: 0.0623 | 0.0314
Epoch 101/300, seasonal_1 Loss: 0.0620 | 0.0320
Epoch 102/300, seasonal_1 Loss: 0.0620 | 0.0320
Epoch 103/300, seasonal_1 Loss: 0.0617 | 0.0317
Epoch 104/300, seasonal_1 Loss: 0.0613 | 0.0312
Epoch 105/300, seasonal_1 Loss: 0.0616 | 0.0320
Epoch 106/300, seasonal_1 Loss: 0.0620 | 0.0317
Epoch 107/300, seasonal_1 Loss: 0.0617 | 0.0309
Epoch 108/300, seasonal_1 Loss: 0.0610 | 0.0312
Epoch 109/300, seasonal_1 Loss: 0.0612 | 0.0316
Epoch 110/300, seasonal_1 Loss: 0.0610 | 0.0314
Epoch 111/300, seasonal_1 Loss: 0.0605 | 0.0312
Epoch 112/300, seasonal_1 Loss: 0.0605 | 0.0314
Epoch 113/300, seasonal_1 Loss: 0.0608 | 0.0315
Epoch 114/300, seasonal_1 Loss: 0.0608 | 0.0307
Epoch 115/300, seasonal_1 Loss: 0.0602 | 0.0304
Epoch 116/300, seasonal_1 Loss: 0.0600 | 0.0310
Epoch 117/300, seasonal_1 Loss: 0.0600 | 0.0314
Epoch 118/300, seasonal_1 Loss: 0.0596 | 0.0313
Epoch 119/300, seasonal_1 Loss: 0.0595 | 0.0310
Epoch 120/300, seasonal_1 Loss: 0.0596 | 0.0311
Epoch 121/300, seasonal_1 Loss: 0.0597 | 0.0308
Epoch 122/300, seasonal_1 Loss: 0.0594 | 0.0304
Epoch 123/300, seasonal_1 Loss: 0.0590 | 0.0307
Epoch 124/300, seasonal_1 Loss: 0.0590 | 0.0312
Epoch 125/300, seasonal_1 Loss: 0.0589 | 0.0313
Epoch 126/300, seasonal_1 Loss: 0.0586 | 0.0308
Epoch 127/300, seasonal_1 Loss: 0.0586 | 0.0308
Epoch 128/300, seasonal_1 Loss: 0.0588 | 0.0308
Epoch 129/300, seasonal_1 Loss: 0.0587 | 0.0303
Epoch 130/300, seasonal_1 Loss: 0.0582 | 0.0302
Epoch 131/300, seasonal_1 Loss: 0.0582 | 0.0308
Epoch 132/300, seasonal_1 Loss: 0.0583 | 0.0310
Epoch 133/300, seasonal_1 Loss: 0.0580 | 0.0307
Epoch 134/300, seasonal_1 Loss: 0.0578 | 0.0305
Epoch 135/300, seasonal_1 Loss: 0.0580 | 0.0307
Epoch 136/300, seasonal_1 Loss: 0.0580 | 0.0303
Epoch 137/300, seasonal_1 Loss: 0.0577 | 0.0303
Epoch 138/300, seasonal_1 Loss: 0.0575 | 0.0308
Epoch 139/300, seasonal_1 Loss: 0.0576 | 0.0310
Epoch 140/300, seasonal_1 Loss: 0.0574 | 0.0309
Epoch 141/300, seasonal_1 Loss: 0.0572 | 0.0308
Epoch 142/300, seasonal_1 Loss: 0.0575 | 0.0310
Epoch 143/300, seasonal_1 Loss: 0.0575 | 0.0307
Epoch 144/300, seasonal_1 Loss: 0.0570 | 0.0306
Epoch 145/300, seasonal_1 Loss: 0.0569 | 0.0308
Epoch 146/300, seasonal_1 Loss: 0.0570 | 0.0307
Epoch 147/300, seasonal_1 Loss: 0.0568 | 0.0304
Epoch 148/300, seasonal_1 Loss: 0.0567 | 0.0304
Epoch 149/300, seasonal_1 Loss: 0.0568 | 0.0304
Epoch 150/300, seasonal_1 Loss: 0.0568 | 0.0302
Epoch 151/300, seasonal_1 Loss: 0.0564 | 0.0305
Epoch 152/300, seasonal_1 Loss: 0.0564 | 0.0308
Epoch 153/300, seasonal_1 Loss: 0.0564 | 0.0307
Epoch 154/300, seasonal_1 Loss: 0.0561 | 0.0305
Epoch 155/300, seasonal_1 Loss: 0.0561 | 0.0305
Epoch 156/300, seasonal_1 Loss: 0.0562 | 0.0304
Epoch 157/300, seasonal_1 Loss: 0.0559 | 0.0305
Epoch 158/300, seasonal_1 Loss: 0.0558 | 0.0307
Epoch 159/300, seasonal_1 Loss: 0.0559 | 0.0306
Epoch 160/300, seasonal_1 Loss: 0.0557 | 0.0303
Epoch 161/300, seasonal_1 Loss: 0.0556 | 0.0303
Epoch 162/300, seasonal_1 Loss: 0.0557 | 0.0303
Epoch 163/300, seasonal_1 Loss: 0.0555 | 0.0305
Epoch 164/300, seasonal_1 Loss: 0.0553 | 0.0307
Epoch 165/300, seasonal_1 Loss: 0.0554 | 0.0307
Epoch 166/300, seasonal_1 Loss: 0.0552 | 0.0304
Epoch 167/300, seasonal_1 Loss: 0.0551 | 0.0303
Epoch 168/300, seasonal_1 Loss: 0.0552 | 0.0303
Epoch 169/300, seasonal_1 Loss: 0.0551 | 0.0305
Epoch 170/300, seasonal_1 Loss: 0.0549 | 0.0306
Epoch 171/300, seasonal_1 Loss: 0.0549 | 0.0305
Epoch 172/300, seasonal_1 Loss: 0.0548 | 0.0304
Epoch 173/300, seasonal_1 Loss: 0.0547 | 0.0304
Epoch 174/300, seasonal_1 Loss: 0.0547 | 0.0304
Epoch 175/300, seasonal_1 Loss: 0.0547 | 0.0305
Epoch 176/300, seasonal_1 Loss: 0.0546 | 0.0306
Epoch 177/300, seasonal_1 Loss: 0.0545 | 0.0305
Epoch 178/300, seasonal_1 Loss: 0.0545 | 0.0304
Epoch 179/300, seasonal_1 Loss: 0.0544 | 0.0304
Epoch 180/300, seasonal_1 Loss: 0.0543 | 0.0304
Epoch 181/300, seasonal_1 Loss: 0.0543 | 0.0305
Epoch 182/300, seasonal_1 Loss: 0.0542 | 0.0306
Epoch 183/300, seasonal_1 Loss: 0.0542 | 0.0306
Epoch 184/300, seasonal_1 Loss: 0.0541 | 0.0305
Epoch 185/300, seasonal_1 Loss: 0.0541 | 0.0305
Epoch 186/300, seasonal_1 Loss: 0.0540 | 0.0305
Epoch 187/300, seasonal_1 Loss: 0.0540 | 0.0305
Epoch 188/300, seasonal_1 Loss: 0.0539 | 0.0306
Epoch 189/300, seasonal_1 Loss: 0.0539 | 0.0306
Epoch 190/300, seasonal_1 Loss: 0.0538 | 0.0306
Epoch 191/300, seasonal_1 Loss: 0.0538 | 0.0305
Epoch 192/300, seasonal_1 Loss: 0.0537 | 0.0305
Epoch 193/300, seasonal_1 Loss: 0.0537 | 0.0306
Epoch 194/300, seasonal_1 Loss: 0.0536 | 0.0306
Epoch 195/300, seasonal_1 Loss: 0.0536 | 0.0306
Epoch 196/300, seasonal_1 Loss: 0.0535 | 0.0306
Epoch 197/300, seasonal_1 Loss: 0.0535 | 0.0306
Epoch 198/300, seasonal_1 Loss: 0.0534 | 0.0306
Epoch 199/300, seasonal_1 Loss: 0.0534 | 0.0307
Epoch 200/300, seasonal_1 Loss: 0.0534 | 0.0307
Epoch 201/300, seasonal_1 Loss: 0.0533 | 0.0307
Epoch 202/300, seasonal_1 Loss: 0.0533 | 0.0307
Epoch 203/300, seasonal_1 Loss: 0.0532 | 0.0307
Epoch 204/300, seasonal_1 Loss: 0.0532 | 0.0307
Epoch 205/300, seasonal_1 Loss: 0.0532 | 0.0307
Epoch 206/300, seasonal_1 Loss: 0.0531 | 0.0307
Epoch 207/300, seasonal_1 Loss: 0.0531 | 0.0308
Epoch 208/300, seasonal_1 Loss: 0.0530 | 0.0308
Epoch 209/300, seasonal_1 Loss: 0.0530 | 0.0308
Epoch 210/300, seasonal_1 Loss: 0.0530 | 0.0308
Epoch 211/300, seasonal_1 Loss: 0.0529 | 0.0308
Epoch 212/300, seasonal_1 Loss: 0.0529 | 0.0308
Epoch 213/300, seasonal_1 Loss: 0.0529 | 0.0308
Epoch 214/300, seasonal_1 Loss: 0.0528 | 0.0308
Epoch 215/300, seasonal_1 Loss: 0.0528 | 0.0308
Epoch 216/300, seasonal_1 Loss: 0.0527 | 0.0309
Epoch 217/300, seasonal_1 Loss: 0.0527 | 0.0309
Epoch 218/300, seasonal_1 Loss: 0.0527 | 0.0309
Epoch 219/300, seasonal_1 Loss: 0.0526 | 0.0309
Epoch 220/300, seasonal_1 Loss: 0.0526 | 0.0309
Epoch 221/300, seasonal_1 Loss: 0.0526 | 0.0309
Epoch 222/300, seasonal_1 Loss: 0.0526 | 0.0309
Epoch 223/300, seasonal_1 Loss: 0.0525 | 0.0310
Epoch 224/300, seasonal_1 Loss: 0.0525 | 0.0310
Epoch 225/300, seasonal_1 Loss: 0.0525 | 0.0310
Epoch 226/300, seasonal_1 Loss: 0.0524 | 0.0310
Epoch 227/300, seasonal_1 Loss: 0.0524 | 0.0310
Epoch 228/300, seasonal_1 Loss: 0.0524 | 0.0310
Epoch 229/300, seasonal_1 Loss: 0.0523 | 0.0310
Epoch 230/300, seasonal_1 Loss: 0.0523 | 0.0310
Epoch 231/300, seasonal_1 Loss: 0.0523 | 0.0311
Epoch 232/300, seasonal_1 Loss: 0.0522 | 0.0311
Epoch 233/300, seasonal_1 Loss: 0.0522 | 0.0311
Epoch 234/300, seasonal_1 Loss: 0.0522 | 0.0311
Epoch 235/300, seasonal_1 Loss: 0.0522 | 0.0311
Epoch 236/300, seasonal_1 Loss: 0.0521 | 0.0311
Epoch 237/300, seasonal_1 Loss: 0.0521 | 0.0311
Epoch 238/300, seasonal_1 Loss: 0.0521 | 0.0311
Epoch 239/300, seasonal_1 Loss: 0.0521 | 0.0312
Epoch 240/300, seasonal_1 Loss: 0.0520 | 0.0312
Epoch 241/300, seasonal_1 Loss: 0.0520 | 0.0312
Epoch 242/300, seasonal_1 Loss: 0.0520 | 0.0312
Epoch 243/300, seasonal_1 Loss: 0.0520 | 0.0312
Epoch 244/300, seasonal_1 Loss: 0.0519 | 0.0312
Epoch 245/300, seasonal_1 Loss: 0.0519 | 0.0312
Epoch 246/300, seasonal_1 Loss: 0.0519 | 0.0312
Epoch 247/300, seasonal_1 Loss: 0.0519 | 0.0313
Epoch 248/300, seasonal_1 Loss: 0.0518 | 0.0313
Epoch 249/300, seasonal_1 Loss: 0.0518 | 0.0313
Epoch 250/300, seasonal_1 Loss: 0.0518 | 0.0313
Epoch 251/300, seasonal_1 Loss: 0.0518 | 0.0313
Epoch 252/300, seasonal_1 Loss: 0.0517 | 0.0313
Epoch 253/300, seasonal_1 Loss: 0.0517 | 0.0313
Epoch 254/300, seasonal_1 Loss: 0.0517 | 0.0313
Epoch 255/300, seasonal_1 Loss: 0.0517 | 0.0313
Epoch 256/300, seasonal_1 Loss: 0.0517 | 0.0314
Epoch 257/300, seasonal_1 Loss: 0.0516 | 0.0314
Epoch 258/300, seasonal_1 Loss: 0.0516 | 0.0314
Epoch 259/300, seasonal_1 Loss: 0.0516 | 0.0314
Epoch 260/300, seasonal_1 Loss: 0.0516 | 0.0314
Epoch 261/300, seasonal_1 Loss: 0.0516 | 0.0314
Epoch 262/300, seasonal_1 Loss: 0.0515 | 0.0314
Epoch 263/300, seasonal_1 Loss: 0.0515 | 0.0314
Epoch 264/300, seasonal_1 Loss: 0.0515 | 0.0314
Epoch 265/300, seasonal_1 Loss: 0.0515 | 0.0314
Epoch 266/300, seasonal_1 Loss: 0.0515 | 0.0315
Epoch 267/300, seasonal_1 Loss: 0.0514 | 0.0315
Epoch 268/300, seasonal_1 Loss: 0.0514 | 0.0315
Epoch 269/300, seasonal_1 Loss: 0.0514 | 0.0315
Epoch 270/300, seasonal_1 Loss: 0.0514 | 0.0315
Epoch 271/300, seasonal_1 Loss: 0.0514 | 0.0315
Epoch 272/300, seasonal_1 Loss: 0.0513 | 0.0315
Epoch 273/300, seasonal_1 Loss: 0.0513 | 0.0315
Epoch 274/300, seasonal_1 Loss: 0.0513 | 0.0315
Epoch 275/300, seasonal_1 Loss: 0.0513 | 0.0315
Epoch 276/300, seasonal_1 Loss: 0.0513 | 0.0316
Epoch 277/300, seasonal_1 Loss: 0.0513 | 0.0316
Epoch 278/300, seasonal_1 Loss: 0.0512 | 0.0316
Epoch 279/300, seasonal_1 Loss: 0.0512 | 0.0316
Epoch 280/300, seasonal_1 Loss: 0.0512 | 0.0316
Epoch 281/300, seasonal_1 Loss: 0.0512 | 0.0316
Epoch 282/300, seasonal_1 Loss: 0.0512 | 0.0316
Epoch 283/300, seasonal_1 Loss: 0.0512 | 0.0316
Epoch 284/300, seasonal_1 Loss: 0.0511 | 0.0316
Epoch 285/300, seasonal_1 Loss: 0.0511 | 0.0316
Epoch 286/300, seasonal_1 Loss: 0.0511 | 0.0316
Epoch 287/300, seasonal_1 Loss: 0.0511 | 0.0316
Epoch 288/300, seasonal_1 Loss: 0.0511 | 0.0316
Epoch 289/300, seasonal_1 Loss: 0.0511 | 0.0317
Epoch 290/300, seasonal_1 Loss: 0.0511 | 0.0317
Epoch 291/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 292/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 293/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 294/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 295/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 296/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 297/300, seasonal_1 Loss: 0.0510 | 0.0318
Epoch 298/300, seasonal_1 Loss: 0.0510 | 0.0317
Epoch 299/300, seasonal_1 Loss: 0.0510 | 0.0318
Epoch 300/300, seasonal_1 Loss: 0.0510 | 0.0317
Training seasonal_2 component with params: {'observation_period_num': 14, 'train_rates': 0.9138269142879545, 'learning_rate': 0.0009476912444006162, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9761858467843287}
Epoch 1/300, seasonal_2 Loss: 0.2461 | 0.0938
Epoch 2/300, seasonal_2 Loss: 0.1296 | 0.0904
Epoch 3/300, seasonal_2 Loss: 0.1184 | 0.0691
Epoch 4/300, seasonal_2 Loss: 0.1097 | 0.0601
Epoch 5/300, seasonal_2 Loss: 0.1026 | 0.0495
Epoch 6/300, seasonal_2 Loss: 0.0975 | 0.0431
Epoch 7/300, seasonal_2 Loss: 0.0917 | 0.0439
Epoch 8/300, seasonal_2 Loss: 0.0890 | 0.0386
Epoch 9/300, seasonal_2 Loss: 0.0823 | 0.0364
Epoch 10/300, seasonal_2 Loss: 0.0788 | 0.0346
Epoch 11/300, seasonal_2 Loss: 0.0768 | 0.0354
Epoch 12/300, seasonal_2 Loss: 0.0770 | 0.0357
Epoch 13/300, seasonal_2 Loss: 0.0736 | 0.0332
Epoch 14/300, seasonal_2 Loss: 0.0708 | 0.0403
Epoch 15/300, seasonal_2 Loss: 0.0769 | 0.0348
Epoch 16/300, seasonal_2 Loss: 0.0695 | 0.0309
Epoch 17/300, seasonal_2 Loss: 0.0672 | 0.0385
Epoch 18/300, seasonal_2 Loss: 0.0672 | 0.0360
Epoch 19/300, seasonal_2 Loss: 0.0671 | 0.0482
Epoch 20/300, seasonal_2 Loss: 0.0697 | 0.0456
Epoch 21/300, seasonal_2 Loss: 0.0686 | 0.0452
Epoch 22/300, seasonal_2 Loss: 0.0657 | 0.0530
Epoch 23/300, seasonal_2 Loss: 0.0684 | 0.0346
Epoch 24/300, seasonal_2 Loss: 0.0624 | 0.0402
Epoch 25/300, seasonal_2 Loss: 0.0592 | 0.0395
Epoch 26/300, seasonal_2 Loss: 0.0597 | 0.0447
Epoch 27/300, seasonal_2 Loss: 0.0602 | 0.0345
Epoch 28/300, seasonal_2 Loss: 0.0575 | 0.0302
Epoch 29/300, seasonal_2 Loss: 0.0555 | 0.0278
Epoch 30/300, seasonal_2 Loss: 0.0538 | 0.0263
Epoch 31/300, seasonal_2 Loss: 0.0525 | 0.0267
Epoch 32/300, seasonal_2 Loss: 0.0509 | 0.0277
Epoch 33/300, seasonal_2 Loss: 0.0508 | 0.0278
Epoch 34/300, seasonal_2 Loss: 0.0495 | 0.0291
Epoch 35/300, seasonal_2 Loss: 0.0530 | 0.0368
Epoch 36/300, seasonal_2 Loss: 0.0588 | 0.0321
Epoch 37/300, seasonal_2 Loss: 0.0537 | 0.0299
Epoch 38/300, seasonal_2 Loss: 0.0506 | 0.0305
Epoch 39/300, seasonal_2 Loss: 0.0505 | 0.0290
Epoch 40/300, seasonal_2 Loss: 0.0487 | 0.0276
Epoch 41/300, seasonal_2 Loss: 0.0480 | 0.0317
Epoch 42/300, seasonal_2 Loss: 0.0467 | 0.0303
Epoch 43/300, seasonal_2 Loss: 0.0467 | 0.0309
Epoch 44/300, seasonal_2 Loss: 0.0463 | 0.0305
Epoch 45/300, seasonal_2 Loss: 0.0463 | 0.0305
Epoch 46/300, seasonal_2 Loss: 0.0448 | 0.0304
Epoch 47/300, seasonal_2 Loss: 0.0455 | 0.0354
Epoch 48/300, seasonal_2 Loss: 0.0482 | 0.0305
Epoch 49/300, seasonal_2 Loss: 0.0443 | 0.0323
Epoch 50/300, seasonal_2 Loss: 0.0456 | 0.0333
Epoch 51/300, seasonal_2 Loss: 0.0451 | 0.0440
Epoch 52/300, seasonal_2 Loss: 0.0494 | 0.0452
Epoch 53/300, seasonal_2 Loss: 0.0565 | 0.0390
Epoch 54/300, seasonal_2 Loss: 0.0495 | 0.0350
Epoch 55/300, seasonal_2 Loss: 0.0481 | 0.0465
Epoch 56/300, seasonal_2 Loss: 0.0504 | 0.0393
Epoch 57/300, seasonal_2 Loss: 0.0487 | 0.0398
Epoch 58/300, seasonal_2 Loss: 0.0495 | 0.0373
Epoch 59/300, seasonal_2 Loss: 0.0445 | 0.0303
Epoch 60/300, seasonal_2 Loss: 0.0424 | 0.0286
Epoch 61/300, seasonal_2 Loss: 0.0413 | 0.0290
Epoch 62/300, seasonal_2 Loss: 0.0405 | 0.0292
Epoch 63/300, seasonal_2 Loss: 0.0404 | 0.0316
Epoch 64/300, seasonal_2 Loss: 0.0411 | 0.0317
Epoch 65/300, seasonal_2 Loss: 0.0403 | 0.0356
Epoch 66/300, seasonal_2 Loss: 0.0382 | 0.0379
Epoch 67/300, seasonal_2 Loss: 0.0433 | 0.0319
Epoch 68/300, seasonal_2 Loss: 0.0402 | 0.0303
Epoch 69/300, seasonal_2 Loss: 0.0384 | 0.0339
Epoch 70/300, seasonal_2 Loss: 0.0385 | 0.0335
Epoch 71/300, seasonal_2 Loss: 0.0420 | 0.0325
Epoch 72/300, seasonal_2 Loss: 0.0385 | 0.0418
Epoch 73/300, seasonal_2 Loss: 0.0388 | 0.0427
Epoch 74/300, seasonal_2 Loss: 0.0362 | 0.0274
Epoch 75/300, seasonal_2 Loss: 0.0408 | 0.0327
Epoch 76/300, seasonal_2 Loss: 0.0407 | 0.0457
Epoch 77/300, seasonal_2 Loss: 0.0516 | 0.0302
Epoch 78/300, seasonal_2 Loss: 0.0401 | 0.0533
Epoch 79/300, seasonal_2 Loss: 0.0389 | 0.0474
Epoch 80/300, seasonal_2 Loss: 0.0487 | 0.0332
Epoch 81/300, seasonal_2 Loss: 0.0424 | 0.0309
Epoch 82/300, seasonal_2 Loss: 0.0408 | 0.0296
Epoch 83/300, seasonal_2 Loss: 0.0384 | 0.0332
Epoch 84/300, seasonal_2 Loss: 0.0420 | 0.0325
Epoch 85/300, seasonal_2 Loss: 0.0464 | 0.0469
Epoch 86/300, seasonal_2 Loss: 0.0506 | 0.0388
Epoch 87/300, seasonal_2 Loss: 0.0479 | 0.0281
Epoch 88/300, seasonal_2 Loss: 0.0417 | 0.0280
Epoch 89/300, seasonal_2 Loss: 0.0389 | 0.0290
Epoch 90/300, seasonal_2 Loss: 0.0361 | 0.0279
Epoch 91/300, seasonal_2 Loss: 0.0331 | 0.0273
Epoch 92/300, seasonal_2 Loss: 0.0315 | 0.0274
Epoch 93/300, seasonal_2 Loss: 0.0313 | 0.0349
Epoch 94/300, seasonal_2 Loss: 0.0350 | 0.0281
Epoch 95/300, seasonal_2 Loss: 0.0329 | 0.0285
Epoch 96/300, seasonal_2 Loss: 0.0315 | 0.0276
Epoch 97/300, seasonal_2 Loss: 0.0297 | 0.0278
Epoch 98/300, seasonal_2 Loss: 0.0292 | 0.0280
Epoch 99/300, seasonal_2 Loss: 0.0356 | 0.0478
Epoch 100/300, seasonal_2 Loss: 0.0362 | 0.0299
Epoch 101/300, seasonal_2 Loss: 0.0318 | 0.0322
Epoch 102/300, seasonal_2 Loss: 0.0311 | 0.0302
Epoch 103/300, seasonal_2 Loss: 0.0302 | 0.0315
Epoch 104/300, seasonal_2 Loss: 0.0316 | 0.0334
Epoch 105/300, seasonal_2 Loss: 0.0329 | 0.0332
Epoch 106/300, seasonal_2 Loss: 0.0315 | 0.0305
Epoch 107/300, seasonal_2 Loss: 0.0296 | 0.0304
Epoch 108/300, seasonal_2 Loss: 0.0290 | 0.0303
Epoch 109/300, seasonal_2 Loss: 0.0286 | 0.0302
Epoch 110/300, seasonal_2 Loss: 0.0281 | 0.0317
Epoch 111/300, seasonal_2 Loss: 0.0411 | 0.0302
Epoch 112/300, seasonal_2 Loss: 0.0356 | 0.0549
Epoch 113/300, seasonal_2 Loss: 0.0439 | 0.0323
Epoch 114/300, seasonal_2 Loss: 0.0365 | 0.0321
Epoch 115/300, seasonal_2 Loss: 0.0339 | 0.0329
Epoch 116/300, seasonal_2 Loss: 0.0309 | 0.0299
Epoch 117/300, seasonal_2 Loss: 0.0316 | 0.0348
Epoch 118/300, seasonal_2 Loss: 0.0290 | 0.0312
Epoch 119/300, seasonal_2 Loss: 0.0281 | 0.0308
Epoch 120/300, seasonal_2 Loss: 0.0277 | 0.0305
Epoch 121/300, seasonal_2 Loss: 0.0274 | 0.0303
Epoch 122/300, seasonal_2 Loss: 0.0269 | 0.0301
Epoch 123/300, seasonal_2 Loss: 0.0262 | 0.0307
Epoch 124/300, seasonal_2 Loss: 0.0258 | 0.0307
Epoch 125/300, seasonal_2 Loss: 0.0255 | 0.0309
Epoch 126/300, seasonal_2 Loss: 0.0253 | 0.0312
Epoch 127/300, seasonal_2 Loss: 0.0251 | 0.0312
Epoch 128/300, seasonal_2 Loss: 0.0250 | 0.0312
Epoch 129/300, seasonal_2 Loss: 0.0248 | 0.0314
Epoch 130/300, seasonal_2 Loss: 0.0247 | 0.0313
Epoch 131/300, seasonal_2 Loss: 0.0247 | 0.0316
Epoch 132/300, seasonal_2 Loss: 0.0248 | 0.0318
Epoch 133/300, seasonal_2 Loss: 0.0249 | 0.0316
Epoch 134/300, seasonal_2 Loss: 0.0246 | 0.0318
Epoch 135/300, seasonal_2 Loss: 0.0245 | 0.0316
Epoch 136/300, seasonal_2 Loss: 0.0243 | 0.0318
Epoch 137/300, seasonal_2 Loss: 0.0243 | 0.0317
Epoch 138/300, seasonal_2 Loss: 0.0240 | 0.0315
Epoch 139/300, seasonal_2 Loss: 0.0239 | 0.0321
Epoch 140/300, seasonal_2 Loss: 0.0238 | 0.0318
Epoch 141/300, seasonal_2 Loss: 0.0239 | 0.0321
Epoch 142/300, seasonal_2 Loss: 0.0237 | 0.0323
Epoch 143/300, seasonal_2 Loss: 0.0234 | 0.0320
Epoch 144/300, seasonal_2 Loss: 0.0229 | 0.0322
Epoch 145/300, seasonal_2 Loss: 0.0227 | 0.0321
Epoch 146/300, seasonal_2 Loss: 0.0225 | 0.0324
Epoch 147/300, seasonal_2 Loss: 0.0224 | 0.0320
Epoch 148/300, seasonal_2 Loss: 0.0225 | 0.0326
Epoch 149/300, seasonal_2 Loss: 0.0222 | 0.0322
Epoch 150/300, seasonal_2 Loss: 0.0221 | 0.0325
Epoch 151/300, seasonal_2 Loss: 0.0220 | 0.0324
Epoch 152/300, seasonal_2 Loss: 0.0219 | 0.0327
Epoch 153/300, seasonal_2 Loss: 0.0218 | 0.0325
Epoch 154/300, seasonal_2 Loss: 0.0217 | 0.0327
Epoch 155/300, seasonal_2 Loss: 0.0216 | 0.0325
Epoch 156/300, seasonal_2 Loss: 0.0216 | 0.0331
Epoch 157/300, seasonal_2 Loss: 0.0216 | 0.0327
Epoch 158/300, seasonal_2 Loss: 0.0215 | 0.0333
Epoch 159/300, seasonal_2 Loss: 0.0213 | 0.0327
Epoch 160/300, seasonal_2 Loss: 0.0212 | 0.0333
Epoch 161/300, seasonal_2 Loss: 0.0214 | 0.0337
Epoch 162/300, seasonal_2 Loss: 0.0212 | 0.0334
Epoch 163/300, seasonal_2 Loss: 0.0212 | 0.0348
Epoch 164/300, seasonal_2 Loss: 0.0211 | 0.0339
Epoch 165/300, seasonal_2 Loss: 0.0211 | 0.0340
Epoch 166/300, seasonal_2 Loss: 0.0212 | 0.0323
Epoch 167/300, seasonal_2 Loss: 0.0213 | 0.0321
Epoch 168/300, seasonal_2 Loss: 0.0211 | 0.0325
Epoch 169/300, seasonal_2 Loss: 0.0212 | 0.0337
Epoch 170/300, seasonal_2 Loss: 0.0213 | 0.0344
Epoch 171/300, seasonal_2 Loss: 0.0249 | 0.0343
Epoch 172/300, seasonal_2 Loss: 0.0327 | 0.0668
Epoch 173/300, seasonal_2 Loss: 0.0273 | 0.0337
Epoch 174/300, seasonal_2 Loss: 0.0231 | 0.0324
Epoch 175/300, seasonal_2 Loss: 0.0221 | 0.0329
Epoch 176/300, seasonal_2 Loss: 0.0214 | 0.0330
Epoch 177/300, seasonal_2 Loss: 0.0211 | 0.0329
Epoch 178/300, seasonal_2 Loss: 0.0209 | 0.0330
Epoch 179/300, seasonal_2 Loss: 0.0207 | 0.0329
Epoch 180/300, seasonal_2 Loss: 0.0206 | 0.0330
Epoch 181/300, seasonal_2 Loss: 0.0205 | 0.0329
Epoch 182/300, seasonal_2 Loss: 0.0203 | 0.0330
Epoch 183/300, seasonal_2 Loss: 0.0202 | 0.0329
Epoch 184/300, seasonal_2 Loss: 0.0201 | 0.0329
Epoch 185/300, seasonal_2 Loss: 0.0201 | 0.0328
Epoch 186/300, seasonal_2 Loss: 0.0200 | 0.0329
Epoch 187/300, seasonal_2 Loss: 0.0199 | 0.0328
Epoch 188/300, seasonal_2 Loss: 0.0198 | 0.0329
Epoch 189/300, seasonal_2 Loss: 0.0198 | 0.0328
Epoch 190/300, seasonal_2 Loss: 0.0197 | 0.0330
Epoch 191/300, seasonal_2 Loss: 0.0196 | 0.0328
Epoch 192/300, seasonal_2 Loss: 0.0196 | 0.0330
Epoch 193/300, seasonal_2 Loss: 0.0195 | 0.0329
Epoch 194/300, seasonal_2 Loss: 0.0194 | 0.0330
Epoch 195/300, seasonal_2 Loss: 0.0193 | 0.0330
Epoch 196/300, seasonal_2 Loss: 0.0193 | 0.0331
Epoch 197/300, seasonal_2 Loss: 0.0192 | 0.0330
Epoch 198/300, seasonal_2 Loss: 0.0192 | 0.0332
Epoch 199/300, seasonal_2 Loss: 0.0191 | 0.0331
Epoch 200/300, seasonal_2 Loss: 0.0190 | 0.0332
Epoch 201/300, seasonal_2 Loss: 0.0190 | 0.0332
Epoch 202/300, seasonal_2 Loss: 0.0189 | 0.0333
Epoch 203/300, seasonal_2 Loss: 0.0189 | 0.0333
Epoch 204/300, seasonal_2 Loss: 0.0188 | 0.0334
Epoch 205/300, seasonal_2 Loss: 0.0187 | 0.0334
Epoch 206/300, seasonal_2 Loss: 0.0187 | 0.0335
Epoch 207/300, seasonal_2 Loss: 0.0187 | 0.0336
Epoch 208/300, seasonal_2 Loss: 0.0186 | 0.0336
Epoch 209/300, seasonal_2 Loss: 0.0185 | 0.0337
Epoch 210/300, seasonal_2 Loss: 0.0185 | 0.0340
Epoch 211/300, seasonal_2 Loss: 0.0185 | 0.0335
Epoch 212/300, seasonal_2 Loss: 0.0184 | 0.0338
Epoch 213/300, seasonal_2 Loss: 0.0183 | 0.0336
Epoch 214/300, seasonal_2 Loss: 0.0182 | 0.0338
Epoch 215/300, seasonal_2 Loss: 0.0182 | 0.0335
Epoch 216/300, seasonal_2 Loss: 0.0181 | 0.0338
Epoch 217/300, seasonal_2 Loss: 0.0180 | 0.0337
Epoch 218/300, seasonal_2 Loss: 0.0180 | 0.0339
Epoch 219/300, seasonal_2 Loss: 0.0179 | 0.0338
Epoch 220/300, seasonal_2 Loss: 0.0179 | 0.0339
Epoch 221/300, seasonal_2 Loss: 0.0178 | 0.0338
Epoch 222/300, seasonal_2 Loss: 0.0178 | 0.0339
Epoch 223/300, seasonal_2 Loss: 0.0178 | 0.0339
Epoch 224/300, seasonal_2 Loss: 0.0177 | 0.0339
Epoch 225/300, seasonal_2 Loss: 0.0177 | 0.0340
Epoch 226/300, seasonal_2 Loss: 0.0177 | 0.0340
Epoch 227/300, seasonal_2 Loss: 0.0176 | 0.0342
Epoch 228/300, seasonal_2 Loss: 0.0176 | 0.0340
Epoch 229/300, seasonal_2 Loss: 0.0176 | 0.0340
Epoch 230/300, seasonal_2 Loss: 0.0176 | 0.0341
Epoch 231/300, seasonal_2 Loss: 0.0176 | 0.0342
Epoch 232/300, seasonal_2 Loss: 0.0176 | 0.0343
Epoch 233/300, seasonal_2 Loss: 0.0176 | 0.0344
Epoch 234/300, seasonal_2 Loss: 0.0176 | 0.0342
Epoch 235/300, seasonal_2 Loss: 0.0176 | 0.0345
Epoch 236/300, seasonal_2 Loss: 0.0175 | 0.0340
Epoch 237/300, seasonal_2 Loss: 0.0174 | 0.0345
Epoch 238/300, seasonal_2 Loss: 0.0174 | 0.0341
Epoch 239/300, seasonal_2 Loss: 0.0174 | 0.0345
Epoch 240/300, seasonal_2 Loss: 0.0173 | 0.0342
Epoch 241/300, seasonal_2 Loss: 0.0173 | 0.0345
Epoch 242/300, seasonal_2 Loss: 0.0173 | 0.0343
Epoch 243/300, seasonal_2 Loss: 0.0172 | 0.0345
Epoch 244/300, seasonal_2 Loss: 0.0172 | 0.0343
Epoch 245/300, seasonal_2 Loss: 0.0171 | 0.0345
Epoch 246/300, seasonal_2 Loss: 0.0172 | 0.0344
Epoch 247/300, seasonal_2 Loss: 0.0171 | 0.0346
Epoch 248/300, seasonal_2 Loss: 0.0171 | 0.0345
Epoch 249/300, seasonal_2 Loss: 0.0171 | 0.0347
Epoch 250/300, seasonal_2 Loss: 0.0170 | 0.0345
Epoch 251/300, seasonal_2 Loss: 0.0170 | 0.0348
Epoch 252/300, seasonal_2 Loss: 0.0170 | 0.0345
Epoch 253/300, seasonal_2 Loss: 0.0170 | 0.0349
Epoch 254/300, seasonal_2 Loss: 0.0170 | 0.0345
Epoch 255/300, seasonal_2 Loss: 0.0169 | 0.0349
Epoch 256/300, seasonal_2 Loss: 0.0169 | 0.0346
Epoch 257/300, seasonal_2 Loss: 0.0169 | 0.0349
Epoch 258/300, seasonal_2 Loss: 0.0169 | 0.0346
Epoch 259/300, seasonal_2 Loss: 0.0169 | 0.0349
Epoch 260/300, seasonal_2 Loss: 0.0168 | 0.0347
Epoch 261/300, seasonal_2 Loss: 0.0168 | 0.0350
Epoch 262/300, seasonal_2 Loss: 0.0168 | 0.0347
Epoch 263/300, seasonal_2 Loss: 0.0168 | 0.0350
Epoch 264/300, seasonal_2 Loss: 0.0167 | 0.0348
Epoch 265/300, seasonal_2 Loss: 0.0167 | 0.0350
Epoch 266/300, seasonal_2 Loss: 0.0167 | 0.0348
Epoch 267/300, seasonal_2 Loss: 0.0167 | 0.0350
Epoch 268/300, seasonal_2 Loss: 0.0167 | 0.0348
Epoch 269/300, seasonal_2 Loss: 0.0167 | 0.0350
Epoch 270/300, seasonal_2 Loss: 0.0166 | 0.0348
Epoch 271/300, seasonal_2 Loss: 0.0166 | 0.0349
Epoch 272/300, seasonal_2 Loss: 0.0166 | 0.0348
Epoch 273/300, seasonal_2 Loss: 0.0166 | 0.0349
Epoch 274/300, seasonal_2 Loss: 0.0166 | 0.0349
Epoch 275/300, seasonal_2 Loss: 0.0165 | 0.0349
Epoch 276/300, seasonal_2 Loss: 0.0165 | 0.0350
Epoch 277/300, seasonal_2 Loss: 0.0165 | 0.0349
Epoch 278/300, seasonal_2 Loss: 0.0165 | 0.0350
Epoch 279/300, seasonal_2 Loss: 0.0165 | 0.0350
Epoch 280/300, seasonal_2 Loss: 0.0165 | 0.0350
Epoch 281/300, seasonal_2 Loss: 0.0164 | 0.0350
Epoch 282/300, seasonal_2 Loss: 0.0164 | 0.0350
Epoch 283/300, seasonal_2 Loss: 0.0164 | 0.0351
Epoch 284/300, seasonal_2 Loss: 0.0164 | 0.0350
Epoch 285/300, seasonal_2 Loss: 0.0164 | 0.0350
Epoch 286/300, seasonal_2 Loss: 0.0164 | 0.0351
Epoch 287/300, seasonal_2 Loss: 0.0164 | 0.0351
Epoch 288/300, seasonal_2 Loss: 0.0164 | 0.0352
Epoch 289/300, seasonal_2 Loss: 0.0164 | 0.0352
Epoch 290/300, seasonal_2 Loss: 0.0164 | 0.0353
Epoch 291/300, seasonal_2 Loss: 0.0164 | 0.0353
Epoch 292/300, seasonal_2 Loss: 0.0163 | 0.0352
Epoch 293/300, seasonal_2 Loss: 0.0163 | 0.0352
Epoch 294/300, seasonal_2 Loss: 0.0163 | 0.0351
Epoch 295/300, seasonal_2 Loss: 0.0163 | 0.0353
Epoch 296/300, seasonal_2 Loss: 0.0163 | 0.0352
Epoch 297/300, seasonal_2 Loss: 0.0164 | 0.0355
Epoch 298/300, seasonal_2 Loss: 0.0164 | 0.0352
Epoch 299/300, seasonal_2 Loss: 0.0164 | 0.0351
Epoch 300/300, seasonal_2 Loss: 0.0163 | 0.0350
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.8682057983440555, 'learning_rate': 0.0008762650879126588, 'batch_size': 147, 'step_size': 4, 'gamma': 0.922633925455207}
Epoch 1/300, seasonal_3 Loss: 0.6005 | 0.1913
Epoch 2/300, seasonal_3 Loss: 0.1988 | 0.1025
Epoch 3/300, seasonal_3 Loss: 0.2161 | 0.1606
Epoch 4/300, seasonal_3 Loss: 0.1692 | 0.0821
Epoch 5/300, seasonal_3 Loss: 0.1317 | 0.0730
Epoch 6/300, seasonal_3 Loss: 0.1220 | 0.0642
Epoch 7/300, seasonal_3 Loss: 0.1138 | 0.0588
Epoch 8/300, seasonal_3 Loss: 0.1102 | 0.0583
Epoch 9/300, seasonal_3 Loss: 0.1107 | 0.0596
Epoch 10/300, seasonal_3 Loss: 0.1134 | 0.0646
Epoch 11/300, seasonal_3 Loss: 0.1119 | 0.0652
Epoch 12/300, seasonal_3 Loss: 0.1127 | 0.0644
Epoch 13/300, seasonal_3 Loss: 0.1082 | 0.0583
Epoch 14/300, seasonal_3 Loss: 0.1063 | 0.0567
Epoch 15/300, seasonal_3 Loss: 0.1102 | 0.0662
Epoch 16/300, seasonal_3 Loss: 0.1046 | 0.0530
Epoch 17/300, seasonal_3 Loss: 0.0994 | 0.0506
Epoch 18/300, seasonal_3 Loss: 0.0977 | 0.0583
Epoch 19/300, seasonal_3 Loss: 0.0973 | 0.0562
Epoch 20/300, seasonal_3 Loss: 0.0945 | 0.0493
Epoch 21/300, seasonal_3 Loss: 0.0921 | 0.0472
Epoch 22/300, seasonal_3 Loss: 0.0905 | 0.0437
Epoch 23/300, seasonal_3 Loss: 0.0883 | 0.0420
Epoch 24/300, seasonal_3 Loss: 0.0865 | 0.0403
Epoch 25/300, seasonal_3 Loss: 0.0851 | 0.0406
Epoch 26/300, seasonal_3 Loss: 0.0847 | 0.0399
Epoch 27/300, seasonal_3 Loss: 0.0842 | 0.0397
Epoch 28/300, seasonal_3 Loss: 0.0837 | 0.0392
Epoch 29/300, seasonal_3 Loss: 0.0833 | 0.0390
Epoch 30/300, seasonal_3 Loss: 0.0829 | 0.0388
Epoch 31/300, seasonal_3 Loss: 0.0825 | 0.0385
Epoch 32/300, seasonal_3 Loss: 0.0821 | 0.0384
Epoch 33/300, seasonal_3 Loss: 0.0818 | 0.0382
Epoch 34/300, seasonal_3 Loss: 0.0815 | 0.0382
Epoch 35/300, seasonal_3 Loss: 0.0813 | 0.0379
Epoch 36/300, seasonal_3 Loss: 0.0810 | 0.0377
Epoch 37/300, seasonal_3 Loss: 0.0807 | 0.0372
Epoch 38/300, seasonal_3 Loss: 0.0803 | 0.0369
Epoch 39/300, seasonal_3 Loss: 0.0798 | 0.0365
Epoch 40/300, seasonal_3 Loss: 0.0794 | 0.0362
Epoch 41/300, seasonal_3 Loss: 0.0789 | 0.0361
Epoch 42/300, seasonal_3 Loss: 0.0786 | 0.0358
Epoch 43/300, seasonal_3 Loss: 0.0784 | 0.0358
Epoch 44/300, seasonal_3 Loss: 0.0782 | 0.0354
Epoch 45/300, seasonal_3 Loss: 0.0780 | 0.0355
Epoch 46/300, seasonal_3 Loss: 0.0779 | 0.0351
Epoch 47/300, seasonal_3 Loss: 0.0778 | 0.0353
Epoch 48/300, seasonal_3 Loss: 0.0776 | 0.0349
Epoch 49/300, seasonal_3 Loss: 0.0774 | 0.0351
Epoch 50/300, seasonal_3 Loss: 0.0772 | 0.0346
Epoch 51/300, seasonal_3 Loss: 0.0771 | 0.0348
Epoch 52/300, seasonal_3 Loss: 0.0769 | 0.0342
Epoch 53/300, seasonal_3 Loss: 0.0769 | 0.0346
Epoch 54/300, seasonal_3 Loss: 0.0769 | 0.0341
Epoch 55/300, seasonal_3 Loss: 0.0771 | 0.0347
Epoch 56/300, seasonal_3 Loss: 0.0776 | 0.0353
Epoch 57/300, seasonal_3 Loss: 0.0784 | 0.0351
Epoch 58/300, seasonal_3 Loss: 0.0783 | 0.0343
Epoch 59/300, seasonal_3 Loss: 0.0766 | 0.0341
Epoch 60/300, seasonal_3 Loss: 0.0757 | 0.0337
Epoch 61/300, seasonal_3 Loss: 0.0755 | 0.0333
Epoch 62/300, seasonal_3 Loss: 0.0753 | 0.0332
Epoch 63/300, seasonal_3 Loss: 0.0751 | 0.0331
Epoch 64/300, seasonal_3 Loss: 0.0751 | 0.0331
Epoch 65/300, seasonal_3 Loss: 0.0750 | 0.0332
Epoch 66/300, seasonal_3 Loss: 0.0750 | 0.0333
Epoch 67/300, seasonal_3 Loss: 0.0750 | 0.0335
Epoch 68/300, seasonal_3 Loss: 0.0751 | 0.0338
Epoch 69/300, seasonal_3 Loss: 0.0751 | 0.0339
Epoch 70/300, seasonal_3 Loss: 0.0752 | 0.0336
Epoch 71/300, seasonal_3 Loss: 0.0752 | 0.0331
Epoch 72/300, seasonal_3 Loss: 0.0754 | 0.0333
Epoch 73/300, seasonal_3 Loss: 0.0761 | 0.0342
Epoch 74/300, seasonal_3 Loss: 0.0769 | 0.0340
Epoch 75/300, seasonal_3 Loss: 0.0763 | 0.0328
Epoch 76/300, seasonal_3 Loss: 0.0747 | 0.0324
Epoch 77/300, seasonal_3 Loss: 0.0736 | 0.0323
Epoch 78/300, seasonal_3 Loss: 0.0733 | 0.0323
Epoch 79/300, seasonal_3 Loss: 0.0732 | 0.0323
Epoch 80/300, seasonal_3 Loss: 0.0731 | 0.0322
Epoch 81/300, seasonal_3 Loss: 0.0730 | 0.0321
Epoch 82/300, seasonal_3 Loss: 0.0729 | 0.0320
Epoch 83/300, seasonal_3 Loss: 0.0728 | 0.0320
Epoch 84/300, seasonal_3 Loss: 0.0726 | 0.0319
Epoch 85/300, seasonal_3 Loss: 0.0725 | 0.0319
Epoch 86/300, seasonal_3 Loss: 0.0724 | 0.0318
Epoch 87/300, seasonal_3 Loss: 0.0723 | 0.0318
Epoch 88/300, seasonal_3 Loss: 0.0723 | 0.0317
Epoch 89/300, seasonal_3 Loss: 0.0722 | 0.0317
Epoch 90/300, seasonal_3 Loss: 0.0721 | 0.0317
Epoch 91/300, seasonal_3 Loss: 0.0720 | 0.0316
Epoch 92/300, seasonal_3 Loss: 0.0719 | 0.0316
Epoch 93/300, seasonal_3 Loss: 0.0719 | 0.0316
Epoch 94/300, seasonal_3 Loss: 0.0718 | 0.0315
Epoch 95/300, seasonal_3 Loss: 0.0718 | 0.0315
Epoch 96/300, seasonal_3 Loss: 0.0717 | 0.0315
Epoch 97/300, seasonal_3 Loss: 0.0717 | 0.0314
Epoch 98/300, seasonal_3 Loss: 0.0716 | 0.0314
Epoch 99/300, seasonal_3 Loss: 0.0716 | 0.0314
Epoch 100/300, seasonal_3 Loss: 0.0715 | 0.0314
Epoch 101/300, seasonal_3 Loss: 0.0715 | 0.0313
Epoch 102/300, seasonal_3 Loss: 0.0714 | 0.0313
Epoch 103/300, seasonal_3 Loss: 0.0714 | 0.0313
Epoch 104/300, seasonal_3 Loss: 0.0713 | 0.0313
Epoch 105/300, seasonal_3 Loss: 0.0713 | 0.0312
Epoch 106/300, seasonal_3 Loss: 0.0713 | 0.0312
Epoch 107/300, seasonal_3 Loss: 0.0712 | 0.0312
Epoch 108/300, seasonal_3 Loss: 0.0712 | 0.0312
Epoch 109/300, seasonal_3 Loss: 0.0712 | 0.0311
Epoch 110/300, seasonal_3 Loss: 0.0711 | 0.0311
Epoch 111/300, seasonal_3 Loss: 0.0711 | 0.0311
Epoch 112/300, seasonal_3 Loss: 0.0711 | 0.0311
Epoch 113/300, seasonal_3 Loss: 0.0711 | 0.0311
Epoch 114/300, seasonal_3 Loss: 0.0710 | 0.0311
Epoch 115/300, seasonal_3 Loss: 0.0710 | 0.0310
Epoch 116/300, seasonal_3 Loss: 0.0710 | 0.0310
Epoch 117/300, seasonal_3 Loss: 0.0710 | 0.0310
Epoch 118/300, seasonal_3 Loss: 0.0709 | 0.0310
Epoch 119/300, seasonal_3 Loss: 0.0709 | 0.0310
Epoch 120/300, seasonal_3 Loss: 0.0709 | 0.0310
Epoch 121/300, seasonal_3 Loss: 0.0709 | 0.0309
Epoch 122/300, seasonal_3 Loss: 0.0709 | 0.0309
Epoch 123/300, seasonal_3 Loss: 0.0708 | 0.0309
Epoch 124/300, seasonal_3 Loss: 0.0708 | 0.0309
Epoch 125/300, seasonal_3 Loss: 0.0708 | 0.0309
Epoch 126/300, seasonal_3 Loss: 0.0708 | 0.0309
Epoch 127/300, seasonal_3 Loss: 0.0708 | 0.0309
Epoch 128/300, seasonal_3 Loss: 0.0707 | 0.0309
Epoch 129/300, seasonal_3 Loss: 0.0707 | 0.0308
Epoch 130/300, seasonal_3 Loss: 0.0707 | 0.0308
Epoch 131/300, seasonal_3 Loss: 0.0707 | 0.0308
Epoch 132/300, seasonal_3 Loss: 0.0707 | 0.0308
Epoch 133/300, seasonal_3 Loss: 0.0707 | 0.0308
Epoch 134/300, seasonal_3 Loss: 0.0707 | 0.0308
Epoch 135/300, seasonal_3 Loss: 0.0706 | 0.0308
Epoch 136/300, seasonal_3 Loss: 0.0706 | 0.0308
Epoch 137/300, seasonal_3 Loss: 0.0706 | 0.0308
Epoch 138/300, seasonal_3 Loss: 0.0706 | 0.0308
Epoch 139/300, seasonal_3 Loss: 0.0706 | 0.0307
Epoch 140/300, seasonal_3 Loss: 0.0706 | 0.0307
Epoch 141/300, seasonal_3 Loss: 0.0706 | 0.0307
Epoch 142/300, seasonal_3 Loss: 0.0706 | 0.0307
Epoch 143/300, seasonal_3 Loss: 0.0706 | 0.0307
Epoch 144/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 145/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 146/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 147/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 148/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 149/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 150/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 151/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 152/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 153/300, seasonal_3 Loss: 0.0705 | 0.0307
Epoch 154/300, seasonal_3 Loss: 0.0705 | 0.0306
Epoch 155/300, seasonal_3 Loss: 0.0705 | 0.0306
Epoch 156/300, seasonal_3 Loss: 0.0705 | 0.0306
Epoch 157/300, seasonal_3 Loss: 0.0705 | 0.0306
Epoch 158/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 159/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 160/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 161/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 162/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 163/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 164/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 165/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 166/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 167/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 168/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 169/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 170/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 171/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 172/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 173/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 174/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 175/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 176/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 177/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 178/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 179/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 180/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 181/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 182/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 183/300, seasonal_3 Loss: 0.0704 | 0.0306
Epoch 184/300, seasonal_3 Loss: 0.0703 | 0.0306
Epoch 185/300, seasonal_3 Loss: 0.0703 | 0.0306
Epoch 186/300, seasonal_3 Loss: 0.0703 | 0.0306
Epoch 187/300, seasonal_3 Loss: 0.0703 | 0.0306
Epoch 188/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 189/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 190/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 191/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 192/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 193/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 194/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 195/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 196/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 197/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 198/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 199/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 200/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 201/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 202/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 203/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 204/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 205/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 206/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 207/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 208/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 209/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 210/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 211/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 212/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 213/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 214/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 215/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 216/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 217/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 218/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 219/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 220/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 221/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 222/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 223/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 224/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 225/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 226/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 227/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 228/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 229/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 230/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 231/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 232/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 233/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 234/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 235/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 236/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 237/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 238/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 239/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 240/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 241/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 242/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 243/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 244/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 245/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 246/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 247/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 248/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 249/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 250/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 251/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 252/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 253/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 254/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 255/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 256/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 257/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 258/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 259/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 260/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 261/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 262/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 263/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 264/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 265/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 266/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 267/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 268/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 269/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 270/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 271/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 272/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 273/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 274/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 275/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 276/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 277/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 278/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 279/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 280/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 281/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 282/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 283/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 284/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 285/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 286/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 287/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 288/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 289/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 290/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 291/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 292/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 293/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 294/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 295/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 296/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 297/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 298/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 299/300, seasonal_3 Loss: 0.0703 | 0.0305
Epoch 300/300, seasonal_3 Loss: 0.0703 | 0.0305
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.9317924053095814, 'learning_rate': 6.298769229180126e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.7571794478614324}
Epoch 1/300, resid Loss: 0.1726 | 0.0956
Epoch 2/300, resid Loss: 0.1243 | 0.0742
Epoch 3/300, resid Loss: 0.1144 | 0.0666
Epoch 4/300, resid Loss: 0.1098 | 0.0623
Epoch 5/300, resid Loss: 0.1062 | 0.0588
Epoch 6/300, resid Loss: 0.1029 | 0.0554
Epoch 7/300, resid Loss: 0.0988 | 0.0519
Epoch 8/300, resid Loss: 0.0953 | 0.0492
Epoch 9/300, resid Loss: 0.0922 | 0.0465
Epoch 10/300, resid Loss: 0.0897 | 0.0443
Epoch 11/300, resid Loss: 0.0876 | 0.0426
Epoch 12/300, resid Loss: 0.0859 | 0.0413
Epoch 13/300, resid Loss: 0.0841 | 0.0426
Epoch 14/300, resid Loss: 0.0826 | 0.0413
Epoch 15/300, resid Loss: 0.0815 | 0.0402
Epoch 16/300, resid Loss: 0.0806 | 0.0392
Epoch 17/300, resid Loss: 0.0798 | 0.0384
Epoch 18/300, resid Loss: 0.0789 | 0.0377
Epoch 19/300, resid Loss: 0.0780 | 0.0385
Epoch 20/300, resid Loss: 0.0771 | 0.0379
Epoch 21/300, resid Loss: 0.0765 | 0.0373
Epoch 22/300, resid Loss: 0.0759 | 0.0367
Epoch 23/300, resid Loss: 0.0753 | 0.0361
Epoch 24/300, resid Loss: 0.0747 | 0.0356
Epoch 25/300, resid Loss: 0.0740 | 0.0356
Epoch 26/300, resid Loss: 0.0735 | 0.0353
Epoch 27/300, resid Loss: 0.0731 | 0.0351
Epoch 28/300, resid Loss: 0.0727 | 0.0348
Epoch 29/300, resid Loss: 0.0723 | 0.0345
Epoch 30/300, resid Loss: 0.0719 | 0.0341
Epoch 31/300, resid Loss: 0.0713 | 0.0338
Epoch 32/300, resid Loss: 0.0711 | 0.0337
Epoch 33/300, resid Loss: 0.0708 | 0.0336
Epoch 34/300, resid Loss: 0.0705 | 0.0334
Epoch 35/300, resid Loss: 0.0702 | 0.0332
Epoch 36/300, resid Loss: 0.0699 | 0.0330
Epoch 37/300, resid Loss: 0.0694 | 0.0327
Epoch 38/300, resid Loss: 0.0692 | 0.0326
Epoch 39/300, resid Loss: 0.0690 | 0.0324
Epoch 40/300, resid Loss: 0.0688 | 0.0323
Epoch 41/300, resid Loss: 0.0686 | 0.0322
Epoch 42/300, resid Loss: 0.0684 | 0.0320
Epoch 43/300, resid Loss: 0.0681 | 0.0318
Epoch 44/300, resid Loss: 0.0680 | 0.0317
Epoch 45/300, resid Loss: 0.0678 | 0.0317
Epoch 46/300, resid Loss: 0.0677 | 0.0316
Epoch 47/300, resid Loss: 0.0675 | 0.0315
Epoch 48/300, resid Loss: 0.0674 | 0.0313
Epoch 49/300, resid Loss: 0.0672 | 0.0311
Epoch 50/300, resid Loss: 0.0671 | 0.0311
Epoch 51/300, resid Loss: 0.0670 | 0.0310
Epoch 52/300, resid Loss: 0.0669 | 0.0310
Epoch 53/300, resid Loss: 0.0667 | 0.0309
Epoch 54/300, resid Loss: 0.0666 | 0.0308
Epoch 55/300, resid Loss: 0.0665 | 0.0308
Epoch 56/300, resid Loss: 0.0664 | 0.0307
Epoch 57/300, resid Loss: 0.0663 | 0.0307
Epoch 58/300, resid Loss: 0.0662 | 0.0307
Epoch 59/300, resid Loss: 0.0662 | 0.0306
Epoch 60/300, resid Loss: 0.0661 | 0.0306
Epoch 61/300, resid Loss: 0.0660 | 0.0306
Epoch 62/300, resid Loss: 0.0659 | 0.0306
Epoch 63/300, resid Loss: 0.0658 | 0.0306
Epoch 64/300, resid Loss: 0.0657 | 0.0306
Epoch 65/300, resid Loss: 0.0657 | 0.0306
Epoch 66/300, resid Loss: 0.0656 | 0.0306
Epoch 67/300, resid Loss: 0.0655 | 0.0305
Epoch 68/300, resid Loss: 0.0655 | 0.0305
Epoch 69/300, resid Loss: 0.0654 | 0.0305
Epoch 70/300, resid Loss: 0.0654 | 0.0305
Epoch 71/300, resid Loss: 0.0653 | 0.0305
Epoch 72/300, resid Loss: 0.0652 | 0.0305
Epoch 73/300, resid Loss: 0.0652 | 0.0305
Epoch 74/300, resid Loss: 0.0651 | 0.0305
Epoch 75/300, resid Loss: 0.0651 | 0.0305
Epoch 76/300, resid Loss: 0.0651 | 0.0305
Epoch 77/300, resid Loss: 0.0650 | 0.0305
Epoch 78/300, resid Loss: 0.0650 | 0.0305
Epoch 79/300, resid Loss: 0.0649 | 0.0305
Epoch 80/300, resid Loss: 0.0649 | 0.0305
Epoch 81/300, resid Loss: 0.0649 | 0.0305
Epoch 82/300, resid Loss: 0.0648 | 0.0305
Epoch 83/300, resid Loss: 0.0648 | 0.0305
Epoch 84/300, resid Loss: 0.0648 | 0.0305
Epoch 85/300, resid Loss: 0.0647 | 0.0305
Epoch 86/300, resid Loss: 0.0647 | 0.0305
Epoch 87/300, resid Loss: 0.0647 | 0.0305
Epoch 88/300, resid Loss: 0.0647 | 0.0305
Epoch 89/300, resid Loss: 0.0647 | 0.0304
Epoch 90/300, resid Loss: 0.0646 | 0.0304
Epoch 91/300, resid Loss: 0.0646 | 0.0304
Epoch 92/300, resid Loss: 0.0646 | 0.0304
Epoch 93/300, resid Loss: 0.0646 | 0.0304
Epoch 94/300, resid Loss: 0.0646 | 0.0304
Epoch 95/300, resid Loss: 0.0645 | 0.0304
Epoch 96/300, resid Loss: 0.0645 | 0.0304
Epoch 97/300, resid Loss: 0.0645 | 0.0304
Epoch 98/300, resid Loss: 0.0645 | 0.0304
Epoch 99/300, resid Loss: 0.0645 | 0.0304
Epoch 100/300, resid Loss: 0.0645 | 0.0304
Epoch 101/300, resid Loss: 0.0644 | 0.0304
Epoch 102/300, resid Loss: 0.0644 | 0.0304
Epoch 103/300, resid Loss: 0.0644 | 0.0303
Epoch 104/300, resid Loss: 0.0644 | 0.0303
Epoch 105/300, resid Loss: 0.0644 | 0.0303
Epoch 106/300, resid Loss: 0.0644 | 0.0303
Epoch 107/300, resid Loss: 0.0644 | 0.0303
Epoch 108/300, resid Loss: 0.0644 | 0.0303
Epoch 109/300, resid Loss: 0.0643 | 0.0303
Epoch 110/300, resid Loss: 0.0643 | 0.0303
Epoch 111/300, resid Loss: 0.0643 | 0.0303
Epoch 112/300, resid Loss: 0.0643 | 0.0303
Epoch 113/300, resid Loss: 0.0643 | 0.0303
Epoch 114/300, resid Loss: 0.0643 | 0.0303
Epoch 115/300, resid Loss: 0.0643 | 0.0302
Epoch 116/300, resid Loss: 0.0643 | 0.0302
Epoch 117/300, resid Loss: 0.0643 | 0.0302
Epoch 118/300, resid Loss: 0.0643 | 0.0302
Epoch 119/300, resid Loss: 0.0643 | 0.0302
Epoch 120/300, resid Loss: 0.0643 | 0.0302
Epoch 121/300, resid Loss: 0.0643 | 0.0302
Epoch 122/300, resid Loss: 0.0642 | 0.0302
Epoch 123/300, resid Loss: 0.0642 | 0.0302
Epoch 124/300, resid Loss: 0.0642 | 0.0302
Epoch 125/300, resid Loss: 0.0642 | 0.0301
Epoch 126/300, resid Loss: 0.0642 | 0.0301
Epoch 127/300, resid Loss: 0.0642 | 0.0301
Epoch 128/300, resid Loss: 0.0642 | 0.0301
Epoch 129/300, resid Loss: 0.0642 | 0.0301
Epoch 130/300, resid Loss: 0.0642 | 0.0301
Epoch 131/300, resid Loss: 0.0642 | 0.0301
Epoch 132/300, resid Loss: 0.0642 | 0.0301
Epoch 133/300, resid Loss: 0.0642 | 0.0301
Epoch 134/300, resid Loss: 0.0642 | 0.0301
Epoch 135/300, resid Loss: 0.0642 | 0.0301
Epoch 136/300, resid Loss: 0.0642 | 0.0301
Epoch 137/300, resid Loss: 0.0642 | 0.0301
Epoch 138/300, resid Loss: 0.0642 | 0.0301
Epoch 139/300, resid Loss: 0.0642 | 0.0301
Epoch 140/300, resid Loss: 0.0642 | 0.0301
Epoch 141/300, resid Loss: 0.0642 | 0.0301
Epoch 142/300, resid Loss: 0.0642 | 0.0301
Epoch 143/300, resid Loss: 0.0642 | 0.0301
Epoch 144/300, resid Loss: 0.0642 | 0.0301
Epoch 145/300, resid Loss: 0.0642 | 0.0300
Epoch 146/300, resid Loss: 0.0642 | 0.0300
Epoch 147/300, resid Loss: 0.0642 | 0.0300
Epoch 148/300, resid Loss: 0.0642 | 0.0300
Epoch 149/300, resid Loss: 0.0642 | 0.0300
Epoch 150/300, resid Loss: 0.0642 | 0.0300
Epoch 151/300, resid Loss: 0.0642 | 0.0300
Epoch 152/300, resid Loss: 0.0642 | 0.0300
Epoch 153/300, resid Loss: 0.0642 | 0.0300
Epoch 154/300, resid Loss: 0.0642 | 0.0300
Epoch 155/300, resid Loss: 0.0642 | 0.0300
Epoch 156/300, resid Loss: 0.0642 | 0.0300
Epoch 157/300, resid Loss: 0.0642 | 0.0300
Epoch 158/300, resid Loss: 0.0642 | 0.0300
Epoch 159/300, resid Loss: 0.0642 | 0.0300
Epoch 160/300, resid Loss: 0.0642 | 0.0300
Epoch 161/300, resid Loss: 0.0642 | 0.0300
Epoch 162/300, resid Loss: 0.0642 | 0.0300
Epoch 163/300, resid Loss: 0.0642 | 0.0300
Epoch 164/300, resid Loss: 0.0642 | 0.0300
Epoch 165/300, resid Loss: 0.0642 | 0.0300
Epoch 166/300, resid Loss: 0.0642 | 0.0300
Epoch 167/300, resid Loss: 0.0642 | 0.0300
Epoch 168/300, resid Loss: 0.0642 | 0.0300
Epoch 169/300, resid Loss: 0.0641 | 0.0300
Epoch 170/300, resid Loss: 0.0641 | 0.0300
Epoch 171/300, resid Loss: 0.0641 | 0.0300
Epoch 172/300, resid Loss: 0.0641 | 0.0300
Epoch 173/300, resid Loss: 0.0641 | 0.0300
Epoch 174/300, resid Loss: 0.0641 | 0.0300
Epoch 175/300, resid Loss: 0.0641 | 0.0300
Epoch 176/300, resid Loss: 0.0641 | 0.0300
Epoch 177/300, resid Loss: 0.0641 | 0.0300
Epoch 178/300, resid Loss: 0.0641 | 0.0300
Epoch 179/300, resid Loss: 0.0641 | 0.0300
Epoch 180/300, resid Loss: 0.0641 | 0.0300
Epoch 181/300, resid Loss: 0.0641 | 0.0300
Epoch 182/300, resid Loss: 0.0641 | 0.0300
Epoch 183/300, resid Loss: 0.0641 | 0.0300
Epoch 184/300, resid Loss: 0.0641 | 0.0300
Epoch 185/300, resid Loss: 0.0641 | 0.0300
Epoch 186/300, resid Loss: 0.0641 | 0.0300
Epoch 187/300, resid Loss: 0.0641 | 0.0300
Epoch 188/300, resid Loss: 0.0641 | 0.0300
Epoch 189/300, resid Loss: 0.0641 | 0.0300
Epoch 190/300, resid Loss: 0.0641 | 0.0300
Epoch 191/300, resid Loss: 0.0641 | 0.0300
Epoch 192/300, resid Loss: 0.0641 | 0.0300
Epoch 193/300, resid Loss: 0.0641 | 0.0300
Epoch 194/300, resid Loss: 0.0641 | 0.0300
Epoch 195/300, resid Loss: 0.0641 | 0.0300
Epoch 196/300, resid Loss: 0.0641 | 0.0300
Epoch 197/300, resid Loss: 0.0641 | 0.0300
Epoch 198/300, resid Loss: 0.0641 | 0.0300
Epoch 199/300, resid Loss: 0.0641 | 0.0300
Epoch 200/300, resid Loss: 0.0641 | 0.0300
Epoch 201/300, resid Loss: 0.0641 | 0.0300
Epoch 202/300, resid Loss: 0.0641 | 0.0300
Epoch 203/300, resid Loss: 0.0641 | 0.0300
Epoch 204/300, resid Loss: 0.0641 | 0.0300
Epoch 205/300, resid Loss: 0.0641 | 0.0300
Epoch 206/300, resid Loss: 0.0641 | 0.0300
Epoch 207/300, resid Loss: 0.0641 | 0.0300
Epoch 208/300, resid Loss: 0.0641 | 0.0300
Epoch 209/300, resid Loss: 0.0641 | 0.0300
Epoch 210/300, resid Loss: 0.0641 | 0.0300
Epoch 211/300, resid Loss: 0.0641 | 0.0300
Epoch 212/300, resid Loss: 0.0641 | 0.0300
Epoch 213/300, resid Loss: 0.0641 | 0.0300
Epoch 214/300, resid Loss: 0.0641 | 0.0300
Epoch 215/300, resid Loss: 0.0641 | 0.0300
Epoch 216/300, resid Loss: 0.0641 | 0.0300
Epoch 217/300, resid Loss: 0.0641 | 0.0300
Epoch 218/300, resid Loss: 0.0641 | 0.0300
Epoch 219/300, resid Loss: 0.0641 | 0.0300
Epoch 220/300, resid Loss: 0.0641 | 0.0300
Epoch 221/300, resid Loss: 0.0641 | 0.0300
Epoch 222/300, resid Loss: 0.0641 | 0.0300
Epoch 223/300, resid Loss: 0.0641 | 0.0300
Epoch 224/300, resid Loss: 0.0641 | 0.0300
Epoch 225/300, resid Loss: 0.0641 | 0.0300
Epoch 226/300, resid Loss: 0.0641 | 0.0300
Epoch 227/300, resid Loss: 0.0641 | 0.0300
Epoch 228/300, resid Loss: 0.0641 | 0.0300
Epoch 229/300, resid Loss: 0.0641 | 0.0300
Epoch 230/300, resid Loss: 0.0641 | 0.0300
Epoch 231/300, resid Loss: 0.0641 | 0.0300
Epoch 232/300, resid Loss: 0.0641 | 0.0300
Epoch 233/300, resid Loss: 0.0641 | 0.0300
Epoch 234/300, resid Loss: 0.0641 | 0.0300
Epoch 235/300, resid Loss: 0.0641 | 0.0300
Epoch 236/300, resid Loss: 0.0641 | 0.0300
Epoch 237/300, resid Loss: 0.0641 | 0.0300
Epoch 238/300, resid Loss: 0.0641 | 0.0300
Epoch 239/300, resid Loss: 0.0641 | 0.0300
Epoch 240/300, resid Loss: 0.0641 | 0.0300
Epoch 241/300, resid Loss: 0.0641 | 0.0300
Epoch 242/300, resid Loss: 0.0641 | 0.0300
Epoch 243/300, resid Loss: 0.0641 | 0.0300
Epoch 244/300, resid Loss: 0.0641 | 0.0300
Epoch 245/300, resid Loss: 0.0641 | 0.0300
Epoch 246/300, resid Loss: 0.0641 | 0.0300
Epoch 247/300, resid Loss: 0.0641 | 0.0300
Epoch 248/300, resid Loss: 0.0641 | 0.0300
Epoch 249/300, resid Loss: 0.0641 | 0.0300
Epoch 250/300, resid Loss: 0.0641 | 0.0300
Epoch 251/300, resid Loss: 0.0641 | 0.0300
Epoch 252/300, resid Loss: 0.0641 | 0.0300
Epoch 253/300, resid Loss: 0.0641 | 0.0300
Epoch 254/300, resid Loss: 0.0641 | 0.0300
Epoch 255/300, resid Loss: 0.0641 | 0.0300
Epoch 256/300, resid Loss: 0.0641 | 0.0300
Epoch 257/300, resid Loss: 0.0641 | 0.0300
Epoch 258/300, resid Loss: 0.0641 | 0.0300
Epoch 259/300, resid Loss: 0.0641 | 0.0300
Epoch 260/300, resid Loss: 0.0641 | 0.0300
Epoch 261/300, resid Loss: 0.0641 | 0.0300
Epoch 262/300, resid Loss: 0.0641 | 0.0300
Epoch 263/300, resid Loss: 0.0641 | 0.0300
Epoch 264/300, resid Loss: 0.0641 | 0.0300
Epoch 265/300, resid Loss: 0.0641 | 0.0300
Epoch 266/300, resid Loss: 0.0641 | 0.0300
Epoch 267/300, resid Loss: 0.0641 | 0.0300
Epoch 268/300, resid Loss: 0.0641 | 0.0300
Epoch 269/300, resid Loss: 0.0641 | 0.0300
Epoch 270/300, resid Loss: 0.0641 | 0.0300
Epoch 271/300, resid Loss: 0.0641 | 0.0300
Epoch 272/300, resid Loss: 0.0641 | 0.0300
Epoch 273/300, resid Loss: 0.0641 | 0.0300
Epoch 274/300, resid Loss: 0.0641 | 0.0300
Epoch 275/300, resid Loss: 0.0641 | 0.0300
Epoch 276/300, resid Loss: 0.0641 | 0.0300
Epoch 277/300, resid Loss: 0.0641 | 0.0300
Epoch 278/300, resid Loss: 0.0641 | 0.0300
Epoch 279/300, resid Loss: 0.0641 | 0.0300
Epoch 280/300, resid Loss: 0.0641 | 0.0300
Epoch 281/300, resid Loss: 0.0641 | 0.0300
Epoch 282/300, resid Loss: 0.0641 | 0.0300
Epoch 283/300, resid Loss: 0.0641 | 0.0300
Epoch 284/300, resid Loss: 0.0641 | 0.0300
Epoch 285/300, resid Loss: 0.0641 | 0.0300
Epoch 286/300, resid Loss: 0.0641 | 0.0300
Epoch 287/300, resid Loss: 0.0641 | 0.0300
Epoch 288/300, resid Loss: 0.0641 | 0.0300
Epoch 289/300, resid Loss: 0.0641 | 0.0300
Epoch 290/300, resid Loss: 0.0641 | 0.0300
Epoch 291/300, resid Loss: 0.0641 | 0.0300
Epoch 292/300, resid Loss: 0.0641 | 0.0300
Epoch 293/300, resid Loss: 0.0641 | 0.0300
Epoch 294/300, resid Loss: 0.0641 | 0.0300
Epoch 295/300, resid Loss: 0.0641 | 0.0300
Epoch 296/300, resid Loss: 0.0641 | 0.0300
Epoch 297/300, resid Loss: 0.0641 | 0.0300
Epoch 298/300, resid Loss: 0.0641 | 0.0300
Epoch 299/300, resid Loss: 0.0641 | 0.0300
Epoch 300/300, resid Loss: 0.0641 | 0.0300
Runtime (seconds): 2694.3724541664124
0.0006910777661350586
[158.90607]
[-3.5657325]
[2.8040767]
[15.841035]
[2.6193788]
[21.952438]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 8.367890977300704
RMSE: 2.892730712890625
MAE: 2.892730712890625
R-squared: nan
[198.55727]
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/roop_optuna.py", line 735, in <module>
    plt.plot(predicted_dates, close_data[-output_date:-1].values, color='black', label='Learning Data')
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/pyplot.py", line 3794, in plot
    return gca().plot(
           ^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/axes/_axes.py", line 1779, in plot
    lines = [*self._get_lines(self, *args, data=data, **kwargs)]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/axes/_base.py", line 296, in __call__
    yield from self._plot_args(
               ^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/matplotlib/axes/_base.py", line 486, in _plot_args
    raise ValueError(f"x and y must have same first dimension, but "
ValueError: x and y must have same first dimension, but have shapes (10,) and (9,)
