ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-06 19:37:04,979][0m A new study created in memory with name: no-name-c3afba05-38f3-462e-9040-f91b6670f84f[0m
[32m[I 2025-01-06 19:38:14,124][0m Trial 0 finished with value: 0.2904743584951653 and parameters: {'observation_period_num': 34, 'train_rates': 0.6320694909445131, 'learning_rate': 3.431207170562543e-06, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9496706562970926}. Best is trial 0 with value: 0.2904743584951653.[0m
[32m[I 2025-01-06 19:38:37,992][0m Trial 1 finished with value: 0.48036117931787525 and parameters: {'observation_period_num': 116, 'train_rates': 0.626073046450712, 'learning_rate': 7.48506198459513e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8905758349557183}. Best is trial 0 with value: 0.2904743584951653.[0m
[32m[I 2025-01-06 19:39:16,258][0m Trial 2 finished with value: 0.4980621486902237 and parameters: {'observation_period_num': 6, 'train_rates': 0.6328763992671257, 'learning_rate': 1.2139229910006425e-06, 'batch_size': 130, 'step_size': 6, 'gamma': 0.9106790041537425}. Best is trial 0 with value: 0.2904743584951653.[0m
Early stopping at epoch 89
[32m[I 2025-01-06 19:39:56,764][0m Trial 3 finished with value: 0.8023458654816086 and parameters: {'observation_period_num': 166, 'train_rates': 0.7345752000825181, 'learning_rate': 1.2557730581625643e-05, 'batch_size': 108, 'step_size': 1, 'gamma': 0.8800594078307657}. Best is trial 0 with value: 0.2904743584951653.[0m
[32m[I 2025-01-06 19:40:41,826][0m Trial 4 finished with value: 0.17207950455293472 and parameters: {'observation_period_num': 251, 'train_rates': 0.6527847206690516, 'learning_rate': 0.0006631287985917654, 'batch_size': 102, 'step_size': 14, 'gamma': 0.7633438099152337}. Best is trial 4 with value: 0.17207950455293472.[0m
[32m[I 2025-01-06 19:43:27,359][0m Trial 5 finished with value: 0.0738472256581394 and parameters: {'observation_period_num': 60, 'train_rates': 0.983291763927188, 'learning_rate': 0.000113509207247179, 'batch_size': 36, 'step_size': 8, 'gamma': 0.979880867364226}. Best is trial 5 with value: 0.0738472256581394.[0m
[32m[I 2025-01-06 19:43:57,703][0m Trial 6 finished with value: 0.049073709079281616 and parameters: {'observation_period_num': 29, 'train_rates': 0.653784387912479, 'learning_rate': 0.0008295431335510668, 'batch_size': 212, 'step_size': 2, 'gamma': 0.9566176156815832}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:44:52,978][0m Trial 7 finished with value: 0.3187835614940532 and parameters: {'observation_period_num': 65, 'train_rates': 0.738743778204348, 'learning_rate': 7.650465840636942e-06, 'batch_size': 96, 'step_size': 9, 'gamma': 0.8386474757059474}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:47:39,761][0m Trial 8 finished with value: 0.31941681722646315 and parameters: {'observation_period_num': 189, 'train_rates': 0.7893679568175523, 'learning_rate': 0.00047951648793369067, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9780957007751093}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:49:11,754][0m Trial 9 finished with value: 0.0627735756122722 and parameters: {'observation_period_num': 25, 'train_rates': 0.7158459416124007, 'learning_rate': 3.5263257650175864e-05, 'batch_size': 53, 'step_size': 9, 'gamma': 0.7512491276626029}. Best is trial 6 with value: 0.049073709079281616.[0m
Early stopping at epoch 67
[32m[I 2025-01-06 19:49:43,669][0m Trial 10 finished with value: 0.3360421831308589 and parameters: {'observation_period_num': 103, 'train_rates': 0.9136039215433465, 'learning_rate': 0.0002185568731122377, 'batch_size': 204, 'step_size': 1, 'gamma': 0.8260018576822494}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:50:14,352][0m Trial 11 finished with value: 0.11296827105570187 and parameters: {'observation_period_num': 30, 'train_rates': 0.718197178827968, 'learning_rate': 3.948370482350425e-05, 'batch_size': 186, 'step_size': 10, 'gamma': 0.7731432721165422}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:50:49,004][0m Trial 12 finished with value: 0.05955459919401757 and parameters: {'observation_period_num': 89, 'train_rates': 0.8347436699810071, 'learning_rate': 0.0009699996112504602, 'batch_size': 168, 'step_size': 4, 'gamma': 0.8083385472651233}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:51:21,273][0m Trial 13 finished with value: 0.05118363821960014 and parameters: {'observation_period_num': 77, 'train_rates': 0.8401893634118072, 'learning_rate': 0.0007281233523827084, 'batch_size': 174, 'step_size': 4, 'gamma': 0.8078785195047604}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:51:51,208][0m Trial 14 finished with value: 0.11305710608211458 and parameters: {'observation_period_num': 147, 'train_rates': 0.8615062899954541, 'learning_rate': 0.0002308340997166038, 'batch_size': 236, 'step_size': 3, 'gamma': 0.9234972328713362}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:52:29,425][0m Trial 15 finished with value: 0.05554217729451401 and parameters: {'observation_period_num': 67, 'train_rates': 0.9029218435732489, 'learning_rate': 0.00042968872345946503, 'batch_size': 159, 'step_size': 6, 'gamma': 0.8553054633000018}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:53:00,772][0m Trial 16 finished with value: 0.16801246295909625 and parameters: {'observation_period_num': 81, 'train_rates': 0.7937747854058776, 'learning_rate': 0.0001835215628349736, 'batch_size': 212, 'step_size': 3, 'gamma': 0.8069060312892339}. Best is trial 6 with value: 0.049073709079281616.[0m
[32m[I 2025-01-06 19:53:36,276][0m Trial 17 finished with value: 0.025827567209976737 and parameters: {'observation_period_num': 5, 'train_rates': 0.6756092891150309, 'learning_rate': 0.0009658127962999963, 'batch_size': 145, 'step_size': 6, 'gamma': 0.9390214067223628}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:54:14,628][0m Trial 18 finished with value: 0.05462712220985131 and parameters: {'observation_period_num': 7, 'train_rates': 0.6872013074581615, 'learning_rate': 8.410544159855622e-05, 'batch_size': 139, 'step_size': 6, 'gamma': 0.9445630953153499}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:54:45,063][0m Trial 19 finished with value: 0.10717513367572523 and parameters: {'observation_period_num': 46, 'train_rates': 0.6038020001773761, 'learning_rate': 0.00032385838661890546, 'batch_size': 222, 'step_size': 2, 'gamma': 0.9484323090957022}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:55:21,806][0m Trial 20 finished with value: 0.46868562361671356 and parameters: {'observation_period_num': 226, 'train_rates': 0.6719614296460404, 'learning_rate': 1.0894807413068385e-05, 'batch_size': 143, 'step_size': 12, 'gamma': 0.9098692931286753}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:55:59,856][0m Trial 21 finished with value: 0.05219239592552185 and parameters: {'observation_period_num': 46, 'train_rates': 0.7625338542334632, 'learning_rate': 0.0008811251596536645, 'batch_size': 184, 'step_size': 5, 'gamma': 0.9895436617993739}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:56:33,402][0m Trial 22 finished with value: 0.03538941454929792 and parameters: {'observation_period_num': 14, 'train_rates': 0.8325561351413783, 'learning_rate': 0.0004766899181380155, 'batch_size': 194, 'step_size': 3, 'gamma': 0.9623467415149184}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:57:04,245][0m Trial 23 finished with value: 0.037053481337573864 and parameters: {'observation_period_num': 5, 'train_rates': 0.6877993550597631, 'learning_rate': 0.00035849940567613393, 'batch_size': 199, 'step_size': 2, 'gamma': 0.9628275113471404}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:57:34,604][0m Trial 24 finished with value: 0.06481792296093788 and parameters: {'observation_period_num': 18, 'train_rates': 0.692903026065025, 'learning_rate': 0.00014823253539572302, 'batch_size': 189, 'step_size': 7, 'gamma': 0.9297630388605915}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:58:07,236][0m Trial 25 finished with value: 0.08618830557439082 and parameters: {'observation_period_num': 52, 'train_rates': 0.7747734657812679, 'learning_rate': 0.0003813439284584487, 'batch_size': 244, 'step_size': 3, 'gamma': 0.9666757723890838}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:58:50,986][0m Trial 26 finished with value: 0.04668460823316476 and parameters: {'observation_period_num': 9, 'train_rates': 0.8187851419189163, 'learning_rate': 5.886829680120914e-05, 'batch_size': 149, 'step_size': 5, 'gamma': 0.8997456255754959}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 19:59:37,964][0m Trial 27 finished with value: 0.06679907077550888 and parameters: {'observation_period_num': 138, 'train_rates': 0.8667694379024043, 'learning_rate': 0.00028456919110445293, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9312719309353552}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:00:13,397][0m Trial 28 finished with value: 0.12059516730633649 and parameters: {'observation_period_num': 39, 'train_rates': 0.9170659510557582, 'learning_rate': 0.0004586163702155063, 'batch_size': 198, 'step_size': 5, 'gamma': 0.9697219726986094}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:00:39,752][0m Trial 29 finished with value: 0.11693984924261923 and parameters: {'observation_period_num': 29, 'train_rates': 0.6016599919074389, 'learning_rate': 0.00014303640788623887, 'batch_size': 226, 'step_size': 7, 'gamma': 0.9564183005977199}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:01:14,048][0m Trial 30 finished with value: 0.4549507100165375 and parameters: {'observation_period_num': 94, 'train_rates': 0.7420162436921957, 'learning_rate': 1.9653892143007408e-05, 'batch_size': 159, 'step_size': 1, 'gamma': 0.9375030432097456}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:02:26,288][0m Trial 31 finished with value: 0.03896224407637143 and parameters: {'observation_period_num': 10, 'train_rates': 0.8276554348925075, 'learning_rate': 5.0539358407916666e-05, 'batch_size': 76, 'step_size': 5, 'gamma': 0.9007558602205246}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:03:34,768][0m Trial 32 finished with value: 0.21674971790104122 and parameters: {'observation_period_num': 6, 'train_rates': 0.8147839892850125, 'learning_rate': 4.319071484380961e-06, 'batch_size': 79, 'step_size': 4, 'gamma': 0.8668789357736928}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:04:41,562][0m Trial 33 finished with value: 0.03355057520228763 and parameters: {'observation_period_num': 20, 'train_rates': 0.8829700926295481, 'learning_rate': 0.000554683768370857, 'batch_size': 85, 'step_size': 3, 'gamma': 0.9175326180106195}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:05:10,839][0m Trial 34 finished with value: 0.05869495868682861 and parameters: {'observation_period_num': 37, 'train_rates': 0.9460834194104881, 'learning_rate': 0.0006331819330774337, 'batch_size': 254, 'step_size': 3, 'gamma': 0.9177505843937697}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:06:00,063][0m Trial 35 finished with value: 0.037814779501609705 and parameters: {'observation_period_num': 24, 'train_rates': 0.873681692463801, 'learning_rate': 0.0005070071285233381, 'batch_size': 121, 'step_size': 2, 'gamma': 0.8848896141140999}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:06:37,550][0m Trial 36 finished with value: 0.6080597742911308 and parameters: {'observation_period_num': 55, 'train_rates': 0.6254081109870842, 'learning_rate': 1.8085350392303947e-06, 'batch_size': 126, 'step_size': 1, 'gamma': 0.9658825895887332}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:08:34,198][0m Trial 37 finished with value: 0.033183755456573434 and parameters: {'observation_period_num': 18, 'train_rates': 0.9509380154187081, 'learning_rate': 0.00010607729892601021, 'batch_size': 50, 'step_size': 4, 'gamma': 0.9419755288657294}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:10:16,784][0m Trial 38 finished with value: 0.0898863037719446 and parameters: {'observation_period_num': 114, 'train_rates': 0.9582509527842256, 'learning_rate': 9.056555564041527e-05, 'batch_size': 56, 'step_size': 6, 'gamma': 0.9410450129138308}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:12:32,049][0m Trial 39 finished with value: 0.04011065957285924 and parameters: {'observation_period_num': 20, 'train_rates': 0.8895886632371006, 'learning_rate': 2.3407692627053735e-05, 'batch_size': 41, 'step_size': 8, 'gamma': 0.9889999178187643}. Best is trial 17 with value: 0.025827567209976737.[0m
[32m[I 2025-01-06 20:13:50,155][0m Trial 40 finished with value: 0.019508425146341324 and parameters: {'observation_period_num': 41, 'train_rates': 0.9886700245714038, 'learning_rate': 0.0006447926509531084, 'batch_size': 79, 'step_size': 4, 'gamma': 0.902993793912314}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:14:55,914][0m Trial 41 finished with value: 0.028475789353251457 and parameters: {'observation_period_num': 38, 'train_rates': 0.9748966445613396, 'learning_rate': 0.0006043096449198058, 'batch_size': 93, 'step_size': 4, 'gamma': 0.915539949392392}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:16:07,512][0m Trial 42 finished with value: 0.02841533161699772 and parameters: {'observation_period_num': 40, 'train_rates': 0.9867217867735941, 'learning_rate': 0.0009968810402991076, 'batch_size': 87, 'step_size': 4, 'gamma': 0.8963687457053411}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:17:13,533][0m Trial 43 finished with value: 0.08094502985477448 and parameters: {'observation_period_num': 69, 'train_rates': 0.9892386335117884, 'learning_rate': 0.0009465168228353139, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8965507796484926}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:18:43,729][0m Trial 44 finished with value: 0.041944710205176046 and parameters: {'observation_period_num': 38, 'train_rates': 0.9616623070101153, 'learning_rate': 0.0002604781955712518, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8714546809157938}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:19:40,818][0m Trial 45 finished with value: 0.11639098181174351 and parameters: {'observation_period_num': 186, 'train_rates': 0.929730089587456, 'learning_rate': 0.0006508135595645467, 'batch_size': 106, 'step_size': 5, 'gamma': 0.9083286490647557}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:23:20,730][0m Trial 46 finished with value: 0.06274283648683475 and parameters: {'observation_period_num': 58, 'train_rates': 0.9416316556429093, 'learning_rate': 0.0006965492476737779, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8803320976269511}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:25:30,678][0m Trial 47 finished with value: 0.03790931021431824 and parameters: {'observation_period_num': 47, 'train_rates': 0.973546557285253, 'learning_rate': 0.00015114863927516756, 'batch_size': 45, 'step_size': 4, 'gamma': 0.8913137462832889}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:27:02,475][0m Trial 48 finished with value: 0.0835543215967888 and parameters: {'observation_period_num': 76, 'train_rates': 0.970257654450985, 'learning_rate': 0.0008731091764157085, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9235465871031725}. Best is trial 40 with value: 0.019508425146341324.[0m
[32m[I 2025-01-06 20:27:58,440][0m Trial 49 finished with value: 0.034475214779376984 and parameters: {'observation_period_num': 32, 'train_rates': 0.9893687687588409, 'learning_rate': 0.0003167323902868341, 'batch_size': 112, 'step_size': 4, 'gamma': 0.8523041141167497}. Best is trial 40 with value: 0.019508425146341324.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-06 20:27:58,450][0m A new study created in memory with name: no-name-181189a3-b5d2-4d45-957b-eb267e053eb9[0m
[32m[I 2025-01-06 20:28:43,584][0m Trial 0 finished with value: 0.1296121204294354 and parameters: {'observation_period_num': 37, 'train_rates': 0.7723219118405736, 'learning_rate': 1.9736765563799748e-05, 'batch_size': 132, 'step_size': 13, 'gamma': 0.8813788813225931}. Best is trial 0 with value: 0.1296121204294354.[0m
[32m[I 2025-01-06 20:32:33,687][0m Trial 1 finished with value: 0.17203013896942138 and parameters: {'observation_period_num': 150, 'train_rates': 0.9571797120858409, 'learning_rate': 6.569376613913632e-06, 'batch_size': 24, 'step_size': 14, 'gamma': 0.8815891369558994}. Best is trial 0 with value: 0.1296121204294354.[0m
[32m[I 2025-01-06 20:33:45,926][0m Trial 2 finished with value: 0.11280465265735984 and parameters: {'observation_period_num': 202, 'train_rates': 0.8372244967862879, 'learning_rate': 0.0001695551124139283, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8485290400150602}. Best is trial 2 with value: 0.11280465265735984.[0m
[32m[I 2025-01-06 20:35:09,313][0m Trial 3 finished with value: 0.08554892755276902 and parameters: {'observation_period_num': 134, 'train_rates': 0.8038807399673622, 'learning_rate': 0.0005032005106486236, 'batch_size': 60, 'step_size': 3, 'gamma': 0.8978387572577344}. Best is trial 3 with value: 0.08554892755276902.[0m
Early stopping at epoch 86
[32m[I 2025-01-06 20:35:36,863][0m Trial 4 finished with value: 0.8731846832734649 and parameters: {'observation_period_num': 149, 'train_rates': 0.6904651474703798, 'learning_rate': 2.345915917635985e-05, 'batch_size': 174, 'step_size': 2, 'gamma': 0.761110792779622}. Best is trial 3 with value: 0.08554892755276902.[0m
[32m[I 2025-01-06 20:40:10,133][0m Trial 5 finished with value: 0.0826544436172415 and parameters: {'observation_period_num': 135, 'train_rates': 0.8893577131681771, 'learning_rate': 2.8785630524057058e-05, 'batch_size': 19, 'step_size': 13, 'gamma': 0.827320715377058}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:40:52,787][0m Trial 6 finished with value: 0.08891534389674204 and parameters: {'observation_period_num': 28, 'train_rates': 0.73385344903876, 'learning_rate': 0.0009896004139218295, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8405863669678186}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:43:07,082][0m Trial 7 finished with value: 0.17374358960382996 and parameters: {'observation_period_num': 173, 'train_rates': 0.7581765755113477, 'learning_rate': 0.000523913107702321, 'batch_size': 34, 'step_size': 9, 'gamma': 0.7513075646207458}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:43:50,655][0m Trial 8 finished with value: 0.21583534662061543 and parameters: {'observation_period_num': 155, 'train_rates': 0.6577082280439742, 'learning_rate': 7.18736746397573e-05, 'batch_size': 106, 'step_size': 4, 'gamma': 0.8984008418756975}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:44:25,788][0m Trial 9 finished with value: 0.4325970798329922 and parameters: {'observation_period_num': 109, 'train_rates': 0.6079724050628258, 'learning_rate': 7.205923575975454e-05, 'batch_size': 136, 'step_size': 9, 'gamma': 0.9412230078788326}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:45:03,007][0m Trial 10 finished with value: 0.593464732170105 and parameters: {'observation_period_num': 236, 'train_rates': 0.9148574750205333, 'learning_rate': 3.892761918023836e-06, 'batch_size': 241, 'step_size': 12, 'gamma': 0.7982784109615356}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:46:42,704][0m Trial 11 finished with value: 0.6027668752808415 and parameters: {'observation_period_num': 93, 'train_rates': 0.8611103963584458, 'learning_rate': 1.2235931913450865e-06, 'batch_size': 65, 'step_size': 1, 'gamma': 0.9885449725541426}. Best is trial 5 with value: 0.0826544436172415.[0m
[32m[I 2025-01-06 20:48:03,024][0m Trial 12 finished with value: 0.0630322843620723 and parameters: {'observation_period_num': 79, 'train_rates': 0.8468309182180096, 'learning_rate': 0.0001774449590735401, 'batch_size': 67, 'step_size': 11, 'gamma': 0.815690861429799}. Best is trial 12 with value: 0.0630322843620723.[0m
[32m[I 2025-01-06 20:53:10,960][0m Trial 13 finished with value: 0.07730575777449698 and parameters: {'observation_period_num': 72, 'train_rates': 0.9082268441365062, 'learning_rate': 0.00012676740655051316, 'batch_size': 17, 'step_size': 11, 'gamma': 0.806652212511015}. Best is trial 12 with value: 0.0630322843620723.[0m
[32m[I 2025-01-06 20:54:08,061][0m Trial 14 finished with value: 0.08623969584466919 and parameters: {'observation_period_num': 77, 'train_rates': 0.9562696756812399, 'learning_rate': 0.00015856185396041978, 'batch_size': 95, 'step_size': 11, 'gamma': 0.7915517584804684}. Best is trial 12 with value: 0.0630322843620723.[0m
[32m[I 2025-01-06 20:54:44,263][0m Trial 15 finished with value: 0.08764783732325908 and parameters: {'observation_period_num': 61, 'train_rates': 0.9133558950540763, 'learning_rate': 0.0001301953482152463, 'batch_size': 171, 'step_size': 15, 'gamma': 0.8153048367815625}. Best is trial 12 with value: 0.0630322843620723.[0m
[32m[I 2025-01-06 20:56:59,849][0m Trial 16 finished with value: 0.03112494759261608 and parameters: {'observation_period_num': 6, 'train_rates': 0.9877764415128893, 'learning_rate': 0.0002975341124022672, 'batch_size': 41, 'step_size': 11, 'gamma': 0.7847606671653146}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 20:58:09,371][0m Trial 17 finished with value: 0.035230252891778946 and parameters: {'observation_period_num': 17, 'train_rates': 0.9825387645445129, 'learning_rate': 0.00046223954804040405, 'batch_size': 84, 'step_size': 6, 'gamma': 0.7797418571609751}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 20:59:15,320][0m Trial 18 finished with value: 0.05249088257551193 and parameters: {'observation_period_num': 5, 'train_rates': 0.9895638506237793, 'learning_rate': 0.0004228808234306654, 'batch_size': 91, 'step_size': 6, 'gamma': 0.7717074282106637}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 20:59:53,977][0m Trial 19 finished with value: 0.05370142683386803 and parameters: {'observation_period_num': 10, 'train_rates': 0.9821514203115965, 'learning_rate': 0.0009195622550908349, 'batch_size': 167, 'step_size': 6, 'gamma': 0.7793842844663271}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:01:55,395][0m Trial 20 finished with value: 0.054606662274686836 and parameters: {'observation_period_num': 47, 'train_rates': 0.9439874444155526, 'learning_rate': 0.0003136304509324286, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8570926483455467}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:03:03,123][0m Trial 21 finished with value: 0.05505380406975746 and parameters: {'observation_period_num': 5, 'train_rates': 0.9723934491484459, 'learning_rate': 0.0003413645460883397, 'batch_size': 90, 'step_size': 7, 'gamma': 0.7651482632016597}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:04:23,779][0m Trial 22 finished with value: 0.0657651275396347 and parameters: {'observation_period_num': 21, 'train_rates': 0.9876557149781893, 'learning_rate': 5.799426945975901e-05, 'batch_size': 86, 'step_size': 9, 'gamma': 0.7751012995217924}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:06:17,936][0m Trial 23 finished with value: 0.05862790322367181 and parameters: {'observation_period_num': 45, 'train_rates': 0.9354898624512544, 'learning_rate': 0.000403615228602976, 'batch_size': 48, 'step_size': 7, 'gamma': 0.7859094515121504}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:06:58,892][0m Trial 24 finished with value: 0.0667810381310893 and parameters: {'observation_period_num': 22, 'train_rates': 0.8719134556157779, 'learning_rate': 0.0007071417697858131, 'batch_size': 147, 'step_size': 4, 'gamma': 0.7530887917803042}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:07:55,958][0m Trial 25 finished with value: 0.050824690610170364 and parameters: {'observation_period_num': 57, 'train_rates': 0.9878746843677627, 'learning_rate': 0.00026462089959641155, 'batch_size': 112, 'step_size': 10, 'gamma': 0.826552783868551}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:08:44,071][0m Trial 26 finished with value: 0.09735575318336487 and parameters: {'observation_period_num': 56, 'train_rates': 0.9351576720421515, 'learning_rate': 0.00029353380616197817, 'batch_size': 210, 'step_size': 10, 'gamma': 0.832350039643026}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:09:37,849][0m Trial 27 finished with value: 0.12822946577759112 and parameters: {'observation_period_num': 107, 'train_rates': 0.8963914588743568, 'learning_rate': 4.744884947450419e-05, 'batch_size': 115, 'step_size': 10, 'gamma': 0.7982484806802607}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:10:44,504][0m Trial 28 finished with value: 0.0633932696261876 and parameters: {'observation_period_num': 37, 'train_rates': 0.8051306414153818, 'learning_rate': 0.00023774907383023488, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8210643396166019}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:12:29,074][0m Trial 29 finished with value: 0.086490689667764 and parameters: {'observation_period_num': 31, 'train_rates': 0.9622632747030192, 'learning_rate': 1.3857549530009364e-05, 'batch_size': 52, 'step_size': 13, 'gamma': 0.8672722594169916}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:13:07,951][0m Trial 30 finished with value: 0.09560385722558475 and parameters: {'observation_period_num': 63, 'train_rates': 0.9286765841064301, 'learning_rate': 9.673746547911805e-05, 'batch_size': 150, 'step_size': 12, 'gamma': 0.7929994806314583}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:14:06,240][0m Trial 31 finished with value: 0.05559704825282097 and parameters: {'observation_period_num': 5, 'train_rates': 0.9869188515950909, 'learning_rate': 0.0006019491525026079, 'batch_size': 102, 'step_size': 6, 'gamma': 0.7715237231960274}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:15:19,631][0m Trial 32 finished with value: 0.07895929637280377 and parameters: {'observation_period_num': 23, 'train_rates': 0.9587424604871577, 'learning_rate': 0.00022601700484319593, 'batch_size': 81, 'step_size': 4, 'gamma': 0.7787075917101522}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:16:15,173][0m Trial 33 finished with value: 0.06002705916762352 and parameters: {'observation_period_num': 41, 'train_rates': 0.9613014992467264, 'learning_rate': 0.00038740579374004565, 'batch_size': 117, 'step_size': 7, 'gamma': 0.8043483654496727}. Best is trial 16 with value: 0.03112494759261608.[0m
[32m[I 2025-01-06 21:19:05,188][0m Trial 34 finished with value: 0.030840635299682617 and parameters: {'observation_period_num': 18, 'train_rates': 0.9898785635671847, 'learning_rate': 0.0006987153438608318, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8413606396265654}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:22:08,314][0m Trial 35 finished with value: 0.0868401576667794 and parameters: {'observation_period_num': 51, 'train_rates': 0.8214394434546438, 'learning_rate': 0.0007419616626850594, 'batch_size': 31, 'step_size': 5, 'gamma': 0.8483518889877278}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:24:50,505][0m Trial 36 finished with value: 0.09189041167410748 and parameters: {'observation_period_num': 15, 'train_rates': 0.8834652565992223, 'learning_rate': 1.584001004987188e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.8887296188644089}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:26:42,471][0m Trial 37 finished with value: 0.07299037218878143 and parameters: {'observation_period_num': 33, 'train_rates': 0.9415944507179164, 'learning_rate': 0.00021765744342159115, 'batch_size': 56, 'step_size': 8, 'gamma': 0.8650359601381372}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:28:12,801][0m Trial 38 finished with value: 0.07114845141768456 and parameters: {'observation_period_num': 92, 'train_rates': 0.9663542934745337, 'learning_rate': 0.0005970133113842695, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8369841011685754}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:29:07,372][0m Trial 39 finished with value: 0.07532120129971383 and parameters: {'observation_period_num': 21, 'train_rates': 0.744474001358281, 'learning_rate': 0.0009832294296162497, 'batch_size': 128, 'step_size': 3, 'gamma': 0.8768400110269242}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:31:44,250][0m Trial 40 finished with value: 0.1336532066611706 and parameters: {'observation_period_num': 194, 'train_rates': 0.7813430670580865, 'learning_rate': 4.463266996622465e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8511217096638909}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:32:49,471][0m Trial 41 finished with value: 0.044862598180770874 and parameters: {'observation_period_num': 13, 'train_rates': 0.9884428702687693, 'learning_rate': 0.00045657327626803833, 'batch_size': 105, 'step_size': 5, 'gamma': 0.767843791681094}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:33:49,905][0m Trial 42 finished with value: 0.06473727844273433 and parameters: {'observation_period_num': 36, 'train_rates': 0.9467054287013299, 'learning_rate': 0.0004991989059781795, 'batch_size': 107, 'step_size': 5, 'gamma': 0.7610730583528911}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:34:30,852][0m Trial 43 finished with value: 0.08636536009886892 and parameters: {'observation_period_num': 14, 'train_rates': 0.6981748616554861, 'learning_rate': 0.0002951634944619348, 'batch_size': 127, 'step_size': 3, 'gamma': 0.8256056062453048}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:36:01,406][0m Trial 44 finished with value: 0.09386914654781944 and parameters: {'observation_period_num': 250, 'train_rates': 0.9721535173126321, 'learning_rate': 0.0007225580700825899, 'batch_size': 60, 'step_size': 5, 'gamma': 0.8113603300221234}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:36:49,978][0m Trial 45 finished with value: 0.06416356428823572 and parameters: {'observation_period_num': 31, 'train_rates': 0.9198607682845891, 'learning_rate': 0.00010114521967720752, 'batch_size': 145, 'step_size': 7, 'gamma': 0.9225148608866062}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:39:20,600][0m Trial 46 finished with value: 0.05177253484725952 and parameters: {'observation_period_num': 51, 'train_rates': 0.9883373004329077, 'learning_rate': 0.0004959805865503203, 'batch_size': 40, 'step_size': 12, 'gamma': 0.7845097075551098}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:40:31,312][0m Trial 47 finished with value: 0.5307654654770567 and parameters: {'observation_period_num': 68, 'train_rates': 0.9011815364230451, 'learning_rate': 7.007002019001862e-06, 'batch_size': 98, 'step_size': 4, 'gamma': 0.7512574259614049}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:44:08,067][0m Trial 48 finished with value: 0.14165703367591415 and parameters: {'observation_period_num': 18, 'train_rates': 0.6008820399050603, 'learning_rate': 0.00016091231190457525, 'batch_size': 21, 'step_size': 8, 'gamma': 0.8035145092341223}. Best is trial 34 with value: 0.030840635299682617.[0m
[32m[I 2025-01-06 21:45:48,362][0m Trial 49 finished with value: 0.07782675320802092 and parameters: {'observation_period_num': 117, 'train_rates': 0.9483906711241575, 'learning_rate': 0.00019405219214646145, 'batch_size': 65, 'step_size': 6, 'gamma': 0.8429675340721764}. Best is trial 34 with value: 0.030840635299682617.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-06 21:45:48,372][0m A new study created in memory with name: no-name-cc9dd6f2-73dc-49f2-bc51-22ea80f553be[0m
[32m[I 2025-01-06 21:46:36,570][0m Trial 0 finished with value: 0.1721745878458023 and parameters: {'observation_period_num': 209, 'train_rates': 0.9349488045032673, 'learning_rate': 3.198868621020015e-05, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9387336513870085}. Best is trial 0 with value: 0.1721745878458023.[0m
[32m[I 2025-01-06 21:47:27,449][0m Trial 1 finished with value: 0.12964942187224468 and parameters: {'observation_period_num': 15, 'train_rates': 0.8018132039068786, 'learning_rate': 1.926855471191283e-05, 'batch_size': 131, 'step_size': 8, 'gamma': 0.7547142849097297}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 21:51:22,151][0m Trial 2 finished with value: 0.40823574234610016 and parameters: {'observation_period_num': 214, 'train_rates': 0.8238011493495916, 'learning_rate': 4.8075828702555e-06, 'batch_size': 21, 'step_size': 2, 'gamma': 0.8565244649571088}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 21:51:56,199][0m Trial 3 finished with value: 0.26760975736560244 and parameters: {'observation_period_num': 124, 'train_rates': 0.7668386409677075, 'learning_rate': 2.2775378402024126e-05, 'batch_size': 248, 'step_size': 7, 'gamma': 0.8879012662735}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 21:53:30,192][0m Trial 4 finished with value: 0.17297598315155968 and parameters: {'observation_period_num': 129, 'train_rates': 0.6072746390692203, 'learning_rate': 3.153665192797242e-05, 'batch_size': 52, 'step_size': 6, 'gamma': 0.9386921170509681}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 21:55:11,141][0m Trial 5 finished with value: 0.48872063110156844 and parameters: {'observation_period_num': 184, 'train_rates': 0.8442989894978359, 'learning_rate': 1.7680909234262817e-06, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8629585834189839}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 21:56:37,233][0m Trial 6 finished with value: 0.366416833396303 and parameters: {'observation_period_num': 197, 'train_rates': 0.804330702739964, 'learning_rate': 6.319880546775916e-06, 'batch_size': 64, 'step_size': 10, 'gamma': 0.7834772459210273}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 21:57:19,201][0m Trial 7 finished with value: 0.8238515143084972 and parameters: {'observation_period_num': 100, 'train_rates': 0.681367093619257, 'learning_rate': 2.404431004891876e-06, 'batch_size': 164, 'step_size': 14, 'gamma': 0.8090403393894982}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 22:01:51,569][0m Trial 8 finished with value: 0.38789254426956177 and parameters: {'observation_period_num': 211, 'train_rates': 0.9108486623260044, 'learning_rate': 2.474915184833481e-06, 'batch_size': 20, 'step_size': 5, 'gamma': 0.850525114323206}. Best is trial 1 with value: 0.12964942187224468.[0m
[32m[I 2025-01-06 22:05:52,051][0m Trial 9 finished with value: 0.3157859980420016 and parameters: {'observation_period_num': 89, 'train_rates': 0.6257991430312863, 'learning_rate': 3.6969835670790237e-06, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8543506956075044}. Best is trial 1 with value: 0.12964942187224468.[0m
Early stopping at epoch 46
[32m[I 2025-01-06 22:06:31,762][0m Trial 10 finished with value: 0.10951190441846848 and parameters: {'observation_period_num': 17, 'train_rates': 0.9882851491592005, 'learning_rate': 0.0005441562262368311, 'batch_size': 118, 'step_size': 1, 'gamma': 0.7503304772180478}. Best is trial 10 with value: 0.10951190441846848.[0m
Early stopping at epoch 51
[32m[I 2025-01-06 22:07:07,010][0m Trial 11 finished with value: 0.06526949256658554 and parameters: {'observation_period_num': 17, 'train_rates': 0.9814711132480758, 'learning_rate': 0.0009489642735919253, 'batch_size': 114, 'step_size': 1, 'gamma': 0.7508694093044747}. Best is trial 11 with value: 0.06526949256658554.[0m
Early stopping at epoch 49
[32m[I 2025-01-06 22:07:39,001][0m Trial 12 finished with value: 0.06730081886053085 and parameters: {'observation_period_num': 9, 'train_rates': 0.9796061634341374, 'learning_rate': 0.0009052819209864922, 'batch_size': 113, 'step_size': 1, 'gamma': 0.7545243012089786}. Best is trial 11 with value: 0.06526949256658554.[0m
[32m[I 2025-01-06 22:08:44,263][0m Trial 13 finished with value: 0.0459422804415226 and parameters: {'observation_period_num': 55, 'train_rates': 0.9896798177454738, 'learning_rate': 0.0008926044115439998, 'batch_size': 99, 'step_size': 3, 'gamma': 0.8066817065163784}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:09:58,208][0m Trial 14 finished with value: 0.04908938786270572 and parameters: {'observation_period_num': 58, 'train_rates': 0.8944388449071476, 'learning_rate': 0.00025423059316878106, 'batch_size': 84, 'step_size': 4, 'gamma': 0.809776544304936}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:11:21,675][0m Trial 15 finished with value: 0.04899855611452169 and parameters: {'observation_period_num': 60, 'train_rates': 0.8905650426146667, 'learning_rate': 0.00020287133197968208, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8167440452705088}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:12:04,570][0m Trial 16 finished with value: 0.11657400741039411 and parameters: {'observation_period_num': 53, 'train_rates': 0.8731660665597613, 'learning_rate': 0.00012041355996182334, 'batch_size': 178, 'step_size': 3, 'gamma': 0.8163543709640142}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:13:13,634][0m Trial 17 finished with value: 0.04980641530185449 and parameters: {'observation_period_num': 50, 'train_rates': 0.9369249413158452, 'learning_rate': 0.00013124513151896872, 'batch_size': 90, 'step_size': 4, 'gamma': 0.90531997696286}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:13:56,905][0m Trial 18 finished with value: 0.12370583683424106 and parameters: {'observation_period_num': 162, 'train_rates': 0.751490007580524, 'learning_rate': 0.000259803322749886, 'batch_size': 155, 'step_size': 9, 'gamma': 0.9780967068939419}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:14:32,134][0m Trial 19 finished with value: 0.19448286294937134 and parameters: {'observation_period_num': 244, 'train_rates': 0.9389330913180005, 'learning_rate': 7.19865384807448e-05, 'batch_size': 227, 'step_size': 6, 'gamma': 0.8269006605797165}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:15:36,201][0m Trial 20 finished with value: 0.05692866156178136 and parameters: {'observation_period_num': 80, 'train_rates': 0.8704617340840526, 'learning_rate': 0.0004024718930850119, 'batch_size': 90, 'step_size': 3, 'gamma': 0.7957612655470025}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:16:46,214][0m Trial 21 finished with value: 0.04702137999639318 and parameters: {'observation_period_num': 54, 'train_rates': 0.897848057070162, 'learning_rate': 0.0002315858472748389, 'batch_size': 84, 'step_size': 4, 'gamma': 0.8329100197524775}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:18:03,092][0m Trial 22 finished with value: 0.05361848096748826 and parameters: {'observation_period_num': 42, 'train_rates': 0.9461612071010905, 'learning_rate': 0.00014840596166751477, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8320301931481335}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:19:49,344][0m Trial 23 finished with value: 0.04699817711924329 and parameters: {'observation_period_num': 75, 'train_rates': 0.8593626150981967, 'learning_rate': 0.0004231389538069673, 'batch_size': 52, 'step_size': 3, 'gamma': 0.7777673961635362}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:22:03,377][0m Trial 24 finished with value: 0.054532306595742014 and parameters: {'observation_period_num': 76, 'train_rates': 0.8497170868265078, 'learning_rate': 0.0004972751803104334, 'batch_size': 41, 'step_size': 3, 'gamma': 0.7817869634472938}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:23:08,870][0m Trial 25 finished with value: 0.3057996441565257 and parameters: {'observation_period_num': 114, 'train_rates': 0.7735389426774457, 'learning_rate': 6.700096568592534e-05, 'batch_size': 101, 'step_size': 2, 'gamma': 0.7846062938016891}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:25:28,263][0m Trial 26 finished with value: 0.052473084878852466 and parameters: {'observation_period_num': 38, 'train_rates': 0.7334877002868251, 'learning_rate': 0.00037336571801218255, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8403924990952403}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:26:17,067][0m Trial 27 finished with value: 0.08879314363002777 and parameters: {'observation_period_num': 148, 'train_rates': 0.9564407352929566, 'learning_rate': 0.0006847106766818909, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8841327351585684}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:27:26,665][0m Trial 28 finished with value: 0.1158220567664162 and parameters: {'observation_period_num': 74, 'train_rates': 0.9154634506494463, 'learning_rate': 0.00032610299532726385, 'batch_size': 126, 'step_size': 2, 'gamma': 0.7729040538736444}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:28:38,450][0m Trial 29 finished with value: 0.0877758318837355 and parameters: {'observation_period_num': 102, 'train_rates': 0.9135758843867713, 'learning_rate': 7.652563304419683e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.8012908683839324}. Best is trial 13 with value: 0.0459422804415226.[0m
[32m[I 2025-01-06 22:31:19,013][0m Trial 30 finished with value: 0.04382428710692023 and parameters: {'observation_period_num': 33, 'train_rates': 0.8438393109629564, 'learning_rate': 0.0009419921494199777, 'batch_size': 39, 'step_size': 3, 'gamma': 0.771837969247763}. Best is trial 30 with value: 0.04382428710692023.[0m
[32m[I 2025-01-06 22:33:43,809][0m Trial 31 finished with value: 0.04229236419423152 and parameters: {'observation_period_num': 33, 'train_rates': 0.8384191372516684, 'learning_rate': 0.0006064586229915887, 'batch_size': 40, 'step_size': 3, 'gamma': 0.7677081789303382}. Best is trial 31 with value: 0.04229236419423152.[0m
[32m[I 2025-01-06 22:36:22,161][0m Trial 32 finished with value: 0.036512586515396836 and parameters: {'observation_period_num': 31, 'train_rates': 0.8290607132268818, 'learning_rate': 0.0006688209641503408, 'batch_size': 34, 'step_size': 3, 'gamma': 0.76641563596835}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:39:15,967][0m Trial 33 finished with value: 0.03964835220605743 and parameters: {'observation_period_num': 26, 'train_rates': 0.8326835067080106, 'learning_rate': 0.0006812491419317083, 'batch_size': 34, 'step_size': 2, 'gamma': 0.768407446345904}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:41:49,233][0m Trial 34 finished with value: 0.03840853901783703 and parameters: {'observation_period_num': 28, 'train_rates': 0.8249184925806767, 'learning_rate': 0.0005801479977674317, 'batch_size': 34, 'step_size': 2, 'gamma': 0.7669479045937826}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:44:35,744][0m Trial 35 finished with value: 0.23721077932167228 and parameters: {'observation_period_num': 25, 'train_rates': 0.8130539246678261, 'learning_rate': 8.674320309615583e-06, 'batch_size': 31, 'step_size': 2, 'gamma': 0.7655771021702219}. Best is trial 32 with value: 0.036512586515396836.[0m
Early stopping at epoch 62
[32m[I 2025-01-06 22:45:31,112][0m Trial 36 finished with value: 0.055633022996359166 and parameters: {'observation_period_num': 8, 'train_rates': 0.825989021100383, 'learning_rate': 0.0006189348199617156, 'batch_size': 64, 'step_size': 1, 'gamma': 0.7633369834056678}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:48:24,476][0m Trial 37 finished with value: 0.03919329828461354 and parameters: {'observation_period_num': 29, 'train_rates': 0.782264570979884, 'learning_rate': 0.0006486979492284849, 'batch_size': 30, 'step_size': 2, 'gamma': 0.7919882582164344}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:49:47,642][0m Trial 38 finished with value: 0.12703870421364194 and parameters: {'observation_period_num': 6, 'train_rates': 0.7862769469548275, 'learning_rate': 1.6786768340719415e-05, 'batch_size': 66, 'step_size': 2, 'gamma': 0.7998632289607378}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:52:47,817][0m Trial 39 finished with value: 0.04764242800983497 and parameters: {'observation_period_num': 23, 'train_rates': 0.7125292643274238, 'learning_rate': 4.875216296934823e-05, 'batch_size': 27, 'step_size': 8, 'gamma': 0.7905760403220153}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:54:48,402][0m Trial 40 finished with value: 0.04490216742807896 and parameters: {'observation_period_num': 27, 'train_rates': 0.7960226169302642, 'learning_rate': 0.00016656275402999264, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9106825247551754}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 22:57:26,321][0m Trial 41 finished with value: 0.04499709885567427 and parameters: {'observation_period_num': 38, 'train_rates': 0.8271311145404509, 'learning_rate': 0.0006064885481381694, 'batch_size': 36, 'step_size': 2, 'gamma': 0.7661668589774336}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 23:02:10,494][0m Trial 42 finished with value: 0.053393431660204285 and parameters: {'observation_period_num': 42, 'train_rates': 0.7696126799688692, 'learning_rate': 0.0006810976768653513, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7623424232782816}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 23:03:50,597][0m Trial 43 finished with value: 0.05754724424725344 and parameters: {'observation_period_num': 65, 'train_rates': 0.8358064832077019, 'learning_rate': 0.00035130819418448456, 'batch_size': 56, 'step_size': 5, 'gamma': 0.7896458128478608}. Best is trial 32 with value: 0.036512586515396836.[0m
[32m[I 2025-01-06 23:09:19,669][0m Trial 44 finished with value: 0.03599758587121346 and parameters: {'observation_period_num': 27, 'train_rates': 0.8022712235592796, 'learning_rate': 0.00047020035007374814, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7775585346308385}. Best is trial 44 with value: 0.03599758587121346.[0m
Early stopping at epoch 85
[32m[I 2025-01-06 23:13:43,571][0m Trial 45 finished with value: 0.04868292968853244 and parameters: {'observation_period_num': 20, 'train_rates': 0.8092218058349795, 'learning_rate': 0.0003255759844504351, 'batch_size': 17, 'step_size': 1, 'gamma': 0.7833247260594841}. Best is trial 44 with value: 0.03599758587121346.[0m
[32m[I 2025-01-06 23:16:40,803][0m Trial 46 finished with value: 0.06714810411450942 and parameters: {'observation_period_num': 89, 'train_rates': 0.7471937690122143, 'learning_rate': 0.0004892055940684606, 'batch_size': 29, 'step_size': 2, 'gamma': 0.7768050395440322}. Best is trial 44 with value: 0.03599758587121346.[0m
[32m[I 2025-01-06 23:18:03,756][0m Trial 47 finished with value: 0.7941110466099992 and parameters: {'observation_period_num': 5, 'train_rates': 0.7855753465707367, 'learning_rate': 1.2608626646347963e-06, 'batch_size': 68, 'step_size': 4, 'gamma': 0.7566527527976137}. Best is trial 44 with value: 0.03599758587121346.[0m
Early stopping at epoch 57
[32m[I 2025-01-06 23:18:22,563][0m Trial 48 finished with value: 0.18117841783712602 and parameters: {'observation_period_num': 45, 'train_rates': 0.7092467942964376, 'learning_rate': 0.0007365568277533091, 'batch_size': 214, 'step_size': 1, 'gamma': 0.7513485013223732}. Best is trial 44 with value: 0.03599758587121346.[0m
[32m[I 2025-01-06 23:20:01,985][0m Trial 49 finished with value: 0.03016510694843093 and parameters: {'observation_period_num': 17, 'train_rates': 0.8143765847789818, 'learning_rate': 0.00010269448924050796, 'batch_size': 54, 'step_size': 15, 'gamma': 0.8176680225309224}. Best is trial 49 with value: 0.03016510694843093.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-06 23:20:01,994][0m A new study created in memory with name: no-name-708e5fcf-9904-4b42-a400-bf16c63e392e[0m
[32m[I 2025-01-06 23:20:48,147][0m Trial 0 finished with value: 0.36072703839975817 and parameters: {'observation_period_num': 60, 'train_rates': 0.8988448300770044, 'learning_rate': 8.18356381008595e-06, 'batch_size': 222, 'step_size': 6, 'gamma': 0.8908613804868891}. Best is trial 0 with value: 0.36072703839975817.[0m
[32m[I 2025-01-06 23:22:01,070][0m Trial 1 finished with value: 0.13599298894405365 and parameters: {'observation_period_num': 25, 'train_rates': 0.9693329563296373, 'learning_rate': 2.6508163323743846e-05, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8456193504244535}. Best is trial 1 with value: 0.13599298894405365.[0m
[32m[I 2025-01-06 23:22:48,924][0m Trial 2 finished with value: 0.09870831164959315 and parameters: {'observation_period_num': 61, 'train_rates': 0.9362180296839764, 'learning_rate': 0.0002941695912331844, 'batch_size': 165, 'step_size': 1, 'gamma': 0.9317973823795581}. Best is trial 2 with value: 0.09870831164959315.[0m
[32m[I 2025-01-06 23:24:18,051][0m Trial 3 finished with value: 0.18039161305721493 and parameters: {'observation_period_num': 161, 'train_rates': 0.8692220634836989, 'learning_rate': 0.0007074896840655769, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9872085373965034}. Best is trial 2 with value: 0.09870831164959315.[0m
[32m[I 2025-01-06 23:25:18,205][0m Trial 4 finished with value: 0.5073928236961365 and parameters: {'observation_period_num': 228, 'train_rates': 0.9669128969759342, 'learning_rate': 2.3135581496377128e-06, 'batch_size': 95, 'step_size': 14, 'gamma': 0.8930172285662236}. Best is trial 2 with value: 0.09870831164959315.[0m
[32m[I 2025-01-06 23:26:00,756][0m Trial 5 finished with value: 0.08855219826166903 and parameters: {'observation_period_num': 43, 'train_rates': 0.6832486861767362, 'learning_rate': 0.0001318281923893872, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8355019815907725}. Best is trial 5 with value: 0.08855219826166903.[0m
[32m[I 2025-01-06 23:26:42,450][0m Trial 6 finished with value: 0.19594338277772982 and parameters: {'observation_period_num': 138, 'train_rates': 0.651780439729659, 'learning_rate': 2.9419572708755668e-05, 'batch_size': 117, 'step_size': 13, 'gamma': 0.8827942800852845}. Best is trial 5 with value: 0.08855219826166903.[0m
[32m[I 2025-01-06 23:30:36,845][0m Trial 7 finished with value: 0.1525982834207706 and parameters: {'observation_period_num': 209, 'train_rates': 0.6872831627738426, 'learning_rate': 1.1177112862551185e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8608028818404883}. Best is trial 5 with value: 0.08855219826166903.[0m
[32m[I 2025-01-06 23:32:07,907][0m Trial 8 finished with value: 0.057107415060516566 and parameters: {'observation_period_num': 34, 'train_rates': 0.7509988735883147, 'learning_rate': 0.0006933061515056408, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9590445453467223}. Best is trial 8 with value: 0.057107415060516566.[0m
[32m[I 2025-01-06 23:34:44,935][0m Trial 9 finished with value: 0.16167495713894625 and parameters: {'observation_period_num': 240, 'train_rates': 0.9694549526822487, 'learning_rate': 0.0001547553075621689, 'batch_size': 35, 'step_size': 11, 'gamma': 0.956889166812863}. Best is trial 8 with value: 0.057107415060516566.[0m
[32m[I 2025-01-06 23:35:24,727][0m Trial 10 finished with value: 0.06089358935714699 and parameters: {'observation_period_num': 97, 'train_rates': 0.7758257206111291, 'learning_rate': 0.0009217975296122348, 'batch_size': 193, 'step_size': 9, 'gamma': 0.7640042081505752}. Best is trial 8 with value: 0.057107415060516566.[0m
[32m[I 2025-01-06 23:36:06,268][0m Trial 11 finished with value: 0.06562889582472953 and parameters: {'observation_period_num': 92, 'train_rates': 0.7856157205457251, 'learning_rate': 0.0009918720360103442, 'batch_size': 180, 'step_size': 9, 'gamma': 0.7621138072660029}. Best is trial 8 with value: 0.057107415060516566.[0m
[32m[I 2025-01-06 23:36:46,158][0m Trial 12 finished with value: 0.048275993667219 and parameters: {'observation_period_num': 6, 'train_rates': 0.765963198199028, 'learning_rate': 0.0003853289857418286, 'batch_size': 249, 'step_size': 8, 'gamma': 0.7515199587013018}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:37:20,472][0m Trial 13 finished with value: 0.06692911484821908 and parameters: {'observation_period_num': 10, 'train_rates': 0.7407095424466144, 'learning_rate': 0.0002248078350004532, 'batch_size': 253, 'step_size': 7, 'gamma': 0.7973372467634684}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:38:45,935][0m Trial 14 finished with value: 0.06129048315691199 and parameters: {'observation_period_num': 83, 'train_rates': 0.8557607889364794, 'learning_rate': 7.693245506852761e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9247456534726036}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:39:31,809][0m Trial 15 finished with value: 0.052363121641032837 and parameters: {'observation_period_num': 15, 'train_rates': 0.7394798447460424, 'learning_rate': 0.000400338759381713, 'batch_size': 151, 'step_size': 4, 'gamma': 0.8063510856551364}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:40:02,865][0m Trial 16 finished with value: 0.10643688484764936 and parameters: {'observation_period_num': 7, 'train_rates': 0.6128664325800722, 'learning_rate': 6.553522556619748e-05, 'batch_size': 245, 'step_size': 4, 'gamma': 0.7997682189075224}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:40:48,758][0m Trial 17 finished with value: 0.18492056702574095 and parameters: {'observation_period_num': 162, 'train_rates': 0.8280008853360663, 'learning_rate': 0.0003766304542592038, 'batch_size': 152, 'step_size': 2, 'gamma': 0.8050428005709929}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:41:36,779][0m Trial 18 finished with value: 0.2621614755851496 and parameters: {'observation_period_num': 117, 'train_rates': 0.7201601988357132, 'learning_rate': 6.20231490774186e-05, 'batch_size': 220, 'step_size': 6, 'gamma': 0.7504145153068665}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:42:25,456][0m Trial 19 finished with value: 1.066227716338985 and parameters: {'observation_period_num': 61, 'train_rates': 0.8118964325406184, 'learning_rate': 1.011278071681063e-06, 'batch_size': 206, 'step_size': 4, 'gamma': 0.7779290143168374}. Best is trial 12 with value: 0.048275993667219.[0m
[32m[I 2025-01-06 23:43:05,572][0m Trial 20 finished with value: 0.03803248505733193 and parameters: {'observation_period_num': 6, 'train_rates': 0.702693099448999, 'learning_rate': 0.0003821858659789747, 'batch_size': 142, 'step_size': 7, 'gamma': 0.8188170230974403}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:43:51,308][0m Trial 21 finished with value: 0.041833557824536065 and parameters: {'observation_period_num': 13, 'train_rates': 0.7069500858113036, 'learning_rate': 0.00047639253909028447, 'batch_size': 140, 'step_size': 7, 'gamma': 0.8247394013109479}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:44:37,987][0m Trial 22 finished with value: 0.06317668916433052 and parameters: {'observation_period_num': 44, 'train_rates': 0.6918710040393101, 'learning_rate': 0.00043376854040771766, 'batch_size': 131, 'step_size': 7, 'gamma': 0.8307997764304229}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:45:27,896][0m Trial 23 finished with value: 0.05587089816929587 and parameters: {'observation_period_num': 8, 'train_rates': 0.6379900424699425, 'learning_rate': 0.00014067256707160224, 'batch_size': 177, 'step_size': 8, 'gamma': 0.8221752444515228}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:46:49,105][0m Trial 24 finished with value: 0.05248956993329439 and parameters: {'observation_period_num': 35, 'train_rates': 0.7142231622315408, 'learning_rate': 0.00019520308579870188, 'batch_size': 86, 'step_size': 6, 'gamma': 0.7844078550247703}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:47:37,649][0m Trial 25 finished with value: 0.06556228955990166 and parameters: {'observation_period_num': 73, 'train_rates': 0.6578790722245217, 'learning_rate': 0.00047507429104874004, 'batch_size': 144, 'step_size': 10, 'gamma': 0.860113154083964}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:48:18,019][0m Trial 26 finished with value: 0.12297523415578136 and parameters: {'observation_period_num': 27, 'train_rates': 0.6002456022632368, 'learning_rate': 9.001089357214479e-05, 'batch_size': 226, 'step_size': 8, 'gamma': 0.8170593462504621}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:49:22,139][0m Trial 27 finished with value: 0.2508335165375826 and parameters: {'observation_period_num': 50, 'train_rates': 0.7801154008770073, 'learning_rate': 1.549700275846164e-05, 'batch_size': 126, 'step_size': 7, 'gamma': 0.7883424079277926}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:50:18,986][0m Trial 28 finished with value: 0.0823515235478508 and parameters: {'observation_period_num': 111, 'train_rates': 0.7552394145862864, 'learning_rate': 0.00026407645051037236, 'batch_size': 196, 'step_size': 10, 'gamma': 0.8484367410441537}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:51:18,456][0m Trial 29 finished with value: 0.8004191312000906 and parameters: {'observation_period_num': 71, 'train_rates': 0.7106872174971999, 'learning_rate': 3.641498481783369e-06, 'batch_size': 162, 'step_size': 5, 'gamma': 0.898343584061255}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:52:16,752][0m Trial 30 finished with value: 0.23543455542587652 and parameters: {'observation_period_num': 191, 'train_rates': 0.8220321973107639, 'learning_rate': 4.367654324932284e-05, 'batch_size': 235, 'step_size': 5, 'gamma': 0.8758957512334251}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:53:10,278][0m Trial 31 finished with value: 0.06954127762325202 and parameters: {'observation_period_num': 19, 'train_rates': 0.7305845832614892, 'learning_rate': 0.0005208227025496121, 'batch_size': 141, 'step_size': 3, 'gamma': 0.8037088266901032}. Best is trial 20 with value: 0.03803248505733193.[0m
[32m[I 2025-01-06 23:54:10,744][0m Trial 32 finished with value: 0.03779779784866594 and parameters: {'observation_period_num': 19, 'train_rates': 0.7645469946458189, 'learning_rate': 0.0003085295479041274, 'batch_size': 105, 'step_size': 6, 'gamma': 0.8129439423843955}. Best is trial 32 with value: 0.03779779784866594.[0m
[32m[I 2025-01-06 23:55:24,866][0m Trial 33 finished with value: 0.0381281973869957 and parameters: {'observation_period_num': 26, 'train_rates': 0.7735217443221672, 'learning_rate': 0.00025856265993249103, 'batch_size': 79, 'step_size': 6, 'gamma': 0.7745062062834452}. Best is trial 32 with value: 0.03779779784866594.[0m
[32m[I 2025-01-06 23:56:27,362][0m Trial 34 finished with value: 0.03566745579765554 and parameters: {'observation_period_num': 30, 'train_rates': 0.8049531332713562, 'learning_rate': 0.0002705822958309372, 'batch_size': 104, 'step_size': 6, 'gamma': 0.843067571394118}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-06 23:57:46,812][0m Trial 35 finished with value: 0.048172014564696976 and parameters: {'observation_period_num': 54, 'train_rates': 0.9025056863216578, 'learning_rate': 0.00010947675313223, 'batch_size': 83, 'step_size': 6, 'gamma': 0.8453470303414682}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-06 23:58:55,948][0m Trial 36 finished with value: 0.042516362499183336 and parameters: {'observation_period_num': 30, 'train_rates': 0.8058392786850057, 'learning_rate': 0.0002590891467066655, 'batch_size': 106, 'step_size': 5, 'gamma': 0.7693438441353521}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:00:23,431][0m Trial 37 finished with value: 0.0745151280082603 and parameters: {'observation_period_num': 26, 'train_rates': 0.8562804891955921, 'learning_rate': 4.407948502508045e-05, 'batch_size': 77, 'step_size': 3, 'gamma': 0.8419281755888601}. Best is trial 34 with value: 0.03566745579765554.[0m
Early stopping at epoch 87
[32m[I 2025-01-07 00:01:23,672][0m Trial 38 finished with value: 0.1403400093830865 and parameters: {'observation_period_num': 40, 'train_rates': 0.8407378236214512, 'learning_rate': 0.00018407937793217524, 'batch_size': 104, 'step_size': 1, 'gamma': 0.8167578745714729}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:03:58,028][0m Trial 39 finished with value: 0.10194707324930058 and parameters: {'observation_period_num': 69, 'train_rates': 0.8911201860819116, 'learning_rate': 0.0007634326633036959, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8546785134488553}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:04:51,345][0m Trial 40 finished with value: 0.04452344075304963 and parameters: {'observation_period_num': 23, 'train_rates': 0.7620674305689135, 'learning_rate': 0.0002855450776490901, 'batch_size': 114, 'step_size': 6, 'gamma': 0.7878936114909835}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:05:46,214][0m Trial 41 finished with value: 0.071907623535157 and parameters: {'observation_period_num': 53, 'train_rates': 0.6712350914857289, 'learning_rate': 0.0006011734862737195, 'batch_size': 95, 'step_size': 7, 'gamma': 0.826082850673366}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:06:36,935][0m Trial 42 finished with value: 0.03942488335501382 and parameters: {'observation_period_num': 19, 'train_rates': 0.6976255240316375, 'learning_rate': 0.0005862990169378177, 'batch_size': 128, 'step_size': 7, 'gamma': 0.8357816660156666}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:07:33,102][0m Trial 43 finished with value: 0.05610183019350936 and parameters: {'observation_period_num': 38, 'train_rates': 0.7841361699148411, 'learning_rate': 0.00011845317002003932, 'batch_size': 117, 'step_size': 5, 'gamma': 0.8131441731755656}. Best is trial 34 with value: 0.03566745579765554.[0m
[32m[I 2025-01-07 00:08:55,140][0m Trial 44 finished with value: 0.03268699910164275 and parameters: {'observation_period_num': 21, 'train_rates': 0.7942126719217834, 'learning_rate': 0.00032591711531751005, 'batch_size': 74, 'step_size': 9, 'gamma': 0.8691563955704994}. Best is trial 44 with value: 0.03268699910164275.[0m
[32m[I 2025-01-07 00:10:28,011][0m Trial 45 finished with value: 0.07997166368094358 and parameters: {'observation_period_num': 140, 'train_rates': 0.8043873088953697, 'learning_rate': 0.0003108188438326954, 'batch_size': 56, 'step_size': 9, 'gamma': 0.869475371778282}. Best is trial 44 with value: 0.03268699910164275.[0m
[32m[I 2025-01-07 00:11:45,886][0m Trial 46 finished with value: 0.04157257531808705 and parameters: {'observation_period_num': 45, 'train_rates': 0.7955764859033754, 'learning_rate': 0.0001723249923807989, 'batch_size': 69, 'step_size': 11, 'gamma': 0.886769028793968}. Best is trial 44 with value: 0.03268699910164275.[0m
[32m[I 2025-01-07 00:13:42,332][0m Trial 47 finished with value: 0.032144619446837594 and parameters: {'observation_period_num': 5, 'train_rates': 0.8307541305099913, 'learning_rate': 0.0007513475508938876, 'batch_size': 50, 'step_size': 9, 'gamma': 0.906873442627327}. Best is trial 47 with value: 0.032144619446837594.[0m
[32m[I 2025-01-07 00:15:43,926][0m Trial 48 finished with value: 0.039177103687623206 and parameters: {'observation_period_num': 19, 'train_rates': 0.8906637911309642, 'learning_rate': 0.0008022030064301025, 'batch_size': 47, 'step_size': 9, 'gamma': 0.9074358363221744}. Best is trial 47 with value: 0.032144619446837594.[0m
[32m[I 2025-01-07 00:17:43,067][0m Trial 49 finished with value: 0.03796652446572597 and parameters: {'observation_period_num': 5, 'train_rates': 0.9296327627745757, 'learning_rate': 0.0009957309160873555, 'batch_size': 51, 'step_size': 12, 'gamma': 0.9307036463288656}. Best is trial 47 with value: 0.032144619446837594.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 00:17:43,079][0m A new study created in memory with name: no-name-43574d10-3e45-4718-a819-a68c0cf88b2a[0m
[32m[I 2025-01-07 00:21:41,268][0m Trial 0 finished with value: 0.0687440788841557 and parameters: {'observation_period_num': 78, 'train_rates': 0.8728872621865278, 'learning_rate': 6.505522709461937e-05, 'batch_size': 23, 'step_size': 12, 'gamma': 0.9779518496833862}. Best is trial 0 with value: 0.0687440788841557.[0m
[32m[I 2025-01-07 00:22:48,497][0m Trial 1 finished with value: 0.07831518175822604 and parameters: {'observation_period_num': 86, 'train_rates': 0.7286855802667909, 'learning_rate': 0.00014308715153265626, 'batch_size': 84, 'step_size': 8, 'gamma': 0.9238214897262321}. Best is trial 0 with value: 0.0687440788841557.[0m
[32m[I 2025-01-07 00:23:52,740][0m Trial 2 finished with value: 0.13356150441580383 and parameters: {'observation_period_num': 55, 'train_rates': 0.8659593504012097, 'learning_rate': 1.0344543183466228e-05, 'batch_size': 112, 'step_size': 11, 'gamma': 0.8960149199166488}. Best is trial 0 with value: 0.0687440788841557.[0m
[32m[I 2025-01-07 00:24:35,683][0m Trial 3 finished with value: 0.0417870276353576 and parameters: {'observation_period_num': 26, 'train_rates': 0.7369935733889933, 'learning_rate': 0.000322952891242482, 'batch_size': 228, 'step_size': 11, 'gamma': 0.775941873827621}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:25:27,213][0m Trial 4 finished with value: 0.38524295136771397 and parameters: {'observation_period_num': 34, 'train_rates': 0.7558199948327675, 'learning_rate': 1.5589675359592417e-05, 'batch_size': 166, 'step_size': 4, 'gamma': 0.7623694901873502}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:26:24,584][0m Trial 5 finished with value: 0.177182770822898 and parameters: {'observation_period_num': 170, 'train_rates': 0.8914032995481165, 'learning_rate': 0.0007287116222250564, 'batch_size': 131, 'step_size': 8, 'gamma': 0.9743138219806383}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:28:56,567][0m Trial 6 finished with value: 0.13713806286765362 and parameters: {'observation_period_num': 223, 'train_rates': 0.8422660502200169, 'learning_rate': 0.0004858703511809543, 'batch_size': 36, 'step_size': 8, 'gamma': 0.7780392396396044}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:29:36,296][0m Trial 7 finished with value: 0.1973070651292801 and parameters: {'observation_period_num': 249, 'train_rates': 0.9273463614955804, 'learning_rate': 2.966981957769927e-05, 'batch_size': 228, 'step_size': 15, 'gamma': 0.9097912231832406}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:30:10,002][0m Trial 8 finished with value: 0.18204561334389907 and parameters: {'observation_period_num': 245, 'train_rates': 0.8080074529884351, 'learning_rate': 0.00019962410297396564, 'batch_size': 255, 'step_size': 4, 'gamma': 0.831444189857567}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:31:13,615][0m Trial 9 finished with value: 0.359579473733902 and parameters: {'observation_period_num': 175, 'train_rates': 0.9797696385333355, 'learning_rate': 6.010020004133289e-06, 'batch_size': 95, 'step_size': 9, 'gamma': 0.8532969904526114}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:31:48,208][0m Trial 10 finished with value: 1.335003354100645 and parameters: {'observation_period_num': 15, 'train_rates': 0.6183960869548394, 'learning_rate': 1.1336919844435888e-06, 'batch_size': 192, 'step_size': 15, 'gamma': 0.8054818807660169}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:33:58,914][0m Trial 11 finished with value: 0.1310067706507338 and parameters: {'observation_period_num': 108, 'train_rates': 0.6658271737217387, 'learning_rate': 9.385068937820665e-05, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9885721688000472}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:34:41,009][0m Trial 12 finished with value: 0.09723889924989546 and parameters: {'observation_period_num': 70, 'train_rates': 0.7184436801410349, 'learning_rate': 6.556891423765606e-05, 'batch_size': 178, 'step_size': 12, 'gamma': 0.9414301291152024}. Best is trial 3 with value: 0.0417870276353576.[0m
[32m[I 2025-01-07 00:36:10,632][0m Trial 13 finished with value: 0.028201570917320038 and parameters: {'observation_period_num': 11, 'train_rates': 0.7714790377854563, 'learning_rate': 0.00034981100112560135, 'batch_size': 62, 'step_size': 13, 'gamma': 0.877869650618062}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:37:37,566][0m Trial 14 finished with value: 0.056188026170541604 and parameters: {'observation_period_num': 5, 'train_rates': 0.7776641260835121, 'learning_rate': 0.00029416943961344265, 'batch_size': 70, 'step_size': 1, 'gamma': 0.8725289117178239}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:38:28,142][0m Trial 15 finished with value: 0.0500289473686678 and parameters: {'observation_period_num': 41, 'train_rates': 0.683014187629904, 'learning_rate': 0.0009541868468589381, 'batch_size': 150, 'step_size': 14, 'gamma': 0.811801758237721}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:39:12,086][0m Trial 16 finished with value: 0.10901613079767655 and parameters: {'observation_period_num': 130, 'train_rates': 0.7871531702123369, 'learning_rate': 0.0003914313913648305, 'batch_size': 210, 'step_size': 10, 'gamma': 0.8715797362778287}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:40:40,106][0m Trial 17 finished with value: 0.06963975469911819 and parameters: {'observation_period_num': 29, 'train_rates': 0.6205093709662691, 'learning_rate': 0.0001936118008238697, 'batch_size': 61, 'step_size': 6, 'gamma': 0.8378982697038191}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:41:30,413][0m Trial 18 finished with value: 0.208075357997042 and parameters: {'observation_period_num': 124, 'train_rates': 0.820378079990369, 'learning_rate': 3.6038421164450194e-05, 'batch_size': 246, 'step_size': 13, 'gamma': 0.7907549293969853}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:42:22,689][0m Trial 19 finished with value: 0.8001608115210875 and parameters: {'observation_period_num': 157, 'train_rates': 0.6856103836290608, 'learning_rate': 1.7925358056471094e-06, 'batch_size': 117, 'step_size': 10, 'gamma': 0.893116933619799}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:42:58,479][0m Trial 20 finished with value: 0.05671185894741929 and parameters: {'observation_period_num': 50, 'train_rates': 0.7429086504419818, 'learning_rate': 0.0005355555991364396, 'batch_size': 207, 'step_size': 6, 'gamma': 0.7505699825807861}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:43:49,269][0m Trial 21 finished with value: 0.0480911176181534 and parameters: {'observation_period_num': 36, 'train_rates': 0.6822233801715771, 'learning_rate': 0.0007848069071820712, 'batch_size': 147, 'step_size': 14, 'gamma': 0.814737993650343}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:44:39,347][0m Trial 22 finished with value: 0.032470462777054135 and parameters: {'observation_period_num': 9, 'train_rates': 0.658585184461052, 'learning_rate': 0.0002830295522857466, 'batch_size': 157, 'step_size': 14, 'gamma': 0.8194396310201073}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:45:22,855][0m Trial 23 finished with value: 0.045792853107705984 and parameters: {'observation_period_num': 11, 'train_rates': 0.641878581389372, 'learning_rate': 0.0002789744712649433, 'batch_size': 172, 'step_size': 13, 'gamma': 0.7833524489147637}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:46:16,058][0m Trial 24 finished with value: 0.07024775832125431 and parameters: {'observation_period_num': 62, 'train_rates': 0.7174559022050521, 'learning_rate': 0.00016051042438821114, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8477460734479926}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:47:10,582][0m Trial 25 finished with value: 0.10435677729510537 and parameters: {'observation_period_num': 22, 'train_rates': 0.6516782591340765, 'learning_rate': 0.00010362427598756683, 'batch_size': 229, 'step_size': 14, 'gamma': 0.8208246971991947}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:48:06,737][0m Trial 26 finished with value: 0.03027504474410544 and parameters: {'observation_period_num': 10, 'train_rates': 0.7654248041287546, 'learning_rate': 0.00030507431944415707, 'batch_size': 121, 'step_size': 13, 'gamma': 0.7977274801546101}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:48:58,414][0m Trial 27 finished with value: 0.17042090420638187 and parameters: {'observation_period_num': 99, 'train_rates': 0.6006646024131095, 'learning_rate': 5.74357709750317e-05, 'batch_size': 103, 'step_size': 15, 'gamma': 0.8597070461798615}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:49:55,617][0m Trial 28 finished with value: 0.05941303855575183 and parameters: {'observation_period_num': 8, 'train_rates': 0.768995964079165, 'learning_rate': 3.5701781337295817e-05, 'batch_size': 127, 'step_size': 13, 'gamma': 0.7925344796140473}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:51:35,571][0m Trial 29 finished with value: 0.05135051061523191 and parameters: {'observation_period_num': 76, 'train_rates': 0.8380504190612309, 'learning_rate': 9.493605666103455e-05, 'batch_size': 59, 'step_size': 13, 'gamma': 0.8862416344771903}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:52:45,343][0m Trial 30 finished with value: 0.07543287291984707 and parameters: {'observation_period_num': 48, 'train_rates': 0.7030472090300992, 'learning_rate': 0.00048016877342455086, 'batch_size': 87, 'step_size': 12, 'gamma': 0.8357895319796723}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:57:35,919][0m Trial 31 finished with value: 0.045083704443230524 and parameters: {'observation_period_num': 25, 'train_rates': 0.7534189604983793, 'learning_rate': 0.0002632242525526889, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7651280071795986}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:58:34,586][0m Trial 32 finished with value: 0.033596005176997366 and parameters: {'observation_period_num': 7, 'train_rates': 0.7331170153540295, 'learning_rate': 0.0003082122115401318, 'batch_size': 152, 'step_size': 10, 'gamma': 0.802384420137651}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 00:59:38,791][0m Trial 33 finished with value: 0.039021240575379465 and parameters: {'observation_period_num': 6, 'train_rates': 0.7975036569795169, 'learning_rate': 0.0001824514973315127, 'batch_size': 151, 'step_size': 9, 'gamma': 0.8046401813418034}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:00:31,687][0m Trial 34 finished with value: 0.08354886650576797 and parameters: {'observation_period_num': 87, 'train_rates': 0.7076399840685172, 'learning_rate': 0.00012655854591191068, 'batch_size': 162, 'step_size': 14, 'gamma': 0.8252111726798275}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:01:23,714][0m Trial 35 finished with value: 0.06617560778867523 and parameters: {'observation_period_num': 50, 'train_rates': 0.744558391089088, 'learning_rate': 0.0005798434762961762, 'batch_size': 110, 'step_size': 10, 'gamma': 0.924751784762445}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:02:10,295][0m Trial 36 finished with value: 0.053050463593673415 and parameters: {'observation_period_num': 62, 'train_rates': 0.77147465812392, 'learning_rate': 0.000374354720077833, 'batch_size': 137, 'step_size': 12, 'gamma': 0.8009171242965074}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:02:54,202][0m Trial 37 finished with value: 0.07860306363658574 and parameters: {'observation_period_num': 23, 'train_rates': 0.8918891376757311, 'learning_rate': 6.644929110355336e-05, 'batch_size': 185, 'step_size': 7, 'gamma': 0.7660044938969172}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:04:03,092][0m Trial 38 finished with value: 0.06886428915391303 and parameters: {'observation_period_num': 37, 'train_rates': 0.7309450395075021, 'learning_rate': 0.0009524374649035044, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8454926615163297}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:04:55,536][0m Trial 39 finished with value: 0.06027122345233947 and parameters: {'observation_period_num': 20, 'train_rates': 0.8305412015374379, 'learning_rate': 1.818003675923942e-05, 'batch_size': 122, 'step_size': 15, 'gamma': 0.8782832933686087}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:05:52,729][0m Trial 40 finished with value: 0.17228888759487554 and parameters: {'observation_period_num': 215, 'train_rates': 0.8542383411709087, 'learning_rate': 0.00023581120602284928, 'batch_size': 138, 'step_size': 9, 'gamma': 0.944961913035282}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:06:56,416][0m Trial 41 finished with value: 0.03618428377533654 and parameters: {'observation_period_num': 9, 'train_rates': 0.8020108896923043, 'learning_rate': 0.00016917333356562415, 'batch_size': 153, 'step_size': 9, 'gamma': 0.7982592133052889}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:08:00,379][0m Trial 42 finished with value: 0.03517117620305677 and parameters: {'observation_period_num': 34, 'train_rates': 0.8063670830313804, 'learning_rate': 0.0003970285231057131, 'batch_size': 159, 'step_size': 8, 'gamma': 0.7868001589878673}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:10:32,959][0m Trial 43 finished with value: 0.041575443670480876 and parameters: {'observation_period_num': 35, 'train_rates': 0.7659043488580436, 'learning_rate': 0.0006227102776511471, 'batch_size': 38, 'step_size': 7, 'gamma': 0.7842706017749496}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:11:22,607][0m Trial 44 finished with value: 0.03537283339782765 and parameters: {'observation_period_num': 20, 'train_rates': 0.8705104971565456, 'learning_rate': 0.00040370560770944654, 'batch_size': 137, 'step_size': 7, 'gamma': 0.7687678545454416}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:12:13,056][0m Trial 45 finished with value: 0.04326321131674654 and parameters: {'observation_period_num': 42, 'train_rates': 0.7873796916558601, 'learning_rate': 0.0003359315686450382, 'batch_size': 163, 'step_size': 8, 'gamma': 0.9083203719163092}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:12:54,287][0m Trial 46 finished with value: 0.096049298086892 and parameters: {'observation_period_num': 57, 'train_rates': 0.8252460765208547, 'learning_rate': 0.00013110721517641186, 'batch_size': 187, 'step_size': 4, 'gamma': 0.7542861037662187}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:13:53,875][0m Trial 47 finished with value: 0.2769080884892122 and parameters: {'observation_period_num': 17, 'train_rates': 0.9076953082239617, 'learning_rate': 3.956867091077331e-06, 'batch_size': 101, 'step_size': 12, 'gamma': 0.7762551213887724}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:14:36,444][0m Trial 48 finished with value: 0.03813788667321205 and parameters: {'observation_period_num': 30, 'train_rates': 0.989348593229825, 'learning_rate': 0.00024658731600335113, 'batch_size': 176, 'step_size': 10, 'gamma': 0.859032070099392}. Best is trial 13 with value: 0.028201570917320038.[0m
[32m[I 2025-01-07 01:15:16,052][0m Trial 49 finished with value: 0.04593910836030207 and parameters: {'observation_period_num': 70, 'train_rates': 0.8136296170604224, 'learning_rate': 0.0007243624635301883, 'batch_size': 160, 'step_size': 13, 'gamma': 0.819808642826854}. Best is trial 13 with value: 0.028201570917320038.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 01:15:16,060][0m A new study created in memory with name: no-name-39a41da7-0899-4cf1-bc8b-beddc27ca471[0m
[32m[I 2025-01-07 01:16:14,323][0m Trial 0 finished with value: 0.18413935601711273 and parameters: {'observation_period_num': 242, 'train_rates': 0.9581663715285058, 'learning_rate': 0.00033248234684871294, 'batch_size': 203, 'step_size': 12, 'gamma': 0.9487417908526498}. Best is trial 0 with value: 0.18413935601711273.[0m
[32m[I 2025-01-07 01:17:02,376][0m Trial 1 finished with value: 0.960231590775115 and parameters: {'observation_period_num': 66, 'train_rates': 0.8608628466239336, 'learning_rate': 1.2160386065816378e-06, 'batch_size': 169, 'step_size': 3, 'gamma': 0.808542866088259}. Best is trial 0 with value: 0.18413935601711273.[0m
[32m[I 2025-01-07 01:17:58,379][0m Trial 2 finished with value: 0.17661586222804596 and parameters: {'observation_period_num': 118, 'train_rates': 0.6981039392403561, 'learning_rate': 7.867570817416246e-05, 'batch_size': 184, 'step_size': 5, 'gamma': 0.8550886826559065}. Best is trial 2 with value: 0.17661586222804596.[0m
[32m[I 2025-01-07 01:18:59,356][0m Trial 3 finished with value: 0.17780260799918324 and parameters: {'observation_period_num': 226, 'train_rates': 0.7419144841371516, 'learning_rate': 9.473083551799528e-05, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9665809422127759}. Best is trial 2 with value: 0.17661586222804596.[0m
[32m[I 2025-01-07 01:20:12,706][0m Trial 4 finished with value: 0.05539198509918956 and parameters: {'observation_period_num': 52, 'train_rates': 0.8924999453625472, 'learning_rate': 0.00017790760693746116, 'batch_size': 123, 'step_size': 14, 'gamma': 0.8680710962869547}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:21:12,183][0m Trial 5 finished with value: 0.1330695465469547 and parameters: {'observation_period_num': 126, 'train_rates': 0.8194125549654934, 'learning_rate': 0.00013913675187319087, 'batch_size': 188, 'step_size': 4, 'gamma': 0.8113837873964843}. Best is trial 4 with value: 0.05539198509918956.[0m
Early stopping at epoch 48
[32m[I 2025-01-07 01:21:45,899][0m Trial 6 finished with value: 3.67855166566783 and parameters: {'observation_period_num': 15, 'train_rates': 0.9409323789211773, 'learning_rate': 1.840764350530765e-06, 'batch_size': 136, 'step_size': 1, 'gamma': 0.7951728647357226}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:22:40,988][0m Trial 7 finished with value: 0.2393015000575079 and parameters: {'observation_period_num': 21, 'train_rates': 0.6580331376317898, 'learning_rate': 5.6056984037390775e-06, 'batch_size': 178, 'step_size': 13, 'gamma': 0.8519830400372697}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:23:40,191][0m Trial 8 finished with value: 0.7127735664606861 and parameters: {'observation_period_num': 194, 'train_rates': 0.887261684851915, 'learning_rate': 2.166911594582446e-06, 'batch_size': 201, 'step_size': 11, 'gamma': 0.9492057348063933}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:24:54,088][0m Trial 9 finished with value: 0.11653067309315465 and parameters: {'observation_period_num': 159, 'train_rates': 0.9576124421538438, 'learning_rate': 3.369320610281339e-05, 'batch_size': 81, 'step_size': 5, 'gamma': 0.9848152057510359}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:28:31,875][0m Trial 10 finished with value: 0.27109576452113987 and parameters: {'observation_period_num': 71, 'train_rates': 0.6115204226908647, 'learning_rate': 0.0008980382959688014, 'batch_size': 21, 'step_size': 15, 'gamma': 0.9027065172204171}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:29:49,422][0m Trial 11 finished with value: 0.13315963134708175 and parameters: {'observation_period_num': 172, 'train_rates': 0.9703530606715182, 'learning_rate': 2.5616386406044907e-05, 'batch_size': 78, 'step_size': 8, 'gamma': 0.8901374244024376}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:31:00,229][0m Trial 12 finished with value: 0.0887605689439295 and parameters: {'observation_period_num': 81, 'train_rates': 0.9028421404244639, 'learning_rate': 1.5348754619909382e-05, 'batch_size': 83, 'step_size': 7, 'gamma': 0.9896826061131401}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:32:07,972][0m Trial 13 finished with value: 0.24739727892246344 and parameters: {'observation_period_num': 74, 'train_rates': 0.7947451568416485, 'learning_rate': 1.3326779654446898e-05, 'batch_size': 88, 'step_size': 8, 'gamma': 0.7619201226043444}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:35:31,515][0m Trial 14 finished with value: 0.09077978531055361 and parameters: {'observation_period_num': 99, 'train_rates': 0.887654881983726, 'learning_rate': 9.712742686778113e-06, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9252211220177006}. Best is trial 4 with value: 0.05539198509918956.[0m
[32m[I 2025-01-07 01:36:11,422][0m Trial 15 finished with value: 0.043851088732481 and parameters: {'observation_period_num': 40, 'train_rates': 0.8447403077446588, 'learning_rate': 0.0003632540373562416, 'batch_size': 248, 'step_size': 7, 'gamma': 0.87615864868608}. Best is trial 15 with value: 0.043851088732481.[0m
[32m[I 2025-01-07 01:36:51,456][0m Trial 16 finished with value: 0.04108464959318991 and parameters: {'observation_period_num': 33, 'train_rates': 0.8151934065139044, 'learning_rate': 0.00037974032365062366, 'batch_size': 235, 'step_size': 15, 'gamma': 0.8347245444796174}. Best is trial 16 with value: 0.04108464959318991.[0m
[32m[I 2025-01-07 01:37:27,229][0m Trial 17 finished with value: 0.05072633373707079 and parameters: {'observation_period_num': 37, 'train_rates': 0.79744771903873, 'learning_rate': 0.0008278576675628024, 'batch_size': 255, 'step_size': 10, 'gamma': 0.8317574662122372}. Best is trial 16 with value: 0.04108464959318991.[0m
[32m[I 2025-01-07 01:38:18,844][0m Trial 18 finished with value: 0.05835719996101254 and parameters: {'observation_period_num': 37, 'train_rates': 0.7504282081084028, 'learning_rate': 0.00036502528178591784, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8932559046213766}. Best is trial 16 with value: 0.04108464959318991.[0m
[32m[I 2025-01-07 01:39:12,732][0m Trial 19 finished with value: 0.03900440945482451 and parameters: {'observation_period_num': 12, 'train_rates': 0.8354173797251025, 'learning_rate': 0.0003694162820130072, 'batch_size': 230, 'step_size': 9, 'gamma': 0.7784980614432849}. Best is trial 19 with value: 0.03900440945482451.[0m
[32m[I 2025-01-07 01:39:48,047][0m Trial 20 finished with value: 0.07014781352869315 and parameters: {'observation_period_num': 16, 'train_rates': 0.7554987077275911, 'learning_rate': 5.444324242146323e-05, 'batch_size': 224, 'step_size': 15, 'gamma': 0.7545803118275844}. Best is trial 19 with value: 0.03900440945482451.[0m
[32m[I 2025-01-07 01:40:22,404][0m Trial 21 finished with value: 0.04217323127593244 and parameters: {'observation_period_num': 8, 'train_rates': 0.8316402919198435, 'learning_rate': 0.00036622454709024616, 'batch_size': 226, 'step_size': 6, 'gamma': 0.7818405313522332}. Best is trial 19 with value: 0.03900440945482451.[0m
[32m[I 2025-01-07 01:40:56,788][0m Trial 22 finished with value: 0.045097006846943455 and parameters: {'observation_period_num': 9, 'train_rates': 0.830438330329936, 'learning_rate': 0.0002220722799545842, 'batch_size': 230, 'step_size': 10, 'gamma': 0.7797790599564383}. Best is trial 19 with value: 0.03900440945482451.[0m
[32m[I 2025-01-07 01:41:40,677][0m Trial 23 finished with value: 0.03870662141434583 and parameters: {'observation_period_num': 6, 'train_rates': 0.790159405427964, 'learning_rate': 0.0005787438630776, 'batch_size': 224, 'step_size': 5, 'gamma': 0.8268161865286158}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:42:13,277][0m Trial 24 finished with value: 0.049857391320662546 and parameters: {'observation_period_num': 52, 'train_rates': 0.7813110395082523, 'learning_rate': 0.0007127201617804715, 'batch_size': 213, 'step_size': 9, 'gamma': 0.8368660720603792}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:42:50,618][0m Trial 25 finished with value: 0.04530921911744915 and parameters: {'observation_period_num': 30, 'train_rates': 0.6975038179653862, 'learning_rate': 0.0005125300826520712, 'batch_size': 166, 'step_size': 12, 'gamma': 0.8217628535056163}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:43:30,223][0m Trial 26 finished with value: 0.16728038327865505 and parameters: {'observation_period_num': 97, 'train_rates': 0.7176729540173096, 'learning_rate': 0.0002145251198229769, 'batch_size': 230, 'step_size': 4, 'gamma': 0.7943620585823211}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:44:12,818][0m Trial 27 finished with value: 0.06396695430049244 and parameters: {'observation_period_num': 53, 'train_rates': 0.7780667521660516, 'learning_rate': 0.00011951895528374324, 'batch_size': 144, 'step_size': 13, 'gamma': 0.7686008551420815}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:44:46,900][0m Trial 28 finished with value: 0.08029468862029414 and parameters: {'observation_period_num': 91, 'train_rates': 0.8659563154434079, 'learning_rate': 0.0005294342003464337, 'batch_size': 236, 'step_size': 3, 'gamma': 0.8412747023530032}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:45:18,511][0m Trial 29 finished with value: 0.21928538874679032 and parameters: {'observation_period_num': 252, 'train_rates': 0.8195995254528163, 'learning_rate': 5.961326401366431e-05, 'batch_size': 205, 'step_size': 11, 'gamma': 0.8094181545540213}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:45:57,000][0m Trial 30 finished with value: 0.04433776106813858 and parameters: {'observation_period_num': 7, 'train_rates': 0.8032334368667388, 'learning_rate': 0.00024641302682924224, 'batch_size': 154, 'step_size': 5, 'gamma': 0.7921608437260735}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:46:31,359][0m Trial 31 finished with value: 0.04265495586921187 and parameters: {'observation_period_num': 5, 'train_rates': 0.8502134186882131, 'learning_rate': 0.0003965854642073664, 'batch_size': 215, 'step_size': 6, 'gamma': 0.781638340377264}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:47:08,026][0m Trial 32 finished with value: 0.0454455129802227 and parameters: {'observation_period_num': 26, 'train_rates': 0.9215315393545369, 'learning_rate': 0.0005162879379138223, 'batch_size': 239, 'step_size': 6, 'gamma': 0.8236571069018936}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:47:46,477][0m Trial 33 finished with value: 0.05962485625845561 and parameters: {'observation_period_num': 58, 'train_rates': 0.8542535045061517, 'learning_rate': 0.0002738831987956036, 'batch_size': 196, 'step_size': 6, 'gamma': 0.7679397814908562}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:48:19,033][0m Trial 34 finished with value: 0.08356439295975647 and parameters: {'observation_period_num': 29, 'train_rates': 0.7671999909086331, 'learning_rate': 0.00013765352072285305, 'batch_size': 217, 'step_size': 3, 'gamma': 0.8515738937916756}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:48:48,146][0m Trial 35 finished with value: 0.08862759504797217 and parameters: {'observation_period_num': 47, 'train_rates': 0.7320623115472253, 'learning_rate': 0.0006561713038702678, 'batch_size': 241, 'step_size': 4, 'gamma': 0.8035137856063257}. Best is trial 23 with value: 0.03870662141434583.[0m
[32m[I 2025-01-07 01:49:22,887][0m Trial 36 finished with value: 0.032847659085256935 and parameters: {'observation_period_num': 20, 'train_rates': 0.8161683148593006, 'learning_rate': 0.0009538739877417893, 'batch_size': 186, 'step_size': 9, 'gamma': 0.7763378140216439}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:49:57,433][0m Trial 37 finished with value: 0.11983331469870404 and parameters: {'observation_period_num': 143, 'train_rates': 0.8068658023051563, 'learning_rate': 0.0009614906931302789, 'batch_size': 186, 'step_size': 12, 'gamma': 0.8640192396452931}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:50:35,903][0m Trial 38 finished with value: 0.14615111198103012 and parameters: {'observation_period_num': 227, 'train_rates': 0.87374804556584, 'learning_rate': 0.00017355384066467585, 'batch_size': 167, 'step_size': 9, 'gamma': 0.8190504871844247}. Best is trial 36 with value: 0.032847659085256935.[0m
Early stopping at epoch 57
[32m[I 2025-01-07 01:50:55,459][0m Trial 39 finished with value: 0.4892658646910722 and parameters: {'observation_period_num': 63, 'train_rates': 0.7034458692637859, 'learning_rate': 7.93154721634557e-05, 'batch_size': 198, 'step_size': 1, 'gamma': 0.801622596539164}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:51:34,446][0m Trial 40 finished with value: 0.06295903011914192 and parameters: {'observation_period_num': 117, 'train_rates': 0.91336500707926, 'learning_rate': 0.0005787560424987462, 'batch_size': 178, 'step_size': 9, 'gamma': 0.7538467275184381}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:52:06,395][0m Trial 41 finished with value: 0.05343023384058917 and parameters: {'observation_period_num': 17, 'train_rates': 0.8162744792088695, 'learning_rate': 0.0003454649353857651, 'batch_size': 220, 'step_size': 5, 'gamma': 0.7768735266732372}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:52:39,986][0m Trial 42 finished with value: 0.04501746481527453 and parameters: {'observation_period_num': 22, 'train_rates': 0.8353706127700731, 'learning_rate': 0.00042512828025218174, 'batch_size': 210, 'step_size': 7, 'gamma': 0.7852373792413344}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:53:11,478][0m Trial 43 finished with value: 0.05102034321370216 and parameters: {'observation_period_num': 43, 'train_rates': 0.7847965637415503, 'learning_rate': 0.00028404462057641627, 'batch_size': 242, 'step_size': 14, 'gamma': 0.8146582261999689}. Best is trial 36 with value: 0.032847659085256935.[0m
[32m[I 2025-01-07 01:53:45,963][0m Trial 44 finished with value: 0.028819884068034516 and parameters: {'observation_period_num': 6, 'train_rates': 0.8293946467396848, 'learning_rate': 0.0009206539859814936, 'batch_size': 193, 'step_size': 8, 'gamma': 0.8445158823309186}. Best is trial 44 with value: 0.028819884068034516.[0m
[32m[I 2025-01-07 01:54:18,336][0m Trial 45 finished with value: 0.04759452561913084 and parameters: {'observation_period_num': 27, 'train_rates': 0.8779443951713729, 'learning_rate': 0.0009498722135690405, 'batch_size': 191, 'step_size': 2, 'gamma': 0.8438838011386536}. Best is trial 44 with value: 0.028819884068034516.[0m
[32m[I 2025-01-07 01:55:11,426][0m Trial 46 finished with value: 0.05569631511464101 and parameters: {'observation_period_num': 34, 'train_rates': 0.8170159032988922, 'learning_rate': 0.0006924761233147429, 'batch_size': 106, 'step_size': 11, 'gamma': 0.8769585847803515}. Best is trial 44 with value: 0.028819884068034516.[0m
[32m[I 2025-01-07 01:55:47,796][0m Trial 47 finished with value: 0.5545810928565549 and parameters: {'observation_period_num': 19, 'train_rates': 0.7644313215266262, 'learning_rate': 3.266655859509266e-06, 'batch_size': 205, 'step_size': 8, 'gamma': 0.8626957792020047}. Best is trial 44 with value: 0.028819884068034516.[0m
[32m[I 2025-01-07 01:56:26,397][0m Trial 48 finished with value: 0.20556474754543616 and parameters: {'observation_period_num': 203, 'train_rates': 0.7335110747439028, 'learning_rate': 0.0006419818486212219, 'batch_size': 153, 'step_size': 13, 'gamma': 0.9207172790862875}. Best is trial 44 with value: 0.028819884068034516.[0m
[32m[I 2025-01-07 01:57:07,664][0m Trial 49 finished with value: 0.07512620357280606 and parameters: {'observation_period_num': 77, 'train_rates': 0.8518834289265691, 'learning_rate': 0.00018528821342130342, 'batch_size': 173, 'step_size': 9, 'gamma': 0.8320974716149026}. Best is trial 44 with value: 0.028819884068034516.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 41, 'train_rates': 0.9886700245714038, 'learning_rate': 0.0006447926509531084, 'batch_size': 79, 'step_size': 4, 'gamma': 0.902993793912314}
Epoch 1/300, trend Loss: 0.3121 | 0.2052
Epoch 2/300, trend Loss: 0.1810 | 0.1714
Epoch 3/300, trend Loss: 0.1352 | 0.1250
Epoch 4/300, trend Loss: 0.1162 | 0.1143
Epoch 5/300, trend Loss: 0.1097 | 0.0882
Epoch 6/300, trend Loss: 0.1092 | 0.1177
Epoch 7/300, trend Loss: 0.1090 | 0.0696
Epoch 8/300, trend Loss: 0.1089 | 0.1122
Epoch 9/300, trend Loss: 0.1083 | 0.0763
Epoch 10/300, trend Loss: 0.0926 | 0.0799
Epoch 11/300, trend Loss: 0.0938 | 0.0906
Epoch 12/300, trend Loss: 0.0966 | 0.0864
Epoch 13/300, trend Loss: 0.0929 | 0.0999
Epoch 14/300, trend Loss: 0.0951 | 0.0988
Epoch 15/300, trend Loss: 0.0986 | 0.0952
Epoch 16/300, trend Loss: 0.1175 | 0.1044
Epoch 17/300, trend Loss: 0.0987 | 0.0653
Epoch 18/300, trend Loss: 0.0924 | 0.0620
Epoch 19/300, trend Loss: 0.1036 | 0.0778
Epoch 20/300, trend Loss: 0.1163 | 0.0938
Epoch 21/300, trend Loss: 0.1346 | 0.3811
Epoch 22/300, trend Loss: 0.1298 | 0.5997
Epoch 23/300, trend Loss: 0.1162 | 0.2258
Epoch 24/300, trend Loss: 0.0884 | 0.1606
Epoch 25/300, trend Loss: 0.0776 | 0.1273
Epoch 26/300, trend Loss: 0.0727 | 0.0984
Epoch 27/300, trend Loss: 0.0717 | 0.0971
Epoch 28/300, trend Loss: 0.0683 | 0.0962
Epoch 29/300, trend Loss: 0.0688 | 0.1226
Epoch 30/300, trend Loss: 0.0693 | 0.1372
Epoch 31/300, trend Loss: 0.0676 | 0.1369
Epoch 32/300, trend Loss: 0.0650 | 0.1040
Epoch 33/300, trend Loss: 0.0655 | 0.0744
Epoch 34/300, trend Loss: 0.0703 | 0.0574
Epoch 35/300, trend Loss: 0.0722 | 0.0460
Epoch 36/300, trend Loss: 0.0638 | 0.0468
Epoch 37/300, trend Loss: 0.0625 | 0.0443
Epoch 38/300, trend Loss: 0.0618 | 0.0388
Epoch 39/300, trend Loss: 0.0606 | 0.0370
Epoch 40/300, trend Loss: 0.0599 | 0.0362
Epoch 41/300, trend Loss: 0.0592 | 0.0373
Epoch 42/300, trend Loss: 0.0588 | 0.0395
Epoch 43/300, trend Loss: 0.0584 | 0.0409
Epoch 44/300, trend Loss: 0.0582 | 0.0393
Epoch 45/300, trend Loss: 0.0582 | 0.0361
Epoch 46/300, trend Loss: 0.0582 | 0.0345
Epoch 47/300, trend Loss: 0.0579 | 0.0339
Epoch 48/300, trend Loss: 0.0575 | 0.0342
Epoch 49/300, trend Loss: 0.0572 | 0.0362
Epoch 50/300, trend Loss: 0.0571 | 0.0383
Epoch 51/300, trend Loss: 0.0570 | 0.0384
Epoch 52/300, trend Loss: 0.0568 | 0.0358
Epoch 53/300, trend Loss: 0.0564 | 0.0339
Epoch 54/300, trend Loss: 0.0561 | 0.0335
Epoch 55/300, trend Loss: 0.0559 | 0.0340
Epoch 56/300, trend Loss: 0.0558 | 0.0348
Epoch 57/300, trend Loss: 0.0557 | 0.0348
Epoch 58/300, trend Loss: 0.0555 | 0.0340
Epoch 59/300, trend Loss: 0.0554 | 0.0334
Epoch 60/300, trend Loss: 0.0552 | 0.0333
Epoch 61/300, trend Loss: 0.0551 | 0.0334
Epoch 62/300, trend Loss: 0.0550 | 0.0333
Epoch 63/300, trend Loss: 0.0549 | 0.0331
Epoch 64/300, trend Loss: 0.0548 | 0.0331
Epoch 65/300, trend Loss: 0.0547 | 0.0330
Epoch 66/300, trend Loss: 0.0546 | 0.0330
Epoch 67/300, trend Loss: 0.0545 | 0.0329
Epoch 68/300, trend Loss: 0.0545 | 0.0328
Epoch 69/300, trend Loss: 0.0544 | 0.0328
Epoch 70/300, trend Loss: 0.0543 | 0.0327
Epoch 71/300, trend Loss: 0.0542 | 0.0326
Epoch 72/300, trend Loss: 0.0542 | 0.0326
Epoch 73/300, trend Loss: 0.0541 | 0.0325
Epoch 74/300, trend Loss: 0.0541 | 0.0325
Epoch 75/300, trend Loss: 0.0540 | 0.0324
Epoch 76/300, trend Loss: 0.0540 | 0.0324
Epoch 77/300, trend Loss: 0.0539 | 0.0324
Epoch 78/300, trend Loss: 0.0539 | 0.0323
Epoch 79/300, trend Loss: 0.0538 | 0.0323
Epoch 80/300, trend Loss: 0.0538 | 0.0322
Epoch 81/300, trend Loss: 0.0537 | 0.0322
Epoch 82/300, trend Loss: 0.0537 | 0.0322
Epoch 83/300, trend Loss: 0.0536 | 0.0321
Epoch 84/300, trend Loss: 0.0536 | 0.0321
Epoch 85/300, trend Loss: 0.0536 | 0.0321
Epoch 86/300, trend Loss: 0.0535 | 0.0320
Epoch 87/300, trend Loss: 0.0535 | 0.0320
Epoch 88/300, trend Loss: 0.0535 | 0.0320
Epoch 89/300, trend Loss: 0.0534 | 0.0320
Epoch 90/300, trend Loss: 0.0534 | 0.0319
Epoch 91/300, trend Loss: 0.0534 | 0.0319
Epoch 92/300, trend Loss: 0.0533 | 0.0319
Epoch 93/300, trend Loss: 0.0533 | 0.0319
Epoch 94/300, trend Loss: 0.0533 | 0.0318
Epoch 95/300, trend Loss: 0.0533 | 0.0318
Epoch 96/300, trend Loss: 0.0532 | 0.0318
Epoch 97/300, trend Loss: 0.0532 | 0.0318
Epoch 98/300, trend Loss: 0.0532 | 0.0318
Epoch 99/300, trend Loss: 0.0532 | 0.0318
Epoch 100/300, trend Loss: 0.0531 | 0.0317
Epoch 101/300, trend Loss: 0.0531 | 0.0317
Epoch 102/300, trend Loss: 0.0531 | 0.0317
Epoch 103/300, trend Loss: 0.0531 | 0.0317
Epoch 104/300, trend Loss: 0.0531 | 0.0317
Epoch 105/300, trend Loss: 0.0531 | 0.0317
Epoch 106/300, trend Loss: 0.0530 | 0.0317
Epoch 107/300, trend Loss: 0.0530 | 0.0316
Epoch 108/300, trend Loss: 0.0530 | 0.0316
Epoch 109/300, trend Loss: 0.0530 | 0.0316
Epoch 110/300, trend Loss: 0.0530 | 0.0316
Epoch 111/300, trend Loss: 0.0530 | 0.0316
Epoch 112/300, trend Loss: 0.0530 | 0.0316
Epoch 113/300, trend Loss: 0.0530 | 0.0316
Epoch 114/300, trend Loss: 0.0529 | 0.0316
Epoch 115/300, trend Loss: 0.0529 | 0.0316
Epoch 116/300, trend Loss: 0.0529 | 0.0316
Epoch 117/300, trend Loss: 0.0529 | 0.0316
Epoch 118/300, trend Loss: 0.0529 | 0.0315
Epoch 119/300, trend Loss: 0.0529 | 0.0315
Epoch 120/300, trend Loss: 0.0529 | 0.0315
Epoch 121/300, trend Loss: 0.0529 | 0.0315
Epoch 122/300, trend Loss: 0.0529 | 0.0315
Epoch 123/300, trend Loss: 0.0529 | 0.0315
Epoch 124/300, trend Loss: 0.0529 | 0.0315
Epoch 125/300, trend Loss: 0.0529 | 0.0315
Epoch 126/300, trend Loss: 0.0528 | 0.0315
Epoch 127/300, trend Loss: 0.0528 | 0.0315
Epoch 128/300, trend Loss: 0.0528 | 0.0315
Epoch 129/300, trend Loss: 0.0528 | 0.0315
Epoch 130/300, trend Loss: 0.0528 | 0.0315
Epoch 131/300, trend Loss: 0.0528 | 0.0315
Epoch 132/300, trend Loss: 0.0528 | 0.0315
Epoch 133/300, trend Loss: 0.0528 | 0.0315
Epoch 134/300, trend Loss: 0.0528 | 0.0315
Epoch 135/300, trend Loss: 0.0528 | 0.0315
Epoch 136/300, trend Loss: 0.0528 | 0.0315
Epoch 137/300, trend Loss: 0.0528 | 0.0315
Epoch 138/300, trend Loss: 0.0528 | 0.0314
Epoch 139/300, trend Loss: 0.0528 | 0.0314
Epoch 140/300, trend Loss: 0.0528 | 0.0314
Epoch 141/300, trend Loss: 0.0528 | 0.0314
Epoch 142/300, trend Loss: 0.0528 | 0.0314
Epoch 143/300, trend Loss: 0.0528 | 0.0314
Epoch 144/300, trend Loss: 0.0528 | 0.0314
Epoch 145/300, trend Loss: 0.0528 | 0.0314
Epoch 146/300, trend Loss: 0.0528 | 0.0314
Epoch 147/300, trend Loss: 0.0528 | 0.0314
Epoch 148/300, trend Loss: 0.0528 | 0.0314
Epoch 149/300, trend Loss: 0.0528 | 0.0314
Epoch 150/300, trend Loss: 0.0528 | 0.0314
Epoch 151/300, trend Loss: 0.0528 | 0.0314
Epoch 152/300, trend Loss: 0.0528 | 0.0314
Epoch 153/300, trend Loss: 0.0528 | 0.0314
Epoch 154/300, trend Loss: 0.0527 | 0.0314
Epoch 155/300, trend Loss: 0.0527 | 0.0314
Epoch 156/300, trend Loss: 0.0527 | 0.0314
Epoch 157/300, trend Loss: 0.0527 | 0.0314
Epoch 158/300, trend Loss: 0.0527 | 0.0314
Epoch 159/300, trend Loss: 0.0527 | 0.0314
Epoch 160/300, trend Loss: 0.0527 | 0.0314
Epoch 161/300, trend Loss: 0.0527 | 0.0314
Epoch 162/300, trend Loss: 0.0527 | 0.0314
Epoch 163/300, trend Loss: 0.0527 | 0.0314
Epoch 164/300, trend Loss: 0.0527 | 0.0314
Epoch 165/300, trend Loss: 0.0527 | 0.0314
Epoch 166/300, trend Loss: 0.0527 | 0.0314
Epoch 167/300, trend Loss: 0.0527 | 0.0314
Epoch 168/300, trend Loss: 0.0527 | 0.0314
Epoch 169/300, trend Loss: 0.0527 | 0.0314
Epoch 170/300, trend Loss: 0.0527 | 0.0314
Epoch 171/300, trend Loss: 0.0527 | 0.0314
Epoch 172/300, trend Loss: 0.0527 | 0.0314
Epoch 173/300, trend Loss: 0.0527 | 0.0314
Epoch 174/300, trend Loss: 0.0527 | 0.0314
Epoch 175/300, trend Loss: 0.0527 | 0.0314
Epoch 176/300, trend Loss: 0.0527 | 0.0314
Epoch 177/300, trend Loss: 0.0527 | 0.0314
Epoch 178/300, trend Loss: 0.0527 | 0.0314
Epoch 179/300, trend Loss: 0.0527 | 0.0314
Epoch 180/300, trend Loss: 0.0527 | 0.0314
Epoch 181/300, trend Loss: 0.0527 | 0.0314
Epoch 182/300, trend Loss: 0.0527 | 0.0314
Epoch 183/300, trend Loss: 0.0527 | 0.0314
Epoch 184/300, trend Loss: 0.0527 | 0.0314
Epoch 185/300, trend Loss: 0.0527 | 0.0314
Epoch 186/300, trend Loss: 0.0527 | 0.0314
Epoch 187/300, trend Loss: 0.0527 | 0.0314
Epoch 188/300, trend Loss: 0.0527 | 0.0314
Epoch 189/300, trend Loss: 0.0527 | 0.0314
Epoch 190/300, trend Loss: 0.0527 | 0.0314
Epoch 191/300, trend Loss: 0.0527 | 0.0314
Epoch 192/300, trend Loss: 0.0527 | 0.0314
Epoch 193/300, trend Loss: 0.0527 | 0.0314
Epoch 194/300, trend Loss: 0.0527 | 0.0314
Epoch 195/300, trend Loss: 0.0527 | 0.0314
Epoch 196/300, trend Loss: 0.0527 | 0.0314
Epoch 197/300, trend Loss: 0.0527 | 0.0314
Epoch 198/300, trend Loss: 0.0527 | 0.0314
Epoch 199/300, trend Loss: 0.0527 | 0.0314
Epoch 200/300, trend Loss: 0.0527 | 0.0314
Epoch 201/300, trend Loss: 0.0527 | 0.0314
Epoch 202/300, trend Loss: 0.0527 | 0.0314
Epoch 203/300, trend Loss: 0.0527 | 0.0314
Epoch 204/300, trend Loss: 0.0527 | 0.0314
Epoch 205/300, trend Loss: 0.0527 | 0.0314
Epoch 206/300, trend Loss: 0.0527 | 0.0314
Epoch 207/300, trend Loss: 0.0527 | 0.0314
Epoch 208/300, trend Loss: 0.0527 | 0.0314
Epoch 209/300, trend Loss: 0.0527 | 0.0314
Epoch 210/300, trend Loss: 0.0527 | 0.0314
Epoch 211/300, trend Loss: 0.0527 | 0.0314
Epoch 212/300, trend Loss: 0.0527 | 0.0314
Epoch 213/300, trend Loss: 0.0527 | 0.0314
Epoch 214/300, trend Loss: 0.0527 | 0.0314
Epoch 215/300, trend Loss: 0.0527 | 0.0314
Epoch 216/300, trend Loss: 0.0527 | 0.0314
Epoch 217/300, trend Loss: 0.0527 | 0.0314
Epoch 218/300, trend Loss: 0.0527 | 0.0314
Epoch 219/300, trend Loss: 0.0527 | 0.0314
Epoch 220/300, trend Loss: 0.0527 | 0.0314
Epoch 221/300, trend Loss: 0.0527 | 0.0314
Epoch 222/300, trend Loss: 0.0527 | 0.0314
Epoch 223/300, trend Loss: 0.0527 | 0.0314
Epoch 224/300, trend Loss: 0.0527 | 0.0314
Epoch 225/300, trend Loss: 0.0527 | 0.0314
Epoch 226/300, trend Loss: 0.0527 | 0.0314
Epoch 227/300, trend Loss: 0.0527 | 0.0314
Epoch 228/300, trend Loss: 0.0527 | 0.0314
Epoch 229/300, trend Loss: 0.0527 | 0.0314
Epoch 230/300, trend Loss: 0.0527 | 0.0314
Epoch 231/300, trend Loss: 0.0527 | 0.0314
Epoch 232/300, trend Loss: 0.0527 | 0.0314
Epoch 233/300, trend Loss: 0.0527 | 0.0314
Epoch 234/300, trend Loss: 0.0527 | 0.0314
Epoch 235/300, trend Loss: 0.0527 | 0.0314
Epoch 236/300, trend Loss: 0.0527 | 0.0314
Epoch 237/300, trend Loss: 0.0527 | 0.0314
Epoch 238/300, trend Loss: 0.0527 | 0.0314
Epoch 239/300, trend Loss: 0.0527 | 0.0314
Epoch 240/300, trend Loss: 0.0527 | 0.0314
Epoch 241/300, trend Loss: 0.0527 | 0.0314
Epoch 242/300, trend Loss: 0.0527 | 0.0314
Epoch 243/300, trend Loss: 0.0527 | 0.0314
Epoch 244/300, trend Loss: 0.0527 | 0.0314
Epoch 245/300, trend Loss: 0.0527 | 0.0314
Epoch 246/300, trend Loss: 0.0527 | 0.0314
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 18, 'train_rates': 0.9898785635671847, 'learning_rate': 0.0006987153438608318, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8413606396265654}
Epoch 1/300, seasonal_0 Loss: 0.2146 | 0.1227
Epoch 2/300, seasonal_0 Loss: 0.1226 | 0.0882
Epoch 3/300, seasonal_0 Loss: 0.1121 | 0.0822
Epoch 4/300, seasonal_0 Loss: 0.1050 | 0.0701
Epoch 5/300, seasonal_0 Loss: 0.0989 | 0.0638
Epoch 6/300, seasonal_0 Loss: 0.0961 | 0.0655
Epoch 7/300, seasonal_0 Loss: 0.0918 | 0.0778
Epoch 8/300, seasonal_0 Loss: 0.0898 | 0.0592
Epoch 9/300, seasonal_0 Loss: 0.0824 | 0.0454
Epoch 10/300, seasonal_0 Loss: 0.0741 | 0.0445
Epoch 11/300, seasonal_0 Loss: 0.0711 | 0.0372
Epoch 12/300, seasonal_0 Loss: 0.0679 | 0.0380
Epoch 13/300, seasonal_0 Loss: 0.0665 | 0.0366
Epoch 14/300, seasonal_0 Loss: 0.0643 | 0.0318
Epoch 15/300, seasonal_0 Loss: 0.0629 | 0.0318
Epoch 16/300, seasonal_0 Loss: 0.0619 | 0.0296
Epoch 17/300, seasonal_0 Loss: 0.0607 | 0.0301
Epoch 18/300, seasonal_0 Loss: 0.0600 | 0.0291
Epoch 19/300, seasonal_0 Loss: 0.0587 | 0.0274
Epoch 20/300, seasonal_0 Loss: 0.0580 | 0.0272
Epoch 21/300, seasonal_0 Loss: 0.0562 | 0.0262
Epoch 22/300, seasonal_0 Loss: 0.0550 | 0.0248
Epoch 23/300, seasonal_0 Loss: 0.0537 | 0.0234
Epoch 24/300, seasonal_0 Loss: 0.0531 | 0.0226
Epoch 25/300, seasonal_0 Loss: 0.0528 | 0.0219
Epoch 26/300, seasonal_0 Loss: 0.0524 | 0.0222
Epoch 27/300, seasonal_0 Loss: 0.0525 | 0.0223
Epoch 28/300, seasonal_0 Loss: 0.0516 | 0.0220
Epoch 29/300, seasonal_0 Loss: 0.0509 | 0.0217
Epoch 30/300, seasonal_0 Loss: 0.0504 | 0.0219
Epoch 31/300, seasonal_0 Loss: 0.0501 | 0.0217
Epoch 32/300, seasonal_0 Loss: 0.0497 | 0.0218
Epoch 33/300, seasonal_0 Loss: 0.0494 | 0.0217
Epoch 34/300, seasonal_0 Loss: 0.0490 | 0.0216
Epoch 35/300, seasonal_0 Loss: 0.0488 | 0.0215
Epoch 36/300, seasonal_0 Loss: 0.0486 | 0.0220
Epoch 37/300, seasonal_0 Loss: 0.0483 | 0.0223
Epoch 38/300, seasonal_0 Loss: 0.0478 | 0.0230
Epoch 39/300, seasonal_0 Loss: 0.0475 | 0.0222
Epoch 40/300, seasonal_0 Loss: 0.0476 | 0.0221
Epoch 41/300, seasonal_0 Loss: 0.0475 | 0.0233
Epoch 42/300, seasonal_0 Loss: 0.0474 | 0.0235
Epoch 43/300, seasonal_0 Loss: 0.0470 | 0.0236
Epoch 44/300, seasonal_0 Loss: 0.0468 | 0.0246
Epoch 45/300, seasonal_0 Loss: 0.0468 | 0.0248
Epoch 46/300, seasonal_0 Loss: 0.0470 | 0.0237
Epoch 47/300, seasonal_0 Loss: 0.0475 | 0.0232
Epoch 48/300, seasonal_0 Loss: 0.0476 | 0.0226
Epoch 49/300, seasonal_0 Loss: 0.0467 | 0.0224
Epoch 50/300, seasonal_0 Loss: 0.0458 | 0.0221
Epoch 51/300, seasonal_0 Loss: 0.0454 | 0.0221
Epoch 52/300, seasonal_0 Loss: 0.0453 | 0.0219
Epoch 53/300, seasonal_0 Loss: 0.0452 | 0.0217
Epoch 54/300, seasonal_0 Loss: 0.0451 | 0.0216
Epoch 55/300, seasonal_0 Loss: 0.0450 | 0.0214
Epoch 56/300, seasonal_0 Loss: 0.0450 | 0.0214
Epoch 57/300, seasonal_0 Loss: 0.0450 | 0.0212
Epoch 58/300, seasonal_0 Loss: 0.0448 | 0.0210
Epoch 59/300, seasonal_0 Loss: 0.0449 | 0.0209
Epoch 60/300, seasonal_0 Loss: 0.0448 | 0.0208
Epoch 61/300, seasonal_0 Loss: 0.0448 | 0.0206
Epoch 62/300, seasonal_0 Loss: 0.0447 | 0.0204
Epoch 63/300, seasonal_0 Loss: 0.0446 | 0.0203
Epoch 64/300, seasonal_0 Loss: 0.0446 | 0.0199
Epoch 65/300, seasonal_0 Loss: 0.0445 | 0.0198
Epoch 66/300, seasonal_0 Loss: 0.0444 | 0.0195
Epoch 67/300, seasonal_0 Loss: 0.0443 | 0.0196
Epoch 68/300, seasonal_0 Loss: 0.0441 | 0.0196
Epoch 69/300, seasonal_0 Loss: 0.0440 | 0.0195
Epoch 70/300, seasonal_0 Loss: 0.0439 | 0.0196
Epoch 71/300, seasonal_0 Loss: 0.0438 | 0.0195
Epoch 72/300, seasonal_0 Loss: 0.0437 | 0.0196
Epoch 73/300, seasonal_0 Loss: 0.0436 | 0.0197
Epoch 74/300, seasonal_0 Loss: 0.0435 | 0.0196
Epoch 75/300, seasonal_0 Loss: 0.0434 | 0.0197
Epoch 76/300, seasonal_0 Loss: 0.0434 | 0.0196
Epoch 77/300, seasonal_0 Loss: 0.0433 | 0.0197
Epoch 78/300, seasonal_0 Loss: 0.0433 | 0.0197
Epoch 79/300, seasonal_0 Loss: 0.0433 | 0.0197
Epoch 80/300, seasonal_0 Loss: 0.0432 | 0.0197
Epoch 81/300, seasonal_0 Loss: 0.0432 | 0.0197
Epoch 82/300, seasonal_0 Loss: 0.0432 | 0.0197
Epoch 83/300, seasonal_0 Loss: 0.0431 | 0.0198
Epoch 84/300, seasonal_0 Loss: 0.0431 | 0.0198
Epoch 85/300, seasonal_0 Loss: 0.0431 | 0.0198
Epoch 86/300, seasonal_0 Loss: 0.0430 | 0.0198
Epoch 87/300, seasonal_0 Loss: 0.0430 | 0.0198
Epoch 88/300, seasonal_0 Loss: 0.0430 | 0.0198
Epoch 89/300, seasonal_0 Loss: 0.0430 | 0.0198
Epoch 90/300, seasonal_0 Loss: 0.0430 | 0.0198
Epoch 91/300, seasonal_0 Loss: 0.0430 | 0.0198
Epoch 92/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 93/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 94/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 95/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 96/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 97/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 98/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 99/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 100/300, seasonal_0 Loss: 0.0429 | 0.0198
Epoch 101/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 102/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 103/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 104/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 105/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 106/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 107/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 108/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 109/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 110/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 111/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 112/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 113/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 114/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 115/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 116/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 117/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 118/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 119/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 120/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 121/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 122/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 123/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 124/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 125/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 126/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 127/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 128/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 129/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 130/300, seasonal_0 Loss: 0.0428 | 0.0198
Epoch 131/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 132/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 133/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 134/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 135/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 136/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 137/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 138/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 139/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 140/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 141/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 142/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 143/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 144/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 145/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 146/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 147/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 148/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 149/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 150/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 151/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 152/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 153/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 154/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 155/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 156/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 157/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 158/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 159/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 160/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 161/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 162/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 163/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 164/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 165/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 166/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 167/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 168/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 169/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 170/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 171/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 172/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 173/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 174/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 175/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 176/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 177/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 178/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 179/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 180/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 181/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 182/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 183/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 184/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 185/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 186/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 187/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 188/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 189/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 190/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 191/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 192/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 193/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 194/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 195/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 196/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 197/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 198/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 199/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 200/300, seasonal_0 Loss: 0.0427 | 0.0198
Epoch 201/300, seasonal_0 Loss: 0.0427 | 0.0198
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8143765847789818, 'learning_rate': 0.00010269448924050796, 'batch_size': 54, 'step_size': 15, 'gamma': 0.8176680225309224}
Epoch 1/300, seasonal_1 Loss: 0.3740 | 0.2578
Epoch 2/300, seasonal_1 Loss: 0.1687 | 0.1726
Epoch 3/300, seasonal_1 Loss: 0.1438 | 0.1278
Epoch 4/300, seasonal_1 Loss: 0.1320 | 0.1016
Epoch 5/300, seasonal_1 Loss: 0.1237 | 0.0879
Epoch 6/300, seasonal_1 Loss: 0.1172 | 0.0792
Epoch 7/300, seasonal_1 Loss: 0.1127 | 0.0734
Epoch 8/300, seasonal_1 Loss: 0.1098 | 0.0701
Epoch 9/300, seasonal_1 Loss: 0.1074 | 0.0751
Epoch 10/300, seasonal_1 Loss: 0.1067 | 0.0753
Epoch 11/300, seasonal_1 Loss: 0.1050 | 0.0771
Epoch 12/300, seasonal_1 Loss: 0.1037 | 0.0788
Epoch 13/300, seasonal_1 Loss: 0.1022 | 0.0790
Epoch 14/300, seasonal_1 Loss: 0.1006 | 0.0770
Epoch 15/300, seasonal_1 Loss: 0.0992 | 0.0731
Epoch 16/300, seasonal_1 Loss: 0.0984 | 0.0628
Epoch 17/300, seasonal_1 Loss: 0.0997 | 0.0597
Epoch 18/300, seasonal_1 Loss: 0.1030 | 0.0576
Epoch 19/300, seasonal_1 Loss: 0.1011 | 0.0564
Epoch 20/300, seasonal_1 Loss: 0.0987 | 0.0556
Epoch 21/300, seasonal_1 Loss: 0.0981 | 0.0553
Epoch 22/300, seasonal_1 Loss: 0.0974 | 0.0547
Epoch 23/300, seasonal_1 Loss: 0.0959 | 0.0544
Epoch 24/300, seasonal_1 Loss: 0.0935 | 0.0547
Epoch 25/300, seasonal_1 Loss: 0.0920 | 0.0513
Epoch 26/300, seasonal_1 Loss: 0.0899 | 0.0498
Epoch 27/300, seasonal_1 Loss: 0.0892 | 0.0498
Epoch 28/300, seasonal_1 Loss: 0.0882 | 0.0502
Epoch 29/300, seasonal_1 Loss: 0.0870 | 0.0500
Epoch 30/300, seasonal_1 Loss: 0.0857 | 0.0490
Epoch 31/300, seasonal_1 Loss: 0.0845 | 0.0490
Epoch 32/300, seasonal_1 Loss: 0.0839 | 0.0474
Epoch 33/300, seasonal_1 Loss: 0.0835 | 0.0461
Epoch 34/300, seasonal_1 Loss: 0.0827 | 0.0449
Epoch 35/300, seasonal_1 Loss: 0.0811 | 0.0436
Epoch 36/300, seasonal_1 Loss: 0.0793 | 0.0424
Epoch 37/300, seasonal_1 Loss: 0.0781 | 0.0416
Epoch 38/300, seasonal_1 Loss: 0.0775 | 0.0409
Epoch 39/300, seasonal_1 Loss: 0.0771 | 0.0406
Epoch 40/300, seasonal_1 Loss: 0.0770 | 0.0401
Epoch 41/300, seasonal_1 Loss: 0.0764 | 0.0397
Epoch 42/300, seasonal_1 Loss: 0.0758 | 0.0393
Epoch 43/300, seasonal_1 Loss: 0.0752 | 0.0390
Epoch 44/300, seasonal_1 Loss: 0.0747 | 0.0387
Epoch 45/300, seasonal_1 Loss: 0.0743 | 0.0385
Epoch 46/300, seasonal_1 Loss: 0.0740 | 0.0384
Epoch 47/300, seasonal_1 Loss: 0.0737 | 0.0383
Epoch 48/300, seasonal_1 Loss: 0.0735 | 0.0382
Epoch 49/300, seasonal_1 Loss: 0.0732 | 0.0380
Epoch 50/300, seasonal_1 Loss: 0.0729 | 0.0377
Epoch 51/300, seasonal_1 Loss: 0.0726 | 0.0373
Epoch 52/300, seasonal_1 Loss: 0.0723 | 0.0370
Epoch 53/300, seasonal_1 Loss: 0.0720 | 0.0367
Epoch 54/300, seasonal_1 Loss: 0.0717 | 0.0364
Epoch 55/300, seasonal_1 Loss: 0.0714 | 0.0362
Epoch 56/300, seasonal_1 Loss: 0.0712 | 0.0361
Epoch 57/300, seasonal_1 Loss: 0.0710 | 0.0359
Epoch 58/300, seasonal_1 Loss: 0.0707 | 0.0358
Epoch 59/300, seasonal_1 Loss: 0.0705 | 0.0356
Epoch 60/300, seasonal_1 Loss: 0.0703 | 0.0355
Epoch 61/300, seasonal_1 Loss: 0.0701 | 0.0354
Epoch 62/300, seasonal_1 Loss: 0.0699 | 0.0353
Epoch 63/300, seasonal_1 Loss: 0.0698 | 0.0352
Epoch 64/300, seasonal_1 Loss: 0.0696 | 0.0351
Epoch 65/300, seasonal_1 Loss: 0.0694 | 0.0350
Epoch 66/300, seasonal_1 Loss: 0.0692 | 0.0348
Epoch 67/300, seasonal_1 Loss: 0.0690 | 0.0347
Epoch 68/300, seasonal_1 Loss: 0.0689 | 0.0346
Epoch 69/300, seasonal_1 Loss: 0.0687 | 0.0345
Epoch 70/300, seasonal_1 Loss: 0.0685 | 0.0344
Epoch 71/300, seasonal_1 Loss: 0.0683 | 0.0343
Epoch 72/300, seasonal_1 Loss: 0.0682 | 0.0342
Epoch 73/300, seasonal_1 Loss: 0.0680 | 0.0340
Epoch 74/300, seasonal_1 Loss: 0.0679 | 0.0339
Epoch 75/300, seasonal_1 Loss: 0.0677 | 0.0338
Epoch 76/300, seasonal_1 Loss: 0.0676 | 0.0338
Epoch 77/300, seasonal_1 Loss: 0.0674 | 0.0337
Epoch 78/300, seasonal_1 Loss: 0.0673 | 0.0336
Epoch 79/300, seasonal_1 Loss: 0.0672 | 0.0335
Epoch 80/300, seasonal_1 Loss: 0.0671 | 0.0335
Epoch 81/300, seasonal_1 Loss: 0.0670 | 0.0334
Epoch 82/300, seasonal_1 Loss: 0.0669 | 0.0333
Epoch 83/300, seasonal_1 Loss: 0.0668 | 0.0332
Epoch 84/300, seasonal_1 Loss: 0.0667 | 0.0331
Epoch 85/300, seasonal_1 Loss: 0.0666 | 0.0331
Epoch 86/300, seasonal_1 Loss: 0.0665 | 0.0330
Epoch 87/300, seasonal_1 Loss: 0.0665 | 0.0330
Epoch 88/300, seasonal_1 Loss: 0.0664 | 0.0329
Epoch 89/300, seasonal_1 Loss: 0.0663 | 0.0328
Epoch 90/300, seasonal_1 Loss: 0.0662 | 0.0328
Epoch 91/300, seasonal_1 Loss: 0.0662 | 0.0326
Epoch 92/300, seasonal_1 Loss: 0.0661 | 0.0326
Epoch 93/300, seasonal_1 Loss: 0.0662 | 0.0325
Epoch 94/300, seasonal_1 Loss: 0.0662 | 0.0325
Epoch 95/300, seasonal_1 Loss: 0.0663 | 0.0324
Epoch 96/300, seasonal_1 Loss: 0.0662 | 0.0323
Epoch 97/300, seasonal_1 Loss: 0.0662 | 0.0323
Epoch 98/300, seasonal_1 Loss: 0.0661 | 0.0322
Epoch 99/300, seasonal_1 Loss: 0.0661 | 0.0322
Epoch 100/300, seasonal_1 Loss: 0.0660 | 0.0322
Epoch 101/300, seasonal_1 Loss: 0.0658 | 0.0322
Epoch 102/300, seasonal_1 Loss: 0.0657 | 0.0321
Epoch 103/300, seasonal_1 Loss: 0.0655 | 0.0321
Epoch 104/300, seasonal_1 Loss: 0.0654 | 0.0321
Epoch 105/300, seasonal_1 Loss: 0.0654 | 0.0321
Epoch 106/300, seasonal_1 Loss: 0.0653 | 0.0321
Epoch 107/300, seasonal_1 Loss: 0.0653 | 0.0321
Epoch 108/300, seasonal_1 Loss: 0.0652 | 0.0320
Epoch 109/300, seasonal_1 Loss: 0.0652 | 0.0320
Epoch 110/300, seasonal_1 Loss: 0.0651 | 0.0319
Epoch 111/300, seasonal_1 Loss: 0.0651 | 0.0319
Epoch 112/300, seasonal_1 Loss: 0.0650 | 0.0318
Epoch 113/300, seasonal_1 Loss: 0.0650 | 0.0318
Epoch 114/300, seasonal_1 Loss: 0.0649 | 0.0318
Epoch 115/300, seasonal_1 Loss: 0.0649 | 0.0317
Epoch 116/300, seasonal_1 Loss: 0.0648 | 0.0317
Epoch 117/300, seasonal_1 Loss: 0.0648 | 0.0317
Epoch 118/300, seasonal_1 Loss: 0.0647 | 0.0317
Epoch 119/300, seasonal_1 Loss: 0.0647 | 0.0317
Epoch 120/300, seasonal_1 Loss: 0.0647 | 0.0316
Epoch 121/300, seasonal_1 Loss: 0.0646 | 0.0317
Epoch 122/300, seasonal_1 Loss: 0.0646 | 0.0316
Epoch 123/300, seasonal_1 Loss: 0.0646 | 0.0316
Epoch 124/300, seasonal_1 Loss: 0.0645 | 0.0316
Epoch 125/300, seasonal_1 Loss: 0.0645 | 0.0316
Epoch 126/300, seasonal_1 Loss: 0.0645 | 0.0316
Epoch 127/300, seasonal_1 Loss: 0.0644 | 0.0316
Epoch 128/300, seasonal_1 Loss: 0.0644 | 0.0315
Epoch 129/300, seasonal_1 Loss: 0.0643 | 0.0315
Epoch 130/300, seasonal_1 Loss: 0.0643 | 0.0315
Epoch 131/300, seasonal_1 Loss: 0.0643 | 0.0315
Epoch 132/300, seasonal_1 Loss: 0.0643 | 0.0315
Epoch 133/300, seasonal_1 Loss: 0.0642 | 0.0314
Epoch 134/300, seasonal_1 Loss: 0.0642 | 0.0314
Epoch 135/300, seasonal_1 Loss: 0.0642 | 0.0314
Epoch 136/300, seasonal_1 Loss: 0.0641 | 0.0314
Epoch 137/300, seasonal_1 Loss: 0.0641 | 0.0313
Epoch 138/300, seasonal_1 Loss: 0.0641 | 0.0313
Epoch 139/300, seasonal_1 Loss: 0.0641 | 0.0313
Epoch 140/300, seasonal_1 Loss: 0.0640 | 0.0313
Epoch 141/300, seasonal_1 Loss: 0.0640 | 0.0313
Epoch 142/300, seasonal_1 Loss: 0.0640 | 0.0312
Epoch 143/300, seasonal_1 Loss: 0.0640 | 0.0312
Epoch 144/300, seasonal_1 Loss: 0.0639 | 0.0312
Epoch 145/300, seasonal_1 Loss: 0.0639 | 0.0312
Epoch 146/300, seasonal_1 Loss: 0.0639 | 0.0311
Epoch 147/300, seasonal_1 Loss: 0.0639 | 0.0311
Epoch 148/300, seasonal_1 Loss: 0.0639 | 0.0311
Epoch 149/300, seasonal_1 Loss: 0.0638 | 0.0311
Epoch 150/300, seasonal_1 Loss: 0.0638 | 0.0311
Epoch 151/300, seasonal_1 Loss: 0.0638 | 0.0311
Epoch 152/300, seasonal_1 Loss: 0.0638 | 0.0311
Epoch 153/300, seasonal_1 Loss: 0.0638 | 0.0310
Epoch 154/300, seasonal_1 Loss: 0.0638 | 0.0310
Epoch 155/300, seasonal_1 Loss: 0.0638 | 0.0310
Epoch 156/300, seasonal_1 Loss: 0.0637 | 0.0310
Epoch 157/300, seasonal_1 Loss: 0.0637 | 0.0310
Epoch 158/300, seasonal_1 Loss: 0.0637 | 0.0310
Epoch 159/300, seasonal_1 Loss: 0.0637 | 0.0310
Epoch 160/300, seasonal_1 Loss: 0.0637 | 0.0310
Epoch 161/300, seasonal_1 Loss: 0.0637 | 0.0310
Epoch 162/300, seasonal_1 Loss: 0.0637 | 0.0309
Epoch 163/300, seasonal_1 Loss: 0.0637 | 0.0309
Epoch 164/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 165/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 166/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 167/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 168/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 169/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 170/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 171/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 172/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 173/300, seasonal_1 Loss: 0.0636 | 0.0309
Epoch 174/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 175/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 176/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 177/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 178/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 179/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 180/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 181/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 182/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 183/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 184/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 185/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 186/300, seasonal_1 Loss: 0.0635 | 0.0308
Epoch 187/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 188/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 189/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 190/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 191/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 192/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 193/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 194/300, seasonal_1 Loss: 0.0634 | 0.0308
Epoch 195/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 196/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 197/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 198/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 199/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 200/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 201/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 202/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 203/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 204/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 205/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 206/300, seasonal_1 Loss: 0.0634 | 0.0307
Epoch 207/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 208/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 209/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 210/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 211/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 212/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 213/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 214/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 215/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 216/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 217/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 218/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 219/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 220/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 221/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 222/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 223/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 224/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 225/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 226/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 227/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 228/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 229/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 230/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 231/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 232/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 233/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 234/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 235/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 236/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 237/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 238/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 239/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 240/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 241/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 242/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 243/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 244/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 245/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 246/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 247/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 248/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 249/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 250/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 251/300, seasonal_1 Loss: 0.0633 | 0.0307
Epoch 252/300, seasonal_1 Loss: 0.0633 | 0.0306
Epoch 253/300, seasonal_1 Loss: 0.0633 | 0.0306
Epoch 254/300, seasonal_1 Loss: 0.0633 | 0.0306
Epoch 255/300, seasonal_1 Loss: 0.0633 | 0.0306
Epoch 256/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 257/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 258/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 259/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 260/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 261/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 262/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 263/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 264/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 265/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 266/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 267/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 268/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 269/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 270/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 271/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 272/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 273/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 274/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 275/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 276/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 277/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 278/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 279/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 280/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 281/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 282/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 283/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 284/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 285/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 286/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 287/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 288/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 289/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 290/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 291/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 292/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 293/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 294/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 295/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 296/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 297/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 298/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 299/300, seasonal_1 Loss: 0.0632 | 0.0306
Epoch 300/300, seasonal_1 Loss: 0.0632 | 0.0306
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8307541305099913, 'learning_rate': 0.0007513475508938876, 'batch_size': 50, 'step_size': 9, 'gamma': 0.906873442627327}
Epoch 1/300, seasonal_2 Loss: 0.2639 | 0.1830
Epoch 2/300, seasonal_2 Loss: 0.1214 | 0.0899
Epoch 3/300, seasonal_2 Loss: 0.1046 | 0.0748
Epoch 4/300, seasonal_2 Loss: 0.1011 | 0.0697
Epoch 5/300, seasonal_2 Loss: 0.0968 | 0.0713
Epoch 6/300, seasonal_2 Loss: 0.0928 | 0.0644
Epoch 7/300, seasonal_2 Loss: 0.0887 | 0.0614
Epoch 8/300, seasonal_2 Loss: 0.0870 | 0.0602
Epoch 9/300, seasonal_2 Loss: 0.0839 | 0.0560
Epoch 10/300, seasonal_2 Loss: 0.0817 | 0.0535
Epoch 11/300, seasonal_2 Loss: 0.0812 | 0.0530
Epoch 12/300, seasonal_2 Loss: 0.0833 | 0.0563
Epoch 13/300, seasonal_2 Loss: 0.0825 | 0.0549
Epoch 14/300, seasonal_2 Loss: 0.0846 | 0.0561
Epoch 15/300, seasonal_2 Loss: 0.0790 | 0.0438
Epoch 16/300, seasonal_2 Loss: 0.0771 | 0.0486
Epoch 17/300, seasonal_2 Loss: 0.0811 | 0.0570
Epoch 18/300, seasonal_2 Loss: 0.0765 | 0.0439
Epoch 19/300, seasonal_2 Loss: 0.0782 | 0.0493
Epoch 20/300, seasonal_2 Loss: 0.0732 | 0.0358
Epoch 21/300, seasonal_2 Loss: 0.0714 | 0.0386
Epoch 22/300, seasonal_2 Loss: 0.0702 | 0.0350
Epoch 23/300, seasonal_2 Loss: 0.0683 | 0.0338
Epoch 24/300, seasonal_2 Loss: 0.0677 | 0.0338
Epoch 25/300, seasonal_2 Loss: 0.0669 | 0.0323
Epoch 26/300, seasonal_2 Loss: 0.0658 | 0.0310
Epoch 27/300, seasonal_2 Loss: 0.0658 | 0.0316
Epoch 28/300, seasonal_2 Loss: 0.0657 | 0.0300
Epoch 29/300, seasonal_2 Loss: 0.0661 | 0.0309
Epoch 30/300, seasonal_2 Loss: 0.0676 | 0.0333
Epoch 31/300, seasonal_2 Loss: 0.0651 | 0.0323
Epoch 32/300, seasonal_2 Loss: 0.0640 | 0.0319
Epoch 33/300, seasonal_2 Loss: 0.0631 | 0.0289
Epoch 34/300, seasonal_2 Loss: 0.0619 | 0.0281
Epoch 35/300, seasonal_2 Loss: 0.0613 | 0.0277
Epoch 36/300, seasonal_2 Loss: 0.0608 | 0.0282
Epoch 37/300, seasonal_2 Loss: 0.0605 | 0.0271
Epoch 38/300, seasonal_2 Loss: 0.0605 | 0.0275
Epoch 39/300, seasonal_2 Loss: 0.0602 | 0.0281
Epoch 40/300, seasonal_2 Loss: 0.0602 | 0.0295
Epoch 41/300, seasonal_2 Loss: 0.0617 | 0.0301
Epoch 42/300, seasonal_2 Loss: 0.0598 | 0.0279
Epoch 43/300, seasonal_2 Loss: 0.0595 | 0.0285
Epoch 44/300, seasonal_2 Loss: 0.0592 | 0.0294
Epoch 45/300, seasonal_2 Loss: 0.0595 | 0.0299
Epoch 46/300, seasonal_2 Loss: 0.0593 | 0.0316
Epoch 47/300, seasonal_2 Loss: 0.0596 | 0.0335
Epoch 48/300, seasonal_2 Loss: 0.0599 | 0.0329
Epoch 49/300, seasonal_2 Loss: 0.0595 | 0.0339
Epoch 50/300, seasonal_2 Loss: 0.0592 | 0.0355
Epoch 51/300, seasonal_2 Loss: 0.0586 | 0.0368
Epoch 52/300, seasonal_2 Loss: 0.0596 | 0.0417
Epoch 53/300, seasonal_2 Loss: 0.0582 | 0.0390
Epoch 54/300, seasonal_2 Loss: 0.0597 | 0.0344
Epoch 55/300, seasonal_2 Loss: 0.0575 | 0.0363
Epoch 56/300, seasonal_2 Loss: 0.0579 | 0.0319
Epoch 57/300, seasonal_2 Loss: 0.0567 | 0.0318
Epoch 58/300, seasonal_2 Loss: 0.0570 | 0.0308
Epoch 59/300, seasonal_2 Loss: 0.0553 | 0.0297
Epoch 60/300, seasonal_2 Loss: 0.0550 | 0.0297
Epoch 61/300, seasonal_2 Loss: 0.0549 | 0.0294
Epoch 62/300, seasonal_2 Loss: 0.0547 | 0.0293
Epoch 63/300, seasonal_2 Loss: 0.0545 | 0.0289
Epoch 64/300, seasonal_2 Loss: 0.0540 | 0.0290
Epoch 65/300, seasonal_2 Loss: 0.0539 | 0.0290
Epoch 66/300, seasonal_2 Loss: 0.0537 | 0.0297
Epoch 67/300, seasonal_2 Loss: 0.0532 | 0.0292
Epoch 68/300, seasonal_2 Loss: 0.0534 | 0.0299
Epoch 69/300, seasonal_2 Loss: 0.0530 | 0.0286
Epoch 70/300, seasonal_2 Loss: 0.0525 | 0.0285
Epoch 71/300, seasonal_2 Loss: 0.0528 | 0.0285
Epoch 72/300, seasonal_2 Loss: 0.0525 | 0.0283
Epoch 73/300, seasonal_2 Loss: 0.0523 | 0.0287
Epoch 74/300, seasonal_2 Loss: 0.0526 | 0.0292
Epoch 75/300, seasonal_2 Loss: 0.0524 | 0.0290
Epoch 76/300, seasonal_2 Loss: 0.0522 | 0.0300
Epoch 77/300, seasonal_2 Loss: 0.0524 | 0.0294
Epoch 78/300, seasonal_2 Loss: 0.0524 | 0.0309
Epoch 79/300, seasonal_2 Loss: 0.0524 | 0.0316
Epoch 80/300, seasonal_2 Loss: 0.0527 | 0.0315
Epoch 81/300, seasonal_2 Loss: 0.0527 | 0.0300
Epoch 82/300, seasonal_2 Loss: 0.0525 | 0.0313
Epoch 83/300, seasonal_2 Loss: 0.0522 | 0.0324
Epoch 84/300, seasonal_2 Loss: 0.0519 | 0.0322
Epoch 85/300, seasonal_2 Loss: 0.0516 | 0.0322
Epoch 86/300, seasonal_2 Loss: 0.0514 | 0.0304
Epoch 87/300, seasonal_2 Loss: 0.0509 | 0.0294
Epoch 88/300, seasonal_2 Loss: 0.0509 | 0.0289
Epoch 89/300, seasonal_2 Loss: 0.0505 | 0.0290
Epoch 90/300, seasonal_2 Loss: 0.0502 | 0.0291
Epoch 91/300, seasonal_2 Loss: 0.0499 | 0.0294
Epoch 92/300, seasonal_2 Loss: 0.0497 | 0.0294
Epoch 93/300, seasonal_2 Loss: 0.0495 | 0.0298
Epoch 94/300, seasonal_2 Loss: 0.0495 | 0.0296
Epoch 95/300, seasonal_2 Loss: 0.0493 | 0.0304
Epoch 96/300, seasonal_2 Loss: 0.0494 | 0.0297
Epoch 97/300, seasonal_2 Loss: 0.0492 | 0.0302
Epoch 98/300, seasonal_2 Loss: 0.0488 | 0.0307
Epoch 99/300, seasonal_2 Loss: 0.0497 | 0.0294
Epoch 100/300, seasonal_2 Loss: 0.0499 | 0.0306
Epoch 101/300, seasonal_2 Loss: 0.0491 | 0.0304
Epoch 102/300, seasonal_2 Loss: 0.0485 | 0.0307
Epoch 103/300, seasonal_2 Loss: 0.0496 | 0.0298
Epoch 104/300, seasonal_2 Loss: 0.0503 | 0.0307
Epoch 105/300, seasonal_2 Loss: 0.0503 | 0.0306
Epoch 106/300, seasonal_2 Loss: 0.0492 | 0.0312
Epoch 107/300, seasonal_2 Loss: 0.0472 | 0.0306
Epoch 108/300, seasonal_2 Loss: 0.0484 | 0.0302
Epoch 109/300, seasonal_2 Loss: 0.0514 | 0.0340
Epoch 110/300, seasonal_2 Loss: 0.0470 | 0.0306
Epoch 111/300, seasonal_2 Loss: 0.0482 | 0.0309
Epoch 112/300, seasonal_2 Loss: 0.0456 | 0.0307
Epoch 113/300, seasonal_2 Loss: 0.0446 | 0.0310
Epoch 114/300, seasonal_2 Loss: 0.0451 | 0.0310
Epoch 115/300, seasonal_2 Loss: 0.0443 | 0.0309
Epoch 116/300, seasonal_2 Loss: 0.0436 | 0.0311
Epoch 117/300, seasonal_2 Loss: 0.0437 | 0.0314
Epoch 118/300, seasonal_2 Loss: 0.0439 | 0.0311
Epoch 119/300, seasonal_2 Loss: 0.0429 | 0.0306
Epoch 120/300, seasonal_2 Loss: 0.0459 | 0.0307
Epoch 121/300, seasonal_2 Loss: 0.0443 | 0.0307
Epoch 122/300, seasonal_2 Loss: 0.0435 | 0.0309
Epoch 123/300, seasonal_2 Loss: 0.0429 | 0.0306
Epoch 124/300, seasonal_2 Loss: 0.0424 | 0.0307
Epoch 125/300, seasonal_2 Loss: 0.0418 | 0.0302
Epoch 126/300, seasonal_2 Loss: 0.0463 | 0.0322
Epoch 127/300, seasonal_2 Loss: 0.0432 | 0.0308
Epoch 128/300, seasonal_2 Loss: 0.0421 | 0.0307
Epoch 129/300, seasonal_2 Loss: 0.0413 | 0.0309
Epoch 130/300, seasonal_2 Loss: 0.0411 | 0.0307
Epoch 131/300, seasonal_2 Loss: 0.0454 | 0.0326
Epoch 132/300, seasonal_2 Loss: 0.0440 | 0.0312
Epoch 133/300, seasonal_2 Loss: 0.0422 | 0.0307
Epoch 134/300, seasonal_2 Loss: 0.0414 | 0.0308
Epoch 135/300, seasonal_2 Loss: 0.0412 | 0.0306
Epoch 136/300, seasonal_2 Loss: 0.0428 | 0.0318
Epoch 137/300, seasonal_2 Loss: 0.0419 | 0.0305
Epoch 138/300, seasonal_2 Loss: 0.0412 | 0.0309
Epoch 139/300, seasonal_2 Loss: 0.0409 | 0.0307
Epoch 140/300, seasonal_2 Loss: 0.0407 | 0.0308
Epoch 141/300, seasonal_2 Loss: 0.0404 | 0.0309
Epoch 142/300, seasonal_2 Loss: 0.0401 | 0.0310
Epoch 143/300, seasonal_2 Loss: 0.0399 | 0.0311
Epoch 144/300, seasonal_2 Loss: 0.0397 | 0.0312
Epoch 145/300, seasonal_2 Loss: 0.0395 | 0.0312
Epoch 146/300, seasonal_2 Loss: 0.0394 | 0.0314
Epoch 147/300, seasonal_2 Loss: 0.0407 | 0.0314
Epoch 148/300, seasonal_2 Loss: 0.0400 | 0.0321
Epoch 149/300, seasonal_2 Loss: 0.0410 | 0.0313
Epoch 150/300, seasonal_2 Loss: 0.0399 | 0.0316
Epoch 151/300, seasonal_2 Loss: 0.0392 | 0.0314
Epoch 152/300, seasonal_2 Loss: 0.0391 | 0.0315
Epoch 153/300, seasonal_2 Loss: 0.0401 | 0.0315
Epoch 154/300, seasonal_2 Loss: 0.0393 | 0.0317
Epoch 155/300, seasonal_2 Loss: 0.0393 | 0.0318
Epoch 156/300, seasonal_2 Loss: 0.0402 | 0.0315
Epoch 157/300, seasonal_2 Loss: 0.0396 | 0.0319
Epoch 158/300, seasonal_2 Loss: 0.0390 | 0.0317
Epoch 159/300, seasonal_2 Loss: 0.0389 | 0.0319
Epoch 160/300, seasonal_2 Loss: 0.0396 | 0.0316
Epoch 161/300, seasonal_2 Loss: 0.0389 | 0.0320
Epoch 162/300, seasonal_2 Loss: 0.0385 | 0.0320
Epoch 163/300, seasonal_2 Loss: 0.0388 | 0.0318
Epoch 164/300, seasonal_2 Loss: 0.0387 | 0.0325
Epoch 165/300, seasonal_2 Loss: 0.0394 | 0.0318
Epoch 166/300, seasonal_2 Loss: 0.0389 | 0.0322
Epoch 167/300, seasonal_2 Loss: 0.0382 | 0.0321
Epoch 168/300, seasonal_2 Loss: 0.0381 | 0.0322
Epoch 169/300, seasonal_2 Loss: 0.0382 | 0.0321
Epoch 170/300, seasonal_2 Loss: 0.0388 | 0.0329
Epoch 171/300, seasonal_2 Loss: 0.0396 | 0.0320
Epoch 172/300, seasonal_2 Loss: 0.0391 | 0.0323
Epoch 173/300, seasonal_2 Loss: 0.0387 | 0.0322
Epoch 174/300, seasonal_2 Loss: 0.0381 | 0.0324
Epoch 175/300, seasonal_2 Loss: 0.0378 | 0.0325
Epoch 176/300, seasonal_2 Loss: 0.0378 | 0.0324
Epoch 177/300, seasonal_2 Loss: 0.0377 | 0.0326
Epoch 178/300, seasonal_2 Loss: 0.0377 | 0.0325
Epoch 179/300, seasonal_2 Loss: 0.0377 | 0.0328
Epoch 180/300, seasonal_2 Loss: 0.0377 | 0.0325
Epoch 181/300, seasonal_2 Loss: 0.0377 | 0.0329
Epoch 182/300, seasonal_2 Loss: 0.0378 | 0.0325
Epoch 183/300, seasonal_2 Loss: 0.0375 | 0.0330
Epoch 184/300, seasonal_2 Loss: 0.0375 | 0.0326
Epoch 185/300, seasonal_2 Loss: 0.0374 | 0.0330
Epoch 186/300, seasonal_2 Loss: 0.0374 | 0.0327
Epoch 187/300, seasonal_2 Loss: 0.0373 | 0.0330
Epoch 188/300, seasonal_2 Loss: 0.0373 | 0.0328
Epoch 189/300, seasonal_2 Loss: 0.0373 | 0.0330
Epoch 190/300, seasonal_2 Loss: 0.0373 | 0.0328
Epoch 191/300, seasonal_2 Loss: 0.0372 | 0.0330
Epoch 192/300, seasonal_2 Loss: 0.0372 | 0.0328
Epoch 193/300, seasonal_2 Loss: 0.0372 | 0.0330
Epoch 194/300, seasonal_2 Loss: 0.0372 | 0.0329
Epoch 195/300, seasonal_2 Loss: 0.0371 | 0.0330
Epoch 196/300, seasonal_2 Loss: 0.0371 | 0.0328
Epoch 197/300, seasonal_2 Loss: 0.0371 | 0.0331
Epoch 198/300, seasonal_2 Loss: 0.0371 | 0.0329
Epoch 199/300, seasonal_2 Loss: 0.0370 | 0.0331
Epoch 200/300, seasonal_2 Loss: 0.0370 | 0.0329
Epoch 201/300, seasonal_2 Loss: 0.0370 | 0.0331
Epoch 202/300, seasonal_2 Loss: 0.0370 | 0.0329
Epoch 203/300, seasonal_2 Loss: 0.0370 | 0.0331
Epoch 204/300, seasonal_2 Loss: 0.0370 | 0.0329
Epoch 205/300, seasonal_2 Loss: 0.0369 | 0.0331
Epoch 206/300, seasonal_2 Loss: 0.0369 | 0.0329
Epoch 207/300, seasonal_2 Loss: 0.0369 | 0.0331
Epoch 208/300, seasonal_2 Loss: 0.0369 | 0.0329
Epoch 209/300, seasonal_2 Loss: 0.0368 | 0.0331
Epoch 210/300, seasonal_2 Loss: 0.0368 | 0.0330
Epoch 211/300, seasonal_2 Loss: 0.0368 | 0.0331
Epoch 212/300, seasonal_2 Loss: 0.0368 | 0.0330
Epoch 213/300, seasonal_2 Loss: 0.0367 | 0.0331
Epoch 214/300, seasonal_2 Loss: 0.0367 | 0.0330
Epoch 215/300, seasonal_2 Loss: 0.0367 | 0.0331
Epoch 216/300, seasonal_2 Loss: 0.0367 | 0.0330
Epoch 217/300, seasonal_2 Loss: 0.0367 | 0.0331
Epoch 218/300, seasonal_2 Loss: 0.0366 | 0.0330
Epoch 219/300, seasonal_2 Loss: 0.0366 | 0.0331
Epoch 220/300, seasonal_2 Loss: 0.0366 | 0.0330
Epoch 221/300, seasonal_2 Loss: 0.0366 | 0.0331
Epoch 222/300, seasonal_2 Loss: 0.0366 | 0.0330
Epoch 223/300, seasonal_2 Loss: 0.0365 | 0.0331
Epoch 224/300, seasonal_2 Loss: 0.0365 | 0.0330
Epoch 225/300, seasonal_2 Loss: 0.0365 | 0.0331
Epoch 226/300, seasonal_2 Loss: 0.0365 | 0.0330
Epoch 227/300, seasonal_2 Loss: 0.0365 | 0.0331
Epoch 228/300, seasonal_2 Loss: 0.0365 | 0.0330
Epoch 229/300, seasonal_2 Loss: 0.0364 | 0.0331
Epoch 230/300, seasonal_2 Loss: 0.0364 | 0.0330
Epoch 231/300, seasonal_2 Loss: 0.0364 | 0.0332
Epoch 232/300, seasonal_2 Loss: 0.0364 | 0.0330
Epoch 233/300, seasonal_2 Loss: 0.0364 | 0.0332
Epoch 234/300, seasonal_2 Loss: 0.0364 | 0.0331
Epoch 235/300, seasonal_2 Loss: 0.0364 | 0.0332
Epoch 236/300, seasonal_2 Loss: 0.0364 | 0.0331
Epoch 237/300, seasonal_2 Loss: 0.0363 | 0.0332
Epoch 238/300, seasonal_2 Loss: 0.0363 | 0.0331
Epoch 239/300, seasonal_2 Loss: 0.0363 | 0.0332
Epoch 240/300, seasonal_2 Loss: 0.0363 | 0.0331
Epoch 241/300, seasonal_2 Loss: 0.0363 | 0.0332
Epoch 242/300, seasonal_2 Loss: 0.0363 | 0.0332
Epoch 243/300, seasonal_2 Loss: 0.0363 | 0.0332
Epoch 244/300, seasonal_2 Loss: 0.0363 | 0.0332
Epoch 245/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 246/300, seasonal_2 Loss: 0.0362 | 0.0332
Epoch 247/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 248/300, seasonal_2 Loss: 0.0362 | 0.0332
Epoch 249/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 250/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 251/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 252/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 253/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 254/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 255/300, seasonal_2 Loss: 0.0362 | 0.0333
Epoch 256/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 257/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 258/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 259/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 260/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 261/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 262/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 263/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 264/300, seasonal_2 Loss: 0.0361 | 0.0333
Epoch 265/300, seasonal_2 Loss: 0.0361 | 0.0334
Epoch 266/300, seasonal_2 Loss: 0.0361 | 0.0334
Epoch 267/300, seasonal_2 Loss: 0.0361 | 0.0334
Epoch 268/300, seasonal_2 Loss: 0.0361 | 0.0334
Epoch 269/300, seasonal_2 Loss: 0.0361 | 0.0334
Epoch 270/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 271/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 272/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 273/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 274/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 275/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 276/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 277/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 278/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 279/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 280/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 281/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 282/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 283/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 284/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 285/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 286/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 287/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 288/300, seasonal_2 Loss: 0.0360 | 0.0334
Epoch 289/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 290/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 291/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 292/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 293/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 294/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 295/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 296/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 297/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 298/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 299/300, seasonal_2 Loss: 0.0359 | 0.0335
Epoch 300/300, seasonal_2 Loss: 0.0359 | 0.0335
Training seasonal_3 component with params: {'observation_period_num': 11, 'train_rates': 0.7714790377854563, 'learning_rate': 0.00034981100112560135, 'batch_size': 62, 'step_size': 13, 'gamma': 0.877869650618062}
Epoch 1/300, seasonal_3 Loss: 0.2446 | 0.1982
Epoch 2/300, seasonal_3 Loss: 0.1329 | 0.1041
Epoch 3/300, seasonal_3 Loss: 0.1164 | 0.0774
Epoch 4/300, seasonal_3 Loss: 0.1131 | 0.0674
Epoch 5/300, seasonal_3 Loss: 0.1217 | 0.0681
Epoch 6/300, seasonal_3 Loss: 0.1173 | 0.0644
Epoch 7/300, seasonal_3 Loss: 0.1127 | 0.0644
Epoch 8/300, seasonal_3 Loss: 0.1027 | 0.0595
Epoch 9/300, seasonal_3 Loss: 0.0948 | 0.0580
Epoch 10/300, seasonal_3 Loss: 0.0953 | 0.0650
Epoch 11/300, seasonal_3 Loss: 0.0982 | 0.0580
Epoch 12/300, seasonal_3 Loss: 0.0964 | 0.0547
Epoch 13/300, seasonal_3 Loss: 0.0967 | 0.0518
Epoch 14/300, seasonal_3 Loss: 0.0924 | 0.0484
Epoch 15/300, seasonal_3 Loss: 0.0883 | 0.0489
Epoch 16/300, seasonal_3 Loss: 0.0870 | 0.0532
Epoch 17/300, seasonal_3 Loss: 0.0876 | 0.0547
Epoch 18/300, seasonal_3 Loss: 0.0893 | 0.0541
Epoch 19/300, seasonal_3 Loss: 0.0870 | 0.0532
Epoch 20/300, seasonal_3 Loss: 0.0853 | 0.0661
Epoch 21/300, seasonal_3 Loss: 0.0812 | 0.0592
Epoch 22/300, seasonal_3 Loss: 0.0828 | 0.0795
Epoch 23/300, seasonal_3 Loss: 0.0867 | 0.0479
Epoch 24/300, seasonal_3 Loss: 0.0761 | 0.0484
Epoch 25/300, seasonal_3 Loss: 0.0767 | 0.0423
Epoch 26/300, seasonal_3 Loss: 0.0747 | 0.0459
Epoch 27/300, seasonal_3 Loss: 0.0752 | 0.0384
Epoch 28/300, seasonal_3 Loss: 0.0739 | 0.0445
Epoch 29/300, seasonal_3 Loss: 0.0750 | 0.0350
Epoch 30/300, seasonal_3 Loss: 0.0726 | 0.0404
Epoch 31/300, seasonal_3 Loss: 0.0727 | 0.0330
Epoch 32/300, seasonal_3 Loss: 0.0707 | 0.0370
Epoch 33/300, seasonal_3 Loss: 0.0705 | 0.0320
Epoch 34/300, seasonal_3 Loss: 0.0691 | 0.0359
Epoch 35/300, seasonal_3 Loss: 0.0691 | 0.0317
Epoch 36/300, seasonal_3 Loss: 0.0682 | 0.0343
Epoch 37/300, seasonal_3 Loss: 0.0680 | 0.0315
Epoch 38/300, seasonal_3 Loss: 0.0673 | 0.0332
Epoch 39/300, seasonal_3 Loss: 0.0669 | 0.0312
Epoch 40/300, seasonal_3 Loss: 0.0664 | 0.0324
Epoch 41/300, seasonal_3 Loss: 0.0660 | 0.0306
Epoch 42/300, seasonal_3 Loss: 0.0655 | 0.0308
Epoch 43/300, seasonal_3 Loss: 0.0648 | 0.0295
Epoch 44/300, seasonal_3 Loss: 0.0644 | 0.0296
Epoch 45/300, seasonal_3 Loss: 0.0637 | 0.0288
Epoch 46/300, seasonal_3 Loss: 0.0633 | 0.0291
Epoch 47/300, seasonal_3 Loss: 0.0629 | 0.0285
Epoch 48/300, seasonal_3 Loss: 0.0626 | 0.0278
Epoch 49/300, seasonal_3 Loss: 0.0624 | 0.0280
Epoch 50/300, seasonal_3 Loss: 0.0625 | 0.0289
Epoch 51/300, seasonal_3 Loss: 0.0627 | 0.0291
Epoch 52/300, seasonal_3 Loss: 0.0622 | 0.0288
Epoch 53/300, seasonal_3 Loss: 0.0620 | 0.0277
Epoch 54/300, seasonal_3 Loss: 0.0616 | 0.0274
Epoch 55/300, seasonal_3 Loss: 0.0613 | 0.0283
Epoch 56/300, seasonal_3 Loss: 0.0615 | 0.0296
Epoch 57/300, seasonal_3 Loss: 0.0616 | 0.0291
Epoch 58/300, seasonal_3 Loss: 0.0609 | 0.0283
Epoch 59/300, seasonal_3 Loss: 0.0601 | 0.0277
Epoch 60/300, seasonal_3 Loss: 0.0600 | 0.0286
Epoch 61/300, seasonal_3 Loss: 0.0602 | 0.0284
Epoch 62/300, seasonal_3 Loss: 0.0602 | 0.0288
Epoch 63/300, seasonal_3 Loss: 0.0600 | 0.0289
Epoch 64/300, seasonal_3 Loss: 0.0597 | 0.0292
Epoch 65/300, seasonal_3 Loss: 0.0593 | 0.0293
Epoch 66/300, seasonal_3 Loss: 0.0592 | 0.0292
Epoch 67/300, seasonal_3 Loss: 0.0592 | 0.0293
Epoch 68/300, seasonal_3 Loss: 0.0592 | 0.0279
Epoch 69/300, seasonal_3 Loss: 0.0588 | 0.0276
Epoch 70/300, seasonal_3 Loss: 0.0585 | 0.0268
Epoch 71/300, seasonal_3 Loss: 0.0581 | 0.0267
Epoch 72/300, seasonal_3 Loss: 0.0579 | 0.0266
Epoch 73/300, seasonal_3 Loss: 0.0575 | 0.0264
Epoch 74/300, seasonal_3 Loss: 0.0574 | 0.0265
Epoch 75/300, seasonal_3 Loss: 0.0571 | 0.0271
Epoch 76/300, seasonal_3 Loss: 0.0571 | 0.0270
Epoch 77/300, seasonal_3 Loss: 0.0570 | 0.0269
Epoch 78/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 79/300, seasonal_3 Loss: 0.0569 | 0.0264
Epoch 80/300, seasonal_3 Loss: 0.0572 | 0.0267
Epoch 81/300, seasonal_3 Loss: 0.0570 | 0.0280
Epoch 82/300, seasonal_3 Loss: 0.0568 | 0.0274
Epoch 83/300, seasonal_3 Loss: 0.0565 | 0.0273
Epoch 84/300, seasonal_3 Loss: 0.0562 | 0.0277
Epoch 85/300, seasonal_3 Loss: 0.0560 | 0.0279
Epoch 86/300, seasonal_3 Loss: 0.0558 | 0.0272
Epoch 87/300, seasonal_3 Loss: 0.0557 | 0.0271
Epoch 88/300, seasonal_3 Loss: 0.0556 | 0.0271
Epoch 89/300, seasonal_3 Loss: 0.0558 | 0.0276
Epoch 90/300, seasonal_3 Loss: 0.0560 | 0.0273
Epoch 91/300, seasonal_3 Loss: 0.0564 | 0.0270
Epoch 92/300, seasonal_3 Loss: 0.0563 | 0.0266
Epoch 93/300, seasonal_3 Loss: 0.0567 | 0.0270
Epoch 94/300, seasonal_3 Loss: 0.0566 | 0.0268
Epoch 95/300, seasonal_3 Loss: 0.0566 | 0.0270
Epoch 96/300, seasonal_3 Loss: 0.0561 | 0.0272
Epoch 97/300, seasonal_3 Loss: 0.0559 | 0.0272
Epoch 98/300, seasonal_3 Loss: 0.0556 | 0.0276
Epoch 99/300, seasonal_3 Loss: 0.0556 | 0.0284
Epoch 100/300, seasonal_3 Loss: 0.0555 | 0.0278
Epoch 101/300, seasonal_3 Loss: 0.0556 | 0.0279
Epoch 102/300, seasonal_3 Loss: 0.0557 | 0.0285
Epoch 103/300, seasonal_3 Loss: 0.0565 | 0.0292
Epoch 104/300, seasonal_3 Loss: 0.0565 | 0.0277
Epoch 105/300, seasonal_3 Loss: 0.0557 | 0.0277
Epoch 106/300, seasonal_3 Loss: 0.0559 | 0.0282
Epoch 107/300, seasonal_3 Loss: 0.0566 | 0.0288
Epoch 108/300, seasonal_3 Loss: 0.0592 | 0.0305
Epoch 109/300, seasonal_3 Loss: 0.0560 | 0.0278
Epoch 110/300, seasonal_3 Loss: 0.0579 | 0.0293
Epoch 111/300, seasonal_3 Loss: 0.0554 | 0.0278
Epoch 112/300, seasonal_3 Loss: 0.0557 | 0.0276
Epoch 113/300, seasonal_3 Loss: 0.0560 | 0.0273
Epoch 114/300, seasonal_3 Loss: 0.0555 | 0.0275
Epoch 115/300, seasonal_3 Loss: 0.0548 | 0.0278
Epoch 116/300, seasonal_3 Loss: 0.0543 | 0.0279
Epoch 117/300, seasonal_3 Loss: 0.0539 | 0.0281
Epoch 118/300, seasonal_3 Loss: 0.0536 | 0.0279
Epoch 119/300, seasonal_3 Loss: 0.0534 | 0.0280
Epoch 120/300, seasonal_3 Loss: 0.0531 | 0.0277
Epoch 121/300, seasonal_3 Loss: 0.0529 | 0.0279
Epoch 122/300, seasonal_3 Loss: 0.0527 | 0.0278
Epoch 123/300, seasonal_3 Loss: 0.0526 | 0.0279
Epoch 124/300, seasonal_3 Loss: 0.0525 | 0.0279
Epoch 125/300, seasonal_3 Loss: 0.0523 | 0.0279
Epoch 126/300, seasonal_3 Loss: 0.0522 | 0.0280
Epoch 127/300, seasonal_3 Loss: 0.0521 | 0.0282
Epoch 128/300, seasonal_3 Loss: 0.0519 | 0.0283
Epoch 129/300, seasonal_3 Loss: 0.0517 | 0.0286
Epoch 130/300, seasonal_3 Loss: 0.0515 | 0.0287
Epoch 131/300, seasonal_3 Loss: 0.0514 | 0.0289
Epoch 132/300, seasonal_3 Loss: 0.0512 | 0.0291
Epoch 133/300, seasonal_3 Loss: 0.0510 | 0.0293
Epoch 134/300, seasonal_3 Loss: 0.0509 | 0.0295
Epoch 135/300, seasonal_3 Loss: 0.0507 | 0.0297
Epoch 136/300, seasonal_3 Loss: 0.0505 | 0.0299
Epoch 137/300, seasonal_3 Loss: 0.0502 | 0.0302
Epoch 138/300, seasonal_3 Loss: 0.0500 | 0.0305
Epoch 139/300, seasonal_3 Loss: 0.0498 | 0.0308
Epoch 140/300, seasonal_3 Loss: 0.0495 | 0.0311
Epoch 141/300, seasonal_3 Loss: 0.0493 | 0.0314
Epoch 142/300, seasonal_3 Loss: 0.0490 | 0.0316
Epoch 143/300, seasonal_3 Loss: 0.0487 | 0.0319
Epoch 144/300, seasonal_3 Loss: 0.0484 | 0.0321
Epoch 145/300, seasonal_3 Loss: 0.0481 | 0.0322
Epoch 146/300, seasonal_3 Loss: 0.0477 | 0.0323
Epoch 147/300, seasonal_3 Loss: 0.0474 | 0.0325
Epoch 148/300, seasonal_3 Loss: 0.0471 | 0.0324
Epoch 149/300, seasonal_3 Loss: 0.0468 | 0.0327
Epoch 150/300, seasonal_3 Loss: 0.0465 | 0.0329
Epoch 151/300, seasonal_3 Loss: 0.0463 | 0.0326
Epoch 152/300, seasonal_3 Loss: 0.0461 | 0.0330
Epoch 153/300, seasonal_3 Loss: 0.0459 | 0.0339
Epoch 154/300, seasonal_3 Loss: 0.0457 | 0.0327
Epoch 155/300, seasonal_3 Loss: 0.0475 | 0.0321
Epoch 156/300, seasonal_3 Loss: 0.0476 | 0.0296
Epoch 157/300, seasonal_3 Loss: 0.0467 | 0.0327
Epoch 158/300, seasonal_3 Loss: 0.0467 | 0.0326
Epoch 159/300, seasonal_3 Loss: 0.0463 | 0.0332
Epoch 160/300, seasonal_3 Loss: 0.0456 | 0.0329
Epoch 161/300, seasonal_3 Loss: 0.0452 | 0.0329
Epoch 162/300, seasonal_3 Loss: 0.0449 | 0.0333
Epoch 163/300, seasonal_3 Loss: 0.0447 | 0.0329
Epoch 164/300, seasonal_3 Loss: 0.0445 | 0.0334
Epoch 165/300, seasonal_3 Loss: 0.0442 | 0.0331
Epoch 166/300, seasonal_3 Loss: 0.0441 | 0.0335
Epoch 167/300, seasonal_3 Loss: 0.0438 | 0.0335
Epoch 168/300, seasonal_3 Loss: 0.0436 | 0.0335
Epoch 169/300, seasonal_3 Loss: 0.0435 | 0.0341
Epoch 170/300, seasonal_3 Loss: 0.0434 | 0.0342
Epoch 171/300, seasonal_3 Loss: 0.0443 | 0.0389
Epoch 172/300, seasonal_3 Loss: 0.0439 | 0.0324
Epoch 173/300, seasonal_3 Loss: 0.0432 | 0.0339
Epoch 174/300, seasonal_3 Loss: 0.0430 | 0.0340
Epoch 175/300, seasonal_3 Loss: 0.0429 | 0.0345
Epoch 176/300, seasonal_3 Loss: 0.0448 | 0.0425
Epoch 177/300, seasonal_3 Loss: 0.0441 | 0.0319
Epoch 178/300, seasonal_3 Loss: 0.0434 | 0.0334
Epoch 179/300, seasonal_3 Loss: 0.0430 | 0.0333
Epoch 180/300, seasonal_3 Loss: 0.0428 | 0.0338
Epoch 181/300, seasonal_3 Loss: 0.0426 | 0.0338
Epoch 182/300, seasonal_3 Loss: 0.0424 | 0.0339
Epoch 183/300, seasonal_3 Loss: 0.0423 | 0.0340
Epoch 184/300, seasonal_3 Loss: 0.0422 | 0.0341
Epoch 185/300, seasonal_3 Loss: 0.0421 | 0.0342
Epoch 186/300, seasonal_3 Loss: 0.0420 | 0.0342
Epoch 187/300, seasonal_3 Loss: 0.0419 | 0.0343
Epoch 188/300, seasonal_3 Loss: 0.0419 | 0.0344
Epoch 189/300, seasonal_3 Loss: 0.0418 | 0.0344
Epoch 190/300, seasonal_3 Loss: 0.0417 | 0.0346
Epoch 191/300, seasonal_3 Loss: 0.0417 | 0.0344
Epoch 192/300, seasonal_3 Loss: 0.0417 | 0.0354
Epoch 193/300, seasonal_3 Loss: 0.0420 | 0.0345
Epoch 194/300, seasonal_3 Loss: 0.0422 | 0.0387
Epoch 195/300, seasonal_3 Loss: 0.0421 | 0.0339
Epoch 196/300, seasonal_3 Loss: 0.0423 | 0.0377
Epoch 197/300, seasonal_3 Loss: 0.0423 | 0.0344
Epoch 198/300, seasonal_3 Loss: 0.0416 | 0.0346
Epoch 199/300, seasonal_3 Loss: 0.0414 | 0.0346
Epoch 200/300, seasonal_3 Loss: 0.0414 | 0.0345
Epoch 201/300, seasonal_3 Loss: 0.0414 | 0.0349
Epoch 202/300, seasonal_3 Loss: 0.0413 | 0.0345
Epoch 203/300, seasonal_3 Loss: 0.0413 | 0.0354
Epoch 204/300, seasonal_3 Loss: 0.0414 | 0.0344
Epoch 205/300, seasonal_3 Loss: 0.0414 | 0.0360
Epoch 206/300, seasonal_3 Loss: 0.0415 | 0.0345
Epoch 207/300, seasonal_3 Loss: 0.0413 | 0.0356
Epoch 208/300, seasonal_3 Loss: 0.0413 | 0.0345
Epoch 209/300, seasonal_3 Loss: 0.0412 | 0.0357
Epoch 210/300, seasonal_3 Loss: 0.0413 | 0.0346
Epoch 211/300, seasonal_3 Loss: 0.0411 | 0.0354
Epoch 212/300, seasonal_3 Loss: 0.0411 | 0.0346
Epoch 213/300, seasonal_3 Loss: 0.0411 | 0.0355
Epoch 214/300, seasonal_3 Loss: 0.0411 | 0.0346
Epoch 215/300, seasonal_3 Loss: 0.0411 | 0.0356
Epoch 216/300, seasonal_3 Loss: 0.0411 | 0.0347
Epoch 217/300, seasonal_3 Loss: 0.0410 | 0.0353
Epoch 218/300, seasonal_3 Loss: 0.0410 | 0.0347
Epoch 219/300, seasonal_3 Loss: 0.0409 | 0.0352
Epoch 220/300, seasonal_3 Loss: 0.0409 | 0.0347
Epoch 221/300, seasonal_3 Loss: 0.0409 | 0.0353
Epoch 222/300, seasonal_3 Loss: 0.0409 | 0.0347
Epoch 223/300, seasonal_3 Loss: 0.0409 | 0.0351
Epoch 224/300, seasonal_3 Loss: 0.0408 | 0.0347
Epoch 225/300, seasonal_3 Loss: 0.0408 | 0.0351
Epoch 226/300, seasonal_3 Loss: 0.0408 | 0.0347
Epoch 227/300, seasonal_3 Loss: 0.0408 | 0.0351
Epoch 228/300, seasonal_3 Loss: 0.0408 | 0.0347
Epoch 229/300, seasonal_3 Loss: 0.0407 | 0.0351
Epoch 230/300, seasonal_3 Loss: 0.0407 | 0.0347
Epoch 231/300, seasonal_3 Loss: 0.0407 | 0.0350
Epoch 232/300, seasonal_3 Loss: 0.0407 | 0.0347
Epoch 233/300, seasonal_3 Loss: 0.0407 | 0.0350
Epoch 234/300, seasonal_3 Loss: 0.0406 | 0.0347
Epoch 235/300, seasonal_3 Loss: 0.0406 | 0.0350
Epoch 236/300, seasonal_3 Loss: 0.0406 | 0.0348
Epoch 237/300, seasonal_3 Loss: 0.0406 | 0.0349
Epoch 238/300, seasonal_3 Loss: 0.0406 | 0.0348
Epoch 239/300, seasonal_3 Loss: 0.0406 | 0.0349
Epoch 240/300, seasonal_3 Loss: 0.0406 | 0.0348
Epoch 241/300, seasonal_3 Loss: 0.0405 | 0.0348
Epoch 242/300, seasonal_3 Loss: 0.0405 | 0.0348
Epoch 243/300, seasonal_3 Loss: 0.0405 | 0.0348
Epoch 244/300, seasonal_3 Loss: 0.0405 | 0.0348
Epoch 245/300, seasonal_3 Loss: 0.0405 | 0.0348
Epoch 246/300, seasonal_3 Loss: 0.0405 | 0.0348
Epoch 247/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 248/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 249/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 250/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 251/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 252/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 253/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 254/300, seasonal_3 Loss: 0.0404 | 0.0348
Epoch 255/300, seasonal_3 Loss: 0.0403 | 0.0348
Epoch 256/300, seasonal_3 Loss: 0.0403 | 0.0348
Epoch 257/300, seasonal_3 Loss: 0.0403 | 0.0349
Epoch 258/300, seasonal_3 Loss: 0.0403 | 0.0349
Epoch 259/300, seasonal_3 Loss: 0.0403 | 0.0349
Epoch 260/300, seasonal_3 Loss: 0.0403 | 0.0349
Epoch 261/300, seasonal_3 Loss: 0.0403 | 0.0349
Epoch 262/300, seasonal_3 Loss: 0.0402 | 0.0349
Epoch 263/300, seasonal_3 Loss: 0.0402 | 0.0349
Epoch 264/300, seasonal_3 Loss: 0.0402 | 0.0349
Epoch 265/300, seasonal_3 Loss: 0.0402 | 0.0349
Epoch 266/300, seasonal_3 Loss: 0.0402 | 0.0350
Epoch 267/300, seasonal_3 Loss: 0.0402 | 0.0349
Epoch 268/300, seasonal_3 Loss: 0.0402 | 0.0349
Epoch 269/300, seasonal_3 Loss: 0.0402 | 0.0352
Epoch 270/300, seasonal_3 Loss: 0.0402 | 0.0351
Epoch 271/300, seasonal_3 Loss: 0.0402 | 0.0350
Epoch 272/300, seasonal_3 Loss: 0.0401 | 0.0350
Epoch 273/300, seasonal_3 Loss: 0.0401 | 0.0349
Epoch 274/300, seasonal_3 Loss: 0.0401 | 0.0350
Epoch 275/300, seasonal_3 Loss: 0.0401 | 0.0350
Epoch 276/300, seasonal_3 Loss: 0.0401 | 0.0350
Epoch 277/300, seasonal_3 Loss: 0.0401 | 0.0350
Epoch 278/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 279/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 280/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 281/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 282/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 283/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 284/300, seasonal_3 Loss: 0.0400 | 0.0350
Epoch 285/300, seasonal_3 Loss: 0.0400 | 0.0351
Epoch 286/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 287/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 288/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 289/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 290/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 291/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 292/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 293/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 294/300, seasonal_3 Loss: 0.0399 | 0.0351
Epoch 295/300, seasonal_3 Loss: 0.0398 | 0.0351
Epoch 296/300, seasonal_3 Loss: 0.0398 | 0.0351
Epoch 297/300, seasonal_3 Loss: 0.0398 | 0.0351
Epoch 298/300, seasonal_3 Loss: 0.0398 | 0.0351
Epoch 299/300, seasonal_3 Loss: 0.0398 | 0.0352
Epoch 300/300, seasonal_3 Loss: 0.0398 | 0.0351
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.8293946467396848, 'learning_rate': 0.0009206539859814936, 'batch_size': 193, 'step_size': 8, 'gamma': 0.8445158823309186}
Epoch 1/300, resid Loss: 0.5731 | 0.2256
Epoch 2/300, resid Loss: 0.1984 | 0.1785
Epoch 3/300, resid Loss: 0.1785 | 0.1248
Epoch 4/300, resid Loss: 0.1423 | 0.1323
Epoch 5/300, resid Loss: 0.1340 | 0.1284
Epoch 6/300, resid Loss: 0.1270 | 0.0903
Epoch 7/300, resid Loss: 0.1194 | 0.1017
Epoch 8/300, resid Loss: 0.1322 | 0.0906
Epoch 9/300, resid Loss: 0.1393 | 0.1068
Epoch 10/300, resid Loss: 0.1436 | 0.0876
Epoch 11/300, resid Loss: 0.1246 | 0.0942
Epoch 12/300, resid Loss: 0.1238 | 0.0854
Epoch 13/300, resid Loss: 0.1184 | 0.0929
Epoch 14/300, resid Loss: 0.1235 | 0.0892
Epoch 15/300, resid Loss: 0.1016 | 0.0697
Epoch 16/300, resid Loss: 0.0988 | 0.0598
Epoch 17/300, resid Loss: 0.0966 | 0.0581
Epoch 18/300, resid Loss: 0.0923 | 0.0537
Epoch 19/300, resid Loss: 0.0912 | 0.0555
Epoch 20/300, resid Loss: 0.0927 | 0.0512
Epoch 21/300, resid Loss: 0.0880 | 0.0504
Epoch 22/300, resid Loss: 0.0907 | 0.0513
Epoch 23/300, resid Loss: 0.0884 | 0.0498
Epoch 24/300, resid Loss: 0.0886 | 0.0490
Epoch 25/300, resid Loss: 0.0849 | 0.0476
Epoch 26/300, resid Loss: 0.0855 | 0.0462
Epoch 27/300, resid Loss: 0.0826 | 0.0448
Epoch 28/300, resid Loss: 0.0835 | 0.0445
Epoch 29/300, resid Loss: 0.0816 | 0.0432
Epoch 30/300, resid Loss: 0.0818 | 0.0437
Epoch 31/300, resid Loss: 0.0811 | 0.0426
Epoch 32/300, resid Loss: 0.0810 | 0.0422
Epoch 33/300, resid Loss: 0.0794 | 0.0413
Epoch 34/300, resid Loss: 0.0792 | 0.0417
Epoch 35/300, resid Loss: 0.0785 | 0.0411
Epoch 36/300, resid Loss: 0.0786 | 0.0412
Epoch 37/300, resid Loss: 0.0781 | 0.0402
Epoch 38/300, resid Loss: 0.0780 | 0.0403
Epoch 39/300, resid Loss: 0.0775 | 0.0395
Epoch 40/300, resid Loss: 0.0769 | 0.0394
Epoch 41/300, resid Loss: 0.0764 | 0.0389
Epoch 42/300, resid Loss: 0.0760 | 0.0389
Epoch 43/300, resid Loss: 0.0757 | 0.0384
Epoch 44/300, resid Loss: 0.0754 | 0.0383
Epoch 45/300, resid Loss: 0.0751 | 0.0379
Epoch 46/300, resid Loss: 0.0750 | 0.0379
Epoch 47/300, resid Loss: 0.0749 | 0.0377
Epoch 48/300, resid Loss: 0.0748 | 0.0376
Epoch 49/300, resid Loss: 0.0748 | 0.0373
Epoch 50/300, resid Loss: 0.0747 | 0.0372
Epoch 51/300, resid Loss: 0.0744 | 0.0371
Epoch 52/300, resid Loss: 0.0741 | 0.0370
Epoch 53/300, resid Loss: 0.0740 | 0.0369
Epoch 54/300, resid Loss: 0.0742 | 0.0369
Epoch 55/300, resid Loss: 0.0744 | 0.0367
Epoch 56/300, resid Loss: 0.0743 | 0.0365
Epoch 57/300, resid Loss: 0.0739 | 0.0364
Epoch 58/300, resid Loss: 0.0734 | 0.0365
Epoch 59/300, resid Loss: 0.0732 | 0.0364
Epoch 60/300, resid Loss: 0.0732 | 0.0362
Epoch 61/300, resid Loss: 0.0730 | 0.0359
Epoch 62/300, resid Loss: 0.0727 | 0.0359
Epoch 63/300, resid Loss: 0.0726 | 0.0358
Epoch 64/300, resid Loss: 0.0726 | 0.0357
Epoch 65/300, resid Loss: 0.0725 | 0.0357
Epoch 66/300, resid Loss: 0.0724 | 0.0356
Epoch 67/300, resid Loss: 0.0723 | 0.0355
Epoch 68/300, resid Loss: 0.0722 | 0.0354
Epoch 69/300, resid Loss: 0.0721 | 0.0354
Epoch 70/300, resid Loss: 0.0720 | 0.0353
Epoch 71/300, resid Loss: 0.0720 | 0.0353
Epoch 72/300, resid Loss: 0.0719 | 0.0352
Epoch 73/300, resid Loss: 0.0718 | 0.0351
Epoch 74/300, resid Loss: 0.0717 | 0.0351
Epoch 75/300, resid Loss: 0.0717 | 0.0350
Epoch 76/300, resid Loss: 0.0716 | 0.0350
Epoch 77/300, resid Loss: 0.0716 | 0.0350
Epoch 78/300, resid Loss: 0.0715 | 0.0349
Epoch 79/300, resid Loss: 0.0714 | 0.0349
Epoch 80/300, resid Loss: 0.0714 | 0.0348
Epoch 81/300, resid Loss: 0.0713 | 0.0348
Epoch 82/300, resid Loss: 0.0713 | 0.0348
Epoch 83/300, resid Loss: 0.0712 | 0.0347
Epoch 84/300, resid Loss: 0.0712 | 0.0347
Epoch 85/300, resid Loss: 0.0711 | 0.0346
Epoch 86/300, resid Loss: 0.0711 | 0.0346
Epoch 87/300, resid Loss: 0.0711 | 0.0346
Epoch 88/300, resid Loss: 0.0710 | 0.0345
Epoch 89/300, resid Loss: 0.0710 | 0.0345
Epoch 90/300, resid Loss: 0.0709 | 0.0345
Epoch 91/300, resid Loss: 0.0709 | 0.0344
Epoch 92/300, resid Loss: 0.0708 | 0.0344
Epoch 93/300, resid Loss: 0.0708 | 0.0344
Epoch 94/300, resid Loss: 0.0708 | 0.0344
Epoch 95/300, resid Loss: 0.0707 | 0.0343
Epoch 96/300, resid Loss: 0.0707 | 0.0343
Epoch 97/300, resid Loss: 0.0707 | 0.0343
Epoch 98/300, resid Loss: 0.0706 | 0.0342
Epoch 99/300, resid Loss: 0.0706 | 0.0342
Epoch 100/300, resid Loss: 0.0706 | 0.0342
Epoch 101/300, resid Loss: 0.0705 | 0.0342
Epoch 102/300, resid Loss: 0.0705 | 0.0342
Epoch 103/300, resid Loss: 0.0705 | 0.0341
Epoch 104/300, resid Loss: 0.0705 | 0.0341
Epoch 105/300, resid Loss: 0.0704 | 0.0341
Epoch 106/300, resid Loss: 0.0704 | 0.0341
Epoch 107/300, resid Loss: 0.0704 | 0.0341
Epoch 108/300, resid Loss: 0.0704 | 0.0340
Epoch 109/300, resid Loss: 0.0703 | 0.0340
Epoch 110/300, resid Loss: 0.0703 | 0.0340
Epoch 111/300, resid Loss: 0.0703 | 0.0340
Epoch 112/300, resid Loss: 0.0703 | 0.0340
Epoch 113/300, resid Loss: 0.0703 | 0.0340
Epoch 114/300, resid Loss: 0.0702 | 0.0339
Epoch 115/300, resid Loss: 0.0702 | 0.0339
Epoch 116/300, resid Loss: 0.0702 | 0.0339
Epoch 117/300, resid Loss: 0.0702 | 0.0339
Epoch 118/300, resid Loss: 0.0702 | 0.0339
Epoch 119/300, resid Loss: 0.0701 | 0.0339
Epoch 120/300, resid Loss: 0.0701 | 0.0339
Epoch 121/300, resid Loss: 0.0701 | 0.0339
Epoch 122/300, resid Loss: 0.0701 | 0.0338
Epoch 123/300, resid Loss: 0.0701 | 0.0338
Epoch 124/300, resid Loss: 0.0701 | 0.0338
Epoch 125/300, resid Loss: 0.0701 | 0.0338
Epoch 126/300, resid Loss: 0.0700 | 0.0338
Epoch 127/300, resid Loss: 0.0700 | 0.0338
Epoch 128/300, resid Loss: 0.0700 | 0.0338
Epoch 129/300, resid Loss: 0.0700 | 0.0338
Epoch 130/300, resid Loss: 0.0700 | 0.0338
Epoch 131/300, resid Loss: 0.0700 | 0.0338
Epoch 132/300, resid Loss: 0.0700 | 0.0337
Epoch 133/300, resid Loss: 0.0700 | 0.0337
Epoch 134/300, resid Loss: 0.0700 | 0.0337
Epoch 135/300, resid Loss: 0.0700 | 0.0337
Epoch 136/300, resid Loss: 0.0699 | 0.0337
Epoch 137/300, resid Loss: 0.0699 | 0.0337
Epoch 138/300, resid Loss: 0.0699 | 0.0337
Epoch 139/300, resid Loss: 0.0699 | 0.0337
Epoch 140/300, resid Loss: 0.0699 | 0.0337
Epoch 141/300, resid Loss: 0.0699 | 0.0337
Epoch 142/300, resid Loss: 0.0699 | 0.0337
Epoch 143/300, resid Loss: 0.0699 | 0.0337
Epoch 144/300, resid Loss: 0.0699 | 0.0337
Epoch 145/300, resid Loss: 0.0699 | 0.0337
Epoch 146/300, resid Loss: 0.0699 | 0.0337
Epoch 147/300, resid Loss: 0.0699 | 0.0337
Epoch 148/300, resid Loss: 0.0699 | 0.0337
Epoch 149/300, resid Loss: 0.0698 | 0.0336
Epoch 150/300, resid Loss: 0.0698 | 0.0336
Epoch 151/300, resid Loss: 0.0698 | 0.0336
Epoch 152/300, resid Loss: 0.0698 | 0.0336
Epoch 153/300, resid Loss: 0.0698 | 0.0336
Epoch 154/300, resid Loss: 0.0698 | 0.0336
Epoch 155/300, resid Loss: 0.0698 | 0.0336
Epoch 156/300, resid Loss: 0.0698 | 0.0336
Epoch 157/300, resid Loss: 0.0698 | 0.0336
Epoch 158/300, resid Loss: 0.0698 | 0.0336
Epoch 159/300, resid Loss: 0.0698 | 0.0336
Epoch 160/300, resid Loss: 0.0698 | 0.0336
Epoch 161/300, resid Loss: 0.0698 | 0.0336
Epoch 162/300, resid Loss: 0.0698 | 0.0336
Epoch 163/300, resid Loss: 0.0698 | 0.0336
Epoch 164/300, resid Loss: 0.0698 | 0.0336
Epoch 165/300, resid Loss: 0.0698 | 0.0336
Epoch 166/300, resid Loss: 0.0698 | 0.0336
Epoch 167/300, resid Loss: 0.0698 | 0.0336
Epoch 168/300, resid Loss: 0.0698 | 0.0336
Epoch 169/300, resid Loss: 0.0698 | 0.0336
Epoch 170/300, resid Loss: 0.0698 | 0.0336
Epoch 171/300, resid Loss: 0.0698 | 0.0336
Epoch 172/300, resid Loss: 0.0698 | 0.0336
Epoch 173/300, resid Loss: 0.0698 | 0.0336
Epoch 174/300, resid Loss: 0.0698 | 0.0336
Epoch 175/300, resid Loss: 0.0698 | 0.0336
Epoch 176/300, resid Loss: 0.0698 | 0.0336
Epoch 177/300, resid Loss: 0.0698 | 0.0336
Epoch 178/300, resid Loss: 0.0698 | 0.0336
Epoch 179/300, resid Loss: 0.0698 | 0.0336
Epoch 180/300, resid Loss: 0.0698 | 0.0336
Epoch 181/300, resid Loss: 0.0697 | 0.0336
Epoch 182/300, resid Loss: 0.0697 | 0.0336
Epoch 183/300, resid Loss: 0.0697 | 0.0336
Epoch 184/300, resid Loss: 0.0697 | 0.0336
Epoch 185/300, resid Loss: 0.0697 | 0.0336
Epoch 186/300, resid Loss: 0.0697 | 0.0336
Epoch 187/300, resid Loss: 0.0697 | 0.0336
Epoch 188/300, resid Loss: 0.0697 | 0.0336
Epoch 189/300, resid Loss: 0.0697 | 0.0336
Epoch 190/300, resid Loss: 0.0697 | 0.0336
Epoch 191/300, resid Loss: 0.0697 | 0.0336
Epoch 192/300, resid Loss: 0.0697 | 0.0336
Epoch 193/300, resid Loss: 0.0697 | 0.0336
Epoch 194/300, resid Loss: 0.0697 | 0.0336
Epoch 195/300, resid Loss: 0.0697 | 0.0336
Epoch 196/300, resid Loss: 0.0697 | 0.0336
Epoch 197/300, resid Loss: 0.0697 | 0.0336
Epoch 198/300, resid Loss: 0.0697 | 0.0336
Epoch 199/300, resid Loss: 0.0697 | 0.0336
Epoch 200/300, resid Loss: 0.0697 | 0.0336
Epoch 201/300, resid Loss: 0.0697 | 0.0336
Epoch 202/300, resid Loss: 0.0697 | 0.0336
Epoch 203/300, resid Loss: 0.0697 | 0.0335
Epoch 204/300, resid Loss: 0.0697 | 0.0335
Epoch 205/300, resid Loss: 0.0697 | 0.0335
Epoch 206/300, resid Loss: 0.0697 | 0.0335
Epoch 207/300, resid Loss: 0.0697 | 0.0335
Epoch 208/300, resid Loss: 0.0697 | 0.0335
Epoch 209/300, resid Loss: 0.0697 | 0.0335
Epoch 210/300, resid Loss: 0.0697 | 0.0335
Epoch 211/300, resid Loss: 0.0697 | 0.0335
Epoch 212/300, resid Loss: 0.0697 | 0.0335
Epoch 213/300, resid Loss: 0.0697 | 0.0335
Epoch 214/300, resid Loss: 0.0697 | 0.0335
Epoch 215/300, resid Loss: 0.0697 | 0.0335
Epoch 216/300, resid Loss: 0.0697 | 0.0335
Epoch 217/300, resid Loss: 0.0697 | 0.0335
Epoch 218/300, resid Loss: 0.0697 | 0.0335
Epoch 219/300, resid Loss: 0.0697 | 0.0335
Epoch 220/300, resid Loss: 0.0697 | 0.0335
Epoch 221/300, resid Loss: 0.0697 | 0.0335
Epoch 222/300, resid Loss: 0.0697 | 0.0335
Epoch 223/300, resid Loss: 0.0697 | 0.0335
Epoch 224/300, resid Loss: 0.0697 | 0.0335
Epoch 225/300, resid Loss: 0.0697 | 0.0335
Epoch 226/300, resid Loss: 0.0697 | 0.0335
Epoch 227/300, resid Loss: 0.0697 | 0.0335
Epoch 228/300, resid Loss: 0.0697 | 0.0335
Epoch 229/300, resid Loss: 0.0697 | 0.0335
Epoch 230/300, resid Loss: 0.0697 | 0.0335
Epoch 231/300, resid Loss: 0.0697 | 0.0335
Epoch 232/300, resid Loss: 0.0697 | 0.0335
Epoch 233/300, resid Loss: 0.0697 | 0.0335
Epoch 234/300, resid Loss: 0.0697 | 0.0335
Epoch 235/300, resid Loss: 0.0697 | 0.0335
Epoch 236/300, resid Loss: 0.0697 | 0.0335
Epoch 237/300, resid Loss: 0.0697 | 0.0335
Epoch 238/300, resid Loss: 0.0697 | 0.0335
Epoch 239/300, resid Loss: 0.0697 | 0.0335
Epoch 240/300, resid Loss: 0.0697 | 0.0335
Epoch 241/300, resid Loss: 0.0697 | 0.0335
Epoch 242/300, resid Loss: 0.0697 | 0.0335
Epoch 243/300, resid Loss: 0.0697 | 0.0335
Epoch 244/300, resid Loss: 0.0697 | 0.0335
Epoch 245/300, resid Loss: 0.0697 | 0.0335
Epoch 246/300, resid Loss: 0.0697 | 0.0335
Epoch 247/300, resid Loss: 0.0697 | 0.0335
Epoch 248/300, resid Loss: 0.0697 | 0.0335
Epoch 249/300, resid Loss: 0.0697 | 0.0335
Epoch 250/300, resid Loss: 0.0697 | 0.0335
Epoch 251/300, resid Loss: 0.0697 | 0.0335
Epoch 252/300, resid Loss: 0.0697 | 0.0335
Epoch 253/300, resid Loss: 0.0697 | 0.0335
Epoch 254/300, resid Loss: 0.0697 | 0.0335
Epoch 255/300, resid Loss: 0.0697 | 0.0335
Epoch 256/300, resid Loss: 0.0697 | 0.0335
Epoch 257/300, resid Loss: 0.0697 | 0.0335
Epoch 258/300, resid Loss: 0.0697 | 0.0335
Epoch 259/300, resid Loss: 0.0697 | 0.0335
Epoch 260/300, resid Loss: 0.0697 | 0.0335
Epoch 261/300, resid Loss: 0.0697 | 0.0335
Epoch 262/300, resid Loss: 0.0697 | 0.0335
Epoch 263/300, resid Loss: 0.0697 | 0.0335
Epoch 264/300, resid Loss: 0.0697 | 0.0335
Epoch 265/300, resid Loss: 0.0697 | 0.0335
Epoch 266/300, resid Loss: 0.0697 | 0.0335
Epoch 267/300, resid Loss: 0.0697 | 0.0335
Epoch 268/300, resid Loss: 0.0697 | 0.0335
Epoch 269/300, resid Loss: 0.0697 | 0.0335
Epoch 270/300, resid Loss: 0.0697 | 0.0335
Epoch 271/300, resid Loss: 0.0697 | 0.0335
Epoch 272/300, resid Loss: 0.0697 | 0.0335
Epoch 273/300, resid Loss: 0.0697 | 0.0335
Epoch 274/300, resid Loss: 0.0697 | 0.0335
Epoch 275/300, resid Loss: 0.0697 | 0.0335
Epoch 276/300, resid Loss: 0.0697 | 0.0335
Epoch 277/300, resid Loss: 0.0697 | 0.0335
Epoch 278/300, resid Loss: 0.0697 | 0.0335
Epoch 279/300, resid Loss: 0.0697 | 0.0335
Epoch 280/300, resid Loss: 0.0697 | 0.0335
Epoch 281/300, resid Loss: 0.0697 | 0.0335
Epoch 282/300, resid Loss: 0.0697 | 0.0335
Epoch 283/300, resid Loss: 0.0697 | 0.0335
Epoch 284/300, resid Loss: 0.0697 | 0.0335
Epoch 285/300, resid Loss: 0.0697 | 0.0335
Epoch 286/300, resid Loss: 0.0697 | 0.0335
Epoch 287/300, resid Loss: 0.0697 | 0.0335
Epoch 288/300, resid Loss: 0.0697 | 0.0335
Epoch 289/300, resid Loss: 0.0697 | 0.0335
Epoch 290/300, resid Loss: 0.0697 | 0.0335
Epoch 291/300, resid Loss: 0.0697 | 0.0335
Epoch 292/300, resid Loss: 0.0697 | 0.0335
Epoch 293/300, resid Loss: 0.0697 | 0.0335
Epoch 294/300, resid Loss: 0.0697 | 0.0335
Epoch 295/300, resid Loss: 0.0697 | 0.0335
Epoch 296/300, resid Loss: 0.0697 | 0.0335
Epoch 297/300, resid Loss: 0.0697 | 0.0335
Epoch 298/300, resid Loss: 0.0697 | 0.0335
Epoch 299/300, resid Loss: 0.0697 | 0.0335
Epoch 300/300, resid Loss: 0.0697 | 0.0335
Runtime (seconds): 1477.039606332779
0.0006447926509531084
[216.28899]
[0.3412638]
[-4.1228237]
[3.2196503]
[1.0064406]
[9.396509]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 45.42705698404461
RMSE: 6.739959716796875
MAE: 6.739959716796875
R-squared: nan
[226.13004]
