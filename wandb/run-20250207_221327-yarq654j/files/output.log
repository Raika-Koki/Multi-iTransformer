[32m[I 2025-02-07 22:13:32,974][0m A new study created in memory with name: no-name-d000f88b-05c3-4bc8-b77b-18dac553ffa3[0m
[32m[I 2025-02-07 22:15:55,122][0m Trial 0 finished with value: 0.2868052668409119 and parameters: {'observation_period_num': 13, 'train_rates': 0.7345051643079648, 'learning_rate': 3.0027994314904757e-06, 'batch_size': 35, 'step_size': 9, 'gamma': 0.7842806599005787}. Best is trial 0 with value: 0.2868052668409119.[0m
[32m[I 2025-02-07 22:16:20,710][0m Trial 1 finished with value: 0.29888323327851674 and parameters: {'observation_period_num': 151, 'train_rates': 0.6682933987180898, 'learning_rate': 0.0005354239128663066, 'batch_size': 200, 'step_size': 4, 'gamma': 0.8237990676534912}. Best is trial 0 with value: 0.2868052668409119.[0m
[32m[I 2025-02-07 22:18:02,921][0m Trial 2 finished with value: 0.23201839511211103 and parameters: {'observation_period_num': 182, 'train_rates': 0.972504137931794, 'learning_rate': 0.00023545256938881445, 'batch_size': 56, 'step_size': 1, 'gamma': 0.9655071540836624}. Best is trial 2 with value: 0.23201839511211103.[0m
[32m[I 2025-02-07 22:18:30,196][0m Trial 3 finished with value: 0.28208088874816895 and parameters: {'observation_period_num': 231, 'train_rates': 0.9631858701836711, 'learning_rate': 0.0004350588517842906, 'batch_size': 229, 'step_size': 12, 'gamma': 0.9092297672587406}. Best is trial 2 with value: 0.23201839511211103.[0m
[32m[I 2025-02-07 22:19:07,870][0m Trial 4 finished with value: 0.5157721238690607 and parameters: {'observation_period_num': 216, 'train_rates': 0.7281023788117144, 'learning_rate': 9.068962661124059e-06, 'batch_size': 131, 'step_size': 15, 'gamma': 0.9028062926182233}. Best is trial 2 with value: 0.23201839511211103.[0m
[32m[I 2025-02-07 22:20:11,229][0m Trial 5 finished with value: 0.2134473048392616 and parameters: {'observation_period_num': 136, 'train_rates': 0.6130602632328208, 'learning_rate': 0.00019212353147440451, 'batch_size': 68, 'step_size': 9, 'gamma': 0.9770331221002049}. Best is trial 5 with value: 0.2134473048392616.[0m
[32m[I 2025-02-07 22:20:33,813][0m Trial 6 finished with value: 0.30355653239088193 and parameters: {'observation_period_num': 56, 'train_rates': 0.7340059362352298, 'learning_rate': 0.0003359676173232583, 'batch_size': 242, 'step_size': 2, 'gamma': 0.7932851036212364}. Best is trial 5 with value: 0.2134473048392616.[0m
[32m[I 2025-02-07 22:20:54,252][0m Trial 7 finished with value: 1.6920911681947841 and parameters: {'observation_period_num': 160, 'train_rates': 0.6737940946238605, 'learning_rate': 3.3627631222341204e-06, 'batch_size': 251, 'step_size': 4, 'gamma': 0.8833081096931109}. Best is trial 5 with value: 0.2134473048392616.[0m
[32m[I 2025-02-07 22:21:16,812][0m Trial 8 finished with value: 0.20673300381095883 and parameters: {'observation_period_num': 48, 'train_rates': 0.7397191220514426, 'learning_rate': 7.989535007792697e-05, 'batch_size': 252, 'step_size': 13, 'gamma': 0.9147807788152409}. Best is trial 8 with value: 0.20673300381095883.[0m
[32m[I 2025-02-07 22:22:13,081][0m Trial 9 finished with value: 0.2534901429920858 and parameters: {'observation_period_num': 147, 'train_rates': 0.6260746777986163, 'learning_rate': 0.0007716181995922545, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8717093956257775}. Best is trial 8 with value: 0.20673300381095883.[0m
[32m[I 2025-02-07 22:22:47,186][0m Trial 10 finished with value: 0.11758076080254146 and parameters: {'observation_period_num': 77, 'train_rates': 0.8588771627390398, 'learning_rate': 4.964025284408609e-05, 'batch_size': 177, 'step_size': 15, 'gamma': 0.9383751986745632}. Best is trial 10 with value: 0.11758076080254146.[0m
[32m[I 2025-02-07 22:23:22,271][0m Trial 11 finished with value: 0.11338778637582436 and parameters: {'observation_period_num': 81, 'train_rates': 0.8687041895301386, 'learning_rate': 4.506419999597191e-05, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9463478670589209}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:23:58,516][0m Trial 12 finished with value: 0.1863582939027794 and parameters: {'observation_period_num': 90, 'train_rates': 0.870269864266828, 'learning_rate': 2.703532815926648e-05, 'batch_size': 165, 'step_size': 15, 'gamma': 0.9421221336443857}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:24:41,546][0m Trial 13 finished with value: 0.15587546892072024 and parameters: {'observation_period_num': 102, 'train_rates': 0.882111333914329, 'learning_rate': 3.360613309693421e-05, 'batch_size': 135, 'step_size': 13, 'gamma': 0.9434363728982627}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:25:15,147][0m Trial 14 finished with value: 0.1646876499387953 and parameters: {'observation_period_num': 87, 'train_rates': 0.8455230218944276, 'learning_rate': 5.1821093657053654e-05, 'batch_size': 176, 'step_size': 7, 'gamma': 0.8386397047663783}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:26:04,816][0m Trial 15 finished with value: 0.12856251706359206 and parameters: {'observation_period_num': 47, 'train_rates': 0.8111825747549473, 'learning_rate': 1.2989459424724713e-05, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9376548880359751}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:26:35,648][0m Trial 16 finished with value: 0.40534051895141604 and parameters: {'observation_period_num': 106, 'train_rates': 0.9222809599023828, 'learning_rate': 9.270448925516774e-05, 'batch_size': 201, 'step_size': 15, 'gamma': 0.9884906776840068}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:27:10,066][0m Trial 17 finished with value: 0.47466778935808124 and parameters: {'observation_period_num': 66, 'train_rates': 0.7975580456393799, 'learning_rate': 1.2141646128274542e-05, 'batch_size': 167, 'step_size': 7, 'gamma': 0.7516503460004229}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:28:08,783][0m Trial 18 finished with value: 0.4618442944117955 and parameters: {'observation_period_num': 11, 'train_rates': 0.9180907360760274, 'learning_rate': 1.0668692155146258e-06, 'batch_size': 104, 'step_size': 10, 'gamma': 0.9578684713578332}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:28:37,195][0m Trial 19 finished with value: 0.13151361932188776 and parameters: {'observation_period_num': 119, 'train_rates': 0.8362548814409276, 'learning_rate': 8.816859926620931e-05, 'batch_size': 208, 'step_size': 14, 'gamma': 0.8490521024127495}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:29:16,929][0m Trial 20 finished with value: 0.16430615831779527 and parameters: {'observation_period_num': 79, 'train_rates': 0.9189081238304783, 'learning_rate': 2.5654502112602492e-05, 'batch_size': 154, 'step_size': 13, 'gamma': 0.9168983775452041}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:30:07,795][0m Trial 21 finished with value: 0.11748730304916173 and parameters: {'observation_period_num': 38, 'train_rates': 0.7961125516021672, 'learning_rate': 1.214418634436965e-05, 'batch_size': 109, 'step_size': 11, 'gamma': 0.938956101409863}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:30:59,175][0m Trial 22 finished with value: 0.22551366037508938 and parameters: {'observation_period_num': 39, 'train_rates': 0.7714706300256579, 'learning_rate': 1.678222431680519e-05, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8911946832105192}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:31:33,329][0m Trial 23 finished with value: 0.3784971281238224 and parameters: {'observation_period_num': 29, 'train_rates': 0.8840279834314297, 'learning_rate': 4.906013502683161e-06, 'batch_size': 183, 'step_size': 14, 'gamma': 0.9288371036635878}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:32:11,986][0m Trial 24 finished with value: 0.14695659279823303 and parameters: {'observation_period_num': 70, 'train_rates': 0.8293862386610997, 'learning_rate': 4.483055563458354e-05, 'batch_size': 146, 'step_size': 14, 'gamma': 0.9638146989749843}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:32:57,840][0m Trial 25 finished with value: 0.15915623013684643 and parameters: {'observation_period_num': 27, 'train_rates': 0.7873846333450174, 'learning_rate': 6.152348177960142e-06, 'batch_size': 123, 'step_size': 11, 'gamma': 0.9893686152771639}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:34:02,695][0m Trial 26 finished with value: 0.17597667383347199 and parameters: {'observation_period_num': 115, 'train_rates': 0.855449175181174, 'learning_rate': 0.00015050512812906443, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9475372882029858}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:34:29,383][0m Trial 27 finished with value: 0.3217237467091635 and parameters: {'observation_period_num': 91, 'train_rates': 0.766512211043213, 'learning_rate': 2.0260605794295325e-05, 'batch_size': 220, 'step_size': 12, 'gamma': 0.9233470158929469}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:35:03,707][0m Trial 28 finished with value: 0.1153763390464496 and parameters: {'observation_period_num': 62, 'train_rates': 0.8925701217557778, 'learning_rate': 4.9268395020741144e-05, 'batch_size': 181, 'step_size': 9, 'gamma': 0.8946567259099427}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:39:58,463][0m Trial 29 finished with value: 0.24587659809574391 and parameters: {'observation_period_num': 23, 'train_rates': 0.9466176245290729, 'learning_rate': 1.4546093507527938e-06, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8658137534465543}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:40:40,957][0m Trial 30 finished with value: 0.13768595126988012 and parameters: {'observation_period_num': 6, 'train_rates': 0.8944947203032944, 'learning_rate': 6.558045774588016e-06, 'batch_size': 148, 'step_size': 8, 'gamma': 0.8944146128280293}. Best is trial 11 with value: 0.11338778637582436.[0m
[32m[I 2025-02-07 22:41:10,980][0m Trial 31 finished with value: 0.09461077424171178 and parameters: {'observation_period_num': 59, 'train_rates': 0.8122541767459855, 'learning_rate': 5.5983131557790624e-05, 'batch_size': 186, 'step_size': 5, 'gamma': 0.9534036936132403}. Best is trial 31 with value: 0.09461077424171178.[0m
[32m[I 2025-02-07 22:41:41,351][0m Trial 32 finished with value: 0.09656359553875045 and parameters: {'observation_period_num': 62, 'train_rates': 0.8116009571864709, 'learning_rate': 6.344683986522044e-05, 'batch_size': 194, 'step_size': 5, 'gamma': 0.9606285129796721}. Best is trial 31 with value: 0.09461077424171178.[0m
[32m[I 2025-02-07 22:42:11,901][0m Trial 33 finished with value: 0.0812716174721148 and parameters: {'observation_period_num': 61, 'train_rates': 0.8221086274561088, 'learning_rate': 0.00012864887987097779, 'batch_size': 191, 'step_size': 5, 'gamma': 0.9716993302432309}. Best is trial 33 with value: 0.0812716174721148.[0m
[32m[I 2025-02-07 22:42:41,760][0m Trial 34 finished with value: 0.11069465785044443 and parameters: {'observation_period_num': 57, 'train_rates': 0.8191340181899778, 'learning_rate': 0.00012622430024366015, 'batch_size': 192, 'step_size': 4, 'gamma': 0.9766640432497803}. Best is trial 33 with value: 0.0812716174721148.[0m
[32m[I 2025-02-07 22:43:09,823][0m Trial 35 finished with value: 0.08074313224972905 and parameters: {'observation_period_num': 55, 'train_rates': 0.8115338228179749, 'learning_rate': 0.00012110397177616995, 'batch_size': 213, 'step_size': 5, 'gamma': 0.9780783777757983}. Best is trial 35 with value: 0.08074313224972905.[0m
[32m[I 2025-02-07 22:43:36,823][0m Trial 36 finished with value: 0.1817668384854318 and parameters: {'observation_period_num': 22, 'train_rates': 0.7577892960872551, 'learning_rate': 0.00026484775753837504, 'batch_size': 218, 'step_size': 5, 'gamma': 0.9724496462654608}. Best is trial 35 with value: 0.08074313224972905.[0m
[32m[I 2025-02-07 22:44:01,881][0m Trial 37 finished with value: 0.25483178292959063 and parameters: {'observation_period_num': 101, 'train_rates': 0.7025209027611604, 'learning_rate': 0.00014873392589855681, 'batch_size': 226, 'step_size': 5, 'gamma': 0.9553114495132999}. Best is trial 35 with value: 0.08074313224972905.[0m
[32m[I 2025-02-07 22:44:28,187][0m Trial 38 finished with value: 0.1917304641587271 and parameters: {'observation_period_num': 196, 'train_rates': 0.8018730862171983, 'learning_rate': 0.000457646082460133, 'batch_size': 206, 'step_size': 2, 'gamma': 0.9697660671938384}. Best is trial 35 with value: 0.08074313224972905.[0m
[32m[I 2025-02-07 22:44:56,584][0m Trial 39 finished with value: 0.2073620933380614 and parameters: {'observation_period_num': 48, 'train_rates': 0.7778738630169867, 'learning_rate': 0.0002381292230911122, 'batch_size': 193, 'step_size': 5, 'gamma': 0.9813885260285714}. Best is trial 35 with value: 0.08074313224972905.[0m
[32m[I 2025-02-07 22:45:18,610][0m Trial 40 finished with value: 0.5146824849692196 and parameters: {'observation_period_num': 237, 'train_rates': 0.7469858138315271, 'learning_rate': 7.458601954794422e-05, 'batch_size': 239, 'step_size': 3, 'gamma': 0.8109338333179903}. Best is trial 35 with value: 0.08074313224972905.[0m
[32m[I 2025-02-07 22:45:48,755][0m Trial 41 finished with value: 0.07538675022960613 and parameters: {'observation_period_num': 65, 'train_rates': 0.8202545601095872, 'learning_rate': 0.00012129287412565408, 'batch_size': 194, 'step_size': 4, 'gamma': 0.9762607155534186}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:46:16,029][0m Trial 42 finished with value: 0.13197673789479517 and parameters: {'observation_period_num': 68, 'train_rates': 0.8239284397100146, 'learning_rate': 0.00012267464038598914, 'batch_size': 235, 'step_size': 6, 'gamma': 0.9623414622806649}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:46:43,880][0m Trial 43 finished with value: 0.08662516720685942 and parameters: {'observation_period_num': 38, 'train_rates': 0.8106519078739661, 'learning_rate': 6.585775669049653e-05, 'batch_size': 215, 'step_size': 3, 'gamma': 0.9560482625275495}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:47:10,500][0m Trial 44 finished with value: 0.17397171851308618 and parameters: {'observation_period_num': 40, 'train_rates': 0.7114376428439235, 'learning_rate': 0.00018693336336207087, 'batch_size': 210, 'step_size': 1, 'gamma': 0.9803340518834013}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:47:33,603][0m Trial 45 finished with value: 0.15839417582803778 and parameters: {'observation_period_num': 167, 'train_rates': 0.8428398216097145, 'learning_rate': 0.0007863605071843951, 'batch_size': 255, 'step_size': 3, 'gamma': 0.9300030005846016}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:47:59,475][0m Trial 46 finished with value: 0.138100741216777 and parameters: {'observation_period_num': 134, 'train_rates': 0.7847189466655463, 'learning_rate': 0.00034147250434527415, 'batch_size': 218, 'step_size': 3, 'gamma': 0.9514305975844366}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:48:33,957][0m Trial 47 finished with value: 0.28746403111801444 and parameters: {'observation_period_num': 250, 'train_rates': 0.8608697934835792, 'learning_rate': 0.00011141498660994342, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9700846148849301}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:48:59,954][0m Trial 48 finished with value: 0.13801735525410022 and parameters: {'observation_period_num': 50, 'train_rates': 0.8321258141443062, 'learning_rate': 3.5141119146132875e-05, 'batch_size': 228, 'step_size': 6, 'gamma': 0.9053064389046663}. Best is trial 41 with value: 0.07538675022960613.[0m
[32m[I 2025-02-07 22:49:28,981][0m Trial 49 finished with value: 0.16934401678647915 and parameters: {'observation_period_num': 31, 'train_rates': 0.7228578736295158, 'learning_rate': 0.00016825280628123164, 'batch_size': 186, 'step_size': 2, 'gamma': 0.9884323409385656}. Best is trial 41 with value: 0.07538675022960613.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.4725 | 0.6569
Epoch 2/300, Loss: 0.3579 | 0.4494
Epoch 3/300, Loss: 0.3111 | 0.4123
Epoch 4/300, Loss: 0.3172 | 0.3089
Epoch 5/300, Loss: 0.2970 | 0.6188
Epoch 6/300, Loss: 0.2525 | 0.3042
Epoch 7/300, Loss: 0.2019 | 0.3747
Epoch 8/300, Loss: 0.1987 | 0.3181
Epoch 9/300, Loss: 0.1834 | 0.2980
Epoch 10/300, Loss: 0.1730 | 0.2804
Epoch 11/300, Loss: 0.1680 | 0.2646
Epoch 12/300, Loss: 0.1605 | 0.2526
Epoch 13/300, Loss: 0.1538 | 0.2340
Epoch 14/300, Loss: 0.1482 | 0.2262
Epoch 15/300, Loss: 0.1438 | 0.2101
Epoch 16/300, Loss: 0.1396 | 0.2010
Epoch 17/300, Loss: 0.1357 | 0.1889
Epoch 18/300, Loss: 0.1329 | 0.1792
Epoch 19/300, Loss: 0.1327 | 0.1792
Epoch 20/300, Loss: 0.1413 | 0.1747
Epoch 21/300, Loss: 0.1622 | 0.1667
Epoch 22/300, Loss: 0.2067 | 0.4603
Epoch 23/300, Loss: 0.1900 | 0.1797
Epoch 24/300, Loss: 0.1480 | 0.3064
Epoch 25/300, Loss: 0.1334 | 0.1853
Epoch 26/300, Loss: 0.1250 | 0.1765
Epoch 27/300, Loss: 0.1215 | 0.1741
Epoch 28/300, Loss: 0.1196 | 0.1567
Epoch 29/300, Loss: 0.1192 | 0.1543
Epoch 30/300, Loss: 0.1139 | 0.1423
Epoch 31/300, Loss: 0.1164 | 0.1418
Epoch 32/300, Loss: 0.1135 | 0.1331
Epoch 33/300, Loss: 0.1182 | 0.1340
Epoch 34/300, Loss: 0.1110 | 0.1244
Epoch 35/300, Loss: 0.1179 | 0.1321
Epoch 36/300, Loss: 0.1154 | 0.1179
Epoch 37/300, Loss: 0.1326 | 0.1882
Epoch 38/300, Loss: 0.1338 | 0.1189
Epoch 39/300, Loss: 0.1584 | 0.4028
Epoch 40/300, Loss: 0.1309 | 0.1483
Epoch 41/300, Loss: 0.1100 | 0.1672
Epoch 42/300, Loss: 0.1053 | 0.1339
Epoch 43/300, Loss: 0.1051 | 0.1230
Epoch 44/300, Loss: 0.1031 | 0.1311
Epoch 45/300, Loss: 0.1006 | 0.1155
Epoch 46/300, Loss: 0.0994 | 0.1141
Epoch 47/300, Loss: 0.0981 | 0.1085
Epoch 48/300, Loss: 0.0968 | 0.1097
Epoch 49/300, Loss: 0.0959 | 0.1001
Epoch 50/300, Loss: 0.0961 | 0.1097
Epoch 51/300, Loss: 0.0960 | 0.0965
Epoch 52/300, Loss: 0.0988 | 0.1242
Epoch 53/300, Loss: 0.1010 | 0.0940
Epoch 54/300, Loss: 0.1121 | 0.2254
Epoch 55/300, Loss: 0.1052 | 0.0991
Epoch 56/300, Loss: 0.1024 | 0.1876
Epoch 57/300, Loss: 0.0962 | 0.1002
Epoch 58/300, Loss: 0.0935 | 0.1205
Epoch 59/300, Loss: 0.0912 | 0.0955
Epoch 60/300, Loss: 0.0905 | 0.1066
Epoch 61/300, Loss: 0.0898 | 0.0895
Epoch 62/300, Loss: 0.0907 | 0.1100
Epoch 63/300, Loss: 0.0905 | 0.0874
Epoch 64/300, Loss: 0.0923 | 0.1229
Epoch 65/300, Loss: 0.0921 | 0.0855
Epoch 66/300, Loss: 0.0958 | 0.1576
Epoch 67/300, Loss: 0.0928 | 0.0875
Epoch 68/300, Loss: 0.0927 | 0.1443
Epoch 69/300, Loss: 0.0892 | 0.0870
Epoch 70/300, Loss: 0.0881 | 0.1164
Epoch 71/300, Loss: 0.0866 | 0.0845
Epoch 72/300, Loss: 0.0868 | 0.1081
Epoch 73/300, Loss: 0.0859 | 0.0826
Epoch 74/300, Loss: 0.0864 | 0.1072
Epoch 75/300, Loss: 0.0857 | 0.0808
Epoch 76/300, Loss: 0.0867 | 0.1129
Epoch 77/300, Loss: 0.0859 | 0.0803
Epoch 78/300, Loss: 0.0867 | 0.1167
Epoch 79/300, Loss: 0.0853 | 0.0798
Epoch 80/300, Loss: 0.0857 | 0.1144
Epoch 81/300, Loss: 0.0844 | 0.0791
Epoch 82/300, Loss: 0.0845 | 0.1085
Epoch 83/300, Loss: 0.0833 | 0.0784
Epoch 84/300, Loss: 0.0833 | 0.1022
Epoch 85/300, Loss: 0.0825 | 0.0772
Epoch 86/300, Loss: 0.0827 | 0.1003
Epoch 87/300, Loss: 0.0820 | 0.0766
Epoch 88/300, Loss: 0.0822 | 0.0982
Epoch 89/300, Loss: 0.0815 | 0.0756
Epoch 90/300, Loss: 0.0817 | 0.0978
Epoch 91/300, Loss: 0.0811 | 0.0750
Epoch 92/300, Loss: 0.0813 | 0.0968
Epoch 93/300, Loss: 0.0806 | 0.0745
Epoch 94/300, Loss: 0.0807 | 0.0947
Epoch 95/300, Loss: 0.0801 | 0.0738
Epoch 96/300, Loss: 0.0802 | 0.0937
Epoch 97/300, Loss: 0.0797 | 0.0734
Epoch 98/300, Loss: 0.0796 | 0.0908
Epoch 99/300, Loss: 0.0791 | 0.0727
Epoch 100/300, Loss: 0.0791 | 0.0897
Epoch 101/300, Loss: 0.0787 | 0.0723
Epoch 102/300, Loss: 0.0787 | 0.0878
Epoch 103/300, Loss: 0.0782 | 0.0718
Epoch 104/300, Loss: 0.0781 | 0.0861
Epoch 105/300, Loss: 0.0778 | 0.0712
Epoch 106/300, Loss: 0.0778 | 0.0854
Epoch 107/300, Loss: 0.0774 | 0.0709
Epoch 108/300, Loss: 0.0773 | 0.0833
Epoch 109/300, Loss: 0.0770 | 0.0703
Epoch 110/300, Loss: 0.0769 | 0.0828
Epoch 111/300, Loss: 0.0767 | 0.0700
Epoch 112/300, Loss: 0.0766 | 0.0813
Epoch 113/300, Loss: 0.0763 | 0.0697
Epoch 114/300, Loss: 0.0762 | 0.0800
Epoch 115/300, Loss: 0.0760 | 0.0692
Epoch 116/300, Loss: 0.0759 | 0.0794
Epoch 117/300, Loss: 0.0756 | 0.0690
Epoch 118/300, Loss: 0.0755 | 0.0777
Epoch 119/300, Loss: 0.0753 | 0.0686
Epoch 120/300, Loss: 0.0752 | 0.0773
Epoch 121/300, Loss: 0.0750 | 0.0683
Epoch 122/300, Loss: 0.0749 | 0.0761
Epoch 123/300, Loss: 0.0747 | 0.0681
Epoch 124/300, Loss: 0.0745 | 0.0750
Epoch 125/300, Loss: 0.0744 | 0.0677
Epoch 126/300, Loss: 0.0742 | 0.0746
Epoch 127/300, Loss: 0.0741 | 0.0676
Epoch 128/300, Loss: 0.0739 | 0.0734
Epoch 129/300, Loss: 0.0738 | 0.0674
Epoch 130/300, Loss: 0.0737 | 0.0728
Epoch 131/300, Loss: 0.0736 | 0.0671
Epoch 132/300, Loss: 0.0734 | 0.0722
Epoch 133/300, Loss: 0.0733 | 0.0670
Epoch 134/300, Loss: 0.0731 | 0.0713
Epoch 135/300, Loss: 0.0730 | 0.0668
Epoch 136/300, Loss: 0.0729 | 0.0709
Epoch 137/300, Loss: 0.0728 | 0.0666
Epoch 138/300, Loss: 0.0727 | 0.0702
Epoch 139/300, Loss: 0.0726 | 0.0665
Epoch 140/300, Loss: 0.0724 | 0.0696
Epoch 141/300, Loss: 0.0724 | 0.0663
Epoch 142/300, Loss: 0.0722 | 0.0692
Epoch 143/300, Loss: 0.0722 | 0.0663
Epoch 144/300, Loss: 0.0720 | 0.0686
Epoch 145/300, Loss: 0.0719 | 0.0662
Epoch 146/300, Loss: 0.0718 | 0.0682
Epoch 147/300, Loss: 0.0717 | 0.0660
Epoch 148/300, Loss: 0.0716 | 0.0679
Epoch 149/300, Loss: 0.0715 | 0.0660
Epoch 150/300, Loss: 0.0714 | 0.0674
Epoch 151/300, Loss: 0.0714 | 0.0659
Epoch 152/300, Loss: 0.0712 | 0.0671
Epoch 153/300, Loss: 0.0712 | 0.0658
Epoch 154/300, Loss: 0.0711 | 0.0668
Epoch 155/300, Loss: 0.0710 | 0.0658
Epoch 156/300, Loss: 0.0709 | 0.0665
Epoch 157/300, Loss: 0.0708 | 0.0657
Epoch 158/300, Loss: 0.0707 | 0.0663
Epoch 159/300, Loss: 0.0707 | 0.0656
Epoch 160/300, Loss: 0.0706 | 0.0661
Epoch 161/300, Loss: 0.0705 | 0.0656
Epoch 162/300, Loss: 0.0704 | 0.0659
Epoch 163/300, Loss: 0.0704 | 0.0655
Epoch 164/300, Loss: 0.0703 | 0.0657
Epoch 165/300, Loss: 0.0702 | 0.0654
Epoch 166/300, Loss: 0.0701 | 0.0656
Epoch 167/300, Loss: 0.0701 | 0.0653
Epoch 168/300, Loss: 0.0700 | 0.0655
Epoch 169/300, Loss: 0.0699 | 0.0653
Epoch 170/300, Loss: 0.0699 | 0.0653
Epoch 171/300, Loss: 0.0698 | 0.0652
Epoch 172/300, Loss: 0.0697 | 0.0652
Epoch 173/300, Loss: 0.0697 | 0.0651
Epoch 174/300, Loss: 0.0696 | 0.0651
Epoch 175/300, Loss: 0.0695 | 0.0650
Epoch 176/300, Loss: 0.0695 | 0.0650
Epoch 177/300, Loss: 0.0694 | 0.0650
Epoch 178/300, Loss: 0.0693 | 0.0649
Epoch 179/300, Loss: 0.0693 | 0.0649
Epoch 180/300, Loss: 0.0692 | 0.0649
Epoch 181/300, Loss: 0.0691 | 0.0648
Epoch 182/300, Loss: 0.0691 | 0.0648
Epoch 183/300, Loss: 0.0690 | 0.0648
Epoch 184/300, Loss: 0.0690 | 0.0647
Epoch 185/300, Loss: 0.0689 | 0.0647
Epoch 186/300, Loss: 0.0688 | 0.0647
Epoch 187/300, Loss: 0.0688 | 0.0646
Epoch 188/300, Loss: 0.0687 | 0.0646
Epoch 189/300, Loss: 0.0687 | 0.0646
Epoch 190/300, Loss: 0.0686 | 0.0645
Epoch 191/300, Loss: 0.0686 | 0.0645
Epoch 192/300, Loss: 0.0685 | 0.0645
Epoch 193/300, Loss: 0.0685 | 0.0645
Epoch 194/300, Loss: 0.0684 | 0.0644
Epoch 195/300, Loss: 0.0684 | 0.0644
Epoch 196/300, Loss: 0.0683 | 0.0644
Epoch 197/300, Loss: 0.0682 | 0.0644
Epoch 198/300, Loss: 0.0682 | 0.0643
Epoch 199/300, Loss: 0.0681 | 0.0643
Epoch 200/300, Loss: 0.0681 | 0.0643
Epoch 201/300, Loss: 0.0680 | 0.0643
Epoch 202/300, Loss: 0.0680 | 0.0642
Epoch 203/300, Loss: 0.0679 | 0.0642
Epoch 204/300, Loss: 0.0679 | 0.0642
Epoch 205/300, Loss: 0.0679 | 0.0642
Epoch 206/300, Loss: 0.0678 | 0.0642
Epoch 207/300, Loss: 0.0678 | 0.0641
Epoch 208/300, Loss: 0.0677 | 0.0642
Epoch 209/300, Loss: 0.0677 | 0.0641
Epoch 210/300, Loss: 0.0676 | 0.0641
Epoch 211/300, Loss: 0.0676 | 0.0641
Epoch 212/300, Loss: 0.0675 | 0.0640
Epoch 213/300, Loss: 0.0675 | 0.0641
Epoch 214/300, Loss: 0.0675 | 0.0640
Epoch 215/300, Loss: 0.0674 | 0.0640
Epoch 216/300, Loss: 0.0674 | 0.0640
Epoch 217/300, Loss: 0.0673 | 0.0639
Epoch 218/300, Loss: 0.0673 | 0.0640
Epoch 219/300, Loss: 0.0672 | 0.0639
Epoch 220/300, Loss: 0.0672 | 0.0639
Epoch 221/300, Loss: 0.0672 | 0.0640
Epoch 222/300, Loss: 0.0671 | 0.0638
Epoch 223/300, Loss: 0.0671 | 0.0640
Epoch 224/300, Loss: 0.0670 | 0.0638
Epoch 225/300, Loss: 0.0670 | 0.0638
Epoch 226/300, Loss: 0.0670 | 0.0639
Epoch 227/300, Loss: 0.0669 | 0.0637
Epoch 228/300, Loss: 0.0669 | 0.0639
Epoch 229/300, Loss: 0.0669 | 0.0638
Epoch 230/300, Loss: 0.0668 | 0.0638
Epoch 231/300, Loss: 0.0668 | 0.0639
Epoch 232/300, Loss: 0.0668 | 0.0637
Epoch 233/300, Loss: 0.0667 | 0.0638
Epoch 234/300, Loss: 0.0667 | 0.0637
Epoch 235/300, Loss: 0.0666 | 0.0637
Epoch 236/300, Loss: 0.0666 | 0.0638
Epoch 237/300, Loss: 0.0666 | 0.0636
Epoch 238/300, Loss: 0.0665 | 0.0638
Epoch 239/300, Loss: 0.0665 | 0.0637
Epoch 240/300, Loss: 0.0665 | 0.0637
Epoch 241/300, Loss: 0.0664 | 0.0637
Epoch 242/300, Loss: 0.0664 | 0.0636
Epoch 243/300, Loss: 0.0664 | 0.0637
Epoch 244/300, Loss: 0.0663 | 0.0636
Epoch 245/300, Loss: 0.0663 | 0.0636
Epoch 246/300, Loss: 0.0663 | 0.0636
Epoch 247/300, Loss: 0.0663 | 0.0636
Epoch 248/300, Loss: 0.0662 | 0.0637
Epoch 249/300, Loss: 0.0662 | 0.0636
Epoch 250/300, Loss: 0.0662 | 0.0636
Epoch 251/300, Loss: 0.0661 | 0.0636
Epoch 252/300, Loss: 0.0661 | 0.0636
Epoch 253/300, Loss: 0.0661 | 0.0636
Epoch 254/300, Loss: 0.0660 | 0.0635
Epoch 255/300, Loss: 0.0660 | 0.0636
Epoch 256/300, Loss: 0.0660 | 0.0635
Epoch 257/300, Loss: 0.0660 | 0.0635
Epoch 258/300, Loss: 0.0659 | 0.0635
Epoch 259/300, Loss: 0.0659 | 0.0635
Epoch 260/300, Loss: 0.0659 | 0.0635
Epoch 261/300, Loss: 0.0659 | 0.0635
Epoch 262/300, Loss: 0.0658 | 0.0635
Epoch 263/300, Loss: 0.0658 | 0.0635
Epoch 264/300, Loss: 0.0658 | 0.0635
Epoch 265/300, Loss: 0.0657 | 0.0635
Epoch 266/300, Loss: 0.0657 | 0.0635
Epoch 267/300, Loss: 0.0657 | 0.0635
Epoch 268/300, Loss: 0.0657 | 0.0635
Epoch 269/300, Loss: 0.0656 | 0.0635
Epoch 270/300, Loss: 0.0656 | 0.0635
Epoch 271/300, Loss: 0.0656 | 0.0635
Epoch 272/300, Loss: 0.0656 | 0.0635
Epoch 273/300, Loss: 0.0656 | 0.0634
Epoch 274/300, Loss: 0.0655 | 0.0634
Epoch 275/300, Loss: 0.0655 | 0.0634
Epoch 276/300, Loss: 0.0655 | 0.0634
Epoch 277/300, Loss: 0.0655 | 0.0634
Epoch 278/300, Loss: 0.0654 | 0.0634
Epoch 279/300, Loss: 0.0654 | 0.0634
Epoch 280/300, Loss: 0.0654 | 0.0634
Epoch 281/300, Loss: 0.0654 | 0.0634
Epoch 282/300, Loss: 0.0653 | 0.0634
Epoch 283/300, Loss: 0.0653 | 0.0634
Epoch 284/300, Loss: 0.0653 | 0.0634
Epoch 285/300, Loss: 0.0653 | 0.0634
Epoch 286/300, Loss: 0.0653 | 0.0634
Epoch 287/300, Loss: 0.0652 | 0.0634
Epoch 288/300, Loss: 0.0652 | 0.0634
Epoch 289/300, Loss: 0.0652 | 0.0634
Epoch 290/300, Loss: 0.0652 | 0.0634
Epoch 291/300, Loss: 0.0652 | 0.0634
Epoch 292/300, Loss: 0.0651 | 0.0634
Epoch 293/300, Loss: 0.0651 | 0.0634
Epoch 294/300, Loss: 0.0651 | 0.0634
Epoch 295/300, Loss: 0.0651 | 0.0634
Epoch 296/300, Loss: 0.0651 | 0.0634
Epoch 297/300, Loss: 0.0650 | 0.0633
Epoch 298/300, Loss: 0.0650 | 0.0633
Epoch 299/300, Loss: 0.0650 | 0.0633
Epoch 300/300, Loss: 0.0650 | 0.0633
Runtime (seconds): 90.0470700263977
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 4.0695132771506906
RMSE: 2.017303466796875
MAE: 2.017303466796875
R-squared: nan
[195.3273]
