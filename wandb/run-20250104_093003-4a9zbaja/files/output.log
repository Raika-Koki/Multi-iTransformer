ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 09:30:04,982][0m A new study created in memory with name: no-name-634b2437-104f-4858-9c45-e1147a5d1f72[0m
[32m[I 2025-01-04 09:30:29,931][0m Trial 0 finished with value: 0.46243178844451904 and parameters: {'observation_period_num': 242, 'train_rates': 0.9105731682064966, 'learning_rate': 9.17861417419842e-06, 'batch_size': 244, 'step_size': 11, 'gamma': 0.7730059509602453}. Best is trial 0 with value: 0.46243178844451904.[0m
[32m[I 2025-01-04 09:31:10,771][0m Trial 1 finished with value: 0.20777229595432248 and parameters: {'observation_period_num': 138, 'train_rates': 0.7950682109145735, 'learning_rate': 0.00010810972897856331, 'batch_size': 129, 'step_size': 4, 'gamma': 0.7544984623270442}. Best is trial 1 with value: 0.20777229595432248.[0m
[32m[I 2025-01-04 09:31:45,470][0m Trial 2 finished with value: 0.09063999354839325 and parameters: {'observation_period_num': 128, 'train_rates': 0.950232320300245, 'learning_rate': 0.0009073208269482515, 'batch_size': 170, 'step_size': 3, 'gamma': 0.8424441967079589}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:32:20,605][0m Trial 3 finished with value: 0.8031931378339466 and parameters: {'observation_period_num': 175, 'train_rates': 0.9179214220032756, 'learning_rate': 1.115771032223532e-06, 'batch_size': 162, 'step_size': 8, 'gamma': 0.9758551587745719}. Best is trial 2 with value: 0.09063999354839325.[0m
Early stopping at epoch 94
[32m[I 2025-01-04 09:32:50,320][0m Trial 4 finished with value: 1.2894853353500366 and parameters: {'observation_period_num': 190, 'train_rates': 0.9674841609712295, 'learning_rate': 4.094581200227366e-06, 'batch_size': 192, 'step_size': 2, 'gamma': 0.7984757252925874}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:36:33,033][0m Trial 5 finished with value: 0.27902512019165127 and parameters: {'observation_period_num': 50, 'train_rates': 0.6169827733548612, 'learning_rate': 6.693500200122784e-06, 'batch_size': 19, 'step_size': 8, 'gamma': 0.8102390754294925}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:37:22,786][0m Trial 6 finished with value: 0.40983593107080785 and parameters: {'observation_period_num': 198, 'train_rates': 0.7331307820547914, 'learning_rate': 9.699649696395881e-06, 'batch_size': 94, 'step_size': 5, 'gamma': 0.9367445737677416}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:38:13,353][0m Trial 7 finished with value: 0.7510636333682111 and parameters: {'observation_period_num': 150, 'train_rates': 0.8525685887922119, 'learning_rate': 1.0351646203405492e-06, 'batch_size': 106, 'step_size': 6, 'gamma': 0.9167820773776312}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:39:41,852][0m Trial 8 finished with value: 0.12864003230293258 and parameters: {'observation_period_num': 209, 'train_rates': 0.913280680681388, 'learning_rate': 0.0006648662056201577, 'batch_size': 61, 'step_size': 4, 'gamma': 0.8694638715383022}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:41:28,263][0m Trial 9 finished with value: 0.8456527635108593 and parameters: {'observation_period_num': 140, 'train_rates': 0.6942021671656942, 'learning_rate': 1.0945587846834546e-06, 'batch_size': 42, 'step_size': 11, 'gamma': 0.7574158160238258}. Best is trial 2 with value: 0.09063999354839325.[0m
Early stopping at epoch 79
[32m[I 2025-01-04 09:41:51,539][0m Trial 10 finished with value: 0.2126125693321228 and parameters: {'observation_period_num': 76, 'train_rates': 0.9896098868394931, 'learning_rate': 0.0009664911475194529, 'batch_size': 219, 'step_size': 1, 'gamma': 0.8500975950849456}. Best is trial 2 with value: 0.09063999354839325.[0m
[32m[I 2025-01-04 09:43:00,557][0m Trial 11 finished with value: 0.08760292538680321 and parameters: {'observation_period_num': 94, 'train_rates': 0.8733082897604875, 'learning_rate': 0.0009232895571630927, 'batch_size': 78, 'step_size': 15, 'gamma': 0.8688240101397381}. Best is trial 11 with value: 0.08760292538680321.[0m
[32m[I 2025-01-04 09:43:34,367][0m Trial 12 finished with value: 0.0909470665362963 and parameters: {'observation_period_num': 94, 'train_rates': 0.8412313223792776, 'learning_rate': 0.00019083308943536602, 'batch_size': 167, 'step_size': 12, 'gamma': 0.8354713821212102}. Best is trial 11 with value: 0.08760292538680321.[0m
[32m[I 2025-01-04 09:44:31,578][0m Trial 13 finished with value: 0.034025383721056734 and parameters: {'observation_period_num': 7, 'train_rates': 0.8582421107498694, 'learning_rate': 0.00020613896124301642, 'batch_size': 99, 'step_size': 14, 'gamma': 0.8926791946903674}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:45:38,663][0m Trial 14 finished with value: 0.03726225181256525 and parameters: {'observation_period_num': 17, 'train_rates': 0.8443664895623443, 'learning_rate': 0.00021158561875310895, 'batch_size': 81, 'step_size': 15, 'gamma': 0.9075394304954904}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:46:25,060][0m Trial 15 finished with value: 0.037063020581275606 and parameters: {'observation_period_num': 8, 'train_rates': 0.7626517918713756, 'learning_rate': 8.417666596652505e-05, 'batch_size': 113, 'step_size': 15, 'gamma': 0.911612941216343}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:47:07,053][0m Trial 16 finished with value: 0.04288165804227875 and parameters: {'observation_period_num': 8, 'train_rates': 0.7627480878468935, 'learning_rate': 4.917093482132899e-05, 'batch_size': 128, 'step_size': 14, 'gamma': 0.9543866416103703}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:47:50,292][0m Trial 17 finished with value: 0.08672171412610713 and parameters: {'observation_period_num': 38, 'train_rates': 0.6664572680041441, 'learning_rate': 3.242249523973076e-05, 'batch_size': 114, 'step_size': 13, 'gamma': 0.9112715808543393}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:48:26,781][0m Trial 18 finished with value: 0.05396332849966736 and parameters: {'observation_period_num': 46, 'train_rates': 0.7960378270638692, 'learning_rate': 7.521467434898311e-05, 'batch_size': 151, 'step_size': 10, 'gamma': 0.9005207614086589}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:49:56,644][0m Trial 19 finished with value: 0.1113008321694831 and parameters: {'observation_period_num': 77, 'train_rates': 0.7263784121821558, 'learning_rate': 0.0003584587072778701, 'batch_size': 53, 'step_size': 13, 'gamma': 0.9892976996737678}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:50:26,236][0m Trial 20 finished with value: 0.08042906926422523 and parameters: {'observation_period_num': 25, 'train_rates': 0.8063000925629814, 'learning_rate': 2.343203966440272e-05, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8854110334905028}. Best is trial 13 with value: 0.034025383721056734.[0m
[32m[I 2025-01-04 09:51:31,642][0m Trial 21 finished with value: 0.026341457959347975 and parameters: {'observation_period_num': 5, 'train_rates': 0.8335554633803406, 'learning_rate': 0.00021076009161061788, 'batch_size': 84, 'step_size': 15, 'gamma': 0.9429049476971336}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 09:52:30,241][0m Trial 22 finished with value: 0.029011188275792927 and parameters: {'observation_period_num': 8, 'train_rates': 0.7703482096137859, 'learning_rate': 0.00013409862698874142, 'batch_size': 88, 'step_size': 15, 'gamma': 0.9461493231762539}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 09:53:42,230][0m Trial 23 finished with value: 0.07061340141052243 and parameters: {'observation_period_num': 57, 'train_rates': 0.8777429199411056, 'learning_rate': 0.0002971040163208539, 'batch_size': 77, 'step_size': 14, 'gamma': 0.9452205727509841}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 09:56:30,192][0m Trial 24 finished with value: 0.07862080120067032 and parameters: {'observation_period_num': 26, 'train_rates': 0.8229638037542372, 'learning_rate': 0.00013812314255731833, 'batch_size': 31, 'step_size': 13, 'gamma': 0.9616212161119918}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 09:57:52,632][0m Trial 25 finished with value: 0.06261999234328607 and parameters: {'observation_period_num': 35, 'train_rates': 0.7766046496144594, 'learning_rate': 0.00042830928197059903, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9345095101728045}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 09:58:55,539][0m Trial 26 finished with value: 0.03468676804247515 and parameters: {'observation_period_num': 5, 'train_rates': 0.8788385953841996, 'learning_rate': 5.4507728850431606e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9305907967412061}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 09:59:32,889][0m Trial 27 finished with value: 0.05964273108111229 and parameters: {'observation_period_num': 63, 'train_rates': 0.7342683126449139, 'learning_rate': 0.0002159414943137829, 'batch_size': 135, 'step_size': 15, 'gamma': 0.8897802305926532}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:00:17,723][0m Trial 28 finished with value: 0.09766493104065635 and parameters: {'observation_period_num': 100, 'train_rates': 0.6859435148639307, 'learning_rate': 0.0005004129072770133, 'batch_size': 103, 'step_size': 12, 'gamma': 0.9753462773957416}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:01:32,186][0m Trial 29 finished with value: 0.16563393476697588 and parameters: {'observation_period_num': 240, 'train_rates': 0.8214102295112042, 'learning_rate': 2.475163514560015e-05, 'batch_size': 67, 'step_size': 10, 'gamma': 0.9609925927173882}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:02:13,346][0m Trial 30 finished with value: 0.07046870301592159 and parameters: {'observation_period_num': 114, 'train_rates': 0.9343492776629136, 'learning_rate': 0.00014129740067581, 'batch_size': 146, 'step_size': 14, 'gamma': 0.8836146754394439}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:03:15,778][0m Trial 31 finished with value: 0.0360637862770747 and parameters: {'observation_period_num': 5, 'train_rates': 0.8892951486155389, 'learning_rate': 4.74808102776777e-05, 'batch_size': 91, 'step_size': 14, 'gamma': 0.9276297175987147}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:04:21,697][0m Trial 32 finished with value: 0.05065738291013986 and parameters: {'observation_period_num': 26, 'train_rates': 0.8908837905791986, 'learning_rate': 6.451101700588285e-05, 'batch_size': 86, 'step_size': 13, 'gamma': 0.9286476913968785}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:05:05,936][0m Trial 33 finished with value: 0.05406704172492027 and parameters: {'observation_period_num': 34, 'train_rates': 0.8460538639556833, 'learning_rate': 0.00010915345985055443, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9474292595370846}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:05:29,542][0m Trial 34 finished with value: 0.09164127369941269 and parameters: {'observation_period_num': 5, 'train_rates': 0.815917473682527, 'learning_rate': 1.4639569529334824e-05, 'batch_size': 250, 'step_size': 15, 'gamma': 0.9225713586709359}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:07:39,183][0m Trial 35 finished with value: 0.10969984530921284 and parameters: {'observation_period_num': 64, 'train_rates': 0.8660463530004197, 'learning_rate': 0.00027368570892013057, 'batch_size': 41, 'step_size': 14, 'gamma': 0.9712809488948433}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:08:24,338][0m Trial 36 finished with value: 0.03551016164399465 and parameters: {'observation_period_num': 19, 'train_rates': 0.7794382650551228, 'learning_rate': 0.00013886573956609026, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8988963630979689}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:09:23,265][0m Trial 37 finished with value: 0.061727381531107295 and parameters: {'observation_period_num': 48, 'train_rates': 0.9389611300030523, 'learning_rate': 4.292183423320894e-05, 'batch_size': 98, 'step_size': 7, 'gamma': 0.9410008846210101}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:10:39,403][0m Trial 38 finished with value: 0.2466690386645496 and parameters: {'observation_period_num': 75, 'train_rates': 0.9110674042289708, 'learning_rate': 0.0006171547480974401, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9849751673367814}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:12:27,929][0m Trial 39 finished with value: 0.02947847958070686 and parameters: {'observation_period_num': 19, 'train_rates': 0.8304038706186643, 'learning_rate': 8.05003571298891e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.856208902586462}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:13:56,747][0m Trial 40 finished with value: 0.10906229765094512 and parameters: {'observation_period_num': 162, 'train_rates': 0.7551642481766787, 'learning_rate': 9.397318043672555e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.8218198141019483}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:16:51,806][0m Trial 41 finished with value: 0.032922472425563606 and parameters: {'observation_period_num': 18, 'train_rates': 0.8293733383099866, 'learning_rate': 6.0034135693128386e-05, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8580218502748652}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:19:11,408][0m Trial 42 finished with value: 0.033333994517314065 and parameters: {'observation_period_num': 19, 'train_rates': 0.834759363374814, 'learning_rate': 0.00017035840363930065, 'batch_size': 38, 'step_size': 15, 'gamma': 0.8552354357689004}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:22:47,383][0m Trial 43 finished with value: 0.12854429266669534 and parameters: {'observation_period_num': 38, 'train_rates': 0.8301941621043939, 'learning_rate': 2.4418781477033934e-06, 'batch_size': 24, 'step_size': 15, 'gamma': 0.8503660318950693}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:25:01,312][0m Trial 44 finished with value: 0.03592376938205006 and parameters: {'observation_period_num': 20, 'train_rates': 0.7906281361434093, 'learning_rate': 0.00011224725143765787, 'batch_size': 38, 'step_size': 15, 'gamma': 0.7927380149313931}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:29:40,925][0m Trial 45 finished with value: 0.04873334108606765 and parameters: {'observation_period_num': 47, 'train_rates': 0.8039486652717953, 'learning_rate': 0.00016090449733031218, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8356154021307999}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:31:20,488][0m Trial 46 finished with value: 0.03650395968091874 and parameters: {'observation_period_num': 19, 'train_rates': 0.8317952351738189, 'learning_rate': 3.566283279855244e-05, 'batch_size': 54, 'step_size': 15, 'gamma': 0.8588375147694388}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:33:40,148][0m Trial 47 finished with value: 0.0419382908145519 and parameters: {'observation_period_num': 33, 'train_rates': 0.7485853579168531, 'learning_rate': 7.310026089802911e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8717595502188599}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:35:35,567][0m Trial 48 finished with value: 0.05562437163220758 and parameters: {'observation_period_num': 57, 'train_rates': 0.8512806651747161, 'learning_rate': 0.0002826569063834791, 'batch_size': 46, 'step_size': 3, 'gamma': 0.824493769124628}. Best is trial 21 with value: 0.026341457959347975.[0m
[32m[I 2025-01-04 10:38:33,440][0m Trial 49 finished with value: 0.0443872346178345 and parameters: {'observation_period_num': 17, 'train_rates': 0.7808350512773249, 'learning_rate': 1.4876493990175593e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8625868366568752}. Best is trial 21 with value: 0.026341457959347975.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 10:38:33,450][0m A new study created in memory with name: no-name-a87e72ed-aec5-4979-b9f2-afd479dbbca6[0m
[32m[I 2025-01-04 10:39:03,556][0m Trial 0 finished with value: 0.11992596012764964 and parameters: {'observation_period_num': 205, 'train_rates': 0.7888707408402316, 'learning_rate': 0.00019829304426538863, 'batch_size': 178, 'step_size': 7, 'gamma': 0.9148493726171714}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:39:30,331][0m Trial 1 finished with value: 0.6948986053466797 and parameters: {'observation_period_num': 151, 'train_rates': 0.9764440900003657, 'learning_rate': 2.5395300657794754e-06, 'batch_size': 247, 'step_size': 13, 'gamma': 0.8303010631501284}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:40:08,361][0m Trial 2 finished with value: 0.1777182740099049 and parameters: {'observation_period_num': 235, 'train_rates': 0.7216114442302216, 'learning_rate': 0.00021243042926341978, 'batch_size': 123, 'step_size': 5, 'gamma': 0.7610107712422662}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:40:37,357][0m Trial 3 finished with value: 0.18029499023039805 and parameters: {'observation_period_num': 239, 'train_rates': 0.9114020031761436, 'learning_rate': 0.0002776872682151509, 'batch_size': 205, 'step_size': 7, 'gamma': 0.9468941071106179}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:41:04,233][0m Trial 4 finished with value: 1.0535557270050049 and parameters: {'observation_period_num': 158, 'train_rates': 0.9242142367957054, 'learning_rate': 1.7915211264166867e-06, 'batch_size': 221, 'step_size': 2, 'gamma': 0.9009779371412397}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:41:34,200][0m Trial 5 finished with value: 0.12960428849001912 and parameters: {'observation_period_num': 90, 'train_rates': 0.7757233385728634, 'learning_rate': 3.664934989448321e-05, 'batch_size': 180, 'step_size': 8, 'gamma': 0.9116288369136306}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:42:27,134][0m Trial 6 finished with value: 0.9269550380724815 and parameters: {'observation_period_num': 237, 'train_rates': 0.6106641324667681, 'learning_rate': 1.053029149106252e-06, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8580618788756422}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:43:04,231][0m Trial 7 finished with value: 0.23961048138638338 and parameters: {'observation_period_num': 136, 'train_rates': 0.76167673534454, 'learning_rate': 7.641683278813684e-05, 'batch_size': 140, 'step_size': 3, 'gamma': 0.8107686686823871}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:44:02,340][0m Trial 8 finished with value: 1.025662101932643 and parameters: {'observation_period_num': 108, 'train_rates': 0.6917252106949997, 'learning_rate': 1.2546179430338322e-06, 'batch_size': 79, 'step_size': 3, 'gamma': 0.8102417064040652}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:44:43,785][0m Trial 9 finished with value: 0.19280327845502782 and parameters: {'observation_period_num': 228, 'train_rates': 0.6038690440676059, 'learning_rate': 0.0006048022844530421, 'batch_size': 98, 'step_size': 11, 'gamma': 0.9032879073607684}. Best is trial 0 with value: 0.11992596012764964.[0m
[32m[I 2025-01-04 10:48:34,094][0m Trial 10 finished with value: 0.04425783028240463 and parameters: {'observation_period_num': 26, 'train_rates': 0.8555667570013197, 'learning_rate': 1.7744798914240778e-05, 'batch_size': 23, 'step_size': 11, 'gamma': 0.9636100981925405}. Best is trial 10 with value: 0.04425783028240463.[0m
[32m[I 2025-01-04 10:51:33,403][0m Trial 11 finished with value: 0.037187089555357634 and parameters: {'observation_period_num': 10, 'train_rates': 0.8644804405573779, 'learning_rate': 1.2747368416844393e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.989329048533891}. Best is trial 11 with value: 0.037187089555357634.[0m
[32m[I 2025-01-04 10:57:07,815][0m Trial 12 finished with value: 0.04433682475771223 and parameters: {'observation_period_num': 21, 'train_rates': 0.8686636797171825, 'learning_rate': 6.659683935651265e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9879266718784353}. Best is trial 11 with value: 0.037187089555357634.[0m
[32m[I 2025-01-04 11:02:14,662][0m Trial 13 finished with value: 0.03322403734024321 and parameters: {'observation_period_num': 10, 'train_rates': 0.8362874392001258, 'learning_rate': 1.1415281960341609e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9869570818332364}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:04:03,452][0m Trial 14 finished with value: 0.06283822363790344 and parameters: {'observation_period_num': 62, 'train_rates': 0.8532394336233459, 'learning_rate': 1.0420308040708435e-05, 'batch_size': 49, 'step_size': 11, 'gamma': 0.9832292898651002}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:05:49,018][0m Trial 15 finished with value: 0.06722596828910438 and parameters: {'observation_period_num': 5, 'train_rates': 0.813566449160507, 'learning_rate': 4.795114203841968e-06, 'batch_size': 49, 'step_size': 13, 'gamma': 0.946536562833102}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:07:54,752][0m Trial 16 finished with value: 0.06812680087983608 and parameters: {'observation_period_num': 57, 'train_rates': 0.9657956569853936, 'learning_rate': 3.0207731209992715e-05, 'batch_size': 46, 'step_size': 10, 'gamma': 0.941409473773098}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:08:45,622][0m Trial 17 finished with value: 0.09515943135198299 and parameters: {'observation_period_num': 53, 'train_rates': 0.9038148094277394, 'learning_rate': 5.684641285863834e-05, 'batch_size': 118, 'step_size': 15, 'gamma': 0.8681509848577523}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:10:03,381][0m Trial 18 finished with value: 0.24695520237844776 and parameters: {'observation_period_num': 85, 'train_rates': 0.8422376890418237, 'learning_rate': 3.3870016347100577e-06, 'batch_size': 68, 'step_size': 13, 'gamma': 0.9695358569799676}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:14:48,031][0m Trial 19 finished with value: 0.05949662977390075 and parameters: {'observation_period_num': 35, 'train_rates': 0.6950846505070282, 'learning_rate': 1.410775634587191e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9272573209208721}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:15:43,396][0m Trial 20 finished with value: 0.07275655284635167 and parameters: {'observation_period_num': 10, 'train_rates': 0.817971029132764, 'learning_rate': 9.271552176841777e-06, 'batch_size': 98, 'step_size': 9, 'gamma': 0.965533441446534}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:18:56,226][0m Trial 21 finished with value: 0.04346359676283074 and parameters: {'observation_period_num': 34, 'train_rates': 0.879236075960633, 'learning_rate': 2.1391250963612077e-05, 'batch_size': 28, 'step_size': 12, 'gamma': 0.9601222166106003}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:21:10,610][0m Trial 22 finished with value: 0.07154351917819844 and parameters: {'observation_period_num': 38, 'train_rates': 0.8891496751427161, 'learning_rate': 2.1152665918257934e-05, 'batch_size': 41, 'step_size': 12, 'gamma': 0.988781670870995}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:23:41,316][0m Trial 23 finished with value: 0.05972032972744533 and parameters: {'observation_period_num': 77, 'train_rates': 0.9391754949159828, 'learning_rate': 4.384288194214635e-05, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9625883426413054}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:24:57,419][0m Trial 24 finished with value: 0.16407907284146186 and parameters: {'observation_period_num': 40, 'train_rates': 0.7442305741855818, 'learning_rate': 5.86630398444348e-06, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8865347922944855}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:27:23,295][0m Trial 25 finished with value: 0.03359377777881243 and parameters: {'observation_period_num': 10, 'train_rates': 0.8206592106316533, 'learning_rate': 2.267688105451413e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9354603965110384}. Best is trial 13 with value: 0.03322403734024321.[0m
[32m[I 2025-01-04 11:28:20,833][0m Trial 26 finished with value: 0.03242326034676461 and parameters: {'observation_period_num': 9, 'train_rates': 0.8217143119302553, 'learning_rate': 0.00013088537035220134, 'batch_size': 95, 'step_size': 6, 'gamma': 0.9328052040051201}. Best is trial 26 with value: 0.03242326034676461.[0m
[32m[I 2025-01-04 11:29:13,810][0m Trial 27 finished with value: 0.062007732200346395 and parameters: {'observation_period_num': 107, 'train_rates': 0.8181155536503681, 'learning_rate': 0.00011877172486085265, 'batch_size': 99, 'step_size': 5, 'gamma': 0.9361822426491352}. Best is trial 26 with value: 0.03242326034676461.[0m
[32m[I 2025-01-04 11:29:49,790][0m Trial 28 finished with value: 0.13461754834929177 and parameters: {'observation_period_num': 185, 'train_rates': 0.8290930037260291, 'learning_rate': 9.148647460436599e-05, 'batch_size': 144, 'step_size': 5, 'gamma': 0.8894644402585056}. Best is trial 26 with value: 0.03242326034676461.[0m
[32m[I 2025-01-04 11:31:15,332][0m Trial 29 finished with value: 0.05565242783416182 and parameters: {'observation_period_num': 68, 'train_rates': 0.7946904760740625, 'learning_rate': 0.00028487892362393246, 'batch_size': 60, 'step_size': 6, 'gamma': 0.9186136270614731}. Best is trial 26 with value: 0.03242326034676461.[0m
[32m[I 2025-01-04 11:31:49,329][0m Trial 30 finished with value: 0.036381963421845166 and parameters: {'observation_period_num': 20, 'train_rates': 0.7880052603150753, 'learning_rate': 0.0005337545175482075, 'batch_size': 159, 'step_size': 7, 'gamma': 0.8497943152754425}. Best is trial 26 with value: 0.03242326034676461.[0m
[32m[I 2025-01-04 11:32:23,561][0m Trial 31 finished with value: 0.031149320699126515 and parameters: {'observation_period_num': 16, 'train_rates': 0.7943771019220662, 'learning_rate': 0.0007014126058589618, 'batch_size': 160, 'step_size': 7, 'gamma': 0.836981947187389}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:32:53,321][0m Trial 32 finished with value: 0.04726110621958577 and parameters: {'observation_period_num': 48, 'train_rates': 0.765664922641082, 'learning_rate': 0.0007772835424721973, 'batch_size': 179, 'step_size': 9, 'gamma': 0.8209407685146564}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:33:26,386][0m Trial 33 finished with value: 0.05888820462006861 and parameters: {'observation_period_num': 21, 'train_rates': 0.7393525005880187, 'learning_rate': 0.0001451881768163475, 'batch_size': 158, 'step_size': 9, 'gamma': 0.7850855169864912}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:34:13,506][0m Trial 34 finished with value: 0.03393243490117975 and parameters: {'observation_period_num': 9, 'train_rates': 0.804979421486193, 'learning_rate': 0.0003822885592608789, 'batch_size': 117, 'step_size': 6, 'gamma': 0.8437425448249327}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:34:41,626][0m Trial 35 finished with value: 0.042322668508797824 and parameters: {'observation_period_num': 47, 'train_rates': 0.8374447936585739, 'learning_rate': 0.0009185640375926458, 'batch_size': 204, 'step_size': 6, 'gamma': 0.8868614587312827}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:35:36,494][0m Trial 36 finished with value: 0.11556605213111447 and parameters: {'observation_period_num': 171, 'train_rates': 0.7214961898735017, 'learning_rate': 0.00020258935901304827, 'batch_size': 87, 'step_size': 8, 'gamma': 0.8371696380615327}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:36:00,225][0m Trial 37 finished with value: 0.21646664834022522 and parameters: {'observation_period_num': 28, 'train_rates': 0.7865496030705728, 'learning_rate': 5.311274175699095e-05, 'batch_size': 238, 'step_size': 4, 'gamma': 0.7864159987580518}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:36:28,299][0m Trial 38 finished with value: 0.07066688589427782 and parameters: {'observation_period_num': 72, 'train_rates': 0.75251381193142, 'learning_rate': 0.000355946877780844, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8730167295387343}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:37:07,424][0m Trial 39 finished with value: 0.5102986407455923 and parameters: {'observation_period_num': 112, 'train_rates': 0.7140762895861582, 'learning_rate': 2.8243624488861895e-05, 'batch_size': 130, 'step_size': 1, 'gamma': 0.9248516980473364}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:37:43,076][0m Trial 40 finished with value: 0.13960956523292942 and parameters: {'observation_period_num': 210, 'train_rates': 0.8963710402068331, 'learning_rate': 8.510619173790883e-05, 'batch_size': 154, 'step_size': 8, 'gamma': 0.9111828959516006}. Best is trial 31 with value: 0.031149320699126515.[0m
[32m[I 2025-01-04 11:38:30,788][0m Trial 41 finished with value: 0.03005516766908878 and parameters: {'observation_period_num': 8, 'train_rates': 0.8038893514060613, 'learning_rate': 0.0005217407621398729, 'batch_size': 117, 'step_size': 6, 'gamma': 0.8402733809863824}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:39:18,943][0m Trial 42 finished with value: 0.036652705376106806 and parameters: {'observation_period_num': 6, 'train_rates': 0.7753797401577905, 'learning_rate': 0.0005075599019583878, 'batch_size': 110, 'step_size': 4, 'gamma': 0.8250584775520448}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:39:53,366][0m Trial 43 finished with value: 0.04658894758125779 and parameters: {'observation_period_num': 19, 'train_rates': 0.8396403319373896, 'learning_rate': 0.000170479738768527, 'batch_size': 171, 'step_size': 6, 'gamma': 0.8613438692457237}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:40:29,497][0m Trial 44 finished with value: 0.05225396567377551 and parameters: {'observation_period_num': 18, 'train_rates': 0.6443694542826982, 'learning_rate': 0.00028055634345308016, 'batch_size': 132, 'step_size': 7, 'gamma': 0.799556104550173}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:41:33,688][0m Trial 45 finished with value: 0.04171649478440363 and parameters: {'observation_period_num': 32, 'train_rates': 0.8023943636389164, 'learning_rate': 0.0006153429866340761, 'batch_size': 83, 'step_size': 10, 'gamma': 0.9750648336457625}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:42:15,077][0m Trial 46 finished with value: 0.04518853749380362 and parameters: {'observation_period_num': 46, 'train_rates': 0.9214357454543232, 'learning_rate': 0.0009517879847848731, 'batch_size': 144, 'step_size': 5, 'gamma': 0.9500654079727664}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:43:47,371][0m Trial 47 finished with value: 0.03334388715531631 and parameters: {'observation_period_num': 15, 'train_rates': 0.8628461213488012, 'learning_rate': 0.00043077381175292954, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9003937579203065}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:44:38,016][0m Trial 48 finished with value: 0.0676797478335775 and parameters: {'observation_period_num': 143, 'train_rates': 0.8744880966739563, 'learning_rate': 0.00041195741617787646, 'batch_size': 108, 'step_size': 4, 'gamma': 0.8977289968837807}. Best is trial 41 with value: 0.03005516766908878.[0m
[32m[I 2025-01-04 11:45:47,666][0m Trial 49 finished with value: 0.20567055379030524 and parameters: {'observation_period_num': 252, 'train_rates': 0.8625439194840915, 'learning_rate': 0.0007150584018905294, 'batch_size': 74, 'step_size': 8, 'gamma': 0.8744740010112797}. Best is trial 41 with value: 0.03005516766908878.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 11:45:47,676][0m A new study created in memory with name: no-name-429741a3-6491-4770-a4c3-770af3cf914b[0m
[32m[I 2025-01-04 11:46:09,722][0m Trial 0 finished with value: 0.24636822417478701 and parameters: {'observation_period_num': 244, 'train_rates': 0.7257922229959438, 'learning_rate': 7.611667165908556e-05, 'batch_size': 234, 'step_size': 14, 'gamma': 0.9567259633487688}. Best is trial 0 with value: 0.24636822417478701.[0m
[32m[I 2025-01-04 11:46:34,161][0m Trial 1 finished with value: 0.6132602066412602 and parameters: {'observation_period_num': 176, 'train_rates': 0.6837491319017215, 'learning_rate': 5.618152742310589e-06, 'batch_size': 190, 'step_size': 6, 'gamma': 0.8409668877582566}. Best is trial 0 with value: 0.24636822417478701.[0m
[32m[I 2025-01-04 11:47:09,850][0m Trial 2 finished with value: 0.8595010638237 and parameters: {'observation_period_num': 198, 'train_rates': 0.9507055574137269, 'learning_rate': 2.4600409107308036e-06, 'batch_size': 171, 'step_size': 9, 'gamma': 0.794182686158409}. Best is trial 0 with value: 0.24636822417478701.[0m
[32m[I 2025-01-04 11:48:24,255][0m Trial 3 finished with value: 0.028134089345942335 and parameters: {'observation_period_num': 8, 'train_rates': 0.8630100731728187, 'learning_rate': 9.81393986454679e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.829244247686254}. Best is trial 3 with value: 0.028134089345942335.[0m
[32m[I 2025-01-04 11:49:11,714][0m Trial 4 finished with value: 0.3774447219116698 and parameters: {'observation_period_num': 65, 'train_rates': 0.6635902958227957, 'learning_rate': 5.543136251985982e-06, 'batch_size': 98, 'step_size': 9, 'gamma': 0.9278672127192108}. Best is trial 3 with value: 0.028134089345942335.[0m
[32m[I 2025-01-04 11:53:37,236][0m Trial 5 finished with value: 0.09292674407467141 and parameters: {'observation_period_num': 73, 'train_rates': 0.751657778597534, 'learning_rate': 8.82171182926609e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7841802224967889}. Best is trial 3 with value: 0.028134089345942335.[0m
[32m[I 2025-01-04 11:54:33,414][0m Trial 6 finished with value: 0.1367003380152203 and parameters: {'observation_period_num': 189, 'train_rates': 0.6764858216981795, 'learning_rate': 9.456586812778114e-05, 'batch_size': 78, 'step_size': 14, 'gamma': 0.8242310552097981}. Best is trial 3 with value: 0.028134089345942335.[0m
Early stopping at epoch 61
[32m[I 2025-01-04 11:54:53,852][0m Trial 7 finished with value: 0.33181731344208004 and parameters: {'observation_period_num': 74, 'train_rates': 0.7796993114590502, 'learning_rate': 8.936928581897177e-05, 'batch_size': 169, 'step_size': 1, 'gamma': 0.8097037632145786}. Best is trial 3 with value: 0.028134089345942335.[0m
[32m[I 2025-01-04 11:55:32,354][0m Trial 8 finished with value: 0.8563844903570706 and parameters: {'observation_period_num': 133, 'train_rates': 0.8920579918385401, 'learning_rate': 1.3272100139646661e-06, 'batch_size': 153, 'step_size': 12, 'gamma': 0.9723841597121904}. Best is trial 3 with value: 0.028134089345942335.[0m
[32m[I 2025-01-04 11:57:01,124][0m Trial 9 finished with value: 0.2161469297564548 and parameters: {'observation_period_num': 164, 'train_rates': 0.901274209301051, 'learning_rate': 9.298347074669489e-06, 'batch_size': 60, 'step_size': 6, 'gamma': 0.884513778055386}. Best is trial 3 with value: 0.028134089345942335.[0m
Early stopping at epoch 48
[32m[I 2025-01-04 11:57:26,180][0m Trial 10 finished with value: 0.05718327046446659 and parameters: {'observation_period_num': 12, 'train_rates': 0.83901810531671, 'learning_rate': 0.0007983447304083719, 'batch_size': 116, 'step_size': 1, 'gamma': 0.7504049186277739}. Best is trial 3 with value: 0.028134089345942335.[0m
Early stopping at epoch 49
[32m[I 2025-01-04 11:57:51,104][0m Trial 11 finished with value: 0.05917921119294818 and parameters: {'observation_period_num': 8, 'train_rates': 0.8435888631534447, 'learning_rate': 0.0008658716777443917, 'batch_size': 116, 'step_size': 1, 'gamma': 0.7515208704062474}. Best is trial 3 with value: 0.028134089345942335.[0m
[32m[I 2025-01-04 11:59:58,769][0m Trial 12 finished with value: 0.022889092429838283 and parameters: {'observation_period_num': 5, 'train_rates': 0.8405577022438272, 'learning_rate': 0.0007176526017094414, 'batch_size': 42, 'step_size': 4, 'gamma': 0.8829341221158721}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:02:57,840][0m Trial 13 finished with value: 0.03484310293249015 and parameters: {'observation_period_num': 40, 'train_rates': 0.9803717586518721, 'learning_rate': 0.0002767587488387078, 'batch_size': 33, 'step_size': 5, 'gamma': 0.8756384612049857}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:04:31,235][0m Trial 14 finished with value: 0.0766125252303859 and parameters: {'observation_period_num': 102, 'train_rates': 0.8314960095564606, 'learning_rate': 0.0002916788059719316, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9129101706979145}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:05:59,776][0m Trial 15 finished with value: 0.2771888571528754 and parameters: {'observation_period_num': 36, 'train_rates': 0.6084680950927265, 'learning_rate': 2.089771183076543e-05, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8550358808584945}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:07:05,678][0m Trial 16 finished with value: 0.06085710690438169 and parameters: {'observation_period_num': 118, 'train_rates': 0.8911469654235098, 'learning_rate': 0.0002506324979491503, 'batch_size': 83, 'step_size': 3, 'gamma': 0.9014523073854999}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:10:56,163][0m Trial 17 finished with value: 0.04284609542687384 and parameters: {'observation_period_num': 45, 'train_rates': 0.8047765088592133, 'learning_rate': 4.7757742941228554e-05, 'batch_size': 22, 'step_size': 8, 'gamma': 0.8446623205024004}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:11:45,738][0m Trial 18 finished with value: 0.04073361380462441 and parameters: {'observation_period_num': 8, 'train_rates': 0.9293197358474885, 'learning_rate': 0.0004338660685707761, 'batch_size': 120, 'step_size': 10, 'gamma': 0.9396280710458479}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:12:55,097][0m Trial 19 finished with value: 0.05155425135101845 and parameters: {'observation_period_num': 96, 'train_rates': 0.859537273305774, 'learning_rate': 0.0001637378543668924, 'batch_size': 77, 'step_size': 7, 'gamma': 0.8902753935820168}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:14:49,183][0m Trial 20 finished with value: 0.050195345453249045 and parameters: {'observation_period_num': 32, 'train_rates': 0.7789166105583162, 'learning_rate': 2.8095473597287578e-05, 'batch_size': 44, 'step_size': 12, 'gamma': 0.8605238687549193}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:17:46,420][0m Trial 21 finished with value: 0.036246796717514865 and parameters: {'observation_period_num': 45, 'train_rates': 0.9746172166244885, 'learning_rate': 0.0004770364516202505, 'batch_size': 33, 'step_size': 4, 'gamma': 0.885681575110187}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:23:54,209][0m Trial 22 finished with value: 0.032872597500681874 and parameters: {'observation_period_num': 29, 'train_rates': 0.986510011761504, 'learning_rate': 0.0001592708390716228, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8676759036341523}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:25:23,154][0m Trial 23 finished with value: 0.05298513568499509 and parameters: {'observation_period_num': 20, 'train_rates': 0.9248146429714739, 'learning_rate': 0.0001565827989231573, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8277970475485483}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:30:35,303][0m Trial 24 finished with value: 0.04046951431445975 and parameters: {'observation_period_num': 64, 'train_rates': 0.8692525744159693, 'learning_rate': 4.8417107860104184e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.864089907780945}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:31:33,963][0m Trial 25 finished with value: 0.03426678501041641 and parameters: {'observation_period_num': 30, 'train_rates': 0.8174634299706686, 'learning_rate': 0.0005480123039225546, 'batch_size': 92, 'step_size': 3, 'gamma': 0.9163145310600642}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:33:49,451][0m Trial 26 finished with value: 0.06448128167539835 and parameters: {'observation_period_num': 94, 'train_rates': 0.9415150090330839, 'learning_rate': 0.00016544726463117926, 'batch_size': 42, 'step_size': 5, 'gamma': 0.8135571690238348}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:34:12,125][0m Trial 27 finished with value: 0.03560714991276638 and parameters: {'observation_period_num': 5, 'train_rates': 0.7207681958848052, 'learning_rate': 0.0009598479479849544, 'batch_size': 239, 'step_size': 8, 'gamma': 0.8389771144172465}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:35:39,320][0m Trial 28 finished with value: 0.05234249805410703 and parameters: {'observation_period_num': 52, 'train_rates': 0.8697543188541385, 'learning_rate': 5.323138049424912e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.7750034892517066}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:36:03,291][0m Trial 29 finished with value: 0.21699020344617587 and parameters: {'observation_period_num': 250, 'train_rates': 0.745983186191273, 'learning_rate': 0.00011439580858801033, 'batch_size': 204, 'step_size': 2, 'gamma': 0.9375569665983325}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:36:30,345][0m Trial 30 finished with value: 0.22610177099704742 and parameters: {'observation_period_num': 23, 'train_rates': 0.9546905411754351, 'learning_rate': 2.3043505167273443e-05, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9029023230075027}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:37:25,660][0m Trial 31 finished with value: 0.033394057979787374 and parameters: {'observation_period_num': 26, 'train_rates': 0.8081210979490523, 'learning_rate': 0.000507899325462532, 'batch_size': 97, 'step_size': 3, 'gamma': 0.9240717069680062}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:38:16,681][0m Trial 32 finished with value: 0.053873754696246924 and parameters: {'observation_period_num': 56, 'train_rates': 0.7969171941532521, 'learning_rate': 0.0003283667624681154, 'batch_size': 102, 'step_size': 4, 'gamma': 0.9899400307404247}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:38:56,009][0m Trial 33 finished with value: 0.17293652184620625 and parameters: {'observation_period_num': 219, 'train_rates': 0.9129516244354898, 'learning_rate': 0.000595292988950098, 'batch_size': 139, 'step_size': 2, 'gamma': 0.9599606190713494}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:40:08,548][0m Trial 34 finished with value: 0.03733769323055943 and parameters: {'observation_period_num': 22, 'train_rates': 0.72163935857287, 'learning_rate': 0.00020390920879969452, 'batch_size': 68, 'step_size': 7, 'gamma': 0.8738931462878421}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:42:42,610][0m Trial 35 finished with value: 0.08570797939495017 and parameters: {'observation_period_num': 81, 'train_rates': 0.7694657823487123, 'learning_rate': 0.0003954233307855609, 'batch_size': 32, 'step_size': 6, 'gamma': 0.8548423371410703}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:43:40,359][0m Trial 36 finished with value: 0.043417213559722716 and parameters: {'observation_period_num': 25, 'train_rates': 0.8220130068118339, 'learning_rate': 0.00012242037980999542, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9276592430192205}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:44:40,814][0m Trial 37 finished with value: 0.09460546066723416 and parameters: {'observation_period_num': 133, 'train_rates': 0.6966755831361989, 'learning_rate': 7.041693729774317e-05, 'batch_size': 78, 'step_size': 13, 'gamma': 0.7958897537332448}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:45:23,146][0m Trial 38 finished with value: 0.06266092422107855 and parameters: {'observation_period_num': 63, 'train_rates': 0.8547251751398564, 'learning_rate': 0.000558405480298842, 'batch_size': 133, 'step_size': 2, 'gamma': 0.8305710722791613}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:46:58,536][0m Trial 39 finished with value: 0.10922217690434026 and parameters: {'observation_period_num': 166, 'train_rates': 0.7571762150790214, 'learning_rate': 0.0002213990863911039, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8943913195161068}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:50:09,092][0m Trial 40 finished with value: 0.09194256973559739 and parameters: {'observation_period_num': 228, 'train_rates': 0.888361078918075, 'learning_rate': 7.08063407234915e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.8458724440032284}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:51:10,741][0m Trial 41 finished with value: 0.03394350992827229 and parameters: {'observation_period_num': 25, 'train_rates': 0.8082432419290653, 'learning_rate': 0.0006382148639292893, 'batch_size': 87, 'step_size': 3, 'gamma': 0.9145319933996165}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:52:04,226][0m Trial 42 finished with value: 0.029706809901357316 and parameters: {'observation_period_num': 17, 'train_rates': 0.8030118798244715, 'learning_rate': 0.0008051972364271929, 'batch_size': 102, 'step_size': 4, 'gamma': 0.9148329033920856}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:52:56,781][0m Trial 43 finished with value: 0.028073358471496418 and parameters: {'observation_period_num': 14, 'train_rates': 0.7871037464229615, 'learning_rate': 0.0009575162785002221, 'batch_size': 102, 'step_size': 4, 'gamma': 0.9408522110787146}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:53:46,822][0m Trial 44 finished with value: 0.027155574029684066 and parameters: {'observation_period_num': 13, 'train_rates': 0.7874726666767394, 'learning_rate': 0.0009755904906554702, 'batch_size': 108, 'step_size': 5, 'gamma': 0.9465480281780178}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:54:36,826][0m Trial 45 finished with value: 0.029111967591399498 and parameters: {'observation_period_num': 14, 'train_rates': 0.7831648621919697, 'learning_rate': 0.0009534527670781295, 'batch_size': 109, 'step_size': 4, 'gamma': 0.9466676361038717}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:55:19,270][0m Trial 46 finished with value: 0.06017798535964068 and parameters: {'observation_period_num': 9, 'train_rates': 0.7402637850425856, 'learning_rate': 0.0009991831977068117, 'batch_size': 126, 'step_size': 5, 'gamma': 0.9527025028227096}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:55:56,460][0m Trial 47 finished with value: 0.0598068959883546 and parameters: {'observation_period_num': 42, 'train_rates': 0.7793213603371572, 'learning_rate': 0.0006872472523745542, 'batch_size': 146, 'step_size': 10, 'gamma': 0.9661011126435802}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:56:30,249][0m Trial 48 finished with value: 0.2079118681140244 and parameters: {'observation_period_num': 79, 'train_rates': 0.8331092903531416, 'learning_rate': 1.3643250654737817e-05, 'batch_size': 165, 'step_size': 4, 'gamma': 0.9458831240603492}. Best is trial 12 with value: 0.022889092429838283.[0m
[32m[I 2025-01-04 12:57:16,684][0m Trial 49 finished with value: 0.029058828686872835 and parameters: {'observation_period_num': 13, 'train_rates': 0.7616411738845668, 'learning_rate': 0.0003594304195088543, 'batch_size': 113, 'step_size': 6, 'gamma': 0.9871608598765316}. Best is trial 12 with value: 0.022889092429838283.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 12:57:16,694][0m A new study created in memory with name: no-name-ed3f03c0-cc50-43d0-be5d-ebc2bd4aec30[0m
[32m[I 2025-01-04 12:57:39,804][0m Trial 0 finished with value: 0.386814357987208 and parameters: {'observation_period_num': 227, 'train_rates': 0.6215037553427406, 'learning_rate': 0.00019405558948171948, 'batch_size': 191, 'step_size': 10, 'gamma': 0.8726483604197905}. Best is trial 0 with value: 0.386814357987208.[0m
[32m[I 2025-01-04 12:58:08,003][0m Trial 1 finished with value: 0.7484387522069816 and parameters: {'observation_period_num': 228, 'train_rates': 0.7852507447155769, 'learning_rate': 2.4251735767583303e-06, 'batch_size': 184, 'step_size': 6, 'gamma': 0.8954503694153338}. Best is trial 0 with value: 0.386814357987208.[0m
[32m[I 2025-01-04 12:59:56,702][0m Trial 2 finished with value: 0.0722365408081491 and parameters: {'observation_period_num': 95, 'train_rates': 0.8146245680725165, 'learning_rate': 1.916009394823897e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8526447014799035}. Best is trial 2 with value: 0.0722365408081491.[0m
[32m[I 2025-01-04 13:00:24,495][0m Trial 3 finished with value: 0.18633675384115797 and parameters: {'observation_period_num': 240, 'train_rates': 0.7888092956198317, 'learning_rate': 0.00021780294207902532, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8833770241634585}. Best is trial 2 with value: 0.0722365408081491.[0m
[32m[I 2025-01-04 13:00:48,318][0m Trial 4 finished with value: 0.4540070692206373 and parameters: {'observation_period_num': 156, 'train_rates': 0.783300991153824, 'learning_rate': 8.42756587000778e-06, 'batch_size': 231, 'step_size': 4, 'gamma': 0.929582270877743}. Best is trial 2 with value: 0.0722365408081491.[0m
[32m[I 2025-01-04 13:01:25,310][0m Trial 5 finished with value: 0.4692113896210988 and parameters: {'observation_period_num': 224, 'train_rates': 0.6174686947317666, 'learning_rate': 1.277777092550108e-05, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9600837683095413}. Best is trial 2 with value: 0.0722365408081491.[0m
[32m[I 2025-01-04 13:06:37,069][0m Trial 6 finished with value: 0.10617499636151852 and parameters: {'observation_period_num': 234, 'train_rates': 0.9427298498968147, 'learning_rate': 1.2749391248157852e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9511536758959975}. Best is trial 2 with value: 0.0722365408081491.[0m
[32m[I 2025-01-04 13:07:01,300][0m Trial 7 finished with value: 0.69937417584624 and parameters: {'observation_period_num': 63, 'train_rates': 0.7549706397649898, 'learning_rate': 4.186574284654436e-06, 'batch_size': 230, 'step_size': 14, 'gamma': 0.8230667208544274}. Best is trial 2 with value: 0.0722365408081491.[0m
[32m[I 2025-01-04 13:07:28,276][0m Trial 8 finished with value: 0.046966660767793655 and parameters: {'observation_period_num': 34, 'train_rates': 0.9270897767885665, 'learning_rate': 0.0003605100651428675, 'batch_size': 241, 'step_size': 8, 'gamma': 0.7857241705843504}. Best is trial 8 with value: 0.046966660767793655.[0m
[32m[I 2025-01-04 13:08:58,303][0m Trial 9 finished with value: 0.28424938834795804 and parameters: {'observation_period_num': 187, 'train_rates': 0.7293122790121462, 'learning_rate': 0.0006241956100144167, 'batch_size': 51, 'step_size': 3, 'gamma': 0.946018534511266}. Best is trial 8 with value: 0.046966660767793655.[0m
[32m[I 2025-01-04 13:09:49,703][0m Trial 10 finished with value: 0.052986666560173035 and parameters: {'observation_period_num': 22, 'train_rates': 0.9879840875728954, 'learning_rate': 0.00010805498188130763, 'batch_size': 122, 'step_size': 6, 'gamma': 0.7509755302992496}. Best is trial 8 with value: 0.046966660767793655.[0m
Early stopping at epoch 44
[32m[I 2025-01-04 13:10:14,277][0m Trial 11 finished with value: 0.13173410296440125 and parameters: {'observation_period_num': 5, 'train_rates': 0.9769497421684167, 'learning_rate': 8.530638134461344e-05, 'batch_size': 116, 'step_size': 1, 'gamma': 0.7517552638584692}. Best is trial 8 with value: 0.046966660767793655.[0m
[32m[I 2025-01-04 13:10:56,547][0m Trial 12 finished with value: 0.026651102506245177 and parameters: {'observation_period_num': 5, 'train_rates': 0.9026006185269891, 'learning_rate': 0.0009955194884617193, 'batch_size': 140, 'step_size': 7, 'gamma': 0.7509599369041723}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:11:21,842][0m Trial 13 finished with value: 0.04819841108224498 and parameters: {'observation_period_num': 51, 'train_rates': 0.8991239707672373, 'learning_rate': 0.0009669507257979572, 'batch_size': 251, 'step_size': 7, 'gamma': 0.8020763052012259}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:11:57,645][0m Trial 14 finished with value: 0.05880400327379913 and parameters: {'observation_period_num': 103, 'train_rates': 0.8761147006862549, 'learning_rate': 0.0003721055388324965, 'batch_size': 164, 'step_size': 12, 'gamma': 0.7846018464144096}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:12:24,730][0m Trial 15 finished with value: 0.15947901366316541 and parameters: {'observation_period_num': 43, 'train_rates': 0.8655621534344996, 'learning_rate': 5.48637774808362e-05, 'batch_size': 212, 'step_size': 4, 'gamma': 0.7891368262076408}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:13:03,689][0m Trial 16 finished with value: 0.05725991443601424 and parameters: {'observation_period_num': 77, 'train_rates': 0.9225599823458368, 'learning_rate': 0.0004159148455093354, 'batch_size': 151, 'step_size': 9, 'gamma': 0.8283345654251385}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:14:03,424][0m Trial 17 finished with value: 0.07703746558242552 and parameters: {'observation_period_num': 131, 'train_rates': 0.8427233792502892, 'learning_rate': 0.0009430632861425254, 'batch_size': 90, 'step_size': 6, 'gamma': 0.7775322504122301}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:14:56,450][0m Trial 18 finished with value: 0.06187783077999747 and parameters: {'observation_period_num': 26, 'train_rates': 0.6982202651334305, 'learning_rate': 3.5834144961771214e-05, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8189125734915152}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:15:23,354][0m Trial 19 finished with value: 0.05802285298705101 and parameters: {'observation_period_num': 9, 'train_rates': 0.9402900943766426, 'learning_rate': 0.00022857124665866062, 'batch_size': 251, 'step_size': 8, 'gamma': 0.766829085919988}. Best is trial 12 with value: 0.026651102506245177.[0m
Early stopping at epoch 80
[32m[I 2025-01-04 13:15:46,069][0m Trial 20 finished with value: 0.23240426423386756 and parameters: {'observation_period_num': 89, 'train_rates': 0.8315084118044114, 'learning_rate': 0.00043305313321726456, 'batch_size': 216, 'step_size': 1, 'gamma': 0.8453275004133929}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:16:11,839][0m Trial 21 finished with value: 0.04423540755299454 and parameters: {'observation_period_num': 49, 'train_rates': 0.9048241512332491, 'learning_rate': 0.0009536232988601046, 'batch_size': 247, 'step_size': 7, 'gamma': 0.7997764911748311}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:16:43,098][0m Trial 22 finished with value: 1.0860994713013943 and parameters: {'observation_period_num': 40, 'train_rates': 0.907913773718895, 'learning_rate': 1.121454806162919e-06, 'batch_size': 202, 'step_size': 8, 'gamma': 0.8019753951305575}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:17:08,068][0m Trial 23 finished with value: 0.057215239852666855 and parameters: {'observation_period_num': 64, 'train_rates': 0.8795053838822271, 'learning_rate': 0.0005919693650229191, 'batch_size': 231, 'step_size': 5, 'gamma': 0.7658542263062245}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:17:33,043][0m Trial 24 finished with value: 0.04838433116674423 and parameters: {'observation_period_num': 29, 'train_rates': 0.9573896378596417, 'learning_rate': 0.0009853345147629161, 'batch_size': 255, 'step_size': 7, 'gamma': 0.8047278604605279}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:18:09,643][0m Trial 25 finished with value: 0.09914044065279187 and parameters: {'observation_period_num': 128, 'train_rates': 0.8540146404218688, 'learning_rate': 0.0001283993185840214, 'batch_size': 152, 'step_size': 9, 'gamma': 0.7725280798288442}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:19:12,243][0m Trial 26 finished with value: 0.051911277275015835 and parameters: {'observation_period_num': 61, 'train_rates': 0.9054694566539804, 'learning_rate': 0.00029375221506976206, 'batch_size': 92, 'step_size': 3, 'gamma': 0.846268166401783}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:19:47,697][0m Trial 27 finished with value: 0.0624407182926732 and parameters: {'observation_period_num': 117, 'train_rates': 0.9392886772020386, 'learning_rate': 0.0006272353292873479, 'batch_size': 169, 'step_size': 7, 'gamma': 0.8021056133920468}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:20:16,523][0m Trial 28 finished with value: 0.04531865939497948 and parameters: {'observation_period_num': 6, 'train_rates': 0.9664042535900091, 'learning_rate': 0.00015280310423369312, 'batch_size': 219, 'step_size': 9, 'gamma': 0.9117853665018021}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:20:41,699][0m Trial 29 finished with value: 0.04273705571135388 and parameters: {'observation_period_num': 12, 'train_rates': 0.6507262908126291, 'learning_rate': 0.00014909016052437287, 'batch_size': 195, 'step_size': 10, 'gamma': 0.9882046108790096}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:21:06,954][0m Trial 30 finished with value: 0.09114939079538838 and parameters: {'observation_period_num': 79, 'train_rates': 0.6524097268200689, 'learning_rate': 6.12276783132894e-05, 'batch_size': 192, 'step_size': 11, 'gamma': 0.9170347115899559}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:21:31,574][0m Trial 31 finished with value: 0.03970276592704265 and parameters: {'observation_period_num': 6, 'train_rates': 0.6877239437248197, 'learning_rate': 0.00014899842126641692, 'batch_size': 210, 'step_size': 10, 'gamma': 0.9792391944747985}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:21:58,878][0m Trial 32 finished with value: 0.11294696120097196 and parameters: {'observation_period_num': 20, 'train_rates': 0.6569659442408701, 'learning_rate': 3.681809478169355e-05, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9895644791162617}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:22:24,247][0m Trial 33 finished with value: 0.07073128601592599 and parameters: {'observation_period_num': 20, 'train_rates': 0.6626325663001258, 'learning_rate': 0.0005441626581122397, 'batch_size': 197, 'step_size': 12, 'gamma': 0.9849035643531155}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:23:01,229][0m Trial 34 finished with value: 0.10599241197109223 and parameters: {'observation_period_num': 46, 'train_rates': 0.6991801198975484, 'learning_rate': 0.00017605803191949186, 'batch_size': 140, 'step_size': 11, 'gamma': 0.97396951504247}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:23:29,163][0m Trial 35 finished with value: 0.061342270813541475 and parameters: {'observation_period_num': 17, 'train_rates': 0.6350943670738267, 'learning_rate': 0.0002588748999265332, 'batch_size': 175, 'step_size': 15, 'gamma': 0.8883023287158883}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:23:54,231][0m Trial 36 finished with value: 0.07812646534079792 and parameters: {'observation_period_num': 53, 'train_rates': 0.6999530316675128, 'learning_rate': 7.838112899420367e-05, 'batch_size': 206, 'step_size': 13, 'gamma': 0.9709850423078163}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:24:18,130][0m Trial 37 finished with value: 0.2724273915793816 and parameters: {'observation_period_num': 165, 'train_rates': 0.8070795047584933, 'learning_rate': 2.066028773373868e-05, 'batch_size': 225, 'step_size': 10, 'gamma': 0.8645215915884277}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:24:43,751][0m Trial 38 finished with value: 0.40576207571275247 and parameters: {'observation_period_num': 34, 'train_rates': 0.6010928037590813, 'learning_rate': 6.349294244165784e-06, 'batch_size': 183, 'step_size': 5, 'gamma': 0.9367492040748154}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:25:07,752][0m Trial 39 finished with value: 0.07333864305902571 and parameters: {'observation_period_num': 9, 'train_rates': 0.7675051218836378, 'learning_rate': 2.3558736653787853e-05, 'batch_size': 241, 'step_size': 9, 'gamma': 0.9628115967524019}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:25:40,706][0m Trial 40 finished with value: 0.0474606753322319 and parameters: {'observation_period_num': 38, 'train_rates': 0.7241474792893529, 'learning_rate': 0.00021334606246006511, 'batch_size': 158, 'step_size': 10, 'gamma': 0.9056212527256188}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:26:11,612][0m Trial 41 finished with value: 0.05052105337381363 and parameters: {'observation_period_num': 6, 'train_rates': 0.9717984328856715, 'learning_rate': 0.00013204436242063335, 'batch_size': 219, 'step_size': 9, 'gamma': 0.9211867852235534}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:26:33,417][0m Trial 42 finished with value: 0.06614611431371623 and parameters: {'observation_period_num': 18, 'train_rates': 0.6789245221417539, 'learning_rate': 0.00016441318855846992, 'batch_size': 238, 'step_size': 7, 'gamma': 0.8626443502260291}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:26:56,507][0m Trial 43 finished with value: 0.10031632280427646 and parameters: {'observation_period_num': 31, 'train_rates': 0.6333915381854133, 'learning_rate': 5.0027872465307215e-05, 'batch_size': 209, 'step_size': 11, 'gamma': 0.9499032780176346}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:27:20,437][0m Trial 44 finished with value: 0.03969993232926236 and parameters: {'observation_period_num': 5, 'train_rates': 0.732221976417597, 'learning_rate': 0.00029560534522733164, 'batch_size': 219, 'step_size': 6, 'gamma': 0.8994620789438144}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:28:00,654][0m Trial 45 finished with value: 0.040289137737651914 and parameters: {'observation_period_num': 17, 'train_rates': 0.7294802185491365, 'learning_rate': 0.000287873914927899, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8736740560656203}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:28:46,907][0m Trial 46 finished with value: 0.12337281551811245 and parameters: {'observation_period_num': 220, 'train_rates': 0.7385231132246546, 'learning_rate': 0.00029919032708250633, 'batch_size': 106, 'step_size': 5, 'gamma': 0.8735842450702245}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:29:24,485][0m Trial 47 finished with value: 0.057776451213748414 and parameters: {'observation_period_num': 15, 'train_rates': 0.7537665916998822, 'learning_rate': 7.486687492790431e-05, 'batch_size': 140, 'step_size': 4, 'gamma': 0.899930439862554}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:30:32,873][0m Trial 48 finished with value: 0.05045411993033498 and parameters: {'observation_period_num': 28, 'train_rates': 0.6807076191899503, 'learning_rate': 9.097360275573798e-05, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8790166248001507}. Best is trial 12 with value: 0.026651102506245177.[0m
[32m[I 2025-01-04 13:31:12,729][0m Trial 49 finished with value: 0.0665566475905183 and parameters: {'observation_period_num': 69, 'train_rates': 0.7198553693024663, 'learning_rate': 0.00043156860535315184, 'batch_size': 127, 'step_size': 8, 'gamma': 0.935866883833576}. Best is trial 12 with value: 0.026651102506245177.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 13:31:12,740][0m A new study created in memory with name: no-name-90064ce9-38bd-428f-b303-6a9a3f006f29[0m
[32m[I 2025-01-04 13:31:48,663][0m Trial 0 finished with value: 0.4413660757763441 and parameters: {'observation_period_num': 18, 'train_rates': 0.7948656501255946, 'learning_rate': 1.907551143833547e-06, 'batch_size': 151, 'step_size': 4, 'gamma': 0.9116879072756556}. Best is trial 0 with value: 0.4413660757763441.[0m
[32m[I 2025-01-04 13:32:14,452][0m Trial 1 finished with value: 0.06008617205467696 and parameters: {'observation_period_num': 94, 'train_rates': 0.79855323370681, 'learning_rate': 0.0004760477605616514, 'batch_size': 227, 'step_size': 3, 'gamma': 0.9235721497410523}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:35:33,987][0m Trial 2 finished with value: 0.12625818249246437 and parameters: {'observation_period_num': 102, 'train_rates': 0.8977175553970311, 'learning_rate': 0.00032576252441546093, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9608016299053699}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:36:43,657][0m Trial 3 finished with value: 0.39672791415051906 and parameters: {'observation_period_num': 230, 'train_rates': 0.8416895967759663, 'learning_rate': 2.0487870847389663e-06, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9500608709207417}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:37:04,669][0m Trial 4 finished with value: 0.09375749035027688 and parameters: {'observation_period_num': 102, 'train_rates': 0.6594060784544372, 'learning_rate': 0.0006138705629034335, 'batch_size': 242, 'step_size': 12, 'gamma': 0.8106375279431652}. Best is trial 1 with value: 0.06008617205467696.[0m
Early stopping at epoch 41
[32m[I 2025-01-04 13:37:19,454][0m Trial 5 finished with value: 0.9814747429350347 and parameters: {'observation_period_num': 77, 'train_rates': 0.927473190098038, 'learning_rate': 2.864267889448297e-06, 'batch_size': 183, 'step_size': 1, 'gamma': 0.7934091068941712}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:38:01,383][0m Trial 6 finished with value: 0.08732847872376442 and parameters: {'observation_period_num': 117, 'train_rates': 0.9119612864201433, 'learning_rate': 0.0003242062018833522, 'batch_size': 141, 'step_size': 10, 'gamma': 0.9678211285931363}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:38:24,782][0m Trial 7 finished with value: 0.06890453762864157 and parameters: {'observation_period_num': 104, 'train_rates': 0.8343671198272837, 'learning_rate': 0.0005598266428772158, 'batch_size': 246, 'step_size': 9, 'gamma': 0.7617792404268268}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:40:21,124][0m Trial 8 finished with value: 0.4798296078091912 and parameters: {'observation_period_num': 208, 'train_rates': 0.6678390313468605, 'learning_rate': 1.360621135535426e-06, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9898038173800221}. Best is trial 1 with value: 0.06008617205467696.[0m
[32m[I 2025-01-04 13:41:54,550][0m Trial 9 finished with value: 0.03813500802165696 and parameters: {'observation_period_num': 31, 'train_rates': 0.9331456736774517, 'learning_rate': 0.0006941879067006096, 'batch_size': 63, 'step_size': 6, 'gamma': 0.9494366664056366}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:43:03,865][0m Trial 10 finished with value: 0.054823820013552904 and parameters: {'observation_period_num': 6, 'train_rates': 0.9647337447208821, 'learning_rate': 5.4170597538168276e-05, 'batch_size': 87, 'step_size': 6, 'gamma': 0.8667056601164268}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:44:07,170][0m Trial 11 finished with value: 0.05037888843152258 and parameters: {'observation_period_num': 11, 'train_rates': 0.9632905014380768, 'learning_rate': 5.687685272805242e-05, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8605591484010972}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:45:14,764][0m Trial 12 finished with value: 0.0728934183716774 and parameters: {'observation_period_num': 48, 'train_rates': 0.9884852387747002, 'learning_rate': 5.8923956208266665e-05, 'batch_size': 91, 'step_size': 6, 'gamma': 0.8701247027684893}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:46:06,099][0m Trial 13 finished with value: 0.24401287432937396 and parameters: {'observation_period_num': 157, 'train_rates': 0.8798696488170082, 'learning_rate': 1.559364107529934e-05, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8569382186826292}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:47:33,218][0m Trial 14 finished with value: 0.04863459531693382 and parameters: {'observation_period_num': 44, 'train_rates': 0.741496995110033, 'learning_rate': 0.00012447103339865282, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9017792127315516}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:49:03,262][0m Trial 15 finished with value: 0.07502274888431065 and parameters: {'observation_period_num': 49, 'train_rates': 0.7394560979033377, 'learning_rate': 0.00017820927614029295, 'batch_size': 54, 'step_size': 1, 'gamma': 0.911192457540458}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:52:38,209][0m Trial 16 finished with value: 0.11067330511167148 and parameters: {'observation_period_num': 165, 'train_rates': 0.7209170366009812, 'learning_rate': 0.00013113212080019442, 'batch_size': 21, 'step_size': 4, 'gamma': 0.8993302766237149}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:53:50,186][0m Trial 17 finished with value: 0.2046504461581612 and parameters: {'observation_period_num': 53, 'train_rates': 0.6029458093168462, 'learning_rate': 1.3123712872585196e-05, 'batch_size': 59, 'step_size': 3, 'gamma': 0.9371487119064483}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:54:32,301][0m Trial 18 finished with value: 0.10543657966962691 and parameters: {'observation_period_num': 149, 'train_rates': 0.746074622413017, 'learning_rate': 0.000900638747224299, 'batch_size': 117, 'step_size': 4, 'gamma': 0.8850310598606707}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:55:01,580][0m Trial 19 finished with value: 0.059345030325154466 and parameters: {'observation_period_num': 37, 'train_rates': 0.6914057647823098, 'learning_rate': 0.00016899819477844057, 'batch_size': 173, 'step_size': 12, 'gamma': 0.9800622762297206}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:56:31,085][0m Trial 20 finished with value: 0.11118033238583142 and parameters: {'observation_period_num': 74, 'train_rates': 0.8127102286412234, 'learning_rate': 1.541300752600742e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.836474736941083}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:57:28,316][0m Trial 21 finished with value: 0.055660491778376356 and parameters: {'observation_period_num': 21, 'train_rates': 0.9384960928718706, 'learning_rate': 8.287057089869164e-05, 'batch_size': 104, 'step_size': 5, 'gamma': 0.8380325212021098}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 13:58:42,116][0m Trial 22 finished with value: 0.050595447421073914 and parameters: {'observation_period_num': 28, 'train_rates': 0.8622464690836875, 'learning_rate': 2.4129279265234248e-05, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9383678249784931}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:00:57,909][0m Trial 23 finished with value: 0.3645220510661602 and parameters: {'observation_period_num': 61, 'train_rates': 0.9585668766957159, 'learning_rate': 5.633332169389689e-06, 'batch_size': 43, 'step_size': 2, 'gamma': 0.8900110053043493}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:01:43,398][0m Trial 24 finished with value: 0.0563050501793623 and parameters: {'observation_period_num': 6, 'train_rates': 0.7627048375222193, 'learning_rate': 4.29940635671383e-05, 'batch_size': 118, 'step_size': 5, 'gamma': 0.8493730744994998}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:03:00,868][0m Trial 25 finished with value: 0.038440313190221786 and parameters: {'observation_period_num': 33, 'train_rates': 0.9748293173870684, 'learning_rate': 0.00010330532325365883, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9289167050394502}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:04:26,248][0m Trial 26 finished with value: 0.09088050574064255 and parameters: {'observation_period_num': 74, 'train_rates': 0.9852334317700981, 'learning_rate': 0.0001099625673115836, 'batch_size': 70, 'step_size': 10, 'gamma': 0.9300833045898823}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:09:46,225][0m Trial 27 finished with value: 0.040172796846824406 and parameters: {'observation_period_num': 37, 'train_rates': 0.8898196741222084, 'learning_rate': 0.0002300422034819565, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9537711900257461}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:12:38,602][0m Trial 28 finished with value: 0.04274366932767856 and parameters: {'observation_period_num': 33, 'train_rates': 0.8917895579357322, 'learning_rate': 0.00022959571970570877, 'batch_size': 31, 'step_size': 8, 'gamma': 0.965165874170108}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:16:40,783][0m Trial 29 finished with value: 0.061598968364339965 and parameters: {'observation_period_num': 134, 'train_rates': 0.9367083443863471, 'learning_rate': 0.000988038118518966, 'batch_size': 22, 'step_size': 11, 'gamma': 0.9484754231792893}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:22:05,822][0m Trial 30 finished with value: 0.07957739152659857 and parameters: {'observation_period_num': 66, 'train_rates': 0.8521902526401605, 'learning_rate': 0.0002660605043607174, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9203328956877993}. Best is trial 9 with value: 0.03813500802165696.[0m
[32m[I 2025-01-04 14:24:26,803][0m Trial 31 finished with value: 0.031639113520344576 and parameters: {'observation_period_num': 27, 'train_rates': 0.8861003291278893, 'learning_rate': 0.00032586327493659684, 'batch_size': 39, 'step_size': 8, 'gamma': 0.9698331756782144}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:26:43,881][0m Trial 32 finished with value: 0.0418717119504105 and parameters: {'observation_period_num': 23, 'train_rates': 0.921185222694939, 'learning_rate': 0.0003861016044081863, 'batch_size': 42, 'step_size': 9, 'gamma': 0.982109185066384}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:28:44,359][0m Trial 33 finished with value: 0.11444915785430866 and parameters: {'observation_period_num': 90, 'train_rates': 0.8745652850797938, 'learning_rate': 0.0005615226062881253, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9504773255903219}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:29:59,680][0m Trial 34 finished with value: 0.03746336501919561 and parameters: {'observation_period_num': 33, 'train_rates': 0.9015200042976604, 'learning_rate': 0.00021092691466440949, 'batch_size': 77, 'step_size': 8, 'gamma': 0.9679257701454846}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:31:04,962][0m Trial 35 finished with value: 0.11928037897898601 and parameters: {'observation_period_num': 187, 'train_rates': 0.8216179668776233, 'learning_rate': 0.00042665857570159843, 'batch_size': 79, 'step_size': 10, 'gamma': 0.9707728560110529}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:31:45,921][0m Trial 36 finished with value: 0.055943618482439825 and parameters: {'observation_period_num': 87, 'train_rates': 0.788541190036991, 'learning_rate': 0.0006638782293975963, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9209332503015475}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:32:25,736][0m Trial 37 finished with value: 0.0516125923384626 and parameters: {'observation_period_num': 21, 'train_rates': 0.9037670601218767, 'learning_rate': 7.964384340827957e-05, 'batch_size': 154, 'step_size': 9, 'gamma': 0.976404098736708}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:33:54,189][0m Trial 38 finished with value: 0.07898865613554205 and parameters: {'observation_period_num': 63, 'train_rates': 0.9517600900993543, 'learning_rate': 0.0003602325003591002, 'batch_size': 65, 'step_size': 12, 'gamma': 0.9407936736412749}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:34:23,424][0m Trial 39 finished with value: 0.14854365454188415 and parameters: {'observation_period_num': 235, 'train_rates': 0.9176068949575088, 'learning_rate': 0.00017699566205733213, 'batch_size': 206, 'step_size': 8, 'gamma': 0.9551796384266504}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:35:31,209][0m Trial 40 finished with value: 0.1318725198507309 and parameters: {'observation_period_num': 114, 'train_rates': 0.9733067527308428, 'learning_rate': 0.000744674605269109, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9622101829761778}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:38:18,227][0m Trial 41 finished with value: 0.0553959789327704 and parameters: {'observation_period_num': 37, 'train_rates': 0.902034123669434, 'learning_rate': 0.00021460631710481064, 'batch_size': 33, 'step_size': 8, 'gamma': 0.9894738608512748}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:40:08,074][0m Trial 42 finished with value: 0.037933756286899246 and parameters: {'observation_period_num': 17, 'train_rates': 0.9458664326319816, 'learning_rate': 0.00027208150585242036, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9492912637755055}. Best is trial 31 with value: 0.031639113520344576.[0m
[32m[I 2025-01-04 14:42:06,351][0m Trial 43 finished with value: 0.030288443382067435 and parameters: {'observation_period_num': 15, 'train_rates': 0.9401360038670005, 'learning_rate': 0.00030605564283522793, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9304086955264044}. Best is trial 43 with value: 0.030288443382067435.[0m
[32m[I 2025-01-04 14:44:01,941][0m Trial 44 finished with value: 0.044259532587602735 and parameters: {'observation_period_num': 15, 'train_rates': 0.9455438347571967, 'learning_rate': 0.00045587177787829184, 'batch_size': 50, 'step_size': 11, 'gamma': 0.9660365514839362}. Best is trial 43 with value: 0.030288443382067435.[0m
[32m[I 2025-01-04 14:45:28,273][0m Trial 45 finished with value: 0.029877435607726082 and parameters: {'observation_period_num': 13, 'train_rates': 0.9305889235969741, 'learning_rate': 0.00032444193821533715, 'batch_size': 67, 'step_size': 10, 'gamma': 0.941989721717922}. Best is trial 45 with value: 0.029877435607726082.[0m
[32m[I 2025-01-04 14:48:17,038][0m Trial 46 finished with value: 0.022632983280345798 and parameters: {'observation_period_num': 7, 'train_rates': 0.9179426890082775, 'learning_rate': 0.00029827240796203615, 'batch_size': 33, 'step_size': 11, 'gamma': 0.7748109661127732}. Best is trial 46 with value: 0.022632983280345798.[0m
[32m[I 2025-01-04 14:51:32,713][0m Trial 47 finished with value: 0.021962821776942004 and parameters: {'observation_period_num': 9, 'train_rates': 0.8708618541456006, 'learning_rate': 0.00030616573778462805, 'batch_size': 28, 'step_size': 13, 'gamma': 0.811757314516297}. Best is trial 47 with value: 0.021962821776942004.[0m
[32m[I 2025-01-04 14:54:27,804][0m Trial 48 finished with value: 0.022267242512042436 and parameters: {'observation_period_num': 6, 'train_rates': 0.8668665028911521, 'learning_rate': 0.00031493065189997307, 'batch_size': 31, 'step_size': 13, 'gamma': 0.7805991086062646}. Best is trial 47 with value: 0.021962821776942004.[0m
[32m[I 2025-01-04 14:57:07,973][0m Trial 49 finished with value: 0.10359113774401077 and parameters: {'observation_period_num': 249, 'train_rates': 0.8610684345668465, 'learning_rate': 0.00015855624581415502, 'batch_size': 31, 'step_size': 15, 'gamma': 0.765837486869577}. Best is trial 47 with value: 0.021962821776942004.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 14:57:07,983][0m A new study created in memory with name: no-name-49591ce7-ef7e-46ab-923c-9e07037d904e[0m
[32m[I 2025-01-04 14:57:39,775][0m Trial 0 finished with value: 0.09358555034912647 and parameters: {'observation_period_num': 140, 'train_rates': 0.933429611166726, 'learning_rate': 0.0003585370052233587, 'batch_size': 177, 'step_size': 8, 'gamma': 0.7692285137562423}. Best is trial 0 with value: 0.09358555034912647.[0m
[32m[I 2025-01-04 14:58:08,197][0m Trial 1 finished with value: 0.17021216847727785 and parameters: {'observation_period_num': 187, 'train_rates': 0.7140721038704657, 'learning_rate': 0.00040236881973099923, 'batch_size': 171, 'step_size': 14, 'gamma': 0.8460876516355538}. Best is trial 0 with value: 0.09358555034912647.[0m
[32m[I 2025-01-04 14:58:55,556][0m Trial 2 finished with value: 0.02722531591437392 and parameters: {'observation_period_num': 13, 'train_rates': 0.8822491081365356, 'learning_rate': 0.0005717771653074142, 'batch_size': 122, 'step_size': 8, 'gamma': 0.7856123044786605}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 14:59:29,155][0m Trial 3 finished with value: 0.05317606292836677 and parameters: {'observation_period_num': 55, 'train_rates': 0.850205919117627, 'learning_rate': 9.16011957088713e-05, 'batch_size': 174, 'step_size': 15, 'gamma': 0.7589063905983056}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:01:53,464][0m Trial 4 finished with value: 0.13819400891661643 and parameters: {'observation_period_num': 252, 'train_rates': 0.9259638146202642, 'learning_rate': 0.00020467963696860168, 'batch_size': 36, 'step_size': 11, 'gamma': 0.8507823517478683}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:03:09,752][0m Trial 5 finished with value: 0.13934660797769374 and parameters: {'observation_period_num': 107, 'train_rates': 0.9228530315754482, 'learning_rate': 4.802184798897866e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8656637292557768}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:03:36,627][0m Trial 6 finished with value: 0.279116077995614 and parameters: {'observation_period_num': 62, 'train_rates': 0.868758334821367, 'learning_rate': 0.0001348199552057623, 'batch_size': 223, 'step_size': 9, 'gamma': 0.9726342221906861}. Best is trial 2 with value: 0.02722531591437392.[0m
Early stopping at epoch 84
[32m[I 2025-01-04 15:04:13,873][0m Trial 7 finished with value: 0.19530447401338022 and parameters: {'observation_period_num': 104, 'train_rates': 0.650275304712863, 'learning_rate': 0.000338912826251416, 'batch_size': 106, 'step_size': 1, 'gamma': 0.8247958553553095}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:04:51,536][0m Trial 8 finished with value: 0.06794442087411881 and parameters: {'observation_period_num': 89, 'train_rates': 0.9093429844248344, 'learning_rate': 0.0005626036267403758, 'batch_size': 156, 'step_size': 15, 'gamma': 0.9631883801193589}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:05:33,115][0m Trial 9 finished with value: 0.0607850863135349 and parameters: {'observation_period_num': 88, 'train_rates': 0.9405318245864197, 'learning_rate': 0.0005171488631299817, 'batch_size': 149, 'step_size': 3, 'gamma': 0.9146422600680665}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:05:56,939][0m Trial 10 finished with value: 0.3076181382354125 and parameters: {'observation_period_num': 5, 'train_rates': 0.7800775505330991, 'learning_rate': 9.573433308328357e-06, 'batch_size': 254, 'step_size': 6, 'gamma': 0.8018500546948129}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:06:46,333][0m Trial 11 finished with value: 0.32713573139241275 and parameters: {'observation_period_num': 9, 'train_rates': 0.8111558568303324, 'learning_rate': 1.413665402871328e-06, 'batch_size': 111, 'step_size': 12, 'gamma': 0.7613697716652995}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:07:14,790][0m Trial 12 finished with value: 0.1088748076539904 and parameters: {'observation_period_num': 43, 'train_rates': 0.8313837555030397, 'learning_rate': 5.422000220908736e-05, 'batch_size': 207, 'step_size': 7, 'gamma': 0.7501567835318604}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:08:04,051][0m Trial 13 finished with value: 0.20442494162974195 and parameters: {'observation_period_num': 42, 'train_rates': 0.7666781920548456, 'learning_rate': 1.0063336104827723e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8024837641759119}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:09:24,083][0m Trial 14 finished with value: 0.0786978367025485 and parameters: {'observation_period_num': 146, 'train_rates': 0.863586690712291, 'learning_rate': 0.00010386885137628134, 'batch_size': 67, 'step_size': 13, 'gamma': 0.9058229220832377}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:09:57,075][0m Trial 15 finished with value: 0.36466941237449646 and parameters: {'observation_period_num': 41, 'train_rates': 0.9722271159523402, 'learning_rate': 1.5747223685076003e-05, 'batch_size': 194, 'step_size': 5, 'gamma': 0.7941662132684144}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:10:35,158][0m Trial 16 finished with value: 0.057208181886661304 and parameters: {'observation_period_num': 63, 'train_rates': 0.7175040156260043, 'learning_rate': 0.0009042561415333112, 'batch_size': 136, 'step_size': 10, 'gamma': 0.7814038472864678}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:11:22,358][0m Trial 17 finished with value: 0.12343120064271991 and parameters: {'observation_period_num': 26, 'train_rates': 0.866455366348327, 'learning_rate': 2.2762916244428005e-05, 'batch_size': 126, 'step_size': 5, 'gamma': 0.8230294805433711}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:12:41,450][0m Trial 18 finished with value: 0.47959744930267334 and parameters: {'observation_period_num': 168, 'train_rates': 0.9890012092954448, 'learning_rate': 1.950896480718748e-06, 'batch_size': 76, 'step_size': 13, 'gamma': 0.9023227652887361}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:16:59,519][0m Trial 19 finished with value: 0.0688566533067534 and parameters: {'observation_period_num': 71, 'train_rates': 0.7418786067253148, 'learning_rate': 8.485248121813363e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8180358655614994}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:17:20,132][0m Trial 20 finished with value: 0.04052398512563245 and parameters: {'observation_period_num': 26, 'train_rates': 0.6393382203406229, 'learning_rate': 0.0009303102940879303, 'batch_size': 244, 'step_size': 15, 'gamma': 0.7744777766829476}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:17:40,719][0m Trial 21 finished with value: 0.04267204854443175 and parameters: {'observation_period_num': 19, 'train_rates': 0.6405211404651515, 'learning_rate': 0.0009658505520563213, 'batch_size': 254, 'step_size': 15, 'gamma': 0.7769714076636656}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:17:59,396][0m Trial 22 finished with value: 0.06650778123808071 and parameters: {'observation_period_num': 21, 'train_rates': 0.6030513123810055, 'learning_rate': 0.0008061530118000609, 'batch_size': 256, 'step_size': 12, 'gamma': 0.7749519684276843}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:18:22,298][0m Trial 23 finished with value: 0.06279192172269535 and parameters: {'observation_period_num': 27, 'train_rates': 0.6527446044506426, 'learning_rate': 0.0002192011041616865, 'batch_size': 231, 'step_size': 15, 'gamma': 0.7810751107117878}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:18:45,033][0m Trial 24 finished with value: 0.03200952085678751 and parameters: {'observation_period_num': 6, 'train_rates': 0.6685636281596267, 'learning_rate': 0.0009828601679659672, 'batch_size': 221, 'step_size': 13, 'gamma': 0.7973671080308552}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:19:07,255][0m Trial 25 finished with value: 0.042630101028206975 and parameters: {'observation_period_num': 5, 'train_rates': 0.6774098180903962, 'learning_rate': 0.00022680158256111247, 'batch_size': 228, 'step_size': 13, 'gamma': 0.8411783952158078}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:19:30,526][0m Trial 26 finished with value: 0.18292862470424143 and parameters: {'observation_period_num': 220, 'train_rates': 0.6117408546523819, 'learning_rate': 0.0006205547111948926, 'batch_size': 203, 'step_size': 11, 'gamma': 0.8829127675427615}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:19:52,662][0m Trial 27 finished with value: 0.7392103956769343 and parameters: {'observation_period_num': 38, 'train_rates': 0.684473417310092, 'learning_rate': 3.0016444668518165e-06, 'batch_size': 237, 'step_size': 9, 'gamma': 0.8005976255963294}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:20:17,986][0m Trial 28 finished with value: 0.09409367552491178 and parameters: {'observation_period_num': 81, 'train_rates': 0.6941362919262544, 'learning_rate': 0.0002976030231553518, 'batch_size': 214, 'step_size': 14, 'gamma': 0.8131400936794476}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:20:45,796][0m Trial 29 finished with value: 0.13886935924169388 and parameters: {'observation_period_num': 133, 'train_rates': 0.7476557613384243, 'learning_rate': 0.000481600911362422, 'batch_size': 189, 'step_size': 7, 'gamma': 0.9417842919601629}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:21:36,297][0m Trial 30 finished with value: 0.5430144706912858 and parameters: {'observation_period_num': 110, 'train_rates': 0.6264610692789094, 'learning_rate': 0.00015972161589854694, 'batch_size': 88, 'step_size': 9, 'gamma': 0.8318398552122462}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:21:58,963][0m Trial 31 finished with value: 0.03992933326304337 and parameters: {'observation_period_num': 6, 'train_rates': 0.6789335891369344, 'learning_rate': 0.0003332367430648063, 'batch_size': 240, 'step_size': 13, 'gamma': 0.8453943234067379}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:22:22,126][0m Trial 32 finished with value: 0.04732206876469937 and parameters: {'observation_period_num': 27, 'train_rates': 0.6589507896084226, 'learning_rate': 0.0003517500324864394, 'batch_size': 239, 'step_size': 14, 'gamma': 0.8584980840601071}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:22:55,992][0m Trial 33 finished with value: 0.05411141160944291 and parameters: {'observation_period_num': 54, 'train_rates': 0.7194951057773797, 'learning_rate': 0.0009903037276737044, 'batch_size': 159, 'step_size': 12, 'gamma': 0.7913171663099211}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:23:23,578][0m Trial 34 finished with value: 0.04430450719976148 and parameters: {'observation_period_num': 15, 'train_rates': 0.6781537764827967, 'learning_rate': 0.0006385779773358712, 'batch_size': 183, 'step_size': 14, 'gamma': 0.8795847502799506}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:23:59,231][0m Trial 35 finished with value: 0.03886412214896522 and parameters: {'observation_period_num': 36, 'train_rates': 0.8906201939253525, 'learning_rate': 0.00039475496536989127, 'batch_size': 171, 'step_size': 11, 'gamma': 0.7543883506852089}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:24:28,440][0m Trial 36 finished with value: 0.05508653223514557 and parameters: {'observation_period_num': 50, 'train_rates': 0.8966420494309091, 'learning_rate': 0.0003210333685284424, 'batch_size': 216, 'step_size': 11, 'gamma': 0.761574174760924}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:25:03,838][0m Trial 37 finished with value: 0.05208680151200821 and parameters: {'observation_period_num': 35, 'train_rates': 0.8913310026932347, 'learning_rate': 0.00014705103568669897, 'batch_size': 168, 'step_size': 10, 'gamma': 0.7517096015216578}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:25:42,424][0m Trial 38 finished with value: 0.11853486118026269 and parameters: {'observation_period_num': 198, 'train_rates': 0.8304242259401294, 'learning_rate': 0.0004540738914816193, 'batch_size': 142, 'step_size': 12, 'gamma': 0.8402307322358287}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:26:16,896][0m Trial 39 finished with value: 0.06772959232330322 and parameters: {'observation_period_num': 67, 'train_rates': 0.9555568271252666, 'learning_rate': 0.00023182977452421637, 'batch_size': 173, 'step_size': 11, 'gamma': 0.808485522799511}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:26:46,650][0m Trial 40 finished with value: 0.05830872460053517 and parameters: {'observation_period_num': 14, 'train_rates': 0.7877031251879196, 'learning_rate': 8.929491409010988e-05, 'batch_size': 192, 'step_size': 7, 'gamma': 0.852587586635888}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:27:07,863][0m Trial 41 finished with value: 0.06230213722041033 and parameters: {'observation_period_num': 30, 'train_rates': 0.6283146426632552, 'learning_rate': 0.0006914321185660341, 'batch_size': 244, 'step_size': 14, 'gamma': 0.7663152881178856}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:27:53,938][0m Trial 42 finished with value: 0.02788212806592847 and parameters: {'observation_period_num': 6, 'train_rates': 0.8834457249788455, 'learning_rate': 0.00040820316429057154, 'batch_size': 124, 'step_size': 14, 'gamma': 0.7893406657457692}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:28:54,344][0m Trial 43 finished with value: 0.027590935416368822 and parameters: {'observation_period_num': 6, 'train_rates': 0.9163798473966897, 'learning_rate': 0.00041374660890247574, 'batch_size': 97, 'step_size': 13, 'gamma': 0.7900786955479723}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:29:55,506][0m Trial 44 finished with value: 0.058574942770697504 and parameters: {'observation_period_num': 50, 'train_rates': 0.9369361816074903, 'learning_rate': 0.00046605341137308394, 'batch_size': 95, 'step_size': 13, 'gamma': 0.7890835648787509}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:30:46,512][0m Trial 45 finished with value: 0.05610495913577707 and parameters: {'observation_period_num': 14, 'train_rates': 0.9097702459636081, 'learning_rate': 5.4085299829112284e-05, 'batch_size': 119, 'step_size': 9, 'gamma': 0.7654067159433812}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:32:24,512][0m Trial 46 finished with value: 0.035047563635239934 and parameters: {'observation_period_num': 17, 'train_rates': 0.8398441700892502, 'learning_rate': 0.0001728764626117398, 'batch_size': 54, 'step_size': 14, 'gamma': 0.9896316606668009}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:33:52,277][0m Trial 47 finished with value: 0.16057035504775855 and parameters: {'observation_period_num': 247, 'train_rates': 0.8452386976907705, 'learning_rate': 0.00016644970474134812, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9794098806155522}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:35:28,950][0m Trial 48 finished with value: 0.028324947263643468 and parameters: {'observation_period_num': 6, 'train_rates': 0.8112969684258821, 'learning_rate': 0.0006192140928127544, 'batch_size': 54, 'step_size': 6, 'gamma': 0.9532690302755917}. Best is trial 2 with value: 0.02722531591437392.[0m
[32m[I 2025-01-04 15:36:23,419][0m Trial 49 finished with value: 0.03266212582536094 and parameters: {'observation_period_num': 6, 'train_rates': 0.8059622915945563, 'learning_rate': 0.0005884633414054639, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9619005313375065}. Best is trial 2 with value: 0.02722531591437392.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8335554633803406, 'learning_rate': 0.00021076009161061788, 'batch_size': 84, 'step_size': 15, 'gamma': 0.9429049476971336}
Epoch 1/300, trend Loss: 0.4459 | 0.2314
Epoch 2/300, trend Loss: 0.1685 | 0.1738
Epoch 3/300, trend Loss: 0.1392 | 0.1325
Epoch 4/300, trend Loss: 0.1271 | 0.0936
Epoch 5/300, trend Loss: 0.1172 | 0.0760
Epoch 6/300, trend Loss: 0.1122 | 0.0676
Epoch 7/300, trend Loss: 0.1099 | 0.0655
Epoch 8/300, trend Loss: 0.1083 | 0.0637
Epoch 9/300, trend Loss: 0.1059 | 0.0592
Epoch 10/300, trend Loss: 0.1031 | 0.0566
Epoch 11/300, trend Loss: 0.1001 | 0.0552
Epoch 12/300, trend Loss: 0.0982 | 0.0543
Epoch 13/300, trend Loss: 0.0971 | 0.0534
Epoch 14/300, trend Loss: 0.0960 | 0.0523
Epoch 15/300, trend Loss: 0.0948 | 0.0510
Epoch 16/300, trend Loss: 0.0932 | 0.0497
Epoch 17/300, trend Loss: 0.0918 | 0.0482
Epoch 18/300, trend Loss: 0.0922 | 0.0482
Epoch 19/300, trend Loss: 0.0928 | 0.0477
Epoch 20/300, trend Loss: 0.0936 | 0.0476
Epoch 21/300, trend Loss: 0.0934 | 0.0458
Epoch 22/300, trend Loss: 0.0907 | 0.0453
Epoch 23/300, trend Loss: 0.0927 | 0.0657
Epoch 24/300, trend Loss: 0.0998 | 0.0726
Epoch 25/300, trend Loss: 0.1000 | 0.0447
Epoch 26/300, trend Loss: 0.0964 | 0.0670
Epoch 27/300, trend Loss: 0.1006 | 0.0658
Epoch 28/300, trend Loss: 0.1003 | 0.0578
Epoch 29/300, trend Loss: 0.0900 | 0.0432
Epoch 30/300, trend Loss: 0.0823 | 0.0394
Epoch 31/300, trend Loss: 0.0793 | 0.0377
Epoch 32/300, trend Loss: 0.0803 | 0.0390
Epoch 33/300, trend Loss: 0.0810 | 0.0407
Epoch 34/300, trend Loss: 0.0803 | 0.0404
Epoch 35/300, trend Loss: 0.0795 | 0.0409
Epoch 36/300, trend Loss: 0.0786 | 0.0395
Epoch 37/300, trend Loss: 0.0775 | 0.0384
Epoch 38/300, trend Loss: 0.0764 | 0.0365
Epoch 39/300, trend Loss: 0.0757 | 0.0358
Epoch 40/300, trend Loss: 0.0754 | 0.0348
Epoch 41/300, trend Loss: 0.0754 | 0.0341
Epoch 42/300, trend Loss: 0.0750 | 0.0335
Epoch 43/300, trend Loss: 0.0747 | 0.0331
Epoch 44/300, trend Loss: 0.0745 | 0.0331
Epoch 45/300, trend Loss: 0.0742 | 0.0333
Epoch 46/300, trend Loss: 0.0741 | 0.0348
Epoch 47/300, trend Loss: 0.0741 | 0.0337
Epoch 48/300, trend Loss: 0.0737 | 0.0338
Epoch 49/300, trend Loss: 0.0737 | 0.0337
Epoch 50/300, trend Loss: 0.0736 | 0.0343
Epoch 51/300, trend Loss: 0.0732 | 0.0336
Epoch 52/300, trend Loss: 0.0724 | 0.0340
Epoch 53/300, trend Loss: 0.0716 | 0.0324
Epoch 54/300, trend Loss: 0.0711 | 0.0330
Epoch 55/300, trend Loss: 0.0711 | 0.0321
Epoch 56/300, trend Loss: 0.0712 | 0.0321
Epoch 57/300, trend Loss: 0.0709 | 0.0310
Epoch 58/300, trend Loss: 0.0695 | 0.0299
Epoch 59/300, trend Loss: 0.0689 | 0.0297
Epoch 60/300, trend Loss: 0.0698 | 0.0303
Epoch 61/300, trend Loss: 0.0712 | 0.0306
Epoch 62/300, trend Loss: 0.0727 | 0.0310
Epoch 63/300, trend Loss: 0.0727 | 0.0323
Epoch 64/300, trend Loss: 0.0718 | 0.0335
Epoch 65/300, trend Loss: 0.0709 | 0.0339
Epoch 66/300, trend Loss: 0.0698 | 0.0291
Epoch 67/300, trend Loss: 0.0691 | 0.0346
Epoch 68/300, trend Loss: 0.0705 | 0.0308
Epoch 69/300, trend Loss: 0.0707 | 0.0366
Epoch 70/300, trend Loss: 0.0712 | 0.0322
Epoch 71/300, trend Loss: 0.0689 | 0.0339
Epoch 72/300, trend Loss: 0.0687 | 0.0305
Epoch 73/300, trend Loss: 0.0687 | 0.0291
Epoch 74/300, trend Loss: 0.0676 | 0.0276
Epoch 75/300, trend Loss: 0.0676 | 0.0273
Epoch 76/300, trend Loss: 0.0671 | 0.0274
Epoch 77/300, trend Loss: 0.0668 | 0.0277
Epoch 78/300, trend Loss: 0.0663 | 0.0277
Epoch 79/300, trend Loss: 0.0658 | 0.0276
Epoch 80/300, trend Loss: 0.0652 | 0.0273
Epoch 81/300, trend Loss: 0.0647 | 0.0272
Epoch 82/300, trend Loss: 0.0644 | 0.0271
Epoch 83/300, trend Loss: 0.0643 | 0.0271
Epoch 84/300, trend Loss: 0.0644 | 0.0275
Epoch 85/300, trend Loss: 0.0647 | 0.0275
Epoch 86/300, trend Loss: 0.0649 | 0.0274
Epoch 87/300, trend Loss: 0.0649 | 0.0272
Epoch 88/300, trend Loss: 0.0647 | 0.0268
Epoch 89/300, trend Loss: 0.0637 | 0.0272
Epoch 90/300, trend Loss: 0.0636 | 0.0273
Epoch 91/300, trend Loss: 0.0637 | 0.0261
Epoch 92/300, trend Loss: 0.0638 | 0.0257
Epoch 93/300, trend Loss: 0.0638 | 0.0255
Epoch 94/300, trend Loss: 0.0643 | 0.0269
Epoch 95/300, trend Loss: 0.0641 | 0.0274
Epoch 96/300, trend Loss: 0.0634 | 0.0263
Epoch 97/300, trend Loss: 0.0627 | 0.0258
Epoch 98/300, trend Loss: 0.0626 | 0.0262
Epoch 99/300, trend Loss: 0.0627 | 0.0276
Epoch 100/300, trend Loss: 0.0629 | 0.0281
Epoch 101/300, trend Loss: 0.0637 | 0.0301
Epoch 102/300, trend Loss: 0.0668 | 0.0346
Epoch 103/300, trend Loss: 0.0691 | 0.0304
Epoch 104/300, trend Loss: 0.0685 | 0.0299
Epoch 105/300, trend Loss: 0.0699 | 0.0415
Epoch 106/300, trend Loss: 0.0714 | 0.0268
Epoch 107/300, trend Loss: 0.0670 | 0.0338
Epoch 108/300, trend Loss: 0.0658 | 0.0287
Epoch 109/300, trend Loss: 0.0649 | 0.0291
Epoch 110/300, trend Loss: 0.0623 | 0.0255
Epoch 111/300, trend Loss: 0.0629 | 0.0279
Epoch 112/300, trend Loss: 0.0618 | 0.0250
Epoch 113/300, trend Loss: 0.0625 | 0.0266
Epoch 114/300, trend Loss: 0.0648 | 0.0271
Epoch 115/300, trend Loss: 0.0633 | 0.0290
Epoch 116/300, trend Loss: 0.0631 | 0.0297
Epoch 117/300, trend Loss: 0.0633 | 0.0287
Epoch 118/300, trend Loss: 0.0619 | 0.0269
Epoch 119/300, trend Loss: 0.0614 | 0.0265
Epoch 120/300, trend Loss: 0.0611 | 0.0262
Epoch 121/300, trend Loss: 0.0612 | 0.0268
Epoch 122/300, trend Loss: 0.0617 | 0.0271
Epoch 123/300, trend Loss: 0.0621 | 0.0285
Epoch 124/300, trend Loss: 0.0618 | 0.0278
Epoch 125/300, trend Loss: 0.0610 | 0.0274
Epoch 126/300, trend Loss: 0.0604 | 0.0266
Epoch 127/300, trend Loss: 0.0600 | 0.0262
Epoch 128/300, trend Loss: 0.0597 | 0.0259
Epoch 129/300, trend Loss: 0.0597 | 0.0256
Epoch 130/300, trend Loss: 0.0600 | 0.0251
Epoch 131/300, trend Loss: 0.0606 | 0.0248
Epoch 132/300, trend Loss: 0.0612 | 0.0248
Epoch 133/300, trend Loss: 0.0619 | 0.0251
Epoch 134/300, trend Loss: 0.0628 | 0.0256
Epoch 135/300, trend Loss: 0.0643 | 0.0262
Epoch 136/300, trend Loss: 0.0661 | 0.0261
Epoch 137/300, trend Loss: 0.0679 | 0.0259
Epoch 138/300, trend Loss: 0.0681 | 0.0261
Epoch 139/300, trend Loss: 0.0677 | 0.0298
Epoch 140/300, trend Loss: 0.0655 | 0.0292
Epoch 141/300, trend Loss: 0.0610 | 0.0270
Epoch 142/300, trend Loss: 0.0611 | 0.0289
Epoch 143/300, trend Loss: 0.0623 | 0.0292
Epoch 144/300, trend Loss: 0.0623 | 0.0291
Epoch 145/300, trend Loss: 0.0623 | 0.0267
Epoch 146/300, trend Loss: 0.0614 | 0.0249
Epoch 147/300, trend Loss: 0.0606 | 0.0235
Epoch 148/300, trend Loss: 0.0597 | 0.0232
Epoch 149/300, trend Loss: 0.0593 | 0.0234
Epoch 150/300, trend Loss: 0.0599 | 0.0236
Epoch 151/300, trend Loss: 0.0607 | 0.0237
Epoch 152/300, trend Loss: 0.0602 | 0.0234
Epoch 153/300, trend Loss: 0.0595 | 0.0236
Epoch 154/300, trend Loss: 0.0581 | 0.0244
Epoch 155/300, trend Loss: 0.0575 | 0.0241
Epoch 156/300, trend Loss: 0.0570 | 0.0233
Epoch 157/300, trend Loss: 0.0567 | 0.0227
Epoch 158/300, trend Loss: 0.0563 | 0.0223
Epoch 159/300, trend Loss: 0.0561 | 0.0220
Epoch 160/300, trend Loss: 0.0561 | 0.0217
Epoch 161/300, trend Loss: 0.0562 | 0.0217
Epoch 162/300, trend Loss: 0.0565 | 0.0217
Epoch 163/300, trend Loss: 0.0567 | 0.0218
Epoch 164/300, trend Loss: 0.0568 | 0.0222
Epoch 165/300, trend Loss: 0.0571 | 0.0223
Epoch 166/300, trend Loss: 0.0566 | 0.0221
Epoch 167/300, trend Loss: 0.0558 | 0.0226
Epoch 168/300, trend Loss: 0.0556 | 0.0229
Epoch 169/300, trend Loss: 0.0556 | 0.0225
Epoch 170/300, trend Loss: 0.0557 | 0.0222
Epoch 171/300, trend Loss: 0.0557 | 0.0220
Epoch 172/300, trend Loss: 0.0556 | 0.0219
Epoch 173/300, trend Loss: 0.0556 | 0.0219
Epoch 174/300, trend Loss: 0.0558 | 0.0219
Epoch 175/300, trend Loss: 0.0566 | 0.0220
Epoch 176/300, trend Loss: 0.0577 | 0.0239
Epoch 177/300, trend Loss: 0.0570 | 0.0230
Epoch 178/300, trend Loss: 0.0578 | 0.0238
Epoch 179/300, trend Loss: 0.0577 | 0.0237
Epoch 180/300, trend Loss: 0.0619 | 0.0294
Epoch 181/300, trend Loss: 0.0608 | 0.0281
Epoch 182/300, trend Loss: 0.0594 | 0.0264
Epoch 183/300, trend Loss: 0.0591 | 0.0254
Epoch 184/300, trend Loss: 0.0587 | 0.0235
Epoch 185/300, trend Loss: 0.0569 | 0.0225
Epoch 186/300, trend Loss: 0.0554 | 0.0218
Epoch 187/300, trend Loss: 0.0549 | 0.0215
Epoch 188/300, trend Loss: 0.0555 | 0.0214
Epoch 189/300, trend Loss: 0.0566 | 0.0218
Epoch 190/300, trend Loss: 0.0579 | 0.0223
Epoch 191/300, trend Loss: 0.0588 | 0.0244
Epoch 192/300, trend Loss: 0.0559 | 0.0213
Epoch 193/300, trend Loss: 0.0553 | 0.0211
Epoch 194/300, trend Loss: 0.0551 | 0.0207
Epoch 195/300, trend Loss: 0.0549 | 0.0207
Epoch 196/300, trend Loss: 0.0545 | 0.0208
Epoch 197/300, trend Loss: 0.0541 | 0.0205
Epoch 198/300, trend Loss: 0.0539 | 0.0204
Epoch 199/300, trend Loss: 0.0539 | 0.0209
Epoch 200/300, trend Loss: 0.0539 | 0.0213
Epoch 201/300, trend Loss: 0.0538 | 0.0215
Epoch 202/300, trend Loss: 0.0537 | 0.0215
Epoch 203/300, trend Loss: 0.0535 | 0.0213
Epoch 204/300, trend Loss: 0.0534 | 0.0214
Epoch 205/300, trend Loss: 0.0535 | 0.0215
Epoch 206/300, trend Loss: 0.0537 | 0.0215
Epoch 207/300, trend Loss: 0.0539 | 0.0214
Epoch 208/300, trend Loss: 0.0540 | 0.0213
Epoch 209/300, trend Loss: 0.0541 | 0.0212
Epoch 210/300, trend Loss: 0.0543 | 0.0212
Epoch 211/300, trend Loss: 0.0546 | 0.0211
Epoch 212/300, trend Loss: 0.0546 | 0.0212
Epoch 213/300, trend Loss: 0.0540 | 0.0212
Epoch 214/300, trend Loss: 0.0536 | 0.0209
Epoch 215/300, trend Loss: 0.0535 | 0.0208
Epoch 216/300, trend Loss: 0.0535 | 0.0207
Epoch 217/300, trend Loss: 0.0535 | 0.0208
Epoch 218/300, trend Loss: 0.0535 | 0.0209
Epoch 219/300, trend Loss: 0.0534 | 0.0211
Epoch 220/300, trend Loss: 0.0532 | 0.0211
Epoch 221/300, trend Loss: 0.0528 | 0.0209
Epoch 222/300, trend Loss: 0.0527 | 0.0205
Epoch 223/300, trend Loss: 0.0529 | 0.0204
Epoch 224/300, trend Loss: 0.0531 | 0.0211
Epoch 225/300, trend Loss: 0.0534 | 0.0207
Epoch 226/300, trend Loss: 0.0533 | 0.0204
Epoch 227/300, trend Loss: 0.0529 | 0.0200
Epoch 228/300, trend Loss: 0.0528 | 0.0202
Epoch 229/300, trend Loss: 0.0528 | 0.0207
Epoch 230/300, trend Loss: 0.0526 | 0.0209
Epoch 231/300, trend Loss: 0.0524 | 0.0206
Epoch 232/300, trend Loss: 0.0523 | 0.0205
Epoch 233/300, trend Loss: 0.0524 | 0.0205
Epoch 234/300, trend Loss: 0.0525 | 0.0204
Epoch 235/300, trend Loss: 0.0525 | 0.0202
Epoch 236/300, trend Loss: 0.0523 | 0.0200
Epoch 237/300, trend Loss: 0.0522 | 0.0198
Epoch 238/300, trend Loss: 0.0521 | 0.0198
Epoch 239/300, trend Loss: 0.0520 | 0.0200
Epoch 240/300, trend Loss: 0.0518 | 0.0202
Epoch 241/300, trend Loss: 0.0519 | 0.0206
Epoch 242/300, trend Loss: 0.0526 | 0.0212
Epoch 243/300, trend Loss: 0.0549 | 0.0212
Epoch 244/300, trend Loss: 0.0534 | 0.0204
Epoch 245/300, trend Loss: 0.0533 | 0.0212
Epoch 246/300, trend Loss: 0.0533 | 0.0208
Epoch 247/300, trend Loss: 0.0529 | 0.0210
Epoch 248/300, trend Loss: 0.0528 | 0.0199
Epoch 249/300, trend Loss: 0.0522 | 0.0196
Epoch 250/300, trend Loss: 0.0517 | 0.0197
Epoch 251/300, trend Loss: 0.0515 | 0.0193
Epoch 252/300, trend Loss: 0.0515 | 0.0191
Epoch 253/300, trend Loss: 0.0516 | 0.0193
Epoch 254/300, trend Loss: 0.0518 | 0.0193
Epoch 255/300, trend Loss: 0.0518 | 0.0193
Epoch 256/300, trend Loss: 0.0518 | 0.0193
Epoch 257/300, trend Loss: 0.0517 | 0.0197
Epoch 258/300, trend Loss: 0.0515 | 0.0200
Epoch 259/300, trend Loss: 0.0512 | 0.0201
Epoch 260/300, trend Loss: 0.0511 | 0.0201
Epoch 261/300, trend Loss: 0.0510 | 0.0202
Epoch 262/300, trend Loss: 0.0511 | 0.0204
Epoch 263/300, trend Loss: 0.0514 | 0.0204
Epoch 264/300, trend Loss: 0.0516 | 0.0200
Epoch 265/300, trend Loss: 0.0517 | 0.0201
Epoch 266/300, trend Loss: 0.0515 | 0.0200
Epoch 267/300, trend Loss: 0.0513 | 0.0197
Epoch 268/300, trend Loss: 0.0510 | 0.0195
Epoch 269/300, trend Loss: 0.0508 | 0.0196
Epoch 270/300, trend Loss: 0.0507 | 0.0195
Epoch 271/300, trend Loss: 0.0507 | 0.0194
Epoch 272/300, trend Loss: 0.0508 | 0.0193
Epoch 273/300, trend Loss: 0.0508 | 0.0194
Epoch 274/300, trend Loss: 0.0508 | 0.0194
Epoch 275/300, trend Loss: 0.0507 | 0.0194
Epoch 276/300, trend Loss: 0.0505 | 0.0195
Epoch 277/300, trend Loss: 0.0504 | 0.0197
Epoch 278/300, trend Loss: 0.0503 | 0.0199
Epoch 279/300, trend Loss: 0.0503 | 0.0200
Epoch 280/300, trend Loss: 0.0503 | 0.0200
Epoch 281/300, trend Loss: 0.0503 | 0.0200
Epoch 282/300, trend Loss: 0.0503 | 0.0199
Epoch 283/300, trend Loss: 0.0503 | 0.0198
Epoch 284/300, trend Loss: 0.0503 | 0.0196
Epoch 285/300, trend Loss: 0.0502 | 0.0196
Epoch 286/300, trend Loss: 0.0501 | 0.0195
Epoch 287/300, trend Loss: 0.0501 | 0.0196
Epoch 288/300, trend Loss: 0.0500 | 0.0196
Epoch 289/300, trend Loss: 0.0500 | 0.0195
Epoch 290/300, trend Loss: 0.0500 | 0.0195
Epoch 291/300, trend Loss: 0.0499 | 0.0195
Epoch 292/300, trend Loss: 0.0499 | 0.0195
Epoch 293/300, trend Loss: 0.0498 | 0.0195
Epoch 294/300, trend Loss: 0.0498 | 0.0196
Epoch 295/300, trend Loss: 0.0498 | 0.0197
Epoch 296/300, trend Loss: 0.0498 | 0.0198
Epoch 297/300, trend Loss: 0.0497 | 0.0199
Epoch 298/300, trend Loss: 0.0497 | 0.0199
Epoch 299/300, trend Loss: 0.0497 | 0.0199
Epoch 300/300, trend Loss: 0.0498 | 0.0199
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.8038893514060613, 'learning_rate': 0.0005217407621398729, 'batch_size': 117, 'step_size': 6, 'gamma': 0.8402733809863824}
Epoch 1/300, seasonal_0 Loss: 0.5172 | 0.2705
Epoch 2/300, seasonal_0 Loss: 0.1670 | 0.1632
Epoch 3/300, seasonal_0 Loss: 0.1610 | 0.1263
Epoch 4/300, seasonal_0 Loss: 0.1439 | 0.1094
Epoch 5/300, seasonal_0 Loss: 0.1404 | 0.1055
Epoch 6/300, seasonal_0 Loss: 0.1295 | 0.0740
Epoch 7/300, seasonal_0 Loss: 0.1108 | 0.0728
Epoch 8/300, seasonal_0 Loss: 0.1099 | 0.0676
Epoch 9/300, seasonal_0 Loss: 0.1051 | 0.0633
Epoch 10/300, seasonal_0 Loss: 0.1062 | 0.0623
Epoch 11/300, seasonal_0 Loss: 0.1065 | 0.0606
Epoch 12/300, seasonal_0 Loss: 0.1048 | 0.0596
Epoch 13/300, seasonal_0 Loss: 0.1051 | 0.0608
Epoch 14/300, seasonal_0 Loss: 0.1073 | 0.0695
Epoch 15/300, seasonal_0 Loss: 0.1110 | 0.0942
Epoch 16/300, seasonal_0 Loss: 0.1146 | 0.0906
Epoch 17/300, seasonal_0 Loss: 0.1108 | 0.0561
Epoch 18/300, seasonal_0 Loss: 0.0974 | 0.0584
Epoch 19/300, seasonal_0 Loss: 0.1032 | 0.0603
Epoch 20/300, seasonal_0 Loss: 0.0961 | 0.0525
Epoch 21/300, seasonal_0 Loss: 0.0932 | 0.0520
Epoch 22/300, seasonal_0 Loss: 0.0926 | 0.0545
Epoch 23/300, seasonal_0 Loss: 0.0908 | 0.0524
Epoch 24/300, seasonal_0 Loss: 0.0891 | 0.0496
Epoch 25/300, seasonal_0 Loss: 0.0879 | 0.0485
Epoch 26/300, seasonal_0 Loss: 0.0874 | 0.0480
Epoch 27/300, seasonal_0 Loss: 0.0871 | 0.0472
Epoch 28/300, seasonal_0 Loss: 0.0867 | 0.0468
Epoch 29/300, seasonal_0 Loss: 0.0865 | 0.0460
Epoch 30/300, seasonal_0 Loss: 0.0863 | 0.0457
Epoch 31/300, seasonal_0 Loss: 0.0868 | 0.0459
Epoch 32/300, seasonal_0 Loss: 0.0877 | 0.0456
Epoch 33/300, seasonal_0 Loss: 0.0878 | 0.0453
Epoch 34/300, seasonal_0 Loss: 0.0880 | 0.0447
Epoch 35/300, seasonal_0 Loss: 0.0871 | 0.0448
Epoch 36/300, seasonal_0 Loss: 0.0853 | 0.0450
Epoch 37/300, seasonal_0 Loss: 0.0846 | 0.0449
Epoch 38/300, seasonal_0 Loss: 0.0843 | 0.0442
Epoch 39/300, seasonal_0 Loss: 0.0839 | 0.0434
Epoch 40/300, seasonal_0 Loss: 0.0832 | 0.0426
Epoch 41/300, seasonal_0 Loss: 0.0822 | 0.0423
Epoch 42/300, seasonal_0 Loss: 0.0813 | 0.0419
Epoch 43/300, seasonal_0 Loss: 0.0809 | 0.0417
Epoch 44/300, seasonal_0 Loss: 0.0806 | 0.0416
Epoch 45/300, seasonal_0 Loss: 0.0803 | 0.0414
Epoch 46/300, seasonal_0 Loss: 0.0801 | 0.0413
Epoch 47/300, seasonal_0 Loss: 0.0798 | 0.0411
Epoch 48/300, seasonal_0 Loss: 0.0796 | 0.0409
Epoch 49/300, seasonal_0 Loss: 0.0794 | 0.0408
Epoch 50/300, seasonal_0 Loss: 0.0793 | 0.0406
Epoch 51/300, seasonal_0 Loss: 0.0791 | 0.0404
Epoch 52/300, seasonal_0 Loss: 0.0789 | 0.0403
Epoch 53/300, seasonal_0 Loss: 0.0788 | 0.0402
Epoch 54/300, seasonal_0 Loss: 0.0787 | 0.0400
Epoch 55/300, seasonal_0 Loss: 0.0785 | 0.0399
Epoch 56/300, seasonal_0 Loss: 0.0784 | 0.0398
Epoch 57/300, seasonal_0 Loss: 0.0783 | 0.0397
Epoch 58/300, seasonal_0 Loss: 0.0782 | 0.0396
Epoch 59/300, seasonal_0 Loss: 0.0781 | 0.0395
Epoch 60/300, seasonal_0 Loss: 0.0780 | 0.0394
Epoch 61/300, seasonal_0 Loss: 0.0779 | 0.0393
Epoch 62/300, seasonal_0 Loss: 0.0778 | 0.0393
Epoch 63/300, seasonal_0 Loss: 0.0778 | 0.0392
Epoch 64/300, seasonal_0 Loss: 0.0777 | 0.0391
Epoch 65/300, seasonal_0 Loss: 0.0776 | 0.0391
Epoch 66/300, seasonal_0 Loss: 0.0776 | 0.0390
Epoch 67/300, seasonal_0 Loss: 0.0775 | 0.0390
Epoch 68/300, seasonal_0 Loss: 0.0774 | 0.0389
Epoch 69/300, seasonal_0 Loss: 0.0774 | 0.0389
Epoch 70/300, seasonal_0 Loss: 0.0773 | 0.0388
Epoch 71/300, seasonal_0 Loss: 0.0773 | 0.0388
Epoch 72/300, seasonal_0 Loss: 0.0772 | 0.0388
Epoch 73/300, seasonal_0 Loss: 0.0772 | 0.0387
Epoch 74/300, seasonal_0 Loss: 0.0772 | 0.0387
Epoch 75/300, seasonal_0 Loss: 0.0771 | 0.0387
Epoch 76/300, seasonal_0 Loss: 0.0771 | 0.0386
Epoch 77/300, seasonal_0 Loss: 0.0770 | 0.0386
Epoch 78/300, seasonal_0 Loss: 0.0770 | 0.0386
Epoch 79/300, seasonal_0 Loss: 0.0770 | 0.0385
Epoch 80/300, seasonal_0 Loss: 0.0769 | 0.0385
Epoch 81/300, seasonal_0 Loss: 0.0769 | 0.0385
Epoch 82/300, seasonal_0 Loss: 0.0769 | 0.0385
Epoch 83/300, seasonal_0 Loss: 0.0769 | 0.0385
Epoch 84/300, seasonal_0 Loss: 0.0768 | 0.0384
Epoch 85/300, seasonal_0 Loss: 0.0768 | 0.0384
Epoch 86/300, seasonal_0 Loss: 0.0768 | 0.0384
Epoch 87/300, seasonal_0 Loss: 0.0768 | 0.0384
Epoch 88/300, seasonal_0 Loss: 0.0767 | 0.0384
Epoch 89/300, seasonal_0 Loss: 0.0767 | 0.0384
Epoch 90/300, seasonal_0 Loss: 0.0767 | 0.0383
Epoch 91/300, seasonal_0 Loss: 0.0767 | 0.0383
Epoch 92/300, seasonal_0 Loss: 0.0767 | 0.0383
Epoch 93/300, seasonal_0 Loss: 0.0767 | 0.0383
Epoch 94/300, seasonal_0 Loss: 0.0766 | 0.0383
Epoch 95/300, seasonal_0 Loss: 0.0766 | 0.0383
Epoch 96/300, seasonal_0 Loss: 0.0766 | 0.0383
Epoch 97/300, seasonal_0 Loss: 0.0766 | 0.0383
Epoch 98/300, seasonal_0 Loss: 0.0766 | 0.0383
Epoch 99/300, seasonal_0 Loss: 0.0766 | 0.0382
Epoch 100/300, seasonal_0 Loss: 0.0766 | 0.0382
Epoch 101/300, seasonal_0 Loss: 0.0766 | 0.0382
Epoch 102/300, seasonal_0 Loss: 0.0766 | 0.0382
Epoch 103/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 104/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 105/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 106/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 107/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 108/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 109/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 110/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 111/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 112/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 113/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 114/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 115/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 116/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 117/300, seasonal_0 Loss: 0.0765 | 0.0382
Epoch 118/300, seasonal_0 Loss: 0.0765 | 0.0381
Epoch 119/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 120/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 121/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 122/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 123/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 124/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 125/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 126/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 127/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 128/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 129/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 130/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 131/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 132/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 133/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 134/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 135/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 136/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 137/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 138/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 139/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 140/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 141/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 142/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 143/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 144/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 145/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 146/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 147/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 148/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 149/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 150/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 151/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 152/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 153/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 154/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 155/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 156/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 157/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 158/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 159/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 160/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 161/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 162/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 163/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 164/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 165/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 166/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 167/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 168/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 169/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 170/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 171/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 172/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 173/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 174/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 175/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 176/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 177/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 178/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 179/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 180/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 181/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 182/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 183/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 184/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 185/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 186/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 187/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 188/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 189/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 190/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 191/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 192/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 193/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 194/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 195/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 196/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 197/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 198/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 199/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 200/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 201/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 202/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 203/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 204/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 205/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 206/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 207/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 208/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 209/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 210/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 211/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 212/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 213/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 214/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 215/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 216/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 217/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 218/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 219/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 220/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 221/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 222/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 223/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 224/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 225/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 226/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 227/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 228/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 229/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 230/300, seasonal_0 Loss: 0.0764 | 0.0381
Epoch 231/300, seasonal_0 Loss: 0.0764 | 0.0381
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.8405577022438272, 'learning_rate': 0.0007176526017094414, 'batch_size': 42, 'step_size': 4, 'gamma': 0.8829341221158721}
Epoch 1/300, seasonal_1 Loss: 0.2185 | 0.0990
Epoch 2/300, seasonal_1 Loss: 0.1115 | 0.0645
Epoch 3/300, seasonal_1 Loss: 0.1029 | 0.0607
Epoch 4/300, seasonal_1 Loss: 0.1006 | 0.0559
Epoch 5/300, seasonal_1 Loss: 0.0945 | 0.0493
Epoch 6/300, seasonal_1 Loss: 0.0911 | 0.0458
Epoch 7/300, seasonal_1 Loss: 0.0872 | 0.0432
Epoch 8/300, seasonal_1 Loss: 0.0855 | 0.0406
Epoch 9/300, seasonal_1 Loss: 0.0832 | 0.0424
Epoch 10/300, seasonal_1 Loss: 0.0808 | 0.0396
Epoch 11/300, seasonal_1 Loss: 0.0793 | 0.0413
Epoch 12/300, seasonal_1 Loss: 0.0782 | 0.0416
Epoch 13/300, seasonal_1 Loss: 0.0765 | 0.0412
Epoch 14/300, seasonal_1 Loss: 0.0756 | 0.0402
Epoch 15/300, seasonal_1 Loss: 0.0746 | 0.0386
Epoch 16/300, seasonal_1 Loss: 0.0739 | 0.0370
Epoch 17/300, seasonal_1 Loss: 0.0724 | 0.0336
Epoch 18/300, seasonal_1 Loss: 0.0710 | 0.0320
Epoch 19/300, seasonal_1 Loss: 0.0700 | 0.0340
Epoch 20/300, seasonal_1 Loss: 0.0694 | 0.0352
Epoch 21/300, seasonal_1 Loss: 0.0684 | 0.0340
Epoch 22/300, seasonal_1 Loss: 0.0680 | 0.0319
Epoch 23/300, seasonal_1 Loss: 0.0671 | 0.0322
Epoch 24/300, seasonal_1 Loss: 0.0671 | 0.0311
Epoch 25/300, seasonal_1 Loss: 0.0662 | 0.0309
Epoch 26/300, seasonal_1 Loss: 0.0655 | 0.0298
Epoch 27/300, seasonal_1 Loss: 0.0647 | 0.0282
Epoch 28/300, seasonal_1 Loss: 0.0641 | 0.0287
Epoch 29/300, seasonal_1 Loss: 0.0637 | 0.0272
Epoch 30/300, seasonal_1 Loss: 0.0633 | 0.0268
Epoch 31/300, seasonal_1 Loss: 0.0632 | 0.0269
Epoch 32/300, seasonal_1 Loss: 0.0632 | 0.0248
Epoch 33/300, seasonal_1 Loss: 0.0625 | 0.0247
Epoch 34/300, seasonal_1 Loss: 0.0622 | 0.0244
Epoch 35/300, seasonal_1 Loss: 0.0619 | 0.0245
Epoch 36/300, seasonal_1 Loss: 0.0618 | 0.0243
Epoch 37/300, seasonal_1 Loss: 0.0614 | 0.0245
Epoch 38/300, seasonal_1 Loss: 0.0613 | 0.0244
Epoch 39/300, seasonal_1 Loss: 0.0611 | 0.0244
Epoch 40/300, seasonal_1 Loss: 0.0610 | 0.0243
Epoch 41/300, seasonal_1 Loss: 0.0610 | 0.0246
Epoch 42/300, seasonal_1 Loss: 0.0610 | 0.0245
Epoch 43/300, seasonal_1 Loss: 0.0609 | 0.0243
Epoch 44/300, seasonal_1 Loss: 0.0608 | 0.0242
Epoch 45/300, seasonal_1 Loss: 0.0606 | 0.0242
Epoch 46/300, seasonal_1 Loss: 0.0604 | 0.0239
Epoch 47/300, seasonal_1 Loss: 0.0602 | 0.0237
Epoch 48/300, seasonal_1 Loss: 0.0600 | 0.0235
Epoch 49/300, seasonal_1 Loss: 0.0599 | 0.0234
Epoch 50/300, seasonal_1 Loss: 0.0598 | 0.0234
Epoch 51/300, seasonal_1 Loss: 0.0596 | 0.0234
Epoch 52/300, seasonal_1 Loss: 0.0595 | 0.0234
Epoch 53/300, seasonal_1 Loss: 0.0594 | 0.0236
Epoch 54/300, seasonal_1 Loss: 0.0593 | 0.0235
Epoch 55/300, seasonal_1 Loss: 0.0592 | 0.0238
Epoch 56/300, seasonal_1 Loss: 0.0591 | 0.0238
Epoch 57/300, seasonal_1 Loss: 0.0590 | 0.0240
Epoch 58/300, seasonal_1 Loss: 0.0589 | 0.0239
Epoch 59/300, seasonal_1 Loss: 0.0588 | 0.0237
Epoch 60/300, seasonal_1 Loss: 0.0588 | 0.0236
Epoch 61/300, seasonal_1 Loss: 0.0587 | 0.0232
Epoch 62/300, seasonal_1 Loss: 0.0586 | 0.0231
Epoch 63/300, seasonal_1 Loss: 0.0586 | 0.0230
Epoch 64/300, seasonal_1 Loss: 0.0586 | 0.0230
Epoch 65/300, seasonal_1 Loss: 0.0585 | 0.0230
Epoch 66/300, seasonal_1 Loss: 0.0585 | 0.0229
Epoch 67/300, seasonal_1 Loss: 0.0583 | 0.0228
Epoch 68/300, seasonal_1 Loss: 0.0582 | 0.0228
Epoch 69/300, seasonal_1 Loss: 0.0581 | 0.0227
Epoch 70/300, seasonal_1 Loss: 0.0581 | 0.0227
Epoch 71/300, seasonal_1 Loss: 0.0580 | 0.0226
Epoch 72/300, seasonal_1 Loss: 0.0580 | 0.0226
Epoch 73/300, seasonal_1 Loss: 0.0579 | 0.0226
Epoch 74/300, seasonal_1 Loss: 0.0579 | 0.0225
Epoch 75/300, seasonal_1 Loss: 0.0578 | 0.0225
Epoch 76/300, seasonal_1 Loss: 0.0578 | 0.0225
Epoch 77/300, seasonal_1 Loss: 0.0577 | 0.0225
Epoch 78/300, seasonal_1 Loss: 0.0577 | 0.0224
Epoch 79/300, seasonal_1 Loss: 0.0577 | 0.0224
Epoch 80/300, seasonal_1 Loss: 0.0577 | 0.0224
Epoch 81/300, seasonal_1 Loss: 0.0576 | 0.0224
Epoch 82/300, seasonal_1 Loss: 0.0576 | 0.0224
Epoch 83/300, seasonal_1 Loss: 0.0576 | 0.0223
Epoch 84/300, seasonal_1 Loss: 0.0576 | 0.0223
Epoch 85/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 86/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 87/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 88/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 89/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 90/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 91/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 92/300, seasonal_1 Loss: 0.0575 | 0.0223
Epoch 93/300, seasonal_1 Loss: 0.0574 | 0.0223
Epoch 94/300, seasonal_1 Loss: 0.0574 | 0.0223
Epoch 95/300, seasonal_1 Loss: 0.0574 | 0.0223
Epoch 96/300, seasonal_1 Loss: 0.0574 | 0.0223
Epoch 97/300, seasonal_1 Loss: 0.0574 | 0.0223
Epoch 98/300, seasonal_1 Loss: 0.0574 | 0.0223
Epoch 99/300, seasonal_1 Loss: 0.0574 | 0.0222
Epoch 100/300, seasonal_1 Loss: 0.0574 | 0.0222
Epoch 101/300, seasonal_1 Loss: 0.0574 | 0.0222
Epoch 102/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 103/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 104/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 105/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 106/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 107/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 108/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 109/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 110/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 111/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 112/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 113/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 114/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 115/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 116/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 117/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 118/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 119/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 120/300, seasonal_1 Loss: 0.0573 | 0.0222
Epoch 121/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 122/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 123/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 124/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 125/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 126/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 127/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 128/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 129/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 130/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 131/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 132/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 133/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 134/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 135/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 136/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 137/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 138/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 139/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 140/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 141/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 142/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 143/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 144/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 145/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 146/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 147/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 148/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 149/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 150/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 151/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 152/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 153/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 154/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 155/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 156/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 157/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 158/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 159/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 160/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 161/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 162/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 163/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 164/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 165/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 166/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 167/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 168/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 169/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 170/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 171/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 172/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 173/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 174/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 175/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 176/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 177/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 178/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 179/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 180/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 181/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 182/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 183/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 184/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 185/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 186/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 187/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 188/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 189/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 190/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 191/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 192/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 193/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 194/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 195/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 196/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 197/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 198/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 199/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 200/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 201/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 202/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 203/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 204/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 205/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 206/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 207/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 208/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 209/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 210/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 211/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 212/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 213/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 214/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 215/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 216/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 217/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 218/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 219/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 220/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 221/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 222/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 223/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 224/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 225/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 226/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 227/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 228/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 229/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 230/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 231/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 232/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 233/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 234/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 235/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 236/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 237/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 238/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 239/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 240/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 241/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 242/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 243/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 244/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 245/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 246/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 247/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 248/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 249/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 250/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 251/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 252/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 253/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 254/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 255/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 256/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 257/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 258/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 259/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 260/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 261/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 262/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 263/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 264/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 265/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 266/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 267/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 268/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 269/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 270/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 271/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 272/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 273/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 274/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 275/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 276/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 277/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 278/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 279/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 280/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 281/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 282/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 283/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 284/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 285/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 286/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 287/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 288/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 289/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 290/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 291/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 292/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 293/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 294/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 295/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 296/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 297/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 298/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 299/300, seasonal_1 Loss: 0.0572 | 0.0222
Epoch 300/300, seasonal_1 Loss: 0.0572 | 0.0222
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.9026006185269891, 'learning_rate': 0.0009955194884617193, 'batch_size': 140, 'step_size': 7, 'gamma': 0.7509599369041723}
Epoch 1/300, seasonal_2 Loss: 0.4754 | 0.1671
Epoch 2/300, seasonal_2 Loss: 0.1566 | 0.2074
Epoch 3/300, seasonal_2 Loss: 0.1194 | 0.1639
Epoch 4/300, seasonal_2 Loss: 0.1126 | 0.1423
Epoch 5/300, seasonal_2 Loss: 0.1044 | 0.1006
Epoch 6/300, seasonal_2 Loss: 0.1039 | 0.0670
Epoch 7/300, seasonal_2 Loss: 0.1062 | 0.0676
Epoch 8/300, seasonal_2 Loss: 0.1096 | 0.0799
Epoch 9/300, seasonal_2 Loss: 0.1115 | 0.0716
Epoch 10/300, seasonal_2 Loss: 0.1051 | 0.0971
Epoch 11/300, seasonal_2 Loss: 0.1030 | 0.0851
Epoch 12/300, seasonal_2 Loss: 0.0941 | 0.0611
Epoch 13/300, seasonal_2 Loss: 0.0983 | 0.0633
Epoch 14/300, seasonal_2 Loss: 0.1031 | 0.0769
Epoch 15/300, seasonal_2 Loss: 0.1023 | 0.0906
Epoch 16/300, seasonal_2 Loss: 0.0971 | 0.0503
Epoch 17/300, seasonal_2 Loss: 0.0859 | 0.0484
Epoch 18/300, seasonal_2 Loss: 0.0833 | 0.0466
Epoch 19/300, seasonal_2 Loss: 0.0808 | 0.0508
Epoch 20/300, seasonal_2 Loss: 0.0795 | 0.0444
Epoch 21/300, seasonal_2 Loss: 0.0788 | 0.0440
Epoch 22/300, seasonal_2 Loss: 0.0778 | 0.0440
Epoch 23/300, seasonal_2 Loss: 0.0781 | 0.0441
Epoch 24/300, seasonal_2 Loss: 0.0777 | 0.0433
Epoch 25/300, seasonal_2 Loss: 0.0773 | 0.0430
Epoch 26/300, seasonal_2 Loss: 0.0769 | 0.0424
Epoch 27/300, seasonal_2 Loss: 0.0765 | 0.0420
Epoch 28/300, seasonal_2 Loss: 0.0759 | 0.0417
Epoch 29/300, seasonal_2 Loss: 0.0756 | 0.0412
Epoch 30/300, seasonal_2 Loss: 0.0757 | 0.0410
Epoch 31/300, seasonal_2 Loss: 0.0752 | 0.0409
Epoch 32/300, seasonal_2 Loss: 0.0746 | 0.0407
Epoch 33/300, seasonal_2 Loss: 0.0742 | 0.0406
Epoch 34/300, seasonal_2 Loss: 0.0739 | 0.0403
Epoch 35/300, seasonal_2 Loss: 0.0737 | 0.0401
Epoch 36/300, seasonal_2 Loss: 0.0735 | 0.0400
Epoch 37/300, seasonal_2 Loss: 0.0734 | 0.0398
Epoch 38/300, seasonal_2 Loss: 0.0732 | 0.0396
Epoch 39/300, seasonal_2 Loss: 0.0731 | 0.0394
Epoch 40/300, seasonal_2 Loss: 0.0730 | 0.0393
Epoch 41/300, seasonal_2 Loss: 0.0728 | 0.0392
Epoch 42/300, seasonal_2 Loss: 0.0727 | 0.0391
Epoch 43/300, seasonal_2 Loss: 0.0726 | 0.0390
Epoch 44/300, seasonal_2 Loss: 0.0726 | 0.0389
Epoch 45/300, seasonal_2 Loss: 0.0725 | 0.0388
Epoch 46/300, seasonal_2 Loss: 0.0724 | 0.0387
Epoch 47/300, seasonal_2 Loss: 0.0723 | 0.0386
Epoch 48/300, seasonal_2 Loss: 0.0723 | 0.0385
Epoch 49/300, seasonal_2 Loss: 0.0722 | 0.0384
Epoch 50/300, seasonal_2 Loss: 0.0721 | 0.0384
Epoch 51/300, seasonal_2 Loss: 0.0721 | 0.0383
Epoch 52/300, seasonal_2 Loss: 0.0720 | 0.0383
Epoch 53/300, seasonal_2 Loss: 0.0720 | 0.0382
Epoch 54/300, seasonal_2 Loss: 0.0719 | 0.0382
Epoch 55/300, seasonal_2 Loss: 0.0719 | 0.0381
Epoch 56/300, seasonal_2 Loss: 0.0719 | 0.0381
Epoch 57/300, seasonal_2 Loss: 0.0718 | 0.0380
Epoch 58/300, seasonal_2 Loss: 0.0718 | 0.0380
Epoch 59/300, seasonal_2 Loss: 0.0718 | 0.0380
Epoch 60/300, seasonal_2 Loss: 0.0717 | 0.0379
Epoch 61/300, seasonal_2 Loss: 0.0717 | 0.0379
Epoch 62/300, seasonal_2 Loss: 0.0717 | 0.0379
Epoch 63/300, seasonal_2 Loss: 0.0717 | 0.0379
Epoch 64/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 65/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 66/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 67/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 68/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 69/300, seasonal_2 Loss: 0.0716 | 0.0378
Epoch 70/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 71/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 72/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 73/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 74/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 75/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 76/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 77/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 78/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 79/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 80/300, seasonal_2 Loss: 0.0715 | 0.0376
Epoch 81/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 82/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 83/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 84/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 85/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 86/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 87/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 88/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 89/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 90/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 91/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 92/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 93/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 94/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 95/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 96/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 97/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 98/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 99/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 100/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 101/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 102/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 103/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 104/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 105/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 106/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 107/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 108/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 109/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 110/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 111/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 112/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 113/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 114/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 115/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 116/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 117/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 118/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 119/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 120/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 121/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 122/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 123/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 124/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 125/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 126/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 127/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 128/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 129/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 130/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 131/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 132/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 133/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 134/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 135/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 136/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 137/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 138/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 139/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 140/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 141/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 142/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 143/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 144/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 145/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 146/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 147/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 148/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 149/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 150/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 151/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 152/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 153/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 154/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 155/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 156/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 157/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 158/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 159/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 160/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 161/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 162/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 163/300, seasonal_2 Loss: 0.0714 | 0.0376
Epoch 164/300, seasonal_2 Loss: 0.0714 | 0.0376
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 9, 'train_rates': 0.8708618541456006, 'learning_rate': 0.00030616573778462805, 'batch_size': 28, 'step_size': 13, 'gamma': 0.811757314516297}
Epoch 1/300, seasonal_3 Loss: 0.1783 | 0.0735
Epoch 2/300, seasonal_3 Loss: 0.1111 | 0.0629
Epoch 3/300, seasonal_3 Loss: 0.1026 | 0.0615
Epoch 4/300, seasonal_3 Loss: 0.0957 | 0.0555
Epoch 5/300, seasonal_3 Loss: 0.0896 | 0.0481
Epoch 6/300, seasonal_3 Loss: 0.0850 | 0.0467
Epoch 7/300, seasonal_3 Loss: 0.0820 | 0.0444
Epoch 8/300, seasonal_3 Loss: 0.0775 | 0.0390
Epoch 9/300, seasonal_3 Loss: 0.0746 | 0.0375
Epoch 10/300, seasonal_3 Loss: 0.0729 | 0.0379
Epoch 11/300, seasonal_3 Loss: 0.0714 | 0.0382
Epoch 12/300, seasonal_3 Loss: 0.0705 | 0.0381
Epoch 13/300, seasonal_3 Loss: 0.0701 | 0.0361
Epoch 14/300, seasonal_3 Loss: 0.0683 | 0.0349
Epoch 15/300, seasonal_3 Loss: 0.0679 | 0.0343
Epoch 16/300, seasonal_3 Loss: 0.0662 | 0.0322
Epoch 17/300, seasonal_3 Loss: 0.0661 | 0.0298
Epoch 18/300, seasonal_3 Loss: 0.0635 | 0.0271
Epoch 19/300, seasonal_3 Loss: 0.0636 | 0.0274
Epoch 20/300, seasonal_3 Loss: 0.0619 | 0.0276
Epoch 21/300, seasonal_3 Loss: 0.0618 | 0.0252
Epoch 22/300, seasonal_3 Loss: 0.0612 | 0.0254
Epoch 23/300, seasonal_3 Loss: 0.0605 | 0.0244
Epoch 24/300, seasonal_3 Loss: 0.0598 | 0.0252
Epoch 25/300, seasonal_3 Loss: 0.0599 | 0.0255
Epoch 26/300, seasonal_3 Loss: 0.0582 | 0.0261
Epoch 27/300, seasonal_3 Loss: 0.0591 | 0.0239
Epoch 28/300, seasonal_3 Loss: 0.0600 | 0.0260
Epoch 29/300, seasonal_3 Loss: 0.0568 | 0.0247
Epoch 30/300, seasonal_3 Loss: 0.0574 | 0.0223
Epoch 31/300, seasonal_3 Loss: 0.0557 | 0.0228
Epoch 32/300, seasonal_3 Loss: 0.0560 | 0.0237
Epoch 33/300, seasonal_3 Loss: 0.0556 | 0.0231
Epoch 34/300, seasonal_3 Loss: 0.0551 | 0.0223
Epoch 35/300, seasonal_3 Loss: 0.0550 | 0.0229
Epoch 36/300, seasonal_3 Loss: 0.0543 | 0.0227
Epoch 37/300, seasonal_3 Loss: 0.0548 | 0.0235
Epoch 38/300, seasonal_3 Loss: 0.0544 | 0.0232
Epoch 39/300, seasonal_3 Loss: 0.0544 | 0.0242
Epoch 40/300, seasonal_3 Loss: 0.0543 | 0.0235
Epoch 41/300, seasonal_3 Loss: 0.0536 | 0.0221
Epoch 42/300, seasonal_3 Loss: 0.0530 | 0.0218
Epoch 43/300, seasonal_3 Loss: 0.0528 | 0.0219
Epoch 44/300, seasonal_3 Loss: 0.0527 | 0.0230
Epoch 45/300, seasonal_3 Loss: 0.0526 | 0.0229
Epoch 46/300, seasonal_3 Loss: 0.0525 | 0.0236
Epoch 47/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 48/300, seasonal_3 Loss: 0.0521 | 0.0237
Epoch 49/300, seasonal_3 Loss: 0.0519 | 0.0224
Epoch 50/300, seasonal_3 Loss: 0.0530 | 0.0217
Epoch 51/300, seasonal_3 Loss: 0.0516 | 0.0216
Epoch 52/300, seasonal_3 Loss: 0.0513 | 0.0212
Epoch 53/300, seasonal_3 Loss: 0.0512 | 0.0240
Epoch 54/300, seasonal_3 Loss: 0.0512 | 0.0249
Epoch 55/300, seasonal_3 Loss: 0.0514 | 0.0243
Epoch 56/300, seasonal_3 Loss: 0.0511 | 0.0239
Epoch 57/300, seasonal_3 Loss: 0.0508 | 0.0237
Epoch 58/300, seasonal_3 Loss: 0.0507 | 0.0232
Epoch 59/300, seasonal_3 Loss: 0.0506 | 0.0228
Epoch 60/300, seasonal_3 Loss: 0.0512 | 0.0295
Epoch 61/300, seasonal_3 Loss: 0.0518 | 0.0351
Epoch 62/300, seasonal_3 Loss: 0.0509 | 0.0222
Epoch 63/300, seasonal_3 Loss: 0.0506 | 0.0215
Epoch 64/300, seasonal_3 Loss: 0.0501 | 0.0209
Epoch 65/300, seasonal_3 Loss: 0.0498 | 0.0206
Epoch 66/300, seasonal_3 Loss: 0.0496 | 0.0210
Epoch 67/300, seasonal_3 Loss: 0.0496 | 0.0213
Epoch 68/300, seasonal_3 Loss: 0.0494 | 0.0206
Epoch 69/300, seasonal_3 Loss: 0.0492 | 0.0202
Epoch 70/300, seasonal_3 Loss: 0.0490 | 0.0203
Epoch 71/300, seasonal_3 Loss: 0.0492 | 0.0229
Epoch 72/300, seasonal_3 Loss: 0.0497 | 0.0271
Epoch 73/300, seasonal_3 Loss: 0.0493 | 0.0210
Epoch 74/300, seasonal_3 Loss: 0.0489 | 0.0204
Epoch 75/300, seasonal_3 Loss: 0.0486 | 0.0205
Epoch 76/300, seasonal_3 Loss: 0.0486 | 0.0204
Epoch 77/300, seasonal_3 Loss: 0.0485 | 0.0204
Epoch 78/300, seasonal_3 Loss: 0.0484 | 0.0204
Epoch 79/300, seasonal_3 Loss: 0.0483 | 0.0203
Epoch 80/300, seasonal_3 Loss: 0.0483 | 0.0203
Epoch 81/300, seasonal_3 Loss: 0.0482 | 0.0204
Epoch 82/300, seasonal_3 Loss: 0.0481 | 0.0204
Epoch 83/300, seasonal_3 Loss: 0.0480 | 0.0204
Epoch 84/300, seasonal_3 Loss: 0.0479 | 0.0204
Epoch 85/300, seasonal_3 Loss: 0.0478 | 0.0204
Epoch 86/300, seasonal_3 Loss: 0.0478 | 0.0207
Epoch 87/300, seasonal_3 Loss: 0.0477 | 0.0206
Epoch 88/300, seasonal_3 Loss: 0.0477 | 0.0208
Epoch 89/300, seasonal_3 Loss: 0.0476 | 0.0206
Epoch 90/300, seasonal_3 Loss: 0.0475 | 0.0209
Epoch 91/300, seasonal_3 Loss: 0.0475 | 0.0207
Epoch 92/300, seasonal_3 Loss: 0.0474 | 0.0212
Epoch 93/300, seasonal_3 Loss: 0.0474 | 0.0209
Epoch 94/300, seasonal_3 Loss: 0.0472 | 0.0213
Epoch 95/300, seasonal_3 Loss: 0.0470 | 0.0210
Epoch 96/300, seasonal_3 Loss: 0.0468 | 0.0214
Epoch 97/300, seasonal_3 Loss: 0.0465 | 0.0212
Epoch 98/300, seasonal_3 Loss: 0.0464 | 0.0215
Epoch 99/300, seasonal_3 Loss: 0.0459 | 0.0218
Epoch 100/300, seasonal_3 Loss: 0.0449 | 0.0219
Epoch 101/300, seasonal_3 Loss: 0.0445 | 0.0217
Epoch 102/300, seasonal_3 Loss: 0.0449 | 0.0219
Epoch 103/300, seasonal_3 Loss: 0.0444 | 0.0215
Epoch 104/300, seasonal_3 Loss: 0.0438 | 0.0215
Epoch 105/300, seasonal_3 Loss: 0.0433 | 0.0216
Epoch 106/300, seasonal_3 Loss: 0.0430 | 0.0215
Epoch 107/300, seasonal_3 Loss: 0.0440 | 0.0220
Epoch 108/300, seasonal_3 Loss: 0.0437 | 0.0215
Epoch 109/300, seasonal_3 Loss: 0.0434 | 0.0228
Epoch 110/300, seasonal_3 Loss: 0.0439 | 0.0216
Epoch 111/300, seasonal_3 Loss: 0.0425 | 0.0214
Epoch 112/300, seasonal_3 Loss: 0.0444 | 0.0230
Epoch 113/300, seasonal_3 Loss: 0.0436 | 0.0219
Epoch 114/300, seasonal_3 Loss: 0.0425 | 0.0217
Epoch 115/300, seasonal_3 Loss: 0.0422 | 0.0218
Epoch 116/300, seasonal_3 Loss: 0.0421 | 0.0215
Epoch 117/300, seasonal_3 Loss: 0.0420 | 0.0224
Epoch 118/300, seasonal_3 Loss: 0.0425 | 0.0216
Epoch 119/300, seasonal_3 Loss: 0.0413 | 0.0215
Epoch 120/300, seasonal_3 Loss: 0.0412 | 0.0214
Epoch 121/300, seasonal_3 Loss: 0.0414 | 0.0224
Epoch 122/300, seasonal_3 Loss: 0.0424 | 0.0215
Epoch 123/300, seasonal_3 Loss: 0.0410 | 0.0214
Epoch 124/300, seasonal_3 Loss: 0.0408 | 0.0217
Epoch 125/300, seasonal_3 Loss: 0.0412 | 0.0214
Epoch 126/300, seasonal_3 Loss: 0.0418 | 0.0227
Epoch 127/300, seasonal_3 Loss: 0.0422 | 0.0217
Epoch 128/300, seasonal_3 Loss: 0.0410 | 0.0214
Epoch 129/300, seasonal_3 Loss: 0.0406 | 0.0216
Epoch 130/300, seasonal_3 Loss: 0.0405 | 0.0214
Epoch 131/300, seasonal_3 Loss: 0.0403 | 0.0217
Epoch 132/300, seasonal_3 Loss: 0.0405 | 0.0215
Epoch 133/300, seasonal_3 Loss: 0.0402 | 0.0218
Epoch 134/300, seasonal_3 Loss: 0.0405 | 0.0215
Epoch 135/300, seasonal_3 Loss: 0.0400 | 0.0220
Epoch 136/300, seasonal_3 Loss: 0.0404 | 0.0215
Epoch 137/300, seasonal_3 Loss: 0.0398 | 0.0220
Epoch 138/300, seasonal_3 Loss: 0.0402 | 0.0216
Epoch 139/300, seasonal_3 Loss: 0.0396 | 0.0218
Epoch 140/300, seasonal_3 Loss: 0.0397 | 0.0216
Epoch 141/300, seasonal_3 Loss: 0.0396 | 0.0222
Epoch 142/300, seasonal_3 Loss: 0.0400 | 0.0217
Epoch 143/300, seasonal_3 Loss: 0.0392 | 0.0219
Epoch 144/300, seasonal_3 Loss: 0.0393 | 0.0217
Epoch 145/300, seasonal_3 Loss: 0.0392 | 0.0221
Epoch 146/300, seasonal_3 Loss: 0.0394 | 0.0217
Epoch 147/300, seasonal_3 Loss: 0.0391 | 0.0222
Epoch 148/300, seasonal_3 Loss: 0.0393 | 0.0217
Epoch 149/300, seasonal_3 Loss: 0.0389 | 0.0222
Epoch 150/300, seasonal_3 Loss: 0.0391 | 0.0218
Epoch 151/300, seasonal_3 Loss: 0.0388 | 0.0221
Epoch 152/300, seasonal_3 Loss: 0.0389 | 0.0218
Epoch 153/300, seasonal_3 Loss: 0.0386 | 0.0220
Epoch 154/300, seasonal_3 Loss: 0.0386 | 0.0218
Epoch 155/300, seasonal_3 Loss: 0.0386 | 0.0221
Epoch 156/300, seasonal_3 Loss: 0.0387 | 0.0218
Epoch 157/300, seasonal_3 Loss: 0.0385 | 0.0220
Epoch 158/300, seasonal_3 Loss: 0.0384 | 0.0218
Epoch 159/300, seasonal_3 Loss: 0.0383 | 0.0219
Epoch 160/300, seasonal_3 Loss: 0.0383 | 0.0218
Epoch 161/300, seasonal_3 Loss: 0.0383 | 0.0219
Epoch 162/300, seasonal_3 Loss: 0.0382 | 0.0218
Epoch 163/300, seasonal_3 Loss: 0.0383 | 0.0219
Epoch 164/300, seasonal_3 Loss: 0.0382 | 0.0217
Epoch 165/300, seasonal_3 Loss: 0.0380 | 0.0218
Epoch 166/300, seasonal_3 Loss: 0.0380 | 0.0218
Epoch 167/300, seasonal_3 Loss: 0.0380 | 0.0218
Epoch 168/300, seasonal_3 Loss: 0.0379 | 0.0218
Epoch 169/300, seasonal_3 Loss: 0.0380 | 0.0218
Epoch 170/300, seasonal_3 Loss: 0.0379 | 0.0217
Epoch 171/300, seasonal_3 Loss: 0.0379 | 0.0218
Epoch 172/300, seasonal_3 Loss: 0.0378 | 0.0217
Epoch 173/300, seasonal_3 Loss: 0.0378 | 0.0218
Epoch 174/300, seasonal_3 Loss: 0.0377 | 0.0217
Epoch 175/300, seasonal_3 Loss: 0.0378 | 0.0218
Epoch 176/300, seasonal_3 Loss: 0.0377 | 0.0217
Epoch 177/300, seasonal_3 Loss: 0.0377 | 0.0218
Epoch 178/300, seasonal_3 Loss: 0.0376 | 0.0217
Epoch 179/300, seasonal_3 Loss: 0.0376 | 0.0218
Epoch 180/300, seasonal_3 Loss: 0.0376 | 0.0217
Epoch 181/300, seasonal_3 Loss: 0.0376 | 0.0218
Epoch 182/300, seasonal_3 Loss: 0.0375 | 0.0217
Epoch 183/300, seasonal_3 Loss: 0.0375 | 0.0218
Epoch 184/300, seasonal_3 Loss: 0.0375 | 0.0217
Epoch 185/300, seasonal_3 Loss: 0.0375 | 0.0218
Epoch 186/300, seasonal_3 Loss: 0.0375 | 0.0217
Epoch 187/300, seasonal_3 Loss: 0.0375 | 0.0218
Epoch 188/300, seasonal_3 Loss: 0.0374 | 0.0217
Epoch 189/300, seasonal_3 Loss: 0.0374 | 0.0218
Epoch 190/300, seasonal_3 Loss: 0.0374 | 0.0217
Epoch 191/300, seasonal_3 Loss: 0.0374 | 0.0218
Epoch 192/300, seasonal_3 Loss: 0.0374 | 0.0217
Epoch 193/300, seasonal_3 Loss: 0.0374 | 0.0218
Epoch 194/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 195/300, seasonal_3 Loss: 0.0373 | 0.0218
Epoch 196/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 197/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 198/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 199/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 200/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 201/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 202/300, seasonal_3 Loss: 0.0373 | 0.0217
Epoch 203/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 204/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 205/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 206/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 207/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 208/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 209/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 210/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 211/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 212/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 213/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 214/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 215/300, seasonal_3 Loss: 0.0372 | 0.0217
Epoch 216/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 217/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 218/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 219/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 220/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 221/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 222/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 223/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 224/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 225/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 226/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 227/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 228/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 229/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 230/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 231/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 232/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 233/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 234/300, seasonal_3 Loss: 0.0371 | 0.0217
Epoch 235/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 236/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 237/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 238/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 239/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 240/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 241/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 242/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 243/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 244/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 245/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 246/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 247/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 248/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 249/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 250/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 251/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 252/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 253/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 254/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 255/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 256/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 257/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 258/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 259/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 260/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 261/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 262/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 263/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 264/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 265/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 266/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 267/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 268/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 269/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 270/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 271/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 272/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 273/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 274/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 275/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 276/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 277/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 278/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 279/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 280/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 281/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 282/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 283/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 284/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 285/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 286/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 287/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 288/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 289/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 290/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 291/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 292/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 293/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 294/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 295/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 296/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 297/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 298/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 299/300, seasonal_3 Loss: 0.0370 | 0.0217
Epoch 300/300, seasonal_3 Loss: 0.0370 | 0.0217
Training resid component with params: {'observation_period_num': 13, 'train_rates': 0.8822491081365356, 'learning_rate': 0.0005717771653074142, 'batch_size': 122, 'step_size': 8, 'gamma': 0.7856123044786605}
Epoch 1/300, resid Loss: 0.3447 | 0.2105
Epoch 2/300, resid Loss: 0.1547 | 0.1227
Epoch 3/300, resid Loss: 0.1375 | 0.0890
Epoch 4/300, resid Loss: 0.1434 | 0.1456
Epoch 5/300, resid Loss: 0.1764 | 0.4884
Epoch 6/300, resid Loss: 0.1622 | 0.1248
Epoch 7/300, resid Loss: 0.1225 | 0.1298
Epoch 8/300, resid Loss: 0.1369 | 0.0833
Epoch 9/300, resid Loss: 0.1129 | 0.0797
Epoch 10/300, resid Loss: 0.1068 | 0.0716
Epoch 11/300, resid Loss: 0.1019 | 0.0640
Epoch 12/300, resid Loss: 0.0999 | 0.0690
Epoch 13/300, resid Loss: 0.0973 | 0.0602
Epoch 14/300, resid Loss: 0.0926 | 0.0552
Epoch 15/300, resid Loss: 0.0899 | 0.0544
Epoch 16/300, resid Loss: 0.0900 | 0.0575
Epoch 17/300, resid Loss: 0.0897 | 0.0539
Epoch 18/300, resid Loss: 0.0873 | 0.0502
Epoch 19/300, resid Loss: 0.0850 | 0.0503
Epoch 20/300, resid Loss: 0.0838 | 0.0493
Epoch 21/300, resid Loss: 0.0828 | 0.0484
Epoch 22/300, resid Loss: 0.0815 | 0.0468
Epoch 23/300, resid Loss: 0.0803 | 0.0450
Epoch 24/300, resid Loss: 0.0790 | 0.0432
Epoch 25/300, resid Loss: 0.0774 | 0.0418
Epoch 26/300, resid Loss: 0.0763 | 0.0406
Epoch 27/300, resid Loss: 0.0754 | 0.0398
Epoch 28/300, resid Loss: 0.0748 | 0.0392
Epoch 29/300, resid Loss: 0.0742 | 0.0388
Epoch 30/300, resid Loss: 0.0739 | 0.0383
Epoch 31/300, resid Loss: 0.0735 | 0.0379
Epoch 32/300, resid Loss: 0.0731 | 0.0375
Epoch 33/300, resid Loss: 0.0728 | 0.0371
Epoch 34/300, resid Loss: 0.0726 | 0.0368
Epoch 35/300, resid Loss: 0.0723 | 0.0365
Epoch 36/300, resid Loss: 0.0718 | 0.0361
Epoch 37/300, resid Loss: 0.0714 | 0.0357
Epoch 38/300, resid Loss: 0.0711 | 0.0355
Epoch 39/300, resid Loss: 0.0710 | 0.0354
Epoch 40/300, resid Loss: 0.0710 | 0.0353
Epoch 41/300, resid Loss: 0.0711 | 0.0352
Epoch 42/300, resid Loss: 0.0713 | 0.0353
Epoch 43/300, resid Loss: 0.0716 | 0.0353
Epoch 44/300, resid Loss: 0.0719 | 0.0352
Epoch 45/300, resid Loss: 0.0726 | 0.0353
Epoch 46/300, resid Loss: 0.0725 | 0.0355
Epoch 47/300, resid Loss: 0.0712 | 0.0361
Epoch 48/300, resid Loss: 0.0708 | 0.0367
Epoch 49/300, resid Loss: 0.0713 | 0.0360
Epoch 50/300, resid Loss: 0.0707 | 0.0352
Epoch 51/300, resid Loss: 0.0696 | 0.0349
Epoch 52/300, resid Loss: 0.0691 | 0.0348
Epoch 53/300, resid Loss: 0.0690 | 0.0348
Epoch 54/300, resid Loss: 0.0688 | 0.0347
Epoch 55/300, resid Loss: 0.0688 | 0.0346
Epoch 56/300, resid Loss: 0.0687 | 0.0346
Epoch 57/300, resid Loss: 0.0686 | 0.0345
Epoch 58/300, resid Loss: 0.0685 | 0.0344
Epoch 59/300, resid Loss: 0.0684 | 0.0343
Epoch 60/300, resid Loss: 0.0684 | 0.0343
Epoch 61/300, resid Loss: 0.0683 | 0.0342
Epoch 62/300, resid Loss: 0.0682 | 0.0342
Epoch 63/300, resid Loss: 0.0682 | 0.0341
Epoch 64/300, resid Loss: 0.0681 | 0.0341
Epoch 65/300, resid Loss: 0.0681 | 0.0341
Epoch 66/300, resid Loss: 0.0680 | 0.0340
Epoch 67/300, resid Loss: 0.0680 | 0.0340
Epoch 68/300, resid Loss: 0.0679 | 0.0340
Epoch 69/300, resid Loss: 0.0679 | 0.0339
Epoch 70/300, resid Loss: 0.0679 | 0.0339
Epoch 71/300, resid Loss: 0.0678 | 0.0339
Epoch 72/300, resid Loss: 0.0678 | 0.0339
Epoch 73/300, resid Loss: 0.0677 | 0.0338
Epoch 74/300, resid Loss: 0.0677 | 0.0338
Epoch 75/300, resid Loss: 0.0677 | 0.0338
Epoch 76/300, resid Loss: 0.0677 | 0.0338
Epoch 77/300, resid Loss: 0.0676 | 0.0337
Epoch 78/300, resid Loss: 0.0676 | 0.0337
Epoch 79/300, resid Loss: 0.0676 | 0.0337
Epoch 80/300, resid Loss: 0.0676 | 0.0337
Epoch 81/300, resid Loss: 0.0675 | 0.0337
Epoch 82/300, resid Loss: 0.0675 | 0.0337
Epoch 83/300, resid Loss: 0.0675 | 0.0337
Epoch 84/300, resid Loss: 0.0675 | 0.0336
Epoch 85/300, resid Loss: 0.0675 | 0.0336
Epoch 86/300, resid Loss: 0.0675 | 0.0336
Epoch 87/300, resid Loss: 0.0674 | 0.0336
Epoch 88/300, resid Loss: 0.0674 | 0.0336
Epoch 89/300, resid Loss: 0.0674 | 0.0336
Epoch 90/300, resid Loss: 0.0674 | 0.0336
Epoch 91/300, resid Loss: 0.0674 | 0.0336
Epoch 92/300, resid Loss: 0.0674 | 0.0336
Epoch 93/300, resid Loss: 0.0674 | 0.0336
Epoch 94/300, resid Loss: 0.0674 | 0.0336
Epoch 95/300, resid Loss: 0.0673 | 0.0335
Epoch 96/300, resid Loss: 0.0673 | 0.0335
Epoch 97/300, resid Loss: 0.0673 | 0.0335
Epoch 98/300, resid Loss: 0.0673 | 0.0335
Epoch 99/300, resid Loss: 0.0673 | 0.0335
Epoch 100/300, resid Loss: 0.0673 | 0.0335
Epoch 101/300, resid Loss: 0.0673 | 0.0335
Epoch 102/300, resid Loss: 0.0673 | 0.0335
Epoch 103/300, resid Loss: 0.0673 | 0.0335
Epoch 104/300, resid Loss: 0.0673 | 0.0335
Epoch 105/300, resid Loss: 0.0673 | 0.0335
Epoch 106/300, resid Loss: 0.0673 | 0.0335
Epoch 107/300, resid Loss: 0.0673 | 0.0335
Epoch 108/300, resid Loss: 0.0672 | 0.0335
Epoch 109/300, resid Loss: 0.0672 | 0.0335
Epoch 110/300, resid Loss: 0.0672 | 0.0335
Epoch 111/300, resid Loss: 0.0672 | 0.0335
Epoch 112/300, resid Loss: 0.0672 | 0.0335
Epoch 113/300, resid Loss: 0.0672 | 0.0335
Epoch 114/300, resid Loss: 0.0672 | 0.0335
Epoch 115/300, resid Loss: 0.0672 | 0.0335
Epoch 116/300, resid Loss: 0.0672 | 0.0335
Epoch 117/300, resid Loss: 0.0672 | 0.0335
Epoch 118/300, resid Loss: 0.0672 | 0.0335
Epoch 119/300, resid Loss: 0.0672 | 0.0335
Epoch 120/300, resid Loss: 0.0672 | 0.0335
Epoch 121/300, resid Loss: 0.0672 | 0.0335
Epoch 122/300, resid Loss: 0.0672 | 0.0335
Epoch 123/300, resid Loss: 0.0672 | 0.0334
Epoch 124/300, resid Loss: 0.0672 | 0.0334
Epoch 125/300, resid Loss: 0.0672 | 0.0334
Epoch 126/300, resid Loss: 0.0672 | 0.0334
Epoch 127/300, resid Loss: 0.0672 | 0.0334
Epoch 128/300, resid Loss: 0.0672 | 0.0334
Epoch 129/300, resid Loss: 0.0672 | 0.0334
Epoch 130/300, resid Loss: 0.0672 | 0.0334
Epoch 131/300, resid Loss: 0.0672 | 0.0334
Epoch 132/300, resid Loss: 0.0672 | 0.0334
Epoch 133/300, resid Loss: 0.0672 | 0.0334
Epoch 134/300, resid Loss: 0.0672 | 0.0334
Epoch 135/300, resid Loss: 0.0672 | 0.0334
Epoch 136/300, resid Loss: 0.0672 | 0.0334
Epoch 137/300, resid Loss: 0.0672 | 0.0334
Epoch 138/300, resid Loss: 0.0672 | 0.0334
Epoch 139/300, resid Loss: 0.0672 | 0.0334
Epoch 140/300, resid Loss: 0.0672 | 0.0334
Epoch 141/300, resid Loss: 0.0672 | 0.0334
Epoch 142/300, resid Loss: 0.0672 | 0.0334
Epoch 143/300, resid Loss: 0.0672 | 0.0334
Epoch 144/300, resid Loss: 0.0672 | 0.0334
Epoch 145/300, resid Loss: 0.0672 | 0.0334
Epoch 146/300, resid Loss: 0.0672 | 0.0334
Epoch 147/300, resid Loss: 0.0672 | 0.0334
Epoch 148/300, resid Loss: 0.0672 | 0.0334
Epoch 149/300, resid Loss: 0.0672 | 0.0334
Epoch 150/300, resid Loss: 0.0672 | 0.0334
Epoch 151/300, resid Loss: 0.0672 | 0.0334
Epoch 152/300, resid Loss: 0.0672 | 0.0334
Epoch 153/300, resid Loss: 0.0672 | 0.0334
Epoch 154/300, resid Loss: 0.0672 | 0.0334
Epoch 155/300, resid Loss: 0.0672 | 0.0334
Epoch 156/300, resid Loss: 0.0672 | 0.0334
Epoch 157/300, resid Loss: 0.0672 | 0.0334
Epoch 158/300, resid Loss: 0.0672 | 0.0334
Epoch 159/300, resid Loss: 0.0672 | 0.0334
Epoch 160/300, resid Loss: 0.0672 | 0.0334
Epoch 161/300, resid Loss: 0.0672 | 0.0334
Epoch 162/300, resid Loss: 0.0672 | 0.0334
Epoch 163/300, resid Loss: 0.0672 | 0.0334
Epoch 164/300, resid Loss: 0.0672 | 0.0334
Epoch 165/300, resid Loss: 0.0672 | 0.0334
Epoch 166/300, resid Loss: 0.0672 | 0.0334
Epoch 167/300, resid Loss: 0.0672 | 0.0334
Epoch 168/300, resid Loss: 0.0672 | 0.0334
Epoch 169/300, resid Loss: 0.0672 | 0.0334
Epoch 170/300, resid Loss: 0.0672 | 0.0334
Epoch 171/300, resid Loss: 0.0672 | 0.0334
Epoch 172/300, resid Loss: 0.0672 | 0.0334
Epoch 173/300, resid Loss: 0.0672 | 0.0334
Epoch 174/300, resid Loss: 0.0672 | 0.0334
Epoch 175/300, resid Loss: 0.0672 | 0.0334
Epoch 176/300, resid Loss: 0.0672 | 0.0334
Epoch 177/300, resid Loss: 0.0672 | 0.0334
Epoch 178/300, resid Loss: 0.0672 | 0.0334
Epoch 179/300, resid Loss: 0.0672 | 0.0334
Epoch 180/300, resid Loss: 0.0672 | 0.0334
Epoch 181/300, resid Loss: 0.0672 | 0.0334
Epoch 182/300, resid Loss: 0.0672 | 0.0334
Epoch 183/300, resid Loss: 0.0672 | 0.0334
Epoch 184/300, resid Loss: 0.0672 | 0.0334
Epoch 185/300, resid Loss: 0.0672 | 0.0334
Epoch 186/300, resid Loss: 0.0672 | 0.0334
Epoch 187/300, resid Loss: 0.0672 | 0.0334
Epoch 188/300, resid Loss: 0.0672 | 0.0334
Epoch 189/300, resid Loss: 0.0672 | 0.0334
Epoch 190/300, resid Loss: 0.0672 | 0.0334
Epoch 191/300, resid Loss: 0.0672 | 0.0334
Epoch 192/300, resid Loss: 0.0672 | 0.0334
Epoch 193/300, resid Loss: 0.0672 | 0.0334
Epoch 194/300, resid Loss: 0.0672 | 0.0334
Epoch 195/300, resid Loss: 0.0672 | 0.0334
Epoch 196/300, resid Loss: 0.0672 | 0.0334
Epoch 197/300, resid Loss: 0.0672 | 0.0334
Epoch 198/300, resid Loss: 0.0672 | 0.0334
Epoch 199/300, resid Loss: 0.0672 | 0.0334
Epoch 200/300, resid Loss: 0.0672 | 0.0334
Epoch 201/300, resid Loss: 0.0672 | 0.0334
Epoch 202/300, resid Loss: 0.0672 | 0.0334
Epoch 203/300, resid Loss: 0.0672 | 0.0334
Epoch 204/300, resid Loss: 0.0672 | 0.0334
Epoch 205/300, resid Loss: 0.0672 | 0.0334
Epoch 206/300, resid Loss: 0.0672 | 0.0334
Epoch 207/300, resid Loss: 0.0672 | 0.0334
Epoch 208/300, resid Loss: 0.0672 | 0.0334
Epoch 209/300, resid Loss: 0.0672 | 0.0334
Epoch 210/300, resid Loss: 0.0672 | 0.0334
Epoch 211/300, resid Loss: 0.0672 | 0.0334
Epoch 212/300, resid Loss: 0.0672 | 0.0334
Epoch 213/300, resid Loss: 0.0672 | 0.0334
Epoch 214/300, resid Loss: 0.0672 | 0.0334
Epoch 215/300, resid Loss: 0.0672 | 0.0334
Epoch 216/300, resid Loss: 0.0672 | 0.0334
Epoch 217/300, resid Loss: 0.0672 | 0.0334
Epoch 218/300, resid Loss: 0.0672 | 0.0334
Epoch 219/300, resid Loss: 0.0672 | 0.0334
Epoch 220/300, resid Loss: 0.0672 | 0.0334
Epoch 221/300, resid Loss: 0.0672 | 0.0334
Epoch 222/300, resid Loss: 0.0672 | 0.0334
Epoch 223/300, resid Loss: 0.0672 | 0.0334
Epoch 224/300, resid Loss: 0.0672 | 0.0334
Epoch 225/300, resid Loss: 0.0672 | 0.0334
Epoch 226/300, resid Loss: 0.0672 | 0.0334
Early stopping for resid
Runtime (seconds): 1419.0774366855621
0.00021076009161061788
[210.78601]
[1.7951565]
[0.3904234]
[5.1736765]
[-0.56744707]
[6.0155554]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 131.48358127311803
RMSE: 11.466629028320312
MAE: 11.466629028320312
R-squared: nan
[223.59337]
