[32m[I 2025-01-04 20:31:51,199][0m A new study created in memory with name: no-name-3584c479-7a2a-4f7d-8a88-2d4d48c4654c[0m
[32m[I 2025-01-04 20:33:18,344][0m Trial 0 finished with value: 0.15965909055935648 and parameters: {'observation_period_num': 231, 'train_rates': 0.7708130053169203, 'learning_rate': 2.8989221454338726e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8055408027541686}. Best is trial 0 with value: 0.15965909055935648.[0m
[32m[I 2025-01-04 20:34:04,047][0m Trial 1 finished with value: 0.38554345414740965 and parameters: {'observation_period_num': 222, 'train_rates': 0.6251163252638361, 'learning_rate': 3.1354167775721205e-05, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9729719033206059}. Best is trial 0 with value: 0.15965909055935648.[0m
[32m[I 2025-01-04 20:35:06,097][0m Trial 2 finished with value: 0.12229396813298642 and parameters: {'observation_period_num': 38, 'train_rates': 0.868691100729799, 'learning_rate': 1.3761723501377013e-05, 'batch_size': 135, 'step_size': 10, 'gamma': 0.7874096856002739}. Best is trial 2 with value: 0.12229396813298642.[0m
[32m[I 2025-01-04 20:35:56,049][0m Trial 3 finished with value: 0.23205313086509705 and parameters: {'observation_period_num': 25, 'train_rates': 0.972287176683077, 'learning_rate': 1.173271401223812e-05, 'batch_size': 117, 'step_size': 4, 'gamma': 0.7995441818460672}. Best is trial 2 with value: 0.12229396813298642.[0m
[32m[I 2025-01-04 20:36:55,101][0m Trial 4 finished with value: 0.3488281590909493 and parameters: {'observation_period_num': 95, 'train_rates': 0.8280505699344484, 'learning_rate': 1.5015204004912902e-06, 'batch_size': 74, 'step_size': 5, 'gamma': 0.9663002320302803}. Best is trial 2 with value: 0.12229396813298642.[0m
[32m[I 2025-01-04 20:38:46,989][0m Trial 5 finished with value: 0.1725957143018071 and parameters: {'observation_period_num': 36, 'train_rates': 0.6727395409052213, 'learning_rate': 0.0009045780670178503, 'batch_size': 45, 'step_size': 10, 'gamma': 0.8207108123282162}. Best is trial 2 with value: 0.12229396813298642.[0m
Early stopping at epoch 52
[32m[I 2025-01-04 20:39:12,955][0m Trial 6 finished with value: 0.9372342188249935 and parameters: {'observation_period_num': 242, 'train_rates': 0.6107620869154976, 'learning_rate': 1.674682344629732e-06, 'batch_size': 152, 'step_size': 1, 'gamma': 0.7980607418362043}. Best is trial 2 with value: 0.12229396813298642.[0m
[32m[I 2025-01-04 20:40:03,995][0m Trial 7 finished with value: 0.3608603504647045 and parameters: {'observation_period_num': 32, 'train_rates': 0.7560084189525531, 'learning_rate': 6.362253991681065e-06, 'batch_size': 149, 'step_size': 8, 'gamma': 0.9542213783096758}. Best is trial 2 with value: 0.12229396813298642.[0m
[32m[I 2025-01-04 20:41:41,813][0m Trial 8 finished with value: 0.3380393950861666 and parameters: {'observation_period_num': 205, 'train_rates': 0.6754034681322935, 'learning_rate': 7.451851261305615e-06, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9236107566624551}. Best is trial 2 with value: 0.12229396813298642.[0m
[32m[I 2025-01-04 20:43:33,854][0m Trial 9 finished with value: 0.07806183100633678 and parameters: {'observation_period_num': 136, 'train_rates': 0.88084570156552, 'learning_rate': 3.479557877839269e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8399843630958788}. Best is trial 9 with value: 0.07806183100633678.[0m
[32m[I 2025-01-04 20:44:15,132][0m Trial 10 finished with value: 0.18132801353931427 and parameters: {'observation_period_num': 160, 'train_rates': 0.9402809928992659, 'learning_rate': 0.0004026389332545754, 'batch_size': 211, 'step_size': 1, 'gamma': 0.8807062618764226}. Best is trial 9 with value: 0.07806183100633678.[0m
[32m[I 2025-01-04 20:48:26,425][0m Trial 11 finished with value: 0.11147539175692059 and parameters: {'observation_period_num': 104, 'train_rates': 0.8821168851323904, 'learning_rate': 0.00010453821270394917, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7606816924616926}. Best is trial 9 with value: 0.07806183100633678.[0m
[32m[I 2025-01-04 20:51:31,755][0m Trial 12 finished with value: 0.09652021967205171 and parameters: {'observation_period_num': 114, 'train_rates': 0.8970271506799774, 'learning_rate': 0.00012804563198511926, 'batch_size': 22, 'step_size': 8, 'gamma': 0.7546242284142433}. Best is trial 9 with value: 0.07806183100633678.[0m
[32m[I 2025-01-04 20:52:31,299][0m Trial 13 finished with value: 0.06722322955989 and parameters: {'observation_period_num': 154, 'train_rates': 0.918763352438266, 'learning_rate': 0.00012304154949968686, 'batch_size': 88, 'step_size': 7, 'gamma': 0.8542005155068486}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:53:29,934][0m Trial 14 finished with value: 0.08769439185251955 and parameters: {'observation_period_num': 171, 'train_rates': 0.9345498531098264, 'learning_rate': 0.00012385745003488442, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8564312034287814}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:54:27,180][0m Trial 15 finished with value: 0.12942505514982974 and parameters: {'observation_period_num': 151, 'train_rates': 0.8234123532771804, 'learning_rate': 6.612142915878407e-05, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8610241093268567}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:55:14,880][0m Trial 16 finished with value: 0.09453028440475464 and parameters: {'observation_period_num': 73, 'train_rates': 0.9895700511834995, 'learning_rate': 0.00044484260056614497, 'batch_size': 176, 'step_size': 6, 'gamma': 0.894088579528858}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:56:16,404][0m Trial 17 finished with value: 0.09947607776112936 and parameters: {'observation_period_num': 184, 'train_rates': 0.8374329918604119, 'learning_rate': 0.00027996538056435235, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8400993793422575}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:58:04,274][0m Trial 18 finished with value: 0.07840762493156252 and parameters: {'observation_period_num': 136, 'train_rates': 0.9258074121012366, 'learning_rate': 5.766313448444204e-05, 'batch_size': 38, 'step_size': 3, 'gamma': 0.9243782288690411}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:59:02,789][0m Trial 19 finished with value: 0.5007538237396815 and parameters: {'observation_period_num': 72, 'train_rates': 0.7304894943997067, 'learning_rate': 3.4254950375570616e-06, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8342331406578902}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 20:59:48,887][0m Trial 20 finished with value: 0.14964663460082614 and parameters: {'observation_period_num': 197, 'train_rates': 0.8593725683929861, 'learning_rate': 2.059371383376665e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.9076356091493112}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:02:03,097][0m Trial 21 finished with value: 0.07814546381788594 and parameters: {'observation_period_num': 138, 'train_rates': 0.9205303286625456, 'learning_rate': 5.286034979484087e-05, 'batch_size': 31, 'step_size': 3, 'gamma': 0.936401968194009}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:04:24,547][0m Trial 22 finished with value: 0.06875219667855377 and parameters: {'observation_period_num': 125, 'train_rates': 0.9085780315148421, 'learning_rate': 5.6654086461533615e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.86865517600675}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:05:38,595][0m Trial 23 finished with value: 0.0990567353963852 and parameters: {'observation_period_num': 122, 'train_rates': 0.9558703030405786, 'learning_rate': 0.00019067809169083484, 'batch_size': 66, 'step_size': 7, 'gamma': 0.8783564912163024}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:07:11,315][0m Trial 24 finished with value: 0.1098147306418928 and parameters: {'observation_period_num': 150, 'train_rates': 0.8997628477873328, 'learning_rate': 5.8903552909565844e-05, 'batch_size': 47, 'step_size': 2, 'gamma': 0.8452302452473838}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:08:09,611][0m Trial 25 finished with value: 0.13425154460128397 and parameters: {'observation_period_num': 84, 'train_rates': 0.799350949578524, 'learning_rate': 2.0061410181817518e-05, 'batch_size': 81, 'step_size': 5, 'gamma': 0.8696554360655165}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:08:57,484][0m Trial 26 finished with value: 0.10795425849157153 and parameters: {'observation_period_num': 176, 'train_rates': 0.8515395784615037, 'learning_rate': 8.887050185724702e-05, 'batch_size': 114, 'step_size': 4, 'gamma': 0.8260874011202276}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:11:10,072][0m Trial 27 finished with value: 0.09545175919463919 and parameters: {'observation_period_num': 125, 'train_rates': 0.9023598991614693, 'learning_rate': 3.927355921511766e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8965075215221248}. Best is trial 13 with value: 0.06722322955989.[0m
[32m[I 2025-01-04 21:15:47,463][0m Trial 28 finished with value: 0.06430162132247207 and parameters: {'observation_period_num': 58, 'train_rates': 0.9667318736504533, 'learning_rate': 0.00019119322281685864, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8545346342321075}. Best is trial 28 with value: 0.06430162132247207.[0m
[32m[I 2025-01-04 21:17:10,910][0m Trial 29 finished with value: 0.031732927434719525 and parameters: {'observation_period_num': 7, 'train_rates': 0.9647326920529089, 'learning_rate': 0.00019405718529795504, 'batch_size': 55, 'step_size': 9, 'gamma': 0.8187494821119851}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:18:32,429][0m Trial 30 finished with value: 0.047829657793045044 and parameters: {'observation_period_num': 8, 'train_rates': 0.9885676389170387, 'learning_rate': 0.00022350204617150118, 'batch_size': 60, 'step_size': 14, 'gamma': 0.7776530603462726}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:20:05,379][0m Trial 31 finished with value: 0.04112134128808975 and parameters: {'observation_period_num': 7, 'train_rates': 0.9865866540277914, 'learning_rate': 0.0002142705357871802, 'batch_size': 53, 'step_size': 15, 'gamma': 0.7759943625379123}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:21:38,270][0m Trial 32 finished with value: 0.058610815554857254 and parameters: {'observation_period_num': 8, 'train_rates': 0.9886291972003342, 'learning_rate': 0.0007375554748519592, 'batch_size': 60, 'step_size': 15, 'gamma': 0.772741409825019}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:23:00,428][0m Trial 33 finished with value: 0.05420608073472977 and parameters: {'observation_period_num': 5, 'train_rates': 0.9899217205942482, 'learning_rate': 0.0008532710618582704, 'batch_size': 58, 'step_size': 15, 'gamma': 0.7751455519499707}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:24:21,536][0m Trial 34 finished with value: 0.032541386755931115 and parameters: {'observation_period_num': 10, 'train_rates': 0.953578491954707, 'learning_rate': 0.0005124891550120212, 'batch_size': 56, 'step_size': 14, 'gamma': 0.7832982776214904}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:25:16,244][0m Trial 35 finished with value: 0.03414021590008186 and parameters: {'observation_period_num': 20, 'train_rates': 0.9558530927792104, 'learning_rate': 0.00043510918529904573, 'batch_size': 101, 'step_size': 13, 'gamma': 0.7839222058598607}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:26:03,290][0m Trial 36 finished with value: 0.0531782639156652 and parameters: {'observation_period_num': 52, 'train_rates': 0.9489366787749723, 'learning_rate': 0.0004972247073288287, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8094949447599283}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:27:07,123][0m Trial 37 finished with value: 0.04241599957458675 and parameters: {'observation_period_num': 25, 'train_rates': 0.9619905023648995, 'learning_rate': 0.0003388375880488861, 'batch_size': 77, 'step_size': 13, 'gamma': 0.7911586451391996}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:28:04,347][0m Trial 38 finished with value: 0.0565584430037296 and parameters: {'observation_period_num': 46, 'train_rates': 0.9498767127338106, 'learning_rate': 0.0007317617933488989, 'batch_size': 102, 'step_size': 12, 'gamma': 0.8062836166275481}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:28:50,766][0m Trial 39 finished with value: 0.04676055908203125 and parameters: {'observation_period_num': 24, 'train_rates': 0.9605066486816557, 'learning_rate': 0.0005518705907766644, 'batch_size': 140, 'step_size': 14, 'gamma': 0.8159878462576298}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:29:34,842][0m Trial 40 finished with value: 0.04896291345357895 and parameters: {'observation_period_num': 19, 'train_rates': 0.9323554437785984, 'learning_rate': 0.00023292690086526154, 'batch_size': 248, 'step_size': 11, 'gamma': 0.7865958519382698}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:30:46,869][0m Trial 41 finished with value: 0.06425575472097447 and parameters: {'observation_period_num': 37, 'train_rates': 0.9679068001154995, 'learning_rate': 0.00028115831500741153, 'batch_size': 73, 'step_size': 13, 'gamma': 0.7908875649847225}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:32:23,467][0m Trial 42 finished with value: 0.04542449630070024 and parameters: {'observation_period_num': 24, 'train_rates': 0.972249475100289, 'learning_rate': 0.0003697917174765648, 'batch_size': 47, 'step_size': 14, 'gamma': 0.7628456810578615}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:33:27,194][0m Trial 43 finished with value: 0.032506640227335806 and parameters: {'observation_period_num': 18, 'train_rates': 0.9460753941135513, 'learning_rate': 0.0006194776306940825, 'batch_size': 79, 'step_size': 12, 'gamma': 0.7938744024065176}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:34:18,059][0m Trial 44 finished with value: 0.14983667928660124 and parameters: {'observation_period_num': 17, 'train_rates': 0.7088674559878174, 'learning_rate': 0.0001685164847293351, 'batch_size': 82, 'step_size': 10, 'gamma': 0.8007283302342918}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:35:05,843][0m Trial 45 finished with value: 0.12276693189566114 and parameters: {'observation_period_num': 47, 'train_rates': 0.8835920488828903, 'learning_rate': 0.0009787787305944767, 'batch_size': 109, 'step_size': 12, 'gamma': 0.988048837498676}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:35:53,491][0m Trial 46 finished with value: 0.06738188684072798 and parameters: {'observation_period_num': 61, 'train_rates': 0.9403271861856543, 'learning_rate': 0.0006111895850294007, 'batch_size': 123, 'step_size': 14, 'gamma': 0.7513849301334016}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:37:17,378][0m Trial 47 finished with value: 0.05020315207163744 and parameters: {'observation_period_num': 35, 'train_rates': 0.9189732724730125, 'learning_rate': 0.0005797243826973501, 'batch_size': 53, 'step_size': 9, 'gamma': 0.7669150122868076}. Best is trial 29 with value: 0.031732927434719525.[0m
[32m[I 2025-01-04 21:39:08,094][0m Trial 48 finished with value: 0.030610540930449574 and parameters: {'observation_period_num': 17, 'train_rates': 0.9479773174293017, 'learning_rate': 0.0003281054511366228, 'batch_size': 65, 'step_size': 11, 'gamma': 0.7829997763159818}. Best is trial 48 with value: 0.030610540930449574.[0m
[32m[I 2025-01-04 21:40:20,222][0m Trial 49 finished with value: 0.048611855287529745 and parameters: {'observation_period_num': 41, 'train_rates': 0.8705993452530905, 'learning_rate': 0.000347545491072898, 'batch_size': 101, 'step_size': 11, 'gamma': 0.8204469015399153}. Best is trial 48 with value: 0.030610540930449574.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AMZN_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4428 | 0.1705
Epoch 2/300, Loss: 0.1562 | 0.1191
Epoch 3/300, Loss: 0.1306 | 0.0835
Epoch 4/300, Loss: 0.1216 | 0.0787
Epoch 5/300, Loss: 0.1207 | 0.1189
Epoch 6/300, Loss: 0.1311 | 0.0873
Epoch 7/300, Loss: 0.1212 | 0.0993
Epoch 8/300, Loss: 0.1122 | 0.0701
Epoch 9/300, Loss: 0.0986 | 0.0660
Epoch 10/300, Loss: 0.1009 | 0.0620
Epoch 11/300, Loss: 0.1001 | 0.0586
Epoch 12/300, Loss: 0.1012 | 0.0578
Epoch 13/300, Loss: 0.0940 | 0.0554
Epoch 14/300, Loss: 0.0856 | 0.0528
Epoch 15/300, Loss: 0.0843 | 0.0518
Epoch 16/300, Loss: 0.0837 | 0.0512
Epoch 17/300, Loss: 0.0844 | 0.0498
Epoch 18/300, Loss: 0.0859 | 0.0481
Epoch 19/300, Loss: 0.0853 | 0.0481
Epoch 20/300, Loss: 0.0830 | 0.0469
Epoch 21/300, Loss: 0.0808 | 0.0459
Epoch 22/300, Loss: 0.0792 | 0.0450
Epoch 23/300, Loss: 0.0781 | 0.0443
Epoch 24/300, Loss: 0.0770 | 0.0433
Epoch 25/300, Loss: 0.0758 | 0.0426
Epoch 26/300, Loss: 0.0747 | 0.0422
Epoch 27/300, Loss: 0.0740 | 0.0418
Epoch 28/300, Loss: 0.0736 | 0.0414
Epoch 29/300, Loss: 0.0732 | 0.0404
Epoch 30/300, Loss: 0.0728 | 0.0398
Epoch 31/300, Loss: 0.0723 | 0.0395
Epoch 32/300, Loss: 0.0719 | 0.0391
Epoch 33/300, Loss: 0.0715 | 0.0388
Epoch 34/300, Loss: 0.0714 | 0.0384
Epoch 35/300, Loss: 0.0711 | 0.0381
Epoch 36/300, Loss: 0.0707 | 0.0379
Epoch 37/300, Loss: 0.0703 | 0.0376
Epoch 38/300, Loss: 0.0699 | 0.0374
Epoch 39/300, Loss: 0.0695 | 0.0371
Epoch 40/300, Loss: 0.0694 | 0.0377
Epoch 41/300, Loss: 0.0690 | 0.0375
Epoch 42/300, Loss: 0.0690 | 0.0375
Epoch 43/300, Loss: 0.0690 | 0.0373
Epoch 44/300, Loss: 0.0688 | 0.0371
Epoch 45/300, Loss: 0.0693 | 0.0370
Epoch 46/300, Loss: 0.0700 | 0.0370
Epoch 47/300, Loss: 0.0709 | 0.0371
Epoch 48/300, Loss: 0.0708 | 0.0370
Epoch 49/300, Loss: 0.0712 | 0.0371
Epoch 50/300, Loss: 0.0709 | 0.0371
Epoch 51/300, Loss: 0.0718 | 0.0427
Epoch 52/300, Loss: 0.0712 | 0.0437
Epoch 53/300, Loss: 0.0713 | 0.0451
Epoch 54/300, Loss: 0.0706 | 0.0454
Epoch 55/300, Loss: 0.0702 | 0.0461
Epoch 56/300, Loss: 0.0695 | 0.0502
Epoch 57/300, Loss: 0.0691 | 0.0464
Epoch 58/300, Loss: 0.0681 | 0.0428
Epoch 59/300, Loss: 0.0680 | 0.0403
Epoch 60/300, Loss: 0.0681 | 0.0386
Epoch 61/300, Loss: 0.0680 | 0.0374
Epoch 62/300, Loss: 0.0674 | 0.0359
Epoch 63/300, Loss: 0.0663 | 0.0357
Epoch 64/300, Loss: 0.0649 | 0.0357
Epoch 65/300, Loss: 0.0642 | 0.0358
Epoch 66/300, Loss: 0.0639 | 0.0358
Epoch 67/300, Loss: 0.0636 | 0.0355
Epoch 68/300, Loss: 0.0635 | 0.0355
Epoch 69/300, Loss: 0.0634 | 0.0354
Epoch 70/300, Loss: 0.0633 | 0.0353
Epoch 71/300, Loss: 0.0632 | 0.0352
Epoch 72/300, Loss: 0.0632 | 0.0350
Epoch 73/300, Loss: 0.0631 | 0.0348
Epoch 74/300, Loss: 0.0631 | 0.0348
Epoch 75/300, Loss: 0.0630 | 0.0347
Epoch 76/300, Loss: 0.0630 | 0.0346
Epoch 77/300, Loss: 0.0629 | 0.0345
Epoch 78/300, Loss: 0.0628 | 0.0345
Epoch 79/300, Loss: 0.0627 | 0.0345
Epoch 80/300, Loss: 0.0625 | 0.0344
Epoch 81/300, Loss: 0.0624 | 0.0343
Epoch 82/300, Loss: 0.0623 | 0.0343
Epoch 83/300, Loss: 0.0622 | 0.0342
Epoch 84/300, Loss: 0.0621 | 0.0342
Epoch 85/300, Loss: 0.0621 | 0.0342
Epoch 86/300, Loss: 0.0620 | 0.0341
Epoch 87/300, Loss: 0.0620 | 0.0341
Epoch 88/300, Loss: 0.0620 | 0.0341
Epoch 89/300, Loss: 0.0620 | 0.0339
Epoch 90/300, Loss: 0.0620 | 0.0339
Epoch 91/300, Loss: 0.0619 | 0.0339
Epoch 92/300, Loss: 0.0619 | 0.0339
Epoch 93/300, Loss: 0.0618 | 0.0338
Epoch 94/300, Loss: 0.0617 | 0.0338
Epoch 95/300, Loss: 0.0617 | 0.0337
Epoch 96/300, Loss: 0.0616 | 0.0336
Epoch 97/300, Loss: 0.0615 | 0.0336
Epoch 98/300, Loss: 0.0614 | 0.0336
Epoch 99/300, Loss: 0.0613 | 0.0336
Epoch 100/300, Loss: 0.0613 | 0.0335
Epoch 101/300, Loss: 0.0613 | 0.0335
Epoch 102/300, Loss: 0.0612 | 0.0334
Epoch 103/300, Loss: 0.0612 | 0.0334
Epoch 104/300, Loss: 0.0612 | 0.0334
Epoch 105/300, Loss: 0.0611 | 0.0334
Epoch 106/300, Loss: 0.0611 | 0.0333
Epoch 107/300, Loss: 0.0611 | 0.0333
Epoch 108/300, Loss: 0.0611 | 0.0332
Epoch 109/300, Loss: 0.0610 | 0.0332
Epoch 110/300, Loss: 0.0610 | 0.0332
Epoch 111/300, Loss: 0.0610 | 0.0332
Epoch 112/300, Loss: 0.0610 | 0.0331
Epoch 113/300, Loss: 0.0609 | 0.0331
Epoch 114/300, Loss: 0.0609 | 0.0331
Epoch 115/300, Loss: 0.0609 | 0.0331
Epoch 116/300, Loss: 0.0609 | 0.0331
Epoch 117/300, Loss: 0.0609 | 0.0330
Epoch 118/300, Loss: 0.0608 | 0.0330
Epoch 119/300, Loss: 0.0608 | 0.0330
Epoch 120/300, Loss: 0.0608 | 0.0330
Epoch 121/300, Loss: 0.0608 | 0.0330
Epoch 122/300, Loss: 0.0608 | 0.0329
Epoch 123/300, Loss: 0.0608 | 0.0329
Epoch 124/300, Loss: 0.0608 | 0.0329
Epoch 125/300, Loss: 0.0607 | 0.0329
Epoch 126/300, Loss: 0.0607 | 0.0329
Epoch 127/300, Loss: 0.0607 | 0.0329
Epoch 128/300, Loss: 0.0607 | 0.0328
Epoch 129/300, Loss: 0.0607 | 0.0328
Epoch 130/300, Loss: 0.0607 | 0.0328
Epoch 131/300, Loss: 0.0607 | 0.0328
Epoch 132/300, Loss: 0.0607 | 0.0328
Epoch 133/300, Loss: 0.0607 | 0.0328
Epoch 134/300, Loss: 0.0606 | 0.0328
Epoch 135/300, Loss: 0.0606 | 0.0328
Epoch 136/300, Loss: 0.0606 | 0.0328
Epoch 137/300, Loss: 0.0606 | 0.0327
Epoch 138/300, Loss: 0.0606 | 0.0327
Epoch 139/300, Loss: 0.0606 | 0.0327
Epoch 140/300, Loss: 0.0606 | 0.0327
Epoch 141/300, Loss: 0.0606 | 0.0327
Epoch 142/300, Loss: 0.0606 | 0.0327
Epoch 143/300, Loss: 0.0606 | 0.0327
Epoch 144/300, Loss: 0.0606 | 0.0327
Epoch 145/300, Loss: 0.0606 | 0.0327
Epoch 146/300, Loss: 0.0606 | 0.0327
Epoch 147/300, Loss: 0.0606 | 0.0327
Epoch 148/300, Loss: 0.0605 | 0.0327
Epoch 149/300, Loss: 0.0605 | 0.0327
Epoch 150/300, Loss: 0.0605 | 0.0326
Epoch 151/300, Loss: 0.0605 | 0.0326
Epoch 152/300, Loss: 0.0605 | 0.0326
Epoch 153/300, Loss: 0.0605 | 0.0326
Epoch 154/300, Loss: 0.0605 | 0.0326
Epoch 155/300, Loss: 0.0605 | 0.0326
Epoch 156/300, Loss: 0.0605 | 0.0326
Epoch 157/300, Loss: 0.0605 | 0.0326
Epoch 158/300, Loss: 0.0605 | 0.0326
Epoch 159/300, Loss: 0.0605 | 0.0326
Epoch 160/300, Loss: 0.0605 | 0.0326
Epoch 161/300, Loss: 0.0605 | 0.0326
Epoch 162/300, Loss: 0.0605 | 0.0326
Epoch 163/300, Loss: 0.0605 | 0.0326
Epoch 164/300, Loss: 0.0605 | 0.0326
Epoch 165/300, Loss: 0.0605 | 0.0326
Epoch 166/300, Loss: 0.0605 | 0.0326
Epoch 167/300, Loss: 0.0605 | 0.0326
Epoch 168/300, Loss: 0.0605 | 0.0326
Epoch 169/300, Loss: 0.0605 | 0.0326
Epoch 170/300, Loss: 0.0605 | 0.0326
Epoch 171/300, Loss: 0.0605 | 0.0326
Epoch 172/300, Loss: 0.0605 | 0.0326
Epoch 173/300, Loss: 0.0605 | 0.0326
Epoch 174/300, Loss: 0.0605 | 0.0326
Epoch 175/300, Loss: 0.0605 | 0.0326
Epoch 176/300, Loss: 0.0605 | 0.0326
Epoch 177/300, Loss: 0.0605 | 0.0326
Epoch 178/300, Loss: 0.0605 | 0.0326
Epoch 179/300, Loss: 0.0605 | 0.0326
Epoch 180/300, Loss: 0.0605 | 0.0325
Epoch 181/300, Loss: 0.0605 | 0.0325
Epoch 182/300, Loss: 0.0605 | 0.0325
Epoch 183/300, Loss: 0.0605 | 0.0325
Epoch 184/300, Loss: 0.0604 | 0.0325
Epoch 185/300, Loss: 0.0604 | 0.0325
Epoch 186/300, Loss: 0.0604 | 0.0325
Epoch 187/300, Loss: 0.0604 | 0.0325
Epoch 188/300, Loss: 0.0604 | 0.0325
Epoch 189/300, Loss: 0.0604 | 0.0325
Epoch 190/300, Loss: 0.0604 | 0.0325
Epoch 191/300, Loss: 0.0604 | 0.0325
Epoch 192/300, Loss: 0.0604 | 0.0325
Epoch 193/300, Loss: 0.0604 | 0.0325
Epoch 194/300, Loss: 0.0604 | 0.0325
Epoch 195/300, Loss: 0.0604 | 0.0325
Epoch 196/300, Loss: 0.0604 | 0.0325
Epoch 197/300, Loss: 0.0604 | 0.0325
Epoch 198/300, Loss: 0.0604 | 0.0325
Epoch 199/300, Loss: 0.0604 | 0.0325
Epoch 200/300, Loss: 0.0604 | 0.0325
Epoch 201/300, Loss: 0.0604 | 0.0325
Epoch 202/300, Loss: 0.0604 | 0.0325
Epoch 203/300, Loss: 0.0604 | 0.0325
Epoch 204/300, Loss: 0.0604 | 0.0325
Epoch 205/300, Loss: 0.0604 | 0.0325
Epoch 206/300, Loss: 0.0604 | 0.0325
Epoch 207/300, Loss: 0.0604 | 0.0325
Epoch 208/300, Loss: 0.0604 | 0.0325
Epoch 209/300, Loss: 0.0604 | 0.0325
Epoch 210/300, Loss: 0.0604 | 0.0325
Epoch 211/300, Loss: 0.0604 | 0.0325
Epoch 212/300, Loss: 0.0604 | 0.0325
Epoch 213/300, Loss: 0.0604 | 0.0325
Epoch 214/300, Loss: 0.0604 | 0.0325
Epoch 215/300, Loss: 0.0604 | 0.0325
Epoch 216/300, Loss: 0.0604 | 0.0325
Epoch 217/300, Loss: 0.0604 | 0.0325
Epoch 218/300, Loss: 0.0604 | 0.0325
Epoch 219/300, Loss: 0.0604 | 0.0325
Epoch 220/300, Loss: 0.0604 | 0.0325
Epoch 221/300, Loss: 0.0604 | 0.0325
Epoch 222/300, Loss: 0.0604 | 0.0325
Epoch 223/300, Loss: 0.0604 | 0.0325
Epoch 224/300, Loss: 0.0604 | 0.0325
Epoch 225/300, Loss: 0.0604 | 0.0325
Epoch 226/300, Loss: 0.0604 | 0.0325
Epoch 227/300, Loss: 0.0604 | 0.0325
Epoch 228/300, Loss: 0.0604 | 0.0325
Epoch 229/300, Loss: 0.0604 | 0.0325
Epoch 230/300, Loss: 0.0604 | 0.0325
Epoch 231/300, Loss: 0.0604 | 0.0325
Epoch 232/300, Loss: 0.0604 | 0.0325
Epoch 233/300, Loss: 0.0604 | 0.0325
Epoch 234/300, Loss: 0.0604 | 0.0325
Epoch 235/300, Loss: 0.0604 | 0.0325
Epoch 236/300, Loss: 0.0604 | 0.0325
Epoch 237/300, Loss: 0.0604 | 0.0325
Epoch 238/300, Loss: 0.0604 | 0.0325
Epoch 239/300, Loss: 0.0604 | 0.0325
Epoch 240/300, Loss: 0.0604 | 0.0325
Epoch 241/300, Loss: 0.0604 | 0.0325
Epoch 242/300, Loss: 0.0604 | 0.0325
Epoch 243/300, Loss: 0.0604 | 0.0325
Epoch 244/300, Loss: 0.0604 | 0.0325
Epoch 245/300, Loss: 0.0604 | 0.0325
Epoch 246/300, Loss: 0.0604 | 0.0325
Epoch 247/300, Loss: 0.0604 | 0.0325
Epoch 248/300, Loss: 0.0604 | 0.0325
Epoch 249/300, Loss: 0.0604 | 0.0325
Epoch 250/300, Loss: 0.0604 | 0.0325
Epoch 251/300, Loss: 0.0604 | 0.0325
Epoch 252/300, Loss: 0.0604 | 0.0325
Epoch 253/300, Loss: 0.0604 | 0.0325
Epoch 254/300, Loss: 0.0604 | 0.0325
Epoch 255/300, Loss: 0.0604 | 0.0325
Epoch 256/300, Loss: 0.0604 | 0.0325
Epoch 257/300, Loss: 0.0604 | 0.0325
Epoch 258/300, Loss: 0.0604 | 0.0325
Epoch 259/300, Loss: 0.0604 | 0.0325
Epoch 260/300, Loss: 0.0604 | 0.0325
Epoch 261/300, Loss: 0.0604 | 0.0325
Epoch 262/300, Loss: 0.0604 | 0.0325
Epoch 263/300, Loss: 0.0604 | 0.0325
Epoch 264/300, Loss: 0.0604 | 0.0325
Epoch 265/300, Loss: 0.0604 | 0.0325
Epoch 266/300, Loss: 0.0604 | 0.0325
Epoch 267/300, Loss: 0.0604 | 0.0325
Epoch 268/300, Loss: 0.0604 | 0.0325
Epoch 269/300, Loss: 0.0604 | 0.0325
Epoch 270/300, Loss: 0.0604 | 0.0325
Epoch 271/300, Loss: 0.0604 | 0.0325
Epoch 272/300, Loss: 0.0604 | 0.0325
Epoch 273/300, Loss: 0.0604 | 0.0325
Epoch 274/300, Loss: 0.0604 | 0.0325
Epoch 275/300, Loss: 0.0604 | 0.0325
Epoch 276/300, Loss: 0.0604 | 0.0325
Epoch 277/300, Loss: 0.0604 | 0.0325
Epoch 278/300, Loss: 0.0604 | 0.0325
Epoch 279/300, Loss: 0.0604 | 0.0325
Epoch 280/300, Loss: 0.0604 | 0.0325
Epoch 281/300, Loss: 0.0604 | 0.0325
Epoch 282/300, Loss: 0.0604 | 0.0325
Epoch 283/300, Loss: 0.0604 | 0.0325
Epoch 284/300, Loss: 0.0604 | 0.0325
Epoch 285/300, Loss: 0.0604 | 0.0325
Epoch 286/300, Loss: 0.0604 | 0.0325
Epoch 287/300, Loss: 0.0604 | 0.0325
Epoch 288/300, Loss: 0.0604 | 0.0325
Epoch 289/300, Loss: 0.0604 | 0.0325
Epoch 290/300, Loss: 0.0604 | 0.0325
Epoch 291/300, Loss: 0.0604 | 0.0325
Epoch 292/300, Loss: 0.0604 | 0.0325
Epoch 293/300, Loss: 0.0604 | 0.0325
Epoch 294/300, Loss: 0.0604 | 0.0325
Epoch 295/300, Loss: 0.0604 | 0.0325
Epoch 296/300, Loss: 0.0604 | 0.0325
Epoch 297/300, Loss: 0.0604 | 0.0325
Epoch 298/300, Loss: 0.0604 | 0.0325
Epoch 299/300, Loss: 0.0604 | 0.0325
Epoch 300/300, Loss: 0.0604 | 0.0325
Runtime (seconds): 316.22865438461304
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 228.85010163881816
RMSE: 15.127792358398438
MAE: 15.127792358398438
R-squared: nan
[190.61221]
