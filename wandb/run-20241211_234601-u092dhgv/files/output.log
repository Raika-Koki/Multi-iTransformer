[*********************100%***********************]  1 of 1 completed
[*********************100%***********************]  1 of 1 completed
Date
2012-05-18     10.692500
2012-05-21     10.905500
2012-05-22     10.766500
2012-05-23     10.864000
2012-05-24     10.762000
                 ...
2023-05-24    116.750000
2023-05-25    115.000000
2023-05-26    120.110001
2023-05-30    121.660004
2023-05-31    120.580002
Name: AMZN, Length: 2776, dtype: float64
Price         Volume    BB_Upper    BB_Lower   BB_Middle      MACD MACD_Signal MACD_Diff        RSI    SMA_50    SMA_200 SMA_200-50
Ticker          AMZN
Date
2023-05-24  63487900  120.005693  100.053308  110.029501  3.584615    3.109054  0.475561  66.184276  104.7736  105.39470    0.62110
2023-05-25  66496700  120.495777  100.081225  110.288501  3.450388    3.177321  0.273067  61.346390  105.1496  105.28055    0.13095
2023-05-26  96779900  121.821939  100.221062  111.021501  3.713539    3.284565  0.428975  68.570791  105.5510  105.16765   -0.38335
2023-05-30  64314800  122.926375  101.077627  112.002001  4.001039    3.427859  0.573180  70.379232  106.0052  105.07275   -0.93245
2023-05-31  72800800  123.673883  102.025119  112.849501  4.094538    3.561195  0.533343  67.466273  106.4626  104.95790   -1.50470
{'observation_period_num': 5, 'train_rates': 0.9674192065835944, 'learning_rate': 0.0006703481785402301, 'batch_size': 197, 'step_size': 5, 'gamma': 0.9119241664414688, 'depth': 4, 'dim': 123}
{0: {'observation_period_num': 14, 'train_rates': 0.9534530255286052, 'learning_rate': 2.383027942061777e-05, 'batch_size': 24, 'step_size': 6, 'gamma': 0.9343469177823375, 'depth': 5, 'dim': 162}, 1: {'observation_period_num': 10, 'train_rates': 0.7859430649262442, 'learning_rate': 0.00011626904101397289, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9591926500011835, 'depth': 5, 'dim': 221}, 2: {'observation_period_num': 5, 'train_rates': 0.9881973682668941, 'learning_rate': 0.0008599253544665889, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8332821730513479, 'depth': 5, 'dim': 127}, 3: {'observation_period_num': 11, 'train_rates': 0.9524554300989908, 'learning_rate': 0.00011007906140572895, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9320524916302059, 'depth': 6, 'dim': 145}}
{'observation_period_num': 13, 'train_rates': 0.8047176482675613, 'learning_rate': 0.0007766732022188664, 'batch_size': 128, 'step_size': 2, 'gamma': 0.925182630519391, 'depth': 4, 'dim': 228}
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Epoch 1/500, trend Loss: 0.7236 | 0.4486
Epoch 2/500, trend Loss: 0.3073 | 0.2137
Epoch 3/500, trend Loss: 0.2051 | 0.2042
Epoch 4/500, trend Loss: 0.1635 | 0.1611
Epoch 5/500, trend Loss: 0.1373 | 0.1637
Epoch 6/500, trend Loss: 0.1290 | 0.0957
Epoch 7/500, trend Loss: 0.1153 | 0.1189
Epoch 8/500, trend Loss: 0.1384 | 0.0909
Epoch 9/500, trend Loss: 0.1189 | 0.1070
Epoch 10/500, trend Loss: 0.1358 | 0.2014
Epoch 11/500, trend Loss: 0.1616 | 0.1036
Epoch 12/500, trend Loss: 0.1322 | 0.0950
Epoch 13/500, trend Loss: 0.1470 | 0.1313
Epoch 14/500, trend Loss: 0.1593 | 0.1199
Epoch 15/500, trend Loss: 0.1794 | 0.1433
Epoch 16/500, trend Loss: 0.1652 | 0.1078
Epoch 17/500, trend Loss: 0.1581 | 0.1755
Epoch 18/500, trend Loss: 0.1265 | 0.0886
Epoch 19/500, trend Loss: 0.1019 | 0.0787
Epoch 20/500, trend Loss: 0.0879 | 0.0637
Epoch 21/500, trend Loss: 0.0811 | 0.0535
Epoch 22/500, trend Loss: 0.0779 | 0.0539
Epoch 23/500, trend Loss: 0.0771 | 0.0575
Epoch 24/500, trend Loss: 0.0799 | 0.0543
Epoch 25/500, trend Loss: 0.0813 | 0.0616
Epoch 26/500, trend Loss: 0.0886 | 0.0700
Epoch 27/500, trend Loss: 0.0865 | 0.0664
Epoch 28/500, trend Loss: 0.0888 | 0.0660
Epoch 29/500, trend Loss: 0.0835 | 0.0641
Epoch 30/500, trend Loss: 0.0775 | 0.0510
Epoch 31/500, trend Loss: 0.0716 | 0.0531
Epoch 32/500, trend Loss: 0.0662 | 0.0454
Epoch 33/500, trend Loss: 0.0652 | 0.0466
Epoch 34/500, trend Loss: 0.0653 | 0.0455
Epoch 35/500, trend Loss: 0.0721 | 0.0589
Epoch 36/500, trend Loss: 0.0712 | 0.0502
Epoch 37/500, trend Loss: 0.0723 | 0.0522
Epoch 38/500, trend Loss: 0.0688 | 0.0499
Epoch 39/500, trend Loss: 0.0659 | 0.0452
Epoch 40/500, trend Loss: 0.0611 | 0.0447
Epoch 41/500, trend Loss: 0.0583 | 0.0415
Epoch 42/500, trend Loss: 0.0562 | 0.0418
Epoch 43/500, trend Loss: 0.0551 | 0.0409
Epoch 44/500, trend Loss: 0.0544 | 0.0408
Epoch 45/500, trend Loss: 0.0540 | 0.0405
Epoch 46/500, trend Loss: 0.0536 | 0.0403
Epoch 47/500, trend Loss: 0.0532 | 0.0400
Epoch 48/500, trend Loss: 0.0529 | 0.0398
Epoch 49/500, trend Loss: 0.0526 | 0.0396
Epoch 50/500, trend Loss: 0.0523 | 0.0395
Epoch 51/500, trend Loss: 0.0521 | 0.0393
Epoch 52/500, trend Loss: 0.0518 | 0.0392
Epoch 53/500, trend Loss: 0.0516 | 0.0390
Epoch 54/500, trend Loss: 0.0514 | 0.0389
Epoch 55/500, trend Loss: 0.0512 | 0.0388
Epoch 56/500, trend Loss: 0.0510 | 0.0387
Epoch 57/500, trend Loss: 0.0508 | 0.0386
Epoch 58/500, trend Loss: 0.0506 | 0.0386
Epoch 59/500, trend Loss: 0.0505 | 0.0385
Epoch 60/500, trend Loss: 0.0503 | 0.0384
Epoch 61/500, trend Loss: 0.0501 | 0.0384
Epoch 62/500, trend Loss: 0.0500 | 0.0383
Epoch 63/500, trend Loss: 0.0498 | 0.0383
Epoch 64/500, trend Loss: 0.0497 | 0.0382
Epoch 65/500, trend Loss: 0.0495 | 0.0382
Epoch 66/500, trend Loss: 0.0494 | 0.0381
Epoch 67/500, trend Loss: 0.0493 | 0.0381
Epoch 68/500, trend Loss: 0.0491 | 0.0380
Epoch 69/500, trend Loss: 0.0490 | 0.0380
Epoch 70/500, trend Loss: 0.0489 | 0.0380
Epoch 71/500, trend Loss: 0.0487 | 0.0379
Epoch 72/500, trend Loss: 0.0486 | 0.0379
Epoch 73/500, trend Loss: 0.0485 | 0.0378
Epoch 74/500, trend Loss: 0.0484 | 0.0378
Epoch 75/500, trend Loss: 0.0482 | 0.0378
Epoch 76/500, trend Loss: 0.0481 | 0.0377
Epoch 77/500, trend Loss: 0.0480 | 0.0377
Epoch 78/500, trend Loss: 0.0479 | 0.0377
Epoch 79/500, trend Loss: 0.0478 | 0.0376
Epoch 80/500, trend Loss: 0.0477 | 0.0376
Epoch 81/500, trend Loss: 0.0476 | 0.0375
Epoch 82/500, trend Loss: 0.0475 | 0.0375
Epoch 83/500, trend Loss: 0.0474 | 0.0375
Epoch 84/500, trend Loss: 0.0473 | 0.0374
Epoch 85/500, trend Loss: 0.0473 | 0.0374
Epoch 86/500, trend Loss: 0.0472 | 0.0374
Epoch 87/500, trend Loss: 0.0471 | 0.0374
Epoch 88/500, trend Loss: 0.0470 | 0.0373
Epoch 89/500, trend Loss: 0.0470 | 0.0373
Epoch 90/500, trend Loss: 0.0469 | 0.0373
Epoch 91/500, trend Loss: 0.0468 | 0.0372
Epoch 92/500, trend Loss: 0.0468 | 0.0372
Epoch 93/500, trend Loss: 0.0467 | 0.0372
Epoch 94/500, trend Loss: 0.0466 | 0.0372
Epoch 95/500, trend Loss: 0.0466 | 0.0371
Epoch 96/500, trend Loss: 0.0465 | 0.0371
Epoch 97/500, trend Loss: 0.0465 | 0.0371
Epoch 98/500, trend Loss: 0.0464 | 0.0371
Epoch 99/500, trend Loss: 0.0464 | 0.0370
Epoch 100/500, trend Loss: 0.0463 | 0.0370
Epoch 101/500, trend Loss: 0.0463 | 0.0370
Epoch 102/500, trend Loss: 0.0462 | 0.0370
Epoch 103/500, trend Loss: 0.0462 | 0.0369
Epoch 104/500, trend Loss: 0.0461 | 0.0369
Epoch 105/500, trend Loss: 0.0461 | 0.0369
Epoch 106/500, trend Loss: 0.0461 | 0.0369
Epoch 107/500, trend Loss: 0.0460 | 0.0369
Epoch 108/500, trend Loss: 0.0460 | 0.0369
Epoch 109/500, trend Loss: 0.0460 | 0.0368
Epoch 110/500, trend Loss: 0.0459 | 0.0368
Epoch 111/500, trend Loss: 0.0459 | 0.0368
Epoch 112/500, trend Loss: 0.0459 | 0.0368
Epoch 113/500, trend Loss: 0.0458 | 0.0368
Epoch 114/500, trend Loss: 0.0458 | 0.0368
Epoch 115/500, trend Loss: 0.0458 | 0.0368
Epoch 116/500, trend Loss: 0.0457 | 0.0367
Epoch 117/500, trend Loss: 0.0457 | 0.0367
Epoch 118/500, trend Loss: 0.0457 | 0.0367
Epoch 119/500, trend Loss: 0.0457 | 0.0367
Epoch 120/500, trend Loss: 0.0456 | 0.0367
Epoch 121/500, trend Loss: 0.0456 | 0.0367
Epoch 122/500, trend Loss: 0.0456 | 0.0367
Epoch 123/500, trend Loss: 0.0456 | 0.0367
Epoch 124/500, trend Loss: 0.0456 | 0.0367
Epoch 125/500, trend Loss: 0.0455 | 0.0366
Epoch 126/500, trend Loss: 0.0455 | 0.0366
Epoch 127/500, trend Loss: 0.0455 | 0.0366
Epoch 128/500, trend Loss: 0.0455 | 0.0366
Epoch 129/500, trend Loss: 0.0455 | 0.0366
Epoch 130/500, trend Loss: 0.0455 | 0.0366
Epoch 131/500, trend Loss: 0.0454 | 0.0366
Epoch 132/500, trend Loss: 0.0454 | 0.0366
Epoch 133/500, trend Loss: 0.0454 | 0.0366
Epoch 134/500, trend Loss: 0.0454 | 0.0366
Epoch 135/500, trend Loss: 0.0454 | 0.0366
Epoch 136/500, trend Loss: 0.0454 | 0.0366
Epoch 137/500, trend Loss: 0.0454 | 0.0366
Epoch 138/500, trend Loss: 0.0454 | 0.0366
Epoch 139/500, trend Loss: 0.0453 | 0.0365
Epoch 140/500, trend Loss: 0.0453 | 0.0365
Epoch 141/500, trend Loss: 0.0453 | 0.0365
Epoch 142/500, trend Loss: 0.0453 | 0.0365
Epoch 143/500, trend Loss: 0.0453 | 0.0365
Epoch 144/500, trend Loss: 0.0453 | 0.0365
Epoch 145/500, trend Loss: 0.0453 | 0.0365
Epoch 146/500, trend Loss: 0.0453 | 0.0365
Epoch 147/500, trend Loss: 0.0453 | 0.0365
Epoch 148/500, trend Loss: 0.0453 | 0.0365
Epoch 149/500, trend Loss: 0.0452 | 0.0365
Epoch 150/500, trend Loss: 0.0452 | 0.0365
Epoch 151/500, trend Loss: 0.0452 | 0.0365
Epoch 152/500, trend Loss: 0.0452 | 0.0365
Epoch 153/500, trend Loss: 0.0452 | 0.0365
Epoch 154/500, trend Loss: 0.0452 | 0.0365
Epoch 155/500, trend Loss: 0.0452 | 0.0365
Epoch 156/500, trend Loss: 0.0452 | 0.0365
Epoch 157/500, trend Loss: 0.0452 | 0.0365
Epoch 158/500, trend Loss: 0.0452 | 0.0365
Epoch 159/500, trend Loss: 0.0452 | 0.0365
Epoch 160/500, trend Loss: 0.0452 | 0.0365
Epoch 161/500, trend Loss: 0.0452 | 0.0365
Epoch 162/500, trend Loss: 0.0452 | 0.0365
Epoch 163/500, trend Loss: 0.0452 | 0.0365
Epoch 164/500, trend Loss: 0.0452 | 0.0365
Epoch 165/500, trend Loss: 0.0451 | 0.0365
Epoch 166/500, trend Loss: 0.0451 | 0.0365
Epoch 167/500, trend Loss: 0.0451 | 0.0365
Epoch 168/500, trend Loss: 0.0451 | 0.0364
Epoch 169/500, trend Loss: 0.0451 | 0.0364
Epoch 170/500, trend Loss: 0.0451 | 0.0364
Epoch 171/500, trend Loss: 0.0451 | 0.0364
Epoch 172/500, trend Loss: 0.0451 | 0.0364
Epoch 173/500, trend Loss: 0.0451 | 0.0364
Epoch 174/500, trend Loss: 0.0451 | 0.0364
Epoch 175/500, trend Loss: 0.0451 | 0.0364
Epoch 176/500, trend Loss: 0.0451 | 0.0364
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/main.py", line 1037, in <module>
    models[comp], train_loss, valid_loss = train(model, train_data, valid_data, optimizer, criterion, scheduler, params['batch_size'], params['observation_period_num'])
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 32, in train
    output = model(data)
             ^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<@beartype(src.model.iTransformer.forward) at 0x7f1375b939c0>", line 66, in forward
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 190, in forward
    x = ff(x) + x
        ^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 77, in forward
    return x * F.gelu(gate)
               ^^^^^^^^^^^^
KeyboardInterrupt
