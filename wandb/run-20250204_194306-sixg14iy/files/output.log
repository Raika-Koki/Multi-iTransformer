[32m[I 2025-02-04 19:43:12,082][0m A new study created in memory with name: no-name-dea08537-de49-45c5-9304-7d66df9bf479[0m
[32m[I 2025-02-04 19:43:33,525][0m Trial 0 finished with value: 1.1056279913555012 and parameters: {'observation_period_num': 60, 'train_rates': 0.6262758282244111, 'learning_rate': 1.2154609501140749e-06, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9187161172825324}. Best is trial 0 with value: 1.1056279913555012.[0m
[32m[I 2025-02-04 19:44:16,602][0m Trial 1 finished with value: 0.05738779577740374 and parameters: {'observation_period_num': 51, 'train_rates': 0.9356593424603737, 'learning_rate': 0.00024258087992433486, 'batch_size': 145, 'step_size': 14, 'gamma': 0.7599189920851928}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:45:50,359][0m Trial 2 finished with value: 0.1276433547027409 and parameters: {'observation_period_num': 162, 'train_rates': 0.7881046896813607, 'learning_rate': 0.00014128960300255708, 'batch_size': 53, 'step_size': 7, 'gamma': 0.9367617544472626}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:46:12,541][0m Trial 3 finished with value: 1.1344579053374957 and parameters: {'observation_period_num': 30, 'train_rates': 0.7464911646974619, 'learning_rate': 3.096815608711929e-06, 'batch_size': 252, 'step_size': 2, 'gamma': 0.9469497776574224}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:46:35,844][0m Trial 4 finished with value: 0.7251053535312947 and parameters: {'observation_period_num': 173, 'train_rates': 0.6778022014830946, 'learning_rate': 1.2687455471078748e-05, 'batch_size': 223, 'step_size': 7, 'gamma': 0.8114789576083399}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:47:07,555][0m Trial 5 finished with value: 0.760377393856001 and parameters: {'observation_period_num': 203, 'train_rates': 0.7814316482060644, 'learning_rate': 5.25795822462907e-06, 'batch_size': 169, 'step_size': 6, 'gamma': 0.840014164660743}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:49:00,942][0m Trial 6 finished with value: 1.2350739920073268 and parameters: {'observation_period_num': 70, 'train_rates': 0.6246562068742693, 'learning_rate': 2.795476007415854e-05, 'batch_size': 39, 'step_size': 2, 'gamma': 0.9403418502257915}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:49:25,001][0m Trial 7 finished with value: 0.8040034228470176 and parameters: {'observation_period_num': 214, 'train_rates': 0.8132959752293241, 'learning_rate': 3.788824833518494e-06, 'batch_size': 234, 'step_size': 4, 'gamma': 0.7703467714914273}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:49:51,029][0m Trial 8 finished with value: 0.5513834444998789 and parameters: {'observation_period_num': 213, 'train_rates': 0.6160531087381426, 'learning_rate': 0.00015322389539572837, 'batch_size': 175, 'step_size': 3, 'gamma': 0.8977676675323825}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:50:15,171][0m Trial 9 finished with value: 0.7079152219945735 and parameters: {'observation_period_num': 103, 'train_rates': 0.8070804791935294, 'learning_rate': 2.231483289564293e-06, 'batch_size': 246, 'step_size': 14, 'gamma': 0.8000384064783095}. Best is trial 1 with value: 0.05738779577740374.[0m
[32m[I 2025-02-04 19:51:17,444][0m Trial 10 finished with value: 0.04742160589233333 and parameters: {'observation_period_num': 16, 'train_rates': 0.9655012172877074, 'learning_rate': 0.0009196347813339157, 'batch_size': 100, 'step_size': 15, 'gamma': 0.8636288547477715}. Best is trial 10 with value: 0.04742160589233333.[0m
[32m[I 2025-02-04 19:52:25,634][0m Trial 11 finished with value: 0.028041614219546318 and parameters: {'observation_period_num': 19, 'train_rates': 0.9809049838780127, 'learning_rate': 0.0008820302997851891, 'batch_size': 91, 'step_size': 15, 'gamma': 0.7524256093139925}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 19:53:36,539][0m Trial 12 finished with value: 0.030411165207624435 and parameters: {'observation_period_num': 7, 'train_rates': 0.9797826573901852, 'learning_rate': 0.0009486633025291047, 'batch_size': 89, 'step_size': 11, 'gamma': 0.867150864773878}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 19:54:46,963][0m Trial 13 finished with value: 0.030619175163997855 and parameters: {'observation_period_num': 7, 'train_rates': 0.8986123267151497, 'learning_rate': 0.0008990023210733625, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9861292245174571}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 19:55:44,423][0m Trial 14 finished with value: 0.09435110339628798 and parameters: {'observation_period_num': 103, 'train_rates': 0.882304714091378, 'learning_rate': 0.00038329985694809254, 'batch_size': 97, 'step_size': 11, 'gamma': 0.838311333918087}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 19:56:31,486][0m Trial 15 finished with value: 0.18173089623451233 and parameters: {'observation_period_num': 251, 'train_rates': 0.973416864017814, 'learning_rate': 6.41964810741374e-05, 'batch_size': 128, 'step_size': 11, 'gamma': 0.887600817192048}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 19:57:59,825][0m Trial 16 finished with value: 0.1641040360316252 and parameters: {'observation_period_num': 92, 'train_rates': 0.877364082475628, 'learning_rate': 0.0004326266574732642, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8168816296036022}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:01:27,561][0m Trial 17 finished with value: 0.04849418956372473 and parameters: {'observation_period_num': 38, 'train_rates': 0.987884368754711, 'learning_rate': 5.935398799224738e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.7858215750763272}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:02:18,041][0m Trial 18 finished with value: 0.1630023590741901 and parameters: {'observation_period_num': 132, 'train_rates': 0.9228702110253766, 'learning_rate': 0.000560168297362777, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8581951691029704}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:03:30,644][0m Trial 19 finished with value: 0.06046786543767933 and parameters: {'observation_period_num': 77, 'train_rates': 0.8484006552264166, 'learning_rate': 9.373364069545804e-05, 'batch_size': 74, 'step_size': 15, 'gamma': 0.7571875743752562}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:04:09,028][0m Trial 20 finished with value: 0.29902262394654683 and parameters: {'observation_period_num': 128, 'train_rates': 0.9366926128719322, 'learning_rate': 1.8785579529649127e-05, 'batch_size': 157, 'step_size': 9, 'gamma': 0.8953035243087234}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:05:19,175][0m Trial 21 finished with value: 0.031150774005175436 and parameters: {'observation_period_num': 5, 'train_rates': 0.8990762654012401, 'learning_rate': 0.0008244325121746312, 'batch_size': 84, 'step_size': 11, 'gamma': 0.964311870939744}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:06:14,042][0m Trial 22 finished with value: 0.09191135572107047 and parameters: {'observation_period_num': 28, 'train_rates': 0.9491746613809815, 'learning_rate': 0.0009771220725737577, 'batch_size': 113, 'step_size': 10, 'gamma': 0.9785740289338646}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:07:22,082][0m Trial 23 finished with value: 0.03298576907478135 and parameters: {'observation_period_num': 5, 'train_rates': 0.9082900912495455, 'learning_rate': 0.00026345320833068915, 'batch_size': 88, 'step_size': 13, 'gamma': 0.9155826832319718}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:09:21,443][0m Trial 24 finished with value: 0.041246868669986725 and parameters: {'observation_period_num': 47, 'train_rates': 0.9874700043757183, 'learning_rate': 0.00047922083222025124, 'batch_size': 51, 'step_size': 12, 'gamma': 0.9844375901792912}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:13:27,032][0m Trial 25 finished with value: 0.05530526385961476 and parameters: {'observation_period_num': 25, 'train_rates': 0.84948072422301, 'learning_rate': 0.0002291866512122785, 'batch_size': 22, 'step_size': 5, 'gamma': 0.8350138030902243}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:14:54,328][0m Trial 26 finished with value: 0.0700061650803456 and parameters: {'observation_period_num': 46, 'train_rates': 0.9554883013756352, 'learning_rate': 0.0006218574850699003, 'batch_size': 69, 'step_size': 10, 'gamma': 0.8752069075156977}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:15:24,412][0m Trial 27 finished with value: 0.042062207734898514 and parameters: {'observation_period_num': 8, 'train_rates': 0.8390181752248386, 'learning_rate': 0.00030573656997355563, 'batch_size': 202, 'step_size': 13, 'gamma': 0.779599636963681}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:16:08,610][0m Trial 28 finished with value: 0.15793427952959493 and parameters: {'observation_period_num': 82, 'train_rates': 0.8867941498609744, 'learning_rate': 0.00016798259800145728, 'batch_size': 132, 'step_size': 12, 'gamma': 0.9607374401411767}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:16:54,521][0m Trial 29 finished with value: 0.26803694751535384 and parameters: {'observation_period_num': 24, 'train_rates': 0.7105699271934736, 'learning_rate': 8.980100162062806e-06, 'batch_size': 110, 'step_size': 8, 'gamma': 0.9176623716792339}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:18:06,182][0m Trial 30 finished with value: 0.12424643230826958 and parameters: {'observation_period_num': 59, 'train_rates': 0.9206569407176461, 'learning_rate': 0.0005771598583175328, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8498013200731284}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:19:10,560][0m Trial 31 finished with value: 0.04448510434293578 and parameters: {'observation_period_num': 6, 'train_rates': 0.9046034909925448, 'learning_rate': 0.0008163271651937914, 'batch_size': 92, 'step_size': 11, 'gamma': 0.9668806783397385}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:20:52,590][0m Trial 32 finished with value: 0.06217281523277052 and parameters: {'observation_period_num': 39, 'train_rates': 0.9562978658696959, 'learning_rate': 0.0009095961896371053, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9630009524725012}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:21:35,553][0m Trial 33 finished with value: 0.13306690784367292 and parameters: {'observation_period_num': 58, 'train_rates': 0.8951434823304024, 'learning_rate': 0.000401844535225972, 'batch_size': 143, 'step_size': 10, 'gamma': 0.9896343759213221}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:23:58,287][0m Trial 34 finished with value: 0.04271528252866119 and parameters: {'observation_period_num': 21, 'train_rates': 0.8691309124230049, 'learning_rate': 0.0007082868075182602, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9251722253848845}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:25:10,716][0m Trial 35 finished with value: 0.11308563034410125 and parameters: {'observation_period_num': 38, 'train_rates': 0.9441152934957229, 'learning_rate': 9.819578543034236e-05, 'batch_size': 82, 'step_size': 15, 'gamma': 0.9503156765397748}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:26:02,052][0m Trial 36 finished with value: 0.4555971369664104 and parameters: {'observation_period_num': 15, 'train_rates': 0.7646741446017766, 'learning_rate': 1.1185084615258123e-06, 'batch_size': 103, 'step_size': 12, 'gamma': 0.973983178638283}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:26:41,354][0m Trial 37 finished with value: 0.22475799307395944 and parameters: {'observation_period_num': 66, 'train_rates': 0.6637646075300235, 'learning_rate': 0.0002232403591511912, 'batch_size': 126, 'step_size': 7, 'gamma': 0.8184382337249956}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:28:52,715][0m Trial 38 finished with value: 0.03501126219529704 and parameters: {'observation_period_num': 5, 'train_rates': 0.9720125641376951, 'learning_rate': 0.0003187752484573612, 'batch_size': 46, 'step_size': 13, 'gamma': 0.9494260924495583}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:30:11,966][0m Trial 39 finished with value: 0.047910052925756536 and parameters: {'observation_period_num': 32, 'train_rates': 0.9253431641332491, 'learning_rate': 0.00063660634922136, 'batch_size': 74, 'step_size': 11, 'gamma': 0.7503797830775284}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:31:00,250][0m Trial 40 finished with value: 0.05484471986272249 and parameters: {'observation_period_num': 54, 'train_rates': 0.8283216046375746, 'learning_rate': 0.0001874830946312235, 'batch_size': 116, 'step_size': 10, 'gamma': 0.7977226026933367}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:32:12,531][0m Trial 41 finished with value: 0.029773084052505283 and parameters: {'observation_period_num': 6, 'train_rates': 0.9072218505243346, 'learning_rate': 0.00033394660370060833, 'batch_size': 82, 'step_size': 14, 'gamma': 0.915148175214604}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:33:16,388][0m Trial 42 finished with value: 0.06648571623696221 and parameters: {'observation_period_num': 18, 'train_rates': 0.865137021288373, 'learning_rate': 0.0004678470469069798, 'batch_size': 88, 'step_size': 14, 'gamma': 0.9291582486918669}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:34:52,762][0m Trial 43 finished with value: 0.0473417786793559 and parameters: {'observation_period_num': 21, 'train_rates': 0.9350787683198166, 'learning_rate': 0.0007589988098488178, 'batch_size': 62, 'step_size': 14, 'gamma': 0.9075242083325324}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:35:45,585][0m Trial 44 finished with value: 0.17143242866840475 and parameters: {'observation_period_num': 184, 'train_rates': 0.9071585642754042, 'learning_rate': 0.0009224139718087925, 'batch_size': 107, 'step_size': 15, 'gamma': 0.8783281412996877}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:37:09,185][0m Trial 45 finished with value: 0.0364883616566658 and parameters: {'observation_period_num': 32, 'train_rates': 0.9826019376330584, 'learning_rate': 0.00034897768271061036, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9319091162453232}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:38:06,346][0m Trial 46 finished with value: 0.175903353955553 and parameters: {'observation_period_num': 14, 'train_rates': 0.7969200604386986, 'learning_rate': 0.0005829055737054213, 'batch_size': 97, 'step_size': 12, 'gamma': 0.9409663103630233}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:38:56,803][0m Trial 47 finished with value: 0.09104321151971817 and parameters: {'observation_period_num': 44, 'train_rates': 0.965550448519157, 'learning_rate': 0.00012855250652793262, 'batch_size': 123, 'step_size': 1, 'gamma': 0.9558216052371173}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:39:36,127][0m Trial 48 finished with value: 0.6224485898064724 and parameters: {'observation_period_num': 151, 'train_rates': 0.8185678329181144, 'learning_rate': 1.8295732914807406e-06, 'batch_size': 142, 'step_size': 8, 'gamma': 0.9677936912953682}. Best is trial 11 with value: 0.028041614219546318.[0m
[32m[I 2025-02-04 20:41:16,840][0m Trial 49 finished with value: 0.1799750575745428 and parameters: {'observation_period_num': 251, 'train_rates': 0.9180133020449377, 'learning_rate': 0.0009977318748518316, 'batch_size': 54, 'step_size': 13, 'gamma': 0.903404747407683}. Best is trial 11 with value: 0.028041614219546318.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3143 | 0.1793
Epoch 2/300, Loss: 0.1433 | 0.1075
Epoch 3/300, Loss: 0.1172 | 0.1037
Epoch 4/300, Loss: 0.1226 | 0.1033
Epoch 5/300, Loss: 0.1215 | 0.2534
Epoch 6/300, Loss: 0.1220 | 0.1680
Epoch 7/300, Loss: 0.1340 | 0.0982
Epoch 8/300, Loss: 0.1278 | 0.0998
Epoch 9/300, Loss: 0.1173 | 0.0925
Epoch 10/300, Loss: 0.1044 | 0.0997
Epoch 11/300, Loss: 0.1064 | 0.1025
Epoch 12/300, Loss: 0.1131 | 0.1075
Epoch 13/300, Loss: 0.1046 | 0.0683
Epoch 14/300, Loss: 0.1014 | 0.1065
Epoch 15/300, Loss: 0.1082 | 0.0847
Epoch 16/300, Loss: 0.1097 | 0.0877
Epoch 17/300, Loss: 0.1127 | 0.0975
Epoch 18/300, Loss: 0.1031 | 0.0803
Epoch 19/300, Loss: 0.1091 | 0.1198
Epoch 20/300, Loss: 0.1293 | 0.1272
Epoch 21/300, Loss: 0.1123 | 0.0697
Epoch 22/300, Loss: 0.0912 | 0.0641
Epoch 23/300, Loss: 0.1040 | 0.1008
Epoch 24/300, Loss: 0.1157 | 0.2220
Epoch 25/300, Loss: 0.0910 | 0.0763
Epoch 26/300, Loss: 0.0793 | 0.0674
Epoch 27/300, Loss: 0.0752 | 0.0623
Epoch 28/300, Loss: 0.0778 | 0.0628
Epoch 29/300, Loss: 0.0883 | 0.0504
Epoch 30/300, Loss: 0.0830 | 0.0508
Epoch 31/300, Loss: 0.0732 | 0.0461
Epoch 32/300, Loss: 0.0748 | 0.0457
Epoch 33/300, Loss: 0.0733 | 0.0425
Epoch 34/300, Loss: 0.0672 | 0.0359
Epoch 35/300, Loss: 0.0642 | 0.0330
Epoch 36/300, Loss: 0.0634 | 0.0312
Epoch 37/300, Loss: 0.0635 | 0.0302
Epoch 38/300, Loss: 0.0644 | 0.0300
Epoch 39/300, Loss: 0.0669 | 0.0346
Epoch 40/300, Loss: 0.0670 | 0.0360
Epoch 41/300, Loss: 0.0641 | 0.0351
Epoch 42/300, Loss: 0.0622 | 0.0317
Epoch 43/300, Loss: 0.0621 | 0.0294
Epoch 44/300, Loss: 0.0637 | 0.0287
Epoch 45/300, Loss: 0.0660 | 0.0296
Epoch 46/300, Loss: 0.0684 | 0.0290
Epoch 47/300, Loss: 0.0674 | 0.0307
Epoch 48/300, Loss: 0.0639 | 0.0353
Epoch 49/300, Loss: 0.0655 | 0.0403
Epoch 50/300, Loss: 0.0655 | 0.0342
Epoch 51/300, Loss: 0.0613 | 0.0310
Epoch 52/300, Loss: 0.0585 | 0.0282
Epoch 53/300, Loss: 0.0583 | 0.0271
Epoch 54/300, Loss: 0.0589 | 0.0269
Epoch 55/300, Loss: 0.0594 | 0.0270
Epoch 56/300, Loss: 0.0595 | 0.0274
Epoch 57/300, Loss: 0.0592 | 0.0277
Epoch 58/300, Loss: 0.0594 | 0.0280
Epoch 59/300, Loss: 0.0601 | 0.0281
Epoch 60/300, Loss: 0.0605 | 0.0282
Epoch 61/300, Loss: 0.0608 | 0.0292
Epoch 62/300, Loss: 0.0599 | 0.0278
Epoch 63/300, Loss: 0.0580 | 0.0276
Epoch 64/300, Loss: 0.0568 | 0.0277
Epoch 65/300, Loss: 0.0562 | 0.0274
Epoch 66/300, Loss: 0.0558 | 0.0271
Epoch 67/300, Loss: 0.0554 | 0.0267
Epoch 68/300, Loss: 0.0551 | 0.0263
Epoch 69/300, Loss: 0.0549 | 0.0259
Epoch 70/300, Loss: 0.0547 | 0.0255
Epoch 71/300, Loss: 0.0542 | 0.0252
Epoch 72/300, Loss: 0.0539 | 0.0248
Epoch 73/300, Loss: 0.0536 | 0.0245
Epoch 74/300, Loss: 0.0534 | 0.0244
Epoch 75/300, Loss: 0.0532 | 0.0242
Epoch 76/300, Loss: 0.0530 | 0.0242
Epoch 77/300, Loss: 0.0529 | 0.0243
Epoch 78/300, Loss: 0.0529 | 0.0244
Epoch 79/300, Loss: 0.0528 | 0.0244
Epoch 80/300, Loss: 0.0528 | 0.0245
Epoch 81/300, Loss: 0.0527 | 0.0246
Epoch 82/300, Loss: 0.0526 | 0.0246
Epoch 83/300, Loss: 0.0525 | 0.0246
Epoch 84/300, Loss: 0.0524 | 0.0246
Epoch 85/300, Loss: 0.0523 | 0.0243
Epoch 86/300, Loss: 0.0522 | 0.0240
Epoch 87/300, Loss: 0.0521 | 0.0238
Epoch 88/300, Loss: 0.0521 | 0.0237
Epoch 89/300, Loss: 0.0521 | 0.0236
Epoch 90/300, Loss: 0.0521 | 0.0236
Epoch 91/300, Loss: 0.0520 | 0.0235
Epoch 92/300, Loss: 0.0518 | 0.0238
Epoch 93/300, Loss: 0.0515 | 0.0238
Epoch 94/300, Loss: 0.0515 | 0.0239
Epoch 95/300, Loss: 0.0514 | 0.0238
Epoch 96/300, Loss: 0.0513 | 0.0238
Epoch 97/300, Loss: 0.0513 | 0.0237
Epoch 98/300, Loss: 0.0512 | 0.0236
Epoch 99/300, Loss: 0.0511 | 0.0236
Epoch 100/300, Loss: 0.0511 | 0.0235
Epoch 101/300, Loss: 0.0510 | 0.0235
Epoch 102/300, Loss: 0.0510 | 0.0235
Epoch 103/300, Loss: 0.0509 | 0.0235
Epoch 104/300, Loss: 0.0509 | 0.0235
Epoch 105/300, Loss: 0.0508 | 0.0235
Epoch 106/300, Loss: 0.0508 | 0.0235
Epoch 107/300, Loss: 0.0507 | 0.0235
Epoch 108/300, Loss: 0.0507 | 0.0235
Epoch 109/300, Loss: 0.0507 | 0.0235
Epoch 110/300, Loss: 0.0506 | 0.0235
Epoch 111/300, Loss: 0.0506 | 0.0235
Epoch 112/300, Loss: 0.0505 | 0.0235
Epoch 113/300, Loss: 0.0505 | 0.0234
Epoch 114/300, Loss: 0.0505 | 0.0235
Epoch 115/300, Loss: 0.0504 | 0.0235
Epoch 116/300, Loss: 0.0504 | 0.0235
Epoch 117/300, Loss: 0.0504 | 0.0234
Epoch 118/300, Loss: 0.0503 | 0.0234
Epoch 119/300, Loss: 0.0503 | 0.0234
Epoch 120/300, Loss: 0.0503 | 0.0234
Epoch 121/300, Loss: 0.0502 | 0.0234
Epoch 122/300, Loss: 0.0502 | 0.0234
Epoch 123/300, Loss: 0.0502 | 0.0234
Epoch 124/300, Loss: 0.0502 | 0.0234
Epoch 125/300, Loss: 0.0501 | 0.0234
Epoch 126/300, Loss: 0.0501 | 0.0234
Epoch 127/300, Loss: 0.0501 | 0.0234
Epoch 128/300, Loss: 0.0501 | 0.0234
Epoch 129/300, Loss: 0.0500 | 0.0234
Epoch 130/300, Loss: 0.0500 | 0.0234
Epoch 131/300, Loss: 0.0500 | 0.0234
Epoch 132/300, Loss: 0.0500 | 0.0234
Epoch 133/300, Loss: 0.0500 | 0.0234
Epoch 134/300, Loss: 0.0499 | 0.0234
Epoch 135/300, Loss: 0.0499 | 0.0233
Epoch 136/300, Loss: 0.0499 | 0.0234
Epoch 137/300, Loss: 0.0499 | 0.0234
Epoch 138/300, Loss: 0.0499 | 0.0233
Epoch 139/300, Loss: 0.0499 | 0.0233
Epoch 140/300, Loss: 0.0498 | 0.0233
Epoch 141/300, Loss: 0.0498 | 0.0233
Epoch 142/300, Loss: 0.0498 | 0.0233
Epoch 143/300, Loss: 0.0498 | 0.0233
Epoch 144/300, Loss: 0.0498 | 0.0233
Epoch 145/300, Loss: 0.0498 | 0.0233
Epoch 146/300, Loss: 0.0498 | 0.0233
Epoch 147/300, Loss: 0.0497 | 0.0233
Epoch 148/300, Loss: 0.0497 | 0.0233
Epoch 149/300, Loss: 0.0497 | 0.0233
Epoch 150/300, Loss: 0.0497 | 0.0233
Epoch 151/300, Loss: 0.0497 | 0.0233
Epoch 152/300, Loss: 0.0497 | 0.0233
Epoch 153/300, Loss: 0.0497 | 0.0233
Epoch 154/300, Loss: 0.0497 | 0.0233
Epoch 155/300, Loss: 0.0497 | 0.0233
Epoch 156/300, Loss: 0.0496 | 0.0233
Epoch 157/300, Loss: 0.0496 | 0.0233
Epoch 158/300, Loss: 0.0496 | 0.0233
Epoch 159/300, Loss: 0.0496 | 0.0233
Epoch 160/300, Loss: 0.0496 | 0.0233
Epoch 161/300, Loss: 0.0496 | 0.0233
Epoch 162/300, Loss: 0.0496 | 0.0233
Epoch 163/300, Loss: 0.0496 | 0.0233
Epoch 164/300, Loss: 0.0496 | 0.0233
Epoch 165/300, Loss: 0.0496 | 0.0233
Epoch 166/300, Loss: 0.0496 | 0.0233
Epoch 167/300, Loss: 0.0496 | 0.0233
Epoch 168/300, Loss: 0.0495 | 0.0233
Epoch 169/300, Loss: 0.0495 | 0.0233
Epoch 170/300, Loss: 0.0495 | 0.0233
Epoch 171/300, Loss: 0.0495 | 0.0233
Epoch 172/300, Loss: 0.0495 | 0.0233
Epoch 173/300, Loss: 0.0495 | 0.0233
Epoch 174/300, Loss: 0.0495 | 0.0233
Epoch 175/300, Loss: 0.0495 | 0.0233
Epoch 176/300, Loss: 0.0495 | 0.0233
Epoch 177/300, Loss: 0.0495 | 0.0233
Epoch 178/300, Loss: 0.0495 | 0.0233
Epoch 179/300, Loss: 0.0495 | 0.0233
Epoch 180/300, Loss: 0.0495 | 0.0233
Epoch 181/300, Loss: 0.0495 | 0.0233
Epoch 182/300, Loss: 0.0495 | 0.0233
Epoch 183/300, Loss: 0.0495 | 0.0233
Epoch 184/300, Loss: 0.0495 | 0.0233
Epoch 185/300, Loss: 0.0495 | 0.0233
Epoch 186/300, Loss: 0.0495 | 0.0233
Epoch 187/300, Loss: 0.0495 | 0.0233
Epoch 188/300, Loss: 0.0495 | 0.0233
Epoch 189/300, Loss: 0.0494 | 0.0233
Epoch 190/300, Loss: 0.0494 | 0.0233
Epoch 191/300, Loss: 0.0494 | 0.0233
Epoch 192/300, Loss: 0.0494 | 0.0233
Epoch 193/300, Loss: 0.0494 | 0.0233
Epoch 194/300, Loss: 0.0494 | 0.0233
Epoch 195/300, Loss: 0.0494 | 0.0233
Epoch 196/300, Loss: 0.0494 | 0.0233
Epoch 197/300, Loss: 0.0494 | 0.0233
Epoch 198/300, Loss: 0.0494 | 0.0233
Epoch 199/300, Loss: 0.0494 | 0.0233
Epoch 200/300, Loss: 0.0494 | 0.0233
Epoch 201/300, Loss: 0.0494 | 0.0233
Epoch 202/300, Loss: 0.0494 | 0.0233
Epoch 203/300, Loss: 0.0494 | 0.0233
Epoch 204/300, Loss: 0.0494 | 0.0233
Epoch 205/300, Loss: 0.0494 | 0.0233
Epoch 206/300, Loss: 0.0494 | 0.0233
Epoch 207/300, Loss: 0.0494 | 0.0233
Epoch 208/300, Loss: 0.0494 | 0.0233
Epoch 209/300, Loss: 0.0494 | 0.0233
Epoch 210/300, Loss: 0.0494 | 0.0233
Epoch 211/300, Loss: 0.0494 | 0.0233
Epoch 212/300, Loss: 0.0494 | 0.0233
Epoch 213/300, Loss: 0.0494 | 0.0233
Epoch 214/300, Loss: 0.0494 | 0.0233
Epoch 215/300, Loss: 0.0494 | 0.0233
Epoch 216/300, Loss: 0.0494 | 0.0233
Epoch 217/300, Loss: 0.0494 | 0.0233
Epoch 218/300, Loss: 0.0494 | 0.0233
Epoch 219/300, Loss: 0.0494 | 0.0233
Epoch 220/300, Loss: 0.0494 | 0.0233
Epoch 221/300, Loss: 0.0494 | 0.0233
Epoch 222/300, Loss: 0.0494 | 0.0233
Epoch 223/300, Loss: 0.0494 | 0.0233
Epoch 224/300, Loss: 0.0494 | 0.0233
Epoch 225/300, Loss: 0.0494 | 0.0233
Epoch 226/300, Loss: 0.0494 | 0.0233
Epoch 227/300, Loss: 0.0494 | 0.0233
Epoch 228/300, Loss: 0.0494 | 0.0233
Epoch 229/300, Loss: 0.0494 | 0.0233
Epoch 230/300, Loss: 0.0494 | 0.0233
Epoch 231/300, Loss: 0.0494 | 0.0233
Epoch 232/300, Loss: 0.0494 | 0.0233
Epoch 233/300, Loss: 0.0494 | 0.0233
Epoch 234/300, Loss: 0.0494 | 0.0233
Epoch 235/300, Loss: 0.0494 | 0.0233
Epoch 236/300, Loss: 0.0494 | 0.0233
Epoch 237/300, Loss: 0.0494 | 0.0233
Epoch 238/300, Loss: 0.0494 | 0.0233
Epoch 239/300, Loss: 0.0494 | 0.0233
Epoch 240/300, Loss: 0.0494 | 0.0233
Epoch 241/300, Loss: 0.0494 | 0.0233
Epoch 242/300, Loss: 0.0494 | 0.0233
Epoch 243/300, Loss: 0.0494 | 0.0233
Epoch 244/300, Loss: 0.0494 | 0.0233
Epoch 245/300, Loss: 0.0494 | 0.0233
Epoch 246/300, Loss: 0.0494 | 0.0233
Epoch 247/300, Loss: 0.0494 | 0.0233
Epoch 248/300, Loss: 0.0494 | 0.0233
Epoch 249/300, Loss: 0.0494 | 0.0233
Epoch 250/300, Loss: 0.0494 | 0.0233
Epoch 251/300, Loss: 0.0494 | 0.0233
Epoch 252/300, Loss: 0.0494 | 0.0233
Epoch 253/300, Loss: 0.0494 | 0.0233
Epoch 254/300, Loss: 0.0494 | 0.0233
Epoch 255/300, Loss: 0.0494 | 0.0233
Epoch 256/300, Loss: 0.0494 | 0.0233
Epoch 257/300, Loss: 0.0494 | 0.0233
Epoch 258/300, Loss: 0.0494 | 0.0233
Epoch 259/300, Loss: 0.0494 | 0.0233
Epoch 260/300, Loss: 0.0494 | 0.0233
Epoch 261/300, Loss: 0.0494 | 0.0233
Epoch 262/300, Loss: 0.0494 | 0.0233
Epoch 263/300, Loss: 0.0494 | 0.0233
Epoch 264/300, Loss: 0.0494 | 0.0233
Epoch 265/300, Loss: 0.0494 | 0.0233
Epoch 266/300, Loss: 0.0494 | 0.0233
Epoch 267/300, Loss: 0.0494 | 0.0233
Epoch 268/300, Loss: 0.0494 | 0.0233
Epoch 269/300, Loss: 0.0494 | 0.0233
Epoch 270/300, Loss: 0.0494 | 0.0233
Epoch 271/300, Loss: 0.0494 | 0.0233
Epoch 272/300, Loss: 0.0494 | 0.0233
Epoch 273/300, Loss: 0.0494 | 0.0233
Epoch 274/300, Loss: 0.0494 | 0.0233
Epoch 275/300, Loss: 0.0494 | 0.0233
Epoch 276/300, Loss: 0.0494 | 0.0233
Epoch 277/300, Loss: 0.0494 | 0.0233
Epoch 278/300, Loss: 0.0494 | 0.0233
Epoch 279/300, Loss: 0.0494 | 0.0233
Epoch 280/300, Loss: 0.0494 | 0.0233
Epoch 281/300, Loss: 0.0494 | 0.0233
Epoch 282/300, Loss: 0.0494 | 0.0233
Epoch 283/300, Loss: 0.0494 | 0.0233
Epoch 284/300, Loss: 0.0494 | 0.0233
Epoch 285/300, Loss: 0.0494 | 0.0233
Epoch 286/300, Loss: 0.0494 | 0.0233
Epoch 287/300, Loss: 0.0494 | 0.0233
Epoch 288/300, Loss: 0.0494 | 0.0233
Epoch 289/300, Loss: 0.0494 | 0.0233
Epoch 290/300, Loss: 0.0494 | 0.0233
Epoch 291/300, Loss: 0.0494 | 0.0233
Epoch 292/300, Loss: 0.0494 | 0.0233
Epoch 293/300, Loss: 0.0494 | 0.0233
Epoch 294/300, Loss: 0.0494 | 0.0233
Epoch 295/300, Loss: 0.0494 | 0.0233
Epoch 296/300, Loss: 0.0494 | 0.0233
Epoch 297/300, Loss: 0.0494 | 0.0233
Epoch 298/300, Loss: 0.0494 | 0.0233
Epoch 299/300, Loss: 0.0494 | 0.0233
Epoch 300/300, Loss: 0.0494 | 0.0233
Runtime (seconds): 203.7949640750885
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 27.083943620324135
RMSE: 5.2042236328125
MAE: 5.2042236328125
R-squared: nan
[165.42221]
