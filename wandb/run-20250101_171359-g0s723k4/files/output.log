ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-01 17:14:02,620][0m A new study created in memory with name: no-name-05fdd074-c593-412a-84a6-baf4f90eb403[0m
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[32m[I 2025-01-01 17:14:42,015][0m Trial 0 finished with value: 0.11540796817877354 and parameters: {'observation_period_num': 42, 'train_rates': 0.8797686309527979, 'learning_rate': 3.3431888598035975e-06, 'batch_size': 159, 'step_size': 6, 'gamma': 0.9797107244685517}. Best is trial 0 with value: 0.11540796817877354.[0m
[32m[I 2025-01-01 17:15:23,590][0m Trial 1 finished with value: 0.058836441274200166 and parameters: {'observation_period_num': 25, 'train_rates': 0.928603261254755, 'learning_rate': 1.9654437024112233e-05, 'batch_size': 156, 'step_size': 7, 'gamma': 0.8986487584646118}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:16:14,881][0m Trial 2 finished with value: 0.3397170684603981 and parameters: {'observation_period_num': 106, 'train_rates': 0.6168402169057831, 'learning_rate': 2.0672201541263043e-05, 'batch_size': 86, 'step_size': 3, 'gamma': 0.8140034577878483}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:16:49,284][0m Trial 3 finished with value: 0.14077231287956238 and parameters: {'observation_period_num': 50, 'train_rates': 0.9618822985532803, 'learning_rate': 1.3193565643288262e-05, 'batch_size': 247, 'step_size': 13, 'gamma': 0.8282036788777287}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:17:19,267][0m Trial 4 finished with value: 0.13149243027999483 and parameters: {'observation_period_num': 217, 'train_rates': 0.8305761391127991, 'learning_rate': 0.00016104493330991294, 'batch_size': 248, 'step_size': 12, 'gamma': 0.9843014912015519}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:17:50,796][0m Trial 5 finished with value: 0.14183905083764267 and parameters: {'observation_period_num': 226, 'train_rates': 0.7904389237658008, 'learning_rate': 0.0005268401222279594, 'batch_size': 181, 'step_size': 12, 'gamma': 0.8939659077063382}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:18:20,538][0m Trial 6 finished with value: 0.12181275462110837 and parameters: {'observation_period_num': 213, 'train_rates': 0.8229163484706115, 'learning_rate': 0.00011333713839066782, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8111513641655163}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:18:53,827][0m Trial 7 finished with value: 0.093450081544091 and parameters: {'observation_period_num': 135, 'train_rates': 0.8598658690563372, 'learning_rate': 0.00038510507489578386, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8304729263277961}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:19:32,557][0m Trial 8 finished with value: 0.07546391007946987 and parameters: {'observation_period_num': 144, 'train_rates': 0.8185628192146209, 'learning_rate': 0.00013040365283783104, 'batch_size': 143, 'step_size': 10, 'gamma': 0.836831706485552}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:20:06,101][0m Trial 9 finished with value: 0.23742388688559235 and parameters: {'observation_period_num': 63, 'train_rates': 0.772231552993619, 'learning_rate': 9.779950911545894e-05, 'batch_size': 176, 'step_size': 13, 'gamma': 0.9332798322675233}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:22:03,790][0m Trial 10 finished with value: 0.12095060606952757 and parameters: {'observation_period_num': 6, 'train_rates': 0.9785839031630403, 'learning_rate': 1.1309149660813312e-06, 'batch_size': 51, 'step_size': 5, 'gamma': 0.8965699642566136}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:22:44,083][0m Trial 11 finished with value: 0.27765941167612895 and parameters: {'observation_period_num': 154, 'train_rates': 0.7048624937872763, 'learning_rate': 3.6195142497144064e-05, 'batch_size': 120, 'step_size': 9, 'gamma': 0.7592066646391865}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:23:30,345][0m Trial 12 finished with value: 0.21116165297501016 and parameters: {'observation_period_num': 174, 'train_rates': 0.9051371726790743, 'learning_rate': 7.41678424047418e-06, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8608046205392201}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:24:34,610][0m Trial 13 finished with value: 0.22422600046890537 and parameters: {'observation_period_num': 103, 'train_rates': 0.7268025328205865, 'learning_rate': 5.129048675445259e-05, 'batch_size': 76, 'step_size': 10, 'gamma': 0.7601675020247005}. Best is trial 1 with value: 0.058836441274200166.[0m
[32m[I 2025-01-01 17:25:17,379][0m Trial 14 finished with value: 0.057681398166389 and parameters: {'observation_period_num': 94, 'train_rates': 0.928499666284234, 'learning_rate': 0.0008350934474368793, 'batch_size': 142, 'step_size': 1, 'gamma': 0.9297912774178537}. Best is trial 14 with value: 0.057681398166389.[0m
[32m[I 2025-01-01 17:26:14,818][0m Trial 15 finished with value: 0.028388584389141873 and parameters: {'observation_period_num': 7, 'train_rates': 0.929257805642402, 'learning_rate': 0.000795510814674034, 'batch_size': 101, 'step_size': 1, 'gamma': 0.9388845509099997}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:31:03,689][0m Trial 16 finished with value: 0.047781971339136364 and parameters: {'observation_period_num': 84, 'train_rates': 0.9303244414275876, 'learning_rate': 0.0008623038865444364, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9443898641966085}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:35:57,755][0m Trial 17 finished with value: 0.07355755390155883 and parameters: {'observation_period_num': 74, 'train_rates': 0.9710575941902598, 'learning_rate': 0.0003122732125113851, 'batch_size': 19, 'step_size': 1, 'gamma': 0.9482765965690527}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:40:29,816][0m Trial 18 finished with value: 0.05066018007444532 and parameters: {'observation_period_num': 10, 'train_rates': 0.879376454029192, 'learning_rate': 0.0008338456643629324, 'batch_size': 20, 'step_size': 3, 'gamma': 0.9622355087136554}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:41:29,861][0m Trial 19 finished with value: 0.10823898297763362 and parameters: {'observation_period_num': 181, 'train_rates': 0.9300830213295251, 'learning_rate': 0.0009940178697571316, 'batch_size': 95, 'step_size': 3, 'gamma': 0.9092450309471456}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:43:04,342][0m Trial 20 finished with value: 0.17362935360814913 and parameters: {'observation_period_num': 81, 'train_rates': 0.6161959723096246, 'learning_rate': 0.00022237174093936538, 'batch_size': 45, 'step_size': 15, 'gamma': 0.8703534416735854}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:48:33,702][0m Trial 21 finished with value: 0.028625069012822107 and parameters: {'observation_period_num': 6, 'train_rates': 0.8836436076351581, 'learning_rate': 0.0005423545330186032, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9598358905293375}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:50:27,283][0m Trial 22 finished with value: 0.052465234139953 and parameters: {'observation_period_num': 35, 'train_rates': 0.8983603459597977, 'learning_rate': 0.00038335481310334364, 'batch_size': 48, 'step_size': 2, 'gamma': 0.9591634619512318}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:51:51,836][0m Trial 23 finished with value: 0.06984460859061921 and parameters: {'observation_period_num': 26, 'train_rates': 0.850719718716128, 'learning_rate': 0.0005548811547190635, 'batch_size': 63, 'step_size': 4, 'gamma': 0.930498919180079}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:52:49,353][0m Trial 24 finished with value: 0.06650036374095139 and parameters: {'observation_period_num': 56, 'train_rates': 0.9461554261202051, 'learning_rate': 7.169525783427693e-05, 'batch_size': 101, 'step_size': 1, 'gamma': 0.9705661156237619}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:55:37,105][0m Trial 25 finished with value: 0.11129236221313477 and parameters: {'observation_period_num': 115, 'train_rates': 0.9897905692912194, 'learning_rate': 0.00022174742106097464, 'batch_size': 33, 'step_size': 5, 'gamma': 0.9445806157885599}. Best is trial 15 with value: 0.028388584389141873.[0m
[32m[I 2025-01-01 17:56:50,911][0m Trial 26 finished with value: 0.023703171354932415 and parameters: {'observation_period_num': 5, 'train_rates': 0.9040558390404843, 'learning_rate': 0.0005819606607873501, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9161717379917367}. Best is trial 26 with value: 0.023703171354932415.[0m
[32m[I 2025-01-01 17:58:05,818][0m Trial 27 finished with value: 0.02620842769032433 and parameters: {'observation_period_num': 16, 'train_rates': 0.900009539704961, 'learning_rate': 0.00027319845986334694, 'batch_size': 70, 'step_size': 4, 'gamma': 0.9182514707201783}. Best is trial 26 with value: 0.023703171354932415.[0m
[32m[I 2025-01-01 17:58:52,919][0m Trial 28 finished with value: 0.15653123302888056 and parameters: {'observation_period_num': 25, 'train_rates': 0.7309436756759553, 'learning_rate': 0.00022736946922021975, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9140255448893037}. Best is trial 26 with value: 0.023703171354932415.[0m
[32m[I 2025-01-01 18:00:13,283][0m Trial 29 finished with value: 0.38271109641884726 and parameters: {'observation_period_num': 47, 'train_rates': 0.904497641171745, 'learning_rate': 2.134859629116795e-06, 'batch_size': 71, 'step_size': 2, 'gamma': 0.8711171855125346}. Best is trial 26 with value: 0.023703171354932415.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-01 18:00:13,288][0m A new study created in memory with name: no-name-9d716697-04dc-4b57-b1e0-9f118d0f87ff[0m
[32m[I 2025-01-01 18:03:41,392][0m Trial 0 finished with value: 0.29498611333206143 and parameters: {'observation_period_num': 87, 'train_rates': 0.7239893164387139, 'learning_rate': 4.278251098531465e-06, 'batch_size': 167, 'step_size': 9, 'gamma': 0.94781996486686}. Best is trial 0 with value: 0.29498611333206143.[0m
[32m[I 2025-01-01 18:07:25,618][0m Trial 1 finished with value: 0.2795023213857892 and parameters: {'observation_period_num': 130, 'train_rates': 0.6922648091999651, 'learning_rate': 0.00038062679019998933, 'batch_size': 136, 'step_size': 2, 'gamma': 0.8080436714973677}. Best is trial 1 with value: 0.2795023213857892.[0m
[32m[I 2025-01-01 18:10:54,656][0m Trial 2 finished with value: 0.6121712932918245 and parameters: {'observation_period_num': 173, 'train_rates': 0.6201038112295136, 'learning_rate': 1.3964965864149916e-05, 'batch_size': 126, 'step_size': 4, 'gamma': 0.9663801935317509}. Best is trial 1 with value: 0.2795023213857892.[0m
[32m[I 2025-01-01 18:15:35,189][0m Trial 3 finished with value: 0.10498186200857162 and parameters: {'observation_period_num': 119, 'train_rates': 0.956003304887036, 'learning_rate': 0.000400168971596767, 'batch_size': 125, 'step_size': 4, 'gamma': 0.7759437349555255}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:19:48,982][0m Trial 4 finished with value: 0.1289452463388443 and parameters: {'observation_period_num': 200, 'train_rates': 0.978495652143297, 'learning_rate': 0.00033549125825067946, 'batch_size': 163, 'step_size': 3, 'gamma': 0.9535707163966983}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:23:26,437][0m Trial 5 finished with value: 0.18716964940722866 and parameters: {'observation_period_num': 25, 'train_rates': 0.7387594665167657, 'learning_rate': 0.0001237368486049112, 'batch_size': 195, 'step_size': 5, 'gamma': 0.9497220219029482}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:27:09,562][0m Trial 6 finished with value: 0.1281139979121882 and parameters: {'observation_period_num': 202, 'train_rates': 0.8812460175410244, 'learning_rate': 0.00026242108598491145, 'batch_size': 228, 'step_size': 4, 'gamma': 0.7693106763711571}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:30:41,152][0m Trial 7 finished with value: 0.18050470598267787 and parameters: {'observation_period_num': 250, 'train_rates': 0.8537927964056028, 'learning_rate': 8.151930398232529e-05, 'batch_size': 233, 'step_size': 2, 'gamma': 0.883958413854819}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:34:56,532][0m Trial 8 finished with value: 0.18114242686165705 and parameters: {'observation_period_num': 14, 'train_rates': 0.7247416149170662, 'learning_rate': 0.0005442155646297893, 'batch_size': 117, 'step_size': 2, 'gamma': 0.8145613868132484}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:39:05,369][0m Trial 9 finished with value: 0.13396500051021576 and parameters: {'observation_period_num': 205, 'train_rates': 0.9806244165731974, 'learning_rate': 3.577950444868788e-05, 'batch_size': 174, 'step_size': 12, 'gamma': 0.7568249942590405}. Best is trial 3 with value: 0.10498186200857162.[0m
[32m[I 2025-01-01 18:48:41,068][0m Trial 10 finished with value: 0.09176846454883444 and parameters: {'observation_period_num': 82, 'train_rates': 0.8992653952689819, 'learning_rate': 1.0401018570627167e-06, 'batch_size': 47, 'step_size': 15, 'gamma': 0.8727625115923441}. Best is trial 10 with value: 0.09176846454883444.[0m
[32m[I 2025-01-01 19:00:46,173][0m Trial 11 finished with value: 0.09377180298917183 and parameters: {'observation_period_num': 89, 'train_rates': 0.9027001364053708, 'learning_rate': 1.0633216625787531e-06, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8853018339211292}. Best is trial 10 with value: 0.09176846454883444.[0m
[32m[I 2025-01-01 19:15:09,195][0m Trial 12 finished with value: 0.07516475456082318 and parameters: {'observation_period_num': 65, 'train_rates': 0.8850902423169612, 'learning_rate': 1.1317342549645233e-06, 'batch_size': 31, 'step_size': 15, 'gamma': 0.8828137706007977}. Best is trial 12 with value: 0.07516475456082318.[0m
[32m[I 2025-01-01 19:36:15,192][0m Trial 13 finished with value: 0.07510569491065465 and parameters: {'observation_period_num': 55, 'train_rates': 0.8207153942344829, 'learning_rate': 1.0724751609183368e-06, 'batch_size': 20, 'step_size': 15, 'gamma': 0.8521942936630248}. Best is trial 13 with value: 0.07510569491065465.[0m
[32m[I 2025-01-01 19:42:29,452][0m Trial 14 finished with value: 0.06773553333847399 and parameters: {'observation_period_num': 43, 'train_rates': 0.829091198693933, 'learning_rate': 3.989834832697153e-06, 'batch_size': 70, 'step_size': 12, 'gamma': 0.8432334285158202}. Best is trial 14 with value: 0.06773553333847399.[0m
[32m[I 2025-01-01 19:47:58,389][0m Trial 15 finished with value: 0.10926521422392292 and parameters: {'observation_period_num': 46, 'train_rates': 0.7860252489405897, 'learning_rate': 4.441450102554597e-06, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8357113078430731}. Best is trial 14 with value: 0.06773553333847399.[0m
[32m[I 2025-01-01 19:54:17,044][0m Trial 16 finished with value: 0.046525656191481884 and parameters: {'observation_period_num': 6, 'train_rates': 0.8193258050419413, 'learning_rate': 3.9362853365757275e-06, 'batch_size': 71, 'step_size': 12, 'gamma': 0.918542234527121}. Best is trial 16 with value: 0.046525656191481884.[0m
[32m[I 2025-01-01 19:59:50,984][0m Trial 17 finished with value: 0.2372446636999807 and parameters: {'observation_period_num': 5, 'train_rates': 0.7898185226351392, 'learning_rate': 5.297930367699709e-06, 'batch_size': 80, 'step_size': 8, 'gamma': 0.9115761399631038}. Best is trial 16 with value: 0.046525656191481884.[0m
[32m[I 2025-01-01 20:05:17,620][0m Trial 18 finished with value: 0.06882726028561592 and parameters: {'observation_period_num': 34, 'train_rates': 0.8297396388003281, 'learning_rate': 1.2676492475388147e-05, 'batch_size': 83, 'step_size': 12, 'gamma': 0.916388429478231}. Best is trial 16 with value: 0.046525656191481884.[0m
[32m[I 2025-01-01 20:11:57,053][0m Trial 19 finished with value: 0.3210033440895109 and parameters: {'observation_period_num': 138, 'train_rates': 0.7643044961492473, 'learning_rate': 2.7414752854735574e-06, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9173260393949083}. Best is trial 16 with value: 0.046525656191481884.[0m
[32m[I 2025-01-01 20:15:53,859][0m Trial 20 finished with value: 0.17134011974654248 and parameters: {'observation_period_num': 35, 'train_rates': 0.6678679206143032, 'learning_rate': 1.2865611825456495e-05, 'batch_size': 109, 'step_size': 7, 'gamma': 0.9890784436158244}. Best is trial 16 with value: 0.046525656191481884.[0m
[32m[I 2025-01-01 20:21:47,602][0m Trial 21 finished with value: 0.06294207167363566 and parameters: {'observation_period_num': 36, 'train_rates': 0.8364260901646333, 'learning_rate': 1.0508190103833723e-05, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9161416048034923}. Best is trial 16 with value: 0.046525656191481884.[0m
[32m[I 2025-01-01 20:28:35,586][0m Trial 22 finished with value: 0.04017289434909349 and parameters: {'observation_period_num': 6, 'train_rates': 0.8284101594132065, 'learning_rate': 8.27321775697815e-06, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9109783600725903}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 20:33:49,311][0m Trial 23 finished with value: 0.04065201118820072 and parameters: {'observation_period_num': 10, 'train_rates': 0.9345560807255147, 'learning_rate': 2.399084108547032e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.9053283378529648}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 20:39:09,420][0m Trial 24 finished with value: 0.059956024196696156 and parameters: {'observation_period_num': 19, 'train_rates': 0.9363413257021538, 'learning_rate': 3.0923542514656756e-05, 'batch_size': 98, 'step_size': 10, 'gamma': 0.9274581617378684}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 20:44:14,056][0m Trial 25 finished with value: 0.07869233882852963 and parameters: {'observation_period_num': 67, 'train_rates': 0.9202993575367635, 'learning_rate': 2.6136282028991607e-05, 'batch_size': 99, 'step_size': 7, 'gamma': 0.8929346332560315}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 20:52:48,072][0m Trial 26 finished with value: 0.04458472654031417 and parameters: {'observation_period_num': 15, 'train_rates': 0.868219218261129, 'learning_rate': 7.135001650821063e-06, 'batch_size': 53, 'step_size': 13, 'gamma': 0.9338371776966343}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 21:01:00,228][0m Trial 27 finished with value: 0.09464159398765228 and parameters: {'observation_period_num': 106, 'train_rates': 0.8655292808645697, 'learning_rate': 6.601393442012272e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.9376603195491838}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 21:26:12,007][0m Trial 28 finished with value: 0.09058733547435087 and parameters: {'observation_period_num': 66, 'train_rates': 0.9473522334361195, 'learning_rate': 6.789635010568628e-06, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9001953787735679}. Best is trial 22 with value: 0.04017289434909349.[0m
[32m[I 2025-01-01 21:30:27,433][0m Trial 29 finished with value: 0.14260896577303153 and parameters: {'observation_period_num': 92, 'train_rates': 0.9124097429396986, 'learning_rate': 2.0408503548502583e-05, 'batch_size': 146, 'step_size': 10, 'gamma': 0.9788081023168264}. Best is trial 22 with value: 0.04017289434909349.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-01 21:30:27,438][0m A new study created in memory with name: no-name-48f743ec-9ff8-408f-b35f-aea1352c3f7f[0m
[32m[I 2025-01-01 21:34:38,654][0m Trial 0 finished with value: 0.11724747307747009 and parameters: {'observation_period_num': 101, 'train_rates': 0.9123198695577903, 'learning_rate': 3.098770071712602e-06, 'batch_size': 160, 'step_size': 10, 'gamma': 0.8679934327072254}. Best is trial 0 with value: 0.11724747307747009.[0m
[32m[I 2025-01-01 21:38:40,053][0m Trial 1 finished with value: 0.18782885620991388 and parameters: {'observation_period_num': 208, 'train_rates': 0.9302715452431524, 'learning_rate': 9.470288356900006e-06, 'batch_size': 160, 'step_size': 2, 'gamma': 0.91131143115703}. Best is trial 0 with value: 0.11724747307747009.[0m
[32m[I 2025-01-01 21:42:27,370][0m Trial 2 finished with value: 0.10012860430146424 and parameters: {'observation_period_num': 20, 'train_rates': 0.8181776199595332, 'learning_rate': 5.0508734673123195e-06, 'batch_size': 191, 'step_size': 5, 'gamma': 0.7604200132025519}. Best is trial 2 with value: 0.10012860430146424.[0m
[32m[I 2025-01-01 21:54:22,293][0m Trial 3 finished with value: 0.055540086523900696 and parameters: {'observation_period_num': 34, 'train_rates': 0.8602024372945494, 'learning_rate': 3.1927752456173613e-06, 'batch_size': 37, 'step_size': 11, 'gamma': 0.8678589061570386}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 21:58:01,079][0m Trial 4 finished with value: 0.29882168493123296 and parameters: {'observation_period_num': 157, 'train_rates': 0.6652989655571536, 'learning_rate': 0.000497505788339525, 'batch_size': 124, 'step_size': 5, 'gamma': 0.7748894720195785}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 22:01:07,612][0m Trial 5 finished with value: 0.27281999862396683 and parameters: {'observation_period_num': 81, 'train_rates': 0.6315353951830861, 'learning_rate': 0.00015611287546694996, 'batch_size': 203, 'step_size': 5, 'gamma': 0.9569351032353075}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 22:07:15,462][0m Trial 6 finished with value: 0.22264130408172128 and parameters: {'observation_period_num': 49, 'train_rates': 0.760969692123419, 'learning_rate': 0.0003220054358563567, 'batch_size': 67, 'step_size': 8, 'gamma': 0.7829608425206}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 22:10:29,688][0m Trial 7 finished with value: 0.5185592701909137 and parameters: {'observation_period_num': 236, 'train_rates': 0.6856895127948434, 'learning_rate': 2.846533646079405e-06, 'batch_size': 232, 'step_size': 5, 'gamma': 0.9157797663611162}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 22:13:53,970][0m Trial 8 finished with value: 0.45342256819735693 and parameters: {'observation_period_num': 242, 'train_rates': 0.7361772158887874, 'learning_rate': 3.4718999899765384e-06, 'batch_size': 180, 'step_size': 13, 'gamma': 0.8401252128258223}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 22:17:54,957][0m Trial 9 finished with value: 1.728242290467353 and parameters: {'observation_period_num': 180, 'train_rates': 0.6736422889074991, 'learning_rate': 0.0009072139210591779, 'batch_size': 101, 'step_size': 15, 'gamma': 0.8753290487195453}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 22:45:31,078][0m Trial 10 finished with value: 0.0675296587869525 and parameters: {'observation_period_num': 5, 'train_rates': 0.8482713772122231, 'learning_rate': 3.173653183278004e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9811103420579935}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 23:07:46,638][0m Trial 11 finished with value: 0.05738170584663749 and parameters: {'observation_period_num': 15, 'train_rates': 0.8477203699085466, 'learning_rate': 3.530299735124648e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.984837321754421}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 23:35:25,064][0m Trial 12 finished with value: 0.07599847705236502 and parameters: {'observation_period_num': 59, 'train_rates': 0.9881215490597604, 'learning_rate': 3.351410985872785e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8225509837423184}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 23:42:57,125][0m Trial 13 finished with value: 0.1334145144804528 and parameters: {'observation_period_num': 124, 'train_rates': 0.8659172837646573, 'learning_rate': 1.0317038688923626e-06, 'batch_size': 58, 'step_size': 13, 'gamma': 0.9216591280471936}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 23:50:05,596][0m Trial 14 finished with value: 0.07839431800865775 and parameters: {'observation_period_num': 47, 'train_rates': 0.7906663950076462, 'learning_rate': 8.230041539119916e-05, 'batch_size': 60, 'step_size': 8, 'gamma': 0.8751249026877213}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-01 23:55:28,112][0m Trial 15 finished with value: 0.0694973819243368 and parameters: {'observation_period_num': 25, 'train_rates': 0.902174404070216, 'learning_rate': 1.3306060317580167e-05, 'batch_size': 92, 'step_size': 12, 'gamma': 0.9518719337935131}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-02 00:07:52,060][0m Trial 16 finished with value: 0.1578467349920954 and parameters: {'observation_period_num': 84, 'train_rates': 0.9709275559615964, 'learning_rate': 7.256672004986302e-05, 'batch_size': 38, 'step_size': 15, 'gamma': 0.8383501569668635}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-02 00:12:56,187][0m Trial 17 finished with value: 0.12134823703498983 and parameters: {'observation_period_num': 126, 'train_rates': 0.857878278322882, 'learning_rate': 1.4857200544812177e-05, 'batch_size': 93, 'step_size': 9, 'gamma': 0.9885974567311304}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-02 00:26:59,312][0m Trial 18 finished with value: 0.06823681717719174 and parameters: {'observation_period_num': 37, 'train_rates': 0.801996797457267, 'learning_rate': 1.2626531831614012e-06, 'batch_size': 30, 'step_size': 13, 'gamma': 0.8071559275748433}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-02 00:31:12,996][0m Trial 19 finished with value: 0.15482245315142604 and parameters: {'observation_period_num': 6, 'train_rates': 0.734865569821982, 'learning_rate': 8.061264610951992e-05, 'batch_size': 127, 'step_size': 7, 'gamma': 0.8849252330174394}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-02 00:41:01,538][0m Trial 20 finished with value: 0.10594995182995894 and parameters: {'observation_period_num': 64, 'train_rates': 0.949265832116402, 'learning_rate': 7.83782547281844e-06, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9419233436027243}. Best is trial 3 with value: 0.055540086523900696.[0m
[32m[I 2025-01-02 01:07:04,824][0m Trial 21 finished with value: 0.05206681899726391 and parameters: {'observation_period_num': 7, 'train_rates': 0.8543369453651454, 'learning_rate': 2.8422310053839492e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9897326322180108}. Best is trial 21 with value: 0.05206681899726391.[0m
[32m[I 2025-01-02 01:13:54,295][0m Trial 22 finished with value: 0.09066555925503501 and parameters: {'observation_period_num': 29, 'train_rates': 0.8808652996899521, 'learning_rate': 4.319852343138436e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9683759053580401}. Best is trial 21 with value: 0.05206681899726391.[0m
[32m[I 2025-01-02 01:24:11,532][0m Trial 23 finished with value: 0.039459646732377056 and parameters: {'observation_period_num': 6, 'train_rates': 0.8211588074319079, 'learning_rate': 2.0206492772491583e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8965628073923795}. Best is trial 23 with value: 0.039459646732377056.[0m
[32m[I 2025-01-02 01:29:23,387][0m Trial 24 finished with value: 0.23809832367114722 and parameters: {'observation_period_num': 74, 'train_rates': 0.7780907011659757, 'learning_rate': 1.7130644187541323e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8926856513562}. Best is trial 23 with value: 0.039459646732377056.[0m
[32m[I 2025-01-02 01:39:51,894][0m Trial 25 finished with value: 0.10600689598842143 and parameters: {'observation_period_num': 101, 'train_rates': 0.8209815575244487, 'learning_rate': 1.865081583961858e-05, 'batch_size': 40, 'step_size': 13, 'gamma': 0.8539939166585105}. Best is trial 23 with value: 0.039459646732377056.[0m
[32m[I 2025-01-02 01:44:36,649][0m Trial 26 finished with value: 0.06117471522193844 and parameters: {'observation_period_num': 41, 'train_rates': 0.8988056425869135, 'learning_rate': 6.2390034751136455e-06, 'batch_size': 111, 'step_size': 14, 'gamma': 0.9010488322861687}. Best is trial 23 with value: 0.039459646732377056.[0m
[32m[I 2025-01-02 01:48:21,405][0m Trial 27 finished with value: 0.08661206738452691 and parameters: {'observation_period_num': 29, 'train_rates': 0.8228682499582509, 'learning_rate': 2.2133609594257973e-06, 'batch_size': 256, 'step_size': 9, 'gamma': 0.9331975500477573}. Best is trial 23 with value: 0.039459646732377056.[0m
[32m[I 2025-01-02 01:53:38,682][0m Trial 28 finished with value: 0.3221949371120085 and parameters: {'observation_period_num': 152, 'train_rates': 0.7525637691666447, 'learning_rate': 0.00016337288869284553, 'batch_size': 77, 'step_size': 12, 'gamma': 0.858024456789498}. Best is trial 23 with value: 0.039459646732377056.[0m
[32m[I 2025-01-02 02:03:21,304][0m Trial 29 finished with value: 0.10241960424643297 and parameters: {'observation_period_num': 94, 'train_rates': 0.9321165882131675, 'learning_rate': 1.760165344691658e-06, 'batch_size': 47, 'step_size': 10, 'gamma': 0.8053289413808689}. Best is trial 23 with value: 0.039459646732377056.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-02 02:03:21,310][0m A new study created in memory with name: no-name-5af45fe1-f3ef-4dcd-ac4a-f6f4f15fb997[0m
Early stopping at epoch 53
[32m[I 2025-01-02 02:05:21,031][0m Trial 0 finished with value: 0.07776737577821079 and parameters: {'observation_period_num': 21, 'train_rates': 0.8058074609848505, 'learning_rate': 0.0005183486548380312, 'batch_size': 237, 'step_size': 1, 'gamma': 0.774936412111622}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:09:08,718][0m Trial 1 finished with value: 0.11601199209690094 and parameters: {'observation_period_num': 243, 'train_rates': 0.9473640988146331, 'learning_rate': 0.0004317619374968829, 'batch_size': 202, 'step_size': 4, 'gamma': 0.8264671187487889}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:14:18,162][0m Trial 2 finished with value: 0.42601138132589833 and parameters: {'observation_period_num': 212, 'train_rates': 0.6554125367505022, 'learning_rate': 2.59184272642465e-05, 'batch_size': 70, 'step_size': 14, 'gamma': 0.7788912211065786}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:31:33,055][0m Trial 3 finished with value: 0.3693477676767442 and parameters: {'observation_period_num': 207, 'train_rates': 0.738159829847889, 'learning_rate': 0.0003111717835468577, 'batch_size': 22, 'step_size': 10, 'gamma': 0.7827952194742528}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:36:06,300][0m Trial 4 finished with value: 0.15408396835510546 and parameters: {'observation_period_num': 127, 'train_rates': 0.7838998248512787, 'learning_rate': 4.172081141267586e-06, 'batch_size': 98, 'step_size': 10, 'gamma': 0.9220521118045341}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:41:00,825][0m Trial 5 finished with value: 0.3361540069182714 and parameters: {'observation_period_num': 250, 'train_rates': 0.6784107494069845, 'learning_rate': 0.0005874267350978306, 'batch_size': 74, 'step_size': 11, 'gamma': 0.9584049411041107}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:44:34,259][0m Trial 6 finished with value: 0.17961048326994244 and parameters: {'observation_period_num': 252, 'train_rates': 0.8595911691646538, 'learning_rate': 4.1075587456701843e-05, 'batch_size': 248, 'step_size': 12, 'gamma': 0.8195706722478314}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:48:04,203][0m Trial 7 finished with value: 0.7516245049099589 and parameters: {'observation_period_num': 195, 'train_rates': 0.6104899760601187, 'learning_rate': 1.2865065132262219e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8256619489294738}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:52:59,528][0m Trial 8 finished with value: 1.7394952353309183 and parameters: {'observation_period_num': 186, 'train_rates': 0.8589712548802808, 'learning_rate': 0.0008093460606342394, 'batch_size': 92, 'step_size': 13, 'gamma': 0.769917575913642}. Best is trial 0 with value: 0.07776737577821079.[0m
[32m[I 2025-01-02 02:56:23,291][0m Trial 9 finished with value: 0.18787830929635496 and parameters: {'observation_period_num': 11, 'train_rates': 0.6948279834843976, 'learning_rate': 3.6333663070120906e-06, 'batch_size': 246, 'step_size': 10, 'gamma': 0.914936628118109}. Best is trial 0 with value: 0.07776737577821079.[0m
Early stopping at epoch 92
[32m[I 2025-01-02 03:00:30,246][0m Trial 10 finished with value: 0.07016489654779434 and parameters: {'observation_period_num': 18, 'train_rates': 0.9856959722277125, 'learning_rate': 0.0001111007346278661, 'batch_size': 177, 'step_size': 1, 'gamma': 0.8740291175432009}. Best is trial 10 with value: 0.07016489654779434.[0m
Early stopping at epoch 94
[32m[I 2025-01-02 03:04:28,319][0m Trial 11 finished with value: 0.06188552938053541 and parameters: {'observation_period_num': 9, 'train_rates': 0.927376811475187, 'learning_rate': 0.00012493526181148878, 'batch_size': 186, 'step_size': 1, 'gamma': 0.8755872698197141}. Best is trial 11 with value: 0.06188552938053541.[0m
Early stopping at epoch 89
[32m[I 2025-01-02 03:08:15,745][0m Trial 12 finished with value: 0.1174338310956955 and parameters: {'observation_period_num': 70, 'train_rates': 0.9613565702826954, 'learning_rate': 0.00010282394537461681, 'batch_size': 174, 'step_size': 1, 'gamma': 0.8721100176925612}. Best is trial 11 with value: 0.06188552938053541.[0m
[32m[I 2025-01-02 03:12:40,402][0m Trial 13 finished with value: 0.07395587116479874 and parameters: {'observation_period_num': 61, 'train_rates': 0.9844213084470944, 'learning_rate': 0.00011524639135785898, 'batch_size': 177, 'step_size': 5, 'gamma': 0.8880445084774982}. Best is trial 11 with value: 0.06188552938053541.[0m
[32m[I 2025-01-02 03:16:57,699][0m Trial 14 finished with value: 0.10309289286776287 and parameters: {'observation_period_num': 56, 'train_rates': 0.9153108600881461, 'learning_rate': 0.00015791280990606812, 'batch_size': 153, 'step_size': 5, 'gamma': 0.982107054258108}. Best is trial 11 with value: 0.06188552938053541.[0m
[32m[I 2025-01-02 03:20:49,987][0m Trial 15 finished with value: 0.09788919322390974 and parameters: {'observation_period_num': 123, 'train_rates': 0.9071579259329643, 'learning_rate': 4.3424804877026664e-05, 'batch_size': 204, 'step_size': 3, 'gamma': 0.8508546567451999}. Best is trial 11 with value: 0.06188552938053541.[0m
[32m[I 2025-01-02 03:24:39,273][0m Trial 16 finished with value: 0.23197264627355044 and parameters: {'observation_period_num': 95, 'train_rates': 0.8933707300509546, 'learning_rate': 1.0995434143921284e-06, 'batch_size': 199, 'step_size': 7, 'gamma': 0.9064714975943416}. Best is trial 11 with value: 0.06188552938053541.[0m
[32m[I 2025-01-02 03:29:18,272][0m Trial 17 finished with value: 0.056233976036310196 and parameters: {'observation_period_num': 38, 'train_rates': 0.9881284708060533, 'learning_rate': 0.00020496325581387964, 'batch_size': 145, 'step_size': 7, 'gamma': 0.8546632863298388}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:33:36,955][0m Trial 18 finished with value: 0.06424491970203661 and parameters: {'observation_period_num': 39, 'train_rates': 0.8360368134207532, 'learning_rate': 0.00022948448153060344, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8516163054235966}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:37:54,385][0m Trial 19 finished with value: 0.09793813060969114 and parameters: {'observation_period_num': 89, 'train_rates': 0.9332547196478758, 'learning_rate': 7.415166802632222e-05, 'batch_size': 152, 'step_size': 15, 'gamma': 0.9409742239150614}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:42:28,191][0m Trial 20 finished with value: 0.09731925177303227 and parameters: {'observation_period_num': 165, 'train_rates': 0.8818521219588249, 'learning_rate': 1.475207082061163e-05, 'batch_size': 111, 'step_size': 8, 'gamma': 0.7994736344280563}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:46:37,680][0m Trial 21 finished with value: 0.05795513483740035 and parameters: {'observation_period_num': 38, 'train_rates': 0.827191280230632, 'learning_rate': 0.00024835108202707416, 'batch_size': 136, 'step_size': 6, 'gamma': 0.8597318274694029}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:50:30,497][0m Trial 22 finished with value: 0.20145333958224368 and parameters: {'observation_period_num': 41, 'train_rates': 0.7852241601584281, 'learning_rate': 0.00024610049014643057, 'batch_size': 156, 'step_size': 6, 'gamma': 0.8899510619060795}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:55:19,324][0m Trial 23 finished with value: 0.07793741520956485 and parameters: {'observation_period_num': 96, 'train_rates': 0.9520709787274768, 'learning_rate': 6.956737443341631e-05, 'batch_size': 119, 'step_size': 3, 'gamma': 0.846842452741915}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 03:58:45,689][0m Trial 24 finished with value: 0.15663125456012736 and parameters: {'observation_period_num': 7, 'train_rates': 0.7446537794285097, 'learning_rate': 0.00021076318177779512, 'batch_size': 221, 'step_size': 8, 'gamma': 0.85065036745542}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 04:02:53,480][0m Trial 25 finished with value: 0.0588206521463844 and parameters: {'observation_period_num': 36, 'train_rates': 0.8185990412642837, 'learning_rate': 0.00035760361416104435, 'batch_size': 148, 'step_size': 6, 'gamma': 0.8891405435726066}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 04:06:51,829][0m Trial 26 finished with value: 1.7139117166659357 and parameters: {'observation_period_num': 36, 'train_rates': 0.823155508660056, 'learning_rate': 0.0009421533817085712, 'batch_size': 151, 'step_size': 8, 'gamma': 0.8128893228732228}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 04:16:52,224][0m Trial 27 finished with value: 0.2517862967439662 and parameters: {'observation_period_num': 77, 'train_rates': 0.7474455078917521, 'learning_rate': 0.0003504090235884332, 'batch_size': 40, 'step_size': 6, 'gamma': 0.8972961961043412}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 04:21:28,320][0m Trial 28 finished with value: 0.11810337294953299 and parameters: {'observation_period_num': 147, 'train_rates': 0.8638699880144114, 'learning_rate': 0.0005110207957177133, 'batch_size': 110, 'step_size': 9, 'gamma': 0.8440919764262466}. Best is trial 17 with value: 0.056233976036310196.[0m
[32m[I 2025-01-02 04:25:36,076][0m Trial 29 finished with value: 0.09080225416848846 and parameters: {'observation_period_num': 32, 'train_rates': 0.8071561677797223, 'learning_rate': 0.0006611571720246389, 'batch_size': 140, 'step_size': 6, 'gamma': 0.9388553750058405}. Best is trial 17 with value: 0.056233976036310196.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-02 04:25:36,081][0m A new study created in memory with name: no-name-151d8140-f07c-42b5-a3a5-7d0ccbc8e82a[0m
[32m[I 2025-01-02 04:42:33,463][0m Trial 0 finished with value: 1.809901501342749 and parameters: {'observation_period_num': 9, 'train_rates': 0.8935319834266364, 'learning_rate': 0.0007098702224753525, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8016405381528429}. Best is trial 0 with value: 1.809901501342749.[0m
Early stopping at epoch 86
[32m[I 2025-01-02 04:47:28,507][0m Trial 1 finished with value: 0.10664415313997341 and parameters: {'observation_period_num': 68, 'train_rates': 0.9095288405556374, 'learning_rate': 0.00034944461277081903, 'batch_size': 82, 'step_size': 1, 'gamma': 0.8343456130886773}. Best is trial 1 with value: 0.10664415313997341.[0m
[32m[I 2025-01-02 04:54:32,601][0m Trial 2 finished with value: 0.0573215938058487 and parameters: {'observation_period_num': 19, 'train_rates': 0.8812186731906864, 'learning_rate': 0.00011667805396000398, 'batch_size': 66, 'step_size': 6, 'gamma': 0.8568119074349058}. Best is trial 2 with value: 0.0573215938058487.[0m
[32m[I 2025-01-02 05:01:50,782][0m Trial 3 finished with value: 0.03504909140368303 and parameters: {'observation_period_num': 6, 'train_rates': 0.9655322013616918, 'learning_rate': 4.053178055183558e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9390275493744156}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:05:41,337][0m Trial 4 finished with value: 0.09766787951602075 and parameters: {'observation_period_num': 125, 'train_rates': 0.8121669554682126, 'learning_rate': 0.0008074999985795073, 'batch_size': 176, 'step_size': 4, 'gamma': 0.7932155839306172}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:10:14,815][0m Trial 5 finished with value: 0.060529904528742746 and parameters: {'observation_period_num': 23, 'train_rates': 0.8426456650689029, 'learning_rate': 1.8876426529259005e-05, 'batch_size': 121, 'step_size': 9, 'gamma': 0.9831277110467642}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:14:28,483][0m Trial 6 finished with value: 0.08469926134169659 and parameters: {'observation_period_num': 20, 'train_rates': 0.8906594754945614, 'learning_rate': 1.4454317018625161e-06, 'batch_size': 162, 'step_size': 5, 'gamma': 0.8959567336010984}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:18:38,186][0m Trial 7 finished with value: 0.25063663721084595 and parameters: {'observation_period_num': 67, 'train_rates': 0.9788734073766383, 'learning_rate': 1.58059023113947e-06, 'batch_size': 248, 'step_size': 3, 'gamma': 0.928140192689712}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:21:56,642][0m Trial 8 finished with value: 0.2597929843999807 and parameters: {'observation_period_num': 114, 'train_rates': 0.7100315692581117, 'learning_rate': 0.00017429526901374732, 'batch_size': 225, 'step_size': 1, 'gamma': 0.9414729460855271}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:39:49,050][0m Trial 9 finished with value: 0.07757892929380987 and parameters: {'observation_period_num': 75, 'train_rates': 0.7863846746400726, 'learning_rate': 0.00033583834622218946, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9569725569405507}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:43:36,762][0m Trial 10 finished with value: 0.7522190382709476 and parameters: {'observation_period_num': 218, 'train_rates': 0.6175816629003149, 'learning_rate': 1.869628547703642e-05, 'batch_size': 103, 'step_size': 12, 'gamma': 0.9063737535104834}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:51:28,911][0m Trial 11 finished with value: 0.15909337997436523 and parameters: {'observation_period_num': 197, 'train_rates': 0.9815962264134863, 'learning_rate': 5.2097560252340674e-05, 'batch_size': 58, 'step_size': 8, 'gamma': 0.8614441238584226}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 05:58:10,164][0m Trial 12 finished with value: 0.14122226782012404 and parameters: {'observation_period_num': 173, 'train_rates': 0.9385148952425137, 'learning_rate': 5.547362248608818e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.8545705352336165}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:02:16,231][0m Trial 13 finished with value: 0.09083122534807338 and parameters: {'observation_period_num': 52, 'train_rates': 0.8518947905601992, 'learning_rate': 5.3351186306322855e-06, 'batch_size': 150, 'step_size': 7, 'gamma': 0.7533452035141633}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:09:51,883][0m Trial 14 finished with value: 0.3497845628145304 and parameters: {'observation_period_num': 252, 'train_rates': 0.7308845467866308, 'learning_rate': 0.00010556976161594825, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8921468883481303}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:14:58,348][0m Trial 15 finished with value: 0.17362137323748456 and parameters: {'observation_period_num': 105, 'train_rates': 0.9521591731072486, 'learning_rate': 8.83066171111393e-06, 'batch_size': 102, 'step_size': 15, 'gamma': 0.9867343803661763}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:19:03,332][0m Trial 16 finished with value: 0.058517247542878174 and parameters: {'observation_period_num': 39, 'train_rates': 0.9267250032677845, 'learning_rate': 0.00010552663290700002, 'batch_size': 194, 'step_size': 6, 'gamma': 0.8287173053750628}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:23:26,366][0m Trial 17 finished with value: 0.12223494933241635 and parameters: {'observation_period_num': 152, 'train_rates': 0.8568598880265943, 'learning_rate': 3.3032692406251304e-05, 'batch_size': 121, 'step_size': 9, 'gamma': 0.9250946350469481}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:28:32,348][0m Trial 18 finished with value: 0.2521517679441807 and parameters: {'observation_period_num': 92, 'train_rates': 0.774674076500043, 'learning_rate': 5.3953369462121096e-06, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8811940142242938}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:40:20,149][0m Trial 19 finished with value: 0.05095280937447741 and parameters: {'observation_period_num': 6, 'train_rates': 0.87474878948319, 'learning_rate': 0.00016467852256321923, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9567587589492879}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 06:52:41,668][0m Trial 20 finished with value: 0.04124503265892234 and parameters: {'observation_period_num': 5, 'train_rates': 0.9637626261414093, 'learning_rate': 0.00023663168750739924, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9579539952845496}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 07:04:36,381][0m Trial 21 finished with value: 0.05814233623499418 and parameters: {'observation_period_num': 48, 'train_rates': 0.9528936079493291, 'learning_rate': 0.00034416022923215885, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9611743799736094}. Best is trial 3 with value: 0.03504909140368303.[0m
[32m[I 2025-01-02 07:34:34,335][0m Trial 22 finished with value: 0.031114341789170316 and parameters: {'observation_period_num': 10, 'train_rates': 0.9873092072126353, 'learning_rate': 0.00020120158182884257, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9624439005724288}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 08:04:07,345][0m Trial 23 finished with value: 0.04794243865069889 and parameters: {'observation_period_num': 36, 'train_rates': 0.9858675734977894, 'learning_rate': 5.683825040002904e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9177507764633231}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 08:33:31,653][0m Trial 24 finished with value: 0.03387760919762538 and parameters: {'observation_period_num': 5, 'train_rates': 0.9494746670258197, 'learning_rate': 0.00021270646845945857, 'batch_size': 16, 'step_size': 13, 'gamma': 0.972819823631882}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 09:00:29,643][0m Trial 25 finished with value: 1.4649687585376558 and parameters: {'observation_period_num': 36, 'train_rates': 0.9282158396738582, 'learning_rate': 0.000477505857785651, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9751001503199807}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 09:06:08,713][0m Trial 26 finished with value: 0.09546739090647963 and parameters: {'observation_period_num': 84, 'train_rates': 0.9250722533593289, 'learning_rate': 3.165205839017349e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.9380210953650379}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 09:13:23,357][0m Trial 27 finished with value: 0.2043024962025186 and parameters: {'observation_period_num': 60, 'train_rates': 0.662219428984261, 'learning_rate': 6.905952995742692e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.9692420198945474}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 09:25:08,516][0m Trial 28 finished with value: 0.14812048050622068 and parameters: {'observation_period_num': 144, 'train_rates': 0.8173192024547509, 'learning_rate': 1.8164570385984525e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9398656643006329}. Best is trial 22 with value: 0.031114341789170316.[0m
[32m[I 2025-01-02 09:41:37,131][0m Trial 29 finished with value: 0.04956272396102416 and parameters: {'observation_period_num': 24, 'train_rates': 0.961740460357224, 'learning_rate': 0.0005923044510067624, 'batch_size': 29, 'step_size': 12, 'gamma': 0.9134798339990544}. Best is trial 22 with value: 0.031114341789170316.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-02 09:41:37,137][0m A new study created in memory with name: no-name-8a6c56f0-9b48-4c4d-8660-c25f7ce1a039[0m
[32m[I 2025-01-02 09:45:45,623][0m Trial 0 finished with value: 0.12276618927717209 and parameters: {'observation_period_num': 72, 'train_rates': 0.9601508425134988, 'learning_rate': 6.6128745232504895e-06, 'batch_size': 187, 'step_size': 13, 'gamma': 0.902626972766442}. Best is trial 0 with value: 0.12276618927717209.[0m
[32m[I 2025-01-02 09:49:09,607][0m Trial 1 finished with value: 0.3242093812762283 and parameters: {'observation_period_num': 195, 'train_rates': 0.723902547542647, 'learning_rate': 8.776696231059589e-05, 'batch_size': 171, 'step_size': 6, 'gamma': 0.7571248697949413}. Best is trial 0 with value: 0.12276618927717209.[0m
[32m[I 2025-01-02 09:53:19,645][0m Trial 2 finished with value: 0.11021676659584045 and parameters: {'observation_period_num': 77, 'train_rates': 0.9845857764006376, 'learning_rate': 0.00019386339136747324, 'batch_size': 246, 'step_size': 10, 'gamma': 0.9567155794740207}. Best is trial 2 with value: 0.11021676659584045.[0m
[32m[I 2025-01-02 09:56:45,241][0m Trial 3 finished with value: 0.27334285429183475 and parameters: {'observation_period_num': 76, 'train_rates': 0.7551025583443909, 'learning_rate': 2.6373050809730446e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9203223889028556}. Best is trial 2 with value: 0.11021676659584045.[0m
[32m[I 2025-01-02 10:05:21,136][0m Trial 4 finished with value: 0.1415049692800453 and parameters: {'observation_period_num': 166, 'train_rates': 0.9203818923557311, 'learning_rate': 2.637689347854716e-06, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8606006359975661}. Best is trial 2 with value: 0.11021676659584045.[0m
[32m[I 2025-01-02 10:09:56,306][0m Trial 5 finished with value: 0.10977721959352493 and parameters: {'observation_period_num': 101, 'train_rates': 0.9742266515591954, 'learning_rate': 5.5402088123119784e-06, 'batch_size': 143, 'step_size': 11, 'gamma': 0.9392396004141177}. Best is trial 5 with value: 0.10977721959352493.[0m
[32m[I 2025-01-02 10:14:16,751][0m Trial 6 finished with value: 0.2612635790126557 and parameters: {'observation_period_num': 139, 'train_rates': 0.7748885432443898, 'learning_rate': 0.00037571603360103326, 'batch_size': 105, 'step_size': 4, 'gamma': 0.8674829814298448}. Best is trial 5 with value: 0.10977721959352493.[0m
[32m[I 2025-01-02 10:17:44,525][0m Trial 7 finished with value: 0.3961054109472564 and parameters: {'observation_period_num': 214, 'train_rates': 0.7566625596736548, 'learning_rate': 1.2416809862529084e-05, 'batch_size': 229, 'step_size': 14, 'gamma': 0.909809274384446}. Best is trial 5 with value: 0.10977721959352493.[0m
[32m[I 2025-01-02 10:28:40,677][0m Trial 8 finished with value: 0.177995743058252 and parameters: {'observation_period_num': 162, 'train_rates': 0.8990092259583002, 'learning_rate': 5.177907631756195e-06, 'batch_size': 40, 'step_size': 1, 'gamma': 0.8993488766886663}. Best is trial 5 with value: 0.10977721959352493.[0m
[32m[I 2025-01-02 10:32:42,817][0m Trial 9 finished with value: 0.05631912872195244 and parameters: {'observation_period_num': 28, 'train_rates': 0.9533522531342851, 'learning_rate': 9.53266758071716e-05, 'batch_size': 215, 'step_size': 11, 'gamma': 0.7569293752805051}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 10:35:58,893][0m Trial 10 finished with value: 0.2165173233674248 and parameters: {'observation_period_num': 7, 'train_rates': 0.6023420085929383, 'learning_rate': 1.0924256943671972e-06, 'batch_size': 192, 'step_size': 15, 'gamma': 0.7789236218187297}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 10:40:41,496][0m Trial 11 finished with value: 1.693197708008653 and parameters: {'observation_period_num': 16, 'train_rates': 0.8395923144333964, 'learning_rate': 0.0009987785828824598, 'batch_size': 115, 'step_size': 10, 'gamma': 0.8091767267976296}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 10:44:58,077][0m Trial 12 finished with value: 0.13200011331745176 and parameters: {'observation_period_num': 53, 'train_rates': 0.8613998453385261, 'learning_rate': 4.8792180565156654e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9706831609167498}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 10:50:55,086][0m Trial 13 finished with value: 0.12055287510156631 and parameters: {'observation_period_num': 110, 'train_rates': 0.9893384389827834, 'learning_rate': 2.1119701716650866e-05, 'batch_size': 82, 'step_size': 12, 'gamma': 0.8262404592034623}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 10:54:50,273][0m Trial 14 finished with value: 0.11633202687222907 and parameters: {'observation_period_num': 110, 'train_rates': 0.9174417493722425, 'learning_rate': 7.849193251005059e-05, 'batch_size': 210, 'step_size': 8, 'gamma': 0.9856778316662903}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 10:58:59,525][0m Trial 15 finished with value: 0.0815361946695993 and parameters: {'observation_period_num': 37, 'train_rates': 0.8371043071392705, 'learning_rate': 1.9178751505329563e-06, 'batch_size': 150, 'step_size': 11, 'gamma': 0.9430157150430607}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:02:24,742][0m Trial 16 finished with value: 0.2756926581036621 and parameters: {'observation_period_num': 39, 'train_rates': 0.6621244201795087, 'learning_rate': 1.481528770334649e-06, 'batch_size': 161, 'step_size': 6, 'gamma': 0.825306619027377}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:06:08,233][0m Trial 17 finished with value: 0.07559498117322003 and parameters: {'observation_period_num': 39, 'train_rates': 0.8276640821405361, 'learning_rate': 0.00012980125793589636, 'batch_size': 211, 'step_size': 15, 'gamma': 0.7970760745662139}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:09:35,799][0m Trial 18 finished with value: 0.2020445096635533 and parameters: {'observation_period_num': 242, 'train_rates': 0.8154717437310374, 'learning_rate': 0.0002079697764193297, 'batch_size': 215, 'step_size': 15, 'gamma': 0.7888974855463994}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:13:33,717][0m Trial 19 finished with value: 0.06673300397746704 and parameters: {'observation_period_num': 27, 'train_rates': 0.8838496724941438, 'learning_rate': 0.0006695531890816533, 'batch_size': 190, 'step_size': 13, 'gamma': 0.7506899026700778}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:17:38,736][0m Trial 20 finished with value: 1.7768832475366727 and parameters: {'observation_period_num': 7, 'train_rates': 0.8797783934053797, 'learning_rate': 0.0007334107708709665, 'batch_size': 184, 'step_size': 13, 'gamma': 0.7547189371965173}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:21:37,515][0m Trial 21 finished with value: 0.05778354033827782 and parameters: {'observation_period_num': 36, 'train_rates': 0.9340843124089145, 'learning_rate': 0.0001906446138250993, 'batch_size': 211, 'step_size': 14, 'gamma': 0.7772029148368483}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:25:37,642][0m Trial 22 finished with value: 0.08476539701223373 and parameters: {'observation_period_num': 64, 'train_rates': 0.9310378977666179, 'learning_rate': 0.0004263308487894904, 'batch_size': 233, 'step_size': 13, 'gamma': 0.7537393768233587}. Best is trial 9 with value: 0.05631912872195244.[0m
[32m[I 2025-01-02 11:29:51,726][0m Trial 23 finished with value: 0.04049397259950638 and parameters: {'observation_period_num': 28, 'train_rates': 0.9516042739893327, 'learning_rate': 0.00041459089717507306, 'batch_size': 198, 'step_size': 14, 'gamma': 0.7733817061639112}. Best is trial 23 with value: 0.04049397259950638.[0m
[32m[I 2025-01-02 11:33:51,497][0m Trial 24 finished with value: 0.0942789763212204 and parameters: {'observation_period_num': 90, 'train_rates': 0.9414866651104917, 'learning_rate': 0.00031504289256021055, 'batch_size': 233, 'step_size': 14, 'gamma': 0.8436601644971243}. Best is trial 23 with value: 0.04049397259950638.[0m
[32m[I 2025-01-02 11:37:55,243][0m Trial 25 finished with value: 0.07717067003250122 and parameters: {'observation_period_num': 54, 'train_rates': 0.9522969498506832, 'learning_rate': 4.81420710036483e-05, 'batch_size': 201, 'step_size': 9, 'gamma': 0.7756942169925637}. Best is trial 23 with value: 0.04049397259950638.[0m
[32m[I 2025-01-02 11:42:05,599][0m Trial 26 finished with value: 0.03818624914238304 and parameters: {'observation_period_num': 23, 'train_rates': 0.9069942590842773, 'learning_rate': 0.00016907613326897983, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8086588913397253}. Best is trial 26 with value: 0.03818624914238304.[0m
[32m[I 2025-01-02 11:46:14,440][0m Trial 27 finished with value: 0.04973210521619505 and parameters: {'observation_period_num': 22, 'train_rates': 0.9034433142784646, 'learning_rate': 0.0001075057742616679, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8059918713451655}. Best is trial 26 with value: 0.03818624914238304.[0m
[32m[I 2025-01-02 11:50:57,132][0m Trial 28 finished with value: 0.076607011567483 and parameters: {'observation_period_num': 54, 'train_rates': 0.8630719713578638, 'learning_rate': 5.4043066899143794e-05, 'batch_size': 119, 'step_size': 12, 'gamma': 0.8119497633621843}. Best is trial 26 with value: 0.03818624914238304.[0m
[32m[I 2025-01-02 11:54:54,529][0m Trial 29 finished with value: 0.11609617403049148 and parameters: {'observation_period_num': 133, 'train_rates': 0.9054928080862301, 'learning_rate': 0.00028554228197820073, 'batch_size': 165, 'step_size': 7, 'gamma': 0.8426648841918941}. Best is trial 26 with value: 0.03818624914238304.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.9040558390404843, 'learning_rate': 0.0005819606607873501, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9161717379917367}
Epoch 1/300, trend Loss: 0.5193 | 0.0960
Epoch 2/300, trend Loss: 0.1461 | 0.1856
Epoch 3/300, trend Loss: 0.1250 | 0.0974
Epoch 4/300, trend Loss: 0.1113 | 0.0782
Epoch 5/300, trend Loss: 0.1046 | 0.0648
Epoch 6/300, trend Loss: 0.1020 | 0.0609
Epoch 7/300, trend Loss: 0.1004 | 0.0602
Epoch 8/300, trend Loss: 0.0986 | 0.0560
Epoch 9/300, trend Loss: 0.0979 | 0.0538
Epoch 10/300, trend Loss: 0.0983 | 0.0573
Epoch 11/300, trend Loss: 0.0962 | 0.0464
Epoch 12/300, trend Loss: 0.0893 | 0.0410
Epoch 13/300, trend Loss: 0.0864 | 0.0400
Epoch 14/300, trend Loss: 0.0841 | 0.0407
Epoch 15/300, trend Loss: 0.0829 | 0.0407
Epoch 16/300, trend Loss: 0.0831 | 0.0404
Epoch 17/300, trend Loss: 0.0840 | 0.0382
Epoch 18/300, trend Loss: 0.0834 | 0.0374
Epoch 19/300, trend Loss: 0.0853 | 0.0375
Epoch 20/300, trend Loss: 0.0810 | 0.0370
Epoch 21/300, trend Loss: 0.0789 | 0.0366
Epoch 22/300, trend Loss: 0.0777 | 0.0361
Epoch 23/300, trend Loss: 0.0769 | 0.0360
Epoch 24/300, trend Loss: 0.0764 | 0.0357
Epoch 25/300, trend Loss: 0.0761 | 0.0358
Epoch 26/300, trend Loss: 0.0759 | 0.0355
Epoch 27/300, trend Loss: 0.0756 | 0.0358
Epoch 28/300, trend Loss: 0.0756 | 0.0357
Epoch 29/300, trend Loss: 0.0754 | 0.0363
Epoch 30/300, trend Loss: 0.0754 | 0.0365
Epoch 31/300, trend Loss: 0.0751 | 0.0369
Epoch 32/300, trend Loss: 0.0750 | 0.0371
Epoch 33/300, trend Loss: 0.0747 | 0.0367
Epoch 34/300, trend Loss: 0.0745 | 0.0364
Epoch 35/300, trend Loss: 0.0743 | 0.0357
Epoch 36/300, trend Loss: 0.0743 | 0.0353
Epoch 37/300, trend Loss: 0.0744 | 0.0350
Epoch 38/300, trend Loss: 0.0746 | 0.0348
Epoch 39/300, trend Loss: 0.0749 | 0.0346
Epoch 40/300, trend Loss: 0.0752 | 0.0342
Epoch 41/300, trend Loss: 0.0754 | 0.0336
Epoch 42/300, trend Loss: 0.0755 | 0.0333
Epoch 43/300, trend Loss: 0.0753 | 0.0331
Epoch 44/300, trend Loss: 0.0749 | 0.0331
Epoch 45/300, trend Loss: 0.0740 | 0.0329
Epoch 46/300, trend Loss: 0.0730 | 0.0327
Epoch 47/300, trend Loss: 0.0722 | 0.0327
Epoch 48/300, trend Loss: 0.0717 | 0.0327
Epoch 49/300, trend Loss: 0.0714 | 0.0326
Epoch 50/300, trend Loss: 0.0712 | 0.0326
Epoch 51/300, trend Loss: 0.0711 | 0.0325
Epoch 52/300, trend Loss: 0.0710 | 0.0324
Epoch 53/300, trend Loss: 0.0709 | 0.0323
Epoch 54/300, trend Loss: 0.0708 | 0.0322
Epoch 55/300, trend Loss: 0.0707 | 0.0322
Epoch 56/300, trend Loss: 0.0707 | 0.0321
Epoch 57/300, trend Loss: 0.0706 | 0.0320
Epoch 58/300, trend Loss: 0.0706 | 0.0320
Epoch 59/300, trend Loss: 0.0705 | 0.0319
Epoch 60/300, trend Loss: 0.0705 | 0.0319
Epoch 61/300, trend Loss: 0.0704 | 0.0318
Epoch 62/300, trend Loss: 0.0704 | 0.0318
Epoch 63/300, trend Loss: 0.0703 | 0.0318
Epoch 64/300, trend Loss: 0.0703 | 0.0317
Epoch 65/300, trend Loss: 0.0703 | 0.0317
Epoch 66/300, trend Loss: 0.0703 | 0.0317
Epoch 67/300, trend Loss: 0.0702 | 0.0316
Epoch 68/300, trend Loss: 0.0702 | 0.0316
Epoch 69/300, trend Loss: 0.0702 | 0.0316
Epoch 70/300, trend Loss: 0.0702 | 0.0315
Epoch 71/300, trend Loss: 0.0701 | 0.0315
Epoch 72/300, trend Loss: 0.0701 | 0.0315
Epoch 73/300, trend Loss: 0.0701 | 0.0315
Epoch 74/300, trend Loss: 0.0701 | 0.0315
Epoch 75/300, trend Loss: 0.0701 | 0.0315
Epoch 76/300, trend Loss: 0.0701 | 0.0314
Epoch 77/300, trend Loss: 0.0701 | 0.0314
Epoch 78/300, trend Loss: 0.0700 | 0.0314
Epoch 79/300, trend Loss: 0.0700 | 0.0314
Epoch 80/300, trend Loss: 0.0700 | 0.0314
Epoch 81/300, trend Loss: 0.0700 | 0.0314
Epoch 82/300, trend Loss: 0.0700 | 0.0314
Epoch 83/300, trend Loss: 0.0700 | 0.0314
Epoch 84/300, trend Loss: 0.0700 | 0.0314
Epoch 85/300, trend Loss: 0.0700 | 0.0314
Epoch 86/300, trend Loss: 0.0700 | 0.0314
Epoch 87/300, trend Loss: 0.0700 | 0.0313
Epoch 88/300, trend Loss: 0.0700 | 0.0313
Epoch 89/300, trend Loss: 0.0700 | 0.0313
Epoch 90/300, trend Loss: 0.0700 | 0.0313
Epoch 91/300, trend Loss: 0.0700 | 0.0313
Epoch 92/300, trend Loss: 0.0700 | 0.0313
Epoch 93/300, trend Loss: 0.0700 | 0.0313
Epoch 94/300, trend Loss: 0.0700 | 0.0313
Epoch 95/300, trend Loss: 0.0699 | 0.0313
Epoch 96/300, trend Loss: 0.0699 | 0.0313
Epoch 97/300, trend Loss: 0.0699 | 0.0313
Epoch 98/300, trend Loss: 0.0699 | 0.0313
Epoch 99/300, trend Loss: 0.0699 | 0.0313
Epoch 100/300, trend Loss: 0.0699 | 0.0313
Epoch 101/300, trend Loss: 0.0699 | 0.0313
Epoch 102/300, trend Loss: 0.0699 | 0.0313
Epoch 103/300, trend Loss: 0.0699 | 0.0313
Epoch 104/300, trend Loss: 0.0699 | 0.0313
Epoch 105/300, trend Loss: 0.0699 | 0.0313
Epoch 106/300, trend Loss: 0.0699 | 0.0313
Epoch 107/300, trend Loss: 0.0699 | 0.0313
Epoch 108/300, trend Loss: 0.0699 | 0.0313
Epoch 109/300, trend Loss: 0.0699 | 0.0313
Epoch 110/300, trend Loss: 0.0699 | 0.0313
Epoch 111/300, trend Loss: 0.0699 | 0.0313
Epoch 112/300, trend Loss: 0.0699 | 0.0313
Epoch 113/300, trend Loss: 0.0699 | 0.0313
Epoch 114/300, trend Loss: 0.0699 | 0.0313
Epoch 115/300, trend Loss: 0.0699 | 0.0313
Epoch 116/300, trend Loss: 0.0699 | 0.0313
Epoch 117/300, trend Loss: 0.0699 | 0.0313
Epoch 118/300, trend Loss: 0.0699 | 0.0313
Epoch 119/300, trend Loss: 0.0699 | 0.0313
Epoch 120/300, trend Loss: 0.0699 | 0.0313
Epoch 121/300, trend Loss: 0.0699 | 0.0313
Epoch 122/300, trend Loss: 0.0699 | 0.0313
Epoch 123/300, trend Loss: 0.0699 | 0.0313
Epoch 124/300, trend Loss: 0.0699 | 0.0313
Epoch 125/300, trend Loss: 0.0699 | 0.0313
Epoch 126/300, trend Loss: 0.0699 | 0.0313
Epoch 127/300, trend Loss: 0.0699 | 0.0313
Epoch 128/300, trend Loss: 0.0699 | 0.0313
Epoch 129/300, trend Loss: 0.0699 | 0.0313
Epoch 130/300, trend Loss: 0.0699 | 0.0313
Epoch 131/300, trend Loss: 0.0699 | 0.0313
Epoch 132/300, trend Loss: 0.0699 | 0.0313
Epoch 133/300, trend Loss: 0.0699 | 0.0313
Epoch 134/300, trend Loss: 0.0699 | 0.0313
Epoch 135/300, trend Loss: 0.0699 | 0.0313
Epoch 136/300, trend Loss: 0.0699 | 0.0313
Epoch 137/300, trend Loss: 0.0699 | 0.0313
Epoch 138/300, trend Loss: 0.0699 | 0.0313
Epoch 139/300, trend Loss: 0.0699 | 0.0313
Epoch 140/300, trend Loss: 0.0699 | 0.0313
Epoch 141/300, trend Loss: 0.0699 | 0.0313
Epoch 142/300, trend Loss: 0.0699 | 0.0313
Epoch 143/300, trend Loss: 0.0699 | 0.0313
Epoch 144/300, trend Loss: 0.0699 | 0.0313
Epoch 145/300, trend Loss: 0.0699 | 0.0313
Epoch 146/300, trend Loss: 0.0699 | 0.0313
Epoch 147/300, trend Loss: 0.0699 | 0.0313
Epoch 148/300, trend Loss: 0.0699 | 0.0313
Epoch 149/300, trend Loss: 0.0699 | 0.0313
Epoch 150/300, trend Loss: 0.0699 | 0.0313
Epoch 151/300, trend Loss: 0.0699 | 0.0313
Epoch 152/300, trend Loss: 0.0699 | 0.0313
Epoch 153/300, trend Loss: 0.0699 | 0.0313
Epoch 154/300, trend Loss: 0.0699 | 0.0313
Epoch 155/300, trend Loss: 0.0699 | 0.0313
Epoch 156/300, trend Loss: 0.0699 | 0.0313
Epoch 157/300, trend Loss: 0.0699 | 0.0313
Epoch 158/300, trend Loss: 0.0699 | 0.0313
Epoch 159/300, trend Loss: 0.0699 | 0.0313
Epoch 160/300, trend Loss: 0.0699 | 0.0313
Epoch 161/300, trend Loss: 0.0699 | 0.0313
Epoch 162/300, trend Loss: 0.0699 | 0.0313
Epoch 163/300, trend Loss: 0.0699 | 0.0313
Epoch 164/300, trend Loss: 0.0699 | 0.0313
Epoch 165/300, trend Loss: 0.0699 | 0.0313
Epoch 166/300, trend Loss: 0.0699 | 0.0313
Epoch 167/300, trend Loss: 0.0699 | 0.0313
Epoch 168/300, trend Loss: 0.0699 | 0.0313
Epoch 169/300, trend Loss: 0.0699 | 0.0313
Epoch 170/300, trend Loss: 0.0699 | 0.0313
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 6, 'train_rates': 0.8284101594132065, 'learning_rate': 8.27321775697815e-06, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9109783600725903}
Epoch 1/300, seasonal_0 Loss: 0.3819 | 0.1796
Epoch 2/300, seasonal_0 Loss: 0.1680 | 0.0943
Epoch 3/300, seasonal_0 Loss: 0.1481 | 0.0821
Epoch 4/300, seasonal_0 Loss: 0.1439 | 0.0794
Epoch 5/300, seasonal_0 Loss: 0.1384 | 0.0797
Epoch 6/300, seasonal_0 Loss: 0.1304 | 0.0794
Epoch 7/300, seasonal_0 Loss: 0.1242 | 0.0807
Epoch 8/300, seasonal_0 Loss: 0.1221 | 0.0794
Epoch 9/300, seasonal_0 Loss: 0.1224 | 0.0774
Epoch 10/300, seasonal_0 Loss: 0.1229 | 0.0741
Epoch 11/300, seasonal_0 Loss: 0.1223 | 0.0703
Epoch 12/300, seasonal_0 Loss: 0.1205 | 0.0663
Epoch 13/300, seasonal_0 Loss: 0.1180 | 0.0660
Epoch 14/300, seasonal_0 Loss: 0.1152 | 0.0666
Epoch 15/300, seasonal_0 Loss: 0.1115 | 0.0669
Epoch 16/300, seasonal_0 Loss: 0.1088 | 0.0663
Epoch 17/300, seasonal_0 Loss: 0.1081 | 0.0652
Epoch 18/300, seasonal_0 Loss: 0.1092 | 0.0641
Epoch 19/300, seasonal_0 Loss: 0.1100 | 0.0628
Epoch 20/300, seasonal_0 Loss: 0.1090 | 0.0617
Epoch 21/300, seasonal_0 Loss: 0.1060 | 0.0606
Epoch 22/300, seasonal_0 Loss: 0.1027 | 0.0599
Epoch 23/300, seasonal_0 Loss: 0.1005 | 0.0597
Epoch 24/300, seasonal_0 Loss: 0.1009 | 0.0592
Epoch 25/300, seasonal_0 Loss: 0.1037 | 0.0582
Epoch 26/300, seasonal_0 Loss: 0.1069 | 0.0570
Epoch 27/300, seasonal_0 Loss: 0.1081 | 0.0596
Epoch 28/300, seasonal_0 Loss: 0.1048 | 0.0627
Epoch 29/300, seasonal_0 Loss: 0.0992 | 0.0604
Epoch 30/300, seasonal_0 Loss: 0.0993 | 0.0587
Epoch 31/300, seasonal_0 Loss: 0.1047 | 0.0624
Epoch 32/300, seasonal_0 Loss: 0.1050 | 0.0597
Epoch 33/300, seasonal_0 Loss: 0.0998 | 0.0538
Epoch 34/300, seasonal_0 Loss: 0.0941 | 0.0503
Epoch 35/300, seasonal_0 Loss: 0.0919 | 0.0496
Epoch 36/300, seasonal_0 Loss: 0.0914 | 0.0495
Epoch 37/300, seasonal_0 Loss: 0.0908 | 0.0492
Epoch 38/300, seasonal_0 Loss: 0.0901 | 0.0490
Epoch 39/300, seasonal_0 Loss: 0.0897 | 0.0489
Epoch 40/300, seasonal_0 Loss: 0.0895 | 0.0490
Epoch 41/300, seasonal_0 Loss: 0.0893 | 0.0486
Epoch 42/300, seasonal_0 Loss: 0.0889 | 0.0480
Epoch 43/300, seasonal_0 Loss: 0.0882 | 0.0474
Epoch 44/300, seasonal_0 Loss: 0.0876 | 0.0469
Epoch 45/300, seasonal_0 Loss: 0.0871 | 0.0466
Epoch 46/300, seasonal_0 Loss: 0.0868 | 0.0463
Epoch 47/300, seasonal_0 Loss: 0.0865 | 0.0462
Epoch 48/300, seasonal_0 Loss: 0.0862 | 0.0460
Epoch 49/300, seasonal_0 Loss: 0.0860 | 0.0459
Epoch 50/300, seasonal_0 Loss: 0.0857 | 0.0458
Epoch 51/300, seasonal_0 Loss: 0.0854 | 0.0457
Epoch 52/300, seasonal_0 Loss: 0.0852 | 0.0455
Epoch 53/300, seasonal_0 Loss: 0.0849 | 0.0453
Epoch 54/300, seasonal_0 Loss: 0.0847 | 0.0450
Epoch 55/300, seasonal_0 Loss: 0.0845 | 0.0448
Epoch 56/300, seasonal_0 Loss: 0.0843 | 0.0446
Epoch 57/300, seasonal_0 Loss: 0.0842 | 0.0444
Epoch 58/300, seasonal_0 Loss: 0.0841 | 0.0442
Epoch 59/300, seasonal_0 Loss: 0.0839 | 0.0441
Epoch 60/300, seasonal_0 Loss: 0.0838 | 0.0440
Epoch 61/300, seasonal_0 Loss: 0.0836 | 0.0440
Epoch 62/300, seasonal_0 Loss: 0.0834 | 0.0439
Epoch 63/300, seasonal_0 Loss: 0.0831 | 0.0438
Epoch 64/300, seasonal_0 Loss: 0.0828 | 0.0437
Epoch 65/300, seasonal_0 Loss: 0.0827 | 0.0436
Epoch 66/300, seasonal_0 Loss: 0.0826 | 0.0436
Epoch 67/300, seasonal_0 Loss: 0.0824 | 0.0436
Epoch 68/300, seasonal_0 Loss: 0.0822 | 0.0435
Epoch 69/300, seasonal_0 Loss: 0.0821 | 0.0433
Epoch 70/300, seasonal_0 Loss: 0.0820 | 0.0431
Epoch 71/300, seasonal_0 Loss: 0.0820 | 0.0429
Epoch 72/300, seasonal_0 Loss: 0.0821 | 0.0427
Epoch 73/300, seasonal_0 Loss: 0.0823 | 0.0424
Epoch 74/300, seasonal_0 Loss: 0.0822 | 0.0424
Epoch 75/300, seasonal_0 Loss: 0.0816 | 0.0426
Epoch 76/300, seasonal_0 Loss: 0.0813 | 0.0428
Epoch 77/300, seasonal_0 Loss: 0.0812 | 0.0431
Epoch 78/300, seasonal_0 Loss: 0.0813 | 0.0440
Epoch 79/300, seasonal_0 Loss: 0.0813 | 0.0442
Epoch 80/300, seasonal_0 Loss: 0.0810 | 0.0439
Epoch 81/300, seasonal_0 Loss: 0.0808 | 0.0435
Epoch 82/300, seasonal_0 Loss: 0.0807 | 0.0430
Epoch 83/300, seasonal_0 Loss: 0.0808 | 0.0426
Epoch 84/300, seasonal_0 Loss: 0.0811 | 0.0427
Epoch 85/300, seasonal_0 Loss: 0.0810 | 0.0427
Epoch 86/300, seasonal_0 Loss: 0.0806 | 0.0429
Epoch 87/300, seasonal_0 Loss: 0.0803 | 0.0431
Epoch 88/300, seasonal_0 Loss: 0.0801 | 0.0433
Epoch 89/300, seasonal_0 Loss: 0.0801 | 0.0439
Epoch 90/300, seasonal_0 Loss: 0.0800 | 0.0433
Epoch 91/300, seasonal_0 Loss: 0.0800 | 0.0427
Epoch 92/300, seasonal_0 Loss: 0.0798 | 0.0423
Epoch 93/300, seasonal_0 Loss: 0.0797 | 0.0420
Epoch 94/300, seasonal_0 Loss: 0.0796 | 0.0417
Epoch 95/300, seasonal_0 Loss: 0.0797 | 0.0415
Epoch 96/300, seasonal_0 Loss: 0.0798 | 0.0414
Epoch 97/300, seasonal_0 Loss: 0.0799 | 0.0413
Epoch 98/300, seasonal_0 Loss: 0.0801 | 0.0414
Epoch 99/300, seasonal_0 Loss: 0.0804 | 0.0414
Epoch 100/300, seasonal_0 Loss: 0.0806 | 0.0415
Epoch 101/300, seasonal_0 Loss: 0.0809 | 0.0414
Epoch 102/300, seasonal_0 Loss: 0.0813 | 0.0412
Epoch 103/300, seasonal_0 Loss: 0.0810 | 0.0409
Epoch 104/300, seasonal_0 Loss: 0.0805 | 0.0409
Epoch 105/300, seasonal_0 Loss: 0.0796 | 0.0410
Epoch 106/300, seasonal_0 Loss: 0.0787 | 0.0413
Epoch 107/300, seasonal_0 Loss: 0.0783 | 0.0415
Epoch 108/300, seasonal_0 Loss: 0.0783 | 0.0416
Epoch 109/300, seasonal_0 Loss: 0.0785 | 0.0414
Epoch 110/300, seasonal_0 Loss: 0.0786 | 0.0411
Epoch 111/300, seasonal_0 Loss: 0.0786 | 0.0409
Epoch 112/300, seasonal_0 Loss: 0.0785 | 0.0407
Epoch 113/300, seasonal_0 Loss: 0.0783 | 0.0407
Epoch 114/300, seasonal_0 Loss: 0.0781 | 0.0407
Epoch 115/300, seasonal_0 Loss: 0.0779 | 0.0407
Epoch 116/300, seasonal_0 Loss: 0.0777 | 0.0408
Epoch 117/300, seasonal_0 Loss: 0.0776 | 0.0408
Epoch 118/300, seasonal_0 Loss: 0.0777 | 0.0408
Epoch 119/300, seasonal_0 Loss: 0.0778 | 0.0407
Epoch 120/300, seasonal_0 Loss: 0.0779 | 0.0407
Epoch 121/300, seasonal_0 Loss: 0.0779 | 0.0407
Epoch 122/300, seasonal_0 Loss: 0.0779 | 0.0406
Epoch 123/300, seasonal_0 Loss: 0.0779 | 0.0406
Epoch 124/300, seasonal_0 Loss: 0.0777 | 0.0406
Epoch 125/300, seasonal_0 Loss: 0.0777 | 0.0406
Epoch 126/300, seasonal_0 Loss: 0.0777 | 0.0406
Epoch 127/300, seasonal_0 Loss: 0.0777 | 0.0406
Epoch 128/300, seasonal_0 Loss: 0.0777 | 0.0406
Epoch 129/300, seasonal_0 Loss: 0.0777 | 0.0405
Epoch 130/300, seasonal_0 Loss: 0.0775 | 0.0405
Epoch 131/300, seasonal_0 Loss: 0.0773 | 0.0404
Epoch 132/300, seasonal_0 Loss: 0.0771 | 0.0404
Epoch 133/300, seasonal_0 Loss: 0.0769 | 0.0404
Epoch 134/300, seasonal_0 Loss: 0.0767 | 0.0403
Epoch 135/300, seasonal_0 Loss: 0.0765 | 0.0403
Epoch 136/300, seasonal_0 Loss: 0.0763 | 0.0403
Epoch 137/300, seasonal_0 Loss: 0.0762 | 0.0403
Epoch 138/300, seasonal_0 Loss: 0.0761 | 0.0403
Epoch 139/300, seasonal_0 Loss: 0.0761 | 0.0402
Epoch 140/300, seasonal_0 Loss: 0.0761 | 0.0402
Epoch 141/300, seasonal_0 Loss: 0.0760 | 0.0402
Epoch 142/300, seasonal_0 Loss: 0.0760 | 0.0401
Epoch 143/300, seasonal_0 Loss: 0.0760 | 0.0401
Epoch 144/300, seasonal_0 Loss: 0.0759 | 0.0400
Epoch 145/300, seasonal_0 Loss: 0.0759 | 0.0400
Epoch 146/300, seasonal_0 Loss: 0.0758 | 0.0400
Epoch 147/300, seasonal_0 Loss: 0.0757 | 0.0399
Epoch 148/300, seasonal_0 Loss: 0.0756 | 0.0399
Epoch 149/300, seasonal_0 Loss: 0.0755 | 0.0398
Epoch 150/300, seasonal_0 Loss: 0.0754 | 0.0398
Epoch 151/300, seasonal_0 Loss: 0.0753 | 0.0398
Epoch 152/300, seasonal_0 Loss: 0.0753 | 0.0397
Epoch 153/300, seasonal_0 Loss: 0.0753 | 0.0397
Epoch 154/300, seasonal_0 Loss: 0.0752 | 0.0397
Epoch 155/300, seasonal_0 Loss: 0.0752 | 0.0396
Epoch 156/300, seasonal_0 Loss: 0.0752 | 0.0396
Epoch 157/300, seasonal_0 Loss: 0.0751 | 0.0396
Epoch 158/300, seasonal_0 Loss: 0.0751 | 0.0396
Epoch 159/300, seasonal_0 Loss: 0.0750 | 0.0395
Epoch 160/300, seasonal_0 Loss: 0.0750 | 0.0395
Epoch 161/300, seasonal_0 Loss: 0.0749 | 0.0395
Epoch 162/300, seasonal_0 Loss: 0.0749 | 0.0395
Epoch 163/300, seasonal_0 Loss: 0.0748 | 0.0395
Epoch 164/300, seasonal_0 Loss: 0.0747 | 0.0395
Epoch 165/300, seasonal_0 Loss: 0.0747 | 0.0394
Epoch 166/300, seasonal_0 Loss: 0.0747 | 0.0394
Epoch 167/300, seasonal_0 Loss: 0.0746 | 0.0394
Epoch 168/300, seasonal_0 Loss: 0.0746 | 0.0394
Epoch 169/300, seasonal_0 Loss: 0.0745 | 0.0394
Epoch 170/300, seasonal_0 Loss: 0.0745 | 0.0393
Epoch 171/300, seasonal_0 Loss: 0.0745 | 0.0393
Epoch 172/300, seasonal_0 Loss: 0.0744 | 0.0393
Epoch 173/300, seasonal_0 Loss: 0.0744 | 0.0393
Epoch 174/300, seasonal_0 Loss: 0.0743 | 0.0393
Epoch 175/300, seasonal_0 Loss: 0.0743 | 0.0392
Epoch 176/300, seasonal_0 Loss: 0.0743 | 0.0392
Epoch 177/300, seasonal_0 Loss: 0.0742 | 0.0392
Epoch 178/300, seasonal_0 Loss: 0.0742 | 0.0392
Epoch 179/300, seasonal_0 Loss: 0.0742 | 0.0392
Epoch 180/300, seasonal_0 Loss: 0.0741 | 0.0391
Epoch 181/300, seasonal_0 Loss: 0.0741 | 0.0391
Epoch 182/300, seasonal_0 Loss: 0.0740 | 0.0391
Epoch 183/300, seasonal_0 Loss: 0.0740 | 0.0391
Epoch 184/300, seasonal_0 Loss: 0.0740 | 0.0390
Epoch 185/300, seasonal_0 Loss: 0.0739 | 0.0390
Epoch 186/300, seasonal_0 Loss: 0.0739 | 0.0390
Epoch 187/300, seasonal_0 Loss: 0.0739 | 0.0390
Epoch 188/300, seasonal_0 Loss: 0.0738 | 0.0390
Epoch 189/300, seasonal_0 Loss: 0.0738 | 0.0390
Epoch 190/300, seasonal_0 Loss: 0.0738 | 0.0389
Epoch 191/300, seasonal_0 Loss: 0.0737 | 0.0389
Epoch 192/300, seasonal_0 Loss: 0.0737 | 0.0389
Epoch 193/300, seasonal_0 Loss: 0.0737 | 0.0389
Epoch 194/300, seasonal_0 Loss: 0.0736 | 0.0389
Epoch 195/300, seasonal_0 Loss: 0.0736 | 0.0389
Epoch 196/300, seasonal_0 Loss: 0.0736 | 0.0389
Epoch 197/300, seasonal_0 Loss: 0.0735 | 0.0388
Epoch 198/300, seasonal_0 Loss: 0.0735 | 0.0388
Epoch 199/300, seasonal_0 Loss: 0.0735 | 0.0388
Epoch 200/300, seasonal_0 Loss: 0.0735 | 0.0388
Epoch 201/300, seasonal_0 Loss: 0.0734 | 0.0388
Epoch 202/300, seasonal_0 Loss: 0.0734 | 0.0388
Epoch 203/300, seasonal_0 Loss: 0.0734 | 0.0387
Epoch 204/300, seasonal_0 Loss: 0.0733 | 0.0387
Epoch 205/300, seasonal_0 Loss: 0.0733 | 0.0387
Epoch 206/300, seasonal_0 Loss: 0.0733 | 0.0387
Epoch 207/300, seasonal_0 Loss: 0.0733 | 0.0387
Epoch 208/300, seasonal_0 Loss: 0.0732 | 0.0387
Epoch 209/300, seasonal_0 Loss: 0.0732 | 0.0387
Epoch 210/300, seasonal_0 Loss: 0.0732 | 0.0386
Epoch 211/300, seasonal_0 Loss: 0.0732 | 0.0386
Epoch 212/300, seasonal_0 Loss: 0.0731 | 0.0386
Epoch 213/300, seasonal_0 Loss: 0.0731 | 0.0386
Epoch 214/300, seasonal_0 Loss: 0.0731 | 0.0386
Epoch 215/300, seasonal_0 Loss: 0.0731 | 0.0386
Epoch 216/300, seasonal_0 Loss: 0.0730 | 0.0386
Epoch 217/300, seasonal_0 Loss: 0.0730 | 0.0386
Epoch 218/300, seasonal_0 Loss: 0.0730 | 0.0386
Epoch 219/300, seasonal_0 Loss: 0.0730 | 0.0385
Epoch 220/300, seasonal_0 Loss: 0.0729 | 0.0385
Epoch 221/300, seasonal_0 Loss: 0.0729 | 0.0385
Epoch 222/300, seasonal_0 Loss: 0.0729 | 0.0385
Epoch 223/300, seasonal_0 Loss: 0.0729 | 0.0385
Epoch 224/300, seasonal_0 Loss: 0.0729 | 0.0385
Epoch 225/300, seasonal_0 Loss: 0.0728 | 0.0385
Epoch 226/300, seasonal_0 Loss: 0.0728 | 0.0385
Epoch 227/300, seasonal_0 Loss: 0.0728 | 0.0385
Epoch 228/300, seasonal_0 Loss: 0.0728 | 0.0384
Epoch 229/300, seasonal_0 Loss: 0.0728 | 0.0384
Epoch 230/300, seasonal_0 Loss: 0.0727 | 0.0384
Epoch 231/300, seasonal_0 Loss: 0.0727 | 0.0384
Epoch 232/300, seasonal_0 Loss: 0.0727 | 0.0384
Epoch 233/300, seasonal_0 Loss: 0.0727 | 0.0384
Epoch 234/300, seasonal_0 Loss: 0.0727 | 0.0384
Epoch 235/300, seasonal_0 Loss: 0.0726 | 0.0384
Epoch 236/300, seasonal_0 Loss: 0.0726 | 0.0384
Epoch 237/300, seasonal_0 Loss: 0.0726 | 0.0384
Epoch 238/300, seasonal_0 Loss: 0.0726 | 0.0384
Epoch 239/300, seasonal_0 Loss: 0.0726 | 0.0383
Epoch 240/300, seasonal_0 Loss: 0.0726 | 0.0383
Epoch 241/300, seasonal_0 Loss: 0.0725 | 0.0383
Epoch 242/300, seasonal_0 Loss: 0.0725 | 0.0383
Epoch 243/300, seasonal_0 Loss: 0.0725 | 0.0383
Epoch 244/300, seasonal_0 Loss: 0.0725 | 0.0383
Epoch 245/300, seasonal_0 Loss: 0.0725 | 0.0383
Epoch 246/300, seasonal_0 Loss: 0.0725 | 0.0383
Epoch 247/300, seasonal_0 Loss: 0.0724 | 0.0383
Epoch 248/300, seasonal_0 Loss: 0.0724 | 0.0383
Epoch 249/300, seasonal_0 Loss: 0.0724 | 0.0383
Epoch 250/300, seasonal_0 Loss: 0.0724 | 0.0383
Epoch 251/300, seasonal_0 Loss: 0.0724 | 0.0383
Epoch 252/300, seasonal_0 Loss: 0.0724 | 0.0382
Epoch 253/300, seasonal_0 Loss: 0.0724 | 0.0382
Epoch 254/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 255/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 256/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 257/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 258/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 259/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 260/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 261/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 262/300, seasonal_0 Loss: 0.0723 | 0.0382
Epoch 263/300, seasonal_0 Loss: 0.0722 | 0.0382
Epoch 264/300, seasonal_0 Loss: 0.0722 | 0.0382
Epoch 265/300, seasonal_0 Loss: 0.0722 | 0.0382
Epoch 266/300, seasonal_0 Loss: 0.0722 | 0.0382
Epoch 267/300, seasonal_0 Loss: 0.0722 | 0.0382
Epoch 268/300, seasonal_0 Loss: 0.0722 | 0.0382
Epoch 269/300, seasonal_0 Loss: 0.0722 | 0.0381
Epoch 270/300, seasonal_0 Loss: 0.0722 | 0.0381
Epoch 271/300, seasonal_0 Loss: 0.0722 | 0.0381
Epoch 272/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 273/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 274/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 275/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 276/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 277/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 278/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 279/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 280/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 281/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 282/300, seasonal_0 Loss: 0.0721 | 0.0381
Epoch 283/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 284/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 285/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 286/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 287/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 288/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 289/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 290/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 291/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 292/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 293/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 294/300, seasonal_0 Loss: 0.0720 | 0.0381
Epoch 295/300, seasonal_0 Loss: 0.0720 | 0.0380
Epoch 296/300, seasonal_0 Loss: 0.0720 | 0.0380
Epoch 297/300, seasonal_0 Loss: 0.0719 | 0.0380
Epoch 298/300, seasonal_0 Loss: 0.0719 | 0.0380
Epoch 299/300, seasonal_0 Loss: 0.0719 | 0.0380
Epoch 300/300, seasonal_0 Loss: 0.0719 | 0.0380
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.8211588074319079, 'learning_rate': 2.0206492772491583e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8965628073923795}
Epoch 1/300, seasonal_1 Loss: 0.2371 | 0.1203
Epoch 2/300, seasonal_1 Loss: 0.1319 | 0.0925
Epoch 3/300, seasonal_1 Loss: 0.1218 | 0.0894
Epoch 4/300, seasonal_1 Loss: 0.1181 | 0.0999
Epoch 5/300, seasonal_1 Loss: 0.1195 | 0.0938
Epoch 6/300, seasonal_1 Loss: 0.1199 | 0.1269
Epoch 7/300, seasonal_1 Loss: 0.1189 | 0.0810
Epoch 8/300, seasonal_1 Loss: 0.1050 | 0.0630
Epoch 9/300, seasonal_1 Loss: 0.0979 | 0.0621
Epoch 10/300, seasonal_1 Loss: 0.0934 | 0.0609
Epoch 11/300, seasonal_1 Loss: 0.0918 | 0.0607
Epoch 12/300, seasonal_1 Loss: 0.0905 | 0.0620
Epoch 13/300, seasonal_1 Loss: 0.0896 | 0.0631
Epoch 14/300, seasonal_1 Loss: 0.0888 | 0.0630
Epoch 15/300, seasonal_1 Loss: 0.0874 | 0.0562
Epoch 16/300, seasonal_1 Loss: 0.0862 | 0.0530
Epoch 17/300, seasonal_1 Loss: 0.0846 | 0.0481
Epoch 18/300, seasonal_1 Loss: 0.0838 | 0.0448
Epoch 19/300, seasonal_1 Loss: 0.0836 | 0.0444
Epoch 20/300, seasonal_1 Loss: 0.0827 | 0.0432
Epoch 21/300, seasonal_1 Loss: 0.0821 | 0.0430
Epoch 22/300, seasonal_1 Loss: 0.0811 | 0.0428
Epoch 23/300, seasonal_1 Loss: 0.0806 | 0.0425
Epoch 24/300, seasonal_1 Loss: 0.0800 | 0.0422
Epoch 25/300, seasonal_1 Loss: 0.0795 | 0.0418
Epoch 26/300, seasonal_1 Loss: 0.0789 | 0.0419
Epoch 27/300, seasonal_1 Loss: 0.0784 | 0.0419
Epoch 28/300, seasonal_1 Loss: 0.0778 | 0.0422
Epoch 29/300, seasonal_1 Loss: 0.0773 | 0.0432
Epoch 30/300, seasonal_1 Loss: 0.0766 | 0.0438
Epoch 31/300, seasonal_1 Loss: 0.0759 | 0.0442
Epoch 32/300, seasonal_1 Loss: 0.0753 | 0.0451
Epoch 33/300, seasonal_1 Loss: 0.0747 | 0.0461
Epoch 34/300, seasonal_1 Loss: 0.0742 | 0.0472
Epoch 35/300, seasonal_1 Loss: 0.0739 | 0.0485
Epoch 36/300, seasonal_1 Loss: 0.0737 | 0.0505
Epoch 37/300, seasonal_1 Loss: 0.0746 | 0.0556
Epoch 38/300, seasonal_1 Loss: 0.0763 | 0.0654
Epoch 39/300, seasonal_1 Loss: 0.0839 | 0.0932
Epoch 40/300, seasonal_1 Loss: 0.0822 | 0.0609
Epoch 41/300, seasonal_1 Loss: 0.0772 | 0.0491
Epoch 42/300, seasonal_1 Loss: 0.0755 | 0.0495
Epoch 43/300, seasonal_1 Loss: 0.0764 | 0.0561
Epoch 44/300, seasonal_1 Loss: 0.0795 | 0.0535
Epoch 45/300, seasonal_1 Loss: 0.0772 | 0.0467
Epoch 46/300, seasonal_1 Loss: 0.0775 | 0.0447
Epoch 47/300, seasonal_1 Loss: 0.0745 | 0.0414
Epoch 48/300, seasonal_1 Loss: 0.0732 | 0.0393
Epoch 49/300, seasonal_1 Loss: 0.0727 | 0.0390
Epoch 50/300, seasonal_1 Loss: 0.0720 | 0.0402
Epoch 51/300, seasonal_1 Loss: 0.0717 | 0.0388
Epoch 52/300, seasonal_1 Loss: 0.0728 | 0.0444
Epoch 53/300, seasonal_1 Loss: 0.0697 | 0.0409
Epoch 54/300, seasonal_1 Loss: 0.0690 | 0.0406
Epoch 55/300, seasonal_1 Loss: 0.0681 | 0.0404
Epoch 56/300, seasonal_1 Loss: 0.0677 | 0.0407
Epoch 57/300, seasonal_1 Loss: 0.0672 | 0.0403
Epoch 58/300, seasonal_1 Loss: 0.0670 | 0.0393
Epoch 59/300, seasonal_1 Loss: 0.0671 | 0.0379
Epoch 60/300, seasonal_1 Loss: 0.0673 | 0.0401
Epoch 61/300, seasonal_1 Loss: 0.0670 | 0.0378
Epoch 62/300, seasonal_1 Loss: 0.0674 | 0.0407
Epoch 63/300, seasonal_1 Loss: 0.0668 | 0.0382
Epoch 64/300, seasonal_1 Loss: 0.0668 | 0.0395
Epoch 65/300, seasonal_1 Loss: 0.0663 | 0.0382
Epoch 66/300, seasonal_1 Loss: 0.0663 | 0.0396
Epoch 67/300, seasonal_1 Loss: 0.0657 | 0.0378
Epoch 68/300, seasonal_1 Loss: 0.0657 | 0.0394
Epoch 69/300, seasonal_1 Loss: 0.0651 | 0.0376
Epoch 70/300, seasonal_1 Loss: 0.0650 | 0.0391
Epoch 71/300, seasonal_1 Loss: 0.0647 | 0.0386
Epoch 72/300, seasonal_1 Loss: 0.0647 | 0.0393
Epoch 73/300, seasonal_1 Loss: 0.0645 | 0.0382
Epoch 74/300, seasonal_1 Loss: 0.0645 | 0.0392
Epoch 75/300, seasonal_1 Loss: 0.0641 | 0.0378
Epoch 76/300, seasonal_1 Loss: 0.0642 | 0.0388
Epoch 77/300, seasonal_1 Loss: 0.0637 | 0.0374
Epoch 78/300, seasonal_1 Loss: 0.0639 | 0.0384
Epoch 79/300, seasonal_1 Loss: 0.0636 | 0.0370
Epoch 80/300, seasonal_1 Loss: 0.0639 | 0.0378
Epoch 81/300, seasonal_1 Loss: 0.0634 | 0.0367
Epoch 82/300, seasonal_1 Loss: 0.0636 | 0.0377
Epoch 83/300, seasonal_1 Loss: 0.0632 | 0.0365
Epoch 84/300, seasonal_1 Loss: 0.0634 | 0.0376
Epoch 85/300, seasonal_1 Loss: 0.0631 | 0.0366
Epoch 86/300, seasonal_1 Loss: 0.0632 | 0.0373
Epoch 87/300, seasonal_1 Loss: 0.0631 | 0.0367
Epoch 88/300, seasonal_1 Loss: 0.0633 | 0.0381
Epoch 89/300, seasonal_1 Loss: 0.0626 | 0.0372
Epoch 90/300, seasonal_1 Loss: 0.0628 | 0.0389
Epoch 91/300, seasonal_1 Loss: 0.0621 | 0.0377
Epoch 92/300, seasonal_1 Loss: 0.0624 | 0.0415
Epoch 93/300, seasonal_1 Loss: 0.0617 | 0.0406
Epoch 94/300, seasonal_1 Loss: 0.0620 | 0.0424
Epoch 95/300, seasonal_1 Loss: 0.0613 | 0.0411
Epoch 96/300, seasonal_1 Loss: 0.0614 | 0.0427
Epoch 97/300, seasonal_1 Loss: 0.0610 | 0.0416
Epoch 98/300, seasonal_1 Loss: 0.0610 | 0.0426
Epoch 99/300, seasonal_1 Loss: 0.0609 | 0.0437
Epoch 100/300, seasonal_1 Loss: 0.0611 | 0.0432
Epoch 101/300, seasonal_1 Loss: 0.0613 | 0.0425
Epoch 102/300, seasonal_1 Loss: 0.0612 | 0.0421
Epoch 103/300, seasonal_1 Loss: 0.0607 | 0.0412
Epoch 104/300, seasonal_1 Loss: 0.0602 | 0.0403
Epoch 105/300, seasonal_1 Loss: 0.0597 | 0.0397
Epoch 106/300, seasonal_1 Loss: 0.0594 | 0.0394
Epoch 107/300, seasonal_1 Loss: 0.0590 | 0.0383
Epoch 108/300, seasonal_1 Loss: 0.0588 | 0.0386
Epoch 109/300, seasonal_1 Loss: 0.0585 | 0.0386
Epoch 110/300, seasonal_1 Loss: 0.0584 | 0.0389
Epoch 111/300, seasonal_1 Loss: 0.0582 | 0.0392
Epoch 112/300, seasonal_1 Loss: 0.0580 | 0.0397
Epoch 113/300, seasonal_1 Loss: 0.0580 | 0.0397
Epoch 114/300, seasonal_1 Loss: 0.0584 | 0.0394
Epoch 115/300, seasonal_1 Loss: 0.0578 | 0.0400
Epoch 116/300, seasonal_1 Loss: 0.0572 | 0.0416
Epoch 117/300, seasonal_1 Loss: 0.0569 | 0.0429
Epoch 118/300, seasonal_1 Loss: 0.0568 | 0.0436
Epoch 119/300, seasonal_1 Loss: 0.0566 | 0.0439
Epoch 120/300, seasonal_1 Loss: 0.0564 | 0.0455
Epoch 121/300, seasonal_1 Loss: 0.0562 | 0.0449
Epoch 122/300, seasonal_1 Loss: 0.0562 | 0.0441
Epoch 123/300, seasonal_1 Loss: 0.0555 | 0.0435
Epoch 124/300, seasonal_1 Loss: 0.0546 | 0.0424
Epoch 125/300, seasonal_1 Loss: 0.0536 | 0.0422
Epoch 126/300, seasonal_1 Loss: 0.0527 | 0.0415
Epoch 127/300, seasonal_1 Loss: 0.0518 | 0.0410
Epoch 128/300, seasonal_1 Loss: 0.0512 | 0.0406
Epoch 129/300, seasonal_1 Loss: 0.0507 | 0.0405
Epoch 130/300, seasonal_1 Loss: 0.0503 | 0.0400
Epoch 131/300, seasonal_1 Loss: 0.0500 | 0.0401
Epoch 132/300, seasonal_1 Loss: 0.0497 | 0.0394
Epoch 133/300, seasonal_1 Loss: 0.0494 | 0.0398
Epoch 134/300, seasonal_1 Loss: 0.0492 | 0.0387
Epoch 135/300, seasonal_1 Loss: 0.0491 | 0.0395
Epoch 136/300, seasonal_1 Loss: 0.0492 | 0.0374
Epoch 137/300, seasonal_1 Loss: 0.0494 | 0.0435
Epoch 138/300, seasonal_1 Loss: 0.0501 | 0.0372
Epoch 139/300, seasonal_1 Loss: 0.0497 | 0.0431
Epoch 140/300, seasonal_1 Loss: 0.0497 | 0.0375
Epoch 141/300, seasonal_1 Loss: 0.0508 | 0.0450
Epoch 142/300, seasonal_1 Loss: 0.0511 | 0.0385
Epoch 143/300, seasonal_1 Loss: 0.0516 | 0.0426
Epoch 144/300, seasonal_1 Loss: 0.0528 | 0.0414
Epoch 145/300, seasonal_1 Loss: 0.0596 | 0.0346
Epoch 146/300, seasonal_1 Loss: 0.0532 | 0.0362
Epoch 147/300, seasonal_1 Loss: 0.0519 | 0.0459
Epoch 148/300, seasonal_1 Loss: 0.0530 | 0.0350
Epoch 149/300, seasonal_1 Loss: 0.0503 | 0.0498
Epoch 150/300, seasonal_1 Loss: 0.0482 | 0.0350
Epoch 151/300, seasonal_1 Loss: 0.0476 | 0.0359
Epoch 152/300, seasonal_1 Loss: 0.0473 | 0.0362
Epoch 153/300, seasonal_1 Loss: 0.0471 | 0.0363
Epoch 154/300, seasonal_1 Loss: 0.0471 | 0.0364
Epoch 155/300, seasonal_1 Loss: 0.0469 | 0.0364
Epoch 156/300, seasonal_1 Loss: 0.0468 | 0.0365
Epoch 157/300, seasonal_1 Loss: 0.0467 | 0.0365
Epoch 158/300, seasonal_1 Loss: 0.0466 | 0.0365
Epoch 159/300, seasonal_1 Loss: 0.0465 | 0.0366
Epoch 160/300, seasonal_1 Loss: 0.0464 | 0.0366
Epoch 161/300, seasonal_1 Loss: 0.0463 | 0.0366
Epoch 162/300, seasonal_1 Loss: 0.0463 | 0.0367
Epoch 163/300, seasonal_1 Loss: 0.0462 | 0.0367
Epoch 164/300, seasonal_1 Loss: 0.0461 | 0.0367
Epoch 165/300, seasonal_1 Loss: 0.0460 | 0.0367
Epoch 166/300, seasonal_1 Loss: 0.0459 | 0.0367
Epoch 167/300, seasonal_1 Loss: 0.0459 | 0.0367
Epoch 168/300, seasonal_1 Loss: 0.0458 | 0.0367
Epoch 169/300, seasonal_1 Loss: 0.0457 | 0.0367
Epoch 170/300, seasonal_1 Loss: 0.0457 | 0.0367
Epoch 171/300, seasonal_1 Loss: 0.0456 | 0.0367
Epoch 172/300, seasonal_1 Loss: 0.0456 | 0.0367
Epoch 173/300, seasonal_1 Loss: 0.0455 | 0.0367
Epoch 174/300, seasonal_1 Loss: 0.0455 | 0.0366
Epoch 175/300, seasonal_1 Loss: 0.0454 | 0.0366
Epoch 176/300, seasonal_1 Loss: 0.0453 | 0.0365
Epoch 177/300, seasonal_1 Loss: 0.0453 | 0.0365
Epoch 178/300, seasonal_1 Loss: 0.0452 | 0.0365
Epoch 179/300, seasonal_1 Loss: 0.0452 | 0.0365
Epoch 180/300, seasonal_1 Loss: 0.0451 | 0.0365
Epoch 181/300, seasonal_1 Loss: 0.0451 | 0.0365
Epoch 182/300, seasonal_1 Loss: 0.0450 | 0.0365
Epoch 183/300, seasonal_1 Loss: 0.0450 | 0.0364
Epoch 184/300, seasonal_1 Loss: 0.0449 | 0.0365
Epoch 185/300, seasonal_1 Loss: 0.0449 | 0.0365
Epoch 186/300, seasonal_1 Loss: 0.0448 | 0.0365
Epoch 187/300, seasonal_1 Loss: 0.0448 | 0.0365
Epoch 188/300, seasonal_1 Loss: 0.0448 | 0.0365
Epoch 189/300, seasonal_1 Loss: 0.0447 | 0.0365
Epoch 190/300, seasonal_1 Loss: 0.0447 | 0.0365
Epoch 191/300, seasonal_1 Loss: 0.0446 | 0.0365
Epoch 192/300, seasonal_1 Loss: 0.0446 | 0.0365
Epoch 193/300, seasonal_1 Loss: 0.0445 | 0.0366
Epoch 194/300, seasonal_1 Loss: 0.0445 | 0.0366
Epoch 195/300, seasonal_1 Loss: 0.0444 | 0.0366
Epoch 196/300, seasonal_1 Loss: 0.0444 | 0.0366
Epoch 197/300, seasonal_1 Loss: 0.0444 | 0.0366
Epoch 198/300, seasonal_1 Loss: 0.0443 | 0.0367
Epoch 199/300, seasonal_1 Loss: 0.0443 | 0.0367
Epoch 200/300, seasonal_1 Loss: 0.0442 | 0.0367
Epoch 201/300, seasonal_1 Loss: 0.0442 | 0.0367
Epoch 202/300, seasonal_1 Loss: 0.0442 | 0.0367
Epoch 203/300, seasonal_1 Loss: 0.0441 | 0.0368
Epoch 204/300, seasonal_1 Loss: 0.0441 | 0.0368
Epoch 205/300, seasonal_1 Loss: 0.0440 | 0.0368
Epoch 206/300, seasonal_1 Loss: 0.0440 | 0.0368
Epoch 207/300, seasonal_1 Loss: 0.0440 | 0.0368
Epoch 208/300, seasonal_1 Loss: 0.0439 | 0.0368
Epoch 209/300, seasonal_1 Loss: 0.0439 | 0.0368
Epoch 210/300, seasonal_1 Loss: 0.0439 | 0.0368
Epoch 211/300, seasonal_1 Loss: 0.0438 | 0.0368
Epoch 212/300, seasonal_1 Loss: 0.0438 | 0.0368
Epoch 213/300, seasonal_1 Loss: 0.0438 | 0.0368
Epoch 214/300, seasonal_1 Loss: 0.0437 | 0.0368
Epoch 215/300, seasonal_1 Loss: 0.0437 | 0.0368
Epoch 216/300, seasonal_1 Loss: 0.0437 | 0.0368
Epoch 217/300, seasonal_1 Loss: 0.0436 | 0.0368
Epoch 218/300, seasonal_1 Loss: 0.0436 | 0.0368
Epoch 219/300, seasonal_1 Loss: 0.0436 | 0.0368
Epoch 220/300, seasonal_1 Loss: 0.0435 | 0.0368
Epoch 221/300, seasonal_1 Loss: 0.0435 | 0.0368
Epoch 222/300, seasonal_1 Loss: 0.0435 | 0.0368
Epoch 223/300, seasonal_1 Loss: 0.0434 | 0.0368
Epoch 224/300, seasonal_1 Loss: 0.0434 | 0.0368
Epoch 225/300, seasonal_1 Loss: 0.0434 | 0.0368
Epoch 226/300, seasonal_1 Loss: 0.0433 | 0.0368
Epoch 227/300, seasonal_1 Loss: 0.0433 | 0.0369
Epoch 228/300, seasonal_1 Loss: 0.0433 | 0.0369
Epoch 229/300, seasonal_1 Loss: 0.0433 | 0.0369
Epoch 230/300, seasonal_1 Loss: 0.0432 | 0.0369
Epoch 231/300, seasonal_1 Loss: 0.0432 | 0.0369
Epoch 232/300, seasonal_1 Loss: 0.0432 | 0.0369
Epoch 233/300, seasonal_1 Loss: 0.0432 | 0.0369
Epoch 234/300, seasonal_1 Loss: 0.0431 | 0.0369
Epoch 235/300, seasonal_1 Loss: 0.0431 | 0.0369
Epoch 236/300, seasonal_1 Loss: 0.0431 | 0.0370
Epoch 237/300, seasonal_1 Loss: 0.0431 | 0.0370
Epoch 238/300, seasonal_1 Loss: 0.0430 | 0.0370
Epoch 239/300, seasonal_1 Loss: 0.0430 | 0.0370
Epoch 240/300, seasonal_1 Loss: 0.0430 | 0.0370
Epoch 241/300, seasonal_1 Loss: 0.0430 | 0.0370
Epoch 242/300, seasonal_1 Loss: 0.0429 | 0.0370
Epoch 243/300, seasonal_1 Loss: 0.0429 | 0.0370
Epoch 244/300, seasonal_1 Loss: 0.0429 | 0.0370
Epoch 245/300, seasonal_1 Loss: 0.0429 | 0.0370
Epoch 246/300, seasonal_1 Loss: 0.0428 | 0.0370
Epoch 247/300, seasonal_1 Loss: 0.0428 | 0.0370
Epoch 248/300, seasonal_1 Loss: 0.0428 | 0.0370
Epoch 249/300, seasonal_1 Loss: 0.0428 | 0.0370
Epoch 250/300, seasonal_1 Loss: 0.0428 | 0.0370
Epoch 251/300, seasonal_1 Loss: 0.0427 | 0.0370
Epoch 252/300, seasonal_1 Loss: 0.0427 | 0.0370
Epoch 253/300, seasonal_1 Loss: 0.0427 | 0.0370
Epoch 254/300, seasonal_1 Loss: 0.0427 | 0.0370
Epoch 255/300, seasonal_1 Loss: 0.0427 | 0.0370
Epoch 256/300, seasonal_1 Loss: 0.0426 | 0.0370
Epoch 257/300, seasonal_1 Loss: 0.0426 | 0.0371
Epoch 258/300, seasonal_1 Loss: 0.0426 | 0.0371
Epoch 259/300, seasonal_1 Loss: 0.0426 | 0.0371
Epoch 260/300, seasonal_1 Loss: 0.0426 | 0.0371
Epoch 261/300, seasonal_1 Loss: 0.0426 | 0.0371
Epoch 262/300, seasonal_1 Loss: 0.0425 | 0.0371
Epoch 263/300, seasonal_1 Loss: 0.0425 | 0.0371
Epoch 264/300, seasonal_1 Loss: 0.0425 | 0.0371
Epoch 265/300, seasonal_1 Loss: 0.0425 | 0.0371
Epoch 266/300, seasonal_1 Loss: 0.0425 | 0.0371
Epoch 267/300, seasonal_1 Loss: 0.0425 | 0.0371
Epoch 268/300, seasonal_1 Loss: 0.0424 | 0.0371
Epoch 269/300, seasonal_1 Loss: 0.0424 | 0.0371
Epoch 270/300, seasonal_1 Loss: 0.0424 | 0.0371
Epoch 271/300, seasonal_1 Loss: 0.0424 | 0.0371
Epoch 272/300, seasonal_1 Loss: 0.0424 | 0.0371
Epoch 273/300, seasonal_1 Loss: 0.0424 | 0.0371
Epoch 274/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 275/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 276/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 277/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 278/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 279/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 280/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 281/300, seasonal_1 Loss: 0.0423 | 0.0371
Epoch 282/300, seasonal_1 Loss: 0.0422 | 0.0371
Epoch 283/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 284/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 285/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 286/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 287/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 288/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 289/300, seasonal_1 Loss: 0.0422 | 0.0372
Epoch 290/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 291/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 292/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 293/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 294/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 295/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 296/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 297/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 298/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 299/300, seasonal_1 Loss: 0.0421 | 0.0372
Epoch 300/300, seasonal_1 Loss: 0.0420 | 0.0372
Training seasonal_2 component with params: {'observation_period_num': 38, 'train_rates': 0.9881284708060533, 'learning_rate': 0.00020496325581387964, 'batch_size': 145, 'step_size': 7, 'gamma': 0.8546632863298388}
Epoch 1/300, seasonal_2 Loss: 1.3387 | 0.2120
Epoch 2/300, seasonal_2 Loss: 0.3157 | 0.3009
Epoch 3/300, seasonal_2 Loss: 0.2302 | 0.1840
Epoch 4/300, seasonal_2 Loss: 0.1876 | 0.1470
Epoch 5/300, seasonal_2 Loss: 0.2084 | 0.1382
Epoch 6/300, seasonal_2 Loss: 0.1721 | 0.1370
Epoch 7/300, seasonal_2 Loss: 0.1655 | 0.1191
Epoch 8/300, seasonal_2 Loss: 0.1815 | 0.1388
Epoch 9/300, seasonal_2 Loss: 0.2207 | 0.1304
Epoch 10/300, seasonal_2 Loss: 0.2862 | 0.1432
Epoch 11/300, seasonal_2 Loss: 0.2922 | 0.2133
Epoch 12/300, seasonal_2 Loss: 0.2871 | 0.4159
Epoch 13/300, seasonal_2 Loss: 0.2088 | 0.1784
Epoch 14/300, seasonal_2 Loss: 0.1886 | 0.1943
Epoch 15/300, seasonal_2 Loss: 0.1922 | 0.1243
Epoch 16/300, seasonal_2 Loss: 0.1913 | 0.0916
Epoch 17/300, seasonal_2 Loss: 0.2184 | 0.0859
Epoch 18/300, seasonal_2 Loss: 0.2118 | 0.1284
Epoch 19/300, seasonal_2 Loss: 0.1807 | 0.1445
Epoch 20/300, seasonal_2 Loss: 0.1598 | 0.1262
Epoch 21/300, seasonal_2 Loss: 0.1562 | 0.0820
Epoch 22/300, seasonal_2 Loss: 0.1419 | 0.0938
Epoch 23/300, seasonal_2 Loss: 0.1269 | 0.0829
Epoch 24/300, seasonal_2 Loss: 0.1191 | 0.0649
Epoch 25/300, seasonal_2 Loss: 0.1023 | 0.0749
Epoch 26/300, seasonal_2 Loss: 0.1504 | 0.0862
Epoch 27/300, seasonal_2 Loss: 0.1332 | 0.1853
Epoch 28/300, seasonal_2 Loss: 0.1207 | 0.0713
Epoch 29/300, seasonal_2 Loss: 0.1069 | 0.0597
Epoch 30/300, seasonal_2 Loss: 0.0946 | 0.0929
Epoch 31/300, seasonal_2 Loss: 0.0953 | 0.1042
Epoch 32/300, seasonal_2 Loss: 0.0900 | 0.0940
Epoch 33/300, seasonal_2 Loss: 0.0839 | 0.0565
Epoch 34/300, seasonal_2 Loss: 0.0833 | 0.0549
Epoch 35/300, seasonal_2 Loss: 0.0794 | 0.0728
Epoch 36/300, seasonal_2 Loss: 0.0775 | 0.0791
Epoch 37/300, seasonal_2 Loss: 0.0731 | 0.0574
Epoch 38/300, seasonal_2 Loss: 0.0738 | 0.0531
Epoch 39/300, seasonal_2 Loss: 0.0716 | 0.0608
Epoch 40/300, seasonal_2 Loss: 0.0719 | 0.0715
Epoch 41/300, seasonal_2 Loss: 0.0690 | 0.0583
Epoch 42/300, seasonal_2 Loss: 0.0695 | 0.0509
Epoch 43/300, seasonal_2 Loss: 0.0680 | 0.0576
Epoch 44/300, seasonal_2 Loss: 0.0678 | 0.0642
Epoch 45/300, seasonal_2 Loss: 0.0664 | 0.0556
Epoch 46/300, seasonal_2 Loss: 0.0662 | 0.0503
Epoch 47/300, seasonal_2 Loss: 0.0654 | 0.0555
Epoch 48/300, seasonal_2 Loss: 0.0651 | 0.0580
Epoch 49/300, seasonal_2 Loss: 0.0644 | 0.0532
Epoch 50/300, seasonal_2 Loss: 0.0641 | 0.0506
Epoch 51/300, seasonal_2 Loss: 0.0635 | 0.0542
Epoch 52/300, seasonal_2 Loss: 0.0633 | 0.0533
Epoch 53/300, seasonal_2 Loss: 0.0627 | 0.0508
Epoch 54/300, seasonal_2 Loss: 0.0626 | 0.0504
Epoch 55/300, seasonal_2 Loss: 0.0622 | 0.0521
Epoch 56/300, seasonal_2 Loss: 0.0620 | 0.0506
Epoch 57/300, seasonal_2 Loss: 0.0616 | 0.0494
Epoch 58/300, seasonal_2 Loss: 0.0614 | 0.0498
Epoch 59/300, seasonal_2 Loss: 0.0612 | 0.0499
Epoch 60/300, seasonal_2 Loss: 0.0610 | 0.0488
Epoch 61/300, seasonal_2 Loss: 0.0607 | 0.0486
Epoch 62/300, seasonal_2 Loss: 0.0606 | 0.0484
Epoch 63/300, seasonal_2 Loss: 0.0604 | 0.0481
Epoch 64/300, seasonal_2 Loss: 0.0602 | 0.0475
Epoch 65/300, seasonal_2 Loss: 0.0600 | 0.0474
Epoch 66/300, seasonal_2 Loss: 0.0599 | 0.0471
Epoch 67/300, seasonal_2 Loss: 0.0597 | 0.0468
Epoch 68/300, seasonal_2 Loss: 0.0595 | 0.0463
Epoch 69/300, seasonal_2 Loss: 0.0594 | 0.0461
Epoch 70/300, seasonal_2 Loss: 0.0593 | 0.0458
Epoch 71/300, seasonal_2 Loss: 0.0591 | 0.0455
Epoch 72/300, seasonal_2 Loss: 0.0590 | 0.0452
Epoch 73/300, seasonal_2 Loss: 0.0589 | 0.0449
Epoch 74/300, seasonal_2 Loss: 0.0587 | 0.0446
Epoch 75/300, seasonal_2 Loss: 0.0586 | 0.0442
Epoch 76/300, seasonal_2 Loss: 0.0585 | 0.0440
Epoch 77/300, seasonal_2 Loss: 0.0584 | 0.0437
Epoch 78/300, seasonal_2 Loss: 0.0583 | 0.0433
Epoch 79/300, seasonal_2 Loss: 0.0582 | 0.0431
Epoch 80/300, seasonal_2 Loss: 0.0581 | 0.0428
Epoch 81/300, seasonal_2 Loss: 0.0580 | 0.0425
Epoch 82/300, seasonal_2 Loss: 0.0578 | 0.0422
Epoch 83/300, seasonal_2 Loss: 0.0577 | 0.0419
Epoch 84/300, seasonal_2 Loss: 0.0577 | 0.0417
Epoch 85/300, seasonal_2 Loss: 0.0576 | 0.0414
Epoch 86/300, seasonal_2 Loss: 0.0575 | 0.0411
Epoch 87/300, seasonal_2 Loss: 0.0574 | 0.0409
Epoch 88/300, seasonal_2 Loss: 0.0573 | 0.0406
Epoch 89/300, seasonal_2 Loss: 0.0572 | 0.0404
Epoch 90/300, seasonal_2 Loss: 0.0571 | 0.0402
Epoch 91/300, seasonal_2 Loss: 0.0571 | 0.0399
Epoch 92/300, seasonal_2 Loss: 0.0570 | 0.0397
Epoch 93/300, seasonal_2 Loss: 0.0569 | 0.0395
Epoch 94/300, seasonal_2 Loss: 0.0569 | 0.0393
Epoch 95/300, seasonal_2 Loss: 0.0568 | 0.0391
Epoch 96/300, seasonal_2 Loss: 0.0567 | 0.0390
Epoch 97/300, seasonal_2 Loss: 0.0567 | 0.0388
Epoch 98/300, seasonal_2 Loss: 0.0566 | 0.0386
Epoch 99/300, seasonal_2 Loss: 0.0565 | 0.0385
Epoch 100/300, seasonal_2 Loss: 0.0565 | 0.0383
Epoch 101/300, seasonal_2 Loss: 0.0564 | 0.0382
Epoch 102/300, seasonal_2 Loss: 0.0564 | 0.0380
Epoch 103/300, seasonal_2 Loss: 0.0563 | 0.0379
Epoch 104/300, seasonal_2 Loss: 0.0563 | 0.0378
Epoch 105/300, seasonal_2 Loss: 0.0562 | 0.0376
Epoch 106/300, seasonal_2 Loss: 0.0562 | 0.0375
Epoch 107/300, seasonal_2 Loss: 0.0562 | 0.0374
Epoch 108/300, seasonal_2 Loss: 0.0561 | 0.0373
Epoch 109/300, seasonal_2 Loss: 0.0561 | 0.0372
Epoch 110/300, seasonal_2 Loss: 0.0560 | 0.0371
Epoch 111/300, seasonal_2 Loss: 0.0560 | 0.0370
Epoch 112/300, seasonal_2 Loss: 0.0560 | 0.0369
Epoch 113/300, seasonal_2 Loss: 0.0559 | 0.0368
Epoch 114/300, seasonal_2 Loss: 0.0559 | 0.0367
Epoch 115/300, seasonal_2 Loss: 0.0559 | 0.0367
Epoch 116/300, seasonal_2 Loss: 0.0559 | 0.0366
Epoch 117/300, seasonal_2 Loss: 0.0558 | 0.0365
Epoch 118/300, seasonal_2 Loss: 0.0558 | 0.0365
Epoch 119/300, seasonal_2 Loss: 0.0558 | 0.0364
Epoch 120/300, seasonal_2 Loss: 0.0557 | 0.0363
Epoch 121/300, seasonal_2 Loss: 0.0557 | 0.0363
Epoch 122/300, seasonal_2 Loss: 0.0557 | 0.0362
Epoch 123/300, seasonal_2 Loss: 0.0557 | 0.0362
Epoch 124/300, seasonal_2 Loss: 0.0557 | 0.0361
Epoch 125/300, seasonal_2 Loss: 0.0556 | 0.0361
Epoch 126/300, seasonal_2 Loss: 0.0556 | 0.0360
Epoch 127/300, seasonal_2 Loss: 0.0556 | 0.0360
Epoch 128/300, seasonal_2 Loss: 0.0556 | 0.0359
Epoch 129/300, seasonal_2 Loss: 0.0556 | 0.0359
Epoch 130/300, seasonal_2 Loss: 0.0555 | 0.0358
Epoch 131/300, seasonal_2 Loss: 0.0555 | 0.0358
Epoch 132/300, seasonal_2 Loss: 0.0555 | 0.0358
Epoch 133/300, seasonal_2 Loss: 0.0555 | 0.0357
Epoch 134/300, seasonal_2 Loss: 0.0555 | 0.0357
Epoch 135/300, seasonal_2 Loss: 0.0555 | 0.0357
Epoch 136/300, seasonal_2 Loss: 0.0555 | 0.0356
Epoch 137/300, seasonal_2 Loss: 0.0554 | 0.0356
Epoch 138/300, seasonal_2 Loss: 0.0554 | 0.0356
Epoch 139/300, seasonal_2 Loss: 0.0554 | 0.0355
Epoch 140/300, seasonal_2 Loss: 0.0554 | 0.0355
Epoch 141/300, seasonal_2 Loss: 0.0554 | 0.0355
Epoch 142/300, seasonal_2 Loss: 0.0554 | 0.0355
Epoch 143/300, seasonal_2 Loss: 0.0554 | 0.0354
Epoch 144/300, seasonal_2 Loss: 0.0554 | 0.0354
Epoch 145/300, seasonal_2 Loss: 0.0554 | 0.0354
Epoch 146/300, seasonal_2 Loss: 0.0553 | 0.0354
Epoch 147/300, seasonal_2 Loss: 0.0553 | 0.0354
Epoch 148/300, seasonal_2 Loss: 0.0553 | 0.0353
Epoch 149/300, seasonal_2 Loss: 0.0553 | 0.0353
Epoch 150/300, seasonal_2 Loss: 0.0553 | 0.0353
Epoch 151/300, seasonal_2 Loss: 0.0553 | 0.0353
Epoch 152/300, seasonal_2 Loss: 0.0553 | 0.0353
Epoch 153/300, seasonal_2 Loss: 0.0553 | 0.0353
Epoch 154/300, seasonal_2 Loss: 0.0553 | 0.0352
Epoch 155/300, seasonal_2 Loss: 0.0553 | 0.0352
Epoch 156/300, seasonal_2 Loss: 0.0553 | 0.0352
Epoch 157/300, seasonal_2 Loss: 0.0553 | 0.0352
Epoch 158/300, seasonal_2 Loss: 0.0553 | 0.0352
Epoch 159/300, seasonal_2 Loss: 0.0552 | 0.0352
Epoch 160/300, seasonal_2 Loss: 0.0552 | 0.0352
Epoch 161/300, seasonal_2 Loss: 0.0552 | 0.0352
Epoch 162/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 163/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 164/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 165/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 166/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 167/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 168/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 169/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 170/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 171/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 172/300, seasonal_2 Loss: 0.0552 | 0.0351
Epoch 173/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 174/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 175/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 176/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 177/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 178/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 179/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 180/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 181/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 182/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 183/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 184/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 185/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 186/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 187/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 188/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 189/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 190/300, seasonal_2 Loss: 0.0552 | 0.0350
Epoch 191/300, seasonal_2 Loss: 0.0551 | 0.0350
Epoch 192/300, seasonal_2 Loss: 0.0551 | 0.0350
Epoch 193/300, seasonal_2 Loss: 0.0551 | 0.0350
Epoch 194/300, seasonal_2 Loss: 0.0551 | 0.0350
Epoch 195/300, seasonal_2 Loss: 0.0551 | 0.0350
Epoch 196/300, seasonal_2 Loss: 0.0551 | 0.0350
Epoch 197/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 198/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 199/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 200/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 201/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 202/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 203/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 204/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 205/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 206/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 207/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 208/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 209/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 210/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 211/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 212/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 213/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 214/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 215/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 216/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 217/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 218/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 219/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 220/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 221/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 222/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 223/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 224/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 225/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 226/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 227/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 228/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 229/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 230/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 231/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 232/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 233/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 234/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 235/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 236/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 237/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 238/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 239/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 240/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 241/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 242/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 243/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 244/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 245/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 246/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 247/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 248/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 249/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 250/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 251/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 252/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 253/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 254/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 255/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 256/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 257/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 258/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 259/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 260/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 261/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 262/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 263/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 264/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 265/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 266/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 267/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 268/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 269/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 270/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 271/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 272/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 273/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 274/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 275/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 276/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 277/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 278/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 279/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 280/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 281/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 282/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 283/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 284/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 285/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 286/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 287/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 288/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 289/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 290/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 291/300, seasonal_2 Loss: 0.0551 | 0.0349
Epoch 292/300, seasonal_2 Loss: 0.0551 | 0.0349
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 10, 'train_rates': 0.9873092072126353, 'learning_rate': 0.00020120158182884257, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9624439005724288}
Epoch 1/300, seasonal_3 Loss: 0.3106 | 0.0912
Epoch 2/300, seasonal_3 Loss: 0.1190 | 0.0629
Epoch 3/300, seasonal_3 Loss: 0.1045 | 0.0593
Epoch 4/300, seasonal_3 Loss: 0.1011 | 0.0667
Epoch 5/300, seasonal_3 Loss: 0.1000 | 0.0615
Epoch 6/300, seasonal_3 Loss: 0.1016 | 0.0534
Epoch 7/300, seasonal_3 Loss: 0.1017 | 0.0579
Epoch 8/300, seasonal_3 Loss: 0.0981 | 0.0814
Epoch 9/300, seasonal_3 Loss: 0.0904 | 0.0596
Epoch 10/300, seasonal_3 Loss: 0.0965 | 0.0623
Epoch 11/300, seasonal_3 Loss: 0.0940 | 0.0553
Epoch 12/300, seasonal_3 Loss: 0.0929 | 0.0577
Epoch 13/300, seasonal_3 Loss: 0.0986 | 0.0478
Epoch 14/300, seasonal_3 Loss: 0.0878 | 0.0551
Epoch 15/300, seasonal_3 Loss: 0.0857 | 0.0495
Epoch 16/300, seasonal_3 Loss: 0.0836 | 0.0564
Epoch 17/300, seasonal_3 Loss: 0.0780 | 0.0574
Epoch 18/300, seasonal_3 Loss: 0.0765 | 0.0427
Epoch 19/300, seasonal_3 Loss: 0.0804 | 0.0470
Epoch 20/300, seasonal_3 Loss: 0.0841 | 0.0434
Epoch 21/300, seasonal_3 Loss: 0.0736 | 0.0540
Epoch 22/300, seasonal_3 Loss: 0.0783 | 0.0491
Epoch 23/300, seasonal_3 Loss: 0.0754 | 0.0512
Epoch 24/300, seasonal_3 Loss: 0.0710 | 0.0406
Epoch 25/300, seasonal_3 Loss: 0.0671 | 0.0400
Epoch 26/300, seasonal_3 Loss: 0.0729 | 0.0364
Epoch 27/300, seasonal_3 Loss: 0.0700 | 0.0424
Epoch 28/300, seasonal_3 Loss: 0.0656 | 0.0368
Epoch 29/300, seasonal_3 Loss: 0.0670 | 0.0435
Epoch 30/300, seasonal_3 Loss: 0.0714 | 0.0503
Epoch 31/300, seasonal_3 Loss: 0.0650 | 0.0459
Epoch 32/300, seasonal_3 Loss: 0.0600 | 0.0368
Epoch 33/300, seasonal_3 Loss: 0.0577 | 0.0467
Epoch 34/300, seasonal_3 Loss: 0.0613 | 0.0457
Epoch 35/300, seasonal_3 Loss: 0.0583 | 0.0627
Epoch 36/300, seasonal_3 Loss: 0.0780 | 0.0546
Epoch 37/300, seasonal_3 Loss: 0.0630 | 0.0330
Epoch 38/300, seasonal_3 Loss: 0.0642 | 0.0409
Epoch 39/300, seasonal_3 Loss: 0.0639 | 0.0422
Epoch 40/300, seasonal_3 Loss: 0.0619 | 0.0391
Epoch 41/300, seasonal_3 Loss: 0.0569 | 0.0438
Epoch 42/300, seasonal_3 Loss: 0.0592 | 0.0389
Epoch 43/300, seasonal_3 Loss: 0.0537 | 0.0513
Epoch 44/300, seasonal_3 Loss: 0.0520 | 0.0538
Epoch 45/300, seasonal_3 Loss: 0.0499 | 0.0508
Epoch 46/300, seasonal_3 Loss: 0.0558 | 0.0525
Epoch 47/300, seasonal_3 Loss: 0.0527 | 0.0362
Epoch 48/300, seasonal_3 Loss: 0.0511 | 0.0504
Epoch 49/300, seasonal_3 Loss: 0.0522 | 0.0478
Epoch 50/300, seasonal_3 Loss: 0.0436 | 0.0369
Epoch 51/300, seasonal_3 Loss: 0.0484 | 0.0375
Epoch 52/300, seasonal_3 Loss: 0.0563 | 0.0441
Epoch 53/300, seasonal_3 Loss: 0.0518 | 0.0368
Epoch 54/300, seasonal_3 Loss: 0.0459 | 0.0428
Epoch 55/300, seasonal_3 Loss: 0.0418 | 0.0387
Epoch 56/300, seasonal_3 Loss: 0.0490 | 0.0401
Epoch 57/300, seasonal_3 Loss: 0.0517 | 0.0463
Epoch 58/300, seasonal_3 Loss: 0.0452 | 0.0450
Epoch 59/300, seasonal_3 Loss: 0.0471 | 0.0399
Epoch 60/300, seasonal_3 Loss: 0.0423 | 0.0367
Epoch 61/300, seasonal_3 Loss: 0.0405 | 0.0320
Epoch 62/300, seasonal_3 Loss: 0.0469 | 0.0420
Epoch 63/300, seasonal_3 Loss: 0.0503 | 0.0325
Epoch 64/300, seasonal_3 Loss: 0.0563 | 0.0386
Epoch 65/300, seasonal_3 Loss: 0.0487 | 0.0413
Epoch 66/300, seasonal_3 Loss: 0.0468 | 0.0328
Epoch 67/300, seasonal_3 Loss: 0.0433 | 0.0357
Epoch 68/300, seasonal_3 Loss: 0.0427 | 0.0496
Epoch 69/300, seasonal_3 Loss: 0.0446 | 0.0355
Epoch 70/300, seasonal_3 Loss: 0.0395 | 0.0333
Epoch 71/300, seasonal_3 Loss: 0.0367 | 0.0281
Epoch 72/300, seasonal_3 Loss: 0.0367 | 0.0302
Epoch 73/300, seasonal_3 Loss: 0.0388 | 0.0301
Epoch 74/300, seasonal_3 Loss: 0.0329 | 0.0374
Epoch 75/300, seasonal_3 Loss: 0.0352 | 0.0506
Epoch 76/300, seasonal_3 Loss: 0.0398 | 0.0537
Epoch 77/300, seasonal_3 Loss: 0.0408 | 0.0392
Epoch 78/300, seasonal_3 Loss: 0.0385 | 0.0528
Epoch 79/300, seasonal_3 Loss: 0.0500 | 0.0402
Epoch 80/300, seasonal_3 Loss: 0.0409 | 0.0299
Epoch 81/300, seasonal_3 Loss: 0.0405 | 0.0310
Epoch 82/300, seasonal_3 Loss: 0.0354 | 0.0273
Epoch 83/300, seasonal_3 Loss: 0.0324 | 0.0291
Epoch 84/300, seasonal_3 Loss: 0.0315 | 0.0284
Epoch 85/300, seasonal_3 Loss: 0.0309 | 0.0350
Epoch 86/300, seasonal_3 Loss: 0.0331 | 0.0410
Epoch 87/300, seasonal_3 Loss: 0.0310 | 0.0300
Epoch 88/300, seasonal_3 Loss: 0.0287 | 0.0324
Epoch 89/300, seasonal_3 Loss: 0.0288 | 0.0346
Epoch 90/300, seasonal_3 Loss: 0.0291 | 0.0355
Epoch 91/300, seasonal_3 Loss: 0.0395 | 0.0406
Epoch 92/300, seasonal_3 Loss: 0.0353 | 0.0396
Epoch 93/300, seasonal_3 Loss: 0.0400 | 0.0446
Epoch 94/300, seasonal_3 Loss: 0.0323 | 0.0321
Epoch 95/300, seasonal_3 Loss: 0.0302 | 0.0286
Epoch 96/300, seasonal_3 Loss: 0.0302 | 0.0284
Epoch 97/300, seasonal_3 Loss: 0.0349 | 0.0398
Epoch 98/300, seasonal_3 Loss: 0.0322 | 0.0313
Epoch 99/300, seasonal_3 Loss: 0.0319 | 0.0377
Epoch 100/300, seasonal_3 Loss: 0.0305 | 0.0401
Epoch 101/300, seasonal_3 Loss: 0.0338 | 0.0464
Epoch 102/300, seasonal_3 Loss: 0.0333 | 0.0393
Epoch 103/300, seasonal_3 Loss: 0.0288 | 0.0348
Epoch 104/300, seasonal_3 Loss: 0.0289 | 0.0305
Epoch 105/300, seasonal_3 Loss: 0.0284 | 0.0313
Epoch 106/300, seasonal_3 Loss: 0.0267 | 0.0368
Epoch 107/300, seasonal_3 Loss: 0.0267 | 0.0341
Epoch 108/300, seasonal_3 Loss: 0.0262 | 0.0363
Epoch 109/300, seasonal_3 Loss: 0.0276 | 0.0320
Epoch 110/300, seasonal_3 Loss: 0.0271 | 0.0382
Epoch 111/300, seasonal_3 Loss: 0.0291 | 0.0342
Epoch 112/300, seasonal_3 Loss: 0.0281 | 0.0396
Epoch 113/300, seasonal_3 Loss: 0.0250 | 0.0354
Epoch 114/300, seasonal_3 Loss: 0.0237 | 0.0323
Epoch 115/300, seasonal_3 Loss: 0.0232 | 0.0341
Epoch 116/300, seasonal_3 Loss: 0.0248 | 0.0315
Epoch 117/300, seasonal_3 Loss: 0.0247 | 0.0277
Epoch 118/300, seasonal_3 Loss: 0.0252 | 0.0310
Epoch 119/300, seasonal_3 Loss: 0.0247 | 0.0296
Epoch 120/300, seasonal_3 Loss: 0.0239 | 0.0296
Epoch 121/300, seasonal_3 Loss: 0.0223 | 0.0278
Epoch 122/300, seasonal_3 Loss: 0.0260 | 0.0323
Epoch 123/300, seasonal_3 Loss: 0.0242 | 0.0309
Epoch 124/300, seasonal_3 Loss: 0.0230 | 0.0390
Epoch 125/300, seasonal_3 Loss: 0.0240 | 0.0327
Epoch 126/300, seasonal_3 Loss: 0.0230 | 0.0309
Epoch 127/300, seasonal_3 Loss: 0.0235 | 0.0343
Epoch 128/300, seasonal_3 Loss: 0.0240 | 0.0401
Epoch 129/300, seasonal_3 Loss: 0.0239 | 0.0349
Epoch 130/300, seasonal_3 Loss: 0.0235 | 0.0413
Epoch 131/300, seasonal_3 Loss: 0.0254 | 0.0451
Epoch 132/300, seasonal_3 Loss: 0.0250 | 0.0408
Epoch 133/300, seasonal_3 Loss: 0.0258 | 0.0370
Epoch 134/300, seasonal_3 Loss: 0.0257 | 0.0257
Epoch 135/300, seasonal_3 Loss: 0.0267 | 0.0354
Epoch 136/300, seasonal_3 Loss: 0.0251 | 0.0285
Epoch 137/300, seasonal_3 Loss: 0.0251 | 0.0380
Epoch 138/300, seasonal_3 Loss: 0.0236 | 0.0495
Epoch 139/300, seasonal_3 Loss: 0.0237 | 0.0618
Epoch 140/300, seasonal_3 Loss: 0.0229 | 0.0562
Epoch 141/300, seasonal_3 Loss: 0.0233 | 0.0560
Epoch 142/300, seasonal_3 Loss: 0.0219 | 0.0649
Epoch 143/300, seasonal_3 Loss: 0.0218 | 0.0564
Epoch 144/300, seasonal_3 Loss: 0.0213 | 0.0360
Epoch 145/300, seasonal_3 Loss: 0.0194 | 0.0335
Epoch 146/300, seasonal_3 Loss: 0.0186 | 0.0285
Epoch 147/300, seasonal_3 Loss: 0.0208 | 0.0297
Epoch 148/300, seasonal_3 Loss: 0.0211 | 0.0299
Epoch 149/300, seasonal_3 Loss: 0.0209 | 0.0330
Epoch 150/300, seasonal_3 Loss: 0.0212 | 0.0337
Epoch 151/300, seasonal_3 Loss: 0.0188 | 0.0352
Epoch 152/300, seasonal_3 Loss: 0.0179 | 0.0342
Epoch 153/300, seasonal_3 Loss: 0.0169 | 0.0331
Epoch 154/300, seasonal_3 Loss: 0.0162 | 0.0319
Epoch 155/300, seasonal_3 Loss: 0.0164 | 0.0357
Epoch 156/300, seasonal_3 Loss: 0.0162 | 0.0360
Epoch 157/300, seasonal_3 Loss: 0.0163 | 0.0404
Epoch 158/300, seasonal_3 Loss: 0.0173 | 0.0456
Epoch 159/300, seasonal_3 Loss: 0.0185 | 0.0588
Epoch 160/300, seasonal_3 Loss: 0.0188 | 0.0589
Epoch 161/300, seasonal_3 Loss: 0.0195 | 0.0397
Epoch 162/300, seasonal_3 Loss: 0.0179 | 0.0428
Epoch 163/300, seasonal_3 Loss: 0.0158 | 0.0470
Epoch 164/300, seasonal_3 Loss: 0.0148 | 0.0500
Epoch 165/300, seasonal_3 Loss: 0.0149 | 0.0513
Epoch 166/300, seasonal_3 Loss: 0.0146 | 0.0364
Epoch 167/300, seasonal_3 Loss: 0.0146 | 0.0277
Epoch 168/300, seasonal_3 Loss: 0.0151 | 0.0263
Epoch 169/300, seasonal_3 Loss: 0.0156 | 0.0259
Epoch 170/300, seasonal_3 Loss: 0.0161 | 0.0271
Epoch 171/300, seasonal_3 Loss: 0.0164 | 0.0286
Epoch 172/300, seasonal_3 Loss: 0.0163 | 0.0338
Epoch 173/300, seasonal_3 Loss: 0.0164 | 0.0412
Epoch 174/300, seasonal_3 Loss: 0.0151 | 0.0469
Epoch 175/300, seasonal_3 Loss: 0.0143 | 0.0513
Epoch 176/300, seasonal_3 Loss: 0.0141 | 0.0407
Epoch 177/300, seasonal_3 Loss: 0.0144 | 0.0287
Epoch 178/300, seasonal_3 Loss: 0.0135 | 0.0285
Epoch 179/300, seasonal_3 Loss: 0.0136 | 0.0305
Epoch 180/300, seasonal_3 Loss: 0.0142 | 0.0394
Epoch 181/300, seasonal_3 Loss: 0.0145 | 0.0502
Epoch 182/300, seasonal_3 Loss: 0.0157 | 0.0509
Epoch 183/300, seasonal_3 Loss: 0.0191 | 0.0423
Epoch 184/300, seasonal_3 Loss: 0.0129 | 0.0373
Epoch 185/300, seasonal_3 Loss: 0.0317 | 0.0359
Epoch 186/300, seasonal_3 Loss: 0.0332 | 0.0380
Epoch 187/300, seasonal_3 Loss: 0.0292 | 0.0378
Epoch 188/300, seasonal_3 Loss: 0.0289 | 0.0401
Epoch 189/300, seasonal_3 Loss: 0.0231 | 0.0295
Epoch 190/300, seasonal_3 Loss: 0.0328 | 0.0269
Epoch 191/300, seasonal_3 Loss: 0.0304 | 0.0312
Epoch 192/300, seasonal_3 Loss: 0.0318 | 0.0303
Epoch 193/300, seasonal_3 Loss: 0.0288 | 0.0327
Epoch 194/300, seasonal_3 Loss: 0.0286 | 0.0315
Epoch 195/300, seasonal_3 Loss: 0.0335 | 0.0289
Epoch 196/300, seasonal_3 Loss: 0.0324 | 0.0277
Epoch 197/300, seasonal_3 Loss: 0.0311 | 0.0282
Epoch 198/300, seasonal_3 Loss: 0.0297 | 0.0338
Epoch 199/300, seasonal_3 Loss: 0.0242 | 0.0396
Epoch 200/300, seasonal_3 Loss: 0.0156 | 0.0286
Epoch 201/300, seasonal_3 Loss: 0.0246 | 0.0308
Epoch 202/300, seasonal_3 Loss: 0.0138 | 0.0335
Epoch 203/300, seasonal_3 Loss: 0.1646 | 0.0298
Epoch 204/300, seasonal_3 Loss: 0.0186 | 0.0303
Epoch 205/300, seasonal_3 Loss: 0.0326 | 0.0288
Epoch 206/300, seasonal_3 Loss: 0.0286 | 0.0344
Epoch 207/300, seasonal_3 Loss: 0.0129 | 0.0345
Epoch 208/300, seasonal_3 Loss: 0.0235 | 0.0334
Epoch 209/300, seasonal_3 Loss: 0.0801 | 0.0698
Epoch 210/300, seasonal_3 Loss: 0.0499 | 0.0441
Epoch 211/300, seasonal_3 Loss: 0.0408 | 0.0431
Epoch 212/300, seasonal_3 Loss: 0.0201 | 0.0426
Epoch 213/300, seasonal_3 Loss: 0.0224 | 0.0335
Epoch 214/300, seasonal_3 Loss: 0.0358 | 0.0303
Epoch 215/300, seasonal_3 Loss: 0.0165 | 0.0343
Epoch 216/300, seasonal_3 Loss: 0.0145 | 0.0312
Epoch 217/300, seasonal_3 Loss: 0.0135 | 0.0312
Epoch 218/300, seasonal_3 Loss: 0.0132 | 0.0318
Epoch 219/300, seasonal_3 Loss: 0.0125 | 0.0320
Epoch 220/300, seasonal_3 Loss: 0.0128 | 0.0322
Epoch 221/300, seasonal_3 Loss: 0.0128 | 0.0320
Epoch 222/300, seasonal_3 Loss: 0.0115 | 0.0327
Epoch 223/300, seasonal_3 Loss: 0.0112 | 0.0329
Epoch 224/300, seasonal_3 Loss: 0.0110 | 0.0329
Epoch 225/300, seasonal_3 Loss: 0.0108 | 0.0330
Epoch 226/300, seasonal_3 Loss: 0.0106 | 0.0329
Epoch 227/300, seasonal_3 Loss: 0.0105 | 0.0328
Epoch 228/300, seasonal_3 Loss: 0.0103 | 0.0326
Epoch 229/300, seasonal_3 Loss: 0.0102 | 0.0328
Epoch 230/300, seasonal_3 Loss: 0.0100 | 0.0326
Epoch 231/300, seasonal_3 Loss: 0.0099 | 0.0320
Epoch 232/300, seasonal_3 Loss: 0.0098 | 0.0319
Epoch 233/300, seasonal_3 Loss: 0.0097 | 0.0320
Epoch 234/300, seasonal_3 Loss: 0.0099 | 0.0350
Epoch 235/300, seasonal_3 Loss: 0.0099 | 0.0336
Epoch 236/300, seasonal_3 Loss: 0.0097 | 0.0334
Epoch 237/300, seasonal_3 Loss: 0.0095 | 0.0333
Epoch 238/300, seasonal_3 Loss: 0.0093 | 0.0335
Epoch 239/300, seasonal_3 Loss: 0.0093 | 0.0325
Epoch 240/300, seasonal_3 Loss: 0.0090 | 0.0335
Epoch 241/300, seasonal_3 Loss: 0.0088 | 0.0316
Epoch 242/300, seasonal_3 Loss: 0.0087 | 0.0320
Epoch 243/300, seasonal_3 Loss: 0.0087 | 0.0312
Epoch 244/300, seasonal_3 Loss: 0.0085 | 0.0308
Epoch 245/300, seasonal_3 Loss: 0.0086 | 0.0314
Epoch 246/300, seasonal_3 Loss: 0.0083 | 0.0298
Epoch 247/300, seasonal_3 Loss: 0.0085 | 0.0279
Epoch 248/300, seasonal_3 Loss: 0.0089 | 0.0282
Epoch 249/300, seasonal_3 Loss: 0.0088 | 0.0292
Epoch 250/300, seasonal_3 Loss: 0.0088 | 0.0290
Epoch 251/300, seasonal_3 Loss: 0.0084 | 0.0270
Epoch 252/300, seasonal_3 Loss: 0.0084 | 0.0255
Epoch 253/300, seasonal_3 Loss: 0.0084 | 0.0249
Epoch 254/300, seasonal_3 Loss: 0.0087 | 0.0273
Epoch 255/300, seasonal_3 Loss: 0.0095 | 0.0376
Epoch 256/300, seasonal_3 Loss: 0.0098 | 0.0377
Epoch 257/300, seasonal_3 Loss: 0.0099 | 0.0313
Epoch 258/300, seasonal_3 Loss: 0.0093 | 0.0312
Epoch 259/300, seasonal_3 Loss: 0.0089 | 0.0308
Epoch 260/300, seasonal_3 Loss: 0.0085 | 0.0284
Epoch 261/300, seasonal_3 Loss: 0.0085 | 0.0340
Epoch 262/300, seasonal_3 Loss: 0.0094 | 0.0392
Epoch 263/300, seasonal_3 Loss: 0.0100 | 0.0491
Epoch 264/300, seasonal_3 Loss: 0.0105 | 0.0396
Epoch 265/300, seasonal_3 Loss: 0.0100 | 0.0262
Epoch 266/300, seasonal_3 Loss: 0.0087 | 0.0245
Epoch 267/300, seasonal_3 Loss: 0.0082 | 0.0249
Epoch 268/300, seasonal_3 Loss: 0.0079 | 0.0259
Epoch 269/300, seasonal_3 Loss: 0.0077 | 0.0263
Epoch 270/300, seasonal_3 Loss: 0.0078 | 0.0267
Epoch 271/300, seasonal_3 Loss: 0.0077 | 0.0320
Epoch 272/300, seasonal_3 Loss: 0.0079 | 0.0493
Epoch 273/300, seasonal_3 Loss: 0.0081 | 0.0821
Epoch 274/300, seasonal_3 Loss: 0.0084 | 0.0663
Epoch 275/300, seasonal_3 Loss: 0.0085 | 0.0374
Epoch 276/300, seasonal_3 Loss: 0.0082 | 0.0362
Epoch 277/300, seasonal_3 Loss: 0.0080 | 0.0316
Epoch 278/300, seasonal_3 Loss: 0.0074 | 0.0384
Epoch 279/300, seasonal_3 Loss: 0.0069 | 0.0485
Epoch 280/300, seasonal_3 Loss: 0.0068 | 0.0564
Epoch 281/300, seasonal_3 Loss: 0.0070 | 0.0443
Epoch 282/300, seasonal_3 Loss: 0.0071 | 0.0279
Epoch 283/300, seasonal_3 Loss: 0.0072 | 0.0306
Epoch 284/300, seasonal_3 Loss: 0.0084 | 0.0305
Epoch 285/300, seasonal_3 Loss: 0.0067 | 0.0365
Epoch 286/300, seasonal_3 Loss: 0.0065 | 0.0646
Epoch 287/300, seasonal_3 Loss: 0.0066 | 0.0811
Epoch 288/300, seasonal_3 Loss: 0.0065 | 0.0521
Epoch 289/300, seasonal_3 Loss: 0.0062 | 0.0307
Epoch 290/300, seasonal_3 Loss: 0.0066 | 0.0355
Epoch 291/300, seasonal_3 Loss: 0.0066 | 0.0302
Epoch 292/300, seasonal_3 Loss: 0.0063 | 0.0462
Epoch 293/300, seasonal_3 Loss: 0.0063 | 0.0517
Epoch 294/300, seasonal_3 Loss: 0.0065 | 0.0425
Epoch 295/300, seasonal_3 Loss: 0.0063 | 0.0348
Epoch 296/300, seasonal_3 Loss: 0.0059 | 0.0305
Epoch 297/300, seasonal_3 Loss: 0.0057 | 0.0333
Epoch 298/300, seasonal_3 Loss: 0.0056 | 0.0455
Epoch 299/300, seasonal_3 Loss: 0.0057 | 0.0555
Epoch 300/300, seasonal_3 Loss: 0.0062 | 0.0420
Training resid component with params: {'observation_period_num': 23, 'train_rates': 0.9069942590842773, 'learning_rate': 0.00016907613326897983, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8086588913397253}
Epoch 1/300, resid Loss: 0.9870 | 0.3090
Epoch 2/300, resid Loss: 0.2655 | 0.1857
Epoch 3/300, resid Loss: 0.2364 | 0.1201
Epoch 4/300, resid Loss: 0.4000 | 0.3120
Epoch 5/300, resid Loss: 0.4596 | 0.2500
Epoch 6/300, resid Loss: 0.4573 | 0.5001
Epoch 7/300, resid Loss: 0.5053 | 0.2117
Epoch 8/300, resid Loss: 0.3420 | 0.1421
Epoch 9/300, resid Loss: 0.2446 | 0.1269
Epoch 10/300, resid Loss: 0.2873 | 0.4729
Epoch 11/300, resid Loss: 0.3087 | 0.1327
Epoch 12/300, resid Loss: 0.3009 | 0.1799
Epoch 13/300, resid Loss: 0.2718 | 0.1103
Epoch 14/300, resid Loss: 0.1689 | 0.2317
Epoch 15/300, resid Loss: 0.1702 | 0.1189
Epoch 16/300, resid Loss: 0.1493 | 0.0899
Epoch 17/300, resid Loss: 0.1697 | 0.1048
Epoch 18/300, resid Loss: 0.1661 | 0.1141
Epoch 19/300, resid Loss: 0.1394 | 0.1104
Epoch 20/300, resid Loss: 0.1716 | 0.1076
Epoch 21/300, resid Loss: 0.1742 | 0.1214
Epoch 22/300, resid Loss: 0.1700 | 0.1106
Epoch 23/300, resid Loss: 0.1214 | 0.1042
Epoch 24/300, resid Loss: 0.1217 | 0.0863
Epoch 25/300, resid Loss: 0.1164 | 0.0738
Epoch 26/300, resid Loss: 0.1080 | 0.0808
Epoch 27/300, resid Loss: 0.1058 | 0.0701
Epoch 28/300, resid Loss: 0.1102 | 0.0926
Epoch 29/300, resid Loss: 0.1131 | 0.0749
Epoch 30/300, resid Loss: 0.1087 | 0.0836
Epoch 31/300, resid Loss: 0.1106 | 0.0951
Epoch 32/300, resid Loss: 0.1118 | 0.0805
Epoch 33/300, resid Loss: 0.1090 | 0.0813
Epoch 34/300, resid Loss: 0.0985 | 0.0732
Epoch 35/300, resid Loss: 0.0936 | 0.0691
Epoch 36/300, resid Loss: 0.0942 | 0.0834
Epoch 37/300, resid Loss: 0.0945 | 0.0687
Epoch 38/300, resid Loss: 0.0885 | 0.0695
Epoch 39/300, resid Loss: 0.0886 | 0.0681
Epoch 40/300, resid Loss: 0.0868 | 0.0767
Epoch 41/300, resid Loss: 0.0916 | 0.0684
Epoch 42/300, resid Loss: 0.0875 | 0.0735
Epoch 43/300, resid Loss: 0.0872 | 0.0739
Epoch 44/300, resid Loss: 0.0828 | 0.0663
Epoch 45/300, resid Loss: 0.0845 | 0.0687
Epoch 46/300, resid Loss: 0.0820 | 0.0718
Epoch 47/300, resid Loss: 0.0823 | 0.0687
Epoch 48/300, resid Loss: 0.0795 | 0.0683
Epoch 49/300, resid Loss: 0.0788 | 0.0680
Epoch 50/300, resid Loss: 0.0774 | 0.0653
Epoch 51/300, resid Loss: 0.0779 | 0.0656
Epoch 52/300, resid Loss: 0.0762 | 0.0659
Epoch 53/300, resid Loss: 0.0775 | 0.0636
Epoch 54/300, resid Loss: 0.0751 | 0.0664
Epoch 55/300, resid Loss: 0.0770 | 0.0637
Epoch 56/300, resid Loss: 0.0734 | 0.0616
Epoch 57/300, resid Loss: 0.0731 | 0.0626
Epoch 58/300, resid Loss: 0.0722 | 0.0617
Epoch 59/300, resid Loss: 0.0721 | 0.0605
Epoch 60/300, resid Loss: 0.0714 | 0.0611
Epoch 61/300, resid Loss: 0.0710 | 0.0605
Epoch 62/300, resid Loss: 0.0703 | 0.0588
Epoch 63/300, resid Loss: 0.0701 | 0.0595
Epoch 64/300, resid Loss: 0.0697 | 0.0585
Epoch 65/300, resid Loss: 0.0695 | 0.0582
Epoch 66/300, resid Loss: 0.0690 | 0.0580
Epoch 67/300, resid Loss: 0.0689 | 0.0579
Epoch 68/300, resid Loss: 0.0684 | 0.0566
Epoch 69/300, resid Loss: 0.0683 | 0.0570
Epoch 70/300, resid Loss: 0.0680 | 0.0561
Epoch 71/300, resid Loss: 0.0678 | 0.0561
Epoch 72/300, resid Loss: 0.0675 | 0.0556
Epoch 73/300, resid Loss: 0.0673 | 0.0555
Epoch 74/300, resid Loss: 0.0672 | 0.0548
Epoch 75/300, resid Loss: 0.0669 | 0.0547
Epoch 76/300, resid Loss: 0.0668 | 0.0544
Epoch 77/300, resid Loss: 0.0666 | 0.0542
Epoch 78/300, resid Loss: 0.0665 | 0.0538
Epoch 79/300, resid Loss: 0.0662 | 0.0536
Epoch 80/300, resid Loss: 0.0662 | 0.0533
Epoch 81/300, resid Loss: 0.0659 | 0.0531
Epoch 82/300, resid Loss: 0.0658 | 0.0528
Epoch 83/300, resid Loss: 0.0657 | 0.0526
Epoch 84/300, resid Loss: 0.0656 | 0.0524
Epoch 85/300, resid Loss: 0.0654 | 0.0522
Epoch 86/300, resid Loss: 0.0653 | 0.0520
Epoch 87/300, resid Loss: 0.0652 | 0.0518
Epoch 88/300, resid Loss: 0.0651 | 0.0516
Epoch 89/300, resid Loss: 0.0650 | 0.0514
Epoch 90/300, resid Loss: 0.0648 | 0.0512
Epoch 91/300, resid Loss: 0.0647 | 0.0511
Epoch 92/300, resid Loss: 0.0646 | 0.0509
Epoch 93/300, resid Loss: 0.0645 | 0.0508
Epoch 94/300, resid Loss: 0.0645 | 0.0506
Epoch 95/300, resid Loss: 0.0644 | 0.0505
Epoch 96/300, resid Loss: 0.0643 | 0.0503
Epoch 97/300, resid Loss: 0.0642 | 0.0502
Epoch 98/300, resid Loss: 0.0641 | 0.0501
Epoch 99/300, resid Loss: 0.0640 | 0.0500
Epoch 100/300, resid Loss: 0.0640 | 0.0498
Epoch 101/300, resid Loss: 0.0639 | 0.0497
Epoch 102/300, resid Loss: 0.0638 | 0.0496
Epoch 103/300, resid Loss: 0.0637 | 0.0495
Epoch 104/300, resid Loss: 0.0637 | 0.0494
Epoch 105/300, resid Loss: 0.0636 | 0.0493
Epoch 106/300, resid Loss: 0.0636 | 0.0492
Epoch 107/300, resid Loss: 0.0635 | 0.0491
Epoch 108/300, resid Loss: 0.0634 | 0.0490
Epoch 109/300, resid Loss: 0.0634 | 0.0489
Epoch 110/300, resid Loss: 0.0633 | 0.0488
Epoch 111/300, resid Loss: 0.0633 | 0.0488
Epoch 112/300, resid Loss: 0.0632 | 0.0487
Epoch 113/300, resid Loss: 0.0632 | 0.0486
Epoch 114/300, resid Loss: 0.0631 | 0.0485
Epoch 115/300, resid Loss: 0.0630 | 0.0484
Epoch 116/300, resid Loss: 0.0630 | 0.0484
Epoch 117/300, resid Loss: 0.0630 | 0.0483
Epoch 118/300, resid Loss: 0.0629 | 0.0482
Epoch 119/300, resid Loss: 0.0629 | 0.0482
Epoch 120/300, resid Loss: 0.0628 | 0.0481
Epoch 121/300, resid Loss: 0.0628 | 0.0480
Epoch 122/300, resid Loss: 0.0627 | 0.0480
Epoch 123/300, resid Loss: 0.0627 | 0.0479
Epoch 124/300, resid Loss: 0.0627 | 0.0479
Epoch 125/300, resid Loss: 0.0626 | 0.0478
Epoch 126/300, resid Loss: 0.0626 | 0.0477
Epoch 127/300, resid Loss: 0.0626 | 0.0477
Epoch 128/300, resid Loss: 0.0625 | 0.0476
Epoch 129/300, resid Loss: 0.0625 | 0.0476
Epoch 130/300, resid Loss: 0.0625 | 0.0475
Epoch 131/300, resid Loss: 0.0624 | 0.0475
Epoch 132/300, resid Loss: 0.0624 | 0.0474
Epoch 133/300, resid Loss: 0.0624 | 0.0474
Epoch 134/300, resid Loss: 0.0623 | 0.0473
Epoch 135/300, resid Loss: 0.0623 | 0.0473
Epoch 136/300, resid Loss: 0.0623 | 0.0472
Epoch 137/300, resid Loss: 0.0622 | 0.0472
Epoch 138/300, resid Loss: 0.0622 | 0.0472
Epoch 139/300, resid Loss: 0.0622 | 0.0471
Epoch 140/300, resid Loss: 0.0622 | 0.0471
Epoch 141/300, resid Loss: 0.0621 | 0.0470
Epoch 142/300, resid Loss: 0.0621 | 0.0470
Epoch 143/300, resid Loss: 0.0621 | 0.0470
Epoch 144/300, resid Loss: 0.0621 | 0.0469
Epoch 145/300, resid Loss: 0.0620 | 0.0469
Epoch 146/300, resid Loss: 0.0620 | 0.0469
Epoch 147/300, resid Loss: 0.0620 | 0.0468
Epoch 148/300, resid Loss: 0.0620 | 0.0468
Epoch 149/300, resid Loss: 0.0619 | 0.0467
Epoch 150/300, resid Loss: 0.0619 | 0.0467
Epoch 151/300, resid Loss: 0.0619 | 0.0467
Epoch 152/300, resid Loss: 0.0619 | 0.0467
Epoch 153/300, resid Loss: 0.0619 | 0.0466
Epoch 154/300, resid Loss: 0.0618 | 0.0466
Epoch 155/300, resid Loss: 0.0618 | 0.0466
Epoch 156/300, resid Loss: 0.0618 | 0.0465
Epoch 157/300, resid Loss: 0.0618 | 0.0465
Epoch 158/300, resid Loss: 0.0618 | 0.0465
Epoch 159/300, resid Loss: 0.0617 | 0.0465
Epoch 160/300, resid Loss: 0.0617 | 0.0464
Epoch 161/300, resid Loss: 0.0617 | 0.0464
Epoch 162/300, resid Loss: 0.0617 | 0.0464
Epoch 163/300, resid Loss: 0.0617 | 0.0464
Epoch 164/300, resid Loss: 0.0617 | 0.0463
Epoch 165/300, resid Loss: 0.0616 | 0.0463
Epoch 166/300, resid Loss: 0.0616 | 0.0463
Epoch 167/300, resid Loss: 0.0616 | 0.0463
Epoch 168/300, resid Loss: 0.0616 | 0.0462
Epoch 169/300, resid Loss: 0.0616 | 0.0462
Epoch 170/300, resid Loss: 0.0616 | 0.0462
Epoch 171/300, resid Loss: 0.0616 | 0.0462
Epoch 172/300, resid Loss: 0.0616 | 0.0462
Epoch 173/300, resid Loss: 0.0615 | 0.0462
Epoch 174/300, resid Loss: 0.0615 | 0.0461
Epoch 175/300, resid Loss: 0.0615 | 0.0461
Epoch 176/300, resid Loss: 0.0615 | 0.0461
Epoch 177/300, resid Loss: 0.0615 | 0.0461
Epoch 178/300, resid Loss: 0.0615 | 0.0461
Epoch 179/300, resid Loss: 0.0615 | 0.0461
Epoch 180/300, resid Loss: 0.0615 | 0.0460
Epoch 181/300, resid Loss: 0.0615 | 0.0460
Epoch 182/300, resid Loss: 0.0614 | 0.0460
Epoch 183/300, resid Loss: 0.0614 | 0.0460
Epoch 184/300, resid Loss: 0.0614 | 0.0460
Epoch 185/300, resid Loss: 0.0614 | 0.0460
Epoch 186/300, resid Loss: 0.0614 | 0.0460
Epoch 187/300, resid Loss: 0.0614 | 0.0459
Epoch 188/300, resid Loss: 0.0614 | 0.0459
Epoch 189/300, resid Loss: 0.0614 | 0.0459
Epoch 190/300, resid Loss: 0.0614 | 0.0459
Epoch 191/300, resid Loss: 0.0614 | 0.0459
Epoch 192/300, resid Loss: 0.0614 | 0.0459
Epoch 193/300, resid Loss: 0.0614 | 0.0459
Epoch 194/300, resid Loss: 0.0613 | 0.0459
Epoch 195/300, resid Loss: 0.0613 | 0.0459
Epoch 196/300, resid Loss: 0.0613 | 0.0458
Epoch 197/300, resid Loss: 0.0613 | 0.0458
Epoch 198/300, resid Loss: 0.0613 | 0.0458
Epoch 199/300, resid Loss: 0.0613 | 0.0458
Epoch 200/300, resid Loss: 0.0613 | 0.0458
Epoch 201/300, resid Loss: 0.0613 | 0.0458
Epoch 202/300, resid Loss: 0.0613 | 0.0458
Epoch 203/300, resid Loss: 0.0613 | 0.0458
Epoch 204/300, resid Loss: 0.0613 | 0.0458
Epoch 205/300, resid Loss: 0.0613 | 0.0458
Epoch 206/300, resid Loss: 0.0613 | 0.0458
Epoch 207/300, resid Loss: 0.0613 | 0.0458
Epoch 208/300, resid Loss: 0.0613 | 0.0458
Epoch 209/300, resid Loss: 0.0613 | 0.0457
Epoch 210/300, resid Loss: 0.0613 | 0.0457
Epoch 211/300, resid Loss: 0.0613 | 0.0457
Epoch 212/300, resid Loss: 0.0613 | 0.0457
Epoch 213/300, resid Loss: 0.0613 | 0.0457
Epoch 214/300, resid Loss: 0.0612 | 0.0457
Epoch 215/300, resid Loss: 0.0612 | 0.0457
Epoch 216/300, resid Loss: 0.0612 | 0.0457
Epoch 217/300, resid Loss: 0.0612 | 0.0457
Epoch 218/300, resid Loss: 0.0612 | 0.0457
Epoch 219/300, resid Loss: 0.0612 | 0.0457
Epoch 220/300, resid Loss: 0.0612 | 0.0457
Epoch 221/300, resid Loss: 0.0612 | 0.0457
Epoch 222/300, resid Loss: 0.0612 | 0.0457
Epoch 223/300, resid Loss: 0.0612 | 0.0457
Epoch 224/300, resid Loss: 0.0612 | 0.0457
Epoch 225/300, resid Loss: 0.0612 | 0.0457
Epoch 226/300, resid Loss: 0.0612 | 0.0457
Epoch 227/300, resid Loss: 0.0612 | 0.0457
Epoch 228/300, resid Loss: 0.0612 | 0.0457
Epoch 229/300, resid Loss: 0.0612 | 0.0457
Epoch 230/300, resid Loss: 0.0612 | 0.0456
Epoch 231/300, resid Loss: 0.0612 | 0.0456
Epoch 232/300, resid Loss: 0.0612 | 0.0456
Epoch 233/300, resid Loss: 0.0612 | 0.0456
Epoch 234/300, resid Loss: 0.0612 | 0.0456
Epoch 235/300, resid Loss: 0.0612 | 0.0456
Epoch 236/300, resid Loss: 0.0612 | 0.0456
Epoch 237/300, resid Loss: 0.0612 | 0.0456
Epoch 238/300, resid Loss: 0.0612 | 0.0456
Epoch 239/300, resid Loss: 0.0612 | 0.0456
Epoch 240/300, resid Loss: 0.0612 | 0.0456
Epoch 241/300, resid Loss: 0.0612 | 0.0456
Epoch 242/300, resid Loss: 0.0612 | 0.0456
Epoch 243/300, resid Loss: 0.0612 | 0.0456
Epoch 244/300, resid Loss: 0.0612 | 0.0456
Epoch 245/300, resid Loss: 0.0612 | 0.0456
Epoch 246/300, resid Loss: 0.0612 | 0.0456
Epoch 247/300, resid Loss: 0.0612 | 0.0456
Epoch 248/300, resid Loss: 0.0612 | 0.0456
Epoch 249/300, resid Loss: 0.0612 | 0.0456
Epoch 250/300, resid Loss: 0.0612 | 0.0456
Epoch 251/300, resid Loss: 0.0612 | 0.0456
Epoch 252/300, resid Loss: 0.0612 | 0.0456
Epoch 253/300, resid Loss: 0.0612 | 0.0456
Epoch 254/300, resid Loss: 0.0612 | 0.0456
Epoch 255/300, resid Loss: 0.0612 | 0.0456
Epoch 256/300, resid Loss: 0.0612 | 0.0456
Epoch 257/300, resid Loss: 0.0612 | 0.0456
Epoch 258/300, resid Loss: 0.0612 | 0.0456
Epoch 259/300, resid Loss: 0.0612 | 0.0456
Epoch 260/300, resid Loss: 0.0612 | 0.0456
Epoch 261/300, resid Loss: 0.0612 | 0.0456
Epoch 262/300, resid Loss: 0.0612 | 0.0456
Epoch 263/300, resid Loss: 0.0612 | 0.0456
Epoch 264/300, resid Loss: 0.0612 | 0.0456
Epoch 265/300, resid Loss: 0.0612 | 0.0456
Epoch 266/300, resid Loss: 0.0612 | 0.0456
Epoch 267/300, resid Loss: 0.0612 | 0.0456
Epoch 268/300, resid Loss: 0.0612 | 0.0456
Epoch 269/300, resid Loss: 0.0612 | 0.0456
Epoch 270/300, resid Loss: 0.0612 | 0.0456
Epoch 271/300, resid Loss: 0.0612 | 0.0456
Epoch 272/300, resid Loss: 0.0612 | 0.0456
Epoch 273/300, resid Loss: 0.0612 | 0.0456
Epoch 274/300, resid Loss: 0.0611 | 0.0456
Epoch 275/300, resid Loss: 0.0611 | 0.0456
Epoch 276/300, resid Loss: 0.0611 | 0.0456
Epoch 277/300, resid Loss: 0.0611 | 0.0456
Epoch 278/300, resid Loss: 0.0611 | 0.0456
Epoch 279/300, resid Loss: 0.0611 | 0.0456
Epoch 280/300, resid Loss: 0.0611 | 0.0456
Epoch 281/300, resid Loss: 0.0611 | 0.0456
Epoch 282/300, resid Loss: 0.0611 | 0.0456
Epoch 283/300, resid Loss: 0.0611 | 0.0456
Epoch 284/300, resid Loss: 0.0611 | 0.0456
Epoch 285/300, resid Loss: 0.0611 | 0.0456
Epoch 286/300, resid Loss: 0.0611 | 0.0456
Epoch 287/300, resid Loss: 0.0611 | 0.0456
Epoch 288/300, resid Loss: 0.0611 | 0.0456
Epoch 289/300, resid Loss: 0.0611 | 0.0456
Epoch 290/300, resid Loss: 0.0611 | 0.0456
Epoch 291/300, resid Loss: 0.0611 | 0.0456
Epoch 292/300, resid Loss: 0.0611 | 0.0456
Epoch 293/300, resid Loss: 0.0611 | 0.0456
Epoch 294/300, resid Loss: 0.0611 | 0.0456
Epoch 295/300, resid Loss: 0.0611 | 0.0456
Epoch 296/300, resid Loss: 0.0611 | 0.0456
Epoch 297/300, resid Loss: 0.0611 | 0.0456
Epoch 298/300, resid Loss: 0.0611 | 0.0456
Epoch 299/300, resid Loss: 0.0611 | 0.0456
Epoch 300/300, resid Loss: 0.0611 | 0.0456
Runtime (seconds): 10189.70796084404
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:696: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[160.414608]
[-5.66546019]
[2.51504332]
[15.22761664]
[2.15586275]
[18.26366942]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 224.36023877285794
RMSE: 14.978659445119177
MAE: 14.978659445119177
R-squared: nan
[192.91133994]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:738: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1]
