[32m[I 2025-02-09 08:15:28,476][0m A new study created in memory with name: no-name-60506d19-7a50-4ec3-825b-d60bee7bca24[0m
[32m[I 2025-02-09 08:15:50,829][0m Trial 0 finished with value: 0.22104004989534418 and parameters: {'observation_period_num': 92, 'train_rates': 0.743999767596334, 'learning_rate': 0.00031278239665562496, 'batch_size': 249, 'step_size': 10, 'gamma': 0.7790417575730355}. Best is trial 0 with value: 0.22104004989534418.[0m
[32m[I 2025-02-09 08:22:08,017][0m Trial 1 finished with value: 0.05187897785351826 and parameters: {'observation_period_num': 14, 'train_rates': 0.9654168632817068, 'learning_rate': 0.0009906314741715931, 'batch_size': 16, 'step_size': 11, 'gamma': 0.784963663841349}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:22:28,894][0m Trial 2 finished with value: 0.7433520992475053 and parameters: {'observation_period_num': 247, 'train_rates': 0.6450726568247268, 'learning_rate': 1.3676915181857044e-05, 'batch_size': 242, 'step_size': 9, 'gamma': 0.8463932297126621}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:23:07,274][0m Trial 3 finished with value: 0.801862004925223 and parameters: {'observation_period_num': 216, 'train_rates': 0.707098941018498, 'learning_rate': 3.6960585513077713e-06, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8867483089324313}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:25:02,244][0m Trial 4 finished with value: 0.18014137629005644 and parameters: {'observation_period_num': 250, 'train_rates': 0.8039423210957389, 'learning_rate': 1.8126029948357104e-05, 'batch_size': 43, 'step_size': 12, 'gamma': 0.8812290050655509}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:26:45,320][0m Trial 5 finished with value: 0.2086126076998097 and parameters: {'observation_period_num': 242, 'train_rates': 0.9269877976142156, 'learning_rate': 7.06575769931145e-05, 'batch_size': 54, 'step_size': 8, 'gamma': 0.7554994420612405}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:27:30,486][0m Trial 6 finished with value: 0.9317009035618075 and parameters: {'observation_period_num': 198, 'train_rates': 0.7347053767302828, 'learning_rate': 1.8335889946393212e-06, 'batch_size': 110, 'step_size': 5, 'gamma': 0.8010307261283033}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:27:55,609][0m Trial 7 finished with value: 1.487011799212055 and parameters: {'observation_period_num': 154, 'train_rates': 0.7516533028383218, 'learning_rate': 2.0438962144555733e-06, 'batch_size': 233, 'step_size': 5, 'gamma': 0.7704780365438861}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:28:32,384][0m Trial 8 finished with value: 0.3534081280231476 and parameters: {'observation_period_num': 101, 'train_rates': 0.9750397376743882, 'learning_rate': 5.363312484898468e-05, 'batch_size': 178, 'step_size': 15, 'gamma': 0.8630940763862001}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:30:16,984][0m Trial 9 finished with value: 0.1752199193081224 and parameters: {'observation_period_num': 246, 'train_rates': 0.9099268984276774, 'learning_rate': 0.0002177477405553909, 'batch_size': 52, 'step_size': 2, 'gamma': 0.9063542483318282}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:34:49,048][0m Trial 10 finished with value: 0.09056368367532598 and parameters: {'observation_period_num': 7, 'train_rates': 0.8402373790305669, 'learning_rate': 0.0007416519486317138, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9807302996737792}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:37:53,194][0m Trial 11 finished with value: 0.06649807613067979 and parameters: {'observation_period_num': 22, 'train_rates': 0.8500532100962, 'learning_rate': 0.0007166052997292385, 'batch_size': 30, 'step_size': 15, 'gamma': 0.978418472679849}. Best is trial 1 with value: 0.05187897785351826.[0m
[32m[I 2025-02-09 08:39:00,072][0m Trial 12 finished with value: 0.04430557955672571 and parameters: {'observation_period_num': 7, 'train_rates': 0.8687091295692244, 'learning_rate': 0.0009308854567667701, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9845781021151098}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:40:10,943][0m Trial 13 finished with value: 0.08159210532903671 and parameters: {'observation_period_num': 48, 'train_rates': 0.9778662964084958, 'learning_rate': 0.00018067882374103104, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9286373928992468}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:40:46,295][0m Trial 14 finished with value: 0.07113416026974345 and parameters: {'observation_period_num': 55, 'train_rates': 0.895824568338192, 'learning_rate': 0.0007991218467174815, 'batch_size': 175, 'step_size': 7, 'gamma': 0.8403603886827805}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:41:57,932][0m Trial 15 finished with value: 0.0568399890587571 and parameters: {'observation_period_num': 49, 'train_rates': 0.8795869314800353, 'learning_rate': 0.0001319970726240587, 'batch_size': 80, 'step_size': 13, 'gamma': 0.8150919871068124}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:42:38,249][0m Trial 16 finished with value: 0.16693732142448425 and parameters: {'observation_period_num': 85, 'train_rates': 0.9495370825174345, 'learning_rate': 0.00035814686851001764, 'batch_size': 159, 'step_size': 11, 'gamma': 0.9317358748555319}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:43:44,391][0m Trial 17 finished with value: 0.27684011502949174 and parameters: {'observation_period_num': 164, 'train_rates': 0.8242730178281654, 'learning_rate': 7.620169901419114e-06, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9525233221624967}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:44:25,090][0m Trial 18 finished with value: 0.06503429509423397 and parameters: {'observation_period_num': 22, 'train_rates': 0.8642858610871876, 'learning_rate': 9.086758317048568e-05, 'batch_size': 143, 'step_size': 6, 'gamma': 0.8206980792922774}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:45:25,344][0m Trial 19 finished with value: 0.21839176729077198 and parameters: {'observation_period_num': 120, 'train_rates': 0.936741392129256, 'learning_rate': 3.393389280253164e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.7933553592806546}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:46:35,357][0m Trial 20 finished with value: 0.1688953841165189 and parameters: {'observation_period_num': 67, 'train_rates': 0.6512154205628986, 'learning_rate': 0.0009292275013257048, 'batch_size': 66, 'step_size': 2, 'gamma': 0.908243426459919}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:47:55,598][0m Trial 21 finished with value: 0.0475376039722317 and parameters: {'observation_period_num': 35, 'train_rates': 0.8810765350873778, 'learning_rate': 0.0001254308390718934, 'batch_size': 71, 'step_size': 13, 'gamma': 0.8300649798735452}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:51:53,725][0m Trial 22 finished with value: 0.05569462560164104 and parameters: {'observation_period_num': 5, 'train_rates': 0.7879303236910019, 'learning_rate': 0.0004595995212821145, 'batch_size': 22, 'step_size': 12, 'gamma': 0.8362011179596992}. Best is trial 12 with value: 0.04430557955672571.[0m
[32m[I 2025-02-09 08:52:44,830][0m Trial 23 finished with value: 0.04035057118579523 and parameters: {'observation_period_num': 28, 'train_rates': 0.8874380556871803, 'learning_rate': 0.00036149915851579293, 'batch_size': 116, 'step_size': 14, 'gamma': 0.7540515208455304}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:53:33,326][0m Trial 24 finished with value: 0.05010108591143031 and parameters: {'observation_period_num': 36, 'train_rates': 0.8865264273104123, 'learning_rate': 0.00013407273561612539, 'batch_size': 125, 'step_size': 14, 'gamma': 0.7525034619369191}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:54:23,654][0m Trial 25 finished with value: 0.07182911500870569 and parameters: {'observation_period_num': 68, 'train_rates': 0.8010469018236038, 'learning_rate': 0.00036309770250760764, 'batch_size': 109, 'step_size': 14, 'gamma': 0.8584886346719469}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:55:48,351][0m Trial 26 finished with value: 0.132345159093921 and parameters: {'observation_period_num': 72, 'train_rates': 0.9113970816355703, 'learning_rate': 0.00023142738263224274, 'batch_size': 69, 'step_size': 14, 'gamma': 0.811977146590659}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:56:18,065][0m Trial 27 finished with value: 0.11692692326854855 and parameters: {'observation_period_num': 44, 'train_rates': 0.8296524695726286, 'learning_rate': 0.0004686777417539171, 'batch_size': 199, 'step_size': 12, 'gamma': 0.9552053436546518}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:56:51,860][0m Trial 28 finished with value: 0.19911969805753 and parameters: {'observation_period_num': 31, 'train_rates': 0.6025471509864433, 'learning_rate': 4.4911860732650604e-05, 'batch_size': 140, 'step_size': 10, 'gamma': 0.8933154597678822}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:57:43,030][0m Trial 29 finished with value: 0.09959905931731013 and parameters: {'observation_period_num': 117, 'train_rates': 0.8686816510733767, 'learning_rate': 0.00012366970209490877, 'batch_size': 109, 'step_size': 10, 'gamma': 0.7733502783771214}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:58:43,292][0m Trial 30 finished with value: 0.14523498571916313 and parameters: {'observation_period_num': 94, 'train_rates': 0.9116318716869656, 'learning_rate': 0.00048131090909355205, 'batch_size': 95, 'step_size': 13, 'gamma': 0.8282040645488977}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 08:59:30,257][0m Trial 31 finished with value: 0.045129978031261606 and parameters: {'observation_period_num': 33, 'train_rates': 0.8826149043295322, 'learning_rate': 0.00024398474446204256, 'batch_size': 126, 'step_size': 14, 'gamma': 0.7543097472846743}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:00:17,807][0m Trial 32 finished with value: 0.04253455543302916 and parameters: {'observation_period_num': 24, 'train_rates': 0.8744457042855963, 'learning_rate': 0.0002630566827332, 'batch_size': 125, 'step_size': 14, 'gamma': 0.7687256730252608}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:01:02,177][0m Trial 33 finished with value: 0.17019680035006893 and parameters: {'observation_period_num': 20, 'train_rates': 0.7766903148866341, 'learning_rate': 0.00023691160748078106, 'batch_size': 131, 'step_size': 15, 'gamma': 0.7677830021183039}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:01:40,753][0m Trial 34 finished with value: 0.06328326658023378 and parameters: {'observation_period_num': 59, 'train_rates': 0.8588191620718504, 'learning_rate': 0.0005366361894970633, 'batch_size': 153, 'step_size': 14, 'gamma': 0.7877664703546979}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:02:34,849][0m Trial 35 finished with value: 0.041552294666568436 and parameters: {'observation_period_num': 5, 'train_rates': 0.9480788399902764, 'learning_rate': 0.0002856170929573208, 'batch_size': 121, 'step_size': 11, 'gamma': 0.7585247114191925}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:03:14,513][0m Trial 36 finished with value: 0.0446908101439476 and parameters: {'observation_period_num': 5, 'train_rates': 0.9475109184844382, 'learning_rate': 0.0003271190471631719, 'batch_size': 163, 'step_size': 11, 'gamma': 0.7809941599804008}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:04:10,304][0m Trial 37 finished with value: 0.061393242329359055 and parameters: {'observation_period_num': 19, 'train_rates': 0.9891734967460775, 'learning_rate': 2.2750221029408325e-05, 'batch_size': 117, 'step_size': 12, 'gamma': 0.7635948792084181}. Best is trial 23 with value: 0.04035057118579523.[0m
[32m[I 2025-02-09 09:04:42,610][0m Trial 38 finished with value: 0.03565332955784268 and parameters: {'observation_period_num': 17, 'train_rates': 0.9248309449281911, 'learning_rate': 0.0009971542729666821, 'batch_size': 200, 'step_size': 9, 'gamma': 0.8030204586364056}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:05:13,862][0m Trial 39 finished with value: 0.5350472927093506 and parameters: {'observation_period_num': 76, 'train_rates': 0.9613314640871042, 'learning_rate': 8.211275897079308e-06, 'batch_size': 214, 'step_size': 9, 'gamma': 0.7978364666185782}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:05:45,517][0m Trial 40 finished with value: 0.2625138364105581 and parameters: {'observation_period_num': 149, 'train_rates': 0.925274463305201, 'learning_rate': 8.338597060854184e-05, 'batch_size': 190, 'step_size': 8, 'gamma': 0.7507856335024641}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:06:14,858][0m Trial 41 finished with value: 0.03866581432521343 and parameters: {'observation_period_num': 14, 'train_rates': 0.923927766632388, 'learning_rate': 0.0005981125605059161, 'batch_size': 227, 'step_size': 11, 'gamma': 0.7778150513879324}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:06:41,910][0m Trial 42 finished with value: 0.04997709020972252 and parameters: {'observation_period_num': 27, 'train_rates': 0.9278421787272204, 'learning_rate': 0.000608664759368867, 'batch_size': 249, 'step_size': 9, 'gamma': 0.7776255619916482}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:07:11,620][0m Trial 43 finished with value: 0.08173348009586334 and parameters: {'observation_period_num': 40, 'train_rates': 0.9552203000688001, 'learning_rate': 0.00036799894370085875, 'batch_size': 235, 'step_size': 10, 'gamma': 0.8015706376026659}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:07:41,064][0m Trial 44 finished with value: 0.04579084575873952 and parameters: {'observation_period_num': 16, 'train_rates': 0.9001279473912301, 'learning_rate': 0.0006134235395524156, 'batch_size': 220, 'step_size': 7, 'gamma': 0.7683904107898577}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:08:10,677][0m Trial 45 finished with value: 0.24687358736991882 and parameters: {'observation_period_num': 189, 'train_rates': 0.9346696455045748, 'learning_rate': 0.00017428271865012354, 'batch_size': 214, 'step_size': 11, 'gamma': 0.7846272983468364}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:08:44,111][0m Trial 46 finished with value: 0.07711545377969742 and parameters: {'observation_period_num': 56, 'train_rates': 0.9708561486362806, 'learning_rate': 0.0009533856963075518, 'batch_size': 200, 'step_size': 8, 'gamma': 0.7609452719299731}. Best is trial 38 with value: 0.03565332955784268.[0m
Early stopping at epoch 72
[32m[I 2025-02-09 09:09:07,788][0m Trial 47 finished with value: 0.1506294698675693 and parameters: {'observation_period_num': 13, 'train_rates': 0.8185279604363986, 'learning_rate': 0.0002706456179255803, 'batch_size': 180, 'step_size': 1, 'gamma': 0.8083093637272043}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:09:50,047][0m Trial 48 finished with value: 0.04468026615679264 and parameters: {'observation_period_num': 28, 'train_rates': 0.9161268063369685, 'learning_rate': 0.000689049584680161, 'batch_size': 147, 'step_size': 11, 'gamma': 0.7902425260286446}. Best is trial 38 with value: 0.03565332955784268.[0m
[32m[I 2025-02-09 09:10:16,117][0m Trial 49 finished with value: 0.25117265901543423 and parameters: {'observation_period_num': 229, 'train_rates': 0.8447111412126271, 'learning_rate': 0.00018939250570983992, 'batch_size': 226, 'step_size': 12, 'gamma': 0.7781748333295078}. Best is trial 38 with value: 0.03565332955784268.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5642 | 0.3832
Epoch 2/300, Loss: 0.2254 | 0.2370
Epoch 3/300, Loss: 0.2038 | 0.1717
Epoch 4/300, Loss: 0.2065 | 0.3463
Epoch 5/300, Loss: 0.2296 | 0.7885
Epoch 6/300, Loss: 0.1545 | 0.2970
Epoch 7/300, Loss: 0.1398 | 0.1866
Epoch 8/300, Loss: 0.1739 | 0.1811
Epoch 9/300, Loss: 0.1406 | 0.1518
Epoch 10/300, Loss: 0.1161 | 0.1426
Epoch 11/300, Loss: 0.1203 | 0.1461
Epoch 12/300, Loss: 0.1178 | 0.0906
Epoch 13/300, Loss: 0.1146 | 0.0936
Epoch 14/300, Loss: 0.1152 | 0.1456
Epoch 15/300, Loss: 0.1121 | 0.1411
Epoch 16/300, Loss: 0.1123 | 0.0826
Epoch 17/300, Loss: 0.1084 | 0.0756
Epoch 18/300, Loss: 0.1082 | 0.1698
Epoch 19/300, Loss: 0.1069 | 0.0803
Epoch 20/300, Loss: 0.0981 | 0.0777
Epoch 21/300, Loss: 0.0966 | 0.0682
Epoch 22/300, Loss: 0.0957 | 0.0737
Epoch 23/300, Loss: 0.0944 | 0.0713
Epoch 24/300, Loss: 0.0904 | 0.1027
Epoch 25/300, Loss: 0.0893 | 0.0662
Epoch 26/300, Loss: 0.0871 | 0.0631
Epoch 27/300, Loss: 0.0847 | 0.0603
Epoch 28/300, Loss: 0.0833 | 0.0650
Epoch 29/300, Loss: 0.0820 | 0.0588
Epoch 30/300, Loss: 0.0806 | 0.0566
Epoch 31/300, Loss: 0.0797 | 0.0582
Epoch 32/300, Loss: 0.0787 | 0.0581
Epoch 33/300, Loss: 0.0777 | 0.0538
Epoch 34/300, Loss: 0.0773 | 0.0518
Epoch 35/300, Loss: 0.0773 | 0.0532
Epoch 36/300, Loss: 0.0770 | 0.0533
Epoch 37/300, Loss: 0.0758 | 0.0519
Epoch 38/300, Loss: 0.0759 | 0.0502
Epoch 39/300, Loss: 0.0757 | 0.0517
Epoch 40/300, Loss: 0.0762 | 0.0531
Epoch 41/300, Loss: 0.0777 | 0.0511
Epoch 42/300, Loss: 0.0796 | 0.0507
Epoch 43/300, Loss: 0.0772 | 0.0534
Epoch 44/300, Loss: 0.0760 | 0.0498
Epoch 45/300, Loss: 0.0755 | 0.0487
Epoch 46/300, Loss: 0.0737 | 0.0536
Epoch 47/300, Loss: 0.0732 | 0.0473
Epoch 48/300, Loss: 0.0728 | 0.0496
Epoch 49/300, Loss: 0.0724 | 0.0477
Epoch 50/300, Loss: 0.0721 | 0.0470
Epoch 51/300, Loss: 0.0719 | 0.0479
Epoch 52/300, Loss: 0.0717 | 0.0470
Epoch 53/300, Loss: 0.0715 | 0.0472
Epoch 54/300, Loss: 0.0714 | 0.0466
Epoch 55/300, Loss: 0.0712 | 0.0469
Epoch 56/300, Loss: 0.0711 | 0.0464
Epoch 57/300, Loss: 0.0709 | 0.0466
Epoch 58/300, Loss: 0.0708 | 0.0460
Epoch 59/300, Loss: 0.0707 | 0.0465
Epoch 60/300, Loss: 0.0706 | 0.0458
Epoch 61/300, Loss: 0.0705 | 0.0462
Epoch 62/300, Loss: 0.0704 | 0.0454
Epoch 63/300, Loss: 0.0703 | 0.0464
Epoch 64/300, Loss: 0.0703 | 0.0454
Epoch 65/300, Loss: 0.0703 | 0.0459
Epoch 66/300, Loss: 0.0704 | 0.0453
Epoch 67/300, Loss: 0.0707 | 0.0463
Epoch 68/300, Loss: 0.0712 | 0.0457
Epoch 69/300, Loss: 0.0718 | 0.0448
Epoch 70/300, Loss: 0.0726 | 0.0539
Epoch 71/300, Loss: 0.0727 | 0.0444
Epoch 72/300, Loss: 0.0732 | 0.0601
Epoch 73/300, Loss: 0.0714 | 0.0443
Epoch 74/300, Loss: 0.0704 | 0.0539
Epoch 75/300, Loss: 0.0697 | 0.0445
Epoch 76/300, Loss: 0.0695 | 0.0475
Epoch 77/300, Loss: 0.0694 | 0.0453
Epoch 78/300, Loss: 0.0693 | 0.0460
Epoch 79/300, Loss: 0.0693 | 0.0456
Epoch 80/300, Loss: 0.0692 | 0.0454
Epoch 81/300, Loss: 0.0692 | 0.0454
Epoch 82/300, Loss: 0.0691 | 0.0453
Epoch 83/300, Loss: 0.0691 | 0.0452
Epoch 84/300, Loss: 0.0691 | 0.0452
Epoch 85/300, Loss: 0.0690 | 0.0451
Epoch 86/300, Loss: 0.0690 | 0.0450
Epoch 87/300, Loss: 0.0690 | 0.0450
Epoch 88/300, Loss: 0.0689 | 0.0449
Epoch 89/300, Loss: 0.0689 | 0.0449
Epoch 90/300, Loss: 0.0689 | 0.0449
Epoch 91/300, Loss: 0.0689 | 0.0448
Epoch 92/300, Loss: 0.0688 | 0.0448
Epoch 93/300, Loss: 0.0688 | 0.0448
Epoch 94/300, Loss: 0.0688 | 0.0447
Epoch 95/300, Loss: 0.0688 | 0.0447
Epoch 96/300, Loss: 0.0687 | 0.0447
Epoch 97/300, Loss: 0.0687 | 0.0446
Epoch 98/300, Loss: 0.0687 | 0.0446
Epoch 99/300, Loss: 0.0687 | 0.0446
Epoch 100/300, Loss: 0.0687 | 0.0446
Epoch 101/300, Loss: 0.0686 | 0.0445
Epoch 102/300, Loss: 0.0686 | 0.0445
Epoch 103/300, Loss: 0.0686 | 0.0445
Epoch 104/300, Loss: 0.0686 | 0.0445
Epoch 105/300, Loss: 0.0686 | 0.0445
Epoch 106/300, Loss: 0.0686 | 0.0444
Epoch 107/300, Loss: 0.0686 | 0.0444
Epoch 108/300, Loss: 0.0685 | 0.0444
Epoch 109/300, Loss: 0.0685 | 0.0444
Epoch 110/300, Loss: 0.0685 | 0.0444
Epoch 111/300, Loss: 0.0685 | 0.0444
Epoch 112/300, Loss: 0.0685 | 0.0444
Epoch 113/300, Loss: 0.0685 | 0.0443
Epoch 114/300, Loss: 0.0685 | 0.0443
Epoch 115/300, Loss: 0.0685 | 0.0443
Epoch 116/300, Loss: 0.0685 | 0.0443
Epoch 117/300, Loss: 0.0684 | 0.0443
Epoch 118/300, Loss: 0.0684 | 0.0443
Epoch 119/300, Loss: 0.0684 | 0.0443
Epoch 120/300, Loss: 0.0684 | 0.0443
Epoch 121/300, Loss: 0.0684 | 0.0443
Epoch 122/300, Loss: 0.0684 | 0.0443
Epoch 123/300, Loss: 0.0684 | 0.0443
Epoch 124/300, Loss: 0.0684 | 0.0442
Epoch 125/300, Loss: 0.0684 | 0.0442
Epoch 126/300, Loss: 0.0684 | 0.0442
Epoch 127/300, Loss: 0.0684 | 0.0442
Epoch 128/300, Loss: 0.0684 | 0.0442
Epoch 129/300, Loss: 0.0684 | 0.0442
Epoch 130/300, Loss: 0.0684 | 0.0442
Epoch 131/300, Loss: 0.0684 | 0.0442
Epoch 132/300, Loss: 0.0683 | 0.0442
Epoch 133/300, Loss: 0.0683 | 0.0442
Epoch 134/300, Loss: 0.0683 | 0.0442
Epoch 135/300, Loss: 0.0683 | 0.0442
Epoch 136/300, Loss: 0.0683 | 0.0442
Epoch 137/300, Loss: 0.0683 | 0.0442
Epoch 138/300, Loss: 0.0683 | 0.0442
Epoch 139/300, Loss: 0.0683 | 0.0442
Epoch 140/300, Loss: 0.0683 | 0.0442
Epoch 141/300, Loss: 0.0683 | 0.0442
Epoch 142/300, Loss: 0.0683 | 0.0442
Epoch 143/300, Loss: 0.0683 | 0.0442
Epoch 144/300, Loss: 0.0683 | 0.0441
Epoch 145/300, Loss: 0.0683 | 0.0441
Epoch 146/300, Loss: 0.0683 | 0.0441
Epoch 147/300, Loss: 0.0683 | 0.0441
Epoch 148/300, Loss: 0.0683 | 0.0441
Epoch 149/300, Loss: 0.0683 | 0.0441
Epoch 150/300, Loss: 0.0683 | 0.0441
Epoch 151/300, Loss: 0.0683 | 0.0441
Epoch 152/300, Loss: 0.0683 | 0.0441
Epoch 153/300, Loss: 0.0683 | 0.0441
Epoch 154/300, Loss: 0.0683 | 0.0441
Epoch 155/300, Loss: 0.0683 | 0.0441
Epoch 156/300, Loss: 0.0683 | 0.0441
Epoch 157/300, Loss: 0.0683 | 0.0441
Epoch 158/300, Loss: 0.0683 | 0.0441
Epoch 159/300, Loss: 0.0683 | 0.0441
Epoch 160/300, Loss: 0.0683 | 0.0441
Epoch 161/300, Loss: 0.0683 | 0.0441
Epoch 162/300, Loss: 0.0683 | 0.0441
Epoch 163/300, Loss: 0.0683 | 0.0441
Epoch 164/300, Loss: 0.0683 | 0.0441
Epoch 165/300, Loss: 0.0683 | 0.0441
Epoch 166/300, Loss: 0.0683 | 0.0441
Epoch 167/300, Loss: 0.0683 | 0.0441
Epoch 168/300, Loss: 0.0683 | 0.0441
Epoch 169/300, Loss: 0.0683 | 0.0441
Epoch 170/300, Loss: 0.0683 | 0.0441
Epoch 171/300, Loss: 0.0683 | 0.0441
Epoch 172/300, Loss: 0.0683 | 0.0441
Epoch 173/300, Loss: 0.0683 | 0.0441
Epoch 174/300, Loss: 0.0683 | 0.0441
Epoch 175/300, Loss: 0.0683 | 0.0441
Epoch 176/300, Loss: 0.0683 | 0.0441
Epoch 177/300, Loss: 0.0683 | 0.0441
Epoch 178/300, Loss: 0.0683 | 0.0441
Epoch 179/300, Loss: 0.0683 | 0.0441
Epoch 180/300, Loss: 0.0683 | 0.0441
Epoch 181/300, Loss: 0.0683 | 0.0441
Epoch 182/300, Loss: 0.0683 | 0.0441
Epoch 183/300, Loss: 0.0683 | 0.0441
Epoch 184/300, Loss: 0.0683 | 0.0441
Epoch 185/300, Loss: 0.0683 | 0.0441
Epoch 186/300, Loss: 0.0683 | 0.0441
Epoch 187/300, Loss: 0.0683 | 0.0441
Epoch 188/300, Loss: 0.0683 | 0.0441
Epoch 189/300, Loss: 0.0683 | 0.0441
Epoch 190/300, Loss: 0.0683 | 0.0441
Epoch 191/300, Loss: 0.0683 | 0.0441
Epoch 192/300, Loss: 0.0683 | 0.0441
Epoch 193/300, Loss: 0.0683 | 0.0441
Epoch 194/300, Loss: 0.0683 | 0.0441
Epoch 195/300, Loss: 0.0683 | 0.0441
Epoch 196/300, Loss: 0.0683 | 0.0441
Epoch 197/300, Loss: 0.0683 | 0.0441
Epoch 198/300, Loss: 0.0683 | 0.0441
Epoch 199/300, Loss: 0.0683 | 0.0441
Epoch 200/300, Loss: 0.0683 | 0.0441
Epoch 201/300, Loss: 0.0683 | 0.0441
Epoch 202/300, Loss: 0.0683 | 0.0441
Epoch 203/300, Loss: 0.0683 | 0.0441
Epoch 204/300, Loss: 0.0683 | 0.0441
Epoch 205/300, Loss: 0.0683 | 0.0441
Epoch 206/300, Loss: 0.0683 | 0.0441
Epoch 207/300, Loss: 0.0683 | 0.0441
Epoch 208/300, Loss: 0.0682 | 0.0441
Epoch 209/300, Loss: 0.0682 | 0.0441
Epoch 210/300, Loss: 0.0682 | 0.0441
Epoch 211/300, Loss: 0.0682 | 0.0441
Epoch 212/300, Loss: 0.0682 | 0.0441
Epoch 213/300, Loss: 0.0682 | 0.0441
Epoch 214/300, Loss: 0.0682 | 0.0441
Epoch 215/300, Loss: 0.0682 | 0.0441
Epoch 216/300, Loss: 0.0682 | 0.0441
Epoch 217/300, Loss: 0.0682 | 0.0441
Epoch 218/300, Loss: 0.0682 | 0.0441
Epoch 219/300, Loss: 0.0682 | 0.0441
Epoch 220/300, Loss: 0.0682 | 0.0441
Epoch 221/300, Loss: 0.0682 | 0.0441
Epoch 222/300, Loss: 0.0682 | 0.0441
Epoch 223/300, Loss: 0.0682 | 0.0441
Epoch 224/300, Loss: 0.0682 | 0.0441
Epoch 225/300, Loss: 0.0682 | 0.0441
Epoch 226/300, Loss: 0.0682 | 0.0441
Epoch 227/300, Loss: 0.0682 | 0.0441
Epoch 228/300, Loss: 0.0682 | 0.0441
Epoch 229/300, Loss: 0.0682 | 0.0441
Epoch 230/300, Loss: 0.0682 | 0.0441
Epoch 231/300, Loss: 0.0682 | 0.0441
Epoch 232/300, Loss: 0.0682 | 0.0441
Epoch 233/300, Loss: 0.0682 | 0.0441
Epoch 234/300, Loss: 0.0682 | 0.0441
Epoch 235/300, Loss: 0.0682 | 0.0441
Epoch 236/300, Loss: 0.0682 | 0.0441
Epoch 237/300, Loss: 0.0682 | 0.0441
Epoch 238/300, Loss: 0.0682 | 0.0441
Epoch 239/300, Loss: 0.0682 | 0.0441
Epoch 240/300, Loss: 0.0682 | 0.0441
Epoch 241/300, Loss: 0.0682 | 0.0441
Epoch 242/300, Loss: 0.0682 | 0.0441
Epoch 243/300, Loss: 0.0682 | 0.0441
Epoch 244/300, Loss: 0.0682 | 0.0441
Epoch 245/300, Loss: 0.0682 | 0.0441
Epoch 246/300, Loss: 0.0682 | 0.0441
Epoch 247/300, Loss: 0.0682 | 0.0441
Epoch 248/300, Loss: 0.0682 | 0.0441
Epoch 249/300, Loss: 0.0682 | 0.0441
Epoch 250/300, Loss: 0.0682 | 0.0441
Epoch 251/300, Loss: 0.0682 | 0.0441
Epoch 252/300, Loss: 0.0682 | 0.0441
Epoch 253/300, Loss: 0.0682 | 0.0441
Epoch 254/300, Loss: 0.0682 | 0.0441
Epoch 255/300, Loss: 0.0682 | 0.0441
Epoch 256/300, Loss: 0.0682 | 0.0441
Epoch 257/300, Loss: 0.0682 | 0.0441
Epoch 258/300, Loss: 0.0682 | 0.0441
Epoch 259/300, Loss: 0.0682 | 0.0441
Epoch 260/300, Loss: 0.0682 | 0.0441
Epoch 261/300, Loss: 0.0682 | 0.0441
Epoch 262/300, Loss: 0.0682 | 0.0441
Epoch 263/300, Loss: 0.0682 | 0.0441
Epoch 264/300, Loss: 0.0682 | 0.0441
Epoch 265/300, Loss: 0.0682 | 0.0441
Epoch 266/300, Loss: 0.0682 | 0.0441
Epoch 267/300, Loss: 0.0682 | 0.0441
Early stopping
Runtime (seconds): 83.22737050056458
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 156.95842751883902
RMSE: 12.528305053710938
MAE: 12.528305053710938
R-squared: nan
[199.6683]
