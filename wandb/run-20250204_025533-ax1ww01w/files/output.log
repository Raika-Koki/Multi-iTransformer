[32m[I 2025-02-04 02:55:36,522][0m A new study created in memory with name: no-name-631e94f8-fecb-43a9-bc11-d286b816f15c[0m
[32m[I 2025-02-04 02:55:58,396][0m Trial 0 finished with value: 0.48781291227652435 and parameters: {'observation_period_num': 241, 'train_rates': 0.6961523310473045, 'learning_rate': 2.1463020431780617e-05, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8632283618151946}. Best is trial 0 with value: 0.48781291227652435.[0m
[32m[I 2025-02-04 02:57:21,416][0m Trial 1 finished with value: 0.4968799629681547 and parameters: {'observation_period_num': 211, 'train_rates': 0.948527088346133, 'learning_rate': 1.2237977862846963e-06, 'batch_size': 68, 'step_size': 11, 'gamma': 0.8217123698232514}. Best is trial 0 with value: 0.48781291227652435.[0m
[32m[I 2025-02-04 02:58:13,050][0m Trial 2 finished with value: 0.16096140444278717 and parameters: {'observation_period_num': 151, 'train_rates': 0.9853686505667136, 'learning_rate': 2.4508698926966348e-05, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9503677352499798}. Best is trial 2 with value: 0.16096140444278717.[0m
[32m[I 2025-02-04 03:00:15,463][0m Trial 3 finished with value: 0.43294142411179737 and parameters: {'observation_period_num': 172, 'train_rates': 0.9739419603209218, 'learning_rate': 1.471836801476438e-06, 'batch_size': 46, 'step_size': 9, 'gamma': 0.9120154527940137}. Best is trial 2 with value: 0.16096140444278717.[0m
[32m[I 2025-02-04 03:00:38,647][0m Trial 4 finished with value: 0.10461726784706116 and parameters: {'observation_period_num': 141, 'train_rates': 0.8457983995999109, 'learning_rate': 0.00020128891605815713, 'batch_size': 240, 'step_size': 11, 'gamma': 0.8992355716777245}. Best is trial 4 with value: 0.10461726784706116.[0m
[32m[I 2025-02-04 03:01:09,608][0m Trial 5 finished with value: 0.6641049981117249 and parameters: {'observation_period_num': 238, 'train_rates': 0.9703091410461417, 'learning_rate': 2.546905563156013e-06, 'batch_size': 194, 'step_size': 8, 'gamma': 0.8308554266922165}. Best is trial 4 with value: 0.10461726784706116.[0m
[32m[I 2025-02-04 03:05:14,385][0m Trial 6 finished with value: 0.043142098382637196 and parameters: {'observation_period_num': 23, 'train_rates': 0.9126397460645344, 'learning_rate': 0.00037378634962628455, 'batch_size': 23, 'step_size': 14, 'gamma': 0.8374274641202897}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:06:03,524][0m Trial 7 finished with value: 0.1721198692133552 and parameters: {'observation_period_num': 215, 'train_rates': 0.7924941724892064, 'learning_rate': 2.6574387147968726e-05, 'batch_size': 105, 'step_size': 15, 'gamma': 0.9651573810983936}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:08:03,986][0m Trial 8 finished with value: 0.04651024469718841 and parameters: {'observation_period_num': 16, 'train_rates': 0.7187744954762632, 'learning_rate': 0.00017182670080196792, 'batch_size': 40, 'step_size': 11, 'gamma': 0.7641417527289067}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:09:31,307][0m Trial 9 finished with value: 0.10143995264780174 and parameters: {'observation_period_num': 176, 'train_rates': 0.8149595936066978, 'learning_rate': 0.00011104870869231363, 'batch_size': 58, 'step_size': 6, 'gamma': 0.8548869082993327}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:10:06,832][0m Trial 10 finished with value: 0.06492494974518195 and parameters: {'observation_period_num': 43, 'train_rates': 0.8904141277770061, 'learning_rate': 0.0008863744044572618, 'batch_size': 169, 'step_size': 3, 'gamma': 0.7639987294123627}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:14:32,286][0m Trial 11 finished with value: 0.05720383055622663 and parameters: {'observation_period_num': 5, 'train_rates': 0.6037842573901829, 'learning_rate': 0.000655547547536738, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7538112227008049}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:18:37,968][0m Trial 12 finished with value: 0.12294232719297192 and parameters: {'observation_period_num': 81, 'train_rates': 0.717643382319787, 'learning_rate': 0.00015461961980549422, 'batch_size': 19, 'step_size': 13, 'gamma': 0.7965809870468434}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:19:39,416][0m Trial 13 finished with value: 0.09979496116696651 and parameters: {'observation_period_num': 88, 'train_rates': 0.7446539188575213, 'learning_rate': 0.00033222287316934213, 'batch_size': 82, 'step_size': 13, 'gamma': 0.7948182620184892}. Best is trial 6 with value: 0.043142098382637196.[0m
Early stopping at epoch 56
[32m[I 2025-02-04 03:19:58,216][0m Trial 14 finished with value: 0.25635706873951253 and parameters: {'observation_period_num': 23, 'train_rates': 0.6500746287822022, 'learning_rate': 7.210135720694357e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.7989460867984464}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:21:00,881][0m Trial 15 finished with value: 0.14095253911927516 and parameters: {'observation_period_num': 65, 'train_rates': 0.9113145446060688, 'learning_rate': 9.492546182465568e-06, 'batch_size': 92, 'step_size': 13, 'gamma': 0.9079071076851895}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:23:02,208][0m Trial 16 finished with value: 0.09089128262511603 and parameters: {'observation_period_num': 104, 'train_rates': 0.766948465061819, 'learning_rate': 0.00038110437478114086, 'batch_size': 41, 'step_size': 5, 'gamma': 0.775004444985765}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:23:44,884][0m Trial 17 finished with value: 0.06671385336977002 and parameters: {'observation_period_num': 44, 'train_rates': 0.8557391990089834, 'learning_rate': 6.610435112431149e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8332802621945151}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:25:50,152][0m Trial 18 finished with value: 0.22933245427631763 and parameters: {'observation_period_num': 115, 'train_rates': 0.6760341603702456, 'learning_rate': 0.0004955143321002514, 'batch_size': 36, 'step_size': 13, 'gamma': 0.8726602066462092}. Best is trial 6 with value: 0.043142098382637196.[0m
[32m[I 2025-02-04 03:27:21,712][0m Trial 19 finished with value: 0.02979936271788567 and parameters: {'observation_period_num': 7, 'train_rates': 0.9142889944854871, 'learning_rate': 0.00025279350221014296, 'batch_size': 63, 'step_size': 15, 'gamma': 0.9333863977700882}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:28:36,156][0m Trial 20 finished with value: 0.0980811629976545 and parameters: {'observation_period_num': 47, 'train_rates': 0.9184934040886996, 'learning_rate': 9.239444540341943e-06, 'batch_size': 78, 'step_size': 15, 'gamma': 0.9272474994964742}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:31:14,150][0m Trial 21 finished with value: 0.04005115658142849 and parameters: {'observation_period_num': 9, 'train_rates': 0.8713524654423124, 'learning_rate': 0.00024136631657609774, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8865212261835438}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:36:28,886][0m Trial 22 finished with value: 0.061551937494765625 and parameters: {'observation_period_num': 32, 'train_rates': 0.8611052125087794, 'learning_rate': 0.0003080720712688487, 'batch_size': 17, 'step_size': 14, 'gamma': 0.8821107701286717}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:38:03,644][0m Trial 23 finished with value: 0.04171118587023372 and parameters: {'observation_period_num': 7, 'train_rates': 0.8914586474967738, 'learning_rate': 0.000980137968546486, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9872244383430495}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:39:29,086][0m Trial 24 finished with value: 0.08080177653089975 and parameters: {'observation_period_num': 64, 'train_rates': 0.8220418324153883, 'learning_rate': 0.0009345303887940363, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9704024517411833}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:40:28,834][0m Trial 25 finished with value: 0.043092184446074745 and parameters: {'observation_period_num': 6, 'train_rates': 0.8848045940358779, 'learning_rate': 5.837324611787632e-05, 'batch_size': 98, 'step_size': 14, 'gamma': 0.9390181486732475}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:42:12,258][0m Trial 26 finished with value: 0.10558300659708354 and parameters: {'observation_period_num': 61, 'train_rates': 0.9365337676610098, 'learning_rate': 0.0002371413087676714, 'batch_size': 56, 'step_size': 15, 'gamma': 0.9858422893472957}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:43:02,398][0m Trial 27 finished with value: 0.05055192982157072 and parameters: {'observation_period_num': 34, 'train_rates': 0.8820845041894936, 'learning_rate': 0.00011195986981108315, 'batch_size': 115, 'step_size': 12, 'gamma': 0.9825620426196505}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:44:16,125][0m Trial 28 finished with value: 0.11796089872985543 and parameters: {'observation_period_num': 86, 'train_rates': 0.9421004706551733, 'learning_rate': 0.0006315697631154894, 'batch_size': 80, 'step_size': 9, 'gamma': 0.9533660793252716}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:44:44,138][0m Trial 29 finished with value: 0.03514671020680193 and parameters: {'observation_period_num': 5, 'train_rates': 0.8339775764751701, 'learning_rate': 0.000982554574512829, 'batch_size': 208, 'step_size': 14, 'gamma': 0.925704658551476}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:45:11,806][0m Trial 30 finished with value: 0.12671521599863617 and parameters: {'observation_period_num': 50, 'train_rates': 0.8294615827613777, 'learning_rate': 4.0126130066933794e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.890920095066923}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:45:40,613][0m Trial 31 finished with value: 0.036566955471084316 and parameters: {'observation_period_num': 5, 'train_rates': 0.8669726074804591, 'learning_rate': 0.0009309241545573913, 'batch_size': 205, 'step_size': 15, 'gamma': 0.9195966477431414}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:46:06,628][0m Trial 32 finished with value: 0.10374964018777331 and parameters: {'observation_period_num': 27, 'train_rates': 0.7931351295530522, 'learning_rate': 0.0005752632960581313, 'batch_size': 215, 'step_size': 15, 'gamma': 0.924881671359994}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:46:38,655][0m Trial 33 finished with value: 0.039432932567947054 and parameters: {'observation_period_num': 20, 'train_rates': 0.8614582434034985, 'learning_rate': 0.0004814014593217425, 'batch_size': 192, 'step_size': 14, 'gamma': 0.9263568568258926}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:47:08,830][0m Trial 34 finished with value: 0.051379167018648315 and parameters: {'observation_period_num': 23, 'train_rates': 0.7998412286963377, 'learning_rate': 0.0005286706460391115, 'batch_size': 193, 'step_size': 12, 'gamma': 0.9244413703980671}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:47:32,998][0m Trial 35 finished with value: 0.09376244891334225 and parameters: {'observation_period_num': 74, 'train_rates': 0.8462373926667118, 'learning_rate': 0.0007541460254349092, 'batch_size': 252, 'step_size': 10, 'gamma': 0.9469595945086637}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:48:02,655][0m Trial 36 finished with value: 0.06415886986877553 and parameters: {'observation_period_num': 40, 'train_rates': 0.8342501811314392, 'learning_rate': 0.00045331651798114576, 'batch_size': 191, 'step_size': 13, 'gamma': 0.9063421682469565}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:48:37,292][0m Trial 37 finished with value: 0.15329104661941528 and parameters: {'observation_period_num': 168, 'train_rates': 0.9611566988957116, 'learning_rate': 0.0001133952625967838, 'batch_size': 172, 'step_size': 15, 'gamma': 0.9357852313838482}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:49:10,003][0m Trial 38 finished with value: 0.046490106731653214 and parameters: {'observation_period_num': 20, 'train_rates': 0.9891335924203648, 'learning_rate': 0.0002845726928632665, 'batch_size': 206, 'step_size': 7, 'gamma': 0.9161499246703139}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:49:34,045][0m Trial 39 finished with value: 0.2137164041622361 and parameters: {'observation_period_num': 55, 'train_rates': 0.7694297515300793, 'learning_rate': 1.1306212806220423e-05, 'batch_size': 234, 'step_size': 14, 'gamma': 0.9609472560889523}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:50:11,273][0m Trial 40 finished with value: 0.10952180791475358 and parameters: {'observation_period_num': 143, 'train_rates': 0.9226845988464716, 'learning_rate': 0.00016202576457397834, 'batch_size': 155, 'step_size': 11, 'gamma': 0.8990048690749977}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:50:45,252][0m Trial 41 finished with value: 0.04739168650989167 and parameters: {'observation_period_num': 16, 'train_rates': 0.8715156122117131, 'learning_rate': 0.0002641399494294507, 'batch_size': 181, 'step_size': 14, 'gamma': 0.8664435566286568}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:51:12,577][0m Trial 42 finished with value: 0.03473168720559376 and parameters: {'observation_period_num': 12, 'train_rates': 0.9027474033859653, 'learning_rate': 0.0004286600739262233, 'batch_size': 228, 'step_size': 15, 'gamma': 0.8522032242642903}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:51:41,799][0m Trial 43 finished with value: 0.10214309213383341 and parameters: {'observation_period_num': 34, 'train_rates': 0.8919164008799562, 'learning_rate': 0.00041728395450130195, 'batch_size': 207, 'step_size': 15, 'gamma': 0.8589456966959371}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:52:10,137][0m Trial 44 finished with value: 0.0368104557373694 and parameters: {'observation_period_num': 14, 'train_rates': 0.9051875835473822, 'learning_rate': 0.0006956310084883338, 'batch_size': 233, 'step_size': 14, 'gamma': 0.9363897723053028}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:52:33,498][0m Trial 45 finished with value: 0.11461753565026987 and parameters: {'observation_period_num': 199, 'train_rates': 0.906361053809865, 'learning_rate': 0.0007077169372346574, 'batch_size': 255, 'step_size': 15, 'gamma': 0.8425128213615083}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:52:59,408][0m Trial 46 finished with value: 0.13784338533878326 and parameters: {'observation_period_num': 251, 'train_rates': 0.8997042081583687, 'learning_rate': 0.0009987192332127795, 'batch_size': 239, 'step_size': 13, 'gamma': 0.8185374046675061}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:53:28,593][0m Trial 47 finished with value: 0.19427451491355896 and parameters: {'observation_period_num': 15, 'train_rates': 0.9630878297703054, 'learning_rate': 4.482058454668033e-06, 'batch_size': 231, 'step_size': 13, 'gamma': 0.9436867939091517}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:53:58,009][0m Trial 48 finished with value: 0.032154348658190834 and parameters: {'observation_period_num': 5, 'train_rates': 0.9238641715667693, 'learning_rate': 0.0007721357224746735, 'batch_size': 223, 'step_size': 15, 'gamma': 0.8488858494385877}. Best is trial 19 with value: 0.02979936271788567.[0m
[32m[I 2025-02-04 03:54:26,979][0m Trial 49 finished with value: 0.045178432017564774 and parameters: {'observation_period_num': 5, 'train_rates': 0.9322120575079679, 'learning_rate': 0.00018784503429999713, 'batch_size': 223, 'step_size': 15, 'gamma': 0.8543734661114547}. Best is trial 19 with value: 0.02979936271788567.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1994 | 0.1288
Epoch 2/300, Loss: 0.1048 | 0.0965
Epoch 3/300, Loss: 0.0896 | 0.0803
Epoch 4/300, Loss: 0.0788 | 0.0691
Epoch 5/300, Loss: 0.0711 | 0.0661
Epoch 6/300, Loss: 0.0654 | 0.0632
Epoch 7/300, Loss: 0.0617 | 0.0630
Epoch 8/300, Loss: 0.0590 | 0.0624
Epoch 9/300, Loss: 0.0562 | 0.0591
Epoch 10/300, Loss: 0.0537 | 0.0560
Epoch 11/300, Loss: 0.0514 | 0.0550
Epoch 12/300, Loss: 0.0491 | 0.0511
Epoch 13/300, Loss: 0.0469 | 0.0487
Epoch 14/300, Loss: 0.0460 | 0.0480
Epoch 15/300, Loss: 0.0460 | 0.0484
Epoch 16/300, Loss: 0.0454 | 0.0446
Epoch 17/300, Loss: 0.0463 | 0.0520
Epoch 18/300, Loss: 0.0511 | 0.0542
Epoch 19/300, Loss: 0.0436 | 0.0446
Epoch 20/300, Loss: 0.0416 | 0.0470
Epoch 21/300, Loss: 0.0392 | 0.0396
Epoch 22/300, Loss: 0.0387 | 0.0401
Epoch 23/300, Loss: 0.0377 | 0.0387
Epoch 24/300, Loss: 0.0372 | 0.0390
Epoch 25/300, Loss: 0.0390 | 0.0429
Epoch 26/300, Loss: 0.0371 | 0.0391
Epoch 27/300, Loss: 0.0410 | 0.0512
Epoch 28/300, Loss: 0.0362 | 0.0371
Epoch 29/300, Loss: 0.0376 | 0.0359
Epoch 30/300, Loss: 0.0372 | 0.0374
Epoch 31/300, Loss: 0.0370 | 0.0364
Epoch 32/300, Loss: 0.0358 | 0.0373
Epoch 33/300, Loss: 0.0352 | 0.0404
Epoch 34/300, Loss: 0.0352 | 0.0402
Epoch 35/300, Loss: 0.0384 | 0.0446
Epoch 36/300, Loss: 0.0345 | 0.0367
Epoch 37/300, Loss: 0.0328 | 0.0352
Epoch 38/300, Loss: 0.0312 | 0.0329
Epoch 39/300, Loss: 0.0300 | 0.0331
Epoch 40/300, Loss: 0.0295 | 0.0330
Epoch 41/300, Loss: 0.0290 | 0.0325
Epoch 42/300, Loss: 0.0287 | 0.0328
Epoch 43/300, Loss: 0.0285 | 0.0321
Epoch 44/300, Loss: 0.0284 | 0.0326
Epoch 45/300, Loss: 0.0284 | 0.0328
Epoch 46/300, Loss: 0.0290 | 0.0336
Epoch 47/300, Loss: 0.0310 | 0.0422
Epoch 48/300, Loss: 0.0295 | 0.0310
Epoch 49/300, Loss: 0.0279 | 0.0305
Epoch 50/300, Loss: 0.0286 | 0.0320
Epoch 51/300, Loss: 0.0295 | 0.0345
Epoch 52/300, Loss: 0.0291 | 0.0310
Epoch 53/300, Loss: 0.0298 | 0.0344
Epoch 54/300, Loss: 0.0291 | 0.0322
Epoch 55/300, Loss: 0.0282 | 0.0328
Epoch 56/300, Loss: 0.0274 | 0.0309
Epoch 57/300, Loss: 0.0280 | 0.0400
Epoch 58/300, Loss: 0.0268 | 0.0297
Epoch 59/300, Loss: 0.0256 | 0.0296
Epoch 60/300, Loss: 0.0252 | 0.0292
Epoch 61/300, Loss: 0.0250 | 0.0302
Epoch 62/300, Loss: 0.0249 | 0.0300
Epoch 63/300, Loss: 0.0247 | 0.0308
Epoch 64/300, Loss: 0.0244 | 0.0300
Epoch 65/300, Loss: 0.0245 | 0.0317
Epoch 66/300, Loss: 0.0245 | 0.0314
Epoch 67/300, Loss: 0.0253 | 0.0369
Epoch 68/300, Loss: 0.0250 | 0.0315
Epoch 69/300, Loss: 0.0269 | 0.0415
Epoch 70/300, Loss: 0.0255 | 0.0304
Epoch 71/300, Loss: 0.0288 | 0.0309
Epoch 72/300, Loss: 0.0262 | 0.0315
Epoch 73/300, Loss: 0.0284 | 0.0337
Epoch 74/300, Loss: 0.0262 | 0.0300
Epoch 75/300, Loss: 0.0278 | 0.0310
Epoch 76/300, Loss: 0.0283 | 0.0303
Epoch 77/300, Loss: 0.0256 | 0.0286
Epoch 78/300, Loss: 0.0234 | 0.0283
Epoch 79/300, Loss: 0.0228 | 0.0279
Epoch 80/300, Loss: 0.0222 | 0.0279
Epoch 81/300, Loss: 0.0222 | 0.0278
Epoch 82/300, Loss: 0.0216 | 0.0276
Epoch 83/300, Loss: 0.0217 | 0.0277
Epoch 84/300, Loss: 0.0213 | 0.0275
Epoch 85/300, Loss: 0.0214 | 0.0277
Epoch 86/300, Loss: 0.0211 | 0.0277
Epoch 87/300, Loss: 0.0210 | 0.0278
Epoch 88/300, Loss: 0.0208 | 0.0279
Epoch 89/300, Loss: 0.0208 | 0.0280
Epoch 90/300, Loss: 0.0206 | 0.0282
Epoch 91/300, Loss: 0.0206 | 0.0281
Epoch 92/300, Loss: 0.0205 | 0.0283
Epoch 93/300, Loss: 0.0206 | 0.0283
Epoch 94/300, Loss: 0.0206 | 0.0302
Epoch 95/300, Loss: 0.0206 | 0.0285
Epoch 96/300, Loss: 0.0204 | 0.0297
Epoch 97/300, Loss: 0.0204 | 0.0287
Epoch 98/300, Loss: 0.0204 | 0.0301
Epoch 99/300, Loss: 0.0204 | 0.0288
Epoch 100/300, Loss: 0.0202 | 0.0289
Epoch 101/300, Loss: 0.0203 | 0.0288
Epoch 102/300, Loss: 0.0202 | 0.0293
Epoch 103/300, Loss: 0.0203 | 0.0290
Epoch 104/300, Loss: 0.0204 | 0.0299
Epoch 105/300, Loss: 0.0206 | 0.0296
Epoch 106/300, Loss: 0.0207 | 0.0296
Epoch 107/300, Loss: 0.0202 | 0.0296
Epoch 108/300, Loss: 0.0206 | 0.0298
Epoch 109/300, Loss: 0.0205 | 0.0305
Epoch 110/300, Loss: 0.0209 | 0.0304
Epoch 111/300, Loss: 0.0206 | 0.0305
Epoch 112/300, Loss: 0.0214 | 0.0319
Epoch 113/300, Loss: 0.0215 | 0.0337
Epoch 114/300, Loss: 0.0204 | 0.0313
Epoch 115/300, Loss: 0.0206 | 0.0319
Epoch 116/300, Loss: 0.0235 | 0.0334
Epoch 117/300, Loss: 0.0233 | 0.0321
Epoch 118/300, Loss: 0.0299 | 0.0350
Epoch 119/300, Loss: 0.0256 | 0.0313
Epoch 120/300, Loss: 0.0291 | 0.0451
Epoch 121/300, Loss: 0.0309 | 0.0307
Epoch 122/300, Loss: 0.0212 | 0.0296
Epoch 123/300, Loss: 0.0202 | 0.0288
Epoch 124/300, Loss: 0.0192 | 0.0306
Epoch 125/300, Loss: 0.0193 | 0.0290
Epoch 126/300, Loss: 0.0185 | 0.0305
Epoch 127/300, Loss: 0.0182 | 0.0289
Epoch 128/300, Loss: 0.0181 | 0.0306
Epoch 129/300, Loss: 0.0176 | 0.0289
Epoch 130/300, Loss: 0.0177 | 0.0301
Epoch 131/300, Loss: 0.0173 | 0.0291
Epoch 132/300, Loss: 0.0177 | 0.0299
Epoch 133/300, Loss: 0.0176 | 0.0294
Epoch 134/300, Loss: 0.0183 | 0.0301
Epoch 135/300, Loss: 0.0185 | 0.0309
Epoch 136/300, Loss: 0.0192 | 0.0340
Epoch 137/300, Loss: 0.0192 | 0.0338
Epoch 138/300, Loss: 0.0186 | 0.0314
Epoch 139/300, Loss: 0.0177 | 0.0302
Epoch 140/300, Loss: 0.0173 | 0.0301
Epoch 141/300, Loss: 0.0175 | 0.0306
Epoch 142/300, Loss: 0.0179 | 0.0316
Epoch 143/300, Loss: 0.0181 | 0.0332
Epoch 144/300, Loss: 0.0183 | 0.0351
Epoch 145/300, Loss: 0.0182 | 0.0370
Epoch 146/300, Loss: 0.0178 | 0.0371
Epoch 147/300, Loss: 0.0170 | 0.0353
Epoch 148/300, Loss: 0.0171 | 0.0341
Epoch 149/300, Loss: 0.0167 | 0.0334
Epoch 150/300, Loss: 0.0168 | 0.0325
Epoch 151/300, Loss: 0.0163 | 0.0317
Epoch 152/300, Loss: 0.0160 | 0.0307
Epoch 153/300, Loss: 0.0161 | 0.0319
Epoch 154/300, Loss: 0.0157 | 0.0304
Epoch 155/300, Loss: 0.0159 | 0.0319
Epoch 156/300, Loss: 0.0155 | 0.0306
Epoch 157/300, Loss: 0.0156 | 0.0318
Epoch 158/300, Loss: 0.0152 | 0.0310
Epoch 159/300, Loss: 0.0154 | 0.0319
Epoch 160/300, Loss: 0.0151 | 0.0313
Epoch 161/300, Loss: 0.0151 | 0.0319
Epoch 162/300, Loss: 0.0148 | 0.0315
Epoch 163/300, Loss: 0.0147 | 0.0321
Epoch 164/300, Loss: 0.0147 | 0.0315
Epoch 165/300, Loss: 0.0147 | 0.0325
Epoch 166/300, Loss: 0.0146 | 0.0324
Epoch 167/300, Loss: 0.0146 | 0.0331
Epoch 168/300, Loss: 0.0144 | 0.0325
Epoch 169/300, Loss: 0.0144 | 0.0331
Epoch 170/300, Loss: 0.0142 | 0.0323
Epoch 171/300, Loss: 0.0143 | 0.0332
Epoch 172/300, Loss: 0.0141 | 0.0322
Epoch 173/300, Loss: 0.0142 | 0.0331
Epoch 174/300, Loss: 0.0139 | 0.0323
Epoch 175/300, Loss: 0.0141 | 0.0328
Epoch 176/300, Loss: 0.0139 | 0.0325
Epoch 177/300, Loss: 0.0141 | 0.0330
Epoch 178/300, Loss: 0.0139 | 0.0330
Epoch 179/300, Loss: 0.0141 | 0.0335
Epoch 180/300, Loss: 0.0139 | 0.0333
Epoch 181/300, Loss: 0.0138 | 0.0336
Epoch 182/300, Loss: 0.0136 | 0.0334
Epoch 183/300, Loss: 0.0135 | 0.0335
Epoch 184/300, Loss: 0.0133 | 0.0333
Epoch 185/300, Loss: 0.0133 | 0.0335
Epoch 186/300, Loss: 0.0133 | 0.0334
Epoch 187/300, Loss: 0.0133 | 0.0341
Epoch 188/300, Loss: 0.0135 | 0.0337
Epoch 189/300, Loss: 0.0135 | 0.0352
Epoch 190/300, Loss: 0.0137 | 0.0345
Epoch 191/300, Loss: 0.0135 | 0.0359
Epoch 192/300, Loss: 0.0138 | 0.0351
Epoch 193/300, Loss: 0.0135 | 0.0366
Epoch 194/300, Loss: 0.0137 | 0.0361
Epoch 195/300, Loss: 0.0132 | 0.0357
Epoch 196/300, Loss: 0.0132 | 0.0348
Epoch 197/300, Loss: 0.0132 | 0.0343
Epoch 198/300, Loss: 0.0131 | 0.0347
Epoch 199/300, Loss: 0.0133 | 0.0355
Epoch 200/300, Loss: 0.0132 | 0.0353
Epoch 201/300, Loss: 0.0129 | 0.0348
Epoch 202/300, Loss: 0.0125 | 0.0346
Epoch 203/300, Loss: 0.0124 | 0.0347
Epoch 204/300, Loss: 0.0123 | 0.0351
Epoch 205/300, Loss: 0.0124 | 0.0356
Epoch 206/300, Loss: 0.0124 | 0.0362
Epoch 207/300, Loss: 0.0123 | 0.0362
Epoch 208/300, Loss: 0.0122 | 0.0362
Epoch 209/300, Loss: 0.0121 | 0.0361
Epoch 210/300, Loss: 0.0121 | 0.0362
Epoch 211/300, Loss: 0.0120 | 0.0360
Epoch 212/300, Loss: 0.0120 | 0.0361
Epoch 213/300, Loss: 0.0119 | 0.0361
Epoch 214/300, Loss: 0.0119 | 0.0363
Epoch 215/300, Loss: 0.0118 | 0.0364
Epoch 216/300, Loss: 0.0118 | 0.0365
Epoch 217/300, Loss: 0.0117 | 0.0362
Epoch 218/300, Loss: 0.0117 | 0.0363
Epoch 219/300, Loss: 0.0116 | 0.0360
Epoch 220/300, Loss: 0.0117 | 0.0360
Epoch 221/300, Loss: 0.0116 | 0.0361
Epoch 222/300, Loss: 0.0115 | 0.0364
Epoch 223/300, Loss: 0.0114 | 0.0363
Epoch 224/300, Loss: 0.0113 | 0.0365
Epoch 225/300, Loss: 0.0112 | 0.0370
Epoch 226/300, Loss: 0.0111 | 0.0371
Epoch 227/300, Loss: 0.0111 | 0.0368
Epoch 228/300, Loss: 0.0110 | 0.0367
Epoch 229/300, Loss: 0.0109 | 0.0365
Epoch 230/300, Loss: 0.0109 | 0.0366
Epoch 231/300, Loss: 0.0109 | 0.0366
Epoch 232/300, Loss: 0.0109 | 0.0369
Epoch 233/300, Loss: 0.0108 | 0.0371
Epoch 234/300, Loss: 0.0108 | 0.0376
Epoch 235/300, Loss: 0.0107 | 0.0375
Epoch 236/300, Loss: 0.0106 | 0.0374
Epoch 237/300, Loss: 0.0105 | 0.0374
Epoch 238/300, Loss: 0.0106 | 0.0374
Epoch 239/300, Loss: 0.0106 | 0.0377
Epoch 240/300, Loss: 0.0107 | 0.0378
Epoch 241/300, Loss: 0.0107 | 0.0385
Epoch 242/300, Loss: 0.0107 | 0.0384
Epoch 243/300, Loss: 0.0105 | 0.0384
Epoch 244/300, Loss: 0.0104 | 0.0379
Epoch 245/300, Loss: 0.0103 | 0.0382
Epoch 246/300, Loss: 0.0103 | 0.0376
Epoch 247/300, Loss: 0.0104 | 0.0383
Epoch 248/300, Loss: 0.0102 | 0.0376
Epoch 249/300, Loss: 0.0104 | 0.0382
Epoch 250/300, Loss: 0.0103 | 0.0378
Epoch 251/300, Loss: 0.0103 | 0.0382
Epoch 252/300, Loss: 0.0101 | 0.0379
Epoch 253/300, Loss: 0.0100 | 0.0381
Epoch 254/300, Loss: 0.0099 | 0.0384
Epoch 255/300, Loss: 0.0099 | 0.0381
Epoch 256/300, Loss: 0.0099 | 0.0386
Epoch 257/300, Loss: 0.0100 | 0.0381
Epoch 258/300, Loss: 0.0099 | 0.0386
Epoch 259/300, Loss: 0.0099 | 0.0379
Epoch 260/300, Loss: 0.0098 | 0.0384
Epoch 261/300, Loss: 0.0097 | 0.0380
Epoch 262/300, Loss: 0.0097 | 0.0384
Epoch 263/300, Loss: 0.0096 | 0.0383
Epoch 264/300, Loss: 0.0096 | 0.0383
Epoch 265/300, Loss: 0.0096 | 0.0384
Epoch 266/300, Loss: 0.0096 | 0.0381
Epoch 267/300, Loss: 0.0096 | 0.0384
Epoch 268/300, Loss: 0.0096 | 0.0383
Epoch 269/300, Loss: 0.0097 | 0.0385
Epoch 270/300, Loss: 0.0098 | 0.0384
Epoch 271/300, Loss: 0.0099 | 0.0388
Epoch 272/300, Loss: 0.0100 | 0.0385
Epoch 273/300, Loss: 0.0103 | 0.0392
Epoch 274/300, Loss: 0.0105 | 0.0387
Epoch 275/300, Loss: 0.0106 | 0.0390
Epoch 276/300, Loss: 0.0102 | 0.0391
Epoch 277/300, Loss: 0.0100 | 0.0391
Epoch 278/300, Loss: 0.0097 | 0.0391
Epoch 279/300, Loss: 0.0095 | 0.0393
Epoch 280/300, Loss: 0.0094 | 0.0393
Epoch 281/300, Loss: 0.0093 | 0.0397
Epoch 282/300, Loss: 0.0094 | 0.0396
Epoch 283/300, Loss: 0.0093 | 0.0398
Epoch 284/300, Loss: 0.0094 | 0.0396
Epoch 285/300, Loss: 0.0092 | 0.0398
Epoch 286/300, Loss: 0.0093 | 0.0396
Epoch 287/300, Loss: 0.0092 | 0.0396
Epoch 288/300, Loss: 0.0092 | 0.0395
Epoch 289/300, Loss: 0.0093 | 0.0396
Epoch 290/300, Loss: 0.0092 | 0.0395
Epoch 291/300, Loss: 0.0092 | 0.0396
Epoch 292/300, Loss: 0.0091 | 0.0395
Epoch 293/300, Loss: 0.0091 | 0.0396
Epoch 294/300, Loss: 0.0090 | 0.0395
Epoch 295/300, Loss: 0.0090 | 0.0396
Epoch 296/300, Loss: 0.0089 | 0.0395
Epoch 297/300, Loss: 0.0089 | 0.0396
Epoch 298/300, Loss: 0.0088 | 0.0396
Epoch 299/300, Loss: 0.0088 | 0.0396
Epoch 300/300, Loss: 0.0088 | 0.0396
Runtime (seconds): 274.2815148830414
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 30.301466879958753
RMSE: 5.504676818847656
MAE: 5.504676818847656
R-squared: nan
[119.07468]
