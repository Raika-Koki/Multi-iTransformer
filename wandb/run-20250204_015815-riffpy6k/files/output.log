[32m[I 2025-02-04 01:58:20,513][0m A new study created in memory with name: no-name-d62f9731-f964-4350-8d83-f5167521a026[0m
[32m[I 2025-02-04 01:59:03,203][0m Trial 0 finished with value: 1.2325914642169846 and parameters: {'observation_period_num': 143, 'train_rates': 0.717450077046976, 'learning_rate': 1.7180164919680076e-06, 'batch_size': 113, 'step_size': 10, 'gamma': 0.801744465496051}. Best is trial 0 with value: 1.2325914642169846.[0m
[32m[I 2025-02-04 01:59:34,377][0m Trial 1 finished with value: 0.5554616204623518 and parameters: {'observation_period_num': 245, 'train_rates': 0.861125120831246, 'learning_rate': 3.2541258634280547e-06, 'batch_size': 175, 'step_size': 4, 'gamma': 0.9871498132408181}. Best is trial 1 with value: 0.5554616204623518.[0m
[32m[I 2025-02-04 02:00:14,676][0m Trial 2 finished with value: 0.09536460275926995 and parameters: {'observation_period_num': 37, 'train_rates': 0.7820021580272343, 'learning_rate': 2.693957403680679e-05, 'batch_size': 131, 'step_size': 15, 'gamma': 0.8238888639707398}. Best is trial 2 with value: 0.09536460275926995.[0m
[32m[I 2025-02-04 02:04:57,131][0m Trial 3 finished with value: 0.06437820050774551 and parameters: {'observation_period_num': 49, 'train_rates': 0.9298455015170445, 'learning_rate': 0.00012044029329084214, 'batch_size': 20, 'step_size': 9, 'gamma': 0.9054004499356167}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:05:43,492][0m Trial 4 finished with value: 0.1431882925958277 and parameters: {'observation_period_num': 204, 'train_rates': 0.6118120396652102, 'learning_rate': 0.0007084313659319162, 'batch_size': 90, 'step_size': 4, 'gamma': 0.7759728989860556}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:07:33,529][0m Trial 5 finished with value: 0.14464971161763338 and parameters: {'observation_period_num': 34, 'train_rates': 0.6483474093316866, 'learning_rate': 3.79789235158808e-06, 'batch_size': 41, 'step_size': 15, 'gamma': 0.9538655669476358}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:07:59,477][0m Trial 6 finished with value: 0.15257128181002286 and parameters: {'observation_period_num': 234, 'train_rates': 0.8868761795147885, 'learning_rate': 0.0008706306037022488, 'batch_size': 226, 'step_size': 9, 'gamma': 0.826173317311936}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:10:22,620][0m Trial 7 finished with value: 0.31630013169456717 and parameters: {'observation_period_num': 198, 'train_rates': 0.8111126046569289, 'learning_rate': 5.119421040410272e-06, 'batch_size': 34, 'step_size': 11, 'gamma': 0.7824215021250178}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:11:44,384][0m Trial 8 finished with value: 0.38733230739885427 and parameters: {'observation_period_num': 8, 'train_rates': 0.6225826084073462, 'learning_rate': 2.7317037381818934e-06, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8257274882867751}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:12:04,871][0m Trial 9 finished with value: 0.7703964800525837 and parameters: {'observation_period_num': 190, 'train_rates': 0.671194929920483, 'learning_rate': 4.2350826124907585e-06, 'batch_size': 240, 'step_size': 4, 'gamma': 0.8346588400683016}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:12:42,423][0m Trial 10 finished with value: 0.2264276146888733 and parameters: {'observation_period_num': 89, 'train_rates': 0.9859549941252432, 'learning_rate': 0.00014562426023026576, 'batch_size': 176, 'step_size': 1, 'gamma': 0.9004796236552165}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:13:27,783][0m Trial 11 finished with value: 0.08372115343809128 and parameters: {'observation_period_num': 66, 'train_rates': 0.967650320532726, 'learning_rate': 3.39602252628064e-05, 'batch_size': 135, 'step_size': 15, 'gamma': 0.8855434944209369}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:14:02,842][0m Trial 12 finished with value: 0.18865501880645752 and parameters: {'observation_period_num': 85, 'train_rates': 0.9877989048142283, 'learning_rate': 3.8381508165480246e-05, 'batch_size': 181, 'step_size': 12, 'gamma': 0.8946239166446824}. Best is trial 3 with value: 0.06437820050774551.[0m
[32m[I 2025-02-04 02:15:21,556][0m Trial 13 finished with value: 0.057987303214978025 and parameters: {'observation_period_num': 82, 'train_rates': 0.9136281461315741, 'learning_rate': 7.304213752207776e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.9230451404902499}. Best is trial 13 with value: 0.057987303214978025.[0m
[32m[I 2025-02-04 02:16:33,838][0m Trial 14 finished with value: 0.09432420396321528 and parameters: {'observation_period_num': 131, 'train_rates': 0.9087269164872284, 'learning_rate': 0.00013466253900517106, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9305245720205295}. Best is trial 13 with value: 0.057987303214978025.[0m
[32m[I 2025-02-04 02:20:31,432][0m Trial 15 finished with value: 0.12195494520564039 and parameters: {'observation_period_num': 107, 'train_rates': 0.9168608589664884, 'learning_rate': 0.00016439437968586098, 'batch_size': 23, 'step_size': 7, 'gamma': 0.9296552133005483}. Best is trial 13 with value: 0.057987303214978025.[0m
[32m[I 2025-02-04 02:21:54,571][0m Trial 16 finished with value: 0.13243545007765592 and parameters: {'observation_period_num': 53, 'train_rates': 0.9317961658744651, 'learning_rate': 1.3840489486741763e-05, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8658051184228371}. Best is trial 13 with value: 0.057987303214978025.[0m
[32m[I 2025-02-04 02:27:23,721][0m Trial 17 finished with value: 0.04164296419800076 and parameters: {'observation_period_num': 7, 'train_rates': 0.8381312162375834, 'learning_rate': 0.0002790589873473393, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9281953944187191}. Best is trial 17 with value: 0.04164296419800076.[0m
[32m[I 2025-02-04 02:28:16,879][0m Trial 18 finished with value: 0.035109210669994353 and parameters: {'observation_period_num': 9, 'train_rates': 0.8306381261062397, 'learning_rate': 0.0003688264543741868, 'batch_size': 104, 'step_size': 13, 'gamma': 0.9891372817666747}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:29:05,899][0m Trial 19 finished with value: 0.049770495353366445 and parameters: {'observation_period_num': 28, 'train_rates': 0.815640515308326, 'learning_rate': 0.0004062331891747506, 'batch_size': 109, 'step_size': 13, 'gamma': 0.9851452748664289}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:29:31,615][0m Trial 20 finished with value: 0.0469446134354387 and parameters: {'observation_period_num': 5, 'train_rates': 0.7582993886397064, 'learning_rate': 0.0003496995776713963, 'batch_size': 204, 'step_size': 13, 'gamma': 0.9549280219091113}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:29:59,795][0m Trial 21 finished with value: 0.04787644429013558 and parameters: {'observation_period_num': 7, 'train_rates': 0.7504240124860643, 'learning_rate': 0.0003296839703492047, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9595701117264294}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:30:37,944][0m Trial 22 finished with value: 0.04141314072029897 and parameters: {'observation_period_num': 6, 'train_rates': 0.8340006683908836, 'learning_rate': 0.0003215713518459519, 'batch_size': 152, 'step_size': 13, 'gamma': 0.9595043233442561}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:31:15,468][0m Trial 23 finished with value: 0.07427245349519783 and parameters: {'observation_period_num': 23, 'train_rates': 0.8469448886493788, 'learning_rate': 0.00024962564015355376, 'batch_size': 155, 'step_size': 11, 'gamma': 0.9695993911835067}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:31:51,585][0m Trial 24 finished with value: 0.06531239524483681 and parameters: {'observation_period_num': 62, 'train_rates': 0.8483550782746214, 'learning_rate': 0.0005720998235948963, 'batch_size': 154, 'step_size': 14, 'gamma': 0.9383080202871278}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:32:37,774][0m Trial 25 finished with value: 0.14509077072143556 and parameters: {'observation_period_num': 159, 'train_rates': 0.7911109276198286, 'learning_rate': 7.646836920343788e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9717095677937875}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:33:14,160][0m Trial 26 finished with value: 0.049559763308993 and parameters: {'observation_period_num': 26, 'train_rates': 0.8727710273397219, 'learning_rate': 0.00024243378666178838, 'batch_size': 160, 'step_size': 12, 'gamma': 0.865135232510526}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:34:47,232][0m Trial 27 finished with value: 0.07179923403441778 and parameters: {'observation_period_num': 104, 'train_rates': 0.836520731820962, 'learning_rate': 6.462611322983953e-05, 'batch_size': 56, 'step_size': 14, 'gamma': 0.7519242901490838}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:35:38,393][0m Trial 28 finished with value: 0.09210823220966008 and parameters: {'observation_period_num': 43, 'train_rates': 0.7078718714972205, 'learning_rate': 0.0005055633591030981, 'batch_size': 94, 'step_size': 12, 'gamma': 0.9469299550243379}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:36:16,633][0m Trial 29 finished with value: 0.17671016871292536 and parameters: {'observation_period_num': 154, 'train_rates': 0.7330608622532162, 'learning_rate': 0.0009857986753632403, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9207997087456492}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:37:15,527][0m Trial 30 finished with value: 0.5403424934907393 and parameters: {'observation_period_num': 19, 'train_rates': 0.8204421844542005, 'learning_rate': 1.0390823659363265e-06, 'batch_size': 96, 'step_size': 10, 'gamma': 0.9738470226120673}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:37:43,392][0m Trial 31 finished with value: 0.07711836513185846 and parameters: {'observation_period_num': 8, 'train_rates': 0.766201660210549, 'learning_rate': 0.00026849981672582445, 'batch_size': 203, 'step_size': 13, 'gamma': 0.9515354776505381}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:38:08,403][0m Trial 32 finished with value: 0.05127819482063733 and parameters: {'observation_period_num': 5, 'train_rates': 0.7018024920465185, 'learning_rate': 0.0004481308780789832, 'batch_size': 218, 'step_size': 14, 'gamma': 0.9871804772185073}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:38:32,279][0m Trial 33 finished with value: 0.09999873685209375 and parameters: {'observation_period_num': 38, 'train_rates': 0.7788663899061832, 'learning_rate': 0.00022674467956597448, 'batch_size': 248, 'step_size': 13, 'gamma': 0.958905774481542}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:39:02,328][0m Trial 34 finished with value: 0.14273549979214842 and parameters: {'observation_period_num': 59, 'train_rates': 0.8003227935577638, 'learning_rate': 1.5297616226571455e-05, 'batch_size': 190, 'step_size': 14, 'gamma': 0.9111006964698646}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:39:28,692][0m Trial 35 finished with value: 0.06740009026759063 and parameters: {'observation_period_num': 18, 'train_rates': 0.7542049212094732, 'learning_rate': 0.00010899580065918616, 'batch_size': 209, 'step_size': 9, 'gamma': 0.9398022331575573}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:40:15,076][0m Trial 36 finished with value: 0.07875945596662286 and parameters: {'observation_period_num': 46, 'train_rates': 0.8747715160629049, 'learning_rate': 0.0006197203675497456, 'batch_size': 122, 'step_size': 12, 'gamma': 0.9895101173409104}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:40:49,541][0m Trial 37 finished with value: 0.06290973590579099 and parameters: {'observation_period_num': 71, 'train_rates': 0.8265341204704183, 'learning_rate': 0.0001948990573037589, 'batch_size': 164, 'step_size': 15, 'gamma': 0.973018488545725}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:41:24,527][0m Trial 38 finished with value: 0.0655307879920598 and parameters: {'observation_period_num': 33, 'train_rates': 0.7316668538387744, 'learning_rate': 0.0004086085732464895, 'batch_size': 143, 'step_size': 11, 'gamma': 0.9577006714374252}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:41:52,320][0m Trial 39 finished with value: 0.05887740952147928 and parameters: {'observation_period_num': 18, 'train_rates': 0.8854089241418989, 'learning_rate': 9.691653881522148e-05, 'batch_size': 226, 'step_size': 9, 'gamma': 0.8464126147603369}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:44:00,701][0m Trial 40 finished with value: 0.07101133169882605 and parameters: {'observation_period_num': 44, 'train_rates': 0.8566056495625665, 'learning_rate': 0.00035935201599911014, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9425526864436057}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:44:29,503][0m Trial 41 finished with value: 0.04728307010738421 and parameters: {'observation_period_num': 5, 'train_rates': 0.7572880450952724, 'learning_rate': 0.0003284642159964249, 'batch_size': 191, 'step_size': 13, 'gamma': 0.9622918680075945}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:44:56,795][0m Trial 42 finished with value: 0.11863591745495797 and parameters: {'observation_period_num': 15, 'train_rates': 0.6873968471834786, 'learning_rate': 0.0007483965326127248, 'batch_size': 184, 'step_size': 13, 'gamma': 0.9670088936154694}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:45:22,682][0m Trial 43 finished with value: 0.05847255597504321 and parameters: {'observation_period_num': 34, 'train_rates': 0.7769128212345566, 'learning_rate': 0.00030644235099818476, 'batch_size': 218, 'step_size': 12, 'gamma': 0.9788462069511091}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:45:55,860][0m Trial 44 finished with value: 0.04729451758317539 and parameters: {'observation_period_num': 5, 'train_rates': 0.7945055625649897, 'learning_rate': 0.00015984061471918236, 'batch_size': 172, 'step_size': 15, 'gamma': 0.9107016619793888}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:46:28,794][0m Trial 45 finished with value: 0.24378769990122823 and parameters: {'observation_period_num': 223, 'train_rates': 0.7320432726326536, 'learning_rate': 5.40575986274197e-05, 'batch_size': 147, 'step_size': 11, 'gamma': 0.963047396440452}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:46:58,443][0m Trial 46 finished with value: 0.05648874858781097 and parameters: {'observation_period_num': 28, 'train_rates': 0.804762339240865, 'learning_rate': 0.000855354968266787, 'batch_size': 193, 'step_size': 13, 'gamma': 0.8016774401246768}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:51:02,334][0m Trial 47 finished with value: 0.2740519460874281 and parameters: {'observation_period_num': 178, 'train_rates': 0.7546031728397719, 'learning_rate': 0.00019233150485238146, 'batch_size': 19, 'step_size': 10, 'gamma': 0.8826335164057346}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:51:47,963][0m Trial 48 finished with value: 0.040063510682036285 and parameters: {'observation_period_num': 17, 'train_rates': 0.8282267185205168, 'learning_rate': 0.0006056428458630935, 'batch_size': 123, 'step_size': 2, 'gamma': 0.9329691752915065}. Best is trial 18 with value: 0.035109210669994353.[0m
[32m[I 2025-02-04 02:52:32,205][0m Trial 49 finished with value: 0.07398641707699753 and parameters: {'observation_period_num': 73, 'train_rates': 0.8349375334624505, 'learning_rate': 0.0006620084709077584, 'batch_size': 121, 'step_size': 1, 'gamma': 0.9201208205349412}. Best is trial 18 with value: 0.035109210669994353.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5359 | 0.2158
Epoch 2/300, Loss: 0.1403 | 0.1527
Epoch 3/300, Loss: 0.1153 | 0.1236
Epoch 4/300, Loss: 0.1009 | 0.0991
Epoch 5/300, Loss: 0.0938 | 0.0882
Epoch 6/300, Loss: 0.0956 | 0.0888
Epoch 7/300, Loss: 0.0894 | 0.0920
Epoch 8/300, Loss: 0.0868 | 0.1086
Epoch 9/300, Loss: 0.0868 | 0.0962
Epoch 10/300, Loss: 0.0760 | 0.0845
Epoch 11/300, Loss: 0.0763 | 0.0932
Epoch 12/300, Loss: 0.0724 | 0.0825
Epoch 13/300, Loss: 0.0660 | 0.0770
Epoch 14/300, Loss: 0.0668 | 0.0795
Epoch 15/300, Loss: 0.0644 | 0.0727
Epoch 16/300, Loss: 0.0600 | 0.0691
Epoch 17/300, Loss: 0.0587 | 0.0675
Epoch 18/300, Loss: 0.0557 | 0.0624
Epoch 19/300, Loss: 0.0521 | 0.0583
Epoch 20/300, Loss: 0.0484 | 0.0546
Epoch 21/300, Loss: 0.0485 | 0.0674
Epoch 22/300, Loss: 0.0539 | 0.0666
Epoch 23/300, Loss: 0.0507 | 0.0544
Epoch 24/300, Loss: 0.0562 | 0.0583
Epoch 25/300, Loss: 0.0479 | 0.0523
Epoch 26/300, Loss: 0.0443 | 0.0517
Epoch 27/300, Loss: 0.0452 | 0.0523
Epoch 28/300, Loss: 0.0499 | 0.0616
Epoch 29/300, Loss: 0.0546 | 0.0588
Epoch 30/300, Loss: 0.0557 | 0.0567
Epoch 31/300, Loss: 0.0491 | 0.0511
Epoch 32/300, Loss: 0.0459 | 0.0524
Epoch 33/300, Loss: 0.0525 | 0.0629
Epoch 34/300, Loss: 0.0474 | 0.0518
Epoch 35/300, Loss: 0.0427 | 0.0470
Epoch 36/300, Loss: 0.0424 | 0.0531
Epoch 37/300, Loss: 0.0462 | 0.0566
Epoch 38/300, Loss: 0.0411 | 0.0465
Epoch 39/300, Loss: 0.0425 | 0.0483
Epoch 40/300, Loss: 0.0475 | 0.0468
Epoch 41/300, Loss: 0.0409 | 0.0488
Epoch 42/300, Loss: 0.0460 | 0.0628
Epoch 43/300, Loss: 0.0511 | 0.0578
Epoch 44/300, Loss: 0.0434 | 0.0443
Epoch 45/300, Loss: 0.0405 | 0.0472
Epoch 46/300, Loss: 0.0410 | 0.0494
Epoch 47/300, Loss: 0.0439 | 0.0550
Epoch 48/300, Loss: 0.0454 | 0.0491
Epoch 49/300, Loss: 0.0452 | 0.0480
Epoch 50/300, Loss: 0.0447 | 0.0505
Epoch 51/300, Loss: 0.0467 | 0.0585
Epoch 52/300, Loss: 0.0476 | 0.0495
Epoch 53/300, Loss: 0.0443 | 0.0454
Epoch 54/300, Loss: 0.0413 | 0.0480
Epoch 55/300, Loss: 0.0518 | 0.0616
Epoch 56/300, Loss: 0.0495 | 0.0490
Epoch 57/300, Loss: 0.0398 | 0.0463
Epoch 58/300, Loss: 0.0367 | 0.0460
Epoch 59/300, Loss: 0.0377 | 0.0407
Epoch 60/300, Loss: 0.0317 | 0.0394
Epoch 61/300, Loss: 0.0317 | 0.0409
Epoch 62/300, Loss: 0.0331 | 0.0386
Epoch 63/300, Loss: 0.0290 | 0.0370
Epoch 64/300, Loss: 0.0301 | 0.0401
Epoch 65/300, Loss: 0.0322 | 0.0391
Epoch 66/300, Loss: 0.0299 | 0.0381
Epoch 67/300, Loss: 0.0306 | 0.0421
Epoch 68/300, Loss: 0.0326 | 0.0403
Epoch 69/300, Loss: 0.0297 | 0.0371
Epoch 70/300, Loss: 0.0294 | 0.0390
Epoch 71/300, Loss: 0.0316 | 0.0390
Epoch 72/300, Loss: 0.0282 | 0.0365
Epoch 73/300, Loss: 0.0288 | 0.0400
Epoch 74/300, Loss: 0.0317 | 0.0394
Epoch 75/300, Loss: 0.0293 | 0.0365
Epoch 76/300, Loss: 0.0285 | 0.0392
Epoch 77/300, Loss: 0.0311 | 0.0371
Epoch 78/300, Loss: 0.0270 | 0.0351
Epoch 79/300, Loss: 0.0274 | 0.0387
Epoch 80/300, Loss: 0.0292 | 0.0370
Epoch 81/300, Loss: 0.0272 | 0.0349
Epoch 82/300, Loss: 0.0277 | 0.0399
Epoch 83/300, Loss: 0.0305 | 0.0385
Epoch 84/300, Loss: 0.0283 | 0.0360
Epoch 85/300, Loss: 0.0290 | 0.0431
Epoch 86/300, Loss: 0.0337 | 0.0370
Epoch 87/300, Loss: 0.0290 | 0.0379
Epoch 88/300, Loss: 0.0283 | 0.0435
Epoch 89/300, Loss: 0.0341 | 0.0389
Epoch 90/300, Loss: 0.0290 | 0.0364
Epoch 91/300, Loss: 0.0303 | 0.0426
Epoch 92/300, Loss: 0.0330 | 0.0380
Epoch 93/300, Loss: 0.0269 | 0.0363
Epoch 94/300, Loss: 0.0279 | 0.0390
Epoch 95/300, Loss: 0.0299 | 0.0375
Epoch 96/300, Loss: 0.0260 | 0.0342
Epoch 97/300, Loss: 0.0245 | 0.0356
Epoch 98/300, Loss: 0.0246 | 0.0334
Epoch 99/300, Loss: 0.0234 | 0.0336
Epoch 100/300, Loss: 0.0241 | 0.0348
Epoch 101/300, Loss: 0.0252 | 0.0345
Epoch 102/300, Loss: 0.0223 | 0.0342
Epoch 103/300, Loss: 0.0224 | 0.0356
Epoch 104/300, Loss: 0.0242 | 0.0344
Epoch 105/300, Loss: 0.0215 | 0.0341
Epoch 106/300, Loss: 0.0217 | 0.0365
Epoch 107/300, Loss: 0.0239 | 0.0352
Epoch 108/300, Loss: 0.0211 | 0.0348
Epoch 109/300, Loss: 0.0213 | 0.0357
Epoch 110/300, Loss: 0.0233 | 0.0342
Epoch 111/300, Loss: 0.0205 | 0.0344
Epoch 112/300, Loss: 0.0206 | 0.0357
Epoch 113/300, Loss: 0.0223 | 0.0341
Epoch 114/300, Loss: 0.0199 | 0.0344
Epoch 115/300, Loss: 0.0200 | 0.0355
Epoch 116/300, Loss: 0.0221 | 0.0348
Epoch 117/300, Loss: 0.0192 | 0.0341
Epoch 118/300, Loss: 0.0195 | 0.0364
Epoch 119/300, Loss: 0.0213 | 0.0351
Epoch 120/300, Loss: 0.0190 | 0.0346
Epoch 121/300, Loss: 0.0193 | 0.0359
Epoch 122/300, Loss: 0.0204 | 0.0349
Epoch 123/300, Loss: 0.0185 | 0.0348
Epoch 124/300, Loss: 0.0190 | 0.0356
Epoch 125/300, Loss: 0.0202 | 0.0351
Epoch 126/300, Loss: 0.0185 | 0.0355
Epoch 127/300, Loss: 0.0192 | 0.0367
Epoch 128/300, Loss: 0.0202 | 0.0378
Epoch 129/300, Loss: 0.0196 | 0.0353
Epoch 130/300, Loss: 0.0196 | 0.0362
Epoch 131/300, Loss: 0.0211 | 0.0387
Epoch 132/300, Loss: 0.0237 | 0.0384
Epoch 133/300, Loss: 0.0233 | 0.0382
Epoch 134/300, Loss: 0.0250 | 0.0388
Epoch 135/300, Loss: 0.0257 | 0.0407
Epoch 136/300, Loss: 0.0255 | 0.0494
Epoch 137/300, Loss: 0.0327 | 0.0435
Epoch 138/300, Loss: 0.0264 | 0.0421
Epoch 139/300, Loss: 0.0229 | 0.0389
Epoch 140/300, Loss: 0.0275 | 0.0423
Epoch 141/300, Loss: 0.0271 | 0.0396
Epoch 142/300, Loss: 0.0191 | 0.0359
Epoch 143/300, Loss: 0.0198 | 0.0371
Epoch 144/300, Loss: 0.0213 | 0.0366
Epoch 145/300, Loss: 0.0174 | 0.0350
Epoch 146/300, Loss: 0.0171 | 0.0355
Epoch 147/300, Loss: 0.0173 | 0.0356
Epoch 148/300, Loss: 0.0168 | 0.0359
Epoch 149/300, Loss: 0.0162 | 0.0356
Epoch 150/300, Loss: 0.0164 | 0.0366
Epoch 151/300, Loss: 0.0166 | 0.0365
Epoch 152/300, Loss: 0.0165 | 0.0364
Epoch 153/300, Loss: 0.0158 | 0.0365
Epoch 154/300, Loss: 0.0154 | 0.0376
Epoch 155/300, Loss: 0.0155 | 0.0366
Epoch 156/300, Loss: 0.0160 | 0.0377
Epoch 157/300, Loss: 0.0156 | 0.0366
Epoch 158/300, Loss: 0.0157 | 0.0378
Epoch 159/300, Loss: 0.0160 | 0.0381
Epoch 160/300, Loss: 0.0149 | 0.0373
Epoch 161/300, Loss: 0.0154 | 0.0382
Epoch 162/300, Loss: 0.0151 | 0.0378
Epoch 163/300, Loss: 0.0148 | 0.0391
Epoch 164/300, Loss: 0.0159 | 0.0386
Epoch 165/300, Loss: 0.0159 | 0.0389
Epoch 166/300, Loss: 0.0152 | 0.0385
Epoch 167/300, Loss: 0.0155 | 0.0370
Epoch 168/300, Loss: 0.0141 | 0.0379
Epoch 169/300, Loss: 0.0141 | 0.0376
Epoch 170/300, Loss: 0.0139 | 0.0378
Epoch 171/300, Loss: 0.0129 | 0.0380
Epoch 172/300, Loss: 0.0127 | 0.0381
Epoch 173/300, Loss: 0.0132 | 0.0378
Epoch 174/300, Loss: 0.0128 | 0.0386
Epoch 175/300, Loss: 0.0124 | 0.0386
Epoch 176/300, Loss: 0.0129 | 0.0393
Epoch 177/300, Loss: 0.0129 | 0.0386
Epoch 178/300, Loss: 0.0125 | 0.0394
Epoch 179/300, Loss: 0.0129 | 0.0403
Epoch 180/300, Loss: 0.0141 | 0.0407
Epoch 181/300, Loss: 0.0150 | 0.0417
Epoch 182/300, Loss: 0.0144 | 0.0429
Epoch 183/300, Loss: 0.0156 | 0.0419
Epoch 184/300, Loss: 0.0170 | 0.0432
Epoch 185/300, Loss: 0.0169 | 0.0409
Epoch 186/300, Loss: 0.0156 | 0.0423
Epoch 187/300, Loss: 0.0154 | 0.0415
Epoch 188/300, Loss: 0.0158 | 0.0414
Epoch 189/300, Loss: 0.0149 | 0.0408
Epoch 190/300, Loss: 0.0140 | 0.0426
Epoch 191/300, Loss: 0.0153 | 0.0432
Epoch 192/300, Loss: 0.0166 | 0.0394
Epoch 193/300, Loss: 0.0133 | 0.0418
Epoch 194/300, Loss: 0.0141 | 0.0430
Epoch 195/300, Loss: 0.0152 | 0.0415
Epoch 196/300, Loss: 0.0134 | 0.0407
Epoch 197/300, Loss: 0.0131 | 0.0405
Epoch 198/300, Loss: 0.0133 | 0.0418
Epoch 199/300, Loss: 0.0118 | 0.0404
Epoch 200/300, Loss: 0.0113 | 0.0413
Epoch 201/300, Loss: 0.0115 | 0.0417
Epoch 202/300, Loss: 0.0116 | 0.0411
Epoch 203/300, Loss: 0.0107 | 0.0423
Epoch 204/300, Loss: 0.0104 | 0.0401
Epoch 205/300, Loss: 0.0101 | 0.0407
Epoch 206/300, Loss: 0.0097 | 0.0402
Epoch 207/300, Loss: 0.0095 | 0.0412
Epoch 208/300, Loss: 0.0094 | 0.0411
Epoch 209/300, Loss: 0.0094 | 0.0414
Epoch 210/300, Loss: 0.0097 | 0.0416
Epoch 211/300, Loss: 0.0102 | 0.0412
Epoch 212/300, Loss: 0.0105 | 0.0420
Epoch 213/300, Loss: 0.0104 | 0.0439
Epoch 214/300, Loss: 0.0111 | 0.0430
Epoch 215/300, Loss: 0.0108 | 0.0424
Epoch 216/300, Loss: 0.0107 | 0.0449
Epoch 217/300, Loss: 0.0115 | 0.0447
Epoch 218/300, Loss: 0.0115 | 0.0435
Epoch 219/300, Loss: 0.0103 | 0.0416
Epoch 220/300, Loss: 0.0105 | 0.0434
Epoch 221/300, Loss: 0.0099 | 0.0419
Epoch 222/300, Loss: 0.0103 | 0.0458
Epoch 223/300, Loss: 0.0112 | 0.0424
Epoch 224/300, Loss: 0.0101 | 0.0434
Epoch 225/300, Loss: 0.0094 | 0.0438
Epoch 226/300, Loss: 0.0085 | 0.0411
Epoch 227/300, Loss: 0.0083 | 0.0428
Epoch 228/300, Loss: 0.0079 | 0.0425
Epoch 229/300, Loss: 0.0077 | 0.0429
Epoch 230/300, Loss: 0.0079 | 0.0425
Epoch 231/300, Loss: 0.0076 | 0.0434
Epoch 232/300, Loss: 0.0078 | 0.0446
Epoch 233/300, Loss: 0.0080 | 0.0442
Epoch 234/300, Loss: 0.0081 | 0.0449
Epoch 235/300, Loss: 0.0080 | 0.0437
Epoch 236/300, Loss: 0.0078 | 0.0425
Epoch 237/300, Loss: 0.0075 | 0.0429
Epoch 238/300, Loss: 0.0073 | 0.0433
Epoch 239/300, Loss: 0.0075 | 0.0435
Epoch 240/300, Loss: 0.0076 | 0.0440
Epoch 241/300, Loss: 0.0076 | 0.0447
Epoch 242/300, Loss: 0.0080 | 0.0442
Epoch 243/300, Loss: 0.0087 | 0.0436
Epoch 244/300, Loss: 0.0088 | 0.0441
Epoch 245/300, Loss: 0.0084 | 0.0434
Epoch 246/300, Loss: 0.0092 | 0.0458
Epoch 247/300, Loss: 0.0116 | 0.0438
Epoch 248/300, Loss: 0.0106 | 0.0454
Epoch 249/300, Loss: 0.0096 | 0.0428
Epoch 250/300, Loss: 0.0118 | 0.0490
Epoch 251/300, Loss: 0.0139 | 0.0475
Epoch 252/300, Loss: 0.0123 | 0.0466
Epoch 253/300, Loss: 0.0098 | 0.0429
Epoch 254/300, Loss: 0.0097 | 0.0436
Epoch 255/300, Loss: 0.0090 | 0.0436
Epoch 256/300, Loss: 0.0078 | 0.0445
Epoch 257/300, Loss: 0.0081 | 0.0444
Epoch 258/300, Loss: 0.0078 | 0.0440
Epoch 259/300, Loss: 0.0072 | 0.0418
Epoch 260/300, Loss: 0.0069 | 0.0424
Epoch 261/300, Loss: 0.0073 | 0.0435
Epoch 262/300, Loss: 0.0070 | 0.0444
Epoch 263/300, Loss: 0.0072 | 0.0454
Epoch 264/300, Loss: 0.0070 | 0.0434
Epoch 265/300, Loss: 0.0065 | 0.0422
Epoch 266/300, Loss: 0.0067 | 0.0434
Epoch 267/300, Loss: 0.0070 | 0.0441
Epoch 268/300, Loss: 0.0068 | 0.0442
Epoch 269/300, Loss: 0.0071 | 0.0440
Epoch 270/300, Loss: 0.0064 | 0.0430
Epoch 271/300, Loss: 0.0065 | 0.0431
Epoch 272/300, Loss: 0.0064 | 0.0443
Epoch 273/300, Loss: 0.0069 | 0.0452
Epoch 274/300, Loss: 0.0077 | 0.0454
Epoch 275/300, Loss: 0.0079 | 0.0443
Epoch 276/300, Loss: 0.0084 | 0.0442
Epoch 277/300, Loss: 0.0078 | 0.0433
Epoch 278/300, Loss: 0.0078 | 0.0427
Epoch 279/300, Loss: 0.0075 | 0.0442
Epoch 280/300, Loss: 0.0081 | 0.0444
Epoch 281/300, Loss: 0.0083 | 0.0436
Epoch 282/300, Loss: 0.0075 | 0.0429
Epoch 283/300, Loss: 0.0075 | 0.0433
Epoch 284/300, Loss: 0.0072 | 0.0442
Epoch 285/300, Loss: 0.0088 | 0.0441
Epoch 286/300, Loss: 0.0074 | 0.0441
Epoch 287/300, Loss: 0.0085 | 0.0464
Epoch 288/300, Loss: 0.0091 | 0.0475
Epoch 289/300, Loss: 0.0090 | 0.0442
Epoch 290/300, Loss: 0.0079 | 0.0437
Epoch 291/300, Loss: 0.0082 | 0.0446
Epoch 292/300, Loss: 0.0083 | 0.0452
Epoch 293/300, Loss: 0.0069 | 0.0433
Epoch 294/300, Loss: 0.0071 | 0.0440
Epoch 295/300, Loss: 0.0067 | 0.0433
Epoch 296/300, Loss: 0.0068 | 0.0429
Epoch 297/300, Loss: 0.0065 | 0.0428
Epoch 298/300, Loss: 0.0061 | 0.0432
Epoch 299/300, Loss: 0.0064 | 0.0426
Epoch 300/300, Loss: 0.0069 | 0.0438
Runtime (seconds): 156.904531955719
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.7288015224039555
RMSE: 0.85369873046875
MAE: 0.85369873046875
R-squared: nan
[113.9263]
