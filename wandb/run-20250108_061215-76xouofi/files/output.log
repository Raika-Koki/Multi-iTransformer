[32m[I 2025-01-08 06:12:16,104][0m A new study created in memory with name: no-name-efc57bde-29de-4fb7-b980-922c455217ed[0m
[32m[I 2025-01-08 06:19:07,582][0m Trial 0 finished with value: 0.12012771220461836 and parameters: {'observation_period_num': 234, 'train_rates': 0.9624630068160629, 'learning_rate': 2.874037543754271e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9131328474062275}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:19:39,496][0m Trial 1 finished with value: 0.6815286874771118 and parameters: {'observation_period_num': 21, 'train_rates': 0.9444833062248466, 'learning_rate': 7.116900339347101e-06, 'batch_size': 236, 'step_size': 14, 'gamma': 0.8042896711243684}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:24:01,340][0m Trial 2 finished with value: 0.8984644645590556 and parameters: {'observation_period_num': 222, 'train_rates': 0.6390242892841339, 'learning_rate': 0.0003127997887982386, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9030760374556308}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:28:00,260][0m Trial 3 finished with value: 1.3610294326659171 and parameters: {'observation_period_num': 167, 'train_rates': 0.9227017932047753, 'learning_rate': 2.3928643202366506e-06, 'batch_size': 189, 'step_size': 7, 'gamma': 0.7730902508313804}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:32:17,001][0m Trial 4 finished with value: 0.24351894855499268 and parameters: {'observation_period_num': 174, 'train_rates': 0.9454550198277698, 'learning_rate': 8.672651841873797e-05, 'batch_size': 255, 'step_size': 7, 'gamma': 0.9655493716606297}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:33:45,544][0m Trial 5 finished with value: 0.4169621294712796 and parameters: {'observation_period_num': 76, 'train_rates': 0.7435213982377453, 'learning_rate': 0.00017065762087576156, 'batch_size': 213, 'step_size': 7, 'gamma': 0.8409203665667151}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:37:21,904][0m Trial 6 finished with value: 1.0809608697891235 and parameters: {'observation_period_num': 150, 'train_rates': 0.9501922129949698, 'learning_rate': 2.626188462634217e-06, 'batch_size': 233, 'step_size': 15, 'gamma': 0.8449398031440174}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:42:38,440][0m Trial 7 finished with value: 0.37791390974718825 and parameters: {'observation_period_num': 203, 'train_rates': 0.9521574003159018, 'learning_rate': 6.866129123856306e-05, 'batch_size': 109, 'step_size': 3, 'gamma': 0.7867525607862252}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:43:46,012][0m Trial 8 finished with value: 0.8406826905813234 and parameters: {'observation_period_num': 55, 'train_rates': 0.8076250790012487, 'learning_rate': 6.849409571919403e-06, 'batch_size': 140, 'step_size': 8, 'gamma': 0.7680455235098856}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:48:00,611][0m Trial 9 finished with value: 0.6477422645049435 and parameters: {'observation_period_num': 197, 'train_rates': 0.7375830634959946, 'learning_rate': 3.2411046377523e-05, 'batch_size': 221, 'step_size': 2, 'gamma': 0.9518412178437964}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:54:25,252][0m Trial 10 finished with value: 2.317296822865804 and parameters: {'observation_period_num': 245, 'train_rates': 0.8548252228520088, 'learning_rate': 0.0006934272455807144, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9085461054710363}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 06:57:25,814][0m Trial 11 finished with value: 0.20775352970238034 and parameters: {'observation_period_num': 118, 'train_rates': 0.8786855789729537, 'learning_rate': 6.674304610041614e-05, 'batch_size': 25, 'step_size': 11, 'gamma': 0.9756811256005185}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:01:02,039][0m Trial 12 finished with value: 0.19379598801161585 and parameters: {'observation_period_num': 113, 'train_rates': 0.8696258261794193, 'learning_rate': 1.789384290724543e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9282937676417796}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:03:32,153][0m Trial 13 finished with value: 0.26322148470153583 and parameters: {'observation_period_num': 110, 'train_rates': 0.880720608567277, 'learning_rate': 9.84600285553364e-06, 'batch_size': 68, 'step_size': 11, 'gamma': 0.9121340357284449}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:05:37,295][0m Trial 14 finished with value: 0.3837437610799337 and parameters: {'observation_period_num': 96, 'train_rates': 0.810805694287336, 'learning_rate': 1.802736653257593e-05, 'batch_size': 57, 'step_size': 9, 'gamma': 0.8777579947063848}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:08:55,804][0m Trial 15 finished with value: 0.21554450469441933 and parameters: {'observation_period_num': 139, 'train_rates': 0.8931650319447871, 'learning_rate': 2.5045451960346435e-05, 'batch_size': 66, 'step_size': 4, 'gamma': 0.943550536779951}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:12:24,029][0m Trial 16 finished with value: 1.3154749592039883 and parameters: {'observation_period_num': 45, 'train_rates': 0.7425672624734372, 'learning_rate': 1.1807843265722693e-06, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9309830096352294}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:19:07,838][0m Trial 17 finished with value: 0.2845402956008911 and parameters: {'observation_period_num': 249, 'train_rates': 0.9802047142220676, 'learning_rate': 1.2947288323217077e-05, 'batch_size': 90, 'step_size': 5, 'gamma': 0.8761958161261717}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:21:07,256][0m Trial 18 finished with value: 0.20834879577159882 and parameters: {'observation_period_num': 85, 'train_rates': 0.9896048029987227, 'learning_rate': 4.237569226600355e-05, 'batch_size': 164, 'step_size': 10, 'gamma': 0.8433580727396127}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:24:26,697][0m Trial 19 finished with value: 1.1586145177964242 and parameters: {'observation_period_num': 173, 'train_rates': 0.6123546718043749, 'learning_rate': 3.733858742270046e-06, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9857551150237903}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:27:30,009][0m Trial 20 finished with value: 0.256571815598129 and parameters: {'observation_period_num': 133, 'train_rates': 0.8576883381755571, 'learning_rate': 0.0001452915139074361, 'batch_size': 92, 'step_size': 5, 'gamma': 0.9321476605541822}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:30:20,514][0m Trial 21 finished with value: 0.271984528686501 and parameters: {'observation_period_num': 120, 'train_rates': 0.8359491320211889, 'learning_rate': 5.642200888085082e-05, 'batch_size': 34, 'step_size': 13, 'gamma': 0.9888521321483345}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:34:12,685][0m Trial 22 finished with value: 0.15132271693641408 and parameters: {'observation_period_num': 111, 'train_rates': 0.9054292252329881, 'learning_rate': 2.4048261402798307e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9617508259240795}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:35:47,769][0m Trial 23 finished with value: 0.18139736541995297 and parameters: {'observation_period_num': 63, 'train_rates': 0.9074391086732028, 'learning_rate': 2.1479347833813458e-05, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8938277004675282}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:37:23,775][0m Trial 24 finished with value: 0.18666049674516771 and parameters: {'observation_period_num': 61, 'train_rates': 0.9155914153175527, 'learning_rate': 2.836473487483492e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.8861232212720316}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:38:14,345][0m Trial 25 finished with value: 0.33934951052342577 and parameters: {'observation_period_num': 16, 'train_rates': 0.9077138117299841, 'learning_rate': 5.459815923488925e-06, 'batch_size': 79, 'step_size': 6, 'gamma': 0.9613687428400053}. Best is trial 0 with value: 0.12012771220461836.[0m
[32m[I 2025-01-08 07:39:53,779][0m Trial 26 finished with value: 0.11167846480384469 and parameters: {'observation_period_num': 45, 'train_rates': 0.9784269246744445, 'learning_rate': 0.00012299669256938503, 'batch_size': 44, 'step_size': 9, 'gamma': 0.89728627583891}. Best is trial 26 with value: 0.11167846480384469.[0m
[32m[I 2025-01-08 07:41:39,109][0m Trial 27 finished with value: 0.06686030668408974 and parameters: {'observation_period_num': 33, 'train_rates': 0.9768335238817017, 'learning_rate': 0.0001261586387860798, 'batch_size': 39, 'step_size': 8, 'gamma': 0.917416535508745}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:42:31,766][0m Trial 28 finished with value: 0.1312311440706253 and parameters: {'observation_period_num': 34, 'train_rates': 0.9711677328711796, 'learning_rate': 0.0003512653455659929, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8623463215111782}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:44:17,000][0m Trial 29 finished with value: 0.10918146800994873 and parameters: {'observation_period_num': 9, 'train_rates': 0.9577990890241962, 'learning_rate': 0.00012845120998530084, 'batch_size': 39, 'step_size': 6, 'gamma': 0.8604938892759502}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:44:55,583][0m Trial 30 finished with value: 0.12191762030124664 and parameters: {'observation_period_num': 6, 'train_rates': 0.9888611783004876, 'learning_rate': 0.0001420686114282976, 'batch_size': 114, 'step_size': 6, 'gamma': 0.8083669135448883}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:46:38,687][0m Trial 31 finished with value: 0.1747860128680865 and parameters: {'observation_period_num': 28, 'train_rates': 0.9543426052544751, 'learning_rate': 0.00011134341945080924, 'batch_size': 39, 'step_size': 4, 'gamma': 0.8600391557423065}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:47:36,461][0m Trial 32 finished with value: 0.13446588954418012 and parameters: {'observation_period_num': 6, 'train_rates': 0.9320369512080702, 'learning_rate': 0.00026580064130748385, 'batch_size': 70, 'step_size': 6, 'gamma': 0.9160512174832497}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:48:37,241][0m Trial 33 finished with value: 0.7064473358067599 and parameters: {'observation_period_num': 43, 'train_rates': 0.691942989133378, 'learning_rate': 0.00038129234751520816, 'batch_size': 56, 'step_size': 8, 'gamma': 0.8201887457976604}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:50:29,649][0m Trial 34 finished with value: 0.13475689508261218 and parameters: {'observation_period_num': 24, 'train_rates': 0.9580631432556519, 'learning_rate': 0.00021184040801632657, 'batch_size': 36, 'step_size': 4, 'gamma': 0.8906352756665395}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:52:13,056][0m Trial 35 finished with value: 0.8930659774710371 and parameters: {'observation_period_num': 76, 'train_rates': 0.9342685586462244, 'learning_rate': 0.0008174045399558311, 'batch_size': 142, 'step_size': 7, 'gamma': 0.9208178976647489}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:53:11,729][0m Trial 36 finished with value: 0.12798083287018996 and parameters: {'observation_period_num': 36, 'train_rates': 0.9648852370996057, 'learning_rate': 9.050407961896353e-05, 'batch_size': 80, 'step_size': 5, 'gamma': 0.904566420269763}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 07:59:08,901][0m Trial 37 finished with value: 0.17614265661584874 and parameters: {'observation_period_num': 223, 'train_rates': 0.9336026420049788, 'learning_rate': 4.629901753226152e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8588211887987713}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:03:56,805][0m Trial 38 finished with value: 0.19400377571582794 and parameters: {'observation_period_num': 190, 'train_rates': 0.971128261398911, 'learning_rate': 0.00010543257682731229, 'batch_size': 187, 'step_size': 3, 'gamma': 0.8962608632098543}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:05:08,000][0m Trial 39 finished with value: 0.6027059255396587 and parameters: {'observation_period_num': 49, 'train_rates': 0.6880691653588968, 'learning_rate': 0.00023697082793567892, 'batch_size': 50, 'step_size': 6, 'gamma': 0.8292350685062929}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:09:00,401][0m Trial 40 finished with value: 2.5364125029424605 and parameters: {'observation_period_num': 155, 'train_rates': 0.9368561973176873, 'learning_rate': 0.0006367341633195686, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9412441702212357}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:09:34,551][0m Trial 41 finished with value: 0.1499694585800171 and parameters: {'observation_period_num': 7, 'train_rates': 0.9773339962305242, 'learning_rate': 0.00012994272016271578, 'batch_size': 132, 'step_size': 6, 'gamma': 0.8091510280938723}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:10:15,553][0m Trial 42 finished with value: 0.16034922003746033 and parameters: {'observation_period_num': 16, 'train_rates': 0.983760559822733, 'learning_rate': 0.0004672985830235004, 'batch_size': 113, 'step_size': 9, 'gamma': 0.7847860032857439}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:10:49,829][0m Trial 43 finished with value: 0.15179312229156494 and parameters: {'observation_period_num': 20, 'train_rates': 0.9898965766362555, 'learning_rate': 0.00016786273839864056, 'batch_size': 160, 'step_size': 8, 'gamma': 0.7514791733571108}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:11:29,930][0m Trial 44 finished with value: 0.27868960097136086 and parameters: {'observation_period_num': 6, 'train_rates': 0.9493023866757223, 'learning_rate': 6.616643255426242e-05, 'batch_size': 116, 'step_size': 5, 'gamma': 0.7921065962487006}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:14:17,646][0m Trial 45 finished with value: 0.15938856322318315 and parameters: {'observation_period_num': 66, 'train_rates': 0.958725204884972, 'learning_rate': 3.558207931032381e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.8816772432505549}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:19:39,836][0m Trial 46 finished with value: 0.3709412661905821 and parameters: {'observation_period_num': 231, 'train_rates': 0.7845582092636112, 'learning_rate': 8.289763616725599e-05, 'batch_size': 78, 'step_size': 3, 'gamma': 0.9020563422890304}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:20:27,107][0m Trial 47 finished with value: 0.19436635076999664 and parameters: {'observation_period_num': 35, 'train_rates': 0.9271960134514037, 'learning_rate': 0.0005158995077224621, 'batch_size': 255, 'step_size': 7, 'gamma': 0.8343027628327473}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:22:48,443][0m Trial 48 finished with value: 0.19403075165687267 and parameters: {'observation_period_num': 94, 'train_rates': 0.8922425160756441, 'learning_rate': 0.00019134985767691191, 'batch_size': 39, 'step_size': 10, 'gamma': 0.8684015310044989}. Best is trial 27 with value: 0.06686030668408974.[0m
[32m[I 2025-01-08 08:28:41,867][0m Trial 49 finished with value: 0.16979176556241923 and parameters: {'observation_period_num': 209, 'train_rates': 0.947736244671069, 'learning_rate': 5.261357263469245e-05, 'batch_size': 22, 'step_size': 4, 'gamma': 0.9184925744440876}. Best is trial 27 with value: 0.06686030668408974.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.5595 | 0.7275
Epoch 2/300, Loss: 0.5301 | 0.5683
Epoch 3/300, Loss: 0.4307 | 0.4450
Epoch 4/300, Loss: 0.3349 | 0.4181
Epoch 5/300, Loss: 0.3071 | 0.4140
Epoch 6/300, Loss: 0.3535 | 0.4217
Epoch 7/300, Loss: 0.3859 | 0.5827
Epoch 8/300, Loss: 0.3630 | 0.4305
Epoch 9/300, Loss: 0.2897 | 0.3849
Epoch 10/300, Loss: 0.2559 | 0.3072
Epoch 11/300, Loss: 0.2186 | 0.2946
Epoch 12/300, Loss: 0.2158 | 0.2747
Epoch 13/300, Loss: 0.2171 | 0.2477
Epoch 14/300, Loss: 0.1948 | 0.2378
Epoch 15/300, Loss: 0.1798 | 0.2352
Epoch 16/300, Loss: 0.1739 | 0.2225
Epoch 17/300, Loss: 0.1787 | 0.2384
Epoch 18/300, Loss: 0.1722 | 0.2192
Epoch 19/300, Loss: 0.1693 | 0.2157
Epoch 20/300, Loss: 0.1680 | 0.2139
Epoch 21/300, Loss: 0.1743 | 0.2122
Epoch 22/300, Loss: 0.1714 | 0.1958
Epoch 23/300, Loss: 0.1702 | 0.2082
Epoch 24/300, Loss: 0.1760 | 0.2193
Epoch 25/300, Loss: 0.1650 | 0.1843
Epoch 26/300, Loss: 0.1579 | 0.1926
Epoch 27/300, Loss: 0.1560 | 0.1896
Epoch 28/300, Loss: 0.1451 | 0.1728
Epoch 29/300, Loss: 0.1413 | 0.1843
Epoch 30/300, Loss: 0.1328 | 0.1626
Epoch 31/300, Loss: 0.1276 | 0.1571
Epoch 32/300, Loss: 0.1282 | 0.1577
Epoch 33/300, Loss: 0.1265 | 0.1590
Epoch 34/300, Loss: 0.1253 | 0.1442
Epoch 35/300, Loss: 0.1310 | 0.1559
Epoch 36/300, Loss: 0.1291 | 0.1530
Epoch 37/300, Loss: 0.1243 | 0.1502
Epoch 38/300, Loss: 0.1218 | 0.1332
Epoch 39/300, Loss: 0.1271 | 0.1343
Epoch 40/300, Loss: 0.1231 | 0.1343
Epoch 41/300, Loss: 0.1185 | 0.1412
Epoch 42/300, Loss: 0.1150 | 0.1337
Epoch 43/300, Loss: 0.1149 | 0.1241
Epoch 44/300, Loss: 0.1182 | 0.1339
Epoch 45/300, Loss: 0.1171 | 0.1427
Epoch 46/300, Loss: 0.1118 | 0.1212
Epoch 47/300, Loss: 0.1130 | 0.1266
Epoch 48/300, Loss: 0.1119 | 0.1256
Epoch 49/300, Loss: 0.1102 | 0.1147
Epoch 50/300, Loss: 0.1081 | 0.1156
Epoch 51/300, Loss: 0.1071 | 0.1181
Epoch 52/300, Loss: 0.1038 | 0.1181
Epoch 53/300, Loss: 0.1043 | 0.1124
Epoch 54/300, Loss: 0.1031 | 0.1184
Epoch 55/300, Loss: 0.1008 | 0.1148
Epoch 56/300, Loss: 0.1027 | 0.1072
Epoch 57/300, Loss: 0.0996 | 0.1108
Epoch 58/300, Loss: 0.0966 | 0.1058
Epoch 59/300, Loss: 0.0956 | 0.1058
Epoch 60/300, Loss: 0.0941 | 0.1043
Epoch 61/300, Loss: 0.0945 | 0.1009
Epoch 62/300, Loss: 0.0935 | 0.1011
Epoch 63/300, Loss: 0.0936 | 0.1058
Epoch 64/300, Loss: 0.0947 | 0.1037
Epoch 65/300, Loss: 0.0944 | 0.0973
Epoch 66/300, Loss: 0.0922 | 0.1028
Epoch 67/300, Loss: 0.0933 | 0.1037
Epoch 68/300, Loss: 0.0913 | 0.0956
Epoch 69/300, Loss: 0.0902 | 0.0941
Epoch 70/300, Loss: 0.0897 | 0.0997
Epoch 71/300, Loss: 0.0882 | 0.1010
Epoch 72/300, Loss: 0.0888 | 0.0952
Epoch 73/300, Loss: 0.0873 | 0.0942
Epoch 74/300, Loss: 0.0863 | 0.0971
Epoch 75/300, Loss: 0.0848 | 0.0932
Epoch 76/300, Loss: 0.0844 | 0.0906
Epoch 77/300, Loss: 0.0849 | 0.0958
Epoch 78/300, Loss: 0.0818 | 0.0909
Epoch 79/300, Loss: 0.0811 | 0.0877
Epoch 80/300, Loss: 0.0806 | 0.0916
Epoch 81/300, Loss: 0.0816 | 0.0921
Epoch 82/300, Loss: 0.0799 | 0.0870
Epoch 83/300, Loss: 0.0780 | 0.0865
Epoch 84/300, Loss: 0.0775 | 0.0880
Epoch 85/300, Loss: 0.0761 | 0.0861
Epoch 86/300, Loss: 0.0760 | 0.0844
Epoch 87/300, Loss: 0.0758 | 0.0870
Epoch 88/300, Loss: 0.0747 | 0.0853
Epoch 89/300, Loss: 0.0749 | 0.0848
Epoch 90/300, Loss: 0.0745 | 0.0852
Epoch 91/300, Loss: 0.0742 | 0.0848
Epoch 92/300, Loss: 0.0736 | 0.0839
Epoch 93/300, Loss: 0.0740 | 0.0845
Epoch 94/300, Loss: 0.0724 | 0.0838
Epoch 95/300, Loss: 0.0717 | 0.0820
Epoch 96/300, Loss: 0.0717 | 0.0816
Epoch 97/300, Loss: 0.0715 | 0.0819
Epoch 98/300, Loss: 0.0714 | 0.0830
Epoch 99/300, Loss: 0.0712 | 0.0808
Epoch 100/300, Loss: 0.0705 | 0.0803
Epoch 101/300, Loss: 0.0702 | 0.0803
Epoch 102/300, Loss: 0.0700 | 0.0807
Epoch 103/300, Loss: 0.0699 | 0.0803
Epoch 104/300, Loss: 0.0690 | 0.0801
Epoch 105/300, Loss: 0.0697 | 0.0795
Epoch 106/300, Loss: 0.0684 | 0.0785
Epoch 107/300, Loss: 0.0687 | 0.0793
Epoch 108/300, Loss: 0.0678 | 0.0792
Epoch 109/300, Loss: 0.0689 | 0.0795
Epoch 110/300, Loss: 0.0675 | 0.0794
Epoch 111/300, Loss: 0.0673 | 0.0785
Epoch 112/300, Loss: 0.0666 | 0.0780
Epoch 113/300, Loss: 0.0670 | 0.0786
Epoch 114/300, Loss: 0.0668 | 0.0780
Epoch 115/300, Loss: 0.0663 | 0.0775
Epoch 116/300, Loss: 0.0666 | 0.0785
Epoch 117/300, Loss: 0.0663 | 0.0772
Epoch 118/300, Loss: 0.0661 | 0.0754
Epoch 119/300, Loss: 0.0655 | 0.0770
Epoch 120/300, Loss: 0.0652 | 0.0767
Epoch 121/300, Loss: 0.0656 | 0.0757
Epoch 122/300, Loss: 0.0651 | 0.0758
Epoch 123/300, Loss: 0.0650 | 0.0759
Epoch 124/300, Loss: 0.0648 | 0.0767
Epoch 125/300, Loss: 0.0643 | 0.0752
Epoch 126/300, Loss: 0.0642 | 0.0756
Epoch 127/300, Loss: 0.0652 | 0.0748
Epoch 128/300, Loss: 0.0643 | 0.0769
Epoch 129/300, Loss: 0.0640 | 0.0760
Epoch 130/300, Loss: 0.0636 | 0.0758
Epoch 131/300, Loss: 0.0632 | 0.0751
Epoch 132/300, Loss: 0.0639 | 0.0758
Epoch 133/300, Loss: 0.0630 | 0.0751
Epoch 134/300, Loss: 0.0633 | 0.0744
Epoch 135/300, Loss: 0.0626 | 0.0739
Epoch 136/300, Loss: 0.0632 | 0.0728
Epoch 137/300, Loss: 0.0627 | 0.0743
Epoch 138/300, Loss: 0.0625 | 0.0736
Epoch 139/300, Loss: 0.0625 | 0.0738
Epoch 140/300, Loss: 0.0624 | 0.0749
Epoch 141/300, Loss: 0.0626 | 0.0753
Epoch 142/300, Loss: 0.0620 | 0.0742
Epoch 143/300, Loss: 0.0623 | 0.0746
Epoch 144/300, Loss: 0.0622 | 0.0732
Epoch 145/300, Loss: 0.0632 | 0.0741
Epoch 146/300, Loss: 0.0620 | 0.0738
Epoch 147/300, Loss: 0.0615 | 0.0748
Epoch 148/300, Loss: 0.0620 | 0.0745
Epoch 149/300, Loss: 0.0616 | 0.0740
Epoch 150/300, Loss: 0.0607 | 0.0738
Epoch 151/300, Loss: 0.0610 | 0.0735
Epoch 152/300, Loss: 0.0607 | 0.0743
Epoch 153/300, Loss: 0.0606 | 0.0729
Epoch 154/300, Loss: 0.0610 | 0.0752
Epoch 155/300, Loss: 0.0606 | 0.0736
Epoch 156/300, Loss: 0.0613 | 0.0734
Epoch 157/300, Loss: 0.0611 | 0.0728
Epoch 158/300, Loss: 0.0606 | 0.0736
Epoch 159/300, Loss: 0.0597 | 0.0731
Epoch 160/300, Loss: 0.0610 | 0.0733
Epoch 161/300, Loss: 0.0600 | 0.0721
Epoch 162/300, Loss: 0.0605 | 0.0721
Epoch 163/300, Loss: 0.0607 | 0.0724
Epoch 164/300, Loss: 0.0603 | 0.0733
Epoch 165/300, Loss: 0.0607 | 0.0726
Epoch 166/300, Loss: 0.0596 | 0.0722
Epoch 167/300, Loss: 0.0600 | 0.0737
Epoch 168/300, Loss: 0.0602 | 0.0729
Epoch 169/300, Loss: 0.0601 | 0.0723
Epoch 170/300, Loss: 0.0599 | 0.0719
Epoch 171/300, Loss: 0.0601 | 0.0721
Epoch 172/300, Loss: 0.0599 | 0.0725
Epoch 173/300, Loss: 0.0597 | 0.0725
Epoch 174/300, Loss: 0.0591 | 0.0734
Epoch 175/300, Loss: 0.0598 | 0.0721
Epoch 176/300, Loss: 0.0601 | 0.0719
Epoch 177/300, Loss: 0.0588 | 0.0724
Epoch 178/300, Loss: 0.0598 | 0.0721
Epoch 179/300, Loss: 0.0594 | 0.0724
Epoch 180/300, Loss: 0.0594 | 0.0724
Epoch 181/300, Loss: 0.0592 | 0.0722
Epoch 182/300, Loss: 0.0591 | 0.0725
Epoch 183/300, Loss: 0.0593 | 0.0729
Epoch 184/300, Loss: 0.0592 | 0.0721
Epoch 185/300, Loss: 0.0594 | 0.0719
Epoch 186/300, Loss: 0.0584 | 0.0720
Epoch 187/300, Loss: 0.0594 | 0.0721
Epoch 188/300, Loss: 0.0589 | 0.0719
Epoch 189/300, Loss: 0.0592 | 0.0721
Epoch 190/300, Loss: 0.0587 | 0.0726
Epoch 191/300, Loss: 0.0584 | 0.0719
Epoch 192/300, Loss: 0.0590 | 0.0722
Epoch 193/300, Loss: 0.0592 | 0.0720
Epoch 194/300, Loss: 0.0585 | 0.0719
Epoch 195/300, Loss: 0.0589 | 0.0721
Epoch 196/300, Loss: 0.0589 | 0.0717
Epoch 197/300, Loss: 0.0588 | 0.0716
Epoch 198/300, Loss: 0.0584 | 0.0720
Epoch 199/300, Loss: 0.0586 | 0.0721
Epoch 200/300, Loss: 0.0588 | 0.0721
Epoch 201/300, Loss: 0.0588 | 0.0719
Epoch 202/300, Loss: 0.0589 | 0.0717
Epoch 203/300, Loss: 0.0585 | 0.0715
Epoch 204/300, Loss: 0.0589 | 0.0716
Epoch 205/300, Loss: 0.0581 | 0.0719
Epoch 206/300, Loss: 0.0583 | 0.0718
Epoch 207/300, Loss: 0.0586 | 0.0722
Epoch 208/300, Loss: 0.0589 | 0.0722
Epoch 209/300, Loss: 0.0585 | 0.0718
Epoch 210/300, Loss: 0.0584 | 0.0718
Epoch 211/300, Loss: 0.0578 | 0.0717
Epoch 212/300, Loss: 0.0584 | 0.0717
Epoch 213/300, Loss: 0.0579 | 0.0713
Epoch 214/300, Loss: 0.0583 | 0.0715
Epoch 215/300, Loss: 0.0588 | 0.0714
Epoch 216/300, Loss: 0.0582 | 0.0715
Epoch 217/300, Loss: 0.0583 | 0.0718
Epoch 218/300, Loss: 0.0584 | 0.0716
Epoch 219/300, Loss: 0.0582 | 0.0714
Epoch 220/300, Loss: 0.0586 | 0.0711
Epoch 221/300, Loss: 0.0581 | 0.0711
Epoch 222/300, Loss: 0.0578 | 0.0711
Epoch 223/300, Loss: 0.0580 | 0.0713
Epoch 224/300, Loss: 0.0577 | 0.0709
Epoch 225/300, Loss: 0.0582 | 0.0709
Epoch 226/300, Loss: 0.0579 | 0.0707
Epoch 227/300, Loss: 0.0583 | 0.0710
Epoch 228/300, Loss: 0.0581 | 0.0711
Epoch 229/300, Loss: 0.0582 | 0.0709
Epoch 230/300, Loss: 0.0580 | 0.0712
Epoch 231/300, Loss: 0.0579 | 0.0715
Epoch 232/300, Loss: 0.0577 | 0.0712
Epoch 233/300, Loss: 0.0586 | 0.0712
Epoch 234/300, Loss: 0.0579 | 0.0712
Epoch 235/300, Loss: 0.0577 | 0.0710
Epoch 236/300, Loss: 0.0584 | 0.0713
Epoch 237/300, Loss: 0.0576 | 0.0713
Epoch 238/300, Loss: 0.0577 | 0.0712
Epoch 239/300, Loss: 0.0580 | 0.0712
Epoch 240/300, Loss: 0.0576 | 0.0709
Epoch 241/300, Loss: 0.0583 | 0.0712
Epoch 242/300, Loss: 0.0581 | 0.0711
Epoch 243/300, Loss: 0.0575 | 0.0710
Epoch 244/300, Loss: 0.0579 | 0.0708
Epoch 245/300, Loss: 0.0583 | 0.0709
Epoch 246/300, Loss: 0.0578 | 0.0713
Epoch 247/300, Loss: 0.0576 | 0.0712
Epoch 248/300, Loss: 0.0574 | 0.0711
Epoch 249/300, Loss: 0.0575 | 0.0711
Epoch 250/300, Loss: 0.0572 | 0.0709
Epoch 251/300, Loss: 0.0576 | 0.0711
Epoch 252/300, Loss: 0.0580 | 0.0709
Epoch 253/300, Loss: 0.0577 | 0.0708
Epoch 254/300, Loss: 0.0578 | 0.0708
Epoch 255/300, Loss: 0.0578 | 0.0708
Epoch 256/300, Loss: 0.0575 | 0.0709
Epoch 257/300, Loss: 0.0574 | 0.0709
Epoch 258/300, Loss: 0.0582 | 0.0710
Epoch 259/300, Loss: 0.0578 | 0.0710
Epoch 260/300, Loss: 0.0573 | 0.0711
Epoch 261/300, Loss: 0.0574 | 0.0710
Epoch 262/300, Loss: 0.0578 | 0.0710
Epoch 263/300, Loss: 0.0577 | 0.0709
Epoch 264/300, Loss: 0.0573 | 0.0709
Epoch 265/300, Loss: 0.0576 | 0.0709
Epoch 266/300, Loss: 0.0577 | 0.0708
Epoch 267/300, Loss: 0.0575 | 0.0708
Epoch 268/300, Loss: 0.0589 | 0.0709
Epoch 269/300, Loss: 0.0571 | 0.0711
Epoch 270/300, Loss: 0.0580 | 0.0711
Epoch 271/300, Loss: 0.0578 | 0.0711
Epoch 272/300, Loss: 0.0575 | 0.0712
Epoch 273/300, Loss: 0.0579 | 0.0710
Epoch 274/300, Loss: 0.0576 | 0.0709
Epoch 275/300, Loss: 0.0581 | 0.0708
Epoch 276/300, Loss: 0.0576 | 0.0708
Epoch 277/300, Loss: 0.0578 | 0.0708
Epoch 278/300, Loss: 0.0578 | 0.0707
Epoch 279/300, Loss: 0.0579 | 0.0708
Epoch 280/300, Loss: 0.0576 | 0.0708
Epoch 281/300, Loss: 0.0578 | 0.0709
Epoch 282/300, Loss: 0.0577 | 0.0708
Epoch 283/300, Loss: 0.0583 | 0.0709
Epoch 284/300, Loss: 0.0576 | 0.0709
Epoch 285/300, Loss: 0.0578 | 0.0708
Epoch 286/300, Loss: 0.0574 | 0.0709
Epoch 287/300, Loss: 0.0578 | 0.0709
Epoch 288/300, Loss: 0.0573 | 0.0709
Epoch 289/300, Loss: 0.0573 | 0.0709
Epoch 290/300, Loss: 0.0574 | 0.0710
Epoch 291/300, Loss: 0.0572 | 0.0711
Epoch 292/300, Loss: 0.0574 | 0.0710
Epoch 293/300, Loss: 0.0572 | 0.0710
Epoch 294/300, Loss: 0.0573 | 0.0709
Epoch 295/300, Loss: 0.0574 | 0.0710
Epoch 296/300, Loss: 0.0575 | 0.0710
Epoch 297/300, Loss: 0.0580 | 0.0710
Epoch 298/300, Loss: 0.0576 | 0.0710
Epoch 299/300, Loss: 0.0583 | 0.0710
Epoch 300/300, Loss: 0.0578 | 0.0709
Runtime (seconds): 337.09081983566284
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 704.6563700607512
RMSE: 26.545364379882812
MAE: 26.545364379882812
R-squared: nan
[231.65465]
