ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-03 17:11:48,391][0m A new study created in memory with name: no-name-020878f7-648a-44c0-a787-66bc3422362e[0m
[32m[I 2025-02-03 17:13:43,426][0m Trial 0 finished with value: 0.24301061034202576 and parameters: {'observation_period_num': 106, 'train_rates': 0.9769725247467054, 'learning_rate': 0.000823882025608762, 'batch_size': 206, 'step_size': 8, 'gamma': 0.9181552777798674}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:16:34,705][0m Trial 1 finished with value: 2.344323992729187 and parameters: {'observation_period_num': 163, 'train_rates': 0.8225850576219859, 'learning_rate': 1.4530045880410532e-06, 'batch_size': 129, 'step_size': 15, 'gamma': 0.7966738057567553}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:19:47,109][0m Trial 2 finished with value: 0.2599894576882482 and parameters: {'observation_period_num': 110, 'train_rates': 0.8794073586563111, 'learning_rate': 0.00042769264264064705, 'batch_size': 23, 'step_size': 13, 'gamma': 0.7592113616345262}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:20:26,956][0m Trial 3 finished with value: 1.5791548122495613 and parameters: {'observation_period_num': 43, 'train_rates': 0.6926830397621441, 'learning_rate': 0.0008747914839239233, 'batch_size': 174, 'step_size': 3, 'gamma': 0.9558270654009648}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:20:53,917][0m Trial 4 finished with value: 1.7770364166985095 and parameters: {'observation_period_num': 28, 'train_rates': 0.684456907328142, 'learning_rate': 1.5203245775449236e-06, 'batch_size': 210, 'step_size': 12, 'gamma': 0.8463244081308062}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:23:54,621][0m Trial 5 finished with value: 1.9956126053201213 and parameters: {'observation_period_num': 184, 'train_rates': 0.7521507628424943, 'learning_rate': 2.4989450099376697e-06, 'batch_size': 233, 'step_size': 14, 'gamma': 0.9477408609523753}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:25:40,035][0m Trial 6 finished with value: 1.9660971057579815 and parameters: {'observation_period_num': 115, 'train_rates': 0.693925519785417, 'learning_rate': 3.5686754046015445e-06, 'batch_size': 120, 'step_size': 4, 'gamma': 0.9515399281203539}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:30:23,639][0m Trial 7 finished with value: 1.3521244525909424 and parameters: {'observation_period_num': 217, 'train_rates': 0.97363866626002, 'learning_rate': 2.3325449957708957e-06, 'batch_size': 231, 'step_size': 15, 'gamma': 0.9665162857363331}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:33:55,680][0m Trial 8 finished with value: 2.215137686140957 and parameters: {'observation_period_num': 196, 'train_rates': 0.800007782789828, 'learning_rate': 1.411892459920104e-06, 'batch_size': 141, 'step_size': 6, 'gamma': 0.9512511637676508}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:34:52,604][0m Trial 9 finished with value: 0.6298247950886368 and parameters: {'observation_period_num': 64, 'train_rates': 0.8113885743602164, 'learning_rate': 4.021516590609203e-05, 'batch_size': 209, 'step_size': 14, 'gamma': 0.890462930189258}. Best is trial 0 with value: 0.24301061034202576.[0m
[32m[I 2025-02-03 17:40:41,482][0m Trial 10 finished with value: 0.13319367170333862 and parameters: {'observation_period_num': 250, 'train_rates': 0.9860472327065772, 'learning_rate': 0.00012748266643377473, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8995782433145573}. Best is trial 10 with value: 0.13319367170333862.[0m
[32m[I 2025-02-03 17:46:29,420][0m Trial 11 finished with value: 0.1257079541683197 and parameters: {'observation_period_num': 252, 'train_rates': 0.9883692072251689, 'learning_rate': 0.00015131830366577263, 'batch_size': 62, 'step_size': 9, 'gamma': 0.8945112161230043}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 17:51:57,520][0m Trial 12 finished with value: 0.1680705168247223 and parameters: {'observation_period_num': 251, 'train_rates': 0.9075640992526853, 'learning_rate': 0.0001306270880481988, 'batch_size': 49, 'step_size': 10, 'gamma': 0.8587092066361648}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 17:55:58,700][0m Trial 13 finished with value: 1.397434590766669 and parameters: {'observation_period_num': 252, 'train_rates': 0.6046270417480366, 'learning_rate': 0.00010933767735315975, 'batch_size': 79, 'step_size': 9, 'gamma': 0.9022972316307964}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:00:50,003][0m Trial 14 finished with value: 0.7276097125785296 and parameters: {'observation_period_num': 225, 'train_rates': 0.921346874851432, 'learning_rate': 1.4066183043728323e-05, 'batch_size': 78, 'step_size': 7, 'gamma': 0.8334507040993696}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:04:02,419][0m Trial 15 finished with value: 0.1275024116039276 and parameters: {'observation_period_num': 153, 'train_rates': 0.9883864217334316, 'learning_rate': 0.00017148631080215682, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9173543029902917}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:05:24,316][0m Trial 16 finished with value: 0.3409273011609912 and parameters: {'observation_period_num': 78, 'train_rates': 0.8609424861537986, 'learning_rate': 4.1205868028211554e-05, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9212951510073207}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:08:50,747][0m Trial 17 finished with value: 0.1549004920563855 and parameters: {'observation_period_num': 149, 'train_rates': 0.935359289142781, 'learning_rate': 0.0002928269861731664, 'batch_size': 24, 'step_size': 1, 'gamma': 0.9885080049167356}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:11:40,109][0m Trial 18 finished with value: 0.8551613765378152 and parameters: {'observation_period_num': 145, 'train_rates': 0.9337797180101777, 'learning_rate': 1.597342626015381e-05, 'batch_size': 162, 'step_size': 6, 'gamma': 0.8754220743578079}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:15:03,515][0m Trial 19 finished with value: 0.31993977212515035 and parameters: {'observation_period_num': 182, 'train_rates': 0.8678255568014509, 'learning_rate': 7.882440294598126e-05, 'batch_size': 96, 'step_size': 11, 'gamma': 0.8194660738392724}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:16:08,843][0m Trial 20 finished with value: 1.1971647648975767 and parameters: {'observation_period_num': 11, 'train_rates': 0.7533872076859341, 'learning_rate': 0.00027198336688743596, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9262841655425212}. Best is trial 11 with value: 0.1257079541683197.[0m
[32m[I 2025-02-03 18:21:05,129][0m Trial 21 finished with value: 0.12319710105657578 and parameters: {'observation_period_num': 222, 'train_rates': 0.9828404923725456, 'learning_rate': 0.00019544699423404, 'batch_size': 48, 'step_size': 9, 'gamma': 0.8818329492645005}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:26:01,878][0m Trial 22 finished with value: 0.1437397173781326 and parameters: {'observation_period_num': 217, 'train_rates': 0.9495114120166487, 'learning_rate': 0.00022430125916331572, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8753270551096106}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:30:45,052][0m Trial 23 finished with value: 0.3430418247637683 and parameters: {'observation_period_num': 232, 'train_rates': 0.8928001482506672, 'learning_rate': 5.753291521285413e-05, 'batch_size': 95, 'step_size': 7, 'gamma': 0.8859527934421236}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:35:05,014][0m Trial 24 finished with value: 0.1817122995853424 and parameters: {'observation_period_num': 199, 'train_rates': 0.9894468935878373, 'learning_rate': 0.0004876584469391674, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8572560609771943}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:38:41,049][0m Trial 25 finished with value: 0.21300180291845686 and parameters: {'observation_period_num': 169, 'train_rates': 0.9548684325191089, 'learning_rate': 1.596801856135083e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9313974147203307}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:43:48,265][0m Trial 26 finished with value: 0.15169436152164753 and parameters: {'observation_period_num': 233, 'train_rates': 0.9523870085688675, 'learning_rate': 0.000179855026300357, 'batch_size': 111, 'step_size': 8, 'gamma': 0.9110787851064361}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:47:59,280][0m Trial 27 finished with value: 0.7213779480761082 and parameters: {'observation_period_num': 207, 'train_rates': 0.9056744329844334, 'learning_rate': 2.445809024452614e-05, 'batch_size': 85, 'step_size': 5, 'gamma': 0.8085688827762411}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:50:49,873][0m Trial 28 finished with value: 0.5376776936834893 and parameters: {'observation_period_num': 140, 'train_rates': 0.8393587029080745, 'learning_rate': 0.0004423598840280595, 'batch_size': 33, 'step_size': 12, 'gamma': 0.8670583733013486}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:55:44,897][0m Trial 29 finished with value: 2.5532371236922895 and parameters: {'observation_period_num': 94, 'train_rates': 0.9672924764743996, 'learning_rate': 0.0006396573293099297, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9317814783502641}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 18:58:35,299][0m Trial 30 finished with value: 0.16654449701309204 and parameters: {'observation_period_num': 131, 'train_rates': 0.9882318667781997, 'learning_rate': 6.856382353197575e-05, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8395241928406392}. Best is trial 21 with value: 0.12319710105657578.[0m
[32m[I 2025-02-03 19:04:07,125][0m Trial 31 finished with value: 0.11328105628490448 and parameters: {'observation_period_num': 242, 'train_rates': 0.9885456141056422, 'learning_rate': 0.0001394235182320582, 'batch_size': 52, 'step_size': 9, 'gamma': 0.902195306455488}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:09:22,045][0m Trial 32 finished with value: 0.13062363124645507 and parameters: {'observation_period_num': 236, 'train_rates': 0.9591157850637264, 'learning_rate': 0.00017390678006271986, 'batch_size': 59, 'step_size': 7, 'gamma': 0.9068724450976519}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:12:31,725][0m Trial 33 finished with value: 0.23157801257796806 and parameters: {'observation_period_num': 166, 'train_rates': 0.927731640630465, 'learning_rate': 0.00010004466581219704, 'batch_size': 139, 'step_size': 9, 'gamma': 0.8950011641581218}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:17:14,840][0m Trial 34 finished with value: 0.14524866422910368 and parameters: {'observation_period_num': 212, 'train_rates': 0.967652588630953, 'learning_rate': 0.0003328008094440759, 'batch_size': 45, 'step_size': 10, 'gamma': 0.7835031605574989}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:22:27,754][0m Trial 35 finished with value: 0.8412594199180603 and parameters: {'observation_period_num': 238, 'train_rates': 0.8871533185781705, 'learning_rate': 0.0009703023932296397, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8814391495683223}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:26:07,831][0m Trial 36 finished with value: 0.21596938473546606 and parameters: {'observation_period_num': 181, 'train_rates': 0.9157137178598976, 'learning_rate': 0.00016822616144495335, 'batch_size': 71, 'step_size': 13, 'gamma': 0.9219111379612374}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:30:28,226][0m Trial 37 finished with value: 0.20540600485707583 and parameters: {'observation_period_num': 203, 'train_rates': 0.9448402448268711, 'learning_rate': 5.233435156411163e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9392968954469613}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:32:39,561][0m Trial 38 finished with value: 0.745959997177124 and parameters: {'observation_period_num': 115, 'train_rates': 0.9716430252557574, 'learning_rate': 6.578719892892377e-06, 'batch_size': 118, 'step_size': 11, 'gamma': 0.9121876297507736}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:37:08,819][0m Trial 39 finished with value: 0.291286497314771 and parameters: {'observation_period_num': 223, 'train_rates': 0.8461729051240172, 'learning_rate': 8.970772567343833e-05, 'batch_size': 49, 'step_size': 6, 'gamma': 0.9749479461164705}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:40:35,197][0m Trial 40 finished with value: 0.906864686242116 and parameters: {'observation_period_num': 193, 'train_rates': 0.7780737484860751, 'learning_rate': 0.000361177444334375, 'batch_size': 250, 'step_size': 12, 'gamma': 0.8575141356700565}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:46:04,654][0m Trial 41 finished with value: 0.4400226668878035 and parameters: {'observation_period_num': 241, 'train_rates': 0.9596277596460824, 'learning_rate': 0.0006825026783497044, 'batch_size': 64, 'step_size': 7, 'gamma': 0.908026978828848}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:51:15,849][0m Trial 42 finished with value: 0.14260561764240265 and parameters: {'observation_period_num': 238, 'train_rates': 0.9719180768922789, 'learning_rate': 0.0001905022620472207, 'batch_size': 105, 'step_size': 7, 'gamma': 0.8942615802245631}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 19:56:26,487][0m Trial 43 finished with value: 0.1347353160381317 and parameters: {'observation_period_num': 225, 'train_rates': 0.9847861826865953, 'learning_rate': 0.0001593820825879209, 'batch_size': 57, 'step_size': 9, 'gamma': 0.9398372726427127}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 20:01:44,744][0m Trial 44 finished with value: 0.17481200818745596 and parameters: {'observation_period_num': 243, 'train_rates': 0.9412871853648109, 'learning_rate': 0.00024020067545614852, 'batch_size': 78, 'step_size': 5, 'gamma': 0.8862588053120224}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 20:07:22,222][0m Trial 45 finished with value: 0.16714023053646088 and parameters: {'observation_period_num': 252, 'train_rates': 0.9895072427440383, 'learning_rate': 0.00011829212478102327, 'batch_size': 161, 'step_size': 8, 'gamma': 0.9008347346315602}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 20:11:12,417][0m Trial 46 finished with value: 1.854873005011658 and parameters: {'observation_period_num': 217, 'train_rates': 0.6702457767712399, 'learning_rate': 0.0005110580425495085, 'batch_size': 52, 'step_size': 10, 'gamma': 0.9570096069405083}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 20:13:36,322][0m Trial 47 finished with value: 0.14661087851289292 and parameters: {'observation_period_num': 42, 'train_rates': 0.9026689176833107, 'learning_rate': 0.00014992415425983712, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8667704949376211}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 20:15:27,990][0m Trial 48 finished with value: 0.6652997732162476 and parameters: {'observation_period_num': 99, 'train_rates': 0.9640819772876922, 'learning_rate': 2.482085158166573e-05, 'batch_size': 127, 'step_size': 3, 'gamma': 0.9133920702395052}. Best is trial 31 with value: 0.11328105628490448.[0m
[32m[I 2025-02-03 20:20:21,960][0m Trial 49 finished with value: 0.17945269146576304 and parameters: {'observation_period_num': 229, 'train_rates': 0.9256326978700475, 'learning_rate': 0.00022638996475968283, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8467615652108961}. Best is trial 31 with value: 0.11328105628490448.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-03 20:20:21,967][0m A new study created in memory with name: no-name-eb50cc41-8252-4faa-8d85-b51d80231a04[0m
[32m[I 2025-02-03 20:21:52,339][0m Trial 0 finished with value: 1.0897836060546857 and parameters: {'observation_period_num': 98, 'train_rates': 0.7077160744047335, 'learning_rate': 2.9386357292835424e-05, 'batch_size': 88, 'step_size': 15, 'gamma': 0.9346085757146017}. Best is trial 0 with value: 1.0897836060546857.[0m
[32m[I 2025-02-03 20:23:40,700][0m Trial 1 finished with value: 2.0141980523864427 and parameters: {'observation_period_num': 118, 'train_rates': 0.7119585224393916, 'learning_rate': 1.5750840051060808e-06, 'batch_size': 85, 'step_size': 1, 'gamma': 0.924275307463733}. Best is trial 0 with value: 1.0897836060546857.[0m
[32m[I 2025-02-03 20:26:59,137][0m Trial 2 finished with value: 0.7194154262542725 and parameters: {'observation_period_num': 170, 'train_rates': 0.9514539804441572, 'learning_rate': 1.2974747686018459e-05, 'batch_size': 203, 'step_size': 11, 'gamma': 0.7985941290391008}. Best is trial 2 with value: 0.7194154262542725.[0m
[32m[I 2025-02-03 20:28:40,202][0m Trial 3 finished with value: 2.26661723739696 and parameters: {'observation_period_num': 106, 'train_rates': 0.7765471183255659, 'learning_rate': 8.550938745902736e-06, 'batch_size': 148, 'step_size': 2, 'gamma': 0.8615880464849557}. Best is trial 2 with value: 0.7194154262542725.[0m
Early stopping at epoch 95
[32m[I 2025-02-03 20:29:29,409][0m Trial 4 finished with value: 1.8008582006485612 and parameters: {'observation_period_num': 37, 'train_rates': 0.6111055613622821, 'learning_rate': 1.057072080447951e-05, 'batch_size': 71, 'step_size': 1, 'gamma': 0.8826519155819372}. Best is trial 2 with value: 0.7194154262542725.[0m
[32m[I 2025-02-03 20:30:45,373][0m Trial 5 finished with value: 1.0416112078415742 and parameters: {'observation_period_num': 93, 'train_rates': 0.6004646887490666, 'learning_rate': 0.00029188778052011274, 'batch_size': 253, 'step_size': 7, 'gamma': 0.7654561578594639}. Best is trial 2 with value: 0.7194154262542725.[0m
[32m[I 2025-02-03 20:33:46,670][0m Trial 6 finished with value: 0.24117807532603236 and parameters: {'observation_period_num': 162, 'train_rates': 0.8662522307730871, 'learning_rate': 0.00023178524096118796, 'batch_size': 74, 'step_size': 5, 'gamma': 0.8546756144566183}. Best is trial 6 with value: 0.24117807532603236.[0m
[32m[I 2025-02-03 20:34:15,937][0m Trial 7 finished with value: 0.919788984151987 and parameters: {'observation_period_num': 29, 'train_rates': 0.7331639657098938, 'learning_rate': 6.675880692246655e-05, 'batch_size': 225, 'step_size': 8, 'gamma': 0.8620571113442497}. Best is trial 6 with value: 0.24117807532603236.[0m
[32m[I 2025-02-03 20:36:29,607][0m Trial 8 finished with value: 1.3897947773476627 and parameters: {'observation_period_num': 139, 'train_rates': 0.6958118598871577, 'learning_rate': 0.0007872573060286932, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9026090222811265}. Best is trial 6 with value: 0.24117807532603236.[0m
[32m[I 2025-02-03 20:39:37,853][0m Trial 9 finished with value: 0.6362586036324501 and parameters: {'observation_period_num': 184, 'train_rates': 0.7832494275736863, 'learning_rate': 0.0001738230646383552, 'batch_size': 198, 'step_size': 11, 'gamma': 0.943477591170766}. Best is trial 6 with value: 0.24117807532603236.[0m
[32m[I 2025-02-03 20:45:06,710][0m Trial 10 finished with value: 2.2673362013581513 and parameters: {'observation_period_num': 245, 'train_rates': 0.914710834283244, 'learning_rate': 0.00095597375198647, 'batch_size': 32, 'step_size': 5, 'gamma': 0.8294550285368831}. Best is trial 6 with value: 0.24117807532603236.[0m
[32m[I 2025-02-03 20:48:53,754][0m Trial 11 finished with value: 0.27516373071947325 and parameters: {'observation_period_num': 199, 'train_rates': 0.8625250986295643, 'learning_rate': 0.00015279476588630868, 'batch_size': 163, 'step_size': 12, 'gamma': 0.9670106424058886}. Best is trial 6 with value: 0.24117807532603236.[0m
[32m[I 2025-02-03 20:53:16,404][0m Trial 12 finished with value: 0.23353524182153784 and parameters: {'observation_period_num': 224, 'train_rates': 0.8737708253555757, 'learning_rate': 0.00018489489946340958, 'batch_size': 150, 'step_size': 15, 'gamma': 0.9800531449426807}. Best is trial 12 with value: 0.23353524182153784.[0m
[32m[I 2025-02-03 20:58:43,747][0m Trial 13 finished with value: 0.22025726085647623 and parameters: {'observation_period_num': 242, 'train_rates': 0.8612643774790235, 'learning_rate': 6.848003793721176e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.9796898425954621}. Best is trial 13 with value: 0.22025726085647623.[0m
[32m[I 2025-02-03 21:04:41,318][0m Trial 14 finished with value: 0.236253218437598 and parameters: {'observation_period_num': 250, 'train_rates': 0.8634172267384858, 'learning_rate': 5.6713113538727e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9887464593875767}. Best is trial 13 with value: 0.22025726085647623.[0m
[32m[I 2025-02-03 21:09:28,772][0m Trial 15 finished with value: 0.16872961819171906 and parameters: {'observation_period_num': 217, 'train_rates': 0.989137608938123, 'learning_rate': 6.463479534219058e-05, 'batch_size': 119, 'step_size': 4, 'gamma': 0.9797272771608506}. Best is trial 15 with value: 0.16872961819171906.[0m
[32m[I 2025-02-03 21:14:04,922][0m Trial 16 finished with value: 0.279261976480484 and parameters: {'observation_period_num': 216, 'train_rates': 0.9764991657754042, 'learning_rate': 2.730513939428163e-05, 'batch_size': 116, 'step_size': 4, 'gamma': 0.9579630240041528}. Best is trial 15 with value: 0.16872961819171906.[0m
[32m[I 2025-02-03 21:19:03,231][0m Trial 17 finished with value: 0.6591502689810657 and parameters: {'observation_period_num': 230, 'train_rates': 0.9307641087888444, 'learning_rate': 3.8681193843137125e-06, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9136517649311504}. Best is trial 15 with value: 0.16872961819171906.[0m
[32m[I 2025-02-03 21:23:04,532][0m Trial 18 finished with value: 0.21431341890996625 and parameters: {'observation_period_num': 202, 'train_rates': 0.9079814275840796, 'learning_rate': 7.081193398576388e-05, 'batch_size': 120, 'step_size': 6, 'gamma': 0.9588568488477174}. Best is trial 15 with value: 0.16872961819171906.[0m
[32m[I 2025-02-03 21:26:01,594][0m Trial 19 finished with value: 0.16203682124614716 and parameters: {'observation_period_num': 145, 'train_rates': 0.9771602533073183, 'learning_rate': 0.0004823849913584694, 'batch_size': 124, 'step_size': 7, 'gamma': 0.9527496644707703}. Best is trial 19 with value: 0.16203682124614716.[0m
[32m[I 2025-02-03 21:27:18,391][0m Trial 20 finished with value: 0.16014772653579712 and parameters: {'observation_period_num': 70, 'train_rates': 0.9892483228035552, 'learning_rate': 0.0005029617530946157, 'batch_size': 169, 'step_size': 9, 'gamma': 0.8968157741229389}. Best is trial 20 with value: 0.16014772653579712.[0m
[32m[I 2025-02-03 21:28:40,099][0m Trial 21 finished with value: 0.14250673353672028 and parameters: {'observation_period_num': 75, 'train_rates': 0.9784595756457143, 'learning_rate': 0.0004495992760815187, 'batch_size': 181, 'step_size': 9, 'gamma': 0.8922639949956196}. Best is trial 21 with value: 0.14250673353672028.[0m
[32m[I 2025-02-03 21:29:50,908][0m Trial 22 finished with value: 0.16746968030929565 and parameters: {'observation_period_num': 66, 'train_rates': 0.9466780985913871, 'learning_rate': 0.00046219224644940596, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8952174123404584}. Best is trial 21 with value: 0.14250673353672028.[0m
[32m[I 2025-02-03 21:31:05,406][0m Trial 23 finished with value: 0.14689379930496216 and parameters: {'observation_period_num': 68, 'train_rates': 0.9871422717879619, 'learning_rate': 0.0004056872940357021, 'batch_size': 201, 'step_size': 9, 'gamma': 0.8274507722598097}. Best is trial 21 with value: 0.14250673353672028.[0m
[32m[I 2025-02-03 21:32:16,380][0m Trial 24 finished with value: 0.1715231802728441 and parameters: {'observation_period_num': 68, 'train_rates': 0.9002737174876018, 'learning_rate': 0.0004885205670096331, 'batch_size': 184, 'step_size': 10, 'gamma': 0.8331098624485398}. Best is trial 21 with value: 0.14250673353672028.[0m
[32m[I 2025-02-03 21:32:40,331][0m Trial 25 finished with value: 0.13142570853233337 and parameters: {'observation_period_num': 8, 'train_rates': 0.9521920266536259, 'learning_rate': 0.0003374384478605072, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8237045326693101}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:33:07,211][0m Trial 26 finished with value: 0.18071259558200836 and parameters: {'observation_period_num': 19, 'train_rates': 0.9577896243782678, 'learning_rate': 0.00012436402701981465, 'batch_size': 232, 'step_size': 13, 'gamma': 0.8037320057085584}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:33:57,365][0m Trial 27 finished with value: 0.3854370408930541 and parameters: {'observation_period_num': 51, 'train_rates': 0.820145674061192, 'learning_rate': 0.00031846211861753034, 'batch_size': 215, 'step_size': 13, 'gamma': 0.8368214133216112}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:34:16,546][0m Trial 28 finished with value: 0.46765934359537414 and parameters: {'observation_period_num': 9, 'train_rates': 0.826352373563591, 'learning_rate': 0.00011016367284526141, 'batch_size': 252, 'step_size': 13, 'gamma': 0.8042499546924702}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:35:49,649][0m Trial 29 finished with value: 0.6193587276124463 and parameters: {'observation_period_num': 90, 'train_rates': 0.9324081716930431, 'learning_rate': 2.907187619931426e-05, 'batch_size': 192, 'step_size': 10, 'gamma': 0.7708720214517383}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:36:36,196][0m Trial 30 finished with value: 0.20732701665932132 and parameters: {'observation_period_num': 46, 'train_rates': 0.8935706266898504, 'learning_rate': 0.0007335849135873172, 'batch_size': 235, 'step_size': 11, 'gamma': 0.7867694402253056}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:37:57,891][0m Trial 31 finished with value: 0.1524391919374466 and parameters: {'observation_period_num': 75, 'train_rates': 0.9653042772853515, 'learning_rate': 0.0003858701297854105, 'batch_size': 172, 'step_size': 8, 'gamma': 0.8844944803661559}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:39:22,462][0m Trial 32 finished with value: 0.15802563726902008 and parameters: {'observation_period_num': 80, 'train_rates': 0.9603443615817661, 'learning_rate': 0.00032350401018507166, 'batch_size': 210, 'step_size': 8, 'gamma': 0.8784123950862529}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:41:51,749][0m Trial 33 finished with value: 0.19707559530551616 and parameters: {'observation_period_num': 129, 'train_rates': 0.9310180898426828, 'learning_rate': 0.0006493549546675553, 'batch_size': 183, 'step_size': 7, 'gamma': 0.8457700909863641}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:43:53,030][0m Trial 34 finished with value: 0.15357594192028046 and parameters: {'observation_period_num': 113, 'train_rates': 0.9617323847510197, 'learning_rate': 0.00029214345002568693, 'batch_size': 158, 'step_size': 10, 'gamma': 0.8212385621917052}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:44:47,018][0m Trial 35 finished with value: 0.23228639364242554 and parameters: {'observation_period_num': 54, 'train_rates': 0.9454326964144497, 'learning_rate': 9.83582081919588e-05, 'batch_size': 219, 'step_size': 8, 'gamma': 0.9194459709664858}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:46:40,200][0m Trial 36 finished with value: 1.2842291593551636 and parameters: {'observation_period_num': 104, 'train_rates': 0.9699645161162742, 'learning_rate': 1.1843806863582575e-06, 'batch_size': 135, 'step_size': 10, 'gamma': 0.8762952967123734}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:47:11,086][0m Trial 37 finished with value: 0.8427893190939979 and parameters: {'observation_period_num': 33, 'train_rates': 0.6629955562284777, 'learning_rate': 0.0002188175681382293, 'batch_size': 201, 'step_size': 12, 'gamma': 0.8182021110850343}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:47:30,803][0m Trial 38 finished with value: 1.1460880929569028 and parameters: {'observation_period_num': 5, 'train_rates': 0.7480897487440216, 'learning_rate': 1.8170523388824645e-05, 'batch_size': 238, 'step_size': 14, 'gamma': 0.8512324490395219}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:49:05,655][0m Trial 39 finished with value: 0.17245102636516094 and parameters: {'observation_period_num': 90, 'train_rates': 0.9302610069693382, 'learning_rate': 0.00031651828270635405, 'batch_size': 181, 'step_size': 6, 'gamma': 0.8661257298765519}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:51:11,252][0m Trial 40 finished with value: 1.2508997201919556 and parameters: {'observation_period_num': 123, 'train_rates': 0.8320874073731603, 'learning_rate': 3.65256797275514e-06, 'batch_size': 140, 'step_size': 12, 'gamma': 0.9351773217041227}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:53:12,721][0m Trial 41 finished with value: 0.17012576758861542 and parameters: {'observation_period_num': 113, 'train_rates': 0.9590786181277364, 'learning_rate': 0.0002718939224595604, 'batch_size': 154, 'step_size': 10, 'gamma': 0.7522016731094887}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:54:40,996][0m Trial 42 finished with value: 0.229867622256279 and parameters: {'observation_period_num': 79, 'train_rates': 0.9735516887927542, 'learning_rate': 0.0009588725560302584, 'batch_size': 166, 'step_size': 9, 'gamma': 0.7899810273396867}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:56:31,211][0m Trial 43 finished with value: 0.15658529102802277 and parameters: {'observation_period_num': 102, 'train_rates': 0.9883283138721669, 'learning_rate': 0.00039223327676085346, 'batch_size': 191, 'step_size': 11, 'gamma': 0.8174909173293371}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:57:31,684][0m Trial 44 finished with value: 0.17157062888145447 and parameters: {'observation_period_num': 57, 'train_rates': 0.9455473328238027, 'learning_rate': 0.0006071920362391296, 'batch_size': 209, 'step_size': 8, 'gamma': 0.8883363054018123}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 21:58:12,811][0m Trial 45 finished with value: 0.21011153796565868 and parameters: {'observation_period_num': 38, 'train_rates': 0.8897896017768789, 'learning_rate': 0.0002033023368917401, 'batch_size': 153, 'step_size': 11, 'gamma': 0.8167258574280285}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 22:01:10,578][0m Trial 46 finished with value: 0.169394533600538 and parameters: {'observation_period_num': 157, 'train_rates': 0.9113914876129683, 'learning_rate': 0.00036462699731390936, 'batch_size': 198, 'step_size': 9, 'gamma': 0.9092323036150965}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 22:03:07,824][0m Trial 47 finished with value: 0.19647309101671667 and parameters: {'observation_period_num': 113, 'train_rates': 0.9236685808465314, 'learning_rate': 0.00014276220868194558, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8588504767536811}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 22:03:39,223][0m Trial 48 finished with value: 0.15267157554626465 and parameters: {'observation_period_num': 22, 'train_rates': 0.9654939708631347, 'learning_rate': 0.00023730489802168962, 'batch_size': 175, 'step_size': 10, 'gamma': 0.8458995127406338}. Best is trial 25 with value: 0.13142570853233337.[0m
[32m[I 2025-02-03 22:04:05,106][0m Trial 49 finished with value: 0.5773366263449362 and parameters: {'observation_period_num': 21, 'train_rates': 0.88031766031535, 'learning_rate': 4.180596083740451e-05, 'batch_size': 245, 'step_size': 8, 'gamma': 0.8447883684792542}. Best is trial 25 with value: 0.13142570853233337.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-03 22:04:05,114][0m A new study created in memory with name: no-name-7bf6bd94-66e0-4b8c-bfbc-dbfa17ed407f[0m
[32m[I 2025-02-03 22:04:41,654][0m Trial 0 finished with value: 1.4938141871625044 and parameters: {'observation_period_num': 35, 'train_rates': 0.7823107087611554, 'learning_rate': 2.511469853610452e-06, 'batch_size': 166, 'step_size': 15, 'gamma': 0.9511399278865389}. Best is trial 0 with value: 1.4938141871625044.[0m
[32m[I 2025-02-03 22:06:13,096][0m Trial 1 finished with value: 2.2239682968112007 and parameters: {'observation_period_num': 101, 'train_rates': 0.7096802470428887, 'learning_rate': 1.937810172226372e-06, 'batch_size': 188, 'step_size': 9, 'gamma': 0.8119507520429892}. Best is trial 0 with value: 1.4938141871625044.[0m
[32m[I 2025-02-03 22:08:45,958][0m Trial 2 finished with value: 1.4900187160959817 and parameters: {'observation_period_num': 57, 'train_rates': 0.7244321186466649, 'learning_rate': 2.423779859426855e-06, 'batch_size': 25, 'step_size': 14, 'gamma': 0.8411161498029264}. Best is trial 2 with value: 1.4900187160959817.[0m
[32m[I 2025-02-03 22:12:21,346][0m Trial 3 finished with value: 1.1074279023389098 and parameters: {'observation_period_num': 202, 'train_rates': 0.7151252982603238, 'learning_rate': 3.634284823280712e-05, 'batch_size': 49, 'step_size': 3, 'gamma': 0.8853549891286601}. Best is trial 3 with value: 1.1074279023389098.[0m
[32m[I 2025-02-03 22:15:55,256][0m Trial 4 finished with value: 1.3211513714617993 and parameters: {'observation_period_num': 143, 'train_rates': 0.7339179372654959, 'learning_rate': 0.0002568704701362376, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8079749694793708}. Best is trial 3 with value: 1.1074279023389098.[0m
[32m[I 2025-02-03 22:19:37,004][0m Trial 5 finished with value: 1.5030604296404382 and parameters: {'observation_period_num': 223, 'train_rates': 0.6629109358176044, 'learning_rate': 0.00018648679389233667, 'batch_size': 105, 'step_size': 12, 'gamma': 0.9420578119850501}. Best is trial 3 with value: 1.1074279023389098.[0m
[32m[I 2025-02-03 22:23:46,264][0m Trial 6 finished with value: 0.319041907787323 and parameters: {'observation_period_num': 202, 'train_rates': 0.967659690779928, 'learning_rate': 4.4497908863840576e-05, 'batch_size': 243, 'step_size': 13, 'gamma': 0.963567059043596}. Best is trial 6 with value: 0.319041907787323.[0m
[32m[I 2025-02-03 22:26:40,324][0m Trial 7 finished with value: 1.314890422673622 and parameters: {'observation_period_num': 188, 'train_rates': 0.6605153744626453, 'learning_rate': 2.344458935272601e-05, 'batch_size': 132, 'step_size': 4, 'gamma': 0.8957707620236732}. Best is trial 6 with value: 0.319041907787323.[0m
[32m[I 2025-02-03 22:29:40,707][0m Trial 8 finished with value: 1.0646261144022755 and parameters: {'observation_period_num': 187, 'train_rates': 0.6686732743417411, 'learning_rate': 7.126628712630138e-05, 'batch_size': 124, 'step_size': 9, 'gamma': 0.97378981789023}. Best is trial 6 with value: 0.319041907787323.[0m
[32m[I 2025-02-03 22:31:29,613][0m Trial 9 finished with value: 0.7745923700043522 and parameters: {'observation_period_num': 110, 'train_rates': 0.7999226046344572, 'learning_rate': 1.636163357044505e-05, 'batch_size': 89, 'step_size': 8, 'gamma': 0.8755303436212206}. Best is trial 6 with value: 0.319041907787323.[0m
[32m[I 2025-02-03 22:34:35,191][0m Trial 10 finished with value: 0.13521119952201843 and parameters: {'observation_period_num': 153, 'train_rates': 0.983133562189056, 'learning_rate': 0.0005613067944907154, 'batch_size': 250, 'step_size': 6, 'gamma': 0.9884240854248634}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:37:37,446][0m Trial 11 finished with value: 0.14795465767383575 and parameters: {'observation_period_num': 153, 'train_rates': 0.9672134495336298, 'learning_rate': 0.0004000517356349646, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9872834800092594}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:40:20,932][0m Trial 12 finished with value: 0.24152055382728577 and parameters: {'observation_period_num': 144, 'train_rates': 0.9668330102785438, 'learning_rate': 0.0009817447134676568, 'batch_size': 241, 'step_size': 6, 'gamma': 0.7501756737777602}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:45:35,757][0m Trial 13 finished with value: 0.2923389385853495 and parameters: {'observation_period_num': 247, 'train_rates': 0.8967327187217732, 'learning_rate': 0.0009035690753427742, 'batch_size': 214, 'step_size': 6, 'gamma': 0.9884516186141149}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:46:54,993][0m Trial 14 finished with value: 0.27423104161725326 and parameters: {'observation_period_num': 78, 'train_rates': 0.8817437829655784, 'learning_rate': 0.0002744682674274281, 'batch_size': 205, 'step_size': 2, 'gamma': 0.926027683548255}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:49:55,094][0m Trial 15 finished with value: 0.18274754285812378 and parameters: {'observation_period_num': 161, 'train_rates': 0.9129664873191657, 'learning_rate': 0.0005084226519878925, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9230439943899129}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:52:06,036][0m Trial 16 finished with value: 0.15244820713996887 and parameters: {'observation_period_num': 120, 'train_rates': 0.9897639858812035, 'learning_rate': 0.0001064101852398159, 'batch_size': 171, 'step_size': 1, 'gamma': 0.9869704896777706}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:55:05,347][0m Trial 17 finished with value: 1.4096329920927322 and parameters: {'observation_period_num': 170, 'train_rates': 0.8421054396737566, 'learning_rate': 7.832929827390277e-06, 'batch_size': 218, 'step_size': 4, 'gamma': 0.9128133095208808}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:56:18,208][0m Trial 18 finished with value: 1.2129106279102486 and parameters: {'observation_period_num': 87, 'train_rates': 0.6027982688551956, 'learning_rate': 0.00045012612247169903, 'batch_size': 158, 'step_size': 11, 'gamma': 0.9504967464353181}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:56:42,209][0m Trial 19 finished with value: 0.36062896251678467 and parameters: {'observation_period_num': 9, 'train_rates': 0.9357681042333195, 'learning_rate': 0.0001145782638625661, 'batch_size': 225, 'step_size': 7, 'gamma': 0.7551372362434816}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 22:59:05,680][0m Trial 20 finished with value: 0.2971046629519957 and parameters: {'observation_period_num': 136, 'train_rates': 0.8493469504284614, 'learning_rate': 0.00045564708333405444, 'batch_size': 191, 'step_size': 5, 'gamma': 0.8556215214675711}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:01:22,161][0m Trial 21 finished with value: 0.21855862438678741 and parameters: {'observation_period_num': 121, 'train_rates': 0.9877048874493705, 'learning_rate': 0.00013256384561986144, 'batch_size': 256, 'step_size': 1, 'gamma': 0.9832230091177501}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:04:11,382][0m Trial 22 finished with value: 0.21220676600933075 and parameters: {'observation_period_num': 148, 'train_rates': 0.9396320698979662, 'learning_rate': 7.487105078078508e-05, 'batch_size': 170, 'step_size': 2, 'gamma': 0.9892756941090122}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:06:23,649][0m Trial 23 finished with value: 0.174821138381958 and parameters: {'observation_period_num': 122, 'train_rates': 0.983863298624724, 'learning_rate': 0.00029099264697223583, 'batch_size': 228, 'step_size': 1, 'gamma': 0.9640305071114}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:07:50,465][0m Trial 24 finished with value: 0.16170941293239594 and parameters: {'observation_period_num': 82, 'train_rates': 0.9409098311911017, 'learning_rate': 0.0005960159952842146, 'batch_size': 197, 'step_size': 4, 'gamma': 0.9377953218768633}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:11:08,165][0m Trial 25 finished with value: 0.2924295773311537 and parameters: {'observation_period_num': 170, 'train_rates': 0.9119003354337776, 'learning_rate': 0.00013211247491735011, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9624327165362662}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:14:12,018][0m Trial 26 finished with value: 0.2362000048160553 and parameters: {'observation_period_num': 160, 'train_rates': 0.9503828398293144, 'learning_rate': 6.926555834076722e-05, 'batch_size': 151, 'step_size': 8, 'gamma': 0.8994988822844328}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:15:54,496][0m Trial 27 finished with value: 0.20797636798080713 and parameters: {'observation_period_num': 99, 'train_rates': 0.870719255693009, 'learning_rate': 0.00030056730614749593, 'batch_size': 179, 'step_size': 3, 'gamma': 0.971406996066269}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:17:55,590][0m Trial 28 finished with value: 0.42280948039363414 and parameters: {'observation_period_num': 125, 'train_rates': 0.8195046460077692, 'learning_rate': 0.0001901548208482479, 'batch_size': 231, 'step_size': 5, 'gamma': 0.9320626463486736}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:21:03,252][0m Trial 29 finished with value: 1.592055489697062 and parameters: {'observation_period_num': 185, 'train_rates': 0.7682184851602608, 'learning_rate': 5.569313632002935e-06, 'batch_size': 151, 'step_size': 5, 'gamma': 0.9429758749901035}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:25:39,107][0m Trial 30 finished with value: 0.20698429644107819 and parameters: {'observation_period_num': 226, 'train_rates': 0.9170331921849499, 'learning_rate': 0.0005547193992758572, 'batch_size': 241, 'step_size': 15, 'gamma': 0.9558284621040477}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:26:41,943][0m Trial 31 finished with value: 0.16263583302497864 and parameters: {'observation_period_num': 62, 'train_rates': 0.9543326822033135, 'learning_rate': 0.00069680962600054, 'batch_size': 202, 'step_size': 3, 'gamma': 0.9766091639138194}. Best is trial 10 with value: 0.13521119952201843.[0m
[32m[I 2025-02-03 23:27:47,144][0m Trial 32 finished with value: 0.12523140013217926 and parameters: {'observation_period_num': 64, 'train_rates': 0.9849288674611233, 'learning_rate': 0.0004018332755047402, 'batch_size': 199, 'step_size': 4, 'gamma': 0.9456745742042806}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:28:29,082][0m Trial 33 finished with value: 0.13027605414390564 and parameters: {'observation_period_num': 33, 'train_rates': 0.989643595278203, 'learning_rate': 0.0003720428085227977, 'batch_size': 178, 'step_size': 2, 'gamma': 0.9891645188206376}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:29:08,291][0m Trial 34 finished with value: 0.12715089321136475 and parameters: {'observation_period_num': 33, 'train_rates': 0.9661275038009985, 'learning_rate': 0.0003440797759195602, 'batch_size': 214, 'step_size': 7, 'gamma': 0.9532085156509662}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:29:46,929][0m Trial 35 finished with value: 1.666733571757441 and parameters: {'observation_period_num': 34, 'train_rates': 0.921252079162354, 'learning_rate': 1.2007700081082492e-06, 'batch_size': 187, 'step_size': 8, 'gamma': 0.9128933640644605}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:30:29,316][0m Trial 36 finished with value: 0.1695864498615265 and parameters: {'observation_period_num': 39, 'train_rates': 0.9659958167406927, 'learning_rate': 0.0001917399653621808, 'batch_size': 204, 'step_size': 2, 'gamma': 0.9560031336691996}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:30:53,611][0m Trial 37 finished with value: 0.26303109638392924 and parameters: {'observation_period_num': 7, 'train_rates': 0.8915795323773181, 'learning_rate': 0.0003400348298975827, 'batch_size': 212, 'step_size': 3, 'gamma': 0.827175683162208}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:31:30,949][0m Trial 38 finished with value: 0.2232116460800171 and parameters: {'observation_period_num': 31, 'train_rates': 0.9849890322085422, 'learning_rate': 0.0008070228167194295, 'batch_size': 186, 'step_size': 9, 'gamma': 0.7908010559947609}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:32:27,932][0m Trial 39 finished with value: 0.15143482069726344 and parameters: {'observation_period_num': 54, 'train_rates': 0.9256733158262934, 'learning_rate': 0.00021547838302411767, 'batch_size': 142, 'step_size': 7, 'gamma': 0.9469839715611272}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:33:09,519][0m Trial 40 finished with value: 0.2669028545381459 and parameters: {'observation_period_num': 20, 'train_rates': 0.9553870623847869, 'learning_rate': 3.6023712633786445e-05, 'batch_size': 124, 'step_size': 4, 'gamma': 0.9663244622769249}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:34:12,489][0m Trial 41 finished with value: 0.14174257218837738 and parameters: {'observation_period_num': 62, 'train_rates': 0.969171105070673, 'learning_rate': 0.0003736580308135045, 'batch_size': 238, 'step_size': 6, 'gamma': 0.9739087066217369}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:35:06,622][0m Trial 42 finished with value: 0.15917740762233734 and parameters: {'observation_period_num': 51, 'train_rates': 0.9721917002900716, 'learning_rate': 0.0003453240421329529, 'batch_size': 235, 'step_size': 5, 'gamma': 0.9729642768463663}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:36:18,159][0m Trial 43 finished with value: 0.2566804885864258 and parameters: {'observation_period_num': 67, 'train_rates': 0.9491055732903919, 'learning_rate': 0.0006576129722175029, 'batch_size': 220, 'step_size': 10, 'gamma': 0.976700644721962}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:37:04,910][0m Trial 44 finished with value: 0.14699146151542664 and parameters: {'observation_period_num': 45, 'train_rates': 0.9725242646619646, 'learning_rate': 0.0002509866277597754, 'batch_size': 246, 'step_size': 7, 'gamma': 0.9558488398493057}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:37:34,121][0m Trial 45 finished with value: 0.2784062922000885 and parameters: {'observation_period_num': 23, 'train_rates': 0.9310880467895996, 'learning_rate': 0.0009976129443042196, 'batch_size': 208, 'step_size': 6, 'gamma': 0.9370970290385764}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:39:52,254][0m Trial 46 finished with value: 0.1574670457664658 and parameters: {'observation_period_num': 71, 'train_rates': 0.9588697692275805, 'learning_rate': 0.00015906675955524937, 'batch_size': 35, 'step_size': 8, 'gamma': 0.9178829168611717}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:41:28,878][0m Trial 47 finished with value: 0.3464705686311464 and parameters: {'observation_period_num': 93, 'train_rates': 0.8965752648544593, 'learning_rate': 9.047043746064066e-05, 'batch_size': 236, 'step_size': 4, 'gamma': 0.8979408952607736}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:43:13,358][0m Trial 48 finished with value: 1.137619623632142 and parameters: {'observation_period_num': 109, 'train_rates': 0.7562559160231064, 'learning_rate': 2.3206303601044666e-05, 'batch_size': 177, 'step_size': 5, 'gamma': 0.9798790909453181}. Best is trial 32 with value: 0.12523140013217926.[0m
[32m[I 2025-02-03 23:43:36,999][0m Trial 49 finished with value: 0.8007644994466084 and parameters: {'observation_period_num': 21, 'train_rates': 0.6977737754456919, 'learning_rate': 0.00041361725732498106, 'batch_size': 225, 'step_size': 2, 'gamma': 0.8858040723387209}. Best is trial 32 with value: 0.12523140013217926.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-03 23:43:37,006][0m A new study created in memory with name: no-name-e1457af0-5993-4e1a-bd03-4c66daf21330[0m
[32m[I 2025-02-03 23:45:30,321][0m Trial 0 finished with value: 2.2128853706213145 and parameters: {'observation_period_num': 115, 'train_rates': 0.7616711559085276, 'learning_rate': 1.0381301230842355e-06, 'batch_size': 110, 'step_size': 4, 'gamma': 0.7565637812992975}. Best is trial 0 with value: 2.2128853706213145.[0m
[32m[I 2025-02-03 23:49:29,275][0m Trial 1 finished with value: 0.2865187693912781 and parameters: {'observation_period_num': 173, 'train_rates': 0.8852989821187904, 'learning_rate': 0.00012271801948898553, 'batch_size': 21, 'step_size': 1, 'gamma': 0.8825875197181177}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-03 23:51:13,373][0m Trial 2 finished with value: 1.0842235894061236 and parameters: {'observation_period_num': 102, 'train_rates': 0.7112747423634811, 'learning_rate': 6.674222543225406e-06, 'batch_size': 45, 'step_size': 15, 'gamma': 0.9817562836517254}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-03 23:52:14,655][0m Trial 3 finished with value: 1.0652339431048536 and parameters: {'observation_period_num': 71, 'train_rates': 0.6225546560705411, 'learning_rate': 0.0002527478812662452, 'batch_size': 191, 'step_size': 5, 'gamma': 0.9394999095954635}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-03 23:52:38,582][0m Trial 4 finished with value: 1.011593134626837 and parameters: {'observation_period_num': 18, 'train_rates': 0.720299249710942, 'learning_rate': 3.946183149664986e-05, 'batch_size': 186, 'step_size': 12, 'gamma': 0.7852808940714548}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-03 23:54:00,298][0m Trial 5 finished with value: 1.138416221802466 and parameters: {'observation_period_num': 95, 'train_rates': 0.6284376664649052, 'learning_rate': 0.0002579226679397027, 'batch_size': 153, 'step_size': 8, 'gamma': 0.885794822849431}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-03 23:57:25,140][0m Trial 6 finished with value: 2.005249859649183 and parameters: {'observation_period_num': 197, 'train_rates': 0.7026925279161058, 'learning_rate': 5.99688196831486e-06, 'batch_size': 114, 'step_size': 4, 'gamma': 0.8507088058357171}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-03 23:58:15,137][0m Trial 7 finished with value: 1.4341998932871083 and parameters: {'observation_period_num': 56, 'train_rates': 0.6266826812203805, 'learning_rate': 2.424750440574453e-05, 'batch_size': 90, 'step_size': 4, 'gamma': 0.8847445214180963}. Best is trial 1 with value: 0.2865187693912781.[0m
[32m[I 2025-02-04 00:03:35,805][0m Trial 8 finished with value: 0.22961042821407318 and parameters: {'observation_period_num': 245, 'train_rates': 0.9659810742343832, 'learning_rate': 5.618129194954376e-05, 'batch_size': 189, 'step_size': 7, 'gamma': 0.9572402551601744}. Best is trial 8 with value: 0.22961042821407318.[0m
[32m[I 2025-02-04 00:05:09,044][0m Trial 9 finished with value: 0.16755380133404235 and parameters: {'observation_period_num': 76, 'train_rates': 0.910130829592606, 'learning_rate': 5.808553385301902e-05, 'batch_size': 54, 'step_size': 7, 'gamma': 0.8744291989074573}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:05:30,036][0m Trial 10 finished with value: 0.6309378125318666 and parameters: {'observation_period_num': 6, 'train_rates': 0.8609364865993163, 'learning_rate': 0.000979627473296617, 'batch_size': 253, 'step_size': 11, 'gamma': 0.8325112264976068}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:08:35,609][0m Trial 11 finished with value: 0.3181532025337219 and parameters: {'observation_period_num': 158, 'train_rates': 0.9862816095196241, 'learning_rate': 4.645807056148357e-05, 'batch_size': 235, 'step_size': 8, 'gamma': 0.9642769034253046}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:13:53,468][0m Trial 12 finished with value: 0.4043795168399811 and parameters: {'observation_period_num': 235, 'train_rates': 0.9842964297763898, 'learning_rate': 1.3852990919988085e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.9235568504727194}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:19:01,979][0m Trial 13 finished with value: 0.21609621993624248 and parameters: {'observation_period_num': 248, 'train_rates': 0.9041057961669503, 'learning_rate': 9.642371994430419e-05, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9263517338982652}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:21:47,347][0m Trial 14 finished with value: 0.22642960666485554 and parameters: {'observation_period_num': 150, 'train_rates': 0.8865170553334404, 'learning_rate': 0.0001394032617343177, 'batch_size': 138, 'step_size': 11, 'gamma': 0.9156545720409003}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:22:39,012][0m Trial 15 finished with value: 0.42691998432079953 and parameters: {'observation_period_num': 51, 'train_rates': 0.8304846135905299, 'learning_rate': 0.0009139350103747355, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8303238876113946}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:26:52,866][0m Trial 16 finished with value: 0.17480342413456934 and parameters: {'observation_period_num': 204, 'train_rates': 0.9229100623517132, 'learning_rate': 0.00014458807255360873, 'batch_size': 78, 'step_size': 10, 'gamma': 0.9094834897064108}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:31:18,359][0m Trial 17 finished with value: 0.21081538001696268 and parameters: {'observation_period_num': 210, 'train_rates': 0.9345174810404631, 'learning_rate': 0.0003990706338966558, 'batch_size': 60, 'step_size': 9, 'gamma': 0.8093674200908313}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:34:38,304][0m Trial 18 finished with value: 0.6089314289044502 and parameters: {'observation_period_num': 132, 'train_rates': 0.8169569684283743, 'learning_rate': 1.3815569041810699e-05, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8606888486702481}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:38:31,745][0m Trial 19 finished with value: 1.1104012519284023 and parameters: {'observation_period_num': 191, 'train_rates': 0.9338192438185076, 'learning_rate': 2.1786345206566704e-06, 'batch_size': 83, 'step_size': 6, 'gamma': 0.9016567525822246}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:40:07,807][0m Trial 20 finished with value: 0.6640717795020655 and parameters: {'observation_period_num': 82, 'train_rates': 0.7816688408960367, 'learning_rate': 7.345778431566104e-05, 'batch_size': 44, 'step_size': 15, 'gamma': 0.8475645539962819}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:44:30,284][0m Trial 21 finished with value: 0.18499919443681676 and parameters: {'observation_period_num': 207, 'train_rates': 0.9323940420742498, 'learning_rate': 0.0002922613630814075, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8060262941997084}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:49:05,613][0m Trial 22 finished with value: 0.16864417797952727 and parameters: {'observation_period_num': 218, 'train_rates': 0.933861395805792, 'learning_rate': 0.0003782294093621931, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8035633636609864}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:53:26,780][0m Trial 23 finished with value: 0.3321702060606194 and parameters: {'observation_period_num': 223, 'train_rates': 0.8500901052103009, 'learning_rate': 0.000141026648223114, 'batch_size': 106, 'step_size': 10, 'gamma': 0.756830952722227}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:56:54,445][0m Trial 24 finished with value: 0.2682886541616626 and parameters: {'observation_period_num': 177, 'train_rates': 0.911454144027301, 'learning_rate': 0.0005154795627028643, 'batch_size': 81, 'step_size': 13, 'gamma': 0.9050625245396932}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 00:59:39,368][0m Trial 25 finished with value: 0.44183221459388733 and parameters: {'observation_period_num': 142, 'train_rates': 0.9602556247544582, 'learning_rate': 2.4078006527582815e-05, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8741245988297961}. Best is trial 9 with value: 0.16755380133404235.[0m
Early stopping at epoch 62
[32m[I 2025-02-04 01:02:35,028][0m Trial 26 finished with value: 0.6224818548686067 and parameters: {'observation_period_num': 225, 'train_rates': 0.8698039282776507, 'learning_rate': 0.00020076033902150138, 'batch_size': 94, 'step_size': 1, 'gamma': 0.8088683604097128}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 01:03:44,377][0m Trial 27 finished with value: 0.16884921149661145 and parameters: {'observation_period_num': 37, 'train_rates': 0.9508349614838137, 'learning_rate': 0.0005779183515796545, 'batch_size': 71, 'step_size': 10, 'gamma': 0.7809816493770636}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 01:06:11,882][0m Trial 28 finished with value: 0.3004833108339554 and parameters: {'observation_period_num': 39, 'train_rates': 0.9599055878851157, 'learning_rate': 0.0005471669892609778, 'batch_size': 33, 'step_size': 13, 'gamma': 0.7800195374631262}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 01:07:58,211][0m Trial 29 finished with value: 1.136317961712392 and parameters: {'observation_period_num': 113, 'train_rates': 0.7554682464832764, 'learning_rate': 0.0006383019914131308, 'batch_size': 101, 'step_size': 7, 'gamma': 0.7544541124956079}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 01:08:34,872][0m Trial 30 finished with value: 0.6260908735104096 and parameters: {'observation_period_num': 32, 'train_rates': 0.82626758558708, 'learning_rate': 0.00038834952529780777, 'batch_size': 130, 'step_size': 2, 'gamma': 0.7780984394750223}. Best is trial 9 with value: 0.16755380133404235.[0m
[32m[I 2025-02-04 01:09:46,499][0m Trial 31 finished with value: 0.13604210856241908 and parameters: {'observation_period_num': 62, 'train_rates': 0.914970831545453, 'learning_rate': 0.00020013922255218691, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8218093051049652}. Best is trial 31 with value: 0.13604210856241908.[0m
[32m[I 2025-02-04 01:11:15,460][0m Trial 32 finished with value: 0.20380534097071618 and parameters: {'observation_period_num': 73, 'train_rates': 0.8924909440264402, 'learning_rate': 8.254500218122966e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.8258422695987276}. Best is trial 31 with value: 0.13604210856241908.[0m
[32m[I 2025-02-04 01:13:25,820][0m Trial 33 finished with value: 0.12153204740493889 and parameters: {'observation_period_num': 54, 'train_rates': 0.9485637682521927, 'learning_rate': 0.00018943127353505577, 'batch_size': 37, 'step_size': 12, 'gamma': 0.7956697416547664}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:15:46,532][0m Trial 34 finished with value: 0.156862766105671 and parameters: {'observation_period_num': 60, 'train_rates': 0.8794998011940101, 'learning_rate': 0.0001827435933727503, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8004336099458123}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:18:02,084][0m Trial 35 finished with value: 0.16677006786195642 and parameters: {'observation_period_num': 61, 'train_rates': 0.8787305161776855, 'learning_rate': 0.0001887265697691609, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8459720165136791}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:20:14,620][0m Trial 36 finished with value: 0.16403293906816815 and parameters: {'observation_period_num': 61, 'train_rates': 0.8752504429708328, 'learning_rate': 0.0001972003143733333, 'batch_size': 34, 'step_size': 15, 'gamma': 0.7937899148844023}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:24:12,116][0m Trial 37 finished with value: 0.25585510860258176 and parameters: {'observation_period_num': 99, 'train_rates': 0.8501886569923421, 'learning_rate': 0.00010584178212501122, 'batch_size': 18, 'step_size': 15, 'gamma': 0.7980005688411023}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:26:08,537][0m Trial 38 finished with value: 0.6130823610644591 and parameters: {'observation_period_num': 16, 'train_rates': 0.7734821140641881, 'learning_rate': 0.00018404565398924698, 'batch_size': 35, 'step_size': 14, 'gamma': 0.7703996266163553}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:30:31,563][0m Trial 39 finished with value: 0.16194500202356382 and parameters: {'observation_period_num': 88, 'train_rates': 0.8949526968986006, 'learning_rate': 0.00027644862663697375, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8164615539299228}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:34:18,107][0m Trial 40 finished with value: 1.3932744022998087 and parameters: {'observation_period_num': 86, 'train_rates': 0.6866254631442441, 'learning_rate': 0.0003071606695761231, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8182019215388526}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:36:02,405][0m Trial 41 finished with value: 0.14718087945158118 and parameters: {'observation_period_num': 64, 'train_rates': 0.8954987703182271, 'learning_rate': 0.00020313653827278843, 'batch_size': 44, 'step_size': 14, 'gamma': 0.7932416850738364}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:38:17,113][0m Trial 42 finished with value: 0.14051184845822198 and parameters: {'observation_period_num': 113, 'train_rates': 0.9014071896094419, 'learning_rate': 0.0002538225035850885, 'batch_size': 44, 'step_size': 12, 'gamma': 0.7660821593288067}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:40:28,424][0m Trial 43 finished with value: 0.19844902797443112 and parameters: {'observation_period_num': 111, 'train_rates': 0.9136462808484762, 'learning_rate': 3.1872717736164944e-05, 'batch_size': 45, 'step_size': 14, 'gamma': 0.7686439088714514}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:42:08,698][0m Trial 44 finished with value: 0.1279451091950004 and parameters: {'observation_period_num': 48, 'train_rates': 0.9747233516385624, 'learning_rate': 6.63298972485387e-05, 'batch_size': 50, 'step_size': 12, 'gamma': 0.7927553890572785}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:43:50,569][0m Trial 45 finished with value: 0.13478765047308225 and parameters: {'observation_period_num': 45, 'train_rates': 0.9772112583530039, 'learning_rate': 6.486198912818648e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.7899472085180143}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:45:18,359][0m Trial 46 finished with value: 0.14616893819520171 and parameters: {'observation_period_num': 47, 'train_rates': 0.9756361320718927, 'learning_rate': 5.833342519207193e-05, 'batch_size': 57, 'step_size': 12, 'gamma': 0.7668429583390357}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:46:50,753][0m Trial 47 finished with value: 0.1708519313927297 and parameters: {'observation_period_num': 21, 'train_rates': 0.9514372982485354, 'learning_rate': 4.167419334020158e-05, 'batch_size': 53, 'step_size': 11, 'gamma': 0.789401757195668}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:48:00,437][0m Trial 48 finished with value: 0.14957232773303986 and parameters: {'observation_period_num': 26, 'train_rates': 0.9881997401595918, 'learning_rate': 6.636110172807415e-05, 'batch_size': 72, 'step_size': 12, 'gamma': 0.8384663420578249}. Best is trial 33 with value: 0.12153204740493889.[0m
[32m[I 2025-02-04 01:48:57,281][0m Trial 49 finished with value: 0.13565443390673335 and parameters: {'observation_period_num': 10, 'train_rates': 0.9690710436502687, 'learning_rate': 0.00010337792602803562, 'batch_size': 88, 'step_size': 11, 'gamma': 0.7660673349909749}. Best is trial 33 with value: 0.12153204740493889.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-04 01:48:57,288][0m A new study created in memory with name: no-name-180f85f6-89cf-482c-8d72-ee029d47b6ca[0m
[32m[I 2025-02-04 01:50:34,703][0m Trial 0 finished with value: 1.2546780043306982 and parameters: {'observation_period_num': 119, 'train_rates': 0.6371396357058743, 'learning_rate': 0.0005509940639635857, 'batch_size': 188, 'step_size': 10, 'gamma': 0.8765277438831707}. Best is trial 0 with value: 1.2546780043306982.[0m
[32m[I 2025-02-04 01:51:07,068][0m Trial 1 finished with value: 0.9593064370362655 and parameters: {'observation_period_num': 17, 'train_rates': 0.7963807189396046, 'learning_rate': 1.241322404083437e-05, 'batch_size': 143, 'step_size': 10, 'gamma': 0.8232101995124183}. Best is trial 1 with value: 0.9593064370362655.[0m
Early stopping at epoch 93
[32m[I 2025-02-04 01:51:36,460][0m Trial 2 finished with value: 1.2792875467614735 and parameters: {'observation_period_num': 38, 'train_rates': 0.6334989188427971, 'learning_rate': 0.00034696777371062226, 'batch_size': 242, 'step_size': 1, 'gamma': 0.8625868211701804}. Best is trial 1 with value: 0.9593064370362655.[0m
[32m[I 2025-02-04 01:54:31,295][0m Trial 3 finished with value: 0.7943391222874203 and parameters: {'observation_period_num': 169, 'train_rates': 0.7632940606345227, 'learning_rate': 3.2227873280225946e-05, 'batch_size': 67, 'step_size': 10, 'gamma': 0.9447058898427461}. Best is trial 3 with value: 0.7943391222874203.[0m
[32m[I 2025-02-04 01:58:06,074][0m Trial 4 finished with value: 0.31767450496108235 and parameters: {'observation_period_num': 190, 'train_rates': 0.8793185186253772, 'learning_rate': 0.0005079659372537715, 'batch_size': 115, 'step_size': 14, 'gamma': 0.9108932140189971}. Best is trial 4 with value: 0.31767450496108235.[0m
[32m[I 2025-02-04 02:01:44,987][0m Trial 5 finished with value: 1.5721164940415921 and parameters: {'observation_period_num': 208, 'train_rates': 0.6832539148618538, 'learning_rate': 0.000416108857644198, 'batch_size': 32, 'step_size': 11, 'gamma': 0.923428151330032}. Best is trial 4 with value: 0.31767450496108235.[0m
[32m[I 2025-02-04 02:02:56,088][0m Trial 6 finished with value: 0.1644413115551872 and parameters: {'observation_period_num': 69, 'train_rates': 0.9049372282236, 'learning_rate': 0.00029265488205563794, 'batch_size': 190, 'step_size': 11, 'gamma': 0.8590764429133542}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:05:07,328][0m Trial 7 finished with value: 1.662425982878683 and parameters: {'observation_period_num': 148, 'train_rates': 0.6525580135002647, 'learning_rate': 2.2614348629998293e-06, 'batch_size': 193, 'step_size': 11, 'gamma': 0.975061301690282}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:07:44,211][0m Trial 8 finished with value: 1.0662845242265109 and parameters: {'observation_period_num': 167, 'train_rates': 0.6012016405077135, 'learning_rate': 8.685166890109699e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8469565839493297}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:09:48,304][0m Trial 9 finished with value: 0.22306522347179114 and parameters: {'observation_period_num': 121, 'train_rates': 0.855948838047486, 'learning_rate': 0.00041171384854598534, 'batch_size': 229, 'step_size': 9, 'gamma': 0.8561540990113826}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:11:06,795][0m Trial 10 finished with value: 0.451210081577301 and parameters: {'observation_period_num': 72, 'train_rates': 0.988358052838676, 'learning_rate': 8.821542288080135e-05, 'batch_size': 159, 'step_size': 5, 'gamma': 0.7565916062116411}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:12:45,957][0m Trial 11 finished with value: 0.18259677369343608 and parameters: {'observation_period_num': 97, 'train_rates': 0.9068102223448333, 'learning_rate': 0.0009854486880653945, 'batch_size': 249, 'step_size': 6, 'gamma': 0.7980622528113043}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:14:03,732][0m Trial 12 finished with value: 0.3491978943347931 and parameters: {'observation_period_num': 74, 'train_rates': 0.9612715132769833, 'learning_rate': 0.00012056441775641723, 'batch_size': 256, 'step_size': 6, 'gamma': 0.7891121440897344}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:19:04,654][0m Trial 13 finished with value: 0.24896488412255383 and parameters: {'observation_period_num': 242, 'train_rates': 0.914194908507038, 'learning_rate': 0.0009698556088660655, 'batch_size': 207, 'step_size': 6, 'gamma': 0.8034788344053866}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:20:32,609][0m Trial 14 finished with value: 0.47144215484236957 and parameters: {'observation_period_num': 87, 'train_rates': 0.8137713930366892, 'learning_rate': 0.00015300238165488064, 'batch_size': 170, 'step_size': 13, 'gamma': 0.7556147990564995}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:21:25,339][0m Trial 15 finished with value: 0.8257403505885083 and parameters: {'observation_period_num': 49, 'train_rates': 0.9209127459731067, 'learning_rate': 1.8734822565808087e-05, 'batch_size': 221, 'step_size': 3, 'gamma': 0.8921909548748435}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:22:57,219][0m Trial 16 finished with value: 2.1865308155552032 and parameters: {'observation_period_num': 100, 'train_rates': 0.7282342162319234, 'learning_rate': 1.3067040765545032e-06, 'batch_size': 108, 'step_size': 7, 'gamma': 0.8151641311719019}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:23:21,772][0m Trial 17 finished with value: 0.36657082019449905 and parameters: {'observation_period_num': 14, 'train_rates': 0.8751676328386652, 'learning_rate': 0.000898494865124358, 'batch_size': 214, 'step_size': 15, 'gamma': 0.7833648758594013}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:24:18,660][0m Trial 18 finished with value: 0.45448994636535645 and parameters: {'observation_period_num': 62, 'train_rates': 0.833997261647841, 'learning_rate': 0.00019083973680483076, 'batch_size': 251, 'step_size': 4, 'gamma': 0.8370590195880484}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:26:04,052][0m Trial 19 finished with value: 0.40193428946476356 and parameters: {'observation_period_num': 102, 'train_rates': 0.9285185228865687, 'learning_rate': 4.1258260701048955e-05, 'batch_size': 178, 'step_size': 8, 'gamma': 0.884237127616534}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:28:25,860][0m Trial 20 finished with value: 1.6496694596027857 and parameters: {'observation_period_num': 144, 'train_rates': 0.7659932714390726, 'learning_rate': 4.19853121150956e-06, 'batch_size': 139, 'step_size': 12, 'gamma': 0.7819368418186013}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:30:15,722][0m Trial 21 finished with value: 0.26562496176235695 and parameters: {'observation_period_num': 112, 'train_rates': 0.8590428410201956, 'learning_rate': 0.0002973642046091715, 'batch_size': 221, 'step_size': 8, 'gamma': 0.8545376063404754}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:32:44,918][0m Trial 22 finished with value: 0.18564442222687735 and parameters: {'observation_period_num': 135, 'train_rates': 0.9049972816256012, 'learning_rate': 0.0002233920734897128, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8362939829181735}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:35:16,437][0m Trial 23 finished with value: 0.22310179046222142 and parameters: {'observation_period_num': 138, 'train_rates': 0.9054437642485416, 'learning_rate': 0.0002166413963203183, 'batch_size': 236, 'step_size': 7, 'gamma': 0.831050404176718}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:36:52,123][0m Trial 24 finished with value: 0.3081592619419098 and parameters: {'observation_period_num': 90, 'train_rates': 0.9567292265269832, 'learning_rate': 6.337853543879141e-05, 'batch_size': 197, 'step_size': 12, 'gamma': 0.802867257452102}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:37:40,977][0m Trial 25 finished with value: 0.2799451947212219 and parameters: {'observation_period_num': 46, 'train_rates': 0.9474679615040851, 'learning_rate': 0.0007382439857550781, 'batch_size': 210, 'step_size': 2, 'gamma': 0.8964613106458335}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:39:03,802][0m Trial 26 finished with value: 0.2674866372713379 and parameters: {'observation_period_num': 81, 'train_rates': 0.8925028934556165, 'learning_rate': 0.00023766933372125555, 'batch_size': 232, 'step_size': 5, 'gamma': 0.8066326420799533}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:42:10,350][0m Trial 27 finished with value: 0.6650325059890747 and parameters: {'observation_period_num': 157, 'train_rates': 0.982751455466903, 'learning_rate': 1.5457500169257558e-05, 'batch_size': 159, 'step_size': 9, 'gamma': 0.8297607899269487}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:44:13,761][0m Trial 28 finished with value: 0.31270259593012006 and parameters: {'observation_period_num': 128, 'train_rates': 0.8359627000885529, 'learning_rate': 0.0006474546749470394, 'batch_size': 239, 'step_size': 7, 'gamma': 0.7655940268153378}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:46:13,460][0m Trial 29 finished with value: 0.82069331407547 and parameters: {'observation_period_num': 113, 'train_rates': 0.941748647923535, 'learning_rate': 8.010315822474179e-06, 'batch_size': 188, 'step_size': 9, 'gamma': 0.8721131314150777}. Best is trial 6 with value: 0.1644413115551872.[0m
[32m[I 2025-02-04 02:46:49,152][0m Trial 30 finished with value: 0.14333828341289861 and parameters: {'observation_period_num': 33, 'train_rates': 0.896525622484928, 'learning_rate': 0.0005813386190762491, 'batch_size': 255, 'step_size': 13, 'gamma': 0.8429320031721051}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:47:18,431][0m Trial 31 finished with value: 0.15436535400070556 and parameters: {'observation_period_num': 26, 'train_rates': 0.9004212770863211, 'learning_rate': 0.0006188381039155361, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8471930209563493}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:47:51,690][0m Trial 32 finished with value: 0.19633102704243488 and parameters: {'observation_period_num': 29, 'train_rates': 0.8581739974295383, 'learning_rate': 0.0005785532014321703, 'batch_size': 250, 'step_size': 15, 'gamma': 0.8469120674444761}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:48:19,396][0m Trial 33 finished with value: 0.14765993505716324 and parameters: {'observation_period_num': 24, 'train_rates': 0.8867256198611787, 'learning_rate': 0.00035176551903456785, 'batch_size': 249, 'step_size': 13, 'gamma': 0.8724938304135348}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:48:42,115][0m Trial 34 finished with value: 0.508321898835187 and parameters: {'observation_period_num': 6, 'train_rates': 0.7951656728058439, 'learning_rate': 0.00034260831117404746, 'batch_size': 205, 'step_size': 13, 'gamma': 0.8738848899350281}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:49:20,314][0m Trial 35 finished with value: 0.42557041489075276 and parameters: {'observation_period_num': 29, 'train_rates': 0.8170137778677057, 'learning_rate': 0.0001329654652635234, 'batch_size': 122, 'step_size': 14, 'gamma': 0.9104250131005159}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:50:24,128][0m Trial 36 finished with value: 0.24593940151878346 and parameters: {'observation_period_num': 59, 'train_rates': 0.8880540895187244, 'learning_rate': 0.0005184560552070367, 'batch_size': 89, 'step_size': 13, 'gamma': 0.935612001298385}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:51:05,890][0m Trial 37 finished with value: 0.1934033481664555 and parameters: {'observation_period_num': 41, 'train_rates': 0.8721363425736665, 'learning_rate': 0.00035642117089645, 'batch_size': 256, 'step_size': 14, 'gamma': 0.883664860140551}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:51:36,303][0m Trial 38 finished with value: 0.7753985902915398 and parameters: {'observation_period_num': 27, 'train_rates': 0.7702975984268011, 'learning_rate': 0.0005581153194637163, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8664235703131589}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:51:59,132][0m Trial 39 finished with value: 0.316956284195788 and parameters: {'observation_period_num': 6, 'train_rates': 0.8325063971221041, 'learning_rate': 0.00034930716191295254, 'batch_size': 220, 'step_size': 11, 'gamma': 0.9041023499857154}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:52:27,296][0m Trial 40 finished with value: 0.3501725196838379 and parameters: {'observation_period_num': 23, 'train_rates': 0.9316346354757878, 'learning_rate': 5.2417095561957e-05, 'batch_size': 242, 'step_size': 15, 'gamma': 0.8496238560327792}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:53:23,021][0m Trial 41 finished with value: 0.2014048144978992 and parameters: {'observation_period_num': 58, 'train_rates': 0.8981411589629944, 'learning_rate': 0.0008571191981785336, 'batch_size': 245, 'step_size': 13, 'gamma': 0.8161339432625598}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:54:01,668][0m Trial 42 finished with value: 0.1745919703778641 and parameters: {'observation_period_num': 37, 'train_rates': 0.8757441731546963, 'learning_rate': 0.0006828636090387447, 'batch_size': 230, 'step_size': 11, 'gamma': 0.8672254847979589}. Best is trial 30 with value: 0.14333828341289861.[0m
[32m[I 2025-02-04 02:54:48,537][0m Trial 43 finished with value: 0.13619239628314972 and parameters: {'observation_period_num': 44, 'train_rates': 0.966585006624815, 'learning_rate': 0.000464545295608573, 'batch_size': 229, 'step_size': 11, 'gamma': 0.8664841736662167}. Best is trial 43 with value: 0.13619239628314972.[0m
[32m[I 2025-02-04 02:55:42,837][0m Trial 44 finished with value: 0.15000887215137482 and parameters: {'observation_period_num': 51, 'train_rates': 0.9727392123386724, 'learning_rate': 0.0004626754123404563, 'batch_size': 241, 'step_size': 11, 'gamma': 0.8586429071339211}. Best is trial 43 with value: 0.13619239628314972.[0m
[32m[I 2025-02-04 02:56:05,476][0m Trial 45 finished with value: 0.11131373047828674 and parameters: {'observation_period_num': 14, 'train_rates': 0.9778110643740422, 'learning_rate': 0.0004605197739807708, 'batch_size': 241, 'step_size': 12, 'gamma': 0.8448936491492371}. Best is trial 45 with value: 0.11131373047828674.[0m
[32m[I 2025-02-04 02:56:27,867][0m Trial 46 finished with value: 0.20102821290493011 and parameters: {'observation_period_num': 13, 'train_rates': 0.970034859602992, 'learning_rate': 9.927105702035575e-05, 'batch_size': 239, 'step_size': 10, 'gamma': 0.8814332324426749}. Best is trial 45 with value: 0.11131373047828674.[0m
[32m[I 2025-02-04 02:58:48,154][0m Trial 47 finished with value: 0.16184931745131811 and parameters: {'observation_period_num': 52, 'train_rates': 0.9742282713797972, 'learning_rate': 0.00042999030143418734, 'batch_size': 35, 'step_size': 12, 'gamma': 0.8586221129699447}. Best is trial 45 with value: 0.11131373047828674.[0m
[32m[I 2025-02-04 02:59:29,894][0m Trial 48 finished with value: 0.1428772658109665 and parameters: {'observation_period_num': 35, 'train_rates': 0.9877374294455525, 'learning_rate': 0.00017344628867893596, 'batch_size': 220, 'step_size': 11, 'gamma': 0.9661632770964798}. Best is trial 45 with value: 0.11131373047828674.[0m
[32m[I 2025-02-04 03:00:14,899][0m Trial 49 finished with value: 0.1411929726600647 and parameters: {'observation_period_num': 40, 'train_rates': 0.9876178701448576, 'learning_rate': 0.00015381498633460845, 'batch_size': 204, 'step_size': 10, 'gamma': 0.9597048911004178}. Best is trial 45 with value: 0.11131373047828674.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-04 03:00:14,906][0m A new study created in memory with name: no-name-52151507-6d0c-40bb-8f88-4fd420ec0def[0m
[32m[I 2025-02-04 03:03:07,124][0m Trial 0 finished with value: 1.5443755512132622 and parameters: {'observation_period_num': 182, 'train_rates': 0.7044934666024976, 'learning_rate': 7.674083810424378e-06, 'batch_size': 199, 'step_size': 11, 'gamma': 0.9686096083540339}. Best is trial 0 with value: 1.5443755512132622.[0m
[32m[I 2025-02-04 03:04:27,433][0m Trial 1 finished with value: 1.2607835988564924 and parameters: {'observation_period_num': 86, 'train_rates': 0.7119708522849733, 'learning_rate': 0.00042222903283912686, 'batch_size': 93, 'step_size': 6, 'gamma': 0.8842198185092582}. Best is trial 1 with value: 1.2607835988564924.[0m
[32m[I 2025-02-04 03:06:56,363][0m Trial 2 finished with value: 1.4718518197536468 and parameters: {'observation_period_num': 145, 'train_rates': 0.7721226424815637, 'learning_rate': 4.130838362126352e-06, 'batch_size': 194, 'step_size': 4, 'gamma': 0.9778661540079988}. Best is trial 1 with value: 1.2607835988564924.[0m
[32m[I 2025-02-04 03:07:49,772][0m Trial 3 finished with value: 1.015847449863914 and parameters: {'observation_period_num': 35, 'train_rates': 0.7183452982260898, 'learning_rate': 0.00015044396969976213, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9059546040072641}. Best is trial 3 with value: 1.015847449863914.[0m
[32m[I 2025-02-04 03:13:07,667][0m Trial 4 finished with value: 0.42404958564829043 and parameters: {'observation_period_num': 246, 'train_rates': 0.831889207697053, 'learning_rate': 6.959333099773383e-05, 'batch_size': 25, 'step_size': 12, 'gamma': 0.862897719181269}. Best is trial 4 with value: 0.42404958564829043.[0m
[32m[I 2025-02-04 03:15:51,597][0m Trial 5 finished with value: 1.901127381351607 and parameters: {'observation_period_num': 188, 'train_rates': 0.6141053706710878, 'learning_rate': 3.6767929931764988e-06, 'batch_size': 168, 'step_size': 13, 'gamma': 0.901236249241228}. Best is trial 4 with value: 0.42404958564829043.[0m
[32m[I 2025-02-04 03:17:32,072][0m Trial 6 finished with value: 1.3929909232430462 and parameters: {'observation_period_num': 125, 'train_rates': 0.6254800400063639, 'learning_rate': 0.00011505093764980183, 'batch_size': 212, 'step_size': 7, 'gamma': 0.8035183886475022}. Best is trial 4 with value: 0.42404958564829043.[0m
[32m[I 2025-02-04 03:18:49,339][0m Trial 7 finished with value: 1.5484614199902638 and parameters: {'observation_period_num': 77, 'train_rates': 0.6822761893646093, 'learning_rate': 0.0002960086255169945, 'batch_size': 70, 'step_size': 15, 'gamma': 0.7704578168799784}. Best is trial 4 with value: 0.42404958564829043.[0m
[32m[I 2025-02-04 03:19:12,432][0m Trial 8 finished with value: 0.7127715247612469 and parameters: {'observation_period_num': 13, 'train_rates': 0.8344475764495387, 'learning_rate': 3.639222233620223e-05, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8149248759867188}. Best is trial 4 with value: 0.42404958564829043.[0m
[32m[I 2025-02-04 03:19:48,807][0m Trial 9 finished with value: 2.0767668870938314 and parameters: {'observation_period_num': 36, 'train_rates': 0.8421327135187202, 'learning_rate': 1.7255634243849285e-06, 'batch_size': 210, 'step_size': 12, 'gamma': 0.8676889112936685}. Best is trial 4 with value: 0.42404958564829043.[0m
[32m[I 2025-02-04 03:26:11,836][0m Trial 10 finished with value: 0.2643496546149254 and parameters: {'observation_period_num': 250, 'train_rates': 0.9630922334113023, 'learning_rate': 3.221563297051515e-05, 'batch_size': 18, 'step_size': 2, 'gamma': 0.8201543015822546}. Best is trial 10 with value: 0.2643496546149254.[0m
[32m[I 2025-02-04 03:32:30,476][0m Trial 11 finished with value: 0.30089231941503347 and parameters: {'observation_period_num': 248, 'train_rates': 0.9644249728521066, 'learning_rate': 2.6014271228677046e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.836876774397151}. Best is trial 10 with value: 0.2643496546149254.[0m
Early stopping at epoch 60
[32m[I 2025-02-04 03:36:15,913][0m Trial 12 finished with value: 0.890285214272941 and parameters: {'observation_period_num': 252, 'train_rates': 0.9851443104631317, 'learning_rate': 1.6298907268904653e-05, 'batch_size': 24, 'step_size': 1, 'gamma': 0.8274434874224329}. Best is trial 10 with value: 0.2643496546149254.[0m
Early stopping at epoch 43
[32m[I 2025-02-04 03:38:15,331][0m Trial 13 finished with value: 1.4811203479766846 and parameters: {'observation_period_num': 212, 'train_rates': 0.9849836765768633, 'learning_rate': 1.931508870681048e-05, 'batch_size': 124, 'step_size': 1, 'gamma': 0.7524808442392299}. Best is trial 10 with value: 0.2643496546149254.[0m
[32m[I 2025-02-04 03:42:59,105][0m Trial 14 finished with value: 0.3899695551472491 and parameters: {'observation_period_num': 223, 'train_rates': 0.9152506655410673, 'learning_rate': 3.972997838129281e-05, 'batch_size': 49, 'step_size': 4, 'gamma': 0.8378162379945288}. Best is trial 10 with value: 0.2643496546149254.[0m
[32m[I 2025-02-04 03:46:00,106][0m Trial 15 finished with value: 1.628170371055603 and parameters: {'observation_period_num': 162, 'train_rates': 0.9150621564615649, 'learning_rate': 1.001248891865939e-05, 'batch_size': 253, 'step_size': 3, 'gamma': 0.7852157806038814}. Best is trial 10 with value: 0.2643496546149254.[0m
[32m[I 2025-02-04 03:50:17,349][0m Trial 16 finished with value: 0.35710879787802696 and parameters: {'observation_period_num': 216, 'train_rates': 0.8949188592407706, 'learning_rate': 0.0008778061069824963, 'batch_size': 117, 'step_size': 3, 'gamma': 0.9330128505560213}. Best is trial 10 with value: 0.2643496546149254.[0m
[32m[I 2025-02-04 03:54:26,876][0m Trial 17 finished with value: 1.6040849151896006 and parameters: {'observation_period_num': 194, 'train_rates': 0.9517413984133644, 'learning_rate': 1.0039774316812506e-06, 'batch_size': 48, 'step_size': 5, 'gamma': 0.8450260302434791}. Best is trial 10 with value: 0.2643496546149254.[0m
[32m[I 2025-02-04 03:58:59,929][0m Trial 18 finished with value: 0.2575231423744789 and parameters: {'observation_period_num': 111, 'train_rates': 0.8764288539278859, 'learning_rate': 8.270783606328783e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7947938928613391}. Best is trial 18 with value: 0.2575231423744789.[0m
Early stopping at epoch 54
[32m[I 2025-02-04 04:00:02,896][0m Trial 19 finished with value: 0.7828038853875706 and parameters: {'observation_period_num': 111, 'train_rates': 0.8748165439112934, 'learning_rate': 6.788912437255532e-05, 'batch_size': 153, 'step_size': 1, 'gamma': 0.7968431094900106}. Best is trial 18 with value: 0.2575231423744789.[0m
[32m[I 2025-02-04 04:01:33,878][0m Trial 20 finished with value: 0.5337654776070728 and parameters: {'observation_period_num': 94, 'train_rates': 0.7882902391241915, 'learning_rate': 0.0001947747598761587, 'batch_size': 103, 'step_size': 9, 'gamma': 0.7638391160643807}. Best is trial 18 with value: 0.2575231423744789.[0m
[32m[I 2025-02-04 04:05:25,381][0m Trial 21 finished with value: 0.17727674935993395 and parameters: {'observation_period_num': 154, 'train_rates': 0.9322477625137368, 'learning_rate': 5.727341728118739e-05, 'batch_size': 20, 'step_size': 4, 'gamma': 0.8191590249402195}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:08:30,049][0m Trial 22 finished with value: 0.3906452543549723 and parameters: {'observation_period_num': 149, 'train_rates': 0.9452773350036833, 'learning_rate': 6.783673934770553e-05, 'batch_size': 52, 'step_size': 3, 'gamma': 0.7845972599198783}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:10:51,881][0m Trial 23 finished with value: 0.22560529558213202 and parameters: {'observation_period_num': 120, 'train_rates': 0.8713963637936514, 'learning_rate': 9.369182497752203e-05, 'batch_size': 36, 'step_size': 5, 'gamma': 0.8154641543499322}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:13:02,313][0m Trial 24 finished with value: 0.2630936243290318 and parameters: {'observation_period_num': 113, 'train_rates': 0.858710638465826, 'learning_rate': 9.981745805864318e-05, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8500977203364585}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:14:16,208][0m Trial 25 finished with value: 0.5464222610643501 and parameters: {'observation_period_num': 66, 'train_rates': 0.8083160157453846, 'learning_rate': 0.0002530220403973883, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8013633572352866}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:17:35,468][0m Trial 26 finished with value: 0.28713584516910795 and parameters: {'observation_period_num': 161, 'train_rates': 0.8990329853780228, 'learning_rate': 0.0005399108533339581, 'batch_size': 36, 'step_size': 9, 'gamma': 0.779091162556586}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:20:11,066][0m Trial 27 finished with value: 0.30810656878027587 and parameters: {'observation_period_num': 133, 'train_rates': 0.9283186714372188, 'learning_rate': 5.411343111139824e-05, 'batch_size': 79, 'step_size': 6, 'gamma': 0.8111145400961991}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:22:14,225][0m Trial 28 finished with value: 0.28792055297632513 and parameters: {'observation_period_num': 61, 'train_rates': 0.8767511386327507, 'learning_rate': 0.00011282955983703835, 'batch_size': 37, 'step_size': 4, 'gamma': 0.7524233442362949}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:24:00,651][0m Trial 29 finished with value: 1.3628285543147463 and parameters: {'observation_period_num': 105, 'train_rates': 0.763393201423612, 'learning_rate': 1.1233192288823326e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9313700280613295}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:27:04,640][0m Trial 30 finished with value: 0.788503271375319 and parameters: {'observation_period_num': 171, 'train_rates': 0.810759770268783, 'learning_rate': 2.0443847363783335e-05, 'batch_size': 93, 'step_size': 6, 'gamma': 0.7919969592162046}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:29:32,796][0m Trial 31 finished with value: 0.22892719380060833 and parameters: {'observation_period_num': 127, 'train_rates': 0.8674052135273428, 'learning_rate': 0.00011525527583588954, 'batch_size': 38, 'step_size': 5, 'gamma': 0.8539678786516876}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:32:20,363][0m Trial 32 finished with value: 0.20861679642053133 and parameters: {'observation_period_num': 131, 'train_rates': 0.8740203620357647, 'learning_rate': 0.0001899459097683926, 'batch_size': 36, 'step_size': 4, 'gamma': 0.8863719233636345}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:35:13,371][0m Trial 33 finished with value: 0.372266262410635 and parameters: {'observation_period_num': 142, 'train_rates': 0.8523629265040711, 'learning_rate': 0.0004430206539294059, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8744700241396574}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:37:28,628][0m Trial 34 finished with value: 0.2002812048040047 and parameters: {'observation_period_num': 125, 'train_rates': 0.8977624518413283, 'learning_rate': 0.00016315020287354008, 'batch_size': 87, 'step_size': 6, 'gamma': 0.8895834618579531}. Best is trial 21 with value: 0.17727674935993395.[0m
[32m[I 2025-02-04 04:40:21,883][0m Trial 35 finished with value: 0.1722407324446572 and parameters: {'observation_period_num': 145, 'train_rates': 0.9296532984001769, 'learning_rate': 0.00018512566019088052, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8958612642839934}. Best is trial 35 with value: 0.1722407324446572.[0m
[32m[I 2025-02-04 04:43:53,576][0m Trial 36 finished with value: 0.17824863665031665 and parameters: {'observation_period_num': 173, 'train_rates': 0.9288295486636007, 'learning_rate': 0.00019913018614243114, 'batch_size': 93, 'step_size': 8, 'gamma': 0.8894591619060194}. Best is trial 35 with value: 0.1722407324446572.[0m
[32m[I 2025-02-04 04:47:34,898][0m Trial 37 finished with value: 0.20026124596904596 and parameters: {'observation_period_num': 181, 'train_rates': 0.9303198459619325, 'learning_rate': 0.00033837117489854694, 'batch_size': 90, 'step_size': 10, 'gamma': 0.9222465274541173}. Best is trial 35 with value: 0.1722407324446572.[0m
[32m[I 2025-02-04 04:51:41,555][0m Trial 38 finished with value: 0.36054186820983886 and parameters: {'observation_period_num': 197, 'train_rates': 0.9331207404435903, 'learning_rate': 0.0007976311077338181, 'batch_size': 148, 'step_size': 9, 'gamma': 0.930429323682544}. Best is trial 35 with value: 0.1722407324446572.[0m
[32m[I 2025-02-04 04:55:02,322][0m Trial 39 finished with value: 0.22644724925359092 and parameters: {'observation_period_num': 176, 'train_rates': 0.9190326067358615, 'learning_rate': 0.00035269385901067076, 'batch_size': 111, 'step_size': 10, 'gamma': 0.9542004832795591}. Best is trial 35 with value: 0.1722407324446572.[0m
[32m[I 2025-02-04 04:58:02,034][0m Trial 40 finished with value: 0.1543010026216507 and parameters: {'observation_period_num': 152, 'train_rates': 0.9719976160904686, 'learning_rate': 0.00024694141347244127, 'batch_size': 127, 'step_size': 11, 'gamma': 0.9117291049686187}. Best is trial 40 with value: 0.1543010026216507.[0m
[32m[I 2025-02-04 05:01:10,625][0m Trial 41 finished with value: 0.16884705424308777 and parameters: {'observation_period_num': 160, 'train_rates': 0.9687967375110551, 'learning_rate': 0.0002503533868455374, 'batch_size': 135, 'step_size': 11, 'gamma': 0.9104749057407878}. Best is trial 40 with value: 0.1543010026216507.[0m
[32m[I 2025-02-04 05:04:10,247][0m Trial 42 finished with value: 0.18125803768634796 and parameters: {'observation_period_num': 154, 'train_rates': 0.962800431625788, 'learning_rate': 0.0005578773506812534, 'batch_size': 179, 'step_size': 13, 'gamma': 0.9042303922814579}. Best is trial 40 with value: 0.1543010026216507.[0m
[32m[I 2025-02-04 05:06:56,923][0m Trial 43 finished with value: 0.1421888917684555 and parameters: {'observation_period_num': 141, 'train_rates': 0.9774744730866827, 'learning_rate': 0.00023306196336975925, 'batch_size': 133, 'step_size': 11, 'gamma': 0.917876735819471}. Best is trial 43 with value: 0.1421888917684555.[0m
[32m[I 2025-02-04 05:09:44,227][0m Trial 44 finished with value: 0.16046705842018127 and parameters: {'observation_period_num': 142, 'train_rates': 0.9879422328054576, 'learning_rate': 0.0002701660810086812, 'batch_size': 139, 'step_size': 11, 'gamma': 0.9504696393213624}. Best is trial 43 with value: 0.1421888917684555.[0m
[32m[I 2025-02-04 05:12:29,418][0m Trial 45 finished with value: 0.1461079716682434 and parameters: {'observation_period_num': 140, 'train_rates': 0.9844089142788808, 'learning_rate': 0.000248931355252464, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9596780657264019}. Best is trial 43 with value: 0.1421888917684555.[0m
[32m[I 2025-02-04 05:14:15,289][0m Trial 46 finished with value: 0.1731383055448532 and parameters: {'observation_period_num': 96, 'train_rates': 0.9827316273595587, 'learning_rate': 0.0002642173650814338, 'batch_size': 140, 'step_size': 12, 'gamma': 0.9875715565072333}. Best is trial 43 with value: 0.1421888917684555.[0m
[32m[I 2025-02-04 05:16:59,022][0m Trial 47 finished with value: 0.20530593395233154 and parameters: {'observation_period_num': 141, 'train_rates': 0.9700038285116234, 'learning_rate': 0.0006019682691025772, 'batch_size': 162, 'step_size': 13, 'gamma': 0.959595674238304}. Best is trial 43 with value: 0.1421888917684555.[0m
[32m[I 2025-02-04 05:20:08,890][0m Trial 48 finished with value: 0.19239891303661175 and parameters: {'observation_period_num': 164, 'train_rates': 0.9524013574281124, 'learning_rate': 0.00038660156794625294, 'batch_size': 128, 'step_size': 11, 'gamma': 0.9473923592618814}. Best is trial 43 with value: 0.1421888917684555.[0m
[32m[I 2025-02-04 05:24:06,483][0m Trial 49 finished with value: 0.12941952049732208 and parameters: {'observation_period_num': 189, 'train_rates': 0.9897070506458603, 'learning_rate': 0.00013529243732510096, 'batch_size': 135, 'step_size': 15, 'gamma': 0.9152230852528475}. Best is trial 49 with value: 0.12941952049732208.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_XOM_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 242, 'train_rates': 0.9885456141056422, 'learning_rate': 0.0001394235182320582, 'batch_size': 52, 'step_size': 9, 'gamma': 0.902195306455488}
Epoch 1/300, trend Loss: 0.8368 | 1.2525
Epoch 2/300, trend Loss: 0.6073 | 0.9873
Epoch 3/300, trend Loss: 0.6041 | 0.8871
Epoch 4/300, trend Loss: 0.4633 | 0.7836
Epoch 5/300, trend Loss: 0.6112 | 0.7287
Epoch 6/300, trend Loss: 0.4780 | 0.6835
Epoch 7/300, trend Loss: 0.3935 | 0.6425
Epoch 8/300, trend Loss: 0.3496 | 0.5540
Epoch 9/300, trend Loss: 0.3899 | 0.5140
Epoch 10/300, trend Loss: 0.3499 | 0.4874
Epoch 11/300, trend Loss: 0.3576 | 0.4389
Epoch 12/300, trend Loss: 0.3694 | 0.4608
Epoch 13/300, trend Loss: 0.3794 | 0.4393
Epoch 14/300, trend Loss: 0.3165 | 0.3906
Epoch 15/300, trend Loss: 0.2625 | 0.3595
Epoch 16/300, trend Loss: 0.2450 | 0.3474
Epoch 17/300, trend Loss: 0.2322 | 0.3296
Epoch 18/300, trend Loss: 0.2214 | 0.3053
Epoch 19/300, trend Loss: 0.2128 | 0.3018
Epoch 20/300, trend Loss: 0.2047 | 0.2819
Epoch 21/300, trend Loss: 0.2000 | 0.2742
Epoch 22/300, trend Loss: 0.1936 | 0.2613
Epoch 23/300, trend Loss: 0.1879 | 0.2558
Epoch 24/300, trend Loss: 0.1827 | 0.2436
Epoch 25/300, trend Loss: 0.1818 | 0.2452
Epoch 26/300, trend Loss: 0.1961 | 0.2351
Epoch 27/300, trend Loss: 0.1841 | 0.2268
Epoch 28/300, trend Loss: 0.1818 | 0.2192
Epoch 29/300, trend Loss: 0.2059 | 0.2163
Epoch 30/300, trend Loss: 0.1975 | 0.2243
Epoch 31/300, trend Loss: 0.2043 | 0.2083
Epoch 32/300, trend Loss: 0.2094 | 0.2425
Epoch 33/300, trend Loss: 0.2222 | 0.2280
Epoch 34/300, trend Loss: 0.2531 | 0.2710
Epoch 35/300, trend Loss: 0.2098 | 0.2354
Epoch 36/300, trend Loss: 0.1881 | 0.2133
Epoch 37/300, trend Loss: 0.1749 | 0.2029
Epoch 38/300, trend Loss: 0.1691 | 0.1971
Epoch 39/300, trend Loss: 0.1609 | 0.1896
Epoch 40/300, trend Loss: 0.1636 | 0.1894
Epoch 41/300, trend Loss: 0.1538 | 0.1893
Epoch 42/300, trend Loss: 0.1538 | 0.1800
Epoch 43/300, trend Loss: 0.1484 | 0.1802
Epoch 44/300, trend Loss: 0.1492 | 0.1774
Epoch 45/300, trend Loss: 0.1438 | 0.1711
Epoch 46/300, trend Loss: 0.1422 | 0.1705
Epoch 47/300, trend Loss: 0.1413 | 0.1691
Epoch 48/300, trend Loss: 0.1388 | 0.1669
Epoch 49/300, trend Loss: 0.1371 | 0.1644
Epoch 50/300, trend Loss: 0.1362 | 0.1613
Epoch 51/300, trend Loss: 0.1352 | 0.1622
Epoch 52/300, trend Loss: 0.1345 | 0.1620
Epoch 53/300, trend Loss: 0.1339 | 0.1558
Epoch 54/300, trend Loss: 0.1328 | 0.1554
Epoch 55/300, trend Loss: 0.1350 | 0.1583
Epoch 56/300, trend Loss: 0.1315 | 0.1524
Epoch 57/300, trend Loss: 0.1312 | 0.1507
Epoch 58/300, trend Loss: 0.1306 | 0.1525
Epoch 59/300, trend Loss: 0.1281 | 0.1499
Epoch 60/300, trend Loss: 0.1274 | 0.1484
Epoch 61/300, trend Loss: 0.1265 | 0.1473
Epoch 62/300, trend Loss: 0.1265 | 0.1458
Epoch 63/300, trend Loss: 0.1264 | 0.1485
Epoch 64/300, trend Loss: 0.1269 | 0.1434
Epoch 65/300, trend Loss: 0.1260 | 0.1406
Epoch 66/300, trend Loss: 0.1271 | 0.1453
Epoch 67/300, trend Loss: 0.1243 | 0.1402
Epoch 68/300, trend Loss: 0.1227 | 0.1379
Epoch 69/300, trend Loss: 0.1234 | 0.1405
Epoch 70/300, trend Loss: 0.1197 | 0.1380
Epoch 71/300, trend Loss: 0.1208 | 0.1420
Epoch 72/300, trend Loss: 0.1223 | 0.1400
Epoch 73/300, trend Loss: 0.1236 | 0.1358
Epoch 74/300, trend Loss: 0.1299 | 0.1442
Epoch 75/300, trend Loss: 0.1195 | 0.1376
Epoch 76/300, trend Loss: 0.1211 | 0.1342
Epoch 77/300, trend Loss: 0.1206 | 0.1364
Epoch 78/300, trend Loss: 0.1152 | 0.1324
Epoch 79/300, trend Loss: 0.1132 | 0.1313
Epoch 80/300, trend Loss: 0.1126 | 0.1310
Epoch 81/300, trend Loss: 0.1114 | 0.1301
Epoch 82/300, trend Loss: 0.1102 | 0.1285
Epoch 83/300, trend Loss: 0.1104 | 0.1297
Epoch 84/300, trend Loss: 0.1088 | 0.1284
Epoch 85/300, trend Loss: 0.1080 | 0.1270
Epoch 86/300, trend Loss: 0.1075 | 0.1265
Epoch 87/300, trend Loss: 0.1078 | 0.1284
Epoch 88/300, trend Loss: 0.1070 | 0.1256
Epoch 89/300, trend Loss: 0.1069 | 0.1250
Epoch 90/300, trend Loss: 0.1065 | 0.1259
Epoch 91/300, trend Loss: 0.1064 | 0.1238
Epoch 92/300, trend Loss: 0.1053 | 0.1229
Epoch 93/300, trend Loss: 0.1052 | 0.1230
Epoch 94/300, trend Loss: 0.1043 | 0.1222
Epoch 95/300, trend Loss: 0.1038 | 0.1214
Epoch 96/300, trend Loss: 0.1027 | 0.1214
Epoch 97/300, trend Loss: 0.1026 | 0.1208
Epoch 98/300, trend Loss: 0.1025 | 0.1210
Epoch 99/300, trend Loss: 0.1030 | 0.1187
Epoch 100/300, trend Loss: 0.1019 | 0.1201
Epoch 101/300, trend Loss: 0.1017 | 0.1189
Epoch 102/300, trend Loss: 0.1015 | 0.1187
Epoch 103/300, trend Loss: 0.1004 | 0.1185
Epoch 104/300, trend Loss: 0.1008 | 0.1184
Epoch 105/300, trend Loss: 0.1003 | 0.1181
Epoch 106/300, trend Loss: 0.0998 | 0.1162
Epoch 107/300, trend Loss: 0.0997 | 0.1170
Epoch 108/300, trend Loss: 0.0996 | 0.1177
Epoch 109/300, trend Loss: 0.0996 | 0.1147
Epoch 110/300, trend Loss: 0.0987 | 0.1157
Epoch 111/300, trend Loss: 0.0988 | 0.1158
Epoch 112/300, trend Loss: 0.0982 | 0.1157
Epoch 113/300, trend Loss: 0.0974 | 0.1150
Epoch 114/300, trend Loss: 0.0984 | 0.1150
Epoch 115/300, trend Loss: 0.0973 | 0.1148
Epoch 116/300, trend Loss: 0.0968 | 0.1144
Epoch 117/300, trend Loss: 0.0974 | 0.1135
Epoch 118/300, trend Loss: 0.0970 | 0.1137
Epoch 119/300, trend Loss: 0.0967 | 0.1135
Epoch 120/300, trend Loss: 0.0966 | 0.1132
Epoch 121/300, trend Loss: 0.0957 | 0.1119
Epoch 122/300, trend Loss: 0.0958 | 0.1132
Epoch 123/300, trend Loss: 0.0960 | 0.1122
Epoch 124/300, trend Loss: 0.0959 | 0.1117
Epoch 125/300, trend Loss: 0.0952 | 0.1115
Epoch 126/300, trend Loss: 0.0951 | 0.1113
Epoch 127/300, trend Loss: 0.0959 | 0.1111
Epoch 128/300, trend Loss: 0.0947 | 0.1108
Epoch 129/300, trend Loss: 0.0940 | 0.1107
Epoch 130/300, trend Loss: 0.0946 | 0.1101
Epoch 131/300, trend Loss: 0.0940 | 0.1106
Epoch 132/300, trend Loss: 0.0940 | 0.1102
Epoch 133/300, trend Loss: 0.0941 | 0.1101
Epoch 134/300, trend Loss: 0.0936 | 0.1104
Epoch 135/300, trend Loss: 0.0940 | 0.1101
Epoch 136/300, trend Loss: 0.0928 | 0.1105
Epoch 137/300, trend Loss: 0.0924 | 0.1098
Epoch 138/300, trend Loss: 0.0929 | 0.1091
Epoch 139/300, trend Loss: 0.0930 | 0.1092
Epoch 140/300, trend Loss: 0.0923 | 0.1093
Epoch 141/300, trend Loss: 0.0923 | 0.1094
Epoch 142/300, trend Loss: 0.0926 | 0.1090
Epoch 143/300, trend Loss: 0.0924 | 0.1093
Epoch 144/300, trend Loss: 0.0922 | 0.1091
Epoch 145/300, trend Loss: 0.0926 | 0.1086
Epoch 146/300, trend Loss: 0.0924 | 0.1089
Epoch 147/300, trend Loss: 0.0920 | 0.1083
Epoch 148/300, trend Loss: 0.0915 | 0.1088
Epoch 149/300, trend Loss: 0.0913 | 0.1083
Epoch 150/300, trend Loss: 0.0918 | 0.1079
Epoch 151/300, trend Loss: 0.0913 | 0.1082
Epoch 152/300, trend Loss: 0.0924 | 0.1078
Epoch 153/300, trend Loss: 0.0907 | 0.1075
Epoch 154/300, trend Loss: 0.0914 | 0.1078
Epoch 155/300, trend Loss: 0.0903 | 0.1073
Epoch 156/300, trend Loss: 0.0908 | 0.1076
Epoch 157/300, trend Loss: 0.0901 | 0.1076
Epoch 158/300, trend Loss: 0.0914 | 0.1077
Epoch 159/300, trend Loss: 0.0907 | 0.1073
Epoch 160/300, trend Loss: 0.0903 | 0.1069
Epoch 161/300, trend Loss: 0.0905 | 0.1070
Epoch 162/300, trend Loss: 0.0900 | 0.1074
Epoch 163/300, trend Loss: 0.0900 | 0.1073
Epoch 164/300, trend Loss: 0.0906 | 0.1071
Epoch 165/300, trend Loss: 0.0901 | 0.1069
Epoch 166/300, trend Loss: 0.0898 | 0.1066
Epoch 167/300, trend Loss: 0.0898 | 0.1064
Epoch 168/300, trend Loss: 0.0900 | 0.1066
Epoch 169/300, trend Loss: 0.0897 | 0.1069
Epoch 170/300, trend Loss: 0.0898 | 0.1064
Epoch 171/300, trend Loss: 0.0891 | 0.1064
Epoch 172/300, trend Loss: 0.0892 | 0.1064
Epoch 173/300, trend Loss: 0.0893 | 0.1063
Epoch 174/300, trend Loss: 0.0891 | 0.1059
Epoch 175/300, trend Loss: 0.0890 | 0.1057
Epoch 176/300, trend Loss: 0.0895 | 0.1058
Epoch 177/300, trend Loss: 0.0889 | 0.1055
Epoch 178/300, trend Loss: 0.0896 | 0.1056
Epoch 179/300, trend Loss: 0.0884 | 0.1057
Epoch 180/300, trend Loss: 0.0892 | 0.1055
Epoch 181/300, trend Loss: 0.0900 | 0.1056
Epoch 182/300, trend Loss: 0.0890 | 0.1055
Epoch 183/300, trend Loss: 0.0885 | 0.1053
Epoch 184/300, trend Loss: 0.0886 | 0.1055
Epoch 185/300, trend Loss: 0.0890 | 0.1056
Epoch 186/300, trend Loss: 0.0889 | 0.1053
Epoch 187/300, trend Loss: 0.0886 | 0.1051
Epoch 188/300, trend Loss: 0.0889 | 0.1052
Epoch 189/300, trend Loss: 0.0886 | 0.1053
Epoch 190/300, trend Loss: 0.0881 | 0.1054
Epoch 191/300, trend Loss: 0.0894 | 0.1052
Epoch 192/300, trend Loss: 0.0883 | 0.1051
Epoch 193/300, trend Loss: 0.0884 | 0.1049
Epoch 194/300, trend Loss: 0.0880 | 0.1049
Epoch 195/300, trend Loss: 0.0887 | 0.1047
Epoch 196/300, trend Loss: 0.0881 | 0.1048
Epoch 197/300, trend Loss: 0.0883 | 0.1047
Epoch 198/300, trend Loss: 0.0884 | 0.1046
Epoch 199/300, trend Loss: 0.0885 | 0.1045
Epoch 200/300, trend Loss: 0.0879 | 0.1046
Epoch 201/300, trend Loss: 0.0888 | 0.1045
Epoch 202/300, trend Loss: 0.0883 | 0.1046
Epoch 203/300, trend Loss: 0.0878 | 0.1045
Epoch 204/300, trend Loss: 0.0876 | 0.1044
Epoch 205/300, trend Loss: 0.0877 | 0.1042
Epoch 206/300, trend Loss: 0.0879 | 0.1043
Epoch 207/300, trend Loss: 0.0880 | 0.1043
Epoch 208/300, trend Loss: 0.0882 | 0.1043
Epoch 209/300, trend Loss: 0.0877 | 0.1041
Epoch 210/300, trend Loss: 0.0874 | 0.1039
Epoch 211/300, trend Loss: 0.0876 | 0.1040
Epoch 212/300, trend Loss: 0.0878 | 0.1041
Epoch 213/300, trend Loss: 0.0882 | 0.1041
Epoch 214/300, trend Loss: 0.0877 | 0.1041
Epoch 215/300, trend Loss: 0.0879 | 0.1040
Epoch 216/300, trend Loss: 0.0878 | 0.1040
Epoch 217/300, trend Loss: 0.0877 | 0.1040
Epoch 218/300, trend Loss: 0.0879 | 0.1039
Epoch 219/300, trend Loss: 0.0875 | 0.1038
Epoch 220/300, trend Loss: 0.0876 | 0.1038
Epoch 221/300, trend Loss: 0.0870 | 0.1039
Epoch 222/300, trend Loss: 0.0868 | 0.1039
Epoch 223/300, trend Loss: 0.0875 | 0.1038
Epoch 224/300, trend Loss: 0.0872 | 0.1038
Epoch 225/300, trend Loss: 0.0878 | 0.1035
Epoch 226/300, trend Loss: 0.0875 | 0.1036
Epoch 227/300, trend Loss: 0.0873 | 0.1036
Epoch 228/300, trend Loss: 0.0868 | 0.1037
Epoch 229/300, trend Loss: 0.0881 | 0.1037
Epoch 230/300, trend Loss: 0.0876 | 0.1036
Epoch 231/300, trend Loss: 0.0874 | 0.1036
Epoch 232/300, trend Loss: 0.0871 | 0.1038
Epoch 233/300, trend Loss: 0.0879 | 0.1038
Epoch 234/300, trend Loss: 0.0878 | 0.1038
Epoch 235/300, trend Loss: 0.0873 | 0.1039
Epoch 236/300, trend Loss: 0.0875 | 0.1037
Epoch 237/300, trend Loss: 0.0869 | 0.1036
Epoch 238/300, trend Loss: 0.0869 | 0.1036
Epoch 239/300, trend Loss: 0.0875 | 0.1036
Epoch 240/300, trend Loss: 0.0870 | 0.1036
Epoch 241/300, trend Loss: 0.0874 | 0.1036
Epoch 242/300, trend Loss: 0.0872 | 0.1037
Epoch 243/300, trend Loss: 0.0866 | 0.1037
Epoch 244/300, trend Loss: 0.0874 | 0.1036
Epoch 245/300, trend Loss: 0.0870 | 0.1036
Epoch 246/300, trend Loss: 0.0868 | 0.1037
Epoch 247/300, trend Loss: 0.0872 | 0.1036
Epoch 248/300, trend Loss: 0.0879 | 0.1036
Epoch 249/300, trend Loss: 0.0867 | 0.1036
Epoch 250/300, trend Loss: 0.0872 | 0.1035
Epoch 251/300, trend Loss: 0.0870 | 0.1035
Epoch 252/300, trend Loss: 0.0877 | 0.1034
Epoch 253/300, trend Loss: 0.0873 | 0.1035
Epoch 254/300, trend Loss: 0.0866 | 0.1035
Epoch 255/300, trend Loss: 0.0869 | 0.1034
Epoch 256/300, trend Loss: 0.0874 | 0.1034
Epoch 257/300, trend Loss: 0.0870 | 0.1033
Epoch 258/300, trend Loss: 0.0865 | 0.1033
Epoch 259/300, trend Loss: 0.0863 | 0.1033
Epoch 260/300, trend Loss: 0.0873 | 0.1033
Epoch 261/300, trend Loss: 0.0867 | 0.1033
Epoch 262/300, trend Loss: 0.0870 | 0.1033
Epoch 263/300, trend Loss: 0.0869 | 0.1034
Epoch 264/300, trend Loss: 0.0868 | 0.1033
Epoch 265/300, trend Loss: 0.0870 | 0.1033
Epoch 266/300, trend Loss: 0.0869 | 0.1033
Epoch 267/300, trend Loss: 0.0868 | 0.1033
Epoch 268/300, trend Loss: 0.0872 | 0.1033
Epoch 269/300, trend Loss: 0.0876 | 0.1033
Epoch 270/300, trend Loss: 0.0874 | 0.1033
Epoch 271/300, trend Loss: 0.0870 | 0.1033
Epoch 272/300, trend Loss: 0.0870 | 0.1033
Epoch 273/300, trend Loss: 0.0867 | 0.1034
Epoch 274/300, trend Loss: 0.0871 | 0.1033
Epoch 275/300, trend Loss: 0.0872 | 0.1033
Epoch 276/300, trend Loss: 0.0874 | 0.1033
Epoch 277/300, trend Loss: 0.0873 | 0.1033
Epoch 278/300, trend Loss: 0.0865 | 0.1033
Epoch 279/300, trend Loss: 0.0868 | 0.1032
Epoch 280/300, trend Loss: 0.0864 | 0.1033
Epoch 281/300, trend Loss: 0.0872 | 0.1033
Epoch 282/300, trend Loss: 0.0871 | 0.1033
Epoch 283/300, trend Loss: 0.0864 | 0.1033
Epoch 284/300, trend Loss: 0.0875 | 0.1033
Epoch 285/300, trend Loss: 0.0869 | 0.1033
Epoch 286/300, trend Loss: 0.0865 | 0.1033
Epoch 287/300, trend Loss: 0.0864 | 0.1033
Epoch 288/300, trend Loss: 0.0869 | 0.1032
Epoch 289/300, trend Loss: 0.0868 | 0.1032
Epoch 290/300, trend Loss: 0.0865 | 0.1032
Epoch 291/300, trend Loss: 0.0866 | 0.1032
Epoch 292/300, trend Loss: 0.0869 | 0.1032
Epoch 293/300, trend Loss: 0.0866 | 0.1032
Epoch 294/300, trend Loss: 0.0864 | 0.1032
Epoch 295/300, trend Loss: 0.0867 | 0.1032
Epoch 296/300, trend Loss: 0.0870 | 0.1032
Epoch 297/300, trend Loss: 0.0868 | 0.1032
Epoch 298/300, trend Loss: 0.0872 | 0.1032
Epoch 299/300, trend Loss: 0.0866 | 0.1031
Epoch 300/300, trend Loss: 0.0865 | 0.1031
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.9521920266536259, 'learning_rate': 0.0003374384478605072, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8237045326693101}
Epoch 1/300, seasonal_0 Loss: 1.0717 | 2.2511
Epoch 2/300, seasonal_0 Loss: 0.7582 | 1.2756
Epoch 3/300, seasonal_0 Loss: 0.5926 | 1.0561
Epoch 4/300, seasonal_0 Loss: 0.5257 | 0.9113
Epoch 5/300, seasonal_0 Loss: 0.5463 | 0.9840
Epoch 6/300, seasonal_0 Loss: 0.5049 | 0.7947
Epoch 7/300, seasonal_0 Loss: 0.4927 | 0.7798
Epoch 8/300, seasonal_0 Loss: 0.4066 | 0.6924
Epoch 9/300, seasonal_0 Loss: 0.3879 | 0.6544
Epoch 10/300, seasonal_0 Loss: 0.3804 | 0.6120
Epoch 11/300, seasonal_0 Loss: 0.3600 | 0.6213
Epoch 12/300, seasonal_0 Loss: 0.3147 | 0.5529
Epoch 13/300, seasonal_0 Loss: 0.3015 | 0.5070
Epoch 14/300, seasonal_0 Loss: 0.2893 | 0.4877
Epoch 15/300, seasonal_0 Loss: 0.2872 | 0.4652
Epoch 16/300, seasonal_0 Loss: 0.2694 | 0.4421
Epoch 17/300, seasonal_0 Loss: 0.2906 | 0.4442
Epoch 18/300, seasonal_0 Loss: 0.2641 | 0.4214
Epoch 19/300, seasonal_0 Loss: 0.2578 | 0.3945
Epoch 20/300, seasonal_0 Loss: 0.2400 | 0.3814
Epoch 21/300, seasonal_0 Loss: 0.2385 | 0.3721
Epoch 22/300, seasonal_0 Loss: 0.2234 | 0.3489
Epoch 23/300, seasonal_0 Loss: 0.2112 | 0.3328
Epoch 24/300, seasonal_0 Loss: 0.2043 | 0.3209
Epoch 25/300, seasonal_0 Loss: 0.2029 | 0.3101
Epoch 26/300, seasonal_0 Loss: 0.1949 | 0.3039
Epoch 27/300, seasonal_0 Loss: 0.1927 | 0.2938
Epoch 28/300, seasonal_0 Loss: 0.1884 | 0.2867
Epoch 29/300, seasonal_0 Loss: 0.1869 | 0.2783
Epoch 30/300, seasonal_0 Loss: 0.1825 | 0.2731
Epoch 31/300, seasonal_0 Loss: 0.1811 | 0.2654
Epoch 32/300, seasonal_0 Loss: 0.1854 | 0.2642
Epoch 33/300, seasonal_0 Loss: 0.1808 | 0.2568
Epoch 34/300, seasonal_0 Loss: 0.1816 | 0.2534
Epoch 35/300, seasonal_0 Loss: 0.1753 | 0.2529
Epoch 36/300, seasonal_0 Loss: 0.1750 | 0.2458
Epoch 37/300, seasonal_0 Loss: 0.1707 | 0.2456
Epoch 38/300, seasonal_0 Loss: 0.1642 | 0.2365
Epoch 39/300, seasonal_0 Loss: 0.1600 | 0.2346
Epoch 40/300, seasonal_0 Loss: 0.1567 | 0.2273
Epoch 41/300, seasonal_0 Loss: 0.1540 | 0.2257
Epoch 42/300, seasonal_0 Loss: 0.1524 | 0.2207
Epoch 43/300, seasonal_0 Loss: 0.1507 | 0.2202
Epoch 44/300, seasonal_0 Loss: 0.1495 | 0.2154
Epoch 45/300, seasonal_0 Loss: 0.1480 | 0.2129
Epoch 46/300, seasonal_0 Loss: 0.1460 | 0.2103
Epoch 47/300, seasonal_0 Loss: 0.1447 | 0.2075
Epoch 48/300, seasonal_0 Loss: 0.1440 | 0.2061
Epoch 49/300, seasonal_0 Loss: 0.1423 | 0.2033
Epoch 50/300, seasonal_0 Loss: 0.1421 | 0.2016
Epoch 51/300, seasonal_0 Loss: 0.1411 | 0.1991
Epoch 52/300, seasonal_0 Loss: 0.1404 | 0.1983
Epoch 53/300, seasonal_0 Loss: 0.1388 | 0.1946
Epoch 54/300, seasonal_0 Loss: 0.1378 | 0.1933
Epoch 55/300, seasonal_0 Loss: 0.1360 | 0.1924
Epoch 56/300, seasonal_0 Loss: 0.1357 | 0.1911
Epoch 57/300, seasonal_0 Loss: 0.1344 | 0.1904
Epoch 58/300, seasonal_0 Loss: 0.1345 | 0.1880
Epoch 59/300, seasonal_0 Loss: 0.1339 | 0.1867
Epoch 60/300, seasonal_0 Loss: 0.1328 | 0.1848
Epoch 61/300, seasonal_0 Loss: 0.1325 | 0.1835
Epoch 62/300, seasonal_0 Loss: 0.1314 | 0.1834
Epoch 63/300, seasonal_0 Loss: 0.1305 | 0.1820
Epoch 64/300, seasonal_0 Loss: 0.1300 | 0.1813
Epoch 65/300, seasonal_0 Loss: 0.1287 | 0.1800
Epoch 66/300, seasonal_0 Loss: 0.1298 | 0.1790
Epoch 67/300, seasonal_0 Loss: 0.1277 | 0.1782
Epoch 68/300, seasonal_0 Loss: 0.1281 | 0.1769
Epoch 69/300, seasonal_0 Loss: 0.1273 | 0.1764
Epoch 70/300, seasonal_0 Loss: 0.1272 | 0.1758
Epoch 71/300, seasonal_0 Loss: 0.1270 | 0.1744
Epoch 72/300, seasonal_0 Loss: 0.1263 | 0.1739
Epoch 73/300, seasonal_0 Loss: 0.1250 | 0.1731
Epoch 74/300, seasonal_0 Loss: 0.1257 | 0.1731
Epoch 75/300, seasonal_0 Loss: 0.1244 | 0.1721
Epoch 76/300, seasonal_0 Loss: 0.1248 | 0.1714
Epoch 77/300, seasonal_0 Loss: 0.1241 | 0.1706
Epoch 78/300, seasonal_0 Loss: 0.1235 | 0.1704
Epoch 79/300, seasonal_0 Loss: 0.1231 | 0.1696
Epoch 80/300, seasonal_0 Loss: 0.1226 | 0.1691
Epoch 81/300, seasonal_0 Loss: 0.1221 | 0.1685
Epoch 82/300, seasonal_0 Loss: 0.1234 | 0.1682
Epoch 83/300, seasonal_0 Loss: 0.1219 | 0.1679
Epoch 84/300, seasonal_0 Loss: 0.1210 | 0.1670
Epoch 85/300, seasonal_0 Loss: 0.1212 | 0.1662
Epoch 86/300, seasonal_0 Loss: 0.1206 | 0.1660
Epoch 87/300, seasonal_0 Loss: 0.1210 | 0.1656
Epoch 88/300, seasonal_0 Loss: 0.1206 | 0.1654
Epoch 89/300, seasonal_0 Loss: 0.1197 | 0.1653
Epoch 90/300, seasonal_0 Loss: 0.1201 | 0.1643
Epoch 91/300, seasonal_0 Loss: 0.1201 | 0.1638
Epoch 92/300, seasonal_0 Loss: 0.1197 | 0.1639
Epoch 93/300, seasonal_0 Loss: 0.1194 | 0.1635
Epoch 94/300, seasonal_0 Loss: 0.1187 | 0.1629
Epoch 95/300, seasonal_0 Loss: 0.1186 | 0.1628
Epoch 96/300, seasonal_0 Loss: 0.1191 | 0.1626
Epoch 97/300, seasonal_0 Loss: 0.1178 | 0.1622
Epoch 98/300, seasonal_0 Loss: 0.1180 | 0.1620
Epoch 99/300, seasonal_0 Loss: 0.1168 | 0.1617
Epoch 100/300, seasonal_0 Loss: 0.1174 | 0.1614
Epoch 101/300, seasonal_0 Loss: 0.1182 | 0.1612
Epoch 102/300, seasonal_0 Loss: 0.1167 | 0.1607
Epoch 103/300, seasonal_0 Loss: 0.1169 | 0.1603
Epoch 104/300, seasonal_0 Loss: 0.1171 | 0.1600
Epoch 105/300, seasonal_0 Loss: 0.1169 | 0.1599
Epoch 106/300, seasonal_0 Loss: 0.1170 | 0.1598
Epoch 107/300, seasonal_0 Loss: 0.1168 | 0.1596
Epoch 108/300, seasonal_0 Loss: 0.1159 | 0.1597
Epoch 109/300, seasonal_0 Loss: 0.1161 | 0.1593
Epoch 110/300, seasonal_0 Loss: 0.1154 | 0.1590
Epoch 111/300, seasonal_0 Loss: 0.1158 | 0.1589
Epoch 112/300, seasonal_0 Loss: 0.1158 | 0.1586
Epoch 113/300, seasonal_0 Loss: 0.1150 | 0.1583
Epoch 114/300, seasonal_0 Loss: 0.1155 | 0.1582
Epoch 115/300, seasonal_0 Loss: 0.1157 | 0.1578
Epoch 116/300, seasonal_0 Loss: 0.1160 | 0.1576
Epoch 117/300, seasonal_0 Loss: 0.1148 | 0.1576
Epoch 118/300, seasonal_0 Loss: 0.1149 | 0.1574
Epoch 119/300, seasonal_0 Loss: 0.1144 | 0.1573
Epoch 120/300, seasonal_0 Loss: 0.1143 | 0.1572
Epoch 121/300, seasonal_0 Loss: 0.1142 | 0.1569
Epoch 122/300, seasonal_0 Loss: 0.1147 | 0.1567
Epoch 123/300, seasonal_0 Loss: 0.1138 | 0.1566
Epoch 124/300, seasonal_0 Loss: 0.1144 | 0.1564
Epoch 125/300, seasonal_0 Loss: 0.1140 | 0.1565
Epoch 126/300, seasonal_0 Loss: 0.1137 | 0.1565
Epoch 127/300, seasonal_0 Loss: 0.1139 | 0.1562
Epoch 128/300, seasonal_0 Loss: 0.1138 | 0.1561
Epoch 129/300, seasonal_0 Loss: 0.1143 | 0.1560
Epoch 130/300, seasonal_0 Loss: 0.1129 | 0.1559
Epoch 131/300, seasonal_0 Loss: 0.1144 | 0.1559
Epoch 132/300, seasonal_0 Loss: 0.1135 | 0.1558
Epoch 133/300, seasonal_0 Loss: 0.1136 | 0.1557
Epoch 134/300, seasonal_0 Loss: 0.1134 | 0.1556
Epoch 135/300, seasonal_0 Loss: 0.1134 | 0.1555
Epoch 136/300, seasonal_0 Loss: 0.1130 | 0.1554
Epoch 137/300, seasonal_0 Loss: 0.1134 | 0.1554
Epoch 138/300, seasonal_0 Loss: 0.1133 | 0.1554
Epoch 139/300, seasonal_0 Loss: 0.1132 | 0.1554
Epoch 140/300, seasonal_0 Loss: 0.1135 | 0.1553
Epoch 141/300, seasonal_0 Loss: 0.1129 | 0.1552
Epoch 142/300, seasonal_0 Loss: 0.1132 | 0.1552
Epoch 143/300, seasonal_0 Loss: 0.1127 | 0.1551
Epoch 144/300, seasonal_0 Loss: 0.1128 | 0.1550
Epoch 145/300, seasonal_0 Loss: 0.1134 | 0.1549
Epoch 146/300, seasonal_0 Loss: 0.1121 | 0.1549
Epoch 147/300, seasonal_0 Loss: 0.1122 | 0.1547
Epoch 148/300, seasonal_0 Loss: 0.1133 | 0.1546
Epoch 149/300, seasonal_0 Loss: 0.1124 | 0.1545
Epoch 150/300, seasonal_0 Loss: 0.1121 | 0.1544
Epoch 151/300, seasonal_0 Loss: 0.1120 | 0.1544
Epoch 152/300, seasonal_0 Loss: 0.1121 | 0.1543
Epoch 153/300, seasonal_0 Loss: 0.1124 | 0.1543
Epoch 154/300, seasonal_0 Loss: 0.1125 | 0.1542
Epoch 155/300, seasonal_0 Loss: 0.1125 | 0.1541
Epoch 156/300, seasonal_0 Loss: 0.1131 | 0.1540
Epoch 157/300, seasonal_0 Loss: 0.1128 | 0.1539
Epoch 158/300, seasonal_0 Loss: 0.1125 | 0.1539
Epoch 159/300, seasonal_0 Loss: 0.1117 | 0.1538
Epoch 160/300, seasonal_0 Loss: 0.1125 | 0.1537
Epoch 161/300, seasonal_0 Loss: 0.1126 | 0.1536
Epoch 162/300, seasonal_0 Loss: 0.1121 | 0.1536
Epoch 163/300, seasonal_0 Loss: 0.1122 | 0.1535
Epoch 164/300, seasonal_0 Loss: 0.1126 | 0.1535
Epoch 165/300, seasonal_0 Loss: 0.1126 | 0.1535
Epoch 166/300, seasonal_0 Loss: 0.1117 | 0.1535
Epoch 167/300, seasonal_0 Loss: 0.1120 | 0.1534
Epoch 168/300, seasonal_0 Loss: 0.1121 | 0.1534
Epoch 169/300, seasonal_0 Loss: 0.1121 | 0.1534
Epoch 170/300, seasonal_0 Loss: 0.1108 | 0.1533
Epoch 171/300, seasonal_0 Loss: 0.1118 | 0.1532
Epoch 172/300, seasonal_0 Loss: 0.1110 | 0.1533
Epoch 173/300, seasonal_0 Loss: 0.1114 | 0.1532
Epoch 174/300, seasonal_0 Loss: 0.1118 | 0.1532
Epoch 175/300, seasonal_0 Loss: 0.1114 | 0.1532
Epoch 176/300, seasonal_0 Loss: 0.1120 | 0.1532
Epoch 177/300, seasonal_0 Loss: 0.1107 | 0.1531
Epoch 178/300, seasonal_0 Loss: 0.1118 | 0.1531
Epoch 179/300, seasonal_0 Loss: 0.1119 | 0.1530
Epoch 180/300, seasonal_0 Loss: 0.1113 | 0.1529
Epoch 181/300, seasonal_0 Loss: 0.1118 | 0.1529
Epoch 182/300, seasonal_0 Loss: 0.1120 | 0.1529
Epoch 183/300, seasonal_0 Loss: 0.1112 | 0.1529
Epoch 184/300, seasonal_0 Loss: 0.1114 | 0.1529
Epoch 185/300, seasonal_0 Loss: 0.1111 | 0.1529
Epoch 186/300, seasonal_0 Loss: 0.1117 | 0.1529
Epoch 187/300, seasonal_0 Loss: 0.1113 | 0.1530
Epoch 188/300, seasonal_0 Loss: 0.1115 | 0.1530
Epoch 189/300, seasonal_0 Loss: 0.1114 | 0.1530
Epoch 190/300, seasonal_0 Loss: 0.1115 | 0.1530
Epoch 191/300, seasonal_0 Loss: 0.1110 | 0.1529
Epoch 192/300, seasonal_0 Loss: 0.1115 | 0.1529
Epoch 193/300, seasonal_0 Loss: 0.1113 | 0.1529
Epoch 194/300, seasonal_0 Loss: 0.1112 | 0.1528
Epoch 195/300, seasonal_0 Loss: 0.1112 | 0.1528
Epoch 196/300, seasonal_0 Loss: 0.1112 | 0.1528
Epoch 197/300, seasonal_0 Loss: 0.1116 | 0.1528
Epoch 198/300, seasonal_0 Loss: 0.1116 | 0.1528
Epoch 199/300, seasonal_0 Loss: 0.1108 | 0.1527
Epoch 200/300, seasonal_0 Loss: 0.1117 | 0.1527
Epoch 201/300, seasonal_0 Loss: 0.1101 | 0.1527
Epoch 202/300, seasonal_0 Loss: 0.1114 | 0.1527
Epoch 203/300, seasonal_0 Loss: 0.1113 | 0.1527
Epoch 204/300, seasonal_0 Loss: 0.1108 | 0.1527
Epoch 205/300, seasonal_0 Loss: 0.1119 | 0.1527
Epoch 206/300, seasonal_0 Loss: 0.1114 | 0.1526
Epoch 207/300, seasonal_0 Loss: 0.1115 | 0.1526
Epoch 208/300, seasonal_0 Loss: 0.1111 | 0.1526
Epoch 209/300, seasonal_0 Loss: 0.1110 | 0.1526
Epoch 210/300, seasonal_0 Loss: 0.1118 | 0.1526
Epoch 211/300, seasonal_0 Loss: 0.1110 | 0.1526
Epoch 212/300, seasonal_0 Loss: 0.1113 | 0.1526
Epoch 213/300, seasonal_0 Loss: 0.1111 | 0.1526
Epoch 214/300, seasonal_0 Loss: 0.1107 | 0.1526
Epoch 215/300, seasonal_0 Loss: 0.1114 | 0.1525
Epoch 216/300, seasonal_0 Loss: 0.1101 | 0.1525
Epoch 217/300, seasonal_0 Loss: 0.1107 | 0.1525
Epoch 218/300, seasonal_0 Loss: 0.1110 | 0.1525
Epoch 219/300, seasonal_0 Loss: 0.1109 | 0.1525
Epoch 220/300, seasonal_0 Loss: 0.1110 | 0.1525
Epoch 221/300, seasonal_0 Loss: 0.1112 | 0.1525
Epoch 222/300, seasonal_0 Loss: 0.1118 | 0.1525
Epoch 223/300, seasonal_0 Loss: 0.1114 | 0.1525
Epoch 224/300, seasonal_0 Loss: 0.1110 | 0.1525
Epoch 225/300, seasonal_0 Loss: 0.1111 | 0.1525
Epoch 226/300, seasonal_0 Loss: 0.1110 | 0.1525
Epoch 227/300, seasonal_0 Loss: 0.1112 | 0.1525
Epoch 228/300, seasonal_0 Loss: 0.1111 | 0.1525
Epoch 229/300, seasonal_0 Loss: 0.1110 | 0.1525
Epoch 230/300, seasonal_0 Loss: 0.1110 | 0.1525
Epoch 231/300, seasonal_0 Loss: 0.1113 | 0.1524
Epoch 232/300, seasonal_0 Loss: 0.1106 | 0.1524
Epoch 233/300, seasonal_0 Loss: 0.1110 | 0.1524
Epoch 234/300, seasonal_0 Loss: 0.1112 | 0.1524
Epoch 235/300, seasonal_0 Loss: 0.1108 | 0.1524
Epoch 236/300, seasonal_0 Loss: 0.1115 | 0.1524
Epoch 237/300, seasonal_0 Loss: 0.1116 | 0.1524
Epoch 238/300, seasonal_0 Loss: 0.1117 | 0.1524
Epoch 239/300, seasonal_0 Loss: 0.1110 | 0.1524
Epoch 240/300, seasonal_0 Loss: 0.1109 | 0.1524
Epoch 241/300, seasonal_0 Loss: 0.1116 | 0.1524
Epoch 242/300, seasonal_0 Loss: 0.1107 | 0.1524
Epoch 243/300, seasonal_0 Loss: 0.1107 | 0.1524
Epoch 244/300, seasonal_0 Loss: 0.1115 | 0.1524
Epoch 245/300, seasonal_0 Loss: 0.1118 | 0.1524
Epoch 246/300, seasonal_0 Loss: 0.1114 | 0.1524
Epoch 247/300, seasonal_0 Loss: 0.1112 | 0.1524
Epoch 248/300, seasonal_0 Loss: 0.1112 | 0.1523
Epoch 249/300, seasonal_0 Loss: 0.1117 | 0.1523
Epoch 250/300, seasonal_0 Loss: 0.1111 | 0.1523
Epoch 251/300, seasonal_0 Loss: 0.1113 | 0.1523
Epoch 252/300, seasonal_0 Loss: 0.1107 | 0.1523
Epoch 253/300, seasonal_0 Loss: 0.1110 | 0.1523
Epoch 254/300, seasonal_0 Loss: 0.1108 | 0.1523
Epoch 255/300, seasonal_0 Loss: 0.1107 | 0.1523
Epoch 256/300, seasonal_0 Loss: 0.1113 | 0.1523
Epoch 257/300, seasonal_0 Loss: 0.1109 | 0.1523
Epoch 258/300, seasonal_0 Loss: 0.1113 | 0.1523
Epoch 259/300, seasonal_0 Loss: 0.1118 | 0.1523
Epoch 260/300, seasonal_0 Loss: 0.1106 | 0.1523
Epoch 261/300, seasonal_0 Loss: 0.1109 | 0.1523
Epoch 262/300, seasonal_0 Loss: 0.1120 | 0.1523
Epoch 263/300, seasonal_0 Loss: 0.1111 | 0.1523
Epoch 264/300, seasonal_0 Loss: 0.1111 | 0.1523
Epoch 265/300, seasonal_0 Loss: 0.1109 | 0.1523
Epoch 266/300, seasonal_0 Loss: 0.1111 | 0.1523
Epoch 267/300, seasonal_0 Loss: 0.1110 | 0.1523
Epoch 268/300, seasonal_0 Loss: 0.1118 | 0.1523
Epoch 269/300, seasonal_0 Loss: 0.1109 | 0.1523
Epoch 270/300, seasonal_0 Loss: 0.1111 | 0.1523
Epoch 271/300, seasonal_0 Loss: 0.1110 | 0.1523
Epoch 272/300, seasonal_0 Loss: 0.1106 | 0.1523
Epoch 273/300, seasonal_0 Loss: 0.1109 | 0.1523
Epoch 274/300, seasonal_0 Loss: 0.1115 | 0.1523
Epoch 275/300, seasonal_0 Loss: 0.1109 | 0.1523
Epoch 276/300, seasonal_0 Loss: 0.1114 | 0.1523
Epoch 277/300, seasonal_0 Loss: 0.1108 | 0.1523
Epoch 278/300, seasonal_0 Loss: 0.1108 | 0.1522
Epoch 279/300, seasonal_0 Loss: 0.1111 | 0.1522
Epoch 280/300, seasonal_0 Loss: 0.1116 | 0.1522
Epoch 281/300, seasonal_0 Loss: 0.1114 | 0.1522
Epoch 282/300, seasonal_0 Loss: 0.1113 | 0.1522
Epoch 283/300, seasonal_0 Loss: 0.1102 | 0.1522
Epoch 284/300, seasonal_0 Loss: 0.1112 | 0.1522
Epoch 285/300, seasonal_0 Loss: 0.1101 | 0.1522
Epoch 286/300, seasonal_0 Loss: 0.1104 | 0.1522
Epoch 287/300, seasonal_0 Loss: 0.1112 | 0.1522
Epoch 288/300, seasonal_0 Loss: 0.1108 | 0.1522
Epoch 289/300, seasonal_0 Loss: 0.1113 | 0.1522
Epoch 290/300, seasonal_0 Loss: 0.1107 | 0.1522
Epoch 291/300, seasonal_0 Loss: 0.1112 | 0.1522
Epoch 292/300, seasonal_0 Loss: 0.1106 | 0.1522
Epoch 293/300, seasonal_0 Loss: 0.1106 | 0.1522
Epoch 294/300, seasonal_0 Loss: 0.1112 | 0.1522
Epoch 295/300, seasonal_0 Loss: 0.1105 | 0.1522
Epoch 296/300, seasonal_0 Loss: 0.1112 | 0.1522
Epoch 297/300, seasonal_0 Loss: 0.1109 | 0.1522
Epoch 298/300, seasonal_0 Loss: 0.1117 | 0.1522
Epoch 299/300, seasonal_0 Loss: 0.1114 | 0.1522
Epoch 300/300, seasonal_0 Loss: 0.1110 | 0.1522
Training seasonal_1 component with params: {'observation_period_num': 64, 'train_rates': 0.9849288674611233, 'learning_rate': 0.0004018332755047402, 'batch_size': 199, 'step_size': 4, 'gamma': 0.9456745742042806}
Epoch 1/300, seasonal_1 Loss: 1.0990 | 1.8876
Epoch 2/300, seasonal_1 Loss: 0.8297 | 1.1038
Epoch 3/300, seasonal_1 Loss: 0.6912 | 0.9467
Epoch 4/300, seasonal_1 Loss: 0.6327 | 0.8214
Epoch 5/300, seasonal_1 Loss: 0.7496 | 0.7314
Epoch 6/300, seasonal_1 Loss: 0.5661 | 0.6890
Epoch 7/300, seasonal_1 Loss: 0.4936 | 0.6626
Epoch 8/300, seasonal_1 Loss: 0.4601 | 0.5912
Epoch 9/300, seasonal_1 Loss: 0.4346 | 0.5412
Epoch 10/300, seasonal_1 Loss: 0.4672 | 0.5460
Epoch 11/300, seasonal_1 Loss: 0.4782 | 0.6052
Epoch 12/300, seasonal_1 Loss: 0.4700 | 0.5366
Epoch 13/300, seasonal_1 Loss: 0.3946 | 0.4948
Epoch 14/300, seasonal_1 Loss: 0.3562 | 0.4531
Epoch 15/300, seasonal_1 Loss: 0.3221 | 0.4159
Epoch 16/300, seasonal_1 Loss: 0.3080 | 0.3809
Epoch 17/300, seasonal_1 Loss: 0.2977 | 0.3561
Epoch 18/300, seasonal_1 Loss: 0.2915 | 0.3378
Epoch 19/300, seasonal_1 Loss: 0.2843 | 0.3326
Epoch 20/300, seasonal_1 Loss: 0.3279 | 0.3459
Epoch 21/300, seasonal_1 Loss: 0.3108 | 0.3641
Epoch 22/300, seasonal_1 Loss: 0.2970 | 0.3226
Epoch 23/300, seasonal_1 Loss: 0.2518 | 0.3031
Epoch 24/300, seasonal_1 Loss: 0.2429 | 0.2972
Epoch 25/300, seasonal_1 Loss: 0.2280 | 0.2840
Epoch 26/300, seasonal_1 Loss: 0.2196 | 0.2690
Epoch 27/300, seasonal_1 Loss: 0.2134 | 0.2595
Epoch 28/300, seasonal_1 Loss: 0.2125 | 0.2558
Epoch 29/300, seasonal_1 Loss: 0.2117 | 0.2483
Epoch 30/300, seasonal_1 Loss: 0.2122 | 0.2407
Epoch 31/300, seasonal_1 Loss: 0.2087 | 0.2377
Epoch 32/300, seasonal_1 Loss: 0.2035 | 0.2289
Epoch 33/300, seasonal_1 Loss: 0.2023 | 0.2293
Epoch 34/300, seasonal_1 Loss: 0.1976 | 0.2214
Epoch 35/300, seasonal_1 Loss: 0.1946 | 0.2212
Epoch 36/300, seasonal_1 Loss: 0.1896 | 0.2141
Epoch 37/300, seasonal_1 Loss: 0.1871 | 0.2150
Epoch 38/300, seasonal_1 Loss: 0.1833 | 0.2071
Epoch 39/300, seasonal_1 Loss: 0.1807 | 0.2100
Epoch 40/300, seasonal_1 Loss: 0.1764 | 0.2005
Epoch 41/300, seasonal_1 Loss: 0.1770 | 0.2065
Epoch 42/300, seasonal_1 Loss: 0.1738 | 0.1948
Epoch 43/300, seasonal_1 Loss: 0.1734 | 0.1997
Epoch 44/300, seasonal_1 Loss: 0.1714 | 0.1919
Epoch 45/300, seasonal_1 Loss: 0.1716 | 0.1918
Epoch 46/300, seasonal_1 Loss: 0.1699 | 0.1884
Epoch 47/300, seasonal_1 Loss: 0.1682 | 0.1894
Epoch 48/300, seasonal_1 Loss: 0.1667 | 0.1848
Epoch 49/300, seasonal_1 Loss: 0.1645 | 0.1863
Epoch 50/300, seasonal_1 Loss: 0.1633 | 0.1817
Epoch 51/300, seasonal_1 Loss: 0.1611 | 0.1836
Epoch 52/300, seasonal_1 Loss: 0.1589 | 0.1788
Epoch 53/300, seasonal_1 Loss: 0.1577 | 0.1810
Epoch 54/300, seasonal_1 Loss: 0.1556 | 0.1768
Epoch 55/300, seasonal_1 Loss: 0.1548 | 0.1770
Epoch 56/300, seasonal_1 Loss: 0.1548 | 0.1748
Epoch 57/300, seasonal_1 Loss: 0.1548 | 0.1752
Epoch 58/300, seasonal_1 Loss: 0.1527 | 0.1731
Epoch 59/300, seasonal_1 Loss: 0.1527 | 0.1730
Epoch 60/300, seasonal_1 Loss: 0.1512 | 0.1716
Epoch 61/300, seasonal_1 Loss: 0.1503 | 0.1711
Epoch 62/300, seasonal_1 Loss: 0.1496 | 0.1699
Epoch 63/300, seasonal_1 Loss: 0.1492 | 0.1684
Epoch 64/300, seasonal_1 Loss: 0.1478 | 0.1689
Epoch 65/300, seasonal_1 Loss: 0.1468 | 0.1680
Epoch 66/300, seasonal_1 Loss: 0.1467 | 0.1680
Epoch 67/300, seasonal_1 Loss: 0.1453 | 0.1670
Epoch 68/300, seasonal_1 Loss: 0.1450 | 0.1656
Epoch 69/300, seasonal_1 Loss: 0.1442 | 0.1655
Epoch 70/300, seasonal_1 Loss: 0.1426 | 0.1648
Epoch 71/300, seasonal_1 Loss: 0.1425 | 0.1641
Epoch 72/300, seasonal_1 Loss: 0.1420 | 0.1636
Epoch 73/300, seasonal_1 Loss: 0.1418 | 0.1627
Epoch 74/300, seasonal_1 Loss: 0.1413 | 0.1619
Epoch 75/300, seasonal_1 Loss: 0.1408 | 0.1620
Epoch 76/300, seasonal_1 Loss: 0.1394 | 0.1615
Epoch 77/300, seasonal_1 Loss: 0.1396 | 0.1611
Epoch 78/300, seasonal_1 Loss: 0.1394 | 0.1608
Epoch 79/300, seasonal_1 Loss: 0.1378 | 0.1604
Epoch 80/300, seasonal_1 Loss: 0.1388 | 0.1593
Epoch 81/300, seasonal_1 Loss: 0.1379 | 0.1590
Epoch 82/300, seasonal_1 Loss: 0.1373 | 0.1588
Epoch 83/300, seasonal_1 Loss: 0.1365 | 0.1584
Epoch 84/300, seasonal_1 Loss: 0.1365 | 0.1571
Epoch 85/300, seasonal_1 Loss: 0.1358 | 0.1577
Epoch 86/300, seasonal_1 Loss: 0.1362 | 0.1569
Epoch 87/300, seasonal_1 Loss: 0.1352 | 0.1562
Epoch 88/300, seasonal_1 Loss: 0.1355 | 0.1557
Epoch 89/300, seasonal_1 Loss: 0.1350 | 0.1562
Epoch 90/300, seasonal_1 Loss: 0.1343 | 0.1558
Epoch 91/300, seasonal_1 Loss: 0.1336 | 0.1546
Epoch 92/300, seasonal_1 Loss: 0.1344 | 0.1546
Epoch 93/300, seasonal_1 Loss: 0.1338 | 0.1544
Epoch 94/300, seasonal_1 Loss: 0.1338 | 0.1544
Epoch 95/300, seasonal_1 Loss: 0.1333 | 0.1543
Epoch 96/300, seasonal_1 Loss: 0.1328 | 0.1535
Epoch 97/300, seasonal_1 Loss: 0.1329 | 0.1529
Epoch 98/300, seasonal_1 Loss: 0.1318 | 0.1526
Epoch 99/300, seasonal_1 Loss: 0.1317 | 0.1519
Epoch 100/300, seasonal_1 Loss: 0.1318 | 0.1514
Epoch 101/300, seasonal_1 Loss: 0.1317 | 0.1514
Epoch 102/300, seasonal_1 Loss: 0.1307 | 0.1519
Epoch 103/300, seasonal_1 Loss: 0.1310 | 0.1512
Epoch 104/300, seasonal_1 Loss: 0.1304 | 0.1514
Epoch 105/300, seasonal_1 Loss: 0.1302 | 0.1507
Epoch 106/300, seasonal_1 Loss: 0.1302 | 0.1505
Epoch 107/300, seasonal_1 Loss: 0.1300 | 0.1503
Epoch 108/300, seasonal_1 Loss: 0.1299 | 0.1499
Epoch 109/300, seasonal_1 Loss: 0.1296 | 0.1502
Epoch 110/300, seasonal_1 Loss: 0.1302 | 0.1501
Epoch 111/300, seasonal_1 Loss: 0.1289 | 0.1496
Epoch 112/300, seasonal_1 Loss: 0.1290 | 0.1496
Epoch 113/300, seasonal_1 Loss: 0.1286 | 0.1495
Epoch 114/300, seasonal_1 Loss: 0.1282 | 0.1492
Epoch 115/300, seasonal_1 Loss: 0.1282 | 0.1487
Epoch 116/300, seasonal_1 Loss: 0.1285 | 0.1488
Epoch 117/300, seasonal_1 Loss: 0.1287 | 0.1485
Epoch 118/300, seasonal_1 Loss: 0.1270 | 0.1483
Epoch 119/300, seasonal_1 Loss: 0.1275 | 0.1482
Epoch 120/300, seasonal_1 Loss: 0.1269 | 0.1481
Epoch 121/300, seasonal_1 Loss: 0.1270 | 0.1482
Epoch 122/300, seasonal_1 Loss: 0.1276 | 0.1480
Epoch 123/300, seasonal_1 Loss: 0.1269 | 0.1478
Epoch 124/300, seasonal_1 Loss: 0.1268 | 0.1478
Epoch 125/300, seasonal_1 Loss: 0.1275 | 0.1476
Epoch 126/300, seasonal_1 Loss: 0.1262 | 0.1472
Epoch 127/300, seasonal_1 Loss: 0.1265 | 0.1469
Epoch 128/300, seasonal_1 Loss: 0.1269 | 0.1468
Epoch 129/300, seasonal_1 Loss: 0.1263 | 0.1472
Epoch 130/300, seasonal_1 Loss: 0.1256 | 0.1468
Epoch 131/300, seasonal_1 Loss: 0.1258 | 0.1465
Epoch 132/300, seasonal_1 Loss: 0.1251 | 0.1463
Epoch 133/300, seasonal_1 Loss: 0.1249 | 0.1463
Epoch 134/300, seasonal_1 Loss: 0.1258 | 0.1463
Epoch 135/300, seasonal_1 Loss: 0.1252 | 0.1460
Epoch 136/300, seasonal_1 Loss: 0.1257 | 0.1460
Epoch 137/300, seasonal_1 Loss: 0.1249 | 0.1458
Epoch 138/300, seasonal_1 Loss: 0.1261 | 0.1458
Epoch 139/300, seasonal_1 Loss: 0.1252 | 0.1460
Epoch 140/300, seasonal_1 Loss: 0.1247 | 0.1459
Epoch 141/300, seasonal_1 Loss: 0.1249 | 0.1457
Epoch 142/300, seasonal_1 Loss: 0.1249 | 0.1453
Epoch 143/300, seasonal_1 Loss: 0.1241 | 0.1450
Epoch 144/300, seasonal_1 Loss: 0.1246 | 0.1452
Epoch 145/300, seasonal_1 Loss: 0.1250 | 0.1455
Epoch 146/300, seasonal_1 Loss: 0.1240 | 0.1454
Epoch 147/300, seasonal_1 Loss: 0.1243 | 0.1450
Epoch 148/300, seasonal_1 Loss: 0.1242 | 0.1449
Epoch 149/300, seasonal_1 Loss: 0.1240 | 0.1450
Epoch 150/300, seasonal_1 Loss: 0.1238 | 0.1447
Epoch 151/300, seasonal_1 Loss: 0.1233 | 0.1447
Epoch 152/300, seasonal_1 Loss: 0.1236 | 0.1446
Epoch 153/300, seasonal_1 Loss: 0.1240 | 0.1444
Epoch 154/300, seasonal_1 Loss: 0.1237 | 0.1445
Epoch 155/300, seasonal_1 Loss: 0.1233 | 0.1445
Epoch 156/300, seasonal_1 Loss: 0.1230 | 0.1443
Epoch 157/300, seasonal_1 Loss: 0.1237 | 0.1444
Epoch 158/300, seasonal_1 Loss: 0.1236 | 0.1441
Epoch 159/300, seasonal_1 Loss: 0.1237 | 0.1441
Epoch 160/300, seasonal_1 Loss: 0.1234 | 0.1439
Epoch 161/300, seasonal_1 Loss: 0.1235 | 0.1439
Epoch 162/300, seasonal_1 Loss: 0.1232 | 0.1438
Epoch 163/300, seasonal_1 Loss: 0.1236 | 0.1436
Epoch 164/300, seasonal_1 Loss: 0.1234 | 0.1437
Epoch 165/300, seasonal_1 Loss: 0.1230 | 0.1438
Epoch 166/300, seasonal_1 Loss: 0.1230 | 0.1437
Epoch 167/300, seasonal_1 Loss: 0.1225 | 0.1436
Epoch 168/300, seasonal_1 Loss: 0.1236 | 0.1435
Epoch 169/300, seasonal_1 Loss: 0.1227 | 0.1434
Epoch 170/300, seasonal_1 Loss: 0.1226 | 0.1434
Epoch 171/300, seasonal_1 Loss: 0.1223 | 0.1432
Epoch 172/300, seasonal_1 Loss: 0.1229 | 0.1432
Epoch 173/300, seasonal_1 Loss: 0.1225 | 0.1432
Epoch 174/300, seasonal_1 Loss: 0.1223 | 0.1433
Epoch 175/300, seasonal_1 Loss: 0.1222 | 0.1434
Epoch 176/300, seasonal_1 Loss: 0.1224 | 0.1434
Epoch 177/300, seasonal_1 Loss: 0.1226 | 0.1433
Epoch 178/300, seasonal_1 Loss: 0.1218 | 0.1432
Epoch 179/300, seasonal_1 Loss: 0.1225 | 0.1432
Epoch 180/300, seasonal_1 Loss: 0.1218 | 0.1433
Epoch 181/300, seasonal_1 Loss: 0.1230 | 0.1433
Epoch 182/300, seasonal_1 Loss: 0.1222 | 0.1432
Epoch 183/300, seasonal_1 Loss: 0.1224 | 0.1431
Epoch 184/300, seasonal_1 Loss: 0.1220 | 0.1430
Epoch 185/300, seasonal_1 Loss: 0.1214 | 0.1429
Epoch 186/300, seasonal_1 Loss: 0.1220 | 0.1430
Epoch 187/300, seasonal_1 Loss: 0.1225 | 0.1428
Epoch 188/300, seasonal_1 Loss: 0.1225 | 0.1426
Epoch 189/300, seasonal_1 Loss: 0.1222 | 0.1427
Epoch 190/300, seasonal_1 Loss: 0.1216 | 0.1426
Epoch 191/300, seasonal_1 Loss: 0.1226 | 0.1425
Epoch 192/300, seasonal_1 Loss: 0.1215 | 0.1425
Epoch 193/300, seasonal_1 Loss: 0.1215 | 0.1425
Epoch 194/300, seasonal_1 Loss: 0.1226 | 0.1425
Epoch 195/300, seasonal_1 Loss: 0.1217 | 0.1424
Epoch 196/300, seasonal_1 Loss: 0.1220 | 0.1425
Epoch 197/300, seasonal_1 Loss: 0.1219 | 0.1425
Epoch 198/300, seasonal_1 Loss: 0.1221 | 0.1425
Epoch 199/300, seasonal_1 Loss: 0.1217 | 0.1425
Epoch 200/300, seasonal_1 Loss: 0.1216 | 0.1425
Epoch 201/300, seasonal_1 Loss: 0.1222 | 0.1425
Epoch 202/300, seasonal_1 Loss: 0.1217 | 0.1425
Epoch 203/300, seasonal_1 Loss: 0.1217 | 0.1424
Epoch 204/300, seasonal_1 Loss: 0.1218 | 0.1424
Epoch 205/300, seasonal_1 Loss: 0.1219 | 0.1424
Epoch 206/300, seasonal_1 Loss: 0.1209 | 0.1424
Epoch 207/300, seasonal_1 Loss: 0.1212 | 0.1425
Epoch 208/300, seasonal_1 Loss: 0.1215 | 0.1425
Epoch 209/300, seasonal_1 Loss: 0.1216 | 0.1425
Epoch 210/300, seasonal_1 Loss: 0.1211 | 0.1425
Epoch 211/300, seasonal_1 Loss: 0.1213 | 0.1424
Epoch 212/300, seasonal_1 Loss: 0.1216 | 0.1423
Epoch 213/300, seasonal_1 Loss: 0.1215 | 0.1422
Epoch 214/300, seasonal_1 Loss: 0.1217 | 0.1422
Epoch 215/300, seasonal_1 Loss: 0.1211 | 0.1422
Epoch 216/300, seasonal_1 Loss: 0.1217 | 0.1422
Epoch 217/300, seasonal_1 Loss: 0.1217 | 0.1422
Epoch 218/300, seasonal_1 Loss: 0.1214 | 0.1422
Epoch 219/300, seasonal_1 Loss: 0.1216 | 0.1422
Epoch 220/300, seasonal_1 Loss: 0.1219 | 0.1421
Epoch 221/300, seasonal_1 Loss: 0.1217 | 0.1421
Epoch 222/300, seasonal_1 Loss: 0.1217 | 0.1421
Epoch 223/300, seasonal_1 Loss: 0.1214 | 0.1421
Epoch 224/300, seasonal_1 Loss: 0.1213 | 0.1421
Epoch 225/300, seasonal_1 Loss: 0.1216 | 0.1421
Epoch 226/300, seasonal_1 Loss: 0.1207 | 0.1421
Epoch 227/300, seasonal_1 Loss: 0.1215 | 0.1420
Epoch 228/300, seasonal_1 Loss: 0.1215 | 0.1420
Epoch 229/300, seasonal_1 Loss: 0.1208 | 0.1420
Epoch 230/300, seasonal_1 Loss: 0.1210 | 0.1420
Epoch 231/300, seasonal_1 Loss: 0.1206 | 0.1420
Epoch 232/300, seasonal_1 Loss: 0.1212 | 0.1420
Epoch 233/300, seasonal_1 Loss: 0.1217 | 0.1421
Epoch 234/300, seasonal_1 Loss: 0.1212 | 0.1420
Epoch 235/300, seasonal_1 Loss: 0.1213 | 0.1420
Epoch 236/300, seasonal_1 Loss: 0.1216 | 0.1421
Epoch 237/300, seasonal_1 Loss: 0.1212 | 0.1421
Epoch 238/300, seasonal_1 Loss: 0.1211 | 0.1421
Epoch 239/300, seasonal_1 Loss: 0.1215 | 0.1421
Epoch 240/300, seasonal_1 Loss: 0.1211 | 0.1420
Epoch 241/300, seasonal_1 Loss: 0.1216 | 0.1420
Epoch 242/300, seasonal_1 Loss: 0.1213 | 0.1419
Epoch 243/300, seasonal_1 Loss: 0.1212 | 0.1419
Epoch 244/300, seasonal_1 Loss: 0.1210 | 0.1419
Epoch 245/300, seasonal_1 Loss: 0.1220 | 0.1419
Epoch 246/300, seasonal_1 Loss: 0.1220 | 0.1419
Epoch 247/300, seasonal_1 Loss: 0.1212 | 0.1419
Epoch 248/300, seasonal_1 Loss: 0.1211 | 0.1419
Epoch 249/300, seasonal_1 Loss: 0.1215 | 0.1419
Epoch 250/300, seasonal_1 Loss: 0.1207 | 0.1419
Epoch 251/300, seasonal_1 Loss: 0.1208 | 0.1419
Epoch 252/300, seasonal_1 Loss: 0.1214 | 0.1418
Epoch 253/300, seasonal_1 Loss: 0.1217 | 0.1418
Epoch 254/300, seasonal_1 Loss: 0.1216 | 0.1418
Epoch 255/300, seasonal_1 Loss: 0.1204 | 0.1418
Epoch 256/300, seasonal_1 Loss: 0.1204 | 0.1418
Epoch 257/300, seasonal_1 Loss: 0.1212 | 0.1418
Epoch 258/300, seasonal_1 Loss: 0.1209 | 0.1418
Epoch 259/300, seasonal_1 Loss: 0.1212 | 0.1418
Epoch 260/300, seasonal_1 Loss: 0.1207 | 0.1418
Epoch 261/300, seasonal_1 Loss: 0.1206 | 0.1418
Epoch 262/300, seasonal_1 Loss: 0.1216 | 0.1418
Epoch 263/300, seasonal_1 Loss: 0.1211 | 0.1418
Epoch 264/300, seasonal_1 Loss: 0.1212 | 0.1418
Epoch 265/300, seasonal_1 Loss: 0.1212 | 0.1418
Epoch 266/300, seasonal_1 Loss: 0.1210 | 0.1418
Epoch 267/300, seasonal_1 Loss: 0.1211 | 0.1418
Epoch 268/300, seasonal_1 Loss: 0.1210 | 0.1418
Epoch 269/300, seasonal_1 Loss: 0.1206 | 0.1418
Epoch 270/300, seasonal_1 Loss: 0.1205 | 0.1418
Epoch 271/300, seasonal_1 Loss: 0.1208 | 0.1418
Epoch 272/300, seasonal_1 Loss: 0.1215 | 0.1418
Epoch 273/300, seasonal_1 Loss: 0.1209 | 0.1418
Epoch 274/300, seasonal_1 Loss: 0.1217 | 0.1418
Epoch 275/300, seasonal_1 Loss: 0.1216 | 0.1418
Epoch 276/300, seasonal_1 Loss: 0.1213 | 0.1418
Epoch 277/300, seasonal_1 Loss: 0.1210 | 0.1418
Epoch 278/300, seasonal_1 Loss: 0.1211 | 0.1418
Epoch 279/300, seasonal_1 Loss: 0.1207 | 0.1418
Epoch 280/300, seasonal_1 Loss: 0.1213 | 0.1417
Epoch 281/300, seasonal_1 Loss: 0.1212 | 0.1417
Epoch 282/300, seasonal_1 Loss: 0.1210 | 0.1417
Epoch 283/300, seasonal_1 Loss: 0.1207 | 0.1417
Epoch 284/300, seasonal_1 Loss: 0.1208 | 0.1417
Epoch 285/300, seasonal_1 Loss: 0.1207 | 0.1417
Epoch 286/300, seasonal_1 Loss: 0.1209 | 0.1417
Epoch 287/300, seasonal_1 Loss: 0.1211 | 0.1417
Epoch 288/300, seasonal_1 Loss: 0.1215 | 0.1417
Epoch 289/300, seasonal_1 Loss: 0.1210 | 0.1417
Epoch 290/300, seasonal_1 Loss: 0.1218 | 0.1417
Epoch 291/300, seasonal_1 Loss: 0.1207 | 0.1417
Epoch 292/300, seasonal_1 Loss: 0.1214 | 0.1417
Epoch 293/300, seasonal_1 Loss: 0.1201 | 0.1417
Epoch 294/300, seasonal_1 Loss: 0.1210 | 0.1417
Epoch 295/300, seasonal_1 Loss: 0.1212 | 0.1417
Epoch 296/300, seasonal_1 Loss: 0.1209 | 0.1417
Epoch 297/300, seasonal_1 Loss: 0.1207 | 0.1417
Epoch 298/300, seasonal_1 Loss: 0.1213 | 0.1417
Epoch 299/300, seasonal_1 Loss: 0.1210 | 0.1417
Epoch 300/300, seasonal_1 Loss: 0.1214 | 0.1417
Training seasonal_2 component with params: {'observation_period_num': 54, 'train_rates': 0.9485637682521927, 'learning_rate': 0.00018943127353505577, 'batch_size': 37, 'step_size': 12, 'gamma': 0.7956697416547664}
Epoch 1/300, seasonal_2 Loss: 0.6825 | 1.0225
Epoch 2/300, seasonal_2 Loss: 0.5624 | 0.8085
Epoch 3/300, seasonal_2 Loss: 0.5364 | 0.6313
Epoch 4/300, seasonal_2 Loss: 0.4220 | 0.5626
Epoch 5/300, seasonal_2 Loss: 0.4053 | 0.5018
Epoch 6/300, seasonal_2 Loss: 0.3682 | 0.4638
Epoch 7/300, seasonal_2 Loss: 0.3441 | 0.3813
Epoch 8/300, seasonal_2 Loss: 0.3183 | 0.3803
Epoch 9/300, seasonal_2 Loss: 0.3019 | 0.3678
Epoch 10/300, seasonal_2 Loss: 0.3021 | 0.3159
Epoch 11/300, seasonal_2 Loss: 0.2977 | 0.2967
Epoch 12/300, seasonal_2 Loss: 0.3288 | 0.2893
Epoch 13/300, seasonal_2 Loss: 0.2665 | 0.2617
Epoch 14/300, seasonal_2 Loss: 0.2433 | 0.2822
Epoch 15/300, seasonal_2 Loss: 0.2489 | 0.2582
Epoch 16/300, seasonal_2 Loss: 0.2755 | 0.2475
Epoch 17/300, seasonal_2 Loss: 0.2915 | 0.2595
Epoch 18/300, seasonal_2 Loss: 0.2768 | 0.2809
Epoch 19/300, seasonal_2 Loss: 0.2544 | 0.2318
Epoch 20/300, seasonal_2 Loss: 0.2064 | 0.2081
Epoch 21/300, seasonal_2 Loss: 0.1955 | 0.2095
Epoch 22/300, seasonal_2 Loss: 0.2001 | 0.2159
Epoch 23/300, seasonal_2 Loss: 0.1919 | 0.2229
Epoch 24/300, seasonal_2 Loss: 0.1988 | 0.2383
Epoch 25/300, seasonal_2 Loss: 0.1945 | 0.2054
Epoch 26/300, seasonal_2 Loss: 0.1945 | 0.1929
Epoch 27/300, seasonal_2 Loss: 0.2123 | 0.1963
Epoch 28/300, seasonal_2 Loss: 0.1861 | 0.2024
Epoch 29/300, seasonal_2 Loss: 0.1779 | 0.1883
Epoch 30/300, seasonal_2 Loss: 0.1681 | 0.1870
Epoch 31/300, seasonal_2 Loss: 0.1620 | 0.1823
Epoch 32/300, seasonal_2 Loss: 0.1566 | 0.1772
Epoch 33/300, seasonal_2 Loss: 0.1498 | 0.1722
Epoch 34/300, seasonal_2 Loss: 0.1464 | 0.1713
Epoch 35/300, seasonal_2 Loss: 0.1440 | 0.1677
Epoch 36/300, seasonal_2 Loss: 0.1404 | 0.1680
Epoch 37/300, seasonal_2 Loss: 0.1382 | 0.1641
Epoch 38/300, seasonal_2 Loss: 0.1373 | 0.1642
Epoch 39/300, seasonal_2 Loss: 0.1362 | 0.1619
Epoch 40/300, seasonal_2 Loss: 0.1339 | 0.1574
Epoch 41/300, seasonal_2 Loss: 0.1321 | 0.1649
Epoch 42/300, seasonal_2 Loss: 0.1314 | 0.1536
Epoch 43/300, seasonal_2 Loss: 0.1295 | 0.1545
Epoch 44/300, seasonal_2 Loss: 0.1283 | 0.1582
Epoch 45/300, seasonal_2 Loss: 0.1276 | 0.1516
Epoch 46/300, seasonal_2 Loss: 0.1262 | 0.1509
Epoch 47/300, seasonal_2 Loss: 0.1253 | 0.1553
Epoch 48/300, seasonal_2 Loss: 0.1230 | 0.1456
Epoch 49/300, seasonal_2 Loss: 0.1237 | 0.1505
Epoch 50/300, seasonal_2 Loss: 0.1242 | 0.1488
Epoch 51/300, seasonal_2 Loss: 0.1214 | 0.1448
Epoch 52/300, seasonal_2 Loss: 0.1203 | 0.1490
Epoch 53/300, seasonal_2 Loss: 0.1198 | 0.1449
Epoch 54/300, seasonal_2 Loss: 0.1163 | 0.1412
Epoch 55/300, seasonal_2 Loss: 0.1158 | 0.1440
Epoch 56/300, seasonal_2 Loss: 0.1142 | 0.1406
Epoch 57/300, seasonal_2 Loss: 0.1128 | 0.1403
Epoch 58/300, seasonal_2 Loss: 0.1119 | 0.1406
Epoch 59/300, seasonal_2 Loss: 0.1110 | 0.1401
Epoch 60/300, seasonal_2 Loss: 0.1098 | 0.1366
Epoch 61/300, seasonal_2 Loss: 0.1097 | 0.1393
Epoch 62/300, seasonal_2 Loss: 0.1083 | 0.1370
Epoch 63/300, seasonal_2 Loss: 0.1087 | 0.1365
Epoch 64/300, seasonal_2 Loss: 0.1075 | 0.1373
Epoch 65/300, seasonal_2 Loss: 0.1069 | 0.1360
Epoch 66/300, seasonal_2 Loss: 0.1073 | 0.1353
Epoch 67/300, seasonal_2 Loss: 0.1064 | 0.1343
Epoch 68/300, seasonal_2 Loss: 0.1061 | 0.1362
Epoch 69/300, seasonal_2 Loss: 0.1051 | 0.1340
Epoch 70/300, seasonal_2 Loss: 0.1038 | 0.1336
Epoch 71/300, seasonal_2 Loss: 0.1044 | 0.1335
Epoch 72/300, seasonal_2 Loss: 0.1030 | 0.1336
Epoch 73/300, seasonal_2 Loss: 0.1026 | 0.1310
Epoch 74/300, seasonal_2 Loss: 0.1014 | 0.1321
Epoch 75/300, seasonal_2 Loss: 0.1007 | 0.1311
Epoch 76/300, seasonal_2 Loss: 0.1004 | 0.1304
Epoch 77/300, seasonal_2 Loss: 0.0995 | 0.1307
Epoch 78/300, seasonal_2 Loss: 0.1002 | 0.1291
Epoch 79/300, seasonal_2 Loss: 0.0990 | 0.1291
Epoch 80/300, seasonal_2 Loss: 0.0990 | 0.1298
Epoch 81/300, seasonal_2 Loss: 0.0978 | 0.1289
Epoch 82/300, seasonal_2 Loss: 0.0978 | 0.1293
Epoch 83/300, seasonal_2 Loss: 0.0980 | 0.1286
Epoch 84/300, seasonal_2 Loss: 0.0976 | 0.1276
Epoch 85/300, seasonal_2 Loss: 0.0966 | 0.1288
Epoch 86/300, seasonal_2 Loss: 0.0967 | 0.1274
Epoch 87/300, seasonal_2 Loss: 0.0956 | 0.1276
Epoch 88/300, seasonal_2 Loss: 0.0958 | 0.1280
Epoch 89/300, seasonal_2 Loss: 0.0957 | 0.1272
Epoch 90/300, seasonal_2 Loss: 0.0952 | 0.1276
Epoch 91/300, seasonal_2 Loss: 0.0959 | 0.1267
Epoch 92/300, seasonal_2 Loss: 0.0950 | 0.1264
Epoch 93/300, seasonal_2 Loss: 0.0948 | 0.1262
Epoch 94/300, seasonal_2 Loss: 0.0953 | 0.1258
Epoch 95/300, seasonal_2 Loss: 0.0946 | 0.1260
Epoch 96/300, seasonal_2 Loss: 0.0937 | 0.1270
Epoch 97/300, seasonal_2 Loss: 0.0939 | 0.1259
Epoch 98/300, seasonal_2 Loss: 0.0941 | 0.1257
Epoch 99/300, seasonal_2 Loss: 0.0938 | 0.1256
Epoch 100/300, seasonal_2 Loss: 0.0939 | 0.1251
Epoch 101/300, seasonal_2 Loss: 0.0933 | 0.1254
Epoch 102/300, seasonal_2 Loss: 0.0922 | 0.1249
Epoch 103/300, seasonal_2 Loss: 0.0923 | 0.1247
Epoch 104/300, seasonal_2 Loss: 0.0922 | 0.1246
Epoch 105/300, seasonal_2 Loss: 0.0926 | 0.1236
Epoch 106/300, seasonal_2 Loss: 0.0921 | 0.1245
Epoch 107/300, seasonal_2 Loss: 0.0926 | 0.1234
Epoch 108/300, seasonal_2 Loss: 0.0926 | 0.1237
Epoch 109/300, seasonal_2 Loss: 0.0914 | 0.1238
Epoch 110/300, seasonal_2 Loss: 0.0927 | 0.1239
Epoch 111/300, seasonal_2 Loss: 0.0924 | 0.1238
Epoch 112/300, seasonal_2 Loss: 0.0915 | 0.1234
Epoch 113/300, seasonal_2 Loss: 0.0920 | 0.1236
Epoch 114/300, seasonal_2 Loss: 0.0910 | 0.1239
Epoch 115/300, seasonal_2 Loss: 0.0926 | 0.1239
Epoch 116/300, seasonal_2 Loss: 0.0909 | 0.1236
Epoch 117/300, seasonal_2 Loss: 0.0913 | 0.1235
Epoch 118/300, seasonal_2 Loss: 0.0912 | 0.1231
Epoch 119/300, seasonal_2 Loss: 0.0915 | 0.1229
Epoch 120/300, seasonal_2 Loss: 0.0904 | 0.1229
Epoch 121/300, seasonal_2 Loss: 0.0899 | 0.1231
Epoch 122/300, seasonal_2 Loss: 0.0910 | 0.1229
Epoch 123/300, seasonal_2 Loss: 0.0909 | 0.1228
Epoch 124/300, seasonal_2 Loss: 0.0906 | 0.1228
Epoch 125/300, seasonal_2 Loss: 0.0905 | 0.1226
Epoch 126/300, seasonal_2 Loss: 0.0905 | 0.1223
Epoch 127/300, seasonal_2 Loss: 0.0905 | 0.1223
Epoch 128/300, seasonal_2 Loss: 0.0911 | 0.1224
Epoch 129/300, seasonal_2 Loss: 0.0901 | 0.1224
Epoch 130/300, seasonal_2 Loss: 0.0899 | 0.1226
Epoch 131/300, seasonal_2 Loss: 0.0899 | 0.1222
Epoch 132/300, seasonal_2 Loss: 0.0898 | 0.1222
Epoch 133/300, seasonal_2 Loss: 0.0897 | 0.1224
Epoch 134/300, seasonal_2 Loss: 0.0909 | 0.1221
Epoch 135/300, seasonal_2 Loss: 0.0896 | 0.1220
Epoch 136/300, seasonal_2 Loss: 0.0900 | 0.1218
Epoch 137/300, seasonal_2 Loss: 0.0899 | 0.1220
Epoch 138/300, seasonal_2 Loss: 0.0901 | 0.1222
Epoch 139/300, seasonal_2 Loss: 0.0897 | 0.1222
Epoch 140/300, seasonal_2 Loss: 0.0899 | 0.1220
Epoch 141/300, seasonal_2 Loss: 0.0897 | 0.1220
Epoch 142/300, seasonal_2 Loss: 0.0901 | 0.1220
Epoch 143/300, seasonal_2 Loss: 0.0901 | 0.1220
Epoch 144/300, seasonal_2 Loss: 0.0896 | 0.1222
Epoch 145/300, seasonal_2 Loss: 0.0900 | 0.1221
Epoch 146/300, seasonal_2 Loss: 0.0899 | 0.1219
Epoch 147/300, seasonal_2 Loss: 0.0896 | 0.1219
Epoch 148/300, seasonal_2 Loss: 0.0899 | 0.1218
Epoch 149/300, seasonal_2 Loss: 0.0895 | 0.1216
Epoch 150/300, seasonal_2 Loss: 0.0895 | 0.1216
Epoch 151/300, seasonal_2 Loss: 0.0893 | 0.1217
Epoch 152/300, seasonal_2 Loss: 0.0896 | 0.1217
Epoch 153/300, seasonal_2 Loss: 0.0888 | 0.1217
Epoch 154/300, seasonal_2 Loss: 0.0895 | 0.1216
Epoch 155/300, seasonal_2 Loss: 0.0899 | 0.1217
Epoch 156/300, seasonal_2 Loss: 0.0888 | 0.1216
Epoch 157/300, seasonal_2 Loss: 0.0895 | 0.1215
Epoch 158/300, seasonal_2 Loss: 0.0894 | 0.1216
Epoch 159/300, seasonal_2 Loss: 0.0891 | 0.1215
Epoch 160/300, seasonal_2 Loss: 0.0891 | 0.1215
Epoch 161/300, seasonal_2 Loss: 0.0902 | 0.1214
Epoch 162/300, seasonal_2 Loss: 0.0883 | 0.1214
Epoch 163/300, seasonal_2 Loss: 0.0893 | 0.1214
Epoch 164/300, seasonal_2 Loss: 0.0889 | 0.1214
Epoch 165/300, seasonal_2 Loss: 0.0893 | 0.1214
Epoch 166/300, seasonal_2 Loss: 0.0888 | 0.1215
Epoch 167/300, seasonal_2 Loss: 0.0888 | 0.1215
Epoch 168/300, seasonal_2 Loss: 0.0893 | 0.1215
Epoch 169/300, seasonal_2 Loss: 0.0900 | 0.1214
Epoch 170/300, seasonal_2 Loss: 0.0895 | 0.1214
Epoch 171/300, seasonal_2 Loss: 0.0886 | 0.1214
Epoch 172/300, seasonal_2 Loss: 0.0887 | 0.1213
Epoch 173/300, seasonal_2 Loss: 0.0892 | 0.1213
Epoch 174/300, seasonal_2 Loss: 0.0896 | 0.1214
Epoch 175/300, seasonal_2 Loss: 0.0891 | 0.1214
Epoch 176/300, seasonal_2 Loss: 0.0887 | 0.1215
Epoch 177/300, seasonal_2 Loss: 0.0892 | 0.1215
Epoch 178/300, seasonal_2 Loss: 0.0889 | 0.1214
Epoch 179/300, seasonal_2 Loss: 0.0893 | 0.1213
Epoch 180/300, seasonal_2 Loss: 0.0888 | 0.1213
Epoch 181/300, seasonal_2 Loss: 0.0889 | 0.1213
Epoch 182/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 183/300, seasonal_2 Loss: 0.0883 | 0.1213
Epoch 184/300, seasonal_2 Loss: 0.0887 | 0.1213
Epoch 185/300, seasonal_2 Loss: 0.0889 | 0.1213
Epoch 186/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 187/300, seasonal_2 Loss: 0.0892 | 0.1212
Epoch 188/300, seasonal_2 Loss: 0.0881 | 0.1212
Epoch 189/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 190/300, seasonal_2 Loss: 0.0891 | 0.1213
Epoch 191/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 192/300, seasonal_2 Loss: 0.0893 | 0.1213
Epoch 193/300, seasonal_2 Loss: 0.0886 | 0.1213
Epoch 194/300, seasonal_2 Loss: 0.0885 | 0.1213
Epoch 195/300, seasonal_2 Loss: 0.0891 | 0.1213
Epoch 196/300, seasonal_2 Loss: 0.0900 | 0.1213
Epoch 197/300, seasonal_2 Loss: 0.0881 | 0.1213
Epoch 198/300, seasonal_2 Loss: 0.0885 | 0.1213
Epoch 199/300, seasonal_2 Loss: 0.0888 | 0.1212
Epoch 200/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 201/300, seasonal_2 Loss: 0.0884 | 0.1212
Epoch 202/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 203/300, seasonal_2 Loss: 0.0900 | 0.1212
Epoch 204/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 205/300, seasonal_2 Loss: 0.0895 | 0.1212
Epoch 206/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 207/300, seasonal_2 Loss: 0.0886 | 0.1212
Epoch 208/300, seasonal_2 Loss: 0.0879 | 0.1212
Epoch 209/300, seasonal_2 Loss: 0.0899 | 0.1212
Epoch 210/300, seasonal_2 Loss: 0.0887 | 0.1212
Epoch 211/300, seasonal_2 Loss: 0.0892 | 0.1213
Epoch 212/300, seasonal_2 Loss: 0.0889 | 0.1212
Epoch 213/300, seasonal_2 Loss: 0.0891 | 0.1212
Epoch 214/300, seasonal_2 Loss: 0.0886 | 0.1212
Epoch 215/300, seasonal_2 Loss: 0.0885 | 0.1212
Epoch 216/300, seasonal_2 Loss: 0.0887 | 0.1212
Epoch 217/300, seasonal_2 Loss: 0.0885 | 0.1212
Epoch 218/300, seasonal_2 Loss: 0.0900 | 0.1212
Epoch 219/300, seasonal_2 Loss: 0.0894 | 0.1212
Epoch 220/300, seasonal_2 Loss: 0.0891 | 0.1212
Epoch 221/300, seasonal_2 Loss: 0.0885 | 0.1212
Epoch 222/300, seasonal_2 Loss: 0.0887 | 0.1212
Epoch 223/300, seasonal_2 Loss: 0.0882 | 0.1212
Epoch 224/300, seasonal_2 Loss: 0.0886 | 0.1213
Epoch 225/300, seasonal_2 Loss: 0.0887 | 0.1213
Epoch 226/300, seasonal_2 Loss: 0.0895 | 0.1213
Epoch 227/300, seasonal_2 Loss: 0.0893 | 0.1213
Epoch 228/300, seasonal_2 Loss: 0.0892 | 0.1213
Epoch 229/300, seasonal_2 Loss: 0.0882 | 0.1213
Epoch 230/300, seasonal_2 Loss: 0.0891 | 0.1213
Epoch 231/300, seasonal_2 Loss: 0.0889 | 0.1213
Epoch 232/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 233/300, seasonal_2 Loss: 0.0888 | 0.1213
Epoch 234/300, seasonal_2 Loss: 0.0878 | 0.1213
Epoch 235/300, seasonal_2 Loss: 0.0880 | 0.1213
Epoch 236/300, seasonal_2 Loss: 0.0883 | 0.1213
Epoch 237/300, seasonal_2 Loss: 0.0895 | 0.1213
Epoch 238/300, seasonal_2 Loss: 0.0892 | 0.1213
Epoch 239/300, seasonal_2 Loss: 0.0886 | 0.1213
Epoch 240/300, seasonal_2 Loss: 0.0895 | 0.1213
Epoch 241/300, seasonal_2 Loss: 0.0892 | 0.1213
Epoch 242/300, seasonal_2 Loss: 0.0894 | 0.1213
Epoch 243/300, seasonal_2 Loss: 0.0888 | 0.1213
Epoch 244/300, seasonal_2 Loss: 0.0883 | 0.1213
Epoch 245/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 246/300, seasonal_2 Loss: 0.0881 | 0.1213
Epoch 247/300, seasonal_2 Loss: 0.0890 | 0.1213
Epoch 248/300, seasonal_2 Loss: 0.0892 | 0.1213
Epoch 249/300, seasonal_2 Loss: 0.0882 | 0.1212
Epoch 250/300, seasonal_2 Loss: 0.0892 | 0.1212
Epoch 251/300, seasonal_2 Loss: 0.0887 | 0.1212
Epoch 252/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 253/300, seasonal_2 Loss: 0.0888 | 0.1212
Epoch 254/300, seasonal_2 Loss: 0.0883 | 0.1212
Epoch 255/300, seasonal_2 Loss: 0.0891 | 0.1212
Epoch 256/300, seasonal_2 Loss: 0.0895 | 0.1212
Epoch 257/300, seasonal_2 Loss: 0.0896 | 0.1212
Epoch 258/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 259/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 260/300, seasonal_2 Loss: 0.0889 | 0.1212
Epoch 261/300, seasonal_2 Loss: 0.0886 | 0.1212
Epoch 262/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 263/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 264/300, seasonal_2 Loss: 0.0891 | 0.1212
Epoch 265/300, seasonal_2 Loss: 0.0886 | 0.1212
Epoch 266/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 267/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 268/300, seasonal_2 Loss: 0.0894 | 0.1212
Epoch 269/300, seasonal_2 Loss: 0.0892 | 0.1212
Epoch 270/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 271/300, seasonal_2 Loss: 0.0884 | 0.1212
Epoch 272/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 273/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 274/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 275/300, seasonal_2 Loss: 0.0884 | 0.1212
Epoch 276/300, seasonal_2 Loss: 0.0886 | 0.1212
Epoch 277/300, seasonal_2 Loss: 0.0888 | 0.1212
Epoch 278/300, seasonal_2 Loss: 0.0894 | 0.1212
Epoch 279/300, seasonal_2 Loss: 0.0881 | 0.1212
Epoch 280/300, seasonal_2 Loss: 0.0887 | 0.1212
Epoch 281/300, seasonal_2 Loss: 0.0881 | 0.1212
Epoch 282/300, seasonal_2 Loss: 0.0878 | 0.1212
Epoch 283/300, seasonal_2 Loss: 0.0897 | 0.1212
Epoch 284/300, seasonal_2 Loss: 0.0892 | 0.1212
Epoch 285/300, seasonal_2 Loss: 0.0883 | 0.1212
Epoch 286/300, seasonal_2 Loss: 0.0894 | 0.1212
Epoch 287/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 288/300, seasonal_2 Loss: 0.0895 | 0.1212
Epoch 289/300, seasonal_2 Loss: 0.0891 | 0.1212
Epoch 290/300, seasonal_2 Loss: 0.0887 | 0.1212
Epoch 291/300, seasonal_2 Loss: 0.0889 | 0.1212
Epoch 292/300, seasonal_2 Loss: 0.0890 | 0.1212
Epoch 293/300, seasonal_2 Loss: 0.0892 | 0.1212
Epoch 294/300, seasonal_2 Loss: 0.0894 | 0.1212
Epoch 295/300, seasonal_2 Loss: 0.0886 | 0.1212
Epoch 296/300, seasonal_2 Loss: 0.0891 | 0.1212
Epoch 297/300, seasonal_2 Loss: 0.0888 | 0.1212
Epoch 298/300, seasonal_2 Loss: 0.0893 | 0.1212
Epoch 299/300, seasonal_2 Loss: 0.0888 | 0.1212
Epoch 300/300, seasonal_2 Loss: 0.0890 | 0.1212
Training seasonal_3 component with params: {'observation_period_num': 14, 'train_rates': 0.9778110643740422, 'learning_rate': 0.0004605197739807708, 'batch_size': 241, 'step_size': 12, 'gamma': 0.8448936491492371}
Epoch 1/300, seasonal_3 Loss: 1.1955 | 2.3833
Epoch 2/300, seasonal_3 Loss: 0.7882 | 1.2453
Epoch 3/300, seasonal_3 Loss: 0.6342 | 0.9360
Epoch 4/300, seasonal_3 Loss: 0.6480 | 0.9507
Epoch 5/300, seasonal_3 Loss: 0.7340 | 0.8158
Epoch 6/300, seasonal_3 Loss: 0.5778 | 0.7319
Epoch 7/300, seasonal_3 Loss: 0.5236 | 0.7735
Epoch 8/300, seasonal_3 Loss: 0.4830 | 0.6861
Epoch 9/300, seasonal_3 Loss: 0.4499 | 0.6315
Epoch 10/300, seasonal_3 Loss: 0.4379 | 0.5797
Epoch 11/300, seasonal_3 Loss: 0.4084 | 0.5545
Epoch 12/300, seasonal_3 Loss: 0.4134 | 0.4960
Epoch 13/300, seasonal_3 Loss: 0.3773 | 0.4851
Epoch 14/300, seasonal_3 Loss: 0.3996 | 0.5709
Epoch 15/300, seasonal_3 Loss: 0.3841 | 0.4716
Epoch 16/300, seasonal_3 Loss: 0.4242 | 0.4310
Epoch 17/300, seasonal_3 Loss: 0.3321 | 0.4162
Epoch 18/300, seasonal_3 Loss: 0.3043 | 0.3864
Epoch 19/300, seasonal_3 Loss: 0.2725 | 0.3819
Epoch 20/300, seasonal_3 Loss: 0.2567 | 0.3539
Epoch 21/300, seasonal_3 Loss: 0.2432 | 0.3452
Epoch 22/300, seasonal_3 Loss: 0.2357 | 0.3320
Epoch 23/300, seasonal_3 Loss: 0.2266 | 0.3212
Epoch 24/300, seasonal_3 Loss: 0.2225 | 0.3075
Epoch 25/300, seasonal_3 Loss: 0.2192 | 0.2987
Epoch 26/300, seasonal_3 Loss: 0.2107 | 0.2853
Epoch 27/300, seasonal_3 Loss: 0.2037 | 0.2772
Epoch 28/300, seasonal_3 Loss: 0.1977 | 0.2727
Epoch 29/300, seasonal_3 Loss: 0.2002 | 0.2580
Epoch 30/300, seasonal_3 Loss: 0.2031 | 0.2686
Epoch 31/300, seasonal_3 Loss: 0.2031 | 0.2458
Epoch 32/300, seasonal_3 Loss: 0.1911 | 0.2519
Epoch 33/300, seasonal_3 Loss: 0.1841 | 0.2400
Epoch 34/300, seasonal_3 Loss: 0.1788 | 0.2289
Epoch 35/300, seasonal_3 Loss: 0.1751 | 0.2350
Epoch 36/300, seasonal_3 Loss: 0.1730 | 0.2163
Epoch 37/300, seasonal_3 Loss: 0.1790 | 0.2303
Epoch 38/300, seasonal_3 Loss: 0.1841 | 0.2154
Epoch 39/300, seasonal_3 Loss: 0.1843 | 0.2131
Epoch 40/300, seasonal_3 Loss: 0.1716 | 0.2150
Epoch 41/300, seasonal_3 Loss: 0.1708 | 0.2054
Epoch 42/300, seasonal_3 Loss: 0.1673 | 0.2110
Epoch 43/300, seasonal_3 Loss: 0.1662 | 0.2079
Epoch 44/300, seasonal_3 Loss: 0.1624 | 0.2051
Epoch 45/300, seasonal_3 Loss: 0.1551 | 0.1998
Epoch 46/300, seasonal_3 Loss: 0.1521 | 0.1935
Epoch 47/300, seasonal_3 Loss: 0.1485 | 0.1947
Epoch 48/300, seasonal_3 Loss: 0.1469 | 0.1866
Epoch 49/300, seasonal_3 Loss: 0.1438 | 0.1876
Epoch 50/300, seasonal_3 Loss: 0.1413 | 0.1837
Epoch 51/300, seasonal_3 Loss: 0.1393 | 0.1804
Epoch 52/300, seasonal_3 Loss: 0.1378 | 0.1808
Epoch 53/300, seasonal_3 Loss: 0.1368 | 0.1770
Epoch 54/300, seasonal_3 Loss: 0.1359 | 0.1759
Epoch 55/300, seasonal_3 Loss: 0.1347 | 0.1730
Epoch 56/300, seasonal_3 Loss: 0.1329 | 0.1724
Epoch 57/300, seasonal_3 Loss: 0.1323 | 0.1703
Epoch 58/300, seasonal_3 Loss: 0.1319 | 0.1694
Epoch 59/300, seasonal_3 Loss: 0.1308 | 0.1676
Epoch 60/300, seasonal_3 Loss: 0.1287 | 0.1670
Epoch 61/300, seasonal_3 Loss: 0.1284 | 0.1639
Epoch 62/300, seasonal_3 Loss: 0.1273 | 0.1634
Epoch 63/300, seasonal_3 Loss: 0.1265 | 0.1611
Epoch 64/300, seasonal_3 Loss: 0.1252 | 0.1609
Epoch 65/300, seasonal_3 Loss: 0.1250 | 0.1584
Epoch 66/300, seasonal_3 Loss: 0.1246 | 0.1581
Epoch 67/300, seasonal_3 Loss: 0.1234 | 0.1572
Epoch 68/300, seasonal_3 Loss: 0.1230 | 0.1563
Epoch 69/300, seasonal_3 Loss: 0.1218 | 0.1553
Epoch 70/300, seasonal_3 Loss: 0.1214 | 0.1548
Epoch 71/300, seasonal_3 Loss: 0.1215 | 0.1538
Epoch 72/300, seasonal_3 Loss: 0.1199 | 0.1526
Epoch 73/300, seasonal_3 Loss: 0.1196 | 0.1516
Epoch 74/300, seasonal_3 Loss: 0.1194 | 0.1520
Epoch 75/300, seasonal_3 Loss: 0.1181 | 0.1501
Epoch 76/300, seasonal_3 Loss: 0.1170 | 0.1490
Epoch 77/300, seasonal_3 Loss: 0.1168 | 0.1478
Epoch 78/300, seasonal_3 Loss: 0.1168 | 0.1480
Epoch 79/300, seasonal_3 Loss: 0.1155 | 0.1478
Epoch 80/300, seasonal_3 Loss: 0.1151 | 0.1463
Epoch 81/300, seasonal_3 Loss: 0.1145 | 0.1456
Epoch 82/300, seasonal_3 Loss: 0.1143 | 0.1454
Epoch 83/300, seasonal_3 Loss: 0.1146 | 0.1448
Epoch 84/300, seasonal_3 Loss: 0.1137 | 0.1435
Epoch 85/300, seasonal_3 Loss: 0.1142 | 0.1430
Epoch 86/300, seasonal_3 Loss: 0.1130 | 0.1433
Epoch 87/300, seasonal_3 Loss: 0.1128 | 0.1424
Epoch 88/300, seasonal_3 Loss: 0.1130 | 0.1421
Epoch 89/300, seasonal_3 Loss: 0.1117 | 0.1414
Epoch 90/300, seasonal_3 Loss: 0.1114 | 0.1412
Epoch 91/300, seasonal_3 Loss: 0.1109 | 0.1408
Epoch 92/300, seasonal_3 Loss: 0.1108 | 0.1403
Epoch 93/300, seasonal_3 Loss: 0.1104 | 0.1392
Epoch 94/300, seasonal_3 Loss: 0.1099 | 0.1389
Epoch 95/300, seasonal_3 Loss: 0.1097 | 0.1388
Epoch 96/300, seasonal_3 Loss: 0.1091 | 0.1382
Epoch 97/300, seasonal_3 Loss: 0.1093 | 0.1375
Epoch 98/300, seasonal_3 Loss: 0.1099 | 0.1374
Epoch 99/300, seasonal_3 Loss: 0.1085 | 0.1366
Epoch 100/300, seasonal_3 Loss: 0.1079 | 0.1367
Epoch 101/300, seasonal_3 Loss: 0.1081 | 0.1361
Epoch 102/300, seasonal_3 Loss: 0.1081 | 0.1360
Epoch 103/300, seasonal_3 Loss: 0.1072 | 0.1354
Epoch 104/300, seasonal_3 Loss: 0.1076 | 0.1353
Epoch 105/300, seasonal_3 Loss: 0.1064 | 0.1350
Epoch 106/300, seasonal_3 Loss: 0.1073 | 0.1350
Epoch 107/300, seasonal_3 Loss: 0.1064 | 0.1344
Epoch 108/300, seasonal_3 Loss: 0.1059 | 0.1334
Epoch 109/300, seasonal_3 Loss: 0.1064 | 0.1336
Epoch 110/300, seasonal_3 Loss: 0.1058 | 0.1337
Epoch 111/300, seasonal_3 Loss: 0.1054 | 0.1336
Epoch 112/300, seasonal_3 Loss: 0.1054 | 0.1332
Epoch 113/300, seasonal_3 Loss: 0.1051 | 0.1327
Epoch 114/300, seasonal_3 Loss: 0.1046 | 0.1326
Epoch 115/300, seasonal_3 Loss: 0.1046 | 0.1323
Epoch 116/300, seasonal_3 Loss: 0.1045 | 0.1318
Epoch 117/300, seasonal_3 Loss: 0.1043 | 0.1314
Epoch 118/300, seasonal_3 Loss: 0.1041 | 0.1312
Epoch 119/300, seasonal_3 Loss: 0.1038 | 0.1313
Epoch 120/300, seasonal_3 Loss: 0.1050 | 0.1312
Epoch 121/300, seasonal_3 Loss: 0.1041 | 0.1312
Epoch 122/300, seasonal_3 Loss: 0.1037 | 0.1307
Epoch 123/300, seasonal_3 Loss: 0.1034 | 0.1307
Epoch 124/300, seasonal_3 Loss: 0.1038 | 0.1305
Epoch 125/300, seasonal_3 Loss: 0.1028 | 0.1300
Epoch 126/300, seasonal_3 Loss: 0.1033 | 0.1297
Epoch 127/300, seasonal_3 Loss: 0.1033 | 0.1295
Epoch 128/300, seasonal_3 Loss: 0.1027 | 0.1295
Epoch 129/300, seasonal_3 Loss: 0.1030 | 0.1293
Epoch 130/300, seasonal_3 Loss: 0.1018 | 0.1293
Epoch 131/300, seasonal_3 Loss: 0.1025 | 0.1290
Epoch 132/300, seasonal_3 Loss: 0.1028 | 0.1287
Epoch 133/300, seasonal_3 Loss: 0.1025 | 0.1285
Epoch 134/300, seasonal_3 Loss: 0.1018 | 0.1284
Epoch 135/300, seasonal_3 Loss: 0.1023 | 0.1284
Epoch 136/300, seasonal_3 Loss: 0.1016 | 0.1283
Epoch 137/300, seasonal_3 Loss: 0.1013 | 0.1278
Epoch 138/300, seasonal_3 Loss: 0.1013 | 0.1279
Epoch 139/300, seasonal_3 Loss: 0.1010 | 0.1278
Epoch 140/300, seasonal_3 Loss: 0.1013 | 0.1275
Epoch 141/300, seasonal_3 Loss: 0.1011 | 0.1276
Epoch 142/300, seasonal_3 Loss: 0.1004 | 0.1275
Epoch 143/300, seasonal_3 Loss: 0.1016 | 0.1273
Epoch 144/300, seasonal_3 Loss: 0.1010 | 0.1275
Epoch 145/300, seasonal_3 Loss: 0.1009 | 0.1274
Epoch 146/300, seasonal_3 Loss: 0.1008 | 0.1271
Epoch 147/300, seasonal_3 Loss: 0.1002 | 0.1269
Epoch 148/300, seasonal_3 Loss: 0.1004 | 0.1270
Epoch 149/300, seasonal_3 Loss: 0.1003 | 0.1269
Epoch 150/300, seasonal_3 Loss: 0.1003 | 0.1266
Epoch 151/300, seasonal_3 Loss: 0.1007 | 0.1265
Epoch 152/300, seasonal_3 Loss: 0.1002 | 0.1266
Epoch 153/300, seasonal_3 Loss: 0.1001 | 0.1266
Epoch 154/300, seasonal_3 Loss: 0.1004 | 0.1265
Epoch 155/300, seasonal_3 Loss: 0.1000 | 0.1264
Epoch 156/300, seasonal_3 Loss: 0.0997 | 0.1263
Epoch 157/300, seasonal_3 Loss: 0.0997 | 0.1263
Epoch 158/300, seasonal_3 Loss: 0.1000 | 0.1264
Epoch 159/300, seasonal_3 Loss: 0.1001 | 0.1262
Epoch 160/300, seasonal_3 Loss: 0.0996 | 0.1260
Epoch 161/300, seasonal_3 Loss: 0.0997 | 0.1260
Epoch 162/300, seasonal_3 Loss: 0.0997 | 0.1259
Epoch 163/300, seasonal_3 Loss: 0.1000 | 0.1258
Epoch 164/300, seasonal_3 Loss: 0.0991 | 0.1257
Epoch 165/300, seasonal_3 Loss: 0.0994 | 0.1256
Epoch 166/300, seasonal_3 Loss: 0.0996 | 0.1255
Epoch 167/300, seasonal_3 Loss: 0.0991 | 0.1255
Epoch 168/300, seasonal_3 Loss: 0.0993 | 0.1255
Epoch 169/300, seasonal_3 Loss: 0.0994 | 0.1253
Epoch 170/300, seasonal_3 Loss: 0.0991 | 0.1252
Epoch 171/300, seasonal_3 Loss: 0.0992 | 0.1251
Epoch 172/300, seasonal_3 Loss: 0.0991 | 0.1251
Epoch 173/300, seasonal_3 Loss: 0.0989 | 0.1251
Epoch 174/300, seasonal_3 Loss: 0.0988 | 0.1249
Epoch 175/300, seasonal_3 Loss: 0.0990 | 0.1249
Epoch 176/300, seasonal_3 Loss: 0.0995 | 0.1248
Epoch 177/300, seasonal_3 Loss: 0.0990 | 0.1248
Epoch 178/300, seasonal_3 Loss: 0.0992 | 0.1247
Epoch 179/300, seasonal_3 Loss: 0.0992 | 0.1247
Epoch 180/300, seasonal_3 Loss: 0.0996 | 0.1246
Epoch 181/300, seasonal_3 Loss: 0.0983 | 0.1246
Epoch 182/300, seasonal_3 Loss: 0.0992 | 0.1245
Epoch 183/300, seasonal_3 Loss: 0.0986 | 0.1244
Epoch 184/300, seasonal_3 Loss: 0.0980 | 0.1244
Epoch 185/300, seasonal_3 Loss: 0.0990 | 0.1244
Epoch 186/300, seasonal_3 Loss: 0.0985 | 0.1244
Epoch 187/300, seasonal_3 Loss: 0.0983 | 0.1244
Epoch 188/300, seasonal_3 Loss: 0.0989 | 0.1243
Epoch 189/300, seasonal_3 Loss: 0.0987 | 0.1242
Epoch 190/300, seasonal_3 Loss: 0.0983 | 0.1241
Epoch 191/300, seasonal_3 Loss: 0.0984 | 0.1241
Epoch 192/300, seasonal_3 Loss: 0.0984 | 0.1241
Epoch 193/300, seasonal_3 Loss: 0.0988 | 0.1240
Epoch 194/300, seasonal_3 Loss: 0.0984 | 0.1240
Epoch 195/300, seasonal_3 Loss: 0.0981 | 0.1240
Epoch 196/300, seasonal_3 Loss: 0.0985 | 0.1239
Epoch 197/300, seasonal_3 Loss: 0.0989 | 0.1240
Epoch 198/300, seasonal_3 Loss: 0.0986 | 0.1240
Epoch 199/300, seasonal_3 Loss: 0.0983 | 0.1240
Epoch 200/300, seasonal_3 Loss: 0.0981 | 0.1240
Epoch 201/300, seasonal_3 Loss: 0.0980 | 0.1240
Epoch 202/300, seasonal_3 Loss: 0.0980 | 0.1239
Epoch 203/300, seasonal_3 Loss: 0.0980 | 0.1239
Epoch 204/300, seasonal_3 Loss: 0.0977 | 0.1239
Epoch 205/300, seasonal_3 Loss: 0.0985 | 0.1239
Epoch 206/300, seasonal_3 Loss: 0.0980 | 0.1238
Epoch 207/300, seasonal_3 Loss: 0.0976 | 0.1238
Epoch 208/300, seasonal_3 Loss: 0.0979 | 0.1238
Epoch 209/300, seasonal_3 Loss: 0.0986 | 0.1237
Epoch 210/300, seasonal_3 Loss: 0.0984 | 0.1237
Epoch 211/300, seasonal_3 Loss: 0.0983 | 0.1237
Epoch 212/300, seasonal_3 Loss: 0.0978 | 0.1237
Epoch 213/300, seasonal_3 Loss: 0.0990 | 0.1236
Epoch 214/300, seasonal_3 Loss: 0.0978 | 0.1236
Epoch 215/300, seasonal_3 Loss: 0.0975 | 0.1235
Epoch 216/300, seasonal_3 Loss: 0.0979 | 0.1235
Epoch 217/300, seasonal_3 Loss: 0.0981 | 0.1235
Epoch 218/300, seasonal_3 Loss: 0.0984 | 0.1235
Epoch 219/300, seasonal_3 Loss: 0.0974 | 0.1235
Epoch 220/300, seasonal_3 Loss: 0.0983 | 0.1235
Epoch 221/300, seasonal_3 Loss: 0.0981 | 0.1235
Epoch 222/300, seasonal_3 Loss: 0.0986 | 0.1235
Epoch 223/300, seasonal_3 Loss: 0.0978 | 0.1235
Epoch 224/300, seasonal_3 Loss: 0.0975 | 0.1235
Epoch 225/300, seasonal_3 Loss: 0.0976 | 0.1235
Epoch 226/300, seasonal_3 Loss: 0.0979 | 0.1234
Epoch 227/300, seasonal_3 Loss: 0.0978 | 0.1234
Epoch 228/300, seasonal_3 Loss: 0.0977 | 0.1234
Epoch 229/300, seasonal_3 Loss: 0.0980 | 0.1234
Epoch 230/300, seasonal_3 Loss: 0.0980 | 0.1234
Epoch 231/300, seasonal_3 Loss: 0.0978 | 0.1234
Epoch 232/300, seasonal_3 Loss: 0.0976 | 0.1234
Epoch 233/300, seasonal_3 Loss: 0.0976 | 0.1234
Epoch 234/300, seasonal_3 Loss: 0.0981 | 0.1233
Epoch 235/300, seasonal_3 Loss: 0.0975 | 0.1233
Epoch 236/300, seasonal_3 Loss: 0.0981 | 0.1233
Epoch 237/300, seasonal_3 Loss: 0.0973 | 0.1233
Epoch 238/300, seasonal_3 Loss: 0.0977 | 0.1233
Epoch 239/300, seasonal_3 Loss: 0.0971 | 0.1232
Epoch 240/300, seasonal_3 Loss: 0.0979 | 0.1232
Epoch 241/300, seasonal_3 Loss: 0.0974 | 0.1232
Epoch 242/300, seasonal_3 Loss: 0.0976 | 0.1231
Epoch 243/300, seasonal_3 Loss: 0.0975 | 0.1231
Epoch 244/300, seasonal_3 Loss: 0.0971 | 0.1231
Epoch 245/300, seasonal_3 Loss: 0.0973 | 0.1231
Epoch 246/300, seasonal_3 Loss: 0.0986 | 0.1231
Epoch 247/300, seasonal_3 Loss: 0.0975 | 0.1231
Epoch 248/300, seasonal_3 Loss: 0.0976 | 0.1231
Epoch 249/300, seasonal_3 Loss: 0.0978 | 0.1231
Epoch 250/300, seasonal_3 Loss: 0.0975 | 0.1231
Epoch 251/300, seasonal_3 Loss: 0.0972 | 0.1231
Epoch 252/300, seasonal_3 Loss: 0.0985 | 0.1231
Epoch 253/300, seasonal_3 Loss: 0.0969 | 0.1231
Epoch 254/300, seasonal_3 Loss: 0.0971 | 0.1231
Epoch 255/300, seasonal_3 Loss: 0.0972 | 0.1231
Epoch 256/300, seasonal_3 Loss: 0.0978 | 0.1230
Epoch 257/300, seasonal_3 Loss: 0.0972 | 0.1230
Epoch 258/300, seasonal_3 Loss: 0.0983 | 0.1230
Epoch 259/300, seasonal_3 Loss: 0.0979 | 0.1230
Epoch 260/300, seasonal_3 Loss: 0.0970 | 0.1230
Epoch 261/300, seasonal_3 Loss: 0.0973 | 0.1230
Epoch 262/300, seasonal_3 Loss: 0.0978 | 0.1230
Epoch 263/300, seasonal_3 Loss: 0.0975 | 0.1230
Epoch 264/300, seasonal_3 Loss: 0.0975 | 0.1230
Epoch 265/300, seasonal_3 Loss: 0.0974 | 0.1230
Epoch 266/300, seasonal_3 Loss: 0.0970 | 0.1230
Epoch 267/300, seasonal_3 Loss: 0.0983 | 0.1229
Epoch 268/300, seasonal_3 Loss: 0.0974 | 0.1229
Epoch 269/300, seasonal_3 Loss: 0.0977 | 0.1229
Epoch 270/300, seasonal_3 Loss: 0.0977 | 0.1229
Epoch 271/300, seasonal_3 Loss: 0.0971 | 0.1229
Epoch 272/300, seasonal_3 Loss: 0.0971 | 0.1229
Epoch 273/300, seasonal_3 Loss: 0.0977 | 0.1229
Epoch 274/300, seasonal_3 Loss: 0.0977 | 0.1229
Epoch 275/300, seasonal_3 Loss: 0.0971 | 0.1229
Epoch 276/300, seasonal_3 Loss: 0.0968 | 0.1229
Epoch 277/300, seasonal_3 Loss: 0.0979 | 0.1229
Epoch 278/300, seasonal_3 Loss: 0.0975 | 0.1229
Epoch 279/300, seasonal_3 Loss: 0.0979 | 0.1229
Epoch 280/300, seasonal_3 Loss: 0.0976 | 0.1229
Epoch 281/300, seasonal_3 Loss: 0.0973 | 0.1229
Epoch 282/300, seasonal_3 Loss: 0.0978 | 0.1229
Epoch 283/300, seasonal_3 Loss: 0.0978 | 0.1229
Epoch 284/300, seasonal_3 Loss: 0.0971 | 0.1229
Epoch 285/300, seasonal_3 Loss: 0.0980 | 0.1229
Epoch 286/300, seasonal_3 Loss: 0.0970 | 0.1229
Epoch 287/300, seasonal_3 Loss: 0.0970 | 0.1229
Epoch 288/300, seasonal_3 Loss: 0.0975 | 0.1229
Epoch 289/300, seasonal_3 Loss: 0.0974 | 0.1229
Epoch 290/300, seasonal_3 Loss: 0.0972 | 0.1229
Epoch 291/300, seasonal_3 Loss: 0.0977 | 0.1229
Epoch 292/300, seasonal_3 Loss: 0.0980 | 0.1229
Epoch 293/300, seasonal_3 Loss: 0.0978 | 0.1229
Epoch 294/300, seasonal_3 Loss: 0.0972 | 0.1229
Epoch 295/300, seasonal_3 Loss: 0.0974 | 0.1229
Epoch 296/300, seasonal_3 Loss: 0.0979 | 0.1229
Epoch 297/300, seasonal_3 Loss: 0.0971 | 0.1229
Epoch 298/300, seasonal_3 Loss: 0.0975 | 0.1229
Epoch 299/300, seasonal_3 Loss: 0.0967 | 0.1229
Epoch 300/300, seasonal_3 Loss: 0.0976 | 0.1229
Training resid component with params: {'observation_period_num': 189, 'train_rates': 0.9897070506458603, 'learning_rate': 0.00013529243732510096, 'batch_size': 135, 'step_size': 15, 'gamma': 0.9152230852528475}
Epoch 1/300, resid Loss: 1.0580 | 1.4952
Epoch 2/300, resid Loss: 0.7036 | 0.9282
Epoch 3/300, resid Loss: 0.5967 | 0.8280
Epoch 4/300, resid Loss: 0.5122 | 0.7695
Epoch 5/300, resid Loss: 0.5070 | 0.7008
Epoch 6/300, resid Loss: 0.4284 | 0.6565
Epoch 7/300, resid Loss: 0.4140 | 0.6030
Epoch 8/300, resid Loss: 0.4602 | 0.5875
Epoch 9/300, resid Loss: 0.4151 | 0.5495
Epoch 10/300, resid Loss: 0.3966 | 0.5286
Epoch 11/300, resid Loss: 0.3452 | 0.5104
Epoch 12/300, resid Loss: 0.3319 | 0.4640
Epoch 13/300, resid Loss: 0.3434 | 0.4625
Epoch 14/300, resid Loss: 0.3463 | 0.4554
Epoch 15/300, resid Loss: 0.4141 | 0.5096
Epoch 16/300, resid Loss: 0.3493 | 0.4243
Epoch 17/300, resid Loss: 0.4233 | 0.4319
Epoch 18/300, resid Loss: 0.3127 | 0.3990
Epoch 19/300, resid Loss: 0.2632 | 0.3941
Epoch 20/300, resid Loss: 0.2519 | 0.3634
Epoch 21/300, resid Loss: 0.2458 | 0.3646
Epoch 22/300, resid Loss: 0.2382 | 0.3333
Epoch 23/300, resid Loss: 0.2292 | 0.3268
Epoch 24/300, resid Loss: 0.2237 | 0.3146
Epoch 25/300, resid Loss: 0.2163 | 0.3009
Epoch 26/300, resid Loss: 0.2107 | 0.2910
Epoch 27/300, resid Loss: 0.2058 | 0.2859
Epoch 28/300, resid Loss: 0.2027 | 0.2743
Epoch 29/300, resid Loss: 0.2036 | 0.2704
Epoch 30/300, resid Loss: 0.2104 | 0.2711
Epoch 31/300, resid Loss: 0.2148 | 0.2581
Epoch 32/300, resid Loss: 0.2155 | 0.2761
Epoch 33/300, resid Loss: 0.1999 | 0.2462
Epoch 34/300, resid Loss: 0.1962 | 0.2426
Epoch 35/300, resid Loss: 0.1951 | 0.2401
Epoch 36/300, resid Loss: 0.1947 | 0.2301
Epoch 37/300, resid Loss: 0.2026 | 0.2349
Epoch 38/300, resid Loss: 0.2123 | 0.2265
Epoch 39/300, resid Loss: 0.2161 | 0.2271
Epoch 40/300, resid Loss: 0.2229 | 0.2385
Epoch 41/300, resid Loss: 0.2003 | 0.2282
Epoch 42/300, resid Loss: 0.2000 | 0.2139
Epoch 43/300, resid Loss: 0.1779 | 0.2189
Epoch 44/300, resid Loss: 0.1735 | 0.2051
Epoch 45/300, resid Loss: 0.1724 | 0.2046
Epoch 46/300, resid Loss: 0.1698 | 0.1994
Epoch 47/300, resid Loss: 0.1674 | 0.1972
Epoch 48/300, resid Loss: 0.1647 | 0.1941
Epoch 49/300, resid Loss: 0.1632 | 0.1919
Epoch 50/300, resid Loss: 0.1629 | 0.1871
Epoch 51/300, resid Loss: 0.1629 | 0.1867
Epoch 52/300, resid Loss: 0.1630 | 0.1829
Epoch 53/300, resid Loss: 0.1629 | 0.1832
Epoch 54/300, resid Loss: 0.1603 | 0.1796
Epoch 55/300, resid Loss: 0.1598 | 0.1782
Epoch 56/300, resid Loss: 0.1579 | 0.1778
Epoch 57/300, resid Loss: 0.1599 | 0.1706
Epoch 58/300, resid Loss: 0.1650 | 0.1741
Epoch 59/300, resid Loss: 0.1649 | 0.1687
Epoch 60/300, resid Loss: 0.1725 | 0.1748
Epoch 61/300, resid Loss: 0.1644 | 0.1697
Epoch 62/300, resid Loss: 0.1619 | 0.1689
Epoch 63/300, resid Loss: 0.1592 | 0.1683
Epoch 64/300, resid Loss: 0.1563 | 0.1617
Epoch 65/300, resid Loss: 0.1573 | 0.1652
Epoch 66/300, resid Loss: 0.1522 | 0.1605
Epoch 67/300, resid Loss: 0.1520 | 0.1608
Epoch 68/300, resid Loss: 0.1514 | 0.1582
Epoch 69/300, resid Loss: 0.1503 | 0.1556
Epoch 70/300, resid Loss: 0.1496 | 0.1578
Epoch 71/300, resid Loss: 0.1535 | 0.1545
Epoch 72/300, resid Loss: 0.1604 | 0.1616
Epoch 73/300, resid Loss: 0.1631 | 0.1534
Epoch 74/300, resid Loss: 0.1585 | 0.1705
Epoch 75/300, resid Loss: 0.1455 | 0.1505
Epoch 76/300, resid Loss: 0.1413 | 0.1518
Epoch 77/300, resid Loss: 0.1373 | 0.1492
Epoch 78/300, resid Loss: 0.1373 | 0.1451
Epoch 79/300, resid Loss: 0.1365 | 0.1484
Epoch 80/300, resid Loss: 0.1361 | 0.1437
Epoch 81/300, resid Loss: 0.1359 | 0.1461
Epoch 82/300, resid Loss: 0.1356 | 0.1422
Epoch 83/300, resid Loss: 0.1367 | 0.1440
Epoch 84/300, resid Loss: 0.1388 | 0.1442
Epoch 85/300, resid Loss: 0.1353 | 0.1399
Epoch 86/300, resid Loss: 0.1337 | 0.1432
Epoch 87/300, resid Loss: 0.1325 | 0.1379
Epoch 88/300, resid Loss: 0.1326 | 0.1399
Epoch 89/300, resid Loss: 0.1340 | 0.1404
Epoch 90/300, resid Loss: 0.1376 | 0.1370
Epoch 91/300, resid Loss: 0.1421 | 0.1440
Epoch 92/300, resid Loss: 0.1382 | 0.1362
Epoch 93/300, resid Loss: 0.1357 | 0.1404
Epoch 94/300, resid Loss: 0.1325 | 0.1362
Epoch 95/300, resid Loss: 0.1289 | 0.1363
Epoch 96/300, resid Loss: 0.1264 | 0.1349
Epoch 97/300, resid Loss: 0.1253 | 0.1334
Epoch 98/300, resid Loss: 0.1260 | 0.1336
Epoch 99/300, resid Loss: 0.1274 | 0.1320
Epoch 100/300, resid Loss: 0.1294 | 0.1347
Epoch 101/300, resid Loss: 0.1302 | 0.1309
Epoch 102/300, resid Loss: 0.1304 | 0.1348
Epoch 103/300, resid Loss: 0.1308 | 0.1310
Epoch 104/300, resid Loss: 0.1303 | 0.1340
Epoch 105/300, resid Loss: 0.1278 | 0.1298
Epoch 106/300, resid Loss: 0.1262 | 0.1332
Epoch 107/300, resid Loss: 0.1223 | 0.1288
Epoch 108/300, resid Loss: 0.1213 | 0.1292
Epoch 109/300, resid Loss: 0.1204 | 0.1290
Epoch 110/300, resid Loss: 0.1226 | 0.1270
Epoch 111/300, resid Loss: 0.1260 | 0.1289
Epoch 112/300, resid Loss: 0.1298 | 0.1268
Epoch 113/300, resid Loss: 0.1337 | 0.1295
Epoch 114/300, resid Loss: 0.1322 | 0.1269
Epoch 115/300, resid Loss: 0.1275 | 0.1296
Epoch 116/300, resid Loss: 0.1255 | 0.1256
Epoch 117/300, resid Loss: 0.1230 | 0.1277
Epoch 118/300, resid Loss: 0.1212 | 0.1244
Epoch 119/300, resid Loss: 0.1214 | 0.1263
Epoch 120/300, resid Loss: 0.1200 | 0.1236
Epoch 121/300, resid Loss: 0.1183 | 0.1259
Epoch 122/300, resid Loss: 0.1166 | 0.1218
Epoch 123/300, resid Loss: 0.1154 | 0.1246
Epoch 124/300, resid Loss: 0.1142 | 0.1212
Epoch 125/300, resid Loss: 0.1136 | 0.1227
Epoch 126/300, resid Loss: 0.1136 | 0.1212
Epoch 127/300, resid Loss: 0.1121 | 0.1225
Epoch 128/300, resid Loss: 0.1125 | 0.1209
Epoch 129/300, resid Loss: 0.1124 | 0.1215
Epoch 130/300, resid Loss: 0.1125 | 0.1204
Epoch 131/300, resid Loss: 0.1120 | 0.1204
Epoch 132/300, resid Loss: 0.1116 | 0.1200
Epoch 133/300, resid Loss: 0.1116 | 0.1191
Epoch 134/300, resid Loss: 0.1114 | 0.1198
Epoch 135/300, resid Loss: 0.1112 | 0.1188
Epoch 136/300, resid Loss: 0.1110 | 0.1190
Epoch 137/300, resid Loss: 0.1104 | 0.1183
Epoch 138/300, resid Loss: 0.1091 | 0.1186
Epoch 139/300, resid Loss: 0.1102 | 0.1180
Epoch 140/300, resid Loss: 0.1099 | 0.1184
Epoch 141/300, resid Loss: 0.1094 | 0.1172
Epoch 142/300, resid Loss: 0.1095 | 0.1178
Epoch 143/300, resid Loss: 0.1097 | 0.1161
Epoch 144/300, resid Loss: 0.1101 | 0.1176
Epoch 145/300, resid Loss: 0.1111 | 0.1160
Epoch 146/300, resid Loss: 0.1108 | 0.1177
Epoch 147/300, resid Loss: 0.1103 | 0.1154
Epoch 148/300, resid Loss: 0.1106 | 0.1173
Epoch 149/300, resid Loss: 0.1111 | 0.1145
Epoch 150/300, resid Loss: 0.1098 | 0.1173
Epoch 151/300, resid Loss: 0.1096 | 0.1142
Epoch 152/300, resid Loss: 0.1088 | 0.1166
Epoch 153/300, resid Loss: 0.1079 | 0.1145
Epoch 154/300, resid Loss: 0.1068 | 0.1163
Epoch 155/300, resid Loss: 0.1068 | 0.1136
Epoch 156/300, resid Loss: 0.1057 | 0.1153
Epoch 157/300, resid Loss: 0.1053 | 0.1130
Epoch 158/300, resid Loss: 0.1052 | 0.1152
Epoch 159/300, resid Loss: 0.1049 | 0.1128
Epoch 160/300, resid Loss: 0.1044 | 0.1140
Epoch 161/300, resid Loss: 0.1045 | 0.1130
Epoch 162/300, resid Loss: 0.1045 | 0.1131
Epoch 163/300, resid Loss: 0.1044 | 0.1128
Epoch 164/300, resid Loss: 0.1037 | 0.1130
Epoch 165/300, resid Loss: 0.1049 | 0.1119
Epoch 166/300, resid Loss: 0.1047 | 0.1130
Epoch 167/300, resid Loss: 0.1038 | 0.1115
Epoch 168/300, resid Loss: 0.1034 | 0.1127
Epoch 169/300, resid Loss: 0.1026 | 0.1115
Epoch 170/300, resid Loss: 0.1035 | 0.1129
Epoch 171/300, resid Loss: 0.1032 | 0.1114
Epoch 172/300, resid Loss: 0.1023 | 0.1123
Epoch 173/300, resid Loss: 0.1025 | 0.1105
Epoch 174/300, resid Loss: 0.1021 | 0.1117
Epoch 175/300, resid Loss: 0.1020 | 0.1110
Epoch 176/300, resid Loss: 0.1016 | 0.1118
Epoch 177/300, resid Loss: 0.1006 | 0.1107
Epoch 178/300, resid Loss: 0.1017 | 0.1112
Epoch 179/300, resid Loss: 0.1014 | 0.1108
Epoch 180/300, resid Loss: 0.1007 | 0.1111
Epoch 181/300, resid Loss: 0.1011 | 0.1100
Epoch 182/300, resid Loss: 0.1013 | 0.1112
Epoch 183/300, resid Loss: 0.1013 | 0.1094
Epoch 184/300, resid Loss: 0.1012 | 0.1112
Epoch 185/300, resid Loss: 0.1013 | 0.1098
Epoch 186/300, resid Loss: 0.1009 | 0.1106
Epoch 187/300, resid Loss: 0.1013 | 0.1092
Epoch 188/300, resid Loss: 0.1007 | 0.1106
Epoch 189/300, resid Loss: 0.1000 | 0.1086
Epoch 190/300, resid Loss: 0.0999 | 0.1100
Epoch 191/300, resid Loss: 0.0992 | 0.1084
Epoch 192/300, resid Loss: 0.0996 | 0.1098
Epoch 193/300, resid Loss: 0.0990 | 0.1085
Epoch 194/300, resid Loss: 0.0987 | 0.1097
Epoch 195/300, resid Loss: 0.0992 | 0.1089
Epoch 196/300, resid Loss: 0.0988 | 0.1084
Epoch 197/300, resid Loss: 0.0993 | 0.1086
Epoch 198/300, resid Loss: 0.0982 | 0.1090
Epoch 199/300, resid Loss: 0.0983 | 0.1077
Epoch 200/300, resid Loss: 0.0984 | 0.1083
Epoch 201/300, resid Loss: 0.0982 | 0.1080
Epoch 202/300, resid Loss: 0.0978 | 0.1086
Epoch 203/300, resid Loss: 0.0974 | 0.1080
Epoch 204/300, resid Loss: 0.0978 | 0.1080
Epoch 205/300, resid Loss: 0.0968 | 0.1077
Epoch 206/300, resid Loss: 0.0983 | 0.1078
Epoch 207/300, resid Loss: 0.0970 | 0.1076
Epoch 208/300, resid Loss: 0.0964 | 0.1073
Epoch 209/300, resid Loss: 0.0971 | 0.1072
Epoch 210/300, resid Loss: 0.0978 | 0.1075
Epoch 211/300, resid Loss: 0.0967 | 0.1071
Epoch 212/300, resid Loss: 0.0967 | 0.1083
Epoch 213/300, resid Loss: 0.0968 | 0.1074
Epoch 214/300, resid Loss: 0.0959 | 0.1081
Epoch 215/300, resid Loss: 0.0965 | 0.1073
Epoch 216/300, resid Loss: 0.0963 | 0.1076
Epoch 217/300, resid Loss: 0.0953 | 0.1069
Epoch 218/300, resid Loss: 0.0963 | 0.1067
Epoch 219/300, resid Loss: 0.0955 | 0.1065
Epoch 220/300, resid Loss: 0.0957 | 0.1071
Epoch 221/300, resid Loss: 0.0952 | 0.1065
Epoch 222/300, resid Loss: 0.0952 | 0.1063
Epoch 223/300, resid Loss: 0.0949 | 0.1065
Epoch 224/300, resid Loss: 0.0953 | 0.1061
Epoch 225/300, resid Loss: 0.0949 | 0.1077
Epoch 226/300, resid Loss: 0.0952 | 0.1062
Epoch 227/300, resid Loss: 0.0944 | 0.1069
Epoch 228/300, resid Loss: 0.0946 | 0.1064
Epoch 229/300, resid Loss: 0.0950 | 0.1067
Epoch 230/300, resid Loss: 0.0950 | 0.1061
Epoch 231/300, resid Loss: 0.0950 | 0.1069
Epoch 232/300, resid Loss: 0.0947 | 0.1061
Epoch 233/300, resid Loss: 0.0936 | 0.1066
Epoch 234/300, resid Loss: 0.0947 | 0.1068
Epoch 235/300, resid Loss: 0.0940 | 0.1067
Epoch 236/300, resid Loss: 0.0939 | 0.1066
Epoch 237/300, resid Loss: 0.0936 | 0.1062
Epoch 238/300, resid Loss: 0.0935 | 0.1057
Epoch 239/300, resid Loss: 0.0937 | 0.1057
Epoch 240/300, resid Loss: 0.0938 | 0.1052
Epoch 241/300, resid Loss: 0.0934 | 0.1056
Epoch 242/300, resid Loss: 0.0928 | 0.1053
Epoch 243/300, resid Loss: 0.0930 | 0.1046
Epoch 244/300, resid Loss: 0.0934 | 0.1052
Epoch 245/300, resid Loss: 0.0924 | 0.1050
Epoch 246/300, resid Loss: 0.0932 | 0.1055
Epoch 247/300, resid Loss: 0.0926 | 0.1050
Epoch 248/300, resid Loss: 0.0927 | 0.1053
Epoch 249/300, resid Loss: 0.0927 | 0.1047
Epoch 250/300, resid Loss: 0.0925 | 0.1044
Epoch 251/300, resid Loss: 0.0922 | 0.1048
Epoch 252/300, resid Loss: 0.0929 | 0.1050
Epoch 253/300, resid Loss: 0.0926 | 0.1049
Epoch 254/300, resid Loss: 0.0918 | 0.1049
Epoch 255/300, resid Loss: 0.0921 | 0.1042
Epoch 256/300, resid Loss: 0.0927 | 0.1047
Epoch 257/300, resid Loss: 0.0916 | 0.1044
Epoch 258/300, resid Loss: 0.0916 | 0.1047
Epoch 259/300, resid Loss: 0.0923 | 0.1047
Epoch 260/300, resid Loss: 0.0920 | 0.1042
Epoch 261/300, resid Loss: 0.0922 | 0.1040
Epoch 262/300, resid Loss: 0.0914 | 0.1038
Epoch 263/300, resid Loss: 0.0915 | 0.1049
Epoch 264/300, resid Loss: 0.0916 | 0.1036
Epoch 265/300, resid Loss: 0.0910 | 0.1036
Epoch 266/300, resid Loss: 0.0921 | 0.1036
Epoch 267/300, resid Loss: 0.0909 | 0.1037
Epoch 268/300, resid Loss: 0.0910 | 0.1037
Epoch 269/300, resid Loss: 0.0908 | 0.1042
Epoch 270/300, resid Loss: 0.0913 | 0.1045
Epoch 271/300, resid Loss: 0.0906 | 0.1035
Epoch 272/300, resid Loss: 0.0913 | 0.1032
Epoch 273/300, resid Loss: 0.0915 | 0.1028
Epoch 274/300, resid Loss: 0.0912 | 0.1029
Epoch 275/300, resid Loss: 0.0913 | 0.1036
Epoch 276/300, resid Loss: 0.0903 | 0.1029
Epoch 277/300, resid Loss: 0.0907 | 0.1029
Epoch 278/300, resid Loss: 0.0906 | 0.1038
Epoch 279/300, resid Loss: 0.0907 | 0.1030
Epoch 280/300, resid Loss: 0.0909 | 0.1043
Epoch 281/300, resid Loss: 0.0905 | 0.1037
Epoch 282/300, resid Loss: 0.0900 | 0.1038
Epoch 283/300, resid Loss: 0.0903 | 0.1035
Epoch 284/300, resid Loss: 0.0899 | 0.1035
Epoch 285/300, resid Loss: 0.0906 | 0.1028
Epoch 286/300, resid Loss: 0.0906 | 0.1028
Epoch 287/300, resid Loss: 0.0902 | 0.1035
Epoch 288/300, resid Loss: 0.0904 | 0.1032
Epoch 289/300, resid Loss: 0.0894 | 0.1030
Epoch 290/300, resid Loss: 0.0899 | 0.1030
Epoch 291/300, resid Loss: 0.0903 | 0.1025
Epoch 292/300, resid Loss: 0.0900 | 0.1029
Epoch 293/300, resid Loss: 0.0893 | 0.1038
Epoch 294/300, resid Loss: 0.0897 | 0.1029
Epoch 295/300, resid Loss: 0.0892 | 0.1032
Epoch 296/300, resid Loss: 0.0896 | 0.1025
Epoch 297/300, resid Loss: 0.0896 | 0.1030
Epoch 298/300, resid Loss: 0.0891 | 0.1029
Epoch 299/300, resid Loss: 0.0899 | 0.1030
Epoch 300/300, resid Loss: 0.0896 | 0.1035
Runtime (seconds): 2418.1817543506622
0.0001394235182320582
[107.660965]
[0.34546995]
[1.1237347]
[-1.0138656]
[7.2501993]
[1.103483]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 28.302559106668923
RMSE: 5.320014953613281
MAE: 5.320014953613281
R-squared: nan
[116.469986]
