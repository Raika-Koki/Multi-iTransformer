ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 23:48:19,093][0m A new study created in memory with name: no-name-f167f10c-9819-4b43-ba1b-4ee254d280da[0m
Early stopping at epoch 84
[32m[I 2025-01-03 23:52:27,114][0m Trial 0 finished with value: 1.4190943035348287 and parameters: {'observation_period_num': 250, 'train_rates': 0.6161655699760028, 'learning_rate': 5.209715828908984e-06, 'batch_size': 57, 'step_size': 2, 'gamma': 0.7646733540117003}. Best is trial 0 with value: 1.4190943035348287.[0m
[32m[I 2025-01-03 23:55:29,346][0m Trial 1 finished with value: 0.9938122915296719 and parameters: {'observation_period_num': 134, 'train_rates': 0.7529430053106174, 'learning_rate': 0.00025940505007392703, 'batch_size': 33, 'step_size': 4, 'gamma': 0.9744442397098757}. Best is trial 1 with value: 0.9938122915296719.[0m
[32m[I 2025-01-03 23:58:43,398][0m Trial 2 finished with value: 0.2905790324254734 and parameters: {'observation_period_num': 135, 'train_rates': 0.8544715117131207, 'learning_rate': 6.642877416044055e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9579778881094083}. Best is trial 2 with value: 0.2905790324254734.[0m
[32m[I 2025-01-04 00:00:57,569][0m Trial 3 finished with value: 0.17678524553775787 and parameters: {'observation_period_num': 100, 'train_rates': 0.9491548337938639, 'learning_rate': 2.3593911412792902e-05, 'batch_size': 198, 'step_size': 14, 'gamma': 0.9837728713777183}. Best is trial 3 with value: 0.17678524553775787.[0m
[32m[I 2025-01-04 00:02:32,488][0m Trial 4 finished with value: 0.3683155406568501 and parameters: {'observation_period_num': 63, 'train_rates': 0.9489535473199372, 'learning_rate': 1.0368121676705408e-05, 'batch_size': 51, 'step_size': 3, 'gamma': 0.8133890067256712}. Best is trial 3 with value: 0.17678524553775787.[0m
[32m[I 2025-01-04 00:04:44,058][0m Trial 5 finished with value: 0.17253967106921 and parameters: {'observation_period_num': 98, 'train_rates': 0.8920237722087224, 'learning_rate': 5.469142413637028e-05, 'batch_size': 66, 'step_size': 7, 'gamma': 0.9328183655131457}. Best is trial 5 with value: 0.17253967106921.[0m
[32m[I 2025-01-04 00:07:54,706][0m Trial 6 finished with value: 0.6677396252004669 and parameters: {'observation_period_num': 134, 'train_rates': 0.8371089713017359, 'learning_rate': 2.5310502752100463e-06, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8029243940264155}. Best is trial 5 with value: 0.17253967106921.[0m
[32m[I 2025-01-04 00:11:17,022][0m Trial 7 finished with value: 0.130818292936858 and parameters: {'observation_period_num': 136, 'train_rates': 0.9278334215520193, 'learning_rate': 8.426336738149975e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9582814504487802}. Best is trial 7 with value: 0.130818292936858.[0m
[32m[I 2025-01-04 00:13:39,618][0m Trial 8 finished with value: 0.3206292860859993 and parameters: {'observation_period_num': 115, 'train_rates': 0.8425872565558277, 'learning_rate': 0.00015626297793002236, 'batch_size': 114, 'step_size': 8, 'gamma': 0.7954622482624885}. Best is trial 7 with value: 0.130818292936858.[0m
[32m[I 2025-01-04 00:15:24,113][0m Trial 9 finished with value: 0.4896062887273729 and parameters: {'observation_period_num': 80, 'train_rates': 0.9111498515217689, 'learning_rate': 3.9849710170630355e-06, 'batch_size': 127, 'step_size': 10, 'gamma': 0.9195776471735255}. Best is trial 7 with value: 0.130818292936858.[0m
[32m[I 2025-01-04 00:19:29,666][0m Trial 10 finished with value: 0.8715982813333545 and parameters: {'observation_period_num': 197, 'train_rates': 0.7273779728522876, 'learning_rate': 0.0008019660937374439, 'batch_size': 243, 'step_size': 15, 'gamma': 0.8844775809623493}. Best is trial 7 with value: 0.130818292936858.[0m
[32m[I 2025-01-04 00:20:19,731][0m Trial 11 finished with value: 0.1036735326051712 and parameters: {'observation_period_num': 17, 'train_rates': 0.977211746106709, 'learning_rate': 3.9735190426479786e-05, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9195552007677906}. Best is trial 11 with value: 0.1036735326051712.[0m
[32m[I 2025-01-04 00:21:08,054][0m Trial 12 finished with value: 0.16921813786029816 and parameters: {'observation_period_num': 12, 'train_rates': 0.9899573520773208, 'learning_rate': 1.9414599768484e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8839074202362333}. Best is trial 11 with value: 0.1036735326051712.[0m
[32m[I 2025-01-04 00:21:39,144][0m Trial 13 finished with value: 0.07970857620239258 and parameters: {'observation_period_num': 13, 'train_rates': 0.9753175281497932, 'learning_rate': 0.00013431963840526654, 'batch_size': 166, 'step_size': 11, 'gamma': 0.9224923586269907}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:22:08,342][0m Trial 14 finished with value: 1.5094285011291504 and parameters: {'observation_period_num': 11, 'train_rates': 0.9697409211443522, 'learning_rate': 1.0946813625758895e-06, 'batch_size': 169, 'step_size': 5, 'gamma': 0.854063495501961}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:22:57,822][0m Trial 15 finished with value: 0.9068624531943078 and parameters: {'observation_period_num': 47, 'train_rates': 0.6479962259938569, 'learning_rate': 0.0002024099979191556, 'batch_size': 161, 'step_size': 10, 'gamma': 0.9143377090137262}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:23:47,480][0m Trial 16 finished with value: 1.4593803446057816 and parameters: {'observation_period_num': 38, 'train_rates': 0.7807751369590523, 'learning_rate': 0.0006929700345529129, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8654363812347889}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:27:53,228][0m Trial 17 finished with value: 0.6473085463559388 and parameters: {'observation_period_num': 181, 'train_rates': 0.8741824436356909, 'learning_rate': 3.4095450919572214e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.90583079848237}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:28:31,320][0m Trial 18 finished with value: 0.7607126053742 and parameters: {'observation_period_num': 34, 'train_rates': 0.7003004541694534, 'learning_rate': 0.00044727537487195397, 'batch_size': 205, 'step_size': 7, 'gamma': 0.8479580385305382}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:28:53,177][0m Trial 19 finished with value: 0.3111452802881464 and parameters: {'observation_period_num': 7, 'train_rates': 0.8118067095896115, 'learning_rate': 0.00012986709799233339, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9406983202524855}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:30:28,480][0m Trial 20 finished with value: 0.27092936635017395 and parameters: {'observation_period_num': 69, 'train_rates': 0.9888022676522679, 'learning_rate': 1.682001680877105e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.8893444796814639}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:34:21,143][0m Trial 21 finished with value: 0.15285329067326606 and parameters: {'observation_period_num': 163, 'train_rates': 0.9160373444216141, 'learning_rate': 7.454918400890782e-05, 'batch_size': 83, 'step_size': 13, 'gamma': 0.9477485434355697}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:39:00,607][0m Trial 22 finished with value: 0.1465030688153216 and parameters: {'observation_period_num': 164, 'train_rates': 0.9396710572340568, 'learning_rate': 0.00010496948528209157, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9652343490254559}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:39:47,889][0m Trial 23 finished with value: 0.15774042960874277 and parameters: {'observation_period_num': 32, 'train_rates': 0.9277805537256292, 'learning_rate': 4.052691915424867e-05, 'batch_size': 113, 'step_size': 13, 'gamma': 0.9279535337076779}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:41:05,917][0m Trial 24 finished with value: 0.17128704488277435 and parameters: {'observation_period_num': 53, 'train_rates': 0.9571510460429631, 'learning_rate': 0.00031166892431698506, 'batch_size': 73, 'step_size': 11, 'gamma': 0.8984401374601342}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:46:01,147][0m Trial 25 finished with value: 0.20653101801872253 and parameters: {'observation_period_num': 208, 'train_rates': 0.8901127702131449, 'learning_rate': 7.885369100779238e-05, 'batch_size': 151, 'step_size': 15, 'gamma': 0.9546474291187119}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:47:53,534][0m Trial 26 finished with value: 0.12832097709178925 and parameters: {'observation_period_num': 82, 'train_rates': 0.9786719099206974, 'learning_rate': 3.805366604332672e-05, 'batch_size': 136, 'step_size': 8, 'gamma': 0.9887269340306696}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:49:58,943][0m Trial 27 finished with value: 0.1510974019765854 and parameters: {'observation_period_num': 91, 'train_rates': 0.9810556320832692, 'learning_rate': 3.2503647495179134e-05, 'batch_size': 140, 'step_size': 8, 'gamma': 0.9878405185700958}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:50:35,933][0m Trial 28 finished with value: 0.504181185743019 and parameters: {'observation_period_num': 26, 'train_rates': 0.8898712221343721, 'learning_rate': 9.366611804448731e-06, 'batch_size': 179, 'step_size': 7, 'gamma': 0.8324869027807207}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:51:39,726][0m Trial 29 finished with value: 1.3477173164931093 and parameters: {'observation_period_num': 63, 'train_rates': 0.616996658563644, 'learning_rate': 1.0270560344992766e-05, 'batch_size': 132, 'step_size': 5, 'gamma': 0.7500785576460177}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:52:22,186][0m Trial 30 finished with value: 0.13980685842037202 and parameters: {'observation_period_num': 22, 'train_rates': 0.9575868624741106, 'learning_rate': 4.6107375054737434e-05, 'batch_size': 106, 'step_size': 3, 'gamma': 0.9398343778582192}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:54:52,052][0m Trial 31 finished with value: 0.15516036637475558 and parameters: {'observation_period_num': 115, 'train_rates': 0.9247541616267818, 'learning_rate': 9.941043108665129e-05, 'batch_size': 177, 'step_size': 9, 'gamma': 0.9713488679373461}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 00:58:39,365][0m Trial 32 finished with value: 0.13630918479418452 and parameters: {'observation_period_num': 150, 'train_rates': 0.972023618745526, 'learning_rate': 0.00017584628648349796, 'batch_size': 54, 'step_size': 11, 'gamma': 0.9890489759642951}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:04:42,702][0m Trial 33 finished with value: 0.13036875193678468 and parameters: {'observation_period_num': 237, 'train_rates': 0.9341686551151335, 'learning_rate': 0.00032724377122569, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9652215615628422}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:11:18,750][0m Trial 34 finished with value: 0.1361255794763565 and parameters: {'observation_period_num': 251, 'train_rates': 0.9514541228275912, 'learning_rate': 0.00034550377578078747, 'batch_size': 213, 'step_size': 8, 'gamma': 0.9719653342989812}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:16:14,583][0m Trial 35 finished with value: 0.22105318956515368 and parameters: {'observation_period_num': 203, 'train_rates': 0.9073148060904878, 'learning_rate': 0.0005039544480338904, 'batch_size': 143, 'step_size': 6, 'gamma': 0.9756321499477881}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:21:45,324][0m Trial 36 finished with value: 0.41168732558443244 and parameters: {'observation_period_num': 231, 'train_rates': 0.8656804050613157, 'learning_rate': 2.41941633337906e-05, 'batch_size': 188, 'step_size': 9, 'gamma': 0.9284717355864628}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:27:48,213][0m Trial 37 finished with value: 0.13576382398605347 and parameters: {'observation_period_num': 234, 'train_rates': 0.9657884911234091, 'learning_rate': 0.0002476554899481173, 'batch_size': 120, 'step_size': 6, 'gamma': 0.9519691682904571}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:29:30,319][0m Trial 38 finished with value: 0.16290932893753052 and parameters: {'observation_period_num': 77, 'train_rates': 0.9493789655106798, 'learning_rate': 5.8647621275664536e-05, 'batch_size': 220, 'step_size': 4, 'gamma': 0.9139342532688433}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:30:40,527][0m Trial 39 finished with value: 0.18684641199727212 and parameters: {'observation_period_num': 53, 'train_rates': 0.9360469066418541, 'learning_rate': 1.3971980402105503e-05, 'batch_size': 143, 'step_size': 10, 'gamma': 0.9802743130367962}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:32:55,957][0m Trial 40 finished with value: 0.4784768644611207 and parameters: {'observation_period_num': 111, 'train_rates': 0.8320311148694706, 'learning_rate': 0.0001341411323902934, 'batch_size': 126, 'step_size': 7, 'gamma': 0.9630612077478502}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:35:00,414][0m Trial 41 finished with value: 0.1775860219249423 and parameters: {'observation_period_num': 84, 'train_rates': 0.9011622182615775, 'learning_rate': 7.61720610524056e-05, 'batch_size': 43, 'step_size': 13, 'gamma': 0.9541359943236529}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:38:54,281][0m Trial 42 finished with value: 0.10809071641415358 and parameters: {'observation_period_num': 138, 'train_rates': 0.932062301229437, 'learning_rate': 4.743307146803092e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9377180274098519}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:41:42,489][0m Trial 43 finished with value: 0.11186462148062644 and parameters: {'observation_period_num': 103, 'train_rates': 0.9891451856522787, 'learning_rate': 2.5554446788732316e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.9296503259361285}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:45:01,108][0m Trial 44 finished with value: 0.11326060025021434 and parameters: {'observation_period_num': 127, 'train_rates': 0.9886768261258859, 'learning_rate': 2.591887816922976e-05, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8980809447549872}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:48:22,115][0m Trial 45 finished with value: 0.10794291204335738 and parameters: {'observation_period_num': 126, 'train_rates': 0.9653959159316251, 'learning_rate': 2.6255151285739634e-05, 'batch_size': 24, 'step_size': 14, 'gamma': 0.8997901643006049}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:52:42,571][0m Trial 46 finished with value: 0.1325262761972052 and parameters: {'observation_period_num': 153, 'train_rates': 0.9664052317019842, 'learning_rate': 6.055180725323326e-06, 'batch_size': 19, 'step_size': 14, 'gamma': 0.8740954288220941}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:55:54,745][0m Trial 47 finished with value: 0.12677487530387366 and parameters: {'observation_period_num': 129, 'train_rates': 0.9448787846324873, 'learning_rate': 4.86785334877822e-05, 'batch_size': 66, 'step_size': 14, 'gamma': 0.9206540001754451}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 01:58:15,057][0m Trial 48 finished with value: 0.6414582834524267 and parameters: {'observation_period_num': 106, 'train_rates': 0.7670490251546138, 'learning_rate': 1.5292350769585495e-05, 'batch_size': 32, 'step_size': 12, 'gamma': 0.9069974213835008}. Best is trial 13 with value: 0.07970857620239258.[0m
[32m[I 2025-01-04 02:02:42,375][0m Trial 49 finished with value: 0.1918458399073831 and parameters: {'observation_period_num': 180, 'train_rates': 0.9686545918606907, 'learning_rate': 7.421503124473738e-06, 'batch_size': 64, 'step_size': 15, 'gamma': 0.941036834835488}. Best is trial 13 with value: 0.07970857620239258.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 02:02:42,382][0m A new study created in memory with name: no-name-478bb940-f9b7-4897-9bd4-62cc26261a8f[0m
[32m[I 2025-01-04 02:04:15,636][0m Trial 0 finished with value: 0.4558777849615356 and parameters: {'observation_period_num': 33, 'train_rates': 0.9723653695797838, 'learning_rate': 0.0004143765123236296, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9425829357480919}. Best is trial 0 with value: 0.4558777849615356.[0m
[32m[I 2025-01-04 02:07:12,617][0m Trial 1 finished with value: 0.942852787176768 and parameters: {'observation_period_num': 161, 'train_rates': 0.6184064685263341, 'learning_rate': 1.418515709606192e-05, 'batch_size': 71, 'step_size': 10, 'gamma': 0.8943470870622001}. Best is trial 0 with value: 0.4558777849615356.[0m
[32m[I 2025-01-04 02:13:12,668][0m Trial 2 finished with value: 0.5863034129142761 and parameters: {'observation_period_num': 228, 'train_rates': 0.9813725503012027, 'learning_rate': 4.0924307279258936e-06, 'batch_size': 185, 'step_size': 8, 'gamma': 0.8821021161728468}. Best is trial 0 with value: 0.4558777849615356.[0m
[32m[I 2025-01-04 02:15:56,376][0m Trial 3 finished with value: 0.9694313006847293 and parameters: {'observation_period_num': 142, 'train_rates': 0.6700975412346444, 'learning_rate': 2.350258350384485e-05, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8470198943089897}. Best is trial 0 with value: 0.4558777849615356.[0m
Early stopping at epoch 74
[32m[I 2025-01-04 02:18:13,974][0m Trial 4 finished with value: 1.2741821746599107 and parameters: {'observation_period_num': 79, 'train_rates': 0.8174826682819831, 'learning_rate': 2.7547798168876625e-06, 'batch_size': 19, 'step_size': 1, 'gamma': 0.8428966746766584}. Best is trial 0 with value: 0.4558777849615356.[0m
[32m[I 2025-01-04 02:21:00,177][0m Trial 5 finished with value: 1.9873950441824995 and parameters: {'observation_period_num': 110, 'train_rates': 0.9046229682946276, 'learning_rate': 0.0008822983547831258, 'batch_size': 29, 'step_size': 11, 'gamma': 0.952991465998263}. Best is trial 0 with value: 0.4558777849615356.[0m
[32m[I 2025-01-04 02:24:11,025][0m Trial 6 finished with value: 0.7102529957294464 and parameters: {'observation_period_num': 153, 'train_rates': 0.7766813077231864, 'learning_rate': 3.5699116839650204e-05, 'batch_size': 189, 'step_size': 14, 'gamma': 0.8250833221319451}. Best is trial 0 with value: 0.4558777849615356.[0m
[32m[I 2025-01-04 02:25:50,412][0m Trial 7 finished with value: 0.1649254411458969 and parameters: {'observation_period_num': 76, 'train_rates': 0.9336546733918394, 'learning_rate': 0.0005915171510724594, 'batch_size': 208, 'step_size': 4, 'gamma': 0.8658294698362876}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:29:40,731][0m Trial 8 finished with value: 0.5760603547096252 and parameters: {'observation_period_num': 161, 'train_rates': 0.9468421109727511, 'learning_rate': 4.619939121643565e-06, 'batch_size': 233, 'step_size': 12, 'gamma': 0.8815750170639192}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:31:27,607][0m Trial 9 finished with value: 1.1312415955111015 and parameters: {'observation_period_num': 99, 'train_rates': 0.660886527597932, 'learning_rate': 2.6824331077194654e-05, 'batch_size': 222, 'step_size': 8, 'gamma': 0.7513733800643562}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:32:04,888][0m Trial 10 finished with value: 0.35571486694472176 and parameters: {'observation_period_num': 25, 'train_rates': 0.856622181920079, 'learning_rate': 0.00017180482738038232, 'batch_size': 144, 'step_size': 3, 'gamma': 0.790571131525363}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:32:38,862][0m Trial 11 finished with value: 0.2771769209797658 and parameters: {'observation_period_num': 9, 'train_rates': 0.8649852876950007, 'learning_rate': 0.00015589644884802286, 'batch_size': 135, 'step_size': 3, 'gamma': 0.7649370119156366}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:33:49,618][0m Trial 12 finished with value: 0.2719352293273677 and parameters: {'observation_period_num': 56, 'train_rates': 0.880914644514636, 'learning_rate': 0.00011832539009354395, 'batch_size': 131, 'step_size': 4, 'gamma': 0.9897009374897892}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:35:02,255][0m Trial 13 finished with value: 0.7347551264776883 and parameters: {'observation_period_num': 63, 'train_rates': 0.7642630294256496, 'learning_rate': 0.0001183962680259412, 'batch_size': 125, 'step_size': 5, 'gamma': 0.9773384347396863}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:36:06,837][0m Trial 14 finished with value: 0.7836466385590984 and parameters: {'observation_period_num': 52, 'train_rates': 0.9110005746834212, 'learning_rate': 0.0009095336642385619, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9161232546966069}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:38:00,500][0m Trial 15 finished with value: 0.3229423119294597 and parameters: {'observation_period_num': 88, 'train_rates': 0.9098397338978529, 'learning_rate': 6.676297446714379e-05, 'batch_size': 169, 'step_size': 1, 'gamma': 0.9191491223830197}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:43:08,026][0m Trial 16 finished with value: 0.3578569305812852 and parameters: {'observation_period_num': 219, 'train_rates': 0.8294480763969625, 'learning_rate': 0.000351787009137956, 'batch_size': 98, 'step_size': 5, 'gamma': 0.808540221680939}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:45:43,737][0m Trial 17 finished with value: 1.026811770679664 and parameters: {'observation_period_num': 122, 'train_rates': 0.8797546872978226, 'learning_rate': 1.0568882215053146e-06, 'batch_size': 112, 'step_size': 3, 'gamma': 0.9859211927499261}. Best is trial 7 with value: 0.1649254411458969.[0m
[32m[I 2025-01-04 02:46:47,432][0m Trial 18 finished with value: 0.13041957186870887 and parameters: {'observation_period_num': 48, 'train_rates': 0.9369865465865885, 'learning_rate': 0.00039819424507881503, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8557356891107897}. Best is trial 18 with value: 0.13041957186870887.[0m
[32m[I 2025-01-04 02:47:39,894][0m Trial 19 finished with value: 0.13791024990110512 and parameters: {'observation_period_num': 38, 'train_rates': 0.9432295735162443, 'learning_rate': 0.00032843804594328457, 'batch_size': 165, 'step_size': 6, 'gamma': 0.8579469566083314}. Best is trial 18 with value: 0.13041957186870887.[0m
[32m[I 2025-01-04 02:48:05,668][0m Trial 20 finished with value: 0.6297982216676076 and parameters: {'observation_period_num': 5, 'train_rates': 0.7455898339534808, 'learning_rate': 0.000271652617568539, 'batch_size': 161, 'step_size': 6, 'gamma': 0.7947172580813556}. Best is trial 18 with value: 0.13041957186870887.[0m
[32m[I 2025-01-04 02:49:01,843][0m Trial 21 finished with value: 0.15983577072620392 and parameters: {'observation_period_num': 42, 'train_rates': 0.9492557663181774, 'learning_rate': 0.0004947698418823499, 'batch_size': 207, 'step_size': 6, 'gamma': 0.8561794710903056}. Best is trial 18 with value: 0.13041957186870887.[0m
[32m[I 2025-01-04 02:49:56,118][0m Trial 22 finished with value: 0.11761733889579773 and parameters: {'observation_period_num': 39, 'train_rates': 0.9887502536782218, 'learning_rate': 0.000274319133329466, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8502339834853416}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 02:50:36,372][0m Trial 23 finished with value: 0.13630880415439606 and parameters: {'observation_period_num': 25, 'train_rates': 0.9869628505306665, 'learning_rate': 6.532232497004275e-05, 'batch_size': 160, 'step_size': 10, 'gamma': 0.8261070325732205}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 02:51:10,151][0m Trial 24 finished with value: 0.16787105798721313 and parameters: {'observation_period_num': 21, 'train_rates': 0.9899106348398214, 'learning_rate': 6.137734202420443e-05, 'batch_size': 184, 'step_size': 10, 'gamma': 0.8186020370959304}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 02:52:40,775][0m Trial 25 finished with value: 0.14541424810886383 and parameters: {'observation_period_num': 65, 'train_rates': 0.9887906522411312, 'learning_rate': 5.7525868606063366e-05, 'batch_size': 156, 'step_size': 15, 'gamma': 0.8286858685069114}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 02:57:22,964][0m Trial 26 finished with value: 0.2663645491943703 and parameters: {'observation_period_num': 191, 'train_rates': 0.9598292559136856, 'learning_rate': 1.3546017175588036e-05, 'batch_size': 91, 'step_size': 12, 'gamma': 0.7857326350515857}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 02:57:55,397][0m Trial 27 finished with value: 0.14589145394837913 and parameters: {'observation_period_num': 21, 'train_rates': 0.9274363274666807, 'learning_rate': 0.00022565075285695577, 'batch_size': 196, 'step_size': 10, 'gamma': 0.9028382218701103}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:04:20,305][0m Trial 28 finished with value: 0.18271060287952423 and parameters: {'observation_period_num': 247, 'train_rates': 0.9576093247903925, 'learning_rate': 9.102768701922629e-05, 'batch_size': 178, 'step_size': 9, 'gamma': 0.840807820012877}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:05:13,202][0m Trial 29 finished with value: 0.18490737676620483 and parameters: {'observation_period_num': 38, 'train_rates': 0.9656688580535188, 'learning_rate': 0.0005061187549903755, 'batch_size': 150, 'step_size': 13, 'gamma': 0.8072373075494346}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:07:06,924][0m Trial 30 finished with value: 0.21864271221809017 and parameters: {'observation_period_num': 87, 'train_rates': 0.8922101140988826, 'learning_rate': 0.00018273645426667969, 'batch_size': 120, 'step_size': 9, 'gamma': 0.8736427567822127}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:07:56,175][0m Trial 31 finished with value: 0.12792989576314862 and parameters: {'observation_period_num': 36, 'train_rates': 0.9277877235031637, 'learning_rate': 0.0003393998991291847, 'batch_size': 168, 'step_size': 7, 'gamma': 0.8623758700430176}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:08:57,917][0m Trial 32 finished with value: 0.189217553082095 and parameters: {'observation_period_num': 47, 'train_rates': 0.924060912664515, 'learning_rate': 0.0006754306648972268, 'batch_size': 203, 'step_size': 7, 'gamma': 0.837608334726675}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:09:38,014][0m Trial 33 finished with value: 0.1308845579624176 and parameters: {'observation_period_num': 26, 'train_rates': 0.9752255307396882, 'learning_rate': 0.00032528474014504195, 'batch_size': 174, 'step_size': 9, 'gamma': 0.895003499928347}. Best is trial 22 with value: 0.11761733889579773.[0m
[32m[I 2025-01-04 03:10:01,617][0m Trial 34 finished with value: 0.10757067799568176 and parameters: {'observation_period_num': 5, 'train_rates': 0.9707094688598665, 'learning_rate': 0.0003974905889163145, 'batch_size': 222, 'step_size': 7, 'gamma': 0.8968141479926932}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:10:24,510][0m Trial 35 finished with value: 0.27963813808993987 and parameters: {'observation_period_num': 5, 'train_rates': 0.853257331420793, 'learning_rate': 0.00041358125645927384, 'batch_size': 226, 'step_size': 7, 'gamma': 0.9251169968282649}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:11:31,899][0m Trial 36 finished with value: 0.4070383415282148 and parameters: {'observation_period_num': 58, 'train_rates': 0.8352056966062279, 'learning_rate': 0.0009448271899879018, 'batch_size': 246, 'step_size': 8, 'gamma': 0.8788042633767852}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:12:52,884][0m Trial 37 finished with value: 0.7071041837684425 and parameters: {'observation_period_num': 73, 'train_rates': 0.7040600948146909, 'learning_rate': 0.0002385609603513298, 'batch_size': 218, 'step_size': 7, 'gamma': 0.8951035338147446}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:14:56,149][0m Trial 38 finished with value: 0.7709487066256194 and parameters: {'observation_period_num': 103, 'train_rates': 0.8028973844220056, 'learning_rate': 1.4582153006107225e-05, 'batch_size': 241, 'step_size': 11, 'gamma': 0.8584173680556796}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:18:04,683][0m Trial 39 finished with value: 0.15924918581292313 and parameters: {'observation_period_num': 137, 'train_rates': 0.9248386872101518, 'learning_rate': 0.0005908990748242361, 'batch_size': 191, 'step_size': 9, 'gamma': 0.9081712832137401}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:18:45,870][0m Trial 40 finished with value: 0.8391367591549997 and parameters: {'observation_period_num': 38, 'train_rates': 0.6174677798326696, 'learning_rate': 4.034262143450572e-05, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9330440616317912}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:19:17,850][0m Trial 41 finished with value: 0.1273641437292099 and parameters: {'observation_period_num': 17, 'train_rates': 0.9663088311589775, 'learning_rate': 0.00034024919543356385, 'batch_size': 176, 'step_size': 9, 'gamma': 0.8917559556880141}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:19:44,824][0m Trial 42 finished with value: 0.2545488774776459 and parameters: {'observation_period_num': 16, 'train_rates': 0.9642431572424695, 'learning_rate': 0.0006712026860457533, 'batch_size': 213, 'step_size': 11, 'gamma': 0.8880765515958983}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:20:27,868][0m Trial 43 finished with value: 0.15185193717479706 and parameters: {'observation_period_num': 30, 'train_rates': 0.9423069054236788, 'learning_rate': 0.00013641602314983393, 'batch_size': 180, 'step_size': 6, 'gamma': 0.8674262617562767}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:21:46,273][0m Trial 44 finished with value: 0.12834294476816732 and parameters: {'observation_period_num': 15, 'train_rates': 0.9051547480611115, 'learning_rate': 0.0001926668238748875, 'batch_size': 54, 'step_size': 8, 'gamma': 0.8494225430705703}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:23:44,427][0m Trial 45 finished with value: 0.14494975103812963 and parameters: {'observation_period_num': 13, 'train_rates': 0.9002138071024675, 'learning_rate': 0.000227037391898578, 'batch_size': 32, 'step_size': 8, 'gamma': 0.9468900915683522}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:25:38,680][0m Trial 46 finished with value: 0.1222318017698196 and parameters: {'observation_period_num': 33, 'train_rates': 0.971753876380131, 'learning_rate': 8.895469445028305e-05, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8859580434664015}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:27:20,781][0m Trial 47 finished with value: 0.15067293122410774 and parameters: {'observation_period_num': 70, 'train_rates': 0.9681934919668534, 'learning_rate': 9.812936674489122e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.8819784202266384}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:28:07,084][0m Trial 48 finished with value: 0.15969665348529816 and parameters: {'observation_period_num': 33, 'train_rates': 0.9788657048526692, 'learning_rate': 4.376001386166919e-05, 'batch_size': 194, 'step_size': 12, 'gamma': 0.8695757549531407}. Best is trial 34 with value: 0.10757067799568176.[0m
[32m[I 2025-01-04 03:29:57,708][0m Trial 49 finished with value: 0.17028872668743134 and parameters: {'observation_period_num': 84, 'train_rates': 0.9535526755458101, 'learning_rate': 9.156820130634696e-05, 'batch_size': 228, 'step_size': 10, 'gamma': 0.9028761049856997}. Best is trial 34 with value: 0.10757067799568176.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 03:29:57,716][0m A new study created in memory with name: no-name-0efe07c5-0b27-487e-853f-59c9bd585782[0m
[32m[I 2025-01-04 03:33:02,564][0m Trial 0 finished with value: 0.3220149145314568 and parameters: {'observation_period_num': 142, 'train_rates': 0.8377964628167415, 'learning_rate': 9.967833422281735e-05, 'batch_size': 163, 'step_size': 11, 'gamma': 0.9534905311269135}. Best is trial 0 with value: 0.3220149145314568.[0m
[32m[I 2025-01-04 03:34:07,786][0m Trial 1 finished with value: 1.1954950725144529 and parameters: {'observation_period_num': 59, 'train_rates': 0.7402334325139764, 'learning_rate': 1.0792554647807237e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.9792876295175281}. Best is trial 0 with value: 0.3220149145314568.[0m
[32m[I 2025-01-04 03:34:49,000][0m Trial 2 finished with value: 0.7216119924223566 and parameters: {'observation_period_num': 35, 'train_rates': 0.7724905502059679, 'learning_rate': 1.947869911581613e-05, 'batch_size': 192, 'step_size': 8, 'gamma': 0.9181856197002972}. Best is trial 0 with value: 0.3220149145314568.[0m
[32m[I 2025-01-04 03:36:20,593][0m Trial 3 finished with value: 0.18393708048043428 and parameters: {'observation_period_num': 5, 'train_rates': 0.9818518811697361, 'learning_rate': 1.5897715849429705e-05, 'batch_size': 46, 'step_size': 2, 'gamma': 0.9196845374123808}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:39:41,287][0m Trial 4 finished with value: 1.1540228994329862 and parameters: {'observation_period_num': 175, 'train_rates': 0.6837359285319172, 'learning_rate': 8.610041538242757e-05, 'batch_size': 173, 'step_size': 3, 'gamma': 0.8054567700435407}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:40:39,432][0m Trial 5 finished with value: 1.3839926214541418 and parameters: {'observation_period_num': 51, 'train_rates': 0.7763275499028646, 'learning_rate': 5.478202970232001e-06, 'batch_size': 200, 'step_size': 4, 'gamma': 0.8762732659574943}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:42:19,613][0m Trial 6 finished with value: 0.7345894746261068 and parameters: {'observation_period_num': 79, 'train_rates': 0.8945081377470347, 'learning_rate': 4.790863271736524e-06, 'batch_size': 220, 'step_size': 11, 'gamma': 0.8950067856821977}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:46:42,164][0m Trial 7 finished with value: 1.527044680658844 and parameters: {'observation_period_num': 222, 'train_rates': 0.647598260537076, 'learning_rate': 1.2516841562841924e-05, 'batch_size': 200, 'step_size': 2, 'gamma': 0.8707238552555421}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:51:46,772][0m Trial 8 finished with value: 1.011105390604894 and parameters: {'observation_period_num': 242, 'train_rates': 0.7104796389214785, 'learning_rate': 0.0009241109130878801, 'batch_size': 133, 'step_size': 2, 'gamma': 0.9282277887136585}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:54:29,782][0m Trial 9 finished with value: 0.7030598583345662 and parameters: {'observation_period_num': 77, 'train_rates': 0.8263756593199527, 'learning_rate': 0.00038553534381773616, 'batch_size': 24, 'step_size': 12, 'gamma': 0.7944290782458622}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:56:16,940][0m Trial 10 finished with value: 0.7340173765877697 and parameters: {'observation_period_num': 7, 'train_rates': 0.9637749417808523, 'learning_rate': 1.2608172563293838e-06, 'batch_size': 45, 'step_size': 6, 'gamma': 0.8290335435416603}. Best is trial 3 with value: 0.18393708048043428.[0m
[32m[I 2025-01-04 03:59:32,259][0m Trial 11 finished with value: 0.1517658829689026 and parameters: {'observation_period_num': 132, 'train_rates': 0.9856396037466268, 'learning_rate': 6.289813973380431e-05, 'batch_size': 93, 'step_size': 14, 'gamma': 0.9736440675594619}. Best is trial 11 with value: 0.1517658829689026.[0m
[32m[I 2025-01-04 04:02:19,581][0m Trial 12 finished with value: 0.13253848254680634 and parameters: {'observation_period_num': 120, 'train_rates': 0.979615005066186, 'learning_rate': 5.675339023000129e-05, 'batch_size': 87, 'step_size': 15, 'gamma': 0.9750789005464604}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:04:52,859][0m Trial 13 finished with value: 0.2267763472121695 and parameters: {'observation_period_num': 117, 'train_rates': 0.9029026561417702, 'learning_rate': 5.8982241530429356e-05, 'batch_size': 96, 'step_size': 15, 'gamma': 0.9870851258049281}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:07:37,035][0m Trial 14 finished with value: 0.15288656047000312 and parameters: {'observation_period_num': 123, 'train_rates': 0.9234348054809551, 'learning_rate': 0.00022015781799524252, 'batch_size': 99, 'step_size': 15, 'gamma': 0.7574478642245982}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:11:37,577][0m Trial 15 finished with value: 0.14903231355987612 and parameters: {'observation_period_num': 168, 'train_rates': 0.9345163761625093, 'learning_rate': 4.284455783175683e-05, 'batch_size': 96, 'step_size': 13, 'gamma': 0.9554461693553703}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:15:56,659][0m Trial 16 finished with value: 0.31794575071163317 and parameters: {'observation_period_num': 191, 'train_rates': 0.849015010778021, 'learning_rate': 3.813670336607858e-05, 'batch_size': 134, 'step_size': 13, 'gamma': 0.9476746253106598}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:20:05,031][0m Trial 17 finished with value: 0.13717786967754364 and parameters: {'observation_period_num': 176, 'train_rates': 0.9316977227849716, 'learning_rate': 0.00020895533244928022, 'batch_size': 256, 'step_size': 9, 'gamma': 0.8426503497808776}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:24:00,647][0m Trial 18 finished with value: 0.929889628552072 and parameters: {'observation_period_num': 208, 'train_rates': 0.6048037251295397, 'learning_rate': 0.00021938595555918142, 'batch_size': 73, 'step_size': 9, 'gamma': 0.8291757772194651}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:26:09,980][0m Trial 19 finished with value: 0.26934199192022024 and parameters: {'observation_period_num': 103, 'train_rates': 0.8801641515847474, 'learning_rate': 0.0008569507737712174, 'batch_size': 254, 'step_size': 6, 'gamma': 0.8231916831803578}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:30:02,009][0m Trial 20 finished with value: 0.13420073688030243 and parameters: {'observation_period_num': 161, 'train_rates': 0.9429527839941501, 'learning_rate': 0.00015760030208232187, 'batch_size': 232, 'step_size': 10, 'gamma': 0.8511147384799468}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:33:46,412][0m Trial 21 finished with value: 0.1422334760427475 and parameters: {'observation_period_num': 158, 'train_rates': 0.9431796679963387, 'learning_rate': 0.00016829051536553055, 'batch_size': 246, 'step_size': 10, 'gamma': 0.8529285728538535}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:37:08,092][0m Trial 22 finished with value: 0.24886190376869619 and parameters: {'observation_period_num': 151, 'train_rates': 0.8699168924021038, 'learning_rate': 0.0004219275214046743, 'batch_size': 228, 'step_size': 6, 'gamma': 0.8465778666773294}. Best is trial 12 with value: 0.13253848254680634.[0m
[32m[I 2025-01-04 04:41:36,357][0m Trial 23 finished with value: 0.1294008046388626 and parameters: {'observation_period_num': 185, 'train_rates': 0.9504402103514317, 'learning_rate': 0.00011556011693108225, 'batch_size': 232, 'step_size': 8, 'gamma': 0.8819702919303827}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 04:46:44,113][0m Trial 24 finished with value: 0.1393451690673828 and parameters: {'observation_period_num': 199, 'train_rates': 0.9882600487437883, 'learning_rate': 0.00012087026512303416, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8868711662222761}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 04:49:02,504][0m Trial 25 finished with value: 0.15308799120512875 and parameters: {'observation_period_num': 103, 'train_rates': 0.9539608164445439, 'learning_rate': 2.878628870501672e-05, 'batch_size': 120, 'step_size': 8, 'gamma': 0.9053130542286983}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 04:54:45,124][0m Trial 26 finished with value: 0.1825161655375987 and parameters: {'observation_period_num': 231, 'train_rates': 0.9110087230700838, 'learning_rate': 0.00035755242874560545, 'batch_size': 177, 'step_size': 10, 'gamma': 0.7662229019474278}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 04:58:51,168][0m Trial 27 finished with value: 0.3084776084305662 and parameters: {'observation_period_num': 187, 'train_rates': 0.8091365226289231, 'learning_rate': 0.00012173974839412106, 'batch_size': 71, 'step_size': 5, 'gamma': 0.7861022778217055}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 05:02:04,063][0m Trial 28 finished with value: 0.36690350067921174 and parameters: {'observation_period_num': 145, 'train_rates': 0.8610795711040133, 'learning_rate': 2.7897972362094516e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.8590210400258883}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 05:07:26,841][0m Trial 29 finished with value: 0.1468297839164734 and parameters: {'observation_period_num': 211, 'train_rates': 0.9605805035798853, 'learning_rate': 7.454736655822823e-05, 'batch_size': 236, 'step_size': 11, 'gamma': 0.9469020930743922}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 05:10:26,651][0m Trial 30 finished with value: 0.648938396977166 and parameters: {'observation_period_num': 139, 'train_rates': 0.8323818365572065, 'learning_rate': 0.0005950869528585434, 'batch_size': 150, 'step_size': 10, 'gamma': 0.9349987061026926}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 05:14:31,770][0m Trial 31 finished with value: 0.1649332046508789 and parameters: {'observation_period_num': 173, 'train_rates': 0.9265295406675834, 'learning_rate': 0.00016488335268853403, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8420003239565251}. Best is trial 23 with value: 0.1294008046388626.[0m
[32m[I 2025-01-04 05:18:19,938][0m Trial 32 finished with value: 0.12820731103420258 and parameters: {'observation_period_num': 159, 'train_rates': 0.9597170085482464, 'learning_rate': 0.00021509610430379795, 'batch_size': 211, 'step_size': 8, 'gamma': 0.8104129326336788}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:22:05,999][0m Trial 33 finished with value: 0.14820444583892822 and parameters: {'observation_period_num': 157, 'train_rates': 0.9600667591209693, 'learning_rate': 0.0001226749973440815, 'batch_size': 205, 'step_size': 7, 'gamma': 0.8116789998832732}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:24:12,400][0m Trial 34 finished with value: 0.19332284886261514 and parameters: {'observation_period_num': 97, 'train_rates': 0.8986595586154424, 'learning_rate': 0.00032878339394627314, 'batch_size': 182, 'step_size': 7, 'gamma': 0.7790768651875507}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:27:23,145][0m Trial 35 finished with value: 0.15030765533447266 and parameters: {'observation_period_num': 131, 'train_rates': 0.9762708184714214, 'learning_rate': 5.481953266946383e-05, 'batch_size': 158, 'step_size': 8, 'gamma': 0.812565397186687}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:29:58,346][0m Trial 36 finished with value: 0.1669679582118988 and parameters: {'observation_period_num': 116, 'train_rates': 0.946227408773055, 'learning_rate': 9.404645929106093e-05, 'batch_size': 211, 'step_size': 5, 'gamma': 0.9076896857216576}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:33:47,148][0m Trial 37 finished with value: 0.43858404941699675 and parameters: {'observation_period_num': 164, 'train_rates': 0.9152957817715013, 'learning_rate': 6.895664758734936e-06, 'batch_size': 235, 'step_size': 12, 'gamma': 0.9680222010749095}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:37:58,105][0m Trial 38 finished with value: 0.45019068284468217 and parameters: {'observation_period_num': 182, 'train_rates': 0.8810259064172928, 'learning_rate': 2.0817931722021016e-05, 'batch_size': 192, 'step_size': 8, 'gamma': 0.8648125919834967}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:40:55,178][0m Trial 39 finished with value: 0.9115653423647487 and parameters: {'observation_period_num': 143, 'train_rates': 0.7712987172514859, 'learning_rate': 0.0006258644724032222, 'batch_size': 114, 'step_size': 4, 'gamma': 0.8873124549121374}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:42:57,009][0m Trial 40 finished with value: 0.13234943151474 and parameters: {'observation_period_num': 89, 'train_rates': 0.9707885193177902, 'learning_rate': 0.0002833029446520643, 'batch_size': 191, 'step_size': 14, 'gamma': 0.8023056124874185}. Best is trial 32 with value: 0.12820731103420258.[0m
[32m[I 2025-01-04 05:44:48,888][0m Trial 41 finished with value: 0.12286964803934097 and parameters: {'observation_period_num': 82, 'train_rates': 0.9702203269864003, 'learning_rate': 0.0002819880680439929, 'batch_size': 192, 'step_size': 14, 'gamma': 0.8014588759824258}. Best is trial 41 with value: 0.12286964803934097.[0m
[32m[I 2025-01-04 05:46:22,333][0m Trial 42 finished with value: 0.12684085965156555 and parameters: {'observation_period_num': 68, 'train_rates': 0.9714541653057522, 'learning_rate': 0.00023644313732348632, 'batch_size': 193, 'step_size': 14, 'gamma': 0.7975483963191423}. Best is trial 41 with value: 0.12286964803934097.[0m
[32m[I 2025-01-04 05:47:33,359][0m Trial 43 finished with value: 0.12560586631298065 and parameters: {'observation_period_num': 54, 'train_rates': 0.9716959657268276, 'learning_rate': 0.0002771181872589438, 'batch_size': 190, 'step_size': 14, 'gamma': 0.7972668965176166}. Best is trial 41 with value: 0.12286964803934097.[0m
[32m[I 2025-01-04 05:48:42,505][0m Trial 44 finished with value: 0.1691751927137375 and parameters: {'observation_period_num': 50, 'train_rates': 0.9885053058209773, 'learning_rate': 0.000539013186731508, 'batch_size': 168, 'step_size': 14, 'gamma': 0.7736706778361454}. Best is trial 41 with value: 0.12286964803934097.[0m
[32m[I 2025-01-04 05:49:47,766][0m Trial 45 finished with value: 0.6904260352253914 and parameters: {'observation_period_num': 61, 'train_rates': 0.7234430671390817, 'learning_rate': 0.00045174817842047915, 'batch_size': 212, 'step_size': 13, 'gamma': 0.7910456137104336}. Best is trial 41 with value: 0.12286964803934097.[0m
[32m[I 2025-01-04 05:50:31,604][0m Trial 46 finished with value: 0.10955360531806946 and parameters: {'observation_period_num': 30, 'train_rates': 0.9646055887146711, 'learning_rate': 0.0002505221045064007, 'batch_size': 187, 'step_size': 14, 'gamma': 0.753838937243689}. Best is trial 46 with value: 0.10955360531806946.[0m
[32m[I 2025-01-04 05:51:15,432][0m Trial 47 finished with value: 0.11989744752645493 and parameters: {'observation_period_num': 31, 'train_rates': 0.9687345182587455, 'learning_rate': 0.0002705172043752349, 'batch_size': 193, 'step_size': 14, 'gamma': 0.7524973413166085}. Best is trial 46 with value: 0.10955360531806946.[0m
[32m[I 2025-01-04 05:51:47,290][0m Trial 48 finished with value: 0.5478802919387817 and parameters: {'observation_period_num': 12, 'train_rates': 0.9722439972710082, 'learning_rate': 0.000738727658557066, 'batch_size': 150, 'step_size': 14, 'gamma': 0.7599411995701588}. Best is trial 46 with value: 0.10955360531806946.[0m
[32m[I 2025-01-04 05:52:20,045][0m Trial 49 finished with value: 0.1501549956126091 and parameters: {'observation_period_num': 21, 'train_rates': 0.9204350580444027, 'learning_rate': 0.0002713686966666071, 'batch_size': 192, 'step_size': 15, 'gamma': 0.7554978000350638}. Best is trial 46 with value: 0.10955360531806946.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 05:52:20,052][0m A new study created in memory with name: no-name-3b033ca1-531e-400d-9416-fac8277240ef[0m
[32m[I 2025-01-04 05:57:13,459][0m Trial 0 finished with value: 0.283747484149604 and parameters: {'observation_period_num': 194, 'train_rates': 0.8950864320095345, 'learning_rate': 4.512730441158807e-06, 'batch_size': 34, 'step_size': 6, 'gamma': 0.9874919414350712}. Best is trial 0 with value: 0.283747484149604.[0m
Early stopping at epoch 73
[32m[I 2025-01-04 06:00:07,008][0m Trial 1 finished with value: 0.28938373561217406 and parameters: {'observation_period_num': 159, 'train_rates': 0.9233685616565911, 'learning_rate': 6.200851507879645e-05, 'batch_size': 47, 'step_size': 1, 'gamma': 0.8371043782499609}. Best is trial 0 with value: 0.283747484149604.[0m
[32m[I 2025-01-04 06:01:43,118][0m Trial 2 finished with value: 0.9083957988074994 and parameters: {'observation_period_num': 89, 'train_rates': 0.6342956308478787, 'learning_rate': 0.0001856394747379181, 'batch_size': 84, 'step_size': 7, 'gamma': 0.9686131910653096}. Best is trial 0 with value: 0.283747484149604.[0m
[32m[I 2025-01-04 06:03:51,556][0m Trial 3 finished with value: 0.21816731475550552 and parameters: {'observation_period_num': 92, 'train_rates': 0.9595117284907821, 'learning_rate': 1.0326202100668868e-05, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8140952953671634}. Best is trial 3 with value: 0.21816731475550552.[0m
[32m[I 2025-01-04 06:06:53,141][0m Trial 4 finished with value: 1.1127713641216015 and parameters: {'observation_period_num': 153, 'train_rates': 0.7099364456606972, 'learning_rate': 3.9662503361179585e-06, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9840142586290868}. Best is trial 3 with value: 0.21816731475550552.[0m
[32m[I 2025-01-04 06:11:04,526][0m Trial 5 finished with value: 0.5957980030461362 and parameters: {'observation_period_num': 175, 'train_rates': 0.9454019927562967, 'learning_rate': 3.907536043746317e-06, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8821000862780346}. Best is trial 3 with value: 0.21816731475550552.[0m
[32m[I 2025-01-04 06:17:24,846][0m Trial 6 finished with value: 0.17017900943756104 and parameters: {'observation_period_num': 247, 'train_rates': 0.9272110986396154, 'learning_rate': 0.0003140116567085238, 'batch_size': 86, 'step_size': 5, 'gamma': 0.7733653473295514}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:17:52,891][0m Trial 7 finished with value: 0.881152990538823 and parameters: {'observation_period_num': 13, 'train_rates': 0.741526906254335, 'learning_rate': 3.457432637926663e-05, 'batch_size': 127, 'step_size': 5, 'gamma': 0.8227647819790472}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:20:30,693][0m Trial 8 finished with value: 0.34333300590515137 and parameters: {'observation_period_num': 115, 'train_rates': 0.9804593435308933, 'learning_rate': 3.3928390627468954e-06, 'batch_size': 136, 'step_size': 11, 'gamma': 0.9770603633537949}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:24:40,202][0m Trial 9 finished with value: 0.3186193121367676 and parameters: {'observation_period_num': 183, 'train_rates': 0.874103733806924, 'learning_rate': 3.281194217285981e-05, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9440158179499069}. Best is trial 6 with value: 0.17017900943756104.[0m
Early stopping at epoch 50
[32m[I 2025-01-04 06:27:35,294][0m Trial 10 finished with value: 0.9896877646164409 and parameters: {'observation_period_num': 238, 'train_rates': 0.8443714115360688, 'learning_rate': 0.0009678737490189878, 'batch_size': 179, 'step_size': 1, 'gamma': 0.7513846981125266}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:29:05,216][0m Trial 11 finished with value: 0.3805660676518711 and parameters: {'observation_period_num': 72, 'train_rates': 0.8109149321366723, 'learning_rate': 0.00029745497717355914, 'batch_size': 84, 'step_size': 11, 'gamma': 0.7676870908961256}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:35:48,578][0m Trial 12 finished with value: 0.1931558347354501 and parameters: {'observation_period_num': 248, 'train_rates': 0.9784414495995135, 'learning_rate': 1.2480286545297751e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.7938988734560014}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:43:06,199][0m Trial 13 finished with value: 0.44109805396624974 and parameters: {'observation_period_num': 248, 'train_rates': 0.9873223354179047, 'learning_rate': 1.0061878758107613e-06, 'batch_size': 16, 'step_size': 9, 'gamma': 0.786607648841112}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:48:18,238][0m Trial 14 finished with value: 0.1922308482050486 and parameters: {'observation_period_num': 210, 'train_rates': 0.8941651406660475, 'learning_rate': 0.00016270538049739186, 'batch_size': 52, 'step_size': 14, 'gamma': 0.8753408594277728}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:53:18,174][0m Trial 15 finished with value: 0.4819578095103189 and parameters: {'observation_period_num': 215, 'train_rates': 0.8414595274032786, 'learning_rate': 0.000158655967943234, 'batch_size': 115, 'step_size': 15, 'gamma': 0.8899142126096253}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 06:57:52,197][0m Trial 16 finished with value: 1.4335884455478554 and parameters: {'observation_period_num': 209, 'train_rates': 0.7596438039101432, 'learning_rate': 0.0008524547861330282, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9171034373089414}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 07:03:21,697][0m Trial 17 finished with value: 0.19479038443746446 and parameters: {'observation_period_num': 217, 'train_rates': 0.9134145819736416, 'learning_rate': 0.0003218775316845543, 'batch_size': 60, 'step_size': 3, 'gamma': 0.8585984694374602}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 07:06:33,951][0m Trial 18 finished with value: 0.32542205780890643 and parameters: {'observation_period_num': 145, 'train_rates': 0.8547456565387233, 'learning_rate': 9.111533288880636e-05, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8571954532862336}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 07:11:37,991][0m Trial 19 finished with value: 0.7450610169673556 and parameters: {'observation_period_num': 225, 'train_rates': 0.7923889940360727, 'learning_rate': 0.00045073630535172305, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9145597691298907}. Best is trial 6 with value: 0.17017900943756104.[0m
[32m[I 2025-01-04 07:16:41,459][0m Trial 20 finished with value: 0.16599937627712885 and parameters: {'observation_period_num': 192, 'train_rates': 0.9024477035943155, 'learning_rate': 0.0001134485731828523, 'batch_size': 18, 'step_size': 3, 'gamma': 0.900259494149992}. Best is trial 20 with value: 0.16599937627712885.[0m
[32m[I 2025-01-04 07:22:13,383][0m Trial 21 finished with value: 0.19342011474865548 and parameters: {'observation_period_num': 202, 'train_rates': 0.8974933317923347, 'learning_rate': 9.36768042202046e-05, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9049045535700534}. Best is trial 20 with value: 0.16599937627712885.[0m
[32m[I 2025-01-04 07:28:07,130][0m Trial 22 finished with value: 0.12550456574984958 and parameters: {'observation_period_num': 230, 'train_rates': 0.9360364237084997, 'learning_rate': 0.00015979076023269127, 'batch_size': 58, 'step_size': 3, 'gamma': 0.9396763459031316}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:34:24,867][0m Trial 23 finished with value: 0.8542190321375814 and parameters: {'observation_period_num': 236, 'train_rates': 0.9347096718435149, 'learning_rate': 0.0005271595497716678, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9429305743323204}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:38:39,282][0m Trial 24 finished with value: 0.15397652398936357 and parameters: {'observation_period_num': 175, 'train_rates': 0.9432132383278179, 'learning_rate': 6.387226856848816e-05, 'batch_size': 64, 'step_size': 2, 'gamma': 0.9371843053621732}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:42:48,541][0m Trial 25 finished with value: 0.13630701700846354 and parameters: {'observation_period_num': 169, 'train_rates': 0.9571483438693589, 'learning_rate': 6.61258195784384e-05, 'batch_size': 68, 'step_size': 2, 'gamma': 0.942256347143463}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:46:01,068][0m Trial 26 finished with value: 0.31493126209509575 and parameters: {'observation_period_num': 129, 'train_rates': 0.958364934233121, 'learning_rate': 1.7537728338257783e-05, 'batch_size': 69, 'step_size': 1, 'gamma': 0.9418555226317278}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:48:59,970][0m Trial 27 finished with value: 1.0104849258674442 and parameters: {'observation_period_num': 164, 'train_rates': 0.6104993940419752, 'learning_rate': 5.3419625795828567e-05, 'batch_size': 109, 'step_size': 2, 'gamma': 0.9597442440907851}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:52:07,395][0m Trial 28 finished with value: 0.4112988895941578 and parameters: {'observation_period_num': 137, 'train_rates': 0.87274443333524, 'learning_rate': 2.1602768446974734e-05, 'batch_size': 67, 'step_size': 2, 'gamma': 0.9223132723022694}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 07:56:04,034][0m Trial 29 finished with value: 0.35125401257311256 and parameters: {'observation_period_num': 174, 'train_rates': 0.814691832903109, 'learning_rate': 5.788424908387622e-05, 'batch_size': 42, 'step_size': 6, 'gamma': 0.933491533178952}. Best is trial 22 with value: 0.12550456574984958.[0m
[32m[I 2025-01-04 08:01:00,042][0m Trial 30 finished with value: 0.11589787524938583 and parameters: {'observation_period_num': 191, 'train_rates': 0.9549212955605125, 'learning_rate': 3.809845885757425e-05, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9559097068499188}. Best is trial 30 with value: 0.11589787524938583.[0m
[32m[I 2025-01-04 08:05:54,354][0m Trial 31 finished with value: 0.11290356631462391 and parameters: {'observation_period_num': 191, 'train_rates': 0.9579627345104076, 'learning_rate': 4.628294811814955e-05, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9617107496926638}. Best is trial 31 with value: 0.11290356631462391.[0m
[32m[I 2025-01-04 08:11:01,187][0m Trial 32 finished with value: 0.17362911012910662 and parameters: {'observation_period_num': 193, 'train_rates': 0.9620467723861033, 'learning_rate': 7.417128731114707e-06, 'batch_size': 34, 'step_size': 4, 'gamma': 0.9554444449611399}. Best is trial 31 with value: 0.11290356631462391.[0m
[32m[I 2025-01-04 08:17:18,997][0m Trial 33 finished with value: 0.11181650310754776 and parameters: {'observation_period_num': 227, 'train_rates': 0.9896992260841736, 'learning_rate': 3.945325673354463e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.9647134759283482}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:23:40,615][0m Trial 34 finished with value: 0.11654647232757674 and parameters: {'observation_period_num': 230, 'train_rates': 0.9870495065923905, 'learning_rate': 2.4979736725395312e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.989322665303394}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:27:48,711][0m Trial 35 finished with value: 0.7338087021873578 and parameters: {'observation_period_num': 199, 'train_rates': 0.6733388933250005, 'learning_rate': 2.0791048302522614e-05, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9685546826262411}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:33:49,356][0m Trial 36 finished with value: 0.13046909868717194 and parameters: {'observation_period_num': 221, 'train_rates': 0.9860007714367919, 'learning_rate': 3.813433165413512e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9827866647835638}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:36:45,852][0m Trial 37 finished with value: 0.13457387693701905 and parameters: {'observation_period_num': 112, 'train_rates': 0.9670139927220909, 'learning_rate': 7.837417567088162e-06, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9867103431431714}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:38:09,038][0m Trial 38 finished with value: 0.1531429963727151 and parameters: {'observation_period_num': 50, 'train_rates': 0.9145666151095969, 'learning_rate': 1.6666515446579217e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9620705673070016}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:41:48,539][0m Trial 39 finished with value: 0.14566677127009123 and parameters: {'observation_period_num': 152, 'train_rates': 0.9454641542514128, 'learning_rate': 4.294741412508611e-05, 'batch_size': 83, 'step_size': 5, 'gamma': 0.9682906998683543}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:46:31,792][0m Trial 40 finished with value: 0.1624363213777542 and parameters: {'observation_period_num': 187, 'train_rates': 0.9891453463250162, 'learning_rate': 2.7736683306653915e-05, 'batch_size': 206, 'step_size': 7, 'gamma': 0.9897242341668547}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:52:25,185][0m Trial 41 finished with value: 0.16686362933878804 and parameters: {'observation_period_num': 231, 'train_rates': 0.9252222810940859, 'learning_rate': 2.64119469012545e-05, 'batch_size': 55, 'step_size': 5, 'gamma': 0.9501968663460398}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 08:58:39,665][0m Trial 42 finished with value: 0.13164815406004587 and parameters: {'observation_period_num': 229, 'train_rates': 0.9451694181817639, 'learning_rate': 0.00011709424017173108, 'batch_size': 26, 'step_size': 4, 'gamma': 0.9770542142094711}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 09:04:53,088][0m Trial 43 finished with value: 0.1239633506282847 and parameters: {'observation_period_num': 239, 'train_rates': 0.9307698706418571, 'learning_rate': 4.0534300855451786e-05, 'batch_size': 45, 'step_size': 6, 'gamma': 0.9720367759108577}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 09:11:41,056][0m Trial 44 finished with value: 0.11497030446403905 and parameters: {'observation_period_num': 250, 'train_rates': 0.9721354615554337, 'learning_rate': 3.6416261890142844e-05, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9737720901024949}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 09:18:14,653][0m Trial 45 finished with value: 0.17851423260606367 and parameters: {'observation_period_num': 248, 'train_rates': 0.9701472974172538, 'learning_rate': 1.193745471723013e-05, 'batch_size': 77, 'step_size': 8, 'gamma': 0.9559895025899043}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 09:23:33,477][0m Trial 46 finished with value: 0.12188121676445007 and parameters: {'observation_period_num': 207, 'train_rates': 0.9704288110462561, 'learning_rate': 7.734984678101612e-05, 'batch_size': 91, 'step_size': 5, 'gamma': 0.9758310530994426}. Best is trial 33 with value: 0.11181650310754776.[0m
[32m[I 2025-01-04 09:30:30,580][0m Trial 47 finished with value: 0.10033188735024404 and parameters: {'observation_period_num': 252, 'train_rates': 0.9565631974326987, 'learning_rate': 4.7662824173442816e-05, 'batch_size': 24, 'step_size': 4, 'gamma': 0.92851798592133}. Best is trial 47 with value: 0.10033188735024404.[0m
[32m[I 2025-01-04 09:37:02,228][0m Trial 48 finished with value: 0.24032052531154877 and parameters: {'observation_period_num': 249, 'train_rates': 0.8793123978199459, 'learning_rate': 4.202325793741299e-05, 'batch_size': 24, 'step_size': 4, 'gamma': 0.9256795456966367}. Best is trial 47 with value: 0.10033188735024404.[0m
[32m[I 2025-01-04 09:43:33,913][0m Trial 49 finished with value: 0.19931855046395527 and parameters: {'observation_period_num': 252, 'train_rates': 0.9126449142447628, 'learning_rate': 1.5885448482248803e-05, 'batch_size': 42, 'step_size': 5, 'gamma': 0.9508722829500205}. Best is trial 47 with value: 0.10033188735024404.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 09:43:33,920][0m A new study created in memory with name: no-name-4dd2432c-ba35-40b7-957f-19ea3d2a3db7[0m
[32m[I 2025-01-04 09:46:10,219][0m Trial 0 finished with value: 1.1237882343510042 and parameters: {'observation_period_num': 125, 'train_rates': 0.7546700416041169, 'learning_rate': 3.0942833709218046e-06, 'batch_size': 43, 'step_size': 12, 'gamma': 0.7574260685988737}. Best is trial 0 with value: 1.1237882343510042.[0m
[32m[I 2025-01-04 09:46:52,656][0m Trial 1 finished with value: 0.915803344935587 and parameters: {'observation_period_num': 33, 'train_rates': 0.7500000583596408, 'learning_rate': 6.416363249910208e-06, 'batch_size': 124, 'step_size': 13, 'gamma': 0.8131765026612869}. Best is trial 1 with value: 0.915803344935587.[0m
[32m[I 2025-01-04 09:47:27,891][0m Trial 2 finished with value: 0.7406121723792132 and parameters: {'observation_period_num': 25, 'train_rates': 0.7967956611568103, 'learning_rate': 1.3662763342347e-05, 'batch_size': 164, 'step_size': 5, 'gamma': 0.830802107240277}. Best is trial 2 with value: 0.7406121723792132.[0m
[32m[I 2025-01-04 09:49:50,857][0m Trial 3 finished with value: 0.3763924241065979 and parameters: {'observation_period_num': 104, 'train_rates': 0.9674832687200972, 'learning_rate': 1.903147909188681e-05, 'batch_size': 113, 'step_size': 4, 'gamma': 0.7799483945718433}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 09:54:49,116][0m Trial 4 finished with value: 0.6886185931286677 and parameters: {'observation_period_num': 220, 'train_rates': 0.779949302895075, 'learning_rate': 2.8248578820248006e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8613475040909079}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 09:57:15,888][0m Trial 5 finished with value: 1.8462907311631673 and parameters: {'observation_period_num': 121, 'train_rates': 0.8109163585125816, 'learning_rate': 1.0001064129816527e-06, 'batch_size': 142, 'step_size': 4, 'gamma': 0.8365230556461112}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 10:02:36,757][0m Trial 6 finished with value: 0.6781424244244894 and parameters: {'observation_period_num': 221, 'train_rates': 0.8737693814729859, 'learning_rate': 6.607109060894925e-06, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8559086732506648}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 10:04:55,344][0m Trial 7 finished with value: 1.1804585797258353 and parameters: {'observation_period_num': 111, 'train_rates': 0.6807500573578684, 'learning_rate': 0.0006184298725792711, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9333445302407917}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 10:09:43,131][0m Trial 8 finished with value: 0.4410807058902074 and parameters: {'observation_period_num': 216, 'train_rates': 0.7939651530020572, 'learning_rate': 7.206314472769816e-05, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8367010331408646}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 10:15:19,828][0m Trial 9 finished with value: 0.9677605168158266 and parameters: {'observation_period_num': 246, 'train_rates': 0.7803973503969688, 'learning_rate': 0.0006076383232908878, 'batch_size': 93, 'step_size': 12, 'gamma': 0.919364391590811}. Best is trial 3 with value: 0.3763924241065979.[0m
[32m[I 2025-01-04 10:16:55,710][0m Trial 10 finished with value: 0.15363778173923492 and parameters: {'observation_period_num': 69, 'train_rates': 0.9887707903494749, 'learning_rate': 8.258133661953247e-05, 'batch_size': 196, 'step_size': 9, 'gamma': 0.7573819589818382}. Best is trial 10 with value: 0.15363778173923492.[0m
[32m[I 2025-01-04 10:18:37,929][0m Trial 11 finished with value: 0.1435157209634781 and parameters: {'observation_period_num': 74, 'train_rates': 0.9870557551147232, 'learning_rate': 0.00013896381790034026, 'batch_size': 198, 'step_size': 9, 'gamma': 0.7507095925157466}. Best is trial 11 with value: 0.1435157209634781.[0m
[32m[I 2025-01-04 10:20:08,102][0m Trial 12 finished with value: 0.1294867992401123 and parameters: {'observation_period_num': 67, 'train_rates': 0.9518471486995709, 'learning_rate': 0.00011389499863620138, 'batch_size': 203, 'step_size': 9, 'gamma': 0.9891926428440633}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:21:26,342][0m Trial 13 finished with value: 0.155200956804067 and parameters: {'observation_period_num': 64, 'train_rates': 0.9145244870026833, 'learning_rate': 0.00021842074049114855, 'batch_size': 205, 'step_size': 9, 'gamma': 0.9872011458885038}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:25:24,685][0m Trial 14 finished with value: 0.1372509884034715 and parameters: {'observation_period_num': 169, 'train_rates': 0.911830951296457, 'learning_rate': 0.00014151120699168487, 'batch_size': 237, 'step_size': 10, 'gamma': 0.9138503418275379}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:29:14,085][0m Trial 15 finished with value: 0.18900528836129893 and parameters: {'observation_period_num': 167, 'train_rates': 0.8936640885649001, 'learning_rate': 0.0002833028455275697, 'batch_size': 235, 'step_size': 11, 'gamma': 0.9881453721321718}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:33:06,790][0m Trial 16 finished with value: 0.15642525255680084 and parameters: {'observation_period_num': 164, 'train_rates': 0.9223537896343837, 'learning_rate': 5.8225585388398196e-05, 'batch_size': 227, 'step_size': 15, 'gamma': 0.9078166489266878}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:36:38,861][0m Trial 17 finished with value: 0.32939160532421535 and parameters: {'observation_period_num': 161, 'train_rates': 0.8453877226578943, 'learning_rate': 0.0002774915380104184, 'batch_size': 168, 'step_size': 6, 'gamma': 0.9482681949313204}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:39:17,730][0m Trial 18 finished with value: 1.0482959270477294 and parameters: {'observation_period_num': 152, 'train_rates': 0.6002113786716955, 'learning_rate': 0.0008400699787799453, 'batch_size': 244, 'step_size': 10, 'gamma': 0.8904530710464011}. Best is trial 12 with value: 0.1294867992401123.[0m
[32m[I 2025-01-04 10:39:43,457][0m Trial 19 finished with value: 0.12467736750841141 and parameters: {'observation_period_num': 10, 'train_rates': 0.9460096747377875, 'learning_rate': 4.30136519272078e-05, 'batch_size': 168, 'step_size': 15, 'gamma': 0.9619327527359277}. Best is trial 19 with value: 0.12467736750841141.[0m
[32m[I 2025-01-04 10:40:09,057][0m Trial 20 finished with value: 0.11615462601184845 and parameters: {'observation_period_num': 5, 'train_rates': 0.9561201449585606, 'learning_rate': 3.679668396062118e-05, 'batch_size': 163, 'step_size': 15, 'gamma': 0.9615182188556523}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:40:35,565][0m Trial 21 finished with value: 0.11768914759159088 and parameters: {'observation_period_num': 8, 'train_rates': 0.9490686843401096, 'learning_rate': 4.343426437734661e-05, 'batch_size': 164, 'step_size': 15, 'gamma': 0.9631803261649744}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:41:02,351][0m Trial 22 finished with value: 0.12234623730182648 and parameters: {'observation_period_num': 7, 'train_rates': 0.9485245608203295, 'learning_rate': 4.1154858017545007e-05, 'batch_size': 156, 'step_size': 15, 'gamma': 0.9558942846411308}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:41:53,377][0m Trial 23 finished with value: 0.25688454068430494 and parameters: {'observation_period_num': 40, 'train_rates': 0.8699039206439483, 'learning_rate': 2.8181757549599436e-05, 'batch_size': 145, 'step_size': 14, 'gamma': 0.9611615894569833}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:42:17,833][0m Trial 24 finished with value: 0.21936915814876556 and parameters: {'observation_period_num': 12, 'train_rates': 0.9409754738328333, 'learning_rate': 1.1919286288547226e-05, 'batch_size': 178, 'step_size': 14, 'gamma': 0.9466019467930451}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:43:15,791][0m Trial 25 finished with value: 0.2790966320848256 and parameters: {'observation_period_num': 47, 'train_rates': 0.8431221307341021, 'learning_rate': 3.55811001234433e-05, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8869690577294662}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:43:47,178][0m Trial 26 finished with value: 0.19649031573146053 and parameters: {'observation_period_num': 7, 'train_rates': 0.8851951671817985, 'learning_rate': 1.8762476967422126e-05, 'batch_size': 127, 'step_size': 15, 'gamma': 0.9673708020347271}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:45:54,457][0m Trial 27 finished with value: 0.15755079686641693 and parameters: {'observation_period_num': 93, 'train_rates': 0.9624974078188381, 'learning_rate': 4.7231353324348446e-05, 'batch_size': 183, 'step_size': 13, 'gamma': 0.9380772732767142}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:46:57,650][0m Trial 28 finished with value: 0.5116611102125147 and parameters: {'observation_period_num': 53, 'train_rates': 0.8432854522367327, 'learning_rate': 7.804675770436701e-06, 'batch_size': 218, 'step_size': 14, 'gamma': 0.9722219971612618}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:47:59,897][0m Trial 29 finished with value: 0.36085610508918764 and parameters: {'observation_period_num': 24, 'train_rates': 0.9318366966513444, 'learning_rate': 2.937514347124095e-06, 'batch_size': 64, 'step_size': 12, 'gamma': 0.92851661002527}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:48:32,452][0m Trial 30 finished with value: 1.1231619816989147 and parameters: {'observation_period_num': 25, 'train_rates': 0.7227736991618772, 'learning_rate': 3.4072629689552045e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8974991622870853}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:48:57,484][0m Trial 31 finished with value: 0.12751750648021698 and parameters: {'observation_period_num': 10, 'train_rates': 0.951640242204741, 'learning_rate': 3.741209437528468e-05, 'batch_size': 180, 'step_size': 15, 'gamma': 0.9577748630257488}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:49:31,319][0m Trial 32 finished with value: 0.14586470704460808 and parameters: {'observation_period_num': 6, 'train_rates': 0.9027091531370074, 'learning_rate': 2.1704795964725714e-05, 'batch_size': 132, 'step_size': 15, 'gamma': 0.9704600145527422}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:50:22,449][0m Trial 33 finished with value: 0.13112081587314606 and parameters: {'observation_period_num': 34, 'train_rates': 0.9702997649515145, 'learning_rate': 5.385987461909421e-05, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9506898381847171}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:50:53,326][0m Trial 34 finished with value: 0.19765947473810075 and parameters: {'observation_period_num': 18, 'train_rates': 0.9359713790322022, 'learning_rate': 1.2423829323442157e-05, 'batch_size': 166, 'step_size': 13, 'gamma': 0.9755799362378041}. Best is trial 20 with value: 0.11615462601184845.[0m
[32m[I 2025-01-04 10:52:00,359][0m Trial 35 finished with value: 0.11612288653850555 and parameters: {'observation_period_num': 49, 'train_rates': 0.9750419909825236, 'learning_rate': 9.356891387175736e-05, 'batch_size': 154, 'step_size': 15, 'gamma': 0.8023825053498185}. Best is trial 35 with value: 0.11612288653850555.[0m
[32m[I 2025-01-04 10:54:01,835][0m Trial 36 finished with value: 0.12026216834783554 and parameters: {'observation_period_num': 87, 'train_rates': 0.9753545420674911, 'learning_rate': 9.395236888640831e-05, 'batch_size': 103, 'step_size': 12, 'gamma': 0.7838448207235117}. Best is trial 35 with value: 0.11612288653850555.[0m
[32m[I 2025-01-04 10:56:00,870][0m Trial 37 finished with value: 0.11775832623243332 and parameters: {'observation_period_num': 84, 'train_rates': 0.975272182083037, 'learning_rate': 9.725914086869731e-05, 'batch_size': 94, 'step_size': 12, 'gamma': 0.7845862313886278}. Best is trial 35 with value: 0.11612288653850555.[0m
[32m[I 2025-01-04 10:57:16,651][0m Trial 38 finished with value: 0.1353175789117813 and parameters: {'observation_period_num': 55, 'train_rates': 0.9716200228560367, 'learning_rate': 0.00021228617546003618, 'batch_size': 116, 'step_size': 14, 'gamma': 0.7942463944664448}. Best is trial 35 with value: 0.11612288653850555.[0m
[32m[I 2025-01-04 10:58:15,448][0m Trial 39 finished with value: 0.10620522499084473 and parameters: {'observation_period_num': 37, 'train_rates': 0.9897306169718632, 'learning_rate': 0.0001670058257189411, 'batch_size': 81, 'step_size': 13, 'gamma': 0.7715733323702043}. Best is trial 39 with value: 0.10620522499084473.[0m
[32m[I 2025-01-04 10:59:09,878][0m Trial 40 finished with value: 0.6295884507925399 and parameters: {'observation_period_num': 34, 'train_rates': 0.8676875904479973, 'learning_rate': 0.0004572422909399082, 'batch_size': 74, 'step_size': 13, 'gamma': 0.8100015477040756}. Best is trial 39 with value: 0.10620522499084473.[0m
[32m[I 2025-01-04 11:00:36,961][0m Trial 41 finished with value: 0.10818971314879715 and parameters: {'observation_period_num': 33, 'train_rates': 0.9792116067087864, 'learning_rate': 0.00016831772984369085, 'batch_size': 47, 'step_size': 13, 'gamma': 0.7720953485958332}. Best is trial 39 with value: 0.10620522499084473.[0m
[32m[I 2025-01-04 11:03:42,781][0m Trial 42 finished with value: 0.10236410349607468 and parameters: {'observation_period_num': 47, 'train_rates': 0.9829256666514199, 'learning_rate': 0.0001870377771546829, 'batch_size': 22, 'step_size': 14, 'gamma': 0.7683027913183}. Best is trial 42 with value: 0.10236410349607468.[0m
[32m[I 2025-01-04 11:07:57,646][0m Trial 43 finished with value: 0.0887164767635496 and parameters: {'observation_period_num': 45, 'train_rates': 0.9869646334596911, 'learning_rate': 0.00017110817008673173, 'batch_size': 16, 'step_size': 13, 'gamma': 0.772653922539655}. Best is trial 43 with value: 0.0887164767635496.[0m
[32m[I 2025-01-04 11:12:08,291][0m Trial 44 finished with value: 0.38719452085883116 and parameters: {'observation_period_num': 44, 'train_rates': 0.9852776053923624, 'learning_rate': 0.00040236893793760736, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7684589513320798}. Best is trial 43 with value: 0.0887164767635496.[0m
[32m[I 2025-01-04 11:13:40,492][0m Trial 45 finished with value: 0.1190485021458613 and parameters: {'observation_period_num': 58, 'train_rates': 0.9230633742246022, 'learning_rate': 0.0001538105152348632, 'batch_size': 44, 'step_size': 13, 'gamma': 0.7694143428231188}. Best is trial 43 with value: 0.0887164767635496.[0m
[32m[I 2025-01-04 11:17:06,480][0m Trial 46 finished with value: 0.20333744585514069 and parameters: {'observation_period_num': 133, 'train_rates': 0.9891923350573528, 'learning_rate': 0.0001936117689612525, 'batch_size': 44, 'step_size': 2, 'gamma': 0.8045693277418632}. Best is trial 43 with value: 0.0887164767635496.[0m
[32m[I 2025-01-04 11:19:00,646][0m Trial 47 finished with value: 0.969158024510968 and parameters: {'observation_period_num': 77, 'train_rates': 0.7397341432927191, 'learning_rate': 0.00032469640706866363, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8256193184861298}. Best is trial 43 with value: 0.0887164767635496.[0m
[32m[I 2025-01-04 11:19:55,255][0m Trial 48 finished with value: 0.663709667206712 and parameters: {'observation_period_num': 29, 'train_rates': 0.6381149136859652, 'learning_rate': 7.633036007127492e-05, 'batch_size': 56, 'step_size': 14, 'gamma': 0.771708923161931}. Best is trial 43 with value: 0.0887164767635496.[0m
[32m[I 2025-01-04 11:23:30,969][0m Trial 49 finished with value: 0.13074582033228166 and parameters: {'observation_period_num': 101, 'train_rates': 0.9648789021555515, 'learning_rate': 0.00017213583229883566, 'batch_size': 18, 'step_size': 8, 'gamma': 0.7505812946880929}. Best is trial 43 with value: 0.0887164767635496.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 11:23:30,976][0m A new study created in memory with name: no-name-ecd32884-d4f8-4abf-8367-b4f6910b7983[0m
[32m[I 2025-01-04 11:27:58,893][0m Trial 0 finished with value: 0.36292958900748024 and parameters: {'observation_period_num': 190, 'train_rates': 0.9093031560545568, 'learning_rate': 2.4699426074779857e-05, 'batch_size': 242, 'step_size': 7, 'gamma': 0.8263823046460264}. Best is trial 0 with value: 0.36292958900748024.[0m
[32m[I 2025-01-04 11:31:38,642][0m Trial 1 finished with value: 0.6637962481856061 and parameters: {'observation_period_num': 165, 'train_rates': 0.8496992171491483, 'learning_rate': 1.2028152404253911e-05, 'batch_size': 118, 'step_size': 5, 'gamma': 0.7742679532277326}. Best is trial 0 with value: 0.36292958900748024.[0m
[32m[I 2025-01-04 11:33:15,049][0m Trial 2 finished with value: 1.2872579329072427 and parameters: {'observation_period_num': 86, 'train_rates': 0.7305017164900018, 'learning_rate': 3.367736212365194e-05, 'batch_size': 186, 'step_size': 2, 'gamma': 0.8135403223843296}. Best is trial 0 with value: 0.36292958900748024.[0m
[32m[I 2025-01-04 11:33:34,748][0m Trial 3 finished with value: 0.26587922677253056 and parameters: {'observation_period_num': 11, 'train_rates': 0.8248906942481063, 'learning_rate': 0.00011167664134516706, 'batch_size': 207, 'step_size': 10, 'gamma': 0.8175690394028965}. Best is trial 3 with value: 0.26587922677253056.[0m
[32m[I 2025-01-04 11:34:35,562][0m Trial 4 finished with value: 1.8410063583426681 and parameters: {'observation_period_num': 56, 'train_rates': 0.6622715947286089, 'learning_rate': 1.487901722898048e-06, 'batch_size': 146, 'step_size': 4, 'gamma': 0.8776323827302506}. Best is trial 3 with value: 0.26587922677253056.[0m
Early stopping at epoch 99
[32m[I 2025-01-04 11:36:33,487][0m Trial 5 finished with value: 1.1911080171681954 and parameters: {'observation_period_num': 99, 'train_rates': 0.799996635970427, 'learning_rate': 1.1345356093084551e-05, 'batch_size': 176, 'step_size': 2, 'gamma': 0.7944862816425642}. Best is trial 3 with value: 0.26587922677253056.[0m
[32m[I 2025-01-04 11:38:42,345][0m Trial 6 finished with value: 0.6624604076952547 and parameters: {'observation_period_num': 112, 'train_rates': 0.765576904477689, 'learning_rate': 0.0002334171350615565, 'batch_size': 168, 'step_size': 4, 'gamma': 0.9588239390393912}. Best is trial 3 with value: 0.26587922677253056.[0m
[32m[I 2025-01-04 11:39:15,591][0m Trial 7 finished with value: 0.7900896507385381 and parameters: {'observation_period_num': 21, 'train_rates': 0.6326610952844066, 'learning_rate': 9.972634218611335e-05, 'batch_size': 96, 'step_size': 12, 'gamma': 0.7690051729087846}. Best is trial 3 with value: 0.26587922677253056.[0m
[32m[I 2025-01-04 11:42:02,752][0m Trial 8 finished with value: 0.8773862042550576 and parameters: {'observation_period_num': 132, 'train_rates': 0.7740612219470725, 'learning_rate': 0.00036361955644461806, 'batch_size': 73, 'step_size': 8, 'gamma': 0.8760854533710534}. Best is trial 3 with value: 0.26587922677253056.[0m
[32m[I 2025-01-04 11:45:27,844][0m Trial 9 finished with value: 0.34372590937076775 and parameters: {'observation_period_num': 156, 'train_rates': 0.8429883641168476, 'learning_rate': 0.0005199160598159351, 'batch_size': 153, 'step_size': 5, 'gamma': 0.8073236484378603}. Best is trial 3 with value: 0.26587922677253056.[0m
[32m[I 2025-01-04 11:52:48,997][0m Trial 10 finished with value: 0.12546798416546412 and parameters: {'observation_period_num': 248, 'train_rates': 0.9871769151939891, 'learning_rate': 9.019002895400029e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9399014499134971}. Best is trial 10 with value: 0.12546798416546412.[0m
[32m[I 2025-01-04 11:54:55,137][0m Trial 11 finished with value: 0.08544641569620226 and parameters: {'observation_period_num': 6, 'train_rates': 0.9861731892976261, 'learning_rate': 8.543098257861018e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9517939292934263}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:01:33,746][0m Trial 12 finished with value: 0.10978066618554294 and parameters: {'observation_period_num': 226, 'train_rates': 0.9883819568278591, 'learning_rate': 5.5150556191624785e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9630470587897945}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:08:40,759][0m Trial 13 finished with value: 0.16220768150829135 and parameters: {'observation_period_num': 250, 'train_rates': 0.9846848084205895, 'learning_rate': 4.436925611458736e-06, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9847667683651601}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:13:49,112][0m Trial 14 finished with value: 1.9331223141063343 and parameters: {'observation_period_num': 205, 'train_rates': 0.9202270285627688, 'learning_rate': 0.0008726066441329239, 'batch_size': 56, 'step_size': 12, 'gamma': 0.9186585899186566}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:15:12,008][0m Trial 15 finished with value: 0.15973226847697278 and parameters: {'observation_period_num': 55, 'train_rates': 0.932342588072517, 'learning_rate': 4.51033831313416e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.913148048516122}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:20:28,329][0m Trial 16 finished with value: 0.2929550907683612 and parameters: {'observation_period_num': 218, 'train_rates': 0.8906925094435109, 'learning_rate': 0.00019083866046415193, 'batch_size': 94, 'step_size': 10, 'gamma': 0.9898573374299433}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:22:12,337][0m Trial 17 finished with value: 0.142413829267025 and parameters: {'observation_period_num': 63, 'train_rates': 0.9567426283968239, 'learning_rate': 1.324241814932032e-05, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9539305476973697}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:25:36,944][0m Trial 18 finished with value: 0.2625466791169891 and parameters: {'observation_period_num': 152, 'train_rates': 0.8695125612011315, 'learning_rate': 5.2153540171117803e-05, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9056942128649779}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:30:17,006][0m Trial 19 finished with value: 0.5635551109568764 and parameters: {'observation_period_num': 189, 'train_rates': 0.9527388382606445, 'learning_rate': 2.79709783420276e-06, 'batch_size': 119, 'step_size': 12, 'gamma': 0.8512107437265459}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:31:42,868][0m Trial 20 finished with value: 0.6502417484425926 and parameters: {'observation_period_num': 29, 'train_rates': 0.7002967953801074, 'learning_rate': 2.5618097187084614e-05, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9680014793564575}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:39:01,914][0m Trial 21 finished with value: 0.13634307836664133 and parameters: {'observation_period_num': 246, 'train_rates': 0.989451221601471, 'learning_rate': 0.00010347164481311889, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9370276756782875}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:45:31,704][0m Trial 22 finished with value: 0.15905939228832722 and parameters: {'observation_period_num': 229, 'train_rates': 0.960554806313838, 'learning_rate': 6.662625437025086e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9351109265303228}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:51:37,229][0m Trial 23 finished with value: 0.1428326666355133 and parameters: {'observation_period_num': 228, 'train_rates': 0.9873263350709738, 'learning_rate': 0.00017915142980273445, 'batch_size': 66, 'step_size': 13, 'gamma': 0.89221172816338}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 12:56:18,037][0m Trial 24 finished with value: 0.2716748394862632 and parameters: {'observation_period_num': 191, 'train_rates': 0.8873815659141429, 'learning_rate': 7.404883808654376e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.937891995087067}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:00:35,230][0m Trial 25 finished with value: 0.1470354038476944 and parameters: {'observation_period_num': 170, 'train_rates': 0.9463974273637982, 'learning_rate': 2.1363953988957887e-05, 'batch_size': 38, 'step_size': 13, 'gamma': 0.9705913318768573}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:06:29,942][0m Trial 26 finished with value: 0.394602382183075 and parameters: {'observation_period_num': 213, 'train_rates': 0.9087712268247583, 'learning_rate': 0.0003575750182678822, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9470262715516308}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:09:45,432][0m Trial 27 finished with value: 0.26616818328266556 and parameters: {'observation_period_num': 133, 'train_rates': 0.9674426959913718, 'learning_rate': 5.842807099847449e-06, 'batch_size': 77, 'step_size': 15, 'gamma': 0.9254384871977455}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:16:16,145][0m Trial 28 finished with value: 0.19066368743200987 and parameters: {'observation_period_num': 252, 'train_rates': 0.9330605125119885, 'learning_rate': 0.00016277515368505826, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8937534007668707}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:20:39,193][0m Trial 29 finished with value: 0.28274407960333914 and parameters: {'observation_period_num': 186, 'train_rates': 0.9044600887493071, 'learning_rate': 3.999914361003239e-05, 'batch_size': 243, 'step_size': 8, 'gamma': 0.8514270407997165}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:26:18,984][0m Trial 30 finished with value: 0.28899009517357177 and parameters: {'observation_period_num': 236, 'train_rates': 0.8697828980663213, 'learning_rate': 1.9313089437462956e-05, 'batch_size': 112, 'step_size': 14, 'gamma': 0.9746812188797153}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:33:08,045][0m Trial 31 finished with value: 0.10907194231237684 and parameters: {'observation_period_num': 242, 'train_rates': 0.9845988292949095, 'learning_rate': 0.00010640904887375956, 'batch_size': 30, 'step_size': 15, 'gamma': 0.941095068551}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:38:34,141][0m Trial 32 finished with value: 0.15200118720531464 and parameters: {'observation_period_num': 204, 'train_rates': 0.9694246552150312, 'learning_rate': 8.94838284310354e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9541652355932209}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:44:34,564][0m Trial 33 finished with value: 0.15260598063468933 and parameters: {'observation_period_num': 236, 'train_rates': 0.932645263104953, 'learning_rate': 0.00013106971367506473, 'batch_size': 217, 'step_size': 13, 'gamma': 0.9280441497895067}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:50:23,868][0m Trial 34 finished with value: 0.12330732494592667 and parameters: {'observation_period_num': 217, 'train_rates': 0.9882483909710486, 'learning_rate': 5.4613442774081716e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.898204678667163}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:52:20,138][0m Trial 35 finished with value: 0.1653799362204693 and parameters: {'observation_period_num': 77, 'train_rates': 0.9437444772477396, 'learning_rate': 3.1569525892891756e-05, 'batch_size': 50, 'step_size': 14, 'gamma': 0.964286144328372}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 13:58:06,552][0m Trial 36 finished with value: 0.13094750344753264 and parameters: {'observation_period_num': 220, 'train_rates': 0.9728005571287172, 'learning_rate': 6.19042319580665e-05, 'batch_size': 67, 'step_size': 13, 'gamma': 0.897935655102492}. Best is trial 11 with value: 0.08544641569620226.[0m
Early stopping at epoch 95
[32m[I 2025-01-04 14:02:26,637][0m Trial 37 finished with value: 0.30942291165763586 and parameters: {'observation_period_num': 178, 'train_rates': 0.9181670461695861, 'learning_rate': 0.00029427508854026863, 'batch_size': 30, 'step_size': 1, 'gamma': 0.8588612554434613}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:02:53,165][0m Trial 38 finished with value: 1.022592511091623 and parameters: {'observation_period_num': 6, 'train_rates': 0.6023447303643259, 'learning_rate': 3.6052723928072144e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9111292095139203}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:05:16,396][0m Trial 39 finished with value: 0.36743736243390185 and parameters: {'observation_period_num': 111, 'train_rates': 0.8230820995820265, 'learning_rate': 1.1472343623791554e-05, 'batch_size': 51, 'step_size': 6, 'gamma': 0.9785780635127785}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:09:25,538][0m Trial 40 finished with value: 0.7568382923833785 and parameters: {'observation_period_num': 202, 'train_rates': 0.7182836339188203, 'learning_rate': 0.00012860047327204946, 'batch_size': 128, 'step_size': 9, 'gamma': 0.8658410709291658}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:15:42,258][0m Trial 41 finished with value: 0.15117965324316174 and parameters: {'observation_period_num': 229, 'train_rates': 0.9765131788870591, 'learning_rate': 7.364876498159864e-05, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9445445323490148}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:22:10,093][0m Trial 42 finished with value: 0.11910570412874222 and parameters: {'observation_period_num': 239, 'train_rates': 0.9890509529349609, 'learning_rate': 4.9931425104304894e-05, 'batch_size': 44, 'step_size': 14, 'gamma': 0.8827705846669182}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:27:46,605][0m Trial 43 finished with value: 0.11481118397343726 and parameters: {'observation_period_num': 215, 'train_rates': 0.9542310644453246, 'learning_rate': 5.033007530683237e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.8823676106114611}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:33:56,536][0m Trial 44 finished with value: 0.15604276524102392 and parameters: {'observation_period_num': 239, 'train_rates': 0.9556649079791101, 'learning_rate': 2.9125948071855746e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8823678757717758}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:37:36,825][0m Trial 45 finished with value: 0.2148591341291155 and parameters: {'observation_period_num': 151, 'train_rates': 0.9376492302602029, 'learning_rate': 1.6718852116030607e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7876524648323107}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:42:56,266][0m Trial 46 finished with value: 0.19059839255588 and parameters: {'observation_period_num': 200, 'train_rates': 0.9688955286798102, 'learning_rate': 0.00024668870439575636, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8337136434599769}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:43:48,603][0m Trial 47 finished with value: 0.4370186425681807 and parameters: {'observation_period_num': 39, 'train_rates': 0.9199785705576402, 'learning_rate': 7.77062531094215e-06, 'batch_size': 200, 'step_size': 15, 'gamma': 0.8247375836489466}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:45:51,680][0m Trial 48 finished with value: 0.3548911239099353 and parameters: {'observation_period_num': 94, 'train_rates': 0.8886295933054175, 'learning_rate': 4.3380695697373565e-05, 'batch_size': 94, 'step_size': 3, 'gamma': 0.7521513414211018}. Best is trial 11 with value: 0.08544641569620226.[0m
[32m[I 2025-01-04 14:52:01,474][0m Trial 49 finished with value: 0.12242539972066879 and parameters: {'observation_period_num': 240, 'train_rates': 0.9534439833732086, 'learning_rate': 0.00013955473568963688, 'batch_size': 150, 'step_size': 12, 'gamma': 0.8827880324980351}. Best is trial 11 with value: 0.08544641569620226.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 13, 'train_rates': 0.9753175281497932, 'learning_rate': 0.00013431963840526654, 'batch_size': 166, 'step_size': 11, 'gamma': 0.9224923586269907}
Epoch 1/300, trend Loss: 0.9626 | 0.9467
Epoch 2/300, trend Loss: 0.6524 | 0.6189
Epoch 3/300, trend Loss: 0.5284 | 0.5424
Epoch 4/300, trend Loss: 0.4456 | 0.4395
Epoch 5/300, trend Loss: 0.4115 | 0.4231
Epoch 6/300, trend Loss: 0.3816 | 0.3994
Epoch 7/300, trend Loss: 0.4256 | 0.3689
Epoch 8/300, trend Loss: 0.3795 | 0.3234
Epoch 9/300, trend Loss: 0.5115 | 0.4274
Epoch 10/300, trend Loss: 0.3619 | 0.3330
Epoch 11/300, trend Loss: 0.2992 | 0.3163
Epoch 12/300, trend Loss: 0.2713 | 0.2805
Epoch 13/300, trend Loss: 0.2821 | 0.2619
Epoch 14/300, trend Loss: 0.2515 | 0.2541
Epoch 15/300, trend Loss: 0.2629 | 0.2275
Epoch 16/300, trend Loss: 0.2342 | 0.2224
Epoch 17/300, trend Loss: 0.2349 | 0.2187
Epoch 18/300, trend Loss: 0.2217 | 0.2224
Epoch 19/300, trend Loss: 0.2196 | 0.2104
Epoch 20/300, trend Loss: 0.2500 | 0.2082
Epoch 21/300, trend Loss: 0.2438 | 0.2027
Epoch 22/300, trend Loss: 0.2492 | 0.1994
Epoch 23/300, trend Loss: 0.1986 | 0.2059
Epoch 24/300, trend Loss: 0.1813 | 0.1764
Epoch 25/300, trend Loss: 0.1747 | 0.1754
Epoch 26/300, trend Loss: 0.1685 | 0.1664
Epoch 27/300, trend Loss: 0.1672 | 0.1652
Epoch 28/300, trend Loss: 0.1654 | 0.1594
Epoch 29/300, trend Loss: 0.1649 | 0.1580
Epoch 30/300, trend Loss: 0.1635 | 0.1534
Epoch 31/300, trend Loss: 0.1629 | 0.1517
Epoch 32/300, trend Loss: 0.1745 | 0.1464
Epoch 33/300, trend Loss: 0.1714 | 0.1475
Epoch 34/300, trend Loss: 0.1848 | 0.1446
Epoch 35/300, trend Loss: 0.1675 | 0.1632
Epoch 36/300, trend Loss: 0.1691 | 0.1414
Epoch 37/300, trend Loss: 0.1725 | 0.1514
Epoch 38/300, trend Loss: 0.1631 | 0.1411
Epoch 39/300, trend Loss: 0.1685 | 0.1432
Epoch 40/300, trend Loss: 0.1589 | 0.1360
Epoch 41/300, trend Loss: 0.1552 | 0.1349
Epoch 42/300, trend Loss: 0.1502 | 0.1334
Epoch 43/300, trend Loss: 0.1503 | 0.1283
Epoch 44/300, trend Loss: 0.1459 | 0.1271
Epoch 45/300, trend Loss: 0.1425 | 0.1249
Epoch 46/300, trend Loss: 0.1414 | 0.1208
Epoch 47/300, trend Loss: 0.1378 | 0.1220
Epoch 48/300, trend Loss: 0.1384 | 0.1169
Epoch 49/300, trend Loss: 0.1406 | 0.1192
Epoch 50/300, trend Loss: 0.1435 | 0.1153
Epoch 51/300, trend Loss: 0.1528 | 0.1130
Epoch 52/300, trend Loss: 0.1445 | 0.1170
Epoch 53/300, trend Loss: 0.1443 | 0.1089
Epoch 54/300, trend Loss: 0.1378 | 0.1199
Epoch 55/300, trend Loss: 0.1362 | 0.1076
Epoch 56/300, trend Loss: 0.1337 | 0.1176
Epoch 57/300, trend Loss: 0.1309 | 0.1075
Epoch 58/300, trend Loss: 0.1295 | 0.1126
Epoch 59/300, trend Loss: 0.1280 | 0.1068
Epoch 60/300, trend Loss: 0.1276 | 0.1076
Epoch 61/300, trend Loss: 0.1261 | 0.1034
Epoch 62/300, trend Loss: 0.1260 | 0.1044
Epoch 63/300, trend Loss: 0.1244 | 0.1008
Epoch 64/300, trend Loss: 0.1222 | 0.1002
Epoch 65/300, trend Loss: 0.1211 | 0.0994
Epoch 66/300, trend Loss: 0.1196 | 0.0967
Epoch 67/300, trend Loss: 0.1181 | 0.0959
Epoch 68/300, trend Loss: 0.1158 | 0.0956
Epoch 69/300, trend Loss: 0.1153 | 0.0934
Epoch 70/300, trend Loss: 0.1143 | 0.0939
Epoch 71/300, trend Loss: 0.1149 | 0.0940
Epoch 72/300, trend Loss: 0.1161 | 0.0931
Epoch 73/300, trend Loss: 0.1163 | 0.0940
Epoch 74/300, trend Loss: 0.1169 | 0.0921
Epoch 75/300, trend Loss: 0.1153 | 0.0918
Epoch 76/300, trend Loss: 0.1151 | 0.0915
Epoch 77/300, trend Loss: 0.1132 | 0.0893
Epoch 78/300, trend Loss: 0.1123 | 0.0887
Epoch 79/300, trend Loss: 0.1104 | 0.0883
Epoch 80/300, trend Loss: 0.1098 | 0.0857
Epoch 81/300, trend Loss: 0.1106 | 0.0856
Epoch 82/300, trend Loss: 0.1108 | 0.0847
Epoch 83/300, trend Loss: 0.1113 | 0.0850
Epoch 84/300, trend Loss: 0.1099 | 0.0835
Epoch 85/300, trend Loss: 0.1086 | 0.0833
Epoch 86/300, trend Loss: 0.1070 | 0.0824
Epoch 87/300, trend Loss: 0.1063 | 0.0827
Epoch 88/300, trend Loss: 0.1055 | 0.0808
Epoch 89/300, trend Loss: 0.1048 | 0.0801
Epoch 90/300, trend Loss: 0.1035 | 0.0800
Epoch 91/300, trend Loss: 0.1036 | 0.0804
Epoch 92/300, trend Loss: 0.1036 | 0.0791
Epoch 93/300, trend Loss: 0.1036 | 0.0791
Epoch 94/300, trend Loss: 0.1034 | 0.0787
Epoch 95/300, trend Loss: 0.1024 | 0.0783
Epoch 96/300, trend Loss: 0.1008 | 0.0778
Epoch 97/300, trend Loss: 0.1001 | 0.0775
Epoch 98/300, trend Loss: 0.0992 | 0.0769
Epoch 99/300, trend Loss: 0.0989 | 0.0766
Epoch 100/300, trend Loss: 0.0993 | 0.0763
Epoch 101/300, trend Loss: 0.0986 | 0.0761
Epoch 102/300, trend Loss: 0.0986 | 0.0752
Epoch 103/300, trend Loss: 0.0977 | 0.0757
Epoch 104/300, trend Loss: 0.0981 | 0.0748
Epoch 105/300, trend Loss: 0.0975 | 0.0750
Epoch 106/300, trend Loss: 0.0975 | 0.0739
Epoch 107/300, trend Loss: 0.0967 | 0.0737
Epoch 108/300, trend Loss: 0.0959 | 0.0738
Epoch 109/300, trend Loss: 0.0951 | 0.0731
Epoch 110/300, trend Loss: 0.0948 | 0.0739
Epoch 111/300, trend Loss: 0.0947 | 0.0732
Epoch 112/300, trend Loss: 0.0949 | 0.0736
Epoch 113/300, trend Loss: 0.0957 | 0.0723
Epoch 114/300, trend Loss: 0.0942 | 0.0729
Epoch 115/300, trend Loss: 0.0947 | 0.0723
Epoch 116/300, trend Loss: 0.0944 | 0.0724
Epoch 117/300, trend Loss: 0.0948 | 0.0708
Epoch 118/300, trend Loss: 0.0939 | 0.0717
Epoch 119/300, trend Loss: 0.0931 | 0.0708
Epoch 120/300, trend Loss: 0.0928 | 0.0708
Epoch 121/300, trend Loss: 0.0928 | 0.0709
Epoch 122/300, trend Loss: 0.0920 | 0.0704
Epoch 123/300, trend Loss: 0.0920 | 0.0708
Epoch 124/300, trend Loss: 0.0914 | 0.0706
Epoch 125/300, trend Loss: 0.0918 | 0.0700
Epoch 126/300, trend Loss: 0.0910 | 0.0698
Epoch 127/300, trend Loss: 0.0904 | 0.0702
Epoch 128/300, trend Loss: 0.0901 | 0.0694
Epoch 129/300, trend Loss: 0.0904 | 0.0693
Epoch 130/300, trend Loss: 0.0900 | 0.0691
Epoch 131/300, trend Loss: 0.0901 | 0.0687
Epoch 132/300, trend Loss: 0.0894 | 0.0689
Epoch 133/300, trend Loss: 0.0889 | 0.0685
Epoch 134/300, trend Loss: 0.0898 | 0.0683
Epoch 135/300, trend Loss: 0.0892 | 0.0676
Epoch 136/300, trend Loss: 0.0886 | 0.0679
Epoch 137/300, trend Loss: 0.0883 | 0.0686
Epoch 138/300, trend Loss: 0.0885 | 0.0679
Epoch 139/300, trend Loss: 0.0879 | 0.0679
Epoch 140/300, trend Loss: 0.0880 | 0.0678
Epoch 141/300, trend Loss: 0.0881 | 0.0676
Epoch 142/300, trend Loss: 0.0881 | 0.0671
Epoch 143/300, trend Loss: 0.0876 | 0.0672
Epoch 144/300, trend Loss: 0.0873 | 0.0668
Epoch 145/300, trend Loss: 0.0877 | 0.0669
Epoch 146/300, trend Loss: 0.0872 | 0.0662
Epoch 147/300, trend Loss: 0.0866 | 0.0664
Epoch 148/300, trend Loss: 0.0870 | 0.0663
Epoch 149/300, trend Loss: 0.0862 | 0.0663
Epoch 150/300, trend Loss: 0.0863 | 0.0660
Epoch 151/300, trend Loss: 0.0862 | 0.0659
Epoch 152/300, trend Loss: 0.0862 | 0.0661
Epoch 153/300, trend Loss: 0.0859 | 0.0659
Epoch 154/300, trend Loss: 0.0856 | 0.0657
Epoch 155/300, trend Loss: 0.0860 | 0.0657
Epoch 156/300, trend Loss: 0.0854 | 0.0650
Epoch 157/300, trend Loss: 0.0855 | 0.0649
Epoch 158/300, trend Loss: 0.0853 | 0.0653
Epoch 159/300, trend Loss: 0.0858 | 0.0652
Epoch 160/300, trend Loss: 0.0847 | 0.0650
Epoch 161/300, trend Loss: 0.0848 | 0.0647
Epoch 162/300, trend Loss: 0.0851 | 0.0649
Epoch 163/300, trend Loss: 0.0857 | 0.0652
Epoch 164/300, trend Loss: 0.0847 | 0.0645
Epoch 165/300, trend Loss: 0.0841 | 0.0639
Epoch 166/300, trend Loss: 0.0845 | 0.0643
Epoch 167/300, trend Loss: 0.0839 | 0.0642
Epoch 168/300, trend Loss: 0.0841 | 0.0642
Epoch 169/300, trend Loss: 0.0834 | 0.0641
Epoch 170/300, trend Loss: 0.0834 | 0.0643
Epoch 171/300, trend Loss: 0.0835 | 0.0639
Epoch 172/300, trend Loss: 0.0831 | 0.0640
Epoch 173/300, trend Loss: 0.0837 | 0.0638
Epoch 174/300, trend Loss: 0.0829 | 0.0635
Epoch 175/300, trend Loss: 0.0832 | 0.0641
Epoch 176/300, trend Loss: 0.0832 | 0.0637
Epoch 177/300, trend Loss: 0.0830 | 0.0631
Epoch 178/300, trend Loss: 0.0826 | 0.0632
Epoch 179/300, trend Loss: 0.0826 | 0.0632
Epoch 180/300, trend Loss: 0.0827 | 0.0630
Epoch 181/300, trend Loss: 0.0824 | 0.0626
Epoch 182/300, trend Loss: 0.0823 | 0.0629
Epoch 183/300, trend Loss: 0.0827 | 0.0627
Epoch 184/300, trend Loss: 0.0824 | 0.0629
Epoch 185/300, trend Loss: 0.0827 | 0.0630
Epoch 186/300, trend Loss: 0.0825 | 0.0630
Epoch 187/300, trend Loss: 0.0819 | 0.0628
Epoch 188/300, trend Loss: 0.0817 | 0.0624
Epoch 189/300, trend Loss: 0.0814 | 0.0625
Epoch 190/300, trend Loss: 0.0819 | 0.0620
Epoch 191/300, trend Loss: 0.0815 | 0.0623
Epoch 192/300, trend Loss: 0.0812 | 0.0623
Epoch 193/300, trend Loss: 0.0812 | 0.0620
Epoch 194/300, trend Loss: 0.0813 | 0.0621
Epoch 195/300, trend Loss: 0.0812 | 0.0619
Epoch 196/300, trend Loss: 0.0811 | 0.0615
Epoch 197/300, trend Loss: 0.0810 | 0.0620
Epoch 198/300, trend Loss: 0.0811 | 0.0624
Epoch 199/300, trend Loss: 0.0807 | 0.0622
Epoch 200/300, trend Loss: 0.0807 | 0.0614
Epoch 201/300, trend Loss: 0.0811 | 0.0616
Epoch 202/300, trend Loss: 0.0806 | 0.0622
Epoch 203/300, trend Loss: 0.0804 | 0.0618
Epoch 204/300, trend Loss: 0.0798 | 0.0614
Epoch 205/300, trend Loss: 0.0803 | 0.0615
Epoch 206/300, trend Loss: 0.0810 | 0.0612
Epoch 207/300, trend Loss: 0.0802 | 0.0611
Epoch 208/300, trend Loss: 0.0805 | 0.0613
Epoch 209/300, trend Loss: 0.0800 | 0.0619
Epoch 210/300, trend Loss: 0.0799 | 0.0617
Epoch 211/300, trend Loss: 0.0800 | 0.0612
Epoch 212/300, trend Loss: 0.0796 | 0.0613
Epoch 213/300, trend Loss: 0.0799 | 0.0612
Epoch 214/300, trend Loss: 0.0795 | 0.0612
Epoch 215/300, trend Loss: 0.0799 | 0.0609
Epoch 216/300, trend Loss: 0.0793 | 0.0609
Epoch 217/300, trend Loss: 0.0797 | 0.0611
Epoch 218/300, trend Loss: 0.0792 | 0.0611
Epoch 219/300, trend Loss: 0.0797 | 0.0611
Epoch 220/300, trend Loss: 0.0802 | 0.0608
Epoch 221/300, trend Loss: 0.0792 | 0.0608
Epoch 222/300, trend Loss: 0.0787 | 0.0607
Epoch 223/300, trend Loss: 0.0789 | 0.0608
Epoch 224/300, trend Loss: 0.0789 | 0.0606
Epoch 225/300, trend Loss: 0.0788 | 0.0606
Epoch 226/300, trend Loss: 0.0791 | 0.0610
Epoch 227/300, trend Loss: 0.0786 | 0.0610
Epoch 228/300, trend Loss: 0.0789 | 0.0607
Epoch 229/300, trend Loss: 0.0781 | 0.0606
Epoch 230/300, trend Loss: 0.0788 | 0.0607
Epoch 231/300, trend Loss: 0.0786 | 0.0606
Epoch 232/300, trend Loss: 0.0792 | 0.0602
Epoch 233/300, trend Loss: 0.0782 | 0.0606
Epoch 234/300, trend Loss: 0.0789 | 0.0607
Epoch 235/300, trend Loss: 0.0787 | 0.0603
Epoch 236/300, trend Loss: 0.0793 | 0.0601
Epoch 237/300, trend Loss: 0.0781 | 0.0603
Epoch 238/300, trend Loss: 0.0782 | 0.0603
Epoch 239/300, trend Loss: 0.0785 | 0.0601
Epoch 240/300, trend Loss: 0.0785 | 0.0599
Epoch 241/300, trend Loss: 0.0785 | 0.0600
Epoch 242/300, trend Loss: 0.0783 | 0.0601
Epoch 243/300, trend Loss: 0.0778 | 0.0601
Epoch 244/300, trend Loss: 0.0785 | 0.0602
Epoch 245/300, trend Loss: 0.0774 | 0.0599
Epoch 246/300, trend Loss: 0.0780 | 0.0597
Epoch 247/300, trend Loss: 0.0777 | 0.0599
Epoch 248/300, trend Loss: 0.0780 | 0.0597
Epoch 249/300, trend Loss: 0.0778 | 0.0598
Epoch 250/300, trend Loss: 0.0783 | 0.0602
Epoch 251/300, trend Loss: 0.0776 | 0.0599
Epoch 252/300, trend Loss: 0.0777 | 0.0599
Epoch 253/300, trend Loss: 0.0783 | 0.0598
Epoch 254/300, trend Loss: 0.0784 | 0.0596
Epoch 255/300, trend Loss: 0.0779 | 0.0595
Epoch 256/300, trend Loss: 0.0773 | 0.0601
Epoch 257/300, trend Loss: 0.0777 | 0.0600
Epoch 258/300, trend Loss: 0.0779 | 0.0597
Epoch 259/300, trend Loss: 0.0771 | 0.0596
Epoch 260/300, trend Loss: 0.0776 | 0.0597
Epoch 261/300, trend Loss: 0.0772 | 0.0597
Epoch 262/300, trend Loss: 0.0778 | 0.0597
Epoch 263/300, trend Loss: 0.0771 | 0.0598
Epoch 264/300, trend Loss: 0.0771 | 0.0598
Epoch 265/300, trend Loss: 0.0774 | 0.0597
Epoch 266/300, trend Loss: 0.0769 | 0.0596
Epoch 267/300, trend Loss: 0.0770 | 0.0595
Epoch 268/300, trend Loss: 0.0774 | 0.0597
Epoch 269/300, trend Loss: 0.0773 | 0.0596
Epoch 270/300, trend Loss: 0.0770 | 0.0597
Epoch 271/300, trend Loss: 0.0776 | 0.0597
Epoch 272/300, trend Loss: 0.0772 | 0.0597
Epoch 273/300, trend Loss: 0.0770 | 0.0597
Epoch 274/300, trend Loss: 0.0770 | 0.0597
Epoch 275/300, trend Loss: 0.0774 | 0.0597
Epoch 276/300, trend Loss: 0.0768 | 0.0595
Epoch 277/300, trend Loss: 0.0768 | 0.0595
Epoch 278/300, trend Loss: 0.0771 | 0.0591
Epoch 279/300, trend Loss: 0.0768 | 0.0590
Epoch 280/300, trend Loss: 0.0766 | 0.0594
Epoch 281/300, trend Loss: 0.0767 | 0.0593
Epoch 282/300, trend Loss: 0.0766 | 0.0591
Epoch 283/300, trend Loss: 0.0771 | 0.0592
Epoch 284/300, trend Loss: 0.0764 | 0.0591
Epoch 285/300, trend Loss: 0.0767 | 0.0591
Epoch 286/300, trend Loss: 0.0767 | 0.0592
Epoch 287/300, trend Loss: 0.0762 | 0.0593
Epoch 288/300, trend Loss: 0.0768 | 0.0592
Epoch 289/300, trend Loss: 0.0764 | 0.0593
Epoch 290/300, trend Loss: 0.0767 | 0.0591
Epoch 291/300, trend Loss: 0.0768 | 0.0589
Epoch 292/300, trend Loss: 0.0764 | 0.0591
Epoch 293/300, trend Loss: 0.0764 | 0.0591
Epoch 294/300, trend Loss: 0.0765 | 0.0591
Epoch 295/300, trend Loss: 0.0765 | 0.0591
Epoch 296/300, trend Loss: 0.0763 | 0.0591
Epoch 297/300, trend Loss: 0.0764 | 0.0591
Epoch 298/300, trend Loss: 0.0756 | 0.0590
Epoch 299/300, trend Loss: 0.0760 | 0.0591
Epoch 300/300, trend Loss: 0.0764 | 0.0590
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.9707094688598665, 'learning_rate': 0.0003974905889163145, 'batch_size': 222, 'step_size': 7, 'gamma': 0.8968141479926932}
Epoch 1/300, seasonal_0 Loss: 1.0627 | 1.2599
Epoch 2/300, seasonal_0 Loss: 0.8055 | 0.8081
Epoch 3/300, seasonal_0 Loss: 0.6621 | 0.7122
Epoch 4/300, seasonal_0 Loss: 0.6048 | 0.7818
Epoch 5/300, seasonal_0 Loss: 0.5722 | 0.5652
Epoch 6/300, seasonal_0 Loss: 0.5132 | 0.6131
Epoch 7/300, seasonal_0 Loss: 0.5170 | 0.5709
Epoch 8/300, seasonal_0 Loss: 0.4349 | 0.4726
Epoch 9/300, seasonal_0 Loss: 0.4601 | 0.4880
Epoch 10/300, seasonal_0 Loss: 0.3960 | 0.4165
Epoch 11/300, seasonal_0 Loss: 0.4423 | 0.4123
Epoch 12/300, seasonal_0 Loss: 0.3725 | 0.4302
Epoch 13/300, seasonal_0 Loss: 0.3456 | 0.4409
Epoch 14/300, seasonal_0 Loss: 0.3135 | 0.3779
Epoch 15/300, seasonal_0 Loss: 0.3165 | 0.3667
Epoch 16/300, seasonal_0 Loss: 0.3918 | 0.4500
Epoch 17/300, seasonal_0 Loss: 0.2961 | 0.3743
Epoch 18/300, seasonal_0 Loss: 0.3074 | 0.4044
Epoch 19/300, seasonal_0 Loss: 0.2725 | 0.3422
Epoch 20/300, seasonal_0 Loss: 0.2300 | 0.3073
Epoch 21/300, seasonal_0 Loss: 0.2186 | 0.3114
Epoch 22/300, seasonal_0 Loss: 0.2159 | 0.2751
Epoch 23/300, seasonal_0 Loss: 0.1965 | 0.2742
Epoch 24/300, seasonal_0 Loss: 0.1921 | 0.2641
Epoch 25/300, seasonal_0 Loss: 0.1894 | 0.2595
Epoch 26/300, seasonal_0 Loss: 0.1898 | 0.2471
Epoch 27/300, seasonal_0 Loss: 0.1822 | 0.2482
Epoch 28/300, seasonal_0 Loss: 0.1779 | 0.2291
Epoch 29/300, seasonal_0 Loss: 0.1846 | 0.2343
Epoch 30/300, seasonal_0 Loss: 0.1774 | 0.2260
Epoch 31/300, seasonal_0 Loss: 0.1760 | 0.2196
Epoch 32/300, seasonal_0 Loss: 0.1662 | 0.2203
Epoch 33/300, seasonal_0 Loss: 0.1647 | 0.2136
Epoch 34/300, seasonal_0 Loss: 0.1580 | 0.2115
Epoch 35/300, seasonal_0 Loss: 0.1563 | 0.2099
Epoch 36/300, seasonal_0 Loss: 0.1535 | 0.2044
Epoch 37/300, seasonal_0 Loss: 0.1506 | 0.2068
Epoch 38/300, seasonal_0 Loss: 0.1494 | 0.1986
Epoch 39/300, seasonal_0 Loss: 0.1483 | 0.1996
Epoch 40/300, seasonal_0 Loss: 0.1465 | 0.1928
Epoch 41/300, seasonal_0 Loss: 0.1440 | 0.1951
Epoch 42/300, seasonal_0 Loss: 0.1410 | 0.1880
Epoch 43/300, seasonal_0 Loss: 0.1383 | 0.1896
Epoch 44/300, seasonal_0 Loss: 0.1360 | 0.1846
Epoch 45/300, seasonal_0 Loss: 0.1338 | 0.1850
Epoch 46/300, seasonal_0 Loss: 0.1330 | 0.1814
Epoch 47/300, seasonal_0 Loss: 0.1314 | 0.1812
Epoch 48/300, seasonal_0 Loss: 0.1303 | 0.1781
Epoch 49/300, seasonal_0 Loss: 0.1281 | 0.1762
Epoch 50/300, seasonal_0 Loss: 0.1267 | 0.1742
Epoch 51/300, seasonal_0 Loss: 0.1269 | 0.1733
Epoch 52/300, seasonal_0 Loss: 0.1246 | 0.1712
Epoch 53/300, seasonal_0 Loss: 0.1245 | 0.1691
Epoch 54/300, seasonal_0 Loss: 0.1234 | 0.1674
Epoch 55/300, seasonal_0 Loss: 0.1224 | 0.1665
Epoch 56/300, seasonal_0 Loss: 0.1211 | 0.1651
Epoch 57/300, seasonal_0 Loss: 0.1206 | 0.1638
Epoch 58/300, seasonal_0 Loss: 0.1203 | 0.1621
Epoch 59/300, seasonal_0 Loss: 0.1199 | 0.1619
Epoch 60/300, seasonal_0 Loss: 0.1180 | 0.1604
Epoch 61/300, seasonal_0 Loss: 0.1183 | 0.1589
Epoch 62/300, seasonal_0 Loss: 0.1174 | 0.1582
Epoch 63/300, seasonal_0 Loss: 0.1164 | 0.1573
Epoch 64/300, seasonal_0 Loss: 0.1162 | 0.1559
Epoch 65/300, seasonal_0 Loss: 0.1159 | 0.1553
Epoch 66/300, seasonal_0 Loss: 0.1148 | 0.1547
Epoch 67/300, seasonal_0 Loss: 0.1145 | 0.1534
Epoch 68/300, seasonal_0 Loss: 0.1141 | 0.1523
Epoch 69/300, seasonal_0 Loss: 0.1129 | 0.1516
Epoch 70/300, seasonal_0 Loss: 0.1127 | 0.1506
Epoch 71/300, seasonal_0 Loss: 0.1123 | 0.1498
Epoch 72/300, seasonal_0 Loss: 0.1118 | 0.1490
Epoch 73/300, seasonal_0 Loss: 0.1116 | 0.1481
Epoch 74/300, seasonal_0 Loss: 0.1107 | 0.1481
Epoch 75/300, seasonal_0 Loss: 0.1104 | 0.1477
Epoch 76/300, seasonal_0 Loss: 0.1093 | 0.1462
Epoch 77/300, seasonal_0 Loss: 0.1090 | 0.1456
Epoch 78/300, seasonal_0 Loss: 0.1093 | 0.1446
Epoch 79/300, seasonal_0 Loss: 0.1085 | 0.1444
Epoch 80/300, seasonal_0 Loss: 0.1081 | 0.1445
Epoch 81/300, seasonal_0 Loss: 0.1078 | 0.1432
Epoch 82/300, seasonal_0 Loss: 0.1079 | 0.1423
Epoch 83/300, seasonal_0 Loss: 0.1074 | 0.1420
Epoch 84/300, seasonal_0 Loss: 0.1070 | 0.1417
Epoch 85/300, seasonal_0 Loss: 0.1059 | 0.1416
Epoch 86/300, seasonal_0 Loss: 0.1060 | 0.1403
Epoch 87/300, seasonal_0 Loss: 0.1056 | 0.1396
Epoch 88/300, seasonal_0 Loss: 0.1054 | 0.1397
Epoch 89/300, seasonal_0 Loss: 0.1045 | 0.1393
Epoch 90/300, seasonal_0 Loss: 0.1046 | 0.1391
Epoch 91/300, seasonal_0 Loss: 0.1042 | 0.1388
Epoch 92/300, seasonal_0 Loss: 0.1047 | 0.1384
Epoch 93/300, seasonal_0 Loss: 0.1041 | 0.1378
Epoch 94/300, seasonal_0 Loss: 0.1036 | 0.1378
Epoch 95/300, seasonal_0 Loss: 0.1038 | 0.1371
Epoch 96/300, seasonal_0 Loss: 0.1030 | 0.1361
Epoch 97/300, seasonal_0 Loss: 0.1022 | 0.1359
Epoch 98/300, seasonal_0 Loss: 0.1027 | 0.1359
Epoch 99/300, seasonal_0 Loss: 0.1024 | 0.1357
Epoch 100/300, seasonal_0 Loss: 0.1024 | 0.1350
Epoch 101/300, seasonal_0 Loss: 0.1013 | 0.1347
Epoch 102/300, seasonal_0 Loss: 0.1020 | 0.1343
Epoch 103/300, seasonal_0 Loss: 0.1016 | 0.1338
Epoch 104/300, seasonal_0 Loss: 0.1011 | 0.1335
Epoch 105/300, seasonal_0 Loss: 0.1005 | 0.1332
Epoch 106/300, seasonal_0 Loss: 0.1011 | 0.1328
Epoch 107/300, seasonal_0 Loss: 0.1009 | 0.1328
Epoch 108/300, seasonal_0 Loss: 0.1010 | 0.1326
Epoch 109/300, seasonal_0 Loss: 0.1009 | 0.1320
Epoch 110/300, seasonal_0 Loss: 0.1008 | 0.1322
Epoch 111/300, seasonal_0 Loss: 0.1003 | 0.1322
Epoch 112/300, seasonal_0 Loss: 0.1000 | 0.1318
Epoch 113/300, seasonal_0 Loss: 0.0998 | 0.1315
Epoch 114/300, seasonal_0 Loss: 0.0999 | 0.1314
Epoch 115/300, seasonal_0 Loss: 0.1002 | 0.1314
Epoch 116/300, seasonal_0 Loss: 0.0999 | 0.1313
Epoch 117/300, seasonal_0 Loss: 0.0999 | 0.1308
Epoch 118/300, seasonal_0 Loss: 0.0999 | 0.1305
Epoch 119/300, seasonal_0 Loss: 0.0988 | 0.1305
Epoch 120/300, seasonal_0 Loss: 0.0993 | 0.1305
Epoch 121/300, seasonal_0 Loss: 0.0993 | 0.1304
Epoch 122/300, seasonal_0 Loss: 0.0992 | 0.1302
Epoch 123/300, seasonal_0 Loss: 0.0999 | 0.1298
Epoch 124/300, seasonal_0 Loss: 0.0992 | 0.1296
Epoch 125/300, seasonal_0 Loss: 0.0984 | 0.1293
Epoch 126/300, seasonal_0 Loss: 0.0987 | 0.1291
Epoch 127/300, seasonal_0 Loss: 0.0984 | 0.1290
Epoch 128/300, seasonal_0 Loss: 0.0991 | 0.1290
Epoch 129/300, seasonal_0 Loss: 0.0983 | 0.1289
Epoch 130/300, seasonal_0 Loss: 0.0983 | 0.1288
Epoch 131/300, seasonal_0 Loss: 0.0982 | 0.1288
Epoch 132/300, seasonal_0 Loss: 0.0984 | 0.1287
Epoch 133/300, seasonal_0 Loss: 0.0987 | 0.1284
Epoch 134/300, seasonal_0 Loss: 0.0981 | 0.1282
Epoch 135/300, seasonal_0 Loss: 0.0975 | 0.1281
Epoch 136/300, seasonal_0 Loss: 0.0977 | 0.1279
Epoch 137/300, seasonal_0 Loss: 0.0977 | 0.1278
Epoch 138/300, seasonal_0 Loss: 0.0977 | 0.1276
Epoch 139/300, seasonal_0 Loss: 0.0975 | 0.1275
Epoch 140/300, seasonal_0 Loss: 0.0970 | 0.1274
Epoch 141/300, seasonal_0 Loss: 0.0976 | 0.1272
Epoch 142/300, seasonal_0 Loss: 0.0974 | 0.1271
Epoch 143/300, seasonal_0 Loss: 0.0978 | 0.1271
Epoch 144/300, seasonal_0 Loss: 0.0975 | 0.1271
Epoch 145/300, seasonal_0 Loss: 0.0973 | 0.1270
Epoch 146/300, seasonal_0 Loss: 0.0968 | 0.1268
Epoch 147/300, seasonal_0 Loss: 0.0976 | 0.1268
Epoch 148/300, seasonal_0 Loss: 0.0974 | 0.1268
Epoch 149/300, seasonal_0 Loss: 0.0964 | 0.1267
Epoch 150/300, seasonal_0 Loss: 0.0971 | 0.1266
Epoch 151/300, seasonal_0 Loss: 0.0969 | 0.1265
Epoch 152/300, seasonal_0 Loss: 0.0971 | 0.1264
Epoch 153/300, seasonal_0 Loss: 0.0966 | 0.1263
Epoch 154/300, seasonal_0 Loss: 0.0973 | 0.1262
Epoch 155/300, seasonal_0 Loss: 0.0966 | 0.1260
Epoch 156/300, seasonal_0 Loss: 0.0972 | 0.1258
Epoch 157/300, seasonal_0 Loss: 0.0973 | 0.1257
Epoch 158/300, seasonal_0 Loss: 0.0968 | 0.1256
Epoch 159/300, seasonal_0 Loss: 0.0967 | 0.1255
Epoch 160/300, seasonal_0 Loss: 0.0971 | 0.1255
Epoch 161/300, seasonal_0 Loss: 0.0967 | 0.1255
Epoch 162/300, seasonal_0 Loss: 0.0965 | 0.1255
Epoch 163/300, seasonal_0 Loss: 0.0966 | 0.1255
Epoch 164/300, seasonal_0 Loss: 0.0961 | 0.1255
Epoch 165/300, seasonal_0 Loss: 0.0961 | 0.1255
Epoch 166/300, seasonal_0 Loss: 0.0965 | 0.1255
Epoch 167/300, seasonal_0 Loss: 0.0965 | 0.1255
Epoch 168/300, seasonal_0 Loss: 0.0963 | 0.1255
Epoch 169/300, seasonal_0 Loss: 0.0966 | 0.1254
Epoch 170/300, seasonal_0 Loss: 0.0963 | 0.1253
Epoch 171/300, seasonal_0 Loss: 0.0964 | 0.1252
Epoch 172/300, seasonal_0 Loss: 0.0969 | 0.1251
Epoch 173/300, seasonal_0 Loss: 0.0962 | 0.1251
Epoch 174/300, seasonal_0 Loss: 0.0965 | 0.1251
Epoch 175/300, seasonal_0 Loss: 0.0964 | 0.1251
Epoch 176/300, seasonal_0 Loss: 0.0960 | 0.1250
Epoch 177/300, seasonal_0 Loss: 0.0967 | 0.1248
Epoch 178/300, seasonal_0 Loss: 0.0956 | 0.1248
Epoch 179/300, seasonal_0 Loss: 0.0966 | 0.1249
Epoch 180/300, seasonal_0 Loss: 0.0960 | 0.1248
Epoch 181/300, seasonal_0 Loss: 0.0964 | 0.1248
Epoch 182/300, seasonal_0 Loss: 0.0961 | 0.1247
Epoch 183/300, seasonal_0 Loss: 0.0963 | 0.1246
Epoch 184/300, seasonal_0 Loss: 0.0964 | 0.1246
Epoch 185/300, seasonal_0 Loss: 0.0963 | 0.1246
Epoch 186/300, seasonal_0 Loss: 0.0963 | 0.1246
Epoch 187/300, seasonal_0 Loss: 0.0963 | 0.1246
Epoch 188/300, seasonal_0 Loss: 0.0960 | 0.1245
Epoch 189/300, seasonal_0 Loss: 0.0961 | 0.1244
Epoch 190/300, seasonal_0 Loss: 0.0954 | 0.1244
Epoch 191/300, seasonal_0 Loss: 0.0958 | 0.1244
Epoch 192/300, seasonal_0 Loss: 0.0958 | 0.1244
Epoch 193/300, seasonal_0 Loss: 0.0956 | 0.1244
Epoch 194/300, seasonal_0 Loss: 0.0959 | 0.1244
Epoch 195/300, seasonal_0 Loss: 0.0951 | 0.1244
Epoch 196/300, seasonal_0 Loss: 0.0959 | 0.1244
Epoch 197/300, seasonal_0 Loss: 0.0960 | 0.1244
Epoch 198/300, seasonal_0 Loss: 0.0957 | 0.1244
Epoch 199/300, seasonal_0 Loss: 0.0961 | 0.1243
Epoch 200/300, seasonal_0 Loss: 0.0959 | 0.1243
Epoch 201/300, seasonal_0 Loss: 0.0957 | 0.1243
Epoch 202/300, seasonal_0 Loss: 0.0958 | 0.1243
Epoch 203/300, seasonal_0 Loss: 0.0956 | 0.1243
Epoch 204/300, seasonal_0 Loss: 0.0958 | 0.1243
Epoch 205/300, seasonal_0 Loss: 0.0953 | 0.1242
Epoch 206/300, seasonal_0 Loss: 0.0963 | 0.1242
Epoch 207/300, seasonal_0 Loss: 0.0957 | 0.1242
Epoch 208/300, seasonal_0 Loss: 0.0957 | 0.1242
Epoch 209/300, seasonal_0 Loss: 0.0959 | 0.1242
Epoch 210/300, seasonal_0 Loss: 0.0962 | 0.1242
Epoch 211/300, seasonal_0 Loss: 0.0963 | 0.1241
Epoch 212/300, seasonal_0 Loss: 0.0962 | 0.1241
Epoch 213/300, seasonal_0 Loss: 0.0959 | 0.1241
Epoch 214/300, seasonal_0 Loss: 0.0959 | 0.1241
Epoch 215/300, seasonal_0 Loss: 0.0956 | 0.1241
Epoch 216/300, seasonal_0 Loss: 0.0955 | 0.1241
Epoch 217/300, seasonal_0 Loss: 0.0963 | 0.1241
Epoch 218/300, seasonal_0 Loss: 0.0956 | 0.1240
Epoch 219/300, seasonal_0 Loss: 0.0959 | 0.1240
Epoch 220/300, seasonal_0 Loss: 0.0957 | 0.1240
Epoch 221/300, seasonal_0 Loss: 0.0965 | 0.1240
Epoch 222/300, seasonal_0 Loss: 0.0956 | 0.1240
Epoch 223/300, seasonal_0 Loss: 0.0949 | 0.1240
Epoch 224/300, seasonal_0 Loss: 0.0961 | 0.1240
Epoch 225/300, seasonal_0 Loss: 0.0956 | 0.1240
Epoch 226/300, seasonal_0 Loss: 0.0955 | 0.1240
Epoch 227/300, seasonal_0 Loss: 0.0955 | 0.1240
Epoch 228/300, seasonal_0 Loss: 0.0957 | 0.1239
Epoch 229/300, seasonal_0 Loss: 0.0955 | 0.1239
Epoch 230/300, seasonal_0 Loss: 0.0953 | 0.1239
Epoch 231/300, seasonal_0 Loss: 0.0953 | 0.1239
Epoch 232/300, seasonal_0 Loss: 0.0954 | 0.1239
Epoch 233/300, seasonal_0 Loss: 0.0958 | 0.1239
Epoch 234/300, seasonal_0 Loss: 0.0952 | 0.1239
Epoch 235/300, seasonal_0 Loss: 0.0955 | 0.1239
Epoch 236/300, seasonal_0 Loss: 0.0961 | 0.1239
Epoch 237/300, seasonal_0 Loss: 0.0958 | 0.1239
Epoch 238/300, seasonal_0 Loss: 0.0961 | 0.1239
Epoch 239/300, seasonal_0 Loss: 0.0956 | 0.1239
Epoch 240/300, seasonal_0 Loss: 0.0957 | 0.1239
Epoch 241/300, seasonal_0 Loss: 0.0960 | 0.1238
Epoch 242/300, seasonal_0 Loss: 0.0952 | 0.1238
Epoch 243/300, seasonal_0 Loss: 0.0960 | 0.1238
Epoch 244/300, seasonal_0 Loss: 0.0956 | 0.1238
Epoch 245/300, seasonal_0 Loss: 0.0952 | 0.1238
Epoch 246/300, seasonal_0 Loss: 0.0957 | 0.1238
Epoch 247/300, seasonal_0 Loss: 0.0954 | 0.1238
Epoch 248/300, seasonal_0 Loss: 0.0953 | 0.1238
Epoch 249/300, seasonal_0 Loss: 0.0963 | 0.1238
Epoch 250/300, seasonal_0 Loss: 0.0955 | 0.1238
Epoch 251/300, seasonal_0 Loss: 0.0960 | 0.1238
Epoch 252/300, seasonal_0 Loss: 0.0955 | 0.1238
Epoch 253/300, seasonal_0 Loss: 0.0951 | 0.1238
Epoch 254/300, seasonal_0 Loss: 0.0953 | 0.1238
Epoch 255/300, seasonal_0 Loss: 0.0949 | 0.1238
Epoch 256/300, seasonal_0 Loss: 0.0958 | 0.1237
Epoch 257/300, seasonal_0 Loss: 0.0957 | 0.1237
Epoch 258/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 259/300, seasonal_0 Loss: 0.0944 | 0.1237
Epoch 260/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 261/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 262/300, seasonal_0 Loss: 0.0950 | 0.1237
Epoch 263/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 264/300, seasonal_0 Loss: 0.0955 | 0.1237
Epoch 265/300, seasonal_0 Loss: 0.0958 | 0.1237
Epoch 266/300, seasonal_0 Loss: 0.0954 | 0.1237
Epoch 267/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 268/300, seasonal_0 Loss: 0.0957 | 0.1237
Epoch 269/300, seasonal_0 Loss: 0.0946 | 0.1237
Epoch 270/300, seasonal_0 Loss: 0.0957 | 0.1237
Epoch 271/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 272/300, seasonal_0 Loss: 0.0961 | 0.1237
Epoch 273/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 274/300, seasonal_0 Loss: 0.0962 | 0.1237
Epoch 275/300, seasonal_0 Loss: 0.0954 | 0.1237
Epoch 276/300, seasonal_0 Loss: 0.0958 | 0.1237
Epoch 277/300, seasonal_0 Loss: 0.0951 | 0.1237
Epoch 278/300, seasonal_0 Loss: 0.0959 | 0.1237
Epoch 279/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 280/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 281/300, seasonal_0 Loss: 0.0957 | 0.1237
Epoch 282/300, seasonal_0 Loss: 0.0962 | 0.1237
Epoch 283/300, seasonal_0 Loss: 0.0952 | 0.1237
Epoch 284/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 285/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 286/300, seasonal_0 Loss: 0.0958 | 0.1237
Epoch 287/300, seasonal_0 Loss: 0.0958 | 0.1237
Epoch 288/300, seasonal_0 Loss: 0.0952 | 0.1237
Epoch 289/300, seasonal_0 Loss: 0.0954 | 0.1237
Epoch 290/300, seasonal_0 Loss: 0.0960 | 0.1237
Epoch 291/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 292/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 293/300, seasonal_0 Loss: 0.0958 | 0.1237
Epoch 294/300, seasonal_0 Loss: 0.0952 | 0.1237
Epoch 295/300, seasonal_0 Loss: 0.0953 | 0.1237
Epoch 296/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 297/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 298/300, seasonal_0 Loss: 0.0952 | 0.1237
Epoch 299/300, seasonal_0 Loss: 0.0956 | 0.1237
Epoch 300/300, seasonal_0 Loss: 0.0959 | 0.1237
Training seasonal_1 component with params: {'observation_period_num': 30, 'train_rates': 0.9646055887146711, 'learning_rate': 0.0002505221045064007, 'batch_size': 187, 'step_size': 14, 'gamma': 0.753838937243689}
Epoch 1/300, seasonal_1 Loss: 0.9594 | 1.1123
Epoch 2/300, seasonal_1 Loss: 0.8186 | 0.7877
Epoch 3/300, seasonal_1 Loss: 0.6614 | 0.6949
Epoch 4/300, seasonal_1 Loss: 0.5487 | 0.5803
Epoch 5/300, seasonal_1 Loss: 0.5173 | 0.5193
Epoch 6/300, seasonal_1 Loss: 0.5815 | 0.4865
Epoch 7/300, seasonal_1 Loss: 0.5449 | 0.6318
Epoch 8/300, seasonal_1 Loss: 0.4572 | 0.4689
Epoch 9/300, seasonal_1 Loss: 0.4178 | 0.4298
Epoch 10/300, seasonal_1 Loss: 0.3632 | 0.4478
Epoch 11/300, seasonal_1 Loss: 0.3652 | 0.3739
Epoch 12/300, seasonal_1 Loss: 0.3405 | 0.3886
Epoch 13/300, seasonal_1 Loss: 0.3081 | 0.3047
Epoch 14/300, seasonal_1 Loss: 0.2876 | 0.3204
Epoch 15/300, seasonal_1 Loss: 0.2844 | 0.3113
Epoch 16/300, seasonal_1 Loss: 0.2435 | 0.2721
Epoch 17/300, seasonal_1 Loss: 0.2234 | 0.2697
Epoch 18/300, seasonal_1 Loss: 0.2178 | 0.2535
Epoch 19/300, seasonal_1 Loss: 0.2169 | 0.2548
Epoch 20/300, seasonal_1 Loss: 0.2177 | 0.2388
Epoch 21/300, seasonal_1 Loss: 0.2196 | 0.2365
Epoch 22/300, seasonal_1 Loss: 0.2101 | 0.2304
Epoch 23/300, seasonal_1 Loss: 0.1987 | 0.2218
Epoch 24/300, seasonal_1 Loss: 0.1989 | 0.2182
Epoch 25/300, seasonal_1 Loss: 0.1945 | 0.2142
Epoch 26/300, seasonal_1 Loss: 0.1949 | 0.2111
Epoch 27/300, seasonal_1 Loss: 0.1872 | 0.2085
Epoch 28/300, seasonal_1 Loss: 0.1833 | 0.2060
Epoch 29/300, seasonal_1 Loss: 0.1775 | 0.2024
Epoch 30/300, seasonal_1 Loss: 0.1736 | 0.2005
Epoch 31/300, seasonal_1 Loss: 0.1713 | 0.1976
Epoch 32/300, seasonal_1 Loss: 0.1703 | 0.1943
Epoch 33/300, seasonal_1 Loss: 0.1693 | 0.1927
Epoch 34/300, seasonal_1 Loss: 0.1682 | 0.1908
Epoch 35/300, seasonal_1 Loss: 0.1669 | 0.1878
Epoch 36/300, seasonal_1 Loss: 0.1653 | 0.1864
Epoch 37/300, seasonal_1 Loss: 0.1643 | 0.1844
Epoch 38/300, seasonal_1 Loss: 0.1626 | 0.1835
Epoch 39/300, seasonal_1 Loss: 0.1611 | 0.1819
Epoch 40/300, seasonal_1 Loss: 0.1607 | 0.1786
Epoch 41/300, seasonal_1 Loss: 0.1598 | 0.1785
Epoch 42/300, seasonal_1 Loss: 0.1590 | 0.1771
Epoch 43/300, seasonal_1 Loss: 0.1576 | 0.1748
Epoch 44/300, seasonal_1 Loss: 0.1561 | 0.1754
Epoch 45/300, seasonal_1 Loss: 0.1566 | 0.1739
Epoch 46/300, seasonal_1 Loss: 0.1548 | 0.1720
Epoch 47/300, seasonal_1 Loss: 0.1547 | 0.1708
Epoch 48/300, seasonal_1 Loss: 0.1536 | 0.1705
Epoch 49/300, seasonal_1 Loss: 0.1530 | 0.1700
Epoch 50/300, seasonal_1 Loss: 0.1530 | 0.1684
Epoch 51/300, seasonal_1 Loss: 0.1518 | 0.1688
Epoch 52/300, seasonal_1 Loss: 0.1513 | 0.1664
Epoch 53/300, seasonal_1 Loss: 0.1504 | 0.1675
Epoch 54/300, seasonal_1 Loss: 0.1502 | 0.1651
Epoch 55/300, seasonal_1 Loss: 0.1500 | 0.1643
Epoch 56/300, seasonal_1 Loss: 0.1492 | 0.1630
Epoch 57/300, seasonal_1 Loss: 0.1487 | 0.1625
Epoch 58/300, seasonal_1 Loss: 0.1482 | 0.1619
Epoch 59/300, seasonal_1 Loss: 0.1478 | 0.1620
Epoch 60/300, seasonal_1 Loss: 0.1471 | 0.1610
Epoch 61/300, seasonal_1 Loss: 0.1468 | 0.1598
Epoch 62/300, seasonal_1 Loss: 0.1462 | 0.1592
Epoch 63/300, seasonal_1 Loss: 0.1457 | 0.1602
Epoch 64/300, seasonal_1 Loss: 0.1452 | 0.1588
Epoch 65/300, seasonal_1 Loss: 0.1452 | 0.1581
Epoch 66/300, seasonal_1 Loss: 0.1442 | 0.1578
Epoch 67/300, seasonal_1 Loss: 0.1446 | 0.1582
Epoch 68/300, seasonal_1 Loss: 0.1439 | 0.1577
Epoch 69/300, seasonal_1 Loss: 0.1430 | 0.1571
Epoch 70/300, seasonal_1 Loss: 0.1426 | 0.1563
Epoch 71/300, seasonal_1 Loss: 0.1435 | 0.1557
Epoch 72/300, seasonal_1 Loss: 0.1430 | 0.1551
Epoch 73/300, seasonal_1 Loss: 0.1427 | 0.1548
Epoch 74/300, seasonal_1 Loss: 0.1422 | 0.1544
Epoch 75/300, seasonal_1 Loss: 0.1417 | 0.1544
Epoch 76/300, seasonal_1 Loss: 0.1421 | 0.1548
Epoch 77/300, seasonal_1 Loss: 0.1410 | 0.1544
Epoch 78/300, seasonal_1 Loss: 0.1411 | 0.1539
Epoch 79/300, seasonal_1 Loss: 0.1409 | 0.1536
Epoch 80/300, seasonal_1 Loss: 0.1410 | 0.1536
Epoch 81/300, seasonal_1 Loss: 0.1398 | 0.1529
Epoch 82/300, seasonal_1 Loss: 0.1405 | 0.1522
Epoch 83/300, seasonal_1 Loss: 0.1397 | 0.1522
Epoch 84/300, seasonal_1 Loss: 0.1401 | 0.1515
Epoch 85/300, seasonal_1 Loss: 0.1397 | 0.1516
Epoch 86/300, seasonal_1 Loss: 0.1392 | 0.1512
Epoch 87/300, seasonal_1 Loss: 0.1395 | 0.1513
Epoch 88/300, seasonal_1 Loss: 0.1398 | 0.1509
Epoch 89/300, seasonal_1 Loss: 0.1390 | 0.1510
Epoch 90/300, seasonal_1 Loss: 0.1387 | 0.1507
Epoch 91/300, seasonal_1 Loss: 0.1391 | 0.1503
Epoch 92/300, seasonal_1 Loss: 0.1386 | 0.1503
Epoch 93/300, seasonal_1 Loss: 0.1386 | 0.1501
Epoch 94/300, seasonal_1 Loss: 0.1387 | 0.1499
Epoch 95/300, seasonal_1 Loss: 0.1383 | 0.1498
Epoch 96/300, seasonal_1 Loss: 0.1382 | 0.1500
Epoch 97/300, seasonal_1 Loss: 0.1380 | 0.1494
Epoch 98/300, seasonal_1 Loss: 0.1382 | 0.1491
Epoch 99/300, seasonal_1 Loss: 0.1376 | 0.1490
Epoch 100/300, seasonal_1 Loss: 0.1380 | 0.1490
Epoch 101/300, seasonal_1 Loss: 0.1375 | 0.1486
Epoch 102/300, seasonal_1 Loss: 0.1370 | 0.1484
Epoch 103/300, seasonal_1 Loss: 0.1377 | 0.1487
Epoch 104/300, seasonal_1 Loss: 0.1375 | 0.1488
Epoch 105/300, seasonal_1 Loss: 0.1362 | 0.1486
Epoch 106/300, seasonal_1 Loss: 0.1369 | 0.1486
Epoch 107/300, seasonal_1 Loss: 0.1370 | 0.1487
Epoch 108/300, seasonal_1 Loss: 0.1370 | 0.1487
Epoch 109/300, seasonal_1 Loss: 0.1371 | 0.1483
Epoch 110/300, seasonal_1 Loss: 0.1366 | 0.1482
Epoch 111/300, seasonal_1 Loss: 0.1362 | 0.1480
Epoch 112/300, seasonal_1 Loss: 0.1368 | 0.1479
Epoch 113/300, seasonal_1 Loss: 0.1363 | 0.1477
Epoch 114/300, seasonal_1 Loss: 0.1367 | 0.1477
Epoch 115/300, seasonal_1 Loss: 0.1370 | 0.1477
Epoch 116/300, seasonal_1 Loss: 0.1356 | 0.1475
Epoch 117/300, seasonal_1 Loss: 0.1368 | 0.1476
Epoch 118/300, seasonal_1 Loss: 0.1362 | 0.1476
Epoch 119/300, seasonal_1 Loss: 0.1361 | 0.1476
Epoch 120/300, seasonal_1 Loss: 0.1364 | 0.1475
Epoch 121/300, seasonal_1 Loss: 0.1365 | 0.1474
Epoch 122/300, seasonal_1 Loss: 0.1365 | 0.1471
Epoch 123/300, seasonal_1 Loss: 0.1362 | 0.1471
Epoch 124/300, seasonal_1 Loss: 0.1360 | 0.1471
Epoch 125/300, seasonal_1 Loss: 0.1357 | 0.1471
Epoch 126/300, seasonal_1 Loss: 0.1360 | 0.1471
Epoch 127/300, seasonal_1 Loss: 0.1355 | 0.1470
Epoch 128/300, seasonal_1 Loss: 0.1360 | 0.1471
Epoch 129/300, seasonal_1 Loss: 0.1357 | 0.1471
Epoch 130/300, seasonal_1 Loss: 0.1356 | 0.1470
Epoch 131/300, seasonal_1 Loss: 0.1357 | 0.1470
Epoch 132/300, seasonal_1 Loss: 0.1355 | 0.1470
Epoch 133/300, seasonal_1 Loss: 0.1355 | 0.1470
Epoch 134/300, seasonal_1 Loss: 0.1351 | 0.1471
Epoch 135/300, seasonal_1 Loss: 0.1355 | 0.1469
Epoch 136/300, seasonal_1 Loss: 0.1355 | 0.1469
Epoch 137/300, seasonal_1 Loss: 0.1364 | 0.1468
Epoch 138/300, seasonal_1 Loss: 0.1356 | 0.1468
Epoch 139/300, seasonal_1 Loss: 0.1350 | 0.1466
Epoch 140/300, seasonal_1 Loss: 0.1352 | 0.1466
Epoch 141/300, seasonal_1 Loss: 0.1348 | 0.1467
Epoch 142/300, seasonal_1 Loss: 0.1350 | 0.1466
Epoch 143/300, seasonal_1 Loss: 0.1346 | 0.1466
Epoch 144/300, seasonal_1 Loss: 0.1345 | 0.1467
Epoch 145/300, seasonal_1 Loss: 0.1350 | 0.1466
Epoch 146/300, seasonal_1 Loss: 0.1349 | 0.1465
Epoch 147/300, seasonal_1 Loss: 0.1353 | 0.1465
Epoch 148/300, seasonal_1 Loss: 0.1350 | 0.1465
Epoch 149/300, seasonal_1 Loss: 0.1350 | 0.1465
Epoch 150/300, seasonal_1 Loss: 0.1353 | 0.1464
Epoch 151/300, seasonal_1 Loss: 0.1357 | 0.1463
Epoch 152/300, seasonal_1 Loss: 0.1350 | 0.1463
Epoch 153/300, seasonal_1 Loss: 0.1342 | 0.1463
Epoch 154/300, seasonal_1 Loss: 0.1354 | 0.1463
Epoch 155/300, seasonal_1 Loss: 0.1356 | 0.1462
Epoch 156/300, seasonal_1 Loss: 0.1355 | 0.1462
Epoch 157/300, seasonal_1 Loss: 0.1352 | 0.1461
Epoch 158/300, seasonal_1 Loss: 0.1353 | 0.1461
Epoch 159/300, seasonal_1 Loss: 0.1356 | 0.1461
Epoch 160/300, seasonal_1 Loss: 0.1351 | 0.1461
Epoch 161/300, seasonal_1 Loss: 0.1353 | 0.1461
Epoch 162/300, seasonal_1 Loss: 0.1347 | 0.1461
Epoch 163/300, seasonal_1 Loss: 0.1352 | 0.1461
Epoch 164/300, seasonal_1 Loss: 0.1349 | 0.1461
Epoch 165/300, seasonal_1 Loss: 0.1354 | 0.1462
Epoch 166/300, seasonal_1 Loss: 0.1348 | 0.1462
Epoch 167/300, seasonal_1 Loss: 0.1344 | 0.1461
Epoch 168/300, seasonal_1 Loss: 0.1352 | 0.1461
Epoch 169/300, seasonal_1 Loss: 0.1346 | 0.1461
Epoch 170/300, seasonal_1 Loss: 0.1348 | 0.1460
Epoch 171/300, seasonal_1 Loss: 0.1346 | 0.1460
Epoch 172/300, seasonal_1 Loss: 0.1347 | 0.1460
Epoch 173/300, seasonal_1 Loss: 0.1349 | 0.1460
Epoch 174/300, seasonal_1 Loss: 0.1350 | 0.1460
Epoch 175/300, seasonal_1 Loss: 0.1347 | 0.1459
Epoch 176/300, seasonal_1 Loss: 0.1344 | 0.1459
Epoch 177/300, seasonal_1 Loss: 0.1352 | 0.1459
Epoch 178/300, seasonal_1 Loss: 0.1353 | 0.1459
Epoch 179/300, seasonal_1 Loss: 0.1351 | 0.1459
Epoch 180/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 181/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 182/300, seasonal_1 Loss: 0.1348 | 0.1459
Epoch 183/300, seasonal_1 Loss: 0.1342 | 0.1459
Epoch 184/300, seasonal_1 Loss: 0.1346 | 0.1459
Epoch 185/300, seasonal_1 Loss: 0.1345 | 0.1459
Epoch 186/300, seasonal_1 Loss: 0.1346 | 0.1459
Epoch 187/300, seasonal_1 Loss: 0.1355 | 0.1459
Epoch 188/300, seasonal_1 Loss: 0.1349 | 0.1459
Epoch 189/300, seasonal_1 Loss: 0.1352 | 0.1459
Epoch 190/300, seasonal_1 Loss: 0.1357 | 0.1459
Epoch 191/300, seasonal_1 Loss: 0.1348 | 0.1458
Epoch 192/300, seasonal_1 Loss: 0.1346 | 0.1458
Epoch 193/300, seasonal_1 Loss: 0.1342 | 0.1458
Epoch 194/300, seasonal_1 Loss: 0.1349 | 0.1459
Epoch 195/300, seasonal_1 Loss: 0.1351 | 0.1459
Epoch 196/300, seasonal_1 Loss: 0.1345 | 0.1459
Epoch 197/300, seasonal_1 Loss: 0.1341 | 0.1459
Epoch 198/300, seasonal_1 Loss: 0.1342 | 0.1459
Epoch 199/300, seasonal_1 Loss: 0.1341 | 0.1459
Epoch 200/300, seasonal_1 Loss: 0.1345 | 0.1459
Epoch 201/300, seasonal_1 Loss: 0.1346 | 0.1459
Epoch 202/300, seasonal_1 Loss: 0.1342 | 0.1459
Epoch 203/300, seasonal_1 Loss: 0.1349 | 0.1459
Epoch 204/300, seasonal_1 Loss: 0.1356 | 0.1459
Epoch 205/300, seasonal_1 Loss: 0.1350 | 0.1459
Epoch 206/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 207/300, seasonal_1 Loss: 0.1346 | 0.1458
Epoch 208/300, seasonal_1 Loss: 0.1340 | 0.1458
Epoch 209/300, seasonal_1 Loss: 0.1347 | 0.1458
Epoch 210/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 211/300, seasonal_1 Loss: 0.1345 | 0.1458
Epoch 212/300, seasonal_1 Loss: 0.1352 | 0.1458
Epoch 213/300, seasonal_1 Loss: 0.1351 | 0.1458
Epoch 214/300, seasonal_1 Loss: 0.1341 | 0.1458
Epoch 215/300, seasonal_1 Loss: 0.1353 | 0.1458
Epoch 216/300, seasonal_1 Loss: 0.1345 | 0.1458
Epoch 217/300, seasonal_1 Loss: 0.1346 | 0.1458
Epoch 218/300, seasonal_1 Loss: 0.1348 | 0.1458
Epoch 219/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 220/300, seasonal_1 Loss: 0.1353 | 0.1458
Epoch 221/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 222/300, seasonal_1 Loss: 0.1352 | 0.1458
Epoch 223/300, seasonal_1 Loss: 0.1351 | 0.1458
Epoch 224/300, seasonal_1 Loss: 0.1346 | 0.1457
Epoch 225/300, seasonal_1 Loss: 0.1353 | 0.1458
Epoch 226/300, seasonal_1 Loss: 0.1349 | 0.1458
Epoch 227/300, seasonal_1 Loss: 0.1347 | 0.1458
Epoch 228/300, seasonal_1 Loss: 0.1347 | 0.1458
Epoch 229/300, seasonal_1 Loss: 0.1350 | 0.1458
Epoch 230/300, seasonal_1 Loss: 0.1342 | 0.1458
Epoch 231/300, seasonal_1 Loss: 0.1344 | 0.1458
Epoch 232/300, seasonal_1 Loss: 0.1346 | 0.1458
Epoch 233/300, seasonal_1 Loss: 0.1346 | 0.1458
Epoch 234/300, seasonal_1 Loss: 0.1341 | 0.1458
Epoch 235/300, seasonal_1 Loss: 0.1348 | 0.1458
Epoch 236/300, seasonal_1 Loss: 0.1347 | 0.1458
Epoch 237/300, seasonal_1 Loss: 0.1346 | 0.1458
Epoch 238/300, seasonal_1 Loss: 0.1352 | 0.1457
Epoch 239/300, seasonal_1 Loss: 0.1345 | 0.1457
Epoch 240/300, seasonal_1 Loss: 0.1349 | 0.1457
Epoch 241/300, seasonal_1 Loss: 0.1347 | 0.1457
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 252, 'train_rates': 0.9565631974326987, 'learning_rate': 4.7662824173442816e-05, 'batch_size': 24, 'step_size': 4, 'gamma': 0.92851798592133}
Epoch 1/300, seasonal_2 Loss: 0.7189 | 0.6645
Epoch 2/300, seasonal_2 Loss: 0.4751 | 0.5090
Epoch 3/300, seasonal_2 Loss: 0.4050 | 0.5443
Epoch 4/300, seasonal_2 Loss: 0.3549 | 0.3995
Epoch 5/300, seasonal_2 Loss: 0.3034 | 0.3353
Epoch 6/300, seasonal_2 Loss: 0.2780 | 0.3832
Epoch 7/300, seasonal_2 Loss: 0.2688 | 0.2972
Epoch 8/300, seasonal_2 Loss: 0.2457 | 0.2777
Epoch 9/300, seasonal_2 Loss: 0.2506 | 0.2944
Epoch 10/300, seasonal_2 Loss: 0.2305 | 0.2689
Epoch 11/300, seasonal_2 Loss: 0.2267 | 0.2496
Epoch 12/300, seasonal_2 Loss: 0.2423 | 0.2637
Epoch 13/300, seasonal_2 Loss: 0.2113 | 0.2264
Epoch 14/300, seasonal_2 Loss: 0.1985 | 0.2270
Epoch 15/300, seasonal_2 Loss: 0.1939 | 0.2009
Epoch 16/300, seasonal_2 Loss: 0.1890 | 0.2066
Epoch 17/300, seasonal_2 Loss: 0.1835 | 0.2106
Epoch 18/300, seasonal_2 Loss: 0.1807 | 0.1937
Epoch 19/300, seasonal_2 Loss: 0.1776 | 0.1832
Epoch 20/300, seasonal_2 Loss: 0.1719 | 0.1865
Epoch 21/300, seasonal_2 Loss: 0.1722 | 0.1806
Epoch 22/300, seasonal_2 Loss: 0.1707 | 0.1857
Epoch 23/300, seasonal_2 Loss: 0.1680 | 0.1692
Epoch 24/300, seasonal_2 Loss: 0.1671 | 0.1724
Epoch 25/300, seasonal_2 Loss: 0.1675 | 0.1728
Epoch 26/300, seasonal_2 Loss: 0.1652 | 0.1704
Epoch 27/300, seasonal_2 Loss: 0.1634 | 0.1599
Epoch 28/300, seasonal_2 Loss: 0.1619 | 0.1614
Epoch 29/300, seasonal_2 Loss: 0.1605 | 0.1603
Epoch 30/300, seasonal_2 Loss: 0.1561 | 0.1524
Epoch 31/300, seasonal_2 Loss: 0.1552 | 0.1586
Epoch 32/300, seasonal_2 Loss: 0.1543 | 0.1531
Epoch 33/300, seasonal_2 Loss: 0.1542 | 0.1492
Epoch 34/300, seasonal_2 Loss: 0.1520 | 0.1459
Epoch 35/300, seasonal_2 Loss: 0.1506 | 0.1529
Epoch 36/300, seasonal_2 Loss: 0.1498 | 0.1459
Epoch 37/300, seasonal_2 Loss: 0.1490 | 0.1384
Epoch 38/300, seasonal_2 Loss: 0.1476 | 0.1418
Epoch 39/300, seasonal_2 Loss: 0.1464 | 0.1453
Epoch 40/300, seasonal_2 Loss: 0.1454 | 0.1365
Epoch 41/300, seasonal_2 Loss: 0.1440 | 0.1325
Epoch 42/300, seasonal_2 Loss: 0.1434 | 0.1395
Epoch 43/300, seasonal_2 Loss: 0.1424 | 0.1384
Epoch 44/300, seasonal_2 Loss: 0.1416 | 0.1322
Epoch 45/300, seasonal_2 Loss: 0.1409 | 0.1312
Epoch 46/300, seasonal_2 Loss: 0.1395 | 0.1333
Epoch 47/300, seasonal_2 Loss: 0.1392 | 0.1329
Epoch 48/300, seasonal_2 Loss: 0.1388 | 0.1295
Epoch 49/300, seasonal_2 Loss: 0.1375 | 0.1288
Epoch 50/300, seasonal_2 Loss: 0.1359 | 0.1297
Epoch 51/300, seasonal_2 Loss: 0.1364 | 0.1278
Epoch 52/300, seasonal_2 Loss: 0.1357 | 0.1265
Epoch 53/300, seasonal_2 Loss: 0.1359 | 0.1263
Epoch 54/300, seasonal_2 Loss: 0.1346 | 0.1266
Epoch 55/300, seasonal_2 Loss: 0.1346 | 0.1263
Epoch 56/300, seasonal_2 Loss: 0.1342 | 0.1253
Epoch 57/300, seasonal_2 Loss: 0.1333 | 0.1255
Epoch 58/300, seasonal_2 Loss: 0.1334 | 0.1244
Epoch 59/300, seasonal_2 Loss: 0.1329 | 0.1241
Epoch 60/300, seasonal_2 Loss: 0.1322 | 0.1242
Epoch 61/300, seasonal_2 Loss: 0.1321 | 0.1228
Epoch 62/300, seasonal_2 Loss: 0.1315 | 0.1220
Epoch 63/300, seasonal_2 Loss: 0.1313 | 0.1220
Epoch 64/300, seasonal_2 Loss: 0.1313 | 0.1217
Epoch 65/300, seasonal_2 Loss: 0.1308 | 0.1211
Epoch 66/300, seasonal_2 Loss: 0.1307 | 0.1207
Epoch 67/300, seasonal_2 Loss: 0.1300 | 0.1213
Epoch 68/300, seasonal_2 Loss: 0.1297 | 0.1208
Epoch 69/300, seasonal_2 Loss: 0.1297 | 0.1202
Epoch 70/300, seasonal_2 Loss: 0.1291 | 0.1203
Epoch 71/300, seasonal_2 Loss: 0.1292 | 0.1201
Epoch 72/300, seasonal_2 Loss: 0.1288 | 0.1196
Epoch 73/300, seasonal_2 Loss: 0.1288 | 0.1189
Epoch 74/300, seasonal_2 Loss: 0.1279 | 0.1197
Epoch 75/300, seasonal_2 Loss: 0.1278 | 0.1184
Epoch 76/300, seasonal_2 Loss: 0.1277 | 0.1186
Epoch 77/300, seasonal_2 Loss: 0.1275 | 0.1188
Epoch 78/300, seasonal_2 Loss: 0.1270 | 0.1185
Epoch 79/300, seasonal_2 Loss: 0.1272 | 0.1183
Epoch 80/300, seasonal_2 Loss: 0.1268 | 0.1182
Epoch 81/300, seasonal_2 Loss: 0.1271 | 0.1179
Epoch 82/300, seasonal_2 Loss: 0.1270 | 0.1175
Epoch 83/300, seasonal_2 Loss: 0.1270 | 0.1186
Epoch 84/300, seasonal_2 Loss: 0.1258 | 0.1182
Epoch 85/300, seasonal_2 Loss: 0.1260 | 0.1177
Epoch 86/300, seasonal_2 Loss: 0.1263 | 0.1172
Epoch 87/300, seasonal_2 Loss: 0.1259 | 0.1176
Epoch 88/300, seasonal_2 Loss: 0.1261 | 0.1171
Epoch 89/300, seasonal_2 Loss: 0.1262 | 0.1174
Epoch 90/300, seasonal_2 Loss: 0.1257 | 0.1172
Epoch 91/300, seasonal_2 Loss: 0.1257 | 0.1168
Epoch 92/300, seasonal_2 Loss: 0.1252 | 0.1167
Epoch 93/300, seasonal_2 Loss: 0.1253 | 0.1167
Epoch 94/300, seasonal_2 Loss: 0.1256 | 0.1167
Epoch 95/300, seasonal_2 Loss: 0.1256 | 0.1162
Epoch 96/300, seasonal_2 Loss: 0.1248 | 0.1160
Epoch 97/300, seasonal_2 Loss: 0.1251 | 0.1160
Epoch 98/300, seasonal_2 Loss: 0.1246 | 0.1162
Epoch 99/300, seasonal_2 Loss: 0.1247 | 0.1165
Epoch 100/300, seasonal_2 Loss: 0.1249 | 0.1162
Epoch 101/300, seasonal_2 Loss: 0.1246 | 0.1161
Epoch 102/300, seasonal_2 Loss: 0.1248 | 0.1161
Epoch 103/300, seasonal_2 Loss: 0.1254 | 0.1163
Epoch 104/300, seasonal_2 Loss: 0.1251 | 0.1161
Epoch 105/300, seasonal_2 Loss: 0.1246 | 0.1159
Epoch 106/300, seasonal_2 Loss: 0.1247 | 0.1158
Epoch 107/300, seasonal_2 Loss: 0.1247 | 0.1159
Epoch 108/300, seasonal_2 Loss: 0.1239 | 0.1157
Epoch 109/300, seasonal_2 Loss: 0.1245 | 0.1158
Epoch 110/300, seasonal_2 Loss: 0.1243 | 0.1155
Epoch 111/300, seasonal_2 Loss: 0.1241 | 0.1154
Epoch 112/300, seasonal_2 Loss: 0.1235 | 0.1153
Epoch 113/300, seasonal_2 Loss: 0.1240 | 0.1153
Epoch 114/300, seasonal_2 Loss: 0.1235 | 0.1152
Epoch 115/300, seasonal_2 Loss: 0.1240 | 0.1153
Epoch 116/300, seasonal_2 Loss: 0.1248 | 0.1152
Epoch 117/300, seasonal_2 Loss: 0.1242 | 0.1150
Epoch 118/300, seasonal_2 Loss: 0.1237 | 0.1151
Epoch 119/300, seasonal_2 Loss: 0.1240 | 0.1152
Epoch 120/300, seasonal_2 Loss: 0.1237 | 0.1150
Epoch 121/300, seasonal_2 Loss: 0.1237 | 0.1149
Epoch 122/300, seasonal_2 Loss: 0.1238 | 0.1151
Epoch 123/300, seasonal_2 Loss: 0.1234 | 0.1151
Epoch 124/300, seasonal_2 Loss: 0.1236 | 0.1150
Epoch 125/300, seasonal_2 Loss: 0.1240 | 0.1150
Epoch 126/300, seasonal_2 Loss: 0.1236 | 0.1148
Epoch 127/300, seasonal_2 Loss: 0.1234 | 0.1150
Epoch 128/300, seasonal_2 Loss: 0.1236 | 0.1150
Epoch 129/300, seasonal_2 Loss: 0.1235 | 0.1149
Epoch 130/300, seasonal_2 Loss: 0.1236 | 0.1147
Epoch 131/300, seasonal_2 Loss: 0.1240 | 0.1148
Epoch 132/300, seasonal_2 Loss: 0.1229 | 0.1147
Epoch 133/300, seasonal_2 Loss: 0.1232 | 0.1147
Epoch 134/300, seasonal_2 Loss: 0.1232 | 0.1146
Epoch 135/300, seasonal_2 Loss: 0.1231 | 0.1146
Epoch 136/300, seasonal_2 Loss: 0.1236 | 0.1146
Epoch 137/300, seasonal_2 Loss: 0.1238 | 0.1147
Epoch 138/300, seasonal_2 Loss: 0.1231 | 0.1146
Epoch 139/300, seasonal_2 Loss: 0.1231 | 0.1146
Epoch 140/300, seasonal_2 Loss: 0.1234 | 0.1147
Epoch 141/300, seasonal_2 Loss: 0.1238 | 0.1146
Epoch 142/300, seasonal_2 Loss: 0.1239 | 0.1145
Epoch 143/300, seasonal_2 Loss: 0.1229 | 0.1145
Epoch 144/300, seasonal_2 Loss: 0.1234 | 0.1145
Epoch 145/300, seasonal_2 Loss: 0.1230 | 0.1145
Epoch 146/300, seasonal_2 Loss: 0.1235 | 0.1145
Epoch 147/300, seasonal_2 Loss: 0.1232 | 0.1146
Epoch 148/300, seasonal_2 Loss: 0.1229 | 0.1146
Epoch 149/300, seasonal_2 Loss: 0.1230 | 0.1146
Epoch 150/300, seasonal_2 Loss: 0.1229 | 0.1146
Epoch 151/300, seasonal_2 Loss: 0.1234 | 0.1145
Epoch 152/300, seasonal_2 Loss: 0.1228 | 0.1145
Epoch 153/300, seasonal_2 Loss: 0.1231 | 0.1145
Epoch 154/300, seasonal_2 Loss: 0.1228 | 0.1145
Epoch 155/300, seasonal_2 Loss: 0.1231 | 0.1145
Epoch 156/300, seasonal_2 Loss: 0.1237 | 0.1145
Epoch 157/300, seasonal_2 Loss: 0.1226 | 0.1145
Epoch 158/300, seasonal_2 Loss: 0.1239 | 0.1145
Epoch 159/300, seasonal_2 Loss: 0.1230 | 0.1145
Epoch 160/300, seasonal_2 Loss: 0.1230 | 0.1144
Epoch 161/300, seasonal_2 Loss: 0.1232 | 0.1144
Epoch 162/300, seasonal_2 Loss: 0.1231 | 0.1144
Epoch 163/300, seasonal_2 Loss: 0.1235 | 0.1144
Epoch 164/300, seasonal_2 Loss: 0.1229 | 0.1144
Epoch 165/300, seasonal_2 Loss: 0.1231 | 0.1144
Epoch 166/300, seasonal_2 Loss: 0.1231 | 0.1144
Epoch 167/300, seasonal_2 Loss: 0.1233 | 0.1144
Epoch 168/300, seasonal_2 Loss: 0.1231 | 0.1144
Epoch 169/300, seasonal_2 Loss: 0.1232 | 0.1144
Epoch 170/300, seasonal_2 Loss: 0.1236 | 0.1143
Epoch 171/300, seasonal_2 Loss: 0.1232 | 0.1143
Epoch 172/300, seasonal_2 Loss: 0.1233 | 0.1143
Epoch 173/300, seasonal_2 Loss: 0.1231 | 0.1143
Epoch 174/300, seasonal_2 Loss: 0.1227 | 0.1143
Epoch 175/300, seasonal_2 Loss: 0.1224 | 0.1144
Epoch 176/300, seasonal_2 Loss: 0.1229 | 0.1144
Epoch 177/300, seasonal_2 Loss: 0.1232 | 0.1143
Epoch 178/300, seasonal_2 Loss: 0.1227 | 0.1143
Epoch 179/300, seasonal_2 Loss: 0.1230 | 0.1143
Epoch 180/300, seasonal_2 Loss: 0.1228 | 0.1143
Epoch 181/300, seasonal_2 Loss: 0.1228 | 0.1144
Epoch 182/300, seasonal_2 Loss: 0.1236 | 0.1144
Epoch 183/300, seasonal_2 Loss: 0.1226 | 0.1144
Epoch 184/300, seasonal_2 Loss: 0.1231 | 0.1144
Epoch 185/300, seasonal_2 Loss: 0.1230 | 0.1144
Epoch 186/300, seasonal_2 Loss: 0.1229 | 0.1144
Epoch 187/300, seasonal_2 Loss: 0.1228 | 0.1144
Epoch 188/300, seasonal_2 Loss: 0.1229 | 0.1144
Epoch 189/300, seasonal_2 Loss: 0.1235 | 0.1144
Epoch 190/300, seasonal_2 Loss: 0.1228 | 0.1143
Epoch 191/300, seasonal_2 Loss: 0.1227 | 0.1143
Epoch 192/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 193/300, seasonal_2 Loss: 0.1228 | 0.1143
Epoch 194/300, seasonal_2 Loss: 0.1231 | 0.1143
Epoch 195/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 196/300, seasonal_2 Loss: 0.1225 | 0.1143
Epoch 197/300, seasonal_2 Loss: 0.1230 | 0.1143
Epoch 198/300, seasonal_2 Loss: 0.1234 | 0.1143
Epoch 199/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 200/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 201/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 202/300, seasonal_2 Loss: 0.1233 | 0.1143
Epoch 203/300, seasonal_2 Loss: 0.1225 | 0.1143
Epoch 204/300, seasonal_2 Loss: 0.1232 | 0.1143
Epoch 205/300, seasonal_2 Loss: 0.1231 | 0.1143
Epoch 206/300, seasonal_2 Loss: 0.1231 | 0.1143
Epoch 207/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 208/300, seasonal_2 Loss: 0.1228 | 0.1143
Epoch 209/300, seasonal_2 Loss: 0.1228 | 0.1143
Epoch 210/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 211/300, seasonal_2 Loss: 0.1225 | 0.1143
Epoch 212/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 213/300, seasonal_2 Loss: 0.1231 | 0.1143
Epoch 214/300, seasonal_2 Loss: 0.1225 | 0.1143
Epoch 215/300, seasonal_2 Loss: 0.1236 | 0.1143
Epoch 216/300, seasonal_2 Loss: 0.1238 | 0.1143
Epoch 217/300, seasonal_2 Loss: 0.1227 | 0.1143
Epoch 218/300, seasonal_2 Loss: 0.1230 | 0.1143
Epoch 219/300, seasonal_2 Loss: 0.1230 | 0.1143
Epoch 220/300, seasonal_2 Loss: 0.1229 | 0.1143
Epoch 221/300, seasonal_2 Loss: 0.1228 | 0.1143
Epoch 222/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 223/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 224/300, seasonal_2 Loss: 0.1234 | 0.1142
Epoch 225/300, seasonal_2 Loss: 0.1223 | 0.1142
Epoch 226/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 227/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 228/300, seasonal_2 Loss: 0.1235 | 0.1142
Epoch 229/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 230/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 231/300, seasonal_2 Loss: 0.1224 | 0.1142
Epoch 232/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 233/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 234/300, seasonal_2 Loss: 0.1226 | 0.1142
Epoch 235/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 236/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 237/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 238/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 239/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 240/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 241/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 242/300, seasonal_2 Loss: 0.1232 | 0.1142
Epoch 243/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 244/300, seasonal_2 Loss: 0.1235 | 0.1142
Epoch 245/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 246/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 247/300, seasonal_2 Loss: 0.1232 | 0.1142
Epoch 248/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 249/300, seasonal_2 Loss: 0.1232 | 0.1142
Epoch 250/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 251/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 252/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 253/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 254/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 255/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 256/300, seasonal_2 Loss: 0.1226 | 0.1142
Epoch 257/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 258/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 259/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 260/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 261/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 262/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 263/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 264/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 265/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 266/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 267/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 268/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 269/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 270/300, seasonal_2 Loss: 0.1226 | 0.1142
Epoch 271/300, seasonal_2 Loss: 0.1233 | 0.1142
Epoch 272/300, seasonal_2 Loss: 0.1226 | 0.1142
Epoch 273/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 274/300, seasonal_2 Loss: 0.1234 | 0.1142
Epoch 275/300, seasonal_2 Loss: 0.1224 | 0.1142
Epoch 276/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 277/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 278/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 279/300, seasonal_2 Loss: 0.1223 | 0.1142
Epoch 280/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 281/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 282/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 283/300, seasonal_2 Loss: 0.1235 | 0.1142
Epoch 284/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 285/300, seasonal_2 Loss: 0.1228 | 0.1142
Epoch 286/300, seasonal_2 Loss: 0.1226 | 0.1142
Epoch 287/300, seasonal_2 Loss: 0.1232 | 0.1142
Epoch 288/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 289/300, seasonal_2 Loss: 0.1226 | 0.1142
Epoch 290/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 291/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 292/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 293/300, seasonal_2 Loss: 0.1231 | 0.1142
Epoch 294/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 295/300, seasonal_2 Loss: 0.1230 | 0.1142
Epoch 296/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 297/300, seasonal_2 Loss: 0.1229 | 0.1142
Epoch 298/300, seasonal_2 Loss: 0.1227 | 0.1142
Epoch 299/300, seasonal_2 Loss: 0.1225 | 0.1142
Epoch 300/300, seasonal_2 Loss: 0.1234 | 0.1142
Training seasonal_3 component with params: {'observation_period_num': 45, 'train_rates': 0.9869646334596911, 'learning_rate': 0.00017110817008673173, 'batch_size': 16, 'step_size': 13, 'gamma': 0.772653922539655}
Epoch 1/300, seasonal_3 Loss: 0.4468 | 0.3399
Epoch 2/300, seasonal_3 Loss: 0.4605 | 0.3259
Epoch 3/300, seasonal_3 Loss: 0.4310 | 0.4257
Epoch 4/300, seasonal_3 Loss: 0.4225 | 0.4626
Epoch 5/300, seasonal_3 Loss: 0.4029 | 0.3879
Epoch 6/300, seasonal_3 Loss: 0.3692 | 0.3364
Epoch 7/300, seasonal_3 Loss: 0.3533 | 0.4225
Epoch 8/300, seasonal_3 Loss: 0.3637 | 0.3152
Epoch 9/300, seasonal_3 Loss: 0.3148 | 0.2339
Epoch 10/300, seasonal_3 Loss: 0.2707 | 0.2565
Epoch 11/300, seasonal_3 Loss: 0.2470 | 0.2587
Epoch 12/300, seasonal_3 Loss: 0.2353 | 0.2509
Epoch 13/300, seasonal_3 Loss: 0.2097 | 0.2433
Epoch 14/300, seasonal_3 Loss: 0.2016 | 0.2244
Epoch 15/300, seasonal_3 Loss: 0.1944 | 0.2254
Epoch 16/300, seasonal_3 Loss: 0.1825 | 0.2036
Epoch 17/300, seasonal_3 Loss: 0.1775 | 0.2082
Epoch 18/300, seasonal_3 Loss: 0.1715 | 0.2110
Epoch 19/300, seasonal_3 Loss: 0.1654 | 0.2371
Epoch 20/300, seasonal_3 Loss: 0.1648 | 0.1789
Epoch 21/300, seasonal_3 Loss: 0.1627 | 0.1837
Epoch 22/300, seasonal_3 Loss: 0.1572 | 0.2002
Epoch 23/300, seasonal_3 Loss: 0.1511 | 0.2015
Epoch 24/300, seasonal_3 Loss: 0.1500 | 0.2180
Epoch 25/300, seasonal_3 Loss: 0.1402 | 0.1776
Epoch 26/300, seasonal_3 Loss: 0.1363 | 0.1550
Epoch 27/300, seasonal_3 Loss: 0.1317 | 0.1543
Epoch 28/300, seasonal_3 Loss: 0.1280 | 0.1497
Epoch 29/300, seasonal_3 Loss: 0.1269 | 0.1424
Epoch 30/300, seasonal_3 Loss: 0.1284 | 0.1379
Epoch 31/300, seasonal_3 Loss: 0.1278 | 0.1413
Epoch 32/300, seasonal_3 Loss: 0.1234 | 0.1559
Epoch 33/300, seasonal_3 Loss: 0.1218 | 0.1375
Epoch 34/300, seasonal_3 Loss: 0.1220 | 0.1482
Epoch 35/300, seasonal_3 Loss: 0.1168 | 0.1634
Epoch 36/300, seasonal_3 Loss: 0.1158 | 0.1429
Epoch 37/300, seasonal_3 Loss: 0.1118 | 0.1204
Epoch 38/300, seasonal_3 Loss: 0.1103 | 0.1448
Epoch 39/300, seasonal_3 Loss: 0.1101 | 0.1517
Epoch 40/300, seasonal_3 Loss: 0.1129 | 0.1429
Epoch 41/300, seasonal_3 Loss: 0.1085 | 0.1283
Epoch 42/300, seasonal_3 Loss: 0.1098 | 0.1335
Epoch 43/300, seasonal_3 Loss: 0.1081 | 0.1469
Epoch 44/300, seasonal_3 Loss: 0.1069 | 0.1330
Epoch 45/300, seasonal_3 Loss: 0.1044 | 0.1181
Epoch 46/300, seasonal_3 Loss: 0.1027 | 0.1100
Epoch 47/300, seasonal_3 Loss: 0.1010 | 0.1377
Epoch 48/300, seasonal_3 Loss: 0.0998 | 0.1792
Epoch 49/300, seasonal_3 Loss: 0.0976 | 0.1245
Epoch 50/300, seasonal_3 Loss: 0.0960 | 0.1168
Epoch 51/300, seasonal_3 Loss: 0.0939 | 0.1109
Epoch 52/300, seasonal_3 Loss: 0.0930 | 0.1361
Epoch 53/300, seasonal_3 Loss: 0.0934 | 0.1082
Epoch 54/300, seasonal_3 Loss: 0.0909 | 0.1053
Epoch 55/300, seasonal_3 Loss: 0.0886 | 0.1098
Epoch 56/300, seasonal_3 Loss: 0.0864 | 0.1211
Epoch 57/300, seasonal_3 Loss: 0.0875 | 0.1126
Epoch 58/300, seasonal_3 Loss: 0.0866 | 0.1044
Epoch 59/300, seasonal_3 Loss: 0.0850 | 0.1090
Epoch 60/300, seasonal_3 Loss: 0.0852 | 0.1136
Epoch 61/300, seasonal_3 Loss: 0.0839 | 0.1019
Epoch 62/300, seasonal_3 Loss: 0.0831 | 0.1058
Epoch 63/300, seasonal_3 Loss: 0.0827 | 0.1098
Epoch 64/300, seasonal_3 Loss: 0.0829 | 0.1115
Epoch 65/300, seasonal_3 Loss: 0.0821 | 0.1014
Epoch 66/300, seasonal_3 Loss: 0.0814 | 0.1051
Epoch 67/300, seasonal_3 Loss: 0.0812 | 0.1040
Epoch 68/300, seasonal_3 Loss: 0.0809 | 0.1090
Epoch 69/300, seasonal_3 Loss: 0.0799 | 0.1025
Epoch 70/300, seasonal_3 Loss: 0.0796 | 0.1009
Epoch 71/300, seasonal_3 Loss: 0.0797 | 0.1026
Epoch 72/300, seasonal_3 Loss: 0.0794 | 0.1117
Epoch 73/300, seasonal_3 Loss: 0.0786 | 0.1001
Epoch 74/300, seasonal_3 Loss: 0.0782 | 0.1020
Epoch 75/300, seasonal_3 Loss: 0.0782 | 0.1024
Epoch 76/300, seasonal_3 Loss: 0.0771 | 0.1054
Epoch 77/300, seasonal_3 Loss: 0.0770 | 0.1032
Epoch 78/300, seasonal_3 Loss: 0.0770 | 0.1029
Epoch 79/300, seasonal_3 Loss: 0.0763 | 0.1015
Epoch 80/300, seasonal_3 Loss: 0.0759 | 0.1005
Epoch 81/300, seasonal_3 Loss: 0.0762 | 0.1041
Epoch 82/300, seasonal_3 Loss: 0.0765 | 0.0981
Epoch 83/300, seasonal_3 Loss: 0.0762 | 0.1003
Epoch 84/300, seasonal_3 Loss: 0.0759 | 0.0982
Epoch 85/300, seasonal_3 Loss: 0.0758 | 0.0995
Epoch 86/300, seasonal_3 Loss: 0.0746 | 0.0991
Epoch 87/300, seasonal_3 Loss: 0.0751 | 0.0981
Epoch 88/300, seasonal_3 Loss: 0.0756 | 0.0986
Epoch 89/300, seasonal_3 Loss: 0.0744 | 0.0969
Epoch 90/300, seasonal_3 Loss: 0.0747 | 0.0980
Epoch 91/300, seasonal_3 Loss: 0.0746 | 0.0985
Epoch 92/300, seasonal_3 Loss: 0.0738 | 0.0984
Epoch 93/300, seasonal_3 Loss: 0.0740 | 0.0999
Epoch 94/300, seasonal_3 Loss: 0.0735 | 0.0979
Epoch 95/300, seasonal_3 Loss: 0.0731 | 0.0985
Epoch 96/300, seasonal_3 Loss: 0.0730 | 0.0988
Epoch 97/300, seasonal_3 Loss: 0.0736 | 0.0982
Epoch 98/300, seasonal_3 Loss: 0.0729 | 0.0983
Epoch 99/300, seasonal_3 Loss: 0.0736 | 0.0972
Epoch 100/300, seasonal_3 Loss: 0.0730 | 0.0973
Epoch 101/300, seasonal_3 Loss: 0.0732 | 0.0992
Epoch 102/300, seasonal_3 Loss: 0.0728 | 0.0980
Epoch 103/300, seasonal_3 Loss: 0.0729 | 0.0991
Epoch 104/300, seasonal_3 Loss: 0.0726 | 0.0971
Epoch 105/300, seasonal_3 Loss: 0.0730 | 0.0969
Epoch 106/300, seasonal_3 Loss: 0.0720 | 0.0991
Epoch 107/300, seasonal_3 Loss: 0.0730 | 0.0995
Epoch 108/300, seasonal_3 Loss: 0.0725 | 0.0993
Epoch 109/300, seasonal_3 Loss: 0.0717 | 0.0987
Epoch 110/300, seasonal_3 Loss: 0.0725 | 0.0982
Epoch 111/300, seasonal_3 Loss: 0.0721 | 0.0991
Epoch 112/300, seasonal_3 Loss: 0.0732 | 0.0969
Epoch 113/300, seasonal_3 Loss: 0.0718 | 0.0983
Epoch 114/300, seasonal_3 Loss: 0.0720 | 0.0985
Epoch 115/300, seasonal_3 Loss: 0.0708 | 0.0974
Epoch 116/300, seasonal_3 Loss: 0.0724 | 0.0984
Epoch 117/300, seasonal_3 Loss: 0.0714 | 0.0981
Epoch 118/300, seasonal_3 Loss: 0.0723 | 0.0982
Epoch 119/300, seasonal_3 Loss: 0.0723 | 0.0980
Epoch 120/300, seasonal_3 Loss: 0.0714 | 0.0978
Epoch 121/300, seasonal_3 Loss: 0.0720 | 0.0971
Epoch 122/300, seasonal_3 Loss: 0.0721 | 0.0974
Epoch 123/300, seasonal_3 Loss: 0.0714 | 0.0982
Epoch 124/300, seasonal_3 Loss: 0.0708 | 0.0984
Epoch 125/300, seasonal_3 Loss: 0.0721 | 0.0982
Epoch 126/300, seasonal_3 Loss: 0.0708 | 0.0980
Epoch 127/300, seasonal_3 Loss: 0.0712 | 0.0977
Epoch 128/300, seasonal_3 Loss: 0.0713 | 0.0971
Epoch 129/300, seasonal_3 Loss: 0.0708 | 0.0974
Epoch 130/300, seasonal_3 Loss: 0.0709 | 0.0981
Epoch 131/300, seasonal_3 Loss: 0.0709 | 0.0976
Epoch 132/300, seasonal_3 Loss: 0.0714 | 0.0978
Epoch 133/300, seasonal_3 Loss: 0.0712 | 0.0981
Epoch 134/300, seasonal_3 Loss: 0.0714 | 0.0975
Epoch 135/300, seasonal_3 Loss: 0.0711 | 0.0977
Epoch 136/300, seasonal_3 Loss: 0.0714 | 0.0977
Epoch 137/300, seasonal_3 Loss: 0.0710 | 0.0976
Epoch 138/300, seasonal_3 Loss: 0.0710 | 0.0976
Epoch 139/300, seasonal_3 Loss: 0.0711 | 0.0980
Epoch 140/300, seasonal_3 Loss: 0.0708 | 0.0982
Epoch 141/300, seasonal_3 Loss: 0.0714 | 0.0979
Epoch 142/300, seasonal_3 Loss: 0.0712 | 0.0984
Epoch 143/300, seasonal_3 Loss: 0.0707 | 0.0982
Epoch 144/300, seasonal_3 Loss: 0.0707 | 0.0982
Epoch 145/300, seasonal_3 Loss: 0.0704 | 0.0985
Epoch 146/300, seasonal_3 Loss: 0.0708 | 0.0981
Epoch 147/300, seasonal_3 Loss: 0.0711 | 0.0982
Epoch 148/300, seasonal_3 Loss: 0.0703 | 0.0982
Epoch 149/300, seasonal_3 Loss: 0.0713 | 0.0978
Epoch 150/300, seasonal_3 Loss: 0.0707 | 0.0977
Epoch 151/300, seasonal_3 Loss: 0.0716 | 0.0979
Epoch 152/300, seasonal_3 Loss: 0.0709 | 0.0980
Epoch 153/300, seasonal_3 Loss: 0.0713 | 0.0978
Epoch 154/300, seasonal_3 Loss: 0.0716 | 0.0977
Epoch 155/300, seasonal_3 Loss: 0.0710 | 0.0973
Epoch 156/300, seasonal_3 Loss: 0.0704 | 0.0975
Epoch 157/300, seasonal_3 Loss: 0.0706 | 0.0977
Epoch 158/300, seasonal_3 Loss: 0.0707 | 0.0976
Epoch 159/300, seasonal_3 Loss: 0.0703 | 0.0974
Epoch 160/300, seasonal_3 Loss: 0.0709 | 0.0975
Epoch 161/300, seasonal_3 Loss: 0.0708 | 0.0978
Epoch 162/300, seasonal_3 Loss: 0.0706 | 0.0978
Epoch 163/300, seasonal_3 Loss: 0.0710 | 0.0978
Epoch 164/300, seasonal_3 Loss: 0.0708 | 0.0977
Epoch 165/300, seasonal_3 Loss: 0.0717 | 0.0976
Epoch 166/300, seasonal_3 Loss: 0.0709 | 0.0976
Epoch 167/300, seasonal_3 Loss: 0.0702 | 0.0977
Epoch 168/300, seasonal_3 Loss: 0.0706 | 0.0976
Epoch 169/300, seasonal_3 Loss: 0.0707 | 0.0974
Epoch 170/300, seasonal_3 Loss: 0.0698 | 0.0974
Epoch 171/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 172/300, seasonal_3 Loss: 0.0711 | 0.0974
Epoch 173/300, seasonal_3 Loss: 0.0716 | 0.0974
Epoch 174/300, seasonal_3 Loss: 0.0710 | 0.0975
Epoch 175/300, seasonal_3 Loss: 0.0706 | 0.0974
Epoch 176/300, seasonal_3 Loss: 0.0710 | 0.0974
Epoch 177/300, seasonal_3 Loss: 0.0715 | 0.0973
Epoch 178/300, seasonal_3 Loss: 0.0708 | 0.0975
Epoch 179/300, seasonal_3 Loss: 0.0706 | 0.0976
Epoch 180/300, seasonal_3 Loss: 0.0707 | 0.0978
Epoch 181/300, seasonal_3 Loss: 0.0712 | 0.0977
Epoch 182/300, seasonal_3 Loss: 0.0710 | 0.0977
Epoch 183/300, seasonal_3 Loss: 0.0710 | 0.0978
Epoch 184/300, seasonal_3 Loss: 0.0708 | 0.0978
Epoch 185/300, seasonal_3 Loss: 0.0708 | 0.0977
Epoch 186/300, seasonal_3 Loss: 0.0711 | 0.0976
Epoch 187/300, seasonal_3 Loss: 0.0710 | 0.0974
Epoch 188/300, seasonal_3 Loss: 0.0707 | 0.0974
Epoch 189/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 190/300, seasonal_3 Loss: 0.0702 | 0.0973
Epoch 191/300, seasonal_3 Loss: 0.0707 | 0.0973
Epoch 192/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 193/300, seasonal_3 Loss: 0.0713 | 0.0972
Epoch 194/300, seasonal_3 Loss: 0.0701 | 0.0972
Epoch 195/300, seasonal_3 Loss: 0.0715 | 0.0972
Epoch 196/300, seasonal_3 Loss: 0.0703 | 0.0972
Epoch 197/300, seasonal_3 Loss: 0.0707 | 0.0973
Epoch 198/300, seasonal_3 Loss: 0.0707 | 0.0973
Epoch 199/300, seasonal_3 Loss: 0.0705 | 0.0973
Epoch 200/300, seasonal_3 Loss: 0.0705 | 0.0973
Epoch 201/300, seasonal_3 Loss: 0.0713 | 0.0974
Epoch 202/300, seasonal_3 Loss: 0.0706 | 0.0974
Epoch 203/300, seasonal_3 Loss: 0.0707 | 0.0974
Epoch 204/300, seasonal_3 Loss: 0.0700 | 0.0974
Epoch 205/300, seasonal_3 Loss: 0.0706 | 0.0974
Epoch 206/300, seasonal_3 Loss: 0.0706 | 0.0974
Epoch 207/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 208/300, seasonal_3 Loss: 0.0707 | 0.0974
Epoch 209/300, seasonal_3 Loss: 0.0717 | 0.0974
Epoch 210/300, seasonal_3 Loss: 0.0704 | 0.0974
Epoch 211/300, seasonal_3 Loss: 0.0704 | 0.0974
Epoch 212/300, seasonal_3 Loss: 0.0712 | 0.0974
Epoch 213/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 214/300, seasonal_3 Loss: 0.0708 | 0.0974
Epoch 215/300, seasonal_3 Loss: 0.0706 | 0.0974
Epoch 216/300, seasonal_3 Loss: 0.0703 | 0.0974
Epoch 217/300, seasonal_3 Loss: 0.0713 | 0.0974
Epoch 218/300, seasonal_3 Loss: 0.0708 | 0.0974
Epoch 219/300, seasonal_3 Loss: 0.0701 | 0.0974
Epoch 220/300, seasonal_3 Loss: 0.0700 | 0.0974
Epoch 221/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 222/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 223/300, seasonal_3 Loss: 0.0708 | 0.0974
Epoch 224/300, seasonal_3 Loss: 0.0703 | 0.0974
Epoch 225/300, seasonal_3 Loss: 0.0703 | 0.0974
Epoch 226/300, seasonal_3 Loss: 0.0712 | 0.0974
Epoch 227/300, seasonal_3 Loss: 0.0708 | 0.0974
Epoch 228/300, seasonal_3 Loss: 0.0694 | 0.0974
Epoch 229/300, seasonal_3 Loss: 0.0711 | 0.0974
Epoch 230/300, seasonal_3 Loss: 0.0707 | 0.0974
Epoch 231/300, seasonal_3 Loss: 0.0712 | 0.0974
Epoch 232/300, seasonal_3 Loss: 0.0710 | 0.0974
Epoch 233/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 234/300, seasonal_3 Loss: 0.0702 | 0.0974
Epoch 235/300, seasonal_3 Loss: 0.0705 | 0.0974
Epoch 236/300, seasonal_3 Loss: 0.0706 | 0.0974
Epoch 237/300, seasonal_3 Loss: 0.0704 | 0.0974
Epoch 238/300, seasonal_3 Loss: 0.0705 | 0.0974
Epoch 239/300, seasonal_3 Loss: 0.0704 | 0.0974
Epoch 240/300, seasonal_3 Loss: 0.0709 | 0.0974
Epoch 241/300, seasonal_3 Loss: 0.0713 | 0.0973
Epoch 242/300, seasonal_3 Loss: 0.0707 | 0.0973
Epoch 243/300, seasonal_3 Loss: 0.0702 | 0.0973
Epoch 244/300, seasonal_3 Loss: 0.0705 | 0.0973
Epoch 245/300, seasonal_3 Loss: 0.0711 | 0.0973
Epoch 246/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 247/300, seasonal_3 Loss: 0.0701 | 0.0973
Epoch 248/300, seasonal_3 Loss: 0.0705 | 0.0973
Epoch 249/300, seasonal_3 Loss: 0.0703 | 0.0973
Epoch 250/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 251/300, seasonal_3 Loss: 0.0701 | 0.0973
Epoch 252/300, seasonal_3 Loss: 0.0702 | 0.0973
Epoch 253/300, seasonal_3 Loss: 0.0710 | 0.0973
Epoch 254/300, seasonal_3 Loss: 0.0700 | 0.0973
Epoch 255/300, seasonal_3 Loss: 0.0711 | 0.0973
Epoch 256/300, seasonal_3 Loss: 0.0699 | 0.0973
Epoch 257/300, seasonal_3 Loss: 0.0702 | 0.0973
Epoch 258/300, seasonal_3 Loss: 0.0707 | 0.0973
Epoch 259/300, seasonal_3 Loss: 0.0716 | 0.0973
Epoch 260/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 261/300, seasonal_3 Loss: 0.0709 | 0.0973
Epoch 262/300, seasonal_3 Loss: 0.0703 | 0.0973
Epoch 263/300, seasonal_3 Loss: 0.0709 | 0.0973
Epoch 264/300, seasonal_3 Loss: 0.0700 | 0.0973
Epoch 265/300, seasonal_3 Loss: 0.0714 | 0.0973
Epoch 266/300, seasonal_3 Loss: 0.0706 | 0.0973
Epoch 267/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 268/300, seasonal_3 Loss: 0.0702 | 0.0973
Epoch 269/300, seasonal_3 Loss: 0.0707 | 0.0973
Epoch 270/300, seasonal_3 Loss: 0.0702 | 0.0973
Epoch 271/300, seasonal_3 Loss: 0.0706 | 0.0973
Epoch 272/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 273/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 274/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 275/300, seasonal_3 Loss: 0.0701 | 0.0973
Epoch 276/300, seasonal_3 Loss: 0.0713 | 0.0973
Epoch 277/300, seasonal_3 Loss: 0.0703 | 0.0973
Epoch 278/300, seasonal_3 Loss: 0.0706 | 0.0973
Epoch 279/300, seasonal_3 Loss: 0.0713 | 0.0973
Epoch 280/300, seasonal_3 Loss: 0.0713 | 0.0973
Epoch 281/300, seasonal_3 Loss: 0.0711 | 0.0973
Epoch 282/300, seasonal_3 Loss: 0.0705 | 0.0973
Epoch 283/300, seasonal_3 Loss: 0.0703 | 0.0973
Epoch 284/300, seasonal_3 Loss: 0.0713 | 0.0973
Epoch 285/300, seasonal_3 Loss: 0.0709 | 0.0973
Epoch 286/300, seasonal_3 Loss: 0.0705 | 0.0973
Epoch 287/300, seasonal_3 Loss: 0.0706 | 0.0973
Epoch 288/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 289/300, seasonal_3 Loss: 0.0709 | 0.0973
Epoch 290/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 291/300, seasonal_3 Loss: 0.0716 | 0.0973
Epoch 292/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 293/300, seasonal_3 Loss: 0.0710 | 0.0973
Epoch 294/300, seasonal_3 Loss: 0.0703 | 0.0973
Epoch 295/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 296/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 297/300, seasonal_3 Loss: 0.0708 | 0.0973
Epoch 298/300, seasonal_3 Loss: 0.0704 | 0.0973
Epoch 299/300, seasonal_3 Loss: 0.0703 | 0.0973
Epoch 300/300, seasonal_3 Loss: 0.0703 | 0.0973
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9861731892976261, 'learning_rate': 8.543098257861018e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9517939292934263}
Epoch 1/300, resid Loss: 0.6825 | 0.7853
Epoch 2/300, resid Loss: 0.5530 | 0.5903
Epoch 3/300, resid Loss: 0.4202 | 0.5213
Epoch 4/300, resid Loss: 0.3276 | 0.4730
Epoch 5/300, resid Loss: 0.3158 | 0.4351
Epoch 6/300, resid Loss: 0.2718 | 0.3348
Epoch 7/300, resid Loss: 0.2720 | 0.3730
Epoch 8/300, resid Loss: 0.2696 | 0.3223
Epoch 9/300, resid Loss: 0.2568 | 0.3387
Epoch 10/300, resid Loss: 0.2572 | 0.2859
Epoch 11/300, resid Loss: 0.2413 | 0.3007
Epoch 12/300, resid Loss: 0.2123 | 0.2413
Epoch 13/300, resid Loss: 0.2143 | 0.2399
Epoch 14/300, resid Loss: 0.2242 | 0.2799
Epoch 15/300, resid Loss: 0.2125 | 0.2412
Epoch 16/300, resid Loss: 0.2365 | 0.2403
Epoch 17/300, resid Loss: 0.2333 | 0.2385
Epoch 18/300, resid Loss: 0.2199 | 0.2234
Epoch 19/300, resid Loss: 0.1998 | 0.2102
Epoch 20/300, resid Loss: 0.1830 | 0.1909
Epoch 21/300, resid Loss: 0.1867 | 0.1957
Epoch 22/300, resid Loss: 0.1775 | 0.2649
Epoch 23/300, resid Loss: 0.1719 | 0.1744
Epoch 24/300, resid Loss: 0.1761 | 0.2463
Epoch 25/300, resid Loss: 0.1997 | 0.1782
Epoch 26/300, resid Loss: 0.1903 | 0.3021
Epoch 27/300, resid Loss: 0.1878 | 0.2215
Epoch 28/300, resid Loss: 0.1856 | 0.2217
Epoch 29/300, resid Loss: 0.1786 | 0.1752
Epoch 30/300, resid Loss: 0.1836 | 0.2874
Epoch 31/300, resid Loss: 0.1610 | 0.1920
Epoch 32/300, resid Loss: 0.1521 | 0.1824
Epoch 33/300, resid Loss: 0.1537 | 0.1562
Epoch 34/300, resid Loss: 0.1496 | 0.1807
Epoch 35/300, resid Loss: 0.1395 | 0.1340
Epoch 36/300, resid Loss: 0.1394 | 0.1491
Epoch 37/300, resid Loss: 0.1456 | 0.2026
Epoch 38/300, resid Loss: 0.1324 | 0.1377
Epoch 39/300, resid Loss: 0.1316 | 0.1384
Epoch 40/300, resid Loss: 0.1328 | 0.1522
Epoch 41/300, resid Loss: 0.1265 | 0.1685
Epoch 42/300, resid Loss: 0.1273 | 0.1213
Epoch 43/300, resid Loss: 0.1330 | 0.1735
Epoch 44/300, resid Loss: 0.1341 | 0.1602
Epoch 45/300, resid Loss: 0.1290 | 0.1217
Epoch 46/300, resid Loss: 0.1435 | 0.1639
Epoch 47/300, resid Loss: 0.1264 | 0.1297
Epoch 48/300, resid Loss: 0.1227 | 0.1457
Epoch 49/300, resid Loss: 0.1372 | 0.1191
Epoch 50/300, resid Loss: 0.1315 | 0.1625
Epoch 51/300, resid Loss: 0.1294 | 0.1658
Epoch 52/300, resid Loss: 0.1248 | 0.1603
Epoch 53/300, resid Loss: 0.1349 | 0.1296
Epoch 54/300, resid Loss: 0.1175 | 0.1175
Epoch 55/300, resid Loss: 0.1207 | 0.2130
Epoch 56/300, resid Loss: 0.1236 | 0.1237
Epoch 57/300, resid Loss: 0.1268 | 0.1086
Epoch 58/300, resid Loss: 0.1169 | 0.1636
Epoch 59/300, resid Loss: 0.1117 | 0.0986
Epoch 60/300, resid Loss: 0.1063 | 0.1186
Epoch 61/300, resid Loss: 0.1104 | 0.1137
Epoch 62/300, resid Loss: 0.1005 | 0.1095
Epoch 63/300, resid Loss: 0.0982 | 0.1097
Epoch 64/300, resid Loss: 0.1026 | 0.1150
Epoch 65/300, resid Loss: 0.1014 | 0.1045
Epoch 66/300, resid Loss: 0.0975 | 0.0878
Epoch 67/300, resid Loss: 0.1016 | 0.1403
Epoch 68/300, resid Loss: 0.1018 | 0.0964
Epoch 69/300, resid Loss: 0.0976 | 0.0989
Epoch 70/300, resid Loss: 0.1049 | 0.1011
Epoch 71/300, resid Loss: 0.0964 | 0.1201
Epoch 72/300, resid Loss: 0.0980 | 0.1249
Epoch 73/300, resid Loss: 0.0961 | 0.1069
Epoch 74/300, resid Loss: 0.1060 | 0.1234
Epoch 75/300, resid Loss: 0.1019 | 0.1036
Epoch 76/300, resid Loss: 0.1010 | 0.1048
Epoch 77/300, resid Loss: 0.0898 | 0.0871
Epoch 78/300, resid Loss: 0.0922 | 0.0904
Epoch 79/300, resid Loss: 0.0892 | 0.1287
Epoch 80/300, resid Loss: 0.0946 | 0.0891
Epoch 81/300, resid Loss: 0.0919 | 0.0899
Epoch 82/300, resid Loss: 0.0916 | 0.0820
Epoch 83/300, resid Loss: 0.1042 | 0.1546
Epoch 84/300, resid Loss: 0.0927 | 0.0978
Epoch 85/300, resid Loss: 0.0919 | 0.0881
Epoch 86/300, resid Loss: 0.0942 | 0.0981
Epoch 87/300, resid Loss: 0.0881 | 0.1133
Epoch 88/300, resid Loss: 0.0957 | 0.1114
Epoch 89/300, resid Loss: 0.0939 | 0.1023
Epoch 90/300, resid Loss: 0.0978 | 0.1062
Epoch 91/300, resid Loss: 0.0921 | 0.1326
Epoch 92/300, resid Loss: 0.0828 | 0.0799
Epoch 93/300, resid Loss: 0.0823 | 0.1098
Epoch 94/300, resid Loss: 0.0799 | 0.0857
Epoch 95/300, resid Loss: 0.0767 | 0.0781
Epoch 96/300, resid Loss: 0.0756 | 0.1018
Epoch 97/300, resid Loss: 0.0774 | 0.0967
Epoch 98/300, resid Loss: 0.0764 | 0.0822
Epoch 99/300, resid Loss: 0.0786 | 0.0806
Epoch 100/300, resid Loss: 0.0788 | 0.1114
Epoch 101/300, resid Loss: 0.0775 | 0.0803
Epoch 102/300, resid Loss: 0.0762 | 0.0715
Epoch 103/300, resid Loss: 0.0727 | 0.0772
Epoch 104/300, resid Loss: 0.0711 | 0.0907
Epoch 105/300, resid Loss: 0.0729 | 0.0880
Epoch 106/300, resid Loss: 0.0728 | 0.0748
Epoch 107/300, resid Loss: 0.0705 | 0.0783
Epoch 108/300, resid Loss: 0.0743 | 0.0983
Epoch 109/300, resid Loss: 0.0686 | 0.0749
Epoch 110/300, resid Loss: 0.0692 | 0.0711
Epoch 111/300, resid Loss: 0.0688 | 0.0881
Epoch 112/300, resid Loss: 0.0676 | 0.0861
Epoch 113/300, resid Loss: 0.0667 | 0.0686
Epoch 114/300, resid Loss: 0.0691 | 0.0728
Epoch 115/300, resid Loss: 0.0694 | 0.0982
Epoch 116/300, resid Loss: 0.0690 | 0.0808
Epoch 117/300, resid Loss: 0.0720 | 0.0761
Epoch 118/300, resid Loss: 0.0722 | 0.0839
Epoch 119/300, resid Loss: 0.0776 | 0.1089
Epoch 120/300, resid Loss: 0.0745 | 0.0745
Epoch 121/300, resid Loss: 0.0706 | 0.0710
Epoch 122/300, resid Loss: 0.0751 | 0.0762
Epoch 123/300, resid Loss: 0.0685 | 0.1056
Epoch 124/300, resid Loss: 0.0656 | 0.0698
Epoch 125/300, resid Loss: 0.0646 | 0.0697
Epoch 126/300, resid Loss: 0.0639 | 0.0941
Epoch 127/300, resid Loss: 0.0632 | 0.0720
Epoch 128/300, resid Loss: 0.0635 | 0.0697
Epoch 129/300, resid Loss: 0.0629 | 0.0698
Epoch 130/300, resid Loss: 0.0634 | 0.0886
Epoch 131/300, resid Loss: 0.0632 | 0.0791
Epoch 132/300, resid Loss: 0.0637 | 0.0643
Epoch 133/300, resid Loss: 0.0661 | 0.0843
Epoch 134/300, resid Loss: 0.0664 | 0.0829
Epoch 135/300, resid Loss: 0.0654 | 0.0658
Epoch 136/300, resid Loss: 0.0641 | 0.0690
Epoch 137/300, resid Loss: 0.0646 | 0.0867
Epoch 138/300, resid Loss: 0.0605 | 0.0662
Epoch 139/300, resid Loss: 0.0588 | 0.0646
Epoch 140/300, resid Loss: 0.0597 | 0.0868
Epoch 141/300, resid Loss: 0.0592 | 0.0784
Epoch 142/300, resid Loss: 0.0601 | 0.0645
Epoch 143/300, resid Loss: 0.0600 | 0.0825
Epoch 144/300, resid Loss: 0.0637 | 0.0786
Epoch 145/300, resid Loss: 0.0644 | 0.0696
Epoch 146/300, resid Loss: 0.0609 | 0.0664
Epoch 147/300, resid Loss: 0.0625 | 0.0851
Epoch 148/300, resid Loss: 0.0599 | 0.0742
Epoch 149/300, resid Loss: 0.0582 | 0.0672
Epoch 150/300, resid Loss: 0.0579 | 0.0674
Epoch 151/300, resid Loss: 0.0584 | 0.0788
Epoch 152/300, resid Loss: 0.0579 | 0.0727
Epoch 153/300, resid Loss: 0.0561 | 0.0625
Epoch 154/300, resid Loss: 0.0574 | 0.0775
Epoch 155/300, resid Loss: 0.0557 | 0.0737
Epoch 156/300, resid Loss: 0.0546 | 0.0625
Epoch 157/300, resid Loss: 0.0556 | 0.0668
Epoch 158/300, resid Loss: 0.0558 | 0.0791
Epoch 159/300, resid Loss: 0.0542 | 0.0625
Epoch 160/300, resid Loss: 0.0523 | 0.0644
Epoch 161/300, resid Loss: 0.0531 | 0.0704
Epoch 162/300, resid Loss: 0.0518 | 0.0646
Epoch 163/300, resid Loss: 0.0521 | 0.0581
Epoch 164/300, resid Loss: 0.0521 | 0.0678
Epoch 165/300, resid Loss: 0.0514 | 0.0718
Epoch 166/300, resid Loss: 0.0520 | 0.0651
Epoch 167/300, resid Loss: 0.0517 | 0.0608
Epoch 168/300, resid Loss: 0.0535 | 0.0709
Epoch 169/300, resid Loss: 0.0532 | 0.0733
Epoch 170/300, resid Loss: 0.0542 | 0.0666
Epoch 171/300, resid Loss: 0.0545 | 0.0641
Epoch 172/300, resid Loss: 0.0583 | 0.0833
Epoch 173/300, resid Loss: 0.0560 | 0.0768
Epoch 174/300, resid Loss: 0.0566 | 0.0612
Epoch 175/300, resid Loss: 0.0535 | 0.0715
Epoch 176/300, resid Loss: 0.0529 | 0.0705
Epoch 177/300, resid Loss: 0.0525 | 0.0594
Epoch 178/300, resid Loss: 0.0488 | 0.0623
Epoch 179/300, resid Loss: 0.0494 | 0.0710
Epoch 180/300, resid Loss: 0.0478 | 0.0637
Epoch 181/300, resid Loss: 0.0480 | 0.0573
Epoch 182/300, resid Loss: 0.0475 | 0.0661
Epoch 183/300, resid Loss: 0.0474 | 0.0649
Epoch 184/300, resid Loss: 0.0478 | 0.0583
Epoch 185/300, resid Loss: 0.0473 | 0.0652
Epoch 186/300, resid Loss: 0.0478 | 0.0697
Epoch 187/300, resid Loss: 0.0483 | 0.0628
Epoch 188/300, resid Loss: 0.0495 | 0.0601
Epoch 189/300, resid Loss: 0.0505 | 0.0771
Epoch 190/300, resid Loss: 0.0508 | 0.0695
Epoch 191/300, resid Loss: 0.0517 | 0.0599
Epoch 192/300, resid Loss: 0.0500 | 0.0675
Epoch 193/300, resid Loss: 0.0498 | 0.0731
Epoch 194/300, resid Loss: 0.0488 | 0.0606
Epoch 195/300, resid Loss: 0.0476 | 0.0577
Epoch 196/300, resid Loss: 0.0481 | 0.0679
Epoch 197/300, resid Loss: 0.0464 | 0.0668
Epoch 198/300, resid Loss: 0.0464 | 0.0580
Epoch 199/300, resid Loss: 0.0463 | 0.0675
Epoch 200/300, resid Loss: 0.0457 | 0.0704
Epoch 201/300, resid Loss: 0.0461 | 0.0588
Epoch 202/300, resid Loss: 0.0463 | 0.0658
Epoch 203/300, resid Loss: 0.0470 | 0.0659
Epoch 204/300, resid Loss: 0.0468 | 0.0612
Epoch 205/300, resid Loss: 0.0465 | 0.0624
Epoch 206/300, resid Loss: 0.0463 | 0.0640
Epoch 207/300, resid Loss: 0.0460 | 0.0626
Epoch 208/300, resid Loss: 0.0449 | 0.0606
Epoch 209/300, resid Loss: 0.0449 | 0.0642
Epoch 210/300, resid Loss: 0.0454 | 0.0624
Epoch 211/300, resid Loss: 0.0451 | 0.0606
Epoch 212/300, resid Loss: 0.0442 | 0.0652
Epoch 213/300, resid Loss: 0.0440 | 0.0600
Epoch 214/300, resid Loss: 0.0438 | 0.0604
Epoch 215/300, resid Loss: 0.0430 | 0.0636
Epoch 216/300, resid Loss: 0.0429 | 0.0617
Epoch 217/300, resid Loss: 0.0427 | 0.0590
Epoch 218/300, resid Loss: 0.0426 | 0.0591
Epoch 219/300, resid Loss: 0.0429 | 0.0610
Epoch 220/300, resid Loss: 0.0431 | 0.0623
Epoch 221/300, resid Loss: 0.0431 | 0.0581
Epoch 222/300, resid Loss: 0.0425 | 0.0633
Epoch 223/300, resid Loss: 0.0427 | 0.0664
Epoch 224/300, resid Loss: 0.0419 | 0.0570
Epoch 225/300, resid Loss: 0.0426 | 0.0560
Epoch 226/300, resid Loss: 0.0435 | 0.0647
Epoch 227/300, resid Loss: 0.0432 | 0.0646
Epoch 228/300, resid Loss: 0.0423 | 0.0556
Epoch 229/300, resid Loss: 0.0422 | 0.0644
Epoch 230/300, resid Loss: 0.0428 | 0.0704
Epoch 231/300, resid Loss: 0.0425 | 0.0576
Epoch 232/300, resid Loss: 0.0424 | 0.0553
Epoch 233/300, resid Loss: 0.0434 | 0.0690
Epoch 234/300, resid Loss: 0.0425 | 0.0619
Epoch 235/300, resid Loss: 0.0429 | 0.0582
Epoch 236/300, resid Loss: 0.0420 | 0.0640
Epoch 237/300, resid Loss: 0.0418 | 0.0615
Epoch 238/300, resid Loss: 0.0419 | 0.0585
Epoch 239/300, resid Loss: 0.0415 | 0.0598
Epoch 240/300, resid Loss: 0.0415 | 0.0655
Epoch 241/300, resid Loss: 0.0415 | 0.0605
Epoch 242/300, resid Loss: 0.0412 | 0.0620
Epoch 243/300, resid Loss: 0.0410 | 0.0625
Epoch 244/300, resid Loss: 0.0402 | 0.0596
Epoch 245/300, resid Loss: 0.0400 | 0.0580
Epoch 246/300, resid Loss: 0.0401 | 0.0624
Epoch 247/300, resid Loss: 0.0396 | 0.0606
Epoch 248/300, resid Loss: 0.0400 | 0.0573
Epoch 249/300, resid Loss: 0.0400 | 0.0610
Epoch 250/300, resid Loss: 0.0390 | 0.0614
Epoch 251/300, resid Loss: 0.0393 | 0.0564
Epoch 252/300, resid Loss: 0.0390 | 0.0591
Epoch 253/300, resid Loss: 0.0394 | 0.0636
Epoch 254/300, resid Loss: 0.0395 | 0.0600
Epoch 255/300, resid Loss: 0.0399 | 0.0560
Epoch 256/300, resid Loss: 0.0395 | 0.0636
Epoch 257/300, resid Loss: 0.0395 | 0.0646
Epoch 258/300, resid Loss: 0.0394 | 0.0560
Epoch 259/300, resid Loss: 0.0390 | 0.0619
Epoch 260/300, resid Loss: 0.0392 | 0.0642
Epoch 261/300, resid Loss: 0.0384 | 0.0596
Epoch 262/300, resid Loss: 0.0384 | 0.0570
Epoch 263/300, resid Loss: 0.0384 | 0.0611
Epoch 264/300, resid Loss: 0.0383 | 0.0598
Epoch 265/300, resid Loss: 0.0383 | 0.0574
Epoch 266/300, resid Loss: 0.0383 | 0.0616
Epoch 267/300, resid Loss: 0.0382 | 0.0599
Epoch 268/300, resid Loss: 0.0383 | 0.0595
Epoch 269/300, resid Loss: 0.0380 | 0.0589
Epoch 270/300, resid Loss: 0.0382 | 0.0599
Epoch 271/300, resid Loss: 0.0379 | 0.0579
Epoch 272/300, resid Loss: 0.0375 | 0.0591
Epoch 273/300, resid Loss: 0.0374 | 0.0605
Epoch 274/300, resid Loss: 0.0374 | 0.0591
Epoch 275/300, resid Loss: 0.0372 | 0.0587
Epoch 276/300, resid Loss: 0.0376 | 0.0598
Epoch 277/300, resid Loss: 0.0369 | 0.0579
Epoch 278/300, resid Loss: 0.0372 | 0.0584
Epoch 279/300, resid Loss: 0.0374 | 0.0610
Epoch 280/300, resid Loss: 0.0369 | 0.0602
Epoch 281/300, resid Loss: 0.0372 | 0.0567
Epoch 282/300, resid Loss: 0.0368 | 0.0571
Epoch 283/300, resid Loss: 0.0366 | 0.0607
Epoch 284/300, resid Loss: 0.0369 | 0.0597
Epoch 285/300, resid Loss: 0.0367 | 0.0583
Epoch 286/300, resid Loss: 0.0370 | 0.0583
Epoch 287/300, resid Loss: 0.0368 | 0.0625
Epoch 288/300, resid Loss: 0.0367 | 0.0580
Epoch 289/300, resid Loss: 0.0366 | 0.0572
Epoch 290/300, resid Loss: 0.0369 | 0.0614
Epoch 291/300, resid Loss: 0.0364 | 0.0624
Epoch 292/300, resid Loss: 0.0368 | 0.0576
Epoch 293/300, resid Loss: 0.0364 | 0.0585
Epoch 294/300, resid Loss: 0.0366 | 0.0622
Epoch 295/300, resid Loss: 0.0360 | 0.0602
Epoch 296/300, resid Loss: 0.0360 | 0.0580
Epoch 297/300, resid Loss: 0.0357 | 0.0602
Epoch 298/300, resid Loss: 0.0360 | 0.0597
Epoch 299/300, resid Loss: 0.0355 | 0.0589
Epoch 300/300, resid Loss: 0.0355 | 0.0578
Runtime (seconds): 2596.2524445056915
0.00013431963840526654
[159.72168]
[-3.1412723]
[3.6430357]
[15.091309]
[0.5149215]
[21.131489]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 20.149599853903055
RMSE: 4.48883056640625
MAE: 4.48883056640625
R-squared: nan
[196.96117]
File amzn_stock_price_prediction_by_Transformer.png exists. Logging to WandB.
