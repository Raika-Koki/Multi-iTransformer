ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-02 02:16:03,474][0m A new study created in memory with name: no-name-6d8793ed-dd70-49d5-a68e-fe8d2f9df598[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-02-02 02:16:33,439][0m Trial 0 finished with value: 0.26504334147649866 and parameters: {'observation_period_num': 115, 'train_rates': 0.8614003952447544, 'learning_rate': 2.207530867571304e-05, 'batch_size': 210, 'step_size': 12, 'gamma': 0.7571944006710137}. Best is trial 0 with value: 0.26504334147649866.[0m
[32m[I 2025-02-02 02:17:00,060][0m Trial 1 finished with value: 0.30772231575218273 and parameters: {'observation_period_num': 239, 'train_rates': 0.8234400624601939, 'learning_rate': 1.7976723575983986e-05, 'batch_size': 234, 'step_size': 10, 'gamma': 0.9149609012526461}. Best is trial 0 with value: 0.26504334147649866.[0m
[32m[I 2025-02-02 02:17:25,720][0m Trial 2 finished with value: 0.22115754135494126 and parameters: {'observation_period_num': 104, 'train_rates': 0.6856836475968362, 'learning_rate': 2.1418176576930063e-05, 'batch_size': 164, 'step_size': 11, 'gamma': 0.8673417406586712}. Best is trial 2 with value: 0.22115754135494126.[0m
[32m[I 2025-02-02 02:18:02,925][0m Trial 3 finished with value: 0.14377382073517653 and parameters: {'observation_period_num': 11, 'train_rates': 0.8874129024834478, 'learning_rate': 0.00029986084758629075, 'batch_size': 106, 'step_size': 1, 'gamma': 0.8822295736137589}. Best is trial 3 with value: 0.14377382073517653.[0m
[32m[I 2025-02-02 02:18:35,459][0m Trial 4 finished with value: 0.12254483252763748 and parameters: {'observation_period_num': 181, 'train_rates': 0.9752239938951215, 'learning_rate': 0.0001874895441430965, 'batch_size': 146, 'step_size': 4, 'gamma': 0.864682505427707}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:18:59,893][0m Trial 5 finished with value: 0.1335852266270287 and parameters: {'observation_period_num': 89, 'train_rates': 0.6574635151774545, 'learning_rate': 0.0002056715157068128, 'batch_size': 255, 'step_size': 9, 'gamma': 0.7903408573904738}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:19:34,544][0m Trial 6 finished with value: 0.1634630262851715 and parameters: {'observation_period_num': 78, 'train_rates': 0.9775624313266353, 'learning_rate': 5.962026084834186e-05, 'batch_size': 128, 'step_size': 1, 'gamma': 0.9563800117425459}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:22:15,570][0m Trial 7 finished with value: 0.1372044991988402 and parameters: {'observation_period_num': 120, 'train_rates': 0.8622618577595269, 'learning_rate': 6.656269783750694e-05, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8681382748841311}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:23:57,054][0m Trial 8 finished with value: 0.16947304393766047 and parameters: {'observation_period_num': 193, 'train_rates': 0.6564091092929009, 'learning_rate': 1.810545412540884e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.9634189707574423}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:24:27,944][0m Trial 9 finished with value: 0.18266174716181993 and parameters: {'observation_period_num': 162, 'train_rates': 0.7283164984881367, 'learning_rate': 3.331210531293518e-05, 'batch_size': 112, 'step_size': 14, 'gamma': 0.8824773165646566}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:24:59,012][0m Trial 10 finished with value: 0.583088219165802 and parameters: {'observation_period_num': 229, 'train_rates': 0.9847506607094849, 'learning_rate': 1.8497350227849571e-06, 'batch_size': 168, 'step_size': 5, 'gamma': 0.8030031586326054}. Best is trial 4 with value: 0.12254483252763748.[0m
[32m[I 2025-02-02 02:25:25,097][0m Trial 11 finished with value: 0.10215578724412208 and parameters: {'observation_period_num': 61, 'train_rates': 0.7562668883002843, 'learning_rate': 0.0008289562145187829, 'batch_size': 243, 'step_size': 6, 'gamma': 0.8151302993103973}. Best is trial 11 with value: 0.10215578724412208.[0m
[32m[I 2025-02-02 02:26:10,709][0m Trial 12 finished with value: 0.0853325242468265 and parameters: {'observation_period_num': 30, 'train_rates': 0.7574804076926218, 'learning_rate': 0.000997312557335684, 'batch_size': 71, 'step_size': 5, 'gamma': 0.8228540224874806}. Best is trial 12 with value: 0.0853325242468265.[0m
[32m[I 2025-02-02 02:26:57,025][0m Trial 13 finished with value: 0.08007102095338796 and parameters: {'observation_period_num': 27, 'train_rates': 0.7513164595716011, 'learning_rate': 0.0009392777724716573, 'batch_size': 68, 'step_size': 6, 'gamma': 0.8206617105185453}. Best is trial 13 with value: 0.08007102095338796.[0m
[32m[I 2025-02-02 02:27:51,404][0m Trial 14 finished with value: 0.07206423724097992 and parameters: {'observation_period_num': 6, 'train_rates': 0.7725461825399718, 'learning_rate': 0.0007860140562454799, 'batch_size': 59, 'step_size': 7, 'gamma': 0.8313003185357101}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:28:46,475][0m Trial 15 finished with value: 0.09102093708262872 and parameters: {'observation_period_num': 35, 'train_rates': 0.8056900563684418, 'learning_rate': 0.0004021801509622241, 'batch_size': 59, 'step_size': 7, 'gamma': 0.8375578804575079}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:29:31,975][0m Trial 16 finished with value: 0.524400482685572 and parameters: {'observation_period_num': 7, 'train_rates': 0.6030972794686338, 'learning_rate': 3.232484470876982e-06, 'batch_size': 60, 'step_size': 3, 'gamma': 0.7649908089767281}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:30:11,442][0m Trial 17 finished with value: 0.09474907572846859 and parameters: {'observation_period_num': 53, 'train_rates': 0.7239245098926713, 'learning_rate': 0.000526774345888576, 'batch_size': 79, 'step_size': 8, 'gamma': 0.8382586767804028}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:30:52,114][0m Trial 18 finished with value: 0.19136133263023888 and parameters: {'observation_period_num': 145, 'train_rates': 0.9125925055032795, 'learning_rate': 0.00012797532535581201, 'batch_size': 91, 'step_size': 7, 'gamma': 0.929876464515821}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:32:09,764][0m Trial 19 finished with value: 0.2560551365984278 and parameters: {'observation_period_num': 40, 'train_rates': 0.777740396091554, 'learning_rate': 7.716691209040276e-06, 'batch_size': 41, 'step_size': 3, 'gamma': 0.7968566495000461}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:33:37,088][0m Trial 20 finished with value: 0.08219248269285474 and parameters: {'observation_period_num': 5, 'train_rates': 0.8194494333439236, 'learning_rate': 0.0006719727441838451, 'batch_size': 38, 'step_size': 15, 'gamma': 0.7782384001598817}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:35:02,472][0m Trial 21 finished with value: 0.09158902688289595 and parameters: {'observation_period_num': 11, 'train_rates': 0.8256846004964451, 'learning_rate': 0.0005837610341235017, 'batch_size': 39, 'step_size': 15, 'gamma': 0.779234937610497}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:36:05,589][0m Trial 22 finished with value: 0.12550659780105927 and parameters: {'observation_period_num': 66, 'train_rates': 0.7164876255874717, 'learning_rate': 0.00032893533441279187, 'batch_size': 48, 'step_size': 13, 'gamma': 0.8414179243582259}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:36:43,898][0m Trial 23 finished with value: 0.08407545091986293 and parameters: {'observation_period_num': 28, 'train_rates': 0.775118039999663, 'learning_rate': 0.0009708116273914006, 'batch_size': 90, 'step_size': 7, 'gamma': 0.751020052268244}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:37:35,248][0m Trial 24 finished with value: 0.10378927219568229 and parameters: {'observation_period_num': 6, 'train_rates': 0.8470657036140695, 'learning_rate': 9.072103719685463e-05, 'batch_size': 66, 'step_size': 6, 'gamma': 0.7797803390827941}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:40:56,530][0m Trial 25 finished with value: 0.18211437090560123 and parameters: {'observation_period_num': 47, 'train_rates': 0.9236600578877571, 'learning_rate': 0.00050978763081393, 'batch_size': 20, 'step_size': 8, 'gamma': 0.8142171842372344}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:41:35,461][0m Trial 26 finished with value: 0.08721926131382071 and parameters: {'observation_period_num': 25, 'train_rates': 0.801734594089546, 'learning_rate': 0.00020866935669471123, 'batch_size': 103, 'step_size': 12, 'gamma': 0.8524142548081466}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:42:52,729][0m Trial 27 finished with value: 0.13136230635174204 and parameters: {'observation_period_num': 83, 'train_rates': 0.68963144372256, 'learning_rate': 0.0005979602456216602, 'batch_size': 44, 'step_size': 3, 'gamma': 0.7775968036424582}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:43:38,164][0m Trial 28 finished with value: 0.10097258917651918 and parameters: {'observation_period_num': 62, 'train_rates': 0.7463112018241136, 'learning_rate': 0.0003241986589513825, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8259151807569235}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:44:08,834][0m Trial 29 finished with value: 0.11882669926041971 and parameters: {'observation_period_num': 21, 'train_rates': 0.8556790535845797, 'learning_rate': 0.00011519716111196976, 'batch_size': 203, 'step_size': 11, 'gamma': 0.9896049189288073}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:44:41,047][0m Trial 30 finished with value: 0.3442303573848396 and parameters: {'observation_period_num': 100, 'train_rates': 0.7884166590448886, 'learning_rate': 6.497103848254719e-06, 'batch_size': 128, 'step_size': 5, 'gamma': 0.8989745163519289}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:45:22,272][0m Trial 31 finished with value: 0.08759660931850621 and parameters: {'observation_period_num': 30, 'train_rates': 0.7714188586656296, 'learning_rate': 0.0009590430509391064, 'batch_size': 92, 'step_size': 7, 'gamma': 0.7633737384060768}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:46:31,468][0m Trial 32 finished with value: 0.11044852172212925 and parameters: {'observation_period_num': 44, 'train_rates': 0.8182193934170193, 'learning_rate': 0.0007096365196273017, 'batch_size': 57, 'step_size': 8, 'gamma': 0.7579739781422623}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:48:29,274][0m Trial 33 finished with value: 0.0984775290608105 and parameters: {'observation_period_num': 20, 'train_rates': 0.8312104742531093, 'learning_rate': 0.000976955832439454, 'batch_size': 34, 'step_size': 6, 'gamma': 0.7505580166164018}. Best is trial 14 with value: 0.07206423724097992.[0m
[32m[I 2025-02-02 02:49:16,024][0m Trial 34 finished with value: 0.0706841167358851 and parameters: {'observation_period_num': 6, 'train_rates': 0.7013216076606603, 'learning_rate': 0.00041536037920347546, 'batch_size': 79, 'step_size': 9, 'gamma': 0.8004306154360642}. Best is trial 34 with value: 0.0706841167358851.[0m
[32m[I 2025-02-02 02:49:49,791][0m Trial 35 finished with value: 0.07135243848457093 and parameters: {'observation_period_num': 5, 'train_rates': 0.6806482384213538, 'learning_rate': 0.0002337495895365299, 'batch_size': 114, 'step_size': 10, 'gamma': 0.8045901318802007}. Best is trial 34 with value: 0.0706841167358851.[0m
[32m[I 2025-02-02 02:50:19,907][0m Trial 36 finished with value: 0.10886880302194432 and parameters: {'observation_period_num': 56, 'train_rates': 0.6842371309510745, 'learning_rate': 0.00021590126048790585, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8062060134968713}. Best is trial 34 with value: 0.0706841167358851.[0m
[32m[I 2025-02-02 02:50:50,147][0m Trial 37 finished with value: 0.07050519554220416 and parameters: {'observation_period_num': 17, 'train_rates': 0.6319169935501193, 'learning_rate': 0.0004036639909254439, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8562384856954706}. Best is trial 37 with value: 0.07050519554220416.[0m
[32m[I 2025-02-02 02:51:21,336][0m Trial 38 finished with value: 0.25979702482169326 and parameters: {'observation_period_num': 17, 'train_rates': 0.6251936110185694, 'learning_rate': 3.349562563274249e-05, 'batch_size': 118, 'step_size': 11, 'gamma': 0.8562159117193564}. Best is trial 37 with value: 0.07050519554220416.[0m
[32m[I 2025-02-02 02:51:47,619][0m Trial 39 finished with value: 0.12605336862837985 and parameters: {'observation_period_num': 72, 'train_rates': 0.6497847076425125, 'learning_rate': 0.0002604410355785541, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8780008029007899}. Best is trial 37 with value: 0.07050519554220416.[0m
[32m[I 2025-02-02 02:52:16,592][0m Trial 40 finished with value: 0.16930280716117502 and parameters: {'observation_period_num': 108, 'train_rates': 0.6879344287822572, 'learning_rate': 0.0004065486179451882, 'batch_size': 141, 'step_size': 12, 'gamma': 0.8968205643076735}. Best is trial 37 with value: 0.07050519554220416.[0m
[32m[I 2025-02-02 02:52:48,493][0m Trial 41 finished with value: 0.09379210210633727 and parameters: {'observation_period_num': 39, 'train_rates': 0.6359467414515058, 'learning_rate': 0.0001412081959597708, 'batch_size': 110, 'step_size': 9, 'gamma': 0.8291258147833533}. Best is trial 37 with value: 0.07050519554220416.[0m
[32m[I 2025-02-02 02:53:25,954][0m Trial 42 finished with value: 0.06941635332527166 and parameters: {'observation_period_num': 17, 'train_rates': 0.7110157076242639, 'learning_rate': 0.0004809934322516847, 'batch_size': 102, 'step_size': 10, 'gamma': 0.8492778625642857}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:53:57,242][0m Trial 43 finished with value: 0.0734557090587182 and parameters: {'observation_period_num': 16, 'train_rates': 0.6707386744518127, 'learning_rate': 0.00043611446829945735, 'batch_size': 126, 'step_size': 11, 'gamma': 0.854379797876734}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:54:34,218][0m Trial 44 finished with value: 0.1827755901188666 and parameters: {'observation_period_num': 200, 'train_rates': 0.6992551034402742, 'learning_rate': 0.0001668263856437116, 'batch_size': 101, 'step_size': 10, 'gamma': 0.7945506642070241}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:55:01,618][0m Trial 45 finished with value: 0.18185641497859692 and parameters: {'observation_period_num': 250, 'train_rates': 0.7051538150690738, 'learning_rate': 0.0002921417936264858, 'batch_size': 154, 'step_size': 10, 'gamma': 0.807401451176393}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:55:33,422][0m Trial 46 finished with value: 0.11778935264003484 and parameters: {'observation_period_num': 16, 'train_rates': 0.6686535635707969, 'learning_rate': 5.0503829161138134e-05, 'batch_size': 120, 'step_size': 9, 'gamma': 0.8482530930023237}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:56:12,886][0m Trial 47 finished with value: 0.10939245715051191 and parameters: {'observation_period_num': 47, 'train_rates': 0.603574216837825, 'learning_rate': 9.178376489579786e-05, 'batch_size': 77, 'step_size': 12, 'gamma': 0.8685914494528953}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:56:40,006][0m Trial 48 finished with value: 0.11667118378013097 and parameters: {'observation_period_num': 92, 'train_rates': 0.7351328884914317, 'learning_rate': 0.0002586555570689593, 'batch_size': 192, 'step_size': 8, 'gamma': 0.8318652075491886}. Best is trial 42 with value: 0.06941635332527166.[0m
[32m[I 2025-02-02 02:57:09,202][0m Trial 49 finished with value: 0.8278794252235471 and parameters: {'observation_period_num': 130, 'train_rates': 0.62839467930887, 'learning_rate': 1.1058065786844458e-06, 'batch_size': 134, 'step_size': 9, 'gamma': 0.8899354865952159}. Best is trial 42 with value: 0.06941635332527166.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-02 02:57:09,209][0m A new study created in memory with name: no-name-fc4032a1-0474-454b-a158-e420b97ccac1[0m
[32m[I 2025-02-02 02:59:23,230][0m Trial 0 finished with value: 0.27404969652642064 and parameters: {'observation_period_num': 191, 'train_rates': 0.9029114668726514, 'learning_rate': 0.0002970083312869845, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9438520122323223}. Best is trial 0 with value: 0.27404969652642064.[0m
[32m[I 2025-02-02 02:59:57,761][0m Trial 1 finished with value: 0.15870105493755726 and parameters: {'observation_period_num': 182, 'train_rates': 0.7682956570598551, 'learning_rate': 0.00024822743975246617, 'batch_size': 94, 'step_size': 8, 'gamma': 0.9514990005654299}. Best is trial 1 with value: 0.15870105493755726.[0m
[32m[I 2025-02-02 03:00:49,263][0m Trial 2 finished with value: 0.19353537726256598 and parameters: {'observation_period_num': 170, 'train_rates': 0.661629219040496, 'learning_rate': 0.000641070422156982, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9342735187339692}. Best is trial 1 with value: 0.15870105493755726.[0m
[32m[I 2025-02-02 03:01:51,052][0m Trial 3 finished with value: 0.13678799422478413 and parameters: {'observation_period_num': 146, 'train_rates': 0.774453091911516, 'learning_rate': 9.333536811384994e-05, 'batch_size': 50, 'step_size': 4, 'gamma': 0.9416120258064749}. Best is trial 3 with value: 0.13678799422478413.[0m
[32m[I 2025-02-02 03:02:51,989][0m Trial 4 finished with value: 0.35483697334340975 and parameters: {'observation_period_num': 221, 'train_rates': 0.9442296496723823, 'learning_rate': 0.0007493781331279585, 'batch_size': 57, 'step_size': 9, 'gamma': 0.8129779723960888}. Best is trial 3 with value: 0.13678799422478413.[0m
[32m[I 2025-02-02 03:03:31,050][0m Trial 5 finished with value: 0.1336590003614363 and parameters: {'observation_period_num': 75, 'train_rates': 0.8547114142819192, 'learning_rate': 0.0002510641720752919, 'batch_size': 93, 'step_size': 4, 'gamma': 0.781648617740702}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:03:59,348][0m Trial 6 finished with value: 0.2551094335876831 and parameters: {'observation_period_num': 250, 'train_rates': 0.8382654472764725, 'learning_rate': 2.8391604348783995e-05, 'batch_size': 172, 'step_size': 14, 'gamma': 0.8777911099016409}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:04:32,090][0m Trial 7 finished with value: 0.25108533531087435 and parameters: {'observation_period_num': 183, 'train_rates': 0.9051257219592608, 'learning_rate': 3.2266517640211496e-05, 'batch_size': 155, 'step_size': 10, 'gamma': 0.9605599730404926}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:05:00,099][0m Trial 8 finished with value: 0.4948181091067346 and parameters: {'observation_period_num': 207, 'train_rates': 0.6622051700441175, 'learning_rate': 3.672332787011141e-06, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9689791148467711}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:05:30,679][0m Trial 9 finished with value: 0.3265419900417328 and parameters: {'observation_period_num': 241, 'train_rates': 0.950733981958386, 'learning_rate': 6.893314625751425e-05, 'batch_size': 208, 'step_size': 9, 'gamma': 0.9796053250913818}. Best is trial 5 with value: 0.1336590003614363.[0m
Early stopping at epoch 34
[32m[I 2025-02-02 03:05:41,526][0m Trial 10 finished with value: 3.1695403191778393 and parameters: {'observation_period_num': 60, 'train_rates': 0.8382952785453799, 'learning_rate': 1.917691925984785e-06, 'batch_size': 247, 'step_size': 1, 'gamma': 0.7500089003155759}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:06:19,992][0m Trial 11 finished with value: 0.15261172230394793 and parameters: {'observation_period_num': 86, 'train_rates': 0.7524353059580332, 'learning_rate': 9.574727558591837e-05, 'batch_size': 100, 'step_size': 3, 'gamma': 0.8791815432652548}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:06:57,211][0m Trial 12 finished with value: 0.4589834814098212 and parameters: {'observation_period_num': 120, 'train_rates': 0.7146277565942223, 'learning_rate': 7.941283214117411e-06, 'batch_size': 99, 'step_size': 5, 'gamma': 0.8111665364075362}. Best is trial 5 with value: 0.1336590003614363.[0m
[32m[I 2025-02-02 03:10:29,858][0m Trial 13 finished with value: 0.08448208959611214 and parameters: {'observation_period_num': 16, 'train_rates': 0.8119388884483846, 'learning_rate': 0.00015163268712908452, 'batch_size': 18, 'step_size': 5, 'gamma': 0.7542703713647445}. Best is trial 13 with value: 0.08448208959611214.[0m
[32m[I 2025-02-02 03:13:20,162][0m Trial 14 finished with value: 0.08712075857353235 and parameters: {'observation_period_num': 6, 'train_rates': 0.8356462061031913, 'learning_rate': 0.00020015779880874643, 'batch_size': 23, 'step_size': 7, 'gamma': 0.7520244581428159}. Best is trial 13 with value: 0.08448208959611214.[0m
[32m[I 2025-02-02 03:17:09,860][0m Trial 15 finished with value: 0.12030283539100503 and parameters: {'observation_period_num': 30, 'train_rates': 0.8123816731483648, 'learning_rate': 1.6451362870311966e-05, 'batch_size': 17, 'step_size': 7, 'gamma': 0.7550018816897067}. Best is trial 13 with value: 0.08448208959611214.[0m
[32m[I 2025-02-02 03:17:59,336][0m Trial 16 finished with value: 0.07490203175415254 and parameters: {'observation_period_num': 6, 'train_rates': 0.61409606655264, 'learning_rate': 0.00015218745718655944, 'batch_size': 68, 'step_size': 12, 'gamma': 0.8403400921949833}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:18:43,747][0m Trial 17 finished with value: 0.11258641131750242 and parameters: {'observation_period_num': 36, 'train_rates': 0.6077711125462945, 'learning_rate': 5.461351353159625e-05, 'batch_size': 72, 'step_size': 12, 'gamma': 0.8434879724461735}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:19:33,230][0m Trial 18 finished with value: 0.07605149913975533 and parameters: {'observation_period_num': 7, 'train_rates': 0.7286594597315637, 'learning_rate': 0.0001562155976738005, 'batch_size': 75, 'step_size': 15, 'gamma': 0.9117685731338648}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:20:01,977][0m Trial 19 finished with value: 0.23843298760053855 and parameters: {'observation_period_num': 102, 'train_rates': 0.6037497055783451, 'learning_rate': 0.0005318536105105551, 'batch_size': 125, 'step_size': 15, 'gamma': 0.9106524689803669}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:20:32,549][0m Trial 20 finished with value: 0.18070464903581226 and parameters: {'observation_period_num': 56, 'train_rates': 0.7078436545179877, 'learning_rate': 1.2319121875345663e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8976678972287728}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:21:21,456][0m Trial 21 finished with value: 0.07828341136562182 and parameters: {'observation_period_num': 5, 'train_rates': 0.7100764500193203, 'learning_rate': 0.00011288782382074192, 'batch_size': 76, 'step_size': 12, 'gamma': 0.8415750273582444}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:22:08,838][0m Trial 22 finished with value: 0.10544506847617845 and parameters: {'observation_period_num': 43, 'train_rates': 0.6593135889138904, 'learning_rate': 0.0001182342852825165, 'batch_size': 74, 'step_size': 12, 'gamma': 0.845564320633997}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:22:53,594][0m Trial 23 finished with value: 0.09912408800896057 and parameters: {'observation_period_num': 13, 'train_rates': 0.7158864342236593, 'learning_rate': 4.821534018878844e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.8481801515109597}. Best is trial 16 with value: 0.07490203175415254.[0m
[32m[I 2025-02-02 03:24:16,160][0m Trial 24 finished with value: 0.07225783760386667 and parameters: {'observation_period_num': 5, 'train_rates': 0.6871906613312136, 'learning_rate': 0.0004486876990811825, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8218707680286051}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:25:37,020][0m Trial 25 finished with value: 0.10720528950348743 and parameters: {'observation_period_num': 30, 'train_rates': 0.6281122588978902, 'learning_rate': 0.0004194380454551291, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8024731129353251}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:26:10,002][0m Trial 26 finished with value: 0.09576005511689652 and parameters: {'observation_period_num': 54, 'train_rates': 0.682124737360142, 'learning_rate': 0.0009971294736127417, 'batch_size': 111, 'step_size': 14, 'gamma': 0.8285671450323807}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:27:05,150][0m Trial 27 finished with value: 0.12189506328964637 and parameters: {'observation_period_num': 85, 'train_rates': 0.629522366222558, 'learning_rate': 0.0003802833906461197, 'batch_size': 62, 'step_size': 15, 'gamma': 0.9117962682070767}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:28:31,621][0m Trial 28 finished with value: 0.09358185067156129 and parameters: {'observation_period_num': 23, 'train_rates': 0.7441763476339794, 'learning_rate': 0.00017169796384809368, 'batch_size': 43, 'step_size': 13, 'gamma': 0.8671556045055765}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:30:06,663][0m Trial 29 finished with value: 0.11193371651416326 and parameters: {'observation_period_num': 73, 'train_rates': 0.685577454815048, 'learning_rate': 0.0003334861533512773, 'batch_size': 36, 'step_size': 11, 'gamma': 0.7845030579352639}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:30:37,951][0m Trial 30 finished with value: 0.14614333659410478 and parameters: {'observation_period_num': 42, 'train_rates': 0.6426284668270306, 'learning_rate': 3.140806866012005e-05, 'batch_size': 112, 'step_size': 13, 'gamma': 0.8685772840315645}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:31:26,794][0m Trial 31 finished with value: 0.08270953557233221 and parameters: {'observation_period_num': 6, 'train_rates': 0.7365324756313387, 'learning_rate': 0.00015142100953447138, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8315221139770206}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:32:17,036][0m Trial 32 finished with value: 0.09833518896530903 and parameters: {'observation_period_num': 23, 'train_rates': 0.6899938230055834, 'learning_rate': 0.00025630247087685116, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8555306490891792}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:34:09,843][0m Trial 33 finished with value: 0.07956932255281852 and parameters: {'observation_period_num': 5, 'train_rates': 0.7794705827616428, 'learning_rate': 8.619136605810087e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8325963930268204}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:35:10,951][0m Trial 34 finished with value: 0.1596066165357546 and parameters: {'observation_period_num': 153, 'train_rates': 0.7358264361267086, 'learning_rate': 0.0005138835093658272, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8923146776487457}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:36:24,770][0m Trial 35 finished with value: 0.09669627725336066 and parameters: {'observation_period_num': 45, 'train_rates': 0.6684311442729359, 'learning_rate': 0.00012461219018698066, 'batch_size': 46, 'step_size': 10, 'gamma': 0.7950735304055745}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:37:05,800][0m Trial 36 finished with value: 0.08783233949899535 and parameters: {'observation_period_num': 23, 'train_rates': 0.7039865835870731, 'learning_rate': 0.0002394045158052597, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9289895589628103}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:38:13,648][0m Trial 37 finished with value: 0.1089655660820761 and parameters: {'observation_period_num': 66, 'train_rates': 0.7587469669026161, 'learning_rate': 0.0006631491210760918, 'batch_size': 55, 'step_size': 8, 'gamma': 0.8190719341124159}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:38:43,565][0m Trial 38 finished with value: 0.15347044702402343 and parameters: {'observation_period_num': 109, 'train_rates': 0.643654597441977, 'learning_rate': 4.6979002261101434e-05, 'batch_size': 115, 'step_size': 14, 'gamma': 0.8558291166472204}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:39:13,068][0m Trial 39 finished with value: 0.16910540807198304 and parameters: {'observation_period_num': 137, 'train_rates': 0.7832805039192312, 'learning_rate': 0.0008846796243253972, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9245195207001669}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:39:41,701][0m Trial 40 finished with value: 0.08301477211740098 and parameters: {'observation_period_num': 16, 'train_rates': 0.7299387140536739, 'learning_rate': 0.00029353330879489223, 'batch_size': 187, 'step_size': 12, 'gamma': 0.7788473214724086}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:41:44,506][0m Trial 41 finished with value: 0.07755619261637095 and parameters: {'observation_period_num': 8, 'train_rates': 0.7846192160482497, 'learning_rate': 8.538404634826638e-05, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8329306455783251}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:43:34,063][0m Trial 42 finished with value: 0.09165724789715644 and parameters: {'observation_period_num': 30, 'train_rates': 0.7640459599634591, 'learning_rate': 7.161869533758674e-05, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8239803443749864}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:44:35,361][0m Trial 43 finished with value: 0.10906089907174821 and parameters: {'observation_period_num': 14, 'train_rates': 0.8777277324220673, 'learning_rate': 0.0001344917692539083, 'batch_size': 63, 'step_size': 15, 'gamma': 0.8409966774667202}. Best is trial 24 with value: 0.07225783760386667.[0m
[32m[I 2025-02-02 03:46:07,809][0m Trial 44 finished with value: 0.0674666091799736 and parameters: {'observation_period_num': 49, 'train_rates': 0.9863156618826562, 'learning_rate': 2.1391097081566665e-05, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8096257809527742}. Best is trial 44 with value: 0.0674666091799736.[0m
[32m[I 2025-02-02 03:47:32,843][0m Trial 45 finished with value: 0.1826897053008384 and parameters: {'observation_period_num': 47, 'train_rates': 0.9191295889402402, 'learning_rate': 2.4737658246495668e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8077996377579837}. Best is trial 44 with value: 0.0674666091799736.[0m
[32m[I 2025-02-02 03:49:56,592][0m Trial 46 finished with value: 0.055778803354179535 and parameters: {'observation_period_num': 34, 'train_rates': 0.9874066316151988, 'learning_rate': 1.7288446551650286e-05, 'batch_size': 31, 'step_size': 15, 'gamma': 0.794056719763528}. Best is trial 46 with value: 0.055778803354179535.[0m
[32m[I 2025-02-02 03:51:35,348][0m Trial 47 finished with value: 0.07961677759885788 and parameters: {'observation_period_num': 32, 'train_rates': 0.9899920901880055, 'learning_rate': 7.770138655820981e-06, 'batch_size': 45, 'step_size': 15, 'gamma': 0.7695193639668353}. Best is trial 46 with value: 0.055778803354179535.[0m
[32m[I 2025-02-02 03:52:19,856][0m Trial 48 finished with value: 0.15368998050689697 and parameters: {'observation_period_num': 166, 'train_rates': 0.9895508950042964, 'learning_rate': 1.7260266703949718e-05, 'batch_size': 85, 'step_size': 15, 'gamma': 0.7935562055279066}. Best is trial 46 with value: 0.055778803354179535.[0m
[32m[I 2025-02-02 03:55:53,745][0m Trial 49 finished with value: 0.509575931307597 and parameters: {'observation_period_num': 202, 'train_rates': 0.971832925035801, 'learning_rate': 5.808387074038457e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8157952899572661}. Best is trial 46 with value: 0.055778803354179535.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-02 03:55:53,753][0m A new study created in memory with name: no-name-38987699-633d-4f08-aa20-aacb5e39b509[0m
[32m[I 2025-02-02 03:56:29,293][0m Trial 0 finished with value: 0.14153303551462898 and parameters: {'observation_period_num': 5, 'train_rates': 0.8084102700640624, 'learning_rate': 5.4989779473815646e-06, 'batch_size': 107, 'step_size': 15, 'gamma': 0.7658507959101166}. Best is trial 0 with value: 0.14153303551462898.[0m
[32m[I 2025-02-02 03:56:57,212][0m Trial 1 finished with value: 0.13256146253219672 and parameters: {'observation_period_num': 128, 'train_rates': 0.8215272883771924, 'learning_rate': 0.00021530487543994537, 'batch_size': 222, 'step_size': 15, 'gamma': 0.9119266761042373}. Best is trial 1 with value: 0.13256146253219672.[0m
[32m[I 2025-02-02 03:58:17,094][0m Trial 2 finished with value: 0.3068140334053359 and parameters: {'observation_period_num': 238, 'train_rates': 0.8241341445127787, 'learning_rate': 1.159165624489995e-05, 'batch_size': 40, 'step_size': 3, 'gamma': 0.8754942927458668}. Best is trial 1 with value: 0.13256146253219672.[0m
[32m[I 2025-02-02 03:59:01,634][0m Trial 3 finished with value: 0.08538202640162294 and parameters: {'observation_period_num': 27, 'train_rates': 0.6925402018582427, 'learning_rate': 0.0003380856303972102, 'batch_size': 68, 'step_size': 4, 'gamma': 0.7766535853578366}. Best is trial 3 with value: 0.08538202640162294.[0m
[32m[I 2025-02-02 03:59:33,431][0m Trial 4 finished with value: 0.6314234089207005 and parameters: {'observation_period_num': 126, 'train_rates': 0.9213892430614647, 'learning_rate': 2.8507045265963122e-06, 'batch_size': 156, 'step_size': 13, 'gamma': 0.7682113461743855}. Best is trial 3 with value: 0.08538202640162294.[0m
Early stopping at epoch 78
[32m[I 2025-02-02 03:59:54,010][0m Trial 5 finished with value: 0.5520548905254504 and parameters: {'observation_period_num': 153, 'train_rates': 0.6882554721003593, 'learning_rate': 3.1719985695513675e-05, 'batch_size': 167, 'step_size': 1, 'gamma': 0.8557842561298523}. Best is trial 3 with value: 0.08538202640162294.[0m
[32m[I 2025-02-02 04:00:25,455][0m Trial 6 finished with value: 0.3206426799297333 and parameters: {'observation_period_num': 85, 'train_rates': 0.9880122712515971, 'learning_rate': 4.914722222540064e-06, 'batch_size': 242, 'step_size': 14, 'gamma': 0.9339607425943197}. Best is trial 3 with value: 0.08538202640162294.[0m
[32m[I 2025-02-02 04:00:58,977][0m Trial 7 finished with value: 0.5524417161941528 and parameters: {'observation_period_num': 206, 'train_rates': 0.9677421256846874, 'learning_rate': 9.209585290056963e-06, 'batch_size': 155, 'step_size': 12, 'gamma': 0.8759369243548352}. Best is trial 3 with value: 0.08538202640162294.[0m
[32m[I 2025-02-02 04:01:50,784][0m Trial 8 finished with value: 0.26283791437139353 and parameters: {'observation_period_num': 247, 'train_rates': 0.7270370349198855, 'learning_rate': 6.87543106784113e-06, 'batch_size': 61, 'step_size': 10, 'gamma': 0.9691613505092582}. Best is trial 3 with value: 0.08538202640162294.[0m
[32m[I 2025-02-02 04:02:16,500][0m Trial 9 finished with value: 0.9143481110161097 and parameters: {'observation_period_num': 172, 'train_rates': 0.673313308729284, 'learning_rate': 1.053196145870218e-06, 'batch_size': 230, 'step_size': 7, 'gamma': 0.9414144738786959}. Best is trial 3 with value: 0.08538202640162294.[0m
[32m[I 2025-02-02 04:02:54,229][0m Trial 10 finished with value: 0.06384234229421533 and parameters: {'observation_period_num': 8, 'train_rates': 0.6087953889699227, 'learning_rate': 0.0009061652631443913, 'batch_size': 90, 'step_size': 6, 'gamma': 0.81319961800779}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:03:29,330][0m Trial 11 finished with value: 0.06459688361883945 and parameters: {'observation_period_num': 17, 'train_rates': 0.6101791007217992, 'learning_rate': 0.0009922389833133501, 'batch_size': 95, 'step_size': 6, 'gamma': 0.8194312766553854}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:04:01,512][0m Trial 12 finished with value: 0.10233329625746192 and parameters: {'observation_period_num': 56, 'train_rates': 0.6072002741014655, 'learning_rate': 0.0009174139358692691, 'batch_size': 108, 'step_size': 6, 'gamma': 0.8141397965296222}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:04:36,781][0m Trial 13 finished with value: 0.09760700219149299 and parameters: {'observation_period_num': 71, 'train_rates': 0.6236481033498953, 'learning_rate': 0.0009885650016741383, 'batch_size': 98, 'step_size': 9, 'gamma': 0.8211481749526196}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:08:15,647][0m Trial 14 finished with value: 0.08456738066836472 and parameters: {'observation_period_num': 32, 'train_rates': 0.7373099030323202, 'learning_rate': 0.0001060569933390907, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8164284650657736}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:08:54,707][0m Trial 15 finished with value: 0.1311847917061819 and parameters: {'observation_period_num': 92, 'train_rates': 0.60156637985822, 'learning_rate': 8.123399127843725e-05, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8423380416337871}. Best is trial 10 with value: 0.06384234229421533.[0m
Early stopping at epoch 60
[32m[I 2025-02-02 04:09:14,350][0m Trial 16 finished with value: 0.13045131365844712 and parameters: {'observation_period_num': 10, 'train_rates': 0.6503751862605309, 'learning_rate': 0.00034512231409122304, 'batch_size': 124, 'step_size': 1, 'gamma': 0.7955271210970867}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:09:43,247][0m Trial 17 finished with value: 0.10712820276848542 and parameters: {'observation_period_num': 56, 'train_rates': 0.7545168195857841, 'learning_rate': 0.0005037529613429429, 'batch_size': 190, 'step_size': 10, 'gamma': 0.8390263992606214}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:10:19,349][0m Trial 18 finished with value: 0.1399572154775979 and parameters: {'observation_period_num': 105, 'train_rates': 0.8685611962523444, 'learning_rate': 0.00012724622395583843, 'batch_size': 137, 'step_size': 6, 'gamma': 0.8929927244953826}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:11:32,032][0m Trial 19 finished with value: 0.16279084377621955 and parameters: {'observation_period_num': 41, 'train_rates': 0.6469932014855962, 'learning_rate': 3.249411953550404e-05, 'batch_size': 45, 'step_size': 3, 'gamma': 0.7509749001837075}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:12:00,703][0m Trial 20 finished with value: 0.10866999877080255 and parameters: {'observation_period_num': 8, 'train_rates': 0.7700487481619, 'learning_rate': 5.5476764261064233e-05, 'batch_size': 193, 'step_size': 7, 'gamma': 0.79756666154467}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:14:56,243][0m Trial 21 finished with value: 0.08297630771994591 and parameters: {'observation_period_num': 35, 'train_rates': 0.7269936811412887, 'learning_rate': 0.00017230648369968274, 'batch_size': 20, 'step_size': 5, 'gamma': 0.8193618412298729}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:18:07,362][0m Trial 22 finished with value: 0.08535290354316018 and parameters: {'observation_period_num': 39, 'train_rates': 0.7121317219014607, 'learning_rate': 0.0005635697962193632, 'batch_size': 18, 'step_size': 4, 'gamma': 0.7941125067421526}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:18:47,186][0m Trial 23 finished with value: 0.09861498105816725 and parameters: {'observation_period_num': 59, 'train_rates': 0.6456392139278156, 'learning_rate': 0.0002355347732678877, 'batch_size': 81, 'step_size': 5, 'gamma': 0.8390757776325558}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:20:20,596][0m Trial 24 finished with value: 0.08079127753013043 and parameters: {'observation_period_num': 22, 'train_rates': 0.6603658499844363, 'learning_rate': 0.0006104407773835093, 'batch_size': 35, 'step_size': 8, 'gamma': 0.855776826851445}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:21:27,020][0m Trial 25 finished with value: 0.07622984378575283 and parameters: {'observation_period_num': 16, 'train_rates': 0.6331181498049361, 'learning_rate': 0.0005947235836661048, 'batch_size': 45, 'step_size': 8, 'gamma': 0.8579391088456015}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:22:11,234][0m Trial 26 finished with value: 0.12017876266812284 and parameters: {'observation_period_num': 73, 'train_rates': 0.6248476138600584, 'learning_rate': 0.0009649659697756621, 'batch_size': 63, 'step_size': 11, 'gamma': 0.9031114481201168}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:22:42,731][0m Trial 27 finished with value: 0.1446461157713419 and parameters: {'observation_period_num': 105, 'train_rates': 0.6232784829700926, 'learning_rate': 0.0003515273866235363, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8539811783558072}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:23:11,881][0m Trial 28 finished with value: 0.07350975959491057 and parameters: {'observation_period_num': 20, 'train_rates': 0.6854163968892399, 'learning_rate': 0.0004950563272477209, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8303556371256707}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:23:40,837][0m Trial 29 finished with value: 0.1373649012101324 and parameters: {'observation_period_num': 51, 'train_rates': 0.6987122343926201, 'learning_rate': 6.160536045731115e-05, 'batch_size': 123, 'step_size': 6, 'gamma': 0.7830552177158306}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:24:14,030][0m Trial 30 finished with value: 0.09093441625391196 and parameters: {'observation_period_num': 8, 'train_rates': 0.7747857299324454, 'learning_rate': 0.0003445094279316821, 'batch_size': 111, 'step_size': 3, 'gamma': 0.8326849954366583}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:24:41,927][0m Trial 31 finished with value: 0.06499245598584441 and parameters: {'observation_period_num': 8, 'train_rates': 0.6294925689174449, 'learning_rate': 0.0006545682352628408, 'batch_size': 130, 'step_size': 7, 'gamma': 0.8081395346758176}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:25:08,791][0m Trial 32 finished with value: 0.07989055777971561 and parameters: {'observation_period_num': 23, 'train_rates': 0.600529822548325, 'learning_rate': 0.0006689237678447188, 'batch_size': 132, 'step_size': 7, 'gamma': 0.8039789447539091}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:25:35,133][0m Trial 33 finished with value: 0.07439953948442753 and parameters: {'observation_period_num': 5, 'train_rates': 0.6780446793618434, 'learning_rate': 0.000190189138874311, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8270582002215577}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:26:02,024][0m Trial 34 finished with value: 0.08925495122895334 and parameters: {'observation_period_num': 42, 'train_rates': 0.6656690906244744, 'learning_rate': 0.00044580608773208156, 'batch_size': 174, 'step_size': 6, 'gamma': 0.7842856116791472}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:26:37,034][0m Trial 35 finished with value: 0.12358115116755168 and parameters: {'observation_period_num': 29, 'train_rates': 0.8492779093405651, 'learning_rate': 0.00022530566335822992, 'batch_size': 112, 'step_size': 4, 'gamma': 0.7603547282084313}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:27:13,581][0m Trial 36 finished with value: 0.07506775656361984 and parameters: {'observation_period_num': 21, 'train_rates': 0.634503628326012, 'learning_rate': 0.0007398815504644331, 'batch_size': 79, 'step_size': 7, 'gamma': 0.8059112482871672}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:27:45,813][0m Trial 37 finished with value: 0.10061707134390699 and parameters: {'observation_period_num': 70, 'train_rates': 0.6982036708482775, 'learning_rate': 0.00026643342051367385, 'batch_size': 102, 'step_size': 9, 'gamma': 0.8724742067609165}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:28:12,094][0m Trial 38 finished with value: 0.21359359398968483 and parameters: {'observation_period_num': 159, 'train_rates': 0.6144484915291467, 'learning_rate': 0.0004637914250585547, 'batch_size': 119, 'step_size': 2, 'gamma': 0.7775332647608666}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:28:37,668][0m Trial 39 finished with value: 0.20109482509641263 and parameters: {'observation_period_num': 212, 'train_rates': 0.6547483625269618, 'learning_rate': 0.00014618711040656729, 'batch_size': 144, 'step_size': 5, 'gamma': 0.7656948209487652}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:29:03,572][0m Trial 40 finished with value: 0.3110567050744157 and parameters: {'observation_period_num': 133, 'train_rates': 0.6788593447939597, 'learning_rate': 2.008274745261631e-05, 'batch_size': 163, 'step_size': 10, 'gamma': 0.8854323924132838}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:29:32,131][0m Trial 41 finished with value: 0.0656307506756189 and parameters: {'observation_period_num': 10, 'train_rates': 0.677554990165032, 'learning_rate': 0.0007796748550737902, 'batch_size': 136, 'step_size': 9, 'gamma': 0.8291734381498775}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:30:04,483][0m Trial 42 finished with value: 0.08298274291729604 and parameters: {'observation_period_num': 19, 'train_rates': 0.8107635724673101, 'learning_rate': 0.0007493226399694836, 'batch_size': 132, 'step_size': 7, 'gamma': 0.8447339158913131}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:30:38,199][0m Trial 43 finished with value: 0.10041039089171336 and parameters: {'observation_period_num': 49, 'train_rates': 0.6406541655446171, 'learning_rate': 0.00038260453527457703, 'batch_size': 94, 'step_size': 15, 'gamma': 0.811796152458446}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:31:05,804][0m Trial 44 finished with value: 0.07861367565904849 and parameters: {'observation_period_num': 31, 'train_rates': 0.7106643168975952, 'learning_rate': 0.0007946145116969742, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8306349239140818}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:31:33,031][0m Trial 45 finished with value: 0.07329724088035815 and parameters: {'observation_period_num': 15, 'train_rates': 0.6682926775019138, 'learning_rate': 0.00028016081311085285, 'batch_size': 152, 'step_size': 9, 'gamma': 0.7893309376000115}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:31:59,018][0m Trial 46 finished with value: 0.07081698016387171 and parameters: {'observation_period_num': 10, 'train_rates': 0.6156803198132479, 'learning_rate': 0.0002858011591096303, 'batch_size': 155, 'step_size': 13, 'gamma': 0.7851969281995679}. Best is trial 10 with value: 0.06384234229421533.[0m
[32m[I 2025-02-02 04:32:42,245][0m Trial 47 finished with value: 0.061521158803805175 and parameters: {'observation_period_num': 6, 'train_rates': 0.6118968039825038, 'learning_rate': 0.0009874329988837538, 'batch_size': 71, 'step_size': 14, 'gamma': 0.7731908737638313}. Best is trial 47 with value: 0.061521158803805175.[0m
[32m[I 2025-02-02 04:33:28,412][0m Trial 48 finished with value: 0.14238416840863163 and parameters: {'observation_period_num': 63, 'train_rates': 0.6119115422212811, 'learning_rate': 0.0009428828436308869, 'batch_size': 66, 'step_size': 14, 'gamma': 0.8047991649088385}. Best is trial 47 with value: 0.061521158803805175.[0m
[32m[I 2025-02-02 04:34:13,520][0m Trial 49 finished with value: 0.1986817748735795 and parameters: {'observation_period_num': 46, 'train_rates': 0.6324999242960024, 'learning_rate': 3.848624039793098e-06, 'batch_size': 73, 'step_size': 14, 'gamma': 0.981627962615238}. Best is trial 47 with value: 0.061521158803805175.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-02 04:34:13,528][0m A new study created in memory with name: no-name-fa4d8c29-3ec0-4ee7-8e84-8eefcac5c832[0m
[32m[I 2025-02-02 04:34:44,046][0m Trial 0 finished with value: 0.18545345962047577 and parameters: {'observation_period_num': 54, 'train_rates': 0.9189540033662058, 'learning_rate': 0.0001322804872826312, 'batch_size': 243, 'step_size': 6, 'gamma': 0.8783301215562224}. Best is trial 0 with value: 0.18545345962047577.[0m
[32m[I 2025-02-02 04:35:22,455][0m Trial 1 finished with value: 0.3058662861585617 and parameters: {'observation_period_num': 240, 'train_rates': 0.8674638318384997, 'learning_rate': 4.6295302505913104e-05, 'batch_size': 105, 'step_size': 2, 'gamma': 0.9063292873419438}. Best is trial 0 with value: 0.18545345962047577.[0m
[32m[I 2025-02-02 04:35:51,242][0m Trial 2 finished with value: 0.608387426887355 and parameters: {'observation_period_num': 20, 'train_rates': 0.6979035645534328, 'learning_rate': 2.649274496893454e-06, 'batch_size': 139, 'step_size': 3, 'gamma': 0.8974750624257625}. Best is trial 0 with value: 0.18545345962047577.[0m
[32m[I 2025-02-02 04:36:19,206][0m Trial 3 finished with value: 0.5735337548952087 and parameters: {'observation_period_num': 194, 'train_rates': 0.7691958303617596, 'learning_rate': 5.112570006846482e-06, 'batch_size': 192, 'step_size': 4, 'gamma': 0.8897962232153513}. Best is trial 0 with value: 0.18545345962047577.[0m
[32m[I 2025-02-02 04:36:48,530][0m Trial 4 finished with value: 0.9308075590080089 and parameters: {'observation_period_num': 41, 'train_rates': 0.816449315307572, 'learning_rate': 1.615700502638479e-06, 'batch_size': 214, 'step_size': 8, 'gamma': 0.785168268600237}. Best is trial 0 with value: 0.18545345962047577.[0m
Early stopping at epoch 90
[32m[I 2025-02-02 04:37:13,858][0m Trial 5 finished with value: 0.12290135287181125 and parameters: {'observation_period_num': 32, 'train_rates': 0.6757852690735053, 'learning_rate': 0.000444458031664163, 'batch_size': 156, 'step_size': 1, 'gamma': 0.8599813043024838}. Best is trial 5 with value: 0.12290135287181125.[0m
[32m[I 2025-02-02 04:37:57,048][0m Trial 6 finished with value: 0.1877315179356989 and parameters: {'observation_period_num': 15, 'train_rates': 0.8197577627737219, 'learning_rate': 2.5252563359032524e-05, 'batch_size': 80, 'step_size': 2, 'gamma': 0.7963707346787665}. Best is trial 5 with value: 0.12290135287181125.[0m
[32m[I 2025-02-02 04:38:25,679][0m Trial 7 finished with value: 0.25134103110915906 and parameters: {'observation_period_num': 89, 'train_rates': 0.7934254285543225, 'learning_rate': 4.843124076307332e-05, 'batch_size': 164, 'step_size': 2, 'gamma': 0.8573131841670274}. Best is trial 5 with value: 0.12290135287181125.[0m
[32m[I 2025-02-02 04:38:54,867][0m Trial 8 finished with value: 1.1594529781369685 and parameters: {'observation_period_num': 167, 'train_rates': 0.8785182434219312, 'learning_rate': 2.776346298145558e-06, 'batch_size': 193, 'step_size': 3, 'gamma': 0.8123157250173041}. Best is trial 5 with value: 0.12290135287181125.[0m
[32m[I 2025-02-02 04:39:42,932][0m Trial 9 finished with value: 0.5528915103744058 and parameters: {'observation_period_num': 145, 'train_rates': 0.7820778702465334, 'learning_rate': 2.058487899724018e-06, 'batch_size': 66, 'step_size': 8, 'gamma': 0.7745240369545379}. Best is trial 5 with value: 0.12290135287181125.[0m
[32m[I 2025-02-02 04:41:38,415][0m Trial 10 finished with value: 0.1304323873113072 and parameters: {'observation_period_num': 97, 'train_rates': 0.6026158964763306, 'learning_rate': 0.0007222936405726295, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9800721225869542}. Best is trial 5 with value: 0.12290135287181125.[0m
[32m[I 2025-02-02 04:44:17,045][0m Trial 11 finished with value: 0.11127245594449141 and parameters: {'observation_period_num': 92, 'train_rates': 0.6090965180096174, 'learning_rate': 0.0009496430075135737, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9859092589597412}. Best is trial 11 with value: 0.11127245594449141.[0m
[32m[I 2025-02-02 04:46:08,368][0m Trial 12 finished with value: 0.13442122935273573 and parameters: {'observation_period_num': 82, 'train_rates': 0.6006001296688065, 'learning_rate': 0.0007901034235925989, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9876717084455405}. Best is trial 11 with value: 0.11127245594449141.[0m
[32m[I 2025-02-02 04:46:36,270][0m Trial 13 finished with value: 0.11578477741784404 and parameters: {'observation_period_num': 121, 'train_rates': 0.6764106395144238, 'learning_rate': 0.0002774621329483679, 'batch_size': 126, 'step_size': 13, 'gamma': 0.9407739791157937}. Best is trial 11 with value: 0.11127245594449141.[0m
[32m[I 2025-02-02 04:47:05,590][0m Trial 14 finished with value: 0.11855472799228585 and parameters: {'observation_period_num': 121, 'train_rates': 0.6751270636555878, 'learning_rate': 0.00026446126664135544, 'batch_size': 108, 'step_size': 12, 'gamma': 0.9475618409789465}. Best is trial 11 with value: 0.11127245594449141.[0m
[32m[I 2025-02-02 04:48:16,221][0m Trial 15 finished with value: 0.12567289173603058 and parameters: {'observation_period_num': 123, 'train_rates': 0.9829536559348195, 'learning_rate': 0.00016436615049093728, 'batch_size': 52, 'step_size': 14, 'gamma': 0.9478964635362425}. Best is trial 11 with value: 0.11127245594449141.[0m
[32m[I 2025-02-02 04:48:46,944][0m Trial 16 finished with value: 0.23612992734678329 and parameters: {'observation_period_num': 186, 'train_rates': 0.7198772017631218, 'learning_rate': 1.5339634233554703e-05, 'batch_size': 107, 'step_size': 11, 'gamma': 0.9385553715673719}. Best is trial 11 with value: 0.11127245594449141.[0m
[32m[I 2025-02-02 04:49:55,965][0m Trial 17 finished with value: 0.1043033457333213 and parameters: {'observation_period_num': 68, 'train_rates': 0.6417689505451432, 'learning_rate': 0.00010282395723404667, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9223426186090327}. Best is trial 17 with value: 0.1043033457333213.[0m
[32m[I 2025-02-02 04:51:08,271][0m Trial 18 finished with value: 0.11677911967039108 and parameters: {'observation_period_num': 67, 'train_rates': 0.6187035920773892, 'learning_rate': 9.205757086911719e-05, 'batch_size': 44, 'step_size': 10, 'gamma': 0.9671652209044694}. Best is trial 17 with value: 0.1043033457333213.[0m
[32m[I 2025-02-02 04:54:26,009][0m Trial 19 finished with value: 0.09758842606024984 and parameters: {'observation_period_num': 64, 'train_rates': 0.6438581349617669, 'learning_rate': 1.9972840208797463e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9165342414281414}. Best is trial 19 with value: 0.09758842606024984.[0m
[32m[I 2025-02-02 04:55:05,287][0m Trial 20 finished with value: 0.15437990679104524 and parameters: {'observation_period_num': 60, 'train_rates': 0.7239614484606305, 'learning_rate': 1.709544812395889e-05, 'batch_size': 78, 'step_size': 9, 'gamma': 0.8349377192114589}. Best is trial 19 with value: 0.09758842606024984.[0m
[32m[I 2025-02-02 04:56:46,211][0m Trial 21 finished with value: 0.22077244463097576 and parameters: {'observation_period_num': 101, 'train_rates': 0.6382784222904792, 'learning_rate': 8.035991745076484e-06, 'batch_size': 27, 'step_size': 7, 'gamma': 0.916622761299896}. Best is trial 19 with value: 0.09758842606024984.[0m
[32m[I 2025-02-02 04:59:50,779][0m Trial 22 finished with value: 0.12303810672262522 and parameters: {'observation_period_num': 73, 'train_rates': 0.6468834959524655, 'learning_rate': 6.388963369237459e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9201738943596496}. Best is trial 19 with value: 0.09758842606024984.[0m
[32m[I 2025-02-02 05:00:59,897][0m Trial 23 finished with value: 0.23830104654932277 and parameters: {'observation_period_num': 49, 'train_rates': 0.7409596378233742, 'learning_rate': 9.191447926709766e-06, 'batch_size': 49, 'step_size': 5, 'gamma': 0.7532364697099315}. Best is trial 19 with value: 0.09758842606024984.[0m
[32m[I 2025-02-02 05:02:07,638][0m Trial 24 finished with value: 0.1123650972457493 and parameters: {'observation_period_num': 103, 'train_rates': 0.6420222275042292, 'learning_rate': 2.9729720071230957e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9659155156214044}. Best is trial 19 with value: 0.09758842606024984.[0m
[32m[I 2025-02-02 05:02:50,830][0m Trial 25 finished with value: 0.0791818676365196 and parameters: {'observation_period_num': 8, 'train_rates': 0.6442739576065043, 'learning_rate': 0.0009867312969811077, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9244378230039927}. Best is trial 25 with value: 0.0791818676365196.[0m
[32m[I 2025-02-02 05:03:34,091][0m Trial 26 finished with value: 0.07268974195256478 and parameters: {'observation_period_num': 12, 'train_rates': 0.6543105300015091, 'learning_rate': 0.00041406763102248465, 'batch_size': 68, 'step_size': 15, 'gamma': 0.9336550627347144}. Best is trial 26 with value: 0.07268974195256478.[0m
[32m[I 2025-02-02 05:04:12,518][0m Trial 27 finished with value: 0.07607393702188425 and parameters: {'observation_period_num': 15, 'train_rates': 0.7494747909299011, 'learning_rate': 0.00040145999654227487, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8826510618352603}. Best is trial 26 with value: 0.07268974195256478.[0m
[32m[I 2025-02-02 05:04:50,041][0m Trial 28 finished with value: 0.07624974058440517 and parameters: {'observation_period_num': 8, 'train_rates': 0.754667155135139, 'learning_rate': 0.00044009793671689293, 'batch_size': 92, 'step_size': 15, 'gamma': 0.8383036652193253}. Best is trial 26 with value: 0.07268974195256478.[0m
[32m[I 2025-02-02 05:05:27,322][0m Trial 29 finished with value: 0.08915367349982262 and parameters: {'observation_period_num': 30, 'train_rates': 0.7534729142255002, 'learning_rate': 0.00043734636417098587, 'batch_size': 90, 'step_size': 14, 'gamma': 0.8803305013574012}. Best is trial 26 with value: 0.07268974195256478.[0m
[32m[I 2025-02-02 05:05:55,186][0m Trial 30 finished with value: 0.1191724608071537 and parameters: {'observation_period_num': 43, 'train_rates': 0.8483286373104078, 'learning_rate': 0.00016675852399670584, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8277181821400104}. Best is trial 26 with value: 0.07268974195256478.[0m
[32m[I 2025-02-02 05:06:40,153][0m Trial 31 finished with value: 0.07194702902391518 and parameters: {'observation_period_num': 11, 'train_rates': 0.7056009980857609, 'learning_rate': 0.00045355538635081545, 'batch_size': 68, 'step_size': 15, 'gamma': 0.8418866876674052}. Best is trial 31 with value: 0.07194702902391518.[0m
[32m[I 2025-02-02 05:07:15,478][0m Trial 32 finished with value: 0.06831229985323843 and parameters: {'observation_period_num': 8, 'train_rates': 0.694726322649592, 'learning_rate': 0.0003960544461304048, 'batch_size': 89, 'step_size': 15, 'gamma': 0.8401400699371064}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:07:45,068][0m Trial 33 finished with value: 0.08466923705247395 and parameters: {'observation_period_num': 24, 'train_rates': 0.7212354159099194, 'learning_rate': 0.0002612366649662963, 'batch_size': 125, 'step_size': 13, 'gamma': 0.8642011214802817}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:08:34,237][0m Trial 34 finished with value: 0.07183581198675865 and parameters: {'observation_period_num': 6, 'train_rates': 0.7073472847446552, 'learning_rate': 0.0005812848817472622, 'batch_size': 67, 'step_size': 15, 'gamma': 0.8473816177022381}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:09:22,191][0m Trial 35 finished with value: 0.1870129300648395 and parameters: {'observation_period_num': 228, 'train_rates': 0.703530658822229, 'learning_rate': 0.0006048349290751726, 'batch_size': 63, 'step_size': 14, 'gamma': 0.8443678160641689}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:09:56,684][0m Trial 36 finished with value: 0.07942684062389462 and parameters: {'observation_period_num': 36, 'train_rates': 0.6921675822724113, 'learning_rate': 0.0001929034635150652, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8987413343274746}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:10:49,016][0m Trial 37 finished with value: 0.06938311346147182 and parameters: {'observation_period_num': 6, 'train_rates': 0.6743080962505055, 'learning_rate': 0.0005451758296650738, 'batch_size': 60, 'step_size': 13, 'gamma': 0.8198192194713495}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:11:20,242][0m Trial 38 finished with value: 0.08144089538562164 and parameters: {'observation_period_num': 26, 'train_rates': 0.695639051175455, 'learning_rate': 0.0006074723365752025, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8183497521688646}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:11:48,153][0m Trial 39 finished with value: 0.08421820134570356 and parameters: {'observation_period_num': 45, 'train_rates': 0.6666422901777407, 'learning_rate': 0.00029353922337778117, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8035116753456236}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:12:32,984][0m Trial 40 finished with value: 0.09688707776367664 and parameters: {'observation_period_num': 24, 'train_rates': 0.8157831154578773, 'learning_rate': 0.00010696567223167151, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8438021501975438}. Best is trial 32 with value: 0.06831229985323843.[0m
[32m[I 2025-02-02 05:13:24,005][0m Trial 41 finished with value: 0.06673866825931737 and parameters: {'observation_period_num': 8, 'train_rates': 0.6690499080860736, 'learning_rate': 0.0005681510694776579, 'batch_size': 62, 'step_size': 15, 'gamma': 0.855351862607861}. Best is trial 41 with value: 0.06673866825931737.[0m
[32m[I 2025-02-02 05:14:20,892][0m Trial 42 finished with value: 0.07114677677551905 and parameters: {'observation_period_num': 5, 'train_rates': 0.7099608337433275, 'learning_rate': 0.000590906795941695, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8521615044763029}. Best is trial 41 with value: 0.06673866825931737.[0m
[32m[I 2025-02-02 05:15:23,558][0m Trial 43 finished with value: 0.10536118821455882 and parameters: {'observation_period_num': 37, 'train_rates': 0.7771685015026752, 'learning_rate': 0.000622432312551301, 'batch_size': 54, 'step_size': 14, 'gamma': 0.85259743026548}. Best is trial 41 with value: 0.06673866825931737.[0m
[32m[I 2025-02-02 05:16:19,659][0m Trial 44 finished with value: 0.06341018542694567 and parameters: {'observation_period_num': 6, 'train_rates': 0.6786031539610612, 'learning_rate': 0.0002118865949482075, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8730542022734815}. Best is trial 44 with value: 0.06341018542694567.[0m
[32m[I 2025-02-02 05:17:51,068][0m Trial 45 finished with value: 0.09427725084686792 and parameters: {'observation_period_num': 24, 'train_rates': 0.6822466744330176, 'learning_rate': 0.0001925458642224225, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8712849266761928}. Best is trial 44 with value: 0.06341018542694567.[0m
[32m[I 2025-02-02 05:18:16,295][0m Trial 46 finished with value: 0.09275537874021364 and parameters: {'observation_period_num': 49, 'train_rates': 0.6256581059362832, 'learning_rate': 0.00031732710377568114, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8284454272625588}. Best is trial 44 with value: 0.06341018542694567.[0m
[32m[I 2025-02-02 05:19:08,503][0m Trial 47 finished with value: 0.07901812405118093 and parameters: {'observation_period_num': 19, 'train_rates': 0.6632345213659704, 'learning_rate': 0.0007946548046718762, 'batch_size': 59, 'step_size': 14, 'gamma': 0.869906151687294}. Best is trial 44 with value: 0.06341018542694567.[0m
[32m[I 2025-02-02 05:20:47,281][0m Trial 48 finished with value: 0.06965638342667227 and parameters: {'observation_period_num': 5, 'train_rates': 0.737477376359948, 'learning_rate': 0.00022172448151298093, 'batch_size': 33, 'step_size': 13, 'gamma': 0.7823156304713992}. Best is trial 44 with value: 0.06341018542694567.[0m
[32m[I 2025-02-02 05:21:13,342][0m Trial 49 finished with value: 0.11566099391936745 and parameters: {'observation_period_num': 34, 'train_rates': 0.6803171609270603, 'learning_rate': 5.4049733520575935e-05, 'batch_size': 211, 'step_size': 11, 'gamma': 0.7792896847213853}. Best is trial 44 with value: 0.06341018542694567.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-02 05:21:13,349][0m A new study created in memory with name: no-name-b95da8a1-f326-4689-a4e1-c0b0365af8b5[0m
[32m[I 2025-02-02 05:24:44,586][0m Trial 0 finished with value: 0.28325447244722335 and parameters: {'observation_period_num': 41, 'train_rates': 0.9581111429009048, 'learning_rate': 0.000320660238173394, 'batch_size': 18, 'step_size': 14, 'gamma': 0.8380665670254938}. Best is trial 0 with value: 0.28325447244722335.[0m
[32m[I 2025-02-02 05:25:26,719][0m Trial 1 finished with value: 0.1509770870417635 and parameters: {'observation_period_num': 115, 'train_rates': 0.6229209350427185, 'learning_rate': 0.00027945403316408785, 'batch_size': 70, 'step_size': 4, 'gamma': 0.9202441117417174}. Best is trial 1 with value: 0.1509770870417635.[0m
[32m[I 2025-02-02 05:25:56,807][0m Trial 2 finished with value: 0.09894672779204405 and parameters: {'observation_period_num': 41, 'train_rates': 0.8104288556515363, 'learning_rate': 0.00046407835203920593, 'batch_size': 165, 'step_size': 5, 'gamma': 0.8143712073571453}. Best is trial 2 with value: 0.09894672779204405.[0m
[32m[I 2025-02-02 05:26:37,531][0m Trial 3 finished with value: 0.1904903636006269 and parameters: {'observation_period_num': 105, 'train_rates': 0.8405013890289644, 'learning_rate': 2.1782339839306695e-05, 'batch_size': 95, 'step_size': 8, 'gamma': 0.8035601773342798}. Best is trial 2 with value: 0.09894672779204405.[0m
[32m[I 2025-02-02 05:27:04,515][0m Trial 4 finished with value: 0.09878971455273805 and parameters: {'observation_period_num': 42, 'train_rates': 0.6474427533290484, 'learning_rate': 0.0005303305495926676, 'batch_size': 152, 'step_size': 1, 'gamma': 0.9302139603021428}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:27:32,558][0m Trial 5 finished with value: 0.5430713354430584 and parameters: {'observation_period_num': 150, 'train_rates': 0.7127732211808159, 'learning_rate': 4.354251362269126e-06, 'batch_size': 148, 'step_size': 6, 'gamma': 0.7900270740839291}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:28:11,027][0m Trial 6 finished with value: 0.23844115606632577 and parameters: {'observation_period_num': 203, 'train_rates': 0.8791571864551875, 'learning_rate': 1.3291469681447084e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.9846692551991334}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:28:39,228][0m Trial 7 finished with value: 0.16821127548321801 and parameters: {'observation_period_num': 180, 'train_rates': 0.8469669564386457, 'learning_rate': 0.00020314554865024395, 'batch_size': 251, 'step_size': 10, 'gamma': 0.8755652265374545}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:29:09,513][0m Trial 8 finished with value: 0.24345308542251587 and parameters: {'observation_period_num': 83, 'train_rates': 0.9346669264937686, 'learning_rate': 0.0001234738764580517, 'batch_size': 245, 'step_size': 4, 'gamma': 0.873328677930472}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:29:37,327][0m Trial 9 finished with value: 0.4566166561405553 and parameters: {'observation_period_num': 106, 'train_rates': 0.6351874878892, 'learning_rate': 5.526821685626108e-06, 'batch_size': 131, 'step_size': 5, 'gamma': 0.848021789428965}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:30:05,528][0m Trial 10 finished with value: 0.5360025722010224 and parameters: {'observation_period_num': 14, 'train_rates': 0.7165325386846717, 'learning_rate': 1.3425700854659146e-06, 'batch_size': 202, 'step_size': 1, 'gamma': 0.9520737658786047}. Best is trial 4 with value: 0.09878971455273805.[0m
Early stopping at epoch 49
[32m[I 2025-02-02 05:30:20,533][0m Trial 11 finished with value: 0.16220328344494564 and parameters: {'observation_period_num': 52, 'train_rates': 0.7542069441238759, 'learning_rate': 0.0006711359097424045, 'batch_size': 172, 'step_size': 1, 'gamma': 0.7530912142088192}. Best is trial 4 with value: 0.09878971455273805.[0m
[32m[I 2025-02-02 05:30:49,621][0m Trial 12 finished with value: 0.0833940624109199 and parameters: {'observation_period_num': 12, 'train_rates': 0.7891816512224874, 'learning_rate': 0.0009724103181062247, 'batch_size': 194, 'step_size': 2, 'gamma': 0.9108939464228522}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:31:15,367][0m Trial 13 finished with value: 0.10367216929189686 and parameters: {'observation_period_num': 5, 'train_rates': 0.6817330344975004, 'learning_rate': 6.955876947010353e-05, 'batch_size': 202, 'step_size': 2, 'gamma': 0.9135914925542938}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:31:45,177][0m Trial 14 finished with value: 0.10753495561174009 and parameters: {'observation_period_num': 69, 'train_rates': 0.7685084097467022, 'learning_rate': 0.0008850331787233914, 'batch_size': 200, 'step_size': 3, 'gamma': 0.9154102744539335}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:32:12,247][0m Trial 15 finished with value: 0.09579858033120939 and parameters: {'observation_period_num': 29, 'train_rates': 0.601771806198456, 'learning_rate': 7.643837661107783e-05, 'batch_size': 129, 'step_size': 8, 'gamma': 0.9565551238365891}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:32:38,312][0m Trial 16 finished with value: 0.26687581410249367 and parameters: {'observation_period_num': 244, 'train_rates': 0.6003513543123682, 'learning_rate': 4.362199239092496e-05, 'batch_size': 116, 'step_size': 9, 'gamma': 0.9857933784596717}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:34:00,328][0m Trial 17 finished with value: 0.12250754881748244 and parameters: {'observation_period_num': 13, 'train_rates': 0.8965768096796863, 'learning_rate': 0.00010674224826770106, 'batch_size': 46, 'step_size': 11, 'gamma': 0.9462668324720401}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:34:25,572][0m Trial 18 finished with value: 0.32678120460899296 and parameters: {'observation_period_num': 142, 'train_rates': 0.6771560567316952, 'learning_rate': 1.049293597942741e-05, 'batch_size': 223, 'step_size': 7, 'gamma': 0.8886430779274713}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:34:59,579][0m Trial 19 finished with value: 0.09313875436782837 and parameters: {'observation_period_num': 78, 'train_rates': 0.9859856452538168, 'learning_rate': 3.7965291686024794e-05, 'batch_size': 175, 'step_size': 12, 'gamma': 0.9609460135944079}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:35:32,795][0m Trial 20 finished with value: 0.3689500093460083 and parameters: {'observation_period_num': 77, 'train_rates': 0.9863403780070026, 'learning_rate': 2.7163661649609653e-06, 'batch_size': 184, 'step_size': 12, 'gamma': 0.8930321433807544}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:36:05,553][0m Trial 21 finished with value: 0.1037507090167921 and parameters: {'observation_period_num': 26, 'train_rates': 0.7914644269285407, 'learning_rate': 2.7173222006341457e-05, 'batch_size': 125, 'step_size': 15, 'gamma': 0.9626976003810137}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:36:35,794][0m Trial 22 finished with value: 0.15520642387370268 and parameters: {'observation_period_num': 61, 'train_rates': 0.8881780687237008, 'learning_rate': 6.0256953375911766e-05, 'batch_size': 225, 'step_size': 10, 'gamma': 0.9658860917372303}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:37:07,467][0m Trial 23 finished with value: 0.19328142301394388 and parameters: {'observation_period_num': 92, 'train_rates': 0.9230225698172982, 'learning_rate': 0.0001379435216272462, 'batch_size': 180, 'step_size': 8, 'gamma': 0.9377984019339844}. Best is trial 12 with value: 0.0833940624109199.[0m
[32m[I 2025-02-02 05:37:42,575][0m Trial 24 finished with value: 0.07933858036994934 and parameters: {'observation_period_num': 32, 'train_rates': 0.9898285761269907, 'learning_rate': 1.578397270923291e-05, 'batch_size': 148, 'step_size': 12, 'gamma': 0.9037276477586152}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:38:17,102][0m Trial 25 finished with value: 0.1323390007019043 and parameters: {'observation_period_num': 62, 'train_rates': 0.9845570490066355, 'learning_rate': 1.2939409092407511e-05, 'batch_size': 154, 'step_size': 12, 'gamma': 0.8981331107529401}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:38:49,671][0m Trial 26 finished with value: 0.42879387736320496 and parameters: {'observation_period_num': 28, 'train_rates': 0.9586990411626215, 'learning_rate': 7.0990107595540695e-06, 'batch_size': 187, 'step_size': 15, 'gamma': 0.848484742273858}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:39:18,677][0m Trial 27 finished with value: 0.7509230442485132 and parameters: {'observation_period_num': 90, 'train_rates': 0.8327647469612603, 'learning_rate': 1.1773450083820014e-06, 'batch_size': 219, 'step_size': 13, 'gamma': 0.8996988096656966}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:39:51,544][0m Trial 28 finished with value: 0.2306647554301379 and parameters: {'observation_period_num': 124, 'train_rates': 0.9194667794952155, 'learning_rate': 2.3833073420584896e-05, 'batch_size': 160, 'step_size': 11, 'gamma': 0.9333237853519861}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:40:32,389][0m Trial 29 finished with value: 0.4904028723637263 and parameters: {'observation_period_num': 47, 'train_rates': 0.9568059994409006, 'learning_rate': 2.884041459212693e-06, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8542288577978437}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:41:05,533][0m Trial 30 finished with value: 0.11816809449306469 and parameters: {'observation_period_num': 31, 'train_rates': 0.8673233184317182, 'learning_rate': 4.5046865251014346e-05, 'batch_size': 138, 'step_size': 13, 'gamma': 0.9727011099840392}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:41:53,786][0m Trial 31 finished with value: 0.2665647554601873 and parameters: {'observation_period_num': 23, 'train_rates': 0.957867679321593, 'learning_rate': 1.4727907330263155e-05, 'batch_size': 83, 'step_size': 9, 'gamma': 0.9556091010807627}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:45:14,075][0m Trial 32 finished with value: 0.11577631159893985 and parameters: {'observation_period_num': 56, 'train_rates': 0.7330139786429877, 'learning_rate': 0.0002521441164392478, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9205588855564728}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:45:45,039][0m Trial 33 finished with value: 0.09856476144636192 and parameters: {'observation_period_num': 38, 'train_rates': 0.7934196338623137, 'learning_rate': 7.628752104300316e-05, 'batch_size': 142, 'step_size': 7, 'gamma': 0.911137594329978}. Best is trial 24 with value: 0.07933858036994934.[0m
[32m[I 2025-02-02 05:46:36,416][0m Trial 34 finished with value: 0.06536127508152276 and parameters: {'observation_period_num': 6, 'train_rates': 0.6741195938787397, 'learning_rate': 0.00034214557347489233, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9457767507618232}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:47:32,229][0m Trial 35 finished with value: 0.08546412715329489 and parameters: {'observation_period_num': 10, 'train_rates': 0.8147857529986103, 'learning_rate': 0.0003956114235334224, 'batch_size': 64, 'step_size': 14, 'gamma': 0.9381811495518897}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:48:33,395][0m Trial 36 finished with value: 0.0746168971354188 and parameters: {'observation_period_num': 8, 'train_rates': 0.6711674858055822, 'learning_rate': 0.00041975023481200874, 'batch_size': 51, 'step_size': 14, 'gamma': 0.9355725388735088}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:49:50,692][0m Trial 37 finished with value: 0.0657421351182011 and parameters: {'observation_period_num': 8, 'train_rates': 0.6579988517594855, 'learning_rate': 0.0005448921995426498, 'batch_size': 38, 'step_size': 15, 'gamma': 0.8843686301125603}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:50:50,745][0m Trial 38 finished with value: 0.08564859908932063 and parameters: {'observation_period_num': 41, 'train_rates': 0.6642642706921456, 'learning_rate': 0.0003896725119321689, 'batch_size': 52, 'step_size': 15, 'gamma': 0.8302527346864714}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:52:12,731][0m Trial 39 finished with value: 0.14845359430856203 and parameters: {'observation_period_num': 173, 'train_rates': 0.6514795007915146, 'learning_rate': 0.00018704261342109504, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8830274515393602}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:53:47,743][0m Trial 40 finished with value: 0.08303912949759758 and parameters: {'observation_period_num': 20, 'train_rates': 0.6939325500863441, 'learning_rate': 0.0005438253756107005, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8649942112544557}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:55:19,152][0m Trial 41 finished with value: 0.0818828788719007 and parameters: {'observation_period_num': 19, 'train_rates': 0.6943108457678203, 'learning_rate': 0.0005787702079138305, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8629928922559327}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:55:59,019][0m Trial 42 finished with value: 0.13203376754504872 and parameters: {'observation_period_num': 39, 'train_rates': 0.624867871391064, 'learning_rate': 0.0002901546154523441, 'batch_size': 69, 'step_size': 15, 'gamma': 0.8248499077409713}. Best is trial 34 with value: 0.06536127508152276.[0m
[32m[I 2025-02-02 05:56:35,008][0m Trial 43 finished with value: 0.06501702607404483 and parameters: {'observation_period_num': 10, 'train_rates': 0.700283271600247, 'learning_rate': 0.0006116146523708826, 'batch_size': 86, 'step_size': 14, 'gamma': 0.8682938331356227}. Best is trial 43 with value: 0.06501702607404483.[0m
[32m[I 2025-02-02 05:57:11,149][0m Trial 44 finished with value: 0.06823832031688795 and parameters: {'observation_period_num': 5, 'train_rates': 0.7214545618710004, 'learning_rate': 0.0007169786590405341, 'batch_size': 90, 'step_size': 14, 'gamma': 0.8798066048995883}. Best is trial 43 with value: 0.06501702607404483.[0m
[32m[I 2025-02-02 05:57:49,481][0m Trial 45 finished with value: 0.07489145826779435 and parameters: {'observation_period_num': 6, 'train_rates': 0.7265005802073919, 'learning_rate': 0.0006929353110059263, 'batch_size': 85, 'step_size': 14, 'gamma': 0.8809642824269437}. Best is trial 43 with value: 0.06501702607404483.[0m
[32m[I 2025-02-02 05:58:47,311][0m Trial 46 finished with value: 0.09565950228437169 and parameters: {'observation_period_num': 50, 'train_rates': 0.7485230672635411, 'learning_rate': 0.00039600731979312273, 'batch_size': 55, 'step_size': 15, 'gamma': 0.7920751140291686}. Best is trial 43 with value: 0.06501702607404483.[0m
[32m[I 2025-02-02 05:59:25,021][0m Trial 47 finished with value: 0.06941760104752517 and parameters: {'observation_period_num': 7, 'train_rates': 0.6474799015165104, 'learning_rate': 0.00020238706177360682, 'batch_size': 81, 'step_size': 14, 'gamma': 0.927970172720003}. Best is trial 43 with value: 0.06501702607404483.[0m
[32m[I 2025-02-02 05:59:58,038][0m Trial 48 finished with value: 0.2624768674450835 and parameters: {'observation_period_num': 219, 'train_rates': 0.6466986086270116, 'learning_rate': 0.00019795086967494432, 'batch_size': 89, 'step_size': 14, 'gamma': 0.8370878647110802}. Best is trial 43 with value: 0.06501702607404483.[0m
[32m[I 2025-02-02 06:00:39,244][0m Trial 49 finished with value: 0.08009733281512321 and parameters: {'observation_period_num': 19, 'train_rates': 0.6218232879763899, 'learning_rate': 0.0008302023265453829, 'batch_size': 75, 'step_size': 15, 'gamma': 0.9262338678534509}. Best is trial 43 with value: 0.06501702607404483.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-02 06:00:39,252][0m A new study created in memory with name: no-name-ba647ae3-73c6-4eea-b3da-8b25f3ff3ea6[0m
[32m[I 2025-02-02 06:01:34,279][0m Trial 0 finished with value: 0.19286403263340124 and parameters: {'observation_period_num': 186, 'train_rates': 0.866462133454943, 'learning_rate': 0.0004635508178795746, 'batch_size': 63, 'step_size': 12, 'gamma': 0.842514042859254}. Best is trial 0 with value: 0.19286403263340124.[0m
[32m[I 2025-02-02 06:02:09,575][0m Trial 1 finished with value: 0.1260510564819121 and parameters: {'observation_period_num': 96, 'train_rates': 0.8169809238923039, 'learning_rate': 0.00010680689182018875, 'batch_size': 108, 'step_size': 6, 'gamma': 0.889820164632765}. Best is trial 1 with value: 0.1260510564819121.[0m
[32m[I 2025-02-02 06:02:36,214][0m Trial 2 finished with value: 0.6287470877253117 and parameters: {'observation_period_num': 250, 'train_rates': 0.7327505232589987, 'learning_rate': 1.8200742655504296e-06, 'batch_size': 179, 'step_size': 10, 'gamma': 0.9627785517366995}. Best is trial 1 with value: 0.1260510564819121.[0m
[32m[I 2025-02-02 06:04:02,339][0m Trial 3 finished with value: 0.34679340720849433 and parameters: {'observation_period_num': 51, 'train_rates': 0.8473636282460415, 'learning_rate': 1.6701922647207804e-06, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8581422279245299}. Best is trial 1 with value: 0.1260510564819121.[0m
[32m[I 2025-02-02 06:04:46,103][0m Trial 4 finished with value: 0.09186761092507478 and parameters: {'observation_period_num': 17, 'train_rates': 0.786262153724954, 'learning_rate': 0.00014232956993290907, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8190553785697229}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:05:40,298][0m Trial 5 finished with value: 0.15246328037740747 and parameters: {'observation_period_num': 77, 'train_rates': 0.7018253504432262, 'learning_rate': 4.0430635208200496e-05, 'batch_size': 58, 'step_size': 5, 'gamma': 0.8076091789404904}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:06:44,901][0m Trial 6 finished with value: 0.33890415801376594 and parameters: {'observation_period_num': 119, 'train_rates': 0.93560137517803, 'learning_rate': 5.43686850810267e-06, 'batch_size': 58, 'step_size': 14, 'gamma': 0.950070333682297}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:07:16,835][0m Trial 7 finished with value: 0.3236832320690155 and parameters: {'observation_period_num': 238, 'train_rates': 0.9509471788424713, 'learning_rate': 0.0006093950383784495, 'batch_size': 146, 'step_size': 7, 'gamma': 0.7765415386861253}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:08:00,167][0m Trial 8 finished with value: 0.6994453678085546 and parameters: {'observation_period_num': 152, 'train_rates': 0.6636290992706964, 'learning_rate': 1.3092301096979608e-06, 'batch_size': 71, 'step_size': 14, 'gamma': 0.8128543008452492}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:08:31,784][0m Trial 9 finished with value: 0.6536332368850708 and parameters: {'observation_period_num': 213, 'train_rates': 0.951098355136365, 'learning_rate': 1.218255920274917e-05, 'batch_size': 158, 'step_size': 6, 'gamma': 0.7778826043924008}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:08:58,796][0m Trial 10 finished with value: 0.19458356699185392 and parameters: {'observation_period_num': 11, 'train_rates': 0.7562888642657889, 'learning_rate': 0.0001068598436323126, 'batch_size': 227, 'step_size': 1, 'gamma': 0.9053556268742049}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:09:35,468][0m Trial 11 finished with value: 0.10143861632869608 and parameters: {'observation_period_num': 5, 'train_rates': 0.8157395900502223, 'learning_rate': 0.0001301611232947443, 'batch_size': 106, 'step_size': 3, 'gamma': 0.8892176630056595}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:10:07,717][0m Trial 12 finished with value: 0.11697522860564542 and parameters: {'observation_period_num': 11, 'train_rates': 0.6064546385786511, 'learning_rate': 0.00018113923676610252, 'batch_size': 104, 'step_size': 1, 'gamma': 0.923026427854461}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:10:42,579][0m Trial 13 finished with value: 0.1660418404391208 and parameters: {'observation_period_num': 47, 'train_rates': 0.7845362325330889, 'learning_rate': 4.345887106215081e-05, 'batch_size': 111, 'step_size': 3, 'gamma': 0.8323362439296697}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:14:04,289][0m Trial 14 finished with value: 0.13539946166324546 and parameters: {'observation_period_num': 42, 'train_rates': 0.8838187560702118, 'learning_rate': 0.00029751683935224206, 'batch_size': 18, 'step_size': 10, 'gamma': 0.7507772367998404}. Best is trial 4 with value: 0.09186761092507478.[0m
[32m[I 2025-02-02 06:14:33,855][0m Trial 15 finished with value: 0.08638841336362232 and parameters: {'observation_period_num': 10, 'train_rates': 0.8051550651169826, 'learning_rate': 0.0009051671134612974, 'batch_size': 194, 'step_size': 4, 'gamma': 0.8760214723774364}. Best is trial 15 with value: 0.08638841336362232.[0m
[32m[I 2025-02-02 06:15:05,122][0m Trial 16 finished with value: 0.1699698219254772 and parameters: {'observation_period_num': 82, 'train_rates': 0.9022549185931994, 'learning_rate': 0.0009837744268871588, 'batch_size': 212, 'step_size': 10, 'gamma': 0.8664766696979768}. Best is trial 15 with value: 0.08638841336362232.[0m
[32m[I 2025-02-02 06:15:32,046][0m Trial 17 finished with value: 0.22457373199924346 and parameters: {'observation_period_num': 153, 'train_rates': 0.7786866397333209, 'learning_rate': 1.747078489214803e-05, 'batch_size': 188, 'step_size': 12, 'gamma': 0.9827958379176529}. Best is trial 15 with value: 0.08638841336362232.[0m
[32m[I 2025-02-02 06:15:59,170][0m Trial 18 finished with value: 0.09378344288121675 and parameters: {'observation_period_num': 37, 'train_rates': 0.6784128629130646, 'learning_rate': 0.0003015650886101566, 'batch_size': 196, 'step_size': 15, 'gamma': 0.8157670174598586}. Best is trial 15 with value: 0.08638841336362232.[0m
[32m[I 2025-02-02 06:16:28,616][0m Trial 19 finished with value: 0.13211216431573072 and parameters: {'observation_period_num': 105, 'train_rates': 0.7337712815237435, 'learning_rate': 0.0008816150381246764, 'batch_size': 133, 'step_size': 8, 'gamma': 0.921680512968453}. Best is trial 15 with value: 0.08638841336362232.[0m
[32m[I 2025-02-02 06:16:56,754][0m Trial 20 finished with value: 0.1769661509990692 and parameters: {'observation_period_num': 70, 'train_rates': 0.8091016009341312, 'learning_rate': 6.123974497077596e-05, 'batch_size': 242, 'step_size': 4, 'gamma': 0.8485076967726848}. Best is trial 15 with value: 0.08638841336362232.[0m
[32m[I 2025-02-02 06:17:22,589][0m Trial 21 finished with value: 0.0778500485602574 and parameters: {'observation_period_num': 29, 'train_rates': 0.6422102161860375, 'learning_rate': 0.00030877140419109905, 'batch_size': 203, 'step_size': 15, 'gamma': 0.8153165215531794}. Best is trial 21 with value: 0.0778500485602574.[0m
[32m[I 2025-02-02 06:17:48,819][0m Trial 22 finished with value: 0.07724233357466777 and parameters: {'observation_period_num': 27, 'train_rates': 0.6110657523988681, 'learning_rate': 0.00029170580933154494, 'batch_size': 162, 'step_size': 13, 'gamma': 0.7923667579209632}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:18:14,477][0m Trial 23 finished with value: 0.08619667538270137 and parameters: {'observation_period_num': 31, 'train_rates': 0.6002156397024517, 'learning_rate': 0.00031666062293112624, 'batch_size': 166, 'step_size': 12, 'gamma': 0.7808695023953277}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:18:39,114][0m Trial 24 finished with value: 0.09894970726368209 and parameters: {'observation_period_num': 63, 'train_rates': 0.6035222904321038, 'learning_rate': 0.0003062549227017852, 'batch_size': 169, 'step_size': 12, 'gamma': 0.789329507782662}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:19:04,441][0m Trial 25 finished with value: 0.15517989323622938 and parameters: {'observation_period_num': 32, 'train_rates': 0.6456232622843125, 'learning_rate': 7.162108820869009e-05, 'batch_size': 213, 'step_size': 15, 'gamma': 0.7511180262449139}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:19:28,939][0m Trial 26 finished with value: 0.0858978400767689 and parameters: {'observation_period_num': 29, 'train_rates': 0.6354343765798272, 'learning_rate': 0.0004510328949091652, 'batch_size': 253, 'step_size': 13, 'gamma': 0.7832726105697001}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:19:54,266][0m Trial 27 finished with value: 0.09919391490164257 and parameters: {'observation_period_num': 58, 'train_rates': 0.6371832020576015, 'learning_rate': 0.0005242527821376656, 'batch_size': 256, 'step_size': 13, 'gamma': 0.7929556085711125}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:20:18,887][0m Trial 28 finished with value: 0.13705825848336345 and parameters: {'observation_period_num': 145, 'train_rates': 0.6890022162105345, 'learning_rate': 0.00020021371266996176, 'batch_size': 227, 'step_size': 11, 'gamma': 0.7597246141758855}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:20:43,722][0m Trial 29 finished with value: 0.2180488587065891 and parameters: {'observation_period_num': 191, 'train_rates': 0.6373049098431071, 'learning_rate': 0.00048455269046070957, 'batch_size': 249, 'step_size': 13, 'gamma': 0.8269612232201802}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:21:10,215][0m Trial 30 finished with value: 0.27760330967353164 and parameters: {'observation_period_num': 92, 'train_rates': 0.7103049805888595, 'learning_rate': 1.8975378575117344e-05, 'batch_size': 211, 'step_size': 15, 'gamma': 0.7986554711890588}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:21:38,528][0m Trial 31 finished with value: 0.08656784679924426 and parameters: {'observation_period_num': 30, 'train_rates': 0.6183758166389316, 'learning_rate': 0.00028090335287458236, 'batch_size': 138, 'step_size': 13, 'gamma': 0.7747392524938598}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:22:06,506][0m Trial 32 finished with value: 0.09080932936023342 and parameters: {'observation_period_num': 25, 'train_rates': 0.6676439730645228, 'learning_rate': 0.0003647303970896059, 'batch_size': 162, 'step_size': 11, 'gamma': 0.8361481813499746}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:22:31,899][0m Trial 33 finished with value: 0.10714719230090508 and parameters: {'observation_period_num': 57, 'train_rates': 0.6321078546785023, 'learning_rate': 0.0001829440032040138, 'batch_size': 179, 'step_size': 11, 'gamma': 0.7677449291964551}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:22:57,729][0m Trial 34 finished with value: 0.07850770198634931 and parameters: {'observation_period_num': 32, 'train_rates': 0.6570967821698526, 'learning_rate': 0.0006834166216906658, 'batch_size': 233, 'step_size': 12, 'gamma': 0.789302530236129}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:23:22,376][0m Trial 35 finished with value: 0.09801993960501768 and parameters: {'observation_period_num': 49, 'train_rates': 0.6516024870709652, 'learning_rate': 0.0005775718293091996, 'batch_size': 238, 'step_size': 9, 'gamma': 0.7978883566974738}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:23:48,117][0m Trial 36 finished with value: 0.12350649037237825 and parameters: {'observation_period_num': 88, 'train_rates': 0.6963263168786064, 'learning_rate': 8.434923672537882e-05, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8062184451144786}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:24:16,807][0m Trial 37 finished with value: 0.5172488858214522 and parameters: {'observation_period_num': 106, 'train_rates': 0.841369121073705, 'learning_rate': 3.5241606473649604e-06, 'batch_size': 204, 'step_size': 15, 'gamma': 0.849966509080846}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:24:43,515][0m Trial 38 finished with value: 0.08319932442039471 and parameters: {'observation_period_num': 22, 'train_rates': 0.7221247644951507, 'learning_rate': 0.0006745454273581913, 'batch_size': 238, 'step_size': 13, 'gamma': 0.8256201575018794}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:25:10,718][0m Trial 39 finished with value: 0.11594072616285851 and parameters: {'observation_period_num': 73, 'train_rates': 0.7320756812530581, 'learning_rate': 0.0006896438947330975, 'batch_size': 180, 'step_size': 14, 'gamma': 0.82423754199494}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:25:36,424][0m Trial 40 finished with value: 0.0939052093308419 and parameters: {'observation_period_num': 19, 'train_rates': 0.7161766737395675, 'learning_rate': 0.0002107351061189725, 'batch_size': 234, 'step_size': 9, 'gamma': 0.839251710870317}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:26:01,261][0m Trial 41 finished with value: 0.08692594246584394 and parameters: {'observation_period_num': 23, 'train_rates': 0.6625105469575785, 'learning_rate': 0.00043985639976110554, 'batch_size': 256, 'step_size': 13, 'gamma': 0.7869228817691255}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:26:27,134][0m Trial 42 finished with value: 0.09006088667848365 and parameters: {'observation_period_num': 42, 'train_rates': 0.6212474740247379, 'learning_rate': 0.0007034811385519761, 'batch_size': 219, 'step_size': 12, 'gamma': 0.8033515140824292}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:26:53,453][0m Trial 43 finished with value: 0.07838430424099384 and parameters: {'observation_period_num': 18, 'train_rates': 0.6862299840437576, 'learning_rate': 0.0004936084156025645, 'batch_size': 244, 'step_size': 14, 'gamma': 0.7656095771093354}. Best is trial 22 with value: 0.07724233357466777.[0m
[32m[I 2025-02-02 06:27:19,772][0m Trial 44 finished with value: 0.06964258222838715 and parameters: {'observation_period_num': 6, 'train_rates': 0.6855705833837656, 'learning_rate': 0.0006657036179008692, 'batch_size': 239, 'step_size': 14, 'gamma': 0.8175371301990764}. Best is trial 44 with value: 0.06964258222838715.[0m
[32m[I 2025-02-02 06:27:48,884][0m Trial 45 finished with value: 0.09801998388443033 and parameters: {'observation_period_num': 9, 'train_rates': 0.7564998064239351, 'learning_rate': 0.00014176314091182023, 'batch_size': 222, 'step_size': 14, 'gamma': 0.7678880712052145}. Best is trial 44 with value: 0.06964258222838715.[0m
[32m[I 2025-02-02 06:28:15,422][0m Trial 46 finished with value: 0.1033591815542161 and parameters: {'observation_period_num': 50, 'train_rates': 0.6798238759542284, 'learning_rate': 0.00023538662868805897, 'batch_size': 203, 'step_size': 15, 'gamma': 0.8138837813712556}. Best is trial 44 with value: 0.06964258222838715.[0m
[32m[I 2025-02-02 06:28:40,500][0m Trial 47 finished with value: 0.07529568555572431 and parameters: {'observation_period_num': 7, 'train_rates': 0.6714230894471964, 'learning_rate': 0.0003775588284180238, 'batch_size': 243, 'step_size': 14, 'gamma': 0.7634982065738539}. Best is trial 44 with value: 0.06964258222838715.[0m
[32m[I 2025-02-02 06:29:07,893][0m Trial 48 finished with value: 0.10111759920176622 and parameters: {'observation_period_num': 14, 'train_rates': 0.7481561273884421, 'learning_rate': 9.985060403007071e-05, 'batch_size': 245, 'step_size': 14, 'gamma': 0.7634772760016344}. Best is trial 44 with value: 0.06964258222838715.[0m
[32m[I 2025-02-02 06:29:36,957][0m Trial 49 finished with value: 0.07074281169652873 and parameters: {'observation_period_num': 5, 'train_rates': 0.6973784089188891, 'learning_rate': 0.0004175812054967478, 'batch_size': 121, 'step_size': 14, 'gamma': 0.7567632291758952}. Best is trial 44 with value: 0.06964258222838715.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 17, 'train_rates': 0.7110157076242639, 'learning_rate': 0.0004809934322516847, 'batch_size': 102, 'step_size': 10, 'gamma': 0.8492778625642857}
Epoch 1/300, trend Loss: 0.4775 | 0.2452
Epoch 2/300, trend Loss: 0.2382 | 0.2271
Epoch 3/300, trend Loss: 0.2163 | 0.1926
Epoch 4/300, trend Loss: 0.1720 | 0.1532
Epoch 5/300, trend Loss: 0.1505 | 0.1412
Epoch 6/300, trend Loss: 0.1478 | 0.1284
Epoch 7/300, trend Loss: 0.1343 | 0.1218
Epoch 8/300, trend Loss: 0.1280 | 0.1156
Epoch 9/300, trend Loss: 0.1222 | 0.1139
Epoch 10/300, trend Loss: 0.1316 | 0.1203
Epoch 11/300, trend Loss: 0.1535 | 0.1307
Epoch 12/300, trend Loss: 0.1437 | 0.1180
Epoch 13/300, trend Loss: 0.1356 | 0.1100
Epoch 14/300, trend Loss: 0.1417 | 0.1132
Epoch 15/300, trend Loss: 0.1361 | 0.1080
Epoch 16/300, trend Loss: 0.1244 | 0.1132
Epoch 17/300, trend Loss: 0.1265 | 0.1144
Epoch 18/300, trend Loss: 0.1159 | 0.1125
Epoch 19/300, trend Loss: 0.1090 | 0.1120
Epoch 20/300, trend Loss: 0.1025 | 0.1114
Epoch 21/300, trend Loss: 0.1063 | 0.1435
Epoch 22/300, trend Loss: 0.1054 | 0.1436
Epoch 23/300, trend Loss: 0.1027 | 0.0955
Epoch 24/300, trend Loss: 0.0971 | 0.1343
Epoch 25/300, trend Loss: 0.0962 | 0.0885
Epoch 26/300, trend Loss: 0.0933 | 0.0930
Epoch 27/300, trend Loss: 0.0917 | 0.0880
Epoch 28/300, trend Loss: 0.0900 | 0.1148
Epoch 29/300, trend Loss: 0.0902 | 0.0849
Epoch 30/300, trend Loss: 0.0885 | 0.1252
Epoch 31/300, trend Loss: 0.0908 | 0.0837
Epoch 32/300, trend Loss: 0.0882 | 0.1146
Epoch 33/300, trend Loss: 0.0908 | 0.0831
Epoch 34/300, trend Loss: 0.0872 | 0.1095
Epoch 35/300, trend Loss: 0.0883 | 0.0819
Epoch 36/300, trend Loss: 0.0851 | 0.1025
Epoch 37/300, trend Loss: 0.0858 | 0.0808
Epoch 38/300, trend Loss: 0.0845 | 0.1071
Epoch 39/300, trend Loss: 0.0868 | 0.0819
Epoch 40/300, trend Loss: 0.0840 | 0.1021
Epoch 41/300, trend Loss: 0.0852 | 0.0797
Epoch 42/300, trend Loss: 0.0829 | 0.0948
Epoch 43/300, trend Loss: 0.0847 | 0.0791
Epoch 44/300, trend Loss: 0.0829 | 0.0954
Epoch 45/300, trend Loss: 0.0850 | 0.0793
Epoch 46/300, trend Loss: 0.0836 | 0.0898
Epoch 47/300, trend Loss: 0.0860 | 0.0805
Epoch 48/300, trend Loss: 0.0834 | 0.0896
Epoch 49/300, trend Loss: 0.0855 | 0.0820
Epoch 50/300, trend Loss: 0.0825 | 0.0877
Epoch 51/300, trend Loss: 0.0834 | 0.0819
Epoch 52/300, trend Loss: 0.0826 | 0.0847
Epoch 53/300, trend Loss: 0.0823 | 0.0806
Epoch 54/300, trend Loss: 0.0803 | 0.0835
Epoch 55/300, trend Loss: 0.0806 | 0.0804
Epoch 56/300, trend Loss: 0.0788 | 0.0800
Epoch 57/300, trend Loss: 0.0788 | 0.0806
Epoch 58/300, trend Loss: 0.0778 | 0.0799
Epoch 59/300, trend Loss: 0.0779 | 0.0798
Epoch 60/300, trend Loss: 0.0772 | 0.0798
Epoch 61/300, trend Loss: 0.0770 | 0.0801
Epoch 62/300, trend Loss: 0.0764 | 0.0782
Epoch 63/300, trend Loss: 0.0761 | 0.0791
Epoch 64/300, trend Loss: 0.0758 | 0.0782
Epoch 65/300, trend Loss: 0.0756 | 0.0785
Epoch 66/300, trend Loss: 0.0752 | 0.0779
Epoch 67/300, trend Loss: 0.0751 | 0.0790
Epoch 68/300, trend Loss: 0.0749 | 0.0780
Epoch 69/300, trend Loss: 0.0749 | 0.0789
Epoch 70/300, trend Loss: 0.0749 | 0.0785
Epoch 71/300, trend Loss: 0.0754 | 0.0798
Epoch 72/300, trend Loss: 0.0767 | 0.0781
Epoch 73/300, trend Loss: 0.0772 | 0.0772
Epoch 74/300, trend Loss: 0.0763 | 0.0764
Epoch 75/300, trend Loss: 0.0756 | 0.0769
Epoch 76/300, trend Loss: 0.0757 | 0.0758
Epoch 77/300, trend Loss: 0.0751 | 0.0766
Epoch 78/300, trend Loss: 0.0743 | 0.0760
Epoch 79/300, trend Loss: 0.0740 | 0.0761
Epoch 80/300, trend Loss: 0.0741 | 0.0755
Epoch 81/300, trend Loss: 0.0745 | 0.0756
Epoch 82/300, trend Loss: 0.0749 | 0.0748
Epoch 83/300, trend Loss: 0.0739 | 0.0751
Epoch 84/300, trend Loss: 0.0728 | 0.0748
Epoch 85/300, trend Loss: 0.0724 | 0.0747
Epoch 86/300, trend Loss: 0.0721 | 0.0745
Epoch 87/300, trend Loss: 0.0719 | 0.0746
Epoch 88/300, trend Loss: 0.0718 | 0.0744
Epoch 89/300, trend Loss: 0.0717 | 0.0744
Epoch 90/300, trend Loss: 0.0716 | 0.0743
Epoch 91/300, trend Loss: 0.0714 | 0.0744
Epoch 92/300, trend Loss: 0.0713 | 0.0743
Epoch 93/300, trend Loss: 0.0712 | 0.0743
Epoch 94/300, trend Loss: 0.0712 | 0.0743
Epoch 95/300, trend Loss: 0.0711 | 0.0742
Epoch 96/300, trend Loss: 0.0710 | 0.0743
Epoch 97/300, trend Loss: 0.0709 | 0.0742
Epoch 98/300, trend Loss: 0.0708 | 0.0742
Epoch 99/300, trend Loss: 0.0708 | 0.0741
Epoch 100/300, trend Loss: 0.0707 | 0.0741
Epoch 101/300, trend Loss: 0.0706 | 0.0741
Epoch 102/300, trend Loss: 0.0706 | 0.0741
Epoch 103/300, trend Loss: 0.0705 | 0.0740
Epoch 104/300, trend Loss: 0.0705 | 0.0740
Epoch 105/300, trend Loss: 0.0704 | 0.0740
Epoch 106/300, trend Loss: 0.0703 | 0.0740
Epoch 107/300, trend Loss: 0.0703 | 0.0740
Epoch 108/300, trend Loss: 0.0703 | 0.0739
Epoch 109/300, trend Loss: 0.0702 | 0.0739
Epoch 110/300, trend Loss: 0.0702 | 0.0739
Epoch 111/300, trend Loss: 0.0701 | 0.0739
Epoch 112/300, trend Loss: 0.0701 | 0.0738
Epoch 113/300, trend Loss: 0.0700 | 0.0738
Epoch 114/300, trend Loss: 0.0700 | 0.0738
Epoch 115/300, trend Loss: 0.0699 | 0.0738
Epoch 116/300, trend Loss: 0.0699 | 0.0738
Epoch 117/300, trend Loss: 0.0699 | 0.0737
Epoch 118/300, trend Loss: 0.0698 | 0.0737
Epoch 119/300, trend Loss: 0.0698 | 0.0737
Epoch 120/300, trend Loss: 0.0698 | 0.0737
Epoch 121/300, trend Loss: 0.0697 | 0.0737
Epoch 122/300, trend Loss: 0.0697 | 0.0737
Epoch 123/300, trend Loss: 0.0697 | 0.0736
Epoch 124/300, trend Loss: 0.0696 | 0.0736
Epoch 125/300, trend Loss: 0.0696 | 0.0736
Epoch 126/300, trend Loss: 0.0696 | 0.0736
Epoch 127/300, trend Loss: 0.0696 | 0.0736
Epoch 128/300, trend Loss: 0.0695 | 0.0736
Epoch 129/300, trend Loss: 0.0695 | 0.0736
Epoch 130/300, trend Loss: 0.0695 | 0.0735
Epoch 131/300, trend Loss: 0.0694 | 0.0735
Epoch 132/300, trend Loss: 0.0694 | 0.0735
Epoch 133/300, trend Loss: 0.0694 | 0.0735
Epoch 134/300, trend Loss: 0.0694 | 0.0735
Epoch 135/300, trend Loss: 0.0694 | 0.0735
Epoch 136/300, trend Loss: 0.0693 | 0.0735
Epoch 137/300, trend Loss: 0.0693 | 0.0735
Epoch 138/300, trend Loss: 0.0693 | 0.0735
Epoch 139/300, trend Loss: 0.0693 | 0.0734
Epoch 140/300, trend Loss: 0.0693 | 0.0734
Epoch 141/300, trend Loss: 0.0692 | 0.0734
Epoch 142/300, trend Loss: 0.0692 | 0.0734
Epoch 143/300, trend Loss: 0.0692 | 0.0734
Epoch 144/300, trend Loss: 0.0692 | 0.0734
Epoch 145/300, trend Loss: 0.0692 | 0.0734
Epoch 146/300, trend Loss: 0.0692 | 0.0734
Epoch 147/300, trend Loss: 0.0691 | 0.0734
Epoch 148/300, trend Loss: 0.0691 | 0.0734
Epoch 149/300, trend Loss: 0.0691 | 0.0734
Epoch 150/300, trend Loss: 0.0691 | 0.0734
Epoch 151/300, trend Loss: 0.0691 | 0.0733
Epoch 152/300, trend Loss: 0.0691 | 0.0733
Epoch 153/300, trend Loss: 0.0691 | 0.0733
Epoch 154/300, trend Loss: 0.0690 | 0.0733
Epoch 155/300, trend Loss: 0.0690 | 0.0733
Epoch 156/300, trend Loss: 0.0690 | 0.0733
Epoch 157/300, trend Loss: 0.0690 | 0.0733
Epoch 158/300, trend Loss: 0.0690 | 0.0733
Epoch 159/300, trend Loss: 0.0690 | 0.0733
Epoch 160/300, trend Loss: 0.0690 | 0.0733
Epoch 161/300, trend Loss: 0.0690 | 0.0733
Epoch 162/300, trend Loss: 0.0689 | 0.0733
Epoch 163/300, trend Loss: 0.0689 | 0.0733
Epoch 164/300, trend Loss: 0.0689 | 0.0733
Epoch 165/300, trend Loss: 0.0689 | 0.0733
Epoch 166/300, trend Loss: 0.0689 | 0.0733
Epoch 167/300, trend Loss: 0.0689 | 0.0733
Epoch 168/300, trend Loss: 0.0689 | 0.0733
Epoch 169/300, trend Loss: 0.0689 | 0.0733
Epoch 170/300, trend Loss: 0.0689 | 0.0732
Epoch 171/300, trend Loss: 0.0689 | 0.0732
Epoch 172/300, trend Loss: 0.0689 | 0.0732
Epoch 173/300, trend Loss: 0.0689 | 0.0732
Epoch 174/300, trend Loss: 0.0688 | 0.0732
Epoch 175/300, trend Loss: 0.0688 | 0.0732
Epoch 176/300, trend Loss: 0.0688 | 0.0732
Epoch 177/300, trend Loss: 0.0688 | 0.0732
Epoch 178/300, trend Loss: 0.0688 | 0.0732
Epoch 179/300, trend Loss: 0.0688 | 0.0732
Epoch 180/300, trend Loss: 0.0688 | 0.0732
Epoch 181/300, trend Loss: 0.0688 | 0.0732
Epoch 182/300, trend Loss: 0.0688 | 0.0732
Epoch 183/300, trend Loss: 0.0688 | 0.0732
Epoch 184/300, trend Loss: 0.0688 | 0.0732
Epoch 185/300, trend Loss: 0.0688 | 0.0732
Epoch 186/300, trend Loss: 0.0688 | 0.0732
Epoch 187/300, trend Loss: 0.0688 | 0.0732
Epoch 188/300, trend Loss: 0.0688 | 0.0732
Epoch 189/300, trend Loss: 0.0688 | 0.0732
Epoch 190/300, trend Loss: 0.0688 | 0.0732
Epoch 191/300, trend Loss: 0.0687 | 0.0732
Epoch 192/300, trend Loss: 0.0687 | 0.0732
Epoch 193/300, trend Loss: 0.0687 | 0.0732
Epoch 194/300, trend Loss: 0.0687 | 0.0732
Epoch 195/300, trend Loss: 0.0687 | 0.0732
Epoch 196/300, trend Loss: 0.0687 | 0.0732
Epoch 197/300, trend Loss: 0.0687 | 0.0732
Epoch 198/300, trend Loss: 0.0687 | 0.0732
Epoch 199/300, trend Loss: 0.0687 | 0.0732
Epoch 200/300, trend Loss: 0.0687 | 0.0732
Epoch 201/300, trend Loss: 0.0687 | 0.0732
Epoch 202/300, trend Loss: 0.0687 | 0.0732
Epoch 203/300, trend Loss: 0.0687 | 0.0732
Epoch 204/300, trend Loss: 0.0687 | 0.0732
Epoch 205/300, trend Loss: 0.0687 | 0.0732
Epoch 206/300, trend Loss: 0.0687 | 0.0732
Epoch 207/300, trend Loss: 0.0687 | 0.0732
Epoch 208/300, trend Loss: 0.0687 | 0.0732
Epoch 209/300, trend Loss: 0.0687 | 0.0732
Epoch 210/300, trend Loss: 0.0687 | 0.0732
Epoch 211/300, trend Loss: 0.0687 | 0.0732
Epoch 212/300, trend Loss: 0.0687 | 0.0732
Epoch 213/300, trend Loss: 0.0687 | 0.0731
Epoch 214/300, trend Loss: 0.0687 | 0.0731
Epoch 215/300, trend Loss: 0.0687 | 0.0731
Epoch 216/300, trend Loss: 0.0687 | 0.0731
Epoch 217/300, trend Loss: 0.0687 | 0.0731
Epoch 218/300, trend Loss: 0.0687 | 0.0731
Epoch 219/300, trend Loss: 0.0687 | 0.0731
Epoch 220/300, trend Loss: 0.0687 | 0.0731
Epoch 221/300, trend Loss: 0.0687 | 0.0731
Epoch 222/300, trend Loss: 0.0687 | 0.0731
Epoch 223/300, trend Loss: 0.0687 | 0.0731
Epoch 224/300, trend Loss: 0.0687 | 0.0731
Epoch 225/300, trend Loss: 0.0687 | 0.0731
Epoch 226/300, trend Loss: 0.0687 | 0.0731
Epoch 227/300, trend Loss: 0.0687 | 0.0731
Epoch 228/300, trend Loss: 0.0686 | 0.0731
Epoch 229/300, trend Loss: 0.0686 | 0.0731
Epoch 230/300, trend Loss: 0.0686 | 0.0731
Epoch 231/300, trend Loss: 0.0686 | 0.0731
Epoch 232/300, trend Loss: 0.0686 | 0.0731
Epoch 233/300, trend Loss: 0.0686 | 0.0731
Epoch 234/300, trend Loss: 0.0686 | 0.0731
Epoch 235/300, trend Loss: 0.0686 | 0.0731
Epoch 236/300, trend Loss: 0.0686 | 0.0731
Epoch 237/300, trend Loss: 0.0686 | 0.0731
Epoch 238/300, trend Loss: 0.0686 | 0.0731
Epoch 239/300, trend Loss: 0.0686 | 0.0731
Epoch 240/300, trend Loss: 0.0686 | 0.0731
Epoch 241/300, trend Loss: 0.0686 | 0.0731
Epoch 242/300, trend Loss: 0.0686 | 0.0731
Epoch 243/300, trend Loss: 0.0686 | 0.0731
Epoch 244/300, trend Loss: 0.0686 | 0.0731
Epoch 245/300, trend Loss: 0.0686 | 0.0731
Epoch 246/300, trend Loss: 0.0686 | 0.0731
Epoch 247/300, trend Loss: 0.0686 | 0.0731
Epoch 248/300, trend Loss: 0.0686 | 0.0731
Epoch 249/300, trend Loss: 0.0686 | 0.0731
Epoch 250/300, trend Loss: 0.0686 | 0.0731
Epoch 251/300, trend Loss: 0.0686 | 0.0731
Epoch 252/300, trend Loss: 0.0686 | 0.0731
Epoch 253/300, trend Loss: 0.0686 | 0.0731
Epoch 254/300, trend Loss: 0.0686 | 0.0731
Epoch 255/300, trend Loss: 0.0686 | 0.0731
Epoch 256/300, trend Loss: 0.0686 | 0.0731
Epoch 257/300, trend Loss: 0.0686 | 0.0731
Epoch 258/300, trend Loss: 0.0686 | 0.0731
Epoch 259/300, trend Loss: 0.0686 | 0.0731
Epoch 260/300, trend Loss: 0.0686 | 0.0731
Epoch 261/300, trend Loss: 0.0686 | 0.0731
Epoch 262/300, trend Loss: 0.0686 | 0.0731
Epoch 263/300, trend Loss: 0.0686 | 0.0731
Epoch 264/300, trend Loss: 0.0686 | 0.0731
Epoch 265/300, trend Loss: 0.0686 | 0.0731
Epoch 266/300, trend Loss: 0.0686 | 0.0731
Epoch 267/300, trend Loss: 0.0686 | 0.0731
Epoch 268/300, trend Loss: 0.0686 | 0.0731
Epoch 269/300, trend Loss: 0.0686 | 0.0731
Epoch 270/300, trend Loss: 0.0686 | 0.0731
Epoch 271/300, trend Loss: 0.0686 | 0.0731
Epoch 272/300, trend Loss: 0.0686 | 0.0731
Epoch 273/300, trend Loss: 0.0686 | 0.0731
Epoch 274/300, trend Loss: 0.0686 | 0.0731
Epoch 275/300, trend Loss: 0.0686 | 0.0731
Epoch 276/300, trend Loss: 0.0686 | 0.0731
Epoch 277/300, trend Loss: 0.0686 | 0.0731
Epoch 278/300, trend Loss: 0.0686 | 0.0731
Epoch 279/300, trend Loss: 0.0686 | 0.0731
Epoch 280/300, trend Loss: 0.0686 | 0.0731
Epoch 281/300, trend Loss: 0.0686 | 0.0731
Epoch 282/300, trend Loss: 0.0686 | 0.0731
Epoch 283/300, trend Loss: 0.0686 | 0.0731
Epoch 284/300, trend Loss: 0.0686 | 0.0731
Epoch 285/300, trend Loss: 0.0686 | 0.0731
Epoch 286/300, trend Loss: 0.0686 | 0.0731
Epoch 287/300, trend Loss: 0.0686 | 0.0731
Epoch 288/300, trend Loss: 0.0686 | 0.0731
Epoch 289/300, trend Loss: 0.0686 | 0.0731
Epoch 290/300, trend Loss: 0.0686 | 0.0731
Epoch 291/300, trend Loss: 0.0686 | 0.0731
Epoch 292/300, trend Loss: 0.0686 | 0.0731
Epoch 293/300, trend Loss: 0.0686 | 0.0731
Epoch 294/300, trend Loss: 0.0686 | 0.0731
Epoch 295/300, trend Loss: 0.0686 | 0.0731
Epoch 296/300, trend Loss: 0.0686 | 0.0731
Epoch 297/300, trend Loss: 0.0686 | 0.0731
Epoch 298/300, trend Loss: 0.0686 | 0.0731
Epoch 299/300, trend Loss: 0.0686 | 0.0731
Epoch 300/300, trend Loss: 0.0686 | 0.0731
Training seasonal_0 component with params: {'observation_period_num': 34, 'train_rates': 0.9874066316151988, 'learning_rate': 1.7288446551650286e-05, 'batch_size': 31, 'step_size': 15, 'gamma': 0.794056719763528}
Epoch 1/300, seasonal_0 Loss: 0.7618 | 0.4430
Epoch 2/300, seasonal_0 Loss: 0.3881 | 0.2793
Epoch 3/300, seasonal_0 Loss: 0.2662 | 0.2110
Epoch 4/300, seasonal_0 Loss: 0.2216 | 0.1762
Epoch 5/300, seasonal_0 Loss: 0.2015 | 0.1556
Epoch 6/300, seasonal_0 Loss: 0.1882 | 0.1416
Epoch 7/300, seasonal_0 Loss: 0.1779 | 0.1309
Epoch 8/300, seasonal_0 Loss: 0.1695 | 0.1222
Epoch 9/300, seasonal_0 Loss: 0.1636 | 0.1157
Epoch 10/300, seasonal_0 Loss: 0.1587 | 0.1104
Epoch 11/300, seasonal_0 Loss: 0.1543 | 0.1056
Epoch 12/300, seasonal_0 Loss: 0.1503 | 0.1014
Epoch 13/300, seasonal_0 Loss: 0.1467 | 0.0976
Epoch 14/300, seasonal_0 Loss: 0.1434 | 0.0942
Epoch 15/300, seasonal_0 Loss: 0.1404 | 0.0912
Epoch 16/300, seasonal_0 Loss: 0.1390 | 0.0912
Epoch 17/300, seasonal_0 Loss: 0.1369 | 0.0894
Epoch 18/300, seasonal_0 Loss: 0.1353 | 0.0879
Epoch 19/300, seasonal_0 Loss: 0.1337 | 0.0867
Epoch 20/300, seasonal_0 Loss: 0.1322 | 0.0855
Epoch 21/300, seasonal_0 Loss: 0.1309 | 0.0845
Epoch 22/300, seasonal_0 Loss: 0.1296 | 0.0836
Epoch 23/300, seasonal_0 Loss: 0.1283 | 0.0827
Epoch 24/300, seasonal_0 Loss: 0.1285 | 0.0961
Epoch 25/300, seasonal_0 Loss: 0.1275 | 0.0977
Epoch 26/300, seasonal_0 Loss: 0.1267 | 0.0986
Epoch 27/300, seasonal_0 Loss: 0.1257 | 0.0986
Epoch 28/300, seasonal_0 Loss: 0.1248 | 0.0981
Epoch 29/300, seasonal_0 Loss: 0.1238 | 0.0970
Epoch 30/300, seasonal_0 Loss: 0.1228 | 0.0956
Epoch 31/300, seasonal_0 Loss: 0.1224 | 0.1030
Epoch 32/300, seasonal_0 Loss: 0.1219 | 0.1002
Epoch 33/300, seasonal_0 Loss: 0.1213 | 0.0972
Epoch 34/300, seasonal_0 Loss: 0.1205 | 0.0943
Epoch 35/300, seasonal_0 Loss: 0.1196 | 0.0917
Epoch 36/300, seasonal_0 Loss: 0.1187 | 0.0892
Epoch 37/300, seasonal_0 Loss: 0.1178 | 0.0870
Epoch 38/300, seasonal_0 Loss: 0.1170 | 0.0850
Epoch 39/300, seasonal_0 Loss: 0.1163 | 0.0812
Epoch 40/300, seasonal_0 Loss: 0.1158 | 0.0795
Epoch 41/300, seasonal_0 Loss: 0.1152 | 0.0782
Epoch 42/300, seasonal_0 Loss: 0.1146 | 0.0771
Epoch 43/300, seasonal_0 Loss: 0.1139 | 0.0761
Epoch 44/300, seasonal_0 Loss: 0.1133 | 0.0751
Epoch 45/300, seasonal_0 Loss: 0.1127 | 0.0743
Epoch 46/300, seasonal_0 Loss: 0.1121 | 0.0714
Epoch 47/300, seasonal_0 Loss: 0.1118 | 0.0708
Epoch 48/300, seasonal_0 Loss: 0.1113 | 0.0703
Epoch 49/300, seasonal_0 Loss: 0.1109 | 0.0699
Epoch 50/300, seasonal_0 Loss: 0.1105 | 0.0694
Epoch 51/300, seasonal_0 Loss: 0.1101 | 0.0690
Epoch 52/300, seasonal_0 Loss: 0.1097 | 0.0686
Epoch 53/300, seasonal_0 Loss: 0.1093 | 0.0682
Epoch 54/300, seasonal_0 Loss: 0.1088 | 0.0666
Epoch 55/300, seasonal_0 Loss: 0.1086 | 0.0663
Epoch 56/300, seasonal_0 Loss: 0.1083 | 0.0661
Epoch 57/300, seasonal_0 Loss: 0.1080 | 0.0658
Epoch 58/300, seasonal_0 Loss: 0.1077 | 0.0656
Epoch 59/300, seasonal_0 Loss: 0.1074 | 0.0654
Epoch 60/300, seasonal_0 Loss: 0.1071 | 0.0651
Epoch 61/300, seasonal_0 Loss: 0.1067 | 0.0643
Epoch 62/300, seasonal_0 Loss: 0.1065 | 0.0641
Epoch 63/300, seasonal_0 Loss: 0.1063 | 0.0639
Epoch 64/300, seasonal_0 Loss: 0.1061 | 0.0638
Epoch 65/300, seasonal_0 Loss: 0.1058 | 0.0636
Epoch 66/300, seasonal_0 Loss: 0.1056 | 0.0635
Epoch 67/300, seasonal_0 Loss: 0.1054 | 0.0633
Epoch 68/300, seasonal_0 Loss: 0.1052 | 0.0631
Epoch 69/300, seasonal_0 Loss: 0.1049 | 0.0627
Epoch 70/300, seasonal_0 Loss: 0.1047 | 0.0626
Epoch 71/300, seasonal_0 Loss: 0.1046 | 0.0625
Epoch 72/300, seasonal_0 Loss: 0.1044 | 0.0624
Epoch 73/300, seasonal_0 Loss: 0.1042 | 0.0623
Epoch 74/300, seasonal_0 Loss: 0.1041 | 0.0622
Epoch 75/300, seasonal_0 Loss: 0.1039 | 0.0620
Epoch 76/300, seasonal_0 Loss: 0.1037 | 0.0619
Epoch 77/300, seasonal_0 Loss: 0.1035 | 0.0618
Epoch 78/300, seasonal_0 Loss: 0.1034 | 0.0617
Epoch 79/300, seasonal_0 Loss: 0.1033 | 0.0616
Epoch 80/300, seasonal_0 Loss: 0.1031 | 0.0615
Epoch 81/300, seasonal_0 Loss: 0.1030 | 0.0615
Epoch 82/300, seasonal_0 Loss: 0.1029 | 0.0614
Epoch 83/300, seasonal_0 Loss: 0.1027 | 0.0613
Epoch 84/300, seasonal_0 Loss: 0.1026 | 0.0612
Epoch 85/300, seasonal_0 Loss: 0.1024 | 0.0612
Epoch 86/300, seasonal_0 Loss: 0.1023 | 0.0611
Epoch 87/300, seasonal_0 Loss: 0.1022 | 0.0610
Epoch 88/300, seasonal_0 Loss: 0.1021 | 0.0610
Epoch 89/300, seasonal_0 Loss: 0.1020 | 0.0609
Epoch 90/300, seasonal_0 Loss: 0.1019 | 0.0609
Epoch 91/300, seasonal_0 Loss: 0.1018 | 0.0608
Epoch 92/300, seasonal_0 Loss: 0.1017 | 0.0608
Epoch 93/300, seasonal_0 Loss: 0.1016 | 0.0608
Epoch 94/300, seasonal_0 Loss: 0.1015 | 0.0607
Epoch 95/300, seasonal_0 Loss: 0.1014 | 0.0607
Epoch 96/300, seasonal_0 Loss: 0.1013 | 0.0606
Epoch 97/300, seasonal_0 Loss: 0.1013 | 0.0606
Epoch 98/300, seasonal_0 Loss: 0.1012 | 0.0605
Epoch 99/300, seasonal_0 Loss: 0.1011 | 0.0605
Epoch 100/300, seasonal_0 Loss: 0.1010 | 0.0605
Epoch 101/300, seasonal_0 Loss: 0.1009 | 0.0604
Epoch 102/300, seasonal_0 Loss: 0.1009 | 0.0604
Epoch 103/300, seasonal_0 Loss: 0.1008 | 0.0604
Epoch 104/300, seasonal_0 Loss: 0.1007 | 0.0603
Epoch 105/300, seasonal_0 Loss: 0.1007 | 0.0603
Epoch 106/300, seasonal_0 Loss: 0.1006 | 0.0603
Epoch 107/300, seasonal_0 Loss: 0.1005 | 0.0603
Epoch 108/300, seasonal_0 Loss: 0.1005 | 0.0602
Epoch 109/300, seasonal_0 Loss: 0.1004 | 0.0602
Epoch 110/300, seasonal_0 Loss: 0.1004 | 0.0602
Epoch 111/300, seasonal_0 Loss: 0.1003 | 0.0602
Epoch 112/300, seasonal_0 Loss: 0.1003 | 0.0601
Epoch 113/300, seasonal_0 Loss: 0.1002 | 0.0601
Epoch 114/300, seasonal_0 Loss: 0.1002 | 0.0601
Epoch 115/300, seasonal_0 Loss: 0.1001 | 0.0601
Epoch 116/300, seasonal_0 Loss: 0.1001 | 0.0601
Epoch 117/300, seasonal_0 Loss: 0.1000 | 0.0601
Epoch 118/300, seasonal_0 Loss: 0.1000 | 0.0600
Epoch 119/300, seasonal_0 Loss: 0.1000 | 0.0600
Epoch 120/300, seasonal_0 Loss: 0.0999 | 0.0600
Epoch 121/300, seasonal_0 Loss: 0.0999 | 0.0600
Epoch 122/300, seasonal_0 Loss: 0.0998 | 0.0600
Epoch 123/300, seasonal_0 Loss: 0.0998 | 0.0600
Epoch 124/300, seasonal_0 Loss: 0.0998 | 0.0599
Epoch 125/300, seasonal_0 Loss: 0.0997 | 0.0599
Epoch 126/300, seasonal_0 Loss: 0.0997 | 0.0599
Epoch 127/300, seasonal_0 Loss: 0.0997 | 0.0599
Epoch 128/300, seasonal_0 Loss: 0.0996 | 0.0599
Epoch 129/300, seasonal_0 Loss: 0.0996 | 0.0599
Epoch 130/300, seasonal_0 Loss: 0.0996 | 0.0599
Epoch 131/300, seasonal_0 Loss: 0.0995 | 0.0598
Epoch 132/300, seasonal_0 Loss: 0.0995 | 0.0598
Epoch 133/300, seasonal_0 Loss: 0.0995 | 0.0598
Epoch 134/300, seasonal_0 Loss: 0.0995 | 0.0598
Epoch 135/300, seasonal_0 Loss: 0.0994 | 0.0598
Epoch 136/300, seasonal_0 Loss: 0.0994 | 0.0598
Epoch 137/300, seasonal_0 Loss: 0.0994 | 0.0598
Epoch 138/300, seasonal_0 Loss: 0.0994 | 0.0598
Epoch 139/300, seasonal_0 Loss: 0.0993 | 0.0598
Epoch 140/300, seasonal_0 Loss: 0.0993 | 0.0598
Epoch 141/300, seasonal_0 Loss: 0.0993 | 0.0597
Epoch 142/300, seasonal_0 Loss: 0.0993 | 0.0597
Epoch 143/300, seasonal_0 Loss: 0.0993 | 0.0597
Epoch 144/300, seasonal_0 Loss: 0.0992 | 0.0597
Epoch 145/300, seasonal_0 Loss: 0.0992 | 0.0597
Epoch 146/300, seasonal_0 Loss: 0.0992 | 0.0597
Epoch 147/300, seasonal_0 Loss: 0.0992 | 0.0597
Epoch 148/300, seasonal_0 Loss: 0.0992 | 0.0597
Epoch 149/300, seasonal_0 Loss: 0.0992 | 0.0597
Epoch 150/300, seasonal_0 Loss: 0.0991 | 0.0597
Epoch 151/300, seasonal_0 Loss: 0.0991 | 0.0597
Epoch 152/300, seasonal_0 Loss: 0.0991 | 0.0597
Epoch 153/300, seasonal_0 Loss: 0.0991 | 0.0597
Epoch 154/300, seasonal_0 Loss: 0.0991 | 0.0597
Epoch 155/300, seasonal_0 Loss: 0.0991 | 0.0597
Epoch 156/300, seasonal_0 Loss: 0.0991 | 0.0596
Epoch 157/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 158/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 159/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 160/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 161/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 162/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 163/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 164/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 165/300, seasonal_0 Loss: 0.0990 | 0.0596
Epoch 166/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 167/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 168/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 169/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 170/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 171/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 172/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 173/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 174/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 175/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 176/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 177/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 178/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 179/300, seasonal_0 Loss: 0.0989 | 0.0596
Epoch 180/300, seasonal_0 Loss: 0.0988 | 0.0596
Epoch 181/300, seasonal_0 Loss: 0.0988 | 0.0596
Epoch 182/300, seasonal_0 Loss: 0.0988 | 0.0596
Epoch 183/300, seasonal_0 Loss: 0.0988 | 0.0596
Epoch 184/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 185/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 186/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 187/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 188/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 189/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 190/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 191/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 192/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 193/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 194/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 195/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 196/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 197/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 198/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 199/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 200/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 201/300, seasonal_0 Loss: 0.0988 | 0.0595
Epoch 202/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 203/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 204/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 205/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 206/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 207/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 208/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 209/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 210/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 211/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 212/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 213/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 214/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 215/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 216/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 217/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 218/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 219/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 220/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 221/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 222/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 223/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 224/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 225/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 226/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 227/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 228/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 229/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 230/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 231/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 232/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 233/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 234/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 235/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 236/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 237/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 238/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 239/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 240/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 241/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 242/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 243/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 244/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 245/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 246/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 247/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 248/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 249/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 250/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 251/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 252/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 253/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 254/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 255/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 256/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 257/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 258/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 259/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 260/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 261/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 262/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 263/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 264/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 265/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 266/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 267/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 268/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 269/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 270/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 271/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 272/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 273/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 274/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 275/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 276/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 277/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 278/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 279/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 280/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 281/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 282/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 283/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 284/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 285/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 286/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 287/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 288/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 289/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 290/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 291/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 292/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 293/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 294/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 295/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 296/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 297/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 298/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 299/300, seasonal_0 Loss: 0.0987 | 0.0595
Epoch 300/300, seasonal_0 Loss: 0.0987 | 0.0595
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.6118968039825038, 'learning_rate': 0.0009874329988837538, 'batch_size': 71, 'step_size': 14, 'gamma': 0.7731908737638313}
Epoch 1/300, seasonal_1 Loss: 0.3826 | 0.2046
Epoch 2/300, seasonal_1 Loss: 0.1660 | 0.1377
Epoch 3/300, seasonal_1 Loss: 0.1505 | 0.1286
Epoch 4/300, seasonal_1 Loss: 0.1345 | 0.1959
Epoch 5/300, seasonal_1 Loss: 0.1250 | 0.3994
Epoch 6/300, seasonal_1 Loss: 0.1215 | 0.4549
Epoch 7/300, seasonal_1 Loss: 0.1328 | 0.1945
Epoch 8/300, seasonal_1 Loss: 0.1324 | 0.1157
Epoch 9/300, seasonal_1 Loss: 0.1331 | 0.0975
Epoch 10/300, seasonal_1 Loss: 0.1120 | 0.1034
Epoch 11/300, seasonal_1 Loss: 0.1061 | 0.0847
Epoch 12/300, seasonal_1 Loss: 0.1074 | 0.0797
Epoch 13/300, seasonal_1 Loss: 0.1005 | 0.0747
Epoch 14/300, seasonal_1 Loss: 0.1021 | 0.0801
Epoch 15/300, seasonal_1 Loss: 0.1023 | 0.0946
Epoch 16/300, seasonal_1 Loss: 0.1113 | 0.0916
Epoch 17/300, seasonal_1 Loss: 0.1044 | 0.0928
Epoch 18/300, seasonal_1 Loss: 0.0980 | 0.0859
Epoch 19/300, seasonal_1 Loss: 0.0966 | 0.0774
Epoch 20/300, seasonal_1 Loss: 0.0964 | 0.0787
Epoch 21/300, seasonal_1 Loss: 0.0959 | 0.0821
Epoch 22/300, seasonal_1 Loss: 0.0991 | 0.0728
Epoch 23/300, seasonal_1 Loss: 0.0956 | 0.0737
Epoch 24/300, seasonal_1 Loss: 0.0935 | 0.0691
Epoch 25/300, seasonal_1 Loss: 0.0926 | 0.0738
Epoch 26/300, seasonal_1 Loss: 0.0943 | 0.0707
Epoch 27/300, seasonal_1 Loss: 0.0925 | 0.0762
Epoch 28/300, seasonal_1 Loss: 0.0928 | 0.0676
Epoch 29/300, seasonal_1 Loss: 0.0890 | 0.0713
Epoch 30/300, seasonal_1 Loss: 0.0890 | 0.0670
Epoch 31/300, seasonal_1 Loss: 0.0871 | 0.0684
Epoch 32/300, seasonal_1 Loss: 0.0870 | 0.0664
Epoch 33/300, seasonal_1 Loss: 0.0857 | 0.0687
Epoch 34/300, seasonal_1 Loss: 0.0860 | 0.0671
Epoch 35/300, seasonal_1 Loss: 0.0849 | 0.0687
Epoch 36/300, seasonal_1 Loss: 0.0844 | 0.0663
Epoch 37/300, seasonal_1 Loss: 0.0832 | 0.0680
Epoch 38/300, seasonal_1 Loss: 0.0829 | 0.0664
Epoch 39/300, seasonal_1 Loss: 0.0822 | 0.0665
Epoch 40/300, seasonal_1 Loss: 0.0825 | 0.0656
Epoch 41/300, seasonal_1 Loss: 0.0816 | 0.0654
Epoch 42/300, seasonal_1 Loss: 0.0817 | 0.0639
Epoch 43/300, seasonal_1 Loss: 0.0813 | 0.0655
Epoch 44/300, seasonal_1 Loss: 0.0813 | 0.0655
Epoch 45/300, seasonal_1 Loss: 0.0806 | 0.0656
Epoch 46/300, seasonal_1 Loss: 0.0806 | 0.0647
Epoch 47/300, seasonal_1 Loss: 0.0804 | 0.0655
Epoch 48/300, seasonal_1 Loss: 0.0803 | 0.0645
Epoch 49/300, seasonal_1 Loss: 0.0804 | 0.0657
Epoch 50/300, seasonal_1 Loss: 0.0801 | 0.0628
Epoch 51/300, seasonal_1 Loss: 0.0808 | 0.0633
Epoch 52/300, seasonal_1 Loss: 0.0807 | 0.0632
Epoch 53/300, seasonal_1 Loss: 0.0804 | 0.0630
Epoch 54/300, seasonal_1 Loss: 0.0800 | 0.0631
Epoch 55/300, seasonal_1 Loss: 0.0796 | 0.0628
Epoch 56/300, seasonal_1 Loss: 0.0792 | 0.0629
Epoch 57/300, seasonal_1 Loss: 0.0787 | 0.0621
Epoch 58/300, seasonal_1 Loss: 0.0785 | 0.0624
Epoch 59/300, seasonal_1 Loss: 0.0782 | 0.0615
Epoch 60/300, seasonal_1 Loss: 0.0781 | 0.0613
Epoch 61/300, seasonal_1 Loss: 0.0781 | 0.0607
Epoch 62/300, seasonal_1 Loss: 0.0782 | 0.0607
Epoch 63/300, seasonal_1 Loss: 0.0779 | 0.0608
Epoch 64/300, seasonal_1 Loss: 0.0772 | 0.0609
Epoch 65/300, seasonal_1 Loss: 0.0767 | 0.0607
Epoch 66/300, seasonal_1 Loss: 0.0764 | 0.0608
Epoch 67/300, seasonal_1 Loss: 0.0761 | 0.0607
Epoch 68/300, seasonal_1 Loss: 0.0759 | 0.0606
Epoch 69/300, seasonal_1 Loss: 0.0757 | 0.0605
Epoch 70/300, seasonal_1 Loss: 0.0756 | 0.0604
Epoch 71/300, seasonal_1 Loss: 0.0753 | 0.0604
Epoch 72/300, seasonal_1 Loss: 0.0752 | 0.0605
Epoch 73/300, seasonal_1 Loss: 0.0750 | 0.0605
Epoch 74/300, seasonal_1 Loss: 0.0749 | 0.0606
Epoch 75/300, seasonal_1 Loss: 0.0748 | 0.0605
Epoch 76/300, seasonal_1 Loss: 0.0747 | 0.0605
Epoch 77/300, seasonal_1 Loss: 0.0746 | 0.0605
Epoch 78/300, seasonal_1 Loss: 0.0744 | 0.0601
Epoch 79/300, seasonal_1 Loss: 0.0744 | 0.0602
Epoch 80/300, seasonal_1 Loss: 0.0743 | 0.0602
Epoch 81/300, seasonal_1 Loss: 0.0742 | 0.0601
Epoch 82/300, seasonal_1 Loss: 0.0740 | 0.0601
Epoch 83/300, seasonal_1 Loss: 0.0739 | 0.0600
Epoch 84/300, seasonal_1 Loss: 0.0738 | 0.0600
Epoch 85/300, seasonal_1 Loss: 0.0737 | 0.0596
Epoch 86/300, seasonal_1 Loss: 0.0737 | 0.0596
Epoch 87/300, seasonal_1 Loss: 0.0737 | 0.0596
Epoch 88/300, seasonal_1 Loss: 0.0736 | 0.0596
Epoch 89/300, seasonal_1 Loss: 0.0734 | 0.0595
Epoch 90/300, seasonal_1 Loss: 0.0734 | 0.0595
Epoch 91/300, seasonal_1 Loss: 0.0733 | 0.0595
Epoch 92/300, seasonal_1 Loss: 0.0732 | 0.0599
Epoch 93/300, seasonal_1 Loss: 0.0733 | 0.0599
Epoch 94/300, seasonal_1 Loss: 0.0732 | 0.0599
Epoch 95/300, seasonal_1 Loss: 0.0731 | 0.0599
Epoch 96/300, seasonal_1 Loss: 0.0730 | 0.0599
Epoch 97/300, seasonal_1 Loss: 0.0729 | 0.0598
Epoch 98/300, seasonal_1 Loss: 0.0728 | 0.0598
Epoch 99/300, seasonal_1 Loss: 0.0728 | 0.0599
Epoch 100/300, seasonal_1 Loss: 0.0728 | 0.0599
Epoch 101/300, seasonal_1 Loss: 0.0728 | 0.0598
Epoch 102/300, seasonal_1 Loss: 0.0727 | 0.0597
Epoch 103/300, seasonal_1 Loss: 0.0726 | 0.0596
Epoch 104/300, seasonal_1 Loss: 0.0725 | 0.0596
Epoch 105/300, seasonal_1 Loss: 0.0724 | 0.0595
Epoch 106/300, seasonal_1 Loss: 0.0723 | 0.0598
Epoch 107/300, seasonal_1 Loss: 0.0722 | 0.0599
Epoch 108/300, seasonal_1 Loss: 0.0721 | 0.0598
Epoch 109/300, seasonal_1 Loss: 0.0720 | 0.0597
Epoch 110/300, seasonal_1 Loss: 0.0720 | 0.0597
Epoch 111/300, seasonal_1 Loss: 0.0719 | 0.0596
Epoch 112/300, seasonal_1 Loss: 0.0719 | 0.0596
Epoch 113/300, seasonal_1 Loss: 0.0719 | 0.0596
Epoch 114/300, seasonal_1 Loss: 0.0720 | 0.0595
Epoch 115/300, seasonal_1 Loss: 0.0720 | 0.0594
Epoch 116/300, seasonal_1 Loss: 0.0720 | 0.0594
Epoch 117/300, seasonal_1 Loss: 0.0721 | 0.0594
Epoch 118/300, seasonal_1 Loss: 0.0722 | 0.0594
Epoch 119/300, seasonal_1 Loss: 0.0723 | 0.0594
Epoch 120/300, seasonal_1 Loss: 0.0723 | 0.0595
Epoch 121/300, seasonal_1 Loss: 0.0721 | 0.0596
Epoch 122/300, seasonal_1 Loss: 0.0719 | 0.0595
Epoch 123/300, seasonal_1 Loss: 0.0717 | 0.0595
Epoch 124/300, seasonal_1 Loss: 0.0716 | 0.0594
Epoch 125/300, seasonal_1 Loss: 0.0716 | 0.0594
Epoch 126/300, seasonal_1 Loss: 0.0715 | 0.0593
Epoch 127/300, seasonal_1 Loss: 0.0714 | 0.0594
Epoch 128/300, seasonal_1 Loss: 0.0714 | 0.0594
Epoch 129/300, seasonal_1 Loss: 0.0713 | 0.0593
Epoch 130/300, seasonal_1 Loss: 0.0713 | 0.0593
Epoch 131/300, seasonal_1 Loss: 0.0713 | 0.0592
Epoch 132/300, seasonal_1 Loss: 0.0712 | 0.0592
Epoch 133/300, seasonal_1 Loss: 0.0712 | 0.0591
Epoch 134/300, seasonal_1 Loss: 0.0712 | 0.0592
Epoch 135/300, seasonal_1 Loss: 0.0712 | 0.0591
Epoch 136/300, seasonal_1 Loss: 0.0711 | 0.0591
Epoch 137/300, seasonal_1 Loss: 0.0711 | 0.0591
Epoch 138/300, seasonal_1 Loss: 0.0711 | 0.0591
Epoch 139/300, seasonal_1 Loss: 0.0711 | 0.0590
Epoch 140/300, seasonal_1 Loss: 0.0711 | 0.0590
Epoch 141/300, seasonal_1 Loss: 0.0711 | 0.0590
Epoch 142/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 143/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 144/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 145/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 146/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 147/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 148/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 149/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 150/300, seasonal_1 Loss: 0.0710 | 0.0590
Epoch 151/300, seasonal_1 Loss: 0.0709 | 0.0590
Epoch 152/300, seasonal_1 Loss: 0.0709 | 0.0590
Epoch 153/300, seasonal_1 Loss: 0.0709 | 0.0590
Epoch 154/300, seasonal_1 Loss: 0.0709 | 0.0590
Epoch 155/300, seasonal_1 Loss: 0.0709 | 0.0590
Epoch 156/300, seasonal_1 Loss: 0.0709 | 0.0590
Epoch 157/300, seasonal_1 Loss: 0.0709 | 0.0589
Epoch 158/300, seasonal_1 Loss: 0.0709 | 0.0589
Epoch 159/300, seasonal_1 Loss: 0.0709 | 0.0589
Epoch 160/300, seasonal_1 Loss: 0.0709 | 0.0589
Epoch 161/300, seasonal_1 Loss: 0.0709 | 0.0589
Epoch 162/300, seasonal_1 Loss: 0.0709 | 0.0589
Epoch 163/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 164/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 165/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 166/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 167/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 168/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 169/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 170/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 171/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 172/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 173/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 174/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 175/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 176/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 177/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 178/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 179/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 180/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 181/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 182/300, seasonal_1 Loss: 0.0708 | 0.0589
Epoch 183/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 184/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 185/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 186/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 187/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 188/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 189/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 190/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 191/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 192/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 193/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 194/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 195/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 196/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 197/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 198/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 199/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 200/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 201/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 202/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 203/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 204/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 205/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 206/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 207/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 208/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 209/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 210/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 211/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 212/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 213/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 214/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 215/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 216/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 217/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 218/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 219/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 220/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 221/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 222/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 223/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 224/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 225/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 226/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 227/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 228/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 229/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 230/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 231/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 232/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 233/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 234/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 235/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 236/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 237/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 238/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 239/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 240/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 241/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 242/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 243/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 244/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 245/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 246/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 247/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 248/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 249/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 250/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 251/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 252/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 253/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 254/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 255/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 256/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 257/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 258/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 259/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 260/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 261/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 262/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 263/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 264/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 265/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 266/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 267/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 268/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 269/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 270/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 271/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 272/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 273/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 274/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 275/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 276/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 277/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 278/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 279/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 280/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 281/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 282/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 283/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 284/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 285/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 286/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 287/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 288/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 289/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 290/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 291/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 292/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 293/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 294/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 295/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 296/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 297/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 298/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 299/300, seasonal_1 Loss: 0.0707 | 0.0589
Epoch 300/300, seasonal_1 Loss: 0.0707 | 0.0589
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.6786031539610612, 'learning_rate': 0.0002118865949482075, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8730542022734815}
Epoch 1/300, seasonal_2 Loss: 0.5039 | 0.2179
Epoch 2/300, seasonal_2 Loss: 0.1735 | 0.1492
Epoch 3/300, seasonal_2 Loss: 0.1506 | 0.1331
Epoch 4/300, seasonal_2 Loss: 0.1425 | 0.1293
Epoch 5/300, seasonal_2 Loss: 0.1356 | 0.1311
Epoch 6/300, seasonal_2 Loss: 0.1293 | 0.1288
Epoch 7/300, seasonal_2 Loss: 0.1243 | 0.1240
Epoch 8/300, seasonal_2 Loss: 0.1213 | 0.1343
Epoch 9/300, seasonal_2 Loss: 0.1210 | 0.1470
Epoch 10/300, seasonal_2 Loss: 0.1191 | 0.1474
Epoch 11/300, seasonal_2 Loss: 0.1154 | 0.1294
Epoch 12/300, seasonal_2 Loss: 0.1109 | 0.1014
Epoch 13/300, seasonal_2 Loss: 0.1076 | 0.0952
Epoch 14/300, seasonal_2 Loss: 0.1054 | 0.0925
Epoch 15/300, seasonal_2 Loss: 0.1036 | 0.0902
Epoch 16/300, seasonal_2 Loss: 0.1018 | 0.0884
Epoch 17/300, seasonal_2 Loss: 0.1002 | 0.0874
Epoch 18/300, seasonal_2 Loss: 0.0987 | 0.0863
Epoch 19/300, seasonal_2 Loss: 0.0974 | 0.0861
Epoch 20/300, seasonal_2 Loss: 0.0964 | 0.0853
Epoch 21/300, seasonal_2 Loss: 0.0954 | 0.0861
Epoch 22/300, seasonal_2 Loss: 0.0948 | 0.0836
Epoch 23/300, seasonal_2 Loss: 0.0941 | 0.0895
Epoch 24/300, seasonal_2 Loss: 0.0941 | 0.0889
Epoch 25/300, seasonal_2 Loss: 0.0948 | 0.1100
Epoch 26/300, seasonal_2 Loss: 0.0999 | 0.1086
Epoch 27/300, seasonal_2 Loss: 0.0971 | 0.0928
Epoch 28/300, seasonal_2 Loss: 0.0935 | 0.0929
Epoch 29/300, seasonal_2 Loss: 0.0941 | 0.0991
Epoch 30/300, seasonal_2 Loss: 0.0949 | 0.1012
Epoch 31/300, seasonal_2 Loss: 0.0950 | 0.0983
Epoch 32/300, seasonal_2 Loss: 0.0946 | 0.0962
Epoch 33/300, seasonal_2 Loss: 0.0939 | 0.0972
Epoch 34/300, seasonal_2 Loss: 0.0934 | 0.0895
Epoch 35/300, seasonal_2 Loss: 0.0921 | 0.0910
Epoch 36/300, seasonal_2 Loss: 0.0920 | 0.0873
Epoch 37/300, seasonal_2 Loss: 0.0910 | 0.0903
Epoch 38/300, seasonal_2 Loss: 0.0914 | 0.0858
Epoch 39/300, seasonal_2 Loss: 0.0900 | 0.0890
Epoch 40/300, seasonal_2 Loss: 0.0900 | 0.0811
Epoch 41/300, seasonal_2 Loss: 0.0884 | 0.0861
Epoch 42/300, seasonal_2 Loss: 0.0887 | 0.0798
Epoch 43/300, seasonal_2 Loss: 0.0876 | 0.0849
Epoch 44/300, seasonal_2 Loss: 0.0878 | 0.0790
Epoch 45/300, seasonal_2 Loss: 0.0866 | 0.0840
Epoch 46/300, seasonal_2 Loss: 0.0870 | 0.0781
Epoch 47/300, seasonal_2 Loss: 0.0856 | 0.0817
Epoch 48/300, seasonal_2 Loss: 0.0853 | 0.0748
Epoch 49/300, seasonal_2 Loss: 0.0843 | 0.0796
Epoch 50/300, seasonal_2 Loss: 0.0843 | 0.0734
Epoch 51/300, seasonal_2 Loss: 0.0833 | 0.0785
Epoch 52/300, seasonal_2 Loss: 0.0833 | 0.0723
Epoch 53/300, seasonal_2 Loss: 0.0824 | 0.0767
Epoch 54/300, seasonal_2 Loss: 0.0821 | 0.0708
Epoch 55/300, seasonal_2 Loss: 0.0814 | 0.0754
Epoch 56/300, seasonal_2 Loss: 0.0812 | 0.0700
Epoch 57/300, seasonal_2 Loss: 0.0806 | 0.0750
Epoch 58/300, seasonal_2 Loss: 0.0805 | 0.0695
Epoch 59/300, seasonal_2 Loss: 0.0801 | 0.0751
Epoch 60/300, seasonal_2 Loss: 0.0800 | 0.0690
Epoch 61/300, seasonal_2 Loss: 0.0795 | 0.0730
Epoch 62/300, seasonal_2 Loss: 0.0793 | 0.0686
Epoch 63/300, seasonal_2 Loss: 0.0790 | 0.0726
Epoch 64/300, seasonal_2 Loss: 0.0788 | 0.0684
Epoch 65/300, seasonal_2 Loss: 0.0786 | 0.0726
Epoch 66/300, seasonal_2 Loss: 0.0785 | 0.0686
Epoch 67/300, seasonal_2 Loss: 0.0783 | 0.0714
Epoch 68/300, seasonal_2 Loss: 0.0782 | 0.0685
Epoch 69/300, seasonal_2 Loss: 0.0780 | 0.0711
Epoch 70/300, seasonal_2 Loss: 0.0779 | 0.0683
Epoch 71/300, seasonal_2 Loss: 0.0777 | 0.0708
Epoch 72/300, seasonal_2 Loss: 0.0776 | 0.0680
Epoch 73/300, seasonal_2 Loss: 0.0774 | 0.0707
Epoch 74/300, seasonal_2 Loss: 0.0775 | 0.0684
Epoch 75/300, seasonal_2 Loss: 0.0772 | 0.0704
Epoch 76/300, seasonal_2 Loss: 0.0770 | 0.0681
Epoch 77/300, seasonal_2 Loss: 0.0767 | 0.0699
Epoch 78/300, seasonal_2 Loss: 0.0766 | 0.0677
Epoch 79/300, seasonal_2 Loss: 0.0764 | 0.0693
Epoch 80/300, seasonal_2 Loss: 0.0763 | 0.0675
Epoch 81/300, seasonal_2 Loss: 0.0760 | 0.0686
Epoch 82/300, seasonal_2 Loss: 0.0759 | 0.0672
Epoch 83/300, seasonal_2 Loss: 0.0757 | 0.0681
Epoch 84/300, seasonal_2 Loss: 0.0756 | 0.0668
Epoch 85/300, seasonal_2 Loss: 0.0754 | 0.0677
Epoch 86/300, seasonal_2 Loss: 0.0754 | 0.0669
Epoch 87/300, seasonal_2 Loss: 0.0753 | 0.0674
Epoch 88/300, seasonal_2 Loss: 0.0752 | 0.0667
Epoch 89/300, seasonal_2 Loss: 0.0751 | 0.0671
Epoch 90/300, seasonal_2 Loss: 0.0750 | 0.0666
Epoch 91/300, seasonal_2 Loss: 0.0749 | 0.0669
Epoch 92/300, seasonal_2 Loss: 0.0749 | 0.0665
Epoch 93/300, seasonal_2 Loss: 0.0749 | 0.0666
Epoch 94/300, seasonal_2 Loss: 0.0749 | 0.0664
Epoch 95/300, seasonal_2 Loss: 0.0749 | 0.0665
Epoch 96/300, seasonal_2 Loss: 0.0751 | 0.0665
Epoch 97/300, seasonal_2 Loss: 0.0753 | 0.0668
Epoch 98/300, seasonal_2 Loss: 0.0757 | 0.0672
Epoch 99/300, seasonal_2 Loss: 0.0760 | 0.0670
Epoch 100/300, seasonal_2 Loss: 0.0758 | 0.0668
Epoch 101/300, seasonal_2 Loss: 0.0752 | 0.0667
Epoch 102/300, seasonal_2 Loss: 0.0748 | 0.0667
Epoch 103/300, seasonal_2 Loss: 0.0745 | 0.0668
Epoch 104/300, seasonal_2 Loss: 0.0743 | 0.0668
Epoch 105/300, seasonal_2 Loss: 0.0743 | 0.0665
Epoch 106/300, seasonal_2 Loss: 0.0743 | 0.0666
Epoch 107/300, seasonal_2 Loss: 0.0743 | 0.0666
Epoch 108/300, seasonal_2 Loss: 0.0743 | 0.0667
Epoch 109/300, seasonal_2 Loss: 0.0742 | 0.0667
Epoch 110/300, seasonal_2 Loss: 0.0742 | 0.0668
Epoch 111/300, seasonal_2 Loss: 0.0742 | 0.0668
Epoch 112/300, seasonal_2 Loss: 0.0743 | 0.0670
Epoch 113/300, seasonal_2 Loss: 0.0744 | 0.0671
Epoch 114/300, seasonal_2 Loss: 0.0744 | 0.0671
Epoch 115/300, seasonal_2 Loss: 0.0743 | 0.0671
Epoch 116/300, seasonal_2 Loss: 0.0742 | 0.0670
Epoch 117/300, seasonal_2 Loss: 0.0740 | 0.0670
Epoch 118/300, seasonal_2 Loss: 0.0740 | 0.0671
Epoch 119/300, seasonal_2 Loss: 0.0740 | 0.0669
Epoch 120/300, seasonal_2 Loss: 0.0738 | 0.0668
Epoch 121/300, seasonal_2 Loss: 0.0737 | 0.0667
Epoch 122/300, seasonal_2 Loss: 0.0736 | 0.0666
Epoch 123/300, seasonal_2 Loss: 0.0736 | 0.0665
Epoch 124/300, seasonal_2 Loss: 0.0736 | 0.0664
Epoch 125/300, seasonal_2 Loss: 0.0737 | 0.0663
Epoch 126/300, seasonal_2 Loss: 0.0740 | 0.0662
Epoch 127/300, seasonal_2 Loss: 0.0742 | 0.0662
Epoch 128/300, seasonal_2 Loss: 0.0743 | 0.0661
Epoch 129/300, seasonal_2 Loss: 0.0744 | 0.0661
Epoch 130/300, seasonal_2 Loss: 0.0744 | 0.0660
Epoch 131/300, seasonal_2 Loss: 0.0743 | 0.0662
Epoch 132/300, seasonal_2 Loss: 0.0743 | 0.0661
Epoch 133/300, seasonal_2 Loss: 0.0738 | 0.0659
Epoch 134/300, seasonal_2 Loss: 0.0733 | 0.0657
Epoch 135/300, seasonal_2 Loss: 0.0730 | 0.0656
Epoch 136/300, seasonal_2 Loss: 0.0727 | 0.0655
Epoch 137/300, seasonal_2 Loss: 0.0725 | 0.0654
Epoch 138/300, seasonal_2 Loss: 0.0723 | 0.0653
Epoch 139/300, seasonal_2 Loss: 0.0722 | 0.0653
Epoch 140/300, seasonal_2 Loss: 0.0721 | 0.0653
Epoch 141/300, seasonal_2 Loss: 0.0719 | 0.0653
Epoch 142/300, seasonal_2 Loss: 0.0718 | 0.0653
Epoch 143/300, seasonal_2 Loss: 0.0718 | 0.0652
Epoch 144/300, seasonal_2 Loss: 0.0717 | 0.0652
Epoch 145/300, seasonal_2 Loss: 0.0716 | 0.0652
Epoch 146/300, seasonal_2 Loss: 0.0716 | 0.0652
Epoch 147/300, seasonal_2 Loss: 0.0715 | 0.0652
Epoch 148/300, seasonal_2 Loss: 0.0715 | 0.0652
Epoch 149/300, seasonal_2 Loss: 0.0714 | 0.0652
Epoch 150/300, seasonal_2 Loss: 0.0714 | 0.0652
Epoch 151/300, seasonal_2 Loss: 0.0713 | 0.0652
Epoch 152/300, seasonal_2 Loss: 0.0713 | 0.0652
Epoch 153/300, seasonal_2 Loss: 0.0713 | 0.0652
Epoch 154/300, seasonal_2 Loss: 0.0712 | 0.0652
Epoch 155/300, seasonal_2 Loss: 0.0712 | 0.0652
Epoch 156/300, seasonal_2 Loss: 0.0712 | 0.0652
Epoch 157/300, seasonal_2 Loss: 0.0711 | 0.0651
Epoch 158/300, seasonal_2 Loss: 0.0711 | 0.0651
Epoch 159/300, seasonal_2 Loss: 0.0711 | 0.0651
Epoch 160/300, seasonal_2 Loss: 0.0710 | 0.0651
Epoch 161/300, seasonal_2 Loss: 0.0710 | 0.0651
Epoch 162/300, seasonal_2 Loss: 0.0710 | 0.0651
Epoch 163/300, seasonal_2 Loss: 0.0710 | 0.0651
Epoch 164/300, seasonal_2 Loss: 0.0709 | 0.0650
Epoch 165/300, seasonal_2 Loss: 0.0709 | 0.0650
Epoch 166/300, seasonal_2 Loss: 0.0709 | 0.0650
Epoch 167/300, seasonal_2 Loss: 0.0709 | 0.0650
Epoch 168/300, seasonal_2 Loss: 0.0708 | 0.0650
Epoch 169/300, seasonal_2 Loss: 0.0708 | 0.0650
Epoch 170/300, seasonal_2 Loss: 0.0708 | 0.0649
Epoch 171/300, seasonal_2 Loss: 0.0708 | 0.0649
Epoch 172/300, seasonal_2 Loss: 0.0707 | 0.0649
Epoch 173/300, seasonal_2 Loss: 0.0707 | 0.0649
Epoch 174/300, seasonal_2 Loss: 0.0707 | 0.0649
Epoch 175/300, seasonal_2 Loss: 0.0707 | 0.0649
Epoch 176/300, seasonal_2 Loss: 0.0707 | 0.0649
Epoch 177/300, seasonal_2 Loss: 0.0706 | 0.0648
Epoch 178/300, seasonal_2 Loss: 0.0706 | 0.0648
Epoch 179/300, seasonal_2 Loss: 0.0706 | 0.0648
Epoch 180/300, seasonal_2 Loss: 0.0706 | 0.0648
Epoch 181/300, seasonal_2 Loss: 0.0706 | 0.0648
Epoch 182/300, seasonal_2 Loss: 0.0706 | 0.0648
Epoch 183/300, seasonal_2 Loss: 0.0705 | 0.0648
Epoch 184/300, seasonal_2 Loss: 0.0705 | 0.0648
Epoch 185/300, seasonal_2 Loss: 0.0705 | 0.0648
Epoch 186/300, seasonal_2 Loss: 0.0705 | 0.0648
Epoch 187/300, seasonal_2 Loss: 0.0705 | 0.0648
Epoch 188/300, seasonal_2 Loss: 0.0705 | 0.0648
Epoch 189/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 190/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 191/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 192/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 193/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 194/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 195/300, seasonal_2 Loss: 0.0704 | 0.0647
Epoch 196/300, seasonal_2 Loss: 0.0703 | 0.0647
Epoch 197/300, seasonal_2 Loss: 0.0703 | 0.0647
Epoch 198/300, seasonal_2 Loss: 0.0703 | 0.0647
Epoch 199/300, seasonal_2 Loss: 0.0703 | 0.0647
Epoch 200/300, seasonal_2 Loss: 0.0703 | 0.0647
Epoch 201/300, seasonal_2 Loss: 0.0703 | 0.0646
Epoch 202/300, seasonal_2 Loss: 0.0703 | 0.0646
Epoch 203/300, seasonal_2 Loss: 0.0703 | 0.0646
Epoch 204/300, seasonal_2 Loss: 0.0703 | 0.0646
Epoch 205/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 206/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 207/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 208/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 209/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 210/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 211/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 212/300, seasonal_2 Loss: 0.0702 | 0.0646
Epoch 213/300, seasonal_2 Loss: 0.0702 | 0.0645
Epoch 214/300, seasonal_2 Loss: 0.0702 | 0.0645
Epoch 215/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 216/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 217/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 218/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 219/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 220/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 221/300, seasonal_2 Loss: 0.0701 | 0.0645
Epoch 222/300, seasonal_2 Loss: 0.0701 | 0.0644
Epoch 223/300, seasonal_2 Loss: 0.0701 | 0.0644
Epoch 224/300, seasonal_2 Loss: 0.0701 | 0.0644
Epoch 225/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 226/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 227/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 228/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 229/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 230/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 231/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 232/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 233/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 234/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 235/300, seasonal_2 Loss: 0.0700 | 0.0644
Epoch 236/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 237/300, seasonal_2 Loss: 0.0699 | 0.0645
Epoch 238/300, seasonal_2 Loss: 0.0699 | 0.0645
Epoch 239/300, seasonal_2 Loss: 0.0699 | 0.0645
Epoch 240/300, seasonal_2 Loss: 0.0699 | 0.0645
Epoch 241/300, seasonal_2 Loss: 0.0699 | 0.0645
Epoch 242/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 243/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 244/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 245/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 246/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 247/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 248/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 249/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 250/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 251/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 252/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 253/300, seasonal_2 Loss: 0.0699 | 0.0644
Epoch 254/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 255/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 256/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 257/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 258/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 259/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 260/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 261/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 262/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 263/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 264/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 265/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 266/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 267/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 268/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 269/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 270/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 271/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 272/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 273/300, seasonal_2 Loss: 0.0698 | 0.0644
Epoch 274/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 275/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 276/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 277/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 278/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 279/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 280/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 281/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 282/300, seasonal_2 Loss: 0.0698 | 0.0643
Epoch 283/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 284/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 285/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 286/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 287/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 288/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 289/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 290/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 291/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 292/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 293/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 294/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 295/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 296/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 297/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 298/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 299/300, seasonal_2 Loss: 0.0697 | 0.0643
Epoch 300/300, seasonal_2 Loss: 0.0697 | 0.0643
Training seasonal_3 component with params: {'observation_period_num': 10, 'train_rates': 0.700283271600247, 'learning_rate': 0.0006116146523708826, 'batch_size': 86, 'step_size': 14, 'gamma': 0.8682938331356227}
Epoch 1/300, seasonal_3 Loss: 0.4041 | 0.1893
Epoch 2/300, seasonal_3 Loss: 0.1546 | 0.1403
Epoch 3/300, seasonal_3 Loss: 0.1405 | 0.1215
Epoch 4/300, seasonal_3 Loss: 0.1306 | 0.1159
Epoch 5/300, seasonal_3 Loss: 0.1241 | 0.1123
Epoch 6/300, seasonal_3 Loss: 0.1258 | 0.1106
Epoch 7/300, seasonal_3 Loss: 0.1393 | 0.1254
Epoch 8/300, seasonal_3 Loss: 0.1203 | 0.1035
Epoch 9/300, seasonal_3 Loss: 0.1181 | 0.0994
Epoch 10/300, seasonal_3 Loss: 0.1149 | 0.0977
Epoch 11/300, seasonal_3 Loss: 0.1204 | 0.0995
Epoch 12/300, seasonal_3 Loss: 0.1234 | 0.1035
Epoch 13/300, seasonal_3 Loss: 0.1257 | 0.0936
Epoch 14/300, seasonal_3 Loss: 0.1071 | 0.0916
Epoch 15/300, seasonal_3 Loss: 0.1141 | 0.0985
Epoch 16/300, seasonal_3 Loss: 0.1010 | 0.0848
Epoch 17/300, seasonal_3 Loss: 0.0956 | 0.0860
Epoch 18/300, seasonal_3 Loss: 0.0951 | 0.0825
Epoch 19/300, seasonal_3 Loss: 0.0949 | 0.0879
Epoch 20/300, seasonal_3 Loss: 0.1101 | 0.0980
Epoch 21/300, seasonal_3 Loss: 0.0996 | 0.0854
Epoch 22/300, seasonal_3 Loss: 0.1012 | 0.0864
Epoch 23/300, seasonal_3 Loss: 0.0895 | 0.0785
Epoch 24/300, seasonal_3 Loss: 0.0861 | 0.0774
Epoch 25/300, seasonal_3 Loss: 0.0867 | 0.0781
Epoch 26/300, seasonal_3 Loss: 0.0856 | 0.0778
Epoch 27/300, seasonal_3 Loss: 0.0836 | 0.0805
Epoch 28/300, seasonal_3 Loss: 0.0856 | 0.0756
Epoch 29/300, seasonal_3 Loss: 0.0846 | 0.0818
Epoch 30/300, seasonal_3 Loss: 0.0875 | 0.0768
Epoch 31/300, seasonal_3 Loss: 0.0840 | 0.0756
Epoch 32/300, seasonal_3 Loss: 0.0838 | 0.0730
Epoch 33/300, seasonal_3 Loss: 0.0836 | 0.0748
Epoch 34/300, seasonal_3 Loss: 0.0838 | 0.0739
Epoch 35/300, seasonal_3 Loss: 0.0817 | 0.0770
Epoch 36/300, seasonal_3 Loss: 0.0805 | 0.0723
Epoch 37/300, seasonal_3 Loss: 0.0837 | 0.0773
Epoch 38/300, seasonal_3 Loss: 0.0897 | 0.0808
Epoch 39/300, seasonal_3 Loss: 0.0832 | 0.0756
Epoch 40/300, seasonal_3 Loss: 0.0802 | 0.0745
Epoch 41/300, seasonal_3 Loss: 0.0830 | 0.0772
Epoch 42/300, seasonal_3 Loss: 0.0875 | 0.0808
Epoch 43/300, seasonal_3 Loss: 0.0910 | 0.0787
Epoch 44/300, seasonal_3 Loss: 0.0830 | 0.0834
Epoch 45/300, seasonal_3 Loss: 0.0846 | 0.0737
Epoch 46/300, seasonal_3 Loss: 0.0794 | 0.0819
Epoch 47/300, seasonal_3 Loss: 0.0861 | 0.0710
Epoch 48/300, seasonal_3 Loss: 0.0783 | 0.0726
Epoch 49/300, seasonal_3 Loss: 0.0804 | 0.0720
Epoch 50/300, seasonal_3 Loss: 0.0771 | 0.0709
Epoch 51/300, seasonal_3 Loss: 0.0745 | 0.0697
Epoch 52/300, seasonal_3 Loss: 0.0724 | 0.0702
Epoch 53/300, seasonal_3 Loss: 0.0723 | 0.0691
Epoch 54/300, seasonal_3 Loss: 0.0716 | 0.0699
Epoch 55/300, seasonal_3 Loss: 0.0711 | 0.0679
Epoch 56/300, seasonal_3 Loss: 0.0705 | 0.0702
Epoch 57/300, seasonal_3 Loss: 0.0704 | 0.0669
Epoch 58/300, seasonal_3 Loss: 0.0700 | 0.0711
Epoch 59/300, seasonal_3 Loss: 0.0702 | 0.0664
Epoch 60/300, seasonal_3 Loss: 0.0695 | 0.0712
Epoch 61/300, seasonal_3 Loss: 0.0698 | 0.0660
Epoch 62/300, seasonal_3 Loss: 0.0690 | 0.0706
Epoch 63/300, seasonal_3 Loss: 0.0694 | 0.0659
Epoch 64/300, seasonal_3 Loss: 0.0687 | 0.0707
Epoch 65/300, seasonal_3 Loss: 0.0683 | 0.0651
Epoch 66/300, seasonal_3 Loss: 0.0676 | 0.0688
Epoch 67/300, seasonal_3 Loss: 0.0675 | 0.0647
Epoch 68/300, seasonal_3 Loss: 0.0672 | 0.0684
Epoch 69/300, seasonal_3 Loss: 0.0672 | 0.0646
Epoch 70/300, seasonal_3 Loss: 0.0673 | 0.0684
Epoch 71/300, seasonal_3 Loss: 0.0676 | 0.0653
Epoch 72/300, seasonal_3 Loss: 0.0667 | 0.0689
Epoch 73/300, seasonal_3 Loss: 0.0663 | 0.0649
Epoch 74/300, seasonal_3 Loss: 0.0660 | 0.0679
Epoch 75/300, seasonal_3 Loss: 0.0659 | 0.0650
Epoch 76/300, seasonal_3 Loss: 0.0657 | 0.0684
Epoch 77/300, seasonal_3 Loss: 0.0656 | 0.0649
Epoch 78/300, seasonal_3 Loss: 0.0654 | 0.0682
Epoch 79/300, seasonal_3 Loss: 0.0653 | 0.0644
Epoch 80/300, seasonal_3 Loss: 0.0651 | 0.0678
Epoch 81/300, seasonal_3 Loss: 0.0650 | 0.0642
Epoch 82/300, seasonal_3 Loss: 0.0649 | 0.0675
Epoch 83/300, seasonal_3 Loss: 0.0648 | 0.0641
Epoch 84/300, seasonal_3 Loss: 0.0648 | 0.0673
Epoch 85/300, seasonal_3 Loss: 0.0646 | 0.0645
Epoch 86/300, seasonal_3 Loss: 0.0647 | 0.0675
Epoch 87/300, seasonal_3 Loss: 0.0645 | 0.0643
Epoch 88/300, seasonal_3 Loss: 0.0643 | 0.0672
Epoch 89/300, seasonal_3 Loss: 0.0641 | 0.0641
Epoch 90/300, seasonal_3 Loss: 0.0641 | 0.0672
Epoch 91/300, seasonal_3 Loss: 0.0639 | 0.0643
Epoch 92/300, seasonal_3 Loss: 0.0640 | 0.0674
Epoch 93/300, seasonal_3 Loss: 0.0638 | 0.0645
Epoch 94/300, seasonal_3 Loss: 0.0638 | 0.0671
Epoch 95/300, seasonal_3 Loss: 0.0637 | 0.0646
Epoch 96/300, seasonal_3 Loss: 0.0639 | 0.0672
Epoch 97/300, seasonal_3 Loss: 0.0635 | 0.0643
Epoch 98/300, seasonal_3 Loss: 0.0636 | 0.0668
Epoch 99/300, seasonal_3 Loss: 0.0634 | 0.0642
Epoch 100/300, seasonal_3 Loss: 0.0636 | 0.0664
Epoch 101/300, seasonal_3 Loss: 0.0635 | 0.0641
Epoch 102/300, seasonal_3 Loss: 0.0636 | 0.0663
Epoch 103/300, seasonal_3 Loss: 0.0635 | 0.0642
Epoch 104/300, seasonal_3 Loss: 0.0635 | 0.0662
Epoch 105/300, seasonal_3 Loss: 0.0634 | 0.0643
Epoch 106/300, seasonal_3 Loss: 0.0642 | 0.0686
Epoch 107/300, seasonal_3 Loss: 0.0659 | 0.0652
Epoch 108/300, seasonal_3 Loss: 0.0635 | 0.0662
Epoch 109/300, seasonal_3 Loss: 0.0631 | 0.0643
Epoch 110/300, seasonal_3 Loss: 0.0645 | 0.0658
Epoch 111/300, seasonal_3 Loss: 0.0637 | 0.0637
Epoch 112/300, seasonal_3 Loss: 0.0626 | 0.0642
Epoch 113/300, seasonal_3 Loss: 0.0621 | 0.0638
Epoch 114/300, seasonal_3 Loss: 0.0620 | 0.0643
Epoch 115/300, seasonal_3 Loss: 0.0621 | 0.0641
Epoch 116/300, seasonal_3 Loss: 0.0623 | 0.0643
Epoch 117/300, seasonal_3 Loss: 0.0622 | 0.0639
Epoch 118/300, seasonal_3 Loss: 0.0624 | 0.0645
Epoch 119/300, seasonal_3 Loss: 0.0625 | 0.0643
Epoch 120/300, seasonal_3 Loss: 0.0622 | 0.0635
Epoch 121/300, seasonal_3 Loss: 0.0616 | 0.0637
Epoch 122/300, seasonal_3 Loss: 0.0618 | 0.0644
Epoch 123/300, seasonal_3 Loss: 0.0617 | 0.0640
Epoch 124/300, seasonal_3 Loss: 0.0615 | 0.0639
Epoch 125/300, seasonal_3 Loss: 0.0610 | 0.0637
Epoch 126/300, seasonal_3 Loss: 0.0605 | 0.0637
Epoch 127/300, seasonal_3 Loss: 0.0605 | 0.0638
Epoch 128/300, seasonal_3 Loss: 0.0608 | 0.0638
Epoch 129/300, seasonal_3 Loss: 0.0609 | 0.0638
Epoch 130/300, seasonal_3 Loss: 0.0606 | 0.0637
Epoch 131/300, seasonal_3 Loss: 0.0605 | 0.0637
Epoch 132/300, seasonal_3 Loss: 0.0605 | 0.0638
Epoch 133/300, seasonal_3 Loss: 0.0604 | 0.0637
Epoch 134/300, seasonal_3 Loss: 0.0603 | 0.0638
Epoch 135/300, seasonal_3 Loss: 0.0602 | 0.0644
Epoch 136/300, seasonal_3 Loss: 0.0600 | 0.0646
Epoch 137/300, seasonal_3 Loss: 0.0598 | 0.0648
Epoch 138/300, seasonal_3 Loss: 0.0594 | 0.0646
Epoch 139/300, seasonal_3 Loss: 0.0582 | 0.0643
Epoch 140/300, seasonal_3 Loss: 0.0575 | 0.0646
Epoch 141/300, seasonal_3 Loss: 0.0580 | 0.0654
Epoch 142/300, seasonal_3 Loss: 0.0563 | 0.0654
Epoch 143/300, seasonal_3 Loss: 0.0557 | 0.0649
Epoch 144/300, seasonal_3 Loss: 0.0550 | 0.0649
Epoch 145/300, seasonal_3 Loss: 0.0547 | 0.0645
Epoch 146/300, seasonal_3 Loss: 0.0541 | 0.0645
Epoch 147/300, seasonal_3 Loss: 0.0530 | 0.0642
Epoch 148/300, seasonal_3 Loss: 0.0525 | 0.0644
Epoch 149/300, seasonal_3 Loss: 0.0529 | 0.0646
Epoch 150/300, seasonal_3 Loss: 0.0534 | 0.0646
Epoch 151/300, seasonal_3 Loss: 0.0522 | 0.0643
Epoch 152/300, seasonal_3 Loss: 0.0524 | 0.0648
Epoch 153/300, seasonal_3 Loss: 0.0528 | 0.0647
Epoch 154/300, seasonal_3 Loss: 0.0520 | 0.0645
Epoch 155/300, seasonal_3 Loss: 0.0517 | 0.0647
Epoch 156/300, seasonal_3 Loss: 0.0514 | 0.0648
Epoch 157/300, seasonal_3 Loss: 0.0516 | 0.0648
Epoch 158/300, seasonal_3 Loss: 0.0516 | 0.0650
Epoch 159/300, seasonal_3 Loss: 0.0515 | 0.0649
Epoch 160/300, seasonal_3 Loss: 0.0516 | 0.0653
Epoch 161/300, seasonal_3 Loss: 0.0517 | 0.0652
Epoch 162/300, seasonal_3 Loss: 0.0519 | 0.0650
Epoch 163/300, seasonal_3 Loss: 0.0519 | 0.0647
Epoch 164/300, seasonal_3 Loss: 0.0516 | 0.0645
Epoch 165/300, seasonal_3 Loss: 0.0514 | 0.0644
Epoch 166/300, seasonal_3 Loss: 0.0512 | 0.0644
Epoch 167/300, seasonal_3 Loss: 0.0509 | 0.0644
Epoch 168/300, seasonal_3 Loss: 0.0508 | 0.0644
Epoch 169/300, seasonal_3 Loss: 0.0505 | 0.0643
Epoch 170/300, seasonal_3 Loss: 0.0503 | 0.0643
Epoch 171/300, seasonal_3 Loss: 0.0500 | 0.0644
Epoch 172/300, seasonal_3 Loss: 0.0499 | 0.0645
Epoch 173/300, seasonal_3 Loss: 0.0497 | 0.0645
Epoch 174/300, seasonal_3 Loss: 0.0497 | 0.0644
Epoch 175/300, seasonal_3 Loss: 0.0496 | 0.0644
Epoch 176/300, seasonal_3 Loss: 0.0496 | 0.0644
Epoch 177/300, seasonal_3 Loss: 0.0496 | 0.0644
Epoch 178/300, seasonal_3 Loss: 0.0496 | 0.0644
Epoch 179/300, seasonal_3 Loss: 0.0496 | 0.0643
Epoch 180/300, seasonal_3 Loss: 0.0496 | 0.0643
Epoch 181/300, seasonal_3 Loss: 0.0497 | 0.0643
Epoch 182/300, seasonal_3 Loss: 0.0498 | 0.0643
Epoch 183/300, seasonal_3 Loss: 0.0496 | 0.0641
Epoch 184/300, seasonal_3 Loss: 0.0494 | 0.0642
Epoch 185/300, seasonal_3 Loss: 0.0492 | 0.0643
Epoch 186/300, seasonal_3 Loss: 0.0491 | 0.0643
Epoch 187/300, seasonal_3 Loss: 0.0491 | 0.0643
Epoch 188/300, seasonal_3 Loss: 0.0491 | 0.0643
Epoch 189/300, seasonal_3 Loss: 0.0490 | 0.0643
Epoch 190/300, seasonal_3 Loss: 0.0490 | 0.0643
Epoch 191/300, seasonal_3 Loss: 0.0490 | 0.0643
Epoch 192/300, seasonal_3 Loss: 0.0490 | 0.0643
Epoch 193/300, seasonal_3 Loss: 0.0490 | 0.0643
Epoch 194/300, seasonal_3 Loss: 0.0490 | 0.0643
Epoch 195/300, seasonal_3 Loss: 0.0490 | 0.0642
Epoch 196/300, seasonal_3 Loss: 0.0490 | 0.0642
Epoch 197/300, seasonal_3 Loss: 0.0489 | 0.0642
Epoch 198/300, seasonal_3 Loss: 0.0488 | 0.0643
Epoch 199/300, seasonal_3 Loss: 0.0487 | 0.0643
Epoch 200/300, seasonal_3 Loss: 0.0487 | 0.0643
Epoch 201/300, seasonal_3 Loss: 0.0486 | 0.0643
Epoch 202/300, seasonal_3 Loss: 0.0486 | 0.0643
Epoch 203/300, seasonal_3 Loss: 0.0486 | 0.0643
Epoch 204/300, seasonal_3 Loss: 0.0486 | 0.0643
Epoch 205/300, seasonal_3 Loss: 0.0486 | 0.0643
Epoch 206/300, seasonal_3 Loss: 0.0486 | 0.0643
Epoch 207/300, seasonal_3 Loss: 0.0485 | 0.0643
Epoch 208/300, seasonal_3 Loss: 0.0485 | 0.0643
Epoch 209/300, seasonal_3 Loss: 0.0485 | 0.0643
Epoch 210/300, seasonal_3 Loss: 0.0485 | 0.0643
Epoch 211/300, seasonal_3 Loss: 0.0484 | 0.0643
Epoch 212/300, seasonal_3 Loss: 0.0484 | 0.0643
Epoch 213/300, seasonal_3 Loss: 0.0484 | 0.0643
Epoch 214/300, seasonal_3 Loss: 0.0483 | 0.0643
Epoch 215/300, seasonal_3 Loss: 0.0483 | 0.0643
Epoch 216/300, seasonal_3 Loss: 0.0483 | 0.0643
Epoch 217/300, seasonal_3 Loss: 0.0482 | 0.0643
Epoch 218/300, seasonal_3 Loss: 0.0482 | 0.0643
Epoch 219/300, seasonal_3 Loss: 0.0482 | 0.0643
Epoch 220/300, seasonal_3 Loss: 0.0482 | 0.0643
Epoch 221/300, seasonal_3 Loss: 0.0482 | 0.0643
Epoch 222/300, seasonal_3 Loss: 0.0482 | 0.0643
Epoch 223/300, seasonal_3 Loss: 0.0481 | 0.0643
Epoch 224/300, seasonal_3 Loss: 0.0481 | 0.0643
Epoch 225/300, seasonal_3 Loss: 0.0481 | 0.0643
Epoch 226/300, seasonal_3 Loss: 0.0481 | 0.0643
Epoch 227/300, seasonal_3 Loss: 0.0480 | 0.0643
Epoch 228/300, seasonal_3 Loss: 0.0480 | 0.0643
Epoch 229/300, seasonal_3 Loss: 0.0480 | 0.0643
Epoch 230/300, seasonal_3 Loss: 0.0480 | 0.0643
Epoch 231/300, seasonal_3 Loss: 0.0480 | 0.0643
Epoch 232/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 233/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 234/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 235/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 236/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 237/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 238/300, seasonal_3 Loss: 0.0479 | 0.0643
Epoch 239/300, seasonal_3 Loss: 0.0478 | 0.0643
Epoch 240/300, seasonal_3 Loss: 0.0478 | 0.0643
Epoch 241/300, seasonal_3 Loss: 0.0478 | 0.0643
Epoch 242/300, seasonal_3 Loss: 0.0478 | 0.0643
Epoch 243/300, seasonal_3 Loss: 0.0478 | 0.0643
Epoch 244/300, seasonal_3 Loss: 0.0478 | 0.0643
Epoch 245/300, seasonal_3 Loss: 0.0477 | 0.0643
Epoch 246/300, seasonal_3 Loss: 0.0477 | 0.0643
Epoch 247/300, seasonal_3 Loss: 0.0477 | 0.0643
Epoch 248/300, seasonal_3 Loss: 0.0477 | 0.0643
Epoch 249/300, seasonal_3 Loss: 0.0477 | 0.0644
Epoch 250/300, seasonal_3 Loss: 0.0477 | 0.0644
Epoch 251/300, seasonal_3 Loss: 0.0477 | 0.0644
Epoch 252/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 253/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 254/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 255/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 256/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 257/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 258/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 259/300, seasonal_3 Loss: 0.0476 | 0.0644
Epoch 260/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 261/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 262/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 263/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 264/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 265/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 266/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 267/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 268/300, seasonal_3 Loss: 0.0475 | 0.0644
Epoch 269/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 270/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 271/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 272/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 273/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 274/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 275/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 276/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 277/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 278/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 279/300, seasonal_3 Loss: 0.0474 | 0.0644
Epoch 280/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 281/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 282/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 283/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 284/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 285/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 286/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 287/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 288/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 289/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 290/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 291/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 292/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 293/300, seasonal_3 Loss: 0.0473 | 0.0644
Epoch 294/300, seasonal_3 Loss: 0.0472 | 0.0644
Epoch 295/300, seasonal_3 Loss: 0.0472 | 0.0644
Epoch 296/300, seasonal_3 Loss: 0.0472 | 0.0644
Epoch 297/300, seasonal_3 Loss: 0.0472 | 0.0644
Epoch 298/300, seasonal_3 Loss: 0.0472 | 0.0644
Epoch 299/300, seasonal_3 Loss: 0.0472 | 0.0644
Epoch 300/300, seasonal_3 Loss: 0.0472 | 0.0644
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.6855705833837656, 'learning_rate': 0.0006657036179008692, 'batch_size': 239, 'step_size': 14, 'gamma': 0.8175371301990764}
Epoch 1/300, resid Loss: 0.7026 | 0.5412
Epoch 2/300, resid Loss: 0.2542 | 0.2086
Epoch 3/300, resid Loss: 0.2402 | 0.2007
Epoch 4/300, resid Loss: 0.1974 | 0.1750
Epoch 5/300, resid Loss: 0.1775 | 0.1632
Epoch 6/300, resid Loss: 0.1571 | 0.1366
Epoch 7/300, resid Loss: 0.1568 | 0.1365
Epoch 8/300, resid Loss: 0.1416 | 0.1252
Epoch 9/300, resid Loss: 0.1399 | 0.1402
Epoch 10/300, resid Loss: 0.1295 | 0.1238
Epoch 11/300, resid Loss: 0.1340 | 0.1292
Epoch 12/300, resid Loss: 0.1262 | 0.1146
Epoch 13/300, resid Loss: 0.1308 | 0.1169
Epoch 14/300, resid Loss: 0.1228 | 0.1041
Epoch 15/300, resid Loss: 0.1276 | 0.1305
Epoch 16/300, resid Loss: 0.1192 | 0.1062
Epoch 17/300, resid Loss: 0.1221 | 0.1237
Epoch 18/300, resid Loss: 0.1153 | 0.1017
Epoch 19/300, resid Loss: 0.1186 | 0.1182
Epoch 20/300, resid Loss: 0.1128 | 0.0977
Epoch 21/300, resid Loss: 0.1158 | 0.1116
Epoch 22/300, resid Loss: 0.1104 | 0.0958
Epoch 23/300, resid Loss: 0.1100 | 0.1053
Epoch 24/300, resid Loss: 0.1073 | 0.0934
Epoch 25/300, resid Loss: 0.1074 | 0.1015
Epoch 26/300, resid Loss: 0.1053 | 0.0916
Epoch 27/300, resid Loss: 0.1054 | 0.0987
Epoch 28/300, resid Loss: 0.1038 | 0.0895
Epoch 29/300, resid Loss: 0.1034 | 0.0994
Epoch 30/300, resid Loss: 0.1013 | 0.0873
Epoch 31/300, resid Loss: 0.1005 | 0.0940
Epoch 32/300, resid Loss: 0.0991 | 0.0867
Epoch 33/300, resid Loss: 0.0981 | 0.0902
Epoch 34/300, resid Loss: 0.0971 | 0.0852
Epoch 35/300, resid Loss: 0.0962 | 0.0884
Epoch 36/300, resid Loss: 0.0951 | 0.0834
Epoch 37/300, resid Loss: 0.0944 | 0.0875
Epoch 38/300, resid Loss: 0.0937 | 0.0826
Epoch 39/300, resid Loss: 0.0932 | 0.0859
Epoch 40/300, resid Loss: 0.0931 | 0.0826
Epoch 41/300, resid Loss: 0.0932 | 0.0847
Epoch 42/300, resid Loss: 0.0930 | 0.0826
Epoch 43/300, resid Loss: 0.0919 | 0.0845
Epoch 44/300, resid Loss: 0.0917 | 0.0814
Epoch 45/300, resid Loss: 0.0911 | 0.0840
Epoch 46/300, resid Loss: 0.0912 | 0.0816
Epoch 47/300, resid Loss: 0.0905 | 0.0827
Epoch 48/300, resid Loss: 0.0910 | 0.0814
Epoch 49/300, resid Loss: 0.0899 | 0.0816
Epoch 50/300, resid Loss: 0.0904 | 0.0815
Epoch 51/300, resid Loss: 0.0896 | 0.0811
Epoch 52/300, resid Loss: 0.0902 | 0.0810
Epoch 53/300, resid Loss: 0.0898 | 0.0805
Epoch 54/300, resid Loss: 0.0900 | 0.0800
Epoch 55/300, resid Loss: 0.0895 | 0.0792
Epoch 56/300, resid Loss: 0.0888 | 0.0789
Epoch 57/300, resid Loss: 0.0882 | 0.0788
Epoch 58/300, resid Loss: 0.0881 | 0.0786
Epoch 59/300, resid Loss: 0.0880 | 0.0787
Epoch 60/300, resid Loss: 0.0876 | 0.0785
Epoch 61/300, resid Loss: 0.0873 | 0.0788
Epoch 62/300, resid Loss: 0.0871 | 0.0782
Epoch 63/300, resid Loss: 0.0871 | 0.0782
Epoch 64/300, resid Loss: 0.0869 | 0.0777
Epoch 65/300, resid Loss: 0.0866 | 0.0774
Epoch 66/300, resid Loss: 0.0863 | 0.0771
Epoch 67/300, resid Loss: 0.0861 | 0.0770
Epoch 68/300, resid Loss: 0.0861 | 0.0769
Epoch 69/300, resid Loss: 0.0859 | 0.0769
Epoch 70/300, resid Loss: 0.0857 | 0.0769
Epoch 71/300, resid Loss: 0.0855 | 0.0771
Epoch 72/300, resid Loss: 0.0855 | 0.0769
Epoch 73/300, resid Loss: 0.0854 | 0.0765
Epoch 74/300, resid Loss: 0.0852 | 0.0763
Epoch 75/300, resid Loss: 0.0850 | 0.0761
Epoch 76/300, resid Loss: 0.0848 | 0.0760
Epoch 77/300, resid Loss: 0.0847 | 0.0759
Epoch 78/300, resid Loss: 0.0846 | 0.0761
Epoch 79/300, resid Loss: 0.0845 | 0.0761
Epoch 80/300, resid Loss: 0.0844 | 0.0759
Epoch 81/300, resid Loss: 0.0843 | 0.0758
Epoch 82/300, resid Loss: 0.0842 | 0.0756
Epoch 83/300, resid Loss: 0.0841 | 0.0754
Epoch 84/300, resid Loss: 0.0840 | 0.0754
Epoch 85/300, resid Loss: 0.0839 | 0.0754
Epoch 86/300, resid Loss: 0.0839 | 0.0753
Epoch 87/300, resid Loss: 0.0838 | 0.0752
Epoch 88/300, resid Loss: 0.0837 | 0.0752
Epoch 89/300, resid Loss: 0.0837 | 0.0751
Epoch 90/300, resid Loss: 0.0836 | 0.0750
Epoch 91/300, resid Loss: 0.0835 | 0.0749
Epoch 92/300, resid Loss: 0.0834 | 0.0749
Epoch 93/300, resid Loss: 0.0834 | 0.0748
Epoch 94/300, resid Loss: 0.0833 | 0.0747
Epoch 95/300, resid Loss: 0.0832 | 0.0747
Epoch 96/300, resid Loss: 0.0832 | 0.0746
Epoch 97/300, resid Loss: 0.0831 | 0.0745
Epoch 98/300, resid Loss: 0.0830 | 0.0745
Epoch 99/300, resid Loss: 0.0830 | 0.0744
Epoch 100/300, resid Loss: 0.0829 | 0.0743
Epoch 101/300, resid Loss: 0.0829 | 0.0743
Epoch 102/300, resid Loss: 0.0828 | 0.0743
Epoch 103/300, resid Loss: 0.0828 | 0.0742
Epoch 104/300, resid Loss: 0.0827 | 0.0741
Epoch 105/300, resid Loss: 0.0826 | 0.0741
Epoch 106/300, resid Loss: 0.0826 | 0.0740
Epoch 107/300, resid Loss: 0.0825 | 0.0740
Epoch 108/300, resid Loss: 0.0825 | 0.0739
Epoch 109/300, resid Loss: 0.0824 | 0.0739
Epoch 110/300, resid Loss: 0.0824 | 0.0738
Epoch 111/300, resid Loss: 0.0823 | 0.0738
Epoch 112/300, resid Loss: 0.0823 | 0.0737
Epoch 113/300, resid Loss: 0.0822 | 0.0737
Epoch 114/300, resid Loss: 0.0822 | 0.0736
Epoch 115/300, resid Loss: 0.0821 | 0.0736
Epoch 116/300, resid Loss: 0.0821 | 0.0736
Epoch 117/300, resid Loss: 0.0820 | 0.0735
Epoch 118/300, resid Loss: 0.0820 | 0.0735
Epoch 119/300, resid Loss: 0.0819 | 0.0734
Epoch 120/300, resid Loss: 0.0819 | 0.0734
Epoch 121/300, resid Loss: 0.0819 | 0.0733
Epoch 122/300, resid Loss: 0.0818 | 0.0733
Epoch 123/300, resid Loss: 0.0818 | 0.0733
Epoch 124/300, resid Loss: 0.0817 | 0.0732
Epoch 125/300, resid Loss: 0.0817 | 0.0732
Epoch 126/300, resid Loss: 0.0817 | 0.0731
Epoch 127/300, resid Loss: 0.0816 | 0.0731
Epoch 128/300, resid Loss: 0.0816 | 0.0731
Epoch 129/300, resid Loss: 0.0815 | 0.0730
Epoch 130/300, resid Loss: 0.0815 | 0.0730
Epoch 131/300, resid Loss: 0.0815 | 0.0730
Epoch 132/300, resid Loss: 0.0814 | 0.0729
Epoch 133/300, resid Loss: 0.0814 | 0.0729
Epoch 134/300, resid Loss: 0.0814 | 0.0729
Epoch 135/300, resid Loss: 0.0813 | 0.0728
Epoch 136/300, resid Loss: 0.0813 | 0.0728
Epoch 137/300, resid Loss: 0.0813 | 0.0728
Epoch 138/300, resid Loss: 0.0812 | 0.0727
Epoch 139/300, resid Loss: 0.0812 | 0.0727
Epoch 140/300, resid Loss: 0.0812 | 0.0727
Epoch 141/300, resid Loss: 0.0812 | 0.0727
Epoch 142/300, resid Loss: 0.0811 | 0.0726
Epoch 143/300, resid Loss: 0.0811 | 0.0726
Epoch 144/300, resid Loss: 0.0811 | 0.0726
Epoch 145/300, resid Loss: 0.0810 | 0.0726
Epoch 146/300, resid Loss: 0.0810 | 0.0725
Epoch 147/300, resid Loss: 0.0810 | 0.0725
Epoch 148/300, resid Loss: 0.0810 | 0.0725
Epoch 149/300, resid Loss: 0.0809 | 0.0725
Epoch 150/300, resid Loss: 0.0809 | 0.0724
Epoch 151/300, resid Loss: 0.0809 | 0.0724
Epoch 152/300, resid Loss: 0.0809 | 0.0724
Epoch 153/300, resid Loss: 0.0809 | 0.0724
Epoch 154/300, resid Loss: 0.0808 | 0.0724
Epoch 155/300, resid Loss: 0.0808 | 0.0723
Epoch 156/300, resid Loss: 0.0808 | 0.0723
Epoch 157/300, resid Loss: 0.0808 | 0.0723
Epoch 158/300, resid Loss: 0.0808 | 0.0723
Epoch 159/300, resid Loss: 0.0807 | 0.0723
Epoch 160/300, resid Loss: 0.0807 | 0.0722
Epoch 161/300, resid Loss: 0.0807 | 0.0722
Epoch 162/300, resid Loss: 0.0807 | 0.0722
Epoch 163/300, resid Loss: 0.0807 | 0.0722
Epoch 164/300, resid Loss: 0.0806 | 0.0722
Epoch 165/300, resid Loss: 0.0806 | 0.0722
Epoch 166/300, resid Loss: 0.0806 | 0.0722
Epoch 167/300, resid Loss: 0.0806 | 0.0721
Epoch 168/300, resid Loss: 0.0806 | 0.0721
Epoch 169/300, resid Loss: 0.0806 | 0.0721
Epoch 170/300, resid Loss: 0.0806 | 0.0721
Epoch 171/300, resid Loss: 0.0805 | 0.0721
Epoch 172/300, resid Loss: 0.0805 | 0.0721
Epoch 173/300, resid Loss: 0.0805 | 0.0721
Epoch 174/300, resid Loss: 0.0805 | 0.0720
Epoch 175/300, resid Loss: 0.0805 | 0.0720
Epoch 176/300, resid Loss: 0.0805 | 0.0720
Epoch 177/300, resid Loss: 0.0805 | 0.0720
Epoch 178/300, resid Loss: 0.0804 | 0.0720
Epoch 179/300, resid Loss: 0.0804 | 0.0720
Epoch 180/300, resid Loss: 0.0804 | 0.0720
Epoch 181/300, resid Loss: 0.0804 | 0.0720
Epoch 182/300, resid Loss: 0.0804 | 0.0720
Epoch 183/300, resid Loss: 0.0804 | 0.0719
Epoch 184/300, resid Loss: 0.0804 | 0.0719
Epoch 185/300, resid Loss: 0.0804 | 0.0719
Epoch 186/300, resid Loss: 0.0804 | 0.0719
Epoch 187/300, resid Loss: 0.0804 | 0.0719
Epoch 188/300, resid Loss: 0.0803 | 0.0719
Epoch 189/300, resid Loss: 0.0803 | 0.0719
Epoch 190/300, resid Loss: 0.0803 | 0.0719
Epoch 191/300, resid Loss: 0.0803 | 0.0719
Epoch 192/300, resid Loss: 0.0803 | 0.0719
Epoch 193/300, resid Loss: 0.0803 | 0.0719
Epoch 194/300, resid Loss: 0.0803 | 0.0719
Epoch 195/300, resid Loss: 0.0803 | 0.0718
Epoch 196/300, resid Loss: 0.0803 | 0.0718
Epoch 197/300, resid Loss: 0.0803 | 0.0718
Epoch 198/300, resid Loss: 0.0803 | 0.0718
Epoch 199/300, resid Loss: 0.0803 | 0.0718
Epoch 200/300, resid Loss: 0.0803 | 0.0718
Epoch 201/300, resid Loss: 0.0802 | 0.0718
Epoch 202/300, resid Loss: 0.0802 | 0.0718
Epoch 203/300, resid Loss: 0.0802 | 0.0718
Epoch 204/300, resid Loss: 0.0802 | 0.0718
Epoch 205/300, resid Loss: 0.0802 | 0.0718
Epoch 206/300, resid Loss: 0.0802 | 0.0718
Epoch 207/300, resid Loss: 0.0802 | 0.0718
Epoch 208/300, resid Loss: 0.0802 | 0.0718
Epoch 209/300, resid Loss: 0.0802 | 0.0718
Epoch 210/300, resid Loss: 0.0802 | 0.0718
Epoch 211/300, resid Loss: 0.0802 | 0.0718
Epoch 212/300, resid Loss: 0.0802 | 0.0718
Epoch 213/300, resid Loss: 0.0802 | 0.0717
Epoch 214/300, resid Loss: 0.0802 | 0.0717
Epoch 215/300, resid Loss: 0.0802 | 0.0717
Epoch 216/300, resid Loss: 0.0802 | 0.0717
Epoch 217/300, resid Loss: 0.0802 | 0.0717
Epoch 218/300, resid Loss: 0.0802 | 0.0717
Epoch 219/300, resid Loss: 0.0801 | 0.0717
Epoch 220/300, resid Loss: 0.0801 | 0.0717
Epoch 221/300, resid Loss: 0.0801 | 0.0717
Epoch 222/300, resid Loss: 0.0801 | 0.0717
Epoch 223/300, resid Loss: 0.0801 | 0.0717
Epoch 224/300, resid Loss: 0.0801 | 0.0717
Epoch 225/300, resid Loss: 0.0801 | 0.0717
Epoch 226/300, resid Loss: 0.0801 | 0.0717
Epoch 227/300, resid Loss: 0.0801 | 0.0717
Epoch 228/300, resid Loss: 0.0801 | 0.0717
Epoch 229/300, resid Loss: 0.0801 | 0.0717
Epoch 230/300, resid Loss: 0.0801 | 0.0717
Epoch 231/300, resid Loss: 0.0801 | 0.0717
Epoch 232/300, resid Loss: 0.0801 | 0.0717
Epoch 233/300, resid Loss: 0.0801 | 0.0717
Epoch 234/300, resid Loss: 0.0801 | 0.0717
Epoch 235/300, resid Loss: 0.0801 | 0.0717
Epoch 236/300, resid Loss: 0.0801 | 0.0717
Epoch 237/300, resid Loss: 0.0801 | 0.0717
Epoch 238/300, resid Loss: 0.0801 | 0.0717
Epoch 239/300, resid Loss: 0.0801 | 0.0717
Epoch 240/300, resid Loss: 0.0801 | 0.0717
Epoch 241/300, resid Loss: 0.0801 | 0.0717
Epoch 242/300, resid Loss: 0.0801 | 0.0717
Epoch 243/300, resid Loss: 0.0801 | 0.0717
Epoch 244/300, resid Loss: 0.0801 | 0.0717
Epoch 245/300, resid Loss: 0.0801 | 0.0717
Epoch 246/300, resid Loss: 0.0801 | 0.0717
Epoch 247/300, resid Loss: 0.0801 | 0.0717
Epoch 248/300, resid Loss: 0.0801 | 0.0716
Epoch 249/300, resid Loss: 0.0801 | 0.0716
Epoch 250/300, resid Loss: 0.0801 | 0.0716
Epoch 251/300, resid Loss: 0.0801 | 0.0716
Epoch 252/300, resid Loss: 0.0801 | 0.0716
Epoch 253/300, resid Loss: 0.0801 | 0.0716
Epoch 254/300, resid Loss: 0.0801 | 0.0716
Epoch 255/300, resid Loss: 0.0801 | 0.0716
Epoch 256/300, resid Loss: 0.0801 | 0.0716
Epoch 257/300, resid Loss: 0.0801 | 0.0716
Epoch 258/300, resid Loss: 0.0801 | 0.0716
Epoch 259/300, resid Loss: 0.0801 | 0.0716
Epoch 260/300, resid Loss: 0.0800 | 0.0716
Epoch 261/300, resid Loss: 0.0800 | 0.0716
Epoch 262/300, resid Loss: 0.0800 | 0.0716
Epoch 263/300, resid Loss: 0.0800 | 0.0716
Epoch 264/300, resid Loss: 0.0800 | 0.0716
Epoch 265/300, resid Loss: 0.0800 | 0.0716
Epoch 266/300, resid Loss: 0.0800 | 0.0716
Epoch 267/300, resid Loss: 0.0800 | 0.0716
Epoch 268/300, resid Loss: 0.0800 | 0.0716
Epoch 269/300, resid Loss: 0.0800 | 0.0716
Epoch 270/300, resid Loss: 0.0800 | 0.0716
Epoch 271/300, resid Loss: 0.0800 | 0.0716
Epoch 272/300, resid Loss: 0.0800 | 0.0716
Epoch 273/300, resid Loss: 0.0800 | 0.0716
Epoch 274/300, resid Loss: 0.0800 | 0.0716
Epoch 275/300, resid Loss: 0.0800 | 0.0716
Epoch 276/300, resid Loss: 0.0800 | 0.0716
Epoch 277/300, resid Loss: 0.0800 | 0.0716
Epoch 278/300, resid Loss: 0.0800 | 0.0716
Epoch 279/300, resid Loss: 0.0800 | 0.0716
Epoch 280/300, resid Loss: 0.0800 | 0.0716
Epoch 281/300, resid Loss: 0.0800 | 0.0716
Epoch 282/300, resid Loss: 0.0800 | 0.0716
Epoch 283/300, resid Loss: 0.0800 | 0.0716
Epoch 284/300, resid Loss: 0.0800 | 0.0716
Epoch 285/300, resid Loss: 0.0800 | 0.0716
Epoch 286/300, resid Loss: 0.0800 | 0.0716
Epoch 287/300, resid Loss: 0.0800 | 0.0716
Epoch 288/300, resid Loss: 0.0800 | 0.0716
Epoch 289/300, resid Loss: 0.0800 | 0.0716
Epoch 290/300, resid Loss: 0.0800 | 0.0716
Epoch 291/300, resid Loss: 0.0800 | 0.0716
Epoch 292/300, resid Loss: 0.0800 | 0.0716
Epoch 293/300, resid Loss: 0.0800 | 0.0716
Epoch 294/300, resid Loss: 0.0800 | 0.0716
Epoch 295/300, resid Loss: 0.0800 | 0.0716
Epoch 296/300, resid Loss: 0.0800 | 0.0716
Epoch 297/300, resid Loss: 0.0800 | 0.0716
Epoch 298/300, resid Loss: 0.0800 | 0.0716
Epoch 299/300, resid Loss: 0.0800 | 0.0716
Epoch 300/300, resid Loss: 0.0800 | 0.0716
Runtime (seconds): 935.11044049263
0.0004809934322516847
[99.6709]
[4.912888]
[-2.0133195]
[0.283222]
[-4.626438]
[-3.1583014]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 55.22055109083885
RMSE: 7.431053161621094
MAE: 7.431053161621094
R-squared: nan
[95.06895]
