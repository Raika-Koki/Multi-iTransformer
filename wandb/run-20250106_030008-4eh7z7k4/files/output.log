ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-06 03:00:09,029][0m A new study created in memory with name: no-name-07753746-3d95-4a85-8ba1-d4290e64bf96[0m
[32m[I 2025-01-06 03:01:38,314][0m Trial 0 finished with value: 0.5759778289959349 and parameters: {'observation_period_num': 228, 'train_rates': 0.9151965424413723, 'learning_rate': 1.9899416513558968e-06, 'batch_size': 206, 'step_size': 14, 'gamma': 0.8118824900895589}. Best is trial 0 with value: 0.5759778289959349.[0m
[32m[I 2025-01-06 03:04:48,347][0m Trial 1 finished with value: 0.19435107634040236 and parameters: {'observation_period_num': 35, 'train_rates': 0.7754652441603289, 'learning_rate': 1.8673093392181428e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.9888492219894094}. Best is trial 1 with value: 0.19435107634040236.[0m
[32m[I 2025-01-06 03:06:25,695][0m Trial 2 finished with value: 0.24931145604999586 and parameters: {'observation_period_num': 210, 'train_rates': 0.7146723495835523, 'learning_rate': 0.0003567182591187469, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8496765084737092}. Best is trial 1 with value: 0.19435107634040236.[0m
[32m[I 2025-01-06 03:07:32,103][0m Trial 3 finished with value: 0.46778451247319425 and parameters: {'observation_period_num': 252, 'train_rates': 0.6101977073565924, 'learning_rate': 2.6184506919335357e-05, 'batch_size': 251, 'step_size': 3, 'gamma': 0.9889913471207052}. Best is trial 1 with value: 0.19435107634040236.[0m
[32m[I 2025-01-06 03:08:52,188][0m Trial 4 finished with value: 0.25387993683012167 and parameters: {'observation_period_num': 198, 'train_rates': 0.7510177818096824, 'learning_rate': 0.0007164977578768337, 'batch_size': 154, 'step_size': 7, 'gamma': 0.7842377252826294}. Best is trial 1 with value: 0.19435107634040236.[0m
[32m[I 2025-01-06 03:09:52,368][0m Trial 5 finished with value: 0.23289639666570838 and parameters: {'observation_period_num': 207, 'train_rates': 0.9487671084447156, 'learning_rate': 1.3894606465732164e-05, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9255427169629988}. Best is trial 1 with value: 0.19435107634040236.[0m
[32m[I 2025-01-06 03:11:05,527][0m Trial 6 finished with value: 0.29162463081664725 and parameters: {'observation_period_num': 213, 'train_rates': 0.6659954538900624, 'learning_rate': 0.00023965960800893254, 'batch_size': 164, 'step_size': 5, 'gamma': 0.779487105314356}. Best is trial 1 with value: 0.19435107634040236.[0m
Early stopping at epoch 48
[32m[I 2025-01-06 03:12:01,019][0m Trial 7 finished with value: 0.742348989041952 and parameters: {'observation_period_num': 124, 'train_rates': 0.7241949147688229, 'learning_rate': 2.109043124078831e-05, 'batch_size': 85, 'step_size': 1, 'gamma': 0.7668526951864163}. Best is trial 1 with value: 0.19435107634040236.[0m
[32m[I 2025-01-06 03:12:50,882][0m Trial 8 finished with value: 0.09394538506037658 and parameters: {'observation_period_num': 163, 'train_rates': 0.8838749599360489, 'learning_rate': 0.0009566049747630693, 'batch_size': 245, 'step_size': 12, 'gamma': 0.7808206621573677}. Best is trial 8 with value: 0.09394538506037658.[0m
[32m[I 2025-01-06 03:14:09,150][0m Trial 9 finished with value: 0.0907112887729442 and parameters: {'observation_period_num': 118, 'train_rates': 0.8940258931379241, 'learning_rate': 0.0009125180596327126, 'batch_size': 111, 'step_size': 11, 'gamma': 0.8825603366133661}. Best is trial 9 with value: 0.0907112887729442.[0m
[32m[I 2025-01-06 03:15:31,307][0m Trial 10 finished with value: 0.05826921330596344 and parameters: {'observation_period_num': 83, 'train_rates': 0.849905146681831, 'learning_rate': 0.00011268723362942269, 'batch_size': 96, 'step_size': 10, 'gamma': 0.9008951715375904}. Best is trial 10 with value: 0.05826921330596344.[0m
[32m[I 2025-01-06 03:16:43,464][0m Trial 11 finished with value: 0.05938753389834779 and parameters: {'observation_period_num': 79, 'train_rates': 0.8568821220058188, 'learning_rate': 0.00011444475553047707, 'batch_size': 111, 'step_size': 10, 'gamma': 0.9038781194694322}. Best is trial 10 with value: 0.05826921330596344.[0m
[32m[I 2025-01-06 03:19:18,048][0m Trial 12 finished with value: 0.05523686167173859 and parameters: {'observation_period_num': 64, 'train_rates': 0.839349134034285, 'learning_rate': 0.00011847083443014002, 'batch_size': 42, 'step_size': 10, 'gamma': 0.9077916502281221}. Best is trial 12 with value: 0.05523686167173859.[0m
[32m[I 2025-01-06 03:23:29,767][0m Trial 13 finished with value: 0.03526623114415511 and parameters: {'observation_period_num': 12, 'train_rates': 0.827385456787188, 'learning_rate': 7.517876629351217e-05, 'batch_size': 24, 'step_size': 9, 'gamma': 0.9396675464498102}. Best is trial 13 with value: 0.03526623114415511.[0m
[32m[I 2025-01-06 03:29:17,636][0m Trial 14 finished with value: 0.03114389456261123 and parameters: {'observation_period_num': 7, 'train_rates': 0.8107991790853111, 'learning_rate': 6.325751548858324e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9488735062179996}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:33:32,374][0m Trial 15 finished with value: 0.05318828567862511 and parameters: {'observation_period_num': 9, 'train_rates': 0.8048611711310765, 'learning_rate': 5.688027016954598e-06, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9438976997941411}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:35:49,616][0m Trial 16 finished with value: 0.04041384905576706 and parameters: {'observation_period_num': 6, 'train_rates': 0.9834152419498148, 'learning_rate': 5.589238036828814e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.9561937735791775}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:37:58,314][0m Trial 17 finished with value: 0.04989004386539828 and parameters: {'observation_period_num': 44, 'train_rates': 0.8043417037047641, 'learning_rate': 4.9815948453929324e-05, 'batch_size': 55, 'step_size': 15, 'gamma': 0.851127314390261}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:44:06,096][0m Trial 18 finished with value: 0.053144780965028296 and parameters: {'observation_period_num': 34, 'train_rates': 0.8061425033038754, 'learning_rate': 8.051937454863831e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9593664131428155}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:45:11,673][0m Trial 19 finished with value: 0.24510261145897919 and parameters: {'observation_period_num': 98, 'train_rates': 0.6982131391085622, 'learning_rate': 5.845309369511637e-05, 'batch_size': 185, 'step_size': 9, 'gamma': 0.937277707228305}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:46:58,526][0m Trial 20 finished with value: 0.43065898279113884 and parameters: {'observation_period_num': 57, 'train_rates': 0.7668091604994552, 'learning_rate': 1.255704236197981e-06, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9696687710151766}. Best is trial 14 with value: 0.03114389456261123.[0m
[32m[I 2025-01-06 03:49:33,227][0m Trial 21 finished with value: 0.030922124271883684 and parameters: {'observation_period_num': 15, 'train_rates': 0.9828011383622061, 'learning_rate': 5.5108426278969326e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9561587573414215}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 03:52:20,030][0m Trial 22 finished with value: 0.04538380727171898 and parameters: {'observation_period_num': 21, 'train_rates': 0.9865533701594664, 'learning_rate': 0.0003197235778566579, 'batch_size': 44, 'step_size': 13, 'gamma': 0.917557344610986}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 03:56:55,938][0m Trial 23 finished with value: 0.08290622302610885 and parameters: {'observation_period_num': 157, 'train_rates': 0.9276190295609907, 'learning_rate': 4.128565821165275e-05, 'batch_size': 23, 'step_size': 14, 'gamma': 0.8809104592765751}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 03:58:31,828][0m Trial 24 finished with value: 0.03416727890074253 and parameters: {'observation_period_num': 5, 'train_rates': 0.8305508139669145, 'learning_rate': 8.613962885439648e-05, 'batch_size': 78, 'step_size': 12, 'gamma': 0.9722264297811075}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:00:14,172][0m Trial 25 finished with value: 0.06438164506355921 and parameters: {'observation_period_num': 51, 'train_rates': 0.8708416961011654, 'learning_rate': 0.000150711248820984, 'batch_size': 76, 'step_size': 13, 'gamma': 0.97228282708999}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:01:47,871][0m Trial 26 finished with value: 0.06878166036709907 and parameters: {'observation_period_num': 27, 'train_rates': 0.9356887832452434, 'learning_rate': 1.0722794489941375e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.9605049298722504}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:03:46,664][0m Trial 27 finished with value: 0.30553599110534113 and parameters: {'observation_period_num': 69, 'train_rates': 0.6559987525341912, 'learning_rate': 3.8627406538476685e-06, 'batch_size': 44, 'step_size': 12, 'gamma': 0.9834571936212616}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:05:20,598][0m Trial 28 finished with value: 0.07073994292454286 and parameters: {'observation_period_num': 102, 'train_rates': 0.9037480517221756, 'learning_rate': 3.470452177611654e-05, 'batch_size': 90, 'step_size': 14, 'gamma': 0.9273790744075545}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:06:47,484][0m Trial 29 finished with value: 0.04111733287572861 and parameters: {'observation_period_num': 25, 'train_rates': 0.9583970344779983, 'learning_rate': 0.00021897673150597137, 'batch_size': 123, 'step_size': 11, 'gamma': 0.8346918098168412}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:08:46,642][0m Trial 30 finished with value: 0.09553096759055048 and parameters: {'observation_period_num': 155, 'train_rates': 0.7825817076104695, 'learning_rate': 6.54976366053568e-05, 'batch_size': 52, 'step_size': 14, 'gamma': 0.9464390679899228}. Best is trial 21 with value: 0.030922124271883684.[0m
[32m[I 2025-01-06 04:11:57,938][0m Trial 31 finished with value: 0.030218464512269474 and parameters: {'observation_period_num': 5, 'train_rates': 0.8255407073315638, 'learning_rate': 8.306320187563025e-05, 'batch_size': 31, 'step_size': 11, 'gamma': 0.9383324338888183}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:14:50,309][0m Trial 32 finished with value: 0.05540763820503272 and parameters: {'observation_period_num': 41, 'train_rates': 0.8249671911024506, 'learning_rate': 3.119889157674227e-05, 'batch_size': 40, 'step_size': 11, 'gamma': 0.9743908294194561}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:16:26,684][0m Trial 33 finished with value: 0.15497159459548618 and parameters: {'observation_period_num': 6, 'train_rates': 0.7418838405267772, 'learning_rate': 8.42404295217684e-05, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9526555150132087}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:20:03,241][0m Trial 34 finished with value: 0.06995870219053406 and parameters: {'observation_period_num': 23, 'train_rates': 0.8703220584921134, 'learning_rate': 0.0001645351875045457, 'batch_size': 31, 'step_size': 12, 'gamma': 0.9896600466040653}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:21:23,055][0m Trial 35 finished with value: 0.2123582127955428 and parameters: {'observation_period_num': 34, 'train_rates': 0.776723781505182, 'learning_rate': 2.202963152903963e-05, 'batch_size': 77, 'step_size': 15, 'gamma': 0.9302296155334744}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:24:00,089][0m Trial 36 finished with value: 0.08501554073399098 and parameters: {'observation_period_num': 48, 'train_rates': 0.8218705657079077, 'learning_rate': 0.0004918574359540671, 'batch_size': 36, 'step_size': 14, 'gamma': 0.9166118208963498}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:25:30,846][0m Trial 37 finished with value: 0.20353874512805414 and parameters: {'observation_period_num': 19, 'train_rates': 0.7553629359564723, 'learning_rate': 1.5510909985652558e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8913690264299071}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:31:15,511][0m Trial 38 finished with value: 0.0996356632238553 and parameters: {'observation_period_num': 183, 'train_rates': 0.9122649196957356, 'learning_rate': 9.12491369904345e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9706010101796638}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:32:30,591][0m Trial 39 finished with value: 0.160806071246294 and parameters: {'observation_period_num': 35, 'train_rates': 0.6823699750625204, 'learning_rate': 0.00024734976032127323, 'batch_size': 151, 'step_size': 11, 'gamma': 0.8162171530594325}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:33:18,473][0m Trial 40 finished with value: 0.13031043808035836 and parameters: {'observation_period_num': 19, 'train_rates': 0.6341655753552611, 'learning_rate': 0.0005006151038861336, 'batch_size': 207, 'step_size': 6, 'gamma': 0.860602313667819}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:36:38,072][0m Trial 41 finished with value: 0.030780160536795307 and parameters: {'observation_period_num': 6, 'train_rates': 0.8340157364638615, 'learning_rate': 7.482487426657194e-05, 'batch_size': 31, 'step_size': 9, 'gamma': 0.9369613878352105}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:39:37,296][0m Trial 42 finished with value: 0.15561902591117144 and parameters: {'observation_period_num': 7, 'train_rates': 0.732726515173317, 'learning_rate': 3.5447289748271215e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9341075493713682}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:41:53,791][0m Trial 43 finished with value: 0.04539120586061304 and parameters: {'observation_period_num': 31, 'train_rates': 0.8587339970629081, 'learning_rate': 0.00015807471802929698, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9185450119943964}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:45:15,119][0m Trial 44 finished with value: 0.04075657593549332 and parameters: {'observation_period_num': 5, 'train_rates': 0.8827562486323403, 'learning_rate': 5.0136247374658425e-05, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9501972679291084}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:46:38,579][0m Trial 45 finished with value: 0.08420348983282147 and parameters: {'observation_period_num': 139, 'train_rates': 0.8354693982468876, 'learning_rate': 2.708641659357177e-05, 'batch_size': 82, 'step_size': 10, 'gamma': 0.9612939339489316}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:48:01,706][0m Trial 46 finished with value: 0.057652754815128764 and parameters: {'observation_period_num': 17, 'train_rates': 0.7907389575598631, 'learning_rate': 7.649279648994375e-05, 'batch_size': 100, 'step_size': 12, 'gamma': 0.9815677307927424}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:50:03,449][0m Trial 47 finished with value: 0.28508461374853405 and parameters: {'observation_period_num': 235, 'train_rates': 0.7642317954537332, 'learning_rate': 0.00010120463209643614, 'batch_size': 46, 'step_size': 8, 'gamma': 0.9435389151198353}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:51:55,146][0m Trial 48 finished with value: 0.05597245501426466 and parameters: {'observation_period_num': 67, 'train_rates': 0.848044963245512, 'learning_rate': 0.00013126252818894301, 'batch_size': 69, 'step_size': 11, 'gamma': 0.9093753861417389}. Best is trial 31 with value: 0.030218464512269474.[0m
[32m[I 2025-01-06 04:56:02,292][0m Trial 49 finished with value: 0.16536278158092677 and parameters: {'observation_period_num': 56, 'train_rates': 0.8151844446855918, 'learning_rate': 0.00021439090389572846, 'batch_size': 23, 'step_size': 9, 'gamma': 0.9659047107099636}. Best is trial 31 with value: 0.030218464512269474.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-06 04:56:02,303][0m A new study created in memory with name: no-name-8db6182a-e2b9-4cdf-8893-96c1d690b601[0m
[32m[I 2025-01-06 04:57:15,249][0m Trial 0 finished with value: 0.09943911132718322 and parameters: {'observation_period_num': 139, 'train_rates': 0.8290729704458133, 'learning_rate': 0.00024056449276532103, 'batch_size': 187, 'step_size': 10, 'gamma': 0.8875177496059501}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 04:58:12,108][0m Trial 1 finished with value: 0.17262198667658937 and parameters: {'observation_period_num': 251, 'train_rates': 0.7938592033390875, 'learning_rate': 0.0004171466700052226, 'batch_size': 194, 'step_size': 9, 'gamma': 0.9331969484922584}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 04:59:19,956][0m Trial 2 finished with value: 0.5947516097155906 and parameters: {'observation_period_num': 109, 'train_rates': 0.638208323223615, 'learning_rate': 3.6833613029947993e-06, 'batch_size': 139, 'step_size': 12, 'gamma': 0.918865902087354}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:01:17,742][0m Trial 3 finished with value: 0.209755613606357 and parameters: {'observation_period_num': 252, 'train_rates': 0.875024386598585, 'learning_rate': 1.2621912403880423e-05, 'batch_size': 58, 'step_size': 10, 'gamma': 0.7607935387217107}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:02:21,834][0m Trial 4 finished with value: 0.19469366242781483 and parameters: {'observation_period_num': 10, 'train_rates': 0.7238596881445122, 'learning_rate': 6.775332108819383e-05, 'batch_size': 136, 'step_size': 15, 'gamma': 0.9720916264108115}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:03:24,587][0m Trial 5 finished with value: 0.28481848300513574 and parameters: {'observation_period_num': 241, 'train_rates': 0.6736499781351442, 'learning_rate': 0.00021983870125290154, 'batch_size': 243, 'step_size': 14, 'gamma': 0.8305505453259503}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:04:45,812][0m Trial 6 finished with value: 0.39987133102496397 and parameters: {'observation_period_num': 67, 'train_rates': 0.6046606866808019, 'learning_rate': 7.669393834759983e-06, 'batch_size': 87, 'step_size': 11, 'gamma': 0.8165638909593964}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:06:14,317][0m Trial 7 finished with value: 0.11830655907468843 and parameters: {'observation_period_num': 205, 'train_rates': 0.8524128731515488, 'learning_rate': 0.00021617534670031804, 'batch_size': 136, 'step_size': 5, 'gamma': 0.7829453007176815}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:07:20,541][0m Trial 8 finished with value: 0.21554488943760955 and parameters: {'observation_period_num': 52, 'train_rates': 0.795814149104956, 'learning_rate': 6.006711833279173e-05, 'batch_size': 203, 'step_size': 2, 'gamma': 0.8416362283823163}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:08:30,737][0m Trial 9 finished with value: 0.9366366490169807 and parameters: {'observation_period_num': 177, 'train_rates': 0.8055271538755808, 'learning_rate': 3.016891953385708e-06, 'batch_size': 154, 'step_size': 3, 'gamma': 0.7987021125287231}. Best is trial 0 with value: 0.09943911132718322.[0m
[32m[I 2025-01-06 05:14:22,521][0m Trial 10 finished with value: 0.0648777601974351 and parameters: {'observation_period_num': 145, 'train_rates': 0.9852958813193065, 'learning_rate': 0.0008966823985111391, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8824164294465696}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:17:56,164][0m Trial 11 finished with value: 0.10165320931091196 and parameters: {'observation_period_num': 148, 'train_rates': 0.970070342877446, 'learning_rate': 0.0009340858353375002, 'batch_size': 31, 'step_size': 6, 'gamma': 0.8829775306942712}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:19:28,899][0m Trial 12 finished with value: 0.08606170117855072 and parameters: {'observation_period_num': 118, 'train_rates': 0.9805441180001557, 'learning_rate': 0.0009663432762399213, 'batch_size': 89, 'step_size': 7, 'gamma': 0.8794021158097818}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:25:48,934][0m Trial 13 finished with value: 0.3707667026254866 and parameters: {'observation_period_num': 100, 'train_rates': 0.9874155378470324, 'learning_rate': 1.0647709217481747e-06, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8616314974759388}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:27:00,201][0m Trial 14 finished with value: 0.14084086595682435 and parameters: {'observation_period_num': 173, 'train_rates': 0.9251383573502783, 'learning_rate': 0.00090580989623466, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9171517505068583}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:28:13,076][0m Trial 15 finished with value: 0.08483948475998088 and parameters: {'observation_period_num': 91, 'train_rates': 0.9322188578541766, 'learning_rate': 7.905866718599813e-05, 'batch_size': 93, 'step_size': 1, 'gamma': 0.9670298801552142}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:29:57,505][0m Trial 16 finished with value: 0.07697189413011074 and parameters: {'observation_period_num': 79, 'train_rates': 0.9083627757075874, 'learning_rate': 3.87054250429769e-05, 'batch_size': 58, 'step_size': 1, 'gamma': 0.982703151316814}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:32:16,309][0m Trial 17 finished with value: 0.1159464044413015 and parameters: {'observation_period_num': 35, 'train_rates': 0.9082687851753357, 'learning_rate': 1.852007825409275e-05, 'batch_size': 49, 'step_size': 1, 'gamma': 0.9476730807081448}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:34:15,177][0m Trial 18 finished with value: 0.07976770392362623 and parameters: {'observation_period_num': 76, 'train_rates': 0.8801543065930552, 'learning_rate': 3.437980282124772e-05, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9829650350742934}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:38:05,237][0m Trial 19 finished with value: 0.27308719990293634 and parameters: {'observation_period_num': 165, 'train_rates': 0.7445852650822158, 'learning_rate': 0.00010258134334398305, 'batch_size': 23, 'step_size': 8, 'gamma': 0.912122859383334}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:39:25,125][0m Trial 20 finished with value: 0.10409355432063609 and parameters: {'observation_period_num': 30, 'train_rates': 0.9242592945357958, 'learning_rate': 3.695015377816229e-05, 'batch_size': 110, 'step_size': 3, 'gamma': 0.8601885423882668}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:41:26,250][0m Trial 21 finished with value: 0.07811970897766102 and parameters: {'observation_period_num': 82, 'train_rates': 0.8784068743362385, 'learning_rate': 2.8335511842883648e-05, 'batch_size': 57, 'step_size': 5, 'gamma': 0.9836939311151415}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:43:52,661][0m Trial 22 finished with value: 0.09512672916999165 and parameters: {'observation_period_num': 76, 'train_rates': 0.9509086842875324, 'learning_rate': 1.3065533271476835e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.9527655145913935}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:45:20,227][0m Trial 23 finished with value: 0.18725176759405074 and parameters: {'observation_period_num': 142, 'train_rates': 0.8913416333537358, 'learning_rate': 4.885627981131271e-06, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9899066436592934}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:48:26,893][0m Trial 24 finished with value: 0.08183069123809499 and parameters: {'observation_period_num': 122, 'train_rates': 0.9530332797292991, 'learning_rate': 0.0001266091077994313, 'batch_size': 36, 'step_size': 3, 'gamma': 0.9515306413685699}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:50:01,660][0m Trial 25 finished with value: 0.14660150878113085 and parameters: {'observation_period_num': 200, 'train_rates': 0.8523791200925517, 'learning_rate': 2.017089140021376e-05, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9041093686773085}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:51:20,317][0m Trial 26 finished with value: 0.13218604185079275 and parameters: {'observation_period_num': 87, 'train_rates': 0.9006010834517146, 'learning_rate': 0.0004968034313182409, 'batch_size': 100, 'step_size': 8, 'gamma': 0.9406988286836837}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 05:57:07,167][0m Trial 27 finished with value: 0.07517381585133609 and parameters: {'observation_period_num': 62, 'train_rates': 0.9537751990829968, 'learning_rate': 4.1987449756301175e-05, 'batch_size': 18, 'step_size': 2, 'gamma': 0.9675081131095188}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 06:03:43,109][0m Trial 28 finished with value: 0.06815950543834613 and parameters: {'observation_period_num': 48, 'train_rates': 0.964491188755509, 'learning_rate': 4.991698774674299e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9673390447219793}. Best is trial 10 with value: 0.0648777601974351.[0m
[32m[I 2025-01-06 06:09:12,928][0m Trial 29 finished with value: 0.05327870510518551 and parameters: {'observation_period_num': 7, 'train_rates': 0.9485057462206954, 'learning_rate': 0.00015772352356754912, 'batch_size': 19, 'step_size': 2, 'gamma': 0.8951211420104148}. Best is trial 29 with value: 0.05327870510518551.[0m
[32m[I 2025-01-06 06:10:35,853][0m Trial 30 finished with value: 0.08046016097068787 and parameters: {'observation_period_num': 8, 'train_rates': 0.9887259172700771, 'learning_rate': 0.00036420010720965333, 'batch_size': 117, 'step_size': 2, 'gamma': 0.8990877351854941}. Best is trial 29 with value: 0.05327870510518551.[0m
[32m[I 2025-01-06 06:13:49,511][0m Trial 31 finished with value: 0.05977259781481563 and parameters: {'observation_period_num': 36, 'train_rates': 0.9527510900794813, 'learning_rate': 0.00014144709453348543, 'batch_size': 32, 'step_size': 2, 'gamma': 0.9262437243942241}. Best is trial 29 with value: 0.05327870510518551.[0m
[32m[I 2025-01-06 06:16:50,523][0m Trial 32 finished with value: 0.05942030187033437 and parameters: {'observation_period_num': 31, 'train_rates': 0.9444817383585699, 'learning_rate': 0.00016063339608297293, 'batch_size': 37, 'step_size': 2, 'gamma': 0.9283669188059416}. Best is trial 29 with value: 0.05327870510518551.[0m
[32m[I 2025-01-06 06:19:39,299][0m Trial 33 finished with value: 0.046415895567490505 and parameters: {'observation_period_num': 26, 'train_rates': 0.9336378713638892, 'learning_rate': 0.00015672815796326575, 'batch_size': 36, 'step_size': 4, 'gamma': 0.9267263833688791}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:22:22,691][0m Trial 34 finished with value: 0.04665911495685578 and parameters: {'observation_period_num': 23, 'train_rates': 0.9369838462810764, 'learning_rate': 0.000147181263283986, 'batch_size': 41, 'step_size': 3, 'gamma': 0.9288622003693374}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:25:07,340][0m Trial 35 finished with value: 0.08379740957915784 and parameters: {'observation_period_num': 21, 'train_rates': 0.8295413039903711, 'learning_rate': 0.0003216311175503953, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9285077157968588}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:27:03,689][0m Trial 36 finished with value: 0.05219195420161272 and parameters: {'observation_period_num': 18, 'train_rates': 0.9335728708670585, 'learning_rate': 0.0002017935151352457, 'batch_size': 70, 'step_size': 3, 'gamma': 0.8947108078279978}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:28:29,253][0m Trial 37 finished with value: 0.20931616708194925 and parameters: {'observation_period_num': 5, 'train_rates': 0.7594574628703298, 'learning_rate': 8.969997439273078e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.904537821769446}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:29:35,071][0m Trial 38 finished with value: 0.06363191352327272 and parameters: {'observation_period_num': 19, 'train_rates': 0.8623952433971631, 'learning_rate': 0.0005310414406422365, 'batch_size': 167, 'step_size': 4, 'gamma': 0.8922605998013724}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:30:16,547][0m Trial 39 finished with value: 0.2264696949139652 and parameters: {'observation_period_num': 59, 'train_rates': 0.6773103826173006, 'learning_rate': 0.0002500086461044926, 'batch_size': 253, 'step_size': 10, 'gamma': 0.8702331601364668}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:31:20,175][0m Trial 40 finished with value: 0.0780969309963678 and parameters: {'observation_period_num': 45, 'train_rates': 0.8366252987820675, 'learning_rate': 0.0001511176127123482, 'batch_size': 119, 'step_size': 13, 'gamma': 0.8440803006523343}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:33:26,460][0m Trial 41 finished with value: 0.05167272632597854 and parameters: {'observation_period_num': 20, 'train_rates': 0.9205720356185481, 'learning_rate': 0.00021802743287380796, 'batch_size': 48, 'step_size': 2, 'gamma': 0.9316356238188412}. Best is trial 33 with value: 0.046415895567490505.[0m
[32m[I 2025-01-06 06:35:38,869][0m Trial 42 finished with value: 0.04145170445891394 and parameters: {'observation_period_num': 16, 'train_rates': 0.9297458819199611, 'learning_rate': 0.00023693882597308703, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9354409346431699}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:37:37,227][0m Trial 43 finished with value: 0.055278574833564 and parameters: {'observation_period_num': 22, 'train_rates': 0.9230724534685595, 'learning_rate': 0.00024374379558313283, 'batch_size': 53, 'step_size': 3, 'gamma': 0.9160184538145512}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:39:13,866][0m Trial 44 finished with value: 0.08657813553737291 and parameters: {'observation_period_num': 44, 'train_rates': 0.8932277811657843, 'learning_rate': 0.0006198876056943078, 'batch_size': 66, 'step_size': 4, 'gamma': 0.936715491766367}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:41:33,499][0m Trial 45 finished with value: 0.05429866898474086 and parameters: {'observation_period_num': 18, 'train_rates': 0.9118248262480071, 'learning_rate': 0.00021321841800470717, 'batch_size': 44, 'step_size': 3, 'gamma': 0.9555553803866141}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:43:06,918][0m Trial 46 finished with value: 0.06810806661080092 and parameters: {'observation_period_num': 27, 'train_rates': 0.9336193694254797, 'learning_rate': 0.00031149491279558185, 'batch_size': 82, 'step_size': 9, 'gamma': 0.9396582672769774}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:44:21,014][0m Trial 47 finished with value: 0.08990580588579178 and parameters: {'observation_period_num': 55, 'train_rates': 0.9736741003996634, 'learning_rate': 6.873704153174068e-05, 'batch_size': 228, 'step_size': 6, 'gamma': 0.9095204647850038}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:47:51,120][0m Trial 48 finished with value: 0.09361138175984826 and parameters: {'observation_period_num': 43, 'train_rates': 0.8640850167046685, 'learning_rate': 0.00010484105356697403, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9200655113801343}. Best is trial 42 with value: 0.04145170445891394.[0m
[32m[I 2025-01-06 06:49:24,046][0m Trial 49 finished with value: 0.10834553392965403 and parameters: {'observation_period_num': 68, 'train_rates': 0.814236880487357, 'learning_rate': 0.00019438558874294905, 'batch_size': 64, 'step_size': 1, 'gamma': 0.8761461377641907}. Best is trial 42 with value: 0.04145170445891394.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-06 06:49:24,056][0m A new study created in memory with name: no-name-418873b4-63a6-4919-a0be-d096715ab8bf[0m
[32m[I 2025-01-06 06:50:05,304][0m Trial 0 finished with value: 0.3167053908253916 and parameters: {'observation_period_num': 106, 'train_rates': 0.6372070915746891, 'learning_rate': 1.7634860418124736e-05, 'batch_size': 196, 'step_size': 13, 'gamma': 0.8804035976934861}. Best is trial 0 with value: 0.3167053908253916.[0m
Early stopping at epoch 55
[32m[I 2025-01-06 06:50:58,202][0m Trial 1 finished with value: 0.18687632181466263 and parameters: {'observation_period_num': 114, 'train_rates': 0.8019732405641977, 'learning_rate': 0.0004010567521725455, 'batch_size': 118, 'step_size': 1, 'gamma': 0.7783108059216094}. Best is trial 1 with value: 0.18687632181466263.[0m
[32m[I 2025-01-06 06:52:10,620][0m Trial 2 finished with value: 1.11767578125 and parameters: {'observation_period_num': 193, 'train_rates': 0.9796028371014365, 'learning_rate': 1.0486352062433136e-06, 'batch_size': 224, 'step_size': 4, 'gamma': 0.8822540558566005}. Best is trial 1 with value: 0.18687632181466263.[0m
[32m[I 2025-01-06 06:53:03,593][0m Trial 3 finished with value: 0.7791343061186426 and parameters: {'observation_period_num': 150, 'train_rates': 0.7522190294267513, 'learning_rate': 2.8988416017889914e-06, 'batch_size': 228, 'step_size': 12, 'gamma': 0.9665430865784969}. Best is trial 1 with value: 0.18687632181466263.[0m
[32m[I 2025-01-06 06:54:06,566][0m Trial 4 finished with value: 0.08825626614077992 and parameters: {'observation_period_num': 122, 'train_rates': 0.946922593539052, 'learning_rate': 6.632051087470497e-05, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9780719756673617}. Best is trial 4 with value: 0.08825626614077992.[0m
[32m[I 2025-01-06 06:54:50,016][0m Trial 5 finished with value: 0.27250718706991617 and parameters: {'observation_period_num': 104, 'train_rates': 0.740904299402371, 'learning_rate': 0.00023350004248926773, 'batch_size': 219, 'step_size': 5, 'gamma': 0.7638836204422591}. Best is trial 4 with value: 0.08825626614077992.[0m
[32m[I 2025-01-06 06:56:45,641][0m Trial 6 finished with value: 0.19389551126082008 and parameters: {'observation_period_num': 182, 'train_rates': 0.9606565408686532, 'learning_rate': 4.596265386497685e-06, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9601603343465118}. Best is trial 4 with value: 0.08825626614077992.[0m
[32m[I 2025-01-06 06:57:49,148][0m Trial 7 finished with value: 0.34615833918363653 and parameters: {'observation_period_num': 72, 'train_rates': 0.6241157464518223, 'learning_rate': 3.524974061718216e-05, 'batch_size': 172, 'step_size': 6, 'gamma': 0.7990676732636747}. Best is trial 4 with value: 0.08825626614077992.[0m
[32m[I 2025-01-06 06:59:17,684][0m Trial 8 finished with value: 0.08789040419188413 and parameters: {'observation_period_num': 47, 'train_rates': 0.9470178608868065, 'learning_rate': 0.00035608122641019605, 'batch_size': 104, 'step_size': 14, 'gamma': 0.9345958126791262}. Best is trial 8 with value: 0.08789040419188413.[0m
[32m[I 2025-01-06 07:00:38,164][0m Trial 9 finished with value: 0.24395547711302887 and parameters: {'observation_period_num': 161, 'train_rates': 0.6524001049102492, 'learning_rate': 0.0007160155005092637, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9008324084601121}. Best is trial 8 with value: 0.08789040419188413.[0m
[32m[I 2025-01-06 07:01:30,774][0m Trial 10 finished with value: 0.045020006326490274 and parameters: {'observation_period_num': 15, 'train_rates': 0.8782146686253406, 'learning_rate': 0.00016550284677282825, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8301574248225722}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:02:27,318][0m Trial 11 finished with value: 0.04717784463314997 and parameters: {'observation_period_num': 11, 'train_rates': 0.8739245017035371, 'learning_rate': 0.00013432643930567876, 'batch_size': 153, 'step_size': 9, 'gamma': 0.8279640212603333}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:03:25,668][0m Trial 12 finished with value: 0.052640157444474034 and parameters: {'observation_period_num': 23, 'train_rates': 0.8665791790099262, 'learning_rate': 0.00010701139578573652, 'batch_size': 147, 'step_size': 10, 'gamma': 0.8345114843423123}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:04:28,221][0m Trial 13 finished with value: 0.05212418587282089 and parameters: {'observation_period_num': 5, 'train_rates': 0.8805468972035309, 'learning_rate': 0.00013749929461565218, 'batch_size': 160, 'step_size': 9, 'gamma': 0.8315689191065038}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:05:13,323][0m Trial 14 finished with value: 0.2240462750196457 and parameters: {'observation_period_num': 234, 'train_rates': 0.8654627912079905, 'learning_rate': 2.2055669833004528e-05, 'batch_size': 183, 'step_size': 11, 'gamma': 0.8346170920093576}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:06:07,663][0m Trial 15 finished with value: 0.07973931150611124 and parameters: {'observation_period_num': 57, 'train_rates': 0.8140043863883655, 'learning_rate': 5.5059749273555916e-05, 'batch_size': 139, 'step_size': 8, 'gamma': 0.8061099174710792}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:06:53,750][0m Trial 16 finished with value: 0.19831516145669742 and parameters: {'observation_period_num': 27, 'train_rates': 0.8973357232878434, 'learning_rate': 9.649986175413438e-06, 'batch_size': 251, 'step_size': 8, 'gamma': 0.8529447113600722}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:07:58,927][0m Trial 17 finished with value: 0.12649058815471348 and parameters: {'observation_period_num': 78, 'train_rates': 0.8329442677842951, 'learning_rate': 0.0008708230654837719, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9179117513207846}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:11:28,342][0m Trial 18 finished with value: 0.16687988666300846 and parameters: {'observation_period_num': 11, 'train_rates': 0.7487628451204521, 'learning_rate': 0.0001672224364775755, 'batch_size': 24, 'step_size': 10, 'gamma': 0.8016643810827624}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:12:31,398][0m Trial 19 finished with value: 0.08751929888632072 and parameters: {'observation_period_num': 38, 'train_rates': 0.9166255013569214, 'learning_rate': 7.658176705617257e-05, 'batch_size': 131, 'step_size': 2, 'gamma': 0.8579727954258691}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:13:34,318][0m Trial 20 finished with value: 0.07807080706508902 and parameters: {'observation_period_num': 80, 'train_rates': 0.8394413707506073, 'learning_rate': 0.00031740555479559297, 'batch_size': 199, 'step_size': 11, 'gamma': 0.7508413691611664}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:14:55,380][0m Trial 21 finished with value: 0.048550050233451415 and parameters: {'observation_period_num': 6, 'train_rates': 0.9038882901314328, 'learning_rate': 0.00014855727976352033, 'batch_size': 160, 'step_size': 9, 'gamma': 0.8289046423333433}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:15:56,764][0m Trial 22 finished with value: 0.09483012598711293 and parameters: {'observation_period_num': 49, 'train_rates': 0.9015092332446533, 'learning_rate': 3.9049240761806886e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8252894301261936}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:17:27,322][0m Trial 23 finished with value: 0.046536829854760854 and parameters: {'observation_period_num': 8, 'train_rates': 0.9288379709507082, 'learning_rate': 0.00013918902996113008, 'batch_size': 150, 'step_size': 11, 'gamma': 0.8598446626331264}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:18:59,375][0m Trial 24 finished with value: 0.055384849508603416 and parameters: {'observation_period_num': 30, 'train_rates': 0.9262268250123802, 'learning_rate': 8.816530972316157e-05, 'batch_size': 131, 'step_size': 12, 'gamma': 0.8649827409726818}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:20:43,386][0m Trial 25 finished with value: 0.05683736354470826 and parameters: {'observation_period_num': 56, 'train_rates': 0.8563821278355408, 'learning_rate': 0.0005109965930059018, 'batch_size': 89, 'step_size': 11, 'gamma': 0.7869198621747557}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:21:52,279][0m Trial 26 finished with value: 0.07923295348882675 and parameters: {'observation_period_num': 88, 'train_rates': 0.988137561958927, 'learning_rate': 0.00021298365655277274, 'batch_size': 188, 'step_size': 7, 'gamma': 0.8937597946994836}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:23:07,831][0m Trial 27 finished with value: 0.2806343724951148 and parameters: {'observation_period_num': 252, 'train_rates': 0.7748954735285477, 'learning_rate': 1.4442892434526977e-05, 'batch_size': 150, 'step_size': 15, 'gamma': 0.8485797199916385}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:24:14,982][0m Trial 28 finished with value: 0.16438640382275727 and parameters: {'observation_period_num': 25, 'train_rates': 0.6982553837131205, 'learning_rate': 0.0002555995115743588, 'batch_size': 209, 'step_size': 10, 'gamma': 0.8144588450208508}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:25:27,637][0m Trial 29 finished with value: 0.07339197412437322 and parameters: {'observation_period_num': 64, 'train_rates': 0.9324438066621539, 'learning_rate': 4.987682950775574e-05, 'batch_size': 172, 'step_size': 13, 'gamma': 0.8778831226530973}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:26:44,259][0m Trial 30 finished with value: 0.06196998199253152 and parameters: {'observation_period_num': 41, 'train_rates': 0.835938128150263, 'learning_rate': 0.0005614239376857627, 'batch_size': 129, 'step_size': 12, 'gamma': 0.8453050490207912}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:27:44,747][0m Trial 31 finished with value: 0.05039576143502427 and parameters: {'observation_period_num': 5, 'train_rates': 0.8940107355560375, 'learning_rate': 0.0001372630269633417, 'batch_size': 156, 'step_size': 9, 'gamma': 0.8157111678093623}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:28:40,909][0m Trial 32 finished with value: 0.0601333761587739 and parameters: {'observation_period_num': 18, 'train_rates': 0.9130796515855798, 'learning_rate': 0.00010399222520035076, 'batch_size': 176, 'step_size': 10, 'gamma': 0.7852223435243649}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:29:48,796][0m Trial 33 finished with value: 0.05769749730825424 and parameters: {'observation_period_num': 35, 'train_rates': 0.9673713973872216, 'learning_rate': 0.00017088968481215072, 'batch_size': 118, 'step_size': 9, 'gamma': 0.8764929187541867}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:30:42,926][0m Trial 34 finished with value: 0.10019598444502557 and parameters: {'observation_period_num': 5, 'train_rates': 0.7975271802749307, 'learning_rate': 2.3710953902071192e-05, 'batch_size': 195, 'step_size': 7, 'gamma': 0.8419867659414204}. Best is trial 10 with value: 0.045020006326490274.[0m
[32m[I 2025-01-06 07:31:43,801][0m Trial 35 finished with value: 0.03728187364976555 and parameters: {'observation_period_num': 17, 'train_rates': 0.8853595916349418, 'learning_rate': 0.00041009427658064576, 'batch_size': 144, 'step_size': 11, 'gamma': 0.8673713334080981}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:32:41,947][0m Trial 36 finished with value: 0.07497509784952423 and parameters: {'observation_period_num': 105, 'train_rates': 0.8814019390036325, 'learning_rate': 0.0005278763723712092, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8955185267290662}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:33:47,096][0m Trial 37 finished with value: 0.08991896931403949 and parameters: {'observation_period_num': 142, 'train_rates': 0.9426038365383622, 'learning_rate': 0.00036841587933441407, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8651169245158433}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:34:42,398][0m Trial 38 finished with value: 0.8591748313724715 and parameters: {'observation_period_num': 96, 'train_rates': 0.7764142843487754, 'learning_rate': 1.137435376761261e-06, 'batch_size': 124, 'step_size': 11, 'gamma': 0.9121031949069256}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:36:00,162][0m Trial 39 finished with value: 0.040920608591975514 and parameters: {'observation_period_num': 22, 'train_rates': 0.8245276952805216, 'learning_rate': 0.0002734921359783007, 'batch_size': 87, 'step_size': 13, 'gamma': 0.8850064616353049}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:37:28,416][0m Trial 40 finished with value: 0.09670709101765444 and parameters: {'observation_period_num': 123, 'train_rates': 0.8136423454855926, 'learning_rate': 0.0002524373155292106, 'batch_size': 98, 'step_size': 14, 'gamma': 0.9327500843600859}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:38:29,860][0m Trial 41 finished with value: 0.046461301725104924 and parameters: {'observation_period_num': 22, 'train_rates': 0.84937727029321, 'learning_rate': 0.0003070390666145138, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8816024843664374}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:40:32,518][0m Trial 42 finished with value: 0.04850093788794569 and parameters: {'observation_period_num': 20, 'train_rates': 0.8486456281607617, 'learning_rate': 0.0008542920130437795, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8864048612460707}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:41:55,535][0m Trial 43 finished with value: 0.054767534230263995 and parameters: {'observation_period_num': 43, 'train_rates': 0.8220457811181813, 'learning_rate': 0.0004213433372151542, 'batch_size': 109, 'step_size': 13, 'gamma': 0.910676283794394}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:43:34,225][0m Trial 44 finished with value: 0.2120587616766754 and parameters: {'observation_period_num': 65, 'train_rates': 0.7696866520981303, 'learning_rate': 0.0002995334900258196, 'batch_size': 77, 'step_size': 14, 'gamma': 0.8699365303184492}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:44:48,073][0m Trial 45 finished with value: 0.04396015778183937 and parameters: {'observation_period_num': 17, 'train_rates': 0.9607334644530805, 'learning_rate': 0.00020428878527106568, 'batch_size': 140, 'step_size': 12, 'gamma': 0.8815381288958102}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:47:47,370][0m Trial 46 finished with value: 0.13015991508370578 and parameters: {'observation_period_num': 203, 'train_rates': 0.9632712594654789, 'learning_rate': 0.00020382546023000112, 'batch_size': 38, 'step_size': 12, 'gamma': 0.9281367323627132}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:48:45,596][0m Trial 47 finished with value: 0.07289799802418005 and parameters: {'observation_period_num': 53, 'train_rates': 0.8817635054070428, 'learning_rate': 0.0005504423784990492, 'batch_size': 136, 'step_size': 15, 'gamma': 0.8869117668292067}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:49:53,351][0m Trial 48 finished with value: 0.1742531634112702 and parameters: {'observation_period_num': 19, 'train_rates': 0.7307907756830526, 'learning_rate': 0.0007599549972130967, 'batch_size': 105, 'step_size': 12, 'gamma': 0.943653633050219}. Best is trial 35 with value: 0.03728187364976555.[0m
[32m[I 2025-01-06 07:50:48,055][0m Trial 49 finished with value: 0.05320861122106916 and parameters: {'observation_period_num': 33, 'train_rates': 0.8063850644798571, 'learning_rate': 0.0003707405224663495, 'batch_size': 169, 'step_size': 14, 'gamma': 0.9081385561566406}. Best is trial 35 with value: 0.03728187364976555.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-06 07:50:48,065][0m A new study created in memory with name: no-name-5cfa2068-576c-46e4-bc89-ce681d682c2c[0m
[32m[I 2025-01-06 07:53:35,960][0m Trial 0 finished with value: 0.6322113363042711 and parameters: {'observation_period_num': 150, 'train_rates': 0.7303819013962263, 'learning_rate': 1.036353096776124e-06, 'batch_size': 30, 'step_size': 13, 'gamma': 0.7514338095204032}. Best is trial 0 with value: 0.6322113363042711.[0m
[32m[I 2025-01-06 07:56:55,371][0m Trial 1 finished with value: 0.14651963606530385 and parameters: {'observation_period_num': 31, 'train_rates': 0.6193646739718082, 'learning_rate': 0.0009008260947318165, 'batch_size': 23, 'step_size': 1, 'gamma': 0.9740108842641049}. Best is trial 1 with value: 0.14651963606530385.[0m
[32m[I 2025-01-06 07:58:16,621][0m Trial 2 finished with value: 0.07293483073894794 and parameters: {'observation_period_num': 66, 'train_rates': 0.8876395339691067, 'learning_rate': 4.1849784935960866e-05, 'batch_size': 197, 'step_size': 11, 'gamma': 0.921078470415648}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 07:59:14,855][0m Trial 3 finished with value: 0.4161276830350115 and parameters: {'observation_period_num': 102, 'train_rates': 0.743263021213905, 'learning_rate': 8.251652009531394e-06, 'batch_size': 205, 'step_size': 10, 'gamma': 0.9793039337634852}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:00:31,474][0m Trial 4 finished with value: 0.2807752405469482 and parameters: {'observation_period_num': 201, 'train_rates': 0.7042327730654554, 'learning_rate': 9.024796944868637e-05, 'batch_size': 88, 'step_size': 15, 'gamma': 0.8462192728491956}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:03:12,499][0m Trial 5 finished with value: 0.1817258264636621 and parameters: {'observation_period_num': 21, 'train_rates': 0.6726702970484654, 'learning_rate': 1.0201801537512882e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.7638198233057332}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:04:38,591][0m Trial 6 finished with value: 0.6576386798627882 and parameters: {'observation_period_num': 106, 'train_rates': 0.835991570309248, 'learning_rate': 1.5175577967909542e-06, 'batch_size': 65, 'step_size': 1, 'gamma': 0.9285346774282113}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:05:53,546][0m Trial 7 finished with value: 0.14775793124574402 and parameters: {'observation_period_num': 166, 'train_rates': 0.8972199546848187, 'learning_rate': 1.557827839220032e-05, 'batch_size': 95, 'step_size': 2, 'gamma': 0.9716934055989243}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:06:51,483][0m Trial 8 finished with value: 0.4830736219882965 and parameters: {'observation_period_num': 193, 'train_rates': 0.9881596638280228, 'learning_rate': 5.41810772277044e-06, 'batch_size': 193, 'step_size': 4, 'gamma': 0.878566374426006}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:08:12,419][0m Trial 9 finished with value: 0.24693981480728733 and parameters: {'observation_period_num': 130, 'train_rates': 0.8705526768736778, 'learning_rate': 4.121702918058429e-06, 'batch_size': 88, 'step_size': 10, 'gamma': 0.9243957705512156}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:09:22,058][0m Trial 10 finished with value: 0.08072452992200851 and parameters: {'observation_period_num': 68, 'train_rates': 0.9542232108840678, 'learning_rate': 8.759030659442387e-05, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8414103329634052}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:10:41,469][0m Trial 11 finished with value: 0.08388181775808334 and parameters: {'observation_period_num': 64, 'train_rates': 0.960284053691392, 'learning_rate': 8.951858067258711e-05, 'batch_size': 157, 'step_size': 6, 'gamma': 0.820888772107033}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:11:38,055][0m Trial 12 finished with value: 0.08610356599092484 and parameters: {'observation_period_num': 65, 'train_rates': 0.9232995160947358, 'learning_rate': 6.921782366393031e-05, 'batch_size': 235, 'step_size': 7, 'gamma': 0.8919671844530804}. Best is trial 2 with value: 0.07293483073894794.[0m
[32m[I 2025-01-06 08:12:55,796][0m Trial 13 finished with value: 0.04995139804221928 and parameters: {'observation_period_num': 69, 'train_rates': 0.8374807040667689, 'learning_rate': 0.00033137094491402446, 'batch_size': 156, 'step_size': 9, 'gamma': 0.8189410375776823}. Best is trial 13 with value: 0.04995139804221928.[0m
[32m[I 2025-01-06 08:13:48,017][0m Trial 14 finished with value: 0.111993294943221 and parameters: {'observation_period_num': 237, 'train_rates': 0.8020479272069055, 'learning_rate': 0.0006855183141706175, 'batch_size': 248, 'step_size': 9, 'gamma': 0.7954124912256438}. Best is trial 13 with value: 0.04995139804221928.[0m
[32m[I 2025-01-06 08:15:01,056][0m Trial 15 finished with value: 0.035094846325384656 and parameters: {'observation_period_num': 5, 'train_rates': 0.8362450684866022, 'learning_rate': 0.0002812378769079952, 'batch_size': 139, 'step_size': 12, 'gamma': 0.9078331815955271}. Best is trial 15 with value: 0.035094846325384656.[0m
[32m[I 2025-01-06 08:16:15,391][0m Trial 16 finished with value: 0.03585000886125179 and parameters: {'observation_period_num': 9, 'train_rates': 0.8066898315759067, 'learning_rate': 0.00027949691216055353, 'batch_size': 135, 'step_size': 14, 'gamma': 0.792411202453192}. Best is trial 15 with value: 0.035094846325384656.[0m
[32m[I 2025-01-06 08:17:14,252][0m Trial 17 finished with value: 0.04663300883109872 and parameters: {'observation_period_num': 33, 'train_rates': 0.7899668129566173, 'learning_rate': 0.0002831875462537871, 'batch_size': 113, 'step_size': 15, 'gamma': 0.7875711207073263}. Best is trial 15 with value: 0.035094846325384656.[0m
[32m[I 2025-01-06 08:18:23,095][0m Trial 18 finished with value: 0.17561950786384167 and parameters: {'observation_period_num': 11, 'train_rates': 0.7799477049856035, 'learning_rate': 0.0002329089290060652, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8614477536336318}. Best is trial 15 with value: 0.035094846325384656.[0m
[32m[I 2025-01-06 08:19:26,043][0m Trial 19 finished with value: 0.038254240918487246 and parameters: {'observation_period_num': 10, 'train_rates': 0.8333851487959614, 'learning_rate': 0.00016869823462946874, 'batch_size': 138, 'step_size': 14, 'gamma': 0.9025699264671262}. Best is trial 15 with value: 0.035094846325384656.[0m
[32m[I 2025-01-06 08:20:39,486][0m Trial 20 finished with value: 0.21582666037397252 and parameters: {'observation_period_num': 45, 'train_rates': 0.7525861171210326, 'learning_rate': 0.0005305667929689553, 'batch_size': 179, 'step_size': 12, 'gamma': 0.946442971373534}. Best is trial 15 with value: 0.035094846325384656.[0m
[32m[I 2025-01-06 08:21:57,983][0m Trial 21 finished with value: 0.034414323130205496 and parameters: {'observation_period_num': 9, 'train_rates': 0.8409133624433447, 'learning_rate': 0.0002767459636730506, 'batch_size': 133, 'step_size': 14, 'gamma': 0.8917363825487329}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:23:23,284][0m Trial 22 finished with value: 0.05924841409861134 and parameters: {'observation_period_num': 43, 'train_rates': 0.8646062565345306, 'learning_rate': 0.00015493284572043795, 'batch_size': 120, 'step_size': 14, 'gamma': 0.8962535676885104}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:24:27,097][0m Trial 23 finished with value: 0.07181803301768466 and parameters: {'observation_period_num': 93, 'train_rates': 0.8159698390593614, 'learning_rate': 0.0004038965760106811, 'batch_size': 143, 'step_size': 13, 'gamma': 0.8691551641343663}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:26:04,861][0m Trial 24 finished with value: 0.19136259062499544 and parameters: {'observation_period_num': 9, 'train_rates': 0.7661064628146612, 'learning_rate': 2.752964805950684e-05, 'batch_size': 108, 'step_size': 15, 'gamma': 0.9046605390609241}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:27:52,968][0m Trial 25 finished with value: 0.046002302073380526 and parameters: {'observation_period_num': 46, 'train_rates': 0.8539072496843931, 'learning_rate': 0.0001593112197724642, 'batch_size': 64, 'step_size': 11, 'gamma': 0.8396760245958647}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:29:19,358][0m Trial 26 finished with value: 0.037178416606558275 and parameters: {'observation_period_num': 24, 'train_rates': 0.9143489918982229, 'learning_rate': 0.0009026312419863066, 'batch_size': 180, 'step_size': 12, 'gamma': 0.9555389290742872}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:30:15,245][0m Trial 27 finished with value: 0.07416614773848378 and parameters: {'observation_period_num': 48, 'train_rates': 0.8057933576479844, 'learning_rate': 3.72880260389291e-05, 'batch_size': 217, 'step_size': 14, 'gamma': 0.8825872982202684}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:31:22,730][0m Trial 28 finished with value: 0.13939312626586256 and parameters: {'observation_period_num': 6, 'train_rates': 0.7055000654475845, 'learning_rate': 0.0004780107060452423, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8572651676360412}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:32:58,408][0m Trial 29 finished with value: 0.18688337330985696 and parameters: {'observation_period_num': 88, 'train_rates': 0.7220479599087918, 'learning_rate': 0.00021951612920178044, 'batch_size': 71, 'step_size': 13, 'gamma': 0.7570187136920594}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:34:06,222][0m Trial 30 finished with value: 0.2539212974952534 and parameters: {'observation_period_num': 128, 'train_rates': 0.7734296803849491, 'learning_rate': 5.7418881810586334e-05, 'batch_size': 133, 'step_size': 14, 'gamma': 0.8155198520006961}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:35:07,417][0m Trial 31 finished with value: 0.05416000058293825 and parameters: {'observation_period_num': 25, 'train_rates': 0.9156751514932953, 'learning_rate': 0.0008912464903296213, 'batch_size': 173, 'step_size': 12, 'gamma': 0.957701392806185}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:36:24,755][0m Trial 32 finished with value: 0.04424686714240881 and parameters: {'observation_period_num': 29, 'train_rates': 0.9269633433822091, 'learning_rate': 0.0009000123832276353, 'batch_size': 147, 'step_size': 13, 'gamma': 0.9378722838016786}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:37:22,151][0m Trial 33 finished with value: 0.04091336081425349 and parameters: {'observation_period_num': 22, 'train_rates': 0.8894940194832062, 'learning_rate': 0.00047859980055149555, 'batch_size': 183, 'step_size': 10, 'gamma': 0.9148359757954959}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:38:36,092][0m Trial 34 finished with value: 0.25918557454459706 and parameters: {'observation_period_num': 37, 'train_rates': 0.6208432018727965, 'learning_rate': 0.00012395418004057106, 'batch_size': 215, 'step_size': 12, 'gamma': 0.9895581902391899}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:39:56,487][0m Trial 35 finished with value: 0.08822394555394575 and parameters: {'observation_period_num': 54, 'train_rates': 0.8554903799628727, 'learning_rate': 0.0006543413314923758, 'batch_size': 124, 'step_size': 11, 'gamma': 0.9607052194689976}. Best is trial 21 with value: 0.034414323130205496.[0m
[32m[I 2025-01-06 08:41:22,597][0m Trial 36 finished with value: 0.032394219275956454 and parameters: {'observation_period_num': 5, 'train_rates': 0.8166160337094224, 'learning_rate': 0.00030181018354605223, 'batch_size': 103, 'step_size': 15, 'gamma': 0.9388935515319967}. Best is trial 36 with value: 0.032394219275956454.[0m
[32m[I 2025-01-06 08:43:00,465][0m Trial 37 finished with value: 0.029135481722013495 and parameters: {'observation_period_num': 6, 'train_rates': 0.8181393396055044, 'learning_rate': 0.0003431980657717307, 'batch_size': 101, 'step_size': 15, 'gamma': 0.9134879429995789}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:45:25,926][0m Trial 38 finished with value: 0.06439314406992493 and parameters: {'observation_period_num': 83, 'train_rates': 0.8326744179881232, 'learning_rate': 2.5917595509885842e-05, 'batch_size': 47, 'step_size': 15, 'gamma': 0.9102816082263457}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:46:56,045][0m Trial 39 finished with value: 0.1550729586077588 and parameters: {'observation_period_num': 5, 'train_rates': 0.7461460159428229, 'learning_rate': 0.00011944676645188309, 'batch_size': 107, 'step_size': 15, 'gamma': 0.9316123114101211}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:48:31,616][0m Trial 40 finished with value: 0.04402678463030869 and parameters: {'observation_period_num': 23, 'train_rates': 0.8790782313239152, 'learning_rate': 0.0003618017765242206, 'batch_size': 96, 'step_size': 15, 'gamma': 0.8864569756673077}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:49:58,550][0m Trial 41 finished with value: 0.033185878504483145 and parameters: {'observation_period_num': 16, 'train_rates': 0.8198442149190576, 'learning_rate': 0.00019427535479143459, 'batch_size': 78, 'step_size': 14, 'gamma': 0.9195829110116635}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:51:34,696][0m Trial 42 finished with value: 0.038524357832613446 and parameters: {'observation_period_num': 17, 'train_rates': 0.8213200058548348, 'learning_rate': 0.00021251407673346376, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9185043446932897}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:53:59,645][0m Trial 43 finished with value: 0.053450925288184375 and parameters: {'observation_period_num': 35, 'train_rates': 0.8468600353254748, 'learning_rate': 0.00032352977985286574, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9399615422468385}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:55:39,655][0m Trial 44 finished with value: 0.06443114745266297 and parameters: {'observation_period_num': 56, 'train_rates': 0.7947029001060594, 'learning_rate': 5.6322631471672004e-05, 'batch_size': 98, 'step_size': 15, 'gamma': 0.9200110772247208}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:57:19,240][0m Trial 45 finished with value: 0.06726512213019614 and parameters: {'observation_period_num': 158, 'train_rates': 0.8319526365549317, 'learning_rate': 0.00010863491584020371, 'batch_size': 77, 'step_size': 14, 'gamma': 0.8751192959637235}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 08:59:26,616][0m Trial 46 finished with value: 0.03664548644195489 and parameters: {'observation_period_num': 18, 'train_rates': 0.8753809517631681, 'learning_rate': 0.0006348833496721293, 'batch_size': 58, 'step_size': 4, 'gamma': 0.8979636377221781}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 09:00:55,881][0m Trial 47 finished with value: 0.3697401631378128 and parameters: {'observation_period_num': 190, 'train_rates': 0.8188728959573274, 'learning_rate': 2.0370957339352286e-06, 'batch_size': 114, 'step_size': 13, 'gamma': 0.9286308614655828}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 09:02:16,126][0m Trial 48 finished with value: 0.2796551731294123 and parameters: {'observation_period_num': 242, 'train_rates': 0.7644232199221138, 'learning_rate': 0.00019093335570722912, 'batch_size': 101, 'step_size': 14, 'gamma': 0.9697462055685775}. Best is trial 37 with value: 0.029135481722013495.[0m
[32m[I 2025-01-06 09:07:51,946][0m Trial 49 finished with value: 0.16897788024184726 and parameters: {'observation_period_num': 128, 'train_rates': 0.7854729032280402, 'learning_rate': 0.00024749398295550193, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9497182735837424}. Best is trial 37 with value: 0.029135481722013495.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-06 09:07:51,956][0m A new study created in memory with name: no-name-a6dc8f5f-f973-4a52-b020-71f472a8a542[0m
[32m[I 2025-01-06 09:09:41,839][0m Trial 0 finished with value: 0.28805696211804616 and parameters: {'observation_period_num': 140, 'train_rates': 0.7854549161447354, 'learning_rate': 5.619838299347321e-06, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8969800285522638}. Best is trial 0 with value: 0.28805696211804616.[0m
[32m[I 2025-01-06 09:12:36,554][0m Trial 1 finished with value: 0.26293355189495204 and parameters: {'observation_period_num': 166, 'train_rates': 0.7689715490286035, 'learning_rate': 1.3109471906350178e-05, 'batch_size': 32, 'step_size': 10, 'gamma': 0.9417755900736335}. Best is trial 1 with value: 0.26293355189495204.[0m
[32m[I 2025-01-06 09:13:39,206][0m Trial 2 finished with value: 0.20111721879595795 and parameters: {'observation_period_num': 115, 'train_rates': 0.6749325819867702, 'learning_rate': 0.000588677261115401, 'batch_size': 95, 'step_size': 7, 'gamma': 0.8891431989892898}. Best is trial 2 with value: 0.20111721879595795.[0m
Early stopping at epoch 76
[32m[I 2025-01-06 09:15:21,330][0m Trial 3 finished with value: 0.25527229108385496 and parameters: {'observation_period_num': 131, 'train_rates': 0.6678933428935123, 'learning_rate': 0.0004035610778864206, 'batch_size': 48, 'step_size': 1, 'gamma': 0.7796528428330289}. Best is trial 2 with value: 0.20111721879595795.[0m
[32m[I 2025-01-06 09:16:22,963][0m Trial 4 finished with value: 0.9363001783688863 and parameters: {'observation_period_num': 180, 'train_rates': 0.742582767422276, 'learning_rate': 1.2295102194585205e-06, 'batch_size': 214, 'step_size': 8, 'gamma': 0.7533447824264242}. Best is trial 2 with value: 0.20111721879595795.[0m
[32m[I 2025-01-06 09:19:07,427][0m Trial 5 finished with value: 0.3559973911365549 and parameters: {'observation_period_num': 164, 'train_rates': 0.6203826789880835, 'learning_rate': 1.0739027190338966e-05, 'batch_size': 32, 'step_size': 2, 'gamma': 0.9603010028669459}. Best is trial 2 with value: 0.20111721879595795.[0m
[32m[I 2025-01-06 09:20:20,895][0m Trial 6 finished with value: 0.05360047823816373 and parameters: {'observation_period_num': 33, 'train_rates': 0.9382186861661981, 'learning_rate': 4.518559630803175e-05, 'batch_size': 139, 'step_size': 15, 'gamma': 0.8560867762580303}. Best is trial 6 with value: 0.05360047823816373.[0m
[32m[I 2025-01-06 09:21:51,320][0m Trial 7 finished with value: 0.4716098227554426 and parameters: {'observation_period_num': 23, 'train_rates': 0.7262708941380018, 'learning_rate': 2.336368893642792e-06, 'batch_size': 148, 'step_size': 3, 'gamma': 0.9523500634546265}. Best is trial 6 with value: 0.05360047823816373.[0m
[32m[I 2025-01-06 09:22:58,932][0m Trial 8 finished with value: 0.2461329779050646 and parameters: {'observation_period_num': 147, 'train_rates': 0.8049201926528267, 'learning_rate': 1.1444327427891552e-05, 'batch_size': 216, 'step_size': 7, 'gamma': 0.9423009764406347}. Best is trial 6 with value: 0.05360047823816373.[0m
[32m[I 2025-01-06 09:23:43,551][0m Trial 9 finished with value: 0.9525479093287725 and parameters: {'observation_period_num': 128, 'train_rates': 0.6061285727724234, 'learning_rate': 4.3709213764407404e-06, 'batch_size': 202, 'step_size': 3, 'gamma': 0.8914047094002167}. Best is trial 6 with value: 0.05360047823816373.[0m
[32m[I 2025-01-06 09:25:22,154][0m Trial 10 finished with value: 0.04641268774867058 and parameters: {'observation_period_num': 12, 'train_rates': 0.9576061046350481, 'learning_rate': 8.464035727509163e-05, 'batch_size': 140, 'step_size': 15, 'gamma': 0.824042923595546}. Best is trial 10 with value: 0.04641268774867058.[0m
[32m[I 2025-01-06 09:26:48,565][0m Trial 11 finished with value: 0.0451701320707798 and parameters: {'observation_period_num': 5, 'train_rates': 0.97577905810552, 'learning_rate': 0.00010187642810702779, 'batch_size': 149, 'step_size': 15, 'gamma': 0.8197369166636427}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:28:14,984][0m Trial 12 finished with value: 0.08442340791225433 and parameters: {'observation_period_num': 68, 'train_rates': 0.9730249539044004, 'learning_rate': 0.0001054578783611613, 'batch_size': 168, 'step_size': 15, 'gamma': 0.8102294490174038}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:29:51,291][0m Trial 13 finished with value: 0.04965601567077057 and parameters: {'observation_period_num': 71, 'train_rates': 0.8860363739076705, 'learning_rate': 0.00012833099287613402, 'batch_size': 100, 'step_size': 12, 'gamma': 0.8280117914996913}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:30:49,011][0m Trial 14 finished with value: 0.10439986864963692 and parameters: {'observation_period_num': 218, 'train_rates': 0.8780205168207463, 'learning_rate': 0.00017643435829092587, 'batch_size': 252, 'step_size': 13, 'gamma': 0.8185432617401444}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:32:10,245][0m Trial 15 finished with value: 0.04960276558995247 and parameters: {'observation_period_num': 11, 'train_rates': 0.9838727416247703, 'learning_rate': 4.3582549166114964e-05, 'batch_size': 107, 'step_size': 12, 'gamma': 0.850630710360695}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:33:22,545][0m Trial 16 finished with value: 0.05625300372511038 and parameters: {'observation_period_num': 60, 'train_rates': 0.9071243973491716, 'learning_rate': 0.0002170624020086321, 'batch_size': 174, 'step_size': 10, 'gamma': 0.7841648595220874}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:34:40,211][0m Trial 17 finished with value: 0.053543891901007064 and parameters: {'observation_period_num': 46, 'train_rates': 0.8391718596706413, 'learning_rate': 7.266901448073473e-05, 'batch_size': 126, 'step_size': 14, 'gamma': 0.7950822693117064}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:35:55,029][0m Trial 18 finished with value: 0.062479902181651566 and parameters: {'observation_period_num': 99, 'train_rates': 0.9363558561027895, 'learning_rate': 0.0009514900842857448, 'batch_size': 172, 'step_size': 10, 'gamma': 0.840013041871262}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:37:26,003][0m Trial 19 finished with value: 0.0791510711352726 and parameters: {'observation_period_num': 91, 'train_rates': 0.8402734331804905, 'learning_rate': 1.8686925810428595e-05, 'batch_size': 77, 'step_size': 5, 'gamma': 0.9888634128581818}. Best is trial 11 with value: 0.0451701320707798.[0m
[32m[I 2025-01-06 09:38:18,582][0m Trial 20 finished with value: 0.03774374551669566 and parameters: {'observation_period_num': 5, 'train_rates': 0.9434702900466325, 'learning_rate': 0.00027091070986145723, 'batch_size': 125, 'step_size': 12, 'gamma': 0.7503355893017455}. Best is trial 20 with value: 0.03774374551669566.[0m
[32m[I 2025-01-06 09:39:17,983][0m Trial 21 finished with value: 0.033862330362592875 and parameters: {'observation_period_num': 18, 'train_rates': 0.9482958219974508, 'learning_rate': 0.00031532021749005115, 'batch_size': 122, 'step_size': 13, 'gamma': 0.7523031161769238}. Best is trial 21 with value: 0.033862330362592875.[0m
[32m[I 2025-01-06 09:40:07,004][0m Trial 22 finished with value: 0.02921479009091854 and parameters: {'observation_period_num': 6, 'train_rates': 0.9155232530817599, 'learning_rate': 0.0003518255018147867, 'batch_size': 127, 'step_size': 12, 'gamma': 0.7504447729739783}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:40:57,051][0m Trial 23 finished with value: 0.042158061742782596 and parameters: {'observation_period_num': 39, 'train_rates': 0.9142842031091835, 'learning_rate': 0.0003500183210084742, 'batch_size': 124, 'step_size': 12, 'gamma': 0.7545617493909744}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:42:06,214][0m Trial 24 finished with value: 0.04636980522481548 and parameters: {'observation_period_num': 50, 'train_rates': 0.8441355537208318, 'learning_rate': 0.00029890934401599265, 'batch_size': 80, 'step_size': 11, 'gamma': 0.7662229947695923}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:42:54,698][0m Trial 25 finished with value: 0.09788003856582302 and parameters: {'observation_period_num': 237, 'train_rates': 0.876527445812709, 'learning_rate': 0.000755403820122138, 'batch_size': 114, 'step_size': 13, 'gamma': 0.7767753850920953}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:43:32,412][0m Trial 26 finished with value: 0.04168091703093413 and parameters: {'observation_period_num': 31, 'train_rates': 0.9322891768262384, 'learning_rate': 0.0003774985590496892, 'batch_size': 190, 'step_size': 9, 'gamma': 0.7983535583575472}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:44:12,223][0m Trial 27 finished with value: 0.055225043325619186 and parameters: {'observation_period_num': 86, 'train_rates': 0.9064174360968986, 'learning_rate': 0.0001949122711721526, 'batch_size': 160, 'step_size': 13, 'gamma': 0.7514273469651408}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:45:21,738][0m Trial 28 finished with value: 0.0386503591713771 and parameters: {'observation_period_num': 26, 'train_rates': 0.9516255886833296, 'learning_rate': 0.0005747382528456396, 'batch_size': 88, 'step_size': 11, 'gamma': 0.7661333888019375}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:47:16,717][0m Trial 29 finished with value: 0.055707622319459915 and parameters: {'observation_period_num': 5, 'train_rates': 0.9890010008268851, 'learning_rate': 4.4159777894967166e-05, 'batch_size': 53, 'step_size': 9, 'gamma': 0.7956332494560627}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:48:11,162][0m Trial 30 finished with value: 0.052117405729439424 and parameters: {'observation_period_num': 64, 'train_rates': 0.8640400946444734, 'learning_rate': 0.00019861995596334945, 'batch_size': 118, 'step_size': 14, 'gamma': 0.7743394012162708}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:49:32,584][0m Trial 31 finished with value: 0.03594553435132617 and parameters: {'observation_period_num': 26, 'train_rates': 0.9499476456400862, 'learning_rate': 0.0006193891355734457, 'batch_size': 91, 'step_size': 11, 'gamma': 0.7696946281307155}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:51:09,631][0m Trial 32 finished with value: 0.0326004592015555 and parameters: {'observation_period_num': 20, 'train_rates': 0.915797536787545, 'learning_rate': 0.000524634058802717, 'batch_size': 69, 'step_size': 11, 'gamma': 0.7508635032895318}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:52:36,505][0m Trial 33 finished with value: 0.03417249549837673 and parameters: {'observation_period_num': 23, 'train_rates': 0.9131528861988114, 'learning_rate': 0.0006047097419898141, 'batch_size': 70, 'step_size': 11, 'gamma': 0.7665014917539704}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 09:54:36,605][0m Trial 34 finished with value: 0.0658913028424721 and parameters: {'observation_period_num': 53, 'train_rates': 0.8163884449119191, 'learning_rate': 0.00048434511713615105, 'batch_size': 62, 'step_size': 9, 'gamma': 0.7884317762395181}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:00:50,565][0m Trial 35 finished with value: 0.04467120791609953 and parameters: {'observation_period_num': 41, 'train_rates': 0.9053150652450241, 'learning_rate': 0.0007860380208414673, 'batch_size': 17, 'step_size': 11, 'gamma': 0.8067359537373935}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:04:08,072][0m Trial 36 finished with value: 0.05935803389030637 and parameters: {'observation_period_num': 23, 'train_rates': 0.86093793672508, 'learning_rate': 0.0009522271906896138, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9074661047440202}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:05:53,078][0m Trial 37 finished with value: 0.20771439029023242 and parameters: {'observation_period_num': 79, 'train_rates': 0.782881142055796, 'learning_rate': 0.000477555596492106, 'batch_size': 73, 'step_size': 8, 'gamma': 0.7631390909484345}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:08:19,058][0m Trial 38 finished with value: 0.09949589130079863 and parameters: {'observation_period_num': 194, 'train_rates': 0.9256046725426228, 'learning_rate': 0.00013697104703133437, 'batch_size': 50, 'step_size': 13, 'gamma': 0.7810661715769484}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:09:56,235][0m Trial 39 finished with value: 0.28093530378942744 and parameters: {'observation_period_num': 105, 'train_rates': 0.7501769628099746, 'learning_rate': 2.2029780387120358e-05, 'batch_size': 69, 'step_size': 6, 'gamma': 0.8749732931894284}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:11:29,845][0m Trial 40 finished with value: 0.14313208127400548 and parameters: {'observation_period_num': 18, 'train_rates': 0.694372234969334, 'learning_rate': 0.00036847521241115594, 'batch_size': 101, 'step_size': 10, 'gamma': 0.7611866131045221}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:13:43,772][0m Trial 41 finished with value: 0.05931128105148673 and parameters: {'observation_period_num': 31, 'train_rates': 0.9591142806987806, 'learning_rate': 0.0006413182824713873, 'batch_size': 59, 'step_size': 11, 'gamma': 0.7686083502330834}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:15:15,636][0m Trial 42 finished with value: 0.03085414030615439 and parameters: {'observation_period_num': 20, 'train_rates': 0.8914205180429432, 'learning_rate': 0.0005552452251790171, 'batch_size': 98, 'step_size': 10, 'gamma': 0.7500996623780231}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:17:01,392][0m Trial 43 finished with value: 0.03310637003698639 and parameters: {'observation_period_num': 17, 'train_rates': 0.8907879273285743, 'learning_rate': 0.0002656160335243422, 'batch_size': 87, 'step_size': 10, 'gamma': 0.7571262221730658}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:18:27,645][0m Trial 44 finished with value: 0.052694223339343295 and parameters: {'observation_period_num': 40, 'train_rates': 0.8912150958195872, 'learning_rate': 0.00025955907890575663, 'batch_size': 111, 'step_size': 7, 'gamma': 0.9132450450859978}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:20:10,278][0m Trial 45 finished with value: 0.8329235957472364 and parameters: {'observation_period_num': 15, 'train_rates': 0.815176860602159, 'learning_rate': 1.02002947251964e-06, 'batch_size': 87, 'step_size': 8, 'gamma': 0.7504634813994754}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:21:23,257][0m Trial 46 finished with value: 0.22718124726428043 and parameters: {'observation_period_num': 52, 'train_rates': 0.9233324044879672, 'learning_rate': 7.1107783123452666e-06, 'batch_size': 140, 'step_size': 9, 'gamma': 0.7849778271738104}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:24:11,397][0m Trial 47 finished with value: 0.1062047952382835 and parameters: {'observation_period_num': 155, 'train_rates': 0.9656262179693915, 'learning_rate': 0.00044445051754732774, 'batch_size': 41, 'step_size': 12, 'gamma': 0.7573398900932551}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:25:08,471][0m Trial 48 finished with value: 0.04548066883041455 and parameters: {'observation_period_num': 15, 'train_rates': 0.8935320268975036, 'learning_rate': 0.00015068417505830455, 'batch_size': 151, 'step_size': 10, 'gamma': 0.7765648454125172}. Best is trial 22 with value: 0.02921479009091854.[0m
[32m[I 2025-01-06 10:26:11,446][0m Trial 49 finished with value: 0.06999598773791618 and parameters: {'observation_period_num': 127, 'train_rates': 0.8629490105834194, 'learning_rate': 6.776261470846836e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.8059876664573778}. Best is trial 22 with value: 0.02921479009091854.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-06 10:26:11,456][0m A new study created in memory with name: no-name-3788357d-febb-4e1a-aeef-d2bbd858ac08[0m
[32m[I 2025-01-06 10:28:22,354][0m Trial 0 finished with value: 0.1883041196002354 and parameters: {'observation_period_num': 10, 'train_rates': 0.7767457764573342, 'learning_rate': 0.0008212826758052297, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9814209208737478}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:30:18,335][0m Trial 1 finished with value: 0.1998296660544761 and parameters: {'observation_period_num': 98, 'train_rates': 0.6693791667498526, 'learning_rate': 0.00012148794255755994, 'batch_size': 45, 'step_size': 5, 'gamma': 0.7790618024753583}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:31:17,733][0m Trial 2 finished with value: 0.27042077856994895 and parameters: {'observation_period_num': 202, 'train_rates': 0.7512577535810661, 'learning_rate': 0.0005103147889864759, 'batch_size': 105, 'step_size': 7, 'gamma': 0.7863278186481325}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:32:03,056][0m Trial 3 finished with value: 0.3227836489241085 and parameters: {'observation_period_num': 163, 'train_rates': 0.7554422673172645, 'learning_rate': 0.00014829096831308226, 'batch_size': 228, 'step_size': 2, 'gamma': 0.9027844548990294}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:32:50,011][0m Trial 4 finished with value: 1.560564162227714 and parameters: {'observation_period_num': 72, 'train_rates': 0.714046793442126, 'learning_rate': 1.5309939857988888e-06, 'batch_size': 195, 'step_size': 5, 'gamma': 0.9116560866785018}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:35:01,494][0m Trial 5 finished with value: 0.7601776347687496 and parameters: {'observation_period_num': 184, 'train_rates': 0.712770927071587, 'learning_rate': 1.939741328477388e-06, 'batch_size': 36, 'step_size': 4, 'gamma': 0.7512063256641541}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:35:56,601][0m Trial 6 finished with value: 0.8386204838752747 and parameters: {'observation_period_num': 127, 'train_rates': 0.9277908142139686, 'learning_rate': 1.6000478310699047e-06, 'batch_size': 249, 'step_size': 13, 'gamma': 0.9574721735398674}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:36:43,811][0m Trial 7 finished with value: 0.4471698353841701 and parameters: {'observation_period_num': 159, 'train_rates': 0.6533319556169952, 'learning_rate': 1.340056321973919e-05, 'batch_size': 166, 'step_size': 14, 'gamma': 0.8058629905746295}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:37:51,632][0m Trial 8 finished with value: 0.24088237545891386 and parameters: {'observation_period_num': 54, 'train_rates': 0.8177996209004296, 'learning_rate': 7.0713459007648195e-06, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8269950660857969}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:38:54,989][0m Trial 9 finished with value: 0.6579787731170654 and parameters: {'observation_period_num': 30, 'train_rates': 0.9489615145721606, 'learning_rate': 1.413804670260477e-06, 'batch_size': 168, 'step_size': 14, 'gamma': 0.9105589223237575}. Best is trial 0 with value: 0.1883041196002354.[0m
[32m[I 2025-01-06 10:40:16,453][0m Trial 10 finished with value: 0.1518791157153387 and parameters: {'observation_period_num': 247, 'train_rates': 0.864337112773133, 'learning_rate': 0.00078758783535355, 'batch_size': 78, 'step_size': 1, 'gamma': 0.9889773892711057}. Best is trial 10 with value: 0.1518791157153387.[0m
[32m[I 2025-01-06 10:41:30,272][0m Trial 11 finished with value: 0.11266739634301165 and parameters: {'observation_period_num': 230, 'train_rates': 0.8468579204175464, 'learning_rate': 0.0009840748740555178, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9863599586324885}. Best is trial 11 with value: 0.11266739634301165.[0m
[32m[I 2025-01-06 10:42:47,871][0m Trial 12 finished with value: 0.13182835990115058 and parameters: {'observation_period_num': 252, 'train_rates': 0.8538408596416733, 'learning_rate': 0.00027321999982602476, 'batch_size': 83, 'step_size': 1, 'gamma': 0.98316903283093}. Best is trial 11 with value: 0.11266739634301165.[0m
[32m[I 2025-01-06 10:44:07,387][0m Trial 13 finished with value: 0.1170448968459291 and parameters: {'observation_period_num': 248, 'train_rates': 0.8736314794624878, 'learning_rate': 0.0001833876096753612, 'batch_size': 82, 'step_size': 11, 'gamma': 0.9444131488132134}. Best is trial 11 with value: 0.11266739634301165.[0m
[32m[I 2025-01-06 10:45:32,271][0m Trial 14 finished with value: 0.09928115018568803 and parameters: {'observation_period_num': 207, 'train_rates': 0.8957908844018853, 'learning_rate': 5.299312361302253e-05, 'batch_size': 88, 'step_size': 11, 'gamma': 0.9424777907896872}. Best is trial 14 with value: 0.09928115018568803.[0m
[32m[I 2025-01-06 10:46:18,864][0m Trial 15 finished with value: 0.10349801286489149 and parameters: {'observation_period_num': 211, 'train_rates': 0.9138858106625714, 'learning_rate': 5.043405402603585e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8534771281828853}. Best is trial 14 with value: 0.09928115018568803.[0m
[32m[I 2025-01-06 10:47:25,828][0m Trial 16 finished with value: 0.1340683549642563 and parameters: {'observation_period_num': 204, 'train_rates': 0.9873634284141182, 'learning_rate': 4.857331304102715e-05, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8540847795273694}. Best is trial 14 with value: 0.09928115018568803.[0m
[32m[I 2025-01-06 10:48:27,206][0m Trial 17 finished with value: 0.09760961713998215 and parameters: {'observation_period_num': 133, 'train_rates': 0.9185878931509864, 'learning_rate': 4.341600599795618e-05, 'batch_size': 160, 'step_size': 10, 'gamma': 0.8700048832205945}. Best is trial 17 with value: 0.09760961713998215.[0m
[32m[I 2025-01-06 10:49:26,778][0m Trial 18 finished with value: 0.2091849446296692 and parameters: {'observation_period_num': 129, 'train_rates': 0.9706613478412007, 'learning_rate': 1.685689551124821e-05, 'batch_size': 185, 'step_size': 12, 'gamma': 0.8879758212880161}. Best is trial 17 with value: 0.09760961713998215.[0m
[32m[I 2025-01-06 10:54:36,003][0m Trial 19 finished with value: 0.08359762501937372 and parameters: {'observation_period_num': 127, 'train_rates': 0.9046847316431111, 'learning_rate': 7.122944110748255e-05, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9387063679043092}. Best is trial 19 with value: 0.08359762501937372.[0m
[32m[I 2025-01-06 11:00:00,862][0m Trial 20 finished with value: 0.12236044365705318 and parameters: {'observation_period_num': 114, 'train_rates': 0.8166863710044014, 'learning_rate': 5.519402408589884e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.874044884633893}. Best is trial 19 with value: 0.08359762501937372.[0m
[32m[I 2025-01-06 11:00:55,765][0m Trial 21 finished with value: 0.09620864234574429 and parameters: {'observation_period_num': 161, 'train_rates': 0.9172742505135527, 'learning_rate': 5.403000309047408e-05, 'batch_size': 156, 'step_size': 9, 'gamma': 0.9385550253896807}. Best is trial 19 with value: 0.08359762501937372.[0m
[32m[I 2025-01-06 11:01:44,571][0m Trial 22 finished with value: 0.1367484310217071 and parameters: {'observation_period_num': 154, 'train_rates': 0.9390592773498114, 'learning_rate': 2.277885464969318e-05, 'batch_size': 155, 'step_size': 8, 'gamma': 0.933037323484683}. Best is trial 19 with value: 0.08359762501937372.[0m
[32m[I 2025-01-06 11:02:23,051][0m Trial 23 finished with value: 0.07427860423922539 and parameters: {'observation_period_num': 91, 'train_rates': 0.8979445405917477, 'learning_rate': 8.032982926446536e-05, 'batch_size': 195, 'step_size': 9, 'gamma': 0.9197047234723752}. Best is trial 23 with value: 0.07427860423922539.[0m
[32m[I 2025-01-06 11:02:59,751][0m Trial 24 finished with value: 0.06723021931737383 and parameters: {'observation_period_num': 83, 'train_rates': 0.8884049104900297, 'learning_rate': 9.533569255930275e-05, 'batch_size': 207, 'step_size': 8, 'gamma': 0.9243868294595367}. Best is trial 24 with value: 0.06723021931737383.[0m
[32m[I 2025-01-06 11:03:26,162][0m Trial 25 finished with value: 0.18891253292299154 and parameters: {'observation_period_num': 84, 'train_rates': 0.6027992813780603, 'learning_rate': 9.242282310803837e-05, 'batch_size': 221, 'step_size': 6, 'gamma': 0.9617859615889839}. Best is trial 24 with value: 0.06723021931737383.[0m
[32m[I 2025-01-06 11:03:57,357][0m Trial 26 finished with value: 0.06537394060419416 and parameters: {'observation_period_num': 58, 'train_rates': 0.8901706359899306, 'learning_rate': 0.00030141524241213195, 'batch_size': 207, 'step_size': 7, 'gamma': 0.9232200109204275}. Best is trial 26 with value: 0.06537394060419416.[0m
[32m[I 2025-01-06 11:04:26,633][0m Trial 27 finished with value: 0.05559163252738389 and parameters: {'observation_period_num': 55, 'train_rates': 0.8255388134987316, 'learning_rate': 0.0002979921002137706, 'batch_size': 207, 'step_size': 7, 'gamma': 0.9157981599773731}. Best is trial 27 with value: 0.05559163252738389.[0m
[32m[I 2025-01-06 11:04:54,894][0m Trial 28 finished with value: 0.05541591530919899 and parameters: {'observation_period_num': 52, 'train_rates': 0.8005918152319755, 'learning_rate': 0.00030482221713798235, 'batch_size': 217, 'step_size': 6, 'gamma': 0.8915396534701955}. Best is trial 28 with value: 0.05541591530919899.[0m
[32m[I 2025-01-06 11:05:20,376][0m Trial 29 finished with value: 0.18262112554064858 and parameters: {'observation_period_num': 9, 'train_rates': 0.7801816099408199, 'learning_rate': 0.00030480614330388137, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8902477543738508}. Best is trial 28 with value: 0.05541591530919899.[0m
[32m[I 2025-01-06 11:05:49,194][0m Trial 30 finished with value: 0.04494977571781848 and parameters: {'observation_period_num': 38, 'train_rates': 0.8239969656583485, 'learning_rate': 0.0004541193866261227, 'batch_size': 232, 'step_size': 6, 'gamma': 0.891287698394165}. Best is trial 30 with value: 0.04494977571781848.[0m
[32m[I 2025-01-06 11:06:17,954][0m Trial 31 finished with value: 0.050231387096959935 and parameters: {'observation_period_num': 38, 'train_rates': 0.8062029135372345, 'learning_rate': 0.00045078261325205845, 'batch_size': 227, 'step_size': 6, 'gamma': 0.8895956707058206}. Best is trial 30 with value: 0.04494977571781848.[0m
[32m[I 2025-01-06 11:06:46,580][0m Trial 32 finished with value: 0.04137738530592221 and parameters: {'observation_period_num': 31, 'train_rates': 0.8178099810952403, 'learning_rate': 0.0005183100321686116, 'batch_size': 228, 'step_size': 6, 'gamma': 0.8916784293727028}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:07:13,242][0m Trial 33 finished with value: 0.04720291957455569 and parameters: {'observation_period_num': 25, 'train_rates': 0.7926534236405806, 'learning_rate': 0.0005619658398718498, 'batch_size': 234, 'step_size': 5, 'gamma': 0.8481483457519258}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:07:38,268][0m Trial 34 finished with value: 0.19095064883861823 and parameters: {'observation_period_num': 25, 'train_rates': 0.7520764615146267, 'learning_rate': 0.0005667245527874238, 'batch_size': 235, 'step_size': 3, 'gamma': 0.8409257213710414}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:08:03,266][0m Trial 35 finished with value: 0.18964697979963743 and parameters: {'observation_period_num': 30, 'train_rates': 0.7777923306358365, 'learning_rate': 0.0006379591429864737, 'batch_size': 241, 'step_size': 5, 'gamma': 0.824682122255525}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:08:33,778][0m Trial 36 finished with value: 0.16892575287482847 and parameters: {'observation_period_num': 22, 'train_rates': 0.7248543533725724, 'learning_rate': 0.0004665995891574633, 'batch_size': 235, 'step_size': 4, 'gamma': 0.8572987986971896}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:09:16,311][0m Trial 37 finished with value: 0.054157515844473474 and parameters: {'observation_period_num': 43, 'train_rates': 0.8348839882156335, 'learning_rate': 0.0001703902794242715, 'batch_size': 185, 'step_size': 5, 'gamma': 0.8790705696574511}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:09:56,488][0m Trial 38 finished with value: 0.054147664156526026 and parameters: {'observation_period_num': 14, 'train_rates': 0.7980444494097708, 'learning_rate': 0.00044947401456655266, 'batch_size': 246, 'step_size': 3, 'gamma': 0.8957335352492494}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:10:33,520][0m Trial 39 finished with value: 0.21122346580145254 and parameters: {'observation_period_num': 67, 'train_rates': 0.73469192427373, 'learning_rate': 0.0001969349064847875, 'batch_size': 222, 'step_size': 6, 'gamma': 0.8190975585320117}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:11:13,134][0m Trial 40 finished with value: 0.1860706342397815 and parameters: {'observation_period_num': 38, 'train_rates': 0.7651746597401009, 'learning_rate': 0.0009971447338037224, 'batch_size': 227, 'step_size': 6, 'gamma': 0.8414893489069211}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:11:52,620][0m Trial 41 finished with value: 0.04709605351090431 and parameters: {'observation_period_num': 8, 'train_rates': 0.7963029657892661, 'learning_rate': 0.00042623137683373084, 'batch_size': 249, 'step_size': 3, 'gamma': 0.9052582778130348}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:12:32,671][0m Trial 42 finished with value: 0.046791805929161726 and parameters: {'observation_period_num': 7, 'train_rates': 0.8036570513764928, 'learning_rate': 0.0006343750203202589, 'batch_size': 252, 'step_size': 2, 'gamma': 0.9075055862070421}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:13:12,481][0m Trial 43 finished with value: 0.0496149847293016 and parameters: {'observation_period_num': 7, 'train_rates': 0.790747977239247, 'learning_rate': 0.0006839204018430626, 'batch_size': 250, 'step_size': 2, 'gamma': 0.9005858937886532}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:13:51,663][0m Trial 44 finished with value: 0.11982827484607697 and parameters: {'observation_period_num': 20, 'train_rates': 0.8365209999248268, 'learning_rate': 0.00012572546050384521, 'batch_size': 256, 'step_size': 2, 'gamma': 0.7825224732321325}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:14:28,938][0m Trial 45 finished with value: 0.15677655913816801 and parameters: {'observation_period_num': 7, 'train_rates': 0.7043386580693132, 'learning_rate': 0.00037682025329747115, 'batch_size': 236, 'step_size': 4, 'gamma': 0.8802629169652948}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:15:07,915][0m Trial 46 finished with value: 0.2189399640658458 and parameters: {'observation_period_num': 69, 'train_rates': 0.7668584966965343, 'learning_rate': 0.0007989805668849473, 'batch_size': 214, 'step_size': 3, 'gamma': 0.8627531163795139}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:15:49,881][0m Trial 47 finished with value: 0.0634781076195012 and parameters: {'observation_period_num': 41, 'train_rates': 0.8671486070441291, 'learning_rate': 0.00022724281748328384, 'batch_size': 242, 'step_size': 5, 'gamma': 0.7982468826647396}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:16:28,884][0m Trial 48 finished with value: 0.15049330001533415 and parameters: {'observation_period_num': 17, 'train_rates': 0.6818753956432854, 'learning_rate': 0.0005564002196065968, 'batch_size': 195, 'step_size': 2, 'gamma': 0.9074500425769587}. Best is trial 32 with value: 0.04137738530592221.[0m
[32m[I 2025-01-06 11:17:53,383][0m Trial 49 finished with value: 0.06069491734318937 and parameters: {'observation_period_num': 31, 'train_rates': 0.847316715277088, 'learning_rate': 0.0003720589080107415, 'batch_size': 67, 'step_size': 4, 'gamma': 0.9629935757803536}. Best is trial 32 with value: 0.04137738530592221.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8255407073315638, 'learning_rate': 8.306320187563025e-05, 'batch_size': 31, 'step_size': 11, 'gamma': 0.9383324338888183}
Epoch 1/300, trend Loss: 0.2580 | 0.1171
Epoch 2/300, trend Loss: 0.1493 | 0.1004
Epoch 3/300, trend Loss: 0.1349 | 0.0847
Epoch 4/300, trend Loss: 0.1248 | 0.0747
Epoch 5/300, trend Loss: 0.1180 | 0.0687
Epoch 6/300, trend Loss: 0.1136 | 0.0656
Epoch 7/300, trend Loss: 0.1103 | 0.0643
Epoch 8/300, trend Loss: 0.1081 | 0.0639
Epoch 9/300, trend Loss: 0.1064 | 0.0648
Epoch 10/300, trend Loss: 0.1050 | 0.0671
Epoch 11/300, trend Loss: 0.1037 | 0.0706
Epoch 12/300, trend Loss: 0.1025 | 0.0768
Epoch 13/300, trend Loss: 0.1015 | 0.0809
Epoch 14/300, trend Loss: 0.1007 | 0.0833
Epoch 15/300, trend Loss: 0.1000 | 0.0824
Epoch 16/300, trend Loss: 0.0996 | 0.0781
Epoch 17/300, trend Loss: 0.0992 | 0.0723
Epoch 18/300, trend Loss: 0.0983 | 0.0619
Epoch 19/300, trend Loss: 0.0971 | 0.0567
Epoch 20/300, trend Loss: 0.0944 | 0.0544
Epoch 21/300, trend Loss: 0.0929 | 0.0528
Epoch 22/300, trend Loss: 0.0921 | 0.0516
Epoch 23/300, trend Loss: 0.0909 | 0.0486
Epoch 24/300, trend Loss: 0.0905 | 0.0482
Epoch 25/300, trend Loss: 0.0888 | 0.0471
Epoch 26/300, trend Loss: 0.0876 | 0.0459
Epoch 27/300, trend Loss: 0.0864 | 0.0446
Epoch 28/300, trend Loss: 0.0854 | 0.0433
Epoch 29/300, trend Loss: 0.0846 | 0.0423
Epoch 30/300, trend Loss: 0.0839 | 0.0416
Epoch 31/300, trend Loss: 0.0830 | 0.0412
Epoch 32/300, trend Loss: 0.0824 | 0.0409
Epoch 33/300, trend Loss: 0.0819 | 0.0407
Epoch 34/300, trend Loss: 0.0814 | 0.0404
Epoch 35/300, trend Loss: 0.0811 | 0.0401
Epoch 36/300, trend Loss: 0.0806 | 0.0398
Epoch 37/300, trend Loss: 0.0802 | 0.0396
Epoch 38/300, trend Loss: 0.0798 | 0.0394
Epoch 39/300, trend Loss: 0.0793 | 0.0392
Epoch 40/300, trend Loss: 0.0789 | 0.0387
Epoch 41/300, trend Loss: 0.0785 | 0.0386
Epoch 42/300, trend Loss: 0.0781 | 0.0385
Epoch 43/300, trend Loss: 0.0776 | 0.0386
Epoch 44/300, trend Loss: 0.0771 | 0.0387
Epoch 45/300, trend Loss: 0.0766 | 0.0383
Epoch 46/300, trend Loss: 0.0763 | 0.0382
Epoch 47/300, trend Loss: 0.0759 | 0.0382
Epoch 48/300, trend Loss: 0.0755 | 0.0381
Epoch 49/300, trend Loss: 0.0751 | 0.0379
Epoch 50/300, trend Loss: 0.0748 | 0.0376
Epoch 51/300, trend Loss: 0.0744 | 0.0368
Epoch 52/300, trend Loss: 0.0742 | 0.0362
Epoch 53/300, trend Loss: 0.0738 | 0.0357
Epoch 54/300, trend Loss: 0.0735 | 0.0354
Epoch 55/300, trend Loss: 0.0732 | 0.0352
Epoch 56/300, trend Loss: 0.0730 | 0.0349
Epoch 57/300, trend Loss: 0.0729 | 0.0347
Epoch 58/300, trend Loss: 0.0727 | 0.0347
Epoch 59/300, trend Loss: 0.0725 | 0.0348
Epoch 60/300, trend Loss: 0.0725 | 0.0353
Epoch 61/300, trend Loss: 0.0726 | 0.0365
Epoch 62/300, trend Loss: 0.0726 | 0.0378
Epoch 63/300, trend Loss: 0.0724 | 0.0373
Epoch 64/300, trend Loss: 0.0717 | 0.0367
Epoch 65/300, trend Loss: 0.0712 | 0.0362
Epoch 66/300, trend Loss: 0.0709 | 0.0357
Epoch 67/300, trend Loss: 0.0705 | 0.0358
Epoch 68/300, trend Loss: 0.0703 | 0.0351
Epoch 69/300, trend Loss: 0.0700 | 0.0346
Epoch 70/300, trend Loss: 0.0697 | 0.0343
Epoch 71/300, trend Loss: 0.0694 | 0.0340
Epoch 72/300, trend Loss: 0.0692 | 0.0339
Epoch 73/300, trend Loss: 0.0690 | 0.0341
Epoch 74/300, trend Loss: 0.0688 | 0.0337
Epoch 75/300, trend Loss: 0.0686 | 0.0334
Epoch 76/300, trend Loss: 0.0683 | 0.0332
Epoch 77/300, trend Loss: 0.0681 | 0.0331
Epoch 78/300, trend Loss: 0.0679 | 0.0331
Epoch 79/300, trend Loss: 0.0678 | 0.0328
Epoch 80/300, trend Loss: 0.0675 | 0.0326
Epoch 81/300, trend Loss: 0.0674 | 0.0324
Epoch 82/300, trend Loss: 0.0672 | 0.0323
Epoch 83/300, trend Loss: 0.0670 | 0.0323
Epoch 84/300, trend Loss: 0.0668 | 0.0323
Epoch 85/300, trend Loss: 0.0666 | 0.0321
Epoch 86/300, trend Loss: 0.0664 | 0.0319
Epoch 87/300, trend Loss: 0.0663 | 0.0319
Epoch 88/300, trend Loss: 0.0661 | 0.0318
Epoch 89/300, trend Loss: 0.0659 | 0.0318
Epoch 90/300, trend Loss: 0.0657 | 0.0316
Epoch 91/300, trend Loss: 0.0656 | 0.0315
Epoch 92/300, trend Loss: 0.0654 | 0.0314
Epoch 93/300, trend Loss: 0.0653 | 0.0314
Epoch 94/300, trend Loss: 0.0651 | 0.0314
Epoch 95/300, trend Loss: 0.0649 | 0.0314
Epoch 96/300, trend Loss: 0.0648 | 0.0312
Epoch 97/300, trend Loss: 0.0647 | 0.0311
Epoch 98/300, trend Loss: 0.0645 | 0.0311
Epoch 99/300, trend Loss: 0.0644 | 0.0310
Epoch 100/300, trend Loss: 0.0642 | 0.0308
Epoch 101/300, trend Loss: 0.0641 | 0.0305
Epoch 102/300, trend Loss: 0.0640 | 0.0304
Epoch 103/300, trend Loss: 0.0639 | 0.0303
Epoch 104/300, trend Loss: 0.0638 | 0.0302
Epoch 105/300, trend Loss: 0.0636 | 0.0301
Epoch 106/300, trend Loss: 0.0635 | 0.0300
Epoch 107/300, trend Loss: 0.0633 | 0.0298
Epoch 108/300, trend Loss: 0.0632 | 0.0298
Epoch 109/300, trend Loss: 0.0631 | 0.0297
Epoch 110/300, trend Loss: 0.0629 | 0.0297
Epoch 111/300, trend Loss: 0.0628 | 0.0297
Epoch 112/300, trend Loss: 0.0627 | 0.0295
Epoch 113/300, trend Loss: 0.0626 | 0.0296
Epoch 114/300, trend Loss: 0.0624 | 0.0295
Epoch 115/300, trend Loss: 0.0623 | 0.0296
Epoch 116/300, trend Loss: 0.0622 | 0.0296
Epoch 117/300, trend Loss: 0.0621 | 0.0296
Epoch 118/300, trend Loss: 0.0620 | 0.0295
Epoch 119/300, trend Loss: 0.0619 | 0.0295
Epoch 120/300, trend Loss: 0.0618 | 0.0295
Epoch 121/300, trend Loss: 0.0617 | 0.0295
Epoch 122/300, trend Loss: 0.0615 | 0.0295
Epoch 123/300, trend Loss: 0.0614 | 0.0295
Epoch 124/300, trend Loss: 0.0613 | 0.0295
Epoch 125/300, trend Loss: 0.0613 | 0.0295
Epoch 126/300, trend Loss: 0.0612 | 0.0295
Epoch 127/300, trend Loss: 0.0611 | 0.0295
Epoch 128/300, trend Loss: 0.0610 | 0.0295
Epoch 129/300, trend Loss: 0.0609 | 0.0294
Epoch 130/300, trend Loss: 0.0608 | 0.0295
Epoch 131/300, trend Loss: 0.0607 | 0.0295
Epoch 132/300, trend Loss: 0.0606 | 0.0295
Epoch 133/300, trend Loss: 0.0605 | 0.0294
Epoch 134/300, trend Loss: 0.0604 | 0.0294
Epoch 135/300, trend Loss: 0.0604 | 0.0294
Epoch 136/300, trend Loss: 0.0603 | 0.0293
Epoch 137/300, trend Loss: 0.0602 | 0.0293
Epoch 138/300, trend Loss: 0.0601 | 0.0293
Epoch 139/300, trend Loss: 0.0601 | 0.0293
Epoch 140/300, trend Loss: 0.0600 | 0.0292
Epoch 141/300, trend Loss: 0.0599 | 0.0292
Epoch 142/300, trend Loss: 0.0598 | 0.0291
Epoch 143/300, trend Loss: 0.0598 | 0.0291
Epoch 144/300, trend Loss: 0.0597 | 0.0291
Epoch 145/300, trend Loss: 0.0597 | 0.0289
Epoch 146/300, trend Loss: 0.0596 | 0.0289
Epoch 147/300, trend Loss: 0.0595 | 0.0289
Epoch 148/300, trend Loss: 0.0594 | 0.0289
Epoch 149/300, trend Loss: 0.0594 | 0.0289
Epoch 150/300, trend Loss: 0.0593 | 0.0289
Epoch 151/300, trend Loss: 0.0592 | 0.0288
Epoch 152/300, trend Loss: 0.0592 | 0.0288
Epoch 153/300, trend Loss: 0.0591 | 0.0288
Epoch 154/300, trend Loss: 0.0590 | 0.0288
Epoch 155/300, trend Loss: 0.0590 | 0.0289
Epoch 156/300, trend Loss: 0.0589 | 0.0288
Epoch 157/300, trend Loss: 0.0589 | 0.0288
Epoch 158/300, trend Loss: 0.0588 | 0.0288
Epoch 159/300, trend Loss: 0.0588 | 0.0289
Epoch 160/300, trend Loss: 0.0587 | 0.0289
Epoch 161/300, trend Loss: 0.0587 | 0.0289
Epoch 162/300, trend Loss: 0.0586 | 0.0288
Epoch 163/300, trend Loss: 0.0586 | 0.0288
Epoch 164/300, trend Loss: 0.0585 | 0.0289
Epoch 165/300, trend Loss: 0.0584 | 0.0289
Epoch 166/300, trend Loss: 0.0584 | 0.0290
Epoch 167/300, trend Loss: 0.0583 | 0.0290
Epoch 168/300, trend Loss: 0.0583 | 0.0290
Epoch 169/300, trend Loss: 0.0582 | 0.0290
Epoch 170/300, trend Loss: 0.0582 | 0.0291
Epoch 171/300, trend Loss: 0.0581 | 0.0291
Epoch 172/300, trend Loss: 0.0581 | 0.0293
Epoch 173/300, trend Loss: 0.0581 | 0.0294
Epoch 174/300, trend Loss: 0.0580 | 0.0295
Epoch 175/300, trend Loss: 0.0580 | 0.0296
Epoch 176/300, trend Loss: 0.0580 | 0.0296
Epoch 177/300, trend Loss: 0.0579 | 0.0296
Epoch 178/300, trend Loss: 0.0580 | 0.0294
Epoch 179/300, trend Loss: 0.0579 | 0.0292
Epoch 180/300, trend Loss: 0.0579 | 0.0291
Epoch 181/300, trend Loss: 0.0579 | 0.0290
Epoch 182/300, trend Loss: 0.0580 | 0.0290
Epoch 183/300, trend Loss: 0.0581 | 0.0294
Epoch 184/300, trend Loss: 0.0583 | 0.0297
Epoch 185/300, trend Loss: 0.0582 | 0.0294
Epoch 186/300, trend Loss: 0.0580 | 0.0294
Epoch 187/300, trend Loss: 0.0579 | 0.0292
Epoch 188/300, trend Loss: 0.0578 | 0.0288
Epoch 189/300, trend Loss: 0.0577 | 0.0287
Epoch 190/300, trend Loss: 0.0576 | 0.0284
Epoch 191/300, trend Loss: 0.0576 | 0.0283
Epoch 192/300, trend Loss: 0.0575 | 0.0281
Epoch 193/300, trend Loss: 0.0575 | 0.0279
Epoch 194/300, trend Loss: 0.0574 | 0.0277
Epoch 195/300, trend Loss: 0.0574 | 0.0275
Epoch 196/300, trend Loss: 0.0574 | 0.0274
Epoch 197/300, trend Loss: 0.0573 | 0.0272
Epoch 198/300, trend Loss: 0.0573 | 0.0271
Epoch 199/300, trend Loss: 0.0573 | 0.0270
Epoch 200/300, trend Loss: 0.0572 | 0.0269
Epoch 201/300, trend Loss: 0.0572 | 0.0268
Epoch 202/300, trend Loss: 0.0572 | 0.0267
Epoch 203/300, trend Loss: 0.0571 | 0.0266
Epoch 204/300, trend Loss: 0.0571 | 0.0266
Epoch 205/300, trend Loss: 0.0571 | 0.0266
Epoch 206/300, trend Loss: 0.0570 | 0.0265
Epoch 207/300, trend Loss: 0.0570 | 0.0265
Epoch 208/300, trend Loss: 0.0569 | 0.0265
Epoch 209/300, trend Loss: 0.0569 | 0.0265
Epoch 210/300, trend Loss: 0.0569 | 0.0266
Epoch 211/300, trend Loss: 0.0568 | 0.0265
Epoch 212/300, trend Loss: 0.0568 | 0.0266
Epoch 213/300, trend Loss: 0.0567 | 0.0266
Epoch 214/300, trend Loss: 0.0567 | 0.0266
Epoch 215/300, trend Loss: 0.0567 | 0.0267
Epoch 216/300, trend Loss: 0.0566 | 0.0268
Epoch 217/300, trend Loss: 0.0566 | 0.0268
Epoch 218/300, trend Loss: 0.0566 | 0.0269
Epoch 219/300, trend Loss: 0.0565 | 0.0269
Epoch 220/300, trend Loss: 0.0565 | 0.0269
Epoch 221/300, trend Loss: 0.0565 | 0.0270
Epoch 222/300, trend Loss: 0.0565 | 0.0270
Epoch 223/300, trend Loss: 0.0564 | 0.0270
Epoch 224/300, trend Loss: 0.0564 | 0.0270
Epoch 225/300, trend Loss: 0.0564 | 0.0270
Epoch 226/300, trend Loss: 0.0564 | 0.0270
Epoch 227/300, trend Loss: 0.0563 | 0.0272
Epoch 228/300, trend Loss: 0.0563 | 0.0272
Epoch 229/300, trend Loss: 0.0563 | 0.0272
Epoch 230/300, trend Loss: 0.0563 | 0.0272
Epoch 231/300, trend Loss: 0.0562 | 0.0272
Epoch 232/300, trend Loss: 0.0562 | 0.0274
Epoch 233/300, trend Loss: 0.0562 | 0.0274
Epoch 234/300, trend Loss: 0.0562 | 0.0274
Epoch 235/300, trend Loss: 0.0561 | 0.0274
Epoch 236/300, trend Loss: 0.0561 | 0.0274
Epoch 237/300, trend Loss: 0.0561 | 0.0275
Epoch 238/300, trend Loss: 0.0561 | 0.0277
Epoch 239/300, trend Loss: 0.0560 | 0.0277
Epoch 240/300, trend Loss: 0.0560 | 0.0277
Epoch 241/300, trend Loss: 0.0560 | 0.0278
Epoch 242/300, trend Loss: 0.0560 | 0.0278
Epoch 243/300, trend Loss: 0.0559 | 0.0280
Epoch 244/300, trend Loss: 0.0559 | 0.0281
Epoch 245/300, trend Loss: 0.0559 | 0.0281
Epoch 246/300, trend Loss: 0.0559 | 0.0282
Epoch 247/300, trend Loss: 0.0558 | 0.0282
Epoch 248/300, trend Loss: 0.0558 | 0.0282
Epoch 249/300, trend Loss: 0.0558 | 0.0284
Epoch 250/300, trend Loss: 0.0558 | 0.0284
Epoch 251/300, trend Loss: 0.0558 | 0.0285
Epoch 252/300, trend Loss: 0.0557 | 0.0285
Epoch 253/300, trend Loss: 0.0557 | 0.0286
Epoch 254/300, trend Loss: 0.0557 | 0.0287
Epoch 255/300, trend Loss: 0.0557 | 0.0287
Epoch 256/300, trend Loss: 0.0556 | 0.0288
Epoch 257/300, trend Loss: 0.0556 | 0.0288
Epoch 258/300, trend Loss: 0.0556 | 0.0289
Epoch 259/300, trend Loss: 0.0556 | 0.0289
Epoch 260/300, trend Loss: 0.0556 | 0.0290
Epoch 261/300, trend Loss: 0.0555 | 0.0290
Epoch 262/300, trend Loss: 0.0555 | 0.0290
Epoch 263/300, trend Loss: 0.0555 | 0.0291
Epoch 264/300, trend Loss: 0.0555 | 0.0291
Epoch 265/300, trend Loss: 0.0555 | 0.0291
Epoch 266/300, trend Loss: 0.0554 | 0.0292
Epoch 267/300, trend Loss: 0.0554 | 0.0292
Epoch 268/300, trend Loss: 0.0554 | 0.0292
Epoch 269/300, trend Loss: 0.0554 | 0.0293
Epoch 270/300, trend Loss: 0.0554 | 0.0293
Epoch 271/300, trend Loss: 0.0553 | 0.0293
Epoch 272/300, trend Loss: 0.0553 | 0.0293
Epoch 273/300, trend Loss: 0.0553 | 0.0294
Epoch 274/300, trend Loss: 0.0553 | 0.0294
Epoch 275/300, trend Loss: 0.0553 | 0.0294
Epoch 276/300, trend Loss: 0.0552 | 0.0294
Epoch 277/300, trend Loss: 0.0552 | 0.0294
Epoch 278/300, trend Loss: 0.0552 | 0.0295
Epoch 279/300, trend Loss: 0.0552 | 0.0295
Epoch 280/300, trend Loss: 0.0552 | 0.0295
Epoch 281/300, trend Loss: 0.0551 | 0.0296
Epoch 282/300, trend Loss: 0.0551 | 0.0295
Epoch 283/300, trend Loss: 0.0551 | 0.0296
Epoch 284/300, trend Loss: 0.0551 | 0.0296
Epoch 285/300, trend Loss: 0.0551 | 0.0296
Epoch 286/300, trend Loss: 0.0550 | 0.0297
Epoch 287/300, trend Loss: 0.0550 | 0.0296
Epoch 288/300, trend Loss: 0.0550 | 0.0297
Epoch 289/300, trend Loss: 0.0550 | 0.0297
Epoch 290/300, trend Loss: 0.0550 | 0.0297
Epoch 291/300, trend Loss: 0.0550 | 0.0297
Epoch 292/300, trend Loss: 0.0549 | 0.0298
Epoch 293/300, trend Loss: 0.0549 | 0.0297
Epoch 294/300, trend Loss: 0.0549 | 0.0297
Epoch 295/300, trend Loss: 0.0549 | 0.0298
Epoch 296/300, trend Loss: 0.0549 | 0.0298
Epoch 297/300, trend Loss: 0.0549 | 0.0298
Epoch 298/300, trend Loss: 0.0548 | 0.0297
Epoch 299/300, trend Loss: 0.0548 | 0.0297
Epoch 300/300, trend Loss: 0.0548 | 0.0297
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.9297458819199611, 'learning_rate': 0.00023693882597308703, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9354409346431699}
Epoch 1/300, seasonal_0 Loss: 0.4424 | 0.1837
Epoch 2/300, seasonal_0 Loss: 0.1771 | 0.1625
Epoch 3/300, seasonal_0 Loss: 0.1512 | 0.1207
Epoch 4/300, seasonal_0 Loss: 0.1383 | 0.0942
Epoch 5/300, seasonal_0 Loss: 0.1308 | 0.0929
Epoch 6/300, seasonal_0 Loss: 0.1251 | 0.0889
Epoch 7/300, seasonal_0 Loss: 0.1209 | 0.0820
Epoch 8/300, seasonal_0 Loss: 0.1172 | 0.0825
Epoch 9/300, seasonal_0 Loss: 0.1140 | 0.0786
Epoch 10/300, seasonal_0 Loss: 0.1116 | 0.0760
Epoch 11/300, seasonal_0 Loss: 0.1095 | 0.0747
Epoch 12/300, seasonal_0 Loss: 0.1072 | 0.0728
Epoch 13/300, seasonal_0 Loss: 0.1050 | 0.0714
Epoch 14/300, seasonal_0 Loss: 0.1030 | 0.0697
Epoch 15/300, seasonal_0 Loss: 0.1008 | 0.0684
Epoch 16/300, seasonal_0 Loss: 0.0989 | 0.0671
Epoch 17/300, seasonal_0 Loss: 0.0974 | 0.0657
Epoch 18/300, seasonal_0 Loss: 0.0960 | 0.0651
Epoch 19/300, seasonal_0 Loss: 0.0949 | 0.0646
Epoch 20/300, seasonal_0 Loss: 0.0943 | 0.0645
Epoch 21/300, seasonal_0 Loss: 0.0940 | 0.0650
Epoch 22/300, seasonal_0 Loss: 0.0940 | 0.0648
Epoch 23/300, seasonal_0 Loss: 0.0953 | 0.0657
Epoch 24/300, seasonal_0 Loss: 0.0965 | 0.0634
Epoch 25/300, seasonal_0 Loss: 0.0985 | 0.0679
Epoch 26/300, seasonal_0 Loss: 0.0974 | 0.0640
Epoch 27/300, seasonal_0 Loss: 0.0895 | 0.0600
Epoch 28/300, seasonal_0 Loss: 0.0874 | 0.0608
Epoch 29/300, seasonal_0 Loss: 0.0876 | 0.0609
Epoch 30/300, seasonal_0 Loss: 0.0861 | 0.0589
Epoch 31/300, seasonal_0 Loss: 0.0853 | 0.0583
Epoch 32/300, seasonal_0 Loss: 0.0850 | 0.0574
Epoch 33/300, seasonal_0 Loss: 0.0845 | 0.0569
Epoch 34/300, seasonal_0 Loss: 0.0841 | 0.0565
Epoch 35/300, seasonal_0 Loss: 0.0836 | 0.0563
Epoch 36/300, seasonal_0 Loss: 0.0830 | 0.0563
Epoch 37/300, seasonal_0 Loss: 0.0825 | 0.0563
Epoch 38/300, seasonal_0 Loss: 0.0820 | 0.0560
Epoch 39/300, seasonal_0 Loss: 0.0816 | 0.0557
Epoch 40/300, seasonal_0 Loss: 0.0812 | 0.0553
Epoch 41/300, seasonal_0 Loss: 0.0808 | 0.0549
Epoch 42/300, seasonal_0 Loss: 0.0804 | 0.0544
Epoch 43/300, seasonal_0 Loss: 0.0799 | 0.0540
Epoch 44/300, seasonal_0 Loss: 0.0794 | 0.0538
Epoch 45/300, seasonal_0 Loss: 0.0788 | 0.0536
Epoch 46/300, seasonal_0 Loss: 0.0783 | 0.0534
Epoch 47/300, seasonal_0 Loss: 0.0777 | 0.0533
Epoch 48/300, seasonal_0 Loss: 0.0773 | 0.0532
Epoch 49/300, seasonal_0 Loss: 0.0769 | 0.0531
Epoch 50/300, seasonal_0 Loss: 0.0766 | 0.0529
Epoch 51/300, seasonal_0 Loss: 0.0763 | 0.0529
Epoch 52/300, seasonal_0 Loss: 0.0761 | 0.0528
Epoch 53/300, seasonal_0 Loss: 0.0759 | 0.0527
Epoch 54/300, seasonal_0 Loss: 0.0757 | 0.0526
Epoch 55/300, seasonal_0 Loss: 0.0755 | 0.0525
Epoch 56/300, seasonal_0 Loss: 0.0753 | 0.0523
Epoch 57/300, seasonal_0 Loss: 0.0750 | 0.0522
Epoch 58/300, seasonal_0 Loss: 0.0748 | 0.0521
Epoch 59/300, seasonal_0 Loss: 0.0746 | 0.0519
Epoch 60/300, seasonal_0 Loss: 0.0743 | 0.0517
Epoch 61/300, seasonal_0 Loss: 0.0741 | 0.0516
Epoch 62/300, seasonal_0 Loss: 0.0740 | 0.0514
Epoch 63/300, seasonal_0 Loss: 0.0738 | 0.0513
Epoch 64/300, seasonal_0 Loss: 0.0737 | 0.0512
Epoch 65/300, seasonal_0 Loss: 0.0735 | 0.0511
Epoch 66/300, seasonal_0 Loss: 0.0734 | 0.0511
Epoch 67/300, seasonal_0 Loss: 0.0733 | 0.0510
Epoch 68/300, seasonal_0 Loss: 0.0732 | 0.0510
Epoch 69/300, seasonal_0 Loss: 0.0731 | 0.0509
Epoch 70/300, seasonal_0 Loss: 0.0730 | 0.0509
Epoch 71/300, seasonal_0 Loss: 0.0730 | 0.0508
Epoch 72/300, seasonal_0 Loss: 0.0729 | 0.0508
Epoch 73/300, seasonal_0 Loss: 0.0728 | 0.0508
Epoch 74/300, seasonal_0 Loss: 0.0728 | 0.0507
Epoch 75/300, seasonal_0 Loss: 0.0727 | 0.0507
Epoch 76/300, seasonal_0 Loss: 0.0726 | 0.0507
Epoch 77/300, seasonal_0 Loss: 0.0725 | 0.0506
Epoch 78/300, seasonal_0 Loss: 0.0724 | 0.0506
Epoch 79/300, seasonal_0 Loss: 0.0723 | 0.0506
Epoch 80/300, seasonal_0 Loss: 0.0722 | 0.0505
Epoch 81/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 82/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 83/300, seasonal_0 Loss: 0.0718 | 0.0504
Epoch 84/300, seasonal_0 Loss: 0.0717 | 0.0503
Epoch 85/300, seasonal_0 Loss: 0.0716 | 0.0502
Epoch 86/300, seasonal_0 Loss: 0.0715 | 0.0502
Epoch 87/300, seasonal_0 Loss: 0.0714 | 0.0501
Epoch 88/300, seasonal_0 Loss: 0.0714 | 0.0500
Epoch 89/300, seasonal_0 Loss: 0.0713 | 0.0499
Epoch 90/300, seasonal_0 Loss: 0.0712 | 0.0499
Epoch 91/300, seasonal_0 Loss: 0.0712 | 0.0498
Epoch 92/300, seasonal_0 Loss: 0.0711 | 0.0498
Epoch 93/300, seasonal_0 Loss: 0.0710 | 0.0497
Epoch 94/300, seasonal_0 Loss: 0.0710 | 0.0497
Epoch 95/300, seasonal_0 Loss: 0.0709 | 0.0497
Epoch 96/300, seasonal_0 Loss: 0.0709 | 0.0497
Epoch 97/300, seasonal_0 Loss: 0.0708 | 0.0497
Epoch 98/300, seasonal_0 Loss: 0.0708 | 0.0497
Epoch 99/300, seasonal_0 Loss: 0.0707 | 0.0496
Epoch 100/300, seasonal_0 Loss: 0.0707 | 0.0496
Epoch 101/300, seasonal_0 Loss: 0.0706 | 0.0496
Epoch 102/300, seasonal_0 Loss: 0.0706 | 0.0496
Epoch 103/300, seasonal_0 Loss: 0.0706 | 0.0496
Epoch 104/300, seasonal_0 Loss: 0.0705 | 0.0495
Epoch 105/300, seasonal_0 Loss: 0.0705 | 0.0495
Epoch 106/300, seasonal_0 Loss: 0.0705 | 0.0495
Epoch 107/300, seasonal_0 Loss: 0.0704 | 0.0495
Epoch 108/300, seasonal_0 Loss: 0.0704 | 0.0495
Epoch 109/300, seasonal_0 Loss: 0.0704 | 0.0494
Epoch 110/300, seasonal_0 Loss: 0.0704 | 0.0494
Epoch 111/300, seasonal_0 Loss: 0.0703 | 0.0494
Epoch 112/300, seasonal_0 Loss: 0.0703 | 0.0494
Epoch 113/300, seasonal_0 Loss: 0.0703 | 0.0494
Epoch 114/300, seasonal_0 Loss: 0.0703 | 0.0494
Epoch 115/300, seasonal_0 Loss: 0.0702 | 0.0494
Epoch 116/300, seasonal_0 Loss: 0.0702 | 0.0493
Epoch 117/300, seasonal_0 Loss: 0.0702 | 0.0493
Epoch 118/300, seasonal_0 Loss: 0.0702 | 0.0493
Epoch 119/300, seasonal_0 Loss: 0.0702 | 0.0493
Epoch 120/300, seasonal_0 Loss: 0.0702 | 0.0493
Epoch 121/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 122/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 123/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 124/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 125/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 126/300, seasonal_0 Loss: 0.0701 | 0.0493
Epoch 127/300, seasonal_0 Loss: 0.0701 | 0.0492
Epoch 128/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 129/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 130/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 131/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 132/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 133/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 134/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 135/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 136/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 137/300, seasonal_0 Loss: 0.0700 | 0.0492
Epoch 138/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 139/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 140/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 141/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 142/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 143/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 144/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 145/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 146/300, seasonal_0 Loss: 0.0699 | 0.0492
Epoch 147/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 148/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 149/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 150/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 151/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 152/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 153/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 154/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 155/300, seasonal_0 Loss: 0.0699 | 0.0491
Epoch 156/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 157/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 158/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 159/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 160/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 161/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 162/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 163/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 164/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 165/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 166/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 167/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 168/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 169/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 170/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 171/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 172/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 173/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 174/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 175/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 176/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 177/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 178/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 179/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 180/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 181/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 182/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 183/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 184/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 185/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 186/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 187/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 188/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 189/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 190/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 191/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 192/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 193/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 194/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 195/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 196/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 197/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 198/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 199/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 200/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 201/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 202/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 203/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 204/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 205/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 206/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 207/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 208/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 209/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 210/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 211/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 212/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 213/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 214/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 215/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 216/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 217/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 218/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 219/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 220/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 221/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 222/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 223/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 224/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 225/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 226/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 227/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 228/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 229/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 230/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 231/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 232/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 233/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 234/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 235/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 236/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 237/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 238/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 239/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 240/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 241/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 242/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 243/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 244/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 245/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 246/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 247/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 248/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 249/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 250/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 251/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 252/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 253/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 254/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 255/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 256/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 257/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 258/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 259/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 260/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 261/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 262/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 263/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 264/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 265/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 266/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 267/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 268/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 269/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 270/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 271/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 272/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 273/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 274/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 275/300, seasonal_0 Loss: 0.0698 | 0.0491
Epoch 276/300, seasonal_0 Loss: 0.0698 | 0.0491
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8853595916349418, 'learning_rate': 0.00041009427658064576, 'batch_size': 144, 'step_size': 11, 'gamma': 0.8673713334080981}
Epoch 1/300, seasonal_1 Loss: 0.7202 | 0.2548
Epoch 2/300, seasonal_1 Loss: 0.2652 | 0.1799
Epoch 3/300, seasonal_1 Loss: 0.1860 | 0.1453
Epoch 4/300, seasonal_1 Loss: 0.1896 | 0.1100
Epoch 5/300, seasonal_1 Loss: 0.1723 | 0.0969
Epoch 6/300, seasonal_1 Loss: 0.2082 | 0.0933
Epoch 7/300, seasonal_1 Loss: 0.2453 | 0.5301
Epoch 8/300, seasonal_1 Loss: 0.2146 | 0.1439
Epoch 9/300, seasonal_1 Loss: 0.1604 | 0.1144
Epoch 10/300, seasonal_1 Loss: 0.1800 | 0.1212
Epoch 11/300, seasonal_1 Loss: 0.1637 | 0.0962
Epoch 12/300, seasonal_1 Loss: 0.1425 | 0.0890
Epoch 13/300, seasonal_1 Loss: 0.1373 | 0.0809
Epoch 14/300, seasonal_1 Loss: 0.1228 | 0.0779
Epoch 15/300, seasonal_1 Loss: 0.1289 | 0.0837
Epoch 16/300, seasonal_1 Loss: 0.1377 | 0.0857
Epoch 17/300, seasonal_1 Loss: 0.1391 | 0.0894
Epoch 18/300, seasonal_1 Loss: 0.1325 | 0.0825
Epoch 19/300, seasonal_1 Loss: 0.1137 | 0.0704
Epoch 20/300, seasonal_1 Loss: 0.1118 | 0.0681
Epoch 21/300, seasonal_1 Loss: 0.1114 | 0.0700
Epoch 22/300, seasonal_1 Loss: 0.1187 | 0.1025
Epoch 23/300, seasonal_1 Loss: 0.1130 | 0.0682
Epoch 24/300, seasonal_1 Loss: 0.1091 | 0.0878
Epoch 25/300, seasonal_1 Loss: 0.1093 | 0.0909
Epoch 26/300, seasonal_1 Loss: 0.1052 | 0.0640
Epoch 27/300, seasonal_1 Loss: 0.1001 | 0.0607
Epoch 28/300, seasonal_1 Loss: 0.1016 | 0.0680
Epoch 29/300, seasonal_1 Loss: 0.1035 | 0.0718
Epoch 30/300, seasonal_1 Loss: 0.1019 | 0.0574
Epoch 31/300, seasonal_1 Loss: 0.0973 | 0.0594
Epoch 32/300, seasonal_1 Loss: 0.0953 | 0.0657
Epoch 33/300, seasonal_1 Loss: 0.0985 | 0.0753
Epoch 34/300, seasonal_1 Loss: 0.1007 | 0.0665
Epoch 35/300, seasonal_1 Loss: 0.0959 | 0.0533
Epoch 36/300, seasonal_1 Loss: 0.0928 | 0.0559
Epoch 37/300, seasonal_1 Loss: 0.0902 | 0.0529
Epoch 38/300, seasonal_1 Loss: 0.0892 | 0.0530
Epoch 39/300, seasonal_1 Loss: 0.0879 | 0.0502
Epoch 40/300, seasonal_1 Loss: 0.0878 | 0.0547
Epoch 41/300, seasonal_1 Loss: 0.0888 | 0.0679
Epoch 42/300, seasonal_1 Loss: 0.0888 | 0.0597
Epoch 43/300, seasonal_1 Loss: 0.0876 | 0.0492
Epoch 44/300, seasonal_1 Loss: 0.0863 | 0.0512
Epoch 45/300, seasonal_1 Loss: 0.0862 | 0.0491
Epoch 46/300, seasonal_1 Loss: 0.0857 | 0.0488
Epoch 47/300, seasonal_1 Loss: 0.0859 | 0.0545
Epoch 48/300, seasonal_1 Loss: 0.0856 | 0.0521
Epoch 49/300, seasonal_1 Loss: 0.0853 | 0.0486
Epoch 50/300, seasonal_1 Loss: 0.0859 | 0.0489
Epoch 51/300, seasonal_1 Loss: 0.0867 | 0.0493
Epoch 52/300, seasonal_1 Loss: 0.0869 | 0.0519
Epoch 53/300, seasonal_1 Loss: 0.0890 | 0.0584
Epoch 54/300, seasonal_1 Loss: 0.0888 | 0.0524
Epoch 55/300, seasonal_1 Loss: 0.0863 | 0.0469
Epoch 56/300, seasonal_1 Loss: 0.0835 | 0.0475
Epoch 57/300, seasonal_1 Loss: 0.0837 | 0.0485
Epoch 58/300, seasonal_1 Loss: 0.0840 | 0.0490
Epoch 59/300, seasonal_1 Loss: 0.0829 | 0.0481
Epoch 60/300, seasonal_1 Loss: 0.0815 | 0.0481
Epoch 61/300, seasonal_1 Loss: 0.0805 | 0.0475
Epoch 62/300, seasonal_1 Loss: 0.0801 | 0.0458
Epoch 63/300, seasonal_1 Loss: 0.0796 | 0.0450
Epoch 64/300, seasonal_1 Loss: 0.0794 | 0.0453
Epoch 65/300, seasonal_1 Loss: 0.0792 | 0.0453
Epoch 66/300, seasonal_1 Loss: 0.0789 | 0.0450
Epoch 67/300, seasonal_1 Loss: 0.0787 | 0.0452
Epoch 68/300, seasonal_1 Loss: 0.0785 | 0.0451
Epoch 69/300, seasonal_1 Loss: 0.0781 | 0.0446
Epoch 70/300, seasonal_1 Loss: 0.0779 | 0.0446
Epoch 71/300, seasonal_1 Loss: 0.0777 | 0.0445
Epoch 72/300, seasonal_1 Loss: 0.0775 | 0.0443
Epoch 73/300, seasonal_1 Loss: 0.0773 | 0.0439
Epoch 74/300, seasonal_1 Loss: 0.0772 | 0.0438
Epoch 75/300, seasonal_1 Loss: 0.0770 | 0.0435
Epoch 76/300, seasonal_1 Loss: 0.0768 | 0.0434
Epoch 77/300, seasonal_1 Loss: 0.0766 | 0.0433
Epoch 78/300, seasonal_1 Loss: 0.0764 | 0.0433
Epoch 79/300, seasonal_1 Loss: 0.0763 | 0.0431
Epoch 80/300, seasonal_1 Loss: 0.0762 | 0.0430
Epoch 81/300, seasonal_1 Loss: 0.0761 | 0.0433
Epoch 82/300, seasonal_1 Loss: 0.0760 | 0.0434
Epoch 83/300, seasonal_1 Loss: 0.0760 | 0.0435
Epoch 84/300, seasonal_1 Loss: 0.0760 | 0.0436
Epoch 85/300, seasonal_1 Loss: 0.0758 | 0.0436
Epoch 86/300, seasonal_1 Loss: 0.0755 | 0.0434
Epoch 87/300, seasonal_1 Loss: 0.0752 | 0.0430
Epoch 88/300, seasonal_1 Loss: 0.0750 | 0.0426
Epoch 89/300, seasonal_1 Loss: 0.0750 | 0.0425
Epoch 90/300, seasonal_1 Loss: 0.0750 | 0.0423
Epoch 91/300, seasonal_1 Loss: 0.0748 | 0.0420
Epoch 92/300, seasonal_1 Loss: 0.0746 | 0.0419
Epoch 93/300, seasonal_1 Loss: 0.0743 | 0.0419
Epoch 94/300, seasonal_1 Loss: 0.0741 | 0.0419
Epoch 95/300, seasonal_1 Loss: 0.0740 | 0.0419
Epoch 96/300, seasonal_1 Loss: 0.0739 | 0.0419
Epoch 97/300, seasonal_1 Loss: 0.0738 | 0.0419
Epoch 98/300, seasonal_1 Loss: 0.0736 | 0.0417
Epoch 99/300, seasonal_1 Loss: 0.0735 | 0.0416
Epoch 100/300, seasonal_1 Loss: 0.0734 | 0.0414
Epoch 101/300, seasonal_1 Loss: 0.0732 | 0.0413
Epoch 102/300, seasonal_1 Loss: 0.0731 | 0.0413
Epoch 103/300, seasonal_1 Loss: 0.0730 | 0.0411
Epoch 104/300, seasonal_1 Loss: 0.0729 | 0.0411
Epoch 105/300, seasonal_1 Loss: 0.0728 | 0.0410
Epoch 106/300, seasonal_1 Loss: 0.0727 | 0.0409
Epoch 107/300, seasonal_1 Loss: 0.0725 | 0.0409
Epoch 108/300, seasonal_1 Loss: 0.0724 | 0.0408
Epoch 109/300, seasonal_1 Loss: 0.0723 | 0.0407
Epoch 110/300, seasonal_1 Loss: 0.0722 | 0.0406
Epoch 111/300, seasonal_1 Loss: 0.0721 | 0.0405
Epoch 112/300, seasonal_1 Loss: 0.0720 | 0.0405
Epoch 113/300, seasonal_1 Loss: 0.0719 | 0.0404
Epoch 114/300, seasonal_1 Loss: 0.0718 | 0.0403
Epoch 115/300, seasonal_1 Loss: 0.0717 | 0.0402
Epoch 116/300, seasonal_1 Loss: 0.0716 | 0.0401
Epoch 117/300, seasonal_1 Loss: 0.0714 | 0.0400
Epoch 118/300, seasonal_1 Loss: 0.0713 | 0.0400
Epoch 119/300, seasonal_1 Loss: 0.0712 | 0.0399
Epoch 120/300, seasonal_1 Loss: 0.0711 | 0.0398
Epoch 121/300, seasonal_1 Loss: 0.0710 | 0.0397
Epoch 122/300, seasonal_1 Loss: 0.0709 | 0.0396
Epoch 123/300, seasonal_1 Loss: 0.0708 | 0.0396
Epoch 124/300, seasonal_1 Loss: 0.0707 | 0.0395
Epoch 125/300, seasonal_1 Loss: 0.0706 | 0.0394
Epoch 126/300, seasonal_1 Loss: 0.0705 | 0.0393
Epoch 127/300, seasonal_1 Loss: 0.0704 | 0.0393
Epoch 128/300, seasonal_1 Loss: 0.0703 | 0.0392
Epoch 129/300, seasonal_1 Loss: 0.0703 | 0.0391
Epoch 130/300, seasonal_1 Loss: 0.0702 | 0.0391
Epoch 131/300, seasonal_1 Loss: 0.0701 | 0.0390
Epoch 132/300, seasonal_1 Loss: 0.0700 | 0.0389
Epoch 133/300, seasonal_1 Loss: 0.0699 | 0.0388
Epoch 134/300, seasonal_1 Loss: 0.0698 | 0.0388
Epoch 135/300, seasonal_1 Loss: 0.0698 | 0.0387
Epoch 136/300, seasonal_1 Loss: 0.0697 | 0.0387
Epoch 137/300, seasonal_1 Loss: 0.0696 | 0.0386
Epoch 138/300, seasonal_1 Loss: 0.0695 | 0.0385
Epoch 139/300, seasonal_1 Loss: 0.0695 | 0.0385
Epoch 140/300, seasonal_1 Loss: 0.0694 | 0.0384
Epoch 141/300, seasonal_1 Loss: 0.0693 | 0.0384
Epoch 142/300, seasonal_1 Loss: 0.0693 | 0.0383
Epoch 143/300, seasonal_1 Loss: 0.0692 | 0.0383
Epoch 144/300, seasonal_1 Loss: 0.0691 | 0.0382
Epoch 145/300, seasonal_1 Loss: 0.0691 | 0.0382
Epoch 146/300, seasonal_1 Loss: 0.0690 | 0.0381
Epoch 147/300, seasonal_1 Loss: 0.0690 | 0.0381
Epoch 148/300, seasonal_1 Loss: 0.0689 | 0.0380
Epoch 149/300, seasonal_1 Loss: 0.0689 | 0.0380
Epoch 150/300, seasonal_1 Loss: 0.0688 | 0.0379
Epoch 151/300, seasonal_1 Loss: 0.0688 | 0.0379
Epoch 152/300, seasonal_1 Loss: 0.0687 | 0.0378
Epoch 153/300, seasonal_1 Loss: 0.0687 | 0.0378
Epoch 154/300, seasonal_1 Loss: 0.0686 | 0.0378
Epoch 155/300, seasonal_1 Loss: 0.0686 | 0.0377
Epoch 156/300, seasonal_1 Loss: 0.0685 | 0.0377
Epoch 157/300, seasonal_1 Loss: 0.0685 | 0.0377
Epoch 158/300, seasonal_1 Loss: 0.0685 | 0.0376
Epoch 159/300, seasonal_1 Loss: 0.0684 | 0.0376
Epoch 160/300, seasonal_1 Loss: 0.0684 | 0.0376
Epoch 161/300, seasonal_1 Loss: 0.0683 | 0.0375
Epoch 162/300, seasonal_1 Loss: 0.0683 | 0.0375
Epoch 163/300, seasonal_1 Loss: 0.0683 | 0.0375
Epoch 164/300, seasonal_1 Loss: 0.0682 | 0.0374
Epoch 165/300, seasonal_1 Loss: 0.0682 | 0.0374
Epoch 166/300, seasonal_1 Loss: 0.0682 | 0.0374
Epoch 167/300, seasonal_1 Loss: 0.0681 | 0.0374
Epoch 168/300, seasonal_1 Loss: 0.0681 | 0.0373
Epoch 169/300, seasonal_1 Loss: 0.0681 | 0.0373
Epoch 170/300, seasonal_1 Loss: 0.0681 | 0.0373
Epoch 171/300, seasonal_1 Loss: 0.0680 | 0.0373
Epoch 172/300, seasonal_1 Loss: 0.0680 | 0.0372
Epoch 173/300, seasonal_1 Loss: 0.0680 | 0.0372
Epoch 174/300, seasonal_1 Loss: 0.0679 | 0.0372
Epoch 175/300, seasonal_1 Loss: 0.0679 | 0.0372
Epoch 176/300, seasonal_1 Loss: 0.0679 | 0.0372
Epoch 177/300, seasonal_1 Loss: 0.0679 | 0.0371
Epoch 178/300, seasonal_1 Loss: 0.0679 | 0.0371
Epoch 179/300, seasonal_1 Loss: 0.0678 | 0.0371
Epoch 180/300, seasonal_1 Loss: 0.0678 | 0.0371
Epoch 181/300, seasonal_1 Loss: 0.0678 | 0.0371
Epoch 182/300, seasonal_1 Loss: 0.0678 | 0.0371
Epoch 183/300, seasonal_1 Loss: 0.0677 | 0.0370
Epoch 184/300, seasonal_1 Loss: 0.0677 | 0.0370
Epoch 185/300, seasonal_1 Loss: 0.0677 | 0.0370
Epoch 186/300, seasonal_1 Loss: 0.0677 | 0.0370
Epoch 187/300, seasonal_1 Loss: 0.0677 | 0.0370
Epoch 188/300, seasonal_1 Loss: 0.0676 | 0.0370
Epoch 189/300, seasonal_1 Loss: 0.0676 | 0.0369
Epoch 190/300, seasonal_1 Loss: 0.0676 | 0.0369
Epoch 191/300, seasonal_1 Loss: 0.0676 | 0.0369
Epoch 192/300, seasonal_1 Loss: 0.0676 | 0.0369
Epoch 193/300, seasonal_1 Loss: 0.0676 | 0.0369
Epoch 194/300, seasonal_1 Loss: 0.0675 | 0.0369
Epoch 195/300, seasonal_1 Loss: 0.0675 | 0.0369
Epoch 196/300, seasonal_1 Loss: 0.0675 | 0.0369
Epoch 197/300, seasonal_1 Loss: 0.0675 | 0.0368
Epoch 198/300, seasonal_1 Loss: 0.0675 | 0.0368
Epoch 199/300, seasonal_1 Loss: 0.0675 | 0.0368
Epoch 200/300, seasonal_1 Loss: 0.0675 | 0.0368
Epoch 201/300, seasonal_1 Loss: 0.0674 | 0.0368
Epoch 202/300, seasonal_1 Loss: 0.0674 | 0.0368
Epoch 203/300, seasonal_1 Loss: 0.0674 | 0.0368
Epoch 204/300, seasonal_1 Loss: 0.0674 | 0.0368
Epoch 205/300, seasonal_1 Loss: 0.0674 | 0.0368
Epoch 206/300, seasonal_1 Loss: 0.0674 | 0.0367
Epoch 207/300, seasonal_1 Loss: 0.0674 | 0.0367
Epoch 208/300, seasonal_1 Loss: 0.0674 | 0.0367
Epoch 209/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 210/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 211/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 212/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 213/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 214/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 215/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 216/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 217/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 218/300, seasonal_1 Loss: 0.0673 | 0.0367
Epoch 219/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 220/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 221/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 222/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 223/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 224/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 225/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 226/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 227/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 228/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 229/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 230/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 231/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 232/300, seasonal_1 Loss: 0.0671 | 0.0366
Epoch 233/300, seasonal_1 Loss: 0.0671 | 0.0366
Epoch 234/300, seasonal_1 Loss: 0.0671 | 0.0366
Epoch 235/300, seasonal_1 Loss: 0.0671 | 0.0366
Epoch 236/300, seasonal_1 Loss: 0.0671 | 0.0366
Epoch 237/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 238/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 239/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 240/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 241/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 242/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 243/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 244/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 245/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 246/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 247/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 248/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 249/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 250/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 251/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 252/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 253/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 254/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 255/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 256/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 257/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 258/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 259/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 260/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 261/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 262/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 263/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 264/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 265/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 266/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 267/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 268/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 269/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 270/300, seasonal_1 Loss: 0.0670 | 0.0365
Epoch 271/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 272/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 273/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 274/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 275/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 276/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 277/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 278/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 279/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 280/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 281/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 282/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 283/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 284/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 285/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 286/300, seasonal_1 Loss: 0.0670 | 0.0364
Epoch 287/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 288/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 289/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 290/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 291/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 292/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 293/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 294/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 295/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 296/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 297/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 298/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 299/300, seasonal_1 Loss: 0.0669 | 0.0364
Epoch 300/300, seasonal_1 Loss: 0.0669 | 0.0364
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.8181393396055044, 'learning_rate': 0.0003431980657717307, 'batch_size': 101, 'step_size': 15, 'gamma': 0.9134879429995789}
Epoch 1/300, seasonal_2 Loss: 0.5914 | 0.2174
Epoch 2/300, seasonal_2 Loss: 0.1874 | 0.1307
Epoch 3/300, seasonal_2 Loss: 0.1728 | 0.1085
Epoch 4/300, seasonal_2 Loss: 0.1584 | 0.0934
Epoch 5/300, seasonal_2 Loss: 0.1441 | 0.0814
Epoch 6/300, seasonal_2 Loss: 0.1381 | 0.0771
Epoch 7/300, seasonal_2 Loss: 0.1342 | 0.0754
Epoch 8/300, seasonal_2 Loss: 0.1295 | 0.0734
Epoch 9/300, seasonal_2 Loss: 0.1258 | 0.0756
Epoch 10/300, seasonal_2 Loss: 0.1236 | 0.0736
Epoch 11/300, seasonal_2 Loss: 0.1199 | 0.0705
Epoch 12/300, seasonal_2 Loss: 0.1167 | 0.0683
Epoch 13/300, seasonal_2 Loss: 0.1138 | 0.0665
Epoch 14/300, seasonal_2 Loss: 0.1113 | 0.0651
Epoch 15/300, seasonal_2 Loss: 0.1094 | 0.0638
Epoch 16/300, seasonal_2 Loss: 0.1071 | 0.0622
Epoch 17/300, seasonal_2 Loss: 0.1050 | 0.0595
Epoch 18/300, seasonal_2 Loss: 0.1003 | 0.0573
Epoch 19/300, seasonal_2 Loss: 0.0966 | 0.0550
Epoch 20/300, seasonal_2 Loss: 0.0957 | 0.0548
Epoch 21/300, seasonal_2 Loss: 0.0994 | 0.0705
Epoch 22/300, seasonal_2 Loss: 0.1171 | 0.0691
Epoch 23/300, seasonal_2 Loss: 0.1088 | 0.0682
Epoch 24/300, seasonal_2 Loss: 0.1165 | 0.0790
Epoch 25/300, seasonal_2 Loss: 0.1035 | 0.0535
Epoch 26/300, seasonal_2 Loss: 0.1009 | 0.0506
Epoch 27/300, seasonal_2 Loss: 0.1013 | 0.0480
Epoch 28/300, seasonal_2 Loss: 0.1007 | 0.0509
Epoch 29/300, seasonal_2 Loss: 0.0974 | 0.0485
Epoch 30/300, seasonal_2 Loss: 0.0902 | 0.0505
Epoch 31/300, seasonal_2 Loss: 0.0898 | 0.0564
Epoch 32/300, seasonal_2 Loss: 0.0892 | 0.0541
Epoch 33/300, seasonal_2 Loss: 0.0870 | 0.0515
Epoch 34/300, seasonal_2 Loss: 0.0857 | 0.0481
Epoch 35/300, seasonal_2 Loss: 0.0856 | 0.0472
Epoch 36/300, seasonal_2 Loss: 0.0853 | 0.0457
Epoch 37/300, seasonal_2 Loss: 0.0856 | 0.0453
Epoch 38/300, seasonal_2 Loss: 0.0864 | 0.0447
Epoch 39/300, seasonal_2 Loss: 0.0873 | 0.0448
Epoch 40/300, seasonal_2 Loss: 0.0878 | 0.0445
Epoch 41/300, seasonal_2 Loss: 0.0859 | 0.0422
Epoch 42/300, seasonal_2 Loss: 0.0824 | 0.0405
Epoch 43/300, seasonal_2 Loss: 0.0800 | 0.0395
Epoch 44/300, seasonal_2 Loss: 0.0790 | 0.0393
Epoch 45/300, seasonal_2 Loss: 0.0796 | 0.0399
Epoch 46/300, seasonal_2 Loss: 0.0793 | 0.0389
Epoch 47/300, seasonal_2 Loss: 0.0780 | 0.0387
Epoch 48/300, seasonal_2 Loss: 0.0775 | 0.0386
Epoch 49/300, seasonal_2 Loss: 0.0768 | 0.0383
Epoch 50/300, seasonal_2 Loss: 0.0764 | 0.0383
Epoch 51/300, seasonal_2 Loss: 0.0761 | 0.0376
Epoch 52/300, seasonal_2 Loss: 0.0758 | 0.0374
Epoch 53/300, seasonal_2 Loss: 0.0754 | 0.0367
Epoch 54/300, seasonal_2 Loss: 0.0751 | 0.0368
Epoch 55/300, seasonal_2 Loss: 0.0749 | 0.0364
Epoch 56/300, seasonal_2 Loss: 0.0748 | 0.0362
Epoch 57/300, seasonal_2 Loss: 0.0746 | 0.0360
Epoch 58/300, seasonal_2 Loss: 0.0744 | 0.0359
Epoch 59/300, seasonal_2 Loss: 0.0742 | 0.0357
Epoch 60/300, seasonal_2 Loss: 0.0740 | 0.0356
Epoch 61/300, seasonal_2 Loss: 0.0737 | 0.0357
Epoch 62/300, seasonal_2 Loss: 0.0735 | 0.0355
Epoch 63/300, seasonal_2 Loss: 0.0731 | 0.0352
Epoch 64/300, seasonal_2 Loss: 0.0732 | 0.0351
Epoch 65/300, seasonal_2 Loss: 0.0737 | 0.0352
Epoch 66/300, seasonal_2 Loss: 0.0746 | 0.0361
Epoch 67/300, seasonal_2 Loss: 0.0776 | 0.0386
Epoch 68/300, seasonal_2 Loss: 0.0817 | 0.0396
Epoch 69/300, seasonal_2 Loss: 0.0847 | 0.0419
Epoch 70/300, seasonal_2 Loss: 0.0866 | 0.0450
Epoch 71/300, seasonal_2 Loss: 0.0761 | 0.0376
Epoch 72/300, seasonal_2 Loss: 0.0747 | 0.0369
Epoch 73/300, seasonal_2 Loss: 0.0734 | 0.0351
Epoch 74/300, seasonal_2 Loss: 0.0737 | 0.0357
Epoch 75/300, seasonal_2 Loss: 0.0727 | 0.0349
Epoch 76/300, seasonal_2 Loss: 0.0729 | 0.0348
Epoch 77/300, seasonal_2 Loss: 0.0721 | 0.0341
Epoch 78/300, seasonal_2 Loss: 0.0713 | 0.0346
Epoch 79/300, seasonal_2 Loss: 0.0707 | 0.0339
Epoch 80/300, seasonal_2 Loss: 0.0709 | 0.0342
Epoch 81/300, seasonal_2 Loss: 0.0703 | 0.0335
Epoch 82/300, seasonal_2 Loss: 0.0703 | 0.0337
Epoch 83/300, seasonal_2 Loss: 0.0700 | 0.0329
Epoch 84/300, seasonal_2 Loss: 0.0700 | 0.0332
Epoch 85/300, seasonal_2 Loss: 0.0698 | 0.0327
Epoch 86/300, seasonal_2 Loss: 0.0698 | 0.0329
Epoch 87/300, seasonal_2 Loss: 0.0697 | 0.0326
Epoch 88/300, seasonal_2 Loss: 0.0698 | 0.0330
Epoch 89/300, seasonal_2 Loss: 0.0699 | 0.0329
Epoch 90/300, seasonal_2 Loss: 0.0701 | 0.0336
Epoch 91/300, seasonal_2 Loss: 0.0717 | 0.0361
Epoch 92/300, seasonal_2 Loss: 0.0722 | 0.0350
Epoch 93/300, seasonal_2 Loss: 0.0715 | 0.0351
Epoch 94/300, seasonal_2 Loss: 0.0731 | 0.0401
Epoch 95/300, seasonal_2 Loss: 0.0762 | 0.0390
Epoch 96/300, seasonal_2 Loss: 0.0727 | 0.0389
Epoch 97/300, seasonal_2 Loss: 0.0736 | 0.0392
Epoch 98/300, seasonal_2 Loss: 0.0723 | 0.0392
Epoch 99/300, seasonal_2 Loss: 0.0751 | 0.0413
Epoch 100/300, seasonal_2 Loss: 0.0713 | 0.0356
Epoch 101/300, seasonal_2 Loss: 0.0715 | 0.0351
Epoch 102/300, seasonal_2 Loss: 0.0718 | 0.0357
Epoch 103/300, seasonal_2 Loss: 0.0712 | 0.0341
Epoch 104/300, seasonal_2 Loss: 0.0716 | 0.0342
Epoch 105/300, seasonal_2 Loss: 0.0699 | 0.0346
Epoch 106/300, seasonal_2 Loss: 0.0708 | 0.0342
Epoch 107/300, seasonal_2 Loss: 0.0704 | 0.0367
Epoch 108/300, seasonal_2 Loss: 0.0717 | 0.0356
Epoch 109/300, seasonal_2 Loss: 0.0702 | 0.0354
Epoch 110/300, seasonal_2 Loss: 0.0691 | 0.0340
Epoch 111/300, seasonal_2 Loss: 0.0679 | 0.0334
Epoch 112/300, seasonal_2 Loss: 0.0678 | 0.0334
Epoch 113/300, seasonal_2 Loss: 0.0698 | 0.0360
Epoch 114/300, seasonal_2 Loss: 0.0687 | 0.0344
Epoch 115/300, seasonal_2 Loss: 0.0708 | 0.0361
Epoch 116/300, seasonal_2 Loss: 0.0707 | 0.0342
Epoch 117/300, seasonal_2 Loss: 0.0690 | 0.0335
Epoch 118/300, seasonal_2 Loss: 0.0697 | 0.0351
Epoch 119/300, seasonal_2 Loss: 0.0684 | 0.0333
Epoch 120/300, seasonal_2 Loss: 0.0678 | 0.0322
Epoch 121/300, seasonal_2 Loss: 0.0680 | 0.0339
Epoch 122/300, seasonal_2 Loss: 0.0713 | 0.0370
Epoch 123/300, seasonal_2 Loss: 0.0713 | 0.0351
Epoch 124/300, seasonal_2 Loss: 0.0687 | 0.0336
Epoch 125/300, seasonal_2 Loss: 0.0683 | 0.0328
Epoch 126/300, seasonal_2 Loss: 0.0679 | 0.0336
Epoch 127/300, seasonal_2 Loss: 0.0666 | 0.0327
Epoch 128/300, seasonal_2 Loss: 0.0665 | 0.0326
Epoch 129/300, seasonal_2 Loss: 0.0663 | 0.0327
Epoch 130/300, seasonal_2 Loss: 0.0664 | 0.0328
Epoch 131/300, seasonal_2 Loss: 0.0663 | 0.0327
Epoch 132/300, seasonal_2 Loss: 0.0662 | 0.0327
Epoch 133/300, seasonal_2 Loss: 0.0663 | 0.0327
Epoch 134/300, seasonal_2 Loss: 0.0662 | 0.0328
Epoch 135/300, seasonal_2 Loss: 0.0661 | 0.0328
Epoch 136/300, seasonal_2 Loss: 0.0660 | 0.0327
Epoch 137/300, seasonal_2 Loss: 0.0658 | 0.0326
Epoch 138/300, seasonal_2 Loss: 0.0653 | 0.0327
Epoch 139/300, seasonal_2 Loss: 0.0653 | 0.0328
Epoch 140/300, seasonal_2 Loss: 0.0652 | 0.0329
Epoch 141/300, seasonal_2 Loss: 0.0654 | 0.0329
Epoch 142/300, seasonal_2 Loss: 0.0655 | 0.0329
Epoch 143/300, seasonal_2 Loss: 0.0657 | 0.0327
Epoch 144/300, seasonal_2 Loss: 0.0660 | 0.0327
Epoch 145/300, seasonal_2 Loss: 0.0664 | 0.0327
Epoch 146/300, seasonal_2 Loss: 0.0665 | 0.0328
Epoch 147/300, seasonal_2 Loss: 0.0667 | 0.0331
Epoch 148/300, seasonal_2 Loss: 0.0669 | 0.0333
Epoch 149/300, seasonal_2 Loss: 0.0669 | 0.0334
Epoch 150/300, seasonal_2 Loss: 0.0666 | 0.0335
Epoch 151/300, seasonal_2 Loss: 0.0662 | 0.0337
Epoch 152/300, seasonal_2 Loss: 0.0657 | 0.0326
Epoch 153/300, seasonal_2 Loss: 0.0649 | 0.0316
Epoch 154/300, seasonal_2 Loss: 0.0648 | 0.0311
Epoch 155/300, seasonal_2 Loss: 0.0654 | 0.0311
Epoch 156/300, seasonal_2 Loss: 0.0662 | 0.0313
Epoch 157/300, seasonal_2 Loss: 0.0667 | 0.0314
Epoch 158/300, seasonal_2 Loss: 0.0667 | 0.0313
Epoch 159/300, seasonal_2 Loss: 0.0661 | 0.0312
Epoch 160/300, seasonal_2 Loss: 0.0650 | 0.0310
Epoch 161/300, seasonal_2 Loss: 0.0641 | 0.0314
Epoch 162/300, seasonal_2 Loss: 0.0640 | 0.0317
Epoch 163/300, seasonal_2 Loss: 0.0637 | 0.0319
Epoch 164/300, seasonal_2 Loss: 0.0634 | 0.0322
Epoch 165/300, seasonal_2 Loss: 0.0632 | 0.0324
Epoch 166/300, seasonal_2 Loss: 0.0629 | 0.0326
Epoch 167/300, seasonal_2 Loss: 0.0626 | 0.0321
Epoch 168/300, seasonal_2 Loss: 0.0622 | 0.0315
Epoch 169/300, seasonal_2 Loss: 0.0619 | 0.0310
Epoch 170/300, seasonal_2 Loss: 0.0617 | 0.0307
Epoch 171/300, seasonal_2 Loss: 0.0617 | 0.0305
Epoch 172/300, seasonal_2 Loss: 0.0616 | 0.0304
Epoch 173/300, seasonal_2 Loss: 0.0616 | 0.0302
Epoch 174/300, seasonal_2 Loss: 0.0615 | 0.0300
Epoch 175/300, seasonal_2 Loss: 0.0614 | 0.0300
Epoch 176/300, seasonal_2 Loss: 0.0613 | 0.0299
Epoch 177/300, seasonal_2 Loss: 0.0612 | 0.0298
Epoch 178/300, seasonal_2 Loss: 0.0611 | 0.0297
Epoch 179/300, seasonal_2 Loss: 0.0610 | 0.0296
Epoch 180/300, seasonal_2 Loss: 0.0609 | 0.0295
Epoch 181/300, seasonal_2 Loss: 0.0609 | 0.0293
Epoch 182/300, seasonal_2 Loss: 0.0608 | 0.0292
Epoch 183/300, seasonal_2 Loss: 0.0607 | 0.0291
Epoch 184/300, seasonal_2 Loss: 0.0606 | 0.0290
Epoch 185/300, seasonal_2 Loss: 0.0605 | 0.0289
Epoch 186/300, seasonal_2 Loss: 0.0604 | 0.0288
Epoch 187/300, seasonal_2 Loss: 0.0604 | 0.0287
Epoch 188/300, seasonal_2 Loss: 0.0603 | 0.0287
Epoch 189/300, seasonal_2 Loss: 0.0603 | 0.0288
Epoch 190/300, seasonal_2 Loss: 0.0603 | 0.0289
Epoch 191/300, seasonal_2 Loss: 0.0603 | 0.0293
Epoch 192/300, seasonal_2 Loss: 0.0605 | 0.0297
Epoch 193/300, seasonal_2 Loss: 0.0605 | 0.0294
Epoch 194/300, seasonal_2 Loss: 0.0603 | 0.0293
Epoch 195/300, seasonal_2 Loss: 0.0601 | 0.0291
Epoch 196/300, seasonal_2 Loss: 0.0600 | 0.0289
Epoch 197/300, seasonal_2 Loss: 0.0599 | 0.0288
Epoch 198/300, seasonal_2 Loss: 0.0598 | 0.0287
Epoch 199/300, seasonal_2 Loss: 0.0598 | 0.0286
Epoch 200/300, seasonal_2 Loss: 0.0597 | 0.0286
Epoch 201/300, seasonal_2 Loss: 0.0597 | 0.0286
Epoch 202/300, seasonal_2 Loss: 0.0596 | 0.0286
Epoch 203/300, seasonal_2 Loss: 0.0596 | 0.0286
Epoch 204/300, seasonal_2 Loss: 0.0595 | 0.0286
Epoch 205/300, seasonal_2 Loss: 0.0595 | 0.0287
Epoch 206/300, seasonal_2 Loss: 0.0594 | 0.0287
Epoch 207/300, seasonal_2 Loss: 0.0594 | 0.0288
Epoch 208/300, seasonal_2 Loss: 0.0593 | 0.0289
Epoch 209/300, seasonal_2 Loss: 0.0592 | 0.0289
Epoch 210/300, seasonal_2 Loss: 0.0592 | 0.0290
Epoch 211/300, seasonal_2 Loss: 0.0592 | 0.0291
Epoch 212/300, seasonal_2 Loss: 0.0591 | 0.0292
Epoch 213/300, seasonal_2 Loss: 0.0591 | 0.0292
Epoch 214/300, seasonal_2 Loss: 0.0591 | 0.0292
Epoch 215/300, seasonal_2 Loss: 0.0590 | 0.0292
Epoch 216/300, seasonal_2 Loss: 0.0590 | 0.0292
Epoch 217/300, seasonal_2 Loss: 0.0589 | 0.0291
Epoch 218/300, seasonal_2 Loss: 0.0589 | 0.0291
Epoch 219/300, seasonal_2 Loss: 0.0588 | 0.0289
Epoch 220/300, seasonal_2 Loss: 0.0588 | 0.0288
Epoch 221/300, seasonal_2 Loss: 0.0588 | 0.0288
Epoch 222/300, seasonal_2 Loss: 0.0588 | 0.0287
Epoch 223/300, seasonal_2 Loss: 0.0588 | 0.0287
Epoch 224/300, seasonal_2 Loss: 0.0587 | 0.0288
Epoch 225/300, seasonal_2 Loss: 0.0586 | 0.0288
Epoch 226/300, seasonal_2 Loss: 0.0586 | 0.0291
Epoch 227/300, seasonal_2 Loss: 0.0584 | 0.0291
Epoch 228/300, seasonal_2 Loss: 0.0583 | 0.0291
Epoch 229/300, seasonal_2 Loss: 0.0582 | 0.0291
Epoch 230/300, seasonal_2 Loss: 0.0581 | 0.0291
Epoch 231/300, seasonal_2 Loss: 0.0580 | 0.0292
Epoch 232/300, seasonal_2 Loss: 0.0579 | 0.0292
Epoch 233/300, seasonal_2 Loss: 0.0578 | 0.0293
Epoch 234/300, seasonal_2 Loss: 0.0578 | 0.0295
Epoch 235/300, seasonal_2 Loss: 0.0576 | 0.0294
Epoch 236/300, seasonal_2 Loss: 0.0576 | 0.0295
Epoch 237/300, seasonal_2 Loss: 0.0575 | 0.0295
Epoch 238/300, seasonal_2 Loss: 0.0573 | 0.0294
Epoch 239/300, seasonal_2 Loss: 0.0572 | 0.0294
Epoch 240/300, seasonal_2 Loss: 0.0571 | 0.0297
Epoch 241/300, seasonal_2 Loss: 0.0575 | 0.0320
Epoch 242/300, seasonal_2 Loss: 0.0589 | 0.0306
Epoch 243/300, seasonal_2 Loss: 0.0563 | 0.0320
Epoch 244/300, seasonal_2 Loss: 0.0590 | 0.0292
Epoch 245/300, seasonal_2 Loss: 0.0592 | 0.0311
Epoch 246/300, seasonal_2 Loss: 0.0591 | 0.0294
Epoch 247/300, seasonal_2 Loss: 0.0555 | 0.0299
Epoch 248/300, seasonal_2 Loss: 0.0543 | 0.0291
Epoch 249/300, seasonal_2 Loss: 0.0540 | 0.0292
Epoch 250/300, seasonal_2 Loss: 0.0537 | 0.0292
Epoch 251/300, seasonal_2 Loss: 0.0533 | 0.0290
Epoch 252/300, seasonal_2 Loss: 0.0529 | 0.0293
Epoch 253/300, seasonal_2 Loss: 0.0528 | 0.0291
Epoch 254/300, seasonal_2 Loss: 0.0528 | 0.0296
Epoch 255/300, seasonal_2 Loss: 0.0529 | 0.0291
Epoch 256/300, seasonal_2 Loss: 0.0539 | 0.0305
Epoch 257/300, seasonal_2 Loss: 0.0526 | 0.0292
Epoch 258/300, seasonal_2 Loss: 0.0525 | 0.0301
Epoch 259/300, seasonal_2 Loss: 0.0522 | 0.0292
Epoch 260/300, seasonal_2 Loss: 0.0524 | 0.0300
Epoch 261/300, seasonal_2 Loss: 0.0520 | 0.0292
Epoch 262/300, seasonal_2 Loss: 0.0521 | 0.0299
Epoch 263/300, seasonal_2 Loss: 0.0518 | 0.0292
Epoch 264/300, seasonal_2 Loss: 0.0518 | 0.0298
Epoch 265/300, seasonal_2 Loss: 0.0515 | 0.0292
Epoch 266/300, seasonal_2 Loss: 0.0515 | 0.0297
Epoch 267/300, seasonal_2 Loss: 0.0514 | 0.0292
Epoch 268/300, seasonal_2 Loss: 0.0515 | 0.0297
Epoch 269/300, seasonal_2 Loss: 0.0513 | 0.0292
Epoch 270/300, seasonal_2 Loss: 0.0514 | 0.0298
Epoch 271/300, seasonal_2 Loss: 0.0512 | 0.0293
Epoch 272/300, seasonal_2 Loss: 0.0511 | 0.0296
Epoch 273/300, seasonal_2 Loss: 0.0510 | 0.0292
Epoch 274/300, seasonal_2 Loss: 0.0510 | 0.0296
Epoch 275/300, seasonal_2 Loss: 0.0509 | 0.0292
Epoch 276/300, seasonal_2 Loss: 0.0510 | 0.0296
Epoch 277/300, seasonal_2 Loss: 0.0508 | 0.0292
Epoch 278/300, seasonal_2 Loss: 0.0510 | 0.0296
Epoch 279/300, seasonal_2 Loss: 0.0508 | 0.0293
Epoch 280/300, seasonal_2 Loss: 0.0508 | 0.0296
Epoch 281/300, seasonal_2 Loss: 0.0508 | 0.0293
Epoch 282/300, seasonal_2 Loss: 0.0508 | 0.0296
Epoch 283/300, seasonal_2 Loss: 0.0507 | 0.0293
Epoch 284/300, seasonal_2 Loss: 0.0508 | 0.0296
Epoch 285/300, seasonal_2 Loss: 0.0506 | 0.0293
Epoch 286/300, seasonal_2 Loss: 0.0506 | 0.0296
Epoch 287/300, seasonal_2 Loss: 0.0505 | 0.0293
Epoch 288/300, seasonal_2 Loss: 0.0505 | 0.0296
Epoch 289/300, seasonal_2 Loss: 0.0505 | 0.0293
Epoch 290/300, seasonal_2 Loss: 0.0505 | 0.0297
Epoch 291/300, seasonal_2 Loss: 0.0505 | 0.0293
Epoch 292/300, seasonal_2 Loss: 0.0505 | 0.0296
Epoch 293/300, seasonal_2 Loss: 0.0504 | 0.0293
Epoch 294/300, seasonal_2 Loss: 0.0504 | 0.0296
Epoch 295/300, seasonal_2 Loss: 0.0503 | 0.0293
Epoch 296/300, seasonal_2 Loss: 0.0502 | 0.0295
Epoch 297/300, seasonal_2 Loss: 0.0502 | 0.0293
Epoch 298/300, seasonal_2 Loss: 0.0502 | 0.0295
Epoch 299/300, seasonal_2 Loss: 0.0502 | 0.0293
Epoch 300/300, seasonal_2 Loss: 0.0502 | 0.0295
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.9155232530817599, 'learning_rate': 0.0003518255018147867, 'batch_size': 127, 'step_size': 12, 'gamma': 0.7504447729739783}
Epoch 1/300, seasonal_3 Loss: 0.3691 | 0.2312
Epoch 2/300, seasonal_3 Loss: 0.1612 | 0.1187
Epoch 3/300, seasonal_3 Loss: 0.1427 | 0.0847
Epoch 4/300, seasonal_3 Loss: 0.1348 | 0.0920
Epoch 5/300, seasonal_3 Loss: 0.1304 | 0.0909
Epoch 6/300, seasonal_3 Loss: 0.1233 | 0.0870
Epoch 7/300, seasonal_3 Loss: 0.1185 | 0.0856
Epoch 8/300, seasonal_3 Loss: 0.1184 | 0.0902
Epoch 9/300, seasonal_3 Loss: 0.1226 | 0.0817
Epoch 10/300, seasonal_3 Loss: 0.1334 | 0.0688
Epoch 11/300, seasonal_3 Loss: 0.1505 | 0.1249
Epoch 12/300, seasonal_3 Loss: 0.1537 | 0.1538
Epoch 13/300, seasonal_3 Loss: 0.1534 | 0.0745
Epoch 14/300, seasonal_3 Loss: 0.1337 | 0.0737
Epoch 15/300, seasonal_3 Loss: 0.1182 | 0.0709
Epoch 16/300, seasonal_3 Loss: 0.1088 | 0.0612
Epoch 17/300, seasonal_3 Loss: 0.1040 | 0.0611
Epoch 18/300, seasonal_3 Loss: 0.1036 | 0.0657
Epoch 19/300, seasonal_3 Loss: 0.1006 | 0.0571
Epoch 20/300, seasonal_3 Loss: 0.0978 | 0.0531
Epoch 21/300, seasonal_3 Loss: 0.0951 | 0.0518
Epoch 22/300, seasonal_3 Loss: 0.0928 | 0.0506
Epoch 23/300, seasonal_3 Loss: 0.0911 | 0.0497
Epoch 24/300, seasonal_3 Loss: 0.0902 | 0.0490
Epoch 25/300, seasonal_3 Loss: 0.0895 | 0.0479
Epoch 26/300, seasonal_3 Loss: 0.0887 | 0.0472
Epoch 27/300, seasonal_3 Loss: 0.0881 | 0.0466
Epoch 28/300, seasonal_3 Loss: 0.0878 | 0.0461
Epoch 29/300, seasonal_3 Loss: 0.0877 | 0.0457
Epoch 30/300, seasonal_3 Loss: 0.0876 | 0.0452
Epoch 31/300, seasonal_3 Loss: 0.0881 | 0.0455
Epoch 32/300, seasonal_3 Loss: 0.0896 | 0.0456
Epoch 33/300, seasonal_3 Loss: 0.0897 | 0.0457
Epoch 34/300, seasonal_3 Loss: 0.0884 | 0.0457
Epoch 35/300, seasonal_3 Loss: 0.0868 | 0.0450
Epoch 36/300, seasonal_3 Loss: 0.0859 | 0.0442
Epoch 37/300, seasonal_3 Loss: 0.0865 | 0.0448
Epoch 38/300, seasonal_3 Loss: 0.0899 | 0.0462
Epoch 39/300, seasonal_3 Loss: 0.0948 | 0.0476
Epoch 40/300, seasonal_3 Loss: 0.0947 | 0.0459
Epoch 41/300, seasonal_3 Loss: 0.0888 | 0.0433
Epoch 42/300, seasonal_3 Loss: 0.0846 | 0.0429
Epoch 43/300, seasonal_3 Loss: 0.0845 | 0.0436
Epoch 44/300, seasonal_3 Loss: 0.0840 | 0.0433
Epoch 45/300, seasonal_3 Loss: 0.0829 | 0.0425
Epoch 46/300, seasonal_3 Loss: 0.0826 | 0.0421
Epoch 47/300, seasonal_3 Loss: 0.0824 | 0.0419
Epoch 48/300, seasonal_3 Loss: 0.0823 | 0.0418
Epoch 49/300, seasonal_3 Loss: 0.0821 | 0.0416
Epoch 50/300, seasonal_3 Loss: 0.0819 | 0.0415
Epoch 51/300, seasonal_3 Loss: 0.0818 | 0.0413
Epoch 52/300, seasonal_3 Loss: 0.0817 | 0.0412
Epoch 53/300, seasonal_3 Loss: 0.0815 | 0.0411
Epoch 54/300, seasonal_3 Loss: 0.0814 | 0.0409
Epoch 55/300, seasonal_3 Loss: 0.0813 | 0.0408
Epoch 56/300, seasonal_3 Loss: 0.0812 | 0.0407
Epoch 57/300, seasonal_3 Loss: 0.0811 | 0.0406
Epoch 58/300, seasonal_3 Loss: 0.0810 | 0.0405
Epoch 59/300, seasonal_3 Loss: 0.0809 | 0.0404
Epoch 60/300, seasonal_3 Loss: 0.0808 | 0.0403
Epoch 61/300, seasonal_3 Loss: 0.0807 | 0.0402
Epoch 62/300, seasonal_3 Loss: 0.0806 | 0.0402
Epoch 63/300, seasonal_3 Loss: 0.0805 | 0.0401
Epoch 64/300, seasonal_3 Loss: 0.0805 | 0.0400
Epoch 65/300, seasonal_3 Loss: 0.0804 | 0.0400
Epoch 66/300, seasonal_3 Loss: 0.0803 | 0.0399
Epoch 67/300, seasonal_3 Loss: 0.0802 | 0.0398
Epoch 68/300, seasonal_3 Loss: 0.0802 | 0.0398
Epoch 69/300, seasonal_3 Loss: 0.0801 | 0.0397
Epoch 70/300, seasonal_3 Loss: 0.0800 | 0.0397
Epoch 71/300, seasonal_3 Loss: 0.0800 | 0.0396
Epoch 72/300, seasonal_3 Loss: 0.0799 | 0.0396
Epoch 73/300, seasonal_3 Loss: 0.0799 | 0.0395
Epoch 74/300, seasonal_3 Loss: 0.0798 | 0.0395
Epoch 75/300, seasonal_3 Loss: 0.0798 | 0.0394
Epoch 76/300, seasonal_3 Loss: 0.0797 | 0.0394
Epoch 77/300, seasonal_3 Loss: 0.0797 | 0.0394
Epoch 78/300, seasonal_3 Loss: 0.0796 | 0.0393
Epoch 79/300, seasonal_3 Loss: 0.0796 | 0.0393
Epoch 80/300, seasonal_3 Loss: 0.0795 | 0.0392
Epoch 81/300, seasonal_3 Loss: 0.0795 | 0.0392
Epoch 82/300, seasonal_3 Loss: 0.0794 | 0.0392
Epoch 83/300, seasonal_3 Loss: 0.0794 | 0.0392
Epoch 84/300, seasonal_3 Loss: 0.0794 | 0.0391
Epoch 85/300, seasonal_3 Loss: 0.0793 | 0.0391
Epoch 86/300, seasonal_3 Loss: 0.0793 | 0.0391
Epoch 87/300, seasonal_3 Loss: 0.0793 | 0.0391
Epoch 88/300, seasonal_3 Loss: 0.0792 | 0.0390
Epoch 89/300, seasonal_3 Loss: 0.0792 | 0.0390
Epoch 90/300, seasonal_3 Loss: 0.0792 | 0.0390
Epoch 91/300, seasonal_3 Loss: 0.0791 | 0.0390
Epoch 92/300, seasonal_3 Loss: 0.0791 | 0.0390
Epoch 93/300, seasonal_3 Loss: 0.0791 | 0.0389
Epoch 94/300, seasonal_3 Loss: 0.0791 | 0.0389
Epoch 95/300, seasonal_3 Loss: 0.0790 | 0.0389
Epoch 96/300, seasonal_3 Loss: 0.0790 | 0.0389
Epoch 97/300, seasonal_3 Loss: 0.0790 | 0.0389
Epoch 98/300, seasonal_3 Loss: 0.0790 | 0.0389
Epoch 99/300, seasonal_3 Loss: 0.0790 | 0.0389
Epoch 100/300, seasonal_3 Loss: 0.0789 | 0.0388
Epoch 101/300, seasonal_3 Loss: 0.0789 | 0.0388
Epoch 102/300, seasonal_3 Loss: 0.0789 | 0.0388
Epoch 103/300, seasonal_3 Loss: 0.0789 | 0.0388
Epoch 104/300, seasonal_3 Loss: 0.0789 | 0.0388
Epoch 105/300, seasonal_3 Loss: 0.0789 | 0.0388
Epoch 106/300, seasonal_3 Loss: 0.0788 | 0.0388
Epoch 107/300, seasonal_3 Loss: 0.0788 | 0.0388
Epoch 108/300, seasonal_3 Loss: 0.0788 | 0.0388
Epoch 109/300, seasonal_3 Loss: 0.0788 | 0.0388
Epoch 110/300, seasonal_3 Loss: 0.0788 | 0.0388
Epoch 111/300, seasonal_3 Loss: 0.0788 | 0.0387
Epoch 112/300, seasonal_3 Loss: 0.0788 | 0.0387
Epoch 113/300, seasonal_3 Loss: 0.0788 | 0.0387
Epoch 114/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 115/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 116/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 117/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 118/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 119/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 120/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 121/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 122/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 123/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 124/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 125/300, seasonal_3 Loss: 0.0787 | 0.0387
Epoch 126/300, seasonal_3 Loss: 0.0786 | 0.0387
Epoch 127/300, seasonal_3 Loss: 0.0786 | 0.0387
Epoch 128/300, seasonal_3 Loss: 0.0786 | 0.0387
Epoch 129/300, seasonal_3 Loss: 0.0786 | 0.0387
Epoch 130/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 131/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 132/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 133/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 134/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 135/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 136/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 137/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 138/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 139/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 140/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 141/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 142/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 143/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 144/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 145/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 146/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 147/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 148/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 149/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 150/300, seasonal_3 Loss: 0.0786 | 0.0386
Epoch 151/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 152/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 153/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 154/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 155/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 156/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 157/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 158/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 159/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 160/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 161/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 162/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 163/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 164/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 165/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 166/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 167/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 168/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 169/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 170/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 171/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 172/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 173/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 174/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 175/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 176/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 177/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 178/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 179/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 180/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 181/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 182/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 183/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 184/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 185/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 186/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 187/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 188/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 189/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 190/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 191/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 192/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 193/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 194/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 195/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 196/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 197/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 198/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 199/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 200/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 201/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 202/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 203/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 204/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 205/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 206/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 207/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 208/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 209/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 210/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 211/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 212/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 213/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 214/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 215/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 216/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 217/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 218/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 219/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 220/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 221/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 222/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 223/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 224/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 225/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 226/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 227/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 228/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 229/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 230/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 231/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 232/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 233/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 234/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 235/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 236/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 237/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 238/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 239/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 240/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 241/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 242/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 243/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 244/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 245/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 246/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 247/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 248/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 249/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 250/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 251/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 252/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 253/300, seasonal_3 Loss: 0.0785 | 0.0386
Epoch 254/300, seasonal_3 Loss: 0.0785 | 0.0386
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 31, 'train_rates': 0.8178099810952403, 'learning_rate': 0.0005183100321686116, 'batch_size': 228, 'step_size': 6, 'gamma': 0.8916784293727028}
Epoch 1/300, resid Loss: 0.5074 | 0.2678
Epoch 2/300, resid Loss: 0.3053 | 0.2445
Epoch 3/300, resid Loss: 0.3076 | 0.2642
Epoch 4/300, resid Loss: 0.2077 | 0.1708
Epoch 5/300, resid Loss: 0.1813 | 0.1661
Epoch 6/300, resid Loss: 0.2333 | 0.1670
Epoch 7/300, resid Loss: 0.1871 | 0.1918
Epoch 8/300, resid Loss: 0.2261 | 0.1378
Epoch 9/300, resid Loss: 0.2236 | 0.1553
Epoch 10/300, resid Loss: 0.2292 | 0.1546
Epoch 11/300, resid Loss: 0.2106 | 0.1711
Epoch 12/300, resid Loss: 0.2066 | 0.1208
Epoch 13/300, resid Loss: 0.1958 | 0.2026
Epoch 14/300, resid Loss: 0.1621 | 0.1092
Epoch 15/300, resid Loss: 0.1383 | 0.1357
Epoch 16/300, resid Loss: 0.1298 | 0.0911
Epoch 17/300, resid Loss: 0.1268 | 0.1178
Epoch 18/300, resid Loss: 0.1185 | 0.0825
Epoch 19/300, resid Loss: 0.1172 | 0.0907
Epoch 20/300, resid Loss: 0.1142 | 0.0763
Epoch 21/300, resid Loss: 0.1204 | 0.0989
Epoch 22/300, resid Loss: 0.1187 | 0.0740
Epoch 23/300, resid Loss: 0.1278 | 0.1416
Epoch 24/300, resid Loss: 0.1146 | 0.0778
Epoch 25/300, resid Loss: 0.1100 | 0.0876
Epoch 26/300, resid Loss: 0.1047 | 0.0713
Epoch 27/300, resid Loss: 0.1053 | 0.0686
Epoch 28/300, resid Loss: 0.1018 | 0.0632
Epoch 29/300, resid Loss: 0.1022 | 0.0687
Epoch 30/300, resid Loss: 0.1026 | 0.0609
Epoch 31/300, resid Loss: 0.1097 | 0.0985
Epoch 32/300, resid Loss: 0.1031 | 0.0632
Epoch 33/300, resid Loss: 0.1046 | 0.0866
Epoch 34/300, resid Loss: 0.0979 | 0.0626
Epoch 35/300, resid Loss: 0.0974 | 0.0659
Epoch 36/300, resid Loss: 0.0946 | 0.0588
Epoch 37/300, resid Loss: 0.0951 | 0.0604
Epoch 38/300, resid Loss: 0.0932 | 0.0565
Epoch 39/300, resid Loss: 0.0943 | 0.0616
Epoch 40/300, resid Loss: 0.0928 | 0.0553
Epoch 41/300, resid Loss: 0.0948 | 0.0666
Epoch 42/300, resid Loss: 0.0927 | 0.0548
Epoch 43/300, resid Loss: 0.0937 | 0.0669
Epoch 44/300, resid Loss: 0.0907 | 0.0550
Epoch 45/300, resid Loss: 0.0910 | 0.0592
Epoch 46/300, resid Loss: 0.0891 | 0.0541
Epoch 47/300, resid Loss: 0.0892 | 0.0556
Epoch 48/300, resid Loss: 0.0881 | 0.0531
Epoch 49/300, resid Loss: 0.0881 | 0.0543
Epoch 50/300, resid Loss: 0.0873 | 0.0524
Epoch 51/300, resid Loss: 0.0873 | 0.0535
Epoch 52/300, resid Loss: 0.0867 | 0.0520
Epoch 53/300, resid Loss: 0.0866 | 0.0529
Epoch 54/300, resid Loss: 0.0861 | 0.0517
Epoch 55/300, resid Loss: 0.0860 | 0.0524
Epoch 56/300, resid Loss: 0.0856 | 0.0515
Epoch 57/300, resid Loss: 0.0855 | 0.0520
Epoch 58/300, resid Loss: 0.0852 | 0.0514
Epoch 59/300, resid Loss: 0.0850 | 0.0516
Epoch 60/300, resid Loss: 0.0847 | 0.0512
Epoch 61/300, resid Loss: 0.0846 | 0.0514
Epoch 62/300, resid Loss: 0.0844 | 0.0511
Epoch 63/300, resid Loss: 0.0842 | 0.0511
Epoch 64/300, resid Loss: 0.0840 | 0.0510
Epoch 65/300, resid Loss: 0.0838 | 0.0510
Epoch 66/300, resid Loss: 0.0837 | 0.0508
Epoch 67/300, resid Loss: 0.0835 | 0.0508
Epoch 68/300, resid Loss: 0.0834 | 0.0507
Epoch 69/300, resid Loss: 0.0832 | 0.0507
Epoch 70/300, resid Loss: 0.0831 | 0.0506
Epoch 71/300, resid Loss: 0.0830 | 0.0506
Epoch 72/300, resid Loss: 0.0828 | 0.0505
Epoch 73/300, resid Loss: 0.0827 | 0.0505
Epoch 74/300, resid Loss: 0.0826 | 0.0504
Epoch 75/300, resid Loss: 0.0825 | 0.0504
Epoch 76/300, resid Loss: 0.0824 | 0.0503
Epoch 77/300, resid Loss: 0.0823 | 0.0503
Epoch 78/300, resid Loss: 0.0822 | 0.0502
Epoch 79/300, resid Loss: 0.0821 | 0.0502
Epoch 80/300, resid Loss: 0.0820 | 0.0502
Epoch 81/300, resid Loss: 0.0819 | 0.0501
Epoch 82/300, resid Loss: 0.0818 | 0.0501
Epoch 83/300, resid Loss: 0.0817 | 0.0501
Epoch 84/300, resid Loss: 0.0817 | 0.0500
Epoch 85/300, resid Loss: 0.0816 | 0.0500
Epoch 86/300, resid Loss: 0.0815 | 0.0500
Epoch 87/300, resid Loss: 0.0814 | 0.0499
Epoch 88/300, resid Loss: 0.0814 | 0.0499
Epoch 89/300, resid Loss: 0.0813 | 0.0499
Epoch 90/300, resid Loss: 0.0813 | 0.0498
Epoch 91/300, resid Loss: 0.0812 | 0.0498
Epoch 92/300, resid Loss: 0.0811 | 0.0498
Epoch 93/300, resid Loss: 0.0811 | 0.0498
Epoch 94/300, resid Loss: 0.0810 | 0.0498
Epoch 95/300, resid Loss: 0.0810 | 0.0497
Epoch 96/300, resid Loss: 0.0809 | 0.0497
Epoch 97/300, resid Loss: 0.0809 | 0.0497
Epoch 98/300, resid Loss: 0.0808 | 0.0497
Epoch 99/300, resid Loss: 0.0808 | 0.0497
Epoch 100/300, resid Loss: 0.0807 | 0.0496
Epoch 101/300, resid Loss: 0.0807 | 0.0496
Epoch 102/300, resid Loss: 0.0807 | 0.0496
Epoch 103/300, resid Loss: 0.0806 | 0.0496
Epoch 104/300, resid Loss: 0.0806 | 0.0496
Epoch 105/300, resid Loss: 0.0805 | 0.0496
Epoch 106/300, resid Loss: 0.0805 | 0.0495
Epoch 107/300, resid Loss: 0.0805 | 0.0495
Epoch 108/300, resid Loss: 0.0804 | 0.0495
Epoch 109/300, resid Loss: 0.0804 | 0.0495
Epoch 110/300, resid Loss: 0.0804 | 0.0495
Epoch 111/300, resid Loss: 0.0804 | 0.0495
Epoch 112/300, resid Loss: 0.0803 | 0.0495
Epoch 113/300, resid Loss: 0.0803 | 0.0495
Epoch 114/300, resid Loss: 0.0803 | 0.0494
Epoch 115/300, resid Loss: 0.0802 | 0.0494
Epoch 116/300, resid Loss: 0.0802 | 0.0494
Epoch 117/300, resid Loss: 0.0802 | 0.0494
Epoch 118/300, resid Loss: 0.0802 | 0.0494
Epoch 119/300, resid Loss: 0.0802 | 0.0494
Epoch 120/300, resid Loss: 0.0801 | 0.0494
Epoch 121/300, resid Loss: 0.0801 | 0.0494
Epoch 122/300, resid Loss: 0.0801 | 0.0494
Epoch 123/300, resid Loss: 0.0801 | 0.0494
Epoch 124/300, resid Loss: 0.0801 | 0.0494
Epoch 125/300, resid Loss: 0.0800 | 0.0493
Epoch 126/300, resid Loss: 0.0800 | 0.0493
Epoch 127/300, resid Loss: 0.0800 | 0.0493
Epoch 128/300, resid Loss: 0.0800 | 0.0493
Epoch 129/300, resid Loss: 0.0800 | 0.0493
Epoch 130/300, resid Loss: 0.0800 | 0.0493
Epoch 131/300, resid Loss: 0.0799 | 0.0493
Epoch 132/300, resid Loss: 0.0799 | 0.0493
Epoch 133/300, resid Loss: 0.0799 | 0.0493
Epoch 134/300, resid Loss: 0.0799 | 0.0493
Epoch 135/300, resid Loss: 0.0799 | 0.0493
Epoch 136/300, resid Loss: 0.0799 | 0.0493
Epoch 137/300, resid Loss: 0.0799 | 0.0493
Epoch 138/300, resid Loss: 0.0799 | 0.0493
Epoch 139/300, resid Loss: 0.0798 | 0.0493
Epoch 140/300, resid Loss: 0.0798 | 0.0493
Epoch 141/300, resid Loss: 0.0798 | 0.0493
Epoch 142/300, resid Loss: 0.0798 | 0.0493
Epoch 143/300, resid Loss: 0.0798 | 0.0492
Epoch 144/300, resid Loss: 0.0798 | 0.0492
Epoch 145/300, resid Loss: 0.0798 | 0.0492
Epoch 146/300, resid Loss: 0.0798 | 0.0492
Epoch 147/300, resid Loss: 0.0798 | 0.0492
Epoch 148/300, resid Loss: 0.0798 | 0.0492
Epoch 149/300, resid Loss: 0.0798 | 0.0492
Epoch 150/300, resid Loss: 0.0797 | 0.0492
Epoch 151/300, resid Loss: 0.0797 | 0.0492
Epoch 152/300, resid Loss: 0.0797 | 0.0492
Epoch 153/300, resid Loss: 0.0797 | 0.0492
Epoch 154/300, resid Loss: 0.0797 | 0.0492
Epoch 155/300, resid Loss: 0.0797 | 0.0492
Epoch 156/300, resid Loss: 0.0797 | 0.0492
Epoch 157/300, resid Loss: 0.0797 | 0.0492
Epoch 158/300, resid Loss: 0.0797 | 0.0492
Epoch 159/300, resid Loss: 0.0797 | 0.0492
Epoch 160/300, resid Loss: 0.0797 | 0.0492
Epoch 161/300, resid Loss: 0.0797 | 0.0492
Epoch 162/300, resid Loss: 0.0797 | 0.0492
Epoch 163/300, resid Loss: 0.0797 | 0.0492
Epoch 164/300, resid Loss: 0.0797 | 0.0492
Epoch 165/300, resid Loss: 0.0797 | 0.0492
Epoch 166/300, resid Loss: 0.0797 | 0.0492
Epoch 167/300, resid Loss: 0.0797 | 0.0492
Epoch 168/300, resid Loss: 0.0796 | 0.0492
Epoch 169/300, resid Loss: 0.0796 | 0.0492
Epoch 170/300, resid Loss: 0.0796 | 0.0492
Epoch 171/300, resid Loss: 0.0796 | 0.0492
Epoch 172/300, resid Loss: 0.0796 | 0.0492
Epoch 173/300, resid Loss: 0.0796 | 0.0492
Epoch 174/300, resid Loss: 0.0796 | 0.0492
Epoch 175/300, resid Loss: 0.0796 | 0.0492
Epoch 176/300, resid Loss: 0.0796 | 0.0492
Epoch 177/300, resid Loss: 0.0796 | 0.0492
Epoch 178/300, resid Loss: 0.0796 | 0.0492
Epoch 179/300, resid Loss: 0.0796 | 0.0492
Epoch 180/300, resid Loss: 0.0796 | 0.0492
Epoch 181/300, resid Loss: 0.0796 | 0.0492
Epoch 182/300, resid Loss: 0.0796 | 0.0492
Epoch 183/300, resid Loss: 0.0796 | 0.0492
Epoch 184/300, resid Loss: 0.0796 | 0.0492
Epoch 185/300, resid Loss: 0.0796 | 0.0492
Epoch 186/300, resid Loss: 0.0796 | 0.0492
Epoch 187/300, resid Loss: 0.0796 | 0.0492
Epoch 188/300, resid Loss: 0.0796 | 0.0492
Epoch 189/300, resid Loss: 0.0796 | 0.0492
Epoch 190/300, resid Loss: 0.0796 | 0.0492
Epoch 191/300, resid Loss: 0.0796 | 0.0492
Epoch 192/300, resid Loss: 0.0796 | 0.0492
Epoch 193/300, resid Loss: 0.0796 | 0.0492
Epoch 194/300, resid Loss: 0.0796 | 0.0492
Epoch 195/300, resid Loss: 0.0796 | 0.0492
Epoch 196/300, resid Loss: 0.0796 | 0.0492
Epoch 197/300, resid Loss: 0.0796 | 0.0492
Epoch 198/300, resid Loss: 0.0796 | 0.0491
Epoch 199/300, resid Loss: 0.0796 | 0.0491
Epoch 200/300, resid Loss: 0.0796 | 0.0491
Epoch 201/300, resid Loss: 0.0796 | 0.0491
Epoch 202/300, resid Loss: 0.0796 | 0.0491
Epoch 203/300, resid Loss: 0.0796 | 0.0491
Epoch 204/300, resid Loss: 0.0796 | 0.0491
Epoch 205/300, resid Loss: 0.0796 | 0.0491
Epoch 206/300, resid Loss: 0.0796 | 0.0491
Epoch 207/300, resid Loss: 0.0796 | 0.0491
Epoch 208/300, resid Loss: 0.0796 | 0.0491
Epoch 209/300, resid Loss: 0.0796 | 0.0491
Epoch 210/300, resid Loss: 0.0796 | 0.0491
Epoch 211/300, resid Loss: 0.0796 | 0.0491
Epoch 212/300, resid Loss: 0.0796 | 0.0491
Epoch 213/300, resid Loss: 0.0796 | 0.0491
Epoch 214/300, resid Loss: 0.0796 | 0.0491
Epoch 215/300, resid Loss: 0.0796 | 0.0491
Epoch 216/300, resid Loss: 0.0796 | 0.0491
Epoch 217/300, resid Loss: 0.0796 | 0.0491
Epoch 218/300, resid Loss: 0.0796 | 0.0491
Epoch 219/300, resid Loss: 0.0796 | 0.0491
Epoch 220/300, resid Loss: 0.0796 | 0.0491
Epoch 221/300, resid Loss: 0.0796 | 0.0491
Epoch 222/300, resid Loss: 0.0796 | 0.0491
Epoch 223/300, resid Loss: 0.0796 | 0.0491
Epoch 224/300, resid Loss: 0.0796 | 0.0491
Epoch 225/300, resid Loss: 0.0796 | 0.0491
Epoch 226/300, resid Loss: 0.0796 | 0.0491
Epoch 227/300, resid Loss: 0.0796 | 0.0491
Epoch 228/300, resid Loss: 0.0796 | 0.0491
Epoch 229/300, resid Loss: 0.0796 | 0.0491
Epoch 230/300, resid Loss: 0.0796 | 0.0491
Epoch 231/300, resid Loss: 0.0796 | 0.0491
Epoch 232/300, resid Loss: 0.0795 | 0.0491
Epoch 233/300, resid Loss: 0.0795 | 0.0491
Epoch 234/300, resid Loss: 0.0795 | 0.0491
Epoch 235/300, resid Loss: 0.0795 | 0.0491
Epoch 236/300, resid Loss: 0.0795 | 0.0491
Epoch 237/300, resid Loss: 0.0795 | 0.0491
Epoch 238/300, resid Loss: 0.0795 | 0.0491
Epoch 239/300, resid Loss: 0.0795 | 0.0491
Epoch 240/300, resid Loss: 0.0795 | 0.0491
Epoch 241/300, resid Loss: 0.0795 | 0.0491
Epoch 242/300, resid Loss: 0.0795 | 0.0491
Epoch 243/300, resid Loss: 0.0795 | 0.0491
Epoch 244/300, resid Loss: 0.0795 | 0.0491
Epoch 245/300, resid Loss: 0.0795 | 0.0491
Epoch 246/300, resid Loss: 0.0795 | 0.0491
Epoch 247/300, resid Loss: 0.0795 | 0.0491
Epoch 248/300, resid Loss: 0.0795 | 0.0491
Epoch 249/300, resid Loss: 0.0795 | 0.0491
Epoch 250/300, resid Loss: 0.0795 | 0.0491
Epoch 251/300, resid Loss: 0.0795 | 0.0491
Epoch 252/300, resid Loss: 0.0795 | 0.0491
Epoch 253/300, resid Loss: 0.0795 | 0.0491
Epoch 254/300, resid Loss: 0.0795 | 0.0491
Epoch 255/300, resid Loss: 0.0795 | 0.0491
Epoch 256/300, resid Loss: 0.0795 | 0.0491
Epoch 257/300, resid Loss: 0.0795 | 0.0491
Epoch 258/300, resid Loss: 0.0795 | 0.0491
Epoch 259/300, resid Loss: 0.0795 | 0.0491
Epoch 260/300, resid Loss: 0.0795 | 0.0491
Epoch 261/300, resid Loss: 0.0795 | 0.0491
Epoch 262/300, resid Loss: 0.0795 | 0.0491
Epoch 263/300, resid Loss: 0.0795 | 0.0491
Epoch 264/300, resid Loss: 0.0795 | 0.0491
Epoch 265/300, resid Loss: 0.0795 | 0.0491
Epoch 266/300, resid Loss: 0.0795 | 0.0491
Epoch 267/300, resid Loss: 0.0795 | 0.0491
Epoch 268/300, resid Loss: 0.0795 | 0.0491
Epoch 269/300, resid Loss: 0.0795 | 0.0491
Epoch 270/300, resid Loss: 0.0795 | 0.0491
Epoch 271/300, resid Loss: 0.0795 | 0.0491
Epoch 272/300, resid Loss: 0.0795 | 0.0491
Epoch 273/300, resid Loss: 0.0795 | 0.0491
Epoch 274/300, resid Loss: 0.0795 | 0.0491
Epoch 275/300, resid Loss: 0.0795 | 0.0491
Epoch 276/300, resid Loss: 0.0795 | 0.0491
Epoch 277/300, resid Loss: 0.0795 | 0.0491
Epoch 278/300, resid Loss: 0.0795 | 0.0491
Epoch 279/300, resid Loss: 0.0795 | 0.0491
Epoch 280/300, resid Loss: 0.0795 | 0.0491
Epoch 281/300, resid Loss: 0.0795 | 0.0491
Epoch 282/300, resid Loss: 0.0795 | 0.0491
Epoch 283/300, resid Loss: 0.0795 | 0.0491
Epoch 284/300, resid Loss: 0.0795 | 0.0491
Epoch 285/300, resid Loss: 0.0795 | 0.0491
Epoch 286/300, resid Loss: 0.0795 | 0.0491
Epoch 287/300, resid Loss: 0.0795 | 0.0491
Epoch 288/300, resid Loss: 0.0795 | 0.0491
Epoch 289/300, resid Loss: 0.0795 | 0.0491
Epoch 290/300, resid Loss: 0.0795 | 0.0491
Epoch 291/300, resid Loss: 0.0795 | 0.0491
Epoch 292/300, resid Loss: 0.0795 | 0.0491
Epoch 293/300, resid Loss: 0.0795 | 0.0491
Epoch 294/300, resid Loss: 0.0795 | 0.0491
Epoch 295/300, resid Loss: 0.0795 | 0.0491
Epoch 296/300, resid Loss: 0.0795 | 0.0491
Epoch 297/300, resid Loss: 0.0795 | 0.0491
Epoch 298/300, resid Loss: 0.0795 | 0.0491
Epoch 299/300, resid Loss: 0.0795 | 0.0491
Epoch 300/300, resid Loss: 0.0795 | 0.0491
Runtime (seconds): 1439.1233360767365
8.306320187563025e-05
[182.26027]
[-1.3722682]
[-0.9967981]
[0.94688094]
[-1.8010948]
[8.131099]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 428.1551307591144
RMSE: 20.691909790039062
MAE: 20.691909790039062
R-squared: nan
[187.16809]
