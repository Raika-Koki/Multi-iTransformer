[32m[I 2025-02-03 22:09:03,270][0m A new study created in memory with name: no-name-476ffd7a-fa4a-4945-876e-c2455ef6d7bc[0m
[32m[I 2025-02-03 22:09:30,680][0m Trial 0 finished with value: 0.1968815708019602 and parameters: {'observation_period_num': 241, 'train_rates': 0.8597557806511928, 'learning_rate': 0.0009261722421609295, 'batch_size': 195, 'step_size': 4, 'gamma': 0.9729285755344745}. Best is trial 0 with value: 0.1968815708019602.[0m
[32m[I 2025-02-03 22:10:21,076][0m Trial 1 finished with value: 0.725646436214447 and parameters: {'observation_period_num': 185, 'train_rates': 0.9655518133877019, 'learning_rate': 1.4709061715147597e-06, 'batch_size': 119, 'step_size': 15, 'gamma': 0.7760620851189881}. Best is trial 0 with value: 0.1968815708019602.[0m
[32m[I 2025-02-03 22:10:59,058][0m Trial 2 finished with value: 0.08020754158496857 and parameters: {'observation_period_num': 52, 'train_rates': 0.9485622162527703, 'learning_rate': 0.00016610361072330574, 'batch_size': 168, 'step_size': 6, 'gamma': 0.7692382173890928}. Best is trial 2 with value: 0.08020754158496857.[0m
[32m[I 2025-02-03 22:11:27,061][0m Trial 3 finished with value: 0.07202731826949375 and parameters: {'observation_period_num': 53, 'train_rates': 0.8716476967241985, 'learning_rate': 0.0005467474842002398, 'batch_size': 227, 'step_size': 12, 'gamma': 0.9525775289547919}. Best is trial 3 with value: 0.07202731826949375.[0m
[32m[I 2025-02-03 22:11:53,923][0m Trial 4 finished with value: 0.05659162586049966 and parameters: {'observation_period_num': 9, 'train_rates': 0.7383214519018027, 'learning_rate': 0.00018515356599604958, 'batch_size': 211, 'step_size': 14, 'gamma': 0.8829043169794438}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:12:36,992][0m Trial 5 finished with value: 0.1631905707071966 and parameters: {'observation_period_num': 104, 'train_rates': 0.764881046472195, 'learning_rate': 0.00041428921376436697, 'batch_size': 127, 'step_size': 15, 'gamma': 0.9070611089970695}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:13:02,696][0m Trial 6 finished with value: 0.12044902890920639 and parameters: {'observation_period_num': 88, 'train_rates': 0.9650637103738527, 'learning_rate': 0.0003280520202536138, 'batch_size': 253, 'step_size': 5, 'gamma': 0.8155916276334624}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:13:42,077][0m Trial 7 finished with value: 0.09822851014671041 and parameters: {'observation_period_num': 95, 'train_rates': 0.8830988191265872, 'learning_rate': 9.814540381646783e-05, 'batch_size': 152, 'step_size': 11, 'gamma': 0.9446091874783836}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:14:04,358][0m Trial 8 finished with value: 0.6923784797841852 and parameters: {'observation_period_num': 192, 'train_rates': 0.8009144638815586, 'learning_rate': 4.0875512480077705e-06, 'batch_size': 254, 'step_size': 4, 'gamma': 0.9681762219190828}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:14:35,844][0m Trial 9 finished with value: 0.13201816380023956 and parameters: {'observation_period_num': 198, 'train_rates': 0.9665162728486344, 'learning_rate': 0.0009542586846798076, 'batch_size': 199, 'step_size': 12, 'gamma': 0.7742771527136357}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:15:58,035][0m Trial 10 finished with value: 0.15537648096380982 and parameters: {'observation_period_num': 9, 'train_rates': 0.6285599578439894, 'learning_rate': 3.2291029479474376e-05, 'batch_size': 55, 'step_size': 1, 'gamma': 0.8603468187235837}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:16:22,734][0m Trial 11 finished with value: 0.08122682402751549 and parameters: {'observation_period_num': 6, 'train_rates': 0.7091991241489222, 'learning_rate': 5.8566040311681064e-05, 'batch_size': 215, 'step_size': 11, 'gamma': 0.8921172099729562}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:16:47,691][0m Trial 12 finished with value: 0.19317068266868592 and parameters: {'observation_period_num': 44, 'train_rates': 0.6992668420093441, 'learning_rate': 1.1981560794102578e-05, 'batch_size': 224, 'step_size': 13, 'gamma': 0.9242576141187262}. Best is trial 4 with value: 0.05659162586049966.[0m
[32m[I 2025-02-03 22:17:50,466][0m Trial 13 finished with value: 0.05244884194223253 and parameters: {'observation_period_num': 48, 'train_rates': 0.8198246846380615, 'learning_rate': 0.00020897337070753405, 'batch_size': 87, 'step_size': 8, 'gamma': 0.8530503124174964}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:18:54,031][0m Trial 14 finished with value: 0.10009639102410232 and parameters: {'observation_period_num': 131, 'train_rates': 0.7887146244156474, 'learning_rate': 0.00017079632266057252, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8508947249455815}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:22:37,554][0m Trial 15 finished with value: 0.07426855828560575 and parameters: {'observation_period_num': 34, 'train_rates': 0.7326235529528775, 'learning_rate': 1.869418374292494e-05, 'batch_size': 22, 'step_size': 9, 'gamma': 0.8273228435545745}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:23:27,592][0m Trial 16 finished with value: 0.3483366186927705 and parameters: {'observation_period_num': 133, 'train_rates': 0.6601022907529752, 'learning_rate': 6.0336086299737125e-05, 'batch_size': 93, 'step_size': 8, 'gamma': 0.8774893244576616}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:24:00,112][0m Trial 17 finished with value: 0.07033374603373937 and parameters: {'observation_period_num': 63, 'train_rates': 0.8278246786685007, 'learning_rate': 0.000208942635959854, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8381744028978385}. Best is trial 13 with value: 0.05244884194223253.[0m
Early stopping at epoch 61
[32m[I 2025-02-03 22:24:36,582][0m Trial 18 finished with value: 0.28641167178236204 and parameters: {'observation_period_num': 27, 'train_rates': 0.752260194850397, 'learning_rate': 8.010233082019979e-05, 'batch_size': 91, 'step_size': 1, 'gamma': 0.8128562821179497}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:26:30,560][0m Trial 19 finished with value: 0.07986151906898466 and parameters: {'observation_period_num': 79, 'train_rates': 0.9180324076456228, 'learning_rate': 2.5181470256665583e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8798274392135982}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:27:19,804][0m Trial 20 finished with value: 0.35158961643992553 and parameters: {'observation_period_num': 153, 'train_rates': 0.8207100619283115, 'learning_rate': 8.487984866320154e-06, 'batch_size': 113, 'step_size': 10, 'gamma': 0.7977360144180038}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:27:51,962][0m Trial 21 finished with value: 0.06592406156757886 and parameters: {'observation_period_num': 72, 'train_rates': 0.8221254388498432, 'learning_rate': 0.00021324509634654542, 'batch_size': 175, 'step_size': 14, 'gamma': 0.85013502008807}. Best is trial 13 with value: 0.05244884194223253.[0m
[32m[I 2025-02-03 22:28:29,290][0m Trial 22 finished with value: 0.04868811204198347 and parameters: {'observation_period_num': 23, 'train_rates': 0.8288533823140923, 'learning_rate': 0.00023959453803427293, 'batch_size': 156, 'step_size': 14, 'gamma': 0.9085427906318667}. Best is trial 22 with value: 0.04868811204198347.[0m
[32m[I 2025-02-03 22:29:06,950][0m Trial 23 finished with value: 0.0564013442417164 and parameters: {'observation_period_num': 27, 'train_rates': 0.7714498758725512, 'learning_rate': 0.00012380378077072292, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9214488309185733}. Best is trial 22 with value: 0.04868811204198347.[0m
[32m[I 2025-02-03 22:29:51,291][0m Trial 24 finished with value: 0.04515911462486431 and parameters: {'observation_period_num': 27, 'train_rates': 0.9046721816524208, 'learning_rate': 0.00010640022697127226, 'batch_size': 134, 'step_size': 13, 'gamma': 0.9215316772877234}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:30:49,935][0m Trial 25 finished with value: 0.055323042266536504 and parameters: {'observation_period_num': 31, 'train_rates': 0.9127108770512466, 'learning_rate': 4.020816431331528e-05, 'batch_size': 102, 'step_size': 10, 'gamma': 0.9077842445124698}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:32:17,302][0m Trial 26 finished with value: 0.13438628338795486 and parameters: {'observation_period_num': 111, 'train_rates': 0.853451643329982, 'learning_rate': 0.00031189719352277614, 'batch_size': 62, 'step_size': 14, 'gamma': 0.9315039989679192}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:33:03,984][0m Trial 27 finished with value: 0.07986113364477793 and parameters: {'observation_period_num': 62, 'train_rates': 0.9144677861463998, 'learning_rate': 0.0005283954038961869, 'batch_size': 129, 'step_size': 12, 'gamma': 0.9025892548487946}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:33:47,680][0m Trial 28 finished with value: 0.06306047331203114 and parameters: {'observation_period_num': 22, 'train_rates': 0.8874665387629576, 'learning_rate': 9.416339444055594e-05, 'batch_size': 144, 'step_size': 7, 'gamma': 0.9495089872440926}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:34:21,850][0m Trial 29 finished with value: 0.24239482926877784 and parameters: {'observation_period_num': 216, 'train_rates': 0.8436031547718417, 'learning_rate': 0.0006828002418178356, 'batch_size': 157, 'step_size': 10, 'gamma': 0.9769651988380407}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:35:28,446][0m Trial 30 finished with value: 0.275448306430158 and parameters: {'observation_period_num': 243, 'train_rates': 0.7950761259363565, 'learning_rate': 0.0002807564521675291, 'batch_size': 74, 'step_size': 13, 'gamma': 0.8654133745577728}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:36:25,771][0m Trial 31 finished with value: 0.057413735625440956 and parameters: {'observation_period_num': 38, 'train_rates': 0.9166138904938623, 'learning_rate': 4.5275542363374436e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9062455097304489}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:37:17,499][0m Trial 32 finished with value: 0.05627538521333906 and parameters: {'observation_period_num': 20, 'train_rates': 0.8942524429444612, 'learning_rate': 4.1974324678615816e-05, 'batch_size': 114, 'step_size': 15, 'gamma': 0.9164862205073582}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:37:51,007][0m Trial 33 finished with value: 0.16817308962345123 and parameters: {'observation_period_num': 47, 'train_rates': 0.9396294181917294, 'learning_rate': 0.00011230501605450007, 'batch_size': 183, 'step_size': 11, 'gamma': 0.934117273524528}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:38:34,989][0m Trial 34 finished with value: 0.12783462506659488 and parameters: {'observation_period_num': 56, 'train_rates': 0.8540009854706568, 'learning_rate': 1.8705910571827068e-05, 'batch_size': 132, 'step_size': 9, 'gamma': 0.9006923893697734}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:39:34,807][0m Trial 35 finished with value: 0.5434136390686035 and parameters: {'observation_period_num': 72, 'train_rates': 0.9844122360481472, 'learning_rate': 1.24318248233674e-06, 'batch_size': 104, 'step_size': 6, 'gamma': 0.8919355734972674}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:42:42,518][0m Trial 36 finished with value: 0.13105030092928144 and parameters: {'observation_period_num': 161, 'train_rates': 0.8711098554805724, 'learning_rate': 0.0001459368679148989, 'batch_size': 28, 'step_size': 12, 'gamma': 0.9692395621828643}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:43:23,075][0m Trial 37 finished with value: 0.06643711766455232 and parameters: {'observation_period_num': 40, 'train_rates': 0.943996152705018, 'learning_rate': 7.290452113595368e-05, 'batch_size': 159, 'step_size': 15, 'gamma': 0.7528587261183995}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:44:34,154][0m Trial 38 finished with value: 0.16935315463761944 and parameters: {'observation_period_num': 14, 'train_rates': 0.8371057961167544, 'learning_rate': 2.6177934171900614e-06, 'batch_size': 77, 'step_size': 4, 'gamma': 0.9889991193385351}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:46:52,289][0m Trial 39 finished with value: 0.081825051973723 and parameters: {'observation_period_num': 90, 'train_rates': 0.9018865276983253, 'learning_rate': 0.0006360415958355912, 'batch_size': 40, 'step_size': 14, 'gamma': 0.9399916151185802}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:47:38,702][0m Trial 40 finished with value: 0.10065311705821181 and parameters: {'observation_period_num': 106, 'train_rates': 0.8718554282216444, 'learning_rate': 0.000436631369783988, 'batch_size': 121, 'step_size': 7, 'gamma': 0.9585706419173128}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:48:36,567][0m Trial 41 finished with value: 0.06342927993912446 and parameters: {'observation_period_num': 21, 'train_rates': 0.8898402512210867, 'learning_rate': 4.2254823776764046e-05, 'batch_size': 102, 'step_size': 14, 'gamma': 0.9155422820893456}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:49:20,399][0m Trial 42 finished with value: 0.056723807007074356 and parameters: {'observation_period_num': 30, 'train_rates': 0.9333249633897084, 'learning_rate': 3.325566986661979e-05, 'batch_size': 141, 'step_size': 15, 'gamma': 0.9144968571370389}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:50:11,748][0m Trial 43 finished with value: 0.049212186552938963 and parameters: {'observation_period_num': 6, 'train_rates': 0.9004166442308156, 'learning_rate': 4.9667297413842026e-05, 'batch_size': 119, 'step_size': 13, 'gamma': 0.8912017136913754}. Best is trial 24 with value: 0.04515911462486431.[0m
[32m[I 2025-02-03 22:51:19,098][0m Trial 44 finished with value: 0.03776497364176058 and parameters: {'observation_period_num': 7, 'train_rates': 0.9618942296901267, 'learning_rate': 0.00023028806353889722, 'batch_size': 91, 'step_size': 13, 'gamma': 0.8915860888982015}. Best is trial 44 with value: 0.03776497364176058.[0m
[32m[I 2025-02-03 22:52:48,849][0m Trial 45 finished with value: 0.037178278804939485 and parameters: {'observation_period_num': 5, 'train_rates': 0.963851421141704, 'learning_rate': 0.00024530194092709754, 'batch_size': 69, 'step_size': 13, 'gamma': 0.8767471935178416}. Best is trial 45 with value: 0.037178278804939485.[0m
[32m[I 2025-02-03 22:54:28,525][0m Trial 46 finished with value: 0.024763284251093864 and parameters: {'observation_period_num': 6, 'train_rates': 0.9891446066302977, 'learning_rate': 0.00027371815228882586, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8884906005021503}. Best is trial 46 with value: 0.024763284251093864.[0m
[32m[I 2025-02-03 22:56:01,231][0m Trial 47 finished with value: 0.029904300346970558 and parameters: {'observation_period_num': 5, 'train_rates': 0.9789502813103241, 'learning_rate': 0.00036493982242709273, 'batch_size': 67, 'step_size': 12, 'gamma': 0.8800816306046807}. Best is trial 46 with value: 0.024763284251093864.[0m
[32m[I 2025-02-03 22:58:37,952][0m Trial 48 finished with value: 0.049470830927876866 and parameters: {'observation_period_num': 12, 'train_rates': 0.9829143181715074, 'learning_rate': 0.0003466984262148043, 'batch_size': 39, 'step_size': 12, 'gamma': 0.87289865169444}. Best is trial 46 with value: 0.024763284251093864.[0m
[32m[I 2025-02-03 23:00:13,447][0m Trial 49 finished with value: 0.034294554511609586 and parameters: {'observation_period_num': 7, 'train_rates': 0.965143793852833, 'learning_rate': 0.00014760463344718887, 'batch_size': 64, 'step_size': 13, 'gamma': 0.8874647334656914}. Best is trial 46 with value: 0.024763284251093864.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2930 | 0.1481
Epoch 2/300, Loss: 0.1062 | 0.1083
Epoch 3/300, Loss: 0.0896 | 0.0929
Epoch 4/300, Loss: 0.0776 | 0.0874
Epoch 5/300, Loss: 0.0701 | 0.0842
Epoch 6/300, Loss: 0.0667 | 0.0828
Epoch 7/300, Loss: 0.0646 | 0.0814
Epoch 8/300, Loss: 0.0619 | 0.0791
Epoch 9/300, Loss: 0.0601 | 0.0774
Epoch 10/300, Loss: 0.0575 | 0.0751
Epoch 11/300, Loss: 0.0546 | 0.0714
Epoch 12/300, Loss: 0.0514 | 0.0662
Epoch 13/300, Loss: 0.0486 | 0.0607
Epoch 14/300, Loss: 0.0465 | 0.0616
Epoch 15/300, Loss: 0.0466 | 0.0632
Epoch 16/300, Loss: 0.0473 | 0.0651
Epoch 17/300, Loss: 0.0565 | 0.0938
Epoch 18/300, Loss: 0.0469 | 0.0570
Epoch 19/300, Loss: 0.0443 | 0.0546
Epoch 20/300, Loss: 0.0455 | 0.0557
Epoch 21/300, Loss: 0.0412 | 0.0514
Epoch 22/300, Loss: 0.0440 | 0.0564
Epoch 23/300, Loss: 0.0398 | 0.0501
Epoch 24/300, Loss: 0.0388 | 0.0498
Epoch 25/300, Loss: 0.0378 | 0.0465
Epoch 26/300, Loss: 0.0371 | 0.0463
Epoch 27/300, Loss: 0.0364 | 0.0443
Epoch 28/300, Loss: 0.0360 | 0.0449
Epoch 29/300, Loss: 0.0357 | 0.0439
Epoch 30/300, Loss: 0.0369 | 0.0454
Epoch 31/300, Loss: 0.0369 | 0.0467
Epoch 32/300, Loss: 0.0395 | 0.0458
Epoch 33/300, Loss: 0.0354 | 0.0423
Epoch 34/300, Loss: 0.0338 | 0.0416
Epoch 35/300, Loss: 0.0342 | 0.0428
Epoch 36/300, Loss: 0.0331 | 0.0390
Epoch 37/300, Loss: 0.0335 | 0.0403
Epoch 38/300, Loss: 0.0332 | 0.0388
Epoch 39/300, Loss: 0.0333 | 0.0389
Epoch 40/300, Loss: 0.0330 | 0.0375
Epoch 41/300, Loss: 0.0327 | 0.0374
Epoch 42/300, Loss: 0.0327 | 0.0372
Epoch 43/300, Loss: 0.0326 | 0.0364
Epoch 44/300, Loss: 0.0324 | 0.0360
Epoch 45/300, Loss: 0.0321 | 0.0354
Epoch 46/300, Loss: 0.0321 | 0.0351
Epoch 47/300, Loss: 0.0319 | 0.0347
Epoch 48/300, Loss: 0.0322 | 0.0345
Epoch 49/300, Loss: 0.0318 | 0.0340
Epoch 50/300, Loss: 0.0317 | 0.0333
Epoch 51/300, Loss: 0.0329 | 0.0349
Epoch 52/300, Loss: 0.0328 | 0.0335
Epoch 53/300, Loss: 0.0352 | 0.0386
Epoch 54/300, Loss: 0.0340 | 0.0350
Epoch 55/300, Loss: 0.0313 | 0.0333
Epoch 56/300, Loss: 0.0314 | 0.0295
Epoch 57/300, Loss: 0.0302 | 0.0296
Epoch 58/300, Loss: 0.0310 | 0.0276
Epoch 59/300, Loss: 0.0302 | 0.0281
Epoch 60/300, Loss: 0.0310 | 0.0284
Epoch 61/300, Loss: 0.0303 | 0.0275
Epoch 62/300, Loss: 0.0307 | 0.0261
Epoch 63/300, Loss: 0.0298 | 0.0253
Epoch 64/300, Loss: 0.0301 | 0.0247
Epoch 65/300, Loss: 0.0296 | 0.0242
Epoch 66/300, Loss: 0.0296 | 0.0241
Epoch 67/300, Loss: 0.0300 | 0.0238
Epoch 68/300, Loss: 0.0298 | 0.0233
Epoch 69/300, Loss: 0.0296 | 0.0231
Epoch 70/300, Loss: 0.0292 | 0.0227
Epoch 71/300, Loss: 0.0287 | 0.0223
Epoch 72/300, Loss: 0.0281 | 0.0219
Epoch 73/300, Loss: 0.0278 | 0.0215
Epoch 74/300, Loss: 0.0272 | 0.0213
Epoch 75/300, Loss: 0.0263 | 0.0211
Epoch 76/300, Loss: 0.0258 | 0.0211
Epoch 77/300, Loss: 0.0255 | 0.0211
Epoch 78/300, Loss: 0.0253 | 0.0210
Epoch 79/300, Loss: 0.0251 | 0.0211
Epoch 80/300, Loss: 0.0248 | 0.0209
Epoch 81/300, Loss: 0.0244 | 0.0203
Epoch 82/300, Loss: 0.0241 | 0.0202
Epoch 83/300, Loss: 0.0239 | 0.0204
Epoch 84/300, Loss: 0.0237 | 0.0202
Epoch 85/300, Loss: 0.0236 | 0.0202
Epoch 86/300, Loss: 0.0236 | 0.0207
Epoch 87/300, Loss: 0.0236 | 0.0208
Epoch 88/300, Loss: 0.0234 | 0.0207
Epoch 89/300, Loss: 0.0233 | 0.0206
Epoch 90/300, Loss: 0.0232 | 0.0207
Epoch 91/300, Loss: 0.0230 | 0.0206
Epoch 92/300, Loss: 0.0231 | 0.0210
Epoch 93/300, Loss: 0.0231 | 0.0211
Epoch 94/300, Loss: 0.0229 | 0.0210
Epoch 95/300, Loss: 0.0228 | 0.0208
Epoch 96/300, Loss: 0.0227 | 0.0210
Epoch 97/300, Loss: 0.0226 | 0.0209
Epoch 98/300, Loss: 0.0226 | 0.0208
Epoch 99/300, Loss: 0.0226 | 0.0212
Epoch 100/300, Loss: 0.0226 | 0.0214
Epoch 101/300, Loss: 0.0225 | 0.0212
Epoch 102/300, Loss: 0.0225 | 0.0210
Epoch 103/300, Loss: 0.0225 | 0.0216
Epoch 104/300, Loss: 0.0223 | 0.0212
Epoch 105/300, Loss: 0.0223 | 0.0212
Epoch 106/300, Loss: 0.0224 | 0.0216
Epoch 107/300, Loss: 0.0224 | 0.0216
Epoch 108/300, Loss: 0.0224 | 0.0212
Epoch 109/300, Loss: 0.0225 | 0.0217
Epoch 110/300, Loss: 0.0224 | 0.0216
Epoch 111/300, Loss: 0.0223 | 0.0211
Epoch 112/300, Loss: 0.0225 | 0.0212
Epoch 113/300, Loss: 0.0225 | 0.0217
Epoch 114/300, Loss: 0.0224 | 0.0210
Epoch 115/300, Loss: 0.0223 | 0.0211
Epoch 116/300, Loss: 0.0229 | 0.0223
Epoch 117/300, Loss: 0.0225 | 0.0213
Epoch 118/300, Loss: 0.0225 | 0.0215
Epoch 119/300, Loss: 0.0226 | 0.0226
Epoch 120/300, Loss: 0.0222 | 0.0217
Epoch 121/300, Loss: 0.0223 | 0.0223
Epoch 122/300, Loss: 0.0224 | 0.0225
Epoch 123/300, Loss: 0.0226 | 0.0226
Epoch 124/300, Loss: 0.0230 | 0.0227
Epoch 125/300, Loss: 0.0236 | 0.0217
Epoch 126/300, Loss: 0.0242 | 0.0225
Epoch 127/300, Loss: 0.0259 | 0.0242
Epoch 128/300, Loss: 0.0250 | 0.0278
Epoch 129/300, Loss: 0.0267 | 0.0319
Epoch 130/300, Loss: 0.0234 | 0.0248
Epoch 131/300, Loss: 0.0236 | 0.0235
Epoch 132/300, Loss: 0.0242 | 0.0265
Epoch 133/300, Loss: 0.0248 | 0.0256
Epoch 134/300, Loss: 0.0242 | 0.0235
Epoch 135/300, Loss: 0.0229 | 0.0218
Epoch 136/300, Loss: 0.0224 | 0.0222
Epoch 137/300, Loss: 0.0223 | 0.0222
Epoch 138/300, Loss: 0.0222 | 0.0219
Epoch 139/300, Loss: 0.0219 | 0.0215
Epoch 140/300, Loss: 0.0216 | 0.0214
Epoch 141/300, Loss: 0.0214 | 0.0213
Epoch 142/300, Loss: 0.0213 | 0.0212
Epoch 143/300, Loss: 0.0213 | 0.0212
Epoch 144/300, Loss: 0.0212 | 0.0210
Epoch 145/300, Loss: 0.0211 | 0.0209
Epoch 146/300, Loss: 0.0210 | 0.0208
Epoch 147/300, Loss: 0.0209 | 0.0208
Epoch 148/300, Loss: 0.0209 | 0.0208
Epoch 149/300, Loss: 0.0208 | 0.0208
Epoch 150/300, Loss: 0.0208 | 0.0208
Epoch 151/300, Loss: 0.0208 | 0.0207
Epoch 152/300, Loss: 0.0207 | 0.0207
Epoch 153/300, Loss: 0.0207 | 0.0206
Epoch 154/300, Loss: 0.0206 | 0.0206
Epoch 155/300, Loss: 0.0206 | 0.0206
Epoch 156/300, Loss: 0.0206 | 0.0206
Epoch 157/300, Loss: 0.0206 | 0.0204
Epoch 158/300, Loss: 0.0205 | 0.0204
Epoch 159/300, Loss: 0.0205 | 0.0204
Epoch 160/300, Loss: 0.0205 | 0.0204
Epoch 161/300, Loss: 0.0205 | 0.0204
Epoch 162/300, Loss: 0.0205 | 0.0204
Epoch 163/300, Loss: 0.0205 | 0.0204
Epoch 164/300, Loss: 0.0206 | 0.0203
Epoch 165/300, Loss: 0.0206 | 0.0204
Epoch 166/300, Loss: 0.0206 | 0.0205
Epoch 167/300, Loss: 0.0206 | 0.0206
Epoch 168/300, Loss: 0.0206 | 0.0207
Epoch 169/300, Loss: 0.0206 | 0.0207
Epoch 170/300, Loss: 0.0206 | 0.0208
Epoch 171/300, Loss: 0.0206 | 0.0209
Epoch 172/300, Loss: 0.0206 | 0.0210
Epoch 173/300, Loss: 0.0205 | 0.0210
Epoch 174/300, Loss: 0.0204 | 0.0210
Epoch 175/300, Loss: 0.0203 | 0.0210
Epoch 176/300, Loss: 0.0203 | 0.0210
Epoch 177/300, Loss: 0.0202 | 0.0210
Epoch 178/300, Loss: 0.0202 | 0.0210
Epoch 179/300, Loss: 0.0201 | 0.0210
Epoch 180/300, Loss: 0.0201 | 0.0209
Epoch 181/300, Loss: 0.0200 | 0.0209
Epoch 182/300, Loss: 0.0200 | 0.0209
Epoch 183/300, Loss: 0.0199 | 0.0208
Epoch 184/300, Loss: 0.0199 | 0.0208
Epoch 185/300, Loss: 0.0198 | 0.0208
Epoch 186/300, Loss: 0.0198 | 0.0208
Epoch 187/300, Loss: 0.0198 | 0.0208
Epoch 188/300, Loss: 0.0197 | 0.0208
Epoch 189/300, Loss: 0.0197 | 0.0207
Epoch 190/300, Loss: 0.0197 | 0.0207
Epoch 191/300, Loss: 0.0197 | 0.0207
Epoch 192/300, Loss: 0.0196 | 0.0207
Epoch 193/300, Loss: 0.0196 | 0.0207
Epoch 194/300, Loss: 0.0196 | 0.0207
Epoch 195/300, Loss: 0.0196 | 0.0207
Epoch 196/300, Loss: 0.0196 | 0.0207
Epoch 197/300, Loss: 0.0196 | 0.0208
Epoch 198/300, Loss: 0.0196 | 0.0208
Epoch 199/300, Loss: 0.0196 | 0.0209
Epoch 200/300, Loss: 0.0196 | 0.0209
Epoch 201/300, Loss: 0.0197 | 0.0210
Epoch 202/300, Loss: 0.0197 | 0.0211
Epoch 203/300, Loss: 0.0197 | 0.0213
Epoch 204/300, Loss: 0.0198 | 0.0214
Epoch 205/300, Loss: 0.0198 | 0.0215
Epoch 206/300, Loss: 0.0198 | 0.0215
Epoch 207/300, Loss: 0.0197 | 0.0215
Epoch 208/300, Loss: 0.0197 | 0.0214
Epoch 209/300, Loss: 0.0196 | 0.0214
Epoch 210/300, Loss: 0.0196 | 0.0214
Epoch 211/300, Loss: 0.0195 | 0.0213
Epoch 212/300, Loss: 0.0195 | 0.0212
Epoch 213/300, Loss: 0.0194 | 0.0212
Epoch 214/300, Loss: 0.0194 | 0.0211
Epoch 215/300, Loss: 0.0194 | 0.0211
Epoch 216/300, Loss: 0.0194 | 0.0211
Epoch 217/300, Loss: 0.0193 | 0.0211
Epoch 218/300, Loss: 0.0193 | 0.0211
Epoch 219/300, Loss: 0.0193 | 0.0211
Epoch 220/300, Loss: 0.0193 | 0.0210
Epoch 221/300, Loss: 0.0193 | 0.0210
Epoch 222/300, Loss: 0.0193 | 0.0211
Epoch 223/300, Loss: 0.0192 | 0.0210
Epoch 224/300, Loss: 0.0192 | 0.0210
Epoch 225/300, Loss: 0.0192 | 0.0210
Epoch 226/300, Loss: 0.0192 | 0.0210
Epoch 227/300, Loss: 0.0192 | 0.0210
Epoch 228/300, Loss: 0.0192 | 0.0210
Epoch 229/300, Loss: 0.0192 | 0.0211
Epoch 230/300, Loss: 0.0192 | 0.0211
Epoch 231/300, Loss: 0.0192 | 0.0211
Epoch 232/300, Loss: 0.0192 | 0.0211
Epoch 233/300, Loss: 0.0192 | 0.0211
Epoch 234/300, Loss: 0.0192 | 0.0211
Epoch 235/300, Loss: 0.0192 | 0.0211
Epoch 236/300, Loss: 0.0192 | 0.0211
Epoch 237/300, Loss: 0.0191 | 0.0211
Epoch 238/300, Loss: 0.0191 | 0.0211
Epoch 239/300, Loss: 0.0191 | 0.0211
Epoch 240/300, Loss: 0.0191 | 0.0211
Epoch 241/300, Loss: 0.0191 | 0.0211
Epoch 242/300, Loss: 0.0191 | 0.0211
Epoch 243/300, Loss: 0.0191 | 0.0211
Epoch 244/300, Loss: 0.0191 | 0.0211
Epoch 245/300, Loss: 0.0190 | 0.0211
Epoch 246/300, Loss: 0.0190 | 0.0211
Epoch 247/300, Loss: 0.0190 | 0.0210
Epoch 248/300, Loss: 0.0190 | 0.0210
Epoch 249/300, Loss: 0.0190 | 0.0210
Epoch 250/300, Loss: 0.0190 | 0.0210
Epoch 251/300, Loss: 0.0190 | 0.0210
Epoch 252/300, Loss: 0.0189 | 0.0210
Epoch 253/300, Loss: 0.0189 | 0.0210
Epoch 254/300, Loss: 0.0189 | 0.0210
Epoch 255/300, Loss: 0.0189 | 0.0210
Epoch 256/300, Loss: 0.0189 | 0.0209
Epoch 257/300, Loss: 0.0189 | 0.0209
Epoch 258/300, Loss: 0.0189 | 0.0209
Epoch 259/300, Loss: 0.0189 | 0.0209
Epoch 260/300, Loss: 0.0189 | 0.0209
Epoch 261/300, Loss: 0.0189 | 0.0209
Epoch 262/300, Loss: 0.0188 | 0.0209
Epoch 263/300, Loss: 0.0188 | 0.0209
Epoch 264/300, Loss: 0.0188 | 0.0209
Epoch 265/300, Loss: 0.0188 | 0.0209
Epoch 266/300, Loss: 0.0188 | 0.0209
Epoch 267/300, Loss: 0.0188 | 0.0209
Epoch 268/300, Loss: 0.0188 | 0.0209
Epoch 269/300, Loss: 0.0188 | 0.0209
Epoch 270/300, Loss: 0.0188 | 0.0209
Epoch 271/300, Loss: 0.0188 | 0.0209
Epoch 272/300, Loss: 0.0188 | 0.0209
Epoch 273/300, Loss: 0.0188 | 0.0209
Epoch 274/300, Loss: 0.0188 | 0.0209
Epoch 275/300, Loss: 0.0188 | 0.0209
Epoch 276/300, Loss: 0.0188 | 0.0209
Epoch 277/300, Loss: 0.0188 | 0.0209
Epoch 278/300, Loss: 0.0188 | 0.0209
Epoch 279/300, Loss: 0.0188 | 0.0209
Epoch 280/300, Loss: 0.0187 | 0.0209
Epoch 281/300, Loss: 0.0187 | 0.0209
Epoch 282/300, Loss: 0.0187 | 0.0209
Epoch 283/300, Loss: 0.0187 | 0.0209
Epoch 284/300, Loss: 0.0187 | 0.0209
Epoch 285/300, Loss: 0.0187 | 0.0209
Epoch 286/300, Loss: 0.0187 | 0.0209
Epoch 287/300, Loss: 0.0187 | 0.0209
Epoch 288/300, Loss: 0.0187 | 0.0209
Epoch 289/300, Loss: 0.0187 | 0.0209
Epoch 290/300, Loss: 0.0187 | 0.0209
Epoch 291/300, Loss: 0.0187 | 0.0209
Epoch 292/300, Loss: 0.0187 | 0.0209
Epoch 293/300, Loss: 0.0187 | 0.0209
Epoch 294/300, Loss: 0.0187 | 0.0209
Epoch 295/300, Loss: 0.0187 | 0.0209
Epoch 296/300, Loss: 0.0187 | 0.0209
Epoch 297/300, Loss: 0.0187 | 0.0209
Epoch 298/300, Loss: 0.0187 | 0.0209
Epoch 299/300, Loss: 0.0187 | 0.0210
Epoch 300/300, Loss: 0.0187 | 0.0210
Runtime (seconds): 293.10931634902954
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 18.508927570888773
RMSE: 4.3022003173828125
MAE: 4.3022003173828125
R-squared: nan
[121.962204]
