Using device: cuda
[*********************100%***********************]  1 of 1 completed
Ticker            AAPL
Date
2012-05-18   15.978589
2012-05-21   16.909508
2012-05-22   16.779659
2012-05-23   17.189087
2012-05-24   17.031218
...                ...
2023-05-24  170.546951
2023-05-25  171.688309
2023-05-26  174.109940
2023-05-30  175.965866
2023-05-31  175.916260

[2776 rows x 1 columns]
/data/student/k2110261/Multi-iTransformer/optunademo.py:105: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.
  df = df.fillna(method='ffill')  # ÂâçÊó•„ÅÆ„Éá„Éº„Çø„Çí‰ΩøÁî®
{'AAPL': Date
2012-05-18     18.353260
2012-05-21     18.337501
2012-05-22     18.321753
2012-05-23     18.306016
2012-05-24     18.290290
                 ...
2023-05-24    161.245406
2023-05-25    161.302977
2023-05-26    161.360544
2023-05-30    161.418108
2023-05-31    161.475668
Name: trend, Length: 2776, dtype: float64, 'DTWEXBGS': Date
2012-05-18     93.1395
2012-05-21     93.0945
2012-05-22     93.1113
2012-05-23     93.8855
2012-05-24     93.8027
                ...
2023-05-24    120.6481
2023-05-25    121.0126
2023-05-26    120.8022
2023-05-30    120.7387
2023-05-31    121.1527
Length: 2776, dtype: float64, 'VIXCLS': Date
2012-05-18    25.10
2012-05-21    22.01
2012-05-22    22.48
2012-05-23    22.33
2012-05-24    21.54
              ...
2023-05-24    20.03
2023-05-25    19.14
2023-05-26    17.95
2023-05-30    17.46
2023-05-31    17.94
Length: 2776, dtype: float64, 'DFII10': Date
2012-05-18   -0.39
2012-05-21   -0.41
2012-05-22   -0.38
2012-05-23   -0.41
2012-05-24   -0.37
              ...
2023-05-24    1.48
2023-05-25    1.58
2023-05-26    1.57
2023-05-30    1.47
2023-05-31    1.46
Length: 2776, dtype: float64, 'T10Y2Y': Date
2012-05-18    1.39
2012-05-21    1.45
2012-05-22    1.49
2012-05-23    1.45
2012-05-24    1.48
              ...
2023-05-24   -0.58
2023-05-25   -0.67
2023-05-26   -0.74
2023-05-30   -0.77
2023-05-31   -0.76
Length: 2776, dtype: float64}
AAPL„Å´„ÅÇ„Çã„Åå‰ªñ„ÅÆ„Éá„Éº„Çø1„Å´„ÅØ„Å™„ÅÑÊó•‰ªò:
DatetimeIndex([], dtype='datetime64[ns]', name='Date', freq=None)
‰ªñ„ÅÆ„Éá„Éº„Çø1„Å´„ÅÇ„Çã„ÅåAAPL„Å´„ÅØ„Å™„ÅÑÊó•‰ªò:
DatetimeIndex([], dtype='datetime64[ns]', name='Date', freq=None)
[32m[I 2024-11-17 02:17:41,692][0m A new study created in memory with name: no-name-c1af9dc6-2a91-44dd-8bfa-7570a103801e[0m
/data/student/k2110261/Multi-iTransformer/optunademo.py:165: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.
  learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[32m[I 2024-11-17 02:17:44,465][0m Trial 0 finished with value: 4.3799638748168945 and parameters: {'learning_rate': 2.3144003746599772e-05, 'batch_size': 157, 'step_size': 8, 'gamma': 0.9403239641428662, 'depth': 3, 'dim': 88}. Best is trial 0 with value: 4.3799638748168945.[0m
[32m[I 2024-11-17 02:17:44,465][0m A new study created in memory with name: no-name-3a69b2fd-0dc1-469a-a947-b59c64d8572d[0m
[32m[I 2024-11-17 02:17:45,594][0m Trial 0 finished with value: 1.595024585723877 and parameters: {'learning_rate': 1.6681266788626346e-06, 'batch_size': 232, 'step_size': 6, 'gamma': 0.932833392410633, 'depth': 4, 'dim': 192}. Best is trial 0 with value: 1.595024585723877.[0m
[32m[I 2024-11-17 02:17:45,595][0m A new study created in memory with name: no-name-3f18905e-356a-41fc-a4fe-0c49b3945ca0[0m
[32m[I 2024-11-17 02:17:47,560][0m Trial 0 finished with value: 0.07494222465902567 and parameters: {'learning_rate': 7.610044128349953e-05, 'batch_size': 42, 'step_size': 15, 'gamma': 0.7563335688069202, 'depth': 4, 'dim': 197}. Best is trial 0 with value: 0.07494222465902567.[0m
Best hyperparameters (trend): {'learning_rate': 2.3144003746599772e-05, 'batch_size': 157, 'step_size': 8, 'gamma': 0.9403239641428662, 'depth': 3, 'dim': 88}
Best hyperparameters (seasonal): {'learning_rate': 1.6681266788626346e-06, 'batch_size': 232, 'step_size': 6, 'gamma': 0.932833392410633, 'depth': 4, 'dim': 192}
Best hyperparameters (resid): {'learning_rate': 7.610044128349953e-05, 'batch_size': 42, 'step_size': 15, 'gamma': 0.7563335688069202, 'depth': 4, 'dim': 197}
Epoch 1/1000, (Training | Validation) Trend Loss: 0.1623 | 0.0980, Seasonal Loss: 0.6356 | 0.3175, Residual Loss: 0.5798 | 0.2247
Epoch 2/1000, (Training | Validation) Trend Loss: 0.0844 | 0.0525, Seasonal Loss: 0.3079 | 0.2018, Residual Loss: 0.2927 | 0.1276
Epoch 3/1000, (Training | Validation) Trend Loss: 0.0652 | 0.0329, Seasonal Loss: 0.2342 | 0.1493, Residual Loss: 0.1850 | 0.0779
Epoch 4/1000, (Training | Validation) Trend Loss: 0.0552 | 0.0263, Seasonal Loss: 0.1999 | 0.1276, Residual Loss: 0.1196 | 0.0452
Epoch 5/1000, (Training | Validation) Trend Loss: 0.0477 | 0.0243, Seasonal Loss: 0.1873 | 0.1522, Residual Loss: 0.0924 | 0.0335
Epoch 6/1000, (Training | Validation) Trend Loss: 0.0420 | 0.0198, Seasonal Loss: 0.1715 | 0.1127, Residual Loss: 0.0821 | 0.0254
Epoch 7/1000, (Training | Validation) Trend Loss: 0.0379 | 0.0225, Seasonal Loss: 0.1610 | 0.1438, Residual Loss: 0.0794 | 0.0378
Epoch 8/1000, (Training | Validation) Trend Loss: 0.0360 | 0.0241, Seasonal Loss: 0.1333 | 0.0948, Residual Loss: 0.0720 | 0.0281
Epoch 9/1000, (Training | Validation) Trend Loss: 0.0346 | 0.0271, Seasonal Loss: 0.1084 | 0.0738, Residual Loss: 0.0629 | 0.0258
Epoch 10/1000, (Training | Validation) Trend Loss: 0.0339 | 0.0329, Seasonal Loss: 0.1114 | 0.0841, Residual Loss: 0.0561 | 0.0251
Epoch 11/1000, (Training | Validation) Trend Loss: 0.0333 | 0.0526, Seasonal Loss: 0.0790 | 0.0724, Residual Loss: 0.0550 | 0.0264
Epoch 12/1000, (Training | Validation) Trend Loss: 0.0330 | 0.0534, Seasonal Loss: 0.0972 | 0.1111, Residual Loss: 0.0556 | 0.0275
Epoch 13/1000, (Training | Validation) Trend Loss: 0.0304 | 0.0463, Seasonal Loss: 0.1031 | 0.0784, Residual Loss: 0.0596 | 0.0375
Epoch 14/1000, (Training | Validation) Trend Loss: 0.0272 | 0.0334, Seasonal Loss: 0.1022 | 0.0856, Residual Loss: 0.0670 | 0.0542
Epoch 15/1000, (Training | Validation) Trend Loss: 0.0239 | 0.0231, Seasonal Loss: 0.1051 | 0.0988, Residual Loss: 0.0785 | 0.0742
Epoch 16/1000, (Training | Validation) Trend Loss: 0.0214 | 0.0168, Seasonal Loss: 0.0981 | 0.0778, Residual Loss: 0.0818 | 0.0594
Epoch 17/1000, (Training | Validation) Trend Loss: 0.0201 | 0.0137, Seasonal Loss: 0.0955 | 0.0708, Residual Loss: 0.0795 | 0.0283
Epoch 18/1000, (Training | Validation) Trend Loss: 0.0193 | 0.0131, Seasonal Loss: 0.0844 | 0.0812, Residual Loss: 0.0654 | 0.0242
Epoch 19/1000, (Training | Validation) Trend Loss: 0.0188 | 0.0125, Seasonal Loss: 0.1119 | 0.1091, Residual Loss: 0.0604 | 0.0259
Epoch 20/1000, (Training | Validation) Trend Loss: 0.0184 | 0.0124, Seasonal Loss: 0.1159 | 0.0838, Residual Loss: 0.0508 | 0.0239
Epoch 21/1000, (Training | Validation) Trend Loss: 0.0181 | 0.0127, Seasonal Loss: 0.1048 | 0.1063, Residual Loss: 0.0495 | 0.0257
Epoch 22/1000, (Training | Validation) Trend Loss: 0.0179 | 0.0126, Seasonal Loss: 0.1171 | 0.1285, Residual Loss: 0.0475 | 0.0253
Epoch 23/1000, (Training | Validation) Trend Loss: 0.0176 | 0.0129, Seasonal Loss: 0.0971 | 0.0907, Residual Loss: 0.0526 | 0.0245
Epoch 24/1000, (Training | Validation) Trend Loss: 0.0175 | 0.0130, Seasonal Loss: 0.0709 | 0.0576, Residual Loss: 0.0503 | 0.0260
Epoch 25/1000, (Training | Validation) Trend Loss: 0.0173 | 0.0133, Seasonal Loss: 0.0424 | 0.0631, Residual Loss: 0.0545 | 0.0242
Epoch 26/1000, (Training | Validation) Trend Loss: 0.0171 | 0.0132, Seasonal Loss: 0.0371 | 0.0553, Residual Loss: 0.0496 | 0.0260
Epoch 27/1000, (Training | Validation) Trend Loss: 0.0170 | 0.0129, Seasonal Loss: 0.0332 | 0.0526, Residual Loss: 0.0511 | 0.0239
Epoch 28/1000, (Training | Validation) Trend Loss: 0.0168 | 0.0131, Seasonal Loss: 0.0326 | 0.0539, Residual Loss: 0.0482 | 0.0261
Epoch 29/1000, (Training | Validation) Trend Loss: 0.0167 | 0.0127, Seasonal Loss: 0.0320 | 0.0525, Residual Loss: 0.0507 | 0.0237
Epoch 30/1000, (Training | Validation) Trend Loss: 0.0166 | 0.0133, Seasonal Loss: 0.0317 | 0.0553, Residual Loss: 0.0470 | 0.0256
Epoch 31/1000, (Training | Validation) Trend Loss: 0.0165 | 0.0125, Seasonal Loss: 0.0334 | 0.0605, Residual Loss: 0.0498 | 0.0232
Epoch 32/1000, (Training | Validation) Trend Loss: 0.0163 | 0.0130, Seasonal Loss: 0.0348 | 0.0623, Residual Loss: 0.0456 | 0.0245
Epoch 33/1000, (Training | Validation) Trend Loss: 0.0163 | 0.0120, Seasonal Loss: 0.0428 | 0.0609, Residual Loss: 0.0473 | 0.0232
Epoch 34/1000, (Training | Validation) Trend Loss: 0.0162 | 0.0136, Seasonal Loss: 0.0450 | 0.0565, Residual Loss: 0.0442 | 0.0235
Epoch 35/1000, (Training | Validation) Trend Loss: 0.0164 | 0.0117, Seasonal Loss: 0.0541 | 0.0504, Residual Loss: 0.0439 | 0.0227
Epoch 36/1000, (Training | Validation) Trend Loss: 0.0165 | 0.0164, Seasonal Loss: 0.0400 | 0.0507, Residual Loss: 0.0422 | 0.0229
Epoch 37/1000, (Training | Validation) Trend Loss: 0.0180 | 0.0131, Seasonal Loss: 0.0434 | 0.0492, Residual Loss: 0.0420 | 0.0225
Epoch 38/1000, (Training | Validation) Trend Loss: 0.0179 | 0.0191, Seasonal Loss: 0.0420 | 0.0526, Residual Loss: 0.0409 | 0.0225
Epoch 39/1000, (Training | Validation) Trend Loss: 0.0205 | 0.0187, Seasonal Loss: 0.0482 | 0.0527, Residual Loss: 0.0409 | 0.0223
Epoch 40/1000, (Training | Validation) Trend Loss: 0.0192 | 0.0182, Seasonal Loss: 0.0440 | 0.0539, Residual Loss: 0.0402 | 0.0225
Epoch 41/1000, (Training | Validation) Trend Loss: 0.0190 | 0.0204, Seasonal Loss: 0.0560 | 0.0793, Residual Loss: 0.0405 | 0.0221
Epoch 42/1000, (Training | Validation) Trend Loss: 0.0181 | 0.0159, Seasonal Loss: 0.0657 | 0.1025, Residual Loss: 0.0399 | 0.0225
Epoch 43/1000, (Training | Validation) Trend Loss: 0.0165 | 0.0131, Seasonal Loss: 0.0685 | 0.1009, Residual Loss: 0.0402 | 0.0220
Epoch 44/1000, (Training | Validation) Trend Loss: 0.0162 | 0.0137, Seasonal Loss: 0.0649 | 0.0849, Residual Loss: 0.0398 | 0.0224
Epoch 45/1000, (Training | Validation) Trend Loss: 0.0159 | 0.0117, Seasonal Loss: 0.0430 | 0.0533, Residual Loss: 0.0397 | 0.0221
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/optunademo.py", line 314, in <module>
    model_seasonal, train_loss_seasonal, valid_loss_seasonal = train(
                                                               ^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 40, in train
    loss.backward()  # ÈÄÜ‰ºùÊí≠
    ^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
