ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 04:30:18,501][0m A new study created in memory with name: no-name-86dc183c-1cf9-4799-b002-3cc91f708502[0m
[32m[I 2025-01-07 04:32:42,705][0m Trial 0 finished with value: 0.07457708092027121 and parameters: {'observation_period_num': 95, 'train_rates': 0.845462663292184, 'learning_rate': 5.825708845301953e-05, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8342870152584314}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:33:26,956][0m Trial 1 finished with value: 0.10502670460939408 and parameters: {'observation_period_num': 210, 'train_rates': 0.9089214163005352, 'learning_rate': 9.966157393487586e-05, 'batch_size': 213, 'step_size': 9, 'gamma': 0.8474362438450137}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:34:58,631][0m Trial 2 finished with value: 0.3868950209096341 and parameters: {'observation_period_num': 239, 'train_rates': 0.8974371867610953, 'learning_rate': 2.5835180628447807e-06, 'batch_size': 61, 'step_size': 3, 'gamma': 0.9779172574862465}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:35:43,258][0m Trial 3 finished with value: 0.7999387968593066 and parameters: {'observation_period_num': 168, 'train_rates': 0.6471702206416065, 'learning_rate': 1.931580273896282e-06, 'batch_size': 180, 'step_size': 15, 'gamma': 0.7745295247793513}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:36:29,967][0m Trial 4 finished with value: 0.14813416528841072 and parameters: {'observation_period_num': 44, 'train_rates': 0.6469225899331349, 'learning_rate': 0.0005440310185467323, 'batch_size': 157, 'step_size': 15, 'gamma': 0.762076701143282}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:37:22,566][0m Trial 5 finished with value: 0.2436924413115482 and parameters: {'observation_period_num': 79, 'train_rates': 0.6721352171300817, 'learning_rate': 0.0008805158211375226, 'batch_size': 161, 'step_size': 15, 'gamma': 0.9272821152303223}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:38:51,112][0m Trial 6 finished with value: 0.39395192436539717 and parameters: {'observation_period_num': 134, 'train_rates': 0.7311702029778032, 'learning_rate': 7.245868125060824e-06, 'batch_size': 75, 'step_size': 15, 'gamma': 0.831989042101105}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:40:13,358][0m Trial 7 finished with value: 0.20189581675487653 and parameters: {'observation_period_num': 27, 'train_rates': 0.7079459201365637, 'learning_rate': 9.35059349112308e-06, 'batch_size': 73, 'step_size': 11, 'gamma': 0.9582667198923853}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:41:00,704][0m Trial 8 finished with value: 1.093362253398382 and parameters: {'observation_period_num': 50, 'train_rates': 0.6671402102167581, 'learning_rate': 2.6253269131772355e-06, 'batch_size': 208, 'step_size': 8, 'gamma': 0.8571499894086094}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:41:56,389][0m Trial 9 finished with value: 0.2547938059355647 and parameters: {'observation_period_num': 148, 'train_rates': 0.7384884795160225, 'learning_rate': 0.0003581257342303996, 'batch_size': 247, 'step_size': 14, 'gamma': 0.9496773089839321}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:48:18,106][0m Trial 10 finished with value: 0.08366761356592178 and parameters: {'observation_period_num': 87, 'train_rates': 0.9888623698707414, 'learning_rate': 5.82056394153324e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9021712310765815}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:52:47,135][0m Trial 11 finished with value: 0.09661788816253344 and parameters: {'observation_period_num': 82, 'train_rates': 0.9898877966657376, 'learning_rate': 6.116010489824519e-05, 'batch_size': 23, 'step_size': 4, 'gamma': 0.8992109769576204}. Best is trial 0 with value: 0.07457708092027121.[0m
[32m[I 2025-01-07 04:58:28,455][0m Trial 12 finished with value: 0.06400944484178878 and parameters: {'observation_period_num': 96, 'train_rates': 0.8424108182059077, 'learning_rate': 2.0867824036969948e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8145196939489053}. Best is trial 12 with value: 0.06400944484178878.[0m
Early stopping at epoch 57
[32m[I 2025-01-07 04:59:11,140][0m Trial 13 finished with value: 0.5755200171470642 and parameters: {'observation_period_num': 110, 'train_rates': 0.824370601399303, 'learning_rate': 1.7331747101265425e-05, 'batch_size': 112, 'step_size': 1, 'gamma': 0.8080512988060773}. Best is trial 12 with value: 0.06400944484178878.[0m
[32m[I 2025-01-07 05:00:17,253][0m Trial 14 finished with value: 0.04728700674185864 and parameters: {'observation_period_num': 8, 'train_rates': 0.8112898680453202, 'learning_rate': 0.00013094996583203416, 'batch_size': 115, 'step_size': 7, 'gamma': 0.8026430459313034}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:01:20,585][0m Trial 15 finished with value: 0.1756993062302189 and parameters: {'observation_period_num': 10, 'train_rates': 0.7840175498701609, 'learning_rate': 0.0001682830553915633, 'batch_size': 111, 'step_size': 7, 'gamma': 0.797049393593687}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:02:24,340][0m Trial 16 finished with value: 0.16452468953849303 and parameters: {'observation_period_num': 179, 'train_rates': 0.8790824444659002, 'learning_rate': 2.5814544681976572e-05, 'batch_size': 115, 'step_size': 6, 'gamma': 0.8045067726929173}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:03:42,336][0m Trial 17 finished with value: 0.18751290750125765 and parameters: {'observation_period_num': 55, 'train_rates': 0.7794464758658156, 'learning_rate': 0.00020048299005829984, 'batch_size': 90, 'step_size': 10, 'gamma': 0.7874619117665648}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:05:31,981][0m Trial 18 finished with value: 0.3335135681856125 and parameters: {'observation_period_num': 7, 'train_rates': 0.6016039433564215, 'learning_rate': 9.872132172399756e-06, 'batch_size': 47, 'step_size': 2, 'gamma': 0.7512978950035459}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:06:23,813][0m Trial 19 finished with value: 0.11207043312914003 and parameters: {'observation_period_num': 112, 'train_rates': 0.8268919195727648, 'learning_rate': 2.9125463337409943e-05, 'batch_size': 140, 'step_size': 12, 'gamma': 0.8847875697718488}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:07:18,232][0m Trial 20 finished with value: 0.5101580023765564 and parameters: {'observation_period_num': 55, 'train_rates': 0.9456512149213863, 'learning_rate': 4.886290282899131e-06, 'batch_size': 256, 'step_size': 6, 'gamma': 0.8144723598009082}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:09:22,267][0m Trial 21 finished with value: 0.07136221401846383 and parameters: {'observation_period_num': 108, 'train_rates': 0.8326031967013856, 'learning_rate': 4.587181688958607e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8294058413312768}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:11:42,327][0m Trial 22 finished with value: 0.09361864976656209 and parameters: {'observation_period_num': 155, 'train_rates': 0.85083874173189, 'learning_rate': 0.0001575896460258379, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8262002366523312}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:12:47,966][0m Trial 23 finished with value: 0.15635032865374568 and parameters: {'observation_period_num': 116, 'train_rates': 0.7934893410191578, 'learning_rate': 2.0263383966591455e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8563337667827757}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:14:23,514][0m Trial 24 finished with value: 0.2911167999948649 and parameters: {'observation_period_num': 199, 'train_rates': 0.753198073674953, 'learning_rate': 9.172466922387488e-05, 'batch_size': 55, 'step_size': 5, 'gamma': 0.782100786999682}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:17:46,904][0m Trial 25 finished with value: 0.06002010663350423 and parameters: {'observation_period_num': 73, 'train_rates': 0.8699790738419133, 'learning_rate': 4.4062289406267505e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8744361017636686}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:21:54,361][0m Trial 26 finished with value: 0.04776277003201796 and parameters: {'observation_period_num': 30, 'train_rates': 0.9340263082295482, 'learning_rate': 1.4663437778873253e-05, 'batch_size': 24, 'step_size': 10, 'gamma': 0.887792211534581}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:23:10,351][0m Trial 27 finished with value: 0.41470375573367213 and parameters: {'observation_period_num': 31, 'train_rates': 0.9391395710541467, 'learning_rate': 1.0110495772048633e-06, 'batch_size': 79, 'step_size': 11, 'gamma': 0.8798138635954122}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:23:57,924][0m Trial 28 finished with value: 0.09113462859729551 and parameters: {'observation_period_num': 67, 'train_rates': 0.9449691543274168, 'learning_rate': 0.00028718272729055984, 'batch_size': 132, 'step_size': 12, 'gamma': 0.9267717987173225}. Best is trial 14 with value: 0.04728700674185864.[0m
[32m[I 2025-01-07 05:27:23,640][0m Trial 29 finished with value: 0.04480139679949859 and parameters: {'observation_period_num': 29, 'train_rates': 0.8713259813403473, 'learning_rate': 3.930695634805667e-05, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8999347493540145}. Best is trial 29 with value: 0.04480139679949859.[0m
[32m[I 2025-01-07 05:28:58,046][0m Trial 30 finished with value: 0.053355356821647056 and parameters: {'observation_period_num': 25, 'train_rates': 0.9159457875252062, 'learning_rate': 1.2624309738442075e-05, 'batch_size': 61, 'step_size': 13, 'gamma': 0.9147567715220853}. Best is trial 29 with value: 0.04480139679949859.[0m
[32m[I 2025-01-07 05:30:36,187][0m Trial 31 finished with value: 0.051312576422071834 and parameters: {'observation_period_num': 25, 'train_rates': 0.913524942003833, 'learning_rate': 1.4203983239741281e-05, 'batch_size': 59, 'step_size': 13, 'gamma': 0.9151914772132519}. Best is trial 29 with value: 0.04480139679949859.[0m
[32m[I 2025-01-07 05:33:08,337][0m Trial 32 finished with value: 0.035403324368078845 and parameters: {'observation_period_num': 35, 'train_rates': 0.8758590687587252, 'learning_rate': 8.806436042568832e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9001543034623491}. Best is trial 32 with value: 0.035403324368078845.[0m
[32m[I 2025-01-07 05:36:00,358][0m Trial 33 finished with value: 0.030800605293036184 and parameters: {'observation_period_num': 8, 'train_rates': 0.8803465763653129, 'learning_rate': 8.687502256666461e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.861843474253027}. Best is trial 33 with value: 0.030800605293036184.[0m
[32m[I 2025-01-07 05:38:33,932][0m Trial 34 finished with value: 0.02556518150891872 and parameters: {'observation_period_num': 6, 'train_rates': 0.8826336436485747, 'learning_rate': 0.00010181138901250208, 'batch_size': 36, 'step_size': 9, 'gamma': 0.851653141022606}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:40:57,779][0m Trial 35 finished with value: 0.04123475948541329 and parameters: {'observation_period_num': 38, 'train_rates': 0.8808244104106456, 'learning_rate': 7.799847360369983e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8544698424961311}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:43:21,154][0m Trial 36 finished with value: 0.04452506071730466 and parameters: {'observation_period_num': 42, 'train_rates': 0.8801943634778754, 'learning_rate': 8.833548861859146e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8506785329722899}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:45:07,774][0m Trial 37 finished with value: 0.12501535102211195 and parameters: {'observation_period_num': 244, 'train_rates': 0.8930184500034923, 'learning_rate': 0.00026177097716680123, 'batch_size': 49, 'step_size': 9, 'gamma': 0.8643189426043661}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:46:29,266][0m Trial 38 finished with value: 0.042529970244096035 and parameters: {'observation_period_num': 17, 'train_rates': 0.9612355575045626, 'learning_rate': 7.356650012716831e-05, 'batch_size': 73, 'step_size': 9, 'gamma': 0.8396236075738965}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:47:30,671][0m Trial 39 finished with value: 0.09045765020731669 and parameters: {'observation_period_num': 60, 'train_rates': 0.8612622705869988, 'learning_rate': 0.0005458586548269087, 'batch_size': 91, 'step_size': 11, 'gamma': 0.8655606527514307}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:48:59,631][0m Trial 40 finished with value: 0.04991384587452017 and parameters: {'observation_period_num': 40, 'train_rates': 0.8985596290939323, 'learning_rate': 0.0001350388265072326, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8472465334148613}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:51:48,200][0m Trial 41 finished with value: 0.03698405689051883 and parameters: {'observation_period_num': 16, 'train_rates': 0.970126424576989, 'learning_rate': 7.25393528305703e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8404141851560452}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:54:28,279][0m Trial 42 finished with value: 0.031325218397923696 and parameters: {'observation_period_num': 5, 'train_rates': 0.9687062783665179, 'learning_rate': 0.0001095580348194492, 'batch_size': 37, 'step_size': 9, 'gamma': 0.8377608996823565}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:57:43,602][0m Trial 43 finished with value: 0.029147275753559604 and parameters: {'observation_period_num': 17, 'train_rates': 0.9686144469567296, 'learning_rate': 0.00011793344237234387, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8425341326085615}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:58:21,209][0m Trial 44 finished with value: 0.045323245227336884 and parameters: {'observation_period_num': 5, 'train_rates': 0.9652988934633386, 'learning_rate': 0.00011662129632141685, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9888094513426046}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 05:59:47,942][0m Trial 45 finished with value: 0.034547204677496224 and parameters: {'observation_period_num': 19, 'train_rates': 0.9156734334608523, 'learning_rate': 0.0004547353207040922, 'batch_size': 67, 'step_size': 7, 'gamma': 0.8712838963849662}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 06:01:16,749][0m Trial 46 finished with value: 0.037195466314125986 and parameters: {'observation_period_num': 20, 'train_rates': 0.9300030813426889, 'learning_rate': 0.000531285199688694, 'batch_size': 66, 'step_size': 7, 'gamma': 0.8223137709575987}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 06:03:14,791][0m Trial 47 finished with value: 0.04928900885420877 and parameters: {'observation_period_num': 47, 'train_rates': 0.9745516028002273, 'learning_rate': 0.0008034633301992343, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8686753010817437}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 06:08:07,885][0m Trial 48 finished with value: 0.030058805866054063 and parameters: {'observation_period_num': 15, 'train_rates': 0.9101387325381437, 'learning_rate': 0.0001993157667669713, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8385349524968673}. Best is trial 34 with value: 0.02556518150891872.[0m
[32m[I 2025-01-07 06:12:57,638][0m Trial 49 finished with value: 0.1309526045806706 and parameters: {'observation_period_num': 216, 'train_rates': 0.9883318520182155, 'learning_rate': 0.00022509759610587064, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8361909176706579}. Best is trial 34 with value: 0.02556518150891872.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 06:12:57,648][0m A new study created in memory with name: no-name-00697c04-deaa-4e83-991a-6ea251be4c39[0m
[32m[I 2025-01-07 06:15:13,223][0m Trial 0 finished with value: 0.0746938492377083 and parameters: {'observation_period_num': 71, 'train_rates': 0.9733470231821224, 'learning_rate': 0.00010000269885708616, 'batch_size': 43, 'step_size': 13, 'gamma': 0.7732246479208944}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:15:43,825][0m Trial 1 finished with value: 0.09446544240821492 and parameters: {'observation_period_num': 35, 'train_rates': 0.8681298088591667, 'learning_rate': 0.00012534670232995418, 'batch_size': 216, 'step_size': 1, 'gamma': 0.9606089777721252}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:16:13,553][0m Trial 2 finished with value: 0.8975357162275525 and parameters: {'observation_period_num': 97, 'train_rates': 0.7779424501617366, 'learning_rate': 3.100119448102324e-06, 'batch_size': 187, 'step_size': 10, 'gamma': 0.8128053144741249}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:16:42,350][0m Trial 3 finished with value: 0.44686886102247697 and parameters: {'observation_period_num': 151, 'train_rates': 0.7032620371408556, 'learning_rate': 1.9647479077547388e-05, 'batch_size': 179, 'step_size': 10, 'gamma': 0.8713388022382025}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:17:52,021][0m Trial 4 finished with value: 0.12837570491987418 and parameters: {'observation_period_num': 196, 'train_rates': 0.8656422805368787, 'learning_rate': 6.629620434504196e-05, 'batch_size': 76, 'step_size': 5, 'gamma': 0.7838216304255929}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:19:11,274][0m Trial 5 finished with value: 0.24692066257097284 and parameters: {'observation_period_num': 190, 'train_rates': 0.7873665792587321, 'learning_rate': 7.150113364422681e-06, 'batch_size': 62, 'step_size': 15, 'gamma': 0.9233500727717878}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:20:41,405][0m Trial 6 finished with value: 0.2832607731062879 and parameters: {'observation_period_num': 20, 'train_rates': 0.7782494153389172, 'learning_rate': 2.8589843555666195e-06, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9722803786982935}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:21:04,813][0m Trial 7 finished with value: 0.21915385857714445 and parameters: {'observation_period_num': 7, 'train_rates': 0.6489997759036307, 'learning_rate': 7.815713612936278e-05, 'batch_size': 226, 'step_size': 4, 'gamma': 0.816777893937661}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:21:28,384][0m Trial 8 finished with value: 0.21104305754162178 and parameters: {'observation_period_num': 19, 'train_rates': 0.6432431476798274, 'learning_rate': 3.8702689211483366e-05, 'batch_size': 233, 'step_size': 13, 'gamma': 0.8257374260683309}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:22:04,993][0m Trial 9 finished with value: 0.14721260239865716 and parameters: {'observation_period_num': 246, 'train_rates': 0.8218252977912941, 'learning_rate': 0.0001625863527252492, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9141605034358016}. Best is trial 0 with value: 0.0746938492377083.[0m
[32m[I 2025-01-07 06:25:59,683][0m Trial 10 finished with value: 0.069051498458499 and parameters: {'observation_period_num': 78, 'train_rates': 0.9564581609009111, 'learning_rate': 0.0008375736930560306, 'batch_size': 24, 'step_size': 9, 'gamma': 0.7708251563061138}. Best is trial 10 with value: 0.069051498458499.[0m
[32m[I 2025-01-07 06:31:35,482][0m Trial 11 finished with value: 0.07666985258460045 and parameters: {'observation_period_num': 82, 'train_rates': 0.9828151764846986, 'learning_rate': 0.0004113957862993255, 'batch_size': 17, 'step_size': 9, 'gamma': 0.7638202229914514}. Best is trial 10 with value: 0.069051498458499.[0m
[32m[I 2025-01-07 06:32:33,067][0m Trial 12 finished with value: 0.06459956616163254 and parameters: {'observation_period_num': 70, 'train_rates': 0.9768718810207752, 'learning_rate': 0.0008975139434696721, 'batch_size': 107, 'step_size': 7, 'gamma': 0.7576444077623293}. Best is trial 12 with value: 0.06459956616163254.[0m
[32m[I 2025-01-07 06:33:30,654][0m Trial 13 finished with value: 0.08233961259302135 and parameters: {'observation_period_num': 118, 'train_rates': 0.9247706910986956, 'learning_rate': 0.0008535446196155439, 'batch_size': 115, 'step_size': 7, 'gamma': 0.7549647771301924}. Best is trial 12 with value: 0.06459956616163254.[0m
[32m[I 2025-01-07 06:34:33,631][0m Trial 14 finished with value: 0.06439420382957906 and parameters: {'observation_period_num': 57, 'train_rates': 0.9227808870967822, 'learning_rate': 0.0008401417935226963, 'batch_size': 105, 'step_size': 7, 'gamma': 0.7983179776802967}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:35:39,529][0m Trial 15 finished with value: 0.06554665112820994 and parameters: {'observation_period_num': 50, 'train_rates': 0.918325955003518, 'learning_rate': 0.0003812755726967522, 'batch_size': 110, 'step_size': 6, 'gamma': 0.848055808222831}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:36:47,766][0m Trial 16 finished with value: 0.78866949073876 and parameters: {'observation_period_num': 53, 'train_rates': 0.8913031879663155, 'learning_rate': 1.0296560361114207e-06, 'batch_size': 93, 'step_size': 3, 'gamma': 0.7933442482908194}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:37:32,317][0m Trial 17 finished with value: 0.07880551442503929 and parameters: {'observation_period_num': 126, 'train_rates': 0.9294790714195507, 'learning_rate': 0.0003214771897660235, 'batch_size': 146, 'step_size': 7, 'gamma': 0.8734487057290358}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:38:28,897][0m Trial 18 finished with value: 0.09056329061587652 and parameters: {'observation_period_num': 148, 'train_rates': 0.8398615622628083, 'learning_rate': 0.000874072040331318, 'batch_size': 119, 'step_size': 2, 'gamma': 0.8433325579271609}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:39:30,114][0m Trial 19 finished with value: 0.2432045400183107 and parameters: {'observation_period_num': 104, 'train_rates': 0.7415422587343125, 'learning_rate': 0.0002410231360690852, 'batch_size': 86, 'step_size': 5, 'gamma': 0.7988643508001104}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:40:24,583][0m Trial 20 finished with value: 0.19681937992572784 and parameters: {'observation_period_num': 55, 'train_rates': 0.9485431910196961, 'learning_rate': 1.783671148768044e-05, 'batch_size': 162, 'step_size': 8, 'gamma': 0.751464910694633}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:41:23,399][0m Trial 21 finished with value: 0.07528447806835174 and parameters: {'observation_period_num': 51, 'train_rates': 0.9053762205700328, 'learning_rate': 0.00041968696420693235, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8447969308387259}. Best is trial 14 with value: 0.06439420382957906.[0m
[32m[I 2025-01-07 06:42:19,875][0m Trial 22 finished with value: 0.052286285907030106 and parameters: {'observation_period_num': 37, 'train_rates': 0.9897206221518304, 'learning_rate': 0.0005649431286332457, 'batch_size': 120, 'step_size': 6, 'gamma': 0.8440917333823916}. Best is trial 22 with value: 0.052286285907030106.[0m
[32m[I 2025-01-07 06:43:19,598][0m Trial 23 finished with value: 0.031011607497930527 and parameters: {'observation_period_num': 29, 'train_rates': 0.9844616018213376, 'learning_rate': 0.0005801486146465605, 'batch_size': 132, 'step_size': 8, 'gamma': 0.8961723542826149}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:44:22,505][0m Trial 24 finished with value: 0.05735296010971069 and parameters: {'observation_period_num': 36, 'train_rates': 0.9894462735599536, 'learning_rate': 0.00020684304861401194, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9011310392414614}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:45:19,579][0m Trial 25 finished with value: 0.06010046945168422 and parameters: {'observation_period_num': 29, 'train_rates': 0.9467477604130069, 'learning_rate': 0.00019473678108804942, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8983964168539226}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:46:10,124][0m Trial 26 finished with value: 0.04862002283334732 and parameters: {'observation_period_num': 34, 'train_rates': 0.9799878293604983, 'learning_rate': 0.0004839328965758135, 'batch_size': 175, 'step_size': 11, 'gamma': 0.8914166555723788}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:47:02,829][0m Trial 27 finished with value: 0.07139495883260046 and parameters: {'observation_period_num': 10, 'train_rates': 0.8869776806489976, 'learning_rate': 0.0004911280110944925, 'batch_size': 200, 'step_size': 11, 'gamma': 0.9373264387478504}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:47:56,095][0m Trial 28 finished with value: 0.11204809275313243 and parameters: {'observation_period_num': 93, 'train_rates': 0.8332885997950702, 'learning_rate': 4.972301323677074e-05, 'batch_size': 166, 'step_size': 9, 'gamma': 0.8902329898708655}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:48:47,369][0m Trial 29 finished with value: 0.21386130115562108 and parameters: {'observation_period_num': 37, 'train_rates': 0.6026394430689629, 'learning_rate': 0.00011137189026281354, 'batch_size': 150, 'step_size': 15, 'gamma': 0.9386915096832079}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:49:45,215][0m Trial 30 finished with value: 0.07503534853458405 and parameters: {'observation_period_num': 69, 'train_rates': 0.9573335156003544, 'learning_rate': 0.0002822798239725364, 'batch_size': 207, 'step_size': 12, 'gamma': 0.8842892901329992}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:50:37,734][0m Trial 31 finished with value: 0.04076118767261505 and parameters: {'observation_period_num': 36, 'train_rates': 0.9831315249084677, 'learning_rate': 0.000559032022135473, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9060674986232762}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:51:15,792][0m Trial 32 finished with value: 0.06966447830200195 and parameters: {'observation_period_num': 32, 'train_rates': 0.9899780412789331, 'learning_rate': 0.00057182611502862, 'batch_size': 248, 'step_size': 8, 'gamma': 0.8609653168789909}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:51:56,928][0m Trial 33 finished with value: 0.04745684191584587 and parameters: {'observation_period_num': 10, 'train_rates': 0.9635431653964697, 'learning_rate': 0.0005592883236448413, 'batch_size': 170, 'step_size': 10, 'gamma': 0.9150682180951092}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:52:37,042][0m Trial 34 finished with value: 0.06947506964206696 and parameters: {'observation_period_num': 8, 'train_rates': 0.9616299298263906, 'learning_rate': 0.0001300452445510422, 'batch_size': 183, 'step_size': 10, 'gamma': 0.949910229836598}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:53:35,752][0m Trial 35 finished with value: 0.04944241791963577 and parameters: {'observation_period_num': 18, 'train_rates': 0.9431524029965712, 'learning_rate': 0.0006467978733440768, 'batch_size': 169, 'step_size': 12, 'gamma': 0.9140892177110975}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:54:29,269][0m Trial 36 finished with value: 0.06701602382103199 and parameters: {'observation_period_num': 25, 'train_rates': 0.8653921182864492, 'learning_rate': 0.00033009326137853226, 'batch_size': 196, 'step_size': 10, 'gamma': 0.9256014052311139}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:55:21,658][0m Trial 37 finished with value: 0.06957366491017276 and parameters: {'observation_period_num': 42, 'train_rates': 0.8996942431335457, 'learning_rate': 0.00014524650696290563, 'batch_size': 138, 'step_size': 14, 'gamma': 0.9092358211847207}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:55:57,222][0m Trial 38 finished with value: 0.25892516796952053 and parameters: {'observation_period_num': 63, 'train_rates': 0.7388508319397262, 'learning_rate': 8.006424545013022e-05, 'batch_size': 178, 'step_size': 12, 'gamma': 0.9823938374752512}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:56:44,312][0m Trial 39 finished with value: 0.11026978492736816 and parameters: {'observation_period_num': 174, 'train_rates': 0.967063756788817, 'learning_rate': 0.0002622991860714388, 'batch_size': 156, 'step_size': 9, 'gamma': 0.8816850630256309}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:57:20,353][0m Trial 40 finished with value: 0.2366010993719101 and parameters: {'observation_period_num': 220, 'train_rates': 0.9401237348679894, 'learning_rate': 1.3501637406236196e-05, 'batch_size': 216, 'step_size': 10, 'gamma': 0.9578009368936637}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:58:02,942][0m Trial 41 finished with value: 0.04752414673566818 and parameters: {'observation_period_num': 18, 'train_rates': 0.9681535535042085, 'learning_rate': 0.0006170701055919135, 'batch_size': 169, 'step_size': 12, 'gamma': 0.9158211699823082}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:58:43,798][0m Trial 42 finished with value: 0.06081460416316986 and parameters: {'observation_period_num': 7, 'train_rates': 0.9678596580061196, 'learning_rate': 0.0006480717460327622, 'batch_size': 190, 'step_size': 11, 'gamma': 0.9291148016232772}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 06:59:31,490][0m Trial 43 finished with value: 0.0499088391661644 and parameters: {'observation_period_num': 21, 'train_rates': 0.96897931782675, 'learning_rate': 0.0004941738911625403, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8951397792419529}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 07:00:15,906][0m Trial 44 finished with value: 0.05207475667576427 and parameters: {'observation_period_num': 44, 'train_rates': 0.9368625274470085, 'learning_rate': 0.0006216364786080867, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8638352776094997}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 07:01:02,266][0m Trial 45 finished with value: 0.07778521102887613 and parameters: {'observation_period_num': 83, 'train_rates': 0.8685226942740032, 'learning_rate': 0.0003550245359232905, 'batch_size': 154, 'step_size': 14, 'gamma': 0.9081141038252194}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 07:01:45,210][0m Trial 46 finished with value: 0.06920527666807175 and parameters: {'observation_period_num': 19, 'train_rates': 0.972893815870399, 'learning_rate': 0.00017286515619851088, 'batch_size': 182, 'step_size': 10, 'gamma': 0.9211348113980966}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 07:02:23,988][0m Trial 47 finished with value: 0.061828188686853365 and parameters: {'observation_period_num': 5, 'train_rates': 0.9146731665515478, 'learning_rate': 0.0009727803396301945, 'batch_size': 174, 'step_size': 13, 'gamma': 0.9403431553769707}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 07:03:12,933][0m Trial 48 finished with value: 0.053864307187754534 and parameters: {'observation_period_num': 28, 'train_rates': 0.9505269767677573, 'learning_rate': 0.00042828518910946743, 'batch_size': 132, 'step_size': 9, 'gamma': 0.8761047513533226}. Best is trial 23 with value: 0.031011607497930527.[0m
[32m[I 2025-01-07 07:03:50,897][0m Trial 49 finished with value: 0.10565831424558864 and parameters: {'observation_period_num': 63, 'train_rates': 0.8764941325216871, 'learning_rate': 2.740716030294945e-05, 'batch_size': 161, 'step_size': 11, 'gamma': 0.9063423372923396}. Best is trial 23 with value: 0.031011607497930527.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 07:03:50,908][0m A new study created in memory with name: no-name-d90670da-a765-4a59-bb0c-ec6c3df1031b[0m
[32m[I 2025-01-07 07:04:49,747][0m Trial 0 finished with value: 0.3208481771352123 and parameters: {'observation_period_num': 167, 'train_rates': 0.6137027701586073, 'learning_rate': 2.1942234704907884e-05, 'batch_size': 72, 'step_size': 8, 'gamma': 0.8647613212621806}. Best is trial 0 with value: 0.3208481771352123.[0m
[32m[I 2025-01-07 07:05:43,655][0m Trial 1 finished with value: 0.24222539005540605 and parameters: {'observation_period_num': 59, 'train_rates': 0.7382505058279152, 'learning_rate': 3.1124172985944866e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.9018645228834801}. Best is trial 1 with value: 0.24222539005540605.[0m
[32m[I 2025-01-07 07:06:13,626][0m Trial 2 finished with value: 0.3729272380154184 and parameters: {'observation_period_num': 90, 'train_rates': 0.7538504817649456, 'learning_rate': 1.1552750879851285e-05, 'batch_size': 205, 'step_size': 14, 'gamma': 0.8563931248722619}. Best is trial 1 with value: 0.24222539005540605.[0m
[32m[I 2025-01-07 07:07:19,707][0m Trial 3 finished with value: 0.13138724738045743 and parameters: {'observation_period_num': 235, 'train_rates': 0.9652943927247191, 'learning_rate': 0.00010591051183019225, 'batch_size': 88, 'step_size': 5, 'gamma': 0.7920513566600733}. Best is trial 3 with value: 0.13138724738045743.[0m
[32m[I 2025-01-07 07:09:10,314][0m Trial 4 finished with value: 0.2593233503483154 and parameters: {'observation_period_num': 107, 'train_rates': 0.7121581905798691, 'learning_rate': 0.0006773336055611054, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9489363552010673}. Best is trial 3 with value: 0.13138724738045743.[0m
[32m[I 2025-01-07 07:09:55,683][0m Trial 5 finished with value: 1.0515211254030794 and parameters: {'observation_period_num': 249, 'train_rates': 0.8656130398519624, 'learning_rate': 7.55055528031616e-06, 'batch_size': 153, 'step_size': 2, 'gamma': 0.799066243368747}. Best is trial 3 with value: 0.13138724738045743.[0m
[32m[I 2025-01-07 07:10:30,796][0m Trial 6 finished with value: 0.29096363457349633 and parameters: {'observation_period_num': 207, 'train_rates': 0.7632989902508405, 'learning_rate': 0.00015677128056119317, 'batch_size': 217, 'step_size': 13, 'gamma': 0.9561394622548066}. Best is trial 3 with value: 0.13138724738045743.[0m
[32m[I 2025-01-07 07:12:21,996][0m Trial 7 finished with value: 0.08729307170708975 and parameters: {'observation_period_num': 226, 'train_rates': 0.8625295763981387, 'learning_rate': 0.00015549804373200438, 'batch_size': 47, 'step_size': 10, 'gamma': 0.753368793830092}. Best is trial 7 with value: 0.08729307170708975.[0m
[32m[I 2025-01-07 07:13:12,227][0m Trial 8 finished with value: 0.29222240929819565 and parameters: {'observation_period_num': 229, 'train_rates': 0.8947525155641489, 'learning_rate': 7.443705813237265e-06, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9005469501269323}. Best is trial 7 with value: 0.08729307170708975.[0m
[32m[I 2025-01-07 07:13:48,742][0m Trial 9 finished with value: 0.15135536041141542 and parameters: {'observation_period_num': 95, 'train_rates': 0.8731132984057342, 'learning_rate': 1.7335147088086352e-05, 'batch_size': 213, 'step_size': 13, 'gamma': 0.8349200282668092}. Best is trial 7 with value: 0.08729307170708975.[0m
[32m[I 2025-01-07 07:18:42,884][0m Trial 10 finished with value: 0.131402536102061 and parameters: {'observation_period_num': 8, 'train_rates': 0.9823311232262044, 'learning_rate': 1.1645067201149151e-06, 'batch_size': 20, 'step_size': 10, 'gamma': 0.7573028040786877}. Best is trial 7 with value: 0.08729307170708975.[0m
[32m[I 2025-01-07 07:19:24,743][0m Trial 11 finished with value: 0.12182540446519852 and parameters: {'observation_period_num': 161, 'train_rates': 0.9859275879299685, 'learning_rate': 0.0001699852199252661, 'batch_size': 151, 'step_size': 4, 'gamma': 0.7588735573855517}. Best is trial 7 with value: 0.08729307170708975.[0m
Early stopping at epoch 46
[32m[I 2025-01-07 07:19:43,988][0m Trial 12 finished with value: 0.26546035250487354 and parameters: {'observation_period_num': 168, 'train_rates': 0.9381160926397, 'learning_rate': 0.00021906609174011582, 'batch_size': 158, 'step_size': 1, 'gamma': 0.7516299766310737}. Best is trial 7 with value: 0.08729307170708975.[0m
[32m[I 2025-01-07 07:20:24,979][0m Trial 13 finished with value: 0.08065927640886092 and parameters: {'observation_period_num': 172, 'train_rates': 0.8244801288522443, 'learning_rate': 0.0008391116291361021, 'batch_size': 131, 'step_size': 4, 'gamma': 0.7973847965344049}. Best is trial 13 with value: 0.08065927640886092.[0m
[32m[I 2025-01-07 07:20:53,735][0m Trial 14 finished with value: 0.10650675837526616 and parameters: {'observation_period_num': 199, 'train_rates': 0.8239199434017608, 'learning_rate': 0.0008566035785397092, 'batch_size': 249, 'step_size': 4, 'gamma': 0.8030823263927434}. Best is trial 13 with value: 0.08065927640886092.[0m
[32m[I 2025-01-07 07:24:52,507][0m Trial 15 finished with value: 0.10520937781468148 and parameters: {'observation_period_num': 141, 'train_rates': 0.8137217204899158, 'learning_rate': 0.0004332218402250474, 'batch_size': 21, 'step_size': 10, 'gamma': 0.8153308661604497}. Best is trial 13 with value: 0.08065927640886092.[0m
[32m[I 2025-01-07 07:25:40,673][0m Trial 16 finished with value: 0.3316878836926998 and parameters: {'observation_period_num': 198, 'train_rates': 0.6754351663825788, 'learning_rate': 8.09604927698997e-05, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7788759438695709}. Best is trial 13 with value: 0.08065927640886092.[0m
[32m[I 2025-01-07 07:27:36,560][0m Trial 17 finished with value: 0.0711484812040869 and parameters: {'observation_period_num': 133, 'train_rates': 0.9171938773355702, 'learning_rate': 0.0004069659989721204, 'batch_size': 55, 'step_size': 3, 'gamma': 0.8252893124230094}. Best is trial 17 with value: 0.0711484812040869.[0m
[32m[I 2025-01-07 07:28:28,835][0m Trial 18 finished with value: 0.08178169835697521 and parameters: {'observation_period_num': 131, 'train_rates': 0.9221960549566898, 'learning_rate': 0.0004111070840187705, 'batch_size': 181, 'step_size': 3, 'gamma': 0.8415757686775666}. Best is trial 17 with value: 0.0711484812040869.[0m
Early stopping at epoch 74
[32m[I 2025-01-07 07:29:46,562][0m Trial 19 finished with value: 0.13319721096286588 and parameters: {'observation_period_num': 58, 'train_rates': 0.824090343256782, 'learning_rate': 6.510168964633455e-05, 'batch_size': 58, 'step_size': 1, 'gamma': 0.8278654630512464}. Best is trial 17 with value: 0.0711484812040869.[0m
[32m[I 2025-01-07 07:30:46,401][0m Trial 20 finished with value: 0.07915242817666795 and parameters: {'observation_period_num': 144, 'train_rates': 0.9199024463365383, 'learning_rate': 0.0009750750788892914, 'batch_size': 104, 'step_size': 3, 'gamma': 0.8971033718889004}. Best is trial 17 with value: 0.0711484812040869.[0m
[32m[I 2025-01-07 07:31:51,721][0m Trial 21 finished with value: 0.07235228822928322 and parameters: {'observation_period_num': 147, 'train_rates': 0.9138027767148056, 'learning_rate': 0.0003637969241834295, 'batch_size': 101, 'step_size': 3, 'gamma': 0.8930936109002271}. Best is trial 17 with value: 0.0711484812040869.[0m
[32m[I 2025-01-07 07:32:58,551][0m Trial 22 finished with value: 0.07204290333649387 and parameters: {'observation_period_num': 117, 'train_rates': 0.9190919988713803, 'learning_rate': 0.00034214940760518754, 'batch_size': 106, 'step_size': 2, 'gamma': 0.8962303010109078}. Best is trial 17 with value: 0.0711484812040869.[0m
[32m[I 2025-01-07 07:34:16,806][0m Trial 23 finished with value: 0.07803441400296594 and parameters: {'observation_period_num': 117, 'train_rates': 0.9464745752590284, 'learning_rate': 0.00039626897669283064, 'batch_size': 75, 'step_size': 2, 'gamma': 0.931891393442778}. Best is trial 17 with value: 0.0711484812040869.[0m
[32m[I 2025-01-07 07:35:43,396][0m Trial 24 finished with value: 0.06864244002167896 and parameters: {'observation_period_num': 76, 'train_rates': 0.9030872803045933, 'learning_rate': 0.0003084549352459026, 'batch_size': 75, 'step_size': 5, 'gamma': 0.8827499650267968}. Best is trial 24 with value: 0.06864244002167896.[0m
[32m[I 2025-01-07 07:37:07,631][0m Trial 25 finished with value: 0.0630591610679403 and parameters: {'observation_period_num': 69, 'train_rates': 0.8669993270569206, 'learning_rate': 4.6837619347795596e-05, 'batch_size': 69, 'step_size': 5, 'gamma': 0.9846218365851005}. Best is trial 25 with value: 0.0630591610679403.[0m
[32m[I 2025-01-07 07:39:42,858][0m Trial 26 finished with value: 0.07712257615908855 and parameters: {'observation_period_num': 63, 'train_rates': 0.8766938816348921, 'learning_rate': 5.11007080866019e-05, 'batch_size': 33, 'step_size': 6, 'gamma': 0.9829148269004989}. Best is trial 25 with value: 0.0630591610679403.[0m
[32m[I 2025-01-07 07:41:05,072][0m Trial 27 finished with value: 0.16602374308543122 and parameters: {'observation_period_num': 31, 'train_rates': 0.7875649784306863, 'learning_rate': 3.420764495336348e-06, 'batch_size': 70, 'step_size': 5, 'gamma': 0.9847660394666221}. Best is trial 25 with value: 0.0630591610679403.[0m
[32m[I 2025-01-07 07:43:30,084][0m Trial 28 finished with value: 0.059754125879339454 and parameters: {'observation_period_num': 78, 'train_rates': 0.8527838016135318, 'learning_rate': 4.741421390252462e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8828520337981132}. Best is trial 28 with value: 0.059754125879339454.[0m
[32m[I 2025-01-07 07:44:48,164][0m Trial 29 finished with value: 0.06327863904578153 and parameters: {'observation_period_num': 76, 'train_rates': 0.8460921590869093, 'learning_rate': 4.5887942041457194e-05, 'batch_size': 79, 'step_size': 8, 'gamma': 0.9275933727473971}. Best is trial 28 with value: 0.059754125879339454.[0m
[32m[I 2025-01-07 07:46:46,621][0m Trial 30 finished with value: 0.15724604283825735 and parameters: {'observation_period_num': 36, 'train_rates': 0.6340646792506072, 'learning_rate': 3.643611703282851e-05, 'batch_size': 40, 'step_size': 8, 'gamma': 0.9227115562775368}. Best is trial 28 with value: 0.059754125879339454.[0m
[32m[I 2025-01-07 07:48:04,890][0m Trial 31 finished with value: 0.09431207016722797 and parameters: {'observation_period_num': 76, 'train_rates': 0.8480021754767069, 'learning_rate': 2.4443089883411562e-05, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8704800787653441}. Best is trial 28 with value: 0.059754125879339454.[0m
[32m[I 2025-01-07 07:49:11,274][0m Trial 32 finished with value: 0.10254020562501458 and parameters: {'observation_period_num': 78, 'train_rates': 0.7862271758210378, 'learning_rate': 3.848593920722936e-05, 'batch_size': 83, 'step_size': 8, 'gamma': 0.8788507942952262}. Best is trial 28 with value: 0.059754125879339454.[0m
[32m[I 2025-01-07 07:50:38,683][0m Trial 33 finished with value: 0.05711120201295681 and parameters: {'observation_period_num': 48, 'train_rates': 0.8467737743417508, 'learning_rate': 9.895051312213642e-05, 'batch_size': 64, 'step_size': 6, 'gamma': 0.9171536448214118}. Best is trial 33 with value: 0.05711120201295681.[0m
[32m[I 2025-01-07 07:52:07,433][0m Trial 34 finished with value: 0.05181681604432329 and parameters: {'observation_period_num': 42, 'train_rates': 0.84304555630957, 'learning_rate': 7.828923073636323e-05, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9203755679562076}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 07:54:52,693][0m Trial 35 finished with value: 0.06359822738907459 and parameters: {'observation_period_num': 39, 'train_rates': 0.84459609056085, 'learning_rate': 9.616150315055688e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.9678311093524394}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 07:56:17,164][0m Trial 36 finished with value: 0.07476037071034639 and parameters: {'observation_period_num': 5, 'train_rates': 0.7989308508027746, 'learning_rate': 1.6337417879979605e-05, 'batch_size': 62, 'step_size': 7, 'gamma': 0.9127827761184134}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 07:57:11,239][0m Trial 37 finished with value: 0.17945551009383054 and parameters: {'observation_period_num': 21, 'train_rates': 0.7272451913851365, 'learning_rate': 6.409235642774426e-05, 'batch_size': 93, 'step_size': 9, 'gamma': 0.9395551376017329}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:02:44,865][0m Trial 38 finished with value: 0.059993080322115803 and parameters: {'observation_period_num': 52, 'train_rates': 0.8859693591026059, 'learning_rate': 0.00012024919013017126, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8554019740392177}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:07:26,933][0m Trial 39 finished with value: 0.21406934449238668 and parameters: {'observation_period_num': 49, 'train_rates': 0.7592987839753172, 'learning_rate': 0.0001288987581409086, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8622442343464963}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:09:24,404][0m Trial 40 finished with value: 0.08744235223309062 and parameters: {'observation_period_num': 94, 'train_rates': 0.8785629965407205, 'learning_rate': 0.0002096701435947671, 'batch_size': 46, 'step_size': 9, 'gamma': 0.8504916267361776}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:12:14,554][0m Trial 41 finished with value: 0.10940530933439732 and parameters: {'observation_period_num': 50, 'train_rates': 0.8898016848136431, 'learning_rate': 0.00010312732010478451, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9142808800277001}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:13:56,502][0m Trial 42 finished with value: 0.056168933375252375 and parameters: {'observation_period_num': 25, 'train_rates': 0.8530185867394631, 'learning_rate': 2.339800456268481e-05, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9487533602992586}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:15:46,807][0m Trial 43 finished with value: 0.05655153287631093 and parameters: {'observation_period_num': 22, 'train_rates': 0.8424124822861961, 'learning_rate': 2.1481686638794847e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9520028173404773}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:17:29,310][0m Trial 44 finished with value: 0.0700020535863885 and parameters: {'observation_period_num': 21, 'train_rates': 0.8504431126820118, 'learning_rate': 1.0367140148194706e-05, 'batch_size': 54, 'step_size': 6, 'gamma': 0.9534146699951973}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:19:50,265][0m Trial 45 finished with value: 0.05250924591846923 and parameters: {'observation_period_num': 21, 'train_rates': 0.8077365043805713, 'learning_rate': 2.603185882725144e-05, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9435090870046146}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:21:41,405][0m Trial 46 finished with value: 0.0593125369816648 and parameters: {'observation_period_num': 20, 'train_rates': 0.808153658694864, 'learning_rate': 2.5473481814672962e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.943647650693445}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:22:43,173][0m Trial 47 finished with value: 0.21659819882960968 and parameters: {'observation_period_num': 27, 'train_rates': 0.7786715961410061, 'learning_rate': 1.5961677089607697e-05, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9610277141545805}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:24:05,093][0m Trial 48 finished with value: 0.2526375299482668 and parameters: {'observation_period_num': 42, 'train_rates': 0.7410861319922215, 'learning_rate': 8.376279861577774e-06, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9648108396665089}. Best is trial 34 with value: 0.05181681604432329.[0m
[32m[I 2025-01-07 08:27:06,629][0m Trial 49 finished with value: 0.07074728569726355 and parameters: {'observation_period_num': 16, 'train_rates': 0.8350043917797598, 'learning_rate': 4.642240009237393e-06, 'batch_size': 30, 'step_size': 15, 'gamma': 0.9099834163733843}. Best is trial 34 with value: 0.05181681604432329.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 08:27:06,639][0m A new study created in memory with name: no-name-ff13b887-46fb-49f9-9d39-6f023923a474[0m
[32m[I 2025-01-07 08:27:42,125][0m Trial 0 finished with value: 1.3742200546073366 and parameters: {'observation_period_num': 178, 'train_rates': 0.6228163053012671, 'learning_rate': 1.345332562152055e-06, 'batch_size': 216, 'step_size': 10, 'gamma': 0.7629055358750217}. Best is trial 0 with value: 1.3742200546073366.[0m
[32m[I 2025-01-07 08:30:17,064][0m Trial 1 finished with value: 0.1123235896229744 and parameters: {'observation_period_num': 126, 'train_rates': 0.9880154914476916, 'learning_rate': 1.4546592925996612e-05, 'batch_size': 43, 'step_size': 3, 'gamma': 0.9704024051227383}. Best is trial 1 with value: 0.1123235896229744.[0m
[32m[I 2025-01-07 08:31:23,444][0m Trial 2 finished with value: 0.33587395897495886 and parameters: {'observation_period_num': 184, 'train_rates': 0.7258942230655102, 'learning_rate': 2.0015579178157613e-05, 'batch_size': 84, 'step_size': 12, 'gamma': 0.8726695134130162}. Best is trial 1 with value: 0.1123235896229744.[0m
[32m[I 2025-01-07 08:31:55,600][0m Trial 3 finished with value: 0.1869123063664487 and parameters: {'observation_period_num': 67, 'train_rates': 0.6743418402440424, 'learning_rate': 0.0003266450517914767, 'batch_size': 255, 'step_size': 8, 'gamma': 0.7782984251138432}. Best is trial 1 with value: 0.1123235896229744.[0m
[32m[I 2025-01-07 08:32:51,714][0m Trial 4 finished with value: 0.13826777643025523 and parameters: {'observation_period_num': 127, 'train_rates': 0.7898331570486606, 'learning_rate': 4.3308912547440884e-05, 'batch_size': 224, 'step_size': 8, 'gamma': 0.9005738135041859}. Best is trial 1 with value: 0.1123235896229744.[0m
[32m[I 2025-01-07 08:34:42,962][0m Trial 5 finished with value: 0.08084338088830312 and parameters: {'observation_period_num': 98, 'train_rates': 0.8688152132225921, 'learning_rate': 0.0006107448984342808, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7595708611597409}. Best is trial 5 with value: 0.08084338088830312.[0m
[32m[I 2025-01-07 08:39:05,647][0m Trial 6 finished with value: 0.24636056549175614 and parameters: {'observation_period_num': 158, 'train_rates': 0.750066122897466, 'learning_rate': 1.8875295439974925e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.9835871169713315}. Best is trial 5 with value: 0.08084338088830312.[0m
[32m[I 2025-01-07 08:39:49,472][0m Trial 7 finished with value: 0.18283385194318238 and parameters: {'observation_period_num': 7, 'train_rates': 0.7810001563912677, 'learning_rate': 0.0002269027677527261, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9238595343802559}. Best is trial 5 with value: 0.08084338088830312.[0m
[32m[I 2025-01-07 08:40:29,257][0m Trial 8 finished with value: 0.3104214823771925 and parameters: {'observation_period_num': 138, 'train_rates': 0.6980315488958071, 'learning_rate': 3.314860967836225e-05, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8787078462187139}. Best is trial 5 with value: 0.08084338088830312.[0m
[32m[I 2025-01-07 08:41:03,508][0m Trial 9 finished with value: 0.05029183548688888 and parameters: {'observation_period_num': 39, 'train_rates': 0.8286058795410687, 'learning_rate': 0.00030405753531099116, 'batch_size': 252, 'step_size': 4, 'gamma': 0.9076193377559674}. Best is trial 9 with value: 0.05029183548688888.[0m
[32m[I 2025-01-07 08:41:53,628][0m Trial 10 finished with value: 0.2320811280316735 and parameters: {'observation_period_num': 248, 'train_rates': 0.8830197280945045, 'learning_rate': 0.0001053870556344213, 'batch_size': 157, 'step_size': 2, 'gamma': 0.8065975478296328}. Best is trial 9 with value: 0.05029183548688888.[0m
[32m[I 2025-01-07 08:42:51,416][0m Trial 11 finished with value: 0.045393394324563296 and parameters: {'observation_period_num': 55, 'train_rates': 0.8595074245414477, 'learning_rate': 0.000742281198053558, 'batch_size': 131, 'step_size': 5, 'gamma': 0.8310935969719261}. Best is trial 11 with value: 0.045393394324563296.[0m
[32m[I 2025-01-07 08:43:51,893][0m Trial 12 finished with value: 0.031749921288186066 and parameters: {'observation_period_num': 7, 'train_rates': 0.8696541178840292, 'learning_rate': 0.0009702505495014734, 'batch_size': 139, 'step_size': 5, 'gamma': 0.8241617900856872}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:44:51,250][0m Trial 13 finished with value: 0.03240823931992054 and parameters: {'observation_period_num': 16, 'train_rates': 0.9455493174580587, 'learning_rate': 0.000880185453103823, 'batch_size': 145, 'step_size': 6, 'gamma': 0.8239896477804018}. Best is trial 12 with value: 0.031749921288186066.[0m
Early stopping at epoch 72
[32m[I 2025-01-07 08:45:34,086][0m Trial 14 finished with value: 0.05835556841436648 and parameters: {'observation_period_num': 7, 'train_rates': 0.9497196907587915, 'learning_rate': 0.0009212910430040588, 'batch_size': 141, 'step_size': 1, 'gamma': 0.8348206226169645}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:46:20,388][0m Trial 15 finished with value: 0.48861604928970337 and parameters: {'observation_period_num': 84, 'train_rates': 0.923053433587431, 'learning_rate': 3.324536261690519e-06, 'batch_size': 176, 'step_size': 6, 'gamma': 0.8321882714883011}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:47:30,862][0m Trial 16 finished with value: 0.04596139929656472 and parameters: {'observation_period_num': 31, 'train_rates': 0.9235575530646967, 'learning_rate': 0.00013582040283594183, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8004999585887873}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:48:18,134][0m Trial 17 finished with value: 0.05767547501703349 and parameters: {'observation_period_num': 29, 'train_rates': 0.8338674535769384, 'learning_rate': 7.399870640514235e-05, 'batch_size': 187, 'step_size': 10, 'gamma': 0.8562549469640299}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:49:23,261][0m Trial 18 finished with value: 0.07724910229444504 and parameters: {'observation_period_num': 92, 'train_rates': 0.9769125175955897, 'learning_rate': 0.0003651656202255418, 'batch_size': 113, 'step_size': 3, 'gamma': 0.7951202975030185}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:50:14,657][0m Trial 19 finished with value: 0.35733869083133746 and parameters: {'observation_period_num': 226, 'train_rates': 0.9110193530202796, 'learning_rate': 6.542927060559045e-06, 'batch_size': 167, 'step_size': 7, 'gamma': 0.8434061842710264}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:51:37,926][0m Trial 20 finished with value: 0.06487342543665349 and parameters: {'observation_period_num': 63, 'train_rates': 0.8244247328830067, 'learning_rate': 0.0001733832529992864, 'batch_size': 82, 'step_size': 1, 'gamma': 0.9431694313489714}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:52:38,727][0m Trial 21 finished with value: 0.04818449475062199 and parameters: {'observation_period_num': 44, 'train_rates': 0.8793044931559895, 'learning_rate': 0.0008843164814834206, 'batch_size': 126, 'step_size': 5, 'gamma': 0.821252727564524}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:53:26,074][0m Trial 22 finished with value: 0.03540936844556767 and parameters: {'observation_period_num': 9, 'train_rates': 0.8530135023952305, 'learning_rate': 0.0005284878597896983, 'batch_size': 141, 'step_size': 5, 'gamma': 0.8561868696514621}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:54:16,095][0m Trial 23 finished with value: 0.037340208888053894 and parameters: {'observation_period_num': 10, 'train_rates': 0.9507214894279735, 'learning_rate': 0.0004153579905077587, 'batch_size': 149, 'step_size': 6, 'gamma': 0.8575613926496917}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:54:56,663][0m Trial 24 finished with value: 0.04518511589245288 and parameters: {'observation_period_num': 23, 'train_rates': 0.9009024420597864, 'learning_rate': 0.00046900380003683713, 'batch_size': 190, 'step_size': 4, 'gamma': 0.8126846157135451}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:55:34,187][0m Trial 25 finished with value: 0.056939525998629816 and parameters: {'observation_period_num': 79, 'train_rates': 0.8494701186532876, 'learning_rate': 0.0009673110857748139, 'batch_size': 208, 'step_size': 9, 'gamma': 0.7827115071389101}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:56:44,931][0m Trial 26 finished with value: 0.06612152314069224 and parameters: {'observation_period_num': 48, 'train_rates': 0.9473759651882654, 'learning_rate': 7.165325656505742e-05, 'batch_size': 100, 'step_size': 6, 'gamma': 0.8811655521581224}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:57:43,894][0m Trial 27 finished with value: 0.04932932787149855 and parameters: {'observation_period_num': 14, 'train_rates': 0.8949774327130544, 'learning_rate': 0.00021565372820617698, 'batch_size': 122, 'step_size': 3, 'gamma': 0.8537603765761465}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:58:34,321][0m Trial 28 finished with value: 0.04176869523898406 and parameters: {'observation_period_num': 32, 'train_rates': 0.8004981840660532, 'learning_rate': 0.00043184616359070874, 'batch_size': 156, 'step_size': 7, 'gamma': 0.8946137427533327}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 08:59:29,981][0m Trial 29 finished with value: 0.8518340931804108 and parameters: {'observation_period_num': 195, 'train_rates': 0.6046315508978042, 'learning_rate': 1.5764623425014817e-06, 'batch_size': 81, 'step_size': 12, 'gamma': 0.7786667806757828}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:00:13,768][0m Trial 30 finished with value: 0.24122809882448837 and parameters: {'observation_period_num': 110, 'train_rates': 0.7616090895864127, 'learning_rate': 0.0005734532859548125, 'batch_size': 194, 'step_size': 4, 'gamma': 0.8203228609010531}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:01:00,303][0m Trial 31 finished with value: 0.035635463893413544 and parameters: {'observation_period_num': 6, 'train_rates': 0.9572159903956916, 'learning_rate': 0.000542367804256841, 'batch_size': 140, 'step_size': 6, 'gamma': 0.8610218379933778}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:01:46,323][0m Trial 32 finished with value: 0.04213293641805649 and parameters: {'observation_period_num': 24, 'train_rates': 0.9719201803823734, 'learning_rate': 0.0006094715280387384, 'batch_size': 142, 'step_size': 6, 'gamma': 0.8514603299201657}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:02:24,178][0m Trial 33 finished with value: 0.04014338346628042 and parameters: {'observation_period_num': 7, 'train_rates': 0.9338703199060592, 'learning_rate': 0.00025639386780730404, 'batch_size': 168, 'step_size': 5, 'gamma': 0.8661134645046087}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:03:20,584][0m Trial 34 finished with value: 0.09965023398399353 and parameters: {'observation_period_num': 67, 'train_rates': 0.9825178170144115, 'learning_rate': 0.0009535916980144829, 'batch_size': 109, 'step_size': 9, 'gamma': 0.886923372360556}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:04:12,253][0m Trial 35 finished with value: 0.06718532741069794 and parameters: {'observation_period_num': 46, 'train_rates': 0.9601995401931149, 'learning_rate': 0.00013254271888900036, 'batch_size': 139, 'step_size': 8, 'gamma': 0.8406137306673994}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:05:20,192][0m Trial 36 finished with value: 0.18477612733840942 and parameters: {'observation_period_num': 22, 'train_rates': 0.9892476100707213, 'learning_rate': 1.058412080384917e-05, 'batch_size': 93, 'step_size': 3, 'gamma': 0.8669720446484437}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:06:08,182][0m Trial 37 finished with value: 0.04645915050929952 and parameters: {'observation_period_num': 56, 'train_rates': 0.8523032076201739, 'learning_rate': 0.0005393980424339426, 'batch_size': 122, 'step_size': 7, 'gamma': 0.7902132837446145}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:06:47,230][0m Trial 38 finished with value: 0.03767999892688205 and parameters: {'observation_period_num': 18, 'train_rates': 0.8007410048987791, 'learning_rate': 0.00032400729863379664, 'batch_size': 157, 'step_size': 4, 'gamma': 0.9233040750187019}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:08:43,503][0m Trial 39 finished with value: 0.089340856919686 and parameters: {'observation_period_num': 149, 'train_rates': 0.909167052542745, 'learning_rate': 0.0006520088935526068, 'batch_size': 47, 'step_size': 8, 'gamma': 0.7532482522834253}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:10:18,245][0m Trial 40 finished with value: 0.0766985898105879 and parameters: {'observation_period_num': 36, 'train_rates': 0.8769450892419246, 'learning_rate': 6.494010908701374e-05, 'batch_size': 63, 'step_size': 2, 'gamma': 0.8186995002017176}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:11:02,496][0m Trial 41 finished with value: 0.0363672088417742 and parameters: {'observation_period_num': 5, 'train_rates': 0.9389592867294874, 'learning_rate': 0.0003932518547546795, 'batch_size': 148, 'step_size': 6, 'gamma': 0.8666334818928868}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:11:48,566][0m Trial 42 finished with value: 0.04127331905690907 and parameters: {'observation_period_num': 17, 'train_rates': 0.931027702568494, 'learning_rate': 0.0001955987584152291, 'batch_size': 132, 'step_size': 5, 'gamma': 0.848152737118236}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:12:25,136][0m Trial 43 finished with value: 0.032150097781478766 and parameters: {'observation_period_num': 5, 'train_rates': 0.8974924838500118, 'learning_rate': 0.0006916420148085398, 'batch_size': 174, 'step_size': 6, 'gamma': 0.8699671594412681}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:12:56,918][0m Trial 44 finished with value: 0.03989463697308057 and parameters: {'observation_period_num': 39, 'train_rates': 0.8911858831094314, 'learning_rate': 0.0006991413072710764, 'batch_size': 207, 'step_size': 7, 'gamma': 0.9085596965048787}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:13:27,610][0m Trial 45 finished with value: 0.15753477890143744 and parameters: {'observation_period_num': 20, 'train_rates': 0.6546222144516896, 'learning_rate': 0.0002911752207802652, 'batch_size': 173, 'step_size': 4, 'gamma': 0.8305311828596608}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:14:02,929][0m Trial 46 finished with value: 0.03878211574417818 and parameters: {'observation_period_num': 6, 'train_rates': 0.8654369463011506, 'learning_rate': 0.0007647398308517871, 'batch_size': 182, 'step_size': 5, 'gamma': 0.7679443702605857}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:14:37,900][0m Trial 47 finished with value: 0.08597384318376758 and parameters: {'observation_period_num': 174, 'train_rates': 0.8390232311043173, 'learning_rate': 0.0004895781579353928, 'batch_size': 163, 'step_size': 9, 'gamma': 0.8725916370687459}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:15:07,939][0m Trial 48 finished with value: 0.2112529617768747 and parameters: {'observation_period_num': 75, 'train_rates': 0.8124330628095237, 'learning_rate': 2.9996459696208236e-05, 'batch_size': 227, 'step_size': 6, 'gamma': 0.8057675966873404}. Best is trial 12 with value: 0.031749921288186066.[0m
[32m[I 2025-01-07 09:16:01,290][0m Trial 49 finished with value: 0.04821378626413159 and parameters: {'observation_period_num': 52, 'train_rates': 0.9204484727818016, 'learning_rate': 0.0007065025334319926, 'batch_size': 118, 'step_size': 5, 'gamma': 0.8883253478264272}. Best is trial 12 with value: 0.031749921288186066.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 09:16:01,300][0m A new study created in memory with name: no-name-121fa2f8-cd3a-4166-97c4-ed64d90207e7[0m
[32m[I 2025-01-07 09:16:38,898][0m Trial 0 finished with value: 0.2445398886700285 and parameters: {'observation_period_num': 221, 'train_rates': 0.6099574209830905, 'learning_rate': 0.0006603713817707836, 'batch_size': 214, 'step_size': 5, 'gamma': 0.8064817234265502}. Best is trial 0 with value: 0.2445398886700285.[0m
[32m[I 2025-01-07 09:17:08,268][0m Trial 1 finished with value: 0.2995479444983185 and parameters: {'observation_period_num': 249, 'train_rates': 0.7937483560994656, 'learning_rate': 5.0349576830557734e-05, 'batch_size': 179, 'step_size': 1, 'gamma': 0.909713579267762}. Best is trial 0 with value: 0.2445398886700285.[0m
[32m[I 2025-01-07 09:18:44,141][0m Trial 2 finished with value: 0.2937433500605893 and parameters: {'observation_period_num': 123, 'train_rates': 0.7068780851148891, 'learning_rate': 2.6584090255377484e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.8327183985652337}. Best is trial 0 with value: 0.2445398886700285.[0m
Early stopping at epoch 59
[32m[I 2025-01-07 09:19:01,074][0m Trial 3 finished with value: 0.6796879235594566 and parameters: {'observation_period_num': 162, 'train_rates': 0.6911274234857945, 'learning_rate': 5.06641427575022e-05, 'batch_size': 201, 'step_size': 1, 'gamma': 0.7940081538925828}. Best is trial 0 with value: 0.2445398886700285.[0m
[32m[I 2025-01-07 09:20:23,775][0m Trial 4 finished with value: 0.10954635590314865 and parameters: {'observation_period_num': 246, 'train_rates': 0.9775299528018482, 'learning_rate': 0.0006644818465177606, 'batch_size': 67, 'step_size': 4, 'gamma': 0.9112817627046643}. Best is trial 4 with value: 0.10954635590314865.[0m
[32m[I 2025-01-07 09:21:05,401][0m Trial 5 finished with value: 0.06762022826296743 and parameters: {'observation_period_num': 92, 'train_rates': 0.9108162118476868, 'learning_rate': 0.0003506745936770808, 'batch_size': 151, 'step_size': 1, 'gamma': 0.9434244292530114}. Best is trial 5 with value: 0.06762022826296743.[0m
[32m[I 2025-01-07 09:21:39,617][0m Trial 6 finished with value: 0.46500706672668457 and parameters: {'observation_period_num': 151, 'train_rates': 0.9480843356482194, 'learning_rate': 4.845516797926808e-06, 'batch_size': 223, 'step_size': 13, 'gamma': 0.79739194180984}. Best is trial 5 with value: 0.06762022826296743.[0m
[32m[I 2025-01-07 09:22:16,176][0m Trial 7 finished with value: 0.3910858631134033 and parameters: {'observation_period_num': 71, 'train_rates': 0.9758806517875069, 'learning_rate': 4.136518552331471e-06, 'batch_size': 193, 'step_size': 4, 'gamma': 0.9288969283040103}. Best is trial 5 with value: 0.06762022826296743.[0m
[32m[I 2025-01-07 09:23:13,182][0m Trial 8 finished with value: 0.6964750791075586 and parameters: {'observation_period_num': 99, 'train_rates': 0.6951648777614026, 'learning_rate': 2.3106799756119017e-06, 'batch_size': 83, 'step_size': 10, 'gamma': 0.8161024649586448}. Best is trial 5 with value: 0.06762022826296743.[0m
[32m[I 2025-01-07 09:23:59,726][0m Trial 9 finished with value: 0.2391791619352629 and parameters: {'observation_period_num': 166, 'train_rates': 0.6580276234267776, 'learning_rate': 5.6827722041279546e-05, 'batch_size': 100, 'step_size': 12, 'gamma': 0.8176869642572515}. Best is trial 5 with value: 0.06762022826296743.[0m
[32m[I 2025-01-07 09:24:47,070][0m Trial 10 finished with value: 0.03449223102513239 and parameters: {'observation_period_num': 6, 'train_rates': 0.8823759425263726, 'learning_rate': 0.00023984027236768945, 'batch_size': 138, 'step_size': 8, 'gamma': 0.9791164500073781}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:25:53,549][0m Trial 11 finished with value: 0.03767267513004216 and parameters: {'observation_period_num': 8, 'train_rates': 0.8769486210885067, 'learning_rate': 0.00016131496007785039, 'batch_size': 129, 'step_size': 8, 'gamma': 0.9839391219922454}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:26:56,154][0m Trial 12 finished with value: 0.03477713817760383 and parameters: {'observation_period_num': 5, 'train_rates': 0.8519859669849706, 'learning_rate': 0.00018573835045507717, 'batch_size': 125, 'step_size': 8, 'gamma': 0.9872947220715982}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:27:46,613][0m Trial 13 finished with value: 0.03884866820841 and parameters: {'observation_period_num': 8, 'train_rates': 0.8230289995560197, 'learning_rate': 0.00021977225723970178, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9882708067379464}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:28:41,745][0m Trial 14 finished with value: 0.054362660057322924 and parameters: {'observation_period_num': 50, 'train_rates': 0.822460788829222, 'learning_rate': 0.00012924672055137553, 'batch_size': 120, 'step_size': 15, 'gamma': 0.754514742193325}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:33:01,090][0m Trial 15 finished with value: 0.04755280375501464 and parameters: {'observation_period_num': 39, 'train_rates': 0.877235303582109, 'learning_rate': 1.5032132822806404e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.9559446153669989}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:33:50,563][0m Trial 16 finished with value: 0.19531831704080105 and parameters: {'observation_period_num': 38, 'train_rates': 0.763934222557663, 'learning_rate': 0.00010470511473454932, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8782941538006117}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:34:50,358][0m Trial 17 finished with value: 0.03875054842805794 and parameters: {'observation_period_num': 5, 'train_rates': 0.8817952787410253, 'learning_rate': 0.0009488349197329631, 'batch_size': 109, 'step_size': 10, 'gamma': 0.9631776227321048}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:35:34,818][0m Trial 18 finished with value: 0.35922707120577496 and parameters: {'observation_period_num': 53, 'train_rates': 0.7684213038407084, 'learning_rate': 1.2701190877644493e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.880872043927555}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:36:40,585][0m Trial 19 finished with value: 0.10758909731429087 and parameters: {'observation_period_num': 197, 'train_rates': 0.919834786109046, 'learning_rate': 0.00030671591926479687, 'batch_size': 90, 'step_size': 12, 'gamma': 0.85080621683779}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:37:25,799][0m Trial 20 finished with value: 0.06814481508705983 and parameters: {'observation_period_num': 80, 'train_rates': 0.8395673304696022, 'learning_rate': 0.00042465678466360095, 'batch_size': 137, 'step_size': 9, 'gamma': 0.969106145824065}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:38:16,024][0m Trial 21 finished with value: 0.04409794759645005 and parameters: {'observation_period_num': 23, 'train_rates': 0.8649984621848971, 'learning_rate': 0.00010796533225724, 'batch_size': 122, 'step_size': 7, 'gamma': 0.9883322360613355}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:39:03,698][0m Trial 22 finished with value: 0.03906344757711843 and parameters: {'observation_period_num': 21, 'train_rates': 0.9091147474242421, 'learning_rate': 0.00019239150018262132, 'batch_size': 140, 'step_size': 8, 'gamma': 0.9377191393526478}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:39:41,865][0m Trial 23 finished with value: 0.054044045915228905 and parameters: {'observation_period_num': 64, 'train_rates': 0.8481565251319401, 'learning_rate': 0.00015360828308856532, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9738415880041782}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:40:31,926][0m Trial 24 finished with value: 0.046143535357177926 and parameters: {'observation_period_num': 27, 'train_rates': 0.9392788230858734, 'learning_rate': 9.493327446284826e-05, 'batch_size': 127, 'step_size': 9, 'gamma': 0.9192971286396951}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:41:52,415][0m Trial 25 finished with value: 0.034719594033460505 and parameters: {'observation_period_num': 8, 'train_rates': 0.8017193617464109, 'learning_rate': 0.0002772609640964797, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9897289735279398}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:43:15,207][0m Trial 26 finished with value: 0.18681169518665092 and parameters: {'observation_period_num': 38, 'train_rates': 0.749633602938963, 'learning_rate': 0.0004303544266984749, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9501707253857957}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:45:49,568][0m Trial 27 finished with value: 0.09169841182462844 and parameters: {'observation_period_num': 114, 'train_rates': 0.8092973227091805, 'learning_rate': 0.00027494479987338677, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8908627513694666}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:47:04,307][0m Trial 28 finished with value: 0.1929451801169377 and parameters: {'observation_period_num': 59, 'train_rates': 0.7304661864964931, 'learning_rate': 2.9051681785239298e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.9701229728637923}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:47:56,563][0m Trial 29 finished with value: 0.14485430914801126 and parameters: {'observation_period_num': 199, 'train_rates': 0.7861083598322909, 'learning_rate': 0.0006736428884585513, 'batch_size': 104, 'step_size': 6, 'gamma': 0.9581287959855994}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:49:55,769][0m Trial 30 finished with value: 0.03525767637111298 and parameters: {'observation_period_num': 20, 'train_rates': 0.8526321244327597, 'learning_rate': 7.515133305579216e-05, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9318037297176309}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:52:11,817][0m Trial 31 finished with value: 0.03906180438819436 and parameters: {'observation_period_num': 22, 'train_rates': 0.8453714224189599, 'learning_rate': 0.0005107403034809176, 'batch_size': 44, 'step_size': 11, 'gamma': 0.9779516098735743}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 09:57:06,470][0m Trial 32 finished with value: 0.035180006361868535 and parameters: {'observation_period_num': 5, 'train_rates': 0.7994226434044074, 'learning_rate': 6.255185805174671e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9390656108636523}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:01:07,669][0m Trial 33 finished with value: 0.039699810336936606 and parameters: {'observation_period_num': 5, 'train_rates': 0.7949135386330042, 'learning_rate': 6.621266731976768e-05, 'batch_size': 22, 'step_size': 9, 'gamma': 0.9895728020690466}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:01:57,718][0m Trial 34 finished with value: 0.06322185333721257 and parameters: {'observation_period_num': 40, 'train_rates': 0.8155330095795688, 'learning_rate': 4.18058362907457e-05, 'batch_size': 173, 'step_size': 8, 'gamma': 0.9501554769854567}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:02:58,716][0m Trial 35 finished with value: 0.14280965277483543 and parameters: {'observation_period_num': 29, 'train_rates': 0.6163339618653872, 'learning_rate': 0.00021424142354092797, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9656406957177374}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:03:56,142][0m Trial 36 finished with value: 0.13805654786135022 and parameters: {'observation_period_num': 78, 'train_rates': 0.9010789182783665, 'learning_rate': 1.9097289061389974e-05, 'batch_size': 186, 'step_size': 5, 'gamma': 0.9400389333421918}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:05:28,076][0m Trial 37 finished with value: 0.056946225860268596 and parameters: {'observation_period_num': 43, 'train_rates': 0.7868200962074751, 'learning_rate': 3.972630671062618e-05, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9047311601549062}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:06:00,351][0m Trial 38 finished with value: 0.15427633258542192 and parameters: {'observation_period_num': 14, 'train_rates': 0.7180296987381083, 'learning_rate': 0.0009973753094886495, 'batch_size': 216, 'step_size': 3, 'gamma': 0.8560851879554214}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:08:29,510][0m Trial 39 finished with value: 0.2567870758778622 and parameters: {'observation_period_num': 136, 'train_rates': 0.7485789460802973, 'learning_rate': 0.00027725189301261975, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9216609803979908}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:09:15,134][0m Trial 40 finished with value: 0.1023690328001976 and parameters: {'observation_period_num': 89, 'train_rates': 0.9505994465577071, 'learning_rate': 0.0005712344107856385, 'batch_size': 147, 'step_size': 5, 'gamma': 0.9765434339832374}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:11:18,935][0m Trial 41 finished with value: 0.03486987776690867 and parameters: {'observation_period_num': 20, 'train_rates': 0.8605025739604394, 'learning_rate': 7.321682070702788e-05, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9379412116113711}. Best is trial 10 with value: 0.03449223102513239.[0m
[32m[I 2025-01-07 10:13:16,014][0m Trial 42 finished with value: 0.02878747101690323 and parameters: {'observation_period_num': 16, 'train_rates': 0.8892739102352035, 'learning_rate': 8.164798088119659e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9079705323893845}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:14:35,386][0m Trial 43 finished with value: 0.042532171919697624 and parameters: {'observation_period_num': 29, 'train_rates': 0.8958636570805325, 'learning_rate': 8.471002609242737e-05, 'batch_size': 73, 'step_size': 14, 'gamma': 0.8981572250972001}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:16:25,636][0m Trial 44 finished with value: 0.02885610654385169 and parameters: {'observation_period_num': 16, 'train_rates': 0.9282149856381022, 'learning_rate': 0.000127013769266356, 'batch_size': 54, 'step_size': 13, 'gamma': 0.9493058661961838}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:17:33,110][0m Trial 45 finished with value: 0.053099018807458404 and parameters: {'observation_period_num': 53, 'train_rates': 0.930433173771488, 'learning_rate': 0.00013946793429041432, 'batch_size': 91, 'step_size': 15, 'gamma': 0.9166337705815202}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:19:24,883][0m Trial 46 finished with value: 0.03969416115432978 and parameters: {'observation_period_num': 31, 'train_rates': 0.9810480410826808, 'learning_rate': 0.000209962119499102, 'batch_size': 54, 'step_size': 13, 'gamma': 0.9531004785799092}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:20:19,334][0m Trial 47 finished with value: 0.03179645331791874 and parameters: {'observation_period_num': 16, 'train_rates': 0.9500467158546256, 'learning_rate': 0.0003696647889469065, 'batch_size': 115, 'step_size': 14, 'gamma': 0.9795317211684421}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:21:34,256][0m Trial 48 finished with value: 0.07669177526185493 and parameters: {'observation_period_num': 69, 'train_rates': 0.9589276057691559, 'learning_rate': 0.0003468310102453189, 'batch_size': 80, 'step_size': 13, 'gamma': 0.9775661575391198}. Best is trial 42 with value: 0.02878747101690323.[0m
[32m[I 2025-01-07 10:22:41,095][0m Trial 49 finished with value: 0.06516152541569578 and parameters: {'observation_period_num': 46, 'train_rates': 0.957723630666735, 'learning_rate': 0.00013524927650367544, 'batch_size': 94, 'step_size': 14, 'gamma': 0.9603156365613166}. Best is trial 42 with value: 0.02878747101690323.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 10:22:41,106][0m A new study created in memory with name: no-name-08f037e3-d52a-4a56-adb4-2bd8923cf600[0m
[32m[I 2025-01-07 10:23:25,654][0m Trial 0 finished with value: 0.2449273318052292 and parameters: {'observation_period_num': 110, 'train_rates': 0.9596534849306677, 'learning_rate': 4.228318544070256e-05, 'batch_size': 213, 'step_size': 5, 'gamma': 0.7508464803513144}. Best is trial 0 with value: 0.2449273318052292.[0m
[32m[I 2025-01-07 10:23:57,210][0m Trial 1 finished with value: 0.40208423137664795 and parameters: {'observation_period_num': 242, 'train_rates': 0.9606810810333501, 'learning_rate': 8.027452290825531e-06, 'batch_size': 237, 'step_size': 12, 'gamma': 0.9016038562698655}. Best is trial 0 with value: 0.2449273318052292.[0m
[32m[I 2025-01-07 10:24:36,899][0m Trial 2 finished with value: 0.14509317278862 and parameters: {'observation_period_num': 215, 'train_rates': 0.9615716909708011, 'learning_rate': 0.00018260073475227993, 'batch_size': 161, 'step_size': 10, 'gamma': 0.927495118914038}. Best is trial 2 with value: 0.14509317278862.[0m
[32m[I 2025-01-07 10:25:55,276][0m Trial 3 finished with value: 0.17990704592850623 and parameters: {'observation_period_num': 58, 'train_rates': 0.664603028916501, 'learning_rate': 2.648992701723112e-05, 'batch_size': 60, 'step_size': 14, 'gamma': 0.955175080385725}. Best is trial 2 with value: 0.14509317278862.[0m
[32m[I 2025-01-07 10:27:04,387][0m Trial 4 finished with value: 0.08047472390059668 and parameters: {'observation_period_num': 124, 'train_rates': 0.9266613547954134, 'learning_rate': 4.979391791036566e-05, 'batch_size': 83, 'step_size': 4, 'gamma': 0.9339958424537936}. Best is trial 4 with value: 0.08047472390059668.[0m
[32m[I 2025-01-07 10:28:48,561][0m Trial 5 finished with value: 0.10307240093730098 and parameters: {'observation_period_num': 83, 'train_rates': 0.902364046240169, 'learning_rate': 8.263490659151623e-06, 'batch_size': 53, 'step_size': 13, 'gamma': 0.9605799331302066}. Best is trial 4 with value: 0.08047472390059668.[0m
[32m[I 2025-01-07 10:29:47,580][0m Trial 6 finished with value: 0.3332309033343988 and parameters: {'observation_period_num': 60, 'train_rates': 0.6761141404921319, 'learning_rate': 8.584070133515536e-06, 'batch_size': 79, 'step_size': 6, 'gamma': 0.885524596706536}. Best is trial 4 with value: 0.08047472390059668.[0m
[32m[I 2025-01-07 10:30:20,072][0m Trial 7 finished with value: 0.24225658698042993 and parameters: {'observation_period_num': 65, 'train_rates': 0.7459580296809712, 'learning_rate': 6.633211978128629e-05, 'batch_size': 217, 'step_size': 12, 'gamma': 0.9824049363018564}. Best is trial 4 with value: 0.08047472390059668.[0m
[32m[I 2025-01-07 10:31:37,300][0m Trial 8 finished with value: 0.12136455120399911 and parameters: {'observation_period_num': 171, 'train_rates': 0.831266988354711, 'learning_rate': 0.0006597290171925697, 'batch_size': 67, 'step_size': 9, 'gamma': 0.7988374797808261}. Best is trial 4 with value: 0.08047472390059668.[0m
[32m[I 2025-01-07 10:32:33,514][0m Trial 9 finished with value: 0.2609415455876154 and parameters: {'observation_period_num': 166, 'train_rates': 0.7109153165799651, 'learning_rate': 8.477333085706346e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.7718612290726188}. Best is trial 4 with value: 0.08047472390059668.[0m
Early stopping at epoch 72
[32m[I 2025-01-07 10:33:04,622][0m Trial 10 finished with value: 0.061431111944349186 and parameters: {'observation_period_num': 11, 'train_rates': 0.8386255272294179, 'learning_rate': 0.0008898098539910247, 'batch_size': 142, 'step_size': 1, 'gamma': 0.829831645044827}. Best is trial 10 with value: 0.061431111944349186.[0m
Early stopping at epoch 55
[32m[I 2025-01-07 10:33:29,141][0m Trial 11 finished with value: 2.82163897090488 and parameters: {'observation_period_num': 24, 'train_rates': 0.8311805154025124, 'learning_rate': 1.319229341577496e-06, 'batch_size': 137, 'step_size': 1, 'gamma': 0.8351708149447878}. Best is trial 10 with value: 0.061431111944349186.[0m
Early stopping at epoch 74
[32m[I 2025-01-07 10:34:09,987][0m Trial 12 finished with value: 0.05757066128861825 and parameters: {'observation_period_num': 12, 'train_rates': 0.8973875173398342, 'learning_rate': 0.0007496318361402823, 'batch_size': 112, 'step_size': 1, 'gamma': 0.8400022091820696}. Best is trial 12 with value: 0.05757066128861825.[0m
Early stopping at epoch 79
[32m[I 2025-01-07 10:34:42,986][0m Trial 13 finished with value: 0.06383400762516898 and parameters: {'observation_period_num': 8, 'train_rates': 0.8699699326137962, 'learning_rate': 0.000873882233132963, 'batch_size': 163, 'step_size': 1, 'gamma': 0.8421998578507801}. Best is trial 12 with value: 0.05757066128861825.[0m
[32m[I 2025-01-07 10:35:30,627][0m Trial 14 finished with value: 0.18999761784081184 and parameters: {'observation_period_num': 30, 'train_rates': 0.7732925975872164, 'learning_rate': 0.0003744377109957387, 'batch_size': 121, 'step_size': 3, 'gamma': 0.8477302850311365}. Best is trial 12 with value: 0.05757066128861825.[0m
[32m[I 2025-01-07 10:39:22,567][0m Trial 15 finished with value: 0.03528406794260566 and parameters: {'observation_period_num': 7, 'train_rates': 0.8372905275961515, 'learning_rate': 0.0002438262086058531, 'batch_size': 23, 'step_size': 2, 'gamma': 0.8115855478390327}. Best is trial 15 with value: 0.03528406794260566.[0m
[32m[I 2025-01-07 10:41:46,428][0m Trial 16 finished with value: 0.08668920083987976 and parameters: {'observation_period_num': 98, 'train_rates': 0.889258061572138, 'learning_rate': 0.00021236335660969926, 'batch_size': 39, 'step_size': 7, 'gamma': 0.7988445305039773}. Best is trial 15 with value: 0.03528406794260566.[0m
[32m[I 2025-01-07 10:46:46,664][0m Trial 17 finished with value: 0.06840058750371171 and parameters: {'observation_period_num': 42, 'train_rates': 0.7895260482721442, 'learning_rate': 0.0003018024390815486, 'batch_size': 18, 'step_size': 3, 'gamma': 0.8015568633398162}. Best is trial 15 with value: 0.03528406794260566.[0m
[32m[I 2025-01-07 10:47:49,538][0m Trial 18 finished with value: 0.09090180817637161 and parameters: {'observation_period_num': 138, 'train_rates': 0.8617738545846865, 'learning_rate': 0.0001348612374665597, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8645315491560681}. Best is trial 15 with value: 0.03528406794260566.[0m
[32m[I 2025-01-07 10:53:33,871][0m Trial 19 finished with value: 0.03178822695686717 and parameters: {'observation_period_num': 5, 'train_rates': 0.9275557833719309, 'learning_rate': 0.00043596274124829595, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8093052282416352}. Best is trial 19 with value: 0.03178822695686717.[0m
[32m[I 2025-01-07 10:56:49,422][0m Trial 20 finished with value: 0.4967902065708341 and parameters: {'observation_period_num': 45, 'train_rates': 0.6150853759165796, 'learning_rate': 1.9591920591436977e-05, 'batch_size': 22, 'step_size': 8, 'gamma': 0.7752272110104572}. Best is trial 19 with value: 0.03178822695686717.[0m
[32m[I 2025-01-07 10:59:26,499][0m Trial 21 finished with value: 0.03237983584403992 and parameters: {'observation_period_num': 6, 'train_rates': 0.9168150491648933, 'learning_rate': 0.00045435278040548594, 'batch_size': 36, 'step_size': 2, 'gamma': 0.8124754883033115}. Best is trial 19 with value: 0.03178822695686717.[0m
[32m[I 2025-01-07 11:02:01,200][0m Trial 22 finished with value: 0.07698670277992885 and parameters: {'observation_period_num': 83, 'train_rates': 0.9853706048285491, 'learning_rate': 0.000370657939178391, 'batch_size': 38, 'step_size': 5, 'gamma': 0.8159824509514094}. Best is trial 19 with value: 0.03178822695686717.[0m
[32m[I 2025-01-07 11:07:50,274][0m Trial 23 finished with value: 0.045617240508204523 and parameters: {'observation_period_num': 31, 'train_rates': 0.9297854947303382, 'learning_rate': 0.00010966991389489216, 'batch_size': 16, 'step_size': 3, 'gamma': 0.8687510344533209}. Best is trial 19 with value: 0.03178822695686717.[0m
[32m[I 2025-01-07 11:10:06,898][0m Trial 24 finished with value: 0.025984326703265108 and parameters: {'observation_period_num': 5, 'train_rates': 0.9246236702065193, 'learning_rate': 0.00043450571421121324, 'batch_size': 42, 'step_size': 5, 'gamma': 0.812442863063571}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:12:15,473][0m Trial 25 finished with value: 0.0555908732900494 and parameters: {'observation_period_num': 46, 'train_rates': 0.9217767211697552, 'learning_rate': 0.0004224385960761702, 'batch_size': 45, 'step_size': 6, 'gamma': 0.7772743235607379}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:13:42,562][0m Trial 26 finished with value: 0.07385278536708324 and parameters: {'observation_period_num': 81, 'train_rates': 0.9465742540647772, 'learning_rate': 0.0005851637827962047, 'batch_size': 69, 'step_size': 5, 'gamma': 0.7526488453479957}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:16:09,702][0m Trial 27 finished with value: 0.29577913880348206 and parameters: {'observation_period_num': 28, 'train_rates': 0.9886436316495134, 'learning_rate': 1.2499135386698852e-06, 'batch_size': 41, 'step_size': 7, 'gamma': 0.7837188084634696}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:17:12,002][0m Trial 28 finished with value: 0.04156143425471997 and parameters: {'observation_period_num': 5, 'train_rates': 0.879498609156816, 'learning_rate': 0.00014626148594695286, 'batch_size': 93, 'step_size': 4, 'gamma': 0.8561541783917865}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:17:49,690][0m Trial 29 finished with value: 0.08180047779143611 and parameters: {'observation_period_num': 106, 'train_rates': 0.8060078081478674, 'learning_rate': 0.0004857296448630544, 'batch_size': 187, 'step_size': 4, 'gamma': 0.8166981422244592}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:20:43,129][0m Trial 30 finished with value: 0.2938478726243216 and parameters: {'observation_period_num': 69, 'train_rates': 0.9126840485511462, 'learning_rate': 2.851006158055302e-06, 'batch_size': 32, 'step_size': 6, 'gamma': 0.7505915179766066}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:22:23,536][0m Trial 31 finished with value: 0.039976960245300744 and parameters: {'observation_period_num': 21, 'train_rates': 0.8552391329972789, 'learning_rate': 0.00027022522688033973, 'batch_size': 54, 'step_size': 3, 'gamma': 0.8188043657432645}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:28:24,331][0m Trial 32 finished with value: 0.05622235520018472 and parameters: {'observation_period_num': 44, 'train_rates': 0.9600800018614561, 'learning_rate': 0.00022484419475058247, 'batch_size': 16, 'step_size': 2, 'gamma': 0.8890647090039984}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:31:33,293][0m Trial 33 finished with value: 0.03289369002160941 and parameters: {'observation_period_num': 6, 'train_rates': 0.9429355188132672, 'learning_rate': 0.0005096220262978914, 'batch_size': 32, 'step_size': 2, 'gamma': 0.7941761917029646}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:33:07,513][0m Trial 34 finished with value: 0.11149916451166172 and parameters: {'observation_period_num': 242, 'train_rates': 0.9627313536600411, 'learning_rate': 0.0005220189271359566, 'batch_size': 58, 'step_size': 2, 'gamma': 0.7987236784204568}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:33:47,980][0m Trial 35 finished with value: 0.056801047176122665 and parameters: {'observation_period_num': 37, 'train_rates': 0.9440942410478093, 'learning_rate': 0.0009897366329692312, 'batch_size': 248, 'step_size': 4, 'gamma': 0.7862804745613425}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:36:47,202][0m Trial 36 finished with value: 0.09589357605610771 and parameters: {'observation_period_num': 213, 'train_rates': 0.9366638766569596, 'learning_rate': 3.662479234932295e-05, 'batch_size': 32, 'step_size': 10, 'gamma': 0.7654192469399923}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:37:52,424][0m Trial 37 finished with value: 0.057858808109393485 and parameters: {'observation_period_num': 19, 'train_rates': 0.9646795725767592, 'learning_rate': 0.00017018311888727933, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8244667375906306}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:39:09,029][0m Trial 38 finished with value: 0.11077643716234271 and parameters: {'observation_period_num': 55, 'train_rates': 0.8982095376614876, 'learning_rate': 6.402536891057197e-05, 'batch_size': 75, 'step_size': 2, 'gamma': 0.7891371143870051}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:41:04,860][0m Trial 39 finished with value: 0.05633739630381266 and parameters: {'observation_period_num': 21, 'train_rates': 0.9131282287655283, 'learning_rate': 1.863882462678088e-05, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8815147093199902}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:42:35,518][0m Trial 40 finished with value: 0.10267367048396005 and parameters: {'observation_period_num': 129, 'train_rates': 0.974512120746398, 'learning_rate': 0.0003023354026391083, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9004404209023549}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:45:47,429][0m Trial 41 finished with value: 0.029316746661889142 and parameters: {'observation_period_num': 9, 'train_rates': 0.9411182030385636, 'learning_rate': 0.0005794943059489124, 'batch_size': 30, 'step_size': 2, 'gamma': 0.8105379411701299}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:49:11,579][0m Trial 42 finished with value: 0.03584772705410918 and parameters: {'observation_period_num': 18, 'train_rates': 0.9428083695346703, 'learning_rate': 0.0005958399915334796, 'batch_size': 28, 'step_size': 4, 'gamma': 0.8090404994784511}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:51:07,218][0m Trial 43 finished with value: 0.030963189230206307 and parameters: {'observation_period_num': 5, 'train_rates': 0.9094899414908906, 'learning_rate': 0.00010327466672952845, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8258103103530414}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:53:01,082][0m Trial 44 finished with value: 0.04934613819932565 and parameters: {'observation_period_num': 32, 'train_rates': 0.9123674127545783, 'learning_rate': 9.96810352318094e-05, 'batch_size': 51, 'step_size': 15, 'gamma': 0.8507219058913553}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:54:41,647][0m Trial 45 finished with value: 0.055330977725559435 and parameters: {'observation_period_num': 50, 'train_rates': 0.8836552753559175, 'learning_rate': 5.34583437360841e-05, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8293834136318025}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:56:05,289][0m Trial 46 finished with value: 0.07634496031232843 and parameters: {'observation_period_num': 68, 'train_rates': 0.9264650799766803, 'learning_rate': 0.0006807391303464296, 'batch_size': 82, 'step_size': 15, 'gamma': 0.8066165430742694}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:58:06,259][0m Trial 47 finished with value: 0.10806462444878857 and parameters: {'observation_period_num': 215, 'train_rates': 0.8516979270343675, 'learning_rate': 0.0003636453977027566, 'batch_size': 43, 'step_size': 12, 'gamma': 0.8352446896774331}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 11:59:00,504][0m Trial 48 finished with value: 0.09228259951308154 and parameters: {'observation_period_num': 168, 'train_rates': 0.9010480194404442, 'learning_rate': 0.00019927862838714773, 'batch_size': 224, 'step_size': 8, 'gamma': 0.7650203589364218}. Best is trial 24 with value: 0.025984326703265108.[0m
[32m[I 2025-01-07 12:00:26,707][0m Trial 49 finished with value: 0.04616876358740798 and parameters: {'observation_period_num': 17, 'train_rates': 0.8180333046464859, 'learning_rate': 0.0007496941239233084, 'batch_size': 73, 'step_size': 14, 'gamma': 0.9387853469403934}. Best is trial 24 with value: 0.025984326703265108.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.8826336436485747, 'learning_rate': 0.00010181138901250208, 'batch_size': 36, 'step_size': 9, 'gamma': 0.851653141022606}
Epoch 1/300, trend Loss: 0.1637 | 0.0828
Epoch 2/300, trend Loss: 0.1276 | 0.0649
Epoch 3/300, trend Loss: 0.1168 | 0.0611
Epoch 4/300, trend Loss: 0.1113 | 0.0608
Epoch 5/300, trend Loss: 0.1084 | 0.0612
Epoch 6/300, trend Loss: 0.1064 | 0.0806
Epoch 7/300, trend Loss: 0.1051 | 0.0856
Epoch 8/300, trend Loss: 0.1025 | 0.0834
Epoch 9/300, trend Loss: 0.0994 | 0.0771
Epoch 10/300, trend Loss: 0.0959 | 0.0647
Epoch 11/300, trend Loss: 0.0935 | 0.0601
Epoch 12/300, trend Loss: 0.0914 | 0.0578
Epoch 13/300, trend Loss: 0.0896 | 0.0568
Epoch 14/300, trend Loss: 0.0883 | 0.0566
Epoch 15/300, trend Loss: 0.0867 | 0.0504
Epoch 16/300, trend Loss: 0.0862 | 0.0505
Epoch 17/300, trend Loss: 0.0852 | 0.0505
Epoch 18/300, trend Loss: 0.0843 | 0.0502
Epoch 19/300, trend Loss: 0.0828 | 0.0453
Epoch 20/300, trend Loss: 0.0823 | 0.0452
Epoch 21/300, trend Loss: 0.0810 | 0.0442
Epoch 22/300, trend Loss: 0.0800 | 0.0431
Epoch 23/300, trend Loss: 0.0791 | 0.0421
Epoch 24/300, trend Loss: 0.0780 | 0.0387
Epoch 25/300, trend Loss: 0.0774 | 0.0377
Epoch 26/300, trend Loss: 0.0767 | 0.0369
Epoch 27/300, trend Loss: 0.0761 | 0.0364
Epoch 28/300, trend Loss: 0.0755 | 0.0362
Epoch 29/300, trend Loss: 0.0750 | 0.0359
Epoch 30/300, trend Loss: 0.0745 | 0.0355
Epoch 31/300, trend Loss: 0.0740 | 0.0352
Epoch 32/300, trend Loss: 0.0736 | 0.0349
Epoch 33/300, trend Loss: 0.0731 | 0.0349
Epoch 34/300, trend Loss: 0.0728 | 0.0347
Epoch 35/300, trend Loss: 0.0725 | 0.0345
Epoch 36/300, trend Loss: 0.0722 | 0.0343
Epoch 37/300, trend Loss: 0.0721 | 0.0349
Epoch 38/300, trend Loss: 0.0721 | 0.0351
Epoch 39/300, trend Loss: 0.0719 | 0.0349
Epoch 40/300, trend Loss: 0.0717 | 0.0345
Epoch 41/300, trend Loss: 0.0715 | 0.0341
Epoch 42/300, trend Loss: 0.0713 | 0.0330
Epoch 43/300, trend Loss: 0.0711 | 0.0330
Epoch 44/300, trend Loss: 0.0709 | 0.0329
Epoch 45/300, trend Loss: 0.0706 | 0.0328
Epoch 46/300, trend Loss: 0.0703 | 0.0329
Epoch 47/300, trend Loss: 0.0701 | 0.0327
Epoch 48/300, trend Loss: 0.0700 | 0.0326
Epoch 49/300, trend Loss: 0.0699 | 0.0325
Epoch 50/300, trend Loss: 0.0697 | 0.0323
Epoch 51/300, trend Loss: 0.0696 | 0.0323
Epoch 52/300, trend Loss: 0.0696 | 0.0322
Epoch 53/300, trend Loss: 0.0695 | 0.0320
Epoch 54/300, trend Loss: 0.0694 | 0.0319
Epoch 55/300, trend Loss: 0.0693 | 0.0321
Epoch 56/300, trend Loss: 0.0691 | 0.0320
Epoch 57/300, trend Loss: 0.0690 | 0.0318
Epoch 58/300, trend Loss: 0.0689 | 0.0317
Epoch 59/300, trend Loss: 0.0688 | 0.0316
Epoch 60/300, trend Loss: 0.0686 | 0.0317
Epoch 61/300, trend Loss: 0.0685 | 0.0315
Epoch 62/300, trend Loss: 0.0684 | 0.0315
Epoch 63/300, trend Loss: 0.0684 | 0.0314
Epoch 64/300, trend Loss: 0.0682 | 0.0312
Epoch 65/300, trend Loss: 0.0682 | 0.0311
Epoch 66/300, trend Loss: 0.0681 | 0.0311
Epoch 67/300, trend Loss: 0.0680 | 0.0310
Epoch 68/300, trend Loss: 0.0679 | 0.0309
Epoch 69/300, trend Loss: 0.0678 | 0.0306
Epoch 70/300, trend Loss: 0.0678 | 0.0305
Epoch 71/300, trend Loss: 0.0677 | 0.0305
Epoch 72/300, trend Loss: 0.0676 | 0.0305
Epoch 73/300, trend Loss: 0.0675 | 0.0301
Epoch 74/300, trend Loss: 0.0675 | 0.0301
Epoch 75/300, trend Loss: 0.0674 | 0.0301
Epoch 76/300, trend Loss: 0.0673 | 0.0300
Epoch 77/300, trend Loss: 0.0673 | 0.0300
Epoch 78/300, trend Loss: 0.0672 | 0.0298
Epoch 79/300, trend Loss: 0.0671 | 0.0298
Epoch 80/300, trend Loss: 0.0670 | 0.0297
Epoch 81/300, trend Loss: 0.0670 | 0.0297
Epoch 82/300, trend Loss: 0.0669 | 0.0296
Epoch 83/300, trend Loss: 0.0669 | 0.0296
Epoch 84/300, trend Loss: 0.0668 | 0.0296
Epoch 85/300, trend Loss: 0.0667 | 0.0295
Epoch 86/300, trend Loss: 0.0667 | 0.0295
Epoch 87/300, trend Loss: 0.0666 | 0.0294
Epoch 88/300, trend Loss: 0.0666 | 0.0294
Epoch 89/300, trend Loss: 0.0665 | 0.0294
Epoch 90/300, trend Loss: 0.0665 | 0.0294
Epoch 91/300, trend Loss: 0.0664 | 0.0293
Epoch 92/300, trend Loss: 0.0664 | 0.0293
Epoch 93/300, trend Loss: 0.0663 | 0.0293
Epoch 94/300, trend Loss: 0.0663 | 0.0293
Epoch 95/300, trend Loss: 0.0663 | 0.0293
Epoch 96/300, trend Loss: 0.0662 | 0.0293
Epoch 97/300, trend Loss: 0.0662 | 0.0292
Epoch 98/300, trend Loss: 0.0662 | 0.0292
Epoch 99/300, trend Loss: 0.0661 | 0.0292
Epoch 100/300, trend Loss: 0.0661 | 0.0292
Epoch 101/300, trend Loss: 0.0661 | 0.0292
Epoch 102/300, trend Loss: 0.0661 | 0.0292
Epoch 103/300, trend Loss: 0.0661 | 0.0292
Epoch 104/300, trend Loss: 0.0661 | 0.0292
Epoch 105/300, trend Loss: 0.0660 | 0.0291
Epoch 106/300, trend Loss: 0.0660 | 0.0291
Epoch 107/300, trend Loss: 0.0660 | 0.0291
Epoch 108/300, trend Loss: 0.0660 | 0.0291
Epoch 109/300, trend Loss: 0.0660 | 0.0291
Epoch 110/300, trend Loss: 0.0660 | 0.0291
Epoch 111/300, trend Loss: 0.0660 | 0.0291
Epoch 112/300, trend Loss: 0.0660 | 0.0291
Epoch 113/300, trend Loss: 0.0660 | 0.0291
Epoch 114/300, trend Loss: 0.0660 | 0.0291
Epoch 115/300, trend Loss: 0.0660 | 0.0291
Epoch 116/300, trend Loss: 0.0660 | 0.0291
Epoch 117/300, trend Loss: 0.0659 | 0.0291
Epoch 118/300, trend Loss: 0.0659 | 0.0291
Epoch 119/300, trend Loss: 0.0659 | 0.0291
Epoch 120/300, trend Loss: 0.0659 | 0.0291
Epoch 121/300, trend Loss: 0.0659 | 0.0291
Epoch 122/300, trend Loss: 0.0659 | 0.0291
Epoch 123/300, trend Loss: 0.0658 | 0.0291
Epoch 124/300, trend Loss: 0.0658 | 0.0291
Epoch 125/300, trend Loss: 0.0658 | 0.0291
Epoch 126/300, trend Loss: 0.0657 | 0.0291
Epoch 127/300, trend Loss: 0.0657 | 0.0292
Epoch 128/300, trend Loss: 0.0657 | 0.0292
Epoch 129/300, trend Loss: 0.0656 | 0.0291
Epoch 130/300, trend Loss: 0.0656 | 0.0291
Epoch 131/300, trend Loss: 0.0656 | 0.0291
Epoch 132/300, trend Loss: 0.0656 | 0.0291
Epoch 133/300, trend Loss: 0.0656 | 0.0291
Epoch 134/300, trend Loss: 0.0655 | 0.0291
Epoch 135/300, trend Loss: 0.0655 | 0.0291
Epoch 136/300, trend Loss: 0.0655 | 0.0291
Epoch 137/300, trend Loss: 0.0655 | 0.0291
Epoch 138/300, trend Loss: 0.0655 | 0.0291
Epoch 139/300, trend Loss: 0.0655 | 0.0291
Epoch 140/300, trend Loss: 0.0655 | 0.0291
Epoch 141/300, trend Loss: 0.0655 | 0.0291
Epoch 142/300, trend Loss: 0.0654 | 0.0291
Epoch 143/300, trend Loss: 0.0654 | 0.0291
Epoch 144/300, trend Loss: 0.0654 | 0.0291
Epoch 145/300, trend Loss: 0.0654 | 0.0291
Epoch 146/300, trend Loss: 0.0654 | 0.0291
Epoch 147/300, trend Loss: 0.0654 | 0.0291
Epoch 148/300, trend Loss: 0.0654 | 0.0291
Epoch 149/300, trend Loss: 0.0654 | 0.0291
Epoch 150/300, trend Loss: 0.0654 | 0.0291
Epoch 151/300, trend Loss: 0.0654 | 0.0291
Epoch 152/300, trend Loss: 0.0654 | 0.0291
Epoch 153/300, trend Loss: 0.0654 | 0.0291
Epoch 154/300, trend Loss: 0.0654 | 0.0291
Epoch 155/300, trend Loss: 0.0654 | 0.0291
Epoch 156/300, trend Loss: 0.0654 | 0.0291
Epoch 157/300, trend Loss: 0.0654 | 0.0291
Epoch 158/300, trend Loss: 0.0653 | 0.0291
Epoch 159/300, trend Loss: 0.0653 | 0.0291
Epoch 160/300, trend Loss: 0.0653 | 0.0291
Epoch 161/300, trend Loss: 0.0653 | 0.0291
Epoch 162/300, trend Loss: 0.0653 | 0.0291
Epoch 163/300, trend Loss: 0.0653 | 0.0291
Epoch 164/300, trend Loss: 0.0653 | 0.0291
Epoch 165/300, trend Loss: 0.0653 | 0.0291
Epoch 166/300, trend Loss: 0.0653 | 0.0291
Epoch 167/300, trend Loss: 0.0653 | 0.0291
Epoch 168/300, trend Loss: 0.0653 | 0.0291
Epoch 169/300, trend Loss: 0.0653 | 0.0291
Epoch 170/300, trend Loss: 0.0653 | 0.0291
Epoch 171/300, trend Loss: 0.0653 | 0.0291
Epoch 172/300, trend Loss: 0.0653 | 0.0291
Epoch 173/300, trend Loss: 0.0653 | 0.0291
Epoch 174/300, trend Loss: 0.0653 | 0.0291
Epoch 175/300, trend Loss: 0.0653 | 0.0291
Epoch 176/300, trend Loss: 0.0653 | 0.0291
Epoch 177/300, trend Loss: 0.0653 | 0.0291
Epoch 178/300, trend Loss: 0.0653 | 0.0291
Epoch 179/300, trend Loss: 0.0653 | 0.0291
Epoch 180/300, trend Loss: 0.0653 | 0.0291
Epoch 181/300, trend Loss: 0.0653 | 0.0291
Epoch 182/300, trend Loss: 0.0653 | 0.0291
Epoch 183/300, trend Loss: 0.0653 | 0.0291
Epoch 184/300, trend Loss: 0.0653 | 0.0291
Epoch 185/300, trend Loss: 0.0653 | 0.0291
Epoch 186/300, trend Loss: 0.0653 | 0.0291
Epoch 187/300, trend Loss: 0.0653 | 0.0291
Epoch 188/300, trend Loss: 0.0653 | 0.0291
Epoch 189/300, trend Loss: 0.0653 | 0.0291
Epoch 190/300, trend Loss: 0.0653 | 0.0291
Epoch 191/300, trend Loss: 0.0653 | 0.0291
Epoch 192/300, trend Loss: 0.0653 | 0.0291
Epoch 193/300, trend Loss: 0.0653 | 0.0291
Epoch 194/300, trend Loss: 0.0653 | 0.0291
Epoch 195/300, trend Loss: 0.0653 | 0.0291
Epoch 196/300, trend Loss: 0.0653 | 0.0291
Epoch 197/300, trend Loss: 0.0653 | 0.0291
Epoch 198/300, trend Loss: 0.0653 | 0.0291
Epoch 199/300, trend Loss: 0.0653 | 0.0291
Epoch 200/300, trend Loss: 0.0653 | 0.0291
Epoch 201/300, trend Loss: 0.0653 | 0.0291
Epoch 202/300, trend Loss: 0.0653 | 0.0291
Epoch 203/300, trend Loss: 0.0653 | 0.0291
Epoch 204/300, trend Loss: 0.0652 | 0.0291
Epoch 205/300, trend Loss: 0.0652 | 0.0291
Epoch 206/300, trend Loss: 0.0652 | 0.0291
Epoch 207/300, trend Loss: 0.0652 | 0.0291
Epoch 208/300, trend Loss: 0.0652 | 0.0291
Epoch 209/300, trend Loss: 0.0652 | 0.0291
Epoch 210/300, trend Loss: 0.0652 | 0.0291
Epoch 211/300, trend Loss: 0.0652 | 0.0291
Epoch 212/300, trend Loss: 0.0652 | 0.0291
Epoch 213/300, trend Loss: 0.0652 | 0.0291
Epoch 214/300, trend Loss: 0.0652 | 0.0291
Epoch 215/300, trend Loss: 0.0652 | 0.0291
Epoch 216/300, trend Loss: 0.0652 | 0.0291
Epoch 217/300, trend Loss: 0.0652 | 0.0291
Epoch 218/300, trend Loss: 0.0652 | 0.0291
Epoch 219/300, trend Loss: 0.0652 | 0.0291
Epoch 220/300, trend Loss: 0.0652 | 0.0291
Epoch 221/300, trend Loss: 0.0652 | 0.0291
Epoch 222/300, trend Loss: 0.0652 | 0.0291
Epoch 223/300, trend Loss: 0.0652 | 0.0291
Epoch 224/300, trend Loss: 0.0652 | 0.0291
Epoch 225/300, trend Loss: 0.0652 | 0.0291
Epoch 226/300, trend Loss: 0.0652 | 0.0291
Epoch 227/300, trend Loss: 0.0652 | 0.0291
Epoch 228/300, trend Loss: 0.0652 | 0.0291
Epoch 229/300, trend Loss: 0.0652 | 0.0291
Epoch 230/300, trend Loss: 0.0652 | 0.0291
Epoch 231/300, trend Loss: 0.0652 | 0.0291
Epoch 232/300, trend Loss: 0.0652 | 0.0291
Epoch 233/300, trend Loss: 0.0652 | 0.0291
Epoch 234/300, trend Loss: 0.0652 | 0.0291
Epoch 235/300, trend Loss: 0.0652 | 0.0291
Epoch 236/300, trend Loss: 0.0652 | 0.0291
Epoch 237/300, trend Loss: 0.0652 | 0.0291
Epoch 238/300, trend Loss: 0.0652 | 0.0291
Epoch 239/300, trend Loss: 0.0652 | 0.0291
Epoch 240/300, trend Loss: 0.0652 | 0.0291
Epoch 241/300, trend Loss: 0.0652 | 0.0291
Epoch 242/300, trend Loss: 0.0652 | 0.0291
Epoch 243/300, trend Loss: 0.0652 | 0.0291
Epoch 244/300, trend Loss: 0.0652 | 0.0291
Epoch 245/300, trend Loss: 0.0652 | 0.0291
Epoch 246/300, trend Loss: 0.0652 | 0.0291
Epoch 247/300, trend Loss: 0.0652 | 0.0291
Epoch 248/300, trend Loss: 0.0652 | 0.0291
Epoch 249/300, trend Loss: 0.0652 | 0.0291
Epoch 250/300, trend Loss: 0.0652 | 0.0291
Epoch 251/300, trend Loss: 0.0652 | 0.0291
Epoch 252/300, trend Loss: 0.0652 | 0.0291
Epoch 253/300, trend Loss: 0.0652 | 0.0291
Epoch 254/300, trend Loss: 0.0652 | 0.0291
Epoch 255/300, trend Loss: 0.0652 | 0.0291
Epoch 256/300, trend Loss: 0.0652 | 0.0291
Epoch 257/300, trend Loss: 0.0652 | 0.0291
Epoch 258/300, trend Loss: 0.0652 | 0.0291
Epoch 259/300, trend Loss: 0.0652 | 0.0291
Epoch 260/300, trend Loss: 0.0652 | 0.0291
Epoch 261/300, trend Loss: 0.0652 | 0.0291
Epoch 262/300, trend Loss: 0.0652 | 0.0291
Epoch 263/300, trend Loss: 0.0652 | 0.0291
Epoch 264/300, trend Loss: 0.0652 | 0.0291
Epoch 265/300, trend Loss: 0.0652 | 0.0291
Epoch 266/300, trend Loss: 0.0652 | 0.0291
Epoch 267/300, trend Loss: 0.0652 | 0.0291
Epoch 268/300, trend Loss: 0.0652 | 0.0291
Epoch 269/300, trend Loss: 0.0652 | 0.0291
Epoch 270/300, trend Loss: 0.0652 | 0.0291
Epoch 271/300, trend Loss: 0.0652 | 0.0291
Epoch 272/300, trend Loss: 0.0652 | 0.0291
Epoch 273/300, trend Loss: 0.0652 | 0.0291
Epoch 274/300, trend Loss: 0.0652 | 0.0291
Epoch 275/300, trend Loss: 0.0652 | 0.0291
Epoch 276/300, trend Loss: 0.0652 | 0.0291
Epoch 277/300, trend Loss: 0.0652 | 0.0291
Epoch 278/300, trend Loss: 0.0652 | 0.0291
Epoch 279/300, trend Loss: 0.0652 | 0.0291
Epoch 280/300, trend Loss: 0.0652 | 0.0291
Epoch 281/300, trend Loss: 0.0652 | 0.0291
Epoch 282/300, trend Loss: 0.0652 | 0.0291
Epoch 283/300, trend Loss: 0.0652 | 0.0291
Epoch 284/300, trend Loss: 0.0652 | 0.0291
Epoch 285/300, trend Loss: 0.0652 | 0.0291
Epoch 286/300, trend Loss: 0.0652 | 0.0291
Epoch 287/300, trend Loss: 0.0652 | 0.0291
Epoch 288/300, trend Loss: 0.0652 | 0.0291
Epoch 289/300, trend Loss: 0.0652 | 0.0291
Epoch 290/300, trend Loss: 0.0652 | 0.0291
Epoch 291/300, trend Loss: 0.0652 | 0.0291
Epoch 292/300, trend Loss: 0.0652 | 0.0291
Epoch 293/300, trend Loss: 0.0652 | 0.0291
Epoch 294/300, trend Loss: 0.0652 | 0.0291
Epoch 295/300, trend Loss: 0.0652 | 0.0291
Epoch 296/300, trend Loss: 0.0652 | 0.0291
Epoch 297/300, trend Loss: 0.0652 | 0.0291
Epoch 298/300, trend Loss: 0.0652 | 0.0291
Epoch 299/300, trend Loss: 0.0652 | 0.0291
Epoch 300/300, trend Loss: 0.0652 | 0.0291
Training seasonal_0 component with params: {'observation_period_num': 29, 'train_rates': 0.9844616018213376, 'learning_rate': 0.0005801486146465605, 'batch_size': 132, 'step_size': 8, 'gamma': 0.8961723542826149}
Epoch 1/300, seasonal_0 Loss: 0.5140 | 0.2466
Epoch 2/300, seasonal_0 Loss: 0.2431 | 0.2381
Epoch 3/300, seasonal_0 Loss: 0.2014 | 0.2188
Epoch 4/300, seasonal_0 Loss: 0.2161 | 0.1528
Epoch 5/300, seasonal_0 Loss: 0.2878 | 0.2121
Epoch 6/300, seasonal_0 Loss: 0.2428 | 0.2236
Epoch 7/300, seasonal_0 Loss: 0.2419 | 0.4780
Epoch 8/300, seasonal_0 Loss: 0.1967 | 0.1579
Epoch 9/300, seasonal_0 Loss: 0.1562 | 0.1834
Epoch 10/300, seasonal_0 Loss: 0.1388 | 0.1582
Epoch 11/300, seasonal_0 Loss: 0.1341 | 0.1377
Epoch 12/300, seasonal_0 Loss: 0.1469 | 0.1299
Epoch 13/300, seasonal_0 Loss: 0.1662 | 0.1021
Epoch 14/300, seasonal_0 Loss: 0.1591 | 0.1051
Epoch 15/300, seasonal_0 Loss: 0.1522 | 0.1002
Epoch 16/300, seasonal_0 Loss: 0.1537 | 0.1164
Epoch 17/300, seasonal_0 Loss: 0.1501 | 0.1611
Epoch 18/300, seasonal_0 Loss: 0.1259 | 0.1300
Epoch 19/300, seasonal_0 Loss: 0.1415 | 0.1094
Epoch 20/300, seasonal_0 Loss: 0.1475 | 0.1303
Epoch 21/300, seasonal_0 Loss: 0.1349 | 0.1327
Epoch 22/300, seasonal_0 Loss: 0.1122 | 0.0903
Epoch 23/300, seasonal_0 Loss: 0.1089 | 0.0820
Epoch 24/300, seasonal_0 Loss: 0.1074 | 0.0787
Epoch 25/300, seasonal_0 Loss: 0.0994 | 0.0716
Epoch 26/300, seasonal_0 Loss: 0.0958 | 0.0736
Epoch 27/300, seasonal_0 Loss: 0.0969 | 0.0763
Epoch 28/300, seasonal_0 Loss: 0.0973 | 0.0758
Epoch 29/300, seasonal_0 Loss: 0.0959 | 0.0742
Epoch 30/300, seasonal_0 Loss: 0.0951 | 0.0677
Epoch 31/300, seasonal_0 Loss: 0.0991 | 0.0658
Epoch 32/300, seasonal_0 Loss: 0.0993 | 0.0654
Epoch 33/300, seasonal_0 Loss: 0.0959 | 0.0647
Epoch 34/300, seasonal_0 Loss: 0.0910 | 0.0673
Epoch 35/300, seasonal_0 Loss: 0.0895 | 0.0645
Epoch 36/300, seasonal_0 Loss: 0.0979 | 0.0783
Epoch 37/300, seasonal_0 Loss: 0.1174 | 0.1936
Epoch 38/300, seasonal_0 Loss: 0.1078 | 0.0829
Epoch 39/300, seasonal_0 Loss: 0.0891 | 0.0653
Epoch 40/300, seasonal_0 Loss: 0.0935 | 0.0636
Epoch 41/300, seasonal_0 Loss: 0.0925 | 0.0655
Epoch 42/300, seasonal_0 Loss: 0.0842 | 0.0616
Epoch 43/300, seasonal_0 Loss: 0.0813 | 0.0666
Epoch 44/300, seasonal_0 Loss: 0.0820 | 0.0682
Epoch 45/300, seasonal_0 Loss: 0.0801 | 0.0621
Epoch 46/300, seasonal_0 Loss: 0.0779 | 0.0581
Epoch 47/300, seasonal_0 Loss: 0.0792 | 0.0583
Epoch 48/300, seasonal_0 Loss: 0.0785 | 0.0589
Epoch 49/300, seasonal_0 Loss: 0.0768 | 0.0560
Epoch 50/300, seasonal_0 Loss: 0.0761 | 0.0593
Epoch 51/300, seasonal_0 Loss: 0.0767 | 0.0637
Epoch 52/300, seasonal_0 Loss: 0.0762 | 0.0616
Epoch 53/300, seasonal_0 Loss: 0.0742 | 0.0559
Epoch 54/300, seasonal_0 Loss: 0.0736 | 0.0538
Epoch 55/300, seasonal_0 Loss: 0.0733 | 0.0537
Epoch 56/300, seasonal_0 Loss: 0.0726 | 0.0528
Epoch 57/300, seasonal_0 Loss: 0.0720 | 0.0521
Epoch 58/300, seasonal_0 Loss: 0.0717 | 0.0545
Epoch 59/300, seasonal_0 Loss: 0.0717 | 0.0552
Epoch 60/300, seasonal_0 Loss: 0.0710 | 0.0535
Epoch 61/300, seasonal_0 Loss: 0.0701 | 0.0504
Epoch 62/300, seasonal_0 Loss: 0.0696 | 0.0498
Epoch 63/300, seasonal_0 Loss: 0.0693 | 0.0490
Epoch 64/300, seasonal_0 Loss: 0.0688 | 0.0489
Epoch 65/300, seasonal_0 Loss: 0.0685 | 0.0489
Epoch 66/300, seasonal_0 Loss: 0.0684 | 0.0500
Epoch 67/300, seasonal_0 Loss: 0.0681 | 0.0486
Epoch 68/300, seasonal_0 Loss: 0.0678 | 0.0486
Epoch 69/300, seasonal_0 Loss: 0.0677 | 0.0478
Epoch 70/300, seasonal_0 Loss: 0.0678 | 0.0484
Epoch 71/300, seasonal_0 Loss: 0.0679 | 0.0477
Epoch 72/300, seasonal_0 Loss: 0.0679 | 0.0486
Epoch 73/300, seasonal_0 Loss: 0.0680 | 0.0468
Epoch 74/300, seasonal_0 Loss: 0.0675 | 0.0461
Epoch 75/300, seasonal_0 Loss: 0.0667 | 0.0447
Epoch 76/300, seasonal_0 Loss: 0.0664 | 0.0458
Epoch 77/300, seasonal_0 Loss: 0.0666 | 0.0448
Epoch 78/300, seasonal_0 Loss: 0.0667 | 0.0455
Epoch 79/300, seasonal_0 Loss: 0.0659 | 0.0442
Epoch 80/300, seasonal_0 Loss: 0.0651 | 0.0458
Epoch 81/300, seasonal_0 Loss: 0.0647 | 0.0445
Epoch 82/300, seasonal_0 Loss: 0.0644 | 0.0454
Epoch 83/300, seasonal_0 Loss: 0.0643 | 0.0441
Epoch 84/300, seasonal_0 Loss: 0.0640 | 0.0451
Epoch 85/300, seasonal_0 Loss: 0.0639 | 0.0440
Epoch 86/300, seasonal_0 Loss: 0.0637 | 0.0449
Epoch 87/300, seasonal_0 Loss: 0.0636 | 0.0436
Epoch 88/300, seasonal_0 Loss: 0.0634 | 0.0444
Epoch 89/300, seasonal_0 Loss: 0.0632 | 0.0432
Epoch 90/300, seasonal_0 Loss: 0.0630 | 0.0439
Epoch 91/300, seasonal_0 Loss: 0.0628 | 0.0429
Epoch 92/300, seasonal_0 Loss: 0.0627 | 0.0435
Epoch 93/300, seasonal_0 Loss: 0.0625 | 0.0426
Epoch 94/300, seasonal_0 Loss: 0.0623 | 0.0431
Epoch 95/300, seasonal_0 Loss: 0.0622 | 0.0425
Epoch 96/300, seasonal_0 Loss: 0.0620 | 0.0429
Epoch 97/300, seasonal_0 Loss: 0.0619 | 0.0424
Epoch 98/300, seasonal_0 Loss: 0.0617 | 0.0426
Epoch 99/300, seasonal_0 Loss: 0.0616 | 0.0422
Epoch 100/300, seasonal_0 Loss: 0.0614 | 0.0424
Epoch 101/300, seasonal_0 Loss: 0.0613 | 0.0421
Epoch 102/300, seasonal_0 Loss: 0.0612 | 0.0421
Epoch 103/300, seasonal_0 Loss: 0.0611 | 0.0419
Epoch 104/300, seasonal_0 Loss: 0.0610 | 0.0419
Epoch 105/300, seasonal_0 Loss: 0.0608 | 0.0418
Epoch 106/300, seasonal_0 Loss: 0.0607 | 0.0417
Epoch 107/300, seasonal_0 Loss: 0.0606 | 0.0416
Epoch 108/300, seasonal_0 Loss: 0.0605 | 0.0416
Epoch 109/300, seasonal_0 Loss: 0.0604 | 0.0415
Epoch 110/300, seasonal_0 Loss: 0.0603 | 0.0414
Epoch 111/300, seasonal_0 Loss: 0.0602 | 0.0414
Epoch 112/300, seasonal_0 Loss: 0.0601 | 0.0413
Epoch 113/300, seasonal_0 Loss: 0.0600 | 0.0413
Epoch 114/300, seasonal_0 Loss: 0.0599 | 0.0412
Epoch 115/300, seasonal_0 Loss: 0.0599 | 0.0412
Epoch 116/300, seasonal_0 Loss: 0.0598 | 0.0411
Epoch 117/300, seasonal_0 Loss: 0.0597 | 0.0410
Epoch 118/300, seasonal_0 Loss: 0.0596 | 0.0410
Epoch 119/300, seasonal_0 Loss: 0.0595 | 0.0410
Epoch 120/300, seasonal_0 Loss: 0.0595 | 0.0409
Epoch 121/300, seasonal_0 Loss: 0.0594 | 0.0409
Epoch 122/300, seasonal_0 Loss: 0.0593 | 0.0408
Epoch 123/300, seasonal_0 Loss: 0.0592 | 0.0408
Epoch 124/300, seasonal_0 Loss: 0.0592 | 0.0407
Epoch 125/300, seasonal_0 Loss: 0.0591 | 0.0407
Epoch 126/300, seasonal_0 Loss: 0.0590 | 0.0406
Epoch 127/300, seasonal_0 Loss: 0.0590 | 0.0406
Epoch 128/300, seasonal_0 Loss: 0.0589 | 0.0405
Epoch 129/300, seasonal_0 Loss: 0.0588 | 0.0405
Epoch 130/300, seasonal_0 Loss: 0.0588 | 0.0405
Epoch 131/300, seasonal_0 Loss: 0.0587 | 0.0405
Epoch 132/300, seasonal_0 Loss: 0.0587 | 0.0404
Epoch 133/300, seasonal_0 Loss: 0.0586 | 0.0404
Epoch 134/300, seasonal_0 Loss: 0.0585 | 0.0403
Epoch 135/300, seasonal_0 Loss: 0.0585 | 0.0403
Epoch 136/300, seasonal_0 Loss: 0.0584 | 0.0403
Epoch 137/300, seasonal_0 Loss: 0.0584 | 0.0403
Epoch 138/300, seasonal_0 Loss: 0.0583 | 0.0402
Epoch 139/300, seasonal_0 Loss: 0.0583 | 0.0402
Epoch 140/300, seasonal_0 Loss: 0.0582 | 0.0402
Epoch 141/300, seasonal_0 Loss: 0.0582 | 0.0402
Epoch 142/300, seasonal_0 Loss: 0.0581 | 0.0401
Epoch 143/300, seasonal_0 Loss: 0.0581 | 0.0401
Epoch 144/300, seasonal_0 Loss: 0.0581 | 0.0401
Epoch 145/300, seasonal_0 Loss: 0.0580 | 0.0401
Epoch 146/300, seasonal_0 Loss: 0.0580 | 0.0400
Epoch 147/300, seasonal_0 Loss: 0.0579 | 0.0400
Epoch 148/300, seasonal_0 Loss: 0.0579 | 0.0400
Epoch 149/300, seasonal_0 Loss: 0.0578 | 0.0400
Epoch 150/300, seasonal_0 Loss: 0.0578 | 0.0399
Epoch 151/300, seasonal_0 Loss: 0.0578 | 0.0400
Epoch 152/300, seasonal_0 Loss: 0.0577 | 0.0399
Epoch 153/300, seasonal_0 Loss: 0.0577 | 0.0399
Epoch 154/300, seasonal_0 Loss: 0.0577 | 0.0398
Epoch 155/300, seasonal_0 Loss: 0.0576 | 0.0399
Epoch 156/300, seasonal_0 Loss: 0.0576 | 0.0398
Epoch 157/300, seasonal_0 Loss: 0.0576 | 0.0398
Epoch 158/300, seasonal_0 Loss: 0.0575 | 0.0398
Epoch 159/300, seasonal_0 Loss: 0.0575 | 0.0398
Epoch 160/300, seasonal_0 Loss: 0.0575 | 0.0398
Epoch 161/300, seasonal_0 Loss: 0.0574 | 0.0398
Epoch 162/300, seasonal_0 Loss: 0.0574 | 0.0397
Epoch 163/300, seasonal_0 Loss: 0.0574 | 0.0397
Epoch 164/300, seasonal_0 Loss: 0.0574 | 0.0397
Epoch 165/300, seasonal_0 Loss: 0.0573 | 0.0397
Epoch 166/300, seasonal_0 Loss: 0.0573 | 0.0397
Epoch 167/300, seasonal_0 Loss: 0.0573 | 0.0397
Epoch 168/300, seasonal_0 Loss: 0.0573 | 0.0397
Epoch 169/300, seasonal_0 Loss: 0.0572 | 0.0396
Epoch 170/300, seasonal_0 Loss: 0.0572 | 0.0396
Epoch 171/300, seasonal_0 Loss: 0.0572 | 0.0396
Epoch 172/300, seasonal_0 Loss: 0.0572 | 0.0396
Epoch 173/300, seasonal_0 Loss: 0.0571 | 0.0396
Epoch 174/300, seasonal_0 Loss: 0.0571 | 0.0396
Epoch 175/300, seasonal_0 Loss: 0.0571 | 0.0396
Epoch 176/300, seasonal_0 Loss: 0.0571 | 0.0396
Epoch 177/300, seasonal_0 Loss: 0.0571 | 0.0396
Epoch 178/300, seasonal_0 Loss: 0.0570 | 0.0396
Epoch 179/300, seasonal_0 Loss: 0.0570 | 0.0395
Epoch 180/300, seasonal_0 Loss: 0.0570 | 0.0395
Epoch 181/300, seasonal_0 Loss: 0.0570 | 0.0395
Epoch 182/300, seasonal_0 Loss: 0.0570 | 0.0395
Epoch 183/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 184/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 185/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 186/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 187/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 188/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 189/300, seasonal_0 Loss: 0.0569 | 0.0395
Epoch 190/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 191/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 192/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 193/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 194/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 195/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 196/300, seasonal_0 Loss: 0.0568 | 0.0394
Epoch 197/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 198/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 199/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 200/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 201/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 202/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 203/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 204/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 205/300, seasonal_0 Loss: 0.0567 | 0.0394
Epoch 206/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 207/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 208/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 209/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 210/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 211/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 212/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 213/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 214/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 215/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 216/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 217/300, seasonal_0 Loss: 0.0566 | 0.0393
Epoch 218/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 219/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 220/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 221/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 222/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 223/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 224/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 225/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 226/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 227/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 228/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 229/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 230/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 231/300, seasonal_0 Loss: 0.0565 | 0.0393
Epoch 232/300, seasonal_0 Loss: 0.0565 | 0.0392
Epoch 233/300, seasonal_0 Loss: 0.0565 | 0.0392
Epoch 234/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 235/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 236/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 237/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 238/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 239/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 240/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 241/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 242/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 243/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 244/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 245/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 246/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 247/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 248/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 249/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 250/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 251/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 252/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 253/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 254/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 255/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 256/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 257/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 258/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 259/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 260/300, seasonal_0 Loss: 0.0564 | 0.0392
Epoch 261/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 262/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 263/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 264/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 265/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 266/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 267/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 268/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 269/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 270/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 271/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 272/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 273/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 274/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 275/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 276/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 277/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 278/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 279/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 280/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 281/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 282/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 283/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 284/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 285/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 286/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 287/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 288/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 289/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 290/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 291/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 292/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 293/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 294/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 295/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 296/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 297/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 298/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 299/300, seasonal_0 Loss: 0.0563 | 0.0392
Epoch 300/300, seasonal_0 Loss: 0.0563 | 0.0392
Training seasonal_1 component with params: {'observation_period_num': 42, 'train_rates': 0.84304555630957, 'learning_rate': 7.828923073636323e-05, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9203755679562076}
Epoch 1/300, seasonal_1 Loss: 0.8499 | 0.4659
Epoch 2/300, seasonal_1 Loss: 0.3897 | 0.2897
Epoch 3/300, seasonal_1 Loss: 0.2851 | 0.2457
Epoch 4/300, seasonal_1 Loss: 0.2412 | 0.2175
Epoch 5/300, seasonal_1 Loss: 0.2113 | 0.1928
Epoch 6/300, seasonal_1 Loss: 0.1905 | 0.1670
Epoch 7/300, seasonal_1 Loss: 0.1750 | 0.1386
Epoch 8/300, seasonal_1 Loss: 0.1635 | 0.1198
Epoch 9/300, seasonal_1 Loss: 0.1559 | 0.1109
Epoch 10/300, seasonal_1 Loss: 0.1508 | 0.1045
Epoch 11/300, seasonal_1 Loss: 0.1465 | 0.1000
Epoch 12/300, seasonal_1 Loss: 0.1432 | 0.0973
Epoch 13/300, seasonal_1 Loss: 0.1410 | 0.1020
Epoch 14/300, seasonal_1 Loss: 0.1398 | 0.1156
Epoch 15/300, seasonal_1 Loss: 0.1391 | 0.1471
Epoch 16/300, seasonal_1 Loss: 0.1381 | 0.1967
Epoch 17/300, seasonal_1 Loss: 0.1377 | 0.2413
Epoch 18/300, seasonal_1 Loss: 0.1439 | 0.1576
Epoch 19/300, seasonal_1 Loss: 0.1579 | 0.1080
Epoch 20/300, seasonal_1 Loss: 0.1568 | 0.1004
Epoch 21/300, seasonal_1 Loss: 0.1348 | 0.0913
Epoch 22/300, seasonal_1 Loss: 0.1230 | 0.0883
Epoch 23/300, seasonal_1 Loss: 0.1200 | 0.0861
Epoch 24/300, seasonal_1 Loss: 0.1181 | 0.0841
Epoch 25/300, seasonal_1 Loss: 0.1159 | 0.0823
Epoch 26/300, seasonal_1 Loss: 0.1137 | 0.0805
Epoch 27/300, seasonal_1 Loss: 0.1118 | 0.0789
Epoch 28/300, seasonal_1 Loss: 0.1105 | 0.0774
Epoch 29/300, seasonal_1 Loss: 0.1097 | 0.0769
Epoch 30/300, seasonal_1 Loss: 0.1095 | 0.0773
Epoch 31/300, seasonal_1 Loss: 0.1092 | 0.0797
Epoch 32/300, seasonal_1 Loss: 0.1084 | 0.0837
Epoch 33/300, seasonal_1 Loss: 0.1068 | 0.0898
Epoch 34/300, seasonal_1 Loss: 0.1055 | 0.0884
Epoch 35/300, seasonal_1 Loss: 0.1067 | 0.0795
Epoch 36/300, seasonal_1 Loss: 0.1131 | 0.0750
Epoch 37/300, seasonal_1 Loss: 0.1168 | 0.0799
Epoch 38/300, seasonal_1 Loss: 0.1109 | 0.0714
Epoch 39/300, seasonal_1 Loss: 0.1037 | 0.0706
Epoch 40/300, seasonal_1 Loss: 0.1009 | 0.0706
Epoch 41/300, seasonal_1 Loss: 0.0996 | 0.0715
Epoch 42/300, seasonal_1 Loss: 0.0990 | 0.0724
Epoch 43/300, seasonal_1 Loss: 0.0984 | 0.0734
Epoch 44/300, seasonal_1 Loss: 0.0975 | 0.0737
Epoch 45/300, seasonal_1 Loss: 0.0964 | 0.0728
Epoch 46/300, seasonal_1 Loss: 0.0960 | 0.0695
Epoch 47/300, seasonal_1 Loss: 0.0972 | 0.0662
Epoch 48/300, seasonal_1 Loss: 0.0999 | 0.0678
Epoch 49/300, seasonal_1 Loss: 0.1008 | 0.0697
Epoch 50/300, seasonal_1 Loss: 0.0985 | 0.0662
Epoch 51/300, seasonal_1 Loss: 0.0955 | 0.0652
Epoch 52/300, seasonal_1 Loss: 0.0941 | 0.0654
Epoch 53/300, seasonal_1 Loss: 0.0938 | 0.0678
Epoch 54/300, seasonal_1 Loss: 0.0937 | 0.0702
Epoch 55/300, seasonal_1 Loss: 0.0931 | 0.0710
Epoch 56/300, seasonal_1 Loss: 0.0920 | 0.0692
Epoch 57/300, seasonal_1 Loss: 0.0909 | 0.0654
Epoch 58/300, seasonal_1 Loss: 0.0910 | 0.0624
Epoch 59/300, seasonal_1 Loss: 0.0923 | 0.0627
Epoch 60/300, seasonal_1 Loss: 0.0935 | 0.0643
Epoch 61/300, seasonal_1 Loss: 0.0926 | 0.0634
Epoch 62/300, seasonal_1 Loss: 0.0909 | 0.0617
Epoch 63/300, seasonal_1 Loss: 0.0894 | 0.0616
Epoch 64/300, seasonal_1 Loss: 0.0888 | 0.0621
Epoch 65/300, seasonal_1 Loss: 0.0885 | 0.0633
Epoch 66/300, seasonal_1 Loss: 0.0881 | 0.0633
Epoch 67/300, seasonal_1 Loss: 0.0875 | 0.0623
Epoch 68/300, seasonal_1 Loss: 0.0869 | 0.0610
Epoch 69/300, seasonal_1 Loss: 0.0866 | 0.0596
Epoch 70/300, seasonal_1 Loss: 0.0866 | 0.0593
Epoch 71/300, seasonal_1 Loss: 0.0868 | 0.0595
Epoch 72/300, seasonal_1 Loss: 0.0868 | 0.0595
Epoch 73/300, seasonal_1 Loss: 0.0864 | 0.0593
Epoch 74/300, seasonal_1 Loss: 0.0858 | 0.0588
Epoch 75/300, seasonal_1 Loss: 0.0853 | 0.0587
Epoch 76/300, seasonal_1 Loss: 0.0850 | 0.0589
Epoch 77/300, seasonal_1 Loss: 0.0848 | 0.0593
Epoch 78/300, seasonal_1 Loss: 0.0845 | 0.0590
Epoch 79/300, seasonal_1 Loss: 0.0841 | 0.0585
Epoch 80/300, seasonal_1 Loss: 0.0838 | 0.0579
Epoch 81/300, seasonal_1 Loss: 0.0836 | 0.0574
Epoch 82/300, seasonal_1 Loss: 0.0835 | 0.0572
Epoch 83/300, seasonal_1 Loss: 0.0835 | 0.0572
Epoch 84/300, seasonal_1 Loss: 0.0834 | 0.0571
Epoch 85/300, seasonal_1 Loss: 0.0831 | 0.0570
Epoch 86/300, seasonal_1 Loss: 0.0829 | 0.0568
Epoch 87/300, seasonal_1 Loss: 0.0826 | 0.0567
Epoch 88/300, seasonal_1 Loss: 0.0824 | 0.0567
Epoch 89/300, seasonal_1 Loss: 0.0822 | 0.0568
Epoch 90/300, seasonal_1 Loss: 0.0820 | 0.0566
Epoch 91/300, seasonal_1 Loss: 0.0818 | 0.0563
Epoch 92/300, seasonal_1 Loss: 0.0816 | 0.0561
Epoch 93/300, seasonal_1 Loss: 0.0814 | 0.0559
Epoch 94/300, seasonal_1 Loss: 0.0813 | 0.0557
Epoch 95/300, seasonal_1 Loss: 0.0812 | 0.0556
Epoch 96/300, seasonal_1 Loss: 0.0811 | 0.0555
Epoch 97/300, seasonal_1 Loss: 0.0810 | 0.0554
Epoch 98/300, seasonal_1 Loss: 0.0808 | 0.0554
Epoch 99/300, seasonal_1 Loss: 0.0807 | 0.0553
Epoch 100/300, seasonal_1 Loss: 0.0805 | 0.0552
Epoch 101/300, seasonal_1 Loss: 0.0803 | 0.0552
Epoch 102/300, seasonal_1 Loss: 0.0802 | 0.0550
Epoch 103/300, seasonal_1 Loss: 0.0801 | 0.0549
Epoch 104/300, seasonal_1 Loss: 0.0799 | 0.0548
Epoch 105/300, seasonal_1 Loss: 0.0798 | 0.0547
Epoch 106/300, seasonal_1 Loss: 0.0797 | 0.0546
Epoch 107/300, seasonal_1 Loss: 0.0796 | 0.0545
Epoch 108/300, seasonal_1 Loss: 0.0795 | 0.0544
Epoch 109/300, seasonal_1 Loss: 0.0794 | 0.0543
Epoch 110/300, seasonal_1 Loss: 0.0793 | 0.0543
Epoch 111/300, seasonal_1 Loss: 0.0792 | 0.0542
Epoch 112/300, seasonal_1 Loss: 0.0790 | 0.0541
Epoch 113/300, seasonal_1 Loss: 0.0789 | 0.0541
Epoch 114/300, seasonal_1 Loss: 0.0788 | 0.0540
Epoch 115/300, seasonal_1 Loss: 0.0787 | 0.0539
Epoch 116/300, seasonal_1 Loss: 0.0786 | 0.0538
Epoch 117/300, seasonal_1 Loss: 0.0785 | 0.0538
Epoch 118/300, seasonal_1 Loss: 0.0784 | 0.0537
Epoch 119/300, seasonal_1 Loss: 0.0783 | 0.0536
Epoch 120/300, seasonal_1 Loss: 0.0782 | 0.0536
Epoch 121/300, seasonal_1 Loss: 0.0781 | 0.0535
Epoch 122/300, seasonal_1 Loss: 0.0781 | 0.0534
Epoch 123/300, seasonal_1 Loss: 0.0780 | 0.0534
Epoch 124/300, seasonal_1 Loss: 0.0779 | 0.0533
Epoch 125/300, seasonal_1 Loss: 0.0778 | 0.0532
Epoch 126/300, seasonal_1 Loss: 0.0777 | 0.0532
Epoch 127/300, seasonal_1 Loss: 0.0776 | 0.0531
Epoch 128/300, seasonal_1 Loss: 0.0776 | 0.0531
Epoch 129/300, seasonal_1 Loss: 0.0775 | 0.0530
Epoch 130/300, seasonal_1 Loss: 0.0774 | 0.0529
Epoch 131/300, seasonal_1 Loss: 0.0773 | 0.0529
Epoch 132/300, seasonal_1 Loss: 0.0773 | 0.0529
Epoch 133/300, seasonal_1 Loss: 0.0772 | 0.0528
Epoch 134/300, seasonal_1 Loss: 0.0771 | 0.0527
Epoch 135/300, seasonal_1 Loss: 0.0771 | 0.0527
Epoch 136/300, seasonal_1 Loss: 0.0770 | 0.0527
Epoch 137/300, seasonal_1 Loss: 0.0769 | 0.0526
Epoch 138/300, seasonal_1 Loss: 0.0768 | 0.0526
Epoch 139/300, seasonal_1 Loss: 0.0768 | 0.0525
Epoch 140/300, seasonal_1 Loss: 0.0767 | 0.0525
Epoch 141/300, seasonal_1 Loss: 0.0766 | 0.0524
Epoch 142/300, seasonal_1 Loss: 0.0766 | 0.0524
Epoch 143/300, seasonal_1 Loss: 0.0765 | 0.0524
Epoch 144/300, seasonal_1 Loss: 0.0765 | 0.0523
Epoch 145/300, seasonal_1 Loss: 0.0764 | 0.0523
Epoch 146/300, seasonal_1 Loss: 0.0763 | 0.0522
Epoch 147/300, seasonal_1 Loss: 0.0763 | 0.0522
Epoch 148/300, seasonal_1 Loss: 0.0762 | 0.0522
Epoch 149/300, seasonal_1 Loss: 0.0762 | 0.0521
Epoch 150/300, seasonal_1 Loss: 0.0761 | 0.0521
Epoch 151/300, seasonal_1 Loss: 0.0761 | 0.0521
Epoch 152/300, seasonal_1 Loss: 0.0760 | 0.0520
Epoch 153/300, seasonal_1 Loss: 0.0759 | 0.0520
Epoch 154/300, seasonal_1 Loss: 0.0759 | 0.0520
Epoch 155/300, seasonal_1 Loss: 0.0758 | 0.0520
Epoch 156/300, seasonal_1 Loss: 0.0758 | 0.0519
Epoch 157/300, seasonal_1 Loss: 0.0757 | 0.0519
Epoch 158/300, seasonal_1 Loss: 0.0757 | 0.0519
Epoch 159/300, seasonal_1 Loss: 0.0757 | 0.0518
Epoch 160/300, seasonal_1 Loss: 0.0756 | 0.0518
Epoch 161/300, seasonal_1 Loss: 0.0756 | 0.0518
Epoch 162/300, seasonal_1 Loss: 0.0755 | 0.0518
Epoch 163/300, seasonal_1 Loss: 0.0755 | 0.0517
Epoch 164/300, seasonal_1 Loss: 0.0754 | 0.0517
Epoch 165/300, seasonal_1 Loss: 0.0754 | 0.0517
Epoch 166/300, seasonal_1 Loss: 0.0754 | 0.0517
Epoch 167/300, seasonal_1 Loss: 0.0753 | 0.0516
Epoch 168/300, seasonal_1 Loss: 0.0753 | 0.0516
Epoch 169/300, seasonal_1 Loss: 0.0753 | 0.0516
Epoch 170/300, seasonal_1 Loss: 0.0752 | 0.0516
Epoch 171/300, seasonal_1 Loss: 0.0752 | 0.0515
Epoch 172/300, seasonal_1 Loss: 0.0752 | 0.0515
Epoch 173/300, seasonal_1 Loss: 0.0751 | 0.0515
Epoch 174/300, seasonal_1 Loss: 0.0751 | 0.0515
Epoch 175/300, seasonal_1 Loss: 0.0750 | 0.0515
Epoch 176/300, seasonal_1 Loss: 0.0750 | 0.0514
Epoch 177/300, seasonal_1 Loss: 0.0750 | 0.0514
Epoch 178/300, seasonal_1 Loss: 0.0750 | 0.0514
Epoch 179/300, seasonal_1 Loss: 0.0749 | 0.0514
Epoch 180/300, seasonal_1 Loss: 0.0749 | 0.0514
Epoch 181/300, seasonal_1 Loss: 0.0749 | 0.0514
Epoch 182/300, seasonal_1 Loss: 0.0748 | 0.0513
Epoch 183/300, seasonal_1 Loss: 0.0748 | 0.0513
Epoch 184/300, seasonal_1 Loss: 0.0748 | 0.0513
Epoch 185/300, seasonal_1 Loss: 0.0748 | 0.0513
Epoch 186/300, seasonal_1 Loss: 0.0747 | 0.0513
Epoch 187/300, seasonal_1 Loss: 0.0747 | 0.0513
Epoch 188/300, seasonal_1 Loss: 0.0747 | 0.0512
Epoch 189/300, seasonal_1 Loss: 0.0747 | 0.0512
Epoch 190/300, seasonal_1 Loss: 0.0746 | 0.0512
Epoch 191/300, seasonal_1 Loss: 0.0746 | 0.0512
Epoch 192/300, seasonal_1 Loss: 0.0746 | 0.0512
Epoch 193/300, seasonal_1 Loss: 0.0746 | 0.0512
Epoch 194/300, seasonal_1 Loss: 0.0745 | 0.0512
Epoch 195/300, seasonal_1 Loss: 0.0745 | 0.0511
Epoch 196/300, seasonal_1 Loss: 0.0745 | 0.0511
Epoch 197/300, seasonal_1 Loss: 0.0745 | 0.0511
Epoch 198/300, seasonal_1 Loss: 0.0744 | 0.0511
Epoch 199/300, seasonal_1 Loss: 0.0744 | 0.0511
Epoch 200/300, seasonal_1 Loss: 0.0744 | 0.0511
Epoch 201/300, seasonal_1 Loss: 0.0744 | 0.0511
Epoch 202/300, seasonal_1 Loss: 0.0744 | 0.0511
Epoch 203/300, seasonal_1 Loss: 0.0744 | 0.0511
Epoch 204/300, seasonal_1 Loss: 0.0743 | 0.0510
Epoch 205/300, seasonal_1 Loss: 0.0743 | 0.0510
Epoch 206/300, seasonal_1 Loss: 0.0743 | 0.0510
Epoch 207/300, seasonal_1 Loss: 0.0743 | 0.0510
Epoch 208/300, seasonal_1 Loss: 0.0743 | 0.0510
Epoch 209/300, seasonal_1 Loss: 0.0742 | 0.0510
Epoch 210/300, seasonal_1 Loss: 0.0742 | 0.0510
Epoch 211/300, seasonal_1 Loss: 0.0742 | 0.0510
Epoch 212/300, seasonal_1 Loss: 0.0742 | 0.0510
Epoch 213/300, seasonal_1 Loss: 0.0742 | 0.0510
Epoch 214/300, seasonal_1 Loss: 0.0742 | 0.0510
Epoch 215/300, seasonal_1 Loss: 0.0742 | 0.0509
Epoch 216/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 217/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 218/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 219/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 220/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 221/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 222/300, seasonal_1 Loss: 0.0741 | 0.0509
Epoch 223/300, seasonal_1 Loss: 0.0740 | 0.0509
Epoch 224/300, seasonal_1 Loss: 0.0740 | 0.0509
Epoch 225/300, seasonal_1 Loss: 0.0740 | 0.0509
Epoch 226/300, seasonal_1 Loss: 0.0740 | 0.0509
Epoch 227/300, seasonal_1 Loss: 0.0740 | 0.0509
Epoch 228/300, seasonal_1 Loss: 0.0740 | 0.0509
Epoch 229/300, seasonal_1 Loss: 0.0740 | 0.0508
Epoch 230/300, seasonal_1 Loss: 0.0740 | 0.0508
Epoch 231/300, seasonal_1 Loss: 0.0740 | 0.0508
Epoch 232/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 233/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 234/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 235/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 236/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 237/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 238/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 239/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 240/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 241/300, seasonal_1 Loss: 0.0739 | 0.0508
Epoch 242/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 243/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 244/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 245/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 246/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 247/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 248/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 249/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 250/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 251/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 252/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 253/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 254/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 255/300, seasonal_1 Loss: 0.0738 | 0.0507
Epoch 256/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 257/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 258/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 259/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 260/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 261/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 262/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 263/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 264/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 265/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 266/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 267/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 268/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 269/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 270/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 271/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 272/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 273/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 274/300, seasonal_1 Loss: 0.0737 | 0.0507
Epoch 275/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 276/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 277/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 278/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 279/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 280/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 281/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 282/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 283/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 284/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 285/300, seasonal_1 Loss: 0.0736 | 0.0507
Epoch 286/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 287/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 288/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 289/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 290/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 291/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 292/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 293/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 294/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 295/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 296/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 297/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 298/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 299/300, seasonal_1 Loss: 0.0736 | 0.0506
Epoch 300/300, seasonal_1 Loss: 0.0736 | 0.0506
Training seasonal_2 component with params: {'observation_period_num': 7, 'train_rates': 0.8696541178840292, 'learning_rate': 0.0009702505495014734, 'batch_size': 139, 'step_size': 5, 'gamma': 0.8241617900856872}
Epoch 1/300, seasonal_2 Loss: 0.5615 | 0.2865
Epoch 2/300, seasonal_2 Loss: 0.1569 | 0.1542
Epoch 3/300, seasonal_2 Loss: 0.1493 | 0.1340
Epoch 4/300, seasonal_2 Loss: 0.1372 | 0.1168
Epoch 5/300, seasonal_2 Loss: 0.1513 | 0.0911
Epoch 6/300, seasonal_2 Loss: 0.1395 | 0.0701
Epoch 7/300, seasonal_2 Loss: 0.1341 | 0.0785
Epoch 8/300, seasonal_2 Loss: 0.1364 | 0.0918
Epoch 9/300, seasonal_2 Loss: 0.1212 | 0.0744
Epoch 10/300, seasonal_2 Loss: 0.1083 | 0.0626
Epoch 11/300, seasonal_2 Loss: 0.1071 | 0.0758
Epoch 12/300, seasonal_2 Loss: 0.1097 | 0.0664
Epoch 13/300, seasonal_2 Loss: 0.1091 | 0.0586
Epoch 14/300, seasonal_2 Loss: 0.1029 | 0.0571
Epoch 15/300, seasonal_2 Loss: 0.0991 | 0.0598
Epoch 16/300, seasonal_2 Loss: 0.0991 | 0.0574
Epoch 17/300, seasonal_2 Loss: 0.0954 | 0.0500
Epoch 18/300, seasonal_2 Loss: 0.0899 | 0.0468
Epoch 19/300, seasonal_2 Loss: 0.0892 | 0.0482
Epoch 20/300, seasonal_2 Loss: 0.0893 | 0.0504
Epoch 21/300, seasonal_2 Loss: 0.0889 | 0.0478
Epoch 22/300, seasonal_2 Loss: 0.0887 | 0.0474
Epoch 23/300, seasonal_2 Loss: 0.0879 | 0.0480
Epoch 24/300, seasonal_2 Loss: 0.0875 | 0.0474
Epoch 25/300, seasonal_2 Loss: 0.0869 | 0.0467
Epoch 26/300, seasonal_2 Loss: 0.0864 | 0.0471
Epoch 27/300, seasonal_2 Loss: 0.0872 | 0.0470
Epoch 28/300, seasonal_2 Loss: 0.0899 | 0.0490
Epoch 29/300, seasonal_2 Loss: 0.0949 | 0.0500
Epoch 30/300, seasonal_2 Loss: 0.0954 | 0.0459
Epoch 31/300, seasonal_2 Loss: 0.0882 | 0.0446
Epoch 32/300, seasonal_2 Loss: 0.0856 | 0.0446
Epoch 33/300, seasonal_2 Loss: 0.0842 | 0.0452
Epoch 34/300, seasonal_2 Loss: 0.0830 | 0.0448
Epoch 35/300, seasonal_2 Loss: 0.0827 | 0.0438
Epoch 36/300, seasonal_2 Loss: 0.0823 | 0.0437
Epoch 37/300, seasonal_2 Loss: 0.0820 | 0.0434
Epoch 38/300, seasonal_2 Loss: 0.0818 | 0.0435
Epoch 39/300, seasonal_2 Loss: 0.0816 | 0.0431
Epoch 40/300, seasonal_2 Loss: 0.0814 | 0.0430
Epoch 41/300, seasonal_2 Loss: 0.0812 | 0.0428
Epoch 42/300, seasonal_2 Loss: 0.0810 | 0.0426
Epoch 43/300, seasonal_2 Loss: 0.0809 | 0.0425
Epoch 44/300, seasonal_2 Loss: 0.0807 | 0.0423
Epoch 45/300, seasonal_2 Loss: 0.0806 | 0.0422
Epoch 46/300, seasonal_2 Loss: 0.0804 | 0.0421
Epoch 47/300, seasonal_2 Loss: 0.0803 | 0.0420
Epoch 48/300, seasonal_2 Loss: 0.0802 | 0.0419
Epoch 49/300, seasonal_2 Loss: 0.0801 | 0.0418
Epoch 50/300, seasonal_2 Loss: 0.0800 | 0.0417
Epoch 51/300, seasonal_2 Loss: 0.0799 | 0.0416
Epoch 52/300, seasonal_2 Loss: 0.0798 | 0.0415
Epoch 53/300, seasonal_2 Loss: 0.0798 | 0.0415
Epoch 54/300, seasonal_2 Loss: 0.0797 | 0.0414
Epoch 55/300, seasonal_2 Loss: 0.0796 | 0.0413
Epoch 56/300, seasonal_2 Loss: 0.0796 | 0.0413
Epoch 57/300, seasonal_2 Loss: 0.0795 | 0.0412
Epoch 58/300, seasonal_2 Loss: 0.0795 | 0.0412
Epoch 59/300, seasonal_2 Loss: 0.0794 | 0.0411
Epoch 60/300, seasonal_2 Loss: 0.0794 | 0.0411
Epoch 61/300, seasonal_2 Loss: 0.0794 | 0.0411
Epoch 62/300, seasonal_2 Loss: 0.0793 | 0.0410
Epoch 63/300, seasonal_2 Loss: 0.0793 | 0.0410
Epoch 64/300, seasonal_2 Loss: 0.0793 | 0.0410
Epoch 65/300, seasonal_2 Loss: 0.0792 | 0.0410
Epoch 66/300, seasonal_2 Loss: 0.0792 | 0.0409
Epoch 67/300, seasonal_2 Loss: 0.0792 | 0.0409
Epoch 68/300, seasonal_2 Loss: 0.0792 | 0.0409
Epoch 69/300, seasonal_2 Loss: 0.0791 | 0.0409
Epoch 70/300, seasonal_2 Loss: 0.0791 | 0.0409
Epoch 71/300, seasonal_2 Loss: 0.0791 | 0.0408
Epoch 72/300, seasonal_2 Loss: 0.0791 | 0.0408
Epoch 73/300, seasonal_2 Loss: 0.0791 | 0.0408
Epoch 74/300, seasonal_2 Loss: 0.0790 | 0.0408
Epoch 75/300, seasonal_2 Loss: 0.0790 | 0.0408
Epoch 76/300, seasonal_2 Loss: 0.0790 | 0.0408
Epoch 77/300, seasonal_2 Loss: 0.0790 | 0.0408
Epoch 78/300, seasonal_2 Loss: 0.0790 | 0.0408
Epoch 79/300, seasonal_2 Loss: 0.0790 | 0.0407
Epoch 80/300, seasonal_2 Loss: 0.0790 | 0.0407
Epoch 81/300, seasonal_2 Loss: 0.0790 | 0.0407
Epoch 82/300, seasonal_2 Loss: 0.0790 | 0.0407
Epoch 83/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 84/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 85/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 86/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 87/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 88/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 89/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 90/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 91/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 92/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 93/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 94/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 95/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 96/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 97/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 98/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 99/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 100/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 101/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 102/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 103/300, seasonal_2 Loss: 0.0789 | 0.0407
Epoch 104/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 105/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 106/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 107/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 108/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 109/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 110/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 111/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 112/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 113/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 114/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 115/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 116/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 117/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 118/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 119/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 120/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 121/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 122/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 123/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 124/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 125/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 126/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 127/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 128/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 129/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 130/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 131/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 132/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 133/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 134/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 135/300, seasonal_2 Loss: 0.0789 | 0.0406
Epoch 136/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 137/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 138/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 139/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 140/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 141/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 142/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 143/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 144/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 145/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 146/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 147/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 148/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 149/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 150/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 151/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 152/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 153/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 154/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 155/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 156/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 157/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 158/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 159/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 160/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 161/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 162/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 163/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 164/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 165/300, seasonal_2 Loss: 0.0788 | 0.0406
Epoch 166/300, seasonal_2 Loss: 0.0788 | 0.0406
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.8892739102352035, 'learning_rate': 8.164798088119659e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9079705323893845}
Epoch 1/300, seasonal_3 Loss: 0.3147 | 0.1784
Epoch 2/300, seasonal_3 Loss: 0.1804 | 0.1192
Epoch 3/300, seasonal_3 Loss: 0.1525 | 0.0952
Epoch 4/300, seasonal_3 Loss: 0.1373 | 0.0820
Epoch 5/300, seasonal_3 Loss: 0.1291 | 0.0759
Epoch 6/300, seasonal_3 Loss: 0.1237 | 0.0728
Epoch 7/300, seasonal_3 Loss: 0.1195 | 0.0709
Epoch 8/300, seasonal_3 Loss: 0.1159 | 0.0688
Epoch 9/300, seasonal_3 Loss: 0.1133 | 0.0682
Epoch 10/300, seasonal_3 Loss: 0.1109 | 0.0678
Epoch 11/300, seasonal_3 Loss: 0.1087 | 0.0672
Epoch 12/300, seasonal_3 Loss: 0.1069 | 0.0658
Epoch 13/300, seasonal_3 Loss: 0.1055 | 0.0636
Epoch 14/300, seasonal_3 Loss: 0.1044 | 0.0568
Epoch 15/300, seasonal_3 Loss: 0.1044 | 0.0563
Epoch 16/300, seasonal_3 Loss: 0.1038 | 0.0583
Epoch 17/300, seasonal_3 Loss: 0.1021 | 0.0595
Epoch 18/300, seasonal_3 Loss: 0.0995 | 0.0592
Epoch 19/300, seasonal_3 Loss: 0.0971 | 0.0578
Epoch 20/300, seasonal_3 Loss: 0.0955 | 0.0556
Epoch 21/300, seasonal_3 Loss: 0.0943 | 0.0538
Epoch 22/300, seasonal_3 Loss: 0.0930 | 0.0534
Epoch 23/300, seasonal_3 Loss: 0.0907 | 0.0515
Epoch 24/300, seasonal_3 Loss: 0.0894 | 0.0499
Epoch 25/300, seasonal_3 Loss: 0.0884 | 0.0485
Epoch 26/300, seasonal_3 Loss: 0.0874 | 0.0470
Epoch 27/300, seasonal_3 Loss: 0.0862 | 0.0455
Epoch 28/300, seasonal_3 Loss: 0.0853 | 0.0441
Epoch 29/300, seasonal_3 Loss: 0.0845 | 0.0429
Epoch 30/300, seasonal_3 Loss: 0.0838 | 0.0418
Epoch 31/300, seasonal_3 Loss: 0.0832 | 0.0409
Epoch 32/300, seasonal_3 Loss: 0.0827 | 0.0402
Epoch 33/300, seasonal_3 Loss: 0.0822 | 0.0396
Epoch 34/300, seasonal_3 Loss: 0.0816 | 0.0388
Epoch 35/300, seasonal_3 Loss: 0.0811 | 0.0384
Epoch 36/300, seasonal_3 Loss: 0.0805 | 0.0380
Epoch 37/300, seasonal_3 Loss: 0.0800 | 0.0377
Epoch 38/300, seasonal_3 Loss: 0.0796 | 0.0374
Epoch 39/300, seasonal_3 Loss: 0.0791 | 0.0372
Epoch 40/300, seasonal_3 Loss: 0.0787 | 0.0368
Epoch 41/300, seasonal_3 Loss: 0.0784 | 0.0366
Epoch 42/300, seasonal_3 Loss: 0.0780 | 0.0364
Epoch 43/300, seasonal_3 Loss: 0.0776 | 0.0362
Epoch 44/300, seasonal_3 Loss: 0.0773 | 0.0360
Epoch 45/300, seasonal_3 Loss: 0.0770 | 0.0358
Epoch 46/300, seasonal_3 Loss: 0.0767 | 0.0356
Epoch 47/300, seasonal_3 Loss: 0.0763 | 0.0353
Epoch 48/300, seasonal_3 Loss: 0.0761 | 0.0352
Epoch 49/300, seasonal_3 Loss: 0.0759 | 0.0350
Epoch 50/300, seasonal_3 Loss: 0.0756 | 0.0348
Epoch 51/300, seasonal_3 Loss: 0.0754 | 0.0347
Epoch 52/300, seasonal_3 Loss: 0.0751 | 0.0345
Epoch 53/300, seasonal_3 Loss: 0.0748 | 0.0345
Epoch 54/300, seasonal_3 Loss: 0.0745 | 0.0343
Epoch 55/300, seasonal_3 Loss: 0.0742 | 0.0342
Epoch 56/300, seasonal_3 Loss: 0.0739 | 0.0340
Epoch 57/300, seasonal_3 Loss: 0.0736 | 0.0338
Epoch 58/300, seasonal_3 Loss: 0.0733 | 0.0337
Epoch 59/300, seasonal_3 Loss: 0.0730 | 0.0335
Epoch 60/300, seasonal_3 Loss: 0.0726 | 0.0336
Epoch 61/300, seasonal_3 Loss: 0.0724 | 0.0334
Epoch 62/300, seasonal_3 Loss: 0.0720 | 0.0333
Epoch 63/300, seasonal_3 Loss: 0.0717 | 0.0332
Epoch 64/300, seasonal_3 Loss: 0.0714 | 0.0330
Epoch 65/300, seasonal_3 Loss: 0.0711 | 0.0329
Epoch 66/300, seasonal_3 Loss: 0.0708 | 0.0330
Epoch 67/300, seasonal_3 Loss: 0.0706 | 0.0330
Epoch 68/300, seasonal_3 Loss: 0.0704 | 0.0329
Epoch 69/300, seasonal_3 Loss: 0.0702 | 0.0328
Epoch 70/300, seasonal_3 Loss: 0.0699 | 0.0327
Epoch 71/300, seasonal_3 Loss: 0.0697 | 0.0327
Epoch 72/300, seasonal_3 Loss: 0.0695 | 0.0326
Epoch 73/300, seasonal_3 Loss: 0.0695 | 0.0332
Epoch 74/300, seasonal_3 Loss: 0.0694 | 0.0333
Epoch 75/300, seasonal_3 Loss: 0.0693 | 0.0335
Epoch 76/300, seasonal_3 Loss: 0.0691 | 0.0335
Epoch 77/300, seasonal_3 Loss: 0.0689 | 0.0334
Epoch 78/300, seasonal_3 Loss: 0.0687 | 0.0333
Epoch 79/300, seasonal_3 Loss: 0.0685 | 0.0330
Epoch 80/300, seasonal_3 Loss: 0.0684 | 0.0330
Epoch 81/300, seasonal_3 Loss: 0.0682 | 0.0328
Epoch 82/300, seasonal_3 Loss: 0.0681 | 0.0326
Epoch 83/300, seasonal_3 Loss: 0.0679 | 0.0324
Epoch 84/300, seasonal_3 Loss: 0.0678 | 0.0323
Epoch 85/300, seasonal_3 Loss: 0.0676 | 0.0322
Epoch 86/300, seasonal_3 Loss: 0.0675 | 0.0322
Epoch 87/300, seasonal_3 Loss: 0.0674 | 0.0322
Epoch 88/300, seasonal_3 Loss: 0.0673 | 0.0323
Epoch 89/300, seasonal_3 Loss: 0.0673 | 0.0323
Epoch 90/300, seasonal_3 Loss: 0.0672 | 0.0323
Epoch 91/300, seasonal_3 Loss: 0.0671 | 0.0322
Epoch 92/300, seasonal_3 Loss: 0.0671 | 0.0321
Epoch 93/300, seasonal_3 Loss: 0.0672 | 0.0321
Epoch 94/300, seasonal_3 Loss: 0.0672 | 0.0321
Epoch 95/300, seasonal_3 Loss: 0.0672 | 0.0322
Epoch 96/300, seasonal_3 Loss: 0.0672 | 0.0322
Epoch 97/300, seasonal_3 Loss: 0.0672 | 0.0322
Epoch 98/300, seasonal_3 Loss: 0.0671 | 0.0322
Epoch 99/300, seasonal_3 Loss: 0.0669 | 0.0319
Epoch 100/300, seasonal_3 Loss: 0.0668 | 0.0318
Epoch 101/300, seasonal_3 Loss: 0.0666 | 0.0315
Epoch 102/300, seasonal_3 Loss: 0.0663 | 0.0313
Epoch 103/300, seasonal_3 Loss: 0.0659 | 0.0311
Epoch 104/300, seasonal_3 Loss: 0.0656 | 0.0309
Epoch 105/300, seasonal_3 Loss: 0.0654 | 0.0308
Epoch 106/300, seasonal_3 Loss: 0.0652 | 0.0307
Epoch 107/300, seasonal_3 Loss: 0.0650 | 0.0305
Epoch 108/300, seasonal_3 Loss: 0.0648 | 0.0304
Epoch 109/300, seasonal_3 Loss: 0.0646 | 0.0304
Epoch 110/300, seasonal_3 Loss: 0.0645 | 0.0303
Epoch 111/300, seasonal_3 Loss: 0.0643 | 0.0302
Epoch 112/300, seasonal_3 Loss: 0.0642 | 0.0302
Epoch 113/300, seasonal_3 Loss: 0.0641 | 0.0301
Epoch 114/300, seasonal_3 Loss: 0.0640 | 0.0301
Epoch 115/300, seasonal_3 Loss: 0.0639 | 0.0301
Epoch 116/300, seasonal_3 Loss: 0.0638 | 0.0300
Epoch 117/300, seasonal_3 Loss: 0.0637 | 0.0300
Epoch 118/300, seasonal_3 Loss: 0.0636 | 0.0300
Epoch 119/300, seasonal_3 Loss: 0.0635 | 0.0300
Epoch 120/300, seasonal_3 Loss: 0.0635 | 0.0300
Epoch 121/300, seasonal_3 Loss: 0.0634 | 0.0300
Epoch 122/300, seasonal_3 Loss: 0.0634 | 0.0299
Epoch 123/300, seasonal_3 Loss: 0.0633 | 0.0299
Epoch 124/300, seasonal_3 Loss: 0.0632 | 0.0299
Epoch 125/300, seasonal_3 Loss: 0.0633 | 0.0298
Epoch 126/300, seasonal_3 Loss: 0.0633 | 0.0297
Epoch 127/300, seasonal_3 Loss: 0.0634 | 0.0297
Epoch 128/300, seasonal_3 Loss: 0.0635 | 0.0297
Epoch 129/300, seasonal_3 Loss: 0.0635 | 0.0297
Epoch 130/300, seasonal_3 Loss: 0.0635 | 0.0297
Epoch 131/300, seasonal_3 Loss: 0.0634 | 0.0302
Epoch 132/300, seasonal_3 Loss: 0.0632 | 0.0301
Epoch 133/300, seasonal_3 Loss: 0.0628 | 0.0299
Epoch 134/300, seasonal_3 Loss: 0.0625 | 0.0297
Epoch 135/300, seasonal_3 Loss: 0.0624 | 0.0296
Epoch 136/300, seasonal_3 Loss: 0.0623 | 0.0295
Epoch 137/300, seasonal_3 Loss: 0.0622 | 0.0294
Epoch 138/300, seasonal_3 Loss: 0.0621 | 0.0294
Epoch 139/300, seasonal_3 Loss: 0.0621 | 0.0293
Epoch 140/300, seasonal_3 Loss: 0.0621 | 0.0293
Epoch 141/300, seasonal_3 Loss: 0.0621 | 0.0292
Epoch 142/300, seasonal_3 Loss: 0.0620 | 0.0292
Epoch 143/300, seasonal_3 Loss: 0.0620 | 0.0291
Epoch 144/300, seasonal_3 Loss: 0.0620 | 0.0291
Epoch 145/300, seasonal_3 Loss: 0.0620 | 0.0290
Epoch 146/300, seasonal_3 Loss: 0.0620 | 0.0290
Epoch 147/300, seasonal_3 Loss: 0.0620 | 0.0290
Epoch 148/300, seasonal_3 Loss: 0.0619 | 0.0290
Epoch 149/300, seasonal_3 Loss: 0.0618 | 0.0290
Epoch 150/300, seasonal_3 Loss: 0.0618 | 0.0290
Epoch 151/300, seasonal_3 Loss: 0.0617 | 0.0290
Epoch 152/300, seasonal_3 Loss: 0.0616 | 0.0290
Epoch 153/300, seasonal_3 Loss: 0.0615 | 0.0290
Epoch 154/300, seasonal_3 Loss: 0.0613 | 0.0290
Epoch 155/300, seasonal_3 Loss: 0.0612 | 0.0290
Epoch 156/300, seasonal_3 Loss: 0.0611 | 0.0290
Epoch 157/300, seasonal_3 Loss: 0.0610 | 0.0291
Epoch 158/300, seasonal_3 Loss: 0.0609 | 0.0291
Epoch 159/300, seasonal_3 Loss: 0.0607 | 0.0291
Epoch 160/300, seasonal_3 Loss: 0.0607 | 0.0291
Epoch 161/300, seasonal_3 Loss: 0.0606 | 0.0291
Epoch 162/300, seasonal_3 Loss: 0.0605 | 0.0291
Epoch 163/300, seasonal_3 Loss: 0.0604 | 0.0291
Epoch 164/300, seasonal_3 Loss: 0.0604 | 0.0291
Epoch 165/300, seasonal_3 Loss: 0.0603 | 0.0291
Epoch 166/300, seasonal_3 Loss: 0.0603 | 0.0291
Epoch 167/300, seasonal_3 Loss: 0.0603 | 0.0291
Epoch 168/300, seasonal_3 Loss: 0.0602 | 0.0290
Epoch 169/300, seasonal_3 Loss: 0.0602 | 0.0290
Epoch 170/300, seasonal_3 Loss: 0.0601 | 0.0290
Epoch 171/300, seasonal_3 Loss: 0.0601 | 0.0290
Epoch 172/300, seasonal_3 Loss: 0.0601 | 0.0290
Epoch 173/300, seasonal_3 Loss: 0.0600 | 0.0290
Epoch 174/300, seasonal_3 Loss: 0.0600 | 0.0290
Epoch 175/300, seasonal_3 Loss: 0.0599 | 0.0290
Epoch 176/300, seasonal_3 Loss: 0.0599 | 0.0289
Epoch 177/300, seasonal_3 Loss: 0.0598 | 0.0288
Epoch 178/300, seasonal_3 Loss: 0.0598 | 0.0288
Epoch 179/300, seasonal_3 Loss: 0.0598 | 0.0288
Epoch 180/300, seasonal_3 Loss: 0.0597 | 0.0288
Epoch 181/300, seasonal_3 Loss: 0.0597 | 0.0288
Epoch 182/300, seasonal_3 Loss: 0.0596 | 0.0288
Epoch 183/300, seasonal_3 Loss: 0.0596 | 0.0286
Epoch 184/300, seasonal_3 Loss: 0.0596 | 0.0286
Epoch 185/300, seasonal_3 Loss: 0.0596 | 0.0286
Epoch 186/300, seasonal_3 Loss: 0.0596 | 0.0286
Epoch 187/300, seasonal_3 Loss: 0.0595 | 0.0286
Epoch 188/300, seasonal_3 Loss: 0.0595 | 0.0286
Epoch 189/300, seasonal_3 Loss: 0.0595 | 0.0285
Epoch 190/300, seasonal_3 Loss: 0.0595 | 0.0285
Epoch 191/300, seasonal_3 Loss: 0.0595 | 0.0285
Epoch 192/300, seasonal_3 Loss: 0.0595 | 0.0285
Epoch 193/300, seasonal_3 Loss: 0.0595 | 0.0285
Epoch 194/300, seasonal_3 Loss: 0.0594 | 0.0285
Epoch 195/300, seasonal_3 Loss: 0.0594 | 0.0285
Epoch 196/300, seasonal_3 Loss: 0.0594 | 0.0285
Epoch 197/300, seasonal_3 Loss: 0.0593 | 0.0285
Epoch 198/300, seasonal_3 Loss: 0.0593 | 0.0285
Epoch 199/300, seasonal_3 Loss: 0.0593 | 0.0285
Epoch 200/300, seasonal_3 Loss: 0.0592 | 0.0285
Epoch 201/300, seasonal_3 Loss: 0.0592 | 0.0285
Epoch 202/300, seasonal_3 Loss: 0.0591 | 0.0285
Epoch 203/300, seasonal_3 Loss: 0.0591 | 0.0285
Epoch 204/300, seasonal_3 Loss: 0.0590 | 0.0285
Epoch 205/300, seasonal_3 Loss: 0.0590 | 0.0284
Epoch 206/300, seasonal_3 Loss: 0.0589 | 0.0284
Epoch 207/300, seasonal_3 Loss: 0.0588 | 0.0284
Epoch 208/300, seasonal_3 Loss: 0.0588 | 0.0284
Epoch 209/300, seasonal_3 Loss: 0.0587 | 0.0284
Epoch 210/300, seasonal_3 Loss: 0.0587 | 0.0284
Epoch 211/300, seasonal_3 Loss: 0.0586 | 0.0284
Epoch 212/300, seasonal_3 Loss: 0.0585 | 0.0284
Epoch 213/300, seasonal_3 Loss: 0.0585 | 0.0284
Epoch 214/300, seasonal_3 Loss: 0.0584 | 0.0284
Epoch 215/300, seasonal_3 Loss: 0.0584 | 0.0284
Epoch 216/300, seasonal_3 Loss: 0.0584 | 0.0284
Epoch 217/300, seasonal_3 Loss: 0.0583 | 0.0284
Epoch 218/300, seasonal_3 Loss: 0.0583 | 0.0284
Epoch 219/300, seasonal_3 Loss: 0.0583 | 0.0284
Epoch 220/300, seasonal_3 Loss: 0.0583 | 0.0283
Epoch 221/300, seasonal_3 Loss: 0.0583 | 0.0283
Epoch 222/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 223/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 224/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 225/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 226/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 227/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 228/300, seasonal_3 Loss: 0.0582 | 0.0283
Epoch 229/300, seasonal_3 Loss: 0.0581 | 0.0283
Epoch 230/300, seasonal_3 Loss: 0.0581 | 0.0283
Epoch 231/300, seasonal_3 Loss: 0.0581 | 0.0283
Epoch 232/300, seasonal_3 Loss: 0.0581 | 0.0283
Epoch 233/300, seasonal_3 Loss: 0.0580 | 0.0283
Epoch 234/300, seasonal_3 Loss: 0.0580 | 0.0283
Epoch 235/300, seasonal_3 Loss: 0.0580 | 0.0283
Epoch 236/300, seasonal_3 Loss: 0.0579 | 0.0283
Epoch 237/300, seasonal_3 Loss: 0.0579 | 0.0283
Epoch 238/300, seasonal_3 Loss: 0.0579 | 0.0284
Epoch 239/300, seasonal_3 Loss: 0.0579 | 0.0284
Epoch 240/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 241/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 242/300, seasonal_3 Loss: 0.0578 | 0.0284
Epoch 243/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 244/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 245/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 246/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 247/300, seasonal_3 Loss: 0.0577 | 0.0284
Epoch 248/300, seasonal_3 Loss: 0.0576 | 0.0284
Epoch 249/300, seasonal_3 Loss: 0.0576 | 0.0284
Epoch 250/300, seasonal_3 Loss: 0.0576 | 0.0284
Epoch 251/300, seasonal_3 Loss: 0.0576 | 0.0284
Epoch 252/300, seasonal_3 Loss: 0.0576 | 0.0284
Epoch 253/300, seasonal_3 Loss: 0.0575 | 0.0284
Epoch 254/300, seasonal_3 Loss: 0.0575 | 0.0284
Epoch 255/300, seasonal_3 Loss: 0.0575 | 0.0284
Epoch 256/300, seasonal_3 Loss: 0.0575 | 0.0284
Epoch 257/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 258/300, seasonal_3 Loss: 0.0575 | 0.0283
Epoch 259/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 260/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 261/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 262/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 263/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 264/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 265/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 266/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 267/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 268/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 269/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 270/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 271/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 272/300, seasonal_3 Loss: 0.0573 | 0.0283
Epoch 273/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 274/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 275/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 276/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 277/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 278/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 279/300, seasonal_3 Loss: 0.0572 | 0.0283
Epoch 280/300, seasonal_3 Loss: 0.0571 | 0.0283
Epoch 281/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 282/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 283/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 284/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 285/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 286/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 287/300, seasonal_3 Loss: 0.0571 | 0.0284
Epoch 288/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 289/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 290/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 291/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 292/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 293/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 294/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 295/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 296/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 297/300, seasonal_3 Loss: 0.0570 | 0.0284
Epoch 298/300, seasonal_3 Loss: 0.0569 | 0.0284
Epoch 299/300, seasonal_3 Loss: 0.0569 | 0.0284
Epoch 300/300, seasonal_3 Loss: 0.0569 | 0.0284
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9246236702065193, 'learning_rate': 0.00043450571421121324, 'batch_size': 42, 'step_size': 5, 'gamma': 0.812442863063571}
Epoch 1/300, resid Loss: 0.2121 | 0.0863
Epoch 2/300, resid Loss: 0.1240 | 0.0671
Epoch 3/300, resid Loss: 0.1097 | 0.0707
Epoch 4/300, resid Loss: 0.1051 | 0.0676
Epoch 5/300, resid Loss: 0.1051 | 0.0638
Epoch 6/300, resid Loss: 0.1011 | 0.0558
Epoch 7/300, resid Loss: 0.0953 | 0.0555
Epoch 8/300, resid Loss: 0.0906 | 0.0548
Epoch 9/300, resid Loss: 0.0868 | 0.0516
Epoch 10/300, resid Loss: 0.0848 | 0.0484
Epoch 11/300, resid Loss: 0.0827 | 0.0436
Epoch 12/300, resid Loss: 0.0825 | 0.0418
Epoch 13/300, resid Loss: 0.0809 | 0.0403
Epoch 14/300, resid Loss: 0.0789 | 0.0394
Epoch 15/300, resid Loss: 0.0781 | 0.0387
Epoch 16/300, resid Loss: 0.0769 | 0.0382
Epoch 17/300, resid Loss: 0.0762 | 0.0376
Epoch 18/300, resid Loss: 0.0752 | 0.0372
Epoch 19/300, resid Loss: 0.0742 | 0.0367
Epoch 20/300, resid Loss: 0.0734 | 0.0358
Epoch 21/300, resid Loss: 0.0723 | 0.0355
Epoch 22/300, resid Loss: 0.0716 | 0.0348
Epoch 23/300, resid Loss: 0.0709 | 0.0344
Epoch 24/300, resid Loss: 0.0704 | 0.0347
Epoch 25/300, resid Loss: 0.0702 | 0.0349
Epoch 26/300, resid Loss: 0.0698 | 0.0351
Epoch 27/300, resid Loss: 0.0698 | 0.0357
Epoch 28/300, resid Loss: 0.0696 | 0.0360
Epoch 29/300, resid Loss: 0.0692 | 0.0346
Epoch 30/300, resid Loss: 0.0689 | 0.0342
Epoch 31/300, resid Loss: 0.0683 | 0.0328
Epoch 32/300, resid Loss: 0.0680 | 0.0325
Epoch 33/300, resid Loss: 0.0676 | 0.0323
Epoch 34/300, resid Loss: 0.0673 | 0.0320
Epoch 35/300, resid Loss: 0.0670 | 0.0319
Epoch 36/300, resid Loss: 0.0667 | 0.0316
Epoch 37/300, resid Loss: 0.0665 | 0.0315
Epoch 38/300, resid Loss: 0.0663 | 0.0314
Epoch 39/300, resid Loss: 0.0660 | 0.0313
Epoch 40/300, resid Loss: 0.0659 | 0.0312
Epoch 41/300, resid Loss: 0.0657 | 0.0312
Epoch 42/300, resid Loss: 0.0656 | 0.0311
Epoch 43/300, resid Loss: 0.0654 | 0.0311
Epoch 44/300, resid Loss: 0.0653 | 0.0310
Epoch 45/300, resid Loss: 0.0652 | 0.0310
Epoch 46/300, resid Loss: 0.0651 | 0.0307
Epoch 47/300, resid Loss: 0.0650 | 0.0307
Epoch 48/300, resid Loss: 0.0650 | 0.0307
Epoch 49/300, resid Loss: 0.0650 | 0.0302
Epoch 50/300, resid Loss: 0.0649 | 0.0302
Epoch 51/300, resid Loss: 0.0649 | 0.0302
Epoch 52/300, resid Loss: 0.0648 | 0.0301
Epoch 53/300, resid Loss: 0.0647 | 0.0301
Epoch 54/300, resid Loss: 0.0646 | 0.0301
Epoch 55/300, resid Loss: 0.0646 | 0.0300
Epoch 56/300, resid Loss: 0.0645 | 0.0299
Epoch 57/300, resid Loss: 0.0644 | 0.0299
Epoch 58/300, resid Loss: 0.0644 | 0.0299
Epoch 59/300, resid Loss: 0.0644 | 0.0298
Epoch 60/300, resid Loss: 0.0643 | 0.0298
Epoch 61/300, resid Loss: 0.0643 | 0.0298
Epoch 62/300, resid Loss: 0.0643 | 0.0298
Epoch 63/300, resid Loss: 0.0643 | 0.0298
Epoch 64/300, resid Loss: 0.0643 | 0.0299
Epoch 65/300, resid Loss: 0.0642 | 0.0299
Epoch 66/300, resid Loss: 0.0642 | 0.0300
Epoch 67/300, resid Loss: 0.0642 | 0.0300
Epoch 68/300, resid Loss: 0.0642 | 0.0300
Epoch 69/300, resid Loss: 0.0641 | 0.0300
Epoch 70/300, resid Loss: 0.0641 | 0.0300
Epoch 71/300, resid Loss: 0.0641 | 0.0299
Epoch 72/300, resid Loss: 0.0640 | 0.0299
Epoch 73/300, resid Loss: 0.0640 | 0.0299
Epoch 74/300, resid Loss: 0.0639 | 0.0299
Epoch 75/300, resid Loss: 0.0639 | 0.0299
Epoch 76/300, resid Loss: 0.0639 | 0.0298
Epoch 77/300, resid Loss: 0.0639 | 0.0298
Epoch 78/300, resid Loss: 0.0638 | 0.0298
Epoch 79/300, resid Loss: 0.0638 | 0.0298
Epoch 80/300, resid Loss: 0.0638 | 0.0298
Epoch 81/300, resid Loss: 0.0638 | 0.0298
Epoch 82/300, resid Loss: 0.0638 | 0.0298
Epoch 83/300, resid Loss: 0.0638 | 0.0298
Epoch 84/300, resid Loss: 0.0638 | 0.0298
Epoch 85/300, resid Loss: 0.0637 | 0.0298
Epoch 86/300, resid Loss: 0.0637 | 0.0297
Epoch 87/300, resid Loss: 0.0637 | 0.0297
Epoch 88/300, resid Loss: 0.0637 | 0.0297
Epoch 89/300, resid Loss: 0.0637 | 0.0297
Epoch 90/300, resid Loss: 0.0637 | 0.0297
Epoch 91/300, resid Loss: 0.0637 | 0.0297
Epoch 92/300, resid Loss: 0.0637 | 0.0297
Epoch 93/300, resid Loss: 0.0637 | 0.0297
Epoch 94/300, resid Loss: 0.0637 | 0.0297
Epoch 95/300, resid Loss: 0.0637 | 0.0297
Epoch 96/300, resid Loss: 0.0637 | 0.0297
Epoch 97/300, resid Loss: 0.0637 | 0.0297
Epoch 98/300, resid Loss: 0.0637 | 0.0297
Epoch 99/300, resid Loss: 0.0637 | 0.0297
Epoch 100/300, resid Loss: 0.0637 | 0.0297
Epoch 101/300, resid Loss: 0.0637 | 0.0297
Epoch 102/300, resid Loss: 0.0637 | 0.0297
Epoch 103/300, resid Loss: 0.0637 | 0.0297
Epoch 104/300, resid Loss: 0.0637 | 0.0297
Epoch 105/300, resid Loss: 0.0637 | 0.0297
Epoch 106/300, resid Loss: 0.0637 | 0.0297
Epoch 107/300, resid Loss: 0.0637 | 0.0297
Epoch 108/300, resid Loss: 0.0637 | 0.0297
Epoch 109/300, resid Loss: 0.0637 | 0.0297
Epoch 110/300, resid Loss: 0.0637 | 0.0297
Epoch 111/300, resid Loss: 0.0637 | 0.0297
Epoch 112/300, resid Loss: 0.0637 | 0.0297
Epoch 113/300, resid Loss: 0.0637 | 0.0297
Epoch 114/300, resid Loss: 0.0637 | 0.0297
Epoch 115/300, resid Loss: 0.0637 | 0.0297
Epoch 116/300, resid Loss: 0.0637 | 0.0297
Epoch 117/300, resid Loss: 0.0637 | 0.0297
Epoch 118/300, resid Loss: 0.0637 | 0.0297
Epoch 119/300, resid Loss: 0.0636 | 0.0297
Epoch 120/300, resid Loss: 0.0636 | 0.0297
Epoch 121/300, resid Loss: 0.0636 | 0.0297
Epoch 122/300, resid Loss: 0.0636 | 0.0297
Epoch 123/300, resid Loss: 0.0636 | 0.0297
Epoch 124/300, resid Loss: 0.0636 | 0.0297
Epoch 125/300, resid Loss: 0.0636 | 0.0297
Epoch 126/300, resid Loss: 0.0636 | 0.0297
Epoch 127/300, resid Loss: 0.0636 | 0.0297
Epoch 128/300, resid Loss: 0.0636 | 0.0297
Epoch 129/300, resid Loss: 0.0636 | 0.0297
Epoch 130/300, resid Loss: 0.0636 | 0.0297
Epoch 131/300, resid Loss: 0.0636 | 0.0297
Epoch 132/300, resid Loss: 0.0636 | 0.0297
Epoch 133/300, resid Loss: 0.0636 | 0.0297
Epoch 134/300, resid Loss: 0.0636 | 0.0297
Epoch 135/300, resid Loss: 0.0636 | 0.0297
Epoch 136/300, resid Loss: 0.0636 | 0.0297
Epoch 137/300, resid Loss: 0.0636 | 0.0297
Epoch 138/300, resid Loss: 0.0636 | 0.0297
Epoch 139/300, resid Loss: 0.0636 | 0.0297
Epoch 140/300, resid Loss: 0.0636 | 0.0297
Epoch 141/300, resid Loss: 0.0636 | 0.0297
Epoch 142/300, resid Loss: 0.0636 | 0.0297
Epoch 143/300, resid Loss: 0.0636 | 0.0297
Epoch 144/300, resid Loss: 0.0636 | 0.0297
Epoch 145/300, resid Loss: 0.0636 | 0.0297
Epoch 146/300, resid Loss: 0.0636 | 0.0297
Epoch 147/300, resid Loss: 0.0636 | 0.0297
Epoch 148/300, resid Loss: 0.0636 | 0.0297
Epoch 149/300, resid Loss: 0.0636 | 0.0297
Epoch 150/300, resid Loss: 0.0636 | 0.0297
Epoch 151/300, resid Loss: 0.0636 | 0.0297
Epoch 152/300, resid Loss: 0.0636 | 0.0297
Epoch 153/300, resid Loss: 0.0636 | 0.0297
Epoch 154/300, resid Loss: 0.0636 | 0.0297
Epoch 155/300, resid Loss: 0.0636 | 0.0297
Epoch 156/300, resid Loss: 0.0636 | 0.0297
Epoch 157/300, resid Loss: 0.0636 | 0.0297
Epoch 158/300, resid Loss: 0.0636 | 0.0297
Epoch 159/300, resid Loss: 0.0636 | 0.0297
Epoch 160/300, resid Loss: 0.0636 | 0.0297
Epoch 161/300, resid Loss: 0.0636 | 0.0297
Epoch 162/300, resid Loss: 0.0636 | 0.0297
Epoch 163/300, resid Loss: 0.0636 | 0.0297
Epoch 164/300, resid Loss: 0.0636 | 0.0297
Epoch 165/300, resid Loss: 0.0636 | 0.0297
Epoch 166/300, resid Loss: 0.0636 | 0.0297
Epoch 167/300, resid Loss: 0.0636 | 0.0297
Epoch 168/300, resid Loss: 0.0636 | 0.0297
Epoch 169/300, resid Loss: 0.0636 | 0.0297
Epoch 170/300, resid Loss: 0.0636 | 0.0297
Epoch 171/300, resid Loss: 0.0636 | 0.0297
Epoch 172/300, resid Loss: 0.0636 | 0.0297
Epoch 173/300, resid Loss: 0.0636 | 0.0297
Epoch 174/300, resid Loss: 0.0636 | 0.0297
Epoch 175/300, resid Loss: 0.0636 | 0.0297
Epoch 176/300, resid Loss: 0.0636 | 0.0297
Epoch 177/300, resid Loss: 0.0636 | 0.0297
Epoch 178/300, resid Loss: 0.0636 | 0.0297
Epoch 179/300, resid Loss: 0.0636 | 0.0297
Early stopping for resid
Runtime (seconds): 1580.5632300376892
0.00010181138901250208
[189.81891]
[-2.0106378]
[-1.5774735]
[0.71467996]
[-1.9247904]
[9.115294]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 188.34856801875867
RMSE: 13.724014282226562
MAE: 13.724014282226562
R-squared: nan
[194.13599]
