[32m[I 2025-01-06 13:00:12,610][0m A new study created in memory with name: no-name-7505ccca-c35b-4306-abba-a23b471bb107[0m
[32m[I 2025-01-06 13:00:48,869][0m Trial 0 finished with value: 0.6312444272611832 and parameters: {'observation_period_num': 23, 'train_rates': 0.7214623885495439, 'learning_rate': 2.506033326595786e-05, 'batch_size': 98, 'step_size': 8, 'gamma': 0.9380591650206014}. Best is trial 0 with value: 0.6312444272611832.[0m
[32m[I 2025-01-06 13:01:27,334][0m Trial 1 finished with value: 0.9295255963736815 and parameters: {'observation_period_num': 38, 'train_rates': 0.6135861025224958, 'learning_rate': 0.0007073404701035911, 'batch_size': 233, 'step_size': 10, 'gamma': 0.8321316560779457}. Best is trial 0 with value: 0.6312444272611832.[0m
[32m[I 2025-01-06 13:02:27,913][0m Trial 2 finished with value: 0.27903689807202636 and parameters: {'observation_period_num': 28, 'train_rates': 0.8374983743584545, 'learning_rate': 2.6624902159491786e-05, 'batch_size': 66, 'step_size': 6, 'gamma': 0.951767382936841}. Best is trial 2 with value: 0.27903689807202636.[0m
[32m[I 2025-01-06 13:08:36,272][0m Trial 3 finished with value: 0.2157813378742763 and parameters: {'observation_period_num': 246, 'train_rates': 0.9097475740513241, 'learning_rate': 0.0006255279147990093, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9532151057344279}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:11:47,461][0m Trial 4 finished with value: 0.8297411746953696 and parameters: {'observation_period_num': 159, 'train_rates': 0.7279242308575435, 'learning_rate': 0.00010644435521799113, 'batch_size': 147, 'step_size': 10, 'gamma': 0.8007170046288071}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:13:20,509][0m Trial 5 finished with value: 0.676576336570408 and parameters: {'observation_period_num': 72, 'train_rates': 0.8804167147977993, 'learning_rate': 3.018837629343782e-06, 'batch_size': 123, 'step_size': 15, 'gamma': 0.7861317287345754}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:14:49,899][0m Trial 6 finished with value: 0.7902482151985168 and parameters: {'observation_period_num': 75, 'train_rates': 0.7670307385711358, 'learning_rate': 0.00013445443930955976, 'batch_size': 120, 'step_size': 13, 'gamma': 0.9640493488805453}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:20:07,277][0m Trial 7 finished with value: 0.5415461283566346 and parameters: {'observation_period_num': 223, 'train_rates': 0.8530970985774533, 'learning_rate': 1.3126409801416323e-05, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8207776422230815}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:25:51,549][0m Trial 8 finished with value: 0.2949715674824318 and parameters: {'observation_period_num': 238, 'train_rates': 0.8541118058853951, 'learning_rate': 0.00019414489493759898, 'batch_size': 254, 'step_size': 11, 'gamma': 0.8814088471489889}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:28:31,658][0m Trial 9 finished with value: 0.9273579778413843 and parameters: {'observation_period_num': 147, 'train_rates': 0.6160800601618222, 'learning_rate': 3.263776070711572e-05, 'batch_size': 103, 'step_size': 14, 'gamma': 0.7925637178907755}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:33:12,953][0m Trial 10 finished with value: 0.28139832615852356 and parameters: {'observation_period_num': 189, 'train_rates': 0.9840679820308276, 'learning_rate': 0.0006796092874874609, 'batch_size': 179, 'step_size': 1, 'gamma': 0.901085638891757}. Best is trial 3 with value: 0.2157813378742763.[0m
[32m[I 2025-01-06 13:36:52,558][0m Trial 11 finished with value: 0.13903269534440418 and parameters: {'observation_period_num': 97, 'train_rates': 0.9469854907790777, 'learning_rate': 4.938052528504647e-06, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9881990555759707}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 13:40:47,773][0m Trial 12 finished with value: 0.2702347350120544 and parameters: {'observation_period_num': 102, 'train_rates': 0.9827573972137351, 'learning_rate': 1.106623955945775e-06, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9875462279070885}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 13:43:28,783][0m Trial 13 finished with value: 0.5979415038856891 and parameters: {'observation_period_num': 122, 'train_rates': 0.9231572476402586, 'learning_rate': 5.787781632564975e-06, 'batch_size': 179, 'step_size': 4, 'gamma': 0.9180471217016688}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 13:48:29,364][0m Trial 14 finished with value: 0.18960560078047356 and parameters: {'observation_period_num': 190, 'train_rates': 0.923464983507879, 'learning_rate': 4.171588916380734e-06, 'batch_size': 23, 'step_size': 4, 'gamma': 0.9839632292423245}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 13:53:54,452][0m Trial 15 finished with value: 0.390039856895546 and parameters: {'observation_period_num': 193, 'train_rates': 0.9377208398792213, 'learning_rate': 1.2054434227077791e-06, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9881165060731489}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 13:58:36,056][0m Trial 16 finished with value: 0.3690622626370146 and parameters: {'observation_period_num': 183, 'train_rates': 0.952806468980222, 'learning_rate': 4.437505740042917e-06, 'batch_size': 43, 'step_size': 5, 'gamma': 0.8620000533056973}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:00:50,782][0m Trial 17 finished with value: 0.9798220825749774 and parameters: {'observation_period_num': 106, 'train_rates': 0.7890609371715208, 'learning_rate': 9.701848121538627e-06, 'batch_size': 55, 'step_size': 3, 'gamma': 0.7560083587250381}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:04:35,979][0m Trial 18 finished with value: 0.48976050712194547 and parameters: {'observation_period_num': 155, 'train_rates': 0.8990734833763662, 'learning_rate': 2.3242660437774578e-06, 'batch_size': 40, 'step_size': 6, 'gamma': 0.9265633360438184}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:06:22,477][0m Trial 19 finished with value: 0.2204878181219101 and parameters: {'observation_period_num': 72, 'train_rates': 0.9897640509837341, 'learning_rate': 1.1505818421226319e-05, 'batch_size': 67, 'step_size': 2, 'gamma': 0.9689077816723134}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:11:22,930][0m Trial 20 finished with value: 0.5752555683046996 and parameters: {'observation_period_num': 210, 'train_rates': 0.8178381689763068, 'learning_rate': 2.286425288139933e-06, 'batch_size': 31, 'step_size': 8, 'gamma': 0.8994635600572067}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:17:36,295][0m Trial 21 finished with value: 0.22187371898401267 and parameters: {'observation_period_num': 249, 'train_rates': 0.8998921718289434, 'learning_rate': 6.049737900207639e-05, 'batch_size': 181, 'step_size': 4, 'gamma': 0.9618674468149746}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:23:04,141][0m Trial 22 finished with value: 0.35261066760986476 and parameters: {'observation_period_num': 217, 'train_rates': 0.9427686064615628, 'learning_rate': 6.664196904253198e-06, 'batch_size': 153, 'step_size': 5, 'gamma': 0.9856330929062376}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:27:02,820][0m Trial 23 finished with value: 0.17536975561783014 and parameters: {'observation_period_num': 172, 'train_rates': 0.8918054955305272, 'learning_rate': 0.00032525817578354395, 'batch_size': 162, 'step_size': 3, 'gamma': 0.938592415270925}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:30:02,830][0m Trial 24 finished with value: 0.234929315967574 and parameters: {'observation_period_num': 134, 'train_rates': 0.8809710264066644, 'learning_rate': 0.00033888927394428635, 'batch_size': 209, 'step_size': 2, 'gamma': 0.9386151067603048}. Best is trial 11 with value: 0.13903269534440418.[0m
[32m[I 2025-01-06 14:33:57,608][0m Trial 25 finished with value: 0.13439314564069113 and parameters: {'observation_period_num': 161, 'train_rates': 0.9616676339147888, 'learning_rate': 4.606188321565417e-05, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9743276851464012}. Best is trial 25 with value: 0.13439314564069113.[0m
[32m[I 2025-01-06 14:38:07,344][0m Trial 26 finished with value: 0.14211556315422058 and parameters: {'observation_period_num': 170, 'train_rates': 0.96743660208958, 'learning_rate': 5.84827096975812e-05, 'batch_size': 98, 'step_size': 6, 'gamma': 0.9111727038144325}. Best is trial 25 with value: 0.13439314564069113.[0m
[32m[I 2025-01-06 14:40:39,913][0m Trial 27 finished with value: 0.13478459409632526 and parameters: {'observation_period_num': 110, 'train_rates': 0.9681206781970643, 'learning_rate': 5.07850472376202e-05, 'batch_size': 90, 'step_size': 7, 'gamma': 0.8579084583624048}. Best is trial 25 with value: 0.13439314564069113.[0m
[32m[I 2025-01-06 14:42:52,291][0m Trial 28 finished with value: 0.1338471541595128 and parameters: {'observation_period_num': 94, 'train_rates': 0.9500163456733245, 'learning_rate': 4.4961765659457865e-05, 'batch_size': 77, 'step_size': 9, 'gamma': 0.8455478591795214}. Best is trial 28 with value: 0.1338471541595128.[0m
[32m[I 2025-01-06 14:46:05,775][0m Trial 29 finished with value: 0.1418820301953115 and parameters: {'observation_period_num': 133, 'train_rates': 0.959628009783648, 'learning_rate': 4.287254345651594e-05, 'batch_size': 87, 'step_size': 9, 'gamma': 0.8564794334549264}. Best is trial 28 with value: 0.1338471541595128.[0m
[32m[I 2025-01-06 14:47:05,521][0m Trial 30 finished with value: 0.8704107026510601 and parameters: {'observation_period_num': 53, 'train_rates': 0.72815264567479, 'learning_rate': 1.8634784624950947e-05, 'batch_size': 121, 'step_size': 8, 'gamma': 0.8444198108953094}. Best is trial 28 with value: 0.1338471541595128.[0m
[32m[I 2025-01-06 14:48:55,439][0m Trial 31 finished with value: 0.8210865550001171 and parameters: {'observation_period_num': 98, 'train_rates': 0.6665872778247564, 'learning_rate': 8.053965835956956e-05, 'batch_size': 77, 'step_size': 7, 'gamma': 0.8741868069835409}. Best is trial 28 with value: 0.1338471541595128.[0m
[32m[I 2025-01-06 14:51:33,437][0m Trial 32 finished with value: 0.22193416401191993 and parameters: {'observation_period_num': 117, 'train_rates': 0.9430436020997657, 'learning_rate': 1.8822921045558046e-05, 'batch_size': 103, 'step_size': 11, 'gamma': 0.8205746939136757}. Best is trial 28 with value: 0.1338471541595128.[0m
[32m[I 2025-01-06 14:53:52,046][0m Trial 33 finished with value: 0.12133331424914874 and parameters: {'observation_period_num': 94, 'train_rates': 0.9638586612909038, 'learning_rate': 4.268957399630868e-05, 'batch_size': 51, 'step_size': 9, 'gamma': 0.8871175880126365}. Best is trial 33 with value: 0.12133331424914874.[0m
[32m[I 2025-01-06 14:55:14,474][0m Trial 34 finished with value: 0.11077628324429194 and parameters: {'observation_period_num': 54, 'train_rates': 0.9742945054605101, 'learning_rate': 3.997972907658802e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8416927486197577}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 14:56:30,047][0m Trial 35 finished with value: 0.13312320863898797 and parameters: {'observation_period_num': 16, 'train_rates': 0.9197252650695152, 'learning_rate': 3.036015658062178e-05, 'batch_size': 52, 'step_size': 10, 'gamma': 0.8347779247489837}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 14:57:41,296][0m Trial 36 finished with value: 0.1341357879778918 and parameters: {'observation_period_num': 5, 'train_rates': 0.9194639765276136, 'learning_rate': 2.7835580719441827e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.8392832056096479}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 14:58:50,150][0m Trial 37 finished with value: 0.2413842702972824 and parameters: {'observation_period_num': 44, 'train_rates': 0.8739029731475089, 'learning_rate': 0.00011619104225186723, 'batch_size': 59, 'step_size': 12, 'gamma': 0.8870898405192268}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 15:00:31,056][0m Trial 38 finished with value: 0.15546839702201176 and parameters: {'observation_period_num': 27, 'train_rates': 0.9151035465080587, 'learning_rate': 1.9616654946849785e-05, 'batch_size': 39, 'step_size': 9, 'gamma': 0.8189941586766806}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 15:02:15,890][0m Trial 39 finished with value: 0.3350902450084686 and parameters: {'observation_period_num': 82, 'train_rates': 0.8262046184018152, 'learning_rate': 3.4437909640334684e-05, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8313806475472751}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 15:03:29,125][0m Trial 40 finished with value: 0.21863005566359095 and parameters: {'observation_period_num': 11, 'train_rates': 0.8469517080770848, 'learning_rate': 9.359364551855263e-05, 'batch_size': 51, 'step_size': 11, 'gamma': 0.802236710048808}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 15:04:52,445][0m Trial 41 finished with value: 0.1646459637905046 and parameters: {'observation_period_num': 55, 'train_rates': 0.9253832285389934, 'learning_rate': 2.6118713664457137e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8461154414067005}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 15:05:52,041][0m Trial 42 finished with value: 0.2184871346653286 and parameters: {'observation_period_num': 15, 'train_rates': 0.8701001017588454, 'learning_rate': 0.00017181480182417797, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8371694079629916}. Best is trial 34 with value: 0.11077628324429194.[0m
[32m[I 2025-01-06 15:07:49,063][0m Trial 43 finished with value: 0.0848508073311103 and parameters: {'observation_period_num': 37, 'train_rates': 0.9739781932196933, 'learning_rate': 7.152603251777385e-05, 'batch_size': 36, 'step_size': 12, 'gamma': 0.8076694850732252}. Best is trial 43 with value: 0.0848508073311103.[0m
[32m[I 2025-01-06 15:09:33,028][0m Trial 44 finished with value: 0.08584993753744208 and parameters: {'observation_period_num': 36, 'train_rates': 0.9763981397701216, 'learning_rate': 7.036890577633644e-05, 'batch_size': 42, 'step_size': 13, 'gamma': 0.7729933716863692}. Best is trial 43 with value: 0.0848508073311103.[0m
[32m[I 2025-01-06 15:11:42,789][0m Trial 45 finished with value: 0.08386370177629968 and parameters: {'observation_period_num': 36, 'train_rates': 0.9759370904787381, 'learning_rate': 7.596562709360802e-05, 'batch_size': 33, 'step_size': 13, 'gamma': 0.7702097114089388}. Best is trial 45 with value: 0.08386370177629968.[0m
[32m[I 2025-01-06 15:13:48,359][0m Trial 46 finished with value: 0.12770551483373385 and parameters: {'observation_period_num': 38, 'train_rates': 0.9749446094352263, 'learning_rate': 0.00017796833615700813, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7615300934941667}. Best is trial 45 with value: 0.08386370177629968.[0m
[32m[I 2025-01-06 15:16:14,991][0m Trial 47 finished with value: 0.08998120922980637 and parameters: {'observation_period_num': 61, 'train_rates': 0.9803019793400659, 'learning_rate': 7.818410575010802e-05, 'batch_size': 30, 'step_size': 12, 'gamma': 0.7795286610653547}. Best is trial 45 with value: 0.08386370177629968.[0m
[32m[I 2025-01-06 15:18:53,562][0m Trial 48 finished with value: 0.1096591746265238 and parameters: {'observation_period_num': 60, 'train_rates': 0.988946767978167, 'learning_rate': 7.725927713725191e-05, 'batch_size': 28, 'step_size': 13, 'gamma': 0.7749929923471792}. Best is trial 45 with value: 0.08386370177629968.[0m
[32m[I 2025-01-06 15:21:31,710][0m Trial 49 finished with value: 0.15193775296211243 and parameters: {'observation_period_num': 66, 'train_rates': 0.9890528833108063, 'learning_rate': 0.00028466996478109516, 'batch_size': 28, 'step_size': 13, 'gamma': 0.7694973959657755}. Best is trial 45 with value: 0.08386370177629968.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer(nomstl).json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.6518 | 0.5814
Epoch 2/300, Loss: 0.5550 | 0.5615
Epoch 3/300, Loss: 0.4137 | 0.4460
Epoch 4/300, Loss: 0.3528 | 0.4541
Epoch 5/300, Loss: 0.3008 | 0.4115
Epoch 6/300, Loss: 0.2700 | 0.3512
Epoch 7/300, Loss: 0.2546 | 0.4060
Epoch 8/300, Loss: 0.2610 | 0.2875
Epoch 9/300, Loss: 0.2380 | 0.2659
Epoch 10/300, Loss: 0.2321 | 0.2674
Epoch 11/300, Loss: 0.2113 | 0.2742
Epoch 12/300, Loss: 0.2104 | 0.2239
Epoch 13/300, Loss: 0.2022 | 0.2207
Epoch 14/300, Loss: 0.2233 | 0.2860
Epoch 15/300, Loss: 0.1947 | 0.2052
Epoch 16/300, Loss: 0.1790 | 0.1916
Epoch 17/300, Loss: 0.1869 | 0.2378
Epoch 18/300, Loss: 0.1720 | 0.1867
Epoch 19/300, Loss: 0.1665 | 0.2007
Epoch 20/300, Loss: 0.1783 | 0.2394
Epoch 21/300, Loss: 0.1693 | 0.1762
Epoch 22/300, Loss: 0.1655 | 0.1823
Epoch 23/300, Loss: 0.1610 | 0.1732
Epoch 24/300, Loss: 0.1610 | 0.1877
Epoch 25/300, Loss: 0.1536 | 0.1697
Epoch 26/300, Loss: 0.1511 | 0.1623
Epoch 27/300, Loss: 0.1544 | 0.1597
Epoch 28/300, Loss: 0.1461 | 0.1453
Epoch 29/300, Loss: 0.1423 | 0.1469
Epoch 30/300, Loss: 0.1409 | 0.1557
Epoch 31/300, Loss: 0.1386 | 0.1486
Epoch 32/300, Loss: 0.1393 | 0.1498
Epoch 33/300, Loss: 0.1376 | 0.1414
Epoch 34/300, Loss: 0.1417 | 0.1876
Epoch 35/300, Loss: 0.1391 | 0.1392
Epoch 36/300, Loss: 0.1359 | 0.1317
Epoch 37/300, Loss: 0.1331 | 0.1261
Epoch 38/300, Loss: 0.1321 | 0.1441
Epoch 39/300, Loss: 0.1302 | 0.1282
Epoch 40/300, Loss: 0.1276 | 0.1200
Epoch 41/300, Loss: 0.1257 | 0.1203
Epoch 42/300, Loss: 0.1250 | 0.1239
Epoch 43/300, Loss: 0.1224 | 0.1217
Epoch 44/300, Loss: 0.1217 | 0.1145
Epoch 45/300, Loss: 0.1205 | 0.1230
Epoch 46/300, Loss: 0.1213 | 0.1192
Epoch 47/300, Loss: 0.1201 | 0.1213
Epoch 48/300, Loss: 0.1195 | 0.1102
Epoch 49/300, Loss: 0.1185 | 0.1157
Epoch 50/300, Loss: 0.1187 | 0.1127
Epoch 51/300, Loss: 0.1172 | 0.1173
Epoch 52/300, Loss: 0.1160 | 0.1081
Epoch 53/300, Loss: 0.1163 | 0.1134
Epoch 54/300, Loss: 0.1152 | 0.1080
Epoch 55/300, Loss: 0.1142 | 0.1078
Epoch 56/300, Loss: 0.1135 | 0.1055
Epoch 57/300, Loss: 0.1121 | 0.1065
Epoch 58/300, Loss: 0.1117 | 0.1054
Epoch 59/300, Loss: 0.1120 | 0.1070
Epoch 60/300, Loss: 0.1111 | 0.1036
Epoch 61/300, Loss: 0.1105 | 0.1032
Epoch 62/300, Loss: 0.1103 | 0.1026
Epoch 63/300, Loss: 0.1087 | 0.1025
Epoch 64/300, Loss: 0.1089 | 0.1020
Epoch 65/300, Loss: 0.1083 | 0.1006
Epoch 66/300, Loss: 0.1080 | 0.1010
Epoch 67/300, Loss: 0.1075 | 0.1004
Epoch 68/300, Loss: 0.1070 | 0.0991
Epoch 69/300, Loss: 0.1075 | 0.0991
Epoch 70/300, Loss: 0.1075 | 0.0993
Epoch 71/300, Loss: 0.1067 | 0.0991
Epoch 72/300, Loss: 0.1063 | 0.0983
Epoch 73/300, Loss: 0.1061 | 0.0976
Epoch 74/300, Loss: 0.1054 | 0.0989
Epoch 75/300, Loss: 0.1051 | 0.0984
Epoch 76/300, Loss: 0.1041 | 0.0971
Epoch 77/300, Loss: 0.1056 | 0.0977
Epoch 78/300, Loss: 0.1052 | 0.0982
Epoch 79/300, Loss: 0.1049 | 0.0964
Epoch 80/300, Loss: 0.1038 | 0.0967
Epoch 81/300, Loss: 0.1046 | 0.0962
Epoch 82/300, Loss: 0.1040 | 0.0964
Epoch 83/300, Loss: 0.1044 | 0.0961
Epoch 84/300, Loss: 0.1032 | 0.0955
Epoch 85/300, Loss: 0.1046 | 0.0957
Epoch 86/300, Loss: 0.1030 | 0.0955
Epoch 87/300, Loss: 0.1031 | 0.0952
Epoch 88/300, Loss: 0.1027 | 0.0951
Epoch 89/300, Loss: 0.1029 | 0.0949
Epoch 90/300, Loss: 0.1026 | 0.0948
Epoch 91/300, Loss: 0.1026 | 0.0949
Epoch 92/300, Loss: 0.1034 | 0.0947
Epoch 93/300, Loss: 0.1028 | 0.0948
Epoch 94/300, Loss: 0.1024 | 0.0950
Epoch 95/300, Loss: 0.1019 | 0.0948
Epoch 96/300, Loss: 0.1027 | 0.0942
Epoch 97/300, Loss: 0.1014 | 0.0941
Epoch 98/300, Loss: 0.1017 | 0.0940
Epoch 99/300, Loss: 0.1017 | 0.0942
Epoch 100/300, Loss: 0.1017 | 0.0939
Epoch 101/300, Loss: 0.1019 | 0.0940
Epoch 102/300, Loss: 0.1020 | 0.0940
Epoch 103/300, Loss: 0.1015 | 0.0935
Epoch 104/300, Loss: 0.1015 | 0.0935
Epoch 105/300, Loss: 0.1019 | 0.0936
Epoch 106/300, Loss: 0.1022 | 0.0937
Epoch 107/300, Loss: 0.1009 | 0.0938
Epoch 108/300, Loss: 0.1007 | 0.0935
Epoch 109/300, Loss: 0.1015 | 0.0934
Epoch 110/300, Loss: 0.1006 | 0.0932
Epoch 111/300, Loss: 0.1010 | 0.0930
Epoch 112/300, Loss: 0.1015 | 0.0932
Epoch 113/300, Loss: 0.1010 | 0.0930
Epoch 114/300, Loss: 0.1011 | 0.0932
Epoch 115/300, Loss: 0.1016 | 0.0931
Epoch 116/300, Loss: 0.1010 | 0.0930
Epoch 117/300, Loss: 0.1007 | 0.0928
Epoch 118/300, Loss: 0.1013 | 0.0927
Epoch 119/300, Loss: 0.1010 | 0.0929
Epoch 120/300, Loss: 0.1008 | 0.0929
Epoch 121/300, Loss: 0.1003 | 0.0927
Epoch 122/300, Loss: 0.1004 | 0.0927
Epoch 123/300, Loss: 0.1004 | 0.0927
Epoch 124/300, Loss: 0.1013 | 0.0929
Epoch 125/300, Loss: 0.1004 | 0.0929
Epoch 126/300, Loss: 0.1004 | 0.0929
Epoch 127/300, Loss: 0.1002 | 0.0927
Epoch 128/300, Loss: 0.1003 | 0.0926
Epoch 129/300, Loss: 0.1010 | 0.0927
Epoch 130/300, Loss: 0.1004 | 0.0927
Epoch 131/300, Loss: 0.1003 | 0.0926
Epoch 132/300, Loss: 0.1006 | 0.0927
Epoch 133/300, Loss: 0.1001 | 0.0926
Epoch 134/300, Loss: 0.1005 | 0.0926
Epoch 135/300, Loss: 0.1004 | 0.0925
Epoch 136/300, Loss: 0.1006 | 0.0925
Epoch 137/300, Loss: 0.1002 | 0.0925
Epoch 138/300, Loss: 0.1003 | 0.0925
Epoch 139/300, Loss: 0.1003 | 0.0924
Epoch 140/300, Loss: 0.0999 | 0.0924
Epoch 141/300, Loss: 0.1005 | 0.0924
Epoch 142/300, Loss: 0.0998 | 0.0924
Epoch 143/300, Loss: 0.1001 | 0.0924
Epoch 144/300, Loss: 0.1008 | 0.0923
Epoch 145/300, Loss: 0.0998 | 0.0924
Epoch 146/300, Loss: 0.0999 | 0.0925
Epoch 147/300, Loss: 0.1004 | 0.0924
Epoch 148/300, Loss: 0.1003 | 0.0923
Epoch 149/300, Loss: 0.1002 | 0.0923
Epoch 150/300, Loss: 0.1000 | 0.0923
Epoch 151/300, Loss: 0.1002 | 0.0923
Epoch 152/300, Loss: 0.1002 | 0.0923
Epoch 153/300, Loss: 0.1003 | 0.0923
Epoch 154/300, Loss: 0.1004 | 0.0922
Epoch 155/300, Loss: 0.1004 | 0.0922
Epoch 156/300, Loss: 0.1003 | 0.0921
Epoch 157/300, Loss: 0.1002 | 0.0921
Epoch 158/300, Loss: 0.1005 | 0.0921
Epoch 159/300, Loss: 0.1000 | 0.0921
Epoch 160/300, Loss: 0.1002 | 0.0921
Epoch 161/300, Loss: 0.1001 | 0.0921
Epoch 162/300, Loss: 0.1001 | 0.0921
Epoch 163/300, Loss: 0.1001 | 0.0922
Epoch 164/300, Loss: 0.1000 | 0.0922
Epoch 165/300, Loss: 0.1000 | 0.0922
Epoch 166/300, Loss: 0.1000 | 0.0922
Epoch 167/300, Loss: 0.0999 | 0.0922
Epoch 168/300, Loss: 0.0997 | 0.0922
Epoch 169/300, Loss: 0.1000 | 0.0922
Epoch 170/300, Loss: 0.1001 | 0.0922
Epoch 171/300, Loss: 0.0999 | 0.0921
Epoch 172/300, Loss: 0.1001 | 0.0922
Epoch 173/300, Loss: 0.0993 | 0.0921
Epoch 174/300, Loss: 0.0997 | 0.0921
Epoch 175/300, Loss: 0.0999 | 0.0921
Epoch 176/300, Loss: 0.0999 | 0.0921
Epoch 177/300, Loss: 0.1001 | 0.0921
Epoch 178/300, Loss: 0.1003 | 0.0921
Epoch 179/300, Loss: 0.1001 | 0.0921
Epoch 180/300, Loss: 0.0997 | 0.0921
Epoch 181/300, Loss: 0.1000 | 0.0921
Epoch 182/300, Loss: 0.1001 | 0.0921
Epoch 183/300, Loss: 0.0997 | 0.0921
Epoch 184/300, Loss: 0.1002 | 0.0921
Epoch 185/300, Loss: 0.1001 | 0.0920
Epoch 186/300, Loss: 0.0997 | 0.0920
Epoch 187/300, Loss: 0.0991 | 0.0920
Epoch 188/300, Loss: 0.1000 | 0.0920
Epoch 189/300, Loss: 0.0994 | 0.0920
Epoch 190/300, Loss: 0.1006 | 0.0921
Epoch 191/300, Loss: 0.1005 | 0.0921
Epoch 192/300, Loss: 0.0999 | 0.0921
Epoch 193/300, Loss: 0.0996 | 0.0921
Epoch 194/300, Loss: 0.0997 | 0.0921
Epoch 195/300, Loss: 0.1001 | 0.0921
Epoch 196/300, Loss: 0.1004 | 0.0921
Epoch 197/300, Loss: 0.0995 | 0.0921
Epoch 198/300, Loss: 0.0997 | 0.0921
Epoch 199/300, Loss: 0.0998 | 0.0921
Epoch 200/300, Loss: 0.0995 | 0.0921
Epoch 201/300, Loss: 0.0997 | 0.0921
Epoch 202/300, Loss: 0.0999 | 0.0921
Epoch 203/300, Loss: 0.1002 | 0.0921
Epoch 204/300, Loss: 0.1002 | 0.0921
Epoch 205/300, Loss: 0.1000 | 0.0921
Epoch 206/300, Loss: 0.0997 | 0.0921
Epoch 207/300, Loss: 0.0999 | 0.0921
Epoch 208/300, Loss: 0.1003 | 0.0920
Epoch 209/300, Loss: 0.0997 | 0.0920
Epoch 210/300, Loss: 0.0996 | 0.0920
Epoch 211/300, Loss: 0.0990 | 0.0920
Epoch 212/300, Loss: 0.1002 | 0.0920
Epoch 213/300, Loss: 0.0995 | 0.0920
Epoch 214/300, Loss: 0.0999 | 0.0920
Epoch 215/300, Loss: 0.1004 | 0.0920
Epoch 216/300, Loss: 0.1001 | 0.0920
Epoch 217/300, Loss: 0.1002 | 0.0920
Epoch 218/300, Loss: 0.0989 | 0.0920
Epoch 219/300, Loss: 0.1000 | 0.0920
Epoch 220/300, Loss: 0.0997 | 0.0920
Epoch 221/300, Loss: 0.0998 | 0.0920
Epoch 222/300, Loss: 0.1003 | 0.0920
Epoch 223/300, Loss: 0.0996 | 0.0920
Epoch 224/300, Loss: 0.0999 | 0.0920
Epoch 225/300, Loss: 0.1007 | 0.0920
Epoch 226/300, Loss: 0.1003 | 0.0920
Epoch 227/300, Loss: 0.1002 | 0.0920
Epoch 228/300, Loss: 0.0999 | 0.0920
Epoch 229/300, Loss: 0.1001 | 0.0920
Epoch 230/300, Loss: 0.0997 | 0.0920
Epoch 231/300, Loss: 0.0994 | 0.0920
Epoch 232/300, Loss: 0.1002 | 0.0920
Epoch 233/300, Loss: 0.1000 | 0.0920
Epoch 234/300, Loss: 0.1005 | 0.0920
Epoch 235/300, Loss: 0.0997 | 0.0920
Epoch 236/300, Loss: 0.1002 | 0.0920
Epoch 237/300, Loss: 0.0999 | 0.0920
Epoch 238/300, Loss: 0.0995 | 0.0920
Epoch 239/300, Loss: 0.1004 | 0.0920
Epoch 240/300, Loss: 0.1000 | 0.0920
Epoch 241/300, Loss: 0.0999 | 0.0920
Epoch 242/300, Loss: 0.0999 | 0.0920
Epoch 243/300, Loss: 0.1002 | 0.0920
Epoch 244/300, Loss: 0.0991 | 0.0920
Epoch 245/300, Loss: 0.1000 | 0.0920
Epoch 246/300, Loss: 0.0999 | 0.0920
Epoch 247/300, Loss: 0.0997 | 0.0920
Epoch 248/300, Loss: 0.0990 | 0.0920
Epoch 249/300, Loss: 0.0998 | 0.0920
Epoch 250/300, Loss: 0.0995 | 0.0920
Epoch 251/300, Loss: 0.0998 | 0.0920
Epoch 252/300, Loss: 0.0998 | 0.0920
Epoch 253/300, Loss: 0.1005 | 0.0920
Epoch 254/300, Loss: 0.0990 | 0.0920
Epoch 255/300, Loss: 0.1004 | 0.0920
Epoch 256/300, Loss: 0.0997 | 0.0920
Epoch 257/300, Loss: 0.0998 | 0.0920
Epoch 258/300, Loss: 0.0999 | 0.0920
Epoch 259/300, Loss: 0.0995 | 0.0920
Epoch 260/300, Loss: 0.0993 | 0.0920
Epoch 261/300, Loss: 0.1007 | 0.0920
Epoch 262/300, Loss: 0.0994 | 0.0920
Epoch 263/300, Loss: 0.0996 | 0.0920
Epoch 264/300, Loss: 0.0998 | 0.0920
Epoch 265/300, Loss: 0.1000 | 0.0920
Epoch 266/300, Loss: 0.0995 | 0.0920
Epoch 267/300, Loss: 0.0997 | 0.0920
Epoch 268/300, Loss: 0.0997 | 0.0920
Epoch 269/300, Loss: 0.0998 | 0.0920
Epoch 270/300, Loss: 0.1000 | 0.0920
Epoch 271/300, Loss: 0.1001 | 0.0920
Epoch 272/300, Loss: 0.1001 | 0.0920
Epoch 273/300, Loss: 0.1006 | 0.0920
Epoch 274/300, Loss: 0.0998 | 0.0920
Epoch 275/300, Loss: 0.1003 | 0.0920
Epoch 276/300, Loss: 0.1002 | 0.0920
Epoch 277/300, Loss: 0.0998 | 0.0920
Epoch 278/300, Loss: 0.0998 | 0.0920
Epoch 279/300, Loss: 0.0993 | 0.0920
Epoch 280/300, Loss: 0.0999 | 0.0920
Epoch 281/300, Loss: 0.0994 | 0.0920
Epoch 282/300, Loss: 0.0995 | 0.0920
Epoch 283/300, Loss: 0.0995 | 0.0920
Epoch 284/300, Loss: 0.1002 | 0.0920
Epoch 285/300, Loss: 0.0994 | 0.0920
Epoch 286/300, Loss: 0.0998 | 0.0920
Epoch 287/300, Loss: 0.0996 | 0.0920
Epoch 288/300, Loss: 0.0998 | 0.0920
Epoch 289/300, Loss: 0.1005 | 0.0920
Epoch 290/300, Loss: 0.0999 | 0.0920
Epoch 291/300, Loss: 0.0999 | 0.0920
Epoch 292/300, Loss: 0.1003 | 0.0920
Epoch 293/300, Loss: 0.0997 | 0.0920
Epoch 294/300, Loss: 0.1001 | 0.0920
Epoch 295/300, Loss: 0.0999 | 0.0920
Epoch 296/300, Loss: 0.0998 | 0.0920
Epoch 297/300, Loss: 0.0993 | 0.0920
Epoch 298/300, Loss: 0.1003 | 0.0920
Epoch 299/300, Loss: 0.0997 | 0.0920
Epoch 300/300, Loss: 0.1006 | 0.0920
Runtime (seconds): 406.4833402633667
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 217.033464089036
RMSE: 14.7320556640625
MAE: 14.7320556640625
R-squared: nan
[198.70795]
