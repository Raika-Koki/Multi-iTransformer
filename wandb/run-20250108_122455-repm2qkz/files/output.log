[32m[I 2025-01-08 12:24:59,117][0m A new study created in memory with name: no-name-b11aedde-9365-4766-94e1-55f2f377faab[0m
[32m[I 2025-01-08 12:28:59,454][0m Trial 0 finished with value: 0.7539545512455871 and parameters: {'observation_period_num': 200, 'train_rates': 0.6692112670313574, 'learning_rate': 0.00021803876942707196, 'batch_size': 133, 'step_size': 7, 'gamma': 0.902324030505982}. Best is trial 0 with value: 0.7539545512455871.[0m
[32m[I 2025-01-08 12:33:43,452][0m Trial 1 finished with value: 2.3073941174373833 and parameters: {'observation_period_num': 183, 'train_rates': 0.8002196791660794, 'learning_rate': 0.0009405568711662374, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9063149084921543}. Best is trial 0 with value: 0.7539545512455871.[0m
[32m[I 2025-01-08 12:34:14,546][0m Trial 2 finished with value: 0.4073407562115254 and parameters: {'observation_period_num': 12, 'train_rates': 0.8949965403681776, 'learning_rate': 1.7609499425856946e-05, 'batch_size': 158, 'step_size': 4, 'gamma': 0.8552448917292004}. Best is trial 2 with value: 0.4073407562115254.[0m
[32m[I 2025-01-08 12:38:25,906][0m Trial 3 finished with value: 1.0013524761071075 and parameters: {'observation_period_num': 168, 'train_rates': 0.9474562602903607, 'learning_rate': 1.9398092814593863e-06, 'batch_size': 69, 'step_size': 8, 'gamma': 0.8043945259840352}. Best is trial 2 with value: 0.4073407562115254.[0m
[32m[I 2025-01-08 12:42:05,647][0m Trial 4 finished with value: 1.6201333865549528 and parameters: {'observation_period_num': 169, 'train_rates': 0.797333653246387, 'learning_rate': 2.146893581149832e-06, 'batch_size': 128, 'step_size': 5, 'gamma': 0.7879734531744478}. Best is trial 2 with value: 0.4073407562115254.[0m
[32m[I 2025-01-08 12:46:19,763][0m Trial 5 finished with value: 0.24926847219467163 and parameters: {'observation_period_num': 169, 'train_rates': 0.979915825362393, 'learning_rate': 4.9396391979875946e-05, 'batch_size': 92, 'step_size': 3, 'gamma': 0.8546091487526629}. Best is trial 5 with value: 0.24926847219467163.[0m
[32m[I 2025-01-08 12:47:58,204][0m Trial 6 finished with value: 0.3937072512469714 and parameters: {'observation_period_num': 10, 'train_rates': 0.8935993105033049, 'learning_rate': 4.934603359007297e-06, 'batch_size': 46, 'step_size': 10, 'gamma': 0.7728390400991462}. Best is trial 5 with value: 0.24926847219467163.[0m
[32m[I 2025-01-08 12:50:18,869][0m Trial 7 finished with value: 0.39891336765140295 and parameters: {'observation_period_num': 108, 'train_rates': 0.9108838760800834, 'learning_rate': 7.499698903451544e-06, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9689269124656148}. Best is trial 5 with value: 0.24926847219467163.[0m
[32m[I 2025-01-08 12:56:32,523][0m Trial 8 finished with value: 0.1966453202989664 and parameters: {'observation_period_num': 246, 'train_rates': 0.9021892907844272, 'learning_rate': 0.00025944576830607476, 'batch_size': 231, 'step_size': 7, 'gamma': 0.785890809611205}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 12:59:04,601][0m Trial 9 finished with value: 0.8118326333125783 and parameters: {'observation_period_num': 113, 'train_rates': 0.9207499013709013, 'learning_rate': 7.147263428416567e-06, 'batch_size': 146, 'step_size': 2, 'gamma': 0.894142454413484}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:04:06,181][0m Trial 10 finished with value: 0.9435278394818306 and parameters: {'observation_period_num': 252, 'train_rates': 0.6327312894363596, 'learning_rate': 0.00011437080539135173, 'batch_size': 255, 'step_size': 11, 'gamma': 0.7504247660447273}. Best is trial 8 with value: 0.1966453202989664.[0m
Early stopping at epoch 72
[32m[I 2025-01-08 13:09:05,283][0m Trial 11 finished with value: 0.6107021570205688 and parameters: {'observation_period_num': 252, 'train_rates': 0.9837144006013375, 'learning_rate': 8.923510408538671e-05, 'batch_size': 210, 'step_size': 1, 'gamma': 0.838701311995947}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:14:23,305][0m Trial 12 finished with value: 0.3802289308732366 and parameters: {'observation_period_num': 218, 'train_rates': 0.8527939607554134, 'learning_rate': 0.0005760510758858875, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8209324070032725}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:16:08,824][0m Trial 13 finished with value: 0.22648176550865173 and parameters: {'observation_period_num': 74, 'train_rates': 0.9886747382728427, 'learning_rate': 4.2889128913161886e-05, 'batch_size': 205, 'step_size': 3, 'gamma': 0.9719502888804552}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:17:24,568][0m Trial 14 finished with value: 0.33782227834065753 and parameters: {'observation_period_num': 64, 'train_rates': 0.8404771706534665, 'learning_rate': 0.0003176984243804909, 'batch_size': 200, 'step_size': 7, 'gamma': 0.9634219446788893}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:18:46,907][0m Trial 15 finished with value: 0.6770012792795714 and parameters: {'observation_period_num': 71, 'train_rates': 0.7181127918773645, 'learning_rate': 2.5162836112208488e-05, 'batch_size': 214, 'step_size': 10, 'gamma': 0.9369239181551426}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:19:55,515][0m Trial 16 finished with value: 0.4908491174242202 and parameters: {'observation_period_num': 61, 'train_rates': 0.7527366770222119, 'learning_rate': 5.685230814092196e-05, 'batch_size': 187, 'step_size': 6, 'gamma': 0.9418202191101259}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:22:57,077][0m Trial 17 finished with value: 0.2544451734751387 and parameters: {'observation_period_num': 136, 'train_rates': 0.8623071609143853, 'learning_rate': 0.00015248597747179394, 'batch_size': 228, 'step_size': 1, 'gamma': 0.9874396334325066}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:24:52,044][0m Trial 18 finished with value: 0.2520781457424164 and parameters: {'observation_period_num': 85, 'train_rates': 0.9478756065334332, 'learning_rate': 0.0003562598354429137, 'batch_size': 172, 'step_size': 3, 'gamma': 0.8913817260491815}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:25:40,220][0m Trial 19 finished with value: 0.5333632826805115 and parameters: {'observation_period_num': 33, 'train_rates': 0.9531322888785962, 'learning_rate': 1.4188278831559216e-05, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8225598258246143}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:29:04,467][0m Trial 20 finished with value: 0.2001698613166809 and parameters: {'observation_period_num': 138, 'train_rates': 0.9872562799248961, 'learning_rate': 4.636512670805667e-05, 'batch_size': 183, 'step_size': 12, 'gamma': 0.7515042896326962}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:32:15,803][0m Trial 21 finished with value: 0.2327267825603485 and parameters: {'observation_period_num': 129, 'train_rates': 0.9889310671771029, 'learning_rate': 4.534194983992505e-05, 'batch_size': 181, 'step_size': 12, 'gamma': 0.7583651724562559}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:37:50,903][0m Trial 22 finished with value: 0.250792920589447 and parameters: {'observation_period_num': 220, 'train_rates': 0.9303116838835854, 'learning_rate': 8.95258846835623e-05, 'batch_size': 220, 'step_size': 8, 'gamma': 0.7886032413012981}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:38:41,617][0m Trial 23 finished with value: 0.29455017274425876 and parameters: {'observation_period_num': 38, 'train_rates': 0.884141224406234, 'learning_rate': 2.7422656437516116e-05, 'batch_size': 195, 'step_size': 14, 'gamma': 0.7818705332680904}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:42:15,053][0m Trial 24 finished with value: 0.5033782124519348 and parameters: {'observation_period_num': 147, 'train_rates': 0.958498475904533, 'learning_rate': 1.2372218854966262e-05, 'batch_size': 240, 'step_size': 12, 'gamma': 0.8093398392058373}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:44:12,144][0m Trial 25 finished with value: 0.36450713056686096 and parameters: {'observation_period_num': 96, 'train_rates': 0.8217854329445698, 'learning_rate': 0.00023627965632401008, 'batch_size': 167, 'step_size': 6, 'gamma': 0.7656288244424904}. Best is trial 8 with value: 0.1966453202989664.[0m
[32m[I 2025-01-08 13:50:15,346][0m Trial 26 finished with value: 0.16413764655590057 and parameters: {'observation_period_num': 228, 'train_rates': 0.9886326089383134, 'learning_rate': 6.303876770206525e-05, 'batch_size': 208, 'step_size': 9, 'gamma': 0.9274858204530991}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 13:55:02,375][0m Trial 27 finished with value: 0.5235971901811234 and parameters: {'observation_period_num': 220, 'train_rates': 0.7520559191607191, 'learning_rate': 0.0001595583021980068, 'batch_size': 152, 'step_size': 10, 'gamma': 0.9212022745755104}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:01:00,463][0m Trial 28 finished with value: 0.19864724781744333 and parameters: {'observation_period_num': 233, 'train_rates': 0.9285470698976421, 'learning_rate': 7.759624912123022e-05, 'batch_size': 115, 'step_size': 9, 'gamma': 0.8733142106170204}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:06:50,112][0m Trial 29 finished with value: 0.24124685585324016 and parameters: {'observation_period_num': 234, 'train_rates': 0.8757098162312812, 'learning_rate': 0.0004706898983088306, 'batch_size': 125, 'step_size': 7, 'gamma': 0.8756618634274487}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:11:39,960][0m Trial 30 finished with value: 0.17631837052683677 and parameters: {'observation_period_num': 194, 'train_rates': 0.9221010229091406, 'learning_rate': 0.00020192311800472135, 'batch_size': 105, 'step_size': 9, 'gamma': 0.8655530727865128}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:16:39,925][0m Trial 31 finished with value: 0.18308052966292476 and parameters: {'observation_period_num': 198, 'train_rates': 0.9233482338789188, 'learning_rate': 0.0002086805021141228, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8728960551632393}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:21:37,291][0m Trial 32 finished with value: 0.16796037256717683 and parameters: {'observation_period_num': 201, 'train_rates': 0.9064464853403463, 'learning_rate': 0.00019917278313277853, 'batch_size': 104, 'step_size': 8, 'gamma': 0.9083865448371655}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:26:09,312][0m Trial 33 finished with value: 0.917001553835013 and parameters: {'observation_period_num': 196, 'train_rates': 0.8317867022965502, 'learning_rate': 0.0009452739802213377, 'batch_size': 106, 'step_size': 9, 'gamma': 0.9120396047920192}. Best is trial 26 with value: 0.16413764655590057.[0m
[32m[I 2025-01-08 14:30:59,429][0m Trial 34 finished with value: 0.14347759012235414 and parameters: {'observation_period_num': 191, 'train_rates': 0.9609856244313472, 'learning_rate': 0.0001861014826460928, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8561108937921063}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 14:35:42,201][0m Trial 35 finished with value: 0.33622403598967054 and parameters: {'observation_period_num': 184, 'train_rates': 0.9624500304481266, 'learning_rate': 0.0005004591435997249, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8527244259915533}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 14:40:26,270][0m Trial 36 finished with value: 0.17921901951756394 and parameters: {'observation_period_num': 182, 'train_rates': 0.9388828694221553, 'learning_rate': 0.00014496338567900516, 'batch_size': 31, 'step_size': 11, 'gamma': 0.8880176392572645}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 14:44:08,125][0m Trial 37 finished with value: 0.19623792055197534 and parameters: {'observation_period_num': 159, 'train_rates': 0.8773951440047306, 'learning_rate': 7.021202075669199e-05, 'batch_size': 79, 'step_size': 6, 'gamma': 0.9280219805745579}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 14:49:37,501][0m Trial 38 finished with value: 0.17708902100199148 and parameters: {'observation_period_num': 210, 'train_rates': 0.9590121883401197, 'learning_rate': 0.00011919371027531127, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9102457826948123}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 14:54:09,229][0m Trial 39 finished with value: 1.1448105361425516 and parameters: {'observation_period_num': 186, 'train_rates': 0.9054477339427969, 'learning_rate': 1.0759755687813083e-06, 'batch_size': 97, 'step_size': 8, 'gamma': 0.8430217610178685}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 14:58:07,420][0m Trial 40 finished with value: 0.5309947729110718 and parameters: {'observation_period_num': 161, 'train_rates': 0.9663967779315766, 'learning_rate': 0.0006542067813970281, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9496580095576401}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 15:03:32,312][0m Trial 41 finished with value: 0.14819516214701506 and parameters: {'observation_period_num': 207, 'train_rates': 0.9599737234989164, 'learning_rate': 0.00011489780575082661, 'batch_size': 58, 'step_size': 11, 'gamma': 0.9069217677073309}. Best is trial 34 with value: 0.14347759012235414.[0m
[32m[I 2025-01-08 15:09:04,602][0m Trial 42 finished with value: 0.12178144785689145 and parameters: {'observation_period_num': 207, 'train_rates': 0.9705531481432578, 'learning_rate': 0.0003207222775921938, 'batch_size': 46, 'step_size': 10, 'gamma': 0.902760848376891}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:15:54,593][0m Trial 43 finished with value: 0.14057749423843163 and parameters: {'observation_period_num': 235, 'train_rates': 0.9718575684876994, 'learning_rate': 0.00036002238705391967, 'batch_size': 21, 'step_size': 13, 'gamma': 0.9002372885888145}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:22:28,136][0m Trial 44 finished with value: 0.17195074126506463 and parameters: {'observation_period_num': 235, 'train_rates': 0.9718242178023404, 'learning_rate': 0.0004443206019835463, 'batch_size': 29, 'step_size': 13, 'gamma': 0.8954493896788849}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:28:37,769][0m Trial 45 finished with value: 0.19579126462340354 and parameters: {'observation_period_num': 213, 'train_rates': 0.9422337565742535, 'learning_rate': 0.00010837587818120368, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9216652597183279}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:35:03,610][0m Trial 46 finished with value: 0.12423247933387756 and parameters: {'observation_period_num': 238, 'train_rates': 0.9726151191295003, 'learning_rate': 0.0003217588869801616, 'batch_size': 48, 'step_size': 13, 'gamma': 0.8840060055157585}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:40:11,868][0m Trial 47 finished with value: 1.1382689360133047 and parameters: {'observation_period_num': 243, 'train_rates': 0.6591318123638046, 'learning_rate': 0.0003230795255538158, 'batch_size': 43, 'step_size': 13, 'gamma': 0.8997545495139634}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:45:33,735][0m Trial 48 finished with value: 1.2770868661160955 and parameters: {'observation_period_num': 207, 'train_rates': 0.9398021781498603, 'learning_rate': 0.0007381631705628627, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8872369824820605}. Best is trial 42 with value: 0.12178144785689145.[0m
[32m[I 2025-01-08 15:51:50,454][0m Trial 49 finished with value: 0.19813812451977883 and parameters: {'observation_period_num': 244, 'train_rates': 0.8980389380940514, 'learning_rate': 0.0002661302250702426, 'batch_size': 40, 'step_size': 14, 'gamma': 0.8513398328766114}. Best is trial 42 with value: 0.12178144785689145.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.5663 | 0.5449
Epoch 2/300, Loss: 0.6211 | 0.5311
Epoch 3/300, Loss: 0.5544 | 0.5711
Epoch 4/300, Loss: 0.4765 | 0.4544
Epoch 5/300, Loss: 0.4921 | 0.5830
Epoch 6/300, Loss: 0.5980 | 0.6560
Epoch 7/300, Loss: 0.5456 | 0.4916
Epoch 8/300, Loss: 0.7149 | 0.6727
Epoch 9/300, Loss: 0.7065 | 0.6873
Epoch 10/300, Loss: 0.5562 | 0.5273
Epoch 11/300, Loss: 0.5309 | 0.5317
Epoch 12/300, Loss: 0.6204 | 0.4853
Epoch 13/300, Loss: 0.5419 | 0.4651
Epoch 14/300, Loss: 0.4415 | 0.4728
Epoch 15/300, Loss: 0.4078 | 0.3818
Epoch 16/300, Loss: 0.4914 | 0.5078
Epoch 17/300, Loss: 0.4585 | 0.5026
Epoch 18/300, Loss: 0.5458 | 0.5550
Epoch 19/300, Loss: 0.5092 | 0.4638
Epoch 20/300, Loss: 0.3853 | 0.4214
Epoch 21/300, Loss: 0.3641 | 0.4114
Epoch 22/300, Loss: 0.3222 | 0.3774
Epoch 23/300, Loss: 0.3144 | 0.3502
Epoch 24/300, Loss: 0.2903 | 0.3466
Epoch 25/300, Loss: 0.2882 | 0.3419
Epoch 26/300, Loss: 0.2694 | 0.3131
Epoch 27/300, Loss: 0.2935 | 0.3544
Epoch 28/300, Loss: 0.2764 | 0.2999
Epoch 29/300, Loss: 0.2992 | 0.3653
Epoch 30/300, Loss: 0.2749 | 0.2856
Epoch 31/300, Loss: 0.2686 | 0.3057
Epoch 32/300, Loss: 0.2312 | 0.2546
Epoch 33/300, Loss: 0.2258 | 0.2703
Epoch 34/300, Loss: 0.2137 | 0.2530
Epoch 35/300, Loss: 0.2109 | 0.2429
Epoch 36/300, Loss: 0.2080 | 0.2662
Epoch 37/300, Loss: 0.1943 | 0.2319
Epoch 38/300, Loss: 0.1932 | 0.2223
Epoch 39/300, Loss: 0.1917 | 0.2564
Epoch 40/300, Loss: 0.1816 | 0.2177
Epoch 41/300, Loss: 0.1812 | 0.2214
Epoch 42/300, Loss: 0.1842 | 0.2248
Epoch 43/300, Loss: 0.1763 | 0.2081
Epoch 44/300, Loss: 0.1765 | 0.2056
Epoch 45/300, Loss: 0.1800 | 0.2172
Epoch 46/300, Loss: 0.1841 | 0.2272
Epoch 47/300, Loss: 0.1916 | 0.2040
Epoch 48/300, Loss: 0.1929 | 0.2388
Epoch 49/300, Loss: 0.1921 | 0.1986
Epoch 50/300, Loss: 0.1817 | 0.1850
Epoch 51/300, Loss: 0.2202 | 0.2095
Epoch 52/300, Loss: 0.1952 | 0.1949
Epoch 53/300, Loss: 0.1794 | 0.1978
Epoch 54/300, Loss: 0.1626 | 0.1950
Epoch 55/300, Loss: 0.1546 | 0.1760
Epoch 56/300, Loss: 0.1506 | 0.1714
Epoch 57/300, Loss: 0.1438 | 0.1762
Epoch 58/300, Loss: 0.1414 | 0.1676
Epoch 59/300, Loss: 0.1398 | 0.1683
Epoch 60/300, Loss: 0.1414 | 0.1773
Epoch 61/300, Loss: 0.1411 | 0.1554
Epoch 62/300, Loss: 0.1353 | 0.1635
Epoch 63/300, Loss: 0.1352 | 0.1692
Epoch 64/300, Loss: 0.1350 | 0.1557
Epoch 65/300, Loss: 0.1363 | 0.1594
Epoch 66/300, Loss: 0.1485 | 0.1643
Epoch 67/300, Loss: 0.1446 | 0.1525
Epoch 68/300, Loss: 0.1400 | 0.1677
Epoch 69/300, Loss: 0.1313 | 0.1683
Epoch 70/300, Loss: 0.1304 | 0.1443
Epoch 71/300, Loss: 0.1310 | 0.1662
Epoch 72/300, Loss: 0.1320 | 0.1507
Epoch 73/300, Loss: 0.1262 | 0.1488
Epoch 74/300, Loss: 0.1255 | 0.1517
Epoch 75/300, Loss: 0.1214 | 0.1468
Epoch 76/300, Loss: 0.1201 | 0.1414
Epoch 77/300, Loss: 0.1206 | 0.1568
Epoch 78/300, Loss: 0.1205 | 0.1399
Epoch 79/300, Loss: 0.1204 | 0.1537
Epoch 80/300, Loss: 0.1248 | 0.1502
Epoch 81/300, Loss: 0.1242 | 0.1444
Epoch 82/300, Loss: 0.1190 | 0.1477
Epoch 83/300, Loss: 0.1146 | 0.1416
Epoch 84/300, Loss: 0.1126 | 0.1401
Epoch 85/300, Loss: 0.1146 | 0.1510
Epoch 86/300, Loss: 0.1190 | 0.1402
Epoch 87/300, Loss: 0.1208 | 0.1405
Epoch 88/300, Loss: 0.1226 | 0.1442
Epoch 89/300, Loss: 0.1169 | 0.1359
Epoch 90/300, Loss: 0.1139 | 0.1388
Epoch 91/300, Loss: 0.1142 | 0.1350
Epoch 92/300, Loss: 0.1136 | 0.1367
Epoch 93/300, Loss: 0.1122 | 0.1327
Epoch 94/300, Loss: 0.1109 | 0.1352
Epoch 95/300, Loss: 0.1070 | 0.1303
Epoch 96/300, Loss: 0.1055 | 0.1346
Epoch 97/300, Loss: 0.1032 | 0.1316
Epoch 98/300, Loss: 0.1010 | 0.1327
Epoch 99/300, Loss: 0.1000 | 0.1289
Epoch 100/300, Loss: 0.1001 | 0.1288
Epoch 101/300, Loss: 0.1008 | 0.1288
Epoch 102/300, Loss: 0.0993 | 0.1293
Epoch 103/300, Loss: 0.0992 | 0.1301
Epoch 104/300, Loss: 0.0986 | 0.1259
Epoch 105/300, Loss: 0.0982 | 0.1252
Epoch 106/300, Loss: 0.0968 | 0.1239
Epoch 107/300, Loss: 0.0947 | 0.1241
Epoch 108/300, Loss: 0.0941 | 0.1214
Epoch 109/300, Loss: 0.0940 | 0.1215
Epoch 110/300, Loss: 0.0938 | 0.1221
Epoch 111/300, Loss: 0.0934 | 0.1244
Epoch 112/300, Loss: 0.0929 | 0.1232
Epoch 113/300, Loss: 0.0913 | 0.1248
Epoch 114/300, Loss: 0.0921 | 0.1236
Epoch 115/300, Loss: 0.0900 | 0.1222
Epoch 116/300, Loss: 0.0921 | 0.1243
Epoch 117/300, Loss: 0.0916 | 0.1230
Epoch 118/300, Loss: 0.0907 | 0.1210
Epoch 119/300, Loss: 0.0885 | 0.1229
Epoch 120/300, Loss: 0.0882 | 0.1215
Epoch 121/300, Loss: 0.0886 | 0.1200
Epoch 122/300, Loss: 0.0873 | 0.1206
Epoch 123/300, Loss: 0.0862 | 0.1196
Epoch 124/300, Loss: 0.0875 | 0.1226
Epoch 125/300, Loss: 0.0868 | 0.1186
Epoch 126/300, Loss: 0.0869 | 0.1189
Epoch 127/300, Loss: 0.0857 | 0.1175
Epoch 128/300, Loss: 0.0851 | 0.1197
Epoch 129/300, Loss: 0.0857 | 0.1180
Epoch 130/300, Loss: 0.0863 | 0.1215
Epoch 131/300, Loss: 0.0860 | 0.1170
Epoch 132/300, Loss: 0.0853 | 0.1175
Epoch 133/300, Loss: 0.0850 | 0.1149
Epoch 134/300, Loss: 0.0839 | 0.1160
Epoch 135/300, Loss: 0.0829 | 0.1162
Epoch 136/300, Loss: 0.0831 | 0.1172
Epoch 137/300, Loss: 0.0827 | 0.1176
Epoch 138/300, Loss: 0.0822 | 0.1166
Epoch 139/300, Loss: 0.0819 | 0.1162
Epoch 140/300, Loss: 0.0806 | 0.1140
Epoch 141/300, Loss: 0.0828 | 0.1154
Epoch 142/300, Loss: 0.0820 | 0.1152
Epoch 143/300, Loss: 0.0811 | 0.1150
Epoch 144/300, Loss: 0.0812 | 0.1144
Epoch 145/300, Loss: 0.0809 | 0.1140
Epoch 146/300, Loss: 0.0817 | 0.1129
Epoch 147/300, Loss: 0.0801 | 0.1128
Epoch 148/300, Loss: 0.0799 | 0.1130
Epoch 149/300, Loss: 0.0797 | 0.1152
Epoch 150/300, Loss: 0.0795 | 0.1149
Epoch 151/300, Loss: 0.0793 | 0.1135
Epoch 152/300, Loss: 0.0793 | 0.1120
Epoch 153/300, Loss: 0.0777 | 0.1121
Epoch 154/300, Loss: 0.0781 | 0.1121
Epoch 155/300, Loss: 0.0781 | 0.1115
Epoch 156/300, Loss: 0.0774 | 0.1137
Epoch 157/300, Loss: 0.0771 | 0.1135
Epoch 158/300, Loss: 0.0777 | 0.1137
Epoch 159/300, Loss: 0.0777 | 0.1141
Epoch 160/300, Loss: 0.0781 | 0.1138
Epoch 161/300, Loss: 0.0768 | 0.1133
Epoch 162/300, Loss: 0.0761 | 0.1143
Epoch 163/300, Loss: 0.0774 | 0.1126
Epoch 164/300, Loss: 0.0765 | 0.1123
Epoch 165/300, Loss: 0.0776 | 0.1112
Epoch 166/300, Loss: 0.0755 | 0.1117
Epoch 167/300, Loss: 0.0754 | 0.1117
Epoch 168/300, Loss: 0.0765 | 0.1114
Epoch 169/300, Loss: 0.0760 | 0.1128
Epoch 170/300, Loss: 0.0763 | 0.1122
Epoch 171/300, Loss: 0.0756 | 0.1137
Epoch 172/300, Loss: 0.0754 | 0.1129
Epoch 173/300, Loss: 0.0749 | 0.1126
Epoch 174/300, Loss: 0.0750 | 0.1113
Epoch 175/300, Loss: 0.0751 | 0.1113
Epoch 176/300, Loss: 0.0752 | 0.1119
Epoch 177/300, Loss: 0.0744 | 0.1126
Epoch 178/300, Loss: 0.0743 | 0.1121
Epoch 179/300, Loss: 0.0746 | 0.1130
Epoch 180/300, Loss: 0.0745 | 0.1121
Epoch 181/300, Loss: 0.0741 | 0.1096
Epoch 182/300, Loss: 0.0751 | 0.1100
Epoch 183/300, Loss: 0.0741 | 0.1103
Epoch 184/300, Loss: 0.0746 | 0.1106
Epoch 185/300, Loss: 0.0745 | 0.1110
Epoch 186/300, Loss: 0.0740 | 0.1108
Epoch 187/300, Loss: 0.0734 | 0.1135
Epoch 188/300, Loss: 0.0743 | 0.1127
Epoch 189/300, Loss: 0.0739 | 0.1121
Epoch 190/300, Loss: 0.0730 | 0.1114
Epoch 191/300, Loss: 0.0728 | 0.1113
Epoch 192/300, Loss: 0.0731 | 0.1108
Epoch 193/300, Loss: 0.0726 | 0.1108
Epoch 194/300, Loss: 0.0730 | 0.1108
Epoch 195/300, Loss: 0.0729 | 0.1119
Epoch 196/300, Loss: 0.0732 | 0.1124
Epoch 197/300, Loss: 0.0727 | 0.1137
Epoch 198/300, Loss: 0.0735 | 0.1141
Epoch 199/300, Loss: 0.0733 | 0.1130
Epoch 200/300, Loss: 0.0727 | 0.1115
Epoch 201/300, Loss: 0.0726 | 0.1114
Epoch 202/300, Loss: 0.0726 | 0.1109
Epoch 203/300, Loss: 0.0720 | 0.1111
Epoch 204/300, Loss: 0.0721 | 0.1111
Epoch 205/300, Loss: 0.0722 | 0.1117
Epoch 206/300, Loss: 0.0723 | 0.1122
Epoch 207/300, Loss: 0.0720 | 0.1126
Epoch 208/300, Loss: 0.0723 | 0.1123
Epoch 209/300, Loss: 0.0721 | 0.1122
Epoch 210/300, Loss: 0.0719 | 0.1108
Epoch 211/300, Loss: 0.0718 | 0.1108
Epoch 212/300, Loss: 0.0714 | 0.1110
Epoch 213/300, Loss: 0.0726 | 0.1109
Epoch 214/300, Loss: 0.0725 | 0.1107
Epoch 215/300, Loss: 0.0719 | 0.1107
Epoch 216/300, Loss: 0.0722 | 0.1114
Epoch 217/300, Loss: 0.0717 | 0.1114
Epoch 218/300, Loss: 0.0714 | 0.1121
Epoch 219/300, Loss: 0.0717 | 0.1129
Epoch 220/300, Loss: 0.0715 | 0.1136
Epoch 221/300, Loss: 0.0717 | 0.1127
Epoch 222/300, Loss: 0.0713 | 0.1126
Epoch 223/300, Loss: 0.0714 | 0.1119
Epoch 224/300, Loss: 0.0714 | 0.1116
Epoch 225/300, Loss: 0.0712 | 0.1112
Epoch 226/300, Loss: 0.0712 | 0.1113
Epoch 227/300, Loss: 0.0714 | 0.1112
Epoch 228/300, Loss: 0.0714 | 0.1118
Epoch 229/300, Loss: 0.0710 | 0.1117
Epoch 230/300, Loss: 0.0721 | 0.1124
Epoch 231/300, Loss: 0.0707 | 0.1114
Epoch 232/300, Loss: 0.0724 | 0.1111
Epoch 233/300, Loss: 0.0710 | 0.1118
Epoch 234/300, Loss: 0.0711 | 0.1126
Epoch 235/300, Loss: 0.0700 | 0.1124
Epoch 236/300, Loss: 0.0711 | 0.1120
Epoch 237/300, Loss: 0.0705 | 0.1119
Epoch 238/300, Loss: 0.0708 | 0.1117
Epoch 239/300, Loss: 0.0708 | 0.1119
Epoch 240/300, Loss: 0.0704 | 0.1119
Epoch 241/300, Loss: 0.0723 | 0.1118
Epoch 242/300, Loss: 0.0710 | 0.1117
Epoch 243/300, Loss: 0.0703 | 0.1117
Epoch 244/300, Loss: 0.0711 | 0.1117
Epoch 245/300, Loss: 0.0706 | 0.1115
Epoch 246/300, Loss: 0.0712 | 0.1114
Epoch 247/300, Loss: 0.0705 | 0.1118
Epoch 248/300, Loss: 0.0708 | 0.1119
Epoch 249/300, Loss: 0.0704 | 0.1120
Epoch 250/300, Loss: 0.0695 | 0.1116
Epoch 251/300, Loss: 0.0705 | 0.1110
Epoch 252/300, Loss: 0.0710 | 0.1111
Epoch 253/300, Loss: 0.0712 | 0.1115
Epoch 254/300, Loss: 0.0705 | 0.1116
Epoch 255/300, Loss: 0.0704 | 0.1116
Epoch 256/300, Loss: 0.0710 | 0.1114
Epoch 257/300, Loss: 0.0704 | 0.1117
Epoch 258/300, Loss: 0.0700 | 0.1116
Epoch 259/300, Loss: 0.0706 | 0.1118
Epoch 260/300, Loss: 0.0703 | 0.1117
Epoch 261/300, Loss: 0.0696 | 0.1121
Epoch 262/300, Loss: 0.0703 | 0.1121
Epoch 263/300, Loss: 0.0706 | 0.1121
Epoch 264/300, Loss: 0.0694 | 0.1120
Epoch 265/300, Loss: 0.0703 | 0.1117
Epoch 266/300, Loss: 0.0699 | 0.1113
Epoch 267/300, Loss: 0.0693 | 0.1113
Epoch 268/300, Loss: 0.0698 | 0.1115
Epoch 269/300, Loss: 0.0701 | 0.1113
Epoch 270/300, Loss: 0.0707 | 0.1114
Epoch 271/300, Loss: 0.0698 | 0.1114
Epoch 272/300, Loss: 0.0702 | 0.1110
Epoch 273/300, Loss: 0.0701 | 0.1111
Epoch 274/300, Loss: 0.0701 | 0.1111
Epoch 275/300, Loss: 0.0697 | 0.1110
Epoch 276/300, Loss: 0.0710 | 0.1112
Epoch 277/300, Loss: 0.0700 | 0.1112
Epoch 278/300, Loss: 0.0700 | 0.1113
Epoch 279/300, Loss: 0.0703 | 0.1110
Epoch 280/300, Loss: 0.0693 | 0.1111
Epoch 281/300, Loss: 0.0698 | 0.1112
Epoch 282/300, Loss: 0.0698 | 0.1113
Epoch 283/300, Loss: 0.0696 | 0.1115
Epoch 284/300, Loss: 0.0697 | 0.1115
Epoch 285/300, Loss: 0.0700 | 0.1114
Epoch 286/300, Loss: 0.0699 | 0.1112
Epoch 287/300, Loss: 0.0700 | 0.1111
Epoch 288/300, Loss: 0.0702 | 0.1110
Epoch 289/300, Loss: 0.0691 | 0.1111
Epoch 290/300, Loss: 0.0698 | 0.1112
Epoch 291/300, Loss: 0.0699 | 0.1113
Epoch 292/300, Loss: 0.0701 | 0.1116
Epoch 293/300, Loss: 0.0697 | 0.1114
Epoch 294/300, Loss: 0.0703 | 0.1113
Epoch 295/300, Loss: 0.0695 | 0.1113
Epoch 296/300, Loss: 0.0698 | 0.1114
Epoch 297/300, Loss: 0.0697 | 0.1114
Epoch 298/300, Loss: 0.0695 | 0.1114
Epoch 299/300, Loss: 0.0696 | 0.1114
Epoch 300/300, Loss: 0.0697 | 0.1115
Runtime (seconds): 988.9348011016846
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 180.64145867922343
RMSE: 13.440292358398438
MAE: 13.440292358398438
R-squared: nan
[238.7597]
