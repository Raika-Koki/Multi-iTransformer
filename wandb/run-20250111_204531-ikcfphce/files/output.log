ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-11 20:45:34,991][0m A new study created in memory with name: no-name-4da83658-79cc-490e-9568-4e584ba2bfd5[0m
[32m[I 2025-01-11 20:48:01,418][0m Trial 0 finished with value: 0.07952398784095194 and parameters: {'observation_period_num': 82, 'train_rates': 0.9430715532205539, 'learning_rate': 3.061813084765699e-05, 'batch_size': 38, 'step_size': 10, 'gamma': 0.9450380944028312}. Best is trial 0 with value: 0.07952398784095194.[0m
[32m[I 2025-01-11 20:49:19,916][0m Trial 1 finished with value: 0.9005407643924596 and parameters: {'observation_period_num': 90, 'train_rates': 0.7390324219578801, 'learning_rate': 3.5988291063024756e-06, 'batch_size': 59, 'step_size': 3, 'gamma': 0.8847750669837984}. Best is trial 0 with value: 0.07952398784095194.[0m
[32m[I 2025-01-11 20:49:42,232][0m Trial 2 finished with value: 0.6524347823893254 and parameters: {'observation_period_num': 193, 'train_rates': 0.8148715325932832, 'learning_rate': 8.460327066206584e-06, 'batch_size': 249, 'step_size': 3, 'gamma': 0.913845661217252}. Best is trial 0 with value: 0.07952398784095194.[0m
[32m[I 2025-01-11 20:50:01,715][0m Trial 3 finished with value: 0.7947969079133714 and parameters: {'observation_period_num': 200, 'train_rates': 0.6270403350847449, 'learning_rate': 6.020256254781371e-06, 'batch_size': 244, 'step_size': 12, 'gamma': 0.9086455403234016}. Best is trial 0 with value: 0.07952398784095194.[0m
[32m[I 2025-01-11 20:50:42,267][0m Trial 4 finished with value: 0.3331867640710592 and parameters: {'observation_period_num': 108, 'train_rates': 0.6249919067205408, 'learning_rate': 8.629629017465414e-05, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8522926554270664}. Best is trial 0 with value: 0.07952398784095194.[0m
[32m[I 2025-01-11 20:51:26,365][0m Trial 5 finished with value: 0.07796463828551377 and parameters: {'observation_period_num': 122, 'train_rates': 0.92479505531989, 'learning_rate': 0.0001722145358110317, 'batch_size': 130, 'step_size': 12, 'gamma': 0.8831857663226572}. Best is trial 5 with value: 0.07796463828551377.[0m
[32m[I 2025-01-11 20:53:03,780][0m Trial 6 finished with value: 0.055697251221921186 and parameters: {'observation_period_num': 20, 'train_rates': 0.827878462150444, 'learning_rate': 0.0002169270473428622, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9519600788290478}. Best is trial 6 with value: 0.055697251221921186.[0m
[32m[I 2025-01-11 20:53:27,599][0m Trial 7 finished with value: 0.40751099031275284 and parameters: {'observation_period_num': 198, 'train_rates': 0.698081533014594, 'learning_rate': 0.00018563390843513922, 'batch_size': 203, 'step_size': 6, 'gamma': 0.754728465183413}. Best is trial 6 with value: 0.055697251221921186.[0m
[32m[I 2025-01-11 20:54:14,950][0m Trial 8 finished with value: 0.22634518146514893 and parameters: {'observation_period_num': 96, 'train_rates': 0.9835438604219214, 'learning_rate': 3.287803228142371e-05, 'batch_size': 123, 'step_size': 7, 'gamma': 0.9022353400982093}. Best is trial 6 with value: 0.055697251221921186.[0m
[32m[I 2025-01-11 20:54:44,943][0m Trial 9 finished with value: 1.5498893963246188 and parameters: {'observation_period_num': 233, 'train_rates': 0.8785540114436583, 'learning_rate': 4.088872880924801e-06, 'batch_size': 175, 'step_size': 4, 'gamma': 0.7917402640019222}. Best is trial 6 with value: 0.055697251221921186.[0m
[32m[I 2025-01-11 20:59:24,888][0m Trial 10 finished with value: 0.03514277391726551 and parameters: {'observation_period_num': 10, 'train_rates': 0.8245676016500548, 'learning_rate': 0.0008913326156530081, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9887820800390957}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:04:37,251][0m Trial 11 finished with value: 0.048837591441716434 and parameters: {'observation_period_num': 17, 'train_rates': 0.8225974997953642, 'learning_rate': 0.0008420025400100141, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9887227908247009}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:08:24,567][0m Trial 12 finished with value: 0.16087461239558654 and parameters: {'observation_period_num': 5, 'train_rates': 0.7622080042705507, 'learning_rate': 0.0009770161383042444, 'batch_size': 21, 'step_size': 1, 'gamma': 0.980850838876955}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:09:30,778][0m Trial 13 finished with value: 0.10416460407041285 and parameters: {'observation_period_num': 43, 'train_rates': 0.8588405375729975, 'learning_rate': 0.0008249872545827095, 'batch_size': 81, 'step_size': 5, 'gamma': 0.989211780183821}. Best is trial 10 with value: 0.03514277391726551.[0m
Early stopping at epoch 64
[32m[I 2025-01-11 21:11:52,314][0m Trial 14 finished with value: 0.980281399326296 and parameters: {'observation_period_num': 57, 'train_rates': 0.769129687381424, 'learning_rate': 1.1189449293756843e-06, 'batch_size': 22, 'step_size': 1, 'gamma': 0.8413390382039927}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:12:48,679][0m Trial 15 finished with value: 0.19329948625905774 and parameters: {'observation_period_num': 44, 'train_rates': 0.7082502080709547, 'learning_rate': 0.0004625733475983336, 'batch_size': 84, 'step_size': 8, 'gamma': 0.9502307407125083}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:13:21,592][0m Trial 16 finished with value: 0.10963929181327363 and parameters: {'observation_period_num': 154, 'train_rates': 0.880806190928699, 'learning_rate': 0.00036954324045842277, 'batch_size': 165, 'step_size': 3, 'gamma': 0.9698790837179809}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:14:30,506][0m Trial 17 finished with value: 0.1462753722254111 and parameters: {'observation_period_num': 152, 'train_rates': 0.807562012506612, 'learning_rate': 5.877711695078494e-05, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9339995630100311}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:18:44,954][0m Trial 18 finished with value: 0.0674779219465835 and parameters: {'observation_period_num': 65, 'train_rates': 0.8518911978886509, 'learning_rate': 0.0004694241232240004, 'batch_size': 20, 'step_size': 10, 'gamma': 0.9658930568909645}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:20:47,657][0m Trial 19 finished with value: 0.09752827459808 and parameters: {'observation_period_num': 23, 'train_rates': 0.9219805599506252, 'learning_rate': 0.00010425224345117836, 'batch_size': 45, 'step_size': 2, 'gamma': 0.8199659062348269}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:21:35,583][0m Trial 20 finished with value: 0.31804433241807695 and parameters: {'observation_period_num': 25, 'train_rates': 0.6830557481581714, 'learning_rate': 1.2612050746853687e-05, 'batch_size': 97, 'step_size': 15, 'gamma': 0.9284211893824534}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:23:25,398][0m Trial 21 finished with value: 0.04264536424811313 and parameters: {'observation_period_num': 5, 'train_rates': 0.82788266203135, 'learning_rate': 0.0002854244611357802, 'batch_size': 47, 'step_size': 1, 'gamma': 0.9644364533031108}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:25:29,773][0m Trial 22 finished with value: 0.1703028456593283 and parameters: {'observation_period_num': 11, 'train_rates': 0.7816732332184136, 'learning_rate': 0.0006179096246630707, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9898712807760885}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:26:52,312][0m Trial 23 finished with value: 0.056790759868624315 and parameters: {'observation_period_num': 42, 'train_rates': 0.835520106655554, 'learning_rate': 0.0002886799458970625, 'batch_size': 63, 'step_size': 1, 'gamma': 0.9626773676563104}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:31:28,030][0m Trial 24 finished with value: 0.07727583450761354 and parameters: {'observation_period_num': 69, 'train_rates': 0.8887613999046117, 'learning_rate': 0.0009655079073851563, 'batch_size': 19, 'step_size': 4, 'gamma': 0.9673527294396023}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:33:29,261][0m Trial 25 finished with value: 0.05070142444166435 and parameters: {'observation_period_num': 37, 'train_rates': 0.7975244975770943, 'learning_rate': 0.0005188204221903684, 'batch_size': 41, 'step_size': 2, 'gamma': 0.9304855340189064}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:34:01,682][0m Trial 26 finished with value: 0.16909745885459193 and parameters: {'observation_period_num': 7, 'train_rates': 0.7234202234194896, 'learning_rate': 0.000299438512544462, 'batch_size': 153, 'step_size': 4, 'gamma': 0.9892792916941757}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:34:46,238][0m Trial 27 finished with value: 0.1882843038800991 and parameters: {'observation_period_num': 30, 'train_rates': 0.6614174920045099, 'learning_rate': 0.00010970378476274154, 'batch_size': 105, 'step_size': 2, 'gamma': 0.9465878046654785}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:36:55,977][0m Trial 28 finished with value: 0.23451286381318712 and parameters: {'observation_period_num': 56, 'train_rates': 0.7621207320381052, 'learning_rate': 0.0006710726090875767, 'batch_size': 37, 'step_size': 8, 'gamma': 0.9688674404732861}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:39:34,517][0m Trial 29 finished with value: 0.0804281042840648 and parameters: {'observation_period_num': 72, 'train_rates': 0.9062342366917441, 'learning_rate': 2.7767714698167906e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.9222844526938351}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:41:01,549][0m Trial 30 finished with value: 0.0697514774817527 and parameters: {'observation_period_num': 85, 'train_rates': 0.8401607786974657, 'learning_rate': 0.0002601159569965021, 'batch_size': 59, 'step_size': 10, 'gamma': 0.943103581377717}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:43:20,238][0m Trial 31 finished with value: 0.07021270350229983 and parameters: {'observation_period_num': 37, 'train_rates': 0.798452080512121, 'learning_rate': 0.0005534379398287664, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9743403533785843}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:44:57,730][0m Trial 32 finished with value: 0.04130422345547559 and parameters: {'observation_period_num': 18, 'train_rates': 0.8061511470106965, 'learning_rate': 0.00042965460569836756, 'batch_size': 52, 'step_size': 3, 'gamma': 0.9556220024247841}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:46:10,657][0m Trial 33 finished with value: 0.06205885614174819 and parameters: {'observation_period_num': 6, 'train_rates': 0.8212039257411641, 'learning_rate': 0.00014636615480838713, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8911152829914715}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:51:02,958][0m Trial 34 finished with value: 0.1702477279695243 and parameters: {'observation_period_num': 21, 'train_rates': 0.7496936820701908, 'learning_rate': 0.0003817249657772309, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9504947136952725}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:52:47,533][0m Trial 35 finished with value: 0.07099151647832226 and parameters: {'observation_period_num': 51, 'train_rates': 0.8628846018817393, 'learning_rate': 6.0851432660411896e-05, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9560048262325468}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:53:43,880][0m Trial 36 finished with value: 0.07218186074141324 and parameters: {'observation_period_num': 23, 'train_rates': 0.7873402345533601, 'learning_rate': 0.0007128833891970824, 'batch_size': 92, 'step_size': 3, 'gamma': 0.8671300682782455}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:54:07,356][0m Trial 37 finished with value: 0.29338567521212233 and parameters: {'observation_period_num': 105, 'train_rates': 0.7395101182696115, 'learning_rate': 0.0003382432097870197, 'batch_size': 223, 'step_size': 1, 'gamma': 0.9771513540659612}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:55:20,113][0m Trial 38 finished with value: 0.1035801424097804 and parameters: {'observation_period_num': 141, 'train_rates': 0.8190067712114631, 'learning_rate': 0.0009989554082122946, 'batch_size': 69, 'step_size': 5, 'gamma': 0.9134903342455324}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:56:09,865][0m Trial 39 finished with value: 0.0936842302777874 and parameters: {'observation_period_num': 74, 'train_rates': 0.9536936876640507, 'learning_rate': 0.0001541473655827131, 'batch_size': 115, 'step_size': 2, 'gamma': 0.9400064320072369}. Best is trial 10 with value: 0.03514277391726551.[0m
[32m[I 2025-01-11 21:57:42,996][0m Trial 40 finished with value: 0.03196628084954094 and parameters: {'observation_period_num': 15, 'train_rates': 0.8379957696440765, 'learning_rate': 0.00021740834374012042, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8998284971100798}. Best is trial 40 with value: 0.03196628084954094.[0m
[32m[I 2025-01-11 22:00:30,202][0m Trial 41 finished with value: 0.030544073058864772 and parameters: {'observation_period_num': 16, 'train_rates': 0.8427565868591751, 'learning_rate': 0.00022184423320473115, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8750039272707331}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:02:04,931][0m Trial 42 finished with value: 0.05011879912917576 and parameters: {'observation_period_num': 28, 'train_rates': 0.849258517368969, 'learning_rate': 0.00017069410493606406, 'batch_size': 56, 'step_size': 14, 'gamma': 0.8603683877516516}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:04:49,569][0m Trial 43 finished with value: 0.10590837735332836 and parameters: {'observation_period_num': 247, 'train_rates': 0.8661974972670358, 'learning_rate': 0.00020990883533811785, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8795003048199804}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:06:35,522][0m Trial 44 finished with value: 0.10917758334566045 and parameters: {'observation_period_num': 179, 'train_rates': 0.9029057701704212, 'learning_rate': 6.471387475229369e-05, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8988957500941137}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:07:43,932][0m Trial 45 finished with value: 0.064382745037702 and parameters: {'observation_period_num': 10, 'train_rates': 0.8357866441945737, 'learning_rate': 3.346438264947124e-05, 'batch_size': 77, 'step_size': 12, 'gamma': 0.8300564333125416}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:10:27,196][0m Trial 46 finished with value: 0.06167743103810689 and parameters: {'observation_period_num': 32, 'train_rates': 0.8075399253206444, 'learning_rate': 0.00024103178483931317, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8796805357069295}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:11:05,006][0m Trial 47 finished with value: 0.20052290591592908 and parameters: {'observation_period_num': 16, 'train_rates': 0.7819989082383441, 'learning_rate': 0.00010841189003823361, 'batch_size': 139, 'step_size': 13, 'gamma': 0.8503695838586081}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:12:06,264][0m Trial 48 finished with value: 1.0483774192955182 and parameters: {'observation_period_num': 50, 'train_rates': 0.8733230619959241, 'learning_rate': 1.1923186317146427e-06, 'batch_size': 88, 'step_size': 9, 'gamma': 0.9206750360746966}. Best is trial 41 with value: 0.030544073058864772.[0m
[32m[I 2025-01-11 22:13:24,642][0m Trial 49 finished with value: 0.12427819139665837 and parameters: {'observation_period_num': 214, 'train_rates': 0.848030603881239, 'learning_rate': 0.0003915736037136518, 'batch_size': 63, 'step_size': 11, 'gamma': 0.8067445932043182}. Best is trial 41 with value: 0.030544073058864772.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-11 22:13:24,652][0m A new study created in memory with name: no-name-9cf0b754-f02e-4fa6-b47f-f1c79920a777[0m
[32m[I 2025-01-11 22:13:51,404][0m Trial 0 finished with value: 0.10122093147947453 and parameters: {'observation_period_num': 77, 'train_rates': 0.8219573716008477, 'learning_rate': 0.00017309208284096636, 'batch_size': 198, 'step_size': 13, 'gamma': 0.80325982131949}. Best is trial 0 with value: 0.10122093147947453.[0m
[32m[I 2025-01-11 22:14:21,620][0m Trial 1 finished with value: 0.08728229808945988 and parameters: {'observation_period_num': 105, 'train_rates': 0.879312345111281, 'learning_rate': 0.00023219975679917632, 'batch_size': 187, 'step_size': 6, 'gamma': 0.8756499125106053}. Best is trial 1 with value: 0.08728229808945988.[0m
[32m[I 2025-01-11 22:14:47,939][0m Trial 2 finished with value: 0.34715344385362007 and parameters: {'observation_period_num': 135, 'train_rates': 0.8741119347228619, 'learning_rate': 2.5390505319724157e-05, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8010973062913056}. Best is trial 1 with value: 0.08728229808945988.[0m
[32m[I 2025-01-11 22:15:40,128][0m Trial 3 finished with value: 0.1727568971329048 and parameters: {'observation_period_num': 43, 'train_rates': 0.958248498552583, 'learning_rate': 3.337708226957039e-05, 'batch_size': 112, 'step_size': 5, 'gamma': 0.9101506692842535}. Best is trial 1 with value: 0.08728229808945988.[0m
[32m[I 2025-01-11 22:16:18,618][0m Trial 4 finished with value: 0.08155386303551496 and parameters: {'observation_period_num': 169, 'train_rates': 0.94264631299356, 'learning_rate': 0.0007321152987606948, 'batch_size': 147, 'step_size': 11, 'gamma': 0.8271424909623227}. Best is trial 4 with value: 0.08155386303551496.[0m
[32m[I 2025-01-11 22:16:42,814][0m Trial 5 finished with value: 0.7680128812789917 and parameters: {'observation_period_num': 176, 'train_rates': 0.9771529613244224, 'learning_rate': 5.1078598491830194e-06, 'batch_size': 255, 'step_size': 6, 'gamma': 0.8837756966973493}. Best is trial 4 with value: 0.08155386303551496.[0m
[32m[I 2025-01-11 22:17:08,683][0m Trial 6 finished with value: 0.13970620930194855 and parameters: {'observation_period_num': 122, 'train_rates': 0.9506268692355069, 'learning_rate': 0.0005247438456320177, 'batch_size': 242, 'step_size': 1, 'gamma': 0.9347696042957413}. Best is trial 4 with value: 0.08155386303551496.[0m
[32m[I 2025-01-11 22:19:04,982][0m Trial 7 finished with value: 0.5721424826163489 and parameters: {'observation_period_num': 54, 'train_rates': 0.7216783826063959, 'learning_rate': 3.835116632588646e-06, 'batch_size': 40, 'step_size': 11, 'gamma': 0.8746897877587226}. Best is trial 4 with value: 0.08155386303551496.[0m
[32m[I 2025-01-11 22:19:51,437][0m Trial 8 finished with value: 0.9429895752734542 and parameters: {'observation_period_num': 158, 'train_rates': 0.6455244755726817, 'learning_rate': 2.064227052372801e-06, 'batch_size': 91, 'step_size': 3, 'gamma': 0.949906416733466}. Best is trial 4 with value: 0.08155386303551496.[0m
[32m[I 2025-01-11 22:20:27,655][0m Trial 9 finished with value: 0.07862129807472229 and parameters: {'observation_period_num': 6, 'train_rates': 0.9626972703487038, 'learning_rate': 0.00010669581900420096, 'batch_size': 172, 'step_size': 7, 'gamma': 0.7744834861375249}. Best is trial 9 with value: 0.07862129807472229.[0m
[32m[I 2025-01-11 22:20:59,041][0m Trial 10 finished with value: 0.4876126719162603 and parameters: {'observation_period_num': 243, 'train_rates': 0.7533351032147563, 'learning_rate': 5.246254967054545e-05, 'batch_size': 156, 'step_size': 9, 'gamma': 0.7678844454402168}. Best is trial 9 with value: 0.07862129807472229.[0m
[32m[I 2025-01-11 22:21:37,421][0m Trial 11 finished with value: 0.03813376800720193 and parameters: {'observation_period_num': 9, 'train_rates': 0.8814309422829758, 'learning_rate': 0.0008308199014111168, 'batch_size': 151, 'step_size': 15, 'gamma': 0.8242293612210936}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:22:45,285][0m Trial 12 finished with value: 0.05076301454201988 and parameters: {'observation_period_num': 9, 'train_rates': 0.8828986832393468, 'learning_rate': 0.000125686436916223, 'batch_size': 82, 'step_size': 15, 'gamma': 0.7500650201031612}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:24:03,484][0m Trial 13 finished with value: 0.038285387510603126 and parameters: {'observation_period_num': 10, 'train_rates': 0.8580593362804918, 'learning_rate': 0.000358109547809393, 'batch_size': 70, 'step_size': 15, 'gamma': 0.8366349332136843}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:26:16,111][0m Trial 14 finished with value: 0.05017304854140482 and parameters: {'observation_period_num': 39, 'train_rates': 0.8202241240685982, 'learning_rate': 0.0008910639669892372, 'batch_size': 38, 'step_size': 15, 'gamma': 0.8399752206300685}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:27:00,244][0m Trial 15 finished with value: 0.22607075074397096 and parameters: {'observation_period_num': 81, 'train_rates': 0.7725198472832256, 'learning_rate': 0.0003153680329418369, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8372921624228715}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:28:26,878][0m Trial 16 finished with value: 0.046605141212542854 and parameters: {'observation_period_num': 22, 'train_rates': 0.8569621925874867, 'learning_rate': 0.0004380014088191365, 'batch_size': 62, 'step_size': 9, 'gamma': 0.8105264070439713}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:29:08,977][0m Trial 17 finished with value: 0.10050968463514365 and parameters: {'observation_period_num': 219, 'train_rates': 0.9069610480499798, 'learning_rate': 7.83174598990849e-05, 'batch_size': 124, 'step_size': 13, 'gamma': 0.9852587303842067}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:32:10,233][0m Trial 18 finished with value: 0.27956291222160373 and parameters: {'observation_period_num': 74, 'train_rates': 0.6784425516891988, 'learning_rate': 1.3754849433624433e-05, 'batch_size': 24, 'step_size': 10, 'gamma': 0.8522191460259507}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:33:18,639][0m Trial 19 finished with value: 0.059163249954878094 and parameters: {'observation_period_num': 35, 'train_rates': 0.8280485852685986, 'learning_rate': 0.000955650618645665, 'batch_size': 77, 'step_size': 15, 'gamma': 0.9017413647625436}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:33:50,268][0m Trial 20 finished with value: 0.17817335548838292 and parameters: {'observation_period_num': 58, 'train_rates': 0.6040400252015982, 'learning_rate': 0.00037727481508499546, 'batch_size': 139, 'step_size': 13, 'gamma': 0.8543370776786163}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:35:10,657][0m Trial 21 finished with value: 0.04215983279083731 and parameters: {'observation_period_num': 18, 'train_rates': 0.8521795526113368, 'learning_rate': 0.00039634999382828036, 'batch_size': 66, 'step_size': 9, 'gamma': 0.8102163466707671}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:36:45,668][0m Trial 22 finished with value: 0.0480860325396061 and parameters: {'observation_period_num': 6, 'train_rates': 0.915470956386939, 'learning_rate': 0.00022489732167768945, 'batch_size': 59, 'step_size': 4, 'gamma': 0.8166932341963481}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:37:35,736][0m Trial 23 finished with value: 0.05688537037412259 and parameters: {'observation_period_num': 29, 'train_rates': 0.7944546506874756, 'learning_rate': 0.0005022435361163827, 'batch_size': 105, 'step_size': 8, 'gamma': 0.7874681236036821}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:39:10,664][0m Trial 24 finished with value: 0.07975023276493197 and parameters: {'observation_period_num': 95, 'train_rates': 0.8507621926253103, 'learning_rate': 0.00019201107640881974, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8499323343443517}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:44:04,153][0m Trial 25 finished with value: 0.07524236134792629 and parameters: {'observation_period_num': 55, 'train_rates': 0.9017920678619515, 'learning_rate': 0.0005819619370968112, 'batch_size': 18, 'step_size': 14, 'gamma': 0.7933924730158144}. Best is trial 11 with value: 0.03813376800720193.[0m
Early stopping at epoch 69
[32m[I 2025-01-11 22:44:42,064][0m Trial 26 finished with value: 0.47102125156710356 and parameters: {'observation_period_num': 22, 'train_rates': 0.839533754522984, 'learning_rate': 6.862722277779669e-05, 'batch_size': 100, 'step_size': 1, 'gamma': 0.8241682248399647}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:45:13,221][0m Trial 27 finished with value: 0.06515669328434323 and parameters: {'observation_period_num': 22, 'train_rates': 0.7923976732634861, 'learning_rate': 0.00030585087659215263, 'batch_size': 170, 'step_size': 14, 'gamma': 0.8628128073927179}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:45:51,524][0m Trial 28 finished with value: 1.0413606988249111 and parameters: {'observation_period_num': 66, 'train_rates': 0.7343449372632497, 'learning_rate': 1.0169879745602178e-06, 'batch_size': 125, 'step_size': 11, 'gamma': 0.7710751507660442}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:46:18,915][0m Trial 29 finished with value: 0.1471986025571823 and parameters: {'observation_period_num': 197, 'train_rates': 0.926834371097206, 'learning_rate': 0.00013052368534523747, 'batch_size': 204, 'step_size': 12, 'gamma': 0.8307947045668346}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:47:28,954][0m Trial 30 finished with value: 0.07096742567282088 and parameters: {'observation_period_num': 89, 'train_rates': 0.8264825630795951, 'learning_rate': 0.0009913212802407868, 'batch_size': 73, 'step_size': 9, 'gamma': 0.8048417518618297}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:48:51,262][0m Trial 31 finished with value: 0.04648592766327194 and parameters: {'observation_period_num': 23, 'train_rates': 0.8653433622510924, 'learning_rate': 0.0004293528012778063, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8162556072041363}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:51:04,283][0m Trial 32 finished with value: 0.051636579546554026 and parameters: {'observation_period_num': 43, 'train_rates': 0.8753886353293588, 'learning_rate': 0.0002771827546166882, 'batch_size': 40, 'step_size': 7, 'gamma': 0.7846599873917955}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:52:19,760][0m Trial 33 finished with value: 0.06568744217507101 and parameters: {'observation_period_num': 17, 'train_rates': 0.8097242210868582, 'learning_rate': 0.00017940752223309498, 'batch_size': 68, 'step_size': 8, 'gamma': 0.8111789106754476}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:53:19,561][0m Trial 34 finished with value: 0.05754042689937011 and parameters: {'observation_period_num': 44, 'train_rates': 0.8623675261495702, 'learning_rate': 0.0004227696613045598, 'batch_size': 92, 'step_size': 10, 'gamma': 0.8405441633522472}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:55:09,528][0m Trial 35 finished with value: 0.05490278399416378 and parameters: {'observation_period_num': 33, 'train_rates': 0.8826877327337441, 'learning_rate': 0.0006384373683987947, 'batch_size': 49, 'step_size': 3, 'gamma': 0.7977739879017585}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:55:45,694][0m Trial 36 finished with value: 0.4368161149569695 and parameters: {'observation_period_num': 114, 'train_rates': 0.8937115524519389, 'learning_rate': 1.475182332508964e-05, 'batch_size': 157, 'step_size': 6, 'gamma': 0.8891749820994849}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:56:54,391][0m Trial 37 finished with value: 0.06278486880619964 and parameters: {'observation_period_num': 5, 'train_rates': 0.9244999795260298, 'learning_rate': 4.5544193959037473e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.8210093758642755}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:57:23,080][0m Trial 38 finished with value: 0.07535144930605388 and parameters: {'observation_period_num': 143, 'train_rates': 0.8609779193385869, 'learning_rate': 0.0006466198358228701, 'batch_size': 194, 'step_size': 5, 'gamma': 0.8663682336333897}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:58:09,121][0m Trial 39 finished with value: 0.09927278757095337 and parameters: {'observation_period_num': 66, 'train_rates': 0.9873951838512447, 'learning_rate': 0.0002480184325922281, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8264306365615447}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 22:58:37,300][0m Trial 40 finished with value: 0.08256898820400238 and parameters: {'observation_period_num': 19, 'train_rates': 0.9371575626020201, 'learning_rate': 0.00014935955166106354, 'batch_size': 224, 'step_size': 11, 'gamma': 0.8067756324147037}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:00:01,852][0m Trial 41 finished with value: 0.04516438364782589 and parameters: {'observation_period_num': 20, 'train_rates': 0.8476442209892888, 'learning_rate': 0.00043799668391021997, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8086050548360044}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:01:57,856][0m Trial 42 finished with value: 0.04485846908775278 and parameters: {'observation_period_num': 30, 'train_rates': 0.843727908831962, 'learning_rate': 0.0003811272185040923, 'batch_size': 45, 'step_size': 10, 'gamma': 0.7815861359204831}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:04:47,679][0m Trial 43 finished with value: 0.08007050473843852 and parameters: {'observation_period_num': 47, 'train_rates': 0.8401954701946459, 'learning_rate': 0.0007491561774043995, 'batch_size': 30, 'step_size': 10, 'gamma': 0.7594888693664518}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:07:18,810][0m Trial 44 finished with value: 0.19269735634964905 and parameters: {'observation_period_num': 34, 'train_rates': 0.7759466927739633, 'learning_rate': 0.00037237753477857783, 'batch_size': 32, 'step_size': 11, 'gamma': 0.778845254762908}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:09:06,802][0m Trial 45 finished with value: 0.07192500830613247 and parameters: {'observation_period_num': 13, 'train_rates': 0.8066218769008294, 'learning_rate': 2.391507016647571e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.799345384597959}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:10:02,435][0m Trial 46 finished with value: 0.0532074858680432 and parameters: {'observation_period_num': 48, 'train_rates': 0.8412191199860837, 'learning_rate': 0.0006711777850359708, 'batch_size': 94, 'step_size': 14, 'gamma': 0.761649473998275}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:10:55,801][0m Trial 47 finished with value: 0.05322447033253259 and parameters: {'observation_period_num': 31, 'train_rates': 0.8858667371080756, 'learning_rate': 0.00023262815254043842, 'batch_size': 107, 'step_size': 15, 'gamma': 0.786373445165798}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:11:57,205][0m Trial 48 finished with value: 0.20447054885226063 and parameters: {'observation_period_num': 5, 'train_rates': 0.7764895740437018, 'learning_rate': 8.82061720057741e-05, 'batch_size': 82, 'step_size': 9, 'gamma': 0.839007628816899}. Best is trial 11 with value: 0.03813376800720193.[0m
[32m[I 2025-01-11 23:12:32,410][0m Trial 49 finished with value: 0.06919936364520218 and parameters: {'observation_period_num': 63, 'train_rates': 0.8210628721510407, 'learning_rate': 0.00052174394633933, 'batch_size': 149, 'step_size': 10, 'gamma': 0.8789871935638331}. Best is trial 11 with value: 0.03813376800720193.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-11 23:12:32,421][0m A new study created in memory with name: no-name-a9ab78e5-09dd-4499-855f-45698b8f7b5b[0m
[32m[I 2025-01-11 23:18:07,291][0m Trial 0 finished with value: 0.4254468555895158 and parameters: {'observation_period_num': 42, 'train_rates': 0.9339951629230419, 'learning_rate': 2.379033482511492e-06, 'batch_size': 16, 'step_size': 2, 'gamma': 0.935346811134683}. Best is trial 0 with value: 0.4254468555895158.[0m
[32m[I 2025-01-11 23:19:21,313][0m Trial 1 finished with value: 0.128860833144736 and parameters: {'observation_period_num': 135, 'train_rates': 0.9383675078080236, 'learning_rate': 0.00020685390237381844, 'batch_size': 74, 'step_size': 3, 'gamma': 0.7712780906565496}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:20:29,293][0m Trial 2 finished with value: 0.3688730100383524 and parameters: {'observation_period_num': 127, 'train_rates': 0.8275657271465432, 'learning_rate': 7.733505557876343e-06, 'batch_size': 74, 'step_size': 12, 'gamma': 0.8231702328163926}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:23:47,267][0m Trial 3 finished with value: 0.18938906940482778 and parameters: {'observation_period_num': 205, 'train_rates': 0.8157732818054292, 'learning_rate': 0.00012811145545806084, 'batch_size': 24, 'step_size': 3, 'gamma': 0.9730034510664541}. Best is trial 1 with value: 0.128860833144736.[0m
Early stopping at epoch 61
[32m[I 2025-01-11 23:24:16,477][0m Trial 4 finished with value: 0.9676553803682327 and parameters: {'observation_period_num': 100, 'train_rates': 0.6847135576338675, 'learning_rate': 2.4991618961688027e-05, 'batch_size': 99, 'step_size': 1, 'gamma': 0.8010582564213159}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:24:35,663][0m Trial 5 finished with value: 0.21448042345607382 and parameters: {'observation_period_num': 38, 'train_rates': 0.6789666817548772, 'learning_rate': 0.0002573898036515638, 'batch_size': 248, 'step_size': 6, 'gamma': 0.877029413847306}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:25:40,634][0m Trial 6 finished with value: 0.7396714376346505 and parameters: {'observation_period_num': 63, 'train_rates': 0.7774817483630077, 'learning_rate': 6.3713696777506005e-06, 'batch_size': 75, 'step_size': 6, 'gamma': 0.7855254315852318}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:25:59,887][0m Trial 7 finished with value: 0.7482316223759725 and parameters: {'observation_period_num': 121, 'train_rates': 0.6593788934093202, 'learning_rate': 1.4770546301834632e-05, 'batch_size': 255, 'step_size': 15, 'gamma': 0.8748923334053095}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:26:31,732][0m Trial 8 finished with value: 0.584080159664154 and parameters: {'observation_period_num': 134, 'train_rates': 0.9829365563431853, 'learning_rate': 6.2096942012884415e-06, 'batch_size': 187, 'step_size': 8, 'gamma': 0.939884179174015}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:27:02,046][0m Trial 9 finished with value: 0.5933994931333205 and parameters: {'observation_period_num': 138, 'train_rates': 0.9275342204335018, 'learning_rate': 6.27710085432799e-06, 'batch_size': 197, 'step_size': 12, 'gamma': 0.9779713775359301}. Best is trial 1 with value: 0.128860833144736.[0m
[32m[I 2025-01-11 23:27:36,356][0m Trial 10 finished with value: 0.10852567911548103 and parameters: {'observation_period_num': 236, 'train_rates': 0.8905912192777485, 'learning_rate': 0.0009675276377020592, 'batch_size': 152, 'step_size': 4, 'gamma': 0.763220550291974}. Best is trial 10 with value: 0.10852567911548103.[0m
[32m[I 2025-01-11 23:28:14,861][0m Trial 11 finished with value: 0.10006428920096605 and parameters: {'observation_period_num': 247, 'train_rates': 0.8865572001278689, 'learning_rate': 0.000992432424338459, 'batch_size': 140, 'step_size': 4, 'gamma': 0.7555912205444655}. Best is trial 11 with value: 0.10006428920096605.[0m
[32m[I 2025-01-11 23:28:49,518][0m Trial 12 finished with value: 0.10087725676260197 and parameters: {'observation_period_num': 249, 'train_rates': 0.8607375342900347, 'learning_rate': 0.0009462075325836002, 'batch_size': 154, 'step_size': 5, 'gamma': 0.753248366505944}. Best is trial 11 with value: 0.10006428920096605.[0m
[32m[I 2025-01-11 23:29:28,266][0m Trial 13 finished with value: 0.09744222541705946 and parameters: {'observation_period_num': 196, 'train_rates': 0.8709914011686544, 'learning_rate': 0.0008662669790507758, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8223237185137439}. Best is trial 13 with value: 0.09744222541705946.[0m
[32m[I 2025-01-11 23:30:08,003][0m Trial 14 finished with value: 0.3979791110973754 and parameters: {'observation_period_num': 198, 'train_rates': 0.7637344019653883, 'learning_rate': 7.60914233395383e-05, 'batch_size': 121, 'step_size': 8, 'gamma': 0.834517211734641}. Best is trial 13 with value: 0.09744222541705946.[0m
[32m[I 2025-01-11 23:30:33,425][0m Trial 15 finished with value: 0.28018245742496417 and parameters: {'observation_period_num': 191, 'train_rates': 0.7343353850790352, 'learning_rate': 0.0004523755789178634, 'batch_size': 190, 'step_size': 10, 'gamma': 0.8413755059390562}. Best is trial 13 with value: 0.09744222541705946.[0m
[32m[I 2025-01-11 23:31:17,870][0m Trial 16 finished with value: 0.2177293236948486 and parameters: {'observation_period_num': 172, 'train_rates': 0.8737453410687417, 'learning_rate': 5.748475034411376e-05, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8013314304375884}. Best is trial 13 with value: 0.09744222541705946.[0m
[32m[I 2025-01-11 23:31:49,964][0m Trial 17 finished with value: 0.08022757549330874 and parameters: {'observation_period_num': 227, 'train_rates': 0.8445820111171564, 'learning_rate': 0.0004965057039508105, 'batch_size': 164, 'step_size': 5, 'gamma': 0.8972746245467581}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:32:18,000][0m Trial 18 finished with value: 0.09702817598978679 and parameters: {'observation_period_num': 168, 'train_rates': 0.8322043814711098, 'learning_rate': 0.0003928236331646202, 'batch_size': 182, 'step_size': 10, 'gamma': 0.9022431523013488}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:32:41,356][0m Trial 19 finished with value: 0.3012990795016605 and parameters: {'observation_period_num': 224, 'train_rates': 0.7236877876618609, 'learning_rate': 0.0003236808831370278, 'batch_size': 216, 'step_size': 11, 'gamma': 0.9054824629578608}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:33:12,199][0m Trial 20 finished with value: 0.1441221833229065 and parameters: {'observation_period_num': 166, 'train_rates': 0.82210855280914, 'learning_rate': 0.00010482627636441512, 'batch_size': 172, 'step_size': 9, 'gamma': 0.9060848460008891}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:33:37,067][0m Trial 21 finished with value: 0.09753318021694819 and parameters: {'observation_period_num': 174, 'train_rates': 0.8382930717407685, 'learning_rate': 0.0004932782070757602, 'batch_size': 219, 'step_size': 15, 'gamma': 0.8589724759616119}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:34:06,524][0m Trial 22 finished with value: 0.12207412818059098 and parameters: {'observation_period_num': 219, 'train_rates': 0.7839446259393529, 'learning_rate': 0.0005012722408500972, 'batch_size': 168, 'step_size': 6, 'gamma': 0.905496471629275}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:34:51,249][0m Trial 23 finished with value: 0.08345865522431077 and parameters: {'observation_period_num': 156, 'train_rates': 0.9047029523395357, 'learning_rate': 0.0001609958887567694, 'batch_size': 121, 'step_size': 9, 'gamma': 0.88994421555538}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:35:49,367][0m Trial 24 finished with value: 0.13033080101013184 and parameters: {'observation_period_num': 91, 'train_rates': 0.9879674520495787, 'learning_rate': 0.00016413112857442197, 'batch_size': 102, 'step_size': 10, 'gamma': 0.9225624861279639}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:36:15,530][0m Trial 25 finished with value: 0.22127207992522696 and parameters: {'observation_period_num': 152, 'train_rates': 0.9122603926996058, 'learning_rate': 4.64686311987344e-05, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8893745389458053}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:37:02,874][0m Trial 26 finished with value: 0.08386540091295036 and parameters: {'observation_period_num': 108, 'train_rates': 0.853586686039248, 'learning_rate': 0.0003115300571648746, 'batch_size': 113, 'step_size': 9, 'gamma': 0.9568418757347167}. Best is trial 17 with value: 0.08022757549330874.[0m
[32m[I 2025-01-11 23:38:01,465][0m Trial 27 finished with value: 0.048175537337859474 and parameters: {'observation_period_num': 12, 'train_rates': 0.9480611303350382, 'learning_rate': 0.0001974659520866158, 'batch_size': 102, 'step_size': 9, 'gamma': 0.9464476233196398}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:39:13,104][0m Trial 28 finished with value: 0.061001645512560494 and parameters: {'observation_period_num': 17, 'train_rates': 0.9601654209407995, 'learning_rate': 9.1414290687465e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9577664917573326}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:41:40,221][0m Trial 29 finished with value: 0.7820221647620201 and parameters: {'observation_period_num': 7, 'train_rates': 0.9593885505023385, 'learning_rate': 1.4682856577466364e-06, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9453759610597084}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:43:28,788][0m Trial 30 finished with value: 0.05798014388843016 and parameters: {'observation_period_num': 6, 'train_rates': 0.9515904574482338, 'learning_rate': 3.858573804712029e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.9884478026298578}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:45:10,556][0m Trial 31 finished with value: 0.06391336382740606 and parameters: {'observation_period_num': 6, 'train_rates': 0.953755951992782, 'learning_rate': 3.443932001690021e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9885977474188431}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:46:56,655][0m Trial 32 finished with value: 0.06657252818225853 and parameters: {'observation_period_num': 11, 'train_rates': 0.9580951538721265, 'learning_rate': 2.446946808241726e-05, 'batch_size': 55, 'step_size': 7, 'gamma': 0.9885620773703294}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:48:38,025][0m Trial 33 finished with value: 0.07031384973928628 and parameters: {'observation_period_num': 30, 'train_rates': 0.9486047381314983, 'learning_rate': 3.6318574991077483e-05, 'batch_size': 56, 'step_size': 8, 'gamma': 0.9627055535610984}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:49:44,345][0m Trial 34 finished with value: 0.12883037349707643 and parameters: {'observation_period_num': 54, 'train_rates': 0.9268276409157853, 'learning_rate': 1.8387263742994572e-05, 'batch_size': 86, 'step_size': 7, 'gamma': 0.9839907247869163}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:51:01,363][0m Trial 35 finished with value: 0.20353819318642255 and parameters: {'observation_period_num': 21, 'train_rates': 0.6139905174297108, 'learning_rate': 7.45918372479039e-05, 'batch_size': 55, 'step_size': 8, 'gamma': 0.9252829451861397}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:55:56,401][0m Trial 36 finished with value: 0.12340931796733244 and parameters: {'observation_period_num': 71, 'train_rates': 0.9719935503823445, 'learning_rate': 1.0633091748563894e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9662587997443063}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-11 23:58:35,084][0m Trial 37 finished with value: 0.06172167399415263 and parameters: {'observation_period_num': 46, 'train_rates': 0.9353025189771094, 'learning_rate': 8.437258157268806e-05, 'batch_size': 35, 'step_size': 2, 'gamma': 0.9473870514823033}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:01:17,273][0m Trial 38 finished with value: 0.062242694650635574 and parameters: {'observation_period_num': 35, 'train_rates': 0.9322664959896109, 'learning_rate': 0.00010085756929260951, 'batch_size': 34, 'step_size': 1, 'gamma': 0.9525586325516452}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:02:22,565][0m Trial 39 finished with value: 0.07266886672857917 and parameters: {'observation_period_num': 51, 'train_rates': 0.9059879752230404, 'learning_rate': 0.00021041282143574472, 'batch_size': 85, 'step_size': 2, 'gamma': 0.9286417449907458}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:04:59,239][0m Trial 40 finished with value: 0.10102120786905289 and parameters: {'observation_period_num': 80, 'train_rates': 0.9708287432132563, 'learning_rate': 5.2813431854315286e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9702492484489605}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:08:04,796][0m Trial 41 finished with value: 0.05984374203124837 and parameters: {'observation_period_num': 36, 'train_rates': 0.9318834033979921, 'learning_rate': 0.00010872736099059818, 'batch_size': 30, 'step_size': 1, 'gamma': 0.9509006914316098}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:09:26,298][0m Trial 42 finished with value: 0.07568418215960264 and parameters: {'observation_period_num': 26, 'train_rates': 0.9388127478085192, 'learning_rate': 0.0001123085298708275, 'batch_size': 69, 'step_size': 1, 'gamma': 0.9460061907642437}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:11:34,860][0m Trial 43 finished with value: 0.07278582585495633 and parameters: {'observation_period_num': 45, 'train_rates': 0.921477409153927, 'learning_rate': 7.739754108929142e-05, 'batch_size': 43, 'step_size': 2, 'gamma': 0.9365501143872723}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:15:29,931][0m Trial 44 finished with value: 0.05237221159040928 and parameters: {'observation_period_num': 19, 'train_rates': 0.9898642836264503, 'learning_rate': 0.00013869658366175784, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9729458871704155}. Best is trial 27 with value: 0.048175537337859474.[0m
[32m[I 2025-01-12 00:20:32,968][0m Trial 45 finished with value: 0.035124908002917885 and parameters: {'observation_period_num': 19, 'train_rates': 0.9794401577852343, 'learning_rate': 0.00015240867527994295, 'batch_size': 19, 'step_size': 3, 'gamma': 0.9768263991282031}. Best is trial 45 with value: 0.035124908002917885.[0m
[32m[I 2025-01-12 00:25:51,792][0m Trial 46 finished with value: 0.028947549800460156 and parameters: {'observation_period_num': 22, 'train_rates': 0.9824810249199024, 'learning_rate': 0.00017786620380025816, 'batch_size': 18, 'step_size': 3, 'gamma': 0.9772529196605334}. Best is trial 46 with value: 0.028947549800460156.[0m
[32m[I 2025-01-12 00:30:36,357][0m Trial 47 finished with value: 0.09506496129667058 and parameters: {'observation_period_num': 67, 'train_rates': 0.988316170275717, 'learning_rate': 0.00017269259984862146, 'batch_size': 20, 'step_size': 3, 'gamma': 0.9775733700926122}. Best is trial 46 with value: 0.028947549800460156.[0m
[32m[I 2025-01-12 00:32:39,477][0m Trial 48 finished with value: 0.03439968043110437 and parameters: {'observation_period_num': 19, 'train_rates': 0.9755001350671089, 'learning_rate': 0.00024004565718237914, 'batch_size': 47, 'step_size': 4, 'gamma': 0.9760992506570108}. Best is trial 46 with value: 0.028947549800460156.[0m
[32m[I 2025-01-12 00:34:07,556][0m Trial 49 finished with value: 0.05366122783983455 and parameters: {'observation_period_num': 21, 'train_rates': 0.9711071903707068, 'learning_rate': 0.00020241969793785143, 'batch_size': 67, 'step_size': 3, 'gamma': 0.9738908051649272}. Best is trial 46 with value: 0.028947549800460156.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-12 00:34:07,567][0m A new study created in memory with name: no-name-bd143cf8-aea3-4164-a0b3-170a13dcd18f[0m
[32m[I 2025-01-12 00:34:44,436][0m Trial 0 finished with value: 0.2187741775947339 and parameters: {'observation_period_num': 203, 'train_rates': 0.829510394528771, 'learning_rate': 5.494694377961849e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.8110663268288204}. Best is trial 0 with value: 0.2187741775947339.[0m
Early stopping at epoch 50
[32m[I 2025-01-12 00:34:55,877][0m Trial 1 finished with value: 0.47127353432309865 and parameters: {'observation_period_num': 50, 'train_rates': 0.7063466554235982, 'learning_rate': 0.00036816281031012586, 'batch_size': 229, 'step_size': 1, 'gamma': 0.7695328648683704}. Best is trial 0 with value: 0.2187741775947339.[0m
[32m[I 2025-01-12 00:35:36,129][0m Trial 2 finished with value: 0.25235560867741963 and parameters: {'observation_period_num': 131, 'train_rates': 0.7340600526777792, 'learning_rate': 0.00014251778319790485, 'batch_size': 118, 'step_size': 13, 'gamma': 0.9670375272715246}. Best is trial 0 with value: 0.2187741775947339.[0m
[32m[I 2025-01-12 00:39:24,552][0m Trial 3 finished with value: 0.5372342389042133 and parameters: {'observation_period_num': 54, 'train_rates': 0.6311245755637003, 'learning_rate': 1.0590427415953948e-06, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9591944819845781}. Best is trial 0 with value: 0.2187741775947339.[0m
[32m[I 2025-01-12 00:39:45,913][0m Trial 4 finished with value: 0.31815679714774453 and parameters: {'observation_period_num': 248, 'train_rates': 0.720955516978476, 'learning_rate': 0.0002402339009380962, 'batch_size': 241, 'step_size': 11, 'gamma': 0.812296551122214}. Best is trial 0 with value: 0.2187741775947339.[0m
[32m[I 2025-01-12 00:40:20,054][0m Trial 5 finished with value: 0.10409979112446308 and parameters: {'observation_period_num': 56, 'train_rates': 0.8895458416879156, 'learning_rate': 5.6331136423711825e-05, 'batch_size': 168, 'step_size': 10, 'gamma': 0.9126876422930345}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:43:07,847][0m Trial 6 finished with value: 1.7723320112831291 and parameters: {'observation_period_num': 60, 'train_rates': 0.7596264054414773, 'learning_rate': 1.7938499693028615e-06, 'batch_size': 28, 'step_size': 4, 'gamma': 0.7506278748709972}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:43:35,042][0m Trial 7 finished with value: 0.1999859087094912 and parameters: {'observation_period_num': 146, 'train_rates': 0.7857202377769698, 'learning_rate': 8.326668354458038e-05, 'batch_size': 192, 'step_size': 10, 'gamma': 0.8185960759818965}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:44:56,996][0m Trial 8 finished with value: 0.27523069844601 and parameters: {'observation_period_num': 198, 'train_rates': 0.7442094616410992, 'learning_rate': 5.1024078639686896e-05, 'batch_size': 54, 'step_size': 13, 'gamma': 0.9058601772349557}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:45:31,347][0m Trial 9 finished with value: 0.4596876431319673 and parameters: {'observation_period_num': 65, 'train_rates': 0.9183387546301452, 'learning_rate': 7.12817995422997e-06, 'batch_size': 176, 'step_size': 12, 'gamma': 0.9836253226581152}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:46:43,023][0m Trial 10 finished with value: 0.14500591561600967 and parameters: {'observation_period_num': 11, 'train_rates': 0.9625368345991846, 'learning_rate': 1.2884722755686021e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9037368714940357}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:47:47,231][0m Trial 11 finished with value: 0.1512042135000229 and parameters: {'observation_period_num': 17, 'train_rates': 0.9762846945847948, 'learning_rate': 1.354983837596691e-05, 'batch_size': 93, 'step_size': 7, 'gamma': 0.9006121137590146}. Best is trial 5 with value: 0.10409979112446308.[0m
[32m[I 2025-01-12 00:48:58,505][0m Trial 12 finished with value: 0.02953826442114578 and parameters: {'observation_period_num': 9, 'train_rates': 0.8769136556459388, 'learning_rate': 0.0009779259533058064, 'batch_size': 78, 'step_size': 5, 'gamma': 0.9135063776364785}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:49:31,316][0m Trial 13 finished with value: 0.08809441700577736 and parameters: {'observation_period_num': 90, 'train_rates': 0.8631278406189786, 'learning_rate': 0.0007772921241124589, 'batch_size': 168, 'step_size': 15, 'gamma': 0.8643254941325492}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:50:26,304][0m Trial 14 finished with value: 0.07731642366024645 and parameters: {'observation_period_num': 101, 'train_rates': 0.8532032891821275, 'learning_rate': 0.0008649273247006312, 'batch_size': 97, 'step_size': 15, 'gamma': 0.8567998873584698}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:51:32,043][0m Trial 15 finished with value: 0.06847342357039451 and parameters: {'observation_period_num': 101, 'train_rates': 0.8247959900015254, 'learning_rate': 0.0008910392595129122, 'batch_size': 76, 'step_size': 4, 'gamma': 0.8598359114360034}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:52:53,395][0m Trial 16 finished with value: 0.06998453813116831 and parameters: {'observation_period_num': 152, 'train_rates': 0.815474129093019, 'learning_rate': 0.0004117805376706257, 'batch_size': 61, 'step_size': 3, 'gamma': 0.9438474100719701}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:54:30,434][0m Trial 17 finished with value: 0.08005539306572505 and parameters: {'observation_period_num': 104, 'train_rates': 0.914014007846689, 'learning_rate': 0.00019433957314112714, 'batch_size': 56, 'step_size': 4, 'gamma': 0.8380573374621888}. Best is trial 12 with value: 0.02953826442114578.[0m
Early stopping at epoch 99
[32m[I 2025-01-12 00:55:07,747][0m Trial 18 finished with value: 0.31240936265937214 and parameters: {'observation_period_num': 182, 'train_rates': 0.6650066676700936, 'learning_rate': 0.0009553752458611802, 'batch_size': 120, 'step_size': 1, 'gamma': 0.8779911600512021}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:56:25,785][0m Trial 19 finished with value: 0.048119382527859314 and parameters: {'observation_period_num': 34, 'train_rates': 0.9291935384813014, 'learning_rate': 0.0004353923382169062, 'batch_size': 72, 'step_size': 5, 'gamma': 0.9350891957938852}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 00:59:14,406][0m Trial 20 finished with value: 0.04128761657991925 and parameters: {'observation_period_num': 28, 'train_rates': 0.9369120463651502, 'learning_rate': 0.00033831320210329186, 'batch_size': 33, 'step_size': 6, 'gamma': 0.9348032289399522}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 01:01:37,661][0m Trial 21 finished with value: 0.03934230830739526 and parameters: {'observation_period_num': 28, 'train_rates': 0.9419618269866711, 'learning_rate': 0.00036717808155296004, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9335314525500682}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 01:04:10,396][0m Trial 22 finished with value: 0.045666589238843716 and parameters: {'observation_period_num': 29, 'train_rates': 0.951087629284416, 'learning_rate': 0.00015133946899738185, 'batch_size': 37, 'step_size': 8, 'gamma': 0.9336297286607256}. Best is trial 12 with value: 0.02953826442114578.[0m
[32m[I 2025-01-12 01:06:20,763][0m Trial 23 finished with value: 0.027685364708304405 and parameters: {'observation_period_num': 11, 'train_rates': 0.9861019604832479, 'learning_rate': 0.0004198397542093248, 'batch_size': 45, 'step_size': 6, 'gamma': 0.9890952663386134}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:08:15,682][0m Trial 24 finished with value: 0.028514035046100616 and parameters: {'observation_period_num': 6, 'train_rates': 0.9837448893710856, 'learning_rate': 0.0005208342622002663, 'batch_size': 51, 'step_size': 2, 'gamma': 0.9837209133271981}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:09:13,566][0m Trial 25 finished with value: 0.03736311197280884 and parameters: {'observation_period_num': 7, 'train_rates': 0.9735242230796237, 'learning_rate': 0.0005316925580657094, 'batch_size': 103, 'step_size': 2, 'gamma': 0.9858829506170037}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:14:34,495][0m Trial 26 finished with value: 0.05595503805461758 and parameters: {'observation_period_num': 73, 'train_rates': 0.8841044534004181, 'learning_rate': 0.00010375036002418553, 'batch_size': 16, 'step_size': 3, 'gamma': 0.967323996169966}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:16:20,898][0m Trial 27 finished with value: 0.0871623083949089 and parameters: {'observation_period_num': 39, 'train_rates': 0.988339106854037, 'learning_rate': 2.6827265818689847e-05, 'batch_size': 55, 'step_size': 2, 'gamma': 0.989574683980691}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:17:01,731][0m Trial 28 finished with value: 0.04453693853904588 and parameters: {'observation_period_num': 6, 'train_rates': 0.8904811335337772, 'learning_rate': 0.00023196209915621918, 'batch_size': 144, 'step_size': 5, 'gamma': 0.9572813697372008}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:17:40,771][0m Trial 29 finished with value: 0.05726199487603649 and parameters: {'observation_period_num': 80, 'train_rates': 0.8496563817169924, 'learning_rate': 0.0005797683041526086, 'batch_size': 139, 'step_size': 8, 'gamma': 0.8836511241735239}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:18:22,427][0m Trial 30 finished with value: 0.9577000450283342 and parameters: {'observation_period_num': 244, 'train_rates': 0.7922933574293821, 'learning_rate': 3.2226797298775644e-06, 'batch_size': 114, 'step_size': 3, 'gamma': 0.9190226355275906}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:19:24,350][0m Trial 31 finished with value: 0.04586677998304367 and parameters: {'observation_period_num': 5, 'train_rates': 0.9894742371509374, 'learning_rate': 0.0005500820961845786, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9788653217784671}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:20:45,871][0m Trial 32 finished with value: 0.05478173405478854 and parameters: {'observation_period_num': 44, 'train_rates': 0.9626562710112085, 'learning_rate': 0.0006093496825208991, 'batch_size': 71, 'step_size': 1, 'gamma': 0.9756191946962159}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:21:48,086][0m Trial 33 finished with value: 0.049365208982541556 and parameters: {'observation_period_num': 23, 'train_rates': 0.9078004514655964, 'learning_rate': 0.0002852258804803962, 'batch_size': 91, 'step_size': 2, 'gamma': 0.9512858292920823}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:22:43,247][0m Trial 34 finished with value: 0.057389456778764725 and parameters: {'observation_period_num': 6, 'train_rates': 0.9654642774776273, 'learning_rate': 0.00015656428007293743, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9705338206527142}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:24:29,860][0m Trial 35 finished with value: 0.054083845644657104 and parameters: {'observation_period_num': 45, 'train_rates': 0.9422857373959397, 'learning_rate': 0.0005495420508999492, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9897917667041654}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:26:57,499][0m Trial 36 finished with value: 0.04911647563756898 and parameters: {'observation_period_num': 20, 'train_rates': 0.9855378784262335, 'learning_rate': 0.00010694986028143387, 'batch_size': 40, 'step_size': 3, 'gamma': 0.9613635087646868}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:27:41,413][0m Trial 37 finished with value: 0.05787364033326878 and parameters: {'observation_period_num': 50, 'train_rates': 0.8756532065229712, 'learning_rate': 0.00022424951090132227, 'batch_size': 125, 'step_size': 7, 'gamma': 0.9504493799234892}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:28:50,519][0m Trial 38 finished with value: 0.04177581496647934 and parameters: {'observation_period_num': 17, 'train_rates': 0.9089023507485181, 'learning_rate': 0.00030525490220930296, 'batch_size': 82, 'step_size': 4, 'gamma': 0.9733196196768567}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:29:14,625][0m Trial 39 finished with value: 0.11112191528081894 and parameters: {'observation_period_num': 38, 'train_rates': 0.9567499677057678, 'learning_rate': 5.920315337477754e-05, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9206920652305742}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:29:39,578][0m Trial 40 finished with value: 0.25487278434965344 and parameters: {'observation_period_num': 63, 'train_rates': 0.7664614088529454, 'learning_rate': 0.0006416945869728339, 'batch_size': 210, 'step_size': 5, 'gamma': 0.7913946010839742}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:31:47,769][0m Trial 41 finished with value: 0.039295261919172736 and parameters: {'observation_period_num': 25, 'train_rates': 0.9377575596638932, 'learning_rate': 0.0004111051724927009, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9429664438465627}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:33:17,353][0m Trial 42 finished with value: 0.04523760979955501 and parameters: {'observation_period_num': 18, 'train_rates': 0.9682161681356322, 'learning_rate': 0.00045687013075908065, 'batch_size': 65, 'step_size': 9, 'gamma': 0.9629660659243372}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:36:51,203][0m Trial 43 finished with value: 0.028180024511105305 and parameters: {'observation_period_num': 7, 'train_rates': 0.9249558423893945, 'learning_rate': 0.0009516348998454614, 'batch_size': 26, 'step_size': 6, 'gamma': 0.8951450430013286}. Best is trial 23 with value: 0.027685364708304405.[0m
[32m[I 2025-01-12 01:40:10,676][0m Trial 44 finished with value: 0.02502541670477704 and parameters: {'observation_period_num': 6, 'train_rates': 0.9227579348531737, 'learning_rate': 0.0007033093239394565, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8946151854206078}. Best is trial 44 with value: 0.02502541670477704.[0m
[32m[I 2025-01-12 01:43:49,744][0m Trial 45 finished with value: 0.05859814786186842 and parameters: {'observation_period_num': 55, 'train_rates': 0.9261879670775609, 'learning_rate': 0.0009656510125529395, 'batch_size': 25, 'step_size': 7, 'gamma': 0.8940093874573417}. Best is trial 44 with value: 0.02502541670477704.[0m
[32m[I 2025-01-12 01:47:35,343][0m Trial 46 finished with value: 0.042798967445141646 and parameters: {'observation_period_num': 36, 'train_rates': 0.9001725924800364, 'learning_rate': 0.0007209257773087546, 'batch_size': 24, 'step_size': 6, 'gamma': 0.8892341384850541}. Best is trial 44 with value: 0.02502541670477704.[0m
[32m[I 2025-01-12 01:49:07,656][0m Trial 47 finished with value: 0.12930514133660648 and parameters: {'observation_period_num': 15, 'train_rates': 0.6129817060336672, 'learning_rate': 0.000744468658100224, 'batch_size': 46, 'step_size': 8, 'gamma': 0.846201492866625}. Best is trial 44 with value: 0.02502541670477704.[0m
[32m[I 2025-01-12 01:52:19,114][0m Trial 48 finished with value: 0.07959196192246897 and parameters: {'observation_period_num': 117, 'train_rates': 0.8382524377829081, 'learning_rate': 3.382797116926937e-05, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8684479279990813}. Best is trial 44 with value: 0.02502541670477704.[0m
[32m[I 2025-01-12 01:53:47,721][0m Trial 49 finished with value: 0.22064869766282202 and parameters: {'observation_period_num': 71, 'train_rates': 0.6823642883899053, 'learning_rate': 0.0009722365273848963, 'batch_size': 50, 'step_size': 8, 'gamma': 0.913346425250112}. Best is trial 44 with value: 0.02502541670477704.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-12 01:53:47,731][0m A new study created in memory with name: no-name-dc5f2ff3-ed0b-4c5f-a6c1-9b57c40c4f9a[0m
[32m[I 2025-01-12 01:55:35,492][0m Trial 0 finished with value: 0.1523789850535331 and parameters: {'observation_period_num': 185, 'train_rates': 0.8611183321095771, 'learning_rate': 2.7504604264800164e-05, 'batch_size': 46, 'step_size': 10, 'gamma': 0.7785018847311553}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 01:57:26,050][0m Trial 1 finished with value: 0.2146528830922587 and parameters: {'observation_period_num': 115, 'train_rates': 0.6772071063284736, 'learning_rate': 0.00044385276189143005, 'batch_size': 39, 'step_size': 6, 'gamma': 0.97769690609882}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 01:57:49,229][0m Trial 2 finished with value: 1.3810247629654788 and parameters: {'observation_period_num': 156, 'train_rates': 0.7644887046251203, 'learning_rate': 1.2499624093476772e-06, 'batch_size': 219, 'step_size': 9, 'gamma': 0.7639283517669602}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 01:58:25,211][0m Trial 3 finished with value: 0.7672725795088587 and parameters: {'observation_period_num': 237, 'train_rates': 0.7275443632980758, 'learning_rate': 1.987790234695611e-05, 'batch_size': 125, 'step_size': 4, 'gamma': 0.7622644141896748}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 01:58:52,602][0m Trial 4 finished with value: 0.2648851748566334 and parameters: {'observation_period_num': 236, 'train_rates': 0.7371842786996503, 'learning_rate': 0.00040967834552969683, 'batch_size': 178, 'step_size': 6, 'gamma': 0.8757978588159363}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 02:00:12,278][0m Trial 5 finished with value: 0.34124679760686283 and parameters: {'observation_period_num': 28, 'train_rates': 0.7124409927928879, 'learning_rate': 7.057837855727813e-06, 'batch_size': 58, 'step_size': 11, 'gamma': 0.9531173187706984}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 02:00:52,198][0m Trial 6 finished with value: 0.6951367256291415 and parameters: {'observation_period_num': 40, 'train_rates': 0.7757115223667033, 'learning_rate': 6.138531995919208e-06, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8730691057373439}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 02:02:31,135][0m Trial 7 finished with value: 0.8240189340339243 and parameters: {'observation_period_num': 110, 'train_rates': 0.7685488277102898, 'learning_rate': 1.3761796982878249e-06, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9813816459043847}. Best is trial 0 with value: 0.1523789850535331.[0m
[32m[I 2025-01-12 02:05:19,396][0m Trial 8 finished with value: 0.08608838053126085 and parameters: {'observation_period_num': 220, 'train_rates': 0.9864375570102171, 'learning_rate': 0.0004936378523491468, 'batch_size': 32, 'step_size': 11, 'gamma': 0.9308125949522598}. Best is trial 8 with value: 0.08608838053126085.[0m
[32m[I 2025-01-12 02:05:56,617][0m Trial 9 finished with value: 0.07407104367246994 and parameters: {'observation_period_num': 118, 'train_rates': 0.8074122068203067, 'learning_rate': 0.0009814679841881934, 'batch_size': 141, 'step_size': 5, 'gamma': 0.779497807157749}. Best is trial 9 with value: 0.07407104367246994.[0m
[32m[I 2025-01-12 02:06:23,357][0m Trial 10 finished with value: 0.12018607722388373 and parameters: {'observation_period_num': 73, 'train_rates': 0.9094977237017162, 'learning_rate': 8.516913941309145e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.8173039342165749}. Best is trial 9 with value: 0.07407104367246994.[0m
[32m[I 2025-01-12 02:07:03,802][0m Trial 11 finished with value: 0.30762300324526937 and parameters: {'observation_period_num': 183, 'train_rates': 0.6040012177264008, 'learning_rate': 0.0009530005713647514, 'batch_size': 100, 'step_size': 1, 'gamma': 0.9182817037427389}. Best is trial 9 with value: 0.07407104367246994.[0m
[32m[I 2025-01-12 02:07:40,812][0m Trial 12 finished with value: 0.18340003490447998 and parameters: {'observation_period_num': 82, 'train_rates': 0.9814264337067169, 'learning_rate': 7.720578113918428e-05, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8286383150089062}. Best is trial 9 with value: 0.07407104367246994.[0m
[32m[I 2025-01-12 02:08:38,686][0m Trial 13 finished with value: 0.07675006753569531 and parameters: {'observation_period_num': 165, 'train_rates': 0.8411084639257355, 'learning_rate': 0.00016734436263144407, 'batch_size': 87, 'step_size': 12, 'gamma': 0.9180158012763929}. Best is trial 9 with value: 0.07407104367246994.[0m
[32m[I 2025-01-12 02:09:35,766][0m Trial 14 finished with value: 0.08119532523991226 and parameters: {'observation_period_num': 142, 'train_rates': 0.8356523090440342, 'learning_rate': 0.00011329701413490628, 'batch_size': 89, 'step_size': 15, 'gamma': 0.9044046970008169}. Best is trial 9 with value: 0.07407104367246994.[0m
Early stopping at epoch 67
[32m[I 2025-01-12 02:09:57,583][0m Trial 15 finished with value: 0.39152800492436324 and parameters: {'observation_period_num': 176, 'train_rates': 0.8463867977120599, 'learning_rate': 0.0001780133753485393, 'batch_size': 174, 'step_size': 1, 'gamma': 0.827914455308594}. Best is trial 9 with value: 0.07407104367246994.[0m
[32m[I 2025-01-12 02:11:04,495][0m Trial 16 finished with value: 0.05741785691019328 and parameters: {'observation_period_num': 88, 'train_rates': 0.9060851144753266, 'learning_rate': 0.0009779052906471906, 'batch_size': 82, 'step_size': 7, 'gamma': 0.7985379931556962}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:11:32,647][0m Trial 17 finished with value: 0.06248662913667745 and parameters: {'observation_period_num': 79, 'train_rates': 0.8994069865873251, 'learning_rate': 0.0009651380634453201, 'batch_size': 207, 'step_size': 7, 'gamma': 0.7994336012227646}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:12:02,788][0m Trial 18 finished with value: 0.0647209879364176 and parameters: {'observation_period_num': 5, 'train_rates': 0.9212782841580368, 'learning_rate': 0.00021732700034416837, 'batch_size': 207, 'step_size': 7, 'gamma': 0.8056567767512913}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:12:26,821][0m Trial 19 finished with value: 0.4548623561859131 and parameters: {'observation_period_num': 79, 'train_rates': 0.9244491553866608, 'learning_rate': 4.23832963457701e-05, 'batch_size': 254, 'step_size': 3, 'gamma': 0.84259207741786}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:12:57,426][0m Trial 20 finished with value: 0.06040541866892263 and parameters: {'observation_period_num': 49, 'train_rates': 0.8954898910925229, 'learning_rate': 0.0003581937600798761, 'batch_size': 196, 'step_size': 8, 'gamma': 0.7951797917317812}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:13:27,676][0m Trial 21 finished with value: 0.06213440713011771 and parameters: {'observation_period_num': 61, 'train_rates': 0.8987022719484059, 'learning_rate': 0.0005876881342396819, 'batch_size': 194, 'step_size': 8, 'gamma': 0.798493594159364}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:13:59,781][0m Trial 22 finished with value: 0.0706518292427063 and parameters: {'observation_period_num': 51, 'train_rates': 0.9519718207704311, 'learning_rate': 0.0003718208441286554, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8555605429035456}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:14:36,251][0m Trial 23 finished with value: 0.06713760504101077 and parameters: {'observation_period_num': 59, 'train_rates': 0.8789890722461777, 'learning_rate': 0.0003012467739305851, 'batch_size': 151, 'step_size': 8, 'gamma': 0.7941184857220536}. Best is trial 16 with value: 0.05741785691019328.[0m
[32m[I 2025-01-12 02:15:29,905][0m Trial 24 finished with value: 0.04109345306339976 and parameters: {'observation_period_num': 9, 'train_rates': 0.9479810737461036, 'learning_rate': 0.0006149502856861771, 'batch_size': 111, 'step_size': 9, 'gamma': 0.7507423197180648}. Best is trial 24 with value: 0.04109345306339976.[0m
[32m[I 2025-01-12 02:16:26,245][0m Trial 25 finished with value: 0.03811240790190397 and parameters: {'observation_period_num': 13, 'train_rates': 0.9516398396919914, 'learning_rate': 0.0005968883582295454, 'batch_size': 106, 'step_size': 10, 'gamma': 0.7514445611004563}. Best is trial 25 with value: 0.03811240790190397.[0m
[32m[I 2025-01-12 02:17:43,025][0m Trial 26 finished with value: 0.03730200362247481 and parameters: {'observation_period_num': 9, 'train_rates': 0.9520001091286616, 'learning_rate': 0.0007220778356011528, 'batch_size': 76, 'step_size': 10, 'gamma': 0.7547204205870902}. Best is trial 26 with value: 0.03730200362247481.[0m
[32m[I 2025-01-12 02:18:36,994][0m Trial 27 finished with value: 0.07257544489936059 and parameters: {'observation_period_num': 12, 'train_rates': 0.9454636911900449, 'learning_rate': 4.9142794054654926e-05, 'batch_size': 110, 'step_size': 13, 'gamma': 0.7565400200080815}. Best is trial 26 with value: 0.03730200362247481.[0m
[32m[I 2025-01-12 02:20:02,434][0m Trial 28 finished with value: 0.05371021723840386 and parameters: {'observation_period_num': 26, 'train_rates': 0.9563906255677265, 'learning_rate': 0.00024255940075246154, 'batch_size': 67, 'step_size': 10, 'gamma': 0.7714489449273285}. Best is trial 26 with value: 0.03730200362247481.[0m
[32m[I 2025-01-12 02:25:36,868][0m Trial 29 finished with value: 0.06487290374934673 and parameters: {'observation_period_num': 24, 'train_rates': 0.9652654597520546, 'learning_rate': 2.1348293338562363e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7820873297715466}. Best is trial 26 with value: 0.03730200362247481.[0m
[32m[I 2025-01-12 02:26:31,152][0m Trial 30 finished with value: 0.03553369483925442 and parameters: {'observation_period_num': 7, 'train_rates': 0.9366141356568191, 'learning_rate': 0.0006210966269858409, 'batch_size': 109, 'step_size': 13, 'gamma': 0.7530656317218757}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:27:22,992][0m Trial 31 finished with value: 0.03565475485439217 and parameters: {'observation_period_num': 6, 'train_rates': 0.9417207451413262, 'learning_rate': 0.0005677852912456312, 'batch_size': 115, 'step_size': 13, 'gamma': 0.7512028808387318}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:28:39,371][0m Trial 32 finished with value: 0.052697941856948954 and parameters: {'observation_period_num': 37, 'train_rates': 0.9350153495950706, 'learning_rate': 0.0005817074886418599, 'batch_size': 74, 'step_size': 14, 'gamma': 0.7742359506056368}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:29:31,541][0m Trial 33 finished with value: 0.05583989135296651 and parameters: {'observation_period_num': 24, 'train_rates': 0.8780053120527396, 'learning_rate': 0.0001325165618208352, 'batch_size': 105, 'step_size': 12, 'gamma': 0.754967492997126}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:30:23,126][0m Trial 34 finished with value: 0.04630256071686745 and parameters: {'observation_period_num': 21, 'train_rates': 0.9884338478257129, 'learning_rate': 0.0006485757588099384, 'batch_size': 117, 'step_size': 13, 'gamma': 0.7656176670742217}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:31:07,346][0m Trial 35 finished with value: 0.06560184806585312 and parameters: {'observation_period_num': 38, 'train_rates': 0.9696843789863098, 'learning_rate': 0.0003025432719170946, 'batch_size': 136, 'step_size': 11, 'gamma': 0.7501089675203714}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:32:35,644][0m Trial 36 finished with value: 0.08174214297500641 and parameters: {'observation_period_num': 6, 'train_rates': 0.87530697287632, 'learning_rate': 1.0845451470350266e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.7838667924668576}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:33:34,473][0m Trial 37 finished with value: 1.5187730300717237 and parameters: {'observation_period_num': 40, 'train_rates': 0.929832774686085, 'learning_rate': 2.8556517731059294e-06, 'batch_size': 98, 'step_size': 12, 'gamma': 0.7654071174524333}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:34:17,757][0m Trial 38 finished with value: 0.08138360702372216 and parameters: {'observation_period_num': 206, 'train_rates': 0.859843666612638, 'learning_rate': 0.0004390379786787855, 'batch_size': 120, 'step_size': 14, 'gamma': 0.7843158787679521}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:34:55,089][0m Trial 39 finished with value: 0.09609401670895217 and parameters: {'observation_period_num': 100, 'train_rates': 0.8088757274924785, 'learning_rate': 0.00024561866612842013, 'batch_size': 137, 'step_size': 11, 'gamma': 0.7682074716819816}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:35:27,197][0m Trial 40 finished with value: 0.1553095282980572 and parameters: {'observation_period_num': 17, 'train_rates': 0.6924112312025813, 'learning_rate': 0.0006278414066050375, 'batch_size': 153, 'step_size': 12, 'gamma': 0.7621381565767835}. Best is trial 30 with value: 0.03553369483925442.[0m
[32m[I 2025-01-12 02:36:31,039][0m Trial 41 finished with value: 0.03383124495546023 and parameters: {'observation_period_num': 10, 'train_rates': 0.9451887035983674, 'learning_rate': 0.0007026848088906771, 'batch_size': 92, 'step_size': 10, 'gamma': 0.7532484771312513}. Best is trial 41 with value: 0.03383124495546023.[0m
[32m[I 2025-01-12 02:38:30,498][0m Trial 42 finished with value: 0.041408484311480274 and parameters: {'observation_period_num': 31, 'train_rates': 0.9677721595800899, 'learning_rate': 0.0007460609294837027, 'batch_size': 48, 'step_size': 10, 'gamma': 0.751554650803745}. Best is trial 41 with value: 0.03383124495546023.[0m
[32m[I 2025-01-12 02:39:44,854][0m Trial 43 finished with value: 0.03464174518982569 and parameters: {'observation_period_num': 5, 'train_rates': 0.9330384057156756, 'learning_rate': 0.00043190117070644155, 'batch_size': 77, 'step_size': 10, 'gamma': 0.8126131620029953}. Best is trial 41 with value: 0.03383124495546023.[0m
[32m[I 2025-01-12 02:41:04,085][0m Trial 44 finished with value: 0.053415621020670594 and parameters: {'observation_period_num': 50, 'train_rates': 0.9203704922273684, 'learning_rate': 0.0003780474303358417, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8152430475219485}. Best is trial 41 with value: 0.03383124495546023.[0m
[32m[I 2025-01-12 02:41:50,883][0m Trial 45 finished with value: 0.16796669701850692 and parameters: {'observation_period_num': 66, 'train_rates': 0.642576819521136, 'learning_rate': 0.0004702480505502398, 'batch_size': 93, 'step_size': 11, 'gamma': 0.8881735906009265}. Best is trial 41 with value: 0.03383124495546023.[0m
[32m[I 2025-01-12 02:43:03,703][0m Trial 46 finished with value: 0.02943262840444977 and parameters: {'observation_period_num': 5, 'train_rates': 0.9372368203203963, 'learning_rate': 0.000761201312494682, 'batch_size': 80, 'step_size': 11, 'gamma': 0.9627776108089214}. Best is trial 46 with value: 0.02943262840444977.[0m
[32m[I 2025-01-12 02:45:15,057][0m Trial 47 finished with value: 0.18620005553519284 and parameters: {'observation_period_num': 33, 'train_rates': 0.750369919532071, 'learning_rate': 0.00018837780116599446, 'batch_size': 36, 'step_size': 13, 'gamma': 0.946030862667084}. Best is trial 46 with value: 0.02943262840444977.[0m
[32m[I 2025-01-12 02:46:51,496][0m Trial 48 finished with value: 0.046728317234954686 and parameters: {'observation_period_num': 18, 'train_rates': 0.9344048632733977, 'learning_rate': 8.3845753390527e-05, 'batch_size': 59, 'step_size': 12, 'gamma': 0.9628578793195322}. Best is trial 46 with value: 0.02943262840444977.[0m
[32m[I 2025-01-12 02:47:36,911][0m Trial 49 finished with value: 0.10650976747274399 and parameters: {'observation_period_num': 135, 'train_rates': 0.9778046557370826, 'learning_rate': 0.000125174476825494, 'batch_size': 128, 'step_size': 11, 'gamma': 0.8492151739564121}. Best is trial 46 with value: 0.02943262840444977.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-12 02:47:36,921][0m A new study created in memory with name: no-name-40bf7007-7bba-4942-b63a-ebe5f5e7a11b[0m
[32m[I 2025-01-12 02:49:36,440][0m Trial 0 finished with value: 0.5456072157248855 and parameters: {'observation_period_num': 234, 'train_rates': 0.882513175454241, 'learning_rate': 1.8796954104406442e-06, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8368649049590373}. Best is trial 0 with value: 0.5456072157248855.[0m
[32m[I 2025-01-12 02:50:09,337][0m Trial 1 finished with value: 0.23533269184306987 and parameters: {'observation_period_num': 101, 'train_rates': 0.7834929903096011, 'learning_rate': 6.309991470663829e-05, 'batch_size': 153, 'step_size': 6, 'gamma': 0.8660818694153346}. Best is trial 1 with value: 0.23533269184306987.[0m
[32m[I 2025-01-12 02:50:30,439][0m Trial 2 finished with value: 0.806223687072806 and parameters: {'observation_period_num': 241, 'train_rates': 0.7569635970718944, 'learning_rate': 1.2188939722862343e-05, 'batch_size': 251, 'step_size': 6, 'gamma': 0.922081096698907}. Best is trial 1 with value: 0.23533269184306987.[0m
[32m[I 2025-01-12 02:50:54,733][0m Trial 3 finished with value: 0.11218502817791887 and parameters: {'observation_period_num': 89, 'train_rates': 0.9109243074785124, 'learning_rate': 0.00020354486951825926, 'batch_size': 241, 'step_size': 8, 'gamma': 0.9104614884616511}. Best is trial 3 with value: 0.11218502817791887.[0m
[32m[I 2025-01-12 02:51:24,499][0m Trial 4 finished with value: 0.7947165326068276 and parameters: {'observation_period_num': 203, 'train_rates': 0.7584468591308645, 'learning_rate': 6.717138224453868e-06, 'batch_size': 161, 'step_size': 8, 'gamma': 0.8441496587531717}. Best is trial 3 with value: 0.11218502817791887.[0m
[32m[I 2025-01-12 02:51:53,378][0m Trial 5 finished with value: 0.28700780475485155 and parameters: {'observation_period_num': 147, 'train_rates': 0.6310677617094298, 'learning_rate': 0.00043022121324283967, 'batch_size': 158, 'step_size': 4, 'gamma': 0.7633070277611465}. Best is trial 3 with value: 0.11218502817791887.[0m
[32m[I 2025-01-12 02:53:28,590][0m Trial 6 finished with value: 0.0862425001959006 and parameters: {'observation_period_num': 162, 'train_rates': 0.9355336804950922, 'learning_rate': 0.00019498255818252015, 'batch_size': 57, 'step_size': 7, 'gamma': 0.923900796829219}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 02:53:56,554][0m Trial 7 finished with value: 0.2037874062168557 and parameters: {'observation_period_num': 8, 'train_rates': 0.9020290317567045, 'learning_rate': 9.378929391757785e-06, 'batch_size': 210, 'step_size': 11, 'gamma': 0.9508334846058638}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 02:54:29,067][0m Trial 8 finished with value: 0.19480787387824264 and parameters: {'observation_period_num': 56, 'train_rates': 0.6795828694745236, 'learning_rate': 0.00032805291957869076, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8318525025545402}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 02:54:50,115][0m Trial 9 finished with value: 0.341500915348802 and parameters: {'observation_period_num': 207, 'train_rates': 0.7610178365726005, 'learning_rate': 0.00016366925489666478, 'batch_size': 246, 'step_size': 11, 'gamma': 0.807598874428801}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 02:57:26,030][0m Trial 10 finished with value: 0.10347957096316597 and parameters: {'observation_period_num': 165, 'train_rates': 0.9608666509383086, 'learning_rate': 0.0008909447794010209, 'batch_size': 35, 'step_size': 3, 'gamma': 0.9867190126986903}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 03:00:37,585][0m Trial 11 finished with value: 0.11765844198010808 and parameters: {'observation_period_num': 158, 'train_rates': 0.9503789104465704, 'learning_rate': 0.0009164147825224644, 'batch_size': 28, 'step_size': 1, 'gamma': 0.9883750888129857}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 03:01:51,534][0m Trial 12 finished with value: 0.11894700059603 and parameters: {'observation_period_num': 173, 'train_rates': 0.9687184028644076, 'learning_rate': 7.345011482718107e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.987995272620829}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 03:02:49,304][0m Trial 13 finished with value: 0.09713363891933113 and parameters: {'observation_period_num': 124, 'train_rates': 0.8529029841445772, 'learning_rate': 0.0007542765074559343, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9335035235070198}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 03:03:42,476][0m Trial 14 finished with value: 0.11791194587641833 and parameters: {'observation_period_num': 112, 'train_rates': 0.8277770450293865, 'learning_rate': 8.291630919751692e-05, 'batch_size': 95, 'step_size': 5, 'gamma': 0.9080447018462335}. Best is trial 6 with value: 0.0862425001959006.[0m
[32m[I 2025-01-12 03:04:33,393][0m Trial 15 finished with value: 0.0582444680346684 and parameters: {'observation_period_num': 66, 'train_rates': 0.847609825089513, 'learning_rate': 0.0003757398709205599, 'batch_size': 102, 'step_size': 3, 'gamma': 0.9365428993405502}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:05:20,603][0m Trial 16 finished with value: 0.12874930729158224 and parameters: {'observation_period_num': 69, 'train_rates': 0.8335110862323031, 'learning_rate': 3.115808044176678e-05, 'batch_size': 113, 'step_size': 14, 'gamma': 0.88868042067896}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:06:47,395][0m Trial 17 finished with value: 0.06184709072113037 and parameters: {'observation_period_num': 31, 'train_rates': 0.9881947115368728, 'learning_rate': 0.00015980797054630016, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9531783373133059}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:07:26,468][0m Trial 18 finished with value: 0.44608988873745814 and parameters: {'observation_period_num': 30, 'train_rates': 0.701119372242163, 'learning_rate': 2.6784287784036052e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.954856512570955}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:07:56,659][0m Trial 19 finished with value: 0.058876879889755804 and parameters: {'observation_period_num': 45, 'train_rates': 0.870998013459291, 'learning_rate': 0.00037916681921418765, 'batch_size': 193, 'step_size': 3, 'gamma': 0.9514418464713856}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:08:24,899][0m Trial 20 finished with value: 0.07830137348250497 and parameters: {'observation_period_num': 79, 'train_rates': 0.862575147797689, 'learning_rate': 0.00041079318851680883, 'batch_size': 201, 'step_size': 3, 'gamma': 0.8879374764512324}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:08:58,974][0m Trial 21 finished with value: 0.100038081407547 and parameters: {'observation_period_num': 46, 'train_rates': 0.9885244816196245, 'learning_rate': 0.00012251828042566935, 'batch_size': 184, 'step_size': 2, 'gamma': 0.9564320869343874}. Best is trial 15 with value: 0.0582444680346684.[0m
[32m[I 2025-01-12 03:10:13,832][0m Trial 22 finished with value: 0.033708070043702514 and parameters: {'observation_period_num': 6, 'train_rates': 0.8005807282376157, 'learning_rate': 0.0004020002553249557, 'batch_size': 68, 'step_size': 4, 'gamma': 0.9657770142884828}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:11:04,178][0m Trial 23 finished with value: 0.04298088162952164 and parameters: {'observation_period_num': 8, 'train_rates': 0.8087659703306521, 'learning_rate': 0.0003968400754817657, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9673241261011338}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:11:56,038][0m Trial 24 finished with value: 0.04302957830792767 and parameters: {'observation_period_num': 8, 'train_rates': 0.7999104863537073, 'learning_rate': 0.0005724931584281648, 'batch_size': 102, 'step_size': 5, 'gamma': 0.9695151541093682}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:12:35,623][0m Trial 25 finished with value: 0.06398977522017821 and parameters: {'observation_period_num': 9, 'train_rates': 0.8021811445123158, 'learning_rate': 0.0006137533582274204, 'batch_size': 132, 'step_size': 5, 'gamma': 0.9668817676683036}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:13:35,608][0m Trial 26 finished with value: 0.16303567076898876 and parameters: {'observation_period_num': 26, 'train_rates': 0.7206580532421887, 'learning_rate': 0.0002518341345296538, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9715695491223061}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:18:28,940][0m Trial 27 finished with value: 0.05915386240518344 and parameters: {'observation_period_num': 14, 'train_rates': 0.8056670394036224, 'learning_rate': 4.695851503482399e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7631220253307058}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:19:56,256][0m Trial 28 finished with value: 0.6460357322160332 and parameters: {'observation_period_num': 38, 'train_rates': 0.7327176395031796, 'learning_rate': 2.2256483462354965e-06, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9721154819893988}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:20:42,210][0m Trial 29 finished with value: 0.04354531184990743 and parameters: {'observation_period_num': 6, 'train_rates': 0.7939173595212472, 'learning_rate': 0.0005726936930951625, 'batch_size': 115, 'step_size': 14, 'gamma': 0.8941974327653344}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:22:12,421][0m Trial 30 finished with value: 0.14810097900529703 and parameters: {'observation_period_num': 23, 'train_rates': 0.6632057436882793, 'learning_rate': 0.0001056490974741929, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9385330178645345}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:22:58,280][0m Trial 31 finished with value: 0.04022743270176349 and parameters: {'observation_period_num': 11, 'train_rates': 0.8110590039883523, 'learning_rate': 0.0005549823588976582, 'batch_size': 114, 'step_size': 15, 'gamma': 0.8892125556802905}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:23:48,129][0m Trial 32 finished with value: 0.0686335247227422 and parameters: {'observation_period_num': 52, 'train_rates': 0.8229558759220406, 'learning_rate': 0.0002684347502585433, 'batch_size': 106, 'step_size': 13, 'gamma': 0.8527421385486296}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:24:47,190][0m Trial 33 finished with value: 0.1890606265630067 and parameters: {'observation_period_num': 23, 'train_rates': 0.779001309821589, 'learning_rate': 0.0005365768195411561, 'batch_size': 85, 'step_size': 7, 'gamma': 0.7929215611723404}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:25:29,520][0m Trial 34 finished with value: 0.03586052865499542 and parameters: {'observation_period_num': 5, 'train_rates': 0.8933468775778047, 'learning_rate': 0.0006281068162399501, 'batch_size': 136, 'step_size': 6, 'gamma': 0.8746907165838921}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:26:07,964][0m Trial 35 finished with value: 0.07867464331849928 and parameters: {'observation_period_num': 99, 'train_rates': 0.8915505539446222, 'learning_rate': 0.00025001950891156185, 'batch_size': 143, 'step_size': 6, 'gamma': 0.8702881216714509}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:26:36,627][0m Trial 36 finished with value: 0.36382982713543127 and parameters: {'observation_period_num': 61, 'train_rates': 0.7384900780274062, 'learning_rate': 4.328934234012197e-05, 'batch_size': 175, 'step_size': 9, 'gamma': 0.8704278886936889}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:27:20,738][0m Trial 37 finished with value: 2.089140212007344 and parameters: {'observation_period_num': 40, 'train_rates': 0.9304552916572331, 'learning_rate': 1.106519497770162e-06, 'batch_size': 129, 'step_size': 15, 'gamma': 0.8262737474874339}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:27:51,444][0m Trial 38 finished with value: 0.7533022927860897 and parameters: {'observation_period_num': 19, 'train_rates': 0.7736981644851727, 'learning_rate': 1.583081827831256e-05, 'batch_size': 171, 'step_size': 4, 'gamma': 0.909849868314568}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:29:13,751][0m Trial 39 finished with value: 0.08358997533544463 and parameters: {'observation_period_num': 80, 'train_rates': 0.8811365907734504, 'learning_rate': 0.0008222795933389719, 'batch_size': 65, 'step_size': 7, 'gamma': 0.8536808993755934}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:29:50,051][0m Trial 40 finished with value: 0.10703608719240718 and parameters: {'observation_period_num': 229, 'train_rates': 0.9128518267506037, 'learning_rate': 0.00014182946996644304, 'batch_size': 152, 'step_size': 12, 'gamma': 0.9186644192701965}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:30:37,502][0m Trial 41 finished with value: 0.03859342864536224 and parameters: {'observation_period_num': 5, 'train_rates': 0.8086573215959585, 'learning_rate': 0.0005547252367770322, 'batch_size': 112, 'step_size': 5, 'gamma': 0.9709486110301448}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:31:23,138][0m Trial 42 finished with value: 0.04121874546475499 and parameters: {'observation_period_num': 5, 'train_rates': 0.8186056271935385, 'learning_rate': 0.0003095781388283557, 'batch_size': 118, 'step_size': 8, 'gamma': 0.9286444892324659}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:32:10,069][0m Trial 43 finished with value: 0.06000280127194783 and parameters: {'observation_period_num': 34, 'train_rates': 0.846352177534037, 'learning_rate': 0.0009714705392837666, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8966461274884109}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:32:48,390][0m Trial 44 finished with value: 0.19502383526546735 and parameters: {'observation_period_num': 22, 'train_rates': 0.7594600287333977, 'learning_rate': 0.000223715618907886, 'batch_size': 136, 'step_size': 8, 'gamma': 0.878253653049106}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:33:12,645][0m Trial 45 finished with value: 0.056936751627786594 and parameters: {'observation_period_num': 18, 'train_rates': 0.8202553339018209, 'learning_rate': 0.0003136749760171904, 'batch_size': 222, 'step_size': 10, 'gamma': 0.9280827540319564}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:33:39,569][0m Trial 46 finished with value: 0.19838532866132114 and parameters: {'observation_period_num': 51, 'train_rates': 0.602368110621048, 'learning_rate': 0.0005352034239879894, 'batch_size': 163, 'step_size': 8, 'gamma': 0.9029509185828459}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:34:19,030][0m Trial 47 finished with value: 0.058185469743452574 and parameters: {'observation_period_num': 37, 'train_rates': 0.8373477295051752, 'learning_rate': 0.0006561705140328023, 'batch_size': 142, 'step_size': 6, 'gamma': 0.9420187463408946}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:34:58,275][0m Trial 48 finished with value: 0.14276691177659784 and parameters: {'observation_period_num': 188, 'train_rates': 0.7812730579640434, 'learning_rate': 0.00018156280977514743, 'batch_size': 122, 'step_size': 6, 'gamma': 0.9794374483689898}. Best is trial 22 with value: 0.033708070043702514.[0m
[32m[I 2025-01-12 03:35:55,664][0m Trial 49 finished with value: 0.5877568924084656 and parameters: {'observation_period_num': 141, 'train_rates': 0.8638513066727396, 'learning_rate': 5.848913292724565e-06, 'batch_size': 92, 'step_size': 4, 'gamma': 0.8773715682696546}. Best is trial 22 with value: 0.033708070043702514.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 16, 'train_rates': 0.8427565868591751, 'learning_rate': 0.00022184423320473115, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8750039272707331}
Epoch 1/300, trend Loss: 0.4087 | 0.2150
Epoch 2/300, trend Loss: 0.1934 | 0.1230
Epoch 3/300, trend Loss: 0.1546 | 0.0996
Epoch 4/300, trend Loss: 0.1422 | 0.0852
Epoch 5/300, trend Loss: 0.1339 | 0.0784
Epoch 6/300, trend Loss: 0.1281 | 0.0752
Epoch 7/300, trend Loss: 0.1229 | 0.0737
Epoch 8/300, trend Loss: 0.1188 | 0.0733
Epoch 9/300, trend Loss: 0.1166 | 0.0740
Epoch 10/300, trend Loss: 0.1156 | 0.0711
Epoch 11/300, trend Loss: 0.1180 | 0.0615
Epoch 12/300, trend Loss: 0.1211 | 0.0660
Epoch 13/300, trend Loss: 0.1165 | 0.0641
Epoch 14/300, trend Loss: 0.1098 | 0.0579
Epoch 15/300, trend Loss: 0.1080 | 0.0543
Epoch 16/300, trend Loss: 0.1057 | 0.0521
Epoch 17/300, trend Loss: 0.1034 | 0.0508
Epoch 18/300, trend Loss: 0.1016 | 0.0499
Epoch 19/300, trend Loss: 0.1002 | 0.0493
Epoch 20/300, trend Loss: 0.0990 | 0.0488
Epoch 21/300, trend Loss: 0.0982 | 0.0472
Epoch 22/300, trend Loss: 0.0976 | 0.0469
Epoch 23/300, trend Loss: 0.0960 | 0.0470
Epoch 24/300, trend Loss: 0.0949 | 0.0472
Epoch 25/300, trend Loss: 0.0941 | 0.0469
Epoch 26/300, trend Loss: 0.0933 | 0.0468
Epoch 27/300, trend Loss: 0.0926 | 0.0455
Epoch 28/300, trend Loss: 0.0922 | 0.0453
Epoch 29/300, trend Loss: 0.0912 | 0.0450
Epoch 30/300, trend Loss: 0.0903 | 0.0446
Epoch 31/300, trend Loss: 0.0895 | 0.0441
Epoch 32/300, trend Loss: 0.0888 | 0.0437
Epoch 33/300, trend Loss: 0.0882 | 0.0433
Epoch 34/300, trend Loss: 0.0875 | 0.0418
Epoch 35/300, trend Loss: 0.0871 | 0.0415
Epoch 36/300, trend Loss: 0.0865 | 0.0413
Epoch 37/300, trend Loss: 0.0861 | 0.0409
Epoch 38/300, trend Loss: 0.0857 | 0.0405
Epoch 39/300, trend Loss: 0.0854 | 0.0400
Epoch 40/300, trend Loss: 0.0850 | 0.0394
Epoch 41/300, trend Loss: 0.0849 | 0.0391
Epoch 42/300, trend Loss: 0.0844 | 0.0387
Epoch 43/300, trend Loss: 0.0841 | 0.0384
Epoch 44/300, trend Loss: 0.0838 | 0.0381
Epoch 45/300, trend Loss: 0.0835 | 0.0378
Epoch 46/300, trend Loss: 0.0832 | 0.0376
Epoch 47/300, trend Loss: 0.0829 | 0.0376
Epoch 48/300, trend Loss: 0.0827 | 0.0374
Epoch 49/300, trend Loss: 0.0822 | 0.0373
Epoch 50/300, trend Loss: 0.0817 | 0.0372
Epoch 51/300, trend Loss: 0.0813 | 0.0370
Epoch 52/300, trend Loss: 0.0810 | 0.0369
Epoch 53/300, trend Loss: 0.0806 | 0.0369
Epoch 54/300, trend Loss: 0.0803 | 0.0367
Epoch 55/300, trend Loss: 0.0799 | 0.0366
Epoch 56/300, trend Loss: 0.0796 | 0.0364
Epoch 57/300, trend Loss: 0.0792 | 0.0363
Epoch 58/300, trend Loss: 0.0789 | 0.0361
Epoch 59/300, trend Loss: 0.0787 | 0.0360
Epoch 60/300, trend Loss: 0.0784 | 0.0361
Epoch 61/300, trend Loss: 0.0781 | 0.0359
Epoch 62/300, trend Loss: 0.0778 | 0.0358
Epoch 63/300, trend Loss: 0.0775 | 0.0357
Epoch 64/300, trend Loss: 0.0773 | 0.0356
Epoch 65/300, trend Loss: 0.0770 | 0.0355
Epoch 66/300, trend Loss: 0.0767 | 0.0355
Epoch 67/300, trend Loss: 0.0763 | 0.0354
Epoch 68/300, trend Loss: 0.0761 | 0.0353
Epoch 69/300, trend Loss: 0.0758 | 0.0352
Epoch 70/300, trend Loss: 0.0756 | 0.0351
Epoch 71/300, trend Loss: 0.0753 | 0.0351
Epoch 72/300, trend Loss: 0.0750 | 0.0350
Epoch 73/300, trend Loss: 0.0747 | 0.0349
Epoch 74/300, trend Loss: 0.0744 | 0.0349
Epoch 75/300, trend Loss: 0.0741 | 0.0348
Epoch 76/300, trend Loss: 0.0739 | 0.0347
Epoch 77/300, trend Loss: 0.0737 | 0.0346
Epoch 78/300, trend Loss: 0.0734 | 0.0346
Epoch 79/300, trend Loss: 0.0732 | 0.0345
Epoch 80/300, trend Loss: 0.0729 | 0.0344
Epoch 81/300, trend Loss: 0.0728 | 0.0344
Epoch 82/300, trend Loss: 0.0725 | 0.0343
Epoch 83/300, trend Loss: 0.0723 | 0.0343
Epoch 84/300, trend Loss: 0.0721 | 0.0342
Epoch 85/300, trend Loss: 0.0719 | 0.0342
Epoch 86/300, trend Loss: 0.0717 | 0.0341
Epoch 87/300, trend Loss: 0.0715 | 0.0341
Epoch 88/300, trend Loss: 0.0713 | 0.0340
Epoch 89/300, trend Loss: 0.0711 | 0.0340
Epoch 90/300, trend Loss: 0.0710 | 0.0340
Epoch 91/300, trend Loss: 0.0708 | 0.0340
Epoch 92/300, trend Loss: 0.0707 | 0.0339
Epoch 93/300, trend Loss: 0.0705 | 0.0339
Epoch 94/300, trend Loss: 0.0704 | 0.0339
Epoch 95/300, trend Loss: 0.0703 | 0.0339
Epoch 96/300, trend Loss: 0.0702 | 0.0339
Epoch 97/300, trend Loss: 0.0701 | 0.0338
Epoch 98/300, trend Loss: 0.0700 | 0.0338
Epoch 99/300, trend Loss: 0.0699 | 0.0338
Epoch 100/300, trend Loss: 0.0697 | 0.0338
Epoch 101/300, trend Loss: 0.0697 | 0.0338
Epoch 102/300, trend Loss: 0.0696 | 0.0338
Epoch 103/300, trend Loss: 0.0695 | 0.0338
Epoch 104/300, trend Loss: 0.0694 | 0.0338
Epoch 105/300, trend Loss: 0.0693 | 0.0338
Epoch 106/300, trend Loss: 0.0692 | 0.0337
Epoch 107/300, trend Loss: 0.0691 | 0.0337
Epoch 108/300, trend Loss: 0.0690 | 0.0337
Epoch 109/300, trend Loss: 0.0690 | 0.0338
Epoch 110/300, trend Loss: 0.0689 | 0.0338
Epoch 111/300, trend Loss: 0.0688 | 0.0338
Epoch 112/300, trend Loss: 0.0687 | 0.0338
Epoch 113/300, trend Loss: 0.0686 | 0.0339
Epoch 114/300, trend Loss: 0.0685 | 0.0339
Epoch 115/300, trend Loss: 0.0685 | 0.0340
Epoch 116/300, trend Loss: 0.0684 | 0.0340
Epoch 117/300, trend Loss: 0.0683 | 0.0341
Epoch 118/300, trend Loss: 0.0682 | 0.0344
Epoch 119/300, trend Loss: 0.0682 | 0.0345
Epoch 120/300, trend Loss: 0.0681 | 0.0346
Epoch 121/300, trend Loss: 0.0680 | 0.0347
Epoch 122/300, trend Loss: 0.0680 | 0.0348
Epoch 123/300, trend Loss: 0.0679 | 0.0348
Epoch 124/300, trend Loss: 0.0678 | 0.0349
Epoch 125/300, trend Loss: 0.0678 | 0.0354
Epoch 126/300, trend Loss: 0.0677 | 0.0356
Epoch 127/300, trend Loss: 0.0677 | 0.0358
Epoch 128/300, trend Loss: 0.0676 | 0.0359
Epoch 129/300, trend Loss: 0.0676 | 0.0360
Epoch 130/300, trend Loss: 0.0675 | 0.0361
Epoch 131/300, trend Loss: 0.0675 | 0.0363
Epoch 132/300, trend Loss: 0.0674 | 0.0366
Epoch 133/300, trend Loss: 0.0674 | 0.0367
Epoch 134/300, trend Loss: 0.0674 | 0.0367
Epoch 135/300, trend Loss: 0.0674 | 0.0366
Epoch 136/300, trend Loss: 0.0673 | 0.0365
Epoch 137/300, trend Loss: 0.0673 | 0.0363
Epoch 138/300, trend Loss: 0.0672 | 0.0355
Epoch 139/300, trend Loss: 0.0672 | 0.0353
Epoch 140/300, trend Loss: 0.0671 | 0.0351
Epoch 141/300, trend Loss: 0.0671 | 0.0349
Epoch 142/300, trend Loss: 0.0670 | 0.0347
Epoch 143/300, trend Loss: 0.0669 | 0.0346
Epoch 144/300, trend Loss: 0.0669 | 0.0342
Epoch 145/300, trend Loss: 0.0668 | 0.0341
Epoch 146/300, trend Loss: 0.0668 | 0.0341
Epoch 147/300, trend Loss: 0.0667 | 0.0340
Epoch 148/300, trend Loss: 0.0667 | 0.0340
Epoch 149/300, trend Loss: 0.0666 | 0.0340
Epoch 150/300, trend Loss: 0.0666 | 0.0340
Epoch 151/300, trend Loss: 0.0665 | 0.0338
Epoch 152/300, trend Loss: 0.0665 | 0.0338
Epoch 153/300, trend Loss: 0.0665 | 0.0338
Epoch 154/300, trend Loss: 0.0664 | 0.0338
Epoch 155/300, trend Loss: 0.0664 | 0.0338
Epoch 156/300, trend Loss: 0.0663 | 0.0338
Epoch 157/300, trend Loss: 0.0663 | 0.0337
Epoch 158/300, trend Loss: 0.0663 | 0.0337
Epoch 159/300, trend Loss: 0.0662 | 0.0337
Epoch 160/300, trend Loss: 0.0662 | 0.0337
Epoch 161/300, trend Loss: 0.0662 | 0.0337
Epoch 162/300, trend Loss: 0.0661 | 0.0337
Epoch 163/300, trend Loss: 0.0661 | 0.0337
Epoch 164/300, trend Loss: 0.0660 | 0.0335
Epoch 165/300, trend Loss: 0.0660 | 0.0335
Epoch 166/300, trend Loss: 0.0660 | 0.0335
Epoch 167/300, trend Loss: 0.0660 | 0.0335
Epoch 168/300, trend Loss: 0.0659 | 0.0335
Epoch 169/300, trend Loss: 0.0659 | 0.0335
Epoch 170/300, trend Loss: 0.0659 | 0.0334
Epoch 171/300, trend Loss: 0.0658 | 0.0334
Epoch 172/300, trend Loss: 0.0658 | 0.0334
Epoch 173/300, trend Loss: 0.0658 | 0.0334
Epoch 174/300, trend Loss: 0.0658 | 0.0333
Epoch 175/300, trend Loss: 0.0658 | 0.0333
Epoch 176/300, trend Loss: 0.0657 | 0.0333
Epoch 177/300, trend Loss: 0.0657 | 0.0332
Epoch 178/300, trend Loss: 0.0657 | 0.0332
Epoch 179/300, trend Loss: 0.0657 | 0.0332
Epoch 180/300, trend Loss: 0.0656 | 0.0332
Epoch 181/300, trend Loss: 0.0656 | 0.0332
Epoch 182/300, trend Loss: 0.0656 | 0.0332
Epoch 183/300, trend Loss: 0.0656 | 0.0332
Epoch 184/300, trend Loss: 0.0655 | 0.0331
Epoch 185/300, trend Loss: 0.0655 | 0.0331
Epoch 186/300, trend Loss: 0.0655 | 0.0331
Epoch 187/300, trend Loss: 0.0655 | 0.0331
Epoch 188/300, trend Loss: 0.0655 | 0.0331
Epoch 189/300, trend Loss: 0.0655 | 0.0331
Epoch 190/300, trend Loss: 0.0654 | 0.0331
Epoch 191/300, trend Loss: 0.0654 | 0.0331
Epoch 192/300, trend Loss: 0.0654 | 0.0331
Epoch 193/300, trend Loss: 0.0654 | 0.0331
Epoch 194/300, trend Loss: 0.0654 | 0.0330
Epoch 195/300, trend Loss: 0.0654 | 0.0330
Epoch 196/300, trend Loss: 0.0653 | 0.0330
Epoch 197/300, trend Loss: 0.0653 | 0.0330
Epoch 198/300, trend Loss: 0.0653 | 0.0330
Epoch 199/300, trend Loss: 0.0653 | 0.0330
Epoch 200/300, trend Loss: 0.0653 | 0.0330
Epoch 201/300, trend Loss: 0.0653 | 0.0330
Epoch 202/300, trend Loss: 0.0653 | 0.0330
Epoch 203/300, trend Loss: 0.0652 | 0.0329
Epoch 204/300, trend Loss: 0.0652 | 0.0329
Epoch 205/300, trend Loss: 0.0652 | 0.0329
Epoch 206/300, trend Loss: 0.0652 | 0.0329
Epoch 207/300, trend Loss: 0.0652 | 0.0329
Epoch 208/300, trend Loss: 0.0652 | 0.0329
Epoch 209/300, trend Loss: 0.0652 | 0.0329
Epoch 210/300, trend Loss: 0.0652 | 0.0329
Epoch 211/300, trend Loss: 0.0651 | 0.0329
Epoch 212/300, trend Loss: 0.0651 | 0.0329
Epoch 213/300, trend Loss: 0.0651 | 0.0329
Epoch 214/300, trend Loss: 0.0651 | 0.0329
Epoch 215/300, trend Loss: 0.0651 | 0.0329
Epoch 216/300, trend Loss: 0.0651 | 0.0328
Epoch 217/300, trend Loss: 0.0651 | 0.0328
Epoch 218/300, trend Loss: 0.0651 | 0.0328
Epoch 219/300, trend Loss: 0.0651 | 0.0328
Epoch 220/300, trend Loss: 0.0651 | 0.0328
Epoch 221/300, trend Loss: 0.0650 | 0.0328
Epoch 222/300, trend Loss: 0.0650 | 0.0328
Epoch 223/300, trend Loss: 0.0650 | 0.0328
Epoch 224/300, trend Loss: 0.0650 | 0.0328
Epoch 225/300, trend Loss: 0.0650 | 0.0328
Epoch 226/300, trend Loss: 0.0650 | 0.0328
Epoch 227/300, trend Loss: 0.0650 | 0.0328
Epoch 228/300, trend Loss: 0.0650 | 0.0328
Epoch 229/300, trend Loss: 0.0650 | 0.0328
Epoch 230/300, trend Loss: 0.0650 | 0.0328
Epoch 231/300, trend Loss: 0.0650 | 0.0328
Epoch 232/300, trend Loss: 0.0650 | 0.0328
Epoch 233/300, trend Loss: 0.0650 | 0.0328
Epoch 234/300, trend Loss: 0.0649 | 0.0328
Epoch 235/300, trend Loss: 0.0649 | 0.0328
Epoch 236/300, trend Loss: 0.0649 | 0.0328
Epoch 237/300, trend Loss: 0.0649 | 0.0327
Epoch 238/300, trend Loss: 0.0649 | 0.0327
Epoch 239/300, trend Loss: 0.0649 | 0.0327
Epoch 240/300, trend Loss: 0.0649 | 0.0327
Epoch 241/300, trend Loss: 0.0649 | 0.0327
Epoch 242/300, trend Loss: 0.0649 | 0.0327
Epoch 243/300, trend Loss: 0.0649 | 0.0327
Epoch 244/300, trend Loss: 0.0649 | 0.0327
Epoch 245/300, trend Loss: 0.0649 | 0.0327
Epoch 246/300, trend Loss: 0.0649 | 0.0327
Epoch 247/300, trend Loss: 0.0649 | 0.0327
Epoch 248/300, trend Loss: 0.0649 | 0.0327
Epoch 249/300, trend Loss: 0.0649 | 0.0327
Epoch 250/300, trend Loss: 0.0648 | 0.0327
Epoch 251/300, trend Loss: 0.0648 | 0.0327
Epoch 252/300, trend Loss: 0.0648 | 0.0327
Epoch 253/300, trend Loss: 0.0648 | 0.0327
Epoch 254/300, trend Loss: 0.0648 | 0.0327
Epoch 255/300, trend Loss: 0.0648 | 0.0327
Epoch 256/300, trend Loss: 0.0648 | 0.0327
Epoch 257/300, trend Loss: 0.0648 | 0.0327
Epoch 258/300, trend Loss: 0.0648 | 0.0327
Epoch 259/300, trend Loss: 0.0648 | 0.0327
Epoch 260/300, trend Loss: 0.0648 | 0.0327
Epoch 261/300, trend Loss: 0.0648 | 0.0327
Epoch 262/300, trend Loss: 0.0648 | 0.0327
Epoch 263/300, trend Loss: 0.0648 | 0.0327
Epoch 264/300, trend Loss: 0.0648 | 0.0327
Epoch 265/300, trend Loss: 0.0648 | 0.0327
Epoch 266/300, trend Loss: 0.0648 | 0.0327
Epoch 267/300, trend Loss: 0.0648 | 0.0327
Epoch 268/300, trend Loss: 0.0648 | 0.0327
Epoch 269/300, trend Loss: 0.0648 | 0.0327
Epoch 270/300, trend Loss: 0.0648 | 0.0327
Epoch 271/300, trend Loss: 0.0648 | 0.0327
Epoch 272/300, trend Loss: 0.0648 | 0.0327
Epoch 273/300, trend Loss: 0.0648 | 0.0327
Epoch 274/300, trend Loss: 0.0647 | 0.0327
Epoch 275/300, trend Loss: 0.0647 | 0.0326
Epoch 276/300, trend Loss: 0.0647 | 0.0326
Epoch 277/300, trend Loss: 0.0647 | 0.0326
Epoch 278/300, trend Loss: 0.0647 | 0.0326
Epoch 279/300, trend Loss: 0.0647 | 0.0326
Epoch 280/300, trend Loss: 0.0647 | 0.0326
Epoch 281/300, trend Loss: 0.0647 | 0.0326
Epoch 282/300, trend Loss: 0.0647 | 0.0326
Epoch 283/300, trend Loss: 0.0647 | 0.0326
Epoch 284/300, trend Loss: 0.0647 | 0.0326
Epoch 285/300, trend Loss: 0.0647 | 0.0326
Epoch 286/300, trend Loss: 0.0647 | 0.0326
Epoch 287/300, trend Loss: 0.0647 | 0.0326
Epoch 288/300, trend Loss: 0.0647 | 0.0326
Epoch 289/300, trend Loss: 0.0647 | 0.0326
Epoch 290/300, trend Loss: 0.0647 | 0.0326
Epoch 291/300, trend Loss: 0.0647 | 0.0326
Epoch 292/300, trend Loss: 0.0647 | 0.0326
Epoch 293/300, trend Loss: 0.0647 | 0.0326
Epoch 294/300, trend Loss: 0.0647 | 0.0326
Epoch 295/300, trend Loss: 0.0647 | 0.0326
Epoch 296/300, trend Loss: 0.0647 | 0.0326
Epoch 297/300, trend Loss: 0.0647 | 0.0326
Epoch 298/300, trend Loss: 0.0647 | 0.0326
Epoch 299/300, trend Loss: 0.0647 | 0.0326
Epoch 300/300, trend Loss: 0.0647 | 0.0326
Training seasonal_0 component with params: {'observation_period_num': 9, 'train_rates': 0.8814309422829758, 'learning_rate': 0.0008308199014111168, 'batch_size': 151, 'step_size': 15, 'gamma': 0.8242293612210936}
Epoch 1/300, seasonal_0 Loss: 2.9059 | 1.3896
Epoch 2/300, seasonal_0 Loss: 1.1216 | 1.3935
Epoch 3/300, seasonal_0 Loss: 1.0155 | 1.3546
Epoch 4/300, seasonal_0 Loss: 1.0831 | 1.3611
Epoch 5/300, seasonal_0 Loss: 1.0131 | 1.3535
Epoch 6/300, seasonal_0 Loss: 1.0926 | 1.4008
Epoch 7/300, seasonal_0 Loss: 1.2440 | 1.4315
Epoch 8/300, seasonal_0 Loss: 1.2711 | 1.3568
Epoch 9/300, seasonal_0 Loss: 1.3463 | 1.7567
Epoch 10/300, seasonal_0 Loss: 1.0113 | 1.7283
Epoch 11/300, seasonal_0 Loss: 0.9745 | 1.7013
Epoch 12/300, seasonal_0 Loss: 0.9897 | 1.7311
Epoch 13/300, seasonal_0 Loss: 0.9755 | 1.7189
Epoch 14/300, seasonal_0 Loss: 0.9792 | 1.7253
Epoch 15/300, seasonal_0 Loss: 0.9764 | 1.7251
Epoch 16/300, seasonal_0 Loss: 0.9637 | 1.7661
Epoch 17/300, seasonal_0 Loss: 0.9492 | 1.7481
Epoch 18/300, seasonal_0 Loss: 0.9554 | 1.7551
Epoch 19/300, seasonal_0 Loss: 0.9532 | 1.7520
Epoch 20/300, seasonal_0 Loss: 0.9542 | 1.7533
Epoch 21/300, seasonal_0 Loss: 0.9538 | 1.7531
Epoch 22/300, seasonal_0 Loss: 0.9537 | 1.7536
Epoch 23/300, seasonal_0 Loss: 0.9534 | 1.7540
Epoch 24/300, seasonal_0 Loss: 0.9435 | 1.7790
Epoch 25/300, seasonal_0 Loss: 0.9362 | 1.7723
Epoch 26/300, seasonal_0 Loss: 0.9383 | 1.7731
Epoch 27/300, seasonal_0 Loss: 0.9381 | 1.7732
Epoch 28/300, seasonal_0 Loss: 0.9381 | 1.7729
Epoch 29/300, seasonal_0 Loss: 0.9382 | 1.7731
Epoch 30/300, seasonal_0 Loss: 0.9382 | 1.7731
Epoch 31/300, seasonal_0 Loss: 0.9305 | 1.7888
Epoch 32/300, seasonal_0 Loss: 0.9268 | 1.7882
Epoch 33/300, seasonal_0 Loss: 0.9270 | 1.7859
Epoch 34/300, seasonal_0 Loss: 0.9277 | 1.7872
Epoch 35/300, seasonal_0 Loss: 0.9274 | 1.7864
Epoch 36/300, seasonal_0 Loss: 0.9277 | 1.7865
Epoch 37/300, seasonal_0 Loss: 0.9277 | 1.7864
Epoch 38/300, seasonal_0 Loss: 0.9278 | 1.7864
Epoch 39/300, seasonal_0 Loss: 0.9216 | 1.7965
Epoch 40/300, seasonal_0 Loss: 0.9198 | 1.7988
Epoch 41/300, seasonal_0 Loss: 0.9192 | 1.7958
Epoch 42/300, seasonal_0 Loss: 0.9200 | 1.7969
Epoch 43/300, seasonal_0 Loss: 0.9198 | 1.7965
Epoch 44/300, seasonal_0 Loss: 0.9200 | 1.7963
Epoch 45/300, seasonal_0 Loss: 0.9200 | 1.7963
Epoch 46/300, seasonal_0 Loss: 0.9151 | 1.8029
Epoch 47/300, seasonal_0 Loss: 0.9142 | 1.8063
Epoch 48/300, seasonal_0 Loss: 0.9135 | 1.8037
Epoch 49/300, seasonal_0 Loss: 0.9141 | 1.8040
Epoch 50/300, seasonal_0 Loss: 0.9141 | 1.8041
Epoch 51/300, seasonal_0 Loss: 0.9141 | 1.8038
Epoch 52/300, seasonal_0 Loss: 0.9142 | 1.8037
Epoch 53/300, seasonal_0 Loss: 0.9143 | 1.8037
Epoch 54/300, seasonal_0 Loss: 0.9103 | 1.8080
Epoch 55/300, seasonal_0 Loss: 0.9099 | 1.8115
Epoch 56/300, seasonal_0 Loss: 0.9092 | 1.8098
Epoch 57/300, seasonal_0 Loss: 0.9095 | 1.8096
Epoch 58/300, seasonal_0 Loss: 0.9097 | 1.8097
Epoch 59/300, seasonal_0 Loss: 0.9097 | 1.8096
Epoch 60/300, seasonal_0 Loss: 0.9098 | 1.8095
Epoch 61/300, seasonal_0 Loss: 0.9065 | 1.8124
Epoch 62/300, seasonal_0 Loss: 0.9063 | 1.8156
Epoch 63/300, seasonal_0 Loss: 0.9058 | 1.8147
Epoch 64/300, seasonal_0 Loss: 0.9060 | 1.8142
Epoch 65/300, seasonal_0 Loss: 0.9061 | 1.8143
Epoch 66/300, seasonal_0 Loss: 0.9062 | 1.8142
Epoch 67/300, seasonal_0 Loss: 0.9062 | 1.8141
Epoch 68/300, seasonal_0 Loss: 0.9063 | 1.8140
Epoch 69/300, seasonal_0 Loss: 0.9035 | 1.8160
Epoch 70/300, seasonal_0 Loss: 0.9035 | 1.8187
Epoch 71/300, seasonal_0 Loss: 0.9031 | 1.8184
Epoch 72/300, seasonal_0 Loss: 0.9032 | 1.8179
Epoch 73/300, seasonal_0 Loss: 0.9033 | 1.8178
Epoch 74/300, seasonal_0 Loss: 0.9034 | 1.8178
Epoch 75/300, seasonal_0 Loss: 0.9034 | 1.8178
Epoch 76/300, seasonal_0 Loss: 0.9011 | 1.8191
Epoch 77/300, seasonal_0 Loss: 0.9012 | 1.8213
Epoch 78/300, seasonal_0 Loss: 0.9009 | 1.8214
Epoch 79/300, seasonal_0 Loss: 0.9009 | 1.8210
Epoch 80/300, seasonal_0 Loss: 0.9010 | 1.8208
Epoch 81/300, seasonal_0 Loss: 0.9010 | 1.8208
Epoch 82/300, seasonal_0 Loss: 0.9011 | 1.8208
Epoch 83/300, seasonal_0 Loss: 0.9011 | 1.8207
Epoch 84/300, seasonal_0 Loss: 0.8992 | 1.8216
Epoch 85/300, seasonal_0 Loss: 0.8993 | 1.8234
Epoch 86/300, seasonal_0 Loss: 0.8991 | 1.8237
Epoch 87/300, seasonal_0 Loss: 0.8990 | 1.8234
Epoch 88/300, seasonal_0 Loss: 0.8991 | 1.8233
Epoch 89/300, seasonal_0 Loss: 0.8991 | 1.8232
Epoch 90/300, seasonal_0 Loss: 0.8992 | 1.8232
Epoch 91/300, seasonal_0 Loss: 0.8976 | 1.8238
Epoch 92/300, seasonal_0 Loss: 0.8977 | 1.8252
Epoch 93/300, seasonal_0 Loss: 0.8975 | 1.8256
Epoch 94/300, seasonal_0 Loss: 0.8975 | 1.8255
Epoch 95/300, seasonal_0 Loss: 0.8975 | 1.8254
Epoch 96/300, seasonal_0 Loss: 0.8976 | 1.8253
Epoch 97/300, seasonal_0 Loss: 0.8976 | 1.8253
Epoch 98/300, seasonal_0 Loss: 0.8976 | 1.8252
Epoch 99/300, seasonal_0 Loss: 0.8963 | 1.8257
Epoch 100/300, seasonal_0 Loss: 0.8963 | 1.8267
Epoch 101/300, seasonal_0 Loss: 0.8963 | 1.8271
Epoch 102/300, seasonal_0 Loss: 0.8962 | 1.8271
Epoch 103/300, seasonal_0 Loss: 0.8962 | 1.8270
Epoch 104/300, seasonal_0 Loss: 0.8963 | 1.8270
Epoch 105/300, seasonal_0 Loss: 0.8963 | 1.8269
Epoch 106/300, seasonal_0 Loss: 0.8952 | 1.8272
Epoch 107/300, seasonal_0 Loss: 0.8952 | 1.8280
Epoch 108/300, seasonal_0 Loss: 0.8952 | 1.8284
Epoch 109/300, seasonal_0 Loss: 0.8951 | 1.8285
Epoch 110/300, seasonal_0 Loss: 0.8952 | 1.8284
Epoch 111/300, seasonal_0 Loss: 0.8952 | 1.8284
Epoch 112/300, seasonal_0 Loss: 0.8952 | 1.8284
Epoch 113/300, seasonal_0 Loss: 0.8952 | 1.8283
Epoch 114/300, seasonal_0 Loss: 0.8943 | 1.8285
Epoch 115/300, seasonal_0 Loss: 0.8943 | 1.8291
Epoch 116/300, seasonal_0 Loss: 0.8943 | 1.8294
Epoch 117/300, seasonal_0 Loss: 0.8943 | 1.8296
Epoch 118/300, seasonal_0 Loss: 0.8943 | 1.8296
Epoch 119/300, seasonal_0 Loss: 0.8943 | 1.8296
Epoch 120/300, seasonal_0 Loss: 0.8943 | 1.8295
Epoch 121/300, seasonal_0 Loss: 0.8935 | 1.8297
Epoch 122/300, seasonal_0 Loss: 0.8935 | 1.8301
Epoch 123/300, seasonal_0 Loss: 0.8935 | 1.8304
Epoch 124/300, seasonal_0 Loss: 0.8935 | 1.8305
Epoch 125/300, seasonal_0 Loss: 0.8935 | 1.8305
Epoch 126/300, seasonal_0 Loss: 0.8935 | 1.8305
Epoch 127/300, seasonal_0 Loss: 0.8935 | 1.8305
Epoch 128/300, seasonal_0 Loss: 0.8935 | 1.8305
Epoch 129/300, seasonal_0 Loss: 0.8929 | 1.8306
Epoch 130/300, seasonal_0 Loss: 0.8929 | 1.8309
Epoch 131/300, seasonal_0 Loss: 0.8929 | 1.8311
Epoch 132/300, seasonal_0 Loss: 0.8929 | 1.8313
Epoch 133/300, seasonal_0 Loss: 0.8929 | 1.8313
Epoch 134/300, seasonal_0 Loss: 0.8929 | 1.8313
Epoch 135/300, seasonal_0 Loss: 0.8929 | 1.8313
Epoch 136/300, seasonal_0 Loss: 0.8923 | 1.8314
Epoch 137/300, seasonal_0 Loss: 0.8924 | 1.8316
Epoch 138/300, seasonal_0 Loss: 0.8924 | 1.8318
Epoch 139/300, seasonal_0 Loss: 0.8923 | 1.8319
Epoch 140/300, seasonal_0 Loss: 0.8923 | 1.8320
Epoch 141/300, seasonal_0 Loss: 0.8923 | 1.8320
Epoch 142/300, seasonal_0 Loss: 0.8924 | 1.8320
Epoch 143/300, seasonal_0 Loss: 0.8924 | 1.8320
Epoch 144/300, seasonal_0 Loss: 0.8919 | 1.8321
Epoch 145/300, seasonal_0 Loss: 0.8919 | 1.8322
Epoch 146/300, seasonal_0 Loss: 0.8919 | 1.8324
Epoch 147/300, seasonal_0 Loss: 0.8919 | 1.8325
Epoch 148/300, seasonal_0 Loss: 0.8919 | 1.8325
Epoch 149/300, seasonal_0 Loss: 0.8919 | 1.8326
Epoch 150/300, seasonal_0 Loss: 0.8919 | 1.8326
Epoch 151/300, seasonal_0 Loss: 0.8915 | 1.8326
Epoch 152/300, seasonal_0 Loss: 0.8915 | 1.8328
Epoch 153/300, seasonal_0 Loss: 0.8915 | 1.8329
Epoch 154/300, seasonal_0 Loss: 0.8915 | 1.8329
Epoch 155/300, seasonal_0 Loss: 0.8915 | 1.8330
Epoch 156/300, seasonal_0 Loss: 0.8915 | 1.8330
Epoch 157/300, seasonal_0 Loss: 0.8915 | 1.8331
Epoch 158/300, seasonal_0 Loss: 0.8915 | 1.8331
Epoch 159/300, seasonal_0 Loss: 0.8912 | 1.8331
Epoch 160/300, seasonal_0 Loss: 0.8912 | 1.8332
Epoch 161/300, seasonal_0 Loss: 0.8912 | 1.8333
Epoch 162/300, seasonal_0 Loss: 0.8912 | 1.8333
Epoch 163/300, seasonal_0 Loss: 0.8912 | 1.8334
Epoch 164/300, seasonal_0 Loss: 0.8912 | 1.8334
Epoch 165/300, seasonal_0 Loss: 0.8912 | 1.8335
Epoch 166/300, seasonal_0 Loss: 0.8909 | 1.8335
Epoch 167/300, seasonal_0 Loss: 0.8909 | 1.8336
Epoch 168/300, seasonal_0 Loss: 0.8909 | 1.8336
Epoch 169/300, seasonal_0 Loss: 0.8909 | 1.8337
Epoch 170/300, seasonal_0 Loss: 0.8909 | 1.8337
Epoch 171/300, seasonal_0 Loss: 0.8909 | 1.8338
Epoch 172/300, seasonal_0 Loss: 0.8909 | 1.8338
Epoch 173/300, seasonal_0 Loss: 0.8909 | 1.8338
Epoch 174/300, seasonal_0 Loss: 0.8907 | 1.8338
Epoch 175/300, seasonal_0 Loss: 0.8907 | 1.8339
Epoch 176/300, seasonal_0 Loss: 0.8907 | 1.8339
Epoch 177/300, seasonal_0 Loss: 0.8907 | 1.8340
Epoch 178/300, seasonal_0 Loss: 0.8907 | 1.8340
Epoch 179/300, seasonal_0 Loss: 0.8907 | 1.8340
Epoch 180/300, seasonal_0 Loss: 0.8907 | 1.8341
Epoch 181/300, seasonal_0 Loss: 0.8905 | 1.8341
Epoch 182/300, seasonal_0 Loss: 0.8905 | 1.8341
Epoch 183/300, seasonal_0 Loss: 0.8905 | 1.8342
Epoch 184/300, seasonal_0 Loss: 0.8905 | 1.8342
Epoch 185/300, seasonal_0 Loss: 0.8905 | 1.8342
Epoch 186/300, seasonal_0 Loss: 0.8905 | 1.8343
Epoch 187/300, seasonal_0 Loss: 0.8905 | 1.8343
Epoch 188/300, seasonal_0 Loss: 0.8905 | 1.8343
Epoch 189/300, seasonal_0 Loss: 0.8904 | 1.8343
Epoch 190/300, seasonal_0 Loss: 0.8904 | 1.8344
Epoch 191/300, seasonal_0 Loss: 0.8904 | 1.8344
Epoch 192/300, seasonal_0 Loss: 0.8904 | 1.8344
Epoch 193/300, seasonal_0 Loss: 0.8904 | 1.8344
Epoch 194/300, seasonal_0 Loss: 0.8904 | 1.8345
Epoch 195/300, seasonal_0 Loss: 0.8904 | 1.8345
Epoch 196/300, seasonal_0 Loss: 0.8902 | 1.8345
Epoch 197/300, seasonal_0 Loss: 0.8902 | 1.8345
Epoch 198/300, seasonal_0 Loss: 0.8902 | 1.8346
Epoch 199/300, seasonal_0 Loss: 0.8902 | 1.8346
Epoch 200/300, seasonal_0 Loss: 0.8902 | 1.8346
Epoch 201/300, seasonal_0 Loss: 0.8902 | 1.8346
Epoch 202/300, seasonal_0 Loss: 0.8902 | 1.8346
Epoch 203/300, seasonal_0 Loss: 0.8902 | 1.8347
Epoch 204/300, seasonal_0 Loss: 0.8901 | 1.8347
Epoch 205/300, seasonal_0 Loss: 0.8901 | 1.8347
Epoch 206/300, seasonal_0 Loss: 0.8901 | 1.8347
Epoch 207/300, seasonal_0 Loss: 0.8901 | 1.8347
Epoch 208/300, seasonal_0 Loss: 0.8901 | 1.8348
Epoch 209/300, seasonal_0 Loss: 0.8901 | 1.8348
Epoch 210/300, seasonal_0 Loss: 0.8901 | 1.8348
Epoch 211/300, seasonal_0 Loss: 0.8900 | 1.8348
Epoch 212/300, seasonal_0 Loss: 0.8900 | 1.8348
Epoch 213/300, seasonal_0 Loss: 0.8900 | 1.8348
Epoch 214/300, seasonal_0 Loss: 0.8900 | 1.8349
Epoch 215/300, seasonal_0 Loss: 0.8900 | 1.8349
Epoch 216/300, seasonal_0 Loss: 0.8900 | 1.8349
Epoch 217/300, seasonal_0 Loss: 0.8900 | 1.8349
Epoch 218/300, seasonal_0 Loss: 0.8900 | 1.8349
Epoch 219/300, seasonal_0 Loss: 0.8899 | 1.8349
Epoch 220/300, seasonal_0 Loss: 0.8899 | 1.8349
Epoch 221/300, seasonal_0 Loss: 0.8899 | 1.8349
Epoch 222/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 223/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 224/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 225/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 226/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 227/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 228/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 229/300, seasonal_0 Loss: 0.8899 | 1.8350
Epoch 230/300, seasonal_0 Loss: 0.8899 | 1.8351
Epoch 231/300, seasonal_0 Loss: 0.8899 | 1.8351
Epoch 232/300, seasonal_0 Loss: 0.8899 | 1.8351
Epoch 233/300, seasonal_0 Loss: 0.8899 | 1.8351
Epoch 234/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 235/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 236/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 237/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 238/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 239/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 240/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 241/300, seasonal_0 Loss: 0.8898 | 1.8351
Epoch 242/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 243/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 244/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 245/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 246/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 247/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 248/300, seasonal_0 Loss: 0.8898 | 1.8352
Epoch 249/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 250/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 251/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 252/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 253/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 254/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 255/300, seasonal_0 Loss: 0.8897 | 1.8352
Epoch 256/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 257/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 258/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 259/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 260/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 261/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 262/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 263/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 264/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 265/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 266/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 267/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 268/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 269/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 270/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 271/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 272/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 273/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 274/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 275/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 276/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 277/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 278/300, seasonal_0 Loss: 0.8897 | 1.8353
Epoch 279/300, seasonal_0 Loss: 0.8896 | 1.8353
Epoch 280/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 281/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 282/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 283/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 284/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 285/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 286/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 287/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 288/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 289/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 290/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 291/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 292/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 293/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 294/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 295/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 296/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 297/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 298/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 299/300, seasonal_0 Loss: 0.8896 | 1.8354
Epoch 300/300, seasonal_0 Loss: 0.8896 | 1.8354
Training seasonal_1 component with params: {'observation_period_num': 22, 'train_rates': 0.9824810249199024, 'learning_rate': 0.00017786620380025816, 'batch_size': 18, 'step_size': 3, 'gamma': 0.9772529196605334}
Epoch 1/300, seasonal_1 Loss: 0.2658 | 0.1559
Epoch 2/300, seasonal_1 Loss: 0.1383 | 0.0991
Epoch 3/300, seasonal_1 Loss: 0.1116 | 0.0680
Epoch 4/300, seasonal_1 Loss: 0.1081 | 0.0577
Epoch 5/300, seasonal_1 Loss: 0.1018 | 0.0609
Epoch 6/300, seasonal_1 Loss: 0.1052 | 0.0612
Epoch 7/300, seasonal_1 Loss: 0.1037 | 0.0537
Epoch 8/300, seasonal_1 Loss: 0.0845 | 0.0452
Epoch 9/300, seasonal_1 Loss: 0.1047 | 0.0797
Epoch 10/300, seasonal_1 Loss: 0.0847 | 0.0694
Epoch 11/300, seasonal_1 Loss: 0.0889 | 0.0469
Epoch 12/300, seasonal_1 Loss: 0.0874 | 0.0485
Epoch 13/300, seasonal_1 Loss: 0.0856 | 0.0576
Epoch 14/300, seasonal_1 Loss: 0.0907 | 0.0654
Epoch 15/300, seasonal_1 Loss: 0.0870 | 0.0602
Epoch 16/300, seasonal_1 Loss: 0.0849 | 0.0604
Epoch 17/300, seasonal_1 Loss: 0.0858 | 0.0437
Epoch 18/300, seasonal_1 Loss: 0.0768 | 0.0343
Epoch 19/300, seasonal_1 Loss: 0.0758 | 0.0377
Epoch 20/300, seasonal_1 Loss: 0.0746 | 0.0402
Epoch 21/300, seasonal_1 Loss: 0.0743 | 0.0457
Epoch 22/300, seasonal_1 Loss: 0.0675 | 0.0386
Epoch 23/300, seasonal_1 Loss: 0.0655 | 0.0501
Epoch 24/300, seasonal_1 Loss: 0.0607 | 0.0368
Epoch 25/300, seasonal_1 Loss: 0.0590 | 0.0397
Epoch 26/300, seasonal_1 Loss: 0.0550 | 0.0481
Epoch 27/300, seasonal_1 Loss: 0.0509 | 0.0443
Epoch 28/300, seasonal_1 Loss: 0.0480 | 0.0325
Epoch 29/300, seasonal_1 Loss: 0.0492 | 0.0462
Epoch 30/300, seasonal_1 Loss: 0.0487 | 0.0363
Epoch 31/300, seasonal_1 Loss: 0.0471 | 0.0309
Epoch 32/300, seasonal_1 Loss: 0.0450 | 0.0463
Epoch 33/300, seasonal_1 Loss: 0.0443 | 0.0486
Epoch 34/300, seasonal_1 Loss: 0.0476 | 0.0516
Epoch 35/300, seasonal_1 Loss: 0.0526 | 0.0535
Epoch 36/300, seasonal_1 Loss: 0.0455 | 0.0499
Epoch 37/300, seasonal_1 Loss: 0.0616 | 0.0388
Epoch 38/300, seasonal_1 Loss: 0.0514 | 0.0368
Epoch 39/300, seasonal_1 Loss: 0.0555 | 0.0357
Epoch 40/300, seasonal_1 Loss: 0.0541 | 0.0318
Epoch 41/300, seasonal_1 Loss: 0.0487 | 0.0389
Epoch 42/300, seasonal_1 Loss: 0.0476 | 0.0348
Epoch 43/300, seasonal_1 Loss: 0.0446 | 0.0405
Epoch 44/300, seasonal_1 Loss: 0.0414 | 0.0411
Epoch 45/300, seasonal_1 Loss: 0.0445 | 0.0433
Epoch 46/300, seasonal_1 Loss: 0.0375 | 0.0449
Epoch 47/300, seasonal_1 Loss: 0.0336 | 0.0334
Epoch 48/300, seasonal_1 Loss: 0.0318 | 0.0400
Epoch 49/300, seasonal_1 Loss: 0.0310 | 0.0476
Epoch 50/300, seasonal_1 Loss: 0.0305 | 0.0460
Epoch 51/300, seasonal_1 Loss: 0.0298 | 0.0421
Epoch 52/300, seasonal_1 Loss: 0.0300 | 0.0402
Epoch 53/300, seasonal_1 Loss: 0.0301 | 0.0462
Epoch 54/300, seasonal_1 Loss: 0.0304 | 0.0359
Epoch 55/300, seasonal_1 Loss: 0.0294 | 0.0313
Epoch 56/300, seasonal_1 Loss: 0.0290 | 0.0324
Epoch 57/300, seasonal_1 Loss: 0.0298 | 0.0348
Epoch 58/300, seasonal_1 Loss: 0.0298 | 0.0324
Epoch 59/300, seasonal_1 Loss: 0.0289 | 0.0345
Epoch 60/300, seasonal_1 Loss: 0.0284 | 0.0329
Epoch 61/300, seasonal_1 Loss: 0.0281 | 0.0325
Epoch 62/300, seasonal_1 Loss: 0.0278 | 0.0369
Epoch 63/300, seasonal_1 Loss: 0.0277 | 0.0403
Epoch 64/300, seasonal_1 Loss: 0.0279 | 0.0379
Epoch 65/300, seasonal_1 Loss: 0.0286 | 0.0418
Epoch 66/300, seasonal_1 Loss: 0.0298 | 0.0465
Epoch 67/300, seasonal_1 Loss: 0.0309 | 0.0402
Epoch 68/300, seasonal_1 Loss: 0.0291 | 0.0344
Epoch 69/300, seasonal_1 Loss: 0.0305 | 0.0354
Epoch 70/300, seasonal_1 Loss: 0.0303 | 0.0356
Epoch 71/300, seasonal_1 Loss: 0.0317 | 0.0342
Epoch 72/300, seasonal_1 Loss: 0.0311 | 0.0472
Epoch 73/300, seasonal_1 Loss: 0.0314 | 0.0541
Epoch 74/300, seasonal_1 Loss: 0.0301 | 0.0547
Epoch 75/300, seasonal_1 Loss: 0.0285 | 0.0414
Epoch 76/300, seasonal_1 Loss: 0.0280 | 0.0405
Epoch 77/300, seasonal_1 Loss: 0.0276 | 0.0413
Epoch 78/300, seasonal_1 Loss: 0.0278 | 0.0537
Epoch 79/300, seasonal_1 Loss: 0.0260 | 0.0607
Epoch 80/300, seasonal_1 Loss: 0.0249 | 0.0379
Epoch 81/300, seasonal_1 Loss: 0.0247 | 0.0346
Epoch 82/300, seasonal_1 Loss: 0.0242 | 0.0331
Epoch 83/300, seasonal_1 Loss: 0.0244 | 0.0296
Epoch 84/300, seasonal_1 Loss: 0.0245 | 0.0312
Epoch 85/300, seasonal_1 Loss: 0.0251 | 0.0291
Epoch 86/300, seasonal_1 Loss: 0.0250 | 0.0345
Epoch 87/300, seasonal_1 Loss: 0.0252 | 0.0357
Epoch 88/300, seasonal_1 Loss: 0.0259 | 0.0319
Epoch 89/300, seasonal_1 Loss: 0.0254 | 0.0325
Epoch 90/300, seasonal_1 Loss: 0.0250 | 0.0291
Epoch 91/300, seasonal_1 Loss: 0.0255 | 0.0313
Epoch 92/300, seasonal_1 Loss: 0.0251 | 0.0322
Epoch 93/300, seasonal_1 Loss: 0.0245 | 0.0315
Epoch 94/300, seasonal_1 Loss: 0.0254 | 0.0413
Epoch 95/300, seasonal_1 Loss: 0.0247 | 0.0306
Epoch 96/300, seasonal_1 Loss: 0.0244 | 0.0331
Epoch 97/300, seasonal_1 Loss: 0.0232 | 0.0301
Epoch 98/300, seasonal_1 Loss: 0.0232 | 0.0458
Epoch 99/300, seasonal_1 Loss: 0.0233 | 0.0557
Epoch 100/300, seasonal_1 Loss: 0.0240 | 0.0543
Epoch 101/300, seasonal_1 Loss: 0.0239 | 0.0486
Epoch 102/300, seasonal_1 Loss: 0.0237 | 0.0368
Epoch 103/300, seasonal_1 Loss: 0.0235 | 0.0330
Epoch 104/300, seasonal_1 Loss: 0.0222 | 0.0312
Epoch 105/300, seasonal_1 Loss: 0.0217 | 0.0349
Epoch 106/300, seasonal_1 Loss: 0.0214 | 0.0314
Epoch 107/300, seasonal_1 Loss: 0.0213 | 0.0298
Epoch 108/300, seasonal_1 Loss: 0.0211 | 0.0297
Epoch 109/300, seasonal_1 Loss: 0.0212 | 0.0306
Epoch 110/300, seasonal_1 Loss: 0.0212 | 0.0327
Epoch 111/300, seasonal_1 Loss: 0.0215 | 0.0379
Epoch 112/300, seasonal_1 Loss: 0.0329 | 0.0688
Epoch 113/300, seasonal_1 Loss: 0.0325 | 0.0823
Epoch 114/300, seasonal_1 Loss: 0.0266 | 0.0469
Epoch 115/300, seasonal_1 Loss: 0.0335 | 0.0389
Epoch 116/300, seasonal_1 Loss: 0.0310 | 0.0371
Epoch 117/300, seasonal_1 Loss: 0.0296 | 0.0308
Epoch 118/300, seasonal_1 Loss: 0.0266 | 0.0316
Epoch 119/300, seasonal_1 Loss: 0.0246 | 0.0355
Epoch 120/300, seasonal_1 Loss: 0.0233 | 0.0296
Epoch 121/300, seasonal_1 Loss: 0.0217 | 0.0290
Epoch 122/300, seasonal_1 Loss: 0.0209 | 0.0294
Epoch 123/300, seasonal_1 Loss: 0.0204 | 0.0300
Epoch 124/300, seasonal_1 Loss: 0.0202 | 0.0298
Epoch 125/300, seasonal_1 Loss: 0.0199 | 0.0297
Epoch 126/300, seasonal_1 Loss: 0.0197 | 0.0305
Epoch 127/300, seasonal_1 Loss: 0.0195 | 0.0315
Epoch 128/300, seasonal_1 Loss: 0.0194 | 0.0311
Epoch 129/300, seasonal_1 Loss: 0.0193 | 0.0310
Epoch 130/300, seasonal_1 Loss: 0.0191 | 0.0323
Epoch 131/300, seasonal_1 Loss: 0.0190 | 0.0350
Epoch 132/300, seasonal_1 Loss: 0.0190 | 0.0355
Epoch 133/300, seasonal_1 Loss: 0.0192 | 0.0389
Epoch 134/300, seasonal_1 Loss: 0.0190 | 0.0372
Epoch 135/300, seasonal_1 Loss: 0.0193 | 0.0325
Epoch 136/300, seasonal_1 Loss: 0.0192 | 0.0338
Epoch 137/300, seasonal_1 Loss: 0.0194 | 0.0376
Epoch 138/300, seasonal_1 Loss: 0.0199 | 0.0348
Epoch 139/300, seasonal_1 Loss: 0.0193 | 0.0420
Epoch 140/300, seasonal_1 Loss: 0.0194 | 0.0501
Epoch 141/300, seasonal_1 Loss: 0.0198 | 0.0384
Epoch 142/300, seasonal_1 Loss: 0.0198 | 0.0315
Epoch 143/300, seasonal_1 Loss: 0.0201 | 0.0304
Epoch 144/300, seasonal_1 Loss: 0.0197 | 0.0346
Epoch 145/300, seasonal_1 Loss: 0.0198 | 0.0302
Epoch 146/300, seasonal_1 Loss: 0.0194 | 0.0304
Epoch 147/300, seasonal_1 Loss: 0.0192 | 0.0468
Epoch 148/300, seasonal_1 Loss: 0.0188 | 0.0437
Epoch 149/300, seasonal_1 Loss: 0.0189 | 0.0357
Epoch 150/300, seasonal_1 Loss: 0.0187 | 0.0325
Epoch 151/300, seasonal_1 Loss: 0.0186 | 0.0343
Epoch 152/300, seasonal_1 Loss: 0.0186 | 0.0431
Epoch 153/300, seasonal_1 Loss: 0.0190 | 0.0492
Epoch 154/300, seasonal_1 Loss: 0.0191 | 0.0834
Epoch 155/300, seasonal_1 Loss: 0.0190 | 0.0418
Epoch 156/300, seasonal_1 Loss: 0.0187 | 0.0272
Epoch 157/300, seasonal_1 Loss: 0.0190 | 0.0363
Epoch 158/300, seasonal_1 Loss: 0.0182 | 0.0556
Epoch 159/300, seasonal_1 Loss: 0.0182 | 0.0328
Epoch 160/300, seasonal_1 Loss: 0.0182 | 0.0281
Epoch 161/300, seasonal_1 Loss: 0.0181 | 0.0320
Epoch 162/300, seasonal_1 Loss: 0.0179 | 0.0358
Epoch 163/300, seasonal_1 Loss: 0.0178 | 0.0343
Epoch 164/300, seasonal_1 Loss: 0.0178 | 0.0316
Epoch 165/300, seasonal_1 Loss: 0.0179 | 0.0318
Epoch 166/300, seasonal_1 Loss: 0.0179 | 0.0342
Epoch 167/300, seasonal_1 Loss: 0.0179 | 0.0377
Epoch 168/300, seasonal_1 Loss: 0.0180 | 0.0403
Epoch 169/300, seasonal_1 Loss: 0.0182 | 0.0385
Epoch 170/300, seasonal_1 Loss: 0.0182 | 0.0346
Epoch 171/300, seasonal_1 Loss: 0.0183 | 0.0356
Epoch 172/300, seasonal_1 Loss: 0.0183 | 0.0446
Epoch 173/300, seasonal_1 Loss: 0.0182 | 0.0576
Epoch 174/300, seasonal_1 Loss: 0.0181 | 0.0380
Epoch 175/300, seasonal_1 Loss: 0.0180 | 0.0271
Epoch 176/300, seasonal_1 Loss: 0.0181 | 0.0281
Epoch 177/300, seasonal_1 Loss: 0.0178 | 0.0303
Epoch 178/300, seasonal_1 Loss: 0.0178 | 0.0321
Epoch 179/300, seasonal_1 Loss: 0.0177 | 0.0317
Epoch 180/300, seasonal_1 Loss: 0.0175 | 0.0307
Epoch 181/300, seasonal_1 Loss: 0.0173 | 0.0335
Epoch 182/300, seasonal_1 Loss: 0.0173 | 0.0361
Epoch 183/300, seasonal_1 Loss: 0.0173 | 0.0457
Epoch 184/300, seasonal_1 Loss: 0.0172 | 0.0721
Epoch 185/300, seasonal_1 Loss: 0.0172 | 0.0494
Epoch 186/300, seasonal_1 Loss: 0.0172 | 0.0282
Epoch 187/300, seasonal_1 Loss: 0.0176 | 0.0275
Epoch 188/300, seasonal_1 Loss: 0.0172 | 0.0365
Epoch 189/300, seasonal_1 Loss: 0.0170 | 0.0378
Epoch 190/300, seasonal_1 Loss: 0.0168 | 0.0331
Epoch 191/300, seasonal_1 Loss: 0.0168 | 0.0294
Epoch 192/300, seasonal_1 Loss: 0.0167 | 0.0314
Epoch 193/300, seasonal_1 Loss: 0.0167 | 0.0343
Epoch 194/300, seasonal_1 Loss: 0.0166 | 0.0370
Epoch 195/300, seasonal_1 Loss: 0.0166 | 0.0367
Epoch 196/300, seasonal_1 Loss: 0.0165 | 0.0355
Epoch 197/300, seasonal_1 Loss: 0.0165 | 0.0340
Epoch 198/300, seasonal_1 Loss: 0.0165 | 0.0344
Epoch 199/300, seasonal_1 Loss: 0.0165 | 0.0345
Epoch 200/300, seasonal_1 Loss: 0.0164 | 0.0345
Epoch 201/300, seasonal_1 Loss: 0.0164 | 0.0340
Epoch 202/300, seasonal_1 Loss: 0.0164 | 0.0335
Epoch 203/300, seasonal_1 Loss: 0.0164 | 0.0340
Epoch 204/300, seasonal_1 Loss: 0.0164 | 0.0336
Epoch 205/300, seasonal_1 Loss: 0.0164 | 0.0344
Epoch 206/300, seasonal_1 Loss: 0.0164 | 0.0331
Epoch 207/300, seasonal_1 Loss: 0.0164 | 0.0339
Epoch 208/300, seasonal_1 Loss: 0.0164 | 0.0350
Epoch 209/300, seasonal_1 Loss: 0.0164 | 0.0399
Epoch 210/300, seasonal_1 Loss: 0.0164 | 0.0486
Epoch 211/300, seasonal_1 Loss: 0.0165 | 0.0480
Epoch 212/300, seasonal_1 Loss: 0.0166 | 0.0307
Epoch 213/300, seasonal_1 Loss: 0.0167 | 0.0291
Epoch 214/300, seasonal_1 Loss: 0.0166 | 0.0339
Epoch 215/300, seasonal_1 Loss: 0.0165 | 0.0381
Epoch 216/300, seasonal_1 Loss: 0.0163 | 0.0366
Epoch 217/300, seasonal_1 Loss: 0.0162 | 0.0337
Epoch 218/300, seasonal_1 Loss: 0.0162 | 0.0338
Epoch 219/300, seasonal_1 Loss: 0.0162 | 0.0361
Epoch 220/300, seasonal_1 Loss: 0.0162 | 0.0391
Epoch 221/300, seasonal_1 Loss: 0.0161 | 0.0385
Epoch 222/300, seasonal_1 Loss: 0.0161 | 0.0354
Epoch 223/300, seasonal_1 Loss: 0.0161 | 0.0335
Epoch 224/300, seasonal_1 Loss: 0.0161 | 0.0344
Epoch 225/300, seasonal_1 Loss: 0.0160 | 0.0363
Epoch 226/300, seasonal_1 Loss: 0.0160 | 0.0373
Epoch 227/300, seasonal_1 Loss: 0.0160 | 0.0370
Epoch 228/300, seasonal_1 Loss: 0.0160 | 0.0368
Epoch 229/300, seasonal_1 Loss: 0.0160 | 0.0375
Epoch 230/300, seasonal_1 Loss: 0.0160 | 0.0388
Epoch 231/300, seasonal_1 Loss: 0.0160 | 0.0392
Epoch 232/300, seasonal_1 Loss: 0.0160 | 0.0386
Epoch 233/300, seasonal_1 Loss: 0.0161 | 0.0367
Epoch 234/300, seasonal_1 Loss: 0.0161 | 0.0329
Epoch 235/300, seasonal_1 Loss: 0.0162 | 0.0300
Epoch 236/300, seasonal_1 Loss: 0.0163 | 0.0314
Epoch 237/300, seasonal_1 Loss: 0.0162 | 0.0347
Epoch 238/300, seasonal_1 Loss: 0.0161 | 0.0360
Epoch 239/300, seasonal_1 Loss: 0.0160 | 0.0375
Epoch 240/300, seasonal_1 Loss: 0.0159 | 0.0396
Epoch 241/300, seasonal_1 Loss: 0.0159 | 0.0402
Epoch 242/300, seasonal_1 Loss: 0.0159 | 0.0367
Epoch 243/300, seasonal_1 Loss: 0.0159 | 0.0331
Epoch 244/300, seasonal_1 Loss: 0.0158 | 0.0322
Epoch 245/300, seasonal_1 Loss: 0.0158 | 0.0332
Epoch 246/300, seasonal_1 Loss: 0.0158 | 0.0344
Epoch 247/300, seasonal_1 Loss: 0.0157 | 0.0351
Epoch 248/300, seasonal_1 Loss: 0.0157 | 0.0356
Epoch 249/300, seasonal_1 Loss: 0.0157 | 0.0361
Epoch 250/300, seasonal_1 Loss: 0.0157 | 0.0360
Epoch 251/300, seasonal_1 Loss: 0.0157 | 0.0350
Epoch 252/300, seasonal_1 Loss: 0.0156 | 0.0335
Epoch 253/300, seasonal_1 Loss: 0.0156 | 0.0327
Epoch 254/300, seasonal_1 Loss: 0.0156 | 0.0327
Epoch 255/300, seasonal_1 Loss: 0.0156 | 0.0336
Epoch 256/300, seasonal_1 Loss: 0.0156 | 0.0347
Epoch 257/300, seasonal_1 Loss: 0.0156 | 0.0362
Epoch 258/300, seasonal_1 Loss: 0.0156 | 0.0375
Epoch 259/300, seasonal_1 Loss: 0.0156 | 0.0386
Epoch 260/300, seasonal_1 Loss: 0.0156 | 0.0380
Epoch 261/300, seasonal_1 Loss: 0.0156 | 0.0362
Epoch 262/300, seasonal_1 Loss: 0.0156 | 0.0350
Epoch 263/300, seasonal_1 Loss: 0.0156 | 0.0348
Epoch 264/300, seasonal_1 Loss: 0.0155 | 0.0346
Epoch 265/300, seasonal_1 Loss: 0.0155 | 0.0344
Epoch 266/300, seasonal_1 Loss: 0.0155 | 0.0353
Epoch 267/300, seasonal_1 Loss: 0.0155 | 0.0369
Epoch 268/300, seasonal_1 Loss: 0.0155 | 0.0388
Epoch 269/300, seasonal_1 Loss: 0.0155 | 0.0398
Epoch 270/300, seasonal_1 Loss: 0.0155 | 0.0394
Epoch 271/300, seasonal_1 Loss: 0.0155 | 0.0379
Epoch 272/300, seasonal_1 Loss: 0.0155 | 0.0363
Epoch 273/300, seasonal_1 Loss: 0.0155 | 0.0353
Epoch 274/300, seasonal_1 Loss: 0.0155 | 0.0350
Epoch 275/300, seasonal_1 Loss: 0.0155 | 0.0360
Epoch 276/300, seasonal_1 Loss: 0.0155 | 0.0375
Epoch 277/300, seasonal_1 Loss: 0.0155 | 0.0387
Epoch 278/300, seasonal_1 Loss: 0.0155 | 0.0387
Epoch 279/300, seasonal_1 Loss: 0.0154 | 0.0378
Epoch 280/300, seasonal_1 Loss: 0.0154 | 0.0364
Epoch 281/300, seasonal_1 Loss: 0.0154 | 0.0355
Epoch 282/300, seasonal_1 Loss: 0.0154 | 0.0352
Epoch 283/300, seasonal_1 Loss: 0.0154 | 0.0356
Epoch 284/300, seasonal_1 Loss: 0.0154 | 0.0363
Epoch 285/300, seasonal_1 Loss: 0.0154 | 0.0370
Epoch 286/300, seasonal_1 Loss: 0.0154 | 0.0373
Epoch 287/300, seasonal_1 Loss: 0.0154 | 0.0372
Epoch 288/300, seasonal_1 Loss: 0.0154 | 0.0365
Epoch 289/300, seasonal_1 Loss: 0.0154 | 0.0357
Epoch 290/300, seasonal_1 Loss: 0.0154 | 0.0350
Epoch 291/300, seasonal_1 Loss: 0.0154 | 0.0351
Epoch 292/300, seasonal_1 Loss: 0.0154 | 0.0352
Epoch 293/300, seasonal_1 Loss: 0.0154 | 0.0357
Epoch 294/300, seasonal_1 Loss: 0.0154 | 0.0358
Epoch 295/300, seasonal_1 Loss: 0.0154 | 0.0357
Epoch 296/300, seasonal_1 Loss: 0.0154 | 0.0353
Epoch 297/300, seasonal_1 Loss: 0.0154 | 0.0350
Epoch 298/300, seasonal_1 Loss: 0.0154 | 0.0349
Epoch 299/300, seasonal_1 Loss: 0.0154 | 0.0352
Epoch 300/300, seasonal_1 Loss: 0.0154 | 0.0354
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9227579348531737, 'learning_rate': 0.0007033093239394565, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8946151854206078}
Epoch 1/300, seasonal_2 Loss: 0.8916 | 0.3906
Epoch 2/300, seasonal_2 Loss: 0.4816 | 0.5074
Epoch 3/300, seasonal_2 Loss: 0.4950 | 0.4440
Epoch 4/300, seasonal_2 Loss: 0.4806 | 0.4210
Epoch 5/300, seasonal_2 Loss: 0.5688 | 0.8114
Epoch 6/300, seasonal_2 Loss: 0.5580 | 0.8702
Epoch 7/300, seasonal_2 Loss: 0.5580 | 0.8995
Epoch 8/300, seasonal_2 Loss: 0.6931 | 1.3367
Epoch 9/300, seasonal_2 Loss: 0.8793 | 1.3592
Epoch 10/300, seasonal_2 Loss: 0.8732 | 1.3481
Epoch 11/300, seasonal_2 Loss: 0.8834 | 1.3378
Epoch 12/300, seasonal_2 Loss: 0.8895 | 1.4507
Epoch 13/300, seasonal_2 Loss: 0.8759 | 1.4329
Epoch 14/300, seasonal_2 Loss: 0.8789 | 1.4388
Epoch 15/300, seasonal_2 Loss: 0.8833 | 1.4264
Epoch 16/300, seasonal_2 Loss: 0.8872 | 1.4320
Epoch 17/300, seasonal_2 Loss: 0.8820 | 1.3941
Epoch 18/300, seasonal_2 Loss: 0.8911 | 1.3863
Epoch 19/300, seasonal_2 Loss: 0.8995 | 1.4199
Epoch 20/300, seasonal_2 Loss: 0.8823 | 1.4228
Epoch 21/300, seasonal_2 Loss: 0.9007 | 1.4252
Epoch 22/300, seasonal_2 Loss: 0.9147 | 1.4261
Epoch 23/300, seasonal_2 Loss: 0.9519 | 1.4965
Epoch 24/300, seasonal_2 Loss: 0.9571 | 1.5581
Epoch 25/300, seasonal_2 Loss: 0.9568 | 1.6118
Epoch 26/300, seasonal_2 Loss: 0.9593 | 1.7002
Epoch 27/300, seasonal_2 Loss: 0.9481 | 1.7324
Epoch 28/300, seasonal_2 Loss: 0.9452 | 1.7577
Epoch 29/300, seasonal_2 Loss: 0.9440 | 1.8012
Epoch 30/300, seasonal_2 Loss: 0.9382 | 1.8183
Epoch 31/300, seasonal_2 Loss: 0.9361 | 1.8312
Epoch 32/300, seasonal_2 Loss: 0.9343 | 1.8417
Epoch 33/300, seasonal_2 Loss: 0.9322 | 1.8549
Epoch 34/300, seasonal_2 Loss: 0.9302 | 1.8582
Epoch 35/300, seasonal_2 Loss: 0.9275 | 1.8465
Epoch 36/300, seasonal_2 Loss: 0.9298 | 1.8563
Epoch 37/300, seasonal_2 Loss: 0.9285 | 1.8584
Epoch 38/300, seasonal_2 Loss: 0.9278 | 1.8584
Epoch 39/300, seasonal_2 Loss: 0.9272 | 1.8551
Epoch 40/300, seasonal_2 Loss: 0.9285 | 1.8671
Epoch 41/300, seasonal_2 Loss: 0.9276 | 1.8764
Epoch 42/300, seasonal_2 Loss: 0.9265 | 1.8812
Epoch 43/300, seasonal_2 Loss: 0.9262 | 1.8901
Epoch 44/300, seasonal_2 Loss: 0.9256 | 1.8988
Epoch 45/300, seasonal_2 Loss: 0.9247 | 1.9036
Epoch 46/300, seasonal_2 Loss: 0.9241 | 1.9049
Epoch 47/300, seasonal_2 Loss: 0.9238 | 1.9092
Epoch 48/300, seasonal_2 Loss: 0.9235 | 1.9145
Epoch 49/300, seasonal_2 Loss: 0.9230 | 1.9173
Epoch 50/300, seasonal_2 Loss: 0.9225 | 1.9211
Epoch 51/300, seasonal_2 Loss: 0.9222 | 1.9252
Epoch 52/300, seasonal_2 Loss: 0.9219 | 1.9276
Epoch 53/300, seasonal_2 Loss: 0.9217 | 1.9275
Epoch 54/300, seasonal_2 Loss: 0.9214 | 1.9288
Epoch 55/300, seasonal_2 Loss: 0.9213 | 1.9317
Epoch 56/300, seasonal_2 Loss: 0.9211 | 1.9334
Epoch 57/300, seasonal_2 Loss: 0.9207 | 1.9362
Epoch 58/300, seasonal_2 Loss: 0.9204 | 1.9396
Epoch 59/300, seasonal_2 Loss: 0.9202 | 1.9421
Epoch 60/300, seasonal_2 Loss: 0.9201 | 1.9430
Epoch 61/300, seasonal_2 Loss: 0.9198 | 1.9444
Epoch 62/300, seasonal_2 Loss: 0.9196 | 1.9461
Epoch 63/300, seasonal_2 Loss: 0.9196 | 1.9472
Epoch 64/300, seasonal_2 Loss: 0.9192 | 1.9489
Epoch 65/300, seasonal_2 Loss: 0.9190 | 1.9511
Epoch 66/300, seasonal_2 Loss: 0.9189 | 1.9528
Epoch 67/300, seasonal_2 Loss: 0.9189 | 1.9539
Epoch 68/300, seasonal_2 Loss: 0.9185 | 1.9552
Epoch 69/300, seasonal_2 Loss: 0.9183 | 1.9570
Epoch 70/300, seasonal_2 Loss: 0.9183 | 1.9584
Epoch 71/300, seasonal_2 Loss: 0.9179 | 1.9599
Epoch 72/300, seasonal_2 Loss: 0.9178 | 1.9615
Epoch 73/300, seasonal_2 Loss: 0.9177 | 1.9627
Epoch 74/300, seasonal_2 Loss: 0.9178 | 1.9634
Epoch 75/300, seasonal_2 Loss: 0.9175 | 1.9642
Epoch 76/300, seasonal_2 Loss: 0.9174 | 1.9651
Epoch 77/300, seasonal_2 Loss: 0.9174 | 1.9658
Epoch 78/300, seasonal_2 Loss: 0.9171 | 1.9667
Epoch 79/300, seasonal_2 Loss: 0.9171 | 1.9677
Epoch 80/300, seasonal_2 Loss: 0.9170 | 1.9685
Epoch 81/300, seasonal_2 Loss: 0.9170 | 1.9691
Epoch 82/300, seasonal_2 Loss: 0.9168 | 1.9697
Epoch 83/300, seasonal_2 Loss: 0.9168 | 1.9703
Epoch 84/300, seasonal_2 Loss: 0.9168 | 1.9709
Epoch 85/300, seasonal_2 Loss: 0.9166 | 1.9715
Epoch 86/300, seasonal_2 Loss: 0.9165 | 1.9721
Epoch 87/300, seasonal_2 Loss: 0.9165 | 1.9727
Epoch 88/300, seasonal_2 Loss: 0.9165 | 1.9731
Epoch 89/300, seasonal_2 Loss: 0.9163 | 1.9735
Epoch 90/300, seasonal_2 Loss: 0.9163 | 1.9740
Epoch 91/300, seasonal_2 Loss: 0.9163 | 1.9745
Epoch 92/300, seasonal_2 Loss: 0.9161 | 1.9749
Epoch 93/300, seasonal_2 Loss: 0.9161 | 1.9754
Epoch 94/300, seasonal_2 Loss: 0.9161 | 1.9758
Epoch 95/300, seasonal_2 Loss: 0.9161 | 1.9761
Epoch 96/300, seasonal_2 Loss: 0.9159 | 1.9765
Epoch 97/300, seasonal_2 Loss: 0.9159 | 1.9768
Epoch 98/300, seasonal_2 Loss: 0.9159 | 1.9772
Epoch 99/300, seasonal_2 Loss: 0.9157 | 1.9775
Epoch 100/300, seasonal_2 Loss: 0.9157 | 1.9778
Epoch 101/300, seasonal_2 Loss: 0.9157 | 1.9756
Epoch 102/300, seasonal_2 Loss: 0.9135 | 1.9728
Epoch 103/300, seasonal_2 Loss: 0.9133 | 1.9723
Epoch 104/300, seasonal_2 Loss: 0.9133 | 1.9720
Epoch 105/300, seasonal_2 Loss: 0.9133 | 1.9717
Epoch 106/300, seasonal_2 Loss: 0.9132 | 1.9716
Epoch 107/300, seasonal_2 Loss: 0.9132 | 1.9714
Epoch 108/300, seasonal_2 Loss: 0.9132 | 1.9713
Epoch 109/300, seasonal_2 Loss: 0.9132 | 1.9712
Epoch 110/300, seasonal_2 Loss: 0.9131 | 1.9711
Epoch 111/300, seasonal_2 Loss: 0.9131 | 1.9711
Epoch 112/300, seasonal_2 Loss: 0.9131 | 1.9710
Epoch 113/300, seasonal_2 Loss: 0.9130 | 1.9710
Epoch 114/300, seasonal_2 Loss: 0.9129 | 1.9709
Epoch 115/300, seasonal_2 Loss: 0.9130 | 1.9709
Epoch 116/300, seasonal_2 Loss: 0.9129 | 1.9709
Epoch 117/300, seasonal_2 Loss: 0.9129 | 1.9709
Epoch 118/300, seasonal_2 Loss: 0.9129 | 1.9709
Epoch 119/300, seasonal_2 Loss: 0.9129 | 1.9709
Epoch 120/300, seasonal_2 Loss: 0.9128 | 1.9709
Epoch 121/300, seasonal_2 Loss: 0.9129 | 1.9710
Epoch 122/300, seasonal_2 Loss: 0.9129 | 1.9710
Epoch 123/300, seasonal_2 Loss: 0.9129 | 1.9711
Epoch 124/300, seasonal_2 Loss: 0.9128 | 1.9711
Epoch 125/300, seasonal_2 Loss: 0.9128 | 1.9712
Epoch 126/300, seasonal_2 Loss: 0.9128 | 1.9712
Epoch 127/300, seasonal_2 Loss: 0.9127 | 1.9713
Epoch 128/300, seasonal_2 Loss: 0.9127 | 1.9713
Epoch 129/300, seasonal_2 Loss: 0.9127 | 1.9714
Epoch 130/300, seasonal_2 Loss: 0.9127 | 1.9714
Epoch 131/300, seasonal_2 Loss: 0.9127 | 1.9714
Epoch 132/300, seasonal_2 Loss: 0.9127 | 1.9715
Epoch 133/300, seasonal_2 Loss: 0.9127 | 1.9715
Epoch 134/300, seasonal_2 Loss: 0.9126 | 1.9716
Epoch 135/300, seasonal_2 Loss: 0.9126 | 1.9716
Epoch 136/300, seasonal_2 Loss: 0.9126 | 1.9717
Epoch 137/300, seasonal_2 Loss: 0.9126 | 1.9717
Epoch 138/300, seasonal_2 Loss: 0.9126 | 1.9717
Epoch 139/300, seasonal_2 Loss: 0.9126 | 1.9718
Epoch 140/300, seasonal_2 Loss: 0.9126 | 1.9718
Epoch 141/300, seasonal_2 Loss: 0.9125 | 1.9719
Epoch 142/300, seasonal_2 Loss: 0.9125 | 1.9719
Epoch 143/300, seasonal_2 Loss: 0.9125 | 1.9719
Epoch 144/300, seasonal_2 Loss: 0.9125 | 1.9720
Epoch 145/300, seasonal_2 Loss: 0.9125 | 1.9720
Epoch 146/300, seasonal_2 Loss: 0.9125 | 1.9720
Epoch 147/300, seasonal_2 Loss: 0.9125 | 1.9721
Epoch 148/300, seasonal_2 Loss: 0.9125 | 1.9721
Epoch 149/300, seasonal_2 Loss: 0.9125 | 1.9721
Epoch 150/300, seasonal_2 Loss: 0.9125 | 1.9722
Epoch 151/300, seasonal_2 Loss: 0.9125 | 1.9722
Epoch 152/300, seasonal_2 Loss: 0.9124 | 1.9722
Epoch 153/300, seasonal_2 Loss: 0.9124 | 1.9723
Epoch 154/300, seasonal_2 Loss: 0.9124 | 1.9723
Epoch 155/300, seasonal_2 Loss: 0.9124 | 1.9723
Epoch 156/300, seasonal_2 Loss: 0.9124 | 1.9723
Epoch 157/300, seasonal_2 Loss: 0.9124 | 1.9724
Epoch 158/300, seasonal_2 Loss: 0.9124 | 1.9724
Epoch 159/300, seasonal_2 Loss: 0.9124 | 1.9724
Epoch 160/300, seasonal_2 Loss: 0.9124 | 1.9724
Epoch 161/300, seasonal_2 Loss: 0.9124 | 1.9725
Epoch 162/300, seasonal_2 Loss: 0.9124 | 1.9725
Epoch 163/300, seasonal_2 Loss: 0.9124 | 1.9725
Epoch 164/300, seasonal_2 Loss: 0.9124 | 1.9725
Epoch 165/300, seasonal_2 Loss: 0.9124 | 1.9725
Epoch 166/300, seasonal_2 Loss: 0.9123 | 1.9726
Epoch 167/300, seasonal_2 Loss: 0.9123 | 1.9726
Epoch 168/300, seasonal_2 Loss: 0.9123 | 1.9726
Epoch 169/300, seasonal_2 Loss: 0.9123 | 1.9726
Epoch 170/300, seasonal_2 Loss: 0.9123 | 1.9726
Epoch 171/300, seasonal_2 Loss: 0.9123 | 1.9726
Epoch 172/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 173/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 174/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 175/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 176/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 177/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 178/300, seasonal_2 Loss: 0.9123 | 1.9727
Epoch 179/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 180/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 181/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 182/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 183/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 184/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 185/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 186/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 187/300, seasonal_2 Loss: 0.9123 | 1.9728
Epoch 188/300, seasonal_2 Loss: 0.9123 | 1.9729
Epoch 189/300, seasonal_2 Loss: 0.9123 | 1.9729
Epoch 190/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 191/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 192/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 193/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 194/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 195/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 196/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 197/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 198/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 199/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 200/300, seasonal_2 Loss: 0.9122 | 1.9729
Epoch 201/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 202/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 203/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 204/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 205/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 206/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 207/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 208/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 209/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 210/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 211/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 212/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 213/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 214/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 215/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 216/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 217/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 218/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 219/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 220/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 221/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 222/300, seasonal_2 Loss: 0.9122 | 1.9730
Epoch 223/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 224/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 225/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 226/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 227/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 228/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 229/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 230/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 231/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 232/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 233/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 234/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 235/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 236/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 237/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 238/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 239/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 240/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 241/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 242/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 243/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 244/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 245/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 246/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 247/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 248/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 249/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 250/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 251/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 252/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 253/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 254/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 255/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 256/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 257/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 258/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 259/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 260/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 261/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 262/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 263/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 264/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 265/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 266/300, seasonal_2 Loss: 0.9122 | 1.9731
Epoch 267/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 268/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 269/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 270/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 271/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 272/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 273/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 274/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 275/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 276/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 277/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 278/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 279/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 280/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 281/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 282/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 283/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 284/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 285/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 286/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 287/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 288/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 289/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 290/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 291/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 292/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 293/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 294/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 295/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 296/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 297/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 298/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 299/300, seasonal_2 Loss: 0.9121 | 1.9731
Epoch 300/300, seasonal_2 Loss: 0.9121 | 1.9731
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.9372368203203963, 'learning_rate': 0.000761201312494682, 'batch_size': 80, 'step_size': 11, 'gamma': 0.9627776108089214}
Epoch 1/300, seasonal_3 Loss: 1.8734 | 1.5427
Epoch 2/300, seasonal_3 Loss: 0.9389 | 1.5506
Epoch 3/300, seasonal_3 Loss: 0.9515 | 1.5013
Epoch 4/300, seasonal_3 Loss: 0.9672 | 1.4198
Epoch 5/300, seasonal_3 Loss: 1.0095 | 1.4104
Epoch 6/300, seasonal_3 Loss: 1.0311 | 1.4271
Epoch 7/300, seasonal_3 Loss: 0.9647 | 1.4216
Epoch 8/300, seasonal_3 Loss: 0.9726 | 1.4217
Epoch 9/300, seasonal_3 Loss: 0.9624 | 1.4615
Epoch 10/300, seasonal_3 Loss: 0.9488 | 1.4227
Epoch 11/300, seasonal_3 Loss: 0.9647 | 1.4392
Epoch 12/300, seasonal_3 Loss: 0.9886 | 1.5137
Epoch 13/300, seasonal_3 Loss: 0.9401 | 1.4646
Epoch 14/300, seasonal_3 Loss: 0.9579 | 1.4274
Epoch 15/300, seasonal_3 Loss: 1.0364 | 1.4194
Epoch 16/300, seasonal_3 Loss: 1.1503 | 1.4870
Epoch 17/300, seasonal_3 Loss: 1.1440 | 1.6257
Epoch 18/300, seasonal_3 Loss: 1.0802 | 1.7247
Epoch 19/300, seasonal_3 Loss: 1.0332 | 1.6693
Epoch 20/300, seasonal_3 Loss: 1.0359 | 1.7607
Epoch 21/300, seasonal_3 Loss: 1.0208 | 1.7843
Epoch 22/300, seasonal_3 Loss: 1.0108 | 1.7968
Epoch 23/300, seasonal_3 Loss: 1.0045 | 1.8185
Epoch 24/300, seasonal_3 Loss: 0.9970 | 1.8255
Epoch 25/300, seasonal_3 Loss: 0.9939 | 1.8340
Epoch 26/300, seasonal_3 Loss: 0.9905 | 1.8415
Epoch 27/300, seasonal_3 Loss: 0.9874 | 1.8486
Epoch 28/300, seasonal_3 Loss: 0.9845 | 1.8550
Epoch 29/300, seasonal_3 Loss: 0.9808 | 1.8660
Epoch 30/300, seasonal_3 Loss: 0.9774 | 1.8704
Epoch 31/300, seasonal_3 Loss: 0.9755 | 1.8748
Epoch 32/300, seasonal_3 Loss: 0.9736 | 1.8792
Epoch 33/300, seasonal_3 Loss: 0.9718 | 1.8833
Epoch 34/300, seasonal_3 Loss: 0.9692 | 1.8902
Epoch 35/300, seasonal_3 Loss: 0.9671 | 1.8936
Epoch 36/300, seasonal_3 Loss: 0.9658 | 1.8965
Epoch 37/300, seasonal_3 Loss: 0.9645 | 1.8995
Epoch 38/300, seasonal_3 Loss: 0.9632 | 1.9023
Epoch 39/300, seasonal_3 Loss: 0.9620 | 1.9050
Epoch 40/300, seasonal_3 Loss: 0.9601 | 1.9093
Epoch 41/300, seasonal_3 Loss: 0.9588 | 1.9121
Epoch 42/300, seasonal_3 Loss: 0.9578 | 1.9141
Epoch 43/300, seasonal_3 Loss: 0.9569 | 1.9161
Epoch 44/300, seasonal_3 Loss: 0.9560 | 1.9181
Epoch 45/300, seasonal_3 Loss: 0.9546 | 1.9210
Epoch 46/300, seasonal_3 Loss: 0.9537 | 1.9232
Epoch 47/300, seasonal_3 Loss: 0.9529 | 1.9248
Epoch 48/300, seasonal_3 Loss: 0.9522 | 1.9262
Epoch 49/300, seasonal_3 Loss: 0.9515 | 1.9277
Epoch 50/300, seasonal_3 Loss: 0.9508 | 1.9291
Epoch 51/300, seasonal_3 Loss: 0.9497 | 1.9311
Epoch 52/300, seasonal_3 Loss: 0.9491 | 1.9327
Epoch 53/300, seasonal_3 Loss: 0.9485 | 1.9339
Epoch 54/300, seasonal_3 Loss: 0.9480 | 1.9350
Epoch 55/300, seasonal_3 Loss: 0.9475 | 1.9361
Epoch 56/300, seasonal_3 Loss: 0.9466 | 1.9374
Epoch 57/300, seasonal_3 Loss: 0.9462 | 1.9387
Epoch 58/300, seasonal_3 Loss: 0.9457 | 1.9396
Epoch 59/300, seasonal_3 Loss: 0.9453 | 1.9405
Epoch 60/300, seasonal_3 Loss: 0.9449 | 1.9413
Epoch 61/300, seasonal_3 Loss: 0.9445 | 1.9420
Epoch 62/300, seasonal_3 Loss: 0.9438 | 1.9429
Epoch 63/300, seasonal_3 Loss: 0.9435 | 1.9438
Epoch 64/300, seasonal_3 Loss: 0.9432 | 1.9445
Epoch 65/300, seasonal_3 Loss: 0.9428 | 1.9451
Epoch 66/300, seasonal_3 Loss: 0.9425 | 1.9457
Epoch 67/300, seasonal_3 Loss: 0.9420 | 1.9463
Epoch 68/300, seasonal_3 Loss: 0.9418 | 1.9469
Epoch 69/300, seasonal_3 Loss: 0.9415 | 1.9474
Epoch 70/300, seasonal_3 Loss: 0.9413 | 1.9478
Epoch 71/300, seasonal_3 Loss: 0.9411 | 1.9482
Epoch 72/300, seasonal_3 Loss: 0.9409 | 1.9484
Epoch 73/300, seasonal_3 Loss: 0.9406 | 1.9487
Epoch 74/300, seasonal_3 Loss: 0.9404 | 1.9490
Epoch 75/300, seasonal_3 Loss: 0.9402 | 1.9492
Epoch 76/300, seasonal_3 Loss: 0.9400 | 1.9495
Epoch 77/300, seasonal_3 Loss: 0.9399 | 1.9497
Epoch 78/300, seasonal_3 Loss: 0.9397 | 1.9499
Epoch 79/300, seasonal_3 Loss: 0.9395 | 1.9500
Epoch 80/300, seasonal_3 Loss: 0.9393 | 1.9502
Epoch 81/300, seasonal_3 Loss: 0.9392 | 1.9504
Epoch 82/300, seasonal_3 Loss: 0.9391 | 1.9505
Epoch 83/300, seasonal_3 Loss: 0.9390 | 1.9506
Epoch 84/300, seasonal_3 Loss: 0.9388 | 1.9507
Epoch 85/300, seasonal_3 Loss: 0.9386 | 1.9509
Epoch 86/300, seasonal_3 Loss: 0.9385 | 1.9510
Epoch 87/300, seasonal_3 Loss: 0.9384 | 1.9511
Epoch 88/300, seasonal_3 Loss: 0.9383 | 1.9512
Epoch 89/300, seasonal_3 Loss: 0.9381 | 1.9513
Epoch 90/300, seasonal_3 Loss: 0.9380 | 1.9514
Epoch 91/300, seasonal_3 Loss: 0.9379 | 1.9515
Epoch 92/300, seasonal_3 Loss: 0.9379 | 1.9516
Epoch 93/300, seasonal_3 Loss: 0.9378 | 1.9516
Epoch 94/300, seasonal_3 Loss: 0.9378 | 1.9516
Epoch 95/300, seasonal_3 Loss: 0.9377 | 1.9516
Epoch 96/300, seasonal_3 Loss: 0.9377 | 1.9516
Epoch 97/300, seasonal_3 Loss: 0.9377 | 1.9516
Epoch 98/300, seasonal_3 Loss: 0.9377 | 1.9515
Epoch 99/300, seasonal_3 Loss: 0.9378 | 1.9515
Epoch 100/300, seasonal_3 Loss: 0.9378 | 1.9514
Epoch 101/300, seasonal_3 Loss: 0.9376 | 1.9513
Epoch 102/300, seasonal_3 Loss: 0.9375 | 1.9512
Epoch 103/300, seasonal_3 Loss: 0.9374 | 1.9512
Epoch 104/300, seasonal_3 Loss: 0.9374 | 1.9512
Epoch 105/300, seasonal_3 Loss: 0.9374 | 1.9512
Epoch 106/300, seasonal_3 Loss: 0.9373 | 1.9512
Epoch 107/300, seasonal_3 Loss: 0.9372 | 1.9512
Epoch 108/300, seasonal_3 Loss: 0.9372 | 1.9512
Epoch 109/300, seasonal_3 Loss: 0.9372 | 1.9512
Epoch 110/300, seasonal_3 Loss: 0.9372 | 1.9512
Epoch 111/300, seasonal_3 Loss: 0.9372 | 1.9513
Epoch 112/300, seasonal_3 Loss: 0.9371 | 1.9513
Epoch 113/300, seasonal_3 Loss: 0.9371 | 1.9513
Epoch 114/300, seasonal_3 Loss: 0.9370 | 1.9513
Epoch 115/300, seasonal_3 Loss: 0.9371 | 1.9513
Epoch 116/300, seasonal_3 Loss: 0.9371 | 1.9513
Epoch 117/300, seasonal_3 Loss: 0.9371 | 1.9514
Epoch 118/300, seasonal_3 Loss: 0.9370 | 1.9514
Epoch 119/300, seasonal_3 Loss: 0.9370 | 1.9514
Epoch 120/300, seasonal_3 Loss: 0.9370 | 1.9514
Epoch 121/300, seasonal_3 Loss: 0.9370 | 1.9514
Epoch 122/300, seasonal_3 Loss: 0.9370 | 1.9515
Epoch 123/300, seasonal_3 Loss: 0.9369 | 1.9515
Epoch 124/300, seasonal_3 Loss: 0.9368 | 1.9515
Epoch 125/300, seasonal_3 Loss: 0.9368 | 1.9516
Epoch 126/300, seasonal_3 Loss: 0.9368 | 1.9516
Epoch 127/300, seasonal_3 Loss: 0.9369 | 1.9517
Epoch 128/300, seasonal_3 Loss: 0.9369 | 1.9518
Epoch 129/300, seasonal_3 Loss: 0.9369 | 1.9518
Epoch 130/300, seasonal_3 Loss: 0.9368 | 1.9518
Epoch 131/300, seasonal_3 Loss: 0.9368 | 1.9519
Epoch 132/300, seasonal_3 Loss: 0.9368 | 1.9519
Epoch 133/300, seasonal_3 Loss: 0.9368 | 1.9520
Epoch 134/300, seasonal_3 Loss: 0.9367 | 1.9521
Epoch 135/300, seasonal_3 Loss: 0.9367 | 1.9521
Epoch 136/300, seasonal_3 Loss: 0.9367 | 1.9522
Epoch 137/300, seasonal_3 Loss: 0.9367 | 1.9523
Epoch 138/300, seasonal_3 Loss: 0.9368 | 1.9524
Epoch 139/300, seasonal_3 Loss: 0.9367 | 1.9525
Epoch 140/300, seasonal_3 Loss: 0.9367 | 1.9525
Epoch 141/300, seasonal_3 Loss: 0.9366 | 1.9526
Epoch 142/300, seasonal_3 Loss: 0.9367 | 1.9527
Epoch 143/300, seasonal_3 Loss: 0.9368 | 1.9528
Epoch 144/300, seasonal_3 Loss: 0.9368 | 1.9529
Epoch 145/300, seasonal_3 Loss: 0.9367 | 1.9529
Epoch 146/300, seasonal_3 Loss: 0.9367 | 1.9530
Epoch 147/300, seasonal_3 Loss: 0.9367 | 1.9530
Epoch 148/300, seasonal_3 Loss: 0.9368 | 1.9532
Epoch 149/300, seasonal_3 Loss: 0.9369 | 1.9532
Epoch 150/300, seasonal_3 Loss: 0.9371 | 1.9535
Epoch 151/300, seasonal_3 Loss: 0.9369 | 1.9534
Epoch 152/300, seasonal_3 Loss: 0.9370 | 1.9533
Epoch 153/300, seasonal_3 Loss: 0.9372 | 1.9536
Epoch 154/300, seasonal_3 Loss: 0.9371 | 1.9537
Epoch 155/300, seasonal_3 Loss: 0.9367 | 1.9536
Epoch 156/300, seasonal_3 Loss: 0.9365 | 1.9535
Epoch 157/300, seasonal_3 Loss: 0.9364 | 1.9536
Epoch 158/300, seasonal_3 Loss: 0.9364 | 1.9537
Epoch 159/300, seasonal_3 Loss: 0.9364 | 1.9538
Epoch 160/300, seasonal_3 Loss: 0.9365 | 1.9539
Epoch 161/300, seasonal_3 Loss: 0.9365 | 1.9541
Epoch 162/300, seasonal_3 Loss: 0.9365 | 1.9541
Epoch 163/300, seasonal_3 Loss: 0.9365 | 1.9542
Epoch 164/300, seasonal_3 Loss: 0.9365 | 1.9543
Epoch 165/300, seasonal_3 Loss: 0.9366 | 1.9544
Epoch 166/300, seasonal_3 Loss: 0.9367 | 1.9546
Epoch 167/300, seasonal_3 Loss: 0.9367 | 1.9547
Epoch 168/300, seasonal_3 Loss: 0.9367 | 1.9547
Epoch 169/300, seasonal_3 Loss: 0.9367 | 1.9548
Epoch 170/300, seasonal_3 Loss: 0.9367 | 1.9549
Epoch 171/300, seasonal_3 Loss: 0.9368 | 1.9550
Epoch 172/300, seasonal_3 Loss: 0.9368 | 1.9551
Epoch 173/300, seasonal_3 Loss: 0.9367 | 1.9551
Epoch 174/300, seasonal_3 Loss: 0.9366 | 1.9551
Epoch 175/300, seasonal_3 Loss: 0.9366 | 1.9552
Epoch 176/300, seasonal_3 Loss: 0.9366 | 1.9552
Epoch 177/300, seasonal_3 Loss: 0.9365 | 1.9553
Epoch 178/300, seasonal_3 Loss: 0.9365 | 1.9553
Epoch 179/300, seasonal_3 Loss: 0.9364 | 1.9554
Epoch 180/300, seasonal_3 Loss: 0.9364 | 1.9554
Epoch 181/300, seasonal_3 Loss: 0.9365 | 1.9555
Epoch 182/300, seasonal_3 Loss: 0.9365 | 1.9556
Epoch 183/300, seasonal_3 Loss: 0.9366 | 1.9558
Epoch 184/300, seasonal_3 Loss: 0.9365 | 1.9558
Epoch 185/300, seasonal_3 Loss: 0.9365 | 1.9559
Epoch 186/300, seasonal_3 Loss: 0.9366 | 1.9559
Epoch 187/300, seasonal_3 Loss: 0.9367 | 1.9560
Epoch 188/300, seasonal_3 Loss: 0.9368 | 1.9561
Epoch 189/300, seasonal_3 Loss: 0.9369 | 1.9562
Epoch 190/300, seasonal_3 Loss: 0.9369 | 1.9562
Epoch 191/300, seasonal_3 Loss: 0.9369 | 1.9563
Epoch 192/300, seasonal_3 Loss: 0.9369 | 1.9563
Epoch 193/300, seasonal_3 Loss: 0.9367 | 1.9563
Epoch 194/300, seasonal_3 Loss: 0.9365 | 1.9563
Epoch 195/300, seasonal_3 Loss: 0.9363 | 1.9562
Epoch 196/300, seasonal_3 Loss: 0.9362 | 1.9562
Epoch 197/300, seasonal_3 Loss: 0.9362 | 1.9562
Epoch 198/300, seasonal_3 Loss: 0.9361 | 1.9563
Epoch 199/300, seasonal_3 Loss: 0.9361 | 1.9564
Epoch 200/300, seasonal_3 Loss: 0.9362 | 1.9565
Epoch 201/300, seasonal_3 Loss: 0.9362 | 1.9566
Epoch 202/300, seasonal_3 Loss: 0.9364 | 1.9566
Epoch 203/300, seasonal_3 Loss: 0.9366 | 1.9568
Epoch 204/300, seasonal_3 Loss: 0.9370 | 1.9569
Epoch 205/300, seasonal_3 Loss: 0.9373 | 1.9572
Epoch 206/300, seasonal_3 Loss: 0.9371 | 1.9571
Epoch 207/300, seasonal_3 Loss: 0.9369 | 1.9571
Epoch 208/300, seasonal_3 Loss: 0.9366 | 1.9569
Epoch 209/300, seasonal_3 Loss: 0.9365 | 1.9569
Epoch 210/300, seasonal_3 Loss: 0.9363 | 1.9569
Epoch 211/300, seasonal_3 Loss: 0.9362 | 1.9568
Epoch 212/300, seasonal_3 Loss: 0.9362 | 1.9569
Epoch 213/300, seasonal_3 Loss: 0.9362 | 1.9569
Epoch 214/300, seasonal_3 Loss: 0.9362 | 1.9570
Epoch 215/300, seasonal_3 Loss: 0.9363 | 1.9571
Epoch 216/300, seasonal_3 Loss: 0.9363 | 1.9571
Epoch 217/300, seasonal_3 Loss: 0.9364 | 1.9572
Epoch 218/300, seasonal_3 Loss: 0.9364 | 1.9573
Epoch 219/300, seasonal_3 Loss: 0.9365 | 1.9573
Epoch 220/300, seasonal_3 Loss: 0.9366 | 1.9574
Epoch 221/300, seasonal_3 Loss: 0.9366 | 1.9575
Epoch 222/300, seasonal_3 Loss: 0.9366 | 1.9575
Epoch 223/300, seasonal_3 Loss: 0.9366 | 1.9575
Epoch 224/300, seasonal_3 Loss: 0.9365 | 1.9575
Epoch 225/300, seasonal_3 Loss: 0.9366 | 1.9575
Epoch 226/300, seasonal_3 Loss: 0.9366 | 1.9575
Epoch 227/300, seasonal_3 Loss: 0.9365 | 1.9576
Epoch 228/300, seasonal_3 Loss: 0.9363 | 1.9575
Epoch 229/300, seasonal_3 Loss: 0.9363 | 1.9575
Epoch 230/300, seasonal_3 Loss: 0.9362 | 1.9575
Epoch 231/300, seasonal_3 Loss: 0.9362 | 1.9575
Epoch 232/300, seasonal_3 Loss: 0.9362 | 1.9576
Epoch 233/300, seasonal_3 Loss: 0.9361 | 1.9576
Epoch 234/300, seasonal_3 Loss: 0.9361 | 1.9576
Epoch 235/300, seasonal_3 Loss: 0.9361 | 1.9577
Epoch 236/300, seasonal_3 Loss: 0.9362 | 1.9577
Epoch 237/300, seasonal_3 Loss: 0.9362 | 1.9578
Epoch 238/300, seasonal_3 Loss: 0.9363 | 1.9579
Epoch 239/300, seasonal_3 Loss: 0.9363 | 1.9579
Epoch 240/300, seasonal_3 Loss: 0.9363 | 1.9579
Epoch 241/300, seasonal_3 Loss: 0.9364 | 1.9580
Epoch 242/300, seasonal_3 Loss: 0.9365 | 1.9580
Epoch 243/300, seasonal_3 Loss: 0.9365 | 1.9581
Epoch 244/300, seasonal_3 Loss: 0.9364 | 1.9581
Epoch 245/300, seasonal_3 Loss: 0.9364 | 1.9580
Epoch 246/300, seasonal_3 Loss: 0.9363 | 1.9581
Epoch 247/300, seasonal_3 Loss: 0.9363 | 1.9581
Epoch 248/300, seasonal_3 Loss: 0.9363 | 1.9581
Epoch 249/300, seasonal_3 Loss: 0.9362 | 1.9581
Epoch 250/300, seasonal_3 Loss: 0.9362 | 1.9581
Epoch 251/300, seasonal_3 Loss: 0.9361 | 1.9581
Epoch 252/300, seasonal_3 Loss: 0.9361 | 1.9581
Epoch 253/300, seasonal_3 Loss: 0.9362 | 1.9581
Epoch 254/300, seasonal_3 Loss: 0.9362 | 1.9582
Epoch 255/300, seasonal_3 Loss: 0.9362 | 1.9582
Epoch 256/300, seasonal_3 Loss: 0.9362 | 1.9582
Epoch 257/300, seasonal_3 Loss: 0.9363 | 1.9583
Epoch 258/300, seasonal_3 Loss: 0.9365 | 1.9583
Epoch 259/300, seasonal_3 Loss: 0.9367 | 1.9585
Epoch 260/300, seasonal_3 Loss: 0.9367 | 1.9586
Epoch 261/300, seasonal_3 Loss: 0.9365 | 1.9585
Epoch 262/300, seasonal_3 Loss: 0.9363 | 1.9584
Epoch 263/300, seasonal_3 Loss: 0.9362 | 1.9583
Epoch 264/300, seasonal_3 Loss: 0.9361 | 1.9583
Epoch 265/300, seasonal_3 Loss: 0.9361 | 1.9583
Epoch 266/300, seasonal_3 Loss: 0.9360 | 1.9583
Epoch 267/300, seasonal_3 Loss: 0.9360 | 1.9583
Epoch 268/300, seasonal_3 Loss: 0.9360 | 1.9583
Epoch 269/300, seasonal_3 Loss: 0.9360 | 1.9584
Epoch 270/300, seasonal_3 Loss: 0.9361 | 1.9584
Epoch 271/300, seasonal_3 Loss: 0.9361 | 1.9585
Epoch 272/300, seasonal_3 Loss: 0.9362 | 1.9585
Epoch 273/300, seasonal_3 Loss: 0.9362 | 1.9585
Epoch 274/300, seasonal_3 Loss: 0.9363 | 1.9586
Epoch 275/300, seasonal_3 Loss: 0.9363 | 1.9586
Epoch 276/300, seasonal_3 Loss: 0.9363 | 1.9587
Epoch 277/300, seasonal_3 Loss: 0.9362 | 1.9586
Epoch 278/300, seasonal_3 Loss: 0.9361 | 1.9586
Epoch 279/300, seasonal_3 Loss: 0.9361 | 1.9586
Epoch 280/300, seasonal_3 Loss: 0.9361 | 1.9586
Epoch 281/300, seasonal_3 Loss: 0.9361 | 1.9586
Epoch 282/300, seasonal_3 Loss: 0.9360 | 1.9587
Epoch 283/300, seasonal_3 Loss: 0.9360 | 1.9587
Epoch 284/300, seasonal_3 Loss: 0.9360 | 1.9587
Epoch 285/300, seasonal_3 Loss: 0.9360 | 1.9587
Epoch 286/300, seasonal_3 Loss: 0.9361 | 1.9587
Epoch 287/300, seasonal_3 Loss: 0.9361 | 1.9588
Epoch 288/300, seasonal_3 Loss: 0.9362 | 1.9588
Epoch 289/300, seasonal_3 Loss: 0.9362 | 1.9589
Epoch 290/300, seasonal_3 Loss: 0.9363 | 1.9589
Epoch 291/300, seasonal_3 Loss: 0.9364 | 1.9589
Epoch 292/300, seasonal_3 Loss: 0.9364 | 1.9590
Epoch 293/300, seasonal_3 Loss: 0.9363 | 1.9590
Epoch 294/300, seasonal_3 Loss: 0.9361 | 1.9589
Epoch 295/300, seasonal_3 Loss: 0.9360 | 1.9588
Epoch 296/300, seasonal_3 Loss: 0.9360 | 1.9588
Epoch 297/300, seasonal_3 Loss: 0.9360 | 1.9588
Epoch 298/300, seasonal_3 Loss: 0.9360 | 1.9589
Epoch 299/300, seasonal_3 Loss: 0.9360 | 1.9589
Epoch 300/300, seasonal_3 Loss: 0.9359 | 1.9589
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.8005807282376157, 'learning_rate': 0.0004020002553249557, 'batch_size': 68, 'step_size': 4, 'gamma': 0.9657770142884828}
Epoch 1/300, resid Loss: 1.1491 | 0.2443
Epoch 2/300, resid Loss: 0.2052 | 0.1128
Epoch 3/300, resid Loss: 0.1855 | 0.1024
Epoch 4/300, resid Loss: 0.2137 | 0.1340
Epoch 5/300, resid Loss: 0.1649 | 0.1423
Epoch 6/300, resid Loss: 0.1471 | 0.1013
Epoch 7/300, resid Loss: 0.1805 | 0.1238
Epoch 8/300, resid Loss: 0.1760 | 0.1169
Epoch 9/300, resid Loss: 0.1812 | 0.1852
Epoch 10/300, resid Loss: 0.1689 | 0.1140
Epoch 11/300, resid Loss: 0.1620 | 0.0810
Epoch 12/300, resid Loss: 0.1382 | 0.0969
Epoch 13/300, resid Loss: 0.1305 | 0.0791
Epoch 14/300, resid Loss: 0.1565 | 0.1030
Epoch 15/300, resid Loss: 0.1290 | 0.1048
Epoch 16/300, resid Loss: 0.1470 | 0.2348
Epoch 17/300, resid Loss: 0.1779 | 0.1917
Epoch 18/300, resid Loss: 0.1680 | 0.1289
Epoch 19/300, resid Loss: 0.1740 | 0.1000
Epoch 20/300, resid Loss: 0.1380 | 0.0875
Epoch 21/300, resid Loss: 0.1651 | 0.0709
Epoch 22/300, resid Loss: 0.1411 | 0.0975
Epoch 23/300, resid Loss: 0.1475 | 0.0846
Epoch 24/300, resid Loss: 0.1241 | 0.0684
Epoch 25/300, resid Loss: 0.1300 | 0.0602
Epoch 26/300, resid Loss: 0.1241 | 0.0621
Epoch 27/300, resid Loss: 0.1180 | 0.0665
Epoch 28/300, resid Loss: 0.1371 | 0.0828
Epoch 29/300, resid Loss: 0.1443 | 0.0628
Epoch 30/300, resid Loss: 0.1228 | 0.0549
Epoch 31/300, resid Loss: 0.1070 | 0.0549
Epoch 32/300, resid Loss: 0.0972 | 0.0516
Epoch 33/300, resid Loss: 0.0901 | 0.0454
Epoch 34/300, resid Loss: 0.0868 | 0.0417
Epoch 35/300, resid Loss: 0.0814 | 0.0414
Epoch 36/300, resid Loss: 0.0825 | 0.0431
Epoch 37/300, resid Loss: 0.0799 | 0.0513
Epoch 38/300, resid Loss: 0.0917 | 0.0541
Epoch 39/300, resid Loss: 0.0981 | 0.0693
Epoch 40/300, resid Loss: 0.1192 | 0.0694
Epoch 41/300, resid Loss: 0.0931 | 0.0529
Epoch 42/300, resid Loss: 0.0978 | 0.0508
Epoch 43/300, resid Loss: 0.0892 | 0.0580
Epoch 44/300, resid Loss: 0.0849 | 0.0643
Epoch 45/300, resid Loss: 0.0866 | 0.0485
Epoch 46/300, resid Loss: 0.0831 | 0.0796
Epoch 47/300, resid Loss: 0.0859 | 0.0590
Epoch 48/300, resid Loss: 0.1109 | 0.0528
Epoch 49/300, resid Loss: 0.1028 | 0.1117
Epoch 50/300, resid Loss: 0.0969 | 0.0736
Epoch 51/300, resid Loss: 0.1073 | 0.0722
Epoch 52/300, resid Loss: 0.0994 | 0.0620
Epoch 53/300, resid Loss: 0.1120 | 0.0689
Epoch 54/300, resid Loss: 0.1326 | 0.0934
Epoch 55/300, resid Loss: 0.1098 | 0.0577
Epoch 56/300, resid Loss: 0.1021 | 0.0882
Epoch 57/300, resid Loss: 0.1365 | 0.0472
Epoch 58/300, resid Loss: 0.1011 | 0.0545
Epoch 59/300, resid Loss: 0.1228 | 0.0663
Epoch 60/300, resid Loss: 0.1316 | 0.0613
Epoch 61/300, resid Loss: 0.1157 | 0.0710
Epoch 62/300, resid Loss: 0.1044 | 0.0538
Epoch 63/300, resid Loss: 0.1046 | 0.0621
Epoch 64/300, resid Loss: 0.1049 | 0.0591
Epoch 65/300, resid Loss: 0.1076 | 0.0437
Epoch 66/300, resid Loss: 0.1108 | 0.0613
Epoch 67/300, resid Loss: 0.0949 | 0.0866
Epoch 68/300, resid Loss: 0.0944 | 0.0599
Epoch 69/300, resid Loss: 0.0780 | 0.0727
Epoch 70/300, resid Loss: 0.0736 | 0.0677
Epoch 71/300, resid Loss: 0.0850 | 0.0599
Epoch 72/300, resid Loss: 0.0942 | 0.0643
Epoch 73/300, resid Loss: 0.0760 | 0.0557
Epoch 74/300, resid Loss: 0.0720 | 0.0669
Epoch 75/300, resid Loss: 0.0724 | 0.0559
Epoch 76/300, resid Loss: 0.0758 | 0.0922
Epoch 77/300, resid Loss: 0.0773 | 0.0479
Epoch 78/300, resid Loss: 0.0689 | 0.0409
Epoch 79/300, resid Loss: 0.0673 | 0.0529
Epoch 80/300, resid Loss: 0.0619 | 0.0492
Epoch 81/300, resid Loss: 0.0641 | 0.0575
Epoch 82/300, resid Loss: 0.0598 | 0.0553
Epoch 83/300, resid Loss: 0.0672 | 0.0492
Epoch 84/300, resid Loss: 0.0627 | 0.0649
Epoch 85/300, resid Loss: 0.0637 | 0.0490
Epoch 86/300, resid Loss: 0.0653 | 0.0463
Epoch 87/300, resid Loss: 0.0629 | 0.0457
Epoch 88/300, resid Loss: 0.0609 | 0.0451
Epoch 89/300, resid Loss: 0.0605 | 0.0466
Epoch 90/300, resid Loss: 0.0573 | 0.0532
Epoch 91/300, resid Loss: 0.0576 | 0.0597
Epoch 92/300, resid Loss: 0.0719 | 0.0474
Epoch 93/300, resid Loss: 0.0737 | 0.0409
Epoch 94/300, resid Loss: 0.0666 | 0.0378
Epoch 95/300, resid Loss: 0.0666 | 0.0405
Epoch 96/300, resid Loss: 0.0659 | 0.0443
Epoch 97/300, resid Loss: 0.0661 | 0.0510
Epoch 98/300, resid Loss: 0.0706 | 0.0575
Epoch 99/300, resid Loss: 0.0653 | 0.0481
Epoch 100/300, resid Loss: 0.0724 | 0.0493
Epoch 101/300, resid Loss: 0.0637 | 0.0407
Epoch 102/300, resid Loss: 0.0582 | 0.0458
Epoch 103/300, resid Loss: 0.0632 | 0.0588
Epoch 104/300, resid Loss: 0.0985 | 0.0488
Epoch 105/300, resid Loss: 0.0796 | 0.0533
Epoch 106/300, resid Loss: 0.0833 | 0.0440
Epoch 107/300, resid Loss: 0.0690 | 0.0416
Epoch 108/300, resid Loss: 0.0655 | 0.0401
Epoch 109/300, resid Loss: 0.0642 | 0.0420
Epoch 110/300, resid Loss: 0.0653 | 0.0473
Epoch 111/300, resid Loss: 0.0693 | 0.0544
Epoch 112/300, resid Loss: 0.0794 | 0.0615
Epoch 113/300, resid Loss: 0.0749 | 0.0536
Epoch 114/300, resid Loss: 0.0666 | 0.0506
Epoch 115/300, resid Loss: 0.0658 | 0.0409
Epoch 116/300, resid Loss: 0.0724 | 0.0394
Epoch 117/300, resid Loss: 0.0738 | 0.0399
Epoch 118/300, resid Loss: 0.0676 | 0.0450
Epoch 119/300, resid Loss: 0.0770 | 0.0523
Epoch 120/300, resid Loss: 0.0596 | 0.0463
Epoch 121/300, resid Loss: 0.0579 | 0.0396
Epoch 122/300, resid Loss: 0.0557 | 0.0373
Epoch 123/300, resid Loss: 0.0601 | 0.0434
Epoch 124/300, resid Loss: 0.0547 | 0.0506
Epoch 125/300, resid Loss: 0.0518 | 0.0538
Epoch 126/300, resid Loss: 0.0489 | 0.0438
Epoch 127/300, resid Loss: 0.0499 | 0.0389
Epoch 128/300, resid Loss: 0.0486 | 0.0391
Epoch 129/300, resid Loss: 0.0480 | 0.0418
Epoch 130/300, resid Loss: 0.0480 | 0.0465
Epoch 131/300, resid Loss: 0.0479 | 0.0459
Epoch 132/300, resid Loss: 0.0471 | 0.0468
Epoch 133/300, resid Loss: 0.0471 | 0.0436
Epoch 134/300, resid Loss: 0.0491 | 0.0419
Epoch 135/300, resid Loss: 0.0494 | 0.0393
Epoch 136/300, resid Loss: 0.0485 | 0.0395
Epoch 137/300, resid Loss: 0.0500 | 0.0390
Epoch 138/300, resid Loss: 0.0486 | 0.0438
Epoch 139/300, resid Loss: 0.0483 | 0.0484
Epoch 140/300, resid Loss: 0.0483 | 0.0529
Epoch 141/300, resid Loss: 0.0479 | 0.0422
Epoch 142/300, resid Loss: 0.0510 | 0.0379
Epoch 143/300, resid Loss: 0.0498 | 0.0377
Epoch 144/300, resid Loss: 0.0481 | 0.0458
Epoch 145/300, resid Loss: 0.0495 | 0.0448
Epoch 146/300, resid Loss: 0.0468 | 0.0448
Epoch 147/300, resid Loss: 0.0453 | 0.0427
Epoch 148/300, resid Loss: 0.0473 | 0.0389
Epoch 149/300, resid Loss: 0.0472 | 0.0388
Epoch 150/300, resid Loss: 0.0459 | 0.0393
Epoch 151/300, resid Loss: 0.0455 | 0.0412
Epoch 152/300, resid Loss: 0.0442 | 0.0423
Epoch 153/300, resid Loss: 0.0436 | 0.0447
Epoch 154/300, resid Loss: 0.0436 | 0.0429
Epoch 155/300, resid Loss: 0.0431 | 0.0388
Epoch 156/300, resid Loss: 0.0437 | 0.0368
Epoch 157/300, resid Loss: 0.0425 | 0.0397
Epoch 158/300, resid Loss: 0.0429 | 0.0428
Epoch 159/300, resid Loss: 0.0433 | 0.0447
Epoch 160/300, resid Loss: 0.0422 | 0.0410
Epoch 161/300, resid Loss: 0.0426 | 0.0376
Epoch 162/300, resid Loss: 0.0425 | 0.0379
Epoch 163/300, resid Loss: 0.0416 | 0.0421
Epoch 164/300, resid Loss: 0.0415 | 0.0427
Epoch 165/300, resid Loss: 0.0412 | 0.0412
Epoch 166/300, resid Loss: 0.0408 | 0.0393
Epoch 167/300, resid Loss: 0.0417 | 0.0402
Epoch 168/300, resid Loss: 0.0415 | 0.0403
Epoch 169/300, resid Loss: 0.0415 | 0.0403
Epoch 170/300, resid Loss: 0.0415 | 0.0401
Epoch 171/300, resid Loss: 0.0417 | 0.0407
Epoch 172/300, resid Loss: 0.0409 | 0.0414
Epoch 173/300, resid Loss: 0.0401 | 0.0419
Epoch 174/300, resid Loss: 0.0403 | 0.0393
Epoch 175/300, resid Loss: 0.0401 | 0.0388
Epoch 176/300, resid Loss: 0.0405 | 0.0392
Epoch 177/300, resid Loss: 0.0400 | 0.0427
Epoch 178/300, resid Loss: 0.0400 | 0.0420
Epoch 179/300, resid Loss: 0.0401 | 0.0411
Epoch 180/300, resid Loss: 0.0401 | 0.0391
Epoch 181/300, resid Loss: 0.0404 | 0.0378
Epoch 182/300, resid Loss: 0.0400 | 0.0395
Epoch 183/300, resid Loss: 0.0405 | 0.0426
Epoch 184/300, resid Loss: 0.0400 | 0.0425
Epoch 185/300, resid Loss: 0.0395 | 0.0401
Epoch 186/300, resid Loss: 0.0390 | 0.0383
Epoch 187/300, resid Loss: 0.0389 | 0.0407
Epoch 188/300, resid Loss: 0.0385 | 0.0421
Epoch 189/300, resid Loss: 0.0383 | 0.0414
Epoch 190/300, resid Loss: 0.0379 | 0.0405
Epoch 191/300, resid Loss: 0.0378 | 0.0405
Epoch 192/300, resid Loss: 0.0376 | 0.0414
Epoch 193/300, resid Loss: 0.0374 | 0.0416
Epoch 194/300, resid Loss: 0.0373 | 0.0402
Epoch 195/300, resid Loss: 0.0370 | 0.0406
Epoch 196/300, resid Loss: 0.0370 | 0.0406
Epoch 197/300, resid Loss: 0.0368 | 0.0421
Epoch 198/300, resid Loss: 0.0367 | 0.0419
Epoch 199/300, resid Loss: 0.0365 | 0.0411
Epoch 200/300, resid Loss: 0.0366 | 0.0403
Epoch 201/300, resid Loss: 0.0368 | 0.0404
Epoch 202/300, resid Loss: 0.0367 | 0.0412
Epoch 203/300, resid Loss: 0.0377 | 0.0418
Epoch 204/300, resid Loss: 0.0375 | 0.0423
Epoch 205/300, resid Loss: 0.0371 | 0.0411
Epoch 206/300, resid Loss: 0.0377 | 0.0385
Epoch 207/300, resid Loss: 0.0374 | 0.0394
Epoch 208/300, resid Loss: 0.0372 | 0.0433
Epoch 209/300, resid Loss: 0.0378 | 0.0430
Epoch 210/300, resid Loss: 0.0367 | 0.0408
Epoch 211/300, resid Loss: 0.0369 | 0.0397
Epoch 212/300, resid Loss: 0.0368 | 0.0405
Epoch 213/300, resid Loss: 0.0363 | 0.0430
Epoch 214/300, resid Loss: 0.0367 | 0.0422
Epoch 215/300, resid Loss: 0.0357 | 0.0421
Epoch 216/300, resid Loss: 0.0358 | 0.0408
Epoch 217/300, resid Loss: 0.0353 | 0.0412
Epoch 218/300, resid Loss: 0.0353 | 0.0431
Epoch 219/300, resid Loss: 0.0352 | 0.0426
Epoch 220/300, resid Loss: 0.0351 | 0.0412
Epoch 221/300, resid Loss: 0.0351 | 0.0414
Epoch 222/300, resid Loss: 0.0348 | 0.0420
Epoch 223/300, resid Loss: 0.0349 | 0.0433
Epoch 224/300, resid Loss: 0.0347 | 0.0425
Epoch 225/300, resid Loss: 0.0347 | 0.0414
Epoch 226/300, resid Loss: 0.0348 | 0.0415
Epoch 227/300, resid Loss: 0.0346 | 0.0427
Epoch 228/300, resid Loss: 0.0347 | 0.0430
Epoch 229/300, resid Loss: 0.0347 | 0.0425
Epoch 230/300, resid Loss: 0.0346 | 0.0414
Epoch 231/300, resid Loss: 0.0349 | 0.0421
Epoch 232/300, resid Loss: 0.0345 | 0.0426
Epoch 233/300, resid Loss: 0.0346 | 0.0426
Epoch 234/300, resid Loss: 0.0344 | 0.0423
Epoch 235/300, resid Loss: 0.0343 | 0.0418
Epoch 236/300, resid Loss: 0.0345 | 0.0425
Epoch 237/300, resid Loss: 0.0340 | 0.0421
Epoch 238/300, resid Loss: 0.0342 | 0.0423
Epoch 239/300, resid Loss: 0.0338 | 0.0428
Epoch 240/300, resid Loss: 0.0338 | 0.0428
Epoch 241/300, resid Loss: 0.0336 | 0.0423
Epoch 242/300, resid Loss: 0.0335 | 0.0420
Epoch 243/300, resid Loss: 0.0335 | 0.0430
Epoch 244/300, resid Loss: 0.0332 | 0.0437
Epoch 245/300, resid Loss: 0.0332 | 0.0434
Epoch 246/300, resid Loss: 0.0331 | 0.0423
Epoch 247/300, resid Loss: 0.0331 | 0.0423
Epoch 248/300, resid Loss: 0.0330 | 0.0434
Epoch 249/300, resid Loss: 0.0330 | 0.0444
Epoch 250/300, resid Loss: 0.0330 | 0.0435
Epoch 251/300, resid Loss: 0.0330 | 0.0424
Epoch 252/300, resid Loss: 0.0331 | 0.0422
Epoch 253/300, resid Loss: 0.0330 | 0.0434
Epoch 254/300, resid Loss: 0.0333 | 0.0440
Epoch 255/300, resid Loss: 0.0332 | 0.0435
Epoch 256/300, resid Loss: 0.0334 | 0.0422
Epoch 257/300, resid Loss: 0.0336 | 0.0420
Epoch 258/300, resid Loss: 0.0333 | 0.0428
Epoch 259/300, resid Loss: 0.0335 | 0.0444
Epoch 260/300, resid Loss: 0.0330 | 0.0438
Epoch 261/300, resid Loss: 0.0331 | 0.0421
Epoch 262/300, resid Loss: 0.0328 | 0.0424
Epoch 263/300, resid Loss: 0.0327 | 0.0446
Epoch 264/300, resid Loss: 0.0325 | 0.0446
Epoch 265/300, resid Loss: 0.0324 | 0.0430
Epoch 266/300, resid Loss: 0.0323 | 0.0429
Epoch 267/300, resid Loss: 0.0322 | 0.0444
Epoch 268/300, resid Loss: 0.0321 | 0.0447
Epoch 269/300, resid Loss: 0.0320 | 0.0437
Epoch 270/300, resid Loss: 0.0321 | 0.0433
Epoch 271/300, resid Loss: 0.0320 | 0.0440
Epoch 272/300, resid Loss: 0.0320 | 0.0447
Epoch 273/300, resid Loss: 0.0319 | 0.0443
Epoch 274/300, resid Loss: 0.0319 | 0.0437
Epoch 275/300, resid Loss: 0.0319 | 0.0437
Epoch 276/300, resid Loss: 0.0318 | 0.0444
Epoch 277/300, resid Loss: 0.0319 | 0.0448
Epoch 278/300, resid Loss: 0.0318 | 0.0441
Epoch 279/300, resid Loss: 0.0319 | 0.0436
Epoch 280/300, resid Loss: 0.0317 | 0.0441
Epoch 281/300, resid Loss: 0.0319 | 0.0450
Epoch 282/300, resid Loss: 0.0316 | 0.0445
Epoch 283/300, resid Loss: 0.0318 | 0.0437
Epoch 284/300, resid Loss: 0.0316 | 0.0440
Epoch 285/300, resid Loss: 0.0317 | 0.0451
Epoch 286/300, resid Loss: 0.0315 | 0.0448
Epoch 287/300, resid Loss: 0.0316 | 0.0439
Epoch 288/300, resid Loss: 0.0315 | 0.0441
Epoch 289/300, resid Loss: 0.0314 | 0.0451
Epoch 290/300, resid Loss: 0.0314 | 0.0450
Epoch 291/300, resid Loss: 0.0313 | 0.0442
Epoch 292/300, resid Loss: 0.0313 | 0.0443
Epoch 293/300, resid Loss: 0.0313 | 0.0451
Epoch 294/300, resid Loss: 0.0312 | 0.0452
Epoch 295/300, resid Loss: 0.0312 | 0.0445
Epoch 296/300, resid Loss: 0.0312 | 0.0444
Epoch 297/300, resid Loss: 0.0311 | 0.0451
Epoch 298/300, resid Loss: 0.0311 | 0.0453
Epoch 299/300, resid Loss: 0.0310 | 0.0447
Epoch 300/300, resid Loss: 0.0311 | 0.0446
Runtime (seconds): 11661.996973276138
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[157.50122]
[-0.13660331]
[1.621027]
[-0.41669112]
[-0.90692616]
[18.196566]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1024.0888691155706
RMSE: 32.00138854980469
MAE: 32.00138854980469
R-squared: nan
[175.85861]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
