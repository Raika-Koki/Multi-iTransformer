[*********************100%***********************]  1 of 1 completed
[*********************100%***********************]  1 of 1 completed
Date
2012-05-18     10.692500
2012-05-21     10.905500
2012-05-22     10.766500
2012-05-23     10.864000
2012-05-24     10.762000
                 ...
2023-05-24    116.750000
2023-05-25    115.000000
2023-05-26    120.110001
2023-05-30    121.660004
2023-05-31    120.580002
Name: AMZN, Length: 2776, dtype: float64
Price         Volume    BB_Upper    BB_Lower   BB_Middle      MACD MACD_Signal MACD_Diff        RSI    SMA_50    SMA_200 SMA_200-50
Ticker          AMZN
Date
2023-05-24  63487900  120.005693  100.053308  110.029501  3.584615    3.109054  0.475561  66.184276  104.7736  105.39470    0.62110
2023-05-25  66496700  120.495777  100.081225  110.288501  3.450388    3.177321  0.273067  61.346390  105.1496  105.28055    0.13095
2023-05-26  96779900  121.821939  100.221062  111.021501  3.713539    3.284565  0.428975  68.570791  105.5510  105.16765   -0.38335
2023-05-30  64314800  122.926375  101.077627  112.002001  4.001039    3.427859  0.573180  70.379232  106.0052  105.07275   -0.93245
2023-05-31  72800800  123.673883  102.025119  112.849501  4.094538    3.561195  0.533343  67.466273  106.4626  104.95790   -1.50470
{'observation_period_num': 5, 'train_rates': 0.9674192065835944, 'learning_rate': 0.0006703481785402301, 'batch_size': 197, 'step_size': 5, 'gamma': 0.9119241664414688, 'depth': 4, 'dim': 123}
{0: {'observation_period_num': 14, 'train_rates': 0.9534530255286052, 'learning_rate': 2.383027942061777e-05, 'batch_size': 24, 'step_size': 6, 'gamma': 0.9343469177823375, 'depth': 5, 'dim': 162}, 1: {'observation_period_num': 10, 'train_rates': 0.7859430649262442, 'learning_rate': 0.00011626904101397289, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9591926500011835, 'depth': 5, 'dim': 221}, 2: {'observation_period_num': 5, 'train_rates': 0.9881973682668941, 'learning_rate': 0.0008599253544665889, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8332821730513479, 'depth': 5, 'dim': 127}, 3: {'observation_period_num': 11, 'train_rates': 0.9524554300989908, 'learning_rate': 0.00011007906140572895, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9320524916302059, 'depth': 6, 'dim': 145}}
{'observation_period_num': 13, 'train_rates': 0.8047176482675613, 'learning_rate': 0.0007766732022188664, 'batch_size': 128, 'step_size': 2, 'gamma': 0.925182630519391, 'depth': 4, 'dim': 228}
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Epoch 1/500, trend Loss: 0.4772 | 0.2885
Epoch 2/500, trend Loss: 0.1820 | 0.1688
Epoch 3/500, trend Loss: 0.1622 | 0.1245
Epoch 4/500, trend Loss: 0.1696 | 0.2029
Epoch 5/500, trend Loss: 0.1545 | 0.1686
Epoch 6/500, trend Loss: 0.1424 | 0.1234
Epoch 7/500, trend Loss: 0.1084 | 0.0836
Epoch 8/500, trend Loss: 0.1002 | 0.0757
Epoch 9/500, trend Loss: 0.0958 | 0.0722
Epoch 10/500, trend Loss: 0.1034 | 0.1317
Epoch 11/500, trend Loss: 0.1348 | 0.1511
Epoch 12/500, trend Loss: 0.1293 | 0.1089
Epoch 13/500, trend Loss: 0.1216 | 0.0791
Epoch 14/500, trend Loss: 0.0933 | 0.0656
Epoch 15/500, trend Loss: 0.1017 | 0.0859
Epoch 16/500, trend Loss: 0.1059 | 0.0568
Epoch 17/500, trend Loss: 0.1194 | 0.1040
Epoch 18/500, trend Loss: 0.1095 | 0.0631
Epoch 19/500, trend Loss: 0.1122 | 0.0792
Epoch 20/500, trend Loss: 0.1091 | 0.0593
Epoch 21/500, trend Loss: 0.1087 | 0.1067
Epoch 22/500, trend Loss: 0.0934 | 0.0652
Epoch 23/500, trend Loss: 0.0947 | 0.0679
Epoch 24/500, trend Loss: 0.0857 | 0.0615
Epoch 25/500, trend Loss: 0.0770 | 0.0562
Epoch 26/500, trend Loss: 0.0719 | 0.0516
Epoch 27/500, trend Loss: 0.0722 | 0.0543
Epoch 28/500, trend Loss: 0.0715 | 0.0538
Epoch 29/500, trend Loss: 0.0686 | 0.0495
Epoch 30/500, trend Loss: 0.0639 | 0.0468
Epoch 31/500, trend Loss: 0.0620 | 0.0449
Epoch 32/500, trend Loss: 0.0604 | 0.0457
Epoch 33/500, trend Loss: 0.0603 | 0.0445
Epoch 34/500, trend Loss: 0.0580 | 0.0439
Epoch 35/500, trend Loss: 0.0566 | 0.0415
Epoch 36/500, trend Loss: 0.0553 | 0.0414
Epoch 37/500, trend Loss: 0.0545 | 0.0405
Epoch 38/500, trend Loss: 0.0537 | 0.0398
Epoch 39/500, trend Loss: 0.0532 | 0.0398
Epoch 40/500, trend Loss: 0.0530 | 0.0392
Epoch 41/500, trend Loss: 0.0523 | 0.0395
Epoch 42/500, trend Loss: 0.0525 | 0.0390
Epoch 43/500, trend Loss: 0.0514 | 0.0382
Epoch 44/500, trend Loss: 0.0510 | 0.0382
Epoch 45/500, trend Loss: 0.0504 | 0.0380
Epoch 46/500, trend Loss: 0.0501 | 0.0377
Epoch 47/500, trend Loss: 0.0498 | 0.0377
Epoch 48/500, trend Loss: 0.0495 | 0.0376
Epoch 49/500, trend Loss: 0.0493 | 0.0376
Epoch 50/500, trend Loss: 0.0491 | 0.0374
Epoch 51/500, trend Loss: 0.0488 | 0.0373
Epoch 52/500, trend Loss: 0.0486 | 0.0372
Epoch 53/500, trend Loss: 0.0484 | 0.0371
Epoch 54/500, trend Loss: 0.0482 | 0.0370
Epoch 55/500, trend Loss: 0.0480 | 0.0370
Epoch 56/500, trend Loss: 0.0478 | 0.0369
Epoch 57/500, trend Loss: 0.0477 | 0.0368
Epoch 58/500, trend Loss: 0.0475 | 0.0367
Epoch 59/500, trend Loss: 0.0473 | 0.0366
Epoch 60/500, trend Loss: 0.0471 | 0.0365
Epoch 61/500, trend Loss: 0.0469 | 0.0364
Epoch 62/500, trend Loss: 0.0468 | 0.0363
Epoch 63/500, trend Loss: 0.0466 | 0.0362
Epoch 64/500, trend Loss: 0.0464 | 0.0361
Epoch 65/500, trend Loss: 0.0463 | 0.0360
Epoch 66/500, trend Loss: 0.0461 | 0.0359
Epoch 67/500, trend Loss: 0.0460 | 0.0358
Epoch 68/500, trend Loss: 0.0458 | 0.0357
Epoch 69/500, trend Loss: 0.0457 | 0.0356
Epoch 70/500, trend Loss: 0.0456 | 0.0355
Epoch 71/500, trend Loss: 0.0454 | 0.0355
Epoch 72/500, trend Loss: 0.0453 | 0.0354
Epoch 73/500, trend Loss: 0.0452 | 0.0354
Epoch 74/500, trend Loss: 0.0451 | 0.0353
Epoch 75/500, trend Loss: 0.0450 | 0.0353
Epoch 76/500, trend Loss: 0.0449 | 0.0352
Epoch 77/500, trend Loss: 0.0448 | 0.0352
Epoch 78/500, trend Loss: 0.0447 | 0.0352
Epoch 79/500, trend Loss: 0.0446 | 0.0352
Epoch 80/500, trend Loss: 0.0445 | 0.0352
Epoch 81/500, trend Loss: 0.0445 | 0.0351
Epoch 82/500, trend Loss: 0.0444 | 0.0351
Epoch 83/500, trend Loss: 0.0443 | 0.0350
Epoch 84/500, trend Loss: 0.0442 | 0.0349
Epoch 85/500, trend Loss: 0.0442 | 0.0349
Epoch 86/500, trend Loss: 0.0441 | 0.0349
Epoch 87/500, trend Loss: 0.0441 | 0.0349
Epoch 88/500, trend Loss: 0.0440 | 0.0349
Epoch 89/500, trend Loss: 0.0439 | 0.0348
Epoch 90/500, trend Loss: 0.0439 | 0.0348
Epoch 91/500, trend Loss: 0.0438 | 0.0348
Epoch 92/500, trend Loss: 0.0438 | 0.0348
Epoch 93/500, trend Loss: 0.0437 | 0.0348
Epoch 94/500, trend Loss: 0.0437 | 0.0347
Epoch 95/500, trend Loss: 0.0436 | 0.0347
Epoch 96/500, trend Loss: 0.0436 | 0.0347
Epoch 97/500, trend Loss: 0.0436 | 0.0347
Epoch 98/500, trend Loss: 0.0435 | 0.0347
Epoch 99/500, trend Loss: 0.0435 | 0.0347
Epoch 100/500, trend Loss: 0.0434 | 0.0346
Epoch 101/500, trend Loss: 0.0434 | 0.0346
Epoch 102/500, trend Loss: 0.0434 | 0.0346
Epoch 103/500, trend Loss: 0.0433 | 0.0346
Epoch 104/500, trend Loss: 0.0433 | 0.0346
Epoch 105/500, trend Loss: 0.0433 | 0.0346
Epoch 106/500, trend Loss: 0.0432 | 0.0346
Epoch 107/500, trend Loss: 0.0432 | 0.0345
Epoch 108/500, trend Loss: 0.0432 | 0.0345
Epoch 109/500, trend Loss: 0.0431 | 0.0345
Epoch 110/500, trend Loss: 0.0431 | 0.0345
Epoch 111/500, trend Loss: 0.0431 | 0.0345
Epoch 112/500, trend Loss: 0.0431 | 0.0345
Epoch 113/500, trend Loss: 0.0430 | 0.0345
Epoch 114/500, trend Loss: 0.0430 | 0.0345
Epoch 115/500, trend Loss: 0.0430 | 0.0345
Epoch 116/500, trend Loss: 0.0430 | 0.0344
Epoch 117/500, trend Loss: 0.0429 | 0.0344
Epoch 118/500, trend Loss: 0.0429 | 0.0344
Epoch 119/500, trend Loss: 0.0429 | 0.0344
Epoch 120/500, trend Loss: 0.0429 | 0.0344
Epoch 121/500, trend Loss: 0.0428 | 0.0344
Epoch 122/500, trend Loss: 0.0428 | 0.0344
Epoch 123/500, trend Loss: 0.0428 | 0.0344
Epoch 124/500, trend Loss: 0.0428 | 0.0344
Epoch 125/500, trend Loss: 0.0428 | 0.0344
Epoch 126/500, trend Loss: 0.0428 | 0.0344
Epoch 127/500, trend Loss: 0.0427 | 0.0344
Epoch 128/500, trend Loss: 0.0427 | 0.0344
Epoch 129/500, trend Loss: 0.0427 | 0.0343
Epoch 130/500, trend Loss: 0.0427 | 0.0343
Epoch 131/500, trend Loss: 0.0427 | 0.0343
Epoch 132/500, trend Loss: 0.0427 | 0.0343
Epoch 133/500, trend Loss: 0.0427 | 0.0343
Epoch 134/500, trend Loss: 0.0426 | 0.0343
Epoch 135/500, trend Loss: 0.0426 | 0.0343
Epoch 136/500, trend Loss: 0.0426 | 0.0343
Epoch 137/500, trend Loss: 0.0426 | 0.0343
Epoch 138/500, trend Loss: 0.0426 | 0.0343
Epoch 139/500, trend Loss: 0.0426 | 0.0343
Epoch 140/500, trend Loss: 0.0426 | 0.0343
Epoch 141/500, trend Loss: 0.0426 | 0.0343
Epoch 142/500, trend Loss: 0.0425 | 0.0343
Epoch 143/500, trend Loss: 0.0425 | 0.0343
Epoch 144/500, trend Loss: 0.0425 | 0.0343
Epoch 145/500, trend Loss: 0.0425 | 0.0343
Epoch 146/500, trend Loss: 0.0425 | 0.0343
Epoch 147/500, trend Loss: 0.0425 | 0.0343
Epoch 148/500, trend Loss: 0.0425 | 0.0342
Epoch 149/500, trend Loss: 0.0425 | 0.0342
Epoch 150/500, trend Loss: 0.0425 | 0.0342
Epoch 151/500, trend Loss: 0.0425 | 0.0342
Epoch 152/500, trend Loss: 0.0425 | 0.0342
Epoch 153/500, trend Loss: 0.0425 | 0.0342
Epoch 154/500, trend Loss: 0.0424 | 0.0342
Epoch 155/500, trend Loss: 0.0424 | 0.0342
Epoch 156/500, trend Loss: 0.0424 | 0.0342
Epoch 157/500, trend Loss: 0.0424 | 0.0342
Epoch 158/500, trend Loss: 0.0424 | 0.0342
Epoch 159/500, trend Loss: 0.0424 | 0.0342
Epoch 160/500, trend Loss: 0.0424 | 0.0342
Epoch 161/500, trend Loss: 0.0424 | 0.0342
Epoch 162/500, trend Loss: 0.0424 | 0.0342
Epoch 163/500, trend Loss: 0.0424 | 0.0342
Epoch 164/500, trend Loss: 0.0424 | 0.0342
Epoch 165/500, trend Loss: 0.0424 | 0.0342
Epoch 166/500, trend Loss: 0.0424 | 0.0342
Epoch 167/500, trend Loss: 0.0424 | 0.0342
Epoch 168/500, trend Loss: 0.0424 | 0.0342
Epoch 169/500, trend Loss: 0.0424 | 0.0342
Epoch 170/500, trend Loss: 0.0424 | 0.0342
Epoch 171/500, trend Loss: 0.0424 | 0.0342
Epoch 172/500, trend Loss: 0.0424 | 0.0342
Epoch 173/500, trend Loss: 0.0423 | 0.0342
Epoch 174/500, trend Loss: 0.0423 | 0.0342
Epoch 175/500, trend Loss: 0.0423 | 0.0342
Epoch 176/500, trend Loss: 0.0423 | 0.0342
Epoch 177/500, trend Loss: 0.0423 | 0.0342
Epoch 178/500, trend Loss: 0.0423 | 0.0342
Epoch 179/500, trend Loss: 0.0423 | 0.0342
Epoch 180/500, trend Loss: 0.0423 | 0.0342
Epoch 181/500, trend Loss: 0.0423 | 0.0342
Epoch 182/500, trend Loss: 0.0423 | 0.0342
Epoch 183/500, trend Loss: 0.0423 | 0.0342
Epoch 184/500, trend Loss: 0.0423 | 0.0342
Epoch 185/500, trend Loss: 0.0423 | 0.0342
Epoch 186/500, trend Loss: 0.0423 | 0.0342
Epoch 187/500, trend Loss: 0.0423 | 0.0342
Epoch 188/500, trend Loss: 0.0423 | 0.0342
Epoch 189/500, trend Loss: 0.0423 | 0.0342
Epoch 190/500, trend Loss: 0.0423 | 0.0342
Epoch 191/500, trend Loss: 0.0423 | 0.0342
Epoch 192/500, trend Loss: 0.0423 | 0.0342
Epoch 193/500, trend Loss: 0.0423 | 0.0342
Epoch 194/500, trend Loss: 0.0423 | 0.0342
Epoch 195/500, trend Loss: 0.0423 | 0.0342
Epoch 196/500, trend Loss: 0.0423 | 0.0342
Epoch 197/500, trend Loss: 0.0423 | 0.0342
Epoch 198/500, trend Loss: 0.0423 | 0.0342
Epoch 199/500, trend Loss: 0.0423 | 0.0342
Epoch 200/500, trend Loss: 0.0423 | 0.0342
Epoch 201/500, trend Loss: 0.0423 | 0.0342
Epoch 202/500, trend Loss: 0.0423 | 0.0342
Epoch 203/500, trend Loss: 0.0423 | 0.0342
Epoch 204/500, trend Loss: 0.0423 | 0.0342
Epoch 205/500, trend Loss: 0.0423 | 0.0342
Epoch 206/500, trend Loss: 0.0423 | 0.0341
Epoch 207/500, trend Loss: 0.0423 | 0.0341
Epoch 208/500, trend Loss: 0.0423 | 0.0341
Epoch 209/500, trend Loss: 0.0423 | 0.0341
Epoch 210/500, trend Loss: 0.0423 | 0.0341
Epoch 211/500, trend Loss: 0.0423 | 0.0341
Epoch 212/500, trend Loss: 0.0423 | 0.0341
Epoch 213/500, trend Loss: 0.0423 | 0.0341
Epoch 214/500, trend Loss: 0.0423 | 0.0341
Epoch 215/500, trend Loss: 0.0423 | 0.0341
Epoch 216/500, trend Loss: 0.0423 | 0.0341
Epoch 217/500, trend Loss: 0.0423 | 0.0341
Epoch 218/500, trend Loss: 0.0423 | 0.0341
Epoch 219/500, trend Loss: 0.0423 | 0.0341
Epoch 220/500, trend Loss: 0.0423 | 0.0341
Epoch 221/500, trend Loss: 0.0423 | 0.0341
Epoch 222/500, trend Loss: 0.0423 | 0.0341
Epoch 223/500, trend Loss: 0.0423 | 0.0341
Epoch 224/500, trend Loss: 0.0423 | 0.0341
Epoch 225/500, trend Loss: 0.0423 | 0.0341
Epoch 226/500, trend Loss: 0.0423 | 0.0341
Epoch 227/500, trend Loss: 0.0423 | 0.0341
Epoch 228/500, trend Loss: 0.0423 | 0.0341
Epoch 229/500, trend Loss: 0.0423 | 0.0341
Epoch 230/500, trend Loss: 0.0423 | 0.0341
Epoch 231/500, trend Loss: 0.0423 | 0.0341
Epoch 232/500, trend Loss: 0.0423 | 0.0341
Epoch 233/500, trend Loss: 0.0423 | 0.0341
Epoch 234/500, trend Loss: 0.0423 | 0.0341
Epoch 235/500, trend Loss: 0.0423 | 0.0341
Epoch 236/500, trend Loss: 0.0423 | 0.0341
Epoch 237/500, trend Loss: 0.0423 | 0.0341
Epoch 238/500, trend Loss: 0.0423 | 0.0341
Epoch 239/500, trend Loss: 0.0423 | 0.0341
Epoch 240/500, trend Loss: 0.0423 | 0.0341
Epoch 241/500, trend Loss: 0.0423 | 0.0341
Epoch 242/500, trend Loss: 0.0423 | 0.0341
Epoch 243/500, trend Loss: 0.0422 | 0.0341
Epoch 244/500, trend Loss: 0.0422 | 0.0341
Epoch 245/500, trend Loss: 0.0422 | 0.0341
Epoch 246/500, trend Loss: 0.0422 | 0.0341
Epoch 247/500, trend Loss: 0.0422 | 0.0341
Epoch 248/500, trend Loss: 0.0422 | 0.0341
Epoch 249/500, trend Loss: 0.0422 | 0.0341
Epoch 250/500, trend Loss: 0.0422 | 0.0341
Epoch 251/500, trend Loss: 0.0422 | 0.0341
Epoch 252/500, trend Loss: 0.0422 | 0.0341
Epoch 253/500, trend Loss: 0.0422 | 0.0341
Epoch 254/500, trend Loss: 0.0422 | 0.0341
Epoch 255/500, trend Loss: 0.0422 | 0.0341
Epoch 256/500, trend Loss: 0.0422 | 0.0341
Epoch 257/500, trend Loss: 0.0422 | 0.0341
Epoch 258/500, trend Loss: 0.0422 | 0.0341
Epoch 259/500, trend Loss: 0.0422 | 0.0341
Epoch 260/500, trend Loss: 0.0422 | 0.0341
Epoch 261/500, trend Loss: 0.0422 | 0.0341
Epoch 262/500, trend Loss: 0.0422 | 0.0341
Epoch 263/500, trend Loss: 0.0422 | 0.0341
Epoch 264/500, trend Loss: 0.0422 | 0.0341
Epoch 265/500, trend Loss: 0.0422 | 0.0341
Epoch 266/500, trend Loss: 0.0422 | 0.0341
Epoch 267/500, trend Loss: 0.0422 | 0.0341
Epoch 268/500, trend Loss: 0.0422 | 0.0341
Epoch 269/500, trend Loss: 0.0422 | 0.0341
Epoch 270/500, trend Loss: 0.0422 | 0.0341
Epoch 271/500, trend Loss: 0.0422 | 0.0341
Epoch 272/500, trend Loss: 0.0422 | 0.0341
Epoch 273/500, trend Loss: 0.0422 | 0.0341
Epoch 274/500, trend Loss: 0.0422 | 0.0341
Epoch 275/500, trend Loss: 0.0422 | 0.0341
Epoch 276/500, trend Loss: 0.0422 | 0.0341
Epoch 277/500, trend Loss: 0.0422 | 0.0341
Epoch 278/500, trend Loss: 0.0422 | 0.0341
Epoch 279/500, trend Loss: 0.0422 | 0.0341
Epoch 280/500, trend Loss: 0.0422 | 0.0341
Epoch 281/500, trend Loss: 0.0422 | 0.0341
Epoch 282/500, trend Loss: 0.0422 | 0.0341
Epoch 283/500, trend Loss: 0.0422 | 0.0341
Epoch 284/500, trend Loss: 0.0422 | 0.0341
Epoch 285/500, trend Loss: 0.0422 | 0.0341
Epoch 286/500, trend Loss: 0.0422 | 0.0341
Epoch 287/500, trend Loss: 0.0422 | 0.0341
Epoch 288/500, trend Loss: 0.0422 | 0.0341
Epoch 289/500, trend Loss: 0.0422 | 0.0341
Epoch 290/500, trend Loss: 0.0422 | 0.0341
Epoch 291/500, trend Loss: 0.0422 | 0.0341
Epoch 292/500, trend Loss: 0.0422 | 0.0341
Epoch 293/500, trend Loss: 0.0422 | 0.0341
Epoch 294/500, trend Loss: 0.0422 | 0.0341
Epoch 295/500, trend Loss: 0.0422 | 0.0341
Epoch 296/500, trend Loss: 0.0422 | 0.0341
Epoch 297/500, trend Loss: 0.0422 | 0.0341
Epoch 298/500, trend Loss: 0.0422 | 0.0341
Epoch 299/500, trend Loss: 0.0422 | 0.0341
Epoch 300/500, trend Loss: 0.0422 | 0.0341
Epoch 301/500, trend Loss: 0.0422 | 0.0341
Epoch 302/500, trend Loss: 0.0422 | 0.0341
Epoch 303/500, trend Loss: 0.0422 | 0.0341
Epoch 304/500, trend Loss: 0.0422 | 0.0341
Epoch 305/500, trend Loss: 0.0422 | 0.0341
Epoch 306/500, trend Loss: 0.0422 | 0.0341
Epoch 307/500, trend Loss: 0.0422 | 0.0341
Epoch 308/500, trend Loss: 0.0422 | 0.0341
Epoch 309/500, trend Loss: 0.0422 | 0.0341
Epoch 310/500, trend Loss: 0.0422 | 0.0341
Epoch 311/500, trend Loss: 0.0422 | 0.0341
Epoch 312/500, trend Loss: 0.0422 | 0.0341
Epoch 313/500, trend Loss: 0.0422 | 0.0341
Epoch 314/500, trend Loss: 0.0422 | 0.0341
Epoch 315/500, trend Loss: 0.0422 | 0.0341
Epoch 316/500, trend Loss: 0.0422 | 0.0341
Epoch 317/500, trend Loss: 0.0422 | 0.0341
Epoch 318/500, trend Loss: 0.0422 | 0.0341
Epoch 319/500, trend Loss: 0.0422 | 0.0341
Epoch 320/500, trend Loss: 0.0422 | 0.0341
Epoch 321/500, trend Loss: 0.0422 | 0.0341
Early stopping for trend
Epoch 1/500, seasonal_0 Loss: 0.2469 | 0.2044
Epoch 2/500, seasonal_0 Loss: 0.1381 | 0.1502
Epoch 3/500, seasonal_0 Loss: 0.1137 | 0.1213
Epoch 4/500, seasonal_0 Loss: 0.1016 | 0.1035
Epoch 5/500, seasonal_0 Loss: 0.0949 | 0.0927
Epoch 6/500, seasonal_0 Loss: 0.0903 | 0.0861
Epoch 7/500, seasonal_0 Loss: 0.0866 | 0.0821
Epoch 8/500, seasonal_0 Loss: 0.0836 | 0.0784
Epoch 9/500, seasonal_0 Loss: 0.0809 | 0.0757
Epoch 10/500, seasonal_0 Loss: 0.0786 | 0.0738
Epoch 11/500, seasonal_0 Loss: 0.0767 | 0.0721
Epoch 12/500, seasonal_0 Loss: 0.0750 | 0.0710
Epoch 13/500, seasonal_0 Loss: 0.0735 | 0.0701
Epoch 14/500, seasonal_0 Loss: 0.0724 | 0.0696
Epoch 15/500, seasonal_0 Loss: 0.0713 | 0.0694
Epoch 16/500, seasonal_0 Loss: 0.0705 | 0.0692
Epoch 17/500, seasonal_0 Loss: 0.0703 | 0.0699
Epoch 18/500, seasonal_0 Loss: 0.0702 | 0.0716
Epoch 19/500, seasonal_0 Loss: 0.0703 | 0.0798
Epoch 20/500, seasonal_0 Loss: 0.0704 | 0.0882
Epoch 21/500, seasonal_0 Loss: 0.0696 | 0.0912
Epoch 22/500, seasonal_0 Loss: 0.0680 | 0.0921
Epoch 23/500, seasonal_0 Loss: 0.0665 | 0.0857
Epoch 24/500, seasonal_0 Loss: 0.0650 | 0.0819
Epoch 25/500, seasonal_0 Loss: 0.0638 | 0.0807
Epoch 26/500, seasonal_0 Loss: 0.0629 | 0.0773
Epoch 27/500, seasonal_0 Loss: 0.0620 | 0.0753
Epoch 28/500, seasonal_0 Loss: 0.0611 | 0.0746
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/main.py", line 1040, in <module>
    models[comp], train_loss, valid_loss = train(model, train_data, valid_data, optimizer, criterion, scheduler, params['batch_size'], params['observation_period_num'])
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 32, in train
    output = model(data)
             ^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<@beartype(src.model.iTransformer.forward) at 0x7fa2021e7a60>", line 66, in forward
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 188, in forward
    x = attn(x) + x
        ^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 67, in forward
    out = self.attend(q, k, v)
          ^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/attend.py", line 123, in forward
    return self.flash_attn(q, k, v)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/attend.py", line 98, in flash_attn
    out = F.scaled_dot_product_attention(
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
