[32m[I 2025-01-04 21:46:00,206][0m A new study created in memory with name: no-name-8de1098c-79b9-4879-860d-51862a0b977f[0m
[32m[I 2025-01-04 21:46:43,293][0m Trial 0 finished with value: 0.18943073709184924 and parameters: {'observation_period_num': 217, 'train_rates': 0.8949562914461435, 'learning_rate': 0.00021373291929778355, 'batch_size': 123, 'step_size': 2, 'gamma': 0.7820852613865129}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:47:21,193][0m Trial 1 finished with value: 0.3792877024583485 and parameters: {'observation_period_num': 162, 'train_rates': 0.7634446487484222, 'learning_rate': 1.0705394107660498e-05, 'batch_size': 146, 'step_size': 8, 'gamma': 0.9604653051742538}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:47:55,702][0m Trial 2 finished with value: 0.41382270077255207 and parameters: {'observation_period_num': 46, 'train_rates': 0.6186099862452497, 'learning_rate': 8.346489635455193e-06, 'batch_size': 157, 'step_size': 7, 'gamma': 0.8686428864646065}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:48:43,366][0m Trial 3 finished with value: 0.5757464146387549 and parameters: {'observation_period_num': 140, 'train_rates': 0.7009531165721296, 'learning_rate': 2.3046755707903386e-05, 'batch_size': 87, 'step_size': 2, 'gamma': 0.7550966834665633}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:49:22,836][0m Trial 4 finished with value: 0.20753399857600255 and parameters: {'observation_period_num': 52, 'train_rates': 0.8010714057742367, 'learning_rate': 7.085276555734913e-06, 'batch_size': 193, 'step_size': 14, 'gamma': 0.9472230866283615}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:49:56,489][0m Trial 5 finished with value: 0.33805198970372896 and parameters: {'observation_period_num': 72, 'train_rates': 0.6452899226989772, 'learning_rate': 2.2951055126864768e-05, 'batch_size': 172, 'step_size': 2, 'gamma': 0.9413593791095092}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:51:21,279][0m Trial 6 finished with value: 0.334020907890575 and parameters: {'observation_period_num': 244, 'train_rates': 0.6427869651834164, 'learning_rate': 0.00016569840322986463, 'batch_size': 37, 'step_size': 3, 'gamma': 0.9825081282559327}. Best is trial 0 with value: 0.18943073709184924.[0m
[32m[I 2025-01-04 21:54:35,990][0m Trial 7 finished with value: 0.06746636993187642 and parameters: {'observation_period_num': 67, 'train_rates': 0.8290598516957839, 'learning_rate': 2.1099304428641285e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9287644112318361}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 21:55:19,123][0m Trial 8 finished with value: 0.0884056238763368 and parameters: {'observation_period_num': 62, 'train_rates': 0.8842974472873457, 'learning_rate': 0.0002619629111013513, 'batch_size': 141, 'step_size': 2, 'gamma': 0.7902336931109827}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 21:56:52,173][0m Trial 9 finished with value: 0.49516218652327854 and parameters: {'observation_period_num': 233, 'train_rates': 0.7487477049427944, 'learning_rate': 1.345870568443361e-05, 'batch_size': 38, 'step_size': 3, 'gamma': 0.7577863303946202}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 21:57:33,239][0m Trial 10 finished with value: 0.36426177620887756 and parameters: {'observation_period_num': 8, 'train_rates': 0.9648606377494209, 'learning_rate': 1.2477766896411753e-06, 'batch_size': 251, 'step_size': 15, 'gamma': 0.9034826162922943}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 21:58:29,817][0m Trial 11 finished with value: 0.10514168065096358 and parameters: {'observation_period_num': 89, 'train_rates': 0.8583082987668893, 'learning_rate': 0.0008182846083172371, 'batch_size': 89, 'step_size': 11, 'gamma': 0.825951694802294}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 21:59:23,117][0m Trial 12 finished with value: 0.07091406753379337 and parameters: {'observation_period_num': 113, 'train_rates': 0.8726932155200984, 'learning_rate': 0.00011586481923111357, 'batch_size': 100, 'step_size': 6, 'gamma': 0.8404409277773425}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 22:00:32,022][0m Trial 13 finished with value: 0.08860314637422562 and parameters: {'observation_period_num': 110, 'train_rates': 0.9855952874479754, 'learning_rate': 6.988958350234801e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.87108598731558}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 22:01:42,684][0m Trial 14 finished with value: 0.08956475165405113 and parameters: {'observation_period_num': 176, 'train_rates': 0.8284238908923885, 'learning_rate': 6.058305326606029e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8982300533046156}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 22:04:34,778][0m Trial 15 finished with value: 0.29944953979814753 and parameters: {'observation_period_num': 112, 'train_rates': 0.9403028460244311, 'learning_rate': 2.875152173537586e-06, 'batch_size': 25, 'step_size': 5, 'gamma': 0.8423329321824438}. Best is trial 7 with value: 0.06746636993187642.[0m
[32m[I 2025-01-04 22:05:25,161][0m Trial 16 finished with value: 0.046617435588562996 and parameters: {'observation_period_num': 17, 'train_rates': 0.9126063627696607, 'learning_rate': 7.672997265720683e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9164173611788212}. Best is trial 16 with value: 0.046617435588562996.[0m
[32m[I 2025-01-04 22:06:07,299][0m Trial 17 finished with value: 0.052103941060908855 and parameters: {'observation_period_num': 13, 'train_rates': 0.9257591975499361, 'learning_rate': 4.44942009575319e-05, 'batch_size': 215, 'step_size': 11, 'gamma': 0.9179379947718703}. Best is trial 16 with value: 0.046617435588562996.[0m
[32m[I 2025-01-04 22:06:48,254][0m Trial 18 finished with value: 0.032695841044187546 and parameters: {'observation_period_num': 11, 'train_rates': 0.9203187329368708, 'learning_rate': 0.0007039969034355771, 'batch_size': 240, 'step_size': 11, 'gamma': 0.9098199615481207}. Best is trial 18 with value: 0.032695841044187546.[0m
[32m[I 2025-01-04 22:07:28,834][0m Trial 19 finished with value: 0.05017035827040672 and parameters: {'observation_period_num': 33, 'train_rates': 0.918148516056966, 'learning_rate': 0.0005005653530713401, 'batch_size': 243, 'step_size': 10, 'gamma': 0.8785062517701333}. Best is trial 18 with value: 0.032695841044187546.[0m
[32m[I 2025-01-04 22:08:22,572][0m Trial 20 finished with value: 0.045528970244858 and parameters: {'observation_period_num': 26, 'train_rates': 0.9572273122166194, 'learning_rate': 0.0009649168427488226, 'batch_size': 112, 'step_size': 13, 'gamma': 0.9670833771638191}. Best is trial 18 with value: 0.032695841044187546.[0m
[32m[I 2025-01-04 22:09:12,455][0m Trial 21 finished with value: 0.05089125409722328 and parameters: {'observation_period_num': 29, 'train_rates': 0.9665474482640554, 'learning_rate': 0.0007715791499170086, 'batch_size': 125, 'step_size': 13, 'gamma': 0.9843302145415672}. Best is trial 18 with value: 0.032695841044187546.[0m
[32m[I 2025-01-04 22:09:54,800][0m Trial 22 finished with value: 0.039769064635038376 and parameters: {'observation_period_num': 7, 'train_rates': 0.9405084393578712, 'learning_rate': 0.00039639158761316153, 'batch_size': 181, 'step_size': 13, 'gamma': 0.957493326998155}. Best is trial 18 with value: 0.032695841044187546.[0m
[32m[I 2025-01-04 22:10:39,554][0m Trial 23 finished with value: 0.05341054126620293 and parameters: {'observation_period_num': 43, 'train_rates': 0.952312922487116, 'learning_rate': 0.00038518512462870977, 'batch_size': 214, 'step_size': 13, 'gamma': 0.9649121403448931}. Best is trial 18 with value: 0.032695841044187546.[0m
[32m[I 2025-01-04 22:11:25,493][0m Trial 24 finished with value: 0.027379298582673073 and parameters: {'observation_period_num': 7, 'train_rates': 0.9810272864503238, 'learning_rate': 0.0009915678422628338, 'batch_size': 174, 'step_size': 13, 'gamma': 0.9606224167317868}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:12:06,245][0m Trial 25 finished with value: 0.10278274863958359 and parameters: {'observation_period_num': 84, 'train_rates': 0.9822992648311952, 'learning_rate': 0.0004218997022057501, 'batch_size': 179, 'step_size': 12, 'gamma': 0.9396714214759567}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:12:47,688][0m Trial 26 finished with value: 0.029634893078494956 and parameters: {'observation_period_num': 6, 'train_rates': 0.9085806961247835, 'learning_rate': 0.0005614930209367831, 'batch_size': 220, 'step_size': 9, 'gamma': 0.8951882053975556}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:13:26,932][0m Trial 27 finished with value: 0.07542960008167293 and parameters: {'observation_period_num': 39, 'train_rates': 0.8502577428071938, 'learning_rate': 0.000573004487525994, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8926916015472428}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:14:07,122][0m Trial 28 finished with value: 0.04634078512234347 and parameters: {'observation_period_num': 6, 'train_rates': 0.9003190724670438, 'learning_rate': 0.0001455422791286074, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8539881438514392}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:14:45,484][0m Trial 29 finished with value: 0.08151037918445402 and parameters: {'observation_period_num': 90, 'train_rates': 0.8908907987301078, 'learning_rate': 0.00024053097485242855, 'batch_size': 199, 'step_size': 8, 'gamma': 0.9210725346767441}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:15:25,824][0m Trial 30 finished with value: 0.13321198523044586 and parameters: {'observation_period_num': 191, 'train_rates': 0.9892842352131058, 'learning_rate': 0.0002793179128304038, 'batch_size': 238, 'step_size': 12, 'gamma': 0.8859258311553563}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:16:15,323][0m Trial 31 finished with value: 0.08788780122995377 and parameters: {'observation_period_num': 23, 'train_rates': 0.9368386936921618, 'learning_rate': 0.0009739495747333557, 'batch_size': 192, 'step_size': 14, 'gamma': 0.958654190564721}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:17:09,629][0m Trial 32 finished with value: 0.03116957741856043 and parameters: {'observation_period_num': 7, 'train_rates': 0.9242407941313165, 'learning_rate': 0.00036359130781994924, 'batch_size': 162, 'step_size': 12, 'gamma': 0.9891756842287586}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:18:04,615][0m Trial 33 finished with value: 0.07858099370398594 and parameters: {'observation_period_num': 50, 'train_rates': 0.9099362709224104, 'learning_rate': 0.0006091293974262478, 'batch_size': 162, 'step_size': 10, 'gamma': 0.9837585862944868}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:19:00,281][0m Trial 34 finished with value: 0.18504669400720508 and parameters: {'observation_period_num': 26, 'train_rates': 0.7773641528569637, 'learning_rate': 0.0003570055770292545, 'batch_size': 154, 'step_size': 12, 'gamma': 0.9104064382295289}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:19:44,995][0m Trial 35 finished with value: 0.04524824682486619 and parameters: {'observation_period_num': 53, 'train_rates': 0.8683280075800992, 'learning_rate': 0.00060800478876141, 'batch_size': 197, 'step_size': 9, 'gamma': 0.9410742656625779}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:20:21,422][0m Trial 36 finished with value: 0.2543147406955757 and parameters: {'observation_period_num': 147, 'train_rates': 0.7123499581186975, 'learning_rate': 0.00018442254019411007, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9687138119931552}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:21:08,403][0m Trial 37 finished with value: 0.047025495690518414 and parameters: {'observation_period_num': 5, 'train_rates': 0.813096052411632, 'learning_rate': 0.00013063469266419873, 'batch_size': 209, 'step_size': 14, 'gamma': 0.9349779982372529}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:21:59,104][0m Trial 38 finished with value: 0.039777555730607775 and parameters: {'observation_period_num': 36, 'train_rates': 0.8460693348265876, 'learning_rate': 0.00030410014307230305, 'batch_size': 170, 'step_size': 11, 'gamma': 0.9748914442441236}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:22:42,407][0m Trial 39 finished with value: 0.09234209209680558 and parameters: {'observation_period_num': 77, 'train_rates': 0.8837277537120055, 'learning_rate': 0.0006725866453113098, 'batch_size': 134, 'step_size': 12, 'gamma': 0.9477894667845417}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:23:23,852][0m Trial 40 finished with value: 0.0563846081495285 and parameters: {'observation_period_num': 60, 'train_rates': 0.9308174324333998, 'learning_rate': 0.00018583610572672188, 'batch_size': 228, 'step_size': 10, 'gamma': 0.9282429243332296}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:24:09,657][0m Trial 41 finished with value: 0.03319743648171425 and parameters: {'observation_period_num': 18, 'train_rates': 0.9485604494849555, 'learning_rate': 0.0004514056811406572, 'batch_size': 185, 'step_size': 14, 'gamma': 0.9533265840818894}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:24:54,599][0m Trial 42 finished with value: 0.0437830425798893 and parameters: {'observation_period_num': 19, 'train_rates': 0.970627189927171, 'learning_rate': 0.00045145396491834573, 'batch_size': 185, 'step_size': 14, 'gamma': 0.9893932659873129}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:25:40,372][0m Trial 43 finished with value: 0.043994117213386884 and parameters: {'observation_period_num': 19, 'train_rates': 0.9034753842816821, 'learning_rate': 0.0007311126050908105, 'batch_size': 156, 'step_size': 15, 'gamma': 0.951570700998462}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:26:26,510][0m Trial 44 finished with value: 0.0659552738070488 and parameters: {'observation_period_num': 40, 'train_rates': 0.9484996816170223, 'learning_rate': 0.0002567074382163511, 'batch_size': 205, 'step_size': 14, 'gamma': 0.9745603123063923}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:27:12,118][0m Trial 45 finished with value: 0.1399335414171219 and parameters: {'observation_period_num': 214, 'train_rates': 0.9705946269481852, 'learning_rate': 0.00010334644462791406, 'batch_size': 145, 'step_size': 12, 'gamma': 0.9052864854222904}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:27:45,034][0m Trial 46 finished with value: 0.1731041817862013 and parameters: {'observation_period_num': 54, 'train_rates': 0.6128280912113205, 'learning_rate': 0.0005607145887096944, 'batch_size': 243, 'step_size': 13, 'gamma': 0.8633547457913614}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:28:29,924][0m Trial 47 finished with value: 0.03764250191838242 and parameters: {'observation_period_num': 14, 'train_rates': 0.927143043449436, 'learning_rate': 0.0008893295069221025, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9298244231637545}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:29:14,853][0m Trial 48 finished with value: 0.17662647917780572 and parameters: {'observation_period_num': 34, 'train_rates': 0.8817269835126519, 'learning_rate': 6.807242660385309e-06, 'batch_size': 190, 'step_size': 7, 'gamma': 0.9522583649470069}. Best is trial 24 with value: 0.027379298582673073.[0m
[32m[I 2025-01-04 22:29:54,944][0m Trial 49 finished with value: 0.14880899391073116 and parameters: {'observation_period_num': 5, 'train_rates': 0.7284279901954128, 'learning_rate': 0.0003126707788932031, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8043986723849648}. Best is trial 24 with value: 0.027379298582673073.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AMZN_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6387 | 0.2298
Epoch 2/300, Loss: 0.1973 | 0.0964
Epoch 3/300, Loss: 0.1453 | 0.0995
Epoch 4/300, Loss: 0.1666 | 0.1111
Epoch 5/300, Loss: 0.2198 | 0.1763
Epoch 6/300, Loss: 0.2307 | 0.2235
Epoch 7/300, Loss: 0.1874 | 0.1754
Epoch 8/300, Loss: 0.1293 | 0.0907
Epoch 9/300, Loss: 0.1575 | 0.0955
Epoch 10/300, Loss: 0.1478 | 0.0829
Epoch 11/300, Loss: 0.1278 | 0.0705
Epoch 12/300, Loss: 0.1275 | 0.0995
Epoch 13/300, Loss: 0.1398 | 0.0760
Epoch 14/300, Loss: 0.1213 | 0.0749
Epoch 15/300, Loss: 0.1369 | 0.0921
Epoch 16/300, Loss: 0.1222 | 0.0651
Epoch 17/300, Loss: 0.0974 | 0.0591
Epoch 18/300, Loss: 0.1050 | 0.0658
Epoch 19/300, Loss: 0.1003 | 0.0540
Epoch 20/300, Loss: 0.0952 | 0.0499
Epoch 21/300, Loss: 0.0870 | 0.0515
Epoch 22/300, Loss: 0.0893 | 0.0522
Epoch 23/300, Loss: 0.0858 | 0.0484
Epoch 24/300, Loss: 0.0815 | 0.0469
Epoch 25/300, Loss: 0.0808 | 0.0462
Epoch 26/300, Loss: 0.0829 | 0.0483
Epoch 27/300, Loss: 0.0864 | 0.0558
Epoch 28/300, Loss: 0.0936 | 0.0633
Epoch 29/300, Loss: 0.0953 | 0.0577
Epoch 30/300, Loss: 0.0921 | 0.0678
Epoch 31/300, Loss: 0.0817 | 0.0520
Epoch 32/300, Loss: 0.0852 | 0.0527
Epoch 33/300, Loss: 0.0943 | 0.0552
Epoch 34/300, Loss: 0.1027 | 0.0527
Epoch 35/300, Loss: 0.0936 | 0.0554
Epoch 36/300, Loss: 0.0919 | 0.0475
Epoch 37/300, Loss: 0.0820 | 0.0574
Epoch 38/300, Loss: 0.0866 | 0.0562
Epoch 39/300, Loss: 0.0833 | 0.0483
Epoch 40/300, Loss: 0.0832 | 0.0487
Epoch 41/300, Loss: 0.0779 | 0.0430
Epoch 42/300, Loss: 0.0746 | 0.0414
Epoch 43/300, Loss: 0.0738 | 0.0409
Epoch 44/300, Loss: 0.0729 | 0.0397
Epoch 45/300, Loss: 0.0727 | 0.0396
Epoch 46/300, Loss: 0.0725 | 0.0398
Epoch 47/300, Loss: 0.0741 | 0.0420
Epoch 48/300, Loss: 0.0741 | 0.0423
Epoch 49/300, Loss: 0.0751 | 0.0415
Epoch 50/300, Loss: 0.0738 | 0.0401
Epoch 51/300, Loss: 0.0718 | 0.0399
Epoch 52/300, Loss: 0.0710 | 0.0393
Epoch 53/300, Loss: 0.0734 | 0.0408
Epoch 54/300, Loss: 0.0741 | 0.0420
Epoch 55/300, Loss: 0.0755 | 0.0457
Epoch 56/300, Loss: 0.0756 | 0.0526
Epoch 57/300, Loss: 0.0711 | 0.0477
Epoch 58/300, Loss: 0.0727 | 0.0472
Epoch 59/300, Loss: 0.0726 | 0.0399
Epoch 60/300, Loss: 0.0724 | 0.0438
Epoch 61/300, Loss: 0.0736 | 0.0403
Epoch 62/300, Loss: 0.0722 | 0.0399
Epoch 63/300, Loss: 0.0748 | 0.0425
Epoch 64/300, Loss: 0.0842 | 0.0410
Epoch 65/300, Loss: 0.0943 | 0.0506
Epoch 66/300, Loss: 0.0982 | 0.0505
Epoch 67/300, Loss: 0.0891 | 0.0536
Epoch 68/300, Loss: 0.0861 | 0.0450
Epoch 69/300, Loss: 0.0770 | 0.0452
Epoch 70/300, Loss: 0.0773 | 0.0456
Epoch 71/300, Loss: 0.0722 | 0.0452
Epoch 72/300, Loss: 0.0721 | 0.0434
Epoch 73/300, Loss: 0.0717 | 0.0410
Epoch 74/300, Loss: 0.0727 | 0.0417
Epoch 75/300, Loss: 0.0706 | 0.0402
Epoch 76/300, Loss: 0.0689 | 0.0414
Epoch 77/300, Loss: 0.0687 | 0.0401
Epoch 78/300, Loss: 0.0678 | 0.0408
Epoch 79/300, Loss: 0.0672 | 0.0366
Epoch 80/300, Loss: 0.0659 | 0.0357
Epoch 81/300, Loss: 0.0671 | 0.0366
Epoch 82/300, Loss: 0.0671 | 0.0386
Epoch 83/300, Loss: 0.0698 | 0.0369
Epoch 84/300, Loss: 0.0735 | 0.0393
Epoch 85/300, Loss: 0.0732 | 0.0390
Epoch 86/300, Loss: 0.0697 | 0.0352
Epoch 87/300, Loss: 0.0683 | 0.0365
Epoch 88/300, Loss: 0.0690 | 0.0379
Epoch 89/300, Loss: 0.0691 | 0.0392
Epoch 90/300, Loss: 0.0675 | 0.0396
Epoch 91/300, Loss: 0.0667 | 0.0363
Epoch 92/300, Loss: 0.0648 | 0.0350
Epoch 93/300, Loss: 0.0639 | 0.0354
Epoch 94/300, Loss: 0.0652 | 0.0369
Epoch 95/300, Loss: 0.0656 | 0.0400
Epoch 96/300, Loss: 0.0653 | 0.0388
Epoch 97/300, Loss: 0.0638 | 0.0385
Epoch 98/300, Loss: 0.0633 | 0.0357
Epoch 99/300, Loss: 0.0648 | 0.0351
Epoch 100/300, Loss: 0.0653 | 0.0357
Epoch 101/300, Loss: 0.0650 | 0.0350
Epoch 102/300, Loss: 0.0644 | 0.0343
Epoch 103/300, Loss: 0.0630 | 0.0355
Epoch 104/300, Loss: 0.0628 | 0.0332
Epoch 105/300, Loss: 0.0609 | 0.0326
Epoch 106/300, Loss: 0.0603 | 0.0331
Epoch 107/300, Loss: 0.0605 | 0.0345
Epoch 108/300, Loss: 0.0634 | 0.0352
Epoch 109/300, Loss: 0.0655 | 0.0363
Epoch 110/300, Loss: 0.0645 | 0.0351
Epoch 111/300, Loss: 0.0654 | 0.0360
Epoch 112/300, Loss: 0.0668 | 0.0360
Epoch 113/300, Loss: 0.0669 | 0.0378
Epoch 114/300, Loss: 0.0633 | 0.0393
Epoch 115/300, Loss: 0.0632 | 0.0348
Epoch 116/300, Loss: 0.0654 | 0.0333
Epoch 117/300, Loss: 0.0610 | 0.0308
Epoch 118/300, Loss: 0.0620 | 0.0355
Epoch 119/300, Loss: 0.0629 | 0.0348
Epoch 120/300, Loss: 0.0657 | 0.0398
Epoch 121/300, Loss: 0.0649 | 0.0379
Epoch 122/300, Loss: 0.0635 | 0.0384
Epoch 123/300, Loss: 0.0623 | 0.0364
Epoch 124/300, Loss: 0.0639 | 0.0357
Epoch 125/300, Loss: 0.0611 | 0.0361
Epoch 126/300, Loss: 0.0588 | 0.0319
Epoch 127/300, Loss: 0.0591 | 0.0303
Epoch 128/300, Loss: 0.0578 | 0.0300
Epoch 129/300, Loss: 0.0588 | 0.0351
Epoch 130/300, Loss: 0.0611 | 0.0347
Epoch 131/300, Loss: 0.0600 | 0.0328
Epoch 132/300, Loss: 0.0580 | 0.0318
Epoch 133/300, Loss: 0.0568 | 0.0298
Epoch 134/300, Loss: 0.0551 | 0.0292
Epoch 135/300, Loss: 0.0552 | 0.0300
Epoch 136/300, Loss: 0.0556 | 0.0316
Epoch 137/300, Loss: 0.0558 | 0.0330
Epoch 138/300, Loss: 0.0568 | 0.0329
Epoch 139/300, Loss: 0.0582 | 0.0355
Epoch 140/300, Loss: 0.0586 | 0.0333
Epoch 141/300, Loss: 0.0578 | 0.0310
Epoch 142/300, Loss: 0.0568 | 0.0300
Epoch 143/300, Loss: 0.0564 | 0.0339
Epoch 144/300, Loss: 0.0594 | 0.0327
Epoch 145/300, Loss: 0.0581 | 0.0285
Epoch 146/300, Loss: 0.0580 | 0.0319
Epoch 147/300, Loss: 0.0588 | 0.0307
Epoch 148/300, Loss: 0.0589 | 0.0306
Epoch 149/300, Loss: 0.0564 | 0.0306
Epoch 150/300, Loss: 0.0562 | 0.0304
Epoch 151/300, Loss: 0.0540 | 0.0302
Epoch 152/300, Loss: 0.0524 | 0.0326
Epoch 153/300, Loss: 0.0534 | 0.0348
Epoch 154/300, Loss: 0.0546 | 0.0320
Epoch 155/300, Loss: 0.0558 | 0.0318
Epoch 156/300, Loss: 0.0554 | 0.0330
Epoch 157/300, Loss: 0.0554 | 0.0407
Epoch 158/300, Loss: 0.0538 | 0.0347
Epoch 159/300, Loss: 0.0562 | 0.0367
Epoch 160/300, Loss: 0.0602 | 0.0341
Epoch 161/300, Loss: 0.0603 | 0.0292
Epoch 162/300, Loss: 0.0570 | 0.0291
Epoch 163/300, Loss: 0.0541 | 0.0274
Epoch 164/300, Loss: 0.0525 | 0.0267
Epoch 165/300, Loss: 0.0534 | 0.0294
Epoch 166/300, Loss: 0.0545 | 0.0306
Epoch 167/300, Loss: 0.0547 | 0.0307
Epoch 168/300, Loss: 0.0532 | 0.0294
Epoch 169/300, Loss: 0.0528 | 0.0293
Epoch 170/300, Loss: 0.0544 | 0.0311
Epoch 171/300, Loss: 0.0561 | 0.0303
Epoch 172/300, Loss: 0.0561 | 0.0307
Epoch 173/300, Loss: 0.0571 | 0.0318
Epoch 174/300, Loss: 0.0568 | 0.0279
Epoch 175/300, Loss: 0.0537 | 0.0288
Epoch 176/300, Loss: 0.0540 | 0.0310
Epoch 177/300, Loss: 0.0542 | 0.0323
Epoch 178/300, Loss: 0.0539 | 0.0356
Epoch 179/300, Loss: 0.0526 | 0.0302
Epoch 180/300, Loss: 0.0520 | 0.0332
Epoch 181/300, Loss: 0.0528 | 0.0297
Epoch 182/300, Loss: 0.0518 | 0.0305
Epoch 183/300, Loss: 0.0495 | 0.0267
Epoch 184/300, Loss: 0.0497 | 0.0271
Epoch 185/300, Loss: 0.0499 | 0.0266
Epoch 186/300, Loss: 0.0504 | 0.0294
Epoch 187/300, Loss: 0.0501 | 0.0283
Epoch 188/300, Loss: 0.0496 | 0.0318
Epoch 189/300, Loss: 0.0488 | 0.0303
Epoch 190/300, Loss: 0.0492 | 0.0322
Epoch 191/300, Loss: 0.0537 | 0.0291
Epoch 192/300, Loss: 0.0534 | 0.0290
Epoch 193/300, Loss: 0.0505 | 0.0315
Epoch 194/300, Loss: 0.0500 | 0.0316
Epoch 195/300, Loss: 0.0497 | 0.0305
Epoch 196/300, Loss: 0.0480 | 0.0283
Epoch 197/300, Loss: 0.0488 | 0.0314
Epoch 198/300, Loss: 0.0492 | 0.0344
Epoch 199/300, Loss: 0.0490 | 0.0325
Epoch 200/300, Loss: 0.0492 | 0.0353
Epoch 201/300, Loss: 0.0488 | 0.0342
Epoch 202/300, Loss: 0.0480 | 0.0299
Epoch 203/300, Loss: 0.0474 | 0.0340
Epoch 204/300, Loss: 0.0476 | 0.0334
Epoch 205/300, Loss: 0.0484 | 0.0328
Epoch 206/300, Loss: 0.0485 | 0.0338
Epoch 207/300, Loss: 0.0477 | 0.0352
Epoch 208/300, Loss: 0.0464 | 0.0326
Epoch 209/300, Loss: 0.0469 | 0.0332
Epoch 210/300, Loss: 0.0495 | 0.0306
Epoch 211/300, Loss: 0.0488 | 0.0331
Epoch 212/300, Loss: 0.0470 | 0.0294
Epoch 213/300, Loss: 0.0465 | 0.0346
Epoch 214/300, Loss: 0.0468 | 0.0328
Epoch 215/300, Loss: 0.0460 | 0.0344
Epoch 216/300, Loss: 0.0461 | 0.0335
Epoch 217/300, Loss: 0.0458 | 0.0318
Epoch 218/300, Loss: 0.0456 | 0.0343
Epoch 219/300, Loss: 0.0456 | 0.0324
Epoch 220/300, Loss: 0.0451 | 0.0321
Epoch 221/300, Loss: 0.0447 | 0.0331
Epoch 222/300, Loss: 0.0446 | 0.0347
Epoch 223/300, Loss: 0.0445 | 0.0363
Epoch 224/300, Loss: 0.0447 | 0.0337
Epoch 225/300, Loss: 0.0446 | 0.0352
Epoch 226/300, Loss: 0.0441 | 0.0366
Epoch 227/300, Loss: 0.0439 | 0.0354
Epoch 228/300, Loss: 0.0437 | 0.0335
Epoch 229/300, Loss: 0.0438 | 0.0343
Epoch 230/300, Loss: 0.0439 | 0.0331
Epoch 231/300, Loss: 0.0438 | 0.0336
Epoch 232/300, Loss: 0.0437 | 0.0354
Epoch 233/300, Loss: 0.0435 | 0.0347
Epoch 234/300, Loss: 0.0439 | 0.0381
Epoch 235/300, Loss: 0.0447 | 0.0354
Epoch 236/300, Loss: 0.0442 | 0.0391
Epoch 237/300, Loss: 0.0437 | 0.0365
Epoch 238/300, Loss: 0.0434 | 0.0340
Epoch 239/300, Loss: 0.0434 | 0.0345
Epoch 240/300, Loss: 0.0437 | 0.0371
Epoch 241/300, Loss: 0.0440 | 0.0372
Epoch 242/300, Loss: 0.0439 | 0.0391
Epoch 243/300, Loss: 0.0439 | 0.0388
Epoch 244/300, Loss: 0.0443 | 0.0383
Epoch 245/300, Loss: 0.0438 | 0.0404
Epoch 246/300, Loss: 0.0429 | 0.0399
Epoch 247/300, Loss: 0.0429 | 0.0385
Epoch 248/300, Loss: 0.0424 | 0.0393
Epoch 249/300, Loss: 0.0420 | 0.0379
Epoch 250/300, Loss: 0.0419 | 0.0370
Epoch 251/300, Loss: 0.0423 | 0.0388
Epoch 252/300, Loss: 0.0434 | 0.0383
Epoch 253/300, Loss: 0.0443 | 0.0381
Epoch 254/300, Loss: 0.0437 | 0.0372
Epoch 255/300, Loss: 0.0429 | 0.0380
Epoch 256/300, Loss: 0.0431 | 0.0411
Epoch 257/300, Loss: 0.0460 | 0.0421
Epoch 258/300, Loss: 0.0487 | 0.0453
Epoch 259/300, Loss: 0.0482 | 0.0450
Epoch 260/300, Loss: 0.0457 | 0.0481
Epoch 261/300, Loss: 0.0463 | 0.0394
Epoch 262/300, Loss: 0.0485 | 0.0424
Epoch 263/300, Loss: 0.0470 | 0.0396
Epoch 264/300, Loss: 0.0460 | 0.0405
Epoch 265/300, Loss: 0.0433 | 0.0416
Epoch 266/300, Loss: 0.0432 | 0.0378
Epoch 267/300, Loss: 0.0422 | 0.0401
Epoch 268/300, Loss: 0.0421 | 0.0401
Epoch 269/300, Loss: 0.0418 | 0.0382
Epoch 270/300, Loss: 0.0412 | 0.0422
Epoch 271/300, Loss: 0.0400 | 0.0368
Epoch 272/300, Loss: 0.0419 | 0.0398
Epoch 273/300, Loss: 0.0413 | 0.0430
Epoch 274/300, Loss: 0.0404 | 0.0389
Epoch 275/300, Loss: 0.0366 | 0.0390
Epoch 276/300, Loss: 0.0385 | 0.0418
Epoch 277/300, Loss: 0.0472 | 0.0360
Epoch 278/300, Loss: 0.0447 | 0.0354
Epoch 279/300, Loss: 0.0424 | 0.0354
Epoch 280/300, Loss: 0.0416 | 0.0379
Epoch 281/300, Loss: 0.0415 | 0.0401
Epoch 282/300, Loss: 0.0408 | 0.0424
Epoch 283/300, Loss: 0.0404 | 0.0474
Epoch 284/300, Loss: 0.0386 | 0.0425
Epoch 285/300, Loss: 0.0416 | 0.0368
Epoch 286/300, Loss: 0.0417 | 0.0415
Epoch 287/300, Loss: 0.0408 | 0.0503
Epoch 288/300, Loss: 0.0388 | 0.0376
Epoch 289/300, Loss: 0.0417 | 0.0400
Epoch 290/300, Loss: 0.0397 | 0.0502
Epoch 291/300, Loss: 0.0412 | 0.0334
Epoch 292/300, Loss: 0.0422 | 0.0375
Epoch 293/300, Loss: 0.0370 | 0.0361
Epoch 294/300, Loss: 0.0398 | 0.0375
Epoch 295/300, Loss: 0.0367 | 0.0380
Epoch 296/300, Loss: 0.0372 | 0.0384
Epoch 297/300, Loss: 0.0409 | 0.0392
Epoch 298/300, Loss: 0.0476 | 0.0351
Epoch 299/300, Loss: 0.0436 | 0.0291
Epoch 300/300, Loss: 0.0428 | 0.0317
Runtime (seconds): 130.7404453754425
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 129.22856258810498
RMSE: 11.367874145507812
MAE: 11.367874145507812
R-squared: nan
[194.37213]
