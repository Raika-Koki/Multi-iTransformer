[32m[I 2025-02-03 23:05:34,931][0m A new study created in memory with name: no-name-9a61aae1-aee5-4fe8-9cf7-3e3da4605d28[0m
[32m[I 2025-02-03 23:06:30,547][0m Trial 0 finished with value: 0.24500443987224413 and parameters: {'observation_period_num': 100, 'train_rates': 0.9598415062161533, 'learning_rate': 1.5036778184409684e-05, 'batch_size': 111, 'step_size': 5, 'gamma': 0.8928167802809714}. Best is trial 0 with value: 0.24500443987224413.[0m
[32m[I 2025-02-03 23:07:39,565][0m Trial 1 finished with value: 0.5947088373902057 and parameters: {'observation_period_num': 114, 'train_rates': 0.838405119507718, 'learning_rate': 2.9311184493148295e-06, 'batch_size': 77, 'step_size': 4, 'gamma': 0.8237362076806509}. Best is trial 0 with value: 0.24500443987224413.[0m
[32m[I 2025-02-03 23:08:08,347][0m Trial 2 finished with value: 1.6509096591458845 and parameters: {'observation_period_num': 87, 'train_rates': 0.6379970418349176, 'learning_rate': 1.3259482642405792e-06, 'batch_size': 174, 'step_size': 2, 'gamma': 0.8622059267347474}. Best is trial 0 with value: 0.24500443987224413.[0m
[32m[I 2025-02-03 23:08:34,430][0m Trial 3 finished with value: 0.10108775578794026 and parameters: {'observation_period_num': 83, 'train_rates': 0.8173503502937968, 'learning_rate': 9.199352000823433e-05, 'batch_size': 218, 'step_size': 11, 'gamma': 0.8206339547547558}. Best is trial 3 with value: 0.10108775578794026.[0m
[32m[I 2025-02-03 23:09:32,572][0m Trial 4 finished with value: 0.31137363612651825 and parameters: {'observation_period_num': 174, 'train_rates': 0.9288408785799931, 'learning_rate': 6.295191585719018e-06, 'batch_size': 99, 'step_size': 5, 'gamma': 0.9165207707148066}. Best is trial 3 with value: 0.10108775578794026.[0m
[32m[I 2025-02-03 23:11:16,476][0m Trial 5 finished with value: 0.5219897807076357 and parameters: {'observation_period_num': 168, 'train_rates': 0.6038752697673296, 'learning_rate': 3.268434195065621e-06, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9839850816597571}. Best is trial 3 with value: 0.10108775578794026.[0m
[32m[I 2025-02-03 23:11:46,773][0m Trial 6 finished with value: 0.18584117823759316 and parameters: {'observation_period_num': 143, 'train_rates': 0.7876549467695819, 'learning_rate': 0.0007546382255886257, 'batch_size': 175, 'step_size': 10, 'gamma': 0.9684679245123977}. Best is trial 3 with value: 0.10108775578794026.[0m
[32m[I 2025-02-03 23:12:27,465][0m Trial 7 finished with value: 0.18998108804225922 and parameters: {'observation_period_num': 201, 'train_rates': 0.7366222241567892, 'learning_rate': 0.00028190750496932663, 'batch_size': 121, 'step_size': 13, 'gamma': 0.9234419629742454}. Best is trial 3 with value: 0.10108775578794026.[0m
[32m[I 2025-02-03 23:13:22,793][0m Trial 8 finished with value: 0.4886288340203464 and parameters: {'observation_period_num': 35, 'train_rates': 0.8904641548357517, 'learning_rate': 1.0181441097839066e-05, 'batch_size': 105, 'step_size': 1, 'gamma': 0.9310194145459538}. Best is trial 3 with value: 0.10108775578794026.[0m
[32m[I 2025-02-03 23:14:58,484][0m Trial 9 finished with value: 0.06478760967015895 and parameters: {'observation_period_num': 89, 'train_rates': 0.8719199655023067, 'learning_rate': 0.0004890513142985601, 'batch_size': 57, 'step_size': 3, 'gamma': 0.8255838120613189}. Best is trial 9 with value: 0.06478760967015895.[0m
[32m[I 2025-02-03 23:17:17,609][0m Trial 10 finished with value: 0.25731818849551846 and parameters: {'observation_period_num': 243, 'train_rates': 0.7157974190615138, 'learning_rate': 5.71709380730323e-05, 'batch_size': 32, 'step_size': 8, 'gamma': 0.755108634141421}. Best is trial 9 with value: 0.06478760967015895.[0m
[32m[I 2025-02-03 23:17:41,211][0m Trial 11 finished with value: 0.06882218590804509 and parameters: {'observation_period_num': 44, 'train_rates': 0.84630943445608, 'learning_rate': 9.947488351629141e-05, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8080727578524215}. Best is trial 9 with value: 0.06478760967015895.[0m
[32m[I 2025-02-03 23:18:06,810][0m Trial 12 finished with value: 0.046893911561100544 and parameters: {'observation_period_num': 12, 'train_rates': 0.8835768238447982, 'learning_rate': 0.0002731153421740304, 'batch_size': 243, 'step_size': 15, 'gamma': 0.7862979372317981}. Best is trial 12 with value: 0.046893911561100544.[0m
[32m[I 2025-02-03 23:18:45,541][0m Trial 13 finished with value: 0.03808997867484687 and parameters: {'observation_period_num': 5, 'train_rates': 0.8996028080637246, 'learning_rate': 0.0008645737119396396, 'batch_size': 163, 'step_size': 8, 'gamma': 0.7670687346245635}. Best is trial 13 with value: 0.03808997867484687.[0m
[32m[I 2025-02-03 23:19:25,485][0m Trial 14 finished with value: 0.05128762871026993 and parameters: {'observation_period_num': 10, 'train_rates': 0.9673284349647419, 'learning_rate': 0.0002260914535691201, 'batch_size': 162, 'step_size': 7, 'gamma': 0.7608018183926332}. Best is trial 13 with value: 0.03808997867484687.[0m
[32m[I 2025-02-03 23:19:53,391][0m Trial 15 finished with value: 0.034619764569732876 and parameters: {'observation_period_num': 6, 'train_rates': 0.9085100523824214, 'learning_rate': 0.0009907320960458766, 'batch_size': 224, 'step_size': 10, 'gamma': 0.784296085951965}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:20:22,980][0m Trial 16 finished with value: 0.05545578836760622 and parameters: {'observation_period_num': 55, 'train_rates': 0.9191076527579382, 'learning_rate': 0.0008207884587344284, 'batch_size': 212, 'step_size': 10, 'gamma': 0.8567134075398102}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:20:54,524][0m Trial 17 finished with value: 0.13163770735263824 and parameters: {'observation_period_num': 5, 'train_rates': 0.981830956990465, 'learning_rate': 2.1670869993969326e-05, 'batch_size': 207, 'step_size': 7, 'gamma': 0.7784874197406316}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:21:30,468][0m Trial 18 finished with value: 0.11749824025995055 and parameters: {'observation_period_num': 65, 'train_rates': 0.7644153583194085, 'learning_rate': 3.754410976893313e-05, 'batch_size': 149, 'step_size': 12, 'gamma': 0.7916313160135698}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:22:04,739][0m Trial 19 finished with value: 0.05313810025366534 and parameters: {'observation_period_num': 28, 'train_rates': 0.9229297135146938, 'learning_rate': 0.0009472671100312778, 'batch_size': 191, 'step_size': 9, 'gamma': 0.8428373046880212}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:22:41,245][0m Trial 20 finished with value: 0.19222291627907018 and parameters: {'observation_period_num': 66, 'train_rates': 0.685740382726693, 'learning_rate': 0.00013299455446738153, 'batch_size': 136, 'step_size': 13, 'gamma': 0.7713352435383428}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:23:07,833][0m Trial 21 finished with value: 0.04410055941460948 and parameters: {'observation_period_num': 15, 'train_rates': 0.8709342735785026, 'learning_rate': 0.0003346897727985807, 'batch_size': 245, 'step_size': 14, 'gamma': 0.7891631516297898}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:23:33,582][0m Trial 22 finished with value: 0.05479745420117704 and parameters: {'observation_period_num': 40, 'train_rates': 0.8546409618061928, 'learning_rate': 0.0004671700127296806, 'batch_size': 233, 'step_size': 13, 'gamma': 0.8055569762385204}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:24:01,820][0m Trial 23 finished with value: 0.05081301638119513 and parameters: {'observation_period_num': 25, 'train_rates': 0.9053953348785396, 'learning_rate': 0.000489255128922536, 'batch_size': 230, 'step_size': 7, 'gamma': 0.7560524215742911}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:24:32,098][0m Trial 24 finished with value: 0.05575692902008692 and parameters: {'observation_period_num': 6, 'train_rates': 0.8056793221195454, 'learning_rate': 0.00018974711856570476, 'batch_size': 191, 'step_size': 9, 'gamma': 0.7909060527379096}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:24:58,150][0m Trial 25 finished with value: 0.06657973676919937 and parameters: {'observation_period_num': 55, 'train_rates': 0.9410660011607221, 'learning_rate': 0.00044280460290143265, 'batch_size': 251, 'step_size': 11, 'gamma': 0.8384678543492051}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:25:28,361][0m Trial 26 finished with value: 0.06549332780441688 and parameters: {'observation_period_num': 26, 'train_rates': 0.8696668509603661, 'learning_rate': 0.0008731866115192408, 'batch_size': 201, 'step_size': 14, 'gamma': 0.8034649865138386}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:25:54,854][0m Trial 27 finished with value: 0.046098400388749884 and parameters: {'observation_period_num': 22, 'train_rates': 0.8286166994351196, 'learning_rate': 0.0004072058603325672, 'batch_size': 231, 'step_size': 6, 'gamma': 0.8900268802425232}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:26:28,473][0m Trial 28 finished with value: 0.1218157708644867 and parameters: {'observation_period_num': 132, 'train_rates': 0.94636939328084, 'learning_rate': 0.00014699930753141448, 'batch_size': 180, 'step_size': 9, 'gamma': 0.7678698007331627}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:27:06,050][0m Trial 29 finished with value: 0.0823095440864563 and parameters: {'observation_period_num': 106, 'train_rates': 0.8947136020354389, 'learning_rate': 5.827788959029531e-05, 'batch_size': 150, 'step_size': 12, 'gamma': 0.8739971645129975}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:27:35,662][0m Trial 30 finished with value: 0.1363607496023178 and parameters: {'observation_period_num': 76, 'train_rates': 0.9894265172919972, 'learning_rate': 0.00031894643524406737, 'batch_size': 221, 'step_size': 5, 'gamma': 0.7789994959682314}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:28:01,613][0m Trial 31 finished with value: 0.044820299889277486 and parameters: {'observation_period_num': 21, 'train_rates': 0.8245555615805189, 'learning_rate': 0.000578685246980603, 'batch_size': 237, 'step_size': 6, 'gamma': 0.9002027155219314}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:28:25,696][0m Trial 32 finished with value: 0.06271687465171953 and parameters: {'observation_period_num': 49, 'train_rates': 0.7879335798200204, 'learning_rate': 0.0006372998369673479, 'batch_size': 241, 'step_size': 6, 'gamma': 0.8875800100635136}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:29:33,515][0m Trial 33 finished with value: 0.039889476765161275 and parameters: {'observation_period_num': 16, 'train_rates': 0.851990784512756, 'learning_rate': 0.000919758279132161, 'batch_size': 83, 'step_size': 8, 'gamma': 0.9082344962323238}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:30:35,402][0m Trial 34 finished with value: 0.056333458915731376 and parameters: {'observation_period_num': 37, 'train_rates': 0.8515820370758788, 'learning_rate': 0.0009819396661338803, 'batch_size': 89, 'step_size': 8, 'gamma': 0.9398584851647935}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:36:28,371][0m Trial 35 finished with value: 0.04980789234279205 and parameters: {'observation_period_num': 15, 'train_rates': 0.9119578254011557, 'learning_rate': 0.0003327743046621946, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9549225628273282}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:37:48,850][0m Trial 36 finished with value: 0.06842728971193233 and parameters: {'observation_period_num': 65, 'train_rates': 0.875557835544852, 'learning_rate': 0.0006600768485026143, 'batch_size': 69, 'step_size': 8, 'gamma': 0.9027575656600871}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:38:39,716][0m Trial 37 finished with value: 0.04983108169916603 and parameters: {'observation_period_num': 36, 'train_rates': 0.9508732751097404, 'learning_rate': 0.00019129216758642602, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8183616729987113}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:39:42,467][0m Trial 38 finished with value: 0.15185158148290603 and parameters: {'observation_period_num': 204, 'train_rates': 0.8616884333375585, 'learning_rate': 0.0006716039188065607, 'batch_size': 87, 'step_size': 11, 'gamma': 0.839524391291038}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:40:16,321][0m Trial 39 finished with value: 0.970575184450237 and parameters: {'observation_period_num': 119, 'train_rates': 0.7695447347963345, 'learning_rate': 1.664306689452061e-06, 'batch_size': 163, 'step_size': 4, 'gamma': 0.8788246135731884}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:41:45,591][0m Trial 40 finished with value: 0.07153011381626129 and parameters: {'observation_period_num': 5, 'train_rates': 0.8984789175164081, 'learning_rate': 1.1810827277302015e-05, 'batch_size': 66, 'step_size': 14, 'gamma': 0.7507627596759614}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:42:12,007][0m Trial 41 finished with value: 0.04411959948605449 and parameters: {'observation_period_num': 18, 'train_rates': 0.8240825138333012, 'learning_rate': 0.0005815172195671806, 'batch_size': 221, 'step_size': 4, 'gamma': 0.9076207683427866}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:42:42,218][0m Trial 42 finished with value: 0.0523288083434961 and parameters: {'observation_period_num': 22, 'train_rates': 0.8101835078146932, 'learning_rate': 0.00036119908048151195, 'batch_size': 194, 'step_size': 3, 'gamma': 0.9144309913722564}. Best is trial 15 with value: 0.034619764569732876.[0m
Early stopping at epoch 96
[32m[I 2025-02-03 23:43:09,520][0m Trial 43 finished with value: 0.10504477287226535 and parameters: {'observation_period_num': 33, 'train_rates': 0.8394026085803574, 'learning_rate': 0.0009769514793765842, 'batch_size': 219, 'step_size': 1, 'gamma': 0.861066680075112}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:44:00,123][0m Trial 44 finished with value: 0.10975927110585867 and parameters: {'observation_period_num': 153, 'train_rates': 0.9320141283561022, 'learning_rate': 0.0006038109345177497, 'batch_size': 115, 'step_size': 4, 'gamma': 0.944328438381041}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:44:26,083][0m Trial 45 finished with value: 0.07246770187766252 and parameters: {'observation_period_num': 44, 'train_rates': 0.8855067538510577, 'learning_rate': 0.00024972176563012137, 'batch_size': 250, 'step_size': 2, 'gamma': 0.9129780494070123}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:44:52,653][0m Trial 46 finished with value: 0.35390493573594145 and parameters: {'observation_period_num': 91, 'train_rates': 0.830118678932602, 'learning_rate': 6.568053394380749e-06, 'batch_size': 224, 'step_size': 9, 'gamma': 0.9298642249970982}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:46:50,033][0m Trial 47 finished with value: 0.05505196687890523 and parameters: {'observation_period_num': 15, 'train_rates': 0.7723509997555705, 'learning_rate': 0.0007152866644691717, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9890497710164553}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:47:20,722][0m Trial 48 finished with value: 0.24777730037055182 and parameters: {'observation_period_num': 252, 'train_rates': 0.7990733724031678, 'learning_rate': 8.454210774687238e-05, 'batch_size': 179, 'step_size': 12, 'gamma': 0.7676532619322458}. Best is trial 15 with value: 0.034619764569732876.[0m
[32m[I 2025-02-03 23:47:47,899][0m Trial 49 finished with value: 0.07432468590859152 and parameters: {'observation_period_num': 54, 'train_rates': 0.7456355783816071, 'learning_rate': 0.0005157950965589154, 'batch_size': 208, 'step_size': 8, 'gamma': 0.9611823307332714}. Best is trial 15 with value: 0.034619764569732876.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5838 | 0.2485
Epoch 2/300, Loss: 0.2048 | 0.1265
Epoch 3/300, Loss: 0.1395 | 0.3235
Epoch 4/300, Loss: 0.1235 | 0.1162
Epoch 5/300, Loss: 0.0959 | 0.0911
Epoch 6/300, Loss: 0.0902 | 0.0868
Epoch 7/300, Loss: 0.0816 | 0.0728
Epoch 8/300, Loss: 0.0740 | 0.0726
Epoch 9/300, Loss: 0.0711 | 0.0675
Epoch 10/300, Loss: 0.0724 | 0.0724
Epoch 11/300, Loss: 0.0673 | 0.0905
Epoch 12/300, Loss: 0.0654 | 0.0686
Epoch 13/300, Loss: 0.0640 | 0.0717
Epoch 14/300, Loss: 0.0714 | 0.1003
Epoch 15/300, Loss: 0.0695 | 0.0698
Epoch 16/300, Loss: 0.0634 | 0.0631
Epoch 17/300, Loss: 0.0682 | 0.0678
Epoch 18/300, Loss: 0.0643 | 0.0720
Epoch 19/300, Loss: 0.0633 | 0.0629
Epoch 20/300, Loss: 0.0598 | 0.0654
Epoch 21/300, Loss: 0.0704 | 0.0650
Epoch 22/300, Loss: 0.0801 | 0.0795
Epoch 23/300, Loss: 0.1091 | 0.2261
Epoch 24/300, Loss: 0.0989 | 0.0959
Epoch 25/300, Loss: 0.0737 | 0.0725
Epoch 26/300, Loss: 0.0596 | 0.0565
Epoch 27/300, Loss: 0.0555 | 0.0542
Epoch 28/300, Loss: 0.0527 | 0.0508
Epoch 29/300, Loss: 0.0531 | 0.0538
Epoch 30/300, Loss: 0.0531 | 0.0500
Epoch 31/300, Loss: 0.0553 | 0.0645
Epoch 32/300, Loss: 0.0548 | 0.0505
Epoch 33/300, Loss: 0.0585 | 0.0657
Epoch 34/300, Loss: 0.0562 | 0.0500
Epoch 35/300, Loss: 0.0597 | 0.0696
Epoch 36/300, Loss: 0.0531 | 0.0531
Epoch 37/300, Loss: 0.0508 | 0.0620
Epoch 38/300, Loss: 0.0481 | 0.0490
Epoch 39/300, Loss: 0.0469 | 0.0532
Epoch 40/300, Loss: 0.0457 | 0.0471
Epoch 41/300, Loss: 0.0449 | 0.0490
Epoch 42/300, Loss: 0.0445 | 0.0464
Epoch 43/300, Loss: 0.0442 | 0.0469
Epoch 44/300, Loss: 0.0440 | 0.0462
Epoch 45/300, Loss: 0.0438 | 0.0461
Epoch 46/300, Loss: 0.0436 | 0.0460
Epoch 47/300, Loss: 0.0435 | 0.0459
Epoch 48/300, Loss: 0.0433 | 0.0458
Epoch 49/300, Loss: 0.0430 | 0.0457
Epoch 50/300, Loss: 0.0428 | 0.0455
Epoch 51/300, Loss: 0.0426 | 0.0453
Epoch 52/300, Loss: 0.0425 | 0.0451
Epoch 53/300, Loss: 0.0423 | 0.0449
Epoch 54/300, Loss: 0.0422 | 0.0448
Epoch 55/300, Loss: 0.0420 | 0.0446
Epoch 56/300, Loss: 0.0419 | 0.0445
Epoch 57/300, Loss: 0.0418 | 0.0444
Epoch 58/300, Loss: 0.0417 | 0.0443
Epoch 59/300, Loss: 0.0416 | 0.0442
Epoch 60/300, Loss: 0.0414 | 0.0441
Epoch 61/300, Loss: 0.0413 | 0.0439
Epoch 62/300, Loss: 0.0412 | 0.0439
Epoch 63/300, Loss: 0.0411 | 0.0438
Epoch 64/300, Loss: 0.0410 | 0.0437
Epoch 65/300, Loss: 0.0409 | 0.0436
Epoch 66/300, Loss: 0.0408 | 0.0435
Epoch 67/300, Loss: 0.0408 | 0.0434
Epoch 68/300, Loss: 0.0407 | 0.0434
Epoch 69/300, Loss: 0.0406 | 0.0433
Epoch 70/300, Loss: 0.0405 | 0.0432
Epoch 71/300, Loss: 0.0404 | 0.0432
Epoch 72/300, Loss: 0.0404 | 0.0431
Epoch 73/300, Loss: 0.0403 | 0.0431
Epoch 74/300, Loss: 0.0403 | 0.0430
Epoch 75/300, Loss: 0.0402 | 0.0430
Epoch 76/300, Loss: 0.0401 | 0.0429
Epoch 77/300, Loss: 0.0401 | 0.0429
Epoch 78/300, Loss: 0.0400 | 0.0428
Epoch 79/300, Loss: 0.0400 | 0.0428
Epoch 80/300, Loss: 0.0399 | 0.0427
Epoch 81/300, Loss: 0.0399 | 0.0427
Epoch 82/300, Loss: 0.0398 | 0.0427
Epoch 83/300, Loss: 0.0398 | 0.0426
Epoch 84/300, Loss: 0.0397 | 0.0426
Epoch 85/300, Loss: 0.0397 | 0.0426
Epoch 86/300, Loss: 0.0396 | 0.0425
Epoch 87/300, Loss: 0.0396 | 0.0425
Epoch 88/300, Loss: 0.0396 | 0.0425
Epoch 89/300, Loss: 0.0395 | 0.0424
Epoch 90/300, Loss: 0.0395 | 0.0424
Epoch 91/300, Loss: 0.0395 | 0.0424
Epoch 92/300, Loss: 0.0394 | 0.0424
Epoch 93/300, Loss: 0.0394 | 0.0423
Epoch 94/300, Loss: 0.0394 | 0.0423
Epoch 95/300, Loss: 0.0394 | 0.0423
Epoch 96/300, Loss: 0.0393 | 0.0423
Epoch 97/300, Loss: 0.0393 | 0.0423
Epoch 98/300, Loss: 0.0393 | 0.0422
Epoch 99/300, Loss: 0.0393 | 0.0422
Epoch 100/300, Loss: 0.0392 | 0.0422
Epoch 101/300, Loss: 0.0392 | 0.0422
Epoch 102/300, Loss: 0.0392 | 0.0422
Epoch 103/300, Loss: 0.0392 | 0.0422
Epoch 104/300, Loss: 0.0392 | 0.0422
Epoch 105/300, Loss: 0.0391 | 0.0421
Epoch 106/300, Loss: 0.0391 | 0.0421
Epoch 107/300, Loss: 0.0391 | 0.0421
Epoch 108/300, Loss: 0.0391 | 0.0421
Epoch 109/300, Loss: 0.0391 | 0.0421
Epoch 110/300, Loss: 0.0391 | 0.0421
Epoch 111/300, Loss: 0.0391 | 0.0421
Epoch 112/300, Loss: 0.0390 | 0.0421
Epoch 113/300, Loss: 0.0390 | 0.0421
Epoch 114/300, Loss: 0.0390 | 0.0420
Epoch 115/300, Loss: 0.0390 | 0.0420
Epoch 116/300, Loss: 0.0390 | 0.0420
Epoch 117/300, Loss: 0.0390 | 0.0420
Epoch 118/300, Loss: 0.0390 | 0.0420
Epoch 119/300, Loss: 0.0390 | 0.0420
Epoch 120/300, Loss: 0.0390 | 0.0420
Epoch 121/300, Loss: 0.0389 | 0.0420
Epoch 122/300, Loss: 0.0389 | 0.0420
Epoch 123/300, Loss: 0.0389 | 0.0420
Epoch 124/300, Loss: 0.0389 | 0.0420
Epoch 125/300, Loss: 0.0389 | 0.0420
Epoch 126/300, Loss: 0.0389 | 0.0420
Epoch 127/300, Loss: 0.0389 | 0.0420
Epoch 128/300, Loss: 0.0389 | 0.0420
Epoch 129/300, Loss: 0.0389 | 0.0420
Epoch 130/300, Loss: 0.0389 | 0.0419
Epoch 131/300, Loss: 0.0389 | 0.0419
Epoch 132/300, Loss: 0.0389 | 0.0419
Epoch 133/300, Loss: 0.0389 | 0.0419
Epoch 134/300, Loss: 0.0389 | 0.0419
Epoch 135/300, Loss: 0.0389 | 0.0419
Epoch 136/300, Loss: 0.0389 | 0.0419
Epoch 137/300, Loss: 0.0388 | 0.0419
Epoch 138/300, Loss: 0.0388 | 0.0419
Epoch 139/300, Loss: 0.0388 | 0.0419
Epoch 140/300, Loss: 0.0388 | 0.0419
Epoch 141/300, Loss: 0.0388 | 0.0419
Epoch 142/300, Loss: 0.0388 | 0.0419
Epoch 143/300, Loss: 0.0388 | 0.0419
Epoch 144/300, Loss: 0.0388 | 0.0419
Epoch 145/300, Loss: 0.0388 | 0.0419
Epoch 146/300, Loss: 0.0388 | 0.0419
Epoch 147/300, Loss: 0.0388 | 0.0419
Epoch 148/300, Loss: 0.0388 | 0.0419
Epoch 149/300, Loss: 0.0388 | 0.0419
Epoch 150/300, Loss: 0.0388 | 0.0419
Epoch 151/300, Loss: 0.0388 | 0.0419
Epoch 152/300, Loss: 0.0388 | 0.0419
Epoch 153/300, Loss: 0.0388 | 0.0419
Epoch 154/300, Loss: 0.0388 | 0.0419
Epoch 155/300, Loss: 0.0388 | 0.0419
Epoch 156/300, Loss: 0.0388 | 0.0419
Epoch 157/300, Loss: 0.0388 | 0.0419
Epoch 158/300, Loss: 0.0388 | 0.0419
Epoch 159/300, Loss: 0.0388 | 0.0419
Epoch 160/300, Loss: 0.0388 | 0.0419
Epoch 161/300, Loss: 0.0388 | 0.0419
Epoch 162/300, Loss: 0.0388 | 0.0419
Epoch 163/300, Loss: 0.0388 | 0.0419
Epoch 164/300, Loss: 0.0388 | 0.0419
Epoch 165/300, Loss: 0.0388 | 0.0419
Epoch 166/300, Loss: 0.0388 | 0.0419
Epoch 167/300, Loss: 0.0388 | 0.0419
Epoch 168/300, Loss: 0.0388 | 0.0419
Epoch 169/300, Loss: 0.0388 | 0.0419
Epoch 170/300, Loss: 0.0388 | 0.0419
Epoch 171/300, Loss: 0.0388 | 0.0419
Epoch 172/300, Loss: 0.0388 | 0.0419
Epoch 173/300, Loss: 0.0388 | 0.0419
Epoch 174/300, Loss: 0.0388 | 0.0419
Epoch 175/300, Loss: 0.0388 | 0.0419
Epoch 176/300, Loss: 0.0388 | 0.0419
Epoch 177/300, Loss: 0.0388 | 0.0419
Epoch 178/300, Loss: 0.0388 | 0.0419
Epoch 179/300, Loss: 0.0388 | 0.0419
Epoch 180/300, Loss: 0.0388 | 0.0419
Epoch 181/300, Loss: 0.0388 | 0.0419
Epoch 182/300, Loss: 0.0388 | 0.0419
Epoch 183/300, Loss: 0.0388 | 0.0419
Epoch 184/300, Loss: 0.0388 | 0.0419
Epoch 185/300, Loss: 0.0388 | 0.0419
Epoch 186/300, Loss: 0.0388 | 0.0419
Epoch 187/300, Loss: 0.0388 | 0.0419
Epoch 188/300, Loss: 0.0388 | 0.0419
Epoch 189/300, Loss: 0.0388 | 0.0419
Epoch 190/300, Loss: 0.0388 | 0.0419
Epoch 191/300, Loss: 0.0388 | 0.0419
Epoch 192/300, Loss: 0.0388 | 0.0419
Epoch 193/300, Loss: 0.0388 | 0.0419
Epoch 194/300, Loss: 0.0388 | 0.0419
Epoch 195/300, Loss: 0.0388 | 0.0419
Epoch 196/300, Loss: 0.0388 | 0.0419
Epoch 197/300, Loss: 0.0388 | 0.0419
Epoch 198/300, Loss: 0.0388 | 0.0419
Epoch 199/300, Loss: 0.0388 | 0.0419
Epoch 200/300, Loss: 0.0388 | 0.0419
Epoch 201/300, Loss: 0.0388 | 0.0419
Epoch 202/300, Loss: 0.0388 | 0.0419
Epoch 203/300, Loss: 0.0388 | 0.0419
Epoch 204/300, Loss: 0.0388 | 0.0419
Epoch 205/300, Loss: 0.0388 | 0.0419
Epoch 206/300, Loss: 0.0388 | 0.0419
Epoch 207/300, Loss: 0.0388 | 0.0419
Epoch 208/300, Loss: 0.0388 | 0.0419
Epoch 209/300, Loss: 0.0388 | 0.0419
Epoch 210/300, Loss: 0.0388 | 0.0419
Epoch 211/300, Loss: 0.0388 | 0.0419
Epoch 212/300, Loss: 0.0388 | 0.0419
Epoch 213/300, Loss: 0.0388 | 0.0419
Epoch 214/300, Loss: 0.0388 | 0.0419
Epoch 215/300, Loss: 0.0388 | 0.0419
Epoch 216/300, Loss: 0.0388 | 0.0419
Epoch 217/300, Loss: 0.0388 | 0.0419
Epoch 218/300, Loss: 0.0388 | 0.0419
Epoch 219/300, Loss: 0.0388 | 0.0419
Epoch 220/300, Loss: 0.0388 | 0.0419
Epoch 221/300, Loss: 0.0388 | 0.0419
Epoch 222/300, Loss: 0.0388 | 0.0419
Epoch 223/300, Loss: 0.0388 | 0.0419
Epoch 224/300, Loss: 0.0388 | 0.0419
Epoch 225/300, Loss: 0.0388 | 0.0419
Epoch 226/300, Loss: 0.0388 | 0.0419
Epoch 227/300, Loss: 0.0388 | 0.0419
Epoch 228/300, Loss: 0.0388 | 0.0419
Epoch 229/300, Loss: 0.0388 | 0.0419
Epoch 230/300, Loss: 0.0388 | 0.0419
Epoch 231/300, Loss: 0.0388 | 0.0419
Epoch 232/300, Loss: 0.0388 | 0.0419
Epoch 233/300, Loss: 0.0388 | 0.0419
Epoch 234/300, Loss: 0.0388 | 0.0419
Epoch 235/300, Loss: 0.0388 | 0.0419
Epoch 236/300, Loss: 0.0388 | 0.0419
Epoch 237/300, Loss: 0.0388 | 0.0419
Epoch 238/300, Loss: 0.0388 | 0.0419
Epoch 239/300, Loss: 0.0388 | 0.0419
Epoch 240/300, Loss: 0.0388 | 0.0419
Epoch 241/300, Loss: 0.0388 | 0.0419
Epoch 242/300, Loss: 0.0388 | 0.0419
Epoch 243/300, Loss: 0.0388 | 0.0419
Epoch 244/300, Loss: 0.0388 | 0.0419
Epoch 245/300, Loss: 0.0388 | 0.0419
Epoch 246/300, Loss: 0.0388 | 0.0419
Epoch 247/300, Loss: 0.0388 | 0.0419
Epoch 248/300, Loss: 0.0388 | 0.0419
Epoch 249/300, Loss: 0.0388 | 0.0419
Epoch 250/300, Loss: 0.0388 | 0.0419
Epoch 251/300, Loss: 0.0388 | 0.0419
Epoch 252/300, Loss: 0.0388 | 0.0419
Epoch 253/300, Loss: 0.0388 | 0.0419
Epoch 254/300, Loss: 0.0388 | 0.0419
Epoch 255/300, Loss: 0.0388 | 0.0419
Epoch 256/300, Loss: 0.0388 | 0.0419
Epoch 257/300, Loss: 0.0388 | 0.0419
Epoch 258/300, Loss: 0.0388 | 0.0419
Epoch 259/300, Loss: 0.0388 | 0.0419
Epoch 260/300, Loss: 0.0388 | 0.0419
Epoch 261/300, Loss: 0.0388 | 0.0419
Epoch 262/300, Loss: 0.0388 | 0.0419
Early stopping
Runtime (seconds): 72.36449432373047
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1.1192320121917874
RMSE: 1.0579376220703125
MAE: 1.0579376220703125
R-squared: nan
[119.01794]
