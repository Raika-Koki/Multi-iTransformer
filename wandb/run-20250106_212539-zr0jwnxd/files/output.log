ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-06 21:25:42,265][0m A new study created in memory with name: no-name-c16ab678-ca3b-4cd0-8251-8e5cfa55f62b[0m
Early stopping at epoch 46
[32m[I 2025-01-06 21:26:14,661][0m Trial 0 finished with value: 2.2490489519003667 and parameters: {'observation_period_num': 79, 'train_rates': 0.7359068094760122, 'learning_rate': 4.974245371832114e-06, 'batch_size': 161, 'step_size': 1, 'gamma': 0.7706743546331707}. Best is trial 0 with value: 2.2490489519003667.[0m
[32m[I 2025-01-06 21:26:55,080][0m Trial 1 finished with value: 0.2903416971612128 and parameters: {'observation_period_num': 246, 'train_rates': 0.6500421471402174, 'learning_rate': 0.00029444428829615414, 'batch_size': 173, 'step_size': 15, 'gamma': 0.8688113526024108}. Best is trial 1 with value: 0.2903416971612128.[0m
[32m[I 2025-01-06 21:28:11,368][0m Trial 2 finished with value: 0.1954953200832198 and parameters: {'observation_period_num': 133, 'train_rates': 0.9441997167480819, 'learning_rate': 8.946562505966852e-06, 'batch_size': 100, 'step_size': 12, 'gamma': 0.8515828982469341}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:29:16,906][0m Trial 3 finished with value: 0.47853123095300465 and parameters: {'observation_period_num': 62, 'train_rates': 0.9223477557527415, 'learning_rate': 2.814506196420368e-06, 'batch_size': 107, 'step_size': 7, 'gamma': 0.7970124084808896}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:30:28,815][0m Trial 4 finished with value: 0.24879420866568883 and parameters: {'observation_period_num': 149, 'train_rates': 0.7593594319966142, 'learning_rate': 6.609628988348904e-05, 'batch_size': 73, 'step_size': 10, 'gamma': 0.8487121978177236}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:31:20,638][0m Trial 5 finished with value: 0.38061305892142805 and parameters: {'observation_period_num': 57, 'train_rates': 0.8225815262776918, 'learning_rate': 6.829599854394316e-06, 'batch_size': 105, 'step_size': 5, 'gamma': 0.8180846941067521}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:31:49,406][0m Trial 6 finished with value: 0.5736900261062348 and parameters: {'observation_period_num': 39, 'train_rates': 0.656192349163292, 'learning_rate': 1.6337058626010576e-06, 'batch_size': 186, 'step_size': 15, 'gamma': 0.872471360719442}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:32:23,678][0m Trial 7 finished with value: 0.36331685980562006 and parameters: {'observation_period_num': 26, 'train_rates': 0.6613326436288556, 'learning_rate': 1.7276510794059084e-05, 'batch_size': 238, 'step_size': 1, 'gamma': 0.9696928733600595}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:32:55,344][0m Trial 8 finished with value: 0.3023331466242464 and parameters: {'observation_period_num': 147, 'train_rates': 0.6162371577639709, 'learning_rate': 0.00012507682710015327, 'batch_size': 191, 'step_size': 9, 'gamma': 0.7966697039319012}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:33:32,207][0m Trial 9 finished with value: 0.8477600177965666 and parameters: {'observation_period_num': 103, 'train_rates': 0.8335592076136534, 'learning_rate': 2.3042708726872443e-06, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8427619061646694}. Best is trial 2 with value: 0.1954953200832198.[0m
[32m[I 2025-01-06 21:37:20,358][0m Trial 10 finished with value: 0.08509742224758321 and parameters: {'observation_period_num': 208, 'train_rates': 0.9802298525273316, 'learning_rate': 0.0008500633221564862, 'batch_size': 24, 'step_size': 12, 'gamma': 0.9235269317020766}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:41:23,710][0m Trial 11 finished with value: 0.10110167063334409 and parameters: {'observation_period_num': 200, 'train_rates': 0.9753868723103128, 'learning_rate': 0.0006122703082854008, 'batch_size': 23, 'step_size': 12, 'gamma': 0.9282931923111047}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:45:08,285][0m Trial 12 finished with value: 0.127072561532259 and parameters: {'observation_period_num': 212, 'train_rates': 0.9885511622656546, 'learning_rate': 0.000852559923918571, 'batch_size': 26, 'step_size': 12, 'gamma': 0.9311473633398264}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:49:05,835][0m Trial 13 finished with value: 0.18194827897436666 and parameters: {'observation_period_num': 197, 'train_rates': 0.8919109830676655, 'learning_rate': 0.0009997776406387576, 'batch_size': 23, 'step_size': 13, 'gamma': 0.9293812957920335}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:50:53,890][0m Trial 14 finished with value: 0.10870305316582803 and parameters: {'observation_period_num': 184, 'train_rates': 0.9776660794663184, 'learning_rate': 0.0002745912239667481, 'batch_size': 53, 'step_size': 11, 'gamma': 0.9007633582936911}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:52:30,821][0m Trial 15 finished with value: 0.13589391362180883 and parameters: {'observation_period_num': 251, 'train_rates': 0.8775785426855438, 'learning_rate': 0.0003751773120913899, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9634870180689391}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:57:43,095][0m Trial 16 finished with value: 0.08945000654011104 and parameters: {'observation_period_num': 175, 'train_rates': 0.9378018594877378, 'learning_rate': 5.197710677050434e-05, 'batch_size': 18, 'step_size': 4, 'gamma': 0.92183952451867}. Best is trial 10 with value: 0.08509742224758321.[0m
[32m[I 2025-01-06 21:58:54,860][0m Trial 17 finished with value: 0.0815425348346648 and parameters: {'observation_period_num': 182, 'train_rates': 0.8671849545473976, 'learning_rate': 3.9857504148827294e-05, 'batch_size': 84, 'step_size': 3, 'gamma': 0.9890371105626466}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 21:59:45,564][0m Trial 18 finished with value: 0.13936182076758052 and parameters: {'observation_period_num': 227, 'train_rates': 0.8583314070242741, 'learning_rate': 2.396186496584863e-05, 'batch_size': 137, 'step_size': 4, 'gamma': 0.9860224916866276}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:00:54,460][0m Trial 19 finished with value: 0.2458038205255228 and parameters: {'observation_period_num': 167, 'train_rates': 0.765520013911007, 'learning_rate': 0.00012762778454617935, 'batch_size': 80, 'step_size': 3, 'gamma': 0.9503652730390395}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:02:52,954][0m Trial 20 finished with value: 0.116852523286396 and parameters: {'observation_period_num': 107, 'train_rates': 0.9060407444399643, 'learning_rate': 0.00011380154645340071, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9889378067087589}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:05:14,915][0m Trial 21 finished with value: 0.08930098870221306 and parameters: {'observation_period_num': 175, 'train_rates': 0.9389277102002319, 'learning_rate': 5.039829792781096e-05, 'batch_size': 42, 'step_size': 3, 'gamma': 0.8985896024587375}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:06:32,299][0m Trial 22 finished with value: 0.1961047647239493 and parameters: {'observation_period_num': 220, 'train_rates': 0.9492815418128492, 'learning_rate': 1.4434838504330154e-05, 'batch_size': 79, 'step_size': 6, 'gamma': 0.8989757308512121}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:08:39,878][0m Trial 23 finished with value: 0.09874145267531276 and parameters: {'observation_period_num': 160, 'train_rates': 0.8626047588254653, 'learning_rate': 4.8276679218339934e-05, 'batch_size': 44, 'step_size': 2, 'gamma': 0.899971645053127}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:09:45,052][0m Trial 24 finished with value: 0.09298288252423792 and parameters: {'observation_period_num': 193, 'train_rates': 0.9138138369695715, 'learning_rate': 0.00019273575577504758, 'batch_size': 91, 'step_size': 3, 'gamma': 0.9474739346154608}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:10:34,239][0m Trial 25 finished with value: 0.20031099436560687 and parameters: {'observation_period_num': 231, 'train_rates': 0.8233350740085039, 'learning_rate': 3.1559043672988234e-05, 'batch_size': 125, 'step_size': 5, 'gamma': 0.8882644196749829}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:12:54,330][0m Trial 26 finished with value: 0.08175496788291664 and parameters: {'observation_period_num': 118, 'train_rates': 0.9496192380020883, 'learning_rate': 8.52787842727233e-05, 'batch_size': 42, 'step_size': 10, 'gamma': 0.9492079467383259}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:14:25,915][0m Trial 27 finished with value: 0.17253155610999282 and parameters: {'observation_period_num': 121, 'train_rates': 0.9654453095466153, 'learning_rate': 0.0005177347413275314, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9682508075974987}. Best is trial 17 with value: 0.0815425348346648.[0m
[32m[I 2025-01-06 22:15:17,209][0m Trial 28 finished with value: 0.0622778454801547 and parameters: {'observation_period_num': 91, 'train_rates': 0.8929400956839078, 'learning_rate': 8.854407951205719e-05, 'batch_size': 124, 'step_size': 9, 'gamma': 0.9452853204799114}. Best is trial 28 with value: 0.0622778454801547.[0m
[32m[I 2025-01-06 22:16:01,046][0m Trial 29 finished with value: 0.1040710685774684 and parameters: {'observation_period_num': 93, 'train_rates': 0.8490603010387356, 'learning_rate': 7.100978220793892e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.9491039881765192}. Best is trial 28 with value: 0.0622778454801547.[0m
[32m[I 2025-01-06 22:16:40,349][0m Trial 30 finished with value: 0.08330185130160517 and parameters: {'observation_period_num': 83, 'train_rates': 0.8002471757818921, 'learning_rate': 9.488481579613943e-05, 'batch_size': 154, 'step_size': 8, 'gamma': 0.9666651753548499}. Best is trial 28 with value: 0.0622778454801547.[0m
[32m[I 2025-01-06 22:17:20,645][0m Trial 31 finished with value: 0.07990311747034119 and parameters: {'observation_period_num': 78, 'train_rates': 0.7969941104742685, 'learning_rate': 0.00010607290217187057, 'batch_size': 151, 'step_size': 8, 'gamma': 0.9773541967581277}. Best is trial 28 with value: 0.0622778454801547.[0m
[32m[I 2025-01-06 22:18:07,765][0m Trial 32 finished with value: 0.10336169497720127 and parameters: {'observation_period_num': 60, 'train_rates': 0.7896652094693316, 'learning_rate': 0.00016854884272042139, 'batch_size': 119, 'step_size': 10, 'gamma': 0.9818543495441142}. Best is trial 28 with value: 0.0622778454801547.[0m
[32m[I 2025-01-06 22:18:42,493][0m Trial 33 finished with value: 0.30360467984356276 and parameters: {'observation_period_num': 125, 'train_rates': 0.7181864010435355, 'learning_rate': 3.3325341156667685e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.9578911082962113}. Best is trial 28 with value: 0.0622778454801547.[0m
[32m[I 2025-01-06 22:19:28,824][0m Trial 34 finished with value: 0.035123544164113146 and parameters: {'observation_period_num': 8, 'train_rates': 0.8864083383935492, 'learning_rate': 0.00020973287871742157, 'batch_size': 141, 'step_size': 7, 'gamma': 0.9395857113254441}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:20:09,714][0m Trial 35 finished with value: 0.1572236337654863 and parameters: {'observation_period_num': 8, 'train_rates': 0.7204268768446204, 'learning_rate': 0.00023385856196248052, 'batch_size': 141, 'step_size': 7, 'gamma': 0.7608402682551373}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:20:59,686][0m Trial 36 finished with value: 0.149100850531441 and parameters: {'observation_period_num': 77, 'train_rates': 0.8840526031812802, 'learning_rate': 0.00040079431079847696, 'batch_size': 116, 'step_size': 6, 'gamma': 0.9756585269800501}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:21:36,555][0m Trial 37 finished with value: 0.05909594918855212 and parameters: {'observation_period_num': 39, 'train_rates': 0.803825564199401, 'learning_rate': 0.00017445175152445955, 'batch_size': 191, 'step_size': 6, 'gamma': 0.9393323010755009}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:22:15,668][0m Trial 38 finished with value: 0.05636285919459368 and parameters: {'observation_period_num': 40, 'train_rates': 0.7947409777258414, 'learning_rate': 0.00017297661066856092, 'batch_size': 194, 'step_size': 6, 'gamma': 0.9118748049570193}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:23:03,212][0m Trial 39 finished with value: 0.1852371484710602 and parameters: {'observation_period_num': 37, 'train_rates': 0.7364816182760799, 'learning_rate': 0.00017283821104514707, 'batch_size': 207, 'step_size': 6, 'gamma': 0.8761702828117262}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:23:40,761][0m Trial 40 finished with value: 0.14438310544417557 and parameters: {'observation_period_num': 14, 'train_rates': 0.6953216904940673, 'learning_rate': 0.00025692617853073396, 'batch_size': 185, 'step_size': 7, 'gamma': 0.9172989985833455}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:24:25,500][0m Trial 41 finished with value: 0.05844511038581058 and parameters: {'observation_period_num': 46, 'train_rates': 0.7974185875908834, 'learning_rate': 0.00015636778759820644, 'batch_size': 214, 'step_size': 5, 'gamma': 0.942726429781294}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:25:07,937][0m Trial 42 finished with value: 0.20716980905146212 and parameters: {'observation_period_num': 42, 'train_rates': 0.7712090148768083, 'learning_rate': 0.00014799652940970817, 'batch_size': 252, 'step_size': 5, 'gamma': 0.9361155687921057}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:25:54,161][0m Trial 43 finished with value: 0.040668815069790906 and parameters: {'observation_period_num': 20, 'train_rates': 0.837494936476296, 'learning_rate': 0.0003652575255620115, 'batch_size': 209, 'step_size': 6, 'gamma': 0.9140386318112047}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:26:46,350][0m Trial 44 finished with value: 0.03753188995935243 and parameters: {'observation_period_num': 22, 'train_rates': 0.8381773494064092, 'learning_rate': 0.00037014247446012376, 'batch_size': 215, 'step_size': 6, 'gamma': 0.9117597552776991}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:27:39,638][0m Trial 45 finished with value: 0.03875858560204506 and parameters: {'observation_period_num': 24, 'train_rates': 0.8411374682067735, 'learning_rate': 0.0005658041012524233, 'batch_size': 218, 'step_size': 4, 'gamma': 0.9152422435429688}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:28:28,105][0m Trial 46 finished with value: 0.03662782316987819 and parameters: {'observation_period_num': 22, 'train_rates': 0.8395402585007112, 'learning_rate': 0.000636161952370647, 'batch_size': 227, 'step_size': 4, 'gamma': 0.9130865055429547}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:29:11,391][0m Trial 47 finished with value: 0.044524990022182465 and parameters: {'observation_period_num': 24, 'train_rates': 0.8417546567449423, 'learning_rate': 0.0006047124979797219, 'batch_size': 232, 'step_size': 4, 'gamma': 0.8602162882296804}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:29:52,921][0m Trial 48 finished with value: 0.0814232336357236 and parameters: {'observation_period_num': 23, 'train_rates': 0.8224386371760565, 'learning_rate': 0.00041318147293631114, 'batch_size': 247, 'step_size': 1, 'gamma': 0.8852841310094578}. Best is trial 34 with value: 0.035123544164113146.[0m
[32m[I 2025-01-06 22:30:35,589][0m Trial 49 finished with value: 0.03331117620050666 and parameters: {'observation_period_num': 5, 'train_rates': 0.8326280637334829, 'learning_rate': 0.0007281037672022916, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9101469761186562}. Best is trial 49 with value: 0.03331117620050666.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-06 22:30:35,600][0m A new study created in memory with name: no-name-4ad53996-ebaa-44d1-ad4a-9721c4921577[0m
[32m[I 2025-01-06 22:31:06,906][0m Trial 0 finished with value: 0.2201694540474393 and parameters: {'observation_period_num': 42, 'train_rates': 0.6256233115443387, 'learning_rate': 0.0005478945842827673, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8471013359740638}. Best is trial 0 with value: 0.2201694540474393.[0m
[32m[I 2025-01-06 22:32:12,357][0m Trial 1 finished with value: 0.10036799545381583 and parameters: {'observation_period_num': 52, 'train_rates': 0.8593447688112368, 'learning_rate': 0.00040814893529427995, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9692017203099546}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:32:53,185][0m Trial 2 finished with value: 0.12087029963731766 and parameters: {'observation_period_num': 159, 'train_rates': 0.9234232350204371, 'learning_rate': 4.9292650342785025e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.9447473344163546}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:33:43,736][0m Trial 3 finished with value: 0.213101995484222 and parameters: {'observation_period_num': 21, 'train_rates': 0.7763546168190121, 'learning_rate': 0.0004849306644588605, 'batch_size': 119, 'step_size': 4, 'gamma': 0.798376996302586}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:34:45,503][0m Trial 4 finished with value: 0.525339051161971 and parameters: {'observation_period_num': 136, 'train_rates': 0.8101328001652869, 'learning_rate': 1.1353452988622539e-06, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9455604832243153}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:35:39,350][0m Trial 5 finished with value: 0.3064286233836345 and parameters: {'observation_period_num': 226, 'train_rates': 0.6736407625066121, 'learning_rate': 0.0004859125497731313, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9296300990920848}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:36:11,680][0m Trial 6 finished with value: 0.6018538917754782 and parameters: {'observation_period_num': 243, 'train_rates': 0.6814692564803423, 'learning_rate': 1.5209428468733646e-05, 'batch_size': 221, 'step_size': 8, 'gamma': 0.8050942780954338}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:36:46,210][0m Trial 7 finished with value: 0.19160131496191024 and parameters: {'observation_period_num': 115, 'train_rates': 0.8239276001190449, 'learning_rate': 3.24309523818725e-05, 'batch_size': 238, 'step_size': 6, 'gamma': 0.8806039007827762}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:37:50,310][0m Trial 8 finished with value: 0.8771788510506463 and parameters: {'observation_period_num': 68, 'train_rates': 0.613271785316744, 'learning_rate': 1.0513897235726706e-06, 'batch_size': 71, 'step_size': 15, 'gamma': 0.780888191558016}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:38:24,502][0m Trial 9 finished with value: 0.336052094947098 and parameters: {'observation_period_num': 217, 'train_rates': 0.7483386790732004, 'learning_rate': 0.000147554146187252, 'batch_size': 170, 'step_size': 3, 'gamma': 0.9227503684020425}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:43:44,944][0m Trial 10 finished with value: 0.13498015269156424 and parameters: {'observation_period_num': 85, 'train_rates': 0.9894194178091674, 'learning_rate': 5.890014067414224e-06, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9852444793846762}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:44:21,233][0m Trial 11 finished with value: 0.12998655523303188 and parameters: {'observation_period_num': 171, 'train_rates': 0.9147727704937645, 'learning_rate': 8.716601715298365e-05, 'batch_size': 169, 'step_size': 9, 'gamma': 0.9831781080093408}. Best is trial 1 with value: 0.10036799545381583.[0m
[32m[I 2025-01-06 22:44:57,515][0m Trial 12 finished with value: 0.09019362516337187 and parameters: {'observation_period_num': 169, 'train_rates': 0.8897763155889078, 'learning_rate': 0.00013197188580558927, 'batch_size': 161, 'step_size': 6, 'gamma': 0.8847501708723569}. Best is trial 12 with value: 0.09019362516337187.[0m
[32m[I 2025-01-06 22:45:36,929][0m Trial 13 finished with value: 0.09437221316948816 and parameters: {'observation_period_num': 186, 'train_rates': 0.8665704422641126, 'learning_rate': 0.0001822716200599521, 'batch_size': 142, 'step_size': 6, 'gamma': 0.8773772595279467}. Best is trial 12 with value: 0.09019362516337187.[0m
[32m[I 2025-01-06 22:46:12,890][0m Trial 14 finished with value: 0.10243334935663582 and parameters: {'observation_period_num': 192, 'train_rates': 0.8883852468759186, 'learning_rate': 0.00016198820345163136, 'batch_size': 159, 'step_size': 7, 'gamma': 0.8737403275597633}. Best is trial 12 with value: 0.09019362516337187.[0m
[32m[I 2025-01-06 22:47:06,604][0m Trial 15 finished with value: 0.1626003384590149 and parameters: {'observation_period_num': 192, 'train_rates': 0.978022481519043, 'learning_rate': 0.0001806777819755236, 'batch_size': 129, 'step_size': 2, 'gamma': 0.8376810942246147}. Best is trial 12 with value: 0.09019362516337187.[0m
[32m[I 2025-01-06 22:47:50,043][0m Trial 16 finished with value: 0.0891382048228899 and parameters: {'observation_period_num': 126, 'train_rates': 0.8528445697491864, 'learning_rate': 0.0009648591695732586, 'batch_size': 193, 'step_size': 6, 'gamma': 0.8933445276530142}. Best is trial 16 with value: 0.0891382048228899.[0m
[32m[I 2025-01-06 22:48:24,560][0m Trial 17 finished with value: 0.4195924474495912 and parameters: {'observation_period_num': 104, 'train_rates': 0.7370153529332508, 'learning_rate': 1.28557098303847e-05, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9065467312740386}. Best is trial 16 with value: 0.0891382048228899.[0m
[32m[I 2025-01-06 22:49:09,606][0m Trial 18 finished with value: 0.09106919914484024 and parameters: {'observation_period_num': 139, 'train_rates': 0.9526568261210547, 'learning_rate': 0.0009895036527759575, 'batch_size': 189, 'step_size': 4, 'gamma': 0.8410724462544137}. Best is trial 16 with value: 0.0891382048228899.[0m
[32m[I 2025-01-06 22:49:49,093][0m Trial 19 finished with value: 0.10831260111631523 and parameters: {'observation_period_num': 158, 'train_rates': 0.8632805900793366, 'learning_rate': 0.0009763907773325279, 'batch_size': 253, 'step_size': 8, 'gamma': 0.9022631236675748}. Best is trial 16 with value: 0.0891382048228899.[0m
[32m[I 2025-01-06 22:50:24,809][0m Trial 20 finished with value: 0.1587708408541314 and parameters: {'observation_period_num': 90, 'train_rates': 0.8313120050408931, 'learning_rate': 6.806497321249221e-05, 'batch_size': 208, 'step_size': 6, 'gamma': 0.7511060261004388}. Best is trial 16 with value: 0.0891382048228899.[0m
[32m[I 2025-01-06 22:51:01,706][0m Trial 21 finished with value: 0.07458017766475677 and parameters: {'observation_period_num': 135, 'train_rates': 0.9354032072003334, 'learning_rate': 0.0009955408690930604, 'batch_size': 189, 'step_size': 4, 'gamma': 0.8441156459687962}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:51:38,684][0m Trial 22 finished with value: 0.08916035213985958 and parameters: {'observation_period_num': 125, 'train_rates': 0.9216444561195318, 'learning_rate': 0.00029443375519511703, 'batch_size': 192, 'step_size': 4, 'gamma': 0.8594767582904245}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:52:15,169][0m Trial 23 finished with value: 0.11463421583175659 and parameters: {'observation_period_num': 129, 'train_rates': 0.9483390893492499, 'learning_rate': 0.00027482044295684715, 'batch_size': 198, 'step_size': 3, 'gamma': 0.8542056016195936}. Best is trial 21 with value: 0.07458017766475677.[0m
Early stopping at epoch 68
[32m[I 2025-01-06 22:52:41,702][0m Trial 24 finished with value: 0.14872310508129208 and parameters: {'observation_period_num': 116, 'train_rates': 0.9241683914396996, 'learning_rate': 0.0007818098346697821, 'batch_size': 184, 'step_size': 1, 'gamma': 0.8196054304957698}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:53:15,042][0m Trial 25 finished with value: 0.102314293384552 and parameters: {'observation_period_num': 142, 'train_rates': 0.9566866802807303, 'learning_rate': 0.0003113507933586334, 'batch_size': 246, 'step_size': 4, 'gamma': 0.8591357153267658}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:53:58,590][0m Trial 26 finished with value: 0.07465234469946984 and parameters: {'observation_period_num': 102, 'train_rates': 0.9154491953978252, 'learning_rate': 0.0006084076192281721, 'batch_size': 142, 'step_size': 3, 'gamma': 0.8979445332552868}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:54:46,233][0m Trial 27 finished with value: 0.07666390207001353 and parameters: {'observation_period_num': 92, 'train_rates': 0.8878579489509663, 'learning_rate': 0.000619764912630068, 'batch_size': 139, 'step_size': 2, 'gamma': 0.8958190087012152}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:55:38,398][0m Trial 28 finished with value: 0.09138439014448342 and parameters: {'observation_period_num': 88, 'train_rates': 0.9015962346576336, 'learning_rate': 0.0006104376172019043, 'batch_size': 118, 'step_size': 2, 'gamma': 0.9158564377866109}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:56:25,667][0m Trial 29 finished with value: 0.2185667098558852 and parameters: {'observation_period_num': 7, 'train_rates': 0.7803790407007636, 'learning_rate': 0.0005792993518959253, 'batch_size': 142, 'step_size': 2, 'gamma': 0.8334257786878114}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:57:25,203][0m Trial 30 finished with value: 0.6166776297456127 and parameters: {'observation_period_num': 58, 'train_rates': 0.9592611712909884, 'learning_rate': 2.372863367748761e-06, 'batch_size': 104, 'step_size': 3, 'gamma': 0.9444291814568735}. Best is trial 21 with value: 0.07458017766475677.[0m
[32m[I 2025-01-06 22:58:03,893][0m Trial 31 finished with value: 0.06918785593508323 and parameters: {'observation_period_num': 73, 'train_rates': 0.8468273834307609, 'learning_rate': 0.0007167891703490721, 'batch_size': 157, 'step_size': 5, 'gamma': 0.8965276052464594}. Best is trial 31 with value: 0.06918785593508323.[0m
[32m[I 2025-01-06 22:58:45,784][0m Trial 32 finished with value: 0.07530772525891875 and parameters: {'observation_period_num': 42, 'train_rates': 0.8803269810647576, 'learning_rate': 0.00028595107660720723, 'batch_size': 152, 'step_size': 5, 'gamma': 0.899663082756677}. Best is trial 31 with value: 0.06918785593508323.[0m
[32m[I 2025-01-06 22:59:25,155][0m Trial 33 finished with value: 0.07433095500710306 and parameters: {'observation_period_num': 41, 'train_rates': 0.8471183130365179, 'learning_rate': 0.0003351147657212237, 'batch_size': 153, 'step_size': 5, 'gamma': 0.8652029667463199}. Best is trial 31 with value: 0.06918785593508323.[0m
[32m[I 2025-01-06 22:59:59,139][0m Trial 34 finished with value: 0.07914425267473511 and parameters: {'observation_period_num': 72, 'train_rates': 0.8404091405890474, 'learning_rate': 0.0003993583505901843, 'batch_size': 174, 'step_size': 5, 'gamma': 0.8658353941534748}. Best is trial 31 with value: 0.06918785593508323.[0m
[32m[I 2025-01-06 23:00:43,913][0m Trial 35 finished with value: 0.07658989838411893 and parameters: {'observation_period_num': 34, 'train_rates': 0.7883701891227124, 'learning_rate': 0.00044592066735352133, 'batch_size': 122, 'step_size': 7, 'gamma': 0.8262425569625124}. Best is trial 31 with value: 0.06918785593508323.[0m
[32m[I 2025-01-06 23:01:42,118][0m Trial 36 finished with value: 0.06897022827927556 and parameters: {'observation_period_num': 23, 'train_rates': 0.9332378087819829, 'learning_rate': 0.00010039950608726082, 'batch_size': 105, 'step_size': 5, 'gamma': 0.848709797151817}. Best is trial 36 with value: 0.06897022827927556.[0m
[32m[I 2025-01-06 23:02:34,796][0m Trial 37 finished with value: 0.23466678689048215 and parameters: {'observation_period_num': 25, 'train_rates': 0.7550105483520024, 'learning_rate': 5.191604109899631e-05, 'batch_size': 101, 'step_size': 5, 'gamma': 0.811920207641905}. Best is trial 36 with value: 0.06897022827927556.[0m
[32m[I 2025-01-06 23:03:21,385][0m Trial 38 finished with value: 0.08933295147964754 and parameters: {'observation_period_num': 10, 'train_rates': 0.8024898649394853, 'learning_rate': 0.0001026187958777834, 'batch_size': 210, 'step_size': 7, 'gamma': 0.8528418808084406}. Best is trial 36 with value: 0.06897022827927556.[0m
[32m[I 2025-01-06 23:04:51,194][0m Trial 39 finished with value: 0.21263073937490928 and parameters: {'observation_period_num': 49, 'train_rates': 0.7038519394803487, 'learning_rate': 0.000261209923525441, 'batch_size': 58, 'step_size': 5, 'gamma': 0.7902260027473503}. Best is trial 36 with value: 0.06897022827927556.[0m
[32m[I 2025-01-06 23:05:47,265][0m Trial 40 finished with value: 0.10138153795971463 and parameters: {'observation_period_num': 63, 'train_rates': 0.8458097966869216, 'learning_rate': 2.5702232353485602e-05, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8441856011116412}. Best is trial 36 with value: 0.06897022827927556.[0m
[32m[I 2025-01-06 23:06:33,300][0m Trial 41 finished with value: 0.05035464290428806 and parameters: {'observation_period_num': 28, 'train_rates': 0.9369126201947702, 'learning_rate': 0.0006942538881869897, 'batch_size': 147, 'step_size': 3, 'gamma': 0.8860994092421566}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:07:20,809][0m Trial 42 finished with value: 0.057105224378205637 and parameters: {'observation_period_num': 26, 'train_rates': 0.9367960736856623, 'learning_rate': 0.00036462057937386796, 'batch_size': 155, 'step_size': 4, 'gamma': 0.8718582777992806}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:08:04,534][0m Trial 43 finished with value: 0.07272943601769916 and parameters: {'observation_period_num': 20, 'train_rates': 0.8188690161370094, 'learning_rate': 0.00040016149737808534, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8861284104199706}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:09:25,941][0m Trial 44 finished with value: 0.06657956025906299 and parameters: {'observation_period_num': 20, 'train_rates': 0.8185876266909541, 'learning_rate': 0.00019043768001852658, 'batch_size': 67, 'step_size': 3, 'gamma': 0.9343351245388253}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:11:33,799][0m Trial 45 finished with value: 0.06478556245565414 and parameters: {'observation_period_num': 29, 'train_rates': 0.9778688832135051, 'learning_rate': 0.00010632340525139362, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9409884144068958}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:13:14,935][0m Trial 46 finished with value: 0.0820198330667711 and parameters: {'observation_period_num': 27, 'train_rates': 0.9789858535466254, 'learning_rate': 3.905219824231182e-05, 'batch_size': 60, 'step_size': 1, 'gamma': 0.9500109055501981}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:18:03,998][0m Trial 47 finished with value: 0.06130014580633977 and parameters: {'observation_period_num': 5, 'train_rates': 0.9687750385768586, 'learning_rate': 0.0001094500477506021, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9564889711253591}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:21:31,667][0m Trial 48 finished with value: 0.1760755115483562 and parameters: {'observation_period_num': 12, 'train_rates': 0.6468007376280677, 'learning_rate': 0.0002019047633142917, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9657816618420095}. Best is trial 41 with value: 0.05035464290428806.[0m
[32m[I 2025-01-06 23:24:57,287][0m Trial 49 finished with value: 0.054306027131626404 and parameters: {'observation_period_num': 32, 'train_rates': 0.9718933346854326, 'learning_rate': 6.508621037798082e-05, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9612726554131503}. Best is trial 41 with value: 0.05035464290428806.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-06 23:24:57,298][0m A new study created in memory with name: no-name-36c5f634-2627-4088-865e-527da069cf96[0m
[32m[I 2025-01-06 23:26:08,822][0m Trial 0 finished with value: 0.18254631517671469 and parameters: {'observation_period_num': 16, 'train_rates': 0.7039330835018263, 'learning_rate': 0.00011989710122845397, 'batch_size': 70, 'step_size': 3, 'gamma': 0.8132683581162949}. Best is trial 0 with value: 0.18254631517671469.[0m
[32m[I 2025-01-06 23:27:12,704][0m Trial 1 finished with value: 0.18530246379453427 and parameters: {'observation_period_num': 51, 'train_rates': 0.7998745422330651, 'learning_rate': 4.3962939269353776e-06, 'batch_size': 89, 'step_size': 4, 'gamma': 0.9803963761462685}. Best is trial 0 with value: 0.18254631517671469.[0m
[32m[I 2025-01-06 23:28:23,115][0m Trial 2 finished with value: 0.09899527923418926 and parameters: {'observation_period_num': 227, 'train_rates': 0.8714818960063899, 'learning_rate': 0.00011271558485298272, 'batch_size': 87, 'step_size': 9, 'gamma': 0.7549692894095471}. Best is trial 2 with value: 0.09899527923418926.[0m
[32m[I 2025-01-06 23:33:09,852][0m Trial 3 finished with value: 0.08874255208290212 and parameters: {'observation_period_num': 140, 'train_rates': 0.8415274836997628, 'learning_rate': 0.00010947649898082567, 'batch_size': 18, 'step_size': 4, 'gamma': 0.902304765075992}. Best is trial 3 with value: 0.08874255208290212.[0m
[32m[I 2025-01-06 23:33:51,966][0m Trial 4 finished with value: 0.3771326046014569 and parameters: {'observation_period_num': 36, 'train_rates': 0.6675397206013411, 'learning_rate': 3.4233191745981944e-06, 'batch_size': 116, 'step_size': 11, 'gamma': 0.9833887651696802}. Best is trial 3 with value: 0.08874255208290212.[0m
[32m[I 2025-01-06 23:34:38,875][0m Trial 5 finished with value: 0.272345446699809 and parameters: {'observation_period_num': 176, 'train_rates': 0.7358132084855122, 'learning_rate': 0.0003728799345533389, 'batch_size': 106, 'step_size': 4, 'gamma': 0.7749172316289922}. Best is trial 3 with value: 0.08874255208290212.[0m
[32m[I 2025-01-06 23:35:21,572][0m Trial 6 finished with value: 0.5504810810089111 and parameters: {'observation_period_num': 182, 'train_rates': 0.9825737740523006, 'learning_rate': 1.674615228938533e-06, 'batch_size': 178, 'step_size': 8, 'gamma': 0.8302878078015663}. Best is trial 3 with value: 0.08874255208290212.[0m
[32m[I 2025-01-06 23:36:06,183][0m Trial 7 finished with value: 0.07702870666980743 and parameters: {'observation_period_num': 19, 'train_rates': 0.9568031404154882, 'learning_rate': 0.00018027783403017192, 'batch_size': 184, 'step_size': 1, 'gamma': 0.9297621870494458}. Best is trial 7 with value: 0.07702870666980743.[0m
[32m[I 2025-01-06 23:36:49,732][0m Trial 8 finished with value: 0.06500150263309479 and parameters: {'observation_period_num': 67, 'train_rates': 0.9509588152058761, 'learning_rate': 0.00029744130061672776, 'batch_size': 179, 'step_size': 5, 'gamma': 0.9049989195501584}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:37:30,747][0m Trial 9 finished with value: 0.09490332351559758 and parameters: {'observation_period_num': 106, 'train_rates': 0.7833334851411167, 'learning_rate': 0.00018488627795954933, 'batch_size': 180, 'step_size': 6, 'gamma': 0.9291783560792001}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:38:03,482][0m Trial 10 finished with value: 0.08033993825998925 and parameters: {'observation_period_num': 85, 'train_rates': 0.9088204524187253, 'learning_rate': 0.0007876974278247347, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8654512820711163}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:38:42,142][0m Trial 11 finished with value: 0.21604156494140625 and parameters: {'observation_period_num': 71, 'train_rates': 0.989330609213086, 'learning_rate': 2.1119323238356653e-05, 'batch_size': 187, 'step_size': 2, 'gamma': 0.928911319443399}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:39:20,006][0m Trial 12 finished with value: 0.19708599150180817 and parameters: {'observation_period_num': 8, 'train_rates': 0.9276392412848782, 'learning_rate': 2.829888180569996e-05, 'batch_size': 234, 'step_size': 1, 'gamma': 0.9016967599520753}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:39:57,396][0m Trial 13 finished with value: 0.18981420580106925 and parameters: {'observation_period_num': 56, 'train_rates': 0.6158538153140024, 'learning_rate': 0.0009982261516795431, 'batch_size': 153, 'step_size': 6, 'gamma': 0.9434879606767692}. Best is trial 8 with value: 0.06500150263309479.[0m
Early stopping at epoch 90
[32m[I 2025-01-06 23:40:38,879][0m Trial 14 finished with value: 0.17328882217407227 and parameters: {'observation_period_num': 121, 'train_rates': 0.9320474836128979, 'learning_rate': 0.0003278753600882415, 'batch_size': 214, 'step_size': 1, 'gamma': 0.8710931144462895}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:41:36,510][0m Trial 15 finished with value: 0.07481266879211597 and parameters: {'observation_period_num': 87, 'train_rates': 0.8713377544522211, 'learning_rate': 5.0191404957336776e-05, 'batch_size': 138, 'step_size': 6, 'gamma': 0.9561864317430763}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:42:27,063][0m Trial 16 finished with value: 0.08414513964767326 and parameters: {'observation_period_num': 97, 'train_rates': 0.8723366165004225, 'learning_rate': 5.251708514075907e-05, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9631041715371753}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:44:18,082][0m Trial 17 finished with value: 0.13888676384571294 and parameters: {'observation_period_num': 147, 'train_rates': 0.8206320040682905, 'learning_rate': 1.0926380365284457e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.8917996110476374}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:45:16,583][0m Trial 18 finished with value: 0.07670331814903424 and parameters: {'observation_period_num': 76, 'train_rates': 0.886818103332248, 'learning_rate': 5.692465926750426e-05, 'batch_size': 156, 'step_size': 12, 'gamma': 0.9568821791315689}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:45:57,616][0m Trial 19 finished with value: 0.40230691197107166 and parameters: {'observation_period_num': 251, 'train_rates': 0.8457973233818584, 'learning_rate': 1.1401599950134609e-05, 'batch_size': 213, 'step_size': 6, 'gamma': 0.8397800738122885}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:46:56,529][0m Trial 20 finished with value: 0.2344994194774185 and parameters: {'observation_period_num': 120, 'train_rates': 0.7488612973264891, 'learning_rate': 0.0004740925159841341, 'batch_size': 114, 'step_size': 8, 'gamma': 0.8744466398011069}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:47:56,725][0m Trial 21 finished with value: 0.07505801066078922 and parameters: {'observation_period_num': 74, 'train_rates': 0.8930034196442153, 'learning_rate': 5.020025729788758e-05, 'batch_size': 159, 'step_size': 13, 'gamma': 0.9575225331082245}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:49:03,243][0m Trial 22 finished with value: 0.10018552991579163 and parameters: {'observation_period_num': 50, 'train_rates': 0.9087085360859782, 'learning_rate': 1.4897562557400438e-05, 'batch_size': 157, 'step_size': 14, 'gamma': 0.9590617012529127}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:50:01,418][0m Trial 23 finished with value: 0.12192099541425705 and parameters: {'observation_period_num': 97, 'train_rates': 0.9581765763031439, 'learning_rate': 4.867427775225618e-05, 'batch_size': 205, 'step_size': 5, 'gamma': 0.9115715453083038}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:51:01,775][0m Trial 24 finished with value: 0.06870341657495206 and parameters: {'observation_period_num': 71, 'train_rates': 0.8869259001077769, 'learning_rate': 6.244345055036985e-05, 'batch_size': 147, 'step_size': 12, 'gamma': 0.9885497278005577}. Best is trial 8 with value: 0.06500150263309479.[0m
[32m[I 2025-01-06 23:52:09,686][0m Trial 25 finished with value: 0.04694367960934907 and parameters: {'observation_period_num': 37, 'train_rates': 0.8539147488607692, 'learning_rate': 0.00020384056158900292, 'batch_size': 130, 'step_size': 15, 'gamma': 0.9894283985037735}. Best is trial 25 with value: 0.04694367960934907.[0m
[32m[I 2025-01-06 23:53:15,128][0m Trial 26 finished with value: 0.050469857419200834 and parameters: {'observation_period_num': 28, 'train_rates': 0.8403259456240094, 'learning_rate': 0.00023213944531883576, 'batch_size': 120, 'step_size': 15, 'gamma': 0.9898881969107555}. Best is trial 25 with value: 0.04694367960934907.[0m
[32m[I 2025-01-06 23:54:17,749][0m Trial 27 finished with value: 0.06832590621153774 and parameters: {'observation_period_num': 26, 'train_rates': 0.7911993654150539, 'learning_rate': 0.0005683807541172196, 'batch_size': 120, 'step_size': 15, 'gamma': 0.9899411010138723}. Best is trial 25 with value: 0.04694367960934907.[0m
[32m[I 2025-01-06 23:55:17,474][0m Trial 28 finished with value: 0.047964482828974725 and parameters: {'observation_period_num': 39, 'train_rates': 0.8284142853122295, 'learning_rate': 0.000245971084108224, 'batch_size': 126, 'step_size': 15, 'gamma': 0.8009825356447229}. Best is trial 25 with value: 0.04694367960934907.[0m
[32m[I 2025-01-06 23:56:53,792][0m Trial 29 finished with value: 0.1649568663582226 and parameters: {'observation_period_num': 8, 'train_rates': 0.7511511923762431, 'learning_rate': 0.0001843225724585459, 'batch_size': 58, 'step_size': 15, 'gamma': 0.8098006104465734}. Best is trial 25 with value: 0.04694367960934907.[0m
[32m[I 2025-01-06 23:58:13,309][0m Trial 30 finished with value: 0.046524322613109376 and parameters: {'observation_period_num': 37, 'train_rates': 0.835660739620735, 'learning_rate': 0.00019927951288651574, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8011064195712944}. Best is trial 30 with value: 0.046524322613109376.[0m
[32m[I 2025-01-06 23:59:31,419][0m Trial 31 finished with value: 0.053348449946835984 and parameters: {'observation_period_num': 36, 'train_rates': 0.8377663082625084, 'learning_rate': 0.00022991493923306362, 'batch_size': 92, 'step_size': 14, 'gamma': 0.7849147803835047}. Best is trial 30 with value: 0.046524322613109376.[0m
[32m[I 2025-01-07 00:00:57,222][0m Trial 32 finished with value: 0.058259236908900676 and parameters: {'observation_period_num': 42, 'train_rates': 0.8088507240047689, 'learning_rate': 0.00010946482302812777, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8045918068712572}. Best is trial 30 with value: 0.046524322613109376.[0m
[32m[I 2025-01-07 00:01:53,733][0m Trial 33 finished with value: 0.19533173757622396 and parameters: {'observation_period_num': 25, 'train_rates': 0.7774054431698855, 'learning_rate': 9.838206759058754e-05, 'batch_size': 129, 'step_size': 13, 'gamma': 0.7895641852510458}. Best is trial 30 with value: 0.046524322613109376.[0m
[32m[I 2025-01-07 00:03:01,484][0m Trial 34 finished with value: 0.060805176858042105 and parameters: {'observation_period_num': 53, 'train_rates': 0.8527591995404457, 'learning_rate': 0.0006067647649594775, 'batch_size': 98, 'step_size': 13, 'gamma': 0.8292268501647345}. Best is trial 30 with value: 0.046524322613109376.[0m
[32m[I 2025-01-07 00:04:20,076][0m Trial 35 finished with value: 0.045807220209426375 and parameters: {'observation_period_num': 31, 'train_rates': 0.8250106402607481, 'learning_rate': 0.00014318285676327512, 'batch_size': 67, 'step_size': 15, 'gamma': 0.756591723765685}. Best is trial 35 with value: 0.045807220209426375.[0m
[32m[I 2025-01-07 00:07:24,176][0m Trial 36 finished with value: 0.042097726341757215 and parameters: {'observation_period_num': 5, 'train_rates': 0.8224864048842686, 'learning_rate': 8.499385829759511e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7602102483124719}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:10:37,836][0m Trial 37 finished with value: 0.14115664219159796 and parameters: {'observation_period_num': 5, 'train_rates': 0.6877326962959516, 'learning_rate': 8.935125544165666e-05, 'batch_size': 26, 'step_size': 10, 'gamma': 0.7551648312746144}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:13:08,203][0m Trial 38 finished with value: 0.042486021779079876 and parameters: {'observation_period_num': 18, 'train_rates': 0.8087066564136012, 'learning_rate': 7.964215926061919e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7602475137154234}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:15:34,355][0m Trial 39 finished with value: 0.17110822956368282 and parameters: {'observation_period_num': 17, 'train_rates': 0.7664993185074648, 'learning_rate': 0.00013383607005978525, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7502370428131466}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:16:39,324][0m Trial 40 finished with value: 0.3174167467707836 and parameters: {'observation_period_num': 184, 'train_rates': 0.7115766835482299, 'learning_rate': 3.379433234972385e-05, 'batch_size': 77, 'step_size': 11, 'gamma': 0.7694372712316367}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:18:34,147][0m Trial 41 finished with value: 0.06341054633999547 and parameters: {'observation_period_num': 60, 'train_rates': 0.8050172151143843, 'learning_rate': 0.00013298307866942559, 'batch_size': 50, 'step_size': 12, 'gamma': 0.7638434814761152}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:24:21,812][0m Trial 42 finished with value: 0.06465687219191481 and parameters: {'observation_period_num': 42, 'train_rates': 0.8611193034211553, 'learning_rate': 0.00015707847797452113, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7781043233201309}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:27:06,272][0m Trial 43 finished with value: 0.04267766932025552 and parameters: {'observation_period_num': 18, 'train_rates': 0.8145940272795363, 'learning_rate': 8.477155232234744e-05, 'batch_size': 36, 'step_size': 10, 'gamma': 0.7934512897556004}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:29:59,568][0m Trial 44 finished with value: 0.0427261004041154 and parameters: {'observation_period_num': 16, 'train_rates': 0.8097926973216948, 'learning_rate': 8.367282565155915e-05, 'batch_size': 33, 'step_size': 9, 'gamma': 0.7941343996831385}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:32:55,820][0m Trial 45 finished with value: 0.04969877456681112 and parameters: {'observation_period_num': 14, 'train_rates': 0.8182991990726758, 'learning_rate': 3.3683424821107514e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.7617957244350717}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:35:06,481][0m Trial 46 finished with value: 0.0490865695476532 and parameters: {'observation_period_num': 20, 'train_rates': 0.7954692108957331, 'learning_rate': 8.382630138294401e-05, 'batch_size': 42, 'step_size': 9, 'gamma': 0.786996510113955}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:36:39,330][0m Trial 47 finished with value: 0.17589858753864865 and parameters: {'observation_period_num': 26, 'train_rates': 0.7240781448576644, 'learning_rate': 7.576257270305638e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8210920922637431}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:39:43,159][0m Trial 48 finished with value: 0.1969854063628351 and parameters: {'observation_period_num': 12, 'train_rates': 0.768814595695303, 'learning_rate': 2.188768410186709e-05, 'batch_size': 30, 'step_size': 8, 'gamma': 0.7740642254480894}. Best is trial 36 with value: 0.042097726341757215.[0m
[32m[I 2025-01-07 00:41:22,775][0m Trial 49 finished with value: 0.39450137975724153 and parameters: {'observation_period_num': 5, 'train_rates': 0.8043637036004079, 'learning_rate': 1.3393518852447128e-06, 'batch_size': 65, 'step_size': 10, 'gamma': 0.7898619282513937}. Best is trial 36 with value: 0.042097726341757215.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 00:41:22,785][0m A new study created in memory with name: no-name-16a8741f-032f-4055-83b0-073d69da7e0d[0m
[32m[I 2025-01-07 00:44:56,018][0m Trial 0 finished with value: 0.12001389897555756 and parameters: {'observation_period_num': 143, 'train_rates': 0.9505662066213041, 'learning_rate': 4.393272230392424e-06, 'batch_size': 28, 'step_size': 2, 'gamma': 0.9852239459067302}. Best is trial 0 with value: 0.12001389897555756.[0m
[32m[I 2025-01-07 00:46:12,077][0m Trial 1 finished with value: 0.6525471857856662 and parameters: {'observation_period_num': 82, 'train_rates': 0.7288449474487207, 'learning_rate': 2.550783951756081e-06, 'batch_size': 97, 'step_size': 14, 'gamma': 0.7634291226717635}. Best is trial 0 with value: 0.12001389897555756.[0m
[32m[I 2025-01-07 00:46:59,406][0m Trial 2 finished with value: 0.08035249263048172 and parameters: {'observation_period_num': 182, 'train_rates': 0.9383567072729028, 'learning_rate': 0.0005324983637981501, 'batch_size': 192, 'step_size': 6, 'gamma': 0.7648357797514762}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:48:18,575][0m Trial 3 finished with value: 0.17762893237385088 and parameters: {'observation_period_num': 18, 'train_rates': 0.6759002992824296, 'learning_rate': 0.00013089487768539333, 'batch_size': 71, 'step_size': 2, 'gamma': 0.785163572890132}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:49:10,859][0m Trial 4 finished with value: 0.7618442538978906 and parameters: {'observation_period_num': 136, 'train_rates': 0.8109063608197176, 'learning_rate': 1.748078483372416e-06, 'batch_size': 193, 'step_size': 5, 'gamma': 0.7957474826876051}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:51:12,100][0m Trial 5 finished with value: 0.16894766400935743 and parameters: {'observation_period_num': 170, 'train_rates': 0.8788253145793787, 'learning_rate': 3.5986742389090534e-05, 'batch_size': 49, 'step_size': 2, 'gamma': 0.842255196611114}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:51:57,503][0m Trial 6 finished with value: 0.1615098494459206 and parameters: {'observation_period_num': 109, 'train_rates': 0.8002647875621773, 'learning_rate': 2.017578486139253e-05, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8195303215225433}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:52:54,111][0m Trial 7 finished with value: 0.26990644292187116 and parameters: {'observation_period_num': 139, 'train_rates': 0.6618705462114136, 'learning_rate': 4.3933552281840893e-05, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9321600791337417}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:54:35,618][0m Trial 8 finished with value: 0.2206655546653369 and parameters: {'observation_period_num': 139, 'train_rates': 0.7850111135409723, 'learning_rate': 8.139828660115007e-06, 'batch_size': 54, 'step_size': 14, 'gamma': 0.7838431728045394}. Best is trial 2 with value: 0.08035249263048172.[0m
Early stopping at epoch 69
[32m[I 2025-01-07 00:55:11,155][0m Trial 9 finished with value: 1.3168463662818626 and parameters: {'observation_period_num': 113, 'train_rates': 0.7717271039571367, 'learning_rate': 9.43704423460341e-06, 'batch_size': 192, 'step_size': 1, 'gamma': 0.8375976782898886}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:55:58,020][0m Trial 10 finished with value: 0.09715497493743896 and parameters: {'observation_period_num': 250, 'train_rates': 0.9757586731126526, 'learning_rate': 0.000988836667886023, 'batch_size': 256, 'step_size': 10, 'gamma': 0.8894517765882524}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:56:36,719][0m Trial 11 finished with value: 0.10469368100166321 and parameters: {'observation_period_num': 247, 'train_rates': 0.9888033471603679, 'learning_rate': 0.0009766286971198542, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8976530687935727}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:57:19,857][0m Trial 12 finished with value: 0.10537966808607412 and parameters: {'observation_period_num': 240, 'train_rates': 0.9050803822577954, 'learning_rate': 0.0008504053015645261, 'batch_size': 252, 'step_size': 8, 'gamma': 0.8853262969975443}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:58:00,810][0m Trial 13 finished with value: 0.08046400591684065 and parameters: {'observation_period_num': 201, 'train_rates': 0.884754774239618, 'learning_rate': 0.0002644907810474806, 'batch_size': 206, 'step_size': 11, 'gamma': 0.9270380949759482}. Best is trial 2 with value: 0.08035249263048172.[0m
[32m[I 2025-01-07 00:58:55,614][0m Trial 14 finished with value: 0.079684033747611 and parameters: {'observation_period_num': 202, 'train_rates': 0.8657720558337538, 'learning_rate': 0.00018058819125870142, 'batch_size': 201, 'step_size': 11, 'gamma': 0.9488630299447444}. Best is trial 14 with value: 0.079684033747611.[0m
[32m[I 2025-01-07 00:59:57,242][0m Trial 15 finished with value: 0.12193551879349372 and parameters: {'observation_period_num': 193, 'train_rates': 0.8489641831228291, 'learning_rate': 0.00021680068968948753, 'batch_size': 147, 'step_size': 5, 'gamma': 0.9855774899100416}. Best is trial 14 with value: 0.079684033747611.[0m
[32m[I 2025-01-07 01:00:37,117][0m Trial 16 finished with value: 0.10825231671333313 and parameters: {'observation_period_num': 207, 'train_rates': 0.9273448764599573, 'learning_rate': 8.610808260467842e-05, 'batch_size': 219, 'step_size': 6, 'gamma': 0.9482001590258807}. Best is trial 14 with value: 0.079684033747611.[0m
[32m[I 2025-01-07 01:01:18,480][0m Trial 17 finished with value: 0.06639739555262383 and parameters: {'observation_period_num': 173, 'train_rates': 0.848866823172051, 'learning_rate': 0.0003806628438404408, 'batch_size': 166, 'step_size': 11, 'gamma': 0.7528954563567163}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:01:52,819][0m Trial 18 finished with value: 0.34212938108881097 and parameters: {'observation_period_num': 217, 'train_rates': 0.6028879648273172, 'learning_rate': 0.000353776287629324, 'batch_size': 163, 'step_size': 12, 'gamma': 0.8581867273401343}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:02:34,384][0m Trial 19 finished with value: 0.08630619997825098 and parameters: {'observation_period_num': 167, 'train_rates': 0.843506939223511, 'learning_rate': 9.789773247942193e-05, 'batch_size': 168, 'step_size': 9, 'gamma': 0.9666131140218698}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:03:22,877][0m Trial 20 finished with value: 0.1906027077022896 and parameters: {'observation_period_num': 65, 'train_rates': 0.7365197206591406, 'learning_rate': 0.00016107148244372548, 'batch_size': 116, 'step_size': 12, 'gamma': 0.9216114329777553}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:03:57,484][0m Trial 21 finished with value: 0.08190409821413812 and parameters: {'observation_period_num': 178, 'train_rates': 0.8488287111034476, 'learning_rate': 0.00043601978241326574, 'batch_size': 226, 'step_size': 7, 'gamma': 0.7508226017724063}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:04:39,336][0m Trial 22 finished with value: 0.0996399689379925 and parameters: {'observation_period_num': 223, 'train_rates': 0.9205260059349671, 'learning_rate': 0.000487837089665572, 'batch_size': 177, 'step_size': 15, 'gamma': 0.814002376914506}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:05:45,551][0m Trial 23 finished with value: 0.14394655002623188 and parameters: {'observation_period_num': 161, 'train_rates': 0.9474520517351234, 'learning_rate': 7.088068008118555e-05, 'batch_size': 124, 'step_size': 4, 'gamma': 0.7708083866318367}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:06:30,175][0m Trial 24 finished with value: 0.07930148500291741 and parameters: {'observation_period_num': 182, 'train_rates': 0.8772249798980282, 'learning_rate': 0.0004316033284544492, 'batch_size': 227, 'step_size': 10, 'gamma': 0.7518010551549197}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:07:16,465][0m Trial 25 finished with value: 0.13967127735034013 and parameters: {'observation_period_num': 229, 'train_rates': 0.8283199812884979, 'learning_rate': 0.00018892933949951634, 'batch_size': 228, 'step_size': 11, 'gamma': 0.8040015965199154}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:08:17,205][0m Trial 26 finished with value: 0.07219129533941902 and parameters: {'observation_period_num': 156, 'train_rates': 0.8751410134842994, 'learning_rate': 0.0003040981623178305, 'batch_size': 235, 'step_size': 10, 'gamma': 0.7521107879078465}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:09:07,582][0m Trial 27 finished with value: 0.07630118131637573 and parameters: {'observation_period_num': 112, 'train_rates': 0.8943631608547371, 'learning_rate': 0.0003483702294699917, 'batch_size': 234, 'step_size': 9, 'gamma': 0.7517202963186945}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:10:00,786][0m Trial 28 finished with value: 0.10020493154584521 and parameters: {'observation_period_num': 82, 'train_rates': 0.9011300070632281, 'learning_rate': 6.403182584932817e-05, 'batch_size': 243, 'step_size': 9, 'gamma': 0.7767068031384207}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:10:49,176][0m Trial 29 finished with value: 0.22848784923553467 and parameters: {'observation_period_num': 153, 'train_rates': 0.9597403689038562, 'learning_rate': 2.419106356879651e-05, 'batch_size': 211, 'step_size': 8, 'gamma': 0.8273049233186368}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:15:10,469][0m Trial 30 finished with value: 0.09512594337761403 and parameters: {'observation_period_num': 118, 'train_rates': 0.8236214330873425, 'learning_rate': 0.0002868330063079202, 'batch_size': 21, 'step_size': 12, 'gamma': 0.7972191783894329}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:15:54,902][0m Trial 31 finished with value: 0.09242073083322361 and parameters: {'observation_period_num': 153, 'train_rates': 0.8720263659086565, 'learning_rate': 0.000599043446043203, 'batch_size': 235, 'step_size': 10, 'gamma': 0.7511344785860101}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:16:45,859][0m Trial 32 finished with value: 0.0688973108980818 and parameters: {'observation_period_num': 93, 'train_rates': 0.9024110773262215, 'learning_rate': 0.00032518183578326255, 'batch_size': 238, 'step_size': 9, 'gamma': 0.7573162520255832}. Best is trial 17 with value: 0.06639739555262383.[0m
[32m[I 2025-01-07 01:17:40,934][0m Trial 33 finished with value: 0.05793983478818023 and parameters: {'observation_period_num': 96, 'train_rates': 0.9004835514806997, 'learning_rate': 0.0006407967559132165, 'batch_size': 239, 'step_size': 9, 'gamma': 0.766238249678606}. Best is trial 33 with value: 0.05793983478818023.[0m
[32m[I 2025-01-07 01:18:42,948][0m Trial 34 finished with value: 0.05868054529236368 and parameters: {'observation_period_num': 92, 'train_rates': 0.915562429455176, 'learning_rate': 0.000673285944054729, 'batch_size': 173, 'step_size': 7, 'gamma': 0.767965823874223}. Best is trial 33 with value: 0.05793983478818023.[0m
[32m[I 2025-01-07 01:19:38,365][0m Trial 35 finished with value: 0.05370018191250234 and parameters: {'observation_period_num': 86, 'train_rates': 0.9193545793801511, 'learning_rate': 0.0007439777259205712, 'batch_size': 179, 'step_size': 7, 'gamma': 0.7675499311798473}. Best is trial 35 with value: 0.05370018191250234.[0m
[32m[I 2025-01-07 01:20:33,541][0m Trial 36 finished with value: 0.047604648616503584 and parameters: {'observation_period_num': 54, 'train_rates': 0.9240699288547147, 'learning_rate': 0.0006347535561328186, 'batch_size': 181, 'step_size': 7, 'gamma': 0.7718502793344598}. Best is trial 36 with value: 0.047604648616503584.[0m
[32m[I 2025-01-07 01:21:37,023][0m Trial 37 finished with value: 0.058517370373010635 and parameters: {'observation_period_num': 46, 'train_rates': 0.9618001716028827, 'learning_rate': 0.0006274447659870943, 'batch_size': 182, 'step_size': 7, 'gamma': 0.7869293704001934}. Best is trial 36 with value: 0.047604648616503584.[0m
[32m[I 2025-01-07 01:22:34,784][0m Trial 38 finished with value: 0.06327971816062927 and parameters: {'observation_period_num': 42, 'train_rates': 0.9605919626478873, 'learning_rate': 0.0006471354586387282, 'batch_size': 183, 'step_size': 4, 'gamma': 0.7889119491171841}. Best is trial 36 with value: 0.047604648616503584.[0m
[32m[I 2025-01-07 01:23:37,088][0m Trial 39 finished with value: 0.0356167321652174 and parameters: {'observation_period_num': 6, 'train_rates': 0.932230794590996, 'learning_rate': 0.0006722580963491471, 'batch_size': 154, 'step_size': 6, 'gamma': 0.7809302679538981}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:24:31,535][0m Trial 40 finished with value: 2.3509586960522095 and parameters: {'observation_period_num': 11, 'train_rates': 0.9319531367288812, 'learning_rate': 1.3702397360735474e-06, 'batch_size': 153, 'step_size': 6, 'gamma': 0.8075781683817063}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:25:30,489][0m Trial 41 finished with value: 0.0464693211313141 and parameters: {'observation_period_num': 34, 'train_rates': 0.9479209300482682, 'learning_rate': 0.0006663841271966327, 'batch_size': 122, 'step_size': 7, 'gamma': 0.7778144827531156}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:26:38,387][0m Trial 42 finished with value: 0.3411041145665305 and parameters: {'observation_period_num': 29, 'train_rates': 0.9403195899401879, 'learning_rate': 3.6523373125598553e-06, 'batch_size': 94, 'step_size': 5, 'gamma': 0.7755107291519541}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:27:38,797][0m Trial 43 finished with value: 0.06357387453317642 and parameters: {'observation_period_num': 61, 'train_rates': 0.9691919005579771, 'learning_rate': 0.000794867306858339, 'batch_size': 133, 'step_size': 6, 'gamma': 0.7692321223737945}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:28:41,099][0m Trial 44 finished with value: 0.0464886836707592 and parameters: {'observation_period_num': 28, 'train_rates': 0.983763473744329, 'learning_rate': 0.0001254687864410735, 'batch_size': 111, 'step_size': 8, 'gamma': 0.7929369806148787}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:29:44,549][0m Trial 45 finished with value: 0.060071513056755066 and parameters: {'observation_period_num': 25, 'train_rates': 0.9748546939736108, 'learning_rate': 0.00012786599270641128, 'batch_size': 105, 'step_size': 4, 'gamma': 0.8259684228641806}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:31:08,820][0m Trial 46 finished with value: 0.036909159272909164 and parameters: {'observation_period_num': 7, 'train_rates': 0.988420645458129, 'learning_rate': 0.0009987186913389893, 'batch_size': 75, 'step_size': 7, 'gamma': 0.7982465888119836}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:32:42,801][0m Trial 47 finished with value: 0.12351220846176147 and parameters: {'observation_period_num': 6, 'train_rates': 0.9888951481762291, 'learning_rate': 1.4008374630420585e-05, 'batch_size': 75, 'step_size': 3, 'gamma': 0.8485402845204303}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:34:00,933][0m Trial 48 finished with value: 0.04726376412170274 and parameters: {'observation_period_num': 37, 'train_rates': 0.9400471106804578, 'learning_rate': 0.0002466451233047231, 'batch_size': 78, 'step_size': 8, 'gamma': 0.7944287356496763}. Best is trial 39 with value: 0.0356167321652174.[0m
[32m[I 2025-01-07 01:36:25,858][0m Trial 49 finished with value: 0.029139215126633644 and parameters: {'observation_period_num': 32, 'train_rates': 0.9899741120700878, 'learning_rate': 0.00024081927604822638, 'batch_size': 44, 'step_size': 8, 'gamma': 0.7985774186825085}. Best is trial 49 with value: 0.029139215126633644.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 01:36:25,868][0m A new study created in memory with name: no-name-9a1ca6d4-5095-49d4-90d4-2d33ebbef1b2[0m
[32m[I 2025-01-07 01:37:15,443][0m Trial 0 finished with value: 0.22783705223196993 and parameters: {'observation_period_num': 163, 'train_rates': 0.6633855473959602, 'learning_rate': 0.00014393969215608396, 'batch_size': 165, 'step_size': 5, 'gamma': 0.9064678456355563}. Best is trial 0 with value: 0.22783705223196993.[0m
[32m[I 2025-01-07 01:38:01,614][0m Trial 1 finished with value: 0.08446626281065325 and parameters: {'observation_period_num': 91, 'train_rates': 0.8917739042760133, 'learning_rate': 0.0001301556704940273, 'batch_size': 255, 'step_size': 11, 'gamma': 0.9086121691667459}. Best is trial 1 with value: 0.08446626281065325.[0m
Early stopping at epoch 97
[32m[I 2025-01-07 01:39:06,320][0m Trial 2 finished with value: 0.25244951248168945 and parameters: {'observation_period_num': 112, 'train_rates': 0.975111784040745, 'learning_rate': 8.148497748969946e-05, 'batch_size': 155, 'step_size': 1, 'gamma': 0.8901320801051787}. Best is trial 1 with value: 0.08446626281065325.[0m
[32m[I 2025-01-07 01:40:54,002][0m Trial 3 finished with value: 0.05108123729305883 and parameters: {'observation_period_num': 42, 'train_rates': 0.8828456164063386, 'learning_rate': 0.00015496209063507323, 'batch_size': 55, 'step_size': 6, 'gamma': 0.7943956587566852}. Best is trial 3 with value: 0.05108123729305883.[0m
[32m[I 2025-01-07 01:41:33,299][0m Trial 4 finished with value: 0.4206339788760718 and parameters: {'observation_period_num': 231, 'train_rates': 0.7835238511536274, 'learning_rate': 1.1577332805013607e-05, 'batch_size': 181, 'step_size': 4, 'gamma': 0.808551895255659}. Best is trial 3 with value: 0.05108123729305883.[0m
Early stopping at epoch 48
[32m[I 2025-01-07 01:42:33,266][0m Trial 5 finished with value: 0.12319120950996876 and parameters: {'observation_period_num': 70, 'train_rates': 0.9265757055478304, 'learning_rate': 0.00013042518705287093, 'batch_size': 53, 'step_size': 1, 'gamma': 0.773586108007385}. Best is trial 3 with value: 0.05108123729305883.[0m
[32m[I 2025-01-07 01:43:05,270][0m Trial 6 finished with value: 0.4177399558629914 and parameters: {'observation_period_num': 214, 'train_rates': 0.630581673283021, 'learning_rate': 2.348105203464032e-05, 'batch_size': 226, 'step_size': 13, 'gamma': 0.8948847679939184}. Best is trial 3 with value: 0.05108123729305883.[0m
[32m[I 2025-01-07 01:44:02,099][0m Trial 7 finished with value: 0.30351236975934515 and parameters: {'observation_period_num': 84, 'train_rates': 0.7813077514993789, 'learning_rate': 1.299289835366404e-05, 'batch_size': 117, 'step_size': 15, 'gamma': 0.820091663384226}. Best is trial 3 with value: 0.05108123729305883.[0m
[32m[I 2025-01-07 01:46:42,089][0m Trial 8 finished with value: 0.2805490097892818 and parameters: {'observation_period_num': 183, 'train_rates': 0.8066990351436356, 'learning_rate': 6.87749573864591e-06, 'batch_size': 32, 'step_size': 3, 'gamma': 0.8534971411568014}. Best is trial 3 with value: 0.05108123729305883.[0m
[32m[I 2025-01-07 01:47:30,407][0m Trial 9 finished with value: 0.11277172184748456 and parameters: {'observation_period_num': 219, 'train_rates': 0.9370382212789707, 'learning_rate': 4.548486929044612e-05, 'batch_size': 162, 'step_size': 14, 'gamma': 0.7875619932742425}. Best is trial 3 with value: 0.05108123729305883.[0m
[32m[I 2025-01-07 01:48:40,317][0m Trial 10 finished with value: 0.04202250208429716 and parameters: {'observation_period_num': 14, 'train_rates': 0.8531813562947581, 'learning_rate': 0.0008849583126667316, 'batch_size': 86, 'step_size': 8, 'gamma': 0.9801899970300908}. Best is trial 10 with value: 0.04202250208429716.[0m
[32m[I 2025-01-07 01:49:45,196][0m Trial 11 finished with value: 0.039895925228483974 and parameters: {'observation_period_num': 32, 'train_rates': 0.8355991991978969, 'learning_rate': 0.0009696819696238707, 'batch_size': 93, 'step_size': 8, 'gamma': 0.9553306252157537}. Best is trial 11 with value: 0.039895925228483974.[0m
[32m[I 2025-01-07 01:50:41,970][0m Trial 12 finished with value: 0.029970918066407504 and parameters: {'observation_period_num': 8, 'train_rates': 0.8386128882833932, 'learning_rate': 0.0009092349278140076, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9715146842616529}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 01:51:34,232][0m Trial 13 finished with value: 0.15759494626771348 and parameters: {'observation_period_num': 14, 'train_rates': 0.7275452671000835, 'learning_rate': 0.0008448731864949765, 'batch_size': 103, 'step_size': 9, 'gamma': 0.9832720499845904}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 01:52:40,835][0m Trial 14 finished with value: 0.30097813443424776 and parameters: {'observation_period_num': 45, 'train_rates': 0.8333287761677929, 'learning_rate': 1.0762874191549926e-06, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9396979143331784}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 01:53:29,553][0m Trial 15 finished with value: 0.1506852824167878 and parameters: {'observation_period_num': 8, 'train_rates': 0.7403010708075435, 'learning_rate': 0.0005036319381129591, 'batch_size': 132, 'step_size': 11, 'gamma': 0.9532786734473304}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 01:57:24,842][0m Trial 16 finished with value: 0.25747056606442037 and parameters: {'observation_period_num': 137, 'train_rates': 0.7188941675162366, 'learning_rate': 0.0003331901474925339, 'batch_size': 20, 'step_size': 10, 'gamma': 0.9424415543021805}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 01:58:36,362][0m Trial 17 finished with value: 0.038812758667128425 and parameters: {'observation_period_num': 37, 'train_rates': 0.8657259820589333, 'learning_rate': 0.0003532749886402431, 'batch_size': 79, 'step_size': 7, 'gamma': 0.852171896446289}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:00:13,009][0m Trial 18 finished with value: 0.05095531418919563 and parameters: {'observation_period_num': 59, 'train_rates': 0.9819375910794452, 'learning_rate': 0.0003150251633530778, 'batch_size': 63, 'step_size': 6, 'gamma': 0.8464646921402821}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:00:47,286][0m Trial 19 finished with value: 0.5361839452357154 and parameters: {'observation_period_num': 109, 'train_rates': 0.8908231302460148, 'learning_rate': 2.585508029171186e-06, 'batch_size': 200, 'step_size': 12, 'gamma': 0.8677048968234786}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:01:34,897][0m Trial 20 finished with value: 0.0449767276513419 and parameters: {'observation_period_num': 37, 'train_rates': 0.9223669036184922, 'learning_rate': 0.0004058457153519585, 'batch_size': 130, 'step_size': 7, 'gamma': 0.753195047166055}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:02:37,544][0m Trial 21 finished with value: 0.05735258432936722 and parameters: {'observation_period_num': 34, 'train_rates': 0.8466734591576314, 'learning_rate': 0.0009465980445701958, 'batch_size': 93, 'step_size': 9, 'gamma': 0.9673833972497619}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:03:30,331][0m Trial 22 finished with value: 0.06305648683054933 and parameters: {'observation_period_num': 65, 'train_rates': 0.8216864844219168, 'learning_rate': 0.0005844455354340606, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9254435759054118}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:04:51,418][0m Trial 23 finished with value: 0.1690329913006619 and parameters: {'observation_period_num': 22, 'train_rates': 0.765191804939588, 'learning_rate': 0.000267075569660898, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8282490797956443}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:06:09,460][0m Trial 24 finished with value: 0.03757691303882455 and parameters: {'observation_period_num': 5, 'train_rates': 0.8704257866215379, 'learning_rate': 0.0002586022108398495, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9572915852995001}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:08:29,501][0m Trial 25 finished with value: 0.03473969962109219 and parameters: {'observation_period_num': 5, 'train_rates': 0.8695532871665067, 'learning_rate': 5.583482430074373e-05, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8799958463155567}. Best is trial 12 with value: 0.029970918066407504.[0m
[32m[I 2025-01-07 02:11:36,493][0m Trial 26 finished with value: 0.029777014223129852 and parameters: {'observation_period_num': 5, 'train_rates': 0.9065450259796723, 'learning_rate': 6.363505740036789e-05, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9897238042553754}. Best is trial 26 with value: 0.029777014223129852.[0m
[32m[I 2025-01-07 02:14:28,214][0m Trial 27 finished with value: 0.0566342793405056 and parameters: {'observation_period_num': 59, 'train_rates': 0.9413811325652055, 'learning_rate': 4.4281977863371824e-05, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9889109078546832}. Best is trial 26 with value: 0.029777014223129852.[0m
[32m[I 2025-01-07 02:18:53,522][0m Trial 28 finished with value: 0.1174226195788851 and parameters: {'observation_period_num': 143, 'train_rates': 0.9094439663750383, 'learning_rate': 6.997977907156123e-05, 'batch_size': 20, 'step_size': 4, 'gamma': 0.9251797255769116}. Best is trial 26 with value: 0.029777014223129852.[0m
[32m[I 2025-01-07 02:20:58,784][0m Trial 29 finished with value: 0.08494063240030537 and parameters: {'observation_period_num': 162, 'train_rates': 0.9589445039322592, 'learning_rate': 3.113574000456482e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.9704184223640776}. Best is trial 26 with value: 0.029777014223129852.[0m
[32m[I 2025-01-07 02:23:04,967][0m Trial 30 finished with value: 0.12478914031235544 and parameters: {'observation_period_num': 79, 'train_rates': 0.800567176400561, 'learning_rate': 1.6704267436591415e-05, 'batch_size': 41, 'step_size': 2, 'gamma': 0.9166476084234172}. Best is trial 26 with value: 0.029777014223129852.[0m
[32m[I 2025-01-07 02:24:27,148][0m Trial 31 finished with value: 0.02809550576642448 and parameters: {'observation_period_num': 7, 'train_rates': 0.8788296426853188, 'learning_rate': 0.0001731791157046036, 'batch_size': 70, 'step_size': 6, 'gamma': 0.9603340208072857}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:30:09,978][0m Trial 32 finished with value: 0.033788766570157246 and parameters: {'observation_period_num': 20, 'train_rates': 0.9014376174978768, 'learning_rate': 8.52893250246281e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9398900870439695}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:34:59,630][0m Trial 33 finished with value: 0.041648644867793946 and parameters: {'observation_period_num': 19, 'train_rates': 0.9042866601953441, 'learning_rate': 0.00018033070513248225, 'batch_size': 19, 'step_size': 4, 'gamma': 0.9383645880663521}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:36:38,638][0m Trial 34 finished with value: 0.06041640038048471 and parameters: {'observation_period_num': 48, 'train_rates': 0.9510645971930148, 'learning_rate': 9.299936041116842e-05, 'batch_size': 58, 'step_size': 5, 'gamma': 0.971221038060351}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:38:06,809][0m Trial 35 finished with value: 0.03874986432492733 and parameters: {'observation_period_num': 29, 'train_rates': 0.895952172884496, 'learning_rate': 0.00010688603725453568, 'batch_size': 65, 'step_size': 6, 'gamma': 0.9018718716449565}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:39:36,897][0m Trial 36 finished with value: 0.2037335641168315 and parameters: {'observation_period_num': 102, 'train_rates': 0.6948332004292284, 'learning_rate': 0.0001767138692691936, 'batch_size': 51, 'step_size': 6, 'gamma': 0.9632327576489587}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:40:17,755][0m Trial 37 finished with value: 0.08759386738141378 and parameters: {'observation_period_num': 54, 'train_rates': 0.9172578300653172, 'learning_rate': 9.035575443116905e-05, 'batch_size': 148, 'step_size': 10, 'gamma': 0.9455737641047195}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:43:40,805][0m Trial 38 finished with value: 0.04585967496618887 and parameters: {'observation_period_num': 22, 'train_rates': 0.881932048358222, 'learning_rate': 2.4892553123801757e-05, 'batch_size': 27, 'step_size': 3, 'gamma': 0.9755793228118219}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:44:16,597][0m Trial 39 finished with value: 0.279658677748666 and parameters: {'observation_period_num': 246, 'train_rates': 0.6012720913416366, 'learning_rate': 0.00020079396297250572, 'batch_size': 122, 'step_size': 2, 'gamma': 0.9892954396953281}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:44:58,366][0m Trial 40 finished with value: 0.24177005887031555 and parameters: {'observation_period_num': 71, 'train_rates': 0.9592796172790842, 'learning_rate': 8.437705061421e-06, 'batch_size': 147, 'step_size': 4, 'gamma': 0.9289316026988048}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:47:19,438][0m Trial 41 finished with value: 0.033829893437214195 and parameters: {'observation_period_num': 8, 'train_rates': 0.8642534385535263, 'learning_rate': 5.9678938528989185e-05, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8822561944979325}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:48:06,956][0m Trial 42 finished with value: 0.0669138322236614 and parameters: {'observation_period_num': 27, 'train_rates': 0.8177743208650977, 'learning_rate': 6.229728324230003e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9092135817050536}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:50:02,887][0m Trial 43 finished with value: 0.031946584604556065 and parameters: {'observation_period_num': 15, 'train_rates': 0.8573202204782031, 'learning_rate': 0.0001325861618400628, 'batch_size': 52, 'step_size': 6, 'gamma': 0.8881298091303759}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:51:54,465][0m Trial 44 finished with value: 0.04744906582137836 and parameters: {'observation_period_num': 45, 'train_rates': 0.8459467374180671, 'learning_rate': 0.00013036290423220517, 'batch_size': 50, 'step_size': 6, 'gamma': 0.9516428511370008}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:52:51,652][0m Trial 45 finished with value: 0.05907573749399966 and parameters: {'observation_period_num': 22, 'train_rates': 0.8853516777625163, 'learning_rate': 3.709226910857025e-05, 'batch_size': 101, 'step_size': 4, 'gamma': 0.9754697465161184}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:56:04,668][0m Trial 46 finished with value: 0.17275601663593068 and parameters: {'observation_period_num': 16, 'train_rates': 0.7738593299856091, 'learning_rate': 0.00011918569914401752, 'batch_size': 26, 'step_size': 7, 'gamma': 0.9637140439031591}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:57:30,142][0m Trial 47 finished with value: 0.046591547616187806 and parameters: {'observation_period_num': 41, 'train_rates': 0.9293819711752505, 'learning_rate': 0.0005572884296770538, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9173620800063769}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 02:59:02,907][0m Trial 48 finished with value: 0.08547723680824582 and parameters: {'observation_period_num': 96, 'train_rates': 0.826009959566683, 'learning_rate': 0.0002170113991207083, 'batch_size': 57, 'step_size': 8, 'gamma': 0.9369009065056284}. Best is trial 31 with value: 0.02809550576642448.[0m
[32m[I 2025-01-07 03:00:15,170][0m Trial 49 finished with value: 0.08767104302755832 and parameters: {'observation_period_num': 185, 'train_rates': 0.8566960535568509, 'learning_rate': 0.00014726172923923693, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9811301139925305}. Best is trial 31 with value: 0.02809550576642448.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 03:00:15,180][0m A new study created in memory with name: no-name-d75023c7-702d-497e-aa2e-ddb85979306b[0m
[32m[I 2025-01-07 03:01:26,602][0m Trial 0 finished with value: 0.22471863156037628 and parameters: {'observation_period_num': 51, 'train_rates': 0.7291353417904499, 'learning_rate': 0.00013523465789917546, 'batch_size': 71, 'step_size': 2, 'gamma': 0.8133841157428237}. Best is trial 0 with value: 0.22471863156037628.[0m
[32m[I 2025-01-07 03:02:15,685][0m Trial 1 finished with value: 0.18052656127935662 and parameters: {'observation_period_num': 60, 'train_rates': 0.7103130753526606, 'learning_rate': 0.00016679839943857183, 'batch_size': 104, 'step_size': 4, 'gamma': 0.9230243701709211}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:02:54,925][0m Trial 2 finished with value: 0.958334743976593 and parameters: {'observation_period_num': 66, 'train_rates': 0.9425786896023636, 'learning_rate': 1.1508024387162367e-06, 'batch_size': 202, 'step_size': 7, 'gamma': 0.7747026312678579}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:03:48,122][0m Trial 3 finished with value: 0.42538800835609436 and parameters: {'observation_period_num': 65, 'train_rates': 0.9726723587067487, 'learning_rate': 8.57607589065686e-06, 'batch_size': 145, 'step_size': 5, 'gamma': 0.7981039787010645}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:04:28,896][0m Trial 4 finished with value: 0.5579494937852899 and parameters: {'observation_period_num': 91, 'train_rates': 0.779410992482798, 'learning_rate': 6.822117567213657e-06, 'batch_size': 210, 'step_size': 4, 'gamma': 0.7864133795984162}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:05:17,664][0m Trial 5 finished with value: 0.3157192845031085 and parameters: {'observation_period_num': 218, 'train_rates': 0.7247838447274791, 'learning_rate': 0.00017070082804901453, 'batch_size': 123, 'step_size': 5, 'gamma': 0.7725748665914635}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:05:57,830][0m Trial 6 finished with value: 0.23126415224288296 and parameters: {'observation_period_num': 30, 'train_rates': 0.7243363629169359, 'learning_rate': 3.638622244139614e-05, 'batch_size': 165, 'step_size': 6, 'gamma': 0.8011948199106143}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:06:49,801][0m Trial 7 finished with value: 1.1592730001302867 and parameters: {'observation_period_num': 86, 'train_rates': 0.6599008068789466, 'learning_rate': 1.000444507138033e-06, 'batch_size': 108, 'step_size': 2, 'gamma': 0.8762198666084201}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:07:35,606][0m Trial 8 finished with value: 0.25682035118826035 and parameters: {'observation_period_num': 142, 'train_rates': 0.7373770675456119, 'learning_rate': 0.0002460470224030445, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9395769289377139}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:09:14,657][0m Trial 9 finished with value: 0.21447875478770584 and parameters: {'observation_period_num': 111, 'train_rates': 0.8651177845442308, 'learning_rate': 4.457513939045862e-06, 'batch_size': 57, 'step_size': 13, 'gamma': 0.7653052272303896}. Best is trial 1 with value: 0.18052656127935662.[0m
[32m[I 2025-01-07 03:13:36,504][0m Trial 10 finished with value: 0.12735759470807878 and parameters: {'observation_period_num': 6, 'train_rates': 0.6130923696975337, 'learning_rate': 0.0005292151911003638, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9891503364855664}. Best is trial 10 with value: 0.12735759470807878.[0m
[32m[I 2025-01-07 03:15:33,925][0m Trial 11 finished with value: 0.15157601619099342 and parameters: {'observation_period_num': 9, 'train_rates': 0.6042683241961065, 'learning_rate': 0.0008222352464008867, 'batch_size': 38, 'step_size': 11, 'gamma': 0.986520178494017}. Best is trial 10 with value: 0.12735759470807878.[0m
[32m[I 2025-01-07 03:18:55,494][0m Trial 12 finished with value: 0.13157921583402282 and parameters: {'observation_period_num': 10, 'train_rates': 0.6102491448405261, 'learning_rate': 0.0009059700457294053, 'batch_size': 22, 'step_size': 11, 'gamma': 0.9826714041309484}. Best is trial 10 with value: 0.12735759470807878.[0m
[32m[I 2025-01-07 03:23:16,272][0m Trial 13 finished with value: 0.13670165345881335 and parameters: {'observation_period_num': 7, 'train_rates': 0.6057828192051733, 'learning_rate': 0.0005243591393909305, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9786641995285514}. Best is trial 10 with value: 0.12735759470807878.[0m
[32m[I 2025-01-07 03:27:43,803][0m Trial 14 finished with value: 0.2639002615663121 and parameters: {'observation_period_num': 160, 'train_rates': 0.6631005205626957, 'learning_rate': 0.0009951541422345798, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9426049171558624}. Best is trial 10 with value: 0.12735759470807878.[0m
[32m[I 2025-01-07 03:28:59,240][0m Trial 15 finished with value: 0.08988506938653772 and parameters: {'observation_period_num': 189, 'train_rates': 0.8383472345802495, 'learning_rate': 5.775021380369068e-05, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8904816709187904}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:30:10,147][0m Trial 16 finished with value: 0.11178439702973308 and parameters: {'observation_period_num': 215, 'train_rates': 0.8486023494281828, 'learning_rate': 3.911260009714872e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8531237895190138}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:31:25,016][0m Trial 17 finished with value: 0.11842773897981267 and parameters: {'observation_period_num': 248, 'train_rates': 0.8600994887216096, 'learning_rate': 3.469716585725453e-05, 'batch_size': 81, 'step_size': 8, 'gamma': 0.8476134590750157}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:32:06,684][0m Trial 18 finished with value: 0.11148340736279648 and parameters: {'observation_period_num': 189, 'train_rates': 0.8489494263458361, 'learning_rate': 6.704890224086745e-05, 'batch_size': 254, 'step_size': 9, 'gamma': 0.8807989406384473}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:32:51,301][0m Trial 19 finished with value: 0.11094199679791927 and parameters: {'observation_period_num': 175, 'train_rates': 0.9005971702238802, 'learning_rate': 7.572775957133878e-05, 'batch_size': 247, 'step_size': 13, 'gamma': 0.8986723104024819}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:33:35,900][0m Trial 20 finished with value: 0.17102538959847557 and parameters: {'observation_period_num': 175, 'train_rates': 0.9092969202407097, 'learning_rate': 1.574588317124667e-05, 'batch_size': 250, 'step_size': 14, 'gamma': 0.90301831494803}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:34:15,637][0m Trial 21 finished with value: 0.14281181853020053 and parameters: {'observation_period_num': 188, 'train_rates': 0.8126951071201186, 'learning_rate': 8.291348665754682e-05, 'batch_size': 253, 'step_size': 12, 'gamma': 0.8853048482761761}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:34:59,401][0m Trial 22 finished with value: 0.2525649588758295 and parameters: {'observation_period_num': 208, 'train_rates': 0.9000659524863178, 'learning_rate': 1.839948328035346e-05, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8327552955709785}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:35:44,131][0m Trial 23 finished with value: 0.09854663283594194 and parameters: {'observation_period_num': 142, 'train_rates': 0.8346676253834461, 'learning_rate': 8.113549401489722e-05, 'batch_size': 190, 'step_size': 13, 'gamma': 0.8931623196354096}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:36:31,288][0m Trial 24 finished with value: 0.11746078967221688 and parameters: {'observation_period_num': 141, 'train_rates': 0.7946974212869975, 'learning_rate': 7.487550221601734e-05, 'batch_size': 177, 'step_size': 15, 'gamma': 0.9064890588388943}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:37:25,409][0m Trial 25 finished with value: 0.14812118916829944 and parameters: {'observation_period_num': 126, 'train_rates': 0.8913718813727894, 'learning_rate': 1.596328916764102e-05, 'batch_size': 228, 'step_size': 13, 'gamma': 0.9021523179263494}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:38:11,230][0m Trial 26 finished with value: 0.11208070438406642 and parameters: {'observation_period_num': 162, 'train_rates': 0.8185009662202011, 'learning_rate': 0.00030712102728112776, 'batch_size': 188, 'step_size': 13, 'gamma': 0.8603361352651413}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:39:05,271][0m Trial 27 finished with value: 0.14202779531478882 and parameters: {'observation_period_num': 249, 'train_rates': 0.9353311554256845, 'learning_rate': 5.635834157713085e-05, 'batch_size': 232, 'step_size': 12, 'gamma': 0.9320937470223724}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:39:59,860][0m Trial 28 finished with value: 0.10593482106924057 and parameters: {'observation_period_num': 197, 'train_rates': 0.9793303520603278, 'learning_rate': 0.00012580872697501245, 'batch_size': 166, 'step_size': 14, 'gamma': 0.9634968090018201}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:40:52,905][0m Trial 29 finished with value: 0.384657274885678 and parameters: {'observation_period_num': 231, 'train_rates': 0.7619304464585454, 'learning_rate': 0.00030235536457400513, 'batch_size': 164, 'step_size': 15, 'gamma': 0.9488665228254201}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:41:54,138][0m Trial 30 finished with value: 0.10709577798843384 and parameters: {'observation_period_num': 115, 'train_rates': 0.9722466249525532, 'learning_rate': 0.00010813454148655646, 'batch_size': 129, 'step_size': 1, 'gamma': 0.9629276245189909}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:42:51,439][0m Trial 31 finished with value: 0.09383002668619156 and parameters: {'observation_period_num': 106, 'train_rates': 0.9746570917608104, 'learning_rate': 0.00011876580626971143, 'batch_size': 129, 'step_size': 1, 'gamma': 0.9630196100792964}. Best is trial 15 with value: 0.08988506938653772.[0m
[32m[I 2025-01-07 03:43:58,055][0m Trial 32 finished with value: 0.0672906585808458 and parameters: {'observation_period_num': 96, 'train_rates': 0.9392037069541843, 'learning_rate': 0.00020722647178007662, 'batch_size': 104, 'step_size': 3, 'gamma': 0.9207946534262519}. Best is trial 32 with value: 0.0672906585808458.[0m
[32m[I 2025-01-07 03:45:09,331][0m Trial 33 finished with value: 0.06459668405599646 and parameters: {'observation_period_num': 85, 'train_rates': 0.9366669809046088, 'learning_rate': 0.0001939533513083016, 'batch_size': 94, 'step_size': 2, 'gamma': 0.9196401385198039}. Best is trial 33 with value: 0.06459668405599646.[0m
[32m[I 2025-01-07 03:46:21,432][0m Trial 34 finished with value: 0.07174078048862649 and parameters: {'observation_period_num': 91, 'train_rates': 0.9411155811591769, 'learning_rate': 0.00018973108454142928, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9202756086335804}. Best is trial 33 with value: 0.06459668405599646.[0m
[32m[I 2025-01-07 03:47:35,058][0m Trial 35 finished with value: 0.07562455674332957 and parameters: {'observation_period_num': 80, 'train_rates': 0.946135082905317, 'learning_rate': 0.00020110031677159938, 'batch_size': 94, 'step_size': 3, 'gamma': 0.9203404568649837}. Best is trial 33 with value: 0.06459668405599646.[0m
[32m[I 2025-01-07 03:48:46,977][0m Trial 36 finished with value: 0.0672908340804061 and parameters: {'observation_period_num': 77, 'train_rates': 0.9408429541454705, 'learning_rate': 0.0001897446086487198, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9190903839079853}. Best is trial 33 with value: 0.06459668405599646.[0m
[32m[I 2025-01-07 03:49:59,460][0m Trial 37 finished with value: 0.05034862458705902 and parameters: {'observation_period_num': 48, 'train_rates': 0.9311120335432673, 'learning_rate': 0.00044355189222919654, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9195288446163793}. Best is trial 37 with value: 0.05034862458705902.[0m
[32m[I 2025-01-07 03:50:58,788][0m Trial 38 finished with value: 0.04440736003658351 and parameters: {'observation_period_num': 48, 'train_rates': 0.9300349240656278, 'learning_rate': 0.00043038800647595294, 'batch_size': 114, 'step_size': 4, 'gamma': 0.9215850187345475}. Best is trial 38 with value: 0.04440736003658351.[0m
[32m[I 2025-01-07 03:52:04,330][0m Trial 39 finished with value: 0.045875562035905725 and parameters: {'observation_period_num': 46, 'train_rates': 0.8756899005922021, 'learning_rate': 0.0004442131860778325, 'batch_size': 111, 'step_size': 4, 'gamma': 0.9325102014535173}. Best is trial 38 with value: 0.04440736003658351.[0m
[32m[I 2025-01-07 03:53:10,142][0m Trial 40 finished with value: 0.07556503269213513 and parameters: {'observation_period_num': 46, 'train_rates': 0.9180719444332806, 'learning_rate': 0.00046270051253073966, 'batch_size': 114, 'step_size': 5, 'gamma': 0.9495848017171709}. Best is trial 38 with value: 0.04440736003658351.[0m
[32m[I 2025-01-07 03:54:58,502][0m Trial 41 finished with value: 0.06386222855074902 and parameters: {'observation_period_num': 45, 'train_rates': 0.8811945630374288, 'learning_rate': 0.0003980539695825866, 'batch_size': 59, 'step_size': 4, 'gamma': 0.9309044727994767}. Best is trial 38 with value: 0.04440736003658351.[0m
[32m[I 2025-01-07 03:56:52,190][0m Trial 42 finished with value: 0.09424188066469996 and parameters: {'observation_period_num': 46, 'train_rates': 0.8824185312221119, 'learning_rate': 0.0004108690462920066, 'batch_size': 60, 'step_size': 4, 'gamma': 0.9331654917088875}. Best is trial 38 with value: 0.04440736003658351.[0m
[32m[I 2025-01-07 03:57:49,528][0m Trial 43 finished with value: 0.04155350182184857 and parameters: {'observation_period_num': 31, 'train_rates': 0.8760973810962003, 'learning_rate': 0.0006313770942931895, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9108635590102249}. Best is trial 43 with value: 0.04155350182184857.[0m
[32m[I 2025-01-07 03:59:00,720][0m Trial 44 finished with value: 0.04787233212807108 and parameters: {'observation_period_num': 33, 'train_rates': 0.8728831966623605, 'learning_rate': 0.00065901274682558, 'batch_size': 139, 'step_size': 6, 'gamma': 0.9110783526451198}. Best is trial 43 with value: 0.04155350182184857.[0m
[32m[I 2025-01-07 04:00:07,985][0m Trial 45 finished with value: 0.0354617704156666 and parameters: {'observation_period_num': 27, 'train_rates': 0.9153484605138779, 'learning_rate': 0.000682520558473675, 'batch_size': 146, 'step_size': 6, 'gamma': 0.868878852764843}. Best is trial 45 with value: 0.0354617704156666.[0m
[32m[I 2025-01-07 04:01:09,916][0m Trial 46 finished with value: 0.03267172721075261 and parameters: {'observation_period_num': 22, 'train_rates': 0.870145039266407, 'learning_rate': 0.0006221516554713113, 'batch_size': 144, 'step_size': 7, 'gamma': 0.8706786055921777}. Best is trial 46 with value: 0.03267172721075261.[0m
[32m[I 2025-01-07 04:02:14,662][0m Trial 47 finished with value: 0.03854420784034618 and parameters: {'observation_period_num': 26, 'train_rates': 0.9121528061834947, 'learning_rate': 0.0006694237297938765, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8238653101114853}. Best is trial 46 with value: 0.03267172721075261.[0m
[32m[I 2025-01-07 04:03:22,543][0m Trial 48 finished with value: 0.042536478489637375 and parameters: {'observation_period_num': 27, 'train_rates': 0.9521406374199318, 'learning_rate': 0.0007054763974718768, 'batch_size': 151, 'step_size': 7, 'gamma': 0.815672465675251}. Best is trial 46 with value: 0.03267172721075261.[0m
[32m[I 2025-01-07 04:04:27,412][0m Trial 49 finished with value: 0.048613302409648895 and parameters: {'observation_period_num': 28, 'train_rates': 0.9618693644922776, 'learning_rate': 0.0007090004003364637, 'batch_size': 155, 'step_size': 7, 'gamma': 0.804359563886446}. Best is trial 46 with value: 0.03267172721075261.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8326280637334829, 'learning_rate': 0.0007281037672022916, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9101469761186562}
Epoch 1/300, trend Loss: 0.6162 | 0.2645
Epoch 2/300, trend Loss: 0.2358 | 0.1662
Epoch 3/300, trend Loss: 0.1718 | 0.1354
Epoch 4/300, trend Loss: 0.2111 | 0.1624
Epoch 5/300, trend Loss: 0.2007 | 0.0898
Epoch 6/300, trend Loss: 0.1720 | 0.1711
Epoch 7/300, trend Loss: 0.2087 | 0.1019
Epoch 8/300, trend Loss: 0.1719 | 0.1027
Epoch 9/300, trend Loss: 0.1527 | 0.0968
Epoch 10/300, trend Loss: 0.1453 | 0.0938
Epoch 11/300, trend Loss: 0.1510 | 0.0811
Epoch 12/300, trend Loss: 0.1343 | 0.1037
Epoch 13/300, trend Loss: 0.1536 | 0.0835
Epoch 14/300, trend Loss: 0.1511 | 0.0798
Epoch 15/300, trend Loss: 0.1723 | 0.0793
Epoch 16/300, trend Loss: 0.1726 | 0.0884
Epoch 17/300, trend Loss: 0.1980 | 0.1890
Epoch 18/300, trend Loss: 0.1769 | 0.0920
Epoch 19/300, trend Loss: 0.1525 | 0.0876
Epoch 20/300, trend Loss: 0.1436 | 0.0674
Epoch 21/300, trend Loss: 0.1329 | 0.0854
Epoch 22/300, trend Loss: 0.1136 | 0.0631
Epoch 23/300, trend Loss: 0.1139 | 0.0587
Epoch 24/300, trend Loss: 0.1077 | 0.0614
Epoch 25/300, trend Loss: 0.1082 | 0.0519
Epoch 26/300, trend Loss: 0.1039 | 0.0561
Epoch 27/300, trend Loss: 0.1039 | 0.0509
Epoch 28/300, trend Loss: 0.1002 | 0.0510
Epoch 29/300, trend Loss: 0.1009 | 0.0516
Epoch 30/300, trend Loss: 0.0976 | 0.0486
Epoch 31/300, trend Loss: 0.0978 | 0.0493
Epoch 32/300, trend Loss: 0.0951 | 0.0470
Epoch 33/300, trend Loss: 0.0949 | 0.0460
Epoch 34/300, trend Loss: 0.0927 | 0.0448
Epoch 35/300, trend Loss: 0.0922 | 0.0444
Epoch 36/300, trend Loss: 0.0906 | 0.0430
Epoch 37/300, trend Loss: 0.0903 | 0.0439
Epoch 38/300, trend Loss: 0.0895 | 0.0426
Epoch 39/300, trend Loss: 0.0895 | 0.0439
Epoch 40/300, trend Loss: 0.0895 | 0.0424
Epoch 41/300, trend Loss: 0.0890 | 0.0432
Epoch 42/300, trend Loss: 0.0881 | 0.0424
Epoch 43/300, trend Loss: 0.0876 | 0.0420
Epoch 44/300, trend Loss: 0.0870 | 0.0414
Epoch 45/300, trend Loss: 0.0865 | 0.0414
Epoch 46/300, trend Loss: 0.0862 | 0.0410
Epoch 47/300, trend Loss: 0.0859 | 0.0411
Epoch 48/300, trend Loss: 0.0856 | 0.0408
Epoch 49/300, trend Loss: 0.0853 | 0.0407
Epoch 50/300, trend Loss: 0.0850 | 0.0405
Epoch 51/300, trend Loss: 0.0847 | 0.0404
Epoch 52/300, trend Loss: 0.0844 | 0.0402
Epoch 53/300, trend Loss: 0.0841 | 0.0400
Epoch 54/300, trend Loss: 0.0839 | 0.0399
Epoch 55/300, trend Loss: 0.0836 | 0.0397
Epoch 56/300, trend Loss: 0.0834 | 0.0396
Epoch 57/300, trend Loss: 0.0832 | 0.0394
Epoch 58/300, trend Loss: 0.0829 | 0.0393
Epoch 59/300, trend Loss: 0.0827 | 0.0392
Epoch 60/300, trend Loss: 0.0825 | 0.0390
Epoch 61/300, trend Loss: 0.0823 | 0.0389
Epoch 62/300, trend Loss: 0.0822 | 0.0388
Epoch 63/300, trend Loss: 0.0820 | 0.0387
Epoch 64/300, trend Loss: 0.0819 | 0.0386
Epoch 65/300, trend Loss: 0.0817 | 0.0385
Epoch 66/300, trend Loss: 0.0816 | 0.0384
Epoch 67/300, trend Loss: 0.0814 | 0.0383
Epoch 68/300, trend Loss: 0.0813 | 0.0381
Epoch 69/300, trend Loss: 0.0812 | 0.0381
Epoch 70/300, trend Loss: 0.0810 | 0.0380
Epoch 71/300, trend Loss: 0.0809 | 0.0379
Epoch 72/300, trend Loss: 0.0808 | 0.0378
Epoch 73/300, trend Loss: 0.0807 | 0.0378
Epoch 74/300, trend Loss: 0.0806 | 0.0377
Epoch 75/300, trend Loss: 0.0805 | 0.0377
Epoch 76/300, trend Loss: 0.0805 | 0.0376
Epoch 77/300, trend Loss: 0.0804 | 0.0375
Epoch 78/300, trend Loss: 0.0803 | 0.0375
Epoch 79/300, trend Loss: 0.0802 | 0.0374
Epoch 80/300, trend Loss: 0.0802 | 0.0374
Epoch 81/300, trend Loss: 0.0801 | 0.0373
Epoch 82/300, trend Loss: 0.0800 | 0.0373
Epoch 83/300, trend Loss: 0.0800 | 0.0373
Epoch 84/300, trend Loss: 0.0799 | 0.0372
Epoch 85/300, trend Loss: 0.0798 | 0.0372
Epoch 86/300, trend Loss: 0.0798 | 0.0371
Epoch 87/300, trend Loss: 0.0797 | 0.0371
Epoch 88/300, trend Loss: 0.0797 | 0.0371
Epoch 89/300, trend Loss: 0.0796 | 0.0370
Epoch 90/300, trend Loss: 0.0796 | 0.0370
Epoch 91/300, trend Loss: 0.0795 | 0.0370
Epoch 92/300, trend Loss: 0.0795 | 0.0370
Epoch 93/300, trend Loss: 0.0795 | 0.0369
Epoch 94/300, trend Loss: 0.0794 | 0.0369
Epoch 95/300, trend Loss: 0.0794 | 0.0369
Epoch 96/300, trend Loss: 0.0793 | 0.0368
Epoch 97/300, trend Loss: 0.0793 | 0.0368
Epoch 98/300, trend Loss: 0.0793 | 0.0368
Epoch 99/300, trend Loss: 0.0792 | 0.0368
Epoch 100/300, trend Loss: 0.0792 | 0.0368
Epoch 101/300, trend Loss: 0.0792 | 0.0367
Epoch 102/300, trend Loss: 0.0791 | 0.0367
Epoch 103/300, trend Loss: 0.0791 | 0.0367
Epoch 104/300, trend Loss: 0.0791 | 0.0367
Epoch 105/300, trend Loss: 0.0791 | 0.0367
Epoch 106/300, trend Loss: 0.0790 | 0.0366
Epoch 107/300, trend Loss: 0.0790 | 0.0366
Epoch 108/300, trend Loss: 0.0790 | 0.0366
Epoch 109/300, trend Loss: 0.0790 | 0.0366
Epoch 110/300, trend Loss: 0.0789 | 0.0366
Epoch 111/300, trend Loss: 0.0789 | 0.0366
Epoch 112/300, trend Loss: 0.0789 | 0.0365
Epoch 113/300, trend Loss: 0.0789 | 0.0365
Epoch 114/300, trend Loss: 0.0789 | 0.0365
Epoch 115/300, trend Loss: 0.0788 | 0.0365
Epoch 116/300, trend Loss: 0.0788 | 0.0365
Epoch 117/300, trend Loss: 0.0788 | 0.0365
Epoch 118/300, trend Loss: 0.0788 | 0.0365
Epoch 119/300, trend Loss: 0.0788 | 0.0364
Epoch 120/300, trend Loss: 0.0787 | 0.0364
Epoch 121/300, trend Loss: 0.0787 | 0.0364
Epoch 122/300, trend Loss: 0.0787 | 0.0364
Epoch 123/300, trend Loss: 0.0787 | 0.0364
Epoch 124/300, trend Loss: 0.0787 | 0.0364
Epoch 125/300, trend Loss: 0.0787 | 0.0364
Epoch 126/300, trend Loss: 0.0787 | 0.0364
Epoch 127/300, trend Loss: 0.0786 | 0.0364
Epoch 128/300, trend Loss: 0.0786 | 0.0364
Epoch 129/300, trend Loss: 0.0786 | 0.0363
Epoch 130/300, trend Loss: 0.0786 | 0.0363
Epoch 131/300, trend Loss: 0.0786 | 0.0363
Epoch 132/300, trend Loss: 0.0786 | 0.0363
Epoch 133/300, trend Loss: 0.0786 | 0.0363
Epoch 134/300, trend Loss: 0.0786 | 0.0363
Epoch 135/300, trend Loss: 0.0786 | 0.0363
Epoch 136/300, trend Loss: 0.0785 | 0.0363
Epoch 137/300, trend Loss: 0.0785 | 0.0363
Epoch 138/300, trend Loss: 0.0785 | 0.0363
Epoch 139/300, trend Loss: 0.0785 | 0.0363
Epoch 140/300, trend Loss: 0.0785 | 0.0363
Epoch 141/300, trend Loss: 0.0785 | 0.0363
Epoch 142/300, trend Loss: 0.0785 | 0.0363
Epoch 143/300, trend Loss: 0.0785 | 0.0363
Epoch 144/300, trend Loss: 0.0785 | 0.0362
Epoch 145/300, trend Loss: 0.0785 | 0.0362
Epoch 146/300, trend Loss: 0.0785 | 0.0362
Epoch 147/300, trend Loss: 0.0785 | 0.0362
Epoch 148/300, trend Loss: 0.0785 | 0.0362
Epoch 149/300, trend Loss: 0.0784 | 0.0362
Epoch 150/300, trend Loss: 0.0784 | 0.0362
Epoch 151/300, trend Loss: 0.0784 | 0.0362
Epoch 152/300, trend Loss: 0.0784 | 0.0362
Epoch 153/300, trend Loss: 0.0784 | 0.0362
Epoch 154/300, trend Loss: 0.0784 | 0.0362
Epoch 155/300, trend Loss: 0.0784 | 0.0362
Epoch 156/300, trend Loss: 0.0784 | 0.0362
Epoch 157/300, trend Loss: 0.0784 | 0.0362
Epoch 158/300, trend Loss: 0.0784 | 0.0362
Epoch 159/300, trend Loss: 0.0784 | 0.0362
Epoch 160/300, trend Loss: 0.0784 | 0.0362
Epoch 161/300, trend Loss: 0.0784 | 0.0362
Epoch 162/300, trend Loss: 0.0784 | 0.0362
Epoch 163/300, trend Loss: 0.0784 | 0.0362
Epoch 164/300, trend Loss: 0.0784 | 0.0362
Epoch 165/300, trend Loss: 0.0784 | 0.0362
Epoch 166/300, trend Loss: 0.0784 | 0.0362
Epoch 167/300, trend Loss: 0.0784 | 0.0362
Epoch 168/300, trend Loss: 0.0784 | 0.0362
Epoch 169/300, trend Loss: 0.0784 | 0.0362
Epoch 170/300, trend Loss: 0.0784 | 0.0362
Epoch 171/300, trend Loss: 0.0784 | 0.0362
Epoch 172/300, trend Loss: 0.0784 | 0.0362
Epoch 173/300, trend Loss: 0.0784 | 0.0362
Epoch 174/300, trend Loss: 0.0783 | 0.0362
Epoch 175/300, trend Loss: 0.0783 | 0.0361
Epoch 176/300, trend Loss: 0.0783 | 0.0361
Epoch 177/300, trend Loss: 0.0783 | 0.0361
Epoch 178/300, trend Loss: 0.0783 | 0.0361
Epoch 179/300, trend Loss: 0.0783 | 0.0361
Epoch 180/300, trend Loss: 0.0783 | 0.0361
Epoch 181/300, trend Loss: 0.0783 | 0.0361
Epoch 182/300, trend Loss: 0.0783 | 0.0361
Epoch 183/300, trend Loss: 0.0783 | 0.0361
Epoch 184/300, trend Loss: 0.0783 | 0.0361
Epoch 185/300, trend Loss: 0.0783 | 0.0361
Epoch 186/300, trend Loss: 0.0783 | 0.0361
Epoch 187/300, trend Loss: 0.0783 | 0.0361
Epoch 188/300, trend Loss: 0.0783 | 0.0361
Epoch 189/300, trend Loss: 0.0783 | 0.0361
Epoch 190/300, trend Loss: 0.0783 | 0.0361
Epoch 191/300, trend Loss: 0.0783 | 0.0361
Epoch 192/300, trend Loss: 0.0783 | 0.0361
Epoch 193/300, trend Loss: 0.0783 | 0.0361
Epoch 194/300, trend Loss: 0.0783 | 0.0361
Epoch 195/300, trend Loss: 0.0783 | 0.0361
Epoch 196/300, trend Loss: 0.0783 | 0.0361
Epoch 197/300, trend Loss: 0.0783 | 0.0361
Epoch 198/300, trend Loss: 0.0783 | 0.0361
Epoch 199/300, trend Loss: 0.0783 | 0.0361
Epoch 200/300, trend Loss: 0.0783 | 0.0361
Epoch 201/300, trend Loss: 0.0783 | 0.0361
Epoch 202/300, trend Loss: 0.0783 | 0.0361
Epoch 203/300, trend Loss: 0.0783 | 0.0361
Epoch 204/300, trend Loss: 0.0783 | 0.0361
Epoch 205/300, trend Loss: 0.0783 | 0.0361
Epoch 206/300, trend Loss: 0.0783 | 0.0361
Epoch 207/300, trend Loss: 0.0783 | 0.0361
Epoch 208/300, trend Loss: 0.0783 | 0.0361
Epoch 209/300, trend Loss: 0.0783 | 0.0361
Epoch 210/300, trend Loss: 0.0783 | 0.0361
Epoch 211/300, trend Loss: 0.0783 | 0.0361
Epoch 212/300, trend Loss: 0.0783 | 0.0361
Epoch 213/300, trend Loss: 0.0783 | 0.0361
Epoch 214/300, trend Loss: 0.0783 | 0.0361
Epoch 215/300, trend Loss: 0.0783 | 0.0361
Epoch 216/300, trend Loss: 0.0783 | 0.0361
Epoch 217/300, trend Loss: 0.0783 | 0.0361
Epoch 218/300, trend Loss: 0.0783 | 0.0361
Epoch 219/300, trend Loss: 0.0783 | 0.0361
Epoch 220/300, trend Loss: 0.0783 | 0.0361
Epoch 221/300, trend Loss: 0.0783 | 0.0361
Epoch 222/300, trend Loss: 0.0783 | 0.0361
Epoch 223/300, trend Loss: 0.0783 | 0.0361
Epoch 224/300, trend Loss: 0.0783 | 0.0361
Epoch 225/300, trend Loss: 0.0783 | 0.0361
Epoch 226/300, trend Loss: 0.0783 | 0.0361
Epoch 227/300, trend Loss: 0.0783 | 0.0361
Epoch 228/300, trend Loss: 0.0783 | 0.0361
Epoch 229/300, trend Loss: 0.0783 | 0.0361
Epoch 230/300, trend Loss: 0.0783 | 0.0361
Epoch 231/300, trend Loss: 0.0783 | 0.0361
Epoch 232/300, trend Loss: 0.0783 | 0.0361
Epoch 233/300, trend Loss: 0.0783 | 0.0361
Epoch 234/300, trend Loss: 0.0783 | 0.0361
Epoch 235/300, trend Loss: 0.0783 | 0.0361
Epoch 236/300, trend Loss: 0.0783 | 0.0361
Epoch 237/300, trend Loss: 0.0783 | 0.0361
Epoch 238/300, trend Loss: 0.0783 | 0.0361
Epoch 239/300, trend Loss: 0.0783 | 0.0361
Epoch 240/300, trend Loss: 0.0783 | 0.0361
Epoch 241/300, trend Loss: 0.0783 | 0.0361
Epoch 242/300, trend Loss: 0.0783 | 0.0361
Epoch 243/300, trend Loss: 0.0783 | 0.0361
Epoch 244/300, trend Loss: 0.0783 | 0.0361
Epoch 245/300, trend Loss: 0.0783 | 0.0361
Epoch 246/300, trend Loss: 0.0783 | 0.0361
Epoch 247/300, trend Loss: 0.0783 | 0.0361
Epoch 248/300, trend Loss: 0.0783 | 0.0361
Epoch 249/300, trend Loss: 0.0783 | 0.0361
Epoch 250/300, trend Loss: 0.0783 | 0.0361
Epoch 251/300, trend Loss: 0.0783 | 0.0361
Epoch 252/300, trend Loss: 0.0783 | 0.0361
Epoch 253/300, trend Loss: 0.0783 | 0.0361
Epoch 254/300, trend Loss: 0.0783 | 0.0361
Epoch 255/300, trend Loss: 0.0783 | 0.0361
Epoch 256/300, trend Loss: 0.0783 | 0.0361
Epoch 257/300, trend Loss: 0.0783 | 0.0361
Epoch 258/300, trend Loss: 0.0783 | 0.0361
Epoch 259/300, trend Loss: 0.0783 | 0.0361
Epoch 260/300, trend Loss: 0.0783 | 0.0361
Epoch 261/300, trend Loss: 0.0783 | 0.0361
Epoch 262/300, trend Loss: 0.0783 | 0.0361
Epoch 263/300, trend Loss: 0.0783 | 0.0361
Epoch 264/300, trend Loss: 0.0783 | 0.0361
Epoch 265/300, trend Loss: 0.0783 | 0.0361
Epoch 266/300, trend Loss: 0.0783 | 0.0361
Epoch 267/300, trend Loss: 0.0783 | 0.0361
Epoch 268/300, trend Loss: 0.0783 | 0.0361
Epoch 269/300, trend Loss: 0.0783 | 0.0361
Epoch 270/300, trend Loss: 0.0783 | 0.0361
Epoch 271/300, trend Loss: 0.0783 | 0.0361
Epoch 272/300, trend Loss: 0.0783 | 0.0361
Epoch 273/300, trend Loss: 0.0783 | 0.0361
Epoch 274/300, trend Loss: 0.0783 | 0.0361
Epoch 275/300, trend Loss: 0.0783 | 0.0361
Epoch 276/300, trend Loss: 0.0783 | 0.0361
Epoch 277/300, trend Loss: 0.0783 | 0.0361
Epoch 278/300, trend Loss: 0.0783 | 0.0361
Epoch 279/300, trend Loss: 0.0783 | 0.0361
Epoch 280/300, trend Loss: 0.0783 | 0.0361
Epoch 281/300, trend Loss: 0.0783 | 0.0361
Epoch 282/300, trend Loss: 0.0783 | 0.0361
Epoch 283/300, trend Loss: 0.0783 | 0.0361
Epoch 284/300, trend Loss: 0.0783 | 0.0361
Epoch 285/300, trend Loss: 0.0783 | 0.0361
Epoch 286/300, trend Loss: 0.0783 | 0.0361
Epoch 287/300, trend Loss: 0.0783 | 0.0361
Epoch 288/300, trend Loss: 0.0783 | 0.0361
Epoch 289/300, trend Loss: 0.0783 | 0.0361
Epoch 290/300, trend Loss: 0.0783 | 0.0361
Epoch 291/300, trend Loss: 0.0783 | 0.0361
Epoch 292/300, trend Loss: 0.0783 | 0.0361
Epoch 293/300, trend Loss: 0.0783 | 0.0361
Epoch 294/300, trend Loss: 0.0783 | 0.0361
Epoch 295/300, trend Loss: 0.0783 | 0.0361
Epoch 296/300, trend Loss: 0.0783 | 0.0361
Epoch 297/300, trend Loss: 0.0783 | 0.0361
Epoch 298/300, trend Loss: 0.0783 | 0.0361
Epoch 299/300, trend Loss: 0.0783 | 0.0361
Epoch 300/300, trend Loss: 0.0783 | 0.0361
Training seasonal_0 component with params: {'observation_period_num': 28, 'train_rates': 0.9369126201947702, 'learning_rate': 0.0006942538881869897, 'batch_size': 147, 'step_size': 3, 'gamma': 0.8860994092421566}
Epoch 1/300, seasonal_0 Loss: 0.5820 | 0.3555
Epoch 2/300, seasonal_0 Loss: 0.2333 | 0.1918
Epoch 3/300, seasonal_0 Loss: 0.1901 | 0.1379
Epoch 4/300, seasonal_0 Loss: 0.1807 | 0.1220
Epoch 5/300, seasonal_0 Loss: 0.1643 | 0.1143
Epoch 6/300, seasonal_0 Loss: 0.1578 | 0.1168
Epoch 7/300, seasonal_0 Loss: 0.1581 | 0.1097
Epoch 8/300, seasonal_0 Loss: 0.2050 | 0.2197
Epoch 9/300, seasonal_0 Loss: 0.2243 | 0.5013
Epoch 10/300, seasonal_0 Loss: 0.1859 | 0.1273
Epoch 11/300, seasonal_0 Loss: 0.1647 | 0.1355
Epoch 12/300, seasonal_0 Loss: 0.1543 | 0.1103
Epoch 13/300, seasonal_0 Loss: 0.1528 | 0.0952
Epoch 14/300, seasonal_0 Loss: 0.1280 | 0.1125
Epoch 15/300, seasonal_0 Loss: 0.1269 | 0.0850
Epoch 16/300, seasonal_0 Loss: 0.1163 | 0.0923
Epoch 17/300, seasonal_0 Loss: 0.1277 | 0.0961
Epoch 18/300, seasonal_0 Loss: 0.1219 | 0.0851
Epoch 19/300, seasonal_0 Loss: 0.1115 | 0.0802
Epoch 20/300, seasonal_0 Loss: 0.1104 | 0.0797
Epoch 21/300, seasonal_0 Loss: 0.1096 | 0.0809
Epoch 22/300, seasonal_0 Loss: 0.1057 | 0.0768
Epoch 23/300, seasonal_0 Loss: 0.1035 | 0.0754
Epoch 24/300, seasonal_0 Loss: 0.1022 | 0.0746
Epoch 25/300, seasonal_0 Loss: 0.1017 | 0.0746
Epoch 26/300, seasonal_0 Loss: 0.0996 | 0.0728
Epoch 27/300, seasonal_0 Loss: 0.0990 | 0.0721
Epoch 28/300, seasonal_0 Loss: 0.0983 | 0.0714
Epoch 29/300, seasonal_0 Loss: 0.0976 | 0.0707
Epoch 30/300, seasonal_0 Loss: 0.0971 | 0.0702
Epoch 31/300, seasonal_0 Loss: 0.0966 | 0.0697
Epoch 32/300, seasonal_0 Loss: 0.0961 | 0.0693
Epoch 33/300, seasonal_0 Loss: 0.0957 | 0.0688
Epoch 34/300, seasonal_0 Loss: 0.0953 | 0.0686
Epoch 35/300, seasonal_0 Loss: 0.0950 | 0.0682
Epoch 36/300, seasonal_0 Loss: 0.0947 | 0.0679
Epoch 37/300, seasonal_0 Loss: 0.0944 | 0.0677
Epoch 38/300, seasonal_0 Loss: 0.0942 | 0.0674
Epoch 39/300, seasonal_0 Loss: 0.0939 | 0.0673
Epoch 40/300, seasonal_0 Loss: 0.0937 | 0.0670
Epoch 41/300, seasonal_0 Loss: 0.0935 | 0.0669
Epoch 42/300, seasonal_0 Loss: 0.0933 | 0.0666
Epoch 43/300, seasonal_0 Loss: 0.0931 | 0.0666
Epoch 44/300, seasonal_0 Loss: 0.0930 | 0.0664
Epoch 45/300, seasonal_0 Loss: 0.0928 | 0.0663
Epoch 46/300, seasonal_0 Loss: 0.0927 | 0.0661
Epoch 47/300, seasonal_0 Loss: 0.0926 | 0.0660
Epoch 48/300, seasonal_0 Loss: 0.0924 | 0.0659
Epoch 49/300, seasonal_0 Loss: 0.0923 | 0.0658
Epoch 50/300, seasonal_0 Loss: 0.0922 | 0.0658
Epoch 51/300, seasonal_0 Loss: 0.0921 | 0.0656
Epoch 52/300, seasonal_0 Loss: 0.0920 | 0.0656
Epoch 53/300, seasonal_0 Loss: 0.0920 | 0.0655
Epoch 54/300, seasonal_0 Loss: 0.0919 | 0.0655
Epoch 55/300, seasonal_0 Loss: 0.0918 | 0.0654
Epoch 56/300, seasonal_0 Loss: 0.0917 | 0.0654
Epoch 57/300, seasonal_0 Loss: 0.0917 | 0.0653
Epoch 58/300, seasonal_0 Loss: 0.0916 | 0.0653
Epoch 59/300, seasonal_0 Loss: 0.0916 | 0.0652
Epoch 60/300, seasonal_0 Loss: 0.0915 | 0.0652
Epoch 61/300, seasonal_0 Loss: 0.0915 | 0.0651
Epoch 62/300, seasonal_0 Loss: 0.0914 | 0.0651
Epoch 63/300, seasonal_0 Loss: 0.0914 | 0.0651
Epoch 64/300, seasonal_0 Loss: 0.0914 | 0.0650
Epoch 65/300, seasonal_0 Loss: 0.0913 | 0.0650
Epoch 66/300, seasonal_0 Loss: 0.0913 | 0.0650
Epoch 67/300, seasonal_0 Loss: 0.0913 | 0.0650
Epoch 68/300, seasonal_0 Loss: 0.0912 | 0.0649
Epoch 69/300, seasonal_0 Loss: 0.0912 | 0.0649
Epoch 70/300, seasonal_0 Loss: 0.0912 | 0.0649
Epoch 71/300, seasonal_0 Loss: 0.0912 | 0.0649
Epoch 72/300, seasonal_0 Loss: 0.0912 | 0.0649
Epoch 73/300, seasonal_0 Loss: 0.0911 | 0.0649
Epoch 74/300, seasonal_0 Loss: 0.0911 | 0.0648
Epoch 75/300, seasonal_0 Loss: 0.0911 | 0.0648
Epoch 76/300, seasonal_0 Loss: 0.0911 | 0.0648
Epoch 77/300, seasonal_0 Loss: 0.0911 | 0.0648
Epoch 78/300, seasonal_0 Loss: 0.0911 | 0.0648
Epoch 79/300, seasonal_0 Loss: 0.0910 | 0.0648
Epoch 80/300, seasonal_0 Loss: 0.0910 | 0.0648
Epoch 81/300, seasonal_0 Loss: 0.0910 | 0.0648
Epoch 82/300, seasonal_0 Loss: 0.0910 | 0.0648
Epoch 83/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 84/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 85/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 86/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 87/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 88/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 89/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 90/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 91/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 92/300, seasonal_0 Loss: 0.0910 | 0.0647
Epoch 93/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 94/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 95/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 96/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 97/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 98/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 99/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 100/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 101/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 102/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 103/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 104/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 105/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 106/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 107/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 108/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 109/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 110/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 111/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 112/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 113/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 114/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 115/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 116/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 117/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 118/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 119/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 120/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 121/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 122/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 123/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 124/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 125/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 126/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 127/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 128/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 129/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 130/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 131/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 132/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 133/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 134/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 135/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 136/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 137/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 138/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 139/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 140/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 141/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 142/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 143/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 144/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 145/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 146/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 147/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 148/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 149/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 150/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 151/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 152/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 153/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 154/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 155/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 156/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 157/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 158/300, seasonal_0 Loss: 0.0909 | 0.0647
Epoch 159/300, seasonal_0 Loss: 0.0909 | 0.0647
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.8224864048842686, 'learning_rate': 8.499385829759511e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7602102483124719}
Epoch 1/300, seasonal_1 Loss: 0.2814 | 0.1320
Epoch 2/300, seasonal_1 Loss: 0.1542 | 0.1098
Epoch 3/300, seasonal_1 Loss: 0.1377 | 0.0944
Epoch 4/300, seasonal_1 Loss: 0.1273 | 0.0860
Epoch 5/300, seasonal_1 Loss: 0.1217 | 0.0835
Epoch 6/300, seasonal_1 Loss: 0.1186 | 0.0816
Epoch 7/300, seasonal_1 Loss: 0.1163 | 0.0810
Epoch 8/300, seasonal_1 Loss: 0.1145 | 0.0801
Epoch 9/300, seasonal_1 Loss: 0.1132 | 0.0801
Epoch 10/300, seasonal_1 Loss: 0.1120 | 0.0801
Epoch 11/300, seasonal_1 Loss: 0.1108 | 0.0800
Epoch 12/300, seasonal_1 Loss: 0.1092 | 0.0748
Epoch 13/300, seasonal_1 Loss: 0.1082 | 0.0721
Epoch 14/300, seasonal_1 Loss: 0.1049 | 0.0696
Epoch 15/300, seasonal_1 Loss: 0.1027 | 0.0679
Epoch 16/300, seasonal_1 Loss: 0.1009 | 0.0665
Epoch 17/300, seasonal_1 Loss: 0.0994 | 0.0651
Epoch 18/300, seasonal_1 Loss: 0.0977 | 0.0610
Epoch 19/300, seasonal_1 Loss: 0.0973 | 0.0599
Epoch 20/300, seasonal_1 Loss: 0.0960 | 0.0589
Epoch 21/300, seasonal_1 Loss: 0.0951 | 0.0581
Epoch 22/300, seasonal_1 Loss: 0.0943 | 0.0574
Epoch 23/300, seasonal_1 Loss: 0.0935 | 0.0558
Epoch 24/300, seasonal_1 Loss: 0.0932 | 0.0553
Epoch 25/300, seasonal_1 Loss: 0.0924 | 0.0549
Epoch 26/300, seasonal_1 Loss: 0.0918 | 0.0545
Epoch 27/300, seasonal_1 Loss: 0.0913 | 0.0541
Epoch 28/300, seasonal_1 Loss: 0.0908 | 0.0537
Epoch 29/300, seasonal_1 Loss: 0.0904 | 0.0533
Epoch 30/300, seasonal_1 Loss: 0.0902 | 0.0531
Epoch 31/300, seasonal_1 Loss: 0.0897 | 0.0528
Epoch 32/300, seasonal_1 Loss: 0.0893 | 0.0525
Epoch 33/300, seasonal_1 Loss: 0.0890 | 0.0523
Epoch 34/300, seasonal_1 Loss: 0.0887 | 0.0521
Epoch 35/300, seasonal_1 Loss: 0.0886 | 0.0520
Epoch 36/300, seasonal_1 Loss: 0.0883 | 0.0518
Epoch 37/300, seasonal_1 Loss: 0.0881 | 0.0517
Epoch 38/300, seasonal_1 Loss: 0.0879 | 0.0515
Epoch 39/300, seasonal_1 Loss: 0.0877 | 0.0514
Epoch 40/300, seasonal_1 Loss: 0.0874 | 0.0511
Epoch 41/300, seasonal_1 Loss: 0.0873 | 0.0511
Epoch 42/300, seasonal_1 Loss: 0.0871 | 0.0510
Epoch 43/300, seasonal_1 Loss: 0.0870 | 0.0509
Epoch 44/300, seasonal_1 Loss: 0.0869 | 0.0508
Epoch 45/300, seasonal_1 Loss: 0.0867 | 0.0506
Epoch 46/300, seasonal_1 Loss: 0.0866 | 0.0505
Epoch 47/300, seasonal_1 Loss: 0.0865 | 0.0504
Epoch 48/300, seasonal_1 Loss: 0.0864 | 0.0503
Epoch 49/300, seasonal_1 Loss: 0.0862 | 0.0502
Epoch 50/300, seasonal_1 Loss: 0.0861 | 0.0502
Epoch 51/300, seasonal_1 Loss: 0.0860 | 0.0499
Epoch 52/300, seasonal_1 Loss: 0.0859 | 0.0499
Epoch 53/300, seasonal_1 Loss: 0.0858 | 0.0498
Epoch 54/300, seasonal_1 Loss: 0.0858 | 0.0497
Epoch 55/300, seasonal_1 Loss: 0.0857 | 0.0497
Epoch 56/300, seasonal_1 Loss: 0.0856 | 0.0495
Epoch 57/300, seasonal_1 Loss: 0.0855 | 0.0495
Epoch 58/300, seasonal_1 Loss: 0.0855 | 0.0494
Epoch 59/300, seasonal_1 Loss: 0.0854 | 0.0494
Epoch 60/300, seasonal_1 Loss: 0.0853 | 0.0493
Epoch 61/300, seasonal_1 Loss: 0.0852 | 0.0493
Epoch 62/300, seasonal_1 Loss: 0.0851 | 0.0492
Epoch 63/300, seasonal_1 Loss: 0.0851 | 0.0492
Epoch 64/300, seasonal_1 Loss: 0.0850 | 0.0491
Epoch 65/300, seasonal_1 Loss: 0.0849 | 0.0491
Epoch 66/300, seasonal_1 Loss: 0.0848 | 0.0491
Epoch 67/300, seasonal_1 Loss: 0.0847 | 0.0491
Epoch 68/300, seasonal_1 Loss: 0.0847 | 0.0490
Epoch 69/300, seasonal_1 Loss: 0.0846 | 0.0490
Epoch 70/300, seasonal_1 Loss: 0.0845 | 0.0490
Epoch 71/300, seasonal_1 Loss: 0.0845 | 0.0489
Epoch 72/300, seasonal_1 Loss: 0.0844 | 0.0489
Epoch 73/300, seasonal_1 Loss: 0.0844 | 0.0489
Epoch 74/300, seasonal_1 Loss: 0.0843 | 0.0489
Epoch 75/300, seasonal_1 Loss: 0.0843 | 0.0489
Epoch 76/300, seasonal_1 Loss: 0.0842 | 0.0489
Epoch 77/300, seasonal_1 Loss: 0.0842 | 0.0488
Epoch 78/300, seasonal_1 Loss: 0.0841 | 0.0488
Epoch 79/300, seasonal_1 Loss: 0.0841 | 0.0488
Epoch 80/300, seasonal_1 Loss: 0.0841 | 0.0488
Epoch 81/300, seasonal_1 Loss: 0.0840 | 0.0488
Epoch 82/300, seasonal_1 Loss: 0.0840 | 0.0488
Epoch 83/300, seasonal_1 Loss: 0.0840 | 0.0487
Epoch 84/300, seasonal_1 Loss: 0.0840 | 0.0487
Epoch 85/300, seasonal_1 Loss: 0.0839 | 0.0487
Epoch 86/300, seasonal_1 Loss: 0.0839 | 0.0487
Epoch 87/300, seasonal_1 Loss: 0.0839 | 0.0487
Epoch 88/300, seasonal_1 Loss: 0.0839 | 0.0487
Epoch 89/300, seasonal_1 Loss: 0.0839 | 0.0486
Epoch 90/300, seasonal_1 Loss: 0.0839 | 0.0486
Epoch 91/300, seasonal_1 Loss: 0.0838 | 0.0486
Epoch 92/300, seasonal_1 Loss: 0.0838 | 0.0486
Epoch 93/300, seasonal_1 Loss: 0.0838 | 0.0486
Epoch 94/300, seasonal_1 Loss: 0.0838 | 0.0486
Epoch 95/300, seasonal_1 Loss: 0.0838 | 0.0486
Epoch 96/300, seasonal_1 Loss: 0.0837 | 0.0486
Epoch 97/300, seasonal_1 Loss: 0.0837 | 0.0485
Epoch 98/300, seasonal_1 Loss: 0.0837 | 0.0485
Epoch 99/300, seasonal_1 Loss: 0.0837 | 0.0485
Epoch 100/300, seasonal_1 Loss: 0.0837 | 0.0485
Epoch 101/300, seasonal_1 Loss: 0.0836 | 0.0485
Epoch 102/300, seasonal_1 Loss: 0.0836 | 0.0485
Epoch 103/300, seasonal_1 Loss: 0.0836 | 0.0485
Epoch 104/300, seasonal_1 Loss: 0.0836 | 0.0485
Epoch 105/300, seasonal_1 Loss: 0.0836 | 0.0484
Epoch 106/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 107/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 108/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 109/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 110/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 111/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 112/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 113/300, seasonal_1 Loss: 0.0835 | 0.0484
Epoch 114/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 115/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 116/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 117/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 118/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 119/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 120/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 121/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 122/300, seasonal_1 Loss: 0.0834 | 0.0484
Epoch 123/300, seasonal_1 Loss: 0.0834 | 0.0483
Epoch 124/300, seasonal_1 Loss: 0.0834 | 0.0483
Epoch 125/300, seasonal_1 Loss: 0.0834 | 0.0483
Epoch 126/300, seasonal_1 Loss: 0.0834 | 0.0483
Epoch 127/300, seasonal_1 Loss: 0.0834 | 0.0483
Epoch 128/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 129/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 130/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 131/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 132/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 133/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 134/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 135/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 136/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 137/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 138/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 139/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 140/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 141/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 142/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 143/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 144/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 145/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 146/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 147/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 148/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 149/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 150/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 151/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 152/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 153/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 154/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 155/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 156/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 157/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 158/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 159/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 160/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 161/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 162/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 163/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 164/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 165/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 166/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 167/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 168/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 169/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 170/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 171/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 172/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 173/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 174/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 175/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 176/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 177/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 178/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 179/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 180/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 181/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 182/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 183/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 184/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 185/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 186/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 187/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 188/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 189/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 190/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 191/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 192/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 193/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 194/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 195/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 196/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 197/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 198/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 199/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 200/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 201/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 202/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 203/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 204/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 205/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 206/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 207/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 208/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 209/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 210/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 211/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 212/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 213/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 214/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 215/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 216/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 217/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 218/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 219/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 220/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 221/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 222/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 223/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 224/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 225/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 226/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 227/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 228/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 229/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 230/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 231/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 232/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 233/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 234/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 235/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 236/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 237/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 238/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 239/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 240/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 241/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 242/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 243/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 244/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 245/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 246/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 247/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 248/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 249/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 250/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 251/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 252/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 253/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 254/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 255/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 256/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 257/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 258/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 259/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 260/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 261/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 262/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 263/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 264/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 265/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 266/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 267/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 268/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 269/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 270/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 271/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 272/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 273/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 274/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 275/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 276/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 277/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 278/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 279/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 280/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 281/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 282/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 283/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 284/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 285/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 286/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 287/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 288/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 289/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 290/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 291/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 292/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 293/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 294/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 295/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 296/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 297/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 298/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 299/300, seasonal_1 Loss: 0.0833 | 0.0483
Epoch 300/300, seasonal_1 Loss: 0.0833 | 0.0483
Training seasonal_2 component with params: {'observation_period_num': 32, 'train_rates': 0.9899741120700878, 'learning_rate': 0.00024081927604822638, 'batch_size': 44, 'step_size': 8, 'gamma': 0.7985774186825085}
Epoch 1/300, seasonal_2 Loss: 0.2782 | 0.1103
Epoch 2/300, seasonal_2 Loss: 0.1453 | 0.0828
Epoch 3/300, seasonal_2 Loss: 0.1230 | 0.0750
Epoch 4/300, seasonal_2 Loss: 0.1119 | 0.0769
Epoch 5/300, seasonal_2 Loss: 0.1077 | 0.0850
Epoch 6/300, seasonal_2 Loss: 0.1091 | 0.0936
Epoch 7/300, seasonal_2 Loss: 0.1033 | 0.0905
Epoch 8/300, seasonal_2 Loss: 0.0985 | 0.0979
Epoch 9/300, seasonal_2 Loss: 0.0913 | 0.0774
Epoch 10/300, seasonal_2 Loss: 0.0885 | 0.0819
Epoch 11/300, seasonal_2 Loss: 0.0858 | 0.0730
Epoch 12/300, seasonal_2 Loss: 0.0828 | 0.0666
Epoch 13/300, seasonal_2 Loss: 0.0804 | 0.0618
Epoch 14/300, seasonal_2 Loss: 0.0781 | 0.0600
Epoch 15/300, seasonal_2 Loss: 0.0769 | 0.0590
Epoch 16/300, seasonal_2 Loss: 0.0757 | 0.0586
Epoch 17/300, seasonal_2 Loss: 0.0742 | 0.0582
Epoch 18/300, seasonal_2 Loss: 0.0729 | 0.0573
Epoch 19/300, seasonal_2 Loss: 0.0723 | 0.0568
Epoch 20/300, seasonal_2 Loss: 0.0716 | 0.0560
Epoch 21/300, seasonal_2 Loss: 0.0707 | 0.0565
Epoch 22/300, seasonal_2 Loss: 0.0700 | 0.0556
Epoch 23/300, seasonal_2 Loss: 0.0695 | 0.0546
Epoch 24/300, seasonal_2 Loss: 0.0690 | 0.0537
Epoch 25/300, seasonal_2 Loss: 0.0683 | 0.0545
Epoch 26/300, seasonal_2 Loss: 0.0679 | 0.0540
Epoch 27/300, seasonal_2 Loss: 0.0673 | 0.0530
Epoch 28/300, seasonal_2 Loss: 0.0668 | 0.0520
Epoch 29/300, seasonal_2 Loss: 0.0663 | 0.0520
Epoch 30/300, seasonal_2 Loss: 0.0659 | 0.0519
Epoch 31/300, seasonal_2 Loss: 0.0655 | 0.0512
Epoch 32/300, seasonal_2 Loss: 0.0650 | 0.0502
Epoch 33/300, seasonal_2 Loss: 0.0646 | 0.0493
Epoch 34/300, seasonal_2 Loss: 0.0644 | 0.0493
Epoch 35/300, seasonal_2 Loss: 0.0640 | 0.0489
Epoch 36/300, seasonal_2 Loss: 0.0636 | 0.0482
Epoch 37/300, seasonal_2 Loss: 0.0634 | 0.0459
Epoch 38/300, seasonal_2 Loss: 0.0632 | 0.0458
Epoch 39/300, seasonal_2 Loss: 0.0629 | 0.0454
Epoch 40/300, seasonal_2 Loss: 0.0626 | 0.0449
Epoch 41/300, seasonal_2 Loss: 0.0623 | 0.0446
Epoch 42/300, seasonal_2 Loss: 0.0620 | 0.0445
Epoch 43/300, seasonal_2 Loss: 0.0616 | 0.0441
Epoch 44/300, seasonal_2 Loss: 0.0614 | 0.0438
Epoch 45/300, seasonal_2 Loss: 0.0613 | 0.0437
Epoch 46/300, seasonal_2 Loss: 0.0612 | 0.0436
Epoch 47/300, seasonal_2 Loss: 0.0612 | 0.0434
Epoch 48/300, seasonal_2 Loss: 0.0611 | 0.0432
Epoch 49/300, seasonal_2 Loss: 0.0613 | 0.0430
Epoch 50/300, seasonal_2 Loss: 0.0612 | 0.0430
Epoch 51/300, seasonal_2 Loss: 0.0613 | 0.0429
Epoch 52/300, seasonal_2 Loss: 0.0614 | 0.0428
Epoch 53/300, seasonal_2 Loss: 0.0620 | 0.0432
Epoch 54/300, seasonal_2 Loss: 0.0623 | 0.0436
Epoch 55/300, seasonal_2 Loss: 0.0628 | 0.0443
Epoch 56/300, seasonal_2 Loss: 0.0631 | 0.0458
Epoch 57/300, seasonal_2 Loss: 0.0640 | 0.0582
Epoch 58/300, seasonal_2 Loss: 0.0640 | 0.0626
Epoch 59/300, seasonal_2 Loss: 0.0634 | 0.0652
Epoch 60/300, seasonal_2 Loss: 0.0627 | 0.0660
Epoch 61/300, seasonal_2 Loss: 0.0623 | 0.0661
Epoch 62/300, seasonal_2 Loss: 0.0619 | 0.0630
Epoch 63/300, seasonal_2 Loss: 0.0615 | 0.0603
Epoch 64/300, seasonal_2 Loss: 0.0611 | 0.0581
Epoch 65/300, seasonal_2 Loss: 0.0609 | 0.0538
Epoch 66/300, seasonal_2 Loss: 0.0606 | 0.0527
Epoch 67/300, seasonal_2 Loss: 0.0602 | 0.0520
Epoch 68/300, seasonal_2 Loss: 0.0598 | 0.0515
Epoch 69/300, seasonal_2 Loss: 0.0596 | 0.0496
Epoch 70/300, seasonal_2 Loss: 0.0594 | 0.0493
Epoch 71/300, seasonal_2 Loss: 0.0591 | 0.0491
Epoch 72/300, seasonal_2 Loss: 0.0590 | 0.0489
Epoch 73/300, seasonal_2 Loss: 0.0588 | 0.0478
Epoch 74/300, seasonal_2 Loss: 0.0588 | 0.0477
Epoch 75/300, seasonal_2 Loss: 0.0587 | 0.0476
Epoch 76/300, seasonal_2 Loss: 0.0586 | 0.0475
Epoch 77/300, seasonal_2 Loss: 0.0585 | 0.0469
Epoch 78/300, seasonal_2 Loss: 0.0584 | 0.0468
Epoch 79/300, seasonal_2 Loss: 0.0584 | 0.0467
Epoch 80/300, seasonal_2 Loss: 0.0584 | 0.0467
Epoch 81/300, seasonal_2 Loss: 0.0583 | 0.0462
Epoch 82/300, seasonal_2 Loss: 0.0582 | 0.0461
Epoch 83/300, seasonal_2 Loss: 0.0582 | 0.0461
Epoch 84/300, seasonal_2 Loss: 0.0582 | 0.0460
Epoch 85/300, seasonal_2 Loss: 0.0581 | 0.0457
Epoch 86/300, seasonal_2 Loss: 0.0581 | 0.0456
Epoch 87/300, seasonal_2 Loss: 0.0581 | 0.0456
Epoch 88/300, seasonal_2 Loss: 0.0580 | 0.0455
Epoch 89/300, seasonal_2 Loss: 0.0580 | 0.0453
Epoch 90/300, seasonal_2 Loss: 0.0580 | 0.0452
Epoch 91/300, seasonal_2 Loss: 0.0579 | 0.0452
Epoch 92/300, seasonal_2 Loss: 0.0579 | 0.0451
Epoch 93/300, seasonal_2 Loss: 0.0579 | 0.0449
Epoch 94/300, seasonal_2 Loss: 0.0579 | 0.0449
Epoch 95/300, seasonal_2 Loss: 0.0579 | 0.0448
Epoch 96/300, seasonal_2 Loss: 0.0578 | 0.0448
Epoch 97/300, seasonal_2 Loss: 0.0578 | 0.0447
Epoch 98/300, seasonal_2 Loss: 0.0578 | 0.0446
Epoch 99/300, seasonal_2 Loss: 0.0578 | 0.0446
Epoch 100/300, seasonal_2 Loss: 0.0578 | 0.0445
Epoch 101/300, seasonal_2 Loss: 0.0578 | 0.0445
Epoch 102/300, seasonal_2 Loss: 0.0577 | 0.0444
Epoch 103/300, seasonal_2 Loss: 0.0577 | 0.0444
Epoch 104/300, seasonal_2 Loss: 0.0577 | 0.0443
Epoch 105/300, seasonal_2 Loss: 0.0577 | 0.0443
Epoch 106/300, seasonal_2 Loss: 0.0577 | 0.0442
Epoch 107/300, seasonal_2 Loss: 0.0577 | 0.0442
Epoch 108/300, seasonal_2 Loss: 0.0577 | 0.0442
Epoch 109/300, seasonal_2 Loss: 0.0577 | 0.0441
Epoch 110/300, seasonal_2 Loss: 0.0577 | 0.0441
Epoch 111/300, seasonal_2 Loss: 0.0576 | 0.0441
Epoch 112/300, seasonal_2 Loss: 0.0576 | 0.0441
Epoch 113/300, seasonal_2 Loss: 0.0576 | 0.0440
Epoch 114/300, seasonal_2 Loss: 0.0576 | 0.0440
Epoch 115/300, seasonal_2 Loss: 0.0576 | 0.0440
Epoch 116/300, seasonal_2 Loss: 0.0576 | 0.0440
Epoch 117/300, seasonal_2 Loss: 0.0576 | 0.0439
Epoch 118/300, seasonal_2 Loss: 0.0576 | 0.0439
Epoch 119/300, seasonal_2 Loss: 0.0576 | 0.0439
Epoch 120/300, seasonal_2 Loss: 0.0576 | 0.0439
Epoch 121/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 122/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 123/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 124/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 125/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 126/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 127/300, seasonal_2 Loss: 0.0576 | 0.0438
Epoch 128/300, seasonal_2 Loss: 0.0576 | 0.0437
Epoch 129/300, seasonal_2 Loss: 0.0576 | 0.0437
Epoch 130/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 131/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 132/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 133/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 134/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 135/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 136/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 137/300, seasonal_2 Loss: 0.0575 | 0.0437
Epoch 138/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 139/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 140/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 141/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 142/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 143/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 144/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 145/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 146/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 147/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 148/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 149/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 150/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 151/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 152/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 153/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 154/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 155/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 156/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 157/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 158/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 159/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 160/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 161/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 162/300, seasonal_2 Loss: 0.0575 | 0.0436
Epoch 163/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 164/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 165/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 166/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 167/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 168/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 169/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 170/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 171/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 172/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 173/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 174/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 175/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 176/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 177/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 178/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 179/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 180/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 181/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 182/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 183/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 184/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 185/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 186/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 187/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 188/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 189/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 190/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 191/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 192/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 193/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 194/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 195/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 196/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 197/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 198/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 199/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 200/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 201/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 202/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 203/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 204/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 205/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 206/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 207/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 208/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 209/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 210/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 211/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 212/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 213/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 214/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 215/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 216/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 217/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 218/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 219/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 220/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 221/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 222/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 223/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 224/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 225/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 226/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 227/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 228/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 229/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 230/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 231/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 232/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 233/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 234/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 235/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 236/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 237/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 238/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 239/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 240/300, seasonal_2 Loss: 0.0575 | 0.0435
Epoch 241/300, seasonal_2 Loss: 0.0575 | 0.0435
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 7, 'train_rates': 0.8788296426853188, 'learning_rate': 0.0001731791157046036, 'batch_size': 70, 'step_size': 6, 'gamma': 0.9603340208072857}
Epoch 1/300, seasonal_3 Loss: 0.3561 | 0.1380
Epoch 2/300, seasonal_3 Loss: 0.1542 | 0.0927
Epoch 3/300, seasonal_3 Loss: 0.1411 | 0.0813
Epoch 4/300, seasonal_3 Loss: 0.1297 | 0.0757
Epoch 5/300, seasonal_3 Loss: 0.1234 | 0.0702
Epoch 6/300, seasonal_3 Loss: 0.1183 | 0.0668
Epoch 7/300, seasonal_3 Loss: 0.1141 | 0.0665
Epoch 8/300, seasonal_3 Loss: 0.1107 | 0.0673
Epoch 9/300, seasonal_3 Loss: 0.1083 | 0.0699
Epoch 10/300, seasonal_3 Loss: 0.1064 | 0.0756
Epoch 11/300, seasonal_3 Loss: 0.1052 | 0.0801
Epoch 12/300, seasonal_3 Loss: 0.1040 | 0.0799
Epoch 13/300, seasonal_3 Loss: 0.1031 | 0.0732
Epoch 14/300, seasonal_3 Loss: 0.1028 | 0.0685
Epoch 15/300, seasonal_3 Loss: 0.1021 | 0.0643
Epoch 16/300, seasonal_3 Loss: 0.1006 | 0.0613
Epoch 17/300, seasonal_3 Loss: 0.0992 | 0.0598
Epoch 18/300, seasonal_3 Loss: 0.0974 | 0.0571
Epoch 19/300, seasonal_3 Loss: 0.0956 | 0.0540
Epoch 20/300, seasonal_3 Loss: 0.0944 | 0.0515
Epoch 21/300, seasonal_3 Loss: 0.0932 | 0.0502
Epoch 22/300, seasonal_3 Loss: 0.0923 | 0.0493
Epoch 23/300, seasonal_3 Loss: 0.0919 | 0.0489
Epoch 24/300, seasonal_3 Loss: 0.0919 | 0.0489
Epoch 25/300, seasonal_3 Loss: 0.0922 | 0.0495
Epoch 26/300, seasonal_3 Loss: 0.0920 | 0.0493
Epoch 27/300, seasonal_3 Loss: 0.0918 | 0.0479
Epoch 28/300, seasonal_3 Loss: 0.0919 | 0.0457
Epoch 29/300, seasonal_3 Loss: 0.0918 | 0.0449
Epoch 30/300, seasonal_3 Loss: 0.0914 | 0.0443
Epoch 31/300, seasonal_3 Loss: 0.0908 | 0.0455
Epoch 32/300, seasonal_3 Loss: 0.0902 | 0.0465
Epoch 33/300, seasonal_3 Loss: 0.0887 | 0.0477
Epoch 34/300, seasonal_3 Loss: 0.0878 | 0.0513
Epoch 35/300, seasonal_3 Loss: 0.0852 | 0.0456
Epoch 36/300, seasonal_3 Loss: 0.0834 | 0.0478
Epoch 37/300, seasonal_3 Loss: 0.0817 | 0.0450
Epoch 38/300, seasonal_3 Loss: 0.0821 | 0.0489
Epoch 39/300, seasonal_3 Loss: 0.0869 | 0.0808
Epoch 40/300, seasonal_3 Loss: 0.0917 | 0.0459
Epoch 41/300, seasonal_3 Loss: 0.0882 | 0.0529
Epoch 42/300, seasonal_3 Loss: 0.0864 | 0.0412
Epoch 43/300, seasonal_3 Loss: 0.0880 | 0.0426
Epoch 44/300, seasonal_3 Loss: 0.0854 | 0.0406
Epoch 45/300, seasonal_3 Loss: 0.0856 | 0.0406
Epoch 46/300, seasonal_3 Loss: 0.0842 | 0.0396
Epoch 47/300, seasonal_3 Loss: 0.0835 | 0.0402
Epoch 48/300, seasonal_3 Loss: 0.0827 | 0.0406
Epoch 49/300, seasonal_3 Loss: 0.0816 | 0.0389
Epoch 50/300, seasonal_3 Loss: 0.0786 | 0.0382
Epoch 51/300, seasonal_3 Loss: 0.0767 | 0.0376
Epoch 52/300, seasonal_3 Loss: 0.0760 | 0.0373
Epoch 53/300, seasonal_3 Loss: 0.0756 | 0.0373
Epoch 54/300, seasonal_3 Loss: 0.0751 | 0.0372
Epoch 55/300, seasonal_3 Loss: 0.0748 | 0.0370
Epoch 56/300, seasonal_3 Loss: 0.0744 | 0.0367
Epoch 57/300, seasonal_3 Loss: 0.0741 | 0.0363
Epoch 58/300, seasonal_3 Loss: 0.0737 | 0.0360
Epoch 59/300, seasonal_3 Loss: 0.0734 | 0.0356
Epoch 60/300, seasonal_3 Loss: 0.0730 | 0.0353
Epoch 61/300, seasonal_3 Loss: 0.0726 | 0.0351
Epoch 62/300, seasonal_3 Loss: 0.0723 | 0.0349
Epoch 63/300, seasonal_3 Loss: 0.0719 | 0.0348
Epoch 64/300, seasonal_3 Loss: 0.0716 | 0.0346
Epoch 65/300, seasonal_3 Loss: 0.0713 | 0.0345
Epoch 66/300, seasonal_3 Loss: 0.0709 | 0.0345
Epoch 67/300, seasonal_3 Loss: 0.0706 | 0.0345
Epoch 68/300, seasonal_3 Loss: 0.0702 | 0.0344
Epoch 69/300, seasonal_3 Loss: 0.0699 | 0.0345
Epoch 70/300, seasonal_3 Loss: 0.0695 | 0.0345
Epoch 71/300, seasonal_3 Loss: 0.0692 | 0.0346
Epoch 72/300, seasonal_3 Loss: 0.0690 | 0.0346
Epoch 73/300, seasonal_3 Loss: 0.0687 | 0.0349
Epoch 74/300, seasonal_3 Loss: 0.0686 | 0.0361
Epoch 75/300, seasonal_3 Loss: 0.0690 | 0.0332
Epoch 76/300, seasonal_3 Loss: 0.0689 | 0.0320
Epoch 77/300, seasonal_3 Loss: 0.0683 | 0.0328
Epoch 78/300, seasonal_3 Loss: 0.0681 | 0.0330
Epoch 79/300, seasonal_3 Loss: 0.0683 | 0.0318
Epoch 80/300, seasonal_3 Loss: 0.0681 | 0.0320
Epoch 81/300, seasonal_3 Loss: 0.0682 | 0.0317
Epoch 82/300, seasonal_3 Loss: 0.0683 | 0.0316
Epoch 83/300, seasonal_3 Loss: 0.0686 | 0.0320
Epoch 84/300, seasonal_3 Loss: 0.0688 | 0.0321
Epoch 85/300, seasonal_3 Loss: 0.0686 | 0.0316
Epoch 86/300, seasonal_3 Loss: 0.0684 | 0.0315
Epoch 87/300, seasonal_3 Loss: 0.0683 | 0.0312
Epoch 88/300, seasonal_3 Loss: 0.0681 | 0.0309
Epoch 89/300, seasonal_3 Loss: 0.0683 | 0.0313
Epoch 90/300, seasonal_3 Loss: 0.0678 | 0.0311
Epoch 91/300, seasonal_3 Loss: 0.0664 | 0.0305
Epoch 92/300, seasonal_3 Loss: 0.0664 | 0.0304
Epoch 93/300, seasonal_3 Loss: 0.0661 | 0.0301
Epoch 94/300, seasonal_3 Loss: 0.0659 | 0.0300
Epoch 95/300, seasonal_3 Loss: 0.0658 | 0.0298
Epoch 96/300, seasonal_3 Loss: 0.0656 | 0.0296
Epoch 97/300, seasonal_3 Loss: 0.0655 | 0.0296
Epoch 98/300, seasonal_3 Loss: 0.0653 | 0.0294
Epoch 99/300, seasonal_3 Loss: 0.0651 | 0.0293
Epoch 100/300, seasonal_3 Loss: 0.0650 | 0.0294
Epoch 101/300, seasonal_3 Loss: 0.0649 | 0.0293
Epoch 102/300, seasonal_3 Loss: 0.0647 | 0.0293
Epoch 103/300, seasonal_3 Loss: 0.0646 | 0.0294
Epoch 104/300, seasonal_3 Loss: 0.0644 | 0.0294
Epoch 105/300, seasonal_3 Loss: 0.0643 | 0.0294
Epoch 106/300, seasonal_3 Loss: 0.0642 | 0.0293
Epoch 107/300, seasonal_3 Loss: 0.0642 | 0.0290
Epoch 108/300, seasonal_3 Loss: 0.0644 | 0.0286
Epoch 109/300, seasonal_3 Loss: 0.0646 | 0.0288
Epoch 110/300, seasonal_3 Loss: 0.0641 | 0.0286
Epoch 111/300, seasonal_3 Loss: 0.0640 | 0.0289
Epoch 112/300, seasonal_3 Loss: 0.0640 | 0.0290
Epoch 113/300, seasonal_3 Loss: 0.0640 | 0.0292
Epoch 114/300, seasonal_3 Loss: 0.0641 | 0.0292
Epoch 115/300, seasonal_3 Loss: 0.0643 | 0.0288
Epoch 116/300, seasonal_3 Loss: 0.0646 | 0.0286
Epoch 117/300, seasonal_3 Loss: 0.0652 | 0.0285
Epoch 118/300, seasonal_3 Loss: 0.0650 | 0.0280
Epoch 119/300, seasonal_3 Loss: 0.0649 | 0.0283
Epoch 120/300, seasonal_3 Loss: 0.0647 | 0.0286
Epoch 121/300, seasonal_3 Loss: 0.0644 | 0.0285
Epoch 122/300, seasonal_3 Loss: 0.0643 | 0.0288
Epoch 123/300, seasonal_3 Loss: 0.0641 | 0.0285
Epoch 124/300, seasonal_3 Loss: 0.0641 | 0.0293
Epoch 125/300, seasonal_3 Loss: 0.0639 | 0.0285
Epoch 126/300, seasonal_3 Loss: 0.0639 | 0.0295
Epoch 127/300, seasonal_3 Loss: 0.0634 | 0.0283
Epoch 128/300, seasonal_3 Loss: 0.0633 | 0.0289
Epoch 129/300, seasonal_3 Loss: 0.0629 | 0.0282
Epoch 130/300, seasonal_3 Loss: 0.0628 | 0.0284
Epoch 131/300, seasonal_3 Loss: 0.0626 | 0.0280
Epoch 132/300, seasonal_3 Loss: 0.0625 | 0.0280
Epoch 133/300, seasonal_3 Loss: 0.0624 | 0.0279
Epoch 134/300, seasonal_3 Loss: 0.0624 | 0.0279
Epoch 135/300, seasonal_3 Loss: 0.0623 | 0.0278
Epoch 136/300, seasonal_3 Loss: 0.0623 | 0.0279
Epoch 137/300, seasonal_3 Loss: 0.0622 | 0.0278
Epoch 138/300, seasonal_3 Loss: 0.0622 | 0.0278
Epoch 139/300, seasonal_3 Loss: 0.0621 | 0.0278
Epoch 140/300, seasonal_3 Loss: 0.0621 | 0.0278
Epoch 141/300, seasonal_3 Loss: 0.0621 | 0.0277
Epoch 142/300, seasonal_3 Loss: 0.0621 | 0.0277
Epoch 143/300, seasonal_3 Loss: 0.0621 | 0.0276
Epoch 144/300, seasonal_3 Loss: 0.0621 | 0.0276
Epoch 145/300, seasonal_3 Loss: 0.0622 | 0.0276
Epoch 146/300, seasonal_3 Loss: 0.0624 | 0.0277
Epoch 147/300, seasonal_3 Loss: 0.0625 | 0.0278
Epoch 148/300, seasonal_3 Loss: 0.0624 | 0.0277
Epoch 149/300, seasonal_3 Loss: 0.0622 | 0.0276
Epoch 150/300, seasonal_3 Loss: 0.0619 | 0.0275
Epoch 151/300, seasonal_3 Loss: 0.0616 | 0.0275
Epoch 152/300, seasonal_3 Loss: 0.0614 | 0.0275
Epoch 153/300, seasonal_3 Loss: 0.0612 | 0.0274
Epoch 154/300, seasonal_3 Loss: 0.0610 | 0.0274
Epoch 155/300, seasonal_3 Loss: 0.0609 | 0.0274
Epoch 156/300, seasonal_3 Loss: 0.0608 | 0.0274
Epoch 157/300, seasonal_3 Loss: 0.0607 | 0.0273
Epoch 158/300, seasonal_3 Loss: 0.0606 | 0.0273
Epoch 159/300, seasonal_3 Loss: 0.0605 | 0.0273
Epoch 160/300, seasonal_3 Loss: 0.0604 | 0.0273
Epoch 161/300, seasonal_3 Loss: 0.0603 | 0.0272
Epoch 162/300, seasonal_3 Loss: 0.0603 | 0.0272
Epoch 163/300, seasonal_3 Loss: 0.0602 | 0.0272
Epoch 164/300, seasonal_3 Loss: 0.0601 | 0.0272
Epoch 165/300, seasonal_3 Loss: 0.0601 | 0.0272
Epoch 166/300, seasonal_3 Loss: 0.0600 | 0.0272
Epoch 167/300, seasonal_3 Loss: 0.0599 | 0.0272
Epoch 168/300, seasonal_3 Loss: 0.0598 | 0.0271
Epoch 169/300, seasonal_3 Loss: 0.0597 | 0.0271
Epoch 170/300, seasonal_3 Loss: 0.0596 | 0.0271
Epoch 171/300, seasonal_3 Loss: 0.0595 | 0.0271
Epoch 172/300, seasonal_3 Loss: 0.0595 | 0.0271
Epoch 173/300, seasonal_3 Loss: 0.0594 | 0.0271
Epoch 174/300, seasonal_3 Loss: 0.0593 | 0.0270
Epoch 175/300, seasonal_3 Loss: 0.0592 | 0.0270
Epoch 176/300, seasonal_3 Loss: 0.0592 | 0.0270
Epoch 177/300, seasonal_3 Loss: 0.0591 | 0.0270
Epoch 178/300, seasonal_3 Loss: 0.0590 | 0.0270
Epoch 179/300, seasonal_3 Loss: 0.0590 | 0.0270
Epoch 180/300, seasonal_3 Loss: 0.0589 | 0.0270
Epoch 181/300, seasonal_3 Loss: 0.0589 | 0.0270
Epoch 182/300, seasonal_3 Loss: 0.0588 | 0.0270
Epoch 183/300, seasonal_3 Loss: 0.0588 | 0.0269
Epoch 184/300, seasonal_3 Loss: 0.0587 | 0.0269
Epoch 185/300, seasonal_3 Loss: 0.0587 | 0.0269
Epoch 186/300, seasonal_3 Loss: 0.0586 | 0.0269
Epoch 187/300, seasonal_3 Loss: 0.0586 | 0.0269
Epoch 188/300, seasonal_3 Loss: 0.0586 | 0.0269
Epoch 189/300, seasonal_3 Loss: 0.0585 | 0.0269
Epoch 190/300, seasonal_3 Loss: 0.0585 | 0.0269
Epoch 191/300, seasonal_3 Loss: 0.0584 | 0.0269
Epoch 192/300, seasonal_3 Loss: 0.0584 | 0.0268
Epoch 193/300, seasonal_3 Loss: 0.0584 | 0.0268
Epoch 194/300, seasonal_3 Loss: 0.0583 | 0.0268
Epoch 195/300, seasonal_3 Loss: 0.0583 | 0.0268
Epoch 196/300, seasonal_3 Loss: 0.0582 | 0.0268
Epoch 197/300, seasonal_3 Loss: 0.0582 | 0.0268
Epoch 198/300, seasonal_3 Loss: 0.0582 | 0.0268
Epoch 199/300, seasonal_3 Loss: 0.0581 | 0.0268
Epoch 200/300, seasonal_3 Loss: 0.0581 | 0.0267
Epoch 201/300, seasonal_3 Loss: 0.0581 | 0.0267
Epoch 202/300, seasonal_3 Loss: 0.0580 | 0.0267
Epoch 203/300, seasonal_3 Loss: 0.0580 | 0.0267
Epoch 204/300, seasonal_3 Loss: 0.0580 | 0.0267
Epoch 205/300, seasonal_3 Loss: 0.0579 | 0.0267
Epoch 206/300, seasonal_3 Loss: 0.0579 | 0.0267
Epoch 207/300, seasonal_3 Loss: 0.0579 | 0.0267
Epoch 208/300, seasonal_3 Loss: 0.0578 | 0.0267
Epoch 209/300, seasonal_3 Loss: 0.0578 | 0.0267
Epoch 210/300, seasonal_3 Loss: 0.0578 | 0.0267
Epoch 211/300, seasonal_3 Loss: 0.0578 | 0.0267
Epoch 212/300, seasonal_3 Loss: 0.0577 | 0.0267
Epoch 213/300, seasonal_3 Loss: 0.0577 | 0.0266
Epoch 214/300, seasonal_3 Loss: 0.0577 | 0.0266
Epoch 215/300, seasonal_3 Loss: 0.0577 | 0.0266
Epoch 216/300, seasonal_3 Loss: 0.0576 | 0.0266
Epoch 217/300, seasonal_3 Loss: 0.0576 | 0.0266
Epoch 218/300, seasonal_3 Loss: 0.0576 | 0.0266
Epoch 219/300, seasonal_3 Loss: 0.0575 | 0.0266
Epoch 220/300, seasonal_3 Loss: 0.0575 | 0.0266
Epoch 221/300, seasonal_3 Loss: 0.0575 | 0.0266
Epoch 222/300, seasonal_3 Loss: 0.0575 | 0.0266
Epoch 223/300, seasonal_3 Loss: 0.0574 | 0.0266
Epoch 224/300, seasonal_3 Loss: 0.0574 | 0.0266
Epoch 225/300, seasonal_3 Loss: 0.0574 | 0.0266
Epoch 226/300, seasonal_3 Loss: 0.0574 | 0.0266
Epoch 227/300, seasonal_3 Loss: 0.0573 | 0.0265
Epoch 228/300, seasonal_3 Loss: 0.0573 | 0.0265
Epoch 229/300, seasonal_3 Loss: 0.0573 | 0.0265
Epoch 230/300, seasonal_3 Loss: 0.0573 | 0.0265
Epoch 231/300, seasonal_3 Loss: 0.0573 | 0.0265
Epoch 232/300, seasonal_3 Loss: 0.0572 | 0.0265
Epoch 233/300, seasonal_3 Loss: 0.0572 | 0.0265
Epoch 234/300, seasonal_3 Loss: 0.0572 | 0.0265
Epoch 235/300, seasonal_3 Loss: 0.0572 | 0.0265
Epoch 236/300, seasonal_3 Loss: 0.0572 | 0.0265
Epoch 237/300, seasonal_3 Loss: 0.0572 | 0.0265
Epoch 238/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 239/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 240/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 241/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 242/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 243/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 244/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 245/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 246/300, seasonal_3 Loss: 0.0571 | 0.0264
Epoch 247/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 248/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 249/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 250/300, seasonal_3 Loss: 0.0571 | 0.0266
Epoch 251/300, seasonal_3 Loss: 0.0571 | 0.0266
Epoch 252/300, seasonal_3 Loss: 0.0571 | 0.0266
Epoch 253/300, seasonal_3 Loss: 0.0571 | 0.0266
Epoch 254/300, seasonal_3 Loss: 0.0571 | 0.0266
Epoch 255/300, seasonal_3 Loss: 0.0571 | 0.0266
Epoch 256/300, seasonal_3 Loss: 0.0571 | 0.0267
Epoch 257/300, seasonal_3 Loss: 0.0571 | 0.0267
Epoch 258/300, seasonal_3 Loss: 0.0571 | 0.0267
Epoch 259/300, seasonal_3 Loss: 0.0571 | 0.0268
Epoch 260/300, seasonal_3 Loss: 0.0571 | 0.0268
Epoch 261/300, seasonal_3 Loss: 0.0570 | 0.0268
Epoch 262/300, seasonal_3 Loss: 0.0570 | 0.0268
Epoch 263/300, seasonal_3 Loss: 0.0570 | 0.0267
Epoch 264/300, seasonal_3 Loss: 0.0570 | 0.0267
Epoch 265/300, seasonal_3 Loss: 0.0570 | 0.0267
Epoch 266/300, seasonal_3 Loss: 0.0570 | 0.0267
Epoch 267/300, seasonal_3 Loss: 0.0570 | 0.0267
Epoch 268/300, seasonal_3 Loss: 0.0571 | 0.0267
Epoch 269/300, seasonal_3 Loss: 0.0571 | 0.0267
Epoch 270/300, seasonal_3 Loss: 0.0572 | 0.0267
Epoch 271/300, seasonal_3 Loss: 0.0573 | 0.0266
Epoch 272/300, seasonal_3 Loss: 0.0574 | 0.0265
Epoch 273/300, seasonal_3 Loss: 0.0576 | 0.0266
Epoch 274/300, seasonal_3 Loss: 0.0579 | 0.0267
Epoch 275/300, seasonal_3 Loss: 0.0581 | 0.0268
Epoch 276/300, seasonal_3 Loss: 0.0578 | 0.0267
Epoch 277/300, seasonal_3 Loss: 0.0574 | 0.0266
Epoch 278/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 279/300, seasonal_3 Loss: 0.0569 | 0.0265
Epoch 280/300, seasonal_3 Loss: 0.0568 | 0.0265
Epoch 281/300, seasonal_3 Loss: 0.0567 | 0.0265
Epoch 282/300, seasonal_3 Loss: 0.0567 | 0.0265
Epoch 283/300, seasonal_3 Loss: 0.0567 | 0.0265
Epoch 284/300, seasonal_3 Loss: 0.0567 | 0.0265
Epoch 285/300, seasonal_3 Loss: 0.0566 | 0.0265
Epoch 286/300, seasonal_3 Loss: 0.0566 | 0.0265
Epoch 287/300, seasonal_3 Loss: 0.0565 | 0.0265
Epoch 288/300, seasonal_3 Loss: 0.0565 | 0.0265
Epoch 289/300, seasonal_3 Loss: 0.0565 | 0.0265
Epoch 290/300, seasonal_3 Loss: 0.0565 | 0.0265
Epoch 291/300, seasonal_3 Loss: 0.0565 | 0.0265
Epoch 292/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 293/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 294/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 295/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 296/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 297/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 298/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 299/300, seasonal_3 Loss: 0.0564 | 0.0265
Epoch 300/300, seasonal_3 Loss: 0.0564 | 0.0265
Training resid component with params: {'observation_period_num': 22, 'train_rates': 0.870145039266407, 'learning_rate': 0.0006221516554713113, 'batch_size': 144, 'step_size': 7, 'gamma': 0.8706786055921777}
Epoch 1/300, resid Loss: 0.4324 | 0.3102
Epoch 2/300, resid Loss: 0.2442 | 0.1735
Epoch 3/300, resid Loss: 0.2968 | 0.1853
Epoch 4/300, resid Loss: 0.1983 | 0.1055
Epoch 5/300, resid Loss: 0.1854 | 0.3148
Epoch 6/300, resid Loss: 0.1613 | 0.1103
Epoch 7/300, resid Loss: 0.1340 | 0.0904
Epoch 8/300, resid Loss: 0.1194 | 0.0767
Epoch 9/300, resid Loss: 0.1170 | 0.0700
Epoch 10/300, resid Loss: 0.1178 | 0.0695
Epoch 11/300, resid Loss: 0.1149 | 0.0730
Epoch 12/300, resid Loss: 0.1121 | 0.0702
Epoch 13/300, resid Loss: 0.1090 | 0.0570
Epoch 14/300, resid Loss: 0.1074 | 0.0575
Epoch 15/300, resid Loss: 0.1041 | 0.0597
Epoch 16/300, resid Loss: 0.1016 | 0.0588
Epoch 17/300, resid Loss: 0.1003 | 0.0577
Epoch 18/300, resid Loss: 0.1003 | 0.0572
Epoch 19/300, resid Loss: 0.1070 | 0.0576
Epoch 20/300, resid Loss: 0.1194 | 0.0720
Epoch 21/300, resid Loss: 0.1362 | 0.0749
Epoch 22/300, resid Loss: 0.1407 | 0.0990
Epoch 23/300, resid Loss: 0.1225 | 0.0818
Epoch 24/300, resid Loss: 0.1045 | 0.0956
Epoch 25/300, resid Loss: 0.0959 | 0.0510
Epoch 26/300, resid Loss: 0.0942 | 0.0526
Epoch 27/300, resid Loss: 0.0902 | 0.0527
Epoch 28/300, resid Loss: 0.0893 | 0.0526
Epoch 29/300, resid Loss: 0.0871 | 0.0549
Epoch 30/300, resid Loss: 0.0860 | 0.0560
Epoch 31/300, resid Loss: 0.0837 | 0.0488
Epoch 32/300, resid Loss: 0.0830 | 0.0482
Epoch 33/300, resid Loss: 0.0824 | 0.0478
Epoch 34/300, resid Loss: 0.0817 | 0.0508
Epoch 35/300, resid Loss: 0.0805 | 0.0484
Epoch 36/300, resid Loss: 0.0798 | 0.0461
Epoch 37/300, resid Loss: 0.0792 | 0.0452
Epoch 38/300, resid Loss: 0.0786 | 0.0453
Epoch 39/300, resid Loss: 0.0779 | 0.0446
Epoch 40/300, resid Loss: 0.0775 | 0.0448
Epoch 41/300, resid Loss: 0.0771 | 0.0441
Epoch 42/300, resid Loss: 0.0768 | 0.0437
Epoch 43/300, resid Loss: 0.0766 | 0.0431
Epoch 44/300, resid Loss: 0.0765 | 0.0427
Epoch 45/300, resid Loss: 0.0757 | 0.0422
Epoch 46/300, resid Loss: 0.0748 | 0.0419
Epoch 47/300, resid Loss: 0.0741 | 0.0417
Epoch 48/300, resid Loss: 0.0737 | 0.0415
Epoch 49/300, resid Loss: 0.0734 | 0.0413
Epoch 50/300, resid Loss: 0.0730 | 0.0409
Epoch 51/300, resid Loss: 0.0727 | 0.0404
Epoch 52/300, resid Loss: 0.0725 | 0.0402
Epoch 53/300, resid Loss: 0.0722 | 0.0401
Epoch 54/300, resid Loss: 0.0720 | 0.0399
Epoch 55/300, resid Loss: 0.0718 | 0.0396
Epoch 56/300, resid Loss: 0.0715 | 0.0394
Epoch 57/300, resid Loss: 0.0713 | 0.0391
Epoch 58/300, resid Loss: 0.0711 | 0.0389
Epoch 59/300, resid Loss: 0.0708 | 0.0387
Epoch 60/300, resid Loss: 0.0706 | 0.0385
Epoch 61/300, resid Loss: 0.0704 | 0.0384
Epoch 62/300, resid Loss: 0.0702 | 0.0382
Epoch 63/300, resid Loss: 0.0700 | 0.0381
Epoch 64/300, resid Loss: 0.0698 | 0.0379
Epoch 65/300, resid Loss: 0.0697 | 0.0378
Epoch 66/300, resid Loss: 0.0695 | 0.0377
Epoch 67/300, resid Loss: 0.0693 | 0.0376
Epoch 68/300, resid Loss: 0.0691 | 0.0375
Epoch 69/300, resid Loss: 0.0690 | 0.0374
Epoch 70/300, resid Loss: 0.0689 | 0.0373
Epoch 71/300, resid Loss: 0.0687 | 0.0372
Epoch 72/300, resid Loss: 0.0686 | 0.0371
Epoch 73/300, resid Loss: 0.0685 | 0.0370
Epoch 74/300, resid Loss: 0.0683 | 0.0369
Epoch 75/300, resid Loss: 0.0682 | 0.0368
Epoch 76/300, resid Loss: 0.0681 | 0.0367
Epoch 77/300, resid Loss: 0.0680 | 0.0366
Epoch 78/300, resid Loss: 0.0678 | 0.0366
Epoch 79/300, resid Loss: 0.0677 | 0.0365
Epoch 80/300, resid Loss: 0.0676 | 0.0364
Epoch 81/300, resid Loss: 0.0675 | 0.0363
Epoch 82/300, resid Loss: 0.0674 | 0.0363
Epoch 83/300, resid Loss: 0.0674 | 0.0362
Epoch 84/300, resid Loss: 0.0673 | 0.0362
Epoch 85/300, resid Loss: 0.0672 | 0.0361
Epoch 86/300, resid Loss: 0.0671 | 0.0361
Epoch 87/300, resid Loss: 0.0670 | 0.0360
Epoch 88/300, resid Loss: 0.0669 | 0.0360
Epoch 89/300, resid Loss: 0.0669 | 0.0359
Epoch 90/300, resid Loss: 0.0668 | 0.0359
Epoch 91/300, resid Loss: 0.0667 | 0.0358
Epoch 92/300, resid Loss: 0.0666 | 0.0358
Epoch 93/300, resid Loss: 0.0666 | 0.0358
Epoch 94/300, resid Loss: 0.0665 | 0.0357
Epoch 95/300, resid Loss: 0.0665 | 0.0357
Epoch 96/300, resid Loss: 0.0664 | 0.0357
Epoch 97/300, resid Loss: 0.0663 | 0.0356
Epoch 98/300, resid Loss: 0.0663 | 0.0356
Epoch 99/300, resid Loss: 0.0662 | 0.0356
Epoch 100/300, resid Loss: 0.0662 | 0.0355
Epoch 101/300, resid Loss: 0.0661 | 0.0355
Epoch 102/300, resid Loss: 0.0661 | 0.0355
Epoch 103/300, resid Loss: 0.0660 | 0.0355
Epoch 104/300, resid Loss: 0.0660 | 0.0354
Epoch 105/300, resid Loss: 0.0660 | 0.0354
Epoch 106/300, resid Loss: 0.0659 | 0.0354
Epoch 107/300, resid Loss: 0.0659 | 0.0354
Epoch 108/300, resid Loss: 0.0659 | 0.0353
Epoch 109/300, resid Loss: 0.0658 | 0.0353
Epoch 110/300, resid Loss: 0.0658 | 0.0353
Epoch 111/300, resid Loss: 0.0657 | 0.0353
Epoch 112/300, resid Loss: 0.0657 | 0.0353
Epoch 113/300, resid Loss: 0.0657 | 0.0353
Epoch 114/300, resid Loss: 0.0657 | 0.0352
Epoch 115/300, resid Loss: 0.0656 | 0.0352
Epoch 116/300, resid Loss: 0.0656 | 0.0352
Epoch 117/300, resid Loss: 0.0656 | 0.0352
Epoch 118/300, resid Loss: 0.0655 | 0.0352
Epoch 119/300, resid Loss: 0.0655 | 0.0352
Epoch 120/300, resid Loss: 0.0655 | 0.0352
Epoch 121/300, resid Loss: 0.0655 | 0.0352
Epoch 122/300, resid Loss: 0.0655 | 0.0351
Epoch 123/300, resid Loss: 0.0654 | 0.0351
Epoch 124/300, resid Loss: 0.0654 | 0.0351
Epoch 125/300, resid Loss: 0.0654 | 0.0351
Epoch 126/300, resid Loss: 0.0654 | 0.0351
Epoch 127/300, resid Loss: 0.0654 | 0.0351
Epoch 128/300, resid Loss: 0.0653 | 0.0351
Epoch 129/300, resid Loss: 0.0653 | 0.0351
Epoch 130/300, resid Loss: 0.0653 | 0.0351
Epoch 131/300, resid Loss: 0.0653 | 0.0351
Epoch 132/300, resid Loss: 0.0653 | 0.0350
Epoch 133/300, resid Loss: 0.0653 | 0.0350
Epoch 134/300, resid Loss: 0.0652 | 0.0350
Epoch 135/300, resid Loss: 0.0652 | 0.0350
Epoch 136/300, resid Loss: 0.0652 | 0.0350
Epoch 137/300, resid Loss: 0.0652 | 0.0350
Epoch 138/300, resid Loss: 0.0652 | 0.0350
Epoch 139/300, resid Loss: 0.0652 | 0.0350
Epoch 140/300, resid Loss: 0.0652 | 0.0350
Epoch 141/300, resid Loss: 0.0652 | 0.0350
Epoch 142/300, resid Loss: 0.0651 | 0.0350
Epoch 143/300, resid Loss: 0.0651 | 0.0350
Epoch 144/300, resid Loss: 0.0651 | 0.0350
Epoch 145/300, resid Loss: 0.0651 | 0.0350
Epoch 146/300, resid Loss: 0.0651 | 0.0350
Epoch 147/300, resid Loss: 0.0651 | 0.0350
Epoch 148/300, resid Loss: 0.0651 | 0.0350
Epoch 149/300, resid Loss: 0.0651 | 0.0349
Epoch 150/300, resid Loss: 0.0651 | 0.0349
Epoch 151/300, resid Loss: 0.0651 | 0.0349
Epoch 152/300, resid Loss: 0.0651 | 0.0349
Epoch 153/300, resid Loss: 0.0650 | 0.0349
Epoch 154/300, resid Loss: 0.0650 | 0.0349
Epoch 155/300, resid Loss: 0.0650 | 0.0349
Epoch 156/300, resid Loss: 0.0650 | 0.0349
Epoch 157/300, resid Loss: 0.0650 | 0.0349
Epoch 158/300, resid Loss: 0.0650 | 0.0349
Epoch 159/300, resid Loss: 0.0650 | 0.0349
Epoch 160/300, resid Loss: 0.0650 | 0.0349
Epoch 161/300, resid Loss: 0.0650 | 0.0349
Epoch 162/300, resid Loss: 0.0650 | 0.0349
Epoch 163/300, resid Loss: 0.0650 | 0.0349
Epoch 164/300, resid Loss: 0.0650 | 0.0349
Epoch 165/300, resid Loss: 0.0650 | 0.0349
Epoch 166/300, resid Loss: 0.0650 | 0.0349
Epoch 167/300, resid Loss: 0.0650 | 0.0349
Epoch 168/300, resid Loss: 0.0650 | 0.0349
Epoch 169/300, resid Loss: 0.0650 | 0.0349
Epoch 170/300, resid Loss: 0.0650 | 0.0349
Epoch 171/300, resid Loss: 0.0650 | 0.0349
Epoch 172/300, resid Loss: 0.0650 | 0.0349
Epoch 173/300, resid Loss: 0.0649 | 0.0349
Epoch 174/300, resid Loss: 0.0649 | 0.0349
Epoch 175/300, resid Loss: 0.0649 | 0.0349
Epoch 176/300, resid Loss: 0.0649 | 0.0349
Epoch 177/300, resid Loss: 0.0649 | 0.0349
Epoch 178/300, resid Loss: 0.0649 | 0.0349
Epoch 179/300, resid Loss: 0.0649 | 0.0349
Epoch 180/300, resid Loss: 0.0649 | 0.0349
Epoch 181/300, resid Loss: 0.0649 | 0.0349
Epoch 182/300, resid Loss: 0.0649 | 0.0349
Epoch 183/300, resid Loss: 0.0649 | 0.0349
Epoch 184/300, resid Loss: 0.0649 | 0.0349
Epoch 185/300, resid Loss: 0.0649 | 0.0349
Epoch 186/300, resid Loss: 0.0649 | 0.0349
Epoch 187/300, resid Loss: 0.0649 | 0.0349
Epoch 188/300, resid Loss: 0.0649 | 0.0349
Epoch 189/300, resid Loss: 0.0649 | 0.0349
Epoch 190/300, resid Loss: 0.0649 | 0.0349
Epoch 191/300, resid Loss: 0.0649 | 0.0349
Epoch 192/300, resid Loss: 0.0649 | 0.0349
Epoch 193/300, resid Loss: 0.0649 | 0.0349
Epoch 194/300, resid Loss: 0.0649 | 0.0349
Epoch 195/300, resid Loss: 0.0649 | 0.0349
Epoch 196/300, resid Loss: 0.0649 | 0.0349
Epoch 197/300, resid Loss: 0.0649 | 0.0348
Epoch 198/300, resid Loss: 0.0649 | 0.0348
Epoch 199/300, resid Loss: 0.0649 | 0.0348
Epoch 200/300, resid Loss: 0.0649 | 0.0348
Epoch 201/300, resid Loss: 0.0649 | 0.0348
Epoch 202/300, resid Loss: 0.0649 | 0.0348
Epoch 203/300, resid Loss: 0.0649 | 0.0348
Epoch 204/300, resid Loss: 0.0649 | 0.0348
Epoch 205/300, resid Loss: 0.0649 | 0.0348
Epoch 206/300, resid Loss: 0.0649 | 0.0348
Epoch 207/300, resid Loss: 0.0649 | 0.0348
Epoch 208/300, resid Loss: 0.0649 | 0.0348
Epoch 209/300, resid Loss: 0.0649 | 0.0348
Epoch 210/300, resid Loss: 0.0649 | 0.0348
Epoch 211/300, resid Loss: 0.0649 | 0.0348
Epoch 212/300, resid Loss: 0.0649 | 0.0348
Epoch 213/300, resid Loss: 0.0649 | 0.0348
Epoch 214/300, resid Loss: 0.0649 | 0.0348
Epoch 215/300, resid Loss: 0.0649 | 0.0348
Epoch 216/300, resid Loss: 0.0649 | 0.0348
Epoch 217/300, resid Loss: 0.0649 | 0.0348
Epoch 218/300, resid Loss: 0.0649 | 0.0348
Epoch 219/300, resid Loss: 0.0649 | 0.0348
Epoch 220/300, resid Loss: 0.0649 | 0.0348
Epoch 221/300, resid Loss: 0.0649 | 0.0348
Epoch 222/300, resid Loss: 0.0649 | 0.0348
Epoch 223/300, resid Loss: 0.0649 | 0.0348
Epoch 224/300, resid Loss: 0.0649 | 0.0348
Epoch 225/300, resid Loss: 0.0649 | 0.0348
Epoch 226/300, resid Loss: 0.0649 | 0.0348
Epoch 227/300, resid Loss: 0.0649 | 0.0348
Epoch 228/300, resid Loss: 0.0649 | 0.0348
Epoch 229/300, resid Loss: 0.0649 | 0.0348
Epoch 230/300, resid Loss: 0.0649 | 0.0348
Epoch 231/300, resid Loss: 0.0649 | 0.0348
Epoch 232/300, resid Loss: 0.0649 | 0.0348
Epoch 233/300, resid Loss: 0.0649 | 0.0348
Epoch 234/300, resid Loss: 0.0649 | 0.0348
Epoch 235/300, resid Loss: 0.0649 | 0.0348
Epoch 236/300, resid Loss: 0.0649 | 0.0348
Epoch 237/300, resid Loss: 0.0649 | 0.0348
Epoch 238/300, resid Loss: 0.0649 | 0.0348
Epoch 239/300, resid Loss: 0.0649 | 0.0348
Epoch 240/300, resid Loss: 0.0649 | 0.0348
Epoch 241/300, resid Loss: 0.0649 | 0.0348
Epoch 242/300, resid Loss: 0.0649 | 0.0348
Epoch 243/300, resid Loss: 0.0649 | 0.0348
Epoch 244/300, resid Loss: 0.0649 | 0.0348
Epoch 245/300, resid Loss: 0.0649 | 0.0348
Epoch 246/300, resid Loss: 0.0649 | 0.0348
Epoch 247/300, resid Loss: 0.0649 | 0.0348
Epoch 248/300, resid Loss: 0.0649 | 0.0348
Epoch 249/300, resid Loss: 0.0649 | 0.0348
Epoch 250/300, resid Loss: 0.0649 | 0.0348
Epoch 251/300, resid Loss: 0.0649 | 0.0348
Epoch 252/300, resid Loss: 0.0649 | 0.0348
Epoch 253/300, resid Loss: 0.0649 | 0.0348
Epoch 254/300, resid Loss: 0.0649 | 0.0348
Epoch 255/300, resid Loss: 0.0649 | 0.0348
Epoch 256/300, resid Loss: 0.0649 | 0.0348
Epoch 257/300, resid Loss: 0.0649 | 0.0348
Epoch 258/300, resid Loss: 0.0649 | 0.0348
Epoch 259/300, resid Loss: 0.0649 | 0.0348
Epoch 260/300, resid Loss: 0.0649 | 0.0348
Epoch 261/300, resid Loss: 0.0649 | 0.0348
Epoch 262/300, resid Loss: 0.0649 | 0.0348
Epoch 263/300, resid Loss: 0.0649 | 0.0348
Epoch 264/300, resid Loss: 0.0649 | 0.0348
Epoch 265/300, resid Loss: 0.0649 | 0.0348
Epoch 266/300, resid Loss: 0.0649 | 0.0348
Epoch 267/300, resid Loss: 0.0649 | 0.0348
Epoch 268/300, resid Loss: 0.0649 | 0.0348
Epoch 269/300, resid Loss: 0.0649 | 0.0348
Epoch 270/300, resid Loss: 0.0649 | 0.0348
Epoch 271/300, resid Loss: 0.0649 | 0.0348
Epoch 272/300, resid Loss: 0.0649 | 0.0348
Epoch 273/300, resid Loss: 0.0649 | 0.0348
Epoch 274/300, resid Loss: 0.0649 | 0.0348
Epoch 275/300, resid Loss: 0.0649 | 0.0348
Epoch 276/300, resid Loss: 0.0649 | 0.0348
Epoch 277/300, resid Loss: 0.0649 | 0.0348
Epoch 278/300, resid Loss: 0.0649 | 0.0348
Epoch 279/300, resid Loss: 0.0649 | 0.0348
Epoch 280/300, resid Loss: 0.0649 | 0.0348
Epoch 281/300, resid Loss: 0.0649 | 0.0348
Epoch 282/300, resid Loss: 0.0649 | 0.0348
Epoch 283/300, resid Loss: 0.0649 | 0.0348
Epoch 284/300, resid Loss: 0.0649 | 0.0348
Epoch 285/300, resid Loss: 0.0649 | 0.0348
Epoch 286/300, resid Loss: 0.0649 | 0.0348
Epoch 287/300, resid Loss: 0.0649 | 0.0348
Epoch 288/300, resid Loss: 0.0649 | 0.0348
Epoch 289/300, resid Loss: 0.0649 | 0.0348
Epoch 290/300, resid Loss: 0.0649 | 0.0348
Epoch 291/300, resid Loss: 0.0649 | 0.0348
Epoch 292/300, resid Loss: 0.0649 | 0.0348
Epoch 293/300, resid Loss: 0.0649 | 0.0348
Epoch 294/300, resid Loss: 0.0649 | 0.0348
Epoch 295/300, resid Loss: 0.0649 | 0.0348
Epoch 296/300, resid Loss: 0.0649 | 0.0348
Epoch 297/300, resid Loss: 0.0649 | 0.0348
Epoch 298/300, resid Loss: 0.0649 | 0.0348
Epoch 299/300, resid Loss: 0.0649 | 0.0348
Epoch 300/300, resid Loss: 0.0649 | 0.0348
Runtime (seconds): 1514.775316476822
0.0007281037672022916
[184.86148]
[-0.20377655]
[-1.5324805]
[0.71779424]
[-0.2696337]
[10.804294]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 50.018020218471065
RMSE: 7.0723419189453125
MAE: 7.0723419189453125
R-squared: nan
[194.37766]
