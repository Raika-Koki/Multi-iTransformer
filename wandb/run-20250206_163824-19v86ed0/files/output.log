[32m[I 2025-02-06 16:38:29,960][0m A new study created in memory with name: no-name-77d0034c-8a22-4e8b-a397-1c58cbeb3b0d[0m
[32m[I 2025-02-06 16:42:13,262][0m Trial 0 finished with value: 0.5972565077245235 and parameters: {'observation_period_num': 149, 'train_rates': 0.9296785602329184, 'learning_rate': 4.16322375515422e-06, 'batch_size': 25, 'step_size': 3, 'gamma': 0.8264533099180783}. Best is trial 0 with value: 0.5972565077245235.[0m
[32m[I 2025-02-06 16:42:40,739][0m Trial 1 finished with value: 0.20152416825294495 and parameters: {'observation_period_num': 75, 'train_rates': 0.964591387409536, 'learning_rate': 4.288156646507182e-05, 'batch_size': 240, 'step_size': 15, 'gamma': 0.8430658169636094}. Best is trial 1 with value: 0.20152416825294495.[0m
[32m[I 2025-02-06 16:43:25,833][0m Trial 2 finished with value: 0.7938386570049237 and parameters: {'observation_period_num': 184, 'train_rates': 0.7189452485277374, 'learning_rate': 1.8158289576376292e-06, 'batch_size': 112, 'step_size': 4, 'gamma': 0.9534106190880642}. Best is trial 1 with value: 0.20152416825294495.[0m
[32m[I 2025-02-06 16:44:09,357][0m Trial 3 finished with value: 0.46793514656261104 and parameters: {'observation_period_num': 209, 'train_rates': 0.684310551615232, 'learning_rate': 3.578872476756447e-05, 'batch_size': 110, 'step_size': 4, 'gamma': 0.7577474116100676}. Best is trial 1 with value: 0.20152416825294495.[0m
[32m[I 2025-02-06 16:44:45,759][0m Trial 4 finished with value: 0.6286964151427974 and parameters: {'observation_period_num': 162, 'train_rates': 0.8594135302424588, 'learning_rate': 2.5906382750625172e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8448352151625399}. Best is trial 1 with value: 0.20152416825294495.[0m
[32m[I 2025-02-06 16:46:41,800][0m Trial 5 finished with value: 0.2509818339142306 and parameters: {'observation_period_num': 187, 'train_rates': 0.6278889721336418, 'learning_rate': 0.00021640612744172356, 'batch_size': 36, 'step_size': 14, 'gamma': 0.8781514619808874}. Best is trial 1 with value: 0.20152416825294495.[0m
[32m[I 2025-02-06 16:47:19,402][0m Trial 6 finished with value: 0.14752721270999392 and parameters: {'observation_period_num': 98, 'train_rates': 0.9363002831118334, 'learning_rate': 0.00013581320301422052, 'batch_size': 163, 'step_size': 12, 'gamma': 0.8750603665918906}. Best is trial 6 with value: 0.14752721270999392.[0m
[32m[I 2025-02-06 16:47:47,298][0m Trial 7 finished with value: 0.2130902364582892 and parameters: {'observation_period_num': 248, 'train_rates': 0.8173816595859781, 'learning_rate': 0.0007650122526543212, 'batch_size': 189, 'step_size': 11, 'gamma': 0.8546839931886461}. Best is trial 6 with value: 0.14752721270999392.[0m
[32m[I 2025-02-06 16:48:15,380][0m Trial 8 finished with value: 0.15920386453469595 and parameters: {'observation_period_num': 197, 'train_rates': 0.8390246785063625, 'learning_rate': 0.00022164987762830085, 'batch_size': 212, 'step_size': 10, 'gamma': 0.7688212334991577}. Best is trial 6 with value: 0.14752721270999392.[0m
[32m[I 2025-02-06 16:48:39,895][0m Trial 9 finished with value: 0.26864621354267 and parameters: {'observation_period_num': 133, 'train_rates': 0.7203300090133912, 'learning_rate': 9.608634760740978e-05, 'batch_size': 217, 'step_size': 15, 'gamma': 0.7632472288109949}. Best is trial 6 with value: 0.14752721270999392.[0m
[32m[I 2025-02-06 16:49:24,053][0m Trial 10 finished with value: 0.10812473523396032 and parameters: {'observation_period_num': 7, 'train_rates': 0.909640726382404, 'learning_rate': 1.2075091626519933e-05, 'batch_size': 142, 'step_size': 7, 'gamma': 0.9327807344406808}. Best is trial 10 with value: 0.10812473523396032.[0m
[32m[I 2025-02-06 16:50:04,301][0m Trial 11 finished with value: 0.09760399475640674 and parameters: {'observation_period_num': 11, 'train_rates': 0.9060106143888523, 'learning_rate': 1.1812139879585132e-05, 'batch_size': 150, 'step_size': 7, 'gamma': 0.9344631011013343}. Best is trial 11 with value: 0.09760399475640674.[0m
[32m[I 2025-02-06 16:51:18,315][0m Trial 12 finished with value: 0.08857539548323705 and parameters: {'observation_period_num': 8, 'train_rates': 0.8911278208580982, 'learning_rate': 9.056369870962023e-06, 'batch_size': 80, 'step_size': 7, 'gamma': 0.9539407347605544}. Best is trial 12 with value: 0.08857539548323705.[0m
[32m[I 2025-02-06 16:52:36,207][0m Trial 13 finished with value: 0.06410866078800995 and parameters: {'observation_period_num': 7, 'train_rates': 0.8801309836845834, 'learning_rate': 9.955955885325138e-06, 'batch_size': 73, 'step_size': 7, 'gamma': 0.9793865950490562}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 16:53:48,331][0m Trial 14 finished with value: 0.2650606481000315 and parameters: {'observation_period_num': 55, 'train_rates': 0.7715682086188043, 'learning_rate': 7.72858834780349e-06, 'batch_size': 71, 'step_size': 8, 'gamma': 0.9781168692028445}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 16:55:08,211][0m Trial 15 finished with value: 0.487248187715357 and parameters: {'observation_period_num': 43, 'train_rates': 0.8733213461838616, 'learning_rate': 1.1862894469697763e-06, 'batch_size': 70, 'step_size': 1, 'gamma': 0.9834452653156961}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 16:56:34,095][0m Trial 16 finished with value: 0.47981828451156616 and parameters: {'observation_period_num': 95, 'train_rates': 0.9874267160749278, 'learning_rate': 5.669835563637598e-06, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9063494332956261}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 16:57:26,246][0m Trial 17 finished with value: 0.08057582810356342 and parameters: {'observation_period_num': 34, 'train_rates': 0.7861487937461351, 'learning_rate': 1.9388358972663655e-05, 'batch_size': 103, 'step_size': 9, 'gamma': 0.9590798515352925}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 16:58:16,647][0m Trial 18 finished with value: 0.22070491356541974 and parameters: {'observation_period_num': 40, 'train_rates': 0.7810570917247553, 'learning_rate': 2.3009380951615233e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.9139699452475099}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 17:00:01,430][0m Trial 19 finished with value: 0.2272391216486439 and parameters: {'observation_period_num': 104, 'train_rates': 0.7555828720124242, 'learning_rate': 2.3313035145503094e-05, 'batch_size': 47, 'step_size': 5, 'gamma': 0.9873443614157872}. Best is trial 13 with value: 0.06410866078800995.[0m
[32m[I 2025-02-06 17:00:57,720][0m Trial 20 finished with value: 0.05929884859956083 and parameters: {'observation_period_num': 35, 'train_rates': 0.8069192845836719, 'learning_rate': 6.64463461136886e-05, 'batch_size': 97, 'step_size': 13, 'gamma': 0.8002090146630767}. Best is trial 20 with value: 0.05929884859956083.[0m
[32m[I 2025-02-06 17:01:57,115][0m Trial 21 finished with value: 0.05492119752594771 and parameters: {'observation_period_num': 29, 'train_rates': 0.8438376385272841, 'learning_rate': 5.165698109947153e-05, 'batch_size': 95, 'step_size': 13, 'gamma': 0.8041223130043075}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:02:56,489][0m Trial 22 finished with value: 0.06417377326772156 and parameters: {'observation_period_num': 67, 'train_rates': 0.8298994014778582, 'learning_rate': 7.847186973473493e-05, 'batch_size': 93, 'step_size': 12, 'gamma': 0.797182813491285}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:03:40,158][0m Trial 23 finished with value: 0.06384053281866588 and parameters: {'observation_period_num': 20, 'train_rates': 0.8162964214757751, 'learning_rate': 6.22009936910693e-05, 'batch_size': 129, 'step_size': 13, 'gamma': 0.802759694606957}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:04:27,275][0m Trial 24 finished with value: 0.05882151076726824 and parameters: {'observation_period_num': 31, 'train_rates': 0.8195442089853748, 'learning_rate': 5.2349146670182425e-05, 'batch_size': 125, 'step_size': 13, 'gamma': 0.803015137357532}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:05:08,236][0m Trial 25 finished with value: 0.17885282465177982 and parameters: {'observation_period_num': 69, 'train_rates': 0.7249784925932243, 'learning_rate': 0.0003562982262486573, 'batch_size': 128, 'step_size': 13, 'gamma': 0.7929662891029501}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:05:43,063][0m Trial 26 finished with value: 0.07061374581408227 and parameters: {'observation_period_num': 31, 'train_rates': 0.8534129273130405, 'learning_rate': 5.10265212543832e-05, 'batch_size': 175, 'step_size': 14, 'gamma': 0.8250379557092481}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:06:25,719][0m Trial 27 finished with value: 0.19013477654396732 and parameters: {'observation_period_num': 55, 'train_rates': 0.75792993541244, 'learning_rate': 0.0001294854781340179, 'batch_size': 124, 'step_size': 13, 'gamma': 0.7853483963744776}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:08:11,348][0m Trial 28 finished with value: 0.11990491353574541 and parameters: {'observation_period_num': 85, 'train_rates': 0.8034036837877838, 'learning_rate': 0.0004494678055942409, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8160105127755863}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:09:06,508][0m Trial 29 finished with value: 0.29182380692222726 and parameters: {'observation_period_num': 113, 'train_rates': 0.6704148527802716, 'learning_rate': 3.2816226683528447e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.7796793997256793}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:13:33,903][0m Trial 30 finished with value: 0.16743003781223056 and parameters: {'observation_period_num': 123, 'train_rates': 0.9313359677346436, 'learning_rate': 0.00011776073904714603, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8131860737592665}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:14:19,667][0m Trial 31 finished with value: 0.05760361463025739 and parameters: {'observation_period_num': 18, 'train_rates': 0.8196928597208342, 'learning_rate': 7.00377978092826e-05, 'batch_size': 127, 'step_size': 13, 'gamma': 0.8033799025543507}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:15:06,006][0m Trial 32 finished with value: 0.07184439265652548 and parameters: {'observation_period_num': 25, 'train_rates': 0.8405631686175588, 'learning_rate': 5.52834619313108e-05, 'batch_size': 120, 'step_size': 15, 'gamma': 0.8392092022740582}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:16:02,006][0m Trial 33 finished with value: 0.06002372946061684 and parameters: {'observation_period_num': 58, 'train_rates': 0.8030586774108661, 'learning_rate': 0.0001790606773098543, 'batch_size': 95, 'step_size': 10, 'gamma': 0.8289081977932754}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:16:39,665][0m Trial 34 finished with value: 0.19755632405050416 and parameters: {'observation_period_num': 43, 'train_rates': 0.745328895071041, 'learning_rate': 7.701100650587003e-05, 'batch_size': 139, 'step_size': 13, 'gamma': 0.7502434303640029}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:18:22,812][0m Trial 35 finished with value: 0.07590478386443394 and parameters: {'observation_period_num': 79, 'train_rates': 0.8574338294735075, 'learning_rate': 3.5780095010126624e-05, 'batch_size': 53, 'step_size': 15, 'gamma': 0.775939323167673}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:19:11,498][0m Trial 36 finished with value: 0.10473792730284132 and parameters: {'observation_period_num': 21, 'train_rates': 0.7964151383533901, 'learning_rate': 1.6307924923908288e-05, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8093746113345328}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:20:01,446][0m Trial 37 finished with value: 0.30283516018347584 and parameters: {'observation_period_num': 49, 'train_rates': 0.6802752539223931, 'learning_rate': 4.1972021497569286e-05, 'batch_size': 100, 'step_size': 12, 'gamma': 0.8533371464062318}. Best is trial 21 with value: 0.05492119752594771.[0m
[32m[I 2025-02-06 17:20:36,414][0m Trial 38 finished with value: 0.04838442433949398 and parameters: {'observation_period_num': 25, 'train_rates': 0.8140685913947732, 'learning_rate': 0.0002652313066302423, 'batch_size': 166, 'step_size': 10, 'gamma': 0.8257775270816251}. Best is trial 38 with value: 0.04838442433949398.[0m
[32m[I 2025-02-06 17:21:08,619][0m Trial 39 finished with value: 0.1942612571441684 and parameters: {'observation_period_num': 161, 'train_rates': 0.8292646748886939, 'learning_rate': 0.0008115914502132467, 'batch_size': 173, 'step_size': 10, 'gamma': 0.8670012675887171}. Best is trial 38 with value: 0.04838442433949398.[0m
[32m[I 2025-02-06 17:21:32,082][0m Trial 40 finished with value: 0.26327235834487295 and parameters: {'observation_period_num': 241, 'train_rates': 0.6028604358137656, 'learning_rate': 0.0003082360295239774, 'batch_size': 189, 'step_size': 11, 'gamma': 0.828033278870301}. Best is trial 38 with value: 0.04838442433949398.[0m
[32m[I 2025-02-06 17:22:08,854][0m Trial 41 finished with value: 0.04654396392071425 and parameters: {'observation_period_num': 25, 'train_rates': 0.8195730499084914, 'learning_rate': 0.00019950111899676994, 'batch_size': 152, 'step_size': 13, 'gamma': 0.7953486630125577}. Best is trial 41 with value: 0.04654396392071425.[0m
[32m[I 2025-02-06 17:22:48,076][0m Trial 42 finished with value: 0.05220619504424659 and parameters: {'observation_period_num': 21, 'train_rates': 0.8522205688270621, 'learning_rate': 0.000179996735328421, 'batch_size': 156, 'step_size': 12, 'gamma': 0.786758702658282}. Best is trial 41 with value: 0.04654396392071425.[0m
[32m[I 2025-02-06 17:23:24,025][0m Trial 43 finished with value: 0.04667806781561118 and parameters: {'observation_period_num': 21, 'train_rates': 0.8500510706742974, 'learning_rate': 0.0005837023838570264, 'batch_size': 162, 'step_size': 11, 'gamma': 0.7860323256494762}. Best is trial 41 with value: 0.04654396392071425.[0m
[32m[I 2025-02-06 17:24:02,322][0m Trial 44 finished with value: 0.0361695183106722 and parameters: {'observation_period_num': 20, 'train_rates': 0.868955627702269, 'learning_rate': 0.0005201423412176905, 'batch_size': 155, 'step_size': 10, 'gamma': 0.7831770588197294}. Best is trial 44 with value: 0.0361695183106722.[0m
[32m[I 2025-02-06 17:24:40,141][0m Trial 45 finished with value: 0.03632923598687556 and parameters: {'observation_period_num': 17, 'train_rates': 0.9051593631478831, 'learning_rate': 0.000571103762456618, 'batch_size': 162, 'step_size': 9, 'gamma': 0.7668392547897811}. Best is trial 44 with value: 0.0361695183106722.[0m
[32m[I 2025-02-06 17:25:13,154][0m Trial 46 finished with value: 0.043935686349868774 and parameters: {'observation_period_num': 5, 'train_rates': 0.9572620576493992, 'learning_rate': 0.0005619777516696368, 'batch_size': 196, 'step_size': 9, 'gamma': 0.7655607264670098}. Best is trial 44 with value: 0.0361695183106722.[0m
[32m[I 2025-02-06 17:25:40,384][0m Trial 47 finished with value: 0.046371545642614365 and parameters: {'observation_period_num': 9, 'train_rates': 0.9507085572732465, 'learning_rate': 0.0005564591766183925, 'batch_size': 255, 'step_size': 9, 'gamma': 0.7648921183091543}. Best is trial 44 with value: 0.0361695183106722.[0m
[32m[I 2025-02-06 17:26:07,745][0m Trial 48 finished with value: 0.044426895678043365 and parameters: {'observation_period_num': 6, 'train_rates': 0.9439893224709914, 'learning_rate': 0.0009621762085829513, 'batch_size': 254, 'step_size': 8, 'gamma': 0.7642794779048026}. Best is trial 44 with value: 0.0361695183106722.[0m
[32m[I 2025-02-06 17:26:35,006][0m Trial 49 finished with value: 0.046925123780965805 and parameters: {'observation_period_num': 5, 'train_rates': 0.9517556148346793, 'learning_rate': 0.000993248168188081, 'batch_size': 247, 'step_size': 8, 'gamma': 0.7600477194154349}. Best is trial 44 with value: 0.0361695183106722.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.5400 | 0.2936
Epoch 2/300, Loss: 0.2245 | 0.2477
Epoch 3/300, Loss: 0.1913 | 0.2344
Epoch 4/300, Loss: 0.2289 | 0.4809
Epoch 5/300, Loss: 0.1935 | 0.3596
Epoch 6/300, Loss: 0.1875 | 0.1612
Epoch 7/300, Loss: 0.2002 | 0.2168
Epoch 8/300, Loss: 0.1636 | 0.1401
Epoch 9/300, Loss: 0.1616 | 0.1540
Epoch 10/300, Loss: 0.1625 | 0.1007
Epoch 11/300, Loss: 0.1366 | 0.0944
Epoch 12/300, Loss: 0.1167 | 0.1134
Epoch 13/300, Loss: 0.1324 | 0.0971
Epoch 14/300, Loss: 0.1233 | 0.1033
Epoch 15/300, Loss: 0.1147 | 0.0864
Epoch 16/300, Loss: 0.1088 | 0.0934
Epoch 17/300, Loss: 0.1087 | 0.0776
Epoch 18/300, Loss: 0.1041 | 0.0713
Epoch 19/300, Loss: 0.0975 | 0.0704
Epoch 20/300, Loss: 0.0967 | 0.0691
Epoch 21/300, Loss: 0.0949 | 0.0661
Epoch 22/300, Loss: 0.0930 | 0.0730
Epoch 23/300, Loss: 0.0919 | 0.0745
Epoch 24/300, Loss: 0.0911 | 0.0633
Epoch 25/300, Loss: 0.0896 | 0.0652
Epoch 26/300, Loss: 0.0887 | 0.0626
Epoch 27/300, Loss: 0.0882 | 0.0618
Epoch 28/300, Loss: 0.0874 | 0.0626
Epoch 29/300, Loss: 0.0867 | 0.0593
Epoch 30/300, Loss: 0.0861 | 0.0599
Epoch 31/300, Loss: 0.0858 | 0.0600
Epoch 32/300, Loss: 0.0854 | 0.0579
Epoch 33/300, Loss: 0.0849 | 0.0579
Epoch 34/300, Loss: 0.0844 | 0.0572
Epoch 35/300, Loss: 0.0839 | 0.0567
Epoch 36/300, Loss: 0.0835 | 0.0566
Epoch 37/300, Loss: 0.0832 | 0.0563
Epoch 38/300, Loss: 0.0829 | 0.0560
Epoch 39/300, Loss: 0.0826 | 0.0557
Epoch 40/300, Loss: 0.0823 | 0.0555
Epoch 41/300, Loss: 0.0820 | 0.0553
Epoch 42/300, Loss: 0.0817 | 0.0551
Epoch 43/300, Loss: 0.0814 | 0.0548
Epoch 44/300, Loss: 0.0812 | 0.0546
Epoch 45/300, Loss: 0.0809 | 0.0544
Epoch 46/300, Loss: 0.0806 | 0.0543
Epoch 47/300, Loss: 0.0804 | 0.0541
Epoch 48/300, Loss: 0.0801 | 0.0539
Epoch 49/300, Loss: 0.0799 | 0.0538
Epoch 50/300, Loss: 0.0797 | 0.0536
Epoch 51/300, Loss: 0.0794 | 0.0535
Epoch 52/300, Loss: 0.0792 | 0.0533
Epoch 53/300, Loss: 0.0790 | 0.0532
Epoch 54/300, Loss: 0.0788 | 0.0530
Epoch 55/300, Loss: 0.0786 | 0.0529
Epoch 56/300, Loss: 0.0784 | 0.0527
Epoch 57/300, Loss: 0.0782 | 0.0526
Epoch 58/300, Loss: 0.0780 | 0.0525
Epoch 59/300, Loss: 0.0778 | 0.0523
Epoch 60/300, Loss: 0.0777 | 0.0522
Epoch 61/300, Loss: 0.0775 | 0.0521
Epoch 62/300, Loss: 0.0773 | 0.0520
Epoch 63/300, Loss: 0.0772 | 0.0518
Epoch 64/300, Loss: 0.0770 | 0.0517
Epoch 65/300, Loss: 0.0769 | 0.0516
Epoch 66/300, Loss: 0.0767 | 0.0515
Epoch 67/300, Loss: 0.0766 | 0.0514
Epoch 68/300, Loss: 0.0765 | 0.0513
Epoch 69/300, Loss: 0.0763 | 0.0512
Epoch 70/300, Loss: 0.0762 | 0.0510
Epoch 71/300, Loss: 0.0761 | 0.0509
Epoch 72/300, Loss: 0.0760 | 0.0508
Epoch 73/300, Loss: 0.0759 | 0.0508
Epoch 74/300, Loss: 0.0758 | 0.0507
Epoch 75/300, Loss: 0.0757 | 0.0506
Epoch 76/300, Loss: 0.0756 | 0.0505
Epoch 77/300, Loss: 0.0755 | 0.0504
Epoch 78/300, Loss: 0.0754 | 0.0503
Epoch 79/300, Loss: 0.0753 | 0.0502
Epoch 80/300, Loss: 0.0752 | 0.0501
Epoch 81/300, Loss: 0.0751 | 0.0501
Epoch 82/300, Loss: 0.0751 | 0.0500
Epoch 83/300, Loss: 0.0750 | 0.0499
Epoch 84/300, Loss: 0.0749 | 0.0499
Epoch 85/300, Loss: 0.0749 | 0.0498
Epoch 86/300, Loss: 0.0748 | 0.0497
Epoch 87/300, Loss: 0.0747 | 0.0497
Epoch 88/300, Loss: 0.0747 | 0.0496
Epoch 89/300, Loss: 0.0746 | 0.0496
Epoch 90/300, Loss: 0.0746 | 0.0495
Epoch 91/300, Loss: 0.0745 | 0.0495
Epoch 92/300, Loss: 0.0745 | 0.0494
Epoch 93/300, Loss: 0.0744 | 0.0494
Epoch 94/300, Loss: 0.0744 | 0.0493
Epoch 95/300, Loss: 0.0744 | 0.0493
Epoch 96/300, Loss: 0.0743 | 0.0492
Epoch 97/300, Loss: 0.0743 | 0.0492
Epoch 98/300, Loss: 0.0742 | 0.0492
Epoch 99/300, Loss: 0.0742 | 0.0491
Epoch 100/300, Loss: 0.0742 | 0.0491
Epoch 101/300, Loss: 0.0741 | 0.0490
Epoch 102/300, Loss: 0.0741 | 0.0490
Epoch 103/300, Loss: 0.0741 | 0.0490
Epoch 104/300, Loss: 0.0741 | 0.0490
Epoch 105/300, Loss: 0.0740 | 0.0489
Epoch 106/300, Loss: 0.0740 | 0.0489
Epoch 107/300, Loss: 0.0740 | 0.0489
Epoch 108/300, Loss: 0.0740 | 0.0489
Epoch 109/300, Loss: 0.0739 | 0.0488
Epoch 110/300, Loss: 0.0739 | 0.0488
Epoch 111/300, Loss: 0.0739 | 0.0488
Epoch 112/300, Loss: 0.0739 | 0.0488
Epoch 113/300, Loss: 0.0739 | 0.0487
Epoch 114/300, Loss: 0.0738 | 0.0487
Epoch 115/300, Loss: 0.0738 | 0.0487
Epoch 116/300, Loss: 0.0738 | 0.0487
Epoch 117/300, Loss: 0.0738 | 0.0487
Epoch 118/300, Loss: 0.0738 | 0.0487
Epoch 119/300, Loss: 0.0738 | 0.0486
Epoch 120/300, Loss: 0.0737 | 0.0486
Epoch 121/300, Loss: 0.0737 | 0.0486
Epoch 122/300, Loss: 0.0737 | 0.0486
Epoch 123/300, Loss: 0.0737 | 0.0486
Epoch 124/300, Loss: 0.0737 | 0.0486
Epoch 125/300, Loss: 0.0737 | 0.0486
Epoch 126/300, Loss: 0.0737 | 0.0486
Epoch 127/300, Loss: 0.0737 | 0.0485
Epoch 128/300, Loss: 0.0737 | 0.0485
Epoch 129/300, Loss: 0.0737 | 0.0485
Epoch 130/300, Loss: 0.0736 | 0.0485
Epoch 131/300, Loss: 0.0736 | 0.0485
Epoch 132/300, Loss: 0.0736 | 0.0485
Epoch 133/300, Loss: 0.0736 | 0.0485
Epoch 134/300, Loss: 0.0736 | 0.0485
Epoch 135/300, Loss: 0.0736 | 0.0485
Epoch 136/300, Loss: 0.0736 | 0.0485
Epoch 137/300, Loss: 0.0736 | 0.0485
Epoch 138/300, Loss: 0.0736 | 0.0485
Epoch 139/300, Loss: 0.0736 | 0.0484
Epoch 140/300, Loss: 0.0736 | 0.0484
Epoch 141/300, Loss: 0.0736 | 0.0484
Epoch 142/300, Loss: 0.0736 | 0.0484
Epoch 143/300, Loss: 0.0736 | 0.0484
Epoch 144/300, Loss: 0.0736 | 0.0484
Epoch 145/300, Loss: 0.0736 | 0.0484
Epoch 146/300, Loss: 0.0736 | 0.0484
Epoch 147/300, Loss: 0.0735 | 0.0484
Epoch 148/300, Loss: 0.0735 | 0.0484
Epoch 149/300, Loss: 0.0735 | 0.0484
Epoch 150/300, Loss: 0.0735 | 0.0484
Epoch 151/300, Loss: 0.0735 | 0.0484
Epoch 152/300, Loss: 0.0735 | 0.0484
Epoch 153/300, Loss: 0.0735 | 0.0484
Epoch 154/300, Loss: 0.0735 | 0.0484
Epoch 155/300, Loss: 0.0735 | 0.0484
Epoch 156/300, Loss: 0.0735 | 0.0484
Epoch 157/300, Loss: 0.0735 | 0.0484
Epoch 158/300, Loss: 0.0735 | 0.0484
Epoch 159/300, Loss: 0.0735 | 0.0484
Epoch 160/300, Loss: 0.0735 | 0.0484
Epoch 161/300, Loss: 0.0735 | 0.0484
Epoch 162/300, Loss: 0.0735 | 0.0484
Epoch 163/300, Loss: 0.0735 | 0.0484
Epoch 164/300, Loss: 0.0735 | 0.0484
Epoch 165/300, Loss: 0.0735 | 0.0484
Epoch 166/300, Loss: 0.0735 | 0.0484
Epoch 167/300, Loss: 0.0735 | 0.0484
Epoch 168/300, Loss: 0.0735 | 0.0484
Epoch 169/300, Loss: 0.0735 | 0.0483
Epoch 170/300, Loss: 0.0735 | 0.0483
Epoch 171/300, Loss: 0.0735 | 0.0483
Epoch 172/300, Loss: 0.0735 | 0.0483
Epoch 173/300, Loss: 0.0735 | 0.0483
Epoch 174/300, Loss: 0.0735 | 0.0483
Epoch 175/300, Loss: 0.0735 | 0.0483
Epoch 176/300, Loss: 0.0735 | 0.0483
Epoch 177/300, Loss: 0.0735 | 0.0483
Epoch 178/300, Loss: 0.0735 | 0.0483
Epoch 179/300, Loss: 0.0735 | 0.0483
Epoch 180/300, Loss: 0.0735 | 0.0483
Epoch 181/300, Loss: 0.0735 | 0.0483
Epoch 182/300, Loss: 0.0735 | 0.0483
Epoch 183/300, Loss: 0.0735 | 0.0483
Epoch 184/300, Loss: 0.0735 | 0.0483
Epoch 185/300, Loss: 0.0735 | 0.0483
Epoch 186/300, Loss: 0.0735 | 0.0483
Epoch 187/300, Loss: 0.0735 | 0.0483
Epoch 188/300, Loss: 0.0735 | 0.0483
Epoch 189/300, Loss: 0.0735 | 0.0483
Epoch 190/300, Loss: 0.0735 | 0.0483
Epoch 191/300, Loss: 0.0735 | 0.0483
Epoch 192/300, Loss: 0.0735 | 0.0483
Epoch 193/300, Loss: 0.0735 | 0.0483
Epoch 194/300, Loss: 0.0735 | 0.0483
Epoch 195/300, Loss: 0.0735 | 0.0483
Epoch 196/300, Loss: 0.0735 | 0.0483
Epoch 197/300, Loss: 0.0735 | 0.0483
Epoch 198/300, Loss: 0.0735 | 0.0483
Epoch 199/300, Loss: 0.0735 | 0.0483
Epoch 200/300, Loss: 0.0735 | 0.0483
Epoch 201/300, Loss: 0.0735 | 0.0483
Epoch 202/300, Loss: 0.0735 | 0.0483
Epoch 203/300, Loss: 0.0735 | 0.0483
Epoch 204/300, Loss: 0.0735 | 0.0483
Epoch 205/300, Loss: 0.0735 | 0.0483
Epoch 206/300, Loss: 0.0735 | 0.0483
Epoch 207/300, Loss: 0.0735 | 0.0483
Epoch 208/300, Loss: 0.0735 | 0.0483
Epoch 209/300, Loss: 0.0735 | 0.0483
Epoch 210/300, Loss: 0.0735 | 0.0483
Epoch 211/300, Loss: 0.0735 | 0.0483
Epoch 212/300, Loss: 0.0735 | 0.0483
Epoch 213/300, Loss: 0.0735 | 0.0483
Epoch 214/300, Loss: 0.0735 | 0.0483
Epoch 215/300, Loss: 0.0735 | 0.0483
Epoch 216/300, Loss: 0.0735 | 0.0483
Epoch 217/300, Loss: 0.0735 | 0.0483
Epoch 218/300, Loss: 0.0735 | 0.0483
Epoch 219/300, Loss: 0.0735 | 0.0483
Epoch 220/300, Loss: 0.0735 | 0.0483
Epoch 221/300, Loss: 0.0735 | 0.0483
Epoch 222/300, Loss: 0.0735 | 0.0483
Epoch 223/300, Loss: 0.0735 | 0.0483
Epoch 224/300, Loss: 0.0735 | 0.0483
Epoch 225/300, Loss: 0.0735 | 0.0483
Epoch 226/300, Loss: 0.0735 | 0.0483
Epoch 227/300, Loss: 0.0735 | 0.0483
Epoch 228/300, Loss: 0.0735 | 0.0483
Epoch 229/300, Loss: 0.0735 | 0.0483
Epoch 230/300, Loss: 0.0735 | 0.0483
Epoch 231/300, Loss: 0.0735 | 0.0483
Epoch 232/300, Loss: 0.0735 | 0.0483
Epoch 233/300, Loss: 0.0735 | 0.0483
Epoch 234/300, Loss: 0.0735 | 0.0483
Epoch 235/300, Loss: 0.0735 | 0.0483
Epoch 236/300, Loss: 0.0735 | 0.0483
Epoch 237/300, Loss: 0.0735 | 0.0483
Epoch 238/300, Loss: 0.0735 | 0.0483
Epoch 239/300, Loss: 0.0735 | 0.0483
Epoch 240/300, Loss: 0.0735 | 0.0483
Epoch 241/300, Loss: 0.0735 | 0.0483
Epoch 242/300, Loss: 0.0735 | 0.0483
Epoch 243/300, Loss: 0.0735 | 0.0483
Epoch 244/300, Loss: 0.0735 | 0.0483
Epoch 245/300, Loss: 0.0735 | 0.0483
Epoch 246/300, Loss: 0.0735 | 0.0483
Epoch 247/300, Loss: 0.0735 | 0.0483
Epoch 248/300, Loss: 0.0735 | 0.0483
Epoch 249/300, Loss: 0.0735 | 0.0483
Epoch 250/300, Loss: 0.0735 | 0.0483
Epoch 251/300, Loss: 0.0735 | 0.0483
Epoch 252/300, Loss: 0.0735 | 0.0483
Epoch 253/300, Loss: 0.0735 | 0.0483
Epoch 254/300, Loss: 0.0735 | 0.0483
Epoch 255/300, Loss: 0.0735 | 0.0483
Epoch 256/300, Loss: 0.0735 | 0.0483
Epoch 257/300, Loss: 0.0735 | 0.0483
Epoch 258/300, Loss: 0.0735 | 0.0483
Epoch 259/300, Loss: 0.0735 | 0.0483
Epoch 260/300, Loss: 0.0735 | 0.0483
Epoch 261/300, Loss: 0.0735 | 0.0483
Epoch 262/300, Loss: 0.0735 | 0.0483
Epoch 263/300, Loss: 0.0735 | 0.0483
Epoch 264/300, Loss: 0.0735 | 0.0483
Epoch 265/300, Loss: 0.0735 | 0.0483
Epoch 266/300, Loss: 0.0735 | 0.0483
Early stopping
Runtime (seconds): 103.37141799926758
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 22.152121249586344
RMSE: 4.70660400390625
MAE: 4.70660400390625
R-squared: nan
[201.77661]
