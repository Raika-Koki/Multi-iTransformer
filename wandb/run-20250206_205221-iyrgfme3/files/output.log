[32m[I 2025-02-06 20:52:26,851][0m A new study created in memory with name: no-name-f46912b6-e574-43db-8fc3-32260f6aa538[0m
Early stopping at epoch 45
[32m[I 2025-02-06 20:52:38,078][0m Trial 0 finished with value: 2.2809574842453 and parameters: {'observation_period_num': 87, 'train_rates': 0.8557096048097868, 'learning_rate': 4.402590526111392e-06, 'batch_size': 252, 'step_size': 1, 'gamma': 0.7931576045114734}. Best is trial 0 with value: 2.2809574842453.[0m
[32m[I 2025-02-06 20:53:15,702][0m Trial 1 finished with value: 0.18176235258579254 and parameters: {'observation_period_num': 158, 'train_rates': 0.9497058847047977, 'learning_rate': 0.0003360911104682546, 'batch_size': 166, 'step_size': 6, 'gamma': 0.7644836309644265}. Best is trial 1 with value: 0.18176235258579254.[0m
[32m[I 2025-02-06 20:54:49,219][0m Trial 2 finished with value: 0.9602377794005654 and parameters: {'observation_period_num': 103, 'train_rates': 0.977283879620693, 'learning_rate': 2.0797198296570003e-06, 'batch_size': 63, 'step_size': 2, 'gamma': 0.9125379180882576}. Best is trial 1 with value: 0.18176235258579254.[0m
[32m[I 2025-02-06 20:55:14,987][0m Trial 3 finished with value: 0.06886354230039625 and parameters: {'observation_period_num': 21, 'train_rates': 0.909711426652678, 'learning_rate': 6.501754026114e-05, 'batch_size': 249, 'step_size': 15, 'gamma': 0.9013812959436845}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 20:56:13,407][0m Trial 4 finished with value: 0.21528142569016437 and parameters: {'observation_period_num': 233, 'train_rates': 0.8050538640921472, 'learning_rate': 0.0002944432105969435, 'batch_size': 88, 'step_size': 2, 'gamma': 0.8275318949195056}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 20:57:20,418][0m Trial 5 finished with value: 0.14828411801294847 and parameters: {'observation_period_num': 186, 'train_rates': 0.9219412718837912, 'learning_rate': 0.00041770209514437653, 'batch_size': 84, 'step_size': 1, 'gamma': 0.9352390225867806}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 20:58:01,077][0m Trial 6 finished with value: 0.23055345356464385 and parameters: {'observation_period_num': 180, 'train_rates': 0.8225599993327715, 'learning_rate': 0.0009376040632211607, 'batch_size': 130, 'step_size': 12, 'gamma': 0.898044297087591}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 20:58:28,877][0m Trial 7 finished with value: 0.21795207354021662 and parameters: {'observation_period_num': 69, 'train_rates': 0.8341260976671314, 'learning_rate': 2.103746304256178e-05, 'batch_size': 210, 'step_size': 13, 'gamma': 0.7878415788288015}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 20:59:46,406][0m Trial 8 finished with value: 0.21231850541156272 and parameters: {'observation_period_num': 88, 'train_rates': 0.9606871207777667, 'learning_rate': 1.7113264471670865e-05, 'batch_size': 77, 'step_size': 11, 'gamma': 0.8131541479821676}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 21:00:39,503][0m Trial 9 finished with value: 0.14558530139923095 and parameters: {'observation_period_num': 39, 'train_rates': 0.8310578919533678, 'learning_rate': 1.1995054314460853e-05, 'batch_size': 104, 'step_size': 6, 'gamma': 0.9236523938366556}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 21:00:59,910][0m Trial 10 finished with value: 0.17293737071294527 and parameters: {'observation_period_num': 31, 'train_rates': 0.6751490861714446, 'learning_rate': 6.73144397340223e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9857374983912179}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 21:01:35,769][0m Trial 11 finished with value: 0.1691167520493576 and parameters: {'observation_period_num': 9, 'train_rates': 0.707845716706087, 'learning_rate': 7.306849446956014e-05, 'batch_size': 144, 'step_size': 7, 'gamma': 0.8607227438443423}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 21:02:04,522][0m Trial 12 finished with value: 0.4197965215178246 and parameters: {'observation_period_num': 42, 'train_rates': 0.7456117388480245, 'learning_rate': 7.68046623727583e-06, 'batch_size': 184, 'step_size': 9, 'gamma': 0.963129675983063}. Best is trial 3 with value: 0.06886354230039625.[0m
[32m[I 2025-02-06 21:05:36,428][0m Trial 13 finished with value: 0.03020005232104562 and parameters: {'observation_period_num': 10, 'train_rates': 0.8871598166993102, 'learning_rate': 8.455370154195582e-05, 'batch_size': 27, 'step_size': 5, 'gamma': 0.8729879767580977}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:09:05,286][0m Trial 14 finished with value: 0.03252152818898114 and parameters: {'observation_period_num': 10, 'train_rates': 0.8918309388624921, 'learning_rate': 7.88372476189481e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8656740021368475}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:11:39,902][0m Trial 15 finished with value: 0.35029341910211814 and parameters: {'observation_period_num': 126, 'train_rates': 0.6048828968229135, 'learning_rate': 0.00010577417510832212, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8727996871017718}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:15:50,746][0m Trial 16 finished with value: 0.062646886527273 and parameters: {'observation_period_num': 58, 'train_rates': 0.8854896229463965, 'learning_rate': 3.7628889115980906e-05, 'batch_size': 22, 'step_size': 4, 'gamma': 0.8561578270027203}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:17:55,924][0m Trial 17 finished with value: 0.16258381826276333 and parameters: {'observation_period_num': 7, 'train_rates': 0.7705510763067847, 'learning_rate': 0.0001336370394436274, 'batch_size': 41, 'step_size': 4, 'gamma': 0.8431340867072443}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:19:35,184][0m Trial 18 finished with value: 0.2520583770306952 and parameters: {'observation_period_num': 238, 'train_rates': 0.877957705854681, 'learning_rate': 0.00019464357697465348, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8842598436067165}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:25:22,129][0m Trial 19 finished with value: 0.0936026504194295 and parameters: {'observation_period_num': 122, 'train_rates': 0.9249577817813317, 'learning_rate': 3.670117537020958e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9491543029214362}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:26:13,350][0m Trial 20 finished with value: 1.7862931282623955 and parameters: {'observation_period_num': 53, 'train_rates': 0.8751371417018461, 'learning_rate': 1.0317430977750863e-06, 'batch_size': 112, 'step_size': 9, 'gamma': 0.832491996374987}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:28:32,529][0m Trial 21 finished with value: 0.1378142469629799 and parameters: {'observation_period_num': 67, 'train_rates': 0.9009503322703005, 'learning_rate': 3.05341547086379e-05, 'batch_size': 40, 'step_size': 3, 'gamma': 0.8570455755377774}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:33:39,653][0m Trial 22 finished with value: 0.033222472890833306 and parameters: {'observation_period_num': 6, 'train_rates': 0.8684102668409008, 'learning_rate': 4.614799522113427e-05, 'batch_size': 18, 'step_size': 4, 'gamma': 0.879095182511282}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:35:21,163][0m Trial 23 finished with value: 0.044532913983696035 and parameters: {'observation_period_num': 24, 'train_rates': 0.8485510609737503, 'learning_rate': 0.00015993541268045783, 'batch_size': 55, 'step_size': 6, 'gamma': 0.887957247072444}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:37:44,100][0m Trial 24 finished with value: 0.18454535423140778 and parameters: {'observation_period_num': 6, 'train_rates': 0.7712484622178202, 'learning_rate': 5.4547264945248146e-05, 'batch_size': 36, 'step_size': 3, 'gamma': 0.8744499902635466}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:39:13,169][0m Trial 25 finished with value: 0.062159495784881266 and parameters: {'observation_period_num': 38, 'train_rates': 0.9367021627277949, 'learning_rate': 0.0009079431555035946, 'batch_size': 67, 'step_size': 5, 'gamma': 0.8172821518204878}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:44:32,744][0m Trial 26 finished with value: 0.08902607154169544 and parameters: {'observation_period_num': 81, 'train_rates': 0.7945943054604591, 'learning_rate': 1.0777846543624595e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8447538587019405}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:46:37,326][0m Trial 27 finished with value: 0.11520135402679443 and parameters: {'observation_period_num': 26, 'train_rates': 0.9880732164336467, 'learning_rate': 2.732746251956875e-05, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9128431170473427}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:47:36,788][0m Trial 28 finished with value: 0.06447024261693896 and parameters: {'observation_period_num': 54, 'train_rates': 0.8626151092664344, 'learning_rate': 8.892212427628224e-05, 'batch_size': 95, 'step_size': 5, 'gamma': 0.8858141403667935}. Best is trial 13 with value: 0.03020005232104562.[0m
Early stopping at epoch 50
[32m[I 2025-02-06 21:48:57,473][0m Trial 29 finished with value: 0.19877617511638376 and parameters: {'observation_period_num': 147, 'train_rates': 0.894375041595388, 'learning_rate': 0.00021228811869199824, 'batch_size': 35, 'step_size': 1, 'gamma': 0.7504436643970112}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:49:46,836][0m Trial 30 finished with value: 0.4268178044692844 and parameters: {'observation_period_num': 93, 'train_rates': 0.8543254199049861, 'learning_rate': 6.680971870741935e-06, 'batch_size': 113, 'step_size': 7, 'gamma': 0.802589630511446}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:51:27,723][0m Trial 31 finished with value: 0.03934449716362842 and parameters: {'observation_period_num': 19, 'train_rates': 0.8555244259932374, 'learning_rate': 0.00015969135301292285, 'batch_size': 56, 'step_size': 6, 'gamma': 0.8909707235006106}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:54:16,467][0m Trial 32 finished with value: 0.042561896364478505 and parameters: {'observation_period_num': 5, 'train_rates': 0.8579564872368288, 'learning_rate': 4.622330852452155e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.866548002565606}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:56:00,536][0m Trial 33 finished with value: 0.0438175318046258 and parameters: {'observation_period_num': 22, 'train_rates': 0.9562931444347123, 'learning_rate': 0.0005667806672104797, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9033656818440035}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:57:20,255][0m Trial 34 finished with value: 0.06220948237102576 and parameters: {'observation_period_num': 43, 'train_rates': 0.9095952800880838, 'learning_rate': 0.00011163597627561121, 'batch_size': 73, 'step_size': 2, 'gamma': 0.9270472685750095}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 21:59:20,122][0m Trial 35 finished with value: 0.036091056816718155 and parameters: {'observation_period_num': 17, 'train_rates': 0.8116629343800558, 'learning_rate': 0.00015717642754961653, 'batch_size': 44, 'step_size': 6, 'gamma': 0.8451282686048808}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:01:29,637][0m Trial 36 finished with value: 0.03870448291301727 and parameters: {'observation_period_num': 17, 'train_rates': 0.8072149585062138, 'learning_rate': 0.00030444163223805414, 'batch_size': 41, 'step_size': 4, 'gamma': 0.8406990313341702}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:05:01,650][0m Trial 37 finished with value: 0.0702038537258074 and parameters: {'observation_period_num': 71, 'train_rates': 0.7890503911974343, 'learning_rate': 0.00024160527209732404, 'batch_size': 24, 'step_size': 3, 'gamma': 0.7851045017153844}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:05:27,848][0m Trial 38 finished with value: 0.1646776466513984 and parameters: {'observation_period_num': 108, 'train_rates': 0.8227456980615617, 'learning_rate': 0.00046261520918796886, 'batch_size': 226, 'step_size': 9, 'gamma': 0.8753649819540529}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:06:06,046][0m Trial 39 finished with value: 0.28091387867927553 and parameters: {'observation_period_num': 205, 'train_rates': 0.9373777562715327, 'learning_rate': 7.963385477685618e-05, 'batch_size': 161, 'step_size': 6, 'gamma': 0.8245519636137633}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:07:18,791][0m Trial 40 finished with value: 0.2501062098719658 and parameters: {'observation_period_num': 35, 'train_rates': 0.7424404510895661, 'learning_rate': 4.9922583442960334e-05, 'batch_size': 70, 'step_size': 2, 'gamma': 0.8495815836408595}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:09:17,562][0m Trial 41 finished with value: 0.03234525204182946 and parameters: {'observation_period_num': 15, 'train_rates': 0.8117935218634663, 'learning_rate': 0.00032883295557100425, 'batch_size': 45, 'step_size': 4, 'gamma': 0.8381589953419868}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:11:20,360][0m Trial 42 finished with value: 0.07474289135177536 and parameters: {'observation_period_num': 19, 'train_rates': 0.8306283940220871, 'learning_rate': 0.00036286838222510787, 'batch_size': 44, 'step_size': 5, 'gamma': 0.806539725079934}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:14:27,509][0m Trial 43 finished with value: 0.05677674917925619 and parameters: {'observation_period_num': 31, 'train_rates': 0.8047632405196393, 'learning_rate': 0.000598420311376989, 'batch_size': 28, 'step_size': 4, 'gamma': 0.8351049706687269}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:15:27,780][0m Trial 44 finished with value: 0.09589764287159679 and parameters: {'observation_period_num': 44, 'train_rates': 0.84235537585088, 'learning_rate': 1.883987420268484e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.9041629731432274}. Best is trial 13 with value: 0.03020005232104562.[0m
Early stopping at epoch 91
[32m[I 2025-02-06 22:16:31,759][0m Trial 45 finished with value: 0.09146237192848294 and parameters: {'observation_period_num': 15, 'train_rates': 0.870075241444175, 'learning_rate': 0.00010733109154662764, 'batch_size': 82, 'step_size': 1, 'gamma': 0.8615226857236529}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:18:23,559][0m Trial 46 finished with value: 0.06646396085949234 and parameters: {'observation_period_num': 55, 'train_rates': 0.8155832309774828, 'learning_rate': 0.0002498092202392252, 'batch_size': 47, 'step_size': 3, 'gamma': 0.8253408700236163}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:23:24,956][0m Trial 47 finished with value: 0.1592751204183961 and parameters: {'observation_period_num': 5, 'train_rates': 0.776817180898462, 'learning_rate': 0.00014753098015513055, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8516331220597396}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:26:09,002][0m Trial 48 finished with value: 0.30563197716286306 and parameters: {'observation_period_num': 249, 'train_rates': 0.7442999793997674, 'learning_rate': 6.576415194918219e-05, 'batch_size': 28, 'step_size': 4, 'gamma': 0.8787202195761006}. Best is trial 13 with value: 0.03020005232104562.[0m
[32m[I 2025-02-06 22:26:53,487][0m Trial 49 finished with value: 0.057106378674507144 and parameters: {'observation_period_num': 29, 'train_rates': 0.8889995470892775, 'learning_rate': 9.133103001390423e-05, 'batch_size': 132, 'step_size': 6, 'gamma': 0.8675058427672903}. Best is trial 13 with value: 0.03020005232104562.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3554 | 0.2179
Epoch 2/300, Loss: 0.1461 | 0.1606
Epoch 3/300, Loss: 0.1267 | 0.1317
Epoch 4/300, Loss: 0.1157 | 0.0959
Epoch 5/300, Loss: 0.1103 | 0.0812
Epoch 6/300, Loss: 0.1070 | 0.0773
Epoch 7/300, Loss: 0.1055 | 0.0743
Epoch 8/300, Loss: 0.1031 | 0.0723
Epoch 9/300, Loss: 0.1009 | 0.0674
Epoch 10/300, Loss: 0.0994 | 0.0658
Epoch 11/300, Loss: 0.0969 | 0.0617
Epoch 12/300, Loss: 0.0950 | 0.0608
Epoch 13/300, Loss: 0.0929 | 0.0600
Epoch 14/300, Loss: 0.0908 | 0.0575
Epoch 15/300, Loss: 0.0890 | 0.0567
Epoch 16/300, Loss: 0.0873 | 0.0554
Epoch 17/300, Loss: 0.0859 | 0.0546
Epoch 18/300, Loss: 0.0848 | 0.0537
Epoch 19/300, Loss: 0.0836 | 0.0529
Epoch 20/300, Loss: 0.0827 | 0.0518
Epoch 21/300, Loss: 0.0818 | 0.0505
Epoch 22/300, Loss: 0.0810 | 0.0493
Epoch 23/300, Loss: 0.0803 | 0.0483
Epoch 24/300, Loss: 0.0796 | 0.0473
Epoch 25/300, Loss: 0.0789 | 0.0465
Epoch 26/300, Loss: 0.0783 | 0.0456
Epoch 27/300, Loss: 0.0778 | 0.0449
Epoch 28/300, Loss: 0.0773 | 0.0441
Epoch 29/300, Loss: 0.0768 | 0.0434
Epoch 30/300, Loss: 0.0764 | 0.0428
Epoch 31/300, Loss: 0.0760 | 0.0423
Epoch 32/300, Loss: 0.0756 | 0.0418
Epoch 33/300, Loss: 0.0753 | 0.0413
Epoch 34/300, Loss: 0.0749 | 0.0409
Epoch 35/300, Loss: 0.0747 | 0.0405
Epoch 36/300, Loss: 0.0744 | 0.0402
Epoch 37/300, Loss: 0.0741 | 0.0400
Epoch 38/300, Loss: 0.0739 | 0.0397
Epoch 39/300, Loss: 0.0736 | 0.0395
Epoch 40/300, Loss: 0.0734 | 0.0393
Epoch 41/300, Loss: 0.0732 | 0.0393
Epoch 42/300, Loss: 0.0730 | 0.0391
Epoch 43/300, Loss: 0.0728 | 0.0389
Epoch 44/300, Loss: 0.0727 | 0.0389
Epoch 45/300, Loss: 0.0725 | 0.0388
Epoch 46/300, Loss: 0.0723 | 0.0387
Epoch 47/300, Loss: 0.0722 | 0.0386
Epoch 48/300, Loss: 0.0721 | 0.0386
Epoch 49/300, Loss: 0.0719 | 0.0384
Epoch 50/300, Loss: 0.0718 | 0.0383
Epoch 51/300, Loss: 0.0717 | 0.0381
Epoch 52/300, Loss: 0.0716 | 0.0380
Epoch 53/300, Loss: 0.0715 | 0.0380
Epoch 54/300, Loss: 0.0713 | 0.0377
Epoch 55/300, Loss: 0.0713 | 0.0377
Epoch 56/300, Loss: 0.0712 | 0.0374
Epoch 57/300, Loss: 0.0711 | 0.0373
Epoch 58/300, Loss: 0.0710 | 0.0373
Epoch 59/300, Loss: 0.0710 | 0.0370
Epoch 60/300, Loss: 0.0709 | 0.0369
Epoch 61/300, Loss: 0.0708 | 0.0368
Epoch 62/300, Loss: 0.0708 | 0.0367
Epoch 63/300, Loss: 0.0707 | 0.0367
Epoch 64/300, Loss: 0.0707 | 0.0367
Epoch 65/300, Loss: 0.0706 | 0.0366
Epoch 66/300, Loss: 0.0706 | 0.0366
Epoch 67/300, Loss: 0.0705 | 0.0366
Epoch 68/300, Loss: 0.0705 | 0.0365
Epoch 69/300, Loss: 0.0704 | 0.0365
Epoch 70/300, Loss: 0.0704 | 0.0365
Epoch 71/300, Loss: 0.0704 | 0.0364
Epoch 72/300, Loss: 0.0703 | 0.0364
Epoch 73/300, Loss: 0.0703 | 0.0364
Epoch 74/300, Loss: 0.0703 | 0.0364
Epoch 75/300, Loss: 0.0702 | 0.0364
Epoch 76/300, Loss: 0.0702 | 0.0364
Epoch 77/300, Loss: 0.0702 | 0.0363
Epoch 78/300, Loss: 0.0702 | 0.0363
Epoch 79/300, Loss: 0.0701 | 0.0363
Epoch 80/300, Loss: 0.0701 | 0.0363
Epoch 81/300, Loss: 0.0701 | 0.0363
Epoch 82/300, Loss: 0.0700 | 0.0363
Epoch 83/300, Loss: 0.0700 | 0.0362
Epoch 84/300, Loss: 0.0700 | 0.0362
Epoch 85/300, Loss: 0.0700 | 0.0362
Epoch 86/300, Loss: 0.0699 | 0.0362
Epoch 87/300, Loss: 0.0699 | 0.0362
Epoch 88/300, Loss: 0.0699 | 0.0362
Epoch 89/300, Loss: 0.0699 | 0.0361
Epoch 90/300, Loss: 0.0698 | 0.0361
Epoch 91/300, Loss: 0.0698 | 0.0361
Epoch 92/300, Loss: 0.0698 | 0.0361
Epoch 93/300, Loss: 0.0698 | 0.0361
Epoch 94/300, Loss: 0.0697 | 0.0361
Epoch 95/300, Loss: 0.0697 | 0.0361
Epoch 96/300, Loss: 0.0697 | 0.0361
Epoch 97/300, Loss: 0.0697 | 0.0361
Epoch 98/300, Loss: 0.0697 | 0.0361
Epoch 99/300, Loss: 0.0696 | 0.0361
Epoch 100/300, Loss: 0.0696 | 0.0361
Epoch 101/300, Loss: 0.0696 | 0.0361
Epoch 102/300, Loss: 0.0696 | 0.0361
Epoch 103/300, Loss: 0.0696 | 0.0361
Epoch 104/300, Loss: 0.0696 | 0.0361
Epoch 105/300, Loss: 0.0696 | 0.0361
Epoch 106/300, Loss: 0.0695 | 0.0361
Epoch 107/300, Loss: 0.0695 | 0.0360
Epoch 108/300, Loss: 0.0695 | 0.0360
Epoch 109/300, Loss: 0.0695 | 0.0360
Epoch 110/300, Loss: 0.0695 | 0.0360
Epoch 111/300, Loss: 0.0695 | 0.0360
Epoch 112/300, Loss: 0.0695 | 0.0360
Epoch 113/300, Loss: 0.0695 | 0.0360
Epoch 114/300, Loss: 0.0695 | 0.0360
Epoch 115/300, Loss: 0.0695 | 0.0360
Epoch 116/300, Loss: 0.0695 | 0.0360
Epoch 117/300, Loss: 0.0695 | 0.0360
Epoch 118/300, Loss: 0.0695 | 0.0360
Epoch 119/300, Loss: 0.0695 | 0.0360
Epoch 120/300, Loss: 0.0695 | 0.0360
Epoch 121/300, Loss: 0.0695 | 0.0360
Epoch 122/300, Loss: 0.0695 | 0.0360
Epoch 123/300, Loss: 0.0695 | 0.0360
Epoch 124/300, Loss: 0.0695 | 0.0360
Epoch 125/300, Loss: 0.0694 | 0.0360
Epoch 126/300, Loss: 0.0694 | 0.0360
Epoch 127/300, Loss: 0.0694 | 0.0360
Epoch 128/300, Loss: 0.0694 | 0.0360
Epoch 129/300, Loss: 0.0694 | 0.0360
Epoch 130/300, Loss: 0.0694 | 0.0360
Epoch 131/300, Loss: 0.0694 | 0.0360
Epoch 132/300, Loss: 0.0694 | 0.0360
Epoch 133/300, Loss: 0.0694 | 0.0360
Epoch 134/300, Loss: 0.0694 | 0.0360
Epoch 135/300, Loss: 0.0694 | 0.0360
Epoch 136/300, Loss: 0.0694 | 0.0360
Epoch 137/300, Loss: 0.0694 | 0.0360
Epoch 138/300, Loss: 0.0694 | 0.0360
Epoch 139/300, Loss: 0.0694 | 0.0360
Epoch 140/300, Loss: 0.0694 | 0.0360
Epoch 141/300, Loss: 0.0694 | 0.0360
Epoch 142/300, Loss: 0.0694 | 0.0360
Epoch 143/300, Loss: 0.0694 | 0.0360
Epoch 144/300, Loss: 0.0694 | 0.0360
Epoch 145/300, Loss: 0.0694 | 0.0360
Epoch 146/300, Loss: 0.0694 | 0.0360
Epoch 147/300, Loss: 0.0694 | 0.0360
Epoch 148/300, Loss: 0.0694 | 0.0360
Epoch 149/300, Loss: 0.0694 | 0.0360
Epoch 150/300, Loss: 0.0694 | 0.0360
Epoch 151/300, Loss: 0.0694 | 0.0360
Epoch 152/300, Loss: 0.0694 | 0.0360
Epoch 153/300, Loss: 0.0694 | 0.0360
Epoch 154/300, Loss: 0.0694 | 0.0360
Epoch 155/300, Loss: 0.0694 | 0.0360
Epoch 156/300, Loss: 0.0694 | 0.0360
Epoch 157/300, Loss: 0.0694 | 0.0360
Epoch 158/300, Loss: 0.0694 | 0.0360
Epoch 159/300, Loss: 0.0694 | 0.0360
Epoch 160/300, Loss: 0.0694 | 0.0360
Epoch 161/300, Loss: 0.0694 | 0.0360
Epoch 162/300, Loss: 0.0694 | 0.0360
Epoch 163/300, Loss: 0.0694 | 0.0360
Epoch 164/300, Loss: 0.0694 | 0.0360
Epoch 165/300, Loss: 0.0694 | 0.0360
Epoch 166/300, Loss: 0.0694 | 0.0360
Epoch 167/300, Loss: 0.0694 | 0.0360
Epoch 168/300, Loss: 0.0694 | 0.0360
Epoch 169/300, Loss: 0.0694 | 0.0360
Epoch 170/300, Loss: 0.0694 | 0.0360
Epoch 171/300, Loss: 0.0694 | 0.0360
Epoch 172/300, Loss: 0.0694 | 0.0360
Epoch 173/300, Loss: 0.0694 | 0.0360
Epoch 174/300, Loss: 0.0694 | 0.0360
Epoch 175/300, Loss: 0.0694 | 0.0360
Epoch 176/300, Loss: 0.0694 | 0.0360
Epoch 177/300, Loss: 0.0694 | 0.0360
Epoch 178/300, Loss: 0.0694 | 0.0360
Epoch 179/300, Loss: 0.0694 | 0.0360
Epoch 180/300, Loss: 0.0694 | 0.0360
Epoch 181/300, Loss: 0.0694 | 0.0360
Epoch 182/300, Loss: 0.0694 | 0.0360
Epoch 183/300, Loss: 0.0694 | 0.0360
Epoch 184/300, Loss: 0.0694 | 0.0360
Epoch 185/300, Loss: 0.0694 | 0.0360
Epoch 186/300, Loss: 0.0694 | 0.0360
Epoch 187/300, Loss: 0.0694 | 0.0360
Epoch 188/300, Loss: 0.0694 | 0.0360
Epoch 189/300, Loss: 0.0694 | 0.0360
Epoch 190/300, Loss: 0.0694 | 0.0360
Epoch 191/300, Loss: 0.0694 | 0.0360
Epoch 192/300, Loss: 0.0694 | 0.0360
Epoch 193/300, Loss: 0.0694 | 0.0360
Epoch 194/300, Loss: 0.0694 | 0.0360
Epoch 195/300, Loss: 0.0694 | 0.0360
Epoch 196/300, Loss: 0.0694 | 0.0360
Epoch 197/300, Loss: 0.0694 | 0.0360
Epoch 198/300, Loss: 0.0694 | 0.0360
Epoch 199/300, Loss: 0.0694 | 0.0360
Epoch 200/300, Loss: 0.0694 | 0.0360
Epoch 201/300, Loss: 0.0694 | 0.0360
Epoch 202/300, Loss: 0.0694 | 0.0360
Epoch 203/300, Loss: 0.0694 | 0.0360
Epoch 204/300, Loss: 0.0694 | 0.0360
Epoch 205/300, Loss: 0.0694 | 0.0360
Epoch 206/300, Loss: 0.0694 | 0.0360
Epoch 207/300, Loss: 0.0694 | 0.0360
Epoch 208/300, Loss: 0.0694 | 0.0360
Epoch 209/300, Loss: 0.0694 | 0.0360
Epoch 210/300, Loss: 0.0694 | 0.0360
Epoch 211/300, Loss: 0.0694 | 0.0360
Epoch 212/300, Loss: 0.0694 | 0.0360
Epoch 213/300, Loss: 0.0694 | 0.0360
Epoch 214/300, Loss: 0.0694 | 0.0360
Epoch 215/300, Loss: 0.0694 | 0.0360
Epoch 216/300, Loss: 0.0694 | 0.0360
Epoch 217/300, Loss: 0.0694 | 0.0360
Epoch 218/300, Loss: 0.0694 | 0.0360
Epoch 219/300, Loss: 0.0694 | 0.0360
Epoch 220/300, Loss: 0.0694 | 0.0360
Epoch 221/300, Loss: 0.0694 | 0.0360
Epoch 222/300, Loss: 0.0694 | 0.0360
Epoch 223/300, Loss: 0.0694 | 0.0360
Epoch 224/300, Loss: 0.0694 | 0.0360
Epoch 225/300, Loss: 0.0694 | 0.0360
Epoch 226/300, Loss: 0.0694 | 0.0360
Epoch 227/300, Loss: 0.0694 | 0.0360
Epoch 228/300, Loss: 0.0694 | 0.0360
Epoch 229/300, Loss: 0.0694 | 0.0360
Epoch 230/300, Loss: 0.0694 | 0.0360
Epoch 231/300, Loss: 0.0694 | 0.0360
Epoch 232/300, Loss: 0.0694 | 0.0360
Epoch 233/300, Loss: 0.0694 | 0.0360
Epoch 234/300, Loss: 0.0694 | 0.0360
Epoch 235/300, Loss: 0.0694 | 0.0360
Epoch 236/300, Loss: 0.0694 | 0.0360
Epoch 237/300, Loss: 0.0694 | 0.0360
Epoch 238/300, Loss: 0.0694 | 0.0360
Epoch 239/300, Loss: 0.0694 | 0.0360
Epoch 240/300, Loss: 0.0694 | 0.0360
Epoch 241/300, Loss: 0.0694 | 0.0360
Epoch 242/300, Loss: 0.0694 | 0.0360
Epoch 243/300, Loss: 0.0694 | 0.0360
Epoch 244/300, Loss: 0.0694 | 0.0360
Epoch 245/300, Loss: 0.0694 | 0.0360
Epoch 246/300, Loss: 0.0694 | 0.0360
Epoch 247/300, Loss: 0.0694 | 0.0360
Epoch 248/300, Loss: 0.0694 | 0.0360
Epoch 249/300, Loss: 0.0694 | 0.0360
Epoch 250/300, Loss: 0.0694 | 0.0360
Epoch 251/300, Loss: 0.0694 | 0.0360
Epoch 252/300, Loss: 0.0694 | 0.0360
Epoch 253/300, Loss: 0.0694 | 0.0360
Epoch 254/300, Loss: 0.0694 | 0.0360
Epoch 255/300, Loss: 0.0694 | 0.0360
Epoch 256/300, Loss: 0.0694 | 0.0360
Epoch 257/300, Loss: 0.0694 | 0.0360
Epoch 258/300, Loss: 0.0694 | 0.0360
Epoch 259/300, Loss: 0.0694 | 0.0360
Epoch 260/300, Loss: 0.0694 | 0.0360
Epoch 261/300, Loss: 0.0694 | 0.0360
Epoch 262/300, Loss: 0.0694 | 0.0360
Epoch 263/300, Loss: 0.0694 | 0.0360
Epoch 264/300, Loss: 0.0694 | 0.0360
Epoch 265/300, Loss: 0.0694 | 0.0360
Epoch 266/300, Loss: 0.0694 | 0.0360
Epoch 267/300, Loss: 0.0694 | 0.0360
Epoch 268/300, Loss: 0.0694 | 0.0360
Epoch 269/300, Loss: 0.0694 | 0.0360
Epoch 270/300, Loss: 0.0694 | 0.0360
Epoch 271/300, Loss: 0.0694 | 0.0360
Epoch 272/300, Loss: 0.0694 | 0.0360
Epoch 273/300, Loss: 0.0694 | 0.0360
Epoch 274/300, Loss: 0.0694 | 0.0360
Epoch 275/300, Loss: 0.0694 | 0.0360
Epoch 276/300, Loss: 0.0694 | 0.0360
Epoch 277/300, Loss: 0.0694 | 0.0360
Epoch 278/300, Loss: 0.0694 | 0.0360
Epoch 279/300, Loss: 0.0694 | 0.0360
Epoch 280/300, Loss: 0.0694 | 0.0360
Epoch 281/300, Loss: 0.0694 | 0.0360
Epoch 282/300, Loss: 0.0694 | 0.0360
Epoch 283/300, Loss: 0.0694 | 0.0360
Epoch 284/300, Loss: 0.0694 | 0.0360
Epoch 285/300, Loss: 0.0694 | 0.0360
Epoch 286/300, Loss: 0.0694 | 0.0360
Epoch 287/300, Loss: 0.0694 | 0.0360
Epoch 288/300, Loss: 0.0694 | 0.0360
Epoch 289/300, Loss: 0.0694 | 0.0360
Epoch 290/300, Loss: 0.0694 | 0.0360
Epoch 291/300, Loss: 0.0694 | 0.0360
Epoch 292/300, Loss: 0.0694 | 0.0360
Epoch 293/300, Loss: 0.0694 | 0.0360
Epoch 294/300, Loss: 0.0694 | 0.0360
Epoch 295/300, Loss: 0.0694 | 0.0360
Epoch 296/300, Loss: 0.0694 | 0.0360
Epoch 297/300, Loss: 0.0694 | 0.0360
Epoch 298/300, Loss: 0.0694 | 0.0360
Epoch 299/300, Loss: 0.0694 | 0.0360
Epoch 300/300, Loss: 0.0694 | 0.0360
Runtime (seconds): 634.7430830001831
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 90.59300717804581
RMSE: 9.518035888671875
MAE: 9.518035888671875
R-squared: nan
[196.08197]
