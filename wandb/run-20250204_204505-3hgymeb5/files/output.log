[32m[I 2025-02-04 20:45:09,888][0m A new study created in memory with name: no-name-d4c04c07-e7bf-4fc5-aa56-30cfcee02c87[0m
Early stopping at epoch 58
[32m[I 2025-02-04 20:45:24,086][0m Trial 0 finished with value: 1.1144529410517163 and parameters: {'observation_period_num': 136, 'train_rates': 0.7619287503575671, 'learning_rate': 1.7863671818828449e-06, 'batch_size': 246, 'step_size': 1, 'gamma': 0.8411518601840356}. Best is trial 0 with value: 1.1144529410517163.[0m
[32m[I 2025-02-04 20:46:00,726][0m Trial 1 finished with value: 0.18404179754165503 and parameters: {'observation_period_num': 126, 'train_rates': 0.8623114221652434, 'learning_rate': 3.866248942951441e-05, 'batch_size': 162, 'step_size': 6, 'gamma': 0.9230614659605785}. Best is trial 1 with value: 0.18404179754165503.[0m
[32m[I 2025-02-04 20:46:36,653][0m Trial 2 finished with value: 0.9703934621239471 and parameters: {'observation_period_num': 128, 'train_rates': 0.7343495765479056, 'learning_rate': 1.0443421362863868e-06, 'batch_size': 141, 'step_size': 5, 'gamma': 0.8258878184140405}. Best is trial 1 with value: 0.18404179754165503.[0m
[32m[I 2025-02-04 20:47:02,021][0m Trial 3 finished with value: 0.17318323043595374 and parameters: {'observation_period_num': 99, 'train_rates': 0.8020855576185151, 'learning_rate': 2.86337650549452e-05, 'batch_size': 245, 'step_size': 15, 'gamma': 0.8305828072332228}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:47:29,631][0m Trial 4 finished with value: 0.3741177310866694 and parameters: {'observation_period_num': 202, 'train_rates': 0.7072278501735124, 'learning_rate': 9.581628310282302e-05, 'batch_size': 182, 'step_size': 5, 'gamma': 0.7971136111964794}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:49:15,772][0m Trial 5 finished with value: 0.6165693915356714 and parameters: {'observation_period_num': 77, 'train_rates': 0.6181230058400662, 'learning_rate': 1.1391784185707874e-06, 'batch_size': 41, 'step_size': 10, 'gamma': 0.8656286572081673}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:51:08,749][0m Trial 6 finished with value: 0.22199849577332662 and parameters: {'observation_period_num': 217, 'train_rates': 0.8133424008910157, 'learning_rate': 7.664479554446257e-05, 'batch_size': 44, 'step_size': 1, 'gamma': 0.9172394172771006}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:52:17,962][0m Trial 7 finished with value: 0.20977691862034206 and parameters: {'observation_period_num': 103, 'train_rates': 0.6894480430456655, 'learning_rate': 0.00010209805150032263, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9550664941941501}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:52:47,756][0m Trial 8 finished with value: 0.28527352560518515 and parameters: {'observation_period_num': 186, 'train_rates': 0.8959877640038956, 'learning_rate': 0.00017036079827025983, 'batch_size': 193, 'step_size': 9, 'gamma': 0.8801539354161793}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:53:39,788][0m Trial 9 finished with value: 0.9530214959002555 and parameters: {'observation_period_num': 22, 'train_rates': 0.8076691140493975, 'learning_rate': 1.109730219336738e-06, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8448966009909762}. Best is trial 3 with value: 0.17318323043595374.[0m
[32m[I 2025-02-04 20:54:07,203][0m Trial 10 finished with value: 0.03673688322305679 and parameters: {'observation_period_num': 8, 'train_rates': 0.9818612340267477, 'learning_rate': 0.0008805888731010292, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7591001962270778}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:54:34,402][0m Trial 11 finished with value: 0.04155346006155014 and parameters: {'observation_period_num': 5, 'train_rates': 0.9829958573043774, 'learning_rate': 0.000703786928044051, 'batch_size': 253, 'step_size': 15, 'gamma': 0.7511351916645019}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:55:06,369][0m Trial 12 finished with value: 0.049219660460948944 and parameters: {'observation_period_num': 26, 'train_rates': 0.9885952284587554, 'learning_rate': 0.0009623006429265137, 'batch_size': 216, 'step_size': 15, 'gamma': 0.7576888883701925}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:55:33,760][0m Trial 13 finished with value: 0.03821747004985809 and parameters: {'observation_period_num': 6, 'train_rates': 0.9834431770462064, 'learning_rate': 0.000865964188843894, 'batch_size': 252, 'step_size': 13, 'gamma': 0.754200298332349}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:56:03,376][0m Trial 14 finished with value: 0.06788830964937123 and parameters: {'observation_period_num': 49, 'train_rates': 0.9252497285278396, 'learning_rate': 0.00039446213377605305, 'batch_size': 216, 'step_size': 13, 'gamma': 0.7910172340216881}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:56:33,456][0m Trial 15 finished with value: 0.5205054470937546 and parameters: {'observation_period_num': 60, 'train_rates': 0.9244450228055769, 'learning_rate': 9.143455137096946e-06, 'batch_size': 214, 'step_size': 12, 'gamma': 0.7845815009543707}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:57:30,606][0m Trial 16 finished with value: 0.05031652161377972 and parameters: {'observation_period_num': 43, 'train_rates': 0.9393369409021216, 'learning_rate': 0.00028646749240728334, 'batch_size': 108, 'step_size': 13, 'gamma': 0.7707658014413203}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:57:56,285][0m Trial 17 finished with value: 0.20349733259366906 and parameters: {'observation_period_num': 249, 'train_rates': 0.8724738866049024, 'learning_rate': 0.0004772032971237294, 'batch_size': 228, 'step_size': 12, 'gamma': 0.8059492560620665}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:58:29,929][0m Trial 18 finished with value: 0.16675372421741486 and parameters: {'observation_period_num': 14, 'train_rates': 0.9579735448880297, 'learning_rate': 9.089910223575398e-06, 'batch_size': 194, 'step_size': 7, 'gamma': 0.9893717103144023}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:59:06,569][0m Trial 19 finished with value: 0.13113016685596982 and parameters: {'observation_period_num': 169, 'train_rates': 0.8567802436260555, 'learning_rate': 0.0001946049960412067, 'batch_size': 153, 'step_size': 13, 'gamma': 0.7734363186274422}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 20:59:58,060][0m Trial 20 finished with value: 0.0716744065544331 and parameters: {'observation_period_num': 78, 'train_rates': 0.9004922971273885, 'learning_rate': 0.0008916331513587922, 'batch_size': 116, 'step_size': 3, 'gamma': 0.8807820697929323}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:00:26,713][0m Trial 21 finished with value: 0.03829008713364601 and parameters: {'observation_period_num': 6, 'train_rates': 0.9781069106393936, 'learning_rate': 0.0005893337092723761, 'batch_size': 252, 'step_size': 15, 'gamma': 0.7528246709859285}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:00:53,577][0m Trial 22 finished with value: 0.07028378546237946 and parameters: {'observation_period_num': 33, 'train_rates': 0.961204424299127, 'learning_rate': 0.000448726300590338, 'batch_size': 255, 'step_size': 14, 'gamma': 0.7524523977705743}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:01:23,459][0m Trial 23 finished with value: 0.0899338349699974 and parameters: {'observation_period_num': 56, 'train_rates': 0.9821426159560072, 'learning_rate': 0.00021708576262310963, 'batch_size': 232, 'step_size': 12, 'gamma': 0.8114487887364314}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:01:59,173][0m Trial 24 finished with value: 0.03731220215559006 and parameters: {'observation_period_num': 6, 'train_rates': 0.9472139817313387, 'learning_rate': 0.0004953560519846386, 'batch_size': 175, 'step_size': 14, 'gamma': 0.7742964829267902}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:02:34,940][0m Trial 25 finished with value: 0.04526892676949501 and parameters: {'observation_period_num': 30, 'train_rates': 0.9405769937832769, 'learning_rate': 0.00036698205211590276, 'batch_size': 178, 'step_size': 11, 'gamma': 0.776954479740118}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:03:06,583][0m Trial 26 finished with value: 0.10694109106605704 and parameters: {'observation_period_num': 73, 'train_rates': 0.9083893591985868, 'learning_rate': 0.0008664288387274272, 'batch_size': 200, 'step_size': 14, 'gamma': 0.8085118309436617}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:03:36,459][0m Trial 27 finished with value: 0.10216650366783142 and parameters: {'observation_period_num': 40, 'train_rates': 0.9504864594664624, 'learning_rate': 0.00014035873943474682, 'batch_size': 229, 'step_size': 8, 'gamma': 0.7701014049915665}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:04:43,225][0m Trial 28 finished with value: 0.05846660368536648 and parameters: {'observation_period_num': 7, 'train_rates': 0.8324627223131851, 'learning_rate': 3.499584604890773e-05, 'batch_size': 84, 'step_size': 14, 'gamma': 0.7872522006192608}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:05:14,344][0m Trial 29 finished with value: 0.2554661240428686 and parameters: {'observation_period_num': 159, 'train_rates': 0.759784426542712, 'learning_rate': 0.0002522743911088337, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8499499693081699}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:05:59,219][0m Trial 30 finished with value: 0.2441093025883836 and parameters: {'observation_period_num': 90, 'train_rates': 0.8784207254088887, 'learning_rate': 1.2622178816202302e-05, 'batch_size': 130, 'step_size': 11, 'gamma': 0.8219692111241926}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:06:26,427][0m Trial 31 finished with value: 0.03975871205329895 and parameters: {'observation_period_num': 18, 'train_rates': 0.9687434156173621, 'learning_rate': 0.0005750600987737339, 'batch_size': 241, 'step_size': 15, 'gamma': 0.7629986037169522}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:06:57,625][0m Trial 32 finished with value: 0.3184363543987274 and parameters: {'observation_period_num': 8, 'train_rates': 0.9872171066789833, 'learning_rate': 3.839672332016686e-06, 'batch_size': 209, 'step_size': 14, 'gamma': 0.7500639482277723}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:12:51,234][0m Trial 33 finished with value: 0.046117569641633466 and parameters: {'observation_period_num': 32, 'train_rates': 0.9250209587580399, 'learning_rate': 0.000547098731173714, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7660575741093654}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:13:18,745][0m Trial 34 finished with value: 0.10290294885635376 and parameters: {'observation_period_num': 66, 'train_rates': 0.9632441580603274, 'learning_rate': 0.0003204334317040317, 'batch_size': 237, 'step_size': 14, 'gamma': 0.7841293960940573}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:13:44,106][0m Trial 35 finished with value: 0.11684683710336685 and parameters: {'observation_period_num': 46, 'train_rates': 0.9374188462585874, 'learning_rate': 6.342303338424261e-05, 'batch_size': 255, 'step_size': 12, 'gamma': 0.7987108454419188}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:14:05,377][0m Trial 36 finished with value: 0.24315133449193593 and parameters: {'observation_period_num': 111, 'train_rates': 0.60969263543123, 'learning_rate': 0.000624733100450593, 'batch_size': 225, 'step_size': 15, 'gamma': 0.7638451131391453}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:14:26,467][0m Trial 37 finished with value: 0.14843203841277366 and parameters: {'observation_period_num': 20, 'train_rates': 0.6496086175228517, 'learning_rate': 0.000958796020402112, 'batch_size': 242, 'step_size': 11, 'gamma': 0.8318919191147035}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:15:02,128][0m Trial 38 finished with value: 0.1995866245157282 and parameters: {'observation_period_num': 19, 'train_rates': 0.7824206014088522, 'learning_rate': 5.178275482499919e-05, 'batch_size': 156, 'step_size': 13, 'gamma': 0.7798819763479291}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:15:34,303][0m Trial 39 finished with value: 0.15921813033680451 and parameters: {'observation_period_num': 120, 'train_rates': 0.8516234886905122, 'learning_rate': 0.00011374763125104707, 'batch_size': 183, 'step_size': 14, 'gamma': 0.8975605954523158}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:16:16,929][0m Trial 40 finished with value: 0.08775163901992053 and parameters: {'observation_period_num': 147, 'train_rates': 0.9106259267892818, 'learning_rate': 0.0007190009620098994, 'batch_size': 140, 'step_size': 5, 'gamma': 0.7980828459922998}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:16:44,588][0m Trial 41 finished with value: 0.04522264748811722 and parameters: {'observation_period_num': 7, 'train_rates': 0.9693059169633327, 'learning_rate': 0.0005259220636019646, 'batch_size': 241, 'step_size': 15, 'gamma': 0.7619489306316815}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:17:12,132][0m Trial 42 finished with value: 0.048569437116384506 and parameters: {'observation_period_num': 19, 'train_rates': 0.9682122361897109, 'learning_rate': 0.00034109680831237487, 'batch_size': 248, 'step_size': 15, 'gamma': 0.7623221465059704}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:17:44,283][0m Trial 43 finished with value: 0.04986286163330078 and parameters: {'observation_period_num': 35, 'train_rates': 0.9734954179687204, 'learning_rate': 0.0006326753646022468, 'batch_size': 203, 'step_size': 15, 'gamma': 0.7514342318670635}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:18:11,815][0m Trial 44 finished with value: 0.06188023090362549 and parameters: {'observation_period_num': 5, 'train_rates': 0.9445991028086052, 'learning_rate': 0.00015117080435701885, 'batch_size': 242, 'step_size': 14, 'gamma': 0.7807942511812768}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:18:43,507][0m Trial 45 finished with value: 0.1419638693332672 and parameters: {'observation_period_num': 23, 'train_rates': 0.9885687274672984, 'learning_rate': 2.2588243351724495e-05, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8174797077717715}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:19:11,503][0m Trial 46 finished with value: 0.07381000369787216 and parameters: {'observation_period_num': 53, 'train_rates': 0.92234395661538, 'learning_rate': 0.00025255947451001046, 'batch_size': 234, 'step_size': 15, 'gamma': 0.7938039820345194}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:19:37,480][0m Trial 47 finished with value: 0.07488678718905852 and parameters: {'observation_period_num': 16, 'train_rates': 0.8872009774955534, 'learning_rate': 0.0004392517504306732, 'batch_size': 248, 'step_size': 3, 'gamma': 0.770658639967022}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:20:02,370][0m Trial 48 finished with value: 0.16999009596150594 and parameters: {'observation_period_num': 43, 'train_rates': 0.7117363260970474, 'learning_rate': 0.0006766069239601468, 'batch_size': 210, 'step_size': 9, 'gamma': 0.7599147773219741}. Best is trial 10 with value: 0.03673688322305679.[0m
[32m[I 2025-02-04 21:20:31,853][0m Trial 49 finished with value: 0.07901044934988022 and parameters: {'observation_period_num': 28, 'train_rates': 0.9525396601148512, 'learning_rate': 0.0009802785011174848, 'batch_size': 223, 'step_size': 13, 'gamma': 0.9369907545838199}. Best is trial 10 with value: 0.03673688322305679.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6022 | 0.1771
Epoch 2/300, Loss: 0.2134 | 0.1806
Epoch 3/300, Loss: 0.3322 | 0.1425
Epoch 4/300, Loss: 0.4490 | 1.0185
Epoch 5/300, Loss: 0.2927 | 0.2563
Epoch 6/300, Loss: 0.2892 | 0.3183
Epoch 7/300, Loss: 0.1594 | 0.1227
Epoch 8/300, Loss: 0.1344 | 0.1405
Epoch 9/300, Loss: 0.1181 | 0.0917
Epoch 10/300, Loss: 0.1161 | 0.1095
Epoch 11/300, Loss: 0.1172 | 0.0808
Epoch 12/300, Loss: 0.1426 | 0.1836
Epoch 13/300, Loss: 0.1752 | 0.2224
Epoch 14/300, Loss: 0.2086 | 0.1506
Epoch 15/300, Loss: 0.1805 | 0.1381
Epoch 16/300, Loss: 0.1687 | 0.1107
Epoch 17/300, Loss: 0.1469 | 0.1663
Epoch 18/300, Loss: 0.1419 | 0.1018
Epoch 19/300, Loss: 0.1244 | 0.1309
Epoch 20/300, Loss: 0.1120 | 0.0901
Epoch 21/300, Loss: 0.1114 | 0.0899
Epoch 22/300, Loss: 0.1087 | 0.0826
Epoch 23/300, Loss: 0.1112 | 0.0908
Epoch 24/300, Loss: 0.1099 | 0.0785
Epoch 25/300, Loss: 0.1092 | 0.1103
Epoch 26/300, Loss: 0.1079 | 0.0781
Epoch 27/300, Loss: 0.1042 | 0.0869
Epoch 28/300, Loss: 0.1020 | 0.0732
Epoch 29/300, Loss: 0.0995 | 0.0745
Epoch 30/300, Loss: 0.0970 | 0.0684
Epoch 31/300, Loss: 0.0960 | 0.0760
Epoch 32/300, Loss: 0.0936 | 0.0653
Epoch 33/300, Loss: 0.0930 | 0.0712
Epoch 34/300, Loss: 0.0917 | 0.0630
Epoch 35/300, Loss: 0.0918 | 0.0692
Epoch 36/300, Loss: 0.0912 | 0.0617
Epoch 37/300, Loss: 0.0929 | 0.0733
Epoch 38/300, Loss: 0.0928 | 0.0611
Epoch 39/300, Loss: 0.0919 | 0.0772
Epoch 40/300, Loss: 0.0884 | 0.0583
Epoch 41/300, Loss: 0.0867 | 0.0614
Epoch 42/300, Loss: 0.0873 | 0.0593
Epoch 43/300, Loss: 0.0889 | 0.0552
Epoch 44/300, Loss: 0.0827 | 0.0541
Epoch 45/300, Loss: 0.0824 | 0.0529
Epoch 46/300, Loss: 0.0810 | 0.0520
Epoch 47/300, Loss: 0.0798 | 0.0514
Epoch 48/300, Loss: 0.0794 | 0.0503
Epoch 49/300, Loss: 0.0789 | 0.0499
Epoch 50/300, Loss: 0.0785 | 0.0491
Epoch 51/300, Loss: 0.0782 | 0.0486
Epoch 52/300, Loss: 0.0778 | 0.0480
Epoch 53/300, Loss: 0.0775 | 0.0475
Epoch 54/300, Loss: 0.0772 | 0.0471
Epoch 55/300, Loss: 0.0770 | 0.0468
Epoch 56/300, Loss: 0.0768 | 0.0463
Epoch 57/300, Loss: 0.0765 | 0.0460
Epoch 58/300, Loss: 0.0763 | 0.0456
Epoch 59/300, Loss: 0.0761 | 0.0453
Epoch 60/300, Loss: 0.0759 | 0.0449
Epoch 61/300, Loss: 0.0757 | 0.0447
Epoch 62/300, Loss: 0.0755 | 0.0444
Epoch 63/300, Loss: 0.0753 | 0.0441
Epoch 64/300, Loss: 0.0751 | 0.0438
Epoch 65/300, Loss: 0.0750 | 0.0436
Epoch 66/300, Loss: 0.0748 | 0.0433
Epoch 67/300, Loss: 0.0746 | 0.0430
Epoch 68/300, Loss: 0.0744 | 0.0428
Epoch 69/300, Loss: 0.0743 | 0.0426
Epoch 70/300, Loss: 0.0741 | 0.0424
Epoch 71/300, Loss: 0.0740 | 0.0422
Epoch 72/300, Loss: 0.0739 | 0.0420
Epoch 73/300, Loss: 0.0737 | 0.0419
Epoch 74/300, Loss: 0.0736 | 0.0417
Epoch 75/300, Loss: 0.0735 | 0.0415
Epoch 76/300, Loss: 0.0733 | 0.0414
Epoch 77/300, Loss: 0.0732 | 0.0413
Epoch 78/300, Loss: 0.0731 | 0.0412
Epoch 79/300, Loss: 0.0730 | 0.0411
Epoch 80/300, Loss: 0.0729 | 0.0410
Epoch 81/300, Loss: 0.0728 | 0.0408
Epoch 82/300, Loss: 0.0728 | 0.0407
Epoch 83/300, Loss: 0.0727 | 0.0406
Epoch 84/300, Loss: 0.0726 | 0.0406
Epoch 85/300, Loss: 0.0725 | 0.0405
Epoch 86/300, Loss: 0.0724 | 0.0404
Epoch 87/300, Loss: 0.0724 | 0.0404
Epoch 88/300, Loss: 0.0723 | 0.0403
Epoch 89/300, Loss: 0.0722 | 0.0402
Epoch 90/300, Loss: 0.0721 | 0.0401
Epoch 91/300, Loss: 0.0721 | 0.0401
Epoch 92/300, Loss: 0.0720 | 0.0401
Epoch 93/300, Loss: 0.0720 | 0.0400
Epoch 94/300, Loss: 0.0719 | 0.0400
Epoch 95/300, Loss: 0.0719 | 0.0399
Epoch 96/300, Loss: 0.0718 | 0.0399
Epoch 97/300, Loss: 0.0718 | 0.0398
Epoch 98/300, Loss: 0.0717 | 0.0398
Epoch 99/300, Loss: 0.0717 | 0.0397
Epoch 100/300, Loss: 0.0716 | 0.0397
Epoch 101/300, Loss: 0.0716 | 0.0397
Epoch 102/300, Loss: 0.0715 | 0.0396
Epoch 103/300, Loss: 0.0715 | 0.0396
Epoch 104/300, Loss: 0.0715 | 0.0396
Epoch 105/300, Loss: 0.0714 | 0.0395
Epoch 106/300, Loss: 0.0714 | 0.0395
Epoch 107/300, Loss: 0.0714 | 0.0395
Epoch 108/300, Loss: 0.0713 | 0.0395
Epoch 109/300, Loss: 0.0713 | 0.0394
Epoch 110/300, Loss: 0.0713 | 0.0394
Epoch 111/300, Loss: 0.0712 | 0.0394
Epoch 112/300, Loss: 0.0712 | 0.0394
Epoch 113/300, Loss: 0.0712 | 0.0393
Epoch 114/300, Loss: 0.0712 | 0.0393
Epoch 115/300, Loss: 0.0711 | 0.0393
Epoch 116/300, Loss: 0.0711 | 0.0393
Epoch 117/300, Loss: 0.0711 | 0.0393
Epoch 118/300, Loss: 0.0711 | 0.0392
Epoch 119/300, Loss: 0.0710 | 0.0392
Epoch 120/300, Loss: 0.0710 | 0.0392
Epoch 121/300, Loss: 0.0710 | 0.0392
Epoch 122/300, Loss: 0.0710 | 0.0392
Epoch 123/300, Loss: 0.0710 | 0.0392
Epoch 124/300, Loss: 0.0709 | 0.0392
Epoch 125/300, Loss: 0.0709 | 0.0391
Epoch 126/300, Loss: 0.0709 | 0.0391
Epoch 127/300, Loss: 0.0709 | 0.0391
Epoch 128/300, Loss: 0.0709 | 0.0391
Epoch 129/300, Loss: 0.0709 | 0.0391
Epoch 130/300, Loss: 0.0708 | 0.0391
Epoch 131/300, Loss: 0.0708 | 0.0391
Epoch 132/300, Loss: 0.0708 | 0.0391
Epoch 133/300, Loss: 0.0708 | 0.0390
Epoch 134/300, Loss: 0.0708 | 0.0390
Epoch 135/300, Loss: 0.0708 | 0.0390
Epoch 136/300, Loss: 0.0708 | 0.0390
Epoch 137/300, Loss: 0.0707 | 0.0390
Epoch 138/300, Loss: 0.0707 | 0.0390
Epoch 139/300, Loss: 0.0707 | 0.0390
Epoch 140/300, Loss: 0.0707 | 0.0390
Epoch 141/300, Loss: 0.0707 | 0.0390
Epoch 142/300, Loss: 0.0707 | 0.0390
Epoch 143/300, Loss: 0.0707 | 0.0390
Epoch 144/300, Loss: 0.0707 | 0.0389
Epoch 145/300, Loss: 0.0707 | 0.0389
Epoch 146/300, Loss: 0.0707 | 0.0389
Epoch 147/300, Loss: 0.0706 | 0.0389
Epoch 148/300, Loss: 0.0706 | 0.0389
Epoch 149/300, Loss: 0.0706 | 0.0389
Epoch 150/300, Loss: 0.0706 | 0.0389
Epoch 151/300, Loss: 0.0706 | 0.0389
Epoch 152/300, Loss: 0.0706 | 0.0389
Epoch 153/300, Loss: 0.0706 | 0.0389
Epoch 154/300, Loss: 0.0706 | 0.0389
Epoch 155/300, Loss: 0.0706 | 0.0389
Epoch 156/300, Loss: 0.0706 | 0.0389
Epoch 157/300, Loss: 0.0706 | 0.0389
Epoch 158/300, Loss: 0.0706 | 0.0389
Epoch 159/300, Loss: 0.0706 | 0.0389
Epoch 160/300, Loss: 0.0706 | 0.0389
Epoch 161/300, Loss: 0.0706 | 0.0389
Epoch 162/300, Loss: 0.0705 | 0.0388
Epoch 163/300, Loss: 0.0705 | 0.0388
Epoch 164/300, Loss: 0.0705 | 0.0388
Epoch 165/300, Loss: 0.0705 | 0.0388
Epoch 166/300, Loss: 0.0705 | 0.0388
Epoch 167/300, Loss: 0.0705 | 0.0388
Epoch 168/300, Loss: 0.0705 | 0.0388
Epoch 169/300, Loss: 0.0705 | 0.0388
Epoch 170/300, Loss: 0.0705 | 0.0388
Epoch 171/300, Loss: 0.0705 | 0.0388
Epoch 172/300, Loss: 0.0705 | 0.0388
Epoch 173/300, Loss: 0.0705 | 0.0388
Epoch 174/300, Loss: 0.0705 | 0.0388
Epoch 175/300, Loss: 0.0705 | 0.0388
Epoch 176/300, Loss: 0.0705 | 0.0388
Epoch 177/300, Loss: 0.0705 | 0.0388
Epoch 178/300, Loss: 0.0705 | 0.0388
Epoch 179/300, Loss: 0.0705 | 0.0388
Epoch 180/300, Loss: 0.0705 | 0.0388
Epoch 181/300, Loss: 0.0705 | 0.0388
Epoch 182/300, Loss: 0.0705 | 0.0388
Epoch 183/300, Loss: 0.0705 | 0.0388
Epoch 184/300, Loss: 0.0705 | 0.0388
Epoch 185/300, Loss: 0.0705 | 0.0388
Epoch 186/300, Loss: 0.0705 | 0.0388
Epoch 187/300, Loss: 0.0705 | 0.0388
Epoch 188/300, Loss: 0.0705 | 0.0388
Epoch 189/300, Loss: 0.0705 | 0.0388
Epoch 190/300, Loss: 0.0705 | 0.0388
Epoch 191/300, Loss: 0.0705 | 0.0388
Epoch 192/300, Loss: 0.0705 | 0.0388
Epoch 193/300, Loss: 0.0704 | 0.0388
Epoch 194/300, Loss: 0.0704 | 0.0388
Epoch 195/300, Loss: 0.0704 | 0.0388
Epoch 196/300, Loss: 0.0704 | 0.0388
Epoch 197/300, Loss: 0.0704 | 0.0388
Epoch 198/300, Loss: 0.0704 | 0.0388
Epoch 199/300, Loss: 0.0704 | 0.0388
Epoch 200/300, Loss: 0.0704 | 0.0388
Epoch 201/300, Loss: 0.0704 | 0.0388
Epoch 202/300, Loss: 0.0704 | 0.0388
Epoch 203/300, Loss: 0.0704 | 0.0388
Epoch 204/300, Loss: 0.0704 | 0.0388
Epoch 205/300, Loss: 0.0704 | 0.0388
Epoch 206/300, Loss: 0.0704 | 0.0388
Epoch 207/300, Loss: 0.0704 | 0.0388
Epoch 208/300, Loss: 0.0704 | 0.0388
Epoch 209/300, Loss: 0.0704 | 0.0388
Epoch 210/300, Loss: 0.0704 | 0.0388
Epoch 211/300, Loss: 0.0704 | 0.0388
Epoch 212/300, Loss: 0.0704 | 0.0388
Epoch 213/300, Loss: 0.0704 | 0.0388
Epoch 214/300, Loss: 0.0704 | 0.0388
Epoch 215/300, Loss: 0.0704 | 0.0387
Epoch 216/300, Loss: 0.0704 | 0.0387
Epoch 217/300, Loss: 0.0704 | 0.0387
Epoch 218/300, Loss: 0.0704 | 0.0387
Epoch 219/300, Loss: 0.0704 | 0.0387
Epoch 220/300, Loss: 0.0704 | 0.0387
Epoch 221/300, Loss: 0.0704 | 0.0387
Epoch 222/300, Loss: 0.0704 | 0.0387
Epoch 223/300, Loss: 0.0704 | 0.0387
Epoch 224/300, Loss: 0.0704 | 0.0387
Epoch 225/300, Loss: 0.0704 | 0.0387
Epoch 226/300, Loss: 0.0704 | 0.0387
Epoch 227/300, Loss: 0.0704 | 0.0387
Epoch 228/300, Loss: 0.0704 | 0.0387
Epoch 229/300, Loss: 0.0704 | 0.0387
Epoch 230/300, Loss: 0.0704 | 0.0387
Epoch 231/300, Loss: 0.0704 | 0.0387
Epoch 232/300, Loss: 0.0704 | 0.0387
Epoch 233/300, Loss: 0.0704 | 0.0387
Epoch 234/300, Loss: 0.0704 | 0.0387
Epoch 235/300, Loss: 0.0704 | 0.0387
Epoch 236/300, Loss: 0.0704 | 0.0387
Epoch 237/300, Loss: 0.0704 | 0.0387
Epoch 238/300, Loss: 0.0704 | 0.0387
Epoch 239/300, Loss: 0.0704 | 0.0387
Epoch 240/300, Loss: 0.0704 | 0.0387
Epoch 241/300, Loss: 0.0704 | 0.0387
Epoch 242/300, Loss: 0.0704 | 0.0387
Epoch 243/300, Loss: 0.0704 | 0.0387
Epoch 244/300, Loss: 0.0704 | 0.0387
Epoch 245/300, Loss: 0.0704 | 0.0387
Epoch 246/300, Loss: 0.0704 | 0.0387
Epoch 247/300, Loss: 0.0704 | 0.0387
Epoch 248/300, Loss: 0.0704 | 0.0387
Epoch 249/300, Loss: 0.0704 | 0.0387
Epoch 250/300, Loss: 0.0704 | 0.0387
Epoch 251/300, Loss: 0.0704 | 0.0387
Epoch 252/300, Loss: 0.0704 | 0.0387
Epoch 253/300, Loss: 0.0704 | 0.0387
Epoch 254/300, Loss: 0.0704 | 0.0387
Epoch 255/300, Loss: 0.0704 | 0.0387
Epoch 256/300, Loss: 0.0704 | 0.0387
Epoch 257/300, Loss: 0.0704 | 0.0387
Epoch 258/300, Loss: 0.0704 | 0.0387
Epoch 259/300, Loss: 0.0704 | 0.0387
Epoch 260/300, Loss: 0.0704 | 0.0387
Epoch 261/300, Loss: 0.0704 | 0.0387
Epoch 262/300, Loss: 0.0704 | 0.0387
Epoch 263/300, Loss: 0.0704 | 0.0387
Epoch 264/300, Loss: 0.0704 | 0.0387
Epoch 265/300, Loss: 0.0704 | 0.0387
Epoch 266/300, Loss: 0.0704 | 0.0387
Epoch 267/300, Loss: 0.0704 | 0.0387
Epoch 268/300, Loss: 0.0704 | 0.0387
Epoch 269/300, Loss: 0.0704 | 0.0387
Epoch 270/300, Loss: 0.0704 | 0.0387
Epoch 271/300, Loss: 0.0704 | 0.0387
Epoch 272/300, Loss: 0.0704 | 0.0387
Epoch 273/300, Loss: 0.0704 | 0.0387
Epoch 274/300, Loss: 0.0704 | 0.0387
Epoch 275/300, Loss: 0.0704 | 0.0387
Epoch 276/300, Loss: 0.0704 | 0.0387
Epoch 277/300, Loss: 0.0704 | 0.0387
Epoch 278/300, Loss: 0.0704 | 0.0387
Epoch 279/300, Loss: 0.0704 | 0.0387
Epoch 280/300, Loss: 0.0704 | 0.0387
Epoch 281/300, Loss: 0.0704 | 0.0387
Epoch 282/300, Loss: 0.0704 | 0.0387
Epoch 283/300, Loss: 0.0704 | 0.0387
Epoch 284/300, Loss: 0.0704 | 0.0387
Epoch 285/300, Loss: 0.0704 | 0.0387
Epoch 286/300, Loss: 0.0704 | 0.0387
Epoch 287/300, Loss: 0.0704 | 0.0387
Epoch 288/300, Loss: 0.0704 | 0.0387
Epoch 289/300, Loss: 0.0704 | 0.0387
Epoch 290/300, Loss: 0.0704 | 0.0387
Epoch 291/300, Loss: 0.0704 | 0.0387
Epoch 292/300, Loss: 0.0704 | 0.0387
Epoch 293/300, Loss: 0.0704 | 0.0387
Epoch 294/300, Loss: 0.0704 | 0.0387
Epoch 295/300, Loss: 0.0704 | 0.0387
Epoch 296/300, Loss: 0.0704 | 0.0387
Epoch 297/300, Loss: 0.0704 | 0.0387
Epoch 298/300, Loss: 0.0704 | 0.0387
Epoch 299/300, Loss: 0.0704 | 0.0387
Epoch 300/300, Loss: 0.0704 | 0.0387
Runtime (seconds): 80.50122046470642
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 16.6241347938776
RMSE: 4.0772705078125
MAE: 4.0772705078125
R-squared: nan
[166.54916]
