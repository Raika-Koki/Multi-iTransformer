ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 02:40:36,652][0m A new study created in memory with name: no-name-6d72d0a1-57c0-4f48-b859-ca2ea41bd019[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-03 02:41:14,776][0m Trial 0 finished with value: 0.06549381574876206 and parameters: {'observation_period_num': 15, 'train_rates': 0.9367359311778457, 'learning_rate': 3.2607066065568755e-05, 'batch_size': 143, 'step_size': 11, 'gamma': 0.8479307026203275}. Best is trial 0 with value: 0.06549381574876206.[0m
[32m[I 2025-01-03 02:41:53,127][0m Trial 1 finished with value: 0.13421823297228133 and parameters: {'observation_period_num': 252, 'train_rates': 0.9299543551922412, 'learning_rate': 0.0003727730783685206, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8123256231111676}. Best is trial 0 with value: 0.06549381574876206.[0m
[32m[I 2025-01-03 02:43:07,391][0m Trial 2 finished with value: 0.25066476129904025 and parameters: {'observation_period_num': 199, 'train_rates': 0.8510274685246311, 'learning_rate': 6.580218139596207e-06, 'batch_size': 66, 'step_size': 10, 'gamma': 0.9629160840758324}. Best is trial 0 with value: 0.06549381574876206.[0m
[32m[I 2025-01-03 02:43:34,382][0m Trial 3 finished with value: 0.2199026197195053 and parameters: {'observation_period_num': 9, 'train_rates': 0.9343601414880562, 'learning_rate': 6.0893636260222775e-06, 'batch_size': 220, 'step_size': 5, 'gamma': 0.9639293047586853}. Best is trial 0 with value: 0.06549381574876206.[0m
[32m[I 2025-01-03 02:44:28,087][0m Trial 4 finished with value: 0.3236984776631566 and parameters: {'observation_period_num': 120, 'train_rates': 0.6298652616617701, 'learning_rate': 8.126442309367627e-06, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8315594156268213}. Best is trial 0 with value: 0.06549381574876206.[0m
[32m[I 2025-01-03 02:45:35,984][0m Trial 5 finished with value: 0.04355749022215605 and parameters: {'observation_period_num': 42, 'train_rates': 0.7910371427159769, 'learning_rate': 0.00012046391381950981, 'batch_size': 75, 'step_size': 13, 'gamma': 0.967879945004109}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:47:34,270][0m Trial 6 finished with value: 0.2894802403088874 and parameters: {'observation_period_num': 185, 'train_rates': 0.6808695796758033, 'learning_rate': 7.527940131672188e-06, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9484010194563375}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:47:59,609][0m Trial 7 finished with value: 0.5123027628287673 and parameters: {'observation_period_num': 172, 'train_rates': 0.9080811640716934, 'learning_rate': 3.1519913099234143e-06, 'batch_size': 252, 'step_size': 10, 'gamma': 0.9619849828194811}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:48:30,685][0m Trial 8 finished with value: 0.07636153697967529 and parameters: {'observation_period_num': 29, 'train_rates': 0.9578616925245883, 'learning_rate': 8.165117939503921e-05, 'batch_size': 214, 'step_size': 8, 'gamma': 0.9852069971543478}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:49:23,753][0m Trial 9 finished with value: 0.1416179642508753 and parameters: {'observation_period_num': 166, 'train_rates': 0.8247203478170652, 'learning_rate': 0.00021590760324600392, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9869904105056732}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:51:36,452][0m Trial 10 finished with value: 0.13822882254439664 and parameters: {'observation_period_num': 80, 'train_rates': 0.7397545185110626, 'learning_rate': 0.0009809550329669655, 'batch_size': 36, 'step_size': 15, 'gamma': 0.9134073278732462}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:52:16,951][0m Trial 11 finished with value: 0.10575815419245033 and parameters: {'observation_period_num': 61, 'train_rates': 0.7522862307850576, 'learning_rate': 3.28901992051559e-05, 'batch_size': 133, 'step_size': 15, 'gamma': 0.7656205431431625}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:52:52,883][0m Trial 12 finished with value: 0.09283718107967211 and parameters: {'observation_period_num': 64, 'train_rates': 0.8403834279009619, 'learning_rate': 4.138254035949005e-05, 'batch_size': 161, 'step_size': 12, 'gamma': 0.8816839780688986}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:53:39,120][0m Trial 13 finished with value: 0.06404898070489255 and parameters: {'observation_period_num': 104, 'train_rates': 0.7801394454928332, 'learning_rate': 0.00010861912805036293, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8554892691069947}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:54:30,823][0m Trial 14 finished with value: 0.07558062805296624 and parameters: {'observation_period_num': 114, 'train_rates': 0.7686430180916631, 'learning_rate': 0.00018822464872768002, 'batch_size': 99, 'step_size': 14, 'gamma': 0.8822248063493336}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 02:59:14,541][0m Trial 15 finished with value: 0.12029207873399932 and parameters: {'observation_period_num': 87, 'train_rates': 0.7000737186105853, 'learning_rate': 9.133286404683946e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9074752495029323}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 03:00:03,506][0m Trial 16 finished with value: 1.0180051452079355 and parameters: {'observation_period_num': 45, 'train_rates': 0.7921810541231228, 'learning_rate': 1.0219809189914755e-06, 'batch_size': 113, 'step_size': 5, 'gamma': 0.7980448978850909}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 03:00:37,315][0m Trial 17 finished with value: 0.09409009805245763 and parameters: {'observation_period_num': 146, 'train_rates': 0.8882064685786931, 'learning_rate': 0.0007698876403849197, 'batch_size': 170, 'step_size': 9, 'gamma': 0.9269833537136561}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 03:01:51,573][0m Trial 18 finished with value: 0.09889262854544954 and parameters: {'observation_period_num': 98, 'train_rates': 0.6916371258782911, 'learning_rate': 8.399782554477805e-05, 'batch_size': 62, 'step_size': 7, 'gamma': 0.8552684850881227}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 03:02:21,581][0m Trial 19 finished with value: 0.27350492328974124 and parameters: {'observation_period_num': 132, 'train_rates': 0.8144046347763315, 'learning_rate': 1.678615655931596e-05, 'batch_size': 179, 'step_size': 14, 'gamma': 0.7600530836358252}. Best is trial 5 with value: 0.04355749022215605.[0m
Early stopping at epoch 91
[32m[I 2025-01-03 03:03:07,250][0m Trial 20 finished with value: 0.1410984843969345 and parameters: {'observation_period_num': 47, 'train_rates': 0.9889974502556546, 'learning_rate': 0.0003084478341636384, 'batch_size': 125, 'step_size': 1, 'gamma': 0.8737148953424475}. Best is trial 5 with value: 0.04355749022215605.[0m
[32m[I 2025-01-03 03:04:10,665][0m Trial 21 finished with value: 0.03550547216232721 and parameters: {'observation_period_num': 7, 'train_rates': 0.8694716193277744, 'learning_rate': 5.2125099207324064e-05, 'batch_size': 90, 'step_size': 11, 'gamma': 0.839600042046452}. Best is trial 21 with value: 0.03550547216232721.[0m
[32m[I 2025-01-03 03:05:13,598][0m Trial 22 finished with value: 0.0401478230021894 and parameters: {'observation_period_num': 33, 'train_rates': 0.7806947048809754, 'learning_rate': 0.0001160145912779221, 'batch_size': 84, 'step_size': 13, 'gamma': 0.8152872403115277}. Best is trial 21 with value: 0.03550547216232721.[0m
[32m[I 2025-01-03 03:06:23,785][0m Trial 23 finished with value: 0.10666264911437628 and parameters: {'observation_period_num': 31, 'train_rates': 0.8766589189143262, 'learning_rate': 1.7604200314153285e-05, 'batch_size': 82, 'step_size': 11, 'gamma': 0.7882855124350909}. Best is trial 21 with value: 0.03550547216232721.[0m
[32m[I 2025-01-03 03:08:01,030][0m Trial 24 finished with value: 0.027047055193382515 and parameters: {'observation_period_num': 8, 'train_rates': 0.8630526793621277, 'learning_rate': 6.373233032832149e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8273700360659432}. Best is trial 24 with value: 0.027047055193382515.[0m
[32m[I 2025-01-03 03:09:53,344][0m Trial 25 finished with value: 0.02671210077305389 and parameters: {'observation_period_num': 9, 'train_rates': 0.8628915029270156, 'learning_rate': 6.254047235631508e-05, 'batch_size': 49, 'step_size': 10, 'gamma': 0.8171784631799486}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:11:38,744][0m Trial 26 finished with value: 0.03059529730127853 and parameters: {'observation_period_num': 15, 'train_rates': 0.859901525603342, 'learning_rate': 5.5404247302836345e-05, 'batch_size': 52, 'step_size': 10, 'gamma': 0.8392104401688458}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:13:27,325][0m Trial 27 finished with value: 0.15873953173522815 and parameters: {'observation_period_num': 66, 'train_rates': 0.9020595081986391, 'learning_rate': 1.560377708106725e-05, 'batch_size': 51, 'step_size': 6, 'gamma': 0.7823948670561043}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:18:59,329][0m Trial 28 finished with value: 0.030992696479363827 and parameters: {'observation_period_num': 21, 'train_rates': 0.847970775735001, 'learning_rate': 5.71865371236181e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8228974255527449}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:21:07,579][0m Trial 29 finished with value: 0.034865683504876274 and parameters: {'observation_period_num': 15, 'train_rates': 0.8221918637465851, 'learning_rate': 2.824654302501029e-05, 'batch_size': 41, 'step_size': 10, 'gamma': 0.84846551431063}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:22:43,593][0m Trial 30 finished with value: 0.051049608727986404 and parameters: {'observation_period_num': 8, 'train_rates': 0.8630310267754358, 'learning_rate': 2.3999707978278967e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.794114375128562}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:27:38,632][0m Trial 31 finished with value: 0.029514845643265574 and parameters: {'observation_period_num': 27, 'train_rates': 0.8430994323278875, 'learning_rate': 5.460735204876511e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.8204173500137103}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:30:37,984][0m Trial 32 finished with value: 0.054615930654108524 and parameters: {'observation_period_num': 52, 'train_rates': 0.9146683963134442, 'learning_rate': 0.00019323538210435384, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8179129606873152}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:32:15,185][0m Trial 33 finished with value: 0.09987676872041659 and parameters: {'observation_period_num': 215, 'train_rates': 0.8856374781091785, 'learning_rate': 5.272882132796256e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8018251740975438}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:35:36,246][0m Trial 34 finished with value: 0.029715890274335514 and parameters: {'observation_period_num': 19, 'train_rates': 0.8332565068581146, 'learning_rate': 6.270011832646062e-05, 'batch_size': 26, 'step_size': 11, 'gamma': 0.836472885739071}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:38:39,019][0m Trial 35 finished with value: 0.07500772299843098 and parameters: {'observation_period_num': 31, 'train_rates': 0.8133804364236353, 'learning_rate': 0.0005184527602934237, 'batch_size': 28, 'step_size': 11, 'gamma': 0.7735363402932307}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:42:47,444][0m Trial 36 finished with value: 0.029865927718954643 and parameters: {'observation_period_num': 22, 'train_rates': 0.8373931759020411, 'learning_rate': 0.00015851872715583608, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8286456417680651}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:44:07,070][0m Trial 37 finished with value: 0.1658871190921041 and parameters: {'observation_period_num': 244, 'train_rates': 0.9517955748145392, 'learning_rate': 0.0003423369305406314, 'batch_size': 69, 'step_size': 12, 'gamma': 0.8634751607790789}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:46:05,916][0m Trial 38 finished with value: 0.0595423408828998 and parameters: {'observation_period_num': 5, 'train_rates': 0.7331429157230581, 'learning_rate': 1.1364900643463175e-05, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8055600833708492}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:47:47,794][0m Trial 39 finished with value: 0.10082879311764323 and parameters: {'observation_period_num': 41, 'train_rates': 0.6060984968020531, 'learning_rate': 7.408260384919986e-05, 'batch_size': 41, 'step_size': 11, 'gamma': 0.7500935428106204}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:49:11,903][0m Trial 40 finished with value: 0.07552350745781472 and parameters: {'observation_period_num': 80, 'train_rates': 0.9340982737866764, 'learning_rate': 3.515596715756697e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.8365899701012123}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:53:10,116][0m Trial 41 finished with value: 0.030139376804576953 and parameters: {'observation_period_num': 22, 'train_rates': 0.837622984141837, 'learning_rate': 0.00013873171143000127, 'batch_size': 22, 'step_size': 12, 'gamma': 0.8263780557425611}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:56:18,063][0m Trial 42 finished with value: 0.040858763066353276 and parameters: {'observation_period_num': 27, 'train_rates': 0.8030055668441635, 'learning_rate': 0.0001583024993470348, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8105371154974278}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 03:58:09,848][0m Trial 43 finished with value: 0.04292844898127688 and parameters: {'observation_period_num': 56, 'train_rates': 0.8347484850517851, 'learning_rate': 7.042464451278827e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8293686422562551}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 04:01:40,592][0m Trial 44 finished with value: 0.04180026945953325 and parameters: {'observation_period_num': 38, 'train_rates': 0.9014720974040127, 'learning_rate': 0.00024774862997598316, 'batch_size': 26, 'step_size': 11, 'gamma': 0.851127157456174}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 04:04:18,377][0m Trial 45 finished with value: 0.032053427593760425 and parameters: {'observation_period_num': 19, 'train_rates': 0.8566660555790457, 'learning_rate': 4.3728155848051666e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.8688432646883671}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 04:05:26,693][0m Trial 46 finished with value: 0.0770332214437412 and parameters: {'observation_period_num': 74, 'train_rates': 0.8044880635038058, 'learning_rate': 2.5889939930724665e-05, 'batch_size': 75, 'step_size': 10, 'gamma': 0.816041882723537}. Best is trial 25 with value: 0.02671210077305389.[0m
[32m[I 2025-01-03 04:11:04,440][0m Trial 47 finished with value: 0.026445518563670372 and parameters: {'observation_period_num': 17, 'train_rates': 0.8834213065517028, 'learning_rate': 0.00010200149933914854, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8418145174823586}. Best is trial 47 with value: 0.026445518563670372.[0m
[32m[I 2025-01-03 04:11:35,962][0m Trial 48 finished with value: 0.05223083882196712 and parameters: {'observation_period_num': 39, 'train_rates': 0.9152969611985426, 'learning_rate': 0.00011262022673932606, 'batch_size': 196, 'step_size': 15, 'gamma': 0.8931028953052084}. Best is trial 47 with value: 0.026445518563670372.[0m
[32m[I 2025-01-03 04:13:10,994][0m Trial 49 finished with value: 0.0465024768304058 and parameters: {'observation_period_num': 53, 'train_rates': 0.8821613223102622, 'learning_rate': 3.93461917008537e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.8453886933685648}. Best is trial 47 with value: 0.026445518563670372.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 04:13:11,004][0m A new study created in memory with name: no-name-e4043903-dbcf-4f2b-b5d8-41161bf501b7[0m
[32m[I 2025-01-03 04:13:41,606][0m Trial 0 finished with value: 0.07424575090408325 and parameters: {'observation_period_num': 15, 'train_rates': 0.9462841670496989, 'learning_rate': 7.25868463714351e-05, 'batch_size': 206, 'step_size': 10, 'gamma': 0.9840294287852422}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:14:04,469][0m Trial 1 finished with value: 0.2753145853264865 and parameters: {'observation_period_num': 179, 'train_rates': 0.6273356188788215, 'learning_rate': 4.537471397175362e-05, 'batch_size': 200, 'step_size': 9, 'gamma': 0.8346036907803108}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:14:33,046][0m Trial 2 finished with value: 0.10130777688168768 and parameters: {'observation_period_num': 78, 'train_rates': 0.6970236727929277, 'learning_rate': 0.00010194657576566701, 'batch_size': 176, 'step_size': 11, 'gamma': 0.7642599439026865}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:16:01,747][0m Trial 3 finished with value: 0.20138849821283455 and parameters: {'observation_period_num': 141, 'train_rates': 0.6896385790465009, 'learning_rate': 0.0002869589292550347, 'batch_size': 50, 'step_size': 11, 'gamma': 0.9479922545755054}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:17:06,064][0m Trial 4 finished with value: 0.0799189954996109 and parameters: {'observation_period_num': 177, 'train_rates': 0.9818462638491168, 'learning_rate': 0.000159642046673514, 'batch_size': 88, 'step_size': 9, 'gamma': 0.8414302849194405}. Best is trial 0 with value: 0.07424575090408325.[0m
Early stopping at epoch 90
[32m[I 2025-01-03 04:17:41,196][0m Trial 5 finished with value: 2.1710828675164118 and parameters: {'observation_period_num': 171, 'train_rates': 0.6279004260760003, 'learning_rate': 3.5459169530986245e-06, 'batch_size': 115, 'step_size': 1, 'gamma': 0.879057181332605}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:18:20,013][0m Trial 6 finished with value: 0.0782325491309166 and parameters: {'observation_period_num': 222, 'train_rates': 0.9782357378294637, 'learning_rate': 0.0007786566962415966, 'batch_size': 152, 'step_size': 7, 'gamma': 0.8325397382395318}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:18:59,997][0m Trial 7 finished with value: 0.15045661725620232 and parameters: {'observation_period_num': 169, 'train_rates': 0.8186343657372925, 'learning_rate': 2.1846497212413515e-05, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9860149236400626}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:20:58,837][0m Trial 8 finished with value: 0.3482163729896556 and parameters: {'observation_period_num': 247, 'train_rates': 0.6667189605892083, 'learning_rate': 1.0799864370583495e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8676646898038227}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:21:24,109][0m Trial 9 finished with value: 0.8411625139621082 and parameters: {'observation_period_num': 87, 'train_rates': 0.873629276864107, 'learning_rate': 3.0352167096143637e-06, 'batch_size': 241, 'step_size': 9, 'gamma': 0.9106249107843822}. Best is trial 0 with value: 0.07424575090408325.[0m
[32m[I 2025-01-03 04:21:49,248][0m Trial 10 finished with value: 0.046967141432304904 and parameters: {'observation_period_num': 13, 'train_rates': 0.9008417703009597, 'learning_rate': 0.0007796850314814266, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9830443930730652}. Best is trial 10 with value: 0.046967141432304904.[0m
[32m[I 2025-01-03 04:22:14,748][0m Trial 11 finished with value: 0.07302921466996568 and parameters: {'observation_period_num': 16, 'train_rates': 0.8968947294717391, 'learning_rate': 0.0008973349011873055, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9835570233396701}. Best is trial 10 with value: 0.046967141432304904.[0m
[32m[I 2025-01-03 04:22:39,792][0m Trial 12 finished with value: 0.04034363916691612 and parameters: {'observation_period_num': 6, 'train_rates': 0.8845250263365372, 'learning_rate': 0.0005022001915549561, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9391868106306535}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:23:05,533][0m Trial 13 finished with value: 0.07765273607005707 and parameters: {'observation_period_num': 75, 'train_rates': 0.7824585995593668, 'learning_rate': 0.0003421141165330508, 'batch_size': 220, 'step_size': 14, 'gamma': 0.9409390285357023}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:23:28,880][0m Trial 14 finished with value: 0.08749568617592256 and parameters: {'observation_period_num': 38, 'train_rates': 0.835257961044506, 'learning_rate': 0.00036529357191863256, 'batch_size': 250, 'step_size': 15, 'gamma': 0.9398300345486496}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:23:58,710][0m Trial 15 finished with value: 0.06923467815145071 and parameters: {'observation_period_num': 48, 'train_rates': 0.769741741089827, 'learning_rate': 0.0009967924166067533, 'batch_size': 176, 'step_size': 13, 'gamma': 0.9110492774055919}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:24:25,588][0m Trial 16 finished with value: 0.20185364111439213 and parameters: {'observation_period_num': 111, 'train_rates': 0.9040455818749311, 'learning_rate': 0.0001840135292276208, 'batch_size': 220, 'step_size': 12, 'gamma': 0.9604244107911045}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:24:59,415][0m Trial 17 finished with value: 0.06795450479521886 and parameters: {'observation_period_num': 5, 'train_rates': 0.8685813403289601, 'learning_rate': 1.0748924961735283e-05, 'batch_size': 177, 'step_size': 15, 'gamma': 0.909842225395475}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:26:11,322][0m Trial 18 finished with value: 0.8087775055319071 and parameters: {'observation_period_num': 41, 'train_rates': 0.9342675651221273, 'learning_rate': 1.0122184612230678e-06, 'batch_size': 82, 'step_size': 7, 'gamma': 0.7509819202768937}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:26:35,519][0m Trial 19 finished with value: 0.07444361521900891 and parameters: {'observation_period_num': 60, 'train_rates': 0.7401698638709215, 'learning_rate': 0.0005089601739788408, 'batch_size': 229, 'step_size': 12, 'gamma': 0.9624749620276664}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:27:05,092][0m Trial 20 finished with value: 0.07722852742684952 and parameters: {'observation_period_num': 108, 'train_rates': 0.8388063473888071, 'learning_rate': 0.00013779118555964158, 'batch_size': 194, 'step_size': 13, 'gamma': 0.7885438690710489}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:27:42,741][0m Trial 21 finished with value: 0.06680679883008228 and parameters: {'observation_period_num': 5, 'train_rates': 0.8705678992878415, 'learning_rate': 1.545217015009329e-05, 'batch_size': 160, 'step_size': 15, 'gamma': 0.9155835791672619}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:28:21,956][0m Trial 22 finished with value: 0.062161886652486516 and parameters: {'observation_period_num': 24, 'train_rates': 0.9229640081313327, 'learning_rate': 4.238134003813905e-05, 'batch_size': 157, 'step_size': 15, 'gamma': 0.9233502859897927}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:29:21,663][0m Trial 23 finished with value: 0.05772168124140484 and parameters: {'observation_period_num': 35, 'train_rates': 0.9304496665385361, 'learning_rate': 6.566142480224047e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.9359839184269956}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:30:20,895][0m Trial 24 finished with value: 0.07142733286850708 and parameters: {'observation_period_num': 33, 'train_rates': 0.9585784704124221, 'learning_rate': 0.0005753191840362608, 'batch_size': 100, 'step_size': 13, 'gamma': 0.8869806344041693}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:32:01,970][0m Trial 25 finished with value: 0.06740315069994295 and parameters: {'observation_period_num': 60, 'train_rates': 0.9139225701672689, 'learning_rate': 0.0002363101108129362, 'batch_size': 56, 'step_size': 12, 'gamma': 0.9649476159469872}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:33:17,201][0m Trial 26 finished with value: 0.06880152567701284 and parameters: {'observation_period_num': 58, 'train_rates': 0.8811468908764013, 'learning_rate': 0.0005712406378872607, 'batch_size': 74, 'step_size': 14, 'gamma': 0.9276407009917345}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:34:01,204][0m Trial 27 finished with value: 0.042300626083656596 and parameters: {'observation_period_num': 26, 'train_rates': 0.8524067223732501, 'learning_rate': 7.470693517947153e-05, 'batch_size': 128, 'step_size': 11, 'gamma': 0.9672326955040079}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:34:25,113][0m Trial 28 finished with value: 0.0707923562285748 and parameters: {'observation_period_num': 94, 'train_rates': 0.8028571098146678, 'learning_rate': 0.0004844314675578293, 'batch_size': 236, 'step_size': 11, 'gamma': 0.9613513971100776}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:35:09,155][0m Trial 29 finished with value: 0.0420230266479951 and parameters: {'observation_period_num': 12, 'train_rates': 0.8379306914595649, 'learning_rate': 8.652332058773179e-05, 'batch_size': 126, 'step_size': 10, 'gamma': 0.977773238202633}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:35:55,825][0m Trial 30 finished with value: 0.04711964633114652 and parameters: {'observation_period_num': 24, 'train_rates': 0.8419672690127463, 'learning_rate': 7.84099444315486e-05, 'batch_size': 120, 'step_size': 10, 'gamma': 0.9711340894380971}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:36:41,145][0m Trial 31 finished with value: 0.04946231451569074 and parameters: {'observation_period_num': 5, 'train_rates': 0.8520771867501091, 'learning_rate': 2.6688072720108326e-05, 'batch_size': 129, 'step_size': 8, 'gamma': 0.9887048300963185}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:37:08,929][0m Trial 32 finished with value: 0.05396376882525657 and parameters: {'observation_period_num': 21, 'train_rates': 0.810344649620778, 'learning_rate': 0.0001103394910973865, 'batch_size': 207, 'step_size': 10, 'gamma': 0.9748659552449842}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:37:46,120][0m Trial 33 finished with value: 0.06886076580237421 and parameters: {'observation_period_num': 49, 'train_rates': 0.7591016360383234, 'learning_rate': 3.948447147141664e-05, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9494901296514215}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:38:17,794][0m Trial 34 finished with value: 0.06063783633244502 and parameters: {'observation_period_num': 21, 'train_rates': 0.8951379257957498, 'learning_rate': 6.274648932149394e-05, 'batch_size': 192, 'step_size': 7, 'gamma': 0.9541480387640022}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:43:16,418][0m Trial 35 finished with value: 0.09966629892587661 and parameters: {'observation_period_num': 76, 'train_rates': 0.8228759774460309, 'learning_rate': 0.00024042003261431704, 'batch_size': 17, 'step_size': 8, 'gamma': 0.975782559894269}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:44:05,161][0m Trial 36 finished with value: 0.06406426449355326 and parameters: {'observation_period_num': 136, 'train_rates': 0.8517924460958924, 'learning_rate': 0.00011376268887366983, 'batch_size': 109, 'step_size': 9, 'gamma': 0.9861894612951505}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:44:47,834][0m Trial 37 finished with value: 0.0492301844060421 and parameters: {'observation_period_num': 30, 'train_rates': 0.9569109682529188, 'learning_rate': 0.00016375334745090484, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8962207069183287}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:45:18,517][0m Trial 38 finished with value: 0.23741886460445297 and parameters: {'observation_period_num': 157, 'train_rates': 0.7461346390294775, 'learning_rate': 0.0003254301229144299, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9493348316561613}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:45:38,358][0m Trial 39 finished with value: 0.27288607263426984 and parameters: {'observation_period_num': 220, 'train_rates': 0.7156860571801498, 'learning_rate': 8.428960718960826e-05, 'batch_size': 255, 'step_size': 5, 'gamma': 0.8605755735003514}. Best is trial 12 with value: 0.04034363916691612.[0m
[32m[I 2025-01-03 04:46:07,170][0m Trial 40 finished with value: 0.030716689654450485 and parameters: {'observation_period_num': 14, 'train_rates': 0.8827856248774909, 'learning_rate': 0.0006892712289763473, 'batch_size': 212, 'step_size': 14, 'gamma': 0.8133290231108355}. Best is trial 40 with value: 0.030716689654450485.[0m
[32m[I 2025-01-03 04:46:34,246][0m Trial 41 finished with value: 0.03058089309546489 and parameters: {'observation_period_num': 13, 'train_rates': 0.8600698039412418, 'learning_rate': 0.0006958915694883756, 'batch_size': 211, 'step_size': 14, 'gamma': 0.8001465552215162}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:47:03,593][0m Trial 42 finished with value: 0.04646446965541923 and parameters: {'observation_period_num': 47, 'train_rates': 0.862165483188886, 'learning_rate': 0.0003957438709459344, 'batch_size': 204, 'step_size': 14, 'gamma': 0.8043930805673157}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:47:38,347][0m Trial 43 finished with value: 0.07048138166941208 and parameters: {'observation_period_num': 12, 'train_rates': 0.6004633008908018, 'learning_rate': 0.0006703446804024052, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8107824050823813}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:48:04,050][0m Trial 44 finished with value: 0.044024337359940297 and parameters: {'observation_period_num': 30, 'train_rates': 0.7881246087494098, 'learning_rate': 0.00021341382341083436, 'batch_size': 216, 'step_size': 14, 'gamma': 0.7901055270569114}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:48:37,160][0m Trial 45 finished with value: 0.080535389482975 and parameters: {'observation_period_num': 67, 'train_rates': 0.8822944008921679, 'learning_rate': 0.0004243576385154543, 'batch_size': 180, 'step_size': 13, 'gamma': 0.8450498232634522}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:49:02,080][0m Trial 46 finished with value: 0.036126939680226546 and parameters: {'observation_period_num': 14, 'train_rates': 0.8864033800431973, 'learning_rate': 0.0007104844763846184, 'batch_size': 239, 'step_size': 12, 'gamma': 0.8284847603810547}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:49:27,795][0m Trial 47 finished with value: 0.03191601187062794 and parameters: {'observation_period_num': 12, 'train_rates': 0.8240774591690282, 'learning_rate': 0.0009932021711656672, 'batch_size': 233, 'step_size': 12, 'gamma': 0.8241958459228862}. Best is trial 41 with value: 0.03058089309546489.[0m
Early stopping at epoch 65
[32m[I 2025-01-03 04:49:45,090][0m Trial 48 finished with value: 0.28820243856417627 and parameters: {'observation_period_num': 193, 'train_rates': 0.8889319444007473, 'learning_rate': 0.0007796115228342084, 'batch_size': 239, 'step_size': 1, 'gamma': 0.8166324431642802}. Best is trial 41 with value: 0.03058089309546489.[0m
[32m[I 2025-01-03 04:50:10,817][0m Trial 49 finished with value: 0.07234376194407639 and parameters: {'observation_period_num': 41, 'train_rates': 0.8232370394867573, 'learning_rate': 0.0009380264996251736, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8311710990352125}. Best is trial 41 with value: 0.03058089309546489.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 04:50:10,827][0m A new study created in memory with name: no-name-799ae9c3-632e-4eab-994f-2d6ad4808b91[0m
[32m[I 2025-01-03 04:53:46,851][0m Trial 0 finished with value: 0.03378885939018801 and parameters: {'observation_period_num': 15, 'train_rates': 0.7819915418126996, 'learning_rate': 0.00021605420238728684, 'batch_size': 23, 'step_size': 11, 'gamma': 0.8726064543989408}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:54:15,779][0m Trial 1 finished with value: 0.6920475419295036 and parameters: {'observation_period_num': 229, 'train_rates': 0.8699167297917659, 'learning_rate': 2.4608234436292293e-06, 'batch_size': 183, 'step_size': 11, 'gamma': 0.9838044501969437}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:54:51,210][0m Trial 2 finished with value: 0.14970014046302577 and parameters: {'observation_period_num': 160, 'train_rates': 0.7330061785730786, 'learning_rate': 3.937782011939432e-05, 'batch_size': 141, 'step_size': 7, 'gamma': 0.9537803777545124}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:55:39,239][0m Trial 3 finished with value: 0.12186122931713282 and parameters: {'observation_period_num': 5, 'train_rates': 0.854301430221575, 'learning_rate': 7.813732091824386e-06, 'batch_size': 118, 'step_size': 4, 'gamma': 0.896250448767266}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:56:17,013][0m Trial 4 finished with value: 0.05377826772612087 and parameters: {'observation_period_num': 105, 'train_rates': 0.8557304327095899, 'learning_rate': 0.0004084366965608837, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8782418328441647}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:57:14,328][0m Trial 5 finished with value: 0.21156631382240326 and parameters: {'observation_period_num': 150, 'train_rates': 0.8284062903652647, 'learning_rate': 8.28370178151159e-06, 'batch_size': 90, 'step_size': 12, 'gamma': 0.9539670577094763}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:58:38,918][0m Trial 6 finished with value: 0.05154116110079134 and parameters: {'observation_period_num': 37, 'train_rates': 0.6978269984544662, 'learning_rate': 3.7656250295451953e-05, 'batch_size': 55, 'step_size': 6, 'gamma': 0.9851712818393421}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 04:59:01,635][0m Trial 7 finished with value: 0.2608773235686116 and parameters: {'observation_period_num': 89, 'train_rates': 0.636297152057074, 'learning_rate': 2.04839779377844e-05, 'batch_size': 224, 'step_size': 9, 'gamma': 0.7945030872032219}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 05:00:15,320][0m Trial 8 finished with value: 0.24107484786368127 and parameters: {'observation_period_num': 49, 'train_rates': 0.676891806776519, 'learning_rate': 7.158568871729016e-06, 'batch_size': 63, 'step_size': 6, 'gamma': 0.8339074369986283}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 05:02:31,904][0m Trial 9 finished with value: 0.09864949252045452 and parameters: {'observation_period_num': 104, 'train_rates': 0.9016058351462872, 'learning_rate': 0.0007165330045860978, 'batch_size': 40, 'step_size': 1, 'gamma': 0.9639434242590416}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 05:02:56,850][0m Trial 10 finished with value: 0.13843278586864471 and parameters: {'observation_period_num': 246, 'train_rates': 0.9892643597635669, 'learning_rate': 0.00018693205701852256, 'batch_size': 254, 'step_size': 15, 'gamma': 0.758752183418912}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 05:06:18,788][0m Trial 11 finished with value: 0.03462536026152872 and parameters: {'observation_period_num': 12, 'train_rates': 0.746959196383214, 'learning_rate': 9.433762065969592e-05, 'batch_size': 24, 'step_size': 10, 'gamma': 0.9248931568753377}. Best is trial 0 with value: 0.03378885939018801.[0m
[32m[I 2025-01-03 05:11:09,099][0m Trial 12 finished with value: 0.029879927404748485 and parameters: {'observation_period_num': 5, 'train_rates': 0.7683871449850904, 'learning_rate': 0.0001259351176295345, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9100386870852014}. Best is trial 12 with value: 0.029879927404748485.[0m
[32m[I 2025-01-03 05:12:07,212][0m Trial 13 finished with value: 0.05188866839372496 and parameters: {'observation_period_num': 42, 'train_rates': 0.7803134755042702, 'learning_rate': 0.00018068037302035244, 'batch_size': 89, 'step_size': 14, 'gamma': 0.8263725721880902}. Best is trial 12 with value: 0.029879927404748485.[0m
[32m[I 2025-01-03 05:15:32,429][0m Trial 14 finished with value: 0.08365353369269489 and parameters: {'observation_period_num': 69, 'train_rates': 0.790046063050034, 'learning_rate': 0.00011340949853156295, 'batch_size': 24, 'step_size': 13, 'gamma': 0.9127005657495524}. Best is trial 12 with value: 0.029879927404748485.[0m
[32m[I 2025-01-03 05:16:46,616][0m Trial 15 finished with value: 0.1395520880654618 and parameters: {'observation_period_num': 190, 'train_rates': 0.9341694238084914, 'learning_rate': 0.00042641663975899935, 'batch_size': 75, 'step_size': 9, 'gamma': 0.8452966238035682}. Best is trial 12 with value: 0.029879927404748485.[0m
[32m[I 2025-01-03 05:21:03,555][0m Trial 16 finished with value: 0.026828059541869263 and parameters: {'observation_period_num': 6, 'train_rates': 0.7519717122624565, 'learning_rate': 0.0009984734775715291, 'batch_size': 19, 'step_size': 12, 'gamma': 0.8712461516653836}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:21:43,193][0m Trial 17 finished with value: 0.10896929309587643 and parameters: {'observation_period_num': 71, 'train_rates': 0.6256691405881459, 'learning_rate': 0.0008677410357995645, 'batch_size': 113, 'step_size': 13, 'gamma': 0.9249402624245262}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:22:12,136][0m Trial 18 finished with value: 0.7554644390829186 and parameters: {'observation_period_num': 131, 'train_rates': 0.7157000983968423, 'learning_rate': 1.0519905352416626e-06, 'batch_size': 178, 'step_size': 15, 'gamma': 0.8541436961906689}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:23:46,722][0m Trial 19 finished with value: 0.04812766346976022 and parameters: {'observation_period_num': 32, 'train_rates': 0.670315969267076, 'learning_rate': 6.432758545103727e-05, 'batch_size': 48, 'step_size': 8, 'gamma': 0.8979240000122178}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:24:53,073][0m Trial 20 finished with value: 0.04529865228144799 and parameters: {'observation_period_num': 62, 'train_rates': 0.816327972430244, 'learning_rate': 0.000417592707266679, 'batch_size': 81, 'step_size': 12, 'gamma': 0.800471050968366}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:29:39,555][0m Trial 21 finished with value: 0.02906034525177879 and parameters: {'observation_period_num': 12, 'train_rates': 0.7640062477718497, 'learning_rate': 0.00022464737860804869, 'batch_size': 17, 'step_size': 11, 'gamma': 0.8757602240952327}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:34:41,703][0m Trial 22 finished with value: 0.04189973650828571 and parameters: {'observation_period_num': 25, 'train_rates': 0.7576855366987502, 'learning_rate': 0.00030303702486765885, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8894091481743278}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:36:41,623][0m Trial 23 finished with value: 0.027956667761892488 and parameters: {'observation_period_num': 6, 'train_rates': 0.7522859187675192, 'learning_rate': 0.000869836249660903, 'batch_size': 41, 'step_size': 12, 'gamma': 0.8634264795912987}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:38:16,487][0m Trial 24 finished with value: 0.13024564537287903 and parameters: {'observation_period_num': 53, 'train_rates': 0.7124960663190522, 'learning_rate': 0.0007361703125734765, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8608131649909949}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:40:38,731][0m Trial 25 finished with value: 0.04321131458105538 and parameters: {'observation_period_num': 28, 'train_rates': 0.8265664969691427, 'learning_rate': 0.0006369995145987311, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8137416034578353}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:41:46,400][0m Trial 26 finished with value: 0.1420500299027556 and parameters: {'observation_period_num': 87, 'train_rates': 0.6627708788078274, 'learning_rate': 0.0002995684645496039, 'batch_size': 67, 'step_size': 14, 'gamma': 0.8743025974861143}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:42:34,768][0m Trial 27 finished with value: 0.03616867889177573 and parameters: {'observation_period_num': 27, 'train_rates': 0.7475160337414878, 'learning_rate': 0.0009233800337244647, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8443458200878907}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:44:38,814][0m Trial 28 finished with value: 0.052568797488575396 and parameters: {'observation_period_num': 49, 'train_rates': 0.801819170556862, 'learning_rate': 0.0004284323518891633, 'batch_size': 41, 'step_size': 10, 'gamma': 0.866412323627683}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:46:47,279][0m Trial 29 finished with value: 0.0654069230534059 and parameters: {'observation_period_num': 17, 'train_rates': 0.6060566404137167, 'learning_rate': 0.00023265925582574283, 'batch_size': 33, 'step_size': 11, 'gamma': 0.7810973208837528}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:47:35,333][0m Trial 30 finished with value: 0.07051714751240011 and parameters: {'observation_period_num': 79, 'train_rates': 0.7217781199903359, 'learning_rate': 0.0005445079050087108, 'batch_size': 100, 'step_size': 8, 'gamma': 0.8791409566946875}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:52:12,670][0m Trial 31 finished with value: 0.030725709583661328 and parameters: {'observation_period_num': 5, 'train_rates': 0.7771363757026069, 'learning_rate': 0.0001235011773484214, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9053152623312922}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:57:23,162][0m Trial 32 finished with value: 0.04103546267857413 and parameters: {'observation_period_num': 19, 'train_rates': 0.7774389422124047, 'learning_rate': 6.48739663336687e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9271305235642268}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 05:59:43,056][0m Trial 33 finished with value: 0.13381340319253182 and parameters: {'observation_period_num': 187, 'train_rates': 0.7489137224364056, 'learning_rate': 0.0009632735374693088, 'batch_size': 33, 'step_size': 11, 'gamma': 0.9402840026032803}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:01:03,227][0m Trial 34 finished with value: 0.06381751359487005 and parameters: {'observation_period_num': 10, 'train_rates': 0.6945419742299301, 'learning_rate': 2.1861438632371662e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.8872658926251085}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:01:36,163][0m Trial 35 finished with value: 0.054431752061720975 and parameters: {'observation_period_num': 38, 'train_rates': 0.7335265065462122, 'learning_rate': 0.00017238338568597474, 'batch_size': 160, 'step_size': 10, 'gamma': 0.9074585546609992}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:02:17,620][0m Trial 36 finished with value: 0.0433619047987427 and parameters: {'observation_period_num': 7, 'train_rates': 0.763064503812396, 'learning_rate': 6.86818515776047e-05, 'batch_size': 128, 'step_size': 13, 'gamma': 0.8537039845938783}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:05:26,306][0m Trial 37 finished with value: 0.03398341467385894 and parameters: {'observation_period_num': 24, 'train_rates': 0.8485368368733245, 'learning_rate': 0.0002728588005544044, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8257064940816298}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:07:14,929][0m Trial 38 finished with value: 0.04640568550005172 and parameters: {'observation_period_num': 51, 'train_rates': 0.7990331732031426, 'learning_rate': 0.00014772467731044843, 'batch_size': 47, 'step_size': 3, 'gamma': 0.8693061436011298}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:08:29,691][0m Trial 39 finished with value: 0.14601061177941468 and parameters: {'observation_period_num': 203, 'train_rates': 0.8865780746348568, 'learning_rate': 0.0004664030120860155, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8863369039200517}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:10:02,684][0m Trial 40 finished with value: 0.06729091950344994 and parameters: {'observation_period_num': 107, 'train_rates': 0.8160554419842676, 'learning_rate': 2.3399593520500617e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.9699494755407543}. Best is trial 16 with value: 0.026828059541869263.[0m
[32m[I 2025-01-03 06:14:36,955][0m Trial 41 finished with value: 0.026267113428938827 and parameters: {'observation_period_num': 10, 'train_rates': 0.7775837457530692, 'learning_rate': 9.050481338274824e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9040870623989773}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:17:27,902][0m Trial 42 finished with value: 0.032122372426200294 and parameters: {'observation_period_num': 5, 'train_rates': 0.7650908492147435, 'learning_rate': 3.998105577842912e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.9153453666702674}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:22:22,114][0m Trial 43 finished with value: 0.05549357145544021 and parameters: {'observation_period_num': 36, 'train_rates': 0.7342523778166679, 'learning_rate': 8.140901971166613e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8957649930922936}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:24:22,345][0m Trial 44 finished with value: 0.03961776422401649 and parameters: {'observation_period_num': 18, 'train_rates': 0.6970608712453173, 'learning_rate': 5.1930639116708686e-05, 'batch_size': 39, 'step_size': 12, 'gamma': 0.9369736291859406}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:24:50,021][0m Trial 45 finished with value: 0.20922252712106065 and parameters: {'observation_period_num': 60, 'train_rates': 0.8452093602515102, 'learning_rate': 1.2353050298435044e-05, 'batch_size': 213, 'step_size': 11, 'gamma': 0.8770907299926867}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:27:44,566][0m Trial 46 finished with value: 0.04311115765584665 and parameters: {'observation_period_num': 43, 'train_rates': 0.8039380060932578, 'learning_rate': 0.00011426863857254827, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8415772371957223}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:29:32,633][0m Trial 47 finished with value: 0.0302527150662554 and parameters: {'observation_period_num': 18, 'train_rates': 0.7743907061161546, 'learning_rate': 0.00030864458561773937, 'batch_size': 47, 'step_size': 13, 'gamma': 0.8588906885954656}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:31:00,713][0m Trial 48 finished with value: 0.21399914754815522 and parameters: {'observation_period_num': 35, 'train_rates': 0.7896418495153978, 'learning_rate': 3.8002939769568516e-06, 'batch_size': 58, 'step_size': 7, 'gamma': 0.940387933045094}. Best is trial 41 with value: 0.026267113428938827.[0m
[32m[I 2025-01-03 06:31:59,598][0m Trial 49 finished with value: 0.10145142085561765 and parameters: {'observation_period_num': 135, 'train_rates': 0.7341367772677811, 'learning_rate': 0.0002155281184102274, 'batch_size': 82, 'step_size': 11, 'gamma': 0.9179934432756783}. Best is trial 41 with value: 0.026267113428938827.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-03 06:31:59,608][0m A new study created in memory with name: no-name-f2b09251-028a-4097-9982-dfc50c25507d[0m
[32m[I 2025-01-03 06:32:33,608][0m Trial 0 finished with value: 0.7668336214927527 and parameters: {'observation_period_num': 105, 'train_rates': 0.6345897434960845, 'learning_rate': 3.134896710345357e-06, 'batch_size': 136, 'step_size': 3, 'gamma': 0.8735725630540521}. Best is trial 0 with value: 0.7668336214927527.[0m
[32m[I 2025-01-03 06:33:00,362][0m Trial 1 finished with value: 0.7631682583263942 and parameters: {'observation_period_num': 243, 'train_rates': 0.6743908841160176, 'learning_rate': 2.124964479054993e-06, 'batch_size': 180, 'step_size': 13, 'gamma': 0.9699963445232626}. Best is trial 1 with value: 0.7631682583263942.[0m
[32m[I 2025-01-03 06:33:51,119][0m Trial 2 finished with value: 0.5221604511892195 and parameters: {'observation_period_num': 76, 'train_rates': 0.6134172746458593, 'learning_rate': 1.2020748819180392e-06, 'batch_size': 84, 'step_size': 9, 'gamma': 0.9873818018781318}. Best is trial 2 with value: 0.5221604511892195.[0m
Early stopping at epoch 97
[32m[I 2025-01-03 06:34:14,663][0m Trial 3 finished with value: 0.10434399898114957 and parameters: {'observation_period_num': 11, 'train_rates': 0.8190112347214427, 'learning_rate': 0.00013811474122648188, 'batch_size': 252, 'step_size': 2, 'gamma': 0.7672345402313216}. Best is trial 3 with value: 0.10434399898114957.[0m
[32m[I 2025-01-03 06:36:40,076][0m Trial 4 finished with value: 0.19491855494800162 and parameters: {'observation_period_num': 241, 'train_rates': 0.8875528141590912, 'learning_rate': 0.0002604802785537347, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9312422557725728}. Best is trial 3 with value: 0.10434399898114957.[0m
[32m[I 2025-01-03 06:37:04,993][0m Trial 5 finished with value: 0.0844008732367486 and parameters: {'observation_period_num': 77, 'train_rates': 0.6811725871438621, 'learning_rate': 0.0002393197672890587, 'batch_size': 205, 'step_size': 14, 'gamma': 0.8632148410510747}. Best is trial 5 with value: 0.0844008732367486.[0m
[32m[I 2025-01-03 06:37:57,998][0m Trial 6 finished with value: 0.26983414662694466 and parameters: {'observation_period_num': 109, 'train_rates': 0.8188820801269342, 'learning_rate': 7.122198125797615e-06, 'batch_size': 100, 'step_size': 15, 'gamma': 0.9190929267741568}. Best is trial 5 with value: 0.0844008732367486.[0m
[32m[I 2025-01-03 06:38:21,890][0m Trial 7 finished with value: 0.07668126472870128 and parameters: {'observation_period_num': 116, 'train_rates': 0.7061818230406011, 'learning_rate': 0.0008356693610961416, 'batch_size': 218, 'step_size': 8, 'gamma': 0.8023941155646414}. Best is trial 7 with value: 0.07668126472870128.[0m
[32m[I 2025-01-03 06:39:42,600][0m Trial 8 finished with value: 0.7054309248924255 and parameters: {'observation_period_num': 237, 'train_rates': 0.9855650772713167, 'learning_rate': 1.4227209733926626e-06, 'batch_size': 70, 'step_size': 5, 'gamma': 0.8517901734730522}. Best is trial 7 with value: 0.07668126472870128.[0m
[32m[I 2025-01-03 06:40:06,665][0m Trial 9 finished with value: 0.8788263008898537 and parameters: {'observation_period_num': 235, 'train_rates': 0.7259072677500759, 'learning_rate': 1.6563074418872883e-06, 'batch_size': 216, 'step_size': 7, 'gamma': 0.9248799046333617}. Best is trial 7 with value: 0.07668126472870128.[0m
[32m[I 2025-01-03 06:40:41,463][0m Trial 10 finished with value: 0.10157789543010404 and parameters: {'observation_period_num': 173, 'train_rates': 0.7531688436233231, 'learning_rate': 0.0008099763006876667, 'batch_size': 145, 'step_size': 10, 'gamma': 0.7532160484149879}. Best is trial 7 with value: 0.07668126472870128.[0m
[32m[I 2025-01-03 06:41:06,451][0m Trial 11 finished with value: 0.06324831738814034 and parameters: {'observation_period_num': 53, 'train_rates': 0.6963101601362647, 'learning_rate': 0.0009094290270628076, 'batch_size': 207, 'step_size': 11, 'gamma': 0.8131730086994668}. Best is trial 11 with value: 0.06324831738814034.[0m
[32m[I 2025-01-03 06:41:28,336][0m Trial 12 finished with value: 0.035591225788528025 and parameters: {'observation_period_num': 11, 'train_rates': 0.7422941898405241, 'learning_rate': 0.0008594731777274886, 'batch_size': 251, 'step_size': 11, 'gamma': 0.8031016251532178}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:41:49,973][0m Trial 13 finished with value: 0.060354289127869554 and parameters: {'observation_period_num': 5, 'train_rates': 0.758396421454108, 'learning_rate': 5.373050280277732e-05, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8136081199296464}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:42:13,482][0m Trial 14 finished with value: 0.06736901339731718 and parameters: {'observation_period_num': 6, 'train_rates': 0.7742238990166005, 'learning_rate': 3.028369682279736e-05, 'batch_size': 252, 'step_size': 12, 'gamma': 0.8052555253865521}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:42:47,031][0m Trial 15 finished with value: 0.12262278846499375 and parameters: {'observation_period_num': 38, 'train_rates': 0.8605647976741132, 'learning_rate': 3.649047156626556e-05, 'batch_size': 168, 'step_size': 6, 'gamma': 0.8322041859019526}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:43:10,044][0m Trial 16 finished with value: 0.18213319153218835 and parameters: {'observation_period_num': 155, 'train_rates': 0.8979641680282369, 'learning_rate': 4.0323084186754126e-05, 'batch_size': 254, 'step_size': 11, 'gamma': 0.7809151042728876}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:43:33,871][0m Trial 17 finished with value: 0.19655611692813404 and parameters: {'observation_period_num': 37, 'train_rates': 0.7849588896610533, 'learning_rate': 1.1801661155634822e-05, 'batch_size': 232, 'step_size': 9, 'gamma': 0.8907998623648572}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:44:02,510][0m Trial 18 finished with value: 0.07619261084586983 and parameters: {'observation_period_num': 69, 'train_rates': 0.741562890522494, 'learning_rate': 8.699979685997059e-05, 'batch_size': 179, 'step_size': 13, 'gamma': 0.8443269465488841}. Best is trial 12 with value: 0.035591225788528025.[0m
[32m[I 2025-01-03 06:44:47,285][0m Trial 19 finished with value: 0.032166165011080584 and parameters: {'observation_period_num': 25, 'train_rates': 0.8346078149720864, 'learning_rate': 0.0003116785167859238, 'batch_size': 125, 'step_size': 11, 'gamma': 0.7907244237732256}. Best is trial 19 with value: 0.032166165011080584.[0m
[32m[I 2025-01-03 06:45:37,098][0m Trial 20 finished with value: 0.08581504225730896 and parameters: {'observation_period_num': 185, 'train_rates': 0.9861892887614966, 'learning_rate': 0.0003438298540574603, 'batch_size': 121, 'step_size': 4, 'gamma': 0.7845581275252287}. Best is trial 19 with value: 0.032166165011080584.[0m
[32m[I 2025-01-03 06:46:13,460][0m Trial 21 finished with value: 0.03264982191224893 and parameters: {'observation_period_num': 25, 'train_rates': 0.8371498347836902, 'learning_rate': 0.00043740240533994714, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8250466006636912}. Best is trial 19 with value: 0.032166165011080584.[0m
[32m[I 2025-01-03 06:46:49,773][0m Trial 22 finished with value: 0.03213927069575422 and parameters: {'observation_period_num': 31, 'train_rates': 0.8275767169898733, 'learning_rate': 0.00046877327434228215, 'batch_size': 151, 'step_size': 9, 'gamma': 0.83493785524153}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:47:29,450][0m Trial 23 finished with value: 0.03477104871906776 and parameters: {'observation_period_num': 34, 'train_rates': 0.8538972815378487, 'learning_rate': 0.00044141335956144093, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8308753677601763}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:48:06,819][0m Trial 24 finished with value: 0.05708814779153237 and parameters: {'observation_period_num': 58, 'train_rates': 0.9326856628431074, 'learning_rate': 0.00010858293776886633, 'batch_size': 163, 'step_size': 9, 'gamma': 0.8813409966460185}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:48:49,964][0m Trial 25 finished with value: 0.060647155284076125 and parameters: {'observation_period_num': 90, 'train_rates': 0.8191465279316741, 'learning_rate': 0.0004449871753853525, 'batch_size': 126, 'step_size': 13, 'gamma': 0.8309958441999369}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:49:38,165][0m Trial 26 finished with value: 0.03828677691978543 and parameters: {'observation_period_num': 30, 'train_rates': 0.8502613601622643, 'learning_rate': 0.000176794792610169, 'batch_size': 113, 'step_size': 7, 'gamma': 0.7841825800445194}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:50:17,173][0m Trial 27 finished with value: 0.05717292078868116 and parameters: {'observation_period_num': 52, 'train_rates': 0.9176836478402883, 'learning_rate': 7.253136110499581e-05, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8949244311645943}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:50:45,363][0m Trial 28 finished with value: 0.07528640349341936 and parameters: {'observation_period_num': 146, 'train_rates': 0.7957887708230739, 'learning_rate': 0.0004955624175394178, 'batch_size': 187, 'step_size': 12, 'gamma': 0.8482504674077206}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:52:19,898][0m Trial 29 finished with value: 0.036635354593001455 and parameters: {'observation_period_num': 21, 'train_rates': 0.9486289717279391, 'learning_rate': 0.000182976320813035, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8648208342116621}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:53:14,193][0m Trial 30 finished with value: 0.22094250079724714 and parameters: {'observation_period_num': 90, 'train_rates': 0.8780109899100839, 'learning_rate': 2.3634413395265866e-05, 'batch_size': 103, 'step_size': 8, 'gamma': 0.7569196247800438}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:53:54,471][0m Trial 31 finished with value: 0.03348807232450343 and parameters: {'observation_period_num': 42, 'train_rates': 0.8428806480590111, 'learning_rate': 0.00046178531668507355, 'batch_size': 144, 'step_size': 8, 'gamma': 0.8222772163903672}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:54:36,821][0m Trial 32 finished with value: 0.039155411539943355 and parameters: {'observation_period_num': 45, 'train_rates': 0.8379073609749096, 'learning_rate': 0.0003105677785266949, 'batch_size': 133, 'step_size': 6, 'gamma': 0.8222717764726344}. Best is trial 22 with value: 0.03213927069575422.[0m
[32m[I 2025-01-03 06:55:12,897][0m Trial 33 finished with value: 0.03045999427971033 and parameters: {'observation_period_num': 23, 'train_rates': 0.8023031088270607, 'learning_rate': 0.0005461939659112413, 'batch_size': 151, 'step_size': 12, 'gamma': 0.7917906432473089}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 06:55:42,632][0m Trial 34 finished with value: 0.04576591762763272 and parameters: {'observation_period_num': 64, 'train_rates': 0.7966737034978514, 'learning_rate': 0.0005772428918042927, 'batch_size': 188, 'step_size': 12, 'gamma': 0.7920590858365044}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 06:56:14,624][0m Trial 35 finished with value: 0.04220413474694771 and parameters: {'observation_period_num': 20, 'train_rates': 0.6519005003780461, 'learning_rate': 0.00021991422262969407, 'batch_size': 159, 'step_size': 14, 'gamma': 0.7664089847890296}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 06:57:16,966][0m Trial 36 finished with value: 0.034678765986528665 and parameters: {'observation_period_num': 24, 'train_rates': 0.8218242591119903, 'learning_rate': 0.00012409376661941715, 'batch_size': 86, 'step_size': 13, 'gamma': 0.7741118129895863}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 06:57:48,817][0m Trial 37 finished with value: 0.06305077968521372 and parameters: {'observation_period_num': 83, 'train_rates': 0.8105831271178399, 'learning_rate': 0.0006128480872217602, 'batch_size': 173, 'step_size': 10, 'gamma': 0.962715983436793}. Best is trial 33 with value: 0.03045999427971033.[0m
Early stopping at epoch 53
[32m[I 2025-01-03 06:58:06,152][0m Trial 38 finished with value: 1.7993176258527315 and parameters: {'observation_period_num': 20, 'train_rates': 0.875837358153154, 'learning_rate': 4.061130050965735e-06, 'batch_size': 196, 'step_size': 1, 'gamma': 0.7991125724741407}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 06:58:57,745][0m Trial 39 finished with value: 0.1254505460684219 and parameters: {'observation_period_num': 215, 'train_rates': 0.902479331982798, 'learning_rate': 0.00030755387209186684, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8418749413594319}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 06:59:32,896][0m Trial 40 finished with value: 0.06175571084022522 and parameters: {'observation_period_num': 98, 'train_rates': 0.78114591213039, 'learning_rate': 0.0001552365734243004, 'batch_size': 150, 'step_size': 12, 'gamma': 0.8591788405371539}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 07:00:15,560][0m Trial 41 finished with value: 0.03704845722985722 and parameters: {'observation_period_num': 47, 'train_rates': 0.8377479086275403, 'learning_rate': 0.0004163096980674728, 'batch_size': 134, 'step_size': 9, 'gamma': 0.8186267036824264}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 07:00:59,410][0m Trial 42 finished with value: 0.04562250653217579 and parameters: {'observation_period_num': 69, 'train_rates': 0.8390932361747985, 'learning_rate': 0.0006642378463789234, 'batch_size': 124, 'step_size': 7, 'gamma': 0.7934269843814679}. Best is trial 33 with value: 0.03045999427971033.[0m
[32m[I 2025-01-03 07:01:39,209][0m Trial 43 finished with value: 0.029015485011041165 and parameters: {'observation_period_num': 40, 'train_rates': 0.8661696799792566, 'learning_rate': 0.0009939592895815935, 'batch_size': 141, 'step_size': 9, 'gamma': 0.8236454633585995}. Best is trial 43 with value: 0.029015485011041165.[0m
[32m[I 2025-01-03 07:02:41,695][0m Trial 44 finished with value: 0.02644267586055235 and parameters: {'observation_period_num': 14, 'train_rates': 0.8632707598017314, 'learning_rate': 0.000942472856766363, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8347024092322258}. Best is trial 44 with value: 0.02644267586055235.[0m
[32m[I 2025-01-03 07:06:15,373][0m Trial 45 finished with value: 0.11255940806496051 and parameters: {'observation_period_num': 117, 'train_rates': 0.8636900783348271, 'learning_rate': 0.0009668594339654846, 'batch_size': 24, 'step_size': 9, 'gamma': 0.8767909152650134}. Best is trial 44 with value: 0.02644267586055235.[0m
[32m[I 2025-01-03 07:07:24,111][0m Trial 46 finished with value: 0.026332849430332997 and parameters: {'observation_period_num': 14, 'train_rates': 0.8865726077269445, 'learning_rate': 0.0006689572462879971, 'batch_size': 82, 'step_size': 10, 'gamma': 0.8054286618840821}. Best is trial 46 with value: 0.026332849430332997.[0m
[32m[I 2025-01-03 07:09:19,997][0m Trial 47 finished with value: 0.029875741745142834 and parameters: {'observation_period_num': 13, 'train_rates': 0.9373772073032727, 'learning_rate': 0.0006700464380650911, 'batch_size': 50, 'step_size': 10, 'gamma': 0.8080541901843921}. Best is trial 46 with value: 0.026332849430332997.[0m
[32m[I 2025-01-03 07:11:16,131][0m Trial 48 finished with value: 0.041191328988150436 and parameters: {'observation_period_num': 12, 'train_rates': 0.9624732863436265, 'learning_rate': 0.0006944518265187649, 'batch_size': 50, 'step_size': 10, 'gamma': 0.8098867047538248}. Best is trial 46 with value: 0.026332849430332997.[0m
[32m[I 2025-01-03 07:12:28,107][0m Trial 49 finished with value: 0.023930383522383636 and parameters: {'observation_period_num': 5, 'train_rates': 0.9132472828948074, 'learning_rate': 0.000951410109049033, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7680415425559964}. Best is trial 49 with value: 0.023930383522383636.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-03 07:12:28,117][0m A new study created in memory with name: no-name-09feda1c-531b-4b4c-89df-fff997a4cbfc[0m
[32m[I 2025-01-03 07:15:11,677][0m Trial 0 finished with value: 0.3187472372906159 and parameters: {'observation_period_num': 227, 'train_rates': 0.8397470597128603, 'learning_rate': 7.72642583322505e-06, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9048222164179389}. Best is trial 0 with value: 0.3187472372906159.[0m
[32m[I 2025-01-03 07:18:51,871][0m Trial 1 finished with value: 0.3429883973939078 and parameters: {'observation_period_num': 186, 'train_rates': 0.8582483351970318, 'learning_rate': 1.6234433449826354e-06, 'batch_size': 23, 'step_size': 15, 'gamma': 0.8902364681915199}. Best is trial 0 with value: 0.3187472372906159.[0m
[32m[I 2025-01-03 07:19:29,443][0m Trial 2 finished with value: 0.15490985713128386 and parameters: {'observation_period_num': 198, 'train_rates': 0.7473396188244403, 'learning_rate': 0.00025668896883340546, 'batch_size': 133, 'step_size': 12, 'gamma': 0.9039630180448259}. Best is trial 2 with value: 0.15490985713128386.[0m
[32m[I 2025-01-03 07:20:13,494][0m Trial 3 finished with value: 1.130345174244472 and parameters: {'observation_period_num': 108, 'train_rates': 0.847596979715546, 'learning_rate': 1.770847735118905e-06, 'batch_size': 124, 'step_size': 3, 'gamma': 0.8893685788901061}. Best is trial 2 with value: 0.15490985713128386.[0m
[32m[I 2025-01-03 07:20:49,320][0m Trial 4 finished with value: 0.13496813244926623 and parameters: {'observation_period_num': 205, 'train_rates': 0.8582405480128357, 'learning_rate': 0.0001336665278519376, 'batch_size': 151, 'step_size': 6, 'gamma': 0.8752779633940098}. Best is trial 4 with value: 0.13496813244926623.[0m
[32m[I 2025-01-03 07:21:17,454][0m Trial 5 finished with value: 0.20058003004335423 and parameters: {'observation_period_num': 236, 'train_rates': 0.793237922481137, 'learning_rate': 6.50037858945845e-05, 'batch_size': 180, 'step_size': 14, 'gamma': 0.9470015096465163}. Best is trial 4 with value: 0.13496813244926623.[0m
[32m[I 2025-01-03 07:21:48,605][0m Trial 6 finished with value: 0.4242319559950305 and parameters: {'observation_period_num': 109, 'train_rates': 0.7082649572308605, 'learning_rate': 7.354623819331157e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.9292189714662544}. Best is trial 4 with value: 0.13496813244926623.[0m
[32m[I 2025-01-03 07:22:54,878][0m Trial 7 finished with value: 0.5855409059413644 and parameters: {'observation_period_num': 142, 'train_rates': 0.7857317878786036, 'learning_rate': 5.2911845888885476e-06, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8183827128372748}. Best is trial 4 with value: 0.13496813244926623.[0m
[32m[I 2025-01-03 07:23:40,037][0m Trial 8 finished with value: 0.15650385830594205 and parameters: {'observation_period_num': 162, 'train_rates': 0.688073908937611, 'learning_rate': 0.00032513175777899015, 'batch_size': 106, 'step_size': 15, 'gamma': 0.7999138851938287}. Best is trial 4 with value: 0.13496813244926623.[0m
[32m[I 2025-01-03 07:28:26,557][0m Trial 9 finished with value: 0.10339574271521601 and parameters: {'observation_period_num': 126, 'train_rates': 0.7400436994680202, 'learning_rate': 4.251646017888157e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9310299267348379}. Best is trial 9 with value: 0.10339574271521601.[0m
[32m[I 2025-01-03 07:28:47,094][0m Trial 10 finished with value: 0.06886262766139578 and parameters: {'observation_period_num': 31, 'train_rates': 0.6102150184595277, 'learning_rate': 0.0009954097423236297, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9720138482738818}. Best is trial 10 with value: 0.06886262766139578.[0m
[32m[I 2025-01-03 07:29:05,518][0m Trial 11 finished with value: 0.06853532907512383 and parameters: {'observation_period_num': 28, 'train_rates': 0.6080964677548661, 'learning_rate': 0.000588476895922246, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9810394307963638}. Best is trial 11 with value: 0.06853532907512383.[0m
[32m[I 2025-01-03 07:29:26,056][0m Trial 12 finished with value: 0.06138267128338303 and parameters: {'observation_period_num': 21, 'train_rates': 0.6111318136333371, 'learning_rate': 0.0009917418860812858, 'batch_size': 249, 'step_size': 8, 'gamma': 0.9764023942459649}. Best is trial 12 with value: 0.06138267128338303.[0m
[32m[I 2025-01-03 07:29:52,577][0m Trial 13 finished with value: 0.05841583013534546 and parameters: {'observation_period_num': 7, 'train_rates': 0.9462749773644112, 'learning_rate': 0.0009878858085254113, 'batch_size': 248, 'step_size': 6, 'gamma': 0.9789269990888125}. Best is trial 13 with value: 0.05841583013534546.[0m
[32m[I 2025-01-03 07:30:23,567][0m Trial 14 finished with value: 0.03742504492402077 and parameters: {'observation_period_num': 6, 'train_rates': 0.9719948974510483, 'learning_rate': 0.0009436113823174326, 'batch_size': 211, 'step_size': 6, 'gamma': 0.7548079574179137}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:30:54,605][0m Trial 15 finished with value: 0.09708698838949203 and parameters: {'observation_period_num': 72, 'train_rates': 0.9774819061587637, 'learning_rate': 0.00018055503577255386, 'batch_size': 208, 'step_size': 5, 'gamma': 0.7605098627797675}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:31:25,785][0m Trial 16 finished with value: 0.23982177674770355 and parameters: {'observation_period_num': 65, 'train_rates': 0.9826566163184127, 'learning_rate': 1.9777450390783866e-05, 'batch_size': 213, 'step_size': 7, 'gamma': 0.8329681054289729}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:31:54,726][0m Trial 17 finished with value: 0.07084503769874573 and parameters: {'observation_period_num': 9, 'train_rates': 0.9319552262965751, 'learning_rate': 8.824528963995936e-05, 'batch_size': 211, 'step_size': 6, 'gamma': 0.7658436799729615}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:32:27,753][0m Trial 18 finished with value: 0.0527297773453743 and parameters: {'observation_period_num': 60, 'train_rates': 0.9243696660644739, 'learning_rate': 0.00033098756789611277, 'batch_size': 181, 'step_size': 4, 'gamma': 0.8378347258084589}. Best is trial 14 with value: 0.03742504492402077.[0m
Early stopping at epoch 78
[32m[I 2025-01-03 07:32:53,953][0m Trial 19 finished with value: 0.16916509602090407 and parameters: {'observation_period_num': 56, 'train_rates': 0.9201084828061382, 'learning_rate': 0.00042789240440912476, 'batch_size': 181, 'step_size': 1, 'gamma': 0.8440742200653399}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:33:24,862][0m Trial 20 finished with value: 0.4374809899637776 and parameters: {'observation_period_num': 88, 'train_rates': 0.891950599899947, 'learning_rate': 2.0669435481177437e-05, 'batch_size': 185, 'step_size': 4, 'gamma': 0.7863492452266962}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:33:51,665][0m Trial 21 finished with value: 0.0477062352001667 and parameters: {'observation_period_num': 42, 'train_rates': 0.9500995764102584, 'learning_rate': 0.0005669046376294702, 'batch_size': 232, 'step_size': 5, 'gamma': 0.8529343451653271}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:34:20,440][0m Trial 22 finished with value: 0.05550482237743119 and parameters: {'observation_period_num': 44, 'train_rates': 0.8987397374486945, 'learning_rate': 0.00044022519879977523, 'batch_size': 215, 'step_size': 3, 'gamma': 0.8490630479493693}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:34:47,124][0m Trial 23 finished with value: 0.10000263899564743 and parameters: {'observation_period_num': 84, 'train_rates': 0.9507192118002441, 'learning_rate': 0.00017324854557633718, 'batch_size': 230, 'step_size': 5, 'gamma': 0.8599295307366555}. Best is trial 14 with value: 0.03742504492402077.[0m
[32m[I 2025-01-03 07:35:21,753][0m Trial 24 finished with value: 0.034980446100234985 and parameters: {'observation_period_num': 35, 'train_rates': 0.9896566571848633, 'learning_rate': 0.0005505752719049156, 'batch_size': 191, 'step_size': 7, 'gamma': 0.8182075590027829}. Best is trial 24 with value: 0.034980446100234985.[0m
[32m[I 2025-01-03 07:35:50,675][0m Trial 25 finished with value: 0.05077194422483444 and parameters: {'observation_period_num': 40, 'train_rates': 0.9666643857665022, 'learning_rate': 0.0005091719326088031, 'batch_size': 230, 'step_size': 7, 'gamma': 0.8072117523296259}. Best is trial 24 with value: 0.034980446100234985.[0m
[32m[I 2025-01-03 07:36:23,364][0m Trial 26 finished with value: 0.01660703308880329 and parameters: {'observation_period_num': 16, 'train_rates': 0.9892018627975194, 'learning_rate': 0.0006470870502252402, 'batch_size': 195, 'step_size': 10, 'gamma': 0.7517125055085089}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:37:04,164][0m Trial 27 finished with value: 0.04639250412583351 and parameters: {'observation_period_num': 7, 'train_rates': 0.9899606817306498, 'learning_rate': 9.541298554738288e-05, 'batch_size': 157, 'step_size': 11, 'gamma': 0.7504231144244585}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:37:39,352][0m Trial 28 finished with value: 0.03711567096179351 and parameters: {'observation_period_num': 25, 'train_rates': 0.8907594813229247, 'learning_rate': 0.00022522910816070128, 'batch_size': 169, 'step_size': 10, 'gamma': 0.7819708732408956}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:38:35,758][0m Trial 29 finished with value: 0.044385631848698016 and parameters: {'observation_period_num': 90, 'train_rates': 0.8861251629886472, 'learning_rate': 0.00024207674781929892, 'batch_size': 100, 'step_size': 12, 'gamma': 0.7852038692314804}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:39:05,114][0m Trial 30 finished with value: 0.07831143401563168 and parameters: {'observation_period_num': 23, 'train_rates': 0.8212346182952743, 'learning_rate': 4.6779734014761015e-05, 'batch_size': 191, 'step_size': 10, 'gamma': 0.777250558101453}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:39:37,654][0m Trial 31 finished with value: 0.04932466521859169 and parameters: {'observation_period_num': 50, 'train_rates': 0.9635756858848715, 'learning_rate': 0.0006488959001888618, 'batch_size': 199, 'step_size': 8, 'gamma': 0.7508475084270766}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:40:12,633][0m Trial 32 finished with value: 0.024844522163429868 and parameters: {'observation_period_num': 18, 'train_rates': 0.903795961045008, 'learning_rate': 0.0007193653897884748, 'batch_size': 166, 'step_size': 10, 'gamma': 0.7739635327063701}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:40:42,055][0m Trial 33 finished with value: 0.036406135792995606 and parameters: {'observation_period_num': 26, 'train_rates': 0.8235829554697465, 'learning_rate': 0.00023568408412453988, 'batch_size': 173, 'step_size': 13, 'gamma': 0.7731215939948518}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:41:18,445][0m Trial 34 finished with value: 0.04614396318793297 and parameters: {'observation_period_num': 70, 'train_rates': 0.8090760701806068, 'learning_rate': 0.00032921698000637386, 'batch_size': 140, 'step_size': 13, 'gamma': 0.801762066663311}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:41:51,366][0m Trial 35 finished with value: 0.0416462727958903 and parameters: {'observation_period_num': 36, 'train_rates': 0.8433367108077372, 'learning_rate': 0.0001234720527817469, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8203607079007029}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:42:30,409][0m Trial 36 finished with value: 0.04034284513753317 and parameters: {'observation_period_num': 52, 'train_rates': 0.8713133052557984, 'learning_rate': 0.0007217843029841812, 'batch_size': 137, 'step_size': 11, 'gamma': 0.7699187680943851}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:43:11,760][0m Trial 37 finished with value: 0.7369873551651835 and parameters: {'observation_period_num': 219, 'train_rates': 0.765702349204218, 'learning_rate': 2.6190802285262195e-06, 'batch_size': 115, 'step_size': 12, 'gamma': 0.794154889027297}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:44:20,576][0m Trial 38 finished with value: 0.11334744134545326 and parameters: {'observation_period_num': 168, 'train_rates': 0.9103187276484154, 'learning_rate': 0.0003659890713484697, 'batch_size': 77, 'step_size': 14, 'gamma': 0.813712523049537}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:45:01,908][0m Trial 39 finished with value: 0.045323283350824015 and parameters: {'observation_period_num': 20, 'train_rates': 0.9374421020519398, 'learning_rate': 0.00017317058221801409, 'batch_size': 149, 'step_size': 10, 'gamma': 0.7749088030662334}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:45:35,425][0m Trial 40 finished with value: 0.06704654917120934 and parameters: {'observation_period_num': 106, 'train_rates': 0.8666061271352206, 'learning_rate': 0.0006893805920108719, 'batch_size': 165, 'step_size': 14, 'gamma': 0.8280580629199313}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:46:05,247][0m Trial 41 finished with value: 0.0326308618423716 and parameters: {'observation_period_num': 17, 'train_rates': 0.8286711970890555, 'learning_rate': 0.00029106137607090566, 'batch_size': 191, 'step_size': 10, 'gamma': 0.7884132768978503}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:46:34,681][0m Trial 42 finished with value: 0.046462047475238216 and parameters: {'observation_period_num': 20, 'train_rates': 0.8077587082183083, 'learning_rate': 0.0002594540450332676, 'batch_size': 196, 'step_size': 8, 'gamma': 0.7925954298114636}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:47:02,673][0m Trial 43 finished with value: 0.05049461374680201 and parameters: {'observation_period_num': 33, 'train_rates': 0.7739812870715755, 'learning_rate': 0.00012536718564592965, 'batch_size': 200, 'step_size': 10, 'gamma': 0.7656736474963464}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:47:36,420][0m Trial 44 finished with value: 1.1032539010047913 and parameters: {'observation_period_num': 16, 'train_rates': 0.8324904512156611, 'learning_rate': 1.011698626749487e-06, 'batch_size': 171, 'step_size': 11, 'gamma': 0.7738511509614359}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:48:16,213][0m Trial 45 finished with value: 0.04180154129603485 and parameters: {'observation_period_num': 31, 'train_rates': 0.7049834577480594, 'learning_rate': 0.0004216406743578644, 'batch_size': 127, 'step_size': 9, 'gamma': 0.8715405114225717}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:48:51,352][0m Trial 46 finished with value: 0.043727129758953556 and parameters: {'observation_period_num': 47, 'train_rates': 0.7482891912032832, 'learning_rate': 0.0006551514270589713, 'batch_size': 149, 'step_size': 13, 'gamma': 0.8065197709378901}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:50:27,277][0m Trial 47 finished with value: 0.13336753349568545 and parameters: {'observation_period_num': 144, 'train_rates': 0.6657290158980859, 'learning_rate': 0.00027165628204038275, 'batch_size': 45, 'step_size': 7, 'gamma': 0.7914261390591532}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:51:01,110][0m Trial 48 finished with value: 0.022271791947478393 and parameters: {'observation_period_num': 13, 'train_rates': 0.8556844134593362, 'learning_rate': 0.0007904216433539647, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8988159665926764}. Best is trial 26 with value: 0.01660703308880329.[0m
[32m[I 2025-01-03 07:51:32,806][0m Trial 49 finished with value: 0.023649671003558834 and parameters: {'observation_period_num': 14, 'train_rates': 0.8491766685337472, 'learning_rate': 0.0007466780455418606, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8939013455133276}. Best is trial 26 with value: 0.01660703308880329.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-03 07:51:32,816][0m A new study created in memory with name: no-name-324735a9-c500-40e0-aa4a-f3cb7159d5d4[0m
[32m[I 2025-01-03 07:57:20,717][0m Trial 0 finished with value: 0.5412929421121424 and parameters: {'observation_period_num': 115, 'train_rates': 0.9612682809556471, 'learning_rate': 1.2586409926500435e-06, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8957045278649759}. Best is trial 0 with value: 0.5412929421121424.[0m
[32m[I 2025-01-03 07:58:47,296][0m Trial 1 finished with value: 0.15056328113671685 and parameters: {'observation_period_num': 230, 'train_rates': 0.6172711156510091, 'learning_rate': 0.0006096749040129139, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8215056710111802}. Best is trial 1 with value: 0.15056328113671685.[0m
[32m[I 2025-01-03 08:02:10,969][0m Trial 2 finished with value: 0.0786033593063807 and parameters: {'observation_period_num': 136, 'train_rates': 0.8053290808425603, 'learning_rate': 3.2023583285293846e-05, 'batch_size': 24, 'step_size': 8, 'gamma': 0.7501653565004855}. Best is trial 2 with value: 0.0786033593063807.[0m
[32m[I 2025-01-03 08:03:37,885][0m Trial 3 finished with value: 0.21588962537616324 and parameters: {'observation_period_num': 25, 'train_rates': 0.8166722712057566, 'learning_rate': 4.1755552858770245e-06, 'batch_size': 60, 'step_size': 5, 'gamma': 0.8825960137660314}. Best is trial 2 with value: 0.0786033593063807.[0m
[32m[I 2025-01-03 08:05:31,339][0m Trial 4 finished with value: 0.09765624197629783 and parameters: {'observation_period_num': 220, 'train_rates': 0.9764578453047467, 'learning_rate': 0.0003718620012601286, 'batch_size': 49, 'step_size': 11, 'gamma': 0.9742812484440532}. Best is trial 2 with value: 0.0786033593063807.[0m
[32m[I 2025-01-03 08:06:28,280][0m Trial 5 finished with value: 0.08522019242165518 and parameters: {'observation_period_num': 124, 'train_rates': 0.9354219398293093, 'learning_rate': 9.86732222713906e-05, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8817236144768454}. Best is trial 2 with value: 0.0786033593063807.[0m
[32m[I 2025-01-03 08:07:59,504][0m Trial 6 finished with value: 0.5666250546037415 and parameters: {'observation_period_num': 247, 'train_rates': 0.9402049596787392, 'learning_rate': 3.011199641835052e-06, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9019199043367663}. Best is trial 2 with value: 0.0786033593063807.[0m
[32m[I 2025-01-03 08:08:24,069][0m Trial 7 finished with value: 0.23322539191749228 and parameters: {'observation_period_num': 238, 'train_rates': 0.6333768687705444, 'learning_rate': 0.00022759285930975016, 'batch_size': 188, 'step_size': 5, 'gamma': 0.8217359219268872}. Best is trial 2 with value: 0.0786033593063807.[0m
[32m[I 2025-01-03 08:09:15,873][0m Trial 8 finished with value: 0.04669459035602241 and parameters: {'observation_period_num': 56, 'train_rates': 0.8921004955547854, 'learning_rate': 0.0006241021560934033, 'batch_size': 110, 'step_size': 11, 'gamma': 0.8020344369084015}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:12:44,287][0m Trial 9 finished with value: 0.09914347808808088 and parameters: {'observation_period_num': 88, 'train_rates': 0.8605732563136042, 'learning_rate': 7.933218577384961e-05, 'batch_size': 25, 'step_size': 10, 'gamma': 0.781086415332418}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:13:05,749][0m Trial 10 finished with value: 0.12244269946633383 and parameters: {'observation_period_num': 15, 'train_rates': 0.7071044620622283, 'learning_rate': 1.3094422122250969e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.8266701222247902}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:13:40,314][0m Trial 11 finished with value: 0.26892135424285823 and parameters: {'observation_period_num': 173, 'train_rates': 0.7540720830223834, 'learning_rate': 2.5015710437945513e-05, 'batch_size': 140, 'step_size': 13, 'gamma': 0.7549554228296987}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:14:32,227][0m Trial 12 finished with value: 0.06255576302524374 and parameters: {'observation_period_num': 80, 'train_rates': 0.8759025128037823, 'learning_rate': 9.209232587180637e-05, 'batch_size': 109, 'step_size': 8, 'gamma': 0.7521615343074703}. Best is trial 8 with value: 0.04669459035602241.[0m
Early stopping at epoch 73
[32m[I 2025-01-03 08:15:06,916][0m Trial 13 finished with value: 0.11705564943755545 and parameters: {'observation_period_num': 63, 'train_rates': 0.8725094913451605, 'learning_rate': 0.000999542485929486, 'batch_size': 122, 'step_size': 1, 'gamma': 0.7905754549383269}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:15:39,647][0m Trial 14 finished with value: 0.06449154895894668 and parameters: {'observation_period_num': 60, 'train_rates': 0.8826839971713519, 'learning_rate': 0.00014410143884941646, 'batch_size': 176, 'step_size': 10, 'gamma': 0.7867876809328398}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:16:35,473][0m Trial 15 finished with value: 0.05473972470339002 and parameters: {'observation_period_num': 53, 'train_rates': 0.8999561111221975, 'learning_rate': 5.7718052219608587e-05, 'batch_size': 101, 'step_size': 12, 'gamma': 0.8469509736148068}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:17:42,925][0m Trial 16 finished with value: 0.13950842918186893 and parameters: {'observation_period_num': 40, 'train_rates': 0.9162493648945143, 'learning_rate': 9.816041077963168e-06, 'batch_size': 85, 'step_size': 13, 'gamma': 0.850977026962811}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:18:17,663][0m Trial 17 finished with value: 0.05689868401090575 and parameters: {'observation_period_num': 6, 'train_rates': 0.7510624928137535, 'learning_rate': 3.258908447088611e-05, 'batch_size': 153, 'step_size': 15, 'gamma': 0.9344900221108543}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:18:43,311][0m Trial 18 finished with value: 0.09238023219766023 and parameters: {'observation_period_num': 168, 'train_rates': 0.8326012343189133, 'learning_rate': 0.00026126384471222657, 'batch_size': 219, 'step_size': 12, 'gamma': 0.8513914604455508}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:19:45,081][0m Trial 19 finished with value: 0.10920888015741034 and parameters: {'observation_period_num': 97, 'train_rates': 0.9171059884713436, 'learning_rate': 0.0009675642664170677, 'batch_size': 93, 'step_size': 13, 'gamma': 0.9236477870505555}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:20:19,831][0m Trial 20 finished with value: 0.06752734735137088 and parameters: {'observation_period_num': 48, 'train_rates': 0.7601661637217894, 'learning_rate': 5.9831029367033724e-05, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8473597751375845}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:20:53,800][0m Trial 21 finished with value: 0.061242342745083556 and parameters: {'observation_period_num': 16, 'train_rates': 0.681853660072287, 'learning_rate': 3.655456714015625e-05, 'batch_size': 148, 'step_size': 15, 'gamma': 0.9490483608348768}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:21:37,285][0m Trial 22 finished with value: 0.08759567609726482 and parameters: {'observation_period_num': 36, 'train_rates': 0.7730950272711726, 'learning_rate': 1.4600767310720578e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.9174683961016469}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:22:08,272][0m Trial 23 finished with value: 0.0995118647182077 and parameters: {'observation_period_num': 13, 'train_rates': 0.7271657368225858, 'learning_rate': 8.338338608875458e-06, 'batch_size': 175, 'step_size': 12, 'gamma': 0.978733016503431}. Best is trial 8 with value: 0.04669459035602241.[0m
[32m[I 2025-01-03 08:23:09,575][0m Trial 24 finished with value: 0.03578380337860755 and parameters: {'observation_period_num': 5, 'train_rates': 0.6642411546912707, 'learning_rate': 4.863036392835427e-05, 'batch_size': 77, 'step_size': 15, 'gamma': 0.9528031138286387}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:24:10,873][0m Trial 25 finished with value: 0.061837895553559064 and parameters: {'observation_period_num': 68, 'train_rates': 0.6530964158746088, 'learning_rate': 0.00014940322727757102, 'batch_size': 73, 'step_size': 11, 'gamma': 0.8043425818826512}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:25:25,754][0m Trial 26 finished with value: 0.04689787299587177 and parameters: {'observation_period_num': 40, 'train_rates': 0.9063497399307777, 'learning_rate': 0.0004215302308304326, 'batch_size': 76, 'step_size': 14, 'gamma': 0.8658792298684469}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:26:36,243][0m Trial 27 finished with value: 0.05766356935103734 and parameters: {'observation_period_num': 35, 'train_rates': 0.8456852244839131, 'learning_rate': 0.0005153538740131639, 'batch_size': 78, 'step_size': 14, 'gamma': 0.9600278702243152}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:27:17,186][0m Trial 28 finished with value: 0.07974909050432266 and parameters: {'observation_period_num': 107, 'train_rates': 0.6893598395281234, 'learning_rate': 0.0003081386422845092, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9897018122893668}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:28:48,158][0m Trial 29 finished with value: 0.7970549464225769 and parameters: {'observation_period_num': 75, 'train_rates': 0.980871536975572, 'learning_rate': 1.0776525929919747e-06, 'batch_size': 65, 'step_size': 9, 'gamma': 0.869988910369506}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:31:20,247][0m Trial 30 finished with value: 0.040311285103839564 and parameters: {'observation_period_num': 39, 'train_rates': 0.9432345588733945, 'learning_rate': 0.0005549395175068904, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8973159193941164}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:34:21,303][0m Trial 31 finished with value: 0.04790556836215889 and parameters: {'observation_period_num': 29, 'train_rates': 0.9537788424525402, 'learning_rate': 0.0006942553623680262, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8937808861780411}. Best is trial 24 with value: 0.03578380337860755.[0m
[32m[I 2025-01-03 08:37:06,795][0m Trial 32 finished with value: 0.0294131519459188 and parameters: {'observation_period_num': 5, 'train_rates': 0.9068984148999492, 'learning_rate': 0.0004028195072095911, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9137540057881506}. Best is trial 32 with value: 0.0294131519459188.[0m
[32m[I 2025-01-03 08:39:34,031][0m Trial 33 finished with value: 0.035698611059806484 and parameters: {'observation_period_num': 5, 'train_rates': 0.9533192104989914, 'learning_rate': 0.000188207962423521, 'batch_size': 40, 'step_size': 15, 'gamma': 0.9343698334614623}. Best is trial 32 with value: 0.0294131519459188.[0m
[32m[I 2025-01-03 08:45:43,813][0m Trial 34 finished with value: 0.019142864152598076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9867856155777148, 'learning_rate': 0.00019211171180108288, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9446066996476656}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 08:49:12,155][0m Trial 35 finished with value: 0.1047012075057107 and parameters: {'observation_period_num': 19, 'train_rates': 0.6001828285579084, 'learning_rate': 0.00016215179695023627, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9448959659305897}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 08:51:24,489][0m Trial 36 finished with value: 0.023867916650346762 and parameters: {'observation_period_num': 5, 'train_rates': 0.9650270599177405, 'learning_rate': 0.0001832673815501466, 'batch_size': 45, 'step_size': 15, 'gamma': 0.9185344325675374}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 08:53:37,750][0m Trial 37 finished with value: 0.02239108271896839 and parameters: {'observation_period_num': 21, 'train_rates': 0.9898611141708367, 'learning_rate': 0.00020131288584025162, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9154045718744586}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 08:55:34,090][0m Trial 38 finished with value: 0.025972183793783188 and parameters: {'observation_period_num': 23, 'train_rates': 0.9875836507140632, 'learning_rate': 0.00010619637405244934, 'batch_size': 52, 'step_size': 13, 'gamma': 0.9112650607386248}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 08:57:23,538][0m Trial 39 finished with value: 0.11567486822605133 and parameters: {'observation_period_num': 149, 'train_rates': 0.9889783725598411, 'learning_rate': 8.430089054184459e-05, 'batch_size': 53, 'step_size': 7, 'gamma': 0.9070896739406833}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 08:59:18,553][0m Trial 40 finished with value: 0.03717631945743206 and parameters: {'observation_period_num': 24, 'train_rates': 0.9680240607395103, 'learning_rate': 0.00012971294108083856, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8840920319732646}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:05:02,671][0m Trial 41 finished with value: 0.028813196967045467 and parameters: {'observation_period_num': 25, 'train_rates': 0.9887265182308964, 'learning_rate': 0.00035600659727167056, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9112369928517178}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:11:06,034][0m Trial 42 finished with value: 0.03180879663216741 and parameters: {'observation_period_num': 26, 'train_rates': 0.9752752477171633, 'learning_rate': 0.00024432385496377573, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9308881337628563}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:14:30,617][0m Trial 43 finished with value: 0.11920027700630394 and parameters: {'observation_period_num': 200, 'train_rates': 0.932926301241098, 'learning_rate': 0.00011262588290568954, 'batch_size': 26, 'step_size': 13, 'gamma': 0.9654684542860327}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:16:41,543][0m Trial 44 finished with value: 0.09085096118100193 and parameters: {'observation_period_num': 23, 'train_rates': 0.9627628550406788, 'learning_rate': 0.00020645652126106037, 'batch_size': 45, 'step_size': 11, 'gamma': 0.8871351470952364}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:18:16,662][0m Trial 45 finished with value: 0.04525009170174599 and parameters: {'observation_period_num': 49, 'train_rates': 0.9865906266564708, 'learning_rate': 0.00038732471318784887, 'batch_size': 63, 'step_size': 6, 'gamma': 0.9082987525347787}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:23:47,153][0m Trial 46 finished with value: 0.046164614029545734 and parameters: {'observation_period_num': 29, 'train_rates': 0.9357447781840336, 'learning_rate': 0.0003060220062754233, 'batch_size': 17, 'step_size': 2, 'gamma': 0.925017200263039}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:26:48,114][0m Trial 47 finished with value: 0.12254717020155156 and parameters: {'observation_period_num': 130, 'train_rates': 0.9635216812813217, 'learning_rate': 0.00011194238400618725, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8987562224287907}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:28:41,330][0m Trial 48 finished with value: 0.20194884991031056 and parameters: {'observation_period_num': 16, 'train_rates': 0.9240968881172972, 'learning_rate': 1.937230837014468e-06, 'batch_size': 51, 'step_size': 12, 'gamma': 0.9417966969816517}. Best is trial 34 with value: 0.019142864152598076.[0m
[32m[I 2025-01-03 09:31:02,678][0m Trial 49 finished with value: 0.08377574384212494 and parameters: {'observation_period_num': 46, 'train_rates': 0.989407550486663, 'learning_rate': 0.00020809636258146407, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9227716002703436}. Best is trial 34 with value: 0.019142864152598076.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 17, 'train_rates': 0.8834213065517028, 'learning_rate': 0.00010200149933914854, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8418145174823586}
Epoch 1/300, trend Loss: 0.3297 | 0.1408
Epoch 2/300, trend Loss: 0.1279 | 0.0997
Epoch 3/300, trend Loss: 0.1091 | 0.0896
Epoch 4/300, trend Loss: 0.1019 | 0.0837
Epoch 5/300, trend Loss: 0.0982 | 0.0728
Epoch 6/300, trend Loss: 0.0959 | 0.0623
Epoch 7/300, trend Loss: 0.0937 | 0.0586
Epoch 8/300, trend Loss: 0.0903 | 0.0541
Epoch 9/300, trend Loss: 0.0876 | 0.0509
Epoch 10/300, trend Loss: 0.0845 | 0.0494
Epoch 11/300, trend Loss: 0.0822 | 0.0484
Epoch 12/300, trend Loss: 0.0800 | 0.0478
Epoch 13/300, trend Loss: 0.0780 | 0.0476
Epoch 14/300, trend Loss: 0.0755 | 0.0427
Epoch 15/300, trend Loss: 0.0735 | 0.0426
Epoch 16/300, trend Loss: 0.0723 | 0.0434
Epoch 17/300, trend Loss: 0.0714 | 0.0445
Epoch 18/300, trend Loss: 0.0706 | 0.0454
Epoch 19/300, trend Loss: 0.0698 | 0.0458
Epoch 20/300, trend Loss: 0.0690 | 0.0454
Epoch 21/300, trend Loss: 0.0678 | 0.0409
Epoch 22/300, trend Loss: 0.0666 | 0.0380
Epoch 23/300, trend Loss: 0.0657 | 0.0354
Epoch 24/300, trend Loss: 0.0650 | 0.0341
Epoch 25/300, trend Loss: 0.0644 | 0.0335
Epoch 26/300, trend Loss: 0.0638 | 0.0329
Epoch 27/300, trend Loss: 0.0631 | 0.0328
Epoch 28/300, trend Loss: 0.0625 | 0.0319
Epoch 29/300, trend Loss: 0.0619 | 0.0311
Epoch 30/300, trend Loss: 0.0614 | 0.0306
Epoch 31/300, trend Loss: 0.0609 | 0.0302
Epoch 32/300, trend Loss: 0.0605 | 0.0299
Epoch 33/300, trend Loss: 0.0601 | 0.0298
Epoch 34/300, trend Loss: 0.0595 | 0.0295
Epoch 35/300, trend Loss: 0.0591 | 0.0293
Epoch 36/300, trend Loss: 0.0587 | 0.0291
Epoch 37/300, trend Loss: 0.0583 | 0.0291
Epoch 38/300, trend Loss: 0.0580 | 0.0291
Epoch 39/300, trend Loss: 0.0577 | 0.0292
Epoch 40/300, trend Loss: 0.0573 | 0.0292
Epoch 41/300, trend Loss: 0.0570 | 0.0292
Epoch 42/300, trend Loss: 0.0568 | 0.0293
Epoch 43/300, trend Loss: 0.0565 | 0.0295
Epoch 44/300, trend Loss: 0.0562 | 0.0296
Epoch 45/300, trend Loss: 0.0560 | 0.0297
Epoch 46/300, trend Loss: 0.0557 | 0.0297
Epoch 47/300, trend Loss: 0.0554 | 0.0296
Epoch 48/300, trend Loss: 0.0552 | 0.0296
Epoch 49/300, trend Loss: 0.0550 | 0.0296
Epoch 50/300, trend Loss: 0.0548 | 0.0295
Epoch 51/300, trend Loss: 0.0546 | 0.0294
Epoch 52/300, trend Loss: 0.0544 | 0.0294
Epoch 53/300, trend Loss: 0.0542 | 0.0278
Epoch 54/300, trend Loss: 0.0540 | 0.0275
Epoch 55/300, trend Loss: 0.0538 | 0.0274
Epoch 56/300, trend Loss: 0.0536 | 0.0273
Epoch 57/300, trend Loss: 0.0534 | 0.0272
Epoch 58/300, trend Loss: 0.0532 | 0.0272
Epoch 59/300, trend Loss: 0.0530 | 0.0272
Epoch 60/300, trend Loss: 0.0527 | 0.0266
Epoch 61/300, trend Loss: 0.0526 | 0.0265
Epoch 62/300, trend Loss: 0.0524 | 0.0265
Epoch 63/300, trend Loss: 0.0522 | 0.0266
Epoch 64/300, trend Loss: 0.0520 | 0.0267
Epoch 65/300, trend Loss: 0.0519 | 0.0268
Epoch 66/300, trend Loss: 0.0516 | 0.0264
Epoch 67/300, trend Loss: 0.0515 | 0.0265
Epoch 68/300, trend Loss: 0.0513 | 0.0266
Epoch 69/300, trend Loss: 0.0512 | 0.0267
Epoch 70/300, trend Loss: 0.0511 | 0.0268
Epoch 71/300, trend Loss: 0.0509 | 0.0269
Epoch 72/300, trend Loss: 0.0508 | 0.0270
Epoch 73/300, trend Loss: 0.0506 | 0.0266
Epoch 74/300, trend Loss: 0.0505 | 0.0266
Epoch 75/300, trend Loss: 0.0503 | 0.0267
Epoch 76/300, trend Loss: 0.0502 | 0.0267
Epoch 77/300, trend Loss: 0.0501 | 0.0268
Epoch 78/300, trend Loss: 0.0500 | 0.0269
Epoch 79/300, trend Loss: 0.0498 | 0.0265
Epoch 80/300, trend Loss: 0.0497 | 0.0265
Epoch 81/300, trend Loss: 0.0496 | 0.0266
Epoch 82/300, trend Loss: 0.0495 | 0.0266
Epoch 83/300, trend Loss: 0.0494 | 0.0267
Epoch 84/300, trend Loss: 0.0493 | 0.0267
Epoch 85/300, trend Loss: 0.0492 | 0.0268
Epoch 86/300, trend Loss: 0.0490 | 0.0268
Epoch 87/300, trend Loss: 0.0489 | 0.0268
Epoch 88/300, trend Loss: 0.0488 | 0.0268
Epoch 89/300, trend Loss: 0.0487 | 0.0268
Epoch 90/300, trend Loss: 0.0486 | 0.0268
Epoch 91/300, trend Loss: 0.0485 | 0.0268
Epoch 92/300, trend Loss: 0.0484 | 0.0266
Epoch 93/300, trend Loss: 0.0484 | 0.0265
Epoch 94/300, trend Loss: 0.0483 | 0.0265
Epoch 95/300, trend Loss: 0.0482 | 0.0265
Epoch 96/300, trend Loss: 0.0481 | 0.0265
Epoch 97/300, trend Loss: 0.0480 | 0.0265
Epoch 98/300, trend Loss: 0.0478 | 0.0265
Epoch 99/300, trend Loss: 0.0477 | 0.0262
Epoch 100/300, trend Loss: 0.0476 | 0.0262
Epoch 101/300, trend Loss: 0.0475 | 0.0263
Epoch 102/300, trend Loss: 0.0474 | 0.0263
Epoch 103/300, trend Loss: 0.0472 | 0.0263
Epoch 104/300, trend Loss: 0.0471 | 0.0264
Epoch 105/300, trend Loss: 0.0469 | 0.0264
Epoch 106/300, trend Loss: 0.0468 | 0.0264
Epoch 107/300, trend Loss: 0.0467 | 0.0265
Epoch 108/300, trend Loss: 0.0466 | 0.0265
Epoch 109/300, trend Loss: 0.0465 | 0.0265
Epoch 110/300, trend Loss: 0.0464 | 0.0266
Epoch 111/300, trend Loss: 0.0463 | 0.0266
Epoch 112/300, trend Loss: 0.0461 | 0.0268
Epoch 113/300, trend Loss: 0.0460 | 0.0268
Epoch 114/300, trend Loss: 0.0459 | 0.0269
Epoch 115/300, trend Loss: 0.0458 | 0.0269
Epoch 116/300, trend Loss: 0.0457 | 0.0269
Epoch 117/300, trend Loss: 0.0456 | 0.0269
Epoch 118/300, trend Loss: 0.0454 | 0.0272
Epoch 119/300, trend Loss: 0.0453 | 0.0272
Epoch 120/300, trend Loss: 0.0453 | 0.0272
Epoch 121/300, trend Loss: 0.0452 | 0.0272
Epoch 122/300, trend Loss: 0.0451 | 0.0272
Epoch 123/300, trend Loss: 0.0450 | 0.0272
Epoch 124/300, trend Loss: 0.0449 | 0.0272
Epoch 125/300, trend Loss: 0.0448 | 0.0274
Epoch 126/300, trend Loss: 0.0447 | 0.0274
Epoch 127/300, trend Loss: 0.0446 | 0.0274
Epoch 128/300, trend Loss: 0.0445 | 0.0274
Epoch 129/300, trend Loss: 0.0444 | 0.0274
Epoch 130/300, trend Loss: 0.0443 | 0.0274
Epoch 131/300, trend Loss: 0.0442 | 0.0276
Epoch 132/300, trend Loss: 0.0442 | 0.0276
Epoch 133/300, trend Loss: 0.0441 | 0.0276
Epoch 134/300, trend Loss: 0.0440 | 0.0276
Epoch 135/300, trend Loss: 0.0439 | 0.0276
Epoch 136/300, trend Loss: 0.0439 | 0.0276
Epoch 137/300, trend Loss: 0.0438 | 0.0276
Epoch 138/300, trend Loss: 0.0437 | 0.0277
Epoch 139/300, trend Loss: 0.0436 | 0.0277
Epoch 140/300, trend Loss: 0.0436 | 0.0277
Epoch 141/300, trend Loss: 0.0435 | 0.0277
Epoch 142/300, trend Loss: 0.0434 | 0.0277
Epoch 143/300, trend Loss: 0.0434 | 0.0277
Epoch 144/300, trend Loss: 0.0433 | 0.0278
Epoch 145/300, trend Loss: 0.0433 | 0.0278
Epoch 146/300, trend Loss: 0.0432 | 0.0278
Epoch 147/300, trend Loss: 0.0431 | 0.0278
Epoch 148/300, trend Loss: 0.0431 | 0.0278
Epoch 149/300, trend Loss: 0.0430 | 0.0278
Epoch 150/300, trend Loss: 0.0430 | 0.0278
Epoch 151/300, trend Loss: 0.0429 | 0.0279
Epoch 152/300, trend Loss: 0.0429 | 0.0279
Epoch 153/300, trend Loss: 0.0428 | 0.0279
Epoch 154/300, trend Loss: 0.0428 | 0.0279
Epoch 155/300, trend Loss: 0.0427 | 0.0279
Epoch 156/300, trend Loss: 0.0427 | 0.0279
Epoch 157/300, trend Loss: 0.0426 | 0.0278
Epoch 158/300, trend Loss: 0.0426 | 0.0278
Epoch 159/300, trend Loss: 0.0426 | 0.0278
Epoch 160/300, trend Loss: 0.0425 | 0.0278
Epoch 161/300, trend Loss: 0.0425 | 0.0278
Epoch 162/300, trend Loss: 0.0424 | 0.0278
Epoch 163/300, trend Loss: 0.0424 | 0.0279
Epoch 164/300, trend Loss: 0.0424 | 0.0277
Epoch 165/300, trend Loss: 0.0423 | 0.0277
Epoch 166/300, trend Loss: 0.0423 | 0.0277
Epoch 167/300, trend Loss: 0.0423 | 0.0277
Epoch 168/300, trend Loss: 0.0422 | 0.0277
Epoch 169/300, trend Loss: 0.0422 | 0.0277
Epoch 170/300, trend Loss: 0.0422 | 0.0276
Epoch 171/300, trend Loss: 0.0421 | 0.0276
Epoch 172/300, trend Loss: 0.0421 | 0.0276
Epoch 173/300, trend Loss: 0.0421 | 0.0276
Epoch 174/300, trend Loss: 0.0421 | 0.0276
Epoch 175/300, trend Loss: 0.0420 | 0.0276
Epoch 176/300, trend Loss: 0.0420 | 0.0276
Epoch 177/300, trend Loss: 0.0420 | 0.0275
Epoch 178/300, trend Loss: 0.0420 | 0.0275
Epoch 179/300, trend Loss: 0.0419 | 0.0275
Epoch 180/300, trend Loss: 0.0419 | 0.0275
Epoch 181/300, trend Loss: 0.0419 | 0.0275
Epoch 182/300, trend Loss: 0.0419 | 0.0275
Epoch 183/300, trend Loss: 0.0418 | 0.0274
Epoch 184/300, trend Loss: 0.0418 | 0.0274
Epoch 185/300, trend Loss: 0.0418 | 0.0274
Epoch 186/300, trend Loss: 0.0418 | 0.0274
Epoch 187/300, trend Loss: 0.0418 | 0.0274
Epoch 188/300, trend Loss: 0.0417 | 0.0274
Epoch 189/300, trend Loss: 0.0417 | 0.0274
Epoch 190/300, trend Loss: 0.0417 | 0.0274
Epoch 191/300, trend Loss: 0.0417 | 0.0274
Epoch 192/300, trend Loss: 0.0417 | 0.0274
Epoch 193/300, trend Loss: 0.0416 | 0.0274
Epoch 194/300, trend Loss: 0.0416 | 0.0274
Epoch 195/300, trend Loss: 0.0416 | 0.0274
Epoch 196/300, trend Loss: 0.0416 | 0.0274
Epoch 197/300, trend Loss: 0.0416 | 0.0274
Epoch 198/300, trend Loss: 0.0416 | 0.0274
Epoch 199/300, trend Loss: 0.0415 | 0.0274
Epoch 200/300, trend Loss: 0.0415 | 0.0274
Epoch 201/300, trend Loss: 0.0415 | 0.0274
Epoch 202/300, trend Loss: 0.0415 | 0.0274
Epoch 203/300, trend Loss: 0.0415 | 0.0274
Epoch 204/300, trend Loss: 0.0415 | 0.0274
Epoch 205/300, trend Loss: 0.0415 | 0.0274
Epoch 206/300, trend Loss: 0.0414 | 0.0274
Epoch 207/300, trend Loss: 0.0414 | 0.0274
Epoch 208/300, trend Loss: 0.0414 | 0.0274
Epoch 209/300, trend Loss: 0.0414 | 0.0274
Epoch 210/300, trend Loss: 0.0414 | 0.0274
Epoch 211/300, trend Loss: 0.0414 | 0.0274
Epoch 212/300, trend Loss: 0.0414 | 0.0274
Epoch 213/300, trend Loss: 0.0414 | 0.0274
Epoch 214/300, trend Loss: 0.0414 | 0.0274
Epoch 215/300, trend Loss: 0.0414 | 0.0274
Epoch 216/300, trend Loss: 0.0413 | 0.0274
Epoch 217/300, trend Loss: 0.0413 | 0.0274
Epoch 218/300, trend Loss: 0.0413 | 0.0274
Epoch 219/300, trend Loss: 0.0413 | 0.0274
Epoch 220/300, trend Loss: 0.0413 | 0.0274
Epoch 221/300, trend Loss: 0.0413 | 0.0274
Epoch 222/300, trend Loss: 0.0413 | 0.0274
Epoch 223/300, trend Loss: 0.0413 | 0.0274
Epoch 224/300, trend Loss: 0.0413 | 0.0274
Epoch 225/300, trend Loss: 0.0413 | 0.0274
Epoch 226/300, trend Loss: 0.0413 | 0.0273
Epoch 227/300, trend Loss: 0.0413 | 0.0273
Epoch 228/300, trend Loss: 0.0413 | 0.0273
Epoch 229/300, trend Loss: 0.0412 | 0.0273
Epoch 230/300, trend Loss: 0.0412 | 0.0273
Epoch 231/300, trend Loss: 0.0412 | 0.0273
Epoch 232/300, trend Loss: 0.0412 | 0.0273
Epoch 233/300, trend Loss: 0.0412 | 0.0273
Epoch 234/300, trend Loss: 0.0412 | 0.0273
Epoch 235/300, trend Loss: 0.0412 | 0.0273
Epoch 236/300, trend Loss: 0.0412 | 0.0273
Epoch 237/300, trend Loss: 0.0412 | 0.0273
Epoch 238/300, trend Loss: 0.0412 | 0.0273
Epoch 239/300, trend Loss: 0.0412 | 0.0273
Epoch 240/300, trend Loss: 0.0412 | 0.0273
Epoch 241/300, trend Loss: 0.0412 | 0.0273
Epoch 242/300, trend Loss: 0.0412 | 0.0273
Epoch 243/300, trend Loss: 0.0412 | 0.0273
Epoch 244/300, trend Loss: 0.0412 | 0.0273
Epoch 245/300, trend Loss: 0.0412 | 0.0273
Epoch 246/300, trend Loss: 0.0412 | 0.0273
Epoch 247/300, trend Loss: 0.0412 | 0.0273
Epoch 248/300, trend Loss: 0.0411 | 0.0273
Epoch 249/300, trend Loss: 0.0411 | 0.0273
Epoch 250/300, trend Loss: 0.0411 | 0.0273
Epoch 251/300, trend Loss: 0.0411 | 0.0273
Epoch 252/300, trend Loss: 0.0411 | 0.0273
Epoch 253/300, trend Loss: 0.0411 | 0.0273
Epoch 254/300, trend Loss: 0.0411 | 0.0273
Epoch 255/300, trend Loss: 0.0411 | 0.0273
Epoch 256/300, trend Loss: 0.0411 | 0.0273
Epoch 257/300, trend Loss: 0.0411 | 0.0273
Epoch 258/300, trend Loss: 0.0411 | 0.0273
Epoch 259/300, trend Loss: 0.0411 | 0.0273
Epoch 260/300, trend Loss: 0.0411 | 0.0273
Epoch 261/300, trend Loss: 0.0411 | 0.0273
Epoch 262/300, trend Loss: 0.0411 | 0.0273
Epoch 263/300, trend Loss: 0.0411 | 0.0273
Epoch 264/300, trend Loss: 0.0411 | 0.0273
Epoch 265/300, trend Loss: 0.0411 | 0.0273
Epoch 266/300, trend Loss: 0.0411 | 0.0273
Epoch 267/300, trend Loss: 0.0411 | 0.0273
Epoch 268/300, trend Loss: 0.0411 | 0.0273
Epoch 269/300, trend Loss: 0.0411 | 0.0273
Epoch 270/300, trend Loss: 0.0411 | 0.0273
Epoch 271/300, trend Loss: 0.0411 | 0.0273
Epoch 272/300, trend Loss: 0.0411 | 0.0273
Epoch 273/300, trend Loss: 0.0411 | 0.0273
Epoch 274/300, trend Loss: 0.0411 | 0.0273
Epoch 275/300, trend Loss: 0.0411 | 0.0273
Epoch 276/300, trend Loss: 0.0411 | 0.0273
Epoch 277/300, trend Loss: 0.0411 | 0.0273
Epoch 278/300, trend Loss: 0.0411 | 0.0273
Epoch 279/300, trend Loss: 0.0411 | 0.0273
Epoch 280/300, trend Loss: 0.0411 | 0.0273
Epoch 281/300, trend Loss: 0.0411 | 0.0273
Epoch 282/300, trend Loss: 0.0411 | 0.0273
Epoch 283/300, trend Loss: 0.0411 | 0.0273
Epoch 284/300, trend Loss: 0.0411 | 0.0273
Epoch 285/300, trend Loss: 0.0411 | 0.0273
Epoch 286/300, trend Loss: 0.0411 | 0.0273
Epoch 287/300, trend Loss: 0.0411 | 0.0273
Epoch 288/300, trend Loss: 0.0411 | 0.0273
Epoch 289/300, trend Loss: 0.0411 | 0.0273
Epoch 290/300, trend Loss: 0.0411 | 0.0273
Epoch 291/300, trend Loss: 0.0411 | 0.0273
Epoch 292/300, trend Loss: 0.0411 | 0.0273
Epoch 293/300, trend Loss: 0.0411 | 0.0273
Epoch 294/300, trend Loss: 0.0411 | 0.0273
Epoch 295/300, trend Loss: 0.0411 | 0.0273
Epoch 296/300, trend Loss: 0.0411 | 0.0273
Epoch 297/300, trend Loss: 0.0410 | 0.0273
Epoch 298/300, trend Loss: 0.0410 | 0.0273
Epoch 299/300, trend Loss: 0.0410 | 0.0273
Epoch 300/300, trend Loss: 0.0410 | 0.0273
Training seasonal_0 component with params: {'observation_period_num': 13, 'train_rates': 0.8600698039412418, 'learning_rate': 0.0006958915694883756, 'batch_size': 211, 'step_size': 14, 'gamma': 0.8001465552215162}
Epoch 1/300, seasonal_0 Loss: 0.5363 | 0.2441
Epoch 2/300, seasonal_0 Loss: 0.2292 | 0.2577
Epoch 3/300, seasonal_0 Loss: 0.2294 | 0.5072
Epoch 4/300, seasonal_0 Loss: 0.2194 | 0.2129
Epoch 5/300, seasonal_0 Loss: 0.1900 | 0.1790
Epoch 6/300, seasonal_0 Loss: 0.2151 | 0.2377
Epoch 7/300, seasonal_0 Loss: 0.1857 | 0.1500
Epoch 8/300, seasonal_0 Loss: 0.1346 | 0.1155
Epoch 9/300, seasonal_0 Loss: 0.1276 | 0.1033
Epoch 10/300, seasonal_0 Loss: 0.1215 | 0.1155
Epoch 11/300, seasonal_0 Loss: 0.1438 | 0.1468
Epoch 12/300, seasonal_0 Loss: 0.1424 | 0.1170
Epoch 13/300, seasonal_0 Loss: 0.1509 | 0.1758
Epoch 14/300, seasonal_0 Loss: 0.1426 | 0.1005
Epoch 15/300, seasonal_0 Loss: 0.1175 | 0.0959
Epoch 16/300, seasonal_0 Loss: 0.1179 | 0.0894
Epoch 17/300, seasonal_0 Loss: 0.1204 | 0.0775
Epoch 18/300, seasonal_0 Loss: 0.1066 | 0.0749
Epoch 19/300, seasonal_0 Loss: 0.1089 | 0.0762
Epoch 20/300, seasonal_0 Loss: 0.1060 | 0.0818
Epoch 21/300, seasonal_0 Loss: 0.1122 | 0.0714
Epoch 22/300, seasonal_0 Loss: 0.1032 | 0.0670
Epoch 23/300, seasonal_0 Loss: 0.1136 | 0.1069
Epoch 24/300, seasonal_0 Loss: 0.1127 | 0.0673
Epoch 25/300, seasonal_0 Loss: 0.1260 | 0.1399
Epoch 26/300, seasonal_0 Loss: 0.1203 | 0.0657
Epoch 27/300, seasonal_0 Loss: 0.1272 | 0.1584
Epoch 28/300, seasonal_0 Loss: 0.1174 | 0.0654
Epoch 29/300, seasonal_0 Loss: 0.1127 | 0.1418
Epoch 30/300, seasonal_0 Loss: 0.0993 | 0.0612
Epoch 31/300, seasonal_0 Loss: 0.0987 | 0.1077
Epoch 32/300, seasonal_0 Loss: 0.0913 | 0.0552
Epoch 33/300, seasonal_0 Loss: 0.0935 | 0.0878
Epoch 34/300, seasonal_0 Loss: 0.0890 | 0.0528
Epoch 35/300, seasonal_0 Loss: 0.0920 | 0.0844
Epoch 36/300, seasonal_0 Loss: 0.0866 | 0.0512
Epoch 37/300, seasonal_0 Loss: 0.0873 | 0.0706
Epoch 38/300, seasonal_0 Loss: 0.0841 | 0.0488
Epoch 39/300, seasonal_0 Loss: 0.0847 | 0.0622
Epoch 40/300, seasonal_0 Loss: 0.0826 | 0.0475
Epoch 41/300, seasonal_0 Loss: 0.0835 | 0.0586
Epoch 42/300, seasonal_0 Loss: 0.0819 | 0.0464
Epoch 43/300, seasonal_0 Loss: 0.0823 | 0.0602
Epoch 44/300, seasonal_0 Loss: 0.0803 | 0.0448
Epoch 45/300, seasonal_0 Loss: 0.0807 | 0.0547
Epoch 46/300, seasonal_0 Loss: 0.0794 | 0.0442
Epoch 47/300, seasonal_0 Loss: 0.0799 | 0.0521
Epoch 48/300, seasonal_0 Loss: 0.0793 | 0.0443
Epoch 49/300, seasonal_0 Loss: 0.0804 | 0.0519
Epoch 50/300, seasonal_0 Loss: 0.0801 | 0.0426
Epoch 51/300, seasonal_0 Loss: 0.0796 | 0.0514
Epoch 52/300, seasonal_0 Loss: 0.0780 | 0.0431
Epoch 53/300, seasonal_0 Loss: 0.0790 | 0.0483
Epoch 54/300, seasonal_0 Loss: 0.0776 | 0.0425
Epoch 55/300, seasonal_0 Loss: 0.0765 | 0.0463
Epoch 56/300, seasonal_0 Loss: 0.0761 | 0.0422
Epoch 57/300, seasonal_0 Loss: 0.0756 | 0.0453
Epoch 58/300, seasonal_0 Loss: 0.0751 | 0.0416
Epoch 59/300, seasonal_0 Loss: 0.0749 | 0.0438
Epoch 60/300, seasonal_0 Loss: 0.0745 | 0.0416
Epoch 61/300, seasonal_0 Loss: 0.0742 | 0.0428
Epoch 62/300, seasonal_0 Loss: 0.0739 | 0.0414
Epoch 63/300, seasonal_0 Loss: 0.0736 | 0.0421
Epoch 64/300, seasonal_0 Loss: 0.0734 | 0.0411
Epoch 65/300, seasonal_0 Loss: 0.0732 | 0.0417
Epoch 66/300, seasonal_0 Loss: 0.0730 | 0.0410
Epoch 67/300, seasonal_0 Loss: 0.0729 | 0.0412
Epoch 68/300, seasonal_0 Loss: 0.0727 | 0.0409
Epoch 69/300, seasonal_0 Loss: 0.0725 | 0.0409
Epoch 70/300, seasonal_0 Loss: 0.0723 | 0.0407
Epoch 71/300, seasonal_0 Loss: 0.0722 | 0.0407
Epoch 72/300, seasonal_0 Loss: 0.0720 | 0.0405
Epoch 73/300, seasonal_0 Loss: 0.0719 | 0.0405
Epoch 74/300, seasonal_0 Loss: 0.0717 | 0.0404
Epoch 75/300, seasonal_0 Loss: 0.0716 | 0.0403
Epoch 76/300, seasonal_0 Loss: 0.0715 | 0.0402
Epoch 77/300, seasonal_0 Loss: 0.0713 | 0.0401
Epoch 78/300, seasonal_0 Loss: 0.0712 | 0.0401
Epoch 79/300, seasonal_0 Loss: 0.0711 | 0.0400
Epoch 80/300, seasonal_0 Loss: 0.0710 | 0.0399
Epoch 81/300, seasonal_0 Loss: 0.0709 | 0.0399
Epoch 82/300, seasonal_0 Loss: 0.0708 | 0.0398
Epoch 83/300, seasonal_0 Loss: 0.0707 | 0.0398
Epoch 84/300, seasonal_0 Loss: 0.0706 | 0.0397
Epoch 85/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 86/300, seasonal_0 Loss: 0.0704 | 0.0395
Epoch 87/300, seasonal_0 Loss: 0.0703 | 0.0395
Epoch 88/300, seasonal_0 Loss: 0.0702 | 0.0394
Epoch 89/300, seasonal_0 Loss: 0.0701 | 0.0394
Epoch 90/300, seasonal_0 Loss: 0.0700 | 0.0393
Epoch 91/300, seasonal_0 Loss: 0.0700 | 0.0393
Epoch 92/300, seasonal_0 Loss: 0.0699 | 0.0392
Epoch 93/300, seasonal_0 Loss: 0.0698 | 0.0391
Epoch 94/300, seasonal_0 Loss: 0.0697 | 0.0391
Epoch 95/300, seasonal_0 Loss: 0.0697 | 0.0390
Epoch 96/300, seasonal_0 Loss: 0.0696 | 0.0390
Epoch 97/300, seasonal_0 Loss: 0.0695 | 0.0389
Epoch 98/300, seasonal_0 Loss: 0.0695 | 0.0389
Epoch 99/300, seasonal_0 Loss: 0.0694 | 0.0388
Epoch 100/300, seasonal_0 Loss: 0.0693 | 0.0388
Epoch 101/300, seasonal_0 Loss: 0.0693 | 0.0387
Epoch 102/300, seasonal_0 Loss: 0.0692 | 0.0387
Epoch 103/300, seasonal_0 Loss: 0.0692 | 0.0386
Epoch 104/300, seasonal_0 Loss: 0.0691 | 0.0386
Epoch 105/300, seasonal_0 Loss: 0.0691 | 0.0385
Epoch 106/300, seasonal_0 Loss: 0.0690 | 0.0385
Epoch 107/300, seasonal_0 Loss: 0.0690 | 0.0384
Epoch 108/300, seasonal_0 Loss: 0.0689 | 0.0384
Epoch 109/300, seasonal_0 Loss: 0.0689 | 0.0384
Epoch 110/300, seasonal_0 Loss: 0.0688 | 0.0383
Epoch 111/300, seasonal_0 Loss: 0.0688 | 0.0383
Epoch 112/300, seasonal_0 Loss: 0.0687 | 0.0382
Epoch 113/300, seasonal_0 Loss: 0.0687 | 0.0382
Epoch 114/300, seasonal_0 Loss: 0.0686 | 0.0382
Epoch 115/300, seasonal_0 Loss: 0.0686 | 0.0381
Epoch 116/300, seasonal_0 Loss: 0.0686 | 0.0381
Epoch 117/300, seasonal_0 Loss: 0.0685 | 0.0381
Epoch 118/300, seasonal_0 Loss: 0.0685 | 0.0380
Epoch 119/300, seasonal_0 Loss: 0.0684 | 0.0380
Epoch 120/300, seasonal_0 Loss: 0.0684 | 0.0380
Epoch 121/300, seasonal_0 Loss: 0.0684 | 0.0379
Epoch 122/300, seasonal_0 Loss: 0.0683 | 0.0379
Epoch 123/300, seasonal_0 Loss: 0.0683 | 0.0379
Epoch 124/300, seasonal_0 Loss: 0.0683 | 0.0378
Epoch 125/300, seasonal_0 Loss: 0.0682 | 0.0378
Epoch 126/300, seasonal_0 Loss: 0.0682 | 0.0378
Epoch 127/300, seasonal_0 Loss: 0.0682 | 0.0377
Epoch 128/300, seasonal_0 Loss: 0.0682 | 0.0377
Epoch 129/300, seasonal_0 Loss: 0.0681 | 0.0377
Epoch 130/300, seasonal_0 Loss: 0.0681 | 0.0377
Epoch 131/300, seasonal_0 Loss: 0.0681 | 0.0376
Epoch 132/300, seasonal_0 Loss: 0.0680 | 0.0376
Epoch 133/300, seasonal_0 Loss: 0.0680 | 0.0376
Epoch 134/300, seasonal_0 Loss: 0.0680 | 0.0376
Epoch 135/300, seasonal_0 Loss: 0.0680 | 0.0375
Epoch 136/300, seasonal_0 Loss: 0.0679 | 0.0375
Epoch 137/300, seasonal_0 Loss: 0.0679 | 0.0375
Epoch 138/300, seasonal_0 Loss: 0.0679 | 0.0375
Epoch 139/300, seasonal_0 Loss: 0.0679 | 0.0375
Epoch 140/300, seasonal_0 Loss: 0.0679 | 0.0374
Epoch 141/300, seasonal_0 Loss: 0.0678 | 0.0374
Epoch 142/300, seasonal_0 Loss: 0.0678 | 0.0374
Epoch 143/300, seasonal_0 Loss: 0.0678 | 0.0374
Epoch 144/300, seasonal_0 Loss: 0.0678 | 0.0374
Epoch 145/300, seasonal_0 Loss: 0.0678 | 0.0373
Epoch 146/300, seasonal_0 Loss: 0.0677 | 0.0373
Epoch 147/300, seasonal_0 Loss: 0.0677 | 0.0373
Epoch 148/300, seasonal_0 Loss: 0.0677 | 0.0373
Epoch 149/300, seasonal_0 Loss: 0.0677 | 0.0373
Epoch 150/300, seasonal_0 Loss: 0.0677 | 0.0373
Epoch 151/300, seasonal_0 Loss: 0.0677 | 0.0372
Epoch 152/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 153/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 154/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 155/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 156/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 157/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 158/300, seasonal_0 Loss: 0.0676 | 0.0372
Epoch 159/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 160/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 161/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 162/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 163/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 164/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 165/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 166/300, seasonal_0 Loss: 0.0675 | 0.0371
Epoch 167/300, seasonal_0 Loss: 0.0675 | 0.0370
Epoch 168/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 169/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 170/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 171/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 172/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 173/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 174/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 175/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 176/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 177/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 178/300, seasonal_0 Loss: 0.0674 | 0.0370
Epoch 179/300, seasonal_0 Loss: 0.0674 | 0.0369
Epoch 180/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 181/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 182/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 183/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 184/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 185/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 186/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 187/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 188/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 189/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 190/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 191/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 192/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 193/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 194/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 195/300, seasonal_0 Loss: 0.0673 | 0.0369
Epoch 196/300, seasonal_0 Loss: 0.0673 | 0.0368
Epoch 197/300, seasonal_0 Loss: 0.0673 | 0.0368
Epoch 198/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 199/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 200/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 201/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 202/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 203/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 204/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 205/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 206/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 207/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 208/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 209/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 210/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 211/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 212/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 213/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 214/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 215/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 216/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 217/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 218/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 219/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 220/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 221/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 222/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 223/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 224/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 225/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 226/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 227/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 228/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 229/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 230/300, seasonal_0 Loss: 0.0672 | 0.0368
Epoch 231/300, seasonal_0 Loss: 0.0672 | 0.0367
Epoch 232/300, seasonal_0 Loss: 0.0672 | 0.0367
Epoch 233/300, seasonal_0 Loss: 0.0672 | 0.0367
Epoch 234/300, seasonal_0 Loss: 0.0672 | 0.0367
Epoch 235/300, seasonal_0 Loss: 0.0672 | 0.0367
Epoch 236/300, seasonal_0 Loss: 0.0672 | 0.0367
Epoch 237/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 238/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 239/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 240/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 241/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 242/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 243/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 244/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 245/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 246/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 247/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 248/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 249/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 250/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 251/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 252/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 253/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 254/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 255/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 256/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 257/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 258/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 259/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 260/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 261/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 262/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 263/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 264/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 265/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 266/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 267/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 268/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 269/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 270/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 271/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 272/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 273/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 274/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 275/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 276/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 277/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 278/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 279/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 280/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 281/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 282/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 283/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 284/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 285/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 286/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 287/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 288/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 289/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 290/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 291/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 292/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 293/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 294/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 295/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 296/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 297/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 298/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 299/300, seasonal_0 Loss: 0.0671 | 0.0367
Epoch 300/300, seasonal_0 Loss: 0.0671 | 0.0367
Training seasonal_1 component with params: {'observation_period_num': 10, 'train_rates': 0.7775837457530692, 'learning_rate': 9.050481338274824e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9040870623989773}
Epoch 1/300, seasonal_1 Loss: 0.4188 | 0.1702
Epoch 2/300, seasonal_1 Loss: 0.1428 | 0.1315
Epoch 3/300, seasonal_1 Loss: 0.1249 | 0.1041
Epoch 4/300, seasonal_1 Loss: 0.1134 | 0.0827
Epoch 5/300, seasonal_1 Loss: 0.1081 | 0.0749
Epoch 6/300, seasonal_1 Loss: 0.1053 | 0.0700
Epoch 7/300, seasonal_1 Loss: 0.1028 | 0.0652
Epoch 8/300, seasonal_1 Loss: 0.1009 | 0.0613
Epoch 9/300, seasonal_1 Loss: 0.0988 | 0.0584
Epoch 10/300, seasonal_1 Loss: 0.0964 | 0.0561
Epoch 11/300, seasonal_1 Loss: 0.0938 | 0.0545
Epoch 12/300, seasonal_1 Loss: 0.0907 | 0.0515
Epoch 13/300, seasonal_1 Loss: 0.0883 | 0.0512
Epoch 14/300, seasonal_1 Loss: 0.0859 | 0.0515
Epoch 15/300, seasonal_1 Loss: 0.0839 | 0.0514
Epoch 16/300, seasonal_1 Loss: 0.0823 | 0.0509
Epoch 17/300, seasonal_1 Loss: 0.0810 | 0.0499
Epoch 18/300, seasonal_1 Loss: 0.0796 | 0.0475
Epoch 19/300, seasonal_1 Loss: 0.0785 | 0.0459
Epoch 20/300, seasonal_1 Loss: 0.0776 | 0.0447
Epoch 21/300, seasonal_1 Loss: 0.0768 | 0.0435
Epoch 22/300, seasonal_1 Loss: 0.0761 | 0.0425
Epoch 23/300, seasonal_1 Loss: 0.0754 | 0.0410
Epoch 24/300, seasonal_1 Loss: 0.0747 | 0.0404
Epoch 25/300, seasonal_1 Loss: 0.0741 | 0.0398
Epoch 26/300, seasonal_1 Loss: 0.0736 | 0.0392
Epoch 27/300, seasonal_1 Loss: 0.0731 | 0.0386
Epoch 28/300, seasonal_1 Loss: 0.0726 | 0.0380
Epoch 29/300, seasonal_1 Loss: 0.0718 | 0.0372
Epoch 30/300, seasonal_1 Loss: 0.0711 | 0.0369
Epoch 31/300, seasonal_1 Loss: 0.0706 | 0.0367
Epoch 32/300, seasonal_1 Loss: 0.0701 | 0.0364
Epoch 33/300, seasonal_1 Loss: 0.0696 | 0.0360
Epoch 34/300, seasonal_1 Loss: 0.0690 | 0.0356
Epoch 35/300, seasonal_1 Loss: 0.0686 | 0.0352
Epoch 36/300, seasonal_1 Loss: 0.0681 | 0.0347
Epoch 37/300, seasonal_1 Loss: 0.0677 | 0.0342
Epoch 38/300, seasonal_1 Loss: 0.0673 | 0.0337
Epoch 39/300, seasonal_1 Loss: 0.0670 | 0.0333
Epoch 40/300, seasonal_1 Loss: 0.0665 | 0.0330
Epoch 41/300, seasonal_1 Loss: 0.0662 | 0.0326
Epoch 42/300, seasonal_1 Loss: 0.0658 | 0.0324
Epoch 43/300, seasonal_1 Loss: 0.0655 | 0.0322
Epoch 44/300, seasonal_1 Loss: 0.0652 | 0.0321
Epoch 45/300, seasonal_1 Loss: 0.0649 | 0.0316
Epoch 46/300, seasonal_1 Loss: 0.0646 | 0.0314
Epoch 47/300, seasonal_1 Loss: 0.0644 | 0.0312
Epoch 48/300, seasonal_1 Loss: 0.0642 | 0.0311
Epoch 49/300, seasonal_1 Loss: 0.0639 | 0.0309
Epoch 50/300, seasonal_1 Loss: 0.0637 | 0.0308
Epoch 51/300, seasonal_1 Loss: 0.0634 | 0.0300
Epoch 52/300, seasonal_1 Loss: 0.0633 | 0.0298
Epoch 53/300, seasonal_1 Loss: 0.0630 | 0.0298
Epoch 54/300, seasonal_1 Loss: 0.0628 | 0.0298
Epoch 55/300, seasonal_1 Loss: 0.0626 | 0.0297
Epoch 56/300, seasonal_1 Loss: 0.0624 | 0.0295
Epoch 57/300, seasonal_1 Loss: 0.0622 | 0.0295
Epoch 58/300, seasonal_1 Loss: 0.0621 | 0.0293
Epoch 59/300, seasonal_1 Loss: 0.0619 | 0.0293
Epoch 60/300, seasonal_1 Loss: 0.0617 | 0.0292
Epoch 61/300, seasonal_1 Loss: 0.0616 | 0.0291
Epoch 62/300, seasonal_1 Loss: 0.0614 | 0.0286
Epoch 63/300, seasonal_1 Loss: 0.0612 | 0.0285
Epoch 64/300, seasonal_1 Loss: 0.0611 | 0.0284
Epoch 65/300, seasonal_1 Loss: 0.0609 | 0.0283
Epoch 66/300, seasonal_1 Loss: 0.0608 | 0.0282
Epoch 67/300, seasonal_1 Loss: 0.0606 | 0.0277
Epoch 68/300, seasonal_1 Loss: 0.0605 | 0.0276
Epoch 69/300, seasonal_1 Loss: 0.0604 | 0.0275
Epoch 70/300, seasonal_1 Loss: 0.0603 | 0.0273
Epoch 71/300, seasonal_1 Loss: 0.0601 | 0.0272
Epoch 72/300, seasonal_1 Loss: 0.0600 | 0.0271
Epoch 73/300, seasonal_1 Loss: 0.0599 | 0.0265
Epoch 74/300, seasonal_1 Loss: 0.0598 | 0.0263
Epoch 75/300, seasonal_1 Loss: 0.0597 | 0.0262
Epoch 76/300, seasonal_1 Loss: 0.0596 | 0.0261
Epoch 77/300, seasonal_1 Loss: 0.0594 | 0.0260
Epoch 78/300, seasonal_1 Loss: 0.0593 | 0.0259
Epoch 79/300, seasonal_1 Loss: 0.0592 | 0.0259
Epoch 80/300, seasonal_1 Loss: 0.0591 | 0.0259
Epoch 81/300, seasonal_1 Loss: 0.0590 | 0.0258
Epoch 82/300, seasonal_1 Loss: 0.0589 | 0.0258
Epoch 83/300, seasonal_1 Loss: 0.0588 | 0.0258
Epoch 84/300, seasonal_1 Loss: 0.0586 | 0.0258
Epoch 85/300, seasonal_1 Loss: 0.0585 | 0.0258
Epoch 86/300, seasonal_1 Loss: 0.0584 | 0.0258
Epoch 87/300, seasonal_1 Loss: 0.0583 | 0.0257
Epoch 88/300, seasonal_1 Loss: 0.0582 | 0.0257
Epoch 89/300, seasonal_1 Loss: 0.0581 | 0.0254
Epoch 90/300, seasonal_1 Loss: 0.0580 | 0.0254
Epoch 91/300, seasonal_1 Loss: 0.0579 | 0.0253
Epoch 92/300, seasonal_1 Loss: 0.0578 | 0.0253
Epoch 93/300, seasonal_1 Loss: 0.0577 | 0.0253
Epoch 94/300, seasonal_1 Loss: 0.0576 | 0.0252
Epoch 95/300, seasonal_1 Loss: 0.0575 | 0.0250
Epoch 96/300, seasonal_1 Loss: 0.0574 | 0.0250
Epoch 97/300, seasonal_1 Loss: 0.0573 | 0.0250
Epoch 98/300, seasonal_1 Loss: 0.0572 | 0.0249
Epoch 99/300, seasonal_1 Loss: 0.0572 | 0.0249
Epoch 100/300, seasonal_1 Loss: 0.0570 | 0.0248
Epoch 101/300, seasonal_1 Loss: 0.0570 | 0.0248
Epoch 102/300, seasonal_1 Loss: 0.0569 | 0.0248
Epoch 103/300, seasonal_1 Loss: 0.0569 | 0.0247
Epoch 104/300, seasonal_1 Loss: 0.0568 | 0.0247
Epoch 105/300, seasonal_1 Loss: 0.0567 | 0.0247
Epoch 106/300, seasonal_1 Loss: 0.0566 | 0.0247
Epoch 107/300, seasonal_1 Loss: 0.0566 | 0.0247
Epoch 108/300, seasonal_1 Loss: 0.0565 | 0.0247
Epoch 109/300, seasonal_1 Loss: 0.0564 | 0.0247
Epoch 110/300, seasonal_1 Loss: 0.0564 | 0.0247
Epoch 111/300, seasonal_1 Loss: 0.0563 | 0.0247
Epoch 112/300, seasonal_1 Loss: 0.0562 | 0.0247
Epoch 113/300, seasonal_1 Loss: 0.0561 | 0.0247
Epoch 114/300, seasonal_1 Loss: 0.0561 | 0.0247
Epoch 115/300, seasonal_1 Loss: 0.0560 | 0.0247
Epoch 116/300, seasonal_1 Loss: 0.0559 | 0.0247
Epoch 117/300, seasonal_1 Loss: 0.0558 | 0.0248
Epoch 118/300, seasonal_1 Loss: 0.0558 | 0.0248
Epoch 119/300, seasonal_1 Loss: 0.0557 | 0.0248
Epoch 120/300, seasonal_1 Loss: 0.0557 | 0.0248
Epoch 121/300, seasonal_1 Loss: 0.0556 | 0.0248
Epoch 122/300, seasonal_1 Loss: 0.0555 | 0.0248
Epoch 123/300, seasonal_1 Loss: 0.0555 | 0.0248
Epoch 124/300, seasonal_1 Loss: 0.0554 | 0.0248
Epoch 125/300, seasonal_1 Loss: 0.0554 | 0.0248
Epoch 126/300, seasonal_1 Loss: 0.0553 | 0.0248
Epoch 127/300, seasonal_1 Loss: 0.0553 | 0.0248
Epoch 128/300, seasonal_1 Loss: 0.0552 | 0.0247
Epoch 129/300, seasonal_1 Loss: 0.0551 | 0.0247
Epoch 130/300, seasonal_1 Loss: 0.0551 | 0.0247
Epoch 131/300, seasonal_1 Loss: 0.0550 | 0.0247
Epoch 132/300, seasonal_1 Loss: 0.0550 | 0.0247
Epoch 133/300, seasonal_1 Loss: 0.0549 | 0.0247
Epoch 134/300, seasonal_1 Loss: 0.0549 | 0.0247
Epoch 135/300, seasonal_1 Loss: 0.0548 | 0.0247
Epoch 136/300, seasonal_1 Loss: 0.0548 | 0.0247
Epoch 137/300, seasonal_1 Loss: 0.0548 | 0.0247
Epoch 138/300, seasonal_1 Loss: 0.0547 | 0.0247
Epoch 139/300, seasonal_1 Loss: 0.0547 | 0.0247
Epoch 140/300, seasonal_1 Loss: 0.0546 | 0.0247
Epoch 141/300, seasonal_1 Loss: 0.0546 | 0.0247
Epoch 142/300, seasonal_1 Loss: 0.0545 | 0.0247
Epoch 143/300, seasonal_1 Loss: 0.0545 | 0.0247
Epoch 144/300, seasonal_1 Loss: 0.0545 | 0.0247
Epoch 145/300, seasonal_1 Loss: 0.0544 | 0.0247
Epoch 146/300, seasonal_1 Loss: 0.0544 | 0.0247
Epoch 147/300, seasonal_1 Loss: 0.0543 | 0.0247
Epoch 148/300, seasonal_1 Loss: 0.0543 | 0.0247
Epoch 149/300, seasonal_1 Loss: 0.0543 | 0.0247
Epoch 150/300, seasonal_1 Loss: 0.0542 | 0.0247
Epoch 151/300, seasonal_1 Loss: 0.0542 | 0.0247
Epoch 152/300, seasonal_1 Loss: 0.0542 | 0.0247
Epoch 153/300, seasonal_1 Loss: 0.0541 | 0.0247
Epoch 154/300, seasonal_1 Loss: 0.0541 | 0.0247
Epoch 155/300, seasonal_1 Loss: 0.0541 | 0.0247
Epoch 156/300, seasonal_1 Loss: 0.0540 | 0.0247
Epoch 157/300, seasonal_1 Loss: 0.0540 | 0.0248
Epoch 158/300, seasonal_1 Loss: 0.0540 | 0.0248
Epoch 159/300, seasonal_1 Loss: 0.0539 | 0.0248
Epoch 160/300, seasonal_1 Loss: 0.0539 | 0.0248
Epoch 161/300, seasonal_1 Loss: 0.0539 | 0.0248
Epoch 162/300, seasonal_1 Loss: 0.0538 | 0.0248
Epoch 163/300, seasonal_1 Loss: 0.0538 | 0.0248
Epoch 164/300, seasonal_1 Loss: 0.0538 | 0.0248
Epoch 165/300, seasonal_1 Loss: 0.0537 | 0.0249
Epoch 166/300, seasonal_1 Loss: 0.0537 | 0.0249
Epoch 167/300, seasonal_1 Loss: 0.0537 | 0.0249
Epoch 168/300, seasonal_1 Loss: 0.0537 | 0.0249
Epoch 169/300, seasonal_1 Loss: 0.0536 | 0.0249
Epoch 170/300, seasonal_1 Loss: 0.0536 | 0.0250
Epoch 171/300, seasonal_1 Loss: 0.0536 | 0.0250
Epoch 172/300, seasonal_1 Loss: 0.0536 | 0.0250
Epoch 173/300, seasonal_1 Loss: 0.0535 | 0.0250
Epoch 174/300, seasonal_1 Loss: 0.0535 | 0.0251
Epoch 175/300, seasonal_1 Loss: 0.0535 | 0.0251
Epoch 176/300, seasonal_1 Loss: 0.0535 | 0.0251
Epoch 177/300, seasonal_1 Loss: 0.0534 | 0.0252
Epoch 178/300, seasonal_1 Loss: 0.0534 | 0.0252
Epoch 179/300, seasonal_1 Loss: 0.0534 | 0.0252
Epoch 180/300, seasonal_1 Loss: 0.0533 | 0.0252
Epoch 181/300, seasonal_1 Loss: 0.0533 | 0.0252
Epoch 182/300, seasonal_1 Loss: 0.0533 | 0.0252
Epoch 183/300, seasonal_1 Loss: 0.0533 | 0.0253
Epoch 184/300, seasonal_1 Loss: 0.0532 | 0.0253
Epoch 185/300, seasonal_1 Loss: 0.0532 | 0.0253
Epoch 186/300, seasonal_1 Loss: 0.0532 | 0.0253
Epoch 187/300, seasonal_1 Loss: 0.0531 | 0.0253
Epoch 188/300, seasonal_1 Loss: 0.0531 | 0.0253
Epoch 189/300, seasonal_1 Loss: 0.0531 | 0.0253
Epoch 190/300, seasonal_1 Loss: 0.0531 | 0.0253
Epoch 191/300, seasonal_1 Loss: 0.0530 | 0.0253
Epoch 192/300, seasonal_1 Loss: 0.0530 | 0.0253
Epoch 193/300, seasonal_1 Loss: 0.0530 | 0.0253
Epoch 194/300, seasonal_1 Loss: 0.0529 | 0.0253
Epoch 195/300, seasonal_1 Loss: 0.0529 | 0.0253
Epoch 196/300, seasonal_1 Loss: 0.0529 | 0.0253
Epoch 197/300, seasonal_1 Loss: 0.0529 | 0.0253
Epoch 198/300, seasonal_1 Loss: 0.0528 | 0.0253
Epoch 199/300, seasonal_1 Loss: 0.0528 | 0.0253
Epoch 200/300, seasonal_1 Loss: 0.0528 | 0.0253
Epoch 201/300, seasonal_1 Loss: 0.0528 | 0.0253
Epoch 202/300, seasonal_1 Loss: 0.0527 | 0.0253
Epoch 203/300, seasonal_1 Loss: 0.0527 | 0.0253
Epoch 204/300, seasonal_1 Loss: 0.0527 | 0.0253
Epoch 205/300, seasonal_1 Loss: 0.0527 | 0.0253
Epoch 206/300, seasonal_1 Loss: 0.0526 | 0.0253
Epoch 207/300, seasonal_1 Loss: 0.0526 | 0.0253
Epoch 208/300, seasonal_1 Loss: 0.0526 | 0.0253
Epoch 209/300, seasonal_1 Loss: 0.0526 | 0.0253
Epoch 210/300, seasonal_1 Loss: 0.0525 | 0.0253
Epoch 211/300, seasonal_1 Loss: 0.0525 | 0.0254
Epoch 212/300, seasonal_1 Loss: 0.0525 | 0.0254
Epoch 213/300, seasonal_1 Loss: 0.0525 | 0.0254
Epoch 214/300, seasonal_1 Loss: 0.0525 | 0.0254
Epoch 215/300, seasonal_1 Loss: 0.0524 | 0.0254
Epoch 216/300, seasonal_1 Loss: 0.0524 | 0.0254
Epoch 217/300, seasonal_1 Loss: 0.0524 | 0.0254
Epoch 218/300, seasonal_1 Loss: 0.0524 | 0.0254
Epoch 219/300, seasonal_1 Loss: 0.0523 | 0.0254
Epoch 220/300, seasonal_1 Loss: 0.0523 | 0.0254
Epoch 221/300, seasonal_1 Loss: 0.0523 | 0.0254
Epoch 222/300, seasonal_1 Loss: 0.0523 | 0.0254
Epoch 223/300, seasonal_1 Loss: 0.0523 | 0.0254
Epoch 224/300, seasonal_1 Loss: 0.0522 | 0.0254
Epoch 225/300, seasonal_1 Loss: 0.0522 | 0.0254
Epoch 226/300, seasonal_1 Loss: 0.0522 | 0.0254
Epoch 227/300, seasonal_1 Loss: 0.0522 | 0.0254
Epoch 228/300, seasonal_1 Loss: 0.0521 | 0.0254
Epoch 229/300, seasonal_1 Loss: 0.0521 | 0.0254
Epoch 230/300, seasonal_1 Loss: 0.0521 | 0.0254
Epoch 231/300, seasonal_1 Loss: 0.0521 | 0.0254
Epoch 232/300, seasonal_1 Loss: 0.0521 | 0.0254
Epoch 233/300, seasonal_1 Loss: 0.0520 | 0.0254
Epoch 234/300, seasonal_1 Loss: 0.0520 | 0.0254
Epoch 235/300, seasonal_1 Loss: 0.0520 | 0.0254
Epoch 236/300, seasonal_1 Loss: 0.0520 | 0.0255
Epoch 237/300, seasonal_1 Loss: 0.0520 | 0.0255
Epoch 238/300, seasonal_1 Loss: 0.0519 | 0.0255
Epoch 239/300, seasonal_1 Loss: 0.0519 | 0.0255
Epoch 240/300, seasonal_1 Loss: 0.0519 | 0.0255
Epoch 241/300, seasonal_1 Loss: 0.0519 | 0.0255
Epoch 242/300, seasonal_1 Loss: 0.0519 | 0.0255
Epoch 243/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 244/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 245/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 246/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 247/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 248/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 249/300, seasonal_1 Loss: 0.0517 | 0.0255
Epoch 250/300, seasonal_1 Loss: 0.0517 | 0.0255
Epoch 251/300, seasonal_1 Loss: 0.0517 | 0.0255
Epoch 252/300, seasonal_1 Loss: 0.0517 | 0.0255
Epoch 253/300, seasonal_1 Loss: 0.0517 | 0.0255
Epoch 254/300, seasonal_1 Loss: 0.0516 | 0.0255
Epoch 255/300, seasonal_1 Loss: 0.0516 | 0.0255
Epoch 256/300, seasonal_1 Loss: 0.0516 | 0.0255
Epoch 257/300, seasonal_1 Loss: 0.0516 | 0.0255
Epoch 258/300, seasonal_1 Loss: 0.0516 | 0.0256
Epoch 259/300, seasonal_1 Loss: 0.0516 | 0.0256
Epoch 260/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 261/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 262/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 263/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 264/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 265/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 266/300, seasonal_1 Loss: 0.0515 | 0.0256
Epoch 267/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 268/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 269/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 270/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 271/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 272/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 273/300, seasonal_1 Loss: 0.0514 | 0.0256
Epoch 274/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 275/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 276/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 277/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 278/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 279/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 280/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 281/300, seasonal_1 Loss: 0.0513 | 0.0256
Epoch 282/300, seasonal_1 Loss: 0.0512 | 0.0256
Epoch 283/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 284/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 285/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 286/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 287/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 288/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 289/300, seasonal_1 Loss: 0.0512 | 0.0257
Epoch 290/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 291/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 292/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 293/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 294/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 295/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 296/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 297/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 298/300, seasonal_1 Loss: 0.0511 | 0.0257
Epoch 299/300, seasonal_1 Loss: 0.0510 | 0.0257
Epoch 300/300, seasonal_1 Loss: 0.0510 | 0.0257
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.9132472828948074, 'learning_rate': 0.000951410109049033, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7680415425559964}
Epoch 1/300, seasonal_2 Loss: 0.3541 | 0.1357
Epoch 2/300, seasonal_2 Loss: 0.1309 | 0.0833
Epoch 3/300, seasonal_2 Loss: 0.1085 | 0.0667
Epoch 4/300, seasonal_2 Loss: 0.0969 | 0.0595
Epoch 5/300, seasonal_2 Loss: 0.0971 | 0.0605
Epoch 6/300, seasonal_2 Loss: 0.0965 | 0.0549
Epoch 7/300, seasonal_2 Loss: 0.0915 | 0.0535
Epoch 8/300, seasonal_2 Loss: 0.0888 | 0.0482
Epoch 9/300, seasonal_2 Loss: 0.0855 | 0.0439
Epoch 10/300, seasonal_2 Loss: 0.0848 | 0.0474
Epoch 11/300, seasonal_2 Loss: 0.0825 | 0.0458
Epoch 12/300, seasonal_2 Loss: 0.0794 | 0.0440
Epoch 13/300, seasonal_2 Loss: 0.0774 | 0.0391
Epoch 14/300, seasonal_2 Loss: 0.0765 | 0.0457
Epoch 15/300, seasonal_2 Loss: 0.0756 | 0.0387
Epoch 16/300, seasonal_2 Loss: 0.0746 | 0.0408
Epoch 17/300, seasonal_2 Loss: 0.0739 | 0.0380
Epoch 18/300, seasonal_2 Loss: 0.0736 | 0.0392
Epoch 19/300, seasonal_2 Loss: 0.0730 | 0.0369
Epoch 20/300, seasonal_2 Loss: 0.0731 | 0.0379
Epoch 21/300, seasonal_2 Loss: 0.0727 | 0.0368
Epoch 22/300, seasonal_2 Loss: 0.0719 | 0.0381
Epoch 23/300, seasonal_2 Loss: 0.0715 | 0.0377
Epoch 24/300, seasonal_2 Loss: 0.0709 | 0.0375
Epoch 25/300, seasonal_2 Loss: 0.0702 | 0.0372
Epoch 26/300, seasonal_2 Loss: 0.0702 | 0.0376
Epoch 27/300, seasonal_2 Loss: 0.0699 | 0.0369
Epoch 28/300, seasonal_2 Loss: 0.0696 | 0.0371
Epoch 29/300, seasonal_2 Loss: 0.0695 | 0.0366
Epoch 30/300, seasonal_2 Loss: 0.0692 | 0.0364
Epoch 31/300, seasonal_2 Loss: 0.0690 | 0.0363
Epoch 32/300, seasonal_2 Loss: 0.0689 | 0.0359
Epoch 33/300, seasonal_2 Loss: 0.0684 | 0.0357
Epoch 34/300, seasonal_2 Loss: 0.0683 | 0.0350
Epoch 35/300, seasonal_2 Loss: 0.0683 | 0.0347
Epoch 36/300, seasonal_2 Loss: 0.0681 | 0.0345
Epoch 37/300, seasonal_2 Loss: 0.0681 | 0.0346
Epoch 38/300, seasonal_2 Loss: 0.0679 | 0.0343
Epoch 39/300, seasonal_2 Loss: 0.0673 | 0.0339
Epoch 40/300, seasonal_2 Loss: 0.0669 | 0.0337
Epoch 41/300, seasonal_2 Loss: 0.0666 | 0.0337
Epoch 42/300, seasonal_2 Loss: 0.0663 | 0.0337
Epoch 43/300, seasonal_2 Loss: 0.0660 | 0.0337
Epoch 44/300, seasonal_2 Loss: 0.0659 | 0.0336
Epoch 45/300, seasonal_2 Loss: 0.0657 | 0.0334
Epoch 46/300, seasonal_2 Loss: 0.0655 | 0.0333
Epoch 47/300, seasonal_2 Loss: 0.0654 | 0.0332
Epoch 48/300, seasonal_2 Loss: 0.0653 | 0.0331
Epoch 49/300, seasonal_2 Loss: 0.0651 | 0.0330
Epoch 50/300, seasonal_2 Loss: 0.0650 | 0.0329
Epoch 51/300, seasonal_2 Loss: 0.0649 | 0.0329
Epoch 52/300, seasonal_2 Loss: 0.0648 | 0.0328
Epoch 53/300, seasonal_2 Loss: 0.0647 | 0.0327
Epoch 54/300, seasonal_2 Loss: 0.0647 | 0.0327
Epoch 55/300, seasonal_2 Loss: 0.0646 | 0.0327
Epoch 56/300, seasonal_2 Loss: 0.0645 | 0.0326
Epoch 57/300, seasonal_2 Loss: 0.0645 | 0.0326
Epoch 58/300, seasonal_2 Loss: 0.0644 | 0.0325
Epoch 59/300, seasonal_2 Loss: 0.0644 | 0.0325
Epoch 60/300, seasonal_2 Loss: 0.0643 | 0.0325
Epoch 61/300, seasonal_2 Loss: 0.0643 | 0.0324
Epoch 62/300, seasonal_2 Loss: 0.0642 | 0.0324
Epoch 63/300, seasonal_2 Loss: 0.0642 | 0.0324
Epoch 64/300, seasonal_2 Loss: 0.0642 | 0.0324
Epoch 65/300, seasonal_2 Loss: 0.0641 | 0.0323
Epoch 66/300, seasonal_2 Loss: 0.0641 | 0.0323
Epoch 67/300, seasonal_2 Loss: 0.0641 | 0.0323
Epoch 68/300, seasonal_2 Loss: 0.0641 | 0.0323
Epoch 69/300, seasonal_2 Loss: 0.0640 | 0.0323
Epoch 70/300, seasonal_2 Loss: 0.0640 | 0.0322
Epoch 71/300, seasonal_2 Loss: 0.0640 | 0.0322
Epoch 72/300, seasonal_2 Loss: 0.0640 | 0.0322
Epoch 73/300, seasonal_2 Loss: 0.0640 | 0.0322
Epoch 74/300, seasonal_2 Loss: 0.0640 | 0.0322
Epoch 75/300, seasonal_2 Loss: 0.0640 | 0.0322
Epoch 76/300, seasonal_2 Loss: 0.0639 | 0.0322
Epoch 77/300, seasonal_2 Loss: 0.0639 | 0.0322
Epoch 78/300, seasonal_2 Loss: 0.0639 | 0.0322
Epoch 79/300, seasonal_2 Loss: 0.0639 | 0.0322
Epoch 80/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 81/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 82/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 83/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 84/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 85/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 86/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 87/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 88/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 89/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 90/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 91/300, seasonal_2 Loss: 0.0639 | 0.0321
Epoch 92/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 93/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 94/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 95/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 96/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 97/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 98/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 99/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 100/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 101/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 102/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 103/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 104/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 105/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 106/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 107/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 108/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 109/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 110/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 111/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 112/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 113/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 114/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 115/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 116/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 117/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 118/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 119/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 120/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 121/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 122/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 123/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 124/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 125/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 126/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 127/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 128/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 129/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 130/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 131/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 132/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 133/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 134/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 135/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 136/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 137/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 138/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 139/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 140/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 141/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 142/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 143/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 144/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 145/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 146/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 147/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 148/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 149/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 150/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 151/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 152/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 153/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 154/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 155/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 156/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 157/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 158/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 159/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 160/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 161/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 162/300, seasonal_2 Loss: 0.0638 | 0.0321
Epoch 163/300, seasonal_2 Loss: 0.0638 | 0.0321
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.9892018627975194, 'learning_rate': 0.0006470870502252402, 'batch_size': 195, 'step_size': 10, 'gamma': 0.7517125055085089}
Epoch 1/300, seasonal_3 Loss: 0.4483 | 0.2714
Epoch 2/300, seasonal_3 Loss: 0.2039 | 0.1595
Epoch 3/300, seasonal_3 Loss: 0.1836 | 0.2372
Epoch 4/300, seasonal_3 Loss: 0.2571 | 0.3348
Epoch 5/300, seasonal_3 Loss: 0.2847 | 0.8371
Epoch 6/300, seasonal_3 Loss: 0.2097 | 0.2010
Epoch 7/300, seasonal_3 Loss: 0.1960 | 0.2009
Epoch 8/300, seasonal_3 Loss: 0.1348 | 0.1323
Epoch 9/300, seasonal_3 Loss: 0.1403 | 0.1689
Epoch 10/300, seasonal_3 Loss: 0.1486 | 0.1004
Epoch 11/300, seasonal_3 Loss: 0.1517 | 0.0925
Epoch 12/300, seasonal_3 Loss: 0.1160 | 0.0771
Epoch 13/300, seasonal_3 Loss: 0.1309 | 0.1102
Epoch 14/300, seasonal_3 Loss: 0.1205 | 0.0768
Epoch 15/300, seasonal_3 Loss: 0.1089 | 0.0660
Epoch 16/300, seasonal_3 Loss: 0.1130 | 0.0666
Epoch 17/300, seasonal_3 Loss: 0.1106 | 0.1161
Epoch 18/300, seasonal_3 Loss: 0.1041 | 0.0608
Epoch 19/300, seasonal_3 Loss: 0.0968 | 0.0762
Epoch 20/300, seasonal_3 Loss: 0.0936 | 0.0561
Epoch 21/300, seasonal_3 Loss: 0.0894 | 0.0593
Epoch 22/300, seasonal_3 Loss: 0.0880 | 0.0523
Epoch 23/300, seasonal_3 Loss: 0.0865 | 0.0535
Epoch 24/300, seasonal_3 Loss: 0.0855 | 0.0512
Epoch 25/300, seasonal_3 Loss: 0.0845 | 0.0507
Epoch 26/300, seasonal_3 Loss: 0.0836 | 0.0499
Epoch 27/300, seasonal_3 Loss: 0.0830 | 0.0494
Epoch 28/300, seasonal_3 Loss: 0.0824 | 0.0490
Epoch 29/300, seasonal_3 Loss: 0.0819 | 0.0486
Epoch 30/300, seasonal_3 Loss: 0.0813 | 0.0481
Epoch 31/300, seasonal_3 Loss: 0.0807 | 0.0476
Epoch 32/300, seasonal_3 Loss: 0.0803 | 0.0472
Epoch 33/300, seasonal_3 Loss: 0.0798 | 0.0469
Epoch 34/300, seasonal_3 Loss: 0.0794 | 0.0465
Epoch 35/300, seasonal_3 Loss: 0.0789 | 0.0461
Epoch 36/300, seasonal_3 Loss: 0.0785 | 0.0457
Epoch 37/300, seasonal_3 Loss: 0.0782 | 0.0454
Epoch 38/300, seasonal_3 Loss: 0.0778 | 0.0451
Epoch 39/300, seasonal_3 Loss: 0.0775 | 0.0447
Epoch 40/300, seasonal_3 Loss: 0.0771 | 0.0443
Epoch 41/300, seasonal_3 Loss: 0.0768 | 0.0440
Epoch 42/300, seasonal_3 Loss: 0.0765 | 0.0436
Epoch 43/300, seasonal_3 Loss: 0.0762 | 0.0433
Epoch 44/300, seasonal_3 Loss: 0.0759 | 0.0429
Epoch 45/300, seasonal_3 Loss: 0.0755 | 0.0425
Epoch 46/300, seasonal_3 Loss: 0.0752 | 0.0421
Epoch 47/300, seasonal_3 Loss: 0.0749 | 0.0418
Epoch 48/300, seasonal_3 Loss: 0.0747 | 0.0414
Epoch 49/300, seasonal_3 Loss: 0.0744 | 0.0410
Epoch 50/300, seasonal_3 Loss: 0.0741 | 0.0406
Epoch 51/300, seasonal_3 Loss: 0.0738 | 0.0402
Epoch 52/300, seasonal_3 Loss: 0.0736 | 0.0399
Epoch 53/300, seasonal_3 Loss: 0.0734 | 0.0396
Epoch 54/300, seasonal_3 Loss: 0.0732 | 0.0393
Epoch 55/300, seasonal_3 Loss: 0.0730 | 0.0389
Epoch 56/300, seasonal_3 Loss: 0.0728 | 0.0387
Epoch 57/300, seasonal_3 Loss: 0.0727 | 0.0384
Epoch 58/300, seasonal_3 Loss: 0.0725 | 0.0382
Epoch 59/300, seasonal_3 Loss: 0.0724 | 0.0380
Epoch 60/300, seasonal_3 Loss: 0.0723 | 0.0378
Epoch 61/300, seasonal_3 Loss: 0.0721 | 0.0375
Epoch 62/300, seasonal_3 Loss: 0.0720 | 0.0374
Epoch 63/300, seasonal_3 Loss: 0.0719 | 0.0372
Epoch 64/300, seasonal_3 Loss: 0.0718 | 0.0370
Epoch 65/300, seasonal_3 Loss: 0.0718 | 0.0368
Epoch 66/300, seasonal_3 Loss: 0.0717 | 0.0367
Epoch 67/300, seasonal_3 Loss: 0.0716 | 0.0366
Epoch 68/300, seasonal_3 Loss: 0.0715 | 0.0365
Epoch 69/300, seasonal_3 Loss: 0.0715 | 0.0364
Epoch 70/300, seasonal_3 Loss: 0.0714 | 0.0362
Epoch 71/300, seasonal_3 Loss: 0.0713 | 0.0361
Epoch 72/300, seasonal_3 Loss: 0.0713 | 0.0360
Epoch 73/300, seasonal_3 Loss: 0.0712 | 0.0360
Epoch 74/300, seasonal_3 Loss: 0.0712 | 0.0359
Epoch 75/300, seasonal_3 Loss: 0.0711 | 0.0358
Epoch 76/300, seasonal_3 Loss: 0.0711 | 0.0357
Epoch 77/300, seasonal_3 Loss: 0.0710 | 0.0357
Epoch 78/300, seasonal_3 Loss: 0.0710 | 0.0356
Epoch 79/300, seasonal_3 Loss: 0.0710 | 0.0355
Epoch 80/300, seasonal_3 Loss: 0.0709 | 0.0355
Epoch 81/300, seasonal_3 Loss: 0.0709 | 0.0354
Epoch 82/300, seasonal_3 Loss: 0.0709 | 0.0354
Epoch 83/300, seasonal_3 Loss: 0.0708 | 0.0353
Epoch 84/300, seasonal_3 Loss: 0.0708 | 0.0353
Epoch 85/300, seasonal_3 Loss: 0.0708 | 0.0352
Epoch 86/300, seasonal_3 Loss: 0.0708 | 0.0352
Epoch 87/300, seasonal_3 Loss: 0.0707 | 0.0352
Epoch 88/300, seasonal_3 Loss: 0.0707 | 0.0351
Epoch 89/300, seasonal_3 Loss: 0.0707 | 0.0351
Epoch 90/300, seasonal_3 Loss: 0.0707 | 0.0350
Epoch 91/300, seasonal_3 Loss: 0.0707 | 0.0350
Epoch 92/300, seasonal_3 Loss: 0.0706 | 0.0350
Epoch 93/300, seasonal_3 Loss: 0.0706 | 0.0350
Epoch 94/300, seasonal_3 Loss: 0.0706 | 0.0349
Epoch 95/300, seasonal_3 Loss: 0.0706 | 0.0349
Epoch 96/300, seasonal_3 Loss: 0.0706 | 0.0349
Epoch 97/300, seasonal_3 Loss: 0.0706 | 0.0349
Epoch 98/300, seasonal_3 Loss: 0.0706 | 0.0349
Epoch 99/300, seasonal_3 Loss: 0.0705 | 0.0348
Epoch 100/300, seasonal_3 Loss: 0.0705 | 0.0348
Epoch 101/300, seasonal_3 Loss: 0.0705 | 0.0348
Epoch 102/300, seasonal_3 Loss: 0.0705 | 0.0348
Epoch 103/300, seasonal_3 Loss: 0.0705 | 0.0348
Epoch 104/300, seasonal_3 Loss: 0.0705 | 0.0348
Epoch 105/300, seasonal_3 Loss: 0.0705 | 0.0347
Epoch 106/300, seasonal_3 Loss: 0.0705 | 0.0347
Epoch 107/300, seasonal_3 Loss: 0.0705 | 0.0347
Epoch 108/300, seasonal_3 Loss: 0.0705 | 0.0347
Epoch 109/300, seasonal_3 Loss: 0.0704 | 0.0347
Epoch 110/300, seasonal_3 Loss: 0.0704 | 0.0347
Epoch 111/300, seasonal_3 Loss: 0.0704 | 0.0347
Epoch 112/300, seasonal_3 Loss: 0.0704 | 0.0347
Epoch 113/300, seasonal_3 Loss: 0.0704 | 0.0347
Epoch 114/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 115/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 116/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 117/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 118/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 119/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 120/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 121/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 122/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 123/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 124/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 125/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 126/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 127/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 128/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 129/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 130/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 131/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 132/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 133/300, seasonal_3 Loss: 0.0704 | 0.0346
Epoch 134/300, seasonal_3 Loss: 0.0704 | 0.0345
Epoch 135/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 136/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 137/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 138/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 139/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 140/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 141/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 142/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 143/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 144/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 145/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 146/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 147/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 148/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 149/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 150/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 151/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 152/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 153/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 154/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 155/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 156/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 157/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 158/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 159/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 160/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 161/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 162/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 163/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 164/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 165/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 166/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 167/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 168/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 169/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 170/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 171/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 172/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 173/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 174/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 175/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 176/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 177/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 178/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 179/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 180/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 181/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 182/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 183/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 184/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 185/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 186/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 187/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 188/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 189/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 190/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 191/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 192/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 193/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 194/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 195/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 196/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 197/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 198/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 199/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 200/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 201/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 202/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 203/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 204/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 205/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 206/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 207/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 208/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 209/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 210/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 211/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 212/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 213/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 214/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 215/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 216/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 217/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 218/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 219/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 220/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 221/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 222/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 223/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 224/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 225/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 226/300, seasonal_3 Loss: 0.0703 | 0.0345
Epoch 227/300, seasonal_3 Loss: 0.0703 | 0.0345
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9867856155777148, 'learning_rate': 0.00019211171180108288, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9446066996476656}
Epoch 1/300, resid Loss: 0.1358 | 0.0958
Epoch 2/300, resid Loss: 0.0988 | 0.0758
Epoch 3/300, resid Loss: 0.0909 | 0.0658
Epoch 4/300, resid Loss: 0.0851 | 0.0588
Epoch 5/300, resid Loss: 0.0815 | 0.0500
Epoch 6/300, resid Loss: 0.0794 | 0.0424
Epoch 7/300, resid Loss: 0.0768 | 0.0416
Epoch 8/300, resid Loss: 0.0734 | 0.0408
Epoch 9/300, resid Loss: 0.0701 | 0.0396
Epoch 10/300, resid Loss: 0.0675 | 0.0360
Epoch 11/300, resid Loss: 0.0654 | 0.0338
Epoch 12/300, resid Loss: 0.0642 | 0.0325
Epoch 13/300, resid Loss: 0.0630 | 0.0307
Epoch 14/300, resid Loss: 0.0621 | 0.0294
Epoch 15/300, resid Loss: 0.0617 | 0.0295
Epoch 16/300, resid Loss: 0.0611 | 0.0283
Epoch 17/300, resid Loss: 0.0612 | 0.0301
Epoch 18/300, resid Loss: 0.0612 | 0.0288
Epoch 19/300, resid Loss: 0.0602 | 0.0306
Epoch 20/300, resid Loss: 0.0597 | 0.0278
Epoch 21/300, resid Loss: 0.0590 | 0.0272
Epoch 22/300, resid Loss: 0.0587 | 0.0272
Epoch 23/300, resid Loss: 0.0580 | 0.0255
Epoch 24/300, resid Loss: 0.0572 | 0.0263
Epoch 25/300, resid Loss: 0.0568 | 0.0258
Epoch 26/300, resid Loss: 0.0559 | 0.0238
Epoch 27/300, resid Loss: 0.0553 | 0.0236
Epoch 28/300, resid Loss: 0.0543 | 0.0261
Epoch 29/300, resid Loss: 0.0533 | 0.0199
Epoch 30/300, resid Loss: 0.0531 | 0.0189
Epoch 31/300, resid Loss: 0.0522 | 0.0170
Epoch 32/300, resid Loss: 0.0515 | 0.0176
Epoch 33/300, resid Loss: 0.0507 | 0.0164
Epoch 34/300, resid Loss: 0.0503 | 0.0152
Epoch 35/300, resid Loss: 0.0498 | 0.0169
Epoch 36/300, resid Loss: 0.0494 | 0.0166
Epoch 37/300, resid Loss: 0.0500 | 0.0164
Epoch 38/300, resid Loss: 0.0498 | 0.0200
Epoch 39/300, resid Loss: 0.0495 | 0.0159
Epoch 40/300, resid Loss: 0.0493 | 0.0166
Epoch 41/300, resid Loss: 0.0491 | 0.0150
Epoch 42/300, resid Loss: 0.0482 | 0.0165
Epoch 43/300, resid Loss: 0.0482 | 0.0156
Epoch 44/300, resid Loss: 0.0479 | 0.0158
Epoch 45/300, resid Loss: 0.0472 | 0.0164
Epoch 46/300, resid Loss: 0.0506 | 0.0162
Epoch 47/300, resid Loss: 0.0515 | 0.0148
Epoch 48/300, resid Loss: 0.0492 | 0.0153
Epoch 49/300, resid Loss: 0.0476 | 0.0149
Epoch 50/300, resid Loss: 0.0468 | 0.0142
Epoch 51/300, resid Loss: 0.0465 | 0.0157
Epoch 52/300, resid Loss: 0.0462 | 0.0147
Epoch 53/300, resid Loss: 0.0457 | 0.0188
Epoch 54/300, resid Loss: 0.0459 | 0.0143
Epoch 55/300, resid Loss: 0.0457 | 0.0169
Epoch 56/300, resid Loss: 0.0453 | 0.0170
Epoch 57/300, resid Loss: 0.0452 | 0.0154
Epoch 58/300, resid Loss: 0.0449 | 0.0159
Epoch 59/300, resid Loss: 0.0447 | 0.0165
Epoch 60/300, resid Loss: 0.0450 | 0.0157
Epoch 61/300, resid Loss: 0.0446 | 0.0147
Epoch 62/300, resid Loss: 0.0465 | 0.0227
Epoch 63/300, resid Loss: 0.0462 | 0.0154
Epoch 64/300, resid Loss: 0.0450 | 0.0155
Epoch 65/300, resid Loss: 0.0454 | 0.0142
Epoch 66/300, resid Loss: 0.0450 | 0.0151
Epoch 67/300, resid Loss: 0.0447 | 0.0181
Epoch 68/300, resid Loss: 0.0452 | 0.0152
Epoch 69/300, resid Loss: 0.0448 | 0.0149
Epoch 70/300, resid Loss: 0.0433 | 0.0181
Epoch 71/300, resid Loss: 0.0437 | 0.0181
Epoch 72/300, resid Loss: 0.0435 | 0.0194
Epoch 73/300, resid Loss: 0.0425 | 0.0164
Epoch 74/300, resid Loss: 0.0466 | 0.0231
Epoch 75/300, resid Loss: 0.0460 | 0.0143
Epoch 76/300, resid Loss: 0.0445 | 0.0217
Epoch 77/300, resid Loss: 0.0421 | 0.0141
Epoch 78/300, resid Loss: 0.0411 | 0.0142
Epoch 79/300, resid Loss: 0.0435 | 0.0139
Epoch 80/300, resid Loss: 0.0433 | 0.0143
Epoch 81/300, resid Loss: 0.0387 | 0.0147
Epoch 82/300, resid Loss: 0.0382 | 0.0174
Epoch 83/300, resid Loss: 0.0427 | 0.0145
Epoch 84/300, resid Loss: 0.0388 | 0.0149
Epoch 85/300, resid Loss: 0.0380 | 0.0151
Epoch 86/300, resid Loss: 0.0384 | 0.0151
Epoch 87/300, resid Loss: 0.0430 | 0.0157
Epoch 88/300, resid Loss: 0.0562 | 0.0221
Epoch 89/300, resid Loss: 0.0456 | 0.0182
Epoch 90/300, resid Loss: 0.0392 | 0.0156
Epoch 91/300, resid Loss: 0.0381 | 0.0157
Epoch 92/300, resid Loss: 0.0370 | 0.0157
Epoch 93/300, resid Loss: 0.0408 | 0.0166
Epoch 94/300, resid Loss: 0.0409 | 0.0159
Epoch 95/300, resid Loss: 0.0368 | 0.0162
Epoch 96/300, resid Loss: 0.0368 | 0.0163
Epoch 97/300, resid Loss: 0.0358 | 0.0176
Epoch 98/300, resid Loss: 0.0369 | 0.0181
Epoch 99/300, resid Loss: 0.0363 | 0.0183
Epoch 100/300, resid Loss: 0.0398 | 0.0160
Epoch 101/300, resid Loss: 0.0368 | 0.0166
Epoch 102/300, resid Loss: 0.0357 | 0.0168
Epoch 103/300, resid Loss: 0.0355 | 0.0166
Epoch 104/300, resid Loss: 0.0353 | 0.0167
Epoch 105/300, resid Loss: 0.0351 | 0.0170
Epoch 106/300, resid Loss: 0.0349 | 0.0174
Epoch 107/300, resid Loss: 0.0347 | 0.0177
Epoch 108/300, resid Loss: 0.0347 | 0.0204
Epoch 109/300, resid Loss: 0.0347 | 0.0212
Epoch 110/300, resid Loss: 0.0381 | 0.0195
Epoch 111/300, resid Loss: 0.0414 | 0.0191
Epoch 112/300, resid Loss: 0.0406 | 0.0202
Epoch 113/300, resid Loss: 0.0388 | 0.0216
Epoch 114/300, resid Loss: 0.0347 | 0.0200
Epoch 115/300, resid Loss: 0.0389 | 0.0206
Epoch 116/300, resid Loss: 0.0394 | 0.0209
Epoch 117/300, resid Loss: 0.0397 | 0.0196
Epoch 118/300, resid Loss: 0.0405 | 0.0193
Epoch 119/300, resid Loss: 0.0346 | 0.0202
Epoch 120/300, resid Loss: 0.0346 | 0.0195
Epoch 121/300, resid Loss: 0.0345 | 0.0190
Epoch 122/300, resid Loss: 0.0348 | 0.0200
Epoch 123/300, resid Loss: 0.0339 | 0.0175
Epoch 124/300, resid Loss: 0.0362 | 0.0165
Epoch 125/300, resid Loss: 0.0349 | 0.0190
Epoch 126/300, resid Loss: 0.0340 | 0.0181
Epoch 127/300, resid Loss: 0.0335 | 0.0198
Epoch 128/300, resid Loss: 0.0333 | 0.0202
Epoch 129/300, resid Loss: 0.0330 | 0.0205
Epoch 130/300, resid Loss: 0.0329 | 0.0212
Epoch 131/300, resid Loss: 0.0325 | 0.0211
Epoch 132/300, resid Loss: 0.0312 | 0.0208
Epoch 133/300, resid Loss: 0.0316 | 0.0229
Epoch 134/300, resid Loss: 0.0394 | 0.0225
Epoch 135/300, resid Loss: 0.0329 | 0.0231
Epoch 136/300, resid Loss: 0.0318 | 0.0228
Epoch 137/300, resid Loss: 0.0322 | 0.0222
Epoch 138/300, resid Loss: 0.0322 | 0.0224
Epoch 139/300, resid Loss: 0.0320 | 0.0203
Epoch 140/300, resid Loss: 0.0325 | 0.0211
Epoch 141/300, resid Loss: 0.0319 | 0.0208
Epoch 142/300, resid Loss: 0.0325 | 0.0186
Epoch 143/300, resid Loss: 0.0297 | 0.0216
Epoch 144/300, resid Loss: 0.0304 | 0.0240
Epoch 145/300, resid Loss: 0.0289 | 0.0228
Epoch 146/300, resid Loss: 0.0296 | 0.0234
Epoch 147/300, resid Loss: 0.0381 | 0.0233
Epoch 148/300, resid Loss: 0.0391 | 0.0209
Epoch 149/300, resid Loss: 0.0332 | 0.0235
Epoch 150/300, resid Loss: 0.0304 | 0.0228
Epoch 151/300, resid Loss: 0.0278 | 0.0218
Epoch 152/300, resid Loss: 0.0328 | 0.0213
Epoch 153/300, resid Loss: 0.0312 | 0.0207
Epoch 154/300, resid Loss: 0.0287 | 0.0217
Epoch 155/300, resid Loss: 0.0273 | 0.0216
Epoch 156/300, resid Loss: 0.0269 | 0.0206
Epoch 157/300, resid Loss: 0.0273 | 0.0221
Epoch 158/300, resid Loss: 0.0327 | 0.0214
Epoch 159/300, resid Loss: 0.0378 | 0.0209
Epoch 160/300, resid Loss: 0.0327 | 0.0224
Epoch 161/300, resid Loss: 0.0301 | 0.0225
Epoch 162/300, resid Loss: 0.0280 | 0.0233
Epoch 163/300, resid Loss: 0.0272 | 0.0235
Epoch 164/300, resid Loss: 0.0266 | 0.0235
Epoch 165/300, resid Loss: 0.0259 | 0.0230
Epoch 166/300, resid Loss: 0.0255 | 0.0233
Epoch 167/300, resid Loss: 0.0253 | 0.0223
Epoch 168/300, resid Loss: 0.0258 | 0.0225
Epoch 169/300, resid Loss: 0.0278 | 0.0220
Epoch 170/300, resid Loss: 0.0274 | 0.0224
Epoch 171/300, resid Loss: 0.0271 | 0.0221
Epoch 172/300, resid Loss: 0.0269 | 0.0222
Epoch 173/300, resid Loss: 0.0268 | 0.0229
Epoch 174/300, resid Loss: 0.0255 | 0.0244
Epoch 175/300, resid Loss: 0.0267 | 0.0230
Epoch 176/300, resid Loss: 0.0256 | 0.0228
Epoch 177/300, resid Loss: 0.0253 | 0.0228
Epoch 178/300, resid Loss: 0.0268 | 0.0217
Epoch 179/300, resid Loss: 0.0252 | 0.0238
Epoch 180/300, resid Loss: 0.0347 | 0.0227
Epoch 181/300, resid Loss: 0.0310 | 0.0219
Epoch 182/300, resid Loss: 0.0289 | 0.0239
Epoch 183/300, resid Loss: 0.0261 | 0.0230
Epoch 184/300, resid Loss: 0.0247 | 0.0243
Epoch 185/300, resid Loss: 0.0256 | 0.0205
Epoch 186/300, resid Loss: 0.0268 | 0.0204
Epoch 187/300, resid Loss: 0.0257 | 0.0228
Epoch 188/300, resid Loss: 0.0238 | 0.0224
Epoch 189/300, resid Loss: 0.0235 | 0.0236
Epoch 190/300, resid Loss: 0.0230 | 0.0243
Epoch 191/300, resid Loss: 0.0248 | 0.0203
Epoch 192/300, resid Loss: 0.0267 | 0.0215
Epoch 193/300, resid Loss: 0.0235 | 0.0222
Epoch 194/300, resid Loss: 0.0229 | 0.0217
Epoch 195/300, resid Loss: 0.0227 | 0.0226
Epoch 196/300, resid Loss: 0.0226 | 0.0231
Epoch 197/300, resid Loss: 0.0260 | 0.0224
Epoch 198/300, resid Loss: 0.0269 | 0.0218
Epoch 199/300, resid Loss: 0.0240 | 0.0234
Epoch 200/300, resid Loss: 0.0265 | 0.0228
Epoch 201/300, resid Loss: 0.0239 | 0.0219
Epoch 202/300, resid Loss: 0.0240 | 0.0227
Epoch 203/300, resid Loss: 0.0215 | 0.0230
Epoch 204/300, resid Loss: 0.0219 | 0.0225
Epoch 205/300, resid Loss: 0.0227 | 0.0228
Epoch 206/300, resid Loss: 0.0258 | 0.0233
Epoch 207/300, resid Loss: 0.0246 | 0.0236
Epoch 208/300, resid Loss: 0.0243 | 0.0241
Epoch 209/300, resid Loss: 0.0234 | 0.0254
Epoch 210/300, resid Loss: 0.0256 | 0.0244
Epoch 211/300, resid Loss: 0.0266 | 0.0257
Epoch 212/300, resid Loss: 0.0243 | 0.0247
Epoch 213/300, resid Loss: 0.0237 | 0.0239
Epoch 214/300, resid Loss: 0.0225 | 0.0241
Epoch 215/300, resid Loss: 0.0227 | 0.0236
Epoch 216/300, resid Loss: 0.0224 | 0.0260
Epoch 217/300, resid Loss: 0.0315 | 0.0255
Epoch 218/300, resid Loss: 0.0210 | 0.0251
Epoch 219/300, resid Loss: 0.0213 | 0.0225
Epoch 220/300, resid Loss: 0.0259 | 0.0271
Epoch 221/300, resid Loss: 0.0243 | 0.0233
Epoch 222/300, resid Loss: 0.0210 | 0.0240
Epoch 223/300, resid Loss: 0.0230 | 0.0218
Epoch 224/300, resid Loss: 0.0235 | 0.0244
Epoch 225/300, resid Loss: 0.0213 | 0.0247
Epoch 226/300, resid Loss: 0.0205 | 0.0231
Epoch 227/300, resid Loss: 0.0211 | 0.0229
Epoch 228/300, resid Loss: 0.0207 | 0.0230
Epoch 229/300, resid Loss: 0.0204 | 0.0230
Epoch 230/300, resid Loss: 0.0206 | 0.0244
Epoch 231/300, resid Loss: 0.0231 | 0.0207
Epoch 232/300, resid Loss: 0.0249 | 0.0233
Epoch 233/300, resid Loss: 0.0227 | 0.0232
Epoch 234/300, resid Loss: 0.0210 | 0.0237
Epoch 235/300, resid Loss: 0.0208 | 0.0238
Epoch 236/300, resid Loss: 0.0206 | 0.0242
Epoch 237/300, resid Loss: 0.0205 | 0.0241
Epoch 238/300, resid Loss: 0.0204 | 0.0244
Epoch 239/300, resid Loss: 0.0203 | 0.0242
Epoch 240/300, resid Loss: 0.0202 | 0.0244
Epoch 241/300, resid Loss: 0.0202 | 0.0243
Epoch 242/300, resid Loss: 0.0204 | 0.0244
Epoch 243/300, resid Loss: 0.0206 | 0.0245
Epoch 244/300, resid Loss: 0.0204 | 0.0244
Epoch 245/300, resid Loss: 0.0216 | 0.0267
Epoch 246/300, resid Loss: 0.0325 | 0.0237
Epoch 247/300, resid Loss: 0.0207 | 0.0242
Epoch 248/300, resid Loss: 0.0199 | 0.0243
Epoch 249/300, resid Loss: 0.0200 | 0.0244
Epoch 250/300, resid Loss: 0.0197 | 0.0246
Epoch 251/300, resid Loss: 0.0199 | 0.0243
Epoch 252/300, resid Loss: 0.0199 | 0.0245
Epoch 253/300, resid Loss: 0.0196 | 0.0247
Epoch 254/300, resid Loss: 0.0194 | 0.0245
Epoch 255/300, resid Loss: 0.0194 | 0.0247
Epoch 256/300, resid Loss: 0.0193 | 0.0244
Epoch 257/300, resid Loss: 0.0197 | 0.0252
Epoch 258/300, resid Loss: 0.0198 | 0.0238
Epoch 259/300, resid Loss: 0.0200 | 0.0243
Epoch 260/300, resid Loss: 0.0194 | 0.0246
Epoch 261/300, resid Loss: 0.0191 | 0.0247
Epoch 262/300, resid Loss: 0.0187 | 0.0254
Epoch 263/300, resid Loss: 0.0188 | 0.0248
Epoch 264/300, resid Loss: 0.0197 | 0.0246
Epoch 265/300, resid Loss: 0.0190 | 0.0244
Epoch 266/300, resid Loss: 0.0216 | 0.0260
Epoch 267/300, resid Loss: 0.0199 | 0.0239
Epoch 268/300, resid Loss: 0.0199 | 0.0247
Epoch 269/300, resid Loss: 0.0193 | 0.0252
Epoch 270/300, resid Loss: 0.0190 | 0.0251
Epoch 271/300, resid Loss: 0.0188 | 0.0251
Epoch 272/300, resid Loss: 0.0187 | 0.0251
Epoch 273/300, resid Loss: 0.0186 | 0.0252
Epoch 274/300, resid Loss: 0.0185 | 0.0252
Epoch 275/300, resid Loss: 0.0183 | 0.0253
Epoch 276/300, resid Loss: 0.0184 | 0.0249
Epoch 277/300, resid Loss: 0.0198 | 0.0254
Epoch 278/300, resid Loss: 0.0188 | 0.0257
Epoch 279/300, resid Loss: 0.0183 | 0.0253
Epoch 280/300, resid Loss: 0.0184 | 0.0265
Epoch 281/300, resid Loss: 0.0180 | 0.0269
Epoch 282/300, resid Loss: 0.0179 | 0.0253
Epoch 283/300, resid Loss: 0.0182 | 0.0263
Epoch 284/300, resid Loss: 0.0215 | 0.0257
Epoch 285/300, resid Loss: 0.0197 | 0.0259
Epoch 286/300, resid Loss: 0.0210 | 0.0249
Epoch 287/300, resid Loss: 0.0188 | 0.0254
Epoch 288/300, resid Loss: 0.0186 | 0.0265
Epoch 289/300, resid Loss: 0.0191 | 0.0254
Epoch 290/300, resid Loss: 0.0186 | 0.0261
Epoch 291/300, resid Loss: 0.0181 | 0.0253
Epoch 292/300, resid Loss: 0.0183 | 0.0254
Epoch 293/300, resid Loss: 0.0178 | 0.0258
Epoch 294/300, resid Loss: 0.0176 | 0.0258
Epoch 295/300, resid Loss: 0.0174 | 0.0263
Epoch 296/300, resid Loss: 0.0169 | 0.0264
Epoch 297/300, resid Loss: 0.0164 | 0.0265
Epoch 298/300, resid Loss: 0.0175 | 0.0270
Epoch 299/300, resid Loss: 0.0172 | 0.0262
Epoch 300/300, resid Loss: 0.0169 | 0.0259
Runtime (seconds): 3203.516660928726
0.00010200149933914854
[217.31056]
[0.5759149]
[0.69364583]
[4.433118]
[0.5484903]
[6.351817]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.0018978083971887827
RMSE: 0.0435638427734375
MAE: 0.0435638427734375
R-squared: nan
[229.91356]
File aapl_stock_price_prediction_by_iTransformer.png exists. Logging to WandB.
