ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-05 06:51:31,864][0m A new study created in memory with name: no-name-917bc23e-5dd5-475b-86d0-d24c215371da[0m
[32m[I 2025-01-05 06:52:10,823][0m Trial 0 finished with value: 0.29132354259490967 and parameters: {'observation_period_num': 108, 'train_rates': 0.9730170789003689, 'learning_rate': 1.3574042227583114e-05, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8304566387056456}. Best is trial 0 with value: 0.29132354259490967.[0m
[32m[I 2025-01-05 06:52:40,855][0m Trial 1 finished with value: 0.45585614381900635 and parameters: {'observation_period_num': 152, 'train_rates': 0.7907022516051783, 'learning_rate': 9.821487379650555e-06, 'batch_size': 178, 'step_size': 6, 'gamma': 0.7864426928656003}. Best is trial 0 with value: 0.29132354259490967.[0m
[32m[I 2025-01-05 06:53:25,891][0m Trial 2 finished with value: 0.43605324625968933 and parameters: {'observation_period_num': 228, 'train_rates': 0.9722252835379636, 'learning_rate': 6.820217970449686e-06, 'batch_size': 127, 'step_size': 6, 'gamma': 0.7698621779806935}. Best is trial 0 with value: 0.29132354259490967.[0m
[32m[I 2025-01-05 06:55:12,611][0m Trial 3 finished with value: 0.08727781125423262 and parameters: {'observation_period_num': 85, 'train_rates': 0.7947301510266143, 'learning_rate': 0.00018415503645318444, 'batch_size': 47, 'step_size': 14, 'gamma': 0.9575149234438118}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 06:55:44,510][0m Trial 4 finished with value: 0.11535571298901007 and parameters: {'observation_period_num': 184, 'train_rates': 0.8278318257291143, 'learning_rate': 7.539988544480716e-05, 'batch_size': 171, 'step_size': 8, 'gamma': 0.8897364270197434}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 06:56:11,921][0m Trial 5 finished with value: 0.386297440671713 and parameters: {'observation_period_num': 157, 'train_rates': 0.6719663045003695, 'learning_rate': 1.6663083238222016e-05, 'batch_size': 175, 'step_size': 9, 'gamma': 0.9577260335321538}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 06:59:41,953][0m Trial 6 finished with value: 0.20599581445417098 and parameters: {'observation_period_num': 234, 'train_rates': 0.9659702834853107, 'learning_rate': 4.7388249577392845e-06, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8989435615370776}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:01:53,391][0m Trial 7 finished with value: 0.15772764012217522 and parameters: {'observation_period_num': 43, 'train_rates': 0.8595326154180385, 'learning_rate': 2.7405978658687922e-06, 'batch_size': 41, 'step_size': 15, 'gamma': 0.7807321371526362}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:02:53,027][0m Trial 8 finished with value: 0.1559196720069105 and parameters: {'observation_period_num': 231, 'train_rates': 0.8387064615459788, 'learning_rate': 0.0003870394350106606, 'batch_size': 86, 'step_size': 9, 'gamma': 0.8513974362218882}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:03:31,438][0m Trial 9 finished with value: 1.5786048974955826 and parameters: {'observation_period_num': 175, 'train_rates': 0.7061574011725238, 'learning_rate': 2.933739590243663e-06, 'batch_size': 124, 'step_size': 2, 'gamma': 0.7809339176228761}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:03:55,383][0m Trial 10 finished with value: 0.16213664116448914 and parameters: {'observation_period_num': 16, 'train_rates': 0.7431796944084632, 'learning_rate': 0.0007663788688340441, 'batch_size': 240, 'step_size': 14, 'gamma': 0.974815284383278}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:04:22,892][0m Trial 11 finished with value: 0.13727511137640908 and parameters: {'observation_period_num': 104, 'train_rates': 0.8743401187405445, 'learning_rate': 0.00011588292444740356, 'batch_size': 224, 'step_size': 12, 'gamma': 0.9163555508626607}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:05:18,513][0m Trial 12 finished with value: 0.2074676804147216 and parameters: {'observation_period_num': 67, 'train_rates': 0.6290853948431324, 'learning_rate': 8.06036721452884e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9203765393531244}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:05:44,205][0m Trial 13 finished with value: 0.19280009255760092 and parameters: {'observation_period_num': 189, 'train_rates': 0.7841407442959842, 'learning_rate': 6.838680246513169e-05, 'batch_size': 206, 'step_size': 2, 'gamma': 0.9426596909953092}. Best is trial 3 with value: 0.08727781125423262.[0m
[32m[I 2025-01-05 07:06:54,661][0m Trial 14 finished with value: 0.07609785722178433 and parameters: {'observation_period_num': 73, 'train_rates': 0.9122363887776679, 'learning_rate': 0.00019741145765961982, 'batch_size': 80, 'step_size': 12, 'gamma': 0.8816895017931297}. Best is trial 14 with value: 0.07609785722178433.[0m
[32m[I 2025-01-05 07:07:56,786][0m Trial 15 finished with value: 0.08163245981891773 and parameters: {'observation_period_num': 76, 'train_rates': 0.9224345745638702, 'learning_rate': 0.0002529996480913657, 'batch_size': 93, 'step_size': 13, 'gamma': 0.8269008049908421}. Best is trial 14 with value: 0.07609785722178433.[0m
[32m[I 2025-01-05 07:09:01,467][0m Trial 16 finished with value: 0.044089733604415954 and parameters: {'observation_period_num': 44, 'train_rates': 0.9137880077723587, 'learning_rate': 0.0002954560335064723, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8227765658169454}. Best is trial 16 with value: 0.044089733604415954.[0m
[32m[I 2025-01-05 07:10:21,620][0m Trial 17 finished with value: 0.02830572787891416 and parameters: {'observation_period_num': 7, 'train_rates': 0.9078138458194183, 'learning_rate': 0.0009032412836318108, 'batch_size': 72, 'step_size': 11, 'gamma': 0.8601914383525561}. Best is trial 17 with value: 0.02830572787891416.[0m
[32m[I 2025-01-05 07:11:15,226][0m Trial 18 finished with value: 0.02799537314699506 and parameters: {'observation_period_num': 6, 'train_rates': 0.9104336356403475, 'learning_rate': 0.0008357054455663275, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8206007608157193}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:12:59,244][0m Trial 19 finished with value: 0.3036347884142731 and parameters: {'observation_period_num': 5, 'train_rates': 0.8906354058414823, 'learning_rate': 1.1152409346917165e-06, 'batch_size': 54, 'step_size': 10, 'gamma': 0.857705278808988}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:13:51,253][0m Trial 20 finished with value: 0.04822063206110971 and parameters: {'observation_period_num': 39, 'train_rates': 0.9394717051279443, 'learning_rate': 0.0006303613197630312, 'batch_size': 115, 'step_size': 10, 'gamma': 0.7518723017326525}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:14:47,871][0m Trial 21 finished with value: 0.04196613050393156 and parameters: {'observation_period_num': 30, 'train_rates': 0.8988355932532024, 'learning_rate': 0.0009672767700205164, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8120533064491182}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:15:26,315][0m Trial 22 finished with value: 0.039393494258055814 and parameters: {'observation_period_num': 28, 'train_rates': 0.8360078748454823, 'learning_rate': 0.000984573516623884, 'batch_size': 151, 'step_size': 4, 'gamma': 0.8098085966691365}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:16:04,991][0m Trial 23 finished with value: 0.040055799435396666 and parameters: {'observation_period_num': 20, 'train_rates': 0.8340183189641507, 'learning_rate': 0.0004903926742980222, 'batch_size': 144, 'step_size': 4, 'gamma': 0.8018921602414955}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:16:46,481][0m Trial 24 finished with value: 0.12309477762097404 and parameters: {'observation_period_num': 53, 'train_rates': 0.9423986725350894, 'learning_rate': 3.7456716656167436e-05, 'batch_size': 148, 'step_size': 4, 'gamma': 0.8453665241588246}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:18:08,924][0m Trial 25 finished with value: 0.0368317414361697 and parameters: {'observation_period_num': 9, 'train_rates': 0.8632339221604738, 'learning_rate': 0.0009797282996024713, 'batch_size': 67, 'step_size': 1, 'gamma': 0.8676464784957032}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:19:30,738][0m Trial 26 finished with value: 0.03936897331456605 and parameters: {'observation_period_num': 5, 'train_rates': 0.8714219914369065, 'learning_rate': 0.0004509670853231329, 'batch_size': 69, 'step_size': 1, 'gamma': 0.8744340378949449}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:21:01,922][0m Trial 27 finished with value: 0.06202493245874703 and parameters: {'observation_period_num': 55, 'train_rates': 0.9385717894029536, 'learning_rate': 0.00015300769399900017, 'batch_size': 64, 'step_size': 8, 'gamma': 0.865037253270785}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:24:45,976][0m Trial 28 finished with value: 0.16216415103634643 and parameters: {'observation_period_num': 5, 'train_rates': 0.7584080679503349, 'learning_rate': 0.0005834568484043669, 'batch_size': 22, 'step_size': 10, 'gamma': 0.9045325586239246}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:25:43,771][0m Trial 29 finished with value: 0.11442064493894577 and parameters: {'observation_period_num': 100, 'train_rates': 0.986555821242663, 'learning_rate': 3.330862363027724e-05, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8408354319608333}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:27:03,430][0m Trial 30 finished with value: 0.07789248789697432 and parameters: {'observation_period_num': 132, 'train_rates': 0.8837755444748778, 'learning_rate': 0.00031322845117225513, 'batch_size': 68, 'step_size': 5, 'gamma': 0.8349068553216361}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:29:31,608][0m Trial 31 finished with value: 0.042606087714319923 and parameters: {'observation_period_num': 17, 'train_rates': 0.8663219820498808, 'learning_rate': 0.00046731208270334444, 'batch_size': 37, 'step_size': 1, 'gamma': 0.8737528884531923}. Best is trial 18 with value: 0.02799537314699506.[0m
Early stopping at epoch 95
[32m[I 2025-01-05 07:30:50,977][0m Trial 32 finished with value: 0.03773017749550262 and parameters: {'observation_period_num': 7, 'train_rates': 0.8598111598427642, 'learning_rate': 0.0009923683366005352, 'batch_size': 67, 'step_size': 1, 'gamma': 0.8644100080310325}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:32:19,351][0m Trial 33 finished with value: 0.039974651273009004 and parameters: {'observation_period_num': 30, 'train_rates': 0.8194365639970523, 'learning_rate': 0.0008662280524843366, 'batch_size': 60, 'step_size': 2, 'gamma': 0.8610738838100964}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:33:16,668][0m Trial 34 finished with value: 0.05879628884441712 and parameters: {'observation_period_num': 59, 'train_rates': 0.8534314716011416, 'learning_rate': 0.0006358068339200208, 'batch_size': 98, 'step_size': 3, 'gamma': 0.891169709137922}. Best is trial 18 with value: 0.02799537314699506.[0m
Early stopping at epoch 61
[32m[I 2025-01-05 07:33:45,220][0m Trial 35 finished with value: 0.14303939188680342 and parameters: {'observation_period_num': 88, 'train_rates': 0.8056840046638476, 'learning_rate': 0.00034318383137717587, 'batch_size': 121, 'step_size': 1, 'gamma': 0.7941230404782049}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:34:59,442][0m Trial 36 finished with value: 0.3153950260129086 and parameters: {'observation_period_num': 209, 'train_rates': 0.9530251114761404, 'learning_rate': 1.7665709224610356e-05, 'batch_size': 75, 'step_size': 3, 'gamma': 0.8213755482649896}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:35:42,286][0m Trial 37 finished with value: 0.08535801105843055 and parameters: {'observation_period_num': 252, 'train_rates': 0.9013732185688941, 'learning_rate': 0.00021369990592590228, 'batch_size': 133, 'step_size': 6, 'gamma': 0.8482931034620523}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:37:59,179][0m Trial 38 finished with value: 0.2535095738513129 and parameters: {'observation_period_num': 123, 'train_rates': 0.7776163693324682, 'learning_rate': 0.0006820616464818072, 'batch_size': 36, 'step_size': 9, 'gamma': 0.882941230721883}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:39:37,841][0m Trial 39 finished with value: 0.03933107012182201 and parameters: {'observation_period_num': 20, 'train_rates': 0.8515443080306605, 'learning_rate': 0.00013198519824112503, 'batch_size': 54, 'step_size': 8, 'gamma': 0.9126445697152459}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:41:10,483][0m Trial 40 finished with value: 0.08975327688209507 and parameters: {'observation_period_num': 40, 'train_rates': 0.8131247993407326, 'learning_rate': 0.00040851793581576, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8355002108914821}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:46:11,079][0m Trial 41 finished with value: 0.0515740126610075 and parameters: {'observation_period_num': 19, 'train_rates': 0.8514317977112935, 'learning_rate': 0.00012752872797093412, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9285167726698551}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:48:06,364][0m Trial 42 finished with value: 0.041958475185017434 and parameters: {'observation_period_num': 28, 'train_rates': 0.9260463749387565, 'learning_rate': 0.000992070624472715, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9019094822441273}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:50:59,443][0m Trial 43 finished with value: 0.03199932880387335 and parameters: {'observation_period_num': 10, 'train_rates': 0.8867413911776392, 'learning_rate': 0.0006596285515880254, 'batch_size': 32, 'step_size': 13, 'gamma': 0.869534708756672}. Best is trial 18 with value: 0.02799537314699506.[0m
[32m[I 2025-01-05 07:53:48,981][0m Trial 44 finished with value: 0.024782126847968763 and parameters: {'observation_period_num': 5, 'train_rates': 0.8877022044701688, 'learning_rate': 0.0006636125083983942, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8543416215823412}. Best is trial 44 with value: 0.024782126847968763.[0m
[32m[I 2025-01-05 07:56:44,636][0m Trial 45 finished with value: 0.051050164386630056 and parameters: {'observation_period_num': 44, 'train_rates': 0.9572881394670526, 'learning_rate': 0.0006119848512658963, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8522017729115856}. Best is trial 44 with value: 0.024782126847968763.[0m
[32m[I 2025-01-05 08:00:08,239][0m Trial 46 finished with value: 0.04206868658798772 and parameters: {'observation_period_num': 14, 'train_rates': 0.8870343436522424, 'learning_rate': 1.028713135588728e-05, 'batch_size': 27, 'step_size': 14, 'gamma': 0.8857114397478002}. Best is trial 44 with value: 0.024782126847968763.[0m
[32m[I 2025-01-05 08:02:21,103][0m Trial 47 finished with value: 0.04827828545655523 and parameters: {'observation_period_num': 31, 'train_rates': 0.9714397418445501, 'learning_rate': 0.00025963984084335245, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8731393998271341}. Best is trial 44 with value: 0.024782126847968763.[0m
[32m[I 2025-01-05 08:02:56,244][0m Trial 48 finished with value: 0.1074852348542681 and parameters: {'observation_period_num': 154, 'train_rates': 0.9090638628973688, 'learning_rate': 0.0005068337728190439, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8945067476386787}. Best is trial 44 with value: 0.024782126847968763.[0m
[32m[I 2025-01-05 08:04:09,469][0m Trial 49 finished with value: 0.1133826346975742 and parameters: {'observation_period_num': 55, 'train_rates': 0.9306499282552753, 'learning_rate': 0.0007199109595248716, 'batch_size': 80, 'step_size': 14, 'gamma': 0.8580102940907771}. Best is trial 44 with value: 0.024782126847968763.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-05 08:04:09,480][0m A new study created in memory with name: no-name-13a314cf-0f67-4306-83d1-ca57786567a2[0m
[32m[I 2025-01-05 08:08:29,425][0m Trial 0 finished with value: 0.2563549268247692 and parameters: {'observation_period_num': 206, 'train_rates': 0.7283413803746639, 'learning_rate': 8.38367055993815e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9092642466756657}. Best is trial 0 with value: 0.2563549268247692.[0m
[32m[I 2025-01-05 08:09:39,046][0m Trial 1 finished with value: 0.4656989963158317 and parameters: {'observation_period_num': 242, 'train_rates': 0.9663839508521872, 'learning_rate': 3.169847688269689e-06, 'batch_size': 81, 'step_size': 9, 'gamma': 0.839440497525497}. Best is trial 0 with value: 0.2563549268247692.[0m
[32m[I 2025-01-05 08:10:48,263][0m Trial 2 finished with value: 0.18426265927348057 and parameters: {'observation_period_num': 58, 'train_rates': 0.6060256388150238, 'learning_rate': 0.0002410625072613395, 'batch_size': 61, 'step_size': 6, 'gamma': 0.8360703465935938}. Best is trial 2 with value: 0.18426265927348057.[0m
[32m[I 2025-01-05 08:13:14,161][0m Trial 3 finished with value: 0.11000381333590274 and parameters: {'observation_period_num': 224, 'train_rates': 0.8789432132507791, 'learning_rate': 1.0623832255139697e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9072081764497366}. Best is trial 3 with value: 0.11000381333590274.[0m
[32m[I 2025-01-05 08:13:37,926][0m Trial 4 finished with value: 0.15410065544503077 and parameters: {'observation_period_num': 170, 'train_rates': 0.8190392859254716, 'learning_rate': 3.780824303387882e-05, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9809371948262317}. Best is trial 3 with value: 0.11000381333590274.[0m
[32m[I 2025-01-05 08:15:16,863][0m Trial 5 finished with value: 0.22949376304024532 and parameters: {'observation_period_num': 26, 'train_rates': 0.7235222365427855, 'learning_rate': 5.98107776255613e-06, 'batch_size': 49, 'step_size': 12, 'gamma': 0.787984320333308}. Best is trial 3 with value: 0.11000381333590274.[0m
[32m[I 2025-01-05 08:15:41,722][0m Trial 6 finished with value: 0.22352786173749153 and parameters: {'observation_period_num': 94, 'train_rates': 0.638175927295809, 'learning_rate': 8.156805785387146e-05, 'batch_size': 202, 'step_size': 14, 'gamma': 0.793145746496113}. Best is trial 3 with value: 0.11000381333590274.[0m
[32m[I 2025-01-05 08:17:13,341][0m Trial 7 finished with value: 0.1410896934568882 and parameters: {'observation_period_num': 221, 'train_rates': 0.8064335907586139, 'learning_rate': 0.00021872640897481, 'batch_size': 53, 'step_size': 9, 'gamma': 0.7545425305343301}. Best is trial 3 with value: 0.11000381333590274.[0m
[32m[I 2025-01-05 08:17:54,677][0m Trial 8 finished with value: 0.06639809205995074 and parameters: {'observation_period_num': 8, 'train_rates': 0.8103025887845143, 'learning_rate': 2.8933141324183156e-05, 'batch_size': 139, 'step_size': 9, 'gamma': 0.8407694188490773}. Best is trial 8 with value: 0.06639809205995074.[0m
[32m[I 2025-01-05 08:19:16,474][0m Trial 9 finished with value: 0.30618417730189595 and parameters: {'observation_period_num': 152, 'train_rates': 0.7781069890365413, 'learning_rate': 2.8589395917584663e-06, 'batch_size': 59, 'step_size': 13, 'gamma': 0.9810312921725904}. Best is trial 8 with value: 0.06639809205995074.[0m
[32m[I 2025-01-05 08:20:01,700][0m Trial 10 finished with value: 0.053700133050615724 and parameters: {'observation_period_num': 8, 'train_rates': 0.9247578717856353, 'learning_rate': 0.000601743327758726, 'batch_size': 135, 'step_size': 1, 'gamma': 0.8914362269549471}. Best is trial 10 with value: 0.053700133050615724.[0m
[32m[I 2025-01-05 08:20:48,485][0m Trial 11 finished with value: 0.06321068853139877 and parameters: {'observation_period_num': 5, 'train_rates': 0.9838856802697095, 'learning_rate': 0.0007440101739190551, 'batch_size': 137, 'step_size': 1, 'gamma': 0.8915542708056775}. Best is trial 10 with value: 0.053700133050615724.[0m
[32m[I 2025-01-05 08:21:33,827][0m Trial 12 finished with value: 0.08338727056980133 and parameters: {'observation_period_num': 64, 'train_rates': 0.9733559557496546, 'learning_rate': 0.0008930869320364656, 'batch_size': 135, 'step_size': 1, 'gamma': 0.9070452048988774}. Best is trial 10 with value: 0.053700133050615724.[0m
[32m[I 2025-01-05 08:22:17,633][0m Trial 13 finished with value: 0.06577017136112022 and parameters: {'observation_period_num': 109, 'train_rates': 0.9055398419010569, 'learning_rate': 0.0009977986341352825, 'batch_size': 130, 'step_size': 1, 'gamma': 0.9330571940181405}. Best is trial 10 with value: 0.053700133050615724.[0m
[32m[I 2025-01-05 08:22:52,720][0m Trial 14 finished with value: 0.05642387458658594 and parameters: {'observation_period_num': 51, 'train_rates': 0.9126872389463782, 'learning_rate': 0.00038357040869664127, 'batch_size': 173, 'step_size': 3, 'gamma': 0.8718545212867845}. Best is trial 10 with value: 0.053700133050615724.[0m
[32m[I 2025-01-05 08:23:25,719][0m Trial 15 finished with value: 0.06946421050363116 and parameters: {'observation_period_num': 49, 'train_rates': 0.9071101219984127, 'learning_rate': 0.000328634518376621, 'batch_size': 183, 'step_size': 4, 'gamma': 0.8679696832325539}. Best is trial 10 with value: 0.053700133050615724.[0m
[32m[I 2025-01-05 08:23:59,483][0m Trial 16 finished with value: 0.04781119359435195 and parameters: {'observation_period_num': 87, 'train_rates': 0.8657353134778843, 'learning_rate': 0.00042038787395981867, 'batch_size': 176, 'step_size': 4, 'gamma': 0.9449363982102359}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:24:54,748][0m Trial 17 finished with value: 0.057744995141640686 and parameters: {'observation_period_num': 99, 'train_rates': 0.8637712229958772, 'learning_rate': 0.00011773808517060736, 'batch_size': 100, 'step_size': 6, 'gamma': 0.9477066574804651}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:25:21,778][0m Trial 18 finished with value: 0.15266462929489077 and parameters: {'observation_period_num': 130, 'train_rates': 0.8631214928353791, 'learning_rate': 2.7311830476990237e-05, 'batch_size': 217, 'step_size': 3, 'gamma': 0.9422626868461497}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:25:58,683][0m Trial 19 finished with value: 0.831811785697937 and parameters: {'observation_period_num': 72, 'train_rates': 0.9518563412498985, 'learning_rate': 1.0156057230201934e-06, 'batch_size': 169, 'step_size': 7, 'gamma': 0.9542887028546283}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:26:44,723][0m Trial 20 finished with value: 0.2470855552989703 and parameters: {'observation_period_num': 174, 'train_rates': 0.7663154411475637, 'learning_rate': 0.00048498895232379724, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8704690506342564}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:27:21,888][0m Trial 21 finished with value: 0.05712434442506896 and parameters: {'observation_period_num': 32, 'train_rates': 0.9263190411889667, 'learning_rate': 0.00015984275710730363, 'batch_size': 168, 'step_size': 3, 'gamma': 0.8761170891891129}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:27:50,922][0m Trial 22 finished with value: 0.0664377361536026 and parameters: {'observation_period_num': 77, 'train_rates': 0.9377173806852215, 'learning_rate': 0.0004244726092862016, 'batch_size': 221, 'step_size': 5, 'gamma': 0.9261751693031013}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:28:26,562][0m Trial 23 finished with value: 0.056565863505840815 and parameters: {'observation_period_num': 36, 'train_rates': 0.8412570501361093, 'learning_rate': 0.0005127811813752043, 'batch_size': 163, 'step_size': 2, 'gamma': 0.8550203689464346}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:29:17,461][0m Trial 24 finished with value: 0.20942898812782335 and parameters: {'observation_period_num': 124, 'train_rates': 0.8827398360848999, 'learning_rate': 5.39659376596017e-05, 'batch_size': 111, 'step_size': 2, 'gamma': 0.8103159644279819}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:29:48,516][0m Trial 25 finished with value: 0.06124469225024764 and parameters: {'observation_period_num': 80, 'train_rates': 0.9197489100851278, 'learning_rate': 0.0002713478518608298, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8907531545166183}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:30:24,512][0m Trial 26 finished with value: 0.05466793338272059 and parameters: {'observation_period_num': 41, 'train_rates': 0.8533952484077343, 'learning_rate': 0.0001319543383632357, 'batch_size': 160, 'step_size': 2, 'gamma': 0.9657222183342401}. Best is trial 16 with value: 0.04781119359435195.[0m
[32m[I 2025-01-05 08:31:00,896][0m Trial 27 finished with value: 0.040880161345380694 and parameters: {'observation_period_num': 16, 'train_rates': 0.8380463290468482, 'learning_rate': 0.00016965064226313724, 'batch_size': 157, 'step_size': 11, 'gamma': 0.9620486346714775}. Best is trial 27 with value: 0.040880161345380694.[0m
[32m[I 2025-01-05 08:31:36,037][0m Trial 28 finished with value: 0.16596367245854626 and parameters: {'observation_period_num': 25, 'train_rates': 0.7454221214642975, 'learning_rate': 0.0006235611428212829, 'batch_size': 147, 'step_size': 11, 'gamma': 0.989813752710069}. Best is trial 27 with value: 0.040880161345380694.[0m
[32m[I 2025-01-05 08:32:18,397][0m Trial 29 finished with value: 0.16009286335576767 and parameters: {'observation_period_num': 15, 'train_rates': 0.7131625668279491, 'learning_rate': 7.495066025895204e-05, 'batch_size': 120, 'step_size': 11, 'gamma': 0.9226228328668415}. Best is trial 27 with value: 0.040880161345380694.[0m
[32m[I 2025-01-05 08:33:21,300][0m Trial 30 finished with value: 0.10288685068641741 and parameters: {'observation_period_num': 137, 'train_rates': 0.8280918494767021, 'learning_rate': 1.5926671749450562e-05, 'batch_size': 82, 'step_size': 8, 'gamma': 0.9649677654650959}. Best is trial 27 with value: 0.040880161345380694.[0m
[32m[I 2025-01-05 08:33:57,026][0m Trial 31 finished with value: 0.051017278592968195 and parameters: {'observation_period_num': 39, 'train_rates': 0.8436516648437531, 'learning_rate': 0.00013602937476058096, 'batch_size': 154, 'step_size': 2, 'gamma': 0.9651631926906885}. Best is trial 27 with value: 0.040880161345380694.[0m
[32m[I 2025-01-05 08:34:36,712][0m Trial 32 finished with value: 0.037189461290836334 and parameters: {'observation_period_num': 21, 'train_rates': 0.8904002489195233, 'learning_rate': 0.00019926585335918107, 'batch_size': 152, 'step_size': 4, 'gamma': 0.9629769716260607}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:35:14,570][0m Trial 33 finished with value: 0.05938729847902837 and parameters: {'observation_period_num': 55, 'train_rates': 0.8810696555782014, 'learning_rate': 0.0001744676267368127, 'batch_size': 153, 'step_size': 5, 'gamma': 0.9633670329767231}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:35:44,380][0m Trial 34 finished with value: 0.06039449347639984 and parameters: {'observation_period_num': 22, 'train_rates': 0.8375875493216928, 'learning_rate': 0.0001200800585356198, 'batch_size': 197, 'step_size': 10, 'gamma': 0.9398332338948561}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:36:11,455][0m Trial 35 finished with value: 0.08162770252507012 and parameters: {'observation_period_num': 89, 'train_rates': 0.8838580567193068, 'learning_rate': 5.635804350516377e-05, 'batch_size': 212, 'step_size': 7, 'gamma': 0.9725199395087804}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:36:35,140][0m Trial 36 finished with value: 0.06963819471522442 and parameters: {'observation_period_num': 38, 'train_rates': 0.7883333796154268, 'learning_rate': 0.00022418363866451283, 'batch_size': 251, 'step_size': 4, 'gamma': 0.9171809365645032}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:37:06,759][0m Trial 37 finished with value: 0.05753868155627756 and parameters: {'observation_period_num': 66, 'train_rates': 0.8498643510935531, 'learning_rate': 9.120014100889795e-05, 'batch_size': 180, 'step_size': 6, 'gamma': 0.9546803901805986}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:37:32,209][0m Trial 38 finished with value: 0.06036510787206216 and parameters: {'observation_period_num': 47, 'train_rates': 0.8203208627833002, 'learning_rate': 0.000285066670620506, 'batch_size': 234, 'step_size': 15, 'gamma': 0.9848045992907923}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:38:24,528][0m Trial 39 finished with value: 0.16389534337674627 and parameters: {'observation_period_num': 19, 'train_rates': 0.6866259510136111, 'learning_rate': 3.8119689795696595e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9763356818737216}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:42:39,686][0m Trial 40 finished with value: 0.07917984645813704 and parameters: {'observation_period_num': 107, 'train_rates': 0.8946695509917624, 'learning_rate': 0.00017644414731858606, 'batch_size': 21, 'step_size': 4, 'gamma': 0.9556029324051086}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:43:21,169][0m Trial 41 finished with value: 0.045462463051080704 and parameters: {'observation_period_num': 6, 'train_rates': 0.9518723710781448, 'learning_rate': 0.0006189316709835149, 'batch_size': 155, 'step_size': 2, 'gamma': 0.9040341211530091}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:43:59,353][0m Trial 42 finished with value: 0.0518301662064028 and parameters: {'observation_period_num': 26, 'train_rates': 0.8011868795219858, 'learning_rate': 0.00033937611679738655, 'batch_size': 146, 'step_size': 2, 'gamma': 0.9057507789219726}. Best is trial 32 with value: 0.037189461290836334.[0m
[32m[I 2025-01-05 08:44:49,283][0m Trial 43 finished with value: 0.03189589112494769 and parameters: {'observation_period_num': 12, 'train_rates': 0.9397186771461513, 'learning_rate': 0.00023553633243291586, 'batch_size': 124, 'step_size': 12, 'gamma': 0.934763092092416}. Best is trial 43 with value: 0.03189589112494769.[0m
[32m[I 2025-01-05 08:45:38,852][0m Trial 44 finished with value: 0.03184920327460512 and parameters: {'observation_period_num': 10, 'train_rates': 0.9523780464605918, 'learning_rate': 0.0007288796127791349, 'batch_size': 123, 'step_size': 12, 'gamma': 0.9291736236497306}. Best is trial 44 with value: 0.03184920327460512.[0m
[32m[I 2025-01-05 08:46:30,144][0m Trial 45 finished with value: 0.034032121649105775 and parameters: {'observation_period_num': 7, 'train_rates': 0.953074218431051, 'learning_rate': 0.0007008789184749888, 'batch_size': 118, 'step_size': 13, 'gamma': 0.9310843962414361}. Best is trial 44 with value: 0.03184920327460512.[0m
[32m[I 2025-01-05 08:47:21,319][0m Trial 46 finished with value: 0.03176029399037361 and parameters: {'observation_period_num': 16, 'train_rates': 0.9620421701616301, 'learning_rate': 0.0007938350743920344, 'batch_size': 122, 'step_size': 13, 'gamma': 0.9305058971695567}. Best is trial 46 with value: 0.03176029399037361.[0m
[32m[I 2025-01-05 08:48:46,231][0m Trial 47 finished with value: 0.039316452553738716 and parameters: {'observation_period_num': 15, 'train_rates': 0.9609271058016237, 'learning_rate': 0.0009201081287394546, 'batch_size': 70, 'step_size': 13, 'gamma': 0.9286209765121101}. Best is trial 46 with value: 0.03176029399037361.[0m
[32m[I 2025-01-05 08:49:42,098][0m Trial 48 finished with value: 0.03722795844078064 and parameters: {'observation_period_num': 6, 'train_rates': 0.9814737516056088, 'learning_rate': 0.0006618450621263857, 'batch_size': 115, 'step_size': 12, 'gamma': 0.9174808437861159}. Best is trial 46 with value: 0.03176029399037361.[0m
[32m[I 2025-01-05 08:50:27,570][0m Trial 49 finished with value: 0.13193345819812974 and parameters: {'observation_period_num': 204, 'train_rates': 0.9446332038798172, 'learning_rate': 0.0007002999945293475, 'batch_size': 125, 'step_size': 13, 'gamma': 0.9344650537725979}. Best is trial 46 with value: 0.03176029399037361.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-05 08:50:27,580][0m A new study created in memory with name: no-name-13e762b9-87df-46fd-8616-874d78a9c333[0m
[32m[I 2025-01-05 08:50:52,811][0m Trial 0 finished with value: 0.398967955939487 and parameters: {'observation_period_num': 138, 'train_rates': 0.6528659488666897, 'learning_rate': 8.619096092425023e-05, 'batch_size': 191, 'step_size': 1, 'gamma': 0.8927826780152877}. Best is trial 0 with value: 0.398967955939487.[0m
[32m[I 2025-01-05 08:51:33,974][0m Trial 1 finished with value: 0.13825039565563202 and parameters: {'observation_period_num': 174, 'train_rates': 0.9567943481737403, 'learning_rate': 3.403302057652678e-05, 'batch_size': 145, 'step_size': 5, 'gamma': 0.9133735476547569}. Best is trial 1 with value: 0.13825039565563202.[0m
[32m[I 2025-01-05 08:54:37,117][0m Trial 2 finished with value: 0.028322571013687237 and parameters: {'observation_period_num': 5, 'train_rates': 0.8659174441811519, 'learning_rate': 0.00017229043313706128, 'batch_size': 30, 'step_size': 5, 'gamma': 0.8030731258904071}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 08:55:10,180][0m Trial 3 finished with value: 0.18815718995887196 and parameters: {'observation_period_num': 18, 'train_rates': 0.712008760446481, 'learning_rate': 3.081732321067204e-05, 'batch_size': 155, 'step_size': 11, 'gamma': 0.8960244423015252}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 08:56:13,667][0m Trial 4 finished with value: 0.21480559747097855 and parameters: {'observation_period_num': 148, 'train_rates': 0.9580675520215989, 'learning_rate': 6.278180747768463e-06, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9487701880275902}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 08:56:41,349][0m Trial 5 finished with value: 0.36976254790548296 and parameters: {'observation_period_num': 25, 'train_rates': 0.8496466563132362, 'learning_rate': 2.287181394768611e-06, 'batch_size': 224, 'step_size': 15, 'gamma': 0.9721136471723852}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 08:57:10,353][0m Trial 6 finished with value: 0.08973174542188644 and parameters: {'observation_period_num': 59, 'train_rates': 0.9849726523123346, 'learning_rate': 0.0009647788716390291, 'batch_size': 233, 'step_size': 1, 'gamma': 0.8895155217561697}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 08:57:37,247][0m Trial 7 finished with value: 0.6274926364421844 and parameters: {'observation_period_num': 153, 'train_rates': 0.8552111616405709, 'learning_rate': 1.6120569919429585e-06, 'batch_size': 203, 'step_size': 10, 'gamma': 0.859459513167999}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 08:59:40,053][0m Trial 8 finished with value: 0.23875329376453847 and parameters: {'observation_period_num': 116, 'train_rates': 0.6817234712890342, 'learning_rate': 1.8340615898369344e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9041384425558227}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:00:29,668][0m Trial 9 finished with value: 0.12389735023041705 and parameters: {'observation_period_num': 81, 'train_rates': 0.950373886700546, 'learning_rate': 2.9896869582384093e-05, 'batch_size': 119, 'step_size': 3, 'gamma': 0.899366959756003}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:02:37,661][0m Trial 10 finished with value: 0.11964963307747474 and parameters: {'observation_period_num': 223, 'train_rates': 0.7860465856685366, 'learning_rate': 0.0003812295354472592, 'batch_size': 37, 'step_size': 5, 'gamma': 0.7871836649126076}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:03:02,414][0m Trial 11 finished with value: 0.07813196172430727 and parameters: {'observation_period_num': 61, 'train_rates': 0.8795313805946853, 'learning_rate': 0.000868628018902014, 'batch_size': 253, 'step_size': 2, 'gamma': 0.8209777531290035}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:04:15,741][0m Trial 12 finished with value: 0.0358902048105481 and parameters: {'observation_period_num': 5, 'train_rates': 0.8821438413025607, 'learning_rate': 0.0002294717432252238, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8007600679245716}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:05:26,501][0m Trial 13 finished with value: 0.04707663754622141 and parameters: {'observation_period_num': 9, 'train_rates': 0.7923934449773167, 'learning_rate': 0.00017049684081315092, 'batch_size': 74, 'step_size': 6, 'gamma': 0.7501801330838613}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:06:50,130][0m Trial 14 finished with value: 0.06506791906921487 and parameters: {'observation_period_num': 93, 'train_rates': 0.8873100795884992, 'learning_rate': 0.0001521687178604303, 'batch_size': 66, 'step_size': 7, 'gamma': 0.82913201143422}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:11:14,371][0m Trial 15 finished with value: 0.1724850168059531 and parameters: {'observation_period_num': 40, 'train_rates': 0.7395341714873271, 'learning_rate': 0.00030503848402607875, 'batch_size': 18, 'step_size': 4, 'gamma': 0.7874129711158152}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:12:03,869][0m Trial 16 finished with value: 0.09849701087239762 and parameters: {'observation_period_num': 208, 'train_rates': 0.8192756925892921, 'learning_rate': 8.081074210552216e-05, 'batch_size': 105, 'step_size': 12, 'gamma': 0.8392406914038019}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:13:33,794][0m Trial 17 finished with value: 0.04941871558793453 and parameters: {'observation_period_num': 50, 'train_rates': 0.9180752316889983, 'learning_rate': 0.00038911896786590703, 'batch_size': 64, 'step_size': 7, 'gamma': 0.7881072197222896}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:15:24,689][0m Trial 18 finished with value: 0.24733655833392992 and parameters: {'observation_period_num': 98, 'train_rates': 0.7565565176993706, 'learning_rate': 9.15760884728368e-05, 'batch_size': 44, 'step_size': 3, 'gamma': 0.7507059120504814}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:16:13,287][0m Trial 19 finished with value: 0.3652865998964343 and parameters: {'observation_period_num': 32, 'train_rates': 0.6061961968523681, 'learning_rate': 8.034699998138994e-06, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8080953785134508}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:20:52,369][0m Trial 20 finished with value: 0.03145987100221894 and parameters: {'observation_period_num': 5, 'train_rates': 0.9068324764680518, 'learning_rate': 0.00021775479873999097, 'batch_size': 20, 'step_size': 13, 'gamma': 0.848542123695033}. Best is trial 2 with value: 0.028322571013687237.[0m
[32m[I 2025-01-05 09:25:29,045][0m Trial 21 finished with value: 0.027096509108407385 and parameters: {'observation_period_num': 7, 'train_rates': 0.899009460334173, 'learning_rate': 0.00020969797343136357, 'batch_size': 20, 'step_size': 14, 'gamma': 0.8573206594849875}. Best is trial 21 with value: 0.027096509108407385.[0m
[32m[I 2025-01-05 09:29:29,226][0m Trial 22 finished with value: 0.11749250553548336 and parameters: {'observation_period_num': 72, 'train_rates': 0.9204237364062631, 'learning_rate': 0.0004613129692609426, 'batch_size': 23, 'step_size': 15, 'gamma': 0.8561567630215859}. Best is trial 21 with value: 0.027096509108407385.[0m
[32m[I 2025-01-05 09:31:10,687][0m Trial 23 finished with value: 0.044291143666416344 and parameters: {'observation_period_num': 36, 'train_rates': 0.8313458786553747, 'learning_rate': 0.00011999544678860201, 'batch_size': 52, 'step_size': 13, 'gamma': 0.8461226218784381}. Best is trial 21 with value: 0.027096509108407385.[0m
[32m[I 2025-01-05 09:36:39,856][0m Trial 24 finished with value: 0.025086835328791593 and parameters: {'observation_period_num': 9, 'train_rates': 0.9122152544803803, 'learning_rate': 5.8654772177333256e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.873438788892266}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:38:31,199][0m Trial 25 finished with value: 0.12989084422588348 and parameters: {'observation_period_num': 248, 'train_rates': 0.9887526945964824, 'learning_rate': 5.91508621325225e-05, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8740006613342557}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:43:41,992][0m Trial 26 finished with value: 0.0507937421430153 and parameters: {'observation_period_num': 30, 'train_rates': 0.8528162818609152, 'learning_rate': 4.8306760378000244e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9280431883302765}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:44:28,943][0m Trial 27 finished with value: 0.08190338698526224 and parameters: {'observation_period_num': 47, 'train_rates': 0.9177461875800729, 'learning_rate': 1.421610040705722e-05, 'batch_size': 124, 'step_size': 14, 'gamma': 0.8716016352105987}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:45:00,763][0m Trial 28 finished with value: 0.11559734960782875 and parameters: {'observation_period_num': 113, 'train_rates': 0.8191016145450215, 'learning_rate': 4.688866782996013e-05, 'batch_size': 169, 'step_size': 9, 'gamma': 0.7681841700063398}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:47:33,636][0m Trial 29 finished with value: 0.07516171314336102 and parameters: {'observation_period_num': 71, 'train_rates': 0.9397193265951819, 'learning_rate': 0.0006281187915165012, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8255574133094159}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:49:17,108][0m Trial 30 finished with value: 0.031462027152967934 and parameters: {'observation_period_num': 17, 'train_rates': 0.8824028090557785, 'learning_rate': 7.603384577544563e-05, 'batch_size': 53, 'step_size': 14, 'gamma': 0.8776909594053347}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:52:35,635][0m Trial 31 finished with value: 0.03519331554814083 and parameters: {'observation_period_num': 10, 'train_rates': 0.902769220002021, 'learning_rate': 0.00025208126759685215, 'batch_size': 28, 'step_size': 13, 'gamma': 0.8514422351225567}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 09:56:40,085][0m Trial 32 finished with value: 0.0585106681960011 and parameters: {'observation_period_num': 7, 'train_rates': 0.8605145009346908, 'learning_rate': 0.00017287036571514116, 'batch_size': 22, 'step_size': 13, 'gamma': 0.9253712871453746}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:02:32,212][0m Trial 33 finished with value: 0.04624450355768204 and parameters: {'observation_period_num': 22, 'train_rates': 0.9303995005866907, 'learning_rate': 0.0001155879575393063, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8365430898181317}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:04:16,571][0m Trial 34 finished with value: 0.07991909728211871 and parameters: {'observation_period_num': 48, 'train_rates': 0.9596142121431606, 'learning_rate': 0.00021804334506579266, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8116919918470695}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:06:47,625][0m Trial 35 finished with value: 0.04271261941508523 and parameters: {'observation_period_num': 25, 'train_rates': 0.9046612204968558, 'learning_rate': 3.0441111504330402e-05, 'batch_size': 37, 'step_size': 15, 'gamma': 0.8858136152595119}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:07:47,076][0m Trial 36 finished with value: 0.07368810942196953 and parameters: {'observation_period_num': 181, 'train_rates': 0.838713132377599, 'learning_rate': 0.00010591413356434, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8633426230852249}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:08:20,821][0m Trial 37 finished with value: 0.03817815505998566 and parameters: {'observation_period_num': 24, 'train_rates': 0.8718243041744452, 'learning_rate': 0.0006038782696993235, 'batch_size': 182, 'step_size': 10, 'gamma': 0.9201181113317833}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:09:03,850][0m Trial 38 finished with value: 0.0734873041510582 and parameters: {'observation_period_num': 41, 'train_rates': 0.963176240607383, 'learning_rate': 0.00015375790160834175, 'batch_size': 144, 'step_size': 14, 'gamma': 0.9533183651058896}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:11:55,733][0m Trial 39 finished with value: 0.05151968429485957 and parameters: {'observation_period_num': 55, 'train_rates': 0.8968743789044401, 'learning_rate': 6.361590876961988e-05, 'batch_size': 32, 'step_size': 10, 'gamma': 0.8829331620771473}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:13:15,439][0m Trial 40 finished with value: 0.08996764476271882 and parameters: {'observation_period_num': 163, 'train_rates': 0.9378767806929488, 'learning_rate': 4.131685086691746e-05, 'batch_size': 70, 'step_size': 8, 'gamma': 0.8628026830784767}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:15:08,292][0m Trial 41 finished with value: 0.034186443445954735 and parameters: {'observation_period_num': 16, 'train_rates': 0.870407996830281, 'learning_rate': 6.868941849366974e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.9011943502556226}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:18:14,282][0m Trial 42 finished with value: 0.03598401107286152 and parameters: {'observation_period_num': 18, 'train_rates': 0.9032100610923943, 'learning_rate': 8.932436803903104e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8748546982974044}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:19:43,192][0m Trial 43 finished with value: 0.04723515843761888 and parameters: {'observation_period_num': 5, 'train_rates': 0.8112692763875106, 'learning_rate': 2.1735211409112114e-05, 'batch_size': 59, 'step_size': 13, 'gamma': 0.911226231261262}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:22:01,101][0m Trial 44 finished with value: 0.047859643319887776 and parameters: {'observation_period_num': 19, 'train_rates': 0.9717219331434988, 'learning_rate': 0.0002630288895570812, 'batch_size': 43, 'step_size': 12, 'gamma': 0.8422125570753644}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:23:09,049][0m Trial 45 finished with value: 0.33304398771255245 and parameters: {'observation_period_num': 135, 'train_rates': 0.8902345404460056, 'learning_rate': 2.842844602736302e-06, 'batch_size': 80, 'step_size': 14, 'gamma': 0.888741298909869}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:25:48,318][0m Trial 46 finished with value: 0.20811395406449187 and parameters: {'observation_period_num': 65, 'train_rates': 0.7741004399504754, 'learning_rate': 0.00013626810325424667, 'batch_size': 31, 'step_size': 11, 'gamma': 0.8528690536549204}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:26:38,771][0m Trial 47 finished with value: 0.04056656267493963 and parameters: {'observation_period_num': 35, 'train_rates': 0.8496471253438771, 'learning_rate': 0.0003275799440100273, 'batch_size': 110, 'step_size': 15, 'gamma': 0.8148903174431552}. Best is trial 24 with value: 0.025086835328791593.[0m
Early stopping at epoch 69
[32m[I 2025-01-05 10:28:04,095][0m Trial 48 finished with value: 0.04791252252956232 and parameters: {'observation_period_num': 13, 'train_rates': 0.8716966209067708, 'learning_rate': 0.0006178469008976919, 'batch_size': 45, 'step_size': 1, 'gamma': 0.8014469321529885}. Best is trial 24 with value: 0.025086835328791593.[0m
[32m[I 2025-01-05 10:33:58,722][0m Trial 49 finished with value: 0.04556773620316331 and parameters: {'observation_period_num': 30, 'train_rates': 0.9450159321176416, 'learning_rate': 0.00018334310246276558, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8323328876545916}. Best is trial 24 with value: 0.025086835328791593.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-05 10:33:58,732][0m A new study created in memory with name: no-name-3bc4bad4-aefd-43e6-bf6e-c9d9b0bc52f0[0m
[32m[I 2025-01-05 10:36:57,145][0m Trial 0 finished with value: 0.26772323492778477 and parameters: {'observation_period_num': 250, 'train_rates': 0.7470599146312427, 'learning_rate': 3.9920651601599375e-05, 'batch_size': 25, 'step_size': 4, 'gamma': 0.9025827626216381}. Best is trial 0 with value: 0.26772323492778477.[0m
[32m[I 2025-01-05 10:37:19,158][0m Trial 1 finished with value: 0.9137467394158824 and parameters: {'observation_period_num': 225, 'train_rates': 0.680022700742466, 'learning_rate': 1.078871468414149e-06, 'batch_size': 221, 'step_size': 15, 'gamma': 0.8807528538547581}. Best is trial 0 with value: 0.26772323492778477.[0m
[32m[I 2025-01-05 10:38:15,683][0m Trial 2 finished with value: 0.21786587029421256 and parameters: {'observation_period_num': 123, 'train_rates': 0.6547939824707868, 'learning_rate': 0.00025236614132591377, 'batch_size': 78, 'step_size': 4, 'gamma': 0.7949202015608436}. Best is trial 2 with value: 0.21786587029421256.[0m
[32m[I 2025-01-05 10:39:18,756][0m Trial 3 finished with value: 0.41271305164775335 and parameters: {'observation_period_num': 19, 'train_rates': 0.7482777134678041, 'learning_rate': 2.4213337953581673e-06, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8524285090055771}. Best is trial 2 with value: 0.21786587029421256.[0m
[32m[I 2025-01-05 10:39:41,766][0m Trial 4 finished with value: 0.12579318080236787 and parameters: {'observation_period_num': 230, 'train_rates': 0.8529754192449229, 'learning_rate': 0.0002576663697868903, 'batch_size': 239, 'step_size': 5, 'gamma': 0.9396926564878152}. Best is trial 4 with value: 0.12579318080236787.[0m
[32m[I 2025-01-05 10:40:02,371][0m Trial 5 finished with value: 0.3427263769286144 and parameters: {'observation_period_num': 161, 'train_rates': 0.609542693783382, 'learning_rate': 0.00010113805291565797, 'batch_size': 234, 'step_size': 5, 'gamma': 0.8888723075927455}. Best is trial 4 with value: 0.12579318080236787.[0m
[32m[I 2025-01-05 10:40:44,210][0m Trial 6 finished with value: 0.06081983291313623 and parameters: {'observation_period_num': 100, 'train_rates': 0.8995071382862881, 'learning_rate': 0.00010969235911177064, 'batch_size': 138, 'step_size': 12, 'gamma': 0.794651749739245}. Best is trial 6 with value: 0.06081983291313623.[0m
[32m[I 2025-01-05 10:41:06,933][0m Trial 7 finished with value: 0.8130519447736475 and parameters: {'observation_period_num': 152, 'train_rates': 0.6016405689853513, 'learning_rate': 3.944621676748983e-06, 'batch_size': 208, 'step_size': 9, 'gamma': 0.813555840568412}. Best is trial 6 with value: 0.06081983291313623.[0m
[32m[I 2025-01-05 10:44:06,666][0m Trial 8 finished with value: 0.2699135327519218 and parameters: {'observation_period_num': 17, 'train_rates': 0.7861393857211016, 'learning_rate': 3.7287440414767342e-06, 'batch_size': 28, 'step_size': 2, 'gamma': 0.9215762826755132}. Best is trial 6 with value: 0.06081983291313623.[0m
[32m[I 2025-01-05 10:46:38,354][0m Trial 9 finished with value: 0.06486433297395706 and parameters: {'observation_period_num': 32, 'train_rates': 0.9846380181178365, 'learning_rate': 1.2260881140189438e-05, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9292197756040936}. Best is trial 6 with value: 0.06081983291313623.[0m
[32m[I 2025-01-05 10:47:15,184][0m Trial 10 finished with value: 0.06412155729776714 and parameters: {'observation_period_num': 96, 'train_rates': 0.9105031986880131, 'learning_rate': 0.000944586797400383, 'batch_size': 159, 'step_size': 15, 'gamma': 0.752280497391533}. Best is trial 6 with value: 0.06081983291313623.[0m
[32m[I 2025-01-05 10:47:52,362][0m Trial 11 finished with value: 0.06068698854598784 and parameters: {'observation_period_num': 86, 'train_rates': 0.9076211671830613, 'learning_rate': 0.0007740684885453161, 'batch_size': 160, 'step_size': 15, 'gamma': 0.7561401523766137}. Best is trial 11 with value: 0.06068698854598784.[0m
[32m[I 2025-01-05 10:48:30,445][0m Trial 12 finished with value: 0.053349405833903484 and parameters: {'observation_period_num': 81, 'train_rates': 0.9113995167638987, 'learning_rate': 0.0008414500412948416, 'batch_size': 157, 'step_size': 12, 'gamma': 0.7510802460784357}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:49:05,869][0m Trial 13 finished with value: 0.056600991636514664 and parameters: {'observation_period_num': 76, 'train_rates': 0.949441837993605, 'learning_rate': 0.0007575339333046948, 'batch_size': 175, 'step_size': 12, 'gamma': 0.7573391570948892}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:49:40,799][0m Trial 14 finished with value: 0.06918852776288986 and parameters: {'observation_period_num': 61, 'train_rates': 0.9892395214644737, 'learning_rate': 0.00037230930106232994, 'batch_size': 188, 'step_size': 11, 'gamma': 0.9721976463400702}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:50:34,076][0m Trial 15 finished with value: 0.05455788445815576 and parameters: {'observation_period_num': 56, 'train_rates': 0.8440845073834662, 'learning_rate': 7.748955234649705e-05, 'batch_size': 106, 'step_size': 12, 'gamma': 0.8294984696221825}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:51:28,718][0m Trial 16 finished with value: 0.07457051976331022 and parameters: {'observation_period_num': 38, 'train_rates': 0.844805439205858, 'learning_rate': 2.34296589185098e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8395567615284265}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:52:19,687][0m Trial 17 finished with value: 0.05370929822325707 and parameters: {'observation_period_num': 52, 'train_rates': 0.8453008499262207, 'learning_rate': 8.624404081617327e-05, 'batch_size': 111, 'step_size': 13, 'gamma': 0.826151572153018}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:53:02,475][0m Trial 18 finished with value: 0.1819857189845315 and parameters: {'observation_period_num': 130, 'train_rates': 0.8137917788601162, 'learning_rate': 1.5678297874265264e-05, 'batch_size': 126, 'step_size': 13, 'gamma': 0.786023688914757}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:54:27,210][0m Trial 19 finished with value: 0.07388577472580515 and parameters: {'observation_period_num': 167, 'train_rates': 0.8845169861400893, 'learning_rate': 5.322288648189483e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8583084297762897}. Best is trial 12 with value: 0.053349405833903484.[0m
[32m[I 2025-01-05 10:55:14,712][0m Trial 20 finished with value: 0.03693450795253739 and parameters: {'observation_period_num': 5, 'train_rates': 0.9350633359449861, 'learning_rate': 0.00017789166402246078, 'batch_size': 129, 'step_size': 13, 'gamma': 0.7747377432881558}. Best is trial 20 with value: 0.03693450795253739.[0m
[32m[I 2025-01-05 10:55:59,642][0m Trial 21 finished with value: 0.039151261286067135 and parameters: {'observation_period_num': 6, 'train_rates': 0.9413601927644251, 'learning_rate': 0.00018036421564749625, 'batch_size': 135, 'step_size': 13, 'gamma': 0.7775547760914834}. Best is trial 20 with value: 0.03693450795253739.[0m
[32m[I 2025-01-05 10:56:42,665][0m Trial 22 finished with value: 0.029623470811938946 and parameters: {'observation_period_num': 5, 'train_rates': 0.9448786897877071, 'learning_rate': 0.00038958295642877014, 'batch_size': 140, 'step_size': 14, 'gamma': 0.7767017650892818}. Best is trial 22 with value: 0.029623470811938946.[0m
[32m[I 2025-01-05 10:57:27,972][0m Trial 23 finished with value: 0.028899734632836446 and parameters: {'observation_period_num': 7, 'train_rates': 0.9481724791864713, 'learning_rate': 0.00033993473215464856, 'batch_size': 136, 'step_size': 14, 'gamma': 0.7866075009750201}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 10:58:00,662][0m Trial 24 finished with value: 0.041606463491916656 and parameters: {'observation_period_num': 29, 'train_rates': 0.9486690214918531, 'learning_rate': 0.0004560695109293647, 'batch_size': 189, 'step_size': 14, 'gamma': 0.8104074420710455}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 10:58:49,458][0m Trial 25 finished with value: 0.033949367702007294 and parameters: {'observation_period_num': 6, 'train_rates': 0.9615075824360418, 'learning_rate': 0.00045592594800301613, 'batch_size': 127, 'step_size': 14, 'gamma': 0.7825082820620585}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 10:59:32,345][0m Trial 26 finished with value: 0.03697088360786438 and parameters: {'observation_period_num': 42, 'train_rates': 0.9748256466089416, 'learning_rate': 0.00045370188335938026, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8115966336687653}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:00:38,440][0m Trial 27 finished with value: 0.05924029255577599 and parameters: {'observation_period_num': 64, 'train_rates': 0.8852101470884752, 'learning_rate': 0.00016166204405201282, 'batch_size': 85, 'step_size': 14, 'gamma': 0.7704805839431965}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:01:29,705][0m Trial 28 finished with value: 0.03659336268901825 and parameters: {'observation_period_num': 24, 'train_rates': 0.962412453017933, 'learning_rate': 0.0003947558553499634, 'batch_size': 119, 'step_size': 14, 'gamma': 0.8004337248821117}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:02:04,921][0m Trial 29 finished with value: 0.05380068633003511 and parameters: {'observation_period_num': 5, 'train_rates': 0.9300121619925692, 'learning_rate': 4.5053248433704575e-05, 'batch_size': 176, 'step_size': 11, 'gamma': 0.8430142811527988}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:02:59,419][0m Trial 30 finished with value: 0.07423395087488559 and parameters: {'observation_period_num': 198, 'train_rates': 0.8706050173081488, 'learning_rate': 0.0002775827304021027, 'batch_size': 96, 'step_size': 7, 'gamma': 0.7691592686527035}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:03:51,875][0m Trial 31 finished with value: 0.04129016771912575 and parameters: {'observation_period_num': 24, 'train_rates': 0.9644359903201178, 'learning_rate': 0.0005602061193692187, 'batch_size': 120, 'step_size': 14, 'gamma': 0.7917383362376136}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:04:35,044][0m Trial 32 finished with value: 0.05096583068370819 and parameters: {'observation_period_num': 39, 'train_rates': 0.9627240165366231, 'learning_rate': 0.0003277673875322249, 'batch_size': 146, 'step_size': 15, 'gamma': 0.80321304508179}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:05:22,512][0m Trial 33 finished with value: 0.03634449008053967 and parameters: {'observation_period_num': 21, 'train_rates': 0.9237536846177173, 'learning_rate': 0.0005627522403072252, 'batch_size': 125, 'step_size': 14, 'gamma': 0.823522551332212}. Best is trial 23 with value: 0.028899734632836446.[0m
[32m[I 2025-01-05 11:06:24,804][0m Trial 34 finished with value: 0.026697625463778994 and parameters: {'observation_period_num': 16, 'train_rates': 0.9233146624328904, 'learning_rate': 0.0006008585134447917, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8244459280559769}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:07:52,127][0m Trial 35 finished with value: 0.17133842636700544 and parameters: {'observation_period_num': 49, 'train_rates': 0.705722890711837, 'learning_rate': 0.00019607824629266186, 'batch_size': 55, 'step_size': 15, 'gamma': 0.7861012402980261}. Best is trial 34 with value: 0.026697625463778994.[0m
Early stopping at epoch 87
[32m[I 2025-01-05 11:08:54,128][0m Trial 36 finished with value: 0.22529800587857945 and parameters: {'observation_period_num': 252, 'train_rates': 0.8087863265566785, 'learning_rate': 0.00012547591493720952, 'batch_size': 69, 'step_size': 1, 'gamma': 0.8551873114659957}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:09:48,538][0m Trial 37 finished with value: 0.1624273110659618 and parameters: {'observation_period_num': 14, 'train_rates': 0.753965231768972, 'learning_rate': 0.0002382164222272533, 'batch_size': 95, 'step_size': 11, 'gamma': 0.8809789172075061}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:10:51,902][0m Trial 38 finished with value: 0.07032388922843066 and parameters: {'observation_period_num': 67, 'train_rates': 0.8744828624286249, 'learning_rate': 0.0005545963554253088, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7675137386317346}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:11:20,962][0m Trial 39 finished with value: 0.08482732869857966 and parameters: {'observation_period_num': 113, 'train_rates': 0.9243591058675007, 'learning_rate': 6.570494073354181e-05, 'batch_size': 214, 'step_size': 13, 'gamma': 0.7846970186410553}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:12:18,231][0m Trial 40 finished with value: 0.5259228348731995 and parameters: {'observation_period_num': 16, 'train_rates': 0.9867414478007017, 'learning_rate': 1.1587299875020066e-06, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8160891261846072}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:12:43,021][0m Trial 41 finished with value: 0.035734303295612335 and parameters: {'observation_period_num': 25, 'train_rates': 0.9224226884318427, 'learning_rate': 0.0005852934783733766, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8288833886635605}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:13:07,936][0m Trial 42 finished with value: 0.03761316078363055 and parameters: {'observation_period_num': 32, 'train_rates': 0.8980397812514206, 'learning_rate': 0.00097026458598845, 'batch_size': 245, 'step_size': 14, 'gamma': 0.8699925071224355}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:13:37,041][0m Trial 43 finished with value: 0.036041438579559326 and parameters: {'observation_period_num': 12, 'train_rates': 0.9471110879909201, 'learning_rate': 0.0002805778360764159, 'batch_size': 225, 'step_size': 15, 'gamma': 0.8363173408477838}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:15:14,976][0m Trial 44 finished with value: 0.16959236659192684 and parameters: {'observation_period_num': 48, 'train_rates': 0.6299984715842267, 'learning_rate': 0.0006510424050485922, 'batch_size': 44, 'step_size': 13, 'gamma': 0.9069781150750068}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:15:47,980][0m Trial 45 finished with value: 0.059392303228378296 and parameters: {'observation_period_num': 31, 'train_rates': 0.9661603119820158, 'learning_rate': 0.00013475465227203522, 'batch_size': 200, 'step_size': 14, 'gamma': 0.8049420476500976}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:16:23,033][0m Trial 46 finished with value: 0.03044801006714503 and parameters: {'observation_period_num': 18, 'train_rates': 0.9185906692488196, 'learning_rate': 0.0004211158022006876, 'batch_size': 172, 'step_size': 12, 'gamma': 0.7927593068726619}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:16:54,560][0m Trial 47 finished with value: 0.09111454943070144 and parameters: {'observation_period_num': 226, 'train_rates': 0.8633393007348484, 'learning_rate': 0.00023749175669633886, 'batch_size': 170, 'step_size': 12, 'gamma': 0.7626964989010349}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:17:36,320][0m Trial 48 finished with value: 0.16110054585447278 and parameters: {'observation_period_num': 16, 'train_rates': 0.8984733590169691, 'learning_rate': 8.096850039399487e-06, 'batch_size': 142, 'step_size': 10, 'gamma': 0.7947207423930267}. Best is trial 34 with value: 0.026697625463778994.[0m
[32m[I 2025-01-05 11:18:12,924][0m Trial 49 finished with value: 0.04942643323747238 and parameters: {'observation_period_num': 72, 'train_rates': 0.8250976268074793, 'learning_rate': 0.000361565871018422, 'batch_size': 153, 'step_size': 12, 'gamma': 0.7795927878071359}. Best is trial 34 with value: 0.026697625463778994.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 11:18:13,153][0m A new study created in memory with name: no-name-a1708b7a-6e0b-443a-b361-6aaf324a7d54[0m
[32m[I 2025-01-05 11:19:36,503][0m Trial 0 finished with value: 0.11497092310644129 and parameters: {'observation_period_num': 220, 'train_rates': 0.7948037943468838, 'learning_rate': 0.00015716445043125612, 'batch_size': 58, 'step_size': 10, 'gamma': 0.7677200564435512}. Best is trial 0 with value: 0.11497092310644129.[0m
[32m[I 2025-01-05 11:21:36,656][0m Trial 1 finished with value: 0.3124638545665551 and parameters: {'observation_period_num': 199, 'train_rates': 0.7446480318020624, 'learning_rate': 1.825982091212262e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8706199450488643}. Best is trial 0 with value: 0.11497092310644129.[0m
[32m[I 2025-01-05 11:25:02,542][0m Trial 2 finished with value: 0.046187618149049356 and parameters: {'observation_period_num': 25, 'train_rates': 0.943899116517462, 'learning_rate': 1.844436099942348e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.9726969808712868}. Best is trial 2 with value: 0.046187618149049356.[0m
[32m[I 2025-01-05 11:25:24,464][0m Trial 3 finished with value: 0.08114681582754621 and parameters: {'observation_period_num': 46, 'train_rates': 0.7857704177851815, 'learning_rate': 0.0004287445868403894, 'batch_size': 255, 'step_size': 15, 'gamma': 0.9497922999692211}. Best is trial 2 with value: 0.046187618149049356.[0m
[32m[I 2025-01-05 11:25:50,059][0m Trial 4 finished with value: 0.4385204750646665 and parameters: {'observation_period_num': 169, 'train_rates': 0.7785140035648701, 'learning_rate': 5.617303178882116e-06, 'batch_size': 207, 'step_size': 13, 'gamma': 0.7536325094723233}. Best is trial 2 with value: 0.046187618149049356.[0m
[32m[I 2025-01-05 11:26:15,953][0m Trial 5 finished with value: 0.2814142452491074 and parameters: {'observation_period_num': 232, 'train_rates': 0.7236839005908224, 'learning_rate': 0.0002446550387598109, 'batch_size': 191, 'step_size': 11, 'gamma': 0.9012921652183543}. Best is trial 2 with value: 0.046187618149049356.[0m
[32m[I 2025-01-05 11:27:17,912][0m Trial 6 finished with value: 0.3022986358773695 and parameters: {'observation_period_num': 10, 'train_rates': 0.7044128258577251, 'learning_rate': 3.0958870537796377e-06, 'batch_size': 79, 'step_size': 14, 'gamma': 0.7593708730019267}. Best is trial 2 with value: 0.046187618149049356.[0m
[32m[I 2025-01-05 11:27:50,733][0m Trial 7 finished with value: 0.03507989288368119 and parameters: {'observation_period_num': 26, 'train_rates': 0.8002483311689123, 'learning_rate': 0.000587477741114562, 'batch_size': 171, 'step_size': 14, 'gamma': 0.7979538595554179}. Best is trial 7 with value: 0.03507989288368119.[0m
Early stopping at epoch 47
[32m[I 2025-01-05 11:28:03,993][0m Trial 8 finished with value: 0.7671712591902287 and parameters: {'observation_period_num': 41, 'train_rates': 0.7459566673350913, 'learning_rate': 2.526297318930791e-05, 'batch_size': 203, 'step_size': 1, 'gamma': 0.7682300574791382}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:28:31,646][0m Trial 9 finished with value: 0.1592896959428145 and parameters: {'observation_period_num': 51, 'train_rates': 0.6319199539284391, 'learning_rate': 0.00012164097290207353, 'batch_size': 170, 'step_size': 8, 'gamma': 0.8494100824960005}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:29:21,296][0m Trial 10 finished with value: 0.07095373794436455 and parameters: {'observation_period_num': 101, 'train_rates': 0.9133143271950273, 'learning_rate': 0.000812628020826166, 'batch_size': 116, 'step_size': 3, 'gamma': 0.819451112262724}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:30:12,306][0m Trial 11 finished with value: 0.08841317892074585 and parameters: {'observation_period_num': 106, 'train_rates': 0.9858226058456528, 'learning_rate': 5.4895509657127464e-05, 'batch_size': 121, 'step_size': 6, 'gamma': 0.9869664324180375}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:34:56,828][0m Trial 12 finished with value: 0.047589250067121364 and parameters: {'observation_period_num': 13, 'train_rates': 0.8759993370310122, 'learning_rate': 6.590995134004134e-06, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9245323032245512}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:35:36,799][0m Trial 13 finished with value: 0.6290840185605563 and parameters: {'observation_period_num': 65, 'train_rates': 0.8652098054816143, 'learning_rate': 1.2834166996150043e-06, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8144423029631964}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:36:39,692][0m Trial 14 finished with value: 0.10185105353593826 and parameters: {'observation_period_num': 143, 'train_rates': 0.9874363280364011, 'learning_rate': 7.377487020307594e-05, 'batch_size': 94, 'step_size': 11, 'gamma': 0.8130077910420596}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:37:18,784][0m Trial 15 finished with value: 0.09717165309507235 and parameters: {'observation_period_num': 84, 'train_rates': 0.9215210561962943, 'learning_rate': 1.1241890590795705e-05, 'batch_size': 155, 'step_size': 12, 'gamma': 0.9894698514888927}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:37:41,768][0m Trial 16 finished with value: 0.11694870696913812 and parameters: {'observation_period_num': 132, 'train_rates': 0.8465957556329616, 'learning_rate': 4.616926599728365e-05, 'batch_size': 240, 'step_size': 9, 'gamma': 0.8797738389039322}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:38:31,960][0m Trial 17 finished with value: 0.13769284573573512 and parameters: {'observation_period_num': 23, 'train_rates': 0.6489534154870604, 'learning_rate': 0.0008203740551644213, 'batch_size': 92, 'step_size': 4, 'gamma': 0.9523281046875545}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:39:05,039][0m Trial 18 finished with value: 0.145186050701032 and parameters: {'observation_period_num': 78, 'train_rates': 0.9243621164694938, 'learning_rate': 1.7709068682254487e-05, 'batch_size': 179, 'step_size': 7, 'gamma': 0.8423289674903934}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:39:30,824][0m Trial 19 finished with value: 0.07695759074729785 and parameters: {'observation_period_num': 35, 'train_rates': 0.8245352577515531, 'learning_rate': 0.0003417541926242779, 'batch_size': 225, 'step_size': 2, 'gamma': 0.7927450102978784}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:40:51,651][0m Trial 20 finished with value: 0.20652262676732774 and parameters: {'observation_period_num': 5, 'train_rates': 0.6755921770877953, 'learning_rate': 1.6628472113380115e-06, 'batch_size': 58, 'step_size': 15, 'gamma': 0.9169246501520656}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:43:33,437][0m Trial 21 finished with value: 0.050717255898884366 and parameters: {'observation_period_num': 24, 'train_rates': 0.8878726459885569, 'learning_rate': 8.396649949301324e-06, 'batch_size': 34, 'step_size': 8, 'gamma': 0.9473115068397683}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:47:42,281][0m Trial 22 finished with value: 0.09035145959609912 and parameters: {'observation_period_num': 58, 'train_rates': 0.9504149943035609, 'learning_rate': 3.861986171917254e-06, 'batch_size': 23, 'step_size': 9, 'gamma': 0.9151190041719327}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:52:32,329][0m Trial 23 finished with value: 0.0365473453849554 and parameters: {'observation_period_num': 11, 'train_rates': 0.8304815970230571, 'learning_rate': 1.0201462537096309e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.959574325326118}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:54:05,628][0m Trial 24 finished with value: 0.06492734934688824 and parameters: {'observation_period_num': 82, 'train_rates': 0.8285589579736338, 'learning_rate': 3.265853612136447e-05, 'batch_size': 56, 'step_size': 6, 'gamma': 0.9682526258837683}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:54:54,019][0m Trial 25 finished with value: 0.10798607244391308 and parameters: {'observation_period_num': 33, 'train_rates': 0.8171657732667228, 'learning_rate': 1.2034649229368374e-05, 'batch_size': 117, 'step_size': 5, 'gamma': 0.8885300079147075}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:55:59,067][0m Trial 26 finished with value: 0.22070053555979963 and parameters: {'observation_period_num': 106, 'train_rates': 0.7697601677760814, 'learning_rate': 0.00012649169289300803, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9727057457303188}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:58:12,150][0m Trial 27 finished with value: 0.25101338428993747 and parameters: {'observation_period_num': 157, 'train_rates': 0.9479494020741668, 'learning_rate': 2.4660722864467206e-06, 'batch_size': 42, 'step_size': 7, 'gamma': 0.9285346461272261}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 11:58:51,262][0m Trial 28 finished with value: 0.05932861194014549 and parameters: {'observation_period_num': 67, 'train_rates': 0.8990023581518548, 'learning_rate': 7.755958296278486e-05, 'batch_size': 146, 'step_size': 13, 'gamma': 0.8442762595115568}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:00:27,825][0m Trial 29 finished with value: 0.05410360214928855 and parameters: {'observation_period_num': 25, 'train_rates': 0.8028072580824194, 'learning_rate': 3.0514840248322854e-05, 'batch_size': 54, 'step_size': 10, 'gamma': 0.7927054308654267}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:01:01,551][0m Trial 30 finished with value: 0.1942907272109082 and parameters: {'observation_period_num': 251, 'train_rates': 0.8675221328572966, 'learning_rate': 1.3429349550554095e-05, 'batch_size': 165, 'step_size': 7, 'gamma': 0.9390070268828653}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:05:57,077][0m Trial 31 finished with value: 0.04488336169560658 and parameters: {'observation_period_num': 8, 'train_rates': 0.8538241642776442, 'learning_rate': 5.056853212680962e-06, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9684877010530615}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:10:38,246][0m Trial 32 finished with value: 0.046898779971216685 and parameters: {'observation_period_num': 10, 'train_rates': 0.8463123232989054, 'learning_rate': 5.183833625526037e-06, 'batch_size': 19, 'step_size': 9, 'gamma': 0.9639999766281668}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:12:55,094][0m Trial 33 finished with value: 0.1985453894554541 and parameters: {'observation_period_num': 48, 'train_rates': 0.7607894647683536, 'learning_rate': 1.7754642671730567e-05, 'batch_size': 36, 'step_size': 5, 'gamma': 0.9746811443574664}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:14:14,486][0m Trial 34 finished with value: 0.14223459566404162 and parameters: {'observation_period_num': 22, 'train_rates': 0.7928846395937271, 'learning_rate': 2.170421765138888e-06, 'batch_size': 66, 'step_size': 11, 'gamma': 0.9626361092176037}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:16:26,534][0m Trial 35 finished with value: 0.1001063298408677 and parameters: {'observation_period_num': 39, 'train_rates': 0.8488290808972087, 'learning_rate': 3.93232845215015e-06, 'batch_size': 40, 'step_size': 13, 'gamma': 0.8627635867388801}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:19:12,343][0m Trial 36 finished with value: 0.33529376368536506 and parameters: {'observation_period_num': 181, 'train_rates': 0.7212533242360227, 'learning_rate': 8.984986871111774e-06, 'batch_size': 27, 'step_size': 4, 'gamma': 0.93848316243383}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:25:09,785][0m Trial 37 finished with value: 0.0408021716069277 and parameters: {'observation_period_num': 9, 'train_rates': 0.9507095526798959, 'learning_rate': 0.0002184043087466515, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9062937017107239}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:26:11,862][0m Trial 38 finished with value: 0.14314838701740223 and parameters: {'observation_period_num': 11, 'train_rates': 0.6010699585913963, 'learning_rate': 0.0005752335801813984, 'batch_size': 70, 'step_size': 15, 'gamma': 0.9008750445133459}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:28:05,486][0m Trial 39 finished with value: 0.08616869017714635 and parameters: {'observation_period_num': 55, 'train_rates': 0.8015275520408938, 'learning_rate': 0.000474793323929062, 'batch_size': 45, 'step_size': 14, 'gamma': 0.9000590833620701}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:28:38,338][0m Trial 40 finished with value: 0.050026047974824905 and parameters: {'observation_period_num': 31, 'train_rates': 0.9623401881396003, 'learning_rate': 0.00022755644555191696, 'batch_size': 199, 'step_size': 12, 'gamma': 0.8645059713562151}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:34:12,232][0m Trial 41 finished with value: 0.035531136364890976 and parameters: {'observation_period_num': 17, 'train_rates': 0.9338028372214529, 'learning_rate': 0.0002587741495869545, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9516457273190161}. Best is trial 7 with value: 0.03507989288368119.[0m
[32m[I 2025-01-05 12:39:20,672][0m Trial 42 finished with value: 0.032760116307122895 and parameters: {'observation_period_num': 16, 'train_rates': 0.9018374957952444, 'learning_rate': 0.00021313735450778504, 'batch_size': 18, 'step_size': 14, 'gamma': 0.9491112627636014}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:42:34,391][0m Trial 43 finished with value: 0.05464649422371641 and parameters: {'observation_period_num': 41, 'train_rates': 0.9679926029191613, 'learning_rate': 0.00022273395257268213, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9337158033791211}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:44:32,618][0m Trial 44 finished with value: 0.03962025473584662 and parameters: {'observation_period_num': 19, 'train_rates': 0.9027229131053509, 'learning_rate': 0.00033477785107451584, 'batch_size': 48, 'step_size': 14, 'gamma': 0.9510156819708581}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:46:22,316][0m Trial 45 finished with value: 0.09410310122701857 and parameters: {'observation_period_num': 197, 'train_rates': 0.9021652151011823, 'learning_rate': 0.00032340272849287646, 'batch_size': 48, 'step_size': 14, 'gamma': 0.9531208963225171}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:46:55,498][0m Trial 46 finished with value: 0.04062572246160675 and parameters: {'observation_period_num': 19, 'train_rates': 0.8839975288926837, 'learning_rate': 0.0006041656942022128, 'batch_size': 181, 'step_size': 12, 'gamma': 0.9816161672050209}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:47:50,835][0m Trial 47 finished with value: 0.05356570884585381 and parameters: {'observation_period_num': 68, 'train_rates': 0.9307332352034757, 'learning_rate': 0.00015832518578297054, 'batch_size': 104, 'step_size': 13, 'gamma': 0.9457806368401466}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:48:35,701][0m Trial 48 finished with value: 0.07748811924830079 and parameters: {'observation_period_num': 95, 'train_rates': 0.8965839428877421, 'learning_rate': 0.0009329737195819632, 'batch_size': 131, 'step_size': 14, 'gamma': 0.7703488500427434}. Best is trial 42 with value: 0.032760116307122895.[0m
[32m[I 2025-01-05 12:49:05,078][0m Trial 49 finished with value: 0.04685335211633217 and parameters: {'observation_period_num': 48, 'train_rates': 0.9135588383970823, 'learning_rate': 0.0003402178459124292, 'batch_size': 219, 'step_size': 13, 'gamma': 0.829274232474828}. Best is trial 42 with value: 0.032760116307122895.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 12:49:05,088][0m A new study created in memory with name: no-name-7947c4c9-cdc7-4244-b414-0efbe44d83fa[0m
[32m[I 2025-01-05 12:49:36,208][0m Trial 0 finished with value: 0.5594187604322133 and parameters: {'observation_period_num': 188, 'train_rates': 0.8740575311225897, 'learning_rate': 6.353271838265752e-06, 'batch_size': 176, 'step_size': 2, 'gamma': 0.9045396268498679}. Best is trial 0 with value: 0.5594187604322133.[0m
[32m[I 2025-01-05 12:50:03,658][0m Trial 1 finished with value: 0.27750036317781107 and parameters: {'observation_period_num': 27, 'train_rates': 0.8968729194371476, 'learning_rate': 2.8528728260922145e-06, 'batch_size': 222, 'step_size': 13, 'gamma': 0.8103735663547008}. Best is trial 1 with value: 0.27750036317781107.[0m
[32m[I 2025-01-05 12:52:50,019][0m Trial 2 finished with value: 0.13839697087853656 and parameters: {'observation_period_num': 129, 'train_rates': 0.785773506866382, 'learning_rate': 1.3516982787874366e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.7691894474230454}. Best is trial 2 with value: 0.13839697087853656.[0m
[32m[I 2025-01-05 12:54:15,759][0m Trial 3 finished with value: 0.14280084015003272 and parameters: {'observation_period_num': 182, 'train_rates': 0.9498551058573609, 'learning_rate': 0.0008930467283847159, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9458981239779923}. Best is trial 2 with value: 0.13839697087853656.[0m
[32m[I 2025-01-05 12:54:39,068][0m Trial 4 finished with value: 0.18824810914538956 and parameters: {'observation_period_num': 11, 'train_rates': 0.7573288492883812, 'learning_rate': 9.111146904730759e-05, 'batch_size': 241, 'step_size': 12, 'gamma': 0.8201356342587562}. Best is trial 2 with value: 0.13839697087853656.[0m
[32m[I 2025-01-05 12:55:05,857][0m Trial 5 finished with value: 0.7549380660057068 and parameters: {'observation_period_num': 137, 'train_rates': 0.9716890978289081, 'learning_rate': 3.2116635183917855e-06, 'batch_size': 248, 'step_size': 7, 'gamma': 0.8094974710841535}. Best is trial 2 with value: 0.13839697087853656.[0m
[32m[I 2025-01-05 12:58:35,040][0m Trial 6 finished with value: 0.0708844460425576 and parameters: {'observation_period_num': 30, 'train_rates': 0.8324298978610745, 'learning_rate': 3.828253200186008e-06, 'batch_size': 25, 'step_size': 10, 'gamma': 0.9264761125763711}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:00:16,432][0m Trial 7 finished with value: 0.38015103396379724 and parameters: {'observation_period_num': 115, 'train_rates': 0.7761485634979369, 'learning_rate': 3.4622692813795264e-06, 'batch_size': 48, 'step_size': 12, 'gamma': 0.9396261614287096}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:01:17,111][0m Trial 8 finished with value: 0.5300492179103014 and parameters: {'observation_period_num': 169, 'train_rates': 0.6470522447284962, 'learning_rate': 2.3347548634144205e-06, 'batch_size': 72, 'step_size': 13, 'gamma': 0.9053127923223423}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:01:42,046][0m Trial 9 finished with value: 0.8381288647651672 and parameters: {'observation_period_num': 125, 'train_rates': 0.9384279348085229, 'learning_rate': 5.0286262882578115e-06, 'batch_size': 242, 'step_size': 2, 'gamma': 0.8113742782636425}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:02:22,256][0m Trial 10 finished with value: 0.2116450118443891 and parameters: {'observation_period_num': 62, 'train_rates': 0.691835780711204, 'learning_rate': 4.9525404431143907e-05, 'batch_size': 121, 'step_size': 6, 'gamma': 0.9879197219721525}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:06:49,412][0m Trial 11 finished with value: 0.09631978468717756 and parameters: {'observation_period_num': 250, 'train_rates': 0.8360651786557873, 'learning_rate': 2.0070934059766595e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7601245199448178}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:11:03,291][0m Trial 12 finished with value: 0.31400755658951773 and parameters: {'observation_period_num': 248, 'train_rates': 0.8397222860729888, 'learning_rate': 1.0064814685790308e-06, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8636521486473634}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:11:56,554][0m Trial 13 finished with value: 0.20036284605117693 and parameters: {'observation_period_num': 249, 'train_rates': 0.838181543714888, 'learning_rate': 1.9124716463816045e-05, 'batch_size': 97, 'step_size': 5, 'gamma': 0.8557510246294484}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:12:29,729][0m Trial 14 finished with value: 0.18842964649789856 and parameters: {'observation_period_num': 59, 'train_rates': 0.7209639443125164, 'learning_rate': 0.00013752706596217976, 'batch_size': 160, 'step_size': 15, 'gamma': 0.7538833802697897}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:13:26,771][0m Trial 15 finished with value: 0.1456368998402641 and parameters: {'observation_period_num': 87, 'train_rates': 0.8315910774617867, 'learning_rate': 1.2528315606144273e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9136764627887759}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:18:43,893][0m Trial 16 finished with value: 0.13534880158810467 and parameters: {'observation_period_num': 228, 'train_rates': 0.8941374128324235, 'learning_rate': 3.504398779552848e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9865621499434134}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:20:21,144][0m Trial 17 finished with value: 0.2738364130653293 and parameters: {'observation_period_num': 215, 'train_rates': 0.6145246713539658, 'learning_rate': 0.0003096063384076512, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8816504865891048}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:21:00,924][0m Trial 18 finished with value: 0.6173369608403463 and parameters: {'observation_period_num': 48, 'train_rates': 0.7291301131379206, 'learning_rate': 1.1008416020054187e-06, 'batch_size': 130, 'step_size': 15, 'gamma': 0.838912585209239}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:22:21,440][0m Trial 19 finished with value: 0.2550699608790636 and parameters: {'observation_period_num': 97, 'train_rates': 0.808159840941285, 'learning_rate': 7.768900768724342e-06, 'batch_size': 64, 'step_size': 7, 'gamma': 0.7841147543464416}. Best is trial 6 with value: 0.0708844460425576.[0m
[32m[I 2025-01-05 13:22:52,078][0m Trial 20 finished with value: 0.06140961238141085 and parameters: {'observation_period_num': 5, 'train_rates': 0.8724780620796082, 'learning_rate': 2.1293683845329997e-05, 'batch_size': 193, 'step_size': 10, 'gamma': 0.9290395248435076}. Best is trial 20 with value: 0.06140961238141085.[0m
[32m[I 2025-01-05 13:23:22,946][0m Trial 21 finished with value: 0.05922619109378033 and parameters: {'observation_period_num': 20, 'train_rates': 0.8678767046129664, 'learning_rate': 2.7234604999378317e-05, 'batch_size': 195, 'step_size': 10, 'gamma': 0.9528916448270419}. Best is trial 21 with value: 0.05922619109378033.[0m
[32m[I 2025-01-05 13:23:53,830][0m Trial 22 finished with value: 0.049014599195548465 and parameters: {'observation_period_num': 5, 'train_rates': 0.8839733632895163, 'learning_rate': 5.754654193684871e-05, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9576016802710243}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:24:24,597][0m Trial 23 finished with value: 0.0490821715426671 and parameters: {'observation_period_num': 7, 'train_rates': 0.9285465080889714, 'learning_rate': 6.997435891350333e-05, 'batch_size': 201, 'step_size': 11, 'gamma': 0.9614883256719037}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:24:55,396][0m Trial 24 finished with value: 0.0523521420741532 and parameters: {'observation_period_num': 30, 'train_rates': 0.9189319648141049, 'learning_rate': 6.692148165554458e-05, 'batch_size': 205, 'step_size': 13, 'gamma': 0.9566901869191627}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:25:26,121][0m Trial 25 finished with value: 0.059983063489198685 and parameters: {'observation_period_num': 37, 'train_rates': 0.9897961630803045, 'learning_rate': 8.248812210565453e-05, 'batch_size': 213, 'step_size': 14, 'gamma': 0.9700755351302557}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:26:02,956][0m Trial 26 finished with value: 0.07925103029066866 and parameters: {'observation_period_num': 82, 'train_rates': 0.9312799519109579, 'learning_rate': 0.00020711859717362064, 'batch_size': 163, 'step_size': 12, 'gamma': 0.9627309190908978}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:26:32,306][0m Trial 27 finished with value: 0.05306705195142562 and parameters: {'observation_period_num': 5, 'train_rates': 0.9157676901707716, 'learning_rate': 5.508458050969718e-05, 'batch_size': 219, 'step_size': 13, 'gamma': 0.8888258289703398}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:27:13,341][0m Trial 28 finished with value: 0.05666772648692131 and parameters: {'observation_period_num': 45, 'train_rates': 0.9693072083306418, 'learning_rate': 0.00022902633250775336, 'batch_size': 150, 'step_size': 11, 'gamma': 0.9686717767026375}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:27:46,924][0m Trial 29 finished with value: 0.08099865573829254 and parameters: {'observation_period_num': 66, 'train_rates': 0.9218773424555434, 'learning_rate': 0.0004218426660406724, 'batch_size': 181, 'step_size': 14, 'gamma': 0.9215961241375192}. Best is trial 22 with value: 0.049014599195548465.[0m
[32m[I 2025-01-05 13:28:18,715][0m Trial 30 finished with value: 0.04459826499461747 and parameters: {'observation_period_num': 27, 'train_rates': 0.8931875766735545, 'learning_rate': 0.00011016102595139922, 'batch_size': 199, 'step_size': 11, 'gamma': 0.898706783977462}. Best is trial 30 with value: 0.04459826499461747.[0m
[32m[I 2025-01-05 13:28:49,655][0m Trial 31 finished with value: 0.04484893227047965 and parameters: {'observation_period_num': 27, 'train_rates': 0.8897747530191424, 'learning_rate': 0.00010220388793390031, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9546361431509857}. Best is trial 30 with value: 0.04459826499461747.[0m
[32m[I 2025-01-05 13:29:22,707][0m Trial 32 finished with value: 0.0424643236046737 and parameters: {'observation_period_num': 21, 'train_rates': 0.8891549268410271, 'learning_rate': 0.00011407272205269045, 'batch_size': 177, 'step_size': 11, 'gamma': 0.89782906335338}. Best is trial 32 with value: 0.0424643236046737.[0m
[32m[I 2025-01-05 13:29:55,785][0m Trial 33 finished with value: 0.04260071248107314 and parameters: {'observation_period_num': 24, 'train_rates': 0.8953650556791856, 'learning_rate': 0.0001504291391662898, 'batch_size': 177, 'step_size': 11, 'gamma': 0.8888058293320094}. Best is trial 32 with value: 0.0424643236046737.[0m
[32m[I 2025-01-05 13:30:29,380][0m Trial 34 finished with value: 0.051388316500879015 and parameters: {'observation_period_num': 48, 'train_rates': 0.8549980527064767, 'learning_rate': 0.00013684705202368782, 'batch_size': 175, 'step_size': 8, 'gamma': 0.8918885857991921}. Best is trial 32 with value: 0.0424643236046737.[0m
[32m[I 2025-01-05 13:30:56,484][0m Trial 35 finished with value: 0.0365779409694135 and parameters: {'observation_period_num': 23, 'train_rates': 0.8942280807634817, 'learning_rate': 0.0004840566205590131, 'batch_size': 225, 'step_size': 12, 'gamma': 0.8481721635666974}. Best is trial 35 with value: 0.0365779409694135.[0m
[32m[I 2025-01-05 13:31:22,129][0m Trial 36 finished with value: 0.07015284073483097 and parameters: {'observation_period_num': 72, 'train_rates': 0.809725116215558, 'learning_rate': 0.0007540650991896209, 'batch_size': 228, 'step_size': 12, 'gamma': 0.8460374425384005}. Best is trial 35 with value: 0.0365779409694135.[0m
[32m[I 2025-01-05 13:31:47,414][0m Trial 37 finished with value: 0.09610855530960896 and parameters: {'observation_period_num': 155, 'train_rates': 0.9012455358619698, 'learning_rate': 0.0005518678301242679, 'batch_size': 232, 'step_size': 14, 'gamma': 0.8740853439520424}. Best is trial 35 with value: 0.0365779409694135.[0m
[32m[I 2025-01-05 13:32:22,687][0m Trial 38 finished with value: 0.044581338763237 and parameters: {'observation_period_num': 20, 'train_rates': 0.9608668796305846, 'learning_rate': 0.00019053388771010692, 'batch_size': 181, 'step_size': 12, 'gamma': 0.8993376724479486}. Best is trial 35 with value: 0.0365779409694135.[0m
[32m[I 2025-01-05 13:33:06,393][0m Trial 39 finished with value: 0.036484464847793184 and parameters: {'observation_period_num': 18, 'train_rates': 0.951229315171274, 'learning_rate': 0.00021999409560335457, 'batch_size': 143, 'step_size': 12, 'gamma': 0.8711264584529601}. Best is trial 39 with value: 0.036484464847793184.[0m
Early stopping at epoch 76
[32m[I 2025-01-05 13:33:40,862][0m Trial 40 finished with value: 0.10804349586276189 and parameters: {'observation_period_num': 43, 'train_rates': 0.9443038239281213, 'learning_rate': 0.0003505867622149038, 'batch_size': 141, 'step_size': 1, 'gamma': 0.840996668028163}. Best is trial 39 with value: 0.036484464847793184.[0m
[32m[I 2025-01-05 13:34:16,127][0m Trial 41 finished with value: 0.041790157556533813 and parameters: {'observation_period_num': 19, 'train_rates': 0.961024646195879, 'learning_rate': 0.00023179011992442944, 'batch_size': 177, 'step_size': 12, 'gamma': 0.8746660413809667}. Best is trial 39 with value: 0.036484464847793184.[0m
[32m[I 2025-01-05 13:34:55,507][0m Trial 42 finished with value: 0.02629515528678894 and parameters: {'observation_period_num': 19, 'train_rates': 0.9772529378914042, 'learning_rate': 0.000584695690122956, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8251483910992404}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:35:49,058][0m Trial 43 finished with value: 0.04939071461558342 and parameters: {'observation_period_num': 56, 'train_rates': 0.9864774811814799, 'learning_rate': 0.0009173828532247283, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8259685642424012}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:36:28,207][0m Trial 44 finished with value: 0.03297211974859238 and parameters: {'observation_period_num': 19, 'train_rates': 0.9577792023676888, 'learning_rate': 0.0005904340136493062, 'batch_size': 162, 'step_size': 10, 'gamma': 0.7863857062243359}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:37:07,267][0m Trial 45 finished with value: 0.04616745933890343 and parameters: {'observation_period_num': 36, 'train_rates': 0.9554582431469512, 'learning_rate': 0.0005318131693767198, 'batch_size': 158, 'step_size': 9, 'gamma': 0.7945252516862663}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:37:52,208][0m Trial 46 finished with value: 0.029697269201278687 and parameters: {'observation_period_num': 15, 'train_rates': 0.9720682645815524, 'learning_rate': 0.0006917203199339209, 'batch_size': 139, 'step_size': 10, 'gamma': 0.8248031092760174}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:38:43,726][0m Trial 47 finished with value: 0.08401554077863693 and parameters: {'observation_period_num': 114, 'train_rates': 0.9778903868094032, 'learning_rate': 0.0005817908369962379, 'batch_size': 118, 'step_size': 10, 'gamma': 0.7986640822842376}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:39:08,330][0m Trial 48 finished with value: 0.03828592598438263 and parameters: {'observation_period_num': 16, 'train_rates': 0.9439490930417977, 'learning_rate': 0.0007324466282223535, 'batch_size': 255, 'step_size': 7, 'gamma': 0.8262247559354909}. Best is trial 42 with value: 0.02629515528678894.[0m
[32m[I 2025-01-05 13:39:53,556][0m Trial 49 finished with value: 0.05629820004105568 and parameters: {'observation_period_num': 37, 'train_rates': 0.9664009458900661, 'learning_rate': 0.0003408942527084422, 'batch_size': 137, 'step_size': 10, 'gamma': 0.7767351174801306}. Best is trial 42 with value: 0.02629515528678894.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8877022044701688, 'learning_rate': 0.0006636125083983942, 'batch_size': 33, 'step_size': 14, 'gamma': 0.8543416215823412}
Epoch 1/300, trend Loss: 0.2238 | 0.0734
Epoch 2/300, trend Loss: 0.1232 | 0.0601
Epoch 3/300, trend Loss: 0.1153 | 0.0629
Epoch 4/300, trend Loss: 0.1112 | 0.0550
Epoch 5/300, trend Loss: 0.1026 | 0.0515
Epoch 6/300, trend Loss: 0.0959 | 0.0420
Epoch 7/300, trend Loss: 0.0906 | 0.0411
Epoch 8/300, trend Loss: 0.0862 | 0.0368
Epoch 9/300, trend Loss: 0.0825 | 0.0371
Epoch 10/300, trend Loss: 0.0798 | 0.0360
Epoch 11/300, trend Loss: 0.0777 | 0.0369
Epoch 12/300, trend Loss: 0.0769 | 0.0383
Epoch 13/300, trend Loss: 0.0767 | 0.0410
Epoch 14/300, trend Loss: 0.0759 | 0.0407
Epoch 15/300, trend Loss: 0.0734 | 0.0390
Epoch 16/300, trend Loss: 0.0717 | 0.0370
Epoch 17/300, trend Loss: 0.0708 | 0.0346
Epoch 18/300, trend Loss: 0.0700 | 0.0324
Epoch 19/300, trend Loss: 0.0693 | 0.0319
Epoch 20/300, trend Loss: 0.0685 | 0.0315
Epoch 21/300, trend Loss: 0.0674 | 0.0332
Epoch 22/300, trend Loss: 0.0675 | 0.0313
Epoch 23/300, trend Loss: 0.0677 | 0.0337
Epoch 24/300, trend Loss: 0.0671 | 0.0281
Epoch 25/300, trend Loss: 0.0649 | 0.0285
Epoch 26/300, trend Loss: 0.0643 | 0.0283
Epoch 27/300, trend Loss: 0.0646 | 0.0335
Epoch 28/300, trend Loss: 0.0653 | 0.0348
Epoch 29/300, trend Loss: 0.0637 | 0.0313
Epoch 30/300, trend Loss: 0.0634 | 0.0300
Epoch 31/300, trend Loss: 0.0633 | 0.0278
Epoch 32/300, trend Loss: 0.0611 | 0.0265
Epoch 33/300, trend Loss: 0.0616 | 0.0320
Epoch 34/300, trend Loss: 0.0660 | 0.0330
Epoch 35/300, trend Loss: 0.0634 | 0.0301
Epoch 36/300, trend Loss: 0.0609 | 0.0230
Epoch 37/300, trend Loss: 0.0582 | 0.0244
Epoch 38/300, trend Loss: 0.0572 | 0.0247
Epoch 39/300, trend Loss: 0.0592 | 0.0564
Epoch 40/300, trend Loss: 0.0658 | 0.0308
Epoch 41/300, trend Loss: 0.0624 | 0.0260
Epoch 42/300, trend Loss: 0.0588 | 0.0232
Epoch 43/300, trend Loss: 0.0593 | 0.0253
Epoch 44/300, trend Loss: 0.0581 | 0.0274
Epoch 45/300, trend Loss: 0.0586 | 0.0257
Epoch 46/300, trend Loss: 0.0588 | 0.0273
Epoch 47/300, trend Loss: 0.0586 | 0.0276
Epoch 48/300, trend Loss: 0.0599 | 0.0277
Epoch 49/300, trend Loss: 0.0575 | 0.0374
Epoch 50/300, trend Loss: 0.0619 | 0.0448
Epoch 51/300, trend Loss: 0.0606 | 0.0313
Epoch 52/300, trend Loss: 0.0540 | 0.0269
Epoch 53/300, trend Loss: 0.0554 | 0.0351
Epoch 54/300, trend Loss: 0.0628 | 0.0274
Epoch 55/300, trend Loss: 0.0521 | 0.0261
Epoch 56/300, trend Loss: 0.0551 | 0.0237
Epoch 57/300, trend Loss: 0.0520 | 0.0290
Epoch 58/300, trend Loss: 0.0521 | 0.0253
Epoch 59/300, trend Loss: 0.0566 | 0.0247
Epoch 60/300, trend Loss: 0.0536 | 0.0246
Epoch 61/300, trend Loss: 0.0527 | 0.0242
Epoch 62/300, trend Loss: 0.0523 | 0.0241
Epoch 63/300, trend Loss: 0.0521 | 0.0235
Epoch 64/300, trend Loss: 0.0517 | 0.0235
Epoch 65/300, trend Loss: 0.0519 | 0.0243
Epoch 66/300, trend Loss: 0.0509 | 0.0232
Epoch 67/300, trend Loss: 0.0506 | 0.0231
Epoch 68/300, trend Loss: 0.0501 | 0.0231
Epoch 69/300, trend Loss: 0.0499 | 0.0232
Epoch 70/300, trend Loss: 0.0495 | 0.0232
Epoch 71/300, trend Loss: 0.0491 | 0.0238
Epoch 72/300, trend Loss: 0.0486 | 0.0243
Epoch 73/300, trend Loss: 0.0481 | 0.0272
Epoch 74/300, trend Loss: 0.0455 | 0.0251
Epoch 75/300, trend Loss: 0.0439 | 0.0260
Epoch 76/300, trend Loss: 0.0448 | 0.0270
Epoch 77/300, trend Loss: 0.0528 | 0.0243
Epoch 78/300, trend Loss: 0.0501 | 0.0230
Epoch 79/300, trend Loss: 0.0493 | 0.0231
Epoch 80/300, trend Loss: 0.0488 | 0.0232
Epoch 81/300, trend Loss: 0.0485 | 0.0236
Epoch 82/300, trend Loss: 0.0443 | 0.0355
Epoch 83/300, trend Loss: 0.0455 | 0.0254
Epoch 84/300, trend Loss: 0.0499 | 0.0244
Epoch 85/300, trend Loss: 0.0479 | 0.0305
Epoch 86/300, trend Loss: 0.0438 | 0.0256
Epoch 87/300, trend Loss: 0.0427 | 0.0305
Epoch 88/300, trend Loss: 0.0438 | 0.0240
Epoch 89/300, trend Loss: 0.0418 | 0.0274
Epoch 90/300, trend Loss: 0.0407 | 0.0254
Epoch 91/300, trend Loss: 0.0463 | 0.0383
Epoch 92/300, trend Loss: 0.0453 | 0.0246
Epoch 93/300, trend Loss: 0.0486 | 0.0322
Epoch 94/300, trend Loss: 0.0435 | 0.0240
Epoch 95/300, trend Loss: 0.0410 | 0.0275
Epoch 96/300, trend Loss: 0.0400 | 0.0282
Epoch 97/300, trend Loss: 0.0393 | 0.0263
Epoch 98/300, trend Loss: 0.0390 | 0.0244
Epoch 99/300, trend Loss: 0.0383 | 0.0253
Epoch 100/300, trend Loss: 0.0380 | 0.0250
Epoch 101/300, trend Loss: 0.0377 | 0.0254
Epoch 102/300, trend Loss: 0.0375 | 0.0254
Epoch 103/300, trend Loss: 0.0373 | 0.0255
Epoch 104/300, trend Loss: 0.0372 | 0.0255
Epoch 105/300, trend Loss: 0.0370 | 0.0255
Epoch 106/300, trend Loss: 0.0368 | 0.0259
Epoch 107/300, trend Loss: 0.0366 | 0.0259
Epoch 108/300, trend Loss: 0.0365 | 0.0260
Epoch 109/300, trend Loss: 0.0363 | 0.0261
Epoch 110/300, trend Loss: 0.0362 | 0.0261
Epoch 111/300, trend Loss: 0.0360 | 0.0260
Epoch 112/300, trend Loss: 0.0359 | 0.0260
Epoch 113/300, trend Loss: 0.0357 | 0.0265
Epoch 114/300, trend Loss: 0.0356 | 0.0265
Epoch 115/300, trend Loss: 0.0354 | 0.0266
Epoch 116/300, trend Loss: 0.0353 | 0.0266
Epoch 117/300, trend Loss: 0.0352 | 0.0266
Epoch 118/300, trend Loss: 0.0350 | 0.0266
Epoch 119/300, trend Loss: 0.0349 | 0.0266
Epoch 120/300, trend Loss: 0.0348 | 0.0271
Epoch 121/300, trend Loss: 0.0346 | 0.0272
Epoch 122/300, trend Loss: 0.0345 | 0.0273
Epoch 123/300, trend Loss: 0.0344 | 0.0274
Epoch 124/300, trend Loss: 0.0343 | 0.0274
Epoch 125/300, trend Loss: 0.0342 | 0.0274
Epoch 126/300, trend Loss: 0.0340 | 0.0273
Epoch 127/300, trend Loss: 0.0339 | 0.0279
Epoch 128/300, trend Loss: 0.0338 | 0.0280
Epoch 129/300, trend Loss: 0.0337 | 0.0282
Epoch 130/300, trend Loss: 0.0336 | 0.0283
Epoch 131/300, trend Loss: 0.0335 | 0.0283
Epoch 132/300, trend Loss: 0.0333 | 0.0282
Epoch 133/300, trend Loss: 0.0332 | 0.0283
Epoch 134/300, trend Loss: 0.0331 | 0.0285
Epoch 135/300, trend Loss: 0.0330 | 0.0286
Epoch 136/300, trend Loss: 0.0329 | 0.0288
Epoch 137/300, trend Loss: 0.0328 | 0.0292
Epoch 138/300, trend Loss: 0.0327 | 0.0290
Epoch 139/300, trend Loss: 0.0326 | 0.0291
Epoch 140/300, trend Loss: 0.0325 | 0.0289
Epoch 141/300, trend Loss: 0.0324 | 0.0288
Epoch 142/300, trend Loss: 0.0323 | 0.0285
Epoch 143/300, trend Loss: 0.0322 | 0.0291
Epoch 144/300, trend Loss: 0.0321 | 0.0288
Epoch 145/300, trend Loss: 0.0320 | 0.0292
Epoch 146/300, trend Loss: 0.0319 | 0.0287
Epoch 147/300, trend Loss: 0.0319 | 0.0292
Epoch 148/300, trend Loss: 0.0318 | 0.0282
Epoch 149/300, trend Loss: 0.0317 | 0.0287
Epoch 150/300, trend Loss: 0.0316 | 0.0284
Epoch 151/300, trend Loss: 0.0316 | 0.0289
Epoch 152/300, trend Loss: 0.0315 | 0.0286
Epoch 153/300, trend Loss: 0.0314 | 0.0290
Epoch 154/300, trend Loss: 0.0313 | 0.0287
Epoch 155/300, trend Loss: 0.0313 | 0.0287
Epoch 156/300, trend Loss: 0.0312 | 0.0284
Epoch 157/300, trend Loss: 0.0311 | 0.0288
Epoch 158/300, trend Loss: 0.0310 | 0.0286
Epoch 159/300, trend Loss: 0.0310 | 0.0289
Epoch 160/300, trend Loss: 0.0309 | 0.0288
Epoch 161/300, trend Loss: 0.0309 | 0.0290
Epoch 162/300, trend Loss: 0.0308 | 0.0287
Epoch 163/300, trend Loss: 0.0307 | 0.0289
Epoch 164/300, trend Loss: 0.0307 | 0.0288
Epoch 165/300, trend Loss: 0.0306 | 0.0290
Epoch 166/300, trend Loss: 0.0306 | 0.0289
Epoch 167/300, trend Loss: 0.0305 | 0.0290
Epoch 168/300, trend Loss: 0.0304 | 0.0290
Epoch 169/300, trend Loss: 0.0304 | 0.0291
Epoch 170/300, trend Loss: 0.0303 | 0.0290
Epoch 171/300, trend Loss: 0.0303 | 0.0291
Epoch 172/300, trend Loss: 0.0303 | 0.0290
Epoch 173/300, trend Loss: 0.0302 | 0.0291
Epoch 174/300, trend Loss: 0.0302 | 0.0291
Epoch 175/300, trend Loss: 0.0301 | 0.0291
Epoch 176/300, trend Loss: 0.0301 | 0.0289
Epoch 177/300, trend Loss: 0.0301 | 0.0289
Epoch 178/300, trend Loss: 0.0300 | 0.0289
Epoch 179/300, trend Loss: 0.0300 | 0.0289
Epoch 180/300, trend Loss: 0.0300 | 0.0289
Epoch 181/300, trend Loss: 0.0299 | 0.0289
Epoch 182/300, trend Loss: 0.0299 | 0.0289
Epoch 183/300, trend Loss: 0.0299 | 0.0289
Epoch 184/300, trend Loss: 0.0299 | 0.0288
Epoch 185/300, trend Loss: 0.0299 | 0.0289
Epoch 186/300, trend Loss: 0.0299 | 0.0288
Epoch 187/300, trend Loss: 0.0298 | 0.0288
Epoch 188/300, trend Loss: 0.0298 | 0.0288
Epoch 189/300, trend Loss: 0.0298 | 0.0288
Epoch 190/300, trend Loss: 0.0298 | 0.0288
Epoch 191/300, trend Loss: 0.0298 | 0.0288
Epoch 192/300, trend Loss: 0.0298 | 0.0288
Epoch 193/300, trend Loss: 0.0298 | 0.0288
Epoch 194/300, trend Loss: 0.0297 | 0.0288
Epoch 195/300, trend Loss: 0.0297 | 0.0288
Epoch 196/300, trend Loss: 0.0296 | 0.0288
Epoch 197/300, trend Loss: 0.0296 | 0.0289
Epoch 198/300, trend Loss: 0.0296 | 0.0288
Epoch 199/300, trend Loss: 0.0295 | 0.0288
Epoch 200/300, trend Loss: 0.0294 | 0.0288
Epoch 201/300, trend Loss: 0.0294 | 0.0288
Epoch 202/300, trend Loss: 0.0294 | 0.0288
Epoch 203/300, trend Loss: 0.0293 | 0.0288
Epoch 204/300, trend Loss: 0.0293 | 0.0289
Epoch 205/300, trend Loss: 0.0293 | 0.0289
Epoch 206/300, trend Loss: 0.0292 | 0.0289
Epoch 207/300, trend Loss: 0.0292 | 0.0289
Epoch 208/300, trend Loss: 0.0292 | 0.0289
Epoch 209/300, trend Loss: 0.0291 | 0.0289
Epoch 210/300, trend Loss: 0.0291 | 0.0289
Epoch 211/300, trend Loss: 0.0291 | 0.0290
Epoch 212/300, trend Loss: 0.0291 | 0.0290
Epoch 213/300, trend Loss: 0.0291 | 0.0290
Epoch 214/300, trend Loss: 0.0290 | 0.0290
Epoch 215/300, trend Loss: 0.0290 | 0.0290
Epoch 216/300, trend Loss: 0.0290 | 0.0290
Epoch 217/300, trend Loss: 0.0290 | 0.0290
Epoch 218/300, trend Loss: 0.0289 | 0.0292
Epoch 219/300, trend Loss: 0.0289 | 0.0292
Epoch 220/300, trend Loss: 0.0289 | 0.0292
Epoch 221/300, trend Loss: 0.0289 | 0.0292
Epoch 222/300, trend Loss: 0.0288 | 0.0292
Epoch 223/300, trend Loss: 0.0288 | 0.0292
Epoch 224/300, trend Loss: 0.0288 | 0.0293
Epoch 225/300, trend Loss: 0.0288 | 0.0294
Epoch 226/300, trend Loss: 0.0288 | 0.0294
Epoch 227/300, trend Loss: 0.0287 | 0.0294
Epoch 228/300, trend Loss: 0.0287 | 0.0294
Epoch 229/300, trend Loss: 0.0287 | 0.0294
Epoch 230/300, trend Loss: 0.0287 | 0.0294
Epoch 231/300, trend Loss: 0.0287 | 0.0295
Epoch 232/300, trend Loss: 0.0286 | 0.0295
Epoch 233/300, trend Loss: 0.0286 | 0.0296
Epoch 234/300, trend Loss: 0.0286 | 0.0296
Epoch 235/300, trend Loss: 0.0286 | 0.0296
Epoch 236/300, trend Loss: 0.0286 | 0.0296
Epoch 237/300, trend Loss: 0.0286 | 0.0296
Epoch 238/300, trend Loss: 0.0285 | 0.0296
Epoch 239/300, trend Loss: 0.0285 | 0.0296
Epoch 240/300, trend Loss: 0.0285 | 0.0296
Epoch 241/300, trend Loss: 0.0285 | 0.0296
Epoch 242/300, trend Loss: 0.0285 | 0.0296
Epoch 243/300, trend Loss: 0.0285 | 0.0296
Epoch 244/300, trend Loss: 0.0285 | 0.0296
Epoch 245/300, trend Loss: 0.0284 | 0.0296
Epoch 246/300, trend Loss: 0.0284 | 0.0297
Epoch 247/300, trend Loss: 0.0284 | 0.0297
Epoch 248/300, trend Loss: 0.0284 | 0.0297
Epoch 249/300, trend Loss: 0.0284 | 0.0297
Epoch 250/300, trend Loss: 0.0284 | 0.0297
Epoch 251/300, trend Loss: 0.0284 | 0.0297
Epoch 252/300, trend Loss: 0.0284 | 0.0297
Epoch 253/300, trend Loss: 0.0284 | 0.0297
Epoch 254/300, trend Loss: 0.0283 | 0.0297
Epoch 255/300, trend Loss: 0.0283 | 0.0297
Epoch 256/300, trend Loss: 0.0283 | 0.0297
Epoch 257/300, trend Loss: 0.0283 | 0.0297
Epoch 258/300, trend Loss: 0.0283 | 0.0297
Epoch 259/300, trend Loss: 0.0283 | 0.0297
Epoch 260/300, trend Loss: 0.0283 | 0.0298
Epoch 261/300, trend Loss: 0.0283 | 0.0298
Epoch 262/300, trend Loss: 0.0283 | 0.0298
Epoch 263/300, trend Loss: 0.0283 | 0.0298
Epoch 264/300, trend Loss: 0.0283 | 0.0298
Epoch 265/300, trend Loss: 0.0283 | 0.0298
Epoch 266/300, trend Loss: 0.0282 | 0.0298
Epoch 267/300, trend Loss: 0.0282 | 0.0298
Epoch 268/300, trend Loss: 0.0282 | 0.0298
Epoch 269/300, trend Loss: 0.0282 | 0.0298
Epoch 270/300, trend Loss: 0.0282 | 0.0298
Epoch 271/300, trend Loss: 0.0282 | 0.0298
Epoch 272/300, trend Loss: 0.0282 | 0.0298
Epoch 273/300, trend Loss: 0.0282 | 0.0298
Epoch 274/300, trend Loss: 0.0282 | 0.0298
Epoch 275/300, trend Loss: 0.0282 | 0.0298
Epoch 276/300, trend Loss: 0.0282 | 0.0298
Epoch 277/300, trend Loss: 0.0282 | 0.0298
Epoch 278/300, trend Loss: 0.0282 | 0.0298
Epoch 279/300, trend Loss: 0.0282 | 0.0298
Epoch 280/300, trend Loss: 0.0282 | 0.0298
Epoch 281/300, trend Loss: 0.0282 | 0.0298
Epoch 282/300, trend Loss: 0.0282 | 0.0298
Epoch 283/300, trend Loss: 0.0281 | 0.0298
Epoch 284/300, trend Loss: 0.0281 | 0.0298
Epoch 285/300, trend Loss: 0.0281 | 0.0298
Epoch 286/300, trend Loss: 0.0281 | 0.0298
Epoch 287/300, trend Loss: 0.0281 | 0.0298
Epoch 288/300, trend Loss: 0.0281 | 0.0298
Epoch 289/300, trend Loss: 0.0281 | 0.0299
Epoch 290/300, trend Loss: 0.0281 | 0.0299
Epoch 291/300, trend Loss: 0.0281 | 0.0299
Epoch 292/300, trend Loss: 0.0281 | 0.0299
Epoch 293/300, trend Loss: 0.0281 | 0.0299
Epoch 294/300, trend Loss: 0.0281 | 0.0299
Epoch 295/300, trend Loss: 0.0281 | 0.0299
Epoch 296/300, trend Loss: 0.0281 | 0.0299
Epoch 297/300, trend Loss: 0.0281 | 0.0299
Epoch 298/300, trend Loss: 0.0281 | 0.0299
Epoch 299/300, trend Loss: 0.0281 | 0.0299
Epoch 300/300, trend Loss: 0.0281 | 0.0299
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.9620421701616301, 'learning_rate': 0.0007938350743920344, 'batch_size': 122, 'step_size': 13, 'gamma': 0.9305058971695567}
Epoch 1/300, seasonal_0 Loss: 0.4230 | 0.1746
Epoch 2/300, seasonal_0 Loss: 0.1865 | 0.1597
Epoch 3/300, seasonal_0 Loss: 0.1432 | 0.1271
Epoch 4/300, seasonal_0 Loss: 0.1268 | 0.0894
Epoch 5/300, seasonal_0 Loss: 0.1254 | 0.0865
Epoch 6/300, seasonal_0 Loss: 0.1319 | 0.0859
Epoch 7/300, seasonal_0 Loss: 0.1264 | 0.0922
Epoch 8/300, seasonal_0 Loss: 0.1139 | 0.0976
Epoch 9/300, seasonal_0 Loss: 0.1167 | 0.1011
Epoch 10/300, seasonal_0 Loss: 0.1253 | 0.0946
Epoch 11/300, seasonal_0 Loss: 0.1137 | 0.0915
Epoch 12/300, seasonal_0 Loss: 0.0995 | 0.0684
Epoch 13/300, seasonal_0 Loss: 0.1025 | 0.0771
Epoch 14/300, seasonal_0 Loss: 0.1223 | 0.0697
Epoch 15/300, seasonal_0 Loss: 0.1298 | 0.1021
Epoch 16/300, seasonal_0 Loss: 0.1106 | 0.0767
Epoch 17/300, seasonal_0 Loss: 0.1124 | 0.0766
Epoch 18/300, seasonal_0 Loss: 0.0947 | 0.0616
Epoch 19/300, seasonal_0 Loss: 0.0903 | 0.0592
Epoch 20/300, seasonal_0 Loss: 0.0846 | 0.0535
Epoch 21/300, seasonal_0 Loss: 0.0827 | 0.0534
Epoch 22/300, seasonal_0 Loss: 0.0808 | 0.0506
Epoch 23/300, seasonal_0 Loss: 0.0830 | 0.0560
Epoch 24/300, seasonal_0 Loss: 0.0880 | 0.0769
Epoch 25/300, seasonal_0 Loss: 0.0925 | 0.0549
Epoch 26/300, seasonal_0 Loss: 0.0889 | 0.0649
Epoch 27/300, seasonal_0 Loss: 0.1049 | 0.0657
Epoch 28/300, seasonal_0 Loss: 0.1018 | 0.0545
Epoch 29/300, seasonal_0 Loss: 0.0921 | 0.0594
Epoch 30/300, seasonal_0 Loss: 0.0890 | 0.0522
Epoch 31/300, seasonal_0 Loss: 0.0829 | 0.0542
Epoch 32/300, seasonal_0 Loss: 0.0794 | 0.0456
Epoch 33/300, seasonal_0 Loss: 0.0765 | 0.0473
Epoch 34/300, seasonal_0 Loss: 0.0781 | 0.0456
Epoch 35/300, seasonal_0 Loss: 0.0789 | 0.0464
Epoch 36/300, seasonal_0 Loss: 0.0815 | 0.0479
Epoch 37/300, seasonal_0 Loss: 0.0796 | 0.0521
Epoch 38/300, seasonal_0 Loss: 0.0789 | 0.0481
Epoch 39/300, seasonal_0 Loss: 0.0784 | 0.0527
Epoch 40/300, seasonal_0 Loss: 0.0765 | 0.0464
Epoch 41/300, seasonal_0 Loss: 0.0764 | 0.0461
Epoch 42/300, seasonal_0 Loss: 0.0906 | 0.0664
Epoch 43/300, seasonal_0 Loss: 0.0848 | 0.0513
Epoch 44/300, seasonal_0 Loss: 0.0804 | 0.0515
Epoch 45/300, seasonal_0 Loss: 0.0723 | 0.0422
Epoch 46/300, seasonal_0 Loss: 0.0731 | 0.0482
Epoch 47/300, seasonal_0 Loss: 0.0675 | 0.0422
Epoch 48/300, seasonal_0 Loss: 0.0679 | 0.0462
Epoch 49/300, seasonal_0 Loss: 0.0698 | 0.0470
Epoch 50/300, seasonal_0 Loss: 0.0687 | 0.0511
Epoch 51/300, seasonal_0 Loss: 0.0732 | 0.0504
Epoch 52/300, seasonal_0 Loss: 0.0765 | 0.0529
Epoch 53/300, seasonal_0 Loss: 0.0723 | 0.0501
Epoch 54/300, seasonal_0 Loss: 0.0760 | 0.0590
Epoch 55/300, seasonal_0 Loss: 0.0820 | 0.0509
Epoch 56/300, seasonal_0 Loss: 0.0735 | 0.0486
Epoch 57/300, seasonal_0 Loss: 0.0756 | 0.0467
Epoch 58/300, seasonal_0 Loss: 0.0907 | 0.0542
Epoch 59/300, seasonal_0 Loss: 0.0880 | 0.0624
Epoch 60/300, seasonal_0 Loss: 0.0815 | 0.0492
Epoch 61/300, seasonal_0 Loss: 0.0931 | 0.0620
Epoch 62/300, seasonal_0 Loss: 0.1069 | 0.0631
Epoch 63/300, seasonal_0 Loss: 0.0803 | 0.0510
Epoch 64/300, seasonal_0 Loss: 0.0914 | 0.0905
Epoch 65/300, seasonal_0 Loss: 0.0987 | 0.1056
Epoch 66/300, seasonal_0 Loss: 0.0834 | 0.0645
Epoch 67/300, seasonal_0 Loss: 0.0775 | 0.0595
Epoch 68/300, seasonal_0 Loss: 0.0790 | 0.0512
Epoch 69/300, seasonal_0 Loss: 0.0783 | 0.0446
Epoch 70/300, seasonal_0 Loss: 0.0793 | 0.0482
Epoch 71/300, seasonal_0 Loss: 0.0813 | 0.0414
Epoch 72/300, seasonal_0 Loss: 0.0726 | 0.0431
Epoch 73/300, seasonal_0 Loss: 0.0713 | 0.0604
Epoch 74/300, seasonal_0 Loss: 0.0666 | 0.0486
Epoch 75/300, seasonal_0 Loss: 0.0634 | 0.0409
Epoch 76/300, seasonal_0 Loss: 0.0644 | 0.0353
Epoch 77/300, seasonal_0 Loss: 0.0645 | 0.0374
Epoch 78/300, seasonal_0 Loss: 0.0661 | 0.0369
Epoch 79/300, seasonal_0 Loss: 0.0709 | 0.0415
Epoch 80/300, seasonal_0 Loss: 0.0749 | 0.0439
Epoch 81/300, seasonal_0 Loss: 0.0750 | 0.0478
Epoch 82/300, seasonal_0 Loss: 0.0686 | 0.0407
Epoch 83/300, seasonal_0 Loss: 0.0676 | 0.0417
Epoch 84/300, seasonal_0 Loss: 0.0718 | 0.0564
Epoch 85/300, seasonal_0 Loss: 0.0701 | 0.0382
Epoch 86/300, seasonal_0 Loss: 0.0754 | 0.0522
Epoch 87/300, seasonal_0 Loss: 0.0669 | 0.0500
Epoch 88/300, seasonal_0 Loss: 0.0656 | 0.0523
Epoch 89/300, seasonal_0 Loss: 0.0633 | 0.0405
Epoch 90/300, seasonal_0 Loss: 0.0615 | 0.0363
Epoch 91/300, seasonal_0 Loss: 0.0651 | 0.0376
Epoch 92/300, seasonal_0 Loss: 0.0693 | 0.0439
Epoch 93/300, seasonal_0 Loss: 0.0699 | 0.0440
Epoch 94/300, seasonal_0 Loss: 0.0622 | 0.0457
Epoch 95/300, seasonal_0 Loss: 0.0612 | 0.0416
Epoch 96/300, seasonal_0 Loss: 0.0628 | 0.0415
Epoch 97/300, seasonal_0 Loss: 0.0591 | 0.0411
Epoch 98/300, seasonal_0 Loss: 0.0599 | 0.0433
Epoch 99/300, seasonal_0 Loss: 0.0594 | 0.0444
Epoch 100/300, seasonal_0 Loss: 0.0579 | 0.0340
Epoch 101/300, seasonal_0 Loss: 0.0561 | 0.0357
Epoch 102/300, seasonal_0 Loss: 0.0558 | 0.0382
Epoch 103/300, seasonal_0 Loss: 0.0551 | 0.0362
Epoch 104/300, seasonal_0 Loss: 0.0545 | 0.0426
Epoch 105/300, seasonal_0 Loss: 0.0557 | 0.0393
Epoch 106/300, seasonal_0 Loss: 0.0560 | 0.0444
Epoch 107/300, seasonal_0 Loss: 0.0565 | 0.0386
Epoch 108/300, seasonal_0 Loss: 0.0539 | 0.0347
Epoch 109/300, seasonal_0 Loss: 0.0544 | 0.0377
Epoch 110/300, seasonal_0 Loss: 0.0555 | 0.0377
Epoch 111/300, seasonal_0 Loss: 0.0592 | 0.0419
Epoch 112/300, seasonal_0 Loss: 0.0580 | 0.0386
Epoch 113/300, seasonal_0 Loss: 0.0558 | 0.0394
Epoch 114/300, seasonal_0 Loss: 0.0536 | 0.0385
Epoch 115/300, seasonal_0 Loss: 0.0529 | 0.0362
Epoch 116/300, seasonal_0 Loss: 0.0523 | 0.0403
Epoch 117/300, seasonal_0 Loss: 0.0534 | 0.0453
Epoch 118/300, seasonal_0 Loss: 0.0521 | 0.0414
Epoch 119/300, seasonal_0 Loss: 0.0522 | 0.0409
Epoch 120/300, seasonal_0 Loss: 0.0530 | 0.0391
Epoch 121/300, seasonal_0 Loss: 0.0527 | 0.0377
Epoch 122/300, seasonal_0 Loss: 0.0530 | 0.0384
Epoch 123/300, seasonal_0 Loss: 0.0582 | 0.0453
Epoch 124/300, seasonal_0 Loss: 0.0566 | 0.0418
Epoch 125/300, seasonal_0 Loss: 0.0522 | 0.0411
Epoch 126/300, seasonal_0 Loss: 0.0517 | 0.0387
Epoch 127/300, seasonal_0 Loss: 0.0513 | 0.0388
Epoch 128/300, seasonal_0 Loss: 0.0508 | 0.0388
Epoch 129/300, seasonal_0 Loss: 0.0508 | 0.0397
Epoch 130/300, seasonal_0 Loss: 0.0504 | 0.0434
Epoch 131/300, seasonal_0 Loss: 0.0491 | 0.0384
Epoch 132/300, seasonal_0 Loss: 0.0505 | 0.0381
Epoch 133/300, seasonal_0 Loss: 0.0527 | 0.0391
Epoch 134/300, seasonal_0 Loss: 0.0515 | 0.0394
Epoch 135/300, seasonal_0 Loss: 0.0500 | 0.0377
Epoch 136/300, seasonal_0 Loss: 0.0486 | 0.0365
Epoch 137/300, seasonal_0 Loss: 0.0486 | 0.0373
Epoch 138/300, seasonal_0 Loss: 0.0487 | 0.0393
Epoch 139/300, seasonal_0 Loss: 0.0490 | 0.0409
Epoch 140/300, seasonal_0 Loss: 0.0487 | 0.0442
Epoch 141/300, seasonal_0 Loss: 0.0473 | 0.0388
Epoch 142/300, seasonal_0 Loss: 0.0480 | 0.0407
Epoch 143/300, seasonal_0 Loss: 0.0500 | 0.0402
Epoch 144/300, seasonal_0 Loss: 0.0501 | 0.0390
Epoch 145/300, seasonal_0 Loss: 0.0490 | 0.0374
Epoch 146/300, seasonal_0 Loss: 0.0473 | 0.0395
Epoch 147/300, seasonal_0 Loss: 0.0479 | 0.0386
Epoch 148/300, seasonal_0 Loss: 0.0493 | 0.0450
Epoch 149/300, seasonal_0 Loss: 0.0475 | 0.0411
Epoch 150/300, seasonal_0 Loss: 0.0462 | 0.0392
Epoch 151/300, seasonal_0 Loss: 0.0468 | 0.0386
Epoch 152/300, seasonal_0 Loss: 0.0480 | 0.0377
Epoch 153/300, seasonal_0 Loss: 0.0478 | 0.0379
Epoch 154/300, seasonal_0 Loss: 0.0461 | 0.0401
Epoch 155/300, seasonal_0 Loss: 0.0460 | 0.0395
Epoch 156/300, seasonal_0 Loss: 0.0469 | 0.0428
Epoch 157/300, seasonal_0 Loss: 0.0458 | 0.0391
Epoch 158/300, seasonal_0 Loss: 0.0448 | 0.0371
Epoch 159/300, seasonal_0 Loss: 0.0454 | 0.0382
Epoch 160/300, seasonal_0 Loss: 0.0456 | 0.0380
Epoch 161/300, seasonal_0 Loss: 0.0449 | 0.0388
Epoch 162/300, seasonal_0 Loss: 0.0442 | 0.0414
Epoch 163/300, seasonal_0 Loss: 0.0443 | 0.0410
Epoch 164/300, seasonal_0 Loss: 0.0443 | 0.0403
Epoch 165/300, seasonal_0 Loss: 0.0434 | 0.0381
Epoch 166/300, seasonal_0 Loss: 0.0431 | 0.0372
Epoch 167/300, seasonal_0 Loss: 0.0426 | 0.0370
Epoch 168/300, seasonal_0 Loss: 0.0408 | 0.0403
Epoch 169/300, seasonal_0 Loss: 0.0396 | 0.0401
Epoch 170/300, seasonal_0 Loss: 0.0428 | 0.0405
Epoch 171/300, seasonal_0 Loss: 0.0389 | 0.0404
Epoch 172/300, seasonal_0 Loss: 0.0386 | 0.0505
Epoch 173/300, seasonal_0 Loss: 0.0405 | 0.0439
Epoch 174/300, seasonal_0 Loss: 0.0445 | 0.0402
Epoch 175/300, seasonal_0 Loss: 0.0442 | 0.0399
Epoch 176/300, seasonal_0 Loss: 0.0435 | 0.0391
Epoch 177/300, seasonal_0 Loss: 0.0431 | 0.0398
Epoch 178/300, seasonal_0 Loss: 0.0425 | 0.0397
Epoch 179/300, seasonal_0 Loss: 0.0422 | 0.0390
Epoch 180/300, seasonal_0 Loss: 0.0417 | 0.0388
Epoch 181/300, seasonal_0 Loss: 0.0414 | 0.0399
Epoch 182/300, seasonal_0 Loss: 0.0411 | 0.0401
Epoch 183/300, seasonal_0 Loss: 0.0408 | 0.0394
Epoch 184/300, seasonal_0 Loss: 0.0402 | 0.0393
Epoch 185/300, seasonal_0 Loss: 0.0360 | 0.0390
Epoch 186/300, seasonal_0 Loss: 0.0354 | 0.0398
Epoch 187/300, seasonal_0 Loss: 0.0351 | 0.0394
Epoch 188/300, seasonal_0 Loss: 0.0355 | 0.0418
Epoch 189/300, seasonal_0 Loss: 0.0399 | 0.0418
Epoch 190/300, seasonal_0 Loss: 0.0427 | 0.0395
Epoch 191/300, seasonal_0 Loss: 0.0414 | 0.0384
Epoch 192/300, seasonal_0 Loss: 0.0404 | 0.0403
Epoch 193/300, seasonal_0 Loss: 0.0389 | 0.0406
Epoch 194/300, seasonal_0 Loss: 0.0368 | 0.0402
Epoch 195/300, seasonal_0 Loss: 0.0359 | 0.0410
Epoch 196/300, seasonal_0 Loss: 0.0356 | 0.0411
Epoch 197/300, seasonal_0 Loss: 0.0364 | 0.0433
Epoch 198/300, seasonal_0 Loss: 0.0369 | 0.0411
Epoch 199/300, seasonal_0 Loss: 0.0356 | 0.0409
Epoch 200/300, seasonal_0 Loss: 0.0346 | 0.0401
Epoch 201/300, seasonal_0 Loss: 0.0344 | 0.0399
Epoch 202/300, seasonal_0 Loss: 0.0342 | 0.0399
Epoch 203/300, seasonal_0 Loss: 0.0345 | 0.0402
Epoch 204/300, seasonal_0 Loss: 0.0342 | 0.0409
Epoch 205/300, seasonal_0 Loss: 0.0343 | 0.0406
Epoch 206/300, seasonal_0 Loss: 0.0340 | 0.0409
Epoch 207/300, seasonal_0 Loss: 0.0338 | 0.0403
Epoch 208/300, seasonal_0 Loss: 0.0342 | 0.0411
Epoch 209/300, seasonal_0 Loss: 0.0348 | 0.0418
Epoch 210/300, seasonal_0 Loss: 0.0344 | 0.0426
Epoch 211/300, seasonal_0 Loss: 0.0336 | 0.0419
Epoch 212/300, seasonal_0 Loss: 0.0335 | 0.0419
Epoch 213/300, seasonal_0 Loss: 0.0337 | 0.0422
Epoch 214/300, seasonal_0 Loss: 0.0335 | 0.0413
Epoch 215/300, seasonal_0 Loss: 0.0333 | 0.0397
Epoch 216/300, seasonal_0 Loss: 0.0332 | 0.0414
Epoch 217/300, seasonal_0 Loss: 0.0331 | 0.0402
Epoch 218/300, seasonal_0 Loss: 0.0328 | 0.0418
Epoch 219/300, seasonal_0 Loss: 0.0325 | 0.0422
Epoch 220/300, seasonal_0 Loss: 0.0324 | 0.0431
Epoch 221/300, seasonal_0 Loss: 0.0321 | 0.0416
Epoch 222/300, seasonal_0 Loss: 0.0320 | 0.0415
Epoch 223/300, seasonal_0 Loss: 0.0320 | 0.0402
Epoch 224/300, seasonal_0 Loss: 0.0319 | 0.0415
Epoch 225/300, seasonal_0 Loss: 0.0318 | 0.0408
Epoch 226/300, seasonal_0 Loss: 0.0316 | 0.0423
Epoch 227/300, seasonal_0 Loss: 0.0316 | 0.0414
Epoch 228/300, seasonal_0 Loss: 0.0315 | 0.0428
Epoch 229/300, seasonal_0 Loss: 0.0313 | 0.0412
Epoch 230/300, seasonal_0 Loss: 0.0313 | 0.0420
Epoch 231/300, seasonal_0 Loss: 0.0313 | 0.0408
Epoch 232/300, seasonal_0 Loss: 0.0312 | 0.0415
Epoch 233/300, seasonal_0 Loss: 0.0309 | 0.0410
Epoch 234/300, seasonal_0 Loss: 0.0308 | 0.0421
Epoch 235/300, seasonal_0 Loss: 0.0308 | 0.0419
Epoch 236/300, seasonal_0 Loss: 0.0308 | 0.0423
Epoch 237/300, seasonal_0 Loss: 0.0307 | 0.0413
Epoch 238/300, seasonal_0 Loss: 0.0306 | 0.0413
Epoch 239/300, seasonal_0 Loss: 0.0306 | 0.0409
Epoch 240/300, seasonal_0 Loss: 0.0306 | 0.0416
Epoch 241/300, seasonal_0 Loss: 0.0304 | 0.0415
Epoch 242/300, seasonal_0 Loss: 0.0303 | 0.0424
Epoch 243/300, seasonal_0 Loss: 0.0303 | 0.0424
Epoch 244/300, seasonal_0 Loss: 0.0303 | 0.0435
Epoch 245/300, seasonal_0 Loss: 0.0301 | 0.0426
Epoch 246/300, seasonal_0 Loss: 0.0301 | 0.0427
Epoch 247/300, seasonal_0 Loss: 0.0301 | 0.0415
Epoch 248/300, seasonal_0 Loss: 0.0301 | 0.0418
Epoch 249/300, seasonal_0 Loss: 0.0300 | 0.0414
Epoch 250/300, seasonal_0 Loss: 0.0299 | 0.0425
Epoch 251/300, seasonal_0 Loss: 0.0299 | 0.0425
Epoch 252/300, seasonal_0 Loss: 0.0299 | 0.0434
Epoch 253/300, seasonal_0 Loss: 0.0299 | 0.0422
Epoch 254/300, seasonal_0 Loss: 0.0297 | 0.0424
Epoch 255/300, seasonal_0 Loss: 0.0298 | 0.0414
Epoch 256/300, seasonal_0 Loss: 0.0298 | 0.0427
Epoch 257/300, seasonal_0 Loss: 0.0298 | 0.0423
Epoch 258/300, seasonal_0 Loss: 0.0296 | 0.0433
Epoch 259/300, seasonal_0 Loss: 0.0295 | 0.0428
Epoch 260/300, seasonal_0 Loss: 0.0295 | 0.0438
Epoch 261/300, seasonal_0 Loss: 0.0293 | 0.0428
Epoch 262/300, seasonal_0 Loss: 0.0293 | 0.0431
Epoch 263/300, seasonal_0 Loss: 0.0293 | 0.0420
Epoch 264/300, seasonal_0 Loss: 0.0292 | 0.0423
Epoch 265/300, seasonal_0 Loss: 0.0291 | 0.0420
Epoch 266/300, seasonal_0 Loss: 0.0291 | 0.0430
Epoch 267/300, seasonal_0 Loss: 0.0291 | 0.0427
Epoch 268/300, seasonal_0 Loss: 0.0290 | 0.0432
Epoch 269/300, seasonal_0 Loss: 0.0289 | 0.0422
Epoch 270/300, seasonal_0 Loss: 0.0288 | 0.0423
Epoch 271/300, seasonal_0 Loss: 0.0289 | 0.0420
Epoch 272/300, seasonal_0 Loss: 0.0288 | 0.0431
Epoch 273/300, seasonal_0 Loss: 0.0287 | 0.0428
Epoch 274/300, seasonal_0 Loss: 0.0287 | 0.0438
Epoch 275/300, seasonal_0 Loss: 0.0287 | 0.0431
Epoch 276/300, seasonal_0 Loss: 0.0286 | 0.0436
Epoch 277/300, seasonal_0 Loss: 0.0285 | 0.0427
Epoch 278/300, seasonal_0 Loss: 0.0285 | 0.0430
Epoch 279/300, seasonal_0 Loss: 0.0285 | 0.0424
Epoch 280/300, seasonal_0 Loss: 0.0284 | 0.0429
Epoch 281/300, seasonal_0 Loss: 0.0284 | 0.0427
Epoch 282/300, seasonal_0 Loss: 0.0283 | 0.0434
Epoch 283/300, seasonal_0 Loss: 0.0283 | 0.0431
Epoch 284/300, seasonal_0 Loss: 0.0282 | 0.0434
Epoch 285/300, seasonal_0 Loss: 0.0282 | 0.0429
Epoch 286/300, seasonal_0 Loss: 0.0281 | 0.0431
Epoch 287/300, seasonal_0 Loss: 0.0281 | 0.0428
Epoch 288/300, seasonal_0 Loss: 0.0280 | 0.0434
Epoch 289/300, seasonal_0 Loss: 0.0279 | 0.0432
Epoch 290/300, seasonal_0 Loss: 0.0279 | 0.0438
Epoch 291/300, seasonal_0 Loss: 0.0279 | 0.0432
Epoch 292/300, seasonal_0 Loss: 0.0279 | 0.0435
Epoch 293/300, seasonal_0 Loss: 0.0278 | 0.0428
Epoch 294/300, seasonal_0 Loss: 0.0278 | 0.0433
Epoch 295/300, seasonal_0 Loss: 0.0278 | 0.0429
Epoch 296/300, seasonal_0 Loss: 0.0277 | 0.0435
Epoch 297/300, seasonal_0 Loss: 0.0277 | 0.0432
Epoch 298/300, seasonal_0 Loss: 0.0277 | 0.0440
Epoch 299/300, seasonal_0 Loss: 0.0276 | 0.0434
Epoch 300/300, seasonal_0 Loss: 0.0276 | 0.0440
Training seasonal_1 component with params: {'observation_period_num': 9, 'train_rates': 0.9122152544803803, 'learning_rate': 5.8654772177333256e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.873438788892266}
Epoch 1/300, seasonal_1 Loss: 0.1902 | 0.1022
Epoch 2/300, seasonal_1 Loss: 0.1316 | 0.0761
Epoch 3/300, seasonal_1 Loss: 0.1204 | 0.0664
Epoch 4/300, seasonal_1 Loss: 0.1148 | 0.0630
Epoch 5/300, seasonal_1 Loss: 0.1109 | 0.0609
Epoch 6/300, seasonal_1 Loss: 0.1077 | 0.0592
Epoch 7/300, seasonal_1 Loss: 0.1047 | 0.0577
Epoch 8/300, seasonal_1 Loss: 0.1014 | 0.0574
Epoch 9/300, seasonal_1 Loss: 0.0985 | 0.0565
Epoch 10/300, seasonal_1 Loss: 0.0958 | 0.0556
Epoch 11/300, seasonal_1 Loss: 0.0934 | 0.0546
Epoch 12/300, seasonal_1 Loss: 0.0911 | 0.0530
Epoch 13/300, seasonal_1 Loss: 0.0891 | 0.0512
Epoch 14/300, seasonal_1 Loss: 0.0872 | 0.0490
Epoch 15/300, seasonal_1 Loss: 0.0860 | 0.0482
Epoch 16/300, seasonal_1 Loss: 0.0850 | 0.0474
Epoch 17/300, seasonal_1 Loss: 0.0840 | 0.0465
Epoch 18/300, seasonal_1 Loss: 0.0831 | 0.0456
Epoch 19/300, seasonal_1 Loss: 0.0823 | 0.0448
Epoch 20/300, seasonal_1 Loss: 0.0816 | 0.0440
Epoch 21/300, seasonal_1 Loss: 0.0811 | 0.0418
Epoch 22/300, seasonal_1 Loss: 0.0801 | 0.0413
Epoch 23/300, seasonal_1 Loss: 0.0793 | 0.0409
Epoch 24/300, seasonal_1 Loss: 0.0786 | 0.0404
Epoch 25/300, seasonal_1 Loss: 0.0779 | 0.0400
Epoch 26/300, seasonal_1 Loss: 0.0773 | 0.0396
Epoch 27/300, seasonal_1 Loss: 0.0767 | 0.0391
Epoch 28/300, seasonal_1 Loss: 0.0759 | 0.0387
Epoch 29/300, seasonal_1 Loss: 0.0753 | 0.0382
Epoch 30/300, seasonal_1 Loss: 0.0747 | 0.0376
Epoch 31/300, seasonal_1 Loss: 0.0742 | 0.0369
Epoch 32/300, seasonal_1 Loss: 0.0736 | 0.0362
Epoch 33/300, seasonal_1 Loss: 0.0730 | 0.0356
Epoch 34/300, seasonal_1 Loss: 0.0723 | 0.0355
Epoch 35/300, seasonal_1 Loss: 0.0716 | 0.0350
Epoch 36/300, seasonal_1 Loss: 0.0711 | 0.0345
Epoch 37/300, seasonal_1 Loss: 0.0706 | 0.0340
Epoch 38/300, seasonal_1 Loss: 0.0702 | 0.0335
Epoch 39/300, seasonal_1 Loss: 0.0699 | 0.0332
Epoch 40/300, seasonal_1 Loss: 0.0695 | 0.0333
Epoch 41/300, seasonal_1 Loss: 0.0691 | 0.0330
Epoch 42/300, seasonal_1 Loss: 0.0688 | 0.0327
Epoch 43/300, seasonal_1 Loss: 0.0685 | 0.0323
Epoch 44/300, seasonal_1 Loss: 0.0682 | 0.0321
Epoch 45/300, seasonal_1 Loss: 0.0680 | 0.0318
Epoch 46/300, seasonal_1 Loss: 0.0677 | 0.0316
Epoch 47/300, seasonal_1 Loss: 0.0674 | 0.0317
Epoch 48/300, seasonal_1 Loss: 0.0671 | 0.0315
Epoch 49/300, seasonal_1 Loss: 0.0668 | 0.0313
Epoch 50/300, seasonal_1 Loss: 0.0666 | 0.0311
Epoch 51/300, seasonal_1 Loss: 0.0664 | 0.0309
Epoch 52/300, seasonal_1 Loss: 0.0661 | 0.0308
Epoch 53/300, seasonal_1 Loss: 0.0658 | 0.0308
Epoch 54/300, seasonal_1 Loss: 0.0656 | 0.0306
Epoch 55/300, seasonal_1 Loss: 0.0654 | 0.0305
Epoch 56/300, seasonal_1 Loss: 0.0651 | 0.0303
Epoch 57/300, seasonal_1 Loss: 0.0649 | 0.0302
Epoch 58/300, seasonal_1 Loss: 0.0647 | 0.0301
Epoch 59/300, seasonal_1 Loss: 0.0645 | 0.0299
Epoch 60/300, seasonal_1 Loss: 0.0643 | 0.0299
Epoch 61/300, seasonal_1 Loss: 0.0641 | 0.0298
Epoch 62/300, seasonal_1 Loss: 0.0639 | 0.0297
Epoch 63/300, seasonal_1 Loss: 0.0637 | 0.0296
Epoch 64/300, seasonal_1 Loss: 0.0635 | 0.0295
Epoch 65/300, seasonal_1 Loss: 0.0634 | 0.0294
Epoch 66/300, seasonal_1 Loss: 0.0631 | 0.0293
Epoch 67/300, seasonal_1 Loss: 0.0630 | 0.0292
Epoch 68/300, seasonal_1 Loss: 0.0628 | 0.0291
Epoch 69/300, seasonal_1 Loss: 0.0627 | 0.0290
Epoch 70/300, seasonal_1 Loss: 0.0625 | 0.0289
Epoch 71/300, seasonal_1 Loss: 0.0624 | 0.0288
Epoch 72/300, seasonal_1 Loss: 0.0622 | 0.0287
Epoch 73/300, seasonal_1 Loss: 0.0620 | 0.0286
Epoch 74/300, seasonal_1 Loss: 0.0619 | 0.0286
Epoch 75/300, seasonal_1 Loss: 0.0618 | 0.0285
Epoch 76/300, seasonal_1 Loss: 0.0617 | 0.0284
Epoch 77/300, seasonal_1 Loss: 0.0615 | 0.0284
Epoch 78/300, seasonal_1 Loss: 0.0614 | 0.0283
Epoch 79/300, seasonal_1 Loss: 0.0612 | 0.0282
Epoch 80/300, seasonal_1 Loss: 0.0612 | 0.0281
Epoch 81/300, seasonal_1 Loss: 0.0610 | 0.0281
Epoch 82/300, seasonal_1 Loss: 0.0609 | 0.0280
Epoch 83/300, seasonal_1 Loss: 0.0608 | 0.0280
Epoch 84/300, seasonal_1 Loss: 0.0607 | 0.0279
Epoch 85/300, seasonal_1 Loss: 0.0606 | 0.0278
Epoch 86/300, seasonal_1 Loss: 0.0605 | 0.0276
Epoch 87/300, seasonal_1 Loss: 0.0604 | 0.0276
Epoch 88/300, seasonal_1 Loss: 0.0603 | 0.0276
Epoch 89/300, seasonal_1 Loss: 0.0602 | 0.0275
Epoch 90/300, seasonal_1 Loss: 0.0601 | 0.0275
Epoch 91/300, seasonal_1 Loss: 0.0600 | 0.0274
Epoch 92/300, seasonal_1 Loss: 0.0599 | 0.0272
Epoch 93/300, seasonal_1 Loss: 0.0598 | 0.0271
Epoch 94/300, seasonal_1 Loss: 0.0597 | 0.0271
Epoch 95/300, seasonal_1 Loss: 0.0597 | 0.0271
Epoch 96/300, seasonal_1 Loss: 0.0596 | 0.0270
Epoch 97/300, seasonal_1 Loss: 0.0595 | 0.0270
Epoch 98/300, seasonal_1 Loss: 0.0594 | 0.0269
Epoch 99/300, seasonal_1 Loss: 0.0593 | 0.0268
Epoch 100/300, seasonal_1 Loss: 0.0593 | 0.0268
Epoch 101/300, seasonal_1 Loss: 0.0592 | 0.0267
Epoch 102/300, seasonal_1 Loss: 0.0591 | 0.0267
Epoch 103/300, seasonal_1 Loss: 0.0591 | 0.0267
Epoch 104/300, seasonal_1 Loss: 0.0590 | 0.0266
Epoch 105/300, seasonal_1 Loss: 0.0589 | 0.0266
Epoch 106/300, seasonal_1 Loss: 0.0588 | 0.0266
Epoch 107/300, seasonal_1 Loss: 0.0588 | 0.0266
Epoch 108/300, seasonal_1 Loss: 0.0587 | 0.0265
Epoch 109/300, seasonal_1 Loss: 0.0587 | 0.0265
Epoch 110/300, seasonal_1 Loss: 0.0586 | 0.0265
Epoch 111/300, seasonal_1 Loss: 0.0585 | 0.0264
Epoch 112/300, seasonal_1 Loss: 0.0585 | 0.0265
Epoch 113/300, seasonal_1 Loss: 0.0584 | 0.0265
Epoch 114/300, seasonal_1 Loss: 0.0583 | 0.0264
Epoch 115/300, seasonal_1 Loss: 0.0583 | 0.0264
Epoch 116/300, seasonal_1 Loss: 0.0582 | 0.0264
Epoch 117/300, seasonal_1 Loss: 0.0582 | 0.0264
Epoch 118/300, seasonal_1 Loss: 0.0581 | 0.0264
Epoch 119/300, seasonal_1 Loss: 0.0581 | 0.0264
Epoch 120/300, seasonal_1 Loss: 0.0580 | 0.0264
Epoch 121/300, seasonal_1 Loss: 0.0580 | 0.0264
Epoch 122/300, seasonal_1 Loss: 0.0579 | 0.0263
Epoch 123/300, seasonal_1 Loss: 0.0579 | 0.0263
Epoch 124/300, seasonal_1 Loss: 0.0578 | 0.0263
Epoch 125/300, seasonal_1 Loss: 0.0578 | 0.0264
Epoch 126/300, seasonal_1 Loss: 0.0577 | 0.0264
Epoch 127/300, seasonal_1 Loss: 0.0577 | 0.0263
Epoch 128/300, seasonal_1 Loss: 0.0577 | 0.0263
Epoch 129/300, seasonal_1 Loss: 0.0576 | 0.0263
Epoch 130/300, seasonal_1 Loss: 0.0576 | 0.0263
Epoch 131/300, seasonal_1 Loss: 0.0575 | 0.0263
Epoch 132/300, seasonal_1 Loss: 0.0575 | 0.0263
Epoch 133/300, seasonal_1 Loss: 0.0575 | 0.0263
Epoch 134/300, seasonal_1 Loss: 0.0574 | 0.0263
Epoch 135/300, seasonal_1 Loss: 0.0574 | 0.0263
Epoch 136/300, seasonal_1 Loss: 0.0573 | 0.0262
Epoch 137/300, seasonal_1 Loss: 0.0573 | 0.0262
Epoch 138/300, seasonal_1 Loss: 0.0573 | 0.0262
Epoch 139/300, seasonal_1 Loss: 0.0572 | 0.0262
Epoch 140/300, seasonal_1 Loss: 0.0572 | 0.0262
Epoch 141/300, seasonal_1 Loss: 0.0572 | 0.0262
Epoch 142/300, seasonal_1 Loss: 0.0571 | 0.0261
Epoch 143/300, seasonal_1 Loss: 0.0571 | 0.0261
Epoch 144/300, seasonal_1 Loss: 0.0571 | 0.0261
Epoch 145/300, seasonal_1 Loss: 0.0570 | 0.0260
Epoch 146/300, seasonal_1 Loss: 0.0570 | 0.0260
Epoch 147/300, seasonal_1 Loss: 0.0570 | 0.0260
Epoch 148/300, seasonal_1 Loss: 0.0570 | 0.0260
Epoch 149/300, seasonal_1 Loss: 0.0569 | 0.0260
Epoch 150/300, seasonal_1 Loss: 0.0569 | 0.0260
Epoch 151/300, seasonal_1 Loss: 0.0569 | 0.0259
Epoch 152/300, seasonal_1 Loss: 0.0568 | 0.0259
Epoch 153/300, seasonal_1 Loss: 0.0568 | 0.0259
Epoch 154/300, seasonal_1 Loss: 0.0568 | 0.0259
Epoch 155/300, seasonal_1 Loss: 0.0568 | 0.0259
Epoch 156/300, seasonal_1 Loss: 0.0567 | 0.0258
Epoch 157/300, seasonal_1 Loss: 0.0567 | 0.0257
Epoch 158/300, seasonal_1 Loss: 0.0567 | 0.0257
Epoch 159/300, seasonal_1 Loss: 0.0567 | 0.0257
Epoch 160/300, seasonal_1 Loss: 0.0567 | 0.0257
Epoch 161/300, seasonal_1 Loss: 0.0566 | 0.0257
Epoch 162/300, seasonal_1 Loss: 0.0566 | 0.0257
Epoch 163/300, seasonal_1 Loss: 0.0566 | 0.0257
Epoch 164/300, seasonal_1 Loss: 0.0566 | 0.0256
Epoch 165/300, seasonal_1 Loss: 0.0565 | 0.0256
Epoch 166/300, seasonal_1 Loss: 0.0565 | 0.0256
Epoch 167/300, seasonal_1 Loss: 0.0565 | 0.0256
Epoch 168/300, seasonal_1 Loss: 0.0565 | 0.0256
Epoch 169/300, seasonal_1 Loss: 0.0565 | 0.0256
Epoch 170/300, seasonal_1 Loss: 0.0564 | 0.0255
Epoch 171/300, seasonal_1 Loss: 0.0564 | 0.0255
Epoch 172/300, seasonal_1 Loss: 0.0564 | 0.0255
Epoch 173/300, seasonal_1 Loss: 0.0564 | 0.0255
Epoch 174/300, seasonal_1 Loss: 0.0564 | 0.0255
Epoch 175/300, seasonal_1 Loss: 0.0564 | 0.0255
Epoch 176/300, seasonal_1 Loss: 0.0563 | 0.0255
Epoch 177/300, seasonal_1 Loss: 0.0563 | 0.0254
Epoch 178/300, seasonal_1 Loss: 0.0563 | 0.0254
Epoch 179/300, seasonal_1 Loss: 0.0563 | 0.0254
Epoch 180/300, seasonal_1 Loss: 0.0563 | 0.0254
Epoch 181/300, seasonal_1 Loss: 0.0563 | 0.0254
Epoch 182/300, seasonal_1 Loss: 0.0563 | 0.0254
Epoch 183/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 184/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 185/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 186/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 187/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 188/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 189/300, seasonal_1 Loss: 0.0562 | 0.0254
Epoch 190/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 191/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 192/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 193/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 194/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 195/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 196/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 197/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 198/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 199/300, seasonal_1 Loss: 0.0561 | 0.0254
Epoch 200/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 201/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 202/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 203/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 204/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 205/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 206/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 207/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 208/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 209/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 210/300, seasonal_1 Loss: 0.0560 | 0.0254
Epoch 211/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 212/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 213/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 214/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 215/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 216/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 217/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 218/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 219/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 220/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 221/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 222/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 223/300, seasonal_1 Loss: 0.0559 | 0.0254
Epoch 224/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 225/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 226/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 227/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 228/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 229/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 230/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 231/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 232/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 233/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 234/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 235/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 236/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 237/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 238/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 239/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 240/300, seasonal_1 Loss: 0.0558 | 0.0254
Epoch 241/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 242/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 243/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 244/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 245/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 246/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 247/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 248/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 249/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 250/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 251/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 252/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 253/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 254/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 255/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 256/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 257/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 258/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 259/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 260/300, seasonal_1 Loss: 0.0557 | 0.0254
Epoch 261/300, seasonal_1 Loss: 0.0557 | 0.0253
Epoch 262/300, seasonal_1 Loss: 0.0557 | 0.0253
Epoch 263/300, seasonal_1 Loss: 0.0557 | 0.0253
Epoch 264/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 265/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 266/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 267/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 268/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 269/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 270/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 271/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 272/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 273/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 274/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 275/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 276/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 277/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 278/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 279/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 280/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 281/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 282/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 283/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 284/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 285/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 286/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 287/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 288/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 289/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 290/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 291/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 292/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 293/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 294/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 295/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 296/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 297/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 298/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 299/300, seasonal_1 Loss: 0.0556 | 0.0253
Epoch 300/300, seasonal_1 Loss: 0.0556 | 0.0253
Training seasonal_2 component with params: {'observation_period_num': 16, 'train_rates': 0.9233146624328904, 'learning_rate': 0.0006008585134447917, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8244459280559769}
Epoch 1/300, seasonal_2 Loss: 0.2959 | 0.1289
Epoch 2/300, seasonal_2 Loss: 0.1498 | 0.0835
Epoch 3/300, seasonal_2 Loss: 0.1248 | 0.0810
Epoch 4/300, seasonal_2 Loss: 0.1133 | 0.0654
Epoch 5/300, seasonal_2 Loss: 0.1181 | 0.0739
Epoch 6/300, seasonal_2 Loss: 0.1084 | 0.0668
Epoch 7/300, seasonal_2 Loss: 0.1308 | 0.0763
Epoch 8/300, seasonal_2 Loss: 0.1099 | 0.0591
Epoch 9/300, seasonal_2 Loss: 0.0988 | 0.0546
Epoch 10/300, seasonal_2 Loss: 0.0994 | 0.0498
Epoch 11/300, seasonal_2 Loss: 0.0940 | 0.0516
Epoch 12/300, seasonal_2 Loss: 0.0967 | 0.0549
Epoch 13/300, seasonal_2 Loss: 0.0990 | 0.0510
Epoch 14/300, seasonal_2 Loss: 0.0940 | 0.0543
Epoch 15/300, seasonal_2 Loss: 0.0919 | 0.0456
Epoch 16/300, seasonal_2 Loss: 0.0925 | 0.0437
Epoch 17/300, seasonal_2 Loss: 0.0883 | 0.0410
Epoch 18/300, seasonal_2 Loss: 0.0882 | 0.0518
Epoch 19/300, seasonal_2 Loss: 0.0823 | 0.0372
Epoch 20/300, seasonal_2 Loss: 0.0856 | 0.0412
Epoch 21/300, seasonal_2 Loss: 0.0874 | 0.0356
Epoch 22/300, seasonal_2 Loss: 0.0784 | 0.0362
Epoch 23/300, seasonal_2 Loss: 0.0751 | 0.0328
Epoch 24/300, seasonal_2 Loss: 0.0730 | 0.0342
Epoch 25/300, seasonal_2 Loss: 0.0721 | 0.0330
Epoch 26/300, seasonal_2 Loss: 0.0700 | 0.0327
Epoch 27/300, seasonal_2 Loss: 0.0691 | 0.0312
Epoch 28/300, seasonal_2 Loss: 0.0684 | 0.0325
Epoch 29/300, seasonal_2 Loss: 0.0677 | 0.0326
Epoch 30/300, seasonal_2 Loss: 0.0679 | 0.0375
Epoch 31/300, seasonal_2 Loss: 0.0684 | 0.0297
Epoch 32/300, seasonal_2 Loss: 0.0687 | 0.0283
Epoch 33/300, seasonal_2 Loss: 0.0682 | 0.0293
Epoch 34/300, seasonal_2 Loss: 0.0671 | 0.0304
Epoch 35/300, seasonal_2 Loss: 0.0659 | 0.0314
Epoch 36/300, seasonal_2 Loss: 0.0652 | 0.0285
Epoch 37/300, seasonal_2 Loss: 0.0642 | 0.0282
Epoch 38/300, seasonal_2 Loss: 0.0630 | 0.0272
Epoch 39/300, seasonal_2 Loss: 0.0622 | 0.0273
Epoch 40/300, seasonal_2 Loss: 0.0618 | 0.0264
Epoch 41/300, seasonal_2 Loss: 0.0613 | 0.0267
Epoch 42/300, seasonal_2 Loss: 0.0601 | 0.0257
Epoch 43/300, seasonal_2 Loss: 0.0596 | 0.0281
Epoch 44/300, seasonal_2 Loss: 0.0593 | 0.0281
Epoch 45/300, seasonal_2 Loss: 0.0598 | 0.0269
Epoch 46/300, seasonal_2 Loss: 0.0624 | 0.0294
Epoch 47/300, seasonal_2 Loss: 0.0610 | 0.0266
Epoch 48/300, seasonal_2 Loss: 0.0598 | 0.0268
Epoch 49/300, seasonal_2 Loss: 0.0603 | 0.0284
Epoch 50/300, seasonal_2 Loss: 0.0603 | 0.0282
Epoch 51/300, seasonal_2 Loss: 0.0585 | 0.0268
Epoch 52/300, seasonal_2 Loss: 0.0591 | 0.0271
Epoch 53/300, seasonal_2 Loss: 0.0596 | 0.0321
Epoch 54/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 55/300, seasonal_2 Loss: 0.0592 | 0.0273
Epoch 56/300, seasonal_2 Loss: 0.0586 | 0.0280
Epoch 57/300, seasonal_2 Loss: 0.0579 | 0.0278
Epoch 58/300, seasonal_2 Loss: 0.0576 | 0.0276
Epoch 59/300, seasonal_2 Loss: 0.0575 | 0.0280
Epoch 60/300, seasonal_2 Loss: 0.0578 | 0.0303
Epoch 61/300, seasonal_2 Loss: 0.0586 | 0.0294
Epoch 62/300, seasonal_2 Loss: 0.0607 | 0.0303
Epoch 63/300, seasonal_2 Loss: 0.0603 | 0.0338
Epoch 64/300, seasonal_2 Loss: 0.0594 | 0.0313
Epoch 65/300, seasonal_2 Loss: 0.0578 | 0.0292
Epoch 66/300, seasonal_2 Loss: 0.0591 | 0.0298
Epoch 67/300, seasonal_2 Loss: 0.0582 | 0.0314
Epoch 68/300, seasonal_2 Loss: 0.0624 | 0.0300
Epoch 69/300, seasonal_2 Loss: 0.0595 | 0.0318
Epoch 70/300, seasonal_2 Loss: 0.0641 | 0.0415
Epoch 71/300, seasonal_2 Loss: 0.0607 | 0.0376
Epoch 72/300, seasonal_2 Loss: 0.0636 | 0.0522
Epoch 73/300, seasonal_2 Loss: 0.0616 | 0.0458
Epoch 74/300, seasonal_2 Loss: 0.0620 | 0.0473
Epoch 75/300, seasonal_2 Loss: 0.0606 | 0.0395
Epoch 76/300, seasonal_2 Loss: 0.0621 | 0.0298
Epoch 77/300, seasonal_2 Loss: 0.0617 | 0.0285
Epoch 78/300, seasonal_2 Loss: 0.0582 | 0.0307
Epoch 79/300, seasonal_2 Loss: 0.0551 | 0.0303
Epoch 80/300, seasonal_2 Loss: 0.0540 | 0.0281
Epoch 81/300, seasonal_2 Loss: 0.0539 | 0.0282
Epoch 82/300, seasonal_2 Loss: 0.0538 | 0.0277
Epoch 83/300, seasonal_2 Loss: 0.0540 | 0.0277
Epoch 84/300, seasonal_2 Loss: 0.0543 | 0.0274
Epoch 85/300, seasonal_2 Loss: 0.0549 | 0.0275
Epoch 86/300, seasonal_2 Loss: 0.0546 | 0.0274
Epoch 87/300, seasonal_2 Loss: 0.0542 | 0.0276
Epoch 88/300, seasonal_2 Loss: 0.0537 | 0.0276
Epoch 89/300, seasonal_2 Loss: 0.0533 | 0.0276
Epoch 90/300, seasonal_2 Loss: 0.0530 | 0.0274
Epoch 91/300, seasonal_2 Loss: 0.0529 | 0.0268
Epoch 92/300, seasonal_2 Loss: 0.0528 | 0.0269
Epoch 93/300, seasonal_2 Loss: 0.0524 | 0.0270
Epoch 94/300, seasonal_2 Loss: 0.0522 | 0.0270
Epoch 95/300, seasonal_2 Loss: 0.0519 | 0.0269
Epoch 96/300, seasonal_2 Loss: 0.0517 | 0.0268
Epoch 97/300, seasonal_2 Loss: 0.0516 | 0.0267
Epoch 98/300, seasonal_2 Loss: 0.0514 | 0.0267
Epoch 99/300, seasonal_2 Loss: 0.0513 | 0.0261
Epoch 100/300, seasonal_2 Loss: 0.0512 | 0.0261
Epoch 101/300, seasonal_2 Loss: 0.0512 | 0.0261
Epoch 102/300, seasonal_2 Loss: 0.0512 | 0.0261
Epoch 103/300, seasonal_2 Loss: 0.0512 | 0.0261
Epoch 104/300, seasonal_2 Loss: 0.0512 | 0.0261
Epoch 105/300, seasonal_2 Loss: 0.0512 | 0.0261
Epoch 106/300, seasonal_2 Loss: 0.0513 | 0.0259
Epoch 107/300, seasonal_2 Loss: 0.0514 | 0.0260
Epoch 108/300, seasonal_2 Loss: 0.0513 | 0.0259
Epoch 109/300, seasonal_2 Loss: 0.0512 | 0.0257
Epoch 110/300, seasonal_2 Loss: 0.0510 | 0.0256
Epoch 111/300, seasonal_2 Loss: 0.0508 | 0.0255
Epoch 112/300, seasonal_2 Loss: 0.0506 | 0.0254
Epoch 113/300, seasonal_2 Loss: 0.0506 | 0.0253
Epoch 114/300, seasonal_2 Loss: 0.0507 | 0.0256
Epoch 115/300, seasonal_2 Loss: 0.0511 | 0.0260
Epoch 116/300, seasonal_2 Loss: 0.0521 | 0.0275
Epoch 117/300, seasonal_2 Loss: 0.0535 | 0.0271
Epoch 118/300, seasonal_2 Loss: 0.0519 | 0.0272
Epoch 119/300, seasonal_2 Loss: 0.0508 | 0.0270
Epoch 120/300, seasonal_2 Loss: 0.0505 | 0.0269
Epoch 121/300, seasonal_2 Loss: 0.0504 | 0.0272
Epoch 122/300, seasonal_2 Loss: 0.0504 | 0.0272
Epoch 123/300, seasonal_2 Loss: 0.0501 | 0.0272
Epoch 124/300, seasonal_2 Loss: 0.0499 | 0.0272
Epoch 125/300, seasonal_2 Loss: 0.0498 | 0.0273
Epoch 126/300, seasonal_2 Loss: 0.0497 | 0.0273
Epoch 127/300, seasonal_2 Loss: 0.0498 | 0.0273
Epoch 128/300, seasonal_2 Loss: 0.0499 | 0.0274
Epoch 129/300, seasonal_2 Loss: 0.0501 | 0.0264
Epoch 130/300, seasonal_2 Loss: 0.0506 | 0.0260
Epoch 131/300, seasonal_2 Loss: 0.0506 | 0.0256
Epoch 132/300, seasonal_2 Loss: 0.0502 | 0.0254
Epoch 133/300, seasonal_2 Loss: 0.0500 | 0.0252
Epoch 134/300, seasonal_2 Loss: 0.0497 | 0.0255
Epoch 135/300, seasonal_2 Loss: 0.0493 | 0.0260
Epoch 136/300, seasonal_2 Loss: 0.0490 | 0.0266
Epoch 137/300, seasonal_2 Loss: 0.0490 | 0.0267
Epoch 138/300, seasonal_2 Loss: 0.0489 | 0.0267
Epoch 139/300, seasonal_2 Loss: 0.0488 | 0.0266
Epoch 140/300, seasonal_2 Loss: 0.0488 | 0.0265
Epoch 141/300, seasonal_2 Loss: 0.0487 | 0.0265
Epoch 142/300, seasonal_2 Loss: 0.0486 | 0.0264
Epoch 143/300, seasonal_2 Loss: 0.0486 | 0.0264
Epoch 144/300, seasonal_2 Loss: 0.0485 | 0.0262
Epoch 145/300, seasonal_2 Loss: 0.0485 | 0.0262
Epoch 146/300, seasonal_2 Loss: 0.0485 | 0.0263
Epoch 147/300, seasonal_2 Loss: 0.0484 | 0.0263
Epoch 148/300, seasonal_2 Loss: 0.0484 | 0.0264
Epoch 149/300, seasonal_2 Loss: 0.0483 | 0.0264
Epoch 150/300, seasonal_2 Loss: 0.0483 | 0.0265
Epoch 151/300, seasonal_2 Loss: 0.0483 | 0.0265
Epoch 152/300, seasonal_2 Loss: 0.0482 | 0.0265
Epoch 153/300, seasonal_2 Loss: 0.0482 | 0.0265
Epoch 154/300, seasonal_2 Loss: 0.0482 | 0.0265
Epoch 155/300, seasonal_2 Loss: 0.0482 | 0.0265
Epoch 156/300, seasonal_2 Loss: 0.0481 | 0.0265
Epoch 157/300, seasonal_2 Loss: 0.0481 | 0.0265
Epoch 158/300, seasonal_2 Loss: 0.0481 | 0.0265
Epoch 159/300, seasonal_2 Loss: 0.0480 | 0.0265
Epoch 160/300, seasonal_2 Loss: 0.0480 | 0.0265
Epoch 161/300, seasonal_2 Loss: 0.0480 | 0.0266
Epoch 162/300, seasonal_2 Loss: 0.0480 | 0.0266
Epoch 163/300, seasonal_2 Loss: 0.0480 | 0.0266
Epoch 164/300, seasonal_2 Loss: 0.0479 | 0.0266
Epoch 165/300, seasonal_2 Loss: 0.0479 | 0.0266
Epoch 166/300, seasonal_2 Loss: 0.0479 | 0.0266
Epoch 167/300, seasonal_2 Loss: 0.0479 | 0.0266
Epoch 168/300, seasonal_2 Loss: 0.0478 | 0.0266
Epoch 169/300, seasonal_2 Loss: 0.0478 | 0.0266
Epoch 170/300, seasonal_2 Loss: 0.0478 | 0.0266
Epoch 171/300, seasonal_2 Loss: 0.0478 | 0.0266
Epoch 172/300, seasonal_2 Loss: 0.0478 | 0.0266
Epoch 173/300, seasonal_2 Loss: 0.0477 | 0.0266
Epoch 174/300, seasonal_2 Loss: 0.0477 | 0.0266
Epoch 175/300, seasonal_2 Loss: 0.0477 | 0.0266
Epoch 176/300, seasonal_2 Loss: 0.0477 | 0.0267
Epoch 177/300, seasonal_2 Loss: 0.0477 | 0.0267
Epoch 178/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 179/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 180/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 181/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 182/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 183/300, seasonal_2 Loss: 0.0476 | 0.0267
Epoch 184/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 185/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 186/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 187/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 188/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 189/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 190/300, seasonal_2 Loss: 0.0475 | 0.0267
Epoch 191/300, seasonal_2 Loss: 0.0474 | 0.0267
Epoch 192/300, seasonal_2 Loss: 0.0474 | 0.0267
Epoch 193/300, seasonal_2 Loss: 0.0474 | 0.0268
Epoch 194/300, seasonal_2 Loss: 0.0474 | 0.0268
Epoch 195/300, seasonal_2 Loss: 0.0474 | 0.0268
Epoch 196/300, seasonal_2 Loss: 0.0474 | 0.0268
Epoch 197/300, seasonal_2 Loss: 0.0474 | 0.0268
Epoch 198/300, seasonal_2 Loss: 0.0474 | 0.0268
Epoch 199/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 200/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 201/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 202/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 203/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 204/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 205/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 206/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 207/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 208/300, seasonal_2 Loss: 0.0473 | 0.0268
Epoch 209/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 210/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 211/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 212/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 213/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 214/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 215/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 216/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 217/300, seasonal_2 Loss: 0.0472 | 0.0268
Epoch 218/300, seasonal_2 Loss: 0.0472 | 0.0269
Epoch 219/300, seasonal_2 Loss: 0.0472 | 0.0269
Epoch 220/300, seasonal_2 Loss: 0.0472 | 0.0269
Epoch 221/300, seasonal_2 Loss: 0.0472 | 0.0269
Epoch 222/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 223/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 224/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 225/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 226/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 227/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 228/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 229/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 230/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 231/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 232/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 233/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 234/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 235/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 236/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 237/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 238/300, seasonal_2 Loss: 0.0471 | 0.0269
Epoch 239/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 240/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 241/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 242/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 243/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 244/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 245/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 246/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 247/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 248/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 249/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 250/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 251/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 252/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 253/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 254/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 255/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 256/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 257/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 258/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 259/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 260/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 261/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 262/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 263/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 264/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 265/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 266/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 267/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 268/300, seasonal_2 Loss: 0.0470 | 0.0269
Epoch 269/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 270/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 271/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 272/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 273/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 274/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 275/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 276/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 277/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 278/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 279/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 280/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 281/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 282/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 283/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 284/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 285/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 286/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 287/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 288/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 289/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 290/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 291/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 292/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 293/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 294/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 295/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 296/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 297/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 298/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 299/300, seasonal_2 Loss: 0.0469 | 0.0270
Epoch 300/300, seasonal_2 Loss: 0.0469 | 0.0270
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.9018374957952444, 'learning_rate': 0.00021313735450778504, 'batch_size': 18, 'step_size': 14, 'gamma': 0.9491112627636014}
Epoch 1/300, seasonal_3 Loss: 0.2129 | 0.1055
Epoch 2/300, seasonal_3 Loss: 0.1254 | 0.0834
Epoch 3/300, seasonal_3 Loss: 0.1168 | 0.0707
Epoch 4/300, seasonal_3 Loss: 0.1135 | 0.0696
Epoch 5/300, seasonal_3 Loss: 0.1100 | 0.0699
Epoch 6/300, seasonal_3 Loss: 0.1033 | 0.0666
Epoch 7/300, seasonal_3 Loss: 0.0972 | 0.0652
Epoch 8/300, seasonal_3 Loss: 0.0926 | 0.0607
Epoch 9/300, seasonal_3 Loss: 0.0895 | 0.0574
Epoch 10/300, seasonal_3 Loss: 0.0865 | 0.0534
Epoch 11/300, seasonal_3 Loss: 0.0839 | 0.0491
Epoch 12/300, seasonal_3 Loss: 0.0812 | 0.0456
Epoch 13/300, seasonal_3 Loss: 0.0783 | 0.0422
Epoch 14/300, seasonal_3 Loss: 0.0757 | 0.0384
Epoch 15/300, seasonal_3 Loss: 0.0739 | 0.0370
Epoch 16/300, seasonal_3 Loss: 0.0723 | 0.0352
Epoch 17/300, seasonal_3 Loss: 0.0711 | 0.0358
Epoch 18/300, seasonal_3 Loss: 0.0696 | 0.0335
Epoch 19/300, seasonal_3 Loss: 0.0680 | 0.0329
Epoch 20/300, seasonal_3 Loss: 0.0659 | 0.0304
Epoch 21/300, seasonal_3 Loss: 0.0644 | 0.0297
Epoch 22/300, seasonal_3 Loss: 0.0630 | 0.0289
Epoch 23/300, seasonal_3 Loss: 0.0622 | 0.0304
Epoch 24/300, seasonal_3 Loss: 0.0621 | 0.0296
Epoch 25/300, seasonal_3 Loss: 0.0614 | 0.0298
Epoch 26/300, seasonal_3 Loss: 0.0612 | 0.0305
Epoch 27/300, seasonal_3 Loss: 0.0608 | 0.0306
Epoch 28/300, seasonal_3 Loss: 0.0597 | 0.0303
Epoch 29/300, seasonal_3 Loss: 0.0594 | 0.0316
Epoch 30/300, seasonal_3 Loss: 0.0588 | 0.0323
Epoch 31/300, seasonal_3 Loss: 0.0583 | 0.0331
Epoch 32/300, seasonal_3 Loss: 0.0574 | 0.0348
Epoch 33/300, seasonal_3 Loss: 0.0563 | 0.0349
Epoch 34/300, seasonal_3 Loss: 0.0553 | 0.0345
Epoch 35/300, seasonal_3 Loss: 0.0548 | 0.0341
Epoch 36/300, seasonal_3 Loss: 0.0544 | 0.0348
Epoch 37/300, seasonal_3 Loss: 0.0561 | 0.0375
Epoch 38/300, seasonal_3 Loss: 0.0545 | 0.0398
Epoch 39/300, seasonal_3 Loss: 0.0537 | 0.0414
Epoch 40/300, seasonal_3 Loss: 0.0536 | 0.0374
Epoch 41/300, seasonal_3 Loss: 0.0532 | 0.0398
Epoch 42/300, seasonal_3 Loss: 0.0512 | 0.0376
Epoch 43/300, seasonal_3 Loss: 0.0506 | 0.0365
Epoch 44/300, seasonal_3 Loss: 0.0495 | 0.0344
Epoch 45/300, seasonal_3 Loss: 0.0491 | 0.0355
Epoch 46/300, seasonal_3 Loss: 0.0489 | 0.0362
Epoch 47/300, seasonal_3 Loss: 0.0479 | 0.0354
Epoch 48/300, seasonal_3 Loss: 0.0492 | 0.0381
Epoch 49/300, seasonal_3 Loss: 0.0474 | 0.0367
Epoch 50/300, seasonal_3 Loss: 0.0471 | 0.0380
Epoch 51/300, seasonal_3 Loss: 0.0460 | 0.0383
Epoch 52/300, seasonal_3 Loss: 0.0454 | 0.0397
Epoch 53/300, seasonal_3 Loss: 0.0457 | 0.0393
Epoch 54/300, seasonal_3 Loss: 0.0473 | 0.0457
Epoch 55/300, seasonal_3 Loss: 0.0470 | 0.0396
Epoch 56/300, seasonal_3 Loss: 0.0464 | 0.0434
Epoch 57/300, seasonal_3 Loss: 0.0444 | 0.0394
Epoch 58/300, seasonal_3 Loss: 0.0439 | 0.0475
Epoch 59/300, seasonal_3 Loss: 0.0471 | 0.0363
Epoch 60/300, seasonal_3 Loss: 0.0437 | 0.0438
Epoch 61/300, seasonal_3 Loss: 0.0432 | 0.0396
Epoch 62/300, seasonal_3 Loss: 0.0472 | 0.0383
Epoch 63/300, seasonal_3 Loss: 0.0439 | 0.0364
Epoch 64/300, seasonal_3 Loss: 0.0433 | 0.0366
Epoch 65/300, seasonal_3 Loss: 0.0428 | 0.0368
Epoch 66/300, seasonal_3 Loss: 0.0437 | 0.0356
Epoch 67/300, seasonal_3 Loss: 0.0414 | 0.0366
Epoch 68/300, seasonal_3 Loss: 0.0365 | 0.0377
Epoch 69/300, seasonal_3 Loss: 0.0380 | 0.0371
Epoch 70/300, seasonal_3 Loss: 0.0411 | 0.0435
Epoch 71/300, seasonal_3 Loss: 0.0404 | 0.0363
Epoch 72/300, seasonal_3 Loss: 0.0421 | 0.0349
Epoch 73/300, seasonal_3 Loss: 0.0370 | 0.0345
Epoch 74/300, seasonal_3 Loss: 0.0382 | 0.0451
Epoch 75/300, seasonal_3 Loss: 0.0389 | 0.0386
Epoch 76/300, seasonal_3 Loss: 0.0459 | 0.0388
Epoch 77/300, seasonal_3 Loss: 0.0410 | 0.0401
Epoch 78/300, seasonal_3 Loss: 0.0393 | 0.0442
Epoch 79/300, seasonal_3 Loss: 0.0356 | 0.0348
Epoch 80/300, seasonal_3 Loss: 0.0410 | 0.0360
Epoch 81/300, seasonal_3 Loss: 0.0340 | 0.0372
Epoch 82/300, seasonal_3 Loss: 0.0323 | 0.0360
Epoch 83/300, seasonal_3 Loss: 0.0314 | 0.0366
Epoch 84/300, seasonal_3 Loss: 0.0309 | 0.0371
Epoch 85/300, seasonal_3 Loss: 0.0309 | 0.0371
Epoch 86/300, seasonal_3 Loss: 0.0308 | 0.0363
Epoch 87/300, seasonal_3 Loss: 0.0386 | 0.0416
Epoch 88/300, seasonal_3 Loss: 0.0356 | 0.0367
Epoch 89/300, seasonal_3 Loss: 0.0324 | 0.0542
Epoch 90/300, seasonal_3 Loss: 0.0361 | 0.0363
Epoch 91/300, seasonal_3 Loss: 0.0319 | 0.0358
Epoch 92/300, seasonal_3 Loss: 0.0316 | 0.0427
Epoch 93/300, seasonal_3 Loss: 0.0303 | 0.0364
Epoch 94/300, seasonal_3 Loss: 0.0300 | 0.0384
Epoch 95/300, seasonal_3 Loss: 0.0295 | 0.0364
Epoch 96/300, seasonal_3 Loss: 0.0289 | 0.0379
Epoch 97/300, seasonal_3 Loss: 0.0288 | 0.0394
Epoch 98/300, seasonal_3 Loss: 0.0289 | 0.0380
Epoch 99/300, seasonal_3 Loss: 0.0297 | 0.0406
Epoch 100/300, seasonal_3 Loss: 0.0311 | 0.0380
Epoch 101/300, seasonal_3 Loss: 0.0308 | 0.0378
Epoch 102/300, seasonal_3 Loss: 0.0309 | 0.0396
Epoch 103/300, seasonal_3 Loss: 0.0351 | 0.0652
Epoch 104/300, seasonal_3 Loss: 0.0336 | 0.0375
Epoch 105/300, seasonal_3 Loss: 0.0303 | 0.0394
Epoch 106/300, seasonal_3 Loss: 0.0285 | 0.0378
Epoch 107/300, seasonal_3 Loss: 0.0279 | 0.0397
Epoch 108/300, seasonal_3 Loss: 0.0272 | 0.0378
Epoch 109/300, seasonal_3 Loss: 0.0270 | 0.0388
Epoch 110/300, seasonal_3 Loss: 0.0267 | 0.0381
Epoch 111/300, seasonal_3 Loss: 0.0265 | 0.0393
Epoch 112/300, seasonal_3 Loss: 0.0263 | 0.0386
Epoch 113/300, seasonal_3 Loss: 0.0261 | 0.0403
Epoch 114/300, seasonal_3 Loss: 0.0259 | 0.0408
Epoch 115/300, seasonal_3 Loss: 0.0258 | 0.0416
Epoch 116/300, seasonal_3 Loss: 0.0256 | 0.0429
Epoch 117/300, seasonal_3 Loss: 0.0256 | 0.0414
Epoch 118/300, seasonal_3 Loss: 0.0255 | 0.0456
Epoch 119/300, seasonal_3 Loss: 0.0254 | 0.0411
Epoch 120/300, seasonal_3 Loss: 0.0251 | 0.0473
Epoch 121/300, seasonal_3 Loss: 0.0252 | 0.0477
Epoch 122/300, seasonal_3 Loss: 0.0251 | 0.0489
Epoch 123/300, seasonal_3 Loss: 0.0251 | 0.0428
Epoch 124/300, seasonal_3 Loss: 0.0254 | 0.0421
Epoch 125/300, seasonal_3 Loss: 0.0257 | 0.0405
Epoch 126/300, seasonal_3 Loss: 0.0255 | 0.0396
Epoch 127/300, seasonal_3 Loss: 0.0253 | 0.0389
Epoch 128/300, seasonal_3 Loss: 0.0249 | 0.0383
Epoch 129/300, seasonal_3 Loss: 0.0251 | 0.0401
Epoch 130/300, seasonal_3 Loss: 0.0249 | 0.0388
Epoch 131/300, seasonal_3 Loss: 0.0246 | 0.0390
Epoch 132/300, seasonal_3 Loss: 0.0242 | 0.0377
Epoch 133/300, seasonal_3 Loss: 0.0243 | 0.0395
Epoch 134/300, seasonal_3 Loss: 0.0236 | 0.0381
Epoch 135/300, seasonal_3 Loss: 0.0233 | 0.0386
Epoch 136/300, seasonal_3 Loss: 0.0231 | 0.0378
Epoch 137/300, seasonal_3 Loss: 0.0236 | 0.0400
Epoch 138/300, seasonal_3 Loss: 0.0229 | 0.0387
Epoch 139/300, seasonal_3 Loss: 0.0230 | 0.0393
Epoch 140/300, seasonal_3 Loss: 0.0228 | 0.0382
Epoch 141/300, seasonal_3 Loss: 0.0227 | 0.0393
Epoch 142/300, seasonal_3 Loss: 0.0226 | 0.0389
Epoch 143/300, seasonal_3 Loss: 0.0224 | 0.0392
Epoch 144/300, seasonal_3 Loss: 0.0224 | 0.0392
Epoch 145/300, seasonal_3 Loss: 0.0228 | 0.0402
Epoch 146/300, seasonal_3 Loss: 0.0225 | 0.0417
Epoch 147/300, seasonal_3 Loss: 0.0222 | 0.0404
Epoch 148/300, seasonal_3 Loss: 0.0222 | 0.0406
Epoch 149/300, seasonal_3 Loss: 0.0222 | 0.0418
Epoch 150/300, seasonal_3 Loss: 0.0219 | 0.0424
Epoch 151/300, seasonal_3 Loss: 0.0217 | 0.0408
Epoch 152/300, seasonal_3 Loss: 0.0215 | 0.0437
Epoch 153/300, seasonal_3 Loss: 0.0216 | 0.0394
Epoch 154/300, seasonal_3 Loss: 0.0214 | 0.0417
Epoch 155/300, seasonal_3 Loss: 0.0214 | 0.0389
Epoch 156/300, seasonal_3 Loss: 0.0213 | 0.0394
Epoch 157/300, seasonal_3 Loss: 0.0213 | 0.0415
Epoch 158/300, seasonal_3 Loss: 0.0270 | 0.0423
Epoch 159/300, seasonal_3 Loss: 0.0286 | 0.0402
Epoch 160/300, seasonal_3 Loss: 0.0240 | 0.0395
Epoch 161/300, seasonal_3 Loss: 0.0218 | 0.0393
Epoch 162/300, seasonal_3 Loss: 0.0210 | 0.0395
Epoch 163/300, seasonal_3 Loss: 0.0207 | 0.0393
Epoch 164/300, seasonal_3 Loss: 0.0205 | 0.0391
Epoch 165/300, seasonal_3 Loss: 0.0203 | 0.0388
Epoch 166/300, seasonal_3 Loss: 0.0201 | 0.0386
Epoch 167/300, seasonal_3 Loss: 0.0199 | 0.0384
Epoch 168/300, seasonal_3 Loss: 0.0198 | 0.0383
Epoch 169/300, seasonal_3 Loss: 0.0197 | 0.0384
Epoch 170/300, seasonal_3 Loss: 0.0196 | 0.0383
Epoch 171/300, seasonal_3 Loss: 0.0195 | 0.0382
Epoch 172/300, seasonal_3 Loss: 0.0193 | 0.0382
Epoch 173/300, seasonal_3 Loss: 0.0192 | 0.0382
Epoch 174/300, seasonal_3 Loss: 0.0191 | 0.0382
Epoch 175/300, seasonal_3 Loss: 0.0190 | 0.0382
Epoch 176/300, seasonal_3 Loss: 0.0189 | 0.0384
Epoch 177/300, seasonal_3 Loss: 0.0189 | 0.0386
Epoch 178/300, seasonal_3 Loss: 0.0188 | 0.0385
Epoch 179/300, seasonal_3 Loss: 0.0187 | 0.0385
Epoch 180/300, seasonal_3 Loss: 0.0187 | 0.0388
Epoch 181/300, seasonal_3 Loss: 0.0188 | 0.0393
Epoch 182/300, seasonal_3 Loss: 0.0190 | 0.0398
Epoch 183/300, seasonal_3 Loss: 0.0193 | 0.0401
Epoch 184/300, seasonal_3 Loss: 0.0192 | 0.0413
Epoch 185/300, seasonal_3 Loss: 0.0192 | 0.0406
Epoch 186/300, seasonal_3 Loss: 0.0189 | 0.0396
Epoch 187/300, seasonal_3 Loss: 0.0184 | 0.0397
Epoch 188/300, seasonal_3 Loss: 0.0185 | 0.0389
Epoch 189/300, seasonal_3 Loss: 0.0183 | 0.0389
Epoch 190/300, seasonal_3 Loss: 0.0184 | 0.0389
Epoch 191/300, seasonal_3 Loss: 0.0181 | 0.0393
Epoch 192/300, seasonal_3 Loss: 0.0181 | 0.0400
Epoch 193/300, seasonal_3 Loss: 0.0180 | 0.0391
Epoch 194/300, seasonal_3 Loss: 0.0179 | 0.0403
Epoch 195/300, seasonal_3 Loss: 0.0178 | 0.0395
Epoch 196/300, seasonal_3 Loss: 0.0179 | 0.0401
Epoch 197/300, seasonal_3 Loss: 0.0178 | 0.0395
Epoch 198/300, seasonal_3 Loss: 0.0179 | 0.0407
Epoch 199/300, seasonal_3 Loss: 0.0181 | 0.0391
Epoch 200/300, seasonal_3 Loss: 0.0176 | 0.0404
Epoch 201/300, seasonal_3 Loss: 0.0174 | 0.0400
Epoch 202/300, seasonal_3 Loss: 0.0172 | 0.0408
Epoch 203/300, seasonal_3 Loss: 0.0172 | 0.0403
Epoch 204/300, seasonal_3 Loss: 0.0172 | 0.0410
Epoch 205/300, seasonal_3 Loss: 0.0171 | 0.0418
Epoch 206/300, seasonal_3 Loss: 0.0170 | 0.0418
Epoch 207/300, seasonal_3 Loss: 0.0170 | 0.0427
Epoch 208/300, seasonal_3 Loss: 0.0170 | 0.0435
Epoch 209/300, seasonal_3 Loss: 0.0170 | 0.0424
Epoch 210/300, seasonal_3 Loss: 0.0170 | 0.0438
Epoch 211/300, seasonal_3 Loss: 0.0169 | 0.0425
Epoch 212/300, seasonal_3 Loss: 0.0168 | 0.0420
Epoch 213/300, seasonal_3 Loss: 0.0170 | 0.0420
Epoch 214/300, seasonal_3 Loss: 0.0169 | 0.0415
Epoch 215/300, seasonal_3 Loss: 0.0169 | 0.0406
Epoch 216/300, seasonal_3 Loss: 0.0167 | 0.0410
Epoch 217/300, seasonal_3 Loss: 0.0166 | 0.0415
Epoch 218/300, seasonal_3 Loss: 0.0165 | 0.0412
Epoch 219/300, seasonal_3 Loss: 0.0164 | 0.0408
Epoch 220/300, seasonal_3 Loss: 0.0163 | 0.0411
Epoch 221/300, seasonal_3 Loss: 0.0161 | 0.0412
Epoch 222/300, seasonal_3 Loss: 0.0160 | 0.0410
Epoch 223/300, seasonal_3 Loss: 0.0160 | 0.0421
Epoch 224/300, seasonal_3 Loss: 0.0161 | 0.0414
Epoch 225/300, seasonal_3 Loss: 0.0160 | 0.0420
Epoch 226/300, seasonal_3 Loss: 0.0160 | 0.0404
Epoch 227/300, seasonal_3 Loss: 0.0159 | 0.0413
Epoch 228/300, seasonal_3 Loss: 0.0159 | 0.0406
Epoch 229/300, seasonal_3 Loss: 0.0158 | 0.0405
Epoch 230/300, seasonal_3 Loss: 0.0157 | 0.0407
Epoch 231/300, seasonal_3 Loss: 0.0157 | 0.0407
Epoch 232/300, seasonal_3 Loss: 0.0156 | 0.0412
Epoch 233/300, seasonal_3 Loss: 0.0156 | 0.0403
Epoch 234/300, seasonal_3 Loss: 0.0157 | 0.0413
Epoch 235/300, seasonal_3 Loss: 0.0156 | 0.0396
Epoch 236/300, seasonal_3 Loss: 0.0154 | 0.0398
Epoch 237/300, seasonal_3 Loss: 0.0153 | 0.0395
Epoch 238/300, seasonal_3 Loss: 0.0152 | 0.0398
Epoch 239/300, seasonal_3 Loss: 0.0153 | 0.0394
Epoch 240/300, seasonal_3 Loss: 0.0152 | 0.0395
Epoch 241/300, seasonal_3 Loss: 0.0152 | 0.0404
Epoch 242/300, seasonal_3 Loss: 0.0151 | 0.0396
Epoch 243/300, seasonal_3 Loss: 0.0150 | 0.0408
Epoch 244/300, seasonal_3 Loss: 0.0149 | 0.0413
Epoch 245/300, seasonal_3 Loss: 0.0149 | 0.0421
Epoch 246/300, seasonal_3 Loss: 0.0149 | 0.0424
Epoch 247/300, seasonal_3 Loss: 0.0150 | 0.0424
Epoch 248/300, seasonal_3 Loss: 0.0151 | 0.0426
Epoch 249/300, seasonal_3 Loss: 0.0149 | 0.0411
Epoch 250/300, seasonal_3 Loss: 0.0149 | 0.0410
Epoch 251/300, seasonal_3 Loss: 0.0149 | 0.0406
Epoch 252/300, seasonal_3 Loss: 0.0148 | 0.0400
Epoch 253/300, seasonal_3 Loss: 0.0148 | 0.0408
Epoch 254/300, seasonal_3 Loss: 0.0147 | 0.0405
Epoch 255/300, seasonal_3 Loss: 0.0146 | 0.0409
Epoch 256/300, seasonal_3 Loss: 0.0146 | 0.0406
Epoch 257/300, seasonal_3 Loss: 0.0146 | 0.0403
Epoch 258/300, seasonal_3 Loss: 0.0146 | 0.0410
Epoch 259/300, seasonal_3 Loss: 0.0146 | 0.0405
Epoch 260/300, seasonal_3 Loss: 0.0146 | 0.0402
Epoch 261/300, seasonal_3 Loss: 0.0147 | 0.0403
Epoch 262/300, seasonal_3 Loss: 0.0146 | 0.0405
Epoch 263/300, seasonal_3 Loss: 0.0146 | 0.0404
Epoch 264/300, seasonal_3 Loss: 0.0144 | 0.0407
Epoch 265/300, seasonal_3 Loss: 0.0143 | 0.0399
Epoch 266/300, seasonal_3 Loss: 0.0142 | 0.0405
Epoch 267/300, seasonal_3 Loss: 0.0141 | 0.0398
Epoch 268/300, seasonal_3 Loss: 0.0141 | 0.0404
Epoch 269/300, seasonal_3 Loss: 0.0140 | 0.0394
Epoch 270/300, seasonal_3 Loss: 0.0139 | 0.0397
Epoch 271/300, seasonal_3 Loss: 0.0138 | 0.0395
Epoch 272/300, seasonal_3 Loss: 0.0138 | 0.0395
Epoch 273/300, seasonal_3 Loss: 0.0137 | 0.0396
Epoch 274/300, seasonal_3 Loss: 0.0137 | 0.0391
Epoch 275/300, seasonal_3 Loss: 0.0137 | 0.0394
Epoch 276/300, seasonal_3 Loss: 0.0137 | 0.0396
Epoch 277/300, seasonal_3 Loss: 0.0137 | 0.0392
Epoch 278/300, seasonal_3 Loss: 0.0138 | 0.0389
Epoch 279/300, seasonal_3 Loss: 0.0137 | 0.0397
Epoch 280/300, seasonal_3 Loss: 0.0138 | 0.0397
Epoch 281/300, seasonal_3 Loss: 0.0137 | 0.0394
Epoch 282/300, seasonal_3 Loss: 0.0135 | 0.0394
Epoch 283/300, seasonal_3 Loss: 0.0134 | 0.0390
Epoch 284/300, seasonal_3 Loss: 0.0134 | 0.0395
Epoch 285/300, seasonal_3 Loss: 0.0133 | 0.0398
Epoch 286/300, seasonal_3 Loss: 0.0133 | 0.0396
Epoch 287/300, seasonal_3 Loss: 0.0133 | 0.0395
Epoch 288/300, seasonal_3 Loss: 0.0133 | 0.0396
Epoch 289/300, seasonal_3 Loss: 0.0133 | 0.0397
Epoch 290/300, seasonal_3 Loss: 0.0134 | 0.0393
Epoch 291/300, seasonal_3 Loss: 0.0133 | 0.0394
Epoch 292/300, seasonal_3 Loss: 0.0132 | 0.0396
Epoch 293/300, seasonal_3 Loss: 0.0132 | 0.0394
Epoch 294/300, seasonal_3 Loss: 0.0131 | 0.0394
Epoch 295/300, seasonal_3 Loss: 0.0131 | 0.0395
Epoch 296/300, seasonal_3 Loss: 0.0130 | 0.0397
Epoch 297/300, seasonal_3 Loss: 0.0130 | 0.0398
Epoch 298/300, seasonal_3 Loss: 0.0129 | 0.0398
Epoch 299/300, seasonal_3 Loss: 0.0129 | 0.0396
Epoch 300/300, seasonal_3 Loss: 0.0129 | 0.0401
Training resid component with params: {'observation_period_num': 19, 'train_rates': 0.9772529378914042, 'learning_rate': 0.000584695690122956, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8251483910992404}
Epoch 1/300, resid Loss: 0.5162 | 0.1863
Epoch 2/300, resid Loss: 0.1999 | 0.1482
Epoch 3/300, resid Loss: 0.2204 | 0.1624
Epoch 4/300, resid Loss: 0.2880 | 0.4525
Epoch 5/300, resid Loss: 0.2187 | 0.3437
Epoch 6/300, resid Loss: 0.1712 | 0.1201
Epoch 7/300, resid Loss: 0.1516 | 0.1438
Epoch 8/300, resid Loss: 0.1562 | 0.0961
Epoch 9/300, resid Loss: 0.1261 | 0.0940
Epoch 10/300, resid Loss: 0.1366 | 0.0875
Epoch 11/300, resid Loss: 0.1337 | 0.0766
Epoch 12/300, resid Loss: 0.1401 | 0.0820
Epoch 13/300, resid Loss: 0.1467 | 0.0864
Epoch 14/300, resid Loss: 0.1370 | 0.0804
Epoch 15/300, resid Loss: 0.1203 | 0.0894
Epoch 16/300, resid Loss: 0.2132 | 0.1216
Epoch 17/300, resid Loss: 0.1882 | 0.0893
Epoch 18/300, resid Loss: 0.1593 | 0.1624
Epoch 19/300, resid Loss: 0.1320 | 0.0788
Epoch 20/300, resid Loss: 0.1173 | 0.0752
Epoch 21/300, resid Loss: 0.1070 | 0.0686
Epoch 22/300, resid Loss: 0.1028 | 0.0819
Epoch 23/300, resid Loss: 0.1000 | 0.0642
Epoch 24/300, resid Loss: 0.0979 | 0.0643
Epoch 25/300, resid Loss: 0.0946 | 0.0615
Epoch 26/300, resid Loss: 0.0925 | 0.0601
Epoch 27/300, resid Loss: 0.0926 | 0.0624
Epoch 28/300, resid Loss: 0.0926 | 0.0650
Epoch 29/300, resid Loss: 0.0916 | 0.0644
Epoch 30/300, resid Loss: 0.0897 | 0.0606
Epoch 31/300, resid Loss: 0.0880 | 0.0567
Epoch 32/300, resid Loss: 0.0876 | 0.0562
Epoch 33/300, resid Loss: 0.0867 | 0.0553
Epoch 34/300, resid Loss: 0.0855 | 0.0546
Epoch 35/300, resid Loss: 0.0847 | 0.0556
Epoch 36/300, resid Loss: 0.0846 | 0.0579
Epoch 37/300, resid Loss: 0.0855 | 0.0568
Epoch 38/300, resid Loss: 0.0846 | 0.0549
Epoch 39/300, resid Loss: 0.0820 | 0.0527
Epoch 40/300, resid Loss: 0.0821 | 0.0511
Epoch 41/300, resid Loss: 0.0825 | 0.0504
Epoch 42/300, resid Loss: 0.0807 | 0.0501
Epoch 43/300, resid Loss: 0.0805 | 0.0535
Epoch 44/300, resid Loss: 0.0805 | 0.0524
Epoch 45/300, resid Loss: 0.0788 | 0.0488
Epoch 46/300, resid Loss: 0.0779 | 0.0473
Epoch 47/300, resid Loss: 0.0772 | 0.0470
Epoch 48/300, resid Loss: 0.0760 | 0.0475
Epoch 49/300, resid Loss: 0.0756 | 0.0478
Epoch 50/300, resid Loss: 0.0752 | 0.0471
Epoch 51/300, resid Loss: 0.0746 | 0.0461
Epoch 52/300, resid Loss: 0.0743 | 0.0454
Epoch 53/300, resid Loss: 0.0739 | 0.0452
Epoch 54/300, resid Loss: 0.0736 | 0.0453
Epoch 55/300, resid Loss: 0.0735 | 0.0458
Epoch 56/300, resid Loss: 0.0735 | 0.0461
Epoch 57/300, resid Loss: 0.0735 | 0.0441
Epoch 58/300, resid Loss: 0.0729 | 0.0426
Epoch 59/300, resid Loss: 0.0728 | 0.0419
Epoch 60/300, resid Loss: 0.0726 | 0.0416
Epoch 61/300, resid Loss: 0.0721 | 0.0415
Epoch 62/300, resid Loss: 0.0719 | 0.0416
Epoch 63/300, resid Loss: 0.0715 | 0.0415
Epoch 64/300, resid Loss: 0.0710 | 0.0411
Epoch 65/300, resid Loss: 0.0706 | 0.0408
Epoch 66/300, resid Loss: 0.0704 | 0.0406
Epoch 67/300, resid Loss: 0.0702 | 0.0403
Epoch 68/300, resid Loss: 0.0700 | 0.0400
Epoch 69/300, resid Loss: 0.0699 | 0.0397
Epoch 70/300, resid Loss: 0.0697 | 0.0394
Epoch 71/300, resid Loss: 0.0695 | 0.0391
Epoch 72/300, resid Loss: 0.0694 | 0.0390
Epoch 73/300, resid Loss: 0.0692 | 0.0388
Epoch 74/300, resid Loss: 0.0691 | 0.0386
Epoch 75/300, resid Loss: 0.0689 | 0.0384
Epoch 76/300, resid Loss: 0.0688 | 0.0382
Epoch 77/300, resid Loss: 0.0687 | 0.0380
Epoch 78/300, resid Loss: 0.0686 | 0.0378
Epoch 79/300, resid Loss: 0.0684 | 0.0377
Epoch 80/300, resid Loss: 0.0683 | 0.0375
Epoch 81/300, resid Loss: 0.0682 | 0.0374
Epoch 82/300, resid Loss: 0.0681 | 0.0372
Epoch 83/300, resid Loss: 0.0680 | 0.0371
Epoch 84/300, resid Loss: 0.0679 | 0.0370
Epoch 85/300, resid Loss: 0.0678 | 0.0369
Epoch 86/300, resid Loss: 0.0677 | 0.0367
Epoch 87/300, resid Loss: 0.0677 | 0.0366
Epoch 88/300, resid Loss: 0.0676 | 0.0365
Epoch 89/300, resid Loss: 0.0675 | 0.0364
Epoch 90/300, resid Loss: 0.0674 | 0.0364
Epoch 91/300, resid Loss: 0.0673 | 0.0363
Epoch 92/300, resid Loss: 0.0673 | 0.0362
Epoch 93/300, resid Loss: 0.0672 | 0.0361
Epoch 94/300, resid Loss: 0.0672 | 0.0360
Epoch 95/300, resid Loss: 0.0671 | 0.0360
Epoch 96/300, resid Loss: 0.0670 | 0.0359
Epoch 97/300, resid Loss: 0.0670 | 0.0358
Epoch 98/300, resid Loss: 0.0669 | 0.0358
Epoch 99/300, resid Loss: 0.0669 | 0.0357
Epoch 100/300, resid Loss: 0.0668 | 0.0357
Epoch 101/300, resid Loss: 0.0668 | 0.0356
Epoch 102/300, resid Loss: 0.0667 | 0.0356
Epoch 103/300, resid Loss: 0.0667 | 0.0355
Epoch 104/300, resid Loss: 0.0666 | 0.0355
Epoch 105/300, resid Loss: 0.0666 | 0.0354
Epoch 106/300, resid Loss: 0.0665 | 0.0354
Epoch 107/300, resid Loss: 0.0665 | 0.0353
Epoch 108/300, resid Loss: 0.0665 | 0.0353
Epoch 109/300, resid Loss: 0.0664 | 0.0353
Epoch 110/300, resid Loss: 0.0664 | 0.0352
Epoch 111/300, resid Loss: 0.0663 | 0.0352
Epoch 112/300, resid Loss: 0.0663 | 0.0352
Epoch 113/300, resid Loss: 0.0663 | 0.0351
Epoch 114/300, resid Loss: 0.0663 | 0.0351
Epoch 115/300, resid Loss: 0.0662 | 0.0351
Epoch 116/300, resid Loss: 0.0662 | 0.0351
Epoch 117/300, resid Loss: 0.0662 | 0.0350
Epoch 118/300, resid Loss: 0.0661 | 0.0350
Epoch 119/300, resid Loss: 0.0661 | 0.0350
Epoch 120/300, resid Loss: 0.0661 | 0.0350
Epoch 121/300, resid Loss: 0.0661 | 0.0349
Epoch 122/300, resid Loss: 0.0660 | 0.0349
Epoch 123/300, resid Loss: 0.0660 | 0.0349
Epoch 124/300, resid Loss: 0.0660 | 0.0349
Epoch 125/300, resid Loss: 0.0660 | 0.0349
Epoch 126/300, resid Loss: 0.0660 | 0.0348
Epoch 127/300, resid Loss: 0.0659 | 0.0348
Epoch 128/300, resid Loss: 0.0659 | 0.0348
Epoch 129/300, resid Loss: 0.0659 | 0.0348
Epoch 130/300, resid Loss: 0.0659 | 0.0348
Epoch 131/300, resid Loss: 0.0659 | 0.0348
Epoch 132/300, resid Loss: 0.0659 | 0.0348
Epoch 133/300, resid Loss: 0.0658 | 0.0347
Epoch 134/300, resid Loss: 0.0658 | 0.0347
Epoch 135/300, resid Loss: 0.0658 | 0.0347
Epoch 136/300, resid Loss: 0.0658 | 0.0347
Epoch 137/300, resid Loss: 0.0658 | 0.0347
Epoch 138/300, resid Loss: 0.0658 | 0.0347
Epoch 139/300, resid Loss: 0.0658 | 0.0347
Epoch 140/300, resid Loss: 0.0657 | 0.0347
Epoch 141/300, resid Loss: 0.0657 | 0.0346
Epoch 142/300, resid Loss: 0.0657 | 0.0346
Epoch 143/300, resid Loss: 0.0657 | 0.0346
Epoch 144/300, resid Loss: 0.0657 | 0.0346
Epoch 145/300, resid Loss: 0.0657 | 0.0346
Epoch 146/300, resid Loss: 0.0657 | 0.0346
Epoch 147/300, resid Loss: 0.0657 | 0.0346
Epoch 148/300, resid Loss: 0.0657 | 0.0346
Epoch 149/300, resid Loss: 0.0657 | 0.0346
Epoch 150/300, resid Loss: 0.0656 | 0.0346
Epoch 151/300, resid Loss: 0.0656 | 0.0346
Epoch 152/300, resid Loss: 0.0656 | 0.0346
Epoch 153/300, resid Loss: 0.0656 | 0.0345
Epoch 154/300, resid Loss: 0.0656 | 0.0345
Epoch 155/300, resid Loss: 0.0656 | 0.0345
Epoch 156/300, resid Loss: 0.0656 | 0.0345
Epoch 157/300, resid Loss: 0.0656 | 0.0345
Epoch 158/300, resid Loss: 0.0656 | 0.0345
Epoch 159/300, resid Loss: 0.0656 | 0.0345
Epoch 160/300, resid Loss: 0.0656 | 0.0345
Epoch 161/300, resid Loss: 0.0656 | 0.0345
Epoch 162/300, resid Loss: 0.0656 | 0.0345
Epoch 163/300, resid Loss: 0.0656 | 0.0345
Epoch 164/300, resid Loss: 0.0655 | 0.0345
Epoch 165/300, resid Loss: 0.0655 | 0.0345
Epoch 166/300, resid Loss: 0.0655 | 0.0345
Epoch 167/300, resid Loss: 0.0655 | 0.0345
Epoch 168/300, resid Loss: 0.0655 | 0.0345
Epoch 169/300, resid Loss: 0.0655 | 0.0345
Epoch 170/300, resid Loss: 0.0655 | 0.0345
Epoch 171/300, resid Loss: 0.0655 | 0.0345
Epoch 172/300, resid Loss: 0.0655 | 0.0345
Epoch 173/300, resid Loss: 0.0655 | 0.0345
Epoch 174/300, resid Loss: 0.0655 | 0.0345
Epoch 175/300, resid Loss: 0.0655 | 0.0344
Epoch 176/300, resid Loss: 0.0655 | 0.0344
Epoch 177/300, resid Loss: 0.0655 | 0.0344
Epoch 178/300, resid Loss: 0.0655 | 0.0344
Epoch 179/300, resid Loss: 0.0655 | 0.0344
Epoch 180/300, resid Loss: 0.0655 | 0.0344
Epoch 181/300, resid Loss: 0.0655 | 0.0344
Epoch 182/300, resid Loss: 0.0655 | 0.0344
Epoch 183/300, resid Loss: 0.0655 | 0.0344
Epoch 184/300, resid Loss: 0.0655 | 0.0344
Epoch 185/300, resid Loss: 0.0655 | 0.0344
Epoch 186/300, resid Loss: 0.0655 | 0.0344
Epoch 187/300, resid Loss: 0.0655 | 0.0344
Epoch 188/300, resid Loss: 0.0655 | 0.0344
Epoch 189/300, resid Loss: 0.0655 | 0.0344
Epoch 190/300, resid Loss: 0.0655 | 0.0344
Epoch 191/300, resid Loss: 0.0655 | 0.0344
Epoch 192/300, resid Loss: 0.0655 | 0.0344
Epoch 193/300, resid Loss: 0.0655 | 0.0344
Epoch 194/300, resid Loss: 0.0655 | 0.0344
Epoch 195/300, resid Loss: 0.0654 | 0.0344
Epoch 196/300, resid Loss: 0.0654 | 0.0344
Epoch 197/300, resid Loss: 0.0654 | 0.0344
Epoch 198/300, resid Loss: 0.0654 | 0.0344
Epoch 199/300, resid Loss: 0.0654 | 0.0344
Epoch 200/300, resid Loss: 0.0654 | 0.0344
Epoch 201/300, resid Loss: 0.0654 | 0.0344
Epoch 202/300, resid Loss: 0.0654 | 0.0344
Epoch 203/300, resid Loss: 0.0654 | 0.0344
Epoch 204/300, resid Loss: 0.0654 | 0.0344
Epoch 205/300, resid Loss: 0.0654 | 0.0344
Epoch 206/300, resid Loss: 0.0654 | 0.0344
Epoch 207/300, resid Loss: 0.0654 | 0.0344
Epoch 208/300, resid Loss: 0.0654 | 0.0344
Epoch 209/300, resid Loss: 0.0654 | 0.0344
Epoch 210/300, resid Loss: 0.0654 | 0.0344
Epoch 211/300, resid Loss: 0.0654 | 0.0344
Epoch 212/300, resid Loss: 0.0654 | 0.0344
Epoch 213/300, resid Loss: 0.0654 | 0.0344
Epoch 214/300, resid Loss: 0.0654 | 0.0344
Epoch 215/300, resid Loss: 0.0654 | 0.0344
Epoch 216/300, resid Loss: 0.0654 | 0.0344
Epoch 217/300, resid Loss: 0.0654 | 0.0344
Epoch 218/300, resid Loss: 0.0654 | 0.0344
Epoch 219/300, resid Loss: 0.0654 | 0.0344
Epoch 220/300, resid Loss: 0.0654 | 0.0344
Epoch 221/300, resid Loss: 0.0654 | 0.0344
Epoch 222/300, resid Loss: 0.0654 | 0.0344
Epoch 223/300, resid Loss: 0.0654 | 0.0344
Epoch 224/300, resid Loss: 0.0654 | 0.0344
Epoch 225/300, resid Loss: 0.0654 | 0.0344
Epoch 226/300, resid Loss: 0.0654 | 0.0344
Epoch 227/300, resid Loss: 0.0654 | 0.0344
Epoch 228/300, resid Loss: 0.0654 | 0.0344
Epoch 229/300, resid Loss: 0.0654 | 0.0344
Epoch 230/300, resid Loss: 0.0654 | 0.0344
Epoch 231/300, resid Loss: 0.0654 | 0.0344
Epoch 232/300, resid Loss: 0.0654 | 0.0344
Epoch 233/300, resid Loss: 0.0654 | 0.0344
Epoch 234/300, resid Loss: 0.0654 | 0.0344
Epoch 235/300, resid Loss: 0.0654 | 0.0344
Epoch 236/300, resid Loss: 0.0654 | 0.0344
Epoch 237/300, resid Loss: 0.0654 | 0.0344
Epoch 238/300, resid Loss: 0.0654 | 0.0344
Epoch 239/300, resid Loss: 0.0654 | 0.0344
Epoch 240/300, resid Loss: 0.0654 | 0.0344
Epoch 241/300, resid Loss: 0.0654 | 0.0344
Epoch 242/300, resid Loss: 0.0654 | 0.0344
Epoch 243/300, resid Loss: 0.0654 | 0.0344
Epoch 244/300, resid Loss: 0.0654 | 0.0344
Epoch 245/300, resid Loss: 0.0654 | 0.0344
Epoch 246/300, resid Loss: 0.0654 | 0.0344
Epoch 247/300, resid Loss: 0.0654 | 0.0344
Epoch 248/300, resid Loss: 0.0654 | 0.0344
Epoch 249/300, resid Loss: 0.0654 | 0.0344
Epoch 250/300, resid Loss: 0.0654 | 0.0344
Epoch 251/300, resid Loss: 0.0654 | 0.0344
Epoch 252/300, resid Loss: 0.0654 | 0.0344
Epoch 253/300, resid Loss: 0.0654 | 0.0344
Epoch 254/300, resid Loss: 0.0654 | 0.0344
Epoch 255/300, resid Loss: 0.0654 | 0.0344
Epoch 256/300, resid Loss: 0.0654 | 0.0344
Epoch 257/300, resid Loss: 0.0654 | 0.0344
Epoch 258/300, resid Loss: 0.0654 | 0.0344
Epoch 259/300, resid Loss: 0.0654 | 0.0344
Epoch 260/300, resid Loss: 0.0654 | 0.0344
Epoch 261/300, resid Loss: 0.0654 | 0.0344
Epoch 262/300, resid Loss: 0.0654 | 0.0344
Epoch 263/300, resid Loss: 0.0654 | 0.0344
Epoch 264/300, resid Loss: 0.0654 | 0.0344
Epoch 265/300, resid Loss: 0.0654 | 0.0344
Epoch 266/300, resid Loss: 0.0654 | 0.0344
Epoch 267/300, resid Loss: 0.0654 | 0.0344
Epoch 268/300, resid Loss: 0.0654 | 0.0344
Epoch 269/300, resid Loss: 0.0654 | 0.0344
Epoch 270/300, resid Loss: 0.0654 | 0.0344
Epoch 271/300, resid Loss: 0.0654 | 0.0344
Epoch 272/300, resid Loss: 0.0654 | 0.0344
Epoch 273/300, resid Loss: 0.0654 | 0.0344
Epoch 274/300, resid Loss: 0.0654 | 0.0344
Epoch 275/300, resid Loss: 0.0654 | 0.0344
Epoch 276/300, resid Loss: 0.0654 | 0.0344
Epoch 277/300, resid Loss: 0.0654 | 0.0344
Epoch 278/300, resid Loss: 0.0654 | 0.0344
Epoch 279/300, resid Loss: 0.0654 | 0.0344
Epoch 280/300, resid Loss: 0.0654 | 0.0344
Epoch 281/300, resid Loss: 0.0654 | 0.0344
Epoch 282/300, resid Loss: 0.0654 | 0.0344
Epoch 283/300, resid Loss: 0.0654 | 0.0344
Epoch 284/300, resid Loss: 0.0654 | 0.0344
Epoch 285/300, resid Loss: 0.0654 | 0.0344
Epoch 286/300, resid Loss: 0.0654 | 0.0344
Epoch 287/300, resid Loss: 0.0654 | 0.0344
Epoch 288/300, resid Loss: 0.0654 | 0.0344
Epoch 289/300, resid Loss: 0.0654 | 0.0344
Epoch 290/300, resid Loss: 0.0654 | 0.0344
Epoch 291/300, resid Loss: 0.0654 | 0.0344
Epoch 292/300, resid Loss: 0.0654 | 0.0344
Epoch 293/300, resid Loss: 0.0654 | 0.0344
Epoch 294/300, resid Loss: 0.0654 | 0.0344
Epoch 295/300, resid Loss: 0.0654 | 0.0344
Epoch 296/300, resid Loss: 0.0654 | 0.0344
Epoch 297/300, resid Loss: 0.0654 | 0.0344
Epoch 298/300, resid Loss: 0.0654 | 0.0344
Epoch 299/300, resid Loss: 0.0654 | 0.0344
Epoch 300/300, resid Loss: 0.0654 | 0.0344
Runtime (seconds): 2849.2298896312714
0.0006636125083983942
[160.52243]
[-3.299977]
[3.5198486]
[15.534904]
[3.728108]
[25.405418]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 28.08226920873858
RMSE: 5.2992706298828125
MAE: 5.2992706298828125
R-squared: nan
[205.41074]
