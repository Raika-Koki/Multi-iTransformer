[32m[I 2025-02-09 11:31:55,420][0m A new study created in memory with name: no-name-6e3835a8-ab62-40e2-8ce6-17dcada9d4f4[0m
[32m[I 2025-02-09 11:32:43,758][0m Trial 0 finished with value: 0.3210252435910034 and parameters: {'observation_period_num': 123, 'train_rates': 0.6776354114312568, 'learning_rate': 1.6488115218993843e-05, 'batch_size': 98, 'step_size': 15, 'gamma': 0.8630132757906981}. Best is trial 0 with value: 0.3210252435910034.[0m
[32m[I 2025-02-09 11:33:12,457][0m Trial 1 finished with value: 0.22287924560546363 and parameters: {'observation_period_num': 33, 'train_rates': 0.7646956863805843, 'learning_rate': 0.00042762692715604916, 'batch_size': 199, 'step_size': 5, 'gamma': 0.9709646965334802}. Best is trial 1 with value: 0.22287924560546363.[0m
[32m[I 2025-02-09 11:33:40,308][0m Trial 2 finished with value: 0.18702776865258489 and parameters: {'observation_period_num': 47, 'train_rates': 0.6566957690895692, 'learning_rate': 0.00030126490752377833, 'batch_size': 190, 'step_size': 11, 'gamma': 0.7566139478489609}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:34:16,765][0m Trial 3 finished with value: 0.2846669715685834 and parameters: {'observation_period_num': 163, 'train_rates': 0.6817154381000339, 'learning_rate': 0.00012105568964212525, 'batch_size': 131, 'step_size': 8, 'gamma': 0.7863781304008537}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:34:42,108][0m Trial 4 finished with value: 0.2710373103618622 and parameters: {'observation_period_num': 234, 'train_rates': 0.9361463232368071, 'learning_rate': 0.00016583898949307162, 'batch_size': 256, 'step_size': 8, 'gamma': 0.8339200726132181}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:35:58,410][0m Trial 5 finished with value: 0.5106993019580841 and parameters: {'observation_period_num': 242, 'train_rates': 0.9687622680145678, 'learning_rate': 6.724699700062053e-06, 'batch_size': 76, 'step_size': 15, 'gamma': 0.7928094617734118}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:36:42,255][0m Trial 6 finished with value: 0.45382719024075224 and parameters: {'observation_period_num': 65, 'train_rates': 0.795229879192706, 'learning_rate': 2.7577123758546503e-06, 'batch_size': 124, 'step_size': 15, 'gamma': 0.835829334555583}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:37:11,688][0m Trial 7 finished with value: 0.4425303389771871 and parameters: {'observation_period_num': 218, 'train_rates': 0.8753988081689631, 'learning_rate': 1.0010476545662868e-05, 'batch_size': 190, 'step_size': 14, 'gamma': 0.8973617281769571}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:41:46,494][0m Trial 8 finished with value: 0.707813646096313 and parameters: {'observation_period_num': 60, 'train_rates': 0.6349182940221375, 'learning_rate': 2.1712232446522217e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9367159823273093}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:42:29,385][0m Trial 9 finished with value: 0.38543223452220843 and parameters: {'observation_period_num': 193, 'train_rates': 0.7064671130583796, 'learning_rate': 2.713314020450917e-05, 'batch_size': 114, 'step_size': 5, 'gamma': 0.8922418869416973}. Best is trial 2 with value: 0.18702776865258489.[0m
[32m[I 2025-02-09 11:42:54,954][0m Trial 10 finished with value: 0.12416173157647317 and parameters: {'observation_period_num': 9, 'train_rates': 0.6208420857389849, 'learning_rate': 0.0007581957892611803, 'batch_size': 197, 'step_size': 11, 'gamma': 0.7526961191806862}. Best is trial 10 with value: 0.12416173157647317.[0m
[32m[I 2025-02-09 11:43:22,960][0m Trial 11 finished with value: 0.12757852894276586 and parameters: {'observation_period_num': 8, 'train_rates': 0.6139519357109944, 'learning_rate': 0.0006866393162666796, 'batch_size': 181, 'step_size': 11, 'gamma': 0.7637405950431915}. Best is trial 10 with value: 0.12416173157647317.[0m
[32m[I 2025-02-09 11:43:43,797][0m Trial 12 finished with value: 0.12209152509612771 and parameters: {'observation_period_num': 7, 'train_rates': 0.6173919528891708, 'learning_rate': 0.0008613815643941988, 'batch_size': 232, 'step_size': 11, 'gamma': 0.7510959474999451}. Best is trial 12 with value: 0.12209152509612771.[0m
[32m[I 2025-02-09 11:44:05,540][0m Trial 13 finished with value: 0.22038574091880675 and parameters: {'observation_period_num': 98, 'train_rates': 0.7429208178358426, 'learning_rate': 0.0008086644574309824, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8022688996020492}. Best is trial 12 with value: 0.12209152509612771.[0m
[32m[I 2025-02-09 11:44:29,404][0m Trial 14 finished with value: 0.29030257075066984 and parameters: {'observation_period_num': 10, 'train_rates': 0.6151991374509251, 'learning_rate': 7.265968451203069e-05, 'batch_size': 216, 'step_size': 10, 'gamma': 0.7500372630816364}. Best is trial 12 with value: 0.12209152509612771.[0m
[32m[I 2025-02-09 11:45:04,253][0m Trial 15 finished with value: 0.12670459133341797 and parameters: {'observation_period_num': 89, 'train_rates': 0.817275583132252, 'learning_rate': 5.687571378234192e-05, 'batch_size': 160, 'step_size': 13, 'gamma': 0.819542182887169}. Best is trial 12 with value: 0.12209152509612771.[0m
[32m[I 2025-02-09 11:45:25,558][0m Trial 16 finished with value: 0.11885101065101983 and parameters: {'observation_period_num': 5, 'train_rates': 0.6010335665732645, 'learning_rate': 0.0009664148167104311, 'batch_size': 228, 'step_size': 9, 'gamma': 0.776450548286188}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:45:50,443][0m Trial 17 finished with value: 0.2640148852025165 and parameters: {'observation_period_num': 136, 'train_rates': 0.7213484349860375, 'learning_rate': 0.0002539984470803771, 'batch_size': 229, 'step_size': 6, 'gamma': 0.8633816290516325}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:46:28,876][0m Trial 18 finished with value: 0.7331160816818617 and parameters: {'observation_period_num': 35, 'train_rates': 0.8577968106817657, 'learning_rate': 2.0157873365833915e-06, 'batch_size': 153, 'step_size': 7, 'gamma': 0.7821802726894093}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:46:51,966][0m Trial 19 finished with value: 0.37983324979415817 and parameters: {'observation_period_num': 80, 'train_rates': 0.604145920799126, 'learning_rate': 7.429804146622797e-05, 'batch_size': 230, 'step_size': 2, 'gamma': 0.814793693289398}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:48:05,525][0m Trial 20 finished with value: 0.14985149467549838 and parameters: {'observation_period_num': 36, 'train_rates': 0.6688143253606306, 'learning_rate': 0.000399268865297539, 'batch_size': 64, 'step_size': 9, 'gamma': 0.8433671467855535}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:48:28,458][0m Trial 21 finished with value: 0.11969925741549958 and parameters: {'observation_period_num': 7, 'train_rates': 0.6432325133251928, 'learning_rate': 0.000996087357559385, 'batch_size': 222, 'step_size': 12, 'gamma': 0.7756659657814959}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:48:51,360][0m Trial 22 finished with value: 0.14307032892700575 and parameters: {'observation_period_num': 23, 'train_rates': 0.6503773703920808, 'learning_rate': 0.0008320489956382665, 'batch_size': 228, 'step_size': 13, 'gamma': 0.7779537515373662}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:49:20,060][0m Trial 23 finished with value: 0.24725053257474275 and parameters: {'observation_period_num': 56, 'train_rates': 0.6002234957557486, 'learning_rate': 0.00017033484523182608, 'batch_size': 166, 'step_size': 13, 'gamma': 0.7716839329391973}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:49:44,348][0m Trial 24 finished with value: 0.1603031330665604 and parameters: {'observation_period_num': 23, 'train_rates': 0.7134391905866629, 'learning_rate': 0.0009850530065353773, 'batch_size': 217, 'step_size': 9, 'gamma': 0.8045331002651838}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:50:05,746][0m Trial 25 finished with value: 0.20286467396295987 and parameters: {'observation_period_num': 110, 'train_rates': 0.6401981826079206, 'learning_rate': 0.00044918451582203254, 'batch_size': 244, 'step_size': 12, 'gamma': 0.7744480488793348}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:50:30,957][0m Trial 26 finished with value: 0.18030221245670794 and parameters: {'observation_period_num': 69, 'train_rates': 0.6925374311754947, 'learning_rate': 0.00023728757779803642, 'batch_size': 213, 'step_size': 10, 'gamma': 0.7991122427003132}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:51:02,868][0m Trial 27 finished with value: 0.15446060329425396 and parameters: {'observation_period_num': 6, 'train_rates': 0.7416008288101474, 'learning_rate': 0.0005366901498213973, 'batch_size': 173, 'step_size': 9, 'gamma': 0.8160605513144162}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:51:23,747][0m Trial 28 finished with value: 0.5682914002105524 and parameters: {'observation_period_num': 46, 'train_rates': 0.6430793766985439, 'learning_rate': 4.604636355435531e-05, 'batch_size': 243, 'step_size': 12, 'gamma': 0.9220938126474333}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:51:59,629][0m Trial 29 finished with value: 0.2478342700777348 and parameters: {'observation_period_num': 137, 'train_rates': 0.6713512172892098, 'learning_rate': 0.00013985303809699694, 'batch_size': 144, 'step_size': 7, 'gamma': 0.8463045367680836}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:52:26,717][0m Trial 30 finished with value: 0.15319424823148928 and parameters: {'observation_period_num': 22, 'train_rates': 0.6851078628836341, 'learning_rate': 0.00031659355398703777, 'batch_size': 212, 'step_size': 10, 'gamma': 0.9889326376831731}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:52:54,243][0m Trial 31 finished with value: 0.12696188087349414 and parameters: {'observation_period_num': 11, 'train_rates': 0.6274784045793381, 'learning_rate': 0.000660465501463749, 'batch_size': 207, 'step_size': 12, 'gamma': 0.7514472157653473}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:53:20,200][0m Trial 32 finished with value: 0.13231727646531338 and parameters: {'observation_period_num': 30, 'train_rates': 0.6259748493031723, 'learning_rate': 0.0009592961588371701, 'batch_size': 197, 'step_size': 11, 'gamma': 0.7675819550960837}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:53:43,107][0m Trial 33 finished with value: 0.18900293354470982 and parameters: {'observation_period_num': 46, 'train_rates': 0.6523763988740275, 'learning_rate': 0.0005282737731180203, 'batch_size': 239, 'step_size': 10, 'gamma': 0.765067427697643}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:54:10,982][0m Trial 34 finished with value: 0.132749176020196 and parameters: {'observation_period_num': 5, 'train_rates': 0.6650517525687369, 'learning_rate': 0.0003552684748203166, 'batch_size': 189, 'step_size': 14, 'gamma': 0.750518710197024}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:54:34,334][0m Trial 35 finished with value: 0.18344595407979172 and parameters: {'observation_period_num': 42, 'train_rates': 0.606475014058163, 'learning_rate': 0.0005278425629583448, 'batch_size': 200, 'step_size': 12, 'gamma': 0.7880285319011017}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:54:59,225][0m Trial 36 finished with value: 0.2870242904034495 and parameters: {'observation_period_num': 158, 'train_rates': 0.7670107648678154, 'learning_rate': 0.0002124806198711998, 'batch_size': 230, 'step_size': 8, 'gamma': 0.7868397285379055}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:55:22,491][0m Trial 37 finished with value: 0.15426501839975143 and parameters: {'observation_period_num': 25, 'train_rates': 0.6952665149042963, 'learning_rate': 0.0009686629307240262, 'batch_size': 255, 'step_size': 9, 'gamma': 0.7634626400050263}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:55:56,280][0m Trial 38 finished with value: 0.14434013687647307 and parameters: {'observation_period_num': 77, 'train_rates': 0.9112934099423345, 'learning_rate': 9.548372178368673e-05, 'batch_size': 178, 'step_size': 14, 'gamma': 0.8281887216859332}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:56:19,074][0m Trial 39 finished with value: 0.5954848530081694 and parameters: {'observation_period_num': 55, 'train_rates': 0.6277022898466442, 'learning_rate': 5.326172749717237e-06, 'batch_size': 220, 'step_size': 11, 'gamma': 0.7967591215649225}. Best is trial 16 with value: 0.11885101065101983.[0m
Early stopping at epoch 73
[32m[I 2025-02-09 11:56:39,812][0m Trial 40 finished with value: 0.24659509972041968 and parameters: {'observation_period_num': 19, 'train_rates': 0.6549610252713699, 'learning_rate': 0.0003509809951969962, 'batch_size': 195, 'step_size': 1, 'gamma': 0.7772931331164397}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:57:14,109][0m Trial 41 finished with value: 0.3260913954840766 and parameters: {'observation_period_num': 94, 'train_rates': 0.8112263329297902, 'learning_rate': 1.097897519088559e-05, 'batch_size': 160, 'step_size': 13, 'gamma': 0.8160136061553935}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:57:53,268][0m Trial 42 finished with value: 1.097253284499615 and parameters: {'observation_period_num': 190, 'train_rates': 0.8313557206834948, 'learning_rate': 1.137918318694588e-06, 'batch_size': 141, 'step_size': 13, 'gamma': 0.7618649651272592}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:58:48,261][0m Trial 43 finished with value: 0.17082452344543794 and parameters: {'observation_period_num': 17, 'train_rates': 0.7722370218871946, 'learning_rate': 0.0006231223112482992, 'batch_size': 96, 'step_size': 12, 'gamma': 0.8852595400883491}. Best is trial 16 with value: 0.11885101065101983.[0m
[32m[I 2025-02-09 11:59:20,466][0m Trial 44 finished with value: 0.10299895208107128 and parameters: {'observation_period_num': 35, 'train_rates': 0.8360072075285442, 'learning_rate': 3.3107491000339494e-05, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8083101341962462}. Best is trial 44 with value: 0.10299895208107128.[0m
[32m[I 2025-02-09 11:59:49,828][0m Trial 45 finished with value: 0.06640182948940805 and parameters: {'observation_period_num': 33, 'train_rates': 0.882549139525523, 'learning_rate': 0.0006506354221390739, 'batch_size': 205, 'step_size': 14, 'gamma': 0.759306852836563}. Best is trial 45 with value: 0.06640182948940805.[0m
[32m[I 2025-02-09 12:00:20,890][0m Trial 46 finished with value: 0.06801260282079895 and parameters: {'observation_period_num': 34, 'train_rates': 0.896360977331783, 'learning_rate': 0.00019214375913261845, 'batch_size': 206, 'step_size': 15, 'gamma': 0.7889534254940467}. Best is trial 45 with value: 0.06640182948940805.[0m
[32m[I 2025-02-09 12:00:54,926][0m Trial 47 finished with value: 0.2120796506230539 and parameters: {'observation_period_num': 56, 'train_rates': 0.9143344800973635, 'learning_rate': 1.915964311806979e-05, 'batch_size': 181, 'step_size': 15, 'gamma': 0.806327983425068}. Best is trial 45 with value: 0.06640182948940805.[0m
[32m[I 2025-02-09 12:01:24,686][0m Trial 48 finished with value: 0.10379847830676651 and parameters: {'observation_period_num': 36, 'train_rates': 0.8731619250008118, 'learning_rate': 4.490410198803295e-05, 'batch_size': 205, 'step_size': 15, 'gamma': 0.7880247738010059}. Best is trial 45 with value: 0.06640182948940805.[0m
[32m[I 2025-02-09 12:01:54,444][0m Trial 49 finished with value: 0.1320213275457007 and parameters: {'observation_period_num': 68, 'train_rates': 0.8735558547026041, 'learning_rate': 4.522795079685202e-05, 'batch_size': 205, 'step_size': 15, 'gamma': 0.7902841654469597}. Best is trial 45 with value: 0.06640182948940805.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4832 | 0.3333
Epoch 2/300, Loss: 0.2539 | 0.2542
Epoch 3/300, Loss: 0.2758 | 0.3459
Epoch 4/300, Loss: 0.2713 | 0.6206
Epoch 5/300, Loss: 0.1783 | 0.2417
Epoch 6/300, Loss: 0.1706 | 0.3157
Epoch 7/300, Loss: 0.1559 | 0.1732
Epoch 8/300, Loss: 0.1494 | 0.1639
Epoch 9/300, Loss: 0.1258 | 0.1364
Epoch 10/300, Loss: 0.1278 | 0.2535
Epoch 11/300, Loss: 0.1310 | 0.1182
Epoch 12/300, Loss: 0.1234 | 0.1944
Epoch 13/300, Loss: 0.1262 | 0.1113
Epoch 14/300, Loss: 0.1196 | 0.2450
Epoch 15/300, Loss: 0.1175 | 0.1072
Epoch 16/300, Loss: 0.1155 | 0.2174
Epoch 17/300, Loss: 0.1341 | 0.1995
Epoch 18/300, Loss: 0.1501 | 0.2179
Epoch 19/300, Loss: 0.1716 | 0.2560
Epoch 20/300, Loss: 0.1759 | 0.1891
Epoch 21/300, Loss: 0.1416 | 0.1188
Epoch 22/300, Loss: 0.1209 | 0.1424
Epoch 23/300, Loss: 0.1349 | 0.2236
Epoch 24/300, Loss: 0.1450 | 0.1270
Epoch 25/300, Loss: 0.1586 | 0.4544
Epoch 26/300, Loss: 0.1344 | 0.1501
Epoch 27/300, Loss: 0.1065 | 0.1894
Epoch 28/300, Loss: 0.0998 | 0.1086
Epoch 29/300, Loss: 0.0943 | 0.1326
Epoch 30/300, Loss: 0.0956 | 0.0936
Epoch 31/300, Loss: 0.0971 | 0.1412
Epoch 32/300, Loss: 0.0943 | 0.0886
Epoch 33/300, Loss: 0.0982 | 0.1458
Epoch 34/300, Loss: 0.0959 | 0.0855
Epoch 35/300, Loss: 0.1015 | 0.1931
Epoch 36/300, Loss: 0.0904 | 0.0816
Epoch 37/300, Loss: 0.0864 | 0.1246
Epoch 38/300, Loss: 0.0830 | 0.0762
Epoch 39/300, Loss: 0.0827 | 0.0852
Epoch 40/300, Loss: 0.0808 | 0.0695
Epoch 41/300, Loss: 0.0806 | 0.0727
Epoch 42/300, Loss: 0.0796 | 0.0658
Epoch 43/300, Loss: 0.0803 | 0.0760
Epoch 44/300, Loss: 0.0793 | 0.0617
Epoch 45/300, Loss: 0.0809 | 0.0839
Epoch 46/300, Loss: 0.0797 | 0.0611
Epoch 47/300, Loss: 0.0816 | 0.0930
Epoch 48/300, Loss: 0.0798 | 0.0608
Epoch 49/300, Loss: 0.0817 | 0.0989
Epoch 50/300, Loss: 0.0778 | 0.0592
Epoch 51/300, Loss: 0.0772 | 0.0766
Epoch 52/300, Loss: 0.0758 | 0.0587
Epoch 53/300, Loss: 0.0756 | 0.0626
Epoch 54/300, Loss: 0.0750 | 0.0579
Epoch 55/300, Loss: 0.0747 | 0.0580
Epoch 56/300, Loss: 0.0744 | 0.0567
Epoch 57/300, Loss: 0.0740 | 0.0559
Epoch 58/300, Loss: 0.0738 | 0.0561
Epoch 59/300, Loss: 0.0736 | 0.0549
Epoch 60/300, Loss: 0.0734 | 0.0550
Epoch 61/300, Loss: 0.0732 | 0.0541
Epoch 62/300, Loss: 0.0730 | 0.0541
Epoch 63/300, Loss: 0.0728 | 0.0533
Epoch 64/300, Loss: 0.0726 | 0.0534
Epoch 65/300, Loss: 0.0724 | 0.0529
Epoch 66/300, Loss: 0.0723 | 0.0527
Epoch 67/300, Loss: 0.0721 | 0.0524
Epoch 68/300, Loss: 0.0719 | 0.0521
Epoch 69/300, Loss: 0.0718 | 0.0519
Epoch 70/300, Loss: 0.0716 | 0.0516
Epoch 71/300, Loss: 0.0715 | 0.0515
Epoch 72/300, Loss: 0.0714 | 0.0513
Epoch 73/300, Loss: 0.0712 | 0.0511
Epoch 74/300, Loss: 0.0711 | 0.0509
Epoch 75/300, Loss: 0.0710 | 0.0508
Epoch 76/300, Loss: 0.0709 | 0.0506
Epoch 77/300, Loss: 0.0708 | 0.0504
Epoch 78/300, Loss: 0.0707 | 0.0503
Epoch 79/300, Loss: 0.0706 | 0.0502
Epoch 80/300, Loss: 0.0705 | 0.0500
Epoch 81/300, Loss: 0.0704 | 0.0499
Epoch 82/300, Loss: 0.0703 | 0.0498
Epoch 83/300, Loss: 0.0702 | 0.0496
Epoch 84/300, Loss: 0.0701 | 0.0495
Epoch 85/300, Loss: 0.0700 | 0.0494
Epoch 86/300, Loss: 0.0700 | 0.0493
Epoch 87/300, Loss: 0.0699 | 0.0492
Epoch 88/300, Loss: 0.0698 | 0.0491
Epoch 89/300, Loss: 0.0698 | 0.0490
Epoch 90/300, Loss: 0.0697 | 0.0489
Epoch 91/300, Loss: 0.0696 | 0.0489
Epoch 92/300, Loss: 0.0695 | 0.0488
Epoch 93/300, Loss: 0.0695 | 0.0487
Epoch 94/300, Loss: 0.0694 | 0.0486
Epoch 95/300, Loss: 0.0694 | 0.0486
Epoch 96/300, Loss: 0.0693 | 0.0485
Epoch 97/300, Loss: 0.0693 | 0.0484
Epoch 98/300, Loss: 0.0692 | 0.0484
Epoch 99/300, Loss: 0.0692 | 0.0483
Epoch 100/300, Loss: 0.0691 | 0.0483
Epoch 101/300, Loss: 0.0691 | 0.0482
Epoch 102/300, Loss: 0.0691 | 0.0482
Epoch 103/300, Loss: 0.0690 | 0.0481
Epoch 104/300, Loss: 0.0690 | 0.0481
Epoch 105/300, Loss: 0.0689 | 0.0480
Epoch 106/300, Loss: 0.0689 | 0.0480
Epoch 107/300, Loss: 0.0689 | 0.0479
Epoch 108/300, Loss: 0.0688 | 0.0479
Epoch 109/300, Loss: 0.0688 | 0.0478
Epoch 110/300, Loss: 0.0688 | 0.0478
Epoch 111/300, Loss: 0.0687 | 0.0478
Epoch 112/300, Loss: 0.0687 | 0.0477
Epoch 113/300, Loss: 0.0687 | 0.0477
Epoch 114/300, Loss: 0.0686 | 0.0477
Epoch 115/300, Loss: 0.0686 | 0.0476
Epoch 116/300, Loss: 0.0686 | 0.0476
Epoch 117/300, Loss: 0.0686 | 0.0476
Epoch 118/300, Loss: 0.0685 | 0.0475
Epoch 119/300, Loss: 0.0685 | 0.0475
Epoch 120/300, Loss: 0.0685 | 0.0475
Epoch 121/300, Loss: 0.0685 | 0.0475
Epoch 122/300, Loss: 0.0685 | 0.0474
Epoch 123/300, Loss: 0.0684 | 0.0474
Epoch 124/300, Loss: 0.0684 | 0.0474
Epoch 125/300, Loss: 0.0684 | 0.0474
Epoch 126/300, Loss: 0.0684 | 0.0473
Epoch 127/300, Loss: 0.0684 | 0.0473
Epoch 128/300, Loss: 0.0683 | 0.0473
Epoch 129/300, Loss: 0.0683 | 0.0473
Epoch 130/300, Loss: 0.0683 | 0.0473
Epoch 131/300, Loss: 0.0683 | 0.0473
Epoch 132/300, Loss: 0.0683 | 0.0472
Epoch 133/300, Loss: 0.0683 | 0.0472
Epoch 134/300, Loss: 0.0683 | 0.0472
Epoch 135/300, Loss: 0.0682 | 0.0472
Epoch 136/300, Loss: 0.0682 | 0.0472
Epoch 137/300, Loss: 0.0682 | 0.0472
Epoch 138/300, Loss: 0.0682 | 0.0472
Epoch 139/300, Loss: 0.0682 | 0.0471
Epoch 140/300, Loss: 0.0682 | 0.0471
Epoch 141/300, Loss: 0.0682 | 0.0471
Epoch 142/300, Loss: 0.0682 | 0.0471
Epoch 143/300, Loss: 0.0682 | 0.0471
Epoch 144/300, Loss: 0.0681 | 0.0471
Epoch 145/300, Loss: 0.0681 | 0.0471
Epoch 146/300, Loss: 0.0681 | 0.0471
Epoch 147/300, Loss: 0.0681 | 0.0470
Epoch 148/300, Loss: 0.0681 | 0.0470
Epoch 149/300, Loss: 0.0681 | 0.0470
Epoch 150/300, Loss: 0.0681 | 0.0470
Epoch 151/300, Loss: 0.0681 | 0.0470
Epoch 152/300, Loss: 0.0681 | 0.0470
Epoch 153/300, Loss: 0.0681 | 0.0470
Epoch 154/300, Loss: 0.0681 | 0.0470
Epoch 155/300, Loss: 0.0681 | 0.0470
Epoch 156/300, Loss: 0.0681 | 0.0470
Epoch 157/300, Loss: 0.0680 | 0.0470
Epoch 158/300, Loss: 0.0680 | 0.0470
Epoch 159/300, Loss: 0.0680 | 0.0470
Epoch 160/300, Loss: 0.0680 | 0.0470
Epoch 161/300, Loss: 0.0680 | 0.0469
Epoch 162/300, Loss: 0.0680 | 0.0469
Epoch 163/300, Loss: 0.0680 | 0.0469
Epoch 164/300, Loss: 0.0680 | 0.0469
Epoch 165/300, Loss: 0.0680 | 0.0469
Epoch 166/300, Loss: 0.0680 | 0.0469
Epoch 167/300, Loss: 0.0680 | 0.0469
Epoch 168/300, Loss: 0.0680 | 0.0469
Epoch 169/300, Loss: 0.0680 | 0.0469
Epoch 170/300, Loss: 0.0680 | 0.0469
Epoch 171/300, Loss: 0.0680 | 0.0469
Epoch 172/300, Loss: 0.0680 | 0.0469
Epoch 173/300, Loss: 0.0680 | 0.0469
Epoch 174/300, Loss: 0.0680 | 0.0469
Epoch 175/300, Loss: 0.0680 | 0.0469
Epoch 176/300, Loss: 0.0680 | 0.0469
Epoch 177/300, Loss: 0.0680 | 0.0469
Epoch 178/300, Loss: 0.0680 | 0.0469
Epoch 179/300, Loss: 0.0680 | 0.0469
Epoch 180/300, Loss: 0.0680 | 0.0469
Epoch 181/300, Loss: 0.0680 | 0.0469
Epoch 182/300, Loss: 0.0680 | 0.0469
Epoch 183/300, Loss: 0.0679 | 0.0469
Epoch 184/300, Loss: 0.0679 | 0.0469
Epoch 185/300, Loss: 0.0679 | 0.0469
Epoch 186/300, Loss: 0.0679 | 0.0468
Epoch 187/300, Loss: 0.0679 | 0.0468
Epoch 188/300, Loss: 0.0679 | 0.0468
Epoch 189/300, Loss: 0.0679 | 0.0468
Epoch 190/300, Loss: 0.0679 | 0.0468
Epoch 191/300, Loss: 0.0679 | 0.0468
Epoch 192/300, Loss: 0.0679 | 0.0468
Epoch 193/300, Loss: 0.0679 | 0.0468
Epoch 194/300, Loss: 0.0679 | 0.0468
Epoch 195/300, Loss: 0.0679 | 0.0468
Epoch 196/300, Loss: 0.0679 | 0.0468
Epoch 197/300, Loss: 0.0679 | 0.0468
Epoch 198/300, Loss: 0.0679 | 0.0468
Epoch 199/300, Loss: 0.0679 | 0.0468
Epoch 200/300, Loss: 0.0679 | 0.0468
Epoch 201/300, Loss: 0.0679 | 0.0468
Epoch 202/300, Loss: 0.0679 | 0.0468
Epoch 203/300, Loss: 0.0679 | 0.0468
Epoch 204/300, Loss: 0.0679 | 0.0468
Epoch 205/300, Loss: 0.0679 | 0.0468
Epoch 206/300, Loss: 0.0679 | 0.0468
Epoch 207/300, Loss: 0.0679 | 0.0468
Epoch 208/300, Loss: 0.0679 | 0.0468
Epoch 209/300, Loss: 0.0679 | 0.0468
Epoch 210/300, Loss: 0.0679 | 0.0468
Epoch 211/300, Loss: 0.0679 | 0.0468
Epoch 212/300, Loss: 0.0679 | 0.0468
Epoch 213/300, Loss: 0.0679 | 0.0468
Epoch 214/300, Loss: 0.0679 | 0.0468
Epoch 215/300, Loss: 0.0679 | 0.0468
Epoch 216/300, Loss: 0.0679 | 0.0468
Epoch 217/300, Loss: 0.0679 | 0.0468
Epoch 218/300, Loss: 0.0679 | 0.0468
Epoch 219/300, Loss: 0.0679 | 0.0468
Epoch 220/300, Loss: 0.0679 | 0.0468
Epoch 221/300, Loss: 0.0679 | 0.0468
Epoch 222/300, Loss: 0.0679 | 0.0468
Epoch 223/300, Loss: 0.0679 | 0.0468
Epoch 224/300, Loss: 0.0679 | 0.0468
Epoch 225/300, Loss: 0.0679 | 0.0468
Epoch 226/300, Loss: 0.0679 | 0.0468
Epoch 227/300, Loss: 0.0679 | 0.0468
Epoch 228/300, Loss: 0.0679 | 0.0468
Epoch 229/300, Loss: 0.0679 | 0.0468
Epoch 230/300, Loss: 0.0679 | 0.0468
Epoch 231/300, Loss: 0.0679 | 0.0468
Epoch 232/300, Loss: 0.0679 | 0.0468
Epoch 233/300, Loss: 0.0679 | 0.0468
Epoch 234/300, Loss: 0.0679 | 0.0468
Epoch 235/300, Loss: 0.0679 | 0.0468
Epoch 236/300, Loss: 0.0679 | 0.0468
Epoch 237/300, Loss: 0.0679 | 0.0468
Epoch 238/300, Loss: 0.0679 | 0.0468
Epoch 239/300, Loss: 0.0679 | 0.0468
Epoch 240/300, Loss: 0.0679 | 0.0468
Epoch 241/300, Loss: 0.0679 | 0.0468
Epoch 242/300, Loss: 0.0679 | 0.0468
Epoch 243/300, Loss: 0.0679 | 0.0468
Epoch 244/300, Loss: 0.0679 | 0.0468
Epoch 245/300, Loss: 0.0679 | 0.0468
Epoch 246/300, Loss: 0.0679 | 0.0468
Epoch 247/300, Loss: 0.0679 | 0.0468
Epoch 248/300, Loss: 0.0679 | 0.0468
Epoch 249/300, Loss: 0.0679 | 0.0468
Epoch 250/300, Loss: 0.0679 | 0.0468
Epoch 251/300, Loss: 0.0679 | 0.0468
Epoch 252/300, Loss: 0.0679 | 0.0468
Epoch 253/300, Loss: 0.0679 | 0.0468
Epoch 254/300, Loss: 0.0679 | 0.0468
Epoch 255/300, Loss: 0.0679 | 0.0468
Epoch 256/300, Loss: 0.0679 | 0.0468
Epoch 257/300, Loss: 0.0679 | 0.0468
Epoch 258/300, Loss: 0.0679 | 0.0468
Epoch 259/300, Loss: 0.0679 | 0.0468
Epoch 260/300, Loss: 0.0679 | 0.0468
Epoch 261/300, Loss: 0.0679 | 0.0468
Epoch 262/300, Loss: 0.0679 | 0.0468
Epoch 263/300, Loss: 0.0679 | 0.0468
Epoch 264/300, Loss: 0.0679 | 0.0468
Epoch 265/300, Loss: 0.0679 | 0.0468
Epoch 266/300, Loss: 0.0679 | 0.0468
Epoch 267/300, Loss: 0.0679 | 0.0468
Epoch 268/300, Loss: 0.0679 | 0.0468
Epoch 269/300, Loss: 0.0679 | 0.0468
Epoch 270/300, Loss: 0.0679 | 0.0468
Epoch 271/300, Loss: 0.0679 | 0.0468
Epoch 272/300, Loss: 0.0679 | 0.0468
Epoch 273/300, Loss: 0.0679 | 0.0468
Epoch 274/300, Loss: 0.0679 | 0.0468
Epoch 275/300, Loss: 0.0679 | 0.0468
Epoch 276/300, Loss: 0.0679 | 0.0468
Epoch 277/300, Loss: 0.0679 | 0.0468
Epoch 278/300, Loss: 0.0679 | 0.0468
Epoch 279/300, Loss: 0.0679 | 0.0468
Epoch 280/300, Loss: 0.0679 | 0.0468
Epoch 281/300, Loss: 0.0679 | 0.0468
Epoch 282/300, Loss: 0.0679 | 0.0468
Epoch 283/300, Loss: 0.0679 | 0.0468
Epoch 284/300, Loss: 0.0679 | 0.0468
Epoch 285/300, Loss: 0.0679 | 0.0468
Epoch 286/300, Loss: 0.0679 | 0.0468
Epoch 287/300, Loss: 0.0679 | 0.0468
Epoch 288/300, Loss: 0.0679 | 0.0468
Epoch 289/300, Loss: 0.0679 | 0.0468
Epoch 290/300, Loss: 0.0679 | 0.0468
Epoch 291/300, Loss: 0.0679 | 0.0468
Epoch 292/300, Loss: 0.0679 | 0.0468
Epoch 293/300, Loss: 0.0679 | 0.0468
Epoch 294/300, Loss: 0.0679 | 0.0468
Epoch 295/300, Loss: 0.0679 | 0.0468
Epoch 296/300, Loss: 0.0679 | 0.0468
Epoch 297/300, Loss: 0.0679 | 0.0468
Epoch 298/300, Loss: 0.0679 | 0.0468
Epoch 299/300, Loss: 0.0679 | 0.0468
Epoch 300/300, Loss: 0.0679 | 0.0468
Runtime (seconds): 86.68369889259338
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 485.7048066894058
RMSE: 22.038711547851562
MAE: 22.038711547851562
R-squared: nan
[209.17871]
