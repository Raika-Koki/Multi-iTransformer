ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-05 07:06:30,298][0m A new study created in memory with name: no-name-34dde5a4-f8a6-416d-ac39-0a31e62bdfae[0m
[32m[I 2025-02-05 07:08:14,589][0m Trial 0 finished with value: 0.5765798948249038 and parameters: {'observation_period_num': 109, 'train_rates': 0.8276337787617745, 'learning_rate': 0.00029605522633271227, 'batch_size': 176, 'step_size': 14, 'gamma': 0.9574906070302346}. Best is trial 0 with value: 0.5765798948249038.[0m
[32m[I 2025-02-05 07:10:35,350][0m Trial 1 finished with value: 1.1412167982158499 and parameters: {'observation_period_num': 29, 'train_rates': 0.9601848844655705, 'learning_rate': 1.4560723661120697e-06, 'batch_size': 35, 'step_size': 13, 'gamma': 0.8386765278454916}. Best is trial 0 with value: 0.5765798948249038.[0m
[32m[I 2025-02-05 07:13:45,391][0m Trial 2 finished with value: 1.1237577199935913 and parameters: {'observation_period_num': 168, 'train_rates': 0.9707116919939145, 'learning_rate': 1.2085641066255929e-05, 'batch_size': 123, 'step_size': 5, 'gamma': 0.7806466820172288}. Best is trial 0 with value: 0.5765798948249038.[0m
[32m[I 2025-02-05 07:17:19,472][0m Trial 3 finished with value: 0.8674843118333941 and parameters: {'observation_period_num': 194, 'train_rates': 0.8613169975735175, 'learning_rate': 1.7058749335195763e-05, 'batch_size': 234, 'step_size': 9, 'gamma': 0.8094807105664082}. Best is trial 0 with value: 0.5765798948249038.[0m
[32m[I 2025-02-05 07:20:47,261][0m Trial 4 finished with value: 0.9809398651123047 and parameters: {'observation_period_num': 183, 'train_rates': 0.9333277681455034, 'learning_rate': 1.0494721406112322e-05, 'batch_size': 206, 'step_size': 12, 'gamma': 0.9204764924026981}. Best is trial 0 with value: 0.5765798948249038.[0m
[32m[I 2025-02-05 07:21:47,638][0m Trial 5 finished with value: 0.9333602786064148 and parameters: {'observation_period_num': 64, 'train_rates': 0.9538379678085053, 'learning_rate': 1.893445348119666e-05, 'batch_size': 222, 'step_size': 13, 'gamma': 0.8672381111277847}. Best is trial 0 with value: 0.5765798948249038.[0m
[32m[I 2025-02-05 07:24:21,311][0m Trial 6 finished with value: 0.38133023187444715 and parameters: {'observation_period_num': 144, 'train_rates': 0.9280360152121929, 'learning_rate': 6.091033291390557e-05, 'batch_size': 198, 'step_size': 13, 'gamma': 0.910209939426171}. Best is trial 6 with value: 0.38133023187444715.[0m
[32m[I 2025-02-05 07:25:56,171][0m Trial 7 finished with value: 0.6528733848933842 and parameters: {'observation_period_num': 95, 'train_rates': 0.859034689495501, 'learning_rate': 0.0006736482773705088, 'batch_size': 170, 'step_size': 11, 'gamma': 0.9864138415396835}. Best is trial 6 with value: 0.38133023187444715.[0m
[32m[I 2025-02-05 07:28:42,449][0m Trial 8 finished with value: 0.7979472239275236 and parameters: {'observation_period_num': 174, 'train_rates': 0.733737282259037, 'learning_rate': 3.6246236522416905e-05, 'batch_size': 110, 'step_size': 10, 'gamma': 0.9447846690278839}. Best is trial 6 with value: 0.38133023187444715.[0m
[32m[I 2025-02-05 07:32:11,839][0m Trial 9 finished with value: 1.1221378025287594 and parameters: {'observation_period_num': 204, 'train_rates': 0.7135946390241431, 'learning_rate': 4.195374623187702e-06, 'batch_size': 48, 'step_size': 12, 'gamma': 0.8396664145018078}. Best is trial 6 with value: 0.38133023187444715.[0m
[32m[I 2025-02-05 07:36:09,484][0m Trial 10 finished with value: 1.1651316059333037 and parameters: {'observation_period_num': 252, 'train_rates': 0.6011844612451861, 'learning_rate': 0.00010493211000641121, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9013110585836409}. Best is trial 6 with value: 0.38133023187444715.[0m
[32m[I 2025-02-05 07:37:57,655][0m Trial 11 finished with value: 0.5140120896948389 and parameters: {'observation_period_num': 111, 'train_rates': 0.8538966880157159, 'learning_rate': 0.0002736886142173944, 'batch_size': 170, 'step_size': 15, 'gamma': 0.9839825212011057}. Best is trial 6 with value: 0.38133023187444715.[0m
[32m[I 2025-02-05 07:40:08,906][0m Trial 12 finished with value: 0.2871784281114052 and parameters: {'observation_period_num': 127, 'train_rates': 0.8975403798924987, 'learning_rate': 0.00011420715533859079, 'batch_size': 163, 'step_size': 15, 'gamma': 0.985119825772804}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:42:57,080][0m Trial 13 finished with value: 0.3264733652273814 and parameters: {'observation_period_num': 149, 'train_rates': 0.9048701155938677, 'learning_rate': 6.7145565536259e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9042851132958085}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:44:08,128][0m Trial 14 finished with value: 0.6076983710283124 and parameters: {'observation_period_num': 70, 'train_rates': 0.7749012269076616, 'learning_rate': 0.00014114133714247668, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8772689585150846}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:46:44,363][0m Trial 15 finished with value: 0.8976261000270429 and parameters: {'observation_period_num': 144, 'train_rates': 0.9019969791115596, 'learning_rate': 0.0009511490814844575, 'batch_size': 89, 'step_size': 6, 'gamma': 0.944422364367203}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:51:21,700][0m Trial 16 finished with value: 0.5880128582874378 and parameters: {'observation_period_num': 230, 'train_rates': 0.8951731783490943, 'learning_rate': 5.5220266418139004e-05, 'batch_size': 142, 'step_size': 3, 'gamma': 0.8807826295571778}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:53:37,968][0m Trial 17 finished with value: 0.5539837228570076 and parameters: {'observation_period_num': 138, 'train_rates': 0.7949583550673188, 'learning_rate': 0.0001689117331186263, 'batch_size': 140, 'step_size': 8, 'gamma': 0.763039811824854}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:54:19,391][0m Trial 18 finished with value: 1.160147090372832 and parameters: {'observation_period_num': 22, 'train_rates': 0.6073899003106397, 'learning_rate': 0.0003793057983831534, 'batch_size': 88, 'step_size': 4, 'gamma': 0.965681752367399}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:55:53,362][0m Trial 19 finished with value: 0.4815765917301178 and parameters: {'observation_period_num': 77, 'train_rates': 0.987504997503899, 'learning_rate': 7.191097073075394e-05, 'batch_size': 71, 'step_size': 1, 'gamma': 0.9282421792612143}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 07:58:28,699][0m Trial 20 finished with value: 1.3673312797055224 and parameters: {'observation_period_num': 159, 'train_rates': 0.7496752098500643, 'learning_rate': 6.133017626627619e-06, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8511239409899507}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 08:01:03,947][0m Trial 21 finished with value: 0.38959411617185247 and parameters: {'observation_period_num': 142, 'train_rates': 0.9133690113515893, 'learning_rate': 3.7260613025934e-05, 'batch_size': 194, 'step_size': 14, 'gamma': 0.906250824743721}. Best is trial 12 with value: 0.2871784281114052.[0m
[32m[I 2025-02-05 08:05:10,520][0m Trial 22 finished with value: 0.27191793144111576 and parameters: {'observation_period_num': 120, 'train_rates': 0.8839279456825784, 'learning_rate': 7.078545456629118e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.899265817857814}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:07:49,213][0m Trial 23 finished with value: 0.4540923082792543 and parameters: {'observation_period_num': 120, 'train_rates': 0.8247531277084568, 'learning_rate': 0.00010029926972594436, 'batch_size': 27, 'step_size': 3, 'gamma': 0.8911214471836754}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:09:21,917][0m Trial 24 finished with value: 0.34527903257033254 and parameters: {'observation_period_num': 90, 'train_rates': 0.8704210260881003, 'learning_rate': 0.00019787171319536972, 'batch_size': 110, 'step_size': 9, 'gamma': 0.9338536288848992}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:10:48,595][0m Trial 25 finished with value: 0.3930133274623326 and parameters: {'observation_period_num': 53, 'train_rates': 0.8913673371526549, 'learning_rate': 2.7443251578175082e-05, 'batch_size': 54, 'step_size': 8, 'gamma': 0.8197360885776513}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:12:53,350][0m Trial 26 finished with value: 0.4747348205813803 and parameters: {'observation_period_num': 123, 'train_rates': 0.8313029447694666, 'learning_rate': 9.353047995837497e-05, 'batch_size': 90, 'step_size': 15, 'gamma': 0.9649997615223235}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:16:22,986][0m Trial 27 finished with value: 1.4568473379862936 and parameters: {'observation_period_num': 158, 'train_rates': 0.6601188996900827, 'learning_rate': 0.0004380985727947449, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8533816648122441}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:18:14,188][0m Trial 28 finished with value: 0.4212341004610062 and parameters: {'observation_period_num': 103, 'train_rates': 0.9473410498842458, 'learning_rate': 5.177677689954625e-05, 'batch_size': 117, 'step_size': 6, 'gamma': 0.8952904913118379}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:20:17,796][0m Trial 29 finished with value: 0.413693141502009 and parameters: {'observation_period_num': 127, 'train_rates': 0.8257547272263223, 'learning_rate': 0.00024146269087229607, 'batch_size': 160, 'step_size': 4, 'gamma': 0.9526058515237361}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:24:35,257][0m Trial 30 finished with value: 0.5704813095095763 and parameters: {'observation_period_num': 219, 'train_rates': 0.8835902912375819, 'learning_rate': 2.4679184336839855e-05, 'batch_size': 183, 'step_size': 7, 'gamma': 0.9171124868772914}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:26:10,712][0m Trial 31 finished with value: 0.3348470953523472 and parameters: {'observation_period_num': 91, 'train_rates': 0.8699503559624306, 'learning_rate': 0.00019853672947544115, 'batch_size': 103, 'step_size': 9, 'gamma': 0.9378831340025634}. Best is trial 22 with value: 0.27191793144111576.[0m
[32m[I 2025-02-05 08:27:01,545][0m Trial 32 finished with value: 0.2698452438820492 and parameters: {'observation_period_num': 46, 'train_rates': 0.9092240994388052, 'learning_rate': 0.00013409775577153657, 'batch_size': 129, 'step_size': 10, 'gamma': 0.9736953194878115}. Best is trial 32 with value: 0.2698452438820492.[0m
[32m[I 2025-02-05 08:27:41,374][0m Trial 33 finished with value: 1.3302710130151394 and parameters: {'observation_period_num': 5, 'train_rates': 0.9236000216962879, 'learning_rate': 1.1640830801238106e-06, 'batch_size': 127, 'step_size': 11, 'gamma': 0.9719155438575796}. Best is trial 32 with value: 0.2698452438820492.[0m
[32m[I 2025-02-05 08:29:28,314][0m Trial 34 finished with value: 0.2973308533798029 and parameters: {'observation_period_num': 31, 'train_rates': 0.9724517701776065, 'learning_rate': 0.0001319495325253212, 'batch_size': 47, 'step_size': 11, 'gamma': 0.980640387581573}. Best is trial 32 with value: 0.2698452438820492.[0m
[32m[I 2025-02-05 08:31:23,853][0m Trial 35 finished with value: 0.23828086256980896 and parameters: {'observation_period_num': 43, 'train_rates': 0.987950739233235, 'learning_rate': 0.0001357770702754184, 'batch_size': 44, 'step_size': 11, 'gamma': 0.9873164323157256}. Best is trial 35 with value: 0.23828086256980896.[0m
[32m[I 2025-02-05 08:36:31,417][0m Trial 36 finished with value: 0.3103343733521395 and parameters: {'observation_period_num': 44, 'train_rates': 0.9854569219660115, 'learning_rate': 0.0004302926406735784, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9562354364518066}. Best is trial 35 with value: 0.23828086256980896.[0m
[32m[I 2025-02-05 08:38:31,700][0m Trial 37 finished with value: 0.27112823707909234 and parameters: {'observation_period_num': 50, 'train_rates': 0.9392239739506995, 'learning_rate': 4.17375610900927e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9728963436340715}. Best is trial 35 with value: 0.23828086256980896.[0m
[32m[I 2025-02-05 08:40:36,917][0m Trial 38 finished with value: 0.2873484697078635 and parameters: {'observation_period_num': 49, 'train_rates': 0.9439081733233431, 'learning_rate': 1.4118482539813724e-05, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9694777482930272}. Best is trial 35 with value: 0.23828086256980896.[0m
[32m[I 2025-02-05 08:42:02,904][0m Trial 39 finished with value: 0.31666649627213433 and parameters: {'observation_period_num': 12, 'train_rates': 0.9659545905172678, 'learning_rate': 4.449890579947439e-05, 'batch_size': 58, 'step_size': 10, 'gamma': 0.8107804793802146}. Best is trial 35 with value: 0.23828086256980896.[0m
[32m[I 2025-02-05 08:44:53,158][0m Trial 40 finished with value: 0.2373812757040325 and parameters: {'observation_period_num': 34, 'train_rates': 0.935064810081649, 'learning_rate': 2.104417175186477e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.9895771316507851}. Best is trial 40 with value: 0.2373812757040325.[0m
[32m[I 2025-02-05 08:47:23,703][0m Trial 41 finished with value: 0.23586424688498178 and parameters: {'observation_period_num': 27, 'train_rates': 0.9343366725266882, 'learning_rate': 2.2340226588341073e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9735405351591387}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 08:49:31,087][0m Trial 42 finished with value: 0.4284231394012539 and parameters: {'observation_period_num': 34, 'train_rates': 0.9410049161147945, 'learning_rate': 8.85439167801094e-06, 'batch_size': 38, 'step_size': 9, 'gamma': 0.9706874985925773}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 08:51:58,692][0m Trial 43 finished with value: 0.26982725207195724 and parameters: {'observation_period_num': 59, 'train_rates': 0.9258442430082254, 'learning_rate': 2.0645379622963134e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.9893693646525513}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 08:53:15,744][0m Trial 44 finished with value: 0.36988203085604165 and parameters: {'observation_period_num': 62, 'train_rates': 0.9566954882492745, 'learning_rate': 1.981231488649741e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.9895429227172587}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 08:55:58,391][0m Trial 45 finished with value: 0.48209007497293405 and parameters: {'observation_period_num': 22, 'train_rates': 0.9150090323338884, 'learning_rate': 2.86606643383564e-06, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9532625381606759}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 08:57:20,496][0m Trial 46 finished with value: 0.4100773019923104 and parameters: {'observation_period_num': 40, 'train_rates': 0.9754727302316097, 'learning_rate': 9.578013856691238e-06, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9793742557604156}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 09:00:03,593][0m Trial 47 finished with value: 0.2789362503502231 and parameters: {'observation_period_num': 78, 'train_rates': 0.9278416622667992, 'learning_rate': 1.527011689155811e-05, 'batch_size': 29, 'step_size': 9, 'gamma': 0.957607636162233}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 09:00:59,613][0m Trial 48 finished with value: 0.9726905717569239 and parameters: {'observation_period_num': 61, 'train_rates': 0.8472709519116678, 'learning_rate': 6.817933463866581e-06, 'batch_size': 225, 'step_size': 11, 'gamma': 0.9862179683646104}. Best is trial 41 with value: 0.23586424688498178.[0m
[32m[I 2025-02-05 09:02:44,666][0m Trial 49 finished with value: 0.3093059988784008 and parameters: {'observation_period_num': 21, 'train_rates': 0.9586020716187292, 'learning_rate': 2.228029586383918e-05, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9432465065270195}. Best is trial 41 with value: 0.23586424688498178.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-05 09:02:44,674][0m A new study created in memory with name: no-name-c365ac0a-b0d3-43e4-9e22-02e608e73b64[0m
[32m[I 2025-02-05 09:05:04,043][0m Trial 0 finished with value: 1.7344482307679185 and parameters: {'observation_period_num': 154, 'train_rates': 0.61755692683315, 'learning_rate': 1.5144072724897767e-06, 'batch_size': 71, 'step_size': 1, 'gamma': 0.9388707175952465}. Best is trial 0 with value: 1.7344482307679185.[0m
[32m[I 2025-02-05 09:08:20,565][0m Trial 1 finished with value: 1.1690379216482765 and parameters: {'observation_period_num': 209, 'train_rates': 0.6051051126445951, 'learning_rate': 1.8522037422237428e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.890313276081316}. Best is trial 1 with value: 1.1690379216482765.[0m
[32m[I 2025-02-05 09:09:24,108][0m Trial 2 finished with value: 1.1774114710481272 and parameters: {'observation_period_num': 79, 'train_rates': 0.6158922011370016, 'learning_rate': 2.880262501967316e-05, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8326941165244645}. Best is trial 1 with value: 1.1690379216482765.[0m
[32m[I 2025-02-05 09:11:06,307][0m Trial 3 finished with value: 0.44271339290735257 and parameters: {'observation_period_num': 101, 'train_rates': 0.853477811705001, 'learning_rate': 5.7826161623872744e-05, 'batch_size': 109, 'step_size': 8, 'gamma': 0.9562149158401392}. Best is trial 3 with value: 0.44271339290735257.[0m
[32m[I 2025-02-05 09:12:14,463][0m Trial 4 finished with value: 1.0144906866429353 and parameters: {'observation_period_num': 79, 'train_rates': 0.6915161826003072, 'learning_rate': 0.0002408494152505017, 'batch_size': 241, 'step_size': 2, 'gamma': 0.8334650065097795}. Best is trial 3 with value: 0.44271339290735257.[0m
[32m[I 2025-02-05 09:13:00,951][0m Trial 5 finished with value: 1.3869730615615845 and parameters: {'observation_period_num': 41, 'train_rates': 0.8969655483782759, 'learning_rate': 1.628111105130394e-06, 'batch_size': 129, 'step_size': 3, 'gamma': 0.9716753889185399}. Best is trial 3 with value: 0.44271339290735257.[0m
[32m[I 2025-02-05 09:13:48,657][0m Trial 6 finished with value: 1.303923276718706 and parameters: {'observation_period_num': 64, 'train_rates': 0.6012228505895604, 'learning_rate': 7.715492471417109e-06, 'batch_size': 135, 'step_size': 6, 'gamma': 0.9267165485503458}. Best is trial 3 with value: 0.44271339290735257.[0m
[32m[I 2025-02-05 09:14:13,961][0m Trial 7 finished with value: 1.7937899739127579 and parameters: {'observation_period_num': 11, 'train_rates': 0.7718248256275899, 'learning_rate': 4.56189358658635e-06, 'batch_size': 175, 'step_size': 10, 'gamma': 0.8087987422992029}. Best is trial 3 with value: 0.44271339290735257.[0m
[32m[I 2025-02-05 09:18:53,775][0m Trial 8 finished with value: 0.3553434588365107 and parameters: {'observation_period_num': 219, 'train_rates': 0.9456599271161332, 'learning_rate': 3.777279008879495e-05, 'batch_size': 59, 'step_size': 12, 'gamma': 0.8693401835803587}. Best is trial 8 with value: 0.3553434588365107.[0m
[32m[I 2025-02-05 09:19:53,061][0m Trial 9 finished with value: 1.5590623925983635 and parameters: {'observation_period_num': 67, 'train_rates': 0.6935197106869349, 'learning_rate': 9.697245650304205e-06, 'batch_size': 190, 'step_size': 6, 'gamma': 0.7640720672631173}. Best is trial 8 with value: 0.3553434588365107.[0m
[32m[I 2025-02-05 09:26:12,419][0m Trial 10 finished with value: 0.2926575355231762 and parameters: {'observation_period_num': 245, 'train_rates': 0.9795166047789952, 'learning_rate': 0.0007410656982742887, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8847402504710595}. Best is trial 10 with value: 0.2926575355231762.[0m
[32m[I 2025-02-05 09:32:16,316][0m Trial 11 finished with value: 2.0669108292995353 and parameters: {'observation_period_num': 243, 'train_rates': 0.9856867048489123, 'learning_rate': 0.0006482446726550964, 'batch_size': 20, 'step_size': 15, 'gamma': 0.8754436974214072}. Best is trial 10 with value: 0.2926575355231762.[0m
[32m[I 2025-02-05 09:36:37,216][0m Trial 12 finished with value: 0.12272426691548578 and parameters: {'observation_period_num': 177, 'train_rates': 0.9898237105786685, 'learning_rate': 0.00010940143305559868, 'batch_size': 20, 'step_size': 13, 'gamma': 0.9006510760574518}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:41:04,639][0m Trial 13 finished with value: 0.18173468170257714 and parameters: {'observation_period_num': 154, 'train_rates': 0.9814378453666696, 'learning_rate': 0.00015449493513923116, 'batch_size': 18, 'step_size': 13, 'gamma': 0.9144574105119377}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:44:11,223][0m Trial 14 finished with value: 0.23647300166860996 and parameters: {'observation_period_num': 155, 'train_rates': 0.8967564836186522, 'learning_rate': 0.00012974957390686763, 'batch_size': 43, 'step_size': 13, 'gamma': 0.9127114078209876}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:47:11,301][0m Trial 15 finished with value: 0.46730266745735266 and parameters: {'observation_period_num': 172, 'train_rates': 0.8286089288808848, 'learning_rate': 0.00016116530500787573, 'batch_size': 93, 'step_size': 12, 'gamma': 0.9785030868881268}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:49:34,091][0m Trial 16 finished with value: 0.24081088094548744 and parameters: {'observation_period_num': 120, 'train_rates': 0.9223200918434036, 'learning_rate': 7.514511223609554e-05, 'batch_size': 42, 'step_size': 10, 'gamma': 0.905241003409786}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:52:43,358][0m Trial 17 finished with value: 0.5292355916938003 and parameters: {'observation_period_num': 190, 'train_rates': 0.7870701485904585, 'learning_rate': 0.00043228011097248896, 'batch_size': 172, 'step_size': 13, 'gamma': 0.8600928307828491}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:55:34,257][0m Trial 18 finished with value: 0.2679137076650347 and parameters: {'observation_period_num': 132, 'train_rates': 0.95549707097636, 'learning_rate': 0.00023669449572205007, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9460036399394254}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 09:58:57,060][0m Trial 19 finished with value: 0.4440316954523397 and parameters: {'observation_period_num': 190, 'train_rates': 0.8514570950594889, 'learning_rate': 8.861660454167664e-05, 'batch_size': 103, 'step_size': 13, 'gamma': 0.838810064406833}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:01:36,768][0m Trial 20 finished with value: 0.24917682779891023 and parameters: {'observation_period_num': 137, 'train_rates': 0.9039085753674097, 'learning_rate': 0.0002255865813189004, 'batch_size': 63, 'step_size': 11, 'gamma': 0.9143291149185125}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:04:58,363][0m Trial 21 finished with value: 0.25812932036139746 and parameters: {'observation_period_num': 161, 'train_rates': 0.9371859122830775, 'learning_rate': 0.0001213110546153848, 'batch_size': 40, 'step_size': 13, 'gamma': 0.9173999476281842}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:08:50,095][0m Trial 22 finished with value: 0.5465109722287047 and parameters: {'observation_period_num': 177, 'train_rates': 0.8897726247447518, 'learning_rate': 0.00035062159973186634, 'batch_size': 24, 'step_size': 14, 'gamma': 0.9009696808640991}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:11:55,582][0m Trial 23 finished with value: 0.2650946754871345 and parameters: {'observation_period_num': 147, 'train_rates': 0.9708463689757575, 'learning_rate': 4.875081808456168e-05, 'batch_size': 47, 'step_size': 11, 'gamma': 0.930960298524562}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:16:17,056][0m Trial 24 finished with value: 0.4773113013393637 and parameters: {'observation_period_num': 103, 'train_rates': 0.8790321522564438, 'learning_rate': 0.0001403495563339325, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9570782298288095}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:20:57,033][0m Trial 25 finished with value: 0.318619966506958 and parameters: {'observation_period_num': 211, 'train_rates': 0.9886260316530527, 'learning_rate': 1.6950478551031578e-05, 'batch_size': 55, 'step_size': 14, 'gamma': 0.8576821179733075}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:23:09,556][0m Trial 26 finished with value: 0.27816960722842116 and parameters: {'observation_period_num': 121, 'train_rates': 0.9171839807336396, 'learning_rate': 9.271510969156574e-05, 'batch_size': 82, 'step_size': 12, 'gamma': 0.894522846262373}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:27:17,937][0m Trial 27 finished with value: 0.6585432857275009 and parameters: {'observation_period_num': 191, 'train_rates': 0.9495077866994364, 'learning_rate': 0.00040471400992338755, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9861368893998014}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:30:13,265][0m Trial 28 finished with value: 1.186428043976832 and parameters: {'observation_period_num': 164, 'train_rates': 0.8156733142414131, 'learning_rate': 0.0009936017998139204, 'batch_size': 69, 'step_size': 14, 'gamma': 0.917369473139431}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:32:42,081][0m Trial 29 finished with value: 0.8622280671482995 and parameters: {'observation_period_num': 149, 'train_rates': 0.7305063481622702, 'learning_rate': 0.00014834604583962747, 'batch_size': 72, 'step_size': 4, 'gamma': 0.9441212323544956}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:34:26,410][0m Trial 30 finished with value: 0.6314355995574135 and parameters: {'observation_period_num': 104, 'train_rates': 0.8657429942780375, 'learning_rate': 3.105744583828783e-05, 'batch_size': 115, 'step_size': 11, 'gamma': 0.7817330544905919}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:36:59,213][0m Trial 31 finished with value: 0.27486644159345064 and parameters: {'observation_period_num': 123, 'train_rates': 0.9221235720895374, 'learning_rate': 7.663163882879583e-05, 'batch_size': 35, 'step_size': 10, 'gamma': 0.9032902302081317}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:40:01,180][0m Trial 32 finished with value: 0.26714812053574455 and parameters: {'observation_period_num': 150, 'train_rates': 0.9617470201095004, 'learning_rate': 5.8172965217311884e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.9055823901010875}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:43:29,868][0m Trial 33 finished with value: 0.3202888839128541 and parameters: {'observation_period_num': 182, 'train_rates': 0.9260131420971338, 'learning_rate': 9.23708617838379e-05, 'batch_size': 83, 'step_size': 9, 'gamma': 0.8840400942293221}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:45:46,367][0m Trial 34 finished with value: 0.3340055829319874 and parameters: {'observation_period_num': 117, 'train_rates': 0.9368568191985264, 'learning_rate': 2.5937063818146106e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9361348443892465}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:50:27,011][0m Trial 35 finished with value: 0.2701006202170482 and parameters: {'observation_period_num': 203, 'train_rates': 0.962430549628237, 'learning_rate': 5.2861767945770174e-05, 'batch_size': 29, 'step_size': 14, 'gamma': 0.9602649972732991}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:52:00,492][0m Trial 36 finished with value: 0.2800274568327357 and parameters: {'observation_period_num': 91, 'train_rates': 0.9069042934947115, 'learning_rate': 0.0001998881331325067, 'batch_size': 156, 'step_size': 11, 'gamma': 0.8513966453162616}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:54:38,158][0m Trial 37 finished with value: 0.5124059057305635 and parameters: {'observation_period_num': 139, 'train_rates': 0.8789347790404006, 'learning_rate': 1.8648464073451727e-05, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9240289039622429}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 10:58:22,197][0m Trial 38 finished with value: 1.0011709280663135 and parameters: {'observation_period_num': 229, 'train_rates': 0.649230174885986, 'learning_rate': 0.00032232823219260394, 'batch_size': 219, 'step_size': 15, 'gamma': 0.8757295090731115}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:01:27,680][0m Trial 39 finished with value: 0.4579779224900099 and parameters: {'observation_period_num': 165, 'train_rates': 0.8323307641615229, 'learning_rate': 0.00011342034910770164, 'batch_size': 51, 'step_size': 10, 'gamma': 0.9076155273323595}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:04:29,627][0m Trial 40 finished with value: 0.3331693708896637 and parameters: {'observation_period_num': 156, 'train_rates': 0.9722929445979144, 'learning_rate': 6.338467739921047e-05, 'batch_size': 124, 'step_size': 13, 'gamma': 0.8192696290076807}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:07:06,852][0m Trial 41 finished with value: 0.2422992693603254 and parameters: {'observation_period_num': 135, 'train_rates': 0.9019349293272707, 'learning_rate': 0.000218453094949171, 'batch_size': 63, 'step_size': 11, 'gamma': 0.9131457234627274}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:09:46,299][0m Trial 42 finished with value: 0.2327817399655619 and parameters: {'observation_period_num': 110, 'train_rates': 0.9348119323870215, 'learning_rate': 0.00017691509435691723, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8918167285288732}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:12:49,073][0m Trial 43 finished with value: 0.2573466145179488 and parameters: {'observation_period_num': 113, 'train_rates': 0.9381600869288521, 'learning_rate': 4.333877943749175e-05, 'batch_size': 26, 'step_size': 12, 'gamma': 0.8886865096741792}. Best is trial 12 with value: 0.12272426691548578.[0m
Early stopping at epoch 90
[32m[I 2025-02-05 11:14:46,067][0m Trial 44 finished with value: 1.7319087982177734 and parameters: {'observation_period_num': 86, 'train_rates': 0.9895936739489147, 'learning_rate': 1.197469741238343e-06, 'batch_size': 39, 'step_size': 1, 'gamma': 0.8920681536493864}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:19:28,319][0m Trial 45 finished with value: 0.2895272560119629 and parameters: {'observation_period_num': 69, 'train_rates': 0.9569376136758342, 'learning_rate': 0.0001748173574332349, 'batch_size': 17, 'step_size': 12, 'gamma': 0.928617330000382}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:21:58,480][0m Trial 46 finished with value: 0.5701698231111679 and parameters: {'observation_period_num': 48, 'train_rates': 0.9229535142758746, 'learning_rate': 0.0005311038220890867, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8734568028682371}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:23:30,361][0m Trial 47 finished with value: 0.4801685191390067 and parameters: {'observation_period_num': 92, 'train_rates': 0.8530053559745483, 'learning_rate': 0.0003016071077803499, 'batch_size': 92, 'step_size': 13, 'gamma': 0.8975336798903981}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:25:28,709][0m Trial 48 finished with value: 0.6194097048534852 and parameters: {'observation_period_num': 113, 'train_rates': 0.7691142932814504, 'learning_rate': 0.00011599532196272933, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9431626694764628}. Best is trial 12 with value: 0.12272426691548578.[0m
[32m[I 2025-02-05 11:29:24,438][0m Trial 49 finished with value: 0.2519666051504596 and parameters: {'observation_period_num': 170, 'train_rates': 0.9466978981920746, 'learning_rate': 7.770402094514826e-05, 'batch_size': 24, 'step_size': 15, 'gamma': 0.882725105963252}. Best is trial 12 with value: 0.12272426691548578.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-05 11:29:24,446][0m A new study created in memory with name: no-name-419ba1d9-f959-4ad4-9b67-8c3a17f5ef7b[0m
[32m[I 2025-02-05 11:33:12,490][0m Trial 0 finished with value: 1.9446096202650396 and parameters: {'observation_period_num': 225, 'train_rates': 0.6889061226629672, 'learning_rate': 2.1946346002163613e-06, 'batch_size': 125, 'step_size': 3, 'gamma': 0.8028234754573701}. Best is trial 0 with value: 1.9446096202650396.[0m
[32m[I 2025-02-05 11:34:02,947][0m Trial 1 finished with value: 1.1145037244438287 and parameters: {'observation_period_num': 60, 'train_rates': 0.7048639612681018, 'learning_rate': 1.3806495727508364e-05, 'batch_size': 241, 'step_size': 4, 'gamma': 0.9729813390499717}. Best is trial 1 with value: 1.1145037244438287.[0m
Early stopping at epoch 95
[32m[I 2025-02-05 11:35:18,214][0m Trial 2 finished with value: 2.0245917173437435 and parameters: {'observation_period_num': 88, 'train_rates': 0.7302752401257299, 'learning_rate': 1.6735306370495493e-05, 'batch_size': 161, 'step_size': 2, 'gamma': 0.7870847871627586}. Best is trial 1 with value: 1.1145037244438287.[0m
[32m[I 2025-02-05 11:36:34,981][0m Trial 3 finished with value: 0.501804281026125 and parameters: {'observation_period_num': 86, 'train_rates': 0.7991088114222662, 'learning_rate': 0.0008271517792731568, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8001020808328198}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:38:05,237][0m Trial 4 finished with value: 0.5623453748958737 and parameters: {'observation_period_num': 85, 'train_rates': 0.9378731194092695, 'learning_rate': 2.5347219435219073e-05, 'batch_size': 155, 'step_size': 11, 'gamma': 0.8978974174772386}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:41:13,079][0m Trial 5 finished with value: 0.8805242472043321 and parameters: {'observation_period_num': 193, 'train_rates': 0.682701196657469, 'learning_rate': 5.793122544012899e-05, 'batch_size': 176, 'step_size': 9, 'gamma': 0.9389357608619344}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:41:54,454][0m Trial 6 finished with value: 1.7665546732677926 and parameters: {'observation_period_num': 41, 'train_rates': 0.7214264848161427, 'learning_rate': 1.4240307400508397e-06, 'batch_size': 131, 'step_size': 13, 'gamma': 0.92522911640663}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:43:28,496][0m Trial 7 finished with value: 0.999855652372673 and parameters: {'observation_period_num': 89, 'train_rates': 0.6040008061655293, 'learning_rate': 0.00012696389399364006, 'batch_size': 38, 'step_size': 2, 'gamma': 0.9084021184702091}. Best is trial 3 with value: 0.501804281026125.[0m
Early stopping at epoch 61
[32m[I 2025-02-05 11:45:20,010][0m Trial 8 finished with value: 1.1547361836638501 and parameters: {'observation_period_num': 165, 'train_rates': 0.8665341993686158, 'learning_rate': 6.137864122742822e-05, 'batch_size': 122, 'step_size': 1, 'gamma': 0.7895935030485741}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:50:12,263][0m Trial 9 finished with value: 1.265208149624762 and parameters: {'observation_period_num': 227, 'train_rates': 0.9608716454304922, 'learning_rate': 1.2576341073732158e-06, 'batch_size': 86, 'step_size': 8, 'gamma': 0.938957803047121}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:52:37,226][0m Trial 10 finished with value: 0.6011020750710458 and parameters: {'observation_period_num': 142, 'train_rates': 0.8239949221865561, 'learning_rate': 0.0008549593966234025, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8383690984617528}. Best is trial 3 with value: 0.501804281026125.[0m
[32m[I 2025-02-05 11:54:27,467][0m Trial 11 finished with value: 0.26410725712776184 and parameters: {'observation_period_num': 105, 'train_rates': 0.9453404296129706, 'learning_rate': 0.0005120260540963586, 'batch_size': 201, 'step_size': 11, 'gamma': 0.8636072952564213}. Best is trial 11 with value: 0.26410725712776184.[0m
[32m[I 2025-02-05 11:54:52,018][0m Trial 12 finished with value: 0.4183666235383819 and parameters: {'observation_period_num': 17, 'train_rates': 0.8842224548177834, 'learning_rate': 0.000888380929120477, 'batch_size': 201, 'step_size': 10, 'gamma': 0.8465406244515569}. Best is trial 11 with value: 0.26410725712776184.[0m
[32m[I 2025-02-05 11:55:18,826][0m Trial 13 finished with value: 0.26254982283019057 and parameters: {'observation_period_num': 17, 'train_rates': 0.898983861587207, 'learning_rate': 0.00025317654240246423, 'batch_size': 195, 'step_size': 7, 'gamma': 0.8602465400496472}. Best is trial 13 with value: 0.26254982283019057.[0m
[32m[I 2025-02-05 11:55:48,652][0m Trial 14 finished with value: 0.1833314150571823 and parameters: {'observation_period_num': 11, 'train_rates': 0.9833693202562211, 'learning_rate': 0.00023678808870649713, 'batch_size': 188, 'step_size': 7, 'gamma': 0.8672395763753694}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 11:56:09,853][0m Trial 15 finished with value: 0.2803416735139386 and parameters: {'observation_period_num': 7, 'train_rates': 0.8818940848430161, 'learning_rate': 0.00019675751276227493, 'batch_size': 252, 'step_size': 6, 'gamma': 0.8841093133649585}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 11:56:56,494][0m Trial 16 finished with value: 0.25806310772895813 and parameters: {'observation_period_num': 41, 'train_rates': 0.9843466639440271, 'learning_rate': 0.00023138834790032377, 'batch_size': 187, 'step_size': 6, 'gamma': 0.7580686754727017}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 11:57:58,344][0m Trial 17 finished with value: 0.25633177161216736 and parameters: {'observation_period_num': 47, 'train_rates': 0.9866130404606696, 'learning_rate': 0.00032283844982188903, 'batch_size': 85, 'step_size': 5, 'gamma': 0.7587190817792738}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 11:58:59,353][0m Trial 18 finished with value: 1.2252686023712158 and parameters: {'observation_period_num': 39, 'train_rates': 0.9877448671772774, 'learning_rate': 5.824998032691312e-06, 'batch_size': 87, 'step_size': 5, 'gamma': 0.7549752020383288}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:01:48,956][0m Trial 19 finished with value: 0.2356429394283053 and parameters: {'observation_period_num': 130, 'train_rates': 0.9231082785617554, 'learning_rate': 8.058787772538079e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.822051330972103}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:04:38,135][0m Trial 20 finished with value: 0.24915994419471213 and parameters: {'observation_period_num': 134, 'train_rates': 0.9192633717937543, 'learning_rate': 9.057332241890934e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8307524636425874}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:09:25,903][0m Trial 21 finished with value: 0.2316913362776879 and parameters: {'observation_period_num': 136, 'train_rates': 0.9283763095350772, 'learning_rate': 8.474677809407968e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8255108778461407}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:13:22,787][0m Trial 22 finished with value: 0.3619790573480093 and parameters: {'observation_period_num': 164, 'train_rates': 0.8361235955788332, 'learning_rate': 5.2221761212557e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8168256022480934}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:15:29,173][0m Trial 23 finished with value: 0.25275418993741144 and parameters: {'observation_period_num': 114, 'train_rates': 0.918064927268281, 'learning_rate': 0.00012434166668296397, 'batch_size': 56, 'step_size': 9, 'gamma': 0.8782511959924283}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:18:29,350][0m Trial 24 finished with value: 0.501797352643574 and parameters: {'observation_period_num': 161, 'train_rates': 0.8541114780621528, 'learning_rate': 3.7329104578701054e-05, 'batch_size': 63, 'step_size': 9, 'gamma': 0.8220791253741594}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:23:25,199][0m Trial 25 finished with value: 0.1987191556381578 and parameters: {'observation_period_num': 120, 'train_rates': 0.9580029169665321, 'learning_rate': 0.00010288515246754288, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8506918995222096}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:28:04,089][0m Trial 26 finished with value: 0.7478180645771746 and parameters: {'observation_period_num': 251, 'train_rates': 0.7644754745682456, 'learning_rate': 0.0004240177496528617, 'batch_size': 102, 'step_size': 6, 'gamma': 0.8546197564076935}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:31:54,663][0m Trial 27 finished with value: 0.23292785950072176 and parameters: {'observation_period_num': 189, 'train_rates': 0.9599780283748478, 'learning_rate': 0.000185507182140336, 'batch_size': 68, 'step_size': 7, 'gamma': 0.8964938867092519}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:35:54,842][0m Trial 28 finished with value: 0.23353071533032319 and parameters: {'observation_period_num': 64, 'train_rates': 0.9538708127441721, 'learning_rate': 0.00012283281926573305, 'batch_size': 20, 'step_size': 4, 'gamma': 0.8790785477935358}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:39:44,377][0m Trial 29 finished with value: 1.2392115428092632 and parameters: {'observation_period_num': 191, 'train_rates': 0.8980258643353641, 'learning_rate': 3.9309640036219095e-06, 'batch_size': 50, 'step_size': 4, 'gamma': 0.810207623030402}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:42:37,719][0m Trial 30 finished with value: 0.8718149662017822 and parameters: {'observation_period_num': 151, 'train_rates': 0.9692967799398318, 'learning_rate': 3.587403191107215e-05, 'batch_size': 151, 'step_size': 5, 'gamma': 0.7760198332429337}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:46:21,389][0m Trial 31 finished with value: 0.2476877529679993 and parameters: {'observation_period_num': 187, 'train_rates': 0.941648385863307, 'learning_rate': 0.00020062007620899627, 'batch_size': 76, 'step_size': 7, 'gamma': 0.8928741377083962}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:50:59,499][0m Trial 32 finished with value: 0.5487003763516743 and parameters: {'observation_period_num': 211, 'train_rates': 0.967398130157613, 'learning_rate': 0.000482359668143042, 'batch_size': 46, 'step_size': 7, 'gamma': 0.9129590964601222}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:55:43,364][0m Trial 33 finished with value: 0.30885901946132466 and parameters: {'observation_period_num': 110, 'train_rates': 0.917084019781663, 'learning_rate': 0.00014997311318006308, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9710397931879287}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 12:59:15,522][0m Trial 34 finished with value: 0.40699729323387146 and parameters: {'observation_period_num': 179, 'train_rates': 0.9632807637976035, 'learning_rate': 8.138430690385443e-05, 'batch_size': 107, 'step_size': 8, 'gamma': 0.8479893154673823}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:03:45,162][0m Trial 35 finished with value: 0.8768841406878303 and parameters: {'observation_period_num': 214, 'train_rates': 0.938190455234862, 'learning_rate': 2.1179656817251106e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8668870346974732}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:05:50,846][0m Trial 36 finished with value: 0.8030656145678626 and parameters: {'observation_period_num': 66, 'train_rates': 0.7754443001163592, 'learning_rate': 1.0762729811341583e-05, 'batch_size': 33, 'step_size': 10, 'gamma': 0.8371018845396746}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:10:46,491][0m Trial 37 finished with value: 0.2004571408033371 and parameters: {'observation_period_num': 234, 'train_rates': 0.9893716845339532, 'learning_rate': 0.000318993323943495, 'batch_size': 174, 'step_size': 6, 'gamma': 0.7973747170633905}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:15:49,916][0m Trial 38 finished with value: 0.21136786043643951 and parameters: {'observation_period_num': 238, 'train_rates': 0.9884825883263435, 'learning_rate': 0.0006065821866256973, 'batch_size': 179, 'step_size': 6, 'gamma': 0.7978282166550188}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:20:44,902][0m Trial 39 finished with value: 0.2045656442642212 and parameters: {'observation_period_num': 234, 'train_rates': 0.9849045482087252, 'learning_rate': 0.0005574441363304385, 'batch_size': 173, 'step_size': 6, 'gamma': 0.7953407101427954}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:24:10,509][0m Trial 40 finished with value: 0.853171893191441 and parameters: {'observation_period_num': 211, 'train_rates': 0.6636223331909468, 'learning_rate': 0.0006451081694171984, 'batch_size': 161, 'step_size': 3, 'gamma': 0.7844917979613865}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:29:32,266][0m Trial 41 finished with value: 0.22795914113521576 and parameters: {'observation_period_num': 247, 'train_rates': 0.9769221916260434, 'learning_rate': 0.00034359442524395985, 'batch_size': 172, 'step_size': 6, 'gamma': 0.7975142703223187}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:34:25,915][0m Trial 42 finished with value: 0.22547324001789093 and parameters: {'observation_period_num': 232, 'train_rates': 0.985797673510227, 'learning_rate': 0.0006068080868386177, 'batch_size': 145, 'step_size': 5, 'gamma': 0.7766402136240538}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:39:27,565][0m Trial 43 finished with value: 0.38752561807632446 and parameters: {'observation_period_num': 239, 'train_rates': 0.9479988404816753, 'learning_rate': 0.0003282567296229553, 'batch_size': 217, 'step_size': 6, 'gamma': 0.8050086213602338}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:44:04,188][0m Trial 44 finished with value: 0.2705492377281189 and parameters: {'observation_period_num': 224, 'train_rates': 0.9894133463888561, 'learning_rate': 0.0007334725780061756, 'batch_size': 174, 'step_size': 4, 'gamma': 0.7949874271111904}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:48:11,804][0m Trial 45 finished with value: 0.27387911081314087 and parameters: {'observation_period_num': 201, 'train_rates': 0.9654438008566758, 'learning_rate': 0.00036635063520423463, 'batch_size': 186, 'step_size': 9, 'gamma': 0.7707918125747605}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:53:04,349][0m Trial 46 finished with value: 0.4068277026197724 and parameters: {'observation_period_num': 239, 'train_rates': 0.9008173833359365, 'learning_rate': 0.0005864044566123949, 'batch_size': 141, 'step_size': 5, 'gamma': 0.8100612872651516}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:54:28,793][0m Trial 47 finished with value: 0.3787022531032562 and parameters: {'observation_period_num': 81, 'train_rates': 0.946663599952838, 'learning_rate': 0.0008965525519116386, 'batch_size': 237, 'step_size': 6, 'gamma': 0.8384639221406331}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 13:57:51,579][0m Trial 48 finished with value: 1.0231247270074428 and parameters: {'observation_period_num': 219, 'train_rates': 0.6014972949530837, 'learning_rate': 0.0002608387840511379, 'batch_size': 124, 'step_size': 2, 'gamma': 0.788414518795424}. Best is trial 14 with value: 0.1833314150571823.[0m
[32m[I 2025-02-05 14:00:58,839][0m Trial 49 finished with value: 0.7742800994261407 and parameters: {'observation_period_num': 202, 'train_rates': 0.6552599372499055, 'learning_rate': 0.00016349156701008256, 'batch_size': 209, 'step_size': 7, 'gamma': 0.8499996872836503}. Best is trial 14 with value: 0.1833314150571823.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-05 14:00:58,845][0m A new study created in memory with name: no-name-61655d8d-6dce-4b7d-bbe2-69f3f3044173[0m
[32m[I 2025-02-05 14:06:10,362][0m Trial 0 finished with value: 1.5565165281295776 and parameters: {'observation_period_num': 242, 'train_rates': 0.9588988972160739, 'learning_rate': 1.3407942303171702e-06, 'batch_size': 162, 'step_size': 15, 'gamma': 0.9778826218312774}. Best is trial 0 with value: 1.5565165281295776.[0m
[32m[I 2025-02-05 14:08:15,938][0m Trial 1 finished with value: 1.0576344917524227 and parameters: {'observation_period_num': 124, 'train_rates': 0.9199273388711873, 'learning_rate': 6.364226845641116e-06, 'batch_size': 171, 'step_size': 6, 'gamma': 0.8891610321789151}. Best is trial 1 with value: 1.0576344917524227.[0m
[32m[I 2025-02-05 14:09:30,685][0m Trial 2 finished with value: 0.16806307435035706 and parameters: {'observation_period_num': 69, 'train_rates': 0.9875352832146834, 'learning_rate': 0.0002894095104319927, 'batch_size': 195, 'step_size': 9, 'gamma': 0.841599307599911}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:10:01,235][0m Trial 3 finished with value: 0.4849779184326926 and parameters: {'observation_period_num': 27, 'train_rates': 0.8203268870914198, 'learning_rate': 9.821206267489721e-05, 'batch_size': 176, 'step_size': 9, 'gamma': 0.9255413593755889}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:14:34,943][0m Trial 4 finished with value: 0.2281937152147293 and parameters: {'observation_period_num': 223, 'train_rates': 0.9465899792222942, 'learning_rate': 0.0003623866513832473, 'batch_size': 160, 'step_size': 3, 'gamma': 0.9771306069392148}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:16:56,454][0m Trial 5 finished with value: 0.4400182366371155 and parameters: {'observation_period_num': 128, 'train_rates': 0.9892092601966508, 'learning_rate': 2.856266933015317e-05, 'batch_size': 126, 'step_size': 6, 'gamma': 0.8746639844154241}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:21:51,703][0m Trial 6 finished with value: 1.328635616565314 and parameters: {'observation_period_num': 239, 'train_rates': 0.9065994968051012, 'learning_rate': 2.7210541313617883e-06, 'batch_size': 197, 'step_size': 13, 'gamma': 0.8426653754781013}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:23:08,257][0m Trial 7 finished with value: 0.5107204600515346 and parameters: {'observation_period_num': 48, 'train_rates': 0.8350494945779575, 'learning_rate': 0.00021710489257805772, 'batch_size': 58, 'step_size': 4, 'gamma': 0.788332692394089}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:26:41,380][0m Trial 8 finished with value: 1.8194766899093173 and parameters: {'observation_period_num': 226, 'train_rates': 0.6437892373425314, 'learning_rate': 3.0430383633096607e-06, 'batch_size': 159, 'step_size': 15, 'gamma': 0.9413758986558589}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:31:00,262][0m Trial 9 finished with value: 0.7564826973846981 and parameters: {'observation_period_num': 248, 'train_rates': 0.7411205243318066, 'learning_rate': 0.0002353788931963588, 'batch_size': 142, 'step_size': 4, 'gamma': 0.7957992815184814}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:32:16,714][0m Trial 10 finished with value: 0.8189459249964208 and parameters: {'observation_period_num': 87, 'train_rates': 0.72530080044172, 'learning_rate': 0.0008644548774664787, 'batch_size': 234, 'step_size': 10, 'gamma': 0.7532368150019708}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:35:32,903][0m Trial 11 finished with value: 0.7476955139263166 and parameters: {'observation_period_num': 173, 'train_rates': 0.8968950956970053, 'learning_rate': 0.0008474810368014228, 'batch_size': 97, 'step_size': 1, 'gamma': 0.9879255585269752}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:39:12,639][0m Trial 12 finished with value: 0.4829442799091339 and parameters: {'observation_period_num': 185, 'train_rates': 0.9808041633387633, 'learning_rate': 4.327194550313418e-05, 'batch_size': 250, 'step_size': 11, 'gamma': 0.8357328491482193}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:40:35,590][0m Trial 13 finished with value: 0.5644974052906037 and parameters: {'observation_period_num': 83, 'train_rates': 0.8798140998303225, 'learning_rate': 0.0002843656790990714, 'batch_size': 213, 'step_size': 1, 'gamma': 0.91614876366378}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:41:23,692][0m Trial 14 finished with value: 0.5964118783184559 and parameters: {'observation_period_num': 5, 'train_rates': 0.9416005962126338, 'learning_rate': 2.694097992533686e-05, 'batch_size': 105, 'step_size': 7, 'gamma': 0.8359716381896121}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:44:44,541][0m Trial 15 finished with value: 0.30506351525544473 and parameters: {'observation_period_num': 165, 'train_rates': 0.8676329810393861, 'learning_rate': 9.427897694946018e-05, 'batch_size': 32, 'step_size': 4, 'gamma': 0.9580269554742208}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:46:01,148][0m Trial 16 finished with value: 0.6919743340948353 and parameters: {'observation_period_num': 84, 'train_rates': 0.7594141202742354, 'learning_rate': 0.00043983704329682446, 'batch_size': 205, 'step_size': 12, 'gamma': 0.8939988422675396}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:46:49,405][0m Trial 17 finished with value: 0.8680390736507496 and parameters: {'observation_period_num': 55, 'train_rates': 0.6704162259833766, 'learning_rate': 9.407528438574448e-05, 'batch_size': 122, 'step_size': 8, 'gamma': 0.8546118530781037}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:51:09,903][0m Trial 18 finished with value: 1.2420730272435254 and parameters: {'observation_period_num': 212, 'train_rates': 0.941410414450038, 'learning_rate': 1.1885475898291082e-05, 'batch_size': 79, 'step_size': 3, 'gamma': 0.8068926775065635}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:53:34,675][0m Trial 19 finished with value: 0.6478636791923535 and parameters: {'observation_period_num': 150, 'train_rates': 0.781097987491952, 'learning_rate': 0.0005265856678785464, 'batch_size': 193, 'step_size': 9, 'gamma': 0.9081780330667538}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 14:57:16,780][0m Trial 20 finished with value: 0.5192565948984742 and parameters: {'observation_period_num': 200, 'train_rates': 0.8394132828811296, 'learning_rate': 0.00014984299749887177, 'batch_size': 225, 'step_size': 6, 'gamma': 0.81221561531516}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:00:31,769][0m Trial 21 finished with value: 0.30012821436577986 and parameters: {'observation_period_num': 160, 'train_rates': 0.8685587635234783, 'learning_rate': 6.008761270317014e-05, 'batch_size': 28, 'step_size': 3, 'gamma': 0.959573784966829}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:02:53,961][0m Trial 22 finished with value: 0.23593913813432058 and parameters: {'observation_period_num': 111, 'train_rates': 0.9524855474531697, 'learning_rate': 5.354879624857933e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9557088817850374}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:04:49,391][0m Trial 23 finished with value: 0.7640621662139893 and parameters: {'observation_period_num': 107, 'train_rates': 0.956716638432952, 'learning_rate': 1.7830075407498788e-05, 'batch_size': 141, 'step_size': 2, 'gamma': 0.959539750059152}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:06:58,787][0m Trial 24 finished with value: 0.23854464292526245 and parameters: {'observation_period_num': 107, 'train_rates': 0.9842678827386402, 'learning_rate': 0.0003691920907702234, 'batch_size': 57, 'step_size': 5, 'gamma': 0.9400792883122661}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:08:01,199][0m Trial 25 finished with value: 0.2713782382760071 and parameters: {'observation_period_num': 63, 'train_rates': 0.9285837675559516, 'learning_rate': 0.00015711939694187295, 'batch_size': 188, 'step_size': 2, 'gamma': 0.9753206387173924}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:10:45,406][0m Trial 26 finished with value: 0.2951893210411072 and parameters: {'observation_period_num': 143, 'train_rates': 0.9617107030900249, 'learning_rate': 0.0007203647995408775, 'batch_size': 149, 'step_size': 8, 'gamma': 0.8629832577759831}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:12:39,225][0m Trial 27 finished with value: 0.5137567260572987 and parameters: {'observation_period_num': 111, 'train_rates': 0.8910022498397622, 'learning_rate': 6.262565496871886e-05, 'batch_size': 110, 'step_size': 2, 'gamma': 0.9385365076049803}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:13:37,454][0m Trial 28 finished with value: 0.6103224351203868 and parameters: {'observation_period_num': 33, 'train_rates': 0.923461080297316, 'learning_rate': 9.140798620354093e-06, 'batch_size': 85, 'step_size': 5, 'gamma': 0.9897279127211579}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:14:34,141][0m Trial 29 finished with value: 1.036089592091187 and parameters: {'observation_period_num': 72, 'train_rates': 0.6019171088210028, 'learning_rate': 0.00017459810573814828, 'batch_size': 163, 'step_size': 13, 'gamma': 0.8871781978162397}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:19:02,438][0m Trial 30 finished with value: 0.4224658672298704 and parameters: {'observation_period_num': 190, 'train_rates': 0.9569483840008443, 'learning_rate': 0.00036333700444521143, 'batch_size': 19, 'step_size': 7, 'gamma': 0.7685356237085809}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:21:09,897][0m Trial 31 finished with value: 0.30585476756095886 and parameters: {'observation_period_num': 103, 'train_rates': 0.9863984072059897, 'learning_rate': 0.0004576771291503786, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9705480750776714}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:24:01,759][0m Trial 32 finished with value: 1.512614649884841 and parameters: {'observation_period_num': 140, 'train_rates': 0.9637174134333654, 'learning_rate': 1.0109204443143326e-06, 'batch_size': 60, 'step_size': 5, 'gamma': 0.9331467309960033}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:26:05,317][0m Trial 33 finished with value: 0.27014257477111175 and parameters: {'observation_period_num': 119, 'train_rates': 0.9160440380700001, 'learning_rate': 0.000323466453545194, 'batch_size': 177, 'step_size': 5, 'gamma': 0.9487956698592624}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:28:18,397][0m Trial 34 finished with value: 0.23086944832043213 and parameters: {'observation_period_num': 91, 'train_rates': 0.9849430879954388, 'learning_rate': 0.00012562084155296022, 'batch_size': 38, 'step_size': 1, 'gamma': 0.9226900171252826}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:30:18,515][0m Trial 35 finished with value: 0.43570346166105833 and parameters: {'observation_period_num': 96, 'train_rates': 0.940739674851909, 'learning_rate': 0.00011065486794955851, 'batch_size': 42, 'step_size': 1, 'gamma': 0.9180596443201161}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:35:17,738][0m Trial 36 finished with value: 0.2650154723061456 and parameters: {'observation_period_num': 130, 'train_rates': 0.9683816552931246, 'learning_rate': 6.17958026361182e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.895984764618563}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:36:20,473][0m Trial 37 finished with value: 0.42075162127895177 and parameters: {'observation_period_num': 42, 'train_rates': 0.8544146414500435, 'learning_rate': 4.17342227169107e-05, 'batch_size': 74, 'step_size': 10, 'gamma': 0.8806644841621313}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:37:33,724][0m Trial 38 finished with value: 0.35840567738026174 and parameters: {'observation_period_num': 68, 'train_rates': 0.9008706956747834, 'learning_rate': 0.00012309434269700082, 'batch_size': 128, 'step_size': 1, 'gamma': 0.9744118946242308}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:39:44,426][0m Trial 39 finished with value: 0.33946395211103486 and parameters: {'observation_period_num': 126, 'train_rates': 0.9276142675583177, 'learning_rate': 0.00019963238353045196, 'batch_size': 180, 'step_size': 3, 'gamma': 0.9019243221170488}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:40:11,723][0m Trial 40 finished with value: 0.4535499188346371 and parameters: {'observation_period_num': 25, 'train_rates': 0.8081084860085479, 'learning_rate': 0.0006332889620878592, 'batch_size': 217, 'step_size': 7, 'gamma': 0.9258091129927946}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:42:22,153][0m Trial 41 finished with value: 0.16932536114235314 and parameters: {'observation_period_num': 91, 'train_rates': 0.9831892210318739, 'learning_rate': 0.00024347886477802, 'batch_size': 38, 'step_size': 4, 'gamma': 0.9477956194911108}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:44:21,838][0m Trial 42 finished with value: 0.22585936442569451 and parameters: {'observation_period_num': 92, 'train_rates': 0.9718627026684246, 'learning_rate': 0.00027102300896895563, 'batch_size': 42, 'step_size': 4, 'gamma': 0.9517221518910843}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:45:55,103][0m Trial 43 finished with value: 0.28075445168896723 and parameters: {'observation_period_num': 75, 'train_rates': 0.9737499324165355, 'learning_rate': 0.00023622416951629487, 'batch_size': 66, 'step_size': 4, 'gamma': 0.9682357556241435}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:47:41,147][0m Trial 44 finished with value: 0.1918068379163742 and parameters: {'observation_period_num': 97, 'train_rates': 0.9877559279205363, 'learning_rate': 0.0002651479150820309, 'batch_size': 165, 'step_size': 6, 'gamma': 0.8222270806431141}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:52:47,230][0m Trial 45 finished with value: 0.2136031985282898 and parameters: {'observation_period_num': 235, 'train_rates': 0.9884143583732863, 'learning_rate': 0.0005732021165242286, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8423598951851997}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:53:44,876][0m Trial 46 finished with value: 0.32472753524780273 and parameters: {'observation_period_num': 53, 'train_rates': 0.9880542164946412, 'learning_rate': 0.0009010056781844044, 'batch_size': 164, 'step_size': 6, 'gamma': 0.8206008035773739}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:55:08,546][0m Trial 47 finished with value: 0.8676073250021333 and parameters: {'observation_period_num': 96, 'train_rates': 0.7002945264937053, 'learning_rate': 0.0005950466927335932, 'batch_size': 154, 'step_size': 9, 'gamma': 0.8506489880600695}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 15:59:54,863][0m Trial 48 finished with value: 0.2620569169521332 and parameters: {'observation_period_num': 232, 'train_rates': 0.970664531103507, 'learning_rate': 0.00023201993196802841, 'batch_size': 183, 'step_size': 7, 'gamma': 0.8241764954567457}. Best is trial 2 with value: 0.16806307435035706.[0m
[32m[I 2025-02-05 16:01:13,924][0m Trial 49 finished with value: 0.41676968336105347 and parameters: {'observation_period_num': 78, 'train_rates': 0.9366211443617378, 'learning_rate': 0.0009940782655358013, 'batch_size': 196, 'step_size': 4, 'gamma': 0.8648636130058748}. Best is trial 2 with value: 0.16806307435035706.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-05 16:01:13,932][0m A new study created in memory with name: no-name-3ea6c3e0-f8dd-4e14-a15e-87c9acc882e1[0m
[32m[I 2025-02-05 16:04:43,005][0m Trial 0 finished with value: 0.773165709842728 and parameters: {'observation_period_num': 199, 'train_rates': 0.7595723439753969, 'learning_rate': 0.00018144007369387487, 'batch_size': 143, 'step_size': 6, 'gamma': 0.7566147136672705}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:06:02,649][0m Trial 1 finished with value: 0.9747002443063606 and parameters: {'observation_period_num': 62, 'train_rates': 0.9646432183859377, 'learning_rate': 0.0007938902070762255, 'batch_size': 62, 'step_size': 12, 'gamma': 0.8604604744406977}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:09:52,344][0m Trial 2 finished with value: 0.885093242533869 and parameters: {'observation_period_num': 227, 'train_rates': 0.6922158428880925, 'learning_rate': 3.4645717596951144e-05, 'batch_size': 210, 'step_size': 10, 'gamma': 0.9299006657049345}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:11:41,010][0m Trial 3 finished with value: 1.4911076251900233 and parameters: {'observation_period_num': 55, 'train_rates': 0.6723518967786917, 'learning_rate': 0.0006835630240974147, 'batch_size': 34, 'step_size': 7, 'gamma': 0.961846023062194}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:16:09,334][0m Trial 4 finished with value: 1.213406290068771 and parameters: {'observation_period_num': 245, 'train_rates': 0.7562669654952204, 'learning_rate': 2.1644040146316675e-05, 'batch_size': 245, 'step_size': 15, 'gamma': 0.7780219607282608}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:18:34,423][0m Trial 5 finished with value: 1.4202812489932544 and parameters: {'observation_period_num': 145, 'train_rates': 0.7888008573388368, 'learning_rate': 7.842931828906475e-06, 'batch_size': 176, 'step_size': 1, 'gamma': 0.9476194970055686}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:22:53,280][0m Trial 6 finished with value: 1.1298109591007233 and parameters: {'observation_period_num': 220, 'train_rates': 0.8670226141007892, 'learning_rate': 2.942089665693741e-06, 'batch_size': 107, 'step_size': 12, 'gamma': 0.81298638859552}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:23:33,489][0m Trial 7 finished with value: 0.7855861476210297 and parameters: {'observation_period_num': 38, 'train_rates': 0.7906476986455431, 'learning_rate': 6.10821306830563e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.7884774127532205}. Best is trial 0 with value: 0.773165709842728.[0m
[32m[I 2025-02-05 16:26:33,353][0m Trial 8 finished with value: 0.4759329855442047 and parameters: {'observation_period_num': 156, 'train_rates': 0.9753654635656609, 'learning_rate': 5.453602604116694e-05, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9714181287941542}. Best is trial 8 with value: 0.4759329855442047.[0m
[32m[I 2025-02-05 16:28:13,993][0m Trial 9 finished with value: 0.9567436570380841 and parameters: {'observation_period_num': 60, 'train_rates': 0.6257724559994775, 'learning_rate': 1.4107371166489165e-05, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8700620171081813}. Best is trial 8 with value: 0.4759329855442047.[0m
[32m[I 2025-02-05 16:30:48,667][0m Trial 10 finished with value: 0.20125938951969147 and parameters: {'observation_period_num': 135, 'train_rates': 0.9850005192248273, 'learning_rate': 0.00014856281217190268, 'batch_size': 256, 'step_size': 1, 'gamma': 0.9878265123787945}. Best is trial 10 with value: 0.20125938951969147.[0m
[32m[I 2025-02-05 16:33:20,358][0m Trial 11 finished with value: 0.18817317485809326 and parameters: {'observation_period_num': 133, 'train_rates': 0.9835126135176265, 'learning_rate': 0.00014276026087968184, 'batch_size': 251, 'step_size': 1, 'gamma': 0.9885096420506229}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:35:12,060][0m Trial 12 finished with value: 0.2555365599691868 and parameters: {'observation_period_num': 111, 'train_rates': 0.9087707957836417, 'learning_rate': 0.00020064711755125346, 'batch_size': 195, 'step_size': 4, 'gamma': 0.9082610717369038}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:36:59,669][0m Trial 13 finished with value: 0.3193808004685811 and parameters: {'observation_period_num': 109, 'train_rates': 0.8771050149376757, 'learning_rate': 0.00017577334686772413, 'batch_size': 246, 'step_size': 4, 'gamma': 0.9850778195040532}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:40:11,026][0m Trial 14 finished with value: 0.2860726941069332 and parameters: {'observation_period_num': 174, 'train_rates': 0.9217388511387061, 'learning_rate': 0.00030781499337797016, 'batch_size': 177, 'step_size': 3, 'gamma': 0.9050661887658931}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:41:59,908][0m Trial 15 finished with value: 0.4312477461669756 and parameters: {'observation_period_num': 112, 'train_rates': 0.8463733681875281, 'learning_rate': 8.529109496542938e-05, 'batch_size': 209, 'step_size': 2, 'gamma': 0.9888240257472638}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:43:28,235][0m Trial 16 finished with value: 0.23305638134479523 and parameters: {'observation_period_num': 87, 'train_rates': 0.9337368835355128, 'learning_rate': 0.0004149060900113113, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9331940625633155}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:47:07,410][0m Trial 17 finished with value: 0.2844226360321045 and parameters: {'observation_period_num': 188, 'train_rates': 0.9775112613317175, 'learning_rate': 0.00011175281900877592, 'batch_size': 112, 'step_size': 3, 'gamma': 0.8743237172227215}. Best is trial 11 with value: 0.18817317485809326.[0m
Early stopping at epoch 54
[32m[I 2025-02-05 16:47:19,576][0m Trial 18 finished with value: 2.0256190557105866 and parameters: {'observation_period_num': 11, 'train_rates': 0.84420807214588, 'learning_rate': 1.1353498119359338e-06, 'batch_size': 225, 'step_size': 1, 'gamma': 0.8294485964341384}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:49:55,010][0m Trial 19 finished with value: 0.8555768728256226 and parameters: {'observation_period_num': 138, 'train_rates': 0.9875472973843995, 'learning_rate': 8.761559127444825e-06, 'batch_size': 171, 'step_size': 5, 'gamma': 0.899846736325012}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:53:10,331][0m Trial 20 finished with value: 0.27416198723243945 and parameters: {'observation_period_num': 167, 'train_rates': 0.9408845830273824, 'learning_rate': 0.0003556053757357099, 'batch_size': 85, 'step_size': 10, 'gamma': 0.9578635592809704}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:54:44,149][0m Trial 21 finished with value: 0.27106520533561707 and parameters: {'observation_period_num': 91, 'train_rates': 0.9341202126611899, 'learning_rate': 0.0004015195536905746, 'batch_size': 247, 'step_size': 3, 'gamma': 0.9332704496652279}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:56:05,587][0m Trial 22 finished with value: 0.3246845171369355 and parameters: {'observation_period_num': 81, 'train_rates': 0.8992722812126122, 'learning_rate': 0.0009034454260111614, 'batch_size': 252, 'step_size': 5, 'gamma': 0.9362988127541191}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:58:18,322][0m Trial 23 finished with value: 0.2748689353466034 and parameters: {'observation_period_num': 127, 'train_rates': 0.9448414324256528, 'learning_rate': 0.0001280886044094326, 'batch_size': 216, 'step_size': 1, 'gamma': 0.9887511482803546}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 16:59:50,335][0m Trial 24 finished with value: 0.25076955556869507 and parameters: {'observation_period_num': 92, 'train_rates': 0.9577005034013768, 'learning_rate': 0.0004308593414296348, 'batch_size': 255, 'step_size': 2, 'gamma': 0.9655472664184063}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 17:02:05,142][0m Trial 25 finished with value: 0.5532257866859436 and parameters: {'observation_period_num': 130, 'train_rates': 0.8936587024856397, 'learning_rate': 3.420537519220342e-05, 'batch_size': 224, 'step_size': 6, 'gamma': 0.9197245090784664}. Best is trial 11 with value: 0.18817317485809326.[0m
[32m[I 2025-02-05 17:03:27,801][0m Trial 26 finished with value: 0.1625359207391739 and parameters: {'observation_period_num': 80, 'train_rates': 0.9870912623529867, 'learning_rate': 0.0002606650889828518, 'batch_size': 200, 'step_size': 4, 'gamma': 0.9508769948992873}. Best is trial 26 with value: 0.1625359207391739.[0m
[32m[I 2025-02-05 17:04:01,286][0m Trial 27 finished with value: 0.49030016850462943 and parameters: {'observation_period_num': 32, 'train_rates': 0.8475178604656838, 'learning_rate': 7.119877463955447e-05, 'batch_size': 194, 'step_size': 2, 'gamma': 0.9706455874696411}. Best is trial 26 with value: 0.1625359207391739.[0m
[32m[I 2025-02-05 17:07:01,429][0m Trial 28 finished with value: 0.16772930324077606 and parameters: {'observation_period_num': 158, 'train_rates': 0.98893428770793, 'learning_rate': 0.00022949700711294972, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9464723297246723}. Best is trial 26 with value: 0.1625359207391739.[0m
[32m[I 2025-02-05 17:10:25,419][0m Trial 29 finished with value: 0.9128245713015116 and parameters: {'observation_period_num': 197, 'train_rates': 0.7436622879991643, 'learning_rate': 0.00025370643846764313, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8961178768706693}. Best is trial 26 with value: 0.1625359207391739.[0m
[32m[I 2025-02-05 17:13:51,453][0m Trial 30 finished with value: 0.23447231948375702 and parameters: {'observation_period_num': 177, 'train_rates': 0.9608114302993305, 'learning_rate': 0.000542429139378048, 'batch_size': 159, 'step_size': 4, 'gamma': 0.9519456100773959}. Best is trial 26 with value: 0.1625359207391739.[0m
[32m[I 2025-02-05 17:16:52,627][0m Trial 31 finished with value: 0.15591879189014435 and parameters: {'observation_period_num': 154, 'train_rates': 0.983759122713795, 'learning_rate': 0.0001522802078327584, 'batch_size': 128, 'step_size': 2, 'gamma': 0.9742301452043468}. Best is trial 31 with value: 0.15591879189014435.[0m
[32m[I 2025-02-05 17:19:49,880][0m Trial 32 finished with value: 0.28674882650375366 and parameters: {'observation_period_num': 153, 'train_rates': 0.9600347659314834, 'learning_rate': 0.00010707281610707094, 'batch_size': 123, 'step_size': 3, 'gamma': 0.945225058312876}. Best is trial 31 with value: 0.15591879189014435.[0m
[32m[I 2025-02-05 17:22:55,853][0m Trial 33 finished with value: 0.16504114866256714 and parameters: {'observation_period_num': 157, 'train_rates': 0.9858132770705564, 'learning_rate': 0.00020955318285639988, 'batch_size': 129, 'step_size': 2, 'gamma': 0.9748492372767722}. Best is trial 31 with value: 0.15591879189014435.[0m
[32m[I 2025-02-05 17:27:20,805][0m Trial 34 finished with value: 0.23585172860245956 and parameters: {'observation_period_num': 215, 'train_rates': 0.9168244780904898, 'learning_rate': 0.00021056236774699423, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9704816269841681}. Best is trial 31 with value: 0.15591879189014435.[0m
[32m[I 2025-02-05 17:30:21,076][0m Trial 35 finished with value: 0.2769099771976471 and parameters: {'observation_period_num': 160, 'train_rates': 0.9578420660516561, 'learning_rate': 0.0006593385206684839, 'batch_size': 127, 'step_size': 4, 'gamma': 0.9227257345877427}. Best is trial 31 with value: 0.15591879189014435.[0m
[32m[I 2025-02-05 17:33:20,143][0m Trial 36 finished with value: 1.032357192540764 and parameters: {'observation_period_num': 190, 'train_rates': 0.7246359104296569, 'learning_rate': 4.351199844075892e-05, 'batch_size': 160, 'step_size': 2, 'gamma': 0.9465028479055347}. Best is trial 31 with value: 0.15591879189014435.[0m
[32m[I 2025-02-05 17:37:48,022][0m Trial 37 finished with value: 0.14858661592006683 and parameters: {'observation_period_num': 209, 'train_rates': 0.9891252288601043, 'learning_rate': 0.0002936601291895537, 'batch_size': 95, 'step_size': 10, 'gamma': 0.8812938321756656}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 17:42:09,941][0m Trial 38 finished with value: 1.1588479116944148 and parameters: {'observation_period_num': 229, 'train_rates': 0.8154623757444375, 'learning_rate': 0.0009755160179170683, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8452963533006765}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 17:47:14,409][0m Trial 39 finished with value: 0.9598581391552883 and parameters: {'observation_period_num': 248, 'train_rates': 0.8903560205035589, 'learning_rate': 0.0005820778902868622, 'batch_size': 59, 'step_size': 11, 'gamma': 0.8780144003040713}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 17:48:14,805][0m Trial 40 finished with value: 1.0004532581112187 and parameters: {'observation_period_num': 72, 'train_rates': 0.6184256606502792, 'learning_rate': 2.5294207845781593e-05, 'batch_size': 103, 'step_size': 15, 'gamma': 0.8540711945190576}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 17:52:32,414][0m Trial 41 finished with value: 0.22893159091472626 and parameters: {'observation_period_num': 208, 'train_rates': 0.9690150180325161, 'learning_rate': 0.0002751456021019262, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8880138694332484}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 17:55:40,671][0m Trial 42 finished with value: 0.182593435049057 and parameters: {'observation_period_num': 153, 'train_rates': 0.9885166025819159, 'learning_rate': 0.00026623948947493355, 'batch_size': 70, 'step_size': 9, 'gamma': 0.7646567017863125}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:00:30,734][0m Trial 43 finished with value: 0.2704360678792 and parameters: {'observation_period_num': 230, 'train_rates': 0.9452053259971435, 'learning_rate': 9.307131236122618e-05, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9756493333682003}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:03:59,865][0m Trial 44 finished with value: 0.29866549372673035 and parameters: {'observation_period_num': 178, 'train_rates': 0.9668863680558939, 'learning_rate': 0.00018972079452391188, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9578996558799572}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:05:44,206][0m Trial 45 finished with value: 0.7951175863614871 and parameters: {'observation_period_num': 121, 'train_rates': 0.6775641395246125, 'learning_rate': 0.00015416919166654107, 'batch_size': 186, 'step_size': 9, 'gamma': 0.8059951314493575}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:08:33,260][0m Trial 46 finished with value: 0.4429475132999502 and parameters: {'observation_period_num': 147, 'train_rates': 0.9170489415699207, 'learning_rate': 0.0004830316218201336, 'batch_size': 98, 'step_size': 11, 'gamma': 0.9195088056881319}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:10:22,605][0m Trial 47 finished with value: 0.21642595529556274 and parameters: {'observation_period_num': 102, 'train_rates': 0.9865921313801628, 'learning_rate': 4.586989715993439e-05, 'batch_size': 122, 'step_size': 3, 'gamma': 0.9791789988061718}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:11:32,758][0m Trial 48 finished with value: 0.22673541763164465 and parameters: {'observation_period_num': 42, 'train_rates': 0.9295006403809347, 'learning_rate': 7.20502738818153e-05, 'batch_size': 71, 'step_size': 8, 'gamma': 0.9403531472535956}. Best is trial 37 with value: 0.14858661592006683.[0m
[32m[I 2025-02-05 18:14:03,630][0m Trial 49 finished with value: 1.1798420241952667 and parameters: {'observation_period_num': 167, 'train_rates': 0.6460730023623403, 'learning_rate': 0.0003282703636749095, 'batch_size': 150, 'step_size': 5, 'gamma': 0.9600436289721664}. Best is trial 37 with value: 0.14858661592006683.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-05 18:14:03,637][0m A new study created in memory with name: no-name-2745bc68-cff0-4806-9118-48210872e556[0m
[32m[I 2025-02-05 18:14:55,494][0m Trial 0 finished with value: 1.6681717270511691 and parameters: {'observation_period_num': 43, 'train_rates': 0.9189456662480671, 'learning_rate': 1.09561484938802e-06, 'batch_size': 100, 'step_size': 11, 'gamma': 0.8891557607145869}. Best is trial 0 with value: 1.6681717270511691.[0m
[32m[I 2025-02-05 18:16:15,129][0m Trial 1 finished with value: 0.8119615761060563 and parameters: {'observation_period_num': 88, 'train_rates': 0.6699824351117026, 'learning_rate': 0.00011475704674742963, 'batch_size': 93, 'step_size': 15, 'gamma': 0.8993561785179828}. Best is trial 1 with value: 0.8119615761060563.[0m
Early stopping at epoch 70
[32m[I 2025-02-05 18:18:50,751][0m Trial 2 finished with value: 1.851393699645996 and parameters: {'observation_period_num': 225, 'train_rates': 0.6371813697061616, 'learning_rate': 3.7452826926834136e-06, 'batch_size': 120, 'step_size': 1, 'gamma': 0.8651240095671625}. Best is trial 1 with value: 0.8119615761060563.[0m
[32m[I 2025-02-05 18:19:39,747][0m Trial 3 finished with value: 1.9174965620040894 and parameters: {'observation_period_num': 49, 'train_rates': 0.9258924025151668, 'learning_rate': 2.493166870417992e-06, 'batch_size': 224, 'step_size': 11, 'gamma': 0.8453608095475045}. Best is trial 1 with value: 0.8119615761060563.[0m
Early stopping at epoch 68
[32m[I 2025-02-05 18:22:57,317][0m Trial 4 finished with value: 2.5207014083862305 and parameters: {'observation_period_num': 225, 'train_rates': 0.9651939910010043, 'learning_rate': 2.9753387730625128e-06, 'batch_size': 253, 'step_size': 2, 'gamma': 0.7549414401076718}. Best is trial 1 with value: 0.8119615761060563.[0m
[32m[I 2025-02-05 18:24:10,782][0m Trial 5 finished with value: 0.6640313706899944 and parameters: {'observation_period_num': 74, 'train_rates': 0.8613312279911605, 'learning_rate': 0.00022957951774633383, 'batch_size': 231, 'step_size': 3, 'gamma': 0.7517347757416027}. Best is trial 5 with value: 0.6640313706899944.[0m
[32m[I 2025-02-05 18:25:39,172][0m Trial 6 finished with value: 0.4663403778877158 and parameters: {'observation_period_num': 88, 'train_rates': 0.8669948971373086, 'learning_rate': 4.768982148641826e-05, 'batch_size': 142, 'step_size': 6, 'gamma': 0.9494432819783952}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:29:24,979][0m Trial 7 finished with value: 1.7639854807344335 and parameters: {'observation_period_num': 219, 'train_rates': 0.7158560534783286, 'learning_rate': 1.6422492377820741e-06, 'batch_size': 104, 'step_size': 5, 'gamma': 0.9076352457832676}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:31:11,468][0m Trial 8 finished with value: 0.6883330483417994 and parameters: {'observation_period_num': 104, 'train_rates': 0.8200277242948921, 'learning_rate': 1.4705195484940876e-05, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9296378243998837}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:34:29,193][0m Trial 9 finished with value: 0.9931433692984625 and parameters: {'observation_period_num': 214, 'train_rates': 0.6017546100753656, 'learning_rate': 0.000298020842071923, 'batch_size': 180, 'step_size': 14, 'gamma': 0.756575720536803}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:37:18,070][0m Trial 10 finished with value: 0.6931619947059371 and parameters: {'observation_period_num': 154, 'train_rates': 0.7594462271232237, 'learning_rate': 4.4365855734821206e-05, 'batch_size': 40, 'step_size': 6, 'gamma': 0.9895462712433326}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:39:54,708][0m Trial 11 finished with value: 0.6307121721597818 and parameters: {'observation_period_num': 151, 'train_rates': 0.8330612823969495, 'learning_rate': 0.0008031827677793301, 'batch_size': 171, 'step_size': 4, 'gamma': 0.8119597969486322}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:42:40,638][0m Trial 12 finished with value: 0.8047851220299216 and parameters: {'observation_period_num': 156, 'train_rates': 0.8479914234493593, 'learning_rate': 0.0009521524058828877, 'batch_size': 164, 'step_size': 7, 'gamma': 0.8190864902945063}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:45:08,296][0m Trial 13 finished with value: 1.5750039412002814 and parameters: {'observation_period_num': 148, 'train_rates': 0.783357283686344, 'learning_rate': 1.3087433883598648e-05, 'batch_size': 162, 'step_size': 4, 'gamma': 0.8096247322334621}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:47:08,278][0m Trial 14 finished with value: 0.6007289401071514 and parameters: {'observation_period_num': 124, 'train_rates': 0.8820262522482674, 'learning_rate': 0.000977578687512282, 'batch_size': 189, 'step_size': 9, 'gamma': 0.9627589997968216}. Best is trial 6 with value: 0.4663403778877158.[0m
[32m[I 2025-02-05 18:47:34,963][0m Trial 15 finished with value: 0.3804172850862334 and parameters: {'observation_period_num': 18, 'train_rates': 0.8925343902035114, 'learning_rate': 5.336107156721176e-05, 'batch_size': 200, 'step_size': 9, 'gamma': 0.9693125026088735}. Best is trial 15 with value: 0.3804172850862334.[0m
[32m[I 2025-02-05 18:48:13,384][0m Trial 16 finished with value: 0.3736916780471802 and parameters: {'observation_period_num': 10, 'train_rates': 0.9688675136298924, 'learning_rate': 4.892388446949217e-05, 'batch_size': 139, 'step_size': 11, 'gamma': 0.9389904694250804}. Best is trial 16 with value: 0.3736916780471802.[0m
[32m[I 2025-02-05 18:48:40,388][0m Trial 17 finished with value: 0.6840406060218811 and parameters: {'observation_period_num': 5, 'train_rates': 0.9690387635764526, 'learning_rate': 1.564516707387768e-05, 'batch_size': 202, 'step_size': 12, 'gamma': 0.9810296741313579}. Best is trial 16 with value: 0.3736916780471802.[0m
[32m[I 2025-02-05 18:49:18,105][0m Trial 18 finished with value: 0.18020817637443542 and parameters: {'observation_period_num': 6, 'train_rates': 0.9890348201365353, 'learning_rate': 0.00011129206239879321, 'batch_size': 143, 'step_size': 13, 'gamma': 0.9287866523867513}. Best is trial 18 with value: 0.18020817637443542.[0m
[32m[I 2025-02-05 18:50:05,633][0m Trial 19 finished with value: 0.20148859918117523 and parameters: {'observation_period_num': 42, 'train_rates': 0.974032412400938, 'learning_rate': 0.0001313816836731472, 'batch_size': 140, 'step_size': 13, 'gamma': 0.9315452214354438}. Best is trial 18 with value: 0.18020817637443542.[0m
[32m[I 2025-02-05 18:51:25,835][0m Trial 20 finished with value: 0.27136730030179024 and parameters: {'observation_period_num': 32, 'train_rates': 0.9301255895451227, 'learning_rate': 0.00015725756837960738, 'batch_size': 61, 'step_size': 13, 'gamma': 0.9228613868255571}. Best is trial 18 with value: 0.18020817637443542.[0m
[32m[I 2025-02-05 18:56:17,228][0m Trial 21 finished with value: 0.2422971607640732 and parameters: {'observation_period_num': 46, 'train_rates': 0.9852531389982503, 'learning_rate': 0.00013746346879520594, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9229768913175879}. Best is trial 18 with value: 0.18020817637443542.[0m
[32m[I 2025-02-05 18:57:33,932][0m Trial 22 finished with value: 0.19573311507701874 and parameters: {'observation_period_num': 68, 'train_rates': 0.9829142396281451, 'learning_rate': 9.356676120579384e-05, 'batch_size': 127, 'step_size': 15, 'gamma': 0.8811515846563397}. Best is trial 18 with value: 0.18020817637443542.[0m
[32m[I 2025-02-05 18:58:37,234][0m Trial 23 finished with value: 0.1768062710762024 and parameters: {'observation_period_num': 58, 'train_rates': 0.9899364055839437, 'learning_rate': 0.0004611532152884647, 'batch_size': 124, 'step_size': 15, 'gamma': 0.8760399876068905}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 18:59:56,244][0m Trial 24 finished with value: 0.5628749365833673 and parameters: {'observation_period_num': 73, 'train_rates': 0.9388574858533453, 'learning_rate': 0.00039075185656291256, 'batch_size': 125, 'step_size': 15, 'gamma': 0.8763155686961563}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:01:18,304][0m Trial 25 finished with value: 0.24952970445156097 and parameters: {'observation_period_num': 67, 'train_rates': 0.9897057179348563, 'learning_rate': 0.0004764500714893967, 'batch_size': 76, 'step_size': 15, 'gamma': 0.8510258789542987}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:03:21,435][0m Trial 26 finished with value: 0.37489781954458784 and parameters: {'observation_period_num': 119, 'train_rates': 0.9112629574444436, 'learning_rate': 8.61800102109594e-05, 'batch_size': 153, 'step_size': 14, 'gamma': 0.881066399270422}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:04:08,792][0m Trial 27 finished with value: 0.6410863548517227 and parameters: {'observation_period_num': 23, 'train_rates': 0.9401054040530581, 'learning_rate': 2.2625376539752643e-05, 'batch_size': 110, 'step_size': 14, 'gamma': 0.83429608983673}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:05:15,466][0m Trial 28 finished with value: 0.49803109265941237 and parameters: {'observation_period_num': 64, 'train_rates': 0.9002081019473115, 'learning_rate': 0.0005255257223060542, 'batch_size': 82, 'step_size': 12, 'gamma': 0.9082974988410359}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:08:44,176][0m Trial 29 finished with value: 0.27575980450293813 and parameters: {'observation_period_num': 178, 'train_rates': 0.9538094110268888, 'learning_rate': 0.00022307884801800533, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8914380998303351}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:10:29,358][0m Trial 30 finished with value: 0.3135098281659578 and parameters: {'observation_period_num': 98, 'train_rates': 0.913772830963249, 'learning_rate': 8.495977916362924e-05, 'batch_size': 94, 'step_size': 15, 'gamma': 0.868990958058552}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:11:13,799][0m Trial 31 finished with value: 0.21238376200199127 and parameters: {'observation_period_num': 37, 'train_rates': 0.9897169366427152, 'learning_rate': 8.23352219137292e-05, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9516050289396215}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:12:10,233][0m Trial 32 finished with value: 0.3707853211296929 and parameters: {'observation_period_num': 52, 'train_rates': 0.9534824662193782, 'learning_rate': 0.0001852987062691831, 'batch_size': 132, 'step_size': 12, 'gamma': 0.9069490241585908}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:12:55,917][0m Trial 33 finished with value: 0.3496650703779356 and parameters: {'observation_period_num': 29, 'train_rates': 0.9565759015045572, 'learning_rate': 0.00010117084030769525, 'batch_size': 112, 'step_size': 14, 'gamma': 0.8947373223762295}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:13:44,814][0m Trial 34 finished with value: 0.7410157150473476 and parameters: {'observation_period_num': 55, 'train_rates': 0.7077594520414012, 'learning_rate': 0.00034769486667306085, 'batch_size': 148, 'step_size': 10, 'gamma': 0.8571719149732464}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:15:15,149][0m Trial 35 finished with value: 0.5564156147566709 and parameters: {'observation_period_num': 84, 'train_rates': 0.9386817767333449, 'learning_rate': 2.8554473921088735e-05, 'batch_size': 119, 'step_size': 15, 'gamma': 0.7898822005700485}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:16:09,609][0m Trial 36 finished with value: 0.8179589509963989 and parameters: {'observation_period_num': 39, 'train_rates': 0.9745947112212328, 'learning_rate': 6.555760823607611e-06, 'batch_size': 96, 'step_size': 13, 'gamma': 0.8836220534146962}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:16:48,288][0m Trial 37 finished with value: 0.3077423475358797 and parameters: {'observation_period_num': 18, 'train_rates': 0.921624884935581, 'learning_rate': 0.0001321896694151219, 'batch_size': 130, 'step_size': 14, 'gamma': 0.9376371402354455}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:21:42,151][0m Trial 38 finished with value: 0.40442428622712095 and parameters: {'observation_period_num': 243, 'train_rates': 0.8831787985556787, 'learning_rate': 0.0005445381122682682, 'batch_size': 174, 'step_size': 12, 'gamma': 0.8379148812381538}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:22:44,167][0m Trial 39 finished with value: 0.3128833770751953 and parameters: {'observation_period_num': 59, 'train_rates': 0.9588026599014596, 'learning_rate': 0.00027602173136070994, 'batch_size': 161, 'step_size': 10, 'gamma': 0.9127395476466715}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:23:56,870][0m Trial 40 finished with value: 0.9279825689675358 and parameters: {'observation_period_num': 83, 'train_rates': 0.6564378634343984, 'learning_rate': 0.0001921568410431545, 'batch_size': 105, 'step_size': 15, 'gamma': 0.8668059897183646}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:24:43,867][0m Trial 41 finished with value: 0.2125544548034668 and parameters: {'observation_period_num': 41, 'train_rates': 0.9888494614627903, 'learning_rate': 6.655781374459216e-05, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9526087924561648}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:25:26,641][0m Trial 42 finished with value: 0.18978163599967957 and parameters: {'observation_period_num': 34, 'train_rates': 0.9887677467400551, 'learning_rate': 6.903588957157449e-05, 'batch_size': 137, 'step_size': 13, 'gamma': 0.9504682923357558}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:26:04,830][0m Trial 43 finished with value: 0.4473268389701843 and parameters: {'observation_period_num': 12, 'train_rates': 0.9711265368659157, 'learning_rate': 3.605145685948502e-05, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9384363001924355}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:28:00,299][0m Trial 44 finished with value: 0.37551073467030244 and parameters: {'observation_period_num': 107, 'train_rates': 0.9463954027046705, 'learning_rate': 0.00012201681916281975, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9636449744284449}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:28:55,934][0m Trial 45 finished with value: 0.8698771595954895 and parameters: {'observation_period_num': 52, 'train_rates': 0.9717069894739158, 'learning_rate': 6.514204873505998e-05, 'batch_size': 159, 'step_size': 1, 'gamma': 0.8977703302958898}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:29:25,951][0m Trial 46 finished with value: 0.8338516022389134 and parameters: {'observation_period_num': 29, 'train_rates': 0.7371779718965743, 'learning_rate': 2.894859712119715e-05, 'batch_size': 187, 'step_size': 14, 'gamma': 0.9207817108558567}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:30:45,301][0m Trial 47 finished with value: 0.31607023257518485 and parameters: {'observation_period_num': 74, 'train_rates': 0.9257310609597051, 'learning_rate': 0.0002449379248438023, 'batch_size': 135, 'step_size': 10, 'gamma': 0.9327947098769092}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:31:37,587][0m Trial 48 finished with value: 0.8531772289459186 and parameters: {'observation_period_num': 26, 'train_rates': 0.8131358831749017, 'learning_rate': 0.0006763759516994346, 'batch_size': 87, 'step_size': 13, 'gamma': 0.9739040890367556}. Best is trial 23 with value: 0.1768062710762024.[0m
[32m[I 2025-02-05 19:33:15,576][0m Trial 49 finished with value: 0.41875632237969784 and parameters: {'observation_period_num': 92, 'train_rates': 0.8606347889205279, 'learning_rate': 5.662067153016888e-05, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9602867604970434}. Best is trial 23 with value: 0.1768062710762024.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 27, 'train_rates': 0.9343366725266882, 'learning_rate': 2.2340226588341073e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9735405351591387}
Epoch 1/300, trend Loss: 1.0375 | 2.1962
Epoch 2/300, trend Loss: 0.6904 | 1.5892
Epoch 3/300, trend Loss: 0.5565 | 1.3627
Epoch 4/300, trend Loss: 0.4839 | 1.2424
Epoch 5/300, trend Loss: 0.4279 | 1.1591
Epoch 6/300, trend Loss: 0.3895 | 1.0978
Epoch 7/300, trend Loss: 0.3624 | 1.0460
Epoch 8/300, trend Loss: 0.3411 | 1.0056
Epoch 9/300, trend Loss: 0.3220 | 0.9570
Epoch 10/300, trend Loss: 0.3064 | 0.9178
Epoch 11/300, trend Loss: 0.2943 | 0.8852
Epoch 12/300, trend Loss: 0.2844 | 0.8575
Epoch 13/300, trend Loss: 0.2757 | 0.8308
Epoch 14/300, trend Loss: 0.2676 | 0.8030
Epoch 15/300, trend Loss: 0.2592 | 0.7740
Epoch 16/300, trend Loss: 0.2527 | 0.7487
Epoch 17/300, trend Loss: 0.2469 | 0.7291
Epoch 18/300, trend Loss: 0.2414 | 0.7163
Epoch 19/300, trend Loss: 0.2362 | 0.6997
Epoch 20/300, trend Loss: 0.2343 | 0.6711
Epoch 21/300, trend Loss: 0.2301 | 0.6545
Epoch 22/300, trend Loss: 0.2274 | 0.6443
Epoch 23/300, trend Loss: 0.2243 | 0.6417
Epoch 24/300, trend Loss: 0.2197 | 0.6136
Epoch 25/300, trend Loss: 0.2189 | 0.5919
Epoch 26/300, trend Loss: 0.2156 | 0.5825
Epoch 27/300, trend Loss: 0.2133 | 0.5796
Epoch 28/300, trend Loss: 0.2106 | 0.5620
Epoch 29/300, trend Loss: 0.2071 | 0.5440
Epoch 30/300, trend Loss: 0.2084 | 0.5495
Epoch 31/300, trend Loss: 0.2094 | 0.5251
Epoch 32/300, trend Loss: 0.2140 | 0.5243
Epoch 33/300, trend Loss: 0.2046 | 0.5071
Epoch 34/300, trend Loss: 0.1991 | 0.5008
Epoch 35/300, trend Loss: 0.2004 | 0.5012
Epoch 36/300, trend Loss: 0.1912 | 0.4860
Epoch 37/300, trend Loss: 0.1926 | 0.4734
Epoch 38/300, trend Loss: 0.1880 | 0.4728
Epoch 39/300, trend Loss: 0.1856 | 0.4535
Epoch 40/300, trend Loss: 0.1834 | 0.4485
Epoch 41/300, trend Loss: 0.1794 | 0.4415
Epoch 42/300, trend Loss: 0.1788 | 0.4336
Epoch 43/300, trend Loss: 0.1757 | 0.4258
Epoch 44/300, trend Loss: 0.1749 | 0.4220
Epoch 45/300, trend Loss: 0.1739 | 0.4180
Epoch 46/300, trend Loss: 0.1725 | 0.4058
Epoch 47/300, trend Loss: 0.1726 | 0.4037
Epoch 48/300, trend Loss: 0.1726 | 0.4069
Epoch 49/300, trend Loss: 0.1764 | 0.4011
Epoch 50/300, trend Loss: 0.1725 | 0.3830
Epoch 51/300, trend Loss: 0.1749 | 0.3913
Epoch 52/300, trend Loss: 0.1786 | 0.4026
Epoch 53/300, trend Loss: 0.1711 | 0.3866
Epoch 54/300, trend Loss: 0.1745 | 0.3727
Epoch 55/300, trend Loss: 0.1681 | 0.3714
Epoch 56/300, trend Loss: 0.1680 | 0.3723
Epoch 57/300, trend Loss: 0.1655 | 0.3548
Epoch 58/300, trend Loss: 0.1610 | 0.3517
Epoch 59/300, trend Loss: 0.1612 | 0.3525
Epoch 60/300, trend Loss: 0.1581 | 0.3450
Epoch 61/300, trend Loss: 0.1564 | 0.3415
Epoch 62/300, trend Loss: 0.1556 | 0.3388
Epoch 63/300, trend Loss: 0.1546 | 0.3353
Epoch 64/300, trend Loss: 0.1528 | 0.3299
Epoch 65/300, trend Loss: 0.1532 | 0.3272
Epoch 66/300, trend Loss: 0.1520 | 0.3310
Epoch 67/300, trend Loss: 0.1542 | 0.3186
Epoch 68/300, trend Loss: 0.1530 | 0.3246
Epoch 69/300, trend Loss: 0.1521 | 0.3114
Epoch 70/300, trend Loss: 0.1541 | 0.3126
Epoch 71/300, trend Loss: 0.1499 | 0.3081
Epoch 72/300, trend Loss: 0.1513 | 0.3122
Epoch 73/300, trend Loss: 0.1482 | 0.3004
Epoch 74/300, trend Loss: 0.1486 | 0.2975
Epoch 75/300, trend Loss: 0.1500 | 0.3180
Epoch 76/300, trend Loss: 0.1463 | 0.3045
Epoch 77/300, trend Loss: 0.1489 | 0.2940
Epoch 78/300, trend Loss: 0.1469 | 0.2967
Epoch 79/300, trend Loss: 0.1475 | 0.3027
Epoch 80/300, trend Loss: 0.1451 | 0.2854
Epoch 81/300, trend Loss: 0.1429 | 0.2812
Epoch 82/300, trend Loss: 0.1444 | 0.2892
Epoch 83/300, trend Loss: 0.1402 | 0.2833
Epoch 84/300, trend Loss: 0.1403 | 0.2731
Epoch 85/300, trend Loss: 0.1391 | 0.2735
Epoch 86/300, trend Loss: 0.1380 | 0.2800
Epoch 87/300, trend Loss: 0.1370 | 0.2707
Epoch 88/300, trend Loss: 0.1367 | 0.2659
Epoch 89/300, trend Loss: 0.1368 | 0.2711
Epoch 90/300, trend Loss: 0.1356 | 0.2688
Epoch 91/300, trend Loss: 0.1350 | 0.2656
Epoch 92/300, trend Loss: 0.1361 | 0.2617
Epoch 93/300, trend Loss: 0.1345 | 0.2634
Epoch 94/300, trend Loss: 0.1339 | 0.2631
Epoch 95/300, trend Loss: 0.1338 | 0.2625
Epoch 96/300, trend Loss: 0.1338 | 0.2517
Epoch 97/300, trend Loss: 0.1327 | 0.2583
Epoch 98/300, trend Loss: 0.1339 | 0.2631
Epoch 99/300, trend Loss: 0.1319 | 0.2550
Epoch 100/300, trend Loss: 0.1319 | 0.2485
Epoch 101/300, trend Loss: 0.1324 | 0.2562
Epoch 102/300, trend Loss: 0.1317 | 0.2488
Epoch 103/300, trend Loss: 0.1310 | 0.2545
Epoch 104/300, trend Loss: 0.1296 | 0.2425
Epoch 105/300, trend Loss: 0.1314 | 0.2390
Epoch 106/300, trend Loss: 0.1295 | 0.2441
Epoch 107/300, trend Loss: 0.1306 | 0.2570
Epoch 108/300, trend Loss: 0.1289 | 0.2381
Epoch 109/300, trend Loss: 0.1305 | 0.2325
Epoch 110/300, trend Loss: 0.1296 | 0.2536
Epoch 111/300, trend Loss: 0.1271 | 0.2436
Epoch 112/300, trend Loss: 0.1270 | 0.2330
Epoch 113/300, trend Loss: 0.1255 | 0.2306
Epoch 114/300, trend Loss: 0.1267 | 0.2488
Epoch 115/300, trend Loss: 0.1237 | 0.2345
Epoch 116/300, trend Loss: 0.1241 | 0.2278
Epoch 117/300, trend Loss: 0.1239 | 0.2295
Epoch 118/300, trend Loss: 0.1234 | 0.2348
Epoch 119/300, trend Loss: 0.1234 | 0.2326
Epoch 120/300, trend Loss: 0.1217 | 0.2225
Epoch 121/300, trend Loss: 0.1225 | 0.2264
Epoch 122/300, trend Loss: 0.1216 | 0.2334
Epoch 123/300, trend Loss: 0.1208 | 0.2242
Epoch 124/300, trend Loss: 0.1209 | 0.2206
Epoch 125/300, trend Loss: 0.1208 | 0.2223
Epoch 126/300, trend Loss: 0.1199 | 0.2266
Epoch 127/300, trend Loss: 0.1197 | 0.2195
Epoch 128/300, trend Loss: 0.1191 | 0.2161
Epoch 129/300, trend Loss: 0.1189 | 0.2202
Epoch 130/300, trend Loss: 0.1193 | 0.2246
Epoch 131/300, trend Loss: 0.1181 | 0.2160
Epoch 132/300, trend Loss: 0.1192 | 0.2140
Epoch 133/300, trend Loss: 0.1175 | 0.2188
Epoch 134/300, trend Loss: 0.1180 | 0.2208
Epoch 135/300, trend Loss: 0.1162 | 0.2133
Epoch 136/300, trend Loss: 0.1176 | 0.2111
Epoch 137/300, trend Loss: 0.1166 | 0.2152
Epoch 138/300, trend Loss: 0.1166 | 0.2158
Epoch 139/300, trend Loss: 0.1152 | 0.2102
Epoch 140/300, trend Loss: 0.1155 | 0.2082
Epoch 141/300, trend Loss: 0.1146 | 0.2086
Epoch 142/300, trend Loss: 0.1150 | 0.2124
Epoch 143/300, trend Loss: 0.1140 | 0.2078
Epoch 144/300, trend Loss: 0.1130 | 0.2073
Epoch 145/300, trend Loss: 0.1134 | 0.2087
Epoch 146/300, trend Loss: 0.1131 | 0.2067
Epoch 147/300, trend Loss: 0.1124 | 0.2039
Epoch 148/300, trend Loss: 0.1128 | 0.2046
Epoch 149/300, trend Loss: 0.1124 | 0.2076
Epoch 150/300, trend Loss: 0.1122 | 0.2040
Epoch 151/300, trend Loss: 0.1119 | 0.2066
Epoch 152/300, trend Loss: 0.1120 | 0.2029
Epoch 153/300, trend Loss: 0.1116 | 0.2011
Epoch 154/300, trend Loss: 0.1110 | 0.2040
Epoch 155/300, trend Loss: 0.1117 | 0.2047
Epoch 156/300, trend Loss: 0.1109 | 0.2063
Epoch 157/300, trend Loss: 0.1113 | 0.2001
Epoch 158/300, trend Loss: 0.1113 | 0.1975
Epoch 159/300, trend Loss: 0.1106 | 0.2032
Epoch 160/300, trend Loss: 0.1098 | 0.2055
Epoch 161/300, trend Loss: 0.1096 | 0.1970
Epoch 162/300, trend Loss: 0.1105 | 0.1937
Epoch 163/300, trend Loss: 0.1111 | 0.2010
Epoch 164/300, trend Loss: 0.1097 | 0.2084
Epoch 165/300, trend Loss: 0.1095 | 0.1924
Epoch 166/300, trend Loss: 0.1101 | 0.1923
Epoch 167/300, trend Loss: 0.1090 | 0.2016
Epoch 168/300, trend Loss: 0.1083 | 0.2022
Epoch 169/300, trend Loss: 0.1082 | 0.1928
Epoch 170/300, trend Loss: 0.1082 | 0.1905
Epoch 171/300, trend Loss: 0.1080 | 0.2018
Epoch 172/300, trend Loss: 0.1085 | 0.2002
Epoch 173/300, trend Loss: 0.1074 | 0.1904
Epoch 174/300, trend Loss: 0.1065 | 0.1900
Epoch 175/300, trend Loss: 0.1071 | 0.2017
Epoch 176/300, trend Loss: 0.1067 | 0.1958
Epoch 177/300, trend Loss: 0.1068 | 0.1917
Epoch 178/300, trend Loss: 0.1065 | 0.1955
Epoch 179/300, trend Loss: 0.1069 | 0.1968
Epoch 180/300, trend Loss: 0.1051 | 0.1925
Epoch 181/300, trend Loss: 0.1053 | 0.1917
Epoch 182/300, trend Loss: 0.1054 | 0.1923
Epoch 183/300, trend Loss: 0.1046 | 0.1922
Epoch 184/300, trend Loss: 0.1050 | 0.1948
Epoch 185/300, trend Loss: 0.1051 | 0.1900
Epoch 186/300, trend Loss: 0.1041 | 0.1861
Epoch 187/300, trend Loss: 0.1039 | 0.1910
Epoch 188/300, trend Loss: 0.1048 | 0.1951
Epoch 189/300, trend Loss: 0.1038 | 0.1908
Epoch 190/300, trend Loss: 0.1038 | 0.1843
Epoch 191/300, trend Loss: 0.1042 | 0.1900
Epoch 192/300, trend Loss: 0.1040 | 0.1926
Epoch 193/300, trend Loss: 0.1031 | 0.1901
Epoch 194/300, trend Loss: 0.1031 | 0.1845
Epoch 195/300, trend Loss: 0.1030 | 0.1862
Epoch 196/300, trend Loss: 0.1028 | 0.1946
Epoch 197/300, trend Loss: 0.1029 | 0.1880
Epoch 198/300, trend Loss: 0.1028 | 0.1816
Epoch 199/300, trend Loss: 0.1017 | 0.1841
Epoch 200/300, trend Loss: 0.1022 | 0.1938
Epoch 201/300, trend Loss: 0.1019 | 0.1854
Epoch 202/300, trend Loss: 0.1019 | 0.1816
Epoch 203/300, trend Loss: 0.1025 | 0.1837
Epoch 204/300, trend Loss: 0.1022 | 0.1928
Epoch 205/300, trend Loss: 0.1009 | 0.1857
Epoch 206/300, trend Loss: 0.1014 | 0.1809
Epoch 207/300, trend Loss: 0.1010 | 0.1873
Epoch 208/300, trend Loss: 0.1006 | 0.1874
Epoch 209/300, trend Loss: 0.1000 | 0.1815
Epoch 210/300, trend Loss: 0.1010 | 0.1850
Epoch 211/300, trend Loss: 0.1002 | 0.1815
Epoch 212/300, trend Loss: 0.0996 | 0.1832
Epoch 213/300, trend Loss: 0.1001 | 0.1861
Epoch 214/300, trend Loss: 0.0994 | 0.1845
Epoch 215/300, trend Loss: 0.0995 | 0.1833
Epoch 216/300, trend Loss: 0.0993 | 0.1788
Epoch 217/300, trend Loss: 0.0991 | 0.1840
Epoch 218/300, trend Loss: 0.0990 | 0.1855
Epoch 219/300, trend Loss: 0.0991 | 0.1805
Epoch 220/300, trend Loss: 0.0990 | 0.1796
Epoch 221/300, trend Loss: 0.0982 | 0.1846
Epoch 222/300, trend Loss: 0.0986 | 0.1827
Epoch 223/300, trend Loss: 0.0989 | 0.1796
Epoch 224/300, trend Loss: 0.0988 | 0.1790
Epoch 225/300, trend Loss: 0.0989 | 0.1807
Epoch 226/300, trend Loss: 0.0986 | 0.1806
Epoch 227/300, trend Loss: 0.0986 | 0.1795
Epoch 228/300, trend Loss: 0.0981 | 0.1799
Epoch 229/300, trend Loss: 0.0975 | 0.1797
Epoch 230/300, trend Loss: 0.0976 | 0.1824
Epoch 231/300, trend Loss: 0.0973 | 0.1793
Epoch 232/300, trend Loss: 0.0974 | 0.1775
Epoch 233/300, trend Loss: 0.0981 | 0.1817
Epoch 234/300, trend Loss: 0.0968 | 0.1780
Epoch 235/300, trend Loss: 0.0972 | 0.1767
Epoch 236/300, trend Loss: 0.0973 | 0.1800
Epoch 237/300, trend Loss: 0.0973 | 0.1793
Epoch 238/300, trend Loss: 0.0965 | 0.1766
Epoch 239/300, trend Loss: 0.0968 | 0.1776
Epoch 240/300, trend Loss: 0.0966 | 0.1785
Epoch 241/300, trend Loss: 0.0959 | 0.1785
Epoch 242/300, trend Loss: 0.0958 | 0.1772
Epoch 243/300, trend Loss: 0.0966 | 0.1768
Epoch 244/300, trend Loss: 0.0966 | 0.1757
Epoch 245/300, trend Loss: 0.0958 | 0.1780
Epoch 246/300, trend Loss: 0.0957 | 0.1752
Epoch 247/300, trend Loss: 0.0958 | 0.1778
Epoch 248/300, trend Loss: 0.0957 | 0.1782
Epoch 249/300, trend Loss: 0.0951 | 0.1775
Epoch 250/300, trend Loss: 0.0957 | 0.1749
Epoch 251/300, trend Loss: 0.0956 | 0.1747
Epoch 252/300, trend Loss: 0.0949 | 0.1762
Epoch 253/300, trend Loss: 0.0949 | 0.1777
Epoch 254/300, trend Loss: 0.0946 | 0.1750
Epoch 255/300, trend Loss: 0.0953 | 0.1755
Epoch 256/300, trend Loss: 0.0946 | 0.1760
Epoch 257/300, trend Loss: 0.0938 | 0.1789
Epoch 258/300, trend Loss: 0.0942 | 0.1756
Epoch 259/300, trend Loss: 0.0945 | 0.1759
Epoch 260/300, trend Loss: 0.0943 | 0.1759
Epoch 261/300, trend Loss: 0.0942 | 0.1762
Epoch 262/300, trend Loss: 0.0946 | 0.1732
Epoch 263/300, trend Loss: 0.0941 | 0.1739
Epoch 264/300, trend Loss: 0.0939 | 0.1756
Epoch 265/300, trend Loss: 0.0937 | 0.1749
Epoch 266/300, trend Loss: 0.0941 | 0.1729
Epoch 267/300, trend Loss: 0.0938 | 0.1755
Epoch 268/300, trend Loss: 0.0938 | 0.1763
Epoch 269/300, trend Loss: 0.0935 | 0.1732
Epoch 270/300, trend Loss: 0.0934 | 0.1718
Epoch 271/300, trend Loss: 0.0938 | 0.1745
Epoch 272/300, trend Loss: 0.0931 | 0.1785
Epoch 273/300, trend Loss: 0.0937 | 0.1729
Epoch 274/300, trend Loss: 0.0930 | 0.1738
Epoch 275/300, trend Loss: 0.0934 | 0.1769
Epoch 276/300, trend Loss: 0.0926 | 0.1726
Epoch 277/300, trend Loss: 0.0924 | 0.1720
Epoch 278/300, trend Loss: 0.0921 | 0.1743
Epoch 279/300, trend Loss: 0.0923 | 0.1736
Epoch 280/300, trend Loss: 0.0926 | 0.1719
Epoch 281/300, trend Loss: 0.0926 | 0.1725
Epoch 282/300, trend Loss: 0.0923 | 0.1721
Epoch 283/300, trend Loss: 0.0924 | 0.1715
Epoch 284/300, trend Loss: 0.0926 | 0.1760
Epoch 285/300, trend Loss: 0.0921 | 0.1721
Epoch 286/300, trend Loss: 0.0916 | 0.1734
Epoch 287/300, trend Loss: 0.0923 | 0.1742
Epoch 288/300, trend Loss: 0.0921 | 0.1729
Epoch 289/300, trend Loss: 0.0924 | 0.1712
Epoch 290/300, trend Loss: 0.0910 | 0.1717
Epoch 291/300, trend Loss: 0.0911 | 0.1737
Epoch 292/300, trend Loss: 0.0912 | 0.1720
Epoch 293/300, trend Loss: 0.0908 | 0.1709
Epoch 294/300, trend Loss: 0.0920 | 0.1736
Epoch 295/300, trend Loss: 0.0911 | 0.1716
Epoch 296/300, trend Loss: 0.0912 | 0.1737
Epoch 297/300, trend Loss: 0.0914 | 0.1703
Epoch 298/300, trend Loss: 0.0915 | 0.1727
Epoch 299/300, trend Loss: 0.0912 | 0.1710
Epoch 300/300, trend Loss: 0.0911 | 0.1724
Training seasonal_0 component with params: {'observation_period_num': 177, 'train_rates': 0.9898237105786685, 'learning_rate': 0.00010940143305559868, 'batch_size': 20, 'step_size': 13, 'gamma': 0.9006510760574518}
Epoch 1/300, seasonal_0 Loss: 0.6078 | 0.8856
Epoch 2/300, seasonal_0 Loss: 0.4579 | 0.6621
Epoch 3/300, seasonal_0 Loss: 0.3973 | 0.6053
Epoch 4/300, seasonal_0 Loss: 0.3538 | 0.4997
Epoch 5/300, seasonal_0 Loss: 0.3212 | 0.4342
Epoch 6/300, seasonal_0 Loss: 0.3074 | 0.4047
Epoch 7/300, seasonal_0 Loss: 0.2961 | 0.3898
Epoch 8/300, seasonal_0 Loss: 0.2872 | 0.3503
Epoch 9/300, seasonal_0 Loss: 0.2736 | 0.3803
Epoch 10/300, seasonal_0 Loss: 0.2577 | 0.3051
Epoch 11/300, seasonal_0 Loss: 0.2616 | 0.3207
Epoch 12/300, seasonal_0 Loss: 0.2796 | 0.3913
Epoch 13/300, seasonal_0 Loss: 0.2607 | 0.3464
Epoch 14/300, seasonal_0 Loss: 0.2764 | 0.2816
Epoch 15/300, seasonal_0 Loss: 0.2666 | 0.2811
Epoch 16/300, seasonal_0 Loss: 0.2498 | 0.2859
Epoch 17/300, seasonal_0 Loss: 0.2441 | 0.2836
Epoch 18/300, seasonal_0 Loss: 0.2543 | 0.2472
Epoch 19/300, seasonal_0 Loss: 0.2585 | 0.3065
Epoch 20/300, seasonal_0 Loss: 0.2738 | 0.2697
Epoch 21/300, seasonal_0 Loss: 0.2369 | 0.2662
Epoch 22/300, seasonal_0 Loss: 0.2233 | 0.2428
Epoch 23/300, seasonal_0 Loss: 0.2334 | 0.2471
Epoch 24/300, seasonal_0 Loss: 0.2246 | 0.2373
Epoch 25/300, seasonal_0 Loss: 0.2036 | 0.2519
Epoch 26/300, seasonal_0 Loss: 0.1968 | 0.2888
Epoch 27/300, seasonal_0 Loss: 0.1972 | 0.2239
Epoch 28/300, seasonal_0 Loss: 0.1920 | 0.2150
Epoch 29/300, seasonal_0 Loss: 0.1900 | 0.2365
Epoch 30/300, seasonal_0 Loss: 0.1838 | 0.2375
Epoch 31/300, seasonal_0 Loss: 0.1762 | 0.2070
Epoch 32/300, seasonal_0 Loss: 0.1745 | 0.2030
Epoch 33/300, seasonal_0 Loss: 0.1702 | 0.2004
Epoch 34/300, seasonal_0 Loss: 0.1746 | 0.2044
Epoch 35/300, seasonal_0 Loss: 0.1693 | 0.2001
Epoch 36/300, seasonal_0 Loss: 0.1746 | 0.2012
Epoch 37/300, seasonal_0 Loss: 0.1711 | 0.2011
Epoch 38/300, seasonal_0 Loss: 0.1692 | 0.2020
Epoch 39/300, seasonal_0 Loss: 0.1693 | 0.2004
Epoch 40/300, seasonal_0 Loss: 0.1720 | 0.1936
Epoch 41/300, seasonal_0 Loss: 0.1728 | 0.2143
Epoch 42/300, seasonal_0 Loss: 0.1716 | 0.1918
Epoch 43/300, seasonal_0 Loss: 0.1695 | 0.1837
Epoch 44/300, seasonal_0 Loss: 0.1693 | 0.1905
Epoch 45/300, seasonal_0 Loss: 0.1665 | 0.2088
Epoch 46/300, seasonal_0 Loss: 0.1588 | 0.1903
Epoch 47/300, seasonal_0 Loss: 0.1587 | 0.1586
Epoch 48/300, seasonal_0 Loss: 0.1525 | 0.1859
Epoch 49/300, seasonal_0 Loss: 0.1528 | 0.1850
Epoch 50/300, seasonal_0 Loss: 0.1498 | 0.1919
Epoch 51/300, seasonal_0 Loss: 0.1511 | 0.1778
Epoch 52/300, seasonal_0 Loss: 0.1536 | 0.1525
Epoch 53/300, seasonal_0 Loss: 0.1549 | 0.1831
Epoch 54/300, seasonal_0 Loss: 0.1493 | 0.1932
Epoch 55/300, seasonal_0 Loss: 0.1514 | 0.1870
Epoch 56/300, seasonal_0 Loss: 0.1481 | 0.1539
Epoch 57/300, seasonal_0 Loss: 0.1567 | 0.1591
Epoch 58/300, seasonal_0 Loss: 0.1500 | 0.1903
Epoch 59/300, seasonal_0 Loss: 0.1432 | 0.1725
Epoch 60/300, seasonal_0 Loss: 0.1367 | 0.1583
Epoch 61/300, seasonal_0 Loss: 0.1352 | 0.1480
Epoch 62/300, seasonal_0 Loss: 0.1324 | 0.1447
Epoch 63/300, seasonal_0 Loss: 0.1293 | 0.1522
Epoch 64/300, seasonal_0 Loss: 0.1293 | 0.1685
Epoch 65/300, seasonal_0 Loss: 0.1266 | 0.1568
Epoch 66/300, seasonal_0 Loss: 0.1284 | 0.1417
Epoch 67/300, seasonal_0 Loss: 0.1252 | 0.1401
Epoch 68/300, seasonal_0 Loss: 0.1242 | 0.1522
Epoch 69/300, seasonal_0 Loss: 0.1236 | 0.1603
Epoch 70/300, seasonal_0 Loss: 0.1242 | 0.1625
Epoch 71/300, seasonal_0 Loss: 0.1247 | 0.1483
Epoch 72/300, seasonal_0 Loss: 0.1246 | 0.1366
Epoch 73/300, seasonal_0 Loss: 0.1245 | 0.1478
Epoch 74/300, seasonal_0 Loss: 0.1261 | 0.1794
Epoch 75/300, seasonal_0 Loss: 0.1268 | 0.1555
Epoch 76/300, seasonal_0 Loss: 0.1238 | 0.1458
Epoch 77/300, seasonal_0 Loss: 0.1255 | 0.1324
Epoch 78/300, seasonal_0 Loss: 0.1211 | 0.1465
Epoch 79/300, seasonal_0 Loss: 0.1207 | 0.1633
Epoch 80/300, seasonal_0 Loss: 0.1201 | 0.1453
Epoch 81/300, seasonal_0 Loss: 0.1168 | 0.1317
Epoch 82/300, seasonal_0 Loss: 0.1195 | 0.1372
Epoch 83/300, seasonal_0 Loss: 0.1182 | 0.1493
Epoch 84/300, seasonal_0 Loss: 0.1156 | 0.1456
Epoch 85/300, seasonal_0 Loss: 0.1166 | 0.1345
Epoch 86/300, seasonal_0 Loss: 0.1129 | 0.1415
Epoch 87/300, seasonal_0 Loss: 0.1136 | 0.1480
Epoch 88/300, seasonal_0 Loss: 0.1115 | 0.1355
Epoch 89/300, seasonal_0 Loss: 0.1103 | 0.1244
Epoch 90/300, seasonal_0 Loss: 0.1117 | 0.1366
Epoch 91/300, seasonal_0 Loss: 0.1099 | 0.1603
Epoch 92/300, seasonal_0 Loss: 0.1107 | 0.1467
Epoch 93/300, seasonal_0 Loss: 0.1095 | 0.1271
Epoch 94/300, seasonal_0 Loss: 0.1082 | 0.1241
Epoch 95/300, seasonal_0 Loss: 0.1079 | 0.1395
Epoch 96/300, seasonal_0 Loss: 0.1092 | 0.1508
Epoch 97/300, seasonal_0 Loss: 0.1069 | 0.1383
Epoch 98/300, seasonal_0 Loss: 0.1061 | 0.1252
Epoch 99/300, seasonal_0 Loss: 0.1069 | 0.1258
Epoch 100/300, seasonal_0 Loss: 0.1049 | 0.1401
Epoch 101/300, seasonal_0 Loss: 0.1048 | 0.1360
Epoch 102/300, seasonal_0 Loss: 0.1049 | 0.1294
Epoch 103/300, seasonal_0 Loss: 0.1029 | 0.1246
Epoch 104/300, seasonal_0 Loss: 0.1028 | 0.1321
Epoch 105/300, seasonal_0 Loss: 0.1023 | 0.1379
Epoch 106/300, seasonal_0 Loss: 0.1013 | 0.1294
Epoch 107/300, seasonal_0 Loss: 0.1000 | 0.1256
Epoch 108/300, seasonal_0 Loss: 0.0993 | 0.1282
Epoch 109/300, seasonal_0 Loss: 0.0991 | 0.1290
Epoch 110/300, seasonal_0 Loss: 0.0988 | 0.1277
Epoch 111/300, seasonal_0 Loss: 0.0977 | 0.1258
Epoch 112/300, seasonal_0 Loss: 0.0963 | 0.1249
Epoch 113/300, seasonal_0 Loss: 0.0959 | 0.1216
Epoch 114/300, seasonal_0 Loss: 0.0948 | 0.1229
Epoch 115/300, seasonal_0 Loss: 0.0953 | 0.1266
Epoch 116/300, seasonal_0 Loss: 0.0952 | 0.1224
Epoch 117/300, seasonal_0 Loss: 0.0938 | 0.1269
Epoch 118/300, seasonal_0 Loss: 0.0933 | 0.1250
Epoch 119/300, seasonal_0 Loss: 0.0935 | 0.1207
Epoch 120/300, seasonal_0 Loss: 0.0927 | 0.1227
Epoch 121/300, seasonal_0 Loss: 0.0936 | 0.1244
Epoch 122/300, seasonal_0 Loss: 0.0940 | 0.1266
Epoch 123/300, seasonal_0 Loss: 0.0934 | 0.1299
Epoch 124/300, seasonal_0 Loss: 0.0939 | 0.1257
Epoch 125/300, seasonal_0 Loss: 0.0931 | 0.1175
Epoch 126/300, seasonal_0 Loss: 0.0927 | 0.1205
Epoch 127/300, seasonal_0 Loss: 0.0931 | 0.1276
Epoch 128/300, seasonal_0 Loss: 0.0918 | 0.1287
Epoch 129/300, seasonal_0 Loss: 0.0919 | 0.1216
Epoch 130/300, seasonal_0 Loss: 0.0917 | 0.1146
Epoch 131/300, seasonal_0 Loss: 0.0910 | 0.1240
Epoch 132/300, seasonal_0 Loss: 0.0914 | 0.1283
Epoch 133/300, seasonal_0 Loss: 0.0900 | 0.1269
Epoch 134/300, seasonal_0 Loss: 0.0896 | 0.1178
Epoch 135/300, seasonal_0 Loss: 0.0891 | 0.1145
Epoch 136/300, seasonal_0 Loss: 0.0894 | 0.1210
Epoch 137/300, seasonal_0 Loss: 0.0887 | 0.1294
Epoch 138/300, seasonal_0 Loss: 0.0895 | 0.1196
Epoch 139/300, seasonal_0 Loss: 0.0896 | 0.1167
Epoch 140/300, seasonal_0 Loss: 0.0877 | 0.1174
Epoch 141/300, seasonal_0 Loss: 0.0886 | 0.1258
Epoch 142/300, seasonal_0 Loss: 0.0879 | 0.1249
Epoch 143/300, seasonal_0 Loss: 0.0867 | 0.1229
Epoch 144/300, seasonal_0 Loss: 0.0871 | 0.1178
Epoch 145/300, seasonal_0 Loss: 0.0863 | 0.1198
Epoch 146/300, seasonal_0 Loss: 0.0866 | 0.1215
Epoch 147/300, seasonal_0 Loss: 0.0852 | 0.1193
Epoch 148/300, seasonal_0 Loss: 0.0854 | 0.1192
Epoch 149/300, seasonal_0 Loss: 0.0857 | 0.1180
Epoch 150/300, seasonal_0 Loss: 0.0853 | 0.1166
Epoch 151/300, seasonal_0 Loss: 0.0843 | 0.1184
Epoch 152/300, seasonal_0 Loss: 0.0858 | 0.1190
Epoch 153/300, seasonal_0 Loss: 0.0852 | 0.1184
Epoch 154/300, seasonal_0 Loss: 0.0845 | 0.1189
Epoch 155/300, seasonal_0 Loss: 0.0851 | 0.1158
Epoch 156/300, seasonal_0 Loss: 0.0836 | 0.1163
Epoch 157/300, seasonal_0 Loss: 0.0842 | 0.1178
Epoch 158/300, seasonal_0 Loss: 0.0840 | 0.1186
Epoch 159/300, seasonal_0 Loss: 0.0842 | 0.1174
Epoch 160/300, seasonal_0 Loss: 0.0840 | 0.1153
Epoch 161/300, seasonal_0 Loss: 0.0832 | 0.1168
Epoch 162/300, seasonal_0 Loss: 0.0834 | 0.1185
Epoch 163/300, seasonal_0 Loss: 0.0833 | 0.1171
Epoch 164/300, seasonal_0 Loss: 0.0829 | 0.1172
Epoch 165/300, seasonal_0 Loss: 0.0828 | 0.1188
Epoch 166/300, seasonal_0 Loss: 0.0824 | 0.1157
Epoch 167/300, seasonal_0 Loss: 0.0831 | 0.1165
Epoch 168/300, seasonal_0 Loss: 0.0825 | 0.1161
Epoch 169/300, seasonal_0 Loss: 0.0825 | 0.1164
Epoch 170/300, seasonal_0 Loss: 0.0818 | 0.1148
Epoch 171/300, seasonal_0 Loss: 0.0817 | 0.1159
Epoch 172/300, seasonal_0 Loss: 0.0820 | 0.1156
Epoch 173/300, seasonal_0 Loss: 0.0818 | 0.1147
Epoch 174/300, seasonal_0 Loss: 0.0817 | 0.1171
Epoch 175/300, seasonal_0 Loss: 0.0815 | 0.1157
Epoch 176/300, seasonal_0 Loss: 0.0813 | 0.1152
Epoch 177/300, seasonal_0 Loss: 0.0808 | 0.1153
Epoch 178/300, seasonal_0 Loss: 0.0811 | 0.1152
Epoch 179/300, seasonal_0 Loss: 0.0812 | 0.1168
Epoch 180/300, seasonal_0 Loss: 0.0813 | 0.1152
Epoch 181/300, seasonal_0 Loss: 0.0813 | 0.1160
Epoch 182/300, seasonal_0 Loss: 0.0813 | 0.1143
Epoch 183/300, seasonal_0 Loss: 0.0811 | 0.1150
Epoch 184/300, seasonal_0 Loss: 0.0808 | 0.1153
Epoch 185/300, seasonal_0 Loss: 0.0808 | 0.1161
Epoch 186/300, seasonal_0 Loss: 0.0805 | 0.1162
Epoch 187/300, seasonal_0 Loss: 0.0808 | 0.1151
Epoch 188/300, seasonal_0 Loss: 0.0802 | 0.1144
Epoch 189/300, seasonal_0 Loss: 0.0804 | 0.1145
Epoch 190/300, seasonal_0 Loss: 0.0795 | 0.1140
Epoch 191/300, seasonal_0 Loss: 0.0807 | 0.1154
Epoch 192/300, seasonal_0 Loss: 0.0792 | 0.1141
Epoch 193/300, seasonal_0 Loss: 0.0801 | 0.1146
Epoch 194/300, seasonal_0 Loss: 0.0808 | 0.1142
Epoch 195/300, seasonal_0 Loss: 0.0795 | 0.1140
Epoch 196/300, seasonal_0 Loss: 0.0794 | 0.1154
Epoch 197/300, seasonal_0 Loss: 0.0805 | 0.1155
Epoch 198/300, seasonal_0 Loss: 0.0796 | 0.1170
Epoch 199/300, seasonal_0 Loss: 0.0797 | 0.1182
Epoch 200/300, seasonal_0 Loss: 0.0797 | 0.1179
Epoch 201/300, seasonal_0 Loss: 0.0799 | 0.1155
Epoch 202/300, seasonal_0 Loss: 0.0784 | 0.1162
Epoch 203/300, seasonal_0 Loss: 0.0792 | 0.1140
Epoch 204/300, seasonal_0 Loss: 0.0795 | 0.1142
Epoch 205/300, seasonal_0 Loss: 0.0787 | 0.1142
Epoch 206/300, seasonal_0 Loss: 0.0792 | 0.1150
Epoch 207/300, seasonal_0 Loss: 0.0788 | 0.1155
Epoch 208/300, seasonal_0 Loss: 0.0795 | 0.1150
Epoch 209/300, seasonal_0 Loss: 0.0788 | 0.1161
Epoch 210/300, seasonal_0 Loss: 0.0791 | 0.1147
Epoch 211/300, seasonal_0 Loss: 0.0787 | 0.1147
Epoch 212/300, seasonal_0 Loss: 0.0792 | 0.1143
Epoch 213/300, seasonal_0 Loss: 0.0788 | 0.1143
Epoch 214/300, seasonal_0 Loss: 0.0790 | 0.1135
Epoch 215/300, seasonal_0 Loss: 0.0785 | 0.1149
Epoch 216/300, seasonal_0 Loss: 0.0793 | 0.1136
Epoch 217/300, seasonal_0 Loss: 0.0789 | 0.1138
Epoch 218/300, seasonal_0 Loss: 0.0777 | 0.1134
Epoch 219/300, seasonal_0 Loss: 0.0781 | 0.1147
Epoch 220/300, seasonal_0 Loss: 0.0777 | 0.1127
Epoch 221/300, seasonal_0 Loss: 0.0782 | 0.1136
Epoch 222/300, seasonal_0 Loss: 0.0785 | 0.1140
Epoch 223/300, seasonal_0 Loss: 0.0780 | 0.1131
Epoch 224/300, seasonal_0 Loss: 0.0775 | 0.1124
Epoch 225/300, seasonal_0 Loss: 0.0776 | 0.1128
Epoch 226/300, seasonal_0 Loss: 0.0779 | 0.1134
Epoch 227/300, seasonal_0 Loss: 0.0787 | 0.1146
Epoch 228/300, seasonal_0 Loss: 0.0781 | 0.1137
Epoch 229/300, seasonal_0 Loss: 0.0774 | 0.1131
Epoch 230/300, seasonal_0 Loss: 0.0783 | 0.1130
Epoch 231/300, seasonal_0 Loss: 0.0776 | 0.1130
Epoch 232/300, seasonal_0 Loss: 0.0780 | 0.1131
Epoch 233/300, seasonal_0 Loss: 0.0777 | 0.1132
Epoch 234/300, seasonal_0 Loss: 0.0777 | 0.1125
Epoch 235/300, seasonal_0 Loss: 0.0778 | 0.1129
Epoch 236/300, seasonal_0 Loss: 0.0771 | 0.1138
Epoch 237/300, seasonal_0 Loss: 0.0772 | 0.1143
Epoch 238/300, seasonal_0 Loss: 0.0769 | 0.1142
Epoch 239/300, seasonal_0 Loss: 0.0783 | 0.1142
Epoch 240/300, seasonal_0 Loss: 0.0771 | 0.1131
Epoch 241/300, seasonal_0 Loss: 0.0772 | 0.1130
Epoch 242/300, seasonal_0 Loss: 0.0765 | 0.1133
Epoch 243/300, seasonal_0 Loss: 0.0771 | 0.1132
Epoch 244/300, seasonal_0 Loss: 0.0768 | 0.1136
Epoch 245/300, seasonal_0 Loss: 0.0771 | 0.1143
Epoch 246/300, seasonal_0 Loss: 0.0772 | 0.1134
Epoch 247/300, seasonal_0 Loss: 0.0773 | 0.1128
Epoch 248/300, seasonal_0 Loss: 0.0779 | 0.1116
Epoch 249/300, seasonal_0 Loss: 0.0770 | 0.1126
Epoch 250/300, seasonal_0 Loss: 0.0765 | 0.1127
Epoch 251/300, seasonal_0 Loss: 0.0776 | 0.1134
Epoch 252/300, seasonal_0 Loss: 0.0763 | 0.1130
Epoch 253/300, seasonal_0 Loss: 0.0770 | 0.1139
Epoch 254/300, seasonal_0 Loss: 0.0767 | 0.1133
Epoch 255/300, seasonal_0 Loss: 0.0768 | 0.1133
Epoch 256/300, seasonal_0 Loss: 0.0763 | 0.1131
Epoch 257/300, seasonal_0 Loss: 0.0767 | 0.1129
Epoch 258/300, seasonal_0 Loss: 0.0765 | 0.1134
Epoch 259/300, seasonal_0 Loss: 0.0768 | 0.1133
Epoch 260/300, seasonal_0 Loss: 0.0762 | 0.1135
Epoch 261/300, seasonal_0 Loss: 0.0765 | 0.1136
Epoch 262/300, seasonal_0 Loss: 0.0759 | 0.1128
Epoch 263/300, seasonal_0 Loss: 0.0768 | 0.1129
Epoch 264/300, seasonal_0 Loss: 0.0770 | 0.1128
Epoch 265/300, seasonal_0 Loss: 0.0764 | 0.1129
Epoch 266/300, seasonal_0 Loss: 0.0764 | 0.1133
Epoch 267/300, seasonal_0 Loss: 0.0767 | 0.1129
Epoch 268/300, seasonal_0 Loss: 0.0760 | 0.1125
Epoch 269/300, seasonal_0 Loss: 0.0761 | 0.1127
Epoch 270/300, seasonal_0 Loss: 0.0766 | 0.1126
Epoch 271/300, seasonal_0 Loss: 0.0767 | 0.1131
Epoch 272/300, seasonal_0 Loss: 0.0757 | 0.1138
Epoch 273/300, seasonal_0 Loss: 0.0763 | 0.1128
Epoch 274/300, seasonal_0 Loss: 0.0760 | 0.1131
Epoch 275/300, seasonal_0 Loss: 0.0766 | 0.1129
Epoch 276/300, seasonal_0 Loss: 0.0767 | 0.1124
Epoch 277/300, seasonal_0 Loss: 0.0767 | 0.1126
Epoch 278/300, seasonal_0 Loss: 0.0762 | 0.1124
Epoch 279/300, seasonal_0 Loss: 0.0766 | 0.1131
Epoch 280/300, seasonal_0 Loss: 0.0769 | 0.1131
Epoch 281/300, seasonal_0 Loss: 0.0768 | 0.1129
Epoch 282/300, seasonal_0 Loss: 0.0768 | 0.1121
Epoch 283/300, seasonal_0 Loss: 0.0771 | 0.1129
Epoch 284/300, seasonal_0 Loss: 0.0766 | 0.1133
Epoch 285/300, seasonal_0 Loss: 0.0762 | 0.1132
Epoch 286/300, seasonal_0 Loss: 0.0761 | 0.1131
Epoch 287/300, seasonal_0 Loss: 0.0757 | 0.1132
Epoch 288/300, seasonal_0 Loss: 0.0766 | 0.1127
Epoch 289/300, seasonal_0 Loss: 0.0764 | 0.1126
Epoch 290/300, seasonal_0 Loss: 0.0757 | 0.1124
Epoch 291/300, seasonal_0 Loss: 0.0759 | 0.1127
Epoch 292/300, seasonal_0 Loss: 0.0765 | 0.1129
Epoch 293/300, seasonal_0 Loss: 0.0762 | 0.1125
Epoch 294/300, seasonal_0 Loss: 0.0766 | 0.1128
Epoch 295/300, seasonal_0 Loss: 0.0757 | 0.1125
Epoch 296/300, seasonal_0 Loss: 0.0755 | 0.1128
Epoch 297/300, seasonal_0 Loss: 0.0751 | 0.1129
Epoch 298/300, seasonal_0 Loss: 0.0756 | 0.1126
Epoch 299/300, seasonal_0 Loss: 0.0760 | 0.1129
Epoch 300/300, seasonal_0 Loss: 0.0761 | 0.1124
Training seasonal_1 component with params: {'observation_period_num': 11, 'train_rates': 0.9833693202562211, 'learning_rate': 0.00023678808870649713, 'batch_size': 188, 'step_size': 7, 'gamma': 0.8672395763753694}
Epoch 1/300, seasonal_1 Loss: 1.0961 | 1.3520
Epoch 2/300, seasonal_1 Loss: 0.7326 | 0.8175
Epoch 3/300, seasonal_1 Loss: 0.6195 | 0.7421
Epoch 4/300, seasonal_1 Loss: 0.5537 | 0.6832
Epoch 5/300, seasonal_1 Loss: 0.4742 | 0.6377
Epoch 6/300, seasonal_1 Loss: 0.4614 | 0.6385
Epoch 7/300, seasonal_1 Loss: 0.4660 | 0.5974
Epoch 8/300, seasonal_1 Loss: 0.4348 | 0.5552
Epoch 9/300, seasonal_1 Loss: 0.4121 | 0.5380
Epoch 10/300, seasonal_1 Loss: 0.3963 | 0.5121
Epoch 11/300, seasonal_1 Loss: 0.3727 | 0.4932
Epoch 12/300, seasonal_1 Loss: 0.3445 | 0.4761
Epoch 13/300, seasonal_1 Loss: 0.3814 | 0.4615
Epoch 14/300, seasonal_1 Loss: 0.3061 | 0.4699
Epoch 15/300, seasonal_1 Loss: 0.2862 | 0.4293
Epoch 16/300, seasonal_1 Loss: 0.2717 | 0.4223
Epoch 17/300, seasonal_1 Loss: 0.2640 | 0.4065
Epoch 18/300, seasonal_1 Loss: 0.2607 | 0.3953
Epoch 19/300, seasonal_1 Loss: 0.2688 | 0.3835
Epoch 20/300, seasonal_1 Loss: 0.2561 | 0.3847
Epoch 21/300, seasonal_1 Loss: 0.2519 | 0.3556
Epoch 22/300, seasonal_1 Loss: 0.2375 | 0.3681
Epoch 23/300, seasonal_1 Loss: 0.2307 | 0.3423
Epoch 24/300, seasonal_1 Loss: 0.2252 | 0.3443
Epoch 25/300, seasonal_1 Loss: 0.2224 | 0.3312
Epoch 26/300, seasonal_1 Loss: 0.2207 | 0.3300
Epoch 27/300, seasonal_1 Loss: 0.2170 | 0.3198
Epoch 28/300, seasonal_1 Loss: 0.2136 | 0.3214
Epoch 29/300, seasonal_1 Loss: 0.2147 | 0.3077
Epoch 30/300, seasonal_1 Loss: 0.2101 | 0.3137
Epoch 31/300, seasonal_1 Loss: 0.2080 | 0.3025
Epoch 32/300, seasonal_1 Loss: 0.2035 | 0.3028
Epoch 33/300, seasonal_1 Loss: 0.1992 | 0.2965
Epoch 34/300, seasonal_1 Loss: 0.1964 | 0.2931
Epoch 35/300, seasonal_1 Loss: 0.1955 | 0.2903
Epoch 36/300, seasonal_1 Loss: 0.1934 | 0.2861
Epoch 37/300, seasonal_1 Loss: 0.1910 | 0.2843
Epoch 38/300, seasonal_1 Loss: 0.1891 | 0.2804
Epoch 39/300, seasonal_1 Loss: 0.1889 | 0.2777
Epoch 40/300, seasonal_1 Loss: 0.1862 | 0.2758
Epoch 41/300, seasonal_1 Loss: 0.1853 | 0.2734
Epoch 42/300, seasonal_1 Loss: 0.1837 | 0.2714
Epoch 43/300, seasonal_1 Loss: 0.1826 | 0.2682
Epoch 44/300, seasonal_1 Loss: 0.1821 | 0.2671
Epoch 45/300, seasonal_1 Loss: 0.1801 | 0.2648
Epoch 46/300, seasonal_1 Loss: 0.1788 | 0.2629
Epoch 47/300, seasonal_1 Loss: 0.1780 | 0.2617
Epoch 48/300, seasonal_1 Loss: 0.1772 | 0.2598
Epoch 49/300, seasonal_1 Loss: 0.1763 | 0.2579
Epoch 50/300, seasonal_1 Loss: 0.1766 | 0.2565
Epoch 51/300, seasonal_1 Loss: 0.1750 | 0.2549
Epoch 52/300, seasonal_1 Loss: 0.1744 | 0.2537
Epoch 53/300, seasonal_1 Loss: 0.1729 | 0.2531
Epoch 54/300, seasonal_1 Loss: 0.1726 | 0.2507
Epoch 55/300, seasonal_1 Loss: 0.1713 | 0.2493
Epoch 56/300, seasonal_1 Loss: 0.1710 | 0.2482
Epoch 57/300, seasonal_1 Loss: 0.1701 | 0.2473
Epoch 58/300, seasonal_1 Loss: 0.1704 | 0.2472
Epoch 59/300, seasonal_1 Loss: 0.1692 | 0.2459
Epoch 60/300, seasonal_1 Loss: 0.1683 | 0.2448
Epoch 61/300, seasonal_1 Loss: 0.1683 | 0.2434
Epoch 62/300, seasonal_1 Loss: 0.1681 | 0.2423
Epoch 63/300, seasonal_1 Loss: 0.1669 | 0.2413
Epoch 64/300, seasonal_1 Loss: 0.1663 | 0.2406
Epoch 65/300, seasonal_1 Loss: 0.1659 | 0.2400
Epoch 66/300, seasonal_1 Loss: 0.1659 | 0.2389
Epoch 67/300, seasonal_1 Loss: 0.1652 | 0.2382
Epoch 68/300, seasonal_1 Loss: 0.1652 | 0.2372
Epoch 69/300, seasonal_1 Loss: 0.1658 | 0.2365
Epoch 70/300, seasonal_1 Loss: 0.1643 | 0.2355
Epoch 71/300, seasonal_1 Loss: 0.1641 | 0.2351
Epoch 72/300, seasonal_1 Loss: 0.1641 | 0.2346
Epoch 73/300, seasonal_1 Loss: 0.1638 | 0.2342
Epoch 74/300, seasonal_1 Loss: 0.1631 | 0.2336
Epoch 75/300, seasonal_1 Loss: 0.1623 | 0.2334
Epoch 76/300, seasonal_1 Loss: 0.1631 | 0.2330
Epoch 77/300, seasonal_1 Loss: 0.1622 | 0.2319
Epoch 78/300, seasonal_1 Loss: 0.1619 | 0.2311
Epoch 79/300, seasonal_1 Loss: 0.1614 | 0.2306
Epoch 80/300, seasonal_1 Loss: 0.1610 | 0.2308
Epoch 81/300, seasonal_1 Loss: 0.1606 | 0.2300
Epoch 82/300, seasonal_1 Loss: 0.1617 | 0.2295
Epoch 83/300, seasonal_1 Loss: 0.1614 | 0.2293
Epoch 84/300, seasonal_1 Loss: 0.1610 | 0.2290
Epoch 85/300, seasonal_1 Loss: 0.1607 | 0.2286
Epoch 86/300, seasonal_1 Loss: 0.1606 | 0.2283
Epoch 87/300, seasonal_1 Loss: 0.1600 | 0.2279
Epoch 88/300, seasonal_1 Loss: 0.1603 | 0.2274
Epoch 89/300, seasonal_1 Loss: 0.1596 | 0.2272
Epoch 90/300, seasonal_1 Loss: 0.1594 | 0.2267
Epoch 91/300, seasonal_1 Loss: 0.1595 | 0.2261
Epoch 92/300, seasonal_1 Loss: 0.1594 | 0.2257
Epoch 93/300, seasonal_1 Loss: 0.1581 | 0.2256
Epoch 94/300, seasonal_1 Loss: 0.1586 | 0.2255
Epoch 95/300, seasonal_1 Loss: 0.1587 | 0.2253
Epoch 96/300, seasonal_1 Loss: 0.1582 | 0.2250
Epoch 97/300, seasonal_1 Loss: 0.1587 | 0.2246
Epoch 98/300, seasonal_1 Loss: 0.1581 | 0.2241
Epoch 99/300, seasonal_1 Loss: 0.1582 | 0.2242
Epoch 100/300, seasonal_1 Loss: 0.1584 | 0.2239
Epoch 101/300, seasonal_1 Loss: 0.1575 | 0.2237
Epoch 102/300, seasonal_1 Loss: 0.1574 | 0.2236
Epoch 103/300, seasonal_1 Loss: 0.1577 | 0.2234
Epoch 104/300, seasonal_1 Loss: 0.1588 | 0.2232
Epoch 105/300, seasonal_1 Loss: 0.1573 | 0.2230
Epoch 106/300, seasonal_1 Loss: 0.1576 | 0.2229
Epoch 107/300, seasonal_1 Loss: 0.1577 | 0.2226
Epoch 108/300, seasonal_1 Loss: 0.1568 | 0.2223
Epoch 109/300, seasonal_1 Loss: 0.1577 | 0.2223
Epoch 110/300, seasonal_1 Loss: 0.1565 | 0.2221
Epoch 111/300, seasonal_1 Loss: 0.1577 | 0.2221
Epoch 112/300, seasonal_1 Loss: 0.1571 | 0.2220
Epoch 113/300, seasonal_1 Loss: 0.1565 | 0.2218
Epoch 114/300, seasonal_1 Loss: 0.1572 | 0.2218
Epoch 115/300, seasonal_1 Loss: 0.1569 | 0.2217
Epoch 116/300, seasonal_1 Loss: 0.1570 | 0.2215
Epoch 117/300, seasonal_1 Loss: 0.1569 | 0.2213
Epoch 118/300, seasonal_1 Loss: 0.1567 | 0.2213
Epoch 119/300, seasonal_1 Loss: 0.1567 | 0.2212
Epoch 120/300, seasonal_1 Loss: 0.1563 | 0.2211
Epoch 121/300, seasonal_1 Loss: 0.1569 | 0.2209
Epoch 122/300, seasonal_1 Loss: 0.1563 | 0.2207
Epoch 123/300, seasonal_1 Loss: 0.1562 | 0.2207
Epoch 124/300, seasonal_1 Loss: 0.1561 | 0.2207
Epoch 125/300, seasonal_1 Loss: 0.1566 | 0.2206
Epoch 126/300, seasonal_1 Loss: 0.1563 | 0.2206
Epoch 127/300, seasonal_1 Loss: 0.1560 | 0.2205
Epoch 128/300, seasonal_1 Loss: 0.1561 | 0.2204
Epoch 129/300, seasonal_1 Loss: 0.1562 | 0.2204
Epoch 130/300, seasonal_1 Loss: 0.1562 | 0.2204
Epoch 131/300, seasonal_1 Loss: 0.1560 | 0.2203
Epoch 132/300, seasonal_1 Loss: 0.1557 | 0.2203
Epoch 133/300, seasonal_1 Loss: 0.1555 | 0.2201
Epoch 134/300, seasonal_1 Loss: 0.1560 | 0.2200
Epoch 135/300, seasonal_1 Loss: 0.1557 | 0.2199
Epoch 136/300, seasonal_1 Loss: 0.1561 | 0.2198
Epoch 137/300, seasonal_1 Loss: 0.1560 | 0.2198
Epoch 138/300, seasonal_1 Loss: 0.1554 | 0.2197
Epoch 139/300, seasonal_1 Loss: 0.1552 | 0.2197
Epoch 140/300, seasonal_1 Loss: 0.1558 | 0.2196
Epoch 141/300, seasonal_1 Loss: 0.1555 | 0.2195
Epoch 142/300, seasonal_1 Loss: 0.1557 | 0.2194
Epoch 143/300, seasonal_1 Loss: 0.1559 | 0.2194
Epoch 144/300, seasonal_1 Loss: 0.1552 | 0.2194
Epoch 145/300, seasonal_1 Loss: 0.1552 | 0.2193
Epoch 146/300, seasonal_1 Loss: 0.1561 | 0.2193
Epoch 147/300, seasonal_1 Loss: 0.1554 | 0.2193
Epoch 148/300, seasonal_1 Loss: 0.1559 | 0.2193
Epoch 149/300, seasonal_1 Loss: 0.1550 | 0.2192
Epoch 150/300, seasonal_1 Loss: 0.1557 | 0.2192
Epoch 151/300, seasonal_1 Loss: 0.1557 | 0.2191
Epoch 152/300, seasonal_1 Loss: 0.1553 | 0.2191
Epoch 153/300, seasonal_1 Loss: 0.1553 | 0.2190
Epoch 154/300, seasonal_1 Loss: 0.1548 | 0.2190
Epoch 155/300, seasonal_1 Loss: 0.1553 | 0.2190
Epoch 156/300, seasonal_1 Loss: 0.1558 | 0.2189
Epoch 157/300, seasonal_1 Loss: 0.1563 | 0.2189
Epoch 158/300, seasonal_1 Loss: 0.1557 | 0.2189
Epoch 159/300, seasonal_1 Loss: 0.1554 | 0.2189
Epoch 160/300, seasonal_1 Loss: 0.1554 | 0.2188
Epoch 161/300, seasonal_1 Loss: 0.1552 | 0.2188
Epoch 162/300, seasonal_1 Loss: 0.1552 | 0.2188
Epoch 163/300, seasonal_1 Loss: 0.1558 | 0.2188
Epoch 164/300, seasonal_1 Loss: 0.1554 | 0.2188
Epoch 165/300, seasonal_1 Loss: 0.1553 | 0.2187
Epoch 166/300, seasonal_1 Loss: 0.1550 | 0.2187
Epoch 167/300, seasonal_1 Loss: 0.1551 | 0.2187
Epoch 168/300, seasonal_1 Loss: 0.1557 | 0.2187
Epoch 169/300, seasonal_1 Loss: 0.1556 | 0.2187
Epoch 170/300, seasonal_1 Loss: 0.1551 | 0.2187
Epoch 171/300, seasonal_1 Loss: 0.1550 | 0.2186
Epoch 172/300, seasonal_1 Loss: 0.1553 | 0.2186
Epoch 173/300, seasonal_1 Loss: 0.1548 | 0.2186
Epoch 174/300, seasonal_1 Loss: 0.1551 | 0.2186
Epoch 175/300, seasonal_1 Loss: 0.1549 | 0.2186
Epoch 176/300, seasonal_1 Loss: 0.1552 | 0.2186
Epoch 177/300, seasonal_1 Loss: 0.1547 | 0.2186
Epoch 178/300, seasonal_1 Loss: 0.1548 | 0.2185
Epoch 179/300, seasonal_1 Loss: 0.1547 | 0.2185
Epoch 180/300, seasonal_1 Loss: 0.1553 | 0.2185
Epoch 181/300, seasonal_1 Loss: 0.1551 | 0.2185
Epoch 182/300, seasonal_1 Loss: 0.1551 | 0.2185
Epoch 183/300, seasonal_1 Loss: 0.1545 | 0.2185
Epoch 184/300, seasonal_1 Loss: 0.1553 | 0.2185
Epoch 185/300, seasonal_1 Loss: 0.1557 | 0.2185
Epoch 186/300, seasonal_1 Loss: 0.1557 | 0.2185
Epoch 187/300, seasonal_1 Loss: 0.1557 | 0.2185
Epoch 188/300, seasonal_1 Loss: 0.1549 | 0.2185
Epoch 189/300, seasonal_1 Loss: 0.1551 | 0.2185
Epoch 190/300, seasonal_1 Loss: 0.1551 | 0.2185
Epoch 191/300, seasonal_1 Loss: 0.1550 | 0.2185
Epoch 192/300, seasonal_1 Loss: 0.1549 | 0.2185
Epoch 193/300, seasonal_1 Loss: 0.1549 | 0.2185
Epoch 194/300, seasonal_1 Loss: 0.1553 | 0.2185
Epoch 195/300, seasonal_1 Loss: 0.1541 | 0.2185
Epoch 196/300, seasonal_1 Loss: 0.1555 | 0.2184
Epoch 197/300, seasonal_1 Loss: 0.1555 | 0.2184
Epoch 198/300, seasonal_1 Loss: 0.1553 | 0.2184
Epoch 199/300, seasonal_1 Loss: 0.1555 | 0.2184
Epoch 200/300, seasonal_1 Loss: 0.1545 | 0.2184
Epoch 201/300, seasonal_1 Loss: 0.1555 | 0.2184
Epoch 202/300, seasonal_1 Loss: 0.1547 | 0.2184
Epoch 203/300, seasonal_1 Loss: 0.1544 | 0.2184
Epoch 204/300, seasonal_1 Loss: 0.1552 | 0.2184
Epoch 205/300, seasonal_1 Loss: 0.1545 | 0.2184
Epoch 206/300, seasonal_1 Loss: 0.1546 | 0.2184
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 69, 'train_rates': 0.9875352832146834, 'learning_rate': 0.0002894095104319927, 'batch_size': 195, 'step_size': 9, 'gamma': 0.841599307599911}
Epoch 1/300, seasonal_2 Loss: 1.1036 | 1.7243
Epoch 2/300, seasonal_2 Loss: 0.7522 | 1.0253
Epoch 3/300, seasonal_2 Loss: 0.6518 | 0.8124
Epoch 4/300, seasonal_2 Loss: 0.6197 | 0.7620
Epoch 5/300, seasonal_2 Loss: 0.6046 | 0.9044
Epoch 6/300, seasonal_2 Loss: 0.5102 | 0.6810
Epoch 7/300, seasonal_2 Loss: 0.4466 | 0.6303
Epoch 8/300, seasonal_2 Loss: 0.4247 | 0.5891
Epoch 9/300, seasonal_2 Loss: 0.4143 | 0.5667
Epoch 10/300, seasonal_2 Loss: 0.5154 | 0.6319
Epoch 11/300, seasonal_2 Loss: 0.3943 | 0.5389
Epoch 12/300, seasonal_2 Loss: 0.3798 | 0.5229
Epoch 13/300, seasonal_2 Loss: 0.3488 | 0.5193
Epoch 14/300, seasonal_2 Loss: 0.3511 | 0.4983
Epoch 15/300, seasonal_2 Loss: 0.3141 | 0.4739
Epoch 16/300, seasonal_2 Loss: 0.3082 | 0.4553
Epoch 17/300, seasonal_2 Loss: 0.2912 | 0.4420
Epoch 18/300, seasonal_2 Loss: 0.2805 | 0.4286
Epoch 19/300, seasonal_2 Loss: 0.2891 | 0.4153
Epoch 20/300, seasonal_2 Loss: 0.2782 | 0.3999
Epoch 21/300, seasonal_2 Loss: 0.2833 | 0.3977
Epoch 22/300, seasonal_2 Loss: 0.2604 | 0.3779
Epoch 23/300, seasonal_2 Loss: 0.2543 | 0.3842
Epoch 24/300, seasonal_2 Loss: 0.2468 | 0.3597
Epoch 25/300, seasonal_2 Loss: 0.2409 | 0.3734
Epoch 26/300, seasonal_2 Loss: 0.2364 | 0.3485
Epoch 27/300, seasonal_2 Loss: 0.2366 | 0.3505
Epoch 28/300, seasonal_2 Loss: 0.2344 | 0.3462
Epoch 29/300, seasonal_2 Loss: 0.2274 | 0.3264
Epoch 30/300, seasonal_2 Loss: 0.2217 | 0.3292
Epoch 31/300, seasonal_2 Loss: 0.2165 | 0.3176
Epoch 32/300, seasonal_2 Loss: 0.2136 | 0.3168
Epoch 33/300, seasonal_2 Loss: 0.2113 | 0.3089
Epoch 34/300, seasonal_2 Loss: 0.2093 | 0.3062
Epoch 35/300, seasonal_2 Loss: 0.2073 | 0.2999
Epoch 36/300, seasonal_2 Loss: 0.2058 | 0.2981
Epoch 37/300, seasonal_2 Loss: 0.2045 | 0.2919
Epoch 38/300, seasonal_2 Loss: 0.2022 | 0.2900
Epoch 39/300, seasonal_2 Loss: 0.2014 | 0.2868
Epoch 40/300, seasonal_2 Loss: 0.1998 | 0.2838
Epoch 41/300, seasonal_2 Loss: 0.1983 | 0.2803
Epoch 42/300, seasonal_2 Loss: 0.1979 | 0.2778
Epoch 43/300, seasonal_2 Loss: 0.1972 | 0.2749
Epoch 44/300, seasonal_2 Loss: 0.1952 | 0.2725
Epoch 45/300, seasonal_2 Loss: 0.1947 | 0.2699
Epoch 46/300, seasonal_2 Loss: 0.1937 | 0.2678
Epoch 47/300, seasonal_2 Loss: 0.1929 | 0.2665
Epoch 48/300, seasonal_2 Loss: 0.1915 | 0.2649
Epoch 49/300, seasonal_2 Loss: 0.1906 | 0.2622
Epoch 50/300, seasonal_2 Loss: 0.1903 | 0.2608
Epoch 51/300, seasonal_2 Loss: 0.1890 | 0.2586
Epoch 52/300, seasonal_2 Loss: 0.1893 | 0.2574
Epoch 53/300, seasonal_2 Loss: 0.1879 | 0.2572
Epoch 54/300, seasonal_2 Loss: 0.1873 | 0.2544
Epoch 55/300, seasonal_2 Loss: 0.1865 | 0.2531
Epoch 56/300, seasonal_2 Loss: 0.1860 | 0.2520
Epoch 57/300, seasonal_2 Loss: 0.1853 | 0.2498
Epoch 58/300, seasonal_2 Loss: 0.1851 | 0.2488
Epoch 59/300, seasonal_2 Loss: 0.1847 | 0.2477
Epoch 60/300, seasonal_2 Loss: 0.1844 | 0.2471
Epoch 61/300, seasonal_2 Loss: 0.1836 | 0.2461
Epoch 62/300, seasonal_2 Loss: 0.1830 | 0.2450
Epoch 63/300, seasonal_2 Loss: 0.1825 | 0.2447
Epoch 64/300, seasonal_2 Loss: 0.1824 | 0.2441
Epoch 65/300, seasonal_2 Loss: 0.1814 | 0.2423
Epoch 66/300, seasonal_2 Loss: 0.1818 | 0.2412
Epoch 67/300, seasonal_2 Loss: 0.1807 | 0.2410
Epoch 68/300, seasonal_2 Loss: 0.1804 | 0.2403
Epoch 69/300, seasonal_2 Loss: 0.1802 | 0.2387
Epoch 70/300, seasonal_2 Loss: 0.1802 | 0.2383
Epoch 71/300, seasonal_2 Loss: 0.1798 | 0.2378
Epoch 72/300, seasonal_2 Loss: 0.1797 | 0.2366
Epoch 73/300, seasonal_2 Loss: 0.1789 | 0.2357
Epoch 74/300, seasonal_2 Loss: 0.1787 | 0.2353
Epoch 75/300, seasonal_2 Loss: 0.1781 | 0.2350
Epoch 76/300, seasonal_2 Loss: 0.1780 | 0.2345
Epoch 77/300, seasonal_2 Loss: 0.1776 | 0.2332
Epoch 78/300, seasonal_2 Loss: 0.1782 | 0.2327
Epoch 79/300, seasonal_2 Loss: 0.1772 | 0.2320
Epoch 80/300, seasonal_2 Loss: 0.1766 | 0.2317
Epoch 81/300, seasonal_2 Loss: 0.1769 | 0.2313
Epoch 82/300, seasonal_2 Loss: 0.1766 | 0.2309
Epoch 83/300, seasonal_2 Loss: 0.1762 | 0.2306
Epoch 84/300, seasonal_2 Loss: 0.1763 | 0.2301
Epoch 85/300, seasonal_2 Loss: 0.1764 | 0.2295
Epoch 86/300, seasonal_2 Loss: 0.1756 | 0.2291
Epoch 87/300, seasonal_2 Loss: 0.1764 | 0.2285
Epoch 88/300, seasonal_2 Loss: 0.1754 | 0.2285
Epoch 89/300, seasonal_2 Loss: 0.1753 | 0.2282
Epoch 90/300, seasonal_2 Loss: 0.1748 | 0.2279
Epoch 91/300, seasonal_2 Loss: 0.1746 | 0.2279
Epoch 92/300, seasonal_2 Loss: 0.1747 | 0.2272
Epoch 93/300, seasonal_2 Loss: 0.1740 | 0.2267
Epoch 94/300, seasonal_2 Loss: 0.1739 | 0.2262
Epoch 95/300, seasonal_2 Loss: 0.1741 | 0.2258
Epoch 96/300, seasonal_2 Loss: 0.1736 | 0.2258
Epoch 97/300, seasonal_2 Loss: 0.1733 | 0.2254
Epoch 98/300, seasonal_2 Loss: 0.1732 | 0.2250
Epoch 99/300, seasonal_2 Loss: 0.1735 | 0.2249
Epoch 100/300, seasonal_2 Loss: 0.1731 | 0.2245
Epoch 101/300, seasonal_2 Loss: 0.1736 | 0.2243
Epoch 102/300, seasonal_2 Loss: 0.1725 | 0.2241
Epoch 103/300, seasonal_2 Loss: 0.1735 | 0.2239
Epoch 104/300, seasonal_2 Loss: 0.1736 | 0.2240
Epoch 105/300, seasonal_2 Loss: 0.1728 | 0.2241
Epoch 106/300, seasonal_2 Loss: 0.1735 | 0.2239
Epoch 107/300, seasonal_2 Loss: 0.1733 | 0.2235
Epoch 108/300, seasonal_2 Loss: 0.1724 | 0.2232
Epoch 109/300, seasonal_2 Loss: 0.1721 | 0.2232
Epoch 110/300, seasonal_2 Loss: 0.1725 | 0.2229
Epoch 111/300, seasonal_2 Loss: 0.1722 | 0.2225
Epoch 112/300, seasonal_2 Loss: 0.1724 | 0.2222
Epoch 113/300, seasonal_2 Loss: 0.1723 | 0.2219
Epoch 114/300, seasonal_2 Loss: 0.1722 | 0.2217
Epoch 115/300, seasonal_2 Loss: 0.1717 | 0.2215
Epoch 116/300, seasonal_2 Loss: 0.1712 | 0.2213
Epoch 117/300, seasonal_2 Loss: 0.1723 | 0.2213
Epoch 118/300, seasonal_2 Loss: 0.1714 | 0.2213
Epoch 119/300, seasonal_2 Loss: 0.1720 | 0.2212
Epoch 120/300, seasonal_2 Loss: 0.1720 | 0.2211
Epoch 121/300, seasonal_2 Loss: 0.1719 | 0.2210
Epoch 122/300, seasonal_2 Loss: 0.1715 | 0.2211
Epoch 123/300, seasonal_2 Loss: 0.1720 | 0.2210
Epoch 124/300, seasonal_2 Loss: 0.1724 | 0.2208
Epoch 125/300, seasonal_2 Loss: 0.1711 | 0.2207
Epoch 126/300, seasonal_2 Loss: 0.1711 | 0.2206
Epoch 127/300, seasonal_2 Loss: 0.1715 | 0.2205
Epoch 128/300, seasonal_2 Loss: 0.1711 | 0.2204
Epoch 129/300, seasonal_2 Loss: 0.1709 | 0.2203
Epoch 130/300, seasonal_2 Loss: 0.1711 | 0.2201
Epoch 131/300, seasonal_2 Loss: 0.1717 | 0.2199
Epoch 132/300, seasonal_2 Loss: 0.1708 | 0.2198
Epoch 133/300, seasonal_2 Loss: 0.1714 | 0.2198
Epoch 134/300, seasonal_2 Loss: 0.1709 | 0.2198
Epoch 135/300, seasonal_2 Loss: 0.1711 | 0.2198
Epoch 136/300, seasonal_2 Loss: 0.1705 | 0.2197
Epoch 137/300, seasonal_2 Loss: 0.1707 | 0.2196
Epoch 138/300, seasonal_2 Loss: 0.1709 | 0.2196
Epoch 139/300, seasonal_2 Loss: 0.1707 | 0.2195
Epoch 140/300, seasonal_2 Loss: 0.1707 | 0.2193
Epoch 141/300, seasonal_2 Loss: 0.1703 | 0.2192
Epoch 142/300, seasonal_2 Loss: 0.1706 | 0.2192
Epoch 143/300, seasonal_2 Loss: 0.1698 | 0.2192
Epoch 144/300, seasonal_2 Loss: 0.1708 | 0.2192
Epoch 145/300, seasonal_2 Loss: 0.1713 | 0.2191
Epoch 146/300, seasonal_2 Loss: 0.1702 | 0.2191
Epoch 147/300, seasonal_2 Loss: 0.1717 | 0.2192
Epoch 148/300, seasonal_2 Loss: 0.1708 | 0.2192
Epoch 149/300, seasonal_2 Loss: 0.1709 | 0.2192
Epoch 150/300, seasonal_2 Loss: 0.1712 | 0.2192
Epoch 151/300, seasonal_2 Loss: 0.1703 | 0.2191
Epoch 152/300, seasonal_2 Loss: 0.1708 | 0.2190
Epoch 153/300, seasonal_2 Loss: 0.1699 | 0.2190
Epoch 154/300, seasonal_2 Loss: 0.1705 | 0.2190
Epoch 155/300, seasonal_2 Loss: 0.1709 | 0.2190
Epoch 156/300, seasonal_2 Loss: 0.1701 | 0.2189
Epoch 157/300, seasonal_2 Loss: 0.1702 | 0.2189
Epoch 158/300, seasonal_2 Loss: 0.1702 | 0.2187
Epoch 159/300, seasonal_2 Loss: 0.1703 | 0.2187
Epoch 160/300, seasonal_2 Loss: 0.1711 | 0.2187
Epoch 161/300, seasonal_2 Loss: 0.1705 | 0.2187
Epoch 162/300, seasonal_2 Loss: 0.1700 | 0.2187
Epoch 163/300, seasonal_2 Loss: 0.1700 | 0.2186
Epoch 164/300, seasonal_2 Loss: 0.1704 | 0.2186
Epoch 165/300, seasonal_2 Loss: 0.1698 | 0.2185
Epoch 166/300, seasonal_2 Loss: 0.1703 | 0.2185
Epoch 167/300, seasonal_2 Loss: 0.1702 | 0.2184
Epoch 168/300, seasonal_2 Loss: 0.1701 | 0.2184
Epoch 169/300, seasonal_2 Loss: 0.1696 | 0.2184
Epoch 170/300, seasonal_2 Loss: 0.1696 | 0.2183
Epoch 171/300, seasonal_2 Loss: 0.1703 | 0.2183
Epoch 172/300, seasonal_2 Loss: 0.1702 | 0.2182
Epoch 173/300, seasonal_2 Loss: 0.1707 | 0.2182
Epoch 174/300, seasonal_2 Loss: 0.1701 | 0.2182
Epoch 175/300, seasonal_2 Loss: 0.1697 | 0.2182
Epoch 176/300, seasonal_2 Loss: 0.1701 | 0.2182
Epoch 177/300, seasonal_2 Loss: 0.1709 | 0.2182
Epoch 178/300, seasonal_2 Loss: 0.1694 | 0.2181
Epoch 179/300, seasonal_2 Loss: 0.1708 | 0.2181
Epoch 180/300, seasonal_2 Loss: 0.1709 | 0.2181
Epoch 181/300, seasonal_2 Loss: 0.1703 | 0.2180
Epoch 182/300, seasonal_2 Loss: 0.1697 | 0.2180
Epoch 183/300, seasonal_2 Loss: 0.1696 | 0.2180
Epoch 184/300, seasonal_2 Loss: 0.1697 | 0.2180
Epoch 185/300, seasonal_2 Loss: 0.1703 | 0.2180
Epoch 186/300, seasonal_2 Loss: 0.1694 | 0.2179
Epoch 187/300, seasonal_2 Loss: 0.1692 | 0.2179
Epoch 188/300, seasonal_2 Loss: 0.1700 | 0.2179
Epoch 189/300, seasonal_2 Loss: 0.1700 | 0.2179
Epoch 190/300, seasonal_2 Loss: 0.1700 | 0.2179
Epoch 191/300, seasonal_2 Loss: 0.1700 | 0.2179
Epoch 192/300, seasonal_2 Loss: 0.1699 | 0.2179
Epoch 193/300, seasonal_2 Loss: 0.1699 | 0.2179
Epoch 194/300, seasonal_2 Loss: 0.1699 | 0.2179
Epoch 195/300, seasonal_2 Loss: 0.1698 | 0.2179
Epoch 196/300, seasonal_2 Loss: 0.1704 | 0.2179
Epoch 197/300, seasonal_2 Loss: 0.1702 | 0.2179
Epoch 198/300, seasonal_2 Loss: 0.1699 | 0.2179
Epoch 199/300, seasonal_2 Loss: 0.1705 | 0.2179
Epoch 200/300, seasonal_2 Loss: 0.1702 | 0.2179
Epoch 201/300, seasonal_2 Loss: 0.1703 | 0.2179
Epoch 202/300, seasonal_2 Loss: 0.1698 | 0.2179
Epoch 203/300, seasonal_2 Loss: 0.1704 | 0.2179
Epoch 204/300, seasonal_2 Loss: 0.1697 | 0.2178
Epoch 205/300, seasonal_2 Loss: 0.1699 | 0.2178
Epoch 206/300, seasonal_2 Loss: 0.1705 | 0.2178
Epoch 207/300, seasonal_2 Loss: 0.1706 | 0.2178
Epoch 208/300, seasonal_2 Loss: 0.1702 | 0.2178
Epoch 209/300, seasonal_2 Loss: 0.1700 | 0.2178
Epoch 210/300, seasonal_2 Loss: 0.1698 | 0.2178
Epoch 211/300, seasonal_2 Loss: 0.1704 | 0.2178
Epoch 212/300, seasonal_2 Loss: 0.1701 | 0.2178
Epoch 213/300, seasonal_2 Loss: 0.1703 | 0.2178
Epoch 214/300, seasonal_2 Loss: 0.1706 | 0.2178
Epoch 215/300, seasonal_2 Loss: 0.1695 | 0.2178
Epoch 216/300, seasonal_2 Loss: 0.1696 | 0.2178
Epoch 217/300, seasonal_2 Loss: 0.1697 | 0.2178
Epoch 218/300, seasonal_2 Loss: 0.1708 | 0.2178
Epoch 219/300, seasonal_2 Loss: 0.1700 | 0.2178
Epoch 220/300, seasonal_2 Loss: 0.1702 | 0.2178
Epoch 221/300, seasonal_2 Loss: 0.1702 | 0.2178
Epoch 222/300, seasonal_2 Loss: 0.1695 | 0.2178
Epoch 223/300, seasonal_2 Loss: 0.1702 | 0.2177
Epoch 224/300, seasonal_2 Loss: 0.1704 | 0.2177
Epoch 225/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 226/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 227/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 228/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 229/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 230/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 231/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 232/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 233/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 234/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 235/300, seasonal_2 Loss: 0.1708 | 0.2177
Epoch 236/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 237/300, seasonal_2 Loss: 0.1706 | 0.2177
Epoch 238/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 239/300, seasonal_2 Loss: 0.1701 | 0.2177
Epoch 240/300, seasonal_2 Loss: 0.1694 | 0.2177
Epoch 241/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 242/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 243/300, seasonal_2 Loss: 0.1708 | 0.2177
Epoch 244/300, seasonal_2 Loss: 0.1708 | 0.2177
Epoch 245/300, seasonal_2 Loss: 0.1702 | 0.2177
Epoch 246/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 247/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 248/300, seasonal_2 Loss: 0.1701 | 0.2177
Epoch 249/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 250/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 251/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 252/300, seasonal_2 Loss: 0.1704 | 0.2177
Epoch 253/300, seasonal_2 Loss: 0.1693 | 0.2177
Epoch 254/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 255/300, seasonal_2 Loss: 0.1687 | 0.2177
Epoch 256/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 257/300, seasonal_2 Loss: 0.1705 | 0.2177
Epoch 258/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 259/300, seasonal_2 Loss: 0.1693 | 0.2177
Epoch 260/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 261/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 262/300, seasonal_2 Loss: 0.1702 | 0.2177
Epoch 263/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 264/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 265/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 266/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 267/300, seasonal_2 Loss: 0.1701 | 0.2177
Epoch 268/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 269/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 270/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 271/300, seasonal_2 Loss: 0.1694 | 0.2177
Epoch 272/300, seasonal_2 Loss: 0.1695 | 0.2177
Epoch 273/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 274/300, seasonal_2 Loss: 0.1696 | 0.2177
Epoch 275/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 276/300, seasonal_2 Loss: 0.1694 | 0.2177
Epoch 277/300, seasonal_2 Loss: 0.1701 | 0.2177
Epoch 278/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 279/300, seasonal_2 Loss: 0.1694 | 0.2177
Epoch 280/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 281/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 282/300, seasonal_2 Loss: 0.1702 | 0.2177
Epoch 283/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 284/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 285/300, seasonal_2 Loss: 0.1698 | 0.2177
Epoch 286/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 287/300, seasonal_2 Loss: 0.1705 | 0.2177
Epoch 288/300, seasonal_2 Loss: 0.1703 | 0.2177
Epoch 289/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 290/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 291/300, seasonal_2 Loss: 0.1694 | 0.2177
Epoch 292/300, seasonal_2 Loss: 0.1706 | 0.2177
Epoch 293/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 294/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 295/300, seasonal_2 Loss: 0.1699 | 0.2177
Epoch 296/300, seasonal_2 Loss: 0.1694 | 0.2177
Epoch 297/300, seasonal_2 Loss: 0.1700 | 0.2177
Epoch 298/300, seasonal_2 Loss: 0.1693 | 0.2177
Epoch 299/300, seasonal_2 Loss: 0.1697 | 0.2177
Epoch 300/300, seasonal_2 Loss: 0.1696 | 0.2177
Training seasonal_3 component with params: {'observation_period_num': 209, 'train_rates': 0.9891252288601043, 'learning_rate': 0.0002936601291895537, 'batch_size': 95, 'step_size': 10, 'gamma': 0.8812938321756656}
Epoch 1/300, seasonal_3 Loss: 1.0295 | 1.4580
Epoch 2/300, seasonal_3 Loss: 0.8555 | 1.0283
Epoch 3/300, seasonal_3 Loss: 0.6197 | 0.8931
Epoch 4/300, seasonal_3 Loss: 0.6896 | 0.9655
Epoch 5/300, seasonal_3 Loss: 0.6087 | 0.7724
Epoch 6/300, seasonal_3 Loss: 0.5395 | 0.7258
Epoch 7/300, seasonal_3 Loss: 0.4926 | 0.6293
Epoch 8/300, seasonal_3 Loss: 0.4397 | 0.5643
Epoch 9/300, seasonal_3 Loss: 0.4124 | 0.5278
Epoch 10/300, seasonal_3 Loss: 0.4156 | 0.5097
Epoch 11/300, seasonal_3 Loss: 0.4438 | 0.5050
Epoch 12/300, seasonal_3 Loss: 0.4262 | 0.4587
Epoch 13/300, seasonal_3 Loss: 0.4041 | 0.4827
Epoch 14/300, seasonal_3 Loss: 0.3831 | 0.4431
Epoch 15/300, seasonal_3 Loss: 0.3609 | 0.4418
Epoch 16/300, seasonal_3 Loss: 0.3361 | 0.3707
Epoch 17/300, seasonal_3 Loss: 0.3549 | 0.3844
Epoch 18/300, seasonal_3 Loss: 0.3117 | 0.3605
Epoch 19/300, seasonal_3 Loss: 0.2797 | 0.3553
Epoch 20/300, seasonal_3 Loss: 0.2799 | 0.3319
Epoch 21/300, seasonal_3 Loss: 0.2954 | 0.3394
Epoch 22/300, seasonal_3 Loss: 0.2735 | 0.3149
Epoch 23/300, seasonal_3 Loss: 0.2666 | 0.3106
Epoch 24/300, seasonal_3 Loss: 0.3201 | 0.3093
Epoch 25/300, seasonal_3 Loss: 0.2989 | 0.3028
Epoch 26/300, seasonal_3 Loss: 0.3082 | 0.2871
Epoch 27/300, seasonal_3 Loss: 0.3153 | 0.3089
Epoch 28/300, seasonal_3 Loss: 0.2787 | 0.3201
Epoch 29/300, seasonal_3 Loss: 0.2647 | 0.2909
Epoch 30/300, seasonal_3 Loss: 0.2478 | 0.2838
Epoch 31/300, seasonal_3 Loss: 0.2317 | 0.2751
Epoch 32/300, seasonal_3 Loss: 0.2203 | 0.2661
Epoch 33/300, seasonal_3 Loss: 0.2265 | 0.2675
Epoch 34/300, seasonal_3 Loss: 0.2193 | 0.2517
Epoch 35/300, seasonal_3 Loss: 0.2205 | 0.2602
Epoch 36/300, seasonal_3 Loss: 0.2126 | 0.2467
Epoch 37/300, seasonal_3 Loss: 0.2102 | 0.2480
Epoch 38/300, seasonal_3 Loss: 0.2095 | 0.2471
Epoch 39/300, seasonal_3 Loss: 0.2042 | 0.2369
Epoch 40/300, seasonal_3 Loss: 0.1996 | 0.2410
Epoch 41/300, seasonal_3 Loss: 0.1940 | 0.2301
Epoch 42/300, seasonal_3 Loss: 0.1916 | 0.2366
Epoch 43/300, seasonal_3 Loss: 0.1875 | 0.2245
Epoch 44/300, seasonal_3 Loss: 0.1851 | 0.2296
Epoch 45/300, seasonal_3 Loss: 0.1828 | 0.2193
Epoch 46/300, seasonal_3 Loss: 0.1809 | 0.2214
Epoch 47/300, seasonal_3 Loss: 0.1799 | 0.2154
Epoch 48/300, seasonal_3 Loss: 0.1804 | 0.2161
Epoch 49/300, seasonal_3 Loss: 0.1792 | 0.2104
Epoch 50/300, seasonal_3 Loss: 0.1762 | 0.2131
Epoch 51/300, seasonal_3 Loss: 0.1738 | 0.2069
Epoch 52/300, seasonal_3 Loss: 0.1723 | 0.2073
Epoch 53/300, seasonal_3 Loss: 0.1719 | 0.2038
Epoch 54/300, seasonal_3 Loss: 0.1707 | 0.2019
Epoch 55/300, seasonal_3 Loss: 0.1719 | 0.1982
Epoch 56/300, seasonal_3 Loss: 0.1698 | 0.2001
Epoch 57/300, seasonal_3 Loss: 0.1676 | 0.1949
Epoch 58/300, seasonal_3 Loss: 0.1666 | 0.1964
Epoch 59/300, seasonal_3 Loss: 0.1666 | 0.1963
Epoch 60/300, seasonal_3 Loss: 0.1686 | 0.1938
Epoch 61/300, seasonal_3 Loss: 0.1721 | 0.1883
Epoch 62/300, seasonal_3 Loss: 0.1685 | 0.1976
Epoch 63/300, seasonal_3 Loss: 0.1673 | 0.1890
Epoch 64/300, seasonal_3 Loss: 0.1686 | 0.1867
Epoch 65/300, seasonal_3 Loss: 0.1685 | 0.1970
Epoch 66/300, seasonal_3 Loss: 0.1661 | 0.1863
Epoch 67/300, seasonal_3 Loss: 0.1659 | 0.1845
Epoch 68/300, seasonal_3 Loss: 0.1660 | 0.1897
Epoch 69/300, seasonal_3 Loss: 0.1610 | 0.1806
Epoch 70/300, seasonal_3 Loss: 0.1598 | 0.1848
Epoch 71/300, seasonal_3 Loss: 0.1630 | 0.1818
Epoch 72/300, seasonal_3 Loss: 0.1593 | 0.1782
Epoch 73/300, seasonal_3 Loss: 0.1569 | 0.1816
Epoch 74/300, seasonal_3 Loss: 0.1549 | 0.1776
Epoch 75/300, seasonal_3 Loss: 0.1540 | 0.1779
Epoch 76/300, seasonal_3 Loss: 0.1548 | 0.1786
Epoch 77/300, seasonal_3 Loss: 0.1518 | 0.1758
Epoch 78/300, seasonal_3 Loss: 0.1513 | 0.1780
Epoch 79/300, seasonal_3 Loss: 0.1498 | 0.1759
Epoch 80/300, seasonal_3 Loss: 0.1506 | 0.1742
Epoch 81/300, seasonal_3 Loss: 0.1490 | 0.1744
Epoch 82/300, seasonal_3 Loss: 0.1482 | 0.1724
Epoch 83/300, seasonal_3 Loss: 0.1471 | 0.1729
Epoch 84/300, seasonal_3 Loss: 0.1467 | 0.1722
Epoch 85/300, seasonal_3 Loss: 0.1454 | 0.1722
Epoch 86/300, seasonal_3 Loss: 0.1456 | 0.1719
Epoch 87/300, seasonal_3 Loss: 0.1446 | 0.1701
Epoch 88/300, seasonal_3 Loss: 0.1445 | 0.1701
Epoch 89/300, seasonal_3 Loss: 0.1433 | 0.1692
Epoch 90/300, seasonal_3 Loss: 0.1432 | 0.1684
Epoch 91/300, seasonal_3 Loss: 0.1420 | 0.1682
Epoch 92/300, seasonal_3 Loss: 0.1426 | 0.1678
Epoch 93/300, seasonal_3 Loss: 0.1418 | 0.1659
Epoch 94/300, seasonal_3 Loss: 0.1416 | 0.1673
Epoch 95/300, seasonal_3 Loss: 0.1425 | 0.1676
Epoch 96/300, seasonal_3 Loss: 0.1416 | 0.1655
Epoch 97/300, seasonal_3 Loss: 0.1404 | 0.1651
Epoch 98/300, seasonal_3 Loss: 0.1406 | 0.1652
Epoch 99/300, seasonal_3 Loss: 0.1384 | 0.1659
Epoch 100/300, seasonal_3 Loss: 0.1390 | 0.1637
Epoch 101/300, seasonal_3 Loss: 0.1386 | 0.1641
Epoch 102/300, seasonal_3 Loss: 0.1380 | 0.1638
Epoch 103/300, seasonal_3 Loss: 0.1377 | 0.1638
Epoch 104/300, seasonal_3 Loss: 0.1385 | 0.1627
Epoch 105/300, seasonal_3 Loss: 0.1372 | 0.1621
Epoch 106/300, seasonal_3 Loss: 0.1380 | 0.1612
Epoch 107/300, seasonal_3 Loss: 0.1373 | 0.1618
Epoch 108/300, seasonal_3 Loss: 0.1377 | 0.1615
Epoch 109/300, seasonal_3 Loss: 0.1357 | 0.1598
Epoch 110/300, seasonal_3 Loss: 0.1361 | 0.1603
Epoch 111/300, seasonal_3 Loss: 0.1360 | 0.1604
Epoch 112/300, seasonal_3 Loss: 0.1360 | 0.1609
Epoch 113/300, seasonal_3 Loss: 0.1353 | 0.1594
Epoch 114/300, seasonal_3 Loss: 0.1351 | 0.1596
Epoch 115/300, seasonal_3 Loss: 0.1351 | 0.1597
Epoch 116/300, seasonal_3 Loss: 0.1350 | 0.1588
Epoch 117/300, seasonal_3 Loss: 0.1346 | 0.1591
Epoch 118/300, seasonal_3 Loss: 0.1343 | 0.1589
Epoch 119/300, seasonal_3 Loss: 0.1342 | 0.1585
Epoch 120/300, seasonal_3 Loss: 0.1342 | 0.1581
Epoch 121/300, seasonal_3 Loss: 0.1346 | 0.1573
Epoch 122/300, seasonal_3 Loss: 0.1339 | 0.1581
Epoch 123/300, seasonal_3 Loss: 0.1333 | 0.1575
Epoch 124/300, seasonal_3 Loss: 0.1334 | 0.1578
Epoch 125/300, seasonal_3 Loss: 0.1327 | 0.1570
Epoch 126/300, seasonal_3 Loss: 0.1328 | 0.1567
Epoch 127/300, seasonal_3 Loss: 0.1323 | 0.1559
Epoch 128/300, seasonal_3 Loss: 0.1316 | 0.1554
Epoch 129/300, seasonal_3 Loss: 0.1322 | 0.1564
Epoch 130/300, seasonal_3 Loss: 0.1320 | 0.1557
Epoch 131/300, seasonal_3 Loss: 0.1317 | 0.1555
Epoch 132/300, seasonal_3 Loss: 0.1312 | 0.1548
Epoch 133/300, seasonal_3 Loss: 0.1311 | 0.1555
Epoch 134/300, seasonal_3 Loss: 0.1312 | 0.1554
Epoch 135/300, seasonal_3 Loss: 0.1315 | 0.1557
Epoch 136/300, seasonal_3 Loss: 0.1314 | 0.1557
Epoch 137/300, seasonal_3 Loss: 0.1309 | 0.1551
Epoch 138/300, seasonal_3 Loss: 0.1310 | 0.1538
Epoch 139/300, seasonal_3 Loss: 0.1306 | 0.1538
Epoch 140/300, seasonal_3 Loss: 0.1311 | 0.1540
Epoch 141/300, seasonal_3 Loss: 0.1300 | 0.1538
Epoch 142/300, seasonal_3 Loss: 0.1308 | 0.1535
Epoch 143/300, seasonal_3 Loss: 0.1308 | 0.1534
Epoch 144/300, seasonal_3 Loss: 0.1300 | 0.1536
Epoch 145/300, seasonal_3 Loss: 0.1294 | 0.1532
Epoch 146/300, seasonal_3 Loss: 0.1298 | 0.1536
Epoch 147/300, seasonal_3 Loss: 0.1295 | 0.1525
Epoch 148/300, seasonal_3 Loss: 0.1295 | 0.1527
Epoch 149/300, seasonal_3 Loss: 0.1299 | 0.1529
Epoch 150/300, seasonal_3 Loss: 0.1295 | 0.1533
Epoch 151/300, seasonal_3 Loss: 0.1288 | 0.1526
Epoch 152/300, seasonal_3 Loss: 0.1291 | 0.1523
Epoch 153/300, seasonal_3 Loss: 0.1292 | 0.1524
Epoch 154/300, seasonal_3 Loss: 0.1291 | 0.1521
Epoch 155/300, seasonal_3 Loss: 0.1285 | 0.1525
Epoch 156/300, seasonal_3 Loss: 0.1287 | 0.1527
Epoch 157/300, seasonal_3 Loss: 0.1279 | 0.1521
Epoch 158/300, seasonal_3 Loss: 0.1287 | 0.1524
Epoch 159/300, seasonal_3 Loss: 0.1284 | 0.1523
Epoch 160/300, seasonal_3 Loss: 0.1283 | 0.1524
Epoch 161/300, seasonal_3 Loss: 0.1294 | 0.1521
Epoch 162/300, seasonal_3 Loss: 0.1277 | 0.1515
Epoch 163/300, seasonal_3 Loss: 0.1283 | 0.1518
Epoch 164/300, seasonal_3 Loss: 0.1281 | 0.1514
Epoch 165/300, seasonal_3 Loss: 0.1277 | 0.1509
Epoch 166/300, seasonal_3 Loss: 0.1280 | 0.1511
Epoch 167/300, seasonal_3 Loss: 0.1281 | 0.1510
Epoch 168/300, seasonal_3 Loss: 0.1278 | 0.1508
Epoch 169/300, seasonal_3 Loss: 0.1270 | 0.1508
Epoch 170/300, seasonal_3 Loss: 0.1277 | 0.1508
Epoch 171/300, seasonal_3 Loss: 0.1271 | 0.1505
Epoch 172/300, seasonal_3 Loss: 0.1268 | 0.1505
Epoch 173/300, seasonal_3 Loss: 0.1275 | 0.1504
Epoch 174/300, seasonal_3 Loss: 0.1266 | 0.1507
Epoch 175/300, seasonal_3 Loss: 0.1272 | 0.1506
Epoch 176/300, seasonal_3 Loss: 0.1282 | 0.1506
Epoch 177/300, seasonal_3 Loss: 0.1273 | 0.1509
Epoch 178/300, seasonal_3 Loss: 0.1268 | 0.1508
Epoch 179/300, seasonal_3 Loss: 0.1268 | 0.1506
Epoch 180/300, seasonal_3 Loss: 0.1264 | 0.1500
Epoch 181/300, seasonal_3 Loss: 0.1270 | 0.1498
Epoch 182/300, seasonal_3 Loss: 0.1266 | 0.1499
Epoch 183/300, seasonal_3 Loss: 0.1269 | 0.1496
Epoch 184/300, seasonal_3 Loss: 0.1276 | 0.1493
Epoch 185/300, seasonal_3 Loss: 0.1262 | 0.1498
Epoch 186/300, seasonal_3 Loss: 0.1264 | 0.1499
Epoch 187/300, seasonal_3 Loss: 0.1257 | 0.1497
Epoch 188/300, seasonal_3 Loss: 0.1259 | 0.1498
Epoch 189/300, seasonal_3 Loss: 0.1259 | 0.1495
Epoch 190/300, seasonal_3 Loss: 0.1260 | 0.1494
Epoch 191/300, seasonal_3 Loss: 0.1261 | 0.1494
Epoch 192/300, seasonal_3 Loss: 0.1260 | 0.1496
Epoch 193/300, seasonal_3 Loss: 0.1258 | 0.1496
Epoch 194/300, seasonal_3 Loss: 0.1264 | 0.1498
Epoch 195/300, seasonal_3 Loss: 0.1261 | 0.1495
Epoch 196/300, seasonal_3 Loss: 0.1261 | 0.1494
Epoch 197/300, seasonal_3 Loss: 0.1258 | 0.1494
Epoch 198/300, seasonal_3 Loss: 0.1262 | 0.1496
Epoch 199/300, seasonal_3 Loss: 0.1261 | 0.1495
Epoch 200/300, seasonal_3 Loss: 0.1256 | 0.1493
Epoch 201/300, seasonal_3 Loss: 0.1258 | 0.1492
Epoch 202/300, seasonal_3 Loss: 0.1257 | 0.1494
Epoch 203/300, seasonal_3 Loss: 0.1257 | 0.1492
Epoch 204/300, seasonal_3 Loss: 0.1258 | 0.1489
Epoch 205/300, seasonal_3 Loss: 0.1258 | 0.1488
Epoch 206/300, seasonal_3 Loss: 0.1260 | 0.1489
Epoch 207/300, seasonal_3 Loss: 0.1261 | 0.1489
Epoch 208/300, seasonal_3 Loss: 0.1258 | 0.1491
Epoch 209/300, seasonal_3 Loss: 0.1252 | 0.1491
Epoch 210/300, seasonal_3 Loss: 0.1262 | 0.1488
Epoch 211/300, seasonal_3 Loss: 0.1249 | 0.1487
Epoch 212/300, seasonal_3 Loss: 0.1255 | 0.1488
Epoch 213/300, seasonal_3 Loss: 0.1259 | 0.1488
Epoch 214/300, seasonal_3 Loss: 0.1255 | 0.1488
Epoch 215/300, seasonal_3 Loss: 0.1256 | 0.1488
Epoch 216/300, seasonal_3 Loss: 0.1256 | 0.1490
Epoch 217/300, seasonal_3 Loss: 0.1258 | 0.1491
Epoch 218/300, seasonal_3 Loss: 0.1256 | 0.1490
Epoch 219/300, seasonal_3 Loss: 0.1253 | 0.1489
Epoch 220/300, seasonal_3 Loss: 0.1246 | 0.1488
Epoch 221/300, seasonal_3 Loss: 0.1253 | 0.1488
Epoch 222/300, seasonal_3 Loss: 0.1257 | 0.1489
Epoch 223/300, seasonal_3 Loss: 0.1253 | 0.1488
Epoch 224/300, seasonal_3 Loss: 0.1258 | 0.1488
Epoch 225/300, seasonal_3 Loss: 0.1253 | 0.1489
Epoch 226/300, seasonal_3 Loss: 0.1253 | 0.1490
Epoch 227/300, seasonal_3 Loss: 0.1250 | 0.1489
Epoch 228/300, seasonal_3 Loss: 0.1255 | 0.1488
Epoch 229/300, seasonal_3 Loss: 0.1256 | 0.1489
Epoch 230/300, seasonal_3 Loss: 0.1253 | 0.1487
Epoch 231/300, seasonal_3 Loss: 0.1253 | 0.1486
Epoch 232/300, seasonal_3 Loss: 0.1259 | 0.1484
Epoch 233/300, seasonal_3 Loss: 0.1247 | 0.1485
Epoch 234/300, seasonal_3 Loss: 0.1256 | 0.1485
Epoch 235/300, seasonal_3 Loss: 0.1253 | 0.1485
Epoch 236/300, seasonal_3 Loss: 0.1258 | 0.1484
Epoch 237/300, seasonal_3 Loss: 0.1255 | 0.1484
Epoch 238/300, seasonal_3 Loss: 0.1260 | 0.1485
Epoch 239/300, seasonal_3 Loss: 0.1251 | 0.1484
Epoch 240/300, seasonal_3 Loss: 0.1246 | 0.1483
Epoch 241/300, seasonal_3 Loss: 0.1255 | 0.1485
Epoch 242/300, seasonal_3 Loss: 0.1251 | 0.1485
Epoch 243/300, seasonal_3 Loss: 0.1256 | 0.1484
Epoch 244/300, seasonal_3 Loss: 0.1253 | 0.1484
Epoch 245/300, seasonal_3 Loss: 0.1248 | 0.1484
Epoch 246/300, seasonal_3 Loss: 0.1256 | 0.1483
Epoch 247/300, seasonal_3 Loss: 0.1253 | 0.1483
Epoch 248/300, seasonal_3 Loss: 0.1252 | 0.1484
Epoch 249/300, seasonal_3 Loss: 0.1249 | 0.1484
Epoch 250/300, seasonal_3 Loss: 0.1249 | 0.1484
Epoch 251/300, seasonal_3 Loss: 0.1252 | 0.1485
Epoch 252/300, seasonal_3 Loss: 0.1249 | 0.1484
Epoch 253/300, seasonal_3 Loss: 0.1254 | 0.1484
Epoch 254/300, seasonal_3 Loss: 0.1250 | 0.1485
Epoch 255/300, seasonal_3 Loss: 0.1243 | 0.1485
Epoch 256/300, seasonal_3 Loss: 0.1254 | 0.1484
Epoch 257/300, seasonal_3 Loss: 0.1249 | 0.1484
Epoch 258/300, seasonal_3 Loss: 0.1251 | 0.1483
Epoch 259/300, seasonal_3 Loss: 0.1248 | 0.1484
Epoch 260/300, seasonal_3 Loss: 0.1251 | 0.1484
Epoch 261/300, seasonal_3 Loss: 0.1252 | 0.1483
Epoch 262/300, seasonal_3 Loss: 0.1245 | 0.1484
Epoch 263/300, seasonal_3 Loss: 0.1249 | 0.1483
Epoch 264/300, seasonal_3 Loss: 0.1244 | 0.1483
Epoch 265/300, seasonal_3 Loss: 0.1252 | 0.1482
Epoch 266/300, seasonal_3 Loss: 0.1247 | 0.1481
Epoch 267/300, seasonal_3 Loss: 0.1251 | 0.1481
Epoch 268/300, seasonal_3 Loss: 0.1252 | 0.1481
Epoch 269/300, seasonal_3 Loss: 0.1243 | 0.1481
Epoch 270/300, seasonal_3 Loss: 0.1246 | 0.1481
Epoch 271/300, seasonal_3 Loss: 0.1251 | 0.1480
Epoch 272/300, seasonal_3 Loss: 0.1242 | 0.1480
Epoch 273/300, seasonal_3 Loss: 0.1246 | 0.1480
Epoch 274/300, seasonal_3 Loss: 0.1254 | 0.1479
Epoch 275/300, seasonal_3 Loss: 0.1249 | 0.1479
Epoch 276/300, seasonal_3 Loss: 0.1252 | 0.1479
Epoch 277/300, seasonal_3 Loss: 0.1244 | 0.1479
Epoch 278/300, seasonal_3 Loss: 0.1251 | 0.1479
Epoch 279/300, seasonal_3 Loss: 0.1257 | 0.1479
Epoch 280/300, seasonal_3 Loss: 0.1241 | 0.1479
Epoch 281/300, seasonal_3 Loss: 0.1246 | 0.1479
Epoch 282/300, seasonal_3 Loss: 0.1240 | 0.1479
Epoch 283/300, seasonal_3 Loss: 0.1252 | 0.1479
Epoch 284/300, seasonal_3 Loss: 0.1253 | 0.1479
Epoch 285/300, seasonal_3 Loss: 0.1247 | 0.1479
Epoch 286/300, seasonal_3 Loss: 0.1248 | 0.1479
Epoch 287/300, seasonal_3 Loss: 0.1244 | 0.1480
Epoch 288/300, seasonal_3 Loss: 0.1248 | 0.1479
Epoch 289/300, seasonal_3 Loss: 0.1246 | 0.1479
Epoch 290/300, seasonal_3 Loss: 0.1244 | 0.1480
Epoch 291/300, seasonal_3 Loss: 0.1254 | 0.1479
Epoch 292/300, seasonal_3 Loss: 0.1242 | 0.1479
Epoch 293/300, seasonal_3 Loss: 0.1244 | 0.1479
Epoch 294/300, seasonal_3 Loss: 0.1244 | 0.1479
Epoch 295/300, seasonal_3 Loss: 0.1245 | 0.1479
Epoch 296/300, seasonal_3 Loss: 0.1252 | 0.1479
Epoch 297/300, seasonal_3 Loss: 0.1243 | 0.1480
Epoch 298/300, seasonal_3 Loss: 0.1249 | 0.1480
Epoch 299/300, seasonal_3 Loss: 0.1239 | 0.1480
Epoch 300/300, seasonal_3 Loss: 0.1247 | 0.1480
Training resid component with params: {'observation_period_num': 58, 'train_rates': 0.9899364055839437, 'learning_rate': 0.0004611532152884647, 'batch_size': 124, 'step_size': 15, 'gamma': 0.8760399876068905}
Epoch 1/300, resid Loss: 1.0224 | 1.6247
Epoch 2/300, resid Loss: 0.9718 | 1.2725
Epoch 3/300, resid Loss: 0.7049 | 1.1201
Epoch 4/300, resid Loss: 0.6826 | 0.9737
Epoch 5/300, resid Loss: 0.6219 | 0.9990
Epoch 6/300, resid Loss: 0.6702 | 0.9021
Epoch 7/300, resid Loss: 0.6234 | 0.8204
Epoch 8/300, resid Loss: 0.6381 | 0.7311
Epoch 9/300, resid Loss: 0.6324 | 0.6596
Epoch 10/300, resid Loss: 0.5599 | 0.6051
Epoch 11/300, resid Loss: 0.5607 | 0.5945
Epoch 12/300, resid Loss: 0.5381 | 0.6341
Epoch 13/300, resid Loss: 0.4829 | 0.6035
Epoch 14/300, resid Loss: 0.5655 | 0.5393
Epoch 15/300, resid Loss: 0.4294 | 0.5490
Epoch 16/300, resid Loss: 0.4050 | 0.5247
Epoch 17/300, resid Loss: 0.4055 | 0.4987
Epoch 18/300, resid Loss: 0.4087 | 0.4981
Epoch 19/300, resid Loss: 0.4295 | 0.4479
Epoch 20/300, resid Loss: 0.4412 | 0.4837
Epoch 21/300, resid Loss: 0.4021 | 0.4622
Epoch 22/300, resid Loss: 0.3972 | 0.4426
Epoch 23/300, resid Loss: 0.4182 | 0.5578
Epoch 24/300, resid Loss: 0.4240 | 0.4568
Epoch 25/300, resid Loss: 0.4229 | 0.4228
Epoch 26/300, resid Loss: 0.4081 | 0.3963
Epoch 27/300, resid Loss: 0.3472 | 0.3919
Epoch 28/300, resid Loss: 0.3212 | 0.3566
Epoch 29/300, resid Loss: 0.3518 | 0.3491
Epoch 30/300, resid Loss: 0.3166 | 0.3624
Epoch 31/300, resid Loss: 0.3258 | 0.3127
Epoch 32/300, resid Loss: 0.2824 | 0.3272
Epoch 33/300, resid Loss: 0.2802 | 0.2974
Epoch 34/300, resid Loss: 0.2666 | 0.2990
Epoch 35/300, resid Loss: 0.2757 | 0.2851
Epoch 36/300, resid Loss: 0.3050 | 0.3443
Epoch 37/300, resid Loss: 0.3191 | 0.2791
Epoch 38/300, resid Loss: 0.3642 | 0.3421
Epoch 39/300, resid Loss: 0.4016 | 0.4560
Epoch 40/300, resid Loss: 0.3397 | 0.4106
Epoch 41/300, resid Loss: 0.3081 | 0.3663
Epoch 42/300, resid Loss: 0.2664 | 0.3100
Epoch 43/300, resid Loss: 0.2576 | 0.3046
Epoch 44/300, resid Loss: 0.2725 | 0.2759
Epoch 45/300, resid Loss: 0.2808 | 0.2959
Epoch 46/300, resid Loss: 0.2870 | 0.2528
Epoch 47/300, resid Loss: 0.2446 | 0.2560
Epoch 48/300, resid Loss: 0.2320 | 0.2420
Epoch 49/300, resid Loss: 0.2203 | 0.2469
Epoch 50/300, resid Loss: 0.2207 | 0.2350
Epoch 51/300, resid Loss: 0.2188 | 0.2417
Epoch 52/300, resid Loss: 0.2207 | 0.2262
Epoch 53/300, resid Loss: 0.2285 | 0.2380
Epoch 54/300, resid Loss: 0.2173 | 0.2167
Epoch 55/300, resid Loss: 0.2133 | 0.2241
Epoch 56/300, resid Loss: 0.2068 | 0.2094
Epoch 57/300, resid Loss: 0.2025 | 0.2128
Epoch 58/300, resid Loss: 0.2000 | 0.2027
Epoch 59/300, resid Loss: 0.2050 | 0.2090
Epoch 60/300, resid Loss: 0.2186 | 0.2073
Epoch 61/300, resid Loss: 0.2202 | 0.2172
Epoch 62/300, resid Loss: 0.2266 | 0.2106
Epoch 63/300, resid Loss: 0.2168 | 0.2134
Epoch 64/300, resid Loss: 0.2242 | 0.2157
Epoch 65/300, resid Loss: 0.2260 | 0.2131
Epoch 66/300, resid Loss: 0.2388 | 0.2202
Epoch 67/300, resid Loss: 0.2322 | 0.2075
Epoch 68/300, resid Loss: 0.2288 | 0.2140
Epoch 69/300, resid Loss: 0.2119 | 0.2116
Epoch 70/300, resid Loss: 0.1942 | 0.1943
Epoch 71/300, resid Loss: 0.1847 | 0.1899
Epoch 72/300, resid Loss: 0.1799 | 0.1829
Epoch 73/300, resid Loss: 0.1763 | 0.1845
Epoch 74/300, resid Loss: 0.1742 | 0.1780
Epoch 75/300, resid Loss: 0.1728 | 0.1791
Epoch 76/300, resid Loss: 0.1716 | 0.1753
Epoch 77/300, resid Loss: 0.1691 | 0.1769
Epoch 78/300, resid Loss: 0.1685 | 0.1714
Epoch 79/300, resid Loss: 0.1674 | 0.1739
Epoch 80/300, resid Loss: 0.1661 | 0.1703
Epoch 81/300, resid Loss: 0.1660 | 0.1756
Epoch 82/300, resid Loss: 0.1668 | 0.1664
Epoch 83/300, resid Loss: 0.1702 | 0.1736
Epoch 84/300, resid Loss: 0.1704 | 0.1716
Epoch 85/300, resid Loss: 0.1684 | 0.1707
Epoch 86/300, resid Loss: 0.1676 | 0.1699
Epoch 87/300, resid Loss: 0.1658 | 0.1638
Epoch 88/300, resid Loss: 0.1651 | 0.1660
Epoch 89/300, resid Loss: 0.1618 | 0.1628
Epoch 90/300, resid Loss: 0.1604 | 0.1662
Epoch 91/300, resid Loss: 0.1570 | 0.1609
Epoch 92/300, resid Loss: 0.1541 | 0.1592
Epoch 93/300, resid Loss: 0.1524 | 0.1570
Epoch 94/300, resid Loss: 0.1507 | 0.1530
Epoch 95/300, resid Loss: 0.1503 | 0.1576
Epoch 96/300, resid Loss: 0.1489 | 0.1513
Epoch 97/300, resid Loss: 0.1481 | 0.1568
Epoch 98/300, resid Loss: 0.1476 | 0.1508
Epoch 99/300, resid Loss: 0.1454 | 0.1514
Epoch 100/300, resid Loss: 0.1446 | 0.1496
Epoch 101/300, resid Loss: 0.1431 | 0.1514
Epoch 102/300, resid Loss: 0.1428 | 0.1473
Epoch 103/300, resid Loss: 0.1422 | 0.1503
Epoch 104/300, resid Loss: 0.1411 | 0.1473
Epoch 105/300, resid Loss: 0.1404 | 0.1470
Epoch 106/300, resid Loss: 0.1393 | 0.1447
Epoch 107/300, resid Loss: 0.1394 | 0.1475
Epoch 108/300, resid Loss: 0.1386 | 0.1437
Epoch 109/300, resid Loss: 0.1378 | 0.1465
Epoch 110/300, resid Loss: 0.1375 | 0.1430
Epoch 111/300, resid Loss: 0.1369 | 0.1441
Epoch 112/300, resid Loss: 0.1362 | 0.1456
Epoch 113/300, resid Loss: 0.1372 | 0.1419
Epoch 114/300, resid Loss: 0.1397 | 0.1468
Epoch 115/300, resid Loss: 0.1382 | 0.1418
Epoch 116/300, resid Loss: 0.1366 | 0.1430
Epoch 117/300, resid Loss: 0.1361 | 0.1419
Epoch 118/300, resid Loss: 0.1327 | 0.1401
Epoch 119/300, resid Loss: 0.1336 | 0.1465
Epoch 120/300, resid Loss: 0.1365 | 0.1404
Epoch 121/300, resid Loss: 0.1383 | 0.1469
Epoch 122/300, resid Loss: 0.1385 | 0.1378
Epoch 123/300, resid Loss: 0.1347 | 0.1414
Epoch 124/300, resid Loss: 0.1323 | 0.1382
Epoch 125/300, resid Loss: 0.1304 | 0.1396
Epoch 126/300, resid Loss: 0.1297 | 0.1372
Epoch 127/300, resid Loss: 0.1291 | 0.1373
Epoch 128/300, resid Loss: 0.1294 | 0.1366
Epoch 129/300, resid Loss: 0.1286 | 0.1358
Epoch 130/300, resid Loss: 0.1288 | 0.1370
Epoch 131/300, resid Loss: 0.1297 | 0.1360
Epoch 132/300, resid Loss: 0.1294 | 0.1366
Epoch 133/300, resid Loss: 0.1285 | 0.1368
Epoch 134/300, resid Loss: 0.1298 | 0.1374
Epoch 135/300, resid Loss: 0.1297 | 0.1363
Epoch 136/300, resid Loss: 0.1289 | 0.1366
Epoch 137/300, resid Loss: 0.1274 | 0.1356
Epoch 138/300, resid Loss: 0.1265 | 0.1334
Epoch 139/300, resid Loss: 0.1253 | 0.1362
Epoch 140/300, resid Loss: 0.1240 | 0.1350
Epoch 141/300, resid Loss: 0.1233 | 0.1327
Epoch 142/300, resid Loss: 0.1245 | 0.1335
Epoch 143/300, resid Loss: 0.1251 | 0.1325
Epoch 144/300, resid Loss: 0.1235 | 0.1312
Epoch 145/300, resid Loss: 0.1241 | 0.1343
Epoch 146/300, resid Loss: 0.1240 | 0.1324
Epoch 147/300, resid Loss: 0.1247 | 0.1309
Epoch 148/300, resid Loss: 0.1237 | 0.1318
Epoch 149/300, resid Loss: 0.1220 | 0.1330
Epoch 150/300, resid Loss: 0.1225 | 0.1310
Epoch 151/300, resid Loss: 0.1217 | 0.1337
Epoch 152/300, resid Loss: 0.1216 | 0.1313
Epoch 153/300, resid Loss: 0.1211 | 0.1277
Epoch 154/300, resid Loss: 0.1215 | 0.1313
Epoch 155/300, resid Loss: 0.1225 | 0.1301
Epoch 156/300, resid Loss: 0.1203 | 0.1297
Epoch 157/300, resid Loss: 0.1187 | 0.1300
Epoch 158/300, resid Loss: 0.1188 | 0.1284
Epoch 159/300, resid Loss: 0.1183 | 0.1311
Epoch 160/300, resid Loss: 0.1182 | 0.1276
Epoch 161/300, resid Loss: 0.1171 | 0.1295
Epoch 162/300, resid Loss: 0.1175 | 0.1289
Epoch 163/300, resid Loss: 0.1168 | 0.1283
Epoch 164/300, resid Loss: 0.1167 | 0.1302
Epoch 165/300, resid Loss: 0.1171 | 0.1292
Epoch 166/300, resid Loss: 0.1157 | 0.1286
Epoch 167/300, resid Loss: 0.1153 | 0.1289
Epoch 168/300, resid Loss: 0.1159 | 0.1279
Epoch 169/300, resid Loss: 0.1155 | 0.1282
Epoch 170/300, resid Loss: 0.1156 | 0.1285
Epoch 171/300, resid Loss: 0.1152 | 0.1271
Epoch 172/300, resid Loss: 0.1151 | 0.1266
Epoch 173/300, resid Loss: 0.1147 | 0.1263
Epoch 174/300, resid Loss: 0.1150 | 0.1274
Epoch 175/300, resid Loss: 0.1145 | 0.1254
Epoch 176/300, resid Loss: 0.1143 | 0.1275
Epoch 177/300, resid Loss: 0.1136 | 0.1266
Epoch 178/300, resid Loss: 0.1134 | 0.1261
Epoch 179/300, resid Loss: 0.1136 | 0.1257
Epoch 180/300, resid Loss: 0.1139 | 0.1267
Epoch 181/300, resid Loss: 0.1129 | 0.1250
Epoch 182/300, resid Loss: 0.1131 | 0.1260
Epoch 183/300, resid Loss: 0.1128 | 0.1259
Epoch 184/300, resid Loss: 0.1126 | 0.1251
Epoch 185/300, resid Loss: 0.1121 | 0.1244
Epoch 186/300, resid Loss: 0.1124 | 0.1250
Epoch 187/300, resid Loss: 0.1120 | 0.1245
Epoch 188/300, resid Loss: 0.1118 | 0.1247
Epoch 189/300, resid Loss: 0.1120 | 0.1252
Epoch 190/300, resid Loss: 0.1123 | 0.1253
Epoch 191/300, resid Loss: 0.1112 | 0.1249
Epoch 192/300, resid Loss: 0.1114 | 0.1241
Epoch 193/300, resid Loss: 0.1109 | 0.1258
Epoch 194/300, resid Loss: 0.1105 | 0.1242
Epoch 195/300, resid Loss: 0.1102 | 0.1249
Epoch 196/300, resid Loss: 0.1103 | 0.1249
Epoch 197/300, resid Loss: 0.1109 | 0.1245
Epoch 198/300, resid Loss: 0.1105 | 0.1241
Epoch 199/300, resid Loss: 0.1112 | 0.1239
Epoch 200/300, resid Loss: 0.1098 | 0.1238
Epoch 201/300, resid Loss: 0.1100 | 0.1232
Epoch 202/300, resid Loss: 0.1099 | 0.1231
Epoch 203/300, resid Loss: 0.1104 | 0.1240
Epoch 204/300, resid Loss: 0.1093 | 0.1238
Epoch 205/300, resid Loss: 0.1098 | 0.1241
Epoch 206/300, resid Loss: 0.1095 | 0.1235
Epoch 207/300, resid Loss: 0.1094 | 0.1235
Epoch 208/300, resid Loss: 0.1088 | 0.1231
Epoch 209/300, resid Loss: 0.1094 | 0.1232
Epoch 210/300, resid Loss: 0.1090 | 0.1231
Epoch 211/300, resid Loss: 0.1087 | 0.1232
Epoch 212/300, resid Loss: 0.1088 | 0.1230
Epoch 213/300, resid Loss: 0.1092 | 0.1228
Epoch 214/300, resid Loss: 0.1085 | 0.1231
Epoch 215/300, resid Loss: 0.1083 | 0.1223
Epoch 216/300, resid Loss: 0.1087 | 0.1226
Epoch 217/300, resid Loss: 0.1076 | 0.1225
Epoch 218/300, resid Loss: 0.1083 | 0.1226
Epoch 219/300, resid Loss: 0.1080 | 0.1221
Epoch 220/300, resid Loss: 0.1083 | 0.1224
Epoch 221/300, resid Loss: 0.1086 | 0.1224
Epoch 222/300, resid Loss: 0.1077 | 0.1224
Epoch 223/300, resid Loss: 0.1077 | 0.1226
Epoch 224/300, resid Loss: 0.1071 | 0.1220
Epoch 225/300, resid Loss: 0.1069 | 0.1211
Epoch 226/300, resid Loss: 0.1072 | 0.1219
Epoch 227/300, resid Loss: 0.1074 | 0.1226
Epoch 228/300, resid Loss: 0.1076 | 0.1223
Epoch 229/300, resid Loss: 0.1078 | 0.1218
Epoch 230/300, resid Loss: 0.1068 | 0.1208
Epoch 231/300, resid Loss: 0.1072 | 0.1215
Epoch 232/300, resid Loss: 0.1061 | 0.1216
Epoch 233/300, resid Loss: 0.1062 | 0.1215
Epoch 234/300, resid Loss: 0.1070 | 0.1215
Epoch 235/300, resid Loss: 0.1066 | 0.1214
Epoch 236/300, resid Loss: 0.1062 | 0.1212
Epoch 237/300, resid Loss: 0.1057 | 0.1216
Epoch 238/300, resid Loss: 0.1061 | 0.1214
Epoch 239/300, resid Loss: 0.1065 | 0.1209
Epoch 240/300, resid Loss: 0.1060 | 0.1204
Epoch 241/300, resid Loss: 0.1065 | 0.1205
Epoch 242/300, resid Loss: 0.1060 | 0.1207
Epoch 243/300, resid Loss: 0.1052 | 0.1201
Epoch 244/300, resid Loss: 0.1057 | 0.1206
Epoch 245/300, resid Loss: 0.1053 | 0.1202
Epoch 246/300, resid Loss: 0.1049 | 0.1204
Epoch 247/300, resid Loss: 0.1057 | 0.1205
Epoch 248/300, resid Loss: 0.1057 | 0.1203
Epoch 249/300, resid Loss: 0.1059 | 0.1204
Epoch 250/300, resid Loss: 0.1056 | 0.1203
Epoch 251/300, resid Loss: 0.1057 | 0.1205
Epoch 252/300, resid Loss: 0.1044 | 0.1204
Epoch 253/300, resid Loss: 0.1047 | 0.1200
Epoch 254/300, resid Loss: 0.1053 | 0.1200
Epoch 255/300, resid Loss: 0.1053 | 0.1198
Epoch 256/300, resid Loss: 0.1050 | 0.1197
Epoch 257/300, resid Loss: 0.1050 | 0.1199
Epoch 258/300, resid Loss: 0.1047 | 0.1198
Epoch 259/300, resid Loss: 0.1038 | 0.1200
Epoch 260/300, resid Loss: 0.1050 | 0.1201
Epoch 261/300, resid Loss: 0.1036 | 0.1200
Epoch 262/300, resid Loss: 0.1047 | 0.1200
Epoch 263/300, resid Loss: 0.1049 | 0.1196
Epoch 264/300, resid Loss: 0.1046 | 0.1193
Epoch 265/300, resid Loss: 0.1040 | 0.1193
Epoch 266/300, resid Loss: 0.1042 | 0.1194
Epoch 267/300, resid Loss: 0.1032 | 0.1193
Epoch 268/300, resid Loss: 0.1041 | 0.1196
Epoch 269/300, resid Loss: 0.1047 | 0.1195
Epoch 270/300, resid Loss: 0.1039 | 0.1191
Epoch 271/300, resid Loss: 0.1039 | 0.1190
Epoch 272/300, resid Loss: 0.1045 | 0.1191
Epoch 273/300, resid Loss: 0.1041 | 0.1193
Epoch 274/300, resid Loss: 0.1051 | 0.1193
Epoch 275/300, resid Loss: 0.1045 | 0.1194
Epoch 276/300, resid Loss: 0.1041 | 0.1192
Epoch 277/300, resid Loss: 0.1036 | 0.1192
Epoch 278/300, resid Loss: 0.1033 | 0.1195
Epoch 279/300, resid Loss: 0.1026 | 0.1192
Epoch 280/300, resid Loss: 0.1032 | 0.1192
Epoch 281/300, resid Loss: 0.1039 | 0.1193
Epoch 282/300, resid Loss: 0.1036 | 0.1194
Epoch 283/300, resid Loss: 0.1040 | 0.1198
Epoch 284/300, resid Loss: 0.1041 | 0.1197
Epoch 285/300, resid Loss: 0.1039 | 0.1194
Epoch 286/300, resid Loss: 0.1034 | 0.1196
Epoch 287/300, resid Loss: 0.1033 | 0.1196
Epoch 288/300, resid Loss: 0.1034 | 0.1194
Epoch 289/300, resid Loss: 0.1031 | 0.1196
Epoch 290/300, resid Loss: 0.1033 | 0.1199
Epoch 291/300, resid Loss: 0.1038 | 0.1194
Epoch 292/300, resid Loss: 0.1036 | 0.1190
Epoch 293/300, resid Loss: 0.1034 | 0.1193
Epoch 294/300, resid Loss: 0.1031 | 0.1194
Epoch 295/300, resid Loss: 0.1033 | 0.1193
Epoch 296/300, resid Loss: 0.1035 | 0.1192
Epoch 297/300, resid Loss: 0.1033 | 0.1191
Epoch 298/300, resid Loss: 0.1035 | 0.1189
Epoch 299/300, resid Loss: 0.1027 | 0.1188
Epoch 300/300, resid Loss: 0.1037 | 0.1191
Runtime (seconds): 2477.473947763443
2.2340226588341073e-05
[142.33179]
[-0.20083776]
[-2.8165329]
[10.952449]
[4.658623]
[11.318012]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 8.966888421215117
RMSE: 2.994476318359375
MAE: 2.994476318359375
R-squared: nan
[166.24352]
