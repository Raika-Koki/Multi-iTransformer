ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-03 02:58:10,377][0m A new study created in memory with name: no-name-706a2e64-d0f0-4c99-b7f4-1ba6d33d8742[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-02-03 02:58:40,694][0m Trial 0 finished with value: 0.03950654464003084 and parameters: {'observation_period_num': 27, 'train_rates': 0.8413206569186974, 'learning_rate': 0.0002287770833115434, 'batch_size': 223, 'step_size': 8, 'gamma': 0.7922533722197818}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 02:59:21,298][0m Trial 1 finished with value: 0.45014665382249014 and parameters: {'observation_period_num': 165, 'train_rates': 0.9098646386375431, 'learning_rate': 1.3094373487683451e-06, 'batch_size': 146, 'step_size': 10, 'gamma': 0.9019338353401537}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 03:00:18,609][0m Trial 2 finished with value: 0.552956025390064 and parameters: {'observation_period_num': 224, 'train_rates': 0.6759344248522369, 'learning_rate': 2.541529642280798e-06, 'batch_size': 77, 'step_size': 3, 'gamma': 0.7734348151920752}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 03:01:50,180][0m Trial 3 finished with value: 0.0607743557050543 and parameters: {'observation_period_num': 214, 'train_rates': 0.8259198949564883, 'learning_rate': 0.00013085800624669226, 'batch_size': 54, 'step_size': 6, 'gamma': 0.8176620635929381}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 03:02:14,893][0m Trial 4 finished with value: 0.09388554240785428 and parameters: {'observation_period_num': 196, 'train_rates': 0.6597181773041335, 'learning_rate': 0.0005627292449883291, 'batch_size': 195, 'step_size': 6, 'gamma': 0.8145213846295363}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 03:02:39,783][0m Trial 5 finished with value: 0.27597761154174805 and parameters: {'observation_period_num': 109, 'train_rates': 0.9487295018782599, 'learning_rate': 3.2528760301906433e-06, 'batch_size': 252, 'step_size': 14, 'gamma': 0.9138101424471379}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 03:04:43,494][0m Trial 6 finished with value: 0.3903413657180111 and parameters: {'observation_period_num': 239, 'train_rates': 0.7460759457572617, 'learning_rate': 1.0311918255721475e-06, 'batch_size': 37, 'step_size': 6, 'gamma': 0.8676325653377893}. Best is trial 0 with value: 0.03950654464003084.[0m
[32m[I 2025-02-03 03:06:51,233][0m Trial 7 finished with value: 0.021195567578499046 and parameters: {'observation_period_num': 7, 'train_rates': 0.8199982567082441, 'learning_rate': 0.0004885407036096562, 'batch_size': 42, 'step_size': 6, 'gamma': 0.8961747704014176}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:07:19,154][0m Trial 8 finished with value: 0.05160248617607134 and parameters: {'observation_period_num': 104, 'train_rates': 0.8864667490959716, 'learning_rate': 0.0002840327218499728, 'batch_size': 222, 'step_size': 10, 'gamma': 0.8919798739362144}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:08:17,613][0m Trial 9 finished with value: 0.06564419159601474 and parameters: {'observation_period_num': 94, 'train_rates': 0.7464519723898823, 'learning_rate': 4.545414161788734e-05, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8768282942363188}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:09:07,520][0m Trial 10 finished with value: 0.03436136618256569 and parameters: {'observation_period_num': 5, 'train_rates': 0.9794357532303252, 'learning_rate': 0.0009900803288569252, 'batch_size': 127, 'step_size': 2, 'gamma': 0.9727185848239774}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:09:43,375][0m Trial 11 finished with value: 0.040676656978914504 and parameters: {'observation_period_num': 10, 'train_rates': 0.6101060118956358, 'learning_rate': 0.0005487419173001175, 'batch_size': 129, 'step_size': 1, 'gamma': 0.9825223496767146}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:15:26,723][0m Trial 12 finished with value: 0.04157567998070432 and parameters: {'observation_period_num': 40, 'train_rates': 0.9684451595701558, 'learning_rate': 0.0008537244557973565, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9613805067954001}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:16:11,663][0m Trial 13 finished with value: 0.11244993544412109 and parameters: {'observation_period_num': 59, 'train_rates': 0.7801832920182944, 'learning_rate': 3.676689688160351e-05, 'batch_size': 122, 'step_size': 1, 'gamma': 0.9499765215942403}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:16:51,177][0m Trial 14 finished with value: 0.1641770750284195 and parameters: {'observation_period_num': 70, 'train_rates': 0.9834451418936271, 'learning_rate': 9.999839207355751e-06, 'batch_size': 163, 'step_size': 4, 'gamma': 0.9334620317067868}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:17:50,546][0m Trial 15 finished with value: 0.06701680284532362 and parameters: {'observation_period_num': 139, 'train_rates': 0.8872203858469413, 'learning_rate': 7.294096656241491e-05, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8491890200180817}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:18:25,000][0m Trial 16 finished with value: 0.03609634900222654 and parameters: {'observation_period_num': 7, 'train_rates': 0.8437314441396218, 'learning_rate': 0.0002582611238976897, 'batch_size': 168, 'step_size': 4, 'gamma': 0.9884819474207998}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:19:21,695][0m Trial 17 finished with value: 0.04146184356347339 and parameters: {'observation_period_num': 57, 'train_rates': 0.9244609633350538, 'learning_rate': 0.0008786563200449534, 'batch_size': 108, 'step_size': 12, 'gamma': 0.9259833673531095}. Best is trial 7 with value: 0.021195567578499046.[0m
Early stopping at epoch 90
[32m[I 2025-02-03 03:20:39,578][0m Trial 18 finished with value: 0.24330844814019487 and parameters: {'observation_period_num': 81, 'train_rates': 0.7896530593476169, 'learning_rate': 1.478895927797234e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.7504517391790346}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:24:33,299][0m Trial 19 finished with value: 0.03787298422630166 and parameters: {'observation_period_num': 31, 'train_rates': 0.6963497774144711, 'learning_rate': 0.00010364758462069696, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8496103789815596}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:26:06,739][0m Trial 20 finished with value: 0.05085812515576449 and parameters: {'observation_period_num': 139, 'train_rates': 0.8782015215386922, 'learning_rate': 0.00037760929954062, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9473813793434845}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:26:43,224][0m Trial 21 finished with value: 0.04345398272077242 and parameters: {'observation_period_num': 7, 'train_rates': 0.838217914551649, 'learning_rate': 0.00016463192526059895, 'batch_size': 159, 'step_size': 4, 'gamma': 0.9872267294351691}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:27:14,975][0m Trial 22 finished with value: 0.028604169172000756 and parameters: {'observation_period_num': 5, 'train_rates': 0.7462334854053306, 'learning_rate': 0.000996525583764848, 'batch_size': 180, 'step_size': 5, 'gamma': 0.968586782023801}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:27:44,135][0m Trial 23 finished with value: 0.05034305847866435 and parameters: {'observation_period_num': 42, 'train_rates': 0.7314855680704957, 'learning_rate': 0.0009521415739239843, 'batch_size': 187, 'step_size': 7, 'gamma': 0.9521415011663571}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:28:09,702][0m Trial 24 finished with value: 0.0406111431467599 and parameters: {'observation_period_num': 26, 'train_rates': 0.6286142318753787, 'learning_rate': 0.00045396833824556265, 'batch_size': 202, 'step_size': 5, 'gamma': 0.966702077802654}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:28:54,873][0m Trial 25 finished with value: 0.03853066396946199 and parameters: {'observation_period_num': 52, 'train_rates': 0.7108976124904169, 'learning_rate': 0.0005336928413010755, 'batch_size': 111, 'step_size': 2, 'gamma': 0.9228981132265024}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:29:34,237][0m Trial 26 finished with value: 0.06260216718678797 and parameters: {'observation_period_num': 23, 'train_rates': 0.8081077177594009, 'learning_rate': 1.743941694187156e-05, 'batch_size': 142, 'step_size': 9, 'gamma': 0.970810645973463}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:30:05,019][0m Trial 27 finished with value: 0.050943539395362517 and parameters: {'observation_period_num': 6, 'train_rates': 0.7585830619273994, 'learning_rate': 6.574458996979734e-05, 'batch_size': 178, 'step_size': 5, 'gamma': 0.893999100805166}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:30:31,375][0m Trial 28 finished with value: 0.0573161204667463 and parameters: {'observation_period_num': 80, 'train_rates': 0.7742730908181108, 'learning_rate': 0.00018664291344785015, 'batch_size': 217, 'step_size': 2, 'gamma': 0.9415564823038253}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:30:56,778][0m Trial 29 finished with value: 0.03512301113975199 and parameters: {'observation_period_num': 24, 'train_rates': 0.8650226595817221, 'learning_rate': 0.0002820018596910351, 'batch_size': 236, 'step_size': 8, 'gamma': 0.91146808871601}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:31:38,577][0m Trial 30 finished with value: 0.035547982785642515 and parameters: {'observation_period_num': 39, 'train_rates': 0.9447487933467054, 'learning_rate': 0.000936190404255542, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8302500203023618}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:32:05,508][0m Trial 31 finished with value: 0.036338214442366734 and parameters: {'observation_period_num': 21, 'train_rates': 0.8691896592659488, 'learning_rate': 0.0003276357559599154, 'batch_size': 213, 'step_size': 8, 'gamma': 0.9092809992932941}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:32:31,973][0m Trial 32 finished with value: 0.034012761263720756 and parameters: {'observation_period_num': 23, 'train_rates': 0.8072347661956276, 'learning_rate': 0.0006120933703783882, 'batch_size': 235, 'step_size': 10, 'gamma': 0.886125890986584}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:32:54,328][0m Trial 33 finished with value: 0.06183722101523995 and parameters: {'observation_period_num': 170, 'train_rates': 0.7191332138520525, 'learning_rate': 0.000633439296768298, 'batch_size': 241, 'step_size': 11, 'gamma': 0.8736295803464265}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:33:22,284][0m Trial 34 finished with value: 0.03584391934716183 and parameters: {'observation_period_num': 47, 'train_rates': 0.8022232479210871, 'learning_rate': 0.0003943758483916418, 'batch_size': 206, 'step_size': 12, 'gamma': 0.8892310334548548}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:33:49,790][0m Trial 35 finished with value: 0.030681636902079257 and parameters: {'observation_period_num': 19, 'train_rates': 0.6804441115890826, 'learning_rate': 0.0006626277364096131, 'batch_size': 183, 'step_size': 9, 'gamma': 0.8535362295612957}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:34:17,611][0m Trial 36 finished with value: 0.06255166669399154 and parameters: {'observation_period_num': 72, 'train_rates': 0.6760516247019257, 'learning_rate': 0.00012268913581167337, 'batch_size': 184, 'step_size': 10, 'gamma': 0.7924096349186747}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:34:44,403][0m Trial 37 finished with value: 0.04314060099214815 and parameters: {'observation_period_num': 20, 'train_rates': 0.8169515743742852, 'learning_rate': 0.0001930300679850584, 'batch_size': 235, 'step_size': 9, 'gamma': 0.8544199932843822}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:35:05,279][0m Trial 38 finished with value: 0.04364166442441727 and parameters: {'observation_period_num': 37, 'train_rates': 0.6543610626600883, 'learning_rate': 0.0006827525469196816, 'batch_size': 255, 'step_size': 7, 'gamma': 0.8269416219535137}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:35:33,968][0m Trial 39 finished with value: 0.49018666907593056 and parameters: {'observation_period_num': 120, 'train_rates': 0.7618272791516233, 'learning_rate': 2.840611205138735e-06, 'batch_size': 193, 'step_size': 9, 'gamma': 0.8015138852475286}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:36:03,769][0m Trial 40 finished with value: 0.042746344223519826 and parameters: {'observation_period_num': 66, 'train_rates': 0.6859739160886945, 'learning_rate': 0.00043405298509016166, 'batch_size': 174, 'step_size': 11, 'gamma': 0.8623036892498168}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:38:06,651][0m Trial 41 finished with value: 0.024694065437302466 and parameters: {'observation_period_num': 15, 'train_rates': 0.6555042324738561, 'learning_rate': 0.0006721111924474461, 'batch_size': 37, 'step_size': 5, 'gamma': 0.8384024709344362}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:40:12,967][0m Trial 42 finished with value: 0.03295801341997962 and parameters: {'observation_period_num': 19, 'train_rates': 0.6498704767944614, 'learning_rate': 0.0005973874702814546, 'batch_size': 36, 'step_size': 6, 'gamma': 0.8430827841178782}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:42:10,445][0m Trial 43 finished with value: 0.09522428291115566 and parameters: {'observation_period_num': 13, 'train_rates': 0.6431030777328397, 'learning_rate': 6.044475658785641e-06, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8323635209363529}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:43:44,831][0m Trial 44 finished with value: 0.09856651621121987 and parameters: {'observation_period_num': 249, 'train_rates': 0.6038370266689164, 'learning_rate': 0.0007112207659037259, 'batch_size': 42, 'step_size': 6, 'gamma': 0.8416664619703677}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:44:47,664][0m Trial 45 finished with value: 0.11728101600580591 and parameters: {'observation_period_num': 195, 'train_rates': 0.6268220088211256, 'learning_rate': 0.000262454310808064, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8068667115296329}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:47:24,727][0m Trial 46 finished with value: 0.03883889339686859 and parameters: {'observation_period_num': 47, 'train_rates': 0.6601021099839537, 'learning_rate': 0.0004607399300910957, 'batch_size': 29, 'step_size': 7, 'gamma': 0.819313100375344}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:49:06,522][0m Trial 47 finished with value: 0.38123804564153596 and parameters: {'observation_period_num': 33, 'train_rates': 0.7110261078982872, 'learning_rate': 1.984881699136234e-06, 'batch_size': 48, 'step_size': 3, 'gamma': 0.8638219463478988}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:50:08,770][0m Trial 48 finished with value: 0.02699245701666913 and parameters: {'observation_period_num': 13, 'train_rates': 0.6957874554855756, 'learning_rate': 0.0007187787995500299, 'batch_size': 80, 'step_size': 6, 'gamma': 0.7767291443642249}. Best is trial 7 with value: 0.021195567578499046.[0m
[32m[I 2025-02-03 03:51:07,211][0m Trial 49 finished with value: 0.038390740551889894 and parameters: {'observation_period_num': 93, 'train_rates': 0.6943681184308208, 'learning_rate': 0.0007268056066004568, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7672101197103309}. Best is trial 7 with value: 0.021195567578499046.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-03 03:51:07,221][0m A new study created in memory with name: no-name-256e49ff-435a-4696-b75e-119311c7e928[0m
[32m[I 2025-02-03 03:51:36,502][0m Trial 0 finished with value: 0.5038433424555339 and parameters: {'observation_period_num': 234, 'train_rates': 0.7320717232294081, 'learning_rate': 1.3783580755447833e-06, 'batch_size': 175, 'step_size': 14, 'gamma': 0.9493759311817123}. Best is trial 0 with value: 0.5038433424555339.[0m
[32m[I 2025-02-03 03:52:15,149][0m Trial 1 finished with value: 0.16325531803921564 and parameters: {'observation_period_num': 69, 'train_rates': 0.7414121114368197, 'learning_rate': 1.4923780973513924e-05, 'batch_size': 142, 'step_size': 9, 'gamma': 0.7725397434422908}. Best is trial 1 with value: 0.16325531803921564.[0m
[32m[I 2025-02-03 03:53:03,630][0m Trial 2 finished with value: 0.05714978927221054 and parameters: {'observation_period_num': 76, 'train_rates': 0.9187820296500498, 'learning_rate': 8.526570625626924e-05, 'batch_size': 124, 'step_size': 8, 'gamma': 0.893686994922224}. Best is trial 2 with value: 0.05714978927221054.[0m
[32m[I 2025-02-03 03:53:24,574][0m Trial 3 finished with value: 0.05216721085871154 and parameters: {'observation_period_num': 18, 'train_rates': 0.6794399617477528, 'learning_rate': 0.00013711354072417286, 'batch_size': 249, 'step_size': 10, 'gamma': 0.8901723473355161}. Best is trial 3 with value: 0.05216721085871154.[0m
[32m[I 2025-02-03 03:54:09,188][0m Trial 4 finished with value: 0.04202331929486625 and parameters: {'observation_period_num': 37, 'train_rates': 0.6467912169091027, 'learning_rate': 0.0007827739542687384, 'batch_size': 105, 'step_size': 4, 'gamma': 0.8445919295688191}. Best is trial 4 with value: 0.04202331929486625.[0m
Early stopping at epoch 52
[32m[I 2025-02-03 03:54:28,418][0m Trial 5 finished with value: 0.9497745932019942 and parameters: {'observation_period_num': 124, 'train_rates': 0.7579998871214181, 'learning_rate': 1.356096176248272e-06, 'batch_size': 151, 'step_size': 1, 'gamma': 0.8024441017664932}. Best is trial 4 with value: 0.04202331929486625.[0m
[32m[I 2025-02-03 03:55:37,702][0m Trial 6 finished with value: 0.05141972746023223 and parameters: {'observation_period_num': 85, 'train_rates': 0.7637121741056794, 'learning_rate': 0.0008984363743401591, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9105813625148615}. Best is trial 4 with value: 0.04202331929486625.[0m
[32m[I 2025-02-03 03:55:58,721][0m Trial 7 finished with value: 0.23198481982453628 and parameters: {'observation_period_num': 187, 'train_rates': 0.6804829923936961, 'learning_rate': 9.46596733137441e-06, 'batch_size': 245, 'step_size': 9, 'gamma': 0.9632640382824431}. Best is trial 4 with value: 0.04202331929486625.[0m
[32m[I 2025-02-03 03:58:13,579][0m Trial 8 finished with value: 0.19289153380263765 and parameters: {'observation_period_num': 187, 'train_rates': 0.7217011730838474, 'learning_rate': 1.5214362686511036e-05, 'batch_size': 34, 'step_size': 2, 'gamma': 0.8599784811797111}. Best is trial 4 with value: 0.04202331929486625.[0m
[32m[I 2025-02-03 03:59:03,683][0m Trial 9 finished with value: 0.3813813594372376 and parameters: {'observation_period_num': 144, 'train_rates': 0.8033029998509711, 'learning_rate': 1.2903539902453352e-06, 'batch_size': 107, 'step_size': 14, 'gamma': 0.9142975163371355}. Best is trial 4 with value: 0.04202331929486625.[0m
[32m[I 2025-02-03 04:01:15,327][0m Trial 10 finished with value: 0.03849849917846466 and parameters: {'observation_period_num': 21, 'train_rates': 0.6357734047274068, 'learning_rate': 0.0009404172503993498, 'batch_size': 34, 'step_size': 5, 'gamma': 0.8400798114998606}. Best is trial 10 with value: 0.03849849917846466.[0m
[32m[I 2025-02-03 04:04:21,048][0m Trial 11 finished with value: 0.0316871059037024 and parameters: {'observation_period_num': 14, 'train_rates': 0.6304935327262523, 'learning_rate': 0.000983540941826164, 'batch_size': 24, 'step_size': 5, 'gamma': 0.8410677461870922}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:08:46,737][0m Trial 12 finished with value: 0.05232616850358044 and parameters: {'observation_period_num': 19, 'train_rates': 0.6008803260091411, 'learning_rate': 0.00027534741686382114, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8221303474411505}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:10:30,534][0m Trial 13 finished with value: 0.03790079976897687 and parameters: {'observation_period_num': 46, 'train_rates': 0.845879443865907, 'learning_rate': 0.00032068581922009646, 'batch_size': 53, 'step_size': 5, 'gamma': 0.7556195623303049}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:11:52,545][0m Trial 14 finished with value: 0.03792999857168527 and parameters: {'observation_period_num': 46, 'train_rates': 0.8479868138519943, 'learning_rate': 0.0003171663693952038, 'batch_size': 67, 'step_size': 7, 'gamma': 0.7668521395577426}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:13:21,408][0m Trial 15 finished with value: 0.1075219578946693 and parameters: {'observation_period_num': 107, 'train_rates': 0.9723733668469605, 'learning_rate': 4.885023346428181e-05, 'batch_size': 66, 'step_size': 3, 'gamma': 0.7507279841790533}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:15:17,854][0m Trial 16 finished with value: 0.0361532107744082 and parameters: {'observation_period_num': 49, 'train_rates': 0.8507964493086366, 'learning_rate': 0.0002782607846859032, 'batch_size': 47, 'step_size': 6, 'gamma': 0.7929603819437206}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:15:49,705][0m Trial 17 finished with value: 0.05042410735981633 and parameters: {'observation_period_num': 6, 'train_rates': 0.8992820945368278, 'learning_rate': 0.00016565749332220168, 'batch_size': 200, 'step_size': 7, 'gamma': 0.7989080532567804}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:16:48,437][0m Trial 18 finished with value: 0.03590255894674037 and parameters: {'observation_period_num': 62, 'train_rates': 0.8266060069370671, 'learning_rate': 0.0004627871584802571, 'batch_size': 94, 'step_size': 11, 'gamma': 0.7983404833155705}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:17:45,723][0m Trial 19 finished with value: 0.04925875101695981 and parameters: {'observation_period_num': 106, 'train_rates': 0.7995893305702875, 'learning_rate': 0.000520199173388375, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8223999289541877}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:22:00,526][0m Trial 20 finished with value: 0.12581237882710575 and parameters: {'observation_period_num': 143, 'train_rates': 0.9052188012446696, 'learning_rate': 4.884875917125077e-06, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8642206622148634}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:23:54,322][0m Trial 21 finished with value: 0.03288288545959136 and parameters: {'observation_period_num': 59, 'train_rates': 0.8415553421239627, 'learning_rate': 0.00045159205895316425, 'batch_size': 47, 'step_size': 7, 'gamma': 0.8015905468398743}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:24:55,252][0m Trial 22 finished with value: 0.04623225497382794 and parameters: {'observation_period_num': 66, 'train_rates': 0.8307476995274333, 'learning_rate': 6.489057110360967e-05, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8174684198170175}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:26:51,619][0m Trial 23 finished with value: 0.039881279568862976 and parameters: {'observation_period_num': 96, 'train_rates': 0.8678263093195113, 'learning_rate': 0.00013424209750596968, 'batch_size': 47, 'step_size': 7, 'gamma': 0.7870015587408017}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:28:10,650][0m Trial 24 finished with value: 0.04809097945690155 and parameters: {'observation_period_num': 34, 'train_rates': 0.9646688590358407, 'learning_rate': 0.00049235705889566, 'batch_size': 78, 'step_size': 9, 'gamma': 0.8356526329401887}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:28:57,247][0m Trial 25 finished with value: 0.03434565809712544 and parameters: {'observation_period_num': 62, 'train_rates': 0.8095514655642738, 'learning_rate': 0.00047379197149139296, 'batch_size': 117, 'step_size': 12, 'gamma': 0.7778254054530904}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:29:33,529][0m Trial 26 finished with value: 0.0408778527268657 and parameters: {'observation_period_num': 11, 'train_rates': 0.8806432229764913, 'learning_rate': 0.0001775262481668238, 'batch_size': 164, 'step_size': 15, 'gamma': 0.7772945962172882}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:30:16,272][0m Trial 27 finished with value: 0.04655990587578798 and parameters: {'observation_period_num': 87, 'train_rates': 0.7719710729621752, 'learning_rate': 0.0005985837377731457, 'batch_size': 127, 'step_size': 3, 'gamma': 0.8166019731805707}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:30:46,494][0m Trial 28 finished with value: 0.09181708842515945 and parameters: {'observation_period_num': 56, 'train_rates': 0.9340818337509975, 'learning_rate': 3.231218658154113e-05, 'batch_size': 225, 'step_size': 6, 'gamma': 0.8553813048292623}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:31:14,247][0m Trial 29 finished with value: 0.12185565056160813 and parameters: {'observation_period_num': 193, 'train_rates': 0.7152590241443699, 'learning_rate': 0.00010141583379631424, 'batch_size': 187, 'step_size': 13, 'gamma': 0.9840608997616334}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:33:32,059][0m Trial 30 finished with value: 0.09252254911769797 and parameters: {'observation_period_num': 223, 'train_rates': 0.7848625281681433, 'learning_rate': 0.00039390159617443707, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8814496418258729}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:34:22,316][0m Trial 31 finished with value: 0.03277437871590199 and parameters: {'observation_period_num': 32, 'train_rates': 0.823990680650768, 'learning_rate': 0.0009887702017065314, 'batch_size': 110, 'step_size': 11, 'gamma': 0.8078666479439507}. Best is trial 11 with value: 0.0316871059037024.[0m
[32m[I 2025-02-03 04:35:14,090][0m Trial 32 finished with value: 0.028197639185801107 and parameters: {'observation_period_num': 5, 'train_rates': 0.8777006631811535, 'learning_rate': 0.0009545753845990329, 'batch_size': 116, 'step_size': 10, 'gamma': 0.7749068311619396}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:35:58,172][0m Trial 33 finished with value: 0.031838941773990305 and parameters: {'observation_period_num': 32, 'train_rates': 0.9299729696083294, 'learning_rate': 0.0009405552240967709, 'batch_size': 142, 'step_size': 10, 'gamma': 0.8053709200670194}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:36:42,216][0m Trial 34 finished with value: 0.03448982932604849 and parameters: {'observation_period_num': 30, 'train_rates': 0.9398919095491437, 'learning_rate': 0.0008932159401097322, 'batch_size': 143, 'step_size': 10, 'gamma': 0.8284621779611757}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:37:21,334][0m Trial 35 finished with value: 0.0363918044629807 and parameters: {'observation_period_num': 6, 'train_rates': 0.8758939860295617, 'learning_rate': 0.0006376500670582299, 'batch_size': 158, 'step_size': 10, 'gamma': 0.7643629993626776}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:38:09,220][0m Trial 36 finished with value: 0.1834387630224228 and parameters: {'observation_period_num': 27, 'train_rates': 0.9897710855868092, 'learning_rate': 3.6224512739823073e-06, 'batch_size': 136, 'step_size': 9, 'gamma': 0.8111337622321091}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:39:01,535][0m Trial 37 finished with value: 0.03840214879282059 and parameters: {'observation_period_num': 37, 'train_rates': 0.9255916375585972, 'learning_rate': 0.00023077710575424945, 'batch_size': 117, 'step_size': 8, 'gamma': 0.8485146245119699}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:39:35,884][0m Trial 38 finished with value: 0.03623141643877786 and parameters: {'observation_period_num': 79, 'train_rates': 0.8915148363053168, 'learning_rate': 0.0009926953356393816, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8750626838698268}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:40:16,716][0m Trial 39 finished with value: 0.03190595714292648 and parameters: {'observation_period_num': 20, 'train_rates': 0.7465034480014101, 'learning_rate': 0.0006990737546012084, 'batch_size': 133, 'step_size': 10, 'gamma': 0.7875345785341143}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:40:43,541][0m Trial 40 finished with value: 0.03716428886469195 and parameters: {'observation_period_num': 17, 'train_rates': 0.7399537941006067, 'learning_rate': 0.0006877035749504899, 'batch_size': 209, 'step_size': 9, 'gamma': 0.7825395805267024}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:41:20,451][0m Trial 41 finished with value: 0.039299395108358136 and parameters: {'observation_period_num': 27, 'train_rates': 0.6833889897218781, 'learning_rate': 0.0006858958884223892, 'batch_size': 134, 'step_size': 10, 'gamma': 0.809309630503572}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:41:53,491][0m Trial 42 finished with value: 0.05641354592566822 and parameters: {'observation_period_num': 41, 'train_rates': 0.614420030189945, 'learning_rate': 0.0009824319935245855, 'batch_size': 148, 'step_size': 11, 'gamma': 0.789058570045767}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:42:39,701][0m Trial 43 finished with value: 0.035298635093394516 and parameters: {'observation_period_num': 16, 'train_rates': 0.6536853163129921, 'learning_rate': 0.0006909821040964432, 'batch_size': 103, 'step_size': 13, 'gamma': 0.8366328459409025}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:43:12,049][0m Trial 44 finished with value: 0.04029301198966363 and parameters: {'observation_period_num': 28, 'train_rates': 0.709388377233346, 'learning_rate': 0.00037332796926444405, 'batch_size': 167, 'step_size': 8, 'gamma': 0.7667526962740449}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:44:02,470][0m Trial 45 finished with value: 0.03316175836759309 and parameters: {'observation_period_num': 7, 'train_rates': 0.9513698272928861, 'learning_rate': 0.00023179467290100118, 'batch_size': 122, 'step_size': 10, 'gamma': 0.9284989522566799}. Best is trial 32 with value: 0.028197639185801107.[0m
Early stopping at epoch 73
[32m[I 2025-02-03 04:44:42,350][0m Trial 46 finished with value: 0.06229146345618932 and parameters: {'observation_period_num': 73, 'train_rates': 0.906837153430342, 'learning_rate': 0.0007351548241621782, 'batch_size': 112, 'step_size': 1, 'gamma': 0.831119126212612}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:45:22,045][0m Trial 47 finished with value: 0.03574799266158018 and parameters: {'observation_period_num': 53, 'train_rates': 0.8645418822557345, 'learning_rate': 0.00035012133444182103, 'batch_size': 155, 'step_size': 13, 'gamma': 0.8089623786580685}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:46:02,328][0m Trial 48 finished with value: 0.08446222532107878 and parameters: {'observation_period_num': 20, 'train_rates': 0.7592315208989368, 'learning_rate': 1.7635661546140916e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8486255493237052}. Best is trial 32 with value: 0.028197639185801107.[0m
[32m[I 2025-02-03 04:46:56,607][0m Trial 49 finished with value: 0.03603146819827648 and parameters: {'observation_period_num': 39, 'train_rates': 0.8215964622290237, 'learning_rate': 0.0007890607458400213, 'batch_size': 102, 'step_size': 12, 'gamma': 0.7577584491945022}. Best is trial 32 with value: 0.028197639185801107.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-03 04:46:56,617][0m A new study created in memory with name: no-name-16613a00-6fa0-4468-b7f3-38e4914e65f9[0m
[32m[I 2025-02-03 04:47:32,154][0m Trial 0 finished with value: 0.5031842979211039 and parameters: {'observation_period_num': 162, 'train_rates': 0.626564435733472, 'learning_rate': 1.003515201835327e-06, 'batch_size': 127, 'step_size': 10, 'gamma': 0.8580860548897019}. Best is trial 0 with value: 0.5031842979211039.[0m
[32m[I 2025-02-03 04:48:04,964][0m Trial 1 finished with value: 0.10551289903209699 and parameters: {'observation_period_num': 146, 'train_rates': 0.7847450634129663, 'learning_rate': 1.9580413445985946e-05, 'batch_size': 164, 'step_size': 12, 'gamma': 0.9438411401323855}. Best is trial 1 with value: 0.10551289903209699.[0m
[32m[I 2025-02-03 04:48:28,525][0m Trial 2 finished with value: 0.31879779443902484 and parameters: {'observation_period_num': 155, 'train_rates': 0.8944711871728565, 'learning_rate': 4.481969419892063e-06, 'batch_size': 255, 'step_size': 9, 'gamma': 0.9831956492055276}. Best is trial 1 with value: 0.10551289903209699.[0m
[32m[I 2025-02-03 04:49:33,622][0m Trial 3 finished with value: 0.13788448288602737 and parameters: {'observation_period_num': 123, 'train_rates': 0.8180239629952462, 'learning_rate': 7.129096893458517e-06, 'batch_size': 80, 'step_size': 14, 'gamma': 0.8894534186368878}. Best is trial 1 with value: 0.10551289903209699.[0m
[32m[I 2025-02-03 04:49:57,990][0m Trial 4 finished with value: 0.12406930516379466 and parameters: {'observation_period_num': 126, 'train_rates': 0.8045615292828262, 'learning_rate': 3.671591818983613e-05, 'batch_size': 242, 'step_size': 7, 'gamma': 0.8708944718351596}. Best is trial 1 with value: 0.10551289903209699.[0m
[32m[I 2025-02-03 04:50:39,299][0m Trial 5 finished with value: 0.050701019505775255 and parameters: {'observation_period_num': 87, 'train_rates': 0.8801226134248537, 'learning_rate': 0.00020044169829215838, 'batch_size': 148, 'step_size': 9, 'gamma': 0.7509275185751842}. Best is trial 5 with value: 0.050701019505775255.[0m
[32m[I 2025-02-03 04:51:11,594][0m Trial 6 finished with value: 0.043442148303623355 and parameters: {'observation_period_num': 93, 'train_rates': 0.8734260084947637, 'learning_rate': 0.0004523714222915184, 'batch_size': 186, 'step_size': 4, 'gamma': 0.8703497651135865}. Best is trial 6 with value: 0.043442148303623355.[0m
[32m[I 2025-02-03 04:51:40,909][0m Trial 7 finished with value: 0.1148213123367732 and parameters: {'observation_period_num': 140, 'train_rates': 0.7944708428871572, 'learning_rate': 3.8102132732540774e-05, 'batch_size': 198, 'step_size': 10, 'gamma': 0.8435151422469116}. Best is trial 6 with value: 0.043442148303623355.[0m
[32m[I 2025-02-03 04:52:52,176][0m Trial 8 finished with value: 0.13155340504001928 and parameters: {'observation_period_num': 197, 'train_rates': 0.7582283982904705, 'learning_rate': 5.221843082587736e-05, 'batch_size': 69, 'step_size': 14, 'gamma': 0.8808288459726183}. Best is trial 6 with value: 0.043442148303623355.[0m
[32m[I 2025-02-03 04:53:24,120][0m Trial 9 finished with value: 0.7488557421066165 and parameters: {'observation_period_num': 131, 'train_rates': 0.7258978580907952, 'learning_rate': 1.417407759228742e-06, 'batch_size': 168, 'step_size': 3, 'gamma': 0.8566654490929165}. Best is trial 6 with value: 0.043442148303623355.[0m
Early stopping at epoch 54
[32m[I 2025-02-03 04:53:41,869][0m Trial 10 finished with value: 0.06012141704559326 and parameters: {'observation_period_num': 11, 'train_rates': 0.9826473289834542, 'learning_rate': 0.0008368438402915102, 'batch_size': 209, 'step_size': 1, 'gamma': 0.7944953034838366}. Best is trial 6 with value: 0.043442148303623355.[0m
[32m[I 2025-02-03 04:54:30,931][0m Trial 11 finished with value: 0.046315757378776724 and parameters: {'observation_period_num': 61, 'train_rates': 0.9030588836178074, 'learning_rate': 0.000410621002403799, 'batch_size': 120, 'step_size': 4, 'gamma': 0.7618201542947268}. Best is trial 6 with value: 0.043442148303623355.[0m
[32m[I 2025-02-03 05:00:09,313][0m Trial 12 finished with value: 0.032771013543277445 and parameters: {'observation_period_num': 52, 'train_rates': 0.9425222765409939, 'learning_rate': 0.0005898146430924318, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8134578816631769}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:04:53,965][0m Trial 13 finished with value: 0.04532994070233634 and parameters: {'observation_period_num': 35, 'train_rates': 0.9823577719301266, 'learning_rate': 0.000144795586167926, 'batch_size': 21, 'step_size': 5, 'gamma': 0.8115533241086738}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:09:47,457][0m Trial 14 finished with value: 0.05212290084961316 and parameters: {'observation_period_num': 248, 'train_rates': 0.9321686777458104, 'learning_rate': 0.000944772898086506, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9068088846634239}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:10:49,289][0m Trial 15 finished with value: 0.07948669103201497 and parameters: {'observation_period_num': 92, 'train_rates': 0.8539130385348194, 'learning_rate': 0.0001769753299501039, 'batch_size': 89, 'step_size': 2, 'gamma': 0.8176558098356742}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:11:20,773][0m Trial 16 finished with value: 0.04210573807358742 and parameters: {'observation_period_num': 61, 'train_rates': 0.9332015739560426, 'learning_rate': 0.00034140819230732287, 'batch_size': 199, 'step_size': 5, 'gamma': 0.9190388377063363}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:11:50,403][0m Trial 17 finished with value: 0.048385873436927795 and parameters: {'observation_period_num': 46, 'train_rates': 0.9469013232248045, 'learning_rate': 0.00010099254422654619, 'batch_size': 223, 'step_size': 7, 'gamma': 0.9256236507686822}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:13:25,042][0m Trial 18 finished with value: 0.05109909240023366 and parameters: {'observation_period_num': 25, 'train_rates': 0.6936980284406856, 'learning_rate': 0.0003800811480864403, 'batch_size': 50, 'step_size': 5, 'gamma': 0.9632158599293498}. Best is trial 12 with value: 0.032771013543277445.[0m
Early stopping at epoch 53
[32m[I 2025-02-03 05:13:59,242][0m Trial 19 finished with value: 0.13806385934019888 and parameters: {'observation_period_num': 64, 'train_rates': 0.9380071405044813, 'learning_rate': 7.53704778455561e-05, 'batch_size': 100, 'step_size': 1, 'gamma': 0.7905556730262241}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:15:43,410][0m Trial 20 finished with value: 0.05807131780496496 and parameters: {'observation_period_num': 9, 'train_rates': 0.9488344356687127, 'learning_rate': 1.8333235123409043e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.8301795479752275}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:16:16,314][0m Trial 21 finished with value: 0.041195574945588995 and parameters: {'observation_period_num': 93, 'train_rates': 0.8454122631745786, 'learning_rate': 0.000349047196887462, 'batch_size': 185, 'step_size': 4, 'gamma': 0.9051544969013047}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:16:48,527][0m Trial 22 finished with value: 0.039219595419472286 and parameters: {'observation_period_num': 66, 'train_rates': 0.839694500529766, 'learning_rate': 0.00031923282604088415, 'batch_size': 177, 'step_size': 3, 'gamma': 0.9218010962852758}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:17:21,540][0m Trial 23 finished with value: 0.05410030647433502 and parameters: {'observation_period_num': 107, 'train_rates': 0.837953577767819, 'learning_rate': 0.0006378394811539897, 'batch_size': 172, 'step_size': 3, 'gamma': 0.9428219429172532}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:18:01,027][0m Trial 24 finished with value: 0.04430315188235707 and parameters: {'observation_period_num': 75, 'train_rates': 0.8372336995243049, 'learning_rate': 0.00023256323736869873, 'batch_size': 143, 'step_size': 3, 'gamma': 0.894347031421325}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:18:26,444][0m Trial 25 finished with value: 0.07310911060072654 and parameters: {'observation_period_num': 38, 'train_rates': 0.763974983978334, 'learning_rate': 9.55784728827347e-05, 'batch_size': 224, 'step_size': 2, 'gamma': 0.914044814566995}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:19:13,458][0m Trial 26 finished with value: 0.059320103135201835 and parameters: {'observation_period_num': 110, 'train_rates': 0.6749489089771615, 'learning_rate': 0.00026632634928990494, 'batch_size': 106, 'step_size': 6, 'gamma': 0.9380184152330091}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:19:50,332][0m Trial 27 finished with value: 0.033250608343493054 and parameters: {'observation_period_num': 51, 'train_rates': 0.8511314489362215, 'learning_rate': 0.0005941731163553119, 'batch_size': 157, 'step_size': 4, 'gamma': 0.9621885948116803}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:20:31,445][0m Trial 28 finished with value: 0.03704600598324429 and parameters: {'observation_period_num': 49, 'train_rates': 0.9053294191226022, 'learning_rate': 0.00062719237401652, 'batch_size': 151, 'step_size': 2, 'gamma': 0.984332915789662}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:21:18,915][0m Trial 29 finished with value: 0.08358541131019592 and parameters: {'observation_period_num': 182, 'train_rates': 0.9096985718680328, 'learning_rate': 0.000540041991585433, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9883278605469469}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:21:50,800][0m Trial 30 finished with value: 0.05335068126721136 and parameters: {'observation_period_num': 45, 'train_rates': 0.6271344070456564, 'learning_rate': 0.0007114641630725269, 'batch_size': 156, 'step_size': 6, 'gamma': 0.9717177636215146}. Best is trial 12 with value: 0.032771013543277445.[0m
[32m[I 2025-02-03 05:22:34,346][0m Trial 31 finished with value: 0.032443401258205995 and parameters: {'observation_period_num': 24, 'train_rates': 0.8689569333975877, 'learning_rate': 0.0006240381715217427, 'batch_size': 135, 'step_size': 1, 'gamma': 0.9599663877259533}. Best is trial 31 with value: 0.032443401258205995.[0m
[32m[I 2025-02-03 05:23:22,661][0m Trial 32 finished with value: 0.032920819865317336 and parameters: {'observation_period_num': 21, 'train_rates': 0.8715498969519059, 'learning_rate': 0.0009927984955808548, 'batch_size': 125, 'step_size': 1, 'gamma': 0.9592464675763577}. Best is trial 31 with value: 0.032443401258205995.[0m
[32m[I 2025-02-03 05:25:43,032][0m Trial 33 finished with value: 0.024707791867194236 and parameters: {'observation_period_num': 24, 'train_rates': 0.8684864397266845, 'learning_rate': 0.0009201477084187438, 'batch_size': 40, 'step_size': 1, 'gamma': 0.963421222565205}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:28:06,306][0m Trial 34 finished with value: 0.025496296357275464 and parameters: {'observation_period_num': 22, 'train_rates': 0.8702152132938498, 'learning_rate': 0.0009413113638090395, 'batch_size': 39, 'step_size': 1, 'gamma': 0.954016183358209}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:30:39,658][0m Trial 35 finished with value: 0.05833823426681407 and parameters: {'observation_period_num': 6, 'train_rates': 0.8846950330646065, 'learning_rate': 1.489917265990184e-05, 'batch_size': 37, 'step_size': 1, 'gamma': 0.9472770256204347}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:32:59,306][0m Trial 36 finished with value: 0.12777620932861342 and parameters: {'observation_period_num': 25, 'train_rates': 0.8200140297659213, 'learning_rate': 2.671945063770591e-06, 'batch_size': 38, 'step_size': 1, 'gamma': 0.9747316055739159}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:36:06,120][0m Trial 37 finished with value: 0.06001484205668 and parameters: {'observation_period_num': 29, 'train_rates': 0.9702540357930847, 'learning_rate': 7.711074273319133e-06, 'batch_size': 32, 'step_size': 13, 'gamma': 0.9330430380389331}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:37:52,598][0m Trial 38 finished with value: 0.038657891815257764 and parameters: {'observation_period_num': 19, 'train_rates': 0.9178212628246375, 'learning_rate': 0.00014980186487446433, 'batch_size': 54, 'step_size': 11, 'gamma': 0.9565994362462942}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:39:09,672][0m Trial 39 finished with value: 0.04399700559043356 and parameters: {'observation_period_num': 76, 'train_rates': 0.8115838239450828, 'learning_rate': 0.0004768137044799091, 'batch_size': 70, 'step_size': 9, 'gamma': 0.9514752295750831}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:40:17,847][0m Trial 40 finished with value: 0.04191425575329792 and parameters: {'observation_period_num': 37, 'train_rates': 0.7784224240999353, 'learning_rate': 0.0008871483716003134, 'batch_size': 78, 'step_size': 2, 'gamma': 0.7734590116099911}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:43:36,433][0m Trial 41 finished with value: 0.029089817634778595 and parameters: {'observation_period_num': 18, 'train_rates': 0.8752120174973221, 'learning_rate': 0.0009168711781476135, 'batch_size': 28, 'step_size': 1, 'gamma': 0.9733437545393174}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:47:01,163][0m Trial 42 finished with value: 0.029259639617065382 and parameters: {'observation_period_num': 5, 'train_rates': 0.8702410157833704, 'learning_rate': 0.0009928313497057895, 'batch_size': 27, 'step_size': 15, 'gamma': 0.9721150587401342}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:49:06,463][0m Trial 43 finished with value: 0.03404363413373458 and parameters: {'observation_period_num': 5, 'train_rates': 0.8820766761930449, 'learning_rate': 0.0009738003888280733, 'batch_size': 45, 'step_size': 12, 'gamma': 0.973937672059369}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:52:14,603][0m Trial 44 finished with value: 0.029307020109543946 and parameters: {'observation_period_num': 19, 'train_rates': 0.8657687311659347, 'learning_rate': 0.00047208792797478756, 'batch_size': 29, 'step_size': 14, 'gamma': 0.9714333403577423}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:53:38,104][0m Trial 45 finished with value: 0.03480349632983024 and parameters: {'observation_period_num': 17, 'train_rates': 0.8230350793216824, 'learning_rate': 0.000253560196976504, 'batch_size': 64, 'step_size': 15, 'gamma': 0.9897254530052838}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:56:53,806][0m Trial 46 finished with value: 0.031124982205991768 and parameters: {'observation_period_num': 32, 'train_rates': 0.8636586095185472, 'learning_rate': 0.00045026439174336016, 'batch_size': 28, 'step_size': 15, 'gamma': 0.971012347234389}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 05:58:57,300][0m Trial 47 finished with value: 0.06299132443714461 and parameters: {'observation_period_num': 154, 'train_rates': 0.8936381620802537, 'learning_rate': 0.000747686078368875, 'batch_size': 44, 'step_size': 14, 'gamma': 0.9777127115763016}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 06:02:16,537][0m Trial 48 finished with value: 0.02566348481602196 and parameters: {'observation_period_num': 15, 'train_rates': 0.8024985491891921, 'learning_rate': 0.0004975132726309717, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9358011257244762}. Best is trial 33 with value: 0.024707791867194236.[0m
[32m[I 2025-02-03 06:05:44,557][0m Trial 49 finished with value: 0.021779211365025152 and parameters: {'observation_period_num': 13, 'train_rates': 0.794260906095709, 'learning_rate': 0.0007573918320032116, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9362791699632287}. Best is trial 49 with value: 0.021779211365025152.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-03 06:05:44,568][0m A new study created in memory with name: no-name-b3a6fa7d-405c-408b-8f80-c3717855946e[0m
[32m[I 2025-02-03 06:06:34,710][0m Trial 0 finished with value: 0.038614825508928516 and parameters: {'observation_period_num': 23, 'train_rates': 0.8500680042836626, 'learning_rate': 0.0007467965995145164, 'batch_size': 114, 'step_size': 5, 'gamma': 0.7583418122943754}. Best is trial 0 with value: 0.038614825508928516.[0m
[32m[I 2025-02-03 06:08:53,706][0m Trial 1 finished with value: 0.10231859110436349 and parameters: {'observation_period_num': 205, 'train_rates': 0.9614008766389341, 'learning_rate': 3.0750138987775176e-05, 'batch_size': 40, 'step_size': 15, 'gamma': 0.7661372541245205}. Best is trial 0 with value: 0.038614825508928516.[0m
[32m[I 2025-02-03 06:09:27,968][0m Trial 2 finished with value: 0.03815606684513288 and parameters: {'observation_period_num': 67, 'train_rates': 0.873701375414734, 'learning_rate': 0.00036000087675895176, 'batch_size': 173, 'step_size': 5, 'gamma': 0.986128389206986}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:10:01,992][0m Trial 3 finished with value: 0.06477222145379831 and parameters: {'observation_period_num': 192, 'train_rates': 0.7566440039816024, 'learning_rate': 0.0008290910595085668, 'batch_size': 151, 'step_size': 12, 'gamma': 0.8983281699642012}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:10:29,325][0m Trial 4 finished with value: 0.07496761401199688 and parameters: {'observation_period_num': 92, 'train_rates': 0.6700434630654981, 'learning_rate': 0.00032792552084564197, 'batch_size': 190, 'step_size': 2, 'gamma': 0.8721488731161774}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:11:37,693][0m Trial 5 finished with value: 0.09608819553485283 and parameters: {'observation_period_num': 114, 'train_rates': 0.6793933774208801, 'learning_rate': 4.9386615798108324e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.8016027786062044}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:12:13,216][0m Trial 6 finished with value: 0.34378907659925884 and parameters: {'observation_period_num': 142, 'train_rates': 0.7732380591940353, 'learning_rate': 8.00859308709976e-06, 'batch_size': 148, 'step_size': 6, 'gamma': 0.7761449277090163}. Best is trial 2 with value: 0.03815606684513288.[0m
Early stopping at epoch 53
[32m[I 2025-02-03 06:12:34,239][0m Trial 7 finished with value: 0.5256991386413574 and parameters: {'observation_period_num': 92, 'train_rates': 0.7733838210995099, 'learning_rate': 2.4516652030874873e-05, 'batch_size': 144, 'step_size': 1, 'gamma': 0.7918030107743865}. Best is trial 2 with value: 0.03815606684513288.[0m
Early stopping at epoch 91
[32m[I 2025-02-03 06:13:29,846][0m Trial 8 finished with value: 0.07622844181926532 and parameters: {'observation_period_num': 15, 'train_rates': 0.8208561947744342, 'learning_rate': 4.152255566664187e-05, 'batch_size': 90, 'step_size': 2, 'gamma': 0.7566606949988068}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:13:52,566][0m Trial 9 finished with value: 0.17366044598899474 and parameters: {'observation_period_num': 171, 'train_rates': 0.7375067440604708, 'learning_rate': 1.771750362480933e-05, 'batch_size': 239, 'step_size': 6, 'gamma': 0.9276927983656567}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:14:17,892][0m Trial 10 finished with value: 0.0880642905831337 and parameters: {'observation_period_num': 246, 'train_rates': 0.9268786710485241, 'learning_rate': 0.00013607914553956632, 'batch_size': 244, 'step_size': 10, 'gamma': 0.9824842477077159}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:14:51,498][0m Trial 11 finished with value: 0.9061791700285834 and parameters: {'observation_period_num': 20, 'train_rates': 0.8739629672147708, 'learning_rate': 1.1136335849292518e-06, 'batch_size': 193, 'step_size': 4, 'gamma': 0.832693273350823}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:15:50,124][0m Trial 12 finished with value: 0.05451626612971056 and parameters: {'observation_period_num': 45, 'train_rates': 0.8595522273688264, 'learning_rate': 0.0009884974690368517, 'batch_size': 97, 'step_size': 4, 'gamma': 0.9851907080573932}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:16:22,728][0m Trial 13 finished with value: 0.03969773300762834 and parameters: {'observation_period_num': 56, 'train_rates': 0.9001058823494514, 'learning_rate': 0.00020921701486020496, 'batch_size': 196, 'step_size': 9, 'gamma': 0.9412733861310755}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:17:17,563][0m Trial 14 finished with value: 0.05009756609797478 and parameters: {'observation_period_num': 56, 'train_rates': 0.9801816549450433, 'learning_rate': 0.0003471721971471965, 'batch_size': 115, 'step_size': 4, 'gamma': 0.8368615313395955}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:17:51,924][0m Trial 15 finished with value: 0.04606049738469578 and parameters: {'observation_period_num': 6, 'train_rates': 0.8288798431862239, 'learning_rate': 9.732135956607402e-05, 'batch_size': 174, 'step_size': 8, 'gamma': 0.8333533923262483}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:18:30,571][0m Trial 16 finished with value: 0.062316685648168085 and parameters: {'observation_period_num': 81, 'train_rates': 0.62043843932786, 'learning_rate': 0.0006443693603821889, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9394573784427386}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:22:01,600][0m Trial 17 finished with value: 0.06562570701353251 and parameters: {'observation_period_num': 39, 'train_rates': 0.9177464532210737, 'learning_rate': 5.270198073733985e-06, 'batch_size': 27, 'step_size': 12, 'gamma': 0.886781234484209}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:23:32,336][0m Trial 18 finished with value: 0.06513942826668259 and parameters: {'observation_period_num': 135, 'train_rates': 0.8256583083544672, 'learning_rate': 0.0003891834356415706, 'batch_size': 57, 'step_size': 3, 'gamma': 0.9103123949353897}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:24:09,072][0m Trial 19 finished with value: 0.05050778012442332 and parameters: {'observation_period_num': 69, 'train_rates': 0.8709643624461181, 'learning_rate': 7.799849845814537e-05, 'batch_size': 164, 'step_size': 7, 'gamma': 0.8619106177990263}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:24:33,832][0m Trial 20 finished with value: 0.06483159406469852 and parameters: {'observation_period_num': 117, 'train_rates': 0.72387444255629, 'learning_rate': 0.00019649257351891437, 'batch_size': 220, 'step_size': 10, 'gamma': 0.9557080946904599}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:25:03,823][0m Trial 21 finished with value: 0.041085634203184215 and parameters: {'observation_period_num': 31, 'train_rates': 0.9068079381530423, 'learning_rate': 0.00018819523295269356, 'batch_size': 208, 'step_size': 9, 'gamma': 0.9600563079301183}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:25:54,653][0m Trial 22 finished with value: 0.07287293522348327 and parameters: {'observation_period_num': 63, 'train_rates': 0.892727234729547, 'learning_rate': 0.0005112287774669216, 'batch_size': 121, 'step_size': 8, 'gamma': 0.9618895943648836}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:26:30,811][0m Trial 23 finished with value: 0.04131582751870155 and parameters: {'observation_period_num': 42, 'train_rates': 0.9450335738241256, 'learning_rate': 0.00024162689374576874, 'batch_size': 177, 'step_size': 6, 'gamma': 0.9279520329217464}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:26:56,818][0m Trial 24 finished with value: 0.04701769618547602 and parameters: {'observation_period_num': 103, 'train_rates': 0.849517425323454, 'learning_rate': 0.0005165518603330609, 'batch_size': 223, 'step_size': 12, 'gamma': 0.984777796113132}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:27:25,181][0m Trial 25 finished with value: 0.044627732985766645 and parameters: {'observation_period_num': 70, 'train_rates': 0.8094043364283738, 'learning_rate': 0.00010491538743440341, 'batch_size': 203, 'step_size': 5, 'gamma': 0.9436620746465099}. Best is trial 2 with value: 0.03815606684513288.[0m
[32m[I 2025-02-03 06:28:01,346][0m Trial 26 finished with value: 0.033848631737473746 and parameters: {'observation_period_num': 27, 'train_rates': 0.8953970443668676, 'learning_rate': 0.0009878871386133573, 'batch_size': 164, 'step_size': 9, 'gamma': 0.9095284124631318}. Best is trial 26 with value: 0.033848631737473746.[0m
[32m[I 2025-02-03 06:28:44,819][0m Trial 27 finished with value: 0.03516457087269663 and parameters: {'observation_period_num': 26, 'train_rates': 0.8455596529191459, 'learning_rate': 0.000966438208324277, 'batch_size': 133, 'step_size': 14, 'gamma': 0.8117893286585737}. Best is trial 26 with value: 0.033848631737473746.[0m
[32m[I 2025-02-03 06:29:26,399][0m Trial 28 finished with value: 0.07142200320959091 and parameters: {'observation_period_num': 32, 'train_rates': 0.9884667032142477, 'learning_rate': 0.0009216970748518239, 'batch_size': 159, 'step_size': 15, 'gamma': 0.8098669952761697}. Best is trial 26 with value: 0.033848631737473746.[0m
[32m[I 2025-02-03 06:30:08,542][0m Trial 29 finished with value: 0.03120562307411382 and parameters: {'observation_period_num': 9, 'train_rates': 0.7979732337427851, 'learning_rate': 0.0005512819928179935, 'batch_size': 135, 'step_size': 14, 'gamma': 0.8512447880339283}. Best is trial 29 with value: 0.03120562307411382.[0m
[32m[I 2025-02-03 06:30:50,629][0m Trial 30 finished with value: 0.024534852266675088 and parameters: {'observation_period_num': 11, 'train_rates': 0.7910318922890581, 'learning_rate': 0.000625564427498638, 'batch_size': 135, 'step_size': 14, 'gamma': 0.8524433026948389}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:31:34,302][0m Trial 31 finished with value: 0.028145036951173097 and parameters: {'observation_period_num': 6, 'train_rates': 0.7937845267224888, 'learning_rate': 0.0006169547618844415, 'batch_size': 133, 'step_size': 14, 'gamma': 0.8609516224075319}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:32:24,643][0m Trial 32 finished with value: 0.031997796999556676 and parameters: {'observation_period_num': 6, 'train_rates': 0.7029297952347273, 'learning_rate': 0.0005543618293989429, 'batch_size': 100, 'step_size': 13, 'gamma': 0.8570501942751888}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:33:19,194][0m Trial 33 finished with value: 0.0271099730504348 and parameters: {'observation_period_num': 5, 'train_rates': 0.7000785065498527, 'learning_rate': 0.0004812982040001308, 'batch_size': 91, 'step_size': 14, 'gamma': 0.8558385686307927}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:34:30,351][0m Trial 34 finished with value: 0.028811599360778928 and parameters: {'observation_period_num': 5, 'train_rates': 0.7883479290376976, 'learning_rate': 0.0002924006781839686, 'batch_size': 75, 'step_size': 14, 'gamma': 0.8479496157016282}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:35:40,949][0m Trial 35 finished with value: 0.026809082036239735 and parameters: {'observation_period_num': 5, 'train_rates': 0.7769939504890216, 'learning_rate': 0.00029861820626689105, 'batch_size': 76, 'step_size': 14, 'gamma': 0.8796251571686426}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:37:32,049][0m Trial 36 finished with value: 0.037038858510430685 and parameters: {'observation_period_num': 47, 'train_rates': 0.7500949744733696, 'learning_rate': 0.00016034178794836643, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8782100371184517}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:38:26,097][0m Trial 37 finished with value: 0.06712026749421092 and parameters: {'observation_period_num': 19, 'train_rates': 0.6487938088453648, 'learning_rate': 0.00035351697765410243, 'batch_size': 90, 'step_size': 15, 'gamma': 0.891457493986048}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:39:29,873][0m Trial 38 finished with value: 0.07178499214351178 and parameters: {'observation_period_num': 157, 'train_rates': 0.7030713143669926, 'learning_rate': 6.206106234740675e-05, 'batch_size': 73, 'step_size': 11, 'gamma': 0.8718385291179761}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:44:24,238][0m Trial 39 finished with value: 0.06995295569976917 and parameters: {'observation_period_num': 211, 'train_rates': 0.7670096855008607, 'learning_rate': 0.00043132750798744, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8239752271003485}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:45:14,174][0m Trial 40 finished with value: 0.030591813567342706 and parameters: {'observation_period_num': 16, 'train_rates': 0.721296351791701, 'learning_rate': 0.0006998871495892115, 'batch_size': 105, 'step_size': 15, 'gamma': 0.8638659481309707}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:46:29,087][0m Trial 41 finished with value: 0.03652653696771247 and parameters: {'observation_period_num': 5, 'train_rates': 0.7874957467321072, 'learning_rate': 0.00027714986550008824, 'batch_size': 72, 'step_size': 14, 'gamma': 0.853464052063391}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:47:35,253][0m Trial 42 finished with value: 0.034422732448896674 and parameters: {'observation_period_num': 22, 'train_rates': 0.789435549226514, 'learning_rate': 0.0002746804522613661, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8397791440743558}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:48:55,413][0m Trial 43 finished with value: 0.041687412077272445 and parameters: {'observation_period_num': 37, 'train_rates': 0.7563295269033034, 'learning_rate': 0.00012972347629804798, 'batch_size': 64, 'step_size': 13, 'gamma': 0.8796536127099197}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:50:36,081][0m Trial 44 finished with value: 0.03410951307008581 and parameters: {'observation_period_num': 18, 'train_rates': 0.671912111623083, 'learning_rate': 0.0006819497131046301, 'batch_size': 46, 'step_size': 11, 'gamma': 0.845322720621564}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:51:40,701][0m Trial 45 finished with value: 0.04135631542658657 and parameters: {'observation_period_num': 50, 'train_rates': 0.778991959473138, 'learning_rate': 0.00028735380205867144, 'batch_size': 82, 'step_size': 15, 'gamma': 0.9001591685253366}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:52:21,920][0m Trial 46 finished with value: 0.27388486203394435 and parameters: {'observation_period_num': 32, 'train_rates': 0.7397588997424324, 'learning_rate': 1.2168051745663303e-06, 'batch_size': 128, 'step_size': 14, 'gamma': 0.8207967721835389}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:53:14,963][0m Trial 47 finished with value: 0.0546565242696807 and parameters: {'observation_period_num': 12, 'train_rates': 0.805103041646541, 'learning_rate': 1.4224466416245015e-05, 'batch_size': 106, 'step_size': 12, 'gamma': 0.8688756767201341}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:53:50,467][0m Trial 48 finished with value: 0.03552023049623744 and parameters: {'observation_period_num': 5, 'train_rates': 0.6975272399026631, 'learning_rate': 0.00040782257297193255, 'batch_size': 148, 'step_size': 11, 'gamma': 0.847384368992255}. Best is trial 30 with value: 0.024534852266675088.[0m
[32m[I 2025-02-03 06:55:06,148][0m Trial 49 finished with value: 0.08721046254300234 and parameters: {'observation_period_num': 78, 'train_rates': 0.603267837158624, 'learning_rate': 3.881077552843509e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.8250371670630664}. Best is trial 30 with value: 0.024534852266675088.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-03 06:55:06,158][0m A new study created in memory with name: no-name-16270096-fde6-4a34-b2f0-a7e61eaf408e[0m
[32m[I 2025-02-03 06:56:19,513][0m Trial 0 finished with value: 0.25295371255453897 and parameters: {'observation_period_num': 191, 'train_rates': 0.827514537517625, 'learning_rate': 3.6795648362174946e-06, 'batch_size': 70, 'step_size': 4, 'gamma': 0.9101117820953912}. Best is trial 0 with value: 0.25295371255453897.[0m
[32m[I 2025-02-03 06:57:09,758][0m Trial 1 finished with value: 0.044969992858855286 and parameters: {'observation_period_num': 101, 'train_rates': 0.6928597942947524, 'learning_rate': 0.000688082451430361, 'batch_size': 97, 'step_size': 13, 'gamma': 0.7736956938986408}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 06:58:32,207][0m Trial 2 finished with value: 0.08098639568636537 and parameters: {'observation_period_num': 151, 'train_rates': 0.8722552943268601, 'learning_rate': 2.0603371909074493e-05, 'batch_size': 65, 'step_size': 12, 'gamma': 0.9442293671337487}. Best is trial 1 with value: 0.044969992858855286.[0m
Early stopping at epoch 54
[32m[I 2025-02-03 06:58:48,509][0m Trial 3 finished with value: 0.11315589500653664 and parameters: {'observation_period_num': 91, 'train_rates': 0.7916278302403569, 'learning_rate': 0.000704247346039314, 'batch_size': 194, 'step_size': 1, 'gamma': 0.7714179299196412}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 06:59:15,637][0m Trial 4 finished with value: 1.1789736966697537 and parameters: {'observation_period_num': 251, 'train_rates': 0.7277107147374647, 'learning_rate': 1.1057672547633642e-06, 'batch_size': 180, 'step_size': 4, 'gamma': 0.9112121788429806}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 06:59:36,377][0m Trial 5 finished with value: 0.4777664982754251 and parameters: {'observation_period_num': 160, 'train_rates': 0.6953066076820581, 'learning_rate': 7.975454739220503e-06, 'batch_size': 251, 'step_size': 2, 'gamma': 0.9421056208790541}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:03:14,966][0m Trial 6 finished with value: 0.06367659894077433 and parameters: {'observation_period_num': 13, 'train_rates': 0.6698895844815418, 'learning_rate': 2.6702552179094196e-06, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8887875559487589}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:03:43,746][0m Trial 7 finished with value: 0.05856002724992678 and parameters: {'observation_period_num': 30, 'train_rates': 0.7479461379748016, 'learning_rate': 3.074854778594114e-05, 'batch_size': 191, 'step_size': 5, 'gamma': 0.9635767298514756}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:04:11,635][0m Trial 8 finished with value: 0.3561836884345537 and parameters: {'observation_period_num': 204, 'train_rates': 0.8526664771208381, 'learning_rate': 5.990627852264036e-06, 'batch_size': 211, 'step_size': 14, 'gamma': 0.7890185824405627}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:06:58,710][0m Trial 9 finished with value: 0.06369126987630043 and parameters: {'observation_period_num': 154, 'train_rates': 0.9260624358127452, 'learning_rate': 1.976835927734802e-05, 'batch_size': 33, 'step_size': 9, 'gamma': 0.9207491374896725}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:07:37,360][0m Trial 10 finished with value: 0.07324001256330835 and parameters: {'observation_period_num': 85, 'train_rates': 0.6014841549632102, 'learning_rate': 0.0007831583841612378, 'batch_size': 119, 'step_size': 15, 'gamma': 0.8338548991093766}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:08:20,042][0m Trial 11 finished with value: 0.05993902873899545 and parameters: {'observation_period_num': 10, 'train_rates': 0.7396401891416227, 'learning_rate': 0.00013477454241784268, 'batch_size': 124, 'step_size': 7, 'gamma': 0.9895621596096116}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:08:53,044][0m Trial 12 finished with value: 0.0590642184712166 and parameters: {'observation_period_num': 63, 'train_rates': 0.6277180283626543, 'learning_rate': 0.0001634411343426901, 'batch_size': 150, 'step_size': 6, 'gamma': 0.8490267862651173}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:09:48,112][0m Trial 13 finished with value: 0.04620993457603873 and parameters: {'observation_period_num': 72, 'train_rates': 0.7626117488343277, 'learning_rate': 8.712305694155763e-05, 'batch_size': 93, 'step_size': 9, 'gamma': 0.8223259785786248}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:10:44,959][0m Trial 14 finished with value: 0.04734982224032521 and parameters: {'observation_period_num': 110, 'train_rates': 0.7797091853501286, 'learning_rate': 0.00019271928971889205, 'batch_size': 90, 'step_size': 10, 'gamma': 0.7998331742390232}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:11:36,171][0m Trial 15 finished with value: 0.056737225956210426 and parameters: {'observation_period_num': 53, 'train_rates': 0.6570011239209697, 'learning_rate': 6.820253139386067e-05, 'batch_size': 95, 'step_size': 13, 'gamma': 0.7511754601650446}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:12:16,311][0m Trial 16 finished with value: 0.05258343150874354 and parameters: {'observation_period_num': 120, 'train_rates': 0.9440706727305311, 'learning_rate': 0.00030384834992747844, 'batch_size': 157, 'step_size': 9, 'gamma': 0.8207529774015329}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:13:53,251][0m Trial 17 finished with value: 0.05088422869917505 and parameters: {'observation_period_num': 60, 'train_rates': 0.7189943249922275, 'learning_rate': 6.136224778524459e-05, 'batch_size': 50, 'step_size': 12, 'gamma': 0.8613113442787133}. Best is trial 1 with value: 0.044969992858855286.[0m
[32m[I 2025-02-03 07:14:46,877][0m Trial 18 finished with value: 0.03910170881647498 and parameters: {'observation_period_num': 103, 'train_rates': 0.7692790855459665, 'learning_rate': 0.0004117897220884325, 'batch_size': 96, 'step_size': 8, 'gamma': 0.81044063840531}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:15:43,656][0m Trial 19 finished with value: 0.07411528378725052 and parameters: {'observation_period_num': 102, 'train_rates': 0.9874295008673422, 'learning_rate': 0.00039530035289990186, 'batch_size': 112, 'step_size': 8, 'gamma': 0.792439516160333}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:16:23,611][0m Trial 20 finished with value: 0.04245831784792244 and parameters: {'observation_period_num': 134, 'train_rates': 0.8154786832348029, 'learning_rate': 0.0009523601013386896, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7507928749257203}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:17:00,340][0m Trial 21 finished with value: 0.04184257245235482 and parameters: {'observation_period_num': 139, 'train_rates': 0.8271671517776362, 'learning_rate': 0.00042799102195345497, 'batch_size': 148, 'step_size': 15, 'gamma': 0.7519112203917163}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:17:37,149][0m Trial 22 finished with value: 0.04689008223609866 and parameters: {'observation_period_num': 140, 'train_rates': 0.825578302211027, 'learning_rate': 0.00035371296030986337, 'batch_size': 148, 'step_size': 15, 'gamma': 0.7501915158572323}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:18:12,330][0m Trial 23 finished with value: 0.057383536425281746 and parameters: {'observation_period_num': 177, 'train_rates': 0.8903206585252499, 'learning_rate': 0.0009821037053318869, 'batch_size': 161, 'step_size': 15, 'gamma': 0.7699035629888622}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:18:53,421][0m Trial 24 finished with value: 0.0437014624765215 and parameters: {'observation_period_num': 132, 'train_rates': 0.8172248864885335, 'learning_rate': 0.0004474235239092161, 'batch_size': 134, 'step_size': 11, 'gamma': 0.8085120305733733}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:19:18,660][0m Trial 25 finished with value: 0.06802926188028312 and parameters: {'observation_period_num': 217, 'train_rates': 0.8556569088902353, 'learning_rate': 0.00023555960710117853, 'batch_size': 218, 'step_size': 13, 'gamma': 0.7686682740769653}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:19:52,518][0m Trial 26 finished with value: 0.0500030649668524 and parameters: {'observation_period_num': 173, 'train_rates': 0.8979974717048205, 'learning_rate': 0.0005451245552956122, 'batch_size': 171, 'step_size': 7, 'gamma': 0.7874622743083765}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:20:31,962][0m Trial 27 finished with value: 0.05361246176891857 and parameters: {'observation_period_num': 127, 'train_rates': 0.8090248919428704, 'learning_rate': 0.00011386518697887632, 'batch_size': 138, 'step_size': 14, 'gamma': 0.8343470588099149}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:21:20,088][0m Trial 28 finished with value: 0.045500928226160295 and parameters: {'observation_period_num': 122, 'train_rates': 0.7984191001014508, 'learning_rate': 0.0002490659437291236, 'batch_size': 109, 'step_size': 11, 'gamma': 0.7524383771741929}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:22:25,289][0m Trial 29 finished with value: 0.05651289600509781 and parameters: {'observation_period_num': 188, 'train_rates': 0.8393390235201802, 'learning_rate': 0.0009961724907620673, 'batch_size': 80, 'step_size': 3, 'gamma': 0.8833936205672365}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:23:38,431][0m Trial 30 finished with value: 0.05882621777020511 and parameters: {'observation_period_num': 145, 'train_rates': 0.7712110638472369, 'learning_rate': 0.0004847432996063853, 'batch_size': 68, 'step_size': 7, 'gamma': 0.8090340703113251}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:24:19,689][0m Trial 31 finished with value: 0.04574907756578483 and parameters: {'observation_period_num': 136, 'train_rates': 0.8267463283260928, 'learning_rate': 0.0005284417651161094, 'batch_size': 135, 'step_size': 11, 'gamma': 0.8031153210308488}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:25:01,600][0m Trial 32 finished with value: 0.04800789040036318 and parameters: {'observation_period_num': 117, 'train_rates': 0.812098678552627, 'learning_rate': 0.0003237943559728159, 'batch_size': 133, 'step_size': 14, 'gamma': 0.7756786594999989}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:25:52,600][0m Trial 33 finished with value: 0.04889207665282901 and parameters: {'observation_period_num': 165, 'train_rates': 0.8549584895358411, 'learning_rate': 0.0006137222307199344, 'batch_size': 108, 'step_size': 12, 'gamma': 0.8152839877732563}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:26:26,506][0m Trial 34 finished with value: 0.046190939727962385 and parameters: {'observation_period_num': 89, 'train_rates': 0.8875590412566732, 'learning_rate': 0.0004349814254677587, 'batch_size': 170, 'step_size': 10, 'gamma': 0.8486312759294776}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:27:08,185][0m Trial 35 finished with value: 0.05104618470203008 and parameters: {'observation_period_num': 135, 'train_rates': 0.7855849816313734, 'learning_rate': 0.00021474378013755716, 'batch_size': 128, 'step_size': 13, 'gamma': 0.7816052665276108}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:27:44,569][0m Trial 36 finished with value: 0.05217102874150424 and parameters: {'observation_period_num': 107, 'train_rates': 0.7055268095843432, 'learning_rate': 0.000723377920586435, 'batch_size': 141, 'step_size': 14, 'gamma': 0.7623286997146355}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:28:13,394][0m Trial 37 finished with value: 0.08183856657143168 and parameters: {'observation_period_num': 80, 'train_rates': 0.7552996337898983, 'learning_rate': 4.497749916782216e-05, 'batch_size': 188, 'step_size': 12, 'gamma': 0.8742875846830132}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:29:52,612][0m Trial 38 finished with value: 0.053823535922317874 and parameters: {'observation_period_num': 154, 'train_rates': 0.870827436525398, 'learning_rate': 0.00011039488705784637, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8333164884300138}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:30:58,286][0m Trial 39 finished with value: 0.10499285152231609 and parameters: {'observation_period_num': 96, 'train_rates': 0.802840670633871, 'learning_rate': 1.6538949006126192e-05, 'batch_size': 79, 'step_size': 10, 'gamma': 0.7601892193531667}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:31:26,311][0m Trial 40 finished with value: 0.97538554706964 and parameters: {'observation_period_num': 131, 'train_rates': 0.8356413221270313, 'learning_rate': 1.0682431641584448e-06, 'batch_size': 201, 'step_size': 6, 'gamma': 0.780024844339581}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:32:10,025][0m Trial 41 finished with value: 0.05726056620006077 and parameters: {'observation_period_num': 110, 'train_rates': 0.6671785389159743, 'learning_rate': 0.0006924208835234957, 'batch_size': 109, 'step_size': 15, 'gamma': 0.7978273814680877}. Best is trial 18 with value: 0.03910170881647498.[0m
[32m[I 2025-02-03 07:32:59,054][0m Trial 42 finished with value: 0.03591702617051309 and parameters: {'observation_period_num': 46, 'train_rates': 0.6862856262591838, 'learning_rate': 0.0009255561644684865, 'batch_size': 100, 'step_size': 15, 'gamma': 0.7648024334082506}. Best is trial 42 with value: 0.03591702617051309.[0m
[32m[I 2025-02-03 07:33:42,792][0m Trial 43 finished with value: 0.0361271095154979 and parameters: {'observation_period_num': 37, 'train_rates': 0.7399547871927311, 'learning_rate': 0.0009983286017150289, 'batch_size': 121, 'step_size': 14, 'gamma': 0.7689614381922647}. Best is trial 42 with value: 0.03591702617051309.[0m
[32m[I 2025-02-03 07:34:26,223][0m Trial 44 finished with value: 0.03149717065497336 and parameters: {'observation_period_num': 29, 'train_rates': 0.7327001819674946, 'learning_rate': 0.0009020099926459305, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7617049561833054}. Best is trial 44 with value: 0.03149717065497336.[0m
[32m[I 2025-02-03 07:35:09,983][0m Trial 45 finished with value: 0.03359640874948941 and parameters: {'observation_period_num': 27, 'train_rates': 0.7270888216769358, 'learning_rate': 0.0007095127612473652, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7690712880563284}. Best is trial 44 with value: 0.03149717065497336.[0m
[32m[I 2025-02-03 07:36:13,408][0m Trial 46 finished with value: 0.038157513957279496 and parameters: {'observation_period_num': 33, 'train_rates': 0.7263659769411059, 'learning_rate': 0.0007188835476685626, 'batch_size': 81, 'step_size': 14, 'gamma': 0.7657568378150027}. Best is trial 44 with value: 0.03149717065497336.[0m
[32m[I 2025-02-03 07:37:15,146][0m Trial 47 finished with value: 0.03345213634294443 and parameters: {'observation_period_num': 36, 'train_rates': 0.6904797526266606, 'learning_rate': 0.0007697207062611633, 'batch_size': 78, 'step_size': 14, 'gamma': 0.7699175935186049}. Best is trial 44 with value: 0.03149717065497336.[0m
[32m[I 2025-02-03 07:37:57,697][0m Trial 48 finished with value: 0.304411169141531 and parameters: {'observation_period_num': 25, 'train_rates': 0.6800527727015491, 'learning_rate': 1.856839463308944e-06, 'batch_size': 121, 'step_size': 13, 'gamma': 0.7844163815289217}. Best is trial 44 with value: 0.03149717065497336.[0m
[32m[I 2025-02-03 07:39:09,966][0m Trial 49 finished with value: 0.06524428399777506 and parameters: {'observation_period_num': 42, 'train_rates': 0.6464534674939307, 'learning_rate': 0.0007839811899540913, 'batch_size': 59, 'step_size': 14, 'gamma': 0.7743349027768962}. Best is trial 44 with value: 0.03149717065497336.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-03 07:39:09,974][0m A new study created in memory with name: no-name-4b531d70-7240-43cc-9c6c-a130ed4b67ed[0m
[32m[I 2025-02-03 07:40:38,815][0m Trial 0 finished with value: 0.3870872146508191 and parameters: {'observation_period_num': 225, 'train_rates': 0.6243975710567264, 'learning_rate': 5.830407418991953e-06, 'batch_size': 45, 'step_size': 5, 'gamma': 0.7969805797801067}. Best is trial 0 with value: 0.3870872146508191.[0m
[32m[I 2025-02-03 07:41:09,374][0m Trial 1 finished with value: 0.4511523034439578 and parameters: {'observation_period_num': 82, 'train_rates': 0.9190577073421127, 'learning_rate': 5.050038597701164e-06, 'batch_size': 215, 'step_size': 3, 'gamma': 0.846045178150595}. Best is trial 0 with value: 0.3870872146508191.[0m
[32m[I 2025-02-03 07:42:16,677][0m Trial 2 finished with value: 0.07338665981968481 and parameters: {'observation_period_num': 201, 'train_rates': 0.8178819223398411, 'learning_rate': 7.548312139960923e-05, 'batch_size': 76, 'step_size': 11, 'gamma': 0.9077863716280462}. Best is trial 2 with value: 0.07338665981968481.[0m
[32m[I 2025-02-03 07:43:48,780][0m Trial 3 finished with value: 0.23420291184555 and parameters: {'observation_period_num': 174, 'train_rates': 0.7656617427902512, 'learning_rate': 4.322529201738669e-06, 'batch_size': 54, 'step_size': 9, 'gamma': 0.7633177568827725}. Best is trial 2 with value: 0.07338665981968481.[0m
[32m[I 2025-02-03 07:44:43,105][0m Trial 4 finished with value: 0.06375613808631897 and parameters: {'observation_period_num': 65, 'train_rates': 0.9795570072799469, 'learning_rate': 5.639344126967922e-05, 'batch_size': 121, 'step_size': 10, 'gamma': 0.9532799949042459}. Best is trial 4 with value: 0.06375613808631897.[0m
[32m[I 2025-02-03 07:45:31,227][0m Trial 5 finished with value: 0.6911259663375942 and parameters: {'observation_period_num': 77, 'train_rates': 0.770510217720799, 'learning_rate': 1.4757260841703325e-06, 'batch_size': 113, 'step_size': 15, 'gamma': 0.7898624674165906}. Best is trial 4 with value: 0.06375613808631897.[0m
[32m[I 2025-02-03 07:45:57,759][0m Trial 6 finished with value: 0.20840671219057955 and parameters: {'observation_period_num': 237, 'train_rates': 0.7827917320358548, 'learning_rate': 1.2692787107393592e-05, 'batch_size': 209, 'step_size': 2, 'gamma': 0.9745447063565165}. Best is trial 4 with value: 0.06375613808631897.[0m
[32m[I 2025-02-03 07:46:36,197][0m Trial 7 finished with value: 0.40369211014841255 and parameters: {'observation_period_num': 219, 'train_rates': 0.6571247655171627, 'learning_rate': 6.364897103051934e-06, 'batch_size': 122, 'step_size': 9, 'gamma': 0.7995745933348156}. Best is trial 4 with value: 0.06375613808631897.[0m
[32m[I 2025-02-03 07:47:00,252][0m Trial 8 finished with value: 0.7459085957113519 and parameters: {'observation_period_num': 45, 'train_rates': 0.8571694838596323, 'learning_rate': 1.4601530215160935e-06, 'batch_size': 251, 'step_size': 3, 'gamma': 0.8464689057430571}. Best is trial 4 with value: 0.06375613808631897.[0m
[32m[I 2025-02-03 07:48:28,052][0m Trial 9 finished with value: 0.04990395759442943 and parameters: {'observation_period_num': 152, 'train_rates': 0.8579597388639335, 'learning_rate': 0.00030882690800787776, 'batch_size': 59, 'step_size': 13, 'gamma': 0.7509902612829008}. Best is trial 9 with value: 0.04990395759442943.[0m
[32m[I 2025-02-03 07:53:35,660][0m Trial 10 finished with value: 0.055342225559516664 and parameters: {'observation_period_num': 134, 'train_rates': 0.8967946777771418, 'learning_rate': 0.0006653466359322187, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8993674439624433}. Best is trial 9 with value: 0.04990395759442943.[0m
[32m[I 2025-02-03 07:57:48,451][0m Trial 11 finished with value: 0.050260711240155444 and parameters: {'observation_period_num': 140, 'train_rates': 0.8937332022234511, 'learning_rate': 0.0008642848130425126, 'batch_size': 21, 'step_size': 15, 'gamma': 0.8971314723130224}. Best is trial 9 with value: 0.04990395759442943.[0m
[32m[I 2025-02-03 08:03:49,154][0m Trial 12 finished with value: 0.05879010011752447 and parameters: {'observation_period_num': 138, 'train_rates': 0.9873220747865847, 'learning_rate': 0.0009352540675807245, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8936109592568164}. Best is trial 9 with value: 0.04990395759442943.[0m
[32m[I 2025-02-03 08:05:01,097][0m Trial 13 finished with value: 0.03566401775437172 and parameters: {'observation_period_num': 9, 'train_rates': 0.8992667191465713, 'learning_rate': 0.0002485891099230432, 'batch_size': 82, 'step_size': 12, 'gamma': 0.936307313819787}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:06:02,645][0m Trial 14 finished with value: 0.03916692027976642 and parameters: {'observation_period_num': 15, 'train_rates': 0.704370291602346, 'learning_rate': 0.0002153689295175542, 'batch_size': 81, 'step_size': 12, 'gamma': 0.9397588279462323}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:06:34,831][0m Trial 15 finished with value: 0.03833464302710796 and parameters: {'observation_period_num': 7, 'train_rates': 0.6904475214118747, 'learning_rate': 0.00017200669474023953, 'batch_size': 164, 'step_size': 7, 'gamma': 0.9381640946790574}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:07:07,138][0m Trial 16 finished with value: 0.045442838275006836 and parameters: {'observation_period_num': 12, 'train_rates': 0.7141983747320113, 'learning_rate': 0.0001690579718117675, 'batch_size': 164, 'step_size': 6, 'gamma': 0.9884179876529563}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:07:39,927][0m Trial 17 finished with value: 0.09312231192865902 and parameters: {'observation_period_num': 102, 'train_rates': 0.7103217583835117, 'learning_rate': 3.614661801150516e-05, 'batch_size': 164, 'step_size': 7, 'gamma': 0.9420651308213442}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:08:20,234][0m Trial 18 finished with value: 0.04339137587879525 and parameters: {'observation_period_num': 37, 'train_rates': 0.9372629232637932, 'learning_rate': 0.00012399559348938666, 'batch_size': 158, 'step_size': 8, 'gamma': 0.9328332423075615}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:09:05,449][0m Trial 19 finished with value: 0.08276033866984533 and parameters: {'observation_period_num': 106, 'train_rates': 0.6056498166572145, 'learning_rate': 0.00038058823450785287, 'batch_size': 98, 'step_size': 4, 'gamma': 0.8623083531054034}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:09:42,033][0m Trial 20 finished with value: 0.061115307323412 and parameters: {'observation_period_num': 6, 'train_rates': 0.6590907581834613, 'learning_rate': 2.3142234765020066e-05, 'batch_size': 143, 'step_size': 6, 'gamma': 0.9650680586779506}. Best is trial 13 with value: 0.03566401775437172.[0m
[32m[I 2025-02-03 08:10:39,065][0m Trial 21 finished with value: 0.03522152663434593 and parameters: {'observation_period_num': 33, 'train_rates': 0.7177765114273134, 'learning_rate': 0.00020934084212874228, 'batch_size': 88, 'step_size': 12, 'gamma': 0.9276999759408661}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:11:07,556][0m Trial 22 finished with value: 0.04918205811073316 and parameters: {'observation_period_num': 39, 'train_rates': 0.7443277351440544, 'learning_rate': 8.04990841959803e-05, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9241256273862521}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:12:07,638][0m Trial 23 finished with value: 0.04456278048597361 and parameters: {'observation_period_num': 32, 'train_rates': 0.8198353715025258, 'learning_rate': 0.00042648393111555783, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9159695010542119}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:12:36,238][0m Trial 24 finished with value: 0.060392499784467674 and parameters: {'observation_period_num': 64, 'train_rates': 0.6578357929794827, 'learning_rate': 0.00012100105423347679, 'batch_size': 185, 'step_size': 8, 'gamma': 0.8763900815425}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:13:14,866][0m Trial 25 finished with value: 0.05820099071449483 and parameters: {'observation_period_num': 19, 'train_rates': 0.7333683817703939, 'learning_rate': 0.00023483555021205037, 'batch_size': 143, 'step_size': 10, 'gamma': 0.9572703418846662}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:14:18,926][0m Trial 26 finished with value: 0.04838956299066806 and parameters: {'observation_period_num': 59, 'train_rates': 0.6857617644673487, 'learning_rate': 0.0005051419450310984, 'batch_size': 76, 'step_size': 12, 'gamma': 0.9861159877750088}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:15:13,675][0m Trial 27 finished with value: 0.044724005569111214 and parameters: {'observation_period_num': 27, 'train_rates': 0.8121070285385369, 'learning_rate': 3.932828830419113e-05, 'batch_size': 102, 'step_size': 14, 'gamma': 0.8781322594491817}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:15:50,823][0m Trial 28 finished with value: 0.059015948992368956 and parameters: {'observation_period_num': 101, 'train_rates': 0.6787747190577831, 'learning_rate': 0.0001392623456608529, 'batch_size': 133, 'step_size': 7, 'gamma': 0.9245463398687744}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:17:40,661][0m Trial 29 finished with value: 0.05347444004468662 and parameters: {'observation_period_num': 5, 'train_rates': 0.6395769285266836, 'learning_rate': 1.8390297276436386e-05, 'batch_size': 41, 'step_size': 5, 'gamma': 0.9489574574907158}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:19:07,589][0m Trial 30 finished with value: 0.046811906214169016 and parameters: {'observation_period_num': 49, 'train_rates': 0.8424184710857789, 'learning_rate': 0.00028228007890400674, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9675519008355826}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:20:06,461][0m Trial 31 finished with value: 0.06750899156373087 and parameters: {'observation_period_num': 22, 'train_rates': 0.7003345368371193, 'learning_rate': 0.00022312350465937566, 'batch_size': 83, 'step_size': 12, 'gamma': 0.937818501695564}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:22:08,165][0m Trial 32 finished with value: 0.04818599141653507 and parameters: {'observation_period_num': 23, 'train_rates': 0.7354277402686193, 'learning_rate': 7.34841162654838e-05, 'batch_size': 41, 'step_size': 12, 'gamma': 0.9125572966902904}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:23:10,289][0m Trial 33 finished with value: 0.07041467582712525 and parameters: {'observation_period_num': 86, 'train_rates': 0.6214117027430385, 'learning_rate': 0.00017357853994867664, 'batch_size': 73, 'step_size': 11, 'gamma': 0.9370220336324743}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:24:07,301][0m Trial 34 finished with value: 0.04420238205675404 and parameters: {'observation_period_num': 52, 'train_rates': 0.7589648802114841, 'learning_rate': 0.0005795834052614042, 'batch_size': 89, 'step_size': 9, 'gamma': 0.9200875312169782}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:24:53,351][0m Trial 35 finished with value: 0.038626488597638356 and parameters: {'observation_period_num': 15, 'train_rates': 0.6823341179020003, 'learning_rate': 9.53880265052932e-05, 'batch_size': 108, 'step_size': 14, 'gamma': 0.8873090303531544}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:25:33,288][0m Trial 36 finished with value: 0.09922576295944412 and parameters: {'observation_period_num': 172, 'train_rates': 0.6798196906518629, 'learning_rate': 0.00010595550216540825, 'batch_size': 125, 'step_size': 14, 'gamma': 0.8859746881428735}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:26:30,693][0m Trial 37 finished with value: 0.05016128031851409 and parameters: {'observation_period_num': 76, 'train_rates': 0.9466161792306967, 'learning_rate': 6.686108997535136e-05, 'batch_size': 108, 'step_size': 14, 'gamma': 0.8536275754435251}. Best is trial 21 with value: 0.03522152663434593.[0m
Early stopping at epoch 68
[32m[I 2025-02-03 08:26:55,022][0m Trial 38 finished with value: 0.21822998891124373 and parameters: {'observation_period_num': 34, 'train_rates': 0.7955850136884953, 'learning_rate': 4.724739151513871e-05, 'batch_size': 178, 'step_size': 1, 'gamma': 0.8320551637263907}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:27:38,105][0m Trial 39 finished with value: 0.05131826000778299 and parameters: {'observation_period_num': 6, 'train_rates': 0.6323744696246141, 'learning_rate': 9.259658400935884e-05, 'batch_size': 113, 'step_size': 10, 'gamma': 0.8262370375825073}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:28:13,870][0m Trial 40 finished with value: 0.11605011064387141 and parameters: {'observation_period_num': 50, 'train_rates': 0.7239743227454398, 'learning_rate': 0.0003486219396127949, 'batch_size': 140, 'step_size': 11, 'gamma': 0.9527892996346419}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:29:25,374][0m Trial 41 finished with value: 0.07607041919374302 and parameters: {'observation_period_num': 16, 'train_rates': 0.7016464986367105, 'learning_rate': 0.0001780040626685668, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9126956089908493}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:30:27,936][0m Trial 42 finished with value: 0.03584703046279574 and parameters: {'observation_period_num': 28, 'train_rates': 0.7608379635912645, 'learning_rate': 0.00022706380949020922, 'batch_size': 83, 'step_size': 14, 'gamma': 0.9044022642122325}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:32:05,482][0m Trial 43 finished with value: 0.04093158695550643 and parameters: {'observation_period_num': 29, 'train_rates': 0.7548082944954354, 'learning_rate': 0.00028345267857641046, 'batch_size': 52, 'step_size': 14, 'gamma': 0.9038509620670961}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:32:52,405][0m Trial 44 finished with value: 0.2456407361854275 and parameters: {'observation_period_num': 70, 'train_rates': 0.7811241156742289, 'learning_rate': 2.6714859322263577e-06, 'batch_size': 113, 'step_size': 15, 'gamma': 0.8867029432559735}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:33:32,421][0m Trial 45 finished with value: 0.037365687999528704 and parameters: {'observation_period_num': 41, 'train_rates': 0.666648010107012, 'learning_rate': 0.0006706257010092132, 'batch_size': 128, 'step_size': 13, 'gamma': 0.8679523862705951}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:34:11,172][0m Trial 46 finished with value: 0.04663169941138089 and parameters: {'observation_period_num': 87, 'train_rates': 0.8825069393264593, 'learning_rate': 0.0006694606280433003, 'batch_size': 154, 'step_size': 13, 'gamma': 0.929279605755689}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:34:55,145][0m Trial 47 finished with value: 0.04560631656069432 and parameters: {'observation_period_num': 43, 'train_rates': 0.7923637730587885, 'learning_rate': 0.0009560302260471188, 'batch_size': 127, 'step_size': 9, 'gamma': 0.8692585809752965}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:35:19,290][0m Trial 48 finished with value: 0.10036412422464101 and parameters: {'observation_period_num': 199, 'train_rates': 0.6497612310537252, 'learning_rate': 0.0004832652739615115, 'batch_size': 223, 'step_size': 11, 'gamma': 0.9029502648577478}. Best is trial 21 with value: 0.03522152663434593.[0m
[32m[I 2025-02-03 08:36:17,864][0m Trial 49 finished with value: 0.0353968983503426 and parameters: {'observation_period_num': 58, 'train_rates': 0.746013111286525, 'learning_rate': 0.000715252170037329, 'batch_size': 88, 'step_size': 7, 'gamma': 0.8307131609260897}. Best is trial 21 with value: 0.03522152663434593.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_BA_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 7, 'train_rates': 0.8199982567082441, 'learning_rate': 0.0004885407036096562, 'batch_size': 42, 'step_size': 6, 'gamma': 0.8961747704014176}
Epoch 1/300, trend Loss: 0.2107 | 0.0853
Epoch 2/300, trend Loss: 0.1053 | 0.0780
Epoch 3/300, trend Loss: 0.0828 | 0.0654
Epoch 4/300, trend Loss: 0.0712 | 0.0610
Epoch 5/300, trend Loss: 0.0650 | 0.0567
Epoch 6/300, trend Loss: 0.0583 | 0.0565
Epoch 7/300, trend Loss: 0.0545 | 0.0533
Epoch 8/300, trend Loss: 0.0531 | 0.0549
Epoch 9/300, trend Loss: 0.0519 | 0.0513
Epoch 10/300, trend Loss: 0.0610 | 0.0528
Epoch 11/300, trend Loss: 0.0466 | 0.0440
Epoch 12/300, trend Loss: 0.0569 | 0.0458
Epoch 13/300, trend Loss: 0.0438 | 0.0428
Epoch 14/300, trend Loss: 0.0523 | 0.0445
Epoch 15/300, trend Loss: 0.0440 | 0.0423
Epoch 16/300, trend Loss: 0.0535 | 0.0393
Epoch 17/300, trend Loss: 0.0460 | 0.0398
Epoch 18/300, trend Loss: 0.0419 | 0.0372
Epoch 19/300, trend Loss: 0.0359 | 0.0351
Epoch 20/300, trend Loss: 0.0345 | 0.0341
Epoch 21/300, trend Loss: 0.0330 | 0.0333
Epoch 22/300, trend Loss: 0.0321 | 0.0327
Epoch 23/300, trend Loss: 0.0315 | 0.0321
Epoch 24/300, trend Loss: 0.0311 | 0.0317
Epoch 25/300, trend Loss: 0.0302 | 0.0316
Epoch 26/300, trend Loss: 0.0313 | 0.0331
Epoch 27/300, trend Loss: 0.0309 | 0.0312
Epoch 28/300, trend Loss: 0.0310 | 0.0319
Epoch 29/300, trend Loss: 0.0295 | 0.0309
Epoch 30/300, trend Loss: 0.0314 | 0.0319
Epoch 31/300, trend Loss: 0.0307 | 0.0310
Epoch 32/300, trend Loss: 0.0366 | 0.0365
Epoch 33/300, trend Loss: 0.0303 | 0.0315
Epoch 34/300, trend Loss: 0.0332 | 0.0327
Epoch 35/300, trend Loss: 0.0289 | 0.0306
Epoch 36/300, trend Loss: 0.0303 | 0.0314
Epoch 37/300, trend Loss: 0.0295 | 0.0339
Epoch 38/300, trend Loss: 0.0349 | 0.0326
Epoch 39/300, trend Loss: 0.0284 | 0.0305
Epoch 40/300, trend Loss: 0.0273 | 0.0319
Epoch 41/300, trend Loss: 0.0279 | 0.0310
Epoch 42/300, trend Loss: 0.0276 | 0.0320
Epoch 43/300, trend Loss: 0.0282 | 0.0324
Epoch 44/300, trend Loss: 0.0304 | 0.0342
Epoch 45/300, trend Loss: 0.0276 | 0.0302
Epoch 46/300, trend Loss: 0.0266 | 0.0334
Epoch 47/300, trend Loss: 0.0261 | 0.0294
Epoch 48/300, trend Loss: 0.0264 | 0.0325
Epoch 49/300, trend Loss: 0.0255 | 0.0291
Epoch 50/300, trend Loss: 0.0264 | 0.0328
Epoch 51/300, trend Loss: 0.0248 | 0.0280
Epoch 52/300, trend Loss: 0.0252 | 0.0320
Epoch 53/300, trend Loss: 0.0240 | 0.0274
Epoch 54/300, trend Loss: 0.0244 | 0.0306
Epoch 55/300, trend Loss: 0.0236 | 0.0275
Epoch 56/300, trend Loss: 0.0242 | 0.0306
Epoch 57/300, trend Loss: 0.0233 | 0.0270
Epoch 58/300, trend Loss: 0.0235 | 0.0301
Epoch 59/300, trend Loss: 0.0227 | 0.0268
Epoch 60/300, trend Loss: 0.0226 | 0.0288
Epoch 61/300, trend Loss: 0.0221 | 0.0268
Epoch 62/300, trend Loss: 0.0220 | 0.0284
Epoch 63/300, trend Loss: 0.0216 | 0.0265
Epoch 64/300, trend Loss: 0.0214 | 0.0277
Epoch 65/300, trend Loss: 0.0210 | 0.0260
Epoch 66/300, trend Loss: 0.0208 | 0.0270
Epoch 67/300, trend Loss: 0.0207 | 0.0255
Epoch 68/300, trend Loss: 0.0205 | 0.0265
Epoch 69/300, trend Loss: 0.0204 | 0.0252
Epoch 70/300, trend Loss: 0.0202 | 0.0263
Epoch 71/300, trend Loss: 0.0202 | 0.0249
Epoch 72/300, trend Loss: 0.0199 | 0.0258
Epoch 73/300, trend Loss: 0.0199 | 0.0247
Epoch 74/300, trend Loss: 0.0198 | 0.0257
Epoch 75/300, trend Loss: 0.0196 | 0.0246
Epoch 76/300, trend Loss: 0.0196 | 0.0257
Epoch 77/300, trend Loss: 0.0194 | 0.0245
Epoch 78/300, trend Loss: 0.0192 | 0.0254
Epoch 79/300, trend Loss: 0.0190 | 0.0244
Epoch 80/300, trend Loss: 0.0188 | 0.0251
Epoch 81/300, trend Loss: 0.0186 | 0.0243
Epoch 82/300, trend Loss: 0.0185 | 0.0249
Epoch 83/300, trend Loss: 0.0183 | 0.0242
Epoch 84/300, trend Loss: 0.0182 | 0.0244
Epoch 85/300, trend Loss: 0.0181 | 0.0238
Epoch 86/300, trend Loss: 0.0181 | 0.0240
Epoch 87/300, trend Loss: 0.0181 | 0.0236
Epoch 88/300, trend Loss: 0.0181 | 0.0236
Epoch 89/300, trend Loss: 0.0181 | 0.0234
Epoch 90/300, trend Loss: 0.0179 | 0.0236
Epoch 91/300, trend Loss: 0.0178 | 0.0238
Epoch 92/300, trend Loss: 0.0177 | 0.0240
Epoch 93/300, trend Loss: 0.0177 | 0.0239
Epoch 94/300, trend Loss: 0.0176 | 0.0241
Epoch 95/300, trend Loss: 0.0176 | 0.0239
Epoch 96/300, trend Loss: 0.0175 | 0.0238
Epoch 97/300, trend Loss: 0.0175 | 0.0236
Epoch 98/300, trend Loss: 0.0175 | 0.0236
Epoch 99/300, trend Loss: 0.0174 | 0.0236
Epoch 100/300, trend Loss: 0.0174 | 0.0235
Epoch 101/300, trend Loss: 0.0173 | 0.0235
Epoch 102/300, trend Loss: 0.0172 | 0.0234
Epoch 103/300, trend Loss: 0.0171 | 0.0233
Epoch 104/300, trend Loss: 0.0169 | 0.0232
Epoch 105/300, trend Loss: 0.0168 | 0.0231
Epoch 106/300, trend Loss: 0.0168 | 0.0230
Epoch 107/300, trend Loss: 0.0167 | 0.0229
Epoch 108/300, trend Loss: 0.0167 | 0.0229
Epoch 109/300, trend Loss: 0.0166 | 0.0228
Epoch 110/300, trend Loss: 0.0166 | 0.0228
Epoch 111/300, trend Loss: 0.0166 | 0.0227
Epoch 112/300, trend Loss: 0.0165 | 0.0227
Epoch 113/300, trend Loss: 0.0165 | 0.0226
Epoch 114/300, trend Loss: 0.0165 | 0.0226
Epoch 115/300, trend Loss: 0.0165 | 0.0226
Epoch 116/300, trend Loss: 0.0165 | 0.0226
Epoch 117/300, trend Loss: 0.0165 | 0.0226
Epoch 118/300, trend Loss: 0.0165 | 0.0225
Epoch 119/300, trend Loss: 0.0165 | 0.0225
Epoch 120/300, trend Loss: 0.0164 | 0.0225
Epoch 121/300, trend Loss: 0.0165 | 0.0225
Epoch 122/300, trend Loss: 0.0164 | 0.0225
Epoch 123/300, trend Loss: 0.0164 | 0.0225
Epoch 124/300, trend Loss: 0.0164 | 0.0226
Epoch 125/300, trend Loss: 0.0164 | 0.0226
Epoch 126/300, trend Loss: 0.0164 | 0.0226
Epoch 127/300, trend Loss: 0.0164 | 0.0227
Epoch 128/300, trend Loss: 0.0164 | 0.0227
Epoch 129/300, trend Loss: 0.0163 | 0.0228
Epoch 130/300, trend Loss: 0.0163 | 0.0229
Epoch 131/300, trend Loss: 0.0163 | 0.0229
Epoch 132/300, trend Loss: 0.0162 | 0.0229
Epoch 133/300, trend Loss: 0.0162 | 0.0230
Epoch 134/300, trend Loss: 0.0162 | 0.0230
Epoch 135/300, trend Loss: 0.0162 | 0.0230
Epoch 136/300, trend Loss: 0.0161 | 0.0229
Epoch 137/300, trend Loss: 0.0161 | 0.0229
Epoch 138/300, trend Loss: 0.0161 | 0.0229
Epoch 139/300, trend Loss: 0.0161 | 0.0227
Epoch 140/300, trend Loss: 0.0161 | 0.0227
Epoch 141/300, trend Loss: 0.0161 | 0.0227
Epoch 142/300, trend Loss: 0.0161 | 0.0225
Epoch 143/300, trend Loss: 0.0161 | 0.0225
Epoch 144/300, trend Loss: 0.0161 | 0.0225
Epoch 145/300, trend Loss: 0.0161 | 0.0224
Epoch 146/300, trend Loss: 0.0161 | 0.0223
Epoch 147/300, trend Loss: 0.0161 | 0.0223
Epoch 148/300, trend Loss: 0.0161 | 0.0223
Epoch 149/300, trend Loss: 0.0161 | 0.0223
Epoch 150/300, trend Loss: 0.0161 | 0.0223
Epoch 151/300, trend Loss: 0.0160 | 0.0222
Epoch 152/300, trend Loss: 0.0160 | 0.0222
Epoch 153/300, trend Loss: 0.0160 | 0.0222
Epoch 154/300, trend Loss: 0.0160 | 0.0222
Epoch 155/300, trend Loss: 0.0159 | 0.0222
Epoch 156/300, trend Loss: 0.0159 | 0.0222
Epoch 157/300, trend Loss: 0.0159 | 0.0222
Epoch 158/300, trend Loss: 0.0159 | 0.0222
Epoch 159/300, trend Loss: 0.0159 | 0.0222
Epoch 160/300, trend Loss: 0.0158 | 0.0222
Epoch 161/300, trend Loss: 0.0158 | 0.0222
Epoch 162/300, trend Loss: 0.0158 | 0.0222
Epoch 163/300, trend Loss: 0.0158 | 0.0221
Epoch 164/300, trend Loss: 0.0158 | 0.0221
Epoch 165/300, trend Loss: 0.0158 | 0.0221
Epoch 166/300, trend Loss: 0.0158 | 0.0221
Epoch 167/300, trend Loss: 0.0158 | 0.0221
Epoch 168/300, trend Loss: 0.0158 | 0.0221
Epoch 169/300, trend Loss: 0.0158 | 0.0221
Epoch 170/300, trend Loss: 0.0158 | 0.0221
Epoch 171/300, trend Loss: 0.0158 | 0.0221
Epoch 172/300, trend Loss: 0.0157 | 0.0221
Epoch 173/300, trend Loss: 0.0157 | 0.0221
Epoch 174/300, trend Loss: 0.0157 | 0.0221
Epoch 175/300, trend Loss: 0.0157 | 0.0221
Epoch 176/300, trend Loss: 0.0157 | 0.0221
Epoch 177/300, trend Loss: 0.0157 | 0.0221
Epoch 178/300, trend Loss: 0.0157 | 0.0221
Epoch 179/300, trend Loss: 0.0157 | 0.0221
Epoch 180/300, trend Loss: 0.0157 | 0.0221
Epoch 181/300, trend Loss: 0.0157 | 0.0221
Epoch 182/300, trend Loss: 0.0157 | 0.0221
Epoch 183/300, trend Loss: 0.0157 | 0.0221
Epoch 184/300, trend Loss: 0.0157 | 0.0221
Epoch 185/300, trend Loss: 0.0157 | 0.0221
Epoch 186/300, trend Loss: 0.0157 | 0.0221
Epoch 187/300, trend Loss: 0.0157 | 0.0220
Epoch 188/300, trend Loss: 0.0157 | 0.0220
Epoch 189/300, trend Loss: 0.0157 | 0.0220
Epoch 190/300, trend Loss: 0.0157 | 0.0220
Epoch 191/300, trend Loss: 0.0157 | 0.0220
Epoch 192/300, trend Loss: 0.0157 | 0.0220
Epoch 193/300, trend Loss: 0.0157 | 0.0220
Epoch 194/300, trend Loss: 0.0157 | 0.0220
Epoch 195/300, trend Loss: 0.0157 | 0.0220
Epoch 196/300, trend Loss: 0.0157 | 0.0220
Epoch 197/300, trend Loss: 0.0157 | 0.0220
Epoch 198/300, trend Loss: 0.0157 | 0.0220
Epoch 199/300, trend Loss: 0.0157 | 0.0220
Epoch 200/300, trend Loss: 0.0157 | 0.0220
Epoch 201/300, trend Loss: 0.0157 | 0.0220
Epoch 202/300, trend Loss: 0.0157 | 0.0220
Epoch 203/300, trend Loss: 0.0156 | 0.0220
Epoch 204/300, trend Loss: 0.0156 | 0.0220
Epoch 205/300, trend Loss: 0.0156 | 0.0220
Epoch 206/300, trend Loss: 0.0156 | 0.0220
Epoch 207/300, trend Loss: 0.0156 | 0.0220
Epoch 208/300, trend Loss: 0.0156 | 0.0220
Epoch 209/300, trend Loss: 0.0156 | 0.0220
Epoch 210/300, trend Loss: 0.0156 | 0.0220
Epoch 211/300, trend Loss: 0.0156 | 0.0220
Epoch 212/300, trend Loss: 0.0156 | 0.0220
Epoch 213/300, trend Loss: 0.0156 | 0.0220
Epoch 214/300, trend Loss: 0.0156 | 0.0220
Epoch 215/300, trend Loss: 0.0156 | 0.0220
Epoch 216/300, trend Loss: 0.0156 | 0.0220
Epoch 217/300, trend Loss: 0.0156 | 0.0220
Epoch 218/300, trend Loss: 0.0156 | 0.0220
Epoch 219/300, trend Loss: 0.0156 | 0.0220
Epoch 220/300, trend Loss: 0.0156 | 0.0220
Epoch 221/300, trend Loss: 0.0156 | 0.0220
Epoch 222/300, trend Loss: 0.0156 | 0.0220
Epoch 223/300, trend Loss: 0.0156 | 0.0220
Epoch 224/300, trend Loss: 0.0156 | 0.0220
Epoch 225/300, trend Loss: 0.0156 | 0.0220
Epoch 226/300, trend Loss: 0.0156 | 0.0220
Epoch 227/300, trend Loss: 0.0156 | 0.0220
Epoch 228/300, trend Loss: 0.0156 | 0.0220
Epoch 229/300, trend Loss: 0.0156 | 0.0220
Epoch 230/300, trend Loss: 0.0156 | 0.0220
Epoch 231/300, trend Loss: 0.0156 | 0.0220
Epoch 232/300, trend Loss: 0.0156 | 0.0220
Epoch 233/300, trend Loss: 0.0156 | 0.0220
Epoch 234/300, trend Loss: 0.0156 | 0.0220
Epoch 235/300, trend Loss: 0.0156 | 0.0220
Epoch 236/300, trend Loss: 0.0156 | 0.0220
Epoch 237/300, trend Loss: 0.0156 | 0.0220
Epoch 238/300, trend Loss: 0.0156 | 0.0220
Epoch 239/300, trend Loss: 0.0156 | 0.0220
Epoch 240/300, trend Loss: 0.0156 | 0.0220
Epoch 241/300, trend Loss: 0.0156 | 0.0220
Epoch 242/300, trend Loss: 0.0156 | 0.0220
Epoch 243/300, trend Loss: 0.0156 | 0.0220
Epoch 244/300, trend Loss: 0.0156 | 0.0220
Epoch 245/300, trend Loss: 0.0156 | 0.0220
Epoch 246/300, trend Loss: 0.0156 | 0.0220
Epoch 247/300, trend Loss: 0.0156 | 0.0220
Epoch 248/300, trend Loss: 0.0156 | 0.0220
Epoch 249/300, trend Loss: 0.0156 | 0.0220
Epoch 250/300, trend Loss: 0.0156 | 0.0220
Epoch 251/300, trend Loss: 0.0156 | 0.0220
Epoch 252/300, trend Loss: 0.0156 | 0.0220
Epoch 253/300, trend Loss: 0.0156 | 0.0220
Epoch 254/300, trend Loss: 0.0156 | 0.0220
Epoch 255/300, trend Loss: 0.0156 | 0.0220
Epoch 256/300, trend Loss: 0.0156 | 0.0220
Epoch 257/300, trend Loss: 0.0156 | 0.0220
Epoch 258/300, trend Loss: 0.0156 | 0.0220
Epoch 259/300, trend Loss: 0.0156 | 0.0220
Epoch 260/300, trend Loss: 0.0156 | 0.0220
Epoch 261/300, trend Loss: 0.0156 | 0.0220
Epoch 262/300, trend Loss: 0.0156 | 0.0220
Epoch 263/300, trend Loss: 0.0156 | 0.0220
Epoch 264/300, trend Loss: 0.0156 | 0.0220
Epoch 265/300, trend Loss: 0.0156 | 0.0220
Epoch 266/300, trend Loss: 0.0156 | 0.0220
Epoch 267/300, trend Loss: 0.0156 | 0.0220
Epoch 268/300, trend Loss: 0.0156 | 0.0220
Epoch 269/300, trend Loss: 0.0156 | 0.0220
Epoch 270/300, trend Loss: 0.0156 | 0.0220
Epoch 271/300, trend Loss: 0.0156 | 0.0220
Epoch 272/300, trend Loss: 0.0156 | 0.0220
Epoch 273/300, trend Loss: 0.0156 | 0.0220
Epoch 274/300, trend Loss: 0.0156 | 0.0220
Epoch 275/300, trend Loss: 0.0156 | 0.0220
Epoch 276/300, trend Loss: 0.0156 | 0.0220
Epoch 277/300, trend Loss: 0.0156 | 0.0220
Epoch 278/300, trend Loss: 0.0156 | 0.0220
Epoch 279/300, trend Loss: 0.0156 | 0.0220
Epoch 280/300, trend Loss: 0.0156 | 0.0220
Epoch 281/300, trend Loss: 0.0156 | 0.0220
Epoch 282/300, trend Loss: 0.0156 | 0.0220
Epoch 283/300, trend Loss: 0.0156 | 0.0220
Epoch 284/300, trend Loss: 0.0156 | 0.0220
Epoch 285/300, trend Loss: 0.0156 | 0.0220
Epoch 286/300, trend Loss: 0.0156 | 0.0220
Epoch 287/300, trend Loss: 0.0156 | 0.0220
Epoch 288/300, trend Loss: 0.0156 | 0.0220
Epoch 289/300, trend Loss: 0.0156 | 0.0220
Epoch 290/300, trend Loss: 0.0156 | 0.0220
Epoch 291/300, trend Loss: 0.0156 | 0.0220
Epoch 292/300, trend Loss: 0.0156 | 0.0220
Epoch 293/300, trend Loss: 0.0156 | 0.0220
Epoch 294/300, trend Loss: 0.0156 | 0.0220
Epoch 295/300, trend Loss: 0.0156 | 0.0220
Epoch 296/300, trend Loss: 0.0156 | 0.0220
Epoch 297/300, trend Loss: 0.0156 | 0.0220
Epoch 298/300, trend Loss: 0.0156 | 0.0220
Epoch 299/300, trend Loss: 0.0156 | 0.0220
Epoch 300/300, trend Loss: 0.0156 | 0.0220
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.8777006631811535, 'learning_rate': 0.0009545753845990329, 'batch_size': 116, 'step_size': 10, 'gamma': 0.7749068311619396}
Epoch 1/300, seasonal_0 Loss: 0.3109 | 0.1078
Epoch 2/300, seasonal_0 Loss: 0.1301 | 0.0953
Epoch 3/300, seasonal_0 Loss: 0.0965 | 0.1003
Epoch 4/300, seasonal_0 Loss: 0.1031 | 0.2024
Epoch 5/300, seasonal_0 Loss: 0.1102 | 0.1295
Epoch 6/300, seasonal_0 Loss: 0.0955 | 0.0682
Epoch 7/300, seasonal_0 Loss: 0.0955 | 0.0993
Epoch 8/300, seasonal_0 Loss: 0.1049 | 0.0818
Epoch 9/300, seasonal_0 Loss: 0.1119 | 0.0799
Epoch 10/300, seasonal_0 Loss: 0.1285 | 0.0683
Epoch 11/300, seasonal_0 Loss: 0.0935 | 0.0626
Epoch 12/300, seasonal_0 Loss: 0.1204 | 0.0874
Epoch 13/300, seasonal_0 Loss: 0.0975 | 0.0691
Epoch 14/300, seasonal_0 Loss: 0.0857 | 0.0793
Epoch 15/300, seasonal_0 Loss: 0.0700 | 0.0597
Epoch 16/300, seasonal_0 Loss: 0.0598 | 0.0491
Epoch 17/300, seasonal_0 Loss: 0.0563 | 0.0453
Epoch 18/300, seasonal_0 Loss: 0.0530 | 0.0471
Epoch 19/300, seasonal_0 Loss: 0.0509 | 0.0461
Epoch 20/300, seasonal_0 Loss: 0.0494 | 0.0487
Epoch 21/300, seasonal_0 Loss: 0.0530 | 0.0481
Epoch 22/300, seasonal_0 Loss: 0.0479 | 0.0456
Epoch 23/300, seasonal_0 Loss: 0.0480 | 0.0452
Epoch 24/300, seasonal_0 Loss: 0.0468 | 0.0450
Epoch 25/300, seasonal_0 Loss: 0.0475 | 0.0441
Epoch 26/300, seasonal_0 Loss: 0.0455 | 0.0439
Epoch 27/300, seasonal_0 Loss: 0.0445 | 0.0426
Epoch 28/300, seasonal_0 Loss: 0.0437 | 0.0430
Epoch 29/300, seasonal_0 Loss: 0.0434 | 0.0424
Epoch 30/300, seasonal_0 Loss: 0.0431 | 0.0428
Epoch 31/300, seasonal_0 Loss: 0.0429 | 0.0416
Epoch 32/300, seasonal_0 Loss: 0.0424 | 0.0419
Epoch 33/300, seasonal_0 Loss: 0.0424 | 0.0415
Epoch 34/300, seasonal_0 Loss: 0.0423 | 0.0419
Epoch 35/300, seasonal_0 Loss: 0.0423 | 0.0416
Epoch 36/300, seasonal_0 Loss: 0.0425 | 0.0411
Epoch 37/300, seasonal_0 Loss: 0.0430 | 0.0405
Epoch 38/300, seasonal_0 Loss: 0.0433 | 0.0412
Epoch 39/300, seasonal_0 Loss: 0.0439 | 0.0403
Epoch 40/300, seasonal_0 Loss: 0.0440 | 0.0406
Epoch 41/300, seasonal_0 Loss: 0.0447 | 0.0422
Epoch 42/300, seasonal_0 Loss: 0.0451 | 0.0431
Epoch 43/300, seasonal_0 Loss: 0.0436 | 0.0424
Epoch 44/300, seasonal_0 Loss: 0.0420 | 0.0415
Epoch 45/300, seasonal_0 Loss: 0.0409 | 0.0407
Epoch 46/300, seasonal_0 Loss: 0.0401 | 0.0399
Epoch 47/300, seasonal_0 Loss: 0.0396 | 0.0395
Epoch 48/300, seasonal_0 Loss: 0.0393 | 0.0393
Epoch 49/300, seasonal_0 Loss: 0.0390 | 0.0391
Epoch 50/300, seasonal_0 Loss: 0.0388 | 0.0389
Epoch 51/300, seasonal_0 Loss: 0.0385 | 0.0387
Epoch 52/300, seasonal_0 Loss: 0.0383 | 0.0386
Epoch 53/300, seasonal_0 Loss: 0.0381 | 0.0385
Epoch 54/300, seasonal_0 Loss: 0.0379 | 0.0383
Epoch 55/300, seasonal_0 Loss: 0.0377 | 0.0382
Epoch 56/300, seasonal_0 Loss: 0.0375 | 0.0380
Epoch 57/300, seasonal_0 Loss: 0.0373 | 0.0378
Epoch 58/300, seasonal_0 Loss: 0.0371 | 0.0377
Epoch 59/300, seasonal_0 Loss: 0.0369 | 0.0376
Epoch 60/300, seasonal_0 Loss: 0.0367 | 0.0374
Epoch 61/300, seasonal_0 Loss: 0.0365 | 0.0373
Epoch 62/300, seasonal_0 Loss: 0.0363 | 0.0371
Epoch 63/300, seasonal_0 Loss: 0.0362 | 0.0370
Epoch 64/300, seasonal_0 Loss: 0.0360 | 0.0369
Epoch 65/300, seasonal_0 Loss: 0.0359 | 0.0368
Epoch 66/300, seasonal_0 Loss: 0.0358 | 0.0369
Epoch 67/300, seasonal_0 Loss: 0.0358 | 0.0369
Epoch 68/300, seasonal_0 Loss: 0.0357 | 0.0368
Epoch 69/300, seasonal_0 Loss: 0.0354 | 0.0366
Epoch 70/300, seasonal_0 Loss: 0.0352 | 0.0364
Epoch 71/300, seasonal_0 Loss: 0.0349 | 0.0362
Epoch 72/300, seasonal_0 Loss: 0.0348 | 0.0361
Epoch 73/300, seasonal_0 Loss: 0.0346 | 0.0360
Epoch 74/300, seasonal_0 Loss: 0.0345 | 0.0359
Epoch 75/300, seasonal_0 Loss: 0.0344 | 0.0359
Epoch 76/300, seasonal_0 Loss: 0.0343 | 0.0358
Epoch 77/300, seasonal_0 Loss: 0.0342 | 0.0357
Epoch 78/300, seasonal_0 Loss: 0.0341 | 0.0357
Epoch 79/300, seasonal_0 Loss: 0.0341 | 0.0356
Epoch 80/300, seasonal_0 Loss: 0.0340 | 0.0356
Epoch 81/300, seasonal_0 Loss: 0.0339 | 0.0356
Epoch 82/300, seasonal_0 Loss: 0.0339 | 0.0355
Epoch 83/300, seasonal_0 Loss: 0.0338 | 0.0355
Epoch 84/300, seasonal_0 Loss: 0.0338 | 0.0354
Epoch 85/300, seasonal_0 Loss: 0.0337 | 0.0354
Epoch 86/300, seasonal_0 Loss: 0.0336 | 0.0354
Epoch 87/300, seasonal_0 Loss: 0.0336 | 0.0353
Epoch 88/300, seasonal_0 Loss: 0.0336 | 0.0353
Epoch 89/300, seasonal_0 Loss: 0.0335 | 0.0353
Epoch 90/300, seasonal_0 Loss: 0.0335 | 0.0352
Epoch 91/300, seasonal_0 Loss: 0.0334 | 0.0352
Epoch 92/300, seasonal_0 Loss: 0.0334 | 0.0352
Epoch 93/300, seasonal_0 Loss: 0.0334 | 0.0352
Epoch 94/300, seasonal_0 Loss: 0.0333 | 0.0351
Epoch 95/300, seasonal_0 Loss: 0.0333 | 0.0351
Epoch 96/300, seasonal_0 Loss: 0.0333 | 0.0351
Epoch 97/300, seasonal_0 Loss: 0.0333 | 0.0351
Epoch 98/300, seasonal_0 Loss: 0.0332 | 0.0350
Epoch 99/300, seasonal_0 Loss: 0.0332 | 0.0350
Epoch 100/300, seasonal_0 Loss: 0.0332 | 0.0350
Epoch 101/300, seasonal_0 Loss: 0.0331 | 0.0350
Epoch 102/300, seasonal_0 Loss: 0.0331 | 0.0350
Epoch 103/300, seasonal_0 Loss: 0.0331 | 0.0350
Epoch 104/300, seasonal_0 Loss: 0.0331 | 0.0349
Epoch 105/300, seasonal_0 Loss: 0.0331 | 0.0349
Epoch 106/300, seasonal_0 Loss: 0.0330 | 0.0349
Epoch 107/300, seasonal_0 Loss: 0.0330 | 0.0349
Epoch 108/300, seasonal_0 Loss: 0.0330 | 0.0349
Epoch 109/300, seasonal_0 Loss: 0.0330 | 0.0349
Epoch 110/300, seasonal_0 Loss: 0.0330 | 0.0348
Epoch 111/300, seasonal_0 Loss: 0.0330 | 0.0348
Epoch 112/300, seasonal_0 Loss: 0.0330 | 0.0348
Epoch 113/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 114/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 115/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 116/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 117/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 118/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 119/300, seasonal_0 Loss: 0.0329 | 0.0348
Epoch 120/300, seasonal_0 Loss: 0.0329 | 0.0347
Epoch 121/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 122/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 123/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 124/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 125/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 126/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 127/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 128/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 129/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 130/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 131/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 132/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 133/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 134/300, seasonal_0 Loss: 0.0328 | 0.0347
Epoch 135/300, seasonal_0 Loss: 0.0327 | 0.0347
Epoch 136/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 137/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 138/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 139/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 140/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 141/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 142/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 143/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 144/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 145/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 146/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 147/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 148/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 149/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 150/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 151/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 152/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 153/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 154/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 155/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 156/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 157/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 158/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 159/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 160/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 161/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 162/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 163/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 164/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 165/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 166/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 167/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 168/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 169/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 170/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 171/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 172/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 173/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 174/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 175/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 176/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 177/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 178/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 179/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 180/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 181/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 182/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 183/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 184/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 185/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 186/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 187/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 188/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 189/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 190/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 191/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 192/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 193/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 194/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 195/300, seasonal_0 Loss: 0.0327 | 0.0346
Epoch 196/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 197/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 198/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 199/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 200/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 201/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 202/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 203/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 204/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 205/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 206/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 207/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 208/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 209/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 210/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 211/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 212/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 213/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 214/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 215/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 216/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 217/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 218/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 219/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 220/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 221/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 222/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 223/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 224/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 225/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 226/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 227/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 228/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 229/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 230/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 231/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 232/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 233/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 234/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 235/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 236/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 237/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 238/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 239/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 240/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 241/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 242/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 243/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 244/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 245/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 246/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 247/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 248/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 249/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 250/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 251/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 252/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 253/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 254/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 255/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 256/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 257/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 258/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 259/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 260/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 261/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 262/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 263/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 264/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 265/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 266/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 267/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 268/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 269/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 270/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 271/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 272/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 273/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 274/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 275/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 276/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 277/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 278/300, seasonal_0 Loss: 0.0326 | 0.0346
Epoch 279/300, seasonal_0 Loss: 0.0326 | 0.0346
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 13, 'train_rates': 0.794260906095709, 'learning_rate': 0.0007573918320032116, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9362791699632287}
Epoch 1/300, seasonal_1 Loss: 0.1567 | 0.0932
Epoch 2/300, seasonal_1 Loss: 0.0967 | 0.0669
Epoch 3/300, seasonal_1 Loss: 0.0852 | 0.0683
Epoch 4/300, seasonal_1 Loss: 0.0759 | 0.0626
Epoch 5/300, seasonal_1 Loss: 0.0647 | 0.0543
Epoch 6/300, seasonal_1 Loss: 0.0648 | 0.0580
Epoch 7/300, seasonal_1 Loss: 0.0543 | 0.0506
Epoch 8/300, seasonal_1 Loss: 0.0530 | 0.0521
Epoch 9/300, seasonal_1 Loss: 0.0551 | 0.0635
Epoch 10/300, seasonal_1 Loss: 0.0699 | 0.0474
Epoch 11/300, seasonal_1 Loss: 0.0591 | 0.0486
Epoch 12/300, seasonal_1 Loss: 0.0451 | 0.0402
Epoch 13/300, seasonal_1 Loss: 0.0417 | 0.0392
Epoch 14/300, seasonal_1 Loss: 0.0375 | 0.0356
Epoch 15/300, seasonal_1 Loss: 0.0334 | 0.0316
Epoch 16/300, seasonal_1 Loss: 0.0329 | 0.0347
Epoch 17/300, seasonal_1 Loss: 0.0379 | 0.0448
Epoch 18/300, seasonal_1 Loss: 0.0337 | 0.0316
Epoch 19/300, seasonal_1 Loss: 0.0291 | 0.0295
Epoch 20/300, seasonal_1 Loss: 0.0333 | 0.0414
Epoch 21/300, seasonal_1 Loss: 0.0316 | 0.0300
Epoch 22/300, seasonal_1 Loss: 0.0289 | 0.0281
Epoch 23/300, seasonal_1 Loss: 0.0282 | 0.0319
Epoch 24/300, seasonal_1 Loss: 0.0260 | 0.0320
Epoch 25/300, seasonal_1 Loss: 0.0235 | 0.0309
Epoch 26/300, seasonal_1 Loss: 0.0232 | 0.0289
Epoch 27/300, seasonal_1 Loss: 0.0235 | 0.0273
Epoch 28/300, seasonal_1 Loss: 0.0224 | 0.0248
Epoch 29/300, seasonal_1 Loss: 0.0214 | 0.0256
Epoch 30/300, seasonal_1 Loss: 0.0216 | 0.0280
Epoch 31/300, seasonal_1 Loss: 0.0235 | 0.0249
Epoch 32/300, seasonal_1 Loss: 0.0224 | 0.0273
Epoch 33/300, seasonal_1 Loss: 0.0243 | 0.0321
Epoch 34/300, seasonal_1 Loss: 0.0254 | 0.0308
Epoch 35/300, seasonal_1 Loss: 0.0318 | 0.0334
Epoch 36/300, seasonal_1 Loss: 0.0218 | 0.0266
Epoch 37/300, seasonal_1 Loss: 0.0204 | 0.0254
Epoch 38/300, seasonal_1 Loss: 0.0245 | 0.0275
Epoch 39/300, seasonal_1 Loss: 0.0196 | 0.0237
Epoch 40/300, seasonal_1 Loss: 0.0237 | 0.0323
Epoch 41/300, seasonal_1 Loss: 0.0217 | 0.0332
Epoch 42/300, seasonal_1 Loss: 0.0242 | 0.0266
Epoch 43/300, seasonal_1 Loss: 0.0244 | 0.0343
Epoch 44/300, seasonal_1 Loss: 0.0220 | 0.0277
Epoch 45/300, seasonal_1 Loss: 0.0184 | 0.0251
Epoch 46/300, seasonal_1 Loss: 0.0193 | 0.0256
Epoch 47/300, seasonal_1 Loss: 0.0204 | 0.0268
Epoch 48/300, seasonal_1 Loss: 0.0191 | 0.0240
Epoch 49/300, seasonal_1 Loss: 0.0174 | 0.0233
Epoch 50/300, seasonal_1 Loss: 0.0178 | 0.0219
Epoch 51/300, seasonal_1 Loss: 0.0155 | 0.0250
Epoch 52/300, seasonal_1 Loss: 0.0161 | 0.0229
Epoch 53/300, seasonal_1 Loss: 0.0156 | 0.0236
Epoch 54/300, seasonal_1 Loss: 0.0147 | 0.0222
Epoch 55/300, seasonal_1 Loss: 0.0155 | 0.0232
Epoch 56/300, seasonal_1 Loss: 0.0147 | 0.0227
Epoch 57/300, seasonal_1 Loss: 0.0154 | 0.0209
Epoch 58/300, seasonal_1 Loss: 0.0159 | 0.0256
Epoch 59/300, seasonal_1 Loss: 0.0163 | 0.0227
Epoch 60/300, seasonal_1 Loss: 0.0148 | 0.0244
Epoch 61/300, seasonal_1 Loss: 0.0143 | 0.0214
Epoch 62/300, seasonal_1 Loss: 0.0138 | 0.0247
Epoch 63/300, seasonal_1 Loss: 0.0143 | 0.0233
Epoch 64/300, seasonal_1 Loss: 0.0156 | 0.0234
Epoch 65/300, seasonal_1 Loss: 0.0152 | 0.0258
Epoch 66/300, seasonal_1 Loss: 0.0168 | 0.0258
Epoch 67/300, seasonal_1 Loss: 0.0133 | 0.0214
Epoch 68/300, seasonal_1 Loss: 0.0131 | 0.0241
Epoch 69/300, seasonal_1 Loss: 0.0127 | 0.0237
Epoch 70/300, seasonal_1 Loss: 0.0123 | 0.0212
Epoch 71/300, seasonal_1 Loss: 0.0126 | 0.0226
Epoch 72/300, seasonal_1 Loss: 0.0116 | 0.0217
Epoch 73/300, seasonal_1 Loss: 0.0115 | 0.0220
Epoch 74/300, seasonal_1 Loss: 0.0120 | 0.0234
Epoch 75/300, seasonal_1 Loss: 0.0114 | 0.0227
Epoch 76/300, seasonal_1 Loss: 0.0116 | 0.0243
Epoch 77/300, seasonal_1 Loss: 0.0148 | 0.0252
Epoch 78/300, seasonal_1 Loss: 0.0140 | 0.0228
Epoch 79/300, seasonal_1 Loss: 0.0134 | 0.0233
Epoch 80/300, seasonal_1 Loss: 0.0128 | 0.0270
Epoch 81/300, seasonal_1 Loss: 0.0124 | 0.0255
Epoch 82/300, seasonal_1 Loss: 0.0136 | 0.0220
Epoch 83/300, seasonal_1 Loss: 0.0126 | 0.0237
Epoch 84/300, seasonal_1 Loss: 0.0122 | 0.0242
Epoch 85/300, seasonal_1 Loss: 0.0163 | 0.0330
Epoch 86/300, seasonal_1 Loss: 0.0141 | 0.0225
Epoch 87/300, seasonal_1 Loss: 0.0106 | 0.0229
Epoch 88/300, seasonal_1 Loss: 0.0102 | 0.0257
Epoch 89/300, seasonal_1 Loss: 0.0116 | 0.0253
Epoch 90/300, seasonal_1 Loss: 0.0110 | 0.0230
Epoch 91/300, seasonal_1 Loss: 0.0099 | 0.0229
Epoch 92/300, seasonal_1 Loss: 0.0088 | 0.0243
Epoch 93/300, seasonal_1 Loss: 0.0089 | 0.0254
Epoch 94/300, seasonal_1 Loss: 0.0103 | 0.0235
Epoch 95/300, seasonal_1 Loss: 0.0094 | 0.0248
Epoch 96/300, seasonal_1 Loss: 0.0084 | 0.0229
Epoch 97/300, seasonal_1 Loss: 0.0128 | 0.0256
Epoch 98/300, seasonal_1 Loss: 0.0114 | 0.0223
Epoch 99/300, seasonal_1 Loss: 0.0089 | 0.0227
Epoch 100/300, seasonal_1 Loss: 0.0081 | 0.0247
Epoch 101/300, seasonal_1 Loss: 0.0079 | 0.0245
Epoch 102/300, seasonal_1 Loss: 0.0089 | 0.0250
Epoch 103/300, seasonal_1 Loss: 0.0079 | 0.0231
Epoch 104/300, seasonal_1 Loss: 0.0073 | 0.0242
Epoch 105/300, seasonal_1 Loss: 0.0073 | 0.0237
Epoch 106/300, seasonal_1 Loss: 0.0076 | 0.0229
Epoch 107/300, seasonal_1 Loss: 0.0073 | 0.0234
Epoch 108/300, seasonal_1 Loss: 0.0075 | 0.0247
Epoch 109/300, seasonal_1 Loss: 0.0078 | 0.0244
Epoch 110/300, seasonal_1 Loss: 0.0079 | 0.0254
Epoch 111/300, seasonal_1 Loss: 0.0076 | 0.0231
Epoch 112/300, seasonal_1 Loss: 0.0073 | 0.0235
Epoch 113/300, seasonal_1 Loss: 0.0068 | 0.0243
Epoch 114/300, seasonal_1 Loss: 0.0064 | 0.0237
Epoch 115/300, seasonal_1 Loss: 0.0063 | 0.0242
Epoch 116/300, seasonal_1 Loss: 0.0061 | 0.0245
Epoch 117/300, seasonal_1 Loss: 0.0063 | 0.0241
Epoch 118/300, seasonal_1 Loss: 0.0069 | 0.0239
Epoch 119/300, seasonal_1 Loss: 0.0065 | 0.0253
Epoch 120/300, seasonal_1 Loss: 0.0065 | 0.0244
Epoch 121/300, seasonal_1 Loss: 0.0067 | 0.0236
Epoch 122/300, seasonal_1 Loss: 0.0059 | 0.0246
Epoch 123/300, seasonal_1 Loss: 0.0056 | 0.0244
Epoch 124/300, seasonal_1 Loss: 0.0059 | 0.0228
Epoch 125/300, seasonal_1 Loss: 0.0056 | 0.0241
Epoch 126/300, seasonal_1 Loss: 0.0055 | 0.0235
Epoch 127/300, seasonal_1 Loss: 0.0054 | 0.0235
Epoch 128/300, seasonal_1 Loss: 0.0056 | 0.0242
Epoch 129/300, seasonal_1 Loss: 0.0051 | 0.0232
Epoch 130/300, seasonal_1 Loss: 0.0054 | 0.0237
Epoch 131/300, seasonal_1 Loss: 0.0047 | 0.0233
Epoch 132/300, seasonal_1 Loss: 0.0047 | 0.0244
Epoch 133/300, seasonal_1 Loss: 0.0048 | 0.0251
Epoch 134/300, seasonal_1 Loss: 0.0050 | 0.0232
Epoch 135/300, seasonal_1 Loss: 0.0045 | 0.0240
Epoch 136/300, seasonal_1 Loss: 0.0042 | 0.0250
Epoch 137/300, seasonal_1 Loss: 0.0043 | 0.0234
Epoch 138/300, seasonal_1 Loss: 0.0042 | 0.0245
Epoch 139/300, seasonal_1 Loss: 0.0042 | 0.0238
Epoch 140/300, seasonal_1 Loss: 0.0044 | 0.0245
Epoch 141/300, seasonal_1 Loss: 0.0047 | 0.0243
Epoch 142/300, seasonal_1 Loss: 0.0039 | 0.0230
Epoch 143/300, seasonal_1 Loss: 0.0039 | 0.0230
Epoch 144/300, seasonal_1 Loss: 0.0037 | 0.0238
Epoch 145/300, seasonal_1 Loss: 0.0036 | 0.0234
Epoch 146/300, seasonal_1 Loss: 0.0033 | 0.0231
Epoch 147/300, seasonal_1 Loss: 0.0034 | 0.0233
Epoch 148/300, seasonal_1 Loss: 0.0032 | 0.0231
Epoch 149/300, seasonal_1 Loss: 0.0034 | 0.0239
Epoch 150/300, seasonal_1 Loss: 0.0034 | 0.0241
Epoch 151/300, seasonal_1 Loss: 0.0033 | 0.0237
Epoch 152/300, seasonal_1 Loss: 0.0033 | 0.0238
Epoch 153/300, seasonal_1 Loss: 0.0033 | 0.0237
Epoch 154/300, seasonal_1 Loss: 0.0031 | 0.0235
Epoch 155/300, seasonal_1 Loss: 0.0031 | 0.0242
Epoch 156/300, seasonal_1 Loss: 0.0030 | 0.0237
Epoch 157/300, seasonal_1 Loss: 0.0028 | 0.0237
Epoch 158/300, seasonal_1 Loss: 0.0028 | 0.0238
Epoch 159/300, seasonal_1 Loss: 0.0028 | 0.0240
Epoch 160/300, seasonal_1 Loss: 0.0027 | 0.0237
Epoch 161/300, seasonal_1 Loss: 0.0027 | 0.0235
Epoch 162/300, seasonal_1 Loss: 0.0031 | 0.0248
Epoch 163/300, seasonal_1 Loss: 0.0040 | 0.0251
Epoch 164/300, seasonal_1 Loss: 0.0045 | 0.0243
Epoch 165/300, seasonal_1 Loss: 0.0033 | 0.0239
Epoch 166/300, seasonal_1 Loss: 0.0029 | 0.0231
Epoch 167/300, seasonal_1 Loss: 0.0027 | 0.0233
Epoch 168/300, seasonal_1 Loss: 0.0027 | 0.0231
Epoch 169/300, seasonal_1 Loss: 0.0027 | 0.0243
Epoch 170/300, seasonal_1 Loss: 0.0029 | 0.0227
Epoch 171/300, seasonal_1 Loss: 0.0029 | 0.0235
Epoch 172/300, seasonal_1 Loss: 0.0027 | 0.0236
Epoch 173/300, seasonal_1 Loss: 0.0040 | 0.0287
Epoch 174/300, seasonal_1 Loss: 0.0064 | 0.0238
Epoch 175/300, seasonal_1 Loss: 0.0038 | 0.0236
Epoch 176/300, seasonal_1 Loss: 0.0031 | 0.0234
Epoch 177/300, seasonal_1 Loss: 0.0028 | 0.0234
Epoch 178/300, seasonal_1 Loss: 0.0025 | 0.0236
Epoch 179/300, seasonal_1 Loss: 0.0025 | 0.0233
Epoch 180/300, seasonal_1 Loss: 0.0023 | 0.0234
Epoch 181/300, seasonal_1 Loss: 0.0022 | 0.0235
Epoch 182/300, seasonal_1 Loss: 0.0022 | 0.0232
Epoch 183/300, seasonal_1 Loss: 0.0022 | 0.0234
Epoch 184/300, seasonal_1 Loss: 0.0021 | 0.0231
Epoch 185/300, seasonal_1 Loss: 0.0021 | 0.0232
Epoch 186/300, seasonal_1 Loss: 0.0021 | 0.0230
Epoch 187/300, seasonal_1 Loss: 0.0021 | 0.0234
Epoch 188/300, seasonal_1 Loss: 0.0020 | 0.0229
Epoch 189/300, seasonal_1 Loss: 0.0021 | 0.0230
Epoch 190/300, seasonal_1 Loss: 0.0021 | 0.0228
Epoch 191/300, seasonal_1 Loss: 0.0021 | 0.0229
Epoch 192/300, seasonal_1 Loss: 0.0020 | 0.0229
Epoch 193/300, seasonal_1 Loss: 0.0021 | 0.0233
Epoch 194/300, seasonal_1 Loss: 0.0020 | 0.0228
Epoch 195/300, seasonal_1 Loss: 0.0021 | 0.0224
Epoch 196/300, seasonal_1 Loss: 0.0020 | 0.0230
Epoch 197/300, seasonal_1 Loss: 0.0020 | 0.0231
Epoch 198/300, seasonal_1 Loss: 0.0020 | 0.0224
Epoch 199/300, seasonal_1 Loss: 0.0019 | 0.0228
Epoch 200/300, seasonal_1 Loss: 0.0018 | 0.0232
Epoch 201/300, seasonal_1 Loss: 0.0019 | 0.0226
Epoch 202/300, seasonal_1 Loss: 0.0022 | 0.0230
Epoch 203/300, seasonal_1 Loss: 0.0021 | 0.0229
Epoch 204/300, seasonal_1 Loss: 0.0021 | 0.0228
Epoch 205/300, seasonal_1 Loss: 0.0022 | 0.0231
Epoch 206/300, seasonal_1 Loss: 0.0018 | 0.0229
Epoch 207/300, seasonal_1 Loss: 0.0017 | 0.0231
Epoch 208/300, seasonal_1 Loss: 0.0017 | 0.0232
Epoch 209/300, seasonal_1 Loss: 0.0018 | 0.0233
Epoch 210/300, seasonal_1 Loss: 0.0017 | 0.0237
Epoch 211/300, seasonal_1 Loss: 0.0018 | 0.0233
Epoch 212/300, seasonal_1 Loss: 0.0017 | 0.0236
Epoch 213/300, seasonal_1 Loss: 0.0017 | 0.0237
Epoch 214/300, seasonal_1 Loss: 0.0019 | 0.0244
Epoch 215/300, seasonal_1 Loss: 0.0019 | 0.0236
Epoch 216/300, seasonal_1 Loss: 0.0017 | 0.0233
Epoch 217/300, seasonal_1 Loss: 0.0017 | 0.0234
Epoch 218/300, seasonal_1 Loss: 0.0015 | 0.0233
Epoch 219/300, seasonal_1 Loss: 0.0014 | 0.0233
Epoch 220/300, seasonal_1 Loss: 0.0013 | 0.0235
Epoch 221/300, seasonal_1 Loss: 0.0020 | 0.0232
Epoch 222/300, seasonal_1 Loss: 0.0021 | 0.0226
Epoch 223/300, seasonal_1 Loss: 0.0015 | 0.0228
Epoch 224/300, seasonal_1 Loss: 0.0013 | 0.0226
Epoch 225/300, seasonal_1 Loss: 0.0014 | 0.0227
Epoch 226/300, seasonal_1 Loss: 0.0012 | 0.0227
Epoch 227/300, seasonal_1 Loss: 0.0011 | 0.0227
Epoch 228/300, seasonal_1 Loss: 0.0011 | 0.0227
Epoch 229/300, seasonal_1 Loss: 0.0010 | 0.0228
Epoch 230/300, seasonal_1 Loss: 0.0010 | 0.0229
Epoch 231/300, seasonal_1 Loss: 0.0010 | 0.0229
Epoch 232/300, seasonal_1 Loss: 0.0010 | 0.0230
Epoch 233/300, seasonal_1 Loss: 0.0010 | 0.0230
Epoch 234/300, seasonal_1 Loss: 0.0010 | 0.0229
Epoch 235/300, seasonal_1 Loss: 0.0010 | 0.0230
Epoch 236/300, seasonal_1 Loss: 0.0010 | 0.0230
Epoch 237/300, seasonal_1 Loss: 0.0010 | 0.0231
Epoch 238/300, seasonal_1 Loss: 0.0010 | 0.0229
Epoch 239/300, seasonal_1 Loss: 0.0010 | 0.0231
Epoch 240/300, seasonal_1 Loss: 0.0010 | 0.0229
Epoch 241/300, seasonal_1 Loss: 0.0010 | 0.0231
Epoch 242/300, seasonal_1 Loss: 0.0010 | 0.0229
Epoch 243/300, seasonal_1 Loss: 0.0010 | 0.0232
Epoch 244/300, seasonal_1 Loss: 0.0009 | 0.0229
Epoch 245/300, seasonal_1 Loss: 0.0009 | 0.0230
Epoch 246/300, seasonal_1 Loss: 0.0009 | 0.0229
Epoch 247/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 248/300, seasonal_1 Loss: 0.0009 | 0.0229
Epoch 249/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 250/300, seasonal_1 Loss: 0.0009 | 0.0230
Epoch 251/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 252/300, seasonal_1 Loss: 0.0009 | 0.0229
Epoch 253/300, seasonal_1 Loss: 0.0010 | 0.0233
Epoch 254/300, seasonal_1 Loss: 0.0009 | 0.0229
Epoch 255/300, seasonal_1 Loss: 0.0009 | 0.0231
Epoch 256/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 257/300, seasonal_1 Loss: 0.0009 | 0.0231
Epoch 258/300, seasonal_1 Loss: 0.0009 | 0.0230
Epoch 259/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 260/300, seasonal_1 Loss: 0.0008 | 0.0230
Epoch 261/300, seasonal_1 Loss: 0.0008 | 0.0232
Epoch 262/300, seasonal_1 Loss: 0.0008 | 0.0231
Epoch 263/300, seasonal_1 Loss: 0.0008 | 0.0232
Epoch 264/300, seasonal_1 Loss: 0.0008 | 0.0230
Epoch 265/300, seasonal_1 Loss: 0.0008 | 0.0234
Epoch 266/300, seasonal_1 Loss: 0.0008 | 0.0231
Epoch 267/300, seasonal_1 Loss: 0.0008 | 0.0232
Epoch 268/300, seasonal_1 Loss: 0.0008 | 0.0232
Epoch 269/300, seasonal_1 Loss: 0.0008 | 0.0233
Epoch 270/300, seasonal_1 Loss: 0.0009 | 0.0230
Epoch 271/300, seasonal_1 Loss: 0.0008 | 0.0235
Epoch 272/300, seasonal_1 Loss: 0.0008 | 0.0231
Epoch 273/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 274/300, seasonal_1 Loss: 0.0009 | 0.0233
Epoch 275/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 276/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 277/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 278/300, seasonal_1 Loss: 0.0009 | 0.0232
Epoch 279/300, seasonal_1 Loss: 0.0009 | 0.0230
Epoch 280/300, seasonal_1 Loss: 0.0008 | 0.0231
Epoch 281/300, seasonal_1 Loss: 0.0008 | 0.0230
Epoch 282/300, seasonal_1 Loss: 0.0008 | 0.0233
Epoch 283/300, seasonal_1 Loss: 0.0008 | 0.0232
Epoch 284/300, seasonal_1 Loss: 0.0008 | 0.0234
Epoch 285/300, seasonal_1 Loss: 0.0008 | 0.0233
Epoch 286/300, seasonal_1 Loss: 0.0008 | 0.0233
Epoch 287/300, seasonal_1 Loss: 0.0008 | 0.0232
Epoch 288/300, seasonal_1 Loss: 0.0008 | 0.0231
Epoch 289/300, seasonal_1 Loss: 0.0007 | 0.0230
Epoch 290/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 291/300, seasonal_1 Loss: 0.0007 | 0.0230
Epoch 292/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 293/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 294/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 295/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 296/300, seasonal_1 Loss: 0.0007 | 0.0232
Epoch 297/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 298/300, seasonal_1 Loss: 0.0007 | 0.0231
Epoch 299/300, seasonal_1 Loss: 0.0007 | 0.0232
Epoch 300/300, seasonal_1 Loss: 0.0007 | 0.0231
Training seasonal_2 component with params: {'observation_period_num': 11, 'train_rates': 0.7910318922890581, 'learning_rate': 0.000625564427498638, 'batch_size': 135, 'step_size': 14, 'gamma': 0.8524433026948389}
Epoch 1/300, seasonal_2 Loss: 0.4388 | 0.1936
Epoch 2/300, seasonal_2 Loss: 0.2470 | 0.1288
Epoch 3/300, seasonal_2 Loss: 0.2923 | 0.2469
Epoch 4/300, seasonal_2 Loss: 0.1673 | 0.1042
Epoch 5/300, seasonal_2 Loss: 0.1251 | 0.0911
Epoch 6/300, seasonal_2 Loss: 0.1077 | 0.1107
Epoch 7/300, seasonal_2 Loss: 0.1019 | 0.1279
Epoch 8/300, seasonal_2 Loss: 0.0966 | 0.1036
Epoch 9/300, seasonal_2 Loss: 0.0843 | 0.0789
Epoch 10/300, seasonal_2 Loss: 0.0776 | 0.0839
Epoch 11/300, seasonal_2 Loss: 0.0918 | 0.1021
Epoch 12/300, seasonal_2 Loss: 0.0915 | 0.0874
Epoch 13/300, seasonal_2 Loss: 0.0689 | 0.0790
Epoch 14/300, seasonal_2 Loss: 0.0730 | 0.1326
Epoch 15/300, seasonal_2 Loss: 0.0854 | 0.1064
Epoch 16/300, seasonal_2 Loss: 0.0707 | 0.0562
Epoch 17/300, seasonal_2 Loss: 0.0651 | 0.0512
Epoch 18/300, seasonal_2 Loss: 0.0605 | 0.0504
Epoch 19/300, seasonal_2 Loss: 0.0576 | 0.0579
Epoch 20/300, seasonal_2 Loss: 0.0521 | 0.0652
Epoch 21/300, seasonal_2 Loss: 0.0518 | 0.0548
Epoch 22/300, seasonal_2 Loss: 0.0520 | 0.0685
Epoch 23/300, seasonal_2 Loss: 0.0647 | 0.0563
Epoch 24/300, seasonal_2 Loss: 0.0534 | 0.0477
Epoch 25/300, seasonal_2 Loss: 0.0479 | 0.0399
Epoch 26/300, seasonal_2 Loss: 0.0452 | 0.0434
Epoch 27/300, seasonal_2 Loss: 0.0433 | 0.0391
Epoch 28/300, seasonal_2 Loss: 0.0422 | 0.0475
Epoch 29/300, seasonal_2 Loss: 0.0431 | 0.0440
Epoch 30/300, seasonal_2 Loss: 0.0456 | 0.0678
Epoch 31/300, seasonal_2 Loss: 0.0578 | 0.0538
Epoch 32/300, seasonal_2 Loss: 0.0476 | 0.0406
Epoch 33/300, seasonal_2 Loss: 0.0481 | 0.0580
Epoch 34/300, seasonal_2 Loss: 0.0542 | 0.0501
Epoch 35/300, seasonal_2 Loss: 0.0527 | 0.0622
Epoch 36/300, seasonal_2 Loss: 0.0638 | 0.0586
Epoch 37/300, seasonal_2 Loss: 0.0477 | 0.0416
Epoch 38/300, seasonal_2 Loss: 0.0553 | 0.0455
Epoch 39/300, seasonal_2 Loss: 0.0457 | 0.0456
Epoch 40/300, seasonal_2 Loss: 0.0450 | 0.0588
Epoch 41/300, seasonal_2 Loss: 0.0477 | 0.0475
Epoch 42/300, seasonal_2 Loss: 0.0434 | 0.0509
Epoch 43/300, seasonal_2 Loss: 0.0421 | 0.0400
Epoch 44/300, seasonal_2 Loss: 0.0397 | 0.0470
Epoch 45/300, seasonal_2 Loss: 0.0416 | 0.0426
Epoch 46/300, seasonal_2 Loss: 0.0401 | 0.0515
Epoch 47/300, seasonal_2 Loss: 0.0436 | 0.0465
Epoch 48/300, seasonal_2 Loss: 0.0400 | 0.0500
Epoch 49/300, seasonal_2 Loss: 0.0420 | 0.0443
Epoch 50/300, seasonal_2 Loss: 0.0383 | 0.0447
Epoch 51/300, seasonal_2 Loss: 0.0401 | 0.0392
Epoch 52/300, seasonal_2 Loss: 0.0373 | 0.0439
Epoch 53/300, seasonal_2 Loss: 0.0402 | 0.0395
Epoch 54/300, seasonal_2 Loss: 0.0373 | 0.0434
Epoch 55/300, seasonal_2 Loss: 0.0400 | 0.0391
Epoch 56/300, seasonal_2 Loss: 0.0370 | 0.0420
Epoch 57/300, seasonal_2 Loss: 0.0393 | 0.0362
Epoch 58/300, seasonal_2 Loss: 0.0365 | 0.0382
Epoch 59/300, seasonal_2 Loss: 0.0387 | 0.0358
Epoch 60/300, seasonal_2 Loss: 0.0368 | 0.0381
Epoch 61/300, seasonal_2 Loss: 0.0386 | 0.0363
Epoch 62/300, seasonal_2 Loss: 0.0368 | 0.0374
Epoch 63/300, seasonal_2 Loss: 0.0372 | 0.0348
Epoch 64/300, seasonal_2 Loss: 0.0350 | 0.0345
Epoch 65/300, seasonal_2 Loss: 0.0351 | 0.0339
Epoch 66/300, seasonal_2 Loss: 0.0333 | 0.0344
Epoch 67/300, seasonal_2 Loss: 0.0336 | 0.0342
Epoch 68/300, seasonal_2 Loss: 0.0321 | 0.0344
Epoch 69/300, seasonal_2 Loss: 0.0325 | 0.0347
Epoch 70/300, seasonal_2 Loss: 0.0312 | 0.0340
Epoch 71/300, seasonal_2 Loss: 0.0317 | 0.0340
Epoch 72/300, seasonal_2 Loss: 0.0312 | 0.0322
Epoch 73/300, seasonal_2 Loss: 0.0325 | 0.0327
Epoch 74/300, seasonal_2 Loss: 0.0332 | 0.0310
Epoch 75/300, seasonal_2 Loss: 0.0368 | 0.0350
Epoch 76/300, seasonal_2 Loss: 0.0365 | 0.0326
Epoch 77/300, seasonal_2 Loss: 0.0384 | 0.0342
Epoch 78/300, seasonal_2 Loss: 0.0392 | 0.0397
Epoch 79/300, seasonal_2 Loss: 0.0364 | 0.0335
Epoch 80/300, seasonal_2 Loss: 0.0320 | 0.0324
Epoch 81/300, seasonal_2 Loss: 0.0302 | 0.0298
Epoch 82/300, seasonal_2 Loss: 0.0295 | 0.0315
Epoch 83/300, seasonal_2 Loss: 0.0296 | 0.0304
Epoch 84/300, seasonal_2 Loss: 0.0293 | 0.0324
Epoch 85/300, seasonal_2 Loss: 0.0299 | 0.0306
Epoch 86/300, seasonal_2 Loss: 0.0296 | 0.0323
Epoch 87/300, seasonal_2 Loss: 0.0296 | 0.0298
Epoch 88/300, seasonal_2 Loss: 0.0290 | 0.0314
Epoch 89/300, seasonal_2 Loss: 0.0295 | 0.0301
Epoch 90/300, seasonal_2 Loss: 0.0289 | 0.0315
Epoch 91/300, seasonal_2 Loss: 0.0290 | 0.0295
Epoch 92/300, seasonal_2 Loss: 0.0282 | 0.0299
Epoch 93/300, seasonal_2 Loss: 0.0283 | 0.0290
Epoch 94/300, seasonal_2 Loss: 0.0277 | 0.0296
Epoch 95/300, seasonal_2 Loss: 0.0275 | 0.0282
Epoch 96/300, seasonal_2 Loss: 0.0270 | 0.0288
Epoch 97/300, seasonal_2 Loss: 0.0270 | 0.0283
Epoch 98/300, seasonal_2 Loss: 0.0268 | 0.0290
Epoch 99/300, seasonal_2 Loss: 0.0267 | 0.0282
Epoch 100/300, seasonal_2 Loss: 0.0264 | 0.0285
Epoch 101/300, seasonal_2 Loss: 0.0262 | 0.0278
Epoch 102/300, seasonal_2 Loss: 0.0258 | 0.0277
Epoch 103/300, seasonal_2 Loss: 0.0256 | 0.0273
Epoch 104/300, seasonal_2 Loss: 0.0255 | 0.0274
Epoch 105/300, seasonal_2 Loss: 0.0256 | 0.0275
Epoch 106/300, seasonal_2 Loss: 0.0258 | 0.0289
Epoch 107/300, seasonal_2 Loss: 0.0258 | 0.0270
Epoch 108/300, seasonal_2 Loss: 0.0252 | 0.0274
Epoch 109/300, seasonal_2 Loss: 0.0251 | 0.0271
Epoch 110/300, seasonal_2 Loss: 0.0249 | 0.0270
Epoch 111/300, seasonal_2 Loss: 0.0248 | 0.0271
Epoch 112/300, seasonal_2 Loss: 0.0248 | 0.0274
Epoch 113/300, seasonal_2 Loss: 0.0249 | 0.0280
Epoch 114/300, seasonal_2 Loss: 0.0248 | 0.0275
Epoch 115/300, seasonal_2 Loss: 0.0244 | 0.0271
Epoch 116/300, seasonal_2 Loss: 0.0241 | 0.0267
Epoch 117/300, seasonal_2 Loss: 0.0239 | 0.0264
Epoch 118/300, seasonal_2 Loss: 0.0238 | 0.0263
Epoch 119/300, seasonal_2 Loss: 0.0237 | 0.0263
Epoch 120/300, seasonal_2 Loss: 0.0237 | 0.0263
Epoch 121/300, seasonal_2 Loss: 0.0236 | 0.0264
Epoch 122/300, seasonal_2 Loss: 0.0237 | 0.0265
Epoch 123/300, seasonal_2 Loss: 0.0237 | 0.0268
Epoch 124/300, seasonal_2 Loss: 0.0239 | 0.0272
Epoch 125/300, seasonal_2 Loss: 0.0238 | 0.0272
Epoch 126/300, seasonal_2 Loss: 0.0236 | 0.0268
Epoch 127/300, seasonal_2 Loss: 0.0233 | 0.0263
Epoch 128/300, seasonal_2 Loss: 0.0231 | 0.0259
Epoch 129/300, seasonal_2 Loss: 0.0230 | 0.0256
Epoch 130/300, seasonal_2 Loss: 0.0229 | 0.0255
Epoch 131/300, seasonal_2 Loss: 0.0228 | 0.0254
Epoch 132/300, seasonal_2 Loss: 0.0227 | 0.0253
Epoch 133/300, seasonal_2 Loss: 0.0226 | 0.0253
Epoch 134/300, seasonal_2 Loss: 0.0226 | 0.0253
Epoch 135/300, seasonal_2 Loss: 0.0226 | 0.0253
Epoch 136/300, seasonal_2 Loss: 0.0225 | 0.0254
Epoch 137/300, seasonal_2 Loss: 0.0225 | 0.0254
Epoch 138/300, seasonal_2 Loss: 0.0225 | 0.0254
Epoch 139/300, seasonal_2 Loss: 0.0225 | 0.0254
Epoch 140/300, seasonal_2 Loss: 0.0224 | 0.0254
Epoch 141/300, seasonal_2 Loss: 0.0224 | 0.0254
Epoch 142/300, seasonal_2 Loss: 0.0223 | 0.0252
Epoch 143/300, seasonal_2 Loss: 0.0221 | 0.0249
Epoch 144/300, seasonal_2 Loss: 0.0221 | 0.0248
Epoch 145/300, seasonal_2 Loss: 0.0220 | 0.0247
Epoch 146/300, seasonal_2 Loss: 0.0219 | 0.0246
Epoch 147/300, seasonal_2 Loss: 0.0219 | 0.0245
Epoch 148/300, seasonal_2 Loss: 0.0218 | 0.0245
Epoch 149/300, seasonal_2 Loss: 0.0218 | 0.0244
Epoch 150/300, seasonal_2 Loss: 0.0217 | 0.0244
Epoch 151/300, seasonal_2 Loss: 0.0217 | 0.0244
Epoch 152/300, seasonal_2 Loss: 0.0217 | 0.0243
Epoch 153/300, seasonal_2 Loss: 0.0216 | 0.0243
Epoch 154/300, seasonal_2 Loss: 0.0216 | 0.0243
Epoch 155/300, seasonal_2 Loss: 0.0216 | 0.0243
Epoch 156/300, seasonal_2 Loss: 0.0215 | 0.0243
Epoch 157/300, seasonal_2 Loss: 0.0215 | 0.0242
Epoch 158/300, seasonal_2 Loss: 0.0214 | 0.0241
Epoch 159/300, seasonal_2 Loss: 0.0214 | 0.0241
Epoch 160/300, seasonal_2 Loss: 0.0214 | 0.0241
Epoch 161/300, seasonal_2 Loss: 0.0213 | 0.0240
Epoch 162/300, seasonal_2 Loss: 0.0213 | 0.0240
Epoch 163/300, seasonal_2 Loss: 0.0213 | 0.0239
Epoch 164/300, seasonal_2 Loss: 0.0212 | 0.0239
Epoch 165/300, seasonal_2 Loss: 0.0212 | 0.0238
Epoch 166/300, seasonal_2 Loss: 0.0212 | 0.0238
Epoch 167/300, seasonal_2 Loss: 0.0211 | 0.0238
Epoch 168/300, seasonal_2 Loss: 0.0211 | 0.0238
Epoch 169/300, seasonal_2 Loss: 0.0211 | 0.0237
Epoch 170/300, seasonal_2 Loss: 0.0211 | 0.0237
Epoch 171/300, seasonal_2 Loss: 0.0210 | 0.0237
Epoch 172/300, seasonal_2 Loss: 0.0210 | 0.0237
Epoch 173/300, seasonal_2 Loss: 0.0210 | 0.0236
Epoch 174/300, seasonal_2 Loss: 0.0210 | 0.0236
Epoch 175/300, seasonal_2 Loss: 0.0209 | 0.0236
Epoch 176/300, seasonal_2 Loss: 0.0209 | 0.0236
Epoch 177/300, seasonal_2 Loss: 0.0209 | 0.0235
Epoch 178/300, seasonal_2 Loss: 0.0209 | 0.0235
Epoch 179/300, seasonal_2 Loss: 0.0208 | 0.0235
Epoch 180/300, seasonal_2 Loss: 0.0208 | 0.0235
Epoch 181/300, seasonal_2 Loss: 0.0208 | 0.0234
Epoch 182/300, seasonal_2 Loss: 0.0208 | 0.0234
Epoch 183/300, seasonal_2 Loss: 0.0207 | 0.0234
Epoch 184/300, seasonal_2 Loss: 0.0207 | 0.0234
Epoch 185/300, seasonal_2 Loss: 0.0207 | 0.0233
Epoch 186/300, seasonal_2 Loss: 0.0207 | 0.0233
Epoch 187/300, seasonal_2 Loss: 0.0207 | 0.0233
Epoch 188/300, seasonal_2 Loss: 0.0207 | 0.0233
Epoch 189/300, seasonal_2 Loss: 0.0206 | 0.0233
Epoch 190/300, seasonal_2 Loss: 0.0206 | 0.0233
Epoch 191/300, seasonal_2 Loss: 0.0206 | 0.0232
Epoch 192/300, seasonal_2 Loss: 0.0206 | 0.0232
Epoch 193/300, seasonal_2 Loss: 0.0206 | 0.0232
Epoch 194/300, seasonal_2 Loss: 0.0206 | 0.0232
Epoch 195/300, seasonal_2 Loss: 0.0205 | 0.0232
Epoch 196/300, seasonal_2 Loss: 0.0205 | 0.0232
Epoch 197/300, seasonal_2 Loss: 0.0205 | 0.0231
Epoch 198/300, seasonal_2 Loss: 0.0205 | 0.0231
Epoch 199/300, seasonal_2 Loss: 0.0205 | 0.0231
Epoch 200/300, seasonal_2 Loss: 0.0205 | 0.0231
Epoch 201/300, seasonal_2 Loss: 0.0204 | 0.0231
Epoch 202/300, seasonal_2 Loss: 0.0204 | 0.0231
Epoch 203/300, seasonal_2 Loss: 0.0204 | 0.0231
Epoch 204/300, seasonal_2 Loss: 0.0204 | 0.0230
Epoch 205/300, seasonal_2 Loss: 0.0204 | 0.0230
Epoch 206/300, seasonal_2 Loss: 0.0204 | 0.0230
Epoch 207/300, seasonal_2 Loss: 0.0204 | 0.0230
Epoch 208/300, seasonal_2 Loss: 0.0204 | 0.0230
Epoch 209/300, seasonal_2 Loss: 0.0203 | 0.0230
Epoch 210/300, seasonal_2 Loss: 0.0203 | 0.0230
Epoch 211/300, seasonal_2 Loss: 0.0203 | 0.0230
Epoch 212/300, seasonal_2 Loss: 0.0203 | 0.0229
Epoch 213/300, seasonal_2 Loss: 0.0203 | 0.0229
Epoch 214/300, seasonal_2 Loss: 0.0203 | 0.0229
Epoch 215/300, seasonal_2 Loss: 0.0203 | 0.0229
Epoch 216/300, seasonal_2 Loss: 0.0203 | 0.0229
Epoch 217/300, seasonal_2 Loss: 0.0203 | 0.0229
Epoch 218/300, seasonal_2 Loss: 0.0202 | 0.0229
Epoch 219/300, seasonal_2 Loss: 0.0202 | 0.0229
Epoch 220/300, seasonal_2 Loss: 0.0202 | 0.0229
Epoch 221/300, seasonal_2 Loss: 0.0202 | 0.0229
Epoch 222/300, seasonal_2 Loss: 0.0202 | 0.0229
Epoch 223/300, seasonal_2 Loss: 0.0202 | 0.0228
Epoch 224/300, seasonal_2 Loss: 0.0202 | 0.0228
Epoch 225/300, seasonal_2 Loss: 0.0202 | 0.0228
Epoch 226/300, seasonal_2 Loss: 0.0202 | 0.0228
Epoch 227/300, seasonal_2 Loss: 0.0202 | 0.0228
Epoch 228/300, seasonal_2 Loss: 0.0202 | 0.0228
Epoch 229/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 230/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 231/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 232/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 233/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 234/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 235/300, seasonal_2 Loss: 0.0201 | 0.0228
Epoch 236/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 237/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 238/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 239/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 240/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 241/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 242/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 243/300, seasonal_2 Loss: 0.0201 | 0.0227
Epoch 244/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 245/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 246/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 247/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 248/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 249/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 250/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 251/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 252/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 253/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 254/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 255/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 256/300, seasonal_2 Loss: 0.0200 | 0.0227
Epoch 257/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 258/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 259/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 260/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 261/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 262/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 263/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 264/300, seasonal_2 Loss: 0.0200 | 0.0226
Epoch 265/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 266/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 267/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 268/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 269/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 270/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 271/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 272/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 273/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 274/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 275/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 276/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 277/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 278/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 279/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 280/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 281/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 282/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 283/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 284/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 285/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 286/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 287/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 288/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 289/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 290/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 291/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 292/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 293/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 294/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 295/300, seasonal_2 Loss: 0.0199 | 0.0226
Epoch 296/300, seasonal_2 Loss: 0.0199 | 0.0225
Epoch 297/300, seasonal_2 Loss: 0.0199 | 0.0225
Epoch 298/300, seasonal_2 Loss: 0.0199 | 0.0225
Epoch 299/300, seasonal_2 Loss: 0.0199 | 0.0225
Epoch 300/300, seasonal_2 Loss: 0.0199 | 0.0225
Training seasonal_3 component with params: {'observation_period_num': 29, 'train_rates': 0.7327001819674946, 'learning_rate': 0.0009020099926459305, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7617049561833054}
Epoch 1/300, seasonal_3 Loss: 0.4333 | 0.4508
Epoch 2/300, seasonal_3 Loss: 0.2553 | 0.2610
Epoch 3/300, seasonal_3 Loss: 0.2959 | 0.2545
Epoch 4/300, seasonal_3 Loss: 0.2084 | 0.2573
Epoch 5/300, seasonal_3 Loss: 0.2000 | 0.2340
Epoch 6/300, seasonal_3 Loss: 0.1856 | 0.1812
Epoch 7/300, seasonal_3 Loss: 0.1421 | 0.3146
Epoch 8/300, seasonal_3 Loss: 0.1445 | 0.3160
Epoch 9/300, seasonal_3 Loss: 0.1683 | 0.6253
Epoch 10/300, seasonal_3 Loss: 0.1981 | 0.3090
Epoch 11/300, seasonal_3 Loss: 0.2206 | 0.1738
Epoch 12/300, seasonal_3 Loss: 0.1341 | 0.1019
Epoch 13/300, seasonal_3 Loss: 0.0954 | 0.0983
Epoch 14/300, seasonal_3 Loss: 0.0862 | 0.0649
Epoch 15/300, seasonal_3 Loss: 0.0728 | 0.0707
Epoch 16/300, seasonal_3 Loss: 0.0650 | 0.0782
Epoch 17/300, seasonal_3 Loss: 0.0577 | 0.0746
Epoch 18/300, seasonal_3 Loss: 0.0547 | 0.0610
Epoch 19/300, seasonal_3 Loss: 0.0562 | 0.0733
Epoch 20/300, seasonal_3 Loss: 0.0552 | 0.0698
Epoch 21/300, seasonal_3 Loss: 0.0600 | 0.0661
Epoch 22/300, seasonal_3 Loss: 0.0610 | 0.0535
Epoch 23/300, seasonal_3 Loss: 0.0556 | 0.0578
Epoch 24/300, seasonal_3 Loss: 0.0485 | 0.0465
Epoch 25/300, seasonal_3 Loss: 0.0488 | 0.0419
Epoch 26/300, seasonal_3 Loss: 0.0444 | 0.0430
Epoch 27/300, seasonal_3 Loss: 0.0410 | 0.0405
Epoch 28/300, seasonal_3 Loss: 0.0417 | 0.0406
Epoch 29/300, seasonal_3 Loss: 0.0431 | 0.0498
Epoch 30/300, seasonal_3 Loss: 0.0429 | 0.0396
Epoch 31/300, seasonal_3 Loss: 0.0391 | 0.0417
Epoch 32/300, seasonal_3 Loss: 0.0365 | 0.0373
Epoch 33/300, seasonal_3 Loss: 0.0357 | 0.0366
Epoch 34/300, seasonal_3 Loss: 0.0350 | 0.0364
Epoch 35/300, seasonal_3 Loss: 0.0343 | 0.0364
Epoch 36/300, seasonal_3 Loss: 0.0339 | 0.0361
Epoch 37/300, seasonal_3 Loss: 0.0337 | 0.0375
Epoch 38/300, seasonal_3 Loss: 0.0332 | 0.0359
Epoch 39/300, seasonal_3 Loss: 0.0328 | 0.0365
Epoch 40/300, seasonal_3 Loss: 0.0323 | 0.0356
Epoch 41/300, seasonal_3 Loss: 0.0320 | 0.0365
Epoch 42/300, seasonal_3 Loss: 0.0317 | 0.0358
Epoch 43/300, seasonal_3 Loss: 0.0317 | 0.0383
Epoch 44/300, seasonal_3 Loss: 0.0317 | 0.0378
Epoch 45/300, seasonal_3 Loss: 0.0316 | 0.0396
Epoch 46/300, seasonal_3 Loss: 0.0311 | 0.0384
Epoch 47/300, seasonal_3 Loss: 0.0309 | 0.0390
Epoch 48/300, seasonal_3 Loss: 0.0306 | 0.0384
Epoch 49/300, seasonal_3 Loss: 0.0306 | 0.0410
Epoch 50/300, seasonal_3 Loss: 0.0304 | 0.0389
Epoch 51/300, seasonal_3 Loss: 0.0307 | 0.0429
Epoch 52/300, seasonal_3 Loss: 0.0308 | 0.0455
Epoch 53/300, seasonal_3 Loss: 0.0317 | 0.0499
Epoch 54/300, seasonal_3 Loss: 0.0323 | 0.0567
Epoch 55/300, seasonal_3 Loss: 0.0352 | 0.0624
Epoch 56/300, seasonal_3 Loss: 0.0379 | 0.0771
Epoch 57/300, seasonal_3 Loss: 0.0415 | 0.0483
Epoch 58/300, seasonal_3 Loss: 0.0417 | 0.0758
Epoch 59/300, seasonal_3 Loss: 0.0655 | 0.0509
Epoch 60/300, seasonal_3 Loss: 0.0406 | 0.0429
Epoch 61/300, seasonal_3 Loss: 0.0340 | 0.0393
Epoch 62/300, seasonal_3 Loss: 0.0301 | 0.0361
Epoch 63/300, seasonal_3 Loss: 0.0300 | 0.0371
Epoch 64/300, seasonal_3 Loss: 0.0295 | 0.0347
Epoch 65/300, seasonal_3 Loss: 0.0308 | 0.0361
Epoch 66/300, seasonal_3 Loss: 0.0304 | 0.0351
Epoch 67/300, seasonal_3 Loss: 0.0323 | 0.0364
Epoch 68/300, seasonal_3 Loss: 0.0308 | 0.0350
Epoch 69/300, seasonal_3 Loss: 0.0322 | 0.0361
Epoch 70/300, seasonal_3 Loss: 0.0306 | 0.0349
Epoch 71/300, seasonal_3 Loss: 0.0305 | 0.0347
Epoch 72/300, seasonal_3 Loss: 0.0282 | 0.0345
Epoch 73/300, seasonal_3 Loss: 0.0275 | 0.0342
Epoch 74/300, seasonal_3 Loss: 0.0268 | 0.0342
Epoch 75/300, seasonal_3 Loss: 0.0265 | 0.0339
Epoch 76/300, seasonal_3 Loss: 0.0261 | 0.0339
Epoch 77/300, seasonal_3 Loss: 0.0260 | 0.0338
Epoch 78/300, seasonal_3 Loss: 0.0258 | 0.0338
Epoch 79/300, seasonal_3 Loss: 0.0257 | 0.0337
Epoch 80/300, seasonal_3 Loss: 0.0256 | 0.0337
Epoch 81/300, seasonal_3 Loss: 0.0255 | 0.0336
Epoch 82/300, seasonal_3 Loss: 0.0255 | 0.0336
Epoch 83/300, seasonal_3 Loss: 0.0254 | 0.0335
Epoch 84/300, seasonal_3 Loss: 0.0253 | 0.0335
Epoch 85/300, seasonal_3 Loss: 0.0252 | 0.0335
Epoch 86/300, seasonal_3 Loss: 0.0251 | 0.0334
Epoch 87/300, seasonal_3 Loss: 0.0251 | 0.0334
Epoch 88/300, seasonal_3 Loss: 0.0250 | 0.0334
Epoch 89/300, seasonal_3 Loss: 0.0250 | 0.0334
Epoch 90/300, seasonal_3 Loss: 0.0249 | 0.0333
Epoch 91/300, seasonal_3 Loss: 0.0248 | 0.0333
Epoch 92/300, seasonal_3 Loss: 0.0248 | 0.0333
Epoch 93/300, seasonal_3 Loss: 0.0247 | 0.0333
Epoch 94/300, seasonal_3 Loss: 0.0247 | 0.0333
Epoch 95/300, seasonal_3 Loss: 0.0246 | 0.0332
Epoch 96/300, seasonal_3 Loss: 0.0246 | 0.0332
Epoch 97/300, seasonal_3 Loss: 0.0245 | 0.0332
Epoch 98/300, seasonal_3 Loss: 0.0245 | 0.0332
Epoch 99/300, seasonal_3 Loss: 0.0244 | 0.0332
Epoch 100/300, seasonal_3 Loss: 0.0244 | 0.0332
Epoch 101/300, seasonal_3 Loss: 0.0244 | 0.0331
Epoch 102/300, seasonal_3 Loss: 0.0243 | 0.0331
Epoch 103/300, seasonal_3 Loss: 0.0243 | 0.0331
Epoch 104/300, seasonal_3 Loss: 0.0242 | 0.0331
Epoch 105/300, seasonal_3 Loss: 0.0242 | 0.0331
Epoch 106/300, seasonal_3 Loss: 0.0242 | 0.0331
Epoch 107/300, seasonal_3 Loss: 0.0241 | 0.0331
Epoch 108/300, seasonal_3 Loss: 0.0241 | 0.0331
Epoch 109/300, seasonal_3 Loss: 0.0241 | 0.0330
Epoch 110/300, seasonal_3 Loss: 0.0241 | 0.0330
Epoch 111/300, seasonal_3 Loss: 0.0240 | 0.0330
Epoch 112/300, seasonal_3 Loss: 0.0240 | 0.0330
Epoch 113/300, seasonal_3 Loss: 0.0240 | 0.0330
Epoch 114/300, seasonal_3 Loss: 0.0240 | 0.0330
Epoch 115/300, seasonal_3 Loss: 0.0239 | 0.0330
Epoch 116/300, seasonal_3 Loss: 0.0239 | 0.0330
Epoch 117/300, seasonal_3 Loss: 0.0239 | 0.0330
Epoch 118/300, seasonal_3 Loss: 0.0239 | 0.0330
Epoch 119/300, seasonal_3 Loss: 0.0238 | 0.0330
Epoch 120/300, seasonal_3 Loss: 0.0238 | 0.0329
Epoch 121/300, seasonal_3 Loss: 0.0238 | 0.0329
Epoch 122/300, seasonal_3 Loss: 0.0238 | 0.0329
Epoch 123/300, seasonal_3 Loss: 0.0238 | 0.0329
Epoch 124/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 125/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 126/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 127/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 128/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 129/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 130/300, seasonal_3 Loss: 0.0237 | 0.0329
Epoch 131/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 132/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 133/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 134/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 135/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 136/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 137/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 138/300, seasonal_3 Loss: 0.0236 | 0.0329
Epoch 139/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 140/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 141/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 142/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 143/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 144/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 145/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 146/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 147/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 148/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 149/300, seasonal_3 Loss: 0.0235 | 0.0328
Epoch 150/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 151/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 152/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 153/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 154/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 155/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 156/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 157/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 158/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 159/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 160/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 161/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 162/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 163/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 164/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 165/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 166/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 167/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 168/300, seasonal_3 Loss: 0.0234 | 0.0328
Epoch 169/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 170/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 171/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 172/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 173/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 174/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 175/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 176/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 177/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 178/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 179/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 180/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 181/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 182/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 183/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 184/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 185/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 186/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 187/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 188/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 189/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 190/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 191/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 192/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 193/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 194/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 195/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 196/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 197/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 198/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 199/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 200/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 201/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 202/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 203/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 204/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 205/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 206/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 207/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 208/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 209/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 210/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 211/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 212/300, seasonal_3 Loss: 0.0233 | 0.0328
Epoch 213/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 214/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 215/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 216/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 217/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 218/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 219/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 220/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 221/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 222/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 223/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 224/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 225/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 226/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 227/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 228/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 229/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 230/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 231/300, seasonal_3 Loss: 0.0233 | 0.0327
Epoch 232/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 233/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 234/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 235/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 236/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 237/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 238/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 239/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 240/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 241/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 242/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 243/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 244/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 245/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 246/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 247/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 248/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 249/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 250/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 251/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 252/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 253/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 254/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 255/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 256/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 257/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 258/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 259/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 260/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 261/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 262/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 263/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 264/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 265/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 266/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 267/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 268/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 269/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 270/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 271/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 272/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 273/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 274/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 275/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 276/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 277/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 278/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 279/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 280/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 281/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 282/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 283/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 284/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 285/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 286/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 287/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 288/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 289/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 290/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 291/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 292/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 293/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 294/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 295/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 296/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 297/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 298/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 299/300, seasonal_3 Loss: 0.0232 | 0.0327
Epoch 300/300, seasonal_3 Loss: 0.0232 | 0.0327
Training resid component with params: {'observation_period_num': 33, 'train_rates': 0.7177765114273134, 'learning_rate': 0.00020934084212874228, 'batch_size': 88, 'step_size': 12, 'gamma': 0.9276999759408661}
Epoch 1/300, resid Loss: 0.4269 | 0.3178
Epoch 2/300, resid Loss: 0.2509 | 0.2687
Epoch 3/300, resid Loss: 0.2406 | 0.2282
Epoch 4/300, resid Loss: 0.1800 | 0.1802
Epoch 5/300, resid Loss: 0.1822 | 0.1791
Epoch 6/300, resid Loss: 0.1987 | 0.2325
Epoch 7/300, resid Loss: 0.2487 | 0.2034
Epoch 8/300, resid Loss: 0.2333 | 0.0818
Epoch 9/300, resid Loss: 0.1198 | 0.0713
Epoch 10/300, resid Loss: 0.0962 | 0.0805
Epoch 11/300, resid Loss: 0.0885 | 0.0867
Epoch 12/300, resid Loss: 0.0832 | 0.0888
Epoch 13/300, resid Loss: 0.0805 | 0.0831
Epoch 14/300, resid Loss: 0.0784 | 0.0807
Epoch 15/300, resid Loss: 0.0747 | 0.0732
Epoch 16/300, resid Loss: 0.0721 | 0.0674
Epoch 17/300, resid Loss: 0.0715 | 0.0668
Epoch 18/300, resid Loss: 0.0708 | 0.0715
Epoch 19/300, resid Loss: 0.0723 | 0.1048
Epoch 20/300, resid Loss: 0.0746 | 0.1331
Epoch 21/300, resid Loss: 0.1077 | 0.1198
Epoch 22/300, resid Loss: 0.0795 | 0.0672
Epoch 23/300, resid Loss: 0.0705 | 0.0733
Epoch 24/300, resid Loss: 0.0635 | 0.0521
Epoch 25/300, resid Loss: 0.0670 | 0.0718
Epoch 26/300, resid Loss: 0.0732 | 0.0563
Epoch 27/300, resid Loss: 0.0772 | 0.0798
Epoch 28/300, resid Loss: 0.0820 | 0.0751
Epoch 29/300, resid Loss: 0.0778 | 0.0767
Epoch 30/300, resid Loss: 0.0680 | 0.0773
Epoch 31/300, resid Loss: 0.0540 | 0.0646
Epoch 32/300, resid Loss: 0.0701 | 0.0752
Epoch 33/300, resid Loss: 0.0687 | 0.0885
Epoch 34/300, resid Loss: 0.0739 | 0.0526
Epoch 35/300, resid Loss: 0.0682 | 0.0943
Epoch 36/300, resid Loss: 0.0619 | 0.0643
Epoch 37/300, resid Loss: 0.0679 | 0.0965
Epoch 38/300, resid Loss: 0.0523 | 0.0717
Epoch 39/300, resid Loss: 0.0563 | 0.0927
Epoch 40/300, resid Loss: 0.0486 | 0.0678
Epoch 41/300, resid Loss: 0.0588 | 0.0981
Epoch 42/300, resid Loss: 0.0504 | 0.0721
Epoch 43/300, resid Loss: 0.0552 | 0.0714
Epoch 44/300, resid Loss: 0.0480 | 0.0704
Epoch 45/300, resid Loss: 0.0555 | 0.0924
Epoch 46/300, resid Loss: 0.0509 | 0.0842
Epoch 47/300, resid Loss: 0.0556 | 0.0846
Epoch 48/300, resid Loss: 0.0504 | 0.0912
Epoch 49/300, resid Loss: 0.0619 | 0.0875
Epoch 50/300, resid Loss: 0.0526 | 0.0897
Epoch 51/300, resid Loss: 0.0634 | 0.0871
Epoch 52/300, resid Loss: 0.0512 | 0.0782
Epoch 53/300, resid Loss: 0.0612 | 0.0850
Epoch 54/300, resid Loss: 0.0501 | 0.0758
Epoch 55/300, resid Loss: 0.0606 | 0.0786
Epoch 56/300, resid Loss: 0.0487 | 0.0646
Epoch 57/300, resid Loss: 0.0582 | 0.0764
Epoch 58/300, resid Loss: 0.0475 | 0.0582
Epoch 59/300, resid Loss: 0.0529 | 0.0721
Epoch 60/300, resid Loss: 0.0463 | 0.0662
Epoch 61/300, resid Loss: 0.0574 | 0.0719
Epoch 62/300, resid Loss: 0.0454 | 0.0484
Epoch 63/300, resid Loss: 0.0463 | 0.0611
Epoch 64/300, resid Loss: 0.0430 | 0.0580
Epoch 65/300, resid Loss: 0.0537 | 0.0713
Epoch 66/300, resid Loss: 0.0439 | 0.0469
Epoch 67/300, resid Loss: 0.0431 | 0.0550
Epoch 68/300, resid Loss: 0.0404 | 0.0497
Epoch 69/300, resid Loss: 0.0475 | 0.0641
Epoch 70/300, resid Loss: 0.0416 | 0.0463
Epoch 71/300, resid Loss: 0.0431 | 0.0566
Epoch 72/300, resid Loss: 0.0401 | 0.0487
Epoch 73/300, resid Loss: 0.0451 | 0.0587
Epoch 74/300, resid Loss: 0.0403 | 0.0450
Epoch 75/300, resid Loss: 0.0426 | 0.0547
Epoch 76/300, resid Loss: 0.0393 | 0.0448
Epoch 77/300, resid Loss: 0.0424 | 0.0556
Epoch 78/300, resid Loss: 0.0391 | 0.0445
Epoch 79/300, resid Loss: 0.0414 | 0.0523
Epoch 80/300, resid Loss: 0.0383 | 0.0428
Epoch 81/300, resid Loss: 0.0400 | 0.0513
Epoch 82/300, resid Loss: 0.0376 | 0.0426
Epoch 83/300, resid Loss: 0.0391 | 0.0512
Epoch 84/300, resid Loss: 0.0369 | 0.0427
Epoch 85/300, resid Loss: 0.0382 | 0.0496
Epoch 86/300, resid Loss: 0.0358 | 0.0415
Epoch 87/300, resid Loss: 0.0367 | 0.0488
Epoch 88/300, resid Loss: 0.0350 | 0.0411
Epoch 89/300, resid Loss: 0.0362 | 0.0488
Epoch 90/300, resid Loss: 0.0350 | 0.0412
Epoch 91/300, resid Loss: 0.0362 | 0.0472
Epoch 92/300, resid Loss: 0.0352 | 0.0407
Epoch 93/300, resid Loss: 0.0364 | 0.0468
Epoch 94/300, resid Loss: 0.0355 | 0.0410
Epoch 95/300, resid Loss: 0.0367 | 0.0472
Epoch 96/300, resid Loss: 0.0355 | 0.0417
Epoch 97/300, resid Loss: 0.0361 | 0.0458
Epoch 98/300, resid Loss: 0.0339 | 0.0408
Epoch 99/300, resid Loss: 0.0341 | 0.0446
Epoch 100/300, resid Loss: 0.0322 | 0.0398
Epoch 101/300, resid Loss: 0.0329 | 0.0443
Epoch 102/300, resid Loss: 0.0315 | 0.0393
Epoch 103/300, resid Loss: 0.0321 | 0.0429
Epoch 104/300, resid Loss: 0.0308 | 0.0385
Epoch 105/300, resid Loss: 0.0312 | 0.0423
Epoch 106/300, resid Loss: 0.0303 | 0.0383
Epoch 107/300, resid Loss: 0.0306 | 0.0422
Epoch 108/300, resid Loss: 0.0299 | 0.0384
Epoch 109/300, resid Loss: 0.0302 | 0.0415
Epoch 110/300, resid Loss: 0.0295 | 0.0382
Epoch 111/300, resid Loss: 0.0297 | 0.0414
Epoch 112/300, resid Loss: 0.0291 | 0.0383
Epoch 113/300, resid Loss: 0.0295 | 0.0417
Epoch 114/300, resid Loss: 0.0290 | 0.0386
Epoch 115/300, resid Loss: 0.0293 | 0.0412
Epoch 116/300, resid Loss: 0.0287 | 0.0383
Epoch 117/300, resid Loss: 0.0291 | 0.0414
Epoch 118/300, resid Loss: 0.0287 | 0.0385
Epoch 119/300, resid Loss: 0.0292 | 0.0419
Epoch 120/300, resid Loss: 0.0288 | 0.0387
Epoch 121/300, resid Loss: 0.0292 | 0.0413
Epoch 122/300, resid Loss: 0.0287 | 0.0383
Epoch 123/300, resid Loss: 0.0291 | 0.0412
Epoch 124/300, resid Loss: 0.0285 | 0.0384
Epoch 125/300, resid Loss: 0.0291 | 0.0415
Epoch 126/300, resid Loss: 0.0284 | 0.0385
Epoch 127/300, resid Loss: 0.0288 | 0.0406
Epoch 128/300, resid Loss: 0.0279 | 0.0381
Epoch 129/300, resid Loss: 0.0282 | 0.0402
Epoch 130/300, resid Loss: 0.0274 | 0.0379
Epoch 131/300, resid Loss: 0.0277 | 0.0400
Epoch 132/300, resid Loss: 0.0269 | 0.0377
Epoch 133/300, resid Loss: 0.0272 | 0.0392
Epoch 134/300, resid Loss: 0.0264 | 0.0372
Epoch 135/300, resid Loss: 0.0267 | 0.0388
Epoch 136/300, resid Loss: 0.0261 | 0.0371
Epoch 137/300, resid Loss: 0.0264 | 0.0387
Epoch 138/300, resid Loss: 0.0260 | 0.0371
Epoch 139/300, resid Loss: 0.0263 | 0.0383
Epoch 140/300, resid Loss: 0.0259 | 0.0371
Epoch 141/300, resid Loss: 0.0259 | 0.0383
Epoch 142/300, resid Loss: 0.0255 | 0.0370
Epoch 143/300, resid Loss: 0.0254 | 0.0382
Epoch 144/300, resid Loss: 0.0250 | 0.0370
Epoch 145/300, resid Loss: 0.0250 | 0.0377
Epoch 146/300, resid Loss: 0.0247 | 0.0367
Epoch 147/300, resid Loss: 0.0246 | 0.0375
Epoch 148/300, resid Loss: 0.0245 | 0.0368
Epoch 149/300, resid Loss: 0.0246 | 0.0375
Epoch 150/300, resid Loss: 0.0247 | 0.0370
Epoch 151/300, resid Loss: 0.0248 | 0.0374
Epoch 152/300, resid Loss: 0.0248 | 0.0372
Epoch 153/300, resid Loss: 0.0249 | 0.0376
Epoch 154/300, resid Loss: 0.0246 | 0.0373
Epoch 155/300, resid Loss: 0.0245 | 0.0376
Epoch 156/300, resid Loss: 0.0241 | 0.0372
Epoch 157/300, resid Loss: 0.0240 | 0.0371
Epoch 158/300, resid Loss: 0.0235 | 0.0366
Epoch 159/300, resid Loss: 0.0236 | 0.0369
Epoch 160/300, resid Loss: 0.0232 | 0.0364
Epoch 161/300, resid Loss: 0.0235 | 0.0369
Epoch 162/300, resid Loss: 0.0232 | 0.0364
Epoch 163/300, resid Loss: 0.0233 | 0.0367
Epoch 164/300, resid Loss: 0.0230 | 0.0363
Epoch 165/300, resid Loss: 0.0231 | 0.0366
Epoch 166/300, resid Loss: 0.0228 | 0.0363
Epoch 167/300, resid Loss: 0.0228 | 0.0366
Epoch 168/300, resid Loss: 0.0226 | 0.0364
Epoch 169/300, resid Loss: 0.0226 | 0.0364
Epoch 170/300, resid Loss: 0.0224 | 0.0362
Epoch 171/300, resid Loss: 0.0224 | 0.0363
Epoch 172/300, resid Loss: 0.0222 | 0.0362
Epoch 173/300, resid Loss: 0.0223 | 0.0363
Epoch 174/300, resid Loss: 0.0221 | 0.0363
Epoch 175/300, resid Loss: 0.0221 | 0.0362
Epoch 176/300, resid Loss: 0.0220 | 0.0362
Epoch 177/300, resid Loss: 0.0220 | 0.0361
Epoch 178/300, resid Loss: 0.0218 | 0.0361
Epoch 179/300, resid Loss: 0.0218 | 0.0361
Epoch 180/300, resid Loss: 0.0216 | 0.0361
Epoch 181/300, resid Loss: 0.0216 | 0.0359
Epoch 182/300, resid Loss: 0.0214 | 0.0360
Epoch 183/300, resid Loss: 0.0215 | 0.0358
Epoch 184/300, resid Loss: 0.0213 | 0.0359
Epoch 185/300, resid Loss: 0.0213 | 0.0358
Epoch 186/300, resid Loss: 0.0211 | 0.0359
Epoch 187/300, resid Loss: 0.0211 | 0.0357
Epoch 188/300, resid Loss: 0.0210 | 0.0358
Epoch 189/300, resid Loss: 0.0210 | 0.0356
Epoch 190/300, resid Loss: 0.0208 | 0.0358
Epoch 191/300, resid Loss: 0.0208 | 0.0356
Epoch 192/300, resid Loss: 0.0207 | 0.0358
Epoch 193/300, resid Loss: 0.0207 | 0.0355
Epoch 194/300, resid Loss: 0.0206 | 0.0357
Epoch 195/300, resid Loss: 0.0205 | 0.0355
Epoch 196/300, resid Loss: 0.0204 | 0.0357
Epoch 197/300, resid Loss: 0.0204 | 0.0355
Epoch 198/300, resid Loss: 0.0203 | 0.0358
Epoch 199/300, resid Loss: 0.0203 | 0.0354
Epoch 200/300, resid Loss: 0.0202 | 0.0357
Epoch 201/300, resid Loss: 0.0202 | 0.0354
Epoch 202/300, resid Loss: 0.0201 | 0.0356
Epoch 203/300, resid Loss: 0.0201 | 0.0354
Epoch 204/300, resid Loss: 0.0200 | 0.0357
Epoch 205/300, resid Loss: 0.0199 | 0.0353
Epoch 206/300, resid Loss: 0.0198 | 0.0356
Epoch 207/300, resid Loss: 0.0198 | 0.0353
Epoch 208/300, resid Loss: 0.0197 | 0.0356
Epoch 209/300, resid Loss: 0.0197 | 0.0353
Epoch 210/300, resid Loss: 0.0196 | 0.0356
Epoch 211/300, resid Loss: 0.0196 | 0.0352
Epoch 212/300, resid Loss: 0.0195 | 0.0355
Epoch 213/300, resid Loss: 0.0195 | 0.0352
Epoch 214/300, resid Loss: 0.0194 | 0.0355
Epoch 215/300, resid Loss: 0.0194 | 0.0352
Epoch 216/300, resid Loss: 0.0194 | 0.0355
Epoch 217/300, resid Loss: 0.0193 | 0.0352
Epoch 218/300, resid Loss: 0.0193 | 0.0355
Epoch 219/300, resid Loss: 0.0192 | 0.0352
Epoch 220/300, resid Loss: 0.0192 | 0.0355
Epoch 221/300, resid Loss: 0.0191 | 0.0352
Epoch 222/300, resid Loss: 0.0191 | 0.0355
Epoch 223/300, resid Loss: 0.0191 | 0.0352
Epoch 224/300, resid Loss: 0.0190 | 0.0354
Epoch 225/300, resid Loss: 0.0190 | 0.0351
Epoch 226/300, resid Loss: 0.0189 | 0.0354
Epoch 227/300, resid Loss: 0.0189 | 0.0351
Epoch 228/300, resid Loss: 0.0188 | 0.0354
Epoch 229/300, resid Loss: 0.0188 | 0.0351
Epoch 230/300, resid Loss: 0.0188 | 0.0354
Epoch 231/300, resid Loss: 0.0187 | 0.0351
Epoch 232/300, resid Loss: 0.0187 | 0.0354
Epoch 233/300, resid Loss: 0.0187 | 0.0351
Epoch 234/300, resid Loss: 0.0186 | 0.0354
Epoch 235/300, resid Loss: 0.0186 | 0.0351
Epoch 236/300, resid Loss: 0.0185 | 0.0354
Epoch 237/300, resid Loss: 0.0185 | 0.0351
Epoch 238/300, resid Loss: 0.0185 | 0.0354
Epoch 239/300, resid Loss: 0.0184 | 0.0351
Epoch 240/300, resid Loss: 0.0184 | 0.0354
Epoch 241/300, resid Loss: 0.0184 | 0.0351
Epoch 242/300, resid Loss: 0.0183 | 0.0353
Epoch 243/300, resid Loss: 0.0183 | 0.0351
Epoch 244/300, resid Loss: 0.0183 | 0.0353
Epoch 245/300, resid Loss: 0.0182 | 0.0351
Epoch 246/300, resid Loss: 0.0182 | 0.0353
Epoch 247/300, resid Loss: 0.0182 | 0.0351
Epoch 248/300, resid Loss: 0.0181 | 0.0353
Epoch 249/300, resid Loss: 0.0181 | 0.0351
Epoch 250/300, resid Loss: 0.0181 | 0.0353
Epoch 251/300, resid Loss: 0.0180 | 0.0351
Epoch 252/300, resid Loss: 0.0180 | 0.0353
Epoch 253/300, resid Loss: 0.0180 | 0.0351
Epoch 254/300, resid Loss: 0.0180 | 0.0353
Epoch 255/300, resid Loss: 0.0179 | 0.0351
Epoch 256/300, resid Loss: 0.0179 | 0.0353
Epoch 257/300, resid Loss: 0.0179 | 0.0351
Epoch 258/300, resid Loss: 0.0178 | 0.0353
Epoch 259/300, resid Loss: 0.0178 | 0.0351
Epoch 260/300, resid Loss: 0.0178 | 0.0353
Epoch 261/300, resid Loss: 0.0178 | 0.0351
Epoch 262/300, resid Loss: 0.0177 | 0.0353
Epoch 263/300, resid Loss: 0.0177 | 0.0351
Epoch 264/300, resid Loss: 0.0177 | 0.0353
Epoch 265/300, resid Loss: 0.0177 | 0.0352
Epoch 266/300, resid Loss: 0.0176 | 0.0353
Epoch 267/300, resid Loss: 0.0176 | 0.0352
Epoch 268/300, resid Loss: 0.0176 | 0.0353
Epoch 269/300, resid Loss: 0.0176 | 0.0352
Epoch 270/300, resid Loss: 0.0175 | 0.0353
Epoch 271/300, resid Loss: 0.0175 | 0.0352
Epoch 272/300, resid Loss: 0.0175 | 0.0353
Epoch 273/300, resid Loss: 0.0175 | 0.0352
Epoch 274/300, resid Loss: 0.0174 | 0.0353
Epoch 275/300, resid Loss: 0.0174 | 0.0352
Epoch 276/300, resid Loss: 0.0174 | 0.0353
Epoch 277/300, resid Loss: 0.0174 | 0.0352
Epoch 278/300, resid Loss: 0.0174 | 0.0353
Epoch 279/300, resid Loss: 0.0173 | 0.0352
Epoch 280/300, resid Loss: 0.0173 | 0.0353
Epoch 281/300, resid Loss: 0.0173 | 0.0352
Epoch 282/300, resid Loss: 0.0173 | 0.0353
Epoch 283/300, resid Loss: 0.0172 | 0.0352
Epoch 284/300, resid Loss: 0.0172 | 0.0353
Epoch 285/300, resid Loss: 0.0172 | 0.0352
Epoch 286/300, resid Loss: 0.0172 | 0.0353
Epoch 287/300, resid Loss: 0.0172 | 0.0352
Epoch 288/300, resid Loss: 0.0172 | 0.0353
Epoch 289/300, resid Loss: 0.0171 | 0.0352
Epoch 290/300, resid Loss: 0.0171 | 0.0353
Epoch 291/300, resid Loss: 0.0171 | 0.0352
Epoch 292/300, resid Loss: 0.0171 | 0.0353
Epoch 293/300, resid Loss: 0.0171 | 0.0352
Epoch 294/300, resid Loss: 0.0170 | 0.0353
Epoch 295/300, resid Loss: 0.0170 | 0.0352
Epoch 296/300, resid Loss: 0.0170 | 0.0353
Epoch 297/300, resid Loss: 0.0170 | 0.0353
Epoch 298/300, resid Loss: 0.0170 | 0.0353
Epoch 299/300, resid Loss: 0.0169 | 0.0353
Epoch 300/300, resid Loss: 0.0169 | 0.0353
Runtime (seconds): 1569.980212688446
0.0004885407036096562
[113.59001]
[12.427416]
[-20.517157]
[15.259626]
[38.54429]
[-17.060715]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 49.65350115695037
RMSE: 7.0465240478515625
MAE: 7.0465240478515625
R-squared: nan
[142.24347]
