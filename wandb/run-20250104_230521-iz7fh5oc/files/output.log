[32m[I 2025-01-04 23:05:22,953][0m A new study created in memory with name: no-name-f50a7c8e-e6e6-4c82-b8fe-a89fa2169181[0m
[32m[I 2025-01-04 23:07:46,558][0m Trial 0 finished with value: 0.11320999835881156 and parameters: {'observation_period_num': 192, 'train_rates': 0.8092907407238272, 'learning_rate': 0.00010547354759047042, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8006395909786508}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:08:37,452][0m Trial 1 finished with value: 0.17175554000051682 and parameters: {'observation_period_num': 211, 'train_rates': 0.8713349063259728, 'learning_rate': 0.000780717196234454, 'batch_size': 94, 'step_size': 12, 'gamma': 0.857212677423511}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:11:13,050][0m Trial 2 finished with value: 0.18368619360580704 and parameters: {'observation_period_num': 217, 'train_rates': 0.6592611480722613, 'learning_rate': 6.69854532769462e-05, 'batch_size': 21, 'step_size': 7, 'gamma': 0.839853137400724}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:11:50,757][0m Trial 3 finished with value: 0.2630793273415039 and parameters: {'observation_period_num': 140, 'train_rates': 0.6878724767601194, 'learning_rate': 0.00018396430652931666, 'batch_size': 127, 'step_size': 10, 'gamma': 0.910166177143462}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:12:23,699][0m Trial 4 finished with value: 0.7611509915711223 and parameters: {'observation_period_num': 154, 'train_rates': 0.6552667228938109, 'learning_rate': 5.218992764927743e-06, 'batch_size': 217, 'step_size': 5, 'gamma': 0.8667372962324391}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:13:03,474][0m Trial 5 finished with value: 0.3482009820362179 and parameters: {'observation_period_num': 112, 'train_rates': 0.6040194293325003, 'learning_rate': 2.650153125860282e-05, 'batch_size': 104, 'step_size': 6, 'gamma': 0.806883378048377}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:13:43,164][0m Trial 6 finished with value: 0.9578946877093543 and parameters: {'observation_period_num': 46, 'train_rates': 0.7836455702530889, 'learning_rate': 1.6318663384132416e-06, 'batch_size': 159, 'step_size': 7, 'gamma': 0.8471953226398432}. Best is trial 0 with value: 0.11320999835881156.[0m
[32m[I 2025-01-04 23:14:37,460][0m Trial 7 finished with value: 0.03656440366498768 and parameters: {'observation_period_num': 45, 'train_rates': 0.8427297667455165, 'learning_rate': 0.000861968156141684, 'batch_size': 104, 'step_size': 4, 'gamma': 0.8450101348997923}. Best is trial 7 with value: 0.03656440366498768.[0m
Early stopping at epoch 70
[32m[I 2025-01-04 23:15:23,327][0m Trial 8 finished with value: 1.7849788421093706 and parameters: {'observation_period_num': 89, 'train_rates': 0.8541090302547403, 'learning_rate': 2.3097341954472674e-06, 'batch_size': 73, 'step_size': 1, 'gamma': 0.8498685876974883}. Best is trial 7 with value: 0.03656440366498768.[0m
[32m[I 2025-01-04 23:16:16,448][0m Trial 9 finished with value: 0.5705119298625102 and parameters: {'observation_period_num': 250, 'train_rates': 0.9200072611690248, 'learning_rate': 2.7961820348602444e-06, 'batch_size': 103, 'step_size': 4, 'gamma': 0.9112506477295684}. Best is trial 7 with value: 0.03656440366498768.[0m
[32m[I 2025-01-04 23:17:01,038][0m Trial 10 finished with value: 0.03036317229270935 and parameters: {'observation_period_num': 9, 'train_rates': 0.9638808962449953, 'learning_rate': 0.0008423355756768626, 'batch_size': 186, 'step_size': 1, 'gamma': 0.9875970635553863}. Best is trial 10 with value: 0.03036317229270935.[0m
[32m[I 2025-01-04 23:17:45,916][0m Trial 11 finished with value: 0.023059671744704247 and parameters: {'observation_period_num': 18, 'train_rates': 0.9893975366968288, 'learning_rate': 0.0009756546436813974, 'batch_size': 181, 'step_size': 1, 'gamma': 0.9893654018557158}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:18:32,080][0m Trial 12 finished with value: 0.034524280577898026 and parameters: {'observation_period_num': 8, 'train_rates': 0.9750848389501404, 'learning_rate': 0.00034356218765894253, 'batch_size': 197, 'step_size': 1, 'gamma': 0.9845118207654442}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:19:14,417][0m Trial 13 finished with value: 0.061841703951358795 and parameters: {'observation_period_num': 7, 'train_rates': 0.9773800522471656, 'learning_rate': 2.715207457732171e-05, 'batch_size': 175, 'step_size': 2, 'gamma': 0.9871156742950223}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:19:55,048][0m Trial 14 finished with value: 0.06779564917087555 and parameters: {'observation_period_num': 58, 'train_rates': 0.938838682033064, 'learning_rate': 0.0003223694882219202, 'batch_size': 252, 'step_size': 3, 'gamma': 0.9495389012822336}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:20:36,116][0m Trial 15 finished with value: 0.06899507623165846 and parameters: {'observation_period_num': 80, 'train_rates': 0.9111730256018487, 'learning_rate': 0.0009957048608442737, 'batch_size': 220, 'step_size': 13, 'gamma': 0.7531366630222444}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:21:12,842][0m Trial 16 finished with value: 0.03955796998351404 and parameters: {'observation_period_num': 31, 'train_rates': 0.7503451715590859, 'learning_rate': 0.000387983821240383, 'batch_size': 162, 'step_size': 15, 'gamma': 0.9491074420309584}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:21:55,008][0m Trial 17 finished with value: 0.4040006995201111 and parameters: {'observation_period_num': 93, 'train_rates': 0.9717224924498442, 'learning_rate': 8.36277955100409e-06, 'batch_size': 250, 'step_size': 3, 'gamma': 0.9511856704870223}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:22:36,071][0m Trial 18 finished with value: 0.22496154068218877 and parameters: {'observation_period_num': 24, 'train_rates': 0.8912414343197361, 'learning_rate': 5.900151949774934e-05, 'batch_size': 191, 'step_size': 1, 'gamma': 0.9055088305331533}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:23:23,924][0m Trial 19 finished with value: 0.14183787873813083 and parameters: {'observation_period_num': 64, 'train_rates': 0.9518429763936745, 'learning_rate': 0.0005039811610246547, 'batch_size': 139, 'step_size': 9, 'gamma': 0.9712426524848585}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:24:08,510][0m Trial 20 finished with value: 0.17706117033958435 and parameters: {'observation_period_num': 124, 'train_rates': 0.9880994235452867, 'learning_rate': 0.00017839180019899262, 'batch_size': 223, 'step_size': 5, 'gamma': 0.9283064893863161}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:24:55,532][0m Trial 21 finished with value: 0.04034726694226265 and parameters: {'observation_period_num': 6, 'train_rates': 0.9401236033175834, 'learning_rate': 0.00025032156796807325, 'batch_size': 195, 'step_size': 1, 'gamma': 0.9895461337004005}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:25:49,533][0m Trial 22 finished with value: 0.0250808484852314 and parameters: {'observation_period_num': 22, 'train_rates': 0.9878838856640145, 'learning_rate': 0.00046034081434881587, 'batch_size': 190, 'step_size': 2, 'gamma': 0.962388919034024}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:26:29,992][0m Trial 23 finished with value: 0.03622634230580713 and parameters: {'observation_period_num': 31, 'train_rates': 0.9043981676348019, 'learning_rate': 0.0005580197103257578, 'batch_size': 143, 'step_size': 3, 'gamma': 0.9627055484386894}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:27:10,158][0m Trial 24 finished with value: 0.10654249787330627 and parameters: {'observation_period_num': 66, 'train_rates': 0.9454644648262888, 'learning_rate': 0.00015402614898466754, 'batch_size': 175, 'step_size': 2, 'gamma': 0.9320176145606722}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:27:47,412][0m Trial 25 finished with value: 0.04558473465435985 and parameters: {'observation_period_num': 27, 'train_rates': 0.8169750643239505, 'learning_rate': 0.0005651259980720079, 'batch_size': 206, 'step_size': 2, 'gamma': 0.8909926104960131}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:28:30,695][0m Trial 26 finished with value: 0.062333621084690094 and parameters: {'observation_period_num': 45, 'train_rates': 0.9877330287877466, 'learning_rate': 0.00010096394724034469, 'batch_size': 178, 'step_size': 4, 'gamma': 0.9652216283285955}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:29:08,210][0m Trial 27 finished with value: 0.11564734073176336 and parameters: {'observation_period_num': 167, 'train_rates': 0.8922817358185409, 'learning_rate': 0.0006250484075198474, 'batch_size': 229, 'step_size': 2, 'gamma': 0.973403653368335}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:29:51,854][0m Trial 28 finished with value: 0.045235633850097656 and parameters: {'observation_period_num': 21, 'train_rates': 0.9486109365907328, 'learning_rate': 0.0002826384550824509, 'batch_size': 154, 'step_size': 5, 'gamma': 0.9311817873158476}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:31:12,995][0m Trial 29 finished with value: 0.2649218478302792 and parameters: {'observation_period_num': 103, 'train_rates': 0.7240860597833219, 'learning_rate': 1.5172998917141642e-05, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8857613581743142}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:31:59,032][0m Trial 30 finished with value: 0.09111807723571588 and parameters: {'observation_period_num': 74, 'train_rates': 0.9230489168508266, 'learning_rate': 0.00010246693128350335, 'batch_size': 124, 'step_size': 9, 'gamma': 0.9437602614150629}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:32:41,874][0m Trial 31 finished with value: 0.03756525740027428 and parameters: {'observation_period_num': 8, 'train_rates': 0.9683629534399346, 'learning_rate': 0.00040146562538341725, 'batch_size': 196, 'step_size': 1, 'gamma': 0.9818641898386108}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:33:23,625][0m Trial 32 finished with value: 0.05369426682591438 and parameters: {'observation_period_num': 45, 'train_rates': 0.9636249864852606, 'learning_rate': 0.0009155215808733433, 'batch_size': 183, 'step_size': 1, 'gamma': 0.9651955919709874}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:34:03,532][0m Trial 33 finished with value: 0.04645765144377947 and parameters: {'observation_period_num': 14, 'train_rates': 0.8776971522867313, 'learning_rate': 0.0002492296595010431, 'batch_size': 207, 'step_size': 2, 'gamma': 0.9872583757580353}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:34:46,743][0m Trial 34 finished with value: 0.10340715944766998 and parameters: {'observation_period_num': 36, 'train_rates': 0.9884492165141336, 'learning_rate': 5.462626354391518e-05, 'batch_size': 238, 'step_size': 1, 'gamma': 0.973858356111642}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:35:25,924][0m Trial 35 finished with value: 0.16050259229065716 and parameters: {'observation_period_num': 180, 'train_rates': 0.9255658459428561, 'learning_rate': 0.0007203236233611394, 'batch_size': 164, 'step_size': 3, 'gamma': 0.9595751322984137}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:36:06,891][0m Trial 36 finished with value: 0.07189057022333145 and parameters: {'observation_period_num': 57, 'train_rates': 0.9667321356688853, 'learning_rate': 0.00048674828810973454, 'batch_size': 205, 'step_size': 12, 'gamma': 0.822205971934358}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:36:50,857][0m Trial 37 finished with value: 0.048207407457624925 and parameters: {'observation_period_num': 18, 'train_rates': 0.8625560842806795, 'learning_rate': 0.0001308650900600743, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7642827323883119}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:37:26,430][0m Trial 38 finished with value: 0.13632116800111033 and parameters: {'observation_period_num': 213, 'train_rates': 0.7954123933997479, 'learning_rate': 0.00022540410453829394, 'batch_size': 149, 'step_size': 4, 'gamma': 0.9776553320232162}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:38:07,156][0m Trial 39 finished with value: 0.04626406034505045 and parameters: {'observation_period_num': 40, 'train_rates': 0.8225194744890659, 'learning_rate': 0.0003648383910782777, 'batch_size': 210, 'step_size': 2, 'gamma': 0.9227258571303443}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:38:50,975][0m Trial 40 finished with value: 0.16236983239650726 and parameters: {'observation_period_num': 132, 'train_rates': 0.9599203643022584, 'learning_rate': 0.000995190269285502, 'batch_size': 188, 'step_size': 6, 'gamma': 0.9571436418254889}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:39:33,654][0m Trial 41 finished with value: 0.04200651197576187 and parameters: {'observation_period_num': 29, 'train_rates': 0.9031893925602685, 'learning_rate': 0.0006163520395625564, 'batch_size': 143, 'step_size': 3, 'gamma': 0.9898344936512569}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:40:21,395][0m Trial 42 finished with value: 0.036790301025761245 and parameters: {'observation_period_num': 16, 'train_rates': 0.928265305814734, 'learning_rate': 0.0004706050865867787, 'batch_size': 115, 'step_size': 1, 'gamma': 0.9390347648374151}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:41:05,782][0m Trial 43 finished with value: 0.03265375271439552 and parameters: {'observation_period_num': 5, 'train_rates': 0.9885084583768209, 'learning_rate': 0.0006683447913584311, 'batch_size': 170, 'step_size': 2, 'gamma': 0.960584342064612}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:41:49,430][0m Trial 44 finished with value: 0.03812682256102562 and parameters: {'observation_period_num': 53, 'train_rates': 0.973961695202342, 'learning_rate': 0.0008259794644713876, 'batch_size': 166, 'step_size': 2, 'gamma': 0.9792208312864418}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:42:21,952][0m Trial 45 finished with value: 0.0740999658067056 and parameters: {'observation_period_num': 15, 'train_rates': 0.6290324781095998, 'learning_rate': 0.00033269674647758407, 'batch_size': 173, 'step_size': 4, 'gamma': 0.9162809968679975}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:43:04,752][0m Trial 46 finished with value: 0.03139825537800789 and parameters: {'observation_period_num': 7, 'train_rates': 0.952803318056342, 'learning_rate': 0.0007336432033222253, 'batch_size': 233, 'step_size': 7, 'gamma': 0.955383896855386}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:43:44,725][0m Trial 47 finished with value: 0.17109306156635284 and parameters: {'observation_period_num': 230, 'train_rates': 0.9413712681622679, 'learning_rate': 0.0006947261479040687, 'batch_size': 237, 'step_size': 7, 'gamma': 0.9404247525583775}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:44:26,598][0m Trial 48 finished with value: 0.0274656075202487 and parameters: {'observation_period_num': 5, 'train_rates': 0.8420332179715445, 'learning_rate': 0.0007383747593119702, 'batch_size': 215, 'step_size': 10, 'gamma': 0.8662987692116346}. Best is trial 11 with value: 0.023059671744704247.[0m
[32m[I 2025-01-04 23:45:04,158][0m Trial 49 finished with value: 0.1511031768538735 and parameters: {'observation_period_num': 38, 'train_rates': 0.8456498768766157, 'learning_rate': 1.8981251712529997e-05, 'batch_size': 239, 'step_size': 11, 'gamma': 0.8665317218643231}. Best is trial 11 with value: 0.023059671744704247.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AAPL_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.5117 | 0.3247
Epoch 2/300, Loss: 0.1971 | 0.1743
Epoch 3/300, Loss: 0.2064 | 0.2502
Epoch 4/300, Loss: 0.2595 | 0.1507
Epoch 5/300, Loss: 0.4319 | 0.3248
Epoch 6/300, Loss: 0.2871 | 0.5044
Epoch 7/300, Loss: 0.1572 | 0.1651
Epoch 8/300, Loss: 0.1213 | 0.1217
Epoch 9/300, Loss: 0.1227 | 0.1129
Epoch 10/300, Loss: 0.1198 | 0.1077
Epoch 11/300, Loss: 0.1168 | 0.1005
Epoch 12/300, Loss: 0.1175 | 0.0852
Epoch 13/300, Loss: 0.0999 | 0.0736
Epoch 14/300, Loss: 0.1267 | 0.0797
Epoch 15/300, Loss: 0.1792 | 0.1040
Epoch 16/300, Loss: 0.1901 | 0.2186
Epoch 17/300, Loss: 0.1157 | 0.1208
Epoch 18/300, Loss: 0.1169 | 0.0928
Epoch 19/300, Loss: 0.0985 | 0.0650
Epoch 20/300, Loss: 0.1091 | 0.0658
Epoch 21/300, Loss: 0.0916 | 0.0679
Epoch 22/300, Loss: 0.0932 | 0.0711
Epoch 23/300, Loss: 0.0859 | 0.0586
Epoch 24/300, Loss: 0.0862 | 0.0646
Epoch 25/300, Loss: 0.0815 | 0.0636
Epoch 26/300, Loss: 0.0810 | 0.0598
Epoch 27/300, Loss: 0.0770 | 0.0539
Epoch 28/300, Loss: 0.0770 | 0.0518
Epoch 29/300, Loss: 0.0742 | 0.0551
Epoch 30/300, Loss: 0.0724 | 0.0522
Epoch 31/300, Loss: 0.0716 | 0.0507
Epoch 32/300, Loss: 0.0715 | 0.0477
Epoch 33/300, Loss: 0.0716 | 0.0440
Epoch 34/300, Loss: 0.0714 | 0.0461
Epoch 35/300, Loss: 0.0700 | 0.0499
Epoch 36/300, Loss: 0.0694 | 0.0508
Epoch 37/300, Loss: 0.0696 | 0.0484
Epoch 38/300, Loss: 0.0687 | 0.0435
Epoch 39/300, Loss: 0.0683 | 0.0414
Epoch 40/300, Loss: 0.0684 | 0.0377
Epoch 41/300, Loss: 0.0680 | 0.0359
Epoch 42/300, Loss: 0.0699 | 0.0349
Epoch 43/300, Loss: 0.0710 | 0.0396
Epoch 44/300, Loss: 0.0699 | 0.0472
Epoch 45/300, Loss: 0.0702 | 0.0498
Epoch 46/300, Loss: 0.0678 | 0.0422
Epoch 47/300, Loss: 0.0658 | 0.0369
Epoch 48/300, Loss: 0.0641 | 0.0338
Epoch 49/300, Loss: 0.0653 | 0.0366
Epoch 50/300, Loss: 0.0648 | 0.0276
Epoch 51/300, Loss: 0.0675 | 0.0445
Epoch 52/300, Loss: 0.0672 | 0.0460
Epoch 53/300, Loss: 0.0679 | 0.0345
Epoch 54/300, Loss: 0.0658 | 0.0338
Epoch 55/300, Loss: 0.0654 | 0.0365
Epoch 56/300, Loss: 0.0650 | 0.0436
Epoch 57/300, Loss: 0.0655 | 0.0329
Epoch 58/300, Loss: 0.0639 | 0.0288
Epoch 59/300, Loss: 0.0662 | 0.0331
Epoch 60/300, Loss: 0.0635 | 0.0275
Epoch 61/300, Loss: 0.0660 | 0.0283
Epoch 62/300, Loss: 0.0680 | 0.0341
Epoch 63/300, Loss: 0.0627 | 0.0305
Epoch 64/300, Loss: 0.0609 | 0.0274
Epoch 65/300, Loss: 0.0597 | 0.0263
Epoch 66/300, Loss: 0.0591 | 0.0268
Epoch 67/300, Loss: 0.0582 | 0.0261
Epoch 68/300, Loss: 0.0580 | 0.0242
Epoch 69/300, Loss: 0.0574 | 0.0230
Epoch 70/300, Loss: 0.0572 | 0.0239
Epoch 71/300, Loss: 0.0569 | 0.0230
Epoch 72/300, Loss: 0.0566 | 0.0219
Epoch 73/300, Loss: 0.0565 | 0.0220
Epoch 74/300, Loss: 0.0571 | 0.0240
Epoch 75/300, Loss: 0.0584 | 0.0253
Epoch 76/300, Loss: 0.0592 | 0.0238
Epoch 77/300, Loss: 0.0581 | 0.0237
Epoch 78/300, Loss: 0.0564 | 0.0228
Epoch 79/300, Loss: 0.0562 | 0.0211
Epoch 80/300, Loss: 0.0559 | 0.0208
Epoch 81/300, Loss: 0.0555 | 0.0206
Epoch 82/300, Loss: 0.0551 | 0.0203
Epoch 83/300, Loss: 0.0550 | 0.0200
Epoch 84/300, Loss: 0.0552 | 0.0202
Epoch 85/300, Loss: 0.0556 | 0.0217
Epoch 86/300, Loss: 0.0557 | 0.0210
Epoch 87/300, Loss: 0.0552 | 0.0201
Epoch 88/300, Loss: 0.0548 | 0.0205
Epoch 89/300, Loss: 0.0549 | 0.0211
Epoch 90/300, Loss: 0.0545 | 0.0201
Epoch 91/300, Loss: 0.0541 | 0.0190
Epoch 92/300, Loss: 0.0540 | 0.0186
Epoch 93/300, Loss: 0.0542 | 0.0194
Epoch 94/300, Loss: 0.0546 | 0.0201
Epoch 95/300, Loss: 0.0543 | 0.0196
Epoch 96/300, Loss: 0.0536 | 0.0192
Epoch 97/300, Loss: 0.0536 | 0.0200
Epoch 98/300, Loss: 0.0534 | 0.0196
Epoch 99/300, Loss: 0.0531 | 0.0185
Epoch 100/300, Loss: 0.0529 | 0.0177
Epoch 101/300, Loss: 0.0531 | 0.0179
Epoch 102/300, Loss: 0.0533 | 0.0185
Epoch 103/300, Loss: 0.0533 | 0.0185
Epoch 104/300, Loss: 0.0527 | 0.0181
Epoch 105/300, Loss: 0.0525 | 0.0185
Epoch 106/300, Loss: 0.0524 | 0.0189
Epoch 107/300, Loss: 0.0523 | 0.0184
Epoch 108/300, Loss: 0.0522 | 0.0174
Epoch 109/300, Loss: 0.0520 | 0.0168
Epoch 110/300, Loss: 0.0520 | 0.0167
Epoch 111/300, Loss: 0.0522 | 0.0171
Epoch 112/300, Loss: 0.0523 | 0.0173
Epoch 113/300, Loss: 0.0520 | 0.0172
Epoch 114/300, Loss: 0.0516 | 0.0175
Epoch 115/300, Loss: 0.0516 | 0.0182
Epoch 116/300, Loss: 0.0517 | 0.0179
Epoch 117/300, Loss: 0.0516 | 0.0171
Epoch 118/300, Loss: 0.0513 | 0.0162
Epoch 119/300, Loss: 0.0512 | 0.0159
Epoch 120/300, Loss: 0.0514 | 0.0163
Epoch 121/300, Loss: 0.0516 | 0.0166
Epoch 122/300, Loss: 0.0514 | 0.0167
Epoch 123/300, Loss: 0.0510 | 0.0172
Epoch 124/300, Loss: 0.0511 | 0.0176
Epoch 125/300, Loss: 0.0512 | 0.0171
Epoch 126/300, Loss: 0.0509 | 0.0161
Epoch 127/300, Loss: 0.0507 | 0.0156
Epoch 128/300, Loss: 0.0508 | 0.0157
Epoch 129/300, Loss: 0.0509 | 0.0160
Epoch 130/300, Loss: 0.0507 | 0.0163
Epoch 131/300, Loss: 0.0505 | 0.0167
Epoch 132/300, Loss: 0.0505 | 0.0168
Epoch 133/300, Loss: 0.0505 | 0.0164
Epoch 134/300, Loss: 0.0503 | 0.0157
Epoch 135/300, Loss: 0.0502 | 0.0153
Epoch 136/300, Loss: 0.0503 | 0.0154
Epoch 137/300, Loss: 0.0503 | 0.0157
Epoch 138/300, Loss: 0.0501 | 0.0160
Epoch 139/300, Loss: 0.0500 | 0.0163
Epoch 140/300, Loss: 0.0500 | 0.0164
Epoch 141/300, Loss: 0.0499 | 0.0160
Epoch 142/300, Loss: 0.0498 | 0.0156
Epoch 143/300, Loss: 0.0498 | 0.0153
Epoch 144/300, Loss: 0.0498 | 0.0154
Epoch 145/300, Loss: 0.0498 | 0.0156
Epoch 146/300, Loss: 0.0497 | 0.0158
Epoch 147/300, Loss: 0.0496 | 0.0160
Epoch 148/300, Loss: 0.0496 | 0.0161
Epoch 149/300, Loss: 0.0495 | 0.0158
Epoch 150/300, Loss: 0.0495 | 0.0156
Epoch 151/300, Loss: 0.0494 | 0.0154
Epoch 152/300, Loss: 0.0494 | 0.0154
Epoch 153/300, Loss: 0.0494 | 0.0156
Epoch 154/300, Loss: 0.0493 | 0.0157
Epoch 155/300, Loss: 0.0493 | 0.0158
Epoch 156/300, Loss: 0.0492 | 0.0158
Epoch 157/300, Loss: 0.0492 | 0.0157
Epoch 158/300, Loss: 0.0491 | 0.0155
Epoch 159/300, Loss: 0.0491 | 0.0155
Epoch 160/300, Loss: 0.0491 | 0.0155
Epoch 161/300, Loss: 0.0490 | 0.0156
Epoch 162/300, Loss: 0.0490 | 0.0157
Epoch 163/300, Loss: 0.0490 | 0.0157
Epoch 164/300, Loss: 0.0489 | 0.0156
Epoch 165/300, Loss: 0.0489 | 0.0156
Epoch 166/300, Loss: 0.0489 | 0.0155
Epoch 167/300, Loss: 0.0488 | 0.0155
Epoch 168/300, Loss: 0.0488 | 0.0156
Epoch 169/300, Loss: 0.0488 | 0.0156
Epoch 170/300, Loss: 0.0487 | 0.0156
Epoch 171/300, Loss: 0.0487 | 0.0156
Epoch 172/300, Loss: 0.0487 | 0.0156
Epoch 173/300, Loss: 0.0487 | 0.0156
Epoch 174/300, Loss: 0.0486 | 0.0156
Epoch 175/300, Loss: 0.0486 | 0.0156
Epoch 176/300, Loss: 0.0486 | 0.0156
Epoch 177/300, Loss: 0.0486 | 0.0156
Epoch 178/300, Loss: 0.0485 | 0.0156
Epoch 179/300, Loss: 0.0485 | 0.0156
Epoch 180/300, Loss: 0.0485 | 0.0156
Epoch 181/300, Loss: 0.0485 | 0.0156
Epoch 182/300, Loss: 0.0484 | 0.0156
Epoch 183/300, Loss: 0.0484 | 0.0155
Epoch 184/300, Loss: 0.0484 | 0.0156
Epoch 185/300, Loss: 0.0484 | 0.0155
Epoch 186/300, Loss: 0.0484 | 0.0156
Epoch 187/300, Loss: 0.0484 | 0.0155
Epoch 188/300, Loss: 0.0484 | 0.0157
Epoch 189/300, Loss: 0.0485 | 0.0155
Epoch 190/300, Loss: 0.0487 | 0.0158
Epoch 191/300, Loss: 0.0492 | 0.0156
Epoch 192/300, Loss: 0.0501 | 0.0159
Epoch 193/300, Loss: 0.0515 | 0.0179
Epoch 194/300, Loss: 0.0529 | 0.0171
Epoch 195/300, Loss: 0.0530 | 0.0248
Epoch 196/300, Loss: 0.0516 | 0.0179
Epoch 197/300, Loss: 0.0498 | 0.0206
Epoch 198/300, Loss: 0.0489 | 0.0162
Epoch 199/300, Loss: 0.0483 | 0.0167
Epoch 200/300, Loss: 0.0482 | 0.0157
Epoch 201/300, Loss: 0.0481 | 0.0159
Epoch 202/300, Loss: 0.0481 | 0.0157
Epoch 203/300, Loss: 0.0481 | 0.0157
Epoch 204/300, Loss: 0.0480 | 0.0157
Epoch 205/300, Loss: 0.0480 | 0.0157
Epoch 206/300, Loss: 0.0480 | 0.0157
Epoch 207/300, Loss: 0.0480 | 0.0156
Epoch 208/300, Loss: 0.0480 | 0.0156
Epoch 209/300, Loss: 0.0480 | 0.0156
Epoch 210/300, Loss: 0.0479 | 0.0156
Epoch 211/300, Loss: 0.0479 | 0.0156
Epoch 212/300, Loss: 0.0479 | 0.0156
Epoch 213/300, Loss: 0.0479 | 0.0156
Epoch 214/300, Loss: 0.0479 | 0.0156
Epoch 215/300, Loss: 0.0479 | 0.0156
Epoch 216/300, Loss: 0.0479 | 0.0156
Epoch 217/300, Loss: 0.0479 | 0.0156
Epoch 218/300, Loss: 0.0478 | 0.0156
Epoch 219/300, Loss: 0.0478 | 0.0156
Epoch 220/300, Loss: 0.0478 | 0.0156
Epoch 221/300, Loss: 0.0478 | 0.0156
Epoch 222/300, Loss: 0.0478 | 0.0156
Epoch 223/300, Loss: 0.0478 | 0.0156
Epoch 224/300, Loss: 0.0478 | 0.0156
Epoch 225/300, Loss: 0.0478 | 0.0156
Epoch 226/300, Loss: 0.0478 | 0.0156
Epoch 227/300, Loss: 0.0477 | 0.0156
Epoch 228/300, Loss: 0.0477 | 0.0156
Epoch 229/300, Loss: 0.0477 | 0.0156
Epoch 230/300, Loss: 0.0477 | 0.0156
Epoch 231/300, Loss: 0.0477 | 0.0156
Epoch 232/300, Loss: 0.0477 | 0.0156
Epoch 233/300, Loss: 0.0477 | 0.0156
Epoch 234/300, Loss: 0.0477 | 0.0156
Epoch 235/300, Loss: 0.0477 | 0.0156
Epoch 236/300, Loss: 0.0477 | 0.0156
Epoch 237/300, Loss: 0.0476 | 0.0156
Epoch 238/300, Loss: 0.0476 | 0.0156
Epoch 239/300, Loss: 0.0476 | 0.0156
Epoch 240/300, Loss: 0.0476 | 0.0155
Epoch 241/300, Loss: 0.0476 | 0.0155
Epoch 242/300, Loss: 0.0476 | 0.0155
Epoch 243/300, Loss: 0.0476 | 0.0155
Epoch 244/300, Loss: 0.0476 | 0.0155
Epoch 245/300, Loss: 0.0476 | 0.0155
Epoch 246/300, Loss: 0.0476 | 0.0155
Epoch 247/300, Loss: 0.0476 | 0.0155
Epoch 248/300, Loss: 0.0476 | 0.0155
Epoch 249/300, Loss: 0.0476 | 0.0155
Epoch 250/300, Loss: 0.0475 | 0.0155
Epoch 251/300, Loss: 0.0475 | 0.0155
Epoch 252/300, Loss: 0.0475 | 0.0155
Epoch 253/300, Loss: 0.0475 | 0.0155
Epoch 254/300, Loss: 0.0475 | 0.0155
Epoch 255/300, Loss: 0.0475 | 0.0155
Epoch 256/300, Loss: 0.0475 | 0.0155
Epoch 257/300, Loss: 0.0475 | 0.0155
Epoch 258/300, Loss: 0.0475 | 0.0155
Epoch 259/300, Loss: 0.0475 | 0.0155
Epoch 260/300, Loss: 0.0475 | 0.0155
Epoch 261/300, Loss: 0.0475 | 0.0155
Epoch 262/300, Loss: 0.0475 | 0.0155
Epoch 263/300, Loss: 0.0475 | 0.0155
Epoch 264/300, Loss: 0.0475 | 0.0155
Epoch 265/300, Loss: 0.0475 | 0.0155
Epoch 266/300, Loss: 0.0475 | 0.0155
Epoch 267/300, Loss: 0.0474 | 0.0155
Epoch 268/300, Loss: 0.0474 | 0.0155
Epoch 269/300, Loss: 0.0474 | 0.0155
Epoch 270/300, Loss: 0.0474 | 0.0155
Epoch 271/300, Loss: 0.0474 | 0.0155
Epoch 272/300, Loss: 0.0474 | 0.0155
Epoch 273/300, Loss: 0.0474 | 0.0155
Epoch 274/300, Loss: 0.0474 | 0.0155
Epoch 275/300, Loss: 0.0474 | 0.0155
Epoch 276/300, Loss: 0.0474 | 0.0155
Epoch 277/300, Loss: 0.0474 | 0.0155
Epoch 278/300, Loss: 0.0474 | 0.0155
Epoch 279/300, Loss: 0.0474 | 0.0155
Epoch 280/300, Loss: 0.0474 | 0.0155
Epoch 281/300, Loss: 0.0474 | 0.0155
Epoch 282/300, Loss: 0.0474 | 0.0155
Epoch 283/300, Loss: 0.0474 | 0.0155
Epoch 284/300, Loss: 0.0474 | 0.0155
Epoch 285/300, Loss: 0.0474 | 0.0155
Epoch 286/300, Loss: 0.0474 | 0.0155
Epoch 287/300, Loss: 0.0474 | 0.0155
Epoch 288/300, Loss: 0.0474 | 0.0155
Epoch 289/300, Loss: 0.0474 | 0.0155
Epoch 290/300, Loss: 0.0473 | 0.0155
Epoch 291/300, Loss: 0.0473 | 0.0155
Epoch 292/300, Loss: 0.0473 | 0.0155
Epoch 293/300, Loss: 0.0473 | 0.0155
Epoch 294/300, Loss: 0.0473 | 0.0155
Epoch 295/300, Loss: 0.0473 | 0.0155
Epoch 296/300, Loss: 0.0473 | 0.0155
Epoch 297/300, Loss: 0.0473 | 0.0155
Epoch 298/300, Loss: 0.0473 | 0.0155
Epoch 299/300, Loss: 0.0473 | 0.0155
Epoch 300/300, Loss: 0.0473 | 0.0155
Runtime (seconds): 129.0353422164917
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 47.88996481243521
RMSE: 6.920257568359375
MAE: 6.920257568359375
R-squared: nan
[232.66974]
