[32m[I 2025-01-06 09:44:38,052][0m A new study created in memory with name: no-name-a2e77f5a-0ab8-4f50-bd4f-af318ec9f417[0m
[32m[I 2025-01-06 09:47:11,218][0m Trial 0 finished with value: 1.6112363856915293 and parameters: {'observation_period_num': 141, 'train_rates': 0.6152170510322529, 'learning_rate': 1.6172876657100076e-05, 'batch_size': 154, 'step_size': 4, 'gamma': 0.8283884467049188}. Best is trial 0 with value: 1.6112363856915293.[0m
[32m[I 2025-01-06 09:52:15,431][0m Trial 1 finished with value: 0.19348740800518857 and parameters: {'observation_period_num': 210, 'train_rates': 0.8831796618241261, 'learning_rate': 0.00017382352757374555, 'batch_size': 138, 'step_size': 15, 'gamma': 0.8703007850854363}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 09:54:28,003][0m Trial 2 finished with value: 0.4053696540949384 and parameters: {'observation_period_num': 38, 'train_rates': 0.7736847012437572, 'learning_rate': 6.470964908561221e-05, 'batch_size': 28, 'step_size': 3, 'gamma': 0.759142814902032}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 09:57:10,013][0m Trial 3 finished with value: 0.5323475612769797 and parameters: {'observation_period_num': 130, 'train_rates': 0.7811749763516251, 'learning_rate': 2.3615473492686835e-05, 'batch_size': 152, 'step_size': 5, 'gamma': 0.946712838647303}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 09:58:06,676][0m Trial 4 finished with value: 1.900113600163372 and parameters: {'observation_period_num': 52, 'train_rates': 0.7192981058367203, 'learning_rate': 1.976077200140655e-06, 'batch_size': 190, 'step_size': 4, 'gamma': 0.8553991722100964}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:04:02,493][0m Trial 5 finished with value: 0.5795533064492914 and parameters: {'observation_period_num': 235, 'train_rates': 0.9053759164494604, 'learning_rate': 1.0204183721511077e-05, 'batch_size': 77, 'step_size': 3, 'gamma': 0.8123574692312063}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:05:26,871][0m Trial 6 finished with value: 0.48004860430955887 and parameters: {'observation_period_num': 71, 'train_rates': 0.7672132490265888, 'learning_rate': 1.7203490218119286e-05, 'batch_size': 154, 'step_size': 11, 'gamma': 0.9702203928477883}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:08:19,580][0m Trial 7 finished with value: 1.056795782805391 and parameters: {'observation_period_num': 79, 'train_rates': 0.7242882351021356, 'learning_rate': 0.00017449850260610586, 'batch_size': 20, 'step_size': 14, 'gamma': 0.9781580023843095}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:13:25,388][0m Trial 8 finished with value: 0.29966482520103455 and parameters: {'observation_period_num': 197, 'train_rates': 0.9792224140987216, 'learning_rate': 0.00013959844368579123, 'batch_size': 199, 'step_size': 1, 'gamma': 0.8834979044959346}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:15:46,735][0m Trial 9 finished with value: 1.2872556749596653 and parameters: {'observation_period_num': 121, 'train_rates': 0.7366957284270268, 'learning_rate': 0.0008411968125444091, 'batch_size': 90, 'step_size': 7, 'gamma': 0.8524273228732914}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:22:02,720][0m Trial 10 finished with value: 0.21963244850759384 and parameters: {'observation_period_num': 251, 'train_rates': 0.8862513568889673, 'learning_rate': 0.0006978929242473224, 'batch_size': 246, 'step_size': 15, 'gamma': 0.908366093165538}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:27:56,430][0m Trial 11 finished with value: 0.3033332166806707 and parameters: {'observation_period_num': 242, 'train_rates': 0.8635787893303636, 'learning_rate': 0.0008029600762024286, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9109178037536885}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:32:33,247][0m Trial 12 finished with value: 0.2282513896298539 and parameters: {'observation_period_num': 194, 'train_rates': 0.8676822951226256, 'learning_rate': 0.0002640341949291519, 'batch_size': 254, 'step_size': 12, 'gamma': 0.9106646321072654}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:37:08,229][0m Trial 13 finished with value: 0.19692382216453552 and parameters: {'observation_period_num': 188, 'train_rates': 0.9506266157361168, 'learning_rate': 0.00039722715687977826, 'batch_size': 206, 'step_size': 10, 'gamma': 0.916265554546414}. Best is trial 1 with value: 0.19348740800518857.[0m
[32m[I 2025-01-06 10:41:28,541][0m Trial 14 finished with value: 0.12062616646289825 and parameters: {'observation_period_num': 176, 'train_rates': 0.9802672117846113, 'learning_rate': 7.216089535296767e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.9405387348212699}. Best is trial 14 with value: 0.12062616646289825.[0m
[32m[I 2025-01-06 10:45:29,323][0m Trial 15 finished with value: 0.20894390894816472 and parameters: {'observation_period_num': 167, 'train_rates': 0.9302657693756515, 'learning_rate': 5.855867272115232e-05, 'batch_size': 106, 'step_size': 8, 'gamma': 0.7865944136384252}. Best is trial 14 with value: 0.12062616646289825.[0m
[32m[I 2025-01-06 10:50:30,520][0m Trial 16 finished with value: 0.30182366331875576 and parameters: {'observation_period_num': 217, 'train_rates': 0.8254442622491494, 'learning_rate': 7.046363936853876e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.9464766253951453}. Best is trial 14 with value: 0.12062616646289825.[0m
[32m[I 2025-01-06 10:51:45,021][0m Trial 17 finished with value: 0.4000733956694603 and parameters: {'observation_period_num': 9, 'train_rates': 0.9730653272474243, 'learning_rate': 3.842292539899444e-06, 'batch_size': 62, 'step_size': 9, 'gamma': 0.8789342710238125}. Best is trial 14 with value: 0.12062616646289825.[0m
[32m[I 2025-01-06 10:55:57,370][0m Trial 18 finished with value: 0.09089686721563339 and parameters: {'observation_period_num': 162, 'train_rates': 0.9895234126971054, 'learning_rate': 0.0001207411143762595, 'batch_size': 53, 'step_size': 13, 'gamma': 0.9420933280177881}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 10:59:15,336][0m Trial 19 finished with value: 1.0954748052064978 and parameters: {'observation_period_num': 164, 'train_rates': 0.6584956752639919, 'learning_rate': 5.809047769918254e-06, 'batch_size': 49, 'step_size': 13, 'gamma': 0.9495205630759204}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:01:27,324][0m Trial 20 finished with value: 0.38425535699154467 and parameters: {'observation_period_num': 105, 'train_rates': 0.835297050313311, 'learning_rate': 3.5235022082102745e-05, 'batch_size': 96, 'step_size': 7, 'gamma': 0.9852361725752291}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:05:20,934][0m Trial 21 finished with value: 0.18047153860775392 and parameters: {'observation_period_num': 164, 'train_rates': 0.9282545886348597, 'learning_rate': 0.00013051021496200295, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9336330350420976}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:09:25,123][0m Trial 22 finished with value: 0.17012409427586725 and parameters: {'observation_period_num': 162, 'train_rates': 0.9391599256197979, 'learning_rate': 0.00012201371673183884, 'batch_size': 55, 'step_size': 11, 'gamma': 0.9341862110135227}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:13:11,784][0m Trial 23 finished with value: 0.10434173484300745 and parameters: {'observation_period_num': 147, 'train_rates': 0.9793785248414218, 'learning_rate': 8.694372525281163e-05, 'batch_size': 54, 'step_size': 10, 'gamma': 0.9640478378659106}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:16:04,940][0m Trial 24 finished with value: 0.10281354188919067 and parameters: {'observation_period_num': 114, 'train_rates': 0.9880085425249998, 'learning_rate': 3.912982230924927e-05, 'batch_size': 42, 'step_size': 9, 'gamma': 0.964263531574807}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:18:43,580][0m Trial 25 finished with value: 0.10309000886403598 and parameters: {'observation_period_num': 101, 'train_rates': 0.9818924575712582, 'learning_rate': 3.747843405252567e-05, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9699526270407609}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:21:13,892][0m Trial 26 finished with value: 0.1609774909079305 and parameters: {'observation_period_num': 100, 'train_rates': 0.9094046195051494, 'learning_rate': 3.3871208571787535e-05, 'batch_size': 34, 'step_size': 7, 'gamma': 0.9635192245549461}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:23:23,280][0m Trial 27 finished with value: 0.19654446840286255 and parameters: {'observation_period_num': 92, 'train_rates': 0.9492641415308345, 'learning_rate': 4.252528460576032e-05, 'batch_size': 73, 'step_size': 8, 'gamma': 0.9779040559981029}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:26:19,454][0m Trial 28 finished with value: 0.16778786480426788 and parameters: {'observation_period_num': 116, 'train_rates': 0.9869490354337026, 'learning_rate': 7.730464327082306e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9893690202315499}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:29:12,180][0m Trial 29 finished with value: 1.369664447542409 and parameters: {'observation_period_num': 136, 'train_rates': 0.6005314796828196, 'learning_rate': 1.057609949782598e-06, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8961194585898055}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:32:08,430][0m Trial 30 finished with value: 1.3955155561584573 and parameters: {'observation_period_num': 145, 'train_rates': 0.6740596442405891, 'learning_rate': 0.00035054137306556255, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9594877254404381}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:35:47,148][0m Trial 31 finished with value: 0.219860117468569 and parameters: {'observation_period_num': 147, 'train_rates': 0.9615607165030043, 'learning_rate': 1.9228232584113552e-05, 'batch_size': 70, 'step_size': 10, 'gamma': 0.923809703109763}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:38:26,444][0m Trial 32 finished with value: 0.18242988526187046 and parameters: {'observation_period_num': 113, 'train_rates': 0.917400369376467, 'learning_rate': 8.417224450774605e-05, 'batch_size': 51, 'step_size': 11, 'gamma': 0.9632301787366646}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:40:28,867][0m Trial 33 finished with value: 0.11512573063373566 and parameters: {'observation_period_num': 83, 'train_rates': 0.9886365821435036, 'learning_rate': 4.411502247217039e-05, 'batch_size': 81, 'step_size': 12, 'gamma': 0.9542863143774328}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:44:32,638][0m Trial 34 finished with value: 0.16036588350931805 and parameters: {'observation_period_num': 60, 'train_rates': 0.9587513139930696, 'learning_rate': 2.3461677271757866e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9284752296095283}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:47:37,369][0m Trial 35 finished with value: 0.1828745887228025 and parameters: {'observation_period_num': 129, 'train_rates': 0.8937534478856441, 'learning_rate': 0.0002206062606832093, 'batch_size': 62, 'step_size': 6, 'gamma': 0.9741137038205049}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:51:38,557][0m Trial 36 finished with value: 0.1895368976182625 and parameters: {'observation_period_num': 152, 'train_rates': 0.956596460162136, 'learning_rate': 1.276446937444791e-05, 'batch_size': 30, 'step_size': 9, 'gamma': 0.9559851075486264}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:52:32,265][0m Trial 37 finished with value: 0.20797654876583502 and parameters: {'observation_period_num': 32, 'train_rates': 0.935318948334651, 'learning_rate': 4.9632313840077795e-05, 'batch_size': 88, 'step_size': 10, 'gamma': 0.7551784671203704}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:55:08,557][0m Trial 38 finished with value: 0.31525050908521046 and parameters: {'observation_period_num': 128, 'train_rates': 0.8370550817040022, 'learning_rate': 8.704501949322404e-05, 'batch_size': 174, 'step_size': 14, 'gamma': 0.8514365838801432}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 11:59:55,164][0m Trial 39 finished with value: 0.1328240931034088 and parameters: {'observation_period_num': 181, 'train_rates': 0.9898297925790767, 'learning_rate': 2.687757286320126e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.9890978656815896}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:04:46,318][0m Trial 40 finished with value: 0.4841226432713475 and parameters: {'observation_period_num': 209, 'train_rates': 0.8030375154543258, 'learning_rate': 0.00012066941902591459, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9725474016989897}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:06:36,023][0m Trial 41 finished with value: 0.17304722866514227 and parameters: {'observation_period_num': 76, 'train_rates': 0.9686251601509694, 'learning_rate': 4.4100971550873194e-05, 'batch_size': 82, 'step_size': 12, 'gamma': 0.9481049472597519}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:08:48,315][0m Trial 42 finished with value: 0.16386321187019348 and parameters: {'observation_period_num': 92, 'train_rates': 0.9892668110088935, 'learning_rate': 1.5049901834621716e-05, 'batch_size': 73, 'step_size': 11, 'gamma': 0.9631564622739797}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:10:22,814][0m Trial 43 finished with value: 0.1710630609800941 and parameters: {'observation_period_num': 64, 'train_rates': 0.9673964545594249, 'learning_rate': 3.162531142321372e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.896700126725523}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:12:54,324][0m Trial 44 finished with value: 0.14862579565781814 and parameters: {'observation_period_num': 86, 'train_rates': 0.9459162700792249, 'learning_rate': 0.00010086289910101135, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8336119216280886}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:14:04,392][0m Trial 45 finished with value: 0.1730986653128639 and parameters: {'observation_period_num': 51, 'train_rates': 0.911865427352736, 'learning_rate': 5.782955624247427e-05, 'batch_size': 100, 'step_size': 11, 'gamma': 0.9403101757874174}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:16:33,009][0m Trial 46 finished with value: 0.191347356578883 and parameters: {'observation_period_num': 104, 'train_rates': 0.970444278887458, 'learning_rate': 2.2743023550813367e-05, 'batch_size': 82, 'step_size': 10, 'gamma': 0.9519601549547267}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:19:23,644][0m Trial 47 finished with value: 0.21729253992146136 and parameters: {'observation_period_num': 120, 'train_rates': 0.874217774937823, 'learning_rate': 0.0002496864712836597, 'batch_size': 38, 'step_size': 1, 'gamma': 0.9725110998356363}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:23:10,136][0m Trial 48 finished with value: 0.14345663083049487 and parameters: {'observation_period_num': 138, 'train_rates': 0.9250626246424583, 'learning_rate': 4.4497199546602804e-05, 'batch_size': 23, 'step_size': 13, 'gamma': 0.9207879723916984}. Best is trial 18 with value: 0.09089686721563339.[0m
[32m[I 2025-01-06 12:25:09,840][0m Trial 49 finished with value: 0.12662088871002197 and parameters: {'observation_period_num': 85, 'train_rates': 0.9892039210578695, 'learning_rate': 0.00019081435591807345, 'batch_size': 112, 'step_size': 9, 'gamma': 0.9406706764772623}. Best is trial 18 with value: 0.09089686721563339.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.7033 | 0.8976
Epoch 2/300, Loss: 0.5631 | 0.6552
Epoch 3/300, Loss: 0.4678 | 0.6549
Epoch 4/300, Loss: 0.3982 | 0.5167
Epoch 5/300, Loss: 0.3325 | 0.4265
Epoch 6/300, Loss: 0.3307 | 0.3976
Epoch 7/300, Loss: 0.3088 | 0.3631
Epoch 8/300, Loss: 0.2725 | 0.2974
Epoch 9/300, Loss: 0.2516 | 0.3025
Epoch 10/300, Loss: 0.2521 | 0.3091
Epoch 11/300, Loss: 0.2217 | 0.2263
Epoch 12/300, Loss: 0.2412 | 0.2811
Epoch 13/300, Loss: 0.2594 | 0.2872
Epoch 14/300, Loss: 0.2734 | 0.2549
Epoch 15/300, Loss: 0.2941 | 0.2294
Epoch 16/300, Loss: 0.3113 | 0.2701
Epoch 17/300, Loss: 0.2446 | 0.2365
Epoch 18/300, Loss: 0.2254 | 0.2056
Epoch 19/300, Loss: 0.2058 | 0.2189
Epoch 20/300, Loss: 0.2056 | 0.1780
Epoch 21/300, Loss: 0.1975 | 0.1647
Epoch 22/300, Loss: 0.2002 | 0.2051
Epoch 23/300, Loss: 0.1805 | 0.1687
Epoch 24/300, Loss: 0.1829 | 0.1728
Epoch 25/300, Loss: 0.1915 | 0.1789
Epoch 26/300, Loss: 0.1872 | 0.1703
Epoch 27/300, Loss: 0.1817 | 0.1561
Epoch 28/300, Loss: 0.1856 | 0.1594
Epoch 29/300, Loss: 0.1774 | 0.1608
Epoch 30/300, Loss: 0.1796 | 0.1390
Epoch 31/300, Loss: 0.1822 | 0.1827
Epoch 32/300, Loss: 0.2004 | 0.1647
Epoch 33/300, Loss: 0.1996 | 0.1571
Epoch 34/300, Loss: 0.2045 | 0.1760
Epoch 35/300, Loss: 0.1979 | 0.1688
Epoch 36/300, Loss: 0.1946 | 0.1498
Epoch 37/300, Loss: 0.1944 | 0.1773
Epoch 38/300, Loss: 0.1707 | 0.1448
Epoch 39/300, Loss: 0.1642 | 0.1454
Epoch 40/300, Loss: 0.1598 | 0.1429
Epoch 41/300, Loss: 0.1573 | 0.1345
Epoch 42/300, Loss: 0.1517 | 0.1301
Epoch 43/300, Loss: 0.1464 | 0.1312
Epoch 44/300, Loss: 0.1401 | 0.1257
Epoch 45/300, Loss: 0.1342 | 0.1225
Epoch 46/300, Loss: 0.1357 | 0.1172
Epoch 47/300, Loss: 0.1356 | 0.1180
Epoch 48/300, Loss: 0.1330 | 0.1178
Epoch 49/300, Loss: 0.1272 | 0.1142
Epoch 50/300, Loss: 0.1269 | 0.1104
Epoch 51/300, Loss: 0.1250 | 0.1153
Epoch 52/300, Loss: 0.1247 | 0.1183
Epoch 53/300, Loss: 0.1274 | 0.1080
Epoch 54/300, Loss: 0.1284 | 0.1128
Epoch 55/300, Loss: 0.1268 | 0.1312
Epoch 56/300, Loss: 0.1339 | 0.1083
Epoch 57/300, Loss: 0.1422 | 0.1286
Epoch 58/300, Loss: 0.1471 | 0.1291
Epoch 59/300, Loss: 0.1539 | 0.1107
Epoch 60/300, Loss: 0.1510 | 0.1154
Epoch 61/300, Loss: 0.1704 | 0.1403
Epoch 62/300, Loss: 0.1571 | 0.1226
Epoch 63/300, Loss: 0.1582 | 0.1313
Epoch 64/300, Loss: 0.1458 | 0.1197
Epoch 65/300, Loss: 0.1387 | 0.1179
Epoch 66/300, Loss: 0.1363 | 0.1115
Epoch 67/300, Loss: 0.1316 | 0.1317
Epoch 68/300, Loss: 0.1229 | 0.1035
Epoch 69/300, Loss: 0.1176 | 0.1064
Epoch 70/300, Loss: 0.1191 | 0.1045
Epoch 71/300, Loss: 0.1142 | 0.0989
Epoch 72/300, Loss: 0.1105 | 0.1014
Epoch 73/300, Loss: 0.1107 | 0.1009
Epoch 74/300, Loss: 0.1082 | 0.0991
Epoch 75/300, Loss: 0.1070 | 0.1016
Epoch 76/300, Loss: 0.1097 | 0.0949
Epoch 77/300, Loss: 0.1079 | 0.0982
Epoch 78/300, Loss: 0.1061 | 0.0990
Epoch 79/300, Loss: 0.1070 | 0.0930
Epoch 80/300, Loss: 0.1089 | 0.0968
Epoch 81/300, Loss: 0.1091 | 0.1108
Epoch 82/300, Loss: 0.1082 | 0.0944
Epoch 83/300, Loss: 0.1123 | 0.0990
Epoch 84/300, Loss: 0.1202 | 0.1201
Epoch 85/300, Loss: 0.1177 | 0.1021
Epoch 86/300, Loss: 0.1241 | 0.0950
Epoch 87/300, Loss: 0.1500 | 0.1539
Epoch 88/300, Loss: 0.1284 | 0.1152
Epoch 89/300, Loss: 0.1235 | 0.1010
Epoch 90/300, Loss: 0.1362 | 0.1049
Epoch 91/300, Loss: 0.1203 | 0.0996
Epoch 92/300, Loss: 0.1081 | 0.1017
Epoch 93/300, Loss: 0.1114 | 0.1019
Epoch 94/300, Loss: 0.1043 | 0.0952
Epoch 95/300, Loss: 0.1004 | 0.0959
Epoch 96/300, Loss: 0.0982 | 0.0915
Epoch 97/300, Loss: 0.0994 | 0.0953
Epoch 98/300, Loss: 0.0967 | 0.0886
Epoch 99/300, Loss: 0.0945 | 0.0921
Epoch 100/300, Loss: 0.0946 | 0.0908
Epoch 101/300, Loss: 0.0932 | 0.0909
Epoch 102/300, Loss: 0.0943 | 0.0919
Epoch 103/300, Loss: 0.0932 | 0.0873
Epoch 104/300, Loss: 0.0926 | 0.0862
Epoch 105/300, Loss: 0.0923 | 0.0894
Epoch 106/300, Loss: 0.0904 | 0.0864
Epoch 107/300, Loss: 0.0889 | 0.0880
Epoch 108/300, Loss: 0.0885 | 0.0868
Epoch 109/300, Loss: 0.0891 | 0.0831
Epoch 110/300, Loss: 0.0876 | 0.0849
Epoch 111/300, Loss: 0.0872 | 0.0862
Epoch 112/300, Loss: 0.0865 | 0.0847
Epoch 113/300, Loss: 0.0850 | 0.0847
Epoch 114/300, Loss: 0.0879 | 0.0920
Epoch 115/300, Loss: 0.0884 | 0.0850
Epoch 116/300, Loss: 0.0907 | 0.0868
Epoch 117/300, Loss: 0.0977 | 0.0911
Epoch 118/300, Loss: 0.0944 | 0.0850
Epoch 119/300, Loss: 0.0917 | 0.0837
Epoch 120/300, Loss: 0.0939 | 0.0956
Epoch 121/300, Loss: 0.0917 | 0.0837
Epoch 122/300, Loss: 0.0912 | 0.0840
Epoch 123/300, Loss: 0.0930 | 0.0898
Epoch 124/300, Loss: 0.0889 | 0.0843
Epoch 125/300, Loss: 0.0872 | 0.0829
Epoch 126/300, Loss: 0.0858 | 0.0912
Epoch 127/300, Loss: 0.0841 | 0.0816
Epoch 128/300, Loss: 0.0831 | 0.0833
Epoch 129/300, Loss: 0.0828 | 0.0845
Epoch 130/300, Loss: 0.0790 | 0.0790
Epoch 131/300, Loss: 0.0792 | 0.0816
Epoch 132/300, Loss: 0.0798 | 0.0822
Epoch 133/300, Loss: 0.0790 | 0.0785
Epoch 134/300, Loss: 0.0781 | 0.0832
Epoch 135/300, Loss: 0.0786 | 0.0794
Epoch 136/300, Loss: 0.0780 | 0.0771
Epoch 137/300, Loss: 0.0768 | 0.0796
Epoch 138/300, Loss: 0.0788 | 0.0808
Epoch 139/300, Loss: 0.0774 | 0.0763
Epoch 140/300, Loss: 0.0775 | 0.0819
Epoch 141/300, Loss: 0.0791 | 0.0788
Epoch 142/300, Loss: 0.0798 | 0.0773
Epoch 143/300, Loss: 0.0798 | 0.0819
Epoch 144/300, Loss: 0.0770 | 0.0781
Epoch 145/300, Loss: 0.0798 | 0.0754
Epoch 146/300, Loss: 0.0765 | 0.0780
Epoch 147/300, Loss: 0.0739 | 0.0762
Epoch 148/300, Loss: 0.0734 | 0.0758
Epoch 149/300, Loss: 0.0717 | 0.0747
Epoch 150/300, Loss: 0.0728 | 0.0772
Epoch 151/300, Loss: 0.0728 | 0.0763
Epoch 152/300, Loss: 0.0733 | 0.0786
Epoch 153/300, Loss: 0.0735 | 0.0771
Epoch 154/300, Loss: 0.0729 | 0.0749
Epoch 155/300, Loss: 0.0733 | 0.0785
Epoch 156/300, Loss: 0.0753 | 0.0765
Epoch 157/300, Loss: 0.0738 | 0.0746
Epoch 158/300, Loss: 0.0713 | 0.0757
Epoch 159/300, Loss: 0.0701 | 0.0767
Epoch 160/300, Loss: 0.0696 | 0.0747
Epoch 161/300, Loss: 0.0702 | 0.0742
Epoch 162/300, Loss: 0.0699 | 0.0788
Epoch 163/300, Loss: 0.0704 | 0.0725
Epoch 164/300, Loss: 0.0705 | 0.0747
Epoch 165/300, Loss: 0.0687 | 0.0741
Epoch 166/300, Loss: 0.0685 | 0.0723
Epoch 167/300, Loss: 0.0687 | 0.0752
Epoch 168/300, Loss: 0.0691 | 0.0764
Epoch 169/300, Loss: 0.0711 | 0.0713
Epoch 170/300, Loss: 0.0698 | 0.0746
Epoch 171/300, Loss: 0.0684 | 0.0755
Epoch 172/300, Loss: 0.0665 | 0.0716
Epoch 173/300, Loss: 0.0662 | 0.0745
Epoch 174/300, Loss: 0.0664 | 0.0734
Epoch 175/300, Loss: 0.0652 | 0.0729
Epoch 176/300, Loss: 0.0659 | 0.0724
Epoch 177/300, Loss: 0.0660 | 0.0736
Epoch 178/300, Loss: 0.0651 | 0.0717
Epoch 179/300, Loss: 0.0649 | 0.0723
Epoch 180/300, Loss: 0.0651 | 0.0734
Epoch 181/300, Loss: 0.0647 | 0.0707
Epoch 182/300, Loss: 0.0654 | 0.0721
Epoch 183/300, Loss: 0.0650 | 0.0746
Epoch 184/300, Loss: 0.0645 | 0.0702
Epoch 185/300, Loss: 0.0643 | 0.0735
Epoch 186/300, Loss: 0.0657 | 0.0714
Epoch 187/300, Loss: 0.0647 | 0.0712
Epoch 188/300, Loss: 0.0635 | 0.0723
Epoch 189/300, Loss: 0.0647 | 0.0708
Epoch 190/300, Loss: 0.0636 | 0.0708
Epoch 191/300, Loss: 0.0626 | 0.0719
Epoch 192/300, Loss: 0.0624 | 0.0715
Epoch 193/300, Loss: 0.0639 | 0.0717
Epoch 194/300, Loss: 0.0639 | 0.0731
Epoch 195/300, Loss: 0.0634 | 0.0726
Epoch 196/300, Loss: 0.0620 | 0.0706
Epoch 197/300, Loss: 0.0606 | 0.0724
Epoch 198/300, Loss: 0.0604 | 0.0698
Epoch 199/300, Loss: 0.0604 | 0.0687
Epoch 200/300, Loss: 0.0602 | 0.0712
Epoch 201/300, Loss: 0.0605 | 0.0705
Epoch 202/300, Loss: 0.0592 | 0.0700
Epoch 203/300, Loss: 0.0595 | 0.0705
Epoch 204/300, Loss: 0.0600 | 0.0700
Epoch 205/300, Loss: 0.0596 | 0.0696
Epoch 206/300, Loss: 0.0593 | 0.0715
Epoch 207/300, Loss: 0.0592 | 0.0694
Epoch 208/300, Loss: 0.0595 | 0.0705
Epoch 209/300, Loss: 0.0593 | 0.0715
Epoch 210/300, Loss: 0.0582 | 0.0704
Epoch 211/300, Loss: 0.0581 | 0.0707
Epoch 212/300, Loss: 0.0579 | 0.0701
Epoch 213/300, Loss: 0.0576 | 0.0689
Epoch 214/300, Loss: 0.0582 | 0.0714
Epoch 215/300, Loss: 0.0588 | 0.0712
Epoch 216/300, Loss: 0.0586 | 0.0694
Epoch 217/300, Loss: 0.0576 | 0.0705
Epoch 218/300, Loss: 0.0583 | 0.0708
Epoch 219/300, Loss: 0.0589 | 0.0702
Epoch 220/300, Loss: 0.0569 | 0.0715
Epoch 221/300, Loss: 0.0565 | 0.0719
Epoch 222/300, Loss: 0.0566 | 0.0690
Epoch 223/300, Loss: 0.0572 | 0.0714
Epoch 224/300, Loss: 0.0571 | 0.0691
Epoch 225/300, Loss: 0.0570 | 0.0692
Epoch 226/300, Loss: 0.0560 | 0.0703
Epoch 227/300, Loss: 0.0562 | 0.0691
Epoch 228/300, Loss: 0.0579 | 0.0692
Epoch 229/300, Loss: 0.0579 | 0.0705
Epoch 230/300, Loss: 0.0560 | 0.0681
Epoch 231/300, Loss: 0.0560 | 0.0689
Epoch 232/300, Loss: 0.0551 | 0.0683
Epoch 233/300, Loss: 0.0553 | 0.0698
Epoch 234/300, Loss: 0.0557 | 0.0700
Epoch 235/300, Loss: 0.0549 | 0.0687
Epoch 236/300, Loss: 0.0549 | 0.0698
Epoch 237/300, Loss: 0.0544 | 0.0687
Epoch 238/300, Loss: 0.0544 | 0.0699
Epoch 239/300, Loss: 0.0533 | 0.0704
Epoch 240/300, Loss: 0.0539 | 0.0690
Epoch 241/300, Loss: 0.0535 | 0.0684
Epoch 242/300, Loss: 0.0525 | 0.0690
Epoch 243/300, Loss: 0.0531 | 0.0701
Epoch 244/300, Loss: 0.0530 | 0.0683
Epoch 245/300, Loss: 0.0532 | 0.0702
Epoch 246/300, Loss: 0.0543 | 0.0673
Epoch 247/300, Loss: 0.0537 | 0.0684
Epoch 248/300, Loss: 0.0533 | 0.0683
Epoch 249/300, Loss: 0.0530 | 0.0675
Epoch 250/300, Loss: 0.0528 | 0.0700
Epoch 251/300, Loss: 0.0525 | 0.0677
Epoch 252/300, Loss: 0.0526 | 0.0683
Epoch 253/300, Loss: 0.0528 | 0.0671
Epoch 254/300, Loss: 0.0525 | 0.0668
Epoch 255/300, Loss: 0.0513 | 0.0676
Epoch 256/300, Loss: 0.0529 | 0.0676
Epoch 257/300, Loss: 0.0521 | 0.0687
Epoch 258/300, Loss: 0.0514 | 0.0669
Epoch 259/300, Loss: 0.0518 | 0.0674
Epoch 260/300, Loss: 0.0516 | 0.0677
Epoch 261/300, Loss: 0.0517 | 0.0669
Epoch 262/300, Loss: 0.0514 | 0.0685
Epoch 263/300, Loss: 0.0513 | 0.0682
Epoch 264/300, Loss: 0.0513 | 0.0677
Epoch 265/300, Loss: 0.0513 | 0.0679
Epoch 266/300, Loss: 0.0527 | 0.0682
Epoch 267/300, Loss: 0.0511 | 0.0687
Epoch 268/300, Loss: 0.0518 | 0.0688
Epoch 269/300, Loss: 0.0511 | 0.0688
Epoch 270/300, Loss: 0.0503 | 0.0703
Epoch 271/300, Loss: 0.0508 | 0.0689
Epoch 272/300, Loss: 0.0514 | 0.0679
Epoch 273/300, Loss: 0.0500 | 0.0687
Epoch 274/300, Loss: 0.0503 | 0.0699
Epoch 275/300, Loss: 0.0502 | 0.0688
Epoch 276/300, Loss: 0.0498 | 0.0684
Epoch 277/300, Loss: 0.0500 | 0.0687
Epoch 278/300, Loss: 0.0499 | 0.0681
Epoch 279/300, Loss: 0.0496 | 0.0675
Epoch 280/300, Loss: 0.0493 | 0.0683
Epoch 281/300, Loss: 0.0493 | 0.0668
Epoch 282/300, Loss: 0.0495 | 0.0691
Epoch 283/300, Loss: 0.0493 | 0.0680
Epoch 284/300, Loss: 0.0492 | 0.0675
Epoch 285/300, Loss: 0.0487 | 0.0689
Epoch 286/300, Loss: 0.0493 | 0.0674
Epoch 287/300, Loss: 0.0494 | 0.0674
Epoch 288/300, Loss: 0.0487 | 0.0678
Epoch 289/300, Loss: 0.0497 | 0.0684
Epoch 290/300, Loss: 0.0503 | 0.0683
Epoch 291/300, Loss: 0.0491 | 0.0684
Epoch 292/300, Loss: 0.0490 | 0.0675
Epoch 293/300, Loss: 0.0498 | 0.0673
Epoch 294/300, Loss: 0.0486 | 0.0684
Epoch 295/300, Loss: 0.0481 | 0.0676
Epoch 296/300, Loss: 0.0495 | 0.0679
Epoch 297/300, Loss: 0.0490 | 0.0675
Epoch 298/300, Loss: 0.0480 | 0.0675
Epoch 299/300, Loss: 0.0491 | 0.0683
Epoch 300/300, Loss: 0.0492 | 0.0675
Runtime (seconds): 745.3246355056763
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 353.384465072304
RMSE: 18.79852294921875
MAE: 18.79852294921875
R-squared: nan
[220.79147]
