ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-30 17:42:27,919][0m A new study created in memory with name: no-name-c8ae4c2f-70b4-434d-b020-0beea2524b21[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-30 17:42:52,811][0m Trial 0 finished with value: 0.15312033004997191 and parameters: {'observation_period_num': 129, 'train_rates': 0.8714265099019135, 'learning_rate': 0.0007200141723432057, 'batch_size': 246, 'step_size': 10, 'gamma': 0.7570861746061649}. Best is trial 0 with value: 0.15312033004997191.[0m
[32m[I 2025-01-30 17:43:31,790][0m Trial 1 finished with value: 0.4159864155746953 and parameters: {'observation_period_num': 221, 'train_rates': 0.6255883315723987, 'learning_rate': 6.233226312239927e-05, 'batch_size': 109, 'step_size': 1, 'gamma': 0.9183664056286716}. Best is trial 0 with value: 0.15312033004997191.[0m
[32m[I 2025-01-30 17:43:57,097][0m Trial 2 finished with value: 0.1374176392952601 and parameters: {'observation_period_num': 160, 'train_rates': 0.7798967804435721, 'learning_rate': 0.00044848149379748877, 'batch_size': 205, 'step_size': 1, 'gamma': 0.9672982012422864}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:44:17,426][0m Trial 3 finished with value: 0.22534212554720315 and parameters: {'observation_period_num': 205, 'train_rates': 0.7759005765232088, 'learning_rate': 0.0005639642946231153, 'batch_size': 243, 'step_size': 3, 'gamma': 0.7556700026082978}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:44:38,984][0m Trial 4 finished with value: 0.28151410347956385 and parameters: {'observation_period_num': 251, 'train_rates': 0.6539216215597237, 'learning_rate': 3.648071454309133e-05, 'batch_size': 205, 'step_size': 15, 'gamma': 0.920212861629929}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:45:32,096][0m Trial 5 finished with value: 0.17457517610246123 and parameters: {'observation_period_num': 174, 'train_rates': 0.892395766652469, 'learning_rate': 0.0004616961571886913, 'batch_size': 102, 'step_size': 9, 'gamma': 0.8056447222994747}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:46:10,148][0m Trial 6 finished with value: 0.3771359599553622 and parameters: {'observation_period_num': 215, 'train_rates': 0.9050317648395679, 'learning_rate': 1.4312030884624678e-05, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8387456644714576}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:47:21,963][0m Trial 7 finished with value: 0.3065454246877115 and parameters: {'observation_period_num': 17, 'train_rates': 0.7309489825882933, 'learning_rate': 1.4880875566027651e-06, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8836174148764144}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:47:59,425][0m Trial 8 finished with value: 0.17422604755215024 and parameters: {'observation_period_num': 173, 'train_rates': 0.7019613899516525, 'learning_rate': 7.013711312405149e-05, 'batch_size': 126, 'step_size': 9, 'gamma': 0.8925894157619468}. Best is trial 2 with value: 0.1374176392952601.[0m
Early stopping at epoch 55
[32m[I 2025-01-30 17:48:24,375][0m Trial 9 finished with value: 0.44887989654183125 and parameters: {'observation_period_num': 101, 'train_rates': 0.682159150757925, 'learning_rate': 0.0001860003844184051, 'batch_size': 244, 'step_size': 1, 'gamma': 0.7918377056545522}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:49:13,966][0m Trial 10 finished with value: 0.21833738684654236 and parameters: {'observation_period_num': 72, 'train_rates': 0.9860794739849617, 'learning_rate': 7.325531155227042e-06, 'batch_size': 185, 'step_size': 5, 'gamma': 0.989259891060996}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:49:53,562][0m Trial 11 finished with value: 0.16114585549292856 and parameters: {'observation_period_num': 133, 'train_rates': 0.8384664468323707, 'learning_rate': 0.0007128288805898324, 'batch_size': 187, 'step_size': 13, 'gamma': 0.9605488074545832}. Best is trial 2 with value: 0.1374176392952601.[0m
[32m[I 2025-01-30 17:50:16,335][0m Trial 12 finished with value: 0.1323785533507665 and parameters: {'observation_period_num': 126, 'train_rates': 0.815263496518924, 'learning_rate': 0.000290178659325436, 'batch_size': 212, 'step_size': 12, 'gamma': 0.8436860366970024}. Best is trial 12 with value: 0.1323785533507665.[0m
[32m[I 2025-01-30 17:50:48,232][0m Trial 13 finished with value: 0.11018148389337395 and parameters: {'observation_period_num': 67, 'train_rates': 0.778405374091504, 'learning_rate': 0.00018406163426945227, 'batch_size': 161, 'step_size': 6, 'gamma': 0.8514926164192957}. Best is trial 13 with value: 0.11018148389337395.[0m
[32m[I 2025-01-30 17:55:01,080][0m Trial 14 finished with value: 0.10203192422769088 and parameters: {'observation_period_num': 55, 'train_rates': 0.8122397688359654, 'learning_rate': 0.00014933277777404467, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8407444912275314}. Best is trial 14 with value: 0.10203192422769088.[0m
[32m[I 2025-01-30 17:59:28,075][0m Trial 15 finished with value: 0.09298233542195912 and parameters: {'observation_period_num': 44, 'train_rates': 0.7536820221002674, 'learning_rate': 0.00015365781314797687, 'batch_size': 18, 'step_size': 6, 'gamma': 0.849215517747061}. Best is trial 15 with value: 0.09298233542195912.[0m
[32m[I 2025-01-30 18:01:57,845][0m Trial 16 finished with value: 0.07487678406604677 and parameters: {'observation_period_num': 13, 'train_rates': 0.734865540650036, 'learning_rate': 8.875902666272388e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.8097356459796999}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:06:55,904][0m Trial 17 finished with value: 0.10311932261834607 and parameters: {'observation_period_num': 8, 'train_rates': 0.7387925535718065, 'learning_rate': 1.4053375130039224e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8114035923130503}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:08:08,757][0m Trial 18 finished with value: 0.12381292481360764 and parameters: {'observation_period_num': 40, 'train_rates': 0.6014159443004773, 'learning_rate': 9.680790034582457e-05, 'batch_size': 60, 'step_size': 7, 'gamma': 0.7878209928613502}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:09:48,116][0m Trial 19 finished with value: 0.13037384875640748 and parameters: {'observation_period_num': 31, 'train_rates': 0.7313043128598293, 'learning_rate': 2.9951612000501422e-05, 'batch_size': 49, 'step_size': 3, 'gamma': 0.8674257111063558}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:10:46,679][0m Trial 20 finished with value: 0.6310259157961066 and parameters: {'observation_period_num': 93, 'train_rates': 0.6843765255965729, 'learning_rate': 2.0962046928726794e-06, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8174481283426754}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:16:09,656][0m Trial 21 finished with value: 0.10724376747094234 and parameters: {'observation_period_num': 51, 'train_rates': 0.8343806529522876, 'learning_rate': 0.000147924670569042, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8321952997861959}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:18:16,889][0m Trial 22 finished with value: 0.08190741522355484 and parameters: {'observation_period_num': 5, 'train_rates': 0.7478904511274678, 'learning_rate': 4.064542859099726e-05, 'batch_size': 39, 'step_size': 7, 'gamma': 0.8661018686408581}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:20:09,558][0m Trial 23 finished with value: 0.08911837629259449 and parameters: {'observation_period_num': 8, 'train_rates': 0.7519200840249948, 'learning_rate': 3.5600850912503346e-05, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8654854527592439}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:22:03,063][0m Trial 24 finished with value: 0.08241346386227622 and parameters: {'observation_period_num': 9, 'train_rates': 0.7178258183007131, 'learning_rate': 2.922180509849465e-05, 'batch_size': 43, 'step_size': 8, 'gamma': 0.9089707094647307}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:23:00,951][0m Trial 25 finished with value: 0.16582451611929347 and parameters: {'observation_period_num': 30, 'train_rates': 0.7035054760896424, 'learning_rate': 5.831886802591765e-06, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9180765652825735}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:24:54,808][0m Trial 26 finished with value: 0.0898582635819912 and parameters: {'observation_period_num': 22, 'train_rates': 0.6585532229571415, 'learning_rate': 2.0118471984026057e-05, 'batch_size': 40, 'step_size': 8, 'gamma': 0.9431404113743097}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:27:01,249][0m Trial 27 finished with value: 0.10724971920125566 and parameters: {'observation_period_num': 80, 'train_rates': 0.7122775904682359, 'learning_rate': 5.2066532679539244e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.89860230061453}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:27:54,884][0m Trial 28 finished with value: 0.46680402414221955 and parameters: {'observation_period_num': 5, 'train_rates': 0.6533167505245816, 'learning_rate': 7.574817400241954e-06, 'batch_size': 90, 'step_size': 4, 'gamma': 0.77471811910244}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:29:19,615][0m Trial 29 finished with value: 0.12162400847138312 and parameters: {'observation_period_num': 58, 'train_rates': 0.8614817376495095, 'learning_rate': 9.108868242939494e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.9359406998440017}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:31:42,371][0m Trial 30 finished with value: 0.2621215820574307 and parameters: {'observation_period_num': 101, 'train_rates': 0.7604435868176458, 'learning_rate': 4.256254244833324e-06, 'batch_size': 34, 'step_size': 8, 'gamma': 0.8756205870853816}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:33:20,789][0m Trial 31 finished with value: 0.09473658982307127 and parameters: {'observation_period_num': 5, 'train_rates': 0.7523886689137812, 'learning_rate': 2.6953114511448454e-05, 'batch_size': 51, 'step_size': 7, 'gamma': 0.8625873392883145}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:35:47,418][0m Trial 32 finished with value: 0.09145249566453285 and parameters: {'observation_period_num': 28, 'train_rates': 0.7958260149552803, 'learning_rate': 4.105654905180672e-05, 'batch_size': 35, 'step_size': 7, 'gamma': 0.9094234738349544}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:36:32,998][0m Trial 33 finished with value: 0.18094055962104064 and parameters: {'observation_period_num': 21, 'train_rates': 0.711644703176388, 'learning_rate': 1.577077393783624e-05, 'batch_size': 113, 'step_size': 5, 'gamma': 0.888171899186864}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:37:45,012][0m Trial 34 finished with value: 0.10560594711280027 and parameters: {'observation_period_num': 43, 'train_rates': 0.7923716493207789, 'learning_rate': 6.36343306842503e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.858983803944081}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:39:16,896][0m Trial 35 finished with value: 0.09506151337944004 and parameters: {'observation_period_num': 15, 'train_rates': 0.6826035233753754, 'learning_rate': 2.2222438879644515e-05, 'batch_size': 51, 'step_size': 9, 'gamma': 0.9032864079317245}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:40:11,105][0m Trial 36 finished with value: 0.15319779408829554 and parameters: {'observation_period_num': 34, 'train_rates': 0.7672263410936744, 'learning_rate': 4.430072414428889e-05, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8283777595289772}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:40:45,516][0m Trial 37 finished with value: 0.10172426032106784 and parameters: {'observation_period_num': 17, 'train_rates': 0.7299688288719356, 'learning_rate': 9.323844832154665e-05, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9299779812018047}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:41:56,224][0m Trial 38 finished with value: 0.11862837084952522 and parameters: {'observation_period_num': 58, 'train_rates': 0.6356336059863555, 'learning_rate': 0.00030702549271002225, 'batch_size': 62, 'step_size': 2, 'gamma': 0.8754010750656298}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:44:44,831][0m Trial 39 finished with value: 0.19247615760413267 and parameters: {'observation_period_num': 133, 'train_rates': 0.7907876768488451, 'learning_rate': 1.0780086671270962e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.7685389644802226}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:45:37,024][0m Trial 40 finished with value: 0.3386694421506908 and parameters: {'observation_period_num': 199, 'train_rates': 0.947258761199076, 'learning_rate': 3.3984369394833615e-05, 'batch_size': 111, 'step_size': 15, 'gamma': 0.8005286120998334}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:47:22,641][0m Trial 41 finished with value: 0.10937356125107225 and parameters: {'observation_period_num': 22, 'train_rates': 0.6654707913219535, 'learning_rate': 1.9888095758362143e-05, 'batch_size': 43, 'step_size': 8, 'gamma': 0.9630954840311605}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:48:43,598][0m Trial 42 finished with value: 0.08650434878719393 and parameters: {'observation_period_num': 6, 'train_rates': 0.6331390811413138, 'learning_rate': 2.16822207924099e-05, 'batch_size': 55, 'step_size': 8, 'gamma': 0.9166314990570197}. Best is trial 16 with value: 0.07487678406604677.[0m
[32m[I 2025-01-30 18:50:00,437][0m Trial 43 finished with value: 0.07009830658683418 and parameters: {'observation_period_num': 9, 'train_rates': 0.6203078957538845, 'learning_rate': 6.708154129662126e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9153589454735283}. Best is trial 43 with value: 0.07009830658683418.[0m
[32m[I 2025-01-30 18:51:11,873][0m Trial 44 finished with value: 0.09752721964605252 and parameters: {'observation_period_num': 37, 'train_rates': 0.6097014737597535, 'learning_rate': 6.596386441316514e-05, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9450490082544294}. Best is trial 43 with value: 0.07009830658683418.[0m
[32m[I 2025-01-30 18:53:47,027][0m Trial 45 finished with value: 0.0714205239918912 and parameters: {'observation_period_num': 14, 'train_rates': 0.6250008524511389, 'learning_rate': 0.00010596159149325224, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9169834280891125}. Best is trial 43 with value: 0.07009830658683418.[0m
[32m[I 2025-01-30 18:56:23,005][0m Trial 46 finished with value: 0.1380148970979986 and parameters: {'observation_period_num': 71, 'train_rates': 0.6201879087438161, 'learning_rate': 0.00010977432225854188, 'batch_size': 27, 'step_size': 11, 'gamma': 0.9291170895506949}. Best is trial 43 with value: 0.07009830658683418.[0m
[32m[I 2025-01-30 18:59:00,886][0m Trial 47 finished with value: 0.10590108930130364 and parameters: {'observation_period_num': 47, 'train_rates': 0.6705727737000535, 'learning_rate': 0.00022536916278288887, 'batch_size': 28, 'step_size': 10, 'gamma': 0.9529410751492267}. Best is trial 43 with value: 0.07009830658683418.[0m
[32m[I 2025-01-30 19:01:31,698][0m Trial 48 finished with value: 0.278772237751829 and parameters: {'observation_period_num': 242, 'train_rates': 0.6492609927830212, 'learning_rate': 0.00012122592440567727, 'batch_size': 27, 'step_size': 10, 'gamma': 0.8839847269766105}. Best is trial 43 with value: 0.07009830658683418.[0m
[32m[I 2025-01-30 19:02:33,105][0m Trial 49 finished with value: 0.09092791485347489 and parameters: {'observation_period_num': 16, 'train_rates': 0.6923104727667604, 'learning_rate': 5.458563490128043e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.9754173547841052}. Best is trial 43 with value: 0.07009830658683418.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-30 19:02:33,115][0m A new study created in memory with name: no-name-9eada519-67b0-4842-8e3a-6226e7acfe91[0m
[32m[I 2025-01-30 19:03:13,917][0m Trial 0 finished with value: 0.2982993030963942 and parameters: {'observation_period_num': 211, 'train_rates': 0.7645863340697086, 'learning_rate': 1.647541007230627e-05, 'batch_size': 123, 'step_size': 7, 'gamma': 0.9629444912924414}. Best is trial 0 with value: 0.2982993030963942.[0m
[32m[I 2025-01-30 19:05:36,546][0m Trial 1 finished with value: 0.19288091038455482 and parameters: {'observation_period_num': 180, 'train_rates': 0.7855345389296661, 'learning_rate': 0.0008505012922056434, 'batch_size': 33, 'step_size': 6, 'gamma': 0.9745090336719768}. Best is trial 1 with value: 0.19288091038455482.[0m
[32m[I 2025-01-30 19:05:57,021][0m Trial 2 finished with value: 0.40003500295721967 and parameters: {'observation_period_num': 207, 'train_rates': 0.6314014881416125, 'learning_rate': 1.722219221160386e-05, 'batch_size': 227, 'step_size': 10, 'gamma': 0.9490554663400693}. Best is trial 1 with value: 0.19288091038455482.[0m
[32m[I 2025-01-30 19:07:11,519][0m Trial 3 finished with value: 0.22426939391501816 and parameters: {'observation_period_num': 198, 'train_rates': 0.8382110196954716, 'learning_rate': 9.45637626191464e-06, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9566728154146833}. Best is trial 1 with value: 0.19288091038455482.[0m
[32m[I 2025-01-30 19:10:21,264][0m Trial 4 finished with value: 0.13044236389191255 and parameters: {'observation_period_num': 69, 'train_rates': 0.8005456455900489, 'learning_rate': 0.0005918287811020517, 'batch_size': 26, 'step_size': 11, 'gamma': 0.7781481721266621}. Best is trial 4 with value: 0.13044236389191255.[0m
[32m[I 2025-01-30 19:11:51,418][0m Trial 5 finished with value: 0.41142706751823427 and parameters: {'observation_period_num': 169, 'train_rates': 0.9102956227602734, 'learning_rate': 2.9484583399837775e-06, 'batch_size': 60, 'step_size': 7, 'gamma': 0.919437336879935}. Best is trial 4 with value: 0.13044236389191255.[0m
[32m[I 2025-01-30 19:12:15,395][0m Trial 6 finished with value: 0.9130156415653979 and parameters: {'observation_period_num': 97, 'train_rates': 0.7330866319680466, 'learning_rate': 8.367194769047683e-06, 'batch_size': 228, 'step_size': 4, 'gamma': 0.7975757538621483}. Best is trial 4 with value: 0.13044236389191255.[0m
[32m[I 2025-01-30 19:13:13,423][0m Trial 7 finished with value: 0.4921855369234473 and parameters: {'observation_period_num': 146, 'train_rates': 0.780680371382824, 'learning_rate': 2.4502536147607353e-06, 'batch_size': 85, 'step_size': 13, 'gamma': 0.7781732012617035}. Best is trial 4 with value: 0.13044236389191255.[0m
[32m[I 2025-01-30 19:14:15,412][0m Trial 8 finished with value: 0.15982440875865764 and parameters: {'observation_period_num': 197, 'train_rates': 0.7904722563495088, 'learning_rate': 9.027097727770695e-05, 'batch_size': 80, 'step_size': 10, 'gamma': 0.7865135985836953}. Best is trial 4 with value: 0.13044236389191255.[0m
[32m[I 2025-01-30 19:15:02,919][0m Trial 9 finished with value: 0.12312190476974817 and parameters: {'observation_period_num': 84, 'train_rates': 0.7705391065087132, 'learning_rate': 7.986730234104364e-05, 'batch_size': 106, 'step_size': 12, 'gamma': 0.8154224342240427}. Best is trial 9 with value: 0.12312190476974817.[0m
[32m[I 2025-01-30 19:15:40,408][0m Trial 10 finished with value: 0.3508092761039734 and parameters: {'observation_period_num': 23, 'train_rates': 0.970091668457638, 'learning_rate': 0.0001033915424131888, 'batch_size': 169, 'step_size': 3, 'gamma': 0.8524036129024687}. Best is trial 9 with value: 0.12312190476974817.[0m
[32m[I 2025-01-30 19:19:09,878][0m Trial 11 finished with value: 0.12537259151312438 and parameters: {'observation_period_num': 77, 'train_rates': 0.678487914248718, 'learning_rate': 0.0007841618221803801, 'batch_size': 21, 'step_size': 12, 'gamma': 0.8366429098252205}. Best is trial 9 with value: 0.12312190476974817.[0m
[32m[I 2025-01-30 19:19:43,288][0m Trial 12 finished with value: 0.10417719852638145 and parameters: {'observation_period_num': 73, 'train_rates': 0.6672795163139382, 'learning_rate': 0.00021876856729436395, 'batch_size': 142, 'step_size': 13, 'gamma': 0.8498551327658329}. Best is trial 12 with value: 0.10417719852638145.[0m
[32m[I 2025-01-30 19:20:13,932][0m Trial 13 finished with value: 0.08958963661903649 and parameters: {'observation_period_num': 15, 'train_rates': 0.6948441649076811, 'learning_rate': 0.00014008048764908734, 'batch_size': 157, 'step_size': 15, 'gamma': 0.8892426549371022}. Best is trial 13 with value: 0.08958963661903649.[0m
[32m[I 2025-01-30 19:20:41,469][0m Trial 14 finished with value: 0.08681186562436714 and parameters: {'observation_period_num': 11, 'train_rates': 0.6121641681726221, 'learning_rate': 0.00024424695714912536, 'batch_size': 167, 'step_size': 15, 'gamma': 0.895626115735925}. Best is trial 14 with value: 0.08681186562436714.[0m
[32m[I 2025-01-30 19:21:07,097][0m Trial 15 finished with value: 0.07516113633603351 and parameters: {'observation_period_num': 7, 'train_rates': 0.6102722035198626, 'learning_rate': 0.00023271939641354605, 'batch_size': 184, 'step_size': 15, 'gamma': 0.8977899059757748}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:21:32,657][0m Trial 16 finished with value: 0.15963943436870287 and parameters: {'observation_period_num': 39, 'train_rates': 0.6017483212297475, 'learning_rate': 0.0002955240565050155, 'batch_size': 193, 'step_size': 15, 'gamma': 0.893953306843271}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:21:51,067][0m Trial 17 finished with value: 0.1330434410495961 and parameters: {'observation_period_num': 50, 'train_rates': 0.6113020053376137, 'learning_rate': 5.639428725363482e-05, 'batch_size': 255, 'step_size': 9, 'gamma': 0.9219385040789662}. Best is trial 15 with value: 0.07516113633603351.[0m
Early stopping at epoch 97
[32m[I 2025-01-30 19:22:14,838][0m Trial 18 finished with value: 0.3346376199505311 and parameters: {'observation_period_num': 248, 'train_rates': 0.6584948099121795, 'learning_rate': 0.0003603632194176769, 'batch_size': 185, 'step_size': 1, 'gamma': 0.8789309796710135}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:22:40,013][0m Trial 19 finished with value: 0.1951343908905983 and parameters: {'observation_period_num': 119, 'train_rates': 0.7244928864102437, 'learning_rate': 3.386106586717938e-05, 'batch_size': 198, 'step_size': 14, 'gamma': 0.9125927008021429}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:23:06,582][0m Trial 20 finished with value: 0.1145325762265963 and parameters: {'observation_period_num': 7, 'train_rates': 0.8565685980129938, 'learning_rate': 0.00019134018081428842, 'batch_size': 215, 'step_size': 13, 'gamma': 0.9354781732998608}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:23:39,152][0m Trial 21 finished with value: 0.08999197789031092 and parameters: {'observation_period_num': 19, 'train_rates': 0.6865723237871459, 'learning_rate': 0.00016879672167573946, 'batch_size': 154, 'step_size': 15, 'gamma': 0.8912837409677484}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:24:06,645][0m Trial 22 finished with value: 0.099184380542328 and parameters: {'observation_period_num': 43, 'train_rates': 0.6373866981750619, 'learning_rate': 0.0004159666974468506, 'batch_size': 172, 'step_size': 14, 'gamma': 0.8665238138562991}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:24:48,193][0m Trial 23 finished with value: 0.078071293198523 and parameters: {'observation_period_num': 6, 'train_rates': 0.7074528515286492, 'learning_rate': 0.00014445969912565718, 'batch_size': 121, 'step_size': 15, 'gamma': 0.8922835147754182}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:25:29,137][0m Trial 24 finished with value: 0.09931622897591154 and parameters: {'observation_period_num': 5, 'train_rates': 0.7152829990841167, 'learning_rate': 4.511580702405804e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.9022641325640717}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:26:02,918][0m Trial 25 finished with value: 0.09168409161540336 and parameters: {'observation_period_num': 37, 'train_rates': 0.6412073936721499, 'learning_rate': 0.0003403244009925796, 'batch_size': 135, 'step_size': 14, 'gamma': 0.8691116925787937}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:26:46,878][0m Trial 26 finished with value: 0.12203293726453518 and parameters: {'observation_period_num': 57, 'train_rates': 0.6001859622205635, 'learning_rate': 0.0005241999635608647, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8240295906737618}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:27:17,924][0m Trial 27 finished with value: 0.1346530536187134 and parameters: {'observation_period_num': 110, 'train_rates': 0.7422049749076616, 'learning_rate': 0.00012853330850989912, 'batch_size': 174, 'step_size': 14, 'gamma': 0.9319677704448552}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:27:40,677][0m Trial 28 finished with value: 0.13646963629495856 and parameters: {'observation_period_num': 28, 'train_rates': 0.6440103573675957, 'learning_rate': 6.487152701915456e-05, 'batch_size': 210, 'step_size': 13, 'gamma': 0.9054768544927438}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:28:20,455][0m Trial 29 finished with value: 0.1696982877595084 and parameters: {'observation_period_num': 136, 'train_rates': 0.7017662176527037, 'learning_rate': 2.4164495499070994e-05, 'batch_size': 122, 'step_size': 8, 'gamma': 0.9885324757916556}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:28:50,055][0m Trial 30 finished with value: 1.4525099782094564 and parameters: {'observation_period_num': 32, 'train_rates': 0.6249754101710274, 'learning_rate': 1.370022155871465e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.7583927066358831}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:29:23,915][0m Trial 31 finished with value: 0.08579376315077146 and parameters: {'observation_period_num': 7, 'train_rates': 0.6739535857940893, 'learning_rate': 0.0001570904211966041, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8851387376707055}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:29:57,624][0m Trial 32 finished with value: 0.10051702347970227 and parameters: {'observation_period_num': 61, 'train_rates': 0.6596781701527622, 'learning_rate': 0.00023480660807553884, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8585712344255835}. Best is trial 15 with value: 0.07516113633603351.[0m
[32m[I 2025-01-30 19:30:40,954][0m Trial 33 finished with value: 0.07469659299659794 and parameters: {'observation_period_num': 5, 'train_rates': 0.6213409060113201, 'learning_rate': 0.0009681443859197897, 'batch_size': 103, 'step_size': 14, 'gamma': 0.8867310555800487}. Best is trial 33 with value: 0.07469659299659794.[0m
[32m[I 2025-01-30 19:31:26,369][0m Trial 34 finished with value: 0.06988928308240092 and parameters: {'observation_period_num': 5, 'train_rates': 0.6641369197436111, 'learning_rate': 0.0008922522208266279, 'batch_size': 103, 'step_size': 14, 'gamma': 0.882968285842537}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:32:58,042][0m Trial 35 finished with value: 0.11836183309090753 and parameters: {'observation_period_num': 25, 'train_rates': 0.6491219190085857, 'learning_rate': 0.0009184273163573894, 'batch_size': 48, 'step_size': 14, 'gamma': 0.9474851623939506}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:33:41,554][0m Trial 36 finished with value: 0.0907548478481659 and parameters: {'observation_period_num': 48, 'train_rates': 0.6215777986914711, 'learning_rate': 0.0006864475108497843, 'batch_size': 106, 'step_size': 6, 'gamma': 0.8822804053495862}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:34:31,667][0m Trial 37 finished with value: 0.11726356168642213 and parameters: {'observation_period_num': 93, 'train_rates': 0.7434705755669312, 'learning_rate': 0.0005261011902395294, 'batch_size': 94, 'step_size': 12, 'gamma': 0.8408664531691259}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:35:39,446][0m Trial 38 finished with value: 0.12537605658173562 and parameters: {'observation_period_num': 25, 'train_rates': 0.8122268423965211, 'learning_rate': 0.000929398517828012, 'batch_size': 77, 'step_size': 10, 'gamma': 0.8735979992489942}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:37:19,517][0m Trial 39 finished with value: 0.14326451144099378 and parameters: {'observation_period_num': 62, 'train_rates': 0.7124118946518914, 'learning_rate': 1.135727983677177e-05, 'batch_size': 46, 'step_size': 13, 'gamma': 0.924811246935453}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:37:58,081][0m Trial 40 finished with value: 0.3727915214896202 and parameters: {'observation_period_num': 224, 'train_rates': 0.6331960862786175, 'learning_rate': 0.00046516423653208673, 'batch_size': 114, 'step_size': 14, 'gamma': 0.9710577465728084}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:38:50,661][0m Trial 41 finished with value: 0.08351028106685254 and parameters: {'observation_period_num': 8, 'train_rates': 0.6818061200828904, 'learning_rate': 0.0006881965253590072, 'batch_size': 91, 'step_size': 15, 'gamma': 0.9058952772010335}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:40:05,704][0m Trial 42 finished with value: 0.08553780689118161 and parameters: {'observation_period_num': 18, 'train_rates': 0.7523607920600752, 'learning_rate': 0.0005548072616515459, 'batch_size': 66, 'step_size': 14, 'gamma': 0.9076996869395572}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:40:56,404][0m Trial 43 finished with value: 0.14492426348792786 and parameters: {'observation_period_num': 158, 'train_rates': 0.6862168667986677, 'learning_rate': 0.0009815516874125084, 'batch_size': 90, 'step_size': 15, 'gamma': 0.9392035470154387}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:41:32,525][0m Trial 44 finished with value: 0.09200738436620996 and parameters: {'observation_period_num': 35, 'train_rates': 0.6585787764289308, 'learning_rate': 0.0006633101641432123, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8633552193568009}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:42:47,248][0m Trial 45 finished with value: 0.12121270534507236 and parameters: {'observation_period_num': 18, 'train_rates': 0.8903030687125945, 'learning_rate': 0.00035034891131085073, 'batch_size': 75, 'step_size': 4, 'gamma': 0.9133353470240869}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:43:38,410][0m Trial 46 finished with value: 0.07308186125953098 and parameters: {'observation_period_num': 7, 'train_rates': 0.698026606874474, 'learning_rate': 0.0006438933157896108, 'batch_size': 94, 'step_size': 12, 'gamma': 0.9004169731451471}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:44:23,809][0m Trial 47 finished with value: 0.10014009939985458 and parameters: {'observation_period_num': 49, 'train_rates': 0.7680389106373767, 'learning_rate': 0.00028145667092152393, 'batch_size': 112, 'step_size': 12, 'gamma': 0.8475181209850116}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:45:14,239][0m Trial 48 finished with value: 0.09029562423162371 and parameters: {'observation_period_num': 32, 'train_rates': 0.706801479767882, 'learning_rate': 9.83155284518336e-05, 'batch_size': 96, 'step_size': 13, 'gamma': 0.880677362926445}. Best is trial 34 with value: 0.06988928308240092.[0m
[32m[I 2025-01-30 19:46:47,430][0m Trial 49 finished with value: 0.4454543132048387 and parameters: {'observation_period_num': 179, 'train_rates': 0.953271722947812, 'learning_rate': 4.177123713892009e-06, 'batch_size': 60, 'step_size': 11, 'gamma': 0.9540105572837368}. Best is trial 34 with value: 0.06988928308240092.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-30 19:46:47,440][0m A new study created in memory with name: no-name-d0b7b7b1-cb31-43fb-9e65-dfa90659a3c4[0m
[32m[I 2025-01-30 19:47:48,264][0m Trial 0 finished with value: 0.28942300571549323 and parameters: {'observation_period_num': 105, 'train_rates': 0.825798463443836, 'learning_rate': 3.7481010850470465e-06, 'batch_size': 85, 'step_size': 11, 'gamma': 0.9505106363262706}. Best is trial 0 with value: 0.28942300571549323.[0m
[32m[I 2025-01-30 19:48:14,718][0m Trial 1 finished with value: 0.5451840758323669 and parameters: {'observation_period_num': 123, 'train_rates': 0.9404937334902012, 'learning_rate': 5.709948584324268e-06, 'batch_size': 233, 'step_size': 15, 'gamma': 0.9280468922366683}. Best is trial 0 with value: 0.28942300571549323.[0m
[32m[I 2025-01-30 19:49:04,587][0m Trial 2 finished with value: 0.5768949046977976 and parameters: {'observation_period_num': 60, 'train_rates': 0.6244286329208422, 'learning_rate': 2.2571096465599426e-06, 'batch_size': 90, 'step_size': 10, 'gamma': 0.7644825209032708}. Best is trial 0 with value: 0.28942300571549323.[0m
[32m[I 2025-01-30 19:49:37,038][0m Trial 3 finished with value: 0.6251466502161587 and parameters: {'observation_period_num': 79, 'train_rates': 0.7691994285278152, 'learning_rate': 1.8699316688078308e-06, 'batch_size': 165, 'step_size': 5, 'gamma': 0.8693024928592249}. Best is trial 0 with value: 0.28942300571549323.[0m
[32m[I 2025-01-30 19:50:13,714][0m Trial 4 finished with value: 0.08967947378845477 and parameters: {'observation_period_num': 55, 'train_rates': 0.7168456389707807, 'learning_rate': 0.00033329382008878683, 'batch_size': 137, 'step_size': 12, 'gamma': 0.9498055146355135}. Best is trial 4 with value: 0.08967947378845477.[0m
[32m[I 2025-01-30 19:50:46,301][0m Trial 5 finished with value: 0.677757094268664 and parameters: {'observation_period_num': 201, 'train_rates': 0.8972773404268544, 'learning_rate': 2.2613660212985905e-06, 'batch_size': 176, 'step_size': 7, 'gamma': 0.8253270191783882}. Best is trial 4 with value: 0.08967947378845477.[0m
[32m[I 2025-01-30 19:52:33,818][0m Trial 6 finished with value: 0.1931445991773626 and parameters: {'observation_period_num': 251, 'train_rates': 0.6576830744891676, 'learning_rate': 5.4255212885343266e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.7674983040713779}. Best is trial 4 with value: 0.08967947378845477.[0m
[32m[I 2025-01-30 19:53:03,667][0m Trial 7 finished with value: 0.1552620828151703 and parameters: {'observation_period_num': 127, 'train_rates': 0.9890822038554805, 'learning_rate': 1.8911603149155785e-05, 'batch_size': 214, 'step_size': 13, 'gamma': 0.9025111890392143}. Best is trial 4 with value: 0.08967947378845477.[0m
[32m[I 2025-01-30 19:56:50,949][0m Trial 8 finished with value: 0.4430667601179245 and parameters: {'observation_period_num': 249, 'train_rates': 0.6181496017658219, 'learning_rate': 2.0669430449210596e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8238515608317182}. Best is trial 4 with value: 0.08967947378845477.[0m
[32m[I 2025-01-30 19:57:28,720][0m Trial 9 finished with value: 0.338860692049859 and parameters: {'observation_period_num': 83, 'train_rates': 0.6787686582974721, 'learning_rate': 4.570136268409238e-06, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8627474517329728}. Best is trial 4 with value: 0.08967947378845477.[0m
[32m[I 2025-01-30 19:58:09,658][0m Trial 10 finished with value: 0.07656923710038105 and parameters: {'observation_period_num': 8, 'train_rates': 0.7473057823029849, 'learning_rate': 0.0009324846002630221, 'batch_size': 125, 'step_size': 3, 'gamma': 0.9735942831676088}. Best is trial 10 with value: 0.07656923710038105.[0m
[32m[I 2025-01-30 19:58:53,366][0m Trial 11 finished with value: 0.07313827616991687 and parameters: {'observation_period_num': 6, 'train_rates': 0.7465600085530208, 'learning_rate': 0.0008095169185367859, 'batch_size': 121, 'step_size': 1, 'gamma': 0.9825527885121259}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 19:59:50,749][0m Trial 12 finished with value: 0.0764474984338527 and parameters: {'observation_period_num': 8, 'train_rates': 0.7788539068501887, 'learning_rate': 0.00094462762568192, 'batch_size': 91, 'step_size': 1, 'gamma': 0.9843171394553947}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:01:02,718][0m Trial 13 finished with value: 0.08080448427191182 and parameters: {'observation_period_num': 9, 'train_rates': 0.8227239343630104, 'learning_rate': 0.00020934267933522873, 'batch_size': 74, 'step_size': 1, 'gamma': 0.9878358166191281}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:02:17,207][0m Trial 14 finished with value: 0.10787421073764562 and parameters: {'observation_period_num': 33, 'train_rates': 0.862948630033177, 'learning_rate': 0.0009546873659659187, 'batch_size': 74, 'step_size': 2, 'gamma': 0.9053078541959455}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:03:00,558][0m Trial 15 finished with value: 0.14994343763048 and parameters: {'observation_period_num': 188, 'train_rates': 0.721521752147436, 'learning_rate': 0.00019270419720778401, 'batch_size': 107, 'step_size': 4, 'gamma': 0.986369090081066}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:03:30,209][0m Trial 16 finished with value: 0.1498163630222452 and parameters: {'observation_period_num': 161, 'train_rates': 0.7921949055702563, 'learning_rate': 8.5407117601623e-05, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9430112129070869}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:05:02,761][0m Trial 17 finished with value: 0.08905126244788712 and parameters: {'observation_period_num': 37, 'train_rates': 0.688042721584527, 'learning_rate': 0.00046996427422482494, 'batch_size': 49, 'step_size': 1, 'gamma': 0.9138168077187974}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:05:40,877][0m Trial 18 finished with value: 0.11044585859363383 and parameters: {'observation_period_num': 29, 'train_rates': 0.8391202077875223, 'learning_rate': 0.00010686659459205272, 'batch_size': 151, 'step_size': 3, 'gamma': 0.9589069988988244}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:06:09,158][0m Trial 19 finished with value: 0.07630065717408364 and parameters: {'observation_period_num': 5, 'train_rates': 0.7703814808931853, 'learning_rate': 0.00043933511854729146, 'batch_size': 200, 'step_size': 6, 'gamma': 0.8884471325120609}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:06:35,198][0m Trial 20 finished with value: 0.12520563002154617 and parameters: {'observation_period_num': 87, 'train_rates': 0.7303785598808693, 'learning_rate': 0.0005070722443949596, 'batch_size': 199, 'step_size': 7, 'gamma': 0.833413553199988}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:06:58,824][0m Trial 21 finished with value: 0.08580462822613244 and parameters: {'observation_period_num': 11, 'train_rates': 0.7872504624470971, 'learning_rate': 0.0009255906686195476, 'batch_size': 252, 'step_size': 5, 'gamma': 0.7928045806965043}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:07:46,472][0m Trial 22 finished with value: 0.09949578077643675 and parameters: {'observation_period_num': 49, 'train_rates': 0.7636776183227366, 'learning_rate': 0.00027680996220587676, 'batch_size': 108, 'step_size': 1, 'gamma': 0.9676912471868971}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:08:15,257][0m Trial 23 finished with value: 0.11410433531273156 and parameters: {'observation_period_num': 25, 'train_rates': 0.8687225634298239, 'learning_rate': 0.0005090986644168441, 'batch_size': 197, 'step_size': 3, 'gamma': 0.8862877403239567}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:09:04,164][0m Trial 24 finished with value: 0.11237942948800204 and parameters: {'observation_period_num': 63, 'train_rates': 0.8120279120583765, 'learning_rate': 0.00013056575023838712, 'batch_size': 109, 'step_size': 2, 'gamma': 0.9321306179604919}. Best is trial 11 with value: 0.07313827616991687.[0m
[32m[I 2025-01-30 20:09:39,864][0m Trial 25 finished with value: 0.06970717098401215 and parameters: {'observation_period_num': 5, 'train_rates': 0.6970416169132467, 'learning_rate': 0.0005564063623388238, 'batch_size': 144, 'step_size': 6, 'gamma': 0.975350221092885}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:10:13,128][0m Trial 26 finished with value: 0.11624399695042018 and parameters: {'observation_period_num': 41, 'train_rates': 0.6821714300250306, 'learning_rate': 4.351703984508255e-05, 'batch_size': 151, 'step_size': 8, 'gamma': 0.8522429339420233}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:10:37,918][0m Trial 27 finished with value: 0.08101905717593044 and parameters: {'observation_period_num': 22, 'train_rates': 0.6481592865379817, 'learning_rate': 0.0004118801250164113, 'batch_size': 192, 'step_size': 8, 'gamma': 0.8880673367409906}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:10:59,518][0m Trial 28 finished with value: 0.37972412477944784 and parameters: {'observation_period_num': 161, 'train_rates': 0.7031995233120517, 'learning_rate': 1.0742488928187814e-05, 'batch_size': 220, 'step_size': 6, 'gamma': 0.9236819050263398}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:11:33,694][0m Trial 29 finished with value: 0.12468142601950415 and parameters: {'observation_period_num': 97, 'train_rates': 0.746200408601265, 'learning_rate': 0.00020113139493193238, 'batch_size': 150, 'step_size': 10, 'gamma': 0.9574393911173443}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:11:54,223][0m Trial 30 finished with value: 0.09931966375963332 and parameters: {'observation_period_num': 68, 'train_rates': 0.6608003405226691, 'learning_rate': 0.0005729442982350306, 'batch_size': 238, 'step_size': 6, 'gamma': 0.7952222506765859}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:12:56,641][0m Trial 31 finished with value: 0.07364133890428229 and parameters: {'observation_period_num': 7, 'train_rates': 0.7731152193624179, 'learning_rate': 0.0006089350586129858, 'batch_size': 83, 'step_size': 4, 'gamma': 0.974418239621569}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:14:37,334][0m Trial 32 finished with value: 0.37398354160970976 and parameters: {'observation_period_num': 21, 'train_rates': 0.7366337316735198, 'learning_rate': 1.071477227373829e-06, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9406681514004462}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:15:54,840][0m Trial 33 finished with value: 0.0960417866026624 and parameters: {'observation_period_num': 47, 'train_rates': 0.7588411180247978, 'learning_rate': 0.0003011087055657607, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9707243254139053}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:16:47,713][0m Trial 34 finished with value: 0.07640299445190349 and parameters: {'observation_period_num': 5, 'train_rates': 0.798191467112533, 'learning_rate': 0.0005643624910192303, 'batch_size': 100, 'step_size': 6, 'gamma': 0.9640534166998602}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:17:27,080][0m Trial 35 finished with value: 0.12988636090702108 and parameters: {'observation_period_num': 104, 'train_rates': 0.7017622308378048, 'learning_rate': 0.0006508900716802132, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9486885509248039}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:18:06,837][0m Trial 36 finished with value: 0.0977777218905048 and parameters: {'observation_period_num': 20, 'train_rates': 0.8354201327961203, 'learning_rate': 0.00015492644999701377, 'batch_size': 140, 'step_size': 15, 'gamma': 0.9273824482062295}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:18:32,201][0m Trial 37 finished with value: 0.17302915724958928 and parameters: {'observation_period_num': 59, 'train_rates': 0.6026500115462315, 'learning_rate': 7.551924995777787e-05, 'batch_size': 182, 'step_size': 8, 'gamma': 0.9751403964882205}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:19:08,694][0m Trial 38 finished with value: 0.15868798814807478 and parameters: {'observation_period_num': 39, 'train_rates': 0.9179763972449848, 'learning_rate': 0.00032046906199975205, 'batch_size': 162, 'step_size': 2, 'gamma': 0.9395529910124399}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:20:01,327][0m Trial 39 finished with value: 0.11727001722856739 and parameters: {'observation_period_num': 115, 'train_rates': 0.7101306849346014, 'learning_rate': 0.0006806494195867408, 'batch_size': 91, 'step_size': 9, 'gamma': 0.8936014437435367}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:20:44,517][0m Trial 40 finished with value: 0.11153601244678114 and parameters: {'observation_period_num': 72, 'train_rates': 0.7704994255933184, 'learning_rate': 0.0002703543777636657, 'batch_size': 121, 'step_size': 4, 'gamma': 0.916550131331266}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:21:37,005][0m Trial 41 finished with value: 0.0786370434190916 and parameters: {'observation_period_num': 8, 'train_rates': 0.8047291265440147, 'learning_rate': 0.000400750156852828, 'batch_size': 100, 'step_size': 6, 'gamma': 0.9608726522780429}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:22:38,653][0m Trial 42 finished with value: 0.08398638997699519 and parameters: {'observation_period_num': 19, 'train_rates': 0.7930997842389825, 'learning_rate': 0.000717314009767037, 'batch_size': 83, 'step_size': 6, 'gamma': 0.9774804307018544}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:23:20,544][0m Trial 43 finished with value: 0.09754321005291347 and parameters: {'observation_period_num': 5, 'train_rates': 0.8552590195175701, 'learning_rate': 0.0004019584511709752, 'batch_size': 136, 'step_size': 7, 'gamma': 0.9563788896906323}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:24:37,923][0m Trial 44 finished with value: 0.10403250953901358 and parameters: {'observation_period_num': 29, 'train_rates': 0.7527777087510248, 'learning_rate': 0.0006633238615695658, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9889422855596223}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:25:23,335][0m Trial 45 finished with value: 0.22103783016266304 and parameters: {'observation_period_num': 48, 'train_rates': 0.8132267339611933, 'learning_rate': 8.078273631282153e-06, 'batch_size': 118, 'step_size': 3, 'gamma': 0.9683915290288662}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:26:26,721][0m Trial 46 finished with value: 0.10528726130723953 and parameters: {'observation_period_num': 18, 'train_rates': 0.7375845354254158, 'learning_rate': 2.386621398724213e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.9784361840293331}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:27:20,089][0m Trial 47 finished with value: 0.07542528158931108 and parameters: {'observation_period_num': 5, 'train_rates': 0.7715575490656199, 'learning_rate': 0.00023573420743104232, 'batch_size': 97, 'step_size': 9, 'gamma': 0.874655477482439}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:30:23,426][0m Trial 48 finished with value: 0.17594077103024064 and parameters: {'observation_period_num': 230, 'train_rates': 0.7768455267907877, 'learning_rate': 0.00024096285327929932, 'batch_size': 25, 'step_size': 10, 'gamma': 0.8764627537374017}. Best is trial 25 with value: 0.06970717098401215.[0m
[32m[I 2025-01-30 20:30:53,797][0m Trial 49 finished with value: 0.0828704992550369 and parameters: {'observation_period_num': 34, 'train_rates': 0.7220726399440609, 'learning_rate': 0.0009992492851541772, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8491137878931696}. Best is trial 25 with value: 0.06970717098401215.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-30 20:30:53,807][0m A new study created in memory with name: no-name-14d680e6-de5a-417f-a765-c8fe73ea6210[0m
[32m[I 2025-01-30 20:31:15,622][0m Trial 0 finished with value: 0.20308941867164376 and parameters: {'observation_period_num': 138, 'train_rates': 0.687676744705695, 'learning_rate': 5.477458636962902e-05, 'batch_size': 238, 'step_size': 7, 'gamma': 0.8313682097001849}. Best is trial 0 with value: 0.20308941867164376.[0m
[32m[I 2025-01-30 20:31:45,165][0m Trial 1 finished with value: 0.44361509433524177 and parameters: {'observation_period_num': 183, 'train_rates': 0.8092068790739555, 'learning_rate': 7.120228653051536e-06, 'batch_size': 177, 'step_size': 8, 'gamma': 0.8742968170178403}. Best is trial 0 with value: 0.20308941867164376.[0m
[32m[I 2025-01-30 20:32:18,066][0m Trial 2 finished with value: 0.26825594902038574 and parameters: {'observation_period_num': 47, 'train_rates': 0.9586458945143761, 'learning_rate': 0.00018790602819814803, 'batch_size': 194, 'step_size': 4, 'gamma': 0.8508381686020797}. Best is trial 0 with value: 0.20308941867164376.[0m
[32m[I 2025-01-30 20:32:46,629][0m Trial 3 finished with value: 0.18132740411352605 and parameters: {'observation_period_num': 69, 'train_rates': 0.6738883661741076, 'learning_rate': 2.3013970816755516e-05, 'batch_size': 176, 'step_size': 7, 'gamma': 0.8908675517755169}. Best is trial 3 with value: 0.18132740411352605.[0m
[32m[I 2025-01-30 20:33:26,957][0m Trial 4 finished with value: 0.2656502432428894 and parameters: {'observation_period_num': 229, 'train_rates': 0.6679315371950747, 'learning_rate': 0.0006209425441866625, 'batch_size': 110, 'step_size': 12, 'gamma': 0.9011953130392389}. Best is trial 3 with value: 0.18132740411352605.[0m
[32m[I 2025-01-30 20:34:18,790][0m Trial 5 finished with value: 0.6741171677035895 and parameters: {'observation_period_num': 182, 'train_rates': 0.7529417457629672, 'learning_rate': 1.5440534562427689e-06, 'batch_size': 91, 'step_size': 9, 'gamma': 0.974078291195187}. Best is trial 3 with value: 0.18132740411352605.[0m
Early stopping at epoch 82
[32m[I 2025-01-30 20:34:47,860][0m Trial 6 finished with value: 0.3417954576366088 and parameters: {'observation_period_num': 14, 'train_rates': 0.9307318845223345, 'learning_rate': 2.947137835945171e-05, 'batch_size': 177, 'step_size': 1, 'gamma': 0.8676713725044156}. Best is trial 3 with value: 0.18132740411352605.[0m
[32m[I 2025-01-30 20:35:14,992][0m Trial 7 finished with value: 0.0754860400051718 and parameters: {'observation_period_num': 8, 'train_rates': 0.6621954090455742, 'learning_rate': 0.00022473876337022032, 'batch_size': 193, 'step_size': 14, 'gamma': 0.7960089963195343}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:35:36,639][0m Trial 8 finished with value: 0.12809315028375592 and parameters: {'observation_period_num': 154, 'train_rates': 0.6891934792790765, 'learning_rate': 0.0007062499246052898, 'batch_size': 223, 'step_size': 12, 'gamma': 0.783790896419117}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:36:15,844][0m Trial 9 finished with value: 0.15359867808733196 and parameters: {'observation_period_num': 61, 'train_rates': 0.6243834666053614, 'learning_rate': 2.8118687428395922e-05, 'batch_size': 113, 'step_size': 10, 'gamma': 0.8224502558867695}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:41:18,285][0m Trial 10 finished with value: 0.12468299634497741 and parameters: {'observation_period_num': 98, 'train_rates': 0.7967267994519263, 'learning_rate': 0.00014710547687993354, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7534781731497543}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:46:30,211][0m Trial 11 finished with value: 0.14282359266634612 and parameters: {'observation_period_num': 95, 'train_rates': 0.8289265667732462, 'learning_rate': 0.00015008802750682922, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7526496285320097}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:48:55,725][0m Trial 12 finished with value: 0.10256704332941509 and parameters: {'observation_period_num': 6, 'train_rates': 0.8763734454613756, 'learning_rate': 0.00020017934609418335, 'batch_size': 37, 'step_size': 15, 'gamma': 0.7524229803943987}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:50:28,713][0m Trial 13 finished with value: 0.10931732001955863 and parameters: {'observation_period_num': 5, 'train_rates': 0.8901724722857024, 'learning_rate': 0.00033644054031606676, 'batch_size': 60, 'step_size': 13, 'gamma': 0.7956474336698828}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:51:08,868][0m Trial 14 finished with value: 0.12982072056024924 and parameters: {'observation_period_num': 29, 'train_rates': 0.8811988969021742, 'learning_rate': 8.521177205001219e-05, 'batch_size': 148, 'step_size': 15, 'gamma': 0.7901929846025834}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:52:27,869][0m Trial 15 finished with value: 0.12494430803651835 and parameters: {'observation_period_num': 102, 'train_rates': 0.7352602242529087, 'learning_rate': 0.000382619939018981, 'batch_size': 60, 'step_size': 11, 'gamma': 0.9336628451516424}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:53:00,141][0m Trial 16 finished with value: 0.28086071884171643 and parameters: {'observation_period_num': 39, 'train_rates': 0.6001187196237876, 'learning_rate': 9.33407268234369e-06, 'batch_size': 142, 'step_size': 15, 'gamma': 0.772719629943013}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:53:26,535][0m Trial 17 finished with value: 0.12966358506513528 and parameters: {'observation_period_num': 5, 'train_rates': 0.872284606925131, 'learning_rate': 7.045207314579583e-05, 'batch_size': 215, 'step_size': 13, 'gamma': 0.8143612556864681}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:54:46,274][0m Trial 18 finished with value: 0.16781915387689245 and parameters: {'observation_period_num': 244, 'train_rates': 0.7587178148118621, 'learning_rate': 0.00029306680104392133, 'batch_size': 57, 'step_size': 4, 'gamma': 0.8062005079994683}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:55:46,545][0m Trial 19 finished with value: 0.1290243336943542 and parameters: {'observation_period_num': 64, 'train_rates': 0.8473091835648245, 'learning_rate': 0.0009930815077965288, 'batch_size': 88, 'step_size': 11, 'gamma': 0.7657206196007923}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:56:10,978][0m Trial 20 finished with value: 0.4760659635066986 and parameters: {'observation_period_num': 81, 'train_rates': 0.9153509242341964, 'learning_rate': 9.036722428982162e-06, 'batch_size': 253, 'step_size': 13, 'gamma': 0.8388602410357264}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 20:58:11,212][0m Trial 21 finished with value: 0.31477224074769766 and parameters: {'observation_period_num': 5, 'train_rates': 0.9731115823836165, 'learning_rate': 0.00037545621463967054, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7990169763522932}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:00:52,757][0m Trial 22 finished with value: 0.14673523226005286 and parameters: {'observation_period_num': 30, 'train_rates': 0.8994022058105141, 'learning_rate': 0.00021941752275867922, 'batch_size': 34, 'step_size': 15, 'gamma': 0.7846092569740732}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:02:05,452][0m Trial 23 finished with value: 0.11593190983716231 and parameters: {'observation_period_num': 26, 'train_rates': 0.8625389213558448, 'learning_rate': 0.0001106160710194006, 'batch_size': 75, 'step_size': 14, 'gamma': 0.7680531125260066}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:04:30,036][0m Trial 24 finished with value: 0.21643776894811004 and parameters: {'observation_period_num': 49, 'train_rates': 0.935584411461576, 'learning_rate': 0.0005184216588598545, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8057838364946319}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:05:14,507][0m Trial 25 finished with value: 0.10756609438359738 and parameters: {'observation_period_num': 21, 'train_rates': 0.8293579127841284, 'learning_rate': 5.57515049843619e-05, 'batch_size': 122, 'step_size': 14, 'gamma': 0.8429805597851057}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:05:54,306][0m Trial 26 finished with value: 0.13929999929848055 and parameters: {'observation_period_num': 119, 'train_rates': 0.7218042503283616, 'learning_rate': 6.231505077902869e-05, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8519117956345842}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:06:29,117][0m Trial 27 finished with value: 0.14466826704731822 and parameters: {'observation_period_num': 24, 'train_rates': 0.7659220030071291, 'learning_rate': 1.6871158168388155e-05, 'batch_size': 150, 'step_size': 14, 'gamma': 0.8243532109012837}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:07:02,459][0m Trial 28 finished with value: 0.1271427820600171 and parameters: {'observation_period_num': 49, 'train_rates': 0.8195760681307009, 'learning_rate': 4.741525675289742e-05, 'batch_size': 159, 'step_size': 5, 'gamma': 0.9153154595578349}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:07:29,924][0m Trial 29 finished with value: 0.15245427102917258 and parameters: {'observation_period_num': 138, 'train_rates': 0.8410726061357624, 'learning_rate': 0.00012992042853578005, 'batch_size': 205, 'step_size': 11, 'gamma': 0.839286375237489}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:08:10,786][0m Trial 30 finished with value: 0.12935468251522206 and parameters: {'observation_period_num': 78, 'train_rates': 0.7800858872108294, 'learning_rate': 4.947181535541375e-05, 'batch_size': 127, 'step_size': 14, 'gamma': 0.7748721461755392}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:09:26,189][0m Trial 31 finished with value: 0.12936352961935077 and parameters: {'observation_period_num': 16, 'train_rates': 0.9003286550799705, 'learning_rate': 0.00030626390346538945, 'batch_size': 75, 'step_size': 13, 'gamma': 0.7951501216170102}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:10:24,765][0m Trial 32 finished with value: 0.11151561262802435 and parameters: {'observation_period_num': 37, 'train_rates': 0.8575268383179014, 'learning_rate': 0.000239408646991753, 'batch_size': 93, 'step_size': 14, 'gamma': 0.8155079194598193}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:13:11,160][0m Trial 33 finished with value: 0.08151830888491138 and parameters: {'observation_period_num': 6, 'train_rates': 0.8119206467062653, 'learning_rate': 9.670573725284253e-05, 'batch_size': 31, 'step_size': 15, 'gamma': 0.8564626709949907}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:15:52,216][0m Trial 34 finished with value: 0.09994851290696978 and parameters: {'observation_period_num': 46, 'train_rates': 0.7927449785567021, 'learning_rate': 9.25779603979544e-05, 'batch_size': 31, 'step_size': 15, 'gamma': 0.8669585691971666}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:18:29,283][0m Trial 35 finished with value: 0.11630519876813127 and parameters: {'observation_period_num': 51, 'train_rates': 0.8004680372040902, 'learning_rate': 0.00019152885624000365, 'batch_size': 32, 'step_size': 15, 'gamma': 0.8721138437292882}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:18:55,893][0m Trial 36 finished with value: 0.09998880951070642 and parameters: {'observation_period_num': 37, 'train_rates': 0.7151983507207598, 'learning_rate': 9.104516991467966e-05, 'batch_size': 191, 'step_size': 7, 'gamma': 0.8927379987532594}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:19:24,400][0m Trial 37 finished with value: 0.3157151632903549 and parameters: {'observation_period_num': 192, 'train_rates': 0.7034398371078864, 'learning_rate': 1.7893291510099357e-05, 'batch_size': 168, 'step_size': 6, 'gamma': 0.8874549280862747}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:19:48,935][0m Trial 38 finished with value: 0.7509004916214361 and parameters: {'observation_period_num': 41, 'train_rates': 0.6477974363175735, 'learning_rate': 2.0680157545155255e-06, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9325099546676406}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:20:13,937][0m Trial 39 finished with value: 0.16338547877581716 and parameters: {'observation_period_num': 75, 'train_rates': 0.6641087231692452, 'learning_rate': 9.118198705445498e-05, 'batch_size': 191, 'step_size': 8, 'gamma': 0.8888911510894313}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:20:35,577][0m Trial 40 finished with value: 0.304248690662697 and parameters: {'observation_period_num': 59, 'train_rates': 0.7314166753244106, 'learning_rate': 3.822738110371846e-05, 'batch_size': 241, 'step_size': 2, 'gamma': 0.8640262748593929}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:23:20,841][0m Trial 41 finished with value: 0.08612718420652296 and parameters: {'observation_period_num': 15, 'train_rates': 0.7053742734003751, 'learning_rate': 0.00010369118838950855, 'batch_size': 28, 'step_size': 15, 'gamma': 0.9144178328994435}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:26:25,378][0m Trial 42 finished with value: 0.08114902282566822 and parameters: {'observation_period_num': 21, 'train_rates': 0.7021820433255455, 'learning_rate': 0.00011167443394003915, 'batch_size': 25, 'step_size': 6, 'gamma': 0.9090505304063747}. Best is trial 7 with value: 0.0754860400051718.[0m
[32m[I 2025-01-30 21:29:53,805][0m Trial 43 finished with value: 0.07304966751954808 and parameters: {'observation_period_num': 18, 'train_rates': 0.6912581798040126, 'learning_rate': 0.00012671990215831424, 'batch_size': 22, 'step_size': 3, 'gamma': 0.912599958753059}. Best is trial 43 with value: 0.07304966751954808.[0m
[32m[I 2025-01-30 21:33:43,334][0m Trial 44 finished with value: 0.07366190136857495 and parameters: {'observation_period_num': 14, 'train_rates': 0.6909616399888137, 'learning_rate': 0.00014739880959720365, 'batch_size': 20, 'step_size': 2, 'gamma': 0.968183783058919}. Best is trial 43 with value: 0.07304966751954808.[0m
[32m[I 2025-01-30 21:37:56,436][0m Trial 45 finished with value: 0.07562287726542229 and parameters: {'observation_period_num': 18, 'train_rates': 0.6414261441935206, 'learning_rate': 0.0001450273422320554, 'batch_size': 17, 'step_size': 2, 'gamma': 0.9723505815479097}. Best is trial 43 with value: 0.07304966751954808.[0m
[32m[I 2025-01-30 21:41:42,647][0m Trial 46 finished with value: 0.08494048498314494 and parameters: {'observation_period_num': 23, 'train_rates': 0.6427700797577648, 'learning_rate': 0.0001558500810603028, 'batch_size': 19, 'step_size': 2, 'gamma': 0.9896965626460663}. Best is trial 43 with value: 0.07304966751954808.[0m
[32m[I 2025-01-30 21:43:28,857][0m Trial 47 finished with value: 0.08673830930082524 and parameters: {'observation_period_num': 32, 'train_rates': 0.6867423121949736, 'learning_rate': 0.0005116304299771809, 'batch_size': 43, 'step_size': 1, 'gamma': 0.9643418982990019}. Best is trial 43 with value: 0.07304966751954808.[0m
[32m[I 2025-01-30 21:44:44,471][0m Trial 48 finished with value: 0.17046069623973534 and parameters: {'observation_period_num': 210, 'train_rates': 0.6178605724290647, 'learning_rate': 0.0001783315005478154, 'batch_size': 53, 'step_size': 3, 'gamma': 0.9571993465961219}. Best is trial 43 with value: 0.07304966751954808.[0m
[32m[I 2025-01-30 21:47:57,942][0m Trial 49 finished with value: 0.20409946961373818 and parameters: {'observation_period_num': 157, 'train_rates': 0.6771874384162094, 'learning_rate': 0.0002566973711607355, 'batch_size': 22, 'step_size': 4, 'gamma': 0.9477258926833015}. Best is trial 43 with value: 0.07304966751954808.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-30 21:47:57,952][0m A new study created in memory with name: no-name-f4e40d12-224f-4da0-a874-99cb6830a88b[0m
[32m[I 2025-01-30 21:50:22,840][0m Trial 0 finished with value: 0.21236366075411273 and parameters: {'observation_period_num': 124, 'train_rates': 0.8740516336176757, 'learning_rate': 8.20789760913213e-06, 'batch_size': 36, 'step_size': 14, 'gamma': 0.7515956616306647}. Best is trial 0 with value: 0.21236366075411273.[0m
[32m[I 2025-01-30 21:51:58,246][0m Trial 1 finished with value: 0.21183616871822555 and parameters: {'observation_period_num': 202, 'train_rates': 0.7302922248010598, 'learning_rate': 9.269281372065864e-06, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9821541727403988}. Best is trial 1 with value: 0.21183616871822555.[0m
[32m[I 2025-01-30 21:52:22,755][0m Trial 2 finished with value: 0.3198560360920568 and parameters: {'observation_period_num': 135, 'train_rates': 0.9088522433374355, 'learning_rate': 2.8243800175876033e-05, 'batch_size': 244, 'step_size': 6, 'gamma': 0.8895060290230965}. Best is trial 1 with value: 0.21183616871822555.[0m
[32m[I 2025-01-30 21:53:18,229][0m Trial 3 finished with value: 0.1932946057391486 and parameters: {'observation_period_num': 176, 'train_rates': 0.9195048034740385, 'learning_rate': 0.0009568404065884475, 'batch_size': 101, 'step_size': 2, 'gamma': 0.9136520236428569}. Best is trial 3 with value: 0.1932946057391486.[0m
[32m[I 2025-01-30 21:53:39,651][0m Trial 4 finished with value: 0.5227600640696353 and parameters: {'observation_period_num': 171, 'train_rates': 0.8051388953904453, 'learning_rate': 3.175961685960016e-06, 'batch_size': 251, 'step_size': 13, 'gamma': 0.9776147619068922}. Best is trial 3 with value: 0.1932946057391486.[0m
[32m[I 2025-01-30 21:54:23,007][0m Trial 5 finished with value: 0.2869384643082181 and parameters: {'observation_period_num': 205, 'train_rates': 0.6192094250472209, 'learning_rate': 0.00019858137783716005, 'batch_size': 95, 'step_size': 3, 'gamma': 0.9178839072168945}. Best is trial 3 with value: 0.1932946057391486.[0m
[32m[I 2025-01-30 21:56:37,394][0m Trial 6 finished with value: 0.17040191015292858 and parameters: {'observation_period_num': 55, 'train_rates': 0.9001713611232932, 'learning_rate': 0.0009707920683018765, 'batch_size': 41, 'step_size': 5, 'gamma': 0.8759460411443974}. Best is trial 6 with value: 0.17040191015292858.[0m
[32m[I 2025-01-30 21:57:10,086][0m Trial 7 finished with value: 0.5748059204464705 and parameters: {'observation_period_num': 134, 'train_rates': 0.9004822673400905, 'learning_rate': 5.808686179581481e-06, 'batch_size': 174, 'step_size': 1, 'gamma': 0.9725010573822006}. Best is trial 6 with value: 0.17040191015292858.[0m
[32m[I 2025-01-30 21:58:21,610][0m Trial 8 finished with value: 0.11720846637293023 and parameters: {'observation_period_num': 60, 'train_rates': 0.6480370801040787, 'learning_rate': 4.213423360738362e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8188194395955953}. Best is trial 8 with value: 0.11720846637293023.[0m
[32m[I 2025-01-30 22:01:31,107][0m Trial 9 finished with value: 0.2688688387864712 and parameters: {'observation_period_num': 186, 'train_rates': 0.6098820681906436, 'learning_rate': 4.959071619533141e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.9220413740363191}. Best is trial 8 with value: 0.11720846637293023.[0m
[32m[I 2025-01-30 22:02:01,743][0m Trial 10 finished with value: 0.4539086165465835 and parameters: {'observation_period_num': 7, 'train_rates': 0.6991771599776723, 'learning_rate': 1.3029667762426923e-06, 'batch_size': 168, 'step_size': 11, 'gamma': 0.7998045136830397}. Best is trial 8 with value: 0.11720846637293023.[0m
[32m[I 2025-01-30 22:02:58,135][0m Trial 11 finished with value: 0.107890351367331 and parameters: {'observation_period_num': 51, 'train_rates': 0.8156818006185103, 'learning_rate': 0.0009298307432372686, 'batch_size': 93, 'step_size': 10, 'gamma': 0.8321814438154802}. Best is trial 11 with value: 0.107890351367331.[0m
[32m[I 2025-01-30 22:03:56,018][0m Trial 12 finished with value: 0.10179223691555836 and parameters: {'observation_period_num': 64, 'train_rates': 0.804760720160024, 'learning_rate': 0.00021402185302455724, 'batch_size': 91, 'step_size': 10, 'gamma': 0.829340316833727}. Best is trial 12 with value: 0.10179223691555836.[0m
[32m[I 2025-01-30 22:04:38,650][0m Trial 13 finished with value: 0.10748987258486384 and parameters: {'observation_period_num': 68, 'train_rates': 0.8049387583039477, 'learning_rate': 0.00024558914152097614, 'batch_size': 123, 'step_size': 10, 'gamma': 0.8290718757161226}. Best is trial 12 with value: 0.10179223691555836.[0m
[32m[I 2025-01-30 22:05:20,847][0m Trial 14 finished with value: 0.0632578581571579 and parameters: {'observation_period_num': 90, 'train_rates': 0.9883206887075948, 'learning_rate': 0.00020082888946698732, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8434114253816316}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:05:59,237][0m Trial 15 finished with value: 0.07949796319007874 and parameters: {'observation_period_num': 94, 'train_rates': 0.9804657077939674, 'learning_rate': 0.0001714024199861083, 'batch_size': 159, 'step_size': 8, 'gamma': 0.7820239021502868}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:06:33,486][0m Trial 16 finished with value: 0.13344816863536835 and parameters: {'observation_period_num': 238, 'train_rates': 0.9887428104309295, 'learning_rate': 0.00010791839656643795, 'batch_size': 169, 'step_size': 8, 'gamma': 0.7639964495942345}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:07:03,836][0m Trial 17 finished with value: 0.07555508613586426 and parameters: {'observation_period_num': 101, 'train_rates': 0.981727808661407, 'learning_rate': 0.00041310105124220766, 'batch_size': 214, 'step_size': 4, 'gamma': 0.7862215406008954}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:07:32,217][0m Trial 18 finished with value: 0.2801288664340973 and parameters: {'observation_period_num': 101, 'train_rates': 0.9519602107908768, 'learning_rate': 0.0004522693019716559, 'batch_size': 220, 'step_size': 4, 'gamma': 0.8524964079216087}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:08:01,576][0m Trial 19 finished with value: 0.12872009751595317 and parameters: {'observation_period_num': 14, 'train_rates': 0.8509000533792036, 'learning_rate': 7.355104172517888e-05, 'batch_size': 206, 'step_size': 6, 'gamma': 0.7958662826950689}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:08:32,096][0m Trial 20 finished with value: 0.2996858060359955 and parameters: {'observation_period_num': 96, 'train_rates': 0.9586854213958936, 'learning_rate': 0.0004213975839431247, 'batch_size': 198, 'step_size': 4, 'gamma': 0.8608443251868918}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:09:10,646][0m Trial 21 finished with value: 0.09802716970443726 and parameters: {'observation_period_num': 95, 'train_rates': 0.977907073948927, 'learning_rate': 0.00011047710383601008, 'batch_size': 159, 'step_size': 8, 'gamma': 0.7806450137798969}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:09:53,500][0m Trial 22 finished with value: 0.2549307949887881 and parameters: {'observation_period_num': 112, 'train_rates': 0.9443016761771053, 'learning_rate': 0.0004362973509940717, 'batch_size': 139, 'step_size': 8, 'gamma': 0.780328411427669}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:10:34,199][0m Trial 23 finished with value: 0.3571249009495941 and parameters: {'observation_period_num': 152, 'train_rates': 0.9356261618163242, 'learning_rate': 1.8428027809287463e-05, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8031461996905307}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:11:06,645][0m Trial 24 finished with value: 0.07919038832187653 and parameters: {'observation_period_num': 77, 'train_rates': 0.987217000822719, 'learning_rate': 0.0001186939353944148, 'batch_size': 191, 'step_size': 15, 'gamma': 0.7754530076511363}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:11:34,152][0m Trial 25 finished with value: 0.13427041989663688 and parameters: {'observation_period_num': 80, 'train_rates': 0.7480025191824875, 'learning_rate': 8.711193026711719e-05, 'batch_size': 194, 'step_size': 15, 'gamma': 0.7568410461505184}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:11:58,532][0m Trial 26 finished with value: 0.1092158842503148 and parameters: {'observation_period_num': 35, 'train_rates': 0.8602140524179104, 'learning_rate': 0.0004668344658703771, 'batch_size': 230, 'step_size': 15, 'gamma': 0.8473920345635881}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:12:30,975][0m Trial 27 finished with value: 0.33902958035469055 and parameters: {'observation_period_num': 80, 'train_rates': 0.9673904298557511, 'learning_rate': 0.0003080292841855944, 'batch_size': 186, 'step_size': 13, 'gamma': 0.8074431358816854}. Best is trial 14 with value: 0.0632578581571579.[0m
Early stopping at epoch 48
[32m[I 2025-01-30 22:12:55,453][0m Trial 28 finished with value: 0.262794761130443 and parameters: {'observation_period_num': 27, 'train_rates': 0.9289644079144515, 'learning_rate': 0.0001221811570491044, 'batch_size': 120, 'step_size': 1, 'gamma': 0.7743595882533998}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:13:21,899][0m Trial 29 finished with value: 0.1860799060119846 and parameters: {'observation_period_num': 120, 'train_rates': 0.8697660145671036, 'learning_rate': 5.6314375991908164e-05, 'batch_size': 215, 'step_size': 13, 'gamma': 0.7525665502270522}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:13:45,375][0m Trial 30 finished with value: 0.3017013788602914 and parameters: {'observation_period_num': 153, 'train_rates': 0.8319455442596303, 'learning_rate': 2.0163939196257058e-05, 'batch_size': 234, 'step_size': 7, 'gamma': 0.8818760401087086}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:14:25,715][0m Trial 31 finished with value: 0.07239525765180588 and parameters: {'observation_period_num': 80, 'train_rates': 0.9870061098925038, 'learning_rate': 0.00017030850233434926, 'batch_size': 156, 'step_size': 9, 'gamma': 0.7873573827982637}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:15:06,163][0m Trial 32 finished with value: 0.07351434975862503 and parameters: {'observation_period_num': 83, 'train_rates': 0.9846866508082391, 'learning_rate': 0.00014654979094346586, 'batch_size': 149, 'step_size': 9, 'gamma': 0.7931779828739249}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:15:46,924][0m Trial 33 finished with value: 0.2825744152069092 and parameters: {'observation_period_num': 111, 'train_rates': 0.9532849420913525, 'learning_rate': 0.0006210292657396399, 'batch_size': 149, 'step_size': 10, 'gamma': 0.8142189667356395}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:16:36,373][0m Trial 34 finished with value: 0.1519583471119404 and parameters: {'observation_period_num': 44, 'train_rates': 0.9055682214610484, 'learning_rate': 0.0002651945699876801, 'batch_size': 118, 'step_size': 9, 'gamma': 0.8984760098441102}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:17:08,858][0m Trial 35 finished with value: 0.21901518183777918 and parameters: {'observation_period_num': 125, 'train_rates': 0.9250515386361734, 'learning_rate': 0.00016035028867049392, 'batch_size': 179, 'step_size': 7, 'gamma': 0.7912615627501828}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:18:17,877][0m Trial 36 finished with value: 0.1355834829064461 and parameters: {'observation_period_num': 83, 'train_rates': 0.7677254579843878, 'learning_rate': 3.272079860558926e-05, 'batch_size': 73, 'step_size': 9, 'gamma': 0.8407268622763168}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:19:07,164][0m Trial 37 finished with value: 0.16460533764213323 and parameters: {'observation_period_num': 143, 'train_rates': 0.8932192364489592, 'learning_rate': 0.0006793475715503675, 'batch_size': 111, 'step_size': 7, 'gamma': 0.7660282438696613}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:19:31,493][0m Trial 38 finished with value: 0.3360757529735565 and parameters: {'observation_period_num': 108, 'train_rates': 0.9641614507449752, 'learning_rate': 0.0003250381172141539, 'batch_size': 254, 'step_size': 9, 'gamma': 0.9471201318697011}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:20:14,933][0m Trial 39 finished with value: 0.14708759282020073 and parameters: {'observation_period_num': 87, 'train_rates': 0.8897474216336145, 'learning_rate': 0.00018091095682079957, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8199042898366322}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:20:53,429][0m Trial 40 finished with value: 0.19444756789339912 and parameters: {'observation_period_num': 69, 'train_rates': 0.9188182848769008, 'learning_rate': 7.62454195569317e-05, 'batch_size': 154, 'step_size': 5, 'gamma': 0.8621513973573801}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:21:36,002][0m Trial 41 finished with value: 0.08150744438171387 and parameters: {'observation_period_num': 72, 'train_rates': 0.9870736267071994, 'learning_rate': 0.00014871218280641522, 'batch_size': 146, 'step_size': 14, 'gamma': 0.7895265641107919}. Best is trial 14 with value: 0.0632578581571579.[0m
Early stopping at epoch 90
[32m[I 2025-01-30 22:22:05,751][0m Trial 42 finished with value: 0.45705872774124146 and parameters: {'observation_period_num': 119, 'train_rates': 0.9648346158507622, 'learning_rate': 0.00032322301593055666, 'batch_size': 193, 'step_size': 2, 'gamma': 0.7703882308436627}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:22:38,371][0m Trial 43 finished with value: 0.21387898921966553 and parameters: {'observation_period_num': 44, 'train_rates': 0.9420699832664589, 'learning_rate': 0.0006384018053560763, 'batch_size': 183, 'step_size': 12, 'gamma': 0.8114596796180397}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:23:08,874][0m Trial 44 finished with value: 0.0665023922920227 and parameters: {'observation_period_num': 57, 'train_rates': 0.9897179940117544, 'learning_rate': 0.0001290959228041385, 'batch_size': 213, 'step_size': 5, 'gamma': 0.7877399291925553}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:23:31,048][0m Trial 45 finished with value: 0.22740632300892621 and parameters: {'observation_period_num': 55, 'train_rates': 0.7080239333257742, 'learning_rate': 6.280427557711326e-05, 'batch_size': 239, 'step_size': 3, 'gamma': 0.7948368962351423}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:24:01,613][0m Trial 46 finished with value: 0.4208371043205261 and parameters: {'observation_period_num': 28, 'train_rates': 0.9705497003602613, 'learning_rate': 4.042687832256215e-05, 'batch_size': 210, 'step_size': 5, 'gamma': 0.8238882372104765}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:24:29,911][0m Trial 47 finished with value: 0.48491260409355164 and parameters: {'observation_period_num': 59, 'train_rates': 0.9421427795940484, 'learning_rate': 9.325468841594347e-06, 'batch_size': 225, 'step_size': 4, 'gamma': 0.836663789860386}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:24:58,605][0m Trial 48 finished with value: 0.11005589605247887 and parameters: {'observation_period_num': 90, 'train_rates': 0.6648482589631552, 'learning_rate': 0.0002067574652919995, 'batch_size': 168, 'step_size': 6, 'gamma': 0.787944351379267}. Best is trial 14 with value: 0.0632578581571579.[0m
[32m[I 2025-01-30 22:25:53,989][0m Trial 49 finished with value: 0.18479997457205494 and parameters: {'observation_period_num': 135, 'train_rates': 0.9159911624152588, 'learning_rate': 0.00024500011686409966, 'batch_size': 102, 'step_size': 9, 'gamma': 0.7569214084174097}. Best is trial 14 with value: 0.0632578581571579.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-30 22:25:53,999][0m A new study created in memory with name: no-name-96ee2324-fc45-41c9-8f4d-1a16cc0d61af[0m
[32m[I 2025-01-30 22:26:55,469][0m Trial 0 finished with value: 0.3691689056335109 and parameters: {'observation_period_num': 89, 'train_rates': 0.9649687700964127, 'learning_rate': 3.0891000053628386e-05, 'batch_size': 97, 'step_size': 15, 'gamma': 0.9823262525389074}. Best is trial 0 with value: 0.3691689056335109.[0m
[32m[I 2025-01-30 22:27:47,142][0m Trial 1 finished with value: 0.6536964356803434 and parameters: {'observation_period_num': 99, 'train_rates': 0.782152697595802, 'learning_rate': 1.3431695370791392e-06, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9227606285783034}. Best is trial 0 with value: 0.3691689056335109.[0m
[32m[I 2025-01-30 22:28:49,440][0m Trial 2 finished with value: 0.4328433573246002 and parameters: {'observation_period_num': 129, 'train_rates': 0.9860233276417094, 'learning_rate': 4.834152982696804e-06, 'batch_size': 94, 'step_size': 4, 'gamma': 0.7998056456342953}. Best is trial 0 with value: 0.3691689056335109.[0m
[32m[I 2025-01-30 22:29:23,051][0m Trial 3 finished with value: 0.1517167709491871 and parameters: {'observation_period_num': 53, 'train_rates': 0.8418424494276339, 'learning_rate': 2.9137914647425814e-05, 'batch_size': 170, 'step_size': 8, 'gamma': 0.912293787613573}. Best is trial 3 with value: 0.1517167709491871.[0m
[32m[I 2025-01-30 22:30:15,975][0m Trial 4 finished with value: 0.14857126310384472 and parameters: {'observation_period_num': 197, 'train_rates': 0.8050925576098942, 'learning_rate': 0.0006171596971419303, 'batch_size': 95, 'step_size': 11, 'gamma': 0.8117061153180682}. Best is trial 4 with value: 0.14857126310384472.[0m
[32m[I 2025-01-30 22:33:05,578][0m Trial 5 finished with value: 0.5402039348277275 and parameters: {'observation_period_num': 133, 'train_rates': 0.9211466125443242, 'learning_rate': 1.2772727764743359e-06, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8420497037298876}. Best is trial 4 with value: 0.14857126310384472.[0m
[32m[I 2025-01-30 22:34:25,265][0m Trial 6 finished with value: 0.09113876443994084 and parameters: {'observation_period_num': 14, 'train_rates': 0.6877977054960155, 'learning_rate': 0.00029924614327859854, 'batch_size': 58, 'step_size': 1, 'gamma': 0.9150083611438741}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:34:55,817][0m Trial 7 finished with value: 0.37937095761299133 and parameters: {'observation_period_num': 214, 'train_rates': 0.9590469581864176, 'learning_rate': 0.00015712820918290768, 'batch_size': 191, 'step_size': 11, 'gamma': 0.9818934746459805}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:35:26,532][0m Trial 8 finished with value: 0.1737495601851229 and parameters: {'observation_period_num': 91, 'train_rates': 0.71390097977509, 'learning_rate': 2.4340722484131563e-05, 'batch_size': 159, 'step_size': 10, 'gamma': 0.9433311578230216}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:35:46,341][0m Trial 9 finished with value: 0.17820490250047646 and parameters: {'observation_period_num': 172, 'train_rates': 0.6759954021988871, 'learning_rate': 0.0007938094285707716, 'batch_size': 238, 'step_size': 4, 'gamma': 0.9229015663888022}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:38:29,901][0m Trial 10 finished with value: 0.10797929958502452 and parameters: {'observation_period_num': 14, 'train_rates': 0.6169901848736277, 'learning_rate': 0.0001830290869201123, 'batch_size': 26, 'step_size': 1, 'gamma': 0.8636385273073828}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:41:21,846][0m Trial 11 finished with value: 0.09119538197664395 and parameters: {'observation_period_num': 11, 'train_rates': 0.6223109162439189, 'learning_rate': 0.00012444922247712136, 'batch_size': 25, 'step_size': 1, 'gamma': 0.8706223207228347}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:42:50,448][0m Trial 12 finished with value: 0.12582821046683704 and parameters: {'observation_period_num': 5, 'train_rates': 0.6000115025223047, 'learning_rate': 0.0001391356845850858, 'batch_size': 49, 'step_size': 1, 'gamma': 0.8688650487960246}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:44:07,950][0m Trial 13 finished with value: 0.11354845482243323 and parameters: {'observation_period_num': 39, 'train_rates': 0.6868441621174676, 'learning_rate': 0.00030560649235261955, 'batch_size': 59, 'step_size': 4, 'gamma': 0.7561344338292808}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:45:21,379][0m Trial 14 finished with value: 0.09990457020474797 and parameters: {'observation_period_num': 50, 'train_rates': 0.7376619539302489, 'learning_rate': 7.765654066507357e-05, 'batch_size': 66, 'step_size': 6, 'gamma': 0.8861274910149527}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:48:37,890][0m Trial 15 finished with value: 0.10787141707237469 and parameters: {'observation_period_num': 39, 'train_rates': 0.647588266786339, 'learning_rate': 5.135449229285778e-05, 'batch_size': 22, 'step_size': 2, 'gamma': 0.8324079607835807}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:49:14,570][0m Trial 16 finished with value: 0.3063804999855924 and parameters: {'observation_period_num': 69, 'train_rates': 0.7483088661097796, 'learning_rate': 1.0202264207007565e-05, 'batch_size': 137, 'step_size': 3, 'gamma': 0.8974767262843788}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:50:17,118][0m Trial 17 finished with value: 0.18613588329232902 and parameters: {'observation_period_num': 242, 'train_rates': 0.6555689373571164, 'learning_rate': 0.0003651401853804952, 'batch_size': 68, 'step_size': 7, 'gamma': 0.9586954846407336}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:50:37,656][0m Trial 18 finished with value: 0.13538622467958628 and parameters: {'observation_period_num': 20, 'train_rates': 0.6345160318371472, 'learning_rate': 8.668792444480497e-05, 'batch_size': 234, 'step_size': 6, 'gamma': 0.9459927021776174}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:51:14,845][0m Trial 19 finished with value: 0.13737096590696327 and parameters: {'observation_period_num': 160, 'train_rates': 0.7032504168998019, 'learning_rate': 0.0003739862593652983, 'batch_size': 124, 'step_size': 2, 'gamma': 0.8890999694114711}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:55:49,703][0m Trial 20 finished with value: 0.12951348436639665 and parameters: {'observation_period_num': 73, 'train_rates': 0.8696197024730606, 'learning_rate': 0.0009831405173288392, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8480093597502965}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:56:53,814][0m Trial 21 finished with value: 0.0998276292346418 and parameters: {'observation_period_num': 31, 'train_rates': 0.7397037047528888, 'learning_rate': 5.890291769145911e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.8889842438020927}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 22:58:47,989][0m Trial 22 finished with value: 0.13606432516037484 and parameters: {'observation_period_num': 33, 'train_rates': 0.75991787641329, 'learning_rate': 1.3836278392474749e-05, 'batch_size': 43, 'step_size': 5, 'gamma': 0.9052146478580726}. Best is trial 6 with value: 0.09113876443994084.[0m
Early stopping at epoch 99
[32m[I 2025-01-30 22:59:49,509][0m Trial 23 finished with value: 0.16178347525785222 and parameters: {'observation_period_num': 18, 'train_rates': 0.7179053919971129, 'learning_rate': 6.247372159782585e-05, 'batch_size': 79, 'step_size': 1, 'gamma': 0.8758740877618187}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 23:00:29,922][0m Trial 24 finished with value: 0.12329551106394265 and parameters: {'observation_period_num': 66, 'train_rates': 0.6660987643748861, 'learning_rate': 0.00021865555576426848, 'batch_size': 120, 'step_size': 3, 'gamma': 0.8553983191272001}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 23:02:25,995][0m Trial 25 finished with value: 0.15084217397492725 and parameters: {'observation_period_num': 109, 'train_rates': 0.8060926545799503, 'learning_rate': 0.0001042231319821467, 'batch_size': 43, 'step_size': 2, 'gamma': 0.820731675482833}. Best is trial 6 with value: 0.09113876443994084.[0m
[32m[I 2025-01-30 23:03:25,104][0m Trial 26 finished with value: 0.08200720952784053 and parameters: {'observation_period_num': 5, 'train_rates': 0.6336534506430781, 'learning_rate': 4.726001183509952e-05, 'batch_size': 77, 'step_size': 3, 'gamma': 0.9405861885925342}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:04:56,378][0m Trial 27 finished with value: 0.12585082543363862 and parameters: {'observation_period_num': 5, 'train_rates': 0.6234694085121449, 'learning_rate': 1.596962088087564e-05, 'batch_size': 48, 'step_size': 3, 'gamma': 0.9345807993870923}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:05:50,242][0m Trial 28 finished with value: 0.12989697674975584 and parameters: {'observation_period_num': 58, 'train_rates': 0.6060777647637949, 'learning_rate': 0.0005321105914326208, 'batch_size': 81, 'step_size': 1, 'gamma': 0.9643834503762367}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:06:33,326][0m Trial 29 finished with value: 0.3339987621223143 and parameters: {'observation_period_num': 77, 'train_rates': 0.645460589425744, 'learning_rate': 6.281281485203608e-06, 'batch_size': 106, 'step_size': 2, 'gamma': 0.9728606242848509}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:08:48,430][0m Trial 30 finished with value: 0.09030954528313417 and parameters: {'observation_period_num': 28, 'train_rates': 0.6887756117228631, 'learning_rate': 4.197510640425661e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9221708483069357}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:13:29,955][0m Trial 31 finished with value: 0.09094380997202495 and parameters: {'observation_period_num': 22, 'train_rates': 0.6866130506939906, 'learning_rate': 3.975417720251278e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9173998336044022}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:14:51,261][0m Trial 32 finished with value: 0.10818902797376116 and parameters: {'observation_period_num': 28, 'train_rates': 0.6922366723035328, 'learning_rate': 2.8620010634481143e-05, 'batch_size': 57, 'step_size': 14, 'gamma': 0.9230247555586928}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:17:12,705][0m Trial 33 finished with value: 0.1847195341310735 and parameters: {'observation_period_num': 44, 'train_rates': 0.7825624936281231, 'learning_rate': 2.494670233850476e-06, 'batch_size': 35, 'step_size': 13, 'gamma': 0.9365105623476409}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:19:07,389][0m Trial 34 finished with value: 0.15444503954425062 and parameters: {'observation_period_num': 107, 'train_rates': 0.6649606859613272, 'learning_rate': 4.206761980850598e-05, 'batch_size': 38, 'step_size': 13, 'gamma': 0.9169325009686832}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:23:40,393][0m Trial 35 finished with value: 0.08676611758713909 and parameters: {'observation_period_num': 27, 'train_rates': 0.7204485886067048, 'learning_rate': 2.0741428685033086e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9549083237267678}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:28:09,099][0m Trial 36 finished with value: 0.11620785881817053 and parameters: {'observation_period_num': 52, 'train_rates': 0.7149770092949524, 'learning_rate': 3.949351670917415e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9570051155923689}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:33:15,032][0m Trial 37 finished with value: 0.10359392765114146 and parameters: {'observation_period_num': 26, 'train_rates': 0.8480336534334973, 'learning_rate': 1.9899727376392198e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.981412333863704}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:34:11,015][0m Trial 38 finished with value: 0.20483696109847055 and parameters: {'observation_period_num': 86, 'train_rates': 0.7658981787456839, 'learning_rate': 7.380041453929735e-06, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9886638624689716}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:34:56,606][0m Trial 39 finished with value: 0.14706230890151725 and parameters: {'observation_period_num': 128, 'train_rates': 0.7287534802742006, 'learning_rate': 3.3143477055999065e-05, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9500266856989574}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:37:12,812][0m Trial 40 finished with value: 0.2063094453299837 and parameters: {'observation_period_num': 55, 'train_rates': 0.677502524450683, 'learning_rate': 2.9039413460060306e-06, 'batch_size': 33, 'step_size': 12, 'gamma': 0.9312032636059526}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:38:31,761][0m Trial 41 finished with value: 0.10446037600012538 and parameters: {'observation_period_num': 21, 'train_rates': 0.7011671593209068, 'learning_rate': 2.1619760067471687e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9038559685402577}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:40:03,945][0m Trial 42 finished with value: 0.0839189597536223 and parameters: {'observation_period_num': 16, 'train_rates': 0.6869036629014408, 'learning_rate': 4.563586894865608e-05, 'batch_size': 50, 'step_size': 10, 'gamma': 0.9152248897793507}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:42:15,358][0m Trial 43 finished with value: 0.12526665907619003 and parameters: {'observation_period_num': 41, 'train_rates': 0.6387100011709833, 'learning_rate': 1.1356281673954702e-05, 'batch_size': 33, 'step_size': 10, 'gamma': 0.9254755960506906}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:43:19,764][0m Trial 44 finished with value: 0.0820689362527664 and parameters: {'observation_period_num': 8, 'train_rates': 0.6710640159749514, 'learning_rate': 4.099073869462405e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.9661837341415933}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:44:24,007][0m Trial 45 finished with value: 0.09447729783420711 and parameters: {'observation_period_num': 7, 'train_rates': 0.6705430204377396, 'learning_rate': 1.848459295351018e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.9700236767218017}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:45:12,110][0m Trial 46 finished with value: 0.10140691927699844 and parameters: {'observation_period_num': 35, 'train_rates': 0.6548858724322916, 'learning_rate': 2.695678436039375e-05, 'batch_size': 94, 'step_size': 12, 'gamma': 0.9506103779028936}. Best is trial 26 with value: 0.08200720952784053.[0m
[32m[I 2025-01-30 23:46:41,017][0m Trial 47 finished with value: 0.07493786981890192 and parameters: {'observation_period_num': 12, 'train_rates': 0.7254248577132956, 'learning_rate': 7.495406113413543e-05, 'batch_size': 54, 'step_size': 9, 'gamma': 0.9402057922558356}. Best is trial 47 with value: 0.07493786981890192.[0m
[32m[I 2025-01-30 23:47:10,038][0m Trial 48 finished with value: 0.0987273739379853 and parameters: {'observation_period_num': 11, 'train_rates': 0.815396779887346, 'learning_rate': 8.335887000838331e-05, 'batch_size': 191, 'step_size': 8, 'gamma': 0.9403742078874925}. Best is trial 47 with value: 0.07493786981890192.[0m
[32m[I 2025-01-30 23:48:44,443][0m Trial 49 finished with value: 0.07990621049407536 and parameters: {'observation_period_num': 15, 'train_rates': 0.7605751110466816, 'learning_rate': 6.906655523290149e-05, 'batch_size': 53, 'step_size': 9, 'gamma': 0.9763697246664117}. Best is trial 47 with value: 0.07493786981890192.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 9, 'train_rates': 0.6203078957538845, 'learning_rate': 6.708154129662126e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9153589454735283}
Epoch 1/300, trend Loss: 0.7269 | 0.7051
Epoch 2/300, trend Loss: 0.2648 | 0.3911
Epoch 3/300, trend Loss: 0.2113 | 0.2511
Epoch 4/300, trend Loss: 0.1829 | 0.1965
Epoch 5/300, trend Loss: 0.1718 | 0.1762
Epoch 6/300, trend Loss: 0.1653 | 0.1637
Epoch 7/300, trend Loss: 0.1594 | 0.1571
Epoch 8/300, trend Loss: 0.1544 | 0.1525
Epoch 9/300, trend Loss: 0.1502 | 0.1504
Epoch 10/300, trend Loss: 0.1462 | 0.1496
Epoch 11/300, trend Loss: 0.1424 | 0.1489
Epoch 12/300, trend Loss: 0.1390 | 0.1491
Epoch 13/300, trend Loss: 0.1359 | 0.1493
Epoch 14/300, trend Loss: 0.1332 | 0.1493
Epoch 15/300, trend Loss: 0.1307 | 0.1527
Epoch 16/300, trend Loss: 0.1285 | 0.1533
Epoch 17/300, trend Loss: 0.1268 | 0.1542
Epoch 18/300, trend Loss: 0.1252 | 0.1549
Epoch 19/300, trend Loss: 0.1237 | 0.1610
Epoch 20/300, trend Loss: 0.1223 | 0.1613
Epoch 21/300, trend Loss: 0.1212 | 0.1622
Epoch 22/300, trend Loss: 0.1199 | 0.1679
Epoch 23/300, trend Loss: 0.1189 | 0.1675
Epoch 24/300, trend Loss: 0.1180 | 0.1675
Epoch 25/300, trend Loss: 0.1171 | 0.1667
Epoch 26/300, trend Loss: 0.1162 | 0.1704
Epoch 27/300, trend Loss: 0.1154 | 0.1678
Epoch 28/300, trend Loss: 0.1147 | 0.1660
Epoch 29/300, trend Loss: 0.1140 | 0.1677
Epoch 30/300, trend Loss: 0.1133 | 0.1641
Epoch 31/300, trend Loss: 0.1127 | 0.1614
Epoch 32/300, trend Loss: 0.1121 | 0.1579
Epoch 33/300, trend Loss: 0.1116 | 0.1576
Epoch 34/300, trend Loss: 0.1110 | 0.1533
Epoch 35/300, trend Loss: 0.1106 | 0.1501
Epoch 36/300, trend Loss: 0.1101 | 0.1489
Epoch 37/300, trend Loss: 0.1096 | 0.1449
Epoch 38/300, trend Loss: 0.1092 | 0.1418
Epoch 39/300, trend Loss: 0.1088 | 0.1383
Epoch 40/300, trend Loss: 0.1084 | 0.1367
Epoch 41/300, trend Loss: 0.1080 | 0.1331
Epoch 42/300, trend Loss: 0.1077 | 0.1304
Epoch 43/300, trend Loss: 0.1073 | 0.1287
Epoch 44/300, trend Loss: 0.1070 | 0.1258
Epoch 45/300, trend Loss: 0.1067 | 0.1234
Epoch 46/300, trend Loss: 0.1063 | 0.1208
Epoch 47/300, trend Loss: 0.1060 | 0.1193
Epoch 48/300, trend Loss: 0.1057 | 0.1169
Epoch 49/300, trend Loss: 0.1054 | 0.1150
Epoch 50/300, trend Loss: 0.1051 | 0.1135
Epoch 51/300, trend Loss: 0.1048 | 0.1116
Epoch 52/300, trend Loss: 0.1045 | 0.1099
Epoch 53/300, trend Loss: 0.1043 | 0.1083
Epoch 54/300, trend Loss: 0.1040 | 0.1071
Epoch 55/300, trend Loss: 0.1037 | 0.1056
Epoch 56/300, trend Loss: 0.1034 | 0.1042
Epoch 57/300, trend Loss: 0.1031 | 0.1032
Epoch 58/300, trend Loss: 0.1029 | 0.1020
Epoch 59/300, trend Loss: 0.1026 | 0.1008
Epoch 60/300, trend Loss: 0.1024 | 0.0997
Epoch 61/300, trend Loss: 0.1021 | 0.0988
Epoch 62/300, trend Loss: 0.1018 | 0.0979
Epoch 63/300, trend Loss: 0.1016 | 0.0969
Epoch 64/300, trend Loss: 0.1013 | 0.0962
Epoch 65/300, trend Loss: 0.1011 | 0.0954
Epoch 66/300, trend Loss: 0.1009 | 0.0946
Epoch 67/300, trend Loss: 0.1006 | 0.0939
Epoch 68/300, trend Loss: 0.1004 | 0.0932
Epoch 69/300, trend Loss: 0.1001 | 0.0926
Epoch 70/300, trend Loss: 0.0999 | 0.0919
Epoch 71/300, trend Loss: 0.0997 | 0.0914
Epoch 72/300, trend Loss: 0.0995 | 0.0908
Epoch 73/300, trend Loss: 0.0993 | 0.0903
Epoch 74/300, trend Loss: 0.0991 | 0.0898
Epoch 75/300, trend Loss: 0.0989 | 0.0893
Epoch 76/300, trend Loss: 0.0987 | 0.0889
Epoch 77/300, trend Loss: 0.0985 | 0.0884
Epoch 78/300, trend Loss: 0.0983 | 0.0880
Epoch 79/300, trend Loss: 0.0981 | 0.0876
Epoch 80/300, trend Loss: 0.0979 | 0.0872
Epoch 81/300, trend Loss: 0.0978 | 0.0868
Epoch 82/300, trend Loss: 0.0976 | 0.0865
Epoch 83/300, trend Loss: 0.0974 | 0.0862
Epoch 84/300, trend Loss: 0.0973 | 0.0858
Epoch 85/300, trend Loss: 0.0971 | 0.0855
Epoch 86/300, trend Loss: 0.0970 | 0.0852
Epoch 87/300, trend Loss: 0.0968 | 0.0850
Epoch 88/300, trend Loss: 0.0967 | 0.0847
Epoch 89/300, trend Loss: 0.0965 | 0.0845
Epoch 90/300, trend Loss: 0.0964 | 0.0842
Epoch 91/300, trend Loss: 0.0963 | 0.0840
Epoch 92/300, trend Loss: 0.0962 | 0.0838
Epoch 93/300, trend Loss: 0.0960 | 0.0836
Epoch 94/300, trend Loss: 0.0959 | 0.0834
Epoch 95/300, trend Loss: 0.0958 | 0.0832
Epoch 96/300, trend Loss: 0.0957 | 0.0830
Epoch 97/300, trend Loss: 0.0956 | 0.0828
Epoch 98/300, trend Loss: 0.0955 | 0.0826
Epoch 99/300, trend Loss: 0.0954 | 0.0825
Epoch 100/300, trend Loss: 0.0953 | 0.0823
Epoch 101/300, trend Loss: 0.0952 | 0.0822
Epoch 102/300, trend Loss: 0.0951 | 0.0820
Epoch 103/300, trend Loss: 0.0950 | 0.0819
Epoch 104/300, trend Loss: 0.0949 | 0.0818
Epoch 105/300, trend Loss: 0.0949 | 0.0816
Epoch 106/300, trend Loss: 0.0948 | 0.0815
Epoch 107/300, trend Loss: 0.0947 | 0.0814
Epoch 108/300, trend Loss: 0.0946 | 0.0812
Epoch 109/300, trend Loss: 0.0945 | 0.0811
Epoch 110/300, trend Loss: 0.0945 | 0.0810
Epoch 111/300, trend Loss: 0.0944 | 0.0809
Epoch 112/300, trend Loss: 0.0943 | 0.0808
Epoch 113/300, trend Loss: 0.0942 | 0.0807
Epoch 114/300, trend Loss: 0.0942 | 0.0806
Epoch 115/300, trend Loss: 0.0941 | 0.0805
Epoch 116/300, trend Loss: 0.0940 | 0.0803
Epoch 117/300, trend Loss: 0.0940 | 0.0803
Epoch 118/300, trend Loss: 0.0939 | 0.0802
Epoch 119/300, trend Loss: 0.0939 | 0.0801
Epoch 120/300, trend Loss: 0.0938 | 0.0800
Epoch 121/300, trend Loss: 0.0937 | 0.0799
Epoch 122/300, trend Loss: 0.0937 | 0.0798
Epoch 123/300, trend Loss: 0.0936 | 0.0797
Epoch 124/300, trend Loss: 0.0936 | 0.0796
Epoch 125/300, trend Loss: 0.0935 | 0.0795
Epoch 126/300, trend Loss: 0.0935 | 0.0795
Epoch 127/300, trend Loss: 0.0934 | 0.0794
Epoch 128/300, trend Loss: 0.0934 | 0.0793
Epoch 129/300, trend Loss: 0.0933 | 0.0793
Epoch 130/300, trend Loss: 0.0933 | 0.0792
Epoch 131/300, trend Loss: 0.0932 | 0.0791
Epoch 132/300, trend Loss: 0.0932 | 0.0791
Epoch 133/300, trend Loss: 0.0932 | 0.0790
Epoch 134/300, trend Loss: 0.0931 | 0.0790
Epoch 135/300, trend Loss: 0.0931 | 0.0789
Epoch 136/300, trend Loss: 0.0930 | 0.0789
Epoch 137/300, trend Loss: 0.0930 | 0.0788
Epoch 138/300, trend Loss: 0.0930 | 0.0788
Epoch 139/300, trend Loss: 0.0929 | 0.0787
Epoch 140/300, trend Loss: 0.0929 | 0.0787
Epoch 141/300, trend Loss: 0.0928 | 0.0786
Epoch 142/300, trend Loss: 0.0928 | 0.0786
Epoch 143/300, trend Loss: 0.0928 | 0.0786
Epoch 144/300, trend Loss: 0.0927 | 0.0785
Epoch 145/300, trend Loss: 0.0927 | 0.0785
Epoch 146/300, trend Loss: 0.0927 | 0.0785
Epoch 147/300, trend Loss: 0.0927 | 0.0784
Epoch 148/300, trend Loss: 0.0926 | 0.0784
Epoch 149/300, trend Loss: 0.0926 | 0.0784
Epoch 150/300, trend Loss: 0.0926 | 0.0783
Epoch 151/300, trend Loss: 0.0925 | 0.0783
Epoch 152/300, trend Loss: 0.0925 | 0.0783
Epoch 153/300, trend Loss: 0.0925 | 0.0782
Epoch 154/300, trend Loss: 0.0925 | 0.0782
Epoch 155/300, trend Loss: 0.0924 | 0.0782
Epoch 156/300, trend Loss: 0.0924 | 0.0782
Epoch 157/300, trend Loss: 0.0924 | 0.0781
Epoch 158/300, trend Loss: 0.0924 | 0.0781
Epoch 159/300, trend Loss: 0.0923 | 0.0781
Epoch 160/300, trend Loss: 0.0923 | 0.0781
Epoch 161/300, trend Loss: 0.0923 | 0.0780
Epoch 162/300, trend Loss: 0.0923 | 0.0780
Epoch 163/300, trend Loss: 0.0923 | 0.0780
Epoch 164/300, trend Loss: 0.0922 | 0.0780
Epoch 165/300, trend Loss: 0.0922 | 0.0779
Epoch 166/300, trend Loss: 0.0922 | 0.0779
Epoch 167/300, trend Loss: 0.0922 | 0.0779
Epoch 168/300, trend Loss: 0.0922 | 0.0779
Epoch 169/300, trend Loss: 0.0921 | 0.0779
Epoch 170/300, trend Loss: 0.0921 | 0.0778
Epoch 171/300, trend Loss: 0.0921 | 0.0778
Epoch 172/300, trend Loss: 0.0921 | 0.0778
Epoch 173/300, trend Loss: 0.0921 | 0.0778
Epoch 174/300, trend Loss: 0.0921 | 0.0778
Epoch 175/300, trend Loss: 0.0920 | 0.0778
Epoch 176/300, trend Loss: 0.0920 | 0.0778
Epoch 177/300, trend Loss: 0.0920 | 0.0777
Epoch 178/300, trend Loss: 0.0920 | 0.0777
Epoch 179/300, trend Loss: 0.0920 | 0.0777
Epoch 180/300, trend Loss: 0.0920 | 0.0777
Epoch 181/300, trend Loss: 0.0920 | 0.0777
Epoch 182/300, trend Loss: 0.0919 | 0.0777
Epoch 183/300, trend Loss: 0.0919 | 0.0777
Epoch 184/300, trend Loss: 0.0919 | 0.0776
Epoch 185/300, trend Loss: 0.0919 | 0.0776
Epoch 186/300, trend Loss: 0.0919 | 0.0776
Epoch 187/300, trend Loss: 0.0919 | 0.0776
Epoch 188/300, trend Loss: 0.0919 | 0.0776
Epoch 189/300, trend Loss: 0.0919 | 0.0776
Epoch 190/300, trend Loss: 0.0919 | 0.0776
Epoch 191/300, trend Loss: 0.0918 | 0.0776
Epoch 192/300, trend Loss: 0.0918 | 0.0776
Epoch 193/300, trend Loss: 0.0918 | 0.0775
Epoch 194/300, trend Loss: 0.0918 | 0.0775
Epoch 195/300, trend Loss: 0.0918 | 0.0775
Epoch 196/300, trend Loss: 0.0918 | 0.0775
Epoch 197/300, trend Loss: 0.0918 | 0.0775
Epoch 198/300, trend Loss: 0.0918 | 0.0775
Epoch 199/300, trend Loss: 0.0918 | 0.0775
Epoch 200/300, trend Loss: 0.0918 | 0.0775
Epoch 201/300, trend Loss: 0.0918 | 0.0775
Epoch 202/300, trend Loss: 0.0917 | 0.0775
Epoch 203/300, trend Loss: 0.0917 | 0.0775
Epoch 204/300, trend Loss: 0.0917 | 0.0775
Epoch 205/300, trend Loss: 0.0917 | 0.0775
Epoch 206/300, trend Loss: 0.0917 | 0.0774
Epoch 207/300, trend Loss: 0.0917 | 0.0774
Epoch 208/300, trend Loss: 0.0917 | 0.0774
Epoch 209/300, trend Loss: 0.0917 | 0.0774
Epoch 210/300, trend Loss: 0.0917 | 0.0774
Epoch 211/300, trend Loss: 0.0917 | 0.0774
Epoch 212/300, trend Loss: 0.0917 | 0.0774
Epoch 213/300, trend Loss: 0.0917 | 0.0774
Epoch 214/300, trend Loss: 0.0917 | 0.0774
Epoch 215/300, trend Loss: 0.0917 | 0.0774
Epoch 216/300, trend Loss: 0.0917 | 0.0774
Epoch 217/300, trend Loss: 0.0916 | 0.0774
Epoch 218/300, trend Loss: 0.0916 | 0.0774
Epoch 219/300, trend Loss: 0.0916 | 0.0774
Epoch 220/300, trend Loss: 0.0916 | 0.0774
Epoch 221/300, trend Loss: 0.0916 | 0.0774
Epoch 222/300, trend Loss: 0.0916 | 0.0774
Epoch 223/300, trend Loss: 0.0916 | 0.0774
Epoch 224/300, trend Loss: 0.0916 | 0.0774
Epoch 225/300, trend Loss: 0.0916 | 0.0773
Epoch 226/300, trend Loss: 0.0916 | 0.0773
Epoch 227/300, trend Loss: 0.0916 | 0.0773
Epoch 228/300, trend Loss: 0.0916 | 0.0773
Epoch 229/300, trend Loss: 0.0916 | 0.0773
Epoch 230/300, trend Loss: 0.0916 | 0.0773
Epoch 231/300, trend Loss: 0.0916 | 0.0773
Epoch 232/300, trend Loss: 0.0916 | 0.0773
Epoch 233/300, trend Loss: 0.0916 | 0.0773
Epoch 234/300, trend Loss: 0.0916 | 0.0773
Epoch 235/300, trend Loss: 0.0916 | 0.0773
Epoch 236/300, trend Loss: 0.0916 | 0.0773
Epoch 237/300, trend Loss: 0.0916 | 0.0773
Epoch 238/300, trend Loss: 0.0916 | 0.0773
Epoch 239/300, trend Loss: 0.0916 | 0.0773
Epoch 240/300, trend Loss: 0.0916 | 0.0773
Epoch 241/300, trend Loss: 0.0916 | 0.0773
Epoch 242/300, trend Loss: 0.0915 | 0.0773
Epoch 243/300, trend Loss: 0.0915 | 0.0773
Epoch 244/300, trend Loss: 0.0915 | 0.0773
Epoch 245/300, trend Loss: 0.0915 | 0.0773
Epoch 246/300, trend Loss: 0.0915 | 0.0773
Epoch 247/300, trend Loss: 0.0915 | 0.0773
Epoch 248/300, trend Loss: 0.0915 | 0.0773
Epoch 249/300, trend Loss: 0.0915 | 0.0773
Epoch 250/300, trend Loss: 0.0915 | 0.0773
Epoch 251/300, trend Loss: 0.0915 | 0.0773
Epoch 252/300, trend Loss: 0.0915 | 0.0773
Epoch 253/300, trend Loss: 0.0915 | 0.0773
Epoch 254/300, trend Loss: 0.0915 | 0.0773
Epoch 255/300, trend Loss: 0.0915 | 0.0773
Epoch 256/300, trend Loss: 0.0915 | 0.0773
Epoch 257/300, trend Loss: 0.0915 | 0.0773
Epoch 258/300, trend Loss: 0.0915 | 0.0773
Epoch 259/300, trend Loss: 0.0915 | 0.0773
Epoch 260/300, trend Loss: 0.0915 | 0.0773
Epoch 261/300, trend Loss: 0.0915 | 0.0773
Epoch 262/300, trend Loss: 0.0915 | 0.0772
Epoch 263/300, trend Loss: 0.0915 | 0.0772
Epoch 264/300, trend Loss: 0.0915 | 0.0772
Epoch 265/300, trend Loss: 0.0915 | 0.0772
Epoch 266/300, trend Loss: 0.0915 | 0.0772
Epoch 267/300, trend Loss: 0.0915 | 0.0772
Epoch 268/300, trend Loss: 0.0915 | 0.0772
Epoch 269/300, trend Loss: 0.0915 | 0.0772
Epoch 270/300, trend Loss: 0.0915 | 0.0772
Epoch 271/300, trend Loss: 0.0915 | 0.0772
Epoch 272/300, trend Loss: 0.0915 | 0.0772
Epoch 273/300, trend Loss: 0.0915 | 0.0772
Epoch 274/300, trend Loss: 0.0915 | 0.0772
Epoch 275/300, trend Loss: 0.0915 | 0.0772
Epoch 276/300, trend Loss: 0.0915 | 0.0772
Epoch 277/300, trend Loss: 0.0915 | 0.0772
Epoch 278/300, trend Loss: 0.0915 | 0.0772
Epoch 279/300, trend Loss: 0.0915 | 0.0772
Epoch 280/300, trend Loss: 0.0915 | 0.0772
Epoch 281/300, trend Loss: 0.0915 | 0.0772
Epoch 282/300, trend Loss: 0.0915 | 0.0772
Epoch 283/300, trend Loss: 0.0915 | 0.0772
Epoch 284/300, trend Loss: 0.0915 | 0.0772
Epoch 285/300, trend Loss: 0.0915 | 0.0772
Epoch 286/300, trend Loss: 0.0915 | 0.0772
Epoch 287/300, trend Loss: 0.0915 | 0.0772
Epoch 288/300, trend Loss: 0.0915 | 0.0772
Epoch 289/300, trend Loss: 0.0915 | 0.0772
Epoch 290/300, trend Loss: 0.0915 | 0.0772
Epoch 291/300, trend Loss: 0.0915 | 0.0772
Epoch 292/300, trend Loss: 0.0915 | 0.0772
Epoch 293/300, trend Loss: 0.0915 | 0.0772
Epoch 294/300, trend Loss: 0.0915 | 0.0772
Epoch 295/300, trend Loss: 0.0915 | 0.0772
Epoch 296/300, trend Loss: 0.0915 | 0.0772
Epoch 297/300, trend Loss: 0.0915 | 0.0772
Epoch 298/300, trend Loss: 0.0915 | 0.0772
Epoch 299/300, trend Loss: 0.0915 | 0.0772
Epoch 300/300, trend Loss: 0.0915 | 0.0772
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.6641369197436111, 'learning_rate': 0.0008922522208266279, 'batch_size': 103, 'step_size': 14, 'gamma': 0.882968285842537}
Epoch 1/300, seasonal_0 Loss: 0.5097 | 0.2558
Epoch 2/300, seasonal_0 Loss: 0.1774 | 0.1573
Epoch 3/300, seasonal_0 Loss: 0.1449 | 0.1561
Epoch 4/300, seasonal_0 Loss: 0.1349 | 0.1500
Epoch 5/300, seasonal_0 Loss: 0.1251 | 0.1444
Epoch 6/300, seasonal_0 Loss: 0.1191 | 0.1238
Epoch 7/300, seasonal_0 Loss: 0.1131 | 0.1066
Epoch 8/300, seasonal_0 Loss: 0.1100 | 0.1020
Epoch 9/300, seasonal_0 Loss: 0.1065 | 0.0951
Epoch 10/300, seasonal_0 Loss: 0.1032 | 0.0906
Epoch 11/300, seasonal_0 Loss: 0.1006 | 0.0877
Epoch 12/300, seasonal_0 Loss: 0.0986 | 0.0859
Epoch 13/300, seasonal_0 Loss: 0.0977 | 0.1159
Epoch 14/300, seasonal_0 Loss: 0.1079 | 0.1299
Epoch 15/300, seasonal_0 Loss: 0.1048 | 0.1223
Epoch 16/300, seasonal_0 Loss: 0.1017 | 0.0985
Epoch 17/300, seasonal_0 Loss: 0.0979 | 0.1006
Epoch 18/300, seasonal_0 Loss: 0.0998 | 0.1273
Epoch 19/300, seasonal_0 Loss: 0.1178 | 0.0900
Epoch 20/300, seasonal_0 Loss: 0.0994 | 0.0992
Epoch 21/300, seasonal_0 Loss: 0.0951 | 0.0820
Epoch 22/300, seasonal_0 Loss: 0.0958 | 0.0833
Epoch 23/300, seasonal_0 Loss: 0.0925 | 0.0775
Epoch 24/300, seasonal_0 Loss: 0.0924 | 0.1043
Epoch 25/300, seasonal_0 Loss: 0.1007 | 0.0799
Epoch 26/300, seasonal_0 Loss: 0.0937 | 0.0863
Epoch 27/300, seasonal_0 Loss: 0.0932 | 0.0804
Epoch 28/300, seasonal_0 Loss: 0.0921 | 0.1013
Epoch 29/300, seasonal_0 Loss: 0.0990 | 0.0814
Epoch 30/300, seasonal_0 Loss: 0.0909 | 0.0765
Epoch 31/300, seasonal_0 Loss: 0.0881 | 0.0740
Epoch 32/300, seasonal_0 Loss: 0.0873 | 0.0826
Epoch 33/300, seasonal_0 Loss: 0.0912 | 0.0761
Epoch 34/300, seasonal_0 Loss: 0.0896 | 0.0884
Epoch 35/300, seasonal_0 Loss: 0.0960 | 0.0839
Epoch 36/300, seasonal_0 Loss: 0.0876 | 0.0747
Epoch 37/300, seasonal_0 Loss: 0.0847 | 0.0728
Epoch 38/300, seasonal_0 Loss: 0.0847 | 0.0737
Epoch 39/300, seasonal_0 Loss: 0.0852 | 0.0885
Epoch 40/300, seasonal_0 Loss: 0.0902 | 0.0802
Epoch 41/300, seasonal_0 Loss: 0.0852 | 0.0780
Epoch 42/300, seasonal_0 Loss: 0.0845 | 0.0726
Epoch 43/300, seasonal_0 Loss: 0.0851 | 0.0787
Epoch 44/300, seasonal_0 Loss: 0.0863 | 0.0752
Epoch 45/300, seasonal_0 Loss: 0.0842 | 0.0820
Epoch 46/300, seasonal_0 Loss: 0.0873 | 0.0787
Epoch 47/300, seasonal_0 Loss: 0.0830 | 0.0741
Epoch 48/300, seasonal_0 Loss: 0.0836 | 0.0714
Epoch 49/300, seasonal_0 Loss: 0.0819 | 0.0798
Epoch 50/300, seasonal_0 Loss: 0.0849 | 0.0726
Epoch 51/300, seasonal_0 Loss: 0.0806 | 0.0751
Epoch 52/300, seasonal_0 Loss: 0.0826 | 0.0699
Epoch 53/300, seasonal_0 Loss: 0.0801 | 0.0750
Epoch 54/300, seasonal_0 Loss: 0.0825 | 0.0705
Epoch 55/300, seasonal_0 Loss: 0.0805 | 0.0761
Epoch 56/300, seasonal_0 Loss: 0.0826 | 0.0708
Epoch 57/300, seasonal_0 Loss: 0.0800 | 0.0737
Epoch 58/300, seasonal_0 Loss: 0.0805 | 0.0704
Epoch 59/300, seasonal_0 Loss: 0.0794 | 0.0764
Epoch 60/300, seasonal_0 Loss: 0.0807 | 0.0707
Epoch 61/300, seasonal_0 Loss: 0.0802 | 0.0761
Epoch 62/300, seasonal_0 Loss: 0.0815 | 0.0710
Epoch 63/300, seasonal_0 Loss: 0.0808 | 0.0752
Epoch 64/300, seasonal_0 Loss: 0.0801 | 0.0721
Epoch 65/300, seasonal_0 Loss: 0.0805 | 0.0762
Epoch 66/300, seasonal_0 Loss: 0.0787 | 0.0724
Epoch 67/300, seasonal_0 Loss: 0.0789 | 0.0766
Epoch 68/300, seasonal_0 Loss: 0.0784 | 0.0717
Epoch 69/300, seasonal_0 Loss: 0.0795 | 0.0771
Epoch 70/300, seasonal_0 Loss: 0.0778 | 0.0694
Epoch 71/300, seasonal_0 Loss: 0.0783 | 0.0739
Epoch 72/300, seasonal_0 Loss: 0.0771 | 0.0688
Epoch 73/300, seasonal_0 Loss: 0.0763 | 0.0700
Epoch 74/300, seasonal_0 Loss: 0.0754 | 0.0680
Epoch 75/300, seasonal_0 Loss: 0.0752 | 0.0685
Epoch 76/300, seasonal_0 Loss: 0.0746 | 0.0666
Epoch 77/300, seasonal_0 Loss: 0.0745 | 0.0674
Epoch 78/300, seasonal_0 Loss: 0.0742 | 0.0661
Epoch 79/300, seasonal_0 Loss: 0.0741 | 0.0661
Epoch 80/300, seasonal_0 Loss: 0.0738 | 0.0656
Epoch 81/300, seasonal_0 Loss: 0.0735 | 0.0661
Epoch 82/300, seasonal_0 Loss: 0.0735 | 0.0657
Epoch 83/300, seasonal_0 Loss: 0.0732 | 0.0661
Epoch 84/300, seasonal_0 Loss: 0.0730 | 0.0659
Epoch 85/300, seasonal_0 Loss: 0.0725 | 0.0665
Epoch 86/300, seasonal_0 Loss: 0.0724 | 0.0658
Epoch 87/300, seasonal_0 Loss: 0.0720 | 0.0660
Epoch 88/300, seasonal_0 Loss: 0.0719 | 0.0656
Epoch 89/300, seasonal_0 Loss: 0.0717 | 0.0662
Epoch 90/300, seasonal_0 Loss: 0.0718 | 0.0657
Epoch 91/300, seasonal_0 Loss: 0.0715 | 0.0668
Epoch 92/300, seasonal_0 Loss: 0.0715 | 0.0658
Epoch 93/300, seasonal_0 Loss: 0.0713 | 0.0665
Epoch 94/300, seasonal_0 Loss: 0.0713 | 0.0658
Epoch 95/300, seasonal_0 Loss: 0.0712 | 0.0663
Epoch 96/300, seasonal_0 Loss: 0.0710 | 0.0660
Epoch 97/300, seasonal_0 Loss: 0.0709 | 0.0663
Epoch 98/300, seasonal_0 Loss: 0.0712 | 0.0665
Epoch 99/300, seasonal_0 Loss: 0.0716 | 0.0681
Epoch 100/300, seasonal_0 Loss: 0.0709 | 0.0673
Epoch 101/300, seasonal_0 Loss: 0.0705 | 0.0677
Epoch 102/300, seasonal_0 Loss: 0.0705 | 0.0666
Epoch 103/300, seasonal_0 Loss: 0.0700 | 0.0674
Epoch 104/300, seasonal_0 Loss: 0.0700 | 0.0665
Epoch 105/300, seasonal_0 Loss: 0.0699 | 0.0676
Epoch 106/300, seasonal_0 Loss: 0.0700 | 0.0672
Epoch 107/300, seasonal_0 Loss: 0.0699 | 0.0678
Epoch 108/300, seasonal_0 Loss: 0.0699 | 0.0675
Epoch 109/300, seasonal_0 Loss: 0.0698 | 0.0681
Epoch 110/300, seasonal_0 Loss: 0.0699 | 0.0680
Epoch 111/300, seasonal_0 Loss: 0.0700 | 0.0689
Epoch 112/300, seasonal_0 Loss: 0.0702 | 0.0684
Epoch 113/300, seasonal_0 Loss: 0.0708 | 0.0689
Epoch 114/300, seasonal_0 Loss: 0.0720 | 0.0692
Epoch 115/300, seasonal_0 Loss: 0.0728 | 0.0700
Epoch 116/300, seasonal_0 Loss: 0.0748 | 0.0732
Epoch 117/300, seasonal_0 Loss: 0.0751 | 0.0696
Epoch 118/300, seasonal_0 Loss: 0.0739 | 0.0688
Epoch 119/300, seasonal_0 Loss: 0.0718 | 0.0675
Epoch 120/300, seasonal_0 Loss: 0.0708 | 0.0681
Epoch 121/300, seasonal_0 Loss: 0.0699 | 0.0668
Epoch 122/300, seasonal_0 Loss: 0.0695 | 0.0668
Epoch 123/300, seasonal_0 Loss: 0.0693 | 0.0670
Epoch 124/300, seasonal_0 Loss: 0.0689 | 0.0669
Epoch 125/300, seasonal_0 Loss: 0.0688 | 0.0668
Epoch 126/300, seasonal_0 Loss: 0.0686 | 0.0669
Epoch 127/300, seasonal_0 Loss: 0.0685 | 0.0665
Epoch 128/300, seasonal_0 Loss: 0.0685 | 0.0668
Epoch 129/300, seasonal_0 Loss: 0.0684 | 0.0662
Epoch 130/300, seasonal_0 Loss: 0.0685 | 0.0663
Epoch 131/300, seasonal_0 Loss: 0.0686 | 0.0660
Epoch 132/300, seasonal_0 Loss: 0.0688 | 0.0662
Epoch 133/300, seasonal_0 Loss: 0.0694 | 0.0663
Epoch 134/300, seasonal_0 Loss: 0.0701 | 0.0667
Epoch 135/300, seasonal_0 Loss: 0.0721 | 0.0672
Epoch 136/300, seasonal_0 Loss: 0.0733 | 0.0676
Epoch 137/300, seasonal_0 Loss: 0.0747 | 0.0682
Epoch 138/300, seasonal_0 Loss: 0.0730 | 0.0666
Epoch 139/300, seasonal_0 Loss: 0.0718 | 0.0663
Epoch 140/300, seasonal_0 Loss: 0.0714 | 0.0661
Epoch 141/300, seasonal_0 Loss: 0.0726 | 0.0677
Epoch 142/300, seasonal_0 Loss: 0.0732 | 0.0678
Epoch 143/300, seasonal_0 Loss: 0.0725 | 0.0674
Epoch 144/300, seasonal_0 Loss: 0.0701 | 0.0664
Epoch 145/300, seasonal_0 Loss: 0.0673 | 0.0658
Epoch 146/300, seasonal_0 Loss: 0.0665 | 0.0664
Epoch 147/300, seasonal_0 Loss: 0.0662 | 0.0659
Epoch 148/300, seasonal_0 Loss: 0.0660 | 0.0659
Epoch 149/300, seasonal_0 Loss: 0.0658 | 0.0657
Epoch 150/300, seasonal_0 Loss: 0.0656 | 0.0658
Epoch 151/300, seasonal_0 Loss: 0.0654 | 0.0659
Epoch 152/300, seasonal_0 Loss: 0.0652 | 0.0659
Epoch 153/300, seasonal_0 Loss: 0.0651 | 0.0661
Epoch 154/300, seasonal_0 Loss: 0.0648 | 0.0661
Epoch 155/300, seasonal_0 Loss: 0.0645 | 0.0661
Epoch 156/300, seasonal_0 Loss: 0.0646 | 0.0663
Epoch 157/300, seasonal_0 Loss: 0.0643 | 0.0660
Epoch 158/300, seasonal_0 Loss: 0.0639 | 0.0662
Epoch 159/300, seasonal_0 Loss: 0.0634 | 0.0657
Epoch 160/300, seasonal_0 Loss: 0.0632 | 0.0666
Epoch 161/300, seasonal_0 Loss: 0.0627 | 0.0656
Epoch 162/300, seasonal_0 Loss: 0.0621 | 0.0665
Epoch 163/300, seasonal_0 Loss: 0.0618 | 0.0656
Epoch 164/300, seasonal_0 Loss: 0.0612 | 0.0668
Epoch 165/300, seasonal_0 Loss: 0.0612 | 0.0654
Epoch 166/300, seasonal_0 Loss: 0.0603 | 0.0667
Epoch 167/300, seasonal_0 Loss: 0.0600 | 0.0654
Epoch 168/300, seasonal_0 Loss: 0.0592 | 0.0662
Epoch 169/300, seasonal_0 Loss: 0.0587 | 0.0659
Epoch 170/300, seasonal_0 Loss: 0.0585 | 0.0660
Epoch 171/300, seasonal_0 Loss: 0.0656 | 0.0677
Epoch 172/300, seasonal_0 Loss: 0.0610 | 0.0657
Epoch 173/300, seasonal_0 Loss: 0.0595 | 0.0657
Epoch 174/300, seasonal_0 Loss: 0.0582 | 0.0663
Epoch 175/300, seasonal_0 Loss: 0.0580 | 0.0662
Epoch 176/300, seasonal_0 Loss: 0.0589 | 0.0663
Epoch 177/300, seasonal_0 Loss: 0.0579 | 0.0657
Epoch 178/300, seasonal_0 Loss: 0.0611 | 0.0680
Epoch 179/300, seasonal_0 Loss: 0.0617 | 0.0666
Epoch 180/300, seasonal_0 Loss: 0.0590 | 0.0658
Epoch 181/300, seasonal_0 Loss: 0.0581 | 0.0662
Epoch 182/300, seasonal_0 Loss: 0.0576 | 0.0657
Epoch 183/300, seasonal_0 Loss: 0.0573 | 0.0660
Epoch 184/300, seasonal_0 Loss: 0.0570 | 0.0656
Epoch 185/300, seasonal_0 Loss: 0.0566 | 0.0658
Epoch 186/300, seasonal_0 Loss: 0.0563 | 0.0657
Epoch 187/300, seasonal_0 Loss: 0.0560 | 0.0658
Epoch 188/300, seasonal_0 Loss: 0.0559 | 0.0658
Epoch 189/300, seasonal_0 Loss: 0.0557 | 0.0658
Epoch 190/300, seasonal_0 Loss: 0.0556 | 0.0657
Epoch 191/300, seasonal_0 Loss: 0.0555 | 0.0658
Epoch 192/300, seasonal_0 Loss: 0.0554 | 0.0657
Epoch 193/300, seasonal_0 Loss: 0.0553 | 0.0657
Epoch 194/300, seasonal_0 Loss: 0.0553 | 0.0657
Epoch 195/300, seasonal_0 Loss: 0.0552 | 0.0657
Epoch 196/300, seasonal_0 Loss: 0.0551 | 0.0657
Epoch 197/300, seasonal_0 Loss: 0.0551 | 0.0657
Epoch 198/300, seasonal_0 Loss: 0.0550 | 0.0656
Epoch 199/300, seasonal_0 Loss: 0.0550 | 0.0656
Epoch 200/300, seasonal_0 Loss: 0.0549 | 0.0656
Epoch 201/300, seasonal_0 Loss: 0.0549 | 0.0656
Epoch 202/300, seasonal_0 Loss: 0.0548 | 0.0656
Epoch 203/300, seasonal_0 Loss: 0.0548 | 0.0656
Epoch 204/300, seasonal_0 Loss: 0.0547 | 0.0656
Epoch 205/300, seasonal_0 Loss: 0.0547 | 0.0656
Epoch 206/300, seasonal_0 Loss: 0.0546 | 0.0655
Epoch 207/300, seasonal_0 Loss: 0.0546 | 0.0655
Epoch 208/300, seasonal_0 Loss: 0.0546 | 0.0655
Epoch 209/300, seasonal_0 Loss: 0.0545 | 0.0655
Epoch 210/300, seasonal_0 Loss: 0.0545 | 0.0655
Epoch 211/300, seasonal_0 Loss: 0.0545 | 0.0656
Epoch 212/300, seasonal_0 Loss: 0.0544 | 0.0654
Epoch 213/300, seasonal_0 Loss: 0.0544 | 0.0656
Epoch 214/300, seasonal_0 Loss: 0.0543 | 0.0654
Epoch 215/300, seasonal_0 Loss: 0.0543 | 0.0656
Epoch 216/300, seasonal_0 Loss: 0.0543 | 0.0654
Epoch 217/300, seasonal_0 Loss: 0.0543 | 0.0655
Epoch 218/300, seasonal_0 Loss: 0.0542 | 0.0654
Epoch 219/300, seasonal_0 Loss: 0.0542 | 0.0655
Epoch 220/300, seasonal_0 Loss: 0.0541 | 0.0654
Epoch 221/300, seasonal_0 Loss: 0.0541 | 0.0655
Epoch 222/300, seasonal_0 Loss: 0.0540 | 0.0654
Epoch 223/300, seasonal_0 Loss: 0.0540 | 0.0655
Epoch 224/300, seasonal_0 Loss: 0.0540 | 0.0654
Epoch 225/300, seasonal_0 Loss: 0.0540 | 0.0655
Epoch 226/300, seasonal_0 Loss: 0.0539 | 0.0654
Epoch 227/300, seasonal_0 Loss: 0.0539 | 0.0654
Epoch 228/300, seasonal_0 Loss: 0.0538 | 0.0654
Epoch 229/300, seasonal_0 Loss: 0.0538 | 0.0654
Epoch 230/300, seasonal_0 Loss: 0.0538 | 0.0654
Epoch 231/300, seasonal_0 Loss: 0.0538 | 0.0654
Epoch 232/300, seasonal_0 Loss: 0.0537 | 0.0654
Epoch 233/300, seasonal_0 Loss: 0.0537 | 0.0654
Epoch 234/300, seasonal_0 Loss: 0.0537 | 0.0654
Epoch 235/300, seasonal_0 Loss: 0.0536 | 0.0654
Epoch 236/300, seasonal_0 Loss: 0.0536 | 0.0654
Epoch 237/300, seasonal_0 Loss: 0.0536 | 0.0654
Epoch 238/300, seasonal_0 Loss: 0.0535 | 0.0654
Epoch 239/300, seasonal_0 Loss: 0.0535 | 0.0654
Epoch 240/300, seasonal_0 Loss: 0.0535 | 0.0654
Epoch 241/300, seasonal_0 Loss: 0.0535 | 0.0654
Epoch 242/300, seasonal_0 Loss: 0.0534 | 0.0654
Epoch 243/300, seasonal_0 Loss: 0.0534 | 0.0654
Epoch 244/300, seasonal_0 Loss: 0.0534 | 0.0654
Epoch 245/300, seasonal_0 Loss: 0.0533 | 0.0654
Epoch 246/300, seasonal_0 Loss: 0.0533 | 0.0654
Epoch 247/300, seasonal_0 Loss: 0.0533 | 0.0654
Epoch 248/300, seasonal_0 Loss: 0.0533 | 0.0654
Epoch 249/300, seasonal_0 Loss: 0.0532 | 0.0654
Epoch 250/300, seasonal_0 Loss: 0.0532 | 0.0654
Epoch 251/300, seasonal_0 Loss: 0.0532 | 0.0654
Epoch 252/300, seasonal_0 Loss: 0.0532 | 0.0654
Epoch 253/300, seasonal_0 Loss: 0.0531 | 0.0654
Epoch 254/300, seasonal_0 Loss: 0.0531 | 0.0654
Epoch 255/300, seasonal_0 Loss: 0.0531 | 0.0654
Epoch 256/300, seasonal_0 Loss: 0.0531 | 0.0654
Epoch 257/300, seasonal_0 Loss: 0.0530 | 0.0654
Epoch 258/300, seasonal_0 Loss: 0.0530 | 0.0654
Epoch 259/300, seasonal_0 Loss: 0.0530 | 0.0654
Epoch 260/300, seasonal_0 Loss: 0.0530 | 0.0654
Epoch 261/300, seasonal_0 Loss: 0.0529 | 0.0654
Epoch 262/300, seasonal_0 Loss: 0.0529 | 0.0654
Epoch 263/300, seasonal_0 Loss: 0.0529 | 0.0654
Epoch 264/300, seasonal_0 Loss: 0.0529 | 0.0654
Epoch 265/300, seasonal_0 Loss: 0.0529 | 0.0654
Epoch 266/300, seasonal_0 Loss: 0.0528 | 0.0654
Epoch 267/300, seasonal_0 Loss: 0.0528 | 0.0654
Epoch 268/300, seasonal_0 Loss: 0.0528 | 0.0654
Epoch 269/300, seasonal_0 Loss: 0.0528 | 0.0654
Epoch 270/300, seasonal_0 Loss: 0.0528 | 0.0654
Epoch 271/300, seasonal_0 Loss: 0.0527 | 0.0654
Epoch 272/300, seasonal_0 Loss: 0.0527 | 0.0654
Epoch 273/300, seasonal_0 Loss: 0.0527 | 0.0654
Epoch 274/300, seasonal_0 Loss: 0.0527 | 0.0654
Epoch 275/300, seasonal_0 Loss: 0.0527 | 0.0654
Epoch 276/300, seasonal_0 Loss: 0.0526 | 0.0654
Epoch 277/300, seasonal_0 Loss: 0.0526 | 0.0654
Epoch 278/300, seasonal_0 Loss: 0.0526 | 0.0654
Epoch 279/300, seasonal_0 Loss: 0.0526 | 0.0654
Epoch 280/300, seasonal_0 Loss: 0.0526 | 0.0654
Epoch 281/300, seasonal_0 Loss: 0.0526 | 0.0654
Epoch 282/300, seasonal_0 Loss: 0.0525 | 0.0654
Epoch 283/300, seasonal_0 Loss: 0.0525 | 0.0654
Epoch 284/300, seasonal_0 Loss: 0.0525 | 0.0654
Epoch 285/300, seasonal_0 Loss: 0.0525 | 0.0654
Epoch 286/300, seasonal_0 Loss: 0.0525 | 0.0654
Epoch 287/300, seasonal_0 Loss: 0.0525 | 0.0654
Epoch 288/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 289/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 290/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 291/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 292/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 293/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 294/300, seasonal_0 Loss: 0.0524 | 0.0654
Epoch 295/300, seasonal_0 Loss: 0.0523 | 0.0654
Epoch 296/300, seasonal_0 Loss: 0.0523 | 0.0654
Epoch 297/300, seasonal_0 Loss: 0.0523 | 0.0654
Epoch 298/300, seasonal_0 Loss: 0.0523 | 0.0654
Epoch 299/300, seasonal_0 Loss: 0.0523 | 0.0654
Epoch 300/300, seasonal_0 Loss: 0.0523 | 0.0654
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.6970416169132467, 'learning_rate': 0.0005564063623388238, 'batch_size': 144, 'step_size': 6, 'gamma': 0.975350221092885}
Epoch 1/300, seasonal_1 Loss: 0.4124 | 0.2258
Epoch 2/300, seasonal_1 Loss: 0.1877 | 0.1912
Epoch 3/300, seasonal_1 Loss: 0.1910 | 0.1635
Epoch 4/300, seasonal_1 Loss: 0.1578 | 0.1313
Epoch 5/300, seasonal_1 Loss: 0.1387 | 0.1186
Epoch 6/300, seasonal_1 Loss: 0.1297 | 0.1135
Epoch 7/300, seasonal_1 Loss: 0.1226 | 0.1099
Epoch 8/300, seasonal_1 Loss: 0.1236 | 0.1165
Epoch 9/300, seasonal_1 Loss: 0.1249 | 0.1171
Epoch 10/300, seasonal_1 Loss: 0.1231 | 0.1113
Epoch 11/300, seasonal_1 Loss: 0.1180 | 0.1084
Epoch 12/300, seasonal_1 Loss: 0.1121 | 0.1027
Epoch 13/300, seasonal_1 Loss: 0.1079 | 0.0941
Epoch 14/300, seasonal_1 Loss: 0.1057 | 0.0918
Epoch 15/300, seasonal_1 Loss: 0.1040 | 0.0880
Epoch 16/300, seasonal_1 Loss: 0.1013 | 0.0869
Epoch 17/300, seasonal_1 Loss: 0.1000 | 0.0849
Epoch 18/300, seasonal_1 Loss: 0.0986 | 0.0872
Epoch 19/300, seasonal_1 Loss: 0.0979 | 0.0859
Epoch 20/300, seasonal_1 Loss: 0.0979 | 0.1132
Epoch 21/300, seasonal_1 Loss: 0.1050 | 0.0847
Epoch 22/300, seasonal_1 Loss: 0.0967 | 0.0849
Epoch 23/300, seasonal_1 Loss: 0.0941 | 0.0779
Epoch 24/300, seasonal_1 Loss: 0.0924 | 0.0768
Epoch 25/300, seasonal_1 Loss: 0.0934 | 0.0762
Epoch 26/300, seasonal_1 Loss: 0.0997 | 0.0850
Epoch 27/300, seasonal_1 Loss: 0.0976 | 0.0815
Epoch 28/300, seasonal_1 Loss: 0.0948 | 0.1081
Epoch 29/300, seasonal_1 Loss: 0.1047 | 0.0857
Epoch 30/300, seasonal_1 Loss: 0.0944 | 0.0800
Epoch 31/300, seasonal_1 Loss: 0.0913 | 0.0784
Epoch 32/300, seasonal_1 Loss: 0.0929 | 0.0799
Epoch 33/300, seasonal_1 Loss: 0.0905 | 0.0840
Epoch 34/300, seasonal_1 Loss: 0.0917 | 0.0838
Epoch 35/300, seasonal_1 Loss: 0.0964 | 0.0877
Epoch 36/300, seasonal_1 Loss: 0.0977 | 0.0873
Epoch 37/300, seasonal_1 Loss: 0.0883 | 0.0745
Epoch 38/300, seasonal_1 Loss: 0.0878 | 0.0743
Epoch 39/300, seasonal_1 Loss: 0.0859 | 0.0734
Epoch 40/300, seasonal_1 Loss: 0.0880 | 0.0741
Epoch 41/300, seasonal_1 Loss: 0.0901 | 0.0773
Epoch 42/300, seasonal_1 Loss: 0.0857 | 0.0818
Epoch 43/300, seasonal_1 Loss: 0.0903 | 0.0813
Epoch 44/300, seasonal_1 Loss: 0.0890 | 0.0917
Epoch 45/300, seasonal_1 Loss: 0.0976 | 0.0849
Epoch 46/300, seasonal_1 Loss: 0.0881 | 0.0778
Epoch 47/300, seasonal_1 Loss: 0.0873 | 0.0763
Epoch 48/300, seasonal_1 Loss: 0.0861 | 0.0734
Epoch 49/300, seasonal_1 Loss: 0.0836 | 0.0755
Epoch 50/300, seasonal_1 Loss: 0.0830 | 0.0718
Epoch 51/300, seasonal_1 Loss: 0.0829 | 0.0812
Epoch 52/300, seasonal_1 Loss: 0.0848 | 0.0727
Epoch 53/300, seasonal_1 Loss: 0.0834 | 0.0872
Epoch 54/300, seasonal_1 Loss: 0.0855 | 0.0723
Epoch 55/300, seasonal_1 Loss: 0.0828 | 0.0782
Epoch 56/300, seasonal_1 Loss: 0.0831 | 0.0703
Epoch 57/300, seasonal_1 Loss: 0.0820 | 0.0803
Epoch 58/300, seasonal_1 Loss: 0.0826 | 0.0706
Epoch 59/300, seasonal_1 Loss: 0.0805 | 0.0755
Epoch 60/300, seasonal_1 Loss: 0.0803 | 0.0701
Epoch 61/300, seasonal_1 Loss: 0.0795 | 0.0736
Epoch 62/300, seasonal_1 Loss: 0.0791 | 0.0695
Epoch 63/300, seasonal_1 Loss: 0.0789 | 0.0720
Epoch 64/300, seasonal_1 Loss: 0.0790 | 0.0687
Epoch 65/300, seasonal_1 Loss: 0.0784 | 0.0711
Epoch 66/300, seasonal_1 Loss: 0.0794 | 0.0692
Epoch 67/300, seasonal_1 Loss: 0.0779 | 0.0718
Epoch 68/300, seasonal_1 Loss: 0.0780 | 0.0685
Epoch 69/300, seasonal_1 Loss: 0.0771 | 0.0740
Epoch 70/300, seasonal_1 Loss: 0.0773 | 0.0674
Epoch 71/300, seasonal_1 Loss: 0.0764 | 0.0731
Epoch 72/300, seasonal_1 Loss: 0.0762 | 0.0666
Epoch 73/300, seasonal_1 Loss: 0.0756 | 0.0732
Epoch 74/300, seasonal_1 Loss: 0.0750 | 0.0666
Epoch 75/300, seasonal_1 Loss: 0.0754 | 0.0727
Epoch 76/300, seasonal_1 Loss: 0.0752 | 0.0663
Epoch 77/300, seasonal_1 Loss: 0.0760 | 0.0696
Epoch 78/300, seasonal_1 Loss: 0.0768 | 0.0662
Epoch 79/300, seasonal_1 Loss: 0.0765 | 0.0668
Epoch 80/300, seasonal_1 Loss: 0.0763 | 0.0653
Epoch 81/300, seasonal_1 Loss: 0.0762 | 0.0680
Epoch 82/300, seasonal_1 Loss: 0.0753 | 0.0653
Epoch 83/300, seasonal_1 Loss: 0.0744 | 0.0682
Epoch 84/300, seasonal_1 Loss: 0.0750 | 0.0658
Epoch 85/300, seasonal_1 Loss: 0.0743 | 0.0685
Epoch 86/300, seasonal_1 Loss: 0.0739 | 0.0641
Epoch 87/300, seasonal_1 Loss: 0.0728 | 0.0677
Epoch 88/300, seasonal_1 Loss: 0.0725 | 0.0636
Epoch 89/300, seasonal_1 Loss: 0.0726 | 0.0664
Epoch 90/300, seasonal_1 Loss: 0.0723 | 0.0634
Epoch 91/300, seasonal_1 Loss: 0.0724 | 0.0649
Epoch 92/300, seasonal_1 Loss: 0.0729 | 0.0660
Epoch 93/300, seasonal_1 Loss: 0.0730 | 0.0658
Epoch 94/300, seasonal_1 Loss: 0.0730 | 0.0658
Epoch 95/300, seasonal_1 Loss: 0.0730 | 0.0651
Epoch 96/300, seasonal_1 Loss: 0.0749 | 0.0669
Epoch 97/300, seasonal_1 Loss: 0.0738 | 0.0723
Epoch 98/300, seasonal_1 Loss: 0.0748 | 0.0669
Epoch 99/300, seasonal_1 Loss: 0.0738 | 0.0697
Epoch 100/300, seasonal_1 Loss: 0.0740 | 0.0658
Epoch 101/300, seasonal_1 Loss: 0.0720 | 0.0653
Epoch 102/300, seasonal_1 Loss: 0.0710 | 0.0637
Epoch 103/300, seasonal_1 Loss: 0.0713 | 0.0674
Epoch 104/300, seasonal_1 Loss: 0.0704 | 0.0637
Epoch 105/300, seasonal_1 Loss: 0.0700 | 0.0649
Epoch 106/300, seasonal_1 Loss: 0.0695 | 0.0622
Epoch 107/300, seasonal_1 Loss: 0.0685 | 0.0627
Epoch 108/300, seasonal_1 Loss: 0.0682 | 0.0630
Epoch 109/300, seasonal_1 Loss: 0.0681 | 0.0637
Epoch 110/300, seasonal_1 Loss: 0.0680 | 0.0630
Epoch 111/300, seasonal_1 Loss: 0.0678 | 0.0624
Epoch 112/300, seasonal_1 Loss: 0.0681 | 0.0620
Epoch 113/300, seasonal_1 Loss: 0.0678 | 0.0629
Epoch 114/300, seasonal_1 Loss: 0.0684 | 0.0631
Epoch 115/300, seasonal_1 Loss: 0.0728 | 0.0715
Epoch 116/300, seasonal_1 Loss: 0.0717 | 0.0685
Epoch 117/300, seasonal_1 Loss: 0.0711 | 0.0689
Epoch 118/300, seasonal_1 Loss: 0.0706 | 0.0670
Epoch 119/300, seasonal_1 Loss: 0.0698 | 0.0682
Epoch 120/300, seasonal_1 Loss: 0.0686 | 0.0644
Epoch 121/300, seasonal_1 Loss: 0.0698 | 0.0645
Epoch 122/300, seasonal_1 Loss: 0.0686 | 0.0637
Epoch 123/300, seasonal_1 Loss: 0.0683 | 0.0634
Epoch 124/300, seasonal_1 Loss: 0.0678 | 0.0632
Epoch 125/300, seasonal_1 Loss: 0.0676 | 0.0656
Epoch 126/300, seasonal_1 Loss: 0.0677 | 0.0647
Epoch 127/300, seasonal_1 Loss: 0.0682 | 0.0672
Epoch 128/300, seasonal_1 Loss: 0.0681 | 0.0650
Epoch 129/300, seasonal_1 Loss: 0.0676 | 0.0671
Epoch 130/300, seasonal_1 Loss: 0.0675 | 0.0648
Epoch 131/300, seasonal_1 Loss: 0.0678 | 0.0654
Epoch 132/300, seasonal_1 Loss: 0.0669 | 0.0640
Epoch 133/300, seasonal_1 Loss: 0.0671 | 0.0639
Epoch 134/300, seasonal_1 Loss: 0.0680 | 0.0634
Epoch 135/300, seasonal_1 Loss: 0.0684 | 0.0664
Epoch 136/300, seasonal_1 Loss: 0.0698 | 0.0640
Epoch 137/300, seasonal_1 Loss: 0.0688 | 0.0661
Epoch 138/300, seasonal_1 Loss: 0.0696 | 0.0676
Epoch 139/300, seasonal_1 Loss: 0.0690 | 0.0646
Epoch 140/300, seasonal_1 Loss: 0.0676 | 0.0636
Epoch 141/300, seasonal_1 Loss: 0.0665 | 0.0661
Epoch 142/300, seasonal_1 Loss: 0.0674 | 0.0648
Epoch 143/300, seasonal_1 Loss: 0.0661 | 0.0656
Epoch 144/300, seasonal_1 Loss: 0.0671 | 0.0666
Epoch 145/300, seasonal_1 Loss: 0.0674 | 0.0669
Epoch 146/300, seasonal_1 Loss: 0.0688 | 0.0663
Epoch 147/300, seasonal_1 Loss: 0.0740 | 0.0717
Epoch 148/300, seasonal_1 Loss: 0.0714 | 0.0688
Epoch 149/300, seasonal_1 Loss: 0.0734 | 0.0788
Epoch 150/300, seasonal_1 Loss: 0.0735 | 0.0733
Epoch 151/300, seasonal_1 Loss: 0.0743 | 0.0705
Epoch 152/300, seasonal_1 Loss: 0.0741 | 0.0689
Epoch 153/300, seasonal_1 Loss: 0.0760 | 0.0689
Epoch 154/300, seasonal_1 Loss: 0.0896 | 0.0723
Epoch 155/300, seasonal_1 Loss: 0.1005 | 0.0758
Epoch 156/300, seasonal_1 Loss: 0.0925 | 0.0748
Epoch 157/300, seasonal_1 Loss: 0.0871 | 0.0774
Epoch 158/300, seasonal_1 Loss: 0.0769 | 0.0692
Epoch 159/300, seasonal_1 Loss: 0.0717 | 0.0669
Epoch 160/300, seasonal_1 Loss: 0.0700 | 0.0663
Epoch 161/300, seasonal_1 Loss: 0.0684 | 0.0643
Epoch 162/300, seasonal_1 Loss: 0.0652 | 0.0653
Epoch 163/300, seasonal_1 Loss: 0.0659 | 0.0638
Epoch 164/300, seasonal_1 Loss: 0.0652 | 0.0644
Epoch 165/300, seasonal_1 Loss: 0.0639 | 0.0644
Epoch 166/300, seasonal_1 Loss: 0.0636 | 0.0641
Epoch 167/300, seasonal_1 Loss: 0.0636 | 0.0637
Epoch 168/300, seasonal_1 Loss: 0.0632 | 0.0642
Epoch 169/300, seasonal_1 Loss: 0.0634 | 0.0640
Epoch 170/300, seasonal_1 Loss: 0.0633 | 0.0640
Epoch 171/300, seasonal_1 Loss: 0.0630 | 0.0640
Epoch 172/300, seasonal_1 Loss: 0.0626 | 0.0640
Epoch 173/300, seasonal_1 Loss: 0.0625 | 0.0640
Epoch 174/300, seasonal_1 Loss: 0.0625 | 0.0640
Epoch 175/300, seasonal_1 Loss: 0.0624 | 0.0640
Epoch 176/300, seasonal_1 Loss: 0.0623 | 0.0639
Epoch 177/300, seasonal_1 Loss: 0.0623 | 0.0641
Epoch 178/300, seasonal_1 Loss: 0.0622 | 0.0640
Epoch 179/300, seasonal_1 Loss: 0.0621 | 0.0641
Epoch 180/300, seasonal_1 Loss: 0.0619 | 0.0640
Epoch 181/300, seasonal_1 Loss: 0.0618 | 0.0642
Epoch 182/300, seasonal_1 Loss: 0.0616 | 0.0641
Epoch 183/300, seasonal_1 Loss: 0.0616 | 0.0644
Epoch 184/300, seasonal_1 Loss: 0.0615 | 0.0643
Epoch 185/300, seasonal_1 Loss: 0.0614 | 0.0646
Epoch 186/300, seasonal_1 Loss: 0.0613 | 0.0643
Epoch 187/300, seasonal_1 Loss: 0.0611 | 0.0645
Epoch 188/300, seasonal_1 Loss: 0.0608 | 0.0641
Epoch 189/300, seasonal_1 Loss: 0.0608 | 0.0646
Epoch 190/300, seasonal_1 Loss: 0.0608 | 0.0643
Epoch 191/300, seasonal_1 Loss: 0.0611 | 0.0650
Epoch 192/300, seasonal_1 Loss: 0.0612 | 0.0646
Epoch 193/300, seasonal_1 Loss: 0.0601 | 0.0652
Epoch 194/300, seasonal_1 Loss: 0.0587 | 0.0650
Epoch 195/300, seasonal_1 Loss: 0.0564 | 0.0655
Epoch 196/300, seasonal_1 Loss: 0.0611 | 0.0655
Epoch 197/300, seasonal_1 Loss: 0.0622 | 0.0657
Epoch 198/300, seasonal_1 Loss: 0.0616 | 0.0648
Epoch 199/300, seasonal_1 Loss: 0.0586 | 0.0659
Epoch 200/300, seasonal_1 Loss: 0.0559 | 0.0641
Epoch 201/300, seasonal_1 Loss: 0.0593 | 0.0655
Epoch 202/300, seasonal_1 Loss: 0.0559 | 0.0645
Epoch 203/300, seasonal_1 Loss: 0.0633 | 0.0635
Epoch 204/300, seasonal_1 Loss: 0.0627 | 0.0638
Epoch 205/300, seasonal_1 Loss: 0.0622 | 0.0637
Epoch 206/300, seasonal_1 Loss: 0.0616 | 0.0639
Epoch 207/300, seasonal_1 Loss: 0.0613 | 0.0648
Epoch 208/300, seasonal_1 Loss: 0.0613 | 0.0649
Epoch 209/300, seasonal_1 Loss: 0.0614 | 0.0651
Epoch 210/300, seasonal_1 Loss: 0.0613 | 0.0642
Epoch 211/300, seasonal_1 Loss: 0.0604 | 0.0640
Epoch 212/300, seasonal_1 Loss: 0.0565 | 0.0644
Epoch 213/300, seasonal_1 Loss: 0.0558 | 0.0638
Epoch 214/300, seasonal_1 Loss: 0.0540 | 0.0642
Epoch 215/300, seasonal_1 Loss: 0.0525 | 0.0642
Epoch 216/300, seasonal_1 Loss: 0.0521 | 0.0642
Epoch 217/300, seasonal_1 Loss: 0.0519 | 0.0645
Epoch 218/300, seasonal_1 Loss: 0.0516 | 0.0643
Epoch 219/300, seasonal_1 Loss: 0.0514 | 0.0641
Epoch 220/300, seasonal_1 Loss: 0.0512 | 0.0639
Epoch 221/300, seasonal_1 Loss: 0.0512 | 0.0637
Epoch 222/300, seasonal_1 Loss: 0.0512 | 0.0637
Epoch 223/300, seasonal_1 Loss: 0.0513 | 0.0637
Epoch 224/300, seasonal_1 Loss: 0.0509 | 0.0639
Epoch 225/300, seasonal_1 Loss: 0.0509 | 0.0643
Epoch 226/300, seasonal_1 Loss: 0.0509 | 0.0647
Epoch 227/300, seasonal_1 Loss: 0.0512 | 0.0660
Epoch 228/300, seasonal_1 Loss: 0.0517 | 0.0661
Epoch 229/300, seasonal_1 Loss: 0.0519 | 0.0646
Epoch 230/300, seasonal_1 Loss: 0.0510 | 0.0641
Epoch 231/300, seasonal_1 Loss: 0.0508 | 0.0638
Epoch 232/300, seasonal_1 Loss: 0.0510 | 0.0633
Epoch 233/300, seasonal_1 Loss: 0.0515 | 0.0638
Epoch 234/300, seasonal_1 Loss: 0.0515 | 0.0635
Epoch 235/300, seasonal_1 Loss: 0.0511 | 0.0641
Epoch 236/300, seasonal_1 Loss: 0.0508 | 0.0641
Epoch 237/300, seasonal_1 Loss: 0.0507 | 0.0654
Epoch 238/300, seasonal_1 Loss: 0.0508 | 0.0646
Epoch 239/300, seasonal_1 Loss: 0.0507 | 0.0656
Epoch 240/300, seasonal_1 Loss: 0.0511 | 0.0652
Epoch 241/300, seasonal_1 Loss: 0.0513 | 0.0635
Epoch 242/300, seasonal_1 Loss: 0.0507 | 0.0629
Epoch 243/300, seasonal_1 Loss: 0.0501 | 0.0636
Epoch 244/300, seasonal_1 Loss: 0.0498 | 0.0637
Epoch 245/300, seasonal_1 Loss: 0.0496 | 0.0647
Epoch 246/300, seasonal_1 Loss: 0.0495 | 0.0642
Epoch 247/300, seasonal_1 Loss: 0.0495 | 0.0645
Epoch 248/300, seasonal_1 Loss: 0.0493 | 0.0641
Epoch 249/300, seasonal_1 Loss: 0.0493 | 0.0643
Epoch 250/300, seasonal_1 Loss: 0.0492 | 0.0633
Epoch 251/300, seasonal_1 Loss: 0.0492 | 0.0634
Epoch 252/300, seasonal_1 Loss: 0.0493 | 0.0635
Epoch 253/300, seasonal_1 Loss: 0.0493 | 0.0636
Epoch 254/300, seasonal_1 Loss: 0.0491 | 0.0634
Epoch 255/300, seasonal_1 Loss: 0.0487 | 0.0638
Epoch 256/300, seasonal_1 Loss: 0.0485 | 0.0636
Epoch 257/300, seasonal_1 Loss: 0.0485 | 0.0640
Epoch 258/300, seasonal_1 Loss: 0.0486 | 0.0644
Epoch 259/300, seasonal_1 Loss: 0.0489 | 0.0652
Epoch 260/300, seasonal_1 Loss: 0.0487 | 0.0633
Epoch 261/300, seasonal_1 Loss: 0.0483 | 0.0637
Epoch 262/300, seasonal_1 Loss: 0.0482 | 0.0635
Epoch 263/300, seasonal_1 Loss: 0.0480 | 0.0635
Epoch 264/300, seasonal_1 Loss: 0.0479 | 0.0634
Epoch 265/300, seasonal_1 Loss: 0.0478 | 0.0635
Epoch 266/300, seasonal_1 Loss: 0.0477 | 0.0633
Epoch 267/300, seasonal_1 Loss: 0.0476 | 0.0637
Epoch 268/300, seasonal_1 Loss: 0.0476 | 0.0636
Epoch 269/300, seasonal_1 Loss: 0.0477 | 0.0641
Epoch 270/300, seasonal_1 Loss: 0.0477 | 0.0646
Epoch 271/300, seasonal_1 Loss: 0.0479 | 0.0647
Epoch 272/300, seasonal_1 Loss: 0.0476 | 0.0633
Epoch 273/300, seasonal_1 Loss: 0.0474 | 0.0636
Epoch 274/300, seasonal_1 Loss: 0.0475 | 0.0631
Epoch 275/300, seasonal_1 Loss: 0.0475 | 0.0633
Epoch 276/300, seasonal_1 Loss: 0.0474 | 0.0632
Epoch 277/300, seasonal_1 Loss: 0.0472 | 0.0634
Epoch 278/300, seasonal_1 Loss: 0.0471 | 0.0633
Epoch 279/300, seasonal_1 Loss: 0.0470 | 0.0639
Epoch 280/300, seasonal_1 Loss: 0.0471 | 0.0639
Epoch 281/300, seasonal_1 Loss: 0.0472 | 0.0645
Epoch 282/300, seasonal_1 Loss: 0.0474 | 0.0652
Epoch 283/300, seasonal_1 Loss: 0.0476 | 0.0647
Epoch 284/300, seasonal_1 Loss: 0.0472 | 0.0632
Epoch 285/300, seasonal_1 Loss: 0.0472 | 0.0633
Epoch 286/300, seasonal_1 Loss: 0.0471 | 0.0629
Epoch 287/300, seasonal_1 Loss: 0.0468 | 0.0635
Epoch 288/300, seasonal_1 Loss: 0.0467 | 0.0635
Epoch 289/300, seasonal_1 Loss: 0.0466 | 0.0642
Epoch 290/300, seasonal_1 Loss: 0.0468 | 0.0644
Epoch 291/300, seasonal_1 Loss: 0.0470 | 0.0650
Epoch 292/300, seasonal_1 Loss: 0.0469 | 0.0643
Epoch 293/300, seasonal_1 Loss: 0.0466 | 0.0636
Epoch 294/300, seasonal_1 Loss: 0.0466 | 0.0631
Epoch 295/300, seasonal_1 Loss: 0.0470 | 0.0630
Epoch 296/300, seasonal_1 Loss: 0.0470 | 0.0631
Epoch 297/300, seasonal_1 Loss: 0.0465 | 0.0637
Epoch 298/300, seasonal_1 Loss: 0.0462 | 0.0639
Epoch 299/300, seasonal_1 Loss: 0.0465 | 0.0645
Epoch 300/300, seasonal_1 Loss: 0.0467 | 0.0644
Training seasonal_2 component with params: {'observation_period_num': 18, 'train_rates': 0.6912581798040126, 'learning_rate': 0.00012671990215831424, 'batch_size': 22, 'step_size': 3, 'gamma': 0.912599958753059}
Epoch 1/300, seasonal_2 Loss: 0.2506 | 0.2176
Epoch 2/300, seasonal_2 Loss: 0.1556 | 0.1876
Epoch 3/300, seasonal_2 Loss: 0.1386 | 0.1715
Epoch 4/300, seasonal_2 Loss: 0.1303 | 0.1572
Epoch 5/300, seasonal_2 Loss: 0.1251 | 0.1436
Epoch 6/300, seasonal_2 Loss: 0.1202 | 0.1345
Epoch 7/300, seasonal_2 Loss: 0.1167 | 0.1260
Epoch 8/300, seasonal_2 Loss: 0.1140 | 0.1191
Epoch 9/300, seasonal_2 Loss: 0.1110 | 0.1117
Epoch 10/300, seasonal_2 Loss: 0.1084 | 0.1052
Epoch 11/300, seasonal_2 Loss: 0.1061 | 0.1010
Epoch 12/300, seasonal_2 Loss: 0.1036 | 0.0966
Epoch 13/300, seasonal_2 Loss: 0.1012 | 0.0935
Epoch 14/300, seasonal_2 Loss: 0.0991 | 0.0918
Epoch 15/300, seasonal_2 Loss: 0.0970 | 0.0905
Epoch 16/300, seasonal_2 Loss: 0.0951 | 0.0896
Epoch 17/300, seasonal_2 Loss: 0.0935 | 0.0885
Epoch 18/300, seasonal_2 Loss: 0.0920 | 0.0873
Epoch 19/300, seasonal_2 Loss: 0.0907 | 0.0859
Epoch 20/300, seasonal_2 Loss: 0.0896 | 0.0847
Epoch 21/300, seasonal_2 Loss: 0.0886 | 0.0834
Epoch 22/300, seasonal_2 Loss: 0.0878 | 0.0822
Epoch 23/300, seasonal_2 Loss: 0.0870 | 0.0816
Epoch 24/300, seasonal_2 Loss: 0.0864 | 0.0807
Epoch 25/300, seasonal_2 Loss: 0.0857 | 0.0799
Epoch 26/300, seasonal_2 Loss: 0.0852 | 0.0796
Epoch 27/300, seasonal_2 Loss: 0.0847 | 0.0790
Epoch 28/300, seasonal_2 Loss: 0.0842 | 0.0785
Epoch 29/300, seasonal_2 Loss: 0.0838 | 0.0782
Epoch 30/300, seasonal_2 Loss: 0.0833 | 0.0779
Epoch 31/300, seasonal_2 Loss: 0.0830 | 0.0776
Epoch 32/300, seasonal_2 Loss: 0.0827 | 0.0774
Epoch 33/300, seasonal_2 Loss: 0.0823 | 0.0772
Epoch 34/300, seasonal_2 Loss: 0.0820 | 0.0770
Epoch 35/300, seasonal_2 Loss: 0.0818 | 0.0768
Epoch 36/300, seasonal_2 Loss: 0.0815 | 0.0767
Epoch 37/300, seasonal_2 Loss: 0.0813 | 0.0766
Epoch 38/300, seasonal_2 Loss: 0.0811 | 0.0765
Epoch 39/300, seasonal_2 Loss: 0.0809 | 0.0764
Epoch 40/300, seasonal_2 Loss: 0.0807 | 0.0764
Epoch 41/300, seasonal_2 Loss: 0.0806 | 0.0764
Epoch 42/300, seasonal_2 Loss: 0.0805 | 0.0764
Epoch 43/300, seasonal_2 Loss: 0.0804 | 0.0764
Epoch 44/300, seasonal_2 Loss: 0.0803 | 0.0763
Epoch 45/300, seasonal_2 Loss: 0.0801 | 0.0763
Epoch 46/300, seasonal_2 Loss: 0.0799 | 0.0764
Epoch 47/300, seasonal_2 Loss: 0.0797 | 0.0763
Epoch 48/300, seasonal_2 Loss: 0.0796 | 0.0763
Epoch 49/300, seasonal_2 Loss: 0.0794 | 0.0764
Epoch 50/300, seasonal_2 Loss: 0.0793 | 0.0763
Epoch 51/300, seasonal_2 Loss: 0.0792 | 0.0764
Epoch 52/300, seasonal_2 Loss: 0.0791 | 0.0764
Epoch 53/300, seasonal_2 Loss: 0.0791 | 0.0763
Epoch 54/300, seasonal_2 Loss: 0.0790 | 0.0764
Epoch 55/300, seasonal_2 Loss: 0.0789 | 0.0763
Epoch 56/300, seasonal_2 Loss: 0.0788 | 0.0763
Epoch 57/300, seasonal_2 Loss: 0.0787 | 0.0763
Epoch 58/300, seasonal_2 Loss: 0.0786 | 0.0762
Epoch 59/300, seasonal_2 Loss: 0.0786 | 0.0762
Epoch 60/300, seasonal_2 Loss: 0.0785 | 0.0761
Epoch 61/300, seasonal_2 Loss: 0.0785 | 0.0761
Epoch 62/300, seasonal_2 Loss: 0.0784 | 0.0761
Epoch 63/300, seasonal_2 Loss: 0.0783 | 0.0760
Epoch 64/300, seasonal_2 Loss: 0.0783 | 0.0760
Epoch 65/300, seasonal_2 Loss: 0.0782 | 0.0760
Epoch 66/300, seasonal_2 Loss: 0.0782 | 0.0759
Epoch 67/300, seasonal_2 Loss: 0.0782 | 0.0759
Epoch 68/300, seasonal_2 Loss: 0.0781 | 0.0759
Epoch 69/300, seasonal_2 Loss: 0.0781 | 0.0759
Epoch 70/300, seasonal_2 Loss: 0.0780 | 0.0759
Epoch 71/300, seasonal_2 Loss: 0.0780 | 0.0759
Epoch 72/300, seasonal_2 Loss: 0.0780 | 0.0758
Epoch 73/300, seasonal_2 Loss: 0.0779 | 0.0758
Epoch 74/300, seasonal_2 Loss: 0.0779 | 0.0759
Epoch 75/300, seasonal_2 Loss: 0.0779 | 0.0758
Epoch 76/300, seasonal_2 Loss: 0.0779 | 0.0758
Epoch 77/300, seasonal_2 Loss: 0.0778 | 0.0759
Epoch 78/300, seasonal_2 Loss: 0.0778 | 0.0759
Epoch 79/300, seasonal_2 Loss: 0.0778 | 0.0758
Epoch 80/300, seasonal_2 Loss: 0.0777 | 0.0759
Epoch 81/300, seasonal_2 Loss: 0.0777 | 0.0759
Epoch 82/300, seasonal_2 Loss: 0.0777 | 0.0758
Epoch 83/300, seasonal_2 Loss: 0.0776 | 0.0759
Epoch 84/300, seasonal_2 Loss: 0.0776 | 0.0759
Epoch 85/300, seasonal_2 Loss: 0.0776 | 0.0759
Epoch 86/300, seasonal_2 Loss: 0.0776 | 0.0759
Epoch 87/300, seasonal_2 Loss: 0.0775 | 0.0759
Epoch 88/300, seasonal_2 Loss: 0.0775 | 0.0759
Epoch 89/300, seasonal_2 Loss: 0.0775 | 0.0759
Epoch 90/300, seasonal_2 Loss: 0.0775 | 0.0759
Epoch 91/300, seasonal_2 Loss: 0.0775 | 0.0759
Epoch 92/300, seasonal_2 Loss: 0.0775 | 0.0759
Epoch 93/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 94/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 95/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 96/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 97/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 98/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 99/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 100/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 101/300, seasonal_2 Loss: 0.0774 | 0.0759
Epoch 102/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 103/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 104/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 105/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 106/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 107/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 108/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 109/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 110/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 111/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 112/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 113/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 114/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 115/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 116/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 117/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 118/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 119/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 120/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 121/300, seasonal_2 Loss: 0.0773 | 0.0759
Epoch 122/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 123/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 124/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 125/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 126/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 127/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 128/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 129/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 130/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 131/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 132/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 133/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 134/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 135/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 136/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 137/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 138/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 139/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 140/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 141/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 142/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 143/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 144/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 145/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 146/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 147/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 148/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 149/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 150/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 151/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 152/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 153/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 154/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 155/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 156/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 157/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 158/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 159/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 160/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 161/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 162/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 163/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 164/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 165/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 166/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 167/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 168/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 169/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 170/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 171/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 172/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 173/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 174/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 175/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 176/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 177/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 178/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 179/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 180/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 181/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 182/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 183/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 184/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 185/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 186/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 187/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 188/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 189/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 190/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 191/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 192/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 193/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 194/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 195/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 196/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 197/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 198/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 199/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 200/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 201/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 202/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 203/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 204/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 205/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 206/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 207/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 208/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 209/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 210/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 211/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 212/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 213/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 214/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 215/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 216/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 217/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 218/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 219/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 220/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 221/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 222/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 223/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 224/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 225/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 226/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 227/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 228/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 229/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 230/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 231/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 232/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 233/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 234/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 235/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 236/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 237/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 238/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 239/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 240/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 241/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 242/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 243/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 244/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 245/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 246/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 247/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 248/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 249/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 250/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 251/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 252/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 253/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 254/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 255/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 256/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 257/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 258/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 259/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 260/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 261/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 262/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 263/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 264/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 265/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 266/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 267/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 268/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 269/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 270/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 271/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 272/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 273/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 274/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 275/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 276/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 277/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 278/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 279/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 280/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 281/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 282/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 283/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 284/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 285/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 286/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 287/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 288/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 289/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 290/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 291/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 292/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 293/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 294/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 295/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 296/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 297/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 298/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 299/300, seasonal_2 Loss: 0.0772 | 0.0759
Epoch 300/300, seasonal_2 Loss: 0.0772 | 0.0759
Training seasonal_3 component with params: {'observation_period_num': 90, 'train_rates': 0.9883206887075948, 'learning_rate': 0.00020082888946698732, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8434114253816316}
Epoch 1/300, seasonal_3 Loss: 0.3914 | 0.2655
Epoch 2/300, seasonal_3 Loss: 0.2782 | 0.3084
Epoch 3/300, seasonal_3 Loss: 0.2632 | 0.3182
Epoch 4/300, seasonal_3 Loss: 0.2701 | 0.2024
Epoch 5/300, seasonal_3 Loss: 0.3754 | 0.4929
Epoch 6/300, seasonal_3 Loss: 0.2856 | 0.2662
Epoch 7/300, seasonal_3 Loss: 0.3468 | 0.2031
Epoch 8/300, seasonal_3 Loss: 0.1934 | 0.2247
Epoch 9/300, seasonal_3 Loss: 0.1865 | 0.1699
Epoch 10/300, seasonal_3 Loss: 0.1916 | 0.1707
Epoch 11/300, seasonal_3 Loss: 0.1617 | 0.1908
Epoch 12/300, seasonal_3 Loss: 0.1530 | 0.1541
Epoch 13/300, seasonal_3 Loss: 0.1537 | 0.1515
Epoch 14/300, seasonal_3 Loss: 0.1417 | 0.1702
Epoch 15/300, seasonal_3 Loss: 0.1335 | 0.1627
Epoch 16/300, seasonal_3 Loss: 0.1294 | 0.1372
Epoch 17/300, seasonal_3 Loss: 0.1243 | 0.1273
Epoch 18/300, seasonal_3 Loss: 0.1197 | 0.1279
Epoch 19/300, seasonal_3 Loss: 0.1179 | 0.1368
Epoch 20/300, seasonal_3 Loss: 0.1140 | 0.1354
Epoch 21/300, seasonal_3 Loss: 0.1104 | 0.1262
Epoch 22/300, seasonal_3 Loss: 0.1093 | 0.1184
Epoch 23/300, seasonal_3 Loss: 0.1072 | 0.1124
Epoch 24/300, seasonal_3 Loss: 0.1057 | 0.1144
Epoch 25/300, seasonal_3 Loss: 0.1049 | 0.1182
Epoch 26/300, seasonal_3 Loss: 0.1026 | 0.1206
Epoch 27/300, seasonal_3 Loss: 0.1018 | 0.1207
Epoch 28/300, seasonal_3 Loss: 0.1034 | 0.1360
Epoch 29/300, seasonal_3 Loss: 0.1037 | 0.1552
Epoch 30/300, seasonal_3 Loss: 0.1012 | 0.1732
Epoch 31/300, seasonal_3 Loss: 0.0997 | 0.1561
Epoch 32/300, seasonal_3 Loss: 0.1057 | 0.1128
Epoch 33/300, seasonal_3 Loss: 0.1066 | 0.1086
Epoch 34/300, seasonal_3 Loss: 0.1077 | 0.1126
Epoch 35/300, seasonal_3 Loss: 0.1051 | 0.1069
Epoch 36/300, seasonal_3 Loss: 0.0961 | 0.0989
Epoch 37/300, seasonal_3 Loss: 0.0956 | 0.1022
Epoch 38/300, seasonal_3 Loss: 0.0933 | 0.1047
Epoch 39/300, seasonal_3 Loss: 0.0916 | 0.0984
Epoch 40/300, seasonal_3 Loss: 0.0909 | 0.0970
Epoch 41/300, seasonal_3 Loss: 0.0898 | 0.0989
Epoch 42/300, seasonal_3 Loss: 0.0894 | 0.0968
Epoch 43/300, seasonal_3 Loss: 0.0885 | 0.0944
Epoch 44/300, seasonal_3 Loss: 0.0881 | 0.0959
Epoch 45/300, seasonal_3 Loss: 0.0874 | 0.0964
Epoch 46/300, seasonal_3 Loss: 0.0870 | 0.0937
Epoch 47/300, seasonal_3 Loss: 0.0865 | 0.0935
Epoch 48/300, seasonal_3 Loss: 0.0861 | 0.0943
Epoch 49/300, seasonal_3 Loss: 0.0856 | 0.0932
Epoch 50/300, seasonal_3 Loss: 0.0853 | 0.0921
Epoch 51/300, seasonal_3 Loss: 0.0849 | 0.0923
Epoch 52/300, seasonal_3 Loss: 0.0845 | 0.0923
Epoch 53/300, seasonal_3 Loss: 0.0842 | 0.0916
Epoch 54/300, seasonal_3 Loss: 0.0839 | 0.0913
Epoch 55/300, seasonal_3 Loss: 0.0836 | 0.0912
Epoch 56/300, seasonal_3 Loss: 0.0833 | 0.0908
Epoch 57/300, seasonal_3 Loss: 0.0830 | 0.0906
Epoch 58/300, seasonal_3 Loss: 0.0828 | 0.0904
Epoch 59/300, seasonal_3 Loss: 0.0825 | 0.0901
Epoch 60/300, seasonal_3 Loss: 0.0822 | 0.0899
Epoch 61/300, seasonal_3 Loss: 0.0820 | 0.0897
Epoch 62/300, seasonal_3 Loss: 0.0818 | 0.0895
Epoch 63/300, seasonal_3 Loss: 0.0816 | 0.0893
Epoch 64/300, seasonal_3 Loss: 0.0814 | 0.0891
Epoch 65/300, seasonal_3 Loss: 0.0812 | 0.0890
Epoch 66/300, seasonal_3 Loss: 0.0810 | 0.0888
Epoch 67/300, seasonal_3 Loss: 0.0808 | 0.0886
Epoch 68/300, seasonal_3 Loss: 0.0807 | 0.0885
Epoch 69/300, seasonal_3 Loss: 0.0805 | 0.0884
Epoch 70/300, seasonal_3 Loss: 0.0803 | 0.0883
Epoch 71/300, seasonal_3 Loss: 0.0802 | 0.0881
Epoch 72/300, seasonal_3 Loss: 0.0800 | 0.0880
Epoch 73/300, seasonal_3 Loss: 0.0799 | 0.0879
Epoch 74/300, seasonal_3 Loss: 0.0797 | 0.0878
Epoch 75/300, seasonal_3 Loss: 0.0796 | 0.0877
Epoch 76/300, seasonal_3 Loss: 0.0795 | 0.0876
Epoch 77/300, seasonal_3 Loss: 0.0794 | 0.0875
Epoch 78/300, seasonal_3 Loss: 0.0792 | 0.0874
Epoch 79/300, seasonal_3 Loss: 0.0791 | 0.0873
Epoch 80/300, seasonal_3 Loss: 0.0790 | 0.0872
Epoch 81/300, seasonal_3 Loss: 0.0789 | 0.0871
Epoch 82/300, seasonal_3 Loss: 0.0788 | 0.0871
Epoch 83/300, seasonal_3 Loss: 0.0787 | 0.0870
Epoch 84/300, seasonal_3 Loss: 0.0786 | 0.0869
Epoch 85/300, seasonal_3 Loss: 0.0785 | 0.0868
Epoch 86/300, seasonal_3 Loss: 0.0784 | 0.0868
Epoch 87/300, seasonal_3 Loss: 0.0783 | 0.0867
Epoch 88/300, seasonal_3 Loss: 0.0783 | 0.0866
Epoch 89/300, seasonal_3 Loss: 0.0782 | 0.0866
Epoch 90/300, seasonal_3 Loss: 0.0781 | 0.0865
Epoch 91/300, seasonal_3 Loss: 0.0780 | 0.0865
Epoch 92/300, seasonal_3 Loss: 0.0780 | 0.0864
Epoch 93/300, seasonal_3 Loss: 0.0779 | 0.0864
Epoch 94/300, seasonal_3 Loss: 0.0778 | 0.0863
Epoch 95/300, seasonal_3 Loss: 0.0778 | 0.0863
Epoch 96/300, seasonal_3 Loss: 0.0777 | 0.0862
Epoch 97/300, seasonal_3 Loss: 0.0776 | 0.0862
Epoch 98/300, seasonal_3 Loss: 0.0776 | 0.0862
Epoch 99/300, seasonal_3 Loss: 0.0775 | 0.0861
Epoch 100/300, seasonal_3 Loss: 0.0775 | 0.0861
Epoch 101/300, seasonal_3 Loss: 0.0774 | 0.0860
Epoch 102/300, seasonal_3 Loss: 0.0774 | 0.0860
Epoch 103/300, seasonal_3 Loss: 0.0773 | 0.0860
Epoch 104/300, seasonal_3 Loss: 0.0773 | 0.0859
Epoch 105/300, seasonal_3 Loss: 0.0772 | 0.0859
Epoch 106/300, seasonal_3 Loss: 0.0772 | 0.0859
Epoch 107/300, seasonal_3 Loss: 0.0772 | 0.0859
Epoch 108/300, seasonal_3 Loss: 0.0771 | 0.0858
Epoch 109/300, seasonal_3 Loss: 0.0771 | 0.0858
Epoch 110/300, seasonal_3 Loss: 0.0770 | 0.0858
Epoch 111/300, seasonal_3 Loss: 0.0770 | 0.0858
Epoch 112/300, seasonal_3 Loss: 0.0770 | 0.0857
Epoch 113/300, seasonal_3 Loss: 0.0769 | 0.0857
Epoch 114/300, seasonal_3 Loss: 0.0769 | 0.0857
Epoch 115/300, seasonal_3 Loss: 0.0769 | 0.0857
Epoch 116/300, seasonal_3 Loss: 0.0768 | 0.0856
Epoch 117/300, seasonal_3 Loss: 0.0768 | 0.0856
Epoch 118/300, seasonal_3 Loss: 0.0768 | 0.0856
Epoch 119/300, seasonal_3 Loss: 0.0767 | 0.0856
Epoch 120/300, seasonal_3 Loss: 0.0767 | 0.0856
Epoch 121/300, seasonal_3 Loss: 0.0767 | 0.0855
Epoch 122/300, seasonal_3 Loss: 0.0767 | 0.0855
Epoch 123/300, seasonal_3 Loss: 0.0766 | 0.0855
Epoch 124/300, seasonal_3 Loss: 0.0766 | 0.0855
Epoch 125/300, seasonal_3 Loss: 0.0766 | 0.0855
Epoch 126/300, seasonal_3 Loss: 0.0766 | 0.0855
Epoch 127/300, seasonal_3 Loss: 0.0766 | 0.0855
Epoch 128/300, seasonal_3 Loss: 0.0765 | 0.0854
Epoch 129/300, seasonal_3 Loss: 0.0765 | 0.0854
Epoch 130/300, seasonal_3 Loss: 0.0765 | 0.0854
Epoch 131/300, seasonal_3 Loss: 0.0765 | 0.0854
Epoch 132/300, seasonal_3 Loss: 0.0765 | 0.0854
Epoch 133/300, seasonal_3 Loss: 0.0764 | 0.0854
Epoch 134/300, seasonal_3 Loss: 0.0764 | 0.0854
Epoch 135/300, seasonal_3 Loss: 0.0764 | 0.0854
Epoch 136/300, seasonal_3 Loss: 0.0764 | 0.0853
Epoch 137/300, seasonal_3 Loss: 0.0764 | 0.0853
Epoch 138/300, seasonal_3 Loss: 0.0764 | 0.0853
Epoch 139/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 140/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 141/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 142/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 143/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 144/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 145/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 146/300, seasonal_3 Loss: 0.0763 | 0.0853
Epoch 147/300, seasonal_3 Loss: 0.0763 | 0.0852
Epoch 148/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 149/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 150/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 151/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 152/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 153/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 154/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 155/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 156/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 157/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 158/300, seasonal_3 Loss: 0.0762 | 0.0852
Epoch 159/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 160/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 161/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 162/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 163/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 164/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 165/300, seasonal_3 Loss: 0.0761 | 0.0852
Epoch 166/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 167/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 168/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 169/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 170/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 171/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 172/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 173/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 174/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 175/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 176/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 177/300, seasonal_3 Loss: 0.0761 | 0.0851
Epoch 178/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 179/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 180/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 181/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 182/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 183/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 184/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 185/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 186/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 187/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 188/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 189/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 190/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 191/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 192/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 193/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 194/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 195/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 196/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 197/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 198/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 199/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 200/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 201/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 202/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 203/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 204/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 205/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 206/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 207/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 208/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 209/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 210/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 211/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 212/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 213/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 214/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 215/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 216/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 217/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 218/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 219/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 220/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 221/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 222/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 223/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 224/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 225/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 226/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 227/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 228/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 229/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 230/300, seasonal_3 Loss: 0.0760 | 0.0851
Epoch 231/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 232/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 233/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 234/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 235/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 236/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 237/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 238/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 239/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 240/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 241/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 242/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 243/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 244/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 245/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 246/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 247/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 248/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 249/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 250/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 251/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 252/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 253/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 254/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 255/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 256/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 257/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 258/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 259/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 260/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 261/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 262/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 263/300, seasonal_3 Loss: 0.0760 | 0.0850
Epoch 264/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 265/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 266/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 267/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 268/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 269/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 270/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 271/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 272/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 273/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 274/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 275/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 276/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 277/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 278/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 279/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 280/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 281/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 282/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 283/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 284/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 285/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 286/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 287/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 288/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 289/300, seasonal_3 Loss: 0.0759 | 0.0850
Epoch 290/300, seasonal_3 Loss: 0.0759 | 0.0850
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 12, 'train_rates': 0.7254248577132956, 'learning_rate': 7.495406113413543e-05, 'batch_size': 54, 'step_size': 9, 'gamma': 0.9402057922558356}
Epoch 1/300, resid Loss: 0.2842 | 0.2392
Epoch 2/300, resid Loss: 0.1720 | 0.2048
Epoch 3/300, resid Loss: 0.1534 | 0.1774
Epoch 4/300, resid Loss: 0.1413 | 0.1552
Epoch 5/300, resid Loss: 0.1325 | 0.1404
Epoch 6/300, resid Loss: 0.1270 | 0.1334
Epoch 7/300, resid Loss: 0.1240 | 0.1309
Epoch 8/300, resid Loss: 0.1222 | 0.1287
Epoch 9/300, resid Loss: 0.1212 | 0.1263
Epoch 10/300, resid Loss: 0.1205 | 0.1244
Epoch 11/300, resid Loss: 0.1190 | 0.1218
Epoch 12/300, resid Loss: 0.1163 | 0.1188
Epoch 13/300, resid Loss: 0.1152 | 0.1167
Epoch 14/300, resid Loss: 0.1144 | 0.1149
Epoch 15/300, resid Loss: 0.1135 | 0.1120
Epoch 16/300, resid Loss: 0.1125 | 0.1109
Epoch 17/300, resid Loss: 0.1109 | 0.1105
Epoch 18/300, resid Loss: 0.1095 | 0.1104
Epoch 19/300, resid Loss: 0.1081 | 0.1092
Epoch 20/300, resid Loss: 0.1067 | 0.1097
Epoch 21/300, resid Loss: 0.1048 | 0.1105
Epoch 22/300, resid Loss: 0.1031 | 0.1110
Epoch 23/300, resid Loss: 0.1014 | 0.1111
Epoch 24/300, resid Loss: 0.0998 | 0.1105
Epoch 25/300, resid Loss: 0.0985 | 0.1106
Epoch 26/300, resid Loss: 0.0976 | 0.1105
Epoch 27/300, resid Loss: 0.0974 | 0.1093
Epoch 28/300, resid Loss: 0.0973 | 0.1088
Epoch 29/300, resid Loss: 0.0969 | 0.1064
Epoch 30/300, resid Loss: 0.0962 | 0.1041
Epoch 31/300, resid Loss: 0.0952 | 0.1019
Epoch 32/300, resid Loss: 0.0943 | 0.0999
Epoch 33/300, resid Loss: 0.0935 | 0.0990
Epoch 34/300, resid Loss: 0.0928 | 0.0967
Epoch 35/300, resid Loss: 0.0921 | 0.0953
Epoch 36/300, resid Loss: 0.0913 | 0.0936
Epoch 37/300, resid Loss: 0.0904 | 0.0928
Epoch 38/300, resid Loss: 0.0897 | 0.0909
Epoch 39/300, resid Loss: 0.0890 | 0.0903
Epoch 40/300, resid Loss: 0.0881 | 0.0887
Epoch 41/300, resid Loss: 0.0875 | 0.0885
Epoch 42/300, resid Loss: 0.0868 | 0.0873
Epoch 43/300, resid Loss: 0.0863 | 0.0871
Epoch 44/300, resid Loss: 0.0858 | 0.0858
Epoch 45/300, resid Loss: 0.0853 | 0.0860
Epoch 46/300, resid Loss: 0.0847 | 0.0847
Epoch 47/300, resid Loss: 0.0844 | 0.0854
Epoch 48/300, resid Loss: 0.0838 | 0.0831
Epoch 49/300, resid Loss: 0.0835 | 0.0855
Epoch 50/300, resid Loss: 0.0832 | 0.0817
Epoch 51/300, resid Loss: 0.0831 | 0.0885
Epoch 52/300, resid Loss: 0.0837 | 0.0838
Epoch 53/300, resid Loss: 0.0835 | 0.0921
Epoch 54/300, resid Loss: 0.0845 | 0.0859
Epoch 55/300, resid Loss: 0.0830 | 0.0887
Epoch 56/300, resid Loss: 0.0829 | 0.0827
Epoch 57/300, resid Loss: 0.0817 | 0.0855
Epoch 58/300, resid Loss: 0.0816 | 0.0815
Epoch 59/300, resid Loss: 0.0810 | 0.0846
Epoch 60/300, resid Loss: 0.0811 | 0.0807
Epoch 61/300, resid Loss: 0.0805 | 0.0836
Epoch 62/300, resid Loss: 0.0808 | 0.0806
Epoch 63/300, resid Loss: 0.0802 | 0.0832
Epoch 64/300, resid Loss: 0.0805 | 0.0801
Epoch 65/300, resid Loss: 0.0798 | 0.0823
Epoch 66/300, resid Loss: 0.0803 | 0.0799
Epoch 67/300, resid Loss: 0.0796 | 0.0818
Epoch 68/300, resid Loss: 0.0800 | 0.0797
Epoch 69/300, resid Loss: 0.0795 | 0.0811
Epoch 70/300, resid Loss: 0.0798 | 0.0794
Epoch 71/300, resid Loss: 0.0793 | 0.0801
Epoch 72/300, resid Loss: 0.0790 | 0.0783
Epoch 73/300, resid Loss: 0.0792 | 0.0804
Epoch 74/300, resid Loss: 0.0789 | 0.0780
Epoch 75/300, resid Loss: 0.0789 | 0.0802
Epoch 76/300, resid Loss: 0.0786 | 0.0784
Epoch 77/300, resid Loss: 0.0785 | 0.0802
Epoch 78/300, resid Loss: 0.0782 | 0.0782
Epoch 79/300, resid Loss: 0.0781 | 0.0797
Epoch 80/300, resid Loss: 0.0780 | 0.0779
Epoch 81/300, resid Loss: 0.0778 | 0.0792
Epoch 82/300, resid Loss: 0.0778 | 0.0777
Epoch 83/300, resid Loss: 0.0776 | 0.0789
Epoch 84/300, resid Loss: 0.0779 | 0.0783
Epoch 85/300, resid Loss: 0.0776 | 0.0795
Epoch 86/300, resid Loss: 0.0776 | 0.0789
Epoch 87/300, resid Loss: 0.0770 | 0.0798
Epoch 88/300, resid Loss: 0.0770 | 0.0788
Epoch 89/300, resid Loss: 0.0763 | 0.0792
Epoch 90/300, resid Loss: 0.0763 | 0.0781
Epoch 91/300, resid Loss: 0.0759 | 0.0788
Epoch 92/300, resid Loss: 0.0760 | 0.0779
Epoch 93/300, resid Loss: 0.0757 | 0.0784
Epoch 94/300, resid Loss: 0.0757 | 0.0775
Epoch 95/300, resid Loss: 0.0754 | 0.0781
Epoch 96/300, resid Loss: 0.0755 | 0.0775
Epoch 97/300, resid Loss: 0.0752 | 0.0778
Epoch 98/300, resid Loss: 0.0753 | 0.0770
Epoch 99/300, resid Loss: 0.0750 | 0.0775
Epoch 100/300, resid Loss: 0.0751 | 0.0769
Epoch 101/300, resid Loss: 0.0749 | 0.0771
Epoch 102/300, resid Loss: 0.0749 | 0.0766
Epoch 103/300, resid Loss: 0.0748 | 0.0769
Epoch 104/300, resid Loss: 0.0748 | 0.0764
Epoch 105/300, resid Loss: 0.0747 | 0.0765
Epoch 106/300, resid Loss: 0.0747 | 0.0761
Epoch 107/300, resid Loss: 0.0747 | 0.0763
Epoch 108/300, resid Loss: 0.0746 | 0.0759
Epoch 109/300, resid Loss: 0.0747 | 0.0759
Epoch 110/300, resid Loss: 0.0745 | 0.0754
Epoch 111/300, resid Loss: 0.0746 | 0.0756
Epoch 112/300, resid Loss: 0.0744 | 0.0752
Epoch 113/300, resid Loss: 0.0745 | 0.0754
Epoch 114/300, resid Loss: 0.0742 | 0.0747
Epoch 115/300, resid Loss: 0.0744 | 0.0750
Epoch 116/300, resid Loss: 0.0740 | 0.0744
Epoch 117/300, resid Loss: 0.0741 | 0.0747
Epoch 118/300, resid Loss: 0.0738 | 0.0739
Epoch 119/300, resid Loss: 0.0739 | 0.0742
Epoch 120/300, resid Loss: 0.0736 | 0.0737
Epoch 121/300, resid Loss: 0.0736 | 0.0739
Epoch 122/300, resid Loss: 0.0734 | 0.0735
Epoch 123/300, resid Loss: 0.0734 | 0.0735
Epoch 124/300, resid Loss: 0.0732 | 0.0732
Epoch 125/300, resid Loss: 0.0731 | 0.0734
Epoch 126/300, resid Loss: 0.0729 | 0.0731
Epoch 127/300, resid Loss: 0.0729 | 0.0732
Epoch 128/300, resid Loss: 0.0728 | 0.0730
Epoch 129/300, resid Loss: 0.0727 | 0.0732
Epoch 130/300, resid Loss: 0.0726 | 0.0730
Epoch 131/300, resid Loss: 0.0725 | 0.0733
Epoch 132/300, resid Loss: 0.0724 | 0.0730
Epoch 133/300, resid Loss: 0.0723 | 0.0732
Epoch 134/300, resid Loss: 0.0722 | 0.0730
Epoch 135/300, resid Loss: 0.0721 | 0.0733
Epoch 136/300, resid Loss: 0.0720 | 0.0730
Epoch 137/300, resid Loss: 0.0718 | 0.0733
Epoch 138/300, resid Loss: 0.0717 | 0.0731
Epoch 139/300, resid Loss: 0.0716 | 0.0734
Epoch 140/300, resid Loss: 0.0715 | 0.0731
Epoch 141/300, resid Loss: 0.0714 | 0.0735
Epoch 142/300, resid Loss: 0.0713 | 0.0732
Epoch 143/300, resid Loss: 0.0713 | 0.0735
Epoch 144/300, resid Loss: 0.0712 | 0.0732
Epoch 145/300, resid Loss: 0.0711 | 0.0736
Epoch 146/300, resid Loss: 0.0711 | 0.0732
Epoch 147/300, resid Loss: 0.0710 | 0.0735
Epoch 148/300, resid Loss: 0.0710 | 0.0732
Epoch 149/300, resid Loss: 0.0710 | 0.0735
Epoch 150/300, resid Loss: 0.0709 | 0.0732
Epoch 151/300, resid Loss: 0.0709 | 0.0735
Epoch 152/300, resid Loss: 0.0709 | 0.0732
Epoch 153/300, resid Loss: 0.0708 | 0.0734
Epoch 154/300, resid Loss: 0.0708 | 0.0731
Epoch 155/300, resid Loss: 0.0708 | 0.0733
Epoch 156/300, resid Loss: 0.0707 | 0.0730
Epoch 157/300, resid Loss: 0.0707 | 0.0732
Epoch 158/300, resid Loss: 0.0706 | 0.0729
Epoch 159/300, resid Loss: 0.0706 | 0.0730
Epoch 160/300, resid Loss: 0.0706 | 0.0728
Epoch 161/300, resid Loss: 0.0705 | 0.0729
Epoch 162/300, resid Loss: 0.0705 | 0.0727
Epoch 163/300, resid Loss: 0.0705 | 0.0727
Epoch 164/300, resid Loss: 0.0705 | 0.0726
Epoch 165/300, resid Loss: 0.0704 | 0.0726
Epoch 166/300, resid Loss: 0.0704 | 0.0725
Epoch 167/300, resid Loss: 0.0703 | 0.0725
Epoch 168/300, resid Loss: 0.0703 | 0.0723
Epoch 169/300, resid Loss: 0.0703 | 0.0723
Epoch 170/300, resid Loss: 0.0703 | 0.0723
Epoch 171/300, resid Loss: 0.0703 | 0.0723
Epoch 172/300, resid Loss: 0.0703 | 0.0722
Epoch 173/300, resid Loss: 0.0703 | 0.0723
Epoch 174/300, resid Loss: 0.0703 | 0.0722
Epoch 175/300, resid Loss: 0.0703 | 0.0723
Epoch 176/300, resid Loss: 0.0703 | 0.0723
Epoch 177/300, resid Loss: 0.0704 | 0.0724
Epoch 178/300, resid Loss: 0.0705 | 0.0723
Epoch 179/300, resid Loss: 0.0706 | 0.0724
Epoch 180/300, resid Loss: 0.0707 | 0.0724
Epoch 181/300, resid Loss: 0.0707 | 0.0724
Epoch 182/300, resid Loss: 0.0707 | 0.0723
Epoch 183/300, resid Loss: 0.0707 | 0.0723
Epoch 184/300, resid Loss: 0.0706 | 0.0722
Epoch 185/300, resid Loss: 0.0704 | 0.0722
Epoch 186/300, resid Loss: 0.0703 | 0.0722
Epoch 187/300, resid Loss: 0.0701 | 0.0722
Epoch 188/300, resid Loss: 0.0699 | 0.0722
Epoch 189/300, resid Loss: 0.0697 | 0.0722
Epoch 190/300, resid Loss: 0.0696 | 0.0721
Epoch 191/300, resid Loss: 0.0694 | 0.0721
Epoch 192/300, resid Loss: 0.0693 | 0.0721
Epoch 193/300, resid Loss: 0.0692 | 0.0721
Epoch 194/300, resid Loss: 0.0691 | 0.0721
Epoch 195/300, resid Loss: 0.0690 | 0.0720
Epoch 196/300, resid Loss: 0.0689 | 0.0720
Epoch 197/300, resid Loss: 0.0689 | 0.0719
Epoch 198/300, resid Loss: 0.0688 | 0.0719
Epoch 199/300, resid Loss: 0.0688 | 0.0718
Epoch 200/300, resid Loss: 0.0688 | 0.0718
Epoch 201/300, resid Loss: 0.0687 | 0.0717
Epoch 202/300, resid Loss: 0.0687 | 0.0717
Epoch 203/300, resid Loss: 0.0687 | 0.0716
Epoch 204/300, resid Loss: 0.0686 | 0.0716
Epoch 205/300, resid Loss: 0.0686 | 0.0716
Epoch 206/300, resid Loss: 0.0686 | 0.0716
Epoch 207/300, resid Loss: 0.0685 | 0.0716
Epoch 208/300, resid Loss: 0.0685 | 0.0716
Epoch 209/300, resid Loss: 0.0685 | 0.0716
Epoch 210/300, resid Loss: 0.0685 | 0.0716
Epoch 211/300, resid Loss: 0.0685 | 0.0716
Epoch 212/300, resid Loss: 0.0684 | 0.0716
Epoch 213/300, resid Loss: 0.0684 | 0.0716
Epoch 214/300, resid Loss: 0.0684 | 0.0716
Epoch 215/300, resid Loss: 0.0684 | 0.0716
Epoch 216/300, resid Loss: 0.0684 | 0.0716
Epoch 217/300, resid Loss: 0.0684 | 0.0716
Epoch 218/300, resid Loss: 0.0683 | 0.0716
Epoch 219/300, resid Loss: 0.0683 | 0.0716
Epoch 220/300, resid Loss: 0.0683 | 0.0716
Epoch 221/300, resid Loss: 0.0683 | 0.0715
Epoch 222/300, resid Loss: 0.0682 | 0.0715
Epoch 223/300, resid Loss: 0.0682 | 0.0715
Epoch 224/300, resid Loss: 0.0682 | 0.0715
Epoch 225/300, resid Loss: 0.0681 | 0.0715
Epoch 226/300, resid Loss: 0.0681 | 0.0715
Epoch 227/300, resid Loss: 0.0681 | 0.0715
Epoch 228/300, resid Loss: 0.0680 | 0.0715
Epoch 229/300, resid Loss: 0.0680 | 0.0715
Epoch 230/300, resid Loss: 0.0680 | 0.0715
Epoch 231/300, resid Loss: 0.0680 | 0.0714
Epoch 232/300, resid Loss: 0.0679 | 0.0714
Epoch 233/300, resid Loss: 0.0679 | 0.0714
Epoch 234/300, resid Loss: 0.0679 | 0.0714
Epoch 235/300, resid Loss: 0.0679 | 0.0714
Epoch 236/300, resid Loss: 0.0679 | 0.0714
Epoch 237/300, resid Loss: 0.0679 | 0.0714
Epoch 238/300, resid Loss: 0.0678 | 0.0714
Epoch 239/300, resid Loss: 0.0678 | 0.0714
Epoch 240/300, resid Loss: 0.0678 | 0.0714
Epoch 241/300, resid Loss: 0.0678 | 0.0714
Epoch 242/300, resid Loss: 0.0678 | 0.0714
Epoch 243/300, resid Loss: 0.0678 | 0.0714
Epoch 244/300, resid Loss: 0.0677 | 0.0714
Epoch 245/300, resid Loss: 0.0677 | 0.0714
Epoch 246/300, resid Loss: 0.0677 | 0.0714
Epoch 247/300, resid Loss: 0.0677 | 0.0714
Epoch 248/300, resid Loss: 0.0677 | 0.0714
Epoch 249/300, resid Loss: 0.0677 | 0.0714
Epoch 250/300, resid Loss: 0.0676 | 0.0714
Epoch 251/300, resid Loss: 0.0676 | 0.0714
Epoch 252/300, resid Loss: 0.0676 | 0.0714
Epoch 253/300, resid Loss: 0.0676 | 0.0713
Epoch 254/300, resid Loss: 0.0676 | 0.0713
Epoch 255/300, resid Loss: 0.0676 | 0.0713
Epoch 256/300, resid Loss: 0.0675 | 0.0713
Epoch 257/300, resid Loss: 0.0675 | 0.0713
Epoch 258/300, resid Loss: 0.0675 | 0.0713
Epoch 259/300, resid Loss: 0.0675 | 0.0713
Epoch 260/300, resid Loss: 0.0675 | 0.0713
Epoch 261/300, resid Loss: 0.0675 | 0.0713
Epoch 262/300, resid Loss: 0.0674 | 0.0713
Epoch 263/300, resid Loss: 0.0674 | 0.0713
Epoch 264/300, resid Loss: 0.0674 | 0.0713
Epoch 265/300, resid Loss: 0.0674 | 0.0713
Epoch 266/300, resid Loss: 0.0674 | 0.0713
Epoch 267/300, resid Loss: 0.0674 | 0.0713
Epoch 268/300, resid Loss: 0.0674 | 0.0713
Epoch 269/300, resid Loss: 0.0673 | 0.0713
Epoch 270/300, resid Loss: 0.0673 | 0.0713
Epoch 271/300, resid Loss: 0.0673 | 0.0713
Epoch 272/300, resid Loss: 0.0673 | 0.0713
Epoch 273/300, resid Loss: 0.0673 | 0.0713
Epoch 274/300, resid Loss: 0.0673 | 0.0713
Epoch 275/300, resid Loss: 0.0673 | 0.0713
Epoch 276/300, resid Loss: 0.0673 | 0.0713
Epoch 277/300, resid Loss: 0.0673 | 0.0713
Epoch 278/300, resid Loss: 0.0672 | 0.0713
Epoch 279/300, resid Loss: 0.0672 | 0.0713
Epoch 280/300, resid Loss: 0.0672 | 0.0713
Epoch 281/300, resid Loss: 0.0672 | 0.0713
Epoch 282/300, resid Loss: 0.0672 | 0.0713
Epoch 283/300, resid Loss: 0.0672 | 0.0713
Epoch 284/300, resid Loss: 0.0672 | 0.0713
Epoch 285/300, resid Loss: 0.0672 | 0.0713
Epoch 286/300, resid Loss: 0.0672 | 0.0713
Epoch 287/300, resid Loss: 0.0671 | 0.0712
Epoch 288/300, resid Loss: 0.0671 | 0.0712
Epoch 289/300, resid Loss: 0.0671 | 0.0712
Epoch 290/300, resid Loss: 0.0671 | 0.0712
Epoch 291/300, resid Loss: 0.0671 | 0.0712
Epoch 292/300, resid Loss: 0.0671 | 0.0712
Epoch 293/300, resid Loss: 0.0671 | 0.0712
Epoch 294/300, resid Loss: 0.0671 | 0.0712
Epoch 295/300, resid Loss: 0.0671 | 0.0712
Epoch 296/300, resid Loss: 0.0671 | 0.0712
Epoch 297/300, resid Loss: 0.0671 | 0.0712
Epoch 298/300, resid Loss: 0.0670 | 0.0712
Epoch 299/300, resid Loss: 0.0670 | 0.0712
Epoch 300/300, resid Loss: 0.0670 | 0.0712
Runtime (seconds): 1465.271388053894
6.708154129662126e-05
[99.02269]
[4.9688473]
[-1.7320287]
[0.44930196]
[-3.2438707]
[-2.80146]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 34.064934589900076
RMSE: 5.836517333984375
MAE: 5.836517333984375
R-squared: nan
[96.66348]
