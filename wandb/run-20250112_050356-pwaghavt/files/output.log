ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-12 05:04:00,412][0m A new study created in memory with name: no-name-5751ee56-34d3-4b75-8d17-93ab32ad404b[0m
[32m[I 2025-01-12 05:04:26,683][0m Trial 0 finished with value: 0.126257350160317 and parameters: {'observation_period_num': 81, 'train_rates': 0.8622664221613746, 'learning_rate': 0.00013277864617077878, 'batch_size': 210, 'step_size': 13, 'gamma': 0.8398486687676806}. Best is trial 0 with value: 0.126257350160317.[0m
[32m[I 2025-01-12 05:04:51,103][0m Trial 1 finished with value: 2.06747278876582 and parameters: {'observation_period_num': 11, 'train_rates': 0.8772174945278777, 'learning_rate': 5.219070365666046e-06, 'batch_size': 252, 'step_size': 4, 'gamma': 0.8683213924590413}. Best is trial 0 with value: 0.126257350160317.[0m
[32m[I 2025-01-12 05:05:21,697][0m Trial 2 finished with value: 0.10679109085787523 and parameters: {'observation_period_num': 52, 'train_rates': 0.8471724112449177, 'learning_rate': 7.74293447564959e-05, 'batch_size': 182, 'step_size': 15, 'gamma': 0.9075085543231295}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:07:22,000][0m Trial 3 finished with value: 0.505164858945728 and parameters: {'observation_period_num': 41, 'train_rates': 0.8839832465885777, 'learning_rate': 3.214147408681483e-06, 'batch_size': 45, 'step_size': 10, 'gamma': 0.9551141200982678}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:07:58,272][0m Trial 4 finished with value: 1.5257983956226082 and parameters: {'observation_period_num': 123, 'train_rates': 0.7265757492991965, 'learning_rate': 3.1412542307670126e-06, 'batch_size': 135, 'step_size': 8, 'gamma': 0.803112164074474}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:08:19,291][0m Trial 5 finished with value: 0.26959237828850746 and parameters: {'observation_period_num': 234, 'train_rates': 0.7337089585654419, 'learning_rate': 0.00010008334568498934, 'batch_size': 245, 'step_size': 12, 'gamma': 0.9524734432887184}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:09:24,807][0m Trial 6 finished with value: 0.3848378658294678 and parameters: {'observation_period_num': 10, 'train_rates': 0.9729173007114778, 'learning_rate': 5.838247260780231e-06, 'batch_size': 92, 'step_size': 8, 'gamma': 0.9647817189750406}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:09:52,941][0m Trial 7 finished with value: 0.2407404680812232 and parameters: {'observation_period_num': 129, 'train_rates': 0.6998765281320625, 'learning_rate': 0.00020936921267289516, 'batch_size': 178, 'step_size': 6, 'gamma': 0.7781638855546782}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:11:02,551][0m Trial 8 finished with value: 0.6135809283647964 and parameters: {'observation_period_num': 212, 'train_rates': 0.6088788750318869, 'learning_rate': 5.62910990862298e-06, 'batch_size': 56, 'step_size': 6, 'gamma': 0.9698532979379455}. Best is trial 2 with value: 0.10679109085787523.[0m
[32m[I 2025-01-12 05:11:22,157][0m Trial 9 finished with value: 0.10087738148236679 and parameters: {'observation_period_num': 93, 'train_rates': 0.6493767945883949, 'learning_rate': 0.0003595222544122704, 'batch_size': 244, 'step_size': 15, 'gamma': 0.94372001790288}. Best is trial 9 with value: 0.10087738148236679.[0m
[32m[I 2025-01-12 05:11:55,803][0m Trial 10 finished with value: 0.25619592139071096 and parameters: {'observation_period_num': 170, 'train_rates': 0.6214976329555881, 'learning_rate': 0.0007888256704852422, 'batch_size': 128, 'step_size': 15, 'gamma': 0.9155949373147179}. Best is trial 9 with value: 0.10087738148236679.[0m
[32m[I 2025-01-12 05:12:23,035][0m Trial 11 finished with value: 0.3031378578851466 and parameters: {'observation_period_num': 75, 'train_rates': 0.7990159634256774, 'learning_rate': 2.7810545813109e-05, 'batch_size': 196, 'step_size': 15, 'gamma': 0.9127089270496708}. Best is trial 9 with value: 0.10087738148236679.[0m
[32m[I 2025-01-12 05:12:54,415][0m Trial 12 finished with value: 0.12739206589222382 and parameters: {'observation_period_num': 78, 'train_rates': 0.8068205307256999, 'learning_rate': 0.0009727900315457725, 'batch_size': 173, 'step_size': 12, 'gamma': 0.9042154604357946}. Best is trial 9 with value: 0.10087738148236679.[0m
Early stopping at epoch 86
[32m[I 2025-01-12 05:13:18,705][0m Trial 13 finished with value: 0.9720653891563416 and parameters: {'observation_period_num': 118, 'train_rates': 0.9564289610733274, 'learning_rate': 4.2749163023595776e-05, 'batch_size': 222, 'step_size': 1, 'gamma': 0.8796067727043363}. Best is trial 9 with value: 0.10087738148236679.[0m
[32m[I 2025-01-12 05:13:48,365][0m Trial 14 finished with value: 0.07412681726993994 and parameters: {'observation_period_num': 52, 'train_rates': 0.680384843083343, 'learning_rate': 0.00036955213509839595, 'batch_size': 163, 'step_size': 15, 'gamma': 0.9892689359116243}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:14:31,683][0m Trial 15 finished with value: 0.13543178212426282 and parameters: {'observation_period_num': 103, 'train_rates': 0.6811244915020933, 'learning_rate': 0.00036636943858129066, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9899047605975996}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:18:39,143][0m Trial 16 finished with value: 0.30760756518026133 and parameters: {'observation_period_num': 164, 'train_rates': 0.6568234358356626, 'learning_rate': 0.00033975908004281317, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9374385273098251}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:19:12,813][0m Trial 17 finished with value: 0.2869540576241622 and parameters: {'observation_period_num': 43, 'train_rates': 0.751332105445728, 'learning_rate': 1.8784035060526807e-05, 'batch_size': 149, 'step_size': 13, 'gamma': 0.9882770024815702}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:19:32,551][0m Trial 18 finished with value: 1.8632951592131382 and parameters: {'observation_period_num': 166, 'train_rates': 0.6490850902139409, 'learning_rate': 1.0231874782300369e-06, 'batch_size': 230, 'step_size': 10, 'gamma': 0.9343204919618202}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:20:01,825][0m Trial 19 finished with value: 0.15820986597657308 and parameters: {'observation_period_num': 92, 'train_rates': 0.6016688542408419, 'learning_rate': 0.00046115102871979914, 'batch_size': 156, 'step_size': 14, 'gamma': 0.8430688444816934}. Best is trial 14 with value: 0.07412681726993994.[0m
Early stopping at epoch 95
[32m[I 2025-01-12 05:20:48,493][0m Trial 20 finished with value: 0.29251387706456705 and parameters: {'observation_period_num': 57, 'train_rates': 0.7683587847374505, 'learning_rate': 0.00020492758308451504, 'batch_size': 104, 'step_size': 2, 'gamma': 0.7505117741536856}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:21:17,333][0m Trial 21 finished with value: 0.10740105169160026 and parameters: {'observation_period_num': 33, 'train_rates': 0.8345863860451312, 'learning_rate': 9.507426361635774e-05, 'batch_size': 197, 'step_size': 15, 'gamma': 0.8871231760541776}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:21:45,112][0m Trial 22 finished with value: 0.1535763913653495 and parameters: {'observation_period_num': 59, 'train_rates': 0.6956488043420659, 'learning_rate': 4.970352609336427e-05, 'batch_size': 177, 'step_size': 14, 'gamma': 0.9351518614466426}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:22:11,033][0m Trial 23 finished with value: 0.09493285417556763 and parameters: {'observation_period_num': 23, 'train_rates': 0.9356918870463751, 'learning_rate': 0.0004927919521873551, 'batch_size': 230, 'step_size': 13, 'gamma': 0.9286908395201118}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:22:35,133][0m Trial 24 finished with value: 0.1121569573879242 and parameters: {'observation_period_num': 24, 'train_rates': 0.9232569821679476, 'learning_rate': 0.0005173163232644027, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9754326953607987}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:22:56,892][0m Trial 25 finished with value: 0.12439184508153371 and parameters: {'observation_period_num': 107, 'train_rates': 0.6556161791129671, 'learning_rate': 0.00023014877532975274, 'batch_size': 228, 'step_size': 13, 'gamma': 0.9463404261743082}. Best is trial 14 with value: 0.07412681726993994.[0m
[32m[I 2025-01-12 05:23:24,227][0m Trial 26 finished with value: 0.033997409378311465 and parameters: {'observation_period_num': 5, 'train_rates': 0.7816413740183745, 'learning_rate': 0.0007539060750226302, 'batch_size': 207, 'step_size': 14, 'gamma': 0.9235069129502878}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:23:51,767][0m Trial 27 finished with value: 0.04779466262552887 and parameters: {'observation_period_num': 22, 'train_rates': 0.7815333286222408, 'learning_rate': 0.0006340964943085931, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8484808739063688}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:24:25,483][0m Trial 28 finished with value: 0.041680926018778014 and parameters: {'observation_period_num': 8, 'train_rates': 0.7690453780394959, 'learning_rate': 0.000702318049875196, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8291568163037991}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:24:52,876][0m Trial 29 finished with value: 0.043746220026391905 and parameters: {'observation_period_num': 13, 'train_rates': 0.7818809335905215, 'learning_rate': 0.0007555440501860599, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8317456767077246}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:25:20,181][0m Trial 30 finished with value: 0.03507095724207042 and parameters: {'observation_period_num': 8, 'train_rates': 0.8156165425854297, 'learning_rate': 0.0009680459580830426, 'batch_size': 212, 'step_size': 8, 'gamma': 0.8194955637864153}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:25:47,220][0m Trial 31 finished with value: 0.03940544060522659 and parameters: {'observation_period_num': 9, 'train_rates': 0.8162559420637289, 'learning_rate': 0.000947902448600042, 'batch_size': 209, 'step_size': 8, 'gamma': 0.8131046824601473}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:26:14,044][0m Trial 32 finished with value: 0.047715952571797636 and parameters: {'observation_period_num': 11, 'train_rates': 0.8157052872224891, 'learning_rate': 0.0009321279546361085, 'batch_size': 213, 'step_size': 6, 'gamma': 0.8174148221576901}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:26:45,283][0m Trial 33 finished with value: 0.0976665424661446 and parameters: {'observation_period_num': 32, 'train_rates': 0.8627641018405795, 'learning_rate': 0.00016181042870665833, 'batch_size': 182, 'step_size': 9, 'gamma': 0.7908717850170676}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:27:15,577][0m Trial 34 finished with value: 0.04365837230504333 and parameters: {'observation_period_num': 5, 'train_rates': 0.8956168015192157, 'learning_rate': 0.0009893341379669444, 'batch_size': 193, 'step_size': 7, 'gamma': 0.8244739828258368}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:27:52,555][0m Trial 35 finished with value: 0.06582136829628756 and parameters: {'observation_period_num': 40, 'train_rates': 0.8436201837234607, 'learning_rate': 0.00024803224967593496, 'batch_size': 150, 'step_size': 9, 'gamma': 0.8574079649417652}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:28:17,782][0m Trial 36 finished with value: 0.10148014208754978 and parameters: {'observation_period_num': 70, 'train_rates': 0.8203768346549871, 'learning_rate': 0.0005958196165612061, 'batch_size': 219, 'step_size': 4, 'gamma': 0.8021646366986082}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:28:41,006][0m Trial 37 finished with value: 0.7558574448543716 and parameters: {'observation_period_num': 5, 'train_rates': 0.7445376182078779, 'learning_rate': 1.5128877590952933e-05, 'batch_size': 240, 'step_size': 7, 'gamma': 0.8621559325681108}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:29:22,654][0m Trial 38 finished with value: 0.07283575675150813 and parameters: {'observation_period_num': 24, 'train_rates': 0.7684327987137901, 'learning_rate': 0.00013937947543372518, 'batch_size': 119, 'step_size': 9, 'gamma': 0.7727399329459671}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:29:50,770][0m Trial 39 finished with value: 0.08277902674991017 and parameters: {'observation_period_num': 43, 'train_rates': 0.7212186157287062, 'learning_rate': 0.000636264730162617, 'batch_size': 186, 'step_size': 4, 'gamma': 0.809490501696054}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:30:31,451][0m Trial 40 finished with value: 0.13730825425742507 and parameters: {'observation_period_num': 20, 'train_rates': 0.8689673846852617, 'learning_rate': 6.597875673988715e-05, 'batch_size': 138, 'step_size': 7, 'gamma': 0.7909672372175828}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:31:01,706][0m Trial 41 finished with value: 0.04082009585791843 and parameters: {'observation_period_num': 8, 'train_rates': 0.8808545337873577, 'learning_rate': 0.0008777051061378293, 'batch_size': 193, 'step_size': 7, 'gamma': 0.825044836514484}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:31:35,585][0m Trial 42 finished with value: 0.07843530019307697 and parameters: {'observation_period_num': 32, 'train_rates': 0.8979360109149792, 'learning_rate': 0.0002938942927626858, 'batch_size': 171, 'step_size': 5, 'gamma': 0.8339320654808569}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:32:02,430][0m Trial 43 finished with value: 0.0445397020238199 and parameters: {'observation_period_num': 6, 'train_rates': 0.8251102006460127, 'learning_rate': 0.0005052264904210535, 'batch_size': 209, 'step_size': 8, 'gamma': 0.8177618591265101}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:32:30,888][0m Trial 44 finished with value: 0.1404049351409662 and parameters: {'observation_period_num': 247, 'train_rates': 0.8494820543491801, 'learning_rate': 0.000665652621578528, 'batch_size': 191, 'step_size': 11, 'gamma': 0.848556653753868}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:33:02,194][0m Trial 45 finished with value: 0.057205122944555785 and parameters: {'observation_period_num': 67, 'train_rates': 0.7827794619511333, 'learning_rate': 0.0009550238844804323, 'batch_size': 165, 'step_size': 10, 'gamma': 0.7985683681656573}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:33:25,008][0m Trial 46 finished with value: 0.1227977921126588 and parameters: {'observation_period_num': 138, 'train_rates': 0.7988488579586237, 'learning_rate': 0.00041194315540921133, 'batch_size': 242, 'step_size': 6, 'gamma': 0.8766891936862495}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:33:50,423][0m Trial 47 finished with value: 0.07561795220879669 and parameters: {'observation_period_num': 48, 'train_rates': 0.7578243293408997, 'learning_rate': 0.0007386861220293188, 'batch_size': 201, 'step_size': 5, 'gamma': 0.8924999068823082}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:34:16,562][0m Trial 48 finished with value: 0.06935784623828893 and parameters: {'observation_period_num': 14, 'train_rates': 0.8799914783958896, 'learning_rate': 0.00027734735652176534, 'batch_size': 219, 'step_size': 7, 'gamma': 0.7818709062162401}. Best is trial 26 with value: 0.033997409378311465.[0m
[32m[I 2025-01-12 05:34:41,803][0m Trial 49 finished with value: 0.23465968089655412 and parameters: {'observation_period_num': 204, 'train_rates': 0.7358821789425185, 'learning_rate': 0.0001731848780920629, 'batch_size': 188, 'step_size': 9, 'gamma': 0.8136006900174083}. Best is trial 26 with value: 0.033997409378311465.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-12 05:34:41,813][0m A new study created in memory with name: no-name-93c210fe-cd36-4e5f-bda5-4d81cef3b2e3[0m
[32m[I 2025-01-12 05:35:53,566][0m Trial 0 finished with value: 0.6741361033943023 and parameters: {'observation_period_num': 179, 'train_rates': 0.8383101455345568, 'learning_rate': 2.7873972171844856e-06, 'batch_size': 70, 'step_size': 6, 'gamma': 0.7785136159320041}. Best is trial 0 with value: 0.6741361033943023.[0m
[32m[I 2025-01-12 05:36:21,748][0m Trial 1 finished with value: 1.2994165420532227 and parameters: {'observation_period_num': 225, 'train_rates': 0.9684701627666743, 'learning_rate': 1.37794087242727e-06, 'batch_size': 215, 'step_size': 9, 'gamma': 0.9246310899064255}. Best is trial 0 with value: 0.6741361033943023.[0m
[32m[I 2025-01-12 05:37:19,184][0m Trial 2 finished with value: 0.6780921470932115 and parameters: {'observation_period_num': 210, 'train_rates': 0.6276423915123964, 'learning_rate': 1.7474505378608955e-06, 'batch_size': 69, 'step_size': 8, 'gamma': 0.8148211823262116}. Best is trial 0 with value: 0.6741361033943023.[0m
[32m[I 2025-01-12 05:37:55,097][0m Trial 3 finished with value: 0.2130810964391768 and parameters: {'observation_period_num': 25, 'train_rates': 0.7707816893207714, 'learning_rate': 2.754605563278881e-05, 'batch_size': 148, 'step_size': 3, 'gamma': 0.8098817807323995}. Best is trial 3 with value: 0.2130810964391768.[0m
[32m[I 2025-01-12 05:39:49,899][0m Trial 4 finished with value: 0.08363141867026459 and parameters: {'observation_period_num': 57, 'train_rates': 0.6380035905201136, 'learning_rate': 4.106890978578228e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8664800313849348}. Best is trial 4 with value: 0.08363141867026459.[0m
[32m[I 2025-01-12 05:40:11,011][0m Trial 5 finished with value: 0.11371185033526642 and parameters: {'observation_period_num': 150, 'train_rates': 0.7305732309375175, 'learning_rate': 0.0004955195883157222, 'batch_size': 250, 'step_size': 3, 'gamma': 0.8827100813039227}. Best is trial 4 with value: 0.08363141867026459.[0m
[32m[I 2025-01-12 05:41:03,336][0m Trial 6 finished with value: 0.09876233990928128 and parameters: {'observation_period_num': 161, 'train_rates': 0.8917191562279454, 'learning_rate': 5.251141053925563e-05, 'batch_size': 103, 'step_size': 3, 'gamma': 0.954783403488797}. Best is trial 4 with value: 0.08363141867026459.[0m
[32m[I 2025-01-12 05:42:08,470][0m Trial 7 finished with value: 0.7201976734126565 and parameters: {'observation_period_num': 180, 'train_rates': 0.7876896129245052, 'learning_rate': 3.4953786509493218e-06, 'batch_size': 75, 'step_size': 2, 'gamma': 0.8978406732939685}. Best is trial 4 with value: 0.08363141867026459.[0m
[32m[I 2025-01-12 05:44:38,534][0m Trial 8 finished with value: 0.11497998038628494 and parameters: {'observation_period_num': 27, 'train_rates': 0.659419265185455, 'learning_rate': 2.1756032393195625e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.7872927223412616}. Best is trial 4 with value: 0.08363141867026459.[0m
[32m[I 2025-01-12 05:45:49,642][0m Trial 9 finished with value: 0.1888027453755999 and parameters: {'observation_period_num': 196, 'train_rates': 0.7924534955314784, 'learning_rate': 1.1871535452912256e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9179020294822725}. Best is trial 4 with value: 0.08363141867026459.[0m
[32m[I 2025-01-12 05:46:21,678][0m Trial 10 finished with value: 0.07948509067834567 and parameters: {'observation_period_num': 95, 'train_rates': 0.6894723019895265, 'learning_rate': 0.00015765157925614972, 'batch_size': 150, 'step_size': 14, 'gamma': 0.8435136873577653}. Best is trial 10 with value: 0.07948509067834567.[0m
[32m[I 2025-01-12 05:46:51,969][0m Trial 11 finished with value: 0.07856927659976233 and parameters: {'observation_period_num': 86, 'train_rates': 0.6993434833155842, 'learning_rate': 0.00017828198052802647, 'batch_size': 157, 'step_size': 15, 'gamma': 0.8454158773444788}. Best is trial 11 with value: 0.07856927659976233.[0m
[32m[I 2025-01-12 05:47:22,368][0m Trial 12 finished with value: 0.16825107015496077 and parameters: {'observation_period_num': 93, 'train_rates': 0.6991491611024326, 'learning_rate': 0.0002671524715550561, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8482713796394965}. Best is trial 11 with value: 0.07856927659976233.[0m
[32m[I 2025-01-12 05:47:50,335][0m Trial 13 finished with value: 0.09813011123741866 and parameters: {'observation_period_num': 109, 'train_rates': 0.7295645991173806, 'learning_rate': 0.00014747236665987423, 'batch_size': 176, 'step_size': 15, 'gamma': 0.8283494559651042}. Best is trial 11 with value: 0.07856927659976233.[0m
[32m[I 2025-01-12 05:48:29,138][0m Trial 14 finished with value: 0.07977415736654983 and parameters: {'observation_period_num': 74, 'train_rates': 0.6814427099401115, 'learning_rate': 0.000798778161084845, 'batch_size': 116, 'step_size': 13, 'gamma': 0.7595966218251644}. Best is trial 11 with value: 0.07856927659976233.[0m
[32m[I 2025-01-12 05:48:53,649][0m Trial 15 finished with value: 0.19920616260971183 and parameters: {'observation_period_num': 127, 'train_rates': 0.6175803284457999, 'learning_rate': 0.00016294114898278688, 'batch_size': 193, 'step_size': 13, 'gamma': 0.9868056788119279}. Best is trial 11 with value: 0.07856927659976233.[0m
[32m[I 2025-01-12 05:49:31,983][0m Trial 16 finished with value: 0.07084777473493387 and parameters: {'observation_period_num': 61, 'train_rates': 0.7283020520445743, 'learning_rate': 9.013844127052036e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8526694279078563}. Best is trial 16 with value: 0.07084777473493387.[0m
[32m[I 2025-01-12 05:50:19,491][0m Trial 17 finished with value: 0.05161050043818427 and parameters: {'observation_period_num': 49, 'train_rates': 0.8728751459903883, 'learning_rate': 7.808483486168946e-05, 'batch_size': 119, 'step_size': 12, 'gamma': 0.8593757370073364}. Best is trial 17 with value: 0.05161050043818427.[0m
[32m[I 2025-01-12 05:51:08,878][0m Trial 18 finished with value: 0.045091157874489085 and parameters: {'observation_period_num': 5, 'train_rates': 0.8743296060162399, 'learning_rate': 6.300748197496e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8694752705768807}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:52:03,768][0m Trial 19 finished with value: 0.0902011661845095 and parameters: {'observation_period_num': 16, 'train_rates': 0.8900485380604384, 'learning_rate': 1.0337580182188653e-05, 'batch_size': 104, 'step_size': 10, 'gamma': 0.9077057089114888}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:53:07,854][0m Trial 20 finished with value: 0.13889148831367493 and parameters: {'observation_period_num': 5, 'train_rates': 0.9700655872320619, 'learning_rate': 1.0003506332681406e-05, 'batch_size': 94, 'step_size': 6, 'gamma': 0.8706260192288477}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:53:51,168][0m Trial 21 finished with value: 0.051627375425830964 and parameters: {'observation_period_num': 49, 'train_rates': 0.8504986126758988, 'learning_rate': 7.12169333429e-05, 'batch_size': 126, 'step_size': 11, 'gamma': 0.8822903535033788}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:54:32,305][0m Trial 22 finished with value: 0.047223647193642526 and parameters: {'observation_period_num': 43, 'train_rates': 0.855389461942476, 'learning_rate': 7.90200914042783e-05, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8868729320809084}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:55:16,417][0m Trial 23 finished with value: 0.04987668121854464 and parameters: {'observation_period_num': 41, 'train_rates': 0.9196355888422671, 'learning_rate': 0.000328224076570228, 'batch_size': 132, 'step_size': 10, 'gamma': 0.9402101782974112}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:55:48,593][0m Trial 24 finished with value: 0.0815009359141876 and parameters: {'observation_period_num': 35, 'train_rates': 0.9304818279534, 'learning_rate': 0.00045790807298786486, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9414869585686755}. Best is trial 18 with value: 0.045091157874489085.[0m
[32m[I 2025-01-12 05:56:31,049][0m Trial 25 finished with value: 0.03246521435172881 and parameters: {'observation_period_num': 6, 'train_rates': 0.9199071979324327, 'learning_rate': 0.00031022172193661917, 'batch_size': 140, 'step_size': 10, 'gamma': 0.957598029319277}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 05:56:58,112][0m Trial 26 finished with value: 0.06992602524018184 and parameters: {'observation_period_num': 6, 'train_rates': 0.8446948197707194, 'learning_rate': 1.74749575746472e-05, 'batch_size': 218, 'step_size': 10, 'gamma': 0.9799128954314497}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 05:57:59,525][0m Trial 27 finished with value: 0.045319946482777596 and parameters: {'observation_period_num': 24, 'train_rates': 0.9371693246492785, 'learning_rate': 0.0009230697002449479, 'batch_size': 92, 'step_size': 7, 'gamma': 0.8926601639322037}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 05:59:50,336][0m Trial 28 finished with value: 0.045499637436408266 and parameters: {'observation_period_num': 20, 'train_rates': 0.9382391453887855, 'learning_rate': 0.0008287721390577524, 'batch_size': 51, 'step_size': 7, 'gamma': 0.9650261318651626}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 06:00:51,109][0m Trial 29 finished with value: 0.14182780528297792 and parameters: {'observation_period_num': 242, 'train_rates': 0.9043177408410397, 'learning_rate': 0.0009765843680717733, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9341602123333526}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 06:01:53,639][0m Trial 30 finished with value: 0.08861590738468929 and parameters: {'observation_period_num': 73, 'train_rates': 0.8186317440851478, 'learning_rate': 0.00048385640624138267, 'batch_size': 83, 'step_size': 9, 'gamma': 0.9001394960385376}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 06:04:30,068][0m Trial 31 finished with value: 0.0348877561662127 and parameters: {'observation_period_num': 17, 'train_rates': 0.9421984203309854, 'learning_rate': 0.0007362838373950675, 'batch_size': 36, 'step_size': 7, 'gamma': 0.9663412480702789}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 06:09:45,795][0m Trial 32 finished with value: 0.03381896132557857 and parameters: {'observation_period_num': 9, 'train_rates': 0.9583637613489043, 'learning_rate': 0.0002864344486562007, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9693475141633038}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 06:11:43,448][0m Trial 33 finished with value: 0.034726482192593174 and parameters: {'observation_period_num': 9, 'train_rates': 0.9590594695253939, 'learning_rate': 0.0002939814943357365, 'batch_size': 49, 'step_size': 5, 'gamma': 0.9675052511864238}. Best is trial 25 with value: 0.03246521435172881.[0m
[32m[I 2025-01-12 06:13:56,666][0m Trial 34 finished with value: 0.02745640277862549 and parameters: {'observation_period_num': 31, 'train_rates': 0.9866913972441052, 'learning_rate': 0.0002662691933817323, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9660786284246138}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:18:59,239][0m Trial 35 finished with value: 0.06189528563670043 and parameters: {'observation_period_num': 33, 'train_rates': 0.9801996679352064, 'learning_rate': 0.00027477740256995516, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9576019655642465}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:20:45,753][0m Trial 36 finished with value: 0.10565175867177606 and parameters: {'observation_period_num': 73, 'train_rates': 0.9575009051901208, 'learning_rate': 0.0003577954887890086, 'batch_size': 53, 'step_size': 4, 'gamma': 0.9740357870953295}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:22:35,050][0m Trial 37 finished with value: 0.043401699513196945 and parameters: {'observation_period_num': 33, 'train_rates': 0.9877914607955973, 'learning_rate': 0.00012017597194453852, 'batch_size': 54, 'step_size': 1, 'gamma': 0.9514426308956055}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:28:01,265][0m Trial 38 finished with value: 0.0825192479691071 and parameters: {'observation_period_num': 65, 'train_rates': 0.9554839705975966, 'learning_rate': 0.00022336926136865622, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9156111820198358}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:30:29,811][0m Trial 39 finished with value: 0.05763070821052506 and parameters: {'observation_period_num': 54, 'train_rates': 0.9638481033887215, 'learning_rate': 0.0005516851399412096, 'batch_size': 38, 'step_size': 4, 'gamma': 0.9861201985784085}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:31:52,636][0m Trial 40 finished with value: 0.3990779391356877 and parameters: {'observation_period_num': 16, 'train_rates': 0.9190002868410657, 'learning_rate': 1.0251942683236169e-06, 'batch_size': 68, 'step_size': 8, 'gamma': 0.9479062029562708}. Best is trial 34 with value: 0.02745640277862549.[0m
[32m[I 2025-01-12 06:34:35,091][0m Trial 41 finished with value: 0.02172093093395233 and parameters: {'observation_period_num': 16, 'train_rates': 0.9894244644353496, 'learning_rate': 0.0006516498651645316, 'batch_size': 36, 'step_size': 7, 'gamma': 0.9682349442588372}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:38:02,994][0m Trial 42 finished with value: 0.027794588930331744 and parameters: {'observation_period_num': 26, 'train_rates': 0.9868476857935531, 'learning_rate': 0.00037620255996639434, 'batch_size': 28, 'step_size': 6, 'gamma': 0.9262498001353994}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:41:23,403][0m Trial 43 finished with value: 0.033357183821499346 and parameters: {'observation_period_num': 34, 'train_rates': 0.9863135636085764, 'learning_rate': 0.00042321434539820807, 'batch_size': 29, 'step_size': 6, 'gamma': 0.9295397832596394}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:44:21,352][0m Trial 44 finished with value: 0.02674632060031096 and parameters: {'observation_period_num': 28, 'train_rates': 0.9846379455743951, 'learning_rate': 0.0006084083261969585, 'batch_size': 33, 'step_size': 8, 'gamma': 0.9312958555940979}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:45:56,024][0m Trial 45 finished with value: 0.03323793783783913 and parameters: {'observation_period_num': 24, 'train_rates': 0.9890270378756078, 'learning_rate': 0.0006296030815163711, 'batch_size': 62, 'step_size': 9, 'gamma': 0.9217930826851954}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:48:11,130][0m Trial 46 finished with value: 0.2571927780235136 and parameters: {'observation_period_num': 163, 'train_rates': 0.9735332610789246, 'learning_rate': 4.274950864391653e-06, 'batch_size': 41, 'step_size': 8, 'gamma': 0.9553525155619624}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:51:13,368][0m Trial 47 finished with value: 0.05488629934007597 and parameters: {'observation_period_num': 44, 'train_rates': 0.911050113013269, 'learning_rate': 0.00020047119435962792, 'batch_size': 30, 'step_size': 8, 'gamma': 0.931741601358027}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:52:19,223][0m Trial 48 finished with value: 0.06470853490207125 and parameters: {'observation_period_num': 86, 'train_rates': 0.7628943064656181, 'learning_rate': 0.00011511446420462012, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9097876897519657}. Best is trial 41 with value: 0.02172093093395233.[0m
[32m[I 2025-01-12 06:53:52,923][0m Trial 49 finished with value: 0.08973943602805044 and parameters: {'observation_period_num': 60, 'train_rates': 0.9472842135312122, 'learning_rate': 0.0006099939307654185, 'batch_size': 60, 'step_size': 6, 'gamma': 0.9439096859975811}. Best is trial 41 with value: 0.02172093093395233.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-12 06:53:52,933][0m A new study created in memory with name: no-name-ad9d3e35-7bb1-44ca-8e26-8f0403918ee6[0m
[32m[I 2025-01-12 06:54:16,215][0m Trial 0 finished with value: 1.6913045448648731 and parameters: {'observation_period_num': 111, 'train_rates': 0.8097760690801812, 'learning_rate': 1.410869480144003e-06, 'batch_size': 235, 'step_size': 3, 'gamma': 0.7610803579490205}. Best is trial 0 with value: 1.6913045448648731.[0m
[32m[I 2025-01-12 06:54:44,753][0m Trial 1 finished with value: 0.6452521400941643 and parameters: {'observation_period_num': 225, 'train_rates': 0.8539646959602905, 'learning_rate': 2.3617718160794778e-06, 'batch_size': 179, 'step_size': 11, 'gamma': 0.9707222776962685}. Best is trial 1 with value: 0.6452521400941643.[0m
[32m[I 2025-01-12 06:55:49,885][0m Trial 2 finished with value: 0.5259817728960084 and parameters: {'observation_period_num': 208, 'train_rates': 0.8080017231158698, 'learning_rate': 2.455169053093607e-06, 'batch_size': 74, 'step_size': 15, 'gamma': 0.804653881494546}. Best is trial 2 with value: 0.5259817728960084.[0m
[32m[I 2025-01-12 06:56:28,404][0m Trial 3 finished with value: 0.04518908197865074 and parameters: {'observation_period_num': 73, 'train_rates': 0.8594290964395239, 'learning_rate': 0.00042565552081191485, 'batch_size': 140, 'step_size': 3, 'gamma': 0.9664212791331455}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 06:57:07,441][0m Trial 4 finished with value: 1.077232274076631 and parameters: {'observation_period_num': 165, 'train_rates': 0.7332161226417603, 'learning_rate': 1.2062214251606975e-06, 'batch_size': 123, 'step_size': 9, 'gamma': 0.7919130510482421}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 06:57:51,725][0m Trial 5 finished with value: 0.19069391461990534 and parameters: {'observation_period_num': 39, 'train_rates': 0.6595057719960484, 'learning_rate': 5.140750523142164e-06, 'batch_size': 106, 'step_size': 7, 'gamma': 0.9701330099883382}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 06:59:22,546][0m Trial 6 finished with value: 0.29191890452057123 and parameters: {'observation_period_num': 191, 'train_rates': 0.9191053250238785, 'learning_rate': 3.618052424390549e-06, 'batch_size': 58, 'step_size': 13, 'gamma': 0.981900400403856}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 07:00:19,401][0m Trial 7 finished with value: 0.05072753271088004 and parameters: {'observation_period_num': 48, 'train_rates': 0.7935629083460535, 'learning_rate': 0.0002842670774076587, 'batch_size': 87, 'step_size': 10, 'gamma': 0.9055649583783683}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 07:01:03,996][0m Trial 8 finished with value: 0.30099117263503694 and parameters: {'observation_period_num': 47, 'train_rates': 0.8418576194558817, 'learning_rate': 5.475599519981846e-06, 'batch_size': 118, 'step_size': 9, 'gamma': 0.8951654665340915}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 07:01:27,832][0m Trial 9 finished with value: 0.09095548249488314 and parameters: {'observation_period_num': 53, 'train_rates': 0.6841898461909593, 'learning_rate': 5.7382065455249335e-05, 'batch_size': 200, 'step_size': 7, 'gamma': 0.9695171745000373}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 07:04:27,345][0m Trial 10 finished with value: 0.08025069534778595 and parameters: {'observation_period_num': 113, 'train_rates': 0.938453010218324, 'learning_rate': 0.000829575727686215, 'batch_size': 30, 'step_size': 1, 'gamma': 0.8612489480126531}. Best is trial 3 with value: 0.04518908197865074.[0m
[32m[I 2025-01-12 07:05:00,773][0m Trial 11 finished with value: 0.03313120432621491 and parameters: {'observation_period_num': 5, 'train_rates': 0.7494714121306028, 'learning_rate': 0.0005111967366090172, 'batch_size': 155, 'step_size': 4, 'gamma': 0.9157091664201313}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:05:30,982][0m Trial 12 finished with value: 0.045987353357260376 and parameters: {'observation_period_num': 6, 'train_rates': 0.72029214041785, 'learning_rate': 0.00014092121875620923, 'batch_size': 175, 'step_size': 4, 'gamma': 0.9312173493716148}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:06:07,440][0m Trial 13 finished with value: 0.05788760970332729 and parameters: {'observation_period_num': 85, 'train_rates': 0.8832998381616549, 'learning_rate': 0.0008078521302918283, 'batch_size': 158, 'step_size': 5, 'gamma': 0.924126421543225}. Best is trial 11 with value: 0.03313120432621491.[0m
Early stopping at epoch 85
[32m[I 2025-01-12 07:06:33,362][0m Trial 14 finished with value: 0.08275981992483139 and parameters: {'observation_period_num': 6, 'train_rates': 0.9889362420605738, 'learning_rate': 0.0002366574566968429, 'batch_size': 216, 'step_size': 1, 'gamma': 0.8606299857184668}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:07:04,514][0m Trial 15 finished with value: 0.23573440204769752 and parameters: {'observation_period_num': 83, 'train_rates': 0.6174948552557159, 'learning_rate': 1.996221720516004e-05, 'batch_size': 146, 'step_size': 5, 'gamma': 0.9373800272998668}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:07:25,829][0m Trial 16 finished with value: 0.226058594837874 and parameters: {'observation_period_num': 151, 'train_rates': 0.7616508875598224, 'learning_rate': 6.162241141299138e-05, 'batch_size': 256, 'step_size': 3, 'gamma': 0.8893058936021078}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:07:58,949][0m Trial 17 finished with value: 0.06909399306320625 and parameters: {'observation_period_num': 85, 'train_rates': 0.7595654772377188, 'learning_rate': 0.00031161478792237404, 'batch_size': 147, 'step_size': 6, 'gamma': 0.8324524973359522}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:08:30,895][0m Trial 18 finished with value: 0.15919862663358209 and parameters: {'observation_period_num': 17, 'train_rates': 0.9162777269967174, 'learning_rate': 1.7515163904792717e-05, 'batch_size': 185, 'step_size': 2, 'gamma': 0.9469010999125296}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:09:28,609][0m Trial 19 finished with value: 0.04780729592092476 and parameters: {'observation_period_num': 71, 'train_rates': 0.8530957567194329, 'learning_rate': 0.00011534949067884929, 'batch_size': 93, 'step_size': 4, 'gamma': 0.95241231933339}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:10:03,398][0m Trial 20 finished with value: 0.16172633083855234 and parameters: {'observation_period_num': 135, 'train_rates': 0.6945069927053248, 'learning_rate': 0.00041372145321498543, 'batch_size': 132, 'step_size': 3, 'gamma': 0.9892663573226427}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:10:33,109][0m Trial 21 finished with value: 0.052525573410093784 and parameters: {'observation_period_num': 22, 'train_rates': 0.7269414074046723, 'learning_rate': 0.0001154321379826999, 'batch_size': 172, 'step_size': 5, 'gamma': 0.9231796243567411}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:10:57,444][0m Trial 22 finished with value: 0.09364545875049513 and parameters: {'observation_period_num': 6, 'train_rates': 0.630163412439126, 'learning_rate': 0.00016727894359823046, 'batch_size': 205, 'step_size': 4, 'gamma': 0.9176239235498218}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:11:26,948][0m Trial 23 finished with value: 0.04341098620256572 and parameters: {'observation_period_num': 28, 'train_rates': 0.7227736212667453, 'learning_rate': 0.0006511160273064209, 'batch_size': 163, 'step_size': 7, 'gamma': 0.8817780872652997}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:11:58,558][0m Trial 24 finished with value: 0.04074420106505787 and parameters: {'observation_period_num': 35, 'train_rates': 0.752337622138844, 'learning_rate': 0.0005275717389289419, 'batch_size': 158, 'step_size': 7, 'gamma': 0.8819107267595501}. Best is trial 11 with value: 0.03313120432621491.[0m
[32m[I 2025-01-12 07:12:30,229][0m Trial 25 finished with value: 0.03205027313608872 and parameters: {'observation_period_num': 24, 'train_rates': 0.7663919079082934, 'learning_rate': 0.000600039743821849, 'batch_size': 162, 'step_size': 8, 'gamma': 0.8755777196328941}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:12:57,733][0m Trial 26 finished with value: 0.045211104316364194 and parameters: {'observation_period_num': 60, 'train_rates': 0.778234262922832, 'learning_rate': 0.0005865180341771018, 'batch_size': 191, 'step_size': 8, 'gamma': 0.8427452062312195}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:13:19,657][0m Trial 27 finished with value: 0.04399149045348168 and parameters: {'observation_period_num': 33, 'train_rates': 0.6830875954247849, 'learning_rate': 0.0009242772470969259, 'batch_size': 222, 'step_size': 11, 'gamma': 0.874516120885791}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:13:53,167][0m Trial 28 finished with value: 0.14138857814772376 and parameters: {'observation_period_num': 107, 'train_rates': 0.7555611453292664, 'learning_rate': 4.5498582165191336e-05, 'batch_size': 152, 'step_size': 8, 'gamma': 0.84581647176428}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:14:15,893][0m Trial 29 finished with value: 0.09620277888950754 and parameters: {'observation_period_num': 102, 'train_rates': 0.828717545713684, 'learning_rate': 0.00021275610676871923, 'batch_size': 247, 'step_size': 6, 'gamma': 0.7507946231672462}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:14:57,682][0m Trial 30 finished with value: 0.04582595576297993 and parameters: {'observation_period_num': 28, 'train_rates': 0.64902609742327, 'learning_rate': 0.000457026357564298, 'batch_size': 111, 'step_size': 12, 'gamma': 0.8177742252643883}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:15:27,993][0m Trial 31 finished with value: 0.040805967397549575 and parameters: {'observation_period_num': 27, 'train_rates': 0.7095365862646273, 'learning_rate': 0.0005884658915122573, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8792165011831476}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:15:59,234][0m Trial 32 finished with value: 0.045413833034162326 and parameters: {'observation_period_num': 58, 'train_rates': 0.7837652189801436, 'learning_rate': 0.0004780421010879012, 'batch_size': 167, 'step_size': 9, 'gamma': 0.9074287000066906}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:16:36,033][0m Trial 33 finished with value: 0.044316542972983035 and parameters: {'observation_period_num': 36, 'train_rates': 0.7038334622578994, 'learning_rate': 0.00030501810299910295, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8681419469522418}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:17:03,155][0m Trial 34 finished with value: 0.15895790160299542 and parameters: {'observation_period_num': 251, 'train_rates': 0.8152201174045561, 'learning_rate': 0.0008705999066461924, 'batch_size': 185, 'step_size': 6, 'gamma': 0.9039109483359176}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:17:41,422][0m Trial 35 finished with value: 0.04437698153204637 and parameters: {'observation_period_num': 15, 'train_rates': 0.7461255987152272, 'learning_rate': 7.903199225085294e-05, 'batch_size': 132, 'step_size': 15, 'gamma': 0.8790174884235316}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:18:12,005][0m Trial 36 finished with value: 0.07259263810673217 and parameters: {'observation_period_num': 68, 'train_rates': 0.6594706064395397, 'learning_rate': 0.0003528786269567944, 'batch_size': 158, 'step_size': 10, 'gamma': 0.8525234923562964}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:18:40,726][0m Trial 37 finished with value: 0.03981478571788304 and parameters: {'observation_period_num': 42, 'train_rates': 0.8019983932396267, 'learning_rate': 0.0006215985992438785, 'batch_size': 194, 'step_size': 8, 'gamma': 0.7781745462928324}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:19:05,719][0m Trial 38 finished with value: 0.04968633373667087 and parameters: {'observation_period_num': 44, 'train_rates': 0.8074227632226083, 'learning_rate': 0.00021484611215930437, 'batch_size': 221, 'step_size': 8, 'gamma': 0.7900272691236105}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:19:31,051][0m Trial 39 finished with value: 0.31474145015527966 and parameters: {'observation_period_num': 177, 'train_rates': 0.7760835290052781, 'learning_rate': 2.4288872295633374e-05, 'batch_size': 199, 'step_size': 13, 'gamma': 0.7766040695007418}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:19:54,403][0m Trial 40 finished with value: 0.38025844511058593 and parameters: {'observation_period_num': 41, 'train_rates': 0.7403004377039762, 'learning_rate': 7.894436483075516e-06, 'batch_size': 231, 'step_size': 7, 'gamma': 0.8225022696364069}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:20:25,741][0m Trial 41 finished with value: 0.03467425247353892 and parameters: {'observation_period_num': 21, 'train_rates': 0.7990640375113826, 'learning_rate': 0.0005806204055438993, 'batch_size': 179, 'step_size': 9, 'gamma': 0.8969821957354057}. Best is trial 25 with value: 0.03205027313608872.[0m
[32m[I 2025-01-12 07:20:56,852][0m Trial 42 finished with value: 0.02851494324196429 and parameters: {'observation_period_num': 17, 'train_rates': 0.8133241920636358, 'learning_rate': 0.0006291957902043911, 'batch_size': 177, 'step_size': 11, 'gamma': 0.8984477970285962}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:21:23,836][0m Trial 43 finished with value: 0.029079597469715945 and parameters: {'observation_period_num': 14, 'train_rates': 0.8078604476564456, 'learning_rate': 0.0009757459326456769, 'batch_size': 206, 'step_size': 11, 'gamma': 0.8998903893951575}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:21:52,201][0m Trial 44 finished with value: 0.032215735623431406 and parameters: {'observation_period_num': 12, 'train_rates': 0.8779150179695339, 'learning_rate': 0.00038603322960264466, 'batch_size': 207, 'step_size': 12, 'gamma': 0.8982839950610072}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:22:20,322][0m Trial 45 finished with value: 0.03316960815201573 and parameters: {'observation_period_num': 5, 'train_rates': 0.8723868640579168, 'learning_rate': 0.000988387325028615, 'batch_size': 211, 'step_size': 14, 'gamma': 0.9074443487242233}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:22:45,392][0m Trial 46 finished with value: 0.03279350695381303 and parameters: {'observation_period_num': 16, 'train_rates': 0.8300427478836554, 'learning_rate': 0.00033520050737466487, 'batch_size': 232, 'step_size': 12, 'gamma': 0.9161580427123609}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:23:11,800][0m Trial 47 finished with value: 0.03656833468232783 and parameters: {'observation_period_num': 14, 'train_rates': 0.8916030830603058, 'learning_rate': 0.00036139296634501343, 'batch_size': 233, 'step_size': 12, 'gamma': 0.9539131469239979}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:23:34,290][0m Trial 48 finished with value: 0.0921778447115201 and parameters: {'observation_period_num': 52, 'train_rates': 0.8321081549570891, 'learning_rate': 0.0002837368630247237, 'batch_size': 245, 'step_size': 11, 'gamma': 0.8946070074022883}. Best is trial 42 with value: 0.02851494324196429.[0m
[32m[I 2025-01-12 07:24:01,226][0m Trial 49 finished with value: 0.4364615562851423 and parameters: {'observation_period_num': 19, 'train_rates': 0.8510203595699327, 'learning_rate': 2.151942336445009e-06, 'batch_size': 212, 'step_size': 12, 'gamma': 0.9275373314188634}. Best is trial 42 with value: 0.02851494324196429.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-12 07:24:01,236][0m A new study created in memory with name: no-name-75bb9ae8-3022-445e-a4a4-cf4cf48d585d[0m
[32m[I 2025-01-12 07:24:40,144][0m Trial 0 finished with value: 0.05295921862125397 and parameters: {'observation_period_num': 66, 'train_rates': 0.9769008016429097, 'learning_rate': 0.0004244759750124948, 'batch_size': 152, 'step_size': 11, 'gamma': 0.8859956345934724}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:25:01,544][0m Trial 1 finished with value: 0.5041773278910414 and parameters: {'observation_period_num': 32, 'train_rates': 0.6811045023773644, 'learning_rate': 4.726357869049501e-06, 'batch_size': 238, 'step_size': 5, 'gamma': 0.8258237686903627}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:25:49,700][0m Trial 2 finished with value: 0.3296475540189182 and parameters: {'observation_period_num': 113, 'train_rates': 0.940274045721291, 'learning_rate': 8.67110421073834e-06, 'batch_size': 116, 'step_size': 15, 'gamma': 0.8518296414258673}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:26:12,654][0m Trial 3 finished with value: 0.2040261707765639 and parameters: {'observation_period_num': 210, 'train_rates': 0.7562344765094507, 'learning_rate': 0.0007744596970781828, 'batch_size': 224, 'step_size': 8, 'gamma': 0.9509558534455638}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:28:58,298][0m Trial 4 finished with value: 0.3958151555723614 and parameters: {'observation_period_num': 204, 'train_rates': 0.9804332019494422, 'learning_rate': 7.937394733043966e-06, 'batch_size': 33, 'step_size': 2, 'gamma': 0.8337628573076631}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:29:20,748][0m Trial 5 finished with value: 0.7910053131078115 and parameters: {'observation_period_num': 188, 'train_rates': 0.8507721519904301, 'learning_rate': 1.1241820385168249e-05, 'batch_size': 256, 'step_size': 2, 'gamma': 0.8612619018308194}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:30:55,642][0m Trial 6 finished with value: 0.09267333289232826 and parameters: {'observation_period_num': 69, 'train_rates': 0.6559080500543369, 'learning_rate': 0.00023589889177506483, 'batch_size': 45, 'step_size': 8, 'gamma': 0.8652671625767039}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:31:37,829][0m Trial 7 finished with value: 0.2751329641282614 and parameters: {'observation_period_num': 60, 'train_rates': 0.6262861402701263, 'learning_rate': 1.893096758366303e-05, 'batch_size': 106, 'step_size': 4, 'gamma': 0.8158992996695451}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:32:08,751][0m Trial 8 finished with value: 0.15624438378686611 and parameters: {'observation_period_num': 163, 'train_rates': 0.8143225618132592, 'learning_rate': 6.496591550432606e-05, 'batch_size': 174, 'step_size': 7, 'gamma': 0.8892509507648827}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:32:42,402][0m Trial 9 finished with value: 0.3972108883700246 and parameters: {'observation_period_num': 31, 'train_rates': 0.777312616652835, 'learning_rate': 1.1100223206725127e-05, 'batch_size': 157, 'step_size': 4, 'gamma': 0.8224346643923021}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:33:53,475][0m Trial 10 finished with value: 0.6798916629382542 and parameters: {'observation_period_num': 122, 'train_rates': 0.9061870652594086, 'learning_rate': 1.4851325714217151e-06, 'batch_size': 76, 'step_size': 13, 'gamma': 0.770505558269472}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:35:36,391][0m Trial 11 finished with value: 0.13465284295020763 and parameters: {'observation_period_num': 81, 'train_rates': 0.6751109170874833, 'learning_rate': 0.00037237025393156286, 'batch_size': 43, 'step_size': 11, 'gamma': 0.9142140610912722}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:36:00,971][0m Trial 12 finished with value: 0.3181038747361534 and parameters: {'observation_period_num': 80, 'train_rates': 0.6019430299854325, 'learning_rate': 0.00013155445229805088, 'batch_size': 179, 'step_size': 10, 'gamma': 0.9234541182741204}. Best is trial 0 with value: 0.05295921862125397.[0m
[32m[I 2025-01-12 07:37:01,412][0m Trial 13 finished with value: 0.03211844126576359 and parameters: {'observation_period_num': 13, 'train_rates': 0.7197017329534888, 'learning_rate': 0.00024035281457303125, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9712553048575054}. Best is trial 13 with value: 0.03211844126576359.[0m
[32m[I 2025-01-12 07:38:01,455][0m Trial 14 finished with value: 0.02798066339616118 and parameters: {'observation_period_num': 7, 'train_rates': 0.7343581051394905, 'learning_rate': 0.0009158285635455825, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9893934073027534}. Best is trial 14 with value: 0.02798066339616118.[0m
[32m[I 2025-01-12 07:39:05,264][0m Trial 15 finished with value: 0.16092134509387054 and parameters: {'observation_period_num': 251, 'train_rates': 0.726856595731517, 'learning_rate': 7.80008424317296e-05, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9846462977534377}. Best is trial 14 with value: 0.02798066339616118.[0m
[32m[I 2025-01-12 07:39:58,941][0m Trial 16 finished with value: 0.02753877740806979 and parameters: {'observation_period_num': 12, 'train_rates': 0.723514198890371, 'learning_rate': 0.000516110776801153, 'batch_size': 92, 'step_size': 13, 'gamma': 0.9880452407914841}. Best is trial 16 with value: 0.02753877740806979.[0m
[32m[I 2025-01-12 07:40:50,199][0m Trial 17 finished with value: 0.022930277398471803 and parameters: {'observation_period_num': 7, 'train_rates': 0.837859298121245, 'learning_rate': 0.0007622493770942043, 'batch_size': 114, 'step_size': 15, 'gamma': 0.9529719798308209}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:41:39,830][0m Trial 18 finished with value: 0.0499147352109961 and parameters: {'observation_period_num': 37, 'train_rates': 0.8488051506421813, 'learning_rate': 8.209568512902355e-05, 'batch_size': 120, 'step_size': 15, 'gamma': 0.9483943092318826}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:42:23,628][0m Trial 19 finished with value: 0.08326739898873764 and parameters: {'observation_period_num': 101, 'train_rates': 0.8195314424980181, 'learning_rate': 3.540543600778927e-05, 'batch_size': 136, 'step_size': 13, 'gamma': 0.9510087126171366}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:42:54,941][0m Trial 20 finished with value: 0.0882430151104927 and parameters: {'observation_period_num': 143, 'train_rates': 0.8976886760031599, 'learning_rate': 0.0004396477047526107, 'batch_size': 200, 'step_size': 14, 'gamma': 0.9271531213650198}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:43:54,324][0m Trial 21 finished with value: 0.0322079205435604 and parameters: {'observation_period_num': 6, 'train_rates': 0.7288492786997046, 'learning_rate': 0.00094692806325184, 'batch_size': 88, 'step_size': 12, 'gamma': 0.9758139946272696}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:48:40,336][0m Trial 22 finished with value: 0.027664249058790415 and parameters: {'observation_period_num': 7, 'train_rates': 0.7745147509845592, 'learning_rate': 0.0009887169209802883, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9871136491386133}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:51:53,130][0m Trial 23 finished with value: 0.07654621176156375 and parameters: {'observation_period_num': 46, 'train_rates': 0.7778564859788181, 'learning_rate': 0.0001757236131789556, 'batch_size': 25, 'step_size': 15, 'gamma': 0.9568550859211888}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:53:27,828][0m Trial 24 finished with value: 0.043843193379152254 and parameters: {'observation_period_num': 22, 'train_rates': 0.8585272096490905, 'learning_rate': 0.0005235604504588027, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9664636245646451}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:58:16,948][0m Trial 25 finished with value: 0.0506443893118917 and parameters: {'observation_period_num': 50, 'train_rates': 0.808281198912219, 'learning_rate': 0.0005794991364938761, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9267500774002844}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:59:04,865][0m Trial 26 finished with value: 0.07364880953865845 and parameters: {'observation_period_num': 93, 'train_rates': 0.6912275389026886, 'learning_rate': 0.0002855002115929895, 'batch_size': 97, 'step_size': 15, 'gamma': 0.9007672155915495}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 07:59:45,198][0m Trial 27 finished with value: 0.045526785026123 and parameters: {'observation_period_num': 29, 'train_rates': 0.7630005680803889, 'learning_rate': 0.00012692240802260552, 'batch_size': 129, 'step_size': 12, 'gamma': 0.9893564627068072}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:01:19,422][0m Trial 28 finished with value: 0.026307064635238672 and parameters: {'observation_period_num': 5, 'train_rates': 0.8808673417079189, 'learning_rate': 0.0006433990774412168, 'batch_size': 58, 'step_size': 13, 'gamma': 0.939669762886558}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:01:58,572][0m Trial 29 finished with value: 0.059022852310947345 and parameters: {'observation_period_num': 52, 'train_rates': 0.8945677763406752, 'learning_rate': 4.066538156284326e-05, 'batch_size': 146, 'step_size': 9, 'gamma': 0.9403451044327418}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:03:26,612][0m Trial 30 finished with value: 0.11989682018756867 and parameters: {'observation_period_num': 143, 'train_rates': 0.9451141603547699, 'learning_rate': 0.00037294939803936115, 'batch_size': 62, 'step_size': 12, 'gamma': 0.9044756911957035}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:05:04,593][0m Trial 31 finished with value: 0.031879020078728594 and parameters: {'observation_period_num': 17, 'train_rates': 0.8774501709484164, 'learning_rate': 0.0006513917303451684, 'batch_size': 55, 'step_size': 14, 'gamma': 0.96766030314667}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:05:58,261][0m Trial 32 finished with value: 0.03885911844963254 and parameters: {'observation_period_num': 38, 'train_rates': 0.8340370382798493, 'learning_rate': 0.0009890797507469313, 'batch_size': 101, 'step_size': 13, 'gamma': 0.9657194164349083}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:08:38,745][0m Trial 33 finished with value: 0.03001125617690074 and parameters: {'observation_period_num': 5, 'train_rates': 0.9348325981804219, 'learning_rate': 0.0004685171938238925, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9773157810465909}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:13:19,533][0m Trial 34 finished with value: 0.037169929552887906 and parameters: {'observation_period_num': 22, 'train_rates': 0.7049288877783993, 'learning_rate': 0.000617895936206866, 'batch_size': 16, 'step_size': 14, 'gamma': 0.934394525623039}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:14:05,773][0m Trial 35 finished with value: 0.04315967898638476 and parameters: {'observation_period_num': 24, 'train_rates': 0.7850706645324771, 'learning_rate': 0.0002806562935456302, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9551145001382289}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:14:58,776][0m Trial 36 finished with value: 0.05590411393744189 and parameters: {'observation_period_num': 65, 'train_rates': 0.744367555262117, 'learning_rate': 0.00013850806912852508, 'batch_size': 90, 'step_size': 12, 'gamma': 0.9398932364671726}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:16:49,260][0m Trial 37 finished with value: 0.04981143515304488 and parameters: {'observation_period_num': 35, 'train_rates': 0.7980106123849993, 'learning_rate': 0.0007369237600569028, 'batch_size': 45, 'step_size': 15, 'gamma': 0.7967813636964549}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:17:34,194][0m Trial 38 finished with value: 0.8290454138266413 and parameters: {'observation_period_num': 45, 'train_rates': 0.8693572790365691, 'learning_rate': 1.2677572676437934e-06, 'batch_size': 121, 'step_size': 13, 'gamma': 0.8786886862329284}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:18:49,161][0m Trial 39 finished with value: 0.02306305200268186 and parameters: {'observation_period_num': 5, 'train_rates': 0.8259362661034818, 'learning_rate': 0.0001830491631293842, 'batch_size': 70, 'step_size': 7, 'gamma': 0.9617722844013351}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:20:05,541][0m Trial 40 finished with value: 0.045025725239063 and parameters: {'observation_period_num': 60, 'train_rates': 0.8336590511586374, 'learning_rate': 0.0002017821157189768, 'batch_size': 68, 'step_size': 6, 'gamma': 0.9118391962569669}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:22:29,625][0m Trial 41 finished with value: 0.0342800363432616 and parameters: {'observation_period_num': 15, 'train_rates': 0.7602679092323246, 'learning_rate': 0.0003216399842367078, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9596641420746079}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:24:07,596][0m Trial 42 finished with value: 0.04339226660429848 and parameters: {'observation_period_num': 21, 'train_rates': 0.8256668530869724, 'learning_rate': 0.0006987556474490568, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9819975973489306}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:25:12,227][0m Trial 43 finished with value: 0.02679800982117106 and parameters: {'observation_period_num': 5, 'train_rates': 0.9261837004339823, 'learning_rate': 0.00039030776623197224, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9461470368648456}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:26:13,165][0m Trial 44 finished with value: 0.03535655565874677 and parameters: {'observation_period_num': 30, 'train_rates': 0.9272914793524322, 'learning_rate': 0.0004040020463830758, 'batch_size': 96, 'step_size': 5, 'gamma': 0.8487136752019194}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:27:27,230][0m Trial 45 finished with value: 0.4393277983916433 and parameters: {'observation_period_num': 78, 'train_rates': 0.9537666518342636, 'learning_rate': 2.70985407140812e-06, 'batch_size': 77, 'step_size': 7, 'gamma': 0.9437275959331228}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:28:19,836][0m Trial 46 finished with value: 0.06360592536774336 and parameters: {'observation_period_num': 5, 'train_rates': 0.913652854264661, 'learning_rate': 2.16003214447801e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8947997725846955}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:29:38,390][0m Trial 47 finished with value: 0.12026825854006935 and parameters: {'observation_period_num': 249, 'train_rates': 0.968695705831226, 'learning_rate': 0.00020144391574339912, 'batch_size': 70, 'step_size': 6, 'gamma': 0.9146943992936509}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:30:29,627][0m Trial 48 finished with value: 0.20313943233999293 and parameters: {'observation_period_num': 183, 'train_rates': 0.6528067851491153, 'learning_rate': 0.0001023352276784545, 'batch_size': 85, 'step_size': 8, 'gamma': 0.7554175008926169}. Best is trial 17 with value: 0.022930277398471803.[0m
[32m[I 2025-01-12 08:31:12,376][0m Trial 49 finished with value: 0.05966349498314016 and parameters: {'observation_period_num': 40, 'train_rates': 0.8834480218481084, 'learning_rate': 5.149904236340993e-05, 'batch_size': 132, 'step_size': 4, 'gamma': 0.9300318498388432}. Best is trial 17 with value: 0.022930277398471803.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-12 08:31:12,386][0m A new study created in memory with name: no-name-fdf21a4a-4209-4893-a811-1eef61713230[0m
[32m[I 2025-01-12 08:31:47,518][0m Trial 0 finished with value: 0.4175869504026338 and parameters: {'observation_period_num': 48, 'train_rates': 0.8243828252875501, 'learning_rate': 3.632210053767292e-06, 'batch_size': 155, 'step_size': 11, 'gamma': 0.8352635997272316}. Best is trial 0 with value: 0.4175869504026338.[0m
[32m[I 2025-01-12 08:32:23,939][0m Trial 1 finished with value: 1.082426388203884 and parameters: {'observation_period_num': 105, 'train_rates': 0.6080606961520576, 'learning_rate': 2.4125574415309864e-06, 'batch_size': 122, 'step_size': 4, 'gamma': 0.7736883629507688}. Best is trial 0 with value: 0.4175869504026338.[0m
[32m[I 2025-01-12 08:32:49,227][0m Trial 2 finished with value: 1.0118653118841707 and parameters: {'observation_period_num': 166, 'train_rates': 0.7227042088365936, 'learning_rate': 1.523324216025644e-06, 'batch_size': 196, 'step_size': 1, 'gamma': 0.9164501664356997}. Best is trial 0 with value: 0.4175869504026338.[0m
[32m[I 2025-01-12 08:33:26,002][0m Trial 3 finished with value: 0.30187790277848014 and parameters: {'observation_period_num': 180, 'train_rates': 0.713494647667914, 'learning_rate': 2.145700604576187e-05, 'batch_size': 129, 'step_size': 10, 'gamma': 0.8471044116832659}. Best is trial 3 with value: 0.30187790277848014.[0m
[32m[I 2025-01-12 08:34:02,338][0m Trial 4 finished with value: 0.15406982881955833 and parameters: {'observation_period_num': 77, 'train_rates': 0.9135504827677274, 'learning_rate': 2.7170547912788124e-05, 'batch_size': 157, 'step_size': 10, 'gamma': 0.7729208939727813}. Best is trial 4 with value: 0.15406982881955833.[0m
[32m[I 2025-01-12 08:36:07,341][0m Trial 5 finished with value: 0.12466578463522288 and parameters: {'observation_period_num': 196, 'train_rates': 0.8113347511636224, 'learning_rate': 2.1740444350838512e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8659191676472217}. Best is trial 5 with value: 0.12466578463522288.[0m
[32m[I 2025-01-12 08:36:31,776][0m Trial 6 finished with value: 0.8023179764227479 and parameters: {'observation_period_num': 171, 'train_rates': 0.6554471819662256, 'learning_rate': 8.26024482866136e-06, 'batch_size': 191, 'step_size': 2, 'gamma': 0.8060323452042001}. Best is trial 5 with value: 0.12466578463522288.[0m
[32m[I 2025-01-12 08:37:54,589][0m Trial 7 finished with value: 0.03475076248271203 and parameters: {'observation_period_num': 38, 'train_rates': 0.8132104384746832, 'learning_rate': 0.0002631335660292056, 'batch_size': 61, 'step_size': 7, 'gamma': 0.8863894615361527}. Best is trial 7 with value: 0.03475076248271203.[0m
[32m[I 2025-01-12 08:38:28,916][0m Trial 8 finished with value: 0.13208470132374606 and parameters: {'observation_period_num': 225, 'train_rates': 0.7231327911306362, 'learning_rate': 0.000129096583732662, 'batch_size': 133, 'step_size': 12, 'gamma': 0.8711376189843296}. Best is trial 7 with value: 0.03475076248271203.[0m
[32m[I 2025-01-12 08:38:48,228][0m Trial 9 finished with value: 0.3436479986753143 and parameters: {'observation_period_num': 225, 'train_rates': 0.7056956121121557, 'learning_rate': 5.274077934391977e-05, 'batch_size': 251, 'step_size': 6, 'gamma': 0.7899737983545725}. Best is trial 7 with value: 0.03475076248271203.[0m
[32m[I 2025-01-12 08:42:44,469][0m Trial 10 finished with value: 0.04247606938352456 and parameters: {'observation_period_num': 5, 'train_rates': 0.9499354288172918, 'learning_rate': 0.0005873071637950049, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9856137906496092}. Best is trial 7 with value: 0.03475076248271203.[0m
[32m[I 2025-01-12 08:46:44,720][0m Trial 11 finished with value: 0.019640813242982735 and parameters: {'observation_period_num': 21, 'train_rates': 0.9776224556713436, 'learning_rate': 0.000920163818342003, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9830667902181534}. Best is trial 11 with value: 0.019640813242982735.[0m
[32m[I 2025-01-12 08:48:06,285][0m Trial 12 finished with value: 0.023388841640511977 and parameters: {'observation_period_num': 7, 'train_rates': 0.880908879769266, 'learning_rate': 0.0007265296578914418, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9802961101332436}. Best is trial 11 with value: 0.019640813242982735.[0m
[32m[I 2025-01-12 08:49:25,690][0m Trial 13 finished with value: 0.014520448632538319 and parameters: {'observation_period_num': 6, 'train_rates': 0.9880329351848851, 'learning_rate': 0.0009523244689686775, 'batch_size': 75, 'step_size': 15, 'gamma': 0.9787461474248159}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 08:50:31,637][0m Trial 14 finished with value: 0.1256227195262909 and parameters: {'observation_period_num': 123, 'train_rates': 0.9876882754698503, 'learning_rate': 0.0009573283951854533, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9408430621241757}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 08:56:27,006][0m Trial 15 finished with value: 0.0677524883300066 and parameters: {'observation_period_num': 68, 'train_rates': 0.9881887902058152, 'learning_rate': 0.00020582958773790913, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9504498598485172}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 08:57:30,120][0m Trial 16 finished with value: 0.059719089916092 and parameters: {'observation_period_num': 34, 'train_rates': 0.8846890629456708, 'learning_rate': 0.0003310430368885038, 'batch_size': 88, 'step_size': 9, 'gamma': 0.9516699338560847}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 08:59:15,978][0m Trial 17 finished with value: 0.0888054319769864 and parameters: {'observation_period_num': 91, 'train_rates': 0.9277460434870516, 'learning_rate': 0.0001060456389717918, 'batch_size': 52, 'step_size': 13, 'gamma': 0.9132015669482387}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:00:23,588][0m Trial 18 finished with value: 0.07151020542154171 and parameters: {'observation_period_num': 134, 'train_rates': 0.8560223344888399, 'learning_rate': 7.308446696127079e-05, 'batch_size': 76, 'step_size': 15, 'gamma': 0.9889490112905425}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:01:27,580][0m Trial 19 finished with value: 0.05211811523202439 and parameters: {'observation_period_num': 28, 'train_rates': 0.9517106328252329, 'learning_rate': 0.00044136861383538, 'batch_size': 92, 'step_size': 12, 'gamma': 0.9211876796088485}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:02:15,913][0m Trial 20 finished with value: 0.08140738778874233 and parameters: {'observation_period_num': 64, 'train_rates': 0.770877061310803, 'learning_rate': 0.00021488848803061174, 'batch_size': 105, 'step_size': 8, 'gamma': 0.9616584553981351}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:03:59,532][0m Trial 21 finished with value: 0.02648018104465384 and parameters: {'observation_period_num': 7, 'train_rates': 0.8903982343116806, 'learning_rate': 0.0009529918946527571, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9721001922623864}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:06:25,848][0m Trial 22 finished with value: 0.03709338898671434 and parameters: {'observation_period_num': 5, 'train_rates': 0.9524754413659163, 'learning_rate': 0.0005896575147121938, 'batch_size': 39, 'step_size': 13, 'gamma': 0.935311982862016}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:07:41,302][0m Trial 23 finished with value: 0.0347840943035077 and parameters: {'observation_period_num': 25, 'train_rates': 0.8799034256200765, 'learning_rate': 0.0008534186801726661, 'batch_size': 72, 'step_size': 14, 'gamma': 0.9722376576527118}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:09:50,748][0m Trial 24 finished with value: 0.06008076065942406 and parameters: {'observation_period_num': 53, 'train_rates': 0.8492820060399803, 'learning_rate': 0.0004469511959787482, 'batch_size': 40, 'step_size': 14, 'gamma': 0.8962691936071137}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:10:48,213][0m Trial 25 finished with value: 0.023558061569929123 and parameters: {'observation_period_num': 22, 'train_rates': 0.9885109656153673, 'learning_rate': 0.00018924618814434343, 'batch_size': 107, 'step_size': 12, 'gamma': 0.9890441548153611}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:12:07,335][0m Trial 26 finished with value: 0.10239150899675395 and parameters: {'observation_period_num': 87, 'train_rates': 0.9221976202595819, 'learning_rate': 1.0736241733290484e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.9613776794915783}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:17:07,115][0m Trial 27 finished with value: 0.06170410213128823 and parameters: {'observation_period_num': 49, 'train_rates': 0.7739837280794599, 'learning_rate': 0.0005514869855342965, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9312123038166441}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:19:33,483][0m Trial 28 finished with value: 0.04155383118223853 and parameters: {'observation_period_num': 21, 'train_rates': 0.9582138246597235, 'learning_rate': 0.0003222767442174927, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9751693022514542}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:20:13,723][0m Trial 29 finished with value: 0.050535610043688825 and parameters: {'observation_period_num': 54, 'train_rates': 0.9148968653001214, 'learning_rate': 0.0001350340544207806, 'batch_size': 147, 'step_size': 11, 'gamma': 0.8315217967163951}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:21:02,923][0m Trial 30 finished with value: 0.11258065286014292 and parameters: {'observation_period_num': 143, 'train_rates': 0.8518263772662251, 'learning_rate': 5.367151271325465e-05, 'batch_size': 107, 'step_size': 13, 'gamma': 0.9595784238397904}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:22:03,029][0m Trial 31 finished with value: 0.020150335505604744 and parameters: {'observation_period_num': 18, 'train_rates': 0.9873845774418332, 'learning_rate': 0.0007118273532002153, 'batch_size': 101, 'step_size': 12, 'gamma': 0.9883527175294904}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:23:15,184][0m Trial 32 finished with value: 0.04445564067539047 and parameters: {'observation_period_num': 39, 'train_rates': 0.9649965034188684, 'learning_rate': 0.000698625138029293, 'batch_size': 81, 'step_size': 15, 'gamma': 0.7500899691630544}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:24:54,443][0m Trial 33 finished with value: 0.030397722037399516 and parameters: {'observation_period_num': 17, 'train_rates': 0.9712769142574693, 'learning_rate': 0.0003896895620404951, 'batch_size': 59, 'step_size': 14, 'gamma': 0.97563176179005}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:25:44,952][0m Trial 34 finished with value: 0.055952033144421875 and parameters: {'observation_period_num': 42, 'train_rates': 0.9342301379593689, 'learning_rate': 0.000978376393440005, 'batch_size': 117, 'step_size': 12, 'gamma': 0.9564716307827981}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:26:46,172][0m Trial 35 finished with value: 0.24213287203045158 and parameters: {'observation_period_num': 18, 'train_rates': 0.9026608003333618, 'learning_rate': 2.0611384798849305e-06, 'batch_size': 94, 'step_size': 13, 'gamma': 0.9427287270190119}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:27:15,511][0m Trial 36 finished with value: 0.09456397366391461 and parameters: {'observation_period_num': 59, 'train_rates': 0.602754087906694, 'learning_rate': 0.0006486521095754696, 'batch_size': 147, 'step_size': 10, 'gamma': 0.9737848275683864}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:28:01,931][0m Trial 37 finished with value: 0.2146760048293277 and parameters: {'observation_period_num': 116, 'train_rates': 0.9364022429139551, 'learning_rate': 7.6291547354580224e-06, 'batch_size': 121, 'step_size': 3, 'gamma': 0.9898752184104233}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:28:36,056][0m Trial 38 finished with value: 0.10121025890111923 and parameters: {'observation_period_num': 79, 'train_rates': 0.9757996212899742, 'learning_rate': 0.0003384895323173219, 'batch_size': 185, 'step_size': 15, 'gamma': 0.9134356338312489}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:31:26,408][0m Trial 39 finished with value: 0.2114220397921527 and parameters: {'observation_period_num': 248, 'train_rates': 0.8695385285959323, 'learning_rate': 4.590593529897733e-06, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9272226614229372}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:32:45,506][0m Trial 40 finished with value: 0.6068532728567356 and parameters: {'observation_period_num': 106, 'train_rates': 0.8270656535508122, 'learning_rate': 1.1574184760613834e-06, 'batch_size': 64, 'step_size': 14, 'gamma': 0.8175934602486299}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:33:39,586][0m Trial 41 finished with value: 0.030673552304506302 and parameters: {'observation_period_num': 17, 'train_rates': 0.9840176992290307, 'learning_rate': 0.00018858762950385954, 'batch_size': 113, 'step_size': 12, 'gamma': 0.9843904152003713}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:34:39,685][0m Trial 42 finished with value: 0.04248371720314026 and parameters: {'observation_period_num': 31, 'train_rates': 0.9679895225001053, 'learning_rate': 0.0004892974497077067, 'batch_size': 99, 'step_size': 9, 'gamma': 0.9671053093466403}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:35:06,015][0m Trial 43 finished with value: 0.0454569086432457 and parameters: {'observation_period_num': 5, 'train_rates': 0.9431682845124016, 'learning_rate': 0.0007062773723116544, 'batch_size': 246, 'step_size': 11, 'gamma': 0.9814498377699344}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:36:49,387][0m Trial 44 finished with value: 0.04341431997384723 and parameters: {'observation_period_num': 40, 'train_rates': 0.899951812110978, 'learning_rate': 0.00023846827976901585, 'batch_size': 53, 'step_size': 5, 'gamma': 0.9496681228703674}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:37:25,519][0m Trial 45 finished with value: 0.03186165913939476 and parameters: {'observation_period_num': 16, 'train_rates': 0.9862875145571263, 'learning_rate': 0.00014164277845913386, 'batch_size': 174, 'step_size': 13, 'gamma': 0.8524916919248666}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:38:20,890][0m Trial 46 finished with value: 0.051676511615372706 and parameters: {'observation_period_num': 30, 'train_rates': 0.6410364086530536, 'learning_rate': 0.0007471070306479581, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8988799495259985}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:39:06,603][0m Trial 47 finished with value: 0.0859699621796608 and parameters: {'observation_period_num': 152, 'train_rates': 0.9899457792532669, 'learning_rate': 0.0003133539303088671, 'batch_size': 131, 'step_size': 1, 'gamma': 0.9815538981002866}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:39:34,863][0m Trial 48 finished with value: 0.2025790810585022 and parameters: {'observation_period_num': 191, 'train_rates': 0.9631642667377637, 'learning_rate': 3.201406845526649e-05, 'batch_size': 211, 'step_size': 12, 'gamma': 0.9443367134926837}. Best is trial 13 with value: 0.014520448632538319.[0m
[32m[I 2025-01-12 09:40:29,312][0m Trial 49 finished with value: 0.05638738908621681 and parameters: {'observation_period_num': 69, 'train_rates': 0.9137389409901586, 'learning_rate': 8.302244945149111e-05, 'batch_size': 103, 'step_size': 7, 'gamma': 0.9643639033104889}. Best is trial 13 with value: 0.014520448632538319.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-12 09:40:29,323][0m A new study created in memory with name: no-name-b678d5e6-2f93-4100-bca2-f07d5e33beea[0m
[32m[I 2025-01-12 09:44:35,489][0m Trial 0 finished with value: 0.0762448771215907 and parameters: {'observation_period_num': 171, 'train_rates': 0.8433203117946493, 'learning_rate': 3.2608300430356675e-05, 'batch_size': 20, 'step_size': 10, 'gamma': 0.7562002885896242}. Best is trial 0 with value: 0.0762448771215907.[0m
[32m[I 2025-01-12 09:48:24,328][0m Trial 1 finished with value: 0.45889249853059355 and parameters: {'observation_period_num': 72, 'train_rates': 0.9825570723669068, 'learning_rate': 1.1999360609001011e-06, 'batch_size': 25, 'step_size': 2, 'gamma': 0.9250396707127722}. Best is trial 0 with value: 0.0762448771215907.[0m
[32m[I 2025-01-12 09:49:59,644][0m Trial 2 finished with value: 0.12980658875115342 and parameters: {'observation_period_num': 188, 'train_rates': 0.6586085985209795, 'learning_rate': 0.00026417401622196967, 'batch_size': 44, 'step_size': 5, 'gamma': 0.9191730195033901}. Best is trial 0 with value: 0.0762448771215907.[0m
[32m[I 2025-01-12 09:50:41,895][0m Trial 3 finished with value: 0.07070812334616979 and parameters: {'observation_period_num': 10, 'train_rates': 0.9409691149871695, 'learning_rate': 2.806811318855634e-05, 'batch_size': 145, 'step_size': 12, 'gamma': 0.8168323924269711}. Best is trial 3 with value: 0.07070812334616979.[0m
[32m[I 2025-01-12 09:51:08,407][0m Trial 4 finished with value: 0.9340653466522146 and parameters: {'observation_period_num': 111, 'train_rates': 0.6165445262730966, 'learning_rate': 1.7212155285236143e-06, 'batch_size': 162, 'step_size': 15, 'gamma': 0.7602017232156096}. Best is trial 3 with value: 0.07070812334616979.[0m
[32m[I 2025-01-12 09:51:32,159][0m Trial 5 finished with value: 0.664844856713431 and parameters: {'observation_period_num': 56, 'train_rates': 0.655989415642248, 'learning_rate': 1.3148169944544402e-05, 'batch_size': 209, 'step_size': 2, 'gamma': 0.8438180685150725}. Best is trial 3 with value: 0.07070812334616979.[0m
[32m[I 2025-01-12 09:54:10,004][0m Trial 6 finished with value: 0.15774314009217308 and parameters: {'observation_period_num': 247, 'train_rates': 0.7167680533862409, 'learning_rate': 0.00026696699851949044, 'batch_size': 27, 'step_size': 4, 'gamma': 0.7971846968806404}. Best is trial 3 with value: 0.07070812334616979.[0m
[32m[I 2025-01-12 09:54:42,353][0m Trial 7 finished with value: 0.3853939536370729 and parameters: {'observation_period_num': 201, 'train_rates': 0.9171874686025683, 'learning_rate': 8.892486628846759e-06, 'batch_size': 180, 'step_size': 9, 'gamma': 0.9207225189855845}. Best is trial 3 with value: 0.07070812334616979.[0m
[32m[I 2025-01-12 09:55:56,673][0m Trial 8 finished with value: 0.05215640213455398 and parameters: {'observation_period_num': 55, 'train_rates': 0.7649843391941379, 'learning_rate': 0.0002649791242209235, 'batch_size': 67, 'step_size': 10, 'gamma': 0.8936485305467224}. Best is trial 8 with value: 0.05215640213455398.[0m
[32m[I 2025-01-12 09:57:04,943][0m Trial 9 finished with value: 0.4393483661184801 and parameters: {'observation_period_num': 240, 'train_rates': 0.6055782615515118, 'learning_rate': 4.296647325701969e-06, 'batch_size': 57, 'step_size': 9, 'gamma': 0.9131930403262855}. Best is trial 8 with value: 0.05215640213455398.[0m
[32m[I 2025-01-12 09:57:57,406][0m Trial 10 finished with value: 0.028214152314458524 and parameters: {'observation_period_num': 13, 'train_rates': 0.7757764994176849, 'learning_rate': 0.0008133331101716055, 'batch_size': 95, 'step_size': 13, 'gamma': 0.9842631648429588}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 09:58:50,660][0m Trial 11 finished with value: 0.03482622736148249 and parameters: {'observation_period_num': 11, 'train_rates': 0.7617176520110668, 'learning_rate': 0.0009402678718959932, 'batch_size': 95, 'step_size': 13, 'gamma': 0.981341298693384}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 09:59:41,976][0m Trial 12 finished with value: 0.05474361433432652 and parameters: {'observation_period_num': 15, 'train_rates': 0.8274311066074896, 'learning_rate': 0.0008270101336085888, 'batch_size': 104, 'step_size': 15, 'gamma': 0.9861113426663}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:00:30,484][0m Trial 13 finished with value: 0.09327894116441408 and parameters: {'observation_period_num': 103, 'train_rates': 0.7631008310479646, 'learning_rate': 0.0009977933373196623, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9819463331563137}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:01:20,001][0m Trial 14 finished with value: 0.039042473669118505 and parameters: {'observation_period_num': 31, 'train_rates': 0.8525697084877991, 'learning_rate': 9.442302175064847e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.9701910342173594}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:02:16,247][0m Trial 15 finished with value: 0.09740742013240472 and parameters: {'observation_period_num': 147, 'train_rates': 0.7221530287984356, 'learning_rate': 0.00010135730964110902, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9463909271466198}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:02:39,136][0m Trial 16 finished with value: 0.10503140703066811 and parameters: {'observation_period_num': 74, 'train_rates': 0.791309511396426, 'learning_rate': 0.0006023872445281783, 'batch_size': 249, 'step_size': 13, 'gamma': 0.9525148379494863}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:03:17,574][0m Trial 17 finished with value: 0.050435659486845315 and parameters: {'observation_period_num': 42, 'train_rates': 0.714777386092265, 'learning_rate': 0.00010320590897263501, 'batch_size': 127, 'step_size': 14, 'gamma': 0.8579272430350229}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:04:02,215][0m Trial 18 finished with value: 0.08602753755721179 and parameters: {'observation_period_num': 89, 'train_rates': 0.8927223542386801, 'learning_rate': 0.0004477713015599003, 'batch_size': 126, 'step_size': 11, 'gamma': 0.8879121417540266}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:05:05,649][0m Trial 19 finished with value: 0.035403188922115275 and parameters: {'observation_period_num': 6, 'train_rates': 0.7992876592094337, 'learning_rate': 5.7827741780646964e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9537176663231378}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:05:31,625][0m Trial 20 finished with value: 0.20136253163218498 and parameters: {'observation_period_num': 131, 'train_rates': 0.6823612019529666, 'learning_rate': 0.00040283157090998414, 'batch_size': 192, 'step_size': 13, 'gamma': 0.9626869880851872}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:06:36,867][0m Trial 21 finished with value: 0.03620616734117167 and parameters: {'observation_period_num': 9, 'train_rates': 0.8017537810890811, 'learning_rate': 4.2733788352575236e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.94497076723339}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:07:32,257][0m Trial 22 finished with value: 0.03903532551951928 and parameters: {'observation_period_num': 33, 'train_rates': 0.7579844651922137, 'learning_rate': 0.00016410524208966844, 'batch_size': 89, 'step_size': 6, 'gamma': 0.982084408513112}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:09:09,013][0m Trial 23 finished with value: 0.028791637677285407 and parameters: {'observation_period_num': 6, 'train_rates': 0.8777696081021471, 'learning_rate': 0.0009721089905718912, 'batch_size': 55, 'step_size': 8, 'gamma': 0.9430100686439473}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:11:06,540][0m Trial 24 finished with value: 0.12321373678763498 and parameters: {'observation_period_num': 34, 'train_rates': 0.8701147555107864, 'learning_rate': 0.0009406526058956626, 'batch_size': 46, 'step_size': 11, 'gamma': 0.9891182588781077}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:12:32,296][0m Trial 25 finished with value: 0.06391414711656777 and parameters: {'observation_period_num': 55, 'train_rates': 0.8809486115598747, 'learning_rate': 0.0004924151868435137, 'batch_size': 63, 'step_size': 9, 'gamma': 0.9378820307662478}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:13:19,692][0m Trial 26 finished with value: 0.03114378891353096 and parameters: {'observation_period_num': 23, 'train_rates': 0.8232268708024336, 'learning_rate': 0.0003259828369796823, 'batch_size': 111, 'step_size': 4, 'gamma': 0.970301388364454}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:14:06,033][0m Trial 27 finished with value: 0.0536798945004139 and parameters: {'observation_period_num': 77, 'train_rates': 0.8251376791338133, 'learning_rate': 0.00017728386129918858, 'batch_size': 118, 'step_size': 4, 'gamma': 0.962400495959956}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:14:46,158][0m Trial 28 finished with value: 0.09997554131726313 and parameters: {'observation_period_num': 26, 'train_rates': 0.9389038936635815, 'learning_rate': 0.00033911633659336514, 'batch_size': 148, 'step_size': 1, 'gamma': 0.900682554599439}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:15:25,476][0m Trial 29 finished with value: 0.03942317323296778 and parameters: {'observation_period_num': 46, 'train_rates': 0.851656372238291, 'learning_rate': 0.000609538923536769, 'batch_size': 142, 'step_size': 4, 'gamma': 0.931269970059772}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:17:33,590][0m Trial 30 finished with value: 0.029558246205217086 and parameters: {'observation_period_num': 22, 'train_rates': 0.898544034402944, 'learning_rate': 0.0001853825271368429, 'batch_size': 43, 'step_size': 8, 'gamma': 0.8732405108843156}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:19:32,068][0m Trial 31 finished with value: 0.034696034065240786 and parameters: {'observation_period_num': 26, 'train_rates': 0.9017393449551181, 'learning_rate': 0.00017406510521288842, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8777084542906277}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:22:48,933][0m Trial 32 finished with value: 0.08918249120517653 and parameters: {'observation_period_num': 64, 'train_rates': 0.9831495202448293, 'learning_rate': 0.0005894925640687932, 'batch_size': 29, 'step_size': 8, 'gamma': 0.82128689563687}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:24:11,406][0m Trial 33 finished with value: 0.06161663975644779 and parameters: {'observation_period_num': 21, 'train_rates': 0.9512384259621266, 'learning_rate': 0.00035787024215310143, 'batch_size': 70, 'step_size': 8, 'gamma': 0.9698455424583644}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:28:36,457][0m Trial 34 finished with value: 0.0447994578403957 and parameters: {'observation_period_num': 42, 'train_rates': 0.8301133961549374, 'learning_rate': 0.0001660072005296759, 'batch_size': 19, 'step_size': 3, 'gamma': 0.910498120081331}. Best is trial 10 with value: 0.028214152314458524.[0m
[32m[I 2025-01-12 10:30:43,780][0m Trial 35 finished with value: 0.02713974504529134 and parameters: {'observation_period_num': 6, 'train_rates': 0.9220501328933511, 'learning_rate': 0.0006669871545122167, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8637933626221865}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:33:15,869][0m Trial 36 finished with value: 0.11260872779946242 and parameters: {'observation_period_num': 88, 'train_rates': 0.9610602223855994, 'learning_rate': 0.0006241824788821302, 'batch_size': 37, 'step_size': 6, 'gamma': 0.8516836825578014}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:38:46,939][0m Trial 37 finished with value: 0.08311174733516498 and parameters: {'observation_period_num': 157, 'train_rates': 0.9302632587013349, 'learning_rate': 1.6217919424286367e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8692013372521711}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:40:25,702][0m Trial 38 finished with value: 0.09699913064430947 and parameters: {'observation_period_num': 205, 'train_rates': 0.913269899013643, 'learning_rate': 0.00022990444803474305, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8238633727466483}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:42:34,855][0m Trial 39 finished with value: 0.06437712648923519 and parameters: {'observation_period_num': 45, 'train_rates': 0.8723219905702075, 'learning_rate': 0.0006674090907067039, 'batch_size': 41, 'step_size': 5, 'gamma': 0.7828316639018527}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:45:30,756][0m Trial 40 finished with value: 0.030598740483964642 and parameters: {'observation_period_num': 6, 'train_rates': 0.9686217440386475, 'learning_rate': 7.540238807492085e-05, 'batch_size': 33, 'step_size': 8, 'gamma': 0.8457500946723991}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:48:10,687][0m Trial 41 finished with value: 0.041648867611701675 and parameters: {'observation_period_num': 5, 'train_rates': 0.960617785431524, 'learning_rate': 6.146923216336694e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8334544104930656}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:49:34,507][0m Trial 42 finished with value: 0.05383756205643693 and parameters: {'observation_period_num': 17, 'train_rates': 0.975307703080405, 'learning_rate': 2.810800854013583e-05, 'batch_size': 71, 'step_size': 9, 'gamma': 0.8378389805647669}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:52:27,768][0m Trial 43 finished with value: 0.030999376571604183 and parameters: {'observation_period_num': 18, 'train_rates': 0.9238148078614012, 'learning_rate': 8.979186237395155e-05, 'batch_size': 32, 'step_size': 10, 'gamma': 0.802529859777976}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:54:03,445][0m Trial 44 finished with value: 0.050650254542292175 and parameters: {'observation_period_num': 61, 'train_rates': 0.8982144753595975, 'learning_rate': 0.00021312258974703625, 'batch_size': 57, 'step_size': 7, 'gamma': 0.8634209700514621}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:55:47,614][0m Trial 45 finished with value: 0.030342316513375052 and parameters: {'observation_period_num': 6, 'train_rates': 0.9413156030004906, 'learning_rate': 0.00013702527549482436, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8796461189011718}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:57:21,767][0m Trial 46 finished with value: 0.04269132063916278 and parameters: {'observation_period_num': 35, 'train_rates': 0.9137328961764962, 'learning_rate': 0.00013497181471249062, 'batch_size': 58, 'step_size': 11, 'gamma': 0.8792160476191746}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:58:27,944][0m Trial 47 finished with value: 0.2929738615789721 and parameters: {'observation_period_num': 19, 'train_rates': 0.9471654494699598, 'learning_rate': 2.553546895754127e-06, 'batch_size': 88, 'step_size': 12, 'gamma': 0.9032531913402936}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 10:59:45,835][0m Trial 48 finished with value: 0.10704320437136648 and parameters: {'observation_period_num': 49, 'train_rates': 0.8586736268518147, 'learning_rate': 0.0002826083418763519, 'batch_size': 68, 'step_size': 14, 'gamma': 0.9241969581762487}. Best is trial 35 with value: 0.02713974504529134.[0m
[32m[I 2025-01-12 11:00:45,949][0m Trial 49 finished with value: 0.17766661284058424 and parameters: {'observation_period_num': 178, 'train_rates': 0.73796845827549, 'learning_rate': 1.9765710239839432e-05, 'batch_size': 76, 'step_size': 12, 'gamma': 0.8847077363452026}. Best is trial 35 with value: 0.02713974504529134.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.7816413740183745, 'learning_rate': 0.0007539060750226302, 'batch_size': 207, 'step_size': 14, 'gamma': 0.9235069129502878}
Epoch 1/300, trend Loss: 0.4033 | 0.5356
Epoch 2/300, trend Loss: 0.2188 | 0.2354
Epoch 3/300, trend Loss: 0.2036 | 0.1517
Epoch 4/300, trend Loss: 0.1481 | 0.1266
Epoch 5/300, trend Loss: 0.1668 | 0.1285
Epoch 6/300, trend Loss: 0.1468 | 0.1081
Epoch 7/300, trend Loss: 0.1759 | 0.1080
Epoch 8/300, trend Loss: 0.1429 | 0.0938
Epoch 9/300, trend Loss: 0.1541 | 0.1396
Epoch 10/300, trend Loss: 0.1542 | 0.0977
Epoch 11/300, trend Loss: 0.1507 | 0.1710
Epoch 12/300, trend Loss: 0.1450 | 0.0972
Epoch 13/300, trend Loss: 0.1360 | 0.2087
Epoch 14/300, trend Loss: 0.1222 | 0.0830
Epoch 15/300, trend Loss: 0.1136 | 0.0967
Epoch 16/300, trend Loss: 0.1099 | 0.0694
Epoch 17/300, trend Loss: 0.1075 | 0.0732
Epoch 18/300, trend Loss: 0.1056 | 0.0649
Epoch 19/300, trend Loss: 0.1044 | 0.0649
Epoch 20/300, trend Loss: 0.1032 | 0.0626
Epoch 21/300, trend Loss: 0.1021 | 0.0610
Epoch 22/300, trend Loss: 0.1013 | 0.0597
Epoch 23/300, trend Loss: 0.1005 | 0.0595
Epoch 24/300, trend Loss: 0.0999 | 0.0581
Epoch 25/300, trend Loss: 0.0992 | 0.0582
Epoch 26/300, trend Loss: 0.0986 | 0.0566
Epoch 27/300, trend Loss: 0.0980 | 0.0567
Epoch 28/300, trend Loss: 0.0975 | 0.0554
Epoch 29/300, trend Loss: 0.0969 | 0.0559
Epoch 30/300, trend Loss: 0.0964 | 0.0542
Epoch 31/300, trend Loss: 0.0959 | 0.0547
Epoch 32/300, trend Loss: 0.0955 | 0.0527
Epoch 33/300, trend Loss: 0.0949 | 0.0534
Epoch 34/300, trend Loss: 0.0948 | 0.0514
Epoch 35/300, trend Loss: 0.0945 | 0.0529
Epoch 36/300, trend Loss: 0.0946 | 0.0505
Epoch 37/300, trend Loss: 0.0940 | 0.0541
Epoch 38/300, trend Loss: 0.0951 | 0.0514
Epoch 39/300, trend Loss: 0.0960 | 0.0560
Epoch 40/300, trend Loss: 0.0972 | 0.0512
Epoch 41/300, trend Loss: 0.0979 | 0.0627
Epoch 42/300, trend Loss: 0.0992 | 0.0490
Epoch 43/300, trend Loss: 0.1065 | 0.0709
Epoch 44/300, trend Loss: 0.1019 | 0.0563
Epoch 45/300, trend Loss: 0.1066 | 0.0733
Epoch 46/300, trend Loss: 0.1010 | 0.0500
Epoch 47/300, trend Loss: 0.0973 | 0.0626
Epoch 48/300, trend Loss: 0.0987 | 0.0552
Epoch 49/300, trend Loss: 0.0968 | 0.0635
Epoch 50/300, trend Loss: 0.0941 | 0.0475
Epoch 51/300, trend Loss: 0.0916 | 0.0545
Epoch 52/300, trend Loss: 0.0913 | 0.0485
Epoch 53/300, trend Loss: 0.0903 | 0.0517
Epoch 54/300, trend Loss: 0.0896 | 0.0460
Epoch 55/300, trend Loss: 0.0889 | 0.0481
Epoch 56/300, trend Loss: 0.0885 | 0.0462
Epoch 57/300, trend Loss: 0.0880 | 0.0470
Epoch 58/300, trend Loss: 0.0876 | 0.0446
Epoch 59/300, trend Loss: 0.0872 | 0.0456
Epoch 60/300, trend Loss: 0.0870 | 0.0445
Epoch 61/300, trend Loss: 0.0865 | 0.0448
Epoch 62/300, trend Loss: 0.0862 | 0.0433
Epoch 63/300, trend Loss: 0.0858 | 0.0435
Epoch 64/300, trend Loss: 0.0858 | 0.0428
Epoch 65/300, trend Loss: 0.0853 | 0.0432
Epoch 66/300, trend Loss: 0.0851 | 0.0422
Epoch 67/300, trend Loss: 0.0847 | 0.0420
Epoch 68/300, trend Loss: 0.0844 | 0.0412
Epoch 69/300, trend Loss: 0.0842 | 0.0413
Epoch 70/300, trend Loss: 0.0840 | 0.0411
Epoch 71/300, trend Loss: 0.0838 | 0.0413
Epoch 72/300, trend Loss: 0.0839 | 0.0405
Epoch 73/300, trend Loss: 0.0836 | 0.0404
Epoch 74/300, trend Loss: 0.0834 | 0.0401
Epoch 75/300, trend Loss: 0.0837 | 0.0403
Epoch 76/300, trend Loss: 0.0837 | 0.0404
Epoch 77/300, trend Loss: 0.0832 | 0.0403
Epoch 78/300, trend Loss: 0.0827 | 0.0391
Epoch 79/300, trend Loss: 0.0830 | 0.0392
Epoch 80/300, trend Loss: 0.0835 | 0.0390
Epoch 81/300, trend Loss: 0.0823 | 0.0397
Epoch 82/300, trend Loss: 0.0825 | 0.0402
Epoch 83/300, trend Loss: 0.0831 | 0.0395
Epoch 84/300, trend Loss: 0.0817 | 0.0382
Epoch 85/300, trend Loss: 0.0833 | 0.0386
Epoch 86/300, trend Loss: 0.0831 | 0.0392
Epoch 87/300, trend Loss: 0.0816 | 0.0400
Epoch 88/300, trend Loss: 0.0834 | 0.0388
Epoch 89/300, trend Loss: 0.0819 | 0.0383
Epoch 90/300, trend Loss: 0.0831 | 0.0381
Epoch 91/300, trend Loss: 0.0828 | 0.0394
Epoch 92/300, trend Loss: 0.0818 | 0.0398
Epoch 93/300, trend Loss: 0.0830 | 0.0381
Epoch 94/300, trend Loss: 0.0810 | 0.0374
Epoch 95/300, trend Loss: 0.0822 | 0.0387
Epoch 96/300, trend Loss: 0.0808 | 0.0394
Epoch 97/300, trend Loss: 0.0812 | 0.0379
Epoch 98/300, trend Loss: 0.0806 | 0.0370
Epoch 99/300, trend Loss: 0.0807 | 0.0374
Epoch 100/300, trend Loss: 0.0804 | 0.0384
Epoch 101/300, trend Loss: 0.0798 | 0.0377
Epoch 102/300, trend Loss: 0.0799 | 0.0365
Epoch 103/300, trend Loss: 0.0794 | 0.0365
Epoch 104/300, trend Loss: 0.0797 | 0.0370
Epoch 105/300, trend Loss: 0.0789 | 0.0371
Epoch 106/300, trend Loss: 0.0792 | 0.0364
Epoch 107/300, trend Loss: 0.0788 | 0.0360
Epoch 108/300, trend Loss: 0.0788 | 0.0360
Epoch 109/300, trend Loss: 0.0785 | 0.0365
Epoch 110/300, trend Loss: 0.0783 | 0.0361
Epoch 111/300, trend Loss: 0.0784 | 0.0356
Epoch 112/300, trend Loss: 0.0780 | 0.0352
Epoch 113/300, trend Loss: 0.0781 | 0.0356
Epoch 114/300, trend Loss: 0.0778 | 0.0359
Epoch 115/300, trend Loss: 0.0777 | 0.0352
Epoch 116/300, trend Loss: 0.0775 | 0.0348
Epoch 117/300, trend Loss: 0.0775 | 0.0348
Epoch 118/300, trend Loss: 0.0775 | 0.0350
Epoch 119/300, trend Loss: 0.0770 | 0.0352
Epoch 120/300, trend Loss: 0.0772 | 0.0346
Epoch 121/300, trend Loss: 0.0770 | 0.0343
Epoch 122/300, trend Loss: 0.0770 | 0.0341
Epoch 123/300, trend Loss: 0.0768 | 0.0345
Epoch 124/300, trend Loss: 0.0764 | 0.0347
Epoch 125/300, trend Loss: 0.0767 | 0.0342
Epoch 126/300, trend Loss: 0.0763 | 0.0337
Epoch 127/300, trend Loss: 0.0765 | 0.0337
Epoch 128/300, trend Loss: 0.0764 | 0.0341
Epoch 129/300, trend Loss: 0.0757 | 0.0340
Epoch 130/300, trend Loss: 0.0760 | 0.0337
Epoch 131/300, trend Loss: 0.0757 | 0.0333
Epoch 132/300, trend Loss: 0.0758 | 0.0331
Epoch 133/300, trend Loss: 0.0756 | 0.0334
Epoch 134/300, trend Loss: 0.0751 | 0.0336
Epoch 135/300, trend Loss: 0.0753 | 0.0332
Epoch 136/300, trend Loss: 0.0749 | 0.0327
Epoch 137/300, trend Loss: 0.0750 | 0.0325
Epoch 138/300, trend Loss: 0.0749 | 0.0328
Epoch 139/300, trend Loss: 0.0743 | 0.0329
Epoch 140/300, trend Loss: 0.0745 | 0.0327
Epoch 141/300, trend Loss: 0.0743 | 0.0322
Epoch 142/300, trend Loss: 0.0744 | 0.0318
Epoch 143/300, trend Loss: 0.0746 | 0.0320
Epoch 144/300, trend Loss: 0.0738 | 0.0326
Epoch 145/300, trend Loss: 0.0740 | 0.0323
Epoch 146/300, trend Loss: 0.0739 | 0.0317
Epoch 147/300, trend Loss: 0.0739 | 0.0313
Epoch 148/300, trend Loss: 0.0747 | 0.0318
Epoch 149/300, trend Loss: 0.0734 | 0.0325
Epoch 150/300, trend Loss: 0.0740 | 0.0316
Epoch 151/300, trend Loss: 0.0733 | 0.0314
Epoch 152/300, trend Loss: 0.0738 | 0.0311
Epoch 153/300, trend Loss: 0.0733 | 0.0321
Epoch 154/300, trend Loss: 0.0727 | 0.0314
Epoch 155/300, trend Loss: 0.0729 | 0.0310
Epoch 156/300, trend Loss: 0.0724 | 0.0308
Epoch 157/300, trend Loss: 0.0726 | 0.0314
Epoch 158/300, trend Loss: 0.0719 | 0.0312
Epoch 159/300, trend Loss: 0.0720 | 0.0306
Epoch 160/300, trend Loss: 0.0717 | 0.0305
Epoch 161/300, trend Loss: 0.0719 | 0.0307
Epoch 162/300, trend Loss: 0.0715 | 0.0310
Epoch 163/300, trend Loss: 0.0714 | 0.0304
Epoch 164/300, trend Loss: 0.0713 | 0.0303
Epoch 165/300, trend Loss: 0.0712 | 0.0303
Epoch 166/300, trend Loss: 0.0712 | 0.0306
Epoch 167/300, trend Loss: 0.0709 | 0.0305
Epoch 168/300, trend Loss: 0.0711 | 0.0301
Epoch 169/300, trend Loss: 0.0709 | 0.0301
Epoch 170/300, trend Loss: 0.0709 | 0.0302
Epoch 171/300, trend Loss: 0.0708 | 0.0303
Epoch 172/300, trend Loss: 0.0706 | 0.0301
Epoch 173/300, trend Loss: 0.0707 | 0.0299
Epoch 174/300, trend Loss: 0.0705 | 0.0300
Epoch 175/300, trend Loss: 0.0707 | 0.0301
Epoch 176/300, trend Loss: 0.0704 | 0.0301
Epoch 177/300, trend Loss: 0.0705 | 0.0298
Epoch 178/300, trend Loss: 0.0704 | 0.0298
Epoch 179/300, trend Loss: 0.0703 | 0.0299
Epoch 180/300, trend Loss: 0.0703 | 0.0300
Epoch 181/300, trend Loss: 0.0701 | 0.0298
Epoch 182/300, trend Loss: 0.0703 | 0.0296
Epoch 183/300, trend Loss: 0.0700 | 0.0297
Epoch 184/300, trend Loss: 0.0701 | 0.0299
Epoch 185/300, trend Loss: 0.0699 | 0.0298
Epoch 186/300, trend Loss: 0.0699 | 0.0295
Epoch 187/300, trend Loss: 0.0698 | 0.0295
Epoch 188/300, trend Loss: 0.0698 | 0.0297
Epoch 189/300, trend Loss: 0.0698 | 0.0297
Epoch 190/300, trend Loss: 0.0696 | 0.0295
Epoch 191/300, trend Loss: 0.0697 | 0.0294
Epoch 192/300, trend Loss: 0.0695 | 0.0295
Epoch 193/300, trend Loss: 0.0696 | 0.0296
Epoch 194/300, trend Loss: 0.0694 | 0.0295
Epoch 195/300, trend Loss: 0.0694 | 0.0293
Epoch 196/300, trend Loss: 0.0693 | 0.0294
Epoch 197/300, trend Loss: 0.0693 | 0.0295
Epoch 198/300, trend Loss: 0.0693 | 0.0294
Epoch 199/300, trend Loss: 0.0692 | 0.0292
Epoch 200/300, trend Loss: 0.0692 | 0.0292
Epoch 201/300, trend Loss: 0.0691 | 0.0293
Epoch 202/300, trend Loss: 0.0691 | 0.0293
Epoch 203/300, trend Loss: 0.0690 | 0.0292
Epoch 204/300, trend Loss: 0.0690 | 0.0291
Epoch 205/300, trend Loss: 0.0689 | 0.0292
Epoch 206/300, trend Loss: 0.0689 | 0.0292
Epoch 207/300, trend Loss: 0.0689 | 0.0292
Epoch 208/300, trend Loss: 0.0688 | 0.0291
Epoch 209/300, trend Loss: 0.0688 | 0.0291
Epoch 210/300, trend Loss: 0.0687 | 0.0291
Epoch 211/300, trend Loss: 0.0687 | 0.0291
Epoch 212/300, trend Loss: 0.0687 | 0.0290
Epoch 213/300, trend Loss: 0.0686 | 0.0290
Epoch 214/300, trend Loss: 0.0686 | 0.0290
Epoch 215/300, trend Loss: 0.0686 | 0.0290
Epoch 216/300, trend Loss: 0.0685 | 0.0290
Epoch 217/300, trend Loss: 0.0685 | 0.0289
Epoch 218/300, trend Loss: 0.0684 | 0.0289
Epoch 219/300, trend Loss: 0.0684 | 0.0289
Epoch 220/300, trend Loss: 0.0684 | 0.0289
Epoch 221/300, trend Loss: 0.0683 | 0.0288
Epoch 222/300, trend Loss: 0.0683 | 0.0288
Epoch 223/300, trend Loss: 0.0683 | 0.0288
Epoch 224/300, trend Loss: 0.0682 | 0.0288
Epoch 225/300, trend Loss: 0.0682 | 0.0288
Epoch 226/300, trend Loss: 0.0681 | 0.0287
Epoch 227/300, trend Loss: 0.0681 | 0.0287
Epoch 228/300, trend Loss: 0.0681 | 0.0287
Epoch 229/300, trend Loss: 0.0680 | 0.0286
Epoch 230/300, trend Loss: 0.0680 | 0.0286
Epoch 231/300, trend Loss: 0.0680 | 0.0286
Epoch 232/300, trend Loss: 0.0679 | 0.0286
Epoch 233/300, trend Loss: 0.0679 | 0.0285
Epoch 234/300, trend Loss: 0.0679 | 0.0285
Epoch 235/300, trend Loss: 0.0678 | 0.0285
Epoch 236/300, trend Loss: 0.0678 | 0.0285
Epoch 237/300, trend Loss: 0.0677 | 0.0284
Epoch 238/300, trend Loss: 0.0677 | 0.0284
Epoch 239/300, trend Loss: 0.0677 | 0.0284
Epoch 240/300, trend Loss: 0.0676 | 0.0283
Epoch 241/300, trend Loss: 0.0676 | 0.0283
Epoch 242/300, trend Loss: 0.0675 | 0.0283
Epoch 243/300, trend Loss: 0.0675 | 0.0282
Epoch 244/300, trend Loss: 0.0675 | 0.0282
Epoch 245/300, trend Loss: 0.0674 | 0.0282
Epoch 246/300, trend Loss: 0.0674 | 0.0282
Epoch 247/300, trend Loss: 0.0673 | 0.0281
Epoch 248/300, trend Loss: 0.0673 | 0.0281
Epoch 249/300, trend Loss: 0.0673 | 0.0281
Epoch 250/300, trend Loss: 0.0672 | 0.0280
Epoch 251/300, trend Loss: 0.0672 | 0.0280
Epoch 252/300, trend Loss: 0.0671 | 0.0280
Epoch 253/300, trend Loss: 0.0671 | 0.0279
Epoch 254/300, trend Loss: 0.0671 | 0.0279
Epoch 255/300, trend Loss: 0.0670 | 0.0278
Epoch 256/300, trend Loss: 0.0670 | 0.0278
Epoch 257/300, trend Loss: 0.0669 | 0.0278
Epoch 258/300, trend Loss: 0.0669 | 0.0277
Epoch 259/300, trend Loss: 0.0669 | 0.0277
Epoch 260/300, trend Loss: 0.0668 | 0.0277
Epoch 261/300, trend Loss: 0.0668 | 0.0276
Epoch 262/300, trend Loss: 0.0667 | 0.0276
Epoch 263/300, trend Loss: 0.0667 | 0.0276
Epoch 264/300, trend Loss: 0.0667 | 0.0275
Epoch 265/300, trend Loss: 0.0666 | 0.0275
Epoch 266/300, trend Loss: 0.0666 | 0.0275
Epoch 267/300, trend Loss: 0.0665 | 0.0274
Epoch 268/300, trend Loss: 0.0665 | 0.0274
Epoch 269/300, trend Loss: 0.0665 | 0.0274
Epoch 270/300, trend Loss: 0.0664 | 0.0273
Epoch 271/300, trend Loss: 0.0664 | 0.0273
Epoch 272/300, trend Loss: 0.0664 | 0.0273
Epoch 273/300, trend Loss: 0.0663 | 0.0272
Epoch 274/300, trend Loss: 0.0663 | 0.0272
Epoch 275/300, trend Loss: 0.0662 | 0.0272
Epoch 276/300, trend Loss: 0.0662 | 0.0271
Epoch 277/300, trend Loss: 0.0662 | 0.0271
Epoch 278/300, trend Loss: 0.0662 | 0.0271
Epoch 279/300, trend Loss: 0.0661 | 0.0271
Epoch 280/300, trend Loss: 0.0661 | 0.0271
Epoch 281/300, trend Loss: 0.0661 | 0.0270
Epoch 282/300, trend Loss: 0.0660 | 0.0270
Epoch 283/300, trend Loss: 0.0660 | 0.0270
Epoch 284/300, trend Loss: 0.0660 | 0.0269
Epoch 285/300, trend Loss: 0.0660 | 0.0269
Epoch 286/300, trend Loss: 0.0659 | 0.0269
Epoch 287/300, trend Loss: 0.0659 | 0.0269
Epoch 288/300, trend Loss: 0.0659 | 0.0269
Epoch 289/300, trend Loss: 0.0659 | 0.0269
Epoch 290/300, trend Loss: 0.0659 | 0.0268
Epoch 291/300, trend Loss: 0.0658 | 0.0268
Epoch 292/300, trend Loss: 0.0659 | 0.0268
Epoch 293/300, trend Loss: 0.0658 | 0.0267
Epoch 294/300, trend Loss: 0.0657 | 0.0267
Epoch 295/300, trend Loss: 0.0657 | 0.0268
Epoch 296/300, trend Loss: 0.0658 | 0.0267
Epoch 297/300, trend Loss: 0.0657 | 0.0267
Epoch 298/300, trend Loss: 0.0657 | 0.0267
Epoch 299/300, trend Loss: 0.0657 | 0.0266
Epoch 300/300, trend Loss: 0.0656 | 0.0266
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.9894244644353496, 'learning_rate': 0.0006516498651645316, 'batch_size': 36, 'step_size': 7, 'gamma': 0.9682349442588372}
Epoch 1/300, seasonal_0 Loss: 0.7641 | 0.2301
Epoch 2/300, seasonal_0 Loss: 0.1805 | 0.0872
Epoch 3/300, seasonal_0 Loss: 0.1269 | 0.0844
Epoch 4/300, seasonal_0 Loss: 0.1438 | 0.0814
Epoch 5/300, seasonal_0 Loss: 0.1333 | 0.0613
Epoch 6/300, seasonal_0 Loss: 0.1328 | 0.1278
Epoch 7/300, seasonal_0 Loss: 0.1486 | 0.1105
Epoch 8/300, seasonal_0 Loss: 0.1298 | 0.1023
Epoch 9/300, seasonal_0 Loss: 0.1198 | 0.0694
Epoch 10/300, seasonal_0 Loss: 0.0906 | 0.0529
Epoch 11/300, seasonal_0 Loss: 0.0864 | 0.0663
Epoch 12/300, seasonal_0 Loss: 0.0864 | 0.0592
Epoch 13/300, seasonal_0 Loss: 0.0782 | 0.0404
Epoch 14/300, seasonal_0 Loss: 0.0751 | 0.0445
Epoch 15/300, seasonal_0 Loss: 0.0794 | 0.0939
Epoch 16/300, seasonal_0 Loss: 0.0798 | 0.0464
Epoch 17/300, seasonal_0 Loss: 0.0807 | 0.0571
Epoch 18/300, seasonal_0 Loss: 0.0710 | 0.0421
Epoch 19/300, seasonal_0 Loss: 0.0687 | 0.0619
Epoch 20/300, seasonal_0 Loss: 0.0743 | 0.0646
Epoch 21/300, seasonal_0 Loss: 0.0658 | 0.0556
Epoch 22/300, seasonal_0 Loss: 0.0673 | 0.0389
Epoch 23/300, seasonal_0 Loss: 0.0649 | 0.0489
Epoch 24/300, seasonal_0 Loss: 0.0670 | 0.0227
Epoch 25/300, seasonal_0 Loss: 0.0676 | 0.0667
Epoch 26/300, seasonal_0 Loss: 0.0649 | 0.0623
Epoch 27/300, seasonal_0 Loss: 0.0692 | 0.0820
Epoch 28/300, seasonal_0 Loss: 0.0761 | 0.0319
Epoch 29/300, seasonal_0 Loss: 0.0704 | 0.0265
Epoch 30/300, seasonal_0 Loss: 0.0703 | 0.0453
Epoch 31/300, seasonal_0 Loss: 0.0749 | 0.0331
Epoch 32/300, seasonal_0 Loss: 0.0707 | 0.0605
Epoch 33/300, seasonal_0 Loss: 0.0734 | 0.0558
Epoch 34/300, seasonal_0 Loss: 0.0716 | 0.0557
Epoch 35/300, seasonal_0 Loss: 0.0647 | 0.0348
Epoch 36/300, seasonal_0 Loss: 0.0652 | 0.0381
Epoch 37/300, seasonal_0 Loss: 0.0641 | 0.0275
Epoch 38/300, seasonal_0 Loss: 0.0591 | 0.0218
Epoch 39/300, seasonal_0 Loss: 0.0536 | 0.0237
Epoch 40/300, seasonal_0 Loss: 0.0570 | 0.0253
Epoch 41/300, seasonal_0 Loss: 0.0588 | 0.0248
Epoch 42/300, seasonal_0 Loss: 0.0556 | 0.0336
Epoch 43/300, seasonal_0 Loss: 0.0547 | 0.0201
Epoch 44/300, seasonal_0 Loss: 0.0554 | 0.0259
Epoch 45/300, seasonal_0 Loss: 0.0539 | 0.0287
Epoch 46/300, seasonal_0 Loss: 0.0499 | 0.0249
Epoch 47/300, seasonal_0 Loss: 0.0495 | 0.0277
Epoch 48/300, seasonal_0 Loss: 0.0474 | 0.0288
Epoch 49/300, seasonal_0 Loss: 0.0478 | 0.0273
Epoch 50/300, seasonal_0 Loss: 0.0503 | 0.0563
Epoch 51/300, seasonal_0 Loss: 0.0490 | 0.0402
Epoch 52/300, seasonal_0 Loss: 0.0486 | 0.0419
Epoch 53/300, seasonal_0 Loss: 0.0525 | 0.0236
Epoch 54/300, seasonal_0 Loss: 0.0522 | 0.0329
Epoch 55/300, seasonal_0 Loss: 0.0536 | 0.0252
Epoch 56/300, seasonal_0 Loss: 0.0533 | 0.0354
Epoch 57/300, seasonal_0 Loss: 0.0480 | 0.0453
Epoch 58/300, seasonal_0 Loss: 0.0478 | 0.0333
Epoch 59/300, seasonal_0 Loss: 0.0489 | 0.0265
Epoch 60/300, seasonal_0 Loss: 0.0444 | 0.0236
Epoch 61/300, seasonal_0 Loss: 0.0461 | 0.0290
Epoch 62/300, seasonal_0 Loss: 0.0487 | 0.0278
Epoch 63/300, seasonal_0 Loss: 0.0466 | 0.0298
Epoch 64/300, seasonal_0 Loss: 0.0468 | 0.0317
Epoch 65/300, seasonal_0 Loss: 0.0476 | 0.0301
Epoch 66/300, seasonal_0 Loss: 0.0512 | 0.0396
Epoch 67/300, seasonal_0 Loss: 0.0523 | 0.0275
Epoch 68/300, seasonal_0 Loss: 0.0492 | 0.0282
Epoch 69/300, seasonal_0 Loss: 0.0537 | 0.0404
Epoch 70/300, seasonal_0 Loss: 0.0482 | 0.0235
Epoch 71/300, seasonal_0 Loss: 0.0579 | 0.0380
Epoch 72/300, seasonal_0 Loss: 0.0572 | 0.0338
Epoch 73/300, seasonal_0 Loss: 0.0732 | 0.0478
Epoch 74/300, seasonal_0 Loss: 0.0652 | 0.0299
Epoch 75/300, seasonal_0 Loss: 0.0688 | 0.1119
Epoch 76/300, seasonal_0 Loss: 0.1037 | 0.0564
Epoch 77/300, seasonal_0 Loss: 0.0799 | 0.0558
Epoch 78/300, seasonal_0 Loss: 0.0668 | 0.0620
Epoch 79/300, seasonal_0 Loss: 0.0564 | 0.0261
Epoch 80/300, seasonal_0 Loss: 0.0493 | 0.0255
Epoch 81/300, seasonal_0 Loss: 0.0640 | 0.0372
Epoch 82/300, seasonal_0 Loss: 0.0506 | 0.0230
Epoch 83/300, seasonal_0 Loss: 0.0477 | 0.0270
Epoch 84/300, seasonal_0 Loss: 0.0467 | 0.0218
Epoch 85/300, seasonal_0 Loss: 0.0434 | 0.0255
Epoch 86/300, seasonal_0 Loss: 0.0457 | 0.0269
Epoch 87/300, seasonal_0 Loss: 0.0441 | 0.0419
Epoch 88/300, seasonal_0 Loss: 0.0430 | 0.0348
Epoch 89/300, seasonal_0 Loss: 0.0420 | 0.0296
Epoch 90/300, seasonal_0 Loss: 0.0451 | 0.0294
Epoch 91/300, seasonal_0 Loss: 0.0434 | 0.0286
Epoch 92/300, seasonal_0 Loss: 0.0439 | 0.0254
Epoch 93/300, seasonal_0 Loss: 0.0450 | 0.0304
Epoch 94/300, seasonal_0 Loss: 0.0424 | 0.0241
Epoch 95/300, seasonal_0 Loss: 0.0397 | 0.0304
Epoch 96/300, seasonal_0 Loss: 0.0393 | 0.0246
Epoch 97/300, seasonal_0 Loss: 0.0509 | 0.0287
Epoch 98/300, seasonal_0 Loss: 0.0452 | 0.0264
Epoch 99/300, seasonal_0 Loss: 0.0482 | 0.0229
Epoch 100/300, seasonal_0 Loss: 0.0414 | 0.0245
Epoch 101/300, seasonal_0 Loss: 0.0395 | 0.0192
Epoch 102/300, seasonal_0 Loss: 0.0459 | 0.0253
Epoch 103/300, seasonal_0 Loss: 0.0469 | 0.0197
Epoch 104/300, seasonal_0 Loss: 0.0494 | 0.0222
Epoch 105/300, seasonal_0 Loss: 0.0440 | 0.0241
Epoch 106/300, seasonal_0 Loss: 0.0431 | 0.0239
Epoch 107/300, seasonal_0 Loss: 0.0426 | 0.0227
Epoch 108/300, seasonal_0 Loss: 0.0432 | 0.0255
Epoch 109/300, seasonal_0 Loss: 0.0438 | 0.0208
Epoch 110/300, seasonal_0 Loss: 0.0460 | 0.0235
Epoch 111/300, seasonal_0 Loss: 0.0429 | 0.0229
Epoch 112/300, seasonal_0 Loss: 0.0431 | 0.0255
Epoch 113/300, seasonal_0 Loss: 0.0443 | 0.0372
Epoch 114/300, seasonal_0 Loss: 0.0444 | 0.0565
Epoch 115/300, seasonal_0 Loss: 0.0405 | 0.0334
Epoch 116/300, seasonal_0 Loss: 0.0405 | 0.0247
Epoch 117/300, seasonal_0 Loss: 0.0365 | 0.0322
Epoch 118/300, seasonal_0 Loss: 0.0397 | 0.0255
Epoch 119/300, seasonal_0 Loss: 0.0389 | 0.0235
Epoch 120/300, seasonal_0 Loss: 0.0389 | 0.0231
Epoch 121/300, seasonal_0 Loss: 0.0362 | 0.0234
Epoch 122/300, seasonal_0 Loss: 0.0376 | 0.0258
Epoch 123/300, seasonal_0 Loss: 0.0394 | 0.0373
Epoch 124/300, seasonal_0 Loss: 0.0374 | 0.0223
Epoch 125/300, seasonal_0 Loss: 0.0370 | 0.0257
Epoch 126/300, seasonal_0 Loss: 0.0461 | 0.0224
Epoch 127/300, seasonal_0 Loss: 0.0396 | 0.0233
Epoch 128/300, seasonal_0 Loss: 0.0368 | 0.0313
Epoch 129/300, seasonal_0 Loss: 0.0367 | 0.0222
Epoch 130/300, seasonal_0 Loss: 0.0342 | 0.0254
Epoch 131/300, seasonal_0 Loss: 0.0371 | 0.0345
Epoch 132/300, seasonal_0 Loss: 0.0349 | 0.0314
Epoch 133/300, seasonal_0 Loss: 0.0339 | 0.0448
Epoch 134/300, seasonal_0 Loss: 0.0339 | 0.0316
Epoch 135/300, seasonal_0 Loss: 0.0379 | 0.0270
Epoch 136/300, seasonal_0 Loss: 0.0357 | 0.0294
Epoch 137/300, seasonal_0 Loss: 0.0339 | 0.0246
Epoch 138/300, seasonal_0 Loss: 0.0374 | 0.0308
Epoch 139/300, seasonal_0 Loss: 0.0424 | 0.0360
Epoch 140/300, seasonal_0 Loss: 0.0465 | 0.0247
Epoch 141/300, seasonal_0 Loss: 0.0404 | 0.0244
Epoch 142/300, seasonal_0 Loss: 0.0399 | 0.0304
Epoch 143/300, seasonal_0 Loss: 0.0390 | 0.0239
Epoch 144/300, seasonal_0 Loss: 0.0366 | 0.0323
Epoch 145/300, seasonal_0 Loss: 0.0343 | 0.0391
Epoch 146/300, seasonal_0 Loss: 0.0355 | 0.0319
Epoch 147/300, seasonal_0 Loss: 0.0359 | 0.0306
Epoch 148/300, seasonal_0 Loss: 0.0353 | 0.0230
Epoch 149/300, seasonal_0 Loss: 0.0358 | 0.0272
Epoch 150/300, seasonal_0 Loss: 0.0393 | 0.0233
Epoch 151/300, seasonal_0 Loss: 0.0364 | 0.0228
Epoch 152/300, seasonal_0 Loss: 0.0383 | 0.0281
Epoch 153/300, seasonal_0 Loss: 0.0355 | 0.0251
Epoch 154/300, seasonal_0 Loss: 0.0401 | 0.0233
Epoch 155/300, seasonal_0 Loss: 0.0367 | 0.0238
Epoch 156/300, seasonal_0 Loss: 0.0348 | 0.0244
Epoch 157/300, seasonal_0 Loss: 0.0361 | 0.0284
Epoch 158/300, seasonal_0 Loss: 0.0372 | 0.0425
Epoch 159/300, seasonal_0 Loss: 0.0380 | 0.0268
Epoch 160/300, seasonal_0 Loss: 0.0375 | 0.0364
Epoch 161/300, seasonal_0 Loss: 0.0390 | 0.0389
Epoch 162/300, seasonal_0 Loss: 0.0446 | 0.0417
Epoch 163/300, seasonal_0 Loss: 0.0435 | 0.0417
Epoch 164/300, seasonal_0 Loss: 0.0408 | 0.0479
Epoch 165/300, seasonal_0 Loss: 0.0380 | 0.0545
Epoch 166/300, seasonal_0 Loss: 0.0377 | 0.0325
Epoch 167/300, seasonal_0 Loss: 0.0360 | 0.0282
Epoch 168/300, seasonal_0 Loss: 0.0359 | 0.0260
Epoch 169/300, seasonal_0 Loss: 0.0330 | 0.0244
Epoch 170/300, seasonal_0 Loss: 0.0322 | 0.0245
Epoch 171/300, seasonal_0 Loss: 0.0335 | 0.0225
Epoch 172/300, seasonal_0 Loss: 0.0331 | 0.0250
Epoch 173/300, seasonal_0 Loss: 0.0334 | 0.0317
Epoch 174/300, seasonal_0 Loss: 0.0305 | 0.0257
Epoch 175/300, seasonal_0 Loss: 0.0304 | 0.0323
Epoch 176/300, seasonal_0 Loss: 0.0304 | 0.0253
Epoch 177/300, seasonal_0 Loss: 0.0326 | 0.0261
Epoch 178/300, seasonal_0 Loss: 0.0308 | 0.0263
Epoch 179/300, seasonal_0 Loss: 0.0322 | 0.0247
Epoch 180/300, seasonal_0 Loss: 0.0302 | 0.0266
Epoch 181/300, seasonal_0 Loss: 0.0291 | 0.0271
Epoch 182/300, seasonal_0 Loss: 0.0310 | 0.0259
Epoch 183/300, seasonal_0 Loss: 0.0361 | 0.0345
Epoch 184/300, seasonal_0 Loss: 0.0362 | 0.0332
Epoch 185/300, seasonal_0 Loss: 0.0314 | 0.0392
Epoch 186/300, seasonal_0 Loss: 0.0312 | 0.0336
Epoch 187/300, seasonal_0 Loss: 0.0343 | 0.0264
Epoch 188/300, seasonal_0 Loss: 0.0339 | 0.0275
Epoch 189/300, seasonal_0 Loss: 0.0338 | 0.0323
Epoch 190/300, seasonal_0 Loss: 0.0324 | 0.1133
Epoch 191/300, seasonal_0 Loss: 0.0318 | 0.0747
Epoch 192/300, seasonal_0 Loss: 0.0307 | 0.0386
Epoch 193/300, seasonal_0 Loss: 0.0283 | 0.0246
Epoch 194/300, seasonal_0 Loss: 0.0333 | 0.0236
Epoch 195/300, seasonal_0 Loss: 0.0347 | 0.0306
Epoch 196/300, seasonal_0 Loss: 0.0325 | 0.0294
Epoch 197/300, seasonal_0 Loss: 0.0295 | 0.0252
Epoch 198/300, seasonal_0 Loss: 0.0278 | 0.0270
Epoch 199/300, seasonal_0 Loss: 0.0275 | 0.0290
Epoch 200/300, seasonal_0 Loss: 0.0313 | 0.0351
Epoch 201/300, seasonal_0 Loss: 0.0298 | 0.0326
Epoch 202/300, seasonal_0 Loss: 0.0286 | 0.0279
Epoch 203/300, seasonal_0 Loss: 0.0306 | 0.0234
Epoch 204/300, seasonal_0 Loss: 0.0286 | 0.0222
Epoch 205/300, seasonal_0 Loss: 0.0298 | 0.0230
Epoch 206/300, seasonal_0 Loss: 0.0282 | 0.0239
Epoch 207/300, seasonal_0 Loss: 0.0263 | 0.0246
Epoch 208/300, seasonal_0 Loss: 0.0260 | 0.0250
Epoch 209/300, seasonal_0 Loss: 0.0290 | 0.0242
Epoch 210/300, seasonal_0 Loss: 0.0295 | 0.0219
Epoch 211/300, seasonal_0 Loss: 0.0307 | 0.0226
Epoch 212/300, seasonal_0 Loss: 0.0288 | 0.0234
Epoch 213/300, seasonal_0 Loss: 0.0268 | 0.0214
Epoch 214/300, seasonal_0 Loss: 0.0230 | 0.0204
Epoch 215/300, seasonal_0 Loss: 0.0308 | 0.0201
Epoch 216/300, seasonal_0 Loss: 0.0447 | 0.0231
Epoch 217/300, seasonal_0 Loss: 0.0358 | 0.0219
Epoch 218/300, seasonal_0 Loss: 0.0326 | 0.0227
Epoch 219/300, seasonal_0 Loss: 0.0302 | 0.0222
Epoch 220/300, seasonal_0 Loss: 0.0284 | 0.0228
Epoch 221/300, seasonal_0 Loss: 0.0279 | 0.0232
Epoch 222/300, seasonal_0 Loss: 0.0262 | 0.0240
Epoch 223/300, seasonal_0 Loss: 0.0277 | 0.0221
Epoch 224/300, seasonal_0 Loss: 0.0303 | 0.0228
Epoch 225/300, seasonal_0 Loss: 0.0275 | 0.0233
Epoch 226/300, seasonal_0 Loss: 0.0262 | 0.0238
Epoch 227/300, seasonal_0 Loss: 0.0257 | 0.0242
Epoch 228/300, seasonal_0 Loss: 0.0276 | 0.0238
Epoch 229/300, seasonal_0 Loss: 0.0286 | 0.0249
Epoch 230/300, seasonal_0 Loss: 0.0248 | 0.0218
Epoch 231/300, seasonal_0 Loss: 0.0285 | 0.0217
Epoch 232/300, seasonal_0 Loss: 0.0231 | 0.0211
Epoch 233/300, seasonal_0 Loss: 0.0273 | 0.0205
Epoch 234/300, seasonal_0 Loss: 0.0314 | 0.0211
Epoch 235/300, seasonal_0 Loss: 0.0280 | 0.0210
Epoch 236/300, seasonal_0 Loss: 0.0240 | 0.0214
Epoch 237/300, seasonal_0 Loss: 0.0227 | 0.0222
Epoch 238/300, seasonal_0 Loss: 0.0226 | 0.0226
Epoch 239/300, seasonal_0 Loss: 0.0246 | 0.0237
Epoch 240/300, seasonal_0 Loss: 0.0237 | 0.0264
Epoch 241/300, seasonal_0 Loss: 0.0235 | 0.0294
Epoch 242/300, seasonal_0 Loss: 0.0234 | 0.0276
Epoch 243/300, seasonal_0 Loss: 0.0238 | 0.0248
Epoch 244/300, seasonal_0 Loss: 0.0233 | 0.0209
Epoch 245/300, seasonal_0 Loss: 0.0252 | 0.0214
Epoch 246/300, seasonal_0 Loss: 0.0240 | 0.0216
Epoch 247/300, seasonal_0 Loss: 0.0235 | 0.0225
Epoch 248/300, seasonal_0 Loss: 0.0238 | 0.0250
Epoch 249/300, seasonal_0 Loss: 0.0240 | 0.0241
Epoch 250/300, seasonal_0 Loss: 0.0289 | 0.0244
Epoch 251/300, seasonal_0 Loss: 0.0289 | 0.0213
Epoch 252/300, seasonal_0 Loss: 0.0285 | 0.0210
Epoch 253/300, seasonal_0 Loss: 0.0286 | 0.0205
Epoch 254/300, seasonal_0 Loss: 0.0267 | 0.0200
Epoch 255/300, seasonal_0 Loss: 0.0241 | 0.0208
Epoch 256/300, seasonal_0 Loss: 0.0310 | 0.0233
Epoch 257/300, seasonal_0 Loss: 0.0313 | 0.0236
Epoch 258/300, seasonal_0 Loss: 0.0290 | 0.0214
Epoch 259/300, seasonal_0 Loss: 0.0281 | 0.0228
Epoch 260/300, seasonal_0 Loss: 0.0319 | 0.0240
Epoch 261/300, seasonal_0 Loss: 0.0305 | 0.0273
Epoch 262/300, seasonal_0 Loss: 0.0276 | 0.0311
Epoch 263/300, seasonal_0 Loss: 0.0234 | 0.0337
Epoch 264/300, seasonal_0 Loss: 0.0283 | 0.0211
Epoch 265/300, seasonal_0 Loss: 0.0236 | 0.0221
Epoch 266/300, seasonal_0 Loss: 0.0236 | 0.0207
Epoch 267/300, seasonal_0 Loss: 0.0231 | 0.0218
Epoch 268/300, seasonal_0 Loss: 0.0277 | 0.0224
Epoch 269/300, seasonal_0 Loss: 0.0236 | 0.0217
Epoch 270/300, seasonal_0 Loss: 0.0212 | 0.0214
Epoch 271/300, seasonal_0 Loss: 0.0223 | 0.0230
Epoch 272/300, seasonal_0 Loss: 0.0307 | 0.0221
Epoch 273/300, seasonal_0 Loss: 0.0282 | 0.0225
Epoch 274/300, seasonal_0 Loss: 0.0286 | 0.0239
Epoch 275/300, seasonal_0 Loss: 0.0271 | 0.0230
Epoch 276/300, seasonal_0 Loss: 0.0274 | 0.0230
Epoch 277/300, seasonal_0 Loss: 0.0260 | 0.0207
Epoch 278/300, seasonal_0 Loss: 0.0242 | 0.0206
Epoch 279/300, seasonal_0 Loss: 0.0290 | 0.0225
Epoch 280/300, seasonal_0 Loss: 0.0291 | 0.0209
Epoch 281/300, seasonal_0 Loss: 0.0219 | 0.0223
Epoch 282/300, seasonal_0 Loss: 0.0297 | 0.0225
Epoch 283/300, seasonal_0 Loss: 0.0269 | 0.0219
Epoch 284/300, seasonal_0 Loss: 0.0254 | 0.0237
Epoch 285/300, seasonal_0 Loss: 0.0245 | 0.0247
Epoch 286/300, seasonal_0 Loss: 0.0231 | 0.0256
Epoch 287/300, seasonal_0 Loss: 0.0224 | 0.0260
Epoch 288/300, seasonal_0 Loss: 0.0223 | 0.0265
Epoch 289/300, seasonal_0 Loss: 0.0212 | 0.0228
Epoch 290/300, seasonal_0 Loss: 0.0306 | 0.0234
Epoch 291/300, seasonal_0 Loss: 0.0253 | 0.0234
Epoch 292/300, seasonal_0 Loss: 0.0238 | 0.0219
Epoch 293/300, seasonal_0 Loss: 0.0218 | 0.0212
Epoch 294/300, seasonal_0 Loss: 0.0210 | 0.0228
Epoch 295/300, seasonal_0 Loss: 0.0203 | 0.0216
Epoch 296/300, seasonal_0 Loss: 0.0195 | 0.0212
Epoch 297/300, seasonal_0 Loss: 0.0193 | 0.0212
Epoch 298/300, seasonal_0 Loss: 0.0192 | 0.0214
Epoch 299/300, seasonal_0 Loss: 0.0189 | 0.0215
Epoch 300/300, seasonal_0 Loss: 0.0219 | 0.0206
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8133241920636358, 'learning_rate': 0.0006291957902043911, 'batch_size': 177, 'step_size': 11, 'gamma': 0.8984477970285962}
Epoch 1/300, seasonal_1 Loss: 2.1005 | 1.4595
Epoch 2/300, seasonal_1 Loss: 1.0158 | 1.4443
Epoch 3/300, seasonal_1 Loss: 0.9592 | 1.4346
Epoch 4/300, seasonal_1 Loss: 1.2545 | 1.6826
Epoch 5/300, seasonal_1 Loss: 1.0921 | 2.1614
Epoch 6/300, seasonal_1 Loss: 0.8488 | 1.9458
Epoch 7/300, seasonal_1 Loss: 0.8425 | 1.9350
Epoch 8/300, seasonal_1 Loss: 0.8559 | 1.9698
Epoch 9/300, seasonal_1 Loss: 0.8438 | 1.9452
Epoch 10/300, seasonal_1 Loss: 0.8497 | 1.9563
Epoch 11/300, seasonal_1 Loss: 0.8476 | 1.9542
Epoch 12/300, seasonal_1 Loss: 0.8403 | 1.9673
Epoch 13/300, seasonal_1 Loss: 0.8352 | 1.9563
Epoch 14/300, seasonal_1 Loss: 0.8383 | 1.9633
Epoch 15/300, seasonal_1 Loss: 0.8364 | 1.9603
Epoch 16/300, seasonal_1 Loss: 0.8366 | 1.9620
Epoch 17/300, seasonal_1 Loss: 0.8358 | 1.9618
Epoch 18/300, seasonal_1 Loss: 0.8294 | 1.9692
Epoch 19/300, seasonal_1 Loss: 0.8261 | 1.9630
Epoch 20/300, seasonal_1 Loss: 0.8266 | 1.9508
Epoch 21/300, seasonal_1 Loss: 0.8178 | 1.9590
Epoch 22/300, seasonal_1 Loss: 0.8282 | 1.9693
Epoch 23/300, seasonal_1 Loss: 0.8222 | 1.9707
Epoch 24/300, seasonal_1 Loss: 0.8202 | 1.9707
Epoch 25/300, seasonal_1 Loss: 0.8204 | 1.9704
Epoch 26/300, seasonal_1 Loss: 0.8199 | 1.9708
Epoch 27/300, seasonal_1 Loss: 0.8196 | 1.9705
Epoch 28/300, seasonal_1 Loss: 0.8193 | 1.9706
Epoch 29/300, seasonal_1 Loss: 0.8151 | 1.9720
Epoch 30/300, seasonal_1 Loss: 0.8142 | 1.9726
Epoch 31/300, seasonal_1 Loss: 0.8139 | 1.9708
Epoch 32/300, seasonal_1 Loss: 0.8137 | 1.9706
Epoch 33/300, seasonal_1 Loss: 0.8127 | 1.9671
Epoch 34/300, seasonal_1 Loss: 0.8073 | 1.9568
Epoch 35/300, seasonal_1 Loss: 0.7580 | 0.8688
Epoch 36/300, seasonal_1 Loss: 0.8104 | 1.8905
Epoch 37/300, seasonal_1 Loss: 0.8422 | 2.0115
Epoch 38/300, seasonal_1 Loss: 0.8113 | 1.9576
Epoch 39/300, seasonal_1 Loss: 0.8181 | 1.9811
Epoch 40/300, seasonal_1 Loss: 0.8094 | 1.9717
Epoch 41/300, seasonal_1 Loss: 0.8105 | 1.9768
Epoch 42/300, seasonal_1 Loss: 0.8092 | 1.9748
Epoch 43/300, seasonal_1 Loss: 0.8093 | 1.9755
Epoch 44/300, seasonal_1 Loss: 0.8089 | 1.9756
Epoch 45/300, seasonal_1 Loss: 0.8063 | 1.9753
Epoch 46/300, seasonal_1 Loss: 0.8062 | 1.9777
Epoch 47/300, seasonal_1 Loss: 0.8057 | 1.9761
Epoch 48/300, seasonal_1 Loss: 0.8059 | 1.9768
Epoch 49/300, seasonal_1 Loss: 0.8057 | 1.9767
Epoch 50/300, seasonal_1 Loss: 0.8057 | 1.9767
Epoch 51/300, seasonal_1 Loss: 0.8035 | 1.9763
Epoch 52/300, seasonal_1 Loss: 0.8036 | 1.9784
Epoch 53/300, seasonal_1 Loss: 0.8033 | 1.9772
Epoch 54/300, seasonal_1 Loss: 0.8034 | 1.9775
Epoch 55/300, seasonal_1 Loss: 0.8033 | 1.9776
Epoch 56/300, seasonal_1 Loss: 0.8015 | 1.9769
Epoch 57/300, seasonal_1 Loss: 0.8017 | 1.9789
Epoch 58/300, seasonal_1 Loss: 0.8014 | 1.9780
Epoch 59/300, seasonal_1 Loss: 0.8015 | 1.9780
Epoch 60/300, seasonal_1 Loss: 0.8015 | 1.9782
Epoch 61/300, seasonal_1 Loss: 0.8015 | 1.9781
Epoch 62/300, seasonal_1 Loss: 0.7999 | 1.9775
Epoch 63/300, seasonal_1 Loss: 0.8001 | 1.9792
Epoch 64/300, seasonal_1 Loss: 0.7998 | 1.9786
Epoch 65/300, seasonal_1 Loss: 0.7999 | 1.9785
Epoch 66/300, seasonal_1 Loss: 0.7999 | 1.9787
Epoch 67/300, seasonal_1 Loss: 0.7985 | 1.9780
Epoch 68/300, seasonal_1 Loss: 0.7987 | 1.9794
Epoch 69/300, seasonal_1 Loss: 0.7985 | 1.9791
Epoch 70/300, seasonal_1 Loss: 0.7986 | 1.9789
Epoch 71/300, seasonal_1 Loss: 0.7986 | 1.9790
Epoch 72/300, seasonal_1 Loss: 0.7986 | 1.9790
Epoch 73/300, seasonal_1 Loss: 0.7974 | 1.9784
Epoch 74/300, seasonal_1 Loss: 0.7976 | 1.9796
Epoch 75/300, seasonal_1 Loss: 0.7974 | 1.9795
Epoch 76/300, seasonal_1 Loss: 0.7974 | 1.9792
Epoch 77/300, seasonal_1 Loss: 0.7975 | 1.9793
Epoch 78/300, seasonal_1 Loss: 0.7964 | 1.9788
Epoch 79/300, seasonal_1 Loss: 0.7965 | 1.9797
Epoch 80/300, seasonal_1 Loss: 0.7965 | 1.9797
Epoch 81/300, seasonal_1 Loss: 0.7965 | 1.9795
Epoch 82/300, seasonal_1 Loss: 0.7965 | 1.9795
Epoch 83/300, seasonal_1 Loss: 0.7965 | 1.9796
Epoch 84/300, seasonal_1 Loss: 0.7956 | 1.9791
Epoch 85/300, seasonal_1 Loss: 0.7957 | 1.9799
Epoch 86/300, seasonal_1 Loss: 0.7956 | 1.9800
Epoch 87/300, seasonal_1 Loss: 0.7956 | 1.9798
Epoch 88/300, seasonal_1 Loss: 0.7957 | 1.9798
Epoch 89/300, seasonal_1 Loss: 0.7948 | 1.9794
Epoch 90/300, seasonal_1 Loss: 0.7949 | 1.9800
Epoch 91/300, seasonal_1 Loss: 0.7949 | 1.9801
Epoch 92/300, seasonal_1 Loss: 0.7949 | 1.9800
Epoch 93/300, seasonal_1 Loss: 0.7949 | 1.9799
Epoch 94/300, seasonal_1 Loss: 0.7949 | 1.9800
Epoch 95/300, seasonal_1 Loss: 0.7942 | 1.9796
Epoch 96/300, seasonal_1 Loss: 0.7943 | 1.9801
Epoch 97/300, seasonal_1 Loss: 0.7942 | 1.9803
Epoch 98/300, seasonal_1 Loss: 0.7942 | 1.9802
Epoch 99/300, seasonal_1 Loss: 0.7942 | 1.9801
Epoch 100/300, seasonal_1 Loss: 0.7936 | 1.9798
Epoch 101/300, seasonal_1 Loss: 0.7937 | 1.9802
Epoch 102/300, seasonal_1 Loss: 0.7936 | 1.9804
Epoch 103/300, seasonal_1 Loss: 0.7936 | 1.9803
Epoch 104/300, seasonal_1 Loss: 0.7936 | 1.9803
Epoch 105/300, seasonal_1 Loss: 0.7937 | 1.9803
Epoch 106/300, seasonal_1 Loss: 0.7931 | 1.9800
Epoch 107/300, seasonal_1 Loss: 0.7931 | 1.9803
Epoch 108/300, seasonal_1 Loss: 0.7931 | 1.9805
Epoch 109/300, seasonal_1 Loss: 0.7931 | 1.9804
Epoch 110/300, seasonal_1 Loss: 0.7931 | 1.9804
Epoch 111/300, seasonal_1 Loss: 0.7926 | 1.9802
Epoch 112/300, seasonal_1 Loss: 0.7927 | 1.9804
Epoch 113/300, seasonal_1 Loss: 0.7927 | 1.9805
Epoch 114/300, seasonal_1 Loss: 0.7926 | 1.9805
Epoch 115/300, seasonal_1 Loss: 0.7927 | 1.9805
Epoch 116/300, seasonal_1 Loss: 0.7927 | 1.9805
Epoch 117/300, seasonal_1 Loss: 0.7922 | 1.9803
Epoch 118/300, seasonal_1 Loss: 0.7922 | 1.9805
Epoch 119/300, seasonal_1 Loss: 0.7922 | 1.9806
Epoch 120/300, seasonal_1 Loss: 0.7922 | 1.9806
Epoch 121/300, seasonal_1 Loss: 0.7922 | 1.9806
Epoch 122/300, seasonal_1 Loss: 0.7918 | 1.9804
Epoch 123/300, seasonal_1 Loss: 0.7919 | 1.9806
Epoch 124/300, seasonal_1 Loss: 0.7919 | 1.9807
Epoch 125/300, seasonal_1 Loss: 0.7919 | 1.9807
Epoch 126/300, seasonal_1 Loss: 0.7919 | 1.9807
Epoch 127/300, seasonal_1 Loss: 0.7919 | 1.9807
Epoch 128/300, seasonal_1 Loss: 0.7915 | 1.9805
Epoch 129/300, seasonal_1 Loss: 0.7915 | 1.9806
Epoch 130/300, seasonal_1 Loss: 0.7915 | 1.9807
Epoch 131/300, seasonal_1 Loss: 0.7915 | 1.9808
Epoch 132/300, seasonal_1 Loss: 0.7915 | 1.9807
Epoch 133/300, seasonal_1 Loss: 0.7912 | 1.9806
Epoch 134/300, seasonal_1 Loss: 0.7912 | 1.9807
Epoch 135/300, seasonal_1 Loss: 0.7912 | 1.9808
Epoch 136/300, seasonal_1 Loss: 0.7912 | 1.9808
Epoch 137/300, seasonal_1 Loss: 0.7912 | 1.9808
Epoch 138/300, seasonal_1 Loss: 0.7912 | 1.9808
Epoch 139/300, seasonal_1 Loss: 0.7909 | 1.9807
Epoch 140/300, seasonal_1 Loss: 0.7909 | 1.9808
Epoch 141/300, seasonal_1 Loss: 0.7909 | 1.9808
Epoch 142/300, seasonal_1 Loss: 0.7909 | 1.9809
Epoch 143/300, seasonal_1 Loss: 0.7909 | 1.9809
Epoch 144/300, seasonal_1 Loss: 0.7907 | 1.9808
Epoch 145/300, seasonal_1 Loss: 0.7907 | 1.9808
Epoch 146/300, seasonal_1 Loss: 0.7907 | 1.9809
Epoch 147/300, seasonal_1 Loss: 0.7907 | 1.9809
Epoch 148/300, seasonal_1 Loss: 0.7907 | 1.9809
Epoch 149/300, seasonal_1 Loss: 0.7907 | 1.9809
Epoch 150/300, seasonal_1 Loss: 0.7905 | 1.9808
Epoch 151/300, seasonal_1 Loss: 0.7905 | 1.9809
Epoch 152/300, seasonal_1 Loss: 0.7905 | 1.9809
Epoch 153/300, seasonal_1 Loss: 0.7905 | 1.9809
Epoch 154/300, seasonal_1 Loss: 0.7905 | 1.9809
Epoch 155/300, seasonal_1 Loss: 0.7902 | 1.9809
Epoch 156/300, seasonal_1 Loss: 0.7903 | 1.9809
Epoch 157/300, seasonal_1 Loss: 0.7903 | 1.9810
Epoch 158/300, seasonal_1 Loss: 0.7903 | 1.9810
Epoch 159/300, seasonal_1 Loss: 0.7903 | 1.9810
Epoch 160/300, seasonal_1 Loss: 0.7903 | 1.9810
Epoch 161/300, seasonal_1 Loss: 0.7901 | 1.9809
Epoch 162/300, seasonal_1 Loss: 0.7901 | 1.9810
Epoch 163/300, seasonal_1 Loss: 0.7901 | 1.9810
Epoch 164/300, seasonal_1 Loss: 0.7901 | 1.9810
Epoch 165/300, seasonal_1 Loss: 0.7901 | 1.9810
Epoch 166/300, seasonal_1 Loss: 0.7899 | 1.9810
Epoch 167/300, seasonal_1 Loss: 0.7899 | 1.9810
Epoch 168/300, seasonal_1 Loss: 0.7899 | 1.9810
Epoch 169/300, seasonal_1 Loss: 0.7899 | 1.9810
Epoch 170/300, seasonal_1 Loss: 0.7899 | 1.9810
Epoch 171/300, seasonal_1 Loss: 0.7899 | 1.9810
Epoch 172/300, seasonal_1 Loss: 0.7898 | 1.9810
Epoch 173/300, seasonal_1 Loss: 0.7898 | 1.9810
Epoch 174/300, seasonal_1 Loss: 0.7898 | 1.9810
Epoch 175/300, seasonal_1 Loss: 0.7898 | 1.9811
Epoch 176/300, seasonal_1 Loss: 0.7898 | 1.9811
Epoch 177/300, seasonal_1 Loss: 0.7896 | 1.9810
Epoch 178/300, seasonal_1 Loss: 0.7896 | 1.9811
Epoch 179/300, seasonal_1 Loss: 0.7896 | 1.9811
Epoch 180/300, seasonal_1 Loss: 0.7896 | 1.9811
Epoch 181/300, seasonal_1 Loss: 0.7896 | 1.9811
Epoch 182/300, seasonal_1 Loss: 0.7896 | 1.9811
Epoch 183/300, seasonal_1 Loss: 0.7895 | 1.9811
Epoch 184/300, seasonal_1 Loss: 0.7895 | 1.9811
Epoch 185/300, seasonal_1 Loss: 0.7895 | 1.9811
Epoch 186/300, seasonal_1 Loss: 0.7895 | 1.9811
Epoch 187/300, seasonal_1 Loss: 0.7895 | 1.9811
Epoch 188/300, seasonal_1 Loss: 0.7894 | 1.9811
Epoch 189/300, seasonal_1 Loss: 0.7894 | 1.9811
Epoch 190/300, seasonal_1 Loss: 0.7894 | 1.9811
Epoch 191/300, seasonal_1 Loss: 0.7894 | 1.9811
Epoch 192/300, seasonal_1 Loss: 0.7894 | 1.9811
Epoch 193/300, seasonal_1 Loss: 0.7894 | 1.9811
Epoch 194/300, seasonal_1 Loss: 0.7893 | 1.9811
Epoch 195/300, seasonal_1 Loss: 0.7893 | 1.9811
Epoch 196/300, seasonal_1 Loss: 0.7893 | 1.9811
Epoch 197/300, seasonal_1 Loss: 0.7893 | 1.9811
Epoch 198/300, seasonal_1 Loss: 0.7893 | 1.9812
Epoch 199/300, seasonal_1 Loss: 0.7892 | 1.9811
Epoch 200/300, seasonal_1 Loss: 0.7892 | 1.9811
Epoch 201/300, seasonal_1 Loss: 0.7892 | 1.9812
Epoch 202/300, seasonal_1 Loss: 0.7892 | 1.9812
Epoch 203/300, seasonal_1 Loss: 0.7892 | 1.9812
Epoch 204/300, seasonal_1 Loss: 0.7892 | 1.9812
Epoch 205/300, seasonal_1 Loss: 0.7891 | 1.9812
Epoch 206/300, seasonal_1 Loss: 0.7891 | 1.9812
Epoch 207/300, seasonal_1 Loss: 0.7891 | 1.9812
Epoch 208/300, seasonal_1 Loss: 0.7891 | 1.9812
Epoch 209/300, seasonal_1 Loss: 0.7891 | 1.9812
Epoch 210/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 211/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 212/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 213/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 214/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 215/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 216/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 217/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 218/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 219/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 220/300, seasonal_1 Loss: 0.7890 | 1.9812
Epoch 221/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 222/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 223/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 224/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 225/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 226/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 227/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 228/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 229/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 230/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 231/300, seasonal_1 Loss: 0.7889 | 1.9812
Epoch 232/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 233/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 234/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 235/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 236/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 237/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 238/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 239/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 240/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 241/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 242/300, seasonal_1 Loss: 0.7888 | 1.9812
Epoch 243/300, seasonal_1 Loss: 0.7887 | 1.9812
Epoch 244/300, seasonal_1 Loss: 0.7887 | 1.9812
Epoch 245/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 246/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 247/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 248/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 249/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 250/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 251/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 252/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 253/300, seasonal_1 Loss: 0.7887 | 1.9813
Epoch 254/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 255/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 256/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 257/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 258/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 259/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 260/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 261/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 262/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 263/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 264/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 265/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 266/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 267/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 268/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 269/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 270/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 271/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 272/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 273/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 274/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 275/300, seasonal_1 Loss: 0.7886 | 1.9813
Epoch 276/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 277/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 278/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 279/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 280/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 281/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 282/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 283/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 284/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 285/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 286/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 287/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 288/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 289/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 290/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 291/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 292/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 293/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 294/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 295/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 296/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 297/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 298/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 299/300, seasonal_1 Loss: 0.7885 | 1.9813
Epoch 300/300, seasonal_1 Loss: 0.7885 | 1.9813
Training seasonal_2 component with params: {'observation_period_num': 7, 'train_rates': 0.837859298121245, 'learning_rate': 0.0007622493770942043, 'batch_size': 114, 'step_size': 15, 'gamma': 0.9529719798308209}
Epoch 1/300, seasonal_2 Loss: 1.9694 | 1.4202
Epoch 2/300, seasonal_2 Loss: 0.9582 | 1.4153
Epoch 3/300, seasonal_2 Loss: 1.0005 | 1.5338
Epoch 4/300, seasonal_2 Loss: 0.8394 | 1.4515
Epoch 5/300, seasonal_2 Loss: 0.8886 | 1.4600
Epoch 6/300, seasonal_2 Loss: 0.8634 | 1.4399
Epoch 7/300, seasonal_2 Loss: 0.8689 | 1.4402
Epoch 8/300, seasonal_2 Loss: 0.8785 | 1.4263
Epoch 9/300, seasonal_2 Loss: 0.9114 | 1.4120
Epoch 10/300, seasonal_2 Loss: 0.9336 | 1.4162
Epoch 11/300, seasonal_2 Loss: 0.8924 | 1.4389
Epoch 12/300, seasonal_2 Loss: 0.9767 | 1.4184
Epoch 13/300, seasonal_2 Loss: 1.0256 | 1.5284
Epoch 14/300, seasonal_2 Loss: 1.0575 | 1.8849
Epoch 15/300, seasonal_2 Loss: 0.9385 | 1.9874
Epoch 16/300, seasonal_2 Loss: 0.8773 | 1.9467
Epoch 17/300, seasonal_2 Loss: 0.8799 | 1.9528
Epoch 18/300, seasonal_2 Loss: 0.8788 | 1.9598
Epoch 19/300, seasonal_2 Loss: 0.8757 | 1.9614
Epoch 20/300, seasonal_2 Loss: 0.8741 | 1.9649
Epoch 21/300, seasonal_2 Loss: 0.8723 | 1.9679
Epoch 22/300, seasonal_2 Loss: 0.8706 | 1.9707
Epoch 23/300, seasonal_2 Loss: 0.8690 | 1.9733
Epoch 24/300, seasonal_2 Loss: 0.8654 | 1.9858
Epoch 25/300, seasonal_2 Loss: 0.8614 | 1.9826
Epoch 26/300, seasonal_2 Loss: 0.8614 | 1.9858
Epoch 27/300, seasonal_2 Loss: 0.8602 | 1.9870
Epoch 28/300, seasonal_2 Loss: 0.8592 | 1.9887
Epoch 29/300, seasonal_2 Loss: 0.8583 | 1.9902
Epoch 30/300, seasonal_2 Loss: 0.8573 | 1.9918
Epoch 31/300, seasonal_2 Loss: 0.8545 | 1.9998
Epoch 32/300, seasonal_2 Loss: 0.8521 | 1.9980
Epoch 33/300, seasonal_2 Loss: 0.8521 | 2.0000
Epoch 34/300, seasonal_2 Loss: 0.8512 | 2.0007
Epoch 35/300, seasonal_2 Loss: 0.8506 | 2.0018
Epoch 36/300, seasonal_2 Loss: 0.8499 | 2.0028
Epoch 37/300, seasonal_2 Loss: 0.8493 | 2.0038
Epoch 38/300, seasonal_2 Loss: 0.8486 | 2.0047
Epoch 39/300, seasonal_2 Loss: 0.8464 | 2.0100
Epoch 40/300, seasonal_2 Loss: 0.8448 | 2.0093
Epoch 41/300, seasonal_2 Loss: 0.8447 | 2.0104
Epoch 42/300, seasonal_2 Loss: 0.8441 | 2.0109
Epoch 43/300, seasonal_2 Loss: 0.8437 | 2.0116
Epoch 44/300, seasonal_2 Loss: 0.8432 | 2.0123
Epoch 45/300, seasonal_2 Loss: 0.8427 | 2.0130
Epoch 46/300, seasonal_2 Loss: 0.8409 | 2.0166
Epoch 47/300, seasonal_2 Loss: 0.8399 | 2.0166
Epoch 48/300, seasonal_2 Loss: 0.8396 | 2.0170
Epoch 49/300, seasonal_2 Loss: 0.8393 | 2.0176
Epoch 50/300, seasonal_2 Loss: 0.8389 | 2.0180
Epoch 51/300, seasonal_2 Loss: 0.8386 | 2.0185
Epoch 52/300, seasonal_2 Loss: 0.8382 | 2.0190
Epoch 53/300, seasonal_2 Loss: 0.8378 | 2.0195
Epoch 54/300, seasonal_2 Loss: 0.8363 | 2.0219
Epoch 55/300, seasonal_2 Loss: 0.8356 | 2.0224
Epoch 56/300, seasonal_2 Loss: 0.8354 | 2.0225
Epoch 57/300, seasonal_2 Loss: 0.8351 | 2.0230
Epoch 58/300, seasonal_2 Loss: 0.8348 | 2.0233
Epoch 59/300, seasonal_2 Loss: 0.8346 | 2.0236
Epoch 60/300, seasonal_2 Loss: 0.8343 | 2.0240
Epoch 61/300, seasonal_2 Loss: 0.8330 | 2.0257
Epoch 62/300, seasonal_2 Loss: 0.8325 | 2.0264
Epoch 63/300, seasonal_2 Loss: 0.8323 | 2.0263
Epoch 64/300, seasonal_2 Loss: 0.8321 | 2.0267
Epoch 65/300, seasonal_2 Loss: 0.8319 | 2.0269
Epoch 66/300, seasonal_2 Loss: 0.8317 | 2.0272
Epoch 67/300, seasonal_2 Loss: 0.8314 | 2.0274
Epoch 68/300, seasonal_2 Loss: 0.8312 | 2.0277
Epoch 69/300, seasonal_2 Loss: 0.8301 | 2.0289
Epoch 70/300, seasonal_2 Loss: 0.8298 | 2.0297
Epoch 71/300, seasonal_2 Loss: 0.8296 | 2.0295
Epoch 72/300, seasonal_2 Loss: 0.8294 | 2.0298
Epoch 73/300, seasonal_2 Loss: 0.8293 | 2.0300
Epoch 74/300, seasonal_2 Loss: 0.8291 | 2.0302
Epoch 75/300, seasonal_2 Loss: 0.8289 | 2.0304
Epoch 76/300, seasonal_2 Loss: 0.8279 | 2.0312
Epoch 77/300, seasonal_2 Loss: 0.8277 | 2.0320
Epoch 78/300, seasonal_2 Loss: 0.8275 | 2.0319
Epoch 79/300, seasonal_2 Loss: 0.8274 | 2.0321
Epoch 80/300, seasonal_2 Loss: 0.8273 | 2.0322
Epoch 81/300, seasonal_2 Loss: 0.8272 | 2.0324
Epoch 82/300, seasonal_2 Loss: 0.8270 | 2.0325
Epoch 83/300, seasonal_2 Loss: 0.8269 | 2.0327
Epoch 84/300, seasonal_2 Loss: 0.8260 | 2.0333
Epoch 85/300, seasonal_2 Loss: 0.8259 | 2.0340
Epoch 86/300, seasonal_2 Loss: 0.8257 | 2.0339
Epoch 87/300, seasonal_2 Loss: 0.8256 | 2.0340
Epoch 88/300, seasonal_2 Loss: 0.8255 | 2.0342
Epoch 89/300, seasonal_2 Loss: 0.8254 | 2.0343
Epoch 90/300, seasonal_2 Loss: 0.8253 | 2.0344
Epoch 91/300, seasonal_2 Loss: 0.8246 | 2.0348
Epoch 92/300, seasonal_2 Loss: 0.8245 | 2.0355
Epoch 93/300, seasonal_2 Loss: 0.8243 | 2.0355
Epoch 94/300, seasonal_2 Loss: 0.8242 | 2.0355
Epoch 95/300, seasonal_2 Loss: 0.8242 | 2.0356
Epoch 96/300, seasonal_2 Loss: 0.8241 | 2.0357
Epoch 97/300, seasonal_2 Loss: 0.8240 | 2.0358
Epoch 98/300, seasonal_2 Loss: 0.8239 | 2.0359
Epoch 99/300, seasonal_2 Loss: 0.8232 | 2.0362
Epoch 100/300, seasonal_2 Loss: 0.8232 | 2.0368
Epoch 101/300, seasonal_2 Loss: 0.8230 | 2.0368
Epoch 102/300, seasonal_2 Loss: 0.8230 | 2.0368
Epoch 103/300, seasonal_2 Loss: 0.8229 | 2.0369
Epoch 104/300, seasonal_2 Loss: 0.8228 | 2.0370
Epoch 105/300, seasonal_2 Loss: 0.8228 | 2.0371
Epoch 106/300, seasonal_2 Loss: 0.8222 | 2.0373
Epoch 107/300, seasonal_2 Loss: 0.8221 | 2.0379
Epoch 108/300, seasonal_2 Loss: 0.8220 | 2.0379
Epoch 109/300, seasonal_2 Loss: 0.8220 | 2.0379
Epoch 110/300, seasonal_2 Loss: 0.8219 | 2.0379
Epoch 111/300, seasonal_2 Loss: 0.8219 | 2.0380
Epoch 112/300, seasonal_2 Loss: 0.8218 | 2.0380
Epoch 113/300, seasonal_2 Loss: 0.8217 | 2.0381
Epoch 114/300, seasonal_2 Loss: 0.8212 | 2.0383
Epoch 115/300, seasonal_2 Loss: 0.8212 | 2.0388
Epoch 116/300, seasonal_2 Loss: 0.8211 | 2.0388
Epoch 117/300, seasonal_2 Loss: 0.8211 | 2.0388
Epoch 118/300, seasonal_2 Loss: 0.8210 | 2.0388
Epoch 119/300, seasonal_2 Loss: 0.8210 | 2.0389
Epoch 120/300, seasonal_2 Loss: 0.8209 | 2.0389
Epoch 121/300, seasonal_2 Loss: 0.8205 | 2.0391
Epoch 122/300, seasonal_2 Loss: 0.8204 | 2.0395
Epoch 123/300, seasonal_2 Loss: 0.8204 | 2.0395
Epoch 124/300, seasonal_2 Loss: 0.8203 | 2.0395
Epoch 125/300, seasonal_2 Loss: 0.8203 | 2.0396
Epoch 126/300, seasonal_2 Loss: 0.8202 | 2.0396
Epoch 127/300, seasonal_2 Loss: 0.8202 | 2.0396
Epoch 128/300, seasonal_2 Loss: 0.8202 | 2.0397
Epoch 129/300, seasonal_2 Loss: 0.8197 | 2.0398
Epoch 130/300, seasonal_2 Loss: 0.8197 | 2.0401
Epoch 131/300, seasonal_2 Loss: 0.8197 | 2.0402
Epoch 132/300, seasonal_2 Loss: 0.8196 | 2.0402
Epoch 133/300, seasonal_2 Loss: 0.8196 | 2.0402
Epoch 134/300, seasonal_2 Loss: 0.8196 | 2.0402
Epoch 135/300, seasonal_2 Loss: 0.8195 | 2.0403
Epoch 136/300, seasonal_2 Loss: 0.8192 | 2.0403
Epoch 137/300, seasonal_2 Loss: 0.8191 | 2.0407
Epoch 138/300, seasonal_2 Loss: 0.8191 | 2.0407
Epoch 139/300, seasonal_2 Loss: 0.8191 | 2.0407
Epoch 140/300, seasonal_2 Loss: 0.8190 | 2.0408
Epoch 141/300, seasonal_2 Loss: 0.8190 | 2.0408
Epoch 142/300, seasonal_2 Loss: 0.8190 | 2.0408
Epoch 143/300, seasonal_2 Loss: 0.8189 | 2.0408
Epoch 144/300, seasonal_2 Loss: 0.8186 | 2.0409
Epoch 145/300, seasonal_2 Loss: 0.8186 | 2.0412
Epoch 146/300, seasonal_2 Loss: 0.8186 | 2.0412
Epoch 147/300, seasonal_2 Loss: 0.8185 | 2.0412
Epoch 148/300, seasonal_2 Loss: 0.8185 | 2.0412
Epoch 149/300, seasonal_2 Loss: 0.8185 | 2.0413
Epoch 150/300, seasonal_2 Loss: 0.8185 | 2.0413
Epoch 151/300, seasonal_2 Loss: 0.8181 | 2.0413
Epoch 152/300, seasonal_2 Loss: 0.8181 | 2.0416
Epoch 153/300, seasonal_2 Loss: 0.8181 | 2.0417
Epoch 154/300, seasonal_2 Loss: 0.8181 | 2.0417
Epoch 155/300, seasonal_2 Loss: 0.8181 | 2.0417
Epoch 156/300, seasonal_2 Loss: 0.8180 | 2.0417
Epoch 157/300, seasonal_2 Loss: 0.8180 | 2.0417
Epoch 158/300, seasonal_2 Loss: 0.8180 | 2.0417
Epoch 159/300, seasonal_2 Loss: 0.8177 | 2.0417
Epoch 160/300, seasonal_2 Loss: 0.8177 | 2.0420
Epoch 161/300, seasonal_2 Loss: 0.8177 | 2.0420
Epoch 162/300, seasonal_2 Loss: 0.8177 | 2.0420
Epoch 163/300, seasonal_2 Loss: 0.8176 | 2.0421
Epoch 164/300, seasonal_2 Loss: 0.8176 | 2.0421
Epoch 165/300, seasonal_2 Loss: 0.8176 | 2.0421
Epoch 166/300, seasonal_2 Loss: 0.8174 | 2.0421
Epoch 167/300, seasonal_2 Loss: 0.8174 | 2.0423
Epoch 168/300, seasonal_2 Loss: 0.8173 | 2.0424
Epoch 169/300, seasonal_2 Loss: 0.8173 | 2.0424
Epoch 170/300, seasonal_2 Loss: 0.8173 | 2.0424
Epoch 171/300, seasonal_2 Loss: 0.8173 | 2.0424
Epoch 172/300, seasonal_2 Loss: 0.8173 | 2.0424
Epoch 173/300, seasonal_2 Loss: 0.8173 | 2.0424
Epoch 174/300, seasonal_2 Loss: 0.8170 | 2.0424
Epoch 175/300, seasonal_2 Loss: 0.8170 | 2.0426
Epoch 176/300, seasonal_2 Loss: 0.8170 | 2.0427
Epoch 177/300, seasonal_2 Loss: 0.8170 | 2.0427
Epoch 178/300, seasonal_2 Loss: 0.8170 | 2.0427
Epoch 179/300, seasonal_2 Loss: 0.8169 | 2.0427
Epoch 180/300, seasonal_2 Loss: 0.8169 | 2.0427
Epoch 181/300, seasonal_2 Loss: 0.8167 | 2.0427
Epoch 182/300, seasonal_2 Loss: 0.8167 | 2.0429
Epoch 183/300, seasonal_2 Loss: 0.8167 | 2.0429
Epoch 184/300, seasonal_2 Loss: 0.8167 | 2.0430
Epoch 185/300, seasonal_2 Loss: 0.8167 | 2.0430
Epoch 186/300, seasonal_2 Loss: 0.8167 | 2.0430
Epoch 187/300, seasonal_2 Loss: 0.8166 | 2.0430
Epoch 188/300, seasonal_2 Loss: 0.8166 | 2.0430
Epoch 189/300, seasonal_2 Loss: 0.8164 | 2.0430
Epoch 190/300, seasonal_2 Loss: 0.8164 | 2.0431
Epoch 191/300, seasonal_2 Loss: 0.8164 | 2.0432
Epoch 192/300, seasonal_2 Loss: 0.8164 | 2.0432
Epoch 193/300, seasonal_2 Loss: 0.8164 | 2.0432
Epoch 194/300, seasonal_2 Loss: 0.8164 | 2.0432
Epoch 195/300, seasonal_2 Loss: 0.8164 | 2.0432
Epoch 196/300, seasonal_2 Loss: 0.8162 | 2.0432
Epoch 197/300, seasonal_2 Loss: 0.8162 | 2.0433
Epoch 198/300, seasonal_2 Loss: 0.8162 | 2.0434
Epoch 199/300, seasonal_2 Loss: 0.8162 | 2.0434
Epoch 200/300, seasonal_2 Loss: 0.8162 | 2.0434
Epoch 201/300, seasonal_2 Loss: 0.8161 | 2.0434
Epoch 202/300, seasonal_2 Loss: 0.8161 | 2.0434
Epoch 203/300, seasonal_2 Loss: 0.8161 | 2.0434
Epoch 204/300, seasonal_2 Loss: 0.8159 | 2.0434
Epoch 205/300, seasonal_2 Loss: 0.8160 | 2.0436
Epoch 206/300, seasonal_2 Loss: 0.8159 | 2.0436
Epoch 207/300, seasonal_2 Loss: 0.8159 | 2.0436
Epoch 208/300, seasonal_2 Loss: 0.8159 | 2.0436
Epoch 209/300, seasonal_2 Loss: 0.8159 | 2.0436
Epoch 210/300, seasonal_2 Loss: 0.8159 | 2.0436
Epoch 211/300, seasonal_2 Loss: 0.8157 | 2.0436
Epoch 212/300, seasonal_2 Loss: 0.8157 | 2.0437
Epoch 213/300, seasonal_2 Loss: 0.8157 | 2.0438
Epoch 214/300, seasonal_2 Loss: 0.8157 | 2.0438
Epoch 215/300, seasonal_2 Loss: 0.8157 | 2.0438
Epoch 216/300, seasonal_2 Loss: 0.8157 | 2.0438
Epoch 217/300, seasonal_2 Loss: 0.8157 | 2.0438
Epoch 218/300, seasonal_2 Loss: 0.8157 | 2.0438
Epoch 219/300, seasonal_2 Loss: 0.8155 | 2.0438
Epoch 220/300, seasonal_2 Loss: 0.8155 | 2.0439
Epoch 221/300, seasonal_2 Loss: 0.8155 | 2.0440
Epoch 222/300, seasonal_2 Loss: 0.8155 | 2.0440
Epoch 223/300, seasonal_2 Loss: 0.8155 | 2.0440
Epoch 224/300, seasonal_2 Loss: 0.8155 | 2.0440
Epoch 225/300, seasonal_2 Loss: 0.8155 | 2.0440
Epoch 226/300, seasonal_2 Loss: 0.8154 | 2.0440
Epoch 227/300, seasonal_2 Loss: 0.8154 | 2.0441
Epoch 228/300, seasonal_2 Loss: 0.8154 | 2.0441
Epoch 229/300, seasonal_2 Loss: 0.8154 | 2.0442
Epoch 230/300, seasonal_2 Loss: 0.8154 | 2.0442
Epoch 231/300, seasonal_2 Loss: 0.8154 | 2.0442
Epoch 232/300, seasonal_2 Loss: 0.8153 | 2.0442
Epoch 233/300, seasonal_2 Loss: 0.8153 | 2.0442
Epoch 234/300, seasonal_2 Loss: 0.8152 | 2.0442
Epoch 235/300, seasonal_2 Loss: 0.8152 | 2.0442
Epoch 236/300, seasonal_2 Loss: 0.8152 | 2.0443
Epoch 237/300, seasonal_2 Loss: 0.8152 | 2.0443
Epoch 238/300, seasonal_2 Loss: 0.8152 | 2.0443
Epoch 239/300, seasonal_2 Loss: 0.8152 | 2.0443
Epoch 240/300, seasonal_2 Loss: 0.8152 | 2.0443
Epoch 241/300, seasonal_2 Loss: 0.8151 | 2.0443
Epoch 242/300, seasonal_2 Loss: 0.8151 | 2.0444
Epoch 243/300, seasonal_2 Loss: 0.8151 | 2.0444
Epoch 244/300, seasonal_2 Loss: 0.8150 | 2.0445
Epoch 245/300, seasonal_2 Loss: 0.8150 | 2.0445
Epoch 246/300, seasonal_2 Loss: 0.8150 | 2.0445
Epoch 247/300, seasonal_2 Loss: 0.8150 | 2.0445
Epoch 248/300, seasonal_2 Loss: 0.8150 | 2.0445
Epoch 249/300, seasonal_2 Loss: 0.8149 | 2.0445
Epoch 250/300, seasonal_2 Loss: 0.8149 | 2.0445
Epoch 251/300, seasonal_2 Loss: 0.8149 | 2.0446
Epoch 252/300, seasonal_2 Loss: 0.8149 | 2.0446
Epoch 253/300, seasonal_2 Loss: 0.8149 | 2.0446
Epoch 254/300, seasonal_2 Loss: 0.8149 | 2.0446
Epoch 255/300, seasonal_2 Loss: 0.8149 | 2.0446
Epoch 256/300, seasonal_2 Loss: 0.8148 | 2.0446
Epoch 257/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 258/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 259/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 260/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 261/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 262/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 263/300, seasonal_2 Loss: 0.8148 | 2.0447
Epoch 264/300, seasonal_2 Loss: 0.8147 | 2.0447
Epoch 265/300, seasonal_2 Loss: 0.8147 | 2.0448
Epoch 266/300, seasonal_2 Loss: 0.8147 | 2.0448
Epoch 267/300, seasonal_2 Loss: 0.8147 | 2.0449
Epoch 268/300, seasonal_2 Loss: 0.8146 | 2.0449
Epoch 269/300, seasonal_2 Loss: 0.8146 | 2.0449
Epoch 270/300, seasonal_2 Loss: 0.8146 | 2.0449
Epoch 271/300, seasonal_2 Loss: 0.8145 | 2.0449
Epoch 272/300, seasonal_2 Loss: 0.8145 | 2.0449
Epoch 273/300, seasonal_2 Loss: 0.8145 | 2.0449
Epoch 274/300, seasonal_2 Loss: 0.8145 | 2.0450
Epoch 275/300, seasonal_2 Loss: 0.8145 | 2.0450
Epoch 276/300, seasonal_2 Loss: 0.8145 | 2.0450
Epoch 277/300, seasonal_2 Loss: 0.8145 | 2.0450
Epoch 278/300, seasonal_2 Loss: 0.8145 | 2.0450
Epoch 279/300, seasonal_2 Loss: 0.8144 | 2.0450
Epoch 280/300, seasonal_2 Loss: 0.8144 | 2.0450
Epoch 281/300, seasonal_2 Loss: 0.8144 | 2.0451
Epoch 282/300, seasonal_2 Loss: 0.8144 | 2.0451
Epoch 283/300, seasonal_2 Loss: 0.8144 | 2.0451
Epoch 284/300, seasonal_2 Loss: 0.8144 | 2.0451
Epoch 285/300, seasonal_2 Loss: 0.8144 | 2.0451
Epoch 286/300, seasonal_2 Loss: 0.8143 | 2.0451
Epoch 287/300, seasonal_2 Loss: 0.8143 | 2.0451
Epoch 288/300, seasonal_2 Loss: 0.8143 | 2.0452
Epoch 289/300, seasonal_2 Loss: 0.8143 | 2.0452
Epoch 290/300, seasonal_2 Loss: 0.8143 | 2.0452
Epoch 291/300, seasonal_2 Loss: 0.8143 | 2.0452
Epoch 292/300, seasonal_2 Loss: 0.8143 | 2.0452
Epoch 293/300, seasonal_2 Loss: 0.8143 | 2.0452
Epoch 294/300, seasonal_2 Loss: 0.8142 | 2.0452
Epoch 295/300, seasonal_2 Loss: 0.8142 | 2.0452
Epoch 296/300, seasonal_2 Loss: 0.8142 | 2.0449
Epoch 297/300, seasonal_2 Loss: 0.8194 | 2.0435
Epoch 298/300, seasonal_2 Loss: 0.8148 | 2.0434
Epoch 299/300, seasonal_2 Loss: 0.8147 | 2.0437
Epoch 300/300, seasonal_2 Loss: 0.8146 | 2.0441
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.9880329351848851, 'learning_rate': 0.0009523244689686775, 'batch_size': 75, 'step_size': 15, 'gamma': 0.9787461474248159}
Epoch 1/300, seasonal_3 Loss: 1.9901 | 1.7546
Epoch 2/300, seasonal_3 Loss: 1.0097 | 1.7609
Epoch 3/300, seasonal_3 Loss: 0.9871 | 1.7601
Epoch 4/300, seasonal_3 Loss: 1.0034 | 1.7177
Epoch 5/300, seasonal_3 Loss: 1.0744 | 1.7265
Epoch 6/300, seasonal_3 Loss: 1.0838 | 1.7093
Epoch 7/300, seasonal_3 Loss: 1.0226 | 1.7124
Epoch 8/300, seasonal_3 Loss: 0.9805 | 1.7087
Epoch 9/300, seasonal_3 Loss: 1.0395 | 1.7093
Epoch 10/300, seasonal_3 Loss: 1.0708 | 1.7098
Epoch 11/300, seasonal_3 Loss: 1.1414 | 1.7086
Epoch 12/300, seasonal_3 Loss: 1.2123 | 1.7365
Epoch 13/300, seasonal_3 Loss: 1.2252 | 1.8539
Epoch 14/300, seasonal_3 Loss: 1.1765 | 1.9794
Epoch 15/300, seasonal_3 Loss: 1.1235 | 2.0410
Epoch 16/300, seasonal_3 Loss: 1.0959 | 2.0820
Epoch 17/300, seasonal_3 Loss: 1.0791 | 2.1041
Epoch 18/300, seasonal_3 Loss: 1.0694 | 2.1239
Epoch 19/300, seasonal_3 Loss: 1.0609 | 2.1405
Epoch 20/300, seasonal_3 Loss: 1.0537 | 2.1547
Epoch 21/300, seasonal_3 Loss: 1.0474 | 2.1669
Epoch 22/300, seasonal_3 Loss: 1.0418 | 2.1777
Epoch 23/300, seasonal_3 Loss: 1.0368 | 2.1871
Epoch 24/300, seasonal_3 Loss: 1.0316 | 2.1977
Epoch 25/300, seasonal_3 Loss: 1.0272 | 2.2045
Epoch 26/300, seasonal_3 Loss: 1.0239 | 2.2106
Epoch 27/300, seasonal_3 Loss: 1.0207 | 2.2162
Epoch 28/300, seasonal_3 Loss: 1.0178 | 2.2212
Epoch 29/300, seasonal_3 Loss: 1.0152 | 2.2258
Epoch 30/300, seasonal_3 Loss: 1.0127 | 2.2300
Epoch 31/300, seasonal_3 Loss: 1.0099 | 2.2345
Epoch 32/300, seasonal_3 Loss: 1.0077 | 2.2371
Epoch 33/300, seasonal_3 Loss: 1.0072 | 2.2244
Epoch 34/300, seasonal_3 Loss: 1.0089 | 2.2405
Epoch 35/300, seasonal_3 Loss: 1.0039 | 2.2448
Epoch 36/300, seasonal_3 Loss: 1.0017 | 2.2474
Epoch 37/300, seasonal_3 Loss: 1.0000 | 2.2496
Epoch 38/300, seasonal_3 Loss: 0.9986 | 2.2515
Epoch 39/300, seasonal_3 Loss: 0.9971 | 2.2533
Epoch 40/300, seasonal_3 Loss: 0.9960 | 2.2549
Epoch 41/300, seasonal_3 Loss: 0.9950 | 2.2561
Epoch 42/300, seasonal_3 Loss: 0.9941 | 2.2571
Epoch 43/300, seasonal_3 Loss: 0.9932 | 2.2581
Epoch 44/300, seasonal_3 Loss: 0.9923 | 2.2591
Epoch 45/300, seasonal_3 Loss: 0.9916 | 2.2599
Epoch 46/300, seasonal_3 Loss: 0.9907 | 2.2606
Epoch 47/300, seasonal_3 Loss: 0.9901 | 2.2613
Epoch 48/300, seasonal_3 Loss: 0.9896 | 2.2618
Epoch 49/300, seasonal_3 Loss: 0.9891 | 2.2621
Epoch 50/300, seasonal_3 Loss: 0.9887 | 2.2622
Epoch 51/300, seasonal_3 Loss: 0.9886 | 2.2621
Epoch 52/300, seasonal_3 Loss: 0.9888 | 2.2617
Epoch 53/300, seasonal_3 Loss: 0.9889 | 2.2610
Epoch 54/300, seasonal_3 Loss: 0.9886 | 2.2603
Epoch 55/300, seasonal_3 Loss: 0.9886 | 2.2597
Epoch 56/300, seasonal_3 Loss: 0.9886 | 2.2592
Epoch 57/300, seasonal_3 Loss: 0.9887 | 2.2589
Epoch 58/300, seasonal_3 Loss: 0.9883 | 2.2583
Epoch 59/300, seasonal_3 Loss: 0.9883 | 2.2580
Epoch 60/300, seasonal_3 Loss: 0.9882 | 2.2580
Epoch 61/300, seasonal_3 Loss: 0.9875 | 2.2579
Epoch 62/300, seasonal_3 Loss: 0.9871 | 2.2577
Epoch 63/300, seasonal_3 Loss: 0.9870 | 2.2582
Epoch 64/300, seasonal_3 Loss: 0.9867 | 2.2582
Epoch 65/300, seasonal_3 Loss: 0.9865 | 2.2586
Epoch 66/300, seasonal_3 Loss: 0.9862 | 2.2588
Epoch 67/300, seasonal_3 Loss: 0.9859 | 2.2591
Epoch 68/300, seasonal_3 Loss: 0.9857 | 2.2594
Epoch 69/300, seasonal_3 Loss: 0.9856 | 2.2599
Epoch 70/300, seasonal_3 Loss: 0.9856 | 2.2602
Epoch 71/300, seasonal_3 Loss: 0.9858 | 2.2606
Epoch 72/300, seasonal_3 Loss: 0.9860 | 2.2608
Epoch 73/300, seasonal_3 Loss: 0.9864 | 2.2610
Epoch 74/300, seasonal_3 Loss: 0.9870 | 2.2610
Epoch 75/300, seasonal_3 Loss: 0.9879 | 2.2611
Epoch 76/300, seasonal_3 Loss: 0.9883 | 2.2610
Epoch 77/300, seasonal_3 Loss: 0.9882 | 2.2606
Epoch 78/300, seasonal_3 Loss: 0.9877 | 2.2601
Epoch 79/300, seasonal_3 Loss: 0.9871 | 2.2599
Epoch 80/300, seasonal_3 Loss: 0.9863 | 2.2596
Epoch 81/300, seasonal_3 Loss: 0.9859 | 2.2598
Epoch 82/300, seasonal_3 Loss: 0.9858 | 2.2603
Epoch 83/300, seasonal_3 Loss: 0.9860 | 2.2609
Epoch 84/300, seasonal_3 Loss: 0.9860 | 2.2615
Epoch 85/300, seasonal_3 Loss: 0.9859 | 2.2620
Epoch 86/300, seasonal_3 Loss: 0.9858 | 2.2624
Epoch 87/300, seasonal_3 Loss: 0.9857 | 2.2628
Epoch 88/300, seasonal_3 Loss: 0.9857 | 2.2632
Epoch 89/300, seasonal_3 Loss: 0.9858 | 2.2636
Epoch 90/300, seasonal_3 Loss: 0.9863 | 2.2641
Epoch 91/300, seasonal_3 Loss: 0.9870 | 2.2646
Epoch 92/300, seasonal_3 Loss: 0.9878 | 2.2648
Epoch 93/300, seasonal_3 Loss: 0.9888 | 2.2647
Epoch 94/300, seasonal_3 Loss: 0.9900 | 2.2645
Epoch 95/300, seasonal_3 Loss: 0.9910 | 2.2640
Epoch 96/300, seasonal_3 Loss: 0.9915 | 2.2637
Epoch 97/300, seasonal_3 Loss: 0.9908 | 2.2626
Epoch 98/300, seasonal_3 Loss: 0.9898 | 2.2619
Epoch 99/300, seasonal_3 Loss: 0.9881 | 2.2609
Epoch 100/300, seasonal_3 Loss: 0.9870 | 2.2602
Epoch 101/300, seasonal_3 Loss: 0.9866 | 2.2603
Epoch 102/300, seasonal_3 Loss: 0.9861 | 2.2606
Epoch 103/300, seasonal_3 Loss: 0.9860 | 2.2609
Epoch 104/300, seasonal_3 Loss: 0.9868 | 2.2615
Epoch 105/300, seasonal_3 Loss: 0.9883 | 2.2624
Epoch 106/300, seasonal_3 Loss: 0.9891 | 2.2631
Epoch 107/300, seasonal_3 Loss: 0.9888 | 2.2632
Epoch 108/300, seasonal_3 Loss: 0.9878 | 2.2626
Epoch 109/300, seasonal_3 Loss: 0.9871 | 2.2620
Epoch 110/300, seasonal_3 Loss: 0.9870 | 2.2622
Epoch 111/300, seasonal_3 Loss: 0.9869 | 2.2623
Epoch 112/300, seasonal_3 Loss: 0.9875 | 2.2624
Epoch 113/300, seasonal_3 Loss: 0.9888 | 2.2627
Epoch 114/300, seasonal_3 Loss: 0.9900 | 2.2633
Epoch 115/300, seasonal_3 Loss: 0.9899 | 2.2631
Epoch 116/300, seasonal_3 Loss: 0.9893 | 2.2626
Epoch 117/300, seasonal_3 Loss: 0.9885 | 2.2616
Epoch 118/300, seasonal_3 Loss: 0.9882 | 2.2615
Epoch 119/300, seasonal_3 Loss: 0.9879 | 2.2613
Epoch 120/300, seasonal_3 Loss: 0.9876 | 2.2614
Epoch 121/300, seasonal_3 Loss: 0.9869 | 2.2615
Epoch 122/300, seasonal_3 Loss: 0.9861 | 2.2614
Epoch 123/300, seasonal_3 Loss: 0.9856 | 2.2615
Epoch 124/300, seasonal_3 Loss: 0.9854 | 2.2621
Epoch 125/300, seasonal_3 Loss: 0.9856 | 2.2627
Epoch 126/300, seasonal_3 Loss: 0.9859 | 2.2634
Epoch 127/300, seasonal_3 Loss: 0.9864 | 2.2640
Epoch 128/300, seasonal_3 Loss: 0.9871 | 2.2644
Epoch 129/300, seasonal_3 Loss: 0.9877 | 2.2649
Epoch 130/300, seasonal_3 Loss: 0.9880 | 2.2649
Epoch 131/300, seasonal_3 Loss: 0.9885 | 2.2648
Epoch 132/300, seasonal_3 Loss: 0.9888 | 2.2646
Epoch 133/300, seasonal_3 Loss: 0.9891 | 2.2644
Epoch 134/300, seasonal_3 Loss: 0.9893 | 2.2639
Epoch 135/300, seasonal_3 Loss: 0.9897 | 2.2635
Epoch 136/300, seasonal_3 Loss: 0.9898 | 2.2633
Epoch 137/300, seasonal_3 Loss: 0.9896 | 2.2628
Epoch 138/300, seasonal_3 Loss: 0.9891 | 2.2622
Epoch 139/300, seasonal_3 Loss: 0.9890 | 2.2618
Epoch 140/300, seasonal_3 Loss: 0.9888 | 2.2616
Epoch 141/300, seasonal_3 Loss: 0.9882 | 2.2614
Epoch 142/300, seasonal_3 Loss: 0.9876 | 2.2610
Epoch 143/300, seasonal_3 Loss: 0.9873 | 2.2610
Epoch 144/300, seasonal_3 Loss: 0.9873 | 2.2613
Epoch 145/300, seasonal_3 Loss: 0.9872 | 2.2614
Epoch 146/300, seasonal_3 Loss: 0.9875 | 2.2615
Epoch 147/300, seasonal_3 Loss: 0.9885 | 2.2617
Epoch 148/300, seasonal_3 Loss: 0.9897 | 2.2620
Epoch 149/300, seasonal_3 Loss: 0.9907 | 2.2619
Epoch 150/300, seasonal_3 Loss: 0.9912 | 2.2616
Epoch 151/300, seasonal_3 Loss: 0.9907 | 2.2615
Epoch 152/300, seasonal_3 Loss: 0.9893 | 2.2606
Epoch 153/300, seasonal_3 Loss: 0.9879 | 2.2597
Epoch 154/300, seasonal_3 Loss: 0.9868 | 2.2593
Epoch 155/300, seasonal_3 Loss: 0.9862 | 2.2593
Epoch 156/300, seasonal_3 Loss: 0.9859 | 2.2596
Epoch 157/300, seasonal_3 Loss: 0.9859 | 2.2602
Epoch 158/300, seasonal_3 Loss: 0.9862 | 2.2608
Epoch 159/300, seasonal_3 Loss: 0.9866 | 2.2615
Epoch 160/300, seasonal_3 Loss: 0.9873 | 2.2620
Epoch 161/300, seasonal_3 Loss: 0.9885 | 2.2624
Epoch 162/300, seasonal_3 Loss: 0.9899 | 2.2625
Epoch 163/300, seasonal_3 Loss: 0.9913 | 2.2624
Epoch 164/300, seasonal_3 Loss: 0.9923 | 2.2620
Epoch 165/300, seasonal_3 Loss: 0.9926 | 2.2613
Epoch 166/300, seasonal_3 Loss: 0.9920 | 2.2608
Epoch 167/300, seasonal_3 Loss: 0.9906 | 2.2597
Epoch 168/300, seasonal_3 Loss: 0.9892 | 2.2588
Epoch 169/300, seasonal_3 Loss: 0.9879 | 2.2581
Epoch 170/300, seasonal_3 Loss: 0.9870 | 2.2577
Epoch 171/300, seasonal_3 Loss: 0.9865 | 2.2579
Epoch 172/300, seasonal_3 Loss: 0.9863 | 2.2583
Epoch 173/300, seasonal_3 Loss: 0.9862 | 2.2589
Epoch 174/300, seasonal_3 Loss: 0.9860 | 2.2595
Epoch 175/300, seasonal_3 Loss: 0.9858 | 2.2599
Epoch 176/300, seasonal_3 Loss: 0.9857 | 2.2605
Epoch 177/300, seasonal_3 Loss: 0.9858 | 2.2611
Epoch 178/300, seasonal_3 Loss: 0.9862 | 2.2617
Epoch 179/300, seasonal_3 Loss: 0.9868 | 2.2622
Epoch 180/300, seasonal_3 Loss: 0.9880 | 2.2625
Epoch 181/300, seasonal_3 Loss: 0.9896 | 2.2632
Epoch 182/300, seasonal_3 Loss: 0.9909 | 2.2629
Epoch 183/300, seasonal_3 Loss: 0.9922 | 2.2626
Epoch 184/300, seasonal_3 Loss: 0.9931 | 2.2615
Epoch 185/300, seasonal_3 Loss: 0.9940 | 2.2607
Epoch 186/300, seasonal_3 Loss: 0.9943 | 2.2596
Epoch 187/300, seasonal_3 Loss: 0.9942 | 2.2586
Epoch 188/300, seasonal_3 Loss: 0.9934 | 2.2577
Epoch 189/300, seasonal_3 Loss: 0.9912 | 2.2572
Epoch 190/300, seasonal_3 Loss: 0.9886 | 2.2556
Epoch 191/300, seasonal_3 Loss: 0.9865 | 2.2545
Epoch 192/300, seasonal_3 Loss: 0.9852 | 2.2543
Epoch 193/300, seasonal_3 Loss: 0.9845 | 2.2547
Epoch 194/300, seasonal_3 Loss: 0.9842 | 2.2556
Epoch 195/300, seasonal_3 Loss: 0.9841 | 2.2567
Epoch 196/300, seasonal_3 Loss: 0.9840 | 2.2578
Epoch 197/300, seasonal_3 Loss: 0.9840 | 2.2588
Epoch 198/300, seasonal_3 Loss: 0.9840 | 2.2598
Epoch 199/300, seasonal_3 Loss: 0.9842 | 2.2608
Epoch 200/300, seasonal_3 Loss: 0.9846 | 2.2618
Epoch 201/300, seasonal_3 Loss: 0.9855 | 2.2627
Epoch 202/300, seasonal_3 Loss: 0.9872 | 2.2635
Epoch 203/300, seasonal_3 Loss: 0.9896 | 2.2641
Epoch 204/300, seasonal_3 Loss: 0.9921 | 2.2622
Epoch 205/300, seasonal_3 Loss: 0.9954 | 2.2638
Epoch 206/300, seasonal_3 Loss: 0.9958 | 2.2605
Epoch 207/300, seasonal_3 Loss: 0.9971 | 2.2562
Epoch 208/300, seasonal_3 Loss: 0.9990 | 2.2593
Epoch 209/300, seasonal_3 Loss: 0.9960 | 2.2586
Epoch 210/300, seasonal_3 Loss: 0.9915 | 2.2564
Epoch 211/300, seasonal_3 Loss: 0.9869 | 2.2530
Epoch 212/300, seasonal_3 Loss: 0.9847 | 2.2513
Epoch 213/300, seasonal_3 Loss: 0.9841 | 2.2519
Epoch 214/300, seasonal_3 Loss: 0.9840 | 2.2531
Epoch 215/300, seasonal_3 Loss: 0.9840 | 2.2544
Epoch 216/300, seasonal_3 Loss: 0.9840 | 2.2556
Epoch 217/300, seasonal_3 Loss: 0.9839 | 2.2568
Epoch 218/300, seasonal_3 Loss: 0.9839 | 2.2579
Epoch 219/300, seasonal_3 Loss: 0.9838 | 2.2589
Epoch 220/300, seasonal_3 Loss: 0.9838 | 2.2598
Epoch 221/300, seasonal_3 Loss: 0.9838 | 2.2608
Epoch 222/300, seasonal_3 Loss: 0.9837 | 2.2616
Epoch 223/300, seasonal_3 Loss: 0.9837 | 2.2624
Epoch 224/300, seasonal_3 Loss: 0.9838 | 2.2632
Epoch 225/300, seasonal_3 Loss: 0.9840 | 2.2640
Epoch 226/300, seasonal_3 Loss: 0.9846 | 2.2648
Epoch 227/300, seasonal_3 Loss: 0.9859 | 2.2656
Epoch 228/300, seasonal_3 Loss: 0.9879 | 2.2663
Epoch 229/300, seasonal_3 Loss: 0.9901 | 2.2672
Epoch 230/300, seasonal_3 Loss: 0.9912 | 2.2668
Epoch 231/300, seasonal_3 Loss: 0.9922 | 2.2660
Epoch 232/300, seasonal_3 Loss: 0.9928 | 2.2653
Epoch 233/300, seasonal_3 Loss: 0.9931 | 2.2644
Epoch 234/300, seasonal_3 Loss: 0.9923 | 2.2642
Epoch 235/300, seasonal_3 Loss: 0.9898 | 2.2631
Epoch 236/300, seasonal_3 Loss: 0.9869 | 2.2610
Epoch 237/300, seasonal_3 Loss: 0.9850 | 2.2596
Epoch 238/300, seasonal_3 Loss: 0.9843 | 2.2596
Epoch 239/300, seasonal_3 Loss: 0.9841 | 2.2602
Epoch 240/300, seasonal_3 Loss: 0.9839 | 2.2609
Epoch 241/300, seasonal_3 Loss: 0.9837 | 2.2616
Epoch 242/300, seasonal_3 Loss: 0.9836 | 2.2623
Epoch 243/300, seasonal_3 Loss: 0.9836 | 2.2630
Epoch 244/300, seasonal_3 Loss: 0.9835 | 2.2637
Epoch 245/300, seasonal_3 Loss: 0.9835 | 2.2643
Epoch 246/300, seasonal_3 Loss: 0.9834 | 2.2650
Epoch 247/300, seasonal_3 Loss: 0.9834 | 2.2656
Epoch 248/300, seasonal_3 Loss: 0.9835 | 2.2663
Epoch 249/300, seasonal_3 Loss: 0.9835 | 2.2669
Epoch 250/300, seasonal_3 Loss: 0.9837 | 2.2674
Epoch 251/300, seasonal_3 Loss: 0.9842 | 2.2680
Epoch 252/300, seasonal_3 Loss: 0.9853 | 2.2686
Epoch 253/300, seasonal_3 Loss: 0.9874 | 2.2691
Epoch 254/300, seasonal_3 Loss: 0.9908 | 2.2695
Epoch 255/300, seasonal_3 Loss: 0.9943 | 2.2687
Epoch 256/300, seasonal_3 Loss: 0.9969 | 2.2684
Epoch 257/300, seasonal_3 Loss: 0.9977 | 2.2648
Epoch 258/300, seasonal_3 Loss: 0.9979 | 2.2672
Epoch 259/300, seasonal_3 Loss: 0.9928 | 2.2660
Epoch 260/300, seasonal_3 Loss: 0.9870 | 2.2609
Epoch 261/300, seasonal_3 Loss: 0.9844 | 2.2582
Epoch 262/300, seasonal_3 Loss: 0.9837 | 2.2581
Epoch 263/300, seasonal_3 Loss: 0.9837 | 2.2589
Epoch 264/300, seasonal_3 Loss: 0.9837 | 2.2598
Epoch 265/300, seasonal_3 Loss: 0.9835 | 2.2605
Epoch 266/300, seasonal_3 Loss: 0.9834 | 2.2613
Epoch 267/300, seasonal_3 Loss: 0.9834 | 2.2620
Epoch 268/300, seasonal_3 Loss: 0.9833 | 2.2628
Epoch 269/300, seasonal_3 Loss: 0.9833 | 2.2635
Epoch 270/300, seasonal_3 Loss: 0.9833 | 2.2642
Epoch 271/300, seasonal_3 Loss: 0.9833 | 2.2648
Epoch 272/300, seasonal_3 Loss: 0.9833 | 2.2655
Epoch 273/300, seasonal_3 Loss: 0.9832 | 2.2660
Epoch 274/300, seasonal_3 Loss: 0.9832 | 2.2666
Epoch 275/300, seasonal_3 Loss: 0.9832 | 2.2672
Epoch 276/300, seasonal_3 Loss: 0.9832 | 2.2677
Epoch 277/300, seasonal_3 Loss: 0.9833 | 2.2682
Epoch 278/300, seasonal_3 Loss: 0.9834 | 2.2687
Epoch 279/300, seasonal_3 Loss: 0.9838 | 2.2692
Epoch 280/300, seasonal_3 Loss: 0.9847 | 2.2697
Epoch 281/300, seasonal_3 Loss: 0.9864 | 2.2705
Epoch 282/300, seasonal_3 Loss: 0.9885 | 2.2710
Epoch 283/300, seasonal_3 Loss: 0.9906 | 2.2715
Epoch 284/300, seasonal_3 Loss: 0.9918 | 2.2711
Epoch 285/300, seasonal_3 Loss: 0.9923 | 2.2706
Epoch 286/300, seasonal_3 Loss: 0.9912 | 2.2703
Epoch 287/300, seasonal_3 Loss: 0.9889 | 2.2686
Epoch 288/300, seasonal_3 Loss: 0.9866 | 2.2669
Epoch 289/300, seasonal_3 Loss: 0.9850 | 2.2655
Epoch 290/300, seasonal_3 Loss: 0.9841 | 2.2650
Epoch 291/300, seasonal_3 Loss: 0.9837 | 2.2651
Epoch 292/300, seasonal_3 Loss: 0.9835 | 2.2655
Epoch 293/300, seasonal_3 Loss: 0.9834 | 2.2659
Epoch 294/300, seasonal_3 Loss: 0.9834 | 2.2664
Epoch 295/300, seasonal_3 Loss: 0.9833 | 2.2669
Epoch 296/300, seasonal_3 Loss: 0.9832 | 2.2673
Epoch 297/300, seasonal_3 Loss: 0.9832 | 2.2677
Epoch 298/300, seasonal_3 Loss: 0.9832 | 2.2682
Epoch 299/300, seasonal_3 Loss: 0.9832 | 2.2686
Epoch 300/300, seasonal_3 Loss: 0.9831 | 2.2690
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9220501328933511, 'learning_rate': 0.0006669871545122167, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8637933626221865}
Epoch 1/300, resid Loss: 0.8798 | 0.1652
Epoch 2/300, resid Loss: 0.1800 | 0.2799
Epoch 3/300, resid Loss: 0.1745 | 0.0821
Epoch 4/300, resid Loss: 0.1323 | 0.1202
Epoch 5/300, resid Loss: 0.1213 | 0.1223
Epoch 6/300, resid Loss: 0.1135 | 0.0626
Epoch 7/300, resid Loss: 0.1172 | 0.0745
Epoch 8/300, resid Loss: 0.1142 | 0.0886
Epoch 9/300, resid Loss: 0.1128 | 0.0540
Epoch 10/300, resid Loss: 0.1024 | 0.0524
Epoch 11/300, resid Loss: 0.0955 | 0.0444
Epoch 12/300, resid Loss: 0.0958 | 0.0586
Epoch 13/300, resid Loss: 0.0915 | 0.0579
Epoch 14/300, resid Loss: 0.0883 | 0.0862
Epoch 15/300, resid Loss: 0.0918 | 0.0645
Epoch 16/300, resid Loss: 0.0980 | 0.0551
Epoch 17/300, resid Loss: 0.0930 | 0.0523
Epoch 18/300, resid Loss: 0.0963 | 0.0560
Epoch 19/300, resid Loss: 0.0902 | 0.0538
Epoch 20/300, resid Loss: 0.0836 | 0.0453
Epoch 21/300, resid Loss: 0.0793 | 0.0385
Epoch 22/300, resid Loss: 0.0788 | 0.0425
Epoch 23/300, resid Loss: 0.0778 | 0.0463
Epoch 24/300, resid Loss: 0.0764 | 0.0472
Epoch 25/300, resid Loss: 0.0813 | 0.0534
Epoch 26/300, resid Loss: 0.0757 | 0.0389
Epoch 27/300, resid Loss: 0.0718 | 0.0383
Epoch 28/300, resid Loss: 0.0752 | 0.0427
Epoch 29/300, resid Loss: 0.0715 | 0.0346
Epoch 30/300, resid Loss: 0.0652 | 0.0329
Epoch 31/300, resid Loss: 0.0659 | 0.0331
Epoch 32/300, resid Loss: 0.0635 | 0.0360
Epoch 33/300, resid Loss: 0.0648 | 0.0428
Epoch 34/300, resid Loss: 0.0614 | 0.0536
Epoch 35/300, resid Loss: 0.0613 | 0.0464
Epoch 36/300, resid Loss: 0.0613 | 0.0375
Epoch 37/300, resid Loss: 0.0570 | 0.0350
Epoch 38/300, resid Loss: 0.0562 | 0.0335
Epoch 39/300, resid Loss: 0.0558 | 0.0293
Epoch 40/300, resid Loss: 0.0554 | 0.0288
Epoch 41/300, resid Loss: 0.0541 | 0.0306
Epoch 42/300, resid Loss: 0.0541 | 0.0325
Epoch 43/300, resid Loss: 0.0539 | 0.0357
Epoch 44/300, resid Loss: 0.0539 | 0.0338
Epoch 45/300, resid Loss: 0.0537 | 0.0366
Epoch 46/300, resid Loss: 0.0548 | 0.0336
Epoch 47/300, resid Loss: 0.0528 | 0.0341
Epoch 48/300, resid Loss: 0.0522 | 0.0349
Epoch 49/300, resid Loss: 0.0522 | 0.0339
Epoch 50/300, resid Loss: 0.0526 | 0.0349
Epoch 51/300, resid Loss: 0.0536 | 0.0365
Epoch 52/300, resid Loss: 0.0547 | 0.0361
Epoch 53/300, resid Loss: 0.0541 | 0.0468
Epoch 54/300, resid Loss: 0.0563 | 0.0417
Epoch 55/300, resid Loss: 0.0549 | 0.0759
Epoch 56/300, resid Loss: 0.0541 | 0.0678
Epoch 57/300, resid Loss: 0.0524 | 0.0454
Epoch 58/300, resid Loss: 0.0535 | 0.0347
Epoch 59/300, resid Loss: 0.0551 | 0.0343
Epoch 60/300, resid Loss: 0.0556 | 0.0390
Epoch 61/300, resid Loss: 0.0549 | 0.0402
Epoch 62/300, resid Loss: 0.0540 | 0.0416
Epoch 63/300, resid Loss: 0.0526 | 0.0406
Epoch 64/300, resid Loss: 0.0517 | 0.0391
Epoch 65/300, resid Loss: 0.0518 | 0.0369
Epoch 66/300, resid Loss: 0.0518 | 0.0371
Epoch 67/300, resid Loss: 0.0502 | 0.0391
Epoch 68/300, resid Loss: 0.0492 | 0.0371
Epoch 69/300, resid Loss: 0.0493 | 0.0354
Epoch 70/300, resid Loss: 0.0485 | 0.0342
Epoch 71/300, resid Loss: 0.0475 | 0.0337
Epoch 72/300, resid Loss: 0.0467 | 0.0348
Epoch 73/300, resid Loss: 0.0464 | 0.0351
Epoch 74/300, resid Loss: 0.0464 | 0.0353
Epoch 75/300, resid Loss: 0.0464 | 0.0350
Epoch 76/300, resid Loss: 0.0463 | 0.0345
Epoch 77/300, resid Loss: 0.0462 | 0.0341
Epoch 78/300, resid Loss: 0.0460 | 0.0340
Epoch 79/300, resid Loss: 0.0457 | 0.0341
Epoch 80/300, resid Loss: 0.0454 | 0.0342
Epoch 81/300, resid Loss: 0.0453 | 0.0340
Epoch 82/300, resid Loss: 0.0451 | 0.0339
Epoch 83/300, resid Loss: 0.0450 | 0.0338
Epoch 84/300, resid Loss: 0.0449 | 0.0338
Epoch 85/300, resid Loss: 0.0448 | 0.0339
Epoch 86/300, resid Loss: 0.0447 | 0.0340
Epoch 87/300, resid Loss: 0.0447 | 0.0340
Epoch 88/300, resid Loss: 0.0446 | 0.0339
Epoch 89/300, resid Loss: 0.0445 | 0.0338
Epoch 90/300, resid Loss: 0.0445 | 0.0337
Epoch 91/300, resid Loss: 0.0444 | 0.0336
Epoch 92/300, resid Loss: 0.0444 | 0.0337
Epoch 93/300, resid Loss: 0.0443 | 0.0337
Epoch 94/300, resid Loss: 0.0443 | 0.0337
Epoch 95/300, resid Loss: 0.0442 | 0.0337
Epoch 96/300, resid Loss: 0.0441 | 0.0336
Epoch 97/300, resid Loss: 0.0441 | 0.0335
Epoch 98/300, resid Loss: 0.0440 | 0.0335
Epoch 99/300, resid Loss: 0.0440 | 0.0335
Epoch 100/300, resid Loss: 0.0440 | 0.0335
Epoch 101/300, resid Loss: 0.0440 | 0.0335
Epoch 102/300, resid Loss: 0.0439 | 0.0335
Epoch 103/300, resid Loss: 0.0438 | 0.0335
Epoch 104/300, resid Loss: 0.0438 | 0.0335
Epoch 105/300, resid Loss: 0.0438 | 0.0335
Epoch 106/300, resid Loss: 0.0437 | 0.0335
Epoch 107/300, resid Loss: 0.0437 | 0.0335
Epoch 108/300, resid Loss: 0.0437 | 0.0335
Epoch 109/300, resid Loss: 0.0436 | 0.0335
Epoch 110/300, resid Loss: 0.0436 | 0.0335
Epoch 111/300, resid Loss: 0.0436 | 0.0335
Epoch 112/300, resid Loss: 0.0435 | 0.0335
Epoch 113/300, resid Loss: 0.0435 | 0.0334
Epoch 114/300, resid Loss: 0.0435 | 0.0334
Epoch 115/300, resid Loss: 0.0435 | 0.0334
Epoch 116/300, resid Loss: 0.0434 | 0.0334
Epoch 117/300, resid Loss: 0.0434 | 0.0334
Epoch 118/300, resid Loss: 0.0434 | 0.0334
Epoch 119/300, resid Loss: 0.0434 | 0.0334
Epoch 120/300, resid Loss: 0.0434 | 0.0334
Epoch 121/300, resid Loss: 0.0433 | 0.0334
Epoch 122/300, resid Loss: 0.0433 | 0.0334
Epoch 123/300, resid Loss: 0.0433 | 0.0334
Epoch 124/300, resid Loss: 0.0433 | 0.0334
Epoch 125/300, resid Loss: 0.0433 | 0.0334
Epoch 126/300, resid Loss: 0.0433 | 0.0334
Epoch 127/300, resid Loss: 0.0433 | 0.0334
Epoch 128/300, resid Loss: 0.0432 | 0.0334
Epoch 129/300, resid Loss: 0.0432 | 0.0334
Epoch 130/300, resid Loss: 0.0432 | 0.0334
Epoch 131/300, resid Loss: 0.0432 | 0.0334
Epoch 132/300, resid Loss: 0.0432 | 0.0334
Epoch 133/300, resid Loss: 0.0432 | 0.0334
Epoch 134/300, resid Loss: 0.0432 | 0.0334
Epoch 135/300, resid Loss: 0.0432 | 0.0334
Epoch 136/300, resid Loss: 0.0432 | 0.0334
Epoch 137/300, resid Loss: 0.0432 | 0.0334
Epoch 138/300, resid Loss: 0.0431 | 0.0334
Epoch 139/300, resid Loss: 0.0431 | 0.0334
Epoch 140/300, resid Loss: 0.0431 | 0.0334
Epoch 141/300, resid Loss: 0.0431 | 0.0334
Epoch 142/300, resid Loss: 0.0431 | 0.0334
Epoch 143/300, resid Loss: 0.0431 | 0.0334
Epoch 144/300, resid Loss: 0.0431 | 0.0334
Epoch 145/300, resid Loss: 0.0431 | 0.0333
Epoch 146/300, resid Loss: 0.0431 | 0.0333
Epoch 147/300, resid Loss: 0.0431 | 0.0333
Epoch 148/300, resid Loss: 0.0431 | 0.0333
Epoch 149/300, resid Loss: 0.0431 | 0.0333
Epoch 150/300, resid Loss: 0.0431 | 0.0333
Epoch 151/300, resid Loss: 0.0431 | 0.0333
Epoch 152/300, resid Loss: 0.0431 | 0.0333
Epoch 153/300, resid Loss: 0.0431 | 0.0333
Epoch 154/300, resid Loss: 0.0431 | 0.0333
Epoch 155/300, resid Loss: 0.0431 | 0.0333
Epoch 156/300, resid Loss: 0.0431 | 0.0333
Epoch 157/300, resid Loss: 0.0430 | 0.0333
Epoch 158/300, resid Loss: 0.0430 | 0.0333
Epoch 159/300, resid Loss: 0.0430 | 0.0333
Epoch 160/300, resid Loss: 0.0430 | 0.0333
Epoch 161/300, resid Loss: 0.0430 | 0.0333
Epoch 162/300, resid Loss: 0.0430 | 0.0333
Epoch 163/300, resid Loss: 0.0430 | 0.0333
Epoch 164/300, resid Loss: 0.0430 | 0.0333
Epoch 165/300, resid Loss: 0.0430 | 0.0333
Epoch 166/300, resid Loss: 0.0430 | 0.0333
Epoch 167/300, resid Loss: 0.0430 | 0.0333
Epoch 168/300, resid Loss: 0.0430 | 0.0333
Epoch 169/300, resid Loss: 0.0430 | 0.0333
Epoch 170/300, resid Loss: 0.0430 | 0.0333
Epoch 171/300, resid Loss: 0.0430 | 0.0333
Epoch 172/300, resid Loss: 0.0430 | 0.0333
Epoch 173/300, resid Loss: 0.0430 | 0.0333
Epoch 174/300, resid Loss: 0.0430 | 0.0333
Epoch 175/300, resid Loss: 0.0430 | 0.0333
Epoch 176/300, resid Loss: 0.0430 | 0.0333
Epoch 177/300, resid Loss: 0.0430 | 0.0333
Epoch 178/300, resid Loss: 0.0430 | 0.0333
Epoch 179/300, resid Loss: 0.0430 | 0.0333
Epoch 180/300, resid Loss: 0.0430 | 0.0333
Epoch 181/300, resid Loss: 0.0430 | 0.0333
Epoch 182/300, resid Loss: 0.0430 | 0.0333
Epoch 183/300, resid Loss: 0.0430 | 0.0333
Epoch 184/300, resid Loss: 0.0430 | 0.0333
Epoch 185/300, resid Loss: 0.0430 | 0.0333
Epoch 186/300, resid Loss: 0.0430 | 0.0333
Epoch 187/300, resid Loss: 0.0430 | 0.0333
Epoch 188/300, resid Loss: 0.0430 | 0.0333
Epoch 189/300, resid Loss: 0.0430 | 0.0333
Epoch 190/300, resid Loss: 0.0430 | 0.0333
Epoch 191/300, resid Loss: 0.0430 | 0.0333
Epoch 192/300, resid Loss: 0.0430 | 0.0333
Epoch 193/300, resid Loss: 0.0430 | 0.0333
Epoch 194/300, resid Loss: 0.0430 | 0.0333
Epoch 195/300, resid Loss: 0.0430 | 0.0333
Epoch 196/300, resid Loss: 0.0430 | 0.0333
Epoch 197/300, resid Loss: 0.0430 | 0.0333
Epoch 198/300, resid Loss: 0.0430 | 0.0333
Epoch 199/300, resid Loss: 0.0430 | 0.0333
Epoch 200/300, resid Loss: 0.0430 | 0.0333
Epoch 201/300, resid Loss: 0.0430 | 0.0333
Epoch 202/300, resid Loss: 0.0430 | 0.0333
Epoch 203/300, resid Loss: 0.0430 | 0.0333
Epoch 204/300, resid Loss: 0.0430 | 0.0333
Epoch 205/300, resid Loss: 0.0430 | 0.0333
Epoch 206/300, resid Loss: 0.0430 | 0.0333
Epoch 207/300, resid Loss: 0.0430 | 0.0333
Epoch 208/300, resid Loss: 0.0430 | 0.0333
Epoch 209/300, resid Loss: 0.0430 | 0.0333
Epoch 210/300, resid Loss: 0.0430 | 0.0333
Epoch 211/300, resid Loss: 0.0430 | 0.0333
Epoch 212/300, resid Loss: 0.0430 | 0.0333
Epoch 213/300, resid Loss: 0.0430 | 0.0333
Epoch 214/300, resid Loss: 0.0430 | 0.0333
Epoch 215/300, resid Loss: 0.0430 | 0.0333
Epoch 216/300, resid Loss: 0.0430 | 0.0333
Epoch 217/300, resid Loss: 0.0430 | 0.0333
Epoch 218/300, resid Loss: 0.0430 | 0.0333
Epoch 219/300, resid Loss: 0.0430 | 0.0333
Epoch 220/300, resid Loss: 0.0430 | 0.0333
Epoch 221/300, resid Loss: 0.0430 | 0.0333
Epoch 222/300, resid Loss: 0.0430 | 0.0333
Epoch 223/300, resid Loss: 0.0430 | 0.0333
Epoch 224/300, resid Loss: 0.0430 | 0.0333
Epoch 225/300, resid Loss: 0.0430 | 0.0333
Epoch 226/300, resid Loss: 0.0430 | 0.0333
Epoch 227/300, resid Loss: 0.0430 | 0.0333
Epoch 228/300, resid Loss: 0.0430 | 0.0333
Epoch 229/300, resid Loss: 0.0430 | 0.0333
Epoch 230/300, resid Loss: 0.0430 | 0.0333
Epoch 231/300, resid Loss: 0.0430 | 0.0333
Epoch 232/300, resid Loss: 0.0430 | 0.0333
Epoch 233/300, resid Loss: 0.0430 | 0.0333
Epoch 234/300, resid Loss: 0.0430 | 0.0333
Epoch 235/300, resid Loss: 0.0430 | 0.0333
Epoch 236/300, resid Loss: 0.0430 | 0.0333
Epoch 237/300, resid Loss: 0.0430 | 0.0333
Epoch 238/300, resid Loss: 0.0430 | 0.0333
Epoch 239/300, resid Loss: 0.0430 | 0.0333
Epoch 240/300, resid Loss: 0.0430 | 0.0333
Epoch 241/300, resid Loss: 0.0430 | 0.0333
Epoch 242/300, resid Loss: 0.0430 | 0.0333
Epoch 243/300, resid Loss: 0.0430 | 0.0333
Epoch 244/300, resid Loss: 0.0430 | 0.0333
Epoch 245/300, resid Loss: 0.0430 | 0.0333
Epoch 246/300, resid Loss: 0.0430 | 0.0333
Epoch 247/300, resid Loss: 0.0430 | 0.0333
Epoch 248/300, resid Loss: 0.0430 | 0.0333
Epoch 249/300, resid Loss: 0.0430 | 0.0333
Epoch 250/300, resid Loss: 0.0430 | 0.0333
Epoch 251/300, resid Loss: 0.0430 | 0.0333
Epoch 252/300, resid Loss: 0.0430 | 0.0333
Epoch 253/300, resid Loss: 0.0430 | 0.0333
Epoch 254/300, resid Loss: 0.0430 | 0.0333
Epoch 255/300, resid Loss: 0.0430 | 0.0333
Epoch 256/300, resid Loss: 0.0430 | 0.0333
Epoch 257/300, resid Loss: 0.0430 | 0.0333
Epoch 258/300, resid Loss: 0.0430 | 0.0333
Epoch 259/300, resid Loss: 0.0430 | 0.0333
Epoch 260/300, resid Loss: 0.0430 | 0.0333
Epoch 261/300, resid Loss: 0.0430 | 0.0333
Epoch 262/300, resid Loss: 0.0430 | 0.0333
Epoch 263/300, resid Loss: 0.0430 | 0.0333
Epoch 264/300, resid Loss: 0.0430 | 0.0333
Epoch 265/300, resid Loss: 0.0430 | 0.0333
Epoch 266/300, resid Loss: 0.0430 | 0.0333
Epoch 267/300, resid Loss: 0.0430 | 0.0333
Epoch 268/300, resid Loss: 0.0430 | 0.0333
Epoch 269/300, resid Loss: 0.0430 | 0.0333
Epoch 270/300, resid Loss: 0.0430 | 0.0333
Epoch 271/300, resid Loss: 0.0430 | 0.0333
Epoch 272/300, resid Loss: 0.0430 | 0.0333
Epoch 273/300, resid Loss: 0.0430 | 0.0333
Epoch 274/300, resid Loss: 0.0430 | 0.0333
Epoch 275/300, resid Loss: 0.0430 | 0.0333
Epoch 276/300, resid Loss: 0.0430 | 0.0333
Epoch 277/300, resid Loss: 0.0430 | 0.0333
Epoch 278/300, resid Loss: 0.0430 | 0.0333
Epoch 279/300, resid Loss: 0.0430 | 0.0333
Epoch 280/300, resid Loss: 0.0430 | 0.0333
Epoch 281/300, resid Loss: 0.0430 | 0.0333
Epoch 282/300, resid Loss: 0.0430 | 0.0333
Epoch 283/300, resid Loss: 0.0430 | 0.0333
Epoch 284/300, resid Loss: 0.0430 | 0.0333
Epoch 285/300, resid Loss: 0.0430 | 0.0333
Epoch 286/300, resid Loss: 0.0430 | 0.0333
Epoch 287/300, resid Loss: 0.0430 | 0.0333
Epoch 288/300, resid Loss: 0.0430 | 0.0333
Epoch 289/300, resid Loss: 0.0430 | 0.0333
Epoch 290/300, resid Loss: 0.0430 | 0.0333
Epoch 291/300, resid Loss: 0.0430 | 0.0333
Epoch 292/300, resid Loss: 0.0430 | 0.0333
Epoch 293/300, resid Loss: 0.0430 | 0.0333
Epoch 294/300, resid Loss: 0.0430 | 0.0333
Epoch 295/300, resid Loss: 0.0430 | 0.0333
Epoch 296/300, resid Loss: 0.0430 | 0.0333
Epoch 297/300, resid Loss: 0.0430 | 0.0333
Epoch 298/300, resid Loss: 0.0430 | 0.0333
Epoch 299/300, resid Loss: 0.0430 | 0.0333
Epoch 300/300, resid Loss: 0.0430 | 0.0333
Runtime (seconds): 7298.20444726944
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[210.178]
[1.9435478]
[-0.21228223]
[-0.7750185]
[-0.69652]
[6.4474545]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 255.51487848255783
RMSE: 15.984832763671875
MAE: 15.984832763671875
R-squared: nan
[216.88516]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
