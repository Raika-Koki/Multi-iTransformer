ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 20:35:32,122][0m A new study created in memory with name: no-name-12d7c487-b91c-4782-94d3-4e9318c4846d[0m
[32m[I 2025-01-07 20:36:14,673][0m Trial 0 finished with value: 0.3260468275960212 and parameters: {'observation_period_num': 28, 'train_rates': 0.8540891016456218, 'learning_rate': 7.117522848685416e-06, 'batch_size': 135, 'step_size': 13, 'gamma': 0.9842294561036609}. Best is trial 0 with value: 0.3260468275960212.[0m
[32m[I 2025-01-07 20:37:13,625][0m Trial 1 finished with value: 0.4430610775771746 and parameters: {'observation_period_num': 233, 'train_rates': 0.8756068088311737, 'learning_rate': 5.9980598054431244e-06, 'batch_size': 89, 'step_size': 10, 'gamma': 0.8565149335481045}. Best is trial 0 with value: 0.3260468275960212.[0m
[32m[I 2025-01-07 20:38:03,346][0m Trial 2 finished with value: 0.10748512949061206 and parameters: {'observation_period_num': 227, 'train_rates': 0.9535248659298527, 'learning_rate': 0.0004378791158105357, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9327692387934312}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:38:35,762][0m Trial 3 finished with value: 0.27184431380461455 and parameters: {'observation_period_num': 180, 'train_rates': 0.6537255379587007, 'learning_rate': 0.00025968262142787965, 'batch_size': 151, 'step_size': 3, 'gamma': 0.9374025984004336}. Best is trial 2 with value: 0.10748512949061206.[0m
Early stopping at epoch 63
[32m[I 2025-01-07 20:38:53,563][0m Trial 4 finished with value: 0.27152640225926067 and parameters: {'observation_period_num': 42, 'train_rates': 0.7888316533819247, 'learning_rate': 0.00048492002075693904, 'batch_size': 223, 'step_size': 1, 'gamma': 0.8138241111481075}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:39:19,227][0m Trial 5 finished with value: 0.16815538249082035 and parameters: {'observation_period_num': 195, 'train_rates': 0.8696612527806229, 'learning_rate': 0.00015379868452655078, 'batch_size': 230, 'step_size': 14, 'gamma': 0.9665892807043917}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:39:57,311][0m Trial 6 finished with value: 0.9503859776964221 and parameters: {'observation_period_num': 129, 'train_rates': 0.6981167354268268, 'learning_rate': 2.1226641519370333e-06, 'batch_size': 125, 'step_size': 9, 'gamma': 0.7592172781734858}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:40:55,520][0m Trial 7 finished with value: 0.7390780076802334 and parameters: {'observation_period_num': 229, 'train_rates': 0.8036297603548779, 'learning_rate': 1.7640149546478995e-06, 'batch_size': 86, 'step_size': 12, 'gamma': 0.9612598863240293}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:41:28,551][0m Trial 8 finished with value: 2.4155240058898926 and parameters: {'observation_period_num': 18, 'train_rates': 0.9467248315082721, 'learning_rate': 2.8105841759650773e-06, 'batch_size': 194, 'step_size': 2, 'gamma': 0.9347139874386425}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:42:02,006][0m Trial 9 finished with value: 1.6438297392374062 and parameters: {'observation_period_num': 5, 'train_rates': 0.9156441351301363, 'learning_rate': 3.1155690717624766e-06, 'batch_size': 180, 'step_size': 7, 'gamma': 0.773914501209149}. Best is trial 2 with value: 0.10748512949061206.[0m
[32m[I 2025-01-07 20:45:18,161][0m Trial 10 finished with value: 0.0909548695385456 and parameters: {'observation_period_num': 112, 'train_rates': 0.9825434108863547, 'learning_rate': 3.953905830841196e-05, 'batch_size': 29, 'step_size': 6, 'gamma': 0.8918682467295789}. Best is trial 10 with value: 0.0909548695385456.[0m
[32m[I 2025-01-07 20:51:04,724][0m Trial 11 finished with value: 0.07358374988490884 and parameters: {'observation_period_num': 101, 'train_rates': 0.9846882791560038, 'learning_rate': 4.787088569836974e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8912152312253441}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 20:56:56,926][0m Trial 12 finished with value: 0.07552559448862975 and parameters: {'observation_period_num': 87, 'train_rates': 0.9816021856700096, 'learning_rate': 4.630794009708163e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8805936344160996}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 21:01:40,445][0m Trial 13 finished with value: 0.2332372627742998 and parameters: {'observation_period_num': 80, 'train_rates': 0.7468168423582572, 'learning_rate': 3.5701961283636475e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8770628029878618}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 21:03:32,458][0m Trial 14 finished with value: 0.10331724541929532 and parameters: {'observation_period_num': 76, 'train_rates': 0.981615588423773, 'learning_rate': 9.939457855746663e-05, 'batch_size': 52, 'step_size': 4, 'gamma': 0.8345374488921665}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 21:05:05,442][0m Trial 15 finished with value: 0.21073963648329178 and parameters: {'observation_period_num': 125, 'train_rates': 0.9152911638521014, 'learning_rate': 1.375680747183534e-05, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9024480445994026}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 21:06:34,256][0m Trial 16 finished with value: 0.233083521666234 and parameters: {'observation_period_num': 77, 'train_rates': 0.6038767270722752, 'learning_rate': 7.789877390512253e-05, 'batch_size': 47, 'step_size': 11, 'gamma': 0.8194424439310676}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 21:07:43,335][0m Trial 17 finished with value: 0.2926707144637229 and parameters: {'observation_period_num': 159, 'train_rates': 0.9158547305823448, 'learning_rate': 1.8167239210072273e-05, 'batch_size': 83, 'step_size': 6, 'gamma': 0.9054011924105395}. Best is trial 11 with value: 0.07358374988490884.[0m
[32m[I 2025-01-07 21:12:24,231][0m Trial 18 finished with value: 0.06126323675221585 and parameters: {'observation_period_num': 56, 'train_rates': 0.821614695648694, 'learning_rate': 6.679130375534749e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.8537813316933955}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:13:42,942][0m Trial 19 finished with value: 0.06965614985783973 and parameters: {'observation_period_num': 47, 'train_rates': 0.8242123286341434, 'learning_rate': 0.0008497970114849233, 'batch_size': 69, 'step_size': 8, 'gamma': 0.8515997546680273}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:15:00,365][0m Trial 20 finished with value: 0.07378333143822782 and parameters: {'observation_period_num': 46, 'train_rates': 0.8091632443461547, 'learning_rate': 0.0008095775836472982, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8506802855251254}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:17:14,758][0m Trial 21 finished with value: 0.2117994048091276 and parameters: {'observation_period_num': 57, 'train_rates': 0.7560152698653007, 'learning_rate': 0.00017499262492859963, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8002276070853833}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:18:02,458][0m Trial 22 finished with value: 0.5423999878398159 and parameters: {'observation_period_num': 105, 'train_rates': 0.7199889416474364, 'learning_rate': 2.0023881196343997e-05, 'batch_size': 104, 'step_size': 8, 'gamma': 0.8555642328806549}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:20:35,494][0m Trial 23 finished with value: 0.07621284657142173 and parameters: {'observation_period_num': 60, 'train_rates': 0.8369038496917859, 'learning_rate': 7.464312531250578e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.8339747698327429}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:21:55,264][0m Trial 24 finished with value: 0.08805202315532186 and parameters: {'observation_period_num': 98, 'train_rates': 0.8889511829525399, 'learning_rate': 0.0008761298369083431, 'batch_size': 70, 'step_size': 10, 'gamma': 0.7944177688008025}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:26:51,294][0m Trial 25 finished with value: 0.07621653894844808 and parameters: {'observation_period_num': 144, 'train_rates': 0.8309580929074026, 'learning_rate': 0.0002682110814275799, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9123537420518182}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:28:41,053][0m Trial 26 finished with value: 0.4719953841941301 and parameters: {'observation_period_num': 31, 'train_rates': 0.7499847526338076, 'learning_rate': 6.310602926197357e-06, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8675881642688564}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:29:15,511][0m Trial 27 finished with value: 0.5096780359745026 and parameters: {'observation_period_num': 61, 'train_rates': 0.7870562749211086, 'learning_rate': 1.1533725796013926e-05, 'batch_size': 154, 'step_size': 11, 'gamma': 0.8353804977872499}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:30:26,516][0m Trial 28 finished with value: 0.2659371341358532 and parameters: {'observation_period_num': 92, 'train_rates': 0.7119588537131204, 'learning_rate': 6.041612447385999e-05, 'batch_size': 67, 'step_size': 7, 'gamma': 0.8873921922508928}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:31:16,941][0m Trial 29 finished with value: 0.17132027795867688 and parameters: {'observation_period_num': 22, 'train_rates': 0.6703696312403341, 'learning_rate': 0.00011580431594526128, 'batch_size': 97, 'step_size': 9, 'gamma': 0.9245810266573409}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:34:00,429][0m Trial 30 finished with value: 0.06217408333773727 and parameters: {'observation_period_num': 40, 'train_rates': 0.8420865512881099, 'learning_rate': 2.9020207493760482e-05, 'batch_size': 32, 'step_size': 12, 'gamma': 0.8664117522563282}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:36:56,445][0m Trial 31 finished with value: 0.06476851552724838 and parameters: {'observation_period_num': 40, 'train_rates': 0.8476992561116623, 'learning_rate': 2.7871193469430656e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.8675626208683747}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:39:27,723][0m Trial 32 finished with value: 0.0666577652181414 and parameters: {'observation_period_num': 37, 'train_rates': 0.8492706256799442, 'learning_rate': 2.3215785958410375e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.8654609022805605}. Best is trial 18 with value: 0.06126323675221585.[0m
[32m[I 2025-01-07 21:42:12,288][0m Trial 33 finished with value: 0.04899346232985902 and parameters: {'observation_period_num': 6, 'train_rates': 0.8676603304741475, 'learning_rate': 2.8386641919985336e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.8712636928145189}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:43:59,824][0m Trial 34 finished with value: 0.09894501826668207 and parameters: {'observation_period_num': 10, 'train_rates': 0.8849352109040632, 'learning_rate': 1.0568714634600204e-05, 'batch_size': 53, 'step_size': 15, 'gamma': 0.844442215375244}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:47:05,058][0m Trial 35 finished with value: 0.058587652953484885 and parameters: {'observation_period_num': 27, 'train_rates': 0.8579574421599704, 'learning_rate': 3.0196211903703023e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.8705477291755039}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:50:06,590][0m Trial 36 finished with value: 0.0957657100751679 and parameters: {'observation_period_num': 25, 'train_rates': 0.8717385204789996, 'learning_rate': 8.540113642751425e-06, 'batch_size': 30, 'step_size': 13, 'gamma': 0.8073700186482976}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:50:59,766][0m Trial 37 finished with value: 0.10419277592313171 and parameters: {'observation_period_num': 6, 'train_rates': 0.8631070256945561, 'learning_rate': 1.542232281884271e-05, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8760520099785645}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:51:22,440][0m Trial 38 finished with value: 0.6842845779031379 and parameters: {'observation_period_num': 67, 'train_rates': 0.7724940026482674, 'learning_rate': 2.8655590957677012e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8257785352424566}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:52:33,190][0m Trial 39 finished with value: 0.057986486170973094 and parameters: {'observation_period_num': 22, 'train_rates': 0.9023568823948707, 'learning_rate': 5.920114099137666e-05, 'batch_size': 82, 'step_size': 12, 'gamma': 0.9496667012136635}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:53:17,449][0m Trial 40 finished with value: 0.05543542191133661 and parameters: {'observation_period_num': 20, 'train_rates': 0.8995157643354267, 'learning_rate': 0.00020065237736956125, 'batch_size': 134, 'step_size': 12, 'gamma': 0.9726929797312784}. Best is trial 33 with value: 0.04899346232985902.[0m
[32m[I 2025-01-07 21:54:01,899][0m Trial 41 finished with value: 0.048768651438876987 and parameters: {'observation_period_num': 19, 'train_rates': 0.9018965695888785, 'learning_rate': 0.0002725164186762237, 'batch_size': 135, 'step_size': 12, 'gamma': 0.9874281417627508}. Best is trial 41 with value: 0.048768651438876987.[0m
[32m[I 2025-01-07 21:54:49,744][0m Trial 42 finished with value: 0.04949216651065009 and parameters: {'observation_period_num': 18, 'train_rates': 0.9380277180751341, 'learning_rate': 0.00033566994218586966, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9832685406207037}. Best is trial 41 with value: 0.048768651438876987.[0m
[32m[I 2025-01-07 21:55:35,603][0m Trial 43 finished with value: 0.03562026489841736 and parameters: {'observation_period_num': 15, 'train_rates': 0.9400484084044951, 'learning_rate': 0.00038250459515289255, 'batch_size': 138, 'step_size': 12, 'gamma': 0.9899569891931845}. Best is trial 43 with value: 0.03562026489841736.[0m
[32m[I 2025-01-07 21:56:19,373][0m Trial 44 finished with value: 0.05833240922378457 and parameters: {'observation_period_num': 15, 'train_rates': 0.9454020565204617, 'learning_rate': 0.00047461834271293735, 'batch_size': 140, 'step_size': 10, 'gamma': 0.9895066112501496}. Best is trial 43 with value: 0.03562026489841736.[0m
[32m[I 2025-01-07 21:56:57,269][0m Trial 45 finished with value: 0.047461215406656265 and parameters: {'observation_period_num': 7, 'train_rates': 0.9607978849765071, 'learning_rate': 0.0003045922202873265, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9768324155846346}. Best is trial 43 with value: 0.03562026489841736.[0m
[32m[I 2025-01-07 21:57:34,702][0m Trial 46 finished with value: 0.05019736803463987 and parameters: {'observation_period_num': 7, 'train_rates': 0.9372254835692702, 'learning_rate': 0.0003238254741585962, 'batch_size': 170, 'step_size': 11, 'gamma': 0.975302959651678}. Best is trial 43 with value: 0.03562026489841736.[0m
[32m[I 2025-01-07 21:58:08,067][0m Trial 47 finished with value: 0.10060007870197296 and parameters: {'observation_period_num': 202, 'train_rates': 0.9668175982809062, 'learning_rate': 0.00034817964709811137, 'batch_size': 188, 'step_size': 14, 'gamma': 0.9544823756534812}. Best is trial 43 with value: 0.03562026489841736.[0m
[32m[I 2025-01-07 21:58:45,952][0m Trial 48 finished with value: 0.049849116379142404 and parameters: {'observation_period_num': 31, 'train_rates': 0.9320016510385535, 'learning_rate': 0.0006051452292179645, 'batch_size': 161, 'step_size': 11, 'gamma': 0.9838101882071639}. Best is trial 43 with value: 0.03562026489841736.[0m
[32m[I 2025-01-07 21:59:17,329][0m Trial 49 finished with value: 0.05651457607746124 and parameters: {'observation_period_num': 6, 'train_rates': 0.9606879006491638, 'learning_rate': 0.000135982929957024, 'batch_size': 206, 'step_size': 14, 'gamma': 0.9485428301396006}. Best is trial 43 with value: 0.03562026489841736.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 21:59:17,340][0m A new study created in memory with name: no-name-bb0202be-d420-4acb-9707-25d151b2aae3[0m
[32m[I 2025-01-07 22:00:45,446][0m Trial 0 finished with value: 0.6647686306413237 and parameters: {'observation_period_num': 189, 'train_rates': 0.7414450420526985, 'learning_rate': 2.0590803163756422e-06, 'batch_size': 224, 'step_size': 10, 'gamma': 0.7986838043407775}. Best is trial 0 with value: 0.6647686306413237.[0m
[32m[I 2025-01-07 22:02:39,319][0m Trial 1 finished with value: 0.18447670340538025 and parameters: {'observation_period_num': 36, 'train_rates': 0.9348140258615792, 'learning_rate': 2.365242887196681e-05, 'batch_size': 249, 'step_size': 2, 'gamma': 0.8254973113178443}. Best is trial 1 with value: 0.18447670340538025.[0m
[32m[I 2025-01-07 22:04:00,039][0m Trial 2 finished with value: 0.25923276977002163 and parameters: {'observation_period_num': 160, 'train_rates': 0.6250824612116735, 'learning_rate': 0.00021738874857212434, 'batch_size': 126, 'step_size': 4, 'gamma': 0.8056181450956662}. Best is trial 1 with value: 0.18447670340538025.[0m
[32m[I 2025-01-07 22:05:31,382][0m Trial 3 finished with value: 0.1045360139023933 and parameters: {'observation_period_num': 135, 'train_rates': 0.8177986884599843, 'learning_rate': 0.00020945736509487984, 'batch_size': 156, 'step_size': 2, 'gamma': 0.8253231638107263}. Best is trial 3 with value: 0.1045360139023933.[0m
[32m[I 2025-01-07 22:12:26,355][0m Trial 4 finished with value: 0.06655607280436526 and parameters: {'observation_period_num': 64, 'train_rates': 0.9076602278650534, 'learning_rate': 4.7305081684555965e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.8570147131278919}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:14:17,016][0m Trial 5 finished with value: 0.37309632556779043 and parameters: {'observation_period_num': 46, 'train_rates': 0.8943125301384468, 'learning_rate': 3.684957548409705e-06, 'batch_size': 198, 'step_size': 1, 'gamma': 0.9311326916154361}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:16:16,903][0m Trial 6 finished with value: 0.09680818021297455 and parameters: {'observation_period_num': 101, 'train_rates': 0.9531605862661547, 'learning_rate': 2.1867006696893534e-05, 'batch_size': 145, 'step_size': 6, 'gamma': 0.8515950647076773}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:19:08,268][0m Trial 7 finished with value: 0.1426999290660795 and parameters: {'observation_period_num': 129, 'train_rates': 0.8291213645326994, 'learning_rate': 1.9461272710800783e-06, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9732545226327687}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:20:27,129][0m Trial 8 finished with value: 0.20630558902227072 and parameters: {'observation_period_num': 65, 'train_rates': 0.6526306015906472, 'learning_rate': 3.0924176912524036e-05, 'batch_size': 209, 'step_size': 12, 'gamma': 0.979175934446134}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:21:59,825][0m Trial 9 finished with value: 0.15985649502609278 and parameters: {'observation_period_num': 93, 'train_rates': 0.8495288179309112, 'learning_rate': 7.018327592382939e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9384015696835464}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:26:58,028][0m Trial 10 finished with value: 0.3286848735142421 and parameters: {'observation_period_num': 239, 'train_rates': 0.7366026003692626, 'learning_rate': 7.363221011274212e-06, 'batch_size': 25, 'step_size': 15, 'gamma': 0.7512876092350016}. Best is trial 4 with value: 0.06655607280436526.[0m
[32m[I 2025-01-07 22:29:26,246][0m Trial 11 finished with value: 0.03966953232884407 and parameters: {'observation_period_num': 12, 'train_rates': 0.9879910618676314, 'learning_rate': 0.0006938619554405274, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8841666799378178}. Best is trial 11 with value: 0.03966953232884407.[0m
[32m[I 2025-01-07 22:31:57,000][0m Trial 12 finished with value: 0.042221251875162125 and parameters: {'observation_period_num': 7, 'train_rates': 0.9874444845867896, 'learning_rate': 0.0007156035074599198, 'batch_size': 88, 'step_size': 11, 'gamma': 0.8916824629805736}. Best is trial 11 with value: 0.03966953232884407.[0m
[32m[I 2025-01-07 22:34:09,254][0m Trial 13 finished with value: 0.032032351940870285 and parameters: {'observation_period_num': 8, 'train_rates': 0.9734322728739413, 'learning_rate': 0.0006614394112868948, 'batch_size': 87, 'step_size': 11, 'gamma': 0.8966808512301411}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:36:29,989][0m Trial 14 finished with value: 0.04583846032619476 and parameters: {'observation_period_num': 14, 'train_rates': 0.9866794198490669, 'learning_rate': 0.0008691326007980508, 'batch_size': 86, 'step_size': 8, 'gamma': 0.895196918884512}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:38:14,755][0m Trial 15 finished with value: 0.03483679805218515 and parameters: {'observation_period_num': 7, 'train_rates': 0.8773078010916545, 'learning_rate': 0.00028720969812823683, 'batch_size': 95, 'step_size': 6, 'gamma': 0.913077854622185}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:39:59,131][0m Trial 16 finished with value: 0.057929606727904676 and parameters: {'observation_period_num': 91, 'train_rates': 0.8665642454825581, 'learning_rate': 0.0002216375893195968, 'batch_size': 116, 'step_size': 5, 'gamma': 0.9269543450012222}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:42:29,598][0m Trial 17 finished with value: 0.07687569522782217 and parameters: {'observation_period_num': 43, 'train_rates': 0.9188785548184298, 'learning_rate': 0.00010232344579430012, 'batch_size': 64, 'step_size': 10, 'gamma': 0.9131969594380018}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:43:59,877][0m Trial 18 finished with value: 0.304394920544613 and parameters: {'observation_period_num': 215, 'train_rates': 0.7703632866706792, 'learning_rate': 0.00037836716606533824, 'batch_size': 173, 'step_size': 13, 'gamma': 0.953602812285577}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:46:42,760][0m Trial 19 finished with value: 0.18246022841105095 and parameters: {'observation_period_num': 72, 'train_rates': 0.8783750982845391, 'learning_rate': 0.00036526979763782806, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8717672622386374}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:49:01,252][0m Trial 20 finished with value: 0.05397737150390943 and parameters: {'observation_period_num': 37, 'train_rates': 0.9454756136942535, 'learning_rate': 0.00016173834602905697, 'batch_size': 106, 'step_size': 13, 'gamma': 0.9100957950526086}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:51:26,069][0m Trial 21 finished with value: 0.0348062030233369 and parameters: {'observation_period_num': 5, 'train_rates': 0.9658940064101248, 'learning_rate': 0.0005619432376846147, 'batch_size': 93, 'step_size': 7, 'gamma': 0.8753049950551822}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:53:55,788][0m Trial 22 finished with value: 0.032104094305055605 and parameters: {'observation_period_num': 5, 'train_rates': 0.9529334501927517, 'learning_rate': 0.00043430854105200794, 'batch_size': 70, 'step_size': 5, 'gamma': 0.8600599348381972}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 22:56:44,028][0m Trial 23 finished with value: 0.041744292011627784 and parameters: {'observation_period_num': 30, 'train_rates': 0.9557165329181208, 'learning_rate': 0.0005527050709226555, 'batch_size': 71, 'step_size': 4, 'gamma': 0.8456334102227112}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:01:31,924][0m Trial 24 finished with value: 0.04743293276804639 and parameters: {'observation_period_num': 22, 'train_rates': 0.9567103008561199, 'learning_rate': 0.00010256535844429286, 'batch_size': 35, 'step_size': 7, 'gamma': 0.8707267911157713}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:05:19,602][0m Trial 25 finished with value: 0.05432818582927694 and parameters: {'observation_period_num': 56, 'train_rates': 0.915485339058483, 'learning_rate': 0.0009206959854708777, 'batch_size': 43, 'step_size': 4, 'gamma': 0.837192080950885}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:07:33,764][0m Trial 26 finished with value: 0.21892718723938945 and parameters: {'observation_period_num': 78, 'train_rates': 0.6866481453172804, 'learning_rate': 0.0004251252571602826, 'batch_size': 73, 'step_size': 9, 'gamma': 0.8801616388312581}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:09:44,956][0m Trial 27 finished with value: 0.07651147246360779 and parameters: {'observation_period_num': 27, 'train_rates': 0.9639385054932826, 'learning_rate': 1.413664634046639e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.8070135287263636}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:11:52,618][0m Trial 28 finished with value: 0.06876545189653785 and parameters: {'observation_period_num': 114, 'train_rates': 0.9277914363728951, 'learning_rate': 0.00012017725045601732, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8625841785904999}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:13:37,346][0m Trial 29 finished with value: 0.12340403788061027 and parameters: {'observation_period_num': 168, 'train_rates': 0.7913870420692654, 'learning_rate': 0.0005117174731501271, 'batch_size': 162, 'step_size': 9, 'gamma': 0.77455872203178}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:15:50,425][0m Trial 30 finished with value: 0.052221169598076655 and parameters: {'observation_period_num': 50, 'train_rates': 0.8416125939681351, 'learning_rate': 5.329903285771705e-05, 'batch_size': 78, 'step_size': 5, 'gamma': 0.9001554287456958}. Best is trial 13 with value: 0.032032351940870285.[0m
[32m[I 2025-01-07 23:17:56,602][0m Trial 31 finished with value: 0.030983653158616665 and parameters: {'observation_period_num': 8, 'train_rates': 0.8863242316081515, 'learning_rate': 0.0002327530446652938, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9105505052747357}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:20:11,944][0m Trial 32 finished with value: 0.05800255909562111 and parameters: {'observation_period_num': 24, 'train_rates': 0.8979033323109951, 'learning_rate': 0.0003387169891602559, 'batch_size': 105, 'step_size': 7, 'gamma': 0.9583192145450525}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:23:09,095][0m Trial 33 finished with value: 0.050033504179177374 and parameters: {'observation_period_num': 38, 'train_rates': 0.9287680907232592, 'learning_rate': 0.0009566533057794672, 'batch_size': 57, 'step_size': 3, 'gamma': 0.8279450779701967}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:25:00,859][0m Trial 34 finished with value: 0.1909503936767578 and parameters: {'observation_period_num': 6, 'train_rates': 0.9654286118449601, 'learning_rate': 1.0808437641396734e-06, 'batch_size': 125, 'step_size': 5, 'gamma': 0.9039594594833545}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:26:49,388][0m Trial 35 finished with value: 0.046516883643153976 and parameters: {'observation_period_num': 23, 'train_rates': 0.9334406239969422, 'learning_rate': 0.0001838571138551135, 'batch_size': 145, 'step_size': 3, 'gamma': 0.8800433672319186}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:28:58,426][0m Trial 36 finished with value: 0.06449737132770614 and parameters: {'observation_period_num': 55, 'train_rates': 0.9039082722060049, 'learning_rate': 0.0005696607859888615, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9240812646418752}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:32:56,610][0m Trial 37 finished with value: 0.10099380037614278 and parameters: {'observation_period_num': 143, 'train_rates': 0.9679255491409259, 'learning_rate': 0.0002710701687259167, 'batch_size': 39, 'step_size': 3, 'gamma': 0.8570599243257948}. Best is trial 31 with value: 0.030983653158616665.[0m
Early stopping at epoch 70
[32m[I 2025-01-07 23:34:30,314][0m Trial 38 finished with value: 0.165983803751992 and parameters: {'observation_period_num': 179, 'train_rates': 0.9409165955111098, 'learning_rate': 0.00016869024277461168, 'batch_size': 102, 'step_size': 1, 'gamma': 0.8348391598800565}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:36:21,444][0m Trial 39 finished with value: 0.06592372441806164 and parameters: {'observation_period_num': 35, 'train_rates': 0.8650067044825894, 'learning_rate': 0.0005766030509517457, 'batch_size': 120, 'step_size': 10, 'gamma': 0.81728106664777}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:38:06,910][0m Trial 40 finished with value: 0.0338534784222406 and parameters: {'observation_period_num': 20, 'train_rates': 0.8928289548594471, 'learning_rate': 0.0002525533216997976, 'batch_size': 142, 'step_size': 6, 'gamma': 0.9411446268802228}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:40:12,668][0m Trial 41 finished with value: 0.032368560807791144 and parameters: {'observation_period_num': 15, 'train_rates': 0.892019579832278, 'learning_rate': 0.0002559308389203811, 'batch_size': 139, 'step_size': 6, 'gamma': 0.9459656483645248}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:42:07,312][0m Trial 42 finished with value: 0.04173140182959295 and parameters: {'observation_period_num': 17, 'train_rates': 0.8077967481848448, 'learning_rate': 0.00013521020038187274, 'batch_size': 143, 'step_size': 5, 'gamma': 0.9510589988051152}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:43:42,788][0m Trial 43 finished with value: 0.05177505428622837 and parameters: {'observation_period_num': 47, 'train_rates': 0.8881298564444622, 'learning_rate': 7.71696982745535e-05, 'batch_size': 166, 'step_size': 6, 'gamma': 0.9886896777523729}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:45:13,346][0m Trial 44 finished with value: 0.03764949736479145 and parameters: {'observation_period_num': 23, 'train_rates': 0.8476812535747668, 'learning_rate': 0.00024654068508689705, 'batch_size': 194, 'step_size': 6, 'gamma': 0.9347130362665811}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:46:59,665][0m Trial 45 finished with value: 0.05902792107711432 and parameters: {'observation_period_num': 61, 'train_rates': 0.8253334957153193, 'learning_rate': 7.483672373356779e-05, 'batch_size': 137, 'step_size': 8, 'gamma': 0.9434443339220129}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:48:31,557][0m Trial 46 finished with value: 0.046114637096832084 and parameters: {'observation_period_num': 16, 'train_rates': 0.9073791843962855, 'learning_rate': 4.53715420209314e-05, 'batch_size': 183, 'step_size': 2, 'gamma': 0.9650190502130156}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:50:00,750][0m Trial 47 finished with value: 0.0517724181518578 and parameters: {'observation_period_num': 34, 'train_rates': 0.8599610389583694, 'learning_rate': 0.0002720953184290834, 'batch_size': 242, 'step_size': 4, 'gamma': 0.9211548213066805}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:51:24,817][0m Trial 48 finished with value: 0.24406281654940215 and parameters: {'observation_period_num': 72, 'train_rates': 0.7411482010128795, 'learning_rate': 1.7152515722076763e-05, 'batch_size': 150, 'step_size': 8, 'gamma': 0.8925364660909881}. Best is trial 31 with value: 0.030983653158616665.[0m
[32m[I 2025-01-07 23:53:25,259][0m Trial 49 finished with value: 0.03612000440572364 and parameters: {'observation_period_num': 5, 'train_rates': 0.8846244225637322, 'learning_rate': 0.0003966035408500655, 'batch_size': 84, 'step_size': 5, 'gamma': 0.9447793044552336}. Best is trial 31 with value: 0.030983653158616665.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 23:53:25,269][0m A new study created in memory with name: no-name-56b85da0-106c-4fcd-b894-7487ae33cd94[0m
[32m[I 2025-01-07 23:54:41,688][0m Trial 0 finished with value: 0.5897686163116904 and parameters: {'observation_period_num': 251, 'train_rates': 0.6857297629116915, 'learning_rate': 2.2977665907128654e-06, 'batch_size': 138, 'step_size': 12, 'gamma': 0.7624210894722327}. Best is trial 0 with value: 0.5897686163116904.[0m
[32m[I 2025-01-07 23:56:05,583][0m Trial 1 finished with value: 0.2904534982310401 and parameters: {'observation_period_num': 231, 'train_rates': 0.7522360038210602, 'learning_rate': 2.342268091179353e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.9343802085858344}. Best is trial 1 with value: 0.2904534982310401.[0m
[32m[I 2025-01-07 23:57:23,371][0m Trial 2 finished with value: 0.27927522142417727 and parameters: {'observation_period_num': 208, 'train_rates': 0.7086696859025684, 'learning_rate': 0.0003532488859443667, 'batch_size': 183, 'step_size': 4, 'gamma': 0.7845403499192286}. Best is trial 2 with value: 0.27927522142417727.[0m
[32m[I 2025-01-07 23:59:10,485][0m Trial 3 finished with value: 0.34134102276727263 and parameters: {'observation_period_num': 126, 'train_rates': 0.7657686092831335, 'learning_rate': 5.833027888547382e-05, 'batch_size': 143, 'step_size': 1, 'gamma': 0.9064803455850032}. Best is trial 2 with value: 0.27927522142417727.[0m
[32m[I 2025-01-08 00:01:52,629][0m Trial 4 finished with value: 0.1034333582320613 and parameters: {'observation_period_num': 35, 'train_rates': 0.8857344538696177, 'learning_rate': 1.7758101993372354e-06, 'batch_size': 62, 'step_size': 14, 'gamma': 0.9207085525512705}. Best is trial 4 with value: 0.1034333582320613.[0m
Early stopping at epoch 52
[32m[I 2025-01-08 00:02:36,524][0m Trial 5 finished with value: 0.37445008945554886 and parameters: {'observation_period_num': 89, 'train_rates': 0.6294734584358987, 'learning_rate': 7.906775280626885e-05, 'batch_size': 196, 'step_size': 1, 'gamma': 0.7746184031108129}. Best is trial 4 with value: 0.1034333582320613.[0m
[32m[I 2025-01-08 00:03:57,993][0m Trial 6 finished with value: 0.26414396470635715 and parameters: {'observation_period_num': 160, 'train_rates': 0.7431307515432782, 'learning_rate': 3.6489505286031246e-05, 'batch_size': 142, 'step_size': 8, 'gamma': 0.8847053043812227}. Best is trial 4 with value: 0.1034333582320613.[0m
[32m[I 2025-01-08 00:07:08,995][0m Trial 7 finished with value: 0.0799660537257252 and parameters: {'observation_period_num': 64, 'train_rates': 0.9138940678048382, 'learning_rate': 0.0004387111816385204, 'batch_size': 48, 'step_size': 6, 'gamma': 0.928527433088518}. Best is trial 7 with value: 0.0799660537257252.[0m
[32m[I 2025-01-08 00:09:05,700][0m Trial 8 finished with value: 0.20660651011770123 and parameters: {'observation_period_num': 195, 'train_rates': 0.7891705624071583, 'learning_rate': 1.555915893950856e-05, 'batch_size': 235, 'step_size': 6, 'gamma': 0.820102021814121}. Best is trial 7 with value: 0.0799660537257252.[0m
[32m[I 2025-01-08 00:11:09,509][0m Trial 9 finished with value: 0.15945103764533997 and parameters: {'observation_period_num': 179, 'train_rates': 0.9676953997331614, 'learning_rate': 0.0006630464123936923, 'batch_size': 225, 'step_size': 6, 'gamma': 0.9860853797085115}. Best is trial 7 with value: 0.0799660537257252.[0m
[32m[I 2025-01-08 00:19:54,480][0m Trial 10 finished with value: 0.03943058205073449 and parameters: {'observation_period_num': 10, 'train_rates': 0.886376972242189, 'learning_rate': 0.00018159134551481472, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9820195575676803}. Best is trial 10 with value: 0.03943058205073449.[0m
[32m[I 2025-01-08 00:24:05,387][0m Trial 11 finished with value: 0.03207873247437558 and parameters: {'observation_period_num': 10, 'train_rates': 0.9000946795412689, 'learning_rate': 0.00025995527027535925, 'batch_size': 36, 'step_size': 11, 'gamma': 0.9887596287104837}. Best is trial 11 with value: 0.03207873247437558.[0m
[32m[I 2025-01-08 00:31:43,403][0m Trial 12 finished with value: 0.0373406814903302 and parameters: {'observation_period_num': 15, 'train_rates': 0.8557581298637535, 'learning_rate': 0.00016361338450762106, 'batch_size': 19, 'step_size': 11, 'gamma': 0.9891773469559405}. Best is trial 11 with value: 0.03207873247437558.[0m
[32m[I 2025-01-08 00:33:39,593][0m Trial 13 finished with value: 0.03158237563639942 and parameters: {'observation_period_num': 10, 'train_rates': 0.8402236476631473, 'learning_rate': 0.0002016341515940138, 'batch_size': 80, 'step_size': 11, 'gamma': 0.9566296039517822}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:35:58,253][0m Trial 14 finished with value: 0.06023883526932022 and parameters: {'observation_period_num': 60, 'train_rates': 0.8277610347244465, 'learning_rate': 0.0008926656719458069, 'batch_size': 82, 'step_size': 9, 'gamma': 0.9543787079788814}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:38:16,335][0m Trial 15 finished with value: 0.11459193378686905 and parameters: {'observation_period_num': 101, 'train_rates': 0.9728642059643886, 'learning_rate': 7.840237975064625e-06, 'batch_size': 93, 'step_size': 13, 'gamma': 0.955220420917002}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:41:45,342][0m Trial 16 finished with value: 0.10599150597339585 and parameters: {'observation_period_num': 43, 'train_rates': 0.9280603349814538, 'learning_rate': 0.00017308935984671788, 'batch_size': 47, 'step_size': 9, 'gamma': 0.8447648753440122}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:44:08,238][0m Trial 17 finished with value: 0.03356454956782869 and parameters: {'observation_period_num': 7, 'train_rates': 0.8309656980364456, 'learning_rate': 9.464856371480843e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8810415140832885}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:46:32,537][0m Trial 18 finished with value: 0.15976547624015622 and parameters: {'observation_period_num': 88, 'train_rates': 0.8224645079818622, 'learning_rate': 0.0003121092599260103, 'batch_size': 67, 'step_size': 13, 'gamma': 0.9547208537155211}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:50:27,497][0m Trial 19 finished with value: 0.09228753068372217 and parameters: {'observation_period_num': 132, 'train_rates': 0.9325656300964165, 'learning_rate': 7.1023665643231204e-06, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9650056703485593}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:52:23,403][0m Trial 20 finished with value: 0.04432278405198462 and parameters: {'observation_period_num': 38, 'train_rates': 0.8583809003678781, 'learning_rate': 0.00012213635612495568, 'batch_size': 115, 'step_size': 11, 'gamma': 0.9053971349545713}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:54:30,118][0m Trial 21 finished with value: 0.03546211664786818 and parameters: {'observation_period_num': 15, 'train_rates': 0.8187405171496761, 'learning_rate': 8.036017382095705e-05, 'batch_size': 89, 'step_size': 10, 'gamma': 0.8685379544606058}. Best is trial 13 with value: 0.03158237563639942.[0m
[32m[I 2025-01-08 00:56:35,789][0m Trial 22 finished with value: 0.030570832436481617 and parameters: {'observation_period_num': 9, 'train_rates': 0.8641263875085698, 'learning_rate': 0.0002986021608696033, 'batch_size': 115, 'step_size': 8, 'gamma': 0.835195445457884}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 00:58:27,318][0m Trial 23 finished with value: 0.05222098173986938 and parameters: {'observation_period_num': 59, 'train_rates': 0.8801962988754454, 'learning_rate': 0.00025616764284377687, 'batch_size': 167, 'step_size': 7, 'gamma': 0.810005988692194}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:00:49,048][0m Trial 24 finished with value: 0.04965196247212589 and parameters: {'observation_period_num': 34, 'train_rates': 0.9123681844951493, 'learning_rate': 0.0005058929389771777, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8520572133326493}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:04:48,200][0m Trial 25 finished with value: 0.07527607551483965 and parameters: {'observation_period_num': 72, 'train_rates': 0.7909135205113527, 'learning_rate': 0.0009707124579737821, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8220752644040548}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:06:54,995][0m Trial 26 finished with value: 0.05506794511967776 and parameters: {'observation_period_num': 29, 'train_rates': 0.9498691325899449, 'learning_rate': 3.889568257054442e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.968552033406679}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:08:36,256][0m Trial 27 finished with value: 0.04530870614155215 and parameters: {'observation_period_num': 47, 'train_rates': 0.8652710361357503, 'learning_rate': 0.00029619147945407937, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8424084900686112}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:10:27,115][0m Trial 28 finished with value: 0.035479772699010724 and parameters: {'observation_period_num': 22, 'train_rates': 0.9072612404411107, 'learning_rate': 0.0005569093891495607, 'batch_size': 127, 'step_size': 4, 'gamma': 0.9396253145838831}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:12:46,824][0m Trial 29 finished with value: 0.13699250784589023 and parameters: {'observation_period_num': 252, 'train_rates': 0.8483663139251857, 'learning_rate': 0.00021091264075372515, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8069717931768068}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:14:22,233][0m Trial 30 finished with value: 0.18206145213200495 and parameters: {'observation_period_num': 6, 'train_rates': 0.6606614072964552, 'learning_rate': 3.599611753216837e-06, 'batch_size': 98, 'step_size': 10, 'gamma': 0.9010377220886826}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:16:07,317][0m Trial 31 finished with value: 0.03660939735650775 and parameters: {'observation_period_num': 7, 'train_rates': 0.8323888137201225, 'learning_rate': 0.00011128224053212945, 'batch_size': 98, 'step_size': 10, 'gamma': 0.7522880003057897}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:17:42,190][0m Trial 32 finished with value: 0.04280381943199703 and parameters: {'observation_period_num': 24, 'train_rates': 0.8103548455425197, 'learning_rate': 0.00010738888889519808, 'batch_size': 107, 'step_size': 13, 'gamma': 0.8657679269427593}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:19:27,898][0m Trial 33 finished with value: 0.19383159690208743 and parameters: {'observation_period_num': 49, 'train_rates': 0.7661853021253393, 'learning_rate': 5.335530904273611e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8861086990613165}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:20:49,710][0m Trial 34 finished with value: 0.17148159960603743 and parameters: {'observation_period_num': 5, 'train_rates': 0.726859522002796, 'learning_rate': 2.3911316789387425e-05, 'batch_size': 153, 'step_size': 12, 'gamma': 0.8360939898785119}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:22:28,839][0m Trial 35 finished with value: 0.10027411951435895 and parameters: {'observation_period_num': 77, 'train_rates': 0.8904159373761529, 'learning_rate': 0.0003643844570265203, 'batch_size': 127, 'step_size': 10, 'gamma': 0.9400780455464488}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:25:14,307][0m Trial 36 finished with value: 0.043251854157967215 and parameters: {'observation_period_num': 25, 'train_rates': 0.7984017017502159, 'learning_rate': 7.103628836906664e-05, 'batch_size': 53, 'step_size': 8, 'gamma': 0.7805318251350482}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:26:56,886][0m Trial 37 finished with value: 0.09891673406866115 and parameters: {'observation_period_num': 233, 'train_rates': 0.84608252034518, 'learning_rate': 0.00014160613431063871, 'batch_size': 106, 'step_size': 11, 'gamma': 0.7933422139624261}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:28:35,844][0m Trial 38 finished with value: 0.24849002351060168 and parameters: {'observation_period_num': 107, 'train_rates': 0.7701161302881037, 'learning_rate': 0.0002254008793486792, 'batch_size': 134, 'step_size': 14, 'gamma': 0.9713000513358642}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:33:53,949][0m Trial 39 finished with value: 0.0884970428376663 and parameters: {'observation_period_num': 143, 'train_rates': 0.8689972255569997, 'learning_rate': 5.281366012889783e-05, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8839215683777784}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:36:31,629][0m Trial 40 finished with value: 0.0784593122780964 and parameters: {'observation_period_num': 40, 'train_rates': 0.9043187527487975, 'learning_rate': 0.0004013604156571264, 'batch_size': 60, 'step_size': 7, 'gamma': 0.9175294723108706}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:38:19,009][0m Trial 41 finished with value: 0.14933266649977056 and parameters: {'observation_period_num': 19, 'train_rates': 0.8071738119879551, 'learning_rate': 1.057903687285362e-06, 'batch_size': 91, 'step_size': 10, 'gamma': 0.8641865217975935}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:40:14,740][0m Trial 42 finished with value: 0.03320760054841088 and parameters: {'observation_period_num': 18, 'train_rates': 0.8430447009967866, 'learning_rate': 8.570106114308686e-05, 'batch_size': 83, 'step_size': 11, 'gamma': 0.8287395269294112}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:42:11,084][0m Trial 43 finished with value: 0.050280418347668006 and parameters: {'observation_period_num': 55, 'train_rates': 0.8343013691733235, 'learning_rate': 0.00010939726061094618, 'batch_size': 112, 'step_size': 11, 'gamma': 0.8295892116666276}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:44:19,795][0m Trial 44 finished with value: 0.05126511002890766 and parameters: {'observation_period_num': 22, 'train_rates': 0.8772121586327627, 'learning_rate': 0.0006582675500007955, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8551414628788098}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:45:56,498][0m Trial 45 finished with value: 0.04705036263312063 and parameters: {'observation_period_num': 34, 'train_rates': 0.8938929709582587, 'learning_rate': 8.369462141325538e-05, 'batch_size': 206, 'step_size': 14, 'gamma': 0.808172818954609}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:48:11,907][0m Trial 46 finished with value: 0.04566589298404076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9403679514990321, 'learning_rate': 3.533432306957002e-05, 'batch_size': 151, 'step_size': 9, 'gamma': 0.9803313893239451}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:50:38,805][0m Trial 47 finished with value: 0.18303788852104 and parameters: {'observation_period_num': 16, 'train_rates': 0.7824489104342515, 'learning_rate': 0.00016852187533349417, 'batch_size': 85, 'step_size': 2, 'gamma': 0.878858577624843}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:53:42,718][0m Trial 48 finished with value: 0.06672395586569219 and parameters: {'observation_period_num': 72, 'train_rates': 0.8441717663468911, 'learning_rate': 0.0002579286960188289, 'batch_size': 49, 'step_size': 8, 'gamma': 0.799130724517161}. Best is trial 22 with value: 0.030570832436481617.[0m
[32m[I 2025-01-08 01:55:13,822][0m Trial 49 finished with value: 0.29122655772748696 and parameters: {'observation_period_num': 186, 'train_rates': 0.7348201632607876, 'learning_rate': 2.255030880394614e-05, 'batch_size': 137, 'step_size': 13, 'gamma': 0.9282268440885572}. Best is trial 22 with value: 0.030570832436481617.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-08 01:55:13,830][0m A new study created in memory with name: no-name-0bd1b609-cf6a-41cb-9952-112ca0f76837[0m
[32m[I 2025-01-08 01:57:09,880][0m Trial 0 finished with value: 0.10712846368551254 and parameters: {'observation_period_num': 137, 'train_rates': 0.9734898820472055, 'learning_rate': 0.00037413977700993914, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8457419197827765}. Best is trial 0 with value: 0.10712846368551254.[0m
[32m[I 2025-01-08 01:59:16,083][0m Trial 1 finished with value: 0.02513555608531262 and parameters: {'observation_period_num': 6, 'train_rates': 0.9203668235458897, 'learning_rate': 0.0006380393225355177, 'batch_size': 76, 'step_size': 9, 'gamma': 0.7567828631707795}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:01:34,385][0m Trial 2 finished with value: 0.08842583264604277 and parameters: {'observation_period_num': 149, 'train_rates': 0.9206607389115271, 'learning_rate': 7.736090847447388e-06, 'batch_size': 65, 'step_size': 3, 'gamma': 0.9648481861620636}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:02:50,900][0m Trial 3 finished with value: 0.12014641046014607 and parameters: {'observation_period_num': 79, 'train_rates': 0.796544689513244, 'learning_rate': 1.1057789402179804e-05, 'batch_size': 154, 'step_size': 3, 'gamma': 0.9279730316006842}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:04:07,265][0m Trial 4 finished with value: 0.2507003015168359 and parameters: {'observation_period_num': 163, 'train_rates': 0.7699687360847199, 'learning_rate': 4.544597412924968e-05, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9653060929847921}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:05:18,999][0m Trial 5 finished with value: 0.5616406780178264 and parameters: {'observation_period_num': 40, 'train_rates': 0.6964835627764463, 'learning_rate': 1.26864455473391e-06, 'batch_size': 239, 'step_size': 12, 'gamma': 0.7757621006799019}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:06:34,669][0m Trial 6 finished with value: 0.37557365000247955 and parameters: {'observation_period_num': 219, 'train_rates': 0.8158627799169422, 'learning_rate': 1.6792837793387453e-06, 'batch_size': 231, 'step_size': 6, 'gamma': 0.9842871307247647}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:07:47,647][0m Trial 7 finished with value: 0.2290187294816687 and parameters: {'observation_period_num': 105, 'train_rates': 0.7788804646408916, 'learning_rate': 0.00010839943003859165, 'batch_size': 247, 'step_size': 14, 'gamma': 0.8487258586020422}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:09:10,835][0m Trial 8 finished with value: 0.6187054734481009 and parameters: {'observation_period_num': 219, 'train_rates': 0.7639315349405178, 'learning_rate': 1.2735159167986556e-06, 'batch_size': 164, 'step_size': 13, 'gamma': 0.8992312424976421}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:14:11,290][0m Trial 9 finished with value: 0.09802858160351807 and parameters: {'observation_period_num': 148, 'train_rates': 0.8111383651704154, 'learning_rate': 1.778195181385692e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.7604170504138984}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:15:57,712][0m Trial 10 finished with value: 0.057555637241240576 and parameters: {'observation_period_num': 7, 'train_rates': 0.9029099156597828, 'learning_rate': 0.000994007354027143, 'batch_size': 102, 'step_size': 10, 'gamma': 0.8089959130373902}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:17:56,108][0m Trial 11 finished with value: 0.035455043753048025 and parameters: {'observation_period_num': 7, 'train_rates': 0.887569126896856, 'learning_rate': 0.0009873007505951266, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8005456079556145}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:19:41,735][0m Trial 12 finished with value: 0.03417257811749597 and parameters: {'observation_period_num': 5, 'train_rates': 0.8887221071832516, 'learning_rate': 0.0008633719181231231, 'batch_size': 108, 'step_size': 10, 'gamma': 0.7901934105116721}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:24:15,431][0m Trial 13 finished with value: 0.05574302588190351 and parameters: {'observation_period_num': 56, 'train_rates': 0.985555154417297, 'learning_rate': 0.0001808410924546314, 'batch_size': 36, 'step_size': 10, 'gamma': 0.7522991678551583}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:26:35,586][0m Trial 14 finished with value: 0.052902596969814863 and parameters: {'observation_period_num': 38, 'train_rates': 0.8542623871086155, 'learning_rate': 0.0003108566648279774, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8045648785016422}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:27:49,197][0m Trial 15 finished with value: 0.17248431557769572 and parameters: {'observation_period_num': 90, 'train_rates': 0.6400534443711872, 'learning_rate': 6.859579755342196e-05, 'batch_size': 124, 'step_size': 15, 'gamma': 0.8355343740969334}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:29:14,798][0m Trial 16 finished with value: 0.031900160014629364 and parameters: {'observation_period_num': 5, 'train_rates': 0.9314819845356413, 'learning_rate': 0.0003831752406743566, 'batch_size': 203, 'step_size': 11, 'gamma': 0.8770907615455941}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:30:57,761][0m Trial 17 finished with value: 0.07146795094013214 and parameters: {'observation_period_num': 58, 'train_rates': 0.9530006802663591, 'learning_rate': 0.0003497070793465066, 'batch_size': 201, 'step_size': 12, 'gamma': 0.901043503195091}. Best is trial 1 with value: 0.02513555608531262.[0m
Early stopping at epoch 95
[32m[I 2025-01-08 02:32:36,089][0m Trial 18 finished with value: 0.1678011119365692 and parameters: {'observation_period_num': 187, 'train_rates': 0.9384436419397217, 'learning_rate': 0.00013733315985109293, 'batch_size': 185, 'step_size': 1, 'gamma': 0.8771437254636666}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:34:53,828][0m Trial 19 finished with value: 0.0662098752627908 and parameters: {'observation_period_num': 35, 'train_rates': 0.8521680410154518, 'learning_rate': 0.00045714559607383824, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8748269247401577}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:36:50,634][0m Trial 20 finished with value: 0.1392517569198456 and parameters: {'observation_period_num': 69, 'train_rates': 0.8483467012202034, 'learning_rate': 3.826733372997018e-06, 'batch_size': 202, 'step_size': 12, 'gamma': 0.9226864848155433}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:38:41,428][0m Trial 21 finished with value: 0.028668526265966266 and parameters: {'observation_period_num': 6, 'train_rates': 0.8840231095780524, 'learning_rate': 0.0006230574493635275, 'batch_size': 138, 'step_size': 9, 'gamma': 0.7782887348852162}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:40:29,241][0m Trial 22 finished with value: 0.03564577792560198 and parameters: {'observation_period_num': 24, 'train_rates': 0.9383029263181919, 'learning_rate': 0.0002168498005941005, 'batch_size': 139, 'step_size': 9, 'gamma': 0.8233296357224094}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:42:00,668][0m Trial 23 finished with value: 0.07969803994423465 and parameters: {'observation_period_num': 100, 'train_rates': 0.8671037538353872, 'learning_rate': 0.0005442251890627404, 'batch_size': 212, 'step_size': 11, 'gamma': 0.7739946120188614}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:43:47,382][0m Trial 24 finished with value: 0.033661288429530825 and parameters: {'observation_period_num': 25, 'train_rates': 0.91794816846017, 'learning_rate': 0.0005680531683317449, 'batch_size': 135, 'step_size': 8, 'gamma': 0.7741031200732934}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:47:13,427][0m Trial 25 finished with value: 0.0772758184466511 and parameters: {'observation_period_num': 52, 'train_rates': 0.9669945359601982, 'learning_rate': 7.92213039670967e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8598423511264975}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:48:56,546][0m Trial 26 finished with value: 0.15532552378613557 and parameters: {'observation_period_num': 21, 'train_rates': 0.7260353377411949, 'learning_rate': 0.00022524457393803055, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8231342965407072}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:50:26,173][0m Trial 27 finished with value: 0.11428433674019436 and parameters: {'observation_period_num': 113, 'train_rates': 0.8790233336012166, 'learning_rate': 2.4898642926789103e-05, 'batch_size': 120, 'step_size': 5, 'gamma': 0.7508712620149807}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:51:44,146][0m Trial 28 finished with value: 0.16775807957935765 and parameters: {'observation_period_num': 249, 'train_rates': 0.8370519796229704, 'learning_rate': 0.0006279036677071727, 'batch_size': 151, 'step_size': 11, 'gamma': 0.8951722327489866}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:53:18,054][0m Trial 29 finished with value: 0.11133094877004623 and parameters: {'observation_period_num': 75, 'train_rates': 0.9885285782557133, 'learning_rate': 0.0004101853346991605, 'batch_size': 180, 'step_size': 13, 'gamma': 0.8374430370877255}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:55:12,869][0m Trial 30 finished with value: 0.07455618858945613 and parameters: {'observation_period_num': 123, 'train_rates': 0.9137792031045217, 'learning_rate': 0.0002649179562122416, 'batch_size': 87, 'step_size': 9, 'gamma': 0.7857409882277857}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:57:20,814][0m Trial 31 finished with value: 0.035368850334218487 and parameters: {'observation_period_num': 25, 'train_rates': 0.9316109099251667, 'learning_rate': 0.0006167328254495762, 'batch_size': 137, 'step_size': 7, 'gamma': 0.7727807249552275}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 02:59:13,980][0m Trial 32 finished with value: 0.036226533353328705 and parameters: {'observation_period_num': 19, 'train_rates': 0.9588178708405046, 'learning_rate': 0.0006523803460688931, 'batch_size': 218, 'step_size': 8, 'gamma': 0.7643152214556339}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:01:03,643][0m Trial 33 finished with value: 0.04763918209780929 and parameters: {'observation_period_num': 42, 'train_rates': 0.9042332923187757, 'learning_rate': 0.00013848010445849406, 'batch_size': 191, 'step_size': 11, 'gamma': 0.7917894687733293}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:04:30,358][0m Trial 34 finished with value: 0.037909261638023814 and parameters: {'observation_period_num': 21, 'train_rates': 0.91699101167842, 'learning_rate': 0.0003760040388092201, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9271872089331145}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:06:34,897][0m Trial 35 finished with value: 0.040472137133280434 and parameters: {'observation_period_num': 7, 'train_rates': 0.9493542357841016, 'learning_rate': 5.371634874778813e-05, 'batch_size': 122, 'step_size': 7, 'gamma': 0.859141231392571}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:08:28,382][0m Trial 36 finished with value: 0.04891699988505628 and parameters: {'observation_period_num': 44, 'train_rates': 0.8264832881520255, 'learning_rate': 0.0006871725350439121, 'batch_size': 157, 'step_size': 9, 'gamma': 0.7806896988529173}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:10:11,657][0m Trial 37 finished with value: 0.266120617140512 and parameters: {'observation_period_num': 32, 'train_rates': 0.6033884654091946, 'learning_rate': 5.16285754983451e-06, 'batch_size': 171, 'step_size': 8, 'gamma': 0.8231299319546239}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:11:53,023][0m Trial 38 finished with value: 0.06100982427597046 and parameters: {'observation_period_num': 59, 'train_rates': 0.9224454798847027, 'learning_rate': 0.0001547642536460032, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9445757194400398}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:13:33,687][0m Trial 39 finished with value: 0.06192766915520896 and parameters: {'observation_period_num': 82, 'train_rates': 0.8721536557845946, 'learning_rate': 9.280011969426976e-05, 'batch_size': 232, 'step_size': 11, 'gamma': 0.7655098972914351}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:20:57,480][0m Trial 40 finished with value: 0.03919751387440697 and parameters: {'observation_period_num': 17, 'train_rates': 0.797729638577741, 'learning_rate': 0.00030697434861152144, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8122691635253142}. Best is trial 1 with value: 0.02513555608531262.[0m
[32m[I 2025-01-08 03:22:55,333][0m Trial 41 finished with value: 0.02483880501182102 and parameters: {'observation_period_num': 5, 'train_rates': 0.8919408098686068, 'learning_rate': 0.0009404589799346525, 'batch_size': 104, 'step_size': 10, 'gamma': 0.7970746054109646}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:25:04,946][0m Trial 42 finished with value: 0.027847857808306713 and parameters: {'observation_period_num': 14, 'train_rates': 0.8988127342173549, 'learning_rate': 0.0005081117929254484, 'batch_size': 93, 'step_size': 10, 'gamma': 0.7922681362716041}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:27:10,411][0m Trial 43 finished with value: 0.030377813651482503 and parameters: {'observation_period_num': 5, 'train_rates': 0.8877521541550542, 'learning_rate': 0.0008558552548367242, 'batch_size': 89, 'step_size': 10, 'gamma': 0.7955270977222124}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:29:15,210][0m Trial 44 finished with value: 0.033870399813928635 and parameters: {'observation_period_num': 12, 'train_rates': 0.8960422127202226, 'learning_rate': 0.000852621399351664, 'batch_size': 91, 'step_size': 10, 'gamma': 0.7976347041090095}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:31:31,835][0m Trial 45 finished with value: 0.07666827996774596 and parameters: {'observation_period_num': 31, 'train_rates': 0.8660626826295238, 'learning_rate': 0.0007468021440207697, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8124096002810328}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:33:01,649][0m Trial 46 finished with value: 0.11779881082475185 and parameters: {'observation_period_num': 177, 'train_rates': 0.8936040957505026, 'learning_rate': 0.0004699665700214874, 'batch_size': 111, 'step_size': 12, 'gamma': 0.7583743297248905}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:35:46,656][0m Trial 47 finished with value: 0.05219851918950044 and parameters: {'observation_period_num': 47, 'train_rates': 0.8217782277961619, 'learning_rate': 0.0009477250980684125, 'batch_size': 53, 'step_size': 10, 'gamma': 0.7863589506992271}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:37:25,275][0m Trial 48 finished with value: 0.20109433658754292 and parameters: {'observation_period_num': 15, 'train_rates': 0.786991647618232, 'learning_rate': 1.1545486541608745e-05, 'batch_size': 101, 'step_size': 9, 'gamma': 0.7962154051503733}. Best is trial 41 with value: 0.02483880501182102.[0m
[32m[I 2025-01-08 03:39:56,346][0m Trial 49 finished with value: 0.20152396312980747 and parameters: {'observation_period_num': 63, 'train_rates': 0.7558640829149107, 'learning_rate': 0.0002323392089368686, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8073821606204156}. Best is trial 41 with value: 0.02483880501182102.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-08 03:39:56,356][0m A new study created in memory with name: no-name-1c3379ba-4072-428b-b7ac-b5304e3b29c8[0m
[32m[I 2025-01-08 03:41:31,463][0m Trial 0 finished with value: 0.3837573145491636 and parameters: {'observation_period_num': 181, 'train_rates': 0.8772786354889625, 'learning_rate': 1.042245866999138e-06, 'batch_size': 146, 'step_size': 8, 'gamma': 0.930611207954672}. Best is trial 0 with value: 0.3837573145491636.[0m
[32m[I 2025-01-08 03:42:55,990][0m Trial 1 finished with value: 0.27617998827587475 and parameters: {'observation_period_num': 182, 'train_rates': 0.6626437215420486, 'learning_rate': 0.00014025367069112602, 'batch_size': 98, 'step_size': 3, 'gamma': 0.7554757224752591}. Best is trial 1 with value: 0.27617998827587475.[0m
[32m[I 2025-01-08 03:44:05,864][0m Trial 2 finished with value: 0.3275103752414116 and parameters: {'observation_period_num': 241, 'train_rates': 0.7283371662176888, 'learning_rate': 9.489096304761052e-05, 'batch_size': 216, 'step_size': 15, 'gamma': 0.8708001530420293}. Best is trial 1 with value: 0.27617998827587475.[0m
[32m[I 2025-01-08 03:45:23,682][0m Trial 3 finished with value: 0.32636048686350877 and parameters: {'observation_period_num': 142, 'train_rates': 0.8344155174955901, 'learning_rate': 3.067048994319276e-06, 'batch_size': 201, 'step_size': 5, 'gamma': 0.8434743694704512}. Best is trial 1 with value: 0.27617998827587475.[0m
[32m[I 2025-01-08 03:48:14,552][0m Trial 4 finished with value: 0.3120335851152594 and parameters: {'observation_period_num': 227, 'train_rates': 0.6966164910243842, 'learning_rate': 1.0985500012896953e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9603084126224373}. Best is trial 1 with value: 0.27617998827587475.[0m
[32m[I 2025-01-08 03:49:43,001][0m Trial 5 finished with value: 0.34505857046526545 and parameters: {'observation_period_num': 186, 'train_rates': 0.6294313646264268, 'learning_rate': 1.3006091826044456e-05, 'batch_size': 85, 'step_size': 5, 'gamma': 0.8887646189023897}. Best is trial 1 with value: 0.27617998827587475.[0m
[32m[I 2025-01-08 03:50:57,527][0m Trial 6 finished with value: 0.1608607573788862 and parameters: {'observation_period_num': 25, 'train_rates': 0.6680042343560303, 'learning_rate': 1.8479289364631225e-05, 'batch_size': 187, 'step_size': 9, 'gamma': 0.9641309745083129}. Best is trial 6 with value: 0.1608607573788862.[0m
[32m[I 2025-01-08 03:52:04,737][0m Trial 7 finished with value: 0.37090838561623785 and parameters: {'observation_period_num': 248, 'train_rates': 0.6510656309054381, 'learning_rate': 4.8813151889099085e-05, 'batch_size': 230, 'step_size': 10, 'gamma': 0.9202406021628151}. Best is trial 6 with value: 0.1608607573788862.[0m
[32m[I 2025-01-08 03:53:31,937][0m Trial 8 finished with value: 0.4491567621485653 and parameters: {'observation_period_num': 204, 'train_rates': 0.6458659451910639, 'learning_rate': 2.7939923247657067e-06, 'batch_size': 121, 'step_size': 11, 'gamma': 0.9588841821604751}. Best is trial 6 with value: 0.1608607573788862.[0m
[32m[I 2025-01-08 03:55:10,835][0m Trial 9 finished with value: 0.44030856905561505 and parameters: {'observation_period_num': 125, 'train_rates': 0.731843554605007, 'learning_rate': 7.159160588217702e-06, 'batch_size': 164, 'step_size': 3, 'gamma': 0.8701633221518814}. Best is trial 6 with value: 0.1608607573788862.[0m
[32m[I 2025-01-08 03:57:00,165][0m Trial 10 finished with value: 0.0344756543636322 and parameters: {'observation_period_num': 8, 'train_rates': 0.9691778786898739, 'learning_rate': 0.0008926420118765701, 'batch_size': 251, 'step_size': 14, 'gamma': 0.8081441125385617}. Best is trial 10 with value: 0.0344756543636322.[0m
[32m[I 2025-01-08 03:58:46,799][0m Trial 11 finished with value: 0.03434326872229576 and parameters: {'observation_period_num': 16, 'train_rates': 0.9620511867418565, 'learning_rate': 0.0006712530745497508, 'batch_size': 254, 'step_size': 15, 'gamma': 0.8047468583650608}. Best is trial 11 with value: 0.03434326872229576.[0m
[32m[I 2025-01-08 04:00:33,303][0m Trial 12 finished with value: 0.02985144965350628 and parameters: {'observation_period_num': 19, 'train_rates': 0.9826198388174501, 'learning_rate': 0.000978052381074534, 'batch_size': 253, 'step_size': 15, 'gamma': 0.7987847180883156}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:02:11,765][0m Trial 13 finished with value: 0.04616978392004967 and parameters: {'observation_period_num': 58, 'train_rates': 0.9850918551955508, 'learning_rate': 0.0009800979859327262, 'batch_size': 253, 'step_size': 13, 'gamma': 0.7829995917436111}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:03:48,328][0m Trial 14 finished with value: 0.055041870875320885 and parameters: {'observation_period_num': 68, 'train_rates': 0.9125009181427345, 'learning_rate': 0.0003049467345916505, 'batch_size': 179, 'step_size': 12, 'gamma': 0.8187387473456514}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:05:20,698][0m Trial 15 finished with value: 0.04694985970854759 and parameters: {'observation_period_num': 72, 'train_rates': 0.9259636537729755, 'learning_rate': 0.0003293193119742756, 'batch_size': 226, 'step_size': 15, 'gamma': 0.7528008296860502}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:09:58,128][0m Trial 16 finished with value: 0.07241832131075199 and parameters: {'observation_period_num': 39, 'train_rates': 0.8019693914293604, 'learning_rate': 0.0003917656271418476, 'batch_size': 31, 'step_size': 13, 'gamma': 0.7988273618014611}. Best is trial 12 with value: 0.02985144965350628.[0m
Early stopping at epoch 67
[32m[I 2025-01-08 04:11:29,109][0m Trial 17 finished with value: 0.16388678550720215 and parameters: {'observation_period_num': 100, 'train_rates': 0.9363258090643248, 'learning_rate': 0.0001593238803936483, 'batch_size': 241, 'step_size': 1, 'gamma': 0.8297626064641527}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:13:26,140][0m Trial 18 finished with value: 0.044305045251507004 and parameters: {'observation_period_num': 5, 'train_rates': 0.8654034631304162, 'learning_rate': 5.612387968914735e-05, 'batch_size': 211, 'step_size': 11, 'gamma': 0.7845038359729128}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:15:35,284][0m Trial 19 finished with value: 0.08530747145414352 and parameters: {'observation_period_num': 103, 'train_rates': 0.9548911805727849, 'learning_rate': 0.0005453123643734063, 'batch_size': 169, 'step_size': 15, 'gamma': 0.8493739348640048}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:18:46,747][0m Trial 20 finished with value: 0.06321435455266419 and parameters: {'observation_period_num': 42, 'train_rates': 0.8859187731431835, 'learning_rate': 0.00019344232645372136, 'batch_size': 60, 'step_size': 13, 'gamma': 0.7779501325536254}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:20:56,899][0m Trial 21 finished with value: 0.03149479627609253 and parameters: {'observation_period_num': 12, 'train_rates': 0.9718102270595476, 'learning_rate': 0.0009340831434916518, 'batch_size': 251, 'step_size': 14, 'gamma': 0.8089102382157987}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:22:52,634][0m Trial 22 finished with value: 0.030641360208392143 and parameters: {'observation_period_num': 27, 'train_rates': 0.9789949856246843, 'learning_rate': 0.0006193608858698366, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8370677851515322}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:24:43,411][0m Trial 23 finished with value: 0.03692283481359482 and parameters: {'observation_period_num': 38, 'train_rates': 0.988899317566741, 'learning_rate': 0.0004969611094873842, 'batch_size': 198, 'step_size': 12, 'gamma': 0.8380624626014568}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:26:45,350][0m Trial 24 finished with value: 0.08360267797969792 and parameters: {'observation_period_num': 85, 'train_rates': 0.8982996208217819, 'learning_rate': 0.0009910316886349601, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8561738563309754}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:28:32,301][0m Trial 25 finished with value: 0.07372935116291046 and parameters: {'observation_period_num': 52, 'train_rates': 0.9392994372066076, 'learning_rate': 0.0002686534889027189, 'batch_size': 231, 'step_size': 11, 'gamma': 0.889757979518483}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:30:21,488][0m Trial 26 finished with value: 0.04458297988003735 and parameters: {'observation_period_num': 25, 'train_rates': 0.8490918685474217, 'learning_rate': 8.84068844264263e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8266168553344255}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:32:12,390][0m Trial 27 finished with value: 0.09172786043389984 and parameters: {'observation_period_num': 150, 'train_rates': 0.8033237153027185, 'learning_rate': 0.00054975062366124, 'batch_size': 150, 'step_size': 12, 'gamma': 0.7694447788709994}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:33:34,461][0m Trial 28 finished with value: 0.15881963189224846 and parameters: {'observation_period_num': 26, 'train_rates': 0.6018381971505442, 'learning_rate': 0.00020601267989522575, 'batch_size': 208, 'step_size': 9, 'gamma': 0.9888288928445402}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:35:24,609][0m Trial 29 finished with value: 0.09956638630508452 and parameters: {'observation_period_num': 84, 'train_rates': 0.9104126196525288, 'learning_rate': 2.692755559598665e-05, 'batch_size': 235, 'step_size': 7, 'gamma': 0.7939764450448042}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:37:35,281][0m Trial 30 finished with value: 0.07643514520981733 and parameters: {'observation_period_num': 51, 'train_rates': 0.9475642306410671, 'learning_rate': 0.00045023060279062474, 'batch_size': 117, 'step_size': 14, 'gamma': 0.9029457191134558}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:39:39,989][0m Trial 31 finished with value: 0.03718065842986107 and parameters: {'observation_period_num': 17, 'train_rates': 0.9660832098013197, 'learning_rate': 0.0007825282417419587, 'batch_size': 246, 'step_size': 15, 'gamma': 0.8163548374835973}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:41:25,686][0m Trial 32 finished with value: 0.04419577121734619 and parameters: {'observation_period_num': 5, 'train_rates': 0.9869596422728485, 'learning_rate': 0.0006535970116019722, 'batch_size': 219, 'step_size': 13, 'gamma': 0.8069010006516648}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:43:26,089][0m Trial 33 finished with value: 0.04638473317027092 and parameters: {'observation_period_num': 31, 'train_rates': 0.9629943444826441, 'learning_rate': 0.0006612133527369954, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7694697751674096}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:45:33,860][0m Trial 34 finished with value: 0.03774864971637726 and parameters: {'observation_period_num': 16, 'train_rates': 0.9289209895821585, 'learning_rate': 0.00011069376502600433, 'batch_size': 239, 'step_size': 14, 'gamma': 0.8606510743334227}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:47:22,447][0m Trial 35 finished with value: 0.19221754210342157 and parameters: {'observation_period_num': 55, 'train_rates': 0.766688368204073, 'learning_rate': 0.00029363514970331457, 'batch_size': 194, 'step_size': 15, 'gamma': 0.8344287290469294}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:49:06,227][0m Trial 36 finished with value: 0.33947824860808107 and parameters: {'observation_period_num': 69, 'train_rates': 0.8680901727687392, 'learning_rate': 1.009617514095442e-06, 'batch_size': 214, 'step_size': 13, 'gamma': 0.7966646996820155}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:51:02,111][0m Trial 37 finished with value: 0.03831943832873319 and parameters: {'observation_period_num': 21, 'train_rates': 0.8342420885860278, 'learning_rate': 0.0004176616590605748, 'batch_size': 242, 'step_size': 12, 'gamma': 0.81527655606849}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:52:31,160][0m Trial 38 finished with value: 0.09466121345758438 and parameters: {'observation_period_num': 163, 'train_rates': 0.9676798239341555, 'learning_rate': 0.00022318613670554898, 'batch_size': 221, 'step_size': 10, 'gamma': 0.7668858999222972}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:54:07,515][0m Trial 39 finished with value: 0.06561592567554662 and parameters: {'observation_period_num': 40, 'train_rates': 0.9025551056982851, 'learning_rate': 0.0006739189489750886, 'batch_size': 205, 'step_size': 14, 'gamma': 0.844596550668878}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:56:02,793][0m Trial 40 finished with value: 0.07092341413430128 and parameters: {'observation_period_num': 121, 'train_rates': 0.9256886669428382, 'learning_rate': 0.00012444294626995895, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8242513027680614}. Best is trial 12 with value: 0.02985144965350628.[0m
[32m[I 2025-01-08 04:57:31,317][0m Trial 41 finished with value: 0.028797505423426628 and parameters: {'observation_period_num': 9, 'train_rates': 0.9580658057577542, 'learning_rate': 0.0009608779262942285, 'batch_size': 246, 'step_size': 14, 'gamma': 0.804766940989789}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 04:59:01,387][0m Trial 42 finished with value: 0.03330174461007118 and parameters: {'observation_period_num': 18, 'train_rates': 0.9529378685244558, 'learning_rate': 0.0009509997519025487, 'batch_size': 243, 'step_size': 14, 'gamma': 0.8061493714393955}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:00:40,837][0m Trial 43 finished with value: 0.03867071121931076 and parameters: {'observation_period_num': 31, 'train_rates': 0.9892034144061731, 'learning_rate': 0.000953408993951411, 'batch_size': 237, 'step_size': 13, 'gamma': 0.7896510916877697}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:02:33,692][0m Trial 44 finished with value: 0.19199137389659882 and parameters: {'observation_period_num': 13, 'train_rates': 0.9446402390094403, 'learning_rate': 2.3117914476303103e-06, 'batch_size': 242, 'step_size': 14, 'gamma': 0.8096573152017419}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:04:23,420][0m Trial 45 finished with value: 0.10427311062812805 and parameters: {'observation_period_num': 229, 'train_rates': 0.975866051366908, 'learning_rate': 0.00044192257608249223, 'batch_size': 187, 'step_size': 5, 'gamma': 0.881343215577574}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:06:13,906][0m Trial 46 finished with value: 0.05597234517335892 and parameters: {'observation_period_num': 44, 'train_rates': 0.9552681135153152, 'learning_rate': 0.0007882274547753236, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8373462735551066}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:07:56,716][0m Trial 47 finished with value: 0.036474253971218354 and parameters: {'observation_period_num': 27, 'train_rates': 0.8853629696257623, 'learning_rate': 0.0003723430207324038, 'batch_size': 247, 'step_size': 10, 'gamma': 0.8014131029764837}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:10:01,679][0m Trial 48 finished with value: 0.08404932916164398 and parameters: {'observation_period_num': 201, 'train_rates': 0.9182094901332424, 'learning_rate': 0.0005814348589929593, 'batch_size': 229, 'step_size': 15, 'gamma': 0.7592840217981378}. Best is trial 41 with value: 0.028797505423426628.[0m
[32m[I 2025-01-08 05:11:46,649][0m Trial 49 finished with value: 0.18167598801854742 and parameters: {'observation_period_num': 61, 'train_rates': 0.7031405889780811, 'learning_rate': 0.000959352434669038, 'batch_size': 249, 'step_size': 11, 'gamma': 0.7790031013773339}. Best is trial 41 with value: 0.028797505423426628.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-08 05:11:46,657][0m A new study created in memory with name: no-name-15cb8df7-f0de-423d-8b0b-62631e0bc4a8[0m
Early stopping at epoch 84
[32m[I 2025-01-08 05:13:15,823][0m Trial 0 finished with value: 0.49765748731021225 and parameters: {'observation_period_num': 195, 'train_rates': 0.7374150781052466, 'learning_rate': 1.7761686182934134e-05, 'batch_size': 138, 'step_size': 1, 'gamma': 0.8765318499420208}. Best is trial 0 with value: 0.49765748731021225.[0m
[32m[I 2025-01-08 05:15:06,780][0m Trial 1 finished with value: 0.09936852573740239 and parameters: {'observation_period_num': 174, 'train_rates': 0.9121543002723915, 'learning_rate': 0.0001857019869079108, 'batch_size': 211, 'step_size': 7, 'gamma': 0.8983957551741949}. Best is trial 1 with value: 0.09936852573740239.[0m
[32m[I 2025-01-08 05:16:32,485][0m Trial 2 finished with value: 0.2079092338681221 and parameters: {'observation_period_num': 63, 'train_rates': 0.679180391321827, 'learning_rate': 0.0008003494930247333, 'batch_size': 223, 'step_size': 7, 'gamma': 0.9783950845486077}. Best is trial 1 with value: 0.09936852573740239.[0m
[32m[I 2025-01-08 05:18:35,958][0m Trial 3 finished with value: 0.18607255979441106 and parameters: {'observation_period_num': 60, 'train_rates': 0.6905817958859843, 'learning_rate': 1.567849022207267e-05, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9316610153138772}. Best is trial 1 with value: 0.09936852573740239.[0m
[32m[I 2025-01-08 05:20:21,168][0m Trial 4 finished with value: 0.10942530926256752 and parameters: {'observation_period_num': 195, 'train_rates': 0.8433158241953805, 'learning_rate': 3.645515663071461e-05, 'batch_size': 230, 'step_size': 15, 'gamma': 0.8239840844476646}. Best is trial 1 with value: 0.09936852573740239.[0m
[32m[I 2025-01-08 05:23:22,772][0m Trial 5 finished with value: 0.0978947680016033 and parameters: {'observation_period_num': 159, 'train_rates': 0.9563444030012441, 'learning_rate': 7.954833318641264e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8161506041173047}. Best is trial 5 with value: 0.0978947680016033.[0m
[32m[I 2025-01-08 05:25:03,513][0m Trial 6 finished with value: 0.26051584952618234 and parameters: {'observation_period_num': 189, 'train_rates': 0.6706570365234235, 'learning_rate': 7.35476316522224e-05, 'batch_size': 155, 'step_size': 12, 'gamma': 0.8715807384542585}. Best is trial 5 with value: 0.0978947680016033.[0m
[32m[I 2025-01-08 05:28:18,626][0m Trial 7 finished with value: 0.12919012009584555 and parameters: {'observation_period_num': 184, 'train_rates': 0.7904165665880543, 'learning_rate': 1.2026398267001348e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.9633565004668417}. Best is trial 5 with value: 0.0978947680016033.[0m
[32m[I 2025-01-08 05:30:48,824][0m Trial 8 finished with value: 0.2647206326921648 and parameters: {'observation_period_num': 132, 'train_rates': 0.7357076587588589, 'learning_rate': 0.0004668776267466746, 'batch_size': 59, 'step_size': 11, 'gamma': 0.9391965880358824}. Best is trial 5 with value: 0.0978947680016033.[0m
[32m[I 2025-01-08 05:32:18,922][0m Trial 9 finished with value: 0.283115983200175 and parameters: {'observation_period_num': 154, 'train_rates': 0.6653449483304527, 'learning_rate': 4.582568084299337e-05, 'batch_size': 241, 'step_size': 4, 'gamma': 0.9889429042260132}. Best is trial 5 with value: 0.0978947680016033.[0m
[32m[I 2025-01-08 05:39:46,198][0m Trial 10 finished with value: 0.18589660997797802 and parameters: {'observation_period_num': 237, 'train_rates': 0.9851823754913228, 'learning_rate': 1.4820036397790977e-06, 'batch_size': 20, 'step_size': 11, 'gamma': 0.7551556061773228}. Best is trial 5 with value: 0.0978947680016033.[0m
[32m[I 2025-01-08 05:41:13,414][0m Trial 11 finished with value: 0.0765569657087326 and parameters: {'observation_period_num': 92, 'train_rates': 0.9499756493338896, 'learning_rate': 0.00017425231207463863, 'batch_size': 187, 'step_size': 9, 'gamma': 0.825405621196394}. Best is trial 11 with value: 0.0765569657087326.[0m
[32m[I 2025-01-08 05:43:07,079][0m Trial 12 finished with value: 0.08868249505758286 and parameters: {'observation_period_num': 98, 'train_rates': 0.9856597027584595, 'learning_rate': 0.00010942530687487975, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8117108270972434}. Best is trial 11 with value: 0.0765569657087326.[0m
[32m[I 2025-01-08 05:44:38,599][0m Trial 13 finished with value: 0.0656539856392514 and parameters: {'observation_period_num': 94, 'train_rates': 0.8927518408272098, 'learning_rate': 0.000214567196378488, 'batch_size': 175, 'step_size': 14, 'gamma': 0.8070310364982023}. Best is trial 13 with value: 0.0656539856392514.[0m
[32m[I 2025-01-08 05:46:10,767][0m Trial 14 finished with value: 0.061055156208147096 and parameters: {'observation_period_num': 25, 'train_rates': 0.8872021432275401, 'learning_rate': 0.00029945236915447995, 'batch_size': 179, 'step_size': 15, 'gamma': 0.7663335719078966}. Best is trial 14 with value: 0.061055156208147096.[0m
[32m[I 2025-01-08 05:47:35,391][0m Trial 15 finished with value: 0.031160884044532267 and parameters: {'observation_period_num': 18, 'train_rates': 0.857070881603481, 'learning_rate': 0.0003411365481230825, 'batch_size': 177, 'step_size': 15, 'gamma': 0.7662868799607071}. Best is trial 15 with value: 0.031160884044532267.[0m
[32m[I 2025-01-08 05:49:04,183][0m Trial 16 finished with value: 0.028140343260020018 and parameters: {'observation_period_num': 5, 'train_rates': 0.8400129315408693, 'learning_rate': 0.0009660926548442049, 'batch_size': 118, 'step_size': 13, 'gamma': 0.7527738573253232}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 05:50:34,336][0m Trial 17 finished with value: 0.03217216566554271 and parameters: {'observation_period_num': 8, 'train_rates': 0.8262477262161737, 'learning_rate': 0.0008727312433376282, 'batch_size': 114, 'step_size': 13, 'gamma': 0.7800024039227634}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 05:51:58,504][0m Trial 18 finished with value: 0.26961671001564674 and parameters: {'observation_period_num': 51, 'train_rates': 0.6018160578810271, 'learning_rate': 3.8011360246105627e-06, 'batch_size': 109, 'step_size': 13, 'gamma': 0.7873541592721028}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 05:53:41,784][0m Trial 19 finished with value: 0.03914686181163507 and parameters: {'observation_period_num': 5, 'train_rates': 0.8274472559878848, 'learning_rate': 0.0004551881080825412, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8468248574034578}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 05:55:17,007][0m Trial 20 finished with value: 0.0525291044965829 and parameters: {'observation_period_num': 34, 'train_rates': 0.7925733644529054, 'learning_rate': 0.0008825050938597806, 'batch_size': 198, 'step_size': 13, 'gamma': 0.7501457001074606}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 05:57:16,566][0m Trial 21 finished with value: 0.02912613379008515 and parameters: {'observation_period_num': 5, 'train_rates': 0.8448487065273529, 'learning_rate': 0.000938472997130918, 'batch_size': 113, 'step_size': 13, 'gamma': 0.778523148424269}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 05:59:09,798][0m Trial 22 finished with value: 0.051166655318596065 and parameters: {'observation_period_num': 33, 'train_rates': 0.8670338882433711, 'learning_rate': 0.0006104407427885728, 'batch_size': 96, 'step_size': 12, 'gamma': 0.7846110807522367}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:00:48,806][0m Trial 23 finished with value: 0.1649813675588661 and parameters: {'observation_period_num': 20, 'train_rates': 0.7563451991945773, 'learning_rate': 0.00034136523380278816, 'batch_size': 159, 'step_size': 14, 'gamma': 0.7696651805454069}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:02:31,897][0m Trial 24 finished with value: 0.045944890358567525 and parameters: {'observation_period_num': 43, 'train_rates': 0.8563534717751986, 'learning_rate': 0.000431650752593838, 'batch_size': 119, 'step_size': 11, 'gamma': 0.7931851469226598}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:04:35,198][0m Trial 25 finished with value: 0.06621119198634436 and parameters: {'observation_period_num': 73, 'train_rates': 0.9209810017765081, 'learning_rate': 0.0009886943915338052, 'batch_size': 87, 'step_size': 14, 'gamma': 0.8451779199591352}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:06:21,977][0m Trial 26 finished with value: 0.03477354196056855 and parameters: {'observation_period_num': 7, 'train_rates': 0.8085317633486342, 'learning_rate': 0.0001261594074305106, 'batch_size': 130, 'step_size': 12, 'gamma': 0.7707563839893183}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:08:07,538][0m Trial 27 finished with value: 0.21940771280925656 and parameters: {'observation_period_num': 80, 'train_rates': 0.7648269117449973, 'learning_rate': 0.00028843896862770476, 'batch_size': 162, 'step_size': 4, 'gamma': 0.796166503691783}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:10:00,260][0m Trial 28 finished with value: 0.05141368169676174 and parameters: {'observation_period_num': 46, 'train_rates': 0.8789355111374698, 'learning_rate': 0.0005926744536936001, 'batch_size': 128, 'step_size': 10, 'gamma': 0.843617953080554}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:11:50,967][0m Trial 29 finished with value: 0.11740635388902484 and parameters: {'observation_period_num': 117, 'train_rates': 0.921375559232211, 'learning_rate': 2.307131971816969e-05, 'batch_size': 145, 'step_size': 2, 'gamma': 0.9012957788133533}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:13:54,809][0m Trial 30 finished with value: 0.054450474889573205 and parameters: {'observation_period_num': 23, 'train_rates': 0.8429753697836786, 'learning_rate': 7.685783958625202e-06, 'batch_size': 88, 'step_size': 14, 'gamma': 0.756526808588066}. Best is trial 16 with value: 0.028140343260020018.[0m
[32m[I 2025-01-08 06:15:42,440][0m Trial 31 finished with value: 0.02792538134500357 and parameters: {'observation_period_num': 6, 'train_rates': 0.8144403392117611, 'learning_rate': 0.000996901207073466, 'batch_size': 122, 'step_size': 12, 'gamma': 0.7764229380547076}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:17:35,485][0m Trial 32 finished with value: 0.03509011054213377 and parameters: {'observation_period_num': 18, 'train_rates': 0.8078986796150769, 'learning_rate': 0.0006193858454379917, 'batch_size': 126, 'step_size': 13, 'gamma': 0.7718358130068432}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:19:11,873][0m Trial 33 finished with value: 0.1806478090708042 and parameters: {'observation_period_num': 229, 'train_rates': 0.7790807271072255, 'learning_rate': 0.00022339040568827541, 'batch_size': 205, 'step_size': 15, 'gamma': 0.802323244248137}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:21:09,337][0m Trial 34 finished with value: 0.060804730478623926 and parameters: {'observation_period_num': 60, 'train_rates': 0.8207557156429578, 'learning_rate': 0.0009664837741728738, 'batch_size': 148, 'step_size': 12, 'gamma': 0.7512626522718123}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:23:41,538][0m Trial 35 finished with value: 0.08135382396479447 and parameters: {'observation_period_num': 39, 'train_rates': 0.8498230119191797, 'learning_rate': 0.0006251060438422103, 'batch_size': 73, 'step_size': 14, 'gamma': 0.9010449722575645}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:25:31,433][0m Trial 36 finished with value: 0.03388363676411765 and parameters: {'observation_period_num': 5, 'train_rates': 0.9052218860431025, 'learning_rate': 0.00036552182029270307, 'batch_size': 256, 'step_size': 10, 'gamma': 0.7811166076778591}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:27:06,735][0m Trial 37 finished with value: 0.2846495823830551 and parameters: {'observation_period_num': 213, 'train_rates': 0.7063572141142073, 'learning_rate': 0.0005756591953265965, 'batch_size': 164, 'step_size': 8, 'gamma': 0.8274351893488743}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:29:09,651][0m Trial 38 finished with value: 0.042326487753635796 and parameters: {'observation_period_num': 25, 'train_rates': 0.8670534301271334, 'learning_rate': 0.0001510402019031307, 'batch_size': 217, 'step_size': 13, 'gamma': 0.8570990530477631}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:31:04,586][0m Trial 39 finished with value: 0.18396671861890204 and parameters: {'observation_period_num': 51, 'train_rates': 0.7328718111563377, 'learning_rate': 6.512732105806338e-05, 'batch_size': 137, 'step_size': 15, 'gamma': 0.8847908266482404}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:33:37,027][0m Trial 40 finished with value: 0.06939407512545585 and parameters: {'observation_period_num': 66, 'train_rates': 0.9377428525359407, 'learning_rate': 0.0002603879090321857, 'batch_size': 76, 'step_size': 11, 'gamma': 0.7630644733694567}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:35:34,996][0m Trial 41 finished with value: 0.030001482132394782 and parameters: {'observation_period_num': 13, 'train_rates': 0.8341866924044952, 'learning_rate': 0.0007568963646635731, 'batch_size': 116, 'step_size': 13, 'gamma': 0.7796247059323678}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:37:13,209][0m Trial 42 finished with value: 0.03588006674975598 and parameters: {'observation_period_num': 21, 'train_rates': 0.8388600720514516, 'learning_rate': 0.0007831784163617042, 'batch_size': 97, 'step_size': 12, 'gamma': 0.7777647810701674}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:38:45,976][0m Trial 43 finished with value: 0.03069696410268181 and parameters: {'observation_period_num': 15, 'train_rates': 0.805042215911256, 'learning_rate': 0.00043389685964408435, 'batch_size': 118, 'step_size': 14, 'gamma': 0.7935036036287674}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:40:24,773][0m Trial 44 finished with value: 0.04778896414963626 and parameters: {'observation_period_num': 35, 'train_rates': 0.8035346013752467, 'learning_rate': 0.0006840618195022893, 'batch_size': 118, 'step_size': 13, 'gamma': 0.8174473370267012}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:42:03,272][0m Trial 45 finished with value: 0.17212631763821398 and parameters: {'observation_period_num': 12, 'train_rates': 0.7797659091248146, 'learning_rate': 0.0004417028495180532, 'batch_size': 105, 'step_size': 12, 'gamma': 0.7953245380889038}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:44:30,357][0m Trial 46 finished with value: 0.19019635138813584 and parameters: {'observation_period_num': 34, 'train_rates': 0.7647215485838226, 'learning_rate': 0.0004993640774099077, 'batch_size': 57, 'step_size': 9, 'gamma': 0.8011757731626705}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:46:22,458][0m Trial 47 finished with value: 0.11467076665216899 and parameters: {'observation_period_num': 157, 'train_rates': 0.8222252355725183, 'learning_rate': 0.0009806729375989157, 'batch_size': 93, 'step_size': 6, 'gamma': 0.9228446336880335}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:48:16,552][0m Trial 48 finished with value: 0.5074830179239487 and parameters: {'observation_period_num': 118, 'train_rates': 0.74805933850208, 'learning_rate': 1.07779249013112e-06, 'batch_size': 75, 'step_size': 11, 'gamma': 0.7612421783215757}. Best is trial 31 with value: 0.02792538134500357.[0m
[32m[I 2025-01-08 06:50:11,157][0m Trial 49 finished with value: 0.18696208661649286 and parameters: {'observation_period_num': 57, 'train_rates': 0.71707323201942, 'learning_rate': 0.0007090580025253418, 'batch_size': 128, 'step_size': 14, 'gamma': 0.8114423184866498}. Best is trial 31 with value: 0.02792538134500357.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 15, 'train_rates': 0.9400484084044951, 'learning_rate': 0.00038250459515289255, 'batch_size': 138, 'step_size': 12, 'gamma': 0.9899569891931845}
Epoch 1/300, trend Loss: 0.5783 | 0.3901
Epoch 2/300, trend Loss: 0.5355 | 0.4487
Epoch 3/300, trend Loss: 0.2958 | 0.2301
Epoch 4/300, trend Loss: 0.2204 | 0.2112
Epoch 5/300, trend Loss: 0.1863 | 0.1762
Epoch 6/300, trend Loss: 0.1651 | 0.1710
Epoch 7/300, trend Loss: 0.1709 | 0.1519
Epoch 8/300, trend Loss: 0.1534 | 0.1269
Epoch 9/300, trend Loss: 0.1555 | 0.1146
Epoch 10/300, trend Loss: 0.1468 | 0.1022
Epoch 11/300, trend Loss: 0.1366 | 0.1056
Epoch 12/300, trend Loss: 0.1422 | 0.1109
Epoch 13/300, trend Loss: 0.1659 | 0.2249
Epoch 14/300, trend Loss: 0.1500 | 0.2055
Epoch 15/300, trend Loss: 0.1537 | 0.0977
Epoch 16/300, trend Loss: 0.1461 | 0.1460
Epoch 17/300, trend Loss: 0.1278 | 0.0878
Epoch 18/300, trend Loss: 0.1174 | 0.0898
Epoch 19/300, trend Loss: 0.1315 | 0.0969
Epoch 20/300, trend Loss: 0.1292 | 0.0886
Epoch 21/300, trend Loss: 0.1203 | 0.0750
Epoch 22/300, trend Loss: 0.1335 | 0.0736
Epoch 23/300, trend Loss: 0.1302 | 0.0712
Epoch 24/300, trend Loss: 0.1168 | 0.0735
Epoch 25/300, trend Loss: 0.1148 | 0.0899
Epoch 26/300, trend Loss: 0.1361 | 0.1386
Epoch 27/300, trend Loss: 0.1244 | 0.1120
Epoch 28/300, trend Loss: 0.1212 | 0.0736
Epoch 29/300, trend Loss: 0.1322 | 0.0813
Epoch 30/300, trend Loss: 0.1264 | 0.0971
Epoch 31/300, trend Loss: 0.1282 | 0.1205
Epoch 32/300, trend Loss: 0.1118 | 0.0727
Epoch 33/300, trend Loss: 0.1176 | 0.0645
Epoch 34/300, trend Loss: 0.1073 | 0.0640
Epoch 35/300, trend Loss: 0.1093 | 0.0818
Epoch 36/300, trend Loss: 0.1110 | 0.0882
Epoch 37/300, trend Loss: 0.1046 | 0.0752
Epoch 38/300, trend Loss: 0.1077 | 0.0649
Epoch 39/300, trend Loss: 0.1070 | 0.0595
Epoch 40/300, trend Loss: 0.1069 | 0.0634
Epoch 41/300, trend Loss: 0.1112 | 0.0859
Epoch 42/300, trend Loss: 0.1162 | 0.1172
Epoch 43/300, trend Loss: 0.1079 | 0.0929
Epoch 44/300, trend Loss: 0.1109 | 0.0643
Epoch 45/300, trend Loss: 0.1124 | 0.0605
Epoch 46/300, trend Loss: 0.1060 | 0.0610
Epoch 47/300, trend Loss: 0.1137 | 0.1075
Epoch 48/300, trend Loss: 0.1105 | 0.0929
Epoch 49/300, trend Loss: 0.1028 | 0.0622
Epoch 50/300, trend Loss: 0.1062 | 0.0572
Epoch 51/300, trend Loss: 0.1008 | 0.0561
Epoch 52/300, trend Loss: 0.0979 | 0.0675
Epoch 53/300, trend Loss: 0.1057 | 0.0901
Epoch 54/300, trend Loss: 0.1009 | 0.0806
Epoch 55/300, trend Loss: 0.1005 | 0.0626
Epoch 56/300, trend Loss: 0.1006 | 0.0552
Epoch 57/300, trend Loss: 0.1041 | 0.0588
Epoch 58/300, trend Loss: 0.0993 | 0.0622
Epoch 59/300, trend Loss: 0.1071 | 0.1006
Epoch 60/300, trend Loss: 0.1031 | 0.0923
Epoch 61/300, trend Loss: 0.1003 | 0.0605
Epoch 62/300, trend Loss: 0.1009 | 0.0534
Epoch 63/300, trend Loss: 0.1027 | 0.0556
Epoch 64/300, trend Loss: 0.0971 | 0.0669
Epoch 65/300, trend Loss: 0.1042 | 0.0998
Epoch 66/300, trend Loss: 0.0974 | 0.0745
Epoch 67/300, trend Loss: 0.0965 | 0.0542
Epoch 68/300, trend Loss: 0.0961 | 0.0511
Epoch 69/300, trend Loss: 0.0941 | 0.0515
Epoch 70/300, trend Loss: 0.0916 | 0.0627
Epoch 71/300, trend Loss: 0.0965 | 0.0866
Epoch 72/300, trend Loss: 0.0939 | 0.0730
Epoch 73/300, trend Loss: 0.0908 | 0.0536
Epoch 74/300, trend Loss: 0.0921 | 0.0523
Epoch 75/300, trend Loss: 0.0923 | 0.0498
Epoch 76/300, trend Loss: 0.0890 | 0.0560
Epoch 77/300, trend Loss: 0.0922 | 0.0747
Epoch 78/300, trend Loss: 0.0926 | 0.0639
Epoch 79/300, trend Loss: 0.0904 | 0.0488
Epoch 80/300, trend Loss: 0.0908 | 0.0468
Epoch 81/300, trend Loss: 0.0940 | 0.0544
Epoch 82/300, trend Loss: 0.0999 | 0.0716
Epoch 83/300, trend Loss: 0.1005 | 0.0718
Epoch 84/300, trend Loss: 0.0983 | 0.0587
Epoch 85/300, trend Loss: 0.0921 | 0.0485
Epoch 86/300, trend Loss: 0.0963 | 0.0482
Epoch 87/300, trend Loss: 0.0932 | 0.0555
Epoch 88/300, trend Loss: 0.0981 | 0.0841
Epoch 89/300, trend Loss: 0.0949 | 0.0722
Epoch 90/300, trend Loss: 0.0908 | 0.0495
Epoch 91/300, trend Loss: 0.0948 | 0.0459
Epoch 92/300, trend Loss: 0.0884 | 0.0480
Epoch 93/300, trend Loss: 0.0906 | 0.0725
Epoch 94/300, trend Loss: 0.0925 | 0.0857
Epoch 95/300, trend Loss: 0.0880 | 0.0488
Epoch 96/300, trend Loss: 0.0869 | 0.0454
Epoch 97/300, trend Loss: 0.0878 | 0.0441
Epoch 98/300, trend Loss: 0.0841 | 0.0569
Epoch 99/300, trend Loss: 0.0858 | 0.0721
Epoch 100/300, trend Loss: 0.0837 | 0.0529
Epoch 101/300, trend Loss: 0.0813 | 0.0441
Epoch 102/300, trend Loss: 0.0823 | 0.0451
Epoch 103/300, trend Loss: 0.0818 | 0.0486
Epoch 104/300, trend Loss: 0.0828 | 0.0550
Epoch 105/300, trend Loss: 0.0841 | 0.0498
Epoch 106/300, trend Loss: 0.0830 | 0.0436
Epoch 107/300, trend Loss: 0.0860 | 0.0419
Epoch 108/300, trend Loss: 0.0841 | 0.0430
Epoch 109/300, trend Loss: 0.0813 | 0.0469
Epoch 110/300, trend Loss: 0.0857 | 0.0674
Epoch 111/300, trend Loss: 0.0856 | 0.0709
Epoch 112/300, trend Loss: 0.0820 | 0.0486
Epoch 113/300, trend Loss: 0.0807 | 0.0426
Epoch 114/300, trend Loss: 0.0824 | 0.0449
Epoch 115/300, trend Loss: 0.0813 | 0.0441
Epoch 116/300, trend Loss: 0.0834 | 0.0566
Epoch 117/300, trend Loss: 0.0824 | 0.0629
Epoch 118/300, trend Loss: 0.0842 | 0.0431
Epoch 119/300, trend Loss: 0.0853 | 0.0412
Epoch 120/300, trend Loss: 0.0825 | 0.0426
Epoch 121/300, trend Loss: 0.0858 | 0.0656
Epoch 122/300, trend Loss: 0.0924 | 0.0850
Epoch 123/300, trend Loss: 0.0862 | 0.0480
Epoch 124/300, trend Loss: 0.0862 | 0.0440
Epoch 125/300, trend Loss: 0.0857 | 0.0410
Epoch 126/300, trend Loss: 0.0799 | 0.0515
Epoch 127/300, trend Loss: 0.0799 | 0.0676
Epoch 128/300, trend Loss: 0.0792 | 0.0450
Epoch 129/300, trend Loss: 0.0753 | 0.0408
Epoch 130/300, trend Loss: 0.0764 | 0.0380
Epoch 131/300, trend Loss: 0.0750 | 0.0397
Epoch 132/300, trend Loss: 0.0746 | 0.0429
Epoch 133/300, trend Loss: 0.0742 | 0.0453
Epoch 134/300, trend Loss: 0.0776 | 0.0525
Epoch 135/300, trend Loss: 0.0800 | 0.0492
Epoch 136/300, trend Loss: 0.0775 | 0.0409
Epoch 137/300, trend Loss: 0.0833 | 0.0398
Epoch 138/300, trend Loss: 0.0801 | 0.0398
Epoch 139/300, trend Loss: 0.0800 | 0.0399
Epoch 140/300, trend Loss: 0.0801 | 0.0649
Epoch 141/300, trend Loss: 0.0907 | 0.0736
Epoch 142/300, trend Loss: 0.0844 | 0.0440
Epoch 143/300, trend Loss: 0.0894 | 0.0416
Epoch 144/300, trend Loss: 0.0817 | 0.0411
Epoch 145/300, trend Loss: 0.0781 | 0.0463
Epoch 146/300, trend Loss: 0.0769 | 0.0579
Epoch 147/300, trend Loss: 0.0760 | 0.0484
Epoch 148/300, trend Loss: 0.0777 | 0.0406
Epoch 149/300, trend Loss: 0.0764 | 0.0379
Epoch 150/300, trend Loss: 0.0790 | 0.0471
Epoch 151/300, trend Loss: 0.0774 | 0.0472
Epoch 152/300, trend Loss: 0.0792 | 0.0506
Epoch 153/300, trend Loss: 0.0799 | 0.0419
Epoch 154/300, trend Loss: 0.0751 | 0.0406
Epoch 155/300, trend Loss: 0.0801 | 0.0442
Epoch 156/300, trend Loss: 0.0747 | 0.0403
Epoch 157/300, trend Loss: 0.0792 | 0.0422
Epoch 158/300, trend Loss: 0.0716 | 0.0382
Epoch 159/300, trend Loss: 0.0741 | 0.0407
Epoch 160/300, trend Loss: 0.0710 | 0.0376
Epoch 161/300, trend Loss: 0.0735 | 0.0375
Epoch 162/300, trend Loss: 0.0724 | 0.0368
Epoch 163/300, trend Loss: 0.0727 | 0.0386
Epoch 164/300, trend Loss: 0.0724 | 0.0410
Epoch 165/300, trend Loss: 0.0721 | 0.0452
Epoch 166/300, trend Loss: 0.0753 | 0.0428
Epoch 167/300, trend Loss: 0.0694 | 0.0376
Epoch 168/300, trend Loss: 0.0717 | 0.0364
Epoch 169/300, trend Loss: 0.0704 | 0.0333
Epoch 170/300, trend Loss: 0.0699 | 0.0344
Epoch 171/300, trend Loss: 0.0700 | 0.0331
Epoch 172/300, trend Loss: 0.0699 | 0.0402
Epoch 173/300, trend Loss: 0.0707 | 0.0503
Epoch 174/300, trend Loss: 0.0715 | 0.0462
Epoch 175/300, trend Loss: 0.0685 | 0.0352
Epoch 176/300, trend Loss: 0.0709 | 0.0350
Epoch 177/300, trend Loss: 0.0747 | 0.0359
Epoch 178/300, trend Loss: 0.0739 | 0.0365
Epoch 179/300, trend Loss: 0.0751 | 0.0455
Epoch 180/300, trend Loss: 0.0785 | 0.0650
Epoch 181/300, trend Loss: 0.0826 | 0.0589
Epoch 182/300, trend Loss: 0.0744 | 0.0364
Epoch 183/300, trend Loss: 0.0735 | 0.0351
Epoch 184/300, trend Loss: 0.0711 | 0.0409
Epoch 185/300, trend Loss: 0.0688 | 0.0448
Epoch 186/300, trend Loss: 0.0683 | 0.0338
Epoch 187/300, trend Loss: 0.0671 | 0.0320
Epoch 188/300, trend Loss: 0.0687 | 0.0328
Epoch 189/300, trend Loss: 0.0666 | 0.0334
Epoch 190/300, trend Loss: 0.0677 | 0.0414
Epoch 191/300, trend Loss: 0.0686 | 0.0456
Epoch 192/300, trend Loss: 0.0677 | 0.0372
Epoch 193/300, trend Loss: 0.0721 | 0.0327
Epoch 194/300, trend Loss: 0.0704 | 0.0342
Epoch 195/300, trend Loss: 0.0694 | 0.0360
Epoch 196/300, trend Loss: 0.0713 | 0.0507
Epoch 197/300, trend Loss: 0.0742 | 0.0568
Epoch 198/300, trend Loss: 0.0749 | 0.0346
Epoch 199/300, trend Loss: 0.0688 | 0.0327
Epoch 200/300, trend Loss: 0.0698 | 0.0358
Epoch 201/300, trend Loss: 0.0668 | 0.0452
Epoch 202/300, trend Loss: 0.0679 | 0.0380
Epoch 203/300, trend Loss: 0.0663 | 0.0321
Epoch 204/300, trend Loss: 0.0673 | 0.0306
Epoch 205/300, trend Loss: 0.0675 | 0.0343
Epoch 206/300, trend Loss: 0.0672 | 0.0367
Epoch 207/300, trend Loss: 0.0662 | 0.0377
Epoch 208/300, trend Loss: 0.0663 | 0.0370
Epoch 209/300, trend Loss: 0.0640 | 0.0308
Epoch 210/300, trend Loss: 0.0671 | 0.0320
Epoch 211/300, trend Loss: 0.0649 | 0.0298
Epoch 212/300, trend Loss: 0.0649 | 0.0315
Epoch 213/300, trend Loss: 0.0662 | 0.0358
Epoch 214/300, trend Loss: 0.0656 | 0.0415
Epoch 215/300, trend Loss: 0.0669 | 0.0384
Epoch 216/300, trend Loss: 0.0655 | 0.0306
Epoch 217/300, trend Loss: 0.0652 | 0.0306
Epoch 218/300, trend Loss: 0.0658 | 0.0313
Epoch 219/300, trend Loss: 0.0659 | 0.0410
Epoch 220/300, trend Loss: 0.0677 | 0.0481
Epoch 221/300, trend Loss: 0.0669 | 0.0399
Epoch 222/300, trend Loss: 0.0667 | 0.0298
Epoch 223/300, trend Loss: 0.0655 | 0.0306
Epoch 224/300, trend Loss: 0.0692 | 0.0397
Epoch 225/300, trend Loss: 0.0682 | 0.0411
Epoch 226/300, trend Loss: 0.0684 | 0.0407
Epoch 227/300, trend Loss: 0.0666 | 0.0283
Epoch 228/300, trend Loss: 0.0653 | 0.0285
Epoch 229/300, trend Loss: 0.0660 | 0.0293
Epoch 230/300, trend Loss: 0.0636 | 0.0356
Epoch 231/300, trend Loss: 0.0663 | 0.0476
Epoch 232/300, trend Loss: 0.0647 | 0.0360
Epoch 233/300, trend Loss: 0.0675 | 0.0307
Epoch 234/300, trend Loss: 0.0641 | 0.0285
Epoch 235/300, trend Loss: 0.0628 | 0.0304
Epoch 236/300, trend Loss: 0.0652 | 0.0431
Epoch 237/300, trend Loss: 0.0663 | 0.0474
Epoch 238/300, trend Loss: 0.0659 | 0.0316
Epoch 239/300, trend Loss: 0.0625 | 0.0288
Epoch 240/300, trend Loss: 0.0625 | 0.0306
Epoch 241/300, trend Loss: 0.0619 | 0.0406
Epoch 242/300, trend Loss: 0.0649 | 0.0420
Epoch 243/300, trend Loss: 0.0637 | 0.0288
Epoch 244/300, trend Loss: 0.0673 | 0.0334
Epoch 245/300, trend Loss: 0.0667 | 0.0319
Epoch 246/300, trend Loss: 0.0631 | 0.0360
Epoch 247/300, trend Loss: 0.0624 | 0.0364
Epoch 248/300, trend Loss: 0.0619 | 0.0281
Epoch 249/300, trend Loss: 0.0619 | 0.0274
Epoch 250/300, trend Loss: 0.0612 | 0.0303
Epoch 251/300, trend Loss: 0.0613 | 0.0298
Epoch 252/300, trend Loss: 0.0595 | 0.0291
Epoch 253/300, trend Loss: 0.0613 | 0.0349
Epoch 254/300, trend Loss: 0.0602 | 0.0291
Epoch 255/300, trend Loss: 0.0615 | 0.0289
Epoch 256/300, trend Loss: 0.0609 | 0.0257
Epoch 257/300, trend Loss: 0.0591 | 0.0274
Epoch 258/300, trend Loss: 0.0605 | 0.0298
Epoch 259/300, trend Loss: 0.0592 | 0.0284
Epoch 260/300, trend Loss: 0.0602 | 0.0291
Epoch 261/300, trend Loss: 0.0589 | 0.0281
Epoch 262/300, trend Loss: 0.0582 | 0.0280
Epoch 263/300, trend Loss: 0.0584 | 0.0272
Epoch 264/300, trend Loss: 0.0584 | 0.0273
Epoch 265/300, trend Loss: 0.0588 | 0.0273
Epoch 266/300, trend Loss: 0.0591 | 0.0281
Epoch 267/300, trend Loss: 0.0631 | 0.0314
Epoch 268/300, trend Loss: 0.0627 | 0.0324
Epoch 269/300, trend Loss: 0.0592 | 0.0309
Epoch 270/300, trend Loss: 0.0584 | 0.0305
Epoch 271/300, trend Loss: 0.0581 | 0.0302
Epoch 272/300, trend Loss: 0.0585 | 0.0284
Epoch 273/300, trend Loss: 0.0575 | 0.0259
Epoch 274/300, trend Loss: 0.0578 | 0.0252
Epoch 275/300, trend Loss: 0.0583 | 0.0259
Epoch 276/300, trend Loss: 0.0586 | 0.0282
Epoch 277/300, trend Loss: 0.0587 | 0.0306
Epoch 278/300, trend Loss: 0.0590 | 0.0316
Epoch 279/300, trend Loss: 0.0626 | 0.0374
Epoch 280/300, trend Loss: 0.0598 | 0.0332
Epoch 281/300, trend Loss: 0.0581 | 0.0368
Epoch 282/300, trend Loss: 0.0583 | 0.0312
Epoch 283/300, trend Loss: 0.0591 | 0.0272
Epoch 284/300, trend Loss: 0.0603 | 0.0315
Epoch 285/300, trend Loss: 0.0622 | 0.0285
Epoch 286/300, trend Loss: 0.0639 | 0.0357
Epoch 287/300, trend Loss: 0.0672 | 0.0532
Epoch 288/300, trend Loss: 0.0654 | 0.0377
Epoch 289/300, trend Loss: 0.0614 | 0.0299
Epoch 290/300, trend Loss: 0.0594 | 0.0270
Epoch 291/300, trend Loss: 0.0611 | 0.0303
Epoch 292/300, trend Loss: 0.0586 | 0.0323
Epoch 293/300, trend Loss: 0.0582 | 0.0285
Epoch 294/300, trend Loss: 0.0561 | 0.0275
Epoch 295/300, trend Loss: 0.0553 | 0.0276
Epoch 296/300, trend Loss: 0.0552 | 0.0308
Epoch 297/300, trend Loss: 0.0551 | 0.0299
Epoch 298/300, trend Loss: 0.0550 | 0.0265
Epoch 299/300, trend Loss: 0.0546 | 0.0266
Epoch 300/300, trend Loss: 0.0548 | 0.0258
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.8863242316081515, 'learning_rate': 0.0002327530446652938, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9105505052747357}
Epoch 1/300, seasonal_0 Loss: 1.2303 | 0.1583
Epoch 2/300, seasonal_0 Loss: 0.2237 | 0.1109
Epoch 3/300, seasonal_0 Loss: 0.1921 | 0.0897
Epoch 4/300, seasonal_0 Loss: 0.1523 | 0.0767
Epoch 5/300, seasonal_0 Loss: 0.1462 | 0.0808
Epoch 6/300, seasonal_0 Loss: 0.1546 | 0.0800
Epoch 7/300, seasonal_0 Loss: 0.1317 | 0.0775
Epoch 8/300, seasonal_0 Loss: 0.1201 | 0.0703
Epoch 9/300, seasonal_0 Loss: 0.1343 | 0.0833
Epoch 10/300, seasonal_0 Loss: 0.1184 | 0.0715
Epoch 11/300, seasonal_0 Loss: 0.1099 | 0.0611
Epoch 12/300, seasonal_0 Loss: 0.1027 | 0.0831
Epoch 13/300, seasonal_0 Loss: 0.1091 | 0.0565
Epoch 14/300, seasonal_0 Loss: 0.1156 | 0.0920
Epoch 15/300, seasonal_0 Loss: 0.1326 | 0.0530
Epoch 16/300, seasonal_0 Loss: 0.1128 | 0.0496
Epoch 17/300, seasonal_0 Loss: 0.1140 | 0.0627
Epoch 18/300, seasonal_0 Loss: 0.1083 | 0.0500
Epoch 19/300, seasonal_0 Loss: 0.1021 | 0.0614
Epoch 20/300, seasonal_0 Loss: 0.1101 | 0.0659
Epoch 21/300, seasonal_0 Loss: 0.0955 | 0.0497
Epoch 22/300, seasonal_0 Loss: 0.0980 | 0.0667
Epoch 23/300, seasonal_0 Loss: 0.1369 | 0.0619
Epoch 24/300, seasonal_0 Loss: 0.1134 | 0.0658
Epoch 25/300, seasonal_0 Loss: 0.1118 | 0.0754
Epoch 26/300, seasonal_0 Loss: 0.1050 | 0.0632
Epoch 27/300, seasonal_0 Loss: 0.1100 | 0.0690
Epoch 28/300, seasonal_0 Loss: 0.1028 | 0.0512
Epoch 29/300, seasonal_0 Loss: 0.0998 | 0.0491
Epoch 30/300, seasonal_0 Loss: 0.0881 | 0.0460
Epoch 31/300, seasonal_0 Loss: 0.0872 | 0.0457
Epoch 32/300, seasonal_0 Loss: 0.0887 | 0.0462
Epoch 33/300, seasonal_0 Loss: 0.1025 | 0.0490
Epoch 34/300, seasonal_0 Loss: 0.1200 | 0.0651
Epoch 35/300, seasonal_0 Loss: 0.1242 | 0.0560
Epoch 36/300, seasonal_0 Loss: 0.1099 | 0.0567
Epoch 37/300, seasonal_0 Loss: 0.0950 | 0.0406
Epoch 38/300, seasonal_0 Loss: 0.1158 | 0.0424
Epoch 39/300, seasonal_0 Loss: 0.1351 | 0.0557
Epoch 40/300, seasonal_0 Loss: 0.1394 | 0.0553
Epoch 41/300, seasonal_0 Loss: 0.1139 | 0.0458
Epoch 42/300, seasonal_0 Loss: 0.1105 | 0.0558
Epoch 43/300, seasonal_0 Loss: 0.1138 | 0.0670
Epoch 44/300, seasonal_0 Loss: 0.1123 | 0.0879
Epoch 45/300, seasonal_0 Loss: 0.1065 | 0.0603
Epoch 46/300, seasonal_0 Loss: 0.0923 | 0.0569
Epoch 47/300, seasonal_0 Loss: 0.0992 | 0.0666
Epoch 48/300, seasonal_0 Loss: 0.0939 | 0.0457
Epoch 49/300, seasonal_0 Loss: 0.1105 | 0.0542
Epoch 50/300, seasonal_0 Loss: 0.1159 | 0.0699
Epoch 51/300, seasonal_0 Loss: 0.1074 | 0.0566
Epoch 52/300, seasonal_0 Loss: 0.1089 | 0.0650
Epoch 53/300, seasonal_0 Loss: 0.0990 | 0.0671
Epoch 54/300, seasonal_0 Loss: 0.1184 | 0.0534
Epoch 55/300, seasonal_0 Loss: 0.1145 | 0.1417
Epoch 56/300, seasonal_0 Loss: 0.1134 | 0.1256
Epoch 57/300, seasonal_0 Loss: 0.1253 | 0.0548
Epoch 58/300, seasonal_0 Loss: 0.1287 | 0.0557
Epoch 59/300, seasonal_0 Loss: 0.1082 | 0.0693
Epoch 60/300, seasonal_0 Loss: 0.0995 | 0.0671
Epoch 61/300, seasonal_0 Loss: 0.0815 | 0.0548
Epoch 62/300, seasonal_0 Loss: 0.0907 | 0.0523
Epoch 63/300, seasonal_0 Loss: 0.0884 | 0.0586
Epoch 64/300, seasonal_0 Loss: 0.0764 | 0.0595
Epoch 65/300, seasonal_0 Loss: 0.0767 | 0.0558
Epoch 66/300, seasonal_0 Loss: 0.0793 | 0.0442
Epoch 67/300, seasonal_0 Loss: 0.0741 | 0.0494
Epoch 68/300, seasonal_0 Loss: 0.0717 | 0.0552
Epoch 69/300, seasonal_0 Loss: 0.0703 | 0.0484
Epoch 70/300, seasonal_0 Loss: 0.0708 | 0.0513
Epoch 71/300, seasonal_0 Loss: 0.0686 | 0.0492
Epoch 72/300, seasonal_0 Loss: 0.0687 | 0.0440
Epoch 73/300, seasonal_0 Loss: 0.0680 | 0.0440
Epoch 74/300, seasonal_0 Loss: 0.0667 | 0.0445
Epoch 75/300, seasonal_0 Loss: 0.0660 | 0.0470
Epoch 76/300, seasonal_0 Loss: 0.0658 | 0.0455
Epoch 77/300, seasonal_0 Loss: 0.0652 | 0.0439
Epoch 78/300, seasonal_0 Loss: 0.0648 | 0.0425
Epoch 79/300, seasonal_0 Loss: 0.0647 | 0.0418
Epoch 80/300, seasonal_0 Loss: 0.0645 | 0.0428
Epoch 81/300, seasonal_0 Loss: 0.0648 | 0.0450
Epoch 82/300, seasonal_0 Loss: 0.0644 | 0.0429
Epoch 83/300, seasonal_0 Loss: 0.0639 | 0.0420
Epoch 84/300, seasonal_0 Loss: 0.0637 | 0.0414
Epoch 85/300, seasonal_0 Loss: 0.0639 | 0.0406
Epoch 86/300, seasonal_0 Loss: 0.0639 | 0.0406
Epoch 87/300, seasonal_0 Loss: 0.0632 | 0.0409
Epoch 88/300, seasonal_0 Loss: 0.0630 | 0.0409
Epoch 89/300, seasonal_0 Loss: 0.0645 | 0.0415
Epoch 90/300, seasonal_0 Loss: 0.0634 | 0.0396
Epoch 91/300, seasonal_0 Loss: 0.0631 | 0.0405
Epoch 92/300, seasonal_0 Loss: 0.0626 | 0.0405
Epoch 93/300, seasonal_0 Loss: 0.0622 | 0.0398
Epoch 94/300, seasonal_0 Loss: 0.0621 | 0.0391
Epoch 95/300, seasonal_0 Loss: 0.0618 | 0.0394
Epoch 96/300, seasonal_0 Loss: 0.0617 | 0.0401
Epoch 97/300, seasonal_0 Loss: 0.0619 | 0.0410
Epoch 98/300, seasonal_0 Loss: 0.0618 | 0.0397
Epoch 99/300, seasonal_0 Loss: 0.0613 | 0.0398
Epoch 100/300, seasonal_0 Loss: 0.0613 | 0.0392
Epoch 101/300, seasonal_0 Loss: 0.0615 | 0.0386
Epoch 102/300, seasonal_0 Loss: 0.0621 | 0.0387
Epoch 103/300, seasonal_0 Loss: 0.0619 | 0.0382
Epoch 104/300, seasonal_0 Loss: 0.0613 | 0.0396
Epoch 105/300, seasonal_0 Loss: 0.0631 | 0.0373
Epoch 106/300, seasonal_0 Loss: 0.0610 | 0.0380
Epoch 107/300, seasonal_0 Loss: 0.0613 | 0.0381
Epoch 108/300, seasonal_0 Loss: 0.0605 | 0.0384
Epoch 109/300, seasonal_0 Loss: 0.0604 | 0.0386
Epoch 110/300, seasonal_0 Loss: 0.0602 | 0.0386
Epoch 111/300, seasonal_0 Loss: 0.0600 | 0.0382
Epoch 112/300, seasonal_0 Loss: 0.0600 | 0.0378
Epoch 113/300, seasonal_0 Loss: 0.0599 | 0.0375
Epoch 114/300, seasonal_0 Loss: 0.0598 | 0.0374
Epoch 115/300, seasonal_0 Loss: 0.0596 | 0.0376
Epoch 116/300, seasonal_0 Loss: 0.0596 | 0.0380
Epoch 117/300, seasonal_0 Loss: 0.0597 | 0.0384
Epoch 118/300, seasonal_0 Loss: 0.0596 | 0.0380
Epoch 119/300, seasonal_0 Loss: 0.0594 | 0.0375
Epoch 120/300, seasonal_0 Loss: 0.0595 | 0.0371
Epoch 121/300, seasonal_0 Loss: 0.0596 | 0.0370
Epoch 122/300, seasonal_0 Loss: 0.0594 | 0.0369
Epoch 123/300, seasonal_0 Loss: 0.0592 | 0.0374
Epoch 124/300, seasonal_0 Loss: 0.0595 | 0.0383
Epoch 125/300, seasonal_0 Loss: 0.0592 | 0.0372
Epoch 126/300, seasonal_0 Loss: 0.0590 | 0.0371
Epoch 127/300, seasonal_0 Loss: 0.0590 | 0.0368
Epoch 128/300, seasonal_0 Loss: 0.0589 | 0.0368
Epoch 129/300, seasonal_0 Loss: 0.0588 | 0.0370
Epoch 130/300, seasonal_0 Loss: 0.0588 | 0.0376
Epoch 131/300, seasonal_0 Loss: 0.0588 | 0.0374
Epoch 132/300, seasonal_0 Loss: 0.0586 | 0.0371
Epoch 133/300, seasonal_0 Loss: 0.0586 | 0.0368
Epoch 134/300, seasonal_0 Loss: 0.0586 | 0.0366
Epoch 135/300, seasonal_0 Loss: 0.0585 | 0.0366
Epoch 136/300, seasonal_0 Loss: 0.0584 | 0.0369
Epoch 137/300, seasonal_0 Loss: 0.0585 | 0.0373
Epoch 138/300, seasonal_0 Loss: 0.0584 | 0.0372
Epoch 139/300, seasonal_0 Loss: 0.0583 | 0.0368
Epoch 140/300, seasonal_0 Loss: 0.0583 | 0.0366
Epoch 141/300, seasonal_0 Loss: 0.0583 | 0.0364
Epoch 142/300, seasonal_0 Loss: 0.0583 | 0.0365
Epoch 143/300, seasonal_0 Loss: 0.0582 | 0.0369
Epoch 144/300, seasonal_0 Loss: 0.0582 | 0.0372
Epoch 145/300, seasonal_0 Loss: 0.0581 | 0.0369
Epoch 146/300, seasonal_0 Loss: 0.0581 | 0.0365
Epoch 147/300, seasonal_0 Loss: 0.0581 | 0.0363
Epoch 148/300, seasonal_0 Loss: 0.0580 | 0.0363
Epoch 149/300, seasonal_0 Loss: 0.0580 | 0.0366
Epoch 150/300, seasonal_0 Loss: 0.0579 | 0.0369
Epoch 151/300, seasonal_0 Loss: 0.0579 | 0.0368
Epoch 152/300, seasonal_0 Loss: 0.0579 | 0.0364
Epoch 153/300, seasonal_0 Loss: 0.0578 | 0.0363
Epoch 154/300, seasonal_0 Loss: 0.0578 | 0.0363
Epoch 155/300, seasonal_0 Loss: 0.0578 | 0.0365
Epoch 156/300, seasonal_0 Loss: 0.0577 | 0.0366
Epoch 157/300, seasonal_0 Loss: 0.0577 | 0.0365
Epoch 158/300, seasonal_0 Loss: 0.0577 | 0.0364
Epoch 159/300, seasonal_0 Loss: 0.0577 | 0.0363
Epoch 160/300, seasonal_0 Loss: 0.0576 | 0.0363
Epoch 161/300, seasonal_0 Loss: 0.0576 | 0.0364
Epoch 162/300, seasonal_0 Loss: 0.0576 | 0.0364
Epoch 163/300, seasonal_0 Loss: 0.0576 | 0.0363
Epoch 164/300, seasonal_0 Loss: 0.0575 | 0.0363
Epoch 165/300, seasonal_0 Loss: 0.0575 | 0.0363
Epoch 166/300, seasonal_0 Loss: 0.0575 | 0.0363
Epoch 167/300, seasonal_0 Loss: 0.0575 | 0.0363
Epoch 168/300, seasonal_0 Loss: 0.0574 | 0.0363
Epoch 169/300, seasonal_0 Loss: 0.0574 | 0.0363
Epoch 170/300, seasonal_0 Loss: 0.0574 | 0.0362
Epoch 171/300, seasonal_0 Loss: 0.0574 | 0.0362
Epoch 172/300, seasonal_0 Loss: 0.0574 | 0.0362
Epoch 173/300, seasonal_0 Loss: 0.0574 | 0.0362
Epoch 174/300, seasonal_0 Loss: 0.0573 | 0.0362
Epoch 175/300, seasonal_0 Loss: 0.0573 | 0.0362
Epoch 176/300, seasonal_0 Loss: 0.0573 | 0.0362
Epoch 177/300, seasonal_0 Loss: 0.0573 | 0.0362
Epoch 178/300, seasonal_0 Loss: 0.0573 | 0.0362
Epoch 179/300, seasonal_0 Loss: 0.0573 | 0.0362
Epoch 180/300, seasonal_0 Loss: 0.0572 | 0.0362
Epoch 181/300, seasonal_0 Loss: 0.0572 | 0.0362
Epoch 182/300, seasonal_0 Loss: 0.0572 | 0.0361
Epoch 183/300, seasonal_0 Loss: 0.0572 | 0.0361
Epoch 184/300, seasonal_0 Loss: 0.0572 | 0.0361
Epoch 185/300, seasonal_0 Loss: 0.0572 | 0.0361
Epoch 186/300, seasonal_0 Loss: 0.0572 | 0.0361
Epoch 187/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 188/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 189/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 190/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 191/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 192/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 193/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 194/300, seasonal_0 Loss: 0.0571 | 0.0361
Epoch 195/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 196/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 197/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 198/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 199/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 200/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 201/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 202/300, seasonal_0 Loss: 0.0570 | 0.0361
Epoch 203/300, seasonal_0 Loss: 0.0570 | 0.0360
Epoch 204/300, seasonal_0 Loss: 0.0570 | 0.0360
Epoch 205/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 206/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 207/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 208/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 209/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 210/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 211/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 212/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 213/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 214/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 215/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 216/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 217/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 218/300, seasonal_0 Loss: 0.0569 | 0.0360
Epoch 219/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 220/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 221/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 222/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 223/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 224/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 225/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 226/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 227/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 228/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 229/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 230/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 231/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 232/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 233/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 234/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 235/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 236/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 237/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 238/300, seasonal_0 Loss: 0.0568 | 0.0360
Epoch 239/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 240/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 241/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 242/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 243/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 244/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 245/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 246/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 247/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 248/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 249/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 250/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 251/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 252/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 253/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 254/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 255/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 256/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 257/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 258/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 259/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 260/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 261/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 262/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 263/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 264/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 265/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 266/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 267/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 268/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 269/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 270/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 271/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 272/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 273/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 274/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 275/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 276/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 277/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 278/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 279/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 280/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 281/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 282/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 283/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 284/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 285/300, seasonal_0 Loss: 0.0567 | 0.0360
Epoch 286/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 287/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 288/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 289/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 290/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 291/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 292/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 293/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 294/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 295/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 296/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 297/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 298/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 299/300, seasonal_0 Loss: 0.0566 | 0.0360
Epoch 300/300, seasonal_0 Loss: 0.0566 | 0.0360
Training seasonal_1 component with params: {'observation_period_num': 9, 'train_rates': 0.8641263875085698, 'learning_rate': 0.0002986021608696033, 'batch_size': 115, 'step_size': 8, 'gamma': 0.835195445457884}
Epoch 1/300, seasonal_1 Loss: 1.0583 | 0.1666
Epoch 2/300, seasonal_1 Loss: 0.1985 | 0.1625
Epoch 3/300, seasonal_1 Loss: 0.1607 | 0.1050
Epoch 4/300, seasonal_1 Loss: 0.1477 | 0.1124
Epoch 5/300, seasonal_1 Loss: 0.1324 | 0.0868
Epoch 6/300, seasonal_1 Loss: 0.1416 | 0.1269
Epoch 7/300, seasonal_1 Loss: 0.1313 | 0.0807
Epoch 8/300, seasonal_1 Loss: 0.1415 | 0.0944
Epoch 9/300, seasonal_1 Loss: 0.1475 | 0.0710
Epoch 10/300, seasonal_1 Loss: 0.1625 | 0.0854
Epoch 11/300, seasonal_1 Loss: 0.1700 | 0.1056
Epoch 12/300, seasonal_1 Loss: 0.1280 | 0.0715
Epoch 13/300, seasonal_1 Loss: 0.1283 | 0.0556
Epoch 14/300, seasonal_1 Loss: 0.1118 | 0.0644
Epoch 15/300, seasonal_1 Loss: 0.1517 | 0.0984
Epoch 16/300, seasonal_1 Loss: 0.1189 | 0.0558
Epoch 17/300, seasonal_1 Loss: 0.1137 | 0.0488
Epoch 18/300, seasonal_1 Loss: 0.1024 | 0.0493
Epoch 19/300, seasonal_1 Loss: 0.0999 | 0.0695
Epoch 20/300, seasonal_1 Loss: 0.1077 | 0.0726
Epoch 21/300, seasonal_1 Loss: 0.1139 | 0.0645
Epoch 22/300, seasonal_1 Loss: 0.1230 | 0.0657
Epoch 23/300, seasonal_1 Loss: 0.1379 | 0.1242
Epoch 24/300, seasonal_1 Loss: 0.1158 | 0.0707
Epoch 25/300, seasonal_1 Loss: 0.1398 | 0.0629
Epoch 26/300, seasonal_1 Loss: 0.1398 | 0.0572
Epoch 27/300, seasonal_1 Loss: 0.1185 | 0.0531
Epoch 28/300, seasonal_1 Loss: 0.1188 | 0.0559
Epoch 29/300, seasonal_1 Loss: 0.1342 | 0.0624
Epoch 30/300, seasonal_1 Loss: 0.1535 | 0.0680
Epoch 31/300, seasonal_1 Loss: 0.1771 | 0.1645
Epoch 32/300, seasonal_1 Loss: 0.1765 | 0.0923
Epoch 33/300, seasonal_1 Loss: 0.1547 | 0.1056
Epoch 34/300, seasonal_1 Loss: 0.1267 | 0.0843
Epoch 35/300, seasonal_1 Loss: 0.1487 | 0.0993
Epoch 36/300, seasonal_1 Loss: 0.1466 | 0.0878
Epoch 37/300, seasonal_1 Loss: 0.1386 | 0.0671
Epoch 38/300, seasonal_1 Loss: 0.1102 | 0.0640
Epoch 39/300, seasonal_1 Loss: 0.1477 | 0.0728
Epoch 40/300, seasonal_1 Loss: 0.1180 | 0.0802
Epoch 41/300, seasonal_1 Loss: 0.1195 | 0.1443
Epoch 42/300, seasonal_1 Loss: 0.1217 | 0.0755
Epoch 43/300, seasonal_1 Loss: 0.1208 | 0.0596
Epoch 44/300, seasonal_1 Loss: 0.0994 | 0.0714
Epoch 45/300, seasonal_1 Loss: 0.0884 | 0.0757
Epoch 46/300, seasonal_1 Loss: 0.0941 | 0.0516
Epoch 47/300, seasonal_1 Loss: 0.0855 | 0.0610
Epoch 48/300, seasonal_1 Loss: 0.0783 | 0.0601
Epoch 49/300, seasonal_1 Loss: 0.0803 | 0.0563
Epoch 50/300, seasonal_1 Loss: 0.0778 | 0.0533
Epoch 51/300, seasonal_1 Loss: 0.0746 | 0.0588
Epoch 52/300, seasonal_1 Loss: 0.0742 | 0.0531
Epoch 53/300, seasonal_1 Loss: 0.0729 | 0.0530
Epoch 54/300, seasonal_1 Loss: 0.0723 | 0.0520
Epoch 55/300, seasonal_1 Loss: 0.0715 | 0.0515
Epoch 56/300, seasonal_1 Loss: 0.0711 | 0.0502
Epoch 57/300, seasonal_1 Loss: 0.0707 | 0.0494
Epoch 58/300, seasonal_1 Loss: 0.0707 | 0.0494
Epoch 59/300, seasonal_1 Loss: 0.0703 | 0.0480
Epoch 60/300, seasonal_1 Loss: 0.0699 | 0.0493
Epoch 61/300, seasonal_1 Loss: 0.0700 | 0.0492
Epoch 62/300, seasonal_1 Loss: 0.0699 | 0.0494
Epoch 63/300, seasonal_1 Loss: 0.0694 | 0.0463
Epoch 64/300, seasonal_1 Loss: 0.0697 | 0.0475
Epoch 65/300, seasonal_1 Loss: 0.0702 | 0.0462
Epoch 66/300, seasonal_1 Loss: 0.0700 | 0.0472
Epoch 67/300, seasonal_1 Loss: 0.0693 | 0.0458
Epoch 68/300, seasonal_1 Loss: 0.0691 | 0.0456
Epoch 69/300, seasonal_1 Loss: 0.0690 | 0.0459
Epoch 70/300, seasonal_1 Loss: 0.0683 | 0.0456
Epoch 71/300, seasonal_1 Loss: 0.0682 | 0.0460
Epoch 72/300, seasonal_1 Loss: 0.0679 | 0.0438
Epoch 73/300, seasonal_1 Loss: 0.0680 | 0.0448
Epoch 74/300, seasonal_1 Loss: 0.0679 | 0.0443
Epoch 75/300, seasonal_1 Loss: 0.0674 | 0.0451
Epoch 76/300, seasonal_1 Loss: 0.0673 | 0.0445
Epoch 77/300, seasonal_1 Loss: 0.0672 | 0.0441
Epoch 78/300, seasonal_1 Loss: 0.0672 | 0.0437
Epoch 79/300, seasonal_1 Loss: 0.0671 | 0.0438
Epoch 80/300, seasonal_1 Loss: 0.0669 | 0.0439
Epoch 81/300, seasonal_1 Loss: 0.0668 | 0.0443
Epoch 82/300, seasonal_1 Loss: 0.0667 | 0.0440
Epoch 83/300, seasonal_1 Loss: 0.0667 | 0.0436
Epoch 84/300, seasonal_1 Loss: 0.0666 | 0.0433
Epoch 85/300, seasonal_1 Loss: 0.0666 | 0.0435
Epoch 86/300, seasonal_1 Loss: 0.0664 | 0.0437
Epoch 87/300, seasonal_1 Loss: 0.0664 | 0.0437
Epoch 88/300, seasonal_1 Loss: 0.0663 | 0.0434
Epoch 89/300, seasonal_1 Loss: 0.0663 | 0.0432
Epoch 90/300, seasonal_1 Loss: 0.0662 | 0.0433
Epoch 91/300, seasonal_1 Loss: 0.0661 | 0.0433
Epoch 92/300, seasonal_1 Loss: 0.0661 | 0.0432
Epoch 93/300, seasonal_1 Loss: 0.0660 | 0.0431
Epoch 94/300, seasonal_1 Loss: 0.0660 | 0.0431
Epoch 95/300, seasonal_1 Loss: 0.0659 | 0.0431
Epoch 96/300, seasonal_1 Loss: 0.0658 | 0.0430
Epoch 97/300, seasonal_1 Loss: 0.0658 | 0.0430
Epoch 98/300, seasonal_1 Loss: 0.0658 | 0.0430
Epoch 99/300, seasonal_1 Loss: 0.0657 | 0.0429
Epoch 100/300, seasonal_1 Loss: 0.0657 | 0.0428
Epoch 101/300, seasonal_1 Loss: 0.0656 | 0.0429
Epoch 102/300, seasonal_1 Loss: 0.0656 | 0.0429
Epoch 103/300, seasonal_1 Loss: 0.0655 | 0.0428
Epoch 104/300, seasonal_1 Loss: 0.0655 | 0.0427
Epoch 105/300, seasonal_1 Loss: 0.0655 | 0.0428
Epoch 106/300, seasonal_1 Loss: 0.0654 | 0.0428
Epoch 107/300, seasonal_1 Loss: 0.0654 | 0.0427
Epoch 108/300, seasonal_1 Loss: 0.0654 | 0.0426
Epoch 109/300, seasonal_1 Loss: 0.0653 | 0.0427
Epoch 110/300, seasonal_1 Loss: 0.0653 | 0.0426
Epoch 111/300, seasonal_1 Loss: 0.0653 | 0.0426
Epoch 112/300, seasonal_1 Loss: 0.0653 | 0.0426
Epoch 113/300, seasonal_1 Loss: 0.0652 | 0.0426
Epoch 114/300, seasonal_1 Loss: 0.0652 | 0.0426
Epoch 115/300, seasonal_1 Loss: 0.0652 | 0.0425
Epoch 116/300, seasonal_1 Loss: 0.0652 | 0.0425
Epoch 117/300, seasonal_1 Loss: 0.0651 | 0.0425
Epoch 118/300, seasonal_1 Loss: 0.0651 | 0.0425
Epoch 119/300, seasonal_1 Loss: 0.0651 | 0.0425
Epoch 120/300, seasonal_1 Loss: 0.0651 | 0.0424
Epoch 121/300, seasonal_1 Loss: 0.0650 | 0.0425
Epoch 122/300, seasonal_1 Loss: 0.0650 | 0.0424
Epoch 123/300, seasonal_1 Loss: 0.0650 | 0.0424
Epoch 124/300, seasonal_1 Loss: 0.0650 | 0.0424
Epoch 125/300, seasonal_1 Loss: 0.0650 | 0.0424
Epoch 126/300, seasonal_1 Loss: 0.0650 | 0.0424
Epoch 127/300, seasonal_1 Loss: 0.0649 | 0.0424
Epoch 128/300, seasonal_1 Loss: 0.0649 | 0.0423
Epoch 129/300, seasonal_1 Loss: 0.0649 | 0.0424
Epoch 130/300, seasonal_1 Loss: 0.0649 | 0.0423
Epoch 131/300, seasonal_1 Loss: 0.0649 | 0.0423
Epoch 132/300, seasonal_1 Loss: 0.0649 | 0.0423
Epoch 133/300, seasonal_1 Loss: 0.0648 | 0.0423
Epoch 134/300, seasonal_1 Loss: 0.0648 | 0.0423
Epoch 135/300, seasonal_1 Loss: 0.0648 | 0.0423
Epoch 136/300, seasonal_1 Loss: 0.0648 | 0.0423
Epoch 137/300, seasonal_1 Loss: 0.0648 | 0.0423
Epoch 138/300, seasonal_1 Loss: 0.0648 | 0.0423
Epoch 139/300, seasonal_1 Loss: 0.0648 | 0.0422
Epoch 140/300, seasonal_1 Loss: 0.0648 | 0.0422
Epoch 141/300, seasonal_1 Loss: 0.0648 | 0.0422
Epoch 142/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 143/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 144/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 145/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 146/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 147/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 148/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 149/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 150/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 151/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 152/300, seasonal_1 Loss: 0.0647 | 0.0421
Epoch 153/300, seasonal_1 Loss: 0.0647 | 0.0422
Epoch 154/300, seasonal_1 Loss: 0.0647 | 0.0421
Epoch 155/300, seasonal_1 Loss: 0.0647 | 0.0421
Epoch 156/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 157/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 158/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 159/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 160/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 161/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 162/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 163/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 164/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 165/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 166/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 167/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 168/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 169/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 170/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 171/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 172/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 173/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 174/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 175/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 176/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 177/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 178/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 179/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 180/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 181/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 182/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 183/300, seasonal_1 Loss: 0.0646 | 0.0421
Epoch 184/300, seasonal_1 Loss: 0.0646 | 0.0420
Epoch 185/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 186/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 187/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 188/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 189/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 190/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 191/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 192/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 193/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 194/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 195/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 196/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 197/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 198/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 199/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 200/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 201/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 202/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 203/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 204/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 205/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 206/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 207/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 208/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 209/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 210/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 211/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 212/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 213/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 214/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 215/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 216/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 217/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 218/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 219/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 220/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 221/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 222/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 223/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 224/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 225/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 226/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 227/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 228/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 229/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 230/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 231/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 232/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 233/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 234/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 235/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 236/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 237/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 238/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 239/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 240/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 241/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 242/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 243/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 244/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 245/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 246/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 247/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 248/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 249/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 250/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 251/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 252/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 253/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 254/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 255/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 256/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 257/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 258/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 259/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 260/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 261/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 262/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 263/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 264/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 265/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 266/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 267/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 268/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 269/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 270/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 271/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 272/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 273/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 274/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 275/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 276/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 277/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 278/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 279/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 280/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 281/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 282/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 283/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 284/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 285/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 286/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 287/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 288/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 289/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 290/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 291/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 292/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 293/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 294/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 295/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 296/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 297/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 298/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 299/300, seasonal_1 Loss: 0.0645 | 0.0420
Epoch 300/300, seasonal_1 Loss: 0.0645 | 0.0420
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8919408098686068, 'learning_rate': 0.0009404589799346525, 'batch_size': 104, 'step_size': 10, 'gamma': 0.7970746054109646}
Epoch 1/300, seasonal_2 Loss: 2.2932 | 1.2818
Epoch 2/300, seasonal_2 Loss: 1.0408 | 1.3594
Epoch 3/300, seasonal_2 Loss: 0.9628 | 1.3020
Epoch 4/300, seasonal_2 Loss: 1.0065 | 1.3875
Epoch 5/300, seasonal_2 Loss: 0.9366 | 1.3329
Epoch 6/300, seasonal_2 Loss: 0.9990 | 1.3100
Epoch 7/300, seasonal_2 Loss: 1.0096 | 1.2817
Epoch 8/300, seasonal_2 Loss: 1.1690 | 1.2820
Epoch 9/300, seasonal_2 Loss: 1.2462 | 1.3475
Epoch 10/300, seasonal_2 Loss: 1.1907 | 1.5218
Epoch 11/300, seasonal_2 Loss: 1.0719 | 1.7287
Epoch 12/300, seasonal_2 Loss: 0.9793 | 1.6648
Epoch 13/300, seasonal_2 Loss: 0.9957 | 1.6763
Epoch 14/300, seasonal_2 Loss: 0.9932 | 1.6798
Epoch 15/300, seasonal_2 Loss: 0.9912 | 1.6822
Epoch 16/300, seasonal_2 Loss: 0.9805 | 1.7563
Epoch 17/300, seasonal_2 Loss: 0.9580 | 1.7337
Epoch 18/300, seasonal_2 Loss: 0.9642 | 1.7395
Epoch 19/300, seasonal_2 Loss: 0.9627 | 1.7381
Epoch 20/300, seasonal_2 Loss: 0.9631 | 1.7389
Epoch 21/300, seasonal_2 Loss: 0.9544 | 1.7827
Epoch 22/300, seasonal_2 Loss: 0.9435 | 1.7731
Epoch 23/300, seasonal_2 Loss: 0.9460 | 1.7741
Epoch 24/300, seasonal_2 Loss: 0.9459 | 1.7735
Epoch 25/300, seasonal_2 Loss: 0.9461 | 1.7733
Epoch 26/300, seasonal_2 Loss: 0.9392 | 1.8004
Epoch 27/300, seasonal_2 Loss: 0.9337 | 1.7986
Epoch 28/300, seasonal_2 Loss: 0.9342 | 1.7968
Epoch 29/300, seasonal_2 Loss: 0.9348 | 1.7970
Epoch 30/300, seasonal_2 Loss: 0.9348 | 1.7964
Epoch 31/300, seasonal_2 Loss: 0.9292 | 1.8135
Epoch 32/300, seasonal_2 Loss: 0.9265 | 1.8157
Epoch 33/300, seasonal_2 Loss: 0.9262 | 1.8132
Epoch 34/300, seasonal_2 Loss: 0.9267 | 1.8133
Epoch 35/300, seasonal_2 Loss: 0.9269 | 1.8129
Epoch 36/300, seasonal_2 Loss: 0.9222 | 1.8238
Epoch 37/300, seasonal_2 Loss: 0.9209 | 1.8275
Epoch 38/300, seasonal_2 Loss: 0.9204 | 1.8256
Epoch 39/300, seasonal_2 Loss: 0.9208 | 1.8253
Epoch 40/300, seasonal_2 Loss: 0.9209 | 1.8251
Epoch 41/300, seasonal_2 Loss: 0.9171 | 1.8321
Epoch 42/300, seasonal_2 Loss: 0.9165 | 1.8360
Epoch 43/300, seasonal_2 Loss: 0.9160 | 1.8350
Epoch 44/300, seasonal_2 Loss: 0.9162 | 1.8345
Epoch 45/300, seasonal_2 Loss: 0.9164 | 1.8343
Epoch 46/300, seasonal_2 Loss: 0.9133 | 1.8389
Epoch 47/300, seasonal_2 Loss: 0.9130 | 1.8424
Epoch 48/300, seasonal_2 Loss: 0.9126 | 1.8421
Epoch 49/300, seasonal_2 Loss: 0.9127 | 1.8417
Epoch 50/300, seasonal_2 Loss: 0.9128 | 1.8415
Epoch 51/300, seasonal_2 Loss: 0.9103 | 1.8445
Epoch 52/300, seasonal_2 Loss: 0.9102 | 1.8473
Epoch 53/300, seasonal_2 Loss: 0.9099 | 1.8476
Epoch 54/300, seasonal_2 Loss: 0.9100 | 1.8473
Epoch 55/300, seasonal_2 Loss: 0.9101 | 1.8471
Epoch 56/300, seasonal_2 Loss: 0.9080 | 1.8491
Epoch 57/300, seasonal_2 Loss: 0.9080 | 1.8513
Epoch 58/300, seasonal_2 Loss: 0.9078 | 1.8518
Epoch 59/300, seasonal_2 Loss: 0.9078 | 1.8517
Epoch 60/300, seasonal_2 Loss: 0.9078 | 1.8516
Epoch 61/300, seasonal_2 Loss: 0.9061 | 1.8529
Epoch 62/300, seasonal_2 Loss: 0.9062 | 1.8545
Epoch 63/300, seasonal_2 Loss: 0.9060 | 1.8551
Epoch 64/300, seasonal_2 Loss: 0.9060 | 1.8552
Epoch 65/300, seasonal_2 Loss: 0.9061 | 1.8552
Epoch 66/300, seasonal_2 Loss: 0.9047 | 1.8560
Epoch 67/300, seasonal_2 Loss: 0.9047 | 1.8572
Epoch 68/300, seasonal_2 Loss: 0.9046 | 1.8578
Epoch 69/300, seasonal_2 Loss: 0.9046 | 1.8579
Epoch 70/300, seasonal_2 Loss: 0.9046 | 1.8580
Epoch 71/300, seasonal_2 Loss: 0.9035 | 1.8585
Epoch 72/300, seasonal_2 Loss: 0.9035 | 1.8594
Epoch 73/300, seasonal_2 Loss: 0.9035 | 1.8599
Epoch 74/300, seasonal_2 Loss: 0.9035 | 1.8601
Epoch 75/300, seasonal_2 Loss: 0.9035 | 1.8602
Epoch 76/300, seasonal_2 Loss: 0.9025 | 1.8606
Epoch 77/300, seasonal_2 Loss: 0.9026 | 1.8613
Epoch 78/300, seasonal_2 Loss: 0.9025 | 1.8617
Epoch 79/300, seasonal_2 Loss: 0.9025 | 1.8619
Epoch 80/300, seasonal_2 Loss: 0.9025 | 1.8620
Epoch 81/300, seasonal_2 Loss: 0.9018 | 1.8623
Epoch 82/300, seasonal_2 Loss: 0.9018 | 1.8628
Epoch 83/300, seasonal_2 Loss: 0.9018 | 1.8631
Epoch 84/300, seasonal_2 Loss: 0.9018 | 1.8633
Epoch 85/300, seasonal_2 Loss: 0.9018 | 1.8634
Epoch 86/300, seasonal_2 Loss: 0.9011 | 1.8637
Epoch 87/300, seasonal_2 Loss: 0.9012 | 1.8640
Epoch 88/300, seasonal_2 Loss: 0.9012 | 1.8643
Epoch 89/300, seasonal_2 Loss: 0.9012 | 1.8645
Epoch 90/300, seasonal_2 Loss: 0.9012 | 1.8646
Epoch 91/300, seasonal_2 Loss: 0.9006 | 1.8648
Epoch 92/300, seasonal_2 Loss: 0.9007 | 1.8650
Epoch 93/300, seasonal_2 Loss: 0.9006 | 1.8652
Epoch 94/300, seasonal_2 Loss: 0.9006 | 1.8654
Epoch 95/300, seasonal_2 Loss: 0.9006 | 1.8655
Epoch 96/300, seasonal_2 Loss: 0.9002 | 1.8657
Epoch 97/300, seasonal_2 Loss: 0.9002 | 1.8658
Epoch 98/300, seasonal_2 Loss: 0.9002 | 1.8660
Epoch 99/300, seasonal_2 Loss: 0.9002 | 1.8661
Epoch 100/300, seasonal_2 Loss: 0.9002 | 1.8663
Epoch 101/300, seasonal_2 Loss: 0.8999 | 1.8664
Epoch 102/300, seasonal_2 Loss: 0.8999 | 1.8665
Epoch 103/300, seasonal_2 Loss: 0.8999 | 1.8667
Epoch 104/300, seasonal_2 Loss: 0.8999 | 1.8668
Epoch 105/300, seasonal_2 Loss: 0.8999 | 1.8669
Epoch 106/300, seasonal_2 Loss: 0.8996 | 1.8670
Epoch 107/300, seasonal_2 Loss: 0.8996 | 1.8671
Epoch 108/300, seasonal_2 Loss: 0.8996 | 1.8672
Epoch 109/300, seasonal_2 Loss: 0.8996 | 1.8673
Epoch 110/300, seasonal_2 Loss: 0.8996 | 1.8674
Epoch 111/300, seasonal_2 Loss: 0.8994 | 1.8674
Epoch 112/300, seasonal_2 Loss: 0.8994 | 1.8675
Epoch 113/300, seasonal_2 Loss: 0.8994 | 1.8676
Epoch 114/300, seasonal_2 Loss: 0.8994 | 1.8677
Epoch 115/300, seasonal_2 Loss: 0.8994 | 1.8677
Epoch 116/300, seasonal_2 Loss: 0.8992 | 1.8678
Epoch 117/300, seasonal_2 Loss: 0.8992 | 1.8679
Epoch 118/300, seasonal_2 Loss: 0.8992 | 1.8680
Epoch 119/300, seasonal_2 Loss: 0.8992 | 1.8680
Epoch 120/300, seasonal_2 Loss: 0.8992 | 1.8681
Epoch 121/300, seasonal_2 Loss: 0.8991 | 1.8681
Epoch 122/300, seasonal_2 Loss: 0.8991 | 1.8682
Epoch 123/300, seasonal_2 Loss: 0.8991 | 1.8682
Epoch 124/300, seasonal_2 Loss: 0.8991 | 1.8683
Epoch 125/300, seasonal_2 Loss: 0.8991 | 1.8683
Epoch 126/300, seasonal_2 Loss: 0.8990 | 1.8684
Epoch 127/300, seasonal_2 Loss: 0.8990 | 1.8684
Epoch 128/300, seasonal_2 Loss: 0.8990 | 1.8685
Epoch 129/300, seasonal_2 Loss: 0.8990 | 1.8685
Epoch 130/300, seasonal_2 Loss: 0.8990 | 1.8685
Epoch 131/300, seasonal_2 Loss: 0.8989 | 1.8686
Epoch 132/300, seasonal_2 Loss: 0.8989 | 1.8686
Epoch 133/300, seasonal_2 Loss: 0.8989 | 1.8686
Epoch 134/300, seasonal_2 Loss: 0.8989 | 1.8687
Epoch 135/300, seasonal_2 Loss: 0.8989 | 1.8687
Epoch 136/300, seasonal_2 Loss: 0.8988 | 1.8687
Epoch 137/300, seasonal_2 Loss: 0.8988 | 1.8688
Epoch 138/300, seasonal_2 Loss: 0.8988 | 1.8688
Epoch 139/300, seasonal_2 Loss: 0.8988 | 1.8688
Epoch 140/300, seasonal_2 Loss: 0.8988 | 1.8688
Epoch 141/300, seasonal_2 Loss: 0.8987 | 1.8689
Epoch 142/300, seasonal_2 Loss: 0.8987 | 1.8689
Epoch 143/300, seasonal_2 Loss: 0.8987 | 1.8689
Epoch 144/300, seasonal_2 Loss: 0.8987 | 1.8689
Epoch 145/300, seasonal_2 Loss: 0.8987 | 1.8690
Epoch 146/300, seasonal_2 Loss: 0.8987 | 1.8690
Epoch 147/300, seasonal_2 Loss: 0.8987 | 1.8690
Epoch 148/300, seasonal_2 Loss: 0.8987 | 1.8690
Epoch 149/300, seasonal_2 Loss: 0.8987 | 1.8690
Epoch 150/300, seasonal_2 Loss: 0.8987 | 1.8690
Epoch 151/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 152/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 153/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 154/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 155/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 156/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 157/300, seasonal_2 Loss: 0.8986 | 1.8691
Epoch 158/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 159/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 160/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 161/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 162/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 163/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 164/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 165/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 166/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 167/300, seasonal_2 Loss: 0.8986 | 1.8692
Epoch 168/300, seasonal_2 Loss: 0.8986 | 1.8693
Epoch 169/300, seasonal_2 Loss: 0.8986 | 1.8693
Epoch 170/300, seasonal_2 Loss: 0.8986 | 1.8693
Epoch 171/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 172/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 173/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 174/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 175/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 176/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 177/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 178/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 179/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 180/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 181/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 182/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 183/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 184/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 185/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 186/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 187/300, seasonal_2 Loss: 0.8985 | 1.8693
Epoch 188/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 189/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 190/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 191/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 192/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 193/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 194/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 195/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 196/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 197/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 198/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 199/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 200/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 201/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 202/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 203/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 204/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 205/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 206/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 207/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 208/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 209/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 210/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 211/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 212/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 213/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 214/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 215/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 216/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 217/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 218/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 219/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 220/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 221/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 222/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 223/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 224/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 225/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 226/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 227/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 228/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 229/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 230/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 231/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 232/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 233/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 234/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 235/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 236/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 237/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 238/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 239/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 240/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 241/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 242/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 243/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 244/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 245/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 246/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 247/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 248/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 249/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 250/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 251/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 252/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 253/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 254/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 255/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 256/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 257/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 258/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 259/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 260/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 261/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 262/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 263/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 264/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 265/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 266/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 267/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 268/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 269/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 270/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 271/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 272/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 273/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 274/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 275/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 276/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 277/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 278/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 279/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 280/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 281/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 282/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 283/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 284/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 285/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 286/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 287/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 288/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 289/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 290/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 291/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 292/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 293/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 294/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 295/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 296/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 297/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 298/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 299/300, seasonal_2 Loss: 0.8985 | 1.8694
Epoch 300/300, seasonal_2 Loss: 0.8985 | 1.8694
Training seasonal_3 component with params: {'observation_period_num': 9, 'train_rates': 0.9580658057577542, 'learning_rate': 0.0009608779262942285, 'batch_size': 246, 'step_size': 14, 'gamma': 0.804766940989789}
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py", line 629, in <module>
    models[comp], train_loss, valid_loss = train(
                                           ^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 32, in train
    output = model(data)
             ^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<@beartype(src.model.iTransformer.forward) at 0x7f0b02f900e0>", line 66, in forward
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 190, in forward
    x = ff(x) + x
        ^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 5.84 GiB already allocated; 7.44 MiB free; 6.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
