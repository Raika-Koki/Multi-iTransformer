[32m[I 2025-01-07 12:32:25,024][0m A new study created in memory with name: no-name-00201238-4a8e-4ded-823f-36bbc6ec7170[0m
[32m[I 2025-01-07 12:35:00,899][0m Trial 0 finished with value: 1.2044856373433437 and parameters: {'observation_period_num': 134, 'train_rates': 0.6846362144459037, 'learning_rate': 2.9827429228508072e-06, 'batch_size': 248, 'step_size': 14, 'gamma': 0.9867253247749996}. Best is trial 0 with value: 1.2044856373433437.[0m
[32m[I 2025-01-07 12:39:40,922][0m Trial 1 finished with value: 1.1677176904476723 and parameters: {'observation_period_num': 227, 'train_rates': 0.6547114701932709, 'learning_rate': 2.7969435424793174e-06, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9106263547367113}. Best is trial 1 with value: 1.1677176904476723.[0m
[32m[I 2025-01-07 12:44:21,365][0m Trial 2 finished with value: 0.30350897677813626 and parameters: {'observation_period_num': 203, 'train_rates': 0.8158932534346177, 'learning_rate': 4.426022297416039e-05, 'batch_size': 89, 'step_size': 8, 'gamma': 0.9701725911045109}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 12:49:55,251][0m Trial 3 finished with value: 0.3455342639129783 and parameters: {'observation_period_num': 236, 'train_rates': 0.8356880176701724, 'learning_rate': 0.0007233388448667918, 'batch_size': 203, 'step_size': 9, 'gamma': 0.8396076293703721}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 12:52:01,858][0m Trial 4 finished with value: 1.4259981576885497 and parameters: {'observation_period_num': 113, 'train_rates': 0.705715807536091, 'learning_rate': 1.5338889086499174e-06, 'batch_size': 167, 'step_size': 10, 'gamma': 0.9613200444205996}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 12:54:54,543][0m Trial 5 finished with value: 0.9521767406359963 and parameters: {'observation_period_num': 142, 'train_rates': 0.7396104921256843, 'learning_rate': 1.6603479556763498e-05, 'batch_size': 251, 'step_size': 4, 'gamma': 0.8659074220553533}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 12:57:58,285][0m Trial 6 finished with value: 0.5831998308624052 and parameters: {'observation_period_num': 109, 'train_rates': 0.8043810396424192, 'learning_rate': 0.0004681747633530226, 'batch_size': 19, 'step_size': 10, 'gamma': 0.9517489260463168}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 12:59:05,278][0m Trial 7 finished with value: 1.1749747487214894 and parameters: {'observation_period_num': 53, 'train_rates': 0.857245848980029, 'learning_rate': 1.15114350163885e-06, 'batch_size': 136, 'step_size': 12, 'gamma': 0.811472062235617}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 13:00:15,860][0m Trial 8 finished with value: 0.6724754529614603 and parameters: {'observation_period_num': 64, 'train_rates': 0.7331167879611922, 'learning_rate': 0.000440542188715615, 'batch_size': 156, 'step_size': 4, 'gamma': 0.9428193642512935}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 13:03:47,994][0m Trial 9 finished with value: 0.6681801549957003 and parameters: {'observation_period_num': 162, 'train_rates': 0.8208499026407367, 'learning_rate': 7.028654513447942e-05, 'batch_size': 215, 'step_size': 2, 'gamma': 0.8330647042226794}. Best is trial 2 with value: 0.30350897677813626.[0m
[32m[I 2025-01-07 13:08:34,359][0m Trial 10 finished with value: 0.21324093639850616 and parameters: {'observation_period_num': 192, 'train_rates': 0.9679753709451737, 'learning_rate': 5.231889308638361e-05, 'batch_size': 90, 'step_size': 7, 'gamma': 0.7579754617166404}. Best is trial 10 with value: 0.21324093639850616.[0m
[32m[I 2025-01-07 13:13:53,472][0m Trial 11 finished with value: 0.21624492079887575 and parameters: {'observation_period_num': 206, 'train_rates': 0.9628501524260239, 'learning_rate': 5.7527384779555664e-05, 'batch_size': 91, 'step_size': 7, 'gamma': 0.758275321994921}. Best is trial 10 with value: 0.21324093639850616.[0m
[32m[I 2025-01-07 13:18:33,385][0m Trial 12 finished with value: 0.1349145919084549 and parameters: {'observation_period_num': 184, 'train_rates': 0.9747759600577135, 'learning_rate': 0.0001103353434347144, 'batch_size': 93, 'step_size': 6, 'gamma': 0.7554331113145897}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:22:48,143][0m Trial 13 finished with value: 0.23615198306463384 and parameters: {'observation_period_num': 172, 'train_rates': 0.9614565221055102, 'learning_rate': 0.00015446699149754628, 'batch_size': 97, 'step_size': 5, 'gamma': 0.7618050520800687}. Best is trial 12 with value: 0.1349145919084549.[0m
Early stopping at epoch 78
[32m[I 2025-01-07 13:26:14,193][0m Trial 14 finished with value: 0.9647653422704557 and parameters: {'observation_period_num': 178, 'train_rates': 0.9119445536182293, 'learning_rate': 1.2012129298952673e-05, 'batch_size': 60, 'step_size': 1, 'gamma': 0.7940147104733277}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:26:53,791][0m Trial 15 finished with value: 0.19898919761180878 and parameters: {'observation_period_num': 13, 'train_rates': 0.9888855670054736, 'learning_rate': 0.0001468222763345125, 'batch_size': 121, 'step_size': 3, 'gamma': 0.7812099362862609}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:27:29,112][0m Trial 16 finished with value: 0.2479293305199246 and parameters: {'observation_period_num': 5, 'train_rates': 0.8951746684730487, 'learning_rate': 0.00018089022922178078, 'batch_size': 122, 'step_size': 3, 'gamma': 0.7912539051275681}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:27:58,869][0m Trial 17 finished with value: 0.3361870113497453 and parameters: {'observation_period_num': 18, 'train_rates': 0.9182499540036309, 'learning_rate': 0.00020740630247260576, 'batch_size': 172, 'step_size': 1, 'gamma': 0.8834598123499964}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:29:54,967][0m Trial 18 finished with value: 0.43641212582588196 and parameters: {'observation_period_num': 82, 'train_rates': 0.9795169438726967, 'learning_rate': 1.9034019549302214e-05, 'batch_size': 121, 'step_size': 5, 'gamma': 0.7892385468075538}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:36:19,400][0m Trial 19 finished with value: 0.24040906596635042 and parameters: {'observation_period_num': 251, 'train_rates': 0.8833848535459591, 'learning_rate': 0.00010229208588837847, 'batch_size': 58, 'step_size': 3, 'gamma': 0.826224393208141}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:39:05,826][0m Trial 20 finished with value: 0.27125944123893486 and parameters: {'observation_period_num': 28, 'train_rates': 0.9377174395685287, 'learning_rate': 0.00036103076091651977, 'batch_size': 24, 'step_size': 15, 'gamma': 0.8614094106010192}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:43:53,945][0m Trial 21 finished with value: 0.3403502404689789 and parameters: {'observation_period_num': 192, 'train_rates': 0.9870758293547497, 'learning_rate': 2.8743316977386073e-05, 'batch_size': 105, 'step_size': 6, 'gamma': 0.7569823049714635}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:47:39,065][0m Trial 22 finished with value: 0.5807267434582739 and parameters: {'observation_period_num': 155, 'train_rates': 0.9406887450111122, 'learning_rate': 7.63243757266462e-06, 'batch_size': 82, 'step_size': 7, 'gamma': 0.7780109044251815}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:51:57,244][0m Trial 23 finished with value: 0.9571563324458162 and parameters: {'observation_period_num': 222, 'train_rates': 0.6117697309151422, 'learning_rate': 9.354674927346779e-05, 'batch_size': 140, 'step_size': 8, 'gamma': 0.7765561459903276}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:54:25,782][0m Trial 24 finished with value: 0.1841641217470169 and parameters: {'observation_period_num': 102, 'train_rates': 0.9899682465412711, 'learning_rate': 3.1375686636554184e-05, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8103508981480291}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:56:26,406][0m Trial 25 finished with value: 0.27703040628955605 and parameters: {'observation_period_num': 86, 'train_rates': 0.938371049284425, 'learning_rate': 3.5691972350891266e-05, 'batch_size': 73, 'step_size': 5, 'gamma': 0.8081928710234435}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:57:25,852][0m Trial 26 finished with value: 0.2820591103110014 and parameters: {'observation_period_num': 45, 'train_rates': 0.8694044949859439, 'learning_rate': 0.0002469321407821462, 'batch_size': 111, 'step_size': 3, 'gamma': 0.813775080269397}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 13:59:55,022][0m Trial 27 finished with value: 0.40457337764911777 and parameters: {'observation_period_num': 114, 'train_rates': 0.7831774383697282, 'learning_rate': 0.00011572474718507853, 'batch_size': 41, 'step_size': 4, 'gamma': 0.8451958669156766}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 14:01:56,586][0m Trial 28 finished with value: 0.43032721344661334 and parameters: {'observation_period_num': 88, 'train_rates': 0.9120611892250272, 'learning_rate': 8.458216278487831e-06, 'batch_size': 67, 'step_size': 6, 'gamma': 0.7765929462551849}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 14:05:09,799][0m Trial 29 finished with value: 0.3137019872665405 and parameters: {'observation_period_num': 135, 'train_rates': 0.953901733251301, 'learning_rate': 2.8008627719332795e-05, 'batch_size': 192, 'step_size': 12, 'gamma': 0.8020086573126383}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 14:06:00,181][0m Trial 30 finished with value: 0.5555816934222266 and parameters: {'observation_period_num': 34, 'train_rates': 0.9284543014744928, 'learning_rate': 0.0008277220572154731, 'batch_size': 121, 'step_size': 2, 'gamma': 0.774859325843638}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 14:10:49,748][0m Trial 31 finished with value: 0.17254196107387543 and parameters: {'observation_period_num': 188, 'train_rates': 0.9880825321874701, 'learning_rate': 6.62865393315127e-05, 'batch_size': 83, 'step_size': 7, 'gamma': 0.7555660042263045}. Best is trial 12 with value: 0.1349145919084549.[0m
[32m[I 2025-01-07 14:14:43,397][0m Trial 32 finished with value: 0.12427148222923279 and parameters: {'observation_period_num': 151, 'train_rates': 0.9850956647759479, 'learning_rate': 7.569861181304319e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.751138501692635}. Best is trial 32 with value: 0.12427148222923279.[0m
[32m[I 2025-01-07 14:18:41,612][0m Trial 33 finished with value: 0.11100635677576065 and parameters: {'observation_period_num': 150, 'train_rates': 0.9876638678808143, 'learning_rate': 7.614391026318165e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9039759970964345}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:22:29,989][0m Trial 34 finished with value: 0.1593578444925819 and parameters: {'observation_period_num': 148, 'train_rates': 0.9549966772412921, 'learning_rate': 6.700765550309681e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8945894522359946}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:26:17,890][0m Trial 35 finished with value: 0.18113215267658234 and parameters: {'observation_period_num': 148, 'train_rates': 0.954253277929758, 'learning_rate': 8.068837507150742e-05, 'batch_size': 43, 'step_size': 9, 'gamma': 0.9063642825013889}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:29:38,490][0m Trial 36 finished with value: 0.21215139059447247 and parameters: {'observation_period_num': 131, 'train_rates': 0.8988723769930518, 'learning_rate': 0.00029289079919061394, 'batch_size': 33, 'step_size': 11, 'gamma': 0.9214262790395846}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:33:48,706][0m Trial 37 finished with value: 0.4092208464254203 and parameters: {'observation_period_num': 166, 'train_rates': 0.7725924206411493, 'learning_rate': 5.1169957310777766e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9105170525736972}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:36:31,883][0m Trial 38 finished with value: 0.33406782332119217 and parameters: {'observation_period_num': 123, 'train_rates': 0.8518097361620247, 'learning_rate': 1.9907662994974016e-05, 'batch_size': 53, 'step_size': 11, 'gamma': 0.8860356268903903}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:42:17,473][0m Trial 39 finished with value: 0.22584179023768278 and parameters: {'observation_period_num': 214, 'train_rates': 0.9459432602860308, 'learning_rate': 0.00012101421995886466, 'batch_size': 36, 'step_size': 8, 'gamma': 0.9798785971640589}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:45:23,496][0m Trial 40 finished with value: 0.8742617924489444 and parameters: {'observation_period_num': 153, 'train_rates': 0.686798069757854, 'learning_rate': 4.3715209504808856e-06, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9273507596252466}. Best is trial 33 with value: 0.11100635677576065.[0m
[32m[I 2025-01-07 14:50:23,604][0m Trial 41 finished with value: 0.09295018705763394 and parameters: {'observation_period_num': 186, 'train_rates': 0.9716289035495496, 'learning_rate': 7.1004858088211e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.8903285120341123}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 14:55:17,850][0m Trial 42 finished with value: 0.09643216617405415 and parameters: {'observation_period_num': 184, 'train_rates': 0.9712595411127897, 'learning_rate': 4.221294607443073e-05, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8939273708111822}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:00:15,778][0m Trial 43 finished with value: 0.14683062692574406 and parameters: {'observation_period_num': 183, 'train_rates': 0.960911602715599, 'learning_rate': 3.992236041340766e-05, 'batch_size': 27, 'step_size': 8, 'gamma': 0.8541738198038441}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:05:31,340][0m Trial 44 finished with value: 0.12404161127837929 and parameters: {'observation_period_num': 200, 'train_rates': 0.9732857405647746, 'learning_rate': 4.1492907865121894e-05, 'batch_size': 52, 'step_size': 10, 'gamma': 0.9380879364153235}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:10:43,821][0m Trial 45 finished with value: 0.16654227430572605 and parameters: {'observation_period_num': 203, 'train_rates': 0.9269393017042704, 'learning_rate': 4.476206541877005e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.937872943114222}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:17:21,043][0m Trial 46 finished with value: 0.10893384326954146 and parameters: {'observation_period_num': 235, 'train_rates': 0.9730743388882199, 'learning_rate': 2.0840523138078518e-05, 'batch_size': 26, 'step_size': 11, 'gamma': 0.9601301788093505}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:23:38,405][0m Trial 47 finished with value: 0.258540540933609 and parameters: {'observation_period_num': 237, 'train_rates': 0.9689755157438837, 'learning_rate': 1.3312435663780797e-05, 'batch_size': 236, 'step_size': 12, 'gamma': 0.9659233068481241}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:30:07,544][0m Trial 48 finished with value: 0.17384398222839745 and parameters: {'observation_period_num': 230, 'train_rates': 0.8998640959086268, 'learning_rate': 2.7152055363798077e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9491921985617061}. Best is trial 41 with value: 0.09295018705763394.[0m
[32m[I 2025-01-07 15:35:16,183][0m Trial 49 finished with value: 0.2918978679298827 and parameters: {'observation_period_num': 205, 'train_rates': 0.8364629342165948, 'learning_rate': 2.097844794110914e-05, 'batch_size': 26, 'step_size': 11, 'gamma': 0.9893941791628743}. Best is trial 41 with value: 0.09295018705763394.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.5570 | 0.6481
Epoch 2/300, Loss: 0.4115 | 0.4826
Epoch 3/300, Loss: 0.3348 | 0.4106
Epoch 4/300, Loss: 0.2805 | 0.3881
Epoch 5/300, Loss: 0.2536 | 0.3757
Epoch 6/300, Loss: 0.2404 | 0.3028
Epoch 7/300, Loss: 0.2301 | 0.2894
Epoch 8/300, Loss: 0.2254 | 0.2672
Epoch 9/300, Loss: 0.2187 | 0.2570
Epoch 10/300, Loss: 0.2292 | 0.2613
Epoch 11/300, Loss: 0.1996 | 0.2518
Epoch 12/300, Loss: 0.2031 | 0.2504
Epoch 13/300, Loss: 0.1831 | 0.2083
Epoch 14/300, Loss: 0.1933 | 0.2087
Epoch 15/300, Loss: 0.1731 | 0.2028
Epoch 16/300, Loss: 0.1681 | 0.2067
Epoch 17/300, Loss: 0.1691 | 0.1802
Epoch 18/300, Loss: 0.1723 | 0.1952
Epoch 19/300, Loss: 0.1688 | 0.1883
Epoch 20/300, Loss: 0.1663 | 0.1991
Epoch 21/300, Loss: 0.1639 | 0.1785
Epoch 22/300, Loss: 0.1645 | 0.1732
Epoch 23/300, Loss: 0.1634 | 0.1767
Epoch 24/300, Loss: 0.1588 | 0.1793
Epoch 25/300, Loss: 0.1555 | 0.1633
Epoch 26/300, Loss: 0.1518 | 0.1671
Epoch 27/300, Loss: 0.1510 | 0.1591
Epoch 28/300, Loss: 0.1449 | 0.1592
Epoch 29/300, Loss: 0.1438 | 0.1505
Epoch 30/300, Loss: 0.1399 | 0.1421
Epoch 31/300, Loss: 0.1388 | 0.1409
Epoch 32/300, Loss: 0.1351 | 0.1445
Epoch 33/300, Loss: 0.1359 | 0.1414
Epoch 34/300, Loss: 0.1334 | 0.1357
Epoch 35/300, Loss: 0.1323 | 0.1355
Epoch 36/300, Loss: 0.1321 | 0.1390
Epoch 37/300, Loss: 0.1307 | 0.1333
Epoch 38/300, Loss: 0.1286 | 0.1269
Epoch 39/300, Loss: 0.1278 | 0.1273
Epoch 40/300, Loss: 0.1255 | 0.1284
Epoch 41/300, Loss: 0.1256 | 0.1273
Epoch 42/300, Loss: 0.1232 | 0.1267
Epoch 43/300, Loss: 0.1230 | 0.1214
Epoch 44/300, Loss: 0.1229 | 0.1215
Epoch 45/300, Loss: 0.1227 | 0.1221
Epoch 46/300, Loss: 0.1209 | 0.1165
Epoch 47/300, Loss: 0.1201 | 0.1182
Epoch 48/300, Loss: 0.1206 | 0.1183
Epoch 49/300, Loss: 0.1200 | 0.1234
Epoch 50/300, Loss: 0.1197 | 0.1185
Epoch 51/300, Loss: 0.1193 | 0.1109
Epoch 52/300, Loss: 0.1179 | 0.1122
Epoch 53/300, Loss: 0.1178 | 0.1157
Epoch 54/300, Loss: 0.1169 | 0.1153
Epoch 55/300, Loss: 0.1155 | 0.1112
Epoch 56/300, Loss: 0.1149 | 0.1095
Epoch 57/300, Loss: 0.1144 | 0.1106
Epoch 58/300, Loss: 0.1139 | 0.1103
Epoch 59/300, Loss: 0.1125 | 0.1067
Epoch 60/300, Loss: 0.1123 | 0.1056
Epoch 61/300, Loss: 0.1116 | 0.1071
Epoch 62/300, Loss: 0.1109 | 0.1068
Epoch 63/300, Loss: 0.1096 | 0.1042
Epoch 64/300, Loss: 0.1090 | 0.1038
Epoch 65/300, Loss: 0.1081 | 0.1035
Epoch 66/300, Loss: 0.1082 | 0.1040
Epoch 67/300, Loss: 0.1085 | 0.1025
Epoch 68/300, Loss: 0.1069 | 0.1023
Epoch 69/300, Loss: 0.1067 | 0.1013
Epoch 70/300, Loss: 0.1064 | 0.1009
Epoch 71/300, Loss: 0.1060 | 0.1011
Epoch 72/300, Loss: 0.1050 | 0.1010
Epoch 73/300, Loss: 0.1059 | 0.0999
Epoch 74/300, Loss: 0.1043 | 0.0996
Epoch 75/300, Loss: 0.1044 | 0.0993
Epoch 76/300, Loss: 0.1044 | 0.0989
Epoch 77/300, Loss: 0.1044 | 0.0991
Epoch 78/300, Loss: 0.1037 | 0.0983
Epoch 79/300, Loss: 0.1038 | 0.0983
Epoch 80/300, Loss: 0.1036 | 0.0973
Epoch 81/300, Loss: 0.1029 | 0.0988
Epoch 82/300, Loss: 0.1018 | 0.0970
Epoch 83/300, Loss: 0.1022 | 0.0970
Epoch 84/300, Loss: 0.1019 | 0.0969
Epoch 85/300, Loss: 0.1010 | 0.0967
Epoch 86/300, Loss: 0.1013 | 0.0966
Epoch 87/300, Loss: 0.1017 | 0.0963
Epoch 88/300, Loss: 0.1012 | 0.0958
Epoch 89/300, Loss: 0.1002 | 0.0960
Epoch 90/300, Loss: 0.1002 | 0.0957
Epoch 91/300, Loss: 0.1006 | 0.0951
Epoch 92/300, Loss: 0.0997 | 0.0948
Epoch 93/300, Loss: 0.0999 | 0.0947
Epoch 94/300, Loss: 0.0992 | 0.0944
Epoch 95/300, Loss: 0.0995 | 0.0945
Epoch 96/300, Loss: 0.0993 | 0.0944
Epoch 97/300, Loss: 0.0992 | 0.0940
Epoch 98/300, Loss: 0.0990 | 0.0943
Epoch 99/300, Loss: 0.0986 | 0.0938
Epoch 100/300, Loss: 0.0985 | 0.0934
Epoch 101/300, Loss: 0.0989 | 0.0933
Epoch 102/300, Loss: 0.0978 | 0.0930
Epoch 103/300, Loss: 0.0977 | 0.0930
Epoch 104/300, Loss: 0.0978 | 0.0923
Epoch 105/300, Loss: 0.0977 | 0.0927
Epoch 106/300, Loss: 0.0974 | 0.0926
Epoch 107/300, Loss: 0.0976 | 0.0926
Epoch 108/300, Loss: 0.0972 | 0.0926
Epoch 109/300, Loss: 0.0965 | 0.0925
Epoch 110/300, Loss: 0.0967 | 0.0921
Epoch 111/300, Loss: 0.0969 | 0.0926
Epoch 112/300, Loss: 0.0963 | 0.0927
Epoch 113/300, Loss: 0.0966 | 0.0924
Epoch 114/300, Loss: 0.0968 | 0.0922
Epoch 115/300, Loss: 0.0960 | 0.0920
Epoch 116/300, Loss: 0.0964 | 0.0920
Epoch 117/300, Loss: 0.0962 | 0.0918
Epoch 118/300, Loss: 0.0957 | 0.0919
Epoch 119/300, Loss: 0.0962 | 0.0917
Epoch 120/300, Loss: 0.0956 | 0.0916
Epoch 121/300, Loss: 0.0956 | 0.0919
Epoch 122/300, Loss: 0.0959 | 0.0919
Epoch 123/300, Loss: 0.0960 | 0.0917
Epoch 124/300, Loss: 0.0962 | 0.0917
Epoch 125/300, Loss: 0.0955 | 0.0916
Epoch 126/300, Loss: 0.0954 | 0.0917
Epoch 127/300, Loss: 0.0955 | 0.0914
Epoch 128/300, Loss: 0.0958 | 0.0917
Epoch 129/300, Loss: 0.0949 | 0.0915
Epoch 130/300, Loss: 0.0946 | 0.0910
Epoch 131/300, Loss: 0.0954 | 0.0910
Epoch 132/300, Loss: 0.0954 | 0.0908
Epoch 133/300, Loss: 0.0949 | 0.0908
Epoch 134/300, Loss: 0.0960 | 0.0909
Epoch 135/300, Loss: 0.0950 | 0.0911
Epoch 136/300, Loss: 0.0944 | 0.0908
Epoch 137/300, Loss: 0.0943 | 0.0909
Epoch 138/300, Loss: 0.0943 | 0.0908
Epoch 139/300, Loss: 0.0951 | 0.0909
Epoch 140/300, Loss: 0.0946 | 0.0908
Epoch 141/300, Loss: 0.0947 | 0.0905
Epoch 142/300, Loss: 0.0949 | 0.0906
Epoch 143/300, Loss: 0.0951 | 0.0907
Epoch 144/300, Loss: 0.0945 | 0.0907
Epoch 145/300, Loss: 0.0949 | 0.0905
Epoch 146/300, Loss: 0.0946 | 0.0905
Epoch 147/300, Loss: 0.0940 | 0.0905
Epoch 148/300, Loss: 0.0943 | 0.0907
Epoch 149/300, Loss: 0.0947 | 0.0908
Epoch 150/300, Loss: 0.0937 | 0.0907
Epoch 151/300, Loss: 0.0946 | 0.0904
Epoch 152/300, Loss: 0.0938 | 0.0903
Epoch 153/300, Loss: 0.0937 | 0.0903
Epoch 154/300, Loss: 0.0929 | 0.0903
Epoch 155/300, Loss: 0.0944 | 0.0902
Epoch 156/300, Loss: 0.0935 | 0.0902
Epoch 157/300, Loss: 0.0945 | 0.0903
Epoch 158/300, Loss: 0.0940 | 0.0902
Epoch 159/300, Loss: 0.0937 | 0.0902
Epoch 160/300, Loss: 0.0944 | 0.0901
Epoch 161/300, Loss: 0.0937 | 0.0899
Epoch 162/300, Loss: 0.0939 | 0.0898
Epoch 163/300, Loss: 0.0940 | 0.0899
Epoch 164/300, Loss: 0.0931 | 0.0901
Epoch 165/300, Loss: 0.0932 | 0.0902
Epoch 166/300, Loss: 0.0935 | 0.0899
Epoch 167/300, Loss: 0.0940 | 0.0899
Epoch 168/300, Loss: 0.0940 | 0.0900
Epoch 169/300, Loss: 0.0936 | 0.0899
Epoch 170/300, Loss: 0.0946 | 0.0899
Epoch 171/300, Loss: 0.0936 | 0.0899
Epoch 172/300, Loss: 0.0935 | 0.0899
Epoch 173/300, Loss: 0.0934 | 0.0900
Epoch 174/300, Loss: 0.0941 | 0.0900
Epoch 175/300, Loss: 0.0941 | 0.0899
Epoch 176/300, Loss: 0.0928 | 0.0898
Epoch 177/300, Loss: 0.0925 | 0.0898
Epoch 178/300, Loss: 0.0934 | 0.0899
Epoch 179/300, Loss: 0.0933 | 0.0898
Epoch 180/300, Loss: 0.0933 | 0.0898
Epoch 181/300, Loss: 0.0931 | 0.0898
Epoch 182/300, Loss: 0.0941 | 0.0897
Epoch 183/300, Loss: 0.0937 | 0.0899
Epoch 184/300, Loss: 0.0934 | 0.0899
Epoch 185/300, Loss: 0.0936 | 0.0898
Epoch 186/300, Loss: 0.0930 | 0.0898
Epoch 187/300, Loss: 0.0934 | 0.0897
Epoch 188/300, Loss: 0.0934 | 0.0897
Epoch 189/300, Loss: 0.0935 | 0.0897
Epoch 190/300, Loss: 0.0936 | 0.0897
Epoch 191/300, Loss: 0.0936 | 0.0896
Epoch 192/300, Loss: 0.0938 | 0.0897
Epoch 193/300, Loss: 0.0933 | 0.0897
Epoch 194/300, Loss: 0.0932 | 0.0896
Epoch 195/300, Loss: 0.0925 | 0.0897
Epoch 196/300, Loss: 0.0934 | 0.0898
Epoch 197/300, Loss: 0.0938 | 0.0897
Epoch 198/300, Loss: 0.0928 | 0.0897
Epoch 199/300, Loss: 0.0931 | 0.0897
Epoch 200/300, Loss: 0.0930 | 0.0897
Epoch 201/300, Loss: 0.0929 | 0.0897
Epoch 202/300, Loss: 0.0935 | 0.0897
Epoch 203/300, Loss: 0.0932 | 0.0896
Epoch 204/300, Loss: 0.0937 | 0.0896
Epoch 205/300, Loss: 0.0930 | 0.0897
Epoch 206/300, Loss: 0.0932 | 0.0897
Epoch 207/300, Loss: 0.0928 | 0.0897
Epoch 208/300, Loss: 0.0929 | 0.0897
Epoch 209/300, Loss: 0.0932 | 0.0896
Epoch 210/300, Loss: 0.0930 | 0.0896
Epoch 211/300, Loss: 0.0940 | 0.0896
Epoch 212/300, Loss: 0.0933 | 0.0896
Epoch 213/300, Loss: 0.0932 | 0.0896
Epoch 214/300, Loss: 0.0939 | 0.0896
Epoch 215/300, Loss: 0.0930 | 0.0896
Epoch 216/300, Loss: 0.0933 | 0.0896
Epoch 217/300, Loss: 0.0929 | 0.0896
Epoch 218/300, Loss: 0.0934 | 0.0896
Epoch 219/300, Loss: 0.0929 | 0.0896
Epoch 220/300, Loss: 0.0933 | 0.0896
Epoch 221/300, Loss: 0.0935 | 0.0896
Epoch 222/300, Loss: 0.0935 | 0.0896
Epoch 223/300, Loss: 0.0931 | 0.0896
Epoch 224/300, Loss: 0.0935 | 0.0895
Epoch 225/300, Loss: 0.0927 | 0.0895
Epoch 226/300, Loss: 0.0933 | 0.0896
Epoch 227/300, Loss: 0.0925 | 0.0895
Epoch 228/300, Loss: 0.0929 | 0.0895
Epoch 229/300, Loss: 0.0934 | 0.0895
Epoch 230/300, Loss: 0.0936 | 0.0895
Epoch 231/300, Loss: 0.0931 | 0.0895
Epoch 232/300, Loss: 0.0932 | 0.0895
Epoch 233/300, Loss: 0.0929 | 0.0895
Epoch 234/300, Loss: 0.0929 | 0.0895
Epoch 235/300, Loss: 0.0928 | 0.0895
Epoch 236/300, Loss: 0.0926 | 0.0895
Epoch 237/300, Loss: 0.0929 | 0.0895
Epoch 238/300, Loss: 0.0933 | 0.0895
Epoch 239/300, Loss: 0.0931 | 0.0895
Epoch 240/300, Loss: 0.0930 | 0.0895
Epoch 241/300, Loss: 0.0936 | 0.0895
Epoch 242/300, Loss: 0.0932 | 0.0895
Epoch 243/300, Loss: 0.0934 | 0.0895
Epoch 244/300, Loss: 0.0926 | 0.0895
Epoch 245/300, Loss: 0.0929 | 0.0895
Epoch 246/300, Loss: 0.0924 | 0.0895
Epoch 247/300, Loss: 0.0929 | 0.0895
Epoch 248/300, Loss: 0.0931 | 0.0895
Epoch 249/300, Loss: 0.0926 | 0.0895
Epoch 250/300, Loss: 0.0935 | 0.0895
Epoch 251/300, Loss: 0.0934 | 0.0895
Epoch 252/300, Loss: 0.0930 | 0.0895
Epoch 253/300, Loss: 0.0930 | 0.0895
Epoch 254/300, Loss: 0.0931 | 0.0895
Epoch 255/300, Loss: 0.0932 | 0.0895
Epoch 256/300, Loss: 0.0931 | 0.0895
Epoch 257/300, Loss: 0.0933 | 0.0895
Epoch 258/300, Loss: 0.0929 | 0.0895
Epoch 259/300, Loss: 0.0930 | 0.0895
Epoch 260/300, Loss: 0.0934 | 0.0895
Epoch 261/300, Loss: 0.0930 | 0.0895
Epoch 262/300, Loss: 0.0929 | 0.0895
Epoch 263/300, Loss: 0.0931 | 0.0895
Epoch 264/300, Loss: 0.0926 | 0.0895
Epoch 265/300, Loss: 0.0927 | 0.0895
Epoch 266/300, Loss: 0.0928 | 0.0895
Epoch 267/300, Loss: 0.0935 | 0.0895
Epoch 268/300, Loss: 0.0936 | 0.0895
Epoch 269/300, Loss: 0.0940 | 0.0895
Epoch 270/300, Loss: 0.0935 | 0.0895
Epoch 271/300, Loss: 0.0922 | 0.0894
Epoch 272/300, Loss: 0.0925 | 0.0894
Epoch 273/300, Loss: 0.0930 | 0.0894
Epoch 274/300, Loss: 0.0933 | 0.0894
Epoch 275/300, Loss: 0.0932 | 0.0894
Epoch 276/300, Loss: 0.0935 | 0.0894
Epoch 277/300, Loss: 0.0926 | 0.0894
Epoch 278/300, Loss: 0.0938 | 0.0894
Epoch 279/300, Loss: 0.0930 | 0.0894
Epoch 280/300, Loss: 0.0933 | 0.0894
Epoch 281/300, Loss: 0.0930 | 0.0894
Epoch 282/300, Loss: 0.0927 | 0.0894
Epoch 283/300, Loss: 0.0935 | 0.0894
Epoch 284/300, Loss: 0.0925 | 0.0894
Epoch 285/300, Loss: 0.0927 | 0.0894
Epoch 286/300, Loss: 0.0934 | 0.0894
Epoch 287/300, Loss: 0.0929 | 0.0894
Epoch 288/300, Loss: 0.0926 | 0.0894
Epoch 289/300, Loss: 0.0934 | 0.0894
Epoch 290/300, Loss: 0.0931 | 0.0894
Epoch 291/300, Loss: 0.0929 | 0.0894
Epoch 292/300, Loss: 0.0931 | 0.0894
Epoch 293/300, Loss: 0.0928 | 0.0894
Epoch 294/300, Loss: 0.0936 | 0.0894
Epoch 295/300, Loss: 0.0929 | 0.0894
Epoch 296/300, Loss: 0.0931 | 0.0894
Epoch 297/300, Loss: 0.0927 | 0.0894
Epoch 298/300, Loss: 0.0933 | 0.0894
Epoch 299/300, Loss: 0.0929 | 0.0894
Epoch 300/300, Loss: 0.0930 | 0.0894
Runtime (seconds): 899.1258878707886
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 408.00905454182066
RMSE: 20.199234008789062
MAE: 20.199234008789062
R-squared: nan
[230.84076]
