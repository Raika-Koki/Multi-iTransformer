ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-06 17:51:14,319][0m A new study created in memory with name: no-name-abc73cc6-0753-4381-baac-6ab15b140ea2[0m
[32m[I 2025-02-06 17:52:32,672][0m Trial 0 finished with value: 1.0089870950672277 and parameters: {'observation_period_num': 224, 'train_rates': 0.6587633641570477, 'learning_rate': 2.175245890300099e-06, 'batch_size': 56, 'step_size': 4, 'gamma': 0.934605697427141}. Best is trial 0 with value: 1.0089870950672277.[0m
[32m[I 2025-02-06 17:53:08,282][0m Trial 1 finished with value: 1.1729081387076874 and parameters: {'observation_period_num': 102, 'train_rates': 0.9359565074273458, 'learning_rate': 1.2554312580943224e-06, 'batch_size': 169, 'step_size': 6, 'gamma': 0.9049576531330141}. Best is trial 0 with value: 1.0089870950672277.[0m
[32m[I 2025-02-06 17:53:32,181][0m Trial 2 finished with value: 1.463983990476283 and parameters: {'observation_period_num': 124, 'train_rates': 0.7889127550764443, 'learning_rate': 1.567243676470718e-06, 'batch_size': 245, 'step_size': 8, 'gamma': 0.8228301702318184}. Best is trial 0 with value: 1.0089870950672277.[0m
[32m[I 2025-02-06 17:54:22,506][0m Trial 3 finished with value: 0.9611975205304035 and parameters: {'observation_period_num': 117, 'train_rates': 0.6496489446023325, 'learning_rate': 3.0892205628818785e-06, 'batch_size': 91, 'step_size': 2, 'gamma': 0.9068939722266753}. Best is trial 3 with value: 0.9611975205304035.[0m
[32m[I 2025-02-06 17:54:49,784][0m Trial 4 finished with value: 0.5482589305439483 and parameters: {'observation_period_num': 122, 'train_rates': 0.682190292868893, 'learning_rate': 9.288377590999264e-06, 'batch_size': 184, 'step_size': 14, 'gamma': 0.8932187356760337}. Best is trial 4 with value: 0.5482589305439483.[0m
[32m[I 2025-02-06 17:55:25,727][0m Trial 5 finished with value: 0.4007560914417483 and parameters: {'observation_period_num': 143, 'train_rates': 0.6046128437355406, 'learning_rate': 1.6652531992464772e-05, 'batch_size': 129, 'step_size': 13, 'gamma': 0.763864655815517}. Best is trial 5 with value: 0.4007560914417483.[0m
[32m[I 2025-02-06 17:58:19,727][0m Trial 6 finished with value: 0.15021217796133784 and parameters: {'observation_period_num': 243, 'train_rates': 0.9094998446895168, 'learning_rate': 0.0002547995749003247, 'batch_size': 30, 'step_size': 3, 'gamma': 0.8344353398180038}. Best is trial 6 with value: 0.15021217796133784.[0m
[32m[I 2025-02-06 17:59:06,568][0m Trial 7 finished with value: 0.03982359803510162 and parameters: {'observation_period_num': 5, 'train_rates': 0.9464475277935315, 'learning_rate': 0.0001536927821137817, 'batch_size': 138, 'step_size': 13, 'gamma': 0.9239207463838247}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 17:59:41,881][0m Trial 8 finished with value: 0.2123906910419464 and parameters: {'observation_period_num': 203, 'train_rates': 0.9611157207035561, 'learning_rate': 0.00011402420960139244, 'batch_size': 171, 'step_size': 8, 'gamma': 0.9732711023858283}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:00:53,796][0m Trial 9 finished with value: 0.17218567571962834 and parameters: {'observation_period_num': 6, 'train_rates': 0.7924162831756553, 'learning_rate': 0.00010599544313802416, 'batch_size': 75, 'step_size': 15, 'gamma': 0.7800758813650444}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:01:19,690][0m Trial 10 finished with value: 0.04359505274172487 and parameters: {'observation_period_num': 10, 'train_rates': 0.862254327802392, 'learning_rate': 0.0009791757093279181, 'batch_size': 252, 'step_size': 11, 'gamma': 0.954860322390483}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:01:45,049][0m Trial 11 finished with value: 0.043737543020545645 and parameters: {'observation_period_num': 6, 'train_rates': 0.8722339058472784, 'learning_rate': 0.0009599374618122407, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9880802933318326}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:02:16,232][0m Trial 12 finished with value: 0.07504680007696152 and parameters: {'observation_period_num': 47, 'train_rates': 0.9882943254664274, 'learning_rate': 0.0009029070988224323, 'batch_size': 214, 'step_size': 11, 'gamma': 0.9547907840537925}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:03:01,758][0m Trial 13 finished with value: 0.06144731342792511 and parameters: {'observation_period_num': 60, 'train_rates': 0.8498253120430354, 'learning_rate': 0.0002639494103470876, 'batch_size': 126, 'step_size': 11, 'gamma': 0.930385961939015}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:03:29,981][0m Trial 14 finished with value: 0.08615107527744453 and parameters: {'observation_period_num': 52, 'train_rates': 0.8446468871829665, 'learning_rate': 5.328812684821532e-05, 'batch_size': 217, 'step_size': 12, 'gamma': 0.855815146064486}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:04:05,238][0m Trial 15 finished with value: 0.26770005809845654 and parameters: {'observation_period_num': 79, 'train_rates': 0.7356284917111922, 'learning_rate': 0.00026942850559818484, 'batch_size': 149, 'step_size': 9, 'gamma': 0.9498832755458984}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:05:02,673][0m Trial 16 finished with value: 0.14831510305404663 and parameters: {'observation_period_num': 169, 'train_rates': 0.89270586020949, 'learning_rate': 0.0005656048272008121, 'batch_size': 96, 'step_size': 15, 'gamma': 0.8746854305915498}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:05:30,940][0m Trial 17 finished with value: 0.10846474947351398 and parameters: {'observation_period_num': 30, 'train_rates': 0.8195925331564735, 'learning_rate': 4.506110748423767e-05, 'batch_size': 208, 'step_size': 10, 'gamma': 0.9249954854096705}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:09:14,430][0m Trial 18 finished with value: 0.12977315685371074 and parameters: {'observation_period_num': 83, 'train_rates': 0.9288941340992473, 'learning_rate': 0.00011731138846713608, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9618315000968747}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:10:00,710][0m Trial 19 finished with value: 0.1564535525409881 and parameters: {'observation_period_num': 29, 'train_rates': 0.7399567735679674, 'learning_rate': 0.0003639016363231823, 'batch_size': 109, 'step_size': 6, 'gamma': 0.8783297048282246}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:10:28,348][0m Trial 20 finished with value: 0.3827941119670868 and parameters: {'observation_period_num': 162, 'train_rates': 0.9842509421285042, 'learning_rate': 1.0195025630775849e-05, 'batch_size': 238, 'step_size': 13, 'gamma': 0.9847892258620208}. Best is trial 7 with value: 0.03982359803510162.[0m
[32m[I 2025-02-06 18:10:53,996][0m Trial 21 finished with value: 0.03507029089023885 and parameters: {'observation_period_num': 5, 'train_rates': 0.8753383092331523, 'learning_rate': 0.0008087096633273017, 'batch_size': 246, 'step_size': 11, 'gamma': 0.9887110775058746}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:11:26,422][0m Trial 22 finished with value: 0.08408721563836333 and parameters: {'observation_period_num': 30, 'train_rates': 0.8794405581373611, 'learning_rate': 0.0004707614569451068, 'batch_size': 197, 'step_size': 9, 'gamma': 0.9437995648147051}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:11:55,972][0m Trial 23 finished with value: 0.04713820293545723 and parameters: {'observation_period_num': 7, 'train_rates': 0.9401864056695632, 'learning_rate': 0.00016228418929478054, 'batch_size': 228, 'step_size': 12, 'gamma': 0.9675842801031139}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:12:33,175][0m Trial 24 finished with value: 0.12191482557809613 and parameters: {'observation_period_num': 74, 'train_rates': 0.8256064395604532, 'learning_rate': 0.0006479193951249806, 'batch_size': 152, 'step_size': 10, 'gamma': 0.9086975111860858}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:12:59,293][0m Trial 25 finished with value: 0.09264975979476046 and parameters: {'observation_period_num': 27, 'train_rates': 0.9094312735296136, 'learning_rate': 6.847634191071266e-05, 'batch_size': 251, 'step_size': 14, 'gamma': 0.9865630085521957}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:13:29,378][0m Trial 26 finished with value: 0.055345406300491756 and parameters: {'observation_period_num': 45, 'train_rates': 0.8612028790516032, 'learning_rate': 0.00020487597829884296, 'batch_size': 196, 'step_size': 7, 'gamma': 0.9260224354203763}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:13:58,348][0m Trial 27 finished with value: 0.08251769095659256 and parameters: {'observation_period_num': 19, 'train_rates': 0.9588398706996315, 'learning_rate': 0.0004517493670814086, 'batch_size': 232, 'step_size': 12, 'gamma': 0.9676310567312655}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:15:22,456][0m Trial 28 finished with value: 0.2255528777399484 and parameters: {'observation_period_num': 97, 'train_rates': 0.7624334053959533, 'learning_rate': 2.667263981174401e-05, 'batch_size': 60, 'step_size': 10, 'gamma': 0.9431756371559057}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:15:56,333][0m Trial 29 finished with value: 0.08562183367236675 and parameters: {'observation_period_num': 62, 'train_rates': 0.900346410286534, 'learning_rate': 0.0009353292257326941, 'batch_size': 176, 'step_size': 5, 'gamma': 0.9209411798645156}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:16:32,884][0m Trial 30 finished with value: 0.10512054440976337 and parameters: {'observation_period_num': 41, 'train_rates': 0.8222107523937514, 'learning_rate': 0.00035021948172174, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9410913566612519}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:16:58,221][0m Trial 31 finished with value: 0.038722609423778274 and parameters: {'observation_period_num': 9, 'train_rates': 0.8731796702828422, 'learning_rate': 0.0009664291444841021, 'batch_size': 255, 'step_size': 11, 'gamma': 0.9888571688536398}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:17:25,604][0m Trial 32 finished with value: 0.11523424088954926 and parameters: {'observation_period_num': 17, 'train_rates': 0.9295627190012214, 'learning_rate': 0.0006470921170098566, 'batch_size': 234, 'step_size': 9, 'gamma': 0.9804564564773995}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:17:53,432][0m Trial 33 finished with value: 0.051676682773090545 and parameters: {'observation_period_num': 17, 'train_rates': 0.8785572408031836, 'learning_rate': 0.0006433908840627598, 'batch_size': 221, 'step_size': 12, 'gamma': 0.9599798929043353}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:18:17,131][0m Trial 34 finished with value: 0.05860064155930515 and parameters: {'observation_period_num': 37, 'train_rates': 0.8431107867212634, 'learning_rate': 0.00017668141881868192, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9749459762860577}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:18:44,347][0m Trial 35 finished with value: 0.03999364748597145 and parameters: {'observation_period_num': 6, 'train_rates': 0.9552379532208055, 'learning_rate': 0.00038385315017642666, 'batch_size': 241, 'step_size': 13, 'gamma': 0.891004968842578}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:19:17,695][0m Trial 36 finished with value: 0.07764619588851929 and parameters: {'observation_period_num': 56, 'train_rates': 0.9610182558615378, 'learning_rate': 0.0003633897519168678, 'batch_size': 197, 'step_size': 13, 'gamma': 0.8883511160644696}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:19:44,775][0m Trial 37 finished with value: 0.06767842918634415 and parameters: {'observation_period_num': 23, 'train_rates': 0.9443397036839644, 'learning_rate': 7.589282547851086e-05, 'batch_size': 236, 'step_size': 14, 'gamma': 0.8566401518769594}. Best is trial 21 with value: 0.03507029089023885.[0m
Early stopping at epoch 63
[32m[I 2025-02-06 18:20:05,238][0m Trial 38 finished with value: 0.5057839833002175 and parameters: {'observation_period_num': 211, 'train_rates': 0.9179687991872347, 'learning_rate': 0.0001549564641290578, 'batch_size': 187, 'step_size': 1, 'gamma': 0.8181437019800466}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:20:32,154][0m Trial 39 finished with value: 0.8256367444992065 and parameters: {'observation_period_num': 103, 'train_rates': 0.9669627529086771, 'learning_rate': 3.0185422689136364e-06, 'batch_size': 243, 'step_size': 15, 'gamma': 0.908553440040414}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:22:28,618][0m Trial 40 finished with value: 0.0960551358901771 and parameters: {'observation_period_num': 69, 'train_rates': 0.8914766087933655, 'learning_rate': 0.0002790207944954001, 'batch_size': 48, 'step_size': 8, 'gamma': 0.8968742376851664}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:22:52,929][0m Trial 41 finished with value: 0.039847405176413685 and parameters: {'observation_period_num': 5, 'train_rates': 0.806926015715221, 'learning_rate': 0.0006847302852099774, 'batch_size': 246, 'step_size': 13, 'gamma': 0.9132840315502931}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:23:20,203][0m Trial 42 finished with value: 0.16624833497497224 and parameters: {'observation_period_num': 5, 'train_rates': 0.7701193656951195, 'learning_rate': 0.0004868173365943294, 'batch_size': 225, 'step_size': 13, 'gamma': 0.8629204187858348}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:23:48,055][0m Trial 43 finished with value: 0.04210393691138059 and parameters: {'observation_period_num': 19, 'train_rates': 0.7976756756336337, 'learning_rate': 0.000696887913826335, 'batch_size': 207, 'step_size': 12, 'gamma': 0.911867870608392}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:24:10,410][0m Trial 44 finished with value: 0.1639850682698155 and parameters: {'observation_period_num': 36, 'train_rates': 0.7111704127666201, 'learning_rate': 0.00072624480192665, 'batch_size': 242, 'step_size': 14, 'gamma': 0.8881795525964495}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:24:34,524][0m Trial 45 finished with value: 0.03875184857968202 and parameters: {'observation_period_num': 5, 'train_rates': 0.8028658408495637, 'learning_rate': 0.00039207236125343286, 'batch_size': 246, 'step_size': 14, 'gamma': 0.8437247671567323}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:24:58,382][0m Trial 46 finished with value: 0.05631162427567147 and parameters: {'observation_period_num': 17, 'train_rates': 0.798744740191685, 'learning_rate': 0.00023765508311507645, 'batch_size': 256, 'step_size': 15, 'gamma': 0.8357191225821726}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:25:40,451][0m Trial 47 finished with value: 0.3461349638597817 and parameters: {'observation_period_num': 247, 'train_rates': 0.7686378045882417, 'learning_rate': 0.0008246221154885936, 'batch_size': 117, 'step_size': 14, 'gamma': 0.8006703372688283}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:26:03,321][0m Trial 48 finished with value: 0.21397009827597951 and parameters: {'observation_period_num': 137, 'train_rates': 0.6381593026867078, 'learning_rate': 0.000502392450026973, 'batch_size': 210, 'step_size': 11, 'gamma': 0.848457153755704}. Best is trial 21 with value: 0.03507029089023885.[0m
[32m[I 2025-02-06 18:26:37,790][0m Trial 49 finished with value: 0.0642857278500675 and parameters: {'observation_period_num': 46, 'train_rates': 0.8130507680912313, 'learning_rate': 0.00030541813470609644, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8680873152485833}. Best is trial 21 with value: 0.03507029089023885.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-06 18:26:37,800][0m A new study created in memory with name: no-name-907b3ee0-61ac-42a4-a9ea-47480bc9f7a7[0m
[32m[I 2025-02-06 18:27:46,850][0m Trial 0 finished with value: 0.331201632785949 and parameters: {'observation_period_num': 143, 'train_rates': 0.8884092758850659, 'learning_rate': 8.675988900558338e-06, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7898316821942666}. Best is trial 0 with value: 0.331201632785949.[0m
[32m[I 2025-02-06 18:32:54,862][0m Trial 1 finished with value: 0.07242808520165553 and parameters: {'observation_period_num': 16, 'train_rates': 0.830620546548389, 'learning_rate': 8.567790822195475e-06, 'batch_size': 17, 'step_size': 11, 'gamma': 0.7594561754751702}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:33:42,330][0m Trial 2 finished with value: 0.40811852518424785 and parameters: {'observation_period_num': 218, 'train_rates': 0.6011350434476005, 'learning_rate': 0.0003535146346852599, 'batch_size': 95, 'step_size': 5, 'gamma': 0.9839507318213705}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:34:40,533][0m Trial 3 finished with value: 0.39697245946367277 and parameters: {'observation_period_num': 53, 'train_rates': 0.8197004396879063, 'learning_rate': 1.5595130259032323e-06, 'batch_size': 94, 'step_size': 3, 'gamma': 0.9851521541333812}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:36:09,057][0m Trial 4 finished with value: 0.10472223162651062 and parameters: {'observation_period_num': 24, 'train_rates': 0.9556731654547844, 'learning_rate': 9.169799692974793e-06, 'batch_size': 68, 'step_size': 11, 'gamma': 0.9327653261406028}. Best is trial 1 with value: 0.07242808520165553.[0m
Early stopping at epoch 35
[32m[I 2025-02-06 18:36:27,786][0m Trial 5 finished with value: 1.4029477834701538 and parameters: {'observation_period_num': 226, 'train_rates': 0.9845674836484484, 'learning_rate': 1.1709811333363572e-06, 'batch_size': 122, 'step_size': 1, 'gamma': 0.7592040941722724}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:37:07,559][0m Trial 6 finished with value: 0.3064975460821932 and parameters: {'observation_period_num': 18, 'train_rates': 0.6855035132648506, 'learning_rate': 4.3280583669246365e-06, 'batch_size': 126, 'step_size': 12, 'gamma': 0.912372887477244}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:37:43,932][0m Trial 7 finished with value: 0.2824208774691388 and parameters: {'observation_period_num': 212, 'train_rates': 0.6935573348663506, 'learning_rate': 0.00025589822216960005, 'batch_size': 132, 'step_size': 13, 'gamma': 0.9291916162773399}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:38:34,648][0m Trial 8 finished with value: 0.08304874085638511 and parameters: {'observation_period_num': 127, 'train_rates': 0.8525514223595881, 'learning_rate': 0.0005047916952652642, 'batch_size': 108, 'step_size': 6, 'gamma': 0.8207409956640079}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:39:20,185][0m Trial 9 finished with value: 0.7462148569417342 and parameters: {'observation_period_num': 184, 'train_rates': 0.9246127955581473, 'learning_rate': 5.722695702418479e-06, 'batch_size': 125, 'step_size': 1, 'gamma': 0.9549231621304081}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:44:04,624][0m Trial 10 finished with value: 0.21597374389262627 and parameters: {'observation_period_num': 82, 'train_rates': 0.7631946943063063, 'learning_rate': 5.238115081067611e-05, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8580329042717606}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:44:32,668][0m Trial 11 finished with value: 0.11396185159032522 and parameters: {'observation_period_num': 104, 'train_rates': 0.8397600999762682, 'learning_rate': 0.0009321770625297332, 'batch_size': 206, 'step_size': 7, 'gamma': 0.8128288341224588}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:44:57,143][0m Trial 12 finished with value: 0.31623553954364647 and parameters: {'observation_period_num': 140, 'train_rates': 0.7618664401053004, 'learning_rate': 5.188332780962411e-05, 'batch_size': 218, 'step_size': 8, 'gamma': 0.8409565206978937}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:48:52,130][0m Trial 13 finished with value: 0.09756760715146051 and parameters: {'observation_period_num': 69, 'train_rates': 0.8724563110908949, 'learning_rate': 1.917603552043765e-05, 'batch_size': 23, 'step_size': 6, 'gamma': 0.750644599370525}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:49:24,643][0m Trial 14 finished with value: 0.14673320595300038 and parameters: {'observation_period_num': 173, 'train_rates': 0.7858578144315199, 'learning_rate': 0.00011301025415780573, 'batch_size': 161, 'step_size': 10, 'gamma': 0.8031950250771396}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:49:58,544][0m Trial 15 finished with value: 0.3746162505710826 and parameters: {'observation_period_num': 117, 'train_rates': 0.8803519706906923, 'learning_rate': 2.2554977033094364e-05, 'batch_size': 174, 'step_size': 4, 'gamma': 0.886178512677664}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:50:18,296][0m Trial 16 finished with value: 0.34509498529321714 and parameters: {'observation_period_num': 249, 'train_rates': 0.7182621008694827, 'learning_rate': 0.00015053068197319478, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7783363040058903}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:52:15,449][0m Trial 17 finished with value: 0.10553237346041129 and parameters: {'observation_period_num': 45, 'train_rates': 0.832888571657956, 'learning_rate': 0.0007292036616276425, 'batch_size': 46, 'step_size': 9, 'gamma': 0.8276860059575082}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:54:05,567][0m Trial 18 finished with value: 0.36765339757714954 and parameters: {'observation_period_num': 90, 'train_rates': 0.9170592206273284, 'learning_rate': 2.8742790728966763e-06, 'batch_size': 51, 'step_size': 13, 'gamma': 0.8728800802935118}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:54:40,571][0m Trial 19 finished with value: 0.26976741647178476 and parameters: {'observation_period_num': 181, 'train_rates': 0.8019673476248692, 'learning_rate': 4.681905735009119e-05, 'batch_size': 155, 'step_size': 7, 'gamma': 0.7780280761515898}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 18:55:35,850][0m Trial 20 finished with value: 0.20339797709358337 and parameters: {'observation_period_num': 7, 'train_rates': 0.8515223773912529, 'learning_rate': 1.4327522790628137e-05, 'batch_size': 101, 'step_size': 3, 'gamma': 0.8297704015930821}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 19:00:51,659][0m Trial 21 finished with value: 0.08908695861941478 and parameters: {'observation_period_num': 60, 'train_rates': 0.8736358288251764, 'learning_rate': 1.4168048294005815e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7517057724690077}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 19:02:54,539][0m Trial 22 finished with value: 0.3655145689561793 and parameters: {'observation_period_num': 39, 'train_rates': 0.9100369125383374, 'learning_rate': 2.8012389084399346e-06, 'batch_size': 46, 'step_size': 6, 'gamma': 0.7632493881774844}. Best is trial 1 with value: 0.07242808520165553.[0m
[32m[I 2025-02-06 19:05:21,487][0m Trial 23 finished with value: 0.0687772393298092 and parameters: {'observation_period_num': 67, 'train_rates': 0.8558043476547239, 'learning_rate': 3.3858576760163514e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.804465037296409}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:06:44,528][0m Trial 24 finished with value: 0.18854933927769918 and parameters: {'observation_period_num': 31, 'train_rates': 0.7834362167039319, 'learning_rate': 9.610736674724926e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.803890262146591}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:08:51,395][0m Trial 25 finished with value: 0.14397970488829206 and parameters: {'observation_period_num': 122, 'train_rates': 0.8182534692239868, 'learning_rate': 0.0005413233003680886, 'batch_size': 40, 'step_size': 8, 'gamma': 0.8501195784639305}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:11:19,063][0m Trial 26 finished with value: 0.21141524469938533 and parameters: {'observation_period_num': 80, 'train_rates': 0.7472307075951995, 'learning_rate': 3.282423256942726e-05, 'batch_size': 33, 'step_size': 11, 'gamma': 0.7859464306772056}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:12:40,626][0m Trial 27 finished with value: 0.12089386139941045 and parameters: {'observation_period_num': 160, 'train_rates': 0.8494837644249573, 'learning_rate': 0.00024380430077262138, 'batch_size': 65, 'step_size': 13, 'gamma': 0.8196691989737132}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:13:39,335][0m Trial 28 finished with value: 0.19586052026513012 and parameters: {'observation_period_num': 5, 'train_rates': 0.9420142740478535, 'learning_rate': 7.339765760646226e-06, 'batch_size': 109, 'step_size': 9, 'gamma': 0.7923952727631967}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:14:48,064][0m Trial 29 finished with value: 0.08553963889841174 and parameters: {'observation_period_num': 103, 'train_rates': 0.8931292546151683, 'learning_rate': 8.54167931957762e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.7681126487990895}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:15:54,125][0m Trial 30 finished with value: 0.6798863922458148 and parameters: {'observation_period_num': 143, 'train_rates': 0.8021393430172017, 'learning_rate': 2.9383996199922136e-06, 'batch_size': 79, 'step_size': 8, 'gamma': 0.7915122987685363}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:17:03,404][0m Trial 31 finished with value: 0.08318734778599306 and parameters: {'observation_period_num': 100, 'train_rates': 0.8962843881863186, 'learning_rate': 8.045896794055134e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.7743321214239487}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:18:07,079][0m Trial 32 finished with value: 0.1082356482744217 and parameters: {'observation_period_num': 106, 'train_rates': 0.8564534671948668, 'learning_rate': 0.0003989294781571586, 'batch_size': 86, 'step_size': 14, 'gamma': 0.8038832746034226}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:19:22,998][0m Trial 33 finished with value: 0.27331479551237925 and parameters: {'observation_period_num': 68, 'train_rates': 0.6053348917563541, 'learning_rate': 3.248877651652914e-05, 'batch_size': 58, 'step_size': 5, 'gamma': 0.7705335553811302}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:20:18,060][0m Trial 34 finished with value: 0.09627998645700285 and parameters: {'observation_period_num': 133, 'train_rates': 0.8965363551549391, 'learning_rate': 0.00015922654776082395, 'batch_size': 105, 'step_size': 11, 'gamma': 0.795132918230514}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:22:44,321][0m Trial 35 finished with value: 0.11227476357163249 and parameters: {'observation_period_num': 156, 'train_rates': 0.9491565506454962, 'learning_rate': 7.115404306915965e-05, 'batch_size': 38, 'step_size': 7, 'gamma': 0.81670951099234}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:24:00,611][0m Trial 36 finished with value: 0.10073158478644824 and parameters: {'observation_period_num': 50, 'train_rates': 0.8220575531257535, 'learning_rate': 1.4649221612817373e-05, 'batch_size': 72, 'step_size': 12, 'gamma': 0.8350843929264842}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:27:35,679][0m Trial 37 finished with value: 0.08933709779890572 and parameters: {'observation_period_num': 95, 'train_rates': 0.9859449196260034, 'learning_rate': 0.0002669119992227053, 'batch_size': 27, 'step_size': 5, 'gamma': 0.78023690642627}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:28:15,796][0m Trial 38 finished with value: 0.33430611714720726 and parameters: {'observation_period_num': 113, 'train_rates': 0.8649620061917653, 'learning_rate': 1.0162566785445517e-05, 'batch_size': 144, 'step_size': 14, 'gamma': 0.7751466644249781}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:29:09,370][0m Trial 39 finished with value: 0.2575474168573107 and parameters: {'observation_period_num': 23, 'train_rates': 0.9308845388040043, 'learning_rate': 4.931858322382796e-06, 'batch_size': 116, 'step_size': 10, 'gamma': 0.8880725388793371}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:30:54,364][0m Trial 40 finished with value: 0.20947542213476622 and parameters: {'observation_period_num': 63, 'train_rates': 0.9056495286676746, 'learning_rate': 2.5701587271218817e-05, 'batch_size': 54, 'step_size': 3, 'gamma': 0.7586231438138441}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:31:58,200][0m Trial 41 finished with value: 0.07808038959028579 and parameters: {'observation_period_num': 95, 'train_rates': 0.8886430363879801, 'learning_rate': 6.707331631336011e-05, 'batch_size': 90, 'step_size': 15, 'gamma': 0.7677269011491188}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:33:04,718][0m Trial 42 finished with value: 0.08051666603680768 and parameters: {'observation_period_num': 79, 'train_rates': 0.8869755123451527, 'learning_rate': 4.080991879131691e-05, 'batch_size': 84, 'step_size': 15, 'gamma': 0.7633846724705471}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:34:03,217][0m Trial 43 finished with value: 0.10563102830971076 and parameters: {'observation_period_num': 80, 'train_rates': 0.8317446688380808, 'learning_rate': 3.983376611388921e-05, 'batch_size': 95, 'step_size': 14, 'gamma': 0.7589392481879706}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:35:05,919][0m Trial 44 finished with value: 0.14837771118844598 and parameters: {'observation_period_num': 128, 'train_rates': 0.9635863412582029, 'learning_rate': 6.17292784906909e-05, 'batch_size': 94, 'step_size': 12, 'gamma': 0.9741944224844219}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:35:50,193][0m Trial 45 finished with value: 0.35618450114629135 and parameters: {'observation_period_num': 86, 'train_rates': 0.8813303325001591, 'learning_rate': 9.251288199192103e-06, 'batch_size': 133, 'step_size': 14, 'gamma': 0.750143817615705}. Best is trial 23 with value: 0.0687772393298092.[0m
[32m[I 2025-02-06 19:36:39,019][0m Trial 46 finished with value: 0.04580894349650903 and parameters: {'observation_period_num': 34, 'train_rates': 0.8607462222580097, 'learning_rate': 0.00015207790051888234, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8115737157525621}. Best is trial 46 with value: 0.04580894349650903.[0m
[32m[I 2025-02-06 19:37:32,982][0m Trial 47 finished with value: 0.063681960105896 and parameters: {'observation_period_num': 31, 'train_rates': 0.9676345999924357, 'learning_rate': 0.00013602348741597413, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8076130819164894}. Best is trial 46 with value: 0.04580894349650903.[0m
[32m[I 2025-02-06 19:38:15,327][0m Trial 48 finished with value: 0.05361720586864693 and parameters: {'observation_period_num': 37, 'train_rates': 0.9294384397226949, 'learning_rate': 0.00017334992526781432, 'batch_size': 144, 'step_size': 12, 'gamma': 0.8479980519581991}. Best is trial 46 with value: 0.04580894349650903.[0m
[32m[I 2025-02-06 19:38:51,139][0m Trial 49 finished with value: 0.0713605284690857 and parameters: {'observation_period_num': 34, 'train_rates': 0.9633434174865867, 'learning_rate': 0.00015860846652254254, 'batch_size': 184, 'step_size': 12, 'gamma': 0.8449186890207386}. Best is trial 46 with value: 0.04580894349650903.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-06 19:38:51,150][0m A new study created in memory with name: no-name-2d5d1fd6-67f2-401b-9ea1-23d713be9b13[0m
[32m[I 2025-02-06 19:39:19,393][0m Trial 0 finished with value: 1.0015538635624095 and parameters: {'observation_period_num': 141, 'train_rates': 0.8079967653452865, 'learning_rate': 2.306687379529997e-06, 'batch_size': 197, 'step_size': 10, 'gamma': 0.8262182087404917}. Best is trial 0 with value: 1.0015538635624095.[0m
[32m[I 2025-02-06 19:39:46,154][0m Trial 1 finished with value: 0.1242339552496188 and parameters: {'observation_period_num': 49, 'train_rates': 0.8924220090415331, 'learning_rate': 0.00040916836792592655, 'batch_size': 246, 'step_size': 10, 'gamma': 0.977109349356363}. Best is trial 1 with value: 0.1242339552496188.[0m
[32m[I 2025-02-06 19:40:13,909][0m Trial 2 finished with value: 0.6364093483565096 and parameters: {'observation_period_num': 193, 'train_rates': 0.6376039782992424, 'learning_rate': 4.805705185816594e-06, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9405878316120213}. Best is trial 1 with value: 0.1242339552496188.[0m
[32m[I 2025-02-06 19:41:07,496][0m Trial 3 finished with value: 0.33872865097752464 and parameters: {'observation_period_num': 172, 'train_rates': 0.7241922829057823, 'learning_rate': 1.6566467558728815e-05, 'batch_size': 91, 'step_size': 6, 'gamma': 0.9442353877168127}. Best is trial 1 with value: 0.1242339552496188.[0m
[32m[I 2025-02-06 19:42:13,216][0m Trial 4 finished with value: 0.08072637788274072 and parameters: {'observation_period_num': 80, 'train_rates': 0.8089753294713593, 'learning_rate': 3.649994969949086e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8824325718372366}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:43:54,583][0m Trial 5 finished with value: 0.3098767805370021 and parameters: {'observation_period_num': 219, 'train_rates': 0.7505638548058619, 'learning_rate': 0.0006007210183933933, 'batch_size': 46, 'step_size': 6, 'gamma': 0.8765091275101424}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:44:17,836][0m Trial 6 finished with value: 0.1423382361616795 and parameters: {'observation_period_num': 185, 'train_rates': 0.841705455277347, 'learning_rate': 0.0001135192750872978, 'batch_size': 242, 'step_size': 10, 'gamma': 0.8540224823299286}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:44:44,007][0m Trial 7 finished with value: 0.3032010062461823 and parameters: {'observation_period_num': 33, 'train_rates': 0.6133932052772428, 'learning_rate': 2.1396202163275083e-05, 'batch_size': 199, 'step_size': 5, 'gamma': 0.7849019576124711}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:45:11,813][0m Trial 8 finished with value: 0.21618191751540194 and parameters: {'observation_period_num': 26, 'train_rates': 0.7917299698460538, 'learning_rate': 0.00020218741822504332, 'batch_size': 204, 'step_size': 14, 'gamma': 0.9475208007319936}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:46:55,694][0m Trial 9 finished with value: 0.5626885532372651 and parameters: {'observation_period_num': 216, 'train_rates': 0.7055495646889279, 'learning_rate': 1.8208430469658262e-06, 'batch_size': 43, 'step_size': 12, 'gamma': 0.9609847423718892}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:47:49,967][0m Trial 10 finished with value: 0.308465838432312 and parameters: {'observation_period_num': 85, 'train_rates': 0.9758739943337864, 'learning_rate': 6.973178582930069e-05, 'batch_size': 119, 'step_size': 1, 'gamma': 0.8948320340481807}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:48:56,094][0m Trial 11 finished with value: 0.12666587649489477 and parameters: {'observation_period_num': 83, 'train_rates': 0.9034880950551857, 'learning_rate': 0.0007529649622612994, 'batch_size': 89, 'step_size': 10, 'gamma': 0.9883703600853536}. Best is trial 4 with value: 0.08072637788274072.[0m
[32m[I 2025-02-06 19:49:36,467][0m Trial 12 finished with value: 0.07424670745219503 and parameters: {'observation_period_num': 83, 'train_rates': 0.8904992034100593, 'learning_rate': 0.00025234460358350907, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7655871821507084}. Best is trial 12 with value: 0.07424670745219503.[0m
[32m[I 2025-02-06 19:50:17,715][0m Trial 13 finished with value: 0.10002045747306612 and parameters: {'observation_period_num': 99, 'train_rates': 0.8960837197652043, 'learning_rate': 5.5651809538739015e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7613217159120288}. Best is trial 12 with value: 0.07424670745219503.[0m
[32m[I 2025-02-06 19:51:32,727][0m Trial 14 finished with value: 0.4973217844963074 and parameters: {'observation_period_num': 126, 'train_rates': 0.9757538756418521, 'learning_rate': 1.2676168578372546e-05, 'batch_size': 81, 'step_size': 3, 'gamma': 0.8179495295888322}. Best is trial 12 with value: 0.07424670745219503.[0m
[32m[I 2025-02-06 19:52:15,513][0m Trial 15 finished with value: 0.04982539563687569 and parameters: {'observation_period_num': 52, 'train_rates': 0.8501740928393515, 'learning_rate': 0.00022311502590296195, 'batch_size': 134, 'step_size': 13, 'gamma': 0.9017243623970164}. Best is trial 15 with value: 0.04982539563687569.[0m
[32m[I 2025-02-06 19:52:56,773][0m Trial 16 finished with value: 0.038792791210261045 and parameters: {'observation_period_num': 12, 'train_rates': 0.9309211514840551, 'learning_rate': 0.00023704434949700998, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9105316569981063}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:53:36,203][0m Trial 17 finished with value: 0.04202026189179034 and parameters: {'observation_period_num': 14, 'train_rates': 0.9374143105889707, 'learning_rate': 0.00012620826533307945, 'batch_size': 161, 'step_size': 13, 'gamma': 0.9114205623944133}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:54:13,282][0m Trial 18 finished with value: 0.0550974476235834 and parameters: {'observation_period_num': 12, 'train_rates': 0.9402915176510043, 'learning_rate': 0.00011085407922678344, 'batch_size': 167, 'step_size': 12, 'gamma': 0.9199983203839396}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:55:03,304][0m Trial 19 finished with value: 0.4953470239202485 and parameters: {'observation_period_num': 251, 'train_rates': 0.9518030113729944, 'learning_rate': 7.619306487230388e-06, 'batch_size': 114, 'step_size': 12, 'gamma': 0.854818018196327}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:55:42,419][0m Trial 20 finished with value: 0.04115816578269005 and parameters: {'observation_period_num': 8, 'train_rates': 0.9293811707201787, 'learning_rate': 0.00011902608534315454, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9169446923134176}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:56:18,312][0m Trial 21 finished with value: 0.04591917893802277 and parameters: {'observation_period_num': 11, 'train_rates': 0.9329365488884022, 'learning_rate': 0.00010748567516147856, 'batch_size': 173, 'step_size': 13, 'gamma': 0.9176655835962129}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:56:57,620][0m Trial 22 finished with value: 0.05507812276482582 and parameters: {'observation_period_num': 44, 'train_rates': 0.9873837069739771, 'learning_rate': 0.0003637342369001459, 'batch_size': 160, 'step_size': 14, 'gamma': 0.90969839639715}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:57:29,846][0m Trial 23 finished with value: 0.07347840596757942 and parameters: {'observation_period_num': 58, 'train_rates': 0.8628139083113099, 'learning_rate': 0.00015375878409822354, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9278086892037688}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:57:59,426][0m Trial 24 finished with value: 0.08483745902776718 and parameters: {'observation_period_num': 15, 'train_rates': 0.9245501837366258, 'learning_rate': 5.3518096607925623e-05, 'batch_size': 224, 'step_size': 8, 'gamma': 0.8559623988529682}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:58:28,578][0m Trial 25 finished with value: 0.0790327861905098 and parameters: {'observation_period_num': 6, 'train_rates': 0.9597184305866737, 'learning_rate': 3.170988378218518e-05, 'batch_size': 222, 'step_size': 14, 'gamma': 0.8908205761499967}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:59:07,251][0m Trial 26 finished with value: 0.05352576362966287 and parameters: {'observation_period_num': 35, 'train_rates': 0.8644586432850703, 'learning_rate': 0.0004209816842254038, 'batch_size': 152, 'step_size': 13, 'gamma': 0.9664767211572226}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 19:59:58,398][0m Trial 27 finished with value: 0.07911841216541472 and parameters: {'observation_period_num': 62, 'train_rates': 0.9204925543771533, 'learning_rate': 7.893421281424812e-05, 'batch_size': 121, 'step_size': 11, 'gamma': 0.9317106341366889}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:00:30,340][0m Trial 28 finished with value: 0.07953549656322446 and parameters: {'observation_period_num': 112, 'train_rates': 0.8758677008537077, 'learning_rate': 0.0001645916084758449, 'batch_size': 184, 'step_size': 15, 'gamma': 0.865298066291214}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:01:22,366][0m Trial 29 finished with value: 0.15548142861527733 and parameters: {'observation_period_num': 153, 'train_rates': 0.8311383373422391, 'learning_rate': 0.0009344188544820657, 'batch_size': 103, 'step_size': 11, 'gamma': 0.8388048863105236}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:06:03,183][0m Trial 30 finished with value: 0.09383613809319066 and parameters: {'observation_period_num': 66, 'train_rates': 0.8018379020411295, 'learning_rate': 0.0002806211569647859, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9065060578807972}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:06:38,509][0m Trial 31 finished with value: 0.05254688150272137 and parameters: {'observation_period_num': 23, 'train_rates': 0.930419764069832, 'learning_rate': 0.00011017818338107683, 'batch_size': 175, 'step_size': 13, 'gamma': 0.9125529956359771}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:07:20,025][0m Trial 32 finished with value: 0.0543423555791378 and parameters: {'observation_period_num': 7, 'train_rates': 0.9581422348684197, 'learning_rate': 4.6254044502831814e-05, 'batch_size': 153, 'step_size': 13, 'gamma': 0.9249512498516149}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:07:49,368][0m Trial 33 finished with value: 0.08668125938132124 and parameters: {'observation_period_num': 39, 'train_rates': 0.9092174019150223, 'learning_rate': 9.562818456030612e-05, 'batch_size': 214, 'step_size': 12, 'gamma': 0.9553663090138015}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:08:22,773][0m Trial 34 finished with value: 1.166812539100647 and parameters: {'observation_period_num': 27, 'train_rates': 0.944628549131411, 'learning_rate': 1.0560878557779337e-06, 'batch_size': 188, 'step_size': 14, 'gamma': 0.8872553323262573}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:08:58,947][0m Trial 35 finished with value: 0.049645961123809174 and parameters: {'observation_period_num': 23, 'train_rates': 0.8832990947344348, 'learning_rate': 0.00014933080795304928, 'batch_size': 171, 'step_size': 11, 'gamma': 0.9388922035370265}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:09:37,202][0m Trial 36 finished with value: 0.16801235287000013 and parameters: {'observation_period_num': 50, 'train_rates': 0.688285762916725, 'learning_rate': 0.0005642433208487799, 'batch_size': 132, 'step_size': 9, 'gamma': 0.9176204699558687}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:10:12,727][0m Trial 37 finished with value: 0.19602690141927592 and parameters: {'observation_period_num': 6, 'train_rates': 0.7786961658399244, 'learning_rate': 2.5024524397779042e-05, 'batch_size': 156, 'step_size': 13, 'gamma': 0.8713769807614784}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:10:45,030][0m Trial 38 finished with value: 0.07496842564996369 and parameters: {'observation_period_num': 72, 'train_rates': 0.8262966169406094, 'learning_rate': 0.0003571164726029078, 'batch_size': 176, 'step_size': 15, 'gamma': 0.9716207351195867}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:11:16,758][0m Trial 39 finished with value: 0.0693643255667253 and parameters: {'observation_period_num': 42, 'train_rates': 0.9208280099063161, 'learning_rate': 7.414877999437457e-05, 'batch_size': 204, 'step_size': 7, 'gamma': 0.9450595640450616}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:11:42,392][0m Trial 40 finished with value: 0.14446322619915009 and parameters: {'observation_period_num': 143, 'train_rates': 0.9713837548831686, 'learning_rate': 0.0005134094453126979, 'batch_size': 254, 'step_size': 14, 'gamma': 0.9017423808003876}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:12:19,547][0m Trial 41 finished with value: 0.04609177204094516 and parameters: {'observation_period_num': 20, 'train_rates': 0.8943669823345407, 'learning_rate': 0.0001453215434209345, 'batch_size': 166, 'step_size': 11, 'gamma': 0.9352258050144351}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:12:56,917][0m Trial 42 finished with value: 0.04673302967355333 and parameters: {'observation_period_num': 15, 'train_rates': 0.940370920212336, 'learning_rate': 0.00015027796752085828, 'batch_size': 163, 'step_size': 12, 'gamma': 0.9370361402786888}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:13:38,378][0m Trial 43 finished with value: 0.03980354852558955 and parameters: {'observation_period_num': 23, 'train_rates': 0.9083109895155718, 'learning_rate': 0.0002953392635423385, 'batch_size': 144, 'step_size': 10, 'gamma': 0.9537262029955365}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:14:22,776][0m Trial 44 finished with value: 0.043094577041326784 and parameters: {'observation_period_num': 30, 'train_rates': 0.9129212761686846, 'learning_rate': 0.00023047537706484105, 'batch_size': 137, 'step_size': 10, 'gamma': 0.8793558865084864}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:15:08,360][0m Trial 45 finished with value: 0.08511786162853241 and parameters: {'observation_period_num': 34, 'train_rates': 0.9895651505048902, 'learning_rate': 0.0002977048569072661, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9842124420668749}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:15:54,662][0m Trial 46 finished with value: 0.04280301647053825 and parameters: {'observation_period_num': 29, 'train_rates': 0.9079242807780068, 'learning_rate': 0.00019091889905867282, 'batch_size': 129, 'step_size': 10, 'gamma': 0.9541591436466349}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:16:43,627][0m Trial 47 finished with value: 0.10302387298817964 and parameters: {'observation_period_num': 48, 'train_rates': 0.8708665332814451, 'learning_rate': 0.0007080127864899606, 'batch_size': 119, 'step_size': 9, 'gamma': 0.9548003385475319}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:17:31,002][0m Trial 48 finished with value: 0.09931540044011376 and parameters: {'observation_period_num': 70, 'train_rates': 0.902595272552297, 'learning_rate': 0.00044409254264078093, 'batch_size': 125, 'step_size': 6, 'gamma': 0.9762487250283773}. Best is trial 16 with value: 0.038792791210261045.[0m
[32m[I 2025-02-06 20:18:24,239][0m Trial 49 finished with value: 0.10061945108806386 and parameters: {'observation_period_num': 103, 'train_rates': 0.8452305397659876, 'learning_rate': 0.00019075537471398293, 'batch_size': 104, 'step_size': 10, 'gamma': 0.9585022468052481}. Best is trial 16 with value: 0.038792791210261045.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-06 20:18:24,250][0m A new study created in memory with name: no-name-22e81a72-abc9-4298-8302-1e693fa1aa30[0m
[32m[I 2025-02-06 20:19:26,064][0m Trial 0 finished with value: 0.2282829848205987 and parameters: {'observation_period_num': 54, 'train_rates': 0.707917552821663, 'learning_rate': 3.3906639909330755e-05, 'batch_size': 82, 'step_size': 6, 'gamma': 0.8313338112013727}. Best is trial 0 with value: 0.2282829848205987.[0m
[32m[I 2025-02-06 20:20:15,601][0m Trial 1 finished with value: 0.1253064893759214 and parameters: {'observation_period_num': 172, 'train_rates': 0.9347940233381905, 'learning_rate': 0.0002892405797271394, 'batch_size': 124, 'step_size': 4, 'gamma': 0.8257967499674925}. Best is trial 1 with value: 0.1253064893759214.[0m
[32m[I 2025-02-06 20:22:05,416][0m Trial 2 finished with value: 0.31499112813196045 and parameters: {'observation_period_num': 79, 'train_rates': 0.7814068776091111, 'learning_rate': 1.196095964940867e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.7801047704403428}. Best is trial 1 with value: 0.1253064893759214.[0m
Early stopping at epoch 98
[32m[I 2025-02-06 20:22:32,430][0m Trial 3 finished with value: 0.346299778742549 and parameters: {'observation_period_num': 167, 'train_rates': 0.815819056908835, 'learning_rate': 0.00011568471774154984, 'batch_size': 214, 'step_size': 2, 'gamma': 0.7742189514731282}. Best is trial 1 with value: 0.1253064893759214.[0m
[32m[I 2025-02-06 20:23:08,899][0m Trial 4 finished with value: 0.08723275936036198 and parameters: {'observation_period_num': 142, 'train_rates': 0.8468354081135854, 'learning_rate': 0.00047961410809365887, 'batch_size': 158, 'step_size': 4, 'gamma': 0.874793912155544}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:24:02,170][0m Trial 5 finished with value: 0.4815874480538898 and parameters: {'observation_period_num': 206, 'train_rates': 0.9412899262619558, 'learning_rate': 6.3597894863169765e-06, 'batch_size': 111, 'step_size': 7, 'gamma': 0.9273190414416955}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:24:23,104][0m Trial 6 finished with value: 0.6650193584197439 and parameters: {'observation_period_num': 181, 'train_rates': 0.6856925235217673, 'learning_rate': 3.192562339719213e-05, 'batch_size': 254, 'step_size': 4, 'gamma': 0.7793303734758587}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:25:03,767][0m Trial 7 finished with value: 0.7467231266582841 and parameters: {'observation_period_num': 96, 'train_rates': 0.6936922217609195, 'learning_rate': 1.9412303084697577e-06, 'batch_size': 120, 'step_size': 3, 'gamma': 0.9690145262892267}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:25:38,345][0m Trial 8 finished with value: 0.2591737373789928 and parameters: {'observation_period_num': 29, 'train_rates': 0.8078572139469963, 'learning_rate': 7.370446863856418e-06, 'batch_size': 159, 'step_size': 2, 'gamma': 0.9829693316250218}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:26:49,298][0m Trial 9 finished with value: 0.3168841251708761 and parameters: {'observation_period_num': 230, 'train_rates': 0.8028034506298058, 'learning_rate': 1.4555081158508718e-05, 'batch_size': 75, 'step_size': 8, 'gamma': 0.8216237315232916}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:27:14,923][0m Trial 10 finished with value: 0.2788278558451487 and parameters: {'observation_period_num': 125, 'train_rates': 0.6096180508057306, 'learning_rate': 0.0007578581610664934, 'batch_size': 186, 'step_size': 14, 'gamma': 0.8950659612344225}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:27:55,482][0m Trial 11 finished with value: 0.14471353619706398 and parameters: {'observation_period_num': 153, 'train_rates': 0.9455376629610708, 'learning_rate': 0.0007598466511329624, 'batch_size': 152, 'step_size': 11, 'gamma': 0.8584478767277058}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:28:27,742][0m Trial 12 finished with value: 0.1116963683302144 and parameters: {'observation_period_num': 126, 'train_rates': 0.8830160659286622, 'learning_rate': 0.00020373501609648902, 'batch_size': 188, 'step_size': 5, 'gamma': 0.886989450879538}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:28:57,353][0m Trial 13 finished with value: 0.0969724217181036 and parameters: {'observation_period_num': 118, 'train_rates': 0.8814080713377868, 'learning_rate': 0.00013219540670151965, 'batch_size': 199, 'step_size': 5, 'gamma': 0.9065292528643656}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:29:22,844][0m Trial 14 finished with value: 0.2982375161785088 and parameters: {'observation_period_num': 100, 'train_rates': 0.8755044357939463, 'learning_rate': 9.380352142058955e-05, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9291637814166478}. Best is trial 4 with value: 0.08723275936036198.[0m
[32m[I 2025-02-06 20:29:54,420][0m Trial 15 finished with value: 0.08577138371765614 and parameters: {'observation_period_num': 138, 'train_rates': 0.8683038378983781, 'learning_rate': 0.0003247944660316764, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9252448534163642}. Best is trial 15 with value: 0.08577138371765614.[0m
[32m[I 2025-02-06 20:30:31,193][0m Trial 16 finished with value: 0.17601363360881805 and parameters: {'observation_period_num': 207, 'train_rates': 0.9882173076965788, 'learning_rate': 0.00042371569594747447, 'batch_size': 168, 'step_size': 11, 'gamma': 0.9415866498039074}. Best is trial 15 with value: 0.08577138371765614.[0m
[32m[I 2025-02-06 20:31:06,607][0m Trial 17 finished with value: 0.2510691729511072 and parameters: {'observation_period_num': 151, 'train_rates': 0.7502653564294596, 'learning_rate': 8.117728289808315e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8611806670647632}. Best is trial 15 with value: 0.08577138371765614.[0m
[32m[I 2025-02-06 20:31:31,922][0m Trial 18 finished with value: 0.039480268778636814 and parameters: {'observation_period_num': 10, 'train_rates': 0.8497550332950049, 'learning_rate': 0.0009147316891732717, 'batch_size': 228, 'step_size': 10, 'gamma': 0.9546876935792827}. Best is trial 18 with value: 0.039480268778636814.[0m
[32m[I 2025-02-06 20:31:55,412][0m Trial 19 finished with value: 0.04138710291151725 and parameters: {'observation_period_num': 21, 'train_rates': 0.8465638658645351, 'learning_rate': 0.0008766399397233265, 'batch_size': 250, 'step_size': 11, 'gamma': 0.9542262406078007}. Best is trial 18 with value: 0.039480268778636814.[0m
[32m[I 2025-02-06 20:32:19,296][0m Trial 20 finished with value: 0.1838436361571125 and parameters: {'observation_period_num': 15, 'train_rates': 0.7601677339273876, 'learning_rate': 5.953716346116184e-05, 'batch_size': 241, 'step_size': 13, 'gamma': 0.9597217788245406}. Best is trial 18 with value: 0.039480268778636814.[0m
[32m[I 2025-02-06 20:32:47,037][0m Trial 21 finished with value: 0.030143275490211965 and parameters: {'observation_period_num': 5, 'train_rates': 0.8470244223360628, 'learning_rate': 0.0009912063727959598, 'batch_size': 219, 'step_size': 10, 'gamma': 0.95052156951359}. Best is trial 21 with value: 0.030143275490211965.[0m
[32m[I 2025-02-06 20:33:13,177][0m Trial 22 finished with value: 0.06630389475330864 and parameters: {'observation_period_num': 43, 'train_rates': 0.8335088297796461, 'learning_rate': 0.000953988790447633, 'batch_size': 226, 'step_size': 12, 'gamma': 0.9873501124289927}. Best is trial 21 with value: 0.030143275490211965.[0m
[32m[I 2025-02-06 20:33:42,328][0m Trial 23 finished with value: 0.02980863285676533 and parameters: {'observation_period_num': 8, 'train_rates': 0.8978184414630223, 'learning_rate': 0.0009796067374673687, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9548434444079609}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:34:11,430][0m Trial 24 finished with value: 0.03966231520978674 and parameters: {'observation_period_num': 7, 'train_rates': 0.9162091359351614, 'learning_rate': 0.00020904006532331378, 'batch_size': 222, 'step_size': 9, 'gamma': 0.9099947937817966}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:34:42,280][0m Trial 25 finished with value: 0.07334122282596445 and parameters: {'observation_period_num': 60, 'train_rates': 0.9129340700866303, 'learning_rate': 0.0005071950815728549, 'batch_size': 202, 'step_size': 8, 'gamma': 0.9501242060435706}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:35:13,141][0m Trial 26 finished with value: 0.09340263903141022 and parameters: {'observation_period_num': 32, 'train_rates': 0.9698213043573936, 'learning_rate': 0.00019639385185517722, 'batch_size': 211, 'step_size': 9, 'gamma': 0.9735320226169909}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:35:40,679][0m Trial 27 finished with value: 0.6680733328685164 and parameters: {'observation_period_num': 63, 'train_rates': 0.9116648297312966, 'learning_rate': 1.1505241621113006e-06, 'batch_size': 236, 'step_size': 7, 'gamma': 0.9399650155782383}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:39:44,851][0m Trial 28 finished with value: 0.16563012735639773 and parameters: {'observation_period_num': 6, 'train_rates': 0.7800758061767378, 'learning_rate': 0.000546651902155339, 'batch_size': 21, 'step_size': 13, 'gamma': 0.911450915655268}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:40:17,155][0m Trial 29 finished with value: 0.05107662498445834 and parameters: {'observation_period_num': 41, 'train_rates': 0.8381427694359439, 'learning_rate': 0.000994669964667617, 'batch_size': 175, 'step_size': 10, 'gamma': 0.9626215850263904}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:41:10,319][0m Trial 30 finished with value: 0.21343161990272363 and parameters: {'observation_period_num': 73, 'train_rates': 0.7467614988296075, 'learning_rate': 5.502506040359937e-05, 'batch_size': 95, 'step_size': 7, 'gamma': 0.841354067112468}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:41:39,028][0m Trial 31 finished with value: 0.03739066252210101 and parameters: {'observation_period_num': 6, 'train_rates': 0.8966426029556301, 'learning_rate': 0.00022168266282129208, 'batch_size': 217, 'step_size': 9, 'gamma': 0.9169560678954647}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:42:08,077][0m Trial 32 finished with value: 0.09785180273932889 and parameters: {'observation_period_num': 45, 'train_rates': 0.8871143807301821, 'learning_rate': 0.0003435009625829621, 'batch_size': 204, 'step_size': 10, 'gamma': 0.9367309767333949}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:42:34,046][0m Trial 33 finished with value: 0.0890812461332577 and parameters: {'observation_period_num': 24, 'train_rates': 0.9023422411099981, 'learning_rate': 0.0005935932235198324, 'batch_size': 255, 'step_size': 12, 'gamma': 0.9742099095145061}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:43:01,488][0m Trial 34 finished with value: 0.03775827219533862 and parameters: {'observation_period_num': 8, 'train_rates': 0.8599229772588218, 'learning_rate': 0.0002640054179373194, 'batch_size': 215, 'step_size': 8, 'gamma': 0.9155464826112176}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:43:30,963][0m Trial 35 finished with value: 0.0729876309633255 and parameters: {'observation_period_num': 49, 'train_rates': 0.9541290412613697, 'learning_rate': 0.0002496677403173417, 'batch_size': 219, 'step_size': 8, 'gamma': 0.9182452876596902}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:44:01,926][0m Trial 36 finished with value: 0.04451760874655836 and parameters: {'observation_period_num': 27, 'train_rates': 0.8606940768516849, 'learning_rate': 0.00016218805840670825, 'batch_size': 196, 'step_size': 9, 'gamma': 0.8948666329590373}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:44:30,654][0m Trial 37 finished with value: 0.3157368854917511 and parameters: {'observation_period_num': 78, 'train_rates': 0.8274655821711667, 'learning_rate': 2.0249808976283208e-05, 'batch_size': 211, 'step_size': 6, 'gamma': 0.8068559669713226}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:45:06,016][0m Trial 38 finished with value: 0.056663337706223775 and parameters: {'observation_period_num': 34, 'train_rates': 0.9213180641253329, 'learning_rate': 0.00032426600050485264, 'batch_size': 176, 'step_size': 6, 'gamma': 0.7585999269464482}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:45:30,980][0m Trial 39 finished with value: 0.22897225768585516 and parameters: {'observation_period_num': 94, 'train_rates': 0.785148354600715, 'learning_rate': 0.00042580248988563133, 'batch_size': 238, 'step_size': 8, 'gamma': 0.887179673957487}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:45:56,256][0m Trial 40 finished with value: 0.06801608897863873 and parameters: {'observation_period_num': 5, 'train_rates': 0.8933410362317362, 'learning_rate': 4.5591565557554704e-05, 'batch_size': 244, 'step_size': 7, 'gamma': 0.8715635780171579}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:46:23,546][0m Trial 41 finished with value: 0.03311272602237708 and parameters: {'observation_period_num': 17, 'train_rates': 0.8563192652670097, 'learning_rate': 0.0006973683715328383, 'batch_size': 222, 'step_size': 10, 'gamma': 0.9461629283549019}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:46:51,533][0m Trial 42 finished with value: 0.04273591861676644 and parameters: {'observation_period_num': 18, 'train_rates': 0.8148456975332579, 'learning_rate': 0.0006376233130094532, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9455374436746282}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:47:38,016][0m Trial 43 finished with value: 0.12596582648358814 and parameters: {'observation_period_num': 54, 'train_rates': 0.9335931551591177, 'learning_rate': 0.00039710219557729257, 'batch_size': 130, 'step_size': 11, 'gamma': 0.933322379478928}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:48:05,616][0m Trial 44 finished with value: 0.06689058029857438 and parameters: {'observation_period_num': 36, 'train_rates': 0.8663439727309067, 'learning_rate': 0.0006378882923382364, 'batch_size': 225, 'step_size': 9, 'gamma': 0.920126908792684}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:48:31,428][0m Trial 45 finished with value: 0.1779742946940412 and parameters: {'observation_period_num': 18, 'train_rates': 0.649161929164524, 'learning_rate': 0.00013422850520047235, 'batch_size': 192, 'step_size': 12, 'gamma': 0.900035939755507}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:49:04,626][0m Trial 46 finished with value: 0.04219577481551184 and parameters: {'observation_period_num': 21, 'train_rates': 0.8200469464436213, 'learning_rate': 0.00026209578423310055, 'batch_size': 176, 'step_size': 10, 'gamma': 0.967064434727777}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:50:28,191][0m Trial 47 finished with value: 0.19463402287933626 and parameters: {'observation_period_num': 66, 'train_rates': 0.859670834760573, 'learning_rate': 4.115904528315993e-06, 'batch_size': 67, 'step_size': 8, 'gamma': 0.9792801826989893}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:50:58,345][0m Trial 48 finished with value: 0.040140418191590616 and parameters: {'observation_period_num': 32, 'train_rates': 0.8941091655917391, 'learning_rate': 0.0006938047874264406, 'batch_size': 207, 'step_size': 7, 'gamma': 0.8822951838112494}. Best is trial 23 with value: 0.02980863285676533.[0m
[32m[I 2025-02-06 20:51:36,207][0m Trial 49 finished with value: 0.035525595854728594 and parameters: {'observation_period_num': 5, 'train_rates': 0.9263819846408149, 'learning_rate': 0.0004948867627105506, 'batch_size': 163, 'step_size': 5, 'gamma': 0.9279091075723721}. Best is trial 23 with value: 0.02980863285676533.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-06 20:51:36,217][0m A new study created in memory with name: no-name-abc54bb6-ecd4-4d91-970e-3a54403d0a72[0m
[32m[I 2025-02-06 20:52:10,981][0m Trial 0 finished with value: 0.19730592596392005 and parameters: {'observation_period_num': 85, 'train_rates': 0.6712744653425526, 'learning_rate': 0.0004136361404419288, 'batch_size': 140, 'step_size': 10, 'gamma': 0.7561512148199896}. Best is trial 0 with value: 0.19730592596392005.[0m
[32m[I 2025-02-06 20:53:20,570][0m Trial 1 finished with value: 0.05978707486825873 and parameters: {'observation_period_num': 33, 'train_rates': 0.8876644529852006, 'learning_rate': 0.0005836368627580877, 'batch_size': 82, 'step_size': 7, 'gamma': 0.989827925026641}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 20:55:01,835][0m Trial 2 finished with value: 0.18051023175561332 and parameters: {'observation_period_num': 165, 'train_rates': 0.9562404498971049, 'learning_rate': 0.0009100981843714483, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9302428972197515}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 20:57:43,063][0m Trial 3 finished with value: 0.15125339906792956 and parameters: {'observation_period_num': 127, 'train_rates': 0.7981300275269496, 'learning_rate': 4.307789808372458e-05, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9256814712799513}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:00:36,401][0m Trial 4 finished with value: 0.6451143891077775 and parameters: {'observation_period_num': 219, 'train_rates': 0.762590855636776, 'learning_rate': 3.5029995964564936e-06, 'batch_size': 27, 'step_size': 2, 'gamma': 0.8909631930841768}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:01:01,332][0m Trial 5 finished with value: 0.7080204486846924 and parameters: {'observation_period_num': 190, 'train_rates': 0.9412236574070696, 'learning_rate': 2.613488691901551e-06, 'batch_size': 247, 'step_size': 7, 'gamma': 0.8971175170494249}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:01:36,291][0m Trial 6 finished with value: 0.3610001925379038 and parameters: {'observation_period_num': 132, 'train_rates': 0.8301504587731108, 'learning_rate': 9.508907079310807e-06, 'batch_size': 159, 'step_size': 15, 'gamma': 0.7784216008032553}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:05:50,784][0m Trial 7 finished with value: 0.17969809582409185 and parameters: {'observation_period_num': 52, 'train_rates': 0.7450277057779154, 'learning_rate': 9.369043979269253e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8539586229152029}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:07:28,610][0m Trial 8 finished with value: 0.1707996009349428 and parameters: {'observation_period_num': 147, 'train_rates': 0.892671703281004, 'learning_rate': 0.00041988200967655605, 'batch_size': 55, 'step_size': 7, 'gamma': 0.9627720730749306}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:09:40,086][0m Trial 9 finished with value: 0.18198522621607963 and parameters: {'observation_period_num': 31, 'train_rates': 0.7297704004445826, 'learning_rate': 2.216704353996064e-05, 'batch_size': 37, 'step_size': 5, 'gamma': 0.866782778815782}. Best is trial 1 with value: 0.05978707486825873.[0m
[32m[I 2025-02-06 21:10:36,670][0m Trial 10 finished with value: 0.03889167154916288 and parameters: {'observation_period_num': 16, 'train_rates': 0.8651009980100265, 'learning_rate': 0.00011763137378664878, 'batch_size': 100, 'step_size': 11, 'gamma': 0.9884014409189869}. Best is trial 10 with value: 0.03889167154916288.[0m
[32m[I 2025-02-06 21:11:37,234][0m Trial 11 finished with value: 0.04239793567125779 and parameters: {'observation_period_num': 7, 'train_rates': 0.868315739855536, 'learning_rate': 0.000125975931627329, 'batch_size': 97, 'step_size': 11, 'gamma': 0.9848147288946145}. Best is trial 10 with value: 0.03889167154916288.[0m
[32m[I 2025-02-06 21:12:33,435][0m Trial 12 finished with value: 0.03676261960846336 and parameters: {'observation_period_num': 6, 'train_rates': 0.852054368536948, 'learning_rate': 0.00011902237996896928, 'batch_size': 104, 'step_size': 12, 'gamma': 0.9870502118582704}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:13:07,131][0m Trial 13 finished with value: 0.09596335142850876 and parameters: {'observation_period_num': 80, 'train_rates': 0.9853175002333346, 'learning_rate': 0.00014725299414160476, 'batch_size': 191, 'step_size': 14, 'gamma': 0.8199549127820001}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:14:00,641][0m Trial 14 finished with value: 0.0475547164829701 and parameters: {'observation_period_num': 5, 'train_rates': 0.8331281029861629, 'learning_rate': 3.5909248218508096e-05, 'batch_size': 105, 'step_size': 12, 'gamma': 0.9430506942367971}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:14:38,631][0m Trial 15 finished with value: 0.17195745685869446 and parameters: {'observation_period_num': 82, 'train_rates': 0.6040366938103489, 'learning_rate': 0.00020514250596556367, 'batch_size': 123, 'step_size': 13, 'gamma': 0.9594222839454968}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:15:14,097][0m Trial 16 finished with value: 0.2103047728179449 and parameters: {'observation_period_num': 55, 'train_rates': 0.9144957653921231, 'learning_rate': 1.3894092947366718e-05, 'batch_size': 171, 'step_size': 10, 'gamma': 0.9070258868006085}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:15:41,994][0m Trial 17 finished with value: 0.13210124331237996 and parameters: {'observation_period_num': 93, 'train_rates': 0.8422336004039722, 'learning_rate': 5.4025445758519694e-05, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9641960047558087}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:16:44,759][0m Trial 18 finished with value: 0.05103265807585411 and parameters: {'observation_period_num': 43, 'train_rates': 0.796446310674572, 'learning_rate': 0.00025675992018378606, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8331272657240544}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:17:20,774][0m Trial 19 finished with value: 0.8771847751405504 and parameters: {'observation_period_num': 243, 'train_rates': 0.6849041051408743, 'learning_rate': 1.0917077104611286e-06, 'batch_size': 130, 'step_size': 15, 'gamma': 0.9757748504404591}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:18:50,346][0m Trial 20 finished with value: 0.10352417642128801 and parameters: {'observation_period_num': 107, 'train_rates': 0.9250763565569864, 'learning_rate': 8.932113319613214e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9440340954280588}. Best is trial 12 with value: 0.03676261960846336.[0m
[32m[I 2025-02-06 21:19:47,947][0m Trial 21 finished with value: 0.035626586733019584 and parameters: {'observation_period_num': 15, 'train_rates': 0.8773053656662486, 'learning_rate': 9.461870582359808e-05, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9846616865674513}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:20:45,226][0m Trial 22 finished with value: 0.05592592153698206 and parameters: {'observation_period_num': 23, 'train_rates': 0.8652214916436657, 'learning_rate': 6.135659076675145e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.9895796769676982}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:21:35,225][0m Trial 23 finished with value: 0.06594418147813265 and parameters: {'observation_period_num': 61, 'train_rates': 0.8492134760389343, 'learning_rate': 0.00022312875307862104, 'batch_size': 113, 'step_size': 11, 'gamma': 0.949115911390338}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:22:14,476][0m Trial 24 finished with value: 0.07577011829869633 and parameters: {'observation_period_num': 17, 'train_rates': 0.8093221767925614, 'learning_rate': 2.2048162411034318e-05, 'batch_size': 146, 'step_size': 12, 'gamma': 0.9133848383983296}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:23:25,875][0m Trial 25 finished with value: 0.06169104064796485 and parameters: {'observation_period_num': 68, 'train_rates': 0.8940768523196704, 'learning_rate': 7.095378992016438e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.9680119107331309}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:24:10,313][0m Trial 26 finished with value: 0.18134635826360146 and parameters: {'observation_period_num': 39, 'train_rates': 0.7748185444338271, 'learning_rate': 0.0001615581545566059, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9303963568106158}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:25:40,349][0m Trial 27 finished with value: 0.05815323054169615 and parameters: {'observation_period_num': 23, 'train_rates': 0.9673908948395271, 'learning_rate': 0.00027471490902880076, 'batch_size': 67, 'step_size': 11, 'gamma': 0.9760806371997391}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:26:19,913][0m Trial 28 finished with value: 0.05964353170276173 and parameters: {'observation_period_num': 8, 'train_rates': 0.9167145915540549, 'learning_rate': 2.67144404909214e-05, 'batch_size': 157, 'step_size': 13, 'gamma': 0.9487717121539696}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:26:55,838][0m Trial 29 finished with value: 0.1601025729229854 and parameters: {'observation_period_num': 47, 'train_rates': 0.7019199282317301, 'learning_rate': 0.0005054955325891588, 'batch_size': 141, 'step_size': 10, 'gamma': 0.763437885239257}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:27:44,659][0m Trial 30 finished with value: 0.19765483463226363 and parameters: {'observation_period_num': 112, 'train_rates': 0.622328693272978, 'learning_rate': 0.00010870951164351754, 'batch_size': 93, 'step_size': 14, 'gamma': 0.8896054326015165}. Best is trial 21 with value: 0.035626586733019584.[0m
[32m[I 2025-02-06 21:28:39,917][0m Trial 31 finished with value: 0.0349357463703089 and parameters: {'observation_period_num': 7, 'train_rates': 0.8664006071141471, 'learning_rate': 0.00012588250968844096, 'batch_size': 106, 'step_size': 11, 'gamma': 0.9898411961883138}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:29:33,223][0m Trial 32 finished with value: 0.042638864131384784 and parameters: {'observation_period_num': 32, 'train_rates': 0.8602768054028226, 'learning_rate': 0.0002947911867938623, 'batch_size': 114, 'step_size': 11, 'gamma': 0.9866602890542893}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:30:43,479][0m Trial 33 finished with value: 0.03987117452522307 and parameters: {'observation_period_num': 18, 'train_rates': 0.8200122143560467, 'learning_rate': 0.0008445607453544254, 'batch_size': 76, 'step_size': 10, 'gamma': 0.9739806212069062}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:32:49,528][0m Trial 34 finished with value: 0.07367609591966265 and parameters: {'observation_period_num': 67, 'train_rates': 0.8771127480818027, 'learning_rate': 6.530576573706519e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.9549261287691474}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:33:56,475][0m Trial 35 finished with value: 0.04318474542193857 and parameters: {'observation_period_num': 5, 'train_rates': 0.8946246744888874, 'learning_rate': 4.3404719086406116e-05, 'batch_size': 89, 'step_size': 12, 'gamma': 0.931460676910337}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:34:37,909][0m Trial 36 finished with value: 0.1837593259693291 and parameters: {'observation_period_num': 34, 'train_rates': 0.7773689435906772, 'learning_rate': 0.0001329698668654391, 'batch_size': 132, 'step_size': 14, 'gamma': 0.989723750088832}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:35:33,201][0m Trial 37 finished with value: 0.15519958237806955 and parameters: {'observation_period_num': 177, 'train_rates': 0.9525231341161087, 'learning_rate': 0.0006585708837317837, 'batch_size': 110, 'step_size': 8, 'gamma': 0.9764709733115757}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:36:47,323][0m Trial 38 finished with value: 0.03981247607066676 and parameters: {'observation_period_num': 22, 'train_rates': 0.8161673412919724, 'learning_rate': 0.00036877700645698814, 'batch_size': 73, 'step_size': 10, 'gamma': 0.795067658822504}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:37:22,960][0m Trial 39 finished with value: 0.059753163158893584 and parameters: {'observation_period_num': 51, 'train_rates': 0.9296271456168828, 'learning_rate': 0.00017879681794619333, 'batch_size': 177, 'step_size': 5, 'gamma': 0.9190777611102187}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:38:59,135][0m Trial 40 finished with value: 0.15341571600813614 and parameters: {'observation_period_num': 204, 'train_rates': 0.9036326308503851, 'learning_rate': 8.478382801704388e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9612337364957302}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:40:16,534][0m Trial 41 finished with value: 0.036490381012360254 and parameters: {'observation_period_num': 19, 'train_rates': 0.8164472102222583, 'learning_rate': 0.0003513416308788, 'batch_size': 72, 'step_size': 10, 'gamma': 0.7878551688530555}. Best is trial 31 with value: 0.0349357463703089.[0m
[32m[I 2025-02-06 21:41:15,157][0m Trial 42 finished with value: 0.030549519947756117 and parameters: {'observation_period_num': 17, 'train_rates': 0.8543893178343543, 'learning_rate': 0.000322376432011282, 'batch_size': 100, 'step_size': 11, 'gamma': 0.8051384929254203}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:43:08,472][0m Trial 43 finished with value: 0.04182247496817423 and parameters: {'observation_period_num': 32, 'train_rates': 0.8429718358380595, 'learning_rate': 0.0003688763355185885, 'batch_size': 49, 'step_size': 8, 'gamma': 0.7924301217120818}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:44:13,330][0m Trial 44 finished with value: 0.05568972296878963 and parameters: {'observation_period_num': 43, 'train_rates': 0.8855560560614644, 'learning_rate': 0.0006648075043290965, 'batch_size': 90, 'step_size': 10, 'gamma': 0.822894197211702}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:45:05,004][0m Trial 45 finished with value: 0.1617739583163904 and parameters: {'observation_period_num': 14, 'train_rates': 0.7788883834796166, 'learning_rate': 0.0009850152655243246, 'batch_size': 108, 'step_size': 9, 'gamma': 0.8000768353293369}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:45:47,936][0m Trial 46 finished with value: 0.23548015882948783 and parameters: {'observation_period_num': 140, 'train_rates': 0.7475836327786253, 'learning_rate': 0.0005071863409842891, 'batch_size': 120, 'step_size': 12, 'gamma': 0.7510033793303292}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:49:00,032][0m Trial 47 finished with value: 0.041260379379334515 and parameters: {'observation_period_num': 27, 'train_rates': 0.8258498502142074, 'learning_rate': 0.0001918337941572079, 'batch_size': 28, 'step_size': 11, 'gamma': 0.7706739144419741}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:50:20,927][0m Trial 48 finished with value: 0.12350528691099098 and parameters: {'observation_period_num': 6, 'train_rates': 0.8038074195969296, 'learning_rate': 6.382032612528098e-06, 'batch_size': 66, 'step_size': 6, 'gamma': 0.8451490067740541}. Best is trial 42 with value: 0.030549519947756117.[0m
[32m[I 2025-02-06 21:51:27,851][0m Trial 49 finished with value: 0.08167717357476552 and parameters: {'observation_period_num': 73, 'train_rates': 0.8586858141480955, 'learning_rate': 0.0003319126161373776, 'batch_size': 85, 'step_size': 1, 'gamma': 0.879870577695156}. Best is trial 42 with value: 0.030549519947756117.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-06 21:51:27,861][0m A new study created in memory with name: no-name-649896d4-99f2-4d0c-bf36-db47d7c23c97[0m
[32m[I 2025-02-06 21:52:47,807][0m Trial 0 finished with value: 0.34314389238210996 and parameters: {'observation_period_num': 74, 'train_rates': 0.6641580771169022, 'learning_rate': 3.4123462071498894e-05, 'batch_size': 58, 'step_size': 15, 'gamma': 0.8066878491276124}. Best is trial 0 with value: 0.34314389238210996.[0m
[32m[I 2025-02-06 21:53:25,025][0m Trial 1 finished with value: 0.8378304913482505 and parameters: {'observation_period_num': 219, 'train_rates': 0.8272406603862725, 'learning_rate': 1.0050597814722018e-06, 'batch_size': 145, 'step_size': 15, 'gamma': 0.8467285210314597}. Best is trial 0 with value: 0.34314389238210996.[0m
[32m[I 2025-02-06 21:53:50,834][0m Trial 2 finished with value: 0.20723858214914798 and parameters: {'observation_period_num': 113, 'train_rates': 0.8875337519080055, 'learning_rate': 4.700866366447604e-05, 'batch_size': 252, 'step_size': 7, 'gamma': 0.8776326354177908}. Best is trial 2 with value: 0.20723858214914798.[0m
[32m[I 2025-02-06 21:54:29,384][0m Trial 3 finished with value: 0.3092830276681753 and parameters: {'observation_period_num': 123, 'train_rates': 0.6943148693559847, 'learning_rate': 3.6876055696145196e-05, 'batch_size': 124, 'step_size': 4, 'gamma': 0.903086515677562}. Best is trial 2 with value: 0.20723858214914798.[0m
[32m[I 2025-02-06 21:55:34,871][0m Trial 4 finished with value: 0.08914898631848255 and parameters: {'observation_period_num': 113, 'train_rates': 0.8136763746627759, 'learning_rate': 0.0001007832063353928, 'batch_size': 81, 'step_size': 14, 'gamma': 0.8634454077305124}. Best is trial 4 with value: 0.08914898631848255.[0m
[32m[I 2025-02-06 21:56:01,128][0m Trial 5 finished with value: 0.9202635884284973 and parameters: {'observation_period_num': 151, 'train_rates': 0.9240990291364053, 'learning_rate': 4.176013237959104e-06, 'batch_size': 245, 'step_size': 4, 'gamma': 0.9419847747327483}. Best is trial 4 with value: 0.08914898631848255.[0m
[32m[I 2025-02-06 21:56:34,498][0m Trial 6 finished with value: 0.27559855790205406 and parameters: {'observation_period_num': 205, 'train_rates': 0.6904279840416404, 'learning_rate': 0.0004738362223333539, 'batch_size': 148, 'step_size': 12, 'gamma': 0.8209870555474261}. Best is trial 4 with value: 0.08914898631848255.[0m
Early stopping at epoch 66
[32m[I 2025-02-06 21:57:27,606][0m Trial 7 finished with value: 0.7234036928356284 and parameters: {'observation_period_num': 155, 'train_rates': 0.7838744704426027, 'learning_rate': 3.133447476802598e-05, 'batch_size': 63, 'step_size': 1, 'gamma': 0.8169811616491273}. Best is trial 4 with value: 0.08914898631848255.[0m
[32m[I 2025-02-06 21:57:50,496][0m Trial 8 finished with value: 0.5714128439254429 and parameters: {'observation_period_num': 161, 'train_rates': 0.7124566032389454, 'learning_rate': 6.571483960609768e-06, 'batch_size': 235, 'step_size': 10, 'gamma': 0.9641517664027872}. Best is trial 4 with value: 0.08914898631848255.[0m
Early stopping at epoch 75
[32m[I 2025-02-06 21:59:05,005][0m Trial 9 finished with value: 0.6537755750351572 and parameters: {'observation_period_num': 216, 'train_rates': 0.7306066606243451, 'learning_rate': 1.4000106862347108e-05, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8627161515077331}. Best is trial 4 with value: 0.08914898631848255.[0m
[32m[I 2025-02-06 22:00:06,978][0m Trial 10 finished with value: 0.0478370226919651 and parameters: {'observation_period_num': 36, 'train_rates': 0.9875887160718505, 'learning_rate': 0.00036894320116636074, 'batch_size': 100, 'step_size': 10, 'gamma': 0.7633180729349696}. Best is trial 10 with value: 0.0478370226919651.[0m
[32m[I 2025-02-06 22:01:06,517][0m Trial 11 finished with value: 0.04463113844394684 and parameters: {'observation_period_num': 45, 'train_rates': 0.9855608019228951, 'learning_rate': 0.0004325239126556382, 'batch_size': 103, 'step_size': 12, 'gamma': 0.7552017411348821}. Best is trial 11 with value: 0.04463113844394684.[0m
[32m[I 2025-02-06 22:02:04,167][0m Trial 12 finished with value: 0.03324984759092331 and parameters: {'observation_period_num': 5, 'train_rates': 0.9870130887608543, 'learning_rate': 0.0009799523860990146, 'batch_size': 111, 'step_size': 9, 'gamma': 0.7524398492256787}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:08:17,686][0m Trial 13 finished with value: 0.03853186426891221 and parameters: {'observation_period_num': 8, 'train_rates': 0.9880167244892255, 'learning_rate': 0.0009427715649348981, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7563337450836761}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:14:10,149][0m Trial 14 finished with value: 0.03378449474962858 and parameters: {'observation_period_num': 11, 'train_rates': 0.9118467823285128, 'learning_rate': 0.0009577707462765588, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7817210367957836}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:14:43,310][0m Trial 15 finished with value: 0.07031487434647465 and parameters: {'observation_period_num': 20, 'train_rates': 0.9110299202589478, 'learning_rate': 0.00015703867671568303, 'batch_size': 190, 'step_size': 6, 'gamma': 0.7882556224391517}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:17:33,565][0m Trial 16 finished with value: 0.21612940868362784 and parameters: {'observation_period_num': 65, 'train_rates': 0.612869425028369, 'learning_rate': 0.000174328114189791, 'batch_size': 25, 'step_size': 5, 'gamma': 0.7870343768925175}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:18:05,566][0m Trial 17 finished with value: 0.07135813348736633 and parameters: {'observation_period_num': 73, 'train_rates': 0.8608494427365769, 'learning_rate': 0.0009638395887709734, 'batch_size': 185, 'step_size': 10, 'gamma': 0.7833759681713683}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:18:38,502][0m Trial 18 finished with value: 0.04486920312047005 and parameters: {'observation_period_num': 7, 'train_rates': 0.943618321783768, 'learning_rate': 0.00023576438616288345, 'batch_size': 191, 'step_size': 8, 'gamma': 0.8296897814708343}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:20:59,871][0m Trial 19 finished with value: 0.0524853646551093 and parameters: {'observation_period_num': 43, 'train_rates': 0.8748931846092142, 'learning_rate': 8.41688770284501e-05, 'batch_size': 39, 'step_size': 3, 'gamma': 0.9044733081916884}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:22:05,831][0m Trial 20 finished with value: 0.135934992205529 and parameters: {'observation_period_num': 87, 'train_rates': 0.9416981323911793, 'learning_rate': 0.00061442971867169, 'batch_size': 88, 'step_size': 12, 'gamma': 0.7856209758722535}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:27:52,882][0m Trial 21 finished with value: 0.03710044845222281 and parameters: {'observation_period_num': 6, 'train_rates': 0.9650282240621373, 'learning_rate': 0.0008671093338802499, 'batch_size': 17, 'step_size': 8, 'gamma': 0.7544662524410048}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:30:45,900][0m Trial 22 finished with value: 0.05113741719612369 and parameters: {'observation_period_num': 29, 'train_rates': 0.9541665892577307, 'learning_rate': 0.0009515654303487565, 'batch_size': 34, 'step_size': 9, 'gamma': 0.7508593339452027}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:32:12,943][0m Trial 23 finished with value: 0.059793154224137626 and parameters: {'observation_period_num': 53, 'train_rates': 0.9585020652104943, 'learning_rate': 0.00027357232533972524, 'batch_size': 68, 'step_size': 7, 'gamma': 0.76828033689128}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:32:49,261][0m Trial 24 finished with value: 0.038249935490715786 and parameters: {'observation_period_num': 7, 'train_rates': 0.8994573980873544, 'learning_rate': 0.0005860426731177662, 'batch_size': 166, 'step_size': 6, 'gamma': 0.794364222761939}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:37:56,611][0m Trial 25 finished with value: 0.2031356377381805 and parameters: {'observation_period_num': 246, 'train_rates': 0.8440134654162436, 'learning_rate': 9.513216515343002e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7731776768880505}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:39:49,819][0m Trial 26 finished with value: 0.1789290427623652 and parameters: {'observation_period_num': 25, 'train_rates': 0.7803660622721595, 'learning_rate': 0.0002638278989066572, 'batch_size': 45, 'step_size': 11, 'gamma': 0.8353735278632042}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:40:37,452][0m Trial 27 finished with value: 0.08645850379358638 and parameters: {'observation_period_num': 91, 'train_rates': 0.9233316416275236, 'learning_rate': 0.0006500907770465604, 'batch_size': 122, 'step_size': 7, 'gamma': 0.8061542147766414}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:41:06,634][0m Trial 28 finished with value: 0.12337114661931992 and parameters: {'observation_period_num': 54, 'train_rates': 0.9601160521924054, 'learning_rate': 0.0001462830314737033, 'batch_size': 215, 'step_size': 9, 'gamma': 0.750039899010941}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:42:42,606][0m Trial 29 finished with value: 0.09300473215324538 and parameters: {'observation_period_num': 94, 'train_rates': 0.9021668514302128, 'learning_rate': 0.0003149179055631507, 'batch_size': 58, 'step_size': 8, 'gamma': 0.804669128487168}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:43:53,144][0m Trial 30 finished with value: 0.6257491346229526 and parameters: {'observation_period_num': 20, 'train_rates': 0.861358069987415, 'learning_rate': 1.381275981941537e-06, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7742497977396707}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:44:30,924][0m Trial 31 finished with value: 0.040742049320149666 and parameters: {'observation_period_num': 5, 'train_rates': 0.9015388982602772, 'learning_rate': 0.0005968112754054821, 'batch_size': 165, 'step_size': 6, 'gamma': 0.7930867803842169}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:45:06,209][0m Trial 32 finished with value: 0.052716066982124894 and parameters: {'observation_period_num': 24, 'train_rates': 0.9275373478313301, 'learning_rate': 0.0006693171137564166, 'batch_size': 171, 'step_size': 5, 'gamma': 0.8011583019187841}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:45:50,034][0m Trial 33 finished with value: 0.042740121483802795 and parameters: {'observation_period_num': 5, 'train_rates': 0.9684875489247773, 'learning_rate': 0.00044059054132471117, 'batch_size': 143, 'step_size': 7, 'gamma': 0.7720730547593535}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:46:37,194][0m Trial 34 finished with value: 0.07690804412611472 and parameters: {'observation_period_num': 56, 'train_rates': 0.8883073067223427, 'learning_rate': 0.0009496603852917044, 'batch_size': 127, 'step_size': 3, 'gamma': 0.7683808105766851}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:47:11,821][0m Trial 35 finished with value: 0.05130755326301136 and parameters: {'observation_period_num': 37, 'train_rates': 0.8155180841349209, 'learning_rate': 0.0006218789773731805, 'batch_size': 159, 'step_size': 5, 'gamma': 0.8160619828295738}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:47:38,348][0m Trial 36 finished with value: 0.204933486069249 and parameters: {'observation_period_num': 19, 'train_rates': 0.7525097468251196, 'learning_rate': 6.077911231221217e-05, 'batch_size': 212, 'step_size': 8, 'gamma': 0.8466252963164357}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:48:30,949][0m Trial 37 finished with value: 0.2250420131969973 and parameters: {'observation_period_num': 178, 'train_rates': 0.9341610944078957, 'learning_rate': 2.0780374444285187e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.7986272521287547}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:51:11,000][0m Trial 38 finished with value: 0.07740999664518104 and parameters: {'observation_period_num': 69, 'train_rates': 0.8370359222073893, 'learning_rate': 0.0002042208158588159, 'batch_size': 33, 'step_size': 11, 'gamma': 0.894544944271577}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:52:57,678][0m Trial 39 finished with value: 0.1028339546173811 and parameters: {'observation_period_num': 108, 'train_rates': 0.9664642909224792, 'learning_rate': 0.0003750779594208434, 'batch_size': 54, 'step_size': 4, 'gamma': 0.779364435797394}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:54:15,594][0m Trial 40 finished with value: 0.09076638652810029 and parameters: {'observation_period_num': 34, 'train_rates': 0.8852988266162451, 'learning_rate': 0.0005300927564856438, 'batch_size': 72, 'step_size': 6, 'gamma': 0.9283576685901371}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 22:59:16,236][0m Trial 41 finished with value: 0.035741061504398076 and parameters: {'observation_period_num': 15, 'train_rates': 0.9883329224773214, 'learning_rate': 0.0008623897034930538, 'batch_size': 20, 'step_size': 9, 'gamma': 0.7615153869089927}. Best is trial 12 with value: 0.03324984759092331.[0m
[32m[I 2025-02-06 23:02:44,737][0m Trial 42 finished with value: 0.03110003644141598 and parameters: {'observation_period_num': 15, 'train_rates': 0.9766606509762927, 'learning_rate': 0.0007430373211506457, 'batch_size': 29, 'step_size': 9, 'gamma': 0.7655288419055803}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:06:33,098][0m Trial 43 finished with value: 0.13171330376556425 and parameters: {'observation_period_num': 140, 'train_rates': 0.9766618330594672, 'learning_rate': 0.0007779041188929098, 'batch_size': 25, 'step_size': 9, 'gamma': 0.7638401869297979}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:10:11,143][0m Trial 44 finished with value: 0.04318975659675504 and parameters: {'observation_period_num': 16, 'train_rates': 0.9484729617415356, 'learning_rate': 0.0004549960348157798, 'batch_size': 27, 'step_size': 11, 'gamma': 0.7604192973556771}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:12:19,011][0m Trial 45 finished with value: 0.047442287390614735 and parameters: {'observation_period_num': 43, 'train_rates': 0.9722514820844806, 'learning_rate': 0.0003307976115862212, 'batch_size': 47, 'step_size': 10, 'gamma': 0.7578607969020434}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:17:48,569][0m Trial 46 finished with value: 0.0805341899394989 and parameters: {'observation_period_num': 31, 'train_rates': 0.9888883593515989, 'learning_rate': 5.715746056583263e-06, 'batch_size': 18, 'step_size': 13, 'gamma': 0.7767853887382455}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:20:10,860][0m Trial 47 finished with value: 0.04465571104439065 and parameters: {'observation_period_num': 17, 'train_rates': 0.9223448949243047, 'learning_rate': 0.0008006116016876993, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9894560933141204}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:21:49,582][0m Trial 48 finished with value: 0.16211902168956963 and parameters: {'observation_period_num': 189, 'train_rates': 0.9735116497461053, 'learning_rate': 0.000133246026462203, 'batch_size': 58, 'step_size': 8, 'gamma': 0.8156293311615239}. Best is trial 42 with value: 0.03110003644141598.[0m
[32m[I 2025-02-06 23:24:26,128][0m Trial 49 finished with value: 0.17708276193818928 and parameters: {'observation_period_num': 48, 'train_rates': 0.6660822853622248, 'learning_rate': 0.0003616088133184127, 'batch_size': 29, 'step_size': 7, 'gamma': 0.7614305811492974}. Best is trial 42 with value: 0.03110003644141598.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8753383092331523, 'learning_rate': 0.0008087096633273017, 'batch_size': 246, 'step_size': 11, 'gamma': 0.9887110775058746}
Epoch 1/300, trend Loss: 0.4942 | 0.2365
Epoch 2/300, trend Loss: 0.2178 | 0.2273
Epoch 3/300, trend Loss: 0.2069 | 0.2769
Epoch 4/300, trend Loss: 0.2239 | 0.1683
Epoch 5/300, trend Loss: 0.2261 | 0.2890
Epoch 6/300, trend Loss: 0.2383 | 0.1541
Epoch 7/300, trend Loss: 0.1512 | 0.0990
Epoch 8/300, trend Loss: 0.1726 | 0.0939
Epoch 9/300, trend Loss: 0.1593 | 0.1083
Epoch 10/300, trend Loss: 0.1777 | 0.1926
Epoch 11/300, trend Loss: 0.2142 | 0.2851
Epoch 12/300, trend Loss: 0.1936 | 0.1769
Epoch 13/300, trend Loss: 0.1645 | 0.1457
Epoch 14/300, trend Loss: 0.1620 | 0.0965
Epoch 15/300, trend Loss: 0.1503 | 0.1747
Epoch 16/300, trend Loss: 0.1523 | 0.1262
Epoch 17/300, trend Loss: 0.1655 | 0.1722
Epoch 18/300, trend Loss: 0.1541 | 0.0948
Epoch 19/300, trend Loss: 0.1349 | 0.1188
Epoch 20/300, trend Loss: 0.1209 | 0.0864
Epoch 21/300, trend Loss: 0.1285 | 0.1480
Epoch 22/300, trend Loss: 0.1181 | 0.0844
Epoch 23/300, trend Loss: 0.1132 | 0.0752
Epoch 24/300, trend Loss: 0.1068 | 0.0661
Epoch 25/300, trend Loss: 0.1049 | 0.0700
Epoch 26/300, trend Loss: 0.1028 | 0.0687
Epoch 27/300, trend Loss: 0.1049 | 0.0812
Epoch 28/300, trend Loss: 0.1039 | 0.0659
Epoch 29/300, trend Loss: 0.1019 | 0.0721
Epoch 30/300, trend Loss: 0.0976 | 0.0616
Epoch 31/300, trend Loss: 0.0965 | 0.0624
Epoch 32/300, trend Loss: 0.0981 | 0.0624
Epoch 33/300, trend Loss: 0.1052 | 0.0882
Epoch 34/300, trend Loss: 0.1002 | 0.0579
Epoch 35/300, trend Loss: 0.1011 | 0.0684
Epoch 36/300, trend Loss: 0.1027 | 0.0633
Epoch 37/300, trend Loss: 0.1062 | 0.0858
Epoch 38/300, trend Loss: 0.1025 | 0.0564
Epoch 39/300, trend Loss: 0.1069 | 0.0909
Epoch 40/300, trend Loss: 0.1065 | 0.0649
Epoch 41/300, trend Loss: 0.1089 | 0.0910
Epoch 42/300, trend Loss: 0.0997 | 0.0562
Epoch 43/300, trend Loss: 0.0964 | 0.0762
Epoch 44/300, trend Loss: 0.0988 | 0.0559
Epoch 45/300, trend Loss: 0.0967 | 0.0830
Epoch 46/300, trend Loss: 0.0937 | 0.0545
Epoch 47/300, trend Loss: 0.0926 | 0.0644
Epoch 48/300, trend Loss: 0.0923 | 0.0534
Epoch 49/300, trend Loss: 0.0898 | 0.0677
Epoch 50/300, trend Loss: 0.0888 | 0.0517
Epoch 51/300, trend Loss: 0.0876 | 0.0553
Epoch 52/300, trend Loss: 0.0874 | 0.0501
Epoch 53/300, trend Loss: 0.0857 | 0.0540
Epoch 54/300, trend Loss: 0.0858 | 0.0487
Epoch 55/300, trend Loss: 0.0845 | 0.0481
Epoch 56/300, trend Loss: 0.0838 | 0.0471
Epoch 57/300, trend Loss: 0.0834 | 0.0486
Epoch 58/300, trend Loss: 0.0833 | 0.0464
Epoch 59/300, trend Loss: 0.0832 | 0.0485
Epoch 60/300, trend Loss: 0.0831 | 0.0457
Epoch 61/300, trend Loss: 0.0836 | 0.0487
Epoch 62/300, trend Loss: 0.0832 | 0.0459
Epoch 63/300, trend Loss: 0.0832 | 0.0479
Epoch 64/300, trend Loss: 0.0827 | 0.0452
Epoch 65/300, trend Loss: 0.0824 | 0.0478
Epoch 66/300, trend Loss: 0.0824 | 0.0450
Epoch 67/300, trend Loss: 0.0818 | 0.0474
Epoch 68/300, trend Loss: 0.0820 | 0.0449
Epoch 69/300, trend Loss: 0.0814 | 0.0478
Epoch 70/300, trend Loss: 0.0819 | 0.0443
Epoch 71/300, trend Loss: 0.0815 | 0.0465
Epoch 72/300, trend Loss: 0.0802 | 0.0437
Epoch 73/300, trend Loss: 0.0798 | 0.0481
Epoch 74/300, trend Loss: 0.0780 | 0.0422
Epoch 75/300, trend Loss: 0.0756 | 0.0400
Epoch 76/300, trend Loss: 0.0753 | 0.0396
Epoch 77/300, trend Loss: 0.0760 | 0.0472
Epoch 78/300, trend Loss: 0.0752 | 0.0407
Epoch 79/300, trend Loss: 0.0734 | 0.0383
Epoch 80/300, trend Loss: 0.0724 | 0.0375
Epoch 81/300, trend Loss: 0.0712 | 0.0375
Epoch 82/300, trend Loss: 0.0703 | 0.0358
Epoch 83/300, trend Loss: 0.0699 | 0.0364
Epoch 84/300, trend Loss: 0.0691 | 0.0355
Epoch 85/300, trend Loss: 0.0701 | 0.0356
Epoch 86/300, trend Loss: 0.0719 | 0.0375
Epoch 87/300, trend Loss: 0.0733 | 0.0405
Epoch 88/300, trend Loss: 0.0742 | 0.0382
Epoch 89/300, trend Loss: 0.0758 | 0.0396
Epoch 90/300, trend Loss: 0.0789 | 0.0471
Epoch 91/300, trend Loss: 0.0754 | 0.0400
Epoch 92/300, trend Loss: 0.0768 | 0.0477
Epoch 93/300, trend Loss: 0.0807 | 0.0427
Epoch 94/300, trend Loss: 0.0778 | 0.0422
Epoch 95/300, trend Loss: 0.0755 | 0.0426
Epoch 96/300, trend Loss: 0.0717 | 0.0372
Epoch 97/300, trend Loss: 0.0699 | 0.0356
Epoch 98/300, trend Loss: 0.0689 | 0.0353
Epoch 99/300, trend Loss: 0.0683 | 0.0351
Epoch 100/300, trend Loss: 0.0674 | 0.0351
Epoch 101/300, trend Loss: 0.0669 | 0.0361
Epoch 102/300, trend Loss: 0.0674 | 0.0355
Epoch 103/300, trend Loss: 0.0679 | 0.0333
Epoch 104/300, trend Loss: 0.0668 | 0.0332
Epoch 105/300, trend Loss: 0.0659 | 0.0329
Epoch 106/300, trend Loss: 0.0658 | 0.0341
Epoch 107/300, trend Loss: 0.0648 | 0.0315
Epoch 108/300, trend Loss: 0.0642 | 0.0333
Epoch 109/300, trend Loss: 0.0640 | 0.0319
Epoch 110/300, trend Loss: 0.0646 | 0.0330
Epoch 111/300, trend Loss: 0.0642 | 0.0316
Epoch 112/300, trend Loss: 0.0644 | 0.0317
Epoch 113/300, trend Loss: 0.0654 | 0.0350
Epoch 114/300, trend Loss: 0.0678 | 0.0343
Epoch 115/300, trend Loss: 0.0679 | 0.0335
Epoch 116/300, trend Loss: 0.0687 | 0.0345
Epoch 117/300, trend Loss: 0.0690 | 0.0359
Epoch 118/300, trend Loss: 0.0682 | 0.0352
Epoch 119/300, trend Loss: 0.0663 | 0.0323
Epoch 120/300, trend Loss: 0.0647 | 0.0305
Epoch 121/300, trend Loss: 0.0639 | 0.0343
Epoch 122/300, trend Loss: 0.0659 | 0.0335
Epoch 123/300, trend Loss: 0.0665 | 0.0349
Epoch 124/300, trend Loss: 0.0661 | 0.0360
Epoch 125/300, trend Loss: 0.0661 | 0.0346
Epoch 126/300, trend Loss: 0.0650 | 0.0339
Epoch 127/300, trend Loss: 0.0634 | 0.0325
Epoch 128/300, trend Loss: 0.0618 | 0.0309
Epoch 129/300, trend Loss: 0.0616 | 0.0301
Epoch 130/300, trend Loss: 0.0617 | 0.0335
Epoch 131/300, trend Loss: 0.0633 | 0.0320
Epoch 132/300, trend Loss: 0.0616 | 0.0321
Epoch 133/300, trend Loss: 0.0618 | 0.0301
Epoch 134/300, trend Loss: 0.0626 | 0.0356
Epoch 135/300, trend Loss: 0.0622 | 0.0313
Epoch 136/300, trend Loss: 0.0606 | 0.0307
Epoch 137/300, trend Loss: 0.0617 | 0.0299
Epoch 138/300, trend Loss: 0.0624 | 0.0338
Epoch 139/300, trend Loss: 0.0621 | 0.0304
Epoch 140/300, trend Loss: 0.0635 | 0.0350
Epoch 141/300, trend Loss: 0.0635 | 0.0302
Epoch 142/300, trend Loss: 0.0628 | 0.0316
Epoch 143/300, trend Loss: 0.0618 | 0.0308
Epoch 144/300, trend Loss: 0.0620 | 0.0286
Epoch 145/300, trend Loss: 0.0620 | 0.0322
Epoch 146/300, trend Loss: 0.0637 | 0.0305
Epoch 147/300, trend Loss: 0.0675 | 0.0388
Epoch 148/300, trend Loss: 0.0694 | 0.0341
Epoch 149/300, trend Loss: 0.0654 | 0.0394
Epoch 150/300, trend Loss: 0.0660 | 0.0339
Epoch 151/300, trend Loss: 0.0664 | 0.0348
Epoch 152/300, trend Loss: 0.0662 | 0.0337
Epoch 153/300, trend Loss: 0.0693 | 0.0379
Epoch 154/300, trend Loss: 0.0705 | 0.0388
Epoch 155/300, trend Loss: 0.0707 | 0.0415
Epoch 156/300, trend Loss: 0.0656 | 0.0334
Epoch 157/300, trend Loss: 0.0686 | 0.0466
Epoch 158/300, trend Loss: 0.0735 | 0.0358
Epoch 159/300, trend Loss: 0.0696 | 0.0448
Epoch 160/300, trend Loss: 0.0711 | 0.0345
Epoch 161/300, trend Loss: 0.0687 | 0.0385
Epoch 162/300, trend Loss: 0.0654 | 0.0336
Epoch 163/300, trend Loss: 0.0618 | 0.0323
Epoch 164/300, trend Loss: 0.0597 | 0.0305
Epoch 165/300, trend Loss: 0.0597 | 0.0306
Epoch 166/300, trend Loss: 0.0592 | 0.0294
Epoch 167/300, trend Loss: 0.0592 | 0.0318
Epoch 168/300, trend Loss: 0.0601 | 0.0307
Epoch 169/300, trend Loss: 0.0610 | 0.0318
Epoch 170/300, trend Loss: 0.0592 | 0.0325
Epoch 171/300, trend Loss: 0.0631 | 0.0329
Epoch 172/300, trend Loss: 0.0628 | 0.0349
Epoch 173/300, trend Loss: 0.0632 | 0.0311
Epoch 174/300, trend Loss: 0.0582 | 0.0294
Epoch 175/300, trend Loss: 0.0589 | 0.0305
Epoch 176/300, trend Loss: 0.0602 | 0.0291
Epoch 177/300, trend Loss: 0.0573 | 0.0276
Epoch 178/300, trend Loss: 0.0562 | 0.0275
Epoch 179/300, trend Loss: 0.0562 | 0.0277
Epoch 180/300, trend Loss: 0.0559 | 0.0276
Epoch 181/300, trend Loss: 0.0609 | 0.0356
Epoch 182/300, trend Loss: 0.0616 | 0.0352
Epoch 183/300, trend Loss: 0.0620 | 0.0342
Epoch 184/300, trend Loss: 0.0607 | 0.0332
Epoch 185/300, trend Loss: 0.0600 | 0.0315
Epoch 186/300, trend Loss: 0.0592 | 0.0291
Epoch 187/300, trend Loss: 0.0567 | 0.0322
Epoch 188/300, trend Loss: 0.0589 | 0.0287
Epoch 189/300, trend Loss: 0.0599 | 0.0376
Epoch 190/300, trend Loss: 0.0631 | 0.0337
Epoch 191/300, trend Loss: 0.0590 | 0.0300
Epoch 192/300, trend Loss: 0.0580 | 0.0294
Epoch 193/300, trend Loss: 0.0570 | 0.0299
Epoch 194/300, trend Loss: 0.0592 | 0.0278
Epoch 195/300, trend Loss: 0.0580 | 0.0281
Epoch 196/300, trend Loss: 0.0585 | 0.0300
Epoch 197/300, trend Loss: 0.0554 | 0.0262
Epoch 198/300, trend Loss: 0.0547 | 0.0261
Epoch 199/300, trend Loss: 0.0543 | 0.0259
Epoch 200/300, trend Loss: 0.0570 | 0.0301
Epoch 201/300, trend Loss: 0.0557 | 0.0287
Epoch 202/300, trend Loss: 0.0560 | 0.0296
Epoch 203/300, trend Loss: 0.0556 | 0.0275
Epoch 204/300, trend Loss: 0.0556 | 0.0299
Epoch 205/300, trend Loss: 0.0555 | 0.0320
Epoch 206/300, trend Loss: 0.0560 | 0.0293
Epoch 207/300, trend Loss: 0.0559 | 0.0287
Epoch 208/300, trend Loss: 0.0557 | 0.0289
Epoch 209/300, trend Loss: 0.0563 | 0.0284
Epoch 210/300, trend Loss: 0.0552 | 0.0328
Epoch 211/300, trend Loss: 0.0547 | 0.0315
Epoch 212/300, trend Loss: 0.0566 | 0.0327
Epoch 213/300, trend Loss: 0.0578 | 0.0316
Epoch 214/300, trend Loss: 0.0577 | 0.0277
Epoch 215/300, trend Loss: 0.0556 | 0.0271
Epoch 216/300, trend Loss: 0.0555 | 0.0284
Epoch 217/300, trend Loss: 0.0553 | 0.0282
Epoch 218/300, trend Loss: 0.0553 | 0.0356
Epoch 219/300, trend Loss: 0.0555 | 0.0276
Epoch 220/300, trend Loss: 0.0537 | 0.0284
Epoch 221/300, trend Loss: 0.0536 | 0.0278
Epoch 222/300, trend Loss: 0.0529 | 0.0274
Epoch 223/300, trend Loss: 0.0534 | 0.0282
Epoch 224/300, trend Loss: 0.0536 | 0.0274
Epoch 225/300, trend Loss: 0.0530 | 0.0274
Epoch 226/300, trend Loss: 0.0522 | 0.0265
Epoch 227/300, trend Loss: 0.0523 | 0.0283
Epoch 228/300, trend Loss: 0.0527 | 0.0267
Epoch 229/300, trend Loss: 0.0524 | 0.0268
Epoch 230/300, trend Loss: 0.0520 | 0.0266
Epoch 231/300, trend Loss: 0.0513 | 0.0277
Epoch 232/300, trend Loss: 0.0524 | 0.0270
Epoch 233/300, trend Loss: 0.0521 | 0.0276
Epoch 234/300, trend Loss: 0.0534 | 0.0312
Epoch 235/300, trend Loss: 0.0534 | 0.0268
Epoch 236/300, trend Loss: 0.0530 | 0.0290
Epoch 237/300, trend Loss: 0.0529 | 0.0277
Epoch 238/300, trend Loss: 0.0521 | 0.0277
Epoch 239/300, trend Loss: 0.0518 | 0.0300
Epoch 240/300, trend Loss: 0.0526 | 0.0285
Epoch 241/300, trend Loss: 0.0526 | 0.0305
Epoch 242/300, trend Loss: 0.0540 | 0.0308
Epoch 243/300, trend Loss: 0.0519 | 0.0267
Epoch 244/300, trend Loss: 0.0509 | 0.0264
Epoch 245/300, trend Loss: 0.0506 | 0.0284
Epoch 246/300, trend Loss: 0.0507 | 0.0289
Epoch 247/300, trend Loss: 0.0521 | 0.0295
Epoch 248/300, trend Loss: 0.0522 | 0.0283
Epoch 249/300, trend Loss: 0.0512 | 0.0298
Epoch 250/300, trend Loss: 0.0508 | 0.0273
Epoch 251/300, trend Loss: 0.0497 | 0.0270
Epoch 252/300, trend Loss: 0.0509 | 0.0343
Epoch 253/300, trend Loss: 0.0526 | 0.0280
Epoch 254/300, trend Loss: 0.0521 | 0.0295
Epoch 255/300, trend Loss: 0.0523 | 0.0289
Epoch 256/300, trend Loss: 0.0507 | 0.0285
Epoch 257/300, trend Loss: 0.0507 | 0.0277
Epoch 258/300, trend Loss: 0.0514 | 0.0288
Epoch 259/300, trend Loss: 0.0527 | 0.0313
Epoch 260/300, trend Loss: 0.0516 | 0.0286
Epoch 261/300, trend Loss: 0.0505 | 0.0315
Epoch 262/300, trend Loss: 0.0487 | 0.0299
Epoch 263/300, trend Loss: 0.0502 | 0.0371
Epoch 264/300, trend Loss: 0.0548 | 0.0306
Epoch 265/300, trend Loss: 0.0540 | 0.0360
Epoch 266/300, trend Loss: 0.0521 | 0.0304
Epoch 267/300, trend Loss: 0.0510 | 0.0281
Epoch 268/300, trend Loss: 0.0502 | 0.0276
Epoch 269/300, trend Loss: 0.0500 | 0.0317
Epoch 270/300, trend Loss: 0.0499 | 0.0285
Epoch 271/300, trend Loss: 0.0502 | 0.0270
Epoch 272/300, trend Loss: 0.0512 | 0.0333
Epoch 273/300, trend Loss: 0.0498 | 0.0310
Epoch 274/300, trend Loss: 0.0494 | 0.0408
Epoch 275/300, trend Loss: 0.0544 | 0.0284
Epoch 276/300, trend Loss: 0.0547 | 0.0342
Epoch 277/300, trend Loss: 0.0530 | 0.0276
Epoch 278/300, trend Loss: 0.0516 | 0.0310
Epoch 279/300, trend Loss: 0.0510 | 0.0303
Epoch 280/300, trend Loss: 0.0492 | 0.0288
Epoch 281/300, trend Loss: 0.0502 | 0.0281
Epoch 282/300, trend Loss: 0.0504 | 0.0321
Epoch 283/300, trend Loss: 0.0565 | 0.0381
Epoch 284/300, trend Loss: 0.0581 | 0.0337
Epoch 285/300, trend Loss: 0.0564 | 0.0324
Epoch 286/300, trend Loss: 0.0552 | 0.0398
Epoch 287/300, trend Loss: 0.0553 | 0.0326
Epoch 288/300, trend Loss: 0.0562 | 0.0310
Epoch 289/300, trend Loss: 0.0540 | 0.0347
Epoch 290/300, trend Loss: 0.0538 | 0.0314
Epoch 291/300, trend Loss: 0.0517 | 0.0303
Epoch 292/300, trend Loss: 0.0535 | 0.0360
Epoch 293/300, trend Loss: 0.0546 | 0.0308
Epoch 294/300, trend Loss: 0.0532 | 0.0295
Epoch 295/300, trend Loss: 0.0502 | 0.0272
Epoch 296/300, trend Loss: 0.0491 | 0.0354
Epoch 297/300, trend Loss: 0.0497 | 0.0275
Epoch 298/300, trend Loss: 0.0491 | 0.0275
Epoch 299/300, trend Loss: 0.0499 | 0.0281
Epoch 300/300, trend Loss: 0.0485 | 0.0279
Training seasonal_0 component with params: {'observation_period_num': 34, 'train_rates': 0.8607462222580097, 'learning_rate': 0.00015207790051888234, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8115737157525621}
Epoch 1/300, seasonal_0 Loss: 0.4070 | 0.3295
Epoch 2/300, seasonal_0 Loss: 0.2417 | 0.2823
Epoch 3/300, seasonal_0 Loss: 0.1985 | 0.2680
Epoch 4/300, seasonal_0 Loss: 0.1933 | 0.3241
Epoch 5/300, seasonal_0 Loss: 0.1755 | 0.2809
Epoch 6/300, seasonal_0 Loss: 0.1574 | 0.1603
Epoch 7/300, seasonal_0 Loss: 0.1655 | 0.1501
Epoch 8/300, seasonal_0 Loss: 0.1465 | 0.1302
Epoch 9/300, seasonal_0 Loss: 0.1373 | 0.1521
Epoch 10/300, seasonal_0 Loss: 0.1323 | 0.1320
Epoch 11/300, seasonal_0 Loss: 0.1258 | 0.1092
Epoch 12/300, seasonal_0 Loss: 0.1276 | 0.1037
Epoch 13/300, seasonal_0 Loss: 0.1294 | 0.0991
Epoch 14/300, seasonal_0 Loss: 0.1316 | 0.0943
Epoch 15/300, seasonal_0 Loss: 0.1386 | 0.1067
Epoch 16/300, seasonal_0 Loss: 0.1424 | 0.0999
Epoch 17/300, seasonal_0 Loss: 0.1361 | 0.0895
Epoch 18/300, seasonal_0 Loss: 0.1285 | 0.1010
Epoch 19/300, seasonal_0 Loss: 0.1144 | 0.0894
Epoch 20/300, seasonal_0 Loss: 0.1115 | 0.1055
Epoch 21/300, seasonal_0 Loss: 0.1171 | 0.1397
Epoch 22/300, seasonal_0 Loss: 0.1088 | 0.0835
Epoch 23/300, seasonal_0 Loss: 0.1044 | 0.0808
Epoch 24/300, seasonal_0 Loss: 0.1036 | 0.0784
Epoch 25/300, seasonal_0 Loss: 0.0999 | 0.0797
Epoch 26/300, seasonal_0 Loss: 0.0990 | 0.0794
Epoch 27/300, seasonal_0 Loss: 0.0975 | 0.0751
Epoch 28/300, seasonal_0 Loss: 0.0963 | 0.0730
Epoch 29/300, seasonal_0 Loss: 0.0958 | 0.0722
Epoch 30/300, seasonal_0 Loss: 0.0947 | 0.0718
Epoch 31/300, seasonal_0 Loss: 0.0939 | 0.0712
Epoch 32/300, seasonal_0 Loss: 0.0931 | 0.0703
Epoch 33/300, seasonal_0 Loss: 0.0923 | 0.0693
Epoch 34/300, seasonal_0 Loss: 0.0915 | 0.0685
Epoch 35/300, seasonal_0 Loss: 0.0910 | 0.0676
Epoch 36/300, seasonal_0 Loss: 0.0904 | 0.0671
Epoch 37/300, seasonal_0 Loss: 0.0898 | 0.0665
Epoch 38/300, seasonal_0 Loss: 0.0893 | 0.0661
Epoch 39/300, seasonal_0 Loss: 0.0887 | 0.0657
Epoch 40/300, seasonal_0 Loss: 0.0882 | 0.0653
Epoch 41/300, seasonal_0 Loss: 0.0877 | 0.0646
Epoch 42/300, seasonal_0 Loss: 0.0873 | 0.0642
Epoch 43/300, seasonal_0 Loss: 0.0870 | 0.0639
Epoch 44/300, seasonal_0 Loss: 0.0866 | 0.0635
Epoch 45/300, seasonal_0 Loss: 0.0862 | 0.0630
Epoch 46/300, seasonal_0 Loss: 0.0858 | 0.0627
Epoch 47/300, seasonal_0 Loss: 0.0854 | 0.0624
Epoch 48/300, seasonal_0 Loss: 0.0851 | 0.0620
Epoch 49/300, seasonal_0 Loss: 0.0848 | 0.0618
Epoch 50/300, seasonal_0 Loss: 0.0845 | 0.0617
Epoch 51/300, seasonal_0 Loss: 0.0843 | 0.0614
Epoch 52/300, seasonal_0 Loss: 0.0840 | 0.0610
Epoch 53/300, seasonal_0 Loss: 0.0837 | 0.0607
Epoch 54/300, seasonal_0 Loss: 0.0834 | 0.0604
Epoch 55/300, seasonal_0 Loss: 0.0832 | 0.0602
Epoch 56/300, seasonal_0 Loss: 0.0830 | 0.0601
Epoch 57/300, seasonal_0 Loss: 0.0828 | 0.0600
Epoch 58/300, seasonal_0 Loss: 0.0826 | 0.0597
Epoch 59/300, seasonal_0 Loss: 0.0823 | 0.0595
Epoch 60/300, seasonal_0 Loss: 0.0821 | 0.0592
Epoch 61/300, seasonal_0 Loss: 0.0819 | 0.0591
Epoch 62/300, seasonal_0 Loss: 0.0817 | 0.0589
Epoch 63/300, seasonal_0 Loss: 0.0816 | 0.0588
Epoch 64/300, seasonal_0 Loss: 0.0814 | 0.0587
Epoch 65/300, seasonal_0 Loss: 0.0812 | 0.0585
Epoch 66/300, seasonal_0 Loss: 0.0810 | 0.0583
Epoch 67/300, seasonal_0 Loss: 0.0809 | 0.0582
Epoch 68/300, seasonal_0 Loss: 0.0807 | 0.0581
Epoch 69/300, seasonal_0 Loss: 0.0806 | 0.0580
Epoch 70/300, seasonal_0 Loss: 0.0805 | 0.0578
Epoch 71/300, seasonal_0 Loss: 0.0803 | 0.0577
Epoch 72/300, seasonal_0 Loss: 0.0802 | 0.0576
Epoch 73/300, seasonal_0 Loss: 0.0800 | 0.0575
Epoch 74/300, seasonal_0 Loss: 0.0799 | 0.0574
Epoch 75/300, seasonal_0 Loss: 0.0798 | 0.0573
Epoch 76/300, seasonal_0 Loss: 0.0797 | 0.0572
Epoch 77/300, seasonal_0 Loss: 0.0796 | 0.0571
Epoch 78/300, seasonal_0 Loss: 0.0795 | 0.0570
Epoch 79/300, seasonal_0 Loss: 0.0794 | 0.0569
Epoch 80/300, seasonal_0 Loss: 0.0793 | 0.0568
Epoch 81/300, seasonal_0 Loss: 0.0792 | 0.0567
Epoch 82/300, seasonal_0 Loss: 0.0791 | 0.0567
Epoch 83/300, seasonal_0 Loss: 0.0790 | 0.0566
Epoch 84/300, seasonal_0 Loss: 0.0789 | 0.0565
Epoch 85/300, seasonal_0 Loss: 0.0788 | 0.0564
Epoch 86/300, seasonal_0 Loss: 0.0787 | 0.0563
Epoch 87/300, seasonal_0 Loss: 0.0787 | 0.0563
Epoch 88/300, seasonal_0 Loss: 0.0786 | 0.0562
Epoch 89/300, seasonal_0 Loss: 0.0785 | 0.0561
Epoch 90/300, seasonal_0 Loss: 0.0784 | 0.0561
Epoch 91/300, seasonal_0 Loss: 0.0784 | 0.0560
Epoch 92/300, seasonal_0 Loss: 0.0783 | 0.0560
Epoch 93/300, seasonal_0 Loss: 0.0782 | 0.0559
Epoch 94/300, seasonal_0 Loss: 0.0782 | 0.0559
Epoch 95/300, seasonal_0 Loss: 0.0781 | 0.0558
Epoch 96/300, seasonal_0 Loss: 0.0781 | 0.0558
Epoch 97/300, seasonal_0 Loss: 0.0780 | 0.0557
Epoch 98/300, seasonal_0 Loss: 0.0779 | 0.0556
Epoch 99/300, seasonal_0 Loss: 0.0779 | 0.0556
Epoch 100/300, seasonal_0 Loss: 0.0778 | 0.0556
Epoch 101/300, seasonal_0 Loss: 0.0778 | 0.0555
Epoch 102/300, seasonal_0 Loss: 0.0777 | 0.0555
Epoch 103/300, seasonal_0 Loss: 0.0777 | 0.0554
Epoch 104/300, seasonal_0 Loss: 0.0776 | 0.0554
Epoch 105/300, seasonal_0 Loss: 0.0776 | 0.0553
Epoch 106/300, seasonal_0 Loss: 0.0775 | 0.0553
Epoch 107/300, seasonal_0 Loss: 0.0775 | 0.0553
Epoch 108/300, seasonal_0 Loss: 0.0775 | 0.0552
Epoch 109/300, seasonal_0 Loss: 0.0774 | 0.0552
Epoch 110/300, seasonal_0 Loss: 0.0774 | 0.0552
Epoch 111/300, seasonal_0 Loss: 0.0773 | 0.0551
Epoch 112/300, seasonal_0 Loss: 0.0773 | 0.0551
Epoch 113/300, seasonal_0 Loss: 0.0773 | 0.0551
Epoch 114/300, seasonal_0 Loss: 0.0772 | 0.0551
Epoch 115/300, seasonal_0 Loss: 0.0772 | 0.0550
Epoch 116/300, seasonal_0 Loss: 0.0772 | 0.0550
Epoch 117/300, seasonal_0 Loss: 0.0771 | 0.0550
Epoch 118/300, seasonal_0 Loss: 0.0771 | 0.0549
Epoch 119/300, seasonal_0 Loss: 0.0771 | 0.0549
Epoch 120/300, seasonal_0 Loss: 0.0770 | 0.0549
Epoch 121/300, seasonal_0 Loss: 0.0770 | 0.0549
Epoch 122/300, seasonal_0 Loss: 0.0770 | 0.0549
Epoch 123/300, seasonal_0 Loss: 0.0770 | 0.0548
Epoch 124/300, seasonal_0 Loss: 0.0769 | 0.0548
Epoch 125/300, seasonal_0 Loss: 0.0769 | 0.0547
Epoch 126/300, seasonal_0 Loss: 0.0769 | 0.0548
Epoch 127/300, seasonal_0 Loss: 0.0769 | 0.0547
Epoch 128/300, seasonal_0 Loss: 0.0768 | 0.0547
Epoch 129/300, seasonal_0 Loss: 0.0768 | 0.0547
Epoch 130/300, seasonal_0 Loss: 0.0768 | 0.0547
Epoch 131/300, seasonal_0 Loss: 0.0768 | 0.0546
Epoch 132/300, seasonal_0 Loss: 0.0768 | 0.0547
Epoch 133/300, seasonal_0 Loss: 0.0767 | 0.0546
Epoch 134/300, seasonal_0 Loss: 0.0767 | 0.0546
Epoch 135/300, seasonal_0 Loss: 0.0767 | 0.0546
Epoch 136/300, seasonal_0 Loss: 0.0767 | 0.0546
Epoch 137/300, seasonal_0 Loss: 0.0767 | 0.0546
Epoch 138/300, seasonal_0 Loss: 0.0766 | 0.0546
Epoch 139/300, seasonal_0 Loss: 0.0766 | 0.0545
Epoch 140/300, seasonal_0 Loss: 0.0766 | 0.0545
Epoch 141/300, seasonal_0 Loss: 0.0766 | 0.0545
Epoch 142/300, seasonal_0 Loss: 0.0766 | 0.0545
Epoch 143/300, seasonal_0 Loss: 0.0766 | 0.0545
Epoch 144/300, seasonal_0 Loss: 0.0765 | 0.0545
Epoch 145/300, seasonal_0 Loss: 0.0765 | 0.0545
Epoch 146/300, seasonal_0 Loss: 0.0765 | 0.0545
Epoch 147/300, seasonal_0 Loss: 0.0765 | 0.0544
Epoch 148/300, seasonal_0 Loss: 0.0765 | 0.0544
Epoch 149/300, seasonal_0 Loss: 0.0765 | 0.0544
Epoch 150/300, seasonal_0 Loss: 0.0765 | 0.0544
Epoch 151/300, seasonal_0 Loss: 0.0765 | 0.0544
Epoch 152/300, seasonal_0 Loss: 0.0765 | 0.0544
Epoch 153/300, seasonal_0 Loss: 0.0764 | 0.0544
Epoch 154/300, seasonal_0 Loss: 0.0764 | 0.0544
Epoch 155/300, seasonal_0 Loss: 0.0764 | 0.0544
Epoch 156/300, seasonal_0 Loss: 0.0764 | 0.0544
Epoch 157/300, seasonal_0 Loss: 0.0764 | 0.0543
Epoch 158/300, seasonal_0 Loss: 0.0764 | 0.0543
Epoch 159/300, seasonal_0 Loss: 0.0764 | 0.0543
Epoch 160/300, seasonal_0 Loss: 0.0764 | 0.0543
Epoch 161/300, seasonal_0 Loss: 0.0764 | 0.0543
Epoch 162/300, seasonal_0 Loss: 0.0764 | 0.0543
Epoch 163/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 164/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 165/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 166/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 167/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 168/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 169/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 170/300, seasonal_0 Loss: 0.0763 | 0.0543
Epoch 171/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 172/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 173/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 174/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 175/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 176/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 177/300, seasonal_0 Loss: 0.0763 | 0.0542
Epoch 178/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 179/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 180/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 181/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 182/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 183/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 184/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 185/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 186/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 187/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 188/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 189/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 190/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 191/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 192/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 193/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 194/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 195/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 196/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 197/300, seasonal_0 Loss: 0.0762 | 0.0542
Epoch 198/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 199/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 200/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 201/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 202/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 203/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 204/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 205/300, seasonal_0 Loss: 0.0762 | 0.0541
Epoch 206/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 207/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 208/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 209/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 210/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 211/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 212/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 213/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 214/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 215/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 216/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 217/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 218/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 219/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 220/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 221/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 222/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 223/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 224/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 225/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 226/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 227/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 228/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 229/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 230/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 231/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 232/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 233/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 234/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 235/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 236/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 237/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 238/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 239/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 240/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 241/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 242/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 243/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 244/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 245/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 246/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 247/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 248/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 249/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 250/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 251/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 252/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 253/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 254/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 255/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 256/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 257/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 258/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 259/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 260/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 261/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 262/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 263/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 264/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 265/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 266/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 267/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 268/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 269/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 270/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 271/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 272/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 273/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 274/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 275/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 276/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 277/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 278/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 279/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 280/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 281/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 282/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 283/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 284/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 285/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 286/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 287/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 288/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 289/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 290/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 291/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 292/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 293/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 294/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 295/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 296/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 297/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 298/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 299/300, seasonal_0 Loss: 0.0761 | 0.0541
Epoch 300/300, seasonal_0 Loss: 0.0761 | 0.0541
Training seasonal_1 component with params: {'observation_period_num': 12, 'train_rates': 0.9309211514840551, 'learning_rate': 0.00023704434949700998, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9105316569981063}
Epoch 1/300, seasonal_1 Loss: 0.3966 | 0.3231
Epoch 2/300, seasonal_1 Loss: 0.1897 | 0.2762
Epoch 3/300, seasonal_1 Loss: 0.1510 | 0.1932
Epoch 4/300, seasonal_1 Loss: 0.1552 | 0.1379
Epoch 5/300, seasonal_1 Loss: 0.1765 | 0.1661
Epoch 6/300, seasonal_1 Loss: 0.2023 | 0.5797
Epoch 7/300, seasonal_1 Loss: 0.1787 | 0.1985
Epoch 8/300, seasonal_1 Loss: 0.1372 | 0.1309
Epoch 9/300, seasonal_1 Loss: 0.1421 | 0.1252
Epoch 10/300, seasonal_1 Loss: 0.1381 | 0.1191
Epoch 11/300, seasonal_1 Loss: 0.1188 | 0.1037
Epoch 12/300, seasonal_1 Loss: 0.1124 | 0.1034
Epoch 13/300, seasonal_1 Loss: 0.1084 | 0.0917
Epoch 14/300, seasonal_1 Loss: 0.1065 | 0.0821
Epoch 15/300, seasonal_1 Loss: 0.1084 | 0.0810
Epoch 16/300, seasonal_1 Loss: 0.1106 | 0.0807
Epoch 17/300, seasonal_1 Loss: 0.1101 | 0.0764
Epoch 18/300, seasonal_1 Loss: 0.1065 | 0.0745
Epoch 19/300, seasonal_1 Loss: 0.1043 | 0.0748
Epoch 20/300, seasonal_1 Loss: 0.1039 | 0.0744
Epoch 21/300, seasonal_1 Loss: 0.1048 | 0.0883
Epoch 22/300, seasonal_1 Loss: 0.1080 | 0.1275
Epoch 23/300, seasonal_1 Loss: 0.1070 | 0.1136
Epoch 24/300, seasonal_1 Loss: 0.1005 | 0.0680
Epoch 25/300, seasonal_1 Loss: 0.1034 | 0.0887
Epoch 26/300, seasonal_1 Loss: 0.1089 | 0.0753
Epoch 27/300, seasonal_1 Loss: 0.1009 | 0.0708
Epoch 28/300, seasonal_1 Loss: 0.0984 | 0.0718
Epoch 29/300, seasonal_1 Loss: 0.0982 | 0.0656
Epoch 30/300, seasonal_1 Loss: 0.0927 | 0.0607
Epoch 31/300, seasonal_1 Loss: 0.0892 | 0.0581
Epoch 32/300, seasonal_1 Loss: 0.0889 | 0.0577
Epoch 33/300, seasonal_1 Loss: 0.0888 | 0.0587
Epoch 34/300, seasonal_1 Loss: 0.0885 | 0.0613
Epoch 35/300, seasonal_1 Loss: 0.0872 | 0.0579
Epoch 36/300, seasonal_1 Loss: 0.0859 | 0.0544
Epoch 37/300, seasonal_1 Loss: 0.0898 | 0.0806
Epoch 38/300, seasonal_1 Loss: 0.1021 | 0.1356
Epoch 39/300, seasonal_1 Loss: 0.1101 | 0.0921
Epoch 40/300, seasonal_1 Loss: 0.1047 | 0.0683
Epoch 41/300, seasonal_1 Loss: 0.1083 | 0.0861
Epoch 42/300, seasonal_1 Loss: 0.0966 | 0.0589
Epoch 43/300, seasonal_1 Loss: 0.0928 | 0.0748
Epoch 44/300, seasonal_1 Loss: 0.1001 | 0.0762
Epoch 45/300, seasonal_1 Loss: 0.0972 | 0.0588
Epoch 46/300, seasonal_1 Loss: 0.0910 | 0.0677
Epoch 47/300, seasonal_1 Loss: 0.0872 | 0.0555
Epoch 48/300, seasonal_1 Loss: 0.0883 | 0.0679
Epoch 49/300, seasonal_1 Loss: 0.0909 | 0.0827
Epoch 50/300, seasonal_1 Loss: 0.0857 | 0.0511
Epoch 51/300, seasonal_1 Loss: 0.0793 | 0.0489
Epoch 52/300, seasonal_1 Loss: 0.0798 | 0.0474
Epoch 53/300, seasonal_1 Loss: 0.0778 | 0.0478
Epoch 54/300, seasonal_1 Loss: 0.0770 | 0.0510
Epoch 55/300, seasonal_1 Loss: 0.0771 | 0.0472
Epoch 56/300, seasonal_1 Loss: 0.0762 | 0.0440
Epoch 57/300, seasonal_1 Loss: 0.0755 | 0.0436
Epoch 58/300, seasonal_1 Loss: 0.0751 | 0.0440
Epoch 59/300, seasonal_1 Loss: 0.0747 | 0.0440
Epoch 60/300, seasonal_1 Loss: 0.0743 | 0.0431
Epoch 61/300, seasonal_1 Loss: 0.0739 | 0.0423
Epoch 62/300, seasonal_1 Loss: 0.0737 | 0.0420
Epoch 63/300, seasonal_1 Loss: 0.0734 | 0.0418
Epoch 64/300, seasonal_1 Loss: 0.0730 | 0.0417
Epoch 65/300, seasonal_1 Loss: 0.0728 | 0.0413
Epoch 66/300, seasonal_1 Loss: 0.0725 | 0.0409
Epoch 67/300, seasonal_1 Loss: 0.0722 | 0.0406
Epoch 68/300, seasonal_1 Loss: 0.0720 | 0.0403
Epoch 69/300, seasonal_1 Loss: 0.0718 | 0.0402
Epoch 70/300, seasonal_1 Loss: 0.0716 | 0.0400
Epoch 71/300, seasonal_1 Loss: 0.0715 | 0.0397
Epoch 72/300, seasonal_1 Loss: 0.0713 | 0.0394
Epoch 73/300, seasonal_1 Loss: 0.0712 | 0.0391
Epoch 74/300, seasonal_1 Loss: 0.0712 | 0.0389
Epoch 75/300, seasonal_1 Loss: 0.0713 | 0.0388
Epoch 76/300, seasonal_1 Loss: 0.0715 | 0.0387
Epoch 77/300, seasonal_1 Loss: 0.0717 | 0.0387
Epoch 78/300, seasonal_1 Loss: 0.0720 | 0.0388
Epoch 79/300, seasonal_1 Loss: 0.0723 | 0.0392
Epoch 80/300, seasonal_1 Loss: 0.0724 | 0.0390
Epoch 81/300, seasonal_1 Loss: 0.0724 | 0.0393
Epoch 82/300, seasonal_1 Loss: 0.0721 | 0.0400
Epoch 83/300, seasonal_1 Loss: 0.0729 | 0.0404
Epoch 84/300, seasonal_1 Loss: 0.0741 | 0.0416
Epoch 85/300, seasonal_1 Loss: 0.0751 | 0.0447
Epoch 86/300, seasonal_1 Loss: 0.0758 | 0.0441
Epoch 87/300, seasonal_1 Loss: 0.0743 | 0.0411
Epoch 88/300, seasonal_1 Loss: 0.0741 | 0.0400
Epoch 89/300, seasonal_1 Loss: 0.0730 | 0.0388
Epoch 90/300, seasonal_1 Loss: 0.0709 | 0.0378
Epoch 91/300, seasonal_1 Loss: 0.0700 | 0.0384
Epoch 92/300, seasonal_1 Loss: 0.0695 | 0.0383
Epoch 93/300, seasonal_1 Loss: 0.0692 | 0.0375
Epoch 94/300, seasonal_1 Loss: 0.0686 | 0.0368
Epoch 95/300, seasonal_1 Loss: 0.0683 | 0.0384
Epoch 96/300, seasonal_1 Loss: 0.0682 | 0.0393
Epoch 97/300, seasonal_1 Loss: 0.0679 | 0.0369
Epoch 98/300, seasonal_1 Loss: 0.0675 | 0.0363
Epoch 99/300, seasonal_1 Loss: 0.0676 | 0.0366
Epoch 100/300, seasonal_1 Loss: 0.0676 | 0.0362
Epoch 101/300, seasonal_1 Loss: 0.0672 | 0.0366
Epoch 102/300, seasonal_1 Loss: 0.0670 | 0.0368
Epoch 103/300, seasonal_1 Loss: 0.0671 | 0.0360
Epoch 104/300, seasonal_1 Loss: 0.0672 | 0.0356
Epoch 105/300, seasonal_1 Loss: 0.0679 | 0.0368
Epoch 106/300, seasonal_1 Loss: 0.0694 | 0.0378
Epoch 107/300, seasonal_1 Loss: 0.0710 | 0.0361
Epoch 108/300, seasonal_1 Loss: 0.0695 | 0.0367
Epoch 109/300, seasonal_1 Loss: 0.0675 | 0.0402
Epoch 110/300, seasonal_1 Loss: 0.0676 | 0.0403
Epoch 111/300, seasonal_1 Loss: 0.0674 | 0.0356
Epoch 112/300, seasonal_1 Loss: 0.0664 | 0.0339
Epoch 113/300, seasonal_1 Loss: 0.0661 | 0.0341
Epoch 114/300, seasonal_1 Loss: 0.0661 | 0.0346
Epoch 115/300, seasonal_1 Loss: 0.0659 | 0.0348
Epoch 116/300, seasonal_1 Loss: 0.0655 | 0.0343
Epoch 117/300, seasonal_1 Loss: 0.0651 | 0.0339
Epoch 118/300, seasonal_1 Loss: 0.0649 | 0.0339
Epoch 119/300, seasonal_1 Loss: 0.0648 | 0.0340
Epoch 120/300, seasonal_1 Loss: 0.0647 | 0.0340
Epoch 121/300, seasonal_1 Loss: 0.0646 | 0.0339
Epoch 122/300, seasonal_1 Loss: 0.0646 | 0.0339
Epoch 123/300, seasonal_1 Loss: 0.0645 | 0.0340
Epoch 124/300, seasonal_1 Loss: 0.0644 | 0.0339
Epoch 125/300, seasonal_1 Loss: 0.0643 | 0.0337
Epoch 126/300, seasonal_1 Loss: 0.0642 | 0.0335
Epoch 127/300, seasonal_1 Loss: 0.0640 | 0.0333
Epoch 128/300, seasonal_1 Loss: 0.0639 | 0.0332
Epoch 129/300, seasonal_1 Loss: 0.0639 | 0.0330
Epoch 130/300, seasonal_1 Loss: 0.0639 | 0.0329
Epoch 131/300, seasonal_1 Loss: 0.0640 | 0.0329
Epoch 132/300, seasonal_1 Loss: 0.0641 | 0.0329
Epoch 133/300, seasonal_1 Loss: 0.0642 | 0.0329
Epoch 134/300, seasonal_1 Loss: 0.0640 | 0.0328
Epoch 135/300, seasonal_1 Loss: 0.0638 | 0.0328
Epoch 136/300, seasonal_1 Loss: 0.0635 | 0.0328
Epoch 137/300, seasonal_1 Loss: 0.0635 | 0.0331
Epoch 138/300, seasonal_1 Loss: 0.0637 | 0.0336
Epoch 139/300, seasonal_1 Loss: 0.0640 | 0.0338
Epoch 140/300, seasonal_1 Loss: 0.0640 | 0.0335
Epoch 141/300, seasonal_1 Loss: 0.0637 | 0.0331
Epoch 142/300, seasonal_1 Loss: 0.0634 | 0.0327
Epoch 143/300, seasonal_1 Loss: 0.0632 | 0.0325
Epoch 144/300, seasonal_1 Loss: 0.0630 | 0.0322
Epoch 145/300, seasonal_1 Loss: 0.0629 | 0.0321
Epoch 146/300, seasonal_1 Loss: 0.0629 | 0.0322
Epoch 147/300, seasonal_1 Loss: 0.0628 | 0.0324
Epoch 148/300, seasonal_1 Loss: 0.0626 | 0.0324
Epoch 149/300, seasonal_1 Loss: 0.0625 | 0.0323
Epoch 150/300, seasonal_1 Loss: 0.0625 | 0.0323
Epoch 151/300, seasonal_1 Loss: 0.0624 | 0.0323
Epoch 152/300, seasonal_1 Loss: 0.0623 | 0.0323
Epoch 153/300, seasonal_1 Loss: 0.0623 | 0.0323
Epoch 154/300, seasonal_1 Loss: 0.0622 | 0.0323
Epoch 155/300, seasonal_1 Loss: 0.0622 | 0.0323
Epoch 156/300, seasonal_1 Loss: 0.0621 | 0.0322
Epoch 157/300, seasonal_1 Loss: 0.0619 | 0.0319
Epoch 158/300, seasonal_1 Loss: 0.0618 | 0.0317
Epoch 159/300, seasonal_1 Loss: 0.0617 | 0.0315
Epoch 160/300, seasonal_1 Loss: 0.0617 | 0.0314
Epoch 161/300, seasonal_1 Loss: 0.0618 | 0.0313
Epoch 162/300, seasonal_1 Loss: 0.0619 | 0.0313
Epoch 163/300, seasonal_1 Loss: 0.0620 | 0.0312
Epoch 164/300, seasonal_1 Loss: 0.0621 | 0.0311
Epoch 165/300, seasonal_1 Loss: 0.0619 | 0.0312
Epoch 166/300, seasonal_1 Loss: 0.0617 | 0.0316
Epoch 167/300, seasonal_1 Loss: 0.0618 | 0.0324
Epoch 168/300, seasonal_1 Loss: 0.0622 | 0.0324
Epoch 169/300, seasonal_1 Loss: 0.0622 | 0.0321
Epoch 170/300, seasonal_1 Loss: 0.0617 | 0.0313
Epoch 171/300, seasonal_1 Loss: 0.0613 | 0.0309
Epoch 172/300, seasonal_1 Loss: 0.0612 | 0.0308
Epoch 173/300, seasonal_1 Loss: 0.0612 | 0.0308
Epoch 174/300, seasonal_1 Loss: 0.0612 | 0.0308
Epoch 175/300, seasonal_1 Loss: 0.0610 | 0.0309
Epoch 176/300, seasonal_1 Loss: 0.0609 | 0.0311
Epoch 177/300, seasonal_1 Loss: 0.0608 | 0.0313
Epoch 178/300, seasonal_1 Loss: 0.0608 | 0.0313
Epoch 179/300, seasonal_1 Loss: 0.0608 | 0.0311
Epoch 180/300, seasonal_1 Loss: 0.0606 | 0.0309
Epoch 181/300, seasonal_1 Loss: 0.0605 | 0.0307
Epoch 182/300, seasonal_1 Loss: 0.0605 | 0.0306
Epoch 183/300, seasonal_1 Loss: 0.0604 | 0.0304
Epoch 184/300, seasonal_1 Loss: 0.0604 | 0.0304
Epoch 185/300, seasonal_1 Loss: 0.0604 | 0.0304
Epoch 186/300, seasonal_1 Loss: 0.0603 | 0.0305
Epoch 187/300, seasonal_1 Loss: 0.0602 | 0.0305
Epoch 188/300, seasonal_1 Loss: 0.0602 | 0.0306
Epoch 189/300, seasonal_1 Loss: 0.0602 | 0.0307
Epoch 190/300, seasonal_1 Loss: 0.0601 | 0.0308
Epoch 191/300, seasonal_1 Loss: 0.0601 | 0.0307
Epoch 192/300, seasonal_1 Loss: 0.0600 | 0.0306
Epoch 193/300, seasonal_1 Loss: 0.0599 | 0.0304
Epoch 194/300, seasonal_1 Loss: 0.0599 | 0.0302
Epoch 195/300, seasonal_1 Loss: 0.0599 | 0.0301
Epoch 196/300, seasonal_1 Loss: 0.0599 | 0.0300
Epoch 197/300, seasonal_1 Loss: 0.0599 | 0.0300
Epoch 198/300, seasonal_1 Loss: 0.0598 | 0.0300
Epoch 199/300, seasonal_1 Loss: 0.0597 | 0.0301
Epoch 200/300, seasonal_1 Loss: 0.0597 | 0.0303
Epoch 201/300, seasonal_1 Loss: 0.0597 | 0.0305
Epoch 202/300, seasonal_1 Loss: 0.0597 | 0.0307
Epoch 203/300, seasonal_1 Loss: 0.0597 | 0.0306
Epoch 204/300, seasonal_1 Loss: 0.0596 | 0.0303
Epoch 205/300, seasonal_1 Loss: 0.0595 | 0.0301
Epoch 206/300, seasonal_1 Loss: 0.0594 | 0.0299
Epoch 207/300, seasonal_1 Loss: 0.0595 | 0.0298
Epoch 208/300, seasonal_1 Loss: 0.0595 | 0.0297
Epoch 209/300, seasonal_1 Loss: 0.0595 | 0.0297
Epoch 210/300, seasonal_1 Loss: 0.0594 | 0.0299
Epoch 211/300, seasonal_1 Loss: 0.0593 | 0.0302
Epoch 212/300, seasonal_1 Loss: 0.0593 | 0.0305
Epoch 213/300, seasonal_1 Loss: 0.0593 | 0.0304
Epoch 214/300, seasonal_1 Loss: 0.0592 | 0.0302
Epoch 215/300, seasonal_1 Loss: 0.0591 | 0.0300
Epoch 216/300, seasonal_1 Loss: 0.0591 | 0.0298
Epoch 217/300, seasonal_1 Loss: 0.0591 | 0.0297
Epoch 218/300, seasonal_1 Loss: 0.0591 | 0.0296
Epoch 219/300, seasonal_1 Loss: 0.0590 | 0.0297
Epoch 220/300, seasonal_1 Loss: 0.0590 | 0.0298
Epoch 221/300, seasonal_1 Loss: 0.0589 | 0.0299
Epoch 222/300, seasonal_1 Loss: 0.0589 | 0.0301
Epoch 223/300, seasonal_1 Loss: 0.0588 | 0.0300
Epoch 224/300, seasonal_1 Loss: 0.0588 | 0.0299
Epoch 225/300, seasonal_1 Loss: 0.0588 | 0.0298
Epoch 226/300, seasonal_1 Loss: 0.0587 | 0.0297
Epoch 227/300, seasonal_1 Loss: 0.0587 | 0.0297
Epoch 228/300, seasonal_1 Loss: 0.0587 | 0.0297
Epoch 229/300, seasonal_1 Loss: 0.0587 | 0.0297
Epoch 230/300, seasonal_1 Loss: 0.0586 | 0.0298
Epoch 231/300, seasonal_1 Loss: 0.0586 | 0.0299
Epoch 232/300, seasonal_1 Loss: 0.0586 | 0.0299
Epoch 233/300, seasonal_1 Loss: 0.0585 | 0.0298
Epoch 234/300, seasonal_1 Loss: 0.0585 | 0.0298
Epoch 235/300, seasonal_1 Loss: 0.0585 | 0.0297
Epoch 236/300, seasonal_1 Loss: 0.0585 | 0.0297
Epoch 237/300, seasonal_1 Loss: 0.0584 | 0.0297
Epoch 238/300, seasonal_1 Loss: 0.0584 | 0.0297
Epoch 239/300, seasonal_1 Loss: 0.0584 | 0.0297
Epoch 240/300, seasonal_1 Loss: 0.0584 | 0.0297
Epoch 241/300, seasonal_1 Loss: 0.0583 | 0.0297
Epoch 242/300, seasonal_1 Loss: 0.0583 | 0.0297
Epoch 243/300, seasonal_1 Loss: 0.0583 | 0.0297
Epoch 244/300, seasonal_1 Loss: 0.0583 | 0.0297
Epoch 245/300, seasonal_1 Loss: 0.0582 | 0.0296
Epoch 246/300, seasonal_1 Loss: 0.0582 | 0.0296
Epoch 247/300, seasonal_1 Loss: 0.0582 | 0.0296
Epoch 248/300, seasonal_1 Loss: 0.0582 | 0.0296
Epoch 249/300, seasonal_1 Loss: 0.0581 | 0.0296
Epoch 250/300, seasonal_1 Loss: 0.0581 | 0.0296
Epoch 251/300, seasonal_1 Loss: 0.0581 | 0.0296
Epoch 252/300, seasonal_1 Loss: 0.0581 | 0.0296
Epoch 253/300, seasonal_1 Loss: 0.0581 | 0.0296
Epoch 254/300, seasonal_1 Loss: 0.0580 | 0.0296
Epoch 255/300, seasonal_1 Loss: 0.0580 | 0.0296
Epoch 256/300, seasonal_1 Loss: 0.0580 | 0.0296
Epoch 257/300, seasonal_1 Loss: 0.0580 | 0.0296
Epoch 258/300, seasonal_1 Loss: 0.0580 | 0.0296
Epoch 259/300, seasonal_1 Loss: 0.0579 | 0.0296
Epoch 260/300, seasonal_1 Loss: 0.0579 | 0.0295
Epoch 261/300, seasonal_1 Loss: 0.0579 | 0.0295
Epoch 262/300, seasonal_1 Loss: 0.0579 | 0.0295
Epoch 263/300, seasonal_1 Loss: 0.0579 | 0.0295
Epoch 264/300, seasonal_1 Loss: 0.0578 | 0.0295
Epoch 265/300, seasonal_1 Loss: 0.0578 | 0.0295
Epoch 266/300, seasonal_1 Loss: 0.0578 | 0.0295
Epoch 267/300, seasonal_1 Loss: 0.0578 | 0.0295
Epoch 268/300, seasonal_1 Loss: 0.0578 | 0.0295
Epoch 269/300, seasonal_1 Loss: 0.0578 | 0.0295
Epoch 270/300, seasonal_1 Loss: 0.0577 | 0.0295
Epoch 271/300, seasonal_1 Loss: 0.0577 | 0.0295
Epoch 272/300, seasonal_1 Loss: 0.0577 | 0.0295
Epoch 273/300, seasonal_1 Loss: 0.0577 | 0.0295
Epoch 274/300, seasonal_1 Loss: 0.0577 | 0.0295
Epoch 275/300, seasonal_1 Loss: 0.0577 | 0.0295
Epoch 276/300, seasonal_1 Loss: 0.0576 | 0.0295
Epoch 277/300, seasonal_1 Loss: 0.0576 | 0.0295
Epoch 278/300, seasonal_1 Loss: 0.0576 | 0.0294
Epoch 279/300, seasonal_1 Loss: 0.0576 | 0.0294
Epoch 280/300, seasonal_1 Loss: 0.0576 | 0.0294
Epoch 281/300, seasonal_1 Loss: 0.0576 | 0.0294
Epoch 282/300, seasonal_1 Loss: 0.0576 | 0.0294
Epoch 283/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 284/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 285/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 286/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 287/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 288/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 289/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 290/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 291/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 292/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 293/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 294/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 295/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 296/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 297/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 298/300, seasonal_1 Loss: 0.0574 | 0.0294
Epoch 299/300, seasonal_1 Loss: 0.0573 | 0.0293
Epoch 300/300, seasonal_1 Loss: 0.0573 | 0.0294
Training seasonal_2 component with params: {'observation_period_num': 8, 'train_rates': 0.8978184414630223, 'learning_rate': 0.0009796067374673687, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9548434444079609}
Epoch 1/300, seasonal_2 Loss: 0.7316 | 0.2765
Epoch 2/300, seasonal_2 Loss: 0.2240 | 0.2150
Epoch 3/300, seasonal_2 Loss: 0.2101 | 0.3875
Epoch 4/300, seasonal_2 Loss: 0.1838 | 0.1956
Epoch 5/300, seasonal_2 Loss: 0.1372 | 0.1173
Epoch 6/300, seasonal_2 Loss: 0.1289 | 0.0935
Epoch 7/300, seasonal_2 Loss: 0.1218 | 0.1039
Epoch 8/300, seasonal_2 Loss: 0.1208 | 0.1028
Epoch 9/300, seasonal_2 Loss: 0.1127 | 0.1242
Epoch 10/300, seasonal_2 Loss: 0.1088 | 0.1033
Epoch 11/300, seasonal_2 Loss: 0.1035 | 0.0775
Epoch 12/300, seasonal_2 Loss: 0.1267 | 0.1025
Epoch 13/300, seasonal_2 Loss: 0.1228 | 0.0731
Epoch 14/300, seasonal_2 Loss: 0.1060 | 0.0879
Epoch 15/300, seasonal_2 Loss: 0.1067 | 0.1081
Epoch 16/300, seasonal_2 Loss: 0.1031 | 0.1062
Epoch 17/300, seasonal_2 Loss: 0.0967 | 0.0651
Epoch 18/300, seasonal_2 Loss: 0.1173 | 0.0813
Epoch 19/300, seasonal_2 Loss: 0.1231 | 0.1004
Epoch 20/300, seasonal_2 Loss: 0.1169 | 0.0925
Epoch 21/300, seasonal_2 Loss: 0.1128 | 0.0686
Epoch 22/300, seasonal_2 Loss: 0.1075 | 0.0705
Epoch 23/300, seasonal_2 Loss: 0.1288 | 0.1123
Epoch 24/300, seasonal_2 Loss: 0.1289 | 0.1508
Epoch 25/300, seasonal_2 Loss: 0.1234 | 0.1053
Epoch 26/300, seasonal_2 Loss: 0.1040 | 0.0673
Epoch 27/300, seasonal_2 Loss: 0.0954 | 0.0576
Epoch 28/300, seasonal_2 Loss: 0.0919 | 0.0626
Epoch 29/300, seasonal_2 Loss: 0.0865 | 0.0525
Epoch 30/300, seasonal_2 Loss: 0.0854 | 0.0527
Epoch 31/300, seasonal_2 Loss: 0.0817 | 0.0558
Epoch 32/300, seasonal_2 Loss: 0.0823 | 0.0556
Epoch 33/300, seasonal_2 Loss: 0.0836 | 0.0527
Epoch 34/300, seasonal_2 Loss: 0.0819 | 0.0468
Epoch 35/300, seasonal_2 Loss: 0.0805 | 0.0480
Epoch 36/300, seasonal_2 Loss: 0.0774 | 0.0454
Epoch 37/300, seasonal_2 Loss: 0.0774 | 0.0429
Epoch 38/300, seasonal_2 Loss: 0.0777 | 0.0426
Epoch 39/300, seasonal_2 Loss: 0.0769 | 0.0499
Epoch 40/300, seasonal_2 Loss: 0.0775 | 0.0505
Epoch 41/300, seasonal_2 Loss: 0.0766 | 0.0403
Epoch 42/300, seasonal_2 Loss: 0.0756 | 0.0399
Epoch 43/300, seasonal_2 Loss: 0.0748 | 0.0459
Epoch 44/300, seasonal_2 Loss: 0.0750 | 0.0503
Epoch 45/300, seasonal_2 Loss: 0.0749 | 0.0409
Epoch 46/300, seasonal_2 Loss: 0.0737 | 0.0386
Epoch 47/300, seasonal_2 Loss: 0.0724 | 0.0390
Epoch 48/300, seasonal_2 Loss: 0.0725 | 0.0375
Epoch 49/300, seasonal_2 Loss: 0.0715 | 0.0374
Epoch 50/300, seasonal_2 Loss: 0.0711 | 0.0388
Epoch 51/300, seasonal_2 Loss: 0.0712 | 0.0396
Epoch 52/300, seasonal_2 Loss: 0.0704 | 0.0351
Epoch 53/300, seasonal_2 Loss: 0.0700 | 0.0352
Epoch 54/300, seasonal_2 Loss: 0.0696 | 0.0348
Epoch 55/300, seasonal_2 Loss: 0.0692 | 0.0390
Epoch 56/300, seasonal_2 Loss: 0.0689 | 0.0403
Epoch 57/300, seasonal_2 Loss: 0.0689 | 0.0356
Epoch 58/300, seasonal_2 Loss: 0.0692 | 0.0356
Epoch 59/300, seasonal_2 Loss: 0.0693 | 0.0352
Epoch 60/300, seasonal_2 Loss: 0.0707 | 0.0414
Epoch 61/300, seasonal_2 Loss: 0.0722 | 0.0463
Epoch 62/300, seasonal_2 Loss: 0.0721 | 0.0361
Epoch 63/300, seasonal_2 Loss: 0.0712 | 0.0397
Epoch 64/300, seasonal_2 Loss: 0.0805 | 0.0502
Epoch 65/300, seasonal_2 Loss: 0.0762 | 0.0384
Epoch 66/300, seasonal_2 Loss: 0.0743 | 0.0402
Epoch 67/300, seasonal_2 Loss: 0.0734 | 0.0432
Epoch 68/300, seasonal_2 Loss: 0.0692 | 0.0365
Epoch 69/300, seasonal_2 Loss: 0.0686 | 0.0368
Epoch 70/300, seasonal_2 Loss: 0.0690 | 0.0348
Epoch 71/300, seasonal_2 Loss: 0.0683 | 0.0388
Epoch 72/300, seasonal_2 Loss: 0.0694 | 0.0395
Epoch 73/300, seasonal_2 Loss: 0.0665 | 0.0340
Epoch 74/300, seasonal_2 Loss: 0.0674 | 0.0329
Epoch 75/300, seasonal_2 Loss: 0.0678 | 0.0431
Epoch 76/300, seasonal_2 Loss: 0.0682 | 0.0411
Epoch 77/300, seasonal_2 Loss: 0.0691 | 0.0325
Epoch 78/300, seasonal_2 Loss: 0.0689 | 0.0353
Epoch 79/300, seasonal_2 Loss: 0.0689 | 0.0346
Epoch 80/300, seasonal_2 Loss: 0.0676 | 0.0341
Epoch 81/300, seasonal_2 Loss: 0.0666 | 0.0324
Epoch 82/300, seasonal_2 Loss: 0.0666 | 0.0320
Epoch 83/300, seasonal_2 Loss: 0.0668 | 0.0458
Epoch 84/300, seasonal_2 Loss: 0.0681 | 0.0387
Epoch 85/300, seasonal_2 Loss: 0.0697 | 0.0352
Epoch 86/300, seasonal_2 Loss: 0.0710 | 0.0365
Epoch 87/300, seasonal_2 Loss: 0.0727 | 0.0390
Epoch 88/300, seasonal_2 Loss: 0.0701 | 0.0384
Epoch 89/300, seasonal_2 Loss: 0.0690 | 0.0374
Epoch 90/300, seasonal_2 Loss: 0.0672 | 0.0366
Epoch 91/300, seasonal_2 Loss: 0.0727 | 0.0400
Epoch 92/300, seasonal_2 Loss: 0.0694 | 0.0367
Epoch 93/300, seasonal_2 Loss: 0.0689 | 0.0498
Epoch 94/300, seasonal_2 Loss: 0.0712 | 0.0361
Epoch 95/300, seasonal_2 Loss: 0.0684 | 0.0325
Epoch 96/300, seasonal_2 Loss: 0.0655 | 0.0308
Epoch 97/300, seasonal_2 Loss: 0.0648 | 0.0331
Epoch 98/300, seasonal_2 Loss: 0.0626 | 0.0340
Epoch 99/300, seasonal_2 Loss: 0.0630 | 0.0305
Epoch 100/300, seasonal_2 Loss: 0.0624 | 0.0295
Epoch 101/300, seasonal_2 Loss: 0.0617 | 0.0325
Epoch 102/300, seasonal_2 Loss: 0.0616 | 0.0306
Epoch 103/300, seasonal_2 Loss: 0.0616 | 0.0292
Epoch 104/300, seasonal_2 Loss: 0.0607 | 0.0299
Epoch 105/300, seasonal_2 Loss: 0.0601 | 0.0319
Epoch 106/300, seasonal_2 Loss: 0.0598 | 0.0284
Epoch 107/300, seasonal_2 Loss: 0.0592 | 0.0276
Epoch 108/300, seasonal_2 Loss: 0.0589 | 0.0276
Epoch 109/300, seasonal_2 Loss: 0.0586 | 0.0300
Epoch 110/300, seasonal_2 Loss: 0.0585 | 0.0281
Epoch 111/300, seasonal_2 Loss: 0.0585 | 0.0275
Epoch 112/300, seasonal_2 Loss: 0.0586 | 0.0279
Epoch 113/300, seasonal_2 Loss: 0.0593 | 0.0291
Epoch 114/300, seasonal_2 Loss: 0.0607 | 0.0290
Epoch 115/300, seasonal_2 Loss: 0.0602 | 0.0295
Epoch 116/300, seasonal_2 Loss: 0.0609 | 0.0265
Epoch 117/300, seasonal_2 Loss: 0.0598 | 0.0271
Epoch 118/300, seasonal_2 Loss: 0.0597 | 0.0260
Epoch 119/300, seasonal_2 Loss: 0.0593 | 0.0278
Epoch 120/300, seasonal_2 Loss: 0.0580 | 0.0276
Epoch 121/300, seasonal_2 Loss: 0.0581 | 0.0257
Epoch 122/300, seasonal_2 Loss: 0.0580 | 0.0279
Epoch 123/300, seasonal_2 Loss: 0.0619 | 0.0307
Epoch 124/300, seasonal_2 Loss: 0.0621 | 0.0282
Epoch 125/300, seasonal_2 Loss: 0.0590 | 0.0267
Epoch 126/300, seasonal_2 Loss: 0.0600 | 0.0274
Epoch 127/300, seasonal_2 Loss: 0.0578 | 0.0307
Epoch 128/300, seasonal_2 Loss: 0.0573 | 0.0249
Epoch 129/300, seasonal_2 Loss: 0.0565 | 0.0251
Epoch 130/300, seasonal_2 Loss: 0.0571 | 0.0263
Epoch 131/300, seasonal_2 Loss: 0.0582 | 0.0266
Epoch 132/300, seasonal_2 Loss: 0.0586 | 0.0288
Epoch 133/300, seasonal_2 Loss: 0.0568 | 0.0256
Epoch 134/300, seasonal_2 Loss: 0.0565 | 0.0252
Epoch 135/300, seasonal_2 Loss: 0.0580 | 0.0266
Epoch 136/300, seasonal_2 Loss: 0.0600 | 0.0284
Epoch 137/300, seasonal_2 Loss: 0.0569 | 0.0261
Epoch 138/300, seasonal_2 Loss: 0.0566 | 0.0249
Epoch 139/300, seasonal_2 Loss: 0.0560 | 0.0255
Epoch 140/300, seasonal_2 Loss: 0.0555 | 0.0266
Epoch 141/300, seasonal_2 Loss: 0.0557 | 0.0258
Epoch 142/300, seasonal_2 Loss: 0.0551 | 0.0241
Epoch 143/300, seasonal_2 Loss: 0.0543 | 0.0249
Epoch 144/300, seasonal_2 Loss: 0.0543 | 0.0258
Epoch 145/300, seasonal_2 Loss: 0.0538 | 0.0243
Epoch 146/300, seasonal_2 Loss: 0.0536 | 0.0237
Epoch 147/300, seasonal_2 Loss: 0.0536 | 0.0248
Epoch 148/300, seasonal_2 Loss: 0.0534 | 0.0252
Epoch 149/300, seasonal_2 Loss: 0.0533 | 0.0241
Epoch 150/300, seasonal_2 Loss: 0.0532 | 0.0238
Epoch 151/300, seasonal_2 Loss: 0.0532 | 0.0244
Epoch 152/300, seasonal_2 Loss: 0.0531 | 0.0243
Epoch 153/300, seasonal_2 Loss: 0.0529 | 0.0238
Epoch 154/300, seasonal_2 Loss: 0.0528 | 0.0239
Epoch 155/300, seasonal_2 Loss: 0.0527 | 0.0243
Epoch 156/300, seasonal_2 Loss: 0.0526 | 0.0242
Epoch 157/300, seasonal_2 Loss: 0.0526 | 0.0239
Epoch 158/300, seasonal_2 Loss: 0.0527 | 0.0241
Epoch 159/300, seasonal_2 Loss: 0.0530 | 0.0244
Epoch 160/300, seasonal_2 Loss: 0.0536 | 0.0253
Epoch 161/300, seasonal_2 Loss: 0.0546 | 0.0260
Epoch 162/300, seasonal_2 Loss: 0.0546 | 0.0254
Epoch 163/300, seasonal_2 Loss: 0.0533 | 0.0254
Epoch 164/300, seasonal_2 Loss: 0.0533 | 0.0254
Epoch 165/300, seasonal_2 Loss: 0.0532 | 0.0246
Epoch 166/300, seasonal_2 Loss: 0.0531 | 0.0242
Epoch 167/300, seasonal_2 Loss: 0.0526 | 0.0242
Epoch 168/300, seasonal_2 Loss: 0.0525 | 0.0262
Epoch 169/300, seasonal_2 Loss: 0.0526 | 0.0246
Epoch 170/300, seasonal_2 Loss: 0.0526 | 0.0241
Epoch 171/300, seasonal_2 Loss: 0.0528 | 0.0252
Epoch 172/300, seasonal_2 Loss: 0.0530 | 0.0257
Epoch 173/300, seasonal_2 Loss: 0.0526 | 0.0249
Epoch 174/300, seasonal_2 Loss: 0.0520 | 0.0250
Epoch 175/300, seasonal_2 Loss: 0.0521 | 0.0251
Epoch 176/300, seasonal_2 Loss: 0.0521 | 0.0246
Epoch 177/300, seasonal_2 Loss: 0.0518 | 0.0242
Epoch 178/300, seasonal_2 Loss: 0.0515 | 0.0252
Epoch 179/300, seasonal_2 Loss: 0.0516 | 0.0255
Epoch 180/300, seasonal_2 Loss: 0.0517 | 0.0244
Epoch 181/300, seasonal_2 Loss: 0.0518 | 0.0244
Epoch 182/300, seasonal_2 Loss: 0.0519 | 0.0258
Epoch 183/300, seasonal_2 Loss: 0.0517 | 0.0258
Epoch 184/300, seasonal_2 Loss: 0.0514 | 0.0256
Epoch 185/300, seasonal_2 Loss: 0.0515 | 0.0249
Epoch 186/300, seasonal_2 Loss: 0.0514 | 0.0248
Epoch 187/300, seasonal_2 Loss: 0.0511 | 0.0246
Epoch 188/300, seasonal_2 Loss: 0.0509 | 0.0249
Epoch 189/300, seasonal_2 Loss: 0.0510 | 0.0253
Epoch 190/300, seasonal_2 Loss: 0.0511 | 0.0255
Epoch 191/300, seasonal_2 Loss: 0.0511 | 0.0250
Epoch 192/300, seasonal_2 Loss: 0.0509 | 0.0253
Epoch 193/300, seasonal_2 Loss: 0.0510 | 0.0257
Epoch 194/300, seasonal_2 Loss: 0.0513 | 0.0262
Epoch 195/300, seasonal_2 Loss: 0.0512 | 0.0252
Epoch 196/300, seasonal_2 Loss: 0.0508 | 0.0245
Epoch 197/300, seasonal_2 Loss: 0.0507 | 0.0244
Epoch 198/300, seasonal_2 Loss: 0.0509 | 0.0257
Epoch 199/300, seasonal_2 Loss: 0.0512 | 0.0276
Epoch 200/300, seasonal_2 Loss: 0.0511 | 0.0265
Epoch 201/300, seasonal_2 Loss: 0.0508 | 0.0253
Epoch 202/300, seasonal_2 Loss: 0.0509 | 0.0251
Epoch 203/300, seasonal_2 Loss: 0.0509 | 0.0256
Epoch 204/300, seasonal_2 Loss: 0.0505 | 0.0258
Epoch 205/300, seasonal_2 Loss: 0.0504 | 0.0255
Epoch 206/300, seasonal_2 Loss: 0.0504 | 0.0258
Epoch 207/300, seasonal_2 Loss: 0.0503 | 0.0263
Epoch 208/300, seasonal_2 Loss: 0.0501 | 0.0258
Epoch 209/300, seasonal_2 Loss: 0.0502 | 0.0258
Epoch 210/300, seasonal_2 Loss: 0.0505 | 0.0255
Epoch 211/300, seasonal_2 Loss: 0.0504 | 0.0265
Epoch 212/300, seasonal_2 Loss: 0.0503 | 0.0270
Epoch 213/300, seasonal_2 Loss: 0.0503 | 0.0251
Epoch 214/300, seasonal_2 Loss: 0.0503 | 0.0253
Epoch 215/300, seasonal_2 Loss: 0.0504 | 0.0265
Epoch 216/300, seasonal_2 Loss: 0.0503 | 0.0281
Epoch 217/300, seasonal_2 Loss: 0.0502 | 0.0260
Epoch 218/300, seasonal_2 Loss: 0.0499 | 0.0252
Epoch 219/300, seasonal_2 Loss: 0.0498 | 0.0255
Epoch 220/300, seasonal_2 Loss: 0.0497 | 0.0264
Epoch 221/300, seasonal_2 Loss: 0.0498 | 0.0269
Epoch 222/300, seasonal_2 Loss: 0.0497 | 0.0261
Epoch 223/300, seasonal_2 Loss: 0.0496 | 0.0261
Epoch 224/300, seasonal_2 Loss: 0.0497 | 0.0260
Epoch 225/300, seasonal_2 Loss: 0.0496 | 0.0263
Epoch 226/300, seasonal_2 Loss: 0.0494 | 0.0257
Epoch 227/300, seasonal_2 Loss: 0.0493 | 0.0258
Epoch 228/300, seasonal_2 Loss: 0.0494 | 0.0264
Epoch 229/300, seasonal_2 Loss: 0.0494 | 0.0272
Epoch 230/300, seasonal_2 Loss: 0.0493 | 0.0267
Epoch 231/300, seasonal_2 Loss: 0.0493 | 0.0264
Epoch 232/300, seasonal_2 Loss: 0.0495 | 0.0261
Epoch 233/300, seasonal_2 Loss: 0.0494 | 0.0272
Epoch 234/300, seasonal_2 Loss: 0.0492 | 0.0264
Epoch 235/300, seasonal_2 Loss: 0.0493 | 0.0262
Epoch 236/300, seasonal_2 Loss: 0.0494 | 0.0266
Epoch 237/300, seasonal_2 Loss: 0.0493 | 0.0286
Epoch 238/300, seasonal_2 Loss: 0.0493 | 0.0275
Epoch 239/300, seasonal_2 Loss: 0.0494 | 0.0264
Epoch 240/300, seasonal_2 Loss: 0.0493 | 0.0261
Epoch 241/300, seasonal_2 Loss: 0.0491 | 0.0279
Epoch 242/300, seasonal_2 Loss: 0.0492 | 0.0270
Epoch 243/300, seasonal_2 Loss: 0.0494 | 0.0272
Epoch 244/300, seasonal_2 Loss: 0.0491 | 0.0281
Epoch 245/300, seasonal_2 Loss: 0.0491 | 0.0281
Epoch 246/300, seasonal_2 Loss: 0.0491 | 0.0265
Epoch 247/300, seasonal_2 Loss: 0.0490 | 0.0263
Epoch 248/300, seasonal_2 Loss: 0.0489 | 0.0283
Epoch 249/300, seasonal_2 Loss: 0.0491 | 0.0279
Epoch 250/300, seasonal_2 Loss: 0.0491 | 0.0278
Epoch 251/300, seasonal_2 Loss: 0.0489 | 0.0277
Epoch 252/300, seasonal_2 Loss: 0.0490 | 0.0284
Epoch 253/300, seasonal_2 Loss: 0.0489 | 0.0266
Epoch 254/300, seasonal_2 Loss: 0.0486 | 0.0266
Epoch 255/300, seasonal_2 Loss: 0.0487 | 0.0293
Epoch 256/300, seasonal_2 Loss: 0.0488 | 0.0274
Epoch 257/300, seasonal_2 Loss: 0.0487 | 0.0279
Epoch 258/300, seasonal_2 Loss: 0.0488 | 0.0285
Epoch 259/300, seasonal_2 Loss: 0.0487 | 0.0272
Epoch 260/300, seasonal_2 Loss: 0.0485 | 0.0269
Epoch 261/300, seasonal_2 Loss: 0.0484 | 0.0274
Epoch 262/300, seasonal_2 Loss: 0.0484 | 0.0286
Epoch 263/300, seasonal_2 Loss: 0.0484 | 0.0278
Epoch 264/300, seasonal_2 Loss: 0.0483 | 0.0276
Epoch 265/300, seasonal_2 Loss: 0.0484 | 0.0283
Epoch 266/300, seasonal_2 Loss: 0.0483 | 0.0268
Epoch 267/300, seasonal_2 Loss: 0.0481 | 0.0271
Epoch 268/300, seasonal_2 Loss: 0.0482 | 0.0291
Epoch 269/300, seasonal_2 Loss: 0.0482 | 0.0279
Epoch 270/300, seasonal_2 Loss: 0.0481 | 0.0277
Epoch 271/300, seasonal_2 Loss: 0.0482 | 0.0283
Epoch 272/300, seasonal_2 Loss: 0.0481 | 0.0271
Epoch 273/300, seasonal_2 Loss: 0.0479 | 0.0274
Epoch 274/300, seasonal_2 Loss: 0.0480 | 0.0287
Epoch 275/300, seasonal_2 Loss: 0.0480 | 0.0284
Epoch 276/300, seasonal_2 Loss: 0.0479 | 0.0278
Epoch 277/300, seasonal_2 Loss: 0.0480 | 0.0278
Epoch 278/300, seasonal_2 Loss: 0.0478 | 0.0276
Epoch 279/300, seasonal_2 Loss: 0.0477 | 0.0276
Epoch 280/300, seasonal_2 Loss: 0.0478 | 0.0285
Epoch 281/300, seasonal_2 Loss: 0.0477 | 0.0289
Epoch 282/300, seasonal_2 Loss: 0.0477 | 0.0277
Epoch 283/300, seasonal_2 Loss: 0.0478 | 0.0275
Epoch 284/300, seasonal_2 Loss: 0.0476 | 0.0280
Epoch 285/300, seasonal_2 Loss: 0.0476 | 0.0280
Epoch 286/300, seasonal_2 Loss: 0.0476 | 0.0286
Epoch 287/300, seasonal_2 Loss: 0.0475 | 0.0291
Epoch 288/300, seasonal_2 Loss: 0.0476 | 0.0276
Epoch 289/300, seasonal_2 Loss: 0.0475 | 0.0275
Epoch 290/300, seasonal_2 Loss: 0.0474 | 0.0286
Epoch 291/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 292/300, seasonal_2 Loss: 0.0474 | 0.0287
Epoch 293/300, seasonal_2 Loss: 0.0474 | 0.0286
Epoch 294/300, seasonal_2 Loss: 0.0474 | 0.0276
Epoch 295/300, seasonal_2 Loss: 0.0473 | 0.0279
Epoch 296/300, seasonal_2 Loss: 0.0473 | 0.0289
Epoch 297/300, seasonal_2 Loss: 0.0473 | 0.0290
Epoch 298/300, seasonal_2 Loss: 0.0473 | 0.0283
Epoch 299/300, seasonal_2 Loss: 0.0473 | 0.0280
Epoch 300/300, seasonal_2 Loss: 0.0471 | 0.0281
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.8543893178343543, 'learning_rate': 0.000322376432011282, 'batch_size': 100, 'step_size': 11, 'gamma': 0.8051384929254203}
Epoch 1/300, seasonal_3 Loss: 0.2985 | 0.2250
Epoch 2/300, seasonal_3 Loss: 0.1849 | 0.1312
Epoch 3/300, seasonal_3 Loss: 0.1441 | 0.1203
Epoch 4/300, seasonal_3 Loss: 0.1400 | 0.1067
Epoch 5/300, seasonal_3 Loss: 0.1475 | 0.1429
Epoch 6/300, seasonal_3 Loss: 0.1386 | 0.1904
Epoch 7/300, seasonal_3 Loss: 0.1202 | 0.2201
Epoch 8/300, seasonal_3 Loss: 0.1321 | 0.1495
Epoch 9/300, seasonal_3 Loss: 0.1571 | 0.1414
Epoch 10/300, seasonal_3 Loss: 0.1260 | 0.0835
Epoch 11/300, seasonal_3 Loss: 0.1143 | 0.0809
Epoch 12/300, seasonal_3 Loss: 0.1187 | 0.0745
Epoch 13/300, seasonal_3 Loss: 0.1134 | 0.0734
Epoch 14/300, seasonal_3 Loss: 0.1176 | 0.1034
Epoch 15/300, seasonal_3 Loss: 0.1112 | 0.0775
Epoch 16/300, seasonal_3 Loss: 0.1037 | 0.0666
Epoch 17/300, seasonal_3 Loss: 0.1000 | 0.0635
Epoch 18/300, seasonal_3 Loss: 0.0978 | 0.0635
Epoch 19/300, seasonal_3 Loss: 0.0979 | 0.0609
Epoch 20/300, seasonal_3 Loss: 0.0946 | 0.0601
Epoch 21/300, seasonal_3 Loss: 0.0948 | 0.0593
Epoch 22/300, seasonal_3 Loss: 0.0936 | 0.0557
Epoch 23/300, seasonal_3 Loss: 0.0901 | 0.0543
Epoch 24/300, seasonal_3 Loss: 0.0890 | 0.0541
Epoch 25/300, seasonal_3 Loss: 0.0886 | 0.0537
Epoch 26/300, seasonal_3 Loss: 0.0879 | 0.0531
Epoch 27/300, seasonal_3 Loss: 0.0870 | 0.0523
Epoch 28/300, seasonal_3 Loss: 0.0865 | 0.0515
Epoch 29/300, seasonal_3 Loss: 0.0867 | 0.0530
Epoch 30/300, seasonal_3 Loss: 0.0869 | 0.0538
Epoch 31/300, seasonal_3 Loss: 0.0854 | 0.0541
Epoch 32/300, seasonal_3 Loss: 0.0856 | 0.0527
Epoch 33/300, seasonal_3 Loss: 0.0863 | 0.0505
Epoch 34/300, seasonal_3 Loss: 0.0839 | 0.0481
Epoch 35/300, seasonal_3 Loss: 0.0844 | 0.0620
Epoch 36/300, seasonal_3 Loss: 0.0869 | 0.0682
Epoch 37/300, seasonal_3 Loss: 0.0874 | 0.0554
Epoch 38/300, seasonal_3 Loss: 0.0825 | 0.0507
Epoch 39/300, seasonal_3 Loss: 0.0808 | 0.0470
Epoch 40/300, seasonal_3 Loss: 0.0802 | 0.0446
Epoch 41/300, seasonal_3 Loss: 0.0793 | 0.0431
Epoch 42/300, seasonal_3 Loss: 0.0790 | 0.0426
Epoch 43/300, seasonal_3 Loss: 0.0790 | 0.0429
Epoch 44/300, seasonal_3 Loss: 0.0786 | 0.0438
Epoch 45/300, seasonal_3 Loss: 0.0780 | 0.0448
Epoch 46/300, seasonal_3 Loss: 0.0777 | 0.0426
Epoch 47/300, seasonal_3 Loss: 0.0775 | 0.0418
Epoch 48/300, seasonal_3 Loss: 0.0777 | 0.0418
Epoch 49/300, seasonal_3 Loss: 0.0775 | 0.0416
Epoch 50/300, seasonal_3 Loss: 0.0771 | 0.0412
Epoch 51/300, seasonal_3 Loss: 0.0767 | 0.0410
Epoch 52/300, seasonal_3 Loss: 0.0762 | 0.0407
Epoch 53/300, seasonal_3 Loss: 0.0757 | 0.0404
Epoch 54/300, seasonal_3 Loss: 0.0753 | 0.0401
Epoch 55/300, seasonal_3 Loss: 0.0751 | 0.0399
Epoch 56/300, seasonal_3 Loss: 0.0749 | 0.0396
Epoch 57/300, seasonal_3 Loss: 0.0749 | 0.0392
Epoch 58/300, seasonal_3 Loss: 0.0747 | 0.0390
Epoch 59/300, seasonal_3 Loss: 0.0746 | 0.0389
Epoch 60/300, seasonal_3 Loss: 0.0744 | 0.0388
Epoch 61/300, seasonal_3 Loss: 0.0742 | 0.0388
Epoch 62/300, seasonal_3 Loss: 0.0742 | 0.0390
Epoch 63/300, seasonal_3 Loss: 0.0743 | 0.0390
Epoch 64/300, seasonal_3 Loss: 0.0739 | 0.0389
Epoch 65/300, seasonal_3 Loss: 0.0737 | 0.0389
Epoch 66/300, seasonal_3 Loss: 0.0735 | 0.0388
Epoch 67/300, seasonal_3 Loss: 0.0734 | 0.0393
Epoch 68/300, seasonal_3 Loss: 0.0734 | 0.0391
Epoch 69/300, seasonal_3 Loss: 0.0731 | 0.0388
Epoch 70/300, seasonal_3 Loss: 0.0727 | 0.0384
Epoch 71/300, seasonal_3 Loss: 0.0723 | 0.0382
Epoch 72/300, seasonal_3 Loss: 0.0722 | 0.0380
Epoch 73/300, seasonal_3 Loss: 0.0722 | 0.0380
Epoch 74/300, seasonal_3 Loss: 0.0723 | 0.0381
Epoch 75/300, seasonal_3 Loss: 0.0724 | 0.0382
Epoch 76/300, seasonal_3 Loss: 0.0722 | 0.0382
Epoch 77/300, seasonal_3 Loss: 0.0719 | 0.0381
Epoch 78/300, seasonal_3 Loss: 0.0717 | 0.0381
Epoch 79/300, seasonal_3 Loss: 0.0714 | 0.0380
Epoch 80/300, seasonal_3 Loss: 0.0713 | 0.0379
Epoch 81/300, seasonal_3 Loss: 0.0712 | 0.0378
Epoch 82/300, seasonal_3 Loss: 0.0711 | 0.0377
Epoch 83/300, seasonal_3 Loss: 0.0710 | 0.0376
Epoch 84/300, seasonal_3 Loss: 0.0710 | 0.0376
Epoch 85/300, seasonal_3 Loss: 0.0709 | 0.0376
Epoch 86/300, seasonal_3 Loss: 0.0709 | 0.0375
Epoch 87/300, seasonal_3 Loss: 0.0708 | 0.0375
Epoch 88/300, seasonal_3 Loss: 0.0707 | 0.0375
Epoch 89/300, seasonal_3 Loss: 0.0707 | 0.0375
Epoch 90/300, seasonal_3 Loss: 0.0706 | 0.0374
Epoch 91/300, seasonal_3 Loss: 0.0706 | 0.0374
Epoch 92/300, seasonal_3 Loss: 0.0705 | 0.0374
Epoch 93/300, seasonal_3 Loss: 0.0705 | 0.0374
Epoch 94/300, seasonal_3 Loss: 0.0704 | 0.0373
Epoch 95/300, seasonal_3 Loss: 0.0704 | 0.0373
Epoch 96/300, seasonal_3 Loss: 0.0704 | 0.0373
Epoch 97/300, seasonal_3 Loss: 0.0703 | 0.0373
Epoch 98/300, seasonal_3 Loss: 0.0703 | 0.0373
Epoch 99/300, seasonal_3 Loss: 0.0702 | 0.0373
Epoch 100/300, seasonal_3 Loss: 0.0702 | 0.0372
Epoch 101/300, seasonal_3 Loss: 0.0702 | 0.0372
Epoch 102/300, seasonal_3 Loss: 0.0701 | 0.0372
Epoch 103/300, seasonal_3 Loss: 0.0701 | 0.0372
Epoch 104/300, seasonal_3 Loss: 0.0701 | 0.0372
Epoch 105/300, seasonal_3 Loss: 0.0700 | 0.0372
Epoch 106/300, seasonal_3 Loss: 0.0700 | 0.0371
Epoch 107/300, seasonal_3 Loss: 0.0700 | 0.0371
Epoch 108/300, seasonal_3 Loss: 0.0700 | 0.0371
Epoch 109/300, seasonal_3 Loss: 0.0699 | 0.0371
Epoch 110/300, seasonal_3 Loss: 0.0699 | 0.0371
Epoch 111/300, seasonal_3 Loss: 0.0699 | 0.0371
Epoch 112/300, seasonal_3 Loss: 0.0698 | 0.0371
Epoch 113/300, seasonal_3 Loss: 0.0698 | 0.0371
Epoch 114/300, seasonal_3 Loss: 0.0698 | 0.0370
Epoch 115/300, seasonal_3 Loss: 0.0698 | 0.0370
Epoch 116/300, seasonal_3 Loss: 0.0698 | 0.0370
Epoch 117/300, seasonal_3 Loss: 0.0697 | 0.0370
Epoch 118/300, seasonal_3 Loss: 0.0697 | 0.0370
Epoch 119/300, seasonal_3 Loss: 0.0697 | 0.0370
Epoch 120/300, seasonal_3 Loss: 0.0697 | 0.0370
Epoch 121/300, seasonal_3 Loss: 0.0696 | 0.0370
Epoch 122/300, seasonal_3 Loss: 0.0696 | 0.0370
Epoch 123/300, seasonal_3 Loss: 0.0696 | 0.0370
Epoch 124/300, seasonal_3 Loss: 0.0696 | 0.0370
Epoch 125/300, seasonal_3 Loss: 0.0696 | 0.0369
Epoch 126/300, seasonal_3 Loss: 0.0696 | 0.0369
Epoch 127/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 128/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 129/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 130/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 131/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 132/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 133/300, seasonal_3 Loss: 0.0695 | 0.0369
Epoch 134/300, seasonal_3 Loss: 0.0694 | 0.0369
Epoch 135/300, seasonal_3 Loss: 0.0694 | 0.0369
Epoch 136/300, seasonal_3 Loss: 0.0694 | 0.0369
Epoch 137/300, seasonal_3 Loss: 0.0694 | 0.0369
Epoch 138/300, seasonal_3 Loss: 0.0694 | 0.0369
Epoch 139/300, seasonal_3 Loss: 0.0694 | 0.0368
Epoch 140/300, seasonal_3 Loss: 0.0694 | 0.0368
Epoch 141/300, seasonal_3 Loss: 0.0694 | 0.0368
Epoch 142/300, seasonal_3 Loss: 0.0694 | 0.0368
Epoch 143/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 144/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 145/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 146/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 147/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 148/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 149/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 150/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 151/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 152/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 153/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 154/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 155/300, seasonal_3 Loss: 0.0693 | 0.0368
Epoch 156/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 157/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 158/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 159/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 160/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 161/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 162/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 163/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 164/300, seasonal_3 Loss: 0.0692 | 0.0368
Epoch 165/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 166/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 167/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 168/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 169/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 170/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 171/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 172/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 173/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 174/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 175/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 176/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 177/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 178/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 179/300, seasonal_3 Loss: 0.0692 | 0.0367
Epoch 180/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 181/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 182/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 183/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 184/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 185/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 186/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 187/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 188/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 189/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 190/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 191/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 192/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 193/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 194/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 195/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 196/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 197/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 198/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 199/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 200/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 201/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 202/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 203/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 204/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 205/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 206/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 207/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 208/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 209/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 210/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 211/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 212/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 213/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 214/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 215/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 216/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 217/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 218/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 219/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 220/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 221/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 222/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 223/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 224/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 225/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 226/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 227/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 228/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 229/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 230/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 231/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 232/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 233/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 234/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 235/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 236/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 237/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 238/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 239/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 240/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 241/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 242/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 243/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 244/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 245/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 246/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 247/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 248/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 249/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 250/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 251/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 252/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 253/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 254/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 255/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 256/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 257/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 258/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 259/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 260/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 261/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 262/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 263/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 264/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 265/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 266/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 267/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 268/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 269/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 270/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 271/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 272/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 273/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 274/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 275/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 276/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 277/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 278/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 279/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 280/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 281/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 282/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 283/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 284/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 285/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 286/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 287/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 288/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 289/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 290/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 291/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 292/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 293/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 294/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 295/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 296/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 297/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 298/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 299/300, seasonal_3 Loss: 0.0691 | 0.0367
Epoch 300/300, seasonal_3 Loss: 0.0691 | 0.0367
Training resid component with params: {'observation_period_num': 15, 'train_rates': 0.9766606509762927, 'learning_rate': 0.0007430373211506457, 'batch_size': 29, 'step_size': 9, 'gamma': 0.7655288419055803}
Epoch 1/300, resid Loss: 0.2050 | 0.1115
Epoch 2/300, resid Loss: 0.1167 | 0.0890
Epoch 3/300, resid Loss: 0.1091 | 0.0742
Epoch 4/300, resid Loss: 0.0989 | 0.0646
Epoch 5/300, resid Loss: 0.0944 | 0.0619
Epoch 6/300, resid Loss: 0.0879 | 0.0591
Epoch 7/300, resid Loss: 0.0812 | 0.0496
Epoch 8/300, resid Loss: 0.0819 | 0.0487
Epoch 9/300, resid Loss: 0.0756 | 0.0549
Epoch 10/300, resid Loss: 0.0753 | 0.0504
Epoch 11/300, resid Loss: 0.0707 | 0.0463
Epoch 12/300, resid Loss: 0.0684 | 0.0430
Epoch 13/300, resid Loss: 0.0655 | 0.0425
Epoch 14/300, resid Loss: 0.0647 | 0.0351
Epoch 15/300, resid Loss: 0.0623 | 0.0376
Epoch 16/300, resid Loss: 0.0613 | 0.0351
Epoch 17/300, resid Loss: 0.0612 | 0.0349
Epoch 18/300, resid Loss: 0.0596 | 0.0346
Epoch 19/300, resid Loss: 0.0589 | 0.0395
Epoch 20/300, resid Loss: 0.0575 | 0.0372
Epoch 21/300, resid Loss: 0.0570 | 0.0358
Epoch 22/300, resid Loss: 0.0559 | 0.0349
Epoch 23/300, resid Loss: 0.0549 | 0.0324
Epoch 24/300, resid Loss: 0.0543 | 0.0311
Epoch 25/300, resid Loss: 0.0536 | 0.0311
Epoch 26/300, resid Loss: 0.0534 | 0.0303
Epoch 27/300, resid Loss: 0.0526 | 0.0302
Epoch 28/300, resid Loss: 0.0517 | 0.0270
Epoch 29/300, resid Loss: 0.0524 | 0.0270
Epoch 30/300, resid Loss: 0.0538 | 0.0267
Epoch 31/300, resid Loss: 0.0517 | 0.0258
Epoch 32/300, resid Loss: 0.0510 | 0.0254
Epoch 33/300, resid Loss: 0.0504 | 0.0245
Epoch 34/300, resid Loss: 0.0506 | 0.0235
Epoch 35/300, resid Loss: 0.0498 | 0.0235
Epoch 36/300, resid Loss: 0.0491 | 0.0236
Epoch 37/300, resid Loss: 0.0486 | 0.0231
Epoch 38/300, resid Loss: 0.0486 | 0.0233
Epoch 39/300, resid Loss: 0.0478 | 0.0239
Epoch 40/300, resid Loss: 0.0475 | 0.0242
Epoch 41/300, resid Loss: 0.0471 | 0.0243
Epoch 42/300, resid Loss: 0.0466 | 0.0253
Epoch 43/300, resid Loss: 0.0462 | 0.0254
Epoch 44/300, resid Loss: 0.0460 | 0.0254
Epoch 45/300, resid Loss: 0.0458 | 0.0255
Epoch 46/300, resid Loss: 0.0455 | 0.0261
Epoch 47/300, resid Loss: 0.0453 | 0.0261
Epoch 48/300, resid Loss: 0.0451 | 0.0262
Epoch 49/300, resid Loss: 0.0449 | 0.0262
Epoch 50/300, resid Loss: 0.0446 | 0.0263
Epoch 51/300, resid Loss: 0.0443 | 0.0265
Epoch 52/300, resid Loss: 0.0439 | 0.0267
Epoch 53/300, resid Loss: 0.0433 | 0.0271
Epoch 54/300, resid Loss: 0.0421 | 0.0269
Epoch 55/300, resid Loss: 0.0410 | 0.0263
Epoch 56/300, resid Loss: 0.0402 | 0.0266
Epoch 57/300, resid Loss: 0.0400 | 0.0257
Epoch 58/300, resid Loss: 0.0431 | 0.0266
Epoch 59/300, resid Loss: 0.0408 | 0.0285
Epoch 60/300, resid Loss: 0.0401 | 0.0263
Epoch 61/300, resid Loss: 0.0388 | 0.0256
Epoch 62/300, resid Loss: 0.0384 | 0.0262
Epoch 63/300, resid Loss: 0.0383 | 0.0257
Epoch 64/300, resid Loss: 0.0382 | 0.0273
Epoch 65/300, resid Loss: 0.0382 | 0.0266
Epoch 66/300, resid Loss: 0.0376 | 0.0269
Epoch 67/300, resid Loss: 0.0374 | 0.0266
Epoch 68/300, resid Loss: 0.0373 | 0.0271
Epoch 69/300, resid Loss: 0.0378 | 0.0272
Epoch 70/300, resid Loss: 0.0371 | 0.0273
Epoch 71/300, resid Loss: 0.0369 | 0.0272
Epoch 72/300, resid Loss: 0.0368 | 0.0273
Epoch 73/300, resid Loss: 0.0367 | 0.0272
Epoch 74/300, resid Loss: 0.0366 | 0.0272
Epoch 75/300, resid Loss: 0.0366 | 0.0271
Epoch 76/300, resid Loss: 0.0365 | 0.0272
Epoch 77/300, resid Loss: 0.0366 | 0.0271
Epoch 78/300, resid Loss: 0.0362 | 0.0269
Epoch 79/300, resid Loss: 0.0361 | 0.0267
Epoch 80/300, resid Loss: 0.0361 | 0.0267
Epoch 81/300, resid Loss: 0.0360 | 0.0267
Epoch 82/300, resid Loss: 0.0360 | 0.0263
Epoch 83/300, resid Loss: 0.0360 | 0.0262
Epoch 84/300, resid Loss: 0.0359 | 0.0262
Epoch 85/300, resid Loss: 0.0359 | 0.0261
Epoch 86/300, resid Loss: 0.0358 | 0.0261
Epoch 87/300, resid Loss: 0.0358 | 0.0259
Epoch 88/300, resid Loss: 0.0358 | 0.0259
Epoch 89/300, resid Loss: 0.0357 | 0.0259
Epoch 90/300, resid Loss: 0.0357 | 0.0259
Epoch 91/300, resid Loss: 0.0357 | 0.0258
Epoch 92/300, resid Loss: 0.0356 | 0.0258
Epoch 93/300, resid Loss: 0.0356 | 0.0258
Epoch 94/300, resid Loss: 0.0355 | 0.0258
Epoch 95/300, resid Loss: 0.0355 | 0.0258
Epoch 96/300, resid Loss: 0.0354 | 0.0258
Epoch 97/300, resid Loss: 0.0354 | 0.0258
Epoch 98/300, resid Loss: 0.0354 | 0.0258
Epoch 99/300, resid Loss: 0.0353 | 0.0258
Epoch 100/300, resid Loss: 0.0353 | 0.0258
Epoch 101/300, resid Loss: 0.0353 | 0.0258
Epoch 102/300, resid Loss: 0.0352 | 0.0258
Epoch 103/300, resid Loss: 0.0352 | 0.0258
Epoch 104/300, resid Loss: 0.0352 | 0.0258
Epoch 105/300, resid Loss: 0.0352 | 0.0258
Epoch 106/300, resid Loss: 0.0351 | 0.0258
Epoch 107/300, resid Loss: 0.0351 | 0.0258
Epoch 108/300, resid Loss: 0.0351 | 0.0258
Epoch 109/300, resid Loss: 0.0351 | 0.0258
Epoch 110/300, resid Loss: 0.0351 | 0.0258
Epoch 111/300, resid Loss: 0.0351 | 0.0258
Epoch 112/300, resid Loss: 0.0351 | 0.0258
Epoch 113/300, resid Loss: 0.0350 | 0.0258
Epoch 114/300, resid Loss: 0.0350 | 0.0258
Epoch 115/300, resid Loss: 0.0350 | 0.0258
Epoch 116/300, resid Loss: 0.0350 | 0.0258
Epoch 117/300, resid Loss: 0.0350 | 0.0259
Epoch 118/300, resid Loss: 0.0350 | 0.0259
Epoch 119/300, resid Loss: 0.0350 | 0.0259
Epoch 120/300, resid Loss: 0.0350 | 0.0259
Epoch 121/300, resid Loss: 0.0350 | 0.0259
Epoch 122/300, resid Loss: 0.0350 | 0.0259
Epoch 123/300, resid Loss: 0.0350 | 0.0259
Epoch 124/300, resid Loss: 0.0349 | 0.0259
Epoch 125/300, resid Loss: 0.0349 | 0.0259
Epoch 126/300, resid Loss: 0.0349 | 0.0259
Epoch 127/300, resid Loss: 0.0349 | 0.0259
Epoch 128/300, resid Loss: 0.0349 | 0.0259
Epoch 129/300, resid Loss: 0.0349 | 0.0259
Epoch 130/300, resid Loss: 0.0349 | 0.0259
Epoch 131/300, resid Loss: 0.0349 | 0.0259
Epoch 132/300, resid Loss: 0.0349 | 0.0259
Epoch 133/300, resid Loss: 0.0349 | 0.0259
Epoch 134/300, resid Loss: 0.0349 | 0.0259
Epoch 135/300, resid Loss: 0.0349 | 0.0259
Epoch 136/300, resid Loss: 0.0349 | 0.0259
Epoch 137/300, resid Loss: 0.0349 | 0.0259
Epoch 138/300, resid Loss: 0.0349 | 0.0259
Epoch 139/300, resid Loss: 0.0349 | 0.0259
Epoch 140/300, resid Loss: 0.0349 | 0.0259
Epoch 141/300, resid Loss: 0.0349 | 0.0259
Epoch 142/300, resid Loss: 0.0349 | 0.0259
Epoch 143/300, resid Loss: 0.0349 | 0.0259
Epoch 144/300, resid Loss: 0.0349 | 0.0259
Epoch 145/300, resid Loss: 0.0349 | 0.0259
Epoch 146/300, resid Loss: 0.0349 | 0.0259
Epoch 147/300, resid Loss: 0.0349 | 0.0259
Epoch 148/300, resid Loss: 0.0349 | 0.0259
Epoch 149/300, resid Loss: 0.0349 | 0.0259
Epoch 150/300, resid Loss: 0.0349 | 0.0259
Epoch 151/300, resid Loss: 0.0349 | 0.0259
Epoch 152/300, resid Loss: 0.0349 | 0.0259
Epoch 153/300, resid Loss: 0.0349 | 0.0259
Epoch 154/300, resid Loss: 0.0349 | 0.0259
Epoch 155/300, resid Loss: 0.0349 | 0.0259
Epoch 156/300, resid Loss: 0.0349 | 0.0259
Epoch 157/300, resid Loss: 0.0349 | 0.0259
Epoch 158/300, resid Loss: 0.0349 | 0.0259
Epoch 159/300, resid Loss: 0.0349 | 0.0259
Epoch 160/300, resid Loss: 0.0349 | 0.0259
Epoch 161/300, resid Loss: 0.0349 | 0.0259
Epoch 162/300, resid Loss: 0.0349 | 0.0259
Epoch 163/300, resid Loss: 0.0349 | 0.0259
Epoch 164/300, resid Loss: 0.0349 | 0.0259
Epoch 165/300, resid Loss: 0.0349 | 0.0259
Epoch 166/300, resid Loss: 0.0349 | 0.0259
Epoch 167/300, resid Loss: 0.0349 | 0.0259
Epoch 168/300, resid Loss: 0.0349 | 0.0259
Epoch 169/300, resid Loss: 0.0349 | 0.0259
Epoch 170/300, resid Loss: 0.0349 | 0.0259
Epoch 171/300, resid Loss: 0.0349 | 0.0259
Epoch 172/300, resid Loss: 0.0349 | 0.0259
Epoch 173/300, resid Loss: 0.0349 | 0.0259
Epoch 174/300, resid Loss: 0.0349 | 0.0259
Epoch 175/300, resid Loss: 0.0349 | 0.0259
Epoch 176/300, resid Loss: 0.0349 | 0.0259
Epoch 177/300, resid Loss: 0.0349 | 0.0259
Epoch 178/300, resid Loss: 0.0349 | 0.0259
Epoch 179/300, resid Loss: 0.0349 | 0.0259
Epoch 180/300, resid Loss: 0.0349 | 0.0259
Epoch 181/300, resid Loss: 0.0349 | 0.0259
Epoch 182/300, resid Loss: 0.0349 | 0.0259
Epoch 183/300, resid Loss: 0.0349 | 0.0259
Epoch 184/300, resid Loss: 0.0349 | 0.0259
Epoch 185/300, resid Loss: 0.0349 | 0.0259
Epoch 186/300, resid Loss: 0.0349 | 0.0259
Epoch 187/300, resid Loss: 0.0349 | 0.0259
Epoch 188/300, resid Loss: 0.0349 | 0.0259
Epoch 189/300, resid Loss: 0.0349 | 0.0259
Epoch 190/300, resid Loss: 0.0349 | 0.0259
Epoch 191/300, resid Loss: 0.0349 | 0.0259
Epoch 192/300, resid Loss: 0.0348 | 0.0259
Epoch 193/300, resid Loss: 0.0348 | 0.0259
Epoch 194/300, resid Loss: 0.0348 | 0.0259
Epoch 195/300, resid Loss: 0.0348 | 0.0259
Epoch 196/300, resid Loss: 0.0348 | 0.0259
Epoch 197/300, resid Loss: 0.0348 | 0.0259
Epoch 198/300, resid Loss: 0.0348 | 0.0259
Epoch 199/300, resid Loss: 0.0348 | 0.0259
Epoch 200/300, resid Loss: 0.0348 | 0.0259
Epoch 201/300, resid Loss: 0.0348 | 0.0259
Epoch 202/300, resid Loss: 0.0348 | 0.0259
Epoch 203/300, resid Loss: 0.0348 | 0.0259
Epoch 204/300, resid Loss: 0.0348 | 0.0259
Epoch 205/300, resid Loss: 0.0348 | 0.0259
Epoch 206/300, resid Loss: 0.0348 | 0.0259
Epoch 207/300, resid Loss: 0.0348 | 0.0259
Epoch 208/300, resid Loss: 0.0348 | 0.0259
Epoch 209/300, resid Loss: 0.0348 | 0.0259
Epoch 210/300, resid Loss: 0.0348 | 0.0259
Epoch 211/300, resid Loss: 0.0348 | 0.0259
Epoch 212/300, resid Loss: 0.0348 | 0.0259
Epoch 213/300, resid Loss: 0.0348 | 0.0259
Epoch 214/300, resid Loss: 0.0348 | 0.0259
Epoch 215/300, resid Loss: 0.0348 | 0.0259
Epoch 216/300, resid Loss: 0.0348 | 0.0259
Epoch 217/300, resid Loss: 0.0348 | 0.0259
Epoch 218/300, resid Loss: 0.0348 | 0.0259
Epoch 219/300, resid Loss: 0.0348 | 0.0259
Epoch 220/300, resid Loss: 0.0348 | 0.0259
Epoch 221/300, resid Loss: 0.0348 | 0.0259
Epoch 222/300, resid Loss: 0.0348 | 0.0259
Epoch 223/300, resid Loss: 0.0348 | 0.0259
Epoch 224/300, resid Loss: 0.0348 | 0.0259
Epoch 225/300, resid Loss: 0.0348 | 0.0259
Epoch 226/300, resid Loss: 0.0348 | 0.0259
Epoch 227/300, resid Loss: 0.0348 | 0.0259
Epoch 228/300, resid Loss: 0.0348 | 0.0259
Epoch 229/300, resid Loss: 0.0348 | 0.0259
Epoch 230/300, resid Loss: 0.0348 | 0.0259
Epoch 231/300, resid Loss: 0.0348 | 0.0259
Epoch 232/300, resid Loss: 0.0348 | 0.0259
Epoch 233/300, resid Loss: 0.0348 | 0.0259
Epoch 234/300, resid Loss: 0.0348 | 0.0259
Epoch 235/300, resid Loss: 0.0348 | 0.0259
Epoch 236/300, resid Loss: 0.0348 | 0.0259
Epoch 237/300, resid Loss: 0.0348 | 0.0259
Epoch 238/300, resid Loss: 0.0348 | 0.0259
Epoch 239/300, resid Loss: 0.0348 | 0.0259
Epoch 240/300, resid Loss: 0.0348 | 0.0259
Epoch 241/300, resid Loss: 0.0348 | 0.0259
Epoch 242/300, resid Loss: 0.0348 | 0.0259
Epoch 243/300, resid Loss: 0.0348 | 0.0259
Epoch 244/300, resid Loss: 0.0348 | 0.0259
Epoch 245/300, resid Loss: 0.0348 | 0.0259
Epoch 246/300, resid Loss: 0.0348 | 0.0259
Early stopping for resid
Runtime (seconds): 1114.6625037193298
0.0008087096633273017
[155.34628]
[-1.6830829]
[-4.674113]
[10.0297]
[1.9888246]
[7.8733025]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 15.547556085744873
RMSE: 3.9430389404296875
MAE: 3.9430389404296875
R-squared: nan
[168.8809]
