[32m[I 2025-01-05 16:29:43,775][0m A new study created in memory with name: no-name-e8551e2b-892e-42c0-b77a-246689f428d2[0m
[32m[I 2025-01-05 16:31:52,484][0m Trial 0 finished with value: 0.9558267831802368 and parameters: {'observation_period_num': 169, 'train_rates': 0.7162367644682714, 'learning_rate': 7.857032496203879e-05, 'batch_size': 237, 'step_size': 13, 'gamma': 0.9014396196173852}. Best is trial 0 with value: 0.9558267831802368.[0m
[32m[I 2025-01-05 16:33:10,287][0m Trial 1 finished with value: 0.15706811606884002 and parameters: {'observation_period_num': 79, 'train_rates': 0.9043757199051401, 'learning_rate': 0.00013439589671976083, 'batch_size': 79, 'step_size': 6, 'gamma': 0.8372178212483412}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:34:15,429][0m Trial 2 finished with value: 1.1929528557616407 and parameters: {'observation_period_num': 98, 'train_rates': 0.6200163917133815, 'learning_rate': 7.46012814856823e-06, 'batch_size': 127, 'step_size': 8, 'gamma': 0.8961327866124987}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:35:23,266][0m Trial 3 finished with value: 1.326130747795105 and parameters: {'observation_period_num': 78, 'train_rates': 0.9861556946464353, 'learning_rate': 1.0236715485443548e-06, 'batch_size': 219, 'step_size': 5, 'gamma': 0.9456830497899644}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:38:19,369][0m Trial 4 finished with value: 0.22263479232788086 and parameters: {'observation_period_num': 191, 'train_rates': 0.9540490869718783, 'learning_rate': 0.00023860532371022507, 'batch_size': 177, 'step_size': 2, 'gamma': 0.8225771608605635}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:41:36,224][0m Trial 5 finished with value: 0.5842965265500892 and parameters: {'observation_period_num': 204, 'train_rates': 0.9251639620261486, 'learning_rate': 2.0511378972747647e-06, 'batch_size': 67, 'step_size': 13, 'gamma': 0.8138655353960018}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:42:29,603][0m Trial 6 finished with value: 0.1791680244699357 and parameters: {'observation_period_num': 51, 'train_rates': 0.9349001323745139, 'learning_rate': 2.0454007932856745e-05, 'batch_size': 116, 'step_size': 8, 'gamma': 0.9017665336966827}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:43:22,102][0m Trial 7 finished with value: 0.7752156600015504 and parameters: {'observation_period_num': 64, 'train_rates': 0.6969249098663715, 'learning_rate': 0.00022194280275074314, 'batch_size': 93, 'step_size': 2, 'gamma': 0.9211292296449565}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:43:56,348][0m Trial 8 finished with value: 1.2371238026188798 and parameters: {'observation_period_num': 21, 'train_rates': 0.8221554444826692, 'learning_rate': 2.099212957208124e-06, 'batch_size': 164, 'step_size': 1, 'gamma': 0.9705217934039896}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:44:57,584][0m Trial 9 finished with value: 0.891883953737281 and parameters: {'observation_period_num': 78, 'train_rates': 0.7596347917932952, 'learning_rate': 0.0002610561926463584, 'batch_size': 107, 'step_size': 10, 'gamma': 0.9755027905680155}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:49:11,715][0m Trial 10 finished with value: 1.1931418385519579 and parameters: {'observation_period_num': 247, 'train_rates': 0.8731612277694115, 'learning_rate': 0.000875141821503706, 'batch_size': 26, 'step_size': 5, 'gamma': 0.7511086711925721}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:51:06,447][0m Trial 11 finished with value: 0.201820342892016 and parameters: {'observation_period_num': 5, 'train_rates': 0.8918485500941203, 'learning_rate': 1.925553928458097e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.8365100335690903}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:52:52,458][0m Trial 12 finished with value: 0.41927512941425943 and parameters: {'observation_period_num': 126, 'train_rates': 0.8454957579060631, 'learning_rate': 3.8988023772357746e-05, 'batch_size': 80, 'step_size': 5, 'gamma': 0.8640372300447151}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:53:34,035][0m Trial 13 finished with value: 0.3655133820976672 and parameters: {'observation_period_num': 41, 'train_rates': 0.9180171742531079, 'learning_rate': 1.2359281386447996e-05, 'batch_size': 149, 'step_size': 10, 'gamma': 0.7898038931236763}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:55:06,025][0m Trial 14 finished with value: 0.4753351132210611 and parameters: {'observation_period_num': 117, 'train_rates': 0.7932951935639204, 'learning_rate': 5.5123388720625716e-05, 'batch_size': 114, 'step_size': 7, 'gamma': 0.8670386874450288}. Best is trial 1 with value: 0.15706811606884002.[0m
[32m[I 2025-01-05 16:55:49,543][0m Trial 15 finished with value: 0.12979598343372345 and parameters: {'observation_period_num': 48, 'train_rates': 0.9857921219566128, 'learning_rate': 9.312845792767006e-05, 'batch_size': 199, 'step_size': 10, 'gamma': 0.8495626316313718}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 16:56:24,312][0m Trial 16 finished with value: 0.14478734135627747 and parameters: {'observation_period_num': 33, 'train_rates': 0.9880898937250698, 'learning_rate': 0.00010910252508461353, 'batch_size': 194, 'step_size': 11, 'gamma': 0.784231622609883}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 16:56:59,436][0m Trial 17 finished with value: 0.2351956069469452 and parameters: {'observation_period_num': 34, 'train_rates': 0.987865161953516, 'learning_rate': 0.0006532375468717529, 'batch_size': 197, 'step_size': 15, 'gamma': 0.7624891844533949}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 16:57:30,584][0m Trial 18 finished with value: 0.13519175350666046 and parameters: {'observation_period_num': 5, 'train_rates': 0.9594098443797688, 'learning_rate': 9.481018020825666e-05, 'batch_size': 202, 'step_size': 11, 'gamma': 0.7832848914613839}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 16:57:56,530][0m Trial 19 finished with value: 0.6474275152894515 and parameters: {'observation_period_num': 9, 'train_rates': 0.8659576756788786, 'learning_rate': 6.8149078921736934e-06, 'batch_size': 253, 'step_size': 12, 'gamma': 0.7896212199118056}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 17:00:15,886][0m Trial 20 finished with value: 0.13955619931221008 and parameters: {'observation_period_num': 156, 'train_rates': 0.9496625721690761, 'learning_rate': 0.00039931607076785196, 'batch_size': 221, 'step_size': 15, 'gamma': 0.8494208345183178}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 17:02:27,058][0m Trial 21 finished with value: 0.1424880176782608 and parameters: {'observation_period_num': 149, 'train_rates': 0.9449329498591078, 'learning_rate': 0.0006067011815102395, 'batch_size': 213, 'step_size': 15, 'gamma': 0.850975961014769}. Best is trial 15 with value: 0.12979598343372345.[0m
[32m[I 2025-01-05 17:04:39,439][0m Trial 22 finished with value: 0.11700116097927094 and parameters: {'observation_period_num': 150, 'train_rates': 0.956323250764078, 'learning_rate': 0.00036627600100900653, 'batch_size': 228, 'step_size': 14, 'gamma': 0.8105828120141109}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:06:08,571][0m Trial 23 finished with value: 0.13106678426265717 and parameters: {'observation_period_num': 104, 'train_rates': 0.9623975097550254, 'learning_rate': 0.0001343448661507834, 'batch_size': 188, 'step_size': 13, 'gamma': 0.8035396262647245}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:07:37,669][0m Trial 24 finished with value: 0.16929417073726655 and parameters: {'observation_period_num': 106, 'train_rates': 0.9035544232400662, 'learning_rate': 0.00017066375337958074, 'batch_size': 176, 'step_size': 13, 'gamma': 0.8084837290507249}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:09:39,823][0m Trial 25 finished with value: 0.13107213377952576 and parameters: {'observation_period_num': 138, 'train_rates': 0.9683230144656819, 'learning_rate': 0.00040782116950840546, 'batch_size': 241, 'step_size': 13, 'gamma': 0.8158058381540277}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:12:12,121][0m Trial 26 finished with value: 0.3514240766084322 and parameters: {'observation_period_num': 181, 'train_rates': 0.8322531220847686, 'learning_rate': 4.115023729643349e-05, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8831866980681864}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:15:27,687][0m Trial 27 finished with value: 0.2811111509799957 and parameters: {'observation_period_num': 217, 'train_rates': 0.8682768268266811, 'learning_rate': 6.021304195481742e-05, 'batch_size': 180, 'step_size': 10, 'gamma': 0.8010266985433103}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:16:36,744][0m Trial 28 finished with value: 0.8283454579833881 and parameters: {'observation_period_num': 107, 'train_rates': 0.6386252460799933, 'learning_rate': 0.0003499300832181685, 'batch_size': 238, 'step_size': 11, 'gamma': 0.7702371086806232}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:18:36,602][0m Trial 29 finished with value: 0.9616849672100063 and parameters: {'observation_period_num': 163, 'train_rates': 0.7106178441281169, 'learning_rate': 9.482111781596538e-05, 'batch_size': 232, 'step_size': 12, 'gamma': 0.828047313266224}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:20:22,062][0m Trial 30 finished with value: 0.6780492719551011 and parameters: {'observation_period_num': 140, 'train_rates': 0.7472945385182486, 'learning_rate': 6.366813356470446e-05, 'batch_size': 151, 'step_size': 14, 'gamma': 0.8481236901406127}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:22:20,988][0m Trial 31 finished with value: 0.12323420494794846 and parameters: {'observation_period_num': 135, 'train_rates': 0.9725071248851496, 'learning_rate': 0.0003748996932194409, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8051516371643809}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:24:59,935][0m Trial 32 finished with value: 0.12394800037145615 and parameters: {'observation_period_num': 174, 'train_rates': 0.9664085378235399, 'learning_rate': 0.0001725742033575258, 'batch_size': 256, 'step_size': 14, 'gamma': 0.8005050357119928}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:27:25,013][0m Trial 33 finished with value: 0.15422096848487854 and parameters: {'observation_period_num': 166, 'train_rates': 0.9149591586342202, 'learning_rate': 0.00017764786052054077, 'batch_size': 252, 'step_size': 14, 'gamma': 0.8315866861481956}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:30:15,019][0m Trial 34 finished with value: 0.6461264491081238 and parameters: {'observation_period_num': 181, 'train_rates': 0.9732440353410806, 'learning_rate': 0.0009776408937098352, 'batch_size': 224, 'step_size': 12, 'gamma': 0.7741090495702777}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:33:45,508][0m Trial 35 finished with value: 0.1387229859828949 and parameters: {'observation_period_num': 221, 'train_rates': 0.9345945467819644, 'learning_rate': 0.0005061818620569754, 'batch_size': 256, 'step_size': 9, 'gamma': 0.7963377504110369}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:35:30,907][0m Trial 36 finished with value: 0.17517812956463208 and parameters: {'observation_period_num': 127, 'train_rates': 0.8949008754180311, 'learning_rate': 0.0002979578822721563, 'batch_size': 207, 'step_size': 14, 'gamma': 0.8231932593162389}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:36:46,512][0m Trial 37 finished with value: 0.13472987711429596 and parameters: {'observation_period_num': 88, 'train_rates': 0.988777057374727, 'learning_rate': 0.000160529430308234, 'batch_size': 239, 'step_size': 9, 'gamma': 0.84697535454939}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:39:50,067][0m Trial 38 finished with value: 0.13665539026260376 and parameters: {'observation_period_num': 197, 'train_rates': 0.9347340489913152, 'learning_rate': 0.00020621991908135353, 'batch_size': 227, 'step_size': 15, 'gamma': 0.895943787683978}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:42:37,645][0m Trial 39 finished with value: 0.12837287783622742 and parameters: {'observation_period_num': 181, 'train_rates': 0.9650790297926929, 'learning_rate': 0.00012049695770120097, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8802492389928006}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:45:13,830][0m Trial 40 finished with value: 0.1467314064502716 and parameters: {'observation_period_num': 178, 'train_rates': 0.9197273398885722, 'learning_rate': 0.0005248198548709343, 'batch_size': 242, 'step_size': 12, 'gamma': 0.9323097198547506}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:47:25,337][0m Trial 41 finished with value: 0.13209129869937897 and parameters: {'observation_period_num': 147, 'train_rates': 0.9634036167006392, 'learning_rate': 0.00013428143316812023, 'batch_size': 213, 'step_size': 14, 'gamma': 0.8838403786395727}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:49:55,546][0m Trial 42 finished with value: 0.11933187395334244 and parameters: {'observation_period_num': 167, 'train_rates': 0.9481551847323186, 'learning_rate': 0.0002720336612817587, 'batch_size': 231, 'step_size': 13, 'gamma': 0.9150997809803166}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:53:12,458][0m Trial 43 finished with value: 0.12034423649311066 and parameters: {'observation_period_num': 210, 'train_rates': 0.9419250749315198, 'learning_rate': 0.0002785314236757695, 'batch_size': 248, 'step_size': 13, 'gamma': 0.9159066888304622}. Best is trial 22 with value: 0.11700116097927094.[0m
[32m[I 2025-01-05 17:56:27,259][0m Trial 44 finished with value: 0.11514441668987274 and parameters: {'observation_period_num': 206, 'train_rates': 0.9446718435824035, 'learning_rate': 0.0002715673999373618, 'batch_size': 256, 'step_size': 13, 'gamma': 0.9240423038473897}. Best is trial 44 with value: 0.11514441668987274.[0m
[32m[I 2025-01-05 18:00:01,964][0m Trial 45 finished with value: 0.1986093846665826 and parameters: {'observation_period_num': 233, 'train_rates': 0.8847211006393568, 'learning_rate': 0.0002784422979588285, 'batch_size': 231, 'step_size': 13, 'gamma': 0.9511625777954957}. Best is trial 44 with value: 0.11514441668987274.[0m
[32m[I 2025-01-05 18:03:18,522][0m Trial 46 finished with value: 0.18637335300445557 and parameters: {'observation_period_num': 210, 'train_rates': 0.9399238008360731, 'learning_rate': 0.0007609944852824725, 'batch_size': 246, 'step_size': 15, 'gamma': 0.9228751058217232}. Best is trial 44 with value: 0.11514441668987274.[0m
[32m[I 2025-01-05 18:06:14,969][0m Trial 47 finished with value: 0.17598147008770196 and parameters: {'observation_period_num': 196, 'train_rates': 0.9063738793815492, 'learning_rate': 0.0004346458681329182, 'batch_size': 230, 'step_size': 13, 'gamma': 0.9588607798762677}. Best is trial 44 with value: 0.11514441668987274.[0m
[32m[I 2025-01-05 18:09:37,800][0m Trial 48 finished with value: 0.4565666718220492 and parameters: {'observation_period_num': 239, 'train_rates': 0.7990041896352246, 'learning_rate': 0.0003139463739216052, 'batch_size': 244, 'step_size': 11, 'gamma': 0.9102099571838095}. Best is trial 44 with value: 0.11514441668987274.[0m
[32m[I 2025-01-05 18:12:47,363][0m Trial 49 finished with value: 0.163237065076828 and parameters: {'observation_period_num': 205, 'train_rates': 0.9320855703020704, 'learning_rate': 0.0006562698941507655, 'batch_size': 248, 'step_size': 4, 'gamma': 0.9358186547778365}. Best is trial 44 with value: 0.11514441668987274.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 1.1580 | 1.2017
Epoch 2/300, Loss: 0.7341 | 0.7288
Epoch 3/300, Loss: 0.6314 | 0.6092
Epoch 4/300, Loss: 0.5237 | 0.5721
Epoch 5/300, Loss: 0.4765 | 0.5215
Epoch 6/300, Loss: 0.5647 | 0.4737
Epoch 7/300, Loss: 0.4987 | 0.5305
Epoch 8/300, Loss: 0.4550 | 0.5106
Epoch 9/300, Loss: 0.4237 | 0.5661
Epoch 10/300, Loss: 0.3568 | 0.4737
Epoch 11/300, Loss: 0.3170 | 0.3728
Epoch 12/300, Loss: 0.2873 | 0.3338
Epoch 13/300, Loss: 0.2620 | 0.3088
Epoch 14/300, Loss: 0.2480 | 0.2869
Epoch 15/300, Loss: 0.2438 | 0.2616
Epoch 16/300, Loss: 0.2425 | 0.2561
Epoch 17/300, Loss: 0.2732 | 0.2411
Epoch 18/300, Loss: 0.2650 | 0.2445
Epoch 19/300, Loss: 0.2840 | 0.2348
Epoch 20/300, Loss: 0.2498 | 0.2604
Epoch 21/300, Loss: 0.2185 | 0.2332
Epoch 22/300, Loss: 0.2029 | 0.2146
Epoch 23/300, Loss: 0.1967 | 0.2084
Epoch 24/300, Loss: 0.1990 | 0.1979
Epoch 25/300, Loss: 0.1970 | 0.2057
Epoch 26/300, Loss: 0.1945 | 0.1884
Epoch 27/300, Loss: 0.1868 | 0.1931
Epoch 28/300, Loss: 0.1786 | 0.1815
Epoch 29/300, Loss: 0.1755 | 0.1783
Epoch 30/300, Loss: 0.1736 | 0.1758
Epoch 31/300, Loss: 0.1768 | 0.1703
Epoch 32/300, Loss: 0.1780 | 0.1721
Epoch 33/300, Loss: 0.1895 | 0.1659
Epoch 34/300, Loss: 0.1873 | 0.1787
Epoch 35/300, Loss: 0.1873 | 0.1681
Epoch 36/300, Loss: 0.1779 | 0.1728
Epoch 37/300, Loss: 0.1759 | 0.1712
Epoch 38/300, Loss: 0.1713 | 0.1691
Epoch 39/300, Loss: 0.1678 | 0.1600
Epoch 40/300, Loss: 0.1676 | 0.1609
Epoch 41/300, Loss: 0.1681 | 0.1597
Epoch 42/300, Loss: 0.1719 | 0.1625
Epoch 43/300, Loss: 0.1734 | 0.1572
Epoch 44/300, Loss: 0.1821 | 0.1594
Epoch 45/300, Loss: 0.1794 | 0.1543
Epoch 46/300, Loss: 0.1865 | 0.1542
Epoch 47/300, Loss: 0.1741 | 0.1539
Epoch 48/300, Loss: 0.1667 | 0.1553
Epoch 49/300, Loss: 0.1599 | 0.1575
Epoch 50/300, Loss: 0.1554 | 0.1522
Epoch 51/300, Loss: 0.1541 | 0.1526
Epoch 52/300, Loss: 0.1549 | 0.1509
Epoch 53/300, Loss: 0.1587 | 0.1508
Epoch 54/300, Loss: 0.1601 | 0.1501
Epoch 55/300, Loss: 0.1594 | 0.1469
Epoch 56/300, Loss: 0.1573 | 0.1490
Epoch 57/300, Loss: 0.1569 | 0.1464
Epoch 58/300, Loss: 0.1559 | 0.1452
Epoch 59/300, Loss: 0.1584 | 0.1431
Epoch 60/300, Loss: 0.1561 | 0.1421
Epoch 61/300, Loss: 0.1543 | 0.1481
Epoch 62/300, Loss: 0.1520 | 0.1415
Epoch 63/300, Loss: 0.1472 | 0.1498
Epoch 64/300, Loss: 0.1432 | 0.1386
Epoch 65/300, Loss: 0.1397 | 0.1395
Epoch 66/300, Loss: 0.1380 | 0.1389
Epoch 67/300, Loss: 0.1377 | 0.1368
Epoch 68/300, Loss: 0.1368 | 0.1388
Epoch 69/300, Loss: 0.1367 | 0.1337
Epoch 70/300, Loss: 0.1357 | 0.1378
Epoch 71/300, Loss: 0.1372 | 0.1322
Epoch 72/300, Loss: 0.1367 | 0.1362
Epoch 73/300, Loss: 0.1363 | 0.1304
Epoch 74/300, Loss: 0.1353 | 0.1332
Epoch 75/300, Loss: 0.1337 | 0.1299
Epoch 76/300, Loss: 0.1331 | 0.1309
Epoch 77/300, Loss: 0.1312 | 0.1285
Epoch 78/300, Loss: 0.1306 | 0.1286
Epoch 79/300, Loss: 0.1308 | 0.1266
Epoch 80/300, Loss: 0.1304 | 0.1284
Epoch 81/300, Loss: 0.1293 | 0.1273
Epoch 82/300, Loss: 0.1285 | 0.1262
Epoch 83/300, Loss: 0.1284 | 0.1256
Epoch 84/300, Loss: 0.1281 | 0.1256
Epoch 85/300, Loss: 0.1290 | 0.1250
Epoch 86/300, Loss: 0.1343 | 0.1258
Epoch 87/300, Loss: 0.1372 | 0.1276
Epoch 88/300, Loss: 0.1376 | 0.1261
Epoch 89/300, Loss: 0.1341 | 0.1302
Epoch 90/300, Loss: 0.1333 | 0.1216
Epoch 91/300, Loss: 0.1325 | 0.1287
Epoch 92/300, Loss: 0.1322 | 0.1215
Epoch 93/300, Loss: 0.1294 | 0.1276
Epoch 94/300, Loss: 0.1265 | 0.1193
Epoch 95/300, Loss: 0.1243 | 0.1256
Epoch 96/300, Loss: 0.1237 | 0.1199
Epoch 97/300, Loss: 0.1224 | 0.1227
Epoch 98/300, Loss: 0.1222 | 0.1195
Epoch 99/300, Loss: 0.1208 | 0.1228
Epoch 100/300, Loss: 0.1218 | 0.1182
Epoch 101/300, Loss: 0.1206 | 0.1215
Epoch 102/300, Loss: 0.1206 | 0.1182
Epoch 103/300, Loss: 0.1191 | 0.1173
Epoch 104/300, Loss: 0.1200 | 0.1194
Epoch 105/300, Loss: 0.1189 | 0.1179
Epoch 106/300, Loss: 0.1196 | 0.1191
Epoch 107/300, Loss: 0.1190 | 0.1183
Epoch 108/300, Loss: 0.1185 | 0.1180
Epoch 109/300, Loss: 0.1187 | 0.1161
Epoch 110/300, Loss: 0.1193 | 0.1198
Epoch 111/300, Loss: 0.1204 | 0.1153
Epoch 112/300, Loss: 0.1193 | 0.1205
Epoch 113/300, Loss: 0.1180 | 0.1138
Epoch 114/300, Loss: 0.1166 | 0.1186
Epoch 115/300, Loss: 0.1158 | 0.1140
Epoch 116/300, Loss: 0.1160 | 0.1175
Epoch 117/300, Loss: 0.1158 | 0.1130
Epoch 118/300, Loss: 0.1155 | 0.1169
Epoch 119/300, Loss: 0.1150 | 0.1138
Epoch 120/300, Loss: 0.1145 | 0.1155
Epoch 121/300, Loss: 0.1146 | 0.1125
Epoch 122/300, Loss: 0.1135 | 0.1162
Epoch 123/300, Loss: 0.1141 | 0.1138
Epoch 124/300, Loss: 0.1135 | 0.1153
Epoch 125/300, Loss: 0.1129 | 0.1130
Epoch 126/300, Loss: 0.1139 | 0.1153
Epoch 127/300, Loss: 0.1130 | 0.1115
Epoch 128/300, Loss: 0.1128 | 0.1149
Epoch 129/300, Loss: 0.1119 | 0.1123
Epoch 130/300, Loss: 0.1124 | 0.1152
Epoch 131/300, Loss: 0.1128 | 0.1103
Epoch 132/300, Loss: 0.1130 | 0.1159
Epoch 133/300, Loss: 0.1127 | 0.1104
Epoch 134/300, Loss: 0.1123 | 0.1156
Epoch 135/300, Loss: 0.1120 | 0.1098
Epoch 136/300, Loss: 0.1117 | 0.1159
Epoch 137/300, Loss: 0.1109 | 0.1096
Epoch 138/300, Loss: 0.1098 | 0.1132
Epoch 139/300, Loss: 0.1094 | 0.1097
Epoch 140/300, Loss: 0.1089 | 0.1105
Epoch 141/300, Loss: 0.1084 | 0.1117
Epoch 142/300, Loss: 0.1090 | 0.1101
Epoch 143/300, Loss: 0.1084 | 0.1113
Epoch 144/300, Loss: 0.1080 | 0.1108
Epoch 145/300, Loss: 0.1080 | 0.1104
Epoch 146/300, Loss: 0.1078 | 0.1124
Epoch 147/300, Loss: 0.1075 | 0.1102
Epoch 148/300, Loss: 0.1065 | 0.1116
Epoch 149/300, Loss: 0.1072 | 0.1091
Epoch 150/300, Loss: 0.1065 | 0.1085
Epoch 151/300, Loss: 0.1071 | 0.1118
Epoch 152/300, Loss: 0.1080 | 0.1066
Epoch 153/300, Loss: 0.1081 | 0.1146
Epoch 154/300, Loss: 0.1088 | 0.1078
Epoch 155/300, Loss: 0.1077 | 0.1118
Epoch 156/300, Loss: 0.1077 | 0.1056
Epoch 157/300, Loss: 0.1068 | 0.1105
Epoch 158/300, Loss: 0.1055 | 0.1074
Epoch 159/300, Loss: 0.1052 | 0.1098
Epoch 160/300, Loss: 0.1053 | 0.1079
Epoch 161/300, Loss: 0.1043 | 0.1108
Epoch 162/300, Loss: 0.1047 | 0.1076
Epoch 163/300, Loss: 0.1054 | 0.1095
Epoch 164/300, Loss: 0.1047 | 0.1069
Epoch 165/300, Loss: 0.1041 | 0.1089
Epoch 166/300, Loss: 0.1037 | 0.1092
Epoch 167/300, Loss: 0.1040 | 0.1090
Epoch 168/300, Loss: 0.1032 | 0.1095
Epoch 169/300, Loss: 0.1024 | 0.1075
Epoch 170/300, Loss: 0.1031 | 0.1110
Epoch 171/300, Loss: 0.1042 | 0.1059
Epoch 172/300, Loss: 0.1034 | 0.1108
Epoch 173/300, Loss: 0.1034 | 0.1070
Epoch 174/300, Loss: 0.1033 | 0.1081
Epoch 175/300, Loss: 0.1024 | 0.1091
Epoch 176/300, Loss: 0.1019 | 0.1074
Epoch 177/300, Loss: 0.1013 | 0.1094
Epoch 178/300, Loss: 0.1022 | 0.1060
Epoch 179/300, Loss: 0.1015 | 0.1098
Epoch 180/300, Loss: 0.1015 | 0.1062
Epoch 181/300, Loss: 0.1013 | 0.1096
Epoch 182/300, Loss: 0.1016 | 0.1066
Epoch 183/300, Loss: 0.1006 | 0.1091
Epoch 184/300, Loss: 0.1008 | 0.1083
Epoch 185/300, Loss: 0.1011 | 0.1074
Epoch 186/300, Loss: 0.0996 | 0.1069
Epoch 187/300, Loss: 0.1010 | 0.1067
Epoch 188/300, Loss: 0.1005 | 0.1102
Epoch 189/300, Loss: 0.1010 | 0.1043
Epoch 190/300, Loss: 0.1009 | 0.1077
Epoch 191/300, Loss: 0.0998 | 0.1059
Epoch 192/300, Loss: 0.0994 | 0.1059
Epoch 193/300, Loss: 0.0996 | 0.1046
Epoch 194/300, Loss: 0.1000 | 0.1058
Epoch 195/300, Loss: 0.0989 | 0.1049
Epoch 196/300, Loss: 0.0991 | 0.1062
Epoch 197/300, Loss: 0.0999 | 0.1076
Epoch 198/300, Loss: 0.0994 | 0.1043
Epoch 199/300, Loss: 0.0985 | 0.1050
Epoch 200/300, Loss: 0.0987 | 0.1026
Epoch 201/300, Loss: 0.0983 | 0.1049
Epoch 202/300, Loss: 0.0991 | 0.1047
Epoch 203/300, Loss: 0.0978 | 0.1049
Epoch 204/300, Loss: 0.0979 | 0.1072
Epoch 205/300, Loss: 0.0987 | 0.1024
Epoch 206/300, Loss: 0.0986 | 0.1058
Epoch 207/300, Loss: 0.0987 | 0.1039
Epoch 208/300, Loss: 0.0976 | 0.1038
Epoch 209/300, Loss: 0.0982 | 0.1052
Epoch 210/300, Loss: 0.0982 | 0.1054
Epoch 211/300, Loss: 0.0969 | 0.1057
Epoch 212/300, Loss: 0.0971 | 0.1052
Epoch 213/300, Loss: 0.0975 | 0.1051
Epoch 214/300, Loss: 0.0970 | 0.1057
Epoch 215/300, Loss: 0.0971 | 0.1055
Epoch 216/300, Loss: 0.0969 | 0.1048
Epoch 217/300, Loss: 0.0971 | 0.1037
Epoch 218/300, Loss: 0.0969 | 0.1056
Epoch 219/300, Loss: 0.0962 | 0.1041
Epoch 220/300, Loss: 0.0964 | 0.1045
Epoch 221/300, Loss: 0.0963 | 0.1035
Epoch 222/300, Loss: 0.0966 | 0.1033
Epoch 223/300, Loss: 0.0966 | 0.1051
Epoch 224/300, Loss: 0.0959 | 0.1039
Epoch 225/300, Loss: 0.0962 | 0.1039
Epoch 226/300, Loss: 0.0958 | 0.1060
Epoch 227/300, Loss: 0.0962 | 0.1033
Epoch 228/300, Loss: 0.0962 | 0.1044
Epoch 229/300, Loss: 0.0955 | 0.1028
Epoch 230/300, Loss: 0.0953 | 0.1067
Epoch 231/300, Loss: 0.0956 | 0.1027
Epoch 232/300, Loss: 0.0949 | 0.1049
Epoch 233/300, Loss: 0.0947 | 0.1047
Epoch 234/300, Loss: 0.0955 | 0.1033
Epoch 235/300, Loss: 0.0952 | 0.1028
Epoch 236/300, Loss: 0.0950 | 0.1033
Epoch 237/300, Loss: 0.0955 | 0.1046
Epoch 238/300, Loss: 0.0963 | 0.1022
Epoch 239/300, Loss: 0.0948 | 0.1022
Epoch 240/300, Loss: 0.0946 | 0.1027
Epoch 241/300, Loss: 0.0944 | 0.1026
Epoch 242/300, Loss: 0.0942 | 0.1040
Epoch 243/300, Loss: 0.0950 | 0.1035
Epoch 244/300, Loss: 0.0947 | 0.1035
Epoch 245/300, Loss: 0.0947 | 0.1037
Epoch 246/300, Loss: 0.0944 | 0.1032
Epoch 247/300, Loss: 0.0936 | 0.1020
Epoch 248/300, Loss: 0.0940 | 0.1034
Epoch 249/300, Loss: 0.0933 | 0.1024
Epoch 250/300, Loss: 0.0941 | 0.1035
Epoch 251/300, Loss: 0.0935 | 0.1022
Epoch 252/300, Loss: 0.0942 | 0.1017
Epoch 253/300, Loss: 0.0936 | 0.1022
Epoch 254/300, Loss: 0.0940 | 0.1025
Epoch 255/300, Loss: 0.0935 | 0.1028
Epoch 256/300, Loss: 0.0936 | 0.1038
Epoch 257/300, Loss: 0.0932 | 0.1021
Epoch 258/300, Loss: 0.0933 | 0.1021
Epoch 259/300, Loss: 0.0936 | 0.1025
Epoch 260/300, Loss: 0.0929 | 0.1002
Epoch 261/300, Loss: 0.0924 | 0.1029
Epoch 262/300, Loss: 0.0927 | 0.1024
Epoch 263/300, Loss: 0.0927 | 0.1010
Epoch 264/300, Loss: 0.0920 | 0.1011
Epoch 265/300, Loss: 0.0927 | 0.1029
Epoch 266/300, Loss: 0.0924 | 0.1024
Epoch 267/300, Loss: 0.0922 | 0.1039
Epoch 268/300, Loss: 0.0921 | 0.1026
Epoch 269/300, Loss: 0.0929 | 0.1018
Epoch 270/300, Loss: 0.0916 | 0.1025
Epoch 271/300, Loss: 0.0923 | 0.1029
Epoch 272/300, Loss: 0.0921 | 0.1036
Epoch 273/300, Loss: 0.0923 | 0.1019
Epoch 274/300, Loss: 0.0923 | 0.1038
Epoch 275/300, Loss: 0.0918 | 0.1029
Epoch 276/300, Loss: 0.0930 | 0.1019
Epoch 277/300, Loss: 0.0924 | 0.1014
Epoch 278/300, Loss: 0.0920 | 0.1013
Epoch 279/300, Loss: 0.0914 | 0.1013
Epoch 280/300, Loss: 0.0917 | 0.1027
Epoch 281/300, Loss: 0.0910 | 0.1027
Epoch 282/300, Loss: 0.0917 | 0.1025
Epoch 283/300, Loss: 0.0923 | 0.1009
Epoch 284/300, Loss: 0.0913 | 0.1018
Epoch 285/300, Loss: 0.0915 | 0.1028
Epoch 286/300, Loss: 0.0905 | 0.1014
Epoch 287/300, Loss: 0.0915 | 0.1022
Epoch 288/300, Loss: 0.0914 | 0.1030
Epoch 289/300, Loss: 0.0910 | 0.1017
Epoch 290/300, Loss: 0.0918 | 0.1019
Epoch 291/300, Loss: 0.0907 | 0.1022
Epoch 292/300, Loss: 0.0916 | 0.1023
Epoch 293/300, Loss: 0.0916 | 0.1012
Epoch 294/300, Loss: 0.0916 | 0.1011
Epoch 295/300, Loss: 0.0909 | 0.1012
Epoch 296/300, Loss: 0.0907 | 0.1018
Epoch 297/300, Loss: 0.0907 | 0.1025
Epoch 298/300, Loss: 0.0909 | 0.1015
Epoch 299/300, Loss: 0.0913 | 0.1019
Epoch 300/300, Loss: 0.0908 | 0.1031
Runtime (seconds): 582.2687170505524
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 33.06302643031813
RMSE: 5.7500457763671875
MAE: 5.7500457763671875
R-squared: nan
[191.36995]
