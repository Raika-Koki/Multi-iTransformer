ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-11 19:25:07,756][0m A new study created in memory with name: no-name-3014d04e-e7e8-4be5-b386-9f2d9cb5ddca[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-11 19:27:05,981][0m Trial 0 finished with value: 0.23871627114437827 and parameters: {'observation_period_num': 191, 'train_rates': 0.657106904959193, 'learning_rate': 0.00029346608468500335, 'batch_size': 33, 'step_size': 8, 'gamma': 0.854920194285239}. Best is trial 0 with value: 0.23871627114437827.[0m
[32m[I 2025-01-11 19:28:28,176][0m Trial 1 finished with value: 0.30880766314796254 and parameters: {'observation_period_num': 57, 'train_rates': 0.6725085593487934, 'learning_rate': 8.7593750102056e-06, 'batch_size': 52, 'step_size': 13, 'gamma': 0.9790338615105503}. Best is trial 0 with value: 0.23871627114437827.[0m
[32m[I 2025-01-11 19:32:33,971][0m Trial 2 finished with value: 0.08452380159410605 and parameters: {'observation_period_num': 98, 'train_rates': 0.7228687214516887, 'learning_rate': 0.00012144475844527689, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7918274497779796}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:34:42,610][0m Trial 3 finished with value: 0.5868282235914875 and parameters: {'observation_period_num': 16, 'train_rates': 0.7296634777023071, 'learning_rate': 3.5733654552076528e-06, 'batch_size': 33, 'step_size': 3, 'gamma': 0.8385405418187694}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:35:10,443][0m Trial 4 finished with value: 0.9495603920983486 and parameters: {'observation_period_num': 156, 'train_rates': 0.6943663489233004, 'learning_rate': 3.4969363023576356e-06, 'batch_size': 147, 'step_size': 9, 'gamma': 0.8401630025597722}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:38:15,434][0m Trial 5 finished with value: 0.49260153869787854 and parameters: {'observation_period_num': 176, 'train_rates': 0.9656868003096892, 'learning_rate': 7.764734391543841e-06, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8177626003193257}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:38:55,430][0m Trial 6 finished with value: 0.1453170887532067 and parameters: {'observation_period_num': 140, 'train_rates': 0.6653259849580825, 'learning_rate': 0.0006383730967446422, 'batch_size': 96, 'step_size': 11, 'gamma': 0.8363317365791175}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:39:54,526][0m Trial 7 finished with value: 0.1203879291289731 and parameters: {'observation_period_num': 38, 'train_rates': 0.8043695124674516, 'learning_rate': 0.00010249119010241717, 'batch_size': 79, 'step_size': 4, 'gamma': 0.8102104035281884}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:40:47,884][0m Trial 8 finished with value: 0.37897459935208616 and parameters: {'observation_period_num': 207, 'train_rates': 0.6322461422154944, 'learning_rate': 0.00028707596120521446, 'batch_size': 68, 'step_size': 2, 'gamma': 0.8414711988448954}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:42:37,286][0m Trial 9 finished with value: 0.1286723826940243 and parameters: {'observation_period_num': 163, 'train_rates': 0.9068528711470503, 'learning_rate': 8.918056672195683e-05, 'batch_size': 42, 'step_size': 1, 'gamma': 0.9634425237301533}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:43:01,569][0m Trial 10 finished with value: 0.7249415849263852 and parameters: {'observation_period_num': 89, 'train_rates': 0.8001046137299366, 'learning_rate': 3.0606739351341467e-05, 'batch_size': 229, 'step_size': 6, 'gamma': 0.7536933782222415}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:43:44,708][0m Trial 11 finished with value: 0.4791634665536029 and parameters: {'observation_period_num': 89, 'train_rates': 0.8043836660655925, 'learning_rate': 5.4698715298098385e-05, 'batch_size': 113, 'step_size': 4, 'gamma': 0.774560748821888}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:44:21,697][0m Trial 12 finished with value: 0.1214985328151038 and parameters: {'observation_period_num': 10, 'train_rates': 0.8630511974434525, 'learning_rate': 0.0001118644555267744, 'batch_size': 161, 'step_size': 5, 'gamma': 0.7927857209095507}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:45:09,047][0m Trial 13 finished with value: 0.1901320254802704 and parameters: {'observation_period_num': 96, 'train_rates': 0.7462767527540668, 'learning_rate': 3.244704947378579e-05, 'batch_size': 87, 'step_size': 15, 'gamma': 0.9098168650729577}. Best is trial 2 with value: 0.08452380159410605.[0m
[32m[I 2025-01-11 19:45:49,229][0m Trial 14 finished with value: 0.07471727731728879 and parameters: {'observation_period_num': 45, 'train_rates': 0.8490741794516234, 'learning_rate': 0.0001822229393148781, 'batch_size': 121, 'step_size': 10, 'gamma': 0.7937651496650215}. Best is trial 14 with value: 0.07471727731728879.[0m
[32m[I 2025-01-11 19:46:15,359][0m Trial 15 finished with value: 0.12426028338392461 and parameters: {'observation_period_num': 236, 'train_rates': 0.8725084675999485, 'learning_rate': 0.0009965742880796544, 'batch_size': 193, 'step_size': 11, 'gamma': 0.8964311910732012}. Best is trial 14 with value: 0.07471727731728879.[0m
[32m[I 2025-01-11 19:46:43,199][0m Trial 16 finished with value: 0.13386952055623327 and parameters: {'observation_period_num': 116, 'train_rates': 0.7453097218539321, 'learning_rate': 0.0002643411904860676, 'batch_size': 176, 'step_size': 10, 'gamma': 0.7551905273515069}. Best is trial 14 with value: 0.07471727731728879.[0m
[32m[I 2025-01-11 19:47:18,031][0m Trial 17 finished with value: 0.49197241917569584 and parameters: {'observation_period_num': 64, 'train_rates': 0.6093501311326309, 'learning_rate': 1.742848305062362e-05, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8849636273139613}. Best is trial 14 with value: 0.07471727731728879.[0m
[32m[I 2025-01-11 19:47:40,652][0m Trial 18 finished with value: 4.138272290682271 and parameters: {'observation_period_num': 58, 'train_rates': 0.8579324527096269, 'learning_rate': 1.2372155946684234e-06, 'batch_size': 231, 'step_size': 7, 'gamma': 0.8036419018063583}. Best is trial 14 with value: 0.07471727731728879.[0m
[32m[I 2025-01-11 19:48:21,025][0m Trial 19 finished with value: 0.1282776145204421 and parameters: {'observation_period_num': 121, 'train_rates': 0.9454207872360912, 'learning_rate': 0.0001829486137965751, 'batch_size': 117, 'step_size': 9, 'gamma': 0.7748828639903689}. Best is trial 14 with value: 0.07471727731728879.[0m
[32m[I 2025-01-11 19:48:47,133][0m Trial 20 finished with value: 0.0666096395743427 and parameters: {'observation_period_num': 38, 'train_rates': 0.7700155973388528, 'learning_rate': 0.0005266640801064546, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9378762166385656}. Best is trial 20 with value: 0.0666096395743427.[0m
[32m[I 2025-01-11 19:49:10,640][0m Trial 21 finished with value: 0.08237310767085855 and parameters: {'observation_period_num': 35, 'train_rates': 0.76762749464031, 'learning_rate': 0.0005372212639316141, 'batch_size': 196, 'step_size': 13, 'gamma': 0.9318874178609782}. Best is trial 20 with value: 0.0666096395743427.[0m
[32m[I 2025-01-11 19:49:35,172][0m Trial 22 finished with value: 0.06725655801371852 and parameters: {'observation_period_num': 36, 'train_rates': 0.7701319195639326, 'learning_rate': 0.0005597596316813195, 'batch_size': 193, 'step_size': 13, 'gamma': 0.9342485838330498}. Best is trial 20 with value: 0.0666096395743427.[0m
[32m[I 2025-01-11 19:49:59,047][0m Trial 23 finished with value: 0.06132764441937935 and parameters: {'observation_period_num': 31, 'train_rates': 0.8316902581573121, 'learning_rate': 0.000597592117417672, 'batch_size': 208, 'step_size': 15, 'gamma': 0.9398843920807506}. Best is trial 23 with value: 0.06132764441937935.[0m
[32m[I 2025-01-11 19:50:20,247][0m Trial 24 finished with value: 0.036833082048876864 and parameters: {'observation_period_num': 5, 'train_rates': 0.8242966810500074, 'learning_rate': 0.0009831097830855517, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9401375222812759}. Best is trial 24 with value: 0.036833082048876864.[0m
[32m[I 2025-01-11 19:50:40,257][0m Trial 25 finished with value: 0.045725069103800524 and parameters: {'observation_period_num': 7, 'train_rates': 0.8337974600125917, 'learning_rate': 0.00093465466123207, 'batch_size': 255, 'step_size': 15, 'gamma': 0.9462428996248119}. Best is trial 24 with value: 0.036833082048876864.[0m
[32m[I 2025-01-11 19:50:59,413][0m Trial 26 finished with value: 0.03630047499543369 and parameters: {'observation_period_num': 6, 'train_rates': 0.8287043337742016, 'learning_rate': 0.0009674388205993714, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9542654855367153}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:51:22,846][0m Trial 27 finished with value: 0.07497457088902593 and parameters: {'observation_period_num': 7, 'train_rates': 0.9022922478661347, 'learning_rate': 0.0009700962629644798, 'batch_size': 249, 'step_size': 15, 'gamma': 0.9894318128869082}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:51:46,724][0m Trial 28 finished with value: 0.05482366070354066 and parameters: {'observation_period_num': 5, 'train_rates': 0.9004892633065875, 'learning_rate': 0.00039735075733417127, 'batch_size': 256, 'step_size': 14, 'gamma': 0.962016356569488}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:52:10,033][0m Trial 29 finished with value: 0.09088259578341305 and parameters: {'observation_period_num': 73, 'train_rates': 0.824595097666063, 'learning_rate': 0.0002576366356475594, 'batch_size': 234, 'step_size': 12, 'gamma': 0.9180698472237943}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:52:33,573][0m Trial 30 finished with value: 0.07758931070566177 and parameters: {'observation_period_num': 19, 'train_rates': 0.9301529371708039, 'learning_rate': 0.0009364105509608677, 'batch_size': 214, 'step_size': 14, 'gamma': 0.9647657754654888}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:52:55,876][0m Trial 31 finished with value: 0.04837581059998936 and parameters: {'observation_period_num': 5, 'train_rates': 0.8992286856138907, 'learning_rate': 0.0003917599368597571, 'batch_size': 253, 'step_size': 14, 'gamma': 0.9597761892051896}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:53:15,128][0m Trial 32 finished with value: 0.05695781389772033 and parameters: {'observation_period_num': 20, 'train_rates': 0.8818000791508069, 'learning_rate': 0.0004250827145072132, 'batch_size': 243, 'step_size': 14, 'gamma': 0.9483985560832912}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:53:37,383][0m Trial 33 finished with value: 0.08286772800598134 and parameters: {'observation_period_num': 21, 'train_rates': 0.8316911470503335, 'learning_rate': 0.0008434127677059946, 'batch_size': 219, 'step_size': 15, 'gamma': 0.9754259540647673}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:53:55,329][0m Trial 34 finished with value: 0.0920174092438088 and parameters: {'observation_period_num': 48, 'train_rates': 0.8396269317604418, 'learning_rate': 0.0003678755422293959, 'batch_size': 256, 'step_size': 12, 'gamma': 0.915857394582367}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:54:19,276][0m Trial 35 finished with value: 0.12726858258247375 and parameters: {'observation_period_num': 73, 'train_rates': 0.9307039668102121, 'learning_rate': 0.0001825467161054016, 'batch_size': 240, 'step_size': 14, 'gamma': 0.8656394655181635}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:54:44,009][0m Trial 36 finished with value: 0.06632134705208816 and parameters: {'observation_period_num': 23, 'train_rates': 0.8937432955935198, 'learning_rate': 0.0006712253162110723, 'batch_size': 231, 'step_size': 12, 'gamma': 0.9523115779320485}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:55:11,077][0m Trial 37 finished with value: 0.11189893633127213 and parameters: {'observation_period_num': 54, 'train_rates': 0.9717754963921285, 'learning_rate': 0.0003243618444530791, 'batch_size': 219, 'step_size': 15, 'gamma': 0.9807277970007449}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:55:38,515][0m Trial 38 finished with value: 0.039845750096486054 and parameters: {'observation_period_num': 9, 'train_rates': 0.7123213512238346, 'learning_rate': 0.00075847792256126, 'batch_size': 178, 'step_size': 14, 'gamma': 0.8952351259720225}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:56:07,943][0m Trial 39 finished with value: 0.06550407362309 and parameters: {'observation_period_num': 29, 'train_rates': 0.6985781471407686, 'learning_rate': 0.0007640435091855463, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8871104038950615}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:56:38,775][0m Trial 40 finished with value: 0.17702199711620464 and parameters: {'observation_period_num': 77, 'train_rates': 0.6990364614994744, 'learning_rate': 6.876076436943835e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.9029753364588333}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:57:01,625][0m Trial 41 finished with value: 0.046938126323202364 and parameters: {'observation_period_num': 13, 'train_rates': 0.7850086315598848, 'learning_rate': 0.0004556293766798767, 'batch_size': 247, 'step_size': 14, 'gamma': 0.9270293921149949}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:57:25,604][0m Trial 42 finished with value: 0.047711615096515336 and parameters: {'observation_period_num': 20, 'train_rates': 0.7819699258902132, 'learning_rate': 0.0006830994634796985, 'batch_size': 242, 'step_size': 14, 'gamma': 0.9245885022474706}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:57:53,022][0m Trial 43 finished with value: 0.061716640204830975 and parameters: {'observation_period_num': 14, 'train_rates': 0.8097057462706596, 'learning_rate': 0.00048018515663450094, 'batch_size': 225, 'step_size': 13, 'gamma': 0.9466345960971152}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:58:15,321][0m Trial 44 finished with value: 0.18522618640410274 and parameters: {'observation_period_num': 195, 'train_rates': 0.7243804320619845, 'learning_rate': 0.0002229641403911833, 'batch_size': 204, 'step_size': 12, 'gamma': 0.8728190711866268}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:58:40,503][0m Trial 45 finished with value: 0.09498446986258209 and parameters: {'observation_period_num': 49, 'train_rates': 0.6559714272041954, 'learning_rate': 0.000770248364590079, 'batch_size': 183, 'step_size': 15, 'gamma': 0.9270476898555631}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:59:10,700][0m Trial 46 finished with value: 0.11597616768700622 and parameters: {'observation_period_num': 142, 'train_rates': 0.7873147127542924, 'learning_rate': 0.0001376318576317095, 'batch_size': 155, 'step_size': 14, 'gamma': 0.9029549533879769}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:59:30,750][0m Trial 47 finished with value: 0.9130994244403777 and parameters: {'observation_period_num': 29, 'train_rates': 0.6808044212259962, 'learning_rate': 6.812986264084457e-06, 'batch_size': 241, 'step_size': 15, 'gamma': 0.9741718071707808}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 19:59:52,231][0m Trial 48 finished with value: 0.04075935034538215 and parameters: {'observation_period_num': 5, 'train_rates': 0.8202480190142376, 'learning_rate': 0.0009622415316587219, 'batch_size': 244, 'step_size': 13, 'gamma': 0.918246452415228}. Best is trial 26 with value: 0.03630047499543369.[0m
[32m[I 2025-01-11 20:00:14,356][0m Trial 49 finished with value: 0.058830855251246546 and parameters: {'observation_period_num': 42, 'train_rates': 0.8154062642478486, 'learning_rate': 0.0009814150928524805, 'batch_size': 224, 'step_size': 13, 'gamma': 0.8522400084762186}. Best is trial 26 with value: 0.03630047499543369.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-11 20:00:14,364][0m A new study created in memory with name: no-name-bfecf221-f682-44b3-9ae2-e3df6057a5a2[0m
[32m[I 2025-01-11 20:00:43,123][0m Trial 0 finished with value: 0.13181597791716665 and parameters: {'observation_period_num': 158, 'train_rates': 0.821493402363149, 'learning_rate': 0.0007411932909968523, 'batch_size': 182, 'step_size': 2, 'gamma': 0.8246065927173345}. Best is trial 0 with value: 0.13181597791716665.[0m
Early stopping at epoch 96
[32m[I 2025-01-11 20:01:07,825][0m Trial 1 finished with value: 0.15785287666005016 and parameters: {'observation_period_num': 20, 'train_rates': 0.8454906440087593, 'learning_rate': 0.00010615806883963775, 'batch_size': 210, 'step_size': 1, 'gamma': 0.879896036870218}. Best is trial 0 with value: 0.13181597791716665.[0m
Early stopping at epoch 83
[32m[I 2025-01-11 20:01:28,058][0m Trial 2 finished with value: 0.3549581722966556 and parameters: {'observation_period_num': 226, 'train_rates': 0.7871245999559082, 'learning_rate': 0.00023574795030572934, 'batch_size': 204, 'step_size': 1, 'gamma': 0.8486499481319298}. Best is trial 0 with value: 0.13181597791716665.[0m
[32m[I 2025-01-11 20:04:49,249][0m Trial 3 finished with value: 0.11446000368167193 and parameters: {'observation_period_num': 232, 'train_rates': 0.8710373167003437, 'learning_rate': 1.689967495421219e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.8879893322059389}. Best is trial 3 with value: 0.11446000368167193.[0m
[32m[I 2025-01-11 20:06:17,918][0m Trial 4 finished with value: 0.09206205669694281 and parameters: {'observation_period_num': 14, 'train_rates': 0.7233165621937923, 'learning_rate': 1.5119466254857858e-05, 'batch_size': 48, 'step_size': 6, 'gamma': 0.7625944575101494}. Best is trial 4 with value: 0.09206205669694281.[0m
Early stopping at epoch 55
[32m[I 2025-01-11 20:06:35,832][0m Trial 5 finished with value: 2.188320377953032 and parameters: {'observation_period_num': 184, 'train_rates': 0.8732802000161229, 'learning_rate': 2.8952999146390833e-06, 'batch_size': 167, 'step_size': 1, 'gamma': 0.8184956737222757}. Best is trial 4 with value: 0.09206205669694281.[0m
[32m[I 2025-01-11 20:07:13,689][0m Trial 6 finished with value: 0.16642030821892553 and parameters: {'observation_period_num': 248, 'train_rates': 0.6409894789167856, 'learning_rate': 0.0003079135401543936, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9731700898115658}. Best is trial 4 with value: 0.09206205669694281.[0m
[32m[I 2025-01-11 20:08:23,207][0m Trial 7 finished with value: 0.33026130218058825 and parameters: {'observation_period_num': 213, 'train_rates': 0.9301613096741855, 'learning_rate': 4.352144865292326e-06, 'batch_size': 85, 'step_size': 9, 'gamma': 0.9483387560264095}. Best is trial 4 with value: 0.09206205669694281.[0m
[32m[I 2025-01-11 20:09:00,773][0m Trial 8 finished with value: 0.8137064574194737 and parameters: {'observation_period_num': 77, 'train_rates': 0.7877466320458109, 'learning_rate': 1.337865619439788e-06, 'batch_size': 174, 'step_size': 13, 'gamma': 0.958134105078344}. Best is trial 4 with value: 0.09206205669694281.[0m
[32m[I 2025-01-11 20:09:30,960][0m Trial 9 finished with value: 0.26888194388111697 and parameters: {'observation_period_num': 223, 'train_rates': 0.7231653009717833, 'learning_rate': 6.488063598179964e-05, 'batch_size': 186, 'step_size': 9, 'gamma': 0.9722195124500846}. Best is trial 4 with value: 0.09206205669694281.[0m
[32m[I 2025-01-11 20:12:08,712][0m Trial 10 finished with value: 0.08303753454564956 and parameters: {'observation_period_num': 9, 'train_rates': 0.6124389000529478, 'learning_rate': 1.5336317179223522e-05, 'batch_size': 28, 'step_size': 15, 'gamma': 0.7680899163309516}. Best is trial 10 with value: 0.08303753454564956.[0m
[32m[I 2025-01-11 20:15:49,544][0m Trial 11 finished with value: 0.07585223954876615 and parameters: {'observation_period_num': 10, 'train_rates': 0.6141082060909457, 'learning_rate': 1.484377936740792e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.752875643593651}. Best is trial 11 with value: 0.07585223954876615.[0m
[32m[I 2025-01-11 20:16:07,440][0m Trial 12 finished with value: 0.5861951543787938 and parameters: {'observation_period_num': 79, 'train_rates': 0.610704471764395, 'learning_rate': 9.698269273559397e-06, 'batch_size': 253, 'step_size': 15, 'gamma': 0.7531194251731371}. Best is trial 11 with value: 0.07585223954876615.[0m
[32m[I 2025-01-11 20:17:10,640][0m Trial 13 finished with value: 0.0887582628643375 and parameters: {'observation_period_num': 60, 'train_rates': 0.6708223734861523, 'learning_rate': 4.426088438215775e-05, 'batch_size': 71, 'step_size': 13, 'gamma': 0.7881013746068944}. Best is trial 11 with value: 0.07585223954876615.[0m
[32m[I 2025-01-11 20:21:01,271][0m Trial 14 finished with value: 0.18833619280397945 and parameters: {'observation_period_num': 115, 'train_rates': 0.6012562880784522, 'learning_rate': 6.898273716168545e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7883121050626438}. Best is trial 11 with value: 0.07585223954876615.[0m
[32m[I 2025-01-11 20:21:45,023][0m Trial 15 finished with value: 0.09692523464560508 and parameters: {'observation_period_num': 39, 'train_rates': 0.6908730501351213, 'learning_rate': 2.2958334533949312e-05, 'batch_size': 108, 'step_size': 12, 'gamma': 0.7870453549004391}. Best is trial 11 with value: 0.07585223954876615.[0m
[32m[I 2025-01-11 20:23:08,328][0m Trial 16 finished with value: 0.8062671941259633 and parameters: {'observation_period_num': 120, 'train_rates': 0.7401760914139173, 'learning_rate': 1.124523499733151e-06, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9112682151374167}. Best is trial 11 with value: 0.07585223954876615.[0m
[32m[I 2025-01-11 20:24:59,000][0m Trial 17 finished with value: 0.04969039040683571 and parameters: {'observation_period_num': 8, 'train_rates': 0.6591648884197442, 'learning_rate': 3.3461914736198765e-05, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8189954209112651}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:25:47,343][0m Trial 18 finished with value: 0.06789993345272764 and parameters: {'observation_period_num': 49, 'train_rates': 0.6684304347828901, 'learning_rate': 7.763861718428971e-05, 'batch_size': 96, 'step_size': 11, 'gamma': 0.8225033829506201}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:26:35,137][0m Trial 19 finished with value: 0.0889349470546471 and parameters: {'observation_period_num': 48, 'train_rates': 0.6759653732893766, 'learning_rate': 0.00010673796684797964, 'batch_size': 98, 'step_size': 12, 'gamma': 0.8422920523536251}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:27:11,148][0m Trial 20 finished with value: 0.10899406769166985 and parameters: {'observation_period_num': 94, 'train_rates': 0.7543910404192714, 'learning_rate': 4.627551412402366e-05, 'batch_size': 136, 'step_size': 10, 'gamma': 0.8118500956838639}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:28:46,649][0m Trial 21 finished with value: 0.05025285283700425 and parameters: {'observation_period_num': 34, 'train_rates': 0.6458410468126671, 'learning_rate': 0.00010252369874485764, 'batch_size': 45, 'step_size': 14, 'gamma': 0.8523371754069351}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:30:14,882][0m Trial 22 finished with value: 0.06297646562909984 and parameters: {'observation_period_num': 41, 'train_rates': 0.6595422761613611, 'learning_rate': 0.00019741164542766223, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8503560456489017}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:31:37,181][0m Trial 23 finished with value: 0.05587859280254484 and parameters: {'observation_period_num': 32, 'train_rates': 0.6495765814349769, 'learning_rate': 0.00021107665084012017, 'batch_size': 53, 'step_size': 13, 'gamma': 0.8554839365241717}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:32:42,508][0m Trial 24 finished with value: 0.08029698072316031 and parameters: {'observation_period_num': 32, 'train_rates': 0.7000980925777142, 'learning_rate': 0.00044822638748043015, 'batch_size': 71, 'step_size': 14, 'gamma': 0.9167756496229151}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:34:20,568][0m Trial 25 finished with value: 0.08031312149334113 and parameters: {'observation_period_num': 72, 'train_rates': 0.6357034878240527, 'learning_rate': 0.00016362639691567757, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8631388956890811}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:35:24,720][0m Trial 26 finished with value: 0.11584035510100212 and parameters: {'observation_period_num': 97, 'train_rates': 0.6435468655126095, 'learning_rate': 0.0007541551642978693, 'batch_size': 67, 'step_size': 6, 'gamma': 0.8989201741722594}. Best is trial 17 with value: 0.04969039040683571.[0m
[32m[I 2025-01-11 20:38:16,018][0m Trial 27 finished with value: 0.03863598849322345 and parameters: {'observation_period_num': 27, 'train_rates': 0.9876215029346394, 'learning_rate': 3.3352612778253515e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8660010342127795}. Best is trial 27 with value: 0.03863598849322345.[0m
[32m[I 2025-01-11 20:40:54,685][0m Trial 28 finished with value: 0.10603485442698002 and parameters: {'observation_period_num': 152, 'train_rates': 0.9865597146294783, 'learning_rate': 3.7745848270497985e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.8732861772365885}. Best is trial 27 with value: 0.03863598849322345.[0m
[32m[I 2025-01-11 20:41:44,601][0m Trial 29 finished with value: 0.15787331759929657 and parameters: {'observation_period_num': 150, 'train_rates': 0.982966056377151, 'learning_rate': 2.7390651693138033e-05, 'batch_size': 115, 'step_size': 14, 'gamma': 0.8331730534953409}. Best is trial 27 with value: 0.03863598849322345.[0m
[32m[I 2025-01-11 20:42:22,877][0m Trial 30 finished with value: 0.06573347396478452 and parameters: {'observation_period_num': 59, 'train_rates': 0.9348104969353251, 'learning_rate': 0.00044443600177957387, 'batch_size': 151, 'step_size': 11, 'gamma': 0.9258578132994504}. Best is trial 27 with value: 0.03863598849322345.[0m
[32m[I 2025-01-11 20:43:52,282][0m Trial 31 finished with value: 0.04175847736862532 and parameters: {'observation_period_num': 28, 'train_rates': 0.8186002873931759, 'learning_rate': 0.00013479332854679664, 'batch_size': 58, 'step_size': 13, 'gamma': 0.8613642135452101}. Best is trial 27 with value: 0.03863598849322345.[0m
[32m[I 2025-01-11 20:44:56,555][0m Trial 32 finished with value: 0.038220004067966766 and parameters: {'observation_period_num': 24, 'train_rates': 0.8216865385927166, 'learning_rate': 0.0001247296108864916, 'batch_size': 82, 'step_size': 12, 'gamma': 0.8879707738011363}. Best is trial 32 with value: 0.038220004067966766.[0m
[32m[I 2025-01-11 20:46:02,899][0m Trial 33 finished with value: 0.04188212162320901 and parameters: {'observation_period_num': 21, 'train_rates': 0.8271154315383357, 'learning_rate': 5.8022033858473135e-05, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8904515678872227}. Best is trial 32 with value: 0.038220004067966766.[0m
[32m[I 2025-01-11 20:47:04,658][0m Trial 34 finished with value: 0.03823100324763885 and parameters: {'observation_period_num': 22, 'train_rates': 0.8256290452636929, 'learning_rate': 0.00013674083731426086, 'batch_size': 85, 'step_size': 12, 'gamma': 0.8865625232138026}. Best is trial 32 with value: 0.038220004067966766.[0m
[32m[I 2025-01-11 20:48:05,434][0m Trial 35 finished with value: 0.05065594719180578 and parameters: {'observation_period_num': 60, 'train_rates': 0.863596336493398, 'learning_rate': 0.00012823455125788924, 'batch_size': 88, 'step_size': 10, 'gamma': 0.8780191504819772}. Best is trial 32 with value: 0.038220004067966766.[0m
[32m[I 2025-01-11 20:48:53,029][0m Trial 36 finished with value: 0.03919528417802612 and parameters: {'observation_period_num': 22, 'train_rates': 0.8169124497233089, 'learning_rate': 0.00032687352366461984, 'batch_size': 112, 'step_size': 12, 'gamma': 0.9021705662432843}. Best is trial 32 with value: 0.038220004067966766.[0m
[32m[I 2025-01-11 20:49:37,669][0m Trial 37 finished with value: 0.03800736044118874 and parameters: {'observation_period_num': 21, 'train_rates': 0.9067780662270939, 'learning_rate': 0.0003661626661246308, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9366972499118409}. Best is trial 37 with value: 0.03800736044118874.[0m
[32m[I 2025-01-11 20:50:20,389][0m Trial 38 finished with value: 0.03495053582123874 and parameters: {'observation_period_num': 5, 'train_rates': 0.9014803253373121, 'learning_rate': 0.0009401964580222394, 'batch_size': 133, 'step_size': 7, 'gamma': 0.933140327321245}. Best is trial 38 with value: 0.03495053582123874.[0m
[32m[I 2025-01-11 20:51:02,634][0m Trial 39 finished with value: 0.03571728030462026 and parameters: {'observation_period_num': 19, 'train_rates': 0.9083032558356561, 'learning_rate': 0.0009610451611357509, 'batch_size': 139, 'step_size': 7, 'gamma': 0.9378436882254947}. Best is trial 38 with value: 0.03495053582123874.[0m
[32m[I 2025-01-11 20:51:39,003][0m Trial 40 finished with value: 0.1549091027541594 and parameters: {'observation_period_num': 183, 'train_rates': 0.9048907873055098, 'learning_rate': 0.0009388341825510309, 'batch_size': 148, 'step_size': 7, 'gamma': 0.940701096802514}. Best is trial 38 with value: 0.03495053582123874.[0m
[32m[I 2025-01-11 20:52:23,518][0m Trial 41 finished with value: 0.03340959160115181 and parameters: {'observation_period_num': 5, 'train_rates': 0.8993877145945397, 'learning_rate': 0.0005691065107663882, 'batch_size': 128, 'step_size': 4, 'gamma': 0.9285395973556264}. Best is trial 41 with value: 0.03340959160115181.[0m
[32m[I 2025-01-11 20:53:07,788][0m Trial 42 finished with value: 0.03049914061051348 and parameters: {'observation_period_num': 9, 'train_rates': 0.8985212329534181, 'learning_rate': 0.0004978427781099174, 'batch_size': 130, 'step_size': 3, 'gamma': 0.9866610114629634}. Best is trial 42 with value: 0.03049914061051348.[0m
[32m[I 2025-01-11 20:53:52,288][0m Trial 43 finished with value: 0.028362736947512707 and parameters: {'observation_period_num': 5, 'train_rates': 0.8980022709729397, 'learning_rate': 0.0004810161824197297, 'batch_size': 127, 'step_size': 3, 'gamma': 0.978979050277555}. Best is trial 43 with value: 0.028362736947512707.[0m
[32m[I 2025-01-11 20:54:29,758][0m Trial 44 finished with value: 0.07612375169992447 and parameters: {'observation_period_num': 5, 'train_rates': 0.9529465339579026, 'learning_rate': 0.0005582714961496846, 'batch_size': 158, 'step_size': 3, 'gamma': 0.988522663148923}. Best is trial 43 with value: 0.028362736947512707.[0m
[32m[I 2025-01-11 20:55:14,078][0m Trial 45 finished with value: 0.030777389992927683 and parameters: {'observation_period_num': 5, 'train_rates': 0.9017449869943089, 'learning_rate': 0.0006528665061686633, 'batch_size': 128, 'step_size': 4, 'gamma': 0.9613202333512096}. Best is trial 43 with value: 0.028362736947512707.[0m
[32m[I 2025-01-11 20:55:44,303][0m Trial 46 finished with value: 0.04324303031508816 and parameters: {'observation_period_num': 5, 'train_rates': 0.8786825249059667, 'learning_rate': 0.000600307820392447, 'batch_size': 187, 'step_size': 4, 'gamma': 0.962267410932525}. Best is trial 43 with value: 0.028362736947512707.[0m
[32m[I 2025-01-11 20:56:28,516][0m Trial 47 finished with value: 0.03897885418851678 and parameters: {'observation_period_num': 47, 'train_rates': 0.8889159253361354, 'learning_rate': 0.0002699783174730169, 'batch_size': 123, 'step_size': 2, 'gamma': 0.9852260403682689}. Best is trial 43 with value: 0.028362736947512707.[0m
[32m[I 2025-01-11 20:57:01,097][0m Trial 48 finished with value: 0.027130463382317907 and parameters: {'observation_period_num': 14, 'train_rates': 0.8570174697785171, 'learning_rate': 0.0006965727363549433, 'batch_size': 172, 'step_size': 4, 'gamma': 0.972792474375392}. Best is trial 48 with value: 0.027130463382317907.[0m
[32m[I 2025-01-11 20:57:33,657][0m Trial 49 finished with value: 0.04309322304007682 and parameters: {'observation_period_num': 17, 'train_rates': 0.8502754358852196, 'learning_rate': 0.0005463932746910223, 'batch_size': 170, 'step_size': 4, 'gamma': 0.9752854775933}. Best is trial 48 with value: 0.027130463382317907.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-11 20:57:33,666][0m A new study created in memory with name: no-name-09491e62-79cd-4820-bc92-538505e13836[0m
[32m[I 2025-01-11 20:58:20,550][0m Trial 0 finished with value: 0.154453473839354 and parameters: {'observation_period_num': 155, 'train_rates': 0.7312000932709024, 'learning_rate': 3.790573667600584e-05, 'batch_size': 102, 'step_size': 10, 'gamma': 0.8467311939548016}. Best is trial 0 with value: 0.154453473839354.[0m
[32m[I 2025-01-11 20:58:53,741][0m Trial 1 finished with value: 0.08122139439580758 and parameters: {'observation_period_num': 118, 'train_rates': 0.7963320552454068, 'learning_rate': 7.24856170796454e-05, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9142441851423159}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 20:59:47,138][0m Trial 2 finished with value: 0.4929567745565313 and parameters: {'observation_period_num': 240, 'train_rates': 0.7498730508369167, 'learning_rate': 2.7781832276243654e-05, 'batch_size': 87, 'step_size': 2, 'gamma': 0.8378333257206395}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:02:49,217][0m Trial 3 finished with value: 0.18770201261429226 and parameters: {'observation_period_num': 209, 'train_rates': 0.6901049563794971, 'learning_rate': 7.945377349368904e-06, 'batch_size': 23, 'step_size': 7, 'gamma': 0.9739666952628455}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:03:20,424][0m Trial 4 finished with value: 0.1512459553397485 and parameters: {'observation_period_num': 223, 'train_rates': 0.748959966045357, 'learning_rate': 0.0003606802409739346, 'batch_size': 157, 'step_size': 10, 'gamma': 0.8175577663265668}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:07:08,722][0m Trial 5 finished with value: 0.598541970547019 and parameters: {'observation_period_num': 212, 'train_rates': 0.7276682491741855, 'learning_rate': 1.4371360598224775e-06, 'batch_size': 19, 'step_size': 3, 'gamma': 0.927916025539245}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:07:56,335][0m Trial 6 finished with value: 0.11095422157059008 and parameters: {'observation_period_num': 208, 'train_rates': 0.6906898470206937, 'learning_rate': 0.00045468751068715457, 'batch_size': 92, 'step_size': 3, 'gamma': 0.8978531289521827}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:08:18,442][0m Trial 7 finished with value: 0.803380421163507 and parameters: {'observation_period_num': 136, 'train_rates': 0.8176829242019441, 'learning_rate': 2.234781043355121e-06, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8528029341366026}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:08:39,826][0m Trial 8 finished with value: 0.09075462232045252 and parameters: {'observation_period_num': 116, 'train_rates': 0.7039802888822911, 'learning_rate': 0.0006534753368739753, 'batch_size': 222, 'step_size': 8, 'gamma': 0.9117075490468609}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:09:17,906][0m Trial 9 finished with value: 0.17472779876720212 and parameters: {'observation_period_num': 113, 'train_rates': 0.7458347316211593, 'learning_rate': 4.4744987453826007e-05, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7993689147658017}. Best is trial 1 with value: 0.08122139439580758.[0m
[32m[I 2025-01-11 21:09:51,978][0m Trial 10 finished with value: 0.05085307803490888 and parameters: {'observation_period_num': 36, 'train_rates': 0.921318734978295, 'learning_rate': 0.00012264851989692986, 'batch_size': 176, 'step_size': 15, 'gamma': 0.7694642216774361}. Best is trial 10 with value: 0.05085307803490888.[0m
[32m[I 2025-01-11 21:10:25,592][0m Trial 11 finished with value: 0.05906761810183525 and parameters: {'observation_period_num': 27, 'train_rates': 0.9412243118495683, 'learning_rate': 8.490746032634657e-05, 'batch_size': 182, 'step_size': 15, 'gamma': 0.7726222252691852}. Best is trial 10 with value: 0.05085307803490888.[0m
[32m[I 2025-01-11 21:10:58,992][0m Trial 12 finished with value: 0.051435112953186035 and parameters: {'observation_period_num': 6, 'train_rates': 0.958515470418425, 'learning_rate': 0.00016699309049236546, 'batch_size': 188, 'step_size': 15, 'gamma': 0.7537267426088468}. Best is trial 10 with value: 0.05085307803490888.[0m
[32m[I 2025-01-11 21:11:30,472][0m Trial 13 finished with value: 0.036496423184871674 and parameters: {'observation_period_num': 6, 'train_rates': 0.985946907967102, 'learning_rate': 0.00017954575474608627, 'batch_size': 200, 'step_size': 13, 'gamma': 0.7579657178453283}. Best is trial 13 with value: 0.036496423184871674.[0m
[32m[I 2025-01-11 21:11:58,677][0m Trial 14 finished with value: 0.06240508435814647 and parameters: {'observation_period_num': 58, 'train_rates': 0.8889410365901819, 'learning_rate': 0.0001740068513881215, 'batch_size': 205, 'step_size': 13, 'gamma': 0.7945021594448615}. Best is trial 13 with value: 0.036496423184871674.[0m
[32m[I 2025-01-11 21:12:24,552][0m Trial 15 finished with value: 0.35550737380981445 and parameters: {'observation_period_num': 64, 'train_rates': 0.9888441767791859, 'learning_rate': 1.4169192699859029e-05, 'batch_size': 253, 'step_size': 12, 'gamma': 0.7530492606848247}. Best is trial 13 with value: 0.036496423184871674.[0m
[32m[I 2025-01-11 21:12:50,836][0m Trial 16 finished with value: 0.05170337649735999 and parameters: {'observation_period_num': 55, 'train_rates': 0.8703736324622158, 'learning_rate': 0.00021357219850499554, 'batch_size': 220, 'step_size': 13, 'gamma': 0.7882033800609385}. Best is trial 13 with value: 0.036496423184871674.[0m
[32m[I 2025-01-11 21:13:16,933][0m Trial 17 finished with value: 0.10773128618916067 and parameters: {'observation_period_num': 83, 'train_rates': 0.6374094287733145, 'learning_rate': 0.00010969449947204011, 'batch_size': 171, 'step_size': 13, 'gamma': 0.8801014614912681}. Best is trial 13 with value: 0.036496423184871674.[0m
[32m[I 2025-01-11 21:14:01,322][0m Trial 18 finished with value: 0.025290102545856213 and parameters: {'observation_period_num': 10, 'train_rates': 0.9002463015798082, 'learning_rate': 0.0008066377216795653, 'batch_size': 129, 'step_size': 5, 'gamma': 0.8241469609720184}. Best is trial 18 with value: 0.025290102545856213.[0m
[32m[I 2025-01-11 21:15:43,783][0m Trial 19 finished with value: 0.02367872504941414 and parameters: {'observation_period_num': 9, 'train_rates': 0.8673154999476134, 'learning_rate': 0.000966480027074729, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8273171140243673}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:17:19,871][0m Trial 20 finished with value: 0.1450433701204793 and parameters: {'observation_period_num': 173, 'train_rates': 0.8510998967300983, 'learning_rate': 0.0009115326492814659, 'batch_size': 52, 'step_size': 6, 'gamma': 0.8213345234847523}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:18:49,708][0m Trial 21 finished with value: 0.028075763942116368 and parameters: {'observation_period_num': 9, 'train_rates': 0.9068805572555032, 'learning_rate': 0.0009844645230604548, 'batch_size': 62, 'step_size': 5, 'gamma': 0.8685332030287526}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:20:23,788][0m Trial 22 finished with value: 0.04657847378467849 and parameters: {'observation_period_num': 34, 'train_rates': 0.9019075169342499, 'learning_rate': 0.0009568830663030126, 'batch_size': 58, 'step_size': 5, 'gamma': 0.8711678032505186}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:21:48,392][0m Trial 23 finished with value: 0.053564545589134746 and parameters: {'observation_period_num': 86, 'train_rates': 0.839595843453235, 'learning_rate': 0.0003633555562594863, 'batch_size': 61, 'step_size': 4, 'gamma': 0.82272677649587}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:22:34,869][0m Trial 24 finished with value: 0.033662571988507 and parameters: {'observation_period_num': 22, 'train_rates': 0.9123958715655793, 'learning_rate': 0.0004537037317373436, 'batch_size': 122, 'step_size': 8, 'gamma': 0.8593992103800828}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:24:41,561][0m Trial 25 finished with value: 0.045570691426595054 and parameters: {'observation_period_num': 47, 'train_rates': 0.8699730183913462, 'learning_rate': 0.0009779180182273865, 'batch_size': 42, 'step_size': 5, 'gamma': 0.8814257966504708}. Best is trial 19 with value: 0.02367872504941414.[0m
Early stopping at epoch 67
[32m[I 2025-01-11 21:25:30,423][0m Trial 26 finished with value: 0.1670768627679193 and parameters: {'observation_period_num': 82, 'train_rates': 0.9465285363581434, 'learning_rate': 0.0002733970978663588, 'batch_size': 79, 'step_size': 1, 'gamma': 0.8299501872262586}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:26:20,009][0m Trial 27 finished with value: 0.028873421078857264 and parameters: {'observation_period_num': 16, 'train_rates': 0.7909929682626504, 'learning_rate': 0.0006184036323900792, 'batch_size': 105, 'step_size': 4, 'gamma': 0.9471946592389595}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:27:29,774][0m Trial 28 finished with value: 0.3998688298801915 and parameters: {'observation_period_num': 69, 'train_rates': 0.8348660586826557, 'learning_rate': 4.2297536008884325e-06, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8093680352448301}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:28:18,450][0m Trial 29 finished with value: 0.13019495779868737 and parameters: {'observation_period_num': 168, 'train_rates': 0.8664335051203409, 'learning_rate': 0.0005910088749383914, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8477800351841547}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:29:00,341][0m Trial 30 finished with value: 0.08902809496077013 and parameters: {'observation_period_num': 41, 'train_rates': 0.9270646261480828, 'learning_rate': 4.962654381336175e-05, 'batch_size': 138, 'step_size': 5, 'gamma': 0.8415770907990285}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:29:50,684][0m Trial 31 finished with value: 0.03531582923488149 and parameters: {'observation_period_num': 18, 'train_rates': 0.7846152567348426, 'learning_rate': 0.0006554166462979359, 'batch_size': 102, 'step_size': 4, 'gamma': 0.9643227252544728}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:31:54,694][0m Trial 32 finished with value: 0.028909215149763613 and parameters: {'observation_period_num': 15, 'train_rates': 0.7949127937029986, 'learning_rate': 0.00031654594411934184, 'batch_size': 40, 'step_size': 3, 'gamma': 0.945514988540708}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:32:34,628][0m Trial 33 finished with value: 0.06155377257437933 and parameters: {'observation_period_num': 22, 'train_rates': 0.8926913872811912, 'learning_rate': 0.0006119661652342158, 'batch_size': 141, 'step_size': 1, 'gamma': 0.8890711093803227}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:33:43,287][0m Trial 34 finished with value: 0.05020943033994864 and parameters: {'observation_period_num': 45, 'train_rates': 0.8102889173827955, 'learning_rate': 0.0009424524060294884, 'batch_size': 75, 'step_size': 6, 'gamma': 0.8643341439595325}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:34:29,169][0m Trial 35 finished with value: 0.0638318068314866 and parameters: {'observation_period_num': 8, 'train_rates': 0.7795333355270254, 'learning_rate': 1.7545510706181886e-05, 'batch_size': 113, 'step_size': 4, 'gamma': 0.9427236768978285}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:37:08,102][0m Trial 36 finished with value: 0.05112425625324249 and parameters: {'observation_period_num': 33, 'train_rates': 0.8286010146266239, 'learning_rate': 0.00047258270738824285, 'batch_size': 32, 'step_size': 2, 'gamma': 0.8995331738152591}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:37:59,921][0m Trial 37 finished with value: 0.06481728268071302 and parameters: {'observation_period_num': 100, 'train_rates': 0.7710852495520578, 'learning_rate': 0.00024177973547463423, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8318162318747157}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:38:37,965][0m Trial 38 finished with value: 0.05294226109981537 and parameters: {'observation_period_num': 25, 'train_rates': 0.9654047120956359, 'learning_rate': 0.00037739428594590007, 'batch_size': 161, 'step_size': 2, 'gamma': 0.9811035832325521}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:39:53,051][0m Trial 39 finished with value: 0.10358462423220816 and parameters: {'observation_period_num': 143, 'train_rates': 0.8819579895040704, 'learning_rate': 0.0006922251371979551, 'batch_size': 69, 'step_size': 4, 'gamma': 0.9194104221705715}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:40:42,199][0m Trial 40 finished with value: 0.07121385186980851 and parameters: {'observation_period_num': 70, 'train_rates': 0.6319342520850434, 'learning_rate': 0.0005061952752519923, 'batch_size': 87, 'step_size': 7, 'gamma': 0.8475773564164234}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:43:23,267][0m Trial 41 finished with value: 0.03408469151884727 and parameters: {'observation_period_num': 13, 'train_rates': 0.8020220533837388, 'learning_rate': 0.00030988831463470525, 'batch_size': 31, 'step_size': 3, 'gamma': 0.9519744199156804}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:45:08,700][0m Trial 42 finished with value: 0.0292066317065127 and parameters: {'observation_period_num': 15, 'train_rates': 0.7647775290783765, 'learning_rate': 0.0007251029468016635, 'batch_size': 46, 'step_size': 3, 'gamma': 0.9449602919011425}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:49:55,946][0m Trial 43 finished with value: 0.1885447636909353 and parameters: {'observation_period_num': 45, 'train_rates': 0.8569858086549411, 'learning_rate': 0.00032218877650656093, 'batch_size': 18, 'step_size': 5, 'gamma': 0.929733311896251}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:51:57,843][0m Trial 44 finished with value: 0.04303801025369546 and parameters: {'observation_period_num': 28, 'train_rates': 0.7979283543812394, 'learning_rate': 0.0004500594622500704, 'batch_size': 41, 'step_size': 2, 'gamma': 0.9891467821211459}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:52:34,074][0m Trial 45 finished with value: 0.1287407450834099 and parameters: {'observation_period_num': 245, 'train_rates': 0.7285068418629423, 'learning_rate': 0.0007499041551556737, 'batch_size': 125, 'step_size': 3, 'gamma': 0.9012591486253438}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:55:23,786][0m Trial 46 finished with value: 0.025162947336141776 and parameters: {'observation_period_num': 8, 'train_rates': 0.8194070837657853, 'learning_rate': 0.0005132074935511486, 'batch_size': 30, 'step_size': 6, 'gamma': 0.8033443797668589}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:58:26,860][0m Trial 47 finished with value: 0.03197285206897682 and parameters: {'observation_period_num': 5, 'train_rates': 0.8258915967089605, 'learning_rate': 6.348807884012552e-05, 'batch_size': 28, 'step_size': 6, 'gamma': 0.8054504092859527}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 21:59:05,022][0m Trial 48 finished with value: 0.06363741901631538 and parameters: {'observation_period_num': 52, 'train_rates': 0.9105367239360667, 'learning_rate': 0.0001223438098586264, 'batch_size': 151, 'step_size': 8, 'gamma': 0.7845242273268646}. Best is trial 19 with value: 0.02367872504941414.[0m
[32m[I 2025-01-11 22:00:36,061][0m Trial 49 finished with value: 0.0426720467954874 and parameters: {'observation_period_num': 38, 'train_rates': 0.9315690057004247, 'learning_rate': 0.000532133773938231, 'batch_size': 62, 'step_size': 9, 'gamma': 0.8131443624256874}. Best is trial 19 with value: 0.02367872504941414.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-11 22:00:36,071][0m A new study created in memory with name: no-name-1c5137b3-ca20-464e-8854-af5eb413de35[0m
[32m[I 2025-01-11 22:01:11,410][0m Trial 0 finished with value: 1.2247987495826893 and parameters: {'observation_period_num': 117, 'train_rates': 0.6557162603490981, 'learning_rate': 2.5886154252786624e-06, 'batch_size': 130, 'step_size': 2, 'gamma': 0.9111842749756625}. Best is trial 0 with value: 1.2247987495826893.[0m
[32m[I 2025-01-11 22:01:35,370][0m Trial 1 finished with value: 1.222434726504772 and parameters: {'observation_period_num': 182, 'train_rates': 0.6202939033109953, 'learning_rate': 1.135741844092134e-06, 'batch_size': 180, 'step_size': 2, 'gamma': 0.9064325945107209}. Best is trial 1 with value: 1.222434726504772.[0m
Early stopping at epoch 47
[32m[I 2025-01-11 22:01:47,597][0m Trial 2 finished with value: 1.3199007511138916 and parameters: {'observation_period_num': 180, 'train_rates': 0.9251472078624217, 'learning_rate': 7.069852354277338e-06, 'batch_size': 239, 'step_size': 1, 'gamma': 0.7951239085420618}. Best is trial 1 with value: 1.222434726504772.[0m
[32m[I 2025-01-11 22:02:22,380][0m Trial 3 finished with value: 0.08124037646462745 and parameters: {'observation_period_num': 133, 'train_rates': 0.812737528354498, 'learning_rate': 0.0007003195559907951, 'batch_size': 145, 'step_size': 11, 'gamma': 0.8302670331623231}. Best is trial 3 with value: 0.08124037646462745.[0m
[32m[I 2025-01-11 22:03:18,594][0m Trial 4 finished with value: 0.047747837343052324 and parameters: {'observation_period_num': 36, 'train_rates': 0.7025468675446858, 'learning_rate': 0.0003711420038109041, 'batch_size': 85, 'step_size': 6, 'gamma': 0.8648524725703726}. Best is trial 4 with value: 0.047747837343052324.[0m
[32m[I 2025-01-11 22:08:00,801][0m Trial 5 finished with value: 0.11480566732181136 and parameters: {'observation_period_num': 150, 'train_rates': 0.9541388146665286, 'learning_rate': 2.192329314430569e-05, 'batch_size': 19, 'step_size': 6, 'gamma': 0.7596133685629266}. Best is trial 4 with value: 0.047747837343052324.[0m
Early stopping at epoch 55
[32m[I 2025-01-11 22:09:11,406][0m Trial 6 finished with value: 0.5481402817349197 and parameters: {'observation_period_num': 124, 'train_rates': 0.6725089295405171, 'learning_rate': 1.1246597963464189e-05, 'batch_size': 34, 'step_size': 1, 'gamma': 0.8030456296961078}. Best is trial 4 with value: 0.047747837343052324.[0m
[32m[I 2025-01-11 22:09:44,997][0m Trial 7 finished with value: 0.15147860970209165 and parameters: {'observation_period_num': 58, 'train_rates': 0.610819971861239, 'learning_rate': 9.698074783355381e-05, 'batch_size': 128, 'step_size': 10, 'gamma': 0.7775093047334166}. Best is trial 4 with value: 0.047747837343052324.[0m
[32m[I 2025-01-11 22:13:28,822][0m Trial 8 finished with value: 0.08003824270492771 and parameters: {'observation_period_num': 69, 'train_rates': 0.8049799017312593, 'learning_rate': 5.6385753689041e-06, 'batch_size': 22, 'step_size': 13, 'gamma': 0.9084220217162153}. Best is trial 4 with value: 0.047747837343052324.[0m
[32m[I 2025-01-11 22:13:54,017][0m Trial 9 finished with value: 0.07728027229785253 and parameters: {'observation_period_num': 90, 'train_rates': 0.7496730063048547, 'learning_rate': 0.00041591471084782524, 'batch_size': 212, 'step_size': 11, 'gamma': 0.8517431079036655}. Best is trial 4 with value: 0.047747837343052324.[0m
[32m[I 2025-01-11 22:14:53,734][0m Trial 10 finished with value: 0.03601257543937833 and parameters: {'observation_period_num': 7, 'train_rates': 0.7401672141675437, 'learning_rate': 0.00011298902153630337, 'batch_size': 83, 'step_size': 6, 'gamma': 0.9559071601968273}. Best is trial 10 with value: 0.03601257543937833.[0m
[32m[I 2025-01-11 22:15:46,902][0m Trial 11 finished with value: 0.044108257323275994 and parameters: {'observation_period_num': 8, 'train_rates': 0.7328434707038997, 'learning_rate': 0.00010953060478224516, 'batch_size': 90, 'step_size': 6, 'gamma': 0.9777732841013684}. Best is trial 10 with value: 0.03601257543937833.[0m
[32m[I 2025-01-11 22:16:56,656][0m Trial 12 finished with value: 0.030451175410832677 and parameters: {'observation_period_num': 6, 'train_rates': 0.857503511435182, 'learning_rate': 8.160550561583853e-05, 'batch_size': 78, 'step_size': 6, 'gamma': 0.9871163985047021}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:18:10,416][0m Trial 13 finished with value: 0.03264745642335245 and parameters: {'observation_period_num': 17, 'train_rates': 0.870907097891546, 'learning_rate': 0.00010568878226201262, 'batch_size': 74, 'step_size': 8, 'gamma': 0.9842584012919555}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:19:27,233][0m Trial 14 finished with value: 0.11417682011084385 and parameters: {'observation_period_num': 252, 'train_rates': 0.8771877472708814, 'learning_rate': 4.519468340779253e-05, 'batch_size': 64, 'step_size': 8, 'gamma': 0.9855384055447454}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:21:06,460][0m Trial 15 finished with value: 0.11538658142089844 and parameters: {'observation_period_num': 39, 'train_rates': 0.873105386083323, 'learning_rate': 5.471555912234072e-05, 'batch_size': 54, 'step_size': 8, 'gamma': 0.9461733922885639}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:21:57,445][0m Trial 16 finished with value: 0.0324259433832209 and parameters: {'observation_period_num': 31, 'train_rates': 0.8788446920894514, 'learning_rate': 0.00020254709564452417, 'batch_size': 109, 'step_size': 4, 'gamma': 0.9395084292396022}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:22:50,900][0m Trial 17 finished with value: 0.06260893493890762 and parameters: {'observation_period_num': 81, 'train_rates': 0.9865193953091315, 'learning_rate': 0.0002942510758467271, 'batch_size': 110, 'step_size': 3, 'gamma': 0.94092973033355}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:23:23,925][0m Trial 18 finished with value: 0.04025055389655264 and parameters: {'observation_period_num': 39, 'train_rates': 0.823776116672689, 'learning_rate': 0.00023347842561315308, 'batch_size': 165, 'step_size': 4, 'gamma': 0.887050574575064}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:24:16,538][0m Trial 19 finished with value: 0.08838242119735609 and parameters: {'observation_period_num': 101, 'train_rates': 0.9040323420310011, 'learning_rate': 0.0009667930296506327, 'batch_size': 105, 'step_size': 4, 'gamma': 0.9589620165310947}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:26:01,861][0m Trial 20 finished with value: 0.05526231344932223 and parameters: {'observation_period_num': 57, 'train_rates': 0.8471945294788293, 'learning_rate': 2.5699288995342867e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.927607382044293}. Best is trial 12 with value: 0.030451175410832677.[0m
[32m[I 2025-01-11 22:27:19,486][0m Trial 21 finished with value: 0.02425072980193025 and parameters: {'observation_period_num': 5, 'train_rates': 0.8611790711947575, 'learning_rate': 0.00015538563810491506, 'batch_size': 70, 'step_size': 8, 'gamma': 0.9893871089912054}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:28:07,506][0m Trial 22 finished with value: 0.04089402381656906 and parameters: {'observation_period_num': 26, 'train_rates': 0.7773774213087052, 'learning_rate': 0.00017197471265893838, 'batch_size': 108, 'step_size': 5, 'gamma': 0.9662823397426541}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:30:01,997][0m Trial 23 finished with value: 0.034418324290802986 and parameters: {'observation_period_num': 6, 'train_rates': 0.9216113730242634, 'learning_rate': 6.346796668786683e-05, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9326043484670445}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:30:34,577][0m Trial 24 finished with value: 0.042587086124628346 and parameters: {'observation_period_num': 49, 'train_rates': 0.8342723289054923, 'learning_rate': 0.0001638365894474032, 'batch_size': 163, 'step_size': 7, 'gamma': 0.986958534115293}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:31:20,632][0m Trial 25 finished with value: 0.07168658124723645 and parameters: {'observation_period_num': 26, 'train_rates': 0.7833968077464607, 'learning_rate': 1.64443261714515e-05, 'batch_size': 112, 'step_size': 4, 'gamma': 0.965491577984564}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:32:33,360][0m Trial 26 finished with value: 0.11598739321727479 and parameters: {'observation_period_num': 229, 'train_rates': 0.8976618888791181, 'learning_rate': 0.0005643068940177258, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8863622786199403}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:33:35,699][0m Trial 27 finished with value: 0.10571766644716263 and parameters: {'observation_period_num': 70, 'train_rates': 0.951321645411724, 'learning_rate': 3.5995060572492e-05, 'batch_size': 94, 'step_size': 5, 'gamma': 0.9243658851467667}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:34:18,572][0m Trial 28 finished with value: 0.03509891996963309 and parameters: {'observation_period_num': 23, 'train_rates': 0.8510628898666506, 'learning_rate': 0.00020532914004333625, 'batch_size': 125, 'step_size': 7, 'gamma': 0.9672139379395064}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:34:58,996][0m Trial 29 finished with value: 0.05140967200525472 and parameters: {'observation_period_num': 46, 'train_rates': 0.8950174751401287, 'learning_rate': 8.255415585501868e-05, 'batch_size': 143, 'step_size': 3, 'gamma': 0.9482185792712813}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:37:08,428][0m Trial 30 finished with value: 0.09207562758968003 and parameters: {'observation_period_num': 104, 'train_rates': 0.8479069643191232, 'learning_rate': 0.00015304172749030294, 'batch_size': 39, 'step_size': 9, 'gamma': 0.9199436802243703}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:38:22,079][0m Trial 31 finished with value: 0.034367782364367806 and parameters: {'observation_period_num': 23, 'train_rates': 0.8710795308136529, 'learning_rate': 7.141712941256253e-05, 'batch_size': 74, 'step_size': 7, 'gamma': 0.9847145738449996}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:39:48,082][0m Trial 32 finished with value: 0.0340135462069884 and parameters: {'observation_period_num': 17, 'train_rates': 0.8637390236733714, 'learning_rate': 0.000290238095902411, 'batch_size': 63, 'step_size': 9, 'gamma': 0.9716350045316163}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:41:01,644][0m Trial 33 finished with value: 0.03127866629511118 and parameters: {'observation_period_num': 6, 'train_rates': 0.9253170829708917, 'learning_rate': 0.000139078647962099, 'batch_size': 77, 'step_size': 8, 'gamma': 0.9882375796741181}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:42:03,610][0m Trial 34 finished with value: 0.030485018325305126 and parameters: {'observation_period_num': 6, 'train_rates': 0.9420765936653664, 'learning_rate': 0.0004966718110178489, 'batch_size': 94, 'step_size': 5, 'gamma': 0.8942978876522422}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:43:06,264][0m Trial 35 finished with value: 0.09467709809541702 and parameters: {'observation_period_num': 157, 'train_rates': 0.989763432605112, 'learning_rate': 0.0004622876429348135, 'batch_size': 92, 'step_size': 5, 'gamma': 0.8308782495899549}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:43:31,820][0m Trial 36 finished with value: 0.039314426481723785 and parameters: {'observation_period_num': 5, 'train_rates': 0.953302650256265, 'learning_rate': 0.0008038294362108596, 'batch_size': 254, 'step_size': 7, 'gamma': 0.8817674238740606}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:46:00,799][0m Trial 37 finished with value: 0.345383766350023 and parameters: {'observation_period_num': 195, 'train_rates': 0.9235615297346833, 'learning_rate': 1.9678021547639234e-06, 'batch_size': 35, 'step_size': 12, 'gamma': 0.9012907509437258}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:46:46,847][0m Trial 38 finished with value: 0.07657576443126358 and parameters: {'observation_period_num': 59, 'train_rates': 0.9318781302518433, 'learning_rate': 0.0002963929868036141, 'batch_size': 127, 'step_size': 2, 'gamma': 0.8428891176981873}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:48:27,426][0m Trial 39 finished with value: 0.05275847011658012 and parameters: {'observation_period_num': 37, 'train_rates': 0.9374343041847021, 'learning_rate': 0.0005609664002376448, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9542714827063272}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:48:55,787][0m Trial 40 finished with value: 0.1712510664641422 and parameters: {'observation_period_num': 71, 'train_rates': 0.905072222704248, 'learning_rate': 4.516872613265765e-05, 'batch_size': 203, 'step_size': 6, 'gamma': 0.8106246787238928}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:49:55,707][0m Trial 41 finished with value: 0.04730854900553823 and parameters: {'observation_period_num': 30, 'train_rates': 0.9592052077409402, 'learning_rate': 0.0001413928216826234, 'batch_size': 99, 'step_size': 3, 'gamma': 0.9372265594252591}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:50:51,233][0m Trial 42 finished with value: 0.03717987346753394 and parameters: {'observation_period_num': 16, 'train_rates': 0.6396762861885459, 'learning_rate': 0.0002137531547643111, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8998576131215289}. Best is trial 21 with value: 0.02425072980193025.[0m
[32m[I 2025-01-11 22:52:01,277][0m Trial 43 finished with value: 0.022680423182851458 and parameters: {'observation_period_num': 5, 'train_rates': 0.9681185453379287, 'learning_rate': 0.00036899359171248724, 'batch_size': 84, 'step_size': 8, 'gamma': 0.9747892015067233}. Best is trial 43 with value: 0.022680423182851458.[0m
[32m[I 2025-01-11 22:53:14,539][0m Trial 44 finished with value: 0.021930959075689316 and parameters: {'observation_period_num': 6, 'train_rates': 0.9754342431368234, 'learning_rate': 0.00034539161049998825, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9767738170623886}. Best is trial 44 with value: 0.021930959075689316.[0m
[32m[I 2025-01-11 22:54:04,361][0m Trial 45 finished with value: 0.18529540300369263 and parameters: {'observation_period_num': 50, 'train_rates': 0.97655348265145, 'learning_rate': 0.00037436136308301903, 'batch_size': 120, 'step_size': 11, 'gamma': 0.9714157491448152}. Best is trial 44 with value: 0.021930959075689316.[0m
[32m[I 2025-01-11 22:58:35,374][0m Trial 46 finished with value: 0.03807816747575998 and parameters: {'observation_period_num': 16, 'train_rates': 0.971623156386064, 'learning_rate': 0.0006706914656435363, 'batch_size': 21, 'step_size': 9, 'gamma': 0.9756466219359259}. Best is trial 44 with value: 0.021930959075689316.[0m
[32m[I 2025-01-11 22:59:43,393][0m Trial 47 finished with value: 0.02398234212173606 and parameters: {'observation_period_num': 14, 'train_rates': 0.964017717259207, 'learning_rate': 0.000525254758071632, 'batch_size': 86, 'step_size': 7, 'gamma': 0.8653358875230469}. Best is trial 44 with value: 0.021930959075689316.[0m
[32m[I 2025-01-11 23:00:36,320][0m Trial 48 finished with value: 0.12985930832738649 and parameters: {'observation_period_num': 148, 'train_rates': 0.7022089108420437, 'learning_rate': 0.00034233989196415785, 'batch_size': 85, 'step_size': 8, 'gamma': 0.8733232593318209}. Best is trial 44 with value: 0.021930959075689316.[0m
[32m[I 2025-01-11 23:02:09,622][0m Trial 49 finished with value: 0.031164906919002533 and parameters: {'observation_period_num': 16, 'train_rates': 0.9678969225469415, 'learning_rate': 0.0007300413107377017, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8559811104399576}. Best is trial 44 with value: 0.021930959075689316.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-11 23:02:09,633][0m A new study created in memory with name: no-name-d3985fb2-4d9b-494c-9743-139c9004c34e[0m
[32m[I 2025-01-11 23:02:32,428][0m Trial 0 finished with value: 0.04791111157288249 and parameters: {'observation_period_num': 97, 'train_rates': 0.8175470068917701, 'learning_rate': 0.0007764547306085727, 'batch_size': 235, 'step_size': 9, 'gamma': 0.8612642828539411}. Best is trial 0 with value: 0.04791111157288249.[0m
[32m[I 2025-01-11 23:06:16,068][0m Trial 1 finished with value: 0.08777420971599327 and parameters: {'observation_period_num': 135, 'train_rates': 0.7902820896193812, 'learning_rate': 0.0001272661243273116, 'batch_size': 21, 'step_size': 7, 'gamma': 0.7546302910154047}. Best is trial 0 with value: 0.04791111157288249.[0m
[32m[I 2025-01-11 23:07:36,615][0m Trial 2 finished with value: 0.03381523662278441 and parameters: {'observation_period_num': 12, 'train_rates': 0.9002428577679074, 'learning_rate': 6.620841806072998e-05, 'batch_size': 68, 'step_size': 13, 'gamma': 0.8089583429398345}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:08:07,041][0m Trial 3 finished with value: 0.07196996637047773 and parameters: {'observation_period_num': 25, 'train_rates': 0.8867440502807431, 'learning_rate': 5.9559150870205885e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.812608832777241}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:08:35,322][0m Trial 4 finished with value: 0.982510108696787 and parameters: {'observation_period_num': 12, 'train_rates': 0.8644992703876755, 'learning_rate': 1.1489426030603898e-06, 'batch_size': 203, 'step_size': 3, 'gamma': 0.8097909152565598}. Best is trial 2 with value: 0.03381523662278441.[0m
Early stopping at epoch 99
[32m[I 2025-01-11 23:09:29,636][0m Trial 5 finished with value: 0.6202599827340032 and parameters: {'observation_period_num': 190, 'train_rates': 0.8940561992312229, 'learning_rate': 8.024649848154158e-06, 'batch_size': 98, 'step_size': 2, 'gamma': 0.8038843660485522}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:10:01,094][0m Trial 6 finished with value: 0.11359776593457949 and parameters: {'observation_period_num': 21, 'train_rates': 0.6010897160356076, 'learning_rate': 2.935161342429298e-05, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8594373350113766}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:11:48,237][0m Trial 7 finished with value: 0.043208743207419985 and parameters: {'observation_period_num': 12, 'train_rates': 0.8363479167843615, 'learning_rate': 1.4906005174296692e-05, 'batch_size': 49, 'step_size': 7, 'gamma': 0.9573707691475419}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:12:28,049][0m Trial 8 finished with value: 0.06978970379480776 and parameters: {'observation_period_num': 70, 'train_rates': 0.944941561659444, 'learning_rate': 0.0001533919010011994, 'batch_size': 150, 'step_size': 8, 'gamma': 0.7647108462333342}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:12:58,410][0m Trial 9 finished with value: 0.10942726064295996 and parameters: {'observation_period_num': 33, 'train_rates': 0.8922043704408399, 'learning_rate': 2.422885101225191e-05, 'batch_size': 192, 'step_size': 6, 'gamma': 0.8752935915004239}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:13:47,092][0m Trial 10 finished with value: 0.3155560368895531 and parameters: {'observation_period_num': 234, 'train_rates': 0.7240698384116493, 'learning_rate': 0.0008951147442068431, 'batch_size': 93, 'step_size': 15, 'gamma': 0.9356320855119109}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:19:02,892][0m Trial 11 finished with value: 0.06962184136619373 and parameters: {'observation_period_num': 66, 'train_rates': 0.9830838269797059, 'learning_rate': 6.0168378954427345e-06, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9813493920602901}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:20:15,520][0m Trial 12 finished with value: 0.2238166800567082 and parameters: {'observation_period_num': 140, 'train_rates': 0.7409470218539451, 'learning_rate': 8.778615349876277e-06, 'batch_size': 64, 'step_size': 11, 'gamma': 0.9211618684670877}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:21:39,373][0m Trial 13 finished with value: 0.2194083221256733 and parameters: {'observation_period_num': 66, 'train_rates': 0.8114274145174767, 'learning_rate': 2.692052138377582e-06, 'batch_size': 60, 'step_size': 12, 'gamma': 0.9759492394578467}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:23:06,680][0m Trial 14 finished with value: 0.06526928070263985 and parameters: {'observation_period_num': 97, 'train_rates': 0.9456056605001598, 'learning_rate': 0.0001472655502905438, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9062930711898609}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:23:50,088][0m Trial 15 finished with value: 0.2559310147571846 and parameters: {'observation_period_num': 173, 'train_rates': 0.6959375600763305, 'learning_rate': 1.650929411255378e-05, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9453690156763287}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:25:59,121][0m Trial 16 finished with value: 0.03715516563637981 and parameters: {'observation_period_num': 39, 'train_rates': 0.8461022709981225, 'learning_rate': 6.411689898229156e-05, 'batch_size': 40, 'step_size': 13, 'gamma': 0.8363789665822864}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:28:13,543][0m Trial 17 finished with value: 0.04811100322989539 and parameters: {'observation_period_num': 48, 'train_rates': 0.772107999720037, 'learning_rate': 6.239173537623994e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.8365205213489553}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:29:01,684][0m Trial 18 finished with value: 0.07181249386989154 and parameters: {'observation_period_num': 100, 'train_rates': 0.9454396637787061, 'learning_rate': 0.00036977386808031813, 'batch_size': 118, 'step_size': 15, 'gamma': 0.7845127214346721}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:29:36,038][0m Trial 19 finished with value: 0.06140438531318941 and parameters: {'observation_period_num': 47, 'train_rates': 0.8480776589915593, 'learning_rate': 5.180130239569074e-05, 'batch_size': 164, 'step_size': 11, 'gamma': 0.8362464369763037}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:30:30,946][0m Trial 20 finished with value: 0.11627285500928279 and parameters: {'observation_period_num': 82, 'train_rates': 0.6544309812690368, 'learning_rate': 0.0003391030876343121, 'batch_size': 80, 'step_size': 13, 'gamma': 0.891190768573126}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:32:12,666][0m Trial 21 finished with value: 0.049758135025232575 and parameters: {'observation_period_num': 6, 'train_rates': 0.844988998131428, 'learning_rate': 1.3818175748807128e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.8359100635770376}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:34:25,706][0m Trial 22 finished with value: 0.03810179327745628 and parameters: {'observation_period_num': 40, 'train_rates': 0.9097554799426888, 'learning_rate': 9.045222584553858e-05, 'batch_size': 41, 'step_size': 5, 'gamma': 0.9558612233799685}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:36:52,555][0m Trial 23 finished with value: 0.046240965745622115 and parameters: {'observation_period_num': 44, 'train_rates': 0.9146714221725127, 'learning_rate': 9.93206771117757e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.7892126793358674}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:38:04,216][0m Trial 24 finished with value: 0.09274864609220199 and parameters: {'observation_period_num': 119, 'train_rates': 0.9209664012994057, 'learning_rate': 0.0002874521347018354, 'batch_size': 75, 'step_size': 1, 'gamma': 0.8853235227762994}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:38:53,916][0m Trial 25 finished with value: 0.05568519979715347 and parameters: {'observation_period_num': 42, 'train_rates': 0.9861489919316411, 'learning_rate': 4.218397901149842e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.8530216475062227}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:41:23,756][0m Trial 26 finished with value: 0.041905764269061604 and parameters: {'observation_period_num': 58, 'train_rates': 0.8605682471927786, 'learning_rate': 7.943271490417363e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8271677442062503}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:42:34,392][0m Trial 27 finished with value: 0.03446528454329453 and parameters: {'observation_period_num': 30, 'train_rates': 0.9129876446751044, 'learning_rate': 0.0002170740356567693, 'batch_size': 80, 'step_size': 14, 'gamma': 0.778097131466999}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:43:37,331][0m Trial 28 finished with value: 0.060186683520918986 and parameters: {'observation_period_num': 81, 'train_rates': 0.7663698495411526, 'learning_rate': 0.00026601173905144214, 'batch_size': 77, 'step_size': 14, 'gamma': 0.775517271240755}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:44:34,738][0m Trial 29 finished with value: 0.07519575564219849 and parameters: {'observation_period_num': 116, 'train_rates': 0.819462912360228, 'learning_rate': 0.0006476739475288487, 'batch_size': 87, 'step_size': 12, 'gamma': 0.7971096171262905}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:45:22,061][0m Trial 30 finished with value: 0.06833443697808807 and parameters: {'observation_period_num': 158, 'train_rates': 0.8750583381439118, 'learning_rate': 0.0002035929944388921, 'batch_size': 113, 'step_size': 14, 'gamma': 0.7507716485360716}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:47:31,147][0m Trial 31 finished with value: 0.03782928094026531 and parameters: {'observation_period_num': 30, 'train_rates': 0.9155031713694616, 'learning_rate': 0.0004896988884953976, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8180401664335994}. Best is trial 2 with value: 0.03381523662278441.[0m
[32m[I 2025-01-11 23:51:52,943][0m Trial 32 finished with value: 0.03341988127293258 and parameters: {'observation_period_num': 23, 'train_rates': 0.9306620046389306, 'learning_rate': 0.0005331314726180955, 'batch_size': 21, 'step_size': 9, 'gamma': 0.8197561430162138}. Best is trial 32 with value: 0.03341988127293258.[0m
[32m[I 2025-01-11 23:57:09,304][0m Trial 33 finished with value: 0.02343397851749931 and parameters: {'observation_period_num': 5, 'train_rates': 0.969038880983348, 'learning_rate': 0.00016242895905399816, 'batch_size': 18, 'step_size': 12, 'gamma': 0.7721114564007053}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:03:03,150][0m Trial 34 finished with value: 0.042420003736243314 and parameters: {'observation_period_num': 5, 'train_rates': 0.9618310149710624, 'learning_rate': 0.0005066025419313117, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7716359773440976}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:06:42,332][0m Trial 35 finished with value: 0.03213185334906859 and parameters: {'observation_period_num': 18, 'train_rates': 0.9655235815280616, 'learning_rate': 0.00020904903453607086, 'batch_size': 26, 'step_size': 12, 'gamma': 0.7603857544385753}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:10:21,612][0m Trial 36 finished with value: 0.028614825006611277 and parameters: {'observation_period_num': 19, 'train_rates': 0.9656953041934838, 'learning_rate': 0.00013326594153634466, 'batch_size': 26, 'step_size': 10, 'gamma': 0.7602498480935007}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:10:47,424][0m Trial 37 finished with value: 0.03623020276427269 and parameters: {'observation_period_num': 18, 'train_rates': 0.9716252845797153, 'learning_rate': 0.000953991956432048, 'batch_size': 238, 'step_size': 8, 'gamma': 0.7611860887810235}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:14:16,473][0m Trial 38 finished with value: 0.1416415406597985 and parameters: {'observation_period_num': 251, 'train_rates': 0.9567548197483081, 'learning_rate': 0.00012578109728586518, 'batch_size': 25, 'step_size': 10, 'gamma': 0.7960834595376247}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:14:40,447][0m Trial 39 finished with value: 0.05323848873376846 and parameters: {'observation_period_num': 23, 'train_rates': 0.9344452710625701, 'learning_rate': 0.00020958396118858774, 'batch_size': 255, 'step_size': 9, 'gamma': 0.7522244722020023}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:18:20,274][0m Trial 40 finished with value: 0.04048164188861847 and parameters: {'observation_period_num': 19, 'train_rates': 0.9736267275814574, 'learning_rate': 0.0006111432851249728, 'batch_size': 26, 'step_size': 12, 'gamma': 0.762881267243546}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:20:08,458][0m Trial 41 finished with value: 0.026094768177529776 and parameters: {'observation_period_num': 6, 'train_rates': 0.932817689877683, 'learning_rate': 0.0001380792549890382, 'batch_size': 52, 'step_size': 11, 'gamma': 0.8106163830694885}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:21:53,277][0m Trial 42 finished with value: 0.025926544513438166 and parameters: {'observation_period_num': 13, 'train_rates': 0.9341016679557486, 'learning_rate': 0.00040339773492934145, 'batch_size': 54, 'step_size': 11, 'gamma': 0.8071537260586112}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:23:42,702][0m Trial 43 finished with value: 0.03266538475971606 and parameters: {'observation_period_num': 7, 'train_rates': 0.9601755562176266, 'learning_rate': 0.00012861707816261934, 'batch_size': 53, 'step_size': 11, 'gamma': 0.8055041832204184}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:26:49,756][0m Trial 44 finished with value: 0.03351755518662302 and parameters: {'observation_period_num': 58, 'train_rates': 0.9870559769550687, 'learning_rate': 0.000397812735817974, 'batch_size': 31, 'step_size': 12, 'gamma': 0.7845272823038703}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:28:28,968][0m Trial 45 finished with value: 0.023845196272174043 and parameters: {'observation_period_num': 15, 'train_rates': 0.8864361247804853, 'learning_rate': 0.00017796393954986898, 'batch_size': 55, 'step_size': 10, 'gamma': 0.7683573507453548}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:30:00,145][0m Trial 46 finished with value: 0.12238304890119113 and parameters: {'observation_period_num': 217, 'train_rates': 0.8859760120673745, 'learning_rate': 3.9116689355450005e-05, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7722139738414285}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:31:22,249][0m Trial 47 finished with value: 0.03418846446655545 and parameters: {'observation_period_num': 31, 'train_rates': 0.8954372908922488, 'learning_rate': 0.0001192463480966033, 'batch_size': 66, 'step_size': 10, 'gamma': 0.8007571246382841}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:31:56,515][0m Trial 48 finished with value: 0.04028237084443657 and parameters: {'observation_period_num': 5, 'train_rates': 0.9353435668091303, 'learning_rate': 0.00015445519868703016, 'batch_size': 183, 'step_size': 11, 'gamma': 0.7908447017657751}. Best is trial 33 with value: 0.02343397851749931.[0m
[32m[I 2025-01-12 00:33:43,363][0m Trial 49 finished with value: 0.060537432891909375 and parameters: {'observation_period_num': 55, 'train_rates': 0.8785523118901117, 'learning_rate': 2.753287946130331e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.8125062321746597}. Best is trial 33 with value: 0.02343397851749931.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-12 00:33:43,373][0m A new study created in memory with name: no-name-c51c2f89-632a-4e03-ba6d-43db1570098d[0m
[32m[I 2025-01-12 00:34:02,514][0m Trial 0 finished with value: 0.19806143739860788 and parameters: {'observation_period_num': 225, 'train_rates': 0.6554791131150749, 'learning_rate': 0.0004356481874789456, 'batch_size': 236, 'step_size': 7, 'gamma': 0.9144696304366348}. Best is trial 0 with value: 0.19806143739860788.[0m
[32m[I 2025-01-12 00:34:38,555][0m Trial 1 finished with value: 0.19524838323039667 and parameters: {'observation_period_num': 98, 'train_rates': 0.9216967410317485, 'learning_rate': 3.435843256668312e-05, 'batch_size': 164, 'step_size': 3, 'gamma': 0.940970056634176}. Best is trial 1 with value: 0.19524838323039667.[0m
[32m[I 2025-01-12 00:35:02,884][0m Trial 2 finished with value: 0.3324198492682806 and parameters: {'observation_period_num': 112, 'train_rates': 0.6441400899821512, 'learning_rate': 9.727826729576315e-06, 'batch_size': 190, 'step_size': 14, 'gamma': 0.8394689908022588}. Best is trial 1 with value: 0.19524838323039667.[0m
[32m[I 2025-01-12 00:35:38,380][0m Trial 3 finished with value: 0.4193877875804901 and parameters: {'observation_period_num': 117, 'train_rates': 0.9540737682260001, 'learning_rate': 3.462011852506638e-05, 'batch_size': 165, 'step_size': 2, 'gamma': 0.8667867701652715}. Best is trial 1 with value: 0.19524838323039667.[0m
[32m[I 2025-01-12 00:36:04,523][0m Trial 4 finished with value: 0.8393128145308721 and parameters: {'observation_period_num': 212, 'train_rates': 0.90065349713492, 'learning_rate': 3.06337094618117e-06, 'batch_size': 208, 'step_size': 1, 'gamma': 0.9415857615031037}. Best is trial 1 with value: 0.19524838323039667.[0m
[32m[I 2025-01-12 00:36:48,831][0m Trial 5 finished with value: 0.0708804545756528 and parameters: {'observation_period_num': 69, 'train_rates': 0.7379050744178957, 'learning_rate': 6.309325949891473e-05, 'batch_size': 110, 'step_size': 9, 'gamma': 0.9754815365311782}. Best is trial 5 with value: 0.0708804545756528.[0m
[32m[I 2025-01-12 00:37:11,367][0m Trial 6 finished with value: 1.0717282584121635 and parameters: {'observation_period_num': 92, 'train_rates': 0.8060582668968763, 'learning_rate': 3.371933738353004e-06, 'batch_size': 239, 'step_size': 5, 'gamma': 0.8664011927854814}. Best is trial 5 with value: 0.0708804545756528.[0m
[32m[I 2025-01-12 00:37:57,235][0m Trial 7 finished with value: 0.2630321737802583 and parameters: {'observation_period_num': 23, 'train_rates': 0.6547046376675365, 'learning_rate': 3.2889495246499704e-06, 'batch_size': 98, 'step_size': 6, 'gamma': 0.9506558281010761}. Best is trial 5 with value: 0.0708804545756528.[0m
Early stopping at epoch 56
[32m[I 2025-01-12 00:38:35,721][0m Trial 8 finished with value: 1.4231138752675012 and parameters: {'observation_period_num': 166, 'train_rates': 0.8030101590865538, 'learning_rate': 3.2518399555078955e-06, 'batch_size': 73, 'step_size': 1, 'gamma': 0.8183834431896746}. Best is trial 5 with value: 0.0708804545756528.[0m
[32m[I 2025-01-12 00:39:50,802][0m Trial 9 finished with value: 0.10987840526229967 and parameters: {'observation_period_num': 244, 'train_rates': 0.9091618358430736, 'learning_rate': 7.358722682776515e-05, 'batch_size': 70, 'step_size': 7, 'gamma': 0.9449845880577212}. Best is trial 5 with value: 0.0708804545756528.[0m
[32m[I 2025-01-12 00:42:46,725][0m Trial 10 finished with value: 0.04444063542548178 and parameters: {'observation_period_num': 22, 'train_rates': 0.7315605330647429, 'learning_rate': 0.00026833326023525814, 'batch_size': 26, 'step_size': 11, 'gamma': 0.7501290395607842}. Best is trial 10 with value: 0.04444063542548178.[0m
[32m[I 2025-01-12 00:47:36,594][0m Trial 11 finished with value: 0.03462731959622594 and parameters: {'observation_period_num': 15, 'train_rates': 0.7263378398338953, 'learning_rate': 0.00029096033969002453, 'batch_size': 16, 'step_size': 11, 'gamma': 0.7533027223510014}. Best is trial 11 with value: 0.03462731959622594.[0m
[32m[I 2025-01-12 00:50:59,491][0m Trial 12 finished with value: 0.028913489096102456 and parameters: {'observation_period_num': 5, 'train_rates': 0.7320878133178387, 'learning_rate': 0.0005323744294260399, 'batch_size': 23, 'step_size': 11, 'gamma': 0.7577470237476698}. Best is trial 12 with value: 0.028913489096102456.[0m
[32m[I 2025-01-12 00:54:04,304][0m Trial 13 finished with value: 0.07348384244410354 and parameters: {'observation_period_num': 52, 'train_rates': 0.7303629899734647, 'learning_rate': 0.0009549135965933403, 'batch_size': 25, 'step_size': 12, 'gamma': 0.7515393280445468}. Best is trial 12 with value: 0.028913489096102456.[0m
[32m[I 2025-01-12 00:55:55,194][0m Trial 14 finished with value: 0.02474386827512221 and parameters: {'observation_period_num': 7, 'train_rates': 0.8506107579701159, 'learning_rate': 0.00016717728432695566, 'batch_size': 48, 'step_size': 15, 'gamma': 0.7918962080644941}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 00:57:26,779][0m Trial 15 finished with value: 0.04792990002363008 and parameters: {'observation_period_num': 53, 'train_rates': 0.8647669897151609, 'learning_rate': 0.00012454608702606158, 'batch_size': 58, 'step_size': 15, 'gamma': 0.7965018838477317}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 00:59:01,480][0m Trial 16 finished with value: 0.12618122696705641 and parameters: {'observation_period_num': 157, 'train_rates': 0.844191017734493, 'learning_rate': 0.0007660884497772521, 'batch_size': 53, 'step_size': 13, 'gamma': 0.7882824935062411}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 00:59:45,635][0m Trial 17 finished with value: 0.03894674481641977 and parameters: {'observation_period_num': 8, 'train_rates': 0.7740125663471435, 'learning_rate': 0.00015999285198665272, 'batch_size': 118, 'step_size': 9, 'gamma': 0.7823321885432241}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 01:00:44,260][0m Trial 18 finished with value: 0.1386381423870274 and parameters: {'observation_period_num': 67, 'train_rates': 0.8465580024428592, 'learning_rate': 1.3917110240367146e-05, 'batch_size': 89, 'step_size': 15, 'gamma': 0.8274436185394851}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 01:01:16,400][0m Trial 19 finished with value: 0.2326379615887912 and parameters: {'observation_period_num': 151, 'train_rates': 0.6962580887881213, 'learning_rate': 0.0004646940046377398, 'batch_size': 141, 'step_size': 10, 'gamma': 0.8956064277095854}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 01:02:45,983][0m Trial 20 finished with value: 0.08139176232815817 and parameters: {'observation_period_num': 37, 'train_rates': 0.6021827601550944, 'learning_rate': 0.0001367781596681388, 'batch_size': 45, 'step_size': 13, 'gamma': 0.7758361062915232}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 01:07:11,624][0m Trial 21 finished with value: 0.028540945380080215 and parameters: {'observation_period_num': 8, 'train_rates': 0.7667928439518038, 'learning_rate': 0.0002851113046668263, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7675964755145595}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 01:09:20,940][0m Trial 22 finished with value: 0.046420594933960176 and parameters: {'observation_period_num': 41, 'train_rates': 0.7682865461544257, 'learning_rate': 0.0002549784310867819, 'batch_size': 37, 'step_size': 12, 'gamma': 0.8076560979677552}. Best is trial 14 with value: 0.02474386827512221.[0m
[32m[I 2025-01-12 01:10:31,772][0m Trial 23 finished with value: 0.02416581701319874 and parameters: {'observation_period_num': 5, 'train_rates': 0.8187818914762497, 'learning_rate': 0.0005627293719042991, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8366674775619204}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:11:35,046][0m Trial 24 finished with value: 0.6598726581091179 and parameters: {'observation_period_num': 76, 'train_rates': 0.818246004695372, 'learning_rate': 1.1111474593747688e-06, 'batch_size': 79, 'step_size': 9, 'gamma': 0.8531678577055807}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:13:06,545][0m Trial 25 finished with value: 0.03597231535988058 and parameters: {'observation_period_num': 35, 'train_rates': 0.8718675630510425, 'learning_rate': 7.413196229973593e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.8369690271490968}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:15:18,896][0m Trial 26 finished with value: 0.03238175826895441 and parameters: {'observation_period_num': 6, 'train_rates': 0.9603368366199784, 'learning_rate': 0.00020297260974939477, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8068490269450898}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:15:54,535][0m Trial 27 finished with value: 0.134713217953648 and parameters: {'observation_period_num': 182, 'train_rates': 0.7770345862502049, 'learning_rate': 0.0005319917945409833, 'batch_size': 136, 'step_size': 14, 'gamma': 0.7732633479243829}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:16:47,845][0m Trial 28 finished with value: 0.0350662989096127 and parameters: {'observation_period_num': 31, 'train_rates': 0.839683025189547, 'learning_rate': 0.0003411998460050328, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8921863409255881}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:17:46,286][0m Trial 29 finished with value: 0.07401701370441674 and parameters: {'observation_period_num': 53, 'train_rates': 0.6916879649063167, 'learning_rate': 0.00011500637645001228, 'batch_size': 78, 'step_size': 4, 'gamma': 0.8053876580703655}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:19:57,750][0m Trial 30 finished with value: 0.10955932408297828 and parameters: {'observation_period_num': 133, 'train_rates': 0.8841138183376228, 'learning_rate': 0.0006979170918140445, 'batch_size': 39, 'step_size': 8, 'gamma': 0.7716680828904112}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:24:36,166][0m Trial 31 finished with value: 0.028697588604234138 and parameters: {'observation_period_num': 5, 'train_rates': 0.75642460218501, 'learning_rate': 0.0004747527101336694, 'batch_size': 17, 'step_size': 11, 'gamma': 0.7610549090296297}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:26:56,334][0m Trial 32 finished with value: 0.037695377187776394 and parameters: {'observation_period_num': 23, 'train_rates': 0.7643236476043836, 'learning_rate': 0.0004642995588262963, 'batch_size': 34, 'step_size': 10, 'gamma': 0.7878171707727233}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:32:09,770][0m Trial 33 finished with value: 0.046443795159459116 and parameters: {'observation_period_num': 45, 'train_rates': 0.8280735912620211, 'learning_rate': 0.00017765892775910579, 'batch_size': 16, 'step_size': 12, 'gamma': 0.7689278351640553}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:33:35,515][0m Trial 34 finished with value: 0.07906154925145474 and parameters: {'observation_period_num': 89, 'train_rates': 0.7934662414983192, 'learning_rate': 0.00039813695176820326, 'batch_size': 57, 'step_size': 14, 'gamma': 0.8199424327694834}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:35:01,716][0m Trial 35 finished with value: 0.05572456093849959 and parameters: {'observation_period_num': 20, 'train_rates': 0.9265926204677798, 'learning_rate': 4.2751192192356456e-05, 'batch_size': 65, 'step_size': 7, 'gamma': 0.841429104090655}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:37:22,406][0m Trial 36 finished with value: 0.033160108728603956 and parameters: {'observation_period_num': 5, 'train_rates': 0.6953917198105627, 'learning_rate': 0.0007246722212169829, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7979688893762511}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:37:47,434][0m Trial 37 finished with value: 0.12552453812799955 and parameters: {'observation_period_num': 110, 'train_rates': 0.7525477551606872, 'learning_rate': 8.787256962070544e-05, 'batch_size': 206, 'step_size': 9, 'gamma': 0.7642257311533708}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:38:20,724][0m Trial 38 finished with value: 0.15310214663316546 and parameters: {'observation_period_num': 64, 'train_rates': 0.7896558456091289, 'learning_rate': 1.664418913590671e-05, 'batch_size': 162, 'step_size': 14, 'gamma': 0.8525001983603165}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:39:03,788][0m Trial 39 finished with value: 0.09465661936480066 and parameters: {'observation_period_num': 209, 'train_rates': 0.8156811214864664, 'learning_rate': 0.00025193500557893695, 'batch_size': 116, 'step_size': 6, 'gamma': 0.8930344091896938}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:40:09,958][0m Trial 40 finished with value: 0.04962111161770048 and parameters: {'observation_period_num': 31, 'train_rates': 0.9366677624733135, 'learning_rate': 0.0009130104106534621, 'batch_size': 88, 'step_size': 12, 'gamma': 0.7888651226261566}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:44:46,194][0m Trial 41 finished with value: 0.03406320223771232 and parameters: {'observation_period_num': 5, 'train_rates': 0.6779474949519778, 'learning_rate': 0.0005859646942905495, 'batch_size': 16, 'step_size': 11, 'gamma': 0.761277633932288}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:46:27,504][0m Trial 42 finished with value: 0.03593525225489304 and parameters: {'observation_period_num': 21, 'train_rates': 0.7525778408118545, 'learning_rate': 0.0003755438840732173, 'batch_size': 47, 'step_size': 10, 'gamma': 0.7632945050269231}. Best is trial 23 with value: 0.02416581701319874.[0m
[32m[I 2025-01-12 01:49:54,002][0m Trial 43 finished with value: 0.02392623974857005 and parameters: {'observation_period_num': 16, 'train_rates': 0.9815874697866309, 'learning_rate': 0.00020640964113191352, 'batch_size': 28, 'step_size': 13, 'gamma': 0.7794326497921917}. Best is trial 43 with value: 0.02392623974857005.[0m
[32m[I 2025-01-12 01:52:39,804][0m Trial 44 finished with value: 0.034890741456386654 and parameters: {'observation_period_num': 21, 'train_rates': 0.9706786159428511, 'learning_rate': 0.00020832567361817042, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8165883791376446}. Best is trial 43 with value: 0.02392623974857005.[0m
[32m[I 2025-01-12 01:54:00,564][0m Trial 45 finished with value: 0.03820284716784954 and parameters: {'observation_period_num': 15, 'train_rates': 0.8843896299638303, 'learning_rate': 5.522622394442047e-05, 'batch_size': 68, 'step_size': 13, 'gamma': 0.778033784749559}. Best is trial 43 with value: 0.02392623974857005.[0m
[32m[I 2025-01-12 01:55:39,020][0m Trial 46 finished with value: 0.04874960156955247 and parameters: {'observation_period_num': 31, 'train_rates': 0.7161179081253757, 'learning_rate': 2.469207090881461e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.9730648127211103}. Best is trial 43 with value: 0.02392623974857005.[0m
[32m[I 2025-01-12 01:56:04,464][0m Trial 47 finished with value: 0.11319154500961304 and parameters: {'observation_period_num': 79, 'train_rates': 0.9837241219187198, 'learning_rate': 0.0001104773392468289, 'batch_size': 247, 'step_size': 14, 'gamma': 0.7972336047902163}. Best is trial 43 with value: 0.02392623974857005.[0m
[32m[I 2025-01-12 01:59:40,450][0m Trial 48 finished with value: 0.1481983748717783 and parameters: {'observation_period_num': 47, 'train_rates': 0.9103483526041894, 'learning_rate': 0.0002963964672431474, 'batch_size': 25, 'step_size': 13, 'gamma': 0.8255105113289989}. Best is trial 43 with value: 0.02392623974857005.[0m
[32m[I 2025-01-12 02:02:39,036][0m Trial 49 finished with value: 0.08541506290399797 and parameters: {'observation_period_num': 59, 'train_rates': 0.8571064262537699, 'learning_rate': 5.570356232784067e-06, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9249656657649159}. Best is trial 43 with value: 0.02392623974857005.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.8287043337742016, 'learning_rate': 0.0009674388205993714, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9542654855367153}
Epoch 1/300, trend Loss: 0.5609 | 0.5279
Epoch 2/300, trend Loss: 0.2586 | 0.3521
Epoch 3/300, trend Loss: 0.2595 | 0.1862
Epoch 4/300, trend Loss: 0.2177 | 0.3044
Epoch 5/300, trend Loss: 0.1922 | 0.1472
Epoch 6/300, trend Loss: 0.1686 | 0.1376
Epoch 7/300, trend Loss: 0.1529 | 0.1011
Epoch 8/300, trend Loss: 0.1303 | 0.0865
Epoch 9/300, trend Loss: 0.1433 | 0.1129
Epoch 10/300, trend Loss: 0.1432 | 0.0956
Epoch 11/300, trend Loss: 0.1554 | 0.2325
Epoch 12/300, trend Loss: 0.1382 | 0.1041
Epoch 13/300, trend Loss: 0.1394 | 0.1122
Epoch 14/300, trend Loss: 0.1332 | 0.0841
Epoch 15/300, trend Loss: 0.1138 | 0.0728
Epoch 16/300, trend Loss: 0.1101 | 0.0719
Epoch 17/300, trend Loss: 0.1067 | 0.0679
Epoch 18/300, trend Loss: 0.1066 | 0.0707
Epoch 19/300, trend Loss: 0.1043 | 0.0645
Epoch 20/300, trend Loss: 0.1045 | 0.0690
Epoch 21/300, trend Loss: 0.1034 | 0.0649
Epoch 22/300, trend Loss: 0.1049 | 0.0746
Epoch 23/300, trend Loss: 0.1041 | 0.0640
Epoch 24/300, trend Loss: 0.1054 | 0.0874
Epoch 25/300, trend Loss: 0.1032 | 0.0627
Epoch 26/300, trend Loss: 0.1033 | 0.0836
Epoch 27/300, trend Loss: 0.1013 | 0.0600
Epoch 28/300, trend Loss: 0.1005 | 0.0746
Epoch 29/300, trend Loss: 0.0990 | 0.0567
Epoch 30/300, trend Loss: 0.0981 | 0.0683
Epoch 31/300, trend Loss: 0.0966 | 0.0543
Epoch 32/300, trend Loss: 0.0957 | 0.0630
Epoch 33/300, trend Loss: 0.0947 | 0.0522
Epoch 34/300, trend Loss: 0.0939 | 0.0582
Epoch 35/300, trend Loss: 0.0929 | 0.0504
Epoch 36/300, trend Loss: 0.0925 | 0.0554
Epoch 37/300, trend Loss: 0.0918 | 0.0493
Epoch 38/300, trend Loss: 0.0918 | 0.0543
Epoch 39/300, trend Loss: 0.0913 | 0.0487
Epoch 40/300, trend Loss: 0.0914 | 0.0547
Epoch 41/300, trend Loss: 0.0913 | 0.0484
Epoch 42/300, trend Loss: 0.0917 | 0.0562
Epoch 43/300, trend Loss: 0.0919 | 0.0487
Epoch 44/300, trend Loss: 0.0928 | 0.0601
Epoch 45/300, trend Loss: 0.0928 | 0.0494
Epoch 46/300, trend Loss: 0.0934 | 0.0635
Epoch 47/300, trend Loss: 0.0926 | 0.0493
Epoch 48/300, trend Loss: 0.0924 | 0.0589
Epoch 49/300, trend Loss: 0.0916 | 0.0477
Epoch 50/300, trend Loss: 0.0909 | 0.0548
Epoch 51/300, trend Loss: 0.0905 | 0.0466
Epoch 52/300, trend Loss: 0.0903 | 0.0539
Epoch 53/300, trend Loss: 0.0898 | 0.0460
Epoch 54/300, trend Loss: 0.0894 | 0.0541
Epoch 55/300, trend Loss: 0.0888 | 0.0455
Epoch 56/300, trend Loss: 0.0885 | 0.0531
Epoch 57/300, trend Loss: 0.0878 | 0.0445
Epoch 58/300, trend Loss: 0.0877 | 0.0525
Epoch 59/300, trend Loss: 0.0874 | 0.0441
Epoch 60/300, trend Loss: 0.0875 | 0.0534
Epoch 61/300, trend Loss: 0.0866 | 0.0435
Epoch 62/300, trend Loss: 0.0867 | 0.0525
Epoch 63/300, trend Loss: 0.0856 | 0.0421
Epoch 64/300, trend Loss: 0.0855 | 0.0500
Epoch 65/300, trend Loss: 0.0870 | 0.0425
Epoch 66/300, trend Loss: 0.0850 | 0.0472
Epoch 67/300, trend Loss: 0.0844 | 0.0412
Epoch 68/300, trend Loss: 0.0835 | 0.0456
Epoch 69/300, trend Loss: 0.0832 | 0.0395
Epoch 70/300, trend Loss: 0.0817 | 0.0433
Epoch 71/300, trend Loss: 0.0812 | 0.0390
Epoch 72/300, trend Loss: 0.0812 | 0.0411
Epoch 73/300, trend Loss: 0.0807 | 0.0379
Epoch 74/300, trend Loss: 0.0799 | 0.0399
Epoch 75/300, trend Loss: 0.0807 | 0.0369
Epoch 76/300, trend Loss: 0.0793 | 0.0391
Epoch 77/300, trend Loss: 0.0789 | 0.0374
Epoch 78/300, trend Loss: 0.0805 | 0.0401
Epoch 79/300, trend Loss: 0.0802 | 0.0372
Epoch 80/300, trend Loss: 0.0791 | 0.0381
Epoch 81/300, trend Loss: 0.0812 | 0.0362
Epoch 82/300, trend Loss: 0.0817 | 0.0397
Epoch 83/300, trend Loss: 0.0806 | 0.0366
Epoch 84/300, trend Loss: 0.0819 | 0.0430
Epoch 85/300, trend Loss: 0.0829 | 0.0371
Epoch 86/300, trend Loss: 0.0807 | 0.0422
Epoch 87/300, trend Loss: 0.0859 | 0.0374
Epoch 88/300, trend Loss: 0.0826 | 0.0468
Epoch 89/300, trend Loss: 0.0818 | 0.0393
Epoch 90/300, trend Loss: 0.0890 | 0.0502
Epoch 91/300, trend Loss: 0.0880 | 0.0397
Epoch 92/300, trend Loss: 0.0850 | 0.0476
Epoch 93/300, trend Loss: 0.0809 | 0.0398
Epoch 94/300, trend Loss: 0.0849 | 0.0467
Epoch 95/300, trend Loss: 0.0826 | 0.0397
Epoch 96/300, trend Loss: 0.0803 | 0.0404
Epoch 97/300, trend Loss: 0.0776 | 0.0358
Epoch 98/300, trend Loss: 0.0786 | 0.0396
Epoch 99/300, trend Loss: 0.0789 | 0.0373
Epoch 100/300, trend Loss: 0.0769 | 0.0373
Epoch 101/300, trend Loss: 0.0757 | 0.0344
Epoch 102/300, trend Loss: 0.0752 | 0.0353
Epoch 103/300, trend Loss: 0.0756 | 0.0346
Epoch 104/300, trend Loss: 0.0747 | 0.0349
Epoch 105/300, trend Loss: 0.0740 | 0.0337
Epoch 106/300, trend Loss: 0.0738 | 0.0336
Epoch 107/300, trend Loss: 0.0737 | 0.0332
Epoch 108/300, trend Loss: 0.0733 | 0.0333
Epoch 109/300, trend Loss: 0.0729 | 0.0327
Epoch 110/300, trend Loss: 0.0726 | 0.0329
Epoch 111/300, trend Loss: 0.0725 | 0.0324
Epoch 112/300, trend Loss: 0.0722 | 0.0323
Epoch 113/300, trend Loss: 0.0723 | 0.0318
Epoch 114/300, trend Loss: 0.0721 | 0.0316
Epoch 115/300, trend Loss: 0.0720 | 0.0315
Epoch 116/300, trend Loss: 0.0716 | 0.0316
Epoch 117/300, trend Loss: 0.0712 | 0.0321
Epoch 118/300, trend Loss: 0.0714 | 0.0324
Epoch 119/300, trend Loss: 0.0713 | 0.0322
Epoch 120/300, trend Loss: 0.0712 | 0.0311
Epoch 121/300, trend Loss: 0.0711 | 0.0308
Epoch 122/300, trend Loss: 0.0709 | 0.0306
Epoch 123/300, trend Loss: 0.0719 | 0.0304
Epoch 124/300, trend Loss: 0.0716 | 0.0310
Epoch 125/300, trend Loss: 0.0707 | 0.0320
Epoch 126/300, trend Loss: 0.0728 | 0.0344
Epoch 127/300, trend Loss: 0.0734 | 0.0316
Epoch 128/300, trend Loss: 0.0725 | 0.0328
Epoch 129/300, trend Loss: 0.0736 | 0.0322
Epoch 130/300, trend Loss: 0.0734 | 0.0361
Epoch 131/300, trend Loss: 0.0729 | 0.0326
Epoch 132/300, trend Loss: 0.0742 | 0.0376
Epoch 133/300, trend Loss: 0.0743 | 0.0332
Epoch 134/300, trend Loss: 0.0736 | 0.0379
Epoch 135/300, trend Loss: 0.0746 | 0.0327
Epoch 136/300, trend Loss: 0.0740 | 0.0393
Epoch 137/300, trend Loss: 0.0728 | 0.0339
Epoch 138/300, trend Loss: 0.0725 | 0.0391
Epoch 139/300, trend Loss: 0.0717 | 0.0328
Epoch 140/300, trend Loss: 0.0702 | 0.0340
Epoch 141/300, trend Loss: 0.0699 | 0.0309
Epoch 142/300, trend Loss: 0.0690 | 0.0314
Epoch 143/300, trend Loss: 0.0691 | 0.0295
Epoch 144/300, trend Loss: 0.0683 | 0.0298
Epoch 145/300, trend Loss: 0.0681 | 0.0290
Epoch 146/300, trend Loss: 0.0677 | 0.0296
Epoch 147/300, trend Loss: 0.0676 | 0.0296
Epoch 148/300, trend Loss: 0.0680 | 0.0306
Epoch 149/300, trend Loss: 0.0688 | 0.0300
Epoch 150/300, trend Loss: 0.0680 | 0.0284
Epoch 151/300, trend Loss: 0.0680 | 0.0286
Epoch 152/300, trend Loss: 0.0685 | 0.0290
Epoch 153/300, trend Loss: 0.0683 | 0.0289
Epoch 154/300, trend Loss: 0.0672 | 0.0287
Epoch 155/300, trend Loss: 0.0679 | 0.0296
Epoch 156/300, trend Loss: 0.0682 | 0.0286
Epoch 157/300, trend Loss: 0.0675 | 0.0280
Epoch 158/300, trend Loss: 0.0675 | 0.0276
Epoch 159/300, trend Loss: 0.0671 | 0.0283
Epoch 160/300, trend Loss: 0.0665 | 0.0279
Epoch 161/300, trend Loss: 0.0666 | 0.0281
Epoch 162/300, trend Loss: 0.0662 | 0.0276
Epoch 163/300, trend Loss: 0.0661 | 0.0271
Epoch 164/300, trend Loss: 0.0660 | 0.0268
Epoch 165/300, trend Loss: 0.0656 | 0.0266
Epoch 166/300, trend Loss: 0.0656 | 0.0270
Epoch 167/300, trend Loss: 0.0655 | 0.0276
Epoch 168/300, trend Loss: 0.0657 | 0.0277
Epoch 169/300, trend Loss: 0.0662 | 0.0271
Epoch 170/300, trend Loss: 0.0658 | 0.0269
Epoch 171/300, trend Loss: 0.0661 | 0.0263
Epoch 172/300, trend Loss: 0.0661 | 0.0266
Epoch 173/300, trend Loss: 0.0656 | 0.0267
Epoch 174/300, trend Loss: 0.0654 | 0.0266
Epoch 175/300, trend Loss: 0.0656 | 0.0272
Epoch 176/300, trend Loss: 0.0654 | 0.0269
Epoch 177/300, trend Loss: 0.0654 | 0.0268
Epoch 178/300, trend Loss: 0.0651 | 0.0261
Epoch 179/300, trend Loss: 0.0650 | 0.0260
Epoch 180/300, trend Loss: 0.0649 | 0.0256
Epoch 181/300, trend Loss: 0.0647 | 0.0259
Epoch 182/300, trend Loss: 0.0648 | 0.0270
Epoch 183/300, trend Loss: 0.0654 | 0.0274
Epoch 184/300, trend Loss: 0.0661 | 0.0267
Epoch 185/300, trend Loss: 0.0653 | 0.0263
Epoch 186/300, trend Loss: 0.0658 | 0.0275
Epoch 187/300, trend Loss: 0.0650 | 0.0260
Epoch 188/300, trend Loss: 0.0643 | 0.0255
Epoch 189/300, trend Loss: 0.0648 | 0.0257
Epoch 190/300, trend Loss: 0.0646 | 0.0258
Epoch 191/300, trend Loss: 0.0644 | 0.0259
Epoch 192/300, trend Loss: 0.0645 | 0.0266
Epoch 193/300, trend Loss: 0.0655 | 0.0266
Epoch 194/300, trend Loss: 0.0666 | 0.0269
Epoch 195/300, trend Loss: 0.0663 | 0.0268
Epoch 196/300, trend Loss: 0.0660 | 0.0301
Epoch 197/300, trend Loss: 0.0655 | 0.0264
Epoch 198/300, trend Loss: 0.0647 | 0.0265
Epoch 199/300, trend Loss: 0.0654 | 0.0268
Epoch 200/300, trend Loss: 0.0658 | 0.0294
Epoch 201/300, trend Loss: 0.0663 | 0.0274
Epoch 202/300, trend Loss: 0.0662 | 0.0298
Epoch 203/300, trend Loss: 0.0668 | 0.0277
Epoch 204/300, trend Loss: 0.0676 | 0.0336
Epoch 205/300, trend Loss: 0.0675 | 0.0288
Epoch 206/300, trend Loss: 0.0677 | 0.0344
Epoch 207/300, trend Loss: 0.0659 | 0.0277
Epoch 208/300, trend Loss: 0.0653 | 0.0292
Epoch 209/300, trend Loss: 0.0648 | 0.0265
Epoch 210/300, trend Loss: 0.0641 | 0.0275
Epoch 211/300, trend Loss: 0.0643 | 0.0261
Epoch 212/300, trend Loss: 0.0638 | 0.0261
Epoch 213/300, trend Loss: 0.0632 | 0.0251
Epoch 214/300, trend Loss: 0.0633 | 0.0268
Epoch 215/300, trend Loss: 0.0639 | 0.0255
Epoch 216/300, trend Loss: 0.0638 | 0.0255
Epoch 217/300, trend Loss: 0.0633 | 0.0249
Epoch 218/300, trend Loss: 0.0627 | 0.0255
Epoch 219/300, trend Loss: 0.0630 | 0.0257
Epoch 220/300, trend Loss: 0.0632 | 0.0254
Epoch 221/300, trend Loss: 0.0630 | 0.0251
Epoch 222/300, trend Loss: 0.0631 | 0.0250
Epoch 223/300, trend Loss: 0.0631 | 0.0250
Epoch 224/300, trend Loss: 0.0625 | 0.0250
Epoch 225/300, trend Loss: 0.0633 | 0.0257
Epoch 226/300, trend Loss: 0.0635 | 0.0253
Epoch 227/300, trend Loss: 0.0627 | 0.0250
Epoch 228/300, trend Loss: 0.0625 | 0.0248
Epoch 229/300, trend Loss: 0.0622 | 0.0244
Epoch 230/300, trend Loss: 0.0624 | 0.0248
Epoch 231/300, trend Loss: 0.0622 | 0.0254
Epoch 232/300, trend Loss: 0.0625 | 0.0249
Epoch 233/300, trend Loss: 0.0625 | 0.0250
Epoch 234/300, trend Loss: 0.0621 | 0.0242
Epoch 235/300, trend Loss: 0.0619 | 0.0245
Epoch 236/300, trend Loss: 0.0617 | 0.0246
Epoch 237/300, trend Loss: 0.0615 | 0.0246
Epoch 238/300, trend Loss: 0.0619 | 0.0251
Epoch 239/300, trend Loss: 0.0624 | 0.0243
Epoch 240/300, trend Loss: 0.0621 | 0.0248
Epoch 241/300, trend Loss: 0.0629 | 0.0244
Epoch 242/300, trend Loss: 0.0620 | 0.0246
Epoch 243/300, trend Loss: 0.0622 | 0.0247
Epoch 244/300, trend Loss: 0.0629 | 0.0250
Epoch 245/300, trend Loss: 0.0631 | 0.0246
Epoch 246/300, trend Loss: 0.0624 | 0.0244
Epoch 247/300, trend Loss: 0.0620 | 0.0248
Epoch 248/300, trend Loss: 0.0631 | 0.0250
Epoch 249/300, trend Loss: 0.0625 | 0.0245
Epoch 250/300, trend Loss: 0.0625 | 0.0248
Epoch 251/300, trend Loss: 0.0615 | 0.0245
Epoch 252/300, trend Loss: 0.0624 | 0.0248
Epoch 253/300, trend Loss: 0.0623 | 0.0250
Epoch 254/300, trend Loss: 0.0618 | 0.0245
Epoch 255/300, trend Loss: 0.0611 | 0.0244
Epoch 256/300, trend Loss: 0.0616 | 0.0242
Epoch 257/300, trend Loss: 0.0613 | 0.0246
Epoch 258/300, trend Loss: 0.0616 | 0.0247
Epoch 259/300, trend Loss: 0.0611 | 0.0242
Epoch 260/300, trend Loss: 0.0612 | 0.0240
Epoch 261/300, trend Loss: 0.0611 | 0.0246
Epoch 262/300, trend Loss: 0.0614 | 0.0245
Epoch 263/300, trend Loss: 0.0609 | 0.0241
Epoch 264/300, trend Loss: 0.0611 | 0.0237
Epoch 265/300, trend Loss: 0.0607 | 0.0241
Epoch 266/300, trend Loss: 0.0605 | 0.0244
Epoch 267/300, trend Loss: 0.0607 | 0.0242
Epoch 268/300, trend Loss: 0.0610 | 0.0242
Epoch 269/300, trend Loss: 0.0607 | 0.0238
Epoch 270/300, trend Loss: 0.0607 | 0.0240
Epoch 271/300, trend Loss: 0.0603 | 0.0241
Epoch 272/300, trend Loss: 0.0605 | 0.0243
Epoch 273/300, trend Loss: 0.0608 | 0.0240
Epoch 274/300, trend Loss: 0.0606 | 0.0239
Epoch 275/300, trend Loss: 0.0606 | 0.0238
Epoch 276/300, trend Loss: 0.0602 | 0.0239
Epoch 277/300, trend Loss: 0.0603 | 0.0243
Epoch 278/300, trend Loss: 0.0606 | 0.0240
Epoch 279/300, trend Loss: 0.0605 | 0.0240
Epoch 280/300, trend Loss: 0.0604 | 0.0235
Epoch 281/300, trend Loss: 0.0600 | 0.0238
Epoch 282/300, trend Loss: 0.0600 | 0.0242
Epoch 283/300, trend Loss: 0.0602 | 0.0239
Epoch 284/300, trend Loss: 0.0601 | 0.0238
Epoch 285/300, trend Loss: 0.0602 | 0.0235
Epoch 286/300, trend Loss: 0.0598 | 0.0239
Epoch 287/300, trend Loss: 0.0598 | 0.0242
Epoch 288/300, trend Loss: 0.0600 | 0.0238
Epoch 289/300, trend Loss: 0.0599 | 0.0238
Epoch 290/300, trend Loss: 0.0600 | 0.0234
Epoch 291/300, trend Loss: 0.0597 | 0.0239
Epoch 292/300, trend Loss: 0.0596 | 0.0241
Epoch 293/300, trend Loss: 0.0599 | 0.0238
Epoch 294/300, trend Loss: 0.0599 | 0.0238
Epoch 295/300, trend Loss: 0.0598 | 0.0235
Epoch 296/300, trend Loss: 0.0595 | 0.0238
Epoch 297/300, trend Loss: 0.0595 | 0.0239
Epoch 298/300, trend Loss: 0.0597 | 0.0236
Epoch 299/300, trend Loss: 0.0596 | 0.0237
Epoch 300/300, trend Loss: 0.0597 | 0.0237
Training seasonal_0 component with params: {'observation_period_num': 14, 'train_rates': 0.8570174697785171, 'learning_rate': 0.0006965727363549433, 'batch_size': 172, 'step_size': 4, 'gamma': 0.972792474375392}
Epoch 1/300, seasonal_0 Loss: 3.0101 | 1.5339
Epoch 2/300, seasonal_0 Loss: 0.8616 | 1.1969
Epoch 3/300, seasonal_0 Loss: 0.7017 | 0.6203
Epoch 4/300, seasonal_0 Loss: 0.5713 | 0.6564
Epoch 5/300, seasonal_0 Loss: 0.5103 | 0.6520
Epoch 6/300, seasonal_0 Loss: 0.4560 | 0.4094
Epoch 7/300, seasonal_0 Loss: 0.4121 | 0.4204
Epoch 8/300, seasonal_0 Loss: 0.3000 | 0.1757
Epoch 9/300, seasonal_0 Loss: 0.5808 | 0.5202
Epoch 10/300, seasonal_0 Loss: 0.4272 | 0.4693
Epoch 11/300, seasonal_0 Loss: 0.4436 | 0.4556
Epoch 12/300, seasonal_0 Loss: 0.5223 | 0.5863
Epoch 13/300, seasonal_0 Loss: 0.5860 | 1.0474
Epoch 14/300, seasonal_0 Loss: 0.6236 | 0.6025
Epoch 15/300, seasonal_0 Loss: 0.5527 | 0.8603
Epoch 16/300, seasonal_0 Loss: 0.4595 | 0.6242
Epoch 17/300, seasonal_0 Loss: 0.3521 | 0.5647
Epoch 18/300, seasonal_0 Loss: 0.3684 | 0.6013
Epoch 19/300, seasonal_0 Loss: 0.3790 | 0.5188
Epoch 20/300, seasonal_0 Loss: 0.3495 | 0.5133
Epoch 21/300, seasonal_0 Loss: 0.3653 | 0.7137
Epoch 22/300, seasonal_0 Loss: 0.4229 | 1.1026
Epoch 23/300, seasonal_0 Loss: 0.5064 | 0.7612
Epoch 24/300, seasonal_0 Loss: 0.5365 | 0.7715
Epoch 25/300, seasonal_0 Loss: 0.4699 | 0.7821
Epoch 26/300, seasonal_0 Loss: 0.4700 | 0.7306
Epoch 27/300, seasonal_0 Loss: 0.4787 | 0.7733
Epoch 28/300, seasonal_0 Loss: 0.4817 | 0.7693
Epoch 29/300, seasonal_0 Loss: 0.6539 | 1.7580
Epoch 30/300, seasonal_0 Loss: 0.9971 | 2.0698
Epoch 31/300, seasonal_0 Loss: 0.8502 | 2.0699
Epoch 32/300, seasonal_0 Loss: 0.7939 | 2.0112
Epoch 33/300, seasonal_0 Loss: 0.8023 | 2.0675
Epoch 34/300, seasonal_0 Loss: 0.7806 | 2.0494
Epoch 35/300, seasonal_0 Loss: 0.7779 | 2.0595
Epoch 36/300, seasonal_0 Loss: 0.8446 | 2.0813
Epoch 37/300, seasonal_0 Loss: 0.8838 | 2.1320
Epoch 38/300, seasonal_0 Loss: 0.8686 | 2.1095
Epoch 39/300, seasonal_0 Loss: 0.8704 | 2.1237
Epoch 40/300, seasonal_0 Loss: 0.8657 | 2.1185
Epoch 41/300, seasonal_0 Loss: 0.8643 | 2.1244
Epoch 42/300, seasonal_0 Loss: 0.8618 | 2.1224
Epoch 43/300, seasonal_0 Loss: 0.8601 | 2.1262
Epoch 44/300, seasonal_0 Loss: 0.8584 | 2.1252
Epoch 45/300, seasonal_0 Loss: 0.8568 | 2.1279
Epoch 46/300, seasonal_0 Loss: 0.8554 | 2.1274
Epoch 47/300, seasonal_0 Loss: 0.8539 | 2.1293
Epoch 48/300, seasonal_0 Loss: 0.8529 | 2.1293
Epoch 49/300, seasonal_0 Loss: 0.8515 | 2.1306
Epoch 50/300, seasonal_0 Loss: 0.8507 | 2.1309
Epoch 51/300, seasonal_0 Loss: 0.8493 | 2.1317
Epoch 52/300, seasonal_0 Loss: 0.8487 | 2.1322
Epoch 53/300, seasonal_0 Loss: 0.8474 | 2.1328
Epoch 54/300, seasonal_0 Loss: 0.8470 | 2.1334
Epoch 55/300, seasonal_0 Loss: 0.8457 | 2.1337
Epoch 56/300, seasonal_0 Loss: 0.8454 | 2.1345
Epoch 57/300, seasonal_0 Loss: 0.8441 | 2.1345
Epoch 58/300, seasonal_0 Loss: 0.8439 | 2.1354
Epoch 59/300, seasonal_0 Loss: 0.8427 | 2.1353
Epoch 60/300, seasonal_0 Loss: 0.8426 | 2.1362
Epoch 61/300, seasonal_0 Loss: 0.8415 | 2.1361
Epoch 62/300, seasonal_0 Loss: 0.8413 | 2.1369
Epoch 63/300, seasonal_0 Loss: 0.8403 | 2.1367
Epoch 64/300, seasonal_0 Loss: 0.8402 | 2.1376
Epoch 65/300, seasonal_0 Loss: 0.8392 | 2.1374
Epoch 66/300, seasonal_0 Loss: 0.8391 | 2.1382
Epoch 67/300, seasonal_0 Loss: 0.8382 | 2.1379
Epoch 68/300, seasonal_0 Loss: 0.8382 | 2.1387
Epoch 69/300, seasonal_0 Loss: 0.8373 | 2.1385
Epoch 70/300, seasonal_0 Loss: 0.8372 | 2.1392
Epoch 71/300, seasonal_0 Loss: 0.8364 | 2.1390
Epoch 72/300, seasonal_0 Loss: 0.8364 | 2.1397
Epoch 73/300, seasonal_0 Loss: 0.8356 | 2.1394
Epoch 74/300, seasonal_0 Loss: 0.8356 | 2.1401
Epoch 75/300, seasonal_0 Loss: 0.8348 | 2.1399
Epoch 76/300, seasonal_0 Loss: 0.8348 | 2.1405
Epoch 77/300, seasonal_0 Loss: 0.8341 | 2.1403
Epoch 78/300, seasonal_0 Loss: 0.8341 | 2.1409
Epoch 79/300, seasonal_0 Loss: 0.8334 | 2.1407
Epoch 80/300, seasonal_0 Loss: 0.8334 | 2.1412
Epoch 81/300, seasonal_0 Loss: 0.8328 | 2.1410
Epoch 82/300, seasonal_0 Loss: 0.8328 | 2.1416
Epoch 83/300, seasonal_0 Loss: 0.8322 | 2.1414
Epoch 84/300, seasonal_0 Loss: 0.8322 | 2.1419
Epoch 85/300, seasonal_0 Loss: 0.8316 | 2.1417
Epoch 86/300, seasonal_0 Loss: 0.8316 | 2.1422
Epoch 87/300, seasonal_0 Loss: 0.8310 | 2.1420
Epoch 88/300, seasonal_0 Loss: 0.8311 | 2.1424
Epoch 89/300, seasonal_0 Loss: 0.8305 | 2.1423
Epoch 90/300, seasonal_0 Loss: 0.8306 | 2.1427
Epoch 91/300, seasonal_0 Loss: 0.8300 | 2.1425
Epoch 92/300, seasonal_0 Loss: 0.8301 | 2.1429
Epoch 93/300, seasonal_0 Loss: 0.8296 | 2.1428
Epoch 94/300, seasonal_0 Loss: 0.8296 | 2.1432
Epoch 95/300, seasonal_0 Loss: 0.8291 | 2.1430
Epoch 96/300, seasonal_0 Loss: 0.8291 | 2.1434
Epoch 97/300, seasonal_0 Loss: 0.8287 | 2.1433
Epoch 98/300, seasonal_0 Loss: 0.8287 | 2.1436
Epoch 99/300, seasonal_0 Loss: 0.8283 | 2.1435
Epoch 100/300, seasonal_0 Loss: 0.8283 | 2.1438
Epoch 101/300, seasonal_0 Loss: 0.8279 | 2.1437
Epoch 102/300, seasonal_0 Loss: 0.8279 | 2.1440
Epoch 103/300, seasonal_0 Loss: 0.8275 | 2.1439
Epoch 104/300, seasonal_0 Loss: 0.8275 | 2.1442
Epoch 105/300, seasonal_0 Loss: 0.8272 | 2.1441
Epoch 106/300, seasonal_0 Loss: 0.8272 | 2.1443
Epoch 107/300, seasonal_0 Loss: 0.8268 | 2.1442
Epoch 108/300, seasonal_0 Loss: 0.8268 | 2.1445
Epoch 109/300, seasonal_0 Loss: 0.8265 | 2.1444
Epoch 110/300, seasonal_0 Loss: 0.8265 | 2.1446
Epoch 111/300, seasonal_0 Loss: 0.8262 | 2.1446
Epoch 112/300, seasonal_0 Loss: 0.8262 | 2.1448
Epoch 113/300, seasonal_0 Loss: 0.8259 | 2.1447
Epoch 114/300, seasonal_0 Loss: 0.8259 | 2.1449
Epoch 115/300, seasonal_0 Loss: 0.8256 | 2.1449
Epoch 116/300, seasonal_0 Loss: 0.8256 | 2.1451
Epoch 117/300, seasonal_0 Loss: 0.8253 | 2.1450
Epoch 118/300, seasonal_0 Loss: 0.8253 | 2.1452
Epoch 119/300, seasonal_0 Loss: 0.8250 | 2.1452
Epoch 120/300, seasonal_0 Loss: 0.8250 | 2.1453
Epoch 121/300, seasonal_0 Loss: 0.8248 | 2.1453
Epoch 122/300, seasonal_0 Loss: 0.8248 | 2.1455
Epoch 123/300, seasonal_0 Loss: 0.8245 | 2.1454
Epoch 124/300, seasonal_0 Loss: 0.8245 | 2.1456
Epoch 125/300, seasonal_0 Loss: 0.8243 | 2.1455
Epoch 126/300, seasonal_0 Loss: 0.8243 | 2.1457
Epoch 127/300, seasonal_0 Loss: 0.8240 | 2.1457
Epoch 128/300, seasonal_0 Loss: 0.8241 | 2.1458
Epoch 129/300, seasonal_0 Loss: 0.8238 | 2.1458
Epoch 130/300, seasonal_0 Loss: 0.8238 | 2.1459
Epoch 131/300, seasonal_0 Loss: 0.8236 | 2.1459
Epoch 132/300, seasonal_0 Loss: 0.8236 | 2.1460
Epoch 133/300, seasonal_0 Loss: 0.8234 | 2.1460
Epoch 134/300, seasonal_0 Loss: 0.8234 | 2.1461
Epoch 135/300, seasonal_0 Loss: 0.8232 | 2.1461
Epoch 136/300, seasonal_0 Loss: 0.8232 | 2.1462
Epoch 137/300, seasonal_0 Loss: 0.8230 | 2.1462
Epoch 138/300, seasonal_0 Loss: 0.8230 | 2.1463
Epoch 139/300, seasonal_0 Loss: 0.8228 | 2.1463
Epoch 140/300, seasonal_0 Loss: 0.8228 | 2.1464
Epoch 141/300, seasonal_0 Loss: 0.8226 | 2.1464
Epoch 142/300, seasonal_0 Loss: 0.8226 | 2.1465
Epoch 143/300, seasonal_0 Loss: 0.8224 | 2.1465
Epoch 144/300, seasonal_0 Loss: 0.8224 | 2.1466
Epoch 145/300, seasonal_0 Loss: 0.8223 | 2.1465
Epoch 146/300, seasonal_0 Loss: 0.8223 | 2.1466
Epoch 147/300, seasonal_0 Loss: 0.8221 | 2.1466
Epoch 148/300, seasonal_0 Loss: 0.8221 | 2.1467
Epoch 149/300, seasonal_0 Loss: 0.8219 | 2.1467
Epoch 150/300, seasonal_0 Loss: 0.8219 | 2.1468
Epoch 151/300, seasonal_0 Loss: 0.8218 | 2.1468
Epoch 152/300, seasonal_0 Loss: 0.8218 | 2.1469
Epoch 153/300, seasonal_0 Loss: 0.8216 | 2.1468
Epoch 154/300, seasonal_0 Loss: 0.8216 | 2.1469
Epoch 155/300, seasonal_0 Loss: 0.8215 | 2.1469
Epoch 156/300, seasonal_0 Loss: 0.8215 | 2.1470
Epoch 157/300, seasonal_0 Loss: 0.8213 | 2.1470
Epoch 158/300, seasonal_0 Loss: 0.8213 | 2.1471
Epoch 159/300, seasonal_0 Loss: 0.8212 | 2.1471
Epoch 160/300, seasonal_0 Loss: 0.8212 | 2.1471
Epoch 161/300, seasonal_0 Loss: 0.8210 | 2.1471
Epoch 162/300, seasonal_0 Loss: 0.8211 | 2.1472
Epoch 163/300, seasonal_0 Loss: 0.8209 | 2.1472
Epoch 164/300, seasonal_0 Loss: 0.8209 | 2.1473
Epoch 165/300, seasonal_0 Loss: 0.8208 | 2.1472
Epoch 166/300, seasonal_0 Loss: 0.8208 | 2.1473
Epoch 167/300, seasonal_0 Loss: 0.8207 | 2.1473
Epoch 168/300, seasonal_0 Loss: 0.8207 | 2.1474
Epoch 169/300, seasonal_0 Loss: 0.8205 | 2.1474
Epoch 170/300, seasonal_0 Loss: 0.8206 | 2.1474
Epoch 171/300, seasonal_0 Loss: 0.8204 | 2.1474
Epoch 172/300, seasonal_0 Loss: 0.8204 | 2.1475
Epoch 173/300, seasonal_0 Loss: 0.8203 | 2.1475
Epoch 174/300, seasonal_0 Loss: 0.8203 | 2.1475
Epoch 175/300, seasonal_0 Loss: 0.8202 | 2.1475
Epoch 176/300, seasonal_0 Loss: 0.8202 | 2.1476
Epoch 177/300, seasonal_0 Loss: 0.8201 | 2.1476
Epoch 178/300, seasonal_0 Loss: 0.8201 | 2.1476
Epoch 179/300, seasonal_0 Loss: 0.8200 | 2.1476
Epoch 180/300, seasonal_0 Loss: 0.8200 | 2.1477
Epoch 181/300, seasonal_0 Loss: 0.8199 | 2.1477
Epoch 182/300, seasonal_0 Loss: 0.8199 | 2.1477
Epoch 183/300, seasonal_0 Loss: 0.8198 | 2.1477
Epoch 184/300, seasonal_0 Loss: 0.8198 | 2.1478
Epoch 185/300, seasonal_0 Loss: 0.8197 | 2.1478
Epoch 186/300, seasonal_0 Loss: 0.8197 | 2.1478
Epoch 187/300, seasonal_0 Loss: 0.8196 | 2.1478
Epoch 188/300, seasonal_0 Loss: 0.8196 | 2.1478
Epoch 189/300, seasonal_0 Loss: 0.8195 | 2.1478
Epoch 190/300, seasonal_0 Loss: 0.8195 | 2.1479
Epoch 191/300, seasonal_0 Loss: 0.8194 | 2.1479
Epoch 192/300, seasonal_0 Loss: 0.8194 | 2.1479
Epoch 193/300, seasonal_0 Loss: 0.8193 | 2.1479
Epoch 194/300, seasonal_0 Loss: 0.8193 | 2.1480
Epoch 195/300, seasonal_0 Loss: 0.8193 | 2.1480
Epoch 196/300, seasonal_0 Loss: 0.8193 | 2.1480
Epoch 197/300, seasonal_0 Loss: 0.8192 | 2.1480
Epoch 198/300, seasonal_0 Loss: 0.8192 | 2.1480
Epoch 199/300, seasonal_0 Loss: 0.8191 | 2.1480
Epoch 200/300, seasonal_0 Loss: 0.8191 | 2.1481
Epoch 201/300, seasonal_0 Loss: 0.8190 | 2.1481
Epoch 202/300, seasonal_0 Loss: 0.8190 | 2.1481
Epoch 203/300, seasonal_0 Loss: 0.8189 | 2.1481
Epoch 204/300, seasonal_0 Loss: 0.8189 | 2.1481
Epoch 205/300, seasonal_0 Loss: 0.8189 | 2.1482
Epoch 206/300, seasonal_0 Loss: 0.8189 | 2.1482
Epoch 207/300, seasonal_0 Loss: 0.8188 | 2.1482
Epoch 208/300, seasonal_0 Loss: 0.8188 | 2.1482
Epoch 209/300, seasonal_0 Loss: 0.8187 | 2.1482
Epoch 210/300, seasonal_0 Loss: 0.8187 | 2.1482
Epoch 211/300, seasonal_0 Loss: 0.8187 | 2.1482
Epoch 212/300, seasonal_0 Loss: 0.8187 | 2.1483
Epoch 213/300, seasonal_0 Loss: 0.8186 | 2.1483
Epoch 214/300, seasonal_0 Loss: 0.8186 | 2.1483
Epoch 215/300, seasonal_0 Loss: 0.8185 | 2.1483
Epoch 216/300, seasonal_0 Loss: 0.8185 | 2.1483
Epoch 217/300, seasonal_0 Loss: 0.8185 | 2.1483
Epoch 218/300, seasonal_0 Loss: 0.8185 | 2.1484
Epoch 219/300, seasonal_0 Loss: 0.8184 | 2.1484
Epoch 220/300, seasonal_0 Loss: 0.8184 | 2.1484
Epoch 221/300, seasonal_0 Loss: 0.8183 | 2.1484
Epoch 222/300, seasonal_0 Loss: 0.8183 | 2.1484
Epoch 223/300, seasonal_0 Loss: 0.8183 | 2.1484
Epoch 224/300, seasonal_0 Loss: 0.8183 | 2.1484
Epoch 225/300, seasonal_0 Loss: 0.8182 | 2.1484
Epoch 226/300, seasonal_0 Loss: 0.8182 | 2.1485
Epoch 227/300, seasonal_0 Loss: 0.8182 | 2.1485
Epoch 228/300, seasonal_0 Loss: 0.8182 | 2.1485
Epoch 229/300, seasonal_0 Loss: 0.8181 | 2.1485
Epoch 230/300, seasonal_0 Loss: 0.8181 | 2.1485
Epoch 231/300, seasonal_0 Loss: 0.8181 | 2.1485
Epoch 232/300, seasonal_0 Loss: 0.8181 | 2.1485
Epoch 233/300, seasonal_0 Loss: 0.8180 | 2.1485
Epoch 234/300, seasonal_0 Loss: 0.8180 | 2.1486
Epoch 235/300, seasonal_0 Loss: 0.8180 | 2.1486
Epoch 236/300, seasonal_0 Loss: 0.8180 | 2.1486
Epoch 237/300, seasonal_0 Loss: 0.8179 | 2.1486
Epoch 238/300, seasonal_0 Loss: 0.8179 | 2.1486
Epoch 239/300, seasonal_0 Loss: 0.8179 | 2.1486
Epoch 240/300, seasonal_0 Loss: 0.8179 | 2.1486
Epoch 241/300, seasonal_0 Loss: 0.8178 | 2.1486
Epoch 242/300, seasonal_0 Loss: 0.8178 | 2.1486
Epoch 243/300, seasonal_0 Loss: 0.8178 | 2.1487
Epoch 244/300, seasonal_0 Loss: 0.8178 | 2.1487
Epoch 245/300, seasonal_0 Loss: 0.8177 | 2.1487
Epoch 246/300, seasonal_0 Loss: 0.8177 | 2.1487
Epoch 247/300, seasonal_0 Loss: 0.8177 | 2.1487
Epoch 248/300, seasonal_0 Loss: 0.8177 | 2.1487
Epoch 249/300, seasonal_0 Loss: 0.8176 | 2.1487
Epoch 250/300, seasonal_0 Loss: 0.8176 | 2.1487
Epoch 251/300, seasonal_0 Loss: 0.8176 | 2.1487
Epoch 252/300, seasonal_0 Loss: 0.8176 | 2.1487
Epoch 253/300, seasonal_0 Loss: 0.8176 | 2.1487
Epoch 254/300, seasonal_0 Loss: 0.8176 | 2.1488
Epoch 255/300, seasonal_0 Loss: 0.8175 | 2.1488
Epoch 256/300, seasonal_0 Loss: 0.8175 | 2.1488
Epoch 257/300, seasonal_0 Loss: 0.8175 | 2.1488
Epoch 258/300, seasonal_0 Loss: 0.8175 | 2.1488
Epoch 259/300, seasonal_0 Loss: 0.8174 | 2.1488
Epoch 260/300, seasonal_0 Loss: 0.8174 | 2.1488
Epoch 261/300, seasonal_0 Loss: 0.8174 | 2.1488
Epoch 262/300, seasonal_0 Loss: 0.8174 | 2.1488
Epoch 263/300, seasonal_0 Loss: 0.8174 | 2.1488
Epoch 264/300, seasonal_0 Loss: 0.8174 | 2.1488
Epoch 265/300, seasonal_0 Loss: 0.8173 | 2.1488
Epoch 266/300, seasonal_0 Loss: 0.8173 | 2.1489
Epoch 267/300, seasonal_0 Loss: 0.8173 | 2.1489
Epoch 268/300, seasonal_0 Loss: 0.8173 | 2.1489
Epoch 269/300, seasonal_0 Loss: 0.8173 | 2.1489
Epoch 270/300, seasonal_0 Loss: 0.8173 | 2.1489
Epoch 271/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 272/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 273/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 274/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 275/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 276/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 277/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 278/300, seasonal_0 Loss: 0.8172 | 2.1489
Epoch 279/300, seasonal_0 Loss: 0.8171 | 2.1489
Epoch 280/300, seasonal_0 Loss: 0.8171 | 2.1490
Epoch 281/300, seasonal_0 Loss: 0.8171 | 2.1490
Epoch 282/300, seasonal_0 Loss: 0.8171 | 2.1490
Epoch 283/300, seasonal_0 Loss: 0.8171 | 2.1490
Epoch 284/300, seasonal_0 Loss: 0.8171 | 2.1490
Epoch 285/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 286/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 287/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 288/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 289/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 290/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 291/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 292/300, seasonal_0 Loss: 0.8170 | 2.1490
Epoch 293/300, seasonal_0 Loss: 0.8169 | 2.1490
Epoch 294/300, seasonal_0 Loss: 0.8169 | 2.1490
Epoch 295/300, seasonal_0 Loss: 0.8169 | 2.1490
Epoch 296/300, seasonal_0 Loss: 0.8169 | 2.1490
Epoch 297/300, seasonal_0 Loss: 0.8169 | 2.1490
Epoch 298/300, seasonal_0 Loss: 0.8169 | 2.1491
Epoch 299/300, seasonal_0 Loss: 0.8169 | 2.1491
Epoch 300/300, seasonal_0 Loss: 0.8169 | 2.1491
Training seasonal_1 component with params: {'observation_period_num': 9, 'train_rates': 0.8673154999476134, 'learning_rate': 0.000966480027074729, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8273171140243673}
Epoch 1/300, seasonal_1 Loss: 1.7301 | 1.5672
Epoch 2/300, seasonal_1 Loss: 0.8259 | 1.5347
Epoch 3/300, seasonal_1 Loss: 0.8567 | 1.5375
Epoch 4/300, seasonal_1 Loss: 0.8425 | 1.5327
Epoch 5/300, seasonal_1 Loss: 0.8577 | 1.5397
Epoch 6/300, seasonal_1 Loss: 0.8584 | 1.5329
Epoch 7/300, seasonal_1 Loss: 0.8683 | 1.5327
Epoch 8/300, seasonal_1 Loss: 0.8653 | 1.5329
Epoch 9/300, seasonal_1 Loss: 0.8870 | 1.5359
Epoch 10/300, seasonal_1 Loss: 0.8843 | 1.5381
Epoch 11/300, seasonal_1 Loss: 0.8786 | 1.5479
Epoch 12/300, seasonal_1 Loss: 0.8800 | 1.5759
Epoch 13/300, seasonal_1 Loss: 0.8952 | 1.5746
Epoch 14/300, seasonal_1 Loss: 0.9308 | 1.6148
Epoch 15/300, seasonal_1 Loss: 0.9352 | 1.6410
Epoch 16/300, seasonal_1 Loss: 0.9557 | 1.7634
Epoch 17/300, seasonal_1 Loss: 0.9296 | 1.7947
Epoch 18/300, seasonal_1 Loss: 0.9227 | 1.8147
Epoch 19/300, seasonal_1 Loss: 0.9224 | 1.9154
Epoch 20/300, seasonal_1 Loss: 0.8994 | 1.9158
Epoch 21/300, seasonal_1 Loss: 0.8973 | 1.9806
Epoch 22/300, seasonal_1 Loss: 0.8837 | 1.9759
Epoch 23/300, seasonal_1 Loss: 0.8844 | 1.9777
Epoch 24/300, seasonal_1 Loss: 0.8811 | 2.0190
Epoch 25/300, seasonal_1 Loss: 0.8732 | 2.0160
Epoch 26/300, seasonal_1 Loss: 0.8705 | 2.0427
Epoch 27/300, seasonal_1 Loss: 0.8659 | 2.0420
Epoch 28/300, seasonal_1 Loss: 0.8662 | 2.0413
Epoch 29/300, seasonal_1 Loss: 0.8631 | 2.0588
Epoch 30/300, seasonal_1 Loss: 0.8604 | 2.0598
Epoch 31/300, seasonal_1 Loss: 0.8574 | 2.0710
Epoch 32/300, seasonal_1 Loss: 0.8560 | 2.0728
Epoch 33/300, seasonal_1 Loss: 0.8559 | 2.0722
Epoch 34/300, seasonal_1 Loss: 0.8534 | 2.0799
Epoch 35/300, seasonal_1 Loss: 0.8525 | 2.0820
Epoch 36/300, seasonal_1 Loss: 0.8500 | 2.0872
Epoch 37/300, seasonal_1 Loss: 0.8496 | 2.0893
Epoch 38/300, seasonal_1 Loss: 0.8495 | 2.0892
Epoch 39/300, seasonal_1 Loss: 0.8475 | 2.0927
Epoch 40/300, seasonal_1 Loss: 0.8473 | 2.0945
Epoch 41/300, seasonal_1 Loss: 0.8454 | 2.0972
Epoch 42/300, seasonal_1 Loss: 0.8453 | 2.0989
Epoch 43/300, seasonal_1 Loss: 0.8452 | 2.0992
Epoch 44/300, seasonal_1 Loss: 0.8437 | 2.1009
Epoch 45/300, seasonal_1 Loss: 0.8437 | 2.1022
Epoch 46/300, seasonal_1 Loss: 0.8423 | 2.1037
Epoch 47/300, seasonal_1 Loss: 0.8423 | 2.1049
Epoch 48/300, seasonal_1 Loss: 0.8423 | 2.1054
Epoch 49/300, seasonal_1 Loss: 0.8412 | 2.1063
Epoch 50/300, seasonal_1 Loss: 0.8412 | 2.1072
Epoch 51/300, seasonal_1 Loss: 0.8402 | 2.1081
Epoch 52/300, seasonal_1 Loss: 0.8402 | 2.1089
Epoch 53/300, seasonal_1 Loss: 0.8402 | 2.1093
Epoch 54/300, seasonal_1 Loss: 0.8395 | 2.1099
Epoch 55/300, seasonal_1 Loss: 0.8395 | 2.1105
Epoch 56/300, seasonal_1 Loss: 0.8388 | 2.1111
Epoch 57/300, seasonal_1 Loss: 0.8388 | 2.1116
Epoch 58/300, seasonal_1 Loss: 0.8388 | 2.1120
Epoch 59/300, seasonal_1 Loss: 0.8382 | 2.1123
Epoch 60/300, seasonal_1 Loss: 0.8382 | 2.1127
Epoch 61/300, seasonal_1 Loss: 0.8378 | 2.1131
Epoch 62/300, seasonal_1 Loss: 0.8378 | 2.1135
Epoch 63/300, seasonal_1 Loss: 0.8378 | 2.1137
Epoch 64/300, seasonal_1 Loss: 0.8374 | 2.1140
Epoch 65/300, seasonal_1 Loss: 0.8374 | 2.1143
Epoch 66/300, seasonal_1 Loss: 0.8370 | 2.1145
Epoch 67/300, seasonal_1 Loss: 0.8371 | 2.1148
Epoch 68/300, seasonal_1 Loss: 0.8371 | 2.1150
Epoch 69/300, seasonal_1 Loss: 0.8368 | 2.1152
Epoch 70/300, seasonal_1 Loss: 0.8368 | 2.1153
Epoch 71/300, seasonal_1 Loss: 0.8365 | 2.1155
Epoch 72/300, seasonal_1 Loss: 0.8365 | 2.1157
Epoch 73/300, seasonal_1 Loss: 0.8365 | 2.1158
Epoch 74/300, seasonal_1 Loss: 0.8363 | 2.1160
Epoch 75/300, seasonal_1 Loss: 0.8363 | 2.1161
Epoch 76/300, seasonal_1 Loss: 0.8362 | 2.1162
Epoch 77/300, seasonal_1 Loss: 0.8362 | 2.1163
Epoch 78/300, seasonal_1 Loss: 0.8362 | 2.1164
Epoch 79/300, seasonal_1 Loss: 0.8360 | 2.1165
Epoch 80/300, seasonal_1 Loss: 0.8360 | 2.1166
Epoch 81/300, seasonal_1 Loss: 0.8359 | 2.1167
Epoch 82/300, seasonal_1 Loss: 0.8359 | 2.1168
Epoch 83/300, seasonal_1 Loss: 0.8359 | 2.1168
Epoch 84/300, seasonal_1 Loss: 0.8358 | 2.1169
Epoch 85/300, seasonal_1 Loss: 0.8358 | 2.1170
Epoch 86/300, seasonal_1 Loss: 0.8357 | 2.1170
Epoch 87/300, seasonal_1 Loss: 0.8357 | 2.1171
Epoch 88/300, seasonal_1 Loss: 0.8357 | 2.1171
Epoch 89/300, seasonal_1 Loss: 0.8357 | 2.1172
Epoch 90/300, seasonal_1 Loss: 0.8357 | 2.1172
Epoch 91/300, seasonal_1 Loss: 0.8356 | 2.1173
Epoch 92/300, seasonal_1 Loss: 0.8356 | 2.1173
Epoch 93/300, seasonal_1 Loss: 0.8356 | 2.1173
Epoch 94/300, seasonal_1 Loss: 0.8356 | 2.1174
Epoch 95/300, seasonal_1 Loss: 0.8356 | 2.1174
Epoch 96/300, seasonal_1 Loss: 0.8355 | 2.1174
Epoch 97/300, seasonal_1 Loss: 0.8355 | 2.1175
Epoch 98/300, seasonal_1 Loss: 0.8355 | 2.1175
Epoch 99/300, seasonal_1 Loss: 0.8355 | 2.1175
Epoch 100/300, seasonal_1 Loss: 0.8355 | 2.1175
Epoch 101/300, seasonal_1 Loss: 0.8355 | 2.1175
Epoch 102/300, seasonal_1 Loss: 0.8355 | 2.1176
Epoch 103/300, seasonal_1 Loss: 0.8355 | 2.1176
Epoch 104/300, seasonal_1 Loss: 0.8354 | 2.1176
Epoch 105/300, seasonal_1 Loss: 0.8354 | 2.1176
Epoch 106/300, seasonal_1 Loss: 0.8354 | 2.1176
Epoch 107/300, seasonal_1 Loss: 0.8354 | 2.1176
Epoch 108/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 109/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 110/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 111/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 112/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 113/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 114/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 115/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 116/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 117/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 118/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 119/300, seasonal_1 Loss: 0.8354 | 2.1177
Epoch 120/300, seasonal_1 Loss: 0.8354 | 2.1178
Epoch 121/300, seasonal_1 Loss: 0.8354 | 2.1178
Epoch 122/300, seasonal_1 Loss: 0.8354 | 2.1178
Epoch 123/300, seasonal_1 Loss: 0.8354 | 2.1178
Epoch 124/300, seasonal_1 Loss: 0.8354 | 2.1178
Epoch 125/300, seasonal_1 Loss: 0.8354 | 2.1178
Epoch 126/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 127/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 128/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 129/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 130/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 131/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 132/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 133/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 134/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 135/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 136/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 137/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 138/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 139/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 140/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 141/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 142/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 143/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 144/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 145/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 146/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 147/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 148/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 149/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 150/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 151/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 152/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 153/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 154/300, seasonal_1 Loss: 0.8353 | 2.1178
Epoch 155/300, seasonal_1 Loss: 0.8353 | 2.1178
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9754342431368234, 'learning_rate': 0.00034539161049998825, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9767738170623886}
Epoch 1/300, seasonal_2 Loss: 1.1455 | 0.2409
Epoch 2/300, seasonal_2 Loss: 0.2379 | 0.1114
Epoch 3/300, seasonal_2 Loss: 0.1432 | 0.1152
Epoch 4/300, seasonal_2 Loss: 0.1386 | 0.0866
Epoch 5/300, seasonal_2 Loss: 0.1291 | 0.0721
Epoch 6/300, seasonal_2 Loss: 0.1914 | 0.1054
Epoch 7/300, seasonal_2 Loss: 0.1579 | 0.1225
Epoch 8/300, seasonal_2 Loss: 0.1361 | 0.0890
Epoch 9/300, seasonal_2 Loss: 0.1228 | 0.0752
Epoch 10/300, seasonal_2 Loss: 0.1088 | 0.0834
Epoch 11/300, seasonal_2 Loss: 0.1137 | 0.0676
Epoch 12/300, seasonal_2 Loss: 0.1097 | 0.0920
Epoch 13/300, seasonal_2 Loss: 0.1041 | 0.0494
Epoch 14/300, seasonal_2 Loss: 0.0924 | 0.0604
Epoch 15/300, seasonal_2 Loss: 0.0915 | 0.0688
Epoch 16/300, seasonal_2 Loss: 0.0874 | 0.0670
Epoch 17/300, seasonal_2 Loss: 0.1404 | 0.1021
Epoch 18/300, seasonal_2 Loss: 0.1093 | 0.0435
Epoch 19/300, seasonal_2 Loss: 0.1185 | 0.0564
Epoch 20/300, seasonal_2 Loss: 0.1026 | 0.0741
Epoch 21/300, seasonal_2 Loss: 0.0984 | 0.0549
Epoch 22/300, seasonal_2 Loss: 0.0893 | 0.0700
Epoch 23/300, seasonal_2 Loss: 0.0844 | 0.0654
Epoch 24/300, seasonal_2 Loss: 0.1300 | 0.0800
Epoch 25/300, seasonal_2 Loss: 0.0873 | 0.0505
Epoch 26/300, seasonal_2 Loss: 0.0793 | 0.0554
Epoch 27/300, seasonal_2 Loss: 0.0811 | 0.0758
Epoch 28/300, seasonal_2 Loss: 0.1213 | 0.0672
Epoch 29/300, seasonal_2 Loss: 0.1493 | 0.0968
Epoch 30/300, seasonal_2 Loss: 0.0929 | 0.0401
Epoch 31/300, seasonal_2 Loss: 0.0789 | 0.0481
Epoch 32/300, seasonal_2 Loss: 0.0729 | 0.0437
Epoch 33/300, seasonal_2 Loss: 0.0744 | 0.0442
Epoch 34/300, seasonal_2 Loss: 0.0743 | 0.0347
Epoch 35/300, seasonal_2 Loss: 0.0786 | 0.0500
Epoch 36/300, seasonal_2 Loss: 0.0765 | 0.0353
Epoch 37/300, seasonal_2 Loss: 0.0802 | 0.0650
Epoch 38/300, seasonal_2 Loss: 0.0692 | 0.0491
Epoch 39/300, seasonal_2 Loss: 0.0964 | 0.0411
Epoch 40/300, seasonal_2 Loss: 0.0902 | 0.0514
Epoch 41/300, seasonal_2 Loss: 0.0727 | 0.0275
Epoch 42/300, seasonal_2 Loss: 0.0668 | 0.0295
Epoch 43/300, seasonal_2 Loss: 0.0626 | 0.0338
Epoch 44/300, seasonal_2 Loss: 0.0629 | 0.0273
Epoch 45/300, seasonal_2 Loss: 0.0603 | 0.0231
Epoch 46/300, seasonal_2 Loss: 0.0625 | 0.0315
Epoch 47/300, seasonal_2 Loss: 0.0618 | 0.0257
Epoch 48/300, seasonal_2 Loss: 0.0658 | 0.0246
Epoch 49/300, seasonal_2 Loss: 0.0617 | 0.0335
Epoch 50/300, seasonal_2 Loss: 0.0611 | 0.0313
Epoch 51/300, seasonal_2 Loss: 0.0602 | 0.0378
Epoch 52/300, seasonal_2 Loss: 0.0617 | 0.0344
Epoch 53/300, seasonal_2 Loss: 0.0688 | 0.0249
Epoch 54/300, seasonal_2 Loss: 0.0734 | 0.0377
Epoch 55/300, seasonal_2 Loss: 0.0744 | 0.0892
Epoch 56/300, seasonal_2 Loss: 0.0737 | 0.0356
Epoch 57/300, seasonal_2 Loss: 0.0753 | 0.0404
Epoch 58/300, seasonal_2 Loss: 0.0931 | 0.0613
Epoch 59/300, seasonal_2 Loss: 0.0929 | 0.0303
Epoch 60/300, seasonal_2 Loss: 0.0707 | 0.0494
Epoch 61/300, seasonal_2 Loss: 0.0669 | 0.0446
Epoch 62/300, seasonal_2 Loss: 0.0636 | 0.0296
Epoch 63/300, seasonal_2 Loss: 0.0571 | 0.0234
Epoch 64/300, seasonal_2 Loss: 0.0538 | 0.0216
Epoch 65/300, seasonal_2 Loss: 0.0535 | 0.0204
Epoch 66/300, seasonal_2 Loss: 0.0559 | 0.0215
Epoch 67/300, seasonal_2 Loss: 0.0573 | 0.0299
Epoch 68/300, seasonal_2 Loss: 0.0601 | 0.0551
Epoch 69/300, seasonal_2 Loss: 0.0611 | 0.0259
Epoch 70/300, seasonal_2 Loss: 0.0683 | 0.0270
Epoch 71/300, seasonal_2 Loss: 0.0642 | 0.0306
Epoch 72/300, seasonal_2 Loss: 0.0638 | 0.0258
Epoch 73/300, seasonal_2 Loss: 0.0677 | 0.0358
Epoch 74/300, seasonal_2 Loss: 0.0609 | 0.0298
Epoch 75/300, seasonal_2 Loss: 0.0653 | 0.0303
Epoch 76/300, seasonal_2 Loss: 0.0583 | 0.0277
Epoch 77/300, seasonal_2 Loss: 0.0593 | 0.0227
Epoch 78/300, seasonal_2 Loss: 0.0592 | 0.0263
Epoch 79/300, seasonal_2 Loss: 0.0612 | 0.0357
Epoch 80/300, seasonal_2 Loss: 0.0573 | 0.0251
Epoch 81/300, seasonal_2 Loss: 0.0624 | 0.0744
Epoch 82/300, seasonal_2 Loss: 0.0603 | 0.0214
Epoch 83/300, seasonal_2 Loss: 0.0550 | 0.0207
Epoch 84/300, seasonal_2 Loss: 0.0597 | 0.0308
Epoch 85/300, seasonal_2 Loss: 0.0600 | 0.0458
Epoch 86/300, seasonal_2 Loss: 0.0594 | 0.0347
Epoch 87/300, seasonal_2 Loss: 0.0643 | 0.0410
Epoch 88/300, seasonal_2 Loss: 0.0561 | 0.0357
Epoch 89/300, seasonal_2 Loss: 0.0629 | 0.0379
Epoch 90/300, seasonal_2 Loss: 0.0693 | 0.0465
Epoch 91/300, seasonal_2 Loss: 0.0651 | 0.0264
Epoch 92/300, seasonal_2 Loss: 0.0617 | 0.0312
Epoch 93/300, seasonal_2 Loss: 0.0639 | 0.0288
Epoch 94/300, seasonal_2 Loss: 0.0649 | 0.0238
Epoch 95/300, seasonal_2 Loss: 0.0666 | 0.0474
Epoch 96/300, seasonal_2 Loss: 0.0610 | 0.0548
Epoch 97/300, seasonal_2 Loss: 0.0640 | 0.0463
Epoch 98/300, seasonal_2 Loss: 0.0670 | 0.0341
Epoch 99/300, seasonal_2 Loss: 0.0589 | 0.0242
Epoch 100/300, seasonal_2 Loss: 0.0597 | 0.0277
Epoch 101/300, seasonal_2 Loss: 0.0599 | 0.0358
Epoch 102/300, seasonal_2 Loss: 0.0624 | 0.0374
Epoch 103/300, seasonal_2 Loss: 0.0644 | 0.0460
Epoch 104/300, seasonal_2 Loss: 0.0627 | 0.0407
Epoch 105/300, seasonal_2 Loss: 0.0598 | 0.0393
Epoch 106/300, seasonal_2 Loss: 0.0621 | 0.0285
Epoch 107/300, seasonal_2 Loss: 0.0557 | 0.0290
Epoch 108/300, seasonal_2 Loss: 0.0584 | 0.0345
Epoch 109/300, seasonal_2 Loss: 0.0539 | 0.0783
Epoch 110/300, seasonal_2 Loss: 0.0694 | 0.0278
Epoch 111/300, seasonal_2 Loss: 0.0554 | 0.0206
Epoch 112/300, seasonal_2 Loss: 0.0525 | 0.0235
Epoch 113/300, seasonal_2 Loss: 0.0545 | 0.0280
Epoch 114/300, seasonal_2 Loss: 0.0543 | 0.0229
Epoch 115/300, seasonal_2 Loss: 0.0533 | 0.0235
Epoch 116/300, seasonal_2 Loss: 0.0539 | 0.0320
Epoch 117/300, seasonal_2 Loss: 0.0518 | 0.0247
Epoch 118/300, seasonal_2 Loss: 0.0533 | 0.0314
Epoch 119/300, seasonal_2 Loss: 0.0503 | 0.0263
Epoch 120/300, seasonal_2 Loss: 0.0511 | 0.0313
Epoch 121/300, seasonal_2 Loss: 0.0512 | 0.0199
Epoch 122/300, seasonal_2 Loss: 0.0512 | 0.0289
Epoch 123/300, seasonal_2 Loss: 0.0523 | 0.0247
Epoch 124/300, seasonal_2 Loss: 0.0479 | 0.0281
Epoch 125/300, seasonal_2 Loss: 0.0498 | 0.0404
Epoch 126/300, seasonal_2 Loss: 0.0533 | 0.0268
Epoch 127/300, seasonal_2 Loss: 0.0611 | 0.0238
Epoch 128/300, seasonal_2 Loss: 0.0593 | 0.0297
Epoch 129/300, seasonal_2 Loss: 0.0494 | 0.0284
Epoch 130/300, seasonal_2 Loss: 0.0530 | 0.0281
Epoch 131/300, seasonal_2 Loss: 0.0526 | 0.0298
Epoch 132/300, seasonal_2 Loss: 0.0532 | 0.0315
Epoch 133/300, seasonal_2 Loss: 0.0533 | 0.0390
Epoch 134/300, seasonal_2 Loss: 0.0569 | 0.0342
Epoch 135/300, seasonal_2 Loss: 0.0623 | 0.0273
Epoch 136/300, seasonal_2 Loss: 0.0546 | 0.0281
Epoch 137/300, seasonal_2 Loss: 0.0522 | 0.0236
Epoch 138/300, seasonal_2 Loss: 0.0482 | 0.0261
Epoch 139/300, seasonal_2 Loss: 0.0467 | 0.0299
Epoch 140/300, seasonal_2 Loss: 0.0500 | 0.0376
Epoch 141/300, seasonal_2 Loss: 0.0484 | 0.0464
Epoch 142/300, seasonal_2 Loss: 0.0488 | 0.0443
Epoch 143/300, seasonal_2 Loss: 0.0568 | 0.0330
Epoch 144/300, seasonal_2 Loss: 0.0559 | 0.0209
Epoch 145/300, seasonal_2 Loss: 0.0457 | 0.0248
Epoch 146/300, seasonal_2 Loss: 0.0436 | 0.0214
Epoch 147/300, seasonal_2 Loss: 0.0460 | 0.0237
Epoch 148/300, seasonal_2 Loss: 0.0439 | 0.0221
Epoch 149/300, seasonal_2 Loss: 0.0455 | 0.0221
Epoch 150/300, seasonal_2 Loss: 0.0449 | 0.0234
Epoch 151/300, seasonal_2 Loss: 0.0459 | 0.0237
Epoch 152/300, seasonal_2 Loss: 0.0463 | 0.0282
Epoch 153/300, seasonal_2 Loss: 0.0471 | 0.0349
Epoch 154/300, seasonal_2 Loss: 0.0469 | 0.0277
Epoch 155/300, seasonal_2 Loss: 0.0537 | 0.0314
Epoch 156/300, seasonal_2 Loss: 0.0534 | 0.0337
Epoch 157/300, seasonal_2 Loss: 0.0510 | 0.0285
Epoch 158/300, seasonal_2 Loss: 0.0547 | 0.0340
Epoch 159/300, seasonal_2 Loss: 0.0538 | 0.0278
Epoch 160/300, seasonal_2 Loss: 0.0527 | 0.0259
Epoch 161/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 162/300, seasonal_2 Loss: 0.0502 | 0.0311
Epoch 163/300, seasonal_2 Loss: 0.0471 | 0.0258
Epoch 164/300, seasonal_2 Loss: 0.0449 | 0.0230
Epoch 165/300, seasonal_2 Loss: 0.0448 | 0.0264
Epoch 166/300, seasonal_2 Loss: 0.0442 | 0.0253
Epoch 167/300, seasonal_2 Loss: 0.0463 | 0.0314
Epoch 168/300, seasonal_2 Loss: 0.0462 | 0.0323
Epoch 169/300, seasonal_2 Loss: 0.0470 | 0.0325
Epoch 170/300, seasonal_2 Loss: 0.0475 | 0.0320
Epoch 171/300, seasonal_2 Loss: 0.0560 | 0.0346
Epoch 172/300, seasonal_2 Loss: 0.0516 | 0.0265
Epoch 173/300, seasonal_2 Loss: 0.0489 | 0.0294
Epoch 174/300, seasonal_2 Loss: 0.0474 | 0.0349
Epoch 175/300, seasonal_2 Loss: 0.0544 | 0.0314
Epoch 176/300, seasonal_2 Loss: 0.0481 | 0.0297
Epoch 177/300, seasonal_2 Loss: 0.0463 | 0.0252
Epoch 178/300, seasonal_2 Loss: 0.0463 | 0.0331
Epoch 179/300, seasonal_2 Loss: 0.0431 | 0.0311
Epoch 180/300, seasonal_2 Loss: 0.0417 | 0.0245
Epoch 181/300, seasonal_2 Loss: 0.0590 | 0.0266
Epoch 182/300, seasonal_2 Loss: 0.0493 | 0.0250
Epoch 183/300, seasonal_2 Loss: 0.0459 | 0.0267
Epoch 184/300, seasonal_2 Loss: 0.0439 | 0.0266
Epoch 185/300, seasonal_2 Loss: 0.0437 | 0.0234
Epoch 186/300, seasonal_2 Loss: 0.0421 | 0.0286
Epoch 187/300, seasonal_2 Loss: 0.0417 | 0.0235
Epoch 188/300, seasonal_2 Loss: 0.0440 | 0.0230
Epoch 189/300, seasonal_2 Loss: 0.0426 | 0.0228
Epoch 190/300, seasonal_2 Loss: 0.0429 | 0.0283
Epoch 191/300, seasonal_2 Loss: 0.0424 | 0.0278
Epoch 192/300, seasonal_2 Loss: 0.0424 | 0.0310
Epoch 193/300, seasonal_2 Loss: 0.0420 | 0.0340
Epoch 194/300, seasonal_2 Loss: 0.0418 | 0.0322
Epoch 195/300, seasonal_2 Loss: 0.0436 | 0.0305
Epoch 196/300, seasonal_2 Loss: 0.0464 | 0.0270
Epoch 197/300, seasonal_2 Loss: 0.0482 | 0.0276
Epoch 198/300, seasonal_2 Loss: 0.0481 | 0.0275
Epoch 199/300, seasonal_2 Loss: 0.0436 | 0.0297
Epoch 200/300, seasonal_2 Loss: 0.0420 | 0.0242
Epoch 201/300, seasonal_2 Loss: 0.0446 | 0.0385
Epoch 202/300, seasonal_2 Loss: 0.0454 | 0.0338
Epoch 203/300, seasonal_2 Loss: 0.0443 | 0.0267
Epoch 204/300, seasonal_2 Loss: 0.0463 | 0.0294
Epoch 205/300, seasonal_2 Loss: 0.0449 | 0.0254
Epoch 206/300, seasonal_2 Loss: 0.0430 | 0.0240
Epoch 207/300, seasonal_2 Loss: 0.0427 | 0.0250
Epoch 208/300, seasonal_2 Loss: 0.0441 | 0.0303
Epoch 209/300, seasonal_2 Loss: 0.0431 | 0.0393
Epoch 210/300, seasonal_2 Loss: 0.0436 | 0.0354
Epoch 211/300, seasonal_2 Loss: 0.0443 | 0.0279
Epoch 212/300, seasonal_2 Loss: 0.0450 | 0.0209
Epoch 213/300, seasonal_2 Loss: 0.0460 | 0.0282
Epoch 214/300, seasonal_2 Loss: 0.0411 | 0.0300
Epoch 215/300, seasonal_2 Loss: 0.0447 | 0.0308
Epoch 216/300, seasonal_2 Loss: 0.0442 | 0.0309
Epoch 217/300, seasonal_2 Loss: 0.0488 | 0.0245
Epoch 218/300, seasonal_2 Loss: 0.0392 | 0.0316
Epoch 219/300, seasonal_2 Loss: 0.0434 | 0.0244
Epoch 220/300, seasonal_2 Loss: 0.0406 | 0.0240
Epoch 221/300, seasonal_2 Loss: 0.0401 | 0.0266
Epoch 222/300, seasonal_2 Loss: 0.0408 | 0.0300
Epoch 223/300, seasonal_2 Loss: 0.0386 | 0.0297
Epoch 224/300, seasonal_2 Loss: 0.0389 | 0.0306
Epoch 225/300, seasonal_2 Loss: 0.0448 | 0.0311
Epoch 226/300, seasonal_2 Loss: 0.0485 | 0.0246
Epoch 227/300, seasonal_2 Loss: 0.0427 | 0.0281
Epoch 228/300, seasonal_2 Loss: 0.0423 | 0.0329
Epoch 229/300, seasonal_2 Loss: 0.0423 | 0.0356
Epoch 230/300, seasonal_2 Loss: 0.0430 | 0.0292
Epoch 231/300, seasonal_2 Loss: 0.0455 | 0.0259
Epoch 232/300, seasonal_2 Loss: 0.0396 | 0.0255
Epoch 233/300, seasonal_2 Loss: 0.0394 | 0.0237
Epoch 234/300, seasonal_2 Loss: 0.0367 | 0.0245
Epoch 235/300, seasonal_2 Loss: 0.0400 | 0.0247
Epoch 236/300, seasonal_2 Loss: 0.0398 | 0.0242
Epoch 237/300, seasonal_2 Loss: 0.0404 | 0.0236
Epoch 238/300, seasonal_2 Loss: 0.0401 | 0.0231
Epoch 239/300, seasonal_2 Loss: 0.0404 | 0.0217
Epoch 240/300, seasonal_2 Loss: 0.0387 | 0.0258
Epoch 241/300, seasonal_2 Loss: 0.0387 | 0.0302
Epoch 242/300, seasonal_2 Loss: 0.0373 | 0.0229
Epoch 243/300, seasonal_2 Loss: 0.0399 | 0.0232
Epoch 244/300, seasonal_2 Loss: 0.0395 | 0.0225
Epoch 245/300, seasonal_2 Loss: 0.0368 | 0.0222
Epoch 246/300, seasonal_2 Loss: 0.0401 | 0.0242
Epoch 247/300, seasonal_2 Loss: 0.0404 | 0.0271
Epoch 248/300, seasonal_2 Loss: 0.0389 | 0.0477
Epoch 249/300, seasonal_2 Loss: 0.0393 | 0.0269
Epoch 250/300, seasonal_2 Loss: 0.0405 | 0.0334
Epoch 251/300, seasonal_2 Loss: 0.0366 | 0.0303
Epoch 252/300, seasonal_2 Loss: 0.0404 | 0.0284
Epoch 253/300, seasonal_2 Loss: 0.0337 | 0.0218
Epoch 254/300, seasonal_2 Loss: 0.0340 | 0.0228
Epoch 255/300, seasonal_2 Loss: 0.0356 | 0.0246
Epoch 256/300, seasonal_2 Loss: 0.0348 | 0.0264
Epoch 257/300, seasonal_2 Loss: 0.0351 | 0.0206
Epoch 258/300, seasonal_2 Loss: 0.0390 | 0.0240
Epoch 259/300, seasonal_2 Loss: 0.0352 | 0.0240
Epoch 260/300, seasonal_2 Loss: 0.0380 | 0.0254
Epoch 261/300, seasonal_2 Loss: 0.0351 | 0.0261
Epoch 262/300, seasonal_2 Loss: 0.0362 | 0.0252
Epoch 263/300, seasonal_2 Loss: 0.0350 | 0.0271
Epoch 264/300, seasonal_2 Loss: 0.0355 | 0.0245
Epoch 265/300, seasonal_2 Loss: 0.0342 | 0.0362
Epoch 266/300, seasonal_2 Loss: 0.0383 | 0.0253
Epoch 267/300, seasonal_2 Loss: 0.0363 | 0.0293
Epoch 268/300, seasonal_2 Loss: 0.0365 | 0.0270
Epoch 269/300, seasonal_2 Loss: 0.0345 | 0.0268
Epoch 270/300, seasonal_2 Loss: 0.0341 | 0.0218
Epoch 271/300, seasonal_2 Loss: 0.0327 | 0.0248
Epoch 272/300, seasonal_2 Loss: 0.0334 | 0.0318
Epoch 273/300, seasonal_2 Loss: 0.0317 | 0.0247
Epoch 274/300, seasonal_2 Loss: 0.0351 | 0.0247
Epoch 275/300, seasonal_2 Loss: 0.0366 | 0.0271
Epoch 276/300, seasonal_2 Loss: 0.0362 | 0.0250
Epoch 277/300, seasonal_2 Loss: 0.0362 | 0.0256
Epoch 278/300, seasonal_2 Loss: 0.0333 | 0.0260
Epoch 279/300, seasonal_2 Loss: 0.0354 | 0.0256
Epoch 280/300, seasonal_2 Loss: 0.0334 | 0.0315
Epoch 281/300, seasonal_2 Loss: 0.0431 | 0.0239
Epoch 282/300, seasonal_2 Loss: 0.0407 | 0.0238
Epoch 283/300, seasonal_2 Loss: 0.0398 | 0.0232
Epoch 284/300, seasonal_2 Loss: 0.0407 | 0.0223
Epoch 285/300, seasonal_2 Loss: 0.0379 | 0.0260
Epoch 286/300, seasonal_2 Loss: 0.0387 | 0.0265
Epoch 287/300, seasonal_2 Loss: 0.0421 | 0.0249
Epoch 288/300, seasonal_2 Loss: 0.0409 | 0.0302
Epoch 289/300, seasonal_2 Loss: 0.0392 | 0.0281
Epoch 290/300, seasonal_2 Loss: 0.0392 | 0.0250
Epoch 291/300, seasonal_2 Loss: 0.0397 | 0.0250
Epoch 292/300, seasonal_2 Loss: 0.0394 | 0.0257
Epoch 293/300, seasonal_2 Loss: 0.0379 | 0.0267
Epoch 294/300, seasonal_2 Loss: 0.0376 | 0.0295
Epoch 295/300, seasonal_2 Loss: 0.0384 | 0.0308
Epoch 296/300, seasonal_2 Loss: 0.0389 | 0.0287
Epoch 297/300, seasonal_2 Loss: 0.0357 | 0.0259
Epoch 298/300, seasonal_2 Loss: 0.0357 | 0.0237
Epoch 299/300, seasonal_2 Loss: 0.0340 | 0.0228
Epoch 300/300, seasonal_2 Loss: 0.0370 | 0.0232
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.969038880983348, 'learning_rate': 0.00016242895905399816, 'batch_size': 18, 'step_size': 12, 'gamma': 0.7721114564007053}
Epoch 1/300, seasonal_3 Loss: 0.3037 | 0.0969
Epoch 2/300, seasonal_3 Loss: 0.1014 | 0.0605
Epoch 3/300, seasonal_3 Loss: 0.0972 | 0.0793
Epoch 4/300, seasonal_3 Loss: 0.0908 | 0.0881
Epoch 5/300, seasonal_3 Loss: 0.0941 | 0.0862
Epoch 6/300, seasonal_3 Loss: 0.0900 | 0.0724
Epoch 7/300, seasonal_3 Loss: 0.0834 | 0.0585
Epoch 8/300, seasonal_3 Loss: 0.0759 | 0.0593
Epoch 9/300, seasonal_3 Loss: 0.0771 | 0.0616
Epoch 10/300, seasonal_3 Loss: 0.0775 | 0.0535
Epoch 11/300, seasonal_3 Loss: 0.0743 | 0.0641
Epoch 12/300, seasonal_3 Loss: 0.0755 | 0.0602
Epoch 13/300, seasonal_3 Loss: 0.0733 | 0.0853
Epoch 14/300, seasonal_3 Loss: 0.0726 | 0.0506
Epoch 15/300, seasonal_3 Loss: 0.0670 | 0.0455
Epoch 16/300, seasonal_3 Loss: 0.0682 | 0.0491
Epoch 17/300, seasonal_3 Loss: 0.0681 | 0.0568
Epoch 18/300, seasonal_3 Loss: 0.0646 | 0.0529
Epoch 19/300, seasonal_3 Loss: 0.0616 | 0.0267
Epoch 20/300, seasonal_3 Loss: 0.0570 | 0.0257
Epoch 21/300, seasonal_3 Loss: 0.0565 | 0.0266
Epoch 22/300, seasonal_3 Loss: 0.0562 | 0.0261
Epoch 23/300, seasonal_3 Loss: 0.0554 | 0.0336
Epoch 24/300, seasonal_3 Loss: 0.0580 | 0.0407
Epoch 25/300, seasonal_3 Loss: 0.0555 | 0.0247
Epoch 26/300, seasonal_3 Loss: 0.0548 | 0.0233
Epoch 27/300, seasonal_3 Loss: 0.0530 | 0.0251
Epoch 28/300, seasonal_3 Loss: 0.0525 | 0.0243
Epoch 29/300, seasonal_3 Loss: 0.0535 | 0.0281
Epoch 30/300, seasonal_3 Loss: 0.0524 | 0.0273
Epoch 31/300, seasonal_3 Loss: 0.0507 | 0.0257
Epoch 32/300, seasonal_3 Loss: 0.0495 | 0.0253
Epoch 33/300, seasonal_3 Loss: 0.0483 | 0.0254
Epoch 34/300, seasonal_3 Loss: 0.0477 | 0.0254
Epoch 35/300, seasonal_3 Loss: 0.0473 | 0.0298
Epoch 36/300, seasonal_3 Loss: 0.0472 | 0.0260
Epoch 37/300, seasonal_3 Loss: 0.0467 | 0.0291
Epoch 38/300, seasonal_3 Loss: 0.0456 | 0.0298
Epoch 39/300, seasonal_3 Loss: 0.0456 | 0.0286
Epoch 40/300, seasonal_3 Loss: 0.0451 | 0.0265
Epoch 41/300, seasonal_3 Loss: 0.0450 | 0.0299
Epoch 42/300, seasonal_3 Loss: 0.0455 | 0.0334
Epoch 43/300, seasonal_3 Loss: 0.0450 | 0.0259
Epoch 44/300, seasonal_3 Loss: 0.0439 | 0.0261
Epoch 45/300, seasonal_3 Loss: 0.0467 | 0.0274
Epoch 46/300, seasonal_3 Loss: 0.0447 | 0.0257
Epoch 47/300, seasonal_3 Loss: 0.0448 | 0.0286
Epoch 48/300, seasonal_3 Loss: 0.0444 | 0.0314
Epoch 49/300, seasonal_3 Loss: 0.0442 | 0.0325
Epoch 50/300, seasonal_3 Loss: 0.0426 | 0.0254
Epoch 51/300, seasonal_3 Loss: 0.0391 | 0.0248
Epoch 52/300, seasonal_3 Loss: 0.0411 | 0.0222
Epoch 53/300, seasonal_3 Loss: 0.0404 | 0.0256
Epoch 54/300, seasonal_3 Loss: 0.0424 | 0.0236
Epoch 55/300, seasonal_3 Loss: 0.0395 | 0.0245
Epoch 56/300, seasonal_3 Loss: 0.0420 | 0.0249
Epoch 57/300, seasonal_3 Loss: 0.0408 | 0.0235
Epoch 58/300, seasonal_3 Loss: 0.0401 | 0.0249
Epoch 59/300, seasonal_3 Loss: 0.0387 | 0.0243
Epoch 60/300, seasonal_3 Loss: 0.0369 | 0.0229
Epoch 61/300, seasonal_3 Loss: 0.0357 | 0.0273
Epoch 62/300, seasonal_3 Loss: 0.0361 | 0.0235
Epoch 63/300, seasonal_3 Loss: 0.0339 | 0.0241
Epoch 64/300, seasonal_3 Loss: 0.0334 | 0.0237
Epoch 65/300, seasonal_3 Loss: 0.0327 | 0.0236
Epoch 66/300, seasonal_3 Loss: 0.0324 | 0.0234
Epoch 67/300, seasonal_3 Loss: 0.0318 | 0.0231
Epoch 68/300, seasonal_3 Loss: 0.0315 | 0.0239
Epoch 69/300, seasonal_3 Loss: 0.0313 | 0.0221
Epoch 70/300, seasonal_3 Loss: 0.0323 | 0.0226
Epoch 71/300, seasonal_3 Loss: 0.0318 | 0.0242
Epoch 72/300, seasonal_3 Loss: 0.0312 | 0.0235
Epoch 73/300, seasonal_3 Loss: 0.0342 | 0.0261
Epoch 74/300, seasonal_3 Loss: 0.0336 | 0.0247
Epoch 75/300, seasonal_3 Loss: 0.0317 | 0.0255
Epoch 76/300, seasonal_3 Loss: 0.0301 | 0.0233
Epoch 77/300, seasonal_3 Loss: 0.0291 | 0.0285
Epoch 78/300, seasonal_3 Loss: 0.0290 | 0.0226
Epoch 79/300, seasonal_3 Loss: 0.0312 | 0.0284
Epoch 80/300, seasonal_3 Loss: 0.0296 | 0.0234
Epoch 81/300, seasonal_3 Loss: 0.0282 | 0.0233
Epoch 82/300, seasonal_3 Loss: 0.0271 | 0.0230
Epoch 83/300, seasonal_3 Loss: 0.0265 | 0.0231
Epoch 84/300, seasonal_3 Loss: 0.0262 | 0.0234
Epoch 85/300, seasonal_3 Loss: 0.0260 | 0.0233
Epoch 86/300, seasonal_3 Loss: 0.0258 | 0.0233
Epoch 87/300, seasonal_3 Loss: 0.0256 | 0.0340
Epoch 88/300, seasonal_3 Loss: 0.0260 | 0.0237
Epoch 89/300, seasonal_3 Loss: 0.0284 | 0.0362
Epoch 90/300, seasonal_3 Loss: 0.0284 | 0.0336
Epoch 91/300, seasonal_3 Loss: 0.0276 | 0.0238
Epoch 92/300, seasonal_3 Loss: 0.0262 | 0.0238
Epoch 93/300, seasonal_3 Loss: 0.0255 | 0.0239
Epoch 94/300, seasonal_3 Loss: 0.0254 | 0.0239
Epoch 95/300, seasonal_3 Loss: 0.0252 | 0.0239
Epoch 96/300, seasonal_3 Loss: 0.0251 | 0.0239
Epoch 97/300, seasonal_3 Loss: 0.0250 | 0.0243
Epoch 98/300, seasonal_3 Loss: 0.0249 | 0.0242
Epoch 99/300, seasonal_3 Loss: 0.0248 | 0.0241
Epoch 100/300, seasonal_3 Loss: 0.0247 | 0.0251
Epoch 101/300, seasonal_3 Loss: 0.0249 | 0.0244
Epoch 102/300, seasonal_3 Loss: 0.0247 | 0.0244
Epoch 103/300, seasonal_3 Loss: 0.0246 | 0.0245
Epoch 104/300, seasonal_3 Loss: 0.0246 | 0.0244
Epoch 105/300, seasonal_3 Loss: 0.0244 | 0.0241
Epoch 106/300, seasonal_3 Loss: 0.0249 | 0.0287
Epoch 107/300, seasonal_3 Loss: 0.0250 | 0.0260
Epoch 108/300, seasonal_3 Loss: 0.0245 | 0.0256
Epoch 109/300, seasonal_3 Loss: 0.0244 | 0.0254
Epoch 110/300, seasonal_3 Loss: 0.0244 | 0.0253
Epoch 111/300, seasonal_3 Loss: 0.0243 | 0.0252
Epoch 112/300, seasonal_3 Loss: 0.0242 | 0.0251
Epoch 113/300, seasonal_3 Loss: 0.0242 | 0.0250
Epoch 114/300, seasonal_3 Loss: 0.0241 | 0.0249
Epoch 115/300, seasonal_3 Loss: 0.0239 | 0.0249
Epoch 116/300, seasonal_3 Loss: 0.0238 | 0.0249
Epoch 117/300, seasonal_3 Loss: 0.0237 | 0.0248
Epoch 118/300, seasonal_3 Loss: 0.0236 | 0.0249
Epoch 119/300, seasonal_3 Loss: 0.0239 | 0.0251
Epoch 120/300, seasonal_3 Loss: 0.0238 | 0.0257
Epoch 121/300, seasonal_3 Loss: 0.0239 | 0.0255
Epoch 122/300, seasonal_3 Loss: 0.0237 | 0.0253
Epoch 123/300, seasonal_3 Loss: 0.0234 | 0.0251
Epoch 124/300, seasonal_3 Loss: 0.0234 | 0.0254
Epoch 125/300, seasonal_3 Loss: 0.0235 | 0.0251
Epoch 126/300, seasonal_3 Loss: 0.0231 | 0.0253
Epoch 127/300, seasonal_3 Loss: 0.0234 | 0.0253
Epoch 128/300, seasonal_3 Loss: 0.0234 | 0.0256
Epoch 129/300, seasonal_3 Loss: 0.0233 | 0.0253
Epoch 130/300, seasonal_3 Loss: 0.0230 | 0.0253
Epoch 131/300, seasonal_3 Loss: 0.0229 | 0.0253
Epoch 132/300, seasonal_3 Loss: 0.0229 | 0.0254
Epoch 133/300, seasonal_3 Loss: 0.0230 | 0.0254
Epoch 134/300, seasonal_3 Loss: 0.0230 | 0.0256
Epoch 135/300, seasonal_3 Loss: 0.0228 | 0.0254
Epoch 136/300, seasonal_3 Loss: 0.0226 | 0.0255
Epoch 137/300, seasonal_3 Loss: 0.0226 | 0.0255
Epoch 138/300, seasonal_3 Loss: 0.0226 | 0.0255
Epoch 139/300, seasonal_3 Loss: 0.0225 | 0.0255
Epoch 140/300, seasonal_3 Loss: 0.0225 | 0.0256
Epoch 141/300, seasonal_3 Loss: 0.0224 | 0.0256
Epoch 142/300, seasonal_3 Loss: 0.0224 | 0.0256
Epoch 143/300, seasonal_3 Loss: 0.0225 | 0.0256
Epoch 144/300, seasonal_3 Loss: 0.0228 | 0.0257
Epoch 145/300, seasonal_3 Loss: 0.0228 | 0.0258
Epoch 146/300, seasonal_3 Loss: 0.0225 | 0.0257
Epoch 147/300, seasonal_3 Loss: 0.0223 | 0.0257
Epoch 148/300, seasonal_3 Loss: 0.0223 | 0.0257
Epoch 149/300, seasonal_3 Loss: 0.0223 | 0.0257
Epoch 150/300, seasonal_3 Loss: 0.0223 | 0.0257
Epoch 151/300, seasonal_3 Loss: 0.0222 | 0.0257
Epoch 152/300, seasonal_3 Loss: 0.0222 | 0.0258
Epoch 153/300, seasonal_3 Loss: 0.0222 | 0.0258
Epoch 154/300, seasonal_3 Loss: 0.0222 | 0.0258
Epoch 155/300, seasonal_3 Loss: 0.0222 | 0.0258
Epoch 156/300, seasonal_3 Loss: 0.0222 | 0.0258
Epoch 157/300, seasonal_3 Loss: 0.0221 | 0.0258
Epoch 158/300, seasonal_3 Loss: 0.0221 | 0.0258
Epoch 159/300, seasonal_3 Loss: 0.0221 | 0.0258
Epoch 160/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 161/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 162/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 163/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 164/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 165/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 166/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 167/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 168/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 169/300, seasonal_3 Loss: 0.0221 | 0.0259
Epoch 170/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 171/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 172/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 173/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 174/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 175/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 176/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 177/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 178/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 179/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 180/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 181/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 182/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 183/300, seasonal_3 Loss: 0.0220 | 0.0260
Epoch 184/300, seasonal_3 Loss: 0.0219 | 0.0260
Epoch 185/300, seasonal_3 Loss: 0.0219 | 0.0260
Epoch 186/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 187/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 188/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 189/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 190/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 191/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 192/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 193/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 194/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 195/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 196/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 197/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 198/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 199/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 200/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 201/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 202/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 203/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 204/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 205/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 206/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 207/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 208/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 209/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 210/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 211/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 212/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 213/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 214/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 215/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 216/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 217/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 218/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 219/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 220/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 221/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 222/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 223/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 224/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 225/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 226/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 227/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 228/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 229/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 230/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 231/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 232/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 233/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 234/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 235/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 236/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 237/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 238/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 239/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 240/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 241/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 242/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 243/300, seasonal_3 Loss: 0.0219 | 0.0261
Epoch 244/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 245/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 246/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 247/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 248/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 249/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 250/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 251/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 252/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 253/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 254/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 255/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 256/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 257/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 258/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 259/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 260/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 261/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 262/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 263/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 264/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 265/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 266/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 267/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 268/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 269/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 270/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 271/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 272/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 273/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 274/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 275/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 276/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 277/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 278/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 279/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 280/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 281/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 282/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 283/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 284/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 285/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 286/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 287/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 288/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 289/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 290/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 291/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 292/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 293/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 294/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 295/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 296/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 297/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 298/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 299/300, seasonal_3 Loss: 0.0219 | 0.0262
Epoch 300/300, seasonal_3 Loss: 0.0219 | 0.0262
Training resid component with params: {'observation_period_num': 16, 'train_rates': 0.9815874697866309, 'learning_rate': 0.00020640964113191352, 'batch_size': 28, 'step_size': 13, 'gamma': 0.7794326497921917}
Epoch 1/300, resid Loss: 0.2851 | 0.1401
Epoch 2/300, resid Loss: 0.1138 | 0.0828
Epoch 3/300, resid Loss: 0.1143 | 0.1006
Epoch 4/300, resid Loss: 0.0990 | 0.0569
Epoch 5/300, resid Loss: 0.0934 | 0.0451
Epoch 6/300, resid Loss: 0.0865 | 0.0486
Epoch 7/300, resid Loss: 0.0871 | 0.0488
Epoch 8/300, resid Loss: 0.0818 | 0.0588
Epoch 9/300, resid Loss: 0.0805 | 0.0618
Epoch 10/300, resid Loss: 0.0843 | 0.0606
Epoch 11/300, resid Loss: 0.0772 | 0.0662
Epoch 12/300, resid Loss: 0.0755 | 0.0939
Epoch 13/300, resid Loss: 0.0707 | 0.0701
Epoch 14/300, resid Loss: 0.0675 | 0.0328
Epoch 15/300, resid Loss: 0.0612 | 0.0274
Epoch 16/300, resid Loss: 0.0594 | 0.0277
Epoch 17/300, resid Loss: 0.0586 | 0.0268
Epoch 18/300, resid Loss: 0.0539 | 0.0280
Epoch 19/300, resid Loss: 0.0573 | 0.0329
Epoch 20/300, resid Loss: 0.0568 | 0.0335
Epoch 21/300, resid Loss: 0.0537 | 0.0406
Epoch 22/300, resid Loss: 0.0518 | 0.0275
Epoch 23/300, resid Loss: 0.0516 | 0.0346
Epoch 24/300, resid Loss: 0.0510 | 0.0356
Epoch 25/300, resid Loss: 0.0502 | 0.0326
Epoch 26/300, resid Loss: 0.0511 | 0.0270
Epoch 27/300, resid Loss: 0.0485 | 0.0299
Epoch 28/300, resid Loss: 0.0479 | 0.0258
Epoch 29/300, resid Loss: 0.0488 | 0.0294
Epoch 30/300, resid Loss: 0.0458 | 0.0291
Epoch 31/300, resid Loss: 0.0456 | 0.0386
Epoch 32/300, resid Loss: 0.0446 | 0.0341
Epoch 33/300, resid Loss: 0.0435 | 0.0333
Epoch 34/300, resid Loss: 0.0434 | 0.0374
Epoch 35/300, resid Loss: 0.0433 | 0.0273
Epoch 36/300, resid Loss: 0.0419 | 0.0266
Epoch 37/300, resid Loss: 0.0421 | 0.0260
Epoch 38/300, resid Loss: 0.0440 | 0.0259
Epoch 39/300, resid Loss: 0.0425 | 0.0282
Epoch 40/300, resid Loss: 0.0412 | 0.0283
Epoch 41/300, resid Loss: 0.0405 | 0.0282
Epoch 42/300, resid Loss: 0.0394 | 0.0344
Epoch 43/300, resid Loss: 0.0380 | 0.0287
Epoch 44/300, resid Loss: 0.0390 | 0.0307
Epoch 45/300, resid Loss: 0.0367 | 0.0346
Epoch 46/300, resid Loss: 0.0368 | 0.0295
Epoch 47/300, resid Loss: 0.0391 | 0.0303
Epoch 48/300, resid Loss: 0.0369 | 0.0290
Epoch 49/300, resid Loss: 0.0340 | 0.0277
Epoch 50/300, resid Loss: 0.0359 | 0.0320
Epoch 51/300, resid Loss: 0.0373 | 0.0340
Epoch 52/300, resid Loss: 0.0356 | 0.0275
Epoch 53/300, resid Loss: 0.0318 | 0.0376
Epoch 54/300, resid Loss: 0.0312 | 0.0320
Epoch 55/300, resid Loss: 0.0313 | 0.0272
Epoch 56/300, resid Loss: 0.0371 | 0.0293
Epoch 57/300, resid Loss: 0.0316 | 0.0295
Epoch 58/300, resid Loss: 0.0323 | 0.0297
Epoch 59/300, resid Loss: 0.0319 | 0.0306
Epoch 60/300, resid Loss: 0.0314 | 0.0344
Epoch 61/300, resid Loss: 0.0314 | 0.0317
Epoch 62/300, resid Loss: 0.0303 | 0.0318
Epoch 63/300, resid Loss: 0.0311 | 0.0306
Epoch 64/300, resid Loss: 0.0367 | 0.0326
Epoch 65/300, resid Loss: 0.0368 | 0.0306
Epoch 66/300, resid Loss: 0.0377 | 0.0404
Epoch 67/300, resid Loss: 0.0337 | 0.0290
Epoch 68/300, resid Loss: 0.0358 | 0.0851
Epoch 69/300, resid Loss: 0.0335 | 0.0742
Epoch 70/300, resid Loss: 0.0339 | 0.0828
Epoch 71/300, resid Loss: 0.0307 | 0.0552
Epoch 72/300, resid Loss: 0.0295 | 0.0470
Epoch 73/300, resid Loss: 0.0300 | 0.0361
Epoch 74/300, resid Loss: 0.0301 | 0.0331
Epoch 75/300, resid Loss: 0.0299 | 0.0336
Epoch 76/300, resid Loss: 0.0271 | 0.0337
Epoch 77/300, resid Loss: 0.0256 | 0.0328
Epoch 78/300, resid Loss: 0.0274 | 0.0318
Epoch 79/300, resid Loss: 0.0245 | 0.0305
Epoch 80/300, resid Loss: 0.0263 | 0.0303
Epoch 81/300, resid Loss: 0.0255 | 0.0300
Epoch 82/300, resid Loss: 0.0243 | 0.0309
Epoch 83/300, resid Loss: 0.0264 | 0.0313
Epoch 84/300, resid Loss: 0.0319 | 0.0315
Epoch 85/300, resid Loss: 0.0296 | 0.0315
Epoch 86/300, resid Loss: 0.0251 | 0.0321
Epoch 87/300, resid Loss: 0.0246 | 0.0329
Epoch 88/300, resid Loss: 0.0246 | 0.0328
Epoch 89/300, resid Loss: 0.0251 | 0.0343
Epoch 90/300, resid Loss: 0.0248 | 0.0371
Epoch 91/300, resid Loss: 0.0240 | 0.0413
Epoch 92/300, resid Loss: 0.0238 | 0.0457
Epoch 93/300, resid Loss: 0.0238 | 0.0325
Epoch 94/300, resid Loss: 0.0231 | 0.0286
Epoch 95/300, resid Loss: 0.0228 | 0.0286
Epoch 96/300, resid Loss: 0.0224 | 0.0290
Epoch 97/300, resid Loss: 0.0222 | 0.0299
Epoch 98/300, resid Loss: 0.0223 | 0.0305
Epoch 99/300, resid Loss: 0.0243 | 0.0309
Epoch 100/300, resid Loss: 0.0246 | 0.0321
Epoch 101/300, resid Loss: 0.0229 | 0.0315
Epoch 102/300, resid Loss: 0.0225 | 0.0308
Epoch 103/300, resid Loss: 0.0221 | 0.0303
Epoch 104/300, resid Loss: 0.0220 | 0.0300
Epoch 105/300, resid Loss: 0.0219 | 0.0299
Epoch 106/300, resid Loss: 0.0218 | 0.0298
Epoch 107/300, resid Loss: 0.0218 | 0.0299
Epoch 108/300, resid Loss: 0.0217 | 0.0300
Epoch 109/300, resid Loss: 0.0217 | 0.0300
Epoch 110/300, resid Loss: 0.0217 | 0.0301
Epoch 111/300, resid Loss: 0.0216 | 0.0301
Epoch 112/300, resid Loss: 0.0214 | 0.0300
Epoch 113/300, resid Loss: 0.0225 | 0.0303
Epoch 114/300, resid Loss: 0.0219 | 0.0300
Epoch 115/300, resid Loss: 0.0215 | 0.0299
Epoch 116/300, resid Loss: 0.0215 | 0.0298
Epoch 117/300, resid Loss: 0.0214 | 0.0298
Epoch 118/300, resid Loss: 0.0214 | 0.0299
Epoch 119/300, resid Loss: 0.0214 | 0.0299
Epoch 120/300, resid Loss: 0.0214 | 0.0299
Epoch 121/300, resid Loss: 0.0214 | 0.0299
Epoch 122/300, resid Loss: 0.0213 | 0.0299
Epoch 123/300, resid Loss: 0.0213 | 0.0298
Epoch 124/300, resid Loss: 0.0213 | 0.0298
Epoch 125/300, resid Loss: 0.0211 | 0.0298
Epoch 126/300, resid Loss: 0.0208 | 0.0298
Epoch 127/300, resid Loss: 0.0208 | 0.0299
Epoch 128/300, resid Loss: 0.0207 | 0.0299
Epoch 129/300, resid Loss: 0.0206 | 0.0299
Epoch 130/300, resid Loss: 0.0206 | 0.0299
Epoch 131/300, resid Loss: 0.0206 | 0.0299
Epoch 132/300, resid Loss: 0.0205 | 0.0298
Epoch 133/300, resid Loss: 0.0205 | 0.0298
Epoch 134/300, resid Loss: 0.0205 | 0.0298
Epoch 135/300, resid Loss: 0.0205 | 0.0298
Epoch 136/300, resid Loss: 0.0205 | 0.0298
Epoch 137/300, resid Loss: 0.0205 | 0.0298
Epoch 138/300, resid Loss: 0.0204 | 0.0298
Epoch 139/300, resid Loss: 0.0204 | 0.0298
Epoch 140/300, resid Loss: 0.0204 | 0.0298
Epoch 141/300, resid Loss: 0.0204 | 0.0298
Epoch 142/300, resid Loss: 0.0204 | 0.0298
Epoch 143/300, resid Loss: 0.0204 | 0.0298
Epoch 144/300, resid Loss: 0.0204 | 0.0298
Epoch 145/300, resid Loss: 0.0203 | 0.0297
Epoch 146/300, resid Loss: 0.0203 | 0.0297
Epoch 147/300, resid Loss: 0.0203 | 0.0297
Epoch 148/300, resid Loss: 0.0203 | 0.0298
Epoch 149/300, resid Loss: 0.0203 | 0.0298
Epoch 150/300, resid Loss: 0.0203 | 0.0298
Epoch 151/300, resid Loss: 0.0203 | 0.0298
Epoch 152/300, resid Loss: 0.0203 | 0.0298
Epoch 153/300, resid Loss: 0.0203 | 0.0298
Epoch 154/300, resid Loss: 0.0203 | 0.0297
Epoch 155/300, resid Loss: 0.0202 | 0.0297
Epoch 156/300, resid Loss: 0.0202 | 0.0297
Epoch 157/300, resid Loss: 0.0202 | 0.0297
Epoch 158/300, resid Loss: 0.0202 | 0.0297
Epoch 159/300, resid Loss: 0.0202 | 0.0297
Epoch 160/300, resid Loss: 0.0202 | 0.0297
Epoch 161/300, resid Loss: 0.0202 | 0.0297
Epoch 162/300, resid Loss: 0.0202 | 0.0297
Epoch 163/300, resid Loss: 0.0202 | 0.0297
Epoch 164/300, resid Loss: 0.0202 | 0.0297
Epoch 165/300, resid Loss: 0.0202 | 0.0297
Epoch 166/300, resid Loss: 0.0202 | 0.0297
Epoch 167/300, resid Loss: 0.0202 | 0.0297
Epoch 168/300, resid Loss: 0.0202 | 0.0297
Epoch 169/300, resid Loss: 0.0202 | 0.0297
Epoch 170/300, resid Loss: 0.0201 | 0.0297
Epoch 171/300, resid Loss: 0.0201 | 0.0297
Epoch 172/300, resid Loss: 0.0201 | 0.0297
Epoch 173/300, resid Loss: 0.0201 | 0.0297
Epoch 174/300, resid Loss: 0.0201 | 0.0297
Epoch 175/300, resid Loss: 0.0201 | 0.0297
Epoch 176/300, resid Loss: 0.0201 | 0.0297
Epoch 177/300, resid Loss: 0.0201 | 0.0297
Epoch 178/300, resid Loss: 0.0201 | 0.0297
Epoch 179/300, resid Loss: 0.0201 | 0.0297
Epoch 180/300, resid Loss: 0.0201 | 0.0297
Epoch 181/300, resid Loss: 0.0201 | 0.0297
Epoch 182/300, resid Loss: 0.0201 | 0.0297
Epoch 183/300, resid Loss: 0.0201 | 0.0297
Epoch 184/300, resid Loss: 0.0201 | 0.0297
Epoch 185/300, resid Loss: 0.0201 | 0.0297
Epoch 186/300, resid Loss: 0.0201 | 0.0297
Epoch 187/300, resid Loss: 0.0201 | 0.0297
Epoch 188/300, resid Loss: 0.0201 | 0.0297
Epoch 189/300, resid Loss: 0.0201 | 0.0297
Epoch 190/300, resid Loss: 0.0201 | 0.0297
Epoch 191/300, resid Loss: 0.0201 | 0.0297
Epoch 192/300, resid Loss: 0.0201 | 0.0297
Epoch 193/300, resid Loss: 0.0201 | 0.0297
Epoch 194/300, resid Loss: 0.0201 | 0.0297
Epoch 195/300, resid Loss: 0.0201 | 0.0297
Epoch 196/300, resid Loss: 0.0201 | 0.0297
Epoch 197/300, resid Loss: 0.0201 | 0.0297
Epoch 198/300, resid Loss: 0.0201 | 0.0297
Epoch 199/300, resid Loss: 0.0201 | 0.0297
Epoch 200/300, resid Loss: 0.0201 | 0.0297
Epoch 201/300, resid Loss: 0.0201 | 0.0297
Epoch 202/300, resid Loss: 0.0201 | 0.0297
Epoch 203/300, resid Loss: 0.0200 | 0.0297
Epoch 204/300, resid Loss: 0.0200 | 0.0297
Epoch 205/300, resid Loss: 0.0200 | 0.0297
Epoch 206/300, resid Loss: 0.0200 | 0.0297
Epoch 207/300, resid Loss: 0.0200 | 0.0297
Epoch 208/300, resid Loss: 0.0200 | 0.0297
Epoch 209/300, resid Loss: 0.0200 | 0.0297
Epoch 210/300, resid Loss: 0.0200 | 0.0297
Epoch 211/300, resid Loss: 0.0200 | 0.0297
Epoch 212/300, resid Loss: 0.0200 | 0.0297
Epoch 213/300, resid Loss: 0.0200 | 0.0297
Epoch 214/300, resid Loss: 0.0200 | 0.0297
Epoch 215/300, resid Loss: 0.0200 | 0.0297
Epoch 216/300, resid Loss: 0.0200 | 0.0297
Epoch 217/300, resid Loss: 0.0200 | 0.0297
Epoch 218/300, resid Loss: 0.0200 | 0.0297
Epoch 219/300, resid Loss: 0.0200 | 0.0297
Epoch 220/300, resid Loss: 0.0200 | 0.0297
Epoch 221/300, resid Loss: 0.0200 | 0.0297
Epoch 222/300, resid Loss: 0.0200 | 0.0297
Epoch 223/300, resid Loss: 0.0200 | 0.0297
Epoch 224/300, resid Loss: 0.0200 | 0.0297
Epoch 225/300, resid Loss: 0.0200 | 0.0297
Epoch 226/300, resid Loss: 0.0200 | 0.0297
Epoch 227/300, resid Loss: 0.0200 | 0.0297
Epoch 228/300, resid Loss: 0.0200 | 0.0297
Epoch 229/300, resid Loss: 0.0200 | 0.0297
Epoch 230/300, resid Loss: 0.0200 | 0.0297
Epoch 231/300, resid Loss: 0.0200 | 0.0297
Epoch 232/300, resid Loss: 0.0200 | 0.0297
Epoch 233/300, resid Loss: 0.0200 | 0.0297
Epoch 234/300, resid Loss: 0.0200 | 0.0297
Epoch 235/300, resid Loss: 0.0200 | 0.0297
Epoch 236/300, resid Loss: 0.0200 | 0.0297
Epoch 237/300, resid Loss: 0.0200 | 0.0297
Epoch 238/300, resid Loss: 0.0200 | 0.0297
Epoch 239/300, resid Loss: 0.0200 | 0.0297
Epoch 240/300, resid Loss: 0.0200 | 0.0297
Epoch 241/300, resid Loss: 0.0200 | 0.0297
Epoch 242/300, resid Loss: 0.0200 | 0.0297
Epoch 243/300, resid Loss: 0.0200 | 0.0297
Epoch 244/300, resid Loss: 0.0200 | 0.0297
Epoch 245/300, resid Loss: 0.0200 | 0.0297
Epoch 246/300, resid Loss: 0.0200 | 0.0297
Epoch 247/300, resid Loss: 0.0200 | 0.0297
Epoch 248/300, resid Loss: 0.0200 | 0.0297
Epoch 249/300, resid Loss: 0.0200 | 0.0297
Epoch 250/300, resid Loss: 0.0200 | 0.0297
Epoch 251/300, resid Loss: 0.0200 | 0.0297
Epoch 252/300, resid Loss: 0.0200 | 0.0297
Epoch 253/300, resid Loss: 0.0200 | 0.0297
Epoch 254/300, resid Loss: 0.0200 | 0.0297
Epoch 255/300, resid Loss: 0.0200 | 0.0297
Epoch 256/300, resid Loss: 0.0200 | 0.0297
Epoch 257/300, resid Loss: 0.0200 | 0.0297
Epoch 258/300, resid Loss: 0.0200 | 0.0297
Epoch 259/300, resid Loss: 0.0200 | 0.0297
Epoch 260/300, resid Loss: 0.0200 | 0.0297
Epoch 261/300, resid Loss: 0.0200 | 0.0297
Epoch 262/300, resid Loss: 0.0200 | 0.0297
Epoch 263/300, resid Loss: 0.0200 | 0.0297
Epoch 264/300, resid Loss: 0.0200 | 0.0297
Epoch 265/300, resid Loss: 0.0200 | 0.0297
Epoch 266/300, resid Loss: 0.0200 | 0.0297
Epoch 267/300, resid Loss: 0.0200 | 0.0297
Epoch 268/300, resid Loss: 0.0200 | 0.0297
Epoch 269/300, resid Loss: 0.0200 | 0.0297
Epoch 270/300, resid Loss: 0.0200 | 0.0297
Epoch 271/300, resid Loss: 0.0200 | 0.0297
Epoch 272/300, resid Loss: 0.0200 | 0.0297
Epoch 273/300, resid Loss: 0.0200 | 0.0297
Epoch 274/300, resid Loss: 0.0200 | 0.0297
Epoch 275/300, resid Loss: 0.0200 | 0.0297
Epoch 276/300, resid Loss: 0.0200 | 0.0297
Epoch 277/300, resid Loss: 0.0200 | 0.0297
Epoch 278/300, resid Loss: 0.0200 | 0.0297
Epoch 279/300, resid Loss: 0.0200 | 0.0297
Epoch 280/300, resid Loss: 0.0200 | 0.0297
Epoch 281/300, resid Loss: 0.0200 | 0.0297
Epoch 282/300, resid Loss: 0.0200 | 0.0297
Epoch 283/300, resid Loss: 0.0200 | 0.0297
Epoch 284/300, resid Loss: 0.0200 | 0.0297
Epoch 285/300, resid Loss: 0.0200 | 0.0297
Epoch 286/300, resid Loss: 0.0200 | 0.0297
Epoch 287/300, resid Loss: 0.0200 | 0.0297
Epoch 288/300, resid Loss: 0.0200 | 0.0297
Epoch 289/300, resid Loss: 0.0200 | 0.0297
Epoch 290/300, resid Loss: 0.0200 | 0.0297
Epoch 291/300, resid Loss: 0.0200 | 0.0297
Epoch 292/300, resid Loss: 0.0200 | 0.0297
Epoch 293/300, resid Loss: 0.0200 | 0.0297
Epoch 294/300, resid Loss: 0.0200 | 0.0297
Epoch 295/300, resid Loss: 0.0200 | 0.0297
Epoch 296/300, resid Loss: 0.0200 | 0.0297
Epoch 297/300, resid Loss: 0.0200 | 0.0297
Epoch 298/300, resid Loss: 0.0200 | 0.0297
Epoch 299/300, resid Loss: 0.0200 | 0.0297
Epoch 300/300, resid Loss: 0.0200 | 0.0297
Runtime (seconds): 10818.125725269318
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[213.37479]
[-0.2151512]
[-0.13578144]
[4.0666323]
[0.281568]
[6.001054]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 42.209543380886316
RMSE: 6.49688720703125
MAE: 6.49688720703125
R-squared: nan
[223.37311]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
