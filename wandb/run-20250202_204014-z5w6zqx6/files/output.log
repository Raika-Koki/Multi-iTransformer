ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-02 20:40:20,326][0m A new study created in memory with name: no-name-081a7d88-c9b1-4c9a-8350-a8ea6fb1394e[0m
[32m[I 2025-02-02 20:40:40,584][0m Trial 0 finished with value: 0.21829479356361514 and parameters: {'observation_period_num': 132, 'train_rates': 0.8277365925603242, 'learning_rate': 0.00014121967023635566, 'batch_size': 238, 'step_size': 6, 'gamma': 0.7713204171782394}. Best is trial 0 with value: 0.21829479356361514.[0m
[32m[I 2025-02-02 20:43:23,911][0m Trial 1 finished with value: 0.1616755567672776 and parameters: {'observation_period_num': 149, 'train_rates': 0.9854836660493405, 'learning_rate': 1.7088247354968468e-05, 'batch_size': 24, 'step_size': 7, 'gamma': 0.7570912406491613}. Best is trial 1 with value: 0.1616755567672776.[0m
[32m[I 2025-02-02 20:43:48,408][0m Trial 2 finished with value: 0.24698284653157745 and parameters: {'observation_period_num': 235, 'train_rates': 0.6196247591374765, 'learning_rate': 0.00015454673379490007, 'batch_size': 139, 'step_size': 12, 'gamma': 0.7891378127652373}. Best is trial 1 with value: 0.1616755567672776.[0m
[32m[I 2025-02-02 20:45:12,225][0m Trial 3 finished with value: 0.14032426894605848 and parameters: {'observation_period_num': 77, 'train_rates': 0.84964991791108, 'learning_rate': 0.00035966011643907657, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9775494101233468}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:46:00,711][0m Trial 4 finished with value: 0.1567605282797793 and parameters: {'observation_period_num': 229, 'train_rates': 0.7426374083189913, 'learning_rate': 0.0004789400290918314, 'batch_size': 64, 'step_size': 5, 'gamma': 0.915082938764652}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:47:11,692][0m Trial 5 finished with value: 0.41853670296924456 and parameters: {'observation_period_num': 165, 'train_rates': 0.9701337290016608, 'learning_rate': 0.0008064032574284232, 'batch_size': 59, 'step_size': 3, 'gamma': 0.7514873754388286}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:47:32,151][0m Trial 6 finished with value: 0.45333210954829506 and parameters: {'observation_period_num': 231, 'train_rates': 0.656819155849836, 'learning_rate': 1.5020187013314877e-05, 'batch_size': 168, 'step_size': 12, 'gamma': 0.9179590122006831}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:47:55,169][0m Trial 7 finished with value: 1.555980950400007 and parameters: {'observation_period_num': 16, 'train_rates': 0.6861822198708003, 'learning_rate': 2.442583140586893e-06, 'batch_size': 163, 'step_size': 2, 'gamma': 0.8757793347475405}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:48:20,295][0m Trial 8 finished with value: 0.2339127041024607 and parameters: {'observation_period_num': 38, 'train_rates': 0.7818877720941458, 'learning_rate': 2.1574538827313633e-05, 'batch_size': 155, 'step_size': 13, 'gamma': 0.8850134953570375}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:48:40,888][0m Trial 9 finished with value: 0.24002076198007458 and parameters: {'observation_period_num': 217, 'train_rates': 0.6089279636813443, 'learning_rate': 0.0004590380215878949, 'batch_size': 164, 'step_size': 3, 'gamma': 0.8421245780629598}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:49:22,996][0m Trial 10 finished with value: 0.16010331933697064 and parameters: {'observation_period_num': 75, 'train_rates': 0.8957923336797842, 'learning_rate': 0.00010958777625786397, 'batch_size': 95, 'step_size': 9, 'gamma': 0.9858102841887333}. Best is trial 3 with value: 0.14032426894605848.[0m
[32m[I 2025-02-02 20:52:20,633][0m Trial 11 finished with value: 0.13075199299726073 and parameters: {'observation_period_num': 91, 'train_rates': 0.7499271581481077, 'learning_rate': 0.0009211751961687718, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9794024870236048}. Best is trial 11 with value: 0.13075199299726073.[0m
[32m[I 2025-02-02 20:55:08,020][0m Trial 12 finished with value: 0.14389694868482963 and parameters: {'observation_period_num': 82, 'train_rates': 0.8597052505844216, 'learning_rate': 0.0008954049914499206, 'batch_size': 22, 'step_size': 9, 'gamma': 0.9848548962444335}. Best is trial 11 with value: 0.13075199299726073.[0m
[32m[I 2025-02-02 20:55:42,375][0m Trial 13 finished with value: 0.1299735923193666 and parameters: {'observation_period_num': 91, 'train_rates': 0.7403145884671416, 'learning_rate': 7.078507462203665e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9528264002812669}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 20:56:13,173][0m Trial 14 finished with value: 0.9149191816391483 and parameters: {'observation_period_num': 107, 'train_rates': 0.7275578447873708, 'learning_rate': 2.4818244821707425e-06, 'batch_size': 103, 'step_size': 1, 'gamma': 0.9401972231454464}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 20:56:44,997][0m Trial 15 finished with value: 0.1869078569659916 and parameters: {'observation_period_num': 180, 'train_rates': 0.7586413138967033, 'learning_rate': 6.422499407925377e-05, 'batch_size': 100, 'step_size': 4, 'gamma': 0.950607602248515}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 20:57:22,850][0m Trial 16 finished with value: 0.36977638088858367 and parameters: {'observation_period_num': 104, 'train_rates': 0.701436141176291, 'learning_rate': 6.7525922888365995e-06, 'batch_size': 79, 'step_size': 15, 'gamma': 0.9498277282101957}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 20:57:44,938][0m Trial 17 finished with value: 0.17337224167647353 and parameters: {'observation_period_num': 43, 'train_rates': 0.801694429148891, 'learning_rate': 4.427701152746717e-05, 'batch_size': 205, 'step_size': 8, 'gamma': 0.8340245186689574}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 20:58:17,451][0m Trial 18 finished with value: 0.48018128036960095 and parameters: {'observation_period_num': 112, 'train_rates': 0.9148130771144146, 'learning_rate': 6.3892634790135095e-06, 'batch_size': 118, 'step_size': 10, 'gamma': 0.9085184945031233}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 20:59:20,655][0m Trial 19 finished with value: 0.1481027575200086 and parameters: {'observation_period_num': 46, 'train_rates': 0.6681268045834108, 'learning_rate': 7.939361070007436e-05, 'batch_size': 46, 'step_size': 4, 'gamma': 0.960483001946963}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 21:02:17,804][0m Trial 20 finished with value: 0.23943090673725484 and parameters: {'observation_period_num': 192, 'train_rates': 0.7863332552141453, 'learning_rate': 0.00024214033733044595, 'batch_size': 17, 'step_size': 6, 'gamma': 0.930297194397012}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 21:03:32,453][0m Trial 21 finished with value: 0.1400137484225005 and parameters: {'observation_period_num': 78, 'train_rates': 0.8458513244884686, 'learning_rate': 0.0003352311161965903, 'batch_size': 47, 'step_size': 5, 'gamma': 0.970490436978346}. Best is trial 13 with value: 0.1299735923193666.[0m
[32m[I 2025-02-02 21:04:15,644][0m Trial 22 finished with value: 0.11427859589457512 and parameters: {'observation_period_num': 72, 'train_rates': 0.8152382338215389, 'learning_rate': 0.000265421435920827, 'batch_size': 76, 'step_size': 5, 'gamma': 0.9659646499815773}. Best is trial 22 with value: 0.11427859589457512.[0m
[32m[I 2025-02-02 21:04:55,032][0m Trial 23 finished with value: 0.10107072243586765 and parameters: {'observation_period_num': 58, 'train_rates': 0.7260382066115261, 'learning_rate': 0.0002076963803238986, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9642697397526924}. Best is trial 23 with value: 0.10107072243586765.[0m
[32m[I 2025-02-02 21:05:20,474][0m Trial 24 finished with value: 0.08006967078651903 and parameters: {'observation_period_num': 10, 'train_rates': 0.7129901804976123, 'learning_rate': 0.00020029441795952236, 'batch_size': 127, 'step_size': 7, 'gamma': 0.8968988915988503}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:05:44,207][0m Trial 25 finished with value: 0.08175706940923812 and parameters: {'observation_period_num': 8, 'train_rates': 0.647500704477085, 'learning_rate': 0.00018773888822413152, 'batch_size': 129, 'step_size': 8, 'gamma': 0.8919992230643584}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:06:08,053][0m Trial 26 finished with value: 0.08173014937676525 and parameters: {'observation_period_num': 5, 'train_rates': 0.6366930047830848, 'learning_rate': 0.00017700728051398392, 'batch_size': 127, 'step_size': 10, 'gamma': 0.8509498771615768}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:06:30,910][0m Trial 27 finished with value: 0.1393322989344597 and parameters: {'observation_period_num': 8, 'train_rates': 0.643463129209735, 'learning_rate': 3.716299280365393e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8487277264630477}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:06:49,545][0m Trial 28 finished with value: 0.2750195230836066 and parameters: {'observation_period_num': 25, 'train_rates': 0.6345636114040273, 'learning_rate': 0.0001181161097709876, 'batch_size': 189, 'step_size': 11, 'gamma': 0.8133972128201156}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:07:08,999][0m Trial 29 finished with value: 0.09124545421957439 and parameters: {'observation_period_num': 6, 'train_rates': 0.6953803541836194, 'learning_rate': 0.0001676642587873413, 'batch_size': 249, 'step_size': 8, 'gamma': 0.8929437472167475}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:07:32,080][0m Trial 30 finished with value: 0.11204557062432885 and parameters: {'observation_period_num': 31, 'train_rates': 0.6000915375381493, 'learning_rate': 0.0005568038685804575, 'batch_size': 127, 'step_size': 9, 'gamma': 0.8582779968638922}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:07:51,043][0m Trial 31 finished with value: 0.08632800271380402 and parameters: {'observation_period_num': 5, 'train_rates': 0.6968101296510651, 'learning_rate': 0.00017050441496453683, 'batch_size': 250, 'step_size': 8, 'gamma': 0.8943750032131219}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:08:10,292][0m Trial 32 finished with value: 0.12257747435855072 and parameters: {'observation_period_num': 17, 'train_rates': 0.6726548529011522, 'learning_rate': 0.00010970916853070152, 'batch_size': 227, 'step_size': 7, 'gamma': 0.8961175961878598}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:08:32,533][0m Trial 33 finished with value: 0.11241061540048741 and parameters: {'observation_period_num': 50, 'train_rates': 0.7095601442669978, 'learning_rate': 0.00018521021265285932, 'batch_size': 147, 'step_size': 10, 'gamma': 0.8665348085983139}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:08:59,010][0m Trial 34 finished with value: 0.22807906568050385 and parameters: {'observation_period_num': 27, 'train_rates': 0.6458427087666585, 'learning_rate': 5.091812644066726e-05, 'batch_size': 112, 'step_size': 8, 'gamma': 0.8774099676939032}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:09:18,463][0m Trial 35 finished with value: 0.12470299514525898 and parameters: {'observation_period_num': 57, 'train_rates': 0.6343674992296999, 'learning_rate': 0.00031421837942003554, 'batch_size': 197, 'step_size': 11, 'gamma': 0.8237734960152301}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:09:40,645][0m Trial 36 finished with value: 0.41802864536544343 and parameters: {'observation_period_num': 133, 'train_rates': 0.6788122395366261, 'learning_rate': 2.7837364740310618e-05, 'batch_size': 138, 'step_size': 7, 'gamma': 0.7992050523028189}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:10:00,901][0m Trial 37 finished with value: 1.524664122886486 and parameters: {'observation_period_num': 9, 'train_rates': 0.6230117909524957, 'learning_rate': 1.0053148377235542e-06, 'batch_size': 181, 'step_size': 13, 'gamma': 0.9025444831144277}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:10:27,061][0m Trial 38 finished with value: 0.08032451292215767 and parameters: {'observation_period_num': 26, 'train_rates': 0.7177958212692328, 'learning_rate': 0.0006010518411992496, 'batch_size': 120, 'step_size': 9, 'gamma': 0.9242155771812628}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:10:55,126][0m Trial 39 finished with value: 0.0872879816525018 and parameters: {'observation_period_num': 24, 'train_rates': 0.7712564204257096, 'learning_rate': 0.0005808405450515004, 'batch_size': 121, 'step_size': 11, 'gamma': 0.9264662194504268}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:11:16,664][0m Trial 40 finished with value: 0.080485447989675 and parameters: {'observation_period_num': 36, 'train_rates': 0.6547017081708696, 'learning_rate': 0.0004064160798807443, 'batch_size': 148, 'step_size': 12, 'gamma': 0.9191414950047838}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:11:38,079][0m Trial 41 finished with value: 0.09991052067123436 and parameters: {'observation_period_num': 33, 'train_rates': 0.6606723635515133, 'learning_rate': 0.0003956579785471439, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9253297544870588}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:12:04,848][0m Trial 42 finished with value: 0.11632470139672968 and parameters: {'observation_period_num': 60, 'train_rates': 0.6222123076818286, 'learning_rate': 0.0005643081694598796, 'batch_size': 108, 'step_size': 12, 'gamma': 0.8613772640786728}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:12:28,496][0m Trial 43 finished with value: 0.08207223095072835 and parameters: {'observation_period_num': 19, 'train_rates': 0.7218987608989219, 'learning_rate': 0.0006792911858980073, 'batch_size': 143, 'step_size': 14, 'gamma': 0.9112886307454932}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:12:47,965][0m Trial 44 finished with value: 0.2159357023055091 and parameters: {'observation_period_num': 252, 'train_rates': 0.6582146381051259, 'learning_rate': 0.00038212773390398783, 'batch_size': 177, 'step_size': 9, 'gamma': 0.8829569316514698}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:13:12,871][0m Trial 45 finished with value: 0.09250698352505447 and parameters: {'observation_period_num': 38, 'train_rates': 0.6767551943033597, 'learning_rate': 0.0002672978309729347, 'batch_size': 126, 'step_size': 10, 'gamma': 0.771154785618678}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:13:35,662][0m Trial 46 finished with value: 0.09675133983853837 and parameters: {'observation_period_num': 17, 'train_rates': 0.7096160304801381, 'learning_rate': 0.00012054234295187007, 'batch_size': 160, 'step_size': 12, 'gamma': 0.9365167914166139}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:13:58,742][0m Trial 47 finished with value: 0.20085865421512678 and parameters: {'observation_period_num': 66, 'train_rates': 0.6086942417537754, 'learning_rate': 0.0007576231953197949, 'batch_size': 134, 'step_size': 6, 'gamma': 0.913647366595909}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:14:30,422][0m Trial 48 finished with value: 0.14443395926099278 and parameters: {'observation_period_num': 37, 'train_rates': 0.6508422118673036, 'learning_rate': 8.386733109643297e-05, 'batch_size': 91, 'step_size': 11, 'gamma': 0.8749729553022656}. Best is trial 24 with value: 0.08006967078651903.[0m
[32m[I 2025-02-02 21:14:51,667][0m Trial 49 finished with value: 0.09884265742740517 and parameters: {'observation_period_num': 52, 'train_rates': 0.6843409479520366, 'learning_rate': 0.0004300661030800115, 'batch_size': 173, 'step_size': 9, 'gamma': 0.8865085887672421}. Best is trial 24 with value: 0.08006967078651903.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-02 21:14:51,674][0m A new study created in memory with name: no-name-f840853f-999f-492b-a16a-f71b575b5a74[0m
[32m[I 2025-02-02 21:15:10,732][0m Trial 0 finished with value: 0.1735312539459653 and parameters: {'observation_period_num': 201, 'train_rates': 0.7485846595521011, 'learning_rate': 0.0001882431681022731, 'batch_size': 213, 'step_size': 9, 'gamma': 0.8980515173657794}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:16:57,046][0m Trial 1 finished with value: 0.19917559903629578 and parameters: {'observation_period_num': 152, 'train_rates': 0.7413877566428265, 'learning_rate': 0.00045633203665914777, 'batch_size': 28, 'step_size': 7, 'gamma': 0.9235094320585763}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:17:24,262][0m Trial 2 finished with value: 0.26479348836355654 and parameters: {'observation_period_num': 239, 'train_rates': 0.721953133205448, 'learning_rate': 0.0001068703302708215, 'batch_size': 120, 'step_size': 6, 'gamma': 0.8388113343048091}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:17:47,444][0m Trial 3 finished with value: 0.4632187467068434 and parameters: {'observation_period_num': 203, 'train_rates': 0.802305794454202, 'learning_rate': 1.223361460894263e-05, 'batch_size': 159, 'step_size': 15, 'gamma': 0.8701512781151578}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:20:13,389][0m Trial 4 finished with value: 0.27695265249874357 and parameters: {'observation_period_num': 147, 'train_rates': 0.9528312054228456, 'learning_rate': 0.00028435943970256893, 'batch_size': 24, 'step_size': 3, 'gamma': 0.8429259873603789}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:20:32,075][0m Trial 5 finished with value: 0.3060592964868611 and parameters: {'observation_period_num': 136, 'train_rates': 0.6881637498904403, 'learning_rate': 5.305113349219416e-05, 'batch_size': 196, 'step_size': 2, 'gamma': 0.9665339800972582}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:20:50,874][0m Trial 6 finished with value: 0.20600570244773939 and parameters: {'observation_period_num': 240, 'train_rates': 0.6527912524371426, 'learning_rate': 0.0005111723272843891, 'batch_size': 167, 'step_size': 12, 'gamma': 0.8560056695814018}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:21:13,577][0m Trial 7 finished with value: 0.4337344169616699 and parameters: {'observation_period_num': 228, 'train_rates': 0.9737310996805284, 'learning_rate': 0.00010127755228783219, 'batch_size': 211, 'step_size': 1, 'gamma': 0.9145269179828464}. Best is trial 0 with value: 0.1735312539459653.[0m
[32m[I 2025-02-02 21:21:37,093][0m Trial 8 finished with value: 0.13596558570861816 and parameters: {'observation_period_num': 68, 'train_rates': 0.9782198654072793, 'learning_rate': 8.713214977288975e-05, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9762552151115015}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:22:00,174][0m Trial 9 finished with value: 0.18837389210722438 and parameters: {'observation_period_num': 52, 'train_rates': 0.9003172584483325, 'learning_rate': 0.0002625064227388484, 'batch_size': 210, 'step_size': 4, 'gamma': 0.8099322432796962}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:22:21,109][0m Trial 10 finished with value: 2.5664878070761783 and parameters: {'observation_period_num': 47, 'train_rates': 0.8490191933358137, 'learning_rate': 1.0247254678053012e-06, 'batch_size': 255, 'step_size': 10, 'gamma': 0.7507937678840685}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:22:39,271][0m Trial 11 finished with value: 0.4487640695138411 and parameters: {'observation_period_num': 88, 'train_rates': 0.6158508881471132, 'learning_rate': 1.2967468798594738e-05, 'batch_size': 250, 'step_size': 10, 'gamma': 0.9748709993900839}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:23:15,058][0m Trial 12 finished with value: 0.13767148807178414 and parameters: {'observation_period_num': 5, 'train_rates': 0.7940103753837405, 'learning_rate': 2.0951372969801544e-05, 'batch_size': 95, 'step_size': 5, 'gamma': 0.9300295597641529}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:23:52,778][0m Trial 13 finished with value: 0.14563177183673187 and parameters: {'observation_period_num': 6, 'train_rates': 0.8369662001003967, 'learning_rate': 1.3911836575896855e-05, 'batch_size': 94, 'step_size': 5, 'gamma': 0.9506333951255541}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:24:48,221][0m Trial 14 finished with value: 0.24618060955600368 and parameters: {'observation_period_num': 5, 'train_rates': 0.8862394020470897, 'learning_rate': 3.0515969944221015e-06, 'batch_size': 65, 'step_size': 3, 'gamma': 0.9877581006384967}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:25:24,726][0m Trial 15 finished with value: 0.3958421856799024 and parameters: {'observation_period_num': 94, 'train_rates': 0.9178866339105739, 'learning_rate': 3.391118300380115e-05, 'batch_size': 100, 'step_size': 1, 'gamma': 0.944534260182864}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:26:14,051][0m Trial 16 finished with value: 0.34964963087526907 and parameters: {'observation_period_num': 39, 'train_rates': 0.7892403494401722, 'learning_rate': 4.94036879635825e-06, 'batch_size': 67, 'step_size': 5, 'gamma': 0.8916467995073111}. Best is trial 8 with value: 0.13596558570861816.[0m
[32m[I 2025-02-02 21:26:38,211][0m Trial 17 finished with value: 0.12151292017910353 and parameters: {'observation_period_num': 87, 'train_rates': 0.7980143585404263, 'learning_rate': 0.0009651707543047798, 'batch_size': 146, 'step_size': 7, 'gamma': 0.939355481501456}. Best is trial 17 with value: 0.12151292017910353.[0m
[32m[I 2025-02-02 21:27:05,722][0m Trial 18 finished with value: 0.08797941356897354 and parameters: {'observation_period_num': 99, 'train_rates': 0.9889131062561521, 'learning_rate': 0.0009756537577925638, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9557530177155191}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:27:31,225][0m Trial 19 finished with value: 0.14545885803209832 and parameters: {'observation_period_num': 101, 'train_rates': 0.8566432195453108, 'learning_rate': 0.00099380973175902, 'batch_size': 151, 'step_size': 15, 'gamma': 0.9503525824044672}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:27:55,236][0m Trial 20 finished with value: 0.21092674020097482 and parameters: {'observation_period_num': 122, 'train_rates': 0.9298077862864536, 'learning_rate': 0.0008794795481889949, 'batch_size': 182, 'step_size': 13, 'gamma': 0.8901685355682551}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:28:27,038][0m Trial 21 finished with value: 0.09171649068593979 and parameters: {'observation_period_num': 80, 'train_rates': 0.9889899924673669, 'learning_rate': 0.00011982807353377285, 'batch_size': 128, 'step_size': 12, 'gamma': 0.9856274583808133}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:28:56,907][0m Trial 22 finished with value: 0.08931895345449448 and parameters: {'observation_period_num': 114, 'train_rates': 0.9877222271829212, 'learning_rate': 0.0005855437474097552, 'batch_size': 132, 'step_size': 12, 'gamma': 0.9882425584165704}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:29:27,832][0m Trial 23 finished with value: 0.11195175349712372 and parameters: {'observation_period_num': 117, 'train_rates': 0.9886354999190282, 'learning_rate': 0.0004680858718313175, 'batch_size': 126, 'step_size': 12, 'gamma': 0.9894557184304689}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:29:57,437][0m Trial 24 finished with value: 0.3088846584846234 and parameters: {'observation_period_num': 169, 'train_rates': 0.9479338507828977, 'learning_rate': 0.00020005096640051068, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9623394091195885}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:30:22,382][0m Trial 25 finished with value: 0.28087809681892395 and parameters: {'observation_period_num': 68, 'train_rates': 0.9501364660545887, 'learning_rate': 0.0005537319101780733, 'batch_size': 174, 'step_size': 11, 'gamma': 0.9628120865726524}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:30:52,963][0m Trial 26 finished with value: 0.17332652867387194 and parameters: {'observation_period_num': 166, 'train_rates': 0.8828449124762159, 'learning_rate': 0.0003183832716814578, 'batch_size': 114, 'step_size': 14, 'gamma': 0.9889054172577398}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:31:41,967][0m Trial 27 finished with value: 0.19438323689003786 and parameters: {'observation_period_num': 115, 'train_rates': 0.9220415209341019, 'learning_rate': 0.00014290610935651475, 'batch_size': 71, 'step_size': 11, 'gamma': 0.911160492512308}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:32:10,547][0m Trial 28 finished with value: 0.09460195899009705 and parameters: {'observation_period_num': 74, 'train_rates': 0.9858571955690677, 'learning_rate': 6.189214016930225e-05, 'batch_size': 142, 'step_size': 13, 'gamma': 0.8093751494020036}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:32:35,744][0m Trial 29 finished with value: 0.2540917694568634 and parameters: {'observation_period_num': 26, 'train_rates': 0.9591671088339241, 'learning_rate': 0.0001824080162140295, 'batch_size': 184, 'step_size': 8, 'gamma': 0.957944267165478}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:33:07,832][0m Trial 30 finished with value: 0.22788147446770354 and parameters: {'observation_period_num': 105, 'train_rates': 0.9309240116156963, 'learning_rate': 0.0005543004629991771, 'batch_size': 111, 'step_size': 9, 'gamma': 0.9336181736121866}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:33:37,254][0m Trial 31 finished with value: 0.12351758033037186 and parameters: {'observation_period_num': 69, 'train_rates': 0.9892342132394, 'learning_rate': 5.3946801239817226e-05, 'batch_size': 139, 'step_size': 13, 'gamma': 0.7721998477761698}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:34:04,266][0m Trial 32 finished with value: 0.34901463985443115 and parameters: {'observation_period_num': 81, 'train_rates': 0.963031252004883, 'learning_rate': 6.722284328403562e-05, 'batch_size': 140, 'step_size': 14, 'gamma': 0.8124578088858595}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:34:30,790][0m Trial 33 finished with value: 0.20609964430332184 and parameters: {'observation_period_num': 141, 'train_rates': 0.9890917592174426, 'learning_rate': 3.503952745562279e-05, 'batch_size': 157, 'step_size': 12, 'gamma': 0.8057452495358821}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:35:16,573][0m Trial 34 finished with value: 0.2210615494034507 and parameters: {'observation_period_num': 61, 'train_rates': 0.9393193811547221, 'learning_rate': 0.00035888736956026575, 'batch_size': 82, 'step_size': 14, 'gamma': 0.8763072354834034}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:35:46,532][0m Trial 35 finished with value: 0.3584124743938446 and parameters: {'observation_period_num': 106, 'train_rates': 0.9660616068062705, 'learning_rate': 0.00014437452158827475, 'batch_size': 130, 'step_size': 11, 'gamma': 0.7853332744392606}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:37:00,414][0m Trial 36 finished with value: 0.23197330540641214 and parameters: {'observation_period_num': 160, 'train_rates': 0.9136641324002783, 'learning_rate': 0.0006840682787396819, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8386371780158388}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:37:29,573][0m Trial 37 finished with value: 0.2662245372961635 and parameters: {'observation_period_num': 186, 'train_rates': 0.7644433179183469, 'learning_rate': 2.2473062731679975e-05, 'batch_size': 111, 'step_size': 10, 'gamma': 0.9711956665942584}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:37:53,343][0m Trial 38 finished with value: 0.1660550424473823 and parameters: {'observation_period_num': 130, 'train_rates': 0.8775403138501905, 'learning_rate': 0.00023495639718201078, 'batch_size': 169, 'step_size': 12, 'gamma': 0.9191248484362182}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:38:13,354][0m Trial 39 finished with value: 0.1503778461886495 and parameters: {'observation_period_num': 76, 'train_rates': 0.7058207471123441, 'learning_rate': 7.96231816790134e-05, 'batch_size': 191, 'step_size': 13, 'gamma': 0.8233158539176275}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:38:39,395][0m Trial 40 finished with value: 0.15401928253954994 and parameters: {'observation_period_num': 27, 'train_rates': 0.8998203516460906, 'learning_rate': 0.0001340491379263494, 'batch_size': 161, 'step_size': 9, 'gamma': 0.9766856924328533}. Best is trial 18 with value: 0.08797941356897354.[0m
[32m[I 2025-02-02 21:39:10,857][0m Trial 41 finished with value: 0.08574704825878143 and parameters: {'observation_period_num': 117, 'train_rates': 0.9763231303031986, 'learning_rate': 0.00039940954425028794, 'batch_size': 124, 'step_size': 12, 'gamma': 0.9854649542093142}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:39:42,280][0m Trial 42 finished with value: 0.47174546122550964 and parameters: {'observation_period_num': 132, 'train_rates': 0.974071345800494, 'learning_rate': 0.0006650218531588862, 'batch_size': 121, 'step_size': 14, 'gamma': 0.9767811740565335}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:40:09,523][0m Trial 43 finished with value: 0.2722807194584528 and parameters: {'observation_period_num': 152, 'train_rates': 0.9473264122435713, 'learning_rate': 0.0003573647350531736, 'batch_size': 145, 'step_size': 11, 'gamma': 0.8561918732522455}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:40:45,900][0m Trial 44 finished with value: 0.3752405345439911 and parameters: {'observation_period_num': 98, 'train_rates': 0.9692024922401548, 'learning_rate': 0.00035700683089267307, 'batch_size': 103, 'step_size': 12, 'gamma': 0.9824753294232779}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:41:15,080][0m Trial 45 finished with value: 0.1217070147395134 and parameters: {'observation_period_num': 111, 'train_rates': 0.9765104757714103, 'learning_rate': 5.002181069908972e-05, 'batch_size': 134, 'step_size': 10, 'gamma': 0.9568086489447621}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:41:42,798][0m Trial 46 finished with value: 0.11967382580041885 and parameters: {'observation_period_num': 55, 'train_rates': 0.9899109335224638, 'learning_rate': 0.0007350921997693392, 'batch_size': 154, 'step_size': 13, 'gamma': 0.9682729025536669}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:42:06,489][0m Trial 47 finished with value: 0.2803120017051697 and parameters: {'observation_period_num': 78, 'train_rates': 0.944893436268639, 'learning_rate': 0.0001054680583390544, 'batch_size': 203, 'step_size': 15, 'gamma': 0.980598408366544}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:42:47,502][0m Trial 48 finished with value: 0.18318409100174904 and parameters: {'observation_period_num': 90, 'train_rates': 0.9066985344286679, 'learning_rate': 0.00044142966407236954, 'batch_size': 87, 'step_size': 12, 'gamma': 0.9099185519607489}. Best is trial 41 with value: 0.08574704825878143.[0m
[32m[I 2025-02-02 21:43:11,108][0m Trial 49 finished with value: 0.3210250735282898 and parameters: {'observation_period_num': 125, 'train_rates': 0.9571265293224651, 'learning_rate': 0.00023223410105545266, 'batch_size': 225, 'step_size': 14, 'gamma': 0.9436795447637064}. Best is trial 41 with value: 0.08574704825878143.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-02 21:43:11,115][0m A new study created in memory with name: no-name-0ce9e8a4-430d-42ad-9495-e7ddcafb8645[0m
[32m[I 2025-02-02 21:43:31,053][0m Trial 0 finished with value: 0.6432450519731412 and parameters: {'observation_period_num': 165, 'train_rates': 0.776045300129361, 'learning_rate': 3.940020007069372e-06, 'batch_size': 226, 'step_size': 15, 'gamma': 0.9174031031427546}. Best is trial 0 with value: 0.6432450519731412.[0m
[32m[I 2025-02-02 21:44:05,494][0m Trial 1 finished with value: 0.1705141043663025 and parameters: {'observation_period_num': 195, 'train_rates': 0.6463739494399589, 'learning_rate': 0.0005341208465112535, 'batch_size': 78, 'step_size': 5, 'gamma': 0.8466766170418917}. Best is trial 1 with value: 0.1705141043663025.[0m
[32m[I 2025-02-02 21:44:23,899][0m Trial 2 finished with value: 0.4212881724039714 and parameters: {'observation_period_num': 189, 'train_rates': 0.6047300650681076, 'learning_rate': 4.222529562596736e-05, 'batch_size': 208, 'step_size': 7, 'gamma': 0.9317995759226286}. Best is trial 1 with value: 0.1705141043663025.[0m
[32m[I 2025-02-02 21:44:47,449][0m Trial 3 finished with value: 0.2287461104449527 and parameters: {'observation_period_num': 149, 'train_rates': 0.8758971064429735, 'learning_rate': 3.9630012912142145e-05, 'batch_size': 165, 'step_size': 15, 'gamma': 0.8166431563127158}. Best is trial 1 with value: 0.1705141043663025.[0m
[32m[I 2025-02-02 21:45:13,334][0m Trial 4 finished with value: 0.07505769282579422 and parameters: {'observation_period_num': 50, 'train_rates': 0.9811005963225444, 'learning_rate': 9.721763180609181e-05, 'batch_size': 172, 'step_size': 10, 'gamma': 0.8322943426984026}. Best is trial 4 with value: 0.07505769282579422.[0m
[32m[I 2025-02-02 21:45:34,646][0m Trial 5 finished with value: 0.15703170440789593 and parameters: {'observation_period_num': 125, 'train_rates': 0.8778984901151492, 'learning_rate': 0.0005997138125433584, 'batch_size': 208, 'step_size': 9, 'gamma': 0.8039550184158394}. Best is trial 4 with value: 0.07505769282579422.[0m
[32m[I 2025-02-02 21:45:53,418][0m Trial 6 finished with value: 0.44185804443310966 and parameters: {'observation_period_num': 197, 'train_rates': 0.7141252810751808, 'learning_rate': 1.7392880462338544e-05, 'batch_size': 220, 'step_size': 9, 'gamma': 0.9662787720642413}. Best is trial 4 with value: 0.07505769282579422.[0m
[32m[I 2025-02-02 21:46:14,806][0m Trial 7 finished with value: 0.10900218047859303 and parameters: {'observation_period_num': 7, 'train_rates': 0.719152397869832, 'learning_rate': 9.12538442220547e-05, 'batch_size': 192, 'step_size': 8, 'gamma': 0.7954162940433669}. Best is trial 4 with value: 0.07505769282579422.[0m
[32m[I 2025-02-02 21:46:48,359][0m Trial 8 finished with value: 1.2567576169967651 and parameters: {'observation_period_num': 180, 'train_rates': 0.9644246953529673, 'learning_rate': 3.111929456724979e-06, 'batch_size': 111, 'step_size': 2, 'gamma': 0.8377486781613817}. Best is trial 4 with value: 0.07505769282579422.[0m
[32m[I 2025-02-02 21:47:10,892][0m Trial 9 finished with value: 0.4556768589816794 and parameters: {'observation_period_num': 104, 'train_rates': 0.7563073611149693, 'learning_rate': 1.0604933382123694e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.7502755956873138}. Best is trial 4 with value: 0.07505769282579422.[0m
[32m[I 2025-02-02 21:47:34,679][0m Trial 10 finished with value: 0.06938114762306213 and parameters: {'observation_period_num': 46, 'train_rates': 0.9894009814105403, 'learning_rate': 0.00015937239145272288, 'batch_size': 255, 'step_size': 12, 'gamma': 0.8841179760338713}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:47:58,755][0m Trial 11 finished with value: 0.1163986325263977 and parameters: {'observation_period_num': 44, 'train_rates': 0.9752810958970182, 'learning_rate': 0.000152466758476555, 'batch_size': 255, 'step_size': 12, 'gamma': 0.8891127429874753}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:49:30,962][0m Trial 12 finished with value: 0.20314536882481216 and parameters: {'observation_period_num': 69, 'train_rates': 0.8993033862521043, 'learning_rate': 0.00020714212429112142, 'batch_size': 37, 'step_size': 11, 'gamma': 0.878868886152563}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:50:04,096][0m Trial 13 finished with value: 0.15741285613515008 and parameters: {'observation_period_num': 5, 'train_rates': 0.9277805667630346, 'learning_rate': 0.00026412102929580317, 'batch_size': 117, 'step_size': 12, 'gamma': 0.8604321119965446}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:50:23,623][0m Trial 14 finished with value: 0.22611664980649948 and parameters: {'observation_period_num': 249, 'train_rates': 0.8172302517795538, 'learning_rate': 9.855702267691677e-05, 'batch_size': 247, 'step_size': 13, 'gamma': 0.9204127068888098}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:50:46,457][0m Trial 15 finished with value: 0.17830384098684665 and parameters: {'observation_period_num': 67, 'train_rates': 0.8318812853686584, 'learning_rate': 5.6423347195304386e-05, 'batch_size': 173, 'step_size': 10, 'gamma': 0.7693986010261161}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:51:15,638][0m Trial 16 finished with value: 0.6642423272132874 and parameters: {'observation_period_num': 43, 'train_rates': 0.9871774044202913, 'learning_rate': 1.2727716395144792e-06, 'batch_size': 140, 'step_size': 5, 'gamma': 0.8951527617559024}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:52:06,562][0m Trial 17 finished with value: 0.21641179056966614 and parameters: {'observation_period_num': 90, 'train_rates': 0.9190921456847608, 'learning_rate': 0.0008600524891402093, 'batch_size': 71, 'step_size': 13, 'gamma': 0.9891345678161401}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:52:30,884][0m Trial 18 finished with value: 0.3474290072917938 and parameters: {'observation_period_num': 33, 'train_rates': 0.9448255237196915, 'learning_rate': 1.6184597379926114e-05, 'batch_size': 190, 'step_size': 14, 'gamma': 0.8291179100243121}. Best is trial 10 with value: 0.06938114762306213.[0m
Early stopping at epoch 92
[32m[I 2025-02-02 21:52:56,654][0m Trial 19 finished with value: 0.21696311783107766 and parameters: {'observation_period_num': 80, 'train_rates': 0.8541574120213798, 'learning_rate': 0.00034819632570532447, 'batch_size': 136, 'step_size': 1, 'gamma': 0.8619624665706925}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:53:19,611][0m Trial 20 finished with value: 0.3467359244823456 and parameters: {'observation_period_num': 108, 'train_rates': 0.950505540960751, 'learning_rate': 0.00011142835181340498, 'batch_size': 236, 'step_size': 11, 'gamma': 0.7823197366664957}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:53:40,572][0m Trial 21 finished with value: 0.1106418359670941 and parameters: {'observation_period_num': 6, 'train_rates': 0.7106723686836438, 'learning_rate': 9.359102882286587e-05, 'batch_size': 185, 'step_size': 7, 'gamma': 0.7967417843314077}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:54:01,111][0m Trial 22 finished with value: 0.16172529585153492 and parameters: {'observation_period_num': 26, 'train_rates': 0.708309506345895, 'learning_rate': 4.979563782668904e-05, 'batch_size': 199, 'step_size': 9, 'gamma': 0.8156352003353778}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:54:24,174][0m Trial 23 finished with value: 0.1948679789680488 and parameters: {'observation_period_num': 52, 'train_rates': 0.673550455751633, 'learning_rate': 7.308787455275476e-05, 'batch_size': 149, 'step_size': 5, 'gamma': 0.7807379323957371}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:54:46,020][0m Trial 24 finished with value: 0.17182683743416302 and parameters: {'observation_period_num': 22, 'train_rates': 0.746312123758313, 'learning_rate': 2.4798109002665833e-05, 'batch_size': 179, 'step_size': 10, 'gamma': 0.8483918907633318}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:55:14,933][0m Trial 25 finished with value: 0.1174462730058631 and parameters: {'observation_period_num': 59, 'train_rates': 0.797693693020457, 'learning_rate': 0.0001879938594688937, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8208931913603166}. Best is trial 10 with value: 0.06938114762306213.[0m
[32m[I 2025-02-02 21:55:39,189][0m Trial 26 finished with value: 0.05016476288437843 and parameters: {'observation_period_num': 22, 'train_rates': 0.9897327698876274, 'learning_rate': 0.00040105602383563193, 'batch_size': 234, 'step_size': 11, 'gamma': 0.8995158049472992}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:56:02,957][0m Trial 27 finished with value: 0.08000065386295319 and parameters: {'observation_period_num': 84, 'train_rates': 0.9830016942916917, 'learning_rate': 0.00031148689895096784, 'batch_size': 231, 'step_size': 12, 'gamma': 0.9088879834703271}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:56:25,114][0m Trial 28 finished with value: 0.16047963500022888 and parameters: {'observation_period_num': 29, 'train_rates': 0.9140661322795386, 'learning_rate': 0.0004269377229914791, 'batch_size': 255, 'step_size': 13, 'gamma': 0.9475552779933389}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:56:47,637][0m Trial 29 finished with value: 0.2929559648036957 and parameters: {'observation_period_num': 131, 'train_rates': 0.9439255650691389, 'learning_rate': 0.00014549927776186028, 'batch_size': 223, 'step_size': 15, 'gamma': 0.9003693839980668}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:57:09,944][0m Trial 30 finished with value: 0.14455536358206478 and parameters: {'observation_period_num': 51, 'train_rates': 0.8959566233996935, 'learning_rate': 0.0008774529011655198, 'batch_size': 241, 'step_size': 11, 'gamma': 0.8770083405167431}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:57:34,273][0m Trial 31 finished with value: 0.07987744361162186 and parameters: {'observation_period_num': 81, 'train_rates': 0.9893307074065346, 'learning_rate': 0.0003217678472987501, 'batch_size': 230, 'step_size': 12, 'gamma': 0.9112746305874424}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:57:57,589][0m Trial 32 finished with value: 0.32940250635147095 and parameters: {'observation_period_num': 72, 'train_rates': 0.9627422539850471, 'learning_rate': 0.000545772081849429, 'batch_size': 217, 'step_size': 10, 'gamma': 0.931402452604527}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:58:19,906][0m Trial 33 finished with value: 0.26214370131492615 and parameters: {'observation_period_num': 99, 'train_rates': 0.9372819755447837, 'learning_rate': 0.00027570691294842776, 'batch_size': 236, 'step_size': 14, 'gamma': 0.9161515744869695}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:58:44,453][0m Trial 34 finished with value: 0.0599852055311203 and parameters: {'observation_period_num': 20, 'train_rates': 0.9894476757692223, 'learning_rate': 0.00020932781188237232, 'batch_size': 208, 'step_size': 11, 'gamma': 0.9443586696508778}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:59:09,075][0m Trial 35 finished with value: 0.27594852447509766 and parameters: {'observation_period_num': 19, 'train_rates': 0.9592310519951222, 'learning_rate': 0.00015311308036266143, 'batch_size': 210, 'step_size': 11, 'gamma': 0.9488400406358333}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 21:59:32,307][0m Trial 36 finished with value: 0.17100832026875662 and parameters: {'observation_period_num': 38, 'train_rates': 0.8855846140539992, 'learning_rate': 3.271885517191083e-05, 'batch_size': 202, 'step_size': 10, 'gamma': 0.9700020569295779}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:00:00,218][0m Trial 37 finished with value: 0.0585954487323761 and parameters: {'observation_period_num': 20, 'train_rates': 0.9896451563833504, 'learning_rate': 6.797251192403466e-05, 'batch_size': 163, 'step_size': 14, 'gamma': 0.9348660114281946}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:00:26,192][0m Trial 38 finished with value: 0.1153554093448361 and parameters: {'observation_period_num': 17, 'train_rates': 0.8644039662469533, 'learning_rate': 0.0006076060971682483, 'batch_size': 156, 'step_size': 14, 'gamma': 0.9324545599802616}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:00:50,233][0m Trial 39 finished with value: 0.16352323848096764 and parameters: {'observation_period_num': 18, 'train_rates': 0.9074922222447612, 'learning_rate': 6.166078299033856e-05, 'batch_size': 213, 'step_size': 15, 'gamma': 0.9516628914620038}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:01:38,573][0m Trial 40 finished with value: 0.39134212171143673 and parameters: {'observation_period_num': 210, 'train_rates': 0.9659725104272033, 'learning_rate': 0.0001852877851094456, 'batch_size': 79, 'step_size': 13, 'gamma': 0.9286928456149759}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:02:05,656][0m Trial 41 finished with value: 0.059330493211746216 and parameters: {'observation_period_num': 58, 'train_rates': 0.9883107480245096, 'learning_rate': 0.00011717370787728924, 'batch_size': 170, 'step_size': 9, 'gamma': 0.8876666587430636}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:02:32,357][0m Trial 42 finished with value: 0.3658037781715393 and parameters: {'observation_period_num': 61, 'train_rates': 0.9690503336257633, 'learning_rate': 0.00013152979003102464, 'batch_size': 168, 'step_size': 8, 'gamma': 0.8791045025260036}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:02:59,087][0m Trial 43 finished with value: 0.06203314661979675 and parameters: {'observation_period_num': 36, 'train_rates': 0.9884943362076534, 'learning_rate': 7.331184563169802e-05, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8877407206941874}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:03:23,414][0m Trial 44 finished with value: 0.25825593573972583 and parameters: {'observation_period_num': 37, 'train_rates': 0.928877472365738, 'learning_rate': 4.2564806661070346e-05, 'batch_size': 195, 'step_size': 6, 'gamma': 0.8998810752102576}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:03:44,158][0m Trial 45 finished with value: 0.0934468558507158 and parameters: {'observation_period_num': 14, 'train_rates': 0.6287498969832359, 'learning_rate': 7.370826091061674e-05, 'batch_size': 154, 'step_size': 9, 'gamma': 0.9647867817075647}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:04:09,909][0m Trial 46 finished with value: 0.35461553931236267 and parameters: {'observation_period_num': 31, 'train_rates': 0.9522455385374748, 'learning_rate': 2.6818186334328443e-05, 'batch_size': 182, 'step_size': 8, 'gamma': 0.863805443868383}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:04:36,162][0m Trial 47 finished with value: 0.8342053890228271 and parameters: {'observation_period_num': 143, 'train_rates': 0.9729157113097303, 'learning_rate': 8.529571878594607e-06, 'batch_size': 166, 'step_size': 9, 'gamma': 0.9406210832523265}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:05:00,891][0m Trial 48 finished with value: 0.21192793548107147 and parameters: {'observation_period_num': 55, 'train_rates': 0.9296264009213309, 'learning_rate': 0.00022661178492790215, 'batch_size': 204, 'step_size': 7, 'gamma': 0.888931142192595}. Best is trial 26 with value: 0.05016476288437843.[0m
[32m[I 2025-02-02 22:05:33,474][0m Trial 49 finished with value: 0.04219713434576988 and parameters: {'observation_period_num': 5, 'train_rates': 0.9898185095416109, 'learning_rate': 0.00042666169647768927, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9056269493834028}. Best is trial 49 with value: 0.04219713434576988.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-02 22:05:33,482][0m A new study created in memory with name: no-name-c709b547-127d-4864-95bd-ea10f9dc24c7[0m
[32m[I 2025-02-02 22:06:15,976][0m Trial 0 finished with value: 0.6140324544516519 and parameters: {'observation_period_num': 192, 'train_rates': 0.6235248332294006, 'learning_rate': 4.01947834655751e-06, 'batch_size': 67, 'step_size': 12, 'gamma': 0.7735743975279598}. Best is trial 0 with value: 0.6140324544516519.[0m
[32m[I 2025-02-02 22:06:36,967][0m Trial 1 finished with value: 0.35939795184583173 and parameters: {'observation_period_num': 171, 'train_rates': 0.8469510611235974, 'learning_rate': 1.570244743153028e-05, 'batch_size': 220, 'step_size': 10, 'gamma': 0.9162638203628803}. Best is trial 1 with value: 0.35939795184583173.[0m
[32m[I 2025-02-02 22:06:56,303][0m Trial 2 finished with value: 0.9335153217267509 and parameters: {'observation_period_num': 105, 'train_rates': 0.7219981645537908, 'learning_rate': 2.039118326587776e-06, 'batch_size': 241, 'step_size': 7, 'gamma': 0.9240088251348686}. Best is trial 1 with value: 0.35939795184583173.[0m
[32m[I 2025-02-02 22:08:22,176][0m Trial 3 finished with value: 0.15224997193833156 and parameters: {'observation_period_num': 200, 'train_rates': 0.8035583181735046, 'learning_rate': 7.711293918747431e-05, 'batch_size': 37, 'step_size': 5, 'gamma': 0.971243178463131}. Best is trial 3 with value: 0.15224997193833156.[0m
[32m[I 2025-02-02 22:08:46,654][0m Trial 4 finished with value: 0.5203377101674426 and parameters: {'observation_period_num': 164, 'train_rates': 0.7433252481435718, 'learning_rate': 6.808570586176201e-06, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8413558492534889}. Best is trial 3 with value: 0.15224997193833156.[0m
[32m[I 2025-02-02 22:09:18,872][0m Trial 5 finished with value: 0.5687021179070612 and parameters: {'observation_period_num': 37, 'train_rates': 0.6693845230044649, 'learning_rate': 2.4795743111398397e-06, 'batch_size': 101, 'step_size': 8, 'gamma': 0.898619693932036}. Best is trial 3 with value: 0.15224997193833156.[0m
[32m[I 2025-02-02 22:09:49,794][0m Trial 6 finished with value: 0.12752958742245066 and parameters: {'observation_period_num': 57, 'train_rates': 0.8080124912577704, 'learning_rate': 5.991439578105346e-05, 'batch_size': 119, 'step_size': 15, 'gamma': 0.9172283201270159}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:10:35,746][0m Trial 7 finished with value: 0.1282543757231488 and parameters: {'observation_period_num': 120, 'train_rates': 0.8035811310846355, 'learning_rate': 0.0005372428572032908, 'batch_size': 80, 'step_size': 2, 'gamma': 0.9779484044156815}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:11:04,122][0m Trial 8 finished with value: 0.7142583672036515 and parameters: {'observation_period_num': 154, 'train_rates': 0.7479669148972811, 'learning_rate': 9.782364035163986e-06, 'batch_size': 129, 'step_size': 2, 'gamma': 0.9171058322110826}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:11:38,149][0m Trial 9 finished with value: 0.9476718205481665 and parameters: {'observation_period_num': 218, 'train_rates': 0.6829642724220131, 'learning_rate': 3.938145142642284e-06, 'batch_size': 90, 'step_size': 1, 'gamma': 0.9561922353307557}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:12:06,427][0m Trial 10 finished with value: 0.21128317806321908 and parameters: {'observation_period_num': 8, 'train_rates': 0.9423772299157576, 'learning_rate': 0.00010223732604022985, 'batch_size': 167, 'step_size': 15, 'gamma': 0.8440109508603174}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:14:21,235][0m Trial 11 finished with value: 0.17270095410195827 and parameters: {'observation_period_num': 92, 'train_rates': 0.8787871299651134, 'learning_rate': 0.0008040082410801331, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9736244529175584}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:14:46,311][0m Trial 12 finished with value: 0.13612804523093164 and parameters: {'observation_period_num': 67, 'train_rates': 0.8203692273084463, 'learning_rate': 0.0007476275200217161, 'batch_size': 157, 'step_size': 15, 'gamma': 0.9865704508880279}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:15:10,426][0m Trial 13 finished with value: 0.24048279131115494 and parameters: {'observation_period_num': 252, 'train_rates': 0.9128413318279025, 'learning_rate': 0.00023192292475675855, 'batch_size': 185, 'step_size': 5, 'gamma': 0.876285903015869}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:15:54,691][0m Trial 14 finished with value: 0.16097345563292315 and parameters: {'observation_period_num': 132, 'train_rates': 0.7763116853806751, 'learning_rate': 2.87662147339885e-05, 'batch_size': 77, 'step_size': 11, 'gamma': 0.9446248061356863}. Best is trial 6 with value: 0.12752958742245066.[0m
[32m[I 2025-02-02 22:16:31,324][0m Trial 15 finished with value: 0.0730273649096489 and parameters: {'observation_period_num': 76, 'train_rates': 0.9801430206000239, 'learning_rate': 0.0002572975257743654, 'batch_size': 113, 'step_size': 9, 'gamma': 0.792918657624265}. Best is trial 15 with value: 0.0730273649096489.[0m
[32m[I 2025-02-02 22:17:09,287][0m Trial 16 finished with value: 0.09063190966844559 and parameters: {'observation_period_num': 62, 'train_rates': 0.9778028513632083, 'learning_rate': 6.966953783990263e-05, 'batch_size': 113, 'step_size': 13, 'gamma': 0.77278779030414}. Best is trial 15 with value: 0.0730273649096489.[0m
[32m[I 2025-02-02 22:18:30,443][0m Trial 17 finished with value: 0.4161351257562637 and parameters: {'observation_period_num': 83, 'train_rates': 0.9740212294980951, 'learning_rate': 0.0002419123414267876, 'batch_size': 53, 'step_size': 10, 'gamma': 0.760347065873417}. Best is trial 15 with value: 0.0730273649096489.[0m
[32m[I 2025-02-02 22:18:59,146][0m Trial 18 finished with value: 0.04926620051264763 and parameters: {'observation_period_num': 26, 'train_rates': 0.9870510727437779, 'learning_rate': 0.00017446831351692845, 'batch_size': 190, 'step_size': 13, 'gamma': 0.793926945329656}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:19:24,128][0m Trial 19 finished with value: 0.1605089378695116 and parameters: {'observation_period_num': 15, 'train_rates': 0.9042513492074432, 'learning_rate': 0.00022412390100106798, 'batch_size': 199, 'step_size': 8, 'gamma': 0.8052032860117838}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:19:46,873][0m Trial 20 finished with value: 0.23215878009796143 and parameters: {'observation_period_num': 34, 'train_rates': 0.9460417698175533, 'learning_rate': 0.0003795524740535814, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8112785891743223}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:20:25,230][0m Trial 21 finished with value: 0.2883710563182831 and parameters: {'observation_period_num': 47, 'train_rates': 0.9612481815990294, 'learning_rate': 0.00011390118075584162, 'batch_size': 114, 'step_size': 13, 'gamma': 0.7896832937249353}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:20:55,022][0m Trial 22 finished with value: 0.17620833218097687 and parameters: {'observation_period_num': 77, 'train_rates': 0.9774889006585745, 'learning_rate': 3.9633227070181165e-05, 'batch_size': 150, 'step_size': 9, 'gamma': 0.8284520777913912}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:21:20,845][0m Trial 23 finished with value: 0.05567999556660652 and parameters: {'observation_period_num': 27, 'train_rates': 0.9885804430735661, 'learning_rate': 0.00014833268396963949, 'batch_size': 197, 'step_size': 13, 'gamma': 0.7554015678438066}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:21:45,237][0m Trial 24 finished with value: 0.17403711513185702 and parameters: {'observation_period_num': 28, 'train_rates': 0.9187673495853986, 'learning_rate': 0.00014006941162134944, 'batch_size': 200, 'step_size': 14, 'gamma': 0.760491076545189}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:22:11,239][0m Trial 25 finished with value: 0.13071379943376582 and parameters: {'observation_period_num': 19, 'train_rates': 0.887431937324904, 'learning_rate': 0.00033082633757023847, 'batch_size': 172, 'step_size': 11, 'gamma': 0.796098418911691}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:22:35,120][0m Trial 26 finished with value: 0.09042095392942429 and parameters: {'observation_period_num': 48, 'train_rates': 0.9893906487509474, 'learning_rate': 0.00016463281120576175, 'batch_size': 223, 'step_size': 7, 'gamma': 0.7531630397642509}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:22:58,958][0m Trial 27 finished with value: 0.3550480902194977 and parameters: {'observation_period_num': 101, 'train_rates': 0.935368466184963, 'learning_rate': 3.3719036081887295e-05, 'batch_size': 193, 'step_size': 11, 'gamma': 0.8251225836883331}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:23:21,376][0m Trial 28 finished with value: 2.3193680937861054 and parameters: {'observation_period_num': 6, 'train_rates': 0.8555976407176165, 'learning_rate': 1.0064533010012866e-06, 'batch_size': 216, 'step_size': 14, 'gamma': 0.7768593442147547}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:23:47,063][0m Trial 29 finished with value: 0.26241862773895264 and parameters: {'observation_period_num': 71, 'train_rates': 0.9486159312947844, 'learning_rate': 0.0004913971090872213, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8665566792307471}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:24:09,891][0m Trial 30 finished with value: 0.2786868841133334 and parameters: {'observation_period_num': 42, 'train_rates': 0.6221070638816273, 'learning_rate': 2.2971702467708986e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.7826138095810011}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:24:35,348][0m Trial 31 finished with value: 0.07764993607997894 and parameters: {'observation_period_num': 53, 'train_rates': 0.9861191375283523, 'learning_rate': 0.000182175938842711, 'batch_size': 228, 'step_size': 7, 'gamma': 0.7524862597803994}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:25:00,910][0m Trial 32 finished with value: 0.055758584290742874 and parameters: {'observation_period_num': 27, 'train_rates': 0.9853781040865163, 'learning_rate': 0.00017213748574480565, 'batch_size': 227, 'step_size': 7, 'gamma': 0.7716264897009281}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:25:25,260][0m Trial 33 finished with value: 0.1990596503019333 and parameters: {'observation_period_num': 24, 'train_rates': 0.9271036632650091, 'learning_rate': 0.0003432581227502296, 'batch_size': 244, 'step_size': 6, 'gamma': 0.7724771185604167}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:25:50,929][0m Trial 34 finished with value: 0.41691020131111145 and parameters: {'observation_period_num': 27, 'train_rates': 0.9609951643242759, 'learning_rate': 4.975103378491475e-05, 'batch_size': 205, 'step_size': 4, 'gamma': 0.8066690117063889}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:26:13,660][0m Trial 35 finished with value: 0.22881102651357652 and parameters: {'observation_period_num': 110, 'train_rates': 0.8945992141411676, 'learning_rate': 9.156588996140894e-05, 'batch_size': 234, 'step_size': 9, 'gamma': 0.793132041382441}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:26:36,572][0m Trial 36 finished with value: 0.16836726086887907 and parameters: {'observation_period_num': 88, 'train_rates': 0.8687886331668265, 'learning_rate': 0.00012091894856182237, 'batch_size': 212, 'step_size': 9, 'gamma': 0.8237125909527199}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:27:00,621][0m Trial 37 finished with value: 0.11699709296226501 and parameters: {'observation_period_num': 37, 'train_rates': 0.8357329040864585, 'learning_rate': 0.0005613810871871003, 'batch_size': 187, 'step_size': 6, 'gamma': 0.767662810789033}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:27:23,662][0m Trial 38 finished with value: 0.4802602529525757 and parameters: {'observation_period_num': 57, 'train_rates': 0.9584587918436116, 'learning_rate': 1.7720152606685403e-05, 'batch_size': 255, 'step_size': 12, 'gamma': 0.8530716205522634}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:27:54,715][0m Trial 39 finished with value: 0.21603840582359296 and parameters: {'observation_period_num': 140, 'train_rates': 0.926016936177585, 'learning_rate': 0.000313353074981587, 'batch_size': 127, 'step_size': 8, 'gamma': 0.7883262508917671}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:28:26,581][0m Trial 40 finished with value: 0.0819392793109588 and parameters: {'observation_period_num': 21, 'train_rates': 0.6004992971409985, 'learning_rate': 0.0009343207863688781, 'batch_size': 96, 'step_size': 14, 'gamma': 0.8896876112621567}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:28:51,020][0m Trial 41 finished with value: 0.07288989424705505 and parameters: {'observation_period_num': 50, 'train_rates': 0.9867617699998026, 'learning_rate': 0.00016924843937518718, 'batch_size': 230, 'step_size': 7, 'gamma': 0.7564555340053214}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:29:15,842][0m Trial 42 finished with value: 0.07079491019248962 and parameters: {'observation_period_num': 6, 'train_rates': 0.9899266286011129, 'learning_rate': 0.00019121140042366384, 'batch_size': 238, 'step_size': 7, 'gamma': 0.7645208132062158}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:29:39,544][0m Trial 43 finished with value: 0.34501394629478455 and parameters: {'observation_period_num': 10, 'train_rates': 0.963630796587463, 'learning_rate': 6.652205815520957e-05, 'batch_size': 243, 'step_size': 6, 'gamma': 0.7625357743878671}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:30:03,829][0m Trial 44 finished with value: 0.2720395028591156 and parameters: {'observation_period_num': 38, 'train_rates': 0.9450476371924882, 'learning_rate': 0.0001636465493518815, 'batch_size': 229, 'step_size': 7, 'gamma': 0.7508757438843903}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:30:29,083][0m Trial 45 finished with value: 0.32525286078453064 and parameters: {'observation_period_num': 6, 'train_rates': 0.9653544268307652, 'learning_rate': 8.992631510992469e-05, 'batch_size': 212, 'step_size': 5, 'gamma': 0.778420127730874}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:30:48,806][0m Trial 46 finished with value: 0.34825283092841697 and parameters: {'observation_period_num': 184, 'train_rates': 0.6924492861300838, 'learning_rate': 4.934961625804092e-05, 'batch_size': 238, 'step_size': 8, 'gamma': 0.7667929577896626}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:31:16,766][0m Trial 47 finished with value: 0.05892599746584892 and parameters: {'observation_period_num': 18, 'train_rates': 0.9897497491913722, 'learning_rate': 0.0004952349736316305, 'batch_size': 180, 'step_size': 4, 'gamma': 0.7809950537721787}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:31:42,471][0m Trial 48 finished with value: 0.25386932492256165 and parameters: {'observation_period_num': 19, 'train_rates': 0.9478957380393739, 'learning_rate': 0.0006113395489257464, 'batch_size': 182, 'step_size': 3, 'gamma': 0.8153006449187317}. Best is trial 18 with value: 0.04926620051264763.[0m
[32m[I 2025-02-02 22:32:04,737][0m Trial 49 finished with value: 0.10565903114861455 and parameters: {'observation_period_num': 31, 'train_rates': 0.7816898874333642, 'learning_rate': 0.00047701537038712013, 'batch_size': 179, 'step_size': 4, 'gamma': 0.8388621346245762}. Best is trial 18 with value: 0.04926620051264763.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-02 22:32:04,744][0m A new study created in memory with name: no-name-8c62454b-1af4-4315-a234-a33132e9c4c9[0m
[32m[I 2025-02-02 22:32:28,625][0m Trial 0 finished with value: 0.42783813108097424 and parameters: {'observation_period_num': 61, 'train_rates': 0.9049884742652065, 'learning_rate': 4.199947027887621e-05, 'batch_size': 203, 'step_size': 3, 'gamma': 0.805965282892066}. Best is trial 0 with value: 0.42783813108097424.[0m
[32m[I 2025-02-02 22:32:55,307][0m Trial 1 finished with value: 0.18836399219159422 and parameters: {'observation_period_num': 64, 'train_rates': 0.8596260501628578, 'learning_rate': 0.00018975898789151923, 'batch_size': 161, 'step_size': 1, 'gamma': 0.9231083775237297}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:33:18,663][0m Trial 2 finished with value: 0.6619662661158461 and parameters: {'observation_period_num': 101, 'train_rates': 0.8769079687602438, 'learning_rate': 1.3884672903357809e-05, 'batch_size': 194, 'step_size': 5, 'gamma': 0.8104204747974851}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:33:42,055][0m Trial 3 finished with value: 0.31476467035033484 and parameters: {'observation_period_num': 55, 'train_rates': 0.9091625349065913, 'learning_rate': 2.0707148765277068e-05, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8848813269268119}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:34:36,526][0m Trial 4 finished with value: 0.4975447894249418 and parameters: {'observation_period_num': 146, 'train_rates': 0.8659531000219591, 'learning_rate': 4.684760216755246e-06, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8947327225566912}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:34:59,471][0m Trial 5 finished with value: 0.8710456676208056 and parameters: {'observation_period_num': 251, 'train_rates': 0.9038481300468897, 'learning_rate': 1.5530122101575926e-06, 'batch_size': 189, 'step_size': 10, 'gamma': 0.971196612466066}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:35:57,084][0m Trial 6 finished with value: 0.5121355407278646 and parameters: {'observation_period_num': 219, 'train_rates': 0.8712532889256402, 'learning_rate': 2.7417317404630004e-06, 'batch_size': 63, 'step_size': 15, 'gamma': 0.9000857477456344}. Best is trial 1 with value: 0.18836399219159422.[0m
Early stopping at epoch 94
[32m[I 2025-02-02 22:37:36,568][0m Trial 7 finished with value: 1.022096697515563 and parameters: {'observation_period_num': 77, 'train_rates': 0.7356361073281881, 'learning_rate': 5.3723678978191515e-06, 'batch_size': 33, 'step_size': 2, 'gamma': 0.7687233244400397}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:38:31,992][0m Trial 8 finished with value: 0.5310180916526887 and parameters: {'observation_period_num': 169, 'train_rates': 0.7620895271968781, 'learning_rate': 5.656276397857998e-06, 'batch_size': 65, 'step_size': 5, 'gamma': 0.8764025659444762}. Best is trial 1 with value: 0.18836399219159422.[0m
[32m[I 2025-02-02 22:39:01,041][0m Trial 9 finished with value: 0.16766960476919757 and parameters: {'observation_period_num': 218, 'train_rates': 0.8705291295828891, 'learning_rate': 0.0006884924657559424, 'batch_size': 144, 'step_size': 1, 'gamma': 0.9621058507721784}. Best is trial 9 with value: 0.16766960476919757.[0m
[32m[I 2025-02-02 22:39:30,467][0m Trial 10 finished with value: 0.06819483889713608 and parameters: {'observation_period_num': 11, 'train_rates': 0.6315570827075774, 'learning_rate': 0.0007965183410367448, 'batch_size': 121, 'step_size': 11, 'gamma': 0.9867931300252268}. Best is trial 10 with value: 0.06819483889713608.[0m
[32m[I 2025-02-02 22:40:00,366][0m Trial 11 finished with value: 0.27067363942112016 and parameters: {'observation_period_num': 189, 'train_rates': 0.6148545626270034, 'learning_rate': 0.0008967219263283382, 'batch_size': 109, 'step_size': 11, 'gamma': 0.986194612511236}. Best is trial 10 with value: 0.06819483889713608.[0m
[32m[I 2025-02-02 22:40:26,175][0m Trial 12 finished with value: 0.0676589639679621 and parameters: {'observation_period_num': 15, 'train_rates': 0.6181898909059225, 'learning_rate': 0.0009137931864266909, 'batch_size': 124, 'step_size': 11, 'gamma': 0.9467750346069818}. Best is trial 12 with value: 0.0676589639679621.[0m
[32m[I 2025-02-02 22:40:55,134][0m Trial 13 finished with value: 0.07331336754179578 and parameters: {'observation_period_num': 6, 'train_rates': 0.6068194799657995, 'learning_rate': 0.00026251694572786345, 'batch_size': 110, 'step_size': 12, 'gamma': 0.9306485790781872}. Best is trial 12 with value: 0.0676589639679621.[0m
[32m[I 2025-02-02 22:41:26,070][0m Trial 14 finished with value: 0.07957520380960904 and parameters: {'observation_period_num': 12, 'train_rates': 0.6767330395138615, 'learning_rate': 0.0001241422445199926, 'batch_size': 109, 'step_size': 9, 'gamma': 0.9462536263212851}. Best is trial 12 with value: 0.0676589639679621.[0m
[32m[I 2025-02-02 22:41:58,586][0m Trial 15 finished with value: 0.051797542721033096 and parameters: {'observation_period_num': 30, 'train_rates': 0.9875622618920308, 'learning_rate': 0.0004403574490071067, 'batch_size': 132, 'step_size': 13, 'gamma': 0.838518500190211}. Best is trial 15 with value: 0.051797542721033096.[0m
[32m[I 2025-02-02 22:42:26,777][0m Trial 16 finished with value: 0.10701177269220352 and parameters: {'observation_period_num': 108, 'train_rates': 0.9896633518004223, 'learning_rate': 6.65372457395018e-05, 'batch_size': 160, 'step_size': 13, 'gamma': 0.8441430698903962}. Best is trial 15 with value: 0.051797542721033096.[0m
[32m[I 2025-02-02 22:43:18,507][0m Trial 17 finished with value: 0.0388229601085186 and parameters: {'observation_period_num': 35, 'train_rates': 0.9842007644580733, 'learning_rate': 0.00033251831032581644, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8429696349946083}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:44:06,672][0m Trial 18 finished with value: 0.043595101684331894 and parameters: {'observation_period_num': 40, 'train_rates': 0.9889711507652812, 'learning_rate': 0.0004112312861958281, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8370662011971258}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:46:28,945][0m Trial 19 finished with value: 0.23972605630755425 and parameters: {'observation_period_num': 42, 'train_rates': 0.9573730393837405, 'learning_rate': 9.90954842157973e-05, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8452395481001362}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:47:15,384][0m Trial 20 finished with value: 0.24205117672681808 and parameters: {'observation_period_num': 87, 'train_rates': 0.9416696263678466, 'learning_rate': 0.0002779530414774055, 'batch_size': 84, 'step_size': 8, 'gamma': 0.7794291436448075}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:48:03,573][0m Trial 21 finished with value: 0.3478548526763916 and parameters: {'observation_period_num': 32, 'train_rates': 0.9744414776391745, 'learning_rate': 0.0004547589522000922, 'batch_size': 85, 'step_size': 5, 'gamma': 0.8407572023944111}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:49:35,913][0m Trial 22 finished with value: 0.18925829663399665 and parameters: {'observation_period_num': 39, 'train_rates': 0.9352557924050171, 'learning_rate': 0.00040389142044505976, 'batch_size': 41, 'step_size': 7, 'gamma': 0.8159654000208647}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:50:21,399][0m Trial 23 finished with value: 0.07526986300945282 and parameters: {'observation_period_num': 115, 'train_rates': 0.9863937969161467, 'learning_rate': 0.00016386584532565967, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8616315702279622}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:50:48,431][0m Trial 24 finished with value: 0.09919116218414532 and parameters: {'observation_period_num': 32, 'train_rates': 0.826328162865535, 'learning_rate': 0.00045136854906674314, 'batch_size': 144, 'step_size': 13, 'gamma': 0.8271904970899924}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:51:56,877][0m Trial 25 finished with value: 0.2926417600748868 and parameters: {'observation_period_num': 83, 'train_rates': 0.9393770762551058, 'learning_rate': 5.1060684236190394e-05, 'batch_size': 57, 'step_size': 6, 'gamma': 0.7832573747305086}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:52:20,642][0m Trial 26 finished with value: 0.17370810406649206 and parameters: {'observation_period_num': 129, 'train_rates': 0.8200820063803734, 'learning_rate': 8.667746893917567e-05, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8621292813950445}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:53:04,290][0m Trial 27 finished with value: 0.3102412332709019 and parameters: {'observation_period_num': 52, 'train_rates': 0.964244897854527, 'learning_rate': 0.00023205477695285707, 'batch_size': 88, 'step_size': 4, 'gamma': 0.7961597606377233}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:54:24,875][0m Trial 28 finished with value: 0.19257667696192152 and parameters: {'observation_period_num': 75, 'train_rates': 0.92001707973287, 'learning_rate': 0.00041325174610805935, 'batch_size': 44, 'step_size': 8, 'gamma': 0.828530939008671}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:54:47,957][0m Trial 29 finished with value: 0.4077540631865112 and parameters: {'observation_period_num': 24, 'train_rates': 0.9031048305210271, 'learning_rate': 3.148560490659588e-05, 'batch_size': 234, 'step_size': 3, 'gamma': 0.7959384928882527}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:55:29,470][0m Trial 30 finished with value: 0.27915047452999997 and parameters: {'observation_period_num': 60, 'train_rates': 0.9596716828552004, 'learning_rate': 0.0005831387540547859, 'batch_size': 99, 'step_size': 14, 'gamma': 0.8540256387495708}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:55:55,837][0m Trial 31 finished with value: 0.0731990923020211 and parameters: {'observation_period_num': 22, 'train_rates': 0.6894321418872216, 'learning_rate': 0.0009955632174866386, 'batch_size': 129, 'step_size': 11, 'gamma': 0.9147043638795209}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:56:22,112][0m Trial 32 finished with value: 0.100118582389005 and parameters: {'observation_period_num': 42, 'train_rates': 0.7880270525327601, 'learning_rate': 0.0003005771173858678, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8801718142281696}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:56:49,404][0m Trial 33 finished with value: 0.10911587623904577 and parameters: {'observation_period_num': 64, 'train_rates': 0.71919561175803, 'learning_rate': 0.00016109055773758237, 'batch_size': 126, 'step_size': 10, 'gamma': 0.8246954134993514}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:57:14,970][0m Trial 34 finished with value: 0.10833176734243952 and parameters: {'observation_period_num': 49, 'train_rates': 0.8306970629362487, 'learning_rate': 0.0006136448393991617, 'batch_size': 168, 'step_size': 12, 'gamma': 0.7552962874428639}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:57:58,846][0m Trial 35 finished with value: 0.0742685849982518 and parameters: {'observation_period_num': 19, 'train_rates': 0.6593073671823586, 'learning_rate': 0.0003603437983171578, 'batch_size': 75, 'step_size': 10, 'gamma': 0.808929040527575}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 22:58:37,027][0m Trial 36 finished with value: 0.160122802752434 and parameters: {'observation_period_num': 94, 'train_rates': 0.8945646540748591, 'learning_rate': 0.00018822833825972576, 'batch_size': 101, 'step_size': 6, 'gamma': 0.8954563385664888}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:02:37,062][0m Trial 37 finished with value: 0.14933860305373528 and parameters: {'observation_period_num': 5, 'train_rates': 0.9258374232152585, 'learning_rate': 0.0005995289042016529, 'batch_size': 17, 'step_size': 14, 'gamma': 0.865377499089038}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:03:02,558][0m Trial 38 finished with value: 0.2637955844402313 and parameters: {'observation_period_num': 70, 'train_rates': 0.9884430982748631, 'learning_rate': 2.6076278468494692e-05, 'batch_size': 207, 'step_size': 6, 'gamma': 0.9091505461824883}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:03:39,853][0m Trial 39 finished with value: 0.5609808523041099 and parameters: {'observation_period_num': 30, 'train_rates': 0.9542670836124334, 'learning_rate': 7.978442977734684e-06, 'batch_size': 121, 'step_size': 8, 'gamma': 0.8344003786306061}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:04:51,882][0m Trial 40 finished with value: 0.16444348953139615 and parameters: {'observation_period_num': 56, 'train_rates': 0.8528883549404014, 'learning_rate': 1.1908863654995803e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9339120597916644}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:05:18,104][0m Trial 41 finished with value: 0.07818975385778876 and parameters: {'observation_period_num': 16, 'train_rates': 0.6350791656737169, 'learning_rate': 0.0007886802388488548, 'batch_size': 136, 'step_size': 11, 'gamma': 0.9799026969374379}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:05:47,232][0m Trial 42 finished with value: 0.07006121061706995 and parameters: {'observation_period_num': 18, 'train_rates': 0.6405917569699248, 'learning_rate': 0.0009426697497436774, 'batch_size': 119, 'step_size': 15, 'gamma': 0.9621423738658575}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:06:22,990][0m Trial 43 finished with value: 0.09704429893135336 and parameters: {'observation_period_num': 40, 'train_rates': 0.7031502243099023, 'learning_rate': 0.0005094671542017319, 'batch_size': 100, 'step_size': 10, 'gamma': 0.9538684516663678}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:06:47,686][0m Trial 44 finished with value: 1.0575576212385605 and parameters: {'observation_period_num': 30, 'train_rates': 0.6031317780975073, 'learning_rate': 1.064452210836464e-06, 'batch_size': 137, 'step_size': 14, 'gamma': 0.8506152591390017}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:07:38,418][0m Trial 45 finished with value: 0.07361796636494898 and parameters: {'observation_period_num': 8, 'train_rates': 0.7473842895313134, 'learning_rate': 0.0007045719978628746, 'batch_size': 72, 'step_size': 7, 'gamma': 0.9790416699685712}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:07:57,097][0m Trial 46 finished with value: 0.16088655208707203 and parameters: {'observation_period_num': 166, 'train_rates': 0.630577523562865, 'learning_rate': 0.00023100738638537422, 'batch_size': 183, 'step_size': 9, 'gamma': 0.8734934613471237}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:08:25,637][0m Trial 47 finished with value: 0.08953441639887055 and parameters: {'observation_period_num': 49, 'train_rates': 0.6597888217007657, 'learning_rate': 0.0003406411644853975, 'batch_size': 115, 'step_size': 12, 'gamma': 0.9886719490713702}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:08:52,956][0m Trial 48 finished with value: 0.1503711276231928 and parameters: {'observation_period_num': 69, 'train_rates': 0.8861676348002687, 'learning_rate': 0.0001379890105779431, 'batch_size': 152, 'step_size': 11, 'gamma': 0.8863114458840501}. Best is trial 17 with value: 0.0388229601085186.[0m
[32m[I 2025-02-02 23:09:26,238][0m Trial 49 finished with value: 0.06281516700983047 and parameters: {'observation_period_num': 14, 'train_rates': 0.9755200881595314, 'learning_rate': 0.0006990782891782224, 'batch_size': 132, 'step_size': 8, 'gamma': 0.8182988223587422}. Best is trial 17 with value: 0.0388229601085186.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-02 23:09:26,245][0m A new study created in memory with name: no-name-8b0fd3b4-c311-4e60-9e21-a237544291f3[0m
[32m[I 2025-02-02 23:09:45,584][0m Trial 0 finished with value: 0.19935860904161537 and parameters: {'observation_period_num': 214, 'train_rates': 0.7760748072432968, 'learning_rate': 0.00011817225371093303, 'batch_size': 244, 'step_size': 12, 'gamma': 0.8123159511390106}. Best is trial 0 with value: 0.19935860904161537.[0m
[32m[I 2025-02-02 23:10:05,960][0m Trial 1 finished with value: 0.14320717623551873 and parameters: {'observation_period_num': 137, 'train_rates': 0.6990573255238423, 'learning_rate': 0.00014354407887956947, 'batch_size': 192, 'step_size': 8, 'gamma': 0.9379590320763023}. Best is trial 1 with value: 0.14320717623551873.[0m
[32m[I 2025-02-02 23:10:29,497][0m Trial 2 finished with value: 0.7523838748266882 and parameters: {'observation_period_num': 131, 'train_rates': 0.7765182013277521, 'learning_rate': 1.1243190334767337e-06, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8859919374211054}. Best is trial 1 with value: 0.14320717623551873.[0m
[32m[I 2025-02-02 23:11:38,445][0m Trial 3 finished with value: 0.1946459853130838 and parameters: {'observation_period_num': 80, 'train_rates': 0.6396966932814013, 'learning_rate': 1.4728480566962188e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.8572225867281857}. Best is trial 1 with value: 0.14320717623551873.[0m
[32m[I 2025-02-02 23:14:30,473][0m Trial 4 finished with value: 0.6411595678718326 and parameters: {'observation_period_num': 237, 'train_rates': 0.638747852760914, 'learning_rate': 1.6271360042311718e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.91046082151892}. Best is trial 1 with value: 0.14320717623551873.[0m
[32m[I 2025-02-02 23:14:49,056][0m Trial 5 finished with value: 0.2910222855137616 and parameters: {'observation_period_num': 214, 'train_rates': 0.6258423642051927, 'learning_rate': 0.0008958104526668884, 'batch_size': 211, 'step_size': 9, 'gamma': 0.887845011075051}. Best is trial 1 with value: 0.14320717623551873.[0m
[32m[I 2025-02-02 23:15:11,818][0m Trial 6 finished with value: 0.13327626734972 and parameters: {'observation_period_num': 24, 'train_rates': 0.7489903283367871, 'learning_rate': 0.0004190419965433676, 'batch_size': 173, 'step_size': 2, 'gamma': 0.8118868137248064}. Best is trial 6 with value: 0.13327626734972.[0m
[32m[I 2025-02-02 23:15:31,839][0m Trial 7 finished with value: 0.16126361887142496 and parameters: {'observation_period_num': 114, 'train_rates': 0.7823291340945249, 'learning_rate': 0.0008383677650753853, 'batch_size': 222, 'step_size': 1, 'gamma': 0.9233052863488161}. Best is trial 6 with value: 0.13327626734972.[0m
[32m[I 2025-02-02 23:16:01,136][0m Trial 8 finished with value: 0.28206170704277644 and parameters: {'observation_period_num': 32, 'train_rates': 0.8969501615623425, 'learning_rate': 1.3257773990365558e-05, 'batch_size': 145, 'step_size': 11, 'gamma': 0.9755379578691539}. Best is trial 6 with value: 0.13327626734972.[0m
[32m[I 2025-02-02 23:16:24,622][0m Trial 9 finished with value: 0.09778890758752823 and parameters: {'observation_period_num': 177, 'train_rates': 0.988387702516061, 'learning_rate': 0.000913386207775243, 'batch_size': 223, 'step_size': 15, 'gamma': 0.821466938916286}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:17:02,183][0m Trial 10 finished with value: 0.1092931404709816 and parameters: {'observation_period_num': 176, 'train_rates': 0.9852817861341368, 'learning_rate': 6.802338971630759e-05, 'batch_size': 106, 'step_size': 15, 'gamma': 0.7738840568234266}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:17:45,723][0m Trial 11 finished with value: 0.10905767232179642 and parameters: {'observation_period_num': 176, 'train_rates': 0.9869567310525741, 'learning_rate': 7.932370939299138e-05, 'batch_size': 90, 'step_size': 15, 'gamma': 0.754474390433454}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:18:26,928][0m Trial 12 finished with value: 0.15430721640586853 and parameters: {'observation_period_num': 166, 'train_rates': 0.9853154090091485, 'learning_rate': 4.230585361550837e-05, 'batch_size': 96, 'step_size': 13, 'gamma': 0.7535994551557585}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:19:07,961][0m Trial 13 finished with value: 0.17487549025140006 and parameters: {'observation_period_num': 177, 'train_rates': 0.8958360691032354, 'learning_rate': 0.00026664292757326536, 'batch_size': 91, 'step_size': 15, 'gamma': 0.8100348489598738}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:20:08,652][0m Trial 14 finished with value: 0.35779655865582405 and parameters: {'observation_period_num': 85, 'train_rates': 0.8984819993973229, 'learning_rate': 1.288771206362498e-05, 'batch_size': 61, 'step_size': 6, 'gamma': 0.844934902894922}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:20:37,738][0m Trial 15 finished with value: 0.272444429738181 and parameters: {'observation_period_num': 200, 'train_rates': 0.9367196833678597, 'learning_rate': 0.0002780856209441883, 'batch_size': 124, 'step_size': 13, 'gamma': 0.7696723668443544}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:20:56,887][0m Trial 16 finished with value: 0.6698761314220046 and parameters: {'observation_period_num': 245, 'train_rates': 0.8342778684340533, 'learning_rate': 4.282707253453215e-06, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7917869995965893}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:21:43,319][0m Trial 17 finished with value: 0.1879074734477502 and parameters: {'observation_period_num': 157, 'train_rates': 0.848279000683886, 'learning_rate': 2.8197081114089547e-05, 'batch_size': 71, 'step_size': 12, 'gamma': 0.8403130363512632}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:22:12,995][0m Trial 18 finished with value: 1.1433377787470818 and parameters: {'observation_period_num': 104, 'train_rates': 0.9383446477678654, 'learning_rate': 4.470625670883577e-06, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7855087617265245}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:22:36,886][0m Trial 19 finished with value: 0.3462434411048889 and parameters: {'observation_period_num': 151, 'train_rates': 0.9455945398674553, 'learning_rate': 9.163427014289716e-05, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8322863153891766}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:22:57,047][0m Trial 20 finished with value: 0.22569102006601105 and parameters: {'observation_period_num': 193, 'train_rates': 0.8468083768394747, 'learning_rate': 0.0004717315065453805, 'batch_size': 220, 'step_size': 4, 'gamma': 0.760315573613342}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:23:31,538][0m Trial 21 finished with value: 0.17228180170059204 and parameters: {'observation_period_num': 178, 'train_rates': 0.97508977854798, 'learning_rate': 5.515663290852584e-05, 'batch_size': 105, 'step_size': 15, 'gamma': 0.7850012344885262}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:24:18,929][0m Trial 22 finished with value: 0.10440527647733688 and parameters: {'observation_period_num': 185, 'train_rates': 0.9853352996818285, 'learning_rate': 7.095153528922835e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.751006910520702}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:25:09,796][0m Trial 23 finished with value: 0.41758757904707955 and parameters: {'observation_period_num': 218, 'train_rates': 0.9491565720036043, 'learning_rate': 2.1864268952000756e-05, 'batch_size': 69, 'step_size': 13, 'gamma': 0.7566722560877318}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:27:29,066][0m Trial 24 finished with value: 0.2189908422812631 and parameters: {'observation_period_num': 144, 'train_rates': 0.9158329877574666, 'learning_rate': 0.00019039020255404892, 'batch_size': 24, 'step_size': 14, 'gamma': 0.7970455674908765}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:28:55,355][0m Trial 25 finished with value: 0.7215643975623819 and parameters: {'observation_period_num': 195, 'train_rates': 0.9688741440623428, 'learning_rate': 5.15410597380211e-06, 'batch_size': 42, 'step_size': 12, 'gamma': 0.7501810725944648}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:29:23,293][0m Trial 26 finished with value: 0.18031424695200313 and parameters: {'observation_period_num': 120, 'train_rates': 0.8809883623501751, 'learning_rate': 0.0005188768934277667, 'batch_size': 142, 'step_size': 14, 'gamma': 0.8251970046247622}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:30:09,129][0m Trial 27 finished with value: 0.20304001406740746 and parameters: {'observation_period_num': 226, 'train_rates': 0.8673755936331813, 'learning_rate': 8.022006970135364e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8688733772805295}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:30:40,523][0m Trial 28 finished with value: 0.31054482394701816 and parameters: {'observation_period_num': 192, 'train_rates': 0.9209479290221013, 'learning_rate': 4.46155244823508e-05, 'batch_size': 121, 'step_size': 14, 'gamma': 0.7757590251458986}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:31:02,280][0m Trial 29 finished with value: 0.17416691665130202 and parameters: {'observation_period_num': 161, 'train_rates': 0.8167441233534036, 'learning_rate': 0.00019222437089553272, 'batch_size': 239, 'step_size': 11, 'gamma': 0.8006048468382012}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:31:46,139][0m Trial 30 finished with value: 0.3164370731782105 and parameters: {'observation_period_num': 252, 'train_rates': 0.9565309475022679, 'learning_rate': 0.00012859490486574505, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8209488691412008}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:32:23,674][0m Trial 31 finished with value: 0.12135495245456696 and parameters: {'observation_period_num': 176, 'train_rates': 0.9882519947675205, 'learning_rate': 7.9545827441821e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.7736770560313903}. Best is trial 9 with value: 0.09778890758752823.[0m
[32m[I 2025-02-02 23:33:34,571][0m Trial 32 finished with value: 0.09776743422997625 and parameters: {'observation_period_num': 212, 'train_rates': 0.9794051673704159, 'learning_rate': 6.0073539645271813e-05, 'batch_size': 53, 'step_size': 15, 'gamma': 0.767964683374105}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:34:29,602][0m Trial 33 finished with value: 0.2649151168125634 and parameters: {'observation_period_num': 203, 'train_rates': 0.7496098819502961, 'learning_rate': 2.2930965624199574e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.7639323167832288}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:35:51,935][0m Trial 34 finished with value: 0.37355536441592607 and parameters: {'observation_period_num': 212, 'train_rates': 0.9629045193488387, 'learning_rate': 0.0001294335092655555, 'batch_size': 42, 'step_size': 14, 'gamma': 0.7502119055614717}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:36:58,685][0m Trial 35 finished with value: 0.3033755909705508 and parameters: {'observation_period_num': 227, 'train_rates': 0.9243448774708649, 'learning_rate': 3.7072540819413375e-05, 'batch_size': 50, 'step_size': 8, 'gamma': 0.7843684058667623}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:38:40,810][0m Trial 36 finished with value: 0.237026834891592 and parameters: {'observation_period_num': 137, 'train_rates': 0.6813284430040745, 'learning_rate': 8.521694804901271e-06, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8039093434343848}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:39:04,873][0m Trial 37 finished with value: 0.35991379618644714 and parameters: {'observation_period_num': 233, 'train_rates': 0.9624549660705103, 'learning_rate': 0.00020780261725268927, 'batch_size': 200, 'step_size': 13, 'gamma': 0.9847628490833958}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:39:31,144][0m Trial 38 finished with value: 0.2580101592200143 and parameters: {'observation_period_num': 183, 'train_rates': 0.9242227511197029, 'learning_rate': 0.00010139537413066915, 'batch_size': 159, 'step_size': 14, 'gamma': 0.8640820452718143}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:40:09,306][0m Trial 39 finished with value: 0.7719265305254448 and parameters: {'observation_period_num': 208, 'train_rates': 0.6953922591548368, 'learning_rate': 1.5335491539003294e-06, 'batch_size': 80, 'step_size': 10, 'gamma': 0.9029511715707862}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:40:36,691][0m Trial 40 finished with value: 0.37900346517562866 and parameters: {'observation_period_num': 126, 'train_rates': 0.9683192805278155, 'learning_rate': 0.0009236179526941118, 'batch_size': 153, 'step_size': 8, 'gamma': 0.8799418597052658}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:41:14,298][0m Trial 41 finished with value: 0.11212701350450516 and parameters: {'observation_period_num': 167, 'train_rates': 0.9892732641840006, 'learning_rate': 5.996736697535609e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.769230452205984}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:41:53,597][0m Trial 42 finished with value: 0.1010424867272377 and parameters: {'observation_period_num': 185, 'train_rates': 0.988445522056555, 'learning_rate': 6.441135059841994e-05, 'batch_size': 101, 'step_size': 15, 'gamma': 0.7782618949228898}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:42:35,366][0m Trial 43 finished with value: 0.3320000561905074 and parameters: {'observation_period_num': 148, 'train_rates': 0.9492169543053302, 'learning_rate': 2.9119407311638207e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.951336561604359}. Best is trial 32 with value: 0.09776743422997625.[0m
[32m[I 2025-02-02 23:43:38,406][0m Trial 44 finished with value: 0.08786391466856003 and parameters: {'observation_period_num': 53, 'train_rates': 0.9897444139284903, 'learning_rate': 1.8334408738003524e-05, 'batch_size': 65, 'step_size': 15, 'gamma': 0.783084706566163}. Best is trial 44 with value: 0.08786391466856003.[0m
[32m[I 2025-02-02 23:44:39,844][0m Trial 45 finished with value: 0.5404400620609522 and parameters: {'observation_period_num': 46, 'train_rates': 0.9672566832567018, 'learning_rate': 7.3120521529151445e-06, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8166821741856023}. Best is trial 44 with value: 0.08786391466856003.[0m
[32m[I 2025-02-02 23:46:23,871][0m Trial 46 finished with value: 0.24824696871890592 and parameters: {'observation_period_num': 52, 'train_rates': 0.9348404180903501, 'learning_rate': 1.6100201206320277e-05, 'batch_size': 34, 'step_size': 14, 'gamma': 0.7823579634309339}. Best is trial 44 with value: 0.08786391466856003.[0m
[32m[I 2025-02-02 23:47:29,398][0m Trial 47 finished with value: 0.1997785168681659 and parameters: {'observation_period_num': 18, 'train_rates': 0.9132021593203654, 'learning_rate': 1.758075335091469e-05, 'batch_size': 55, 'step_size': 13, 'gamma': 0.7929324919307622}. Best is trial 44 with value: 0.08786391466856003.[0m
[32m[I 2025-02-02 23:48:06,430][0m Trial 48 finished with value: 0.743097190645006 and parameters: {'observation_period_num': 98, 'train_rates': 0.6057984183192937, 'learning_rate': 2.9528509194993895e-06, 'batch_size': 74, 'step_size': 15, 'gamma': 0.8085842648896251}. Best is trial 44 with value: 0.08786391466856003.[0m
[32m[I 2025-02-02 23:48:46,796][0m Trial 49 finished with value: 0.25546538829803467 and parameters: {'observation_period_num': 71, 'train_rates': 0.989948579346145, 'learning_rate': 1.0105183367379858e-05, 'batch_size': 98, 'step_size': 14, 'gamma': 0.7649819573857018}. Best is trial 44 with value: 0.08786391466856003.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 10, 'train_rates': 0.7129901804976123, 'learning_rate': 0.00020029441795952236, 'batch_size': 127, 'step_size': 7, 'gamma': 0.8968988915988503}
Epoch 1/300, trend Loss: 0.7564 | 0.8134
Epoch 2/300, trend Loss: 0.4006 | 0.4271
Epoch 3/300, trend Loss: 0.2249 | 0.2740
Epoch 4/300, trend Loss: 0.1809 | 0.2034
Epoch 5/300, trend Loss: 0.1675 | 0.1864
Epoch 6/300, trend Loss: 0.1563 | 0.1735
Epoch 7/300, trend Loss: 0.1535 | 0.1630
Epoch 8/300, trend Loss: 0.1463 | 0.1543
Epoch 9/300, trend Loss: 0.1415 | 0.1486
Epoch 10/300, trend Loss: 0.1407 | 0.1469
Epoch 11/300, trend Loss: 0.1370 | 0.1447
Epoch 12/300, trend Loss: 0.1304 | 0.1431
Epoch 13/300, trend Loss: 0.1298 | 0.1386
Epoch 14/300, trend Loss: 0.1337 | 0.1342
Epoch 15/300, trend Loss: 0.1350 | 0.1285
Epoch 16/300, trend Loss: 0.1282 | 0.1283
Epoch 17/300, trend Loss: 0.1275 | 0.1323
Epoch 18/300, trend Loss: 0.1276 | 0.1316
Epoch 19/300, trend Loss: 0.1219 | 0.1268
Epoch 20/300, trend Loss: 0.1232 | 0.1211
Epoch 21/300, trend Loss: 0.1239 | 0.1173
Epoch 22/300, trend Loss: 0.1188 | 0.1193
Epoch 23/300, trend Loss: 0.1184 | 0.1224
Epoch 24/300, trend Loss: 0.1172 | 0.1202
Epoch 25/300, trend Loss: 0.1157 | 0.1145
Epoch 26/300, trend Loss: 0.1161 | 0.1123
Epoch 27/300, trend Loss: 0.1143 | 0.1126
Epoch 28/300, trend Loss: 0.1130 | 0.1154
Epoch 29/300, trend Loss: 0.1128 | 0.1131
Epoch 30/300, trend Loss: 0.1117 | 0.1107
Epoch 31/300, trend Loss: 0.1116 | 0.1086
Epoch 32/300, trend Loss: 0.1105 | 0.1099
Epoch 33/300, trend Loss: 0.1099 | 0.1095
Epoch 34/300, trend Loss: 0.1095 | 0.1098
Epoch 35/300, trend Loss: 0.1091 | 0.1071
Epoch 36/300, trend Loss: 0.1087 | 0.1074
Epoch 37/300, trend Loss: 0.1084 | 0.1064
Epoch 38/300, trend Loss: 0.1078 | 0.1082
Epoch 39/300, trend Loss: 0.1077 | 0.1059
Epoch 40/300, trend Loss: 0.1073 | 0.1069
Epoch 41/300, trend Loss: 0.1071 | 0.1045
Epoch 42/300, trend Loss: 0.1066 | 0.1069
Epoch 43/300, trend Loss: 0.1066 | 0.1042
Epoch 44/300, trend Loss: 0.1062 | 0.1066
Epoch 45/300, trend Loss: 0.1062 | 0.1033
Epoch 46/300, trend Loss: 0.1058 | 0.1063
Epoch 47/300, trend Loss: 0.1058 | 0.1027
Epoch 48/300, trend Loss: 0.1055 | 0.1063
Epoch 49/300, trend Loss: 0.1055 | 0.1021
Epoch 50/300, trend Loss: 0.1051 | 0.1062
Epoch 51/300, trend Loss: 0.1052 | 0.1016
Epoch 52/300, trend Loss: 0.1048 | 0.1060
Epoch 53/300, trend Loss: 0.1049 | 0.1012
Epoch 54/300, trend Loss: 0.1045 | 0.1057
Epoch 55/300, trend Loss: 0.1045 | 0.1009
Epoch 56/300, trend Loss: 0.1042 | 0.1052
Epoch 57/300, trend Loss: 0.1042 | 0.1007
Epoch 58/300, trend Loss: 0.1039 | 0.1045
Epoch 59/300, trend Loss: 0.1039 | 0.1006
Epoch 60/300, trend Loss: 0.1036 | 0.1039
Epoch 61/300, trend Loss: 0.1035 | 0.1005
Epoch 62/300, trend Loss: 0.1033 | 0.1032
Epoch 63/300, trend Loss: 0.1032 | 0.1005
Epoch 64/300, trend Loss: 0.1030 | 0.1025
Epoch 65/300, trend Loss: 0.1029 | 0.1005
Epoch 66/300, trend Loss: 0.1028 | 0.1019
Epoch 67/300, trend Loss: 0.1027 | 0.1004
Epoch 68/300, trend Loss: 0.1025 | 0.1014
Epoch 69/300, trend Loss: 0.1025 | 0.1004
Epoch 70/300, trend Loss: 0.1023 | 0.1009
Epoch 71/300, trend Loss: 0.1022 | 0.1003
Epoch 72/300, trend Loss: 0.1021 | 0.1005
Epoch 73/300, trend Loss: 0.1021 | 0.1002
Epoch 74/300, trend Loss: 0.1020 | 0.1003
Epoch 75/300, trend Loss: 0.1019 | 0.1000
Epoch 76/300, trend Loss: 0.1018 | 0.1000
Epoch 77/300, trend Loss: 0.1017 | 0.0998
Epoch 78/300, trend Loss: 0.1016 | 0.0998
Epoch 79/300, trend Loss: 0.1015 | 0.0997
Epoch 80/300, trend Loss: 0.1015 | 0.0996
Epoch 81/300, trend Loss: 0.1014 | 0.0995
Epoch 82/300, trend Loss: 0.1013 | 0.0994
Epoch 83/300, trend Loss: 0.1012 | 0.0993
Epoch 84/300, trend Loss: 0.1012 | 0.0992
Epoch 85/300, trend Loss: 0.1011 | 0.0991
Epoch 86/300, trend Loss: 0.1010 | 0.0991
Epoch 87/300, trend Loss: 0.1010 | 0.0990
Epoch 88/300, trend Loss: 0.1009 | 0.0989
Epoch 89/300, trend Loss: 0.1008 | 0.0988
Epoch 90/300, trend Loss: 0.1008 | 0.0988
Epoch 91/300, trend Loss: 0.1007 | 0.0987
Epoch 92/300, trend Loss: 0.1006 | 0.0986
Epoch 93/300, trend Loss: 0.1006 | 0.0985
Epoch 94/300, trend Loss: 0.1005 | 0.0985
Epoch 95/300, trend Loss: 0.1005 | 0.0984
Epoch 96/300, trend Loss: 0.1004 | 0.0983
Epoch 97/300, trend Loss: 0.1004 | 0.0983
Epoch 98/300, trend Loss: 0.1003 | 0.0982
Epoch 99/300, trend Loss: 0.1002 | 0.0982
Epoch 100/300, trend Loss: 0.1002 | 0.0981
Epoch 101/300, trend Loss: 0.1002 | 0.0981
Epoch 102/300, trend Loss: 0.1001 | 0.0980
Epoch 103/300, trend Loss: 0.1001 | 0.0979
Epoch 104/300, trend Loss: 0.1000 | 0.0979
Epoch 105/300, trend Loss: 0.1000 | 0.0978
Epoch 106/300, trend Loss: 0.0999 | 0.0978
Epoch 107/300, trend Loss: 0.0999 | 0.0977
Epoch 108/300, trend Loss: 0.0998 | 0.0977
Epoch 109/300, trend Loss: 0.0998 | 0.0976
Epoch 110/300, trend Loss: 0.0998 | 0.0976
Epoch 111/300, trend Loss: 0.0997 | 0.0976
Epoch 112/300, trend Loss: 0.0997 | 0.0975
Epoch 113/300, trend Loss: 0.0996 | 0.0975
Epoch 114/300, trend Loss: 0.0996 | 0.0974
Epoch 115/300, trend Loss: 0.0996 | 0.0974
Epoch 116/300, trend Loss: 0.0995 | 0.0974
Epoch 117/300, trend Loss: 0.0995 | 0.0973
Epoch 118/300, trend Loss: 0.0995 | 0.0973
Epoch 119/300, trend Loss: 0.0994 | 0.0972
Epoch 120/300, trend Loss: 0.0994 | 0.0972
Epoch 121/300, trend Loss: 0.0994 | 0.0972
Epoch 122/300, trend Loss: 0.0994 | 0.0971
Epoch 123/300, trend Loss: 0.0993 | 0.0971
Epoch 124/300, trend Loss: 0.0993 | 0.0971
Epoch 125/300, trend Loss: 0.0993 | 0.0970
Epoch 126/300, trend Loss: 0.0992 | 0.0970
Epoch 127/300, trend Loss: 0.0992 | 0.0970
Epoch 128/300, trend Loss: 0.0992 | 0.0970
Epoch 129/300, trend Loss: 0.0992 | 0.0969
Epoch 130/300, trend Loss: 0.0991 | 0.0969
Epoch 131/300, trend Loss: 0.0991 | 0.0969
Epoch 132/300, trend Loss: 0.0991 | 0.0968
Epoch 133/300, trend Loss: 0.0991 | 0.0968
Epoch 134/300, trend Loss: 0.0991 | 0.0968
Epoch 135/300, trend Loss: 0.0990 | 0.0968
Epoch 136/300, trend Loss: 0.0990 | 0.0967
Epoch 137/300, trend Loss: 0.0990 | 0.0967
Epoch 138/300, trend Loss: 0.0990 | 0.0967
Epoch 139/300, trend Loss: 0.0990 | 0.0967
Epoch 140/300, trend Loss: 0.0989 | 0.0967
Epoch 141/300, trend Loss: 0.0989 | 0.0966
Epoch 142/300, trend Loss: 0.0989 | 0.0966
Epoch 143/300, trend Loss: 0.0989 | 0.0966
Epoch 144/300, trend Loss: 0.0989 | 0.0966
Epoch 145/300, trend Loss: 0.0988 | 0.0966
Epoch 146/300, trend Loss: 0.0988 | 0.0965
Epoch 147/300, trend Loss: 0.0988 | 0.0965
Epoch 148/300, trend Loss: 0.0988 | 0.0965
Epoch 149/300, trend Loss: 0.0988 | 0.0965
Epoch 150/300, trend Loss: 0.0988 | 0.0965
Epoch 151/300, trend Loss: 0.0988 | 0.0965
Epoch 152/300, trend Loss: 0.0987 | 0.0964
Epoch 153/300, trend Loss: 0.0987 | 0.0964
Epoch 154/300, trend Loss: 0.0987 | 0.0964
Epoch 155/300, trend Loss: 0.0987 | 0.0964
Epoch 156/300, trend Loss: 0.0987 | 0.0964
Epoch 157/300, trend Loss: 0.0987 | 0.0964
Epoch 158/300, trend Loss: 0.0987 | 0.0964
Epoch 159/300, trend Loss: 0.0987 | 0.0963
Epoch 160/300, trend Loss: 0.0986 | 0.0963
Epoch 161/300, trend Loss: 0.0986 | 0.0963
Epoch 162/300, trend Loss: 0.0986 | 0.0963
Epoch 163/300, trend Loss: 0.0986 | 0.0963
Epoch 164/300, trend Loss: 0.0986 | 0.0963
Epoch 165/300, trend Loss: 0.0986 | 0.0963
Epoch 166/300, trend Loss: 0.0986 | 0.0963
Epoch 167/300, trend Loss: 0.0986 | 0.0963
Epoch 168/300, trend Loss: 0.0986 | 0.0962
Epoch 169/300, trend Loss: 0.0986 | 0.0962
Epoch 170/300, trend Loss: 0.0986 | 0.0962
Epoch 171/300, trend Loss: 0.0985 | 0.0962
Epoch 172/300, trend Loss: 0.0985 | 0.0962
Epoch 173/300, trend Loss: 0.0985 | 0.0962
Epoch 174/300, trend Loss: 0.0985 | 0.0962
Epoch 175/300, trend Loss: 0.0985 | 0.0962
Epoch 176/300, trend Loss: 0.0985 | 0.0962
Epoch 177/300, trend Loss: 0.0985 | 0.0962
Epoch 178/300, trend Loss: 0.0985 | 0.0962
Epoch 179/300, trend Loss: 0.0985 | 0.0962
Epoch 180/300, trend Loss: 0.0985 | 0.0961
Epoch 181/300, trend Loss: 0.0985 | 0.0961
Epoch 182/300, trend Loss: 0.0985 | 0.0961
Epoch 183/300, trend Loss: 0.0985 | 0.0961
Epoch 184/300, trend Loss: 0.0985 | 0.0961
Epoch 185/300, trend Loss: 0.0984 | 0.0961
Epoch 186/300, trend Loss: 0.0984 | 0.0961
Epoch 187/300, trend Loss: 0.0984 | 0.0961
Epoch 188/300, trend Loss: 0.0984 | 0.0961
Epoch 189/300, trend Loss: 0.0984 | 0.0961
Epoch 190/300, trend Loss: 0.0984 | 0.0961
Epoch 191/300, trend Loss: 0.0984 | 0.0961
Epoch 192/300, trend Loss: 0.0984 | 0.0961
Epoch 193/300, trend Loss: 0.0984 | 0.0961
Epoch 194/300, trend Loss: 0.0984 | 0.0961
Epoch 195/300, trend Loss: 0.0984 | 0.0961
Epoch 196/300, trend Loss: 0.0984 | 0.0960
Epoch 197/300, trend Loss: 0.0984 | 0.0960
Epoch 198/300, trend Loss: 0.0984 | 0.0960
Epoch 199/300, trend Loss: 0.0984 | 0.0960
Epoch 200/300, trend Loss: 0.0984 | 0.0960
Epoch 201/300, trend Loss: 0.0984 | 0.0960
Epoch 202/300, trend Loss: 0.0984 | 0.0960
Epoch 203/300, trend Loss: 0.0984 | 0.0960
Epoch 204/300, trend Loss: 0.0984 | 0.0960
Epoch 205/300, trend Loss: 0.0984 | 0.0960
Epoch 206/300, trend Loss: 0.0984 | 0.0960
Epoch 207/300, trend Loss: 0.0984 | 0.0960
Epoch 208/300, trend Loss: 0.0984 | 0.0960
Epoch 209/300, trend Loss: 0.0984 | 0.0960
Epoch 210/300, trend Loss: 0.0983 | 0.0960
Epoch 211/300, trend Loss: 0.0983 | 0.0960
Epoch 212/300, trend Loss: 0.0983 | 0.0960
Epoch 213/300, trend Loss: 0.0983 | 0.0960
Epoch 214/300, trend Loss: 0.0983 | 0.0960
Epoch 215/300, trend Loss: 0.0983 | 0.0960
Epoch 216/300, trend Loss: 0.0983 | 0.0960
Epoch 217/300, trend Loss: 0.0983 | 0.0960
Epoch 218/300, trend Loss: 0.0983 | 0.0960
Epoch 219/300, trend Loss: 0.0983 | 0.0960
Epoch 220/300, trend Loss: 0.0983 | 0.0960
Epoch 221/300, trend Loss: 0.0983 | 0.0960
Epoch 222/300, trend Loss: 0.0983 | 0.0960
Epoch 223/300, trend Loss: 0.0983 | 0.0960
Epoch 224/300, trend Loss: 0.0983 | 0.0960
Epoch 225/300, trend Loss: 0.0983 | 0.0960
Epoch 226/300, trend Loss: 0.0983 | 0.0960
Epoch 227/300, trend Loss: 0.0983 | 0.0960
Epoch 228/300, trend Loss: 0.0983 | 0.0960
Epoch 229/300, trend Loss: 0.0983 | 0.0959
Epoch 230/300, trend Loss: 0.0983 | 0.0959
Epoch 231/300, trend Loss: 0.0983 | 0.0959
Epoch 232/300, trend Loss: 0.0983 | 0.0959
Epoch 233/300, trend Loss: 0.0983 | 0.0959
Epoch 234/300, trend Loss: 0.0983 | 0.0959
Epoch 235/300, trend Loss: 0.0983 | 0.0959
Epoch 236/300, trend Loss: 0.0983 | 0.0959
Epoch 237/300, trend Loss: 0.0983 | 0.0959
Epoch 238/300, trend Loss: 0.0983 | 0.0959
Epoch 239/300, trend Loss: 0.0983 | 0.0959
Epoch 240/300, trend Loss: 0.0983 | 0.0959
Epoch 241/300, trend Loss: 0.0983 | 0.0959
Epoch 242/300, trend Loss: 0.0983 | 0.0959
Epoch 243/300, trend Loss: 0.0983 | 0.0959
Epoch 244/300, trend Loss: 0.0983 | 0.0959
Epoch 245/300, trend Loss: 0.0983 | 0.0959
Epoch 246/300, trend Loss: 0.0983 | 0.0959
Epoch 247/300, trend Loss: 0.0983 | 0.0959
Epoch 248/300, trend Loss: 0.0983 | 0.0959
Epoch 249/300, trend Loss: 0.0983 | 0.0959
Epoch 250/300, trend Loss: 0.0983 | 0.0959
Epoch 251/300, trend Loss: 0.0983 | 0.0959
Epoch 252/300, trend Loss: 0.0983 | 0.0959
Epoch 253/300, trend Loss: 0.0983 | 0.0959
Epoch 254/300, trend Loss: 0.0983 | 0.0959
Epoch 255/300, trend Loss: 0.0983 | 0.0959
Epoch 256/300, trend Loss: 0.0983 | 0.0959
Epoch 257/300, trend Loss: 0.0983 | 0.0959
Epoch 258/300, trend Loss: 0.0983 | 0.0959
Epoch 259/300, trend Loss: 0.0983 | 0.0959
Epoch 260/300, trend Loss: 0.0983 | 0.0959
Epoch 261/300, trend Loss: 0.0983 | 0.0959
Epoch 262/300, trend Loss: 0.0983 | 0.0959
Epoch 263/300, trend Loss: 0.0983 | 0.0959
Epoch 264/300, trend Loss: 0.0983 | 0.0959
Epoch 265/300, trend Loss: 0.0983 | 0.0959
Epoch 266/300, trend Loss: 0.0983 | 0.0959
Epoch 267/300, trend Loss: 0.0983 | 0.0959
Epoch 268/300, trend Loss: 0.0983 | 0.0959
Epoch 269/300, trend Loss: 0.0983 | 0.0959
Epoch 270/300, trend Loss: 0.0983 | 0.0959
Epoch 271/300, trend Loss: 0.0983 | 0.0959
Epoch 272/300, trend Loss: 0.0983 | 0.0959
Epoch 273/300, trend Loss: 0.0983 | 0.0959
Epoch 274/300, trend Loss: 0.0983 | 0.0959
Epoch 275/300, trend Loss: 0.0983 | 0.0959
Epoch 276/300, trend Loss: 0.0983 | 0.0959
Epoch 277/300, trend Loss: 0.0983 | 0.0959
Epoch 278/300, trend Loss: 0.0983 | 0.0959
Epoch 279/300, trend Loss: 0.0983 | 0.0959
Epoch 280/300, trend Loss: 0.0983 | 0.0959
Epoch 281/300, trend Loss: 0.0983 | 0.0959
Epoch 282/300, trend Loss: 0.0983 | 0.0959
Epoch 283/300, trend Loss: 0.0983 | 0.0959
Epoch 284/300, trend Loss: 0.0983 | 0.0959
Epoch 285/300, trend Loss: 0.0983 | 0.0959
Epoch 286/300, trend Loss: 0.0983 | 0.0959
Epoch 287/300, trend Loss: 0.0983 | 0.0959
Epoch 288/300, trend Loss: 0.0983 | 0.0959
Epoch 289/300, trend Loss: 0.0983 | 0.0959
Epoch 290/300, trend Loss: 0.0983 | 0.0959
Epoch 291/300, trend Loss: 0.0983 | 0.0959
Epoch 292/300, trend Loss: 0.0983 | 0.0959
Epoch 293/300, trend Loss: 0.0983 | 0.0959
Epoch 294/300, trend Loss: 0.0983 | 0.0959
Epoch 295/300, trend Loss: 0.0983 | 0.0959
Epoch 296/300, trend Loss: 0.0983 | 0.0959
Epoch 297/300, trend Loss: 0.0983 | 0.0959
Epoch 298/300, trend Loss: 0.0983 | 0.0959
Epoch 299/300, trend Loss: 0.0983 | 0.0959
Epoch 300/300, trend Loss: 0.0983 | 0.0959
Training seasonal_0 component with params: {'observation_period_num': 117, 'train_rates': 0.9763231303031986, 'learning_rate': 0.00039940954425028794, 'batch_size': 124, 'step_size': 12, 'gamma': 0.9854649542093142}
Epoch 1/300, seasonal_0 Loss: 0.6770 | 0.6649
Epoch 2/300, seasonal_0 Loss: 0.4500 | 0.5148
Epoch 3/300, seasonal_0 Loss: 0.3691 | 0.4161
Epoch 4/300, seasonal_0 Loss: 0.5781 | 0.5248
Epoch 5/300, seasonal_0 Loss: 0.3608 | 0.4513
Epoch 6/300, seasonal_0 Loss: 0.2583 | 0.3334
Epoch 7/300, seasonal_0 Loss: 0.2778 | 0.3414
Epoch 8/300, seasonal_0 Loss: 0.2554 | 0.2614
Epoch 9/300, seasonal_0 Loss: 0.2126 | 0.2559
Epoch 10/300, seasonal_0 Loss: 0.1819 | 0.2432
Epoch 11/300, seasonal_0 Loss: 0.1825 | 0.2803
Epoch 12/300, seasonal_0 Loss: 0.1835 | 0.4257
Epoch 13/300, seasonal_0 Loss: 0.1724 | 0.2722
Epoch 14/300, seasonal_0 Loss: 0.1876 | 0.2212
Epoch 15/300, seasonal_0 Loss: 0.1710 | 0.1685
Epoch 16/300, seasonal_0 Loss: 0.1968 | 0.1812
Epoch 17/300, seasonal_0 Loss: 0.1695 | 0.2297
Epoch 18/300, seasonal_0 Loss: 0.1414 | 0.2643
Epoch 19/300, seasonal_0 Loss: 0.1422 | 0.2295
Epoch 20/300, seasonal_0 Loss: 0.1437 | 0.1779
Epoch 21/300, seasonal_0 Loss: 0.1369 | 0.1589
Epoch 22/300, seasonal_0 Loss: 0.1296 | 0.1558
Epoch 23/300, seasonal_0 Loss: 0.1242 | 0.2125
Epoch 24/300, seasonal_0 Loss: 0.1237 | 0.2293
Epoch 25/300, seasonal_0 Loss: 0.1397 | 0.1665
Epoch 26/300, seasonal_0 Loss: 0.1367 | 0.1356
Epoch 27/300, seasonal_0 Loss: 0.1361 | 0.1255
Epoch 28/300, seasonal_0 Loss: 0.1165 | 0.1548
Epoch 29/300, seasonal_0 Loss: 0.1150 | 0.1842
Epoch 30/300, seasonal_0 Loss: 0.1187 | 0.2029
Epoch 31/300, seasonal_0 Loss: 0.1177 | 0.1787
Epoch 32/300, seasonal_0 Loss: 0.1148 | 0.1651
Epoch 33/300, seasonal_0 Loss: 0.1123 | 0.1949
Epoch 34/300, seasonal_0 Loss: 0.1140 | 0.3232
Epoch 35/300, seasonal_0 Loss: 0.1235 | 0.2049
Epoch 36/300, seasonal_0 Loss: 0.1353 | 0.1943
Epoch 37/300, seasonal_0 Loss: 0.1379 | 0.1326
Epoch 38/300, seasonal_0 Loss: 0.1638 | 0.1441
Epoch 39/300, seasonal_0 Loss: 0.1395 | 0.1472
Epoch 40/300, seasonal_0 Loss: 0.1280 | 0.1830
Epoch 41/300, seasonal_0 Loss: 0.1125 | 0.1990
Epoch 42/300, seasonal_0 Loss: 0.1100 | 0.1204
Epoch 43/300, seasonal_0 Loss: 0.1090 | 0.1185
Epoch 44/300, seasonal_0 Loss: 0.1098 | 0.1014
Epoch 45/300, seasonal_0 Loss: 0.1168 | 0.1389
Epoch 46/300, seasonal_0 Loss: 0.1096 | 0.1291
Epoch 47/300, seasonal_0 Loss: 0.1165 | 0.1317
Epoch 48/300, seasonal_0 Loss: 0.1157 | 0.1358
Epoch 49/300, seasonal_0 Loss: 0.1132 | 0.0963
Epoch 50/300, seasonal_0 Loss: 0.1124 | 0.1237
Epoch 51/300, seasonal_0 Loss: 0.1108 | 0.1534
Epoch 52/300, seasonal_0 Loss: 0.1071 | 0.1520
Epoch 53/300, seasonal_0 Loss: 0.1131 | 0.1779
Epoch 54/300, seasonal_0 Loss: 0.1003 | 0.0983
Epoch 55/300, seasonal_0 Loss: 0.0966 | 0.1237
Epoch 56/300, seasonal_0 Loss: 0.1002 | 0.1235
Epoch 57/300, seasonal_0 Loss: 0.1096 | 0.1243
Epoch 58/300, seasonal_0 Loss: 0.1126 | 0.1091
Epoch 59/300, seasonal_0 Loss: 0.1156 | 0.0992
Epoch 60/300, seasonal_0 Loss: 0.1011 | 0.1010
Epoch 61/300, seasonal_0 Loss: 0.1141 | 0.1355
Epoch 62/300, seasonal_0 Loss: 0.1040 | 0.1385
Epoch 63/300, seasonal_0 Loss: 0.0960 | 0.1075
Epoch 64/300, seasonal_0 Loss: 0.0912 | 0.1014
Epoch 65/300, seasonal_0 Loss: 0.0980 | 0.0887
Epoch 66/300, seasonal_0 Loss: 0.0972 | 0.1223
Epoch 67/300, seasonal_0 Loss: 0.0972 | 0.1444
Epoch 68/300, seasonal_0 Loss: 0.0939 | 0.1329
Epoch 69/300, seasonal_0 Loss: 0.0848 | 0.0862
Epoch 70/300, seasonal_0 Loss: 0.0849 | 0.0889
Epoch 71/300, seasonal_0 Loss: 0.0871 | 0.0787
Epoch 72/300, seasonal_0 Loss: 0.0948 | 0.1652
Epoch 73/300, seasonal_0 Loss: 0.0960 | 0.2024
Epoch 74/300, seasonal_0 Loss: 0.0888 | 0.0928
Epoch 75/300, seasonal_0 Loss: 0.0825 | 0.0964
Epoch 76/300, seasonal_0 Loss: 0.0883 | 0.1044
Epoch 77/300, seasonal_0 Loss: 0.0813 | 0.1799
Epoch 78/300, seasonal_0 Loss: 0.0866 | 0.1129
Epoch 79/300, seasonal_0 Loss: 0.0840 | 0.0861
Epoch 80/300, seasonal_0 Loss: 0.0817 | 0.1093
Epoch 81/300, seasonal_0 Loss: 0.0826 | 0.1572
Epoch 82/300, seasonal_0 Loss: 0.0858 | 0.1089
Epoch 83/300, seasonal_0 Loss: 0.0907 | 0.0907
Epoch 84/300, seasonal_0 Loss: 0.0790 | 0.1008
Epoch 85/300, seasonal_0 Loss: 0.0814 | 0.1591
Epoch 86/300, seasonal_0 Loss: 0.0845 | 0.2142
Epoch 87/300, seasonal_0 Loss: 0.0935 | 0.1309
Epoch 88/300, seasonal_0 Loss: 0.0867 | 0.0923
Epoch 89/300, seasonal_0 Loss: 0.0857 | 0.0858
Epoch 90/300, seasonal_0 Loss: 0.0885 | 0.1045
Epoch 91/300, seasonal_0 Loss: 0.0800 | 0.1246
Epoch 92/300, seasonal_0 Loss: 0.0853 | 0.1084
Epoch 93/300, seasonal_0 Loss: 0.0970 | 0.1018
Epoch 94/300, seasonal_0 Loss: 0.0940 | 0.1146
Epoch 95/300, seasonal_0 Loss: 0.0802 | 0.1447
Epoch 96/300, seasonal_0 Loss: 0.0860 | 0.1143
Epoch 97/300, seasonal_0 Loss: 0.0790 | 0.0840
Epoch 98/300, seasonal_0 Loss: 0.0763 | 0.0811
Epoch 99/300, seasonal_0 Loss: 0.0775 | 0.1274
Epoch 100/300, seasonal_0 Loss: 0.0832 | 0.1628
Epoch 101/300, seasonal_0 Loss: 0.0836 | 0.0973
Epoch 102/300, seasonal_0 Loss: 0.0878 | 0.1004
Epoch 103/300, seasonal_0 Loss: 0.0853 | 0.1054
Epoch 104/300, seasonal_0 Loss: 0.0807 | 0.1381
Epoch 105/300, seasonal_0 Loss: 0.0804 | 0.0962
Epoch 106/300, seasonal_0 Loss: 0.0843 | 0.0885
Epoch 107/300, seasonal_0 Loss: 0.0798 | 0.1155
Epoch 108/300, seasonal_0 Loss: 0.0774 | 0.1845
Epoch 109/300, seasonal_0 Loss: 0.0763 | 0.0928
Epoch 110/300, seasonal_0 Loss: 0.0828 | 0.0900
Epoch 111/300, seasonal_0 Loss: 0.0768 | 0.1101
Epoch 112/300, seasonal_0 Loss: 0.0793 | 0.1096
Epoch 113/300, seasonal_0 Loss: 0.0814 | 0.0892
Epoch 114/300, seasonal_0 Loss: 0.0747 | 0.0870
Epoch 115/300, seasonal_0 Loss: 0.0828 | 0.1295
Epoch 116/300, seasonal_0 Loss: 0.0776 | 0.1505
Epoch 117/300, seasonal_0 Loss: 0.0896 | 0.0981
Epoch 118/300, seasonal_0 Loss: 0.0926 | 0.0823
Epoch 119/300, seasonal_0 Loss: 0.0840 | 0.0821
Epoch 120/300, seasonal_0 Loss: 0.0790 | 0.1665
Epoch 121/300, seasonal_0 Loss: 0.0739 | 0.1180
Epoch 122/300, seasonal_0 Loss: 0.0757 | 0.1046
Epoch 123/300, seasonal_0 Loss: 0.0704 | 0.1427
Epoch 124/300, seasonal_0 Loss: 0.0674 | 0.1505
Epoch 125/300, seasonal_0 Loss: 0.0675 | 0.0987
Epoch 126/300, seasonal_0 Loss: 0.0665 | 0.1113
Epoch 127/300, seasonal_0 Loss: 0.0645 | 0.0977
Epoch 128/300, seasonal_0 Loss: 0.0639 | 0.0968
Epoch 129/300, seasonal_0 Loss: 0.0672 | 0.1105
Epoch 130/300, seasonal_0 Loss: 0.0685 | 0.1087
Epoch 131/300, seasonal_0 Loss: 0.0684 | 0.0948
Epoch 132/300, seasonal_0 Loss: 0.0721 | 0.1129
Epoch 133/300, seasonal_0 Loss: 0.0708 | 0.1336
Epoch 134/300, seasonal_0 Loss: 0.0728 | 0.1121
Epoch 135/300, seasonal_0 Loss: 0.0776 | 0.1144
Epoch 136/300, seasonal_0 Loss: 0.0726 | 0.1404
Epoch 137/300, seasonal_0 Loss: 0.0809 | 0.0990
Epoch 138/300, seasonal_0 Loss: 0.0793 | 0.1164
Epoch 139/300, seasonal_0 Loss: 0.0816 | 0.1404
Epoch 140/300, seasonal_0 Loss: 0.0776 | 0.1399
Epoch 141/300, seasonal_0 Loss: 0.0795 | 0.1054
Epoch 142/300, seasonal_0 Loss: 0.0795 | 0.1464
Epoch 143/300, seasonal_0 Loss: 0.0800 | 0.1227
Epoch 144/300, seasonal_0 Loss: 0.0716 | 0.1084
Epoch 145/300, seasonal_0 Loss: 0.0782 | 0.1133
Epoch 146/300, seasonal_0 Loss: 0.0658 | 0.0992
Epoch 147/300, seasonal_0 Loss: 0.0752 | 0.1121
Epoch 148/300, seasonal_0 Loss: 0.0737 | 0.1401
Epoch 149/300, seasonal_0 Loss: 0.0689 | 0.1249
Epoch 150/300, seasonal_0 Loss: 0.0744 | 0.1081
Epoch 151/300, seasonal_0 Loss: 0.0611 | 0.1014
Epoch 152/300, seasonal_0 Loss: 0.0691 | 0.1061
Epoch 153/300, seasonal_0 Loss: 0.0664 | 0.1215
Epoch 154/300, seasonal_0 Loss: 0.0681 | 0.1082
Epoch 155/300, seasonal_0 Loss: 0.0675 | 0.1027
Epoch 156/300, seasonal_0 Loss: 0.0716 | 0.1071
Epoch 157/300, seasonal_0 Loss: 0.0696 | 0.0951
Epoch 158/300, seasonal_0 Loss: 0.0683 | 0.1084
Epoch 159/300, seasonal_0 Loss: 0.0641 | 0.1161
Epoch 160/300, seasonal_0 Loss: 0.0664 | 0.1091
Epoch 161/300, seasonal_0 Loss: 0.0636 | 0.0911
Epoch 162/300, seasonal_0 Loss: 0.0614 | 0.1001
Epoch 163/300, seasonal_0 Loss: 0.0598 | 0.1298
Epoch 164/300, seasonal_0 Loss: 0.0639 | 0.1109
Epoch 165/300, seasonal_0 Loss: 0.0601 | 0.0947
Epoch 166/300, seasonal_0 Loss: 0.0591 | 0.1035
Epoch 167/300, seasonal_0 Loss: 0.0576 | 0.1111
Epoch 168/300, seasonal_0 Loss: 0.0583 | 0.0928
Epoch 169/300, seasonal_0 Loss: 0.0567 | 0.0916
Epoch 170/300, seasonal_0 Loss: 0.0570 | 0.1161
Epoch 171/300, seasonal_0 Loss: 0.0563 | 0.1255
Epoch 172/300, seasonal_0 Loss: 0.0575 | 0.0982
Epoch 173/300, seasonal_0 Loss: 0.0548 | 0.0995
Epoch 174/300, seasonal_0 Loss: 0.0553 | 0.1051
Epoch 175/300, seasonal_0 Loss: 0.0563 | 0.1015
Epoch 176/300, seasonal_0 Loss: 0.0551 | 0.0920
Epoch 177/300, seasonal_0 Loss: 0.0560 | 0.0959
Epoch 178/300, seasonal_0 Loss: 0.0560 | 0.1224
Epoch 179/300, seasonal_0 Loss: 0.0591 | 0.1326
Epoch 180/300, seasonal_0 Loss: 0.0612 | 0.1215
Epoch 181/300, seasonal_0 Loss: 0.0583 | 0.1086
Epoch 182/300, seasonal_0 Loss: 0.0572 | 0.1080
Epoch 183/300, seasonal_0 Loss: 0.0655 | 0.1103
Epoch 184/300, seasonal_0 Loss: 0.0604 | 0.0912
Epoch 185/300, seasonal_0 Loss: 0.0638 | 0.1017
Epoch 186/300, seasonal_0 Loss: 0.0636 | 0.1706
Epoch 187/300, seasonal_0 Loss: 0.0646 | 0.1114
Epoch 188/300, seasonal_0 Loss: 0.0594 | 0.1015
Epoch 189/300, seasonal_0 Loss: 0.0563 | 0.1308
Epoch 190/300, seasonal_0 Loss: 0.0553 | 0.1350
Epoch 191/300, seasonal_0 Loss: 0.0573 | 0.1114
Epoch 192/300, seasonal_0 Loss: 0.0539 | 0.1000
Epoch 193/300, seasonal_0 Loss: 0.0543 | 0.1163
Epoch 194/300, seasonal_0 Loss: 0.0554 | 0.1036
Epoch 195/300, seasonal_0 Loss: 0.0519 | 0.0987
Epoch 196/300, seasonal_0 Loss: 0.0555 | 0.1295
Epoch 197/300, seasonal_0 Loss: 0.0529 | 0.1134
Epoch 198/300, seasonal_0 Loss: 0.0560 | 0.1061
Epoch 199/300, seasonal_0 Loss: 0.0600 | 0.0967
Epoch 200/300, seasonal_0 Loss: 0.0557 | 0.1413
Epoch 201/300, seasonal_0 Loss: 0.0544 | 0.1200
Epoch 202/300, seasonal_0 Loss: 0.0539 | 0.1106
Epoch 203/300, seasonal_0 Loss: 0.0548 | 0.1136
Epoch 204/300, seasonal_0 Loss: 0.0569 | 0.1192
Epoch 205/300, seasonal_0 Loss: 0.0580 | 0.1104
Epoch 206/300, seasonal_0 Loss: 0.0653 | 0.1050
Epoch 207/300, seasonal_0 Loss: 0.0623 | 0.0928
Epoch 208/300, seasonal_0 Loss: 0.0590 | 0.1181
Epoch 209/300, seasonal_0 Loss: 0.0581 | 0.1599
Epoch 210/300, seasonal_0 Loss: 0.0567 | 0.1068
Epoch 211/300, seasonal_0 Loss: 0.0580 | 0.1251
Epoch 212/300, seasonal_0 Loss: 0.0531 | 0.1244
Epoch 213/300, seasonal_0 Loss: 0.0516 | 0.1008
Epoch 214/300, seasonal_0 Loss: 0.0517 | 0.1122
Epoch 215/300, seasonal_0 Loss: 0.0504 | 0.1135
Epoch 216/300, seasonal_0 Loss: 0.0525 | 0.1082
Epoch 217/300, seasonal_0 Loss: 0.0533 | 0.1118
Epoch 218/300, seasonal_0 Loss: 0.0560 | 0.1219
Epoch 219/300, seasonal_0 Loss: 0.0565 | 0.1161
Epoch 220/300, seasonal_0 Loss: 0.0608 | 0.1000
Epoch 221/300, seasonal_0 Loss: 0.0572 | 0.0896
Epoch 222/300, seasonal_0 Loss: 0.0569 | 0.1329
Epoch 223/300, seasonal_0 Loss: 0.0559 | 0.1180
Epoch 224/300, seasonal_0 Loss: 0.0579 | 0.1185
Epoch 225/300, seasonal_0 Loss: 0.0639 | 0.1053
Epoch 226/300, seasonal_0 Loss: 0.0679 | 0.1313
Epoch 227/300, seasonal_0 Loss: 0.0613 | 0.1479
Epoch 228/300, seasonal_0 Loss: 0.0595 | 0.1028
Epoch 229/300, seasonal_0 Loss: 0.0655 | 0.1179
Epoch 230/300, seasonal_0 Loss: 0.0611 | 0.1136
Epoch 231/300, seasonal_0 Loss: 0.0592 | 0.1112
Epoch 232/300, seasonal_0 Loss: 0.0646 | 0.0994
Epoch 233/300, seasonal_0 Loss: 0.0574 | 0.1198
Epoch 234/300, seasonal_0 Loss: 0.0548 | 0.1213
Epoch 235/300, seasonal_0 Loss: 0.0559 | 0.1171
Epoch 236/300, seasonal_0 Loss: 0.0524 | 0.1060
Epoch 237/300, seasonal_0 Loss: 0.0515 | 0.1245
Epoch 238/300, seasonal_0 Loss: 0.0570 | 0.1102
Epoch 239/300, seasonal_0 Loss: 0.0547 | 0.1044
Epoch 240/300, seasonal_0 Loss: 0.0553 | 0.1103
Epoch 241/300, seasonal_0 Loss: 0.0548 | 0.1078
Epoch 242/300, seasonal_0 Loss: 0.0532 | 0.1068
Epoch 243/300, seasonal_0 Loss: 0.0611 | 0.1150
Epoch 244/300, seasonal_0 Loss: 0.0577 | 0.1128
Epoch 245/300, seasonal_0 Loss: 0.0519 | 0.1186
Epoch 246/300, seasonal_0 Loss: 0.0545 | 0.1190
Epoch 247/300, seasonal_0 Loss: 0.0540 | 0.1005
Epoch 248/300, seasonal_0 Loss: 0.0547 | 0.1183
Epoch 249/300, seasonal_0 Loss: 0.0493 | 0.1164
Epoch 250/300, seasonal_0 Loss: 0.0510 | 0.1092
Epoch 251/300, seasonal_0 Loss: 0.0551 | 0.1102
Epoch 252/300, seasonal_0 Loss: 0.0459 | 0.1126
Epoch 253/300, seasonal_0 Loss: 0.0504 | 0.1092
Epoch 254/300, seasonal_0 Loss: 0.0485 | 0.1198
Epoch 255/300, seasonal_0 Loss: 0.0479 | 0.1158
Epoch 256/300, seasonal_0 Loss: 0.0433 | 0.1056
Epoch 257/300, seasonal_0 Loss: 0.0463 | 0.1105
Epoch 258/300, seasonal_0 Loss: 0.0455 | 0.1180
Epoch 259/300, seasonal_0 Loss: 0.0447 | 0.1303
Epoch 260/300, seasonal_0 Loss: 0.0421 | 0.1078
Epoch 261/300, seasonal_0 Loss: 0.0400 | 0.1076
Epoch 262/300, seasonal_0 Loss: 0.0411 | 0.1099
Epoch 263/300, seasonal_0 Loss: 0.0398 | 0.1106
Epoch 264/300, seasonal_0 Loss: 0.0410 | 0.1145
Epoch 265/300, seasonal_0 Loss: 0.0429 | 0.1157
Epoch 266/300, seasonal_0 Loss: 0.0419 | 0.1156
Epoch 267/300, seasonal_0 Loss: 0.0440 | 0.1115
Epoch 268/300, seasonal_0 Loss: 0.0433 | 0.1122
Epoch 269/300, seasonal_0 Loss: 0.0461 | 0.1216
Epoch 270/300, seasonal_0 Loss: 0.0451 | 0.1187
Epoch 271/300, seasonal_0 Loss: 0.0432 | 0.1123
Epoch 272/300, seasonal_0 Loss: 0.0450 | 0.1164
Epoch 273/300, seasonal_0 Loss: 0.0428 | 0.1116
Epoch 274/300, seasonal_0 Loss: 0.0480 | 0.1236
Epoch 275/300, seasonal_0 Loss: 0.0462 | 0.1294
Epoch 276/300, seasonal_0 Loss: 0.0485 | 0.1140
Epoch 277/300, seasonal_0 Loss: 0.0442 | 0.1116
Epoch 278/300, seasonal_0 Loss: 0.0469 | 0.1150
Epoch 279/300, seasonal_0 Loss: 0.0488 | 0.1253
Epoch 280/300, seasonal_0 Loss: 0.0446 | 0.1144
Epoch 281/300, seasonal_0 Loss: 0.0464 | 0.1123
Epoch 282/300, seasonal_0 Loss: 0.0406 | 0.1083
Epoch 283/300, seasonal_0 Loss: 0.0430 | 0.1070
Epoch 284/300, seasonal_0 Loss: 0.0390 | 0.1250
Epoch 285/300, seasonal_0 Loss: 0.0412 | 0.1122
Epoch 286/300, seasonal_0 Loss: 0.0392 | 0.1090
Epoch 287/300, seasonal_0 Loss: 0.0408 | 0.1127
Epoch 288/300, seasonal_0 Loss: 0.0398 | 0.1226
Epoch 289/300, seasonal_0 Loss: 0.0397 | 0.1122
Epoch 290/300, seasonal_0 Loss: 0.0407 | 0.1099
Epoch 291/300, seasonal_0 Loss: 0.0392 | 0.1097
Epoch 292/300, seasonal_0 Loss: 0.0409 | 0.1084
Epoch 293/300, seasonal_0 Loss: 0.0398 | 0.1219
Epoch 294/300, seasonal_0 Loss: 0.0397 | 0.1182
Epoch 295/300, seasonal_0 Loss: 0.0386 | 0.1113
Epoch 296/300, seasonal_0 Loss: 0.0392 | 0.1204
Epoch 297/300, seasonal_0 Loss: 0.0393 | 0.1159
Epoch 298/300, seasonal_0 Loss: 0.0393 | 0.1109
Epoch 299/300, seasonal_0 Loss: 0.0372 | 0.1118
Epoch 300/300, seasonal_0 Loss: 0.0387 | 0.1157
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.9898185095416109, 'learning_rate': 0.00042666169647768927, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9056269493834028}
Epoch 1/300, seasonal_1 Loss: 0.3011 | 0.1524
Epoch 2/300, seasonal_1 Loss: 0.1674 | 0.1089
Epoch 3/300, seasonal_1 Loss: 0.1635 | 0.0988
Epoch 4/300, seasonal_1 Loss: 0.1845 | 0.1009
Epoch 5/300, seasonal_1 Loss: 0.1934 | 0.1956
Epoch 6/300, seasonal_1 Loss: 0.2417 | 0.3453
Epoch 7/300, seasonal_1 Loss: 0.1800 | 0.1049
Epoch 8/300, seasonal_1 Loss: 0.1407 | 0.0853
Epoch 9/300, seasonal_1 Loss: 0.1385 | 0.0818
Epoch 10/300, seasonal_1 Loss: 0.1273 | 0.0766
Epoch 11/300, seasonal_1 Loss: 0.1191 | 0.0752
Epoch 12/300, seasonal_1 Loss: 0.1217 | 0.0758
Epoch 13/300, seasonal_1 Loss: 0.1175 | 0.0729
Epoch 14/300, seasonal_1 Loss: 0.1129 | 0.0718
Epoch 15/300, seasonal_1 Loss: 0.1118 | 0.0720
Epoch 16/300, seasonal_1 Loss: 0.1103 | 0.0717
Epoch 17/300, seasonal_1 Loss: 0.1097 | 0.0724
Epoch 18/300, seasonal_1 Loss: 0.1086 | 0.0723
Epoch 19/300, seasonal_1 Loss: 0.1072 | 0.0721
Epoch 20/300, seasonal_1 Loss: 0.1062 | 0.0714
Epoch 21/300, seasonal_1 Loss: 0.1053 | 0.0710
Epoch 22/300, seasonal_1 Loss: 0.1045 | 0.0706
Epoch 23/300, seasonal_1 Loss: 0.1038 | 0.0701
Epoch 24/300, seasonal_1 Loss: 0.1031 | 0.0695
Epoch 25/300, seasonal_1 Loss: 0.1024 | 0.0688
Epoch 26/300, seasonal_1 Loss: 0.1018 | 0.0681
Epoch 27/300, seasonal_1 Loss: 0.1011 | 0.0674
Epoch 28/300, seasonal_1 Loss: 0.1005 | 0.0667
Epoch 29/300, seasonal_1 Loss: 0.0998 | 0.0659
Epoch 30/300, seasonal_1 Loss: 0.0992 | 0.0651
Epoch 31/300, seasonal_1 Loss: 0.0986 | 0.0644
Epoch 32/300, seasonal_1 Loss: 0.0980 | 0.0636
Epoch 33/300, seasonal_1 Loss: 0.0974 | 0.0630
Epoch 34/300, seasonal_1 Loss: 0.0968 | 0.0623
Epoch 35/300, seasonal_1 Loss: 0.0962 | 0.0617
Epoch 36/300, seasonal_1 Loss: 0.0957 | 0.0612
Epoch 37/300, seasonal_1 Loss: 0.0952 | 0.0607
Epoch 38/300, seasonal_1 Loss: 0.0948 | 0.0603
Epoch 39/300, seasonal_1 Loss: 0.0944 | 0.0599
Epoch 40/300, seasonal_1 Loss: 0.0940 | 0.0596
Epoch 41/300, seasonal_1 Loss: 0.0937 | 0.0593
Epoch 42/300, seasonal_1 Loss: 0.0933 | 0.0590
Epoch 43/300, seasonal_1 Loss: 0.0930 | 0.0588
Epoch 44/300, seasonal_1 Loss: 0.0928 | 0.0585
Epoch 45/300, seasonal_1 Loss: 0.0925 | 0.0583
Epoch 46/300, seasonal_1 Loss: 0.0923 | 0.0580
Epoch 47/300, seasonal_1 Loss: 0.0920 | 0.0578
Epoch 48/300, seasonal_1 Loss: 0.0918 | 0.0576
Epoch 49/300, seasonal_1 Loss: 0.0916 | 0.0574
Epoch 50/300, seasonal_1 Loss: 0.0914 | 0.0572
Epoch 51/300, seasonal_1 Loss: 0.0912 | 0.0571
Epoch 52/300, seasonal_1 Loss: 0.0911 | 0.0569
Epoch 53/300, seasonal_1 Loss: 0.0909 | 0.0567
Epoch 54/300, seasonal_1 Loss: 0.0907 | 0.0566
Epoch 55/300, seasonal_1 Loss: 0.0906 | 0.0564
Epoch 56/300, seasonal_1 Loss: 0.0904 | 0.0563
Epoch 57/300, seasonal_1 Loss: 0.0903 | 0.0561
Epoch 58/300, seasonal_1 Loss: 0.0902 | 0.0560
Epoch 59/300, seasonal_1 Loss: 0.0900 | 0.0559
Epoch 60/300, seasonal_1 Loss: 0.0899 | 0.0558
Epoch 61/300, seasonal_1 Loss: 0.0898 | 0.0557
Epoch 62/300, seasonal_1 Loss: 0.0897 | 0.0555
Epoch 63/300, seasonal_1 Loss: 0.0896 | 0.0554
Epoch 64/300, seasonal_1 Loss: 0.0895 | 0.0553
Epoch 65/300, seasonal_1 Loss: 0.0894 | 0.0552
Epoch 66/300, seasonal_1 Loss: 0.0893 | 0.0552
Epoch 67/300, seasonal_1 Loss: 0.0892 | 0.0551
Epoch 68/300, seasonal_1 Loss: 0.0891 | 0.0550
Epoch 69/300, seasonal_1 Loss: 0.0890 | 0.0549
Epoch 70/300, seasonal_1 Loss: 0.0890 | 0.0548
Epoch 71/300, seasonal_1 Loss: 0.0889 | 0.0548
Epoch 72/300, seasonal_1 Loss: 0.0888 | 0.0547
Epoch 73/300, seasonal_1 Loss: 0.0888 | 0.0546
Epoch 74/300, seasonal_1 Loss: 0.0887 | 0.0546
Epoch 75/300, seasonal_1 Loss: 0.0886 | 0.0545
Epoch 76/300, seasonal_1 Loss: 0.0886 | 0.0545
Epoch 77/300, seasonal_1 Loss: 0.0885 | 0.0544
Epoch 78/300, seasonal_1 Loss: 0.0885 | 0.0544
Epoch 79/300, seasonal_1 Loss: 0.0884 | 0.0543
Epoch 80/300, seasonal_1 Loss: 0.0884 | 0.0543
Epoch 81/300, seasonal_1 Loss: 0.0883 | 0.0542
Epoch 82/300, seasonal_1 Loss: 0.0883 | 0.0542
Epoch 83/300, seasonal_1 Loss: 0.0882 | 0.0541
Epoch 84/300, seasonal_1 Loss: 0.0882 | 0.0541
Epoch 85/300, seasonal_1 Loss: 0.0882 | 0.0541
Epoch 86/300, seasonal_1 Loss: 0.0881 | 0.0540
Epoch 87/300, seasonal_1 Loss: 0.0881 | 0.0540
Epoch 88/300, seasonal_1 Loss: 0.0881 | 0.0540
Epoch 89/300, seasonal_1 Loss: 0.0880 | 0.0539
Epoch 90/300, seasonal_1 Loss: 0.0880 | 0.0539
Epoch 91/300, seasonal_1 Loss: 0.0880 | 0.0539
Epoch 92/300, seasonal_1 Loss: 0.0880 | 0.0539
Epoch 93/300, seasonal_1 Loss: 0.0879 | 0.0538
Epoch 94/300, seasonal_1 Loss: 0.0879 | 0.0538
Epoch 95/300, seasonal_1 Loss: 0.0879 | 0.0538
Epoch 96/300, seasonal_1 Loss: 0.0879 | 0.0538
Epoch 97/300, seasonal_1 Loss: 0.0878 | 0.0537
Epoch 98/300, seasonal_1 Loss: 0.0878 | 0.0537
Epoch 99/300, seasonal_1 Loss: 0.0878 | 0.0537
Epoch 100/300, seasonal_1 Loss: 0.0878 | 0.0537
Epoch 101/300, seasonal_1 Loss: 0.0878 | 0.0537
Epoch 102/300, seasonal_1 Loss: 0.0878 | 0.0536
Epoch 103/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 104/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 105/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 106/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 107/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 108/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 109/300, seasonal_1 Loss: 0.0877 | 0.0536
Epoch 110/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 111/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 112/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 113/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 114/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 115/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 116/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 117/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 118/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 119/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 120/300, seasonal_1 Loss: 0.0876 | 0.0535
Epoch 121/300, seasonal_1 Loss: 0.0875 | 0.0535
Epoch 122/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 123/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 124/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 125/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 126/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 127/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 128/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 129/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 130/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 131/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 132/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 133/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 134/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 135/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 136/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 137/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 138/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 139/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 140/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 141/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 142/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 143/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 144/300, seasonal_1 Loss: 0.0875 | 0.0534
Epoch 145/300, seasonal_1 Loss: 0.0874 | 0.0534
Epoch 146/300, seasonal_1 Loss: 0.0874 | 0.0534
Epoch 147/300, seasonal_1 Loss: 0.0874 | 0.0534
Epoch 148/300, seasonal_1 Loss: 0.0874 | 0.0534
Epoch 149/300, seasonal_1 Loss: 0.0874 | 0.0534
Epoch 150/300, seasonal_1 Loss: 0.0874 | 0.0534
Epoch 151/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 152/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 153/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 154/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 155/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 156/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 157/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 158/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 159/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 160/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 161/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 162/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 163/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 164/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 165/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 166/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 167/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 168/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 169/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 170/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 171/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 172/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 173/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 174/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 175/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 176/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 177/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 178/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 179/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 180/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 181/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 182/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 183/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 184/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 185/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 186/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 187/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 188/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 189/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 190/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 191/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 192/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 193/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 194/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 195/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 196/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 197/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 198/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 199/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 200/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 201/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 202/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 203/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 204/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 205/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 206/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 207/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 208/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 209/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 210/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 211/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 212/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 213/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 214/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 215/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 216/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 217/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 218/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 219/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 220/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 221/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 222/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 223/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 224/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 225/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 226/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 227/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 228/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 229/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 230/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 231/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 232/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 233/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 234/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 235/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 236/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 237/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 238/300, seasonal_1 Loss: 0.0874 | 0.0533
Epoch 239/300, seasonal_1 Loss: 0.0874 | 0.0533
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 26, 'train_rates': 0.9870510727437779, 'learning_rate': 0.00017446831351692845, 'batch_size': 190, 'step_size': 13, 'gamma': 0.793926945329656}
Epoch 1/300, seasonal_2 Loss: 0.8101 | 0.5826
Epoch 2/300, seasonal_2 Loss: 0.5957 | 0.4360
Epoch 3/300, seasonal_2 Loss: 0.4078 | 0.3681
Epoch 4/300, seasonal_2 Loss: 0.3555 | 0.2768
Epoch 5/300, seasonal_2 Loss: 0.2856 | 0.2401
Epoch 6/300, seasonal_2 Loss: 0.2705 | 0.2150
Epoch 7/300, seasonal_2 Loss: 0.2389 | 0.1830
Epoch 8/300, seasonal_2 Loss: 0.2280 | 0.1766
Epoch 9/300, seasonal_2 Loss: 0.2061 | 0.1542
Epoch 10/300, seasonal_2 Loss: 0.1989 | 0.1548
Epoch 11/300, seasonal_2 Loss: 0.1861 | 0.1343
Epoch 12/300, seasonal_2 Loss: 0.1801 | 0.1397
Epoch 13/300, seasonal_2 Loss: 0.1709 | 0.1209
Epoch 14/300, seasonal_2 Loss: 0.1637 | 0.1195
Epoch 15/300, seasonal_2 Loss: 0.1584 | 0.1122
Epoch 16/300, seasonal_2 Loss: 0.1542 | 0.1085
Epoch 17/300, seasonal_2 Loss: 0.1512 | 0.1081
Epoch 18/300, seasonal_2 Loss: 0.1479 | 0.1013
Epoch 19/300, seasonal_2 Loss: 0.1448 | 0.1020
Epoch 20/300, seasonal_2 Loss: 0.1420 | 0.0975
Epoch 21/300, seasonal_2 Loss: 0.1395 | 0.0928
Epoch 22/300, seasonal_2 Loss: 0.1372 | 0.0924
Epoch 23/300, seasonal_2 Loss: 0.1352 | 0.0888
Epoch 24/300, seasonal_2 Loss: 0.1336 | 0.0879
Epoch 25/300, seasonal_2 Loss: 0.1322 | 0.0861
Epoch 26/300, seasonal_2 Loss: 0.1304 | 0.0841
Epoch 27/300, seasonal_2 Loss: 0.1285 | 0.0810
Epoch 28/300, seasonal_2 Loss: 0.1273 | 0.0797
Epoch 29/300, seasonal_2 Loss: 0.1263 | 0.0787
Epoch 30/300, seasonal_2 Loss: 0.1253 | 0.0774
Epoch 31/300, seasonal_2 Loss: 0.1241 | 0.0761
Epoch 32/300, seasonal_2 Loss: 0.1231 | 0.0748
Epoch 33/300, seasonal_2 Loss: 0.1223 | 0.0737
Epoch 34/300, seasonal_2 Loss: 0.1214 | 0.0722
Epoch 35/300, seasonal_2 Loss: 0.1204 | 0.0711
Epoch 36/300, seasonal_2 Loss: 0.1198 | 0.0703
Epoch 37/300, seasonal_2 Loss: 0.1193 | 0.0698
Epoch 38/300, seasonal_2 Loss: 0.1186 | 0.0689
Epoch 39/300, seasonal_2 Loss: 0.1179 | 0.0680
Epoch 40/300, seasonal_2 Loss: 0.1173 | 0.0671
Epoch 41/300, seasonal_2 Loss: 0.1168 | 0.0668
Epoch 42/300, seasonal_2 Loss: 0.1163 | 0.0660
Epoch 43/300, seasonal_2 Loss: 0.1159 | 0.0656
Epoch 44/300, seasonal_2 Loss: 0.1154 | 0.0650
Epoch 45/300, seasonal_2 Loss: 0.1150 | 0.0645
Epoch 46/300, seasonal_2 Loss: 0.1145 | 0.0641
Epoch 47/300, seasonal_2 Loss: 0.1141 | 0.0635
Epoch 48/300, seasonal_2 Loss: 0.1137 | 0.0633
Epoch 49/300, seasonal_2 Loss: 0.1134 | 0.0628
Epoch 50/300, seasonal_2 Loss: 0.1131 | 0.0626
Epoch 51/300, seasonal_2 Loss: 0.1127 | 0.0621
Epoch 52/300, seasonal_2 Loss: 0.1124 | 0.0619
Epoch 53/300, seasonal_2 Loss: 0.1121 | 0.0615
Epoch 54/300, seasonal_2 Loss: 0.1118 | 0.0613
Epoch 55/300, seasonal_2 Loss: 0.1116 | 0.0610
Epoch 56/300, seasonal_2 Loss: 0.1113 | 0.0609
Epoch 57/300, seasonal_2 Loss: 0.1111 | 0.0605
Epoch 58/300, seasonal_2 Loss: 0.1108 | 0.0604
Epoch 59/300, seasonal_2 Loss: 0.1106 | 0.0601
Epoch 60/300, seasonal_2 Loss: 0.1103 | 0.0600
Epoch 61/300, seasonal_2 Loss: 0.1101 | 0.0597
Epoch 62/300, seasonal_2 Loss: 0.1099 | 0.0597
Epoch 63/300, seasonal_2 Loss: 0.1098 | 0.0594
Epoch 64/300, seasonal_2 Loss: 0.1096 | 0.0594
Epoch 65/300, seasonal_2 Loss: 0.1094 | 0.0590
Epoch 66/300, seasonal_2 Loss: 0.1092 | 0.0591
Epoch 67/300, seasonal_2 Loss: 0.1091 | 0.0587
Epoch 68/300, seasonal_2 Loss: 0.1089 | 0.0590
Epoch 69/300, seasonal_2 Loss: 0.1089 | 0.0584
Epoch 70/300, seasonal_2 Loss: 0.1090 | 0.0593
Epoch 71/300, seasonal_2 Loss: 0.1094 | 0.0584
Epoch 72/300, seasonal_2 Loss: 0.1102 | 0.0610
Epoch 73/300, seasonal_2 Loss: 0.1120 | 0.0582
Epoch 74/300, seasonal_2 Loss: 0.1147 | 0.0588
Epoch 75/300, seasonal_2 Loss: 0.1207 | 0.0642
Epoch 76/300, seasonal_2 Loss: 0.1217 | 0.0589
Epoch 77/300, seasonal_2 Loss: 0.1230 | 0.0784
Epoch 78/300, seasonal_2 Loss: 0.1149 | 0.0605
Epoch 79/300, seasonal_2 Loss: 0.1100 | 0.0638
Epoch 80/300, seasonal_2 Loss: 0.1079 | 0.0579
Epoch 81/300, seasonal_2 Loss: 0.1075 | 0.0581
Epoch 82/300, seasonal_2 Loss: 0.1074 | 0.0579
Epoch 83/300, seasonal_2 Loss: 0.1073 | 0.0577
Epoch 84/300, seasonal_2 Loss: 0.1072 | 0.0576
Epoch 85/300, seasonal_2 Loss: 0.1071 | 0.0575
Epoch 86/300, seasonal_2 Loss: 0.1070 | 0.0575
Epoch 87/300, seasonal_2 Loss: 0.1070 | 0.0574
Epoch 88/300, seasonal_2 Loss: 0.1069 | 0.0574
Epoch 89/300, seasonal_2 Loss: 0.1069 | 0.0573
Epoch 90/300, seasonal_2 Loss: 0.1068 | 0.0573
Epoch 91/300, seasonal_2 Loss: 0.1067 | 0.0572
Epoch 92/300, seasonal_2 Loss: 0.1067 | 0.0572
Epoch 93/300, seasonal_2 Loss: 0.1066 | 0.0571
Epoch 94/300, seasonal_2 Loss: 0.1066 | 0.0571
Epoch 95/300, seasonal_2 Loss: 0.1065 | 0.0571
Epoch 96/300, seasonal_2 Loss: 0.1065 | 0.0570
Epoch 97/300, seasonal_2 Loss: 0.1064 | 0.0570
Epoch 98/300, seasonal_2 Loss: 0.1064 | 0.0569
Epoch 99/300, seasonal_2 Loss: 0.1063 | 0.0569
Epoch 100/300, seasonal_2 Loss: 0.1063 | 0.0569
Epoch 101/300, seasonal_2 Loss: 0.1062 | 0.0569
Epoch 102/300, seasonal_2 Loss: 0.1062 | 0.0568
Epoch 103/300, seasonal_2 Loss: 0.1061 | 0.0568
Epoch 104/300, seasonal_2 Loss: 0.1061 | 0.0568
Epoch 105/300, seasonal_2 Loss: 0.1061 | 0.0568
Epoch 106/300, seasonal_2 Loss: 0.1060 | 0.0567
Epoch 107/300, seasonal_2 Loss: 0.1060 | 0.0567
Epoch 108/300, seasonal_2 Loss: 0.1060 | 0.0567
Epoch 109/300, seasonal_2 Loss: 0.1059 | 0.0567
Epoch 110/300, seasonal_2 Loss: 0.1059 | 0.0567
Epoch 111/300, seasonal_2 Loss: 0.1059 | 0.0566
Epoch 112/300, seasonal_2 Loss: 0.1058 | 0.0566
Epoch 113/300, seasonal_2 Loss: 0.1058 | 0.0566
Epoch 114/300, seasonal_2 Loss: 0.1058 | 0.0566
Epoch 115/300, seasonal_2 Loss: 0.1057 | 0.0566
Epoch 116/300, seasonal_2 Loss: 0.1057 | 0.0565
Epoch 117/300, seasonal_2 Loss: 0.1057 | 0.0565
Epoch 118/300, seasonal_2 Loss: 0.1057 | 0.0565
Epoch 119/300, seasonal_2 Loss: 0.1056 | 0.0565
Epoch 120/300, seasonal_2 Loss: 0.1056 | 0.0565
Epoch 121/300, seasonal_2 Loss: 0.1056 | 0.0565
Epoch 122/300, seasonal_2 Loss: 0.1056 | 0.0565
Epoch 123/300, seasonal_2 Loss: 0.1055 | 0.0564
Epoch 124/300, seasonal_2 Loss: 0.1055 | 0.0564
Epoch 125/300, seasonal_2 Loss: 0.1055 | 0.0564
Epoch 126/300, seasonal_2 Loss: 0.1055 | 0.0564
Epoch 127/300, seasonal_2 Loss: 0.1055 | 0.0564
Epoch 128/300, seasonal_2 Loss: 0.1054 | 0.0564
Epoch 129/300, seasonal_2 Loss: 0.1054 | 0.0564
Epoch 130/300, seasonal_2 Loss: 0.1054 | 0.0564
Epoch 131/300, seasonal_2 Loss: 0.1054 | 0.0564
Epoch 132/300, seasonal_2 Loss: 0.1054 | 0.0563
Epoch 133/300, seasonal_2 Loss: 0.1054 | 0.0563
Epoch 134/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 135/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 136/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 137/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 138/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 139/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 140/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 141/300, seasonal_2 Loss: 0.1053 | 0.0563
Epoch 142/300, seasonal_2 Loss: 0.1052 | 0.0563
Epoch 143/300, seasonal_2 Loss: 0.1052 | 0.0563
Epoch 144/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 145/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 146/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 147/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 148/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 149/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 150/300, seasonal_2 Loss: 0.1052 | 0.0562
Epoch 151/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 152/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 153/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 154/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 155/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 156/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 157/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 158/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 159/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 160/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 161/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 162/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 163/300, seasonal_2 Loss: 0.1051 | 0.0562
Epoch 164/300, seasonal_2 Loss: 0.1051 | 0.0561
Epoch 165/300, seasonal_2 Loss: 0.1051 | 0.0561
Epoch 166/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 167/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 168/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 169/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 170/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 171/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 172/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 173/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 174/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 175/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 176/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 177/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 178/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 179/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 180/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 181/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 182/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 183/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 184/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 185/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 186/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 187/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 188/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 189/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 190/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 191/300, seasonal_2 Loss: 0.1050 | 0.0561
Epoch 192/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 193/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 194/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 195/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 196/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 197/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 198/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 199/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 200/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 201/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 202/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 203/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 204/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 205/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 206/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 207/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 208/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 209/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 210/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 211/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 212/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 213/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 214/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 215/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 216/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 217/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 218/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 219/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 220/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 221/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 222/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 223/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 224/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 225/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 226/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 227/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 228/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 229/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 230/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 231/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 232/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 233/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 234/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 235/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 236/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 237/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 238/300, seasonal_2 Loss: 0.1049 | 0.0561
Epoch 239/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 240/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 241/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 242/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 243/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 244/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 245/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 246/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 247/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 248/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 249/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 250/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 251/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 252/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 253/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 254/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 255/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 256/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 257/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 258/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 259/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 260/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 261/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 262/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 263/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 264/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 265/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 266/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 267/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 268/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 269/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 270/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 271/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 272/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 273/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 274/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 275/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 276/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 277/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 278/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 279/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 280/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 281/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 282/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 283/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 284/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 285/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 286/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 287/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 288/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 289/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 290/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 291/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 292/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 293/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 294/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 295/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 296/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 297/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 298/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 299/300, seasonal_2 Loss: 0.1049 | 0.0560
Epoch 300/300, seasonal_2 Loss: 0.1049 | 0.0560
Training seasonal_3 component with params: {'observation_period_num': 35, 'train_rates': 0.9842007644580733, 'learning_rate': 0.00033251831032581644, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8429696349946083}
Epoch 1/300, seasonal_3 Loss: 0.6255 | 0.2804
Epoch 2/300, seasonal_3 Loss: 0.2965 | 0.1896
Epoch 3/300, seasonal_3 Loss: 0.2478 | 0.1530
Epoch 4/300, seasonal_3 Loss: 0.2114 | 0.1309
Epoch 5/300, seasonal_3 Loss: 0.1985 | 0.1126
Epoch 6/300, seasonal_3 Loss: 0.1857 | 0.1045
Epoch 7/300, seasonal_3 Loss: 0.1738 | 0.0977
Epoch 8/300, seasonal_3 Loss: 0.1628 | 0.1005
Epoch 9/300, seasonal_3 Loss: 0.1607 | 0.0866
Epoch 10/300, seasonal_3 Loss: 0.1724 | 0.0920
Epoch 11/300, seasonal_3 Loss: 0.1741 | 0.2141
Epoch 12/300, seasonal_3 Loss: 0.1756 | 0.3452
Epoch 13/300, seasonal_3 Loss: 0.1579 | 0.2189
Epoch 14/300, seasonal_3 Loss: 0.1529 | 0.1213
Epoch 15/300, seasonal_3 Loss: 0.1387 | 0.1152
Epoch 16/300, seasonal_3 Loss: 0.1317 | 0.1176
Epoch 17/300, seasonal_3 Loss: 0.1255 | 0.1041
Epoch 18/300, seasonal_3 Loss: 0.1203 | 0.0913
Epoch 19/300, seasonal_3 Loss: 0.1144 | 0.0817
Epoch 20/300, seasonal_3 Loss: 0.1094 | 0.0726
Epoch 21/300, seasonal_3 Loss: 0.1060 | 0.0651
Epoch 22/300, seasonal_3 Loss: 0.1043 | 0.0610
Epoch 23/300, seasonal_3 Loss: 0.1028 | 0.0581
Epoch 24/300, seasonal_3 Loss: 0.1014 | 0.0560
Epoch 25/300, seasonal_3 Loss: 0.1001 | 0.0554
Epoch 26/300, seasonal_3 Loss: 0.0990 | 0.0543
Epoch 27/300, seasonal_3 Loss: 0.0983 | 0.0534
Epoch 28/300, seasonal_3 Loss: 0.0977 | 0.0526
Epoch 29/300, seasonal_3 Loss: 0.0972 | 0.0524
Epoch 30/300, seasonal_3 Loss: 0.0970 | 0.0518
Epoch 31/300, seasonal_3 Loss: 0.0972 | 0.0515
Epoch 32/300, seasonal_3 Loss: 0.0970 | 0.0515
Epoch 33/300, seasonal_3 Loss: 0.0969 | 0.0508
Epoch 34/300, seasonal_3 Loss: 0.0967 | 0.0499
Epoch 35/300, seasonal_3 Loss: 0.0971 | 0.0492
Epoch 36/300, seasonal_3 Loss: 0.0988 | 0.0580
Epoch 37/300, seasonal_3 Loss: 0.0995 | 0.0759
Epoch 38/300, seasonal_3 Loss: 0.0986 | 0.0634
Epoch 39/300, seasonal_3 Loss: 0.0976 | 0.0535
Epoch 40/300, seasonal_3 Loss: 0.0978 | 0.0499
Epoch 41/300, seasonal_3 Loss: 0.0987 | 0.0661
Epoch 42/300, seasonal_3 Loss: 0.0996 | 0.0772
Epoch 43/300, seasonal_3 Loss: 0.0960 | 0.0678
Epoch 44/300, seasonal_3 Loss: 0.0929 | 0.0568
Epoch 45/300, seasonal_3 Loss: 0.0920 | 0.0493
Epoch 46/300, seasonal_3 Loss: 0.0922 | 0.0472
Epoch 47/300, seasonal_3 Loss: 0.0915 | 0.0468
Epoch 48/300, seasonal_3 Loss: 0.0905 | 0.0468
Epoch 49/300, seasonal_3 Loss: 0.0894 | 0.0467
Epoch 50/300, seasonal_3 Loss: 0.0886 | 0.0463
Epoch 51/300, seasonal_3 Loss: 0.0883 | 0.0463
Epoch 52/300, seasonal_3 Loss: 0.0883 | 0.0466
Epoch 53/300, seasonal_3 Loss: 0.0880 | 0.0470
Epoch 54/300, seasonal_3 Loss: 0.0875 | 0.0466
Epoch 55/300, seasonal_3 Loss: 0.0871 | 0.0462
Epoch 56/300, seasonal_3 Loss: 0.0869 | 0.0459
Epoch 57/300, seasonal_3 Loss: 0.0867 | 0.0457
Epoch 58/300, seasonal_3 Loss: 0.0865 | 0.0456
Epoch 59/300, seasonal_3 Loss: 0.0864 | 0.0455
Epoch 60/300, seasonal_3 Loss: 0.0862 | 0.0455
Epoch 61/300, seasonal_3 Loss: 0.0860 | 0.0454
Epoch 62/300, seasonal_3 Loss: 0.0859 | 0.0453
Epoch 63/300, seasonal_3 Loss: 0.0858 | 0.0453
Epoch 64/300, seasonal_3 Loss: 0.0856 | 0.0452
Epoch 65/300, seasonal_3 Loss: 0.0855 | 0.0451
Epoch 66/300, seasonal_3 Loss: 0.0854 | 0.0451
Epoch 67/300, seasonal_3 Loss: 0.0853 | 0.0450
Epoch 68/300, seasonal_3 Loss: 0.0852 | 0.0450
Epoch 69/300, seasonal_3 Loss: 0.0851 | 0.0449
Epoch 70/300, seasonal_3 Loss: 0.0850 | 0.0449
Epoch 71/300, seasonal_3 Loss: 0.0849 | 0.0448
Epoch 72/300, seasonal_3 Loss: 0.0848 | 0.0448
Epoch 73/300, seasonal_3 Loss: 0.0847 | 0.0447
Epoch 74/300, seasonal_3 Loss: 0.0846 | 0.0447
Epoch 75/300, seasonal_3 Loss: 0.0845 | 0.0447
Epoch 76/300, seasonal_3 Loss: 0.0844 | 0.0446
Epoch 77/300, seasonal_3 Loss: 0.0843 | 0.0446
Epoch 78/300, seasonal_3 Loss: 0.0843 | 0.0445
Epoch 79/300, seasonal_3 Loss: 0.0842 | 0.0445
Epoch 80/300, seasonal_3 Loss: 0.0841 | 0.0445
Epoch 81/300, seasonal_3 Loss: 0.0841 | 0.0444
Epoch 82/300, seasonal_3 Loss: 0.0840 | 0.0444
Epoch 83/300, seasonal_3 Loss: 0.0839 | 0.0444
Epoch 84/300, seasonal_3 Loss: 0.0839 | 0.0443
Epoch 85/300, seasonal_3 Loss: 0.0838 | 0.0443
Epoch 86/300, seasonal_3 Loss: 0.0838 | 0.0443
Epoch 87/300, seasonal_3 Loss: 0.0837 | 0.0442
Epoch 88/300, seasonal_3 Loss: 0.0837 | 0.0442
Epoch 89/300, seasonal_3 Loss: 0.0836 | 0.0442
Epoch 90/300, seasonal_3 Loss: 0.0836 | 0.0442
Epoch 91/300, seasonal_3 Loss: 0.0835 | 0.0441
Epoch 92/300, seasonal_3 Loss: 0.0835 | 0.0441
Epoch 93/300, seasonal_3 Loss: 0.0834 | 0.0441
Epoch 94/300, seasonal_3 Loss: 0.0834 | 0.0441
Epoch 95/300, seasonal_3 Loss: 0.0834 | 0.0441
Epoch 96/300, seasonal_3 Loss: 0.0833 | 0.0440
Epoch 97/300, seasonal_3 Loss: 0.0833 | 0.0440
Epoch 98/300, seasonal_3 Loss: 0.0832 | 0.0440
Epoch 99/300, seasonal_3 Loss: 0.0832 | 0.0440
Epoch 100/300, seasonal_3 Loss: 0.0832 | 0.0440
Epoch 101/300, seasonal_3 Loss: 0.0831 | 0.0439
Epoch 102/300, seasonal_3 Loss: 0.0831 | 0.0439
Epoch 103/300, seasonal_3 Loss: 0.0831 | 0.0439
Epoch 104/300, seasonal_3 Loss: 0.0831 | 0.0439
Epoch 105/300, seasonal_3 Loss: 0.0830 | 0.0439
Epoch 106/300, seasonal_3 Loss: 0.0830 | 0.0439
Epoch 107/300, seasonal_3 Loss: 0.0830 | 0.0439
Epoch 108/300, seasonal_3 Loss: 0.0830 | 0.0438
Epoch 109/300, seasonal_3 Loss: 0.0829 | 0.0438
Epoch 110/300, seasonal_3 Loss: 0.0829 | 0.0438
Epoch 111/300, seasonal_3 Loss: 0.0829 | 0.0438
Epoch 112/300, seasonal_3 Loss: 0.0829 | 0.0438
Epoch 113/300, seasonal_3 Loss: 0.0828 | 0.0438
Epoch 114/300, seasonal_3 Loss: 0.0828 | 0.0438
Epoch 115/300, seasonal_3 Loss: 0.0828 | 0.0438
Epoch 116/300, seasonal_3 Loss: 0.0828 | 0.0438
Epoch 117/300, seasonal_3 Loss: 0.0828 | 0.0437
Epoch 118/300, seasonal_3 Loss: 0.0828 | 0.0437
Epoch 119/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 120/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 121/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 122/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 123/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 124/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 125/300, seasonal_3 Loss: 0.0827 | 0.0437
Epoch 126/300, seasonal_3 Loss: 0.0826 | 0.0437
Epoch 127/300, seasonal_3 Loss: 0.0826 | 0.0437
Epoch 128/300, seasonal_3 Loss: 0.0826 | 0.0437
Epoch 129/300, seasonal_3 Loss: 0.0826 | 0.0437
Epoch 130/300, seasonal_3 Loss: 0.0826 | 0.0436
Epoch 131/300, seasonal_3 Loss: 0.0826 | 0.0436
Epoch 132/300, seasonal_3 Loss: 0.0826 | 0.0436
Epoch 133/300, seasonal_3 Loss: 0.0826 | 0.0436
Epoch 134/300, seasonal_3 Loss: 0.0826 | 0.0436
Epoch 135/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 136/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 137/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 138/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 139/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 140/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 141/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 142/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 143/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 144/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 145/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 146/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 147/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 148/300, seasonal_3 Loss: 0.0825 | 0.0436
Epoch 149/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 150/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 151/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 152/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 153/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 154/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 155/300, seasonal_3 Loss: 0.0824 | 0.0436
Epoch 156/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 157/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 158/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 159/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 160/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 161/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 162/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 163/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 164/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 165/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 166/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 167/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 168/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 169/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 170/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 171/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 172/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 173/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 174/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 175/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 176/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 177/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 178/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 179/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 180/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 181/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 182/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 183/300, seasonal_3 Loss: 0.0824 | 0.0435
Epoch 184/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 185/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 186/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 187/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 188/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 189/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 190/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 191/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 192/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 193/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 194/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 195/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 196/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 197/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 198/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 199/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 200/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 201/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 202/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 203/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 204/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 205/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 206/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 207/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 208/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 209/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 210/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 211/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 212/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 213/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 214/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 215/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 216/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 217/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 218/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 219/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 220/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 221/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 222/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 223/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 224/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 225/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 226/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 227/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 228/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 229/300, seasonal_3 Loss: 0.0823 | 0.0435
Epoch 230/300, seasonal_3 Loss: 0.0823 | 0.0435
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 53, 'train_rates': 0.9897444139284903, 'learning_rate': 1.8334408738003524e-05, 'batch_size': 65, 'step_size': 15, 'gamma': 0.783084706566163}
Epoch 1/300, resid Loss: 0.8808 | 1.0268
Epoch 2/300, resid Loss: 0.6956 | 0.7933
Epoch 3/300, resid Loss: 0.5912 | 0.6604
Epoch 4/300, resid Loss: 0.5082 | 0.5559
Epoch 5/300, resid Loss: 0.4441 | 0.4759
Epoch 6/300, resid Loss: 0.3958 | 0.4162
Epoch 7/300, resid Loss: 0.3602 | 0.3724
Epoch 8/300, resid Loss: 0.3339 | 0.3400
Epoch 9/300, resid Loss: 0.3148 | 0.3188
Epoch 10/300, resid Loss: 0.3008 | 0.3026
Epoch 11/300, resid Loss: 0.2899 | 0.2886
Epoch 12/300, resid Loss: 0.2801 | 0.2762
Epoch 13/300, resid Loss: 0.2714 | 0.2652
Epoch 14/300, resid Loss: 0.2636 | 0.2554
Epoch 15/300, resid Loss: 0.2565 | 0.2465
Epoch 16/300, resid Loss: 0.2502 | 0.2386
Epoch 17/300, resid Loss: 0.2451 | 0.2327
Epoch 18/300, resid Loss: 0.2407 | 0.2271
Epoch 19/300, resid Loss: 0.2366 | 0.2219
Epoch 20/300, resid Loss: 0.2327 | 0.2169
Epoch 21/300, resid Loss: 0.2289 | 0.2122
Epoch 22/300, resid Loss: 0.2254 | 0.2078
Epoch 23/300, resid Loss: 0.2220 | 0.2036
Epoch 24/300, resid Loss: 0.2189 | 0.1993
Epoch 25/300, resid Loss: 0.2163 | 0.1964
Epoch 26/300, resid Loss: 0.2139 | 0.1935
Epoch 27/300, resid Loss: 0.2117 | 0.1906
Epoch 28/300, resid Loss: 0.2094 | 0.1879
Epoch 29/300, resid Loss: 0.2073 | 0.1852
Epoch 30/300, resid Loss: 0.2052 | 0.1827
Epoch 31/300, resid Loss: 0.2031 | 0.1798
Epoch 32/300, resid Loss: 0.2015 | 0.1779
Epoch 33/300, resid Loss: 0.2000 | 0.1760
Epoch 34/300, resid Loss: 0.1985 | 0.1742
Epoch 35/300, resid Loss: 0.1970 | 0.1723
Epoch 36/300, resid Loss: 0.1955 | 0.1705
Epoch 37/300, resid Loss: 0.1940 | 0.1688
Epoch 38/300, resid Loss: 0.1926 | 0.1670
Epoch 39/300, resid Loss: 0.1912 | 0.1651
Epoch 40/300, resid Loss: 0.1901 | 0.1637
Epoch 41/300, resid Loss: 0.1891 | 0.1624
Epoch 42/300, resid Loss: 0.1880 | 0.1611
Epoch 43/300, resid Loss: 0.1870 | 0.1598
Epoch 44/300, resid Loss: 0.1859 | 0.1585
Epoch 45/300, resid Loss: 0.1849 | 0.1572
Epoch 46/300, resid Loss: 0.1839 | 0.1558
Epoch 47/300, resid Loss: 0.1831 | 0.1548
Epoch 48/300, resid Loss: 0.1823 | 0.1538
Epoch 49/300, resid Loss: 0.1815 | 0.1528
Epoch 50/300, resid Loss: 0.1808 | 0.1519
Epoch 51/300, resid Loss: 0.1800 | 0.1509
Epoch 52/300, resid Loss: 0.1792 | 0.1499
Epoch 53/300, resid Loss: 0.1785 | 0.1490
Epoch 54/300, resid Loss: 0.1777 | 0.1480
Epoch 55/300, resid Loss: 0.1771 | 0.1472
Epoch 56/300, resid Loss: 0.1766 | 0.1464
Epoch 57/300, resid Loss: 0.1760 | 0.1457
Epoch 58/300, resid Loss: 0.1754 | 0.1450
Epoch 59/300, resid Loss: 0.1748 | 0.1442
Epoch 60/300, resid Loss: 0.1743 | 0.1435
Epoch 61/300, resid Loss: 0.1737 | 0.1427
Epoch 62/300, resid Loss: 0.1733 | 0.1421
Epoch 63/300, resid Loss: 0.1728 | 0.1416
Epoch 64/300, resid Loss: 0.1724 | 0.1410
Epoch 65/300, resid Loss: 0.1719 | 0.1404
Epoch 66/300, resid Loss: 0.1715 | 0.1398
Epoch 67/300, resid Loss: 0.1711 | 0.1393
Epoch 68/300, resid Loss: 0.1707 | 0.1387
Epoch 69/300, resid Loss: 0.1702 | 0.1382
Epoch 70/300, resid Loss: 0.1699 | 0.1377
Epoch 71/300, resid Loss: 0.1695 | 0.1372
Epoch 72/300, resid Loss: 0.1692 | 0.1368
Epoch 73/300, resid Loss: 0.1689 | 0.1363
Epoch 74/300, resid Loss: 0.1685 | 0.1359
Epoch 75/300, resid Loss: 0.1682 | 0.1355
Epoch 76/300, resid Loss: 0.1679 | 0.1351
Epoch 77/300, resid Loss: 0.1676 | 0.1347
Epoch 78/300, resid Loss: 0.1674 | 0.1343
Epoch 79/300, resid Loss: 0.1671 | 0.1340
Epoch 80/300, resid Loss: 0.1668 | 0.1336
Epoch 81/300, resid Loss: 0.1666 | 0.1333
Epoch 82/300, resid Loss: 0.1663 | 0.1329
Epoch 83/300, resid Loss: 0.1661 | 0.1326
Epoch 84/300, resid Loss: 0.1658 | 0.1323
Epoch 85/300, resid Loss: 0.1656 | 0.1320
Epoch 86/300, resid Loss: 0.1654 | 0.1317
Epoch 87/300, resid Loss: 0.1652 | 0.1314
Epoch 88/300, resid Loss: 0.1650 | 0.1312
Epoch 89/300, resid Loss: 0.1648 | 0.1309
Epoch 90/300, resid Loss: 0.1646 | 0.1306
Epoch 91/300, resid Loss: 0.1644 | 0.1304
Epoch 92/300, resid Loss: 0.1643 | 0.1302
Epoch 93/300, resid Loss: 0.1641 | 0.1299
Epoch 94/300, resid Loss: 0.1640 | 0.1297
Epoch 95/300, resid Loss: 0.1638 | 0.1295
Epoch 96/300, resid Loss: 0.1637 | 0.1293
Epoch 97/300, resid Loss: 0.1635 | 0.1291
Epoch 98/300, resid Loss: 0.1633 | 0.1289
Epoch 99/300, resid Loss: 0.1632 | 0.1287
Epoch 100/300, resid Loss: 0.1631 | 0.1285
Epoch 101/300, resid Loss: 0.1630 | 0.1284
Epoch 102/300, resid Loss: 0.1628 | 0.1282
Epoch 103/300, resid Loss: 0.1627 | 0.1280
Epoch 104/300, resid Loss: 0.1626 | 0.1279
Epoch 105/300, resid Loss: 0.1625 | 0.1277
Epoch 106/300, resid Loss: 0.1623 | 0.1276
Epoch 107/300, resid Loss: 0.1623 | 0.1274
Epoch 108/300, resid Loss: 0.1622 | 0.1273
Epoch 109/300, resid Loss: 0.1621 | 0.1271
Epoch 110/300, resid Loss: 0.1620 | 0.1270
Epoch 111/300, resid Loss: 0.1619 | 0.1269
Epoch 112/300, resid Loss: 0.1618 | 0.1268
Epoch 113/300, resid Loss: 0.1617 | 0.1266
Epoch 114/300, resid Loss: 0.1616 | 0.1265
Epoch 115/300, resid Loss: 0.1615 | 0.1264
Epoch 116/300, resid Loss: 0.1615 | 0.1263
Epoch 117/300, resid Loss: 0.1614 | 0.1262
Epoch 118/300, resid Loss: 0.1613 | 0.1261
Epoch 119/300, resid Loss: 0.1612 | 0.1260
Epoch 120/300, resid Loss: 0.1612 | 0.1259
Epoch 121/300, resid Loss: 0.1611 | 0.1258
Epoch 122/300, resid Loss: 0.1610 | 0.1257
Epoch 123/300, resid Loss: 0.1610 | 0.1256
Epoch 124/300, resid Loss: 0.1609 | 0.1256
Epoch 125/300, resid Loss: 0.1609 | 0.1255
Epoch 126/300, resid Loss: 0.1608 | 0.1254
Epoch 127/300, resid Loss: 0.1607 | 0.1253
Epoch 128/300, resid Loss: 0.1607 | 0.1252
Epoch 129/300, resid Loss: 0.1606 | 0.1252
Epoch 130/300, resid Loss: 0.1606 | 0.1251
Epoch 131/300, resid Loss: 0.1605 | 0.1250
Epoch 132/300, resid Loss: 0.1605 | 0.1250
Epoch 133/300, resid Loss: 0.1604 | 0.1249
Epoch 134/300, resid Loss: 0.1604 | 0.1249
Epoch 135/300, resid Loss: 0.1604 | 0.1248
Epoch 136/300, resid Loss: 0.1603 | 0.1247
Epoch 137/300, resid Loss: 0.1603 | 0.1247
Epoch 138/300, resid Loss: 0.1602 | 0.1246
Epoch 139/300, resid Loss: 0.1602 | 0.1246
Epoch 140/300, resid Loss: 0.1602 | 0.1245
Epoch 141/300, resid Loss: 0.1601 | 0.1245
Epoch 142/300, resid Loss: 0.1601 | 0.1244
Epoch 143/300, resid Loss: 0.1601 | 0.1244
Epoch 144/300, resid Loss: 0.1600 | 0.1243
Epoch 145/300, resid Loss: 0.1600 | 0.1243
Epoch 146/300, resid Loss: 0.1600 | 0.1243
Epoch 147/300, resid Loss: 0.1599 | 0.1242
Epoch 148/300, resid Loss: 0.1599 | 0.1242
Epoch 149/300, resid Loss: 0.1599 | 0.1241
Epoch 150/300, resid Loss: 0.1599 | 0.1241
Epoch 151/300, resid Loss: 0.1598 | 0.1241
Epoch 152/300, resid Loss: 0.1598 | 0.1240
Epoch 153/300, resid Loss: 0.1598 | 0.1240
Epoch 154/300, resid Loss: 0.1598 | 0.1240
Epoch 155/300, resid Loss: 0.1597 | 0.1240
Epoch 156/300, resid Loss: 0.1597 | 0.1239
Epoch 157/300, resid Loss: 0.1597 | 0.1239
Epoch 158/300, resid Loss: 0.1597 | 0.1239
Epoch 159/300, resid Loss: 0.1597 | 0.1238
Epoch 160/300, resid Loss: 0.1596 | 0.1238
Epoch 161/300, resid Loss: 0.1596 | 0.1238
Epoch 162/300, resid Loss: 0.1596 | 0.1238
Epoch 163/300, resid Loss: 0.1596 | 0.1237
Epoch 164/300, resid Loss: 0.1596 | 0.1237
Epoch 165/300, resid Loss: 0.1596 | 0.1237
Epoch 166/300, resid Loss: 0.1595 | 0.1237
Epoch 167/300, resid Loss: 0.1595 | 0.1237
Epoch 168/300, resid Loss: 0.1595 | 0.1236
Epoch 169/300, resid Loss: 0.1595 | 0.1236
Epoch 170/300, resid Loss: 0.1595 | 0.1236
Epoch 171/300, resid Loss: 0.1595 | 0.1236
Epoch 172/300, resid Loss: 0.1595 | 0.1236
Epoch 173/300, resid Loss: 0.1595 | 0.1235
Epoch 174/300, resid Loss: 0.1594 | 0.1235
Epoch 175/300, resid Loss: 0.1594 | 0.1235
Epoch 176/300, resid Loss: 0.1594 | 0.1235
Epoch 177/300, resid Loss: 0.1594 | 0.1235
Epoch 178/300, resid Loss: 0.1594 | 0.1235
Epoch 179/300, resid Loss: 0.1594 | 0.1235
Epoch 180/300, resid Loss: 0.1594 | 0.1234
Epoch 181/300, resid Loss: 0.1594 | 0.1234
Epoch 182/300, resid Loss: 0.1594 | 0.1234
Epoch 183/300, resid Loss: 0.1594 | 0.1234
Epoch 184/300, resid Loss: 0.1593 | 0.1234
Epoch 185/300, resid Loss: 0.1593 | 0.1234
Epoch 186/300, resid Loss: 0.1593 | 0.1234
Epoch 187/300, resid Loss: 0.1593 | 0.1234
Epoch 188/300, resid Loss: 0.1593 | 0.1234
Epoch 189/300, resid Loss: 0.1593 | 0.1233
Epoch 190/300, resid Loss: 0.1593 | 0.1233
Epoch 191/300, resid Loss: 0.1593 | 0.1233
Epoch 192/300, resid Loss: 0.1593 | 0.1233
Epoch 193/300, resid Loss: 0.1593 | 0.1233
Epoch 194/300, resid Loss: 0.1593 | 0.1233
Epoch 195/300, resid Loss: 0.1593 | 0.1233
Epoch 196/300, resid Loss: 0.1593 | 0.1233
Epoch 197/300, resid Loss: 0.1593 | 0.1233
Epoch 198/300, resid Loss: 0.1592 | 0.1233
Epoch 199/300, resid Loss: 0.1592 | 0.1233
Epoch 200/300, resid Loss: 0.1592 | 0.1233
Epoch 201/300, resid Loss: 0.1592 | 0.1232
Epoch 202/300, resid Loss: 0.1592 | 0.1232
Epoch 203/300, resid Loss: 0.1592 | 0.1232
Epoch 204/300, resid Loss: 0.1592 | 0.1232
Epoch 205/300, resid Loss: 0.1592 | 0.1232
Epoch 206/300, resid Loss: 0.1592 | 0.1232
Epoch 207/300, resid Loss: 0.1592 | 0.1232
Epoch 208/300, resid Loss: 0.1592 | 0.1232
Epoch 209/300, resid Loss: 0.1592 | 0.1232
Epoch 210/300, resid Loss: 0.1592 | 0.1232
Epoch 211/300, resid Loss: 0.1592 | 0.1232
Epoch 212/300, resid Loss: 0.1592 | 0.1232
Epoch 213/300, resid Loss: 0.1592 | 0.1232
Epoch 214/300, resid Loss: 0.1592 | 0.1232
Epoch 215/300, resid Loss: 0.1592 | 0.1232
Epoch 216/300, resid Loss: 0.1592 | 0.1232
Epoch 217/300, resid Loss: 0.1592 | 0.1232
Epoch 218/300, resid Loss: 0.1592 | 0.1232
Epoch 219/300, resid Loss: 0.1592 | 0.1232
Epoch 220/300, resid Loss: 0.1592 | 0.1232
Epoch 221/300, resid Loss: 0.1592 | 0.1231
Epoch 222/300, resid Loss: 0.1592 | 0.1231
Epoch 223/300, resid Loss: 0.1592 | 0.1231
Epoch 224/300, resid Loss: 0.1592 | 0.1231
Epoch 225/300, resid Loss: 0.1592 | 0.1231
Epoch 226/300, resid Loss: 0.1592 | 0.1231
Epoch 227/300, resid Loss: 0.1592 | 0.1231
Epoch 228/300, resid Loss: 0.1592 | 0.1231
Epoch 229/300, resid Loss: 0.1591 | 0.1231
Epoch 230/300, resid Loss: 0.1591 | 0.1231
Epoch 231/300, resid Loss: 0.1591 | 0.1231
Epoch 232/300, resid Loss: 0.1591 | 0.1231
Epoch 233/300, resid Loss: 0.1591 | 0.1231
Epoch 234/300, resid Loss: 0.1591 | 0.1231
Epoch 235/300, resid Loss: 0.1591 | 0.1231
Epoch 236/300, resid Loss: 0.1591 | 0.1231
Epoch 237/300, resid Loss: 0.1591 | 0.1231
Epoch 238/300, resid Loss: 0.1591 | 0.1231
Epoch 239/300, resid Loss: 0.1591 | 0.1231
Epoch 240/300, resid Loss: 0.1591 | 0.1231
Epoch 241/300, resid Loss: 0.1591 | 0.1231
Epoch 242/300, resid Loss: 0.1591 | 0.1231
Epoch 243/300, resid Loss: 0.1591 | 0.1231
Epoch 244/300, resid Loss: 0.1591 | 0.1231
Epoch 245/300, resid Loss: 0.1591 | 0.1231
Epoch 246/300, resid Loss: 0.1591 | 0.1231
Epoch 247/300, resid Loss: 0.1591 | 0.1231
Epoch 248/300, resid Loss: 0.1591 | 0.1231
Epoch 249/300, resid Loss: 0.1591 | 0.1231
Epoch 250/300, resid Loss: 0.1591 | 0.1231
Epoch 251/300, resid Loss: 0.1591 | 0.1231
Epoch 252/300, resid Loss: 0.1591 | 0.1231
Epoch 253/300, resid Loss: 0.1591 | 0.1231
Epoch 254/300, resid Loss: 0.1591 | 0.1231
Epoch 255/300, resid Loss: 0.1591 | 0.1231
Epoch 256/300, resid Loss: 0.1591 | 0.1231
Epoch 257/300, resid Loss: 0.1591 | 0.1231
Epoch 258/300, resid Loss: 0.1591 | 0.1231
Epoch 259/300, resid Loss: 0.1591 | 0.1231
Epoch 260/300, resid Loss: 0.1591 | 0.1231
Epoch 261/300, resid Loss: 0.1591 | 0.1231
Epoch 262/300, resid Loss: 0.1591 | 0.1231
Epoch 263/300, resid Loss: 0.1591 | 0.1231
Epoch 264/300, resid Loss: 0.1591 | 0.1231
Epoch 265/300, resid Loss: 0.1591 | 0.1231
Epoch 266/300, resid Loss: 0.1591 | 0.1231
Epoch 267/300, resid Loss: 0.1591 | 0.1231
Epoch 268/300, resid Loss: 0.1591 | 0.1231
Epoch 269/300, resid Loss: 0.1591 | 0.1231
Epoch 270/300, resid Loss: 0.1591 | 0.1231
Epoch 271/300, resid Loss: 0.1591 | 0.1231
Epoch 272/300, resid Loss: 0.1591 | 0.1231
Epoch 273/300, resid Loss: 0.1591 | 0.1231
Epoch 274/300, resid Loss: 0.1591 | 0.1231
Epoch 275/300, resid Loss: 0.1591 | 0.1231
Epoch 276/300, resid Loss: 0.1591 | 0.1231
Epoch 277/300, resid Loss: 0.1591 | 0.1231
Epoch 278/300, resid Loss: 0.1591 | 0.1231
Epoch 279/300, resid Loss: 0.1591 | 0.1231
Epoch 280/300, resid Loss: 0.1591 | 0.1231
Epoch 281/300, resid Loss: 0.1591 | 0.1231
Epoch 282/300, resid Loss: 0.1591 | 0.1231
Epoch 283/300, resid Loss: 0.1591 | 0.1231
Epoch 284/300, resid Loss: 0.1591 | 0.1231
Epoch 285/300, resid Loss: 0.1591 | 0.1231
Epoch 286/300, resid Loss: 0.1591 | 0.1231
Epoch 287/300, resid Loss: 0.1591 | 0.1231
Epoch 288/300, resid Loss: 0.1591 | 0.1231
Epoch 289/300, resid Loss: 0.1591 | 0.1231
Epoch 290/300, resid Loss: 0.1591 | 0.1231
Epoch 291/300, resid Loss: 0.1591 | 0.1231
Epoch 292/300, resid Loss: 0.1591 | 0.1231
Epoch 293/300, resid Loss: 0.1591 | 0.1231
Epoch 294/300, resid Loss: 0.1591 | 0.1231
Epoch 295/300, resid Loss: 0.1591 | 0.1231
Epoch 296/300, resid Loss: 0.1591 | 0.1231
Epoch 297/300, resid Loss: 0.1591 | 0.1231
Epoch 298/300, resid Loss: 0.1591 | 0.1231
Epoch 299/300, resid Loss: 0.1591 | 0.1231
Epoch 300/300, resid Loss: 0.1591 | 0.1231
Runtime (seconds): 604.1077027320862
0.00020029441795952236
[103.246216]
[5.368908]
[-1.6640195]
[0.30727413]
[-4.455373]
[-3.7358065]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 7.688401156105101
RMSE: 2.772796630859375
MAE: 2.772796630859375
R-squared: nan
[99.0672]
