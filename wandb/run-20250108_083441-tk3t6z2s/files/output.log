[32m[I 2025-01-08 08:34:42,417][0m A new study created in memory with name: no-name-3364d4b7-2279-4db5-b8c2-40840e9bbe1f[0m
[32m[I 2025-01-08 08:35:07,412][0m Trial 0 finished with value: 0.739901873734918 and parameters: {'observation_period_num': 18, 'train_rates': 0.7265257718600471, 'learning_rate': 0.0009661489332100095, 'batch_size': 200, 'step_size': 7, 'gamma': 0.7978461898226331}. Best is trial 0 with value: 0.739901873734918.[0m
[32m[I 2025-01-08 08:38:15,990][0m Trial 1 finished with value: 0.6752622102953724 and parameters: {'observation_period_num': 141, 'train_rates': 0.829025859148873, 'learning_rate': 1.4818113820358818e-05, 'batch_size': 156, 'step_size': 5, 'gamma': 0.8358725833207965}. Best is trial 1 with value: 0.6752622102953724.[0m
[32m[I 2025-01-08 08:40:54,273][0m Trial 2 finished with value: 0.9808415507302066 and parameters: {'observation_period_num': 124, 'train_rates': 0.7925215261320004, 'learning_rate': 2.641538505146821e-06, 'batch_size': 62, 'step_size': 4, 'gamma': 0.9066669040967936}. Best is trial 1 with value: 0.6752622102953724.[0m
[32m[I 2025-01-08 08:44:21,968][0m Trial 3 finished with value: 1.0067225499431334 and parameters: {'observation_period_num': 184, 'train_rates': 0.6497916487609989, 'learning_rate': 1.1102092648237938e-05, 'batch_size': 191, 'step_size': 2, 'gamma': 0.9817279579547719}. Best is trial 1 with value: 0.6752622102953724.[0m
[32m[I 2025-01-08 08:49:40,053][0m Trial 4 finished with value: 0.382040483386893 and parameters: {'observation_period_num': 208, 'train_rates': 0.958756082262195, 'learning_rate': 7.82275521350659e-06, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8031691889233837}. Best is trial 4 with value: 0.382040483386893.[0m
[32m[I 2025-01-08 08:50:36,120][0m Trial 5 finished with value: 1.1930524482898965 and parameters: {'observation_period_num': 19, 'train_rates': 0.8218602289561208, 'learning_rate': 0.000985772119539981, 'batch_size': 67, 'step_size': 10, 'gamma': 0.8685855013260496}. Best is trial 4 with value: 0.382040483386893.[0m
[32m[I 2025-01-08 08:54:03,318][0m Trial 6 finished with value: 0.1701672524213791 and parameters: {'observation_period_num': 142, 'train_rates': 0.9654394957509256, 'learning_rate': 0.0001682274591721669, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9329304881140694}. Best is trial 6 with value: 0.1701672524213791.[0m
[32m[I 2025-01-08 08:55:29,823][0m Trial 7 finished with value: 0.7237496960260134 and parameters: {'observation_period_num': 79, 'train_rates': 0.6604906973880169, 'learning_rate': 0.00026358640892236256, 'batch_size': 170, 'step_size': 4, 'gamma': 0.8067724341470567}. Best is trial 6 with value: 0.1701672524213791.[0m
[32m[I 2025-01-08 08:58:34,003][0m Trial 8 finished with value: 1.0220761781088314 and parameters: {'observation_period_num': 166, 'train_rates': 0.6245776947378372, 'learning_rate': 4.941053637539014e-05, 'batch_size': 89, 'step_size': 7, 'gamma': 0.822568385993567}. Best is trial 6 with value: 0.1701672524213791.[0m
[32m[I 2025-01-08 08:59:11,409][0m Trial 9 finished with value: 0.8728371768118871 and parameters: {'observation_period_num': 5, 'train_rates': 0.7281668683664194, 'learning_rate': 5.2057562504043825e-06, 'batch_size': 91, 'step_size': 8, 'gamma': 0.9129130473675786}. Best is trial 6 with value: 0.1701672524213791.[0m
[32m[I 2025-01-08 09:00:59,746][0m Trial 10 finished with value: 0.16622762382030487 and parameters: {'observation_period_num': 79, 'train_rates': 0.9783403000555739, 'learning_rate': 7.248971594872055e-05, 'batch_size': 254, 'step_size': 12, 'gamma': 0.9858381822596967}. Best is trial 10 with value: 0.16622762382030487.[0m
[32m[I 2025-01-08 09:03:00,534][0m Trial 11 finished with value: 0.1663675755262375 and parameters: {'observation_period_num': 87, 'train_rates': 0.9846767866980843, 'learning_rate': 7.64712379005461e-05, 'batch_size': 247, 'step_size': 12, 'gamma': 0.987747641520161}. Best is trial 10 with value: 0.16622762382030487.[0m
[32m[I 2025-01-08 09:04:40,509][0m Trial 12 finished with value: 0.1994186559586826 and parameters: {'observation_period_num': 77, 'train_rates': 0.8961072044872564, 'learning_rate': 5.470974515568094e-05, 'batch_size': 250, 'step_size': 12, 'gamma': 0.9865891090081489}. Best is trial 10 with value: 0.16622762382030487.[0m
[32m[I 2025-01-08 09:06:29,551][0m Trial 13 finished with value: 0.1253853440284729 and parameters: {'observation_period_num': 78, 'train_rates': 0.9864081944429196, 'learning_rate': 0.0001736150529577655, 'batch_size': 254, 'step_size': 13, 'gamma': 0.9527247118232103}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:09:50,555][0m Trial 14 finished with value: 0.23763505303684404 and parameters: {'observation_period_num': 101, 'train_rates': 0.9055095780308797, 'learning_rate': 0.00023853149516734, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9471916067491284}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:16:03,860][0m Trial 15 finished with value: 0.39797680691551807 and parameters: {'observation_period_num': 246, 'train_rates': 0.9020184465618797, 'learning_rate': 2.1839081957465505e-05, 'batch_size': 227, 'step_size': 12, 'gamma': 0.7612224576961534}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:17:08,431][0m Trial 16 finished with value: 0.19385169174870834 and parameters: {'observation_period_num': 48, 'train_rates': 0.9308086074917425, 'learning_rate': 0.00011876488282980282, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9531819727760459}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:17:56,342][0m Trial 17 finished with value: 0.27117253248005696 and parameters: {'observation_period_num': 38, 'train_rates': 0.8508731696855354, 'learning_rate': 0.00043323892384561824, 'batch_size': 252, 'step_size': 13, 'gamma': 0.8805459957476229}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:19:02,918][0m Trial 18 finished with value: 0.3046327004307195 and parameters: {'observation_period_num': 54, 'train_rates': 0.8700996861950202, 'learning_rate': 2.8259555337687872e-05, 'batch_size': 216, 'step_size': 10, 'gamma': 0.9601063756815517}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:21:49,560][0m Trial 19 finished with value: 0.21828995645046234 and parameters: {'observation_period_num': 123, 'train_rates': 0.9449315311351173, 'learning_rate': 0.00047865508295459965, 'batch_size': 169, 'step_size': 14, 'gamma': 0.9185772315995472}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:24:07,683][0m Trial 20 finished with value: 1.4949129819869995 and parameters: {'observation_period_num': 99, 'train_rates': 0.988865576370419, 'learning_rate': 1.1499890962667887e-06, 'batch_size': 230, 'step_size': 11, 'gamma': 0.8909534904628409}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:26:10,731][0m Trial 21 finished with value: 0.16499648988246918 and parameters: {'observation_period_num': 89, 'train_rates': 0.984558134661417, 'learning_rate': 8.509912193627189e-05, 'batch_size': 256, 'step_size': 13, 'gamma': 0.9700805159610176}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:27:32,876][0m Trial 22 finished with value: 0.2055524522580471 and parameters: {'observation_period_num': 63, 'train_rates': 0.9323190839566606, 'learning_rate': 9.616526777608951e-05, 'batch_size': 195, 'step_size': 14, 'gamma': 0.9671787678609681}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:29:59,879][0m Trial 23 finished with value: 0.16509868204593658 and parameters: {'observation_period_num': 106, 'train_rates': 0.9889074001698517, 'learning_rate': 4.796786022319326e-05, 'batch_size': 254, 'step_size': 13, 'gamma': 0.9389852390551864}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:32:14,851][0m Trial 24 finished with value: 0.22413302958011627 and parameters: {'observation_period_num': 102, 'train_rates': 0.9218923694723816, 'learning_rate': 3.992730822200478e-05, 'batch_size': 234, 'step_size': 14, 'gamma': 0.9311994687767431}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:34:44,201][0m Trial 25 finished with value: 0.19634500551478942 and parameters: {'observation_period_num': 115, 'train_rates': 0.8858805210058331, 'learning_rate': 0.00015126329931872717, 'batch_size': 203, 'step_size': 9, 'gamma': 0.8577938764527877}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:38:19,780][0m Trial 26 finished with value: 0.28699299072225887 and parameters: {'observation_period_num': 149, 'train_rates': 0.9492464224642311, 'learning_rate': 2.3184803920034336e-05, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9348572031000302}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:39:40,130][0m Trial 27 finished with value: 0.3693349235332929 and parameters: {'observation_period_num': 66, 'train_rates': 0.7900008372144665, 'learning_rate': 0.00038566443317758384, 'batch_size': 235, 'step_size': 13, 'gamma': 0.9015286313549874}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:40:32,636][0m Trial 28 finished with value: 0.15713298320770264 and parameters: {'observation_period_num': 38, 'train_rates': 0.9664399860975399, 'learning_rate': 0.00021567675897102463, 'batch_size': 180, 'step_size': 11, 'gamma': 0.9683923129176857}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:41:04,593][0m Trial 29 finished with value: 0.668432056015423 and parameters: {'observation_period_num': 24, 'train_rates': 0.7498217989459361, 'learning_rate': 0.0005493001253040815, 'batch_size': 151, 'step_size': 11, 'gamma': 0.9695106198954122}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:41:53,058][0m Trial 30 finished with value: 0.24456088452584004 and parameters: {'observation_period_num': 37, 'train_rates': 0.8607505695446706, 'learning_rate': 0.00018036338144085658, 'batch_size': 182, 'step_size': 9, 'gamma': 0.9689779695144215}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:43:59,749][0m Trial 31 finished with value: 0.19425171613693237 and parameters: {'observation_period_num': 93, 'train_rates': 0.9613604437276702, 'learning_rate': 0.0002670714977004306, 'batch_size': 214, 'step_size': 11, 'gamma': 0.9396762408999471}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:45:17,859][0m Trial 32 finished with value: 0.16582545638084412 and parameters: {'observation_period_num': 58, 'train_rates': 0.9839439592377611, 'learning_rate': 0.00011336884684815798, 'batch_size': 238, 'step_size': 15, 'gamma': 0.9516544753258528}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:47:44,648][0m Trial 33 finished with value: 0.24198477492373213 and parameters: {'observation_period_num': 112, 'train_rates': 0.9183846952502693, 'learning_rate': 3.773690920270507e-05, 'batch_size': 202, 'step_size': 13, 'gamma': 0.9285820380190883}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:48:35,359][0m Trial 34 finished with value: 0.21873870491981506 and parameters: {'observation_period_num': 38, 'train_rates': 0.9410256732907607, 'learning_rate': 6.193449243786977e-05, 'batch_size': 255, 'step_size': 14, 'gamma': 0.9737238260448209}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:51:46,907][0m Trial 35 finished with value: 0.26674938201904297 and parameters: {'observation_period_num': 132, 'train_rates': 0.9605440617267867, 'learning_rate': 0.0006145722258890213, 'batch_size': 219, 'step_size': 11, 'gamma': 0.9577398757603819}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:53:27,337][0m Trial 36 finished with value: 0.24455787241458893 and parameters: {'observation_period_num': 72, 'train_rates': 0.9670076536433149, 'learning_rate': 1.548113517731839e-05, 'batch_size': 147, 'step_size': 13, 'gamma': 0.9180775671426001}. Best is trial 13 with value: 0.1253853440284729.[0m
Early stopping at epoch 75
[32m[I 2025-01-08 09:56:26,039][0m Trial 37 finished with value: 0.4406358301639557 and parameters: {'observation_period_num': 154, 'train_rates': 0.9884286534759514, 'learning_rate': 9.2553665020933e-05, 'batch_size': 116, 'step_size': 1, 'gamma': 0.8507952998148871}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 09:58:37,018][0m Trial 38 finished with value: 0.4397581421493198 and parameters: {'observation_period_num': 112, 'train_rates': 0.7734264438144848, 'learning_rate': 0.00021608301540022393, 'batch_size': 181, 'step_size': 9, 'gamma': 0.899963066739976}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:02:24,053][0m Trial 39 finished with value: 0.4104279477985538 and parameters: {'observation_period_num': 172, 'train_rates': 0.8252604454506043, 'learning_rate': 0.0003256425057406256, 'batch_size': 242, 'step_size': 15, 'gamma': 0.944900344512449}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:04:19,804][0m Trial 40 finished with value: 0.7197852669443403 and parameters: {'observation_period_num': 89, 'train_rates': 0.8788005121961231, 'learning_rate': 0.0007626232726569976, 'batch_size': 208, 'step_size': 5, 'gamma': 0.9768805700957336}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:05:35,803][0m Trial 41 finished with value: 0.17440831661224365 and parameters: {'observation_period_num': 57, 'train_rates': 0.9689822645267567, 'learning_rate': 0.00013414884055940447, 'batch_size': 237, 'step_size': 15, 'gamma': 0.9585943589723442}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:06:00,064][0m Trial 42 finished with value: 0.19126659631729126 and parameters: {'observation_period_num': 11, 'train_rates': 0.953612773185655, 'learning_rate': 9.44598158751081e-05, 'batch_size': 228, 'step_size': 14, 'gamma': 0.9517139635046864}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:07:02,842][0m Trial 43 finished with value: 0.14687404036521912 and parameters: {'observation_period_num': 47, 'train_rates': 0.9738684174301953, 'learning_rate': 0.0001297366218985063, 'batch_size': 245, 'step_size': 15, 'gamma': 0.9257263663371894}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:07:36,362][0m Trial 44 finished with value: 0.1644757091999054 and parameters: {'observation_period_num': 24, 'train_rates': 0.9143267703875759, 'learning_rate': 0.0001526164950281133, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9309992691381063}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:08:09,067][0m Trial 45 finished with value: 0.1601298451423645 and parameters: {'observation_period_num': 21, 'train_rates': 0.9258332228768601, 'learning_rate': 0.00017436110456534433, 'batch_size': 222, 'step_size': 12, 'gamma': 0.925673665051846}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:08:45,259][0m Trial 46 finished with value: 0.2448142149432778 and parameters: {'observation_period_num': 26, 'train_rates': 0.8446999068600286, 'learning_rate': 0.00017431846753361807, 'batch_size': 186, 'step_size': 7, 'gamma': 0.9112551130127716}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:09:14,714][0m Trial 47 finished with value: 0.13623233876074223 and parameters: {'observation_period_num': 15, 'train_rates': 0.9112674742699871, 'learning_rate': 0.00028344525625348843, 'batch_size': 169, 'step_size': 12, 'gamma': 0.8849626662116781}. Best is trial 13 with value: 0.1253853440284729.[0m
[32m[I 2025-01-08 10:09:43,029][0m Trial 48 finished with value: 0.12200427830590638 and parameters: {'observation_period_num': 5, 'train_rates': 0.8918469355314108, 'learning_rate': 0.0003218154047300309, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8835278180114635}. Best is trial 48 with value: 0.12200427830590638.[0m
[32m[I 2025-01-08 10:10:13,205][0m Trial 49 finished with value: 0.12821620044320128 and parameters: {'observation_period_num': 8, 'train_rates': 0.8986308694741048, 'learning_rate': 0.00031183671366289637, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8789233837312547}. Best is trial 48 with value: 0.12200427830590638.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.8364 | 1.0937
Epoch 2/300, Loss: 0.7928 | 0.8118
Epoch 3/300, Loss: 0.5479 | 0.7237
Epoch 4/300, Loss: 0.4948 | 0.6338
Epoch 5/300, Loss: 0.5795 | 0.8049
Epoch 6/300, Loss: 0.5139 | 0.7661
Epoch 7/300, Loss: 0.5146 | 0.6692
Epoch 8/300, Loss: 0.5267 | 0.6033
Epoch 9/300, Loss: 0.4382 | 0.5484
Epoch 10/300, Loss: 0.3972 | 0.5234
Epoch 11/300, Loss: 0.3373 | 0.4369
Epoch 12/300, Loss: 0.3297 | 0.4829
Epoch 13/300, Loss: 0.2915 | 0.3860
Epoch 14/300, Loss: 0.3161 | 0.3623
Epoch 15/300, Loss: 0.2744 | 0.3544
Epoch 16/300, Loss: 0.2752 | 0.4525
Epoch 17/300, Loss: 0.2724 | 0.4145
Epoch 18/300, Loss: 0.2850 | 0.3571
Epoch 19/300, Loss: 0.2552 | 0.3406
Epoch 20/300, Loss: 0.2508 | 0.3643
Epoch 21/300, Loss: 0.2254 | 0.3490
Epoch 22/300, Loss: 0.2252 | 0.3135
Epoch 23/300, Loss: 0.2217 | 0.2694
Epoch 24/300, Loss: 0.2181 | 0.2799
Epoch 25/300, Loss: 0.1949 | 0.2608
Epoch 26/300, Loss: 0.2077 | 0.2606
Epoch 27/300, Loss: 0.2236 | 0.2972
Epoch 28/300, Loss: 0.2274 | 0.2989
Epoch 29/300, Loss: 0.2817 | 0.2913
Epoch 30/300, Loss: 0.2255 | 0.3108
Epoch 31/300, Loss: 0.2196 | 0.3086
Epoch 32/300, Loss: 0.1769 | 0.2765
Epoch 33/300, Loss: 0.1621 | 0.2665
Epoch 34/300, Loss: 0.1506 | 0.2386
Epoch 35/300, Loss: 0.1439 | 0.2505
Epoch 36/300, Loss: 0.1416 | 0.2253
Epoch 37/300, Loss: 0.1384 | 0.2405
Epoch 38/300, Loss: 0.1350 | 0.2137
Epoch 39/300, Loss: 0.1311 | 0.2169
Epoch 40/300, Loss: 0.1303 | 0.2084
Epoch 41/300, Loss: 0.1297 | 0.2113
Epoch 42/300, Loss: 0.1316 | 0.2081
Epoch 43/300, Loss: 0.1326 | 0.2107
Epoch 44/300, Loss: 0.1285 | 0.2053
Epoch 45/300, Loss: 0.1261 | 0.1991
Epoch 46/300, Loss: 0.1256 | 0.2056
Epoch 47/300, Loss: 0.1275 | 0.1938
Epoch 48/300, Loss: 0.1258 | 0.2021
Epoch 49/300, Loss: 0.1230 | 0.1879
Epoch 50/300, Loss: 0.1176 | 0.1918
Epoch 51/300, Loss: 0.1154 | 0.1819
Epoch 52/300, Loss: 0.1132 | 0.1842
Epoch 53/300, Loss: 0.1120 | 0.1770
Epoch 54/300, Loss: 0.1097 | 0.1803
Epoch 55/300, Loss: 0.1094 | 0.1748
Epoch 56/300, Loss: 0.1078 | 0.1751
Epoch 57/300, Loss: 0.1064 | 0.1737
Epoch 58/300, Loss: 0.1048 | 0.1709
Epoch 59/300, Loss: 0.1037 | 0.1696
Epoch 60/300, Loss: 0.1026 | 0.1678
Epoch 61/300, Loss: 0.1018 | 0.1665
Epoch 62/300, Loss: 0.1005 | 0.1645
Epoch 63/300, Loss: 0.0997 | 0.1627
Epoch 64/300, Loss: 0.0986 | 0.1636
Epoch 65/300, Loss: 0.0982 | 0.1600
Epoch 66/300, Loss: 0.0967 | 0.1614
Epoch 67/300, Loss: 0.0964 | 0.1574
Epoch 68/300, Loss: 0.0965 | 0.1612
Epoch 69/300, Loss: 0.0960 | 0.1542
Epoch 70/300, Loss: 0.0955 | 0.1603
Epoch 71/300, Loss: 0.0957 | 0.1527
Epoch 72/300, Loss: 0.0949 | 0.1565
Epoch 73/300, Loss: 0.0944 | 0.1522
Epoch 74/300, Loss: 0.0935 | 0.1549
Epoch 75/300, Loss: 0.0927 | 0.1502
Epoch 76/300, Loss: 0.0914 | 0.1517
Epoch 77/300, Loss: 0.0901 | 0.1481
Epoch 78/300, Loss: 0.0899 | 0.1494
Epoch 79/300, Loss: 0.0898 | 0.1480
Epoch 80/300, Loss: 0.0894 | 0.1480
Epoch 81/300, Loss: 0.0894 | 0.1465
Epoch 82/300, Loss: 0.0895 | 0.1479
Epoch 83/300, Loss: 0.0887 | 0.1465
Epoch 84/300, Loss: 0.0896 | 0.1468
Epoch 85/300, Loss: 0.0897 | 0.1456
Epoch 86/300, Loss: 0.0885 | 0.1440
Epoch 87/300, Loss: 0.0878 | 0.1418
Epoch 88/300, Loss: 0.0865 | 0.1423
Epoch 89/300, Loss: 0.0858 | 0.1405
Epoch 90/300, Loss: 0.0855 | 0.1421
Epoch 91/300, Loss: 0.0847 | 0.1394
Epoch 92/300, Loss: 0.0847 | 0.1407
Epoch 93/300, Loss: 0.0844 | 0.1393
Epoch 94/300, Loss: 0.0848 | 0.1404
Epoch 95/300, Loss: 0.0840 | 0.1376
Epoch 96/300, Loss: 0.0838 | 0.1376
Epoch 97/300, Loss: 0.0833 | 0.1370
Epoch 98/300, Loss: 0.0829 | 0.1362
Epoch 99/300, Loss: 0.0822 | 0.1366
Epoch 100/300, Loss: 0.0822 | 0.1356
Epoch 101/300, Loss: 0.0820 | 0.1359
Epoch 102/300, Loss: 0.0819 | 0.1354
Epoch 103/300, Loss: 0.0819 | 0.1347
Epoch 104/300, Loss: 0.0809 | 0.1346
Epoch 105/300, Loss: 0.0808 | 0.1361
Epoch 106/300, Loss: 0.0809 | 0.1346
Epoch 107/300, Loss: 0.0811 | 0.1346
Epoch 108/300, Loss: 0.0802 | 0.1331
Epoch 109/300, Loss: 0.0804 | 0.1339
Epoch 110/300, Loss: 0.0794 | 0.1329
Epoch 111/300, Loss: 0.0795 | 0.1323
Epoch 112/300, Loss: 0.0802 | 0.1316
Epoch 113/300, Loss: 0.0790 | 0.1330
Epoch 114/300, Loss: 0.0790 | 0.1324
Epoch 115/300, Loss: 0.0794 | 0.1319
Epoch 116/300, Loss: 0.0789 | 0.1316
Epoch 117/300, Loss: 0.0789 | 0.1310
Epoch 118/300, Loss: 0.0784 | 0.1304
Epoch 119/300, Loss: 0.0781 | 0.1306
Epoch 120/300, Loss: 0.0783 | 0.1308
Epoch 121/300, Loss: 0.0779 | 0.1310
Epoch 122/300, Loss: 0.0776 | 0.1296
Epoch 123/300, Loss: 0.0773 | 0.1302
Epoch 124/300, Loss: 0.0775 | 0.1296
Epoch 125/300, Loss: 0.0775 | 0.1298
Epoch 126/300, Loss: 0.0775 | 0.1304
Epoch 127/300, Loss: 0.0775 | 0.1276
Epoch 128/300, Loss: 0.0769 | 0.1288
Epoch 129/300, Loss: 0.0767 | 0.1284
Epoch 130/300, Loss: 0.0769 | 0.1282
Epoch 131/300, Loss: 0.0762 | 0.1272
Epoch 132/300, Loss: 0.0767 | 0.1266
Epoch 133/300, Loss: 0.0757 | 0.1271
Epoch 134/300, Loss: 0.0759 | 0.1271
Epoch 135/300, Loss: 0.0757 | 0.1276
Epoch 136/300, Loss: 0.0755 | 0.1269
Epoch 137/300, Loss: 0.0756 | 0.1261
Epoch 138/300, Loss: 0.0764 | 0.1280
Epoch 139/300, Loss: 0.0759 | 0.1273
Epoch 140/300, Loss: 0.0752 | 0.1272
Epoch 141/300, Loss: 0.0756 | 0.1267
Epoch 142/300, Loss: 0.0754 | 0.1268
Epoch 143/300, Loss: 0.0747 | 0.1264
Epoch 144/300, Loss: 0.0754 | 0.1263
Epoch 145/300, Loss: 0.0754 | 0.1259
Epoch 146/300, Loss: 0.0751 | 0.1258
Epoch 147/300, Loss: 0.0750 | 0.1260
Epoch 148/300, Loss: 0.0742 | 0.1260
Epoch 149/300, Loss: 0.0747 | 0.1247
Epoch 150/300, Loss: 0.0738 | 0.1249
Epoch 151/300, Loss: 0.0743 | 0.1255
Epoch 152/300, Loss: 0.0739 | 0.1256
Epoch 153/300, Loss: 0.0745 | 0.1256
Epoch 154/300, Loss: 0.0739 | 0.1262
Epoch 155/300, Loss: 0.0735 | 0.1251
Epoch 156/300, Loss: 0.0738 | 0.1250
Epoch 157/300, Loss: 0.0735 | 0.1239
Epoch 158/300, Loss: 0.0738 | 0.1241
Epoch 159/300, Loss: 0.0737 | 0.1245
Epoch 160/300, Loss: 0.0735 | 0.1249
Epoch 161/300, Loss: 0.0733 | 0.1247
Epoch 162/300, Loss: 0.0732 | 0.1243
Epoch 163/300, Loss: 0.0731 | 0.1240
Epoch 164/300, Loss: 0.0723 | 0.1242
Epoch 165/300, Loss: 0.0734 | 0.1249
Epoch 166/300, Loss: 0.0729 | 0.1243
Epoch 167/300, Loss: 0.0734 | 0.1239
Epoch 168/300, Loss: 0.0731 | 0.1235
Epoch 169/300, Loss: 0.0727 | 0.1240
Epoch 170/300, Loss: 0.0723 | 0.1242
Epoch 171/300, Loss: 0.0722 | 0.1238
Epoch 172/300, Loss: 0.0721 | 0.1242
Epoch 173/300, Loss: 0.0724 | 0.1237
Epoch 174/300, Loss: 0.0722 | 0.1232
Epoch 175/300, Loss: 0.0722 | 0.1230
Epoch 176/300, Loss: 0.0721 | 0.1230
Epoch 177/300, Loss: 0.0715 | 0.1227
Epoch 178/300, Loss: 0.0723 | 0.1227
Epoch 179/300, Loss: 0.0726 | 0.1233
Epoch 180/300, Loss: 0.0721 | 0.1228
Epoch 181/300, Loss: 0.0716 | 0.1229
Epoch 182/300, Loss: 0.0719 | 0.1230
Epoch 183/300, Loss: 0.0721 | 0.1226
Epoch 184/300, Loss: 0.0716 | 0.1222
Epoch 185/300, Loss: 0.0720 | 0.1227
Epoch 186/300, Loss: 0.0713 | 0.1222
Epoch 187/300, Loss: 0.0717 | 0.1225
Epoch 188/300, Loss: 0.0712 | 0.1225
Epoch 189/300, Loss: 0.0717 | 0.1221
Epoch 190/300, Loss: 0.0724 | 0.1226
Epoch 191/300, Loss: 0.0714 | 0.1226
Epoch 192/300, Loss: 0.0714 | 0.1223
Epoch 193/300, Loss: 0.0715 | 0.1226
Epoch 194/300, Loss: 0.0717 | 0.1223
Epoch 195/300, Loss: 0.0710 | 0.1222
Epoch 196/300, Loss: 0.0711 | 0.1224
Epoch 197/300, Loss: 0.0708 | 0.1226
Epoch 198/300, Loss: 0.0710 | 0.1224
Epoch 199/300, Loss: 0.0712 | 0.1218
Epoch 200/300, Loss: 0.0707 | 0.1221
Epoch 201/300, Loss: 0.0708 | 0.1221
Epoch 202/300, Loss: 0.0713 | 0.1220
Epoch 203/300, Loss: 0.0711 | 0.1222
Epoch 204/300, Loss: 0.0709 | 0.1217
Epoch 205/300, Loss: 0.0706 | 0.1219
Epoch 206/300, Loss: 0.0708 | 0.1213
Epoch 207/300, Loss: 0.0714 | 0.1211
Epoch 208/300, Loss: 0.0709 | 0.1211
Epoch 209/300, Loss: 0.0702 | 0.1211
Epoch 210/300, Loss: 0.0700 | 0.1216
Epoch 211/300, Loss: 0.0704 | 0.1215
Epoch 212/300, Loss: 0.0708 | 0.1215
Epoch 213/300, Loss: 0.0708 | 0.1214
Epoch 214/300, Loss: 0.0704 | 0.1215
Epoch 215/300, Loss: 0.0705 | 0.1214
Epoch 216/300, Loss: 0.0706 | 0.1211
Epoch 217/300, Loss: 0.0701 | 0.1208
Epoch 218/300, Loss: 0.0703 | 0.1210
Epoch 219/300, Loss: 0.0701 | 0.1210
Epoch 220/300, Loss: 0.0705 | 0.1213
Epoch 221/300, Loss: 0.0701 | 0.1207
Epoch 222/300, Loss: 0.0706 | 0.1203
Epoch 223/300, Loss: 0.0705 | 0.1201
Epoch 224/300, Loss: 0.0702 | 0.1201
Epoch 225/300, Loss: 0.0706 | 0.1204
Epoch 226/300, Loss: 0.0703 | 0.1203
Epoch 227/300, Loss: 0.0706 | 0.1202
Epoch 228/300, Loss: 0.0698 | 0.1202
Epoch 229/300, Loss: 0.0699 | 0.1199
Epoch 230/300, Loss: 0.0703 | 0.1198
Epoch 231/300, Loss: 0.0709 | 0.1200
Epoch 232/300, Loss: 0.0703 | 0.1203
Epoch 233/300, Loss: 0.0698 | 0.1204
Epoch 234/300, Loss: 0.0697 | 0.1203
Epoch 235/300, Loss: 0.0707 | 0.1204
Epoch 236/300, Loss: 0.0697 | 0.1206
Epoch 237/300, Loss: 0.0699 | 0.1204
Epoch 238/300, Loss: 0.0704 | 0.1206
Epoch 239/300, Loss: 0.0699 | 0.1209
Epoch 240/300, Loss: 0.0701 | 0.1210
Epoch 241/300, Loss: 0.0700 | 0.1209
Epoch 242/300, Loss: 0.0700 | 0.1208
Epoch 243/300, Loss: 0.0699 | 0.1206
Epoch 244/300, Loss: 0.0703 | 0.1206
Epoch 245/300, Loss: 0.0697 | 0.1206
Epoch 246/300, Loss: 0.0700 | 0.1206
Epoch 247/300, Loss: 0.0697 | 0.1203
Epoch 248/300, Loss: 0.0700 | 0.1202
Epoch 249/300, Loss: 0.0698 | 0.1200
Epoch 250/300, Loss: 0.0699 | 0.1199
Epoch 251/300, Loss: 0.0698 | 0.1201
Epoch 252/300, Loss: 0.0694 | 0.1204
Epoch 253/300, Loss: 0.0694 | 0.1204
Epoch 254/300, Loss: 0.0695 | 0.1203
Epoch 255/300, Loss: 0.0700 | 0.1202
Epoch 256/300, Loss: 0.0698 | 0.1202
Epoch 257/300, Loss: 0.0695 | 0.1203
Epoch 258/300, Loss: 0.0698 | 0.1203
Epoch 259/300, Loss: 0.0703 | 0.1203
Epoch 260/300, Loss: 0.0701 | 0.1203
Epoch 261/300, Loss: 0.0698 | 0.1204
Epoch 262/300, Loss: 0.0695 | 0.1205
Epoch 263/300, Loss: 0.0693 | 0.1205
Epoch 264/300, Loss: 0.0700 | 0.1205
Epoch 265/300, Loss: 0.0687 | 0.1203
Epoch 266/300, Loss: 0.0694 | 0.1202
Epoch 267/300, Loss: 0.0695 | 0.1202
Epoch 268/300, Loss: 0.0690 | 0.1204
Epoch 269/300, Loss: 0.0708 | 0.1204
Epoch 270/300, Loss: 0.0699 | 0.1203
Epoch 271/300, Loss: 0.0694 | 0.1202
Epoch 272/300, Loss: 0.0700 | 0.1201
Epoch 273/300, Loss: 0.0701 | 0.1201
Epoch 274/300, Loss: 0.0692 | 0.1200
Epoch 275/300, Loss: 0.0694 | 0.1200
Epoch 276/300, Loss: 0.0697 | 0.1200
Epoch 277/300, Loss: 0.0691 | 0.1200
Epoch 278/300, Loss: 0.0688 | 0.1200
Epoch 279/300, Loss: 0.0692 | 0.1202
Epoch 280/300, Loss: 0.0693 | 0.1201
Epoch 281/300, Loss: 0.0697 | 0.1199
Epoch 282/300, Loss: 0.0695 | 0.1199
Epoch 283/300, Loss: 0.0701 | 0.1199
Epoch 284/300, Loss: 0.0699 | 0.1200
Epoch 285/300, Loss: 0.0697 | 0.1200
Epoch 286/300, Loss: 0.0697 | 0.1199
Epoch 287/300, Loss: 0.0695 | 0.1198
Epoch 288/300, Loss: 0.0689 | 0.1197
Epoch 289/300, Loss: 0.0695 | 0.1197
Epoch 290/300, Loss: 0.0696 | 0.1196
Epoch 291/300, Loss: 0.0694 | 0.1196
Epoch 292/300, Loss: 0.0691 | 0.1196
Epoch 293/300, Loss: 0.0694 | 0.1196
Epoch 294/300, Loss: 0.0694 | 0.1196
Epoch 295/300, Loss: 0.0695 | 0.1197
Epoch 296/300, Loss: 0.0691 | 0.1197
Epoch 297/300, Loss: 0.0695 | 0.1197
Epoch 298/300, Loss: 0.0692 | 0.1197
Epoch 299/300, Loss: 0.0692 | 0.1196
Epoch 300/300, Loss: 0.0691 | 0.1196
Runtime (seconds): 87.02873921394348
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3121.664369926788
RMSE: 55.871856689453125
MAE: 55.871856689453125
R-squared: nan
[203.14813]
