[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
/home/raikakoki/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
Dataset created successfully.
Epoch 1/100, Training Loss: 0.3637, Validation Loss: 0.2311
Epoch 2/100, Training Loss: 0.1504, Validation Loss: 0.1315
Epoch 3/100, Training Loss: 0.1150, Validation Loss: 0.2184
Epoch 4/100, Training Loss: 0.1367, Validation Loss: 0.1483
Epoch 5/100, Training Loss: 0.0952, Validation Loss: 0.2092
Epoch 6/100, Training Loss: 0.0959, Validation Loss: 0.0771
Epoch 7/100, Training Loss: 0.0812, Validation Loss: 0.1856
Epoch 8/100, Training Loss: 0.0754, Validation Loss: 0.0739
Epoch 9/100, Training Loss: 0.0745, Validation Loss: 0.1655
Epoch 10/100, Training Loss: 0.0639, Validation Loss: 0.0692
Epoch 11/100, Training Loss: 0.0548, Validation Loss: 0.1273
Epoch 12/100, Training Loss: 0.0495, Validation Loss: 0.0433
Epoch 13/100, Training Loss: 0.0492, Validation Loss: 0.1100
Epoch 14/100, Training Loss: 0.0401, Validation Loss: 0.0376
Epoch 15/100, Training Loss: 0.0395, Validation Loss: 0.0937
Epoch 16/100, Training Loss: 0.0351, Validation Loss: 0.0371
Epoch 17/100, Training Loss: 0.0365, Validation Loss: 0.0857
Epoch 18/100, Training Loss: 0.0309, Validation Loss: 0.0318
Epoch 19/100, Training Loss: 0.0324, Validation Loss: 0.0777
Epoch 20/100, Training Loss: 0.0293, Validation Loss: 0.0294
Epoch 21/100, Training Loss: 0.0286, Validation Loss: 0.0688
Epoch 22/100, Training Loss: 0.0252, Validation Loss: 0.0287
Epoch 23/100, Training Loss: 0.0264, Validation Loss: 0.0623
Epoch 24/100, Training Loss: 0.0232, Validation Loss: 0.0275
Epoch 25/100, Training Loss: 0.0236, Validation Loss: 0.0563
Epoch 26/100, Training Loss: 0.0218, Validation Loss: 0.0266
Epoch 27/100, Training Loss: 0.0219, Validation Loss: 0.0507
Epoch 28/100, Training Loss: 0.0199, Validation Loss: 0.0266
Epoch 29/100, Training Loss: 0.0203, Validation Loss: 0.0465
Epoch 30/100, Training Loss: 0.0190, Validation Loss: 0.0262
Epoch 31/100, Training Loss: 0.0190, Validation Loss: 0.0424
Epoch 32/100, Training Loss: 0.0179, Validation Loss: 0.0262
Epoch 33/100, Training Loss: 0.0180, Validation Loss: 0.0393
Epoch 34/100, Training Loss: 0.0171, Validation Loss: 0.0263
Epoch 35/100, Training Loss: 0.0171, Validation Loss: 0.0363
Epoch 36/100, Training Loss: 0.0164, Validation Loss: 0.0264
Epoch 37/100, Training Loss: 0.0164, Validation Loss: 0.0340
Epoch 38/100, Training Loss: 0.0159, Validation Loss: 0.0266
Epoch 39/100, Training Loss: 0.0158, Validation Loss: 0.0318
Epoch 40/100, Training Loss: 0.0154, Validation Loss: 0.0267
Epoch 41/100, Training Loss: 0.0153, Validation Loss: 0.0302
Epoch 42/100, Training Loss: 0.0151, Validation Loss: 0.0268
Epoch 43/100, Training Loss: 0.0150, Validation Loss: 0.0288
Epoch 44/100, Training Loss: 0.0148, Validation Loss: 0.0268
Epoch 45/100, Training Loss: 0.0147, Validation Loss: 0.0277
Epoch 46/100, Training Loss: 0.0145, Validation Loss: 0.0267
Epoch 47/100, Training Loss: 0.0144, Validation Loss: 0.0270
Epoch 48/100, Training Loss: 0.0143, Validation Loss: 0.0264
Epoch 49/100, Training Loss: 0.0142, Validation Loss: 0.0264
Epoch 50/100, Training Loss: 0.0141, Validation Loss: 0.0261
Epoch 51/100, Training Loss: 0.0140, Validation Loss: 0.0259
Epoch 52/100, Training Loss: 0.0140, Validation Loss: 0.0258
Epoch 53/100, Training Loss: 0.0139, Validation Loss: 0.0256
Epoch 54/100, Training Loss: 0.0138, Validation Loss: 0.0254
Epoch 55/100, Training Loss: 0.0137, Validation Loss: 0.0252
Epoch 56/100, Training Loss: 0.0137, Validation Loss: 0.0251
Epoch 57/100, Training Loss: 0.0136, Validation Loss: 0.0250
Epoch 58/100, Training Loss: 0.0136, Validation Loss: 0.0248
Epoch 59/100, Training Loss: 0.0135, Validation Loss: 0.0247
Epoch 60/100, Training Loss: 0.0135, Validation Loss: 0.0246
Epoch 61/100, Training Loss: 0.0134, Validation Loss: 0.0245
Epoch 62/100, Training Loss: 0.0134, Validation Loss: 0.0244
Epoch 63/100, Training Loss: 0.0133, Validation Loss: 0.0243
Epoch 64/100, Training Loss: 0.0133, Validation Loss: 0.0242
Epoch 65/100, Training Loss: 0.0132, Validation Loss: 0.0241
Epoch 66/100, Training Loss: 0.0132, Validation Loss: 0.0240
Epoch 67/100, Training Loss: 0.0132, Validation Loss: 0.0239
Epoch 68/100, Training Loss: 0.0131, Validation Loss: 0.0238
Epoch 69/100, Training Loss: 0.0131, Validation Loss: 0.0237
Epoch 70/100, Training Loss: 0.0131, Validation Loss: 0.0237
Epoch 71/100, Training Loss: 0.0130, Validation Loss: 0.0236
Epoch 72/100, Training Loss: 0.0130, Validation Loss: 0.0235
Epoch 73/100, Training Loss: 0.0130, Validation Loss: 0.0234
Epoch 74/100, Training Loss: 0.0130, Validation Loss: 0.0234
Epoch 75/100, Training Loss: 0.0129, Validation Loss: 0.0233
Epoch 76/100, Training Loss: 0.0129, Validation Loss: 0.0233
Epoch 77/100, Training Loss: 0.0129, Validation Loss: 0.0232
Epoch 78/100, Training Loss: 0.0129, Validation Loss: 0.0232
Epoch 79/100, Training Loss: 0.0129, Validation Loss: 0.0231
Epoch 80/100, Training Loss: 0.0128, Validation Loss: 0.0231
Epoch 81/100, Training Loss: 0.0128, Validation Loss: 0.0230
Epoch 82/100, Training Loss: 0.0128, Validation Loss: 0.0230
Epoch 83/100, Training Loss: 0.0128, Validation Loss: 0.0229
Epoch 84/100, Training Loss: 0.0128, Validation Loss: 0.0229
Epoch 85/100, Training Loss: 0.0128, Validation Loss: 0.0229
Epoch 86/100, Training Loss: 0.0127, Validation Loss: 0.0228
Epoch 87/100, Training Loss: 0.0127, Validation Loss: 0.0228
Epoch 88/100, Training Loss: 0.0127, Validation Loss: 0.0228
Epoch 89/100, Training Loss: 0.0127, Validation Loss: 0.0227
Epoch 90/100, Training Loss: 0.0127, Validation Loss: 0.0227
Epoch 91/100, Training Loss: 0.0127, Validation Loss: 0.0227
Epoch 92/100, Training Loss: 0.0127, Validation Loss: 0.0227
Epoch 93/100, Training Loss: 0.0127, Validation Loss: 0.0226
Epoch 94/100, Training Loss: 0.0127, Validation Loss: 0.0226
Epoch 95/100, Training Loss: 0.0126, Validation Loss: 0.0226
Epoch 96/100, Training Loss: 0.0126, Validation Loss: 0.0226
Epoch 97/100, Training Loss: 0.0126, Validation Loss: 0.0225
Epoch 98/100, Training Loss: 0.0126, Validation Loss: 0.0225
Epoch 99/100, Training Loss: 0.0126, Validation Loss: 0.0225
Epoch 100/100, Training Loss: 0.0126, Validation Loss: 0.0225
['2023-05-17', '2023-05-18', '2023-05-19', '2023-05-22', '2023-05-23', '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-30', '2023-05-31']
tensor([[[ 1.3244,  0.8188,  0.1978, -0.7242,  1.5041]]])
[171.5791626  173.9239502  174.0332489  173.07940674 170.45639038
 170.73458862 171.87721252 174.30149841 176.1594696  171.40447998]
[171.5791626  173.9239502  174.0332489  173.07940674 170.45639038
 170.73458862 171.87721252 174.30149841 176.1594696  176.10980225]
/mnt/c/Users/RAIKA KOKI/B4研究/Multi_iTransformer/demo.py:115: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_apple_price = predicted_apple_price.cpu().numpy().flatten() * std_list[0] + mean_list[0]  # Using AAPL normalization factors
/mnt/c/Users/RAIKA KOKI/B4研究/Multi_iTransformer/demo.py:147: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()