[32m[I 2025-02-03 15:02:20,747][0m A new study created in memory with name: no-name-32a15121-8225-45ec-84f3-cbab491db794[0m
[32m[I 2025-02-03 15:03:37,263][0m Trial 0 finished with value: 0.23919139966882508 and parameters: {'observation_period_num': 235, 'train_rates': 0.9469119555218404, 'learning_rate': 2.571846867464956e-06, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8556752394680327}. Best is trial 0 with value: 0.23919139966882508.[0m
[32m[I 2025-02-03 15:05:29,874][0m Trial 1 finished with value: 0.05011532083153725 and parameters: {'observation_period_num': 17, 'train_rates': 0.9839594701673042, 'learning_rate': 5.013897392661957e-05, 'batch_size': 54, 'step_size': 9, 'gamma': 0.7866858545151467}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:08:19,870][0m Trial 2 finished with value: 0.1190235185803789 and parameters: {'observation_period_num': 36, 'train_rates': 0.8646507766877485, 'learning_rate': 2.7541657704370795e-06, 'batch_size': 32, 'step_size': 9, 'gamma': 0.8166992065263297}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:08:44,106][0m Trial 3 finished with value: 0.651273237845628 and parameters: {'observation_period_num': 133, 'train_rates': 0.8029866696047648, 'learning_rate': 1.9838809381332458e-06, 'batch_size': 227, 'step_size': 5, 'gamma': 0.8099248902972294}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:09:59,384][0m Trial 4 finished with value: 0.184668309902404 and parameters: {'observation_period_num': 233, 'train_rates': 0.7404460784443564, 'learning_rate': 2.0893418937308602e-05, 'batch_size': 62, 'step_size': 1, 'gamma': 0.9400389898589986}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:10:48,134][0m Trial 5 finished with value: 0.13811712829165634 and parameters: {'observation_period_num': 171, 'train_rates': 0.8928353472204794, 'learning_rate': 1.2041662397635544e-05, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8854525045537012}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:11:12,564][0m Trial 6 finished with value: 0.1609592067125516 and parameters: {'observation_period_num': 240, 'train_rates': 0.7705142248086277, 'learning_rate': 2.5775054098181236e-05, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8104862665103814}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:12:05,702][0m Trial 7 finished with value: 0.4044778838768557 and parameters: {'observation_period_num': 158, 'train_rates': 0.6419601278231906, 'learning_rate': 2.1604677446917026e-06, 'batch_size': 82, 'step_size': 12, 'gamma': 0.7568632014970674}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:12:36,393][0m Trial 8 finished with value: 0.1383640513259129 and parameters: {'observation_period_num': 205, 'train_rates': 0.7514328466310523, 'learning_rate': 2.9047729794733077e-05, 'batch_size': 170, 'step_size': 5, 'gamma': 0.936683483489624}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:13:13,397][0m Trial 9 finished with value: 0.07780466920372406 and parameters: {'observation_period_num': 22, 'train_rates': 0.6201441977258881, 'learning_rate': 5.235293524539138e-05, 'batch_size': 128, 'step_size': 4, 'gamma': 0.8721074635786042}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:18:22,006][0m Trial 10 finished with value: 0.07096349490540368 and parameters: {'observation_period_num': 77, 'train_rates': 0.9879386322106789, 'learning_rate': 0.00046161432144616347, 'batch_size': 19, 'step_size': 9, 'gamma': 0.7627679358617342}. Best is trial 1 with value: 0.05011532083153725.[0m
[32m[I 2025-02-03 15:23:43,445][0m Trial 11 finished with value: 0.04632546669588639 and parameters: {'observation_period_num': 74, 'train_rates': 0.9776778717564358, 'learning_rate': 0.000612443759895748, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7629918351969223}. Best is trial 11 with value: 0.04632546669588639.[0m
[32m[I 2025-02-03 15:29:06,515][0m Trial 12 finished with value: 0.045400401467786115 and parameters: {'observation_period_num': 75, 'train_rates': 0.9825604719742524, 'learning_rate': 0.0008308513375113617, 'batch_size': 18, 'step_size': 11, 'gamma': 0.7778293497020535}. Best is trial 12 with value: 0.045400401467786115.[0m
[32m[I 2025-02-03 15:29:39,322][0m Trial 13 finished with value: 0.041744607026284596 and parameters: {'observation_period_num': 86, 'train_rates': 0.9077915371067241, 'learning_rate': 0.0008270534492855167, 'batch_size': 175, 'step_size': 11, 'gamma': 0.7532490522514506}. Best is trial 13 with value: 0.041744607026284596.[0m
[32m[I 2025-02-03 15:30:12,583][0m Trial 14 finished with value: 0.048464070158314616 and parameters: {'observation_period_num': 93, 'train_rates': 0.9035121332306477, 'learning_rate': 0.00021142245997696319, 'batch_size': 175, 'step_size': 12, 'gamma': 0.8307601463552775}. Best is trial 13 with value: 0.041744607026284596.[0m
[32m[I 2025-02-03 15:30:45,317][0m Trial 15 finished with value: 0.04860785282277442 and parameters: {'observation_period_num': 105, 'train_rates': 0.8653731710176581, 'learning_rate': 0.00017693573490285957, 'batch_size': 179, 'step_size': 11, 'gamma': 0.9745629333082623}. Best is trial 13 with value: 0.041744607026284596.[0m
[32m[I 2025-02-03 15:31:11,356][0m Trial 16 finished with value: 0.03617590293288231 and parameters: {'observation_period_num': 52, 'train_rates': 0.9300300723062463, 'learning_rate': 0.0009382654377759444, 'batch_size': 255, 'step_size': 7, 'gamma': 0.7871698632085946}. Best is trial 16 with value: 0.03617590293288231.[0m
[32m[I 2025-02-03 15:31:36,077][0m Trial 17 finished with value: 0.057309152169360056 and parameters: {'observation_period_num': 121, 'train_rates': 0.8416714101439953, 'learning_rate': 0.00021990950836225495, 'batch_size': 248, 'step_size': 6, 'gamma': 0.8459857992878351}. Best is trial 16 with value: 0.03617590293288231.[0m
[32m[I 2025-02-03 15:32:09,654][0m Trial 18 finished with value: 0.03457784802417889 and parameters: {'observation_period_num': 48, 'train_rates': 0.926790695055083, 'learning_rate': 0.000984713260055402, 'batch_size': 202, 'step_size': 7, 'gamma': 0.7898832440079513}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:32:34,429][0m Trial 19 finished with value: 0.059986491256561976 and parameters: {'observation_period_num': 48, 'train_rates': 0.6906230499756711, 'learning_rate': 9.404499109147597e-05, 'batch_size': 208, 'step_size': 7, 'gamma': 0.902445384061678}. Best is trial 18 with value: 0.03457784802417889.[0m
Early stopping at epoch 88
[32m[I 2025-02-03 15:32:58,738][0m Trial 20 finished with value: 0.724407970905304 and parameters: {'observation_period_num': 51, 'train_rates': 0.9330770059660064, 'learning_rate': 6.375605723403958e-06, 'batch_size': 246, 'step_size': 2, 'gamma': 0.7853669724574404}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:33:30,162][0m Trial 21 finished with value: 0.039362581029756746 and parameters: {'observation_period_num': 54, 'train_rates': 0.9309997731000288, 'learning_rate': 0.0009677910432928419, 'batch_size': 198, 'step_size': 7, 'gamma': 0.7925846361015374}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:33:58,327][0m Trial 22 finished with value: 0.051556211709976194 and parameters: {'observation_period_num': 52, 'train_rates': 0.8195790151491019, 'learning_rate': 0.0004012024217840767, 'batch_size': 210, 'step_size': 3, 'gamma': 0.7970240375342316}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:34:29,597][0m Trial 23 finished with value: 0.038762178272008896 and parameters: {'observation_period_num': 9, 'train_rates': 0.9395590815083202, 'learning_rate': 0.0003473659676996683, 'batch_size': 207, 'step_size': 7, 'gamma': 0.8343713507933028}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:34:54,307][0m Trial 24 finished with value: 0.041604507714509964 and parameters: {'observation_period_num': 9, 'train_rates': 0.9476257502045867, 'learning_rate': 0.00032482036888713267, 'batch_size': 254, 'step_size': 7, 'gamma': 0.8350479192302871}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:35:32,442][0m Trial 25 finished with value: 0.04369473803646404 and parameters: {'observation_period_num': 5, 'train_rates': 0.8848229974580789, 'learning_rate': 0.00011634197066200451, 'batch_size': 158, 'step_size': 8, 'gamma': 0.823031679795062}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:36:10,992][0m Trial 26 finished with value: 0.03563832557101629 and parameters: {'observation_period_num': 31, 'train_rates': 0.8368482360154154, 'learning_rate': 0.0003252751286328436, 'batch_size': 148, 'step_size': 5, 'gamma': 0.8567053764395403}. Best is trial 18 with value: 0.03457784802417889.[0m
[32m[I 2025-02-03 15:37:06,237][0m Trial 27 finished with value: 0.031725526039659134 and parameters: {'observation_period_num': 31, 'train_rates': 0.8546345859147816, 'learning_rate': 0.0005400597641878482, 'batch_size': 104, 'step_size': 5, 'gamma': 0.898330923784688}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:38:01,455][0m Trial 28 finished with value: 0.03836876645180412 and parameters: {'observation_period_num': 30, 'train_rates': 0.8335383943359519, 'learning_rate': 0.00011266459194021244, 'batch_size': 99, 'step_size': 4, 'gamma': 0.9139367481248131}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:38:38,606][0m Trial 29 finished with value: 0.03593088050940118 and parameters: {'observation_period_num': 35, 'train_rates': 0.7884350798651193, 'learning_rate': 0.0005449309952795136, 'batch_size': 147, 'step_size': 5, 'gamma': 0.863808881548705}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:39:27,501][0m Trial 30 finished with value: 0.04138872892824265 and parameters: {'observation_period_num': 62, 'train_rates': 0.8538883730839886, 'learning_rate': 0.0003008594204771503, 'batch_size': 113, 'step_size': 3, 'gamma': 0.8766856887173581}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:40:06,533][0m Trial 31 finished with value: 0.035144940617784955 and parameters: {'observation_period_num': 35, 'train_rates': 0.7852936037033905, 'learning_rate': 0.0005659082904251101, 'batch_size': 143, 'step_size': 5, 'gamma': 0.8642286246785992}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:40:45,588][0m Trial 32 finished with value: 0.03469922367590838 and parameters: {'observation_period_num': 37, 'train_rates': 0.7153714692520574, 'learning_rate': 0.000515096183579753, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8937365700490871}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:41:41,224][0m Trial 33 finished with value: 0.03813294151326725 and parameters: {'observation_period_num': 22, 'train_rates': 0.7119700176638993, 'learning_rate': 0.0005753915753601786, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8971970585967434}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:42:16,787][0m Trial 34 finished with value: 0.045617424369115256 and parameters: {'observation_period_num': 41, 'train_rates': 0.6820645803082597, 'learning_rate': 0.00016469426687799394, 'batch_size': 138, 'step_size': 4, 'gamma': 0.9295971819360149}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:44:09,856][0m Trial 35 finished with value: 0.3337683402279936 and parameters: {'observation_period_num': 68, 'train_rates': 0.7312373092413099, 'learning_rate': 6.220188888488007e-05, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9642287268373803}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:44:55,170][0m Trial 36 finished with value: 0.045873059912442324 and parameters: {'observation_period_num': 102, 'train_rates': 0.8010299462390674, 'learning_rate': 0.0006925794656924366, 'batch_size': 118, 'step_size': 8, 'gamma': 0.9153346723417535}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:46:06,296][0m Trial 37 finished with value: 0.5194013901054859 and parameters: {'observation_period_num': 139, 'train_rates': 0.6680675259392713, 'learning_rate': 5.406754499456429e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.893286654186637}. Best is trial 27 with value: 0.031725526039659134.[0m
[32m[I 2025-02-03 15:47:02,770][0m Trial 38 finished with value: 0.030767654915650686 and parameters: {'observation_period_num': 23, 'train_rates': 0.8723686481403631, 'learning_rate': 0.0004744536226561014, 'batch_size': 101, 'step_size': 2, 'gamma': 0.9480681309288531}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:48:01,414][0m Trial 39 finished with value: 0.03493660666486796 and parameters: {'observation_period_num': 22, 'train_rates': 0.8785316772322268, 'learning_rate': 0.0002513115174535069, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9510302020888579}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:49:18,547][0m Trial 40 finished with value: 0.07300813227545383 and parameters: {'observation_period_num': 20, 'train_rates': 0.7660494367340965, 'learning_rate': 1.5263129990540643e-05, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9219491479178413}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:50:15,587][0m Trial 41 finished with value: 0.0358441719759817 and parameters: {'observation_period_num': 21, 'train_rates': 0.8759677817477936, 'learning_rate': 0.0002574902946012991, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9530516864538008}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:51:25,098][0m Trial 42 finished with value: 0.03538868901057121 and parameters: {'observation_period_num': 38, 'train_rates': 0.9068183286085127, 'learning_rate': 0.00042223864003493414, 'batch_size': 85, 'step_size': 2, 'gamma': 0.9511126355070668}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:52:14,053][0m Trial 43 finished with value: 0.08694972842931747 and parameters: {'observation_period_num': 197, 'train_rates': 0.963394082857984, 'learning_rate': 0.00015445251077601455, 'batch_size': 124, 'step_size': 1, 'gamma': 0.977194783880664}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:53:05,878][0m Trial 44 finished with value: 0.03724328445312672 and parameters: {'observation_period_num': 65, 'train_rates': 0.8163125957531948, 'learning_rate': 0.0004686314196408863, 'batch_size': 104, 'step_size': 4, 'gamma': 0.9401127798650297}. Best is trial 38 with value: 0.030767654915650686.[0m
[32m[I 2025-02-03 15:54:51,228][0m Trial 45 finished with value: 0.028833073140133326 and parameters: {'observation_period_num': 14, 'train_rates': 0.8605942405415258, 'learning_rate': 0.0006855919713370899, 'batch_size': 52, 'step_size': 8, 'gamma': 0.9848653423520041}. Best is trial 45 with value: 0.028833073140133326.[0m
[32m[I 2025-02-03 15:56:56,295][0m Trial 46 finished with value: 0.02545127473232976 and parameters: {'observation_period_num': 13, 'train_rates': 0.8593842915871747, 'learning_rate': 0.0007269028605862533, 'batch_size': 44, 'step_size': 10, 'gamma': 0.8833972858769691}. Best is trial 46 with value: 0.02545127473232976.[0m
[32m[I 2025-02-03 15:58:57,847][0m Trial 47 finished with value: 0.026095788788231086 and parameters: {'observation_period_num': 11, 'train_rates': 0.8533646588449352, 'learning_rate': 0.000742005186510952, 'batch_size': 45, 'step_size': 10, 'gamma': 0.8824696407229992}. Best is trial 46 with value: 0.02545127473232976.[0m
[32m[I 2025-02-03 16:01:09,512][0m Trial 48 finished with value: 0.026039744429807696 and parameters: {'observation_period_num': 14, 'train_rates': 0.8534662345133892, 'learning_rate': 0.0007286458300097474, 'batch_size': 42, 'step_size': 10, 'gamma': 0.8847176456751733}. Best is trial 46 with value: 0.02545127473232976.[0m
[32m[I 2025-02-03 16:03:14,200][0m Trial 49 finished with value: 0.02663163516779676 and parameters: {'observation_period_num': 12, 'train_rates': 0.8959835568317572, 'learning_rate': 0.0006714755104550332, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9895649527305923}. Best is trial 46 with value: 0.02545127473232976.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_BA_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2380 | 0.0778
Epoch 2/300, Loss: 0.1209 | 0.0701
Epoch 3/300, Loss: 0.0936 | 0.0719
Epoch 4/300, Loss: 0.0793 | 0.0610
Epoch 5/300, Loss: 0.0647 | 0.0537
Epoch 6/300, Loss: 0.0587 | 0.0489
Epoch 7/300, Loss: 0.0501 | 0.0460
Epoch 8/300, Loss: 0.0474 | 0.0525
Epoch 9/300, Loss: 0.0513 | 0.0491
Epoch 10/300, Loss: 0.0618 | 0.0519
Epoch 11/300, Loss: 0.0489 | 0.0470
Epoch 12/300, Loss: 0.0401 | 0.0406
Epoch 13/300, Loss: 0.0412 | 0.0396
Epoch 14/300, Loss: 0.0367 | 0.0394
Epoch 15/300, Loss: 0.0343 | 0.0369
Epoch 16/300, Loss: 0.0330 | 0.0345
Epoch 17/300, Loss: 0.0337 | 0.0410
Epoch 18/300, Loss: 0.0407 | 0.0346
Epoch 19/300, Loss: 0.0330 | 0.0361
Epoch 20/300, Loss: 0.0292 | 0.0359
Epoch 21/300, Loss: 0.0290 | 0.0355
Epoch 22/300, Loss: 0.0285 | 0.0324
Epoch 23/300, Loss: 0.0279 | 0.0330
Epoch 24/300, Loss: 0.0268 | 0.0305
Epoch 25/300, Loss: 0.0260 | 0.0320
Epoch 26/300, Loss: 0.0259 | 0.0355
Epoch 27/300, Loss: 0.0267 | 0.0331
Epoch 28/300, Loss: 0.0284 | 0.0381
Epoch 29/300, Loss: 0.0260 | 0.0311
Epoch 30/300, Loss: 0.0252 | 0.0336
Epoch 31/300, Loss: 0.0239 | 0.0297
Epoch 32/300, Loss: 0.0253 | 0.0292
Epoch 33/300, Loss: 0.0257 | 0.0276
Epoch 34/300, Loss: 0.0243 | 0.0281
Epoch 35/300, Loss: 0.0227 | 0.0349
Epoch 36/300, Loss: 0.0254 | 0.0279
Epoch 37/300, Loss: 0.0290 | 0.0365
Epoch 38/300, Loss: 0.0327 | 0.0363
Epoch 39/300, Loss: 0.0225 | 0.0287
Epoch 40/300, Loss: 0.0250 | 0.0280
Epoch 41/300, Loss: 0.0214 | 0.0235
Epoch 42/300, Loss: 0.0208 | 0.0237
Epoch 43/300, Loss: 0.0211 | 0.0235
Epoch 44/300, Loss: 0.0206 | 0.0257
Epoch 45/300, Loss: 0.0255 | 0.0306
Epoch 46/300, Loss: 0.0206 | 0.0268
Epoch 47/300, Loss: 0.0242 | 0.0295
Epoch 48/300, Loss: 0.0194 | 0.0227
Epoch 49/300, Loss: 0.0198 | 0.0240
Epoch 50/300, Loss: 0.0217 | 0.0267
Epoch 51/300, Loss: 0.0256 | 0.0247
Epoch 52/300, Loss: 0.0214 | 0.0239
Epoch 53/300, Loss: 0.0202 | 0.0233
Epoch 54/300, Loss: 0.0196 | 0.0232
Epoch 55/300, Loss: 0.0214 | 0.0248
Epoch 56/300, Loss: 0.0179 | 0.0215
Epoch 57/300, Loss: 0.0177 | 0.0243
Epoch 58/300, Loss: 0.0173 | 0.0219
Epoch 59/300, Loss: 0.0181 | 0.0256
Epoch 60/300, Loss: 0.0172 | 0.0215
Epoch 61/300, Loss: 0.0174 | 0.0263
Epoch 62/300, Loss: 0.0168 | 0.0212
Epoch 63/300, Loss: 0.0167 | 0.0249
Epoch 64/300, Loss: 0.0162 | 0.0214
Epoch 65/300, Loss: 0.0166 | 0.0245
Epoch 66/300, Loss: 0.0157 | 0.0216
Epoch 67/300, Loss: 0.0158 | 0.0252
Epoch 68/300, Loss: 0.0152 | 0.0217
Epoch 69/300, Loss: 0.0153 | 0.0253
Epoch 70/300, Loss: 0.0148 | 0.0217
Epoch 71/300, Loss: 0.0151 | 0.0268
Epoch 72/300, Loss: 0.0144 | 0.0220
Epoch 73/300, Loss: 0.0142 | 0.0259
Epoch 74/300, Loss: 0.0139 | 0.0230
Epoch 75/300, Loss: 0.0141 | 0.0254
Epoch 76/300, Loss: 0.0140 | 0.0230
Epoch 77/300, Loss: 0.0140 | 0.0244
Epoch 78/300, Loss: 0.0135 | 0.0218
Epoch 79/300, Loss: 0.0135 | 0.0239
Epoch 80/300, Loss: 0.0131 | 0.0218
Epoch 81/300, Loss: 0.0132 | 0.0234
Epoch 82/300, Loss: 0.0129 | 0.0217
Epoch 83/300, Loss: 0.0128 | 0.0229
Epoch 84/300, Loss: 0.0127 | 0.0218
Epoch 85/300, Loss: 0.0127 | 0.0228
Epoch 86/300, Loss: 0.0125 | 0.0217
Epoch 87/300, Loss: 0.0127 | 0.0235
Epoch 88/300, Loss: 0.0123 | 0.0216
Epoch 89/300, Loss: 0.0122 | 0.0225
Epoch 90/300, Loss: 0.0121 | 0.0221
Epoch 91/300, Loss: 0.0125 | 0.0227
Epoch 92/300, Loss: 0.0119 | 0.0217
Epoch 93/300, Loss: 0.0118 | 0.0217
Epoch 94/300, Loss: 0.0116 | 0.0219
Epoch 95/300, Loss: 0.0118 | 0.0219
Epoch 96/300, Loss: 0.0118 | 0.0222
Epoch 97/300, Loss: 0.0120 | 0.0223
Epoch 98/300, Loss: 0.0117 | 0.0220
Epoch 99/300, Loss: 0.0113 | 0.0223
Epoch 100/300, Loss: 0.0113 | 0.0222
Epoch 101/300, Loss: 0.0110 | 0.0225
Epoch 102/300, Loss: 0.0108 | 0.0219
Epoch 103/300, Loss: 0.0106 | 0.0220
Epoch 104/300, Loss: 0.0105 | 0.0216
Epoch 105/300, Loss: 0.0103 | 0.0218
Epoch 106/300, Loss: 0.0104 | 0.0216
Epoch 107/300, Loss: 0.0102 | 0.0217
Epoch 108/300, Loss: 0.0103 | 0.0214
Epoch 109/300, Loss: 0.0101 | 0.0217
Epoch 110/300, Loss: 0.0101 | 0.0215
Epoch 111/300, Loss: 0.0100 | 0.0217
Epoch 112/300, Loss: 0.0100 | 0.0216
Epoch 113/300, Loss: 0.0100 | 0.0217
Epoch 114/300, Loss: 0.0100 | 0.0218
Epoch 115/300, Loss: 0.0099 | 0.0217
Epoch 116/300, Loss: 0.0100 | 0.0220
Epoch 117/300, Loss: 0.0099 | 0.0216
Epoch 118/300, Loss: 0.0100 | 0.0217
Epoch 119/300, Loss: 0.0100 | 0.0216
Epoch 120/300, Loss: 0.0102 | 0.0221
Epoch 121/300, Loss: 0.0106 | 0.0220
Epoch 122/300, Loss: 0.0106 | 0.0225
Epoch 123/300, Loss: 0.0107 | 0.0226
Epoch 124/300, Loss: 0.0106 | 0.0227
Epoch 125/300, Loss: 0.0105 | 0.0227
Epoch 126/300, Loss: 0.0106 | 0.0222
Epoch 127/300, Loss: 0.0112 | 0.0232
Epoch 128/300, Loss: 0.0112 | 0.0228
Epoch 129/300, Loss: 0.0105 | 0.0224
Epoch 130/300, Loss: 0.0098 | 0.0220
Epoch 131/300, Loss: 0.0094 | 0.0219
Epoch 132/300, Loss: 0.0091 | 0.0217
Epoch 133/300, Loss: 0.0089 | 0.0218
Epoch 134/300, Loss: 0.0088 | 0.0217
Epoch 135/300, Loss: 0.0088 | 0.0218
Epoch 136/300, Loss: 0.0087 | 0.0218
Epoch 137/300, Loss: 0.0087 | 0.0217
Epoch 138/300, Loss: 0.0086 | 0.0218
Epoch 139/300, Loss: 0.0086 | 0.0218
Epoch 140/300, Loss: 0.0086 | 0.0218
Epoch 141/300, Loss: 0.0085 | 0.0218
Epoch 142/300, Loss: 0.0085 | 0.0218
Epoch 143/300, Loss: 0.0085 | 0.0219
Epoch 144/300, Loss: 0.0084 | 0.0219
Epoch 145/300, Loss: 0.0084 | 0.0219
Epoch 146/300, Loss: 0.0084 | 0.0220
Epoch 147/300, Loss: 0.0084 | 0.0220
Epoch 148/300, Loss: 0.0084 | 0.0220
Epoch 149/300, Loss: 0.0083 | 0.0220
Epoch 150/300, Loss: 0.0083 | 0.0220
Epoch 151/300, Loss: 0.0083 | 0.0220
Epoch 152/300, Loss: 0.0083 | 0.0221
Epoch 153/300, Loss: 0.0083 | 0.0221
Epoch 154/300, Loss: 0.0082 | 0.0222
Epoch 155/300, Loss: 0.0082 | 0.0222
Epoch 156/300, Loss: 0.0082 | 0.0223
Epoch 157/300, Loss: 0.0081 | 0.0223
Epoch 158/300, Loss: 0.0081 | 0.0223
Epoch 159/300, Loss: 0.0081 | 0.0222
Epoch 160/300, Loss: 0.0081 | 0.0222
Epoch 161/300, Loss: 0.0080 | 0.0222
Epoch 162/300, Loss: 0.0080 | 0.0222
Epoch 163/300, Loss: 0.0080 | 0.0222
Epoch 164/300, Loss: 0.0080 | 0.0222
Epoch 165/300, Loss: 0.0079 | 0.0223
Epoch 166/300, Loss: 0.0079 | 0.0223
Epoch 167/300, Loss: 0.0079 | 0.0223
Epoch 168/300, Loss: 0.0079 | 0.0223
Epoch 169/300, Loss: 0.0079 | 0.0224
Epoch 170/300, Loss: 0.0078 | 0.0224
Epoch 171/300, Loss: 0.0078 | 0.0224
Epoch 172/300, Loss: 0.0078 | 0.0224
Epoch 173/300, Loss: 0.0078 | 0.0224
Epoch 174/300, Loss: 0.0078 | 0.0224
Epoch 175/300, Loss: 0.0078 | 0.0224
Epoch 176/300, Loss: 0.0078 | 0.0225
Epoch 177/300, Loss: 0.0078 | 0.0225
Epoch 178/300, Loss: 0.0077 | 0.0225
Epoch 179/300, Loss: 0.0077 | 0.0225
Epoch 180/300, Loss: 0.0077 | 0.0225
Epoch 181/300, Loss: 0.0077 | 0.0226
Epoch 182/300, Loss: 0.0077 | 0.0226
Epoch 183/300, Loss: 0.0077 | 0.0226
Epoch 184/300, Loss: 0.0077 | 0.0226
Epoch 185/300, Loss: 0.0077 | 0.0227
Epoch 186/300, Loss: 0.0077 | 0.0227
Epoch 187/300, Loss: 0.0077 | 0.0228
Epoch 188/300, Loss: 0.0077 | 0.0228
Epoch 189/300, Loss: 0.0077 | 0.0228
Epoch 190/300, Loss: 0.0077 | 0.0228
Epoch 191/300, Loss: 0.0077 | 0.0228
Epoch 192/300, Loss: 0.0076 | 0.0228
Epoch 193/300, Loss: 0.0076 | 0.0228
Epoch 194/300, Loss: 0.0076 | 0.0228
Epoch 195/300, Loss: 0.0076 | 0.0228
Epoch 196/300, Loss: 0.0076 | 0.0228
Epoch 197/300, Loss: 0.0076 | 0.0228
Epoch 198/300, Loss: 0.0075 | 0.0228
Epoch 199/300, Loss: 0.0075 | 0.0228
Epoch 200/300, Loss: 0.0075 | 0.0228
Epoch 201/300, Loss: 0.0075 | 0.0227
Epoch 202/300, Loss: 0.0075 | 0.0227
Epoch 203/300, Loss: 0.0075 | 0.0227
Epoch 204/300, Loss: 0.0075 | 0.0227
Epoch 205/300, Loss: 0.0074 | 0.0227
Epoch 206/300, Loss: 0.0074 | 0.0226
Epoch 207/300, Loss: 0.0074 | 0.0226
Epoch 208/300, Loss: 0.0074 | 0.0226
Epoch 209/300, Loss: 0.0074 | 0.0227
Epoch 210/300, Loss: 0.0074 | 0.0227
Epoch 211/300, Loss: 0.0074 | 0.0227
Epoch 212/300, Loss: 0.0073 | 0.0227
Epoch 213/300, Loss: 0.0073 | 0.0227
Epoch 214/300, Loss: 0.0073 | 0.0227
Epoch 215/300, Loss: 0.0073 | 0.0227
Epoch 216/300, Loss: 0.0073 | 0.0227
Epoch 217/300, Loss: 0.0073 | 0.0227
Epoch 218/300, Loss: 0.0073 | 0.0228
Epoch 219/300, Loss: 0.0073 | 0.0228
Epoch 220/300, Loss: 0.0073 | 0.0228
Epoch 221/300, Loss: 0.0073 | 0.0228
Epoch 222/300, Loss: 0.0073 | 0.0228
Epoch 223/300, Loss: 0.0072 | 0.0228
Epoch 224/300, Loss: 0.0072 | 0.0228
Epoch 225/300, Loss: 0.0072 | 0.0228
Epoch 226/300, Loss: 0.0072 | 0.0228
Epoch 227/300, Loss: 0.0072 | 0.0228
Epoch 228/300, Loss: 0.0072 | 0.0228
Epoch 229/300, Loss: 0.0072 | 0.0228
Epoch 230/300, Loss: 0.0072 | 0.0229
Epoch 231/300, Loss: 0.0072 | 0.0229
Epoch 232/300, Loss: 0.0072 | 0.0229
Epoch 233/300, Loss: 0.0072 | 0.0229
Epoch 234/300, Loss: 0.0072 | 0.0229
Epoch 235/300, Loss: 0.0072 | 0.0229
Epoch 236/300, Loss: 0.0072 | 0.0229
Epoch 237/300, Loss: 0.0072 | 0.0229
Epoch 238/300, Loss: 0.0072 | 0.0229
Epoch 239/300, Loss: 0.0072 | 0.0229
Epoch 240/300, Loss: 0.0072 | 0.0229
Epoch 241/300, Loss: 0.0072 | 0.0229
Epoch 242/300, Loss: 0.0072 | 0.0229
Epoch 243/300, Loss: 0.0071 | 0.0229
Epoch 244/300, Loss: 0.0071 | 0.0229
Epoch 245/300, Loss: 0.0071 | 0.0229
Epoch 246/300, Loss: 0.0071 | 0.0229
Epoch 247/300, Loss: 0.0071 | 0.0229
Epoch 248/300, Loss: 0.0071 | 0.0229
Epoch 249/300, Loss: 0.0071 | 0.0230
Epoch 250/300, Loss: 0.0071 | 0.0230
Epoch 251/300, Loss: 0.0071 | 0.0230
Epoch 252/300, Loss: 0.0071 | 0.0230
Epoch 253/300, Loss: 0.0071 | 0.0230
Epoch 254/300, Loss: 0.0071 | 0.0230
Epoch 255/300, Loss: 0.0071 | 0.0230
Epoch 256/300, Loss: 0.0071 | 0.0230
Epoch 257/300, Loss: 0.0071 | 0.0230
Epoch 258/300, Loss: 0.0071 | 0.0230
Epoch 259/300, Loss: 0.0071 | 0.0230
Epoch 260/300, Loss: 0.0071 | 0.0230
Epoch 261/300, Loss: 0.0071 | 0.0230
Epoch 262/300, Loss: 0.0071 | 0.0230
Epoch 263/300, Loss: 0.0071 | 0.0230
Epoch 264/300, Loss: 0.0071 | 0.0230
Epoch 265/300, Loss: 0.0071 | 0.0230
Epoch 266/300, Loss: 0.0071 | 0.0230
Epoch 267/300, Loss: 0.0071 | 0.0230
Epoch 268/300, Loss: 0.0071 | 0.0230
Epoch 269/300, Loss: 0.0071 | 0.0230
Epoch 270/300, Loss: 0.0071 | 0.0230
Epoch 271/300, Loss: 0.0071 | 0.0230
Epoch 272/300, Loss: 0.0071 | 0.0230
Epoch 273/300, Loss: 0.0071 | 0.0230
Epoch 274/300, Loss: 0.0071 | 0.0230
Epoch 275/300, Loss: 0.0071 | 0.0230
Epoch 276/300, Loss: 0.0071 | 0.0230
Epoch 277/300, Loss: 0.0071 | 0.0230
Epoch 278/300, Loss: 0.0071 | 0.0230
Epoch 279/300, Loss: 0.0071 | 0.0230
Epoch 280/300, Loss: 0.0071 | 0.0230
Epoch 281/300, Loss: 0.0071 | 0.0230
Epoch 282/300, Loss: 0.0071 | 0.0230
Epoch 283/300, Loss: 0.0071 | 0.0230
Epoch 284/300, Loss: 0.0070 | 0.0230
Epoch 285/300, Loss: 0.0070 | 0.0230
Epoch 286/300, Loss: 0.0070 | 0.0230
Epoch 287/300, Loss: 0.0070 | 0.0230
Epoch 288/300, Loss: 0.0070 | 0.0230
Epoch 289/300, Loss: 0.0070 | 0.0230
Epoch 290/300, Loss: 0.0070 | 0.0231
Epoch 291/300, Loss: 0.0070 | 0.0231
Epoch 292/300, Loss: 0.0070 | 0.0231
Epoch 293/300, Loss: 0.0070 | 0.0231
Epoch 294/300, Loss: 0.0070 | 0.0231
Epoch 295/300, Loss: 0.0070 | 0.0231
Epoch 296/300, Loss: 0.0070 | 0.0231
Epoch 297/300, Loss: 0.0070 | 0.0231
Epoch 298/300, Loss: 0.0070 | 0.0231
Epoch 299/300, Loss: 0.0070 | 0.0231
Epoch 300/300, Loss: 0.0070 | 0.0231
Runtime (seconds): 376.71541237831116
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 4.387304056668654
RMSE: 2.0945892333984375
MAE: 2.0945892333984375
R-squared: nan
[158.76459]
