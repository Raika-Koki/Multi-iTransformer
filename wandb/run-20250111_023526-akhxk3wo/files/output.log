ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-11 02:35:28,453][0m A new study created in memory with name: no-name-daf1df70-cc5b-4419-aa6e-33ce02ba2620[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-11 02:36:27,995][0m Trial 0 finished with value: 0.06901426675194867 and parameters: {'observation_period_num': 63, 'train_rates': 0.7122279638489434, 'learning_rate': 0.00024750910051232056, 'batch_size': 78, 'step_size': 15, 'gamma': 0.7525331565029616}. Best is trial 0 with value: 0.06901426675194867.[0m
[32m[I 2025-01-11 02:36:52,720][0m Trial 1 finished with value: 0.7820899892764487 and parameters: {'observation_period_num': 104, 'train_rates': 0.8898947958969452, 'learning_rate': 6.778844486062603e-06, 'batch_size': 218, 'step_size': 11, 'gamma': 0.9860544778017848}. Best is trial 0 with value: 0.06901426675194867.[0m
[32m[I 2025-01-11 02:37:28,097][0m Trial 2 finished with value: 0.9485223889350891 and parameters: {'observation_period_num': 86, 'train_rates': 0.9588273076618323, 'learning_rate': 6.38262102106743e-06, 'batch_size': 167, 'step_size': 11, 'gamma': 0.7972187139536904}. Best is trial 0 with value: 0.06901426675194867.[0m
[32m[I 2025-01-11 02:38:42,546][0m Trial 3 finished with value: 0.05362059319338964 and parameters: {'observation_period_num': 49, 'train_rates': 0.7815265316650208, 'learning_rate': 0.00019305041294893175, 'batch_size': 65, 'step_size': 14, 'gamma': 0.8202068555920776}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:39:13,569][0m Trial 4 finished with value: 0.7285578060898749 and parameters: {'observation_period_num': 191, 'train_rates': 0.6192832976754982, 'learning_rate': 1.8857266061335133e-05, 'batch_size': 133, 'step_size': 1, 'gamma': 0.9372628936578468}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:41:03,267][0m Trial 5 finished with value: 0.15868897275018184 and parameters: {'observation_period_num': 222, 'train_rates': 0.6224142216008215, 'learning_rate': 7.647909433506187e-05, 'batch_size': 36, 'step_size': 13, 'gamma': 0.913877433425257}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:42:04,454][0m Trial 6 finished with value: 0.7053274924304628 and parameters: {'observation_period_num': 56, 'train_rates': 0.9253413599279409, 'learning_rate': 4.684423462891912e-06, 'batch_size': 90, 'step_size': 10, 'gamma': 0.9601766894411974}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:43:13,369][0m Trial 7 finished with value: 0.29737676601510493 and parameters: {'observation_period_num': 114, 'train_rates': 0.6322365185249228, 'learning_rate': 0.00010611648403499309, 'batch_size': 60, 'step_size': 2, 'gamma': 0.8609113345900268}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:43:38,974][0m Trial 8 finished with value: 2.2610118950114533 and parameters: {'observation_period_num': 128, 'train_rates': 0.9037388795158583, 'learning_rate': 1.4818044241020966e-06, 'batch_size': 224, 'step_size': 15, 'gamma': 0.848233257987741}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:44:23,202][0m Trial 9 finished with value: 0.480123132972394 and parameters: {'observation_period_num': 17, 'train_rates': 0.8994535020731942, 'learning_rate': 5.013908439782148e-06, 'batch_size': 128, 'step_size': 13, 'gamma': 0.9715156220490492}. Best is trial 3 with value: 0.05362059319338964.[0m
[32m[I 2025-01-11 02:49:25,913][0m Trial 10 finished with value: 0.02455597943398618 and parameters: {'observation_period_num': 8, 'train_rates': 0.7986432002112183, 'learning_rate': 0.0007652123471841687, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7933634287267952}. Best is trial 10 with value: 0.02455597943398618.[0m
[32m[I 2025-01-11 02:53:35,128][0m Trial 11 finished with value: 0.026262780103658925 and parameters: {'observation_period_num': 6, 'train_rates': 0.795919902422214, 'learning_rate': 0.0009692915837646141, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8000928435280338}. Best is trial 10 with value: 0.02455597943398618.[0m
[32m[I 2025-01-11 02:57:21,418][0m Trial 12 finished with value: 0.023266856786934575 and parameters: {'observation_period_num': 10, 'train_rates': 0.8074030968285543, 'learning_rate': 0.0007073725859948118, 'batch_size': 22, 'step_size': 6, 'gamma': 0.7606563160269512}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:00:36,519][0m Trial 13 finished with value: 0.023563656841705943 and parameters: {'observation_period_num': 6, 'train_rates': 0.8173703520548969, 'learning_rate': 0.000916765763917156, 'batch_size': 26, 'step_size': 6, 'gamma': 0.750480490116581}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:01:23,806][0m Trial 14 finished with value: 0.20403491670840254 and parameters: {'observation_period_num': 158, 'train_rates': 0.7319251183250777, 'learning_rate': 0.0003338133586302421, 'batch_size': 98, 'step_size': 4, 'gamma': 0.7561552885419549}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:03:08,850][0m Trial 15 finished with value: 0.0670909922260905 and parameters: {'observation_period_num': 38, 'train_rates': 0.8428111608378277, 'learning_rate': 4.284213724687947e-05, 'batch_size': 50, 'step_size': 8, 'gamma': 0.8932425803732075}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:03:37,278][0m Trial 16 finished with value: 0.17912804123458512 and parameters: {'observation_period_num': 249, 'train_rates': 0.8386312108511356, 'learning_rate': 0.0004500552175623096, 'batch_size': 176, 'step_size': 6, 'gamma': 0.773492408911692}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:05:34,447][0m Trial 17 finished with value: 0.08299266229846113 and parameters: {'observation_period_num': 33, 'train_rates': 0.7005964767775631, 'learning_rate': 0.0009813102680899517, 'batch_size': 39, 'step_size': 4, 'gamma': 0.8364908897443571}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:06:18,675][0m Trial 18 finished with value: 0.10393359753787938 and parameters: {'observation_period_num': 81, 'train_rates': 0.7532031255939645, 'learning_rate': 0.00015313761066740688, 'batch_size': 110, 'step_size': 9, 'gamma': 0.7760249078691109}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:06:44,950][0m Trial 19 finished with value: 0.24481197577779706 and parameters: {'observation_period_num': 158, 'train_rates': 0.6707881527281938, 'learning_rate': 0.0005164428456424425, 'batch_size': 182, 'step_size': 4, 'gamma': 0.8214911279539443}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:07:55,130][0m Trial 20 finished with value: 0.3951463785269713 and parameters: {'observation_period_num': 75, 'train_rates': 0.8482891257441674, 'learning_rate': 2.4963344318518502e-05, 'batch_size': 74, 'step_size': 8, 'gamma': 0.7745013874033014}. Best is trial 12 with value: 0.023266856786934575.[0m
[32m[I 2025-01-11 03:13:01,657][0m Trial 21 finished with value: 0.022318420555404943 and parameters: {'observation_period_num': 11, 'train_rates': 0.7981524305770709, 'learning_rate': 0.0005689261112441613, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7972821710517621}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:15:39,265][0m Trial 22 finished with value: 0.04054330841783335 and parameters: {'observation_period_num': 30, 'train_rates': 0.8200304712231828, 'learning_rate': 0.00046641347137119863, 'batch_size': 32, 'step_size': 5, 'gamma': 0.7517688483504732}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:15:59,824][0m Trial 23 finished with value: 0.09200863466485516 and parameters: {'observation_period_num': 26, 'train_rates': 0.7511441788233467, 'learning_rate': 0.000310826441163265, 'batch_size': 253, 'step_size': 7, 'gamma': 0.810242496380416}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:17:44,375][0m Trial 24 finished with value: 0.03561145833123531 and parameters: {'observation_period_num': 5, 'train_rates': 0.8637426585567949, 'learning_rate': 0.00059509961996578, 'batch_size': 51, 'step_size': 3, 'gamma': 0.784569384934275}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:22:39,228][0m Trial 25 finished with value: 0.08512029358569313 and parameters: {'observation_period_num': 44, 'train_rates': 0.7721524583402689, 'learning_rate': 6.154846807277133e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7714285286846658}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:24:43,951][0m Trial 26 finished with value: 0.0545567630065812 and parameters: {'observation_period_num': 64, 'train_rates': 0.8130448100215205, 'learning_rate': 0.00013815261868620156, 'batch_size': 40, 'step_size': 8, 'gamma': 0.872269033206058}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:25:23,582][0m Trial 27 finished with value: 0.2082810401916504 and parameters: {'observation_period_num': 24, 'train_rates': 0.9694061203528322, 'learning_rate': 0.0002520147340818847, 'batch_size': 151, 'step_size': 2, 'gamma': 0.830702960432931}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:26:22,610][0m Trial 28 finished with value: 0.06987653046443655 and parameters: {'observation_period_num': 149, 'train_rates': 0.8679845975582164, 'learning_rate': 0.0006186186914473568, 'batch_size': 87, 'step_size': 7, 'gamma': 0.7581366502019207}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:27:05,934][0m Trial 29 finished with value: 0.13723800997985036 and parameters: {'observation_period_num': 66, 'train_rates': 0.7101631873582493, 'learning_rate': 0.00029409392524931195, 'batch_size': 110, 'step_size': 5, 'gamma': 0.7515176219468135}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:28:32,334][0m Trial 30 finished with value: 0.05306900524012578 and parameters: {'observation_period_num': 48, 'train_rates': 0.7683157498580356, 'learning_rate': 0.0009854392753736014, 'batch_size': 57, 'step_size': 7, 'gamma': 0.7833596628589149}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:33:41,575][0m Trial 31 finished with value: 0.025180794049859675 and parameters: {'observation_period_num': 9, 'train_rates': 0.8068533795463516, 'learning_rate': 0.0005993778787716742, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7979293302762626}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:36:37,361][0m Trial 32 finished with value: 0.042084948304626675 and parameters: {'observation_period_num': 20, 'train_rates': 0.8312243942740122, 'learning_rate': 0.0008414917923423899, 'batch_size': 29, 'step_size': 3, 'gamma': 0.7914153725456289}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:39:40,203][0m Trial 33 finished with value: 0.02734362519976588 and parameters: {'observation_period_num': 7, 'train_rates': 0.7897839818472676, 'learning_rate': 0.00036803741975869345, 'batch_size': 27, 'step_size': 6, 'gamma': 0.8067886152299957}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:40:58,048][0m Trial 34 finished with value: 0.08453159167371958 and parameters: {'observation_period_num': 99, 'train_rates': 0.8735722573042345, 'learning_rate': 0.00022368646038609157, 'batch_size': 69, 'step_size': 9, 'gamma': 0.7622817195808105}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:42:35,915][0m Trial 35 finished with value: 0.053176427585264995 and parameters: {'observation_period_num': 38, 'train_rates': 0.7404707372778275, 'learning_rate': 0.0006676650831704434, 'batch_size': 48, 'step_size': 5, 'gamma': 0.7646887291972385}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:44:57,037][0m Trial 36 finished with value: 0.0441343639999772 and parameters: {'observation_period_num': 20, 'train_rates': 0.9296128860472488, 'learning_rate': 0.0001844735562746757, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8158321735957836}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:49:54,109][0m Trial 37 finished with value: 0.33311684698593325 and parameters: {'observation_period_num': 54, 'train_rates': 0.7879615610122225, 'learning_rate': 1.5368688754928825e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.7890817459712802}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:51:13,131][0m Trial 38 finished with value: 0.0664648804709176 and parameters: {'observation_period_num': 99, 'train_rates': 0.812474857131054, 'learning_rate': 0.00039970256531232017, 'batch_size': 64, 'step_size': 11, 'gamma': 0.8521922675149916}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:52:12,344][0m Trial 39 finished with value: 0.13024215306205328 and parameters: {'observation_period_num': 195, 'train_rates': 0.7700369881806125, 'learning_rate': 9.251307220411335e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8892243106058453}. Best is trial 21 with value: 0.022318420555404943.[0m
Early stopping at epoch 59
[32m[I 2025-01-11 03:53:47,210][0m Trial 40 finished with value: 2.3051252022742235 and parameters: {'observation_period_num': 17, 'train_rates': 0.6726346635484809, 'learning_rate': 1.19294358253154e-06, 'batch_size': 28, 'step_size': 1, 'gamma': 0.8353521539005924}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 03:58:38,072][0m Trial 41 finished with value: 0.0238913018976605 and parameters: {'observation_period_num': 5, 'train_rates': 0.806868832404646, 'learning_rate': 0.0006825264241336356, 'batch_size': 17, 'step_size': 6, 'gamma': 0.7997066937930737}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:00:32,018][0m Trial 42 finished with value: 0.0354392953806805 and parameters: {'observation_period_num': 36, 'train_rates': 0.8288406094737675, 'learning_rate': 0.0006463843854569495, 'batch_size': 45, 'step_size': 6, 'gamma': 0.7984454312300309}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:03:30,224][0m Trial 43 finished with value: 0.6138568933725752 and parameters: {'observation_period_num': 5, 'train_rates': 0.7952631139797168, 'learning_rate': 2.2156618332049614e-06, 'batch_size': 28, 'step_size': 5, 'gamma': 0.7665197110951496}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:06:23,273][0m Trial 44 finished with value: 0.03445177083341884 and parameters: {'observation_period_num': 17, 'train_rates': 0.7283744989727239, 'learning_rate': 0.0007371619633910435, 'batch_size': 27, 'step_size': 4, 'gamma': 0.7820398007159783}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:07:47,220][0m Trial 45 finished with value: 0.04581790321210832 and parameters: {'observation_period_num': 51, 'train_rates': 0.8545139274631642, 'learning_rate': 0.00043018345975015, 'batch_size': 63, 'step_size': 7, 'gamma': 0.825566266821105}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:09:27,035][0m Trial 46 finished with value: 0.03994439078305296 and parameters: {'observation_period_num': 30, 'train_rates': 0.8857805152361897, 'learning_rate': 0.00024228722503808644, 'batch_size': 54, 'step_size': 6, 'gamma': 0.9394660385691926}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:11:42,924][0m Trial 47 finished with value: 0.4299018740148868 and parameters: {'observation_period_num': 17, 'train_rates': 0.7988497080979944, 'learning_rate': 8.49211277320501e-06, 'batch_size': 37, 'step_size': 4, 'gamma': 0.8090392580436404}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:12:08,272][0m Trial 48 finished with value: 0.07863633918178665 and parameters: {'observation_period_num': 41, 'train_rates': 0.7543363957009329, 'learning_rate': 0.0007689286562634309, 'batch_size': 205, 'step_size': 10, 'gamma': 0.7890600756813255}. Best is trial 21 with value: 0.022318420555404943.[0m
[32m[I 2025-01-11 04:15:35,603][0m Trial 49 finished with value: 0.06051511490262869 and parameters: {'observation_period_num': 72, 'train_rates': 0.7793992802334295, 'learning_rate': 0.0001267298214278822, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8433950737886794}. Best is trial 21 with value: 0.022318420555404943.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-11 04:15:35,613][0m A new study created in memory with name: no-name-d5ddab10-030d-4e58-8cdc-0ee03fd388d9[0m
[32m[I 2025-01-11 04:16:12,826][0m Trial 0 finished with value: 0.3090100586414337 and parameters: {'observation_period_num': 38, 'train_rates': 0.9837890660871007, 'learning_rate': 1.888029158026572e-05, 'batch_size': 166, 'step_size': 15, 'gamma': 0.7849558542575371}. Best is trial 0 with value: 0.3090100586414337.[0m
[32m[I 2025-01-11 04:19:06,373][0m Trial 1 finished with value: 0.8290032636917507 and parameters: {'observation_period_num': 191, 'train_rates': 0.7219007507136525, 'learning_rate': 1.9218250084390583e-06, 'batch_size': 25, 'step_size': 14, 'gamma': 0.9396225320587124}. Best is trial 0 with value: 0.3090100586414337.[0m
[32m[I 2025-01-11 04:20:32,259][0m Trial 2 finished with value: 0.09205515794162675 and parameters: {'observation_period_num': 140, 'train_rates': 0.9550356364710199, 'learning_rate': 0.0007795578293237937, 'batch_size': 64, 'step_size': 4, 'gamma': 0.7547972189769895}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:20:51,431][0m Trial 3 finished with value: 1.247062594190789 and parameters: {'observation_period_num': 143, 'train_rates': 0.7199042660967113, 'learning_rate': 5.054337887075703e-06, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9184271112639155}. Best is trial 2 with value: 0.09205515794162675.[0m
Early stopping at epoch 55
[32m[I 2025-01-11 04:21:54,901][0m Trial 4 finished with value: 0.8744879330497191 and parameters: {'observation_period_num': 234, 'train_rates': 0.694440620592806, 'learning_rate': 3.249109778051838e-05, 'batch_size': 37, 'step_size': 1, 'gamma': 0.7841775405093477}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:24:51,440][0m Trial 5 finished with value: 0.249924713156177 and parameters: {'observation_period_num': 7, 'train_rates': 0.8252327480661434, 'learning_rate': 5.49618700512702e-06, 'batch_size': 29, 'step_size': 4, 'gamma': 0.8826744188481197}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:25:35,425][0m Trial 6 finished with value: 0.7206661496252668 and parameters: {'observation_period_num': 171, 'train_rates': 0.7720299277895651, 'learning_rate': 2.2710831063572443e-05, 'batch_size': 110, 'step_size': 3, 'gamma': 0.837055004116015}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:25:56,773][0m Trial 7 finished with value: 0.15083797027922607 and parameters: {'observation_period_num': 236, 'train_rates': 0.6415914500432794, 'learning_rate': 0.0008658666558824217, 'batch_size': 214, 'step_size': 13, 'gamma': 0.9229241040906004}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:29:28,053][0m Trial 8 finished with value: 0.7367508586955397 and parameters: {'observation_period_num': 185, 'train_rates': 0.6042066089496598, 'learning_rate': 1.9701095182188672e-05, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9116008799290676}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:32:31,000][0m Trial 9 finished with value: 0.2257436699511712 and parameters: {'observation_period_num': 227, 'train_rates': 0.7490985194324233, 'learning_rate': 0.0007710697866186194, 'batch_size': 24, 'step_size': 4, 'gamma': 0.9658561856376139}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:33:36,571][0m Trial 10 finished with value: 0.10703758895397186 and parameters: {'observation_period_num': 87, 'train_rates': 0.979727991900672, 'learning_rate': 0.00015070947235953778, 'batch_size': 90, 'step_size': 8, 'gamma': 0.7604901580083103}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:34:42,050][0m Trial 11 finished with value: 0.09598025679588318 and parameters: {'observation_period_num': 84, 'train_rates': 0.9894226239258479, 'learning_rate': 0.00017707880125035196, 'batch_size': 91, 'step_size': 8, 'gamma': 0.7553579961679393}. Best is trial 2 with value: 0.09205515794162675.[0m
[32m[I 2025-01-11 04:35:53,238][0m Trial 12 finished with value: 0.06518908822304242 and parameters: {'observation_period_num': 87, 'train_rates': 0.8924233705290933, 'learning_rate': 0.00020100202055835212, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8329810660028704}. Best is trial 12 with value: 0.06518908822304242.[0m
[32m[I 2025-01-11 04:37:14,437][0m Trial 13 finished with value: 0.07811396532697873 and parameters: {'observation_period_num': 101, 'train_rates': 0.8886728270103861, 'learning_rate': 0.00023132878719426766, 'batch_size': 65, 'step_size': 11, 'gamma': 0.8335081984347301}. Best is trial 12 with value: 0.06518908822304242.[0m
[32m[I 2025-01-11 04:37:53,129][0m Trial 14 finished with value: 0.08691984722470827 and parameters: {'observation_period_num': 90, 'train_rates': 0.8780193304052479, 'learning_rate': 0.00016117224593413906, 'batch_size': 142, 'step_size': 11, 'gamma': 0.835954855240863}. Best is trial 12 with value: 0.06518908822304242.[0m
[32m[I 2025-01-11 04:39:20,984][0m Trial 15 finished with value: 0.07471362240158202 and parameters: {'observation_period_num': 57, 'train_rates': 0.8868270627955837, 'learning_rate': 6.849503296286178e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8367383908542094}. Best is trial 12 with value: 0.06518908822304242.[0m
[32m[I 2025-01-11 04:40:05,297][0m Trial 16 finished with value: 0.11941346566933785 and parameters: {'observation_period_num': 45, 'train_rates': 0.9057814087387792, 'learning_rate': 6.364208511469059e-05, 'batch_size': 131, 'step_size': 11, 'gamma': 0.8627578454325895}. Best is trial 12 with value: 0.06518908822304242.[0m
[32m[I 2025-01-11 04:41:26,755][0m Trial 17 finished with value: 0.06838712783453507 and parameters: {'observation_period_num': 48, 'train_rates': 0.8249169293538824, 'learning_rate': 7.71643277839343e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8042732550562199}. Best is trial 12 with value: 0.06518908822304242.[0m
[32m[I 2025-01-11 04:42:00,065][0m Trial 18 finished with value: 0.05578618937713293 and parameters: {'observation_period_num': 18, 'train_rates': 0.8260297528065851, 'learning_rate': 0.00037409782943962505, 'batch_size': 163, 'step_size': 9, 'gamma': 0.7937589728324397}. Best is trial 18 with value: 0.05578618937713293.[0m
[32m[I 2025-01-11 04:42:33,292][0m Trial 19 finished with value: 0.046584400905038195 and parameters: {'observation_period_num': 7, 'train_rates': 0.8210528694359012, 'learning_rate': 0.0003795542836489579, 'batch_size': 169, 'step_size': 9, 'gamma': 0.8080346666845432}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:43:03,687][0m Trial 20 finished with value: 0.05544850720643774 and parameters: {'observation_period_num': 7, 'train_rates': 0.8187046231507209, 'learning_rate': 0.0003535445615074627, 'batch_size': 185, 'step_size': 9, 'gamma': 0.80358788084391}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:43:32,388][0m Trial 21 finished with value: 0.05911066690716175 and parameters: {'observation_period_num': 7, 'train_rates': 0.8148222299560425, 'learning_rate': 0.0004884675291191698, 'batch_size': 196, 'step_size': 7, 'gamma': 0.8045155664935679}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:44:04,667][0m Trial 22 finished with value: 0.06190149658129889 and parameters: {'observation_period_num': 24, 'train_rates': 0.8479080939937638, 'learning_rate': 0.0003737425151392497, 'batch_size': 177, 'step_size': 9, 'gamma': 0.8071492753290366}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:44:29,377][0m Trial 23 finished with value: 0.07146064004316194 and parameters: {'observation_period_num': 20, 'train_rates': 0.784801013946222, 'learning_rate': 0.0003873029419195427, 'batch_size': 222, 'step_size': 6, 'gamma': 0.7798604426824365}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:45:07,662][0m Trial 24 finished with value: 0.07241179775900958 and parameters: {'observation_period_num': 63, 'train_rates': 0.9290801145728189, 'learning_rate': 0.00033985024934803917, 'batch_size': 157, 'step_size': 9, 'gamma': 0.862429316674598}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:45:38,178][0m Trial 25 finished with value: 0.19246955143828545 and parameters: {'observation_period_num': 30, 'train_rates': 0.8515744036039598, 'learning_rate': 9.473420633898915e-05, 'batch_size': 185, 'step_size': 6, 'gamma': 0.8028817326108737}. Best is trial 19 with value: 0.046584400905038195.[0m
[32m[I 2025-01-11 04:46:17,845][0m Trial 26 finished with value: 0.03650419510672117 and parameters: {'observation_period_num': 7, 'train_rates': 0.8008930482270435, 'learning_rate': 0.0005487517920453237, 'batch_size': 132, 'step_size': 9, 'gamma': 0.8812349606900016}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:46:55,879][0m Trial 27 finished with value: 0.06076118576403564 and parameters: {'observation_period_num': 66, 'train_rates': 0.7652133398283443, 'learning_rate': 0.000992268430603905, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8870995626756688}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:47:39,748][0m Trial 28 finished with value: 0.125431627035141 and parameters: {'observation_period_num': 118, 'train_rates': 0.7988671956461847, 'learning_rate': 0.0001067908024977517, 'batch_size': 114, 'step_size': 6, 'gamma': 0.8916320389616061}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:48:07,961][0m Trial 29 finished with value: 0.1555233527093906 and parameters: {'observation_period_num': 37, 'train_rates': 0.8605402224226825, 'learning_rate': 4.747072055510012e-05, 'batch_size': 208, 'step_size': 15, 'gamma': 0.8208773573354801}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:48:31,299][0m Trial 30 finished with value: 0.0733751951206115 and parameters: {'observation_period_num': 36, 'train_rates': 0.7341963004555475, 'learning_rate': 0.0005678245168688451, 'batch_size': 234, 'step_size': 13, 'gamma': 0.855777622190587}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:49:04,690][0m Trial 31 finished with value: 0.054982994562756696 and parameters: {'observation_period_num': 5, 'train_rates': 0.8082007559006633, 'learning_rate': 0.00028866396819652955, 'batch_size': 162, 'step_size': 9, 'gamma': 0.7836466810340361}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:49:40,010][0m Trial 32 finished with value: 0.05781927471030123 and parameters: {'observation_period_num': 7, 'train_rates': 0.8002698733251588, 'learning_rate': 0.0002451824596203186, 'batch_size': 151, 'step_size': 10, 'gamma': 0.7703749185439267}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:50:09,194][0m Trial 33 finished with value: 0.048222315895009596 and parameters: {'observation_period_num': 5, 'train_rates': 0.7525007128008592, 'learning_rate': 0.0006393106861349992, 'batch_size': 182, 'step_size': 7, 'gamma': 0.8185848666291574}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:50:37,240][0m Trial 34 finished with value: 0.06105042626443891 and parameters: {'observation_period_num': 26, 'train_rates': 0.6912419197794412, 'learning_rate': 0.0005945576617589428, 'batch_size': 171, 'step_size': 7, 'gamma': 0.989856897564005}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:51:19,098][0m Trial 35 finished with value: 1.3185111718360933 and parameters: {'observation_period_num': 52, 'train_rates': 0.7576256072158108, 'learning_rate': 1.0627820000716414e-06, 'batch_size': 120, 'step_size': 7, 'gamma': 0.8496961442317157}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:51:53,070][0m Trial 36 finished with value: 0.5252484735955886 and parameters: {'observation_period_num': 19, 'train_rates': 0.7027476095152732, 'learning_rate': 1.1428862458598147e-05, 'batch_size': 147, 'step_size': 10, 'gamma': 0.81961043627399}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:52:23,049][0m Trial 37 finished with value: 0.0873623328047235 and parameters: {'observation_period_num': 69, 'train_rates': 0.7837798015739342, 'learning_rate': 0.0006408081659957936, 'batch_size': 166, 'step_size': 5, 'gamma': 0.7767807920952774}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:52:50,168][0m Trial 38 finished with value: 0.08986879326403141 and parameters: {'observation_period_num': 34, 'train_rates': 0.7366434142687853, 'learning_rate': 0.00026827915056459894, 'batch_size': 192, 'step_size': 7, 'gamma': 0.8194986035359476}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:53:26,662][0m Trial 39 finished with value: 0.13019617026353755 and parameters: {'observation_period_num': 134, 'train_rates': 0.7088794855487948, 'learning_rate': 0.00012142464878720363, 'batch_size': 127, 'step_size': 8, 'gamma': 0.9020970791948402}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:54:12,860][0m Trial 40 finished with value: 0.07836553103679532 and parameters: {'observation_period_num': 162, 'train_rates': 0.7736141533568978, 'learning_rate': 0.0009912433217487584, 'batch_size': 103, 'step_size': 5, 'gamma': 0.8790530600430414}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:54:43,866][0m Trial 41 finished with value: 0.06083607153982675 and parameters: {'observation_period_num': 5, 'train_rates': 0.8384467748750838, 'learning_rate': 0.0003026336004363258, 'batch_size': 184, 'step_size': 9, 'gamma': 0.7911154383258835}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:55:12,423][0m Trial 42 finished with value: 0.07078530051206287 and parameters: {'observation_period_num': 15, 'train_rates': 0.812292275479821, 'learning_rate': 0.0004895724174442517, 'batch_size': 198, 'step_size': 9, 'gamma': 0.7897703739715906}. Best is trial 26 with value: 0.03650419510672117.[0m
[32m[I 2025-01-11 04:55:44,983][0m Trial 43 finished with value: 0.035571678541600704 and parameters: {'observation_period_num': 13, 'train_rates': 0.8641213259034833, 'learning_rate': 0.0007199296523659778, 'batch_size': 175, 'step_size': 8, 'gamma': 0.8752623299537569}. Best is trial 43 with value: 0.035571678541600704.[0m
[32m[I 2025-01-11 04:56:15,218][0m Trial 44 finished with value: 0.06453430401470756 and parameters: {'observation_period_num': 44, 'train_rates': 0.6791028894793, 'learning_rate': 0.0006663816865692823, 'batch_size': 157, 'step_size': 10, 'gamma': 0.8753760106923196}. Best is trial 43 with value: 0.035571678541600704.[0m
[32m[I 2025-01-11 04:56:45,632][0m Trial 45 finished with value: 0.09033402004846433 and parameters: {'observation_period_num': 221, 'train_rates': 0.8707707467214763, 'learning_rate': 0.0004826954666464235, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9274829417396073}. Best is trial 43 with value: 0.035571678541600704.[0m
[32m[I 2025-01-11 04:57:09,444][0m Trial 46 finished with value: 1.3229429721832275 and parameters: {'observation_period_num': 29, 'train_rates': 0.9178117398248892, 'learning_rate': 4.3189796063437476e-06, 'batch_size': 253, 'step_size': 8, 'gamma': 0.902046700845244}. Best is trial 43 with value: 0.035571678541600704.[0m
[32m[I 2025-01-11 04:57:45,616][0m Trial 47 finished with value: 0.03872024939984691 and parameters: {'observation_period_num': 16, 'train_rates': 0.7497125973576663, 'learning_rate': 0.0007466850051084543, 'batch_size': 142, 'step_size': 12, 'gamma': 0.8465030011553095}. Best is trial 43 with value: 0.035571678541600704.[0m
[32m[I 2025-01-11 04:58:18,824][0m Trial 48 finished with value: 0.08046500997510023 and parameters: {'observation_period_num': 78, 'train_rates': 0.651358084016994, 'learning_rate': 0.0008280373764766971, 'batch_size': 140, 'step_size': 13, 'gamma': 0.850245338638803}. Best is trial 43 with value: 0.035571678541600704.[0m
[32m[I 2025-01-11 04:59:04,567][0m Trial 49 finished with value: 0.09061365438695096 and parameters: {'observation_period_num': 110, 'train_rates': 0.727819886586155, 'learning_rate': 0.0007555098724443072, 'batch_size': 101, 'step_size': 12, 'gamma': 0.8682954135955464}. Best is trial 43 with value: 0.035571678541600704.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-11 04:59:04,577][0m A new study created in memory with name: no-name-95928f5e-9bb9-45e1-8e8e-b76f07258f4c[0m
[32m[I 2025-01-11 04:59:35,182][0m Trial 0 finished with value: 0.17734310834696798 and parameters: {'observation_period_num': 226, 'train_rates': 0.8184838864035213, 'learning_rate': 0.00015622729963782076, 'batch_size': 166, 'step_size': 7, 'gamma': 0.8082524601848059}. Best is trial 0 with value: 0.17734310834696798.[0m
[32m[I 2025-01-11 04:59:59,802][0m Trial 1 finished with value: 0.595567069890845 and parameters: {'observation_period_num': 188, 'train_rates': 0.8105973386339843, 'learning_rate': 1.3152312604355505e-05, 'batch_size': 204, 'step_size': 15, 'gamma': 0.8550803103603394}. Best is trial 0 with value: 0.17734310834696798.[0m
[32m[I 2025-01-11 05:00:33,982][0m Trial 2 finished with value: 0.20128163495170537 and parameters: {'observation_period_num': 197, 'train_rates': 0.9273492081035772, 'learning_rate': 8.679590460045539e-05, 'batch_size': 169, 'step_size': 13, 'gamma': 0.8933724867671102}. Best is trial 0 with value: 0.17734310834696798.[0m
[32m[I 2025-01-11 05:01:35,399][0m Trial 3 finished with value: 0.29710559921761964 and parameters: {'observation_period_num': 221, 'train_rates': 0.8210272400373082, 'learning_rate': 4.168750421962541e-05, 'batch_size': 78, 'step_size': 4, 'gamma': 0.8836026717941617}. Best is trial 0 with value: 0.17734310834696798.[0m
[32m[I 2025-01-11 05:02:25,141][0m Trial 4 finished with value: 0.8199737668037415 and parameters: {'observation_period_num': 196, 'train_rates': 0.9691179055946069, 'learning_rate': 3.4702649524754053e-06, 'batch_size': 113, 'step_size': 3, 'gamma': 0.942732549529234}. Best is trial 0 with value: 0.17734310834696798.[0m
[32m[I 2025-01-11 05:03:01,652][0m Trial 5 finished with value: 0.7740937149071175 and parameters: {'observation_period_num': 52, 'train_rates': 0.873283894573796, 'learning_rate': 2.762871927163183e-06, 'batch_size': 151, 'step_size': 12, 'gamma': 0.9309447114813199}. Best is trial 0 with value: 0.17734310834696798.[0m
[32m[I 2025-01-11 05:03:52,838][0m Trial 6 finished with value: 0.060917270332574845 and parameters: {'observation_period_num': 15, 'train_rates': 0.8553317347966609, 'learning_rate': 6.568221595717757e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.9115868521388321}. Best is trial 6 with value: 0.060917270332574845.[0m
[32m[I 2025-01-11 05:04:18,084][0m Trial 7 finished with value: 1.9196050161665137 and parameters: {'observation_period_num': 40, 'train_rates': 0.7733165879145718, 'learning_rate': 1.256683776188324e-06, 'batch_size': 217, 'step_size': 8, 'gamma': 0.8307940063426964}. Best is trial 6 with value: 0.060917270332574845.[0m
[32m[I 2025-01-11 05:05:28,588][0m Trial 8 finished with value: 0.10270937932561142 and parameters: {'observation_period_num': 148, 'train_rates': 0.8233869775519606, 'learning_rate': 8.751963430210823e-05, 'batch_size': 71, 'step_size': 9, 'gamma': 0.9841052673485886}. Best is trial 6 with value: 0.060917270332574845.[0m
[32m[I 2025-01-11 05:08:11,243][0m Trial 9 finished with value: 0.06460450135697186 and parameters: {'observation_period_num': 74, 'train_rates': 0.7980319339568975, 'learning_rate': 5.37929881635506e-05, 'batch_size': 30, 'step_size': 13, 'gamma': 0.9092813404187405}. Best is trial 6 with value: 0.060917270332574845.[0m
Early stopping at epoch 64
[32m[I 2025-01-11 05:08:40,394][0m Trial 10 finished with value: 0.16550332658694597 and parameters: {'observation_period_num': 5, 'train_rates': 0.6613221320744805, 'learning_rate': 0.00035948818505103904, 'batch_size': 107, 'step_size': 1, 'gamma': 0.7678366229783969}. Best is trial 6 with value: 0.060917270332574845.[0m
[32m[I 2025-01-11 05:12:11,745][0m Trial 11 finished with value: 0.307565628088701 and parameters: {'observation_period_num': 101, 'train_rates': 0.7128881701467352, 'learning_rate': 0.0009232120475214233, 'batch_size': 21, 'step_size': 11, 'gamma': 0.9313819916155762}. Best is trial 6 with value: 0.060917270332574845.[0m
[32m[I 2025-01-11 05:16:00,520][0m Trial 12 finished with value: 0.12948247485989284 and parameters: {'observation_period_num': 88, 'train_rates': 0.7367972076319276, 'learning_rate': 1.4252341822983777e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.90513792420656}. Best is trial 6 with value: 0.060917270332574845.[0m
[32m[I 2025-01-11 05:17:38,450][0m Trial 13 finished with value: 0.053790392803734745 and parameters: {'observation_period_num': 9, 'train_rates': 0.8916899170649586, 'learning_rate': 2.858136342544514e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9801094999375047}. Best is trial 13 with value: 0.053790392803734745.[0m
[32m[I 2025-01-11 05:18:02,692][0m Trial 14 finished with value: 0.1638760375526716 and parameters: {'observation_period_num': 8, 'train_rates': 0.8921185948736369, 'learning_rate': 1.7459948668903392e-05, 'batch_size': 256, 'step_size': 10, 'gamma': 0.9887434831131807}. Best is trial 13 with value: 0.053790392803734745.[0m
[32m[I 2025-01-11 05:19:39,521][0m Trial 15 finished with value: 0.3674240708351135 and parameters: {'observation_period_num': 130, 'train_rates': 0.9831153580015829, 'learning_rate': 0.00023001263274152528, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9604965099854134}. Best is trial 13 with value: 0.053790392803734745.[0m
[32m[I 2025-01-11 05:20:30,921][0m Trial 16 finished with value: 0.9353646385042291 and parameters: {'observation_period_num': 37, 'train_rates': 0.876287229323617, 'learning_rate': 6.8675215229110846e-06, 'batch_size': 109, 'step_size': 5, 'gamma': 0.8567624971485358}. Best is trial 13 with value: 0.053790392803734745.[0m
[32m[I 2025-01-11 05:22:14,163][0m Trial 17 finished with value: 0.1037333648803547 and parameters: {'observation_period_num': 62, 'train_rates': 0.923483789149233, 'learning_rate': 2.9444277799752726e-05, 'batch_size': 54, 'step_size': 6, 'gamma': 0.9652070120810642}. Best is trial 13 with value: 0.053790392803734745.[0m
[32m[I 2025-01-11 05:23:16,484][0m Trial 18 finished with value: 0.047301202267408374 and parameters: {'observation_period_num': 26, 'train_rates': 0.9232174390503158, 'learning_rate': 0.0005859048994569145, 'batch_size': 90, 'step_size': 10, 'gamma': 0.9520681831942417}. Best is trial 18 with value: 0.047301202267408374.[0m
[32m[I 2025-01-11 05:24:05,468][0m Trial 19 finished with value: 0.14273497886495495 and parameters: {'observation_period_num': 111, 'train_rates': 0.6068533482121371, 'learning_rate': 0.0009038943595567088, 'batch_size': 86, 'step_size': 11, 'gamma': 0.959595820250429}. Best is trial 18 with value: 0.047301202267408374.[0m
[32m[I 2025-01-11 05:24:49,829][0m Trial 20 finished with value: 0.052777825721672604 and parameters: {'observation_period_num': 34, 'train_rates': 0.935386588548795, 'learning_rate': 0.00045104975610622605, 'batch_size': 135, 'step_size': 13, 'gamma': 0.9437665229447085}. Best is trial 18 with value: 0.047301202267408374.[0m
[32m[I 2025-01-11 05:25:34,119][0m Trial 21 finished with value: 0.10419840869065877 and parameters: {'observation_period_num': 29, 'train_rates': 0.9370071825967938, 'learning_rate': 0.00039308761715100166, 'batch_size': 135, 'step_size': 13, 'gamma': 0.9452340708720868}. Best is trial 18 with value: 0.047301202267408374.[0m
[32m[I 2025-01-11 05:26:18,528][0m Trial 22 finished with value: 0.1453694411336559 and parameters: {'observation_period_num': 69, 'train_rates': 0.949667317601657, 'learning_rate': 0.0005394495156887443, 'batch_size': 134, 'step_size': 11, 'gamma': 0.9771597093791071}. Best is trial 18 with value: 0.047301202267408374.[0m
[32m[I 2025-01-11 05:28:40,610][0m Trial 23 finished with value: 0.03867762320571476 and parameters: {'observation_period_num': 26, 'train_rates': 0.9078998614679715, 'learning_rate': 0.0001901355044734345, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9285055739986681}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:30:44,575][0m Trial 24 finished with value: 0.04482040793419405 and parameters: {'observation_period_num': 33, 'train_rates': 0.9216454501780386, 'learning_rate': 0.0001661990535307761, 'batch_size': 45, 'step_size': 14, 'gamma': 0.9259845667930744}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:33:04,439][0m Trial 25 finished with value: 0.0481630033696559 and parameters: {'observation_period_num': 53, 'train_rates': 0.9095041367471965, 'learning_rate': 0.00016246803920842237, 'batch_size': 39, 'step_size': 15, 'gamma': 0.9240192581196535}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:34:01,982][0m Trial 26 finished with value: 0.05757924070898092 and parameters: {'observation_period_num': 87, 'train_rates': 0.8520988390681034, 'learning_rate': 0.0002051350448390465, 'batch_size': 92, 'step_size': 14, 'gamma': 0.8778853382719995}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:36:15,631][0m Trial 27 finished with value: 0.10603312402963638 and parameters: {'observation_period_num': 153, 'train_rates': 0.988230112575899, 'learning_rate': 0.00012512127734279308, 'batch_size': 42, 'step_size': 14, 'gamma': 0.8987335432436442}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:37:33,496][0m Trial 28 finished with value: 0.059402750328529715 and parameters: {'observation_period_num': 23, 'train_rates': 0.9561028521252333, 'learning_rate': 0.00024353625574977615, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9532563645999592}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:39:38,780][0m Trial 29 finished with value: 0.19649696857600973 and parameters: {'observation_period_num': 49, 'train_rates': 0.8574518735635834, 'learning_rate': 0.0006037024938993347, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9202342876185947}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:40:35,278][0m Trial 30 finished with value: 0.1370409910896873 and parameters: {'observation_period_num': 246, 'train_rates': 0.905122057706037, 'learning_rate': 0.0001299083645281153, 'batch_size': 94, 'step_size': 12, 'gamma': 0.769477811642254}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:43:18,747][0m Trial 31 finished with value: 0.05313085490951733 and parameters: {'observation_period_num': 55, 'train_rates': 0.8987633121589866, 'learning_rate': 0.0002708180677918698, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9251318148202702}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:44:55,956][0m Trial 32 finished with value: 0.04656239644316213 and parameters: {'observation_period_num': 29, 'train_rates': 0.9100390311111801, 'learning_rate': 0.00015208972688819737, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8593649888251511}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:46:19,099][0m Trial 33 finished with value: 0.05132181021146291 and parameters: {'observation_period_num': 26, 'train_rates': 0.8345792807050854, 'learning_rate': 0.00010170690482302032, 'batch_size': 63, 'step_size': 14, 'gamma': 0.8276767437395667}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:48:07,366][0m Trial 34 finished with value: 0.08437294129169348 and parameters: {'observation_period_num': 80, 'train_rates': 0.9540205453500367, 'learning_rate': 0.0006572919951527627, 'batch_size': 52, 'step_size': 15, 'gamma': 0.8602042948751799}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:49:28,543][0m Trial 35 finished with value: 0.04500172072058094 and parameters: {'observation_period_num': 21, 'train_rates': 0.9209548719323343, 'learning_rate': 0.0003208326573175498, 'batch_size': 70, 'step_size': 13, 'gamma': 0.8382236519928417}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:54:18,681][0m Trial 36 finished with value: 0.060790934145450594 and parameters: {'observation_period_num': 43, 'train_rates': 0.8711878839436693, 'learning_rate': 0.00016595885796684922, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8381736187102671}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:54:50,065][0m Trial 37 finished with value: 0.1072683185338974 and parameters: {'observation_period_num': 175, 'train_rates': 0.9621090220610703, 'learning_rate': 0.0003161962375900983, 'batch_size': 189, 'step_size': 14, 'gamma': 0.793027203464277}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:55:53,051][0m Trial 38 finished with value: 0.06991178260383776 and parameters: {'observation_period_num': 18, 'train_rates': 0.7799411949570207, 'learning_rate': 6.499235079514794e-05, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8874956394466831}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:57:50,046][0m Trial 39 finished with value: 0.12289277755833687 and parameters: {'observation_period_num': 100, 'train_rates': 0.9177753591247239, 'learning_rate': 4.0425735702516066e-05, 'batch_size': 46, 'step_size': 15, 'gamma': 0.8181563936867913}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 05:59:06,322][0m Trial 40 finished with value: 0.0696772923575172 and parameters: {'observation_period_num': 65, 'train_rates': 0.8360145030294522, 'learning_rate': 8.32341629929547e-05, 'batch_size': 68, 'step_size': 12, 'gamma': 0.8714455662173519}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 06:00:10,773][0m Trial 41 finished with value: 0.05168317158411189 and parameters: {'observation_period_num': 23, 'train_rates': 0.9302383648622976, 'learning_rate': 0.00018973237642192045, 'batch_size': 89, 'step_size': 13, 'gamma': 0.8414741397199965}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 06:03:08,124][0m Trial 42 finished with value: 0.08015886642809572 and parameters: {'observation_period_num': 41, 'train_rates': 0.8807048516936423, 'learning_rate': 0.00033313869419075243, 'batch_size': 30, 'step_size': 10, 'gamma': 0.8461887957427369}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 06:03:56,337][0m Trial 43 finished with value: 0.052827590440740126 and parameters: {'observation_period_num': 33, 'train_rates': 0.9429605765103445, 'learning_rate': 0.0007045313520628278, 'batch_size': 121, 'step_size': 8, 'gamma': 0.8013660945994024}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 06:04:35,540][0m Trial 44 finished with value: 0.08102520555257797 and parameters: {'observation_period_num': 21, 'train_rates': 0.9712926666076839, 'learning_rate': 9.903568114828566e-05, 'batch_size': 152, 'step_size': 14, 'gamma': 0.8628088315626625}. Best is trial 23 with value: 0.03867762320571476.[0m
[32m[I 2025-01-11 06:05:58,669][0m Trial 45 finished with value: 0.03098189573811025 and parameters: {'observation_period_num': 5, 'train_rates': 0.9169195889916527, 'learning_rate': 0.000488575988577244, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8930607259806284}. Best is trial 45 with value: 0.03098189573811025.[0m
[32m[I 2025-01-11 06:07:14,845][0m Trial 46 finished with value: 0.02940898070878842 and parameters: {'observation_period_num': 5, 'train_rates': 0.8616497094677267, 'learning_rate': 0.0004378804790611763, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8847551321001074}. Best is trial 46 with value: 0.02940898070878842.[0m
[32m[I 2025-01-11 06:08:28,835][0m Trial 47 finished with value: 0.036606442279675425 and parameters: {'observation_period_num': 9, 'train_rates': 0.8557797886475449, 'learning_rate': 0.0004341667544579744, 'batch_size': 72, 'step_size': 3, 'gamma': 0.9116049401441632}. Best is trial 46 with value: 0.02940898070878842.[0m
[32m[I 2025-01-11 06:09:20,254][0m Trial 48 finished with value: 0.05284699405425451 and parameters: {'observation_period_num': 8, 'train_rates': 0.807027529920796, 'learning_rate': 0.0004470718312941541, 'batch_size': 98, 'step_size': 2, 'gamma': 0.8966223858121166}. Best is trial 46 with value: 0.02940898070878842.[0m
[32m[I 2025-01-11 06:12:28,045][0m Trial 49 finished with value: 0.02506900495708973 and parameters: {'observation_period_num': 6, 'train_rates': 0.8561588149194292, 'learning_rate': 0.000486406153888435, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9115661863487297}. Best is trial 49 with value: 0.02506900495708973.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-11 06:12:28,054][0m A new study created in memory with name: no-name-00b250db-a30a-4d75-ab32-b967d692c901[0m
[32m[I 2025-01-11 06:13:53,268][0m Trial 0 finished with value: 0.14034287631511688 and parameters: {'observation_period_num': 166, 'train_rates': 0.8938248470679613, 'learning_rate': 0.0007435281423719468, 'batch_size': 62, 'step_size': 8, 'gamma': 0.8718640939607911}. Best is trial 0 with value: 0.14034287631511688.[0m
[32m[I 2025-01-11 06:14:20,449][0m Trial 1 finished with value: 0.10484863817691803 and parameters: {'observation_period_num': 154, 'train_rates': 0.9522069103357664, 'learning_rate': 0.00033765250335565395, 'batch_size': 219, 'step_size': 6, 'gamma': 0.910710575849464}. Best is trial 1 with value: 0.10484863817691803.[0m
[32m[I 2025-01-11 06:14:43,989][0m Trial 2 finished with value: 1.357574701309204 and parameters: {'observation_period_num': 193, 'train_rates': 0.9483462893092067, 'learning_rate': 3.875616785359464e-06, 'batch_size': 256, 'step_size': 3, 'gamma': 0.930366574695653}. Best is trial 1 with value: 0.10484863817691803.[0m
[32m[I 2025-01-11 06:16:08,163][0m Trial 3 finished with value: 0.8494413101968686 and parameters: {'observation_period_num': 202, 'train_rates': 0.7801077093133766, 'learning_rate': 1.5998924584024208e-06, 'batch_size': 56, 'step_size': 8, 'gamma': 0.9855643101448031}. Best is trial 1 with value: 0.10484863817691803.[0m
[32m[I 2025-01-11 06:16:44,293][0m Trial 4 finished with value: 0.5416603778780197 and parameters: {'observation_period_num': 209, 'train_rates': 0.9056685108484983, 'learning_rate': 5.243128130886984e-06, 'batch_size': 153, 'step_size': 8, 'gamma': 0.9117105823149247}. Best is trial 1 with value: 0.10484863817691803.[0m
[32m[I 2025-01-11 06:17:27,833][0m Trial 5 finished with value: 0.8477614093049665 and parameters: {'observation_period_num': 248, 'train_rates': 0.9208085813287425, 'learning_rate': 2.3260088753228106e-06, 'batch_size': 119, 'step_size': 13, 'gamma': 0.9066925150849432}. Best is trial 1 with value: 0.10484863817691803.[0m
[32m[I 2025-01-11 06:17:57,372][0m Trial 6 finished with value: 0.21346797662355552 and parameters: {'observation_period_num': 60, 'train_rates': 0.7732641571727613, 'learning_rate': 4.887123143540719e-05, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8686135780382946}. Best is trial 1 with value: 0.10484863817691803.[0m
[32m[I 2025-01-11 06:23:03,398][0m Trial 7 finished with value: 0.06791692581982042 and parameters: {'observation_period_num': 85, 'train_rates': 0.8160880735995264, 'learning_rate': 0.0002395800458022034, 'batch_size': 16, 'step_size': 3, 'gamma': 0.8547120812138405}. Best is trial 7 with value: 0.06791692581982042.[0m
[32m[I 2025-01-11 06:23:41,615][0m Trial 8 finished with value: 0.3809811424196156 and parameters: {'observation_period_num': 241, 'train_rates': 0.8871254138516367, 'learning_rate': 5.8452332358463134e-05, 'batch_size': 140, 'step_size': 5, 'gamma': 0.8094976789145962}. Best is trial 7 with value: 0.06791692581982042.[0m
[32m[I 2025-01-11 06:26:34,689][0m Trial 9 finished with value: 0.09839385802019908 and parameters: {'observation_period_num': 244, 'train_rates': 0.9341667023413909, 'learning_rate': 0.00013119263466280218, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9323426294668228}. Best is trial 7 with value: 0.06791692581982042.[0m
Early stopping at epoch 45
[32m[I 2025-01-11 06:26:55,017][0m Trial 10 finished with value: 2.701579031690026 and parameters: {'observation_period_num': 77, 'train_rates': 0.6543563972758307, 'learning_rate': 1.5111466609184654e-05, 'batch_size': 103, 'step_size': 1, 'gamma': 0.7563586608739633}. Best is trial 7 with value: 0.06791692581982042.[0m
[32m[I 2025-01-11 06:31:34,107][0m Trial 11 finished with value: 0.06821226794272661 and parameters: {'observation_period_num': 82, 'train_rates': 0.8438850606593373, 'learning_rate': 0.00021204865624908513, 'batch_size': 18, 'step_size': 4, 'gamma': 0.9751669583884951}. Best is trial 7 with value: 0.06791692581982042.[0m
[32m[I 2025-01-11 06:36:48,256][0m Trial 12 finished with value: 0.026731190346039747 and parameters: {'observation_period_num': 5, 'train_rates': 0.8216310481852048, 'learning_rate': 0.00022809492655890073, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9880828234720889}. Best is trial 12 with value: 0.026731190346039747.[0m
Early stopping at epoch 86
[32m[I 2025-01-11 06:37:44,662][0m Trial 13 finished with value: 0.13936591690554864 and parameters: {'observation_period_num': 24, 'train_rates': 0.7088485877997801, 'learning_rate': 0.0008945525079766261, 'batch_size': 74, 'step_size': 1, 'gamma': 0.832137677189809}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:38:48,935][0m Trial 14 finished with value: 0.1399501589373139 and parameters: {'observation_period_num': 7, 'train_rates': 0.8293598980436053, 'learning_rate': 0.00010818395112449748, 'batch_size': 84, 'step_size': 2, 'gamma': 0.8129362094320294}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:40:49,651][0m Trial 15 finished with value: 0.34482506742879004 and parameters: {'observation_period_num': 123, 'train_rates': 0.7669589369223101, 'learning_rate': 1.258645778730853e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.8562993458971584}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:43:59,579][0m Trial 16 finished with value: 0.080247830434153 and parameters: {'observation_period_num': 114, 'train_rates': 0.833281167144735, 'learning_rate': 0.0003788774042676145, 'batch_size': 26, 'step_size': 3, 'gamma': 0.7775542205229838}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:44:50,997][0m Trial 17 finished with value: 0.1204832035215783 and parameters: {'observation_period_num': 40, 'train_rates': 0.7185597717892379, 'learning_rate': 2.8161161600485384e-05, 'batch_size': 94, 'step_size': 15, 'gamma': 0.9575731579110559}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:46:02,744][0m Trial 18 finished with value: 0.427590766719178 and parameters: {'observation_period_num': 92, 'train_rates': 0.6139446415312257, 'learning_rate': 9.624237742164273e-05, 'batch_size': 57, 'step_size': 1, 'gamma': 0.8864154576915251}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:46:32,109][0m Trial 19 finished with value: 0.09493493718173117 and parameters: {'observation_period_num': 58, 'train_rates': 0.7193706382254131, 'learning_rate': 0.0003894436675890381, 'batch_size': 172, 'step_size': 4, 'gamma': 0.8392583459054093}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:48:38,859][0m Trial 20 finished with value: 0.03265247121453285 and parameters: {'observation_period_num': 31, 'train_rates': 0.9853535991738784, 'learning_rate': 0.0002044238842606243, 'batch_size': 46, 'step_size': 6, 'gamma': 0.942240581173567}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:50:44,913][0m Trial 21 finished with value: 0.0328082156492703 and parameters: {'observation_period_num': 7, 'train_rates': 0.9775170374796706, 'learning_rate': 0.0002019295374861381, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9606951550130178}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:52:53,898][0m Trial 22 finished with value: 0.03193700313568115 and parameters: {'observation_period_num': 5, 'train_rates': 0.9893336958713176, 'learning_rate': 0.00017554005885260667, 'batch_size': 46, 'step_size': 10, 'gamma': 0.9561793799161449}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:54:12,001][0m Trial 23 finished with value: 0.04777810858880601 and parameters: {'observation_period_num': 32, 'train_rates': 0.9720875827340715, 'learning_rate': 0.0005794481719424938, 'batch_size': 75, 'step_size': 10, 'gamma': 0.9537629280925877}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:54:59,130][0m Trial 24 finished with value: 0.06992472310472901 and parameters: {'observation_period_num': 48, 'train_rates': 0.867837320821214, 'learning_rate': 6.849617019180835e-05, 'batch_size': 118, 'step_size': 9, 'gamma': 0.9895690567110373}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:57:11,600][0m Trial 25 finished with value: 0.06627393414576849 and parameters: {'observation_period_num': 21, 'train_rates': 0.9797903632663159, 'learning_rate': 3.0086872380928994e-05, 'batch_size': 44, 'step_size': 14, 'gamma': 0.944916743270922}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 06:58:37,415][0m Trial 26 finished with value: 0.04352382912499006 and parameters: {'observation_period_num': 14, 'train_rates': 0.9378763530623464, 'learning_rate': 0.0001497157404225818, 'batch_size': 67, 'step_size': 10, 'gamma': 0.9721730412713389}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:01:00,300][0m Trial 27 finished with value: 0.07040983777376214 and parameters: {'observation_period_num': 44, 'train_rates': 0.8686718309987013, 'learning_rate': 0.0004356636081286033, 'batch_size': 37, 'step_size': 7, 'gamma': 0.9334342768326946}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:01:55,555][0m Trial 28 finished with value: 0.07510782033205032 and parameters: {'observation_period_num': 63, 'train_rates': 0.9875529626209032, 'learning_rate': 0.0002268646123193831, 'batch_size': 107, 'step_size': 10, 'gamma': 0.969292981852719}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:03:29,577][0m Trial 29 finished with value: 0.0869040917660433 and parameters: {'observation_period_num': 30, 'train_rates': 0.8846009431764196, 'learning_rate': 0.0007123875238124574, 'batch_size': 58, 'step_size': 12, 'gamma': 0.943403610415857}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:05:38,806][0m Trial 30 finished with value: 0.2740437611937523 and parameters: {'observation_period_num': 170, 'train_rates': 0.7483044403464933, 'learning_rate': 1.6454383868584226e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8988978336625345}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:07:36,726][0m Trial 31 finished with value: 0.037693577975166585 and parameters: {'observation_period_num': 6, 'train_rates': 0.9616957995115829, 'learning_rate': 0.000158193233023157, 'batch_size': 49, 'step_size': 6, 'gamma': 0.9642876957119954}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:08:46,098][0m Trial 32 finished with value: 0.039916617531771864 and parameters: {'observation_period_num': 19, 'train_rates': 0.9112427543307222, 'learning_rate': 0.00026688002924696525, 'batch_size': 83, 'step_size': 7, 'gamma': 0.9212679240019541}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:14:22,476][0m Trial 33 finished with value: 0.03197297165170312 and parameters: {'observation_period_num': 11, 'train_rates': 0.9645464693923673, 'learning_rate': 9.17615302029987e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9486840068272044}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:19:07,849][0m Trial 34 finished with value: 0.06993990900981076 and parameters: {'observation_period_num': 147, 'train_rates': 0.951707220325213, 'learning_rate': 8.472944568667941e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9827776085554754}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:19:35,530][0m Trial 35 finished with value: 0.13296325504779816 and parameters: {'observation_period_num': 36, 'train_rates': 0.9895403242965238, 'learning_rate': 5.004786771761108e-05, 'batch_size': 234, 'step_size': 7, 'gamma': 0.9434131359535517}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:22:37,334][0m Trial 36 finished with value: 0.07851346381126888 and parameters: {'observation_period_num': 106, 'train_rates': 0.933796344246846, 'learning_rate': 0.0005500916430638167, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9226031918623462}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:24:17,416][0m Trial 37 finished with value: 0.29361057637342763 and parameters: {'observation_period_num': 52, 'train_rates': 0.9539262897595413, 'learning_rate': 3.735092690737976e-05, 'batch_size': 57, 'step_size': 2, 'gamma': 0.8897484883093204}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:24:43,958][0m Trial 38 finished with value: 0.10359347851754487 and parameters: {'observation_period_num': 72, 'train_rates': 0.8017995233926831, 'learning_rate': 7.900198348892177e-05, 'batch_size': 199, 'step_size': 9, 'gamma': 0.9789539513517055}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:29:49,371][0m Trial 39 finished with value: 0.03613909710010019 and parameters: {'observation_period_num': 23, 'train_rates': 0.9192554879544091, 'learning_rate': 0.0003212762712676238, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9126485864569254}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:31:12,293][0m Trial 40 finished with value: 1.4421193212460561 and parameters: {'observation_period_num': 36, 'train_rates': 0.8924638117878783, 'learning_rate': 1.0051328540909153e-06, 'batch_size': 66, 'step_size': 5, 'gamma': 0.949508430237268}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:33:36,264][0m Trial 41 finished with value: 0.03448844468648281 and parameters: {'observation_period_num': 5, 'train_rates': 0.9631747923694071, 'learning_rate': 0.00017842552505511862, 'batch_size': 40, 'step_size': 6, 'gamma': 0.9639328915508999}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:35:32,369][0m Trial 42 finished with value: 0.05200836310784022 and parameters: {'observation_period_num': 17, 'train_rates': 0.9685224940111531, 'learning_rate': 0.00011752347290752504, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9366554677481792}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:38:59,521][0m Trial 43 finished with value: 0.03125339210161875 and parameters: {'observation_period_num': 6, 'train_rates': 0.9409552128307509, 'learning_rate': 0.0003007361314269445, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9899616077502562}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:42:32,540][0m Trial 44 finished with value: 0.04348683597155563 and parameters: {'observation_period_num': 27, 'train_rates': 0.9372966311685814, 'learning_rate': 0.0002796174500912078, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9864674439946034}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:45:56,427][0m Trial 45 finished with value: 0.030435355829781498 and parameters: {'observation_period_num': 15, 'train_rates': 0.921138710797429, 'learning_rate': 0.00013141574616845297, 'batch_size': 27, 'step_size': 11, 'gamma': 0.9732549474791431}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:50:51,461][0m Trial 46 finished with value: 0.13548981103786203 and parameters: {'observation_period_num': 228, 'train_rates': 0.8578265971983688, 'learning_rate': 0.00012436294770642434, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9749929803117233}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:53:55,165][0m Trial 47 finished with value: 0.03312495186065252 and parameters: {'observation_period_num': 15, 'train_rates': 0.9114738572606311, 'learning_rate': 0.0005000833752189851, 'batch_size': 30, 'step_size': 11, 'gamma': 0.9873323445917677}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:54:35,383][0m Trial 48 finished with value: 0.09095820199663394 and parameters: {'observation_period_num': 69, 'train_rates': 0.9260529935263004, 'learning_rate': 0.00031263601925779147, 'batch_size': 146, 'step_size': 13, 'gamma': 0.9539953295520847}. Best is trial 12 with value: 0.026731190346039747.[0m
[32m[I 2025-01-11 07:58:18,311][0m Trial 49 finished with value: 0.07828484742805876 and parameters: {'observation_period_num': 50, 'train_rates': 0.9001166491115015, 'learning_rate': 0.0009809182473902098, 'batch_size': 24, 'step_size': 2, 'gamma': 0.9713062097362594}. Best is trial 12 with value: 0.026731190346039747.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-11 07:58:18,320][0m A new study created in memory with name: no-name-80b1e1b4-36cf-413d-bb03-1f805e621069[0m
[32m[I 2025-01-11 07:58:45,636][0m Trial 0 finished with value: 0.10440652004157404 and parameters: {'observation_period_num': 140, 'train_rates': 0.7321718183433604, 'learning_rate': 0.00039961616157313176, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8463449300584691}. Best is trial 0 with value: 0.10440652004157404.[0m
[32m[I 2025-01-11 07:59:09,317][0m Trial 1 finished with value: 0.7362088561058044 and parameters: {'observation_period_num': 243, 'train_rates': 0.9600232326709404, 'learning_rate': 7.572206511334907e-06, 'batch_size': 247, 'step_size': 9, 'gamma': 0.8735541729664135}. Best is trial 0 with value: 0.10440652004157404.[0m
[32m[I 2025-01-11 08:00:00,491][0m Trial 2 finished with value: 1.243712420623327 and parameters: {'observation_period_num': 93, 'train_rates': 0.8642181397989415, 'learning_rate': 2.70754740204566e-06, 'batch_size': 105, 'step_size': 2, 'gamma': 0.9271433918783313}. Best is trial 0 with value: 0.10440652004157404.[0m
[32m[I 2025-01-11 08:01:35,919][0m Trial 3 finished with value: 0.09731680793421608 and parameters: {'observation_period_num': 226, 'train_rates': 0.9386716404195152, 'learning_rate': 0.0005746288295434307, 'batch_size': 55, 'step_size': 15, 'gamma': 0.9332707832249338}. Best is trial 3 with value: 0.09731680793421608.[0m
Early stopping at epoch 45
[32m[I 2025-01-11 08:02:13,559][0m Trial 4 finished with value: 4.8938927473266265 and parameters: {'observation_period_num': 37, 'train_rates': 0.6860839037905918, 'learning_rate': 4.956740260967616e-06, 'batch_size': 56, 'step_size': 1, 'gamma': 0.781740972721055}. Best is trial 3 with value: 0.09731680793421608.[0m
[32m[I 2025-01-11 08:02:34,520][0m Trial 5 finished with value: 0.13259898902020245 and parameters: {'observation_period_num': 188, 'train_rates': 0.7289720818047298, 'learning_rate': 0.0005604755438538248, 'batch_size': 227, 'step_size': 15, 'gamma': 0.9009633662424428}. Best is trial 3 with value: 0.09731680793421608.[0m
[32m[I 2025-01-11 08:03:24,264][0m Trial 6 finished with value: 0.12433671628980428 and parameters: {'observation_period_num': 176, 'train_rates': 0.770340935426358, 'learning_rate': 0.000267326224667496, 'batch_size': 96, 'step_size': 8, 'gamma': 0.7703398029482378}. Best is trial 3 with value: 0.09731680793421608.[0m
[32m[I 2025-01-11 08:06:42,699][0m Trial 7 finished with value: 0.2692138123833121 and parameters: {'observation_period_num': 215, 'train_rates': 0.6469663039875845, 'learning_rate': 2.315138753552994e-05, 'batch_size': 20, 'step_size': 7, 'gamma': 0.858056854517492}. Best is trial 3 with value: 0.09731680793421608.[0m
[32m[I 2025-01-11 08:07:11,295][0m Trial 8 finished with value: 0.38815415600518216 and parameters: {'observation_period_num': 94, 'train_rates': 0.8336250289022189, 'learning_rate': 1.4341679562911989e-05, 'batch_size': 191, 'step_size': 7, 'gamma': 0.9747666550994959}. Best is trial 3 with value: 0.09731680793421608.[0m
[32m[I 2025-01-11 08:07:38,134][0m Trial 9 finished with value: 0.4571402445435524 and parameters: {'observation_period_num': 215, 'train_rates': 0.8305626971799721, 'learning_rate': 2.5614087885781053e-05, 'batch_size': 191, 'step_size': 13, 'gamma': 0.7557801847129137}. Best is trial 3 with value: 0.09731680793421608.[0m
[32m[I 2025-01-11 08:12:24,672][0m Trial 10 finished with value: 0.03292560098426683 and parameters: {'observation_period_num': 11, 'train_rates': 0.9836297054391738, 'learning_rate': 9.823129213635967e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9844967465285224}. Best is trial 10 with value: 0.03292560098426683.[0m
[32m[I 2025-01-11 08:17:59,705][0m Trial 11 finished with value: 0.039473077903191246 and parameters: {'observation_period_num': 33, 'train_rates': 0.9889501316548572, 'learning_rate': 0.00010998171156994324, 'batch_size': 17, 'step_size': 15, 'gamma': 0.98336871645176}. Best is trial 10 with value: 0.03292560098426683.[0m
[32m[I 2025-01-11 08:23:39,837][0m Trial 12 finished with value: 0.027367406617850067 and parameters: {'observation_period_num': 8, 'train_rates': 0.9866214331441603, 'learning_rate': 9.815105777747816e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9873827873368126}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:25:04,840][0m Trial 13 finished with value: 0.04590940179133957 and parameters: {'observation_period_num': 8, 'train_rates': 0.9029831474881251, 'learning_rate': 0.00010157928680803283, 'batch_size': 65, 'step_size': 12, 'gamma': 0.9539825207450544}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:25:46,914][0m Trial 14 finished with value: 0.12740458077055283 and parameters: {'observation_period_num': 67, 'train_rates': 0.9096094961610696, 'learning_rate': 8.365995815807753e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8189010662316745}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:28:23,212][0m Trial 15 finished with value: 0.03742084504579598 and parameters: {'observation_period_num': 5, 'train_rates': 0.9820997388258668, 'learning_rate': 5.918849120658513e-05, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9849779090415793}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:29:09,593][0m Trial 16 finished with value: 0.21288728320005676 and parameters: {'observation_period_num': 52, 'train_rates': 0.6047155691360104, 'learning_rate': 0.00021538949096146665, 'batch_size': 90, 'step_size': 4, 'gamma': 0.943173799298799}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:29:51,382][0m Trial 17 finished with value: 0.1636734372915182 and parameters: {'observation_period_num': 86, 'train_rates': 0.9072898825269103, 'learning_rate': 4.740224051571792e-05, 'batch_size': 136, 'step_size': 13, 'gamma': 0.9050282996606407}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:31:05,286][0m Trial 18 finished with value: 0.08580878559957471 and parameters: {'observation_period_num': 145, 'train_rates': 0.9390612616120116, 'learning_rate': 0.0001253465414483528, 'batch_size': 75, 'step_size': 11, 'gamma': 0.9603003871769737}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:31:45,436][0m Trial 19 finished with value: 0.46040319001421015 and parameters: {'observation_period_num': 117, 'train_rates': 0.8674732067801079, 'learning_rate': 1.35286244639528e-05, 'batch_size': 133, 'step_size': 5, 'gamma': 0.9033591565463275}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:34:00,286][0m Trial 20 finished with value: 0.04352706979314788 and parameters: {'observation_period_num': 28, 'train_rates': 0.7749472604282843, 'learning_rate': 0.0009609040365810116, 'batch_size': 36, 'step_size': 14, 'gamma': 0.8208194317607558}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:36:33,567][0m Trial 21 finished with value: 0.03456594794988632 and parameters: {'observation_period_num': 7, 'train_rates': 0.9892843161161762, 'learning_rate': 4.9554548848484754e-05, 'batch_size': 38, 'step_size': 13, 'gamma': 0.9888086974639299}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:39:01,548][0m Trial 22 finished with value: 0.07238392080486256 and parameters: {'observation_period_num': 58, 'train_rates': 0.952853474704514, 'learning_rate': 4.4725818442164447e-05, 'batch_size': 38, 'step_size': 12, 'gamma': 0.9892371538486783}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:44:23,816][0m Trial 23 finished with value: 0.0321417271214373 and parameters: {'observation_period_num': 17, 'train_rates': 0.9885896258844445, 'learning_rate': 0.00021184733218486328, 'batch_size': 18, 'step_size': 14, 'gamma': 0.9635358513728389}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:48:57,529][0m Trial 24 finished with value: 0.39711206153738654 and parameters: {'observation_period_num': 28, 'train_rates': 0.9205826191514631, 'learning_rate': 1.106269465590537e-06, 'batch_size': 20, 'step_size': 14, 'gamma': 0.9640936710384129}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:50:02,648][0m Trial 25 finished with value: 0.058182442508762776 and parameters: {'observation_period_num': 73, 'train_rates': 0.875907992381967, 'learning_rate': 0.00016446463490697928, 'batch_size': 82, 'step_size': 11, 'gamma': 0.919265086016959}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:50:53,696][0m Trial 26 finished with value: 0.0662325769662857 and parameters: {'observation_period_num': 48, 'train_rates': 0.9649244052457833, 'learning_rate': 0.0002839656307140871, 'batch_size': 114, 'step_size': 14, 'gamma': 0.9490246002084568}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:52:33,604][0m Trial 27 finished with value: 0.043054544246258514 and parameters: {'observation_period_num': 18, 'train_rates': 0.9342496823244237, 'learning_rate': 0.0001688980210783562, 'batch_size': 57, 'step_size': 15, 'gamma': 0.9651708957576574}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:54:41,387][0m Trial 28 finished with value: 0.051490851606313996 and parameters: {'observation_period_num': 46, 'train_rates': 0.9641987290914118, 'learning_rate': 0.000936083431181761, 'batch_size': 44, 'step_size': 10, 'gamma': 0.9399387355515357}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:55:17,519][0m Trial 29 finished with value: 0.06954827131790535 and parameters: {'observation_period_num': 114, 'train_rates': 0.8870232274335152, 'learning_rate': 0.00034014925082055826, 'batch_size': 153, 'step_size': 12, 'gamma': 0.878911405168387}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:56:26,726][0m Trial 30 finished with value: 0.09063463555548781 and parameters: {'observation_period_num': 145, 'train_rates': 0.8343743491035572, 'learning_rate': 7.792573841378858e-05, 'batch_size': 73, 'step_size': 10, 'gamma': 0.9710003652949063}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 08:59:55,372][0m Trial 31 finished with value: 0.05742318257689476 and parameters: {'observation_period_num': 19, 'train_rates': 0.9847929167185534, 'learning_rate': 3.14386468553404e-05, 'batch_size': 28, 'step_size': 13, 'gamma': 0.9847721774078604}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 09:01:59,067][0m Trial 32 finished with value: 0.04433704164022437 and parameters: {'observation_period_num': 5, 'train_rates': 0.9649028954330653, 'learning_rate': 6.532464621953393e-05, 'batch_size': 47, 'step_size': 14, 'gamma': 0.9717988315363216}. Best is trial 12 with value: 0.027367406617850067.[0m
[32m[I 2025-01-11 09:07:36,538][0m Trial 33 finished with value: 0.025998133825711332 and parameters: {'observation_period_num': 19, 'train_rates': 0.982482874333066, 'learning_rate': 0.00015049424790659728, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9191662542502691}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:13:01,882][0m Trial 34 finished with value: 0.07511887554869508 and parameters: {'observation_period_num': 39, 'train_rates': 0.943642859639894, 'learning_rate': 0.00045754609049263116, 'batch_size': 17, 'step_size': 9, 'gamma': 0.9229523916615202}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:14:46,052][0m Trial 35 finished with value: 0.046626260690391066 and parameters: {'observation_period_num': 23, 'train_rates': 0.9251737979978836, 'learning_rate': 0.00014798086688184094, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8861869148414439}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:18:00,027][0m Trial 36 finished with value: 0.05371581847375294 and parameters: {'observation_period_num': 68, 'train_rates': 0.9634377555575764, 'learning_rate': 0.00020851690605044337, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9322928055185613}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:18:26,348][0m Trial 37 finished with value: 0.06561694771440979 and parameters: {'observation_period_num': 18, 'train_rates': 0.8958322456851391, 'learning_rate': 0.0004534693359448465, 'batch_size': 236, 'step_size': 15, 'gamma': 0.953164582233313}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:19:37,387][0m Trial 38 finished with value: 0.08695004855282605 and parameters: {'observation_period_num': 84, 'train_rates': 0.7211695486225306, 'learning_rate': 9.855774375344522e-05, 'batch_size': 66, 'step_size': 14, 'gamma': 0.9183139824230634}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:22:50,505][0m Trial 39 finished with value: 0.05054437927901745 and parameters: {'observation_period_num': 38, 'train_rates': 0.9486761661712309, 'learning_rate': 0.00026721908133907156, 'batch_size': 29, 'step_size': 9, 'gamma': 0.9389512384565827}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:24:34,069][0m Trial 40 finished with value: 0.12039705082316207 and parameters: {'observation_period_num': 51, 'train_rates': 0.8543656256384596, 'learning_rate': 0.0006536056819019165, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8940139980421395}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:28:16,144][0m Trial 41 finished with value: 0.0346145239762134 and parameters: {'observation_period_num': 14, 'train_rates': 0.9877541823020417, 'learning_rate': 3.8372522450409416e-05, 'batch_size': 26, 'step_size': 13, 'gamma': 0.9768102491051902}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:34:05,323][0m Trial 42 finished with value: 0.05229108458435213 and parameters: {'observation_period_num': 28, 'train_rates': 0.974919516737776, 'learning_rate': 1.9407286523613808e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9898905981058498}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:36:26,230][0m Trial 43 finished with value: 0.03692987160844224 and parameters: {'observation_period_num': 6, 'train_rates': 0.9276406817917258, 'learning_rate': 5.9010048945400825e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.962678925713005}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:39:33,190][0m Trial 44 finished with value: 0.14243055344559252 and parameters: {'observation_period_num': 40, 'train_rates': 0.9891558845749094, 'learning_rate': 8.783616548867128e-06, 'batch_size': 31, 'step_size': 11, 'gamma': 0.865306057013841}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:41:11,095][0m Trial 45 finished with value: 0.09493393050645714 and parameters: {'observation_period_num': 181, 'train_rates': 0.9518606443875355, 'learning_rate': 0.00012333800964062434, 'batch_size': 55, 'step_size': 13, 'gamma': 0.9792767163139054}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:41:40,787][0m Trial 46 finished with value: 0.08271384239196777 and parameters: {'observation_period_num': 16, 'train_rates': 0.9725110587329222, 'learning_rate': 8.50717802844184e-05, 'batch_size': 205, 'step_size': 15, 'gamma': 0.9530053056506604}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:42:59,647][0m Trial 47 finished with value: 0.05146161356785909 and parameters: {'observation_period_num': 30, 'train_rates': 0.7984926290746909, 'learning_rate': 0.0002205883635631142, 'batch_size': 63, 'step_size': 1, 'gamma': 0.9735524135262712}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:43:44,291][0m Trial 48 finished with value: 0.13876545990634465 and parameters: {'observation_period_num': 55, 'train_rates': 0.704447737621393, 'learning_rate': 3.445083030290231e-05, 'batch_size': 105, 'step_size': 8, 'gamma': 0.9598584942906901}. Best is trial 33 with value: 0.025998133825711332.[0m
[32m[I 2025-01-11 09:45:19,796][0m Trial 49 finished with value: 0.24858335865409462 and parameters: {'observation_period_num': 252, 'train_rates': 0.6630878089145339, 'learning_rate': 5.679449518494953e-05, 'batch_size': 42, 'step_size': 7, 'gamma': 0.8433861212430293}. Best is trial 33 with value: 0.025998133825711332.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-11 09:45:19,807][0m A new study created in memory with name: no-name-927cfc88-b445-4016-8143-02b0a0311d9e[0m
[32m[I 2025-01-11 09:45:51,957][0m Trial 0 finished with value: 0.1971460637909856 and parameters: {'observation_period_num': 156, 'train_rates': 0.7125381711992801, 'learning_rate': 0.00037401857498919075, 'batch_size': 149, 'step_size': 4, 'gamma': 0.7680614413688358}. Best is trial 0 with value: 0.1971460637909856.[0m
[32m[I 2025-01-11 09:47:00,288][0m Trial 1 finished with value: 0.12977335651329056 and parameters: {'observation_period_num': 239, 'train_rates': 0.905332418789752, 'learning_rate': 7.5822172702969e-05, 'batch_size': 76, 'step_size': 4, 'gamma': 0.9790618334126975}. Best is trial 1 with value: 0.12977335651329056.[0m
[32m[I 2025-01-11 09:47:36,389][0m Trial 2 finished with value: 1.1829828624631844 and parameters: {'observation_period_num': 242, 'train_rates': 0.7178067072667655, 'learning_rate': 1.093498049059411e-06, 'batch_size': 127, 'step_size': 13, 'gamma': 0.939148918580897}. Best is trial 1 with value: 0.12977335651329056.[0m
[32m[I 2025-01-11 09:48:03,263][0m Trial 3 finished with value: 0.7880725077220372 and parameters: {'observation_period_num': 181, 'train_rates': 0.8359185965625286, 'learning_rate': 5.117396900409573e-06, 'batch_size': 208, 'step_size': 9, 'gamma': 0.8353887175362884}. Best is trial 1 with value: 0.12977335651329056.[0m
[32m[I 2025-01-11 09:48:26,904][0m Trial 4 finished with value: 1.038073182106018 and parameters: {'observation_period_num': 137, 'train_rates': 0.9305866183895819, 'learning_rate': 6.843960571005454e-06, 'batch_size': 248, 'step_size': 1, 'gamma': 0.9655943807374021}. Best is trial 1 with value: 0.12977335651329056.[0m
[32m[I 2025-01-11 09:48:50,231][0m Trial 5 finished with value: 0.1429453982213109 and parameters: {'observation_period_num': 43, 'train_rates': 0.699997822850925, 'learning_rate': 5.2227341316835174e-05, 'batch_size': 207, 'step_size': 5, 'gamma': 0.9806707693629967}. Best is trial 1 with value: 0.12977335651329056.[0m
[32m[I 2025-01-11 09:52:14,670][0m Trial 6 finished with value: 0.04759276746878407 and parameters: {'observation_period_num': 68, 'train_rates': 0.89369885999556, 'learning_rate': 0.00022919325873759828, 'batch_size': 26, 'step_size': 13, 'gamma': 0.867542243652726}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 09:52:43,839][0m Trial 7 finished with value: 0.05901397724000972 and parameters: {'observation_period_num': 33, 'train_rates': 0.7803447503381347, 'learning_rate': 0.00044171350808188054, 'batch_size': 182, 'step_size': 13, 'gamma': 0.8386870202190535}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 09:53:04,847][0m Trial 8 finished with value: 0.11930742702986065 and parameters: {'observation_period_num': 54, 'train_rates': 0.711362053183914, 'learning_rate': 0.0006611620162188819, 'batch_size': 256, 'step_size': 3, 'gamma': 0.8707671274198192}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 09:54:20,310][0m Trial 9 finished with value: 0.14462210827655664 and parameters: {'observation_period_num': 237, 'train_rates': 0.64667732287098, 'learning_rate': 0.0006047993133669684, 'batch_size': 53, 'step_size': 12, 'gamma': 0.750511521556206}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 09:59:00,173][0m Trial 10 finished with value: 0.054318760987371206 and parameters: {'observation_period_num': 90, 'train_rates': 0.9779283678214209, 'learning_rate': 0.00011970304391558257, 'batch_size': 20, 'step_size': 9, 'gamma': 0.9095256697073866}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:03:53,623][0m Trial 11 finished with value: 0.06145922828684835 and parameters: {'observation_period_num': 89, 'train_rates': 0.9883761365981835, 'learning_rate': 0.00010590660345350743, 'batch_size': 19, 'step_size': 9, 'gamma': 0.8993142709575318}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:07:24,201][0m Trial 12 finished with value: 0.05441610598936677 and parameters: {'observation_period_num': 86, 'train_rates': 0.9860597574712459, 'learning_rate': 0.0001723178559364284, 'batch_size': 27, 'step_size': 15, 'gamma': 0.9149099970818448}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:08:21,626][0m Trial 13 finished with value: 0.34524538192567944 and parameters: {'observation_period_num': 97, 'train_rates': 0.8616537912101834, 'learning_rate': 2.065529265475189e-05, 'batch_size': 93, 'step_size': 7, 'gamma': 0.8597495860541126}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:09:56,055][0m Trial 14 finished with value: 0.09177009340484771 and parameters: {'observation_period_num': 6, 'train_rates': 0.9190846540920989, 'learning_rate': 2.0256521841772075e-05, 'batch_size': 59, 'step_size': 11, 'gamma': 0.7981071021540171}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:10:51,450][0m Trial 15 finished with value: 0.08266244166427189 and parameters: {'observation_period_num': 113, 'train_rates': 0.8668802093209071, 'learning_rate': 0.00012856701994707733, 'batch_size': 98, 'step_size': 15, 'gamma': 0.9004772668425223}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:13:19,969][0m Trial 16 finished with value: 0.0890762848513467 and parameters: {'observation_period_num': 68, 'train_rates': 0.9540415321789522, 'learning_rate': 0.00022045624551597176, 'batch_size': 38, 'step_size': 7, 'gamma': 0.936184653463666}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:14:03,294][0m Trial 17 finished with value: 0.07647261926025714 and parameters: {'observation_period_num': 182, 'train_rates': 0.8088409985604497, 'learning_rate': 0.0009796161117786454, 'batch_size': 116, 'step_size': 10, 'gamma': 0.8699164554487719}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:15:33,067][0m Trial 18 finished with value: 0.06707865681992956 and parameters: {'observation_period_num': 11, 'train_rates': 0.8873304933376874, 'learning_rate': 4.05622897430034e-05, 'batch_size': 62, 'step_size': 13, 'gamma': 0.8099896434425837}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:16:12,435][0m Trial 19 finished with value: 0.1164439469575882 and parameters: {'observation_period_num': 136, 'train_rates': 0.9522111144469488, 'learning_rate': 0.00021721197235192769, 'batch_size': 146, 'step_size': 7, 'gamma': 0.8885073156570925}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:20:20,274][0m Trial 20 finished with value: 0.15861501672564174 and parameters: {'observation_period_num': 67, 'train_rates': 0.7657008894115692, 'learning_rate': 7.140116543906716e-06, 'batch_size': 19, 'step_size': 11, 'gamma': 0.9338162575745663}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:22:59,207][0m Trial 21 finished with value: 0.06603602177285134 and parameters: {'observation_period_num': 88, 'train_rates': 0.9838566460662531, 'learning_rate': 0.0002040337993115157, 'batch_size': 36, 'step_size': 15, 'gamma': 0.9170959650923631}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:28:43,543][0m Trial 22 finished with value: 0.06706436211713637 and parameters: {'observation_period_num': 110, 'train_rates': 0.965481436519631, 'learning_rate': 0.00011467343913244333, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9182900825934561}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:30:57,239][0m Trial 23 finished with value: 0.0630017340183258 and parameters: {'observation_period_num': 73, 'train_rates': 0.9288227265684837, 'learning_rate': 0.000194601502055065, 'batch_size': 41, 'step_size': 14, 'gamma': 0.9546439970224824}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:32:06,688][0m Trial 24 finished with value: 0.0646919359271301 and parameters: {'observation_period_num': 28, 'train_rates': 0.8879068736341413, 'learning_rate': 6.884496093093677e-05, 'batch_size': 79, 'step_size': 12, 'gamma': 0.8873799862243904}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:34:31,939][0m Trial 25 finished with value: 0.16502511501312256 and parameters: {'observation_period_num': 120, 'train_rates': 0.9894945550565318, 'learning_rate': 2.7166370896920283e-05, 'batch_size': 39, 'step_size': 14, 'gamma': 0.8481869913604976}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:35:49,884][0m Trial 26 finished with value: 0.06521375922124777 and parameters: {'observation_period_num': 55, 'train_rates': 0.9540200701734894, 'learning_rate': 0.00033521440594286105, 'batch_size': 74, 'step_size': 11, 'gamma': 0.9156757395904174}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:36:41,287][0m Trial 27 finished with value: 0.09889533230188217 and parameters: {'observation_period_num': 159, 'train_rates': 0.8425080397388112, 'learning_rate': 0.0001501301767888275, 'batch_size': 101, 'step_size': 9, 'gamma': 0.816606562471315}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:38:00,192][0m Trial 28 finished with value: 0.3098379431624028 and parameters: {'observation_period_num': 99, 'train_rates': 0.6043317133103621, 'learning_rate': 8.658571687885276e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8814380562341348}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:40:59,127][0m Trial 29 finished with value: 0.06133150594751183 and parameters: {'observation_period_num': 79, 'train_rates': 0.9034425047629676, 'learning_rate': 0.00033312761751491687, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9009198984482294}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:41:33,541][0m Trial 30 finished with value: 0.6722074757019679 and parameters: {'observation_period_num': 150, 'train_rates': 0.9357798459779592, 'learning_rate': 1.4378817043775644e-05, 'batch_size': 174, 'step_size': 8, 'gamma': 0.7821018183900458}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:42:02,942][0m Trial 31 finished with value: 0.05773354678891087 and parameters: {'observation_period_num': 39, 'train_rates': 0.7751352604588356, 'learning_rate': 0.0004658791347847032, 'batch_size': 178, 'step_size': 13, 'gamma': 0.834373530521298}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:42:32,366][0m Trial 32 finished with value: 0.0828522483818233 and parameters: {'observation_period_num': 54, 'train_rates': 0.737820712864984, 'learning_rate': 0.0004132969282592766, 'batch_size': 171, 'step_size': 14, 'gamma': 0.8502853542212793}. Best is trial 6 with value: 0.04759276746878407.[0m
[32m[I 2025-01-11 10:42:59,097][0m Trial 33 finished with value: 0.04045137505227622 and parameters: {'observation_period_num': 26, 'train_rates': 0.8203562967858251, 'learning_rate': 0.0009401849474394205, 'batch_size': 200, 'step_size': 13, 'gamma': 0.8266745550238258}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:43:25,677][0m Trial 34 finished with value: 0.052767338478840846 and parameters: {'observation_period_num': 22, 'train_rates': 0.8229812091072136, 'learning_rate': 0.0008774385424050834, 'batch_size': 207, 'step_size': 5, 'gamma': 0.8233943490065762}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:43:50,186][0m Trial 35 finished with value: 0.07381303936516473 and parameters: {'observation_period_num': 21, 'train_rates': 0.8140955483418857, 'learning_rate': 0.0007461105893381159, 'batch_size': 224, 'step_size': 3, 'gamma': 0.8206921427497726}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:44:14,637][0m Trial 36 finished with value: 0.05717363131457362 and parameters: {'observation_period_num': 17, 'train_rates': 0.8320048903803802, 'learning_rate': 0.0008912202674504944, 'batch_size': 227, 'step_size': 4, 'gamma': 0.7907253601054017}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:44:43,066][0m Trial 37 finished with value: 0.07963223437347063 and parameters: {'observation_period_num': 45, 'train_rates': 0.8589206144796431, 'learning_rate': 0.00030705442651517015, 'batch_size': 194, 'step_size': 6, 'gamma': 0.8274081782001562}. Best is trial 33 with value: 0.04045137505227622.[0m
Early stopping at epoch 42
[32m[I 2025-01-11 10:44:57,456][0m Trial 38 finished with value: 2.22871214353022 and parameters: {'observation_period_num': 28, 'train_rates': 0.7471073182004335, 'learning_rate': 2.4221828281510025e-06, 'batch_size': 157, 'step_size': 1, 'gamma': 0.8044193268110856}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:45:20,066][0m Trial 39 finished with value: 0.10566754176960153 and parameters: {'observation_period_num': 52, 'train_rates': 0.7967172716140783, 'learning_rate': 0.000542133613858504, 'batch_size': 238, 'step_size': 5, 'gamma': 0.7648677428314689}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:45:46,526][0m Trial 40 finished with value: 0.25963242564004724 and parameters: {'observation_period_num': 64, 'train_rates': 0.8322449622089383, 'learning_rate': 6.121465053541215e-05, 'batch_size': 206, 'step_size': 10, 'gamma': 0.8601805483295106}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:46:16,901][0m Trial 41 finished with value: 0.06457299313374928 and parameters: {'observation_period_num': 38, 'train_rates': 0.8846897187101718, 'learning_rate': 0.0002845753242897189, 'batch_size': 192, 'step_size': 3, 'gamma': 0.989379994854423}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:47:04,306][0m Trial 42 finished with value: 0.07525478303432465 and parameters: {'observation_period_num': 82, 'train_rates': 0.9717388301572322, 'learning_rate': 0.000605105663483578, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9458350718270867}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:47:30,622][0m Trial 43 finished with value: 0.15539977426234358 and parameters: {'observation_period_num': 102, 'train_rates': 0.9091230634895585, 'learning_rate': 0.0001630179727408333, 'batch_size': 218, 'step_size': 13, 'gamma': 0.8448325744694807}. Best is trial 33 with value: 0.04045137505227622.[0m
[32m[I 2025-01-11 10:48:51,085][0m Trial 44 finished with value: 0.033807881629813914 and parameters: {'observation_period_num': 6, 'train_rates': 0.9359439916256783, 'learning_rate': 0.0009951404550119485, 'batch_size': 71, 'step_size': 8, 'gamma': 0.873297258564811}. Best is trial 44 with value: 0.033807881629813914.[0m
[32m[I 2025-01-11 10:50:10,388][0m Trial 45 finished with value: 0.030313233858146143 and parameters: {'observation_period_num': 8, 'train_rates': 0.9042617893974346, 'learning_rate': 0.0009818737100685557, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8731808613274266}. Best is trial 45 with value: 0.030313233858146143.[0m
[32m[I 2025-01-11 10:51:19,222][0m Trial 46 finished with value: 0.030704815243560123 and parameters: {'observation_period_num': 5, 'train_rates': 0.8747582251788678, 'learning_rate': 0.0007477786014489872, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8719090822260346}. Best is trial 45 with value: 0.030313233858146143.[0m
[32m[I 2025-01-11 10:52:22,723][0m Trial 47 finished with value: 0.033220875950664586 and parameters: {'observation_period_num': 5, 'train_rates': 0.8685229603905854, 'learning_rate': 0.0006361333140302714, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8750845187170194}. Best is trial 45 with value: 0.030313233858146143.[0m
[32m[I 2025-01-11 10:53:23,672][0m Trial 48 finished with value: 0.0305920522660017 and parameters: {'observation_period_num': 5, 'train_rates': 0.8679641658083762, 'learning_rate': 0.0007042697546464776, 'batch_size': 89, 'step_size': 8, 'gamma': 0.8625140163497113}. Best is trial 45 with value: 0.030313233858146143.[0m
[32m[I 2025-01-11 10:54:27,041][0m Trial 49 finished with value: 0.031182197072813587 and parameters: {'observation_period_num': 8, 'train_rates': 0.8735763834610811, 'learning_rate': 0.0006838134054595931, 'batch_size': 88, 'step_size': 8, 'gamma': 0.875416263607544}. Best is trial 45 with value: 0.030313233858146143.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 11, 'train_rates': 0.7981524305770709, 'learning_rate': 0.0005689261112441613, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7972821710517621}
Epoch 1/300, trend Loss: 0.1981 | 0.1265
Epoch 2/300, trend Loss: 0.1159 | 0.0849
Epoch 3/300, trend Loss: 0.1076 | 0.0797
Epoch 4/300, trend Loss: 0.1014 | 0.0912
Epoch 5/300, trend Loss: 0.0978 | 0.0930
Epoch 6/300, trend Loss: 0.0946 | 0.0678
Epoch 7/300, trend Loss: 0.0913 | 0.0638
Epoch 8/300, trend Loss: 0.0886 | 0.0614
Epoch 9/300, trend Loss: 0.0855 | 0.0530
Epoch 10/300, trend Loss: 0.0823 | 0.0510
Epoch 11/300, trend Loss: 0.0793 | 0.0477
Epoch 12/300, trend Loss: 0.0773 | 0.0460
Epoch 13/300, trend Loss: 0.0760 | 0.0449
Epoch 14/300, trend Loss: 0.0747 | 0.0424
Epoch 15/300, trend Loss: 0.0738 | 0.0406
Epoch 16/300, trend Loss: 0.0725 | 0.0402
Epoch 17/300, trend Loss: 0.0718 | 0.0387
Epoch 18/300, trend Loss: 0.0711 | 0.0377
Epoch 19/300, trend Loss: 0.0704 | 0.0378
Epoch 20/300, trend Loss: 0.0700 | 0.0367
Epoch 21/300, trend Loss: 0.0694 | 0.0363
Epoch 22/300, trend Loss: 0.0690 | 0.0356
Epoch 23/300, trend Loss: 0.0687 | 0.0351
Epoch 24/300, trend Loss: 0.0683 | 0.0339
Epoch 25/300, trend Loss: 0.0680 | 0.0336
Epoch 26/300, trend Loss: 0.0677 | 0.0323
Epoch 27/300, trend Loss: 0.0675 | 0.0319
Epoch 28/300, trend Loss: 0.0673 | 0.0316
Epoch 29/300, trend Loss: 0.0671 | 0.0312
Epoch 30/300, trend Loss: 0.0670 | 0.0309
Epoch 31/300, trend Loss: 0.0667 | 0.0311
Epoch 32/300, trend Loss: 0.0666 | 0.0309
Epoch 33/300, trend Loss: 0.0665 | 0.0308
Epoch 34/300, trend Loss: 0.0663 | 0.0311
Epoch 35/300, trend Loss: 0.0663 | 0.0310
Epoch 36/300, trend Loss: 0.0661 | 0.0312
Epoch 37/300, trend Loss: 0.0660 | 0.0310
Epoch 38/300, trend Loss: 0.0659 | 0.0308
Epoch 39/300, trend Loss: 0.0658 | 0.0308
Epoch 40/300, trend Loss: 0.0657 | 0.0306
Epoch 41/300, trend Loss: 0.0656 | 0.0305
Epoch 42/300, trend Loss: 0.0655 | 0.0304
Epoch 43/300, trend Loss: 0.0654 | 0.0303
Epoch 44/300, trend Loss: 0.0653 | 0.0301
Epoch 45/300, trend Loss: 0.0652 | 0.0301
Epoch 46/300, trend Loss: 0.0652 | 0.0299
Epoch 47/300, trend Loss: 0.0651 | 0.0299
Epoch 48/300, trend Loss: 0.0651 | 0.0299
Epoch 49/300, trend Loss: 0.0650 | 0.0297
Epoch 50/300, trend Loss: 0.0649 | 0.0297
Epoch 51/300, trend Loss: 0.0649 | 0.0296
Epoch 52/300, trend Loss: 0.0649 | 0.0297
Epoch 53/300, trend Loss: 0.0648 | 0.0297
Epoch 54/300, trend Loss: 0.0648 | 0.0296
Epoch 55/300, trend Loss: 0.0647 | 0.0296
Epoch 56/300, trend Loss: 0.0647 | 0.0296
Epoch 57/300, trend Loss: 0.0647 | 0.0296
Epoch 58/300, trend Loss: 0.0647 | 0.0296
Epoch 59/300, trend Loss: 0.0646 | 0.0296
Epoch 60/300, trend Loss: 0.0646 | 0.0296
Epoch 61/300, trend Loss: 0.0646 | 0.0296
Epoch 62/300, trend Loss: 0.0646 | 0.0296
Epoch 63/300, trend Loss: 0.0646 | 0.0296
Epoch 64/300, trend Loss: 0.0646 | 0.0296
Epoch 65/300, trend Loss: 0.0645 | 0.0296
Epoch 66/300, trend Loss: 0.0645 | 0.0296
Epoch 67/300, trend Loss: 0.0645 | 0.0296
Epoch 68/300, trend Loss: 0.0645 | 0.0296
Epoch 69/300, trend Loss: 0.0645 | 0.0296
Epoch 70/300, trend Loss: 0.0645 | 0.0296
Epoch 71/300, trend Loss: 0.0645 | 0.0296
Epoch 72/300, trend Loss: 0.0645 | 0.0296
Epoch 73/300, trend Loss: 0.0645 | 0.0296
Epoch 74/300, trend Loss: 0.0644 | 0.0296
Epoch 75/300, trend Loss: 0.0644 | 0.0296
Epoch 76/300, trend Loss: 0.0644 | 0.0296
Epoch 77/300, trend Loss: 0.0644 | 0.0296
Epoch 78/300, trend Loss: 0.0644 | 0.0296
Epoch 79/300, trend Loss: 0.0644 | 0.0296
Epoch 80/300, trend Loss: 0.0644 | 0.0296
Epoch 81/300, trend Loss: 0.0644 | 0.0296
Epoch 82/300, trend Loss: 0.0644 | 0.0296
Epoch 83/300, trend Loss: 0.0644 | 0.0296
Epoch 84/300, trend Loss: 0.0644 | 0.0296
Epoch 85/300, trend Loss: 0.0644 | 0.0295
Epoch 86/300, trend Loss: 0.0644 | 0.0295
Epoch 87/300, trend Loss: 0.0644 | 0.0295
Epoch 88/300, trend Loss: 0.0644 | 0.0295
Epoch 89/300, trend Loss: 0.0644 | 0.0295
Epoch 90/300, trend Loss: 0.0644 | 0.0295
Epoch 91/300, trend Loss: 0.0644 | 0.0295
Epoch 92/300, trend Loss: 0.0644 | 0.0295
Epoch 93/300, trend Loss: 0.0644 | 0.0295
Epoch 94/300, trend Loss: 0.0644 | 0.0295
Epoch 95/300, trend Loss: 0.0644 | 0.0295
Epoch 96/300, trend Loss: 0.0644 | 0.0295
Epoch 97/300, trend Loss: 0.0644 | 0.0295
Epoch 98/300, trend Loss: 0.0644 | 0.0295
Epoch 99/300, trend Loss: 0.0644 | 0.0295
Epoch 100/300, trend Loss: 0.0644 | 0.0295
Epoch 101/300, trend Loss: 0.0644 | 0.0295
Epoch 102/300, trend Loss: 0.0644 | 0.0295
Epoch 103/300, trend Loss: 0.0644 | 0.0295
Epoch 104/300, trend Loss: 0.0644 | 0.0295
Epoch 105/300, trend Loss: 0.0644 | 0.0295
Epoch 106/300, trend Loss: 0.0644 | 0.0295
Epoch 107/300, trend Loss: 0.0644 | 0.0295
Epoch 108/300, trend Loss: 0.0644 | 0.0295
Epoch 109/300, trend Loss: 0.0644 | 0.0295
Epoch 110/300, trend Loss: 0.0644 | 0.0295
Epoch 111/300, trend Loss: 0.0644 | 0.0295
Epoch 112/300, trend Loss: 0.0644 | 0.0295
Epoch 113/300, trend Loss: 0.0644 | 0.0295
Epoch 114/300, trend Loss: 0.0644 | 0.0295
Epoch 115/300, trend Loss: 0.0644 | 0.0295
Epoch 116/300, trend Loss: 0.0644 | 0.0295
Epoch 117/300, trend Loss: 0.0644 | 0.0295
Epoch 118/300, trend Loss: 0.0644 | 0.0295
Epoch 119/300, trend Loss: 0.0644 | 0.0295
Epoch 120/300, trend Loss: 0.0644 | 0.0295
Epoch 121/300, trend Loss: 0.0644 | 0.0295
Epoch 122/300, trend Loss: 0.0644 | 0.0295
Epoch 123/300, trend Loss: 0.0644 | 0.0295
Epoch 124/300, trend Loss: 0.0644 | 0.0295
Epoch 125/300, trend Loss: 0.0644 | 0.0295
Epoch 126/300, trend Loss: 0.0644 | 0.0295
Epoch 127/300, trend Loss: 0.0644 | 0.0295
Epoch 128/300, trend Loss: 0.0644 | 0.0295
Epoch 129/300, trend Loss: 0.0644 | 0.0295
Epoch 130/300, trend Loss: 0.0644 | 0.0295
Epoch 131/300, trend Loss: 0.0644 | 0.0295
Epoch 132/300, trend Loss: 0.0644 | 0.0295
Epoch 133/300, trend Loss: 0.0644 | 0.0295
Epoch 134/300, trend Loss: 0.0644 | 0.0295
Epoch 135/300, trend Loss: 0.0644 | 0.0295
Epoch 136/300, trend Loss: 0.0644 | 0.0295
Epoch 137/300, trend Loss: 0.0644 | 0.0295
Epoch 138/300, trend Loss: 0.0644 | 0.0295
Epoch 139/300, trend Loss: 0.0644 | 0.0295
Epoch 140/300, trend Loss: 0.0644 | 0.0295
Epoch 141/300, trend Loss: 0.0644 | 0.0295
Epoch 142/300, trend Loss: 0.0644 | 0.0295
Epoch 143/300, trend Loss: 0.0644 | 0.0295
Epoch 144/300, trend Loss: 0.0644 | 0.0295
Epoch 145/300, trend Loss: 0.0644 | 0.0295
Epoch 146/300, trend Loss: 0.0644 | 0.0295
Epoch 147/300, trend Loss: 0.0644 | 0.0295
Epoch 148/300, trend Loss: 0.0644 | 0.0295
Epoch 149/300, trend Loss: 0.0644 | 0.0295
Epoch 150/300, trend Loss: 0.0644 | 0.0295
Epoch 151/300, trend Loss: 0.0644 | 0.0295
Epoch 152/300, trend Loss: 0.0644 | 0.0295
Epoch 153/300, trend Loss: 0.0644 | 0.0295
Epoch 154/300, trend Loss: 0.0644 | 0.0295
Epoch 155/300, trend Loss: 0.0644 | 0.0295
Epoch 156/300, trend Loss: 0.0644 | 0.0295
Epoch 157/300, trend Loss: 0.0644 | 0.0295
Epoch 158/300, trend Loss: 0.0644 | 0.0295
Epoch 159/300, trend Loss: 0.0644 | 0.0295
Epoch 160/300, trend Loss: 0.0644 | 0.0295
Epoch 161/300, trend Loss: 0.0644 | 0.0295
Epoch 162/300, trend Loss: 0.0644 | 0.0295
Epoch 163/300, trend Loss: 0.0644 | 0.0295
Epoch 164/300, trend Loss: 0.0644 | 0.0295
Epoch 165/300, trend Loss: 0.0644 | 0.0295
Epoch 166/300, trend Loss: 0.0644 | 0.0295
Epoch 167/300, trend Loss: 0.0644 | 0.0295
Epoch 168/300, trend Loss: 0.0644 | 0.0295
Epoch 169/300, trend Loss: 0.0644 | 0.0295
Epoch 170/300, trend Loss: 0.0644 | 0.0295
Epoch 171/300, trend Loss: 0.0644 | 0.0295
Epoch 172/300, trend Loss: 0.0644 | 0.0295
Epoch 173/300, trend Loss: 0.0644 | 0.0295
Epoch 174/300, trend Loss: 0.0644 | 0.0295
Epoch 175/300, trend Loss: 0.0644 | 0.0295
Epoch 176/300, trend Loss: 0.0644 | 0.0295
Epoch 177/300, trend Loss: 0.0644 | 0.0295
Epoch 178/300, trend Loss: 0.0644 | 0.0295
Epoch 179/300, trend Loss: 0.0644 | 0.0295
Epoch 180/300, trend Loss: 0.0644 | 0.0295
Epoch 181/300, trend Loss: 0.0644 | 0.0295
Epoch 182/300, trend Loss: 0.0644 | 0.0295
Epoch 183/300, trend Loss: 0.0644 | 0.0295
Epoch 184/300, trend Loss: 0.0644 | 0.0295
Epoch 185/300, trend Loss: 0.0644 | 0.0295
Epoch 186/300, trend Loss: 0.0644 | 0.0295
Epoch 187/300, trend Loss: 0.0644 | 0.0295
Epoch 188/300, trend Loss: 0.0644 | 0.0295
Epoch 189/300, trend Loss: 0.0644 | 0.0295
Epoch 190/300, trend Loss: 0.0644 | 0.0295
Epoch 191/300, trend Loss: 0.0644 | 0.0295
Epoch 192/300, trend Loss: 0.0644 | 0.0295
Epoch 193/300, trend Loss: 0.0644 | 0.0295
Epoch 194/300, trend Loss: 0.0644 | 0.0295
Epoch 195/300, trend Loss: 0.0644 | 0.0295
Epoch 196/300, trend Loss: 0.0644 | 0.0295
Epoch 197/300, trend Loss: 0.0644 | 0.0295
Epoch 198/300, trend Loss: 0.0644 | 0.0295
Epoch 199/300, trend Loss: 0.0644 | 0.0295
Epoch 200/300, trend Loss: 0.0644 | 0.0295
Epoch 201/300, trend Loss: 0.0644 | 0.0295
Epoch 202/300, trend Loss: 0.0644 | 0.0295
Epoch 203/300, trend Loss: 0.0644 | 0.0295
Epoch 204/300, trend Loss: 0.0644 | 0.0295
Epoch 205/300, trend Loss: 0.0644 | 0.0295
Epoch 206/300, trend Loss: 0.0644 | 0.0295
Epoch 207/300, trend Loss: 0.0644 | 0.0295
Epoch 208/300, trend Loss: 0.0644 | 0.0295
Epoch 209/300, trend Loss: 0.0644 | 0.0295
Epoch 210/300, trend Loss: 0.0644 | 0.0295
Epoch 211/300, trend Loss: 0.0644 | 0.0295
Epoch 212/300, trend Loss: 0.0644 | 0.0295
Epoch 213/300, trend Loss: 0.0644 | 0.0295
Epoch 214/300, trend Loss: 0.0644 | 0.0295
Epoch 215/300, trend Loss: 0.0644 | 0.0295
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 13, 'train_rates': 0.8641213259034833, 'learning_rate': 0.0007199296523659778, 'batch_size': 175, 'step_size': 8, 'gamma': 0.8752623299537569}
Epoch 1/300, seasonal_0 Loss: 2.8080 | 1.6345
Epoch 2/300, seasonal_0 Loss: 0.9112 | 1.7127
Epoch 3/300, seasonal_0 Loss: 0.8892 | 1.6144
Epoch 4/300, seasonal_0 Loss: 1.1761 | 1.6607
Epoch 5/300, seasonal_0 Loss: 1.4173 | 1.8653
Epoch 6/300, seasonal_0 Loss: 1.1087 | 2.2726
Epoch 7/300, seasonal_0 Loss: 0.8849 | 2.0933
Epoch 8/300, seasonal_0 Loss: 0.8999 | 2.1058
Epoch 9/300, seasonal_0 Loss: 0.8946 | 2.1484
Epoch 10/300, seasonal_0 Loss: 0.8769 | 2.1157
Epoch 11/300, seasonal_0 Loss: 0.8870 | 2.1340
Epoch 12/300, seasonal_0 Loss: 0.8819 | 2.1252
Epoch 13/300, seasonal_0 Loss: 0.8755 | 2.1429
Epoch 14/300, seasonal_0 Loss: 0.8700 | 2.1329
Epoch 15/300, seasonal_0 Loss: 0.8732 | 2.1380
Epoch 16/300, seasonal_0 Loss: 0.8718 | 2.1355
Epoch 17/300, seasonal_0 Loss: 0.8655 | 2.1454
Epoch 18/300, seasonal_0 Loss: 0.8626 | 2.1417
Epoch 19/300, seasonal_0 Loss: 0.8639 | 2.1428
Epoch 20/300, seasonal_0 Loss: 0.8636 | 2.1425
Epoch 21/300, seasonal_0 Loss: 0.8579 | 2.1478
Epoch 22/300, seasonal_0 Loss: 0.8565 | 2.1480
Epoch 23/300, seasonal_0 Loss: 0.8568 | 2.1466
Epoch 24/300, seasonal_0 Loss: 0.8571 | 2.1477
Epoch 25/300, seasonal_0 Loss: 0.8519 | 2.1501
Epoch 26/300, seasonal_0 Loss: 0.8515 | 2.1523
Epoch 27/300, seasonal_0 Loss: 0.8512 | 2.1499
Epoch 28/300, seasonal_0 Loss: 0.8518 | 2.1512
Epoch 29/300, seasonal_0 Loss: 0.8473 | 2.1524
Epoch 30/300, seasonal_0 Loss: 0.8473 | 2.1551
Epoch 31/300, seasonal_0 Loss: 0.8469 | 2.1528
Epoch 32/300, seasonal_0 Loss: 0.8474 | 2.1538
Epoch 33/300, seasonal_0 Loss: 0.8437 | 2.1544
Epoch 34/300, seasonal_0 Loss: 0.8438 | 2.1572
Epoch 35/300, seasonal_0 Loss: 0.8433 | 2.1552
Epoch 36/300, seasonal_0 Loss: 0.8438 | 2.1558
Epoch 37/300, seasonal_0 Loss: 0.8406 | 2.1562
Epoch 38/300, seasonal_0 Loss: 0.8408 | 2.1588
Epoch 39/300, seasonal_0 Loss: 0.8404 | 2.1573
Epoch 40/300, seasonal_0 Loss: 0.8407 | 2.1574
Epoch 41/300, seasonal_0 Loss: 0.8381 | 2.1577
Epoch 42/300, seasonal_0 Loss: 0.8383 | 2.1600
Epoch 43/300, seasonal_0 Loss: 0.8380 | 2.1589
Epoch 44/300, seasonal_0 Loss: 0.8382 | 2.1588
Epoch 45/300, seasonal_0 Loss: 0.8360 | 2.1589
Epoch 46/300, seasonal_0 Loss: 0.8362 | 2.1610
Epoch 47/300, seasonal_0 Loss: 0.8359 | 2.1603
Epoch 48/300, seasonal_0 Loss: 0.8360 | 2.1600
Epoch 49/300, seasonal_0 Loss: 0.8342 | 2.1600
Epoch 50/300, seasonal_0 Loss: 0.8343 | 2.1618
Epoch 51/300, seasonal_0 Loss: 0.8341 | 2.1614
Epoch 52/300, seasonal_0 Loss: 0.8342 | 2.1611
Epoch 53/300, seasonal_0 Loss: 0.8326 | 2.1609
Epoch 54/300, seasonal_0 Loss: 0.8328 | 2.1624
Epoch 55/300, seasonal_0 Loss: 0.8326 | 2.1623
Epoch 56/300, seasonal_0 Loss: 0.8327 | 2.1620
Epoch 57/300, seasonal_0 Loss: 0.8313 | 2.1617
Epoch 58/300, seasonal_0 Loss: 0.8314 | 2.1630
Epoch 59/300, seasonal_0 Loss: 0.8313 | 2.1630
Epoch 60/300, seasonal_0 Loss: 0.8313 | 2.1627
Epoch 61/300, seasonal_0 Loss: 0.8301 | 2.1625
Epoch 62/300, seasonal_0 Loss: 0.8302 | 2.1635
Epoch 63/300, seasonal_0 Loss: 0.8302 | 2.1636
Epoch 64/300, seasonal_0 Loss: 0.8302 | 2.1634
Epoch 65/300, seasonal_0 Loss: 0.8291 | 2.1631
Epoch 66/300, seasonal_0 Loss: 0.8292 | 2.1639
Epoch 67/300, seasonal_0 Loss: 0.8292 | 2.1641
Epoch 68/300, seasonal_0 Loss: 0.8292 | 2.1640
Epoch 69/300, seasonal_0 Loss: 0.8282 | 2.1637
Epoch 70/300, seasonal_0 Loss: 0.8283 | 2.1643
Epoch 71/300, seasonal_0 Loss: 0.8283 | 2.1645
Epoch 72/300, seasonal_0 Loss: 0.8283 | 2.1644
Epoch 73/300, seasonal_0 Loss: 0.8274 | 2.1642
Epoch 74/300, seasonal_0 Loss: 0.8275 | 2.1646
Epoch 75/300, seasonal_0 Loss: 0.8275 | 2.1649
Epoch 76/300, seasonal_0 Loss: 0.8275 | 2.1648
Epoch 77/300, seasonal_0 Loss: 0.8268 | 2.1646
Epoch 78/300, seasonal_0 Loss: 0.8268 | 2.1650
Epoch 79/300, seasonal_0 Loss: 0.8268 | 2.1652
Epoch 80/300, seasonal_0 Loss: 0.8268 | 2.1652
Epoch 81/300, seasonal_0 Loss: 0.8262 | 2.1650
Epoch 82/300, seasonal_0 Loss: 0.8262 | 2.1653
Epoch 83/300, seasonal_0 Loss: 0.8262 | 2.1654
Epoch 84/300, seasonal_0 Loss: 0.8262 | 2.1655
Epoch 85/300, seasonal_0 Loss: 0.8256 | 2.1653
Epoch 86/300, seasonal_0 Loss: 0.8257 | 2.1655
Epoch 87/300, seasonal_0 Loss: 0.8257 | 2.1657
Epoch 88/300, seasonal_0 Loss: 0.8257 | 2.1657
Epoch 89/300, seasonal_0 Loss: 0.8252 | 2.1656
Epoch 90/300, seasonal_0 Loss: 0.8252 | 2.1658
Epoch 91/300, seasonal_0 Loss: 0.8252 | 2.1659
Epoch 92/300, seasonal_0 Loss: 0.8252 | 2.1659
Epoch 93/300, seasonal_0 Loss: 0.8248 | 2.1659
Epoch 94/300, seasonal_0 Loss: 0.8248 | 2.1660
Epoch 95/300, seasonal_0 Loss: 0.8248 | 2.1661
Epoch 96/300, seasonal_0 Loss: 0.8248 | 2.1661
Epoch 97/300, seasonal_0 Loss: 0.8244 | 2.1661
Epoch 98/300, seasonal_0 Loss: 0.8245 | 2.1662
Epoch 99/300, seasonal_0 Loss: 0.8245 | 2.1662
Epoch 100/300, seasonal_0 Loss: 0.8245 | 2.1663
Epoch 101/300, seasonal_0 Loss: 0.8241 | 2.1663
Epoch 102/300, seasonal_0 Loss: 0.8241 | 2.1663
Epoch 103/300, seasonal_0 Loss: 0.8241 | 2.1664
Epoch 104/300, seasonal_0 Loss: 0.8241 | 2.1664
Epoch 105/300, seasonal_0 Loss: 0.8238 | 2.1664
Epoch 106/300, seasonal_0 Loss: 0.8239 | 2.1665
Epoch 107/300, seasonal_0 Loss: 0.8239 | 2.1665
Epoch 108/300, seasonal_0 Loss: 0.8239 | 2.1666
Epoch 109/300, seasonal_0 Loss: 0.8236 | 2.1665
Epoch 110/300, seasonal_0 Loss: 0.8236 | 2.1666
Epoch 111/300, seasonal_0 Loss: 0.8236 | 2.1666
Epoch 112/300, seasonal_0 Loss: 0.8236 | 2.1667
Epoch 113/300, seasonal_0 Loss: 0.8234 | 2.1667
Epoch 114/300, seasonal_0 Loss: 0.8234 | 2.1667
Epoch 115/300, seasonal_0 Loss: 0.8234 | 2.1668
Epoch 116/300, seasonal_0 Loss: 0.8234 | 2.1668
Epoch 117/300, seasonal_0 Loss: 0.8232 | 2.1668
Epoch 118/300, seasonal_0 Loss: 0.8232 | 2.1668
Epoch 119/300, seasonal_0 Loss: 0.8232 | 2.1668
Epoch 120/300, seasonal_0 Loss: 0.8232 | 2.1669
Epoch 121/300, seasonal_0 Loss: 0.8230 | 2.1669
Epoch 122/300, seasonal_0 Loss: 0.8230 | 2.1669
Epoch 123/300, seasonal_0 Loss: 0.8230 | 2.1669
Epoch 124/300, seasonal_0 Loss: 0.8230 | 2.1669
Epoch 125/300, seasonal_0 Loss: 0.8229 | 2.1669
Epoch 126/300, seasonal_0 Loss: 0.8229 | 2.1670
Epoch 127/300, seasonal_0 Loss: 0.8229 | 2.1670
Epoch 128/300, seasonal_0 Loss: 0.8229 | 2.1670
Epoch 129/300, seasonal_0 Loss: 0.8227 | 2.1670
Epoch 130/300, seasonal_0 Loss: 0.8227 | 2.1670
Epoch 131/300, seasonal_0 Loss: 0.8227 | 2.1671
Epoch 132/300, seasonal_0 Loss: 0.8227 | 2.1671
Epoch 133/300, seasonal_0 Loss: 0.8226 | 2.1671
Epoch 134/300, seasonal_0 Loss: 0.8226 | 2.1671
Epoch 135/300, seasonal_0 Loss: 0.8226 | 2.1671
Epoch 136/300, seasonal_0 Loss: 0.8226 | 2.1671
Epoch 137/300, seasonal_0 Loss: 0.8225 | 2.1671
Epoch 138/300, seasonal_0 Loss: 0.8225 | 2.1672
Epoch 139/300, seasonal_0 Loss: 0.8225 | 2.1672
Epoch 140/300, seasonal_0 Loss: 0.8225 | 2.1672
Epoch 141/300, seasonal_0 Loss: 0.8224 | 2.1672
Epoch 142/300, seasonal_0 Loss: 0.8224 | 2.1672
Epoch 143/300, seasonal_0 Loss: 0.8224 | 2.1672
Epoch 144/300, seasonal_0 Loss: 0.8224 | 2.1672
Epoch 145/300, seasonal_0 Loss: 0.8223 | 2.1672
Epoch 146/300, seasonal_0 Loss: 0.8223 | 2.1672
Epoch 147/300, seasonal_0 Loss: 0.8223 | 2.1673
Epoch 148/300, seasonal_0 Loss: 0.8223 | 2.1673
Epoch 149/300, seasonal_0 Loss: 0.8223 | 2.1673
Epoch 150/300, seasonal_0 Loss: 0.8223 | 2.1673
Epoch 151/300, seasonal_0 Loss: 0.8223 | 2.1673
Epoch 152/300, seasonal_0 Loss: 0.8223 | 2.1673
Epoch 153/300, seasonal_0 Loss: 0.8222 | 2.1673
Epoch 154/300, seasonal_0 Loss: 0.8222 | 2.1673
Epoch 155/300, seasonal_0 Loss: 0.8222 | 2.1673
Epoch 156/300, seasonal_0 Loss: 0.8222 | 2.1673
Epoch 157/300, seasonal_0 Loss: 0.8221 | 2.1673
Epoch 158/300, seasonal_0 Loss: 0.8221 | 2.1673
Epoch 159/300, seasonal_0 Loss: 0.8221 | 2.1674
Epoch 160/300, seasonal_0 Loss: 0.8221 | 2.1674
Epoch 161/300, seasonal_0 Loss: 0.8221 | 2.1674
Epoch 162/300, seasonal_0 Loss: 0.8221 | 2.1674
Epoch 163/300, seasonal_0 Loss: 0.8221 | 2.1674
Epoch 164/300, seasonal_0 Loss: 0.8221 | 2.1674
Epoch 165/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 166/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 167/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 168/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 169/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 170/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 171/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 172/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 173/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 174/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 175/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 176/300, seasonal_0 Loss: 0.8220 | 2.1674
Epoch 177/300, seasonal_0 Loss: 0.8219 | 2.1674
Epoch 178/300, seasonal_0 Loss: 0.8219 | 2.1674
Epoch 179/300, seasonal_0 Loss: 0.8219 | 2.1674
Epoch 180/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 181/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 182/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 183/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 184/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 185/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 186/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 187/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 188/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 189/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 190/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 191/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 192/300, seasonal_0 Loss: 0.8219 | 2.1675
Epoch 193/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 194/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 195/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 196/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 197/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 198/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 199/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 200/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 201/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 202/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 203/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 204/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 205/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 206/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 207/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 208/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 209/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 210/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 211/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 212/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 213/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 214/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 215/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 216/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 217/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 218/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 219/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 220/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 221/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 222/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 223/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 224/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 225/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 226/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 227/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 228/300, seasonal_0 Loss: 0.8218 | 2.1675
Epoch 229/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 230/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 231/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 232/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 233/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 234/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 235/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 236/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 237/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 238/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 239/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 240/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 241/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 242/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 243/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 244/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 245/300, seasonal_0 Loss: 0.8217 | 2.1675
Epoch 246/300, seasonal_0 Loss: 0.8217 | 2.1675
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.8561588149194292, 'learning_rate': 0.000486406153888435, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9115661863487297}
Epoch 1/300, seasonal_1 Loss: 0.5754 | 0.1066
Epoch 2/300, seasonal_1 Loss: 0.1286 | 0.1058
Epoch 3/300, seasonal_1 Loss: 0.1160 | 0.1086
Epoch 4/300, seasonal_1 Loss: 0.1103 | 0.0861
Epoch 5/300, seasonal_1 Loss: 0.1103 | 0.0814
Epoch 6/300, seasonal_1 Loss: 0.1064 | 0.0941
Epoch 7/300, seasonal_1 Loss: 0.0893 | 0.0616
Epoch 8/300, seasonal_1 Loss: 0.0914 | 0.0956
Epoch 9/300, seasonal_1 Loss: 0.0928 | 0.0850
Epoch 10/300, seasonal_1 Loss: 0.0853 | 0.0656
Epoch 11/300, seasonal_1 Loss: 0.0857 | 0.0770
Epoch 12/300, seasonal_1 Loss: 0.0818 | 0.0652
Epoch 13/300, seasonal_1 Loss: 0.0800 | 0.0678
Epoch 14/300, seasonal_1 Loss: 0.0794 | 0.0810
Epoch 15/300, seasonal_1 Loss: 0.0747 | 0.0696
Epoch 16/300, seasonal_1 Loss: 0.0771 | 0.0803
Epoch 17/300, seasonal_1 Loss: 0.0721 | 0.0567
Epoch 18/300, seasonal_1 Loss: 0.0749 | 0.0593
Epoch 19/300, seasonal_1 Loss: 0.0680 | 0.0431
Epoch 20/300, seasonal_1 Loss: 0.0689 | 0.0593
Epoch 21/300, seasonal_1 Loss: 0.0658 | 0.0472
Epoch 22/300, seasonal_1 Loss: 0.0659 | 0.0391
Epoch 23/300, seasonal_1 Loss: 0.0608 | 0.0363
Epoch 24/300, seasonal_1 Loss: 0.0599 | 0.0312
Epoch 25/300, seasonal_1 Loss: 0.0600 | 0.0369
Epoch 26/300, seasonal_1 Loss: 0.0591 | 0.0482
Epoch 27/300, seasonal_1 Loss: 0.0580 | 0.0350
Epoch 28/300, seasonal_1 Loss: 0.0563 | 0.0393
Epoch 29/300, seasonal_1 Loss: 0.0556 | 0.0365
Epoch 30/300, seasonal_1 Loss: 0.0545 | 0.0381
Epoch 31/300, seasonal_1 Loss: 0.0538 | 0.0351
Epoch 32/300, seasonal_1 Loss: 0.0533 | 0.0334
Epoch 33/300, seasonal_1 Loss: 0.0530 | 0.0334
Epoch 34/300, seasonal_1 Loss: 0.0528 | 0.0339
Epoch 35/300, seasonal_1 Loss: 0.0531 | 0.0353
Epoch 36/300, seasonal_1 Loss: 0.0515 | 0.0326
Epoch 37/300, seasonal_1 Loss: 0.0513 | 0.0372
Epoch 38/300, seasonal_1 Loss: 0.0499 | 0.0360
Epoch 39/300, seasonal_1 Loss: 0.0609 | 0.0534
Epoch 40/300, seasonal_1 Loss: 0.0545 | 0.0407
Epoch 41/300, seasonal_1 Loss: 0.0539 | 0.0661
Epoch 42/300, seasonal_1 Loss: 0.0501 | 0.0404
Epoch 43/300, seasonal_1 Loss: 0.0482 | 0.0377
Epoch 44/300, seasonal_1 Loss: 0.0469 | 0.0376
Epoch 45/300, seasonal_1 Loss: 0.0467 | 0.0374
Epoch 46/300, seasonal_1 Loss: 0.0496 | 0.0365
Epoch 47/300, seasonal_1 Loss: 0.0489 | 0.0459
Epoch 48/300, seasonal_1 Loss: 0.0490 | 0.0341
Epoch 49/300, seasonal_1 Loss: 0.0464 | 0.0384
Epoch 50/300, seasonal_1 Loss: 0.0444 | 0.0484
Epoch 51/300, seasonal_1 Loss: 0.0437 | 0.0483
Epoch 52/300, seasonal_1 Loss: 0.0485 | 0.0474
Epoch 53/300, seasonal_1 Loss: 0.0475 | 0.0509
Epoch 54/300, seasonal_1 Loss: 0.0469 | 0.0595
Epoch 55/300, seasonal_1 Loss: 0.0456 | 0.0631
Epoch 56/300, seasonal_1 Loss: 0.0420 | 0.0655
Epoch 57/300, seasonal_1 Loss: 0.0466 | 0.0507
Epoch 58/300, seasonal_1 Loss: 0.0478 | 0.0441
Epoch 59/300, seasonal_1 Loss: 0.0435 | 0.0579
Epoch 60/300, seasonal_1 Loss: 0.0429 | 0.0556
Epoch 61/300, seasonal_1 Loss: 0.0399 | 0.0601
Epoch 62/300, seasonal_1 Loss: 0.0398 | 0.0649
Epoch 63/300, seasonal_1 Loss: 0.0422 | 0.0690
Epoch 64/300, seasonal_1 Loss: 0.0416 | 0.0736
Epoch 65/300, seasonal_1 Loss: 0.0423 | 0.0672
Epoch 66/300, seasonal_1 Loss: 0.0414 | 0.0638
Epoch 67/300, seasonal_1 Loss: 0.0411 | 0.0534
Epoch 68/300, seasonal_1 Loss: 0.0397 | 0.0508
Epoch 69/300, seasonal_1 Loss: 0.0390 | 0.0457
Epoch 70/300, seasonal_1 Loss: 0.0374 | 0.0470
Epoch 71/300, seasonal_1 Loss: 0.0363 | 0.0469
Epoch 72/300, seasonal_1 Loss: 0.0372 | 0.0545
Epoch 73/300, seasonal_1 Loss: 0.0392 | 0.0545
Epoch 74/300, seasonal_1 Loss: 0.0422 | 0.0543
Epoch 75/300, seasonal_1 Loss: 0.0410 | 0.0421
Epoch 76/300, seasonal_1 Loss: 0.0379 | 0.0424
Epoch 77/300, seasonal_1 Loss: 0.0367 | 0.0400
Epoch 78/300, seasonal_1 Loss: 0.0358 | 0.0373
Epoch 79/300, seasonal_1 Loss: 0.0354 | 0.0387
Epoch 80/300, seasonal_1 Loss: 0.0353 | 0.0384
Epoch 81/300, seasonal_1 Loss: 0.0354 | 0.0470
Epoch 82/300, seasonal_1 Loss: 0.0367 | 0.0595
Epoch 83/300, seasonal_1 Loss: 0.0373 | 0.1140
Epoch 84/300, seasonal_1 Loss: 0.0380 | 0.0786
Epoch 85/300, seasonal_1 Loss: 0.0345 | 0.0582
Epoch 86/300, seasonal_1 Loss: 0.0335 | 0.0590
Epoch 87/300, seasonal_1 Loss: 0.0318 | 0.0646
Epoch 88/300, seasonal_1 Loss: 0.0327 | 0.0637
Epoch 89/300, seasonal_1 Loss: 0.0301 | 0.0629
Epoch 90/300, seasonal_1 Loss: 0.0377 | 0.0704
Epoch 91/300, seasonal_1 Loss: 0.0308 | 0.0706
Epoch 92/300, seasonal_1 Loss: 0.0310 | 0.0671
Epoch 93/300, seasonal_1 Loss: 0.0322 | 0.0713
Epoch 94/300, seasonal_1 Loss: 0.0308 | 0.0685
Epoch 95/300, seasonal_1 Loss: 0.0302 | 0.0668
Epoch 96/300, seasonal_1 Loss: 0.0296 | 0.0652
Epoch 97/300, seasonal_1 Loss: 0.0287 | 0.0643
Epoch 98/300, seasonal_1 Loss: 0.0283 | 0.0643
Epoch 99/300, seasonal_1 Loss: 0.0282 | 0.0629
Epoch 100/300, seasonal_1 Loss: 0.0281 | 0.0635
Epoch 101/300, seasonal_1 Loss: 0.0286 | 0.0616
Epoch 102/300, seasonal_1 Loss: 0.0278 | 0.0615
Epoch 103/300, seasonal_1 Loss: 0.0277 | 0.0608
Epoch 104/300, seasonal_1 Loss: 0.0276 | 0.0603
Epoch 105/300, seasonal_1 Loss: 0.0275 | 0.0597
Epoch 106/300, seasonal_1 Loss: 0.0274 | 0.0593
Epoch 107/300, seasonal_1 Loss: 0.0273 | 0.0588
Epoch 108/300, seasonal_1 Loss: 0.0273 | 0.0585
Epoch 109/300, seasonal_1 Loss: 0.0272 | 0.0582
Epoch 110/300, seasonal_1 Loss: 0.0271 | 0.0579
Epoch 111/300, seasonal_1 Loss: 0.0271 | 0.0578
Epoch 112/300, seasonal_1 Loss: 0.0270 | 0.0576
Epoch 113/300, seasonal_1 Loss: 0.0269 | 0.0575
Epoch 114/300, seasonal_1 Loss: 0.0269 | 0.0574
Epoch 115/300, seasonal_1 Loss: 0.0268 | 0.0573
Epoch 116/300, seasonal_1 Loss: 0.0268 | 0.0573
Epoch 117/300, seasonal_1 Loss: 0.0267 | 0.0572
Epoch 118/300, seasonal_1 Loss: 0.0267 | 0.0571
Epoch 119/300, seasonal_1 Loss: 0.0266 | 0.0571
Epoch 120/300, seasonal_1 Loss: 0.0266 | 0.0570
Epoch 121/300, seasonal_1 Loss: 0.0265 | 0.0570
Epoch 122/300, seasonal_1 Loss: 0.0265 | 0.0569
Epoch 123/300, seasonal_1 Loss: 0.0265 | 0.0569
Epoch 124/300, seasonal_1 Loss: 0.0264 | 0.0568
Epoch 125/300, seasonal_1 Loss: 0.0264 | 0.0568
Epoch 126/300, seasonal_1 Loss: 0.0264 | 0.0567
Epoch 127/300, seasonal_1 Loss: 0.0263 | 0.0567
Epoch 128/300, seasonal_1 Loss: 0.0263 | 0.0566
Epoch 129/300, seasonal_1 Loss: 0.0262 | 0.0566
Epoch 130/300, seasonal_1 Loss: 0.0262 | 0.0565
Epoch 131/300, seasonal_1 Loss: 0.0262 | 0.0565
Epoch 132/300, seasonal_1 Loss: 0.0262 | 0.0565
Epoch 133/300, seasonal_1 Loss: 0.0261 | 0.0564
Epoch 134/300, seasonal_1 Loss: 0.0261 | 0.0564
Epoch 135/300, seasonal_1 Loss: 0.0261 | 0.0563
Epoch 136/300, seasonal_1 Loss: 0.0260 | 0.0563
Epoch 137/300, seasonal_1 Loss: 0.0260 | 0.0563
Epoch 138/300, seasonal_1 Loss: 0.0260 | 0.0563
Epoch 139/300, seasonal_1 Loss: 0.0260 | 0.0562
Epoch 140/300, seasonal_1 Loss: 0.0259 | 0.0563
Epoch 141/300, seasonal_1 Loss: 0.0259 | 0.0561
Epoch 142/300, seasonal_1 Loss: 0.0259 | 0.0563
Epoch 143/300, seasonal_1 Loss: 0.0259 | 0.0561
Epoch 144/300, seasonal_1 Loss: 0.0259 | 0.0560
Epoch 145/300, seasonal_1 Loss: 0.0258 | 0.0561
Epoch 146/300, seasonal_1 Loss: 0.0258 | 0.0560
Epoch 147/300, seasonal_1 Loss: 0.0258 | 0.0561
Epoch 148/300, seasonal_1 Loss: 0.0258 | 0.0560
Epoch 149/300, seasonal_1 Loss: 0.0258 | 0.0560
Epoch 150/300, seasonal_1 Loss: 0.0258 | 0.0559
Epoch 151/300, seasonal_1 Loss: 0.0257 | 0.0560
Epoch 152/300, seasonal_1 Loss: 0.0257 | 0.0559
Epoch 153/300, seasonal_1 Loss: 0.0257 | 0.0559
Epoch 154/300, seasonal_1 Loss: 0.0257 | 0.0559
Epoch 155/300, seasonal_1 Loss: 0.0257 | 0.0559
Epoch 156/300, seasonal_1 Loss: 0.0257 | 0.0559
Epoch 157/300, seasonal_1 Loss: 0.0256 | 0.0559
Epoch 158/300, seasonal_1 Loss: 0.0256 | 0.0559
Epoch 159/300, seasonal_1 Loss: 0.0256 | 0.0558
Epoch 160/300, seasonal_1 Loss: 0.0256 | 0.0558
Epoch 161/300, seasonal_1 Loss: 0.0256 | 0.0558
Epoch 162/300, seasonal_1 Loss: 0.0256 | 0.0558
Epoch 163/300, seasonal_1 Loss: 0.0256 | 0.0558
Epoch 164/300, seasonal_1 Loss: 0.0256 | 0.0558
Epoch 165/300, seasonal_1 Loss: 0.0255 | 0.0558
Epoch 166/300, seasonal_1 Loss: 0.0255 | 0.0558
Epoch 167/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 168/300, seasonal_1 Loss: 0.0255 | 0.0558
Epoch 169/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 170/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 171/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 172/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 173/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 174/300, seasonal_1 Loss: 0.0255 | 0.0557
Epoch 175/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 176/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 177/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 178/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 179/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 180/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 181/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 182/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 183/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 184/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 185/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 186/300, seasonal_1 Loss: 0.0254 | 0.0556
Epoch 187/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 188/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 189/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 190/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 191/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 192/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 193/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 194/300, seasonal_1 Loss: 0.0254 | 0.0555
Epoch 195/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 196/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 197/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 198/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 199/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 200/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 201/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 202/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 203/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 204/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 205/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 206/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 207/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 208/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 209/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 210/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 211/300, seasonal_1 Loss: 0.0253 | 0.0555
Epoch 212/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 213/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 214/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 215/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 216/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 217/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 218/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 219/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 220/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 221/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 222/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 223/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 224/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 225/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 226/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 227/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 228/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 229/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 230/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 231/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 232/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 233/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 234/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 235/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 236/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 237/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 238/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 239/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 240/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 241/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 242/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 243/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 244/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 245/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 246/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 247/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 248/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 249/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 250/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 251/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 252/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 253/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 254/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 255/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 256/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 257/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 258/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 259/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 260/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 261/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 262/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 263/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 264/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 265/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 266/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 267/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 268/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 269/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 270/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 271/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 272/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 273/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 274/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 275/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 276/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 277/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 278/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 279/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 280/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 281/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 282/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 283/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 284/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 285/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 286/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 287/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 288/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 289/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 290/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 291/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 292/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 293/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 294/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 295/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 296/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 297/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 298/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 299/300, seasonal_1 Loss: 0.0253 | 0.0554
Epoch 300/300, seasonal_1 Loss: 0.0253 | 0.0554
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.8216310481852048, 'learning_rate': 0.00022809492655890073, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9880828234720889}
Epoch 1/300, seasonal_2 Loss: 0.2758 | 0.2018
Epoch 2/300, seasonal_2 Loss: 0.1113 | 0.0726
Epoch 3/300, seasonal_2 Loss: 0.1080 | 0.0922
Epoch 4/300, seasonal_2 Loss: 0.1059 | 0.0847
Epoch 5/300, seasonal_2 Loss: 0.1040 | 0.0775
Epoch 6/300, seasonal_2 Loss: 0.1002 | 0.0567
Epoch 7/300, seasonal_2 Loss: 0.0980 | 0.0765
Epoch 8/300, seasonal_2 Loss: 0.1001 | 0.0741
Epoch 9/300, seasonal_2 Loss: 0.0981 | 0.0888
Epoch 10/300, seasonal_2 Loss: 0.0936 | 0.0973
Epoch 11/300, seasonal_2 Loss: 0.0870 | 0.0628
Epoch 12/300, seasonal_2 Loss: 0.0857 | 0.0771
Epoch 13/300, seasonal_2 Loss: 0.0832 | 0.0905
Epoch 14/300, seasonal_2 Loss: 0.0854 | 0.1045
Epoch 15/300, seasonal_2 Loss: 0.0811 | 0.0535
Epoch 16/300, seasonal_2 Loss: 0.0785 | 0.0723
Epoch 17/300, seasonal_2 Loss: 0.0760 | 0.0626
Epoch 18/300, seasonal_2 Loss: 0.0765 | 0.0807
Epoch 19/300, seasonal_2 Loss: 0.0801 | 0.0940
Epoch 20/300, seasonal_2 Loss: 0.0814 | 0.0545
Epoch 21/300, seasonal_2 Loss: 0.0779 | 0.0433
Epoch 22/300, seasonal_2 Loss: 0.0720 | 0.0402
Epoch 23/300, seasonal_2 Loss: 0.0699 | 0.0390
Epoch 24/300, seasonal_2 Loss: 0.0706 | 0.0383
Epoch 25/300, seasonal_2 Loss: 0.0720 | 0.0527
Epoch 26/300, seasonal_2 Loss: 0.0687 | 0.0556
Epoch 27/300, seasonal_2 Loss: 0.0678 | 0.0536
Epoch 28/300, seasonal_2 Loss: 0.0705 | 0.0566
Epoch 29/300, seasonal_2 Loss: 0.0693 | 0.0547
Epoch 30/300, seasonal_2 Loss: 0.0643 | 0.0354
Epoch 31/300, seasonal_2 Loss: 0.0680 | 0.0589
Epoch 32/300, seasonal_2 Loss: 0.0640 | 0.0497
Epoch 33/300, seasonal_2 Loss: 0.0615 | 0.0369
Epoch 34/300, seasonal_2 Loss: 0.0598 | 0.0330
Epoch 35/300, seasonal_2 Loss: 0.0581 | 0.0321
Epoch 36/300, seasonal_2 Loss: 0.0590 | 0.0328
Epoch 37/300, seasonal_2 Loss: 0.0577 | 0.0324
Epoch 38/300, seasonal_2 Loss: 0.0548 | 0.0346
Epoch 39/300, seasonal_2 Loss: 0.0549 | 0.0287
Epoch 40/300, seasonal_2 Loss: 0.0571 | 0.0276
Epoch 41/300, seasonal_2 Loss: 0.0552 | 0.0289
Epoch 42/300, seasonal_2 Loss: 0.0544 | 0.0298
Epoch 43/300, seasonal_2 Loss: 0.0519 | 0.0306
Epoch 44/300, seasonal_2 Loss: 0.0520 | 0.0341
Epoch 45/300, seasonal_2 Loss: 0.0518 | 0.0314
Epoch 46/300, seasonal_2 Loss: 0.0492 | 0.0327
Epoch 47/300, seasonal_2 Loss: 0.0526 | 0.0311
Epoch 48/300, seasonal_2 Loss: 0.0516 | 0.0281
Epoch 49/300, seasonal_2 Loss: 0.0479 | 0.0269
Epoch 50/300, seasonal_2 Loss: 0.0457 | 0.0298
Epoch 51/300, seasonal_2 Loss: 0.0470 | 0.0313
Epoch 52/300, seasonal_2 Loss: 0.0437 | 0.0444
Epoch 53/300, seasonal_2 Loss: 0.0503 | 0.0394
Epoch 54/300, seasonal_2 Loss: 0.0509 | 0.0304
Epoch 55/300, seasonal_2 Loss: 0.0488 | 0.0283
Epoch 56/300, seasonal_2 Loss: 0.0524 | 0.0327
Epoch 57/300, seasonal_2 Loss: 0.0445 | 0.0360
Epoch 58/300, seasonal_2 Loss: 0.0418 | 0.0442
Epoch 59/300, seasonal_2 Loss: 0.0461 | 0.0403
Epoch 60/300, seasonal_2 Loss: 0.0471 | 0.0342
Epoch 61/300, seasonal_2 Loss: 0.0457 | 0.0362
Epoch 62/300, seasonal_2 Loss: 0.0429 | 0.0313
Epoch 63/300, seasonal_2 Loss: 0.0446 | 0.0304
Epoch 64/300, seasonal_2 Loss: 0.0411 | 0.0338
Epoch 65/300, seasonal_2 Loss: 0.0395 | 0.0313
Epoch 66/300, seasonal_2 Loss: 0.0422 | 0.0313
Epoch 67/300, seasonal_2 Loss: 0.0431 | 0.0290
Epoch 68/300, seasonal_2 Loss: 0.0411 | 0.0302
Epoch 69/300, seasonal_2 Loss: 0.0384 | 0.0291
Epoch 70/300, seasonal_2 Loss: 0.0370 | 0.0338
Epoch 71/300, seasonal_2 Loss: 0.0424 | 0.0331
Epoch 72/300, seasonal_2 Loss: 0.0384 | 0.0330
Epoch 73/300, seasonal_2 Loss: 0.0349 | 0.0334
Epoch 74/300, seasonal_2 Loss: 0.0372 | 0.0328
Epoch 75/300, seasonal_2 Loss: 0.0444 | 0.0321
Epoch 76/300, seasonal_2 Loss: 0.0442 | 0.0296
Epoch 77/300, seasonal_2 Loss: 0.0427 | 0.0312
Epoch 78/300, seasonal_2 Loss: 0.0483 | 0.0288
Epoch 79/300, seasonal_2 Loss: 0.0379 | 0.0323
Epoch 80/300, seasonal_2 Loss: 0.0371 | 0.0278
Epoch 81/300, seasonal_2 Loss: 0.0434 | 0.0310
Epoch 82/300, seasonal_2 Loss: 0.0400 | 0.0325
Epoch 83/300, seasonal_2 Loss: 0.0374 | 0.0311
Epoch 84/300, seasonal_2 Loss: 0.0332 | 0.0320
Epoch 85/300, seasonal_2 Loss: 0.0318 | 0.0336
Epoch 86/300, seasonal_2 Loss: 0.0311 | 0.0312
Epoch 87/300, seasonal_2 Loss: 0.0303 | 0.0314
Epoch 88/300, seasonal_2 Loss: 0.0315 | 0.0300
Epoch 89/300, seasonal_2 Loss: 0.0311 | 0.0312
Epoch 90/300, seasonal_2 Loss: 0.0305 | 0.0380
Epoch 91/300, seasonal_2 Loss: 0.0293 | 0.0308
Epoch 92/300, seasonal_2 Loss: 0.0338 | 0.0294
Epoch 93/300, seasonal_2 Loss: 0.0323 | 0.0293
Epoch 94/300, seasonal_2 Loss: 0.0377 | 0.0313
Epoch 95/300, seasonal_2 Loss: 0.0338 | 0.0318
Epoch 96/300, seasonal_2 Loss: 0.0312 | 0.0364
Epoch 97/300, seasonal_2 Loss: 0.0292 | 0.0353
Epoch 98/300, seasonal_2 Loss: 0.0281 | 0.0355
Epoch 99/300, seasonal_2 Loss: 0.0283 | 0.0315
Epoch 100/300, seasonal_2 Loss: 0.0335 | 0.0388
Epoch 101/300, seasonal_2 Loss: 0.0325 | 0.0357
Epoch 102/300, seasonal_2 Loss: 0.0361 | 0.0304
Epoch 103/300, seasonal_2 Loss: 0.0298 | 0.0318
Epoch 104/300, seasonal_2 Loss: 0.0297 | 0.0338
Epoch 105/300, seasonal_2 Loss: 0.0285 | 0.0343
Epoch 106/300, seasonal_2 Loss: 0.0285 | 0.0354
Epoch 107/300, seasonal_2 Loss: 0.0275 | 0.0380
Epoch 108/300, seasonal_2 Loss: 0.0284 | 0.0358
Epoch 109/300, seasonal_2 Loss: 0.0265 | 0.0348
Epoch 110/300, seasonal_2 Loss: 0.0348 | 0.0357
Epoch 111/300, seasonal_2 Loss: 0.0296 | 0.0399
Epoch 112/300, seasonal_2 Loss: 0.0282 | 0.0331
Epoch 113/300, seasonal_2 Loss: 0.0275 | 0.0348
Epoch 114/300, seasonal_2 Loss: 0.0267 | 0.0350
Epoch 115/300, seasonal_2 Loss: 0.0263 | 0.0357
Epoch 116/300, seasonal_2 Loss: 0.0259 | 0.0369
Epoch 117/300, seasonal_2 Loss: 0.0255 | 0.0382
Epoch 118/300, seasonal_2 Loss: 0.0248 | 0.0459
Epoch 119/300, seasonal_2 Loss: 0.0255 | 0.0409
Epoch 120/300, seasonal_2 Loss: 0.0244 | 0.0371
Epoch 121/300, seasonal_2 Loss: 0.0294 | 0.0361
Epoch 122/300, seasonal_2 Loss: 0.0312 | 0.0372
Epoch 123/300, seasonal_2 Loss: 0.0261 | 0.0370
Epoch 124/300, seasonal_2 Loss: 0.0252 | 0.0383
Epoch 125/300, seasonal_2 Loss: 0.0251 | 0.0370
Epoch 126/300, seasonal_2 Loss: 0.0251 | 0.0381
Epoch 127/300, seasonal_2 Loss: 0.0251 | 0.0470
Epoch 128/300, seasonal_2 Loss: 0.0275 | 0.0410
Epoch 129/300, seasonal_2 Loss: 0.0266 | 0.0401
Epoch 130/300, seasonal_2 Loss: 0.0262 | 0.0392
Epoch 131/300, seasonal_2 Loss: 0.0262 | 0.0375
Epoch 132/300, seasonal_2 Loss: 0.0296 | 0.0535
Epoch 133/300, seasonal_2 Loss: 0.0260 | 0.0463
Epoch 134/300, seasonal_2 Loss: 0.0258 | 0.0447
Epoch 135/300, seasonal_2 Loss: 0.0260 | 0.0425
Epoch 136/300, seasonal_2 Loss: 0.0256 | 0.0415
Epoch 137/300, seasonal_2 Loss: 0.0254 | 0.0400
Epoch 138/300, seasonal_2 Loss: 0.0252 | 0.0383
Epoch 139/300, seasonal_2 Loss: 0.0251 | 0.0339
Epoch 140/300, seasonal_2 Loss: 0.0245 | 0.0321
Epoch 141/300, seasonal_2 Loss: 0.0237 | 0.0497
Epoch 142/300, seasonal_2 Loss: 0.0256 | 0.0424
Epoch 143/300, seasonal_2 Loss: 0.0254 | 0.0410
Epoch 144/300, seasonal_2 Loss: 0.0253 | 0.0403
Epoch 145/300, seasonal_2 Loss: 0.0254 | 0.0368
Epoch 146/300, seasonal_2 Loss: 0.0254 | 0.0356
Epoch 147/300, seasonal_2 Loss: 0.0246 | 0.0347
Epoch 148/300, seasonal_2 Loss: 0.0244 | 0.0341
Epoch 149/300, seasonal_2 Loss: 0.0288 | 0.0447
Epoch 150/300, seasonal_2 Loss: 0.0261 | 0.0578
Epoch 151/300, seasonal_2 Loss: 0.0254 | 0.0421
Epoch 152/300, seasonal_2 Loss: 0.0252 | 0.0436
Epoch 153/300, seasonal_2 Loss: 0.0250 | 0.0434
Epoch 154/300, seasonal_2 Loss: 0.0249 | 0.0423
Epoch 155/300, seasonal_2 Loss: 0.0248 | 0.0408
Epoch 156/300, seasonal_2 Loss: 0.0247 | 0.0389
Epoch 157/300, seasonal_2 Loss: 0.0246 | 0.0372
Epoch 158/300, seasonal_2 Loss: 0.0242 | 0.0354
Epoch 159/300, seasonal_2 Loss: 0.0239 | 0.0355
Epoch 160/300, seasonal_2 Loss: 0.0237 | 0.0353
Epoch 161/300, seasonal_2 Loss: 0.0251 | 0.0459
Epoch 162/300, seasonal_2 Loss: 0.0239 | 0.0410
Epoch 163/300, seasonal_2 Loss: 0.0237 | 0.0408
Epoch 164/300, seasonal_2 Loss: 0.0237 | 0.0402
Epoch 165/300, seasonal_2 Loss: 0.0236 | 0.0400
Epoch 166/300, seasonal_2 Loss: 0.0236 | 0.0398
Epoch 167/300, seasonal_2 Loss: 0.0236 | 0.0397
Epoch 168/300, seasonal_2 Loss: 0.0235 | 0.0395
Epoch 169/300, seasonal_2 Loss: 0.0235 | 0.0395
Epoch 170/300, seasonal_2 Loss: 0.0235 | 0.0393
Epoch 171/300, seasonal_2 Loss: 0.0235 | 0.0393
Epoch 172/300, seasonal_2 Loss: 0.0234 | 0.0391
Epoch 173/300, seasonal_2 Loss: 0.0234 | 0.0391
Epoch 174/300, seasonal_2 Loss: 0.0234 | 0.0390
Epoch 175/300, seasonal_2 Loss: 0.0234 | 0.0389
Epoch 176/300, seasonal_2 Loss: 0.0233 | 0.0388
Epoch 177/300, seasonal_2 Loss: 0.0233 | 0.0389
Epoch 178/300, seasonal_2 Loss: 0.0233 | 0.0388
Epoch 179/300, seasonal_2 Loss: 0.0233 | 0.0388
Epoch 180/300, seasonal_2 Loss: 0.0233 | 0.0389
Epoch 181/300, seasonal_2 Loss: 0.0232 | 0.0389
Epoch 182/300, seasonal_2 Loss: 0.0232 | 0.0389
Epoch 183/300, seasonal_2 Loss: 0.0232 | 0.0390
Epoch 184/300, seasonal_2 Loss: 0.0232 | 0.0391
Epoch 185/300, seasonal_2 Loss: 0.0232 | 0.0390
Epoch 186/300, seasonal_2 Loss: 0.0232 | 0.0391
Epoch 187/300, seasonal_2 Loss: 0.0231 | 0.0391
Epoch 188/300, seasonal_2 Loss: 0.0231 | 0.0391
Epoch 189/300, seasonal_2 Loss: 0.0231 | 0.0391
Epoch 190/300, seasonal_2 Loss: 0.0231 | 0.0392
Epoch 191/300, seasonal_2 Loss: 0.0231 | 0.0390
Epoch 192/300, seasonal_2 Loss: 0.0231 | 0.0391
Epoch 193/300, seasonal_2 Loss: 0.0230 | 0.0389
Epoch 194/300, seasonal_2 Loss: 0.0230 | 0.0389
Epoch 195/300, seasonal_2 Loss: 0.0230 | 0.0388
Epoch 196/300, seasonal_2 Loss: 0.0230 | 0.0388
Epoch 197/300, seasonal_2 Loss: 0.0230 | 0.0387
Epoch 198/300, seasonal_2 Loss: 0.0230 | 0.0387
Epoch 199/300, seasonal_2 Loss: 0.0230 | 0.0386
Epoch 200/300, seasonal_2 Loss: 0.0230 | 0.0387
Epoch 201/300, seasonal_2 Loss: 0.0229 | 0.0388
Epoch 202/300, seasonal_2 Loss: 0.0229 | 0.0389
Epoch 203/300, seasonal_2 Loss: 0.0229 | 0.0389
Epoch 204/300, seasonal_2 Loss: 0.0229 | 0.0390
Epoch 205/300, seasonal_2 Loss: 0.0229 | 0.0389
Epoch 206/300, seasonal_2 Loss: 0.0229 | 0.0391
Epoch 207/300, seasonal_2 Loss: 0.0229 | 0.0391
Epoch 208/300, seasonal_2 Loss: 0.0229 | 0.0392
Epoch 209/300, seasonal_2 Loss: 0.0228 | 0.0390
Epoch 210/300, seasonal_2 Loss: 0.0228 | 0.0388
Epoch 211/300, seasonal_2 Loss: 0.0228 | 0.0382
Epoch 212/300, seasonal_2 Loss: 0.0228 | 0.0375
Epoch 213/300, seasonal_2 Loss: 0.0228 | 0.0369
Epoch 214/300, seasonal_2 Loss: 0.0228 | 0.0367
Epoch 215/300, seasonal_2 Loss: 0.0227 | 0.0362
Epoch 216/300, seasonal_2 Loss: 0.0220 | 0.0359
Epoch 217/300, seasonal_2 Loss: 0.0218 | 0.0357
Epoch 218/300, seasonal_2 Loss: 0.0218 | 0.0357
Epoch 219/300, seasonal_2 Loss: 0.0218 | 0.0359
Epoch 220/300, seasonal_2 Loss: 0.0217 | 0.0359
Epoch 221/300, seasonal_2 Loss: 0.0217 | 0.0360
Epoch 222/300, seasonal_2 Loss: 0.0217 | 0.0359
Epoch 223/300, seasonal_2 Loss: 0.0217 | 0.0358
Epoch 224/300, seasonal_2 Loss: 0.0217 | 0.0357
Epoch 225/300, seasonal_2 Loss: 0.0217 | 0.0357
Epoch 226/300, seasonal_2 Loss: 0.0216 | 0.0356
Epoch 227/300, seasonal_2 Loss: 0.0216 | 0.0357
Epoch 228/300, seasonal_2 Loss: 0.0216 | 0.0357
Epoch 229/300, seasonal_2 Loss: 0.0216 | 0.0358
Epoch 230/300, seasonal_2 Loss: 0.0216 | 0.0358
Epoch 231/300, seasonal_2 Loss: 0.0216 | 0.0359
Epoch 232/300, seasonal_2 Loss: 0.0216 | 0.0359
Epoch 233/300, seasonal_2 Loss: 0.0216 | 0.0359
Epoch 234/300, seasonal_2 Loss: 0.0216 | 0.0359
Epoch 235/300, seasonal_2 Loss: 0.0216 | 0.0359
Epoch 236/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 237/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 238/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 239/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 240/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 241/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 242/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 243/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 244/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 245/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 246/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 247/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 248/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 249/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 250/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 251/300, seasonal_2 Loss: 0.0215 | 0.0359
Epoch 252/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 253/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 254/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 255/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 256/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 257/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 258/300, seasonal_2 Loss: 0.0214 | 0.0360
Epoch 259/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 260/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 261/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 262/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 263/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 264/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 265/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 266/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 267/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 268/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 269/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 270/300, seasonal_2 Loss: 0.0214 | 0.0361
Epoch 271/300, seasonal_2 Loss: 0.0213 | 0.0361
Epoch 272/300, seasonal_2 Loss: 0.0213 | 0.0360
Epoch 273/300, seasonal_2 Loss: 0.0213 | 0.0360
Epoch 274/300, seasonal_2 Loss: 0.0212 | 0.0359
Epoch 275/300, seasonal_2 Loss: 0.0211 | 0.0359
Epoch 276/300, seasonal_2 Loss: 0.0210 | 0.0359
Epoch 277/300, seasonal_2 Loss: 0.0210 | 0.0358
Epoch 278/300, seasonal_2 Loss: 0.0209 | 0.0359
Epoch 279/300, seasonal_2 Loss: 0.0208 | 0.0358
Epoch 280/300, seasonal_2 Loss: 0.0206 | 0.0359
Epoch 281/300, seasonal_2 Loss: 0.0206 | 0.0358
Epoch 282/300, seasonal_2 Loss: 0.0205 | 0.0359
Epoch 283/300, seasonal_2 Loss: 0.0206 | 0.0358
Epoch 284/300, seasonal_2 Loss: 0.0207 | 0.0359
Epoch 285/300, seasonal_2 Loss: 0.0206 | 0.0358
Epoch 286/300, seasonal_2 Loss: 0.0205 | 0.0359
Epoch 287/300, seasonal_2 Loss: 0.0205 | 0.0359
Epoch 288/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 289/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 290/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 291/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 292/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 293/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 294/300, seasonal_2 Loss: 0.0204 | 0.0359
Epoch 295/300, seasonal_2 Loss: 0.0204 | 0.0360
Epoch 296/300, seasonal_2 Loss: 0.0204 | 0.0360
Epoch 297/300, seasonal_2 Loss: 0.0203 | 0.0360
Epoch 298/300, seasonal_2 Loss: 0.0203 | 0.0360
Epoch 299/300, seasonal_2 Loss: 0.0203 | 0.0360
Epoch 300/300, seasonal_2 Loss: 0.0203 | 0.0360
Training seasonal_3 component with params: {'observation_period_num': 19, 'train_rates': 0.982482874333066, 'learning_rate': 0.00015049424790659728, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9191662542502691}
Epoch 1/300, seasonal_3 Loss: 0.2433 | 0.1546
Epoch 2/300, seasonal_3 Loss: 0.1265 | 0.1472
Epoch 3/300, seasonal_3 Loss: 0.1151 | 0.0608
Epoch 4/300, seasonal_3 Loss: 0.1105 | 0.0881
Epoch 5/300, seasonal_3 Loss: 0.1023 | 0.0692
Epoch 6/300, seasonal_3 Loss: 0.1029 | 0.0711
Epoch 7/300, seasonal_3 Loss: 0.1007 | 0.0695
Epoch 8/300, seasonal_3 Loss: 0.0889 | 0.0825
Epoch 9/300, seasonal_3 Loss: 0.0885 | 0.0756
Epoch 10/300, seasonal_3 Loss: 0.0827 | 0.0825
Epoch 11/300, seasonal_3 Loss: 0.0784 | 0.0569
Epoch 12/300, seasonal_3 Loss: 0.0776 | 0.0609
Epoch 13/300, seasonal_3 Loss: 0.0754 | 0.0558
Epoch 14/300, seasonal_3 Loss: 0.0695 | 0.0411
Epoch 15/300, seasonal_3 Loss: 0.0695 | 0.0412
Epoch 16/300, seasonal_3 Loss: 0.0631 | 0.0463
Epoch 17/300, seasonal_3 Loss: 0.0601 | 0.0410
Epoch 18/300, seasonal_3 Loss: 0.0598 | 0.0484
Epoch 19/300, seasonal_3 Loss: 0.0568 | 0.0297
Epoch 20/300, seasonal_3 Loss: 0.0573 | 0.0365
Epoch 21/300, seasonal_3 Loss: 0.0673 | 0.0390
Epoch 22/300, seasonal_3 Loss: 0.0634 | 0.0461
Epoch 23/300, seasonal_3 Loss: 0.0599 | 0.0440
Epoch 24/300, seasonal_3 Loss: 0.0598 | 0.0512
Epoch 25/300, seasonal_3 Loss: 0.0607 | 0.0748
Epoch 26/300, seasonal_3 Loss: 0.0576 | 0.1187
Epoch 27/300, seasonal_3 Loss: 0.0541 | 0.0551
Epoch 28/300, seasonal_3 Loss: 0.0536 | 0.0654
Epoch 29/300, seasonal_3 Loss: 0.0496 | 0.0681
Epoch 30/300, seasonal_3 Loss: 0.0524 | 0.0323
Epoch 31/300, seasonal_3 Loss: 0.0529 | 0.0571
Epoch 32/300, seasonal_3 Loss: 0.0571 | 0.0723
Epoch 33/300, seasonal_3 Loss: 0.0553 | 0.0798
Epoch 34/300, seasonal_3 Loss: 0.0502 | 0.0486
Epoch 35/300, seasonal_3 Loss: 0.0469 | 0.0517
Epoch 36/300, seasonal_3 Loss: 0.0487 | 0.0325
Epoch 37/300, seasonal_3 Loss: 0.0485 | 0.0499
Epoch 38/300, seasonal_3 Loss: 0.0467 | 0.0285
Epoch 39/300, seasonal_3 Loss: 0.0440 | 0.0601
Epoch 40/300, seasonal_3 Loss: 0.0499 | 0.0386
Epoch 41/300, seasonal_3 Loss: 0.0463 | 0.0548
Epoch 42/300, seasonal_3 Loss: 0.0427 | 0.0469
Epoch 43/300, seasonal_3 Loss: 0.0408 | 0.0370
Epoch 44/300, seasonal_3 Loss: 0.0421 | 0.0314
Epoch 45/300, seasonal_3 Loss: 0.0426 | 0.0436
Epoch 46/300, seasonal_3 Loss: 0.0402 | 0.0413
Epoch 47/300, seasonal_3 Loss: 0.0399 | 0.0379
Epoch 48/300, seasonal_3 Loss: 0.0388 | 0.0413
Epoch 49/300, seasonal_3 Loss: 0.0421 | 0.0290
Epoch 50/300, seasonal_3 Loss: 0.0473 | 0.0289
Epoch 51/300, seasonal_3 Loss: 0.0408 | 0.0285
Epoch 52/300, seasonal_3 Loss: 0.0396 | 0.0266
Epoch 53/300, seasonal_3 Loss: 0.0377 | 0.0378
Epoch 54/300, seasonal_3 Loss: 0.0419 | 0.0320
Epoch 55/300, seasonal_3 Loss: 0.0381 | 0.0329
Epoch 56/300, seasonal_3 Loss: 0.0431 | 0.0524
Epoch 57/300, seasonal_3 Loss: 0.0351 | 0.0272
Epoch 58/300, seasonal_3 Loss: 0.0334 | 0.0321
Epoch 59/300, seasonal_3 Loss: 0.0349 | 0.0316
Epoch 60/300, seasonal_3 Loss: 0.0365 | 0.0434
Epoch 61/300, seasonal_3 Loss: 0.0329 | 0.0342
Epoch 62/300, seasonal_3 Loss: 0.0367 | 0.0288
Epoch 63/300, seasonal_3 Loss: 0.0358 | 0.0268
Epoch 64/300, seasonal_3 Loss: 0.0350 | 0.0299
Epoch 65/300, seasonal_3 Loss: 0.0339 | 0.0303
Epoch 66/300, seasonal_3 Loss: 0.0328 | 0.0290
Epoch 67/300, seasonal_3 Loss: 0.0326 | 0.0308
Epoch 68/300, seasonal_3 Loss: 0.0360 | 0.0379
Epoch 69/300, seasonal_3 Loss: 0.0391 | 0.0628
Epoch 70/300, seasonal_3 Loss: 0.0359 | 0.0777
Epoch 71/300, seasonal_3 Loss: 0.0313 | 0.0322
Epoch 72/300, seasonal_3 Loss: 0.0293 | 0.0329
Epoch 73/300, seasonal_3 Loss: 0.0338 | 0.0343
Epoch 74/300, seasonal_3 Loss: 0.0328 | 0.0305
Epoch 75/300, seasonal_3 Loss: 0.0341 | 0.0422
Epoch 76/300, seasonal_3 Loss: 0.0326 | 0.0366
Epoch 77/300, seasonal_3 Loss: 0.0319 | 0.0362
Epoch 78/300, seasonal_3 Loss: 0.0309 | 0.0362
Epoch 79/300, seasonal_3 Loss: 0.0314 | 0.0380
Epoch 80/300, seasonal_3 Loss: 0.0300 | 0.0311
Epoch 81/300, seasonal_3 Loss: 0.0328 | 0.0287
Epoch 82/300, seasonal_3 Loss: 0.0355 | 0.0306
Epoch 83/300, seasonal_3 Loss: 0.0324 | 0.0316
Epoch 84/300, seasonal_3 Loss: 0.0296 | 0.0316
Epoch 85/300, seasonal_3 Loss: 0.0305 | 0.0290
Epoch 86/300, seasonal_3 Loss: 0.0316 | 0.0299
Epoch 87/300, seasonal_3 Loss: 0.0309 | 0.0270
Epoch 88/300, seasonal_3 Loss: 0.0289 | 0.0269
Epoch 89/300, seasonal_3 Loss: 0.0281 | 0.0332
Epoch 90/300, seasonal_3 Loss: 0.0288 | 0.0293
Epoch 91/300, seasonal_3 Loss: 0.0284 | 0.0329
Epoch 92/300, seasonal_3 Loss: 0.0303 | 0.0394
Epoch 93/300, seasonal_3 Loss: 0.0296 | 0.0522
Epoch 94/300, seasonal_3 Loss: 0.0309 | 0.0351
Epoch 95/300, seasonal_3 Loss: 0.0309 | 0.0557
Epoch 96/300, seasonal_3 Loss: 0.0306 | 0.0530
Epoch 97/300, seasonal_3 Loss: 0.0346 | 0.0405
Epoch 98/300, seasonal_3 Loss: 0.0317 | 0.0523
Epoch 99/300, seasonal_3 Loss: 0.0339 | 0.0538
Epoch 100/300, seasonal_3 Loss: 0.0314 | 0.0516
Epoch 101/300, seasonal_3 Loss: 0.0305 | 0.0689
Epoch 102/300, seasonal_3 Loss: 0.0321 | 0.0439
Epoch 103/300, seasonal_3 Loss: 0.0325 | 0.0587
Epoch 104/300, seasonal_3 Loss: 0.0304 | 0.1093
Epoch 105/300, seasonal_3 Loss: 0.0297 | 0.1953
Epoch 106/300, seasonal_3 Loss: 0.0289 | 0.1895
Epoch 107/300, seasonal_3 Loss: 0.0273 | 0.0938
Epoch 108/300, seasonal_3 Loss: 0.0274 | 0.0685
Epoch 109/300, seasonal_3 Loss: 0.0266 | 0.0395
Epoch 110/300, seasonal_3 Loss: 0.0263 | 0.0351
Epoch 111/300, seasonal_3 Loss: 0.0262 | 0.0532
Epoch 112/300, seasonal_3 Loss: 0.0253 | 0.0581
Epoch 113/300, seasonal_3 Loss: 0.0250 | 0.0555
Epoch 114/300, seasonal_3 Loss: 0.0247 | 0.0573
Epoch 115/300, seasonal_3 Loss: 0.0244 | 0.0577
Epoch 116/300, seasonal_3 Loss: 0.0237 | 0.0515
Epoch 117/300, seasonal_3 Loss: 0.0233 | 0.0477
Epoch 118/300, seasonal_3 Loss: 0.0255 | 0.0413
Epoch 119/300, seasonal_3 Loss: 0.0246 | 0.0471
Epoch 120/300, seasonal_3 Loss: 0.0252 | 0.0489
Epoch 121/300, seasonal_3 Loss: 0.0252 | 0.0312
Epoch 122/300, seasonal_3 Loss: 0.0255 | 0.0336
Epoch 123/300, seasonal_3 Loss: 0.0241 | 0.0318
Epoch 124/300, seasonal_3 Loss: 0.0255 | 0.0431
Epoch 125/300, seasonal_3 Loss: 0.0249 | 0.0375
Epoch 126/300, seasonal_3 Loss: 0.0260 | 0.0596
Epoch 127/300, seasonal_3 Loss: 0.0261 | 0.0753
Epoch 128/300, seasonal_3 Loss: 0.0253 | 0.1252
Epoch 129/300, seasonal_3 Loss: 0.0251 | 0.0921
Epoch 130/300, seasonal_3 Loss: 0.0249 | 0.0432
Epoch 131/300, seasonal_3 Loss: 0.0266 | 0.0493
Epoch 132/300, seasonal_3 Loss: 0.0257 | 0.0563
Epoch 133/300, seasonal_3 Loss: 0.0246 | 0.0660
Epoch 134/300, seasonal_3 Loss: 0.0256 | 0.0521
Epoch 135/300, seasonal_3 Loss: 0.0264 | 0.0388
Epoch 136/300, seasonal_3 Loss: 0.0267 | 0.0386
Epoch 137/300, seasonal_3 Loss: 0.0249 | 0.0586
Epoch 138/300, seasonal_3 Loss: 0.0250 | 0.0673
Epoch 139/300, seasonal_3 Loss: 0.0244 | 0.0596
Epoch 140/300, seasonal_3 Loss: 0.0234 | 0.0430
Epoch 141/300, seasonal_3 Loss: 0.0226 | 0.0320
Epoch 142/300, seasonal_3 Loss: 0.0300 | 0.0355
Epoch 143/300, seasonal_3 Loss: 0.0221 | 0.0384
Epoch 144/300, seasonal_3 Loss: 0.0291 | 0.0471
Epoch 145/300, seasonal_3 Loss: 0.0223 | 0.0483
Epoch 146/300, seasonal_3 Loss: 0.0233 | 0.0527
Epoch 147/300, seasonal_3 Loss: 0.0225 | 0.0544
Epoch 148/300, seasonal_3 Loss: 0.0214 | 0.0522
Epoch 149/300, seasonal_3 Loss: 0.0206 | 0.0509
Epoch 150/300, seasonal_3 Loss: 0.0223 | 0.0513
Epoch 151/300, seasonal_3 Loss: 0.0207 | 0.0459
Epoch 152/300, seasonal_3 Loss: 0.0208 | 0.0483
Epoch 153/300, seasonal_3 Loss: 0.0233 | 0.0447
Epoch 154/300, seasonal_3 Loss: 0.0209 | 0.0383
Epoch 155/300, seasonal_3 Loss: 0.0223 | 0.0368
Epoch 156/300, seasonal_3 Loss: 0.0199 | 0.0342
Epoch 157/300, seasonal_3 Loss: 0.0210 | 0.0321
Epoch 158/300, seasonal_3 Loss: 0.0239 | 0.0330
Epoch 159/300, seasonal_3 Loss: 0.0221 | 0.0470
Epoch 160/300, seasonal_3 Loss: 0.0236 | 0.0583
Epoch 161/300, seasonal_3 Loss: 0.0242 | 0.0626
Epoch 162/300, seasonal_3 Loss: 0.0202 | 0.0605
Epoch 163/300, seasonal_3 Loss: 0.0211 | 0.0577
Epoch 164/300, seasonal_3 Loss: 0.0197 | 0.0455
Epoch 165/300, seasonal_3 Loss: 0.0212 | 0.0389
Epoch 166/300, seasonal_3 Loss: 0.0224 | 0.0348
Epoch 167/300, seasonal_3 Loss: 0.0203 | 0.0334
Epoch 168/300, seasonal_3 Loss: 0.0197 | 0.0309
Epoch 169/300, seasonal_3 Loss: 0.0208 | 0.0300
Epoch 170/300, seasonal_3 Loss: 0.0211 | 0.0309
Epoch 171/300, seasonal_3 Loss: 0.0208 | 0.0347
Epoch 172/300, seasonal_3 Loss: 0.0198 | 0.0345
Epoch 173/300, seasonal_3 Loss: 0.0196 | 0.0359
Epoch 174/300, seasonal_3 Loss: 0.0195 | 0.0371
Epoch 175/300, seasonal_3 Loss: 0.0192 | 0.0383
Epoch 176/300, seasonal_3 Loss: 0.0189 | 0.0403
Epoch 177/300, seasonal_3 Loss: 0.0187 | 0.0409
Epoch 178/300, seasonal_3 Loss: 0.0184 | 0.0439
Epoch 179/300, seasonal_3 Loss: 0.0193 | 0.0432
Epoch 180/300, seasonal_3 Loss: 0.0219 | 0.0455
Epoch 181/300, seasonal_3 Loss: 0.0186 | 0.0438
Epoch 182/300, seasonal_3 Loss: 0.0182 | 0.0449
Epoch 183/300, seasonal_3 Loss: 0.0186 | 0.0407
Epoch 184/300, seasonal_3 Loss: 0.0184 | 0.0434
Epoch 185/300, seasonal_3 Loss: 0.0177 | 0.0421
Epoch 186/300, seasonal_3 Loss: 0.0175 | 0.0397
Epoch 187/300, seasonal_3 Loss: 0.0174 | 0.0377
Epoch 188/300, seasonal_3 Loss: 0.0182 | 0.0351
Epoch 189/300, seasonal_3 Loss: 0.0246 | 0.0344
Epoch 190/300, seasonal_3 Loss: 0.0196 | 0.0349
Epoch 191/300, seasonal_3 Loss: 0.0204 | 0.0415
Epoch 192/300, seasonal_3 Loss: 0.0203 | 0.0530
Epoch 193/300, seasonal_3 Loss: 0.0203 | 0.0642
Epoch 194/300, seasonal_3 Loss: 0.0202 | 0.0650
Epoch 195/300, seasonal_3 Loss: 0.0200 | 0.0537
Epoch 196/300, seasonal_3 Loss: 0.0198 | 0.0431
Epoch 197/300, seasonal_3 Loss: 0.0197 | 0.0373
Epoch 198/300, seasonal_3 Loss: 0.0196 | 0.0346
Epoch 199/300, seasonal_3 Loss: 0.0191 | 0.0346
Epoch 200/300, seasonal_3 Loss: 0.0186 | 0.0365
Epoch 201/300, seasonal_3 Loss: 0.0182 | 0.0395
Epoch 202/300, seasonal_3 Loss: 0.0181 | 0.0421
Epoch 203/300, seasonal_3 Loss: 0.0180 | 0.0434
Epoch 204/300, seasonal_3 Loss: 0.0180 | 0.0436
Epoch 205/300, seasonal_3 Loss: 0.0170 | 0.0430
Epoch 206/300, seasonal_3 Loss: 0.0171 | 0.0437
Epoch 207/300, seasonal_3 Loss: 0.0169 | 0.0432
Epoch 208/300, seasonal_3 Loss: 0.0169 | 0.0441
Epoch 209/300, seasonal_3 Loss: 0.0168 | 0.0457
Epoch 210/300, seasonal_3 Loss: 0.0168 | 0.0479
Epoch 211/300, seasonal_3 Loss: 0.0169 | 0.0486
Epoch 212/300, seasonal_3 Loss: 0.0168 | 0.0477
Epoch 213/300, seasonal_3 Loss: 0.0169 | 0.0446
Epoch 214/300, seasonal_3 Loss: 0.0169 | 0.0443
Epoch 215/300, seasonal_3 Loss: 0.0170 | 0.0530
Epoch 216/300, seasonal_3 Loss: 0.0171 | 0.0684
Epoch 217/300, seasonal_3 Loss: 0.0173 | 0.0859
Epoch 218/300, seasonal_3 Loss: 0.0172 | 0.0757
Epoch 219/300, seasonal_3 Loss: 0.0172 | 0.0567
Epoch 220/300, seasonal_3 Loss: 0.0173 | 0.0493
Epoch 221/300, seasonal_3 Loss: 0.0173 | 0.0529
Epoch 222/300, seasonal_3 Loss: 0.0171 | 0.0537
Epoch 223/300, seasonal_3 Loss: 0.0171 | 0.0426
Epoch 224/300, seasonal_3 Loss: 0.0170 | 0.0451
Epoch 225/300, seasonal_3 Loss: 0.0168 | 0.0655
Epoch 226/300, seasonal_3 Loss: 0.0169 | 0.0686
Epoch 227/300, seasonal_3 Loss: 0.0166 | 0.0556
Epoch 228/300, seasonal_3 Loss: 0.0165 | 0.0514
Epoch 229/300, seasonal_3 Loss: 0.0164 | 0.0558
Epoch 230/300, seasonal_3 Loss: 0.0164 | 0.0572
Epoch 231/300, seasonal_3 Loss: 0.0164 | 0.0562
Epoch 232/300, seasonal_3 Loss: 0.0163 | 0.0539
Epoch 233/300, seasonal_3 Loss: 0.0163 | 0.0523
Epoch 234/300, seasonal_3 Loss: 0.0162 | 0.0531
Epoch 235/300, seasonal_3 Loss: 0.0162 | 0.0544
Epoch 236/300, seasonal_3 Loss: 0.0162 | 0.0535
Epoch 237/300, seasonal_3 Loss: 0.0162 | 0.0498
Epoch 238/300, seasonal_3 Loss: 0.0161 | 0.0477
Epoch 239/300, seasonal_3 Loss: 0.0161 | 0.0494
Epoch 240/300, seasonal_3 Loss: 0.0160 | 0.0499
Epoch 241/300, seasonal_3 Loss: 0.0160 | 0.0471
Epoch 242/300, seasonal_3 Loss: 0.0160 | 0.0450
Epoch 243/300, seasonal_3 Loss: 0.0160 | 0.0540
Epoch 244/300, seasonal_3 Loss: 0.0160 | 0.0568
Epoch 245/300, seasonal_3 Loss: 0.0160 | 0.0587
Epoch 246/300, seasonal_3 Loss: 0.0159 | 0.0591
Epoch 247/300, seasonal_3 Loss: 0.0159 | 0.0548
Epoch 248/300, seasonal_3 Loss: 0.0159 | 0.0520
Epoch 249/300, seasonal_3 Loss: 0.0158 | 0.0525
Epoch 250/300, seasonal_3 Loss: 0.0158 | 0.0548
Epoch 251/300, seasonal_3 Loss: 0.0157 | 0.0566
Epoch 252/300, seasonal_3 Loss: 0.0157 | 0.0545
Epoch 253/300, seasonal_3 Loss: 0.0157 | 0.0532
Epoch 254/300, seasonal_3 Loss: 0.0157 | 0.0504
Epoch 255/300, seasonal_3 Loss: 0.0157 | 0.0533
Epoch 256/300, seasonal_3 Loss: 0.0157 | 0.0572
Epoch 257/300, seasonal_3 Loss: 0.0157 | 0.0634
Epoch 258/300, seasonal_3 Loss: 0.0157 | 0.0649
Epoch 259/300, seasonal_3 Loss: 0.0157 | 0.0547
Epoch 260/300, seasonal_3 Loss: 0.0157 | 0.0433
Epoch 261/300, seasonal_3 Loss: 0.0158 | 0.0450
Epoch 262/300, seasonal_3 Loss: 0.0157 | 0.0449
Epoch 263/300, seasonal_3 Loss: 0.0156 | 0.0519
Epoch 264/300, seasonal_3 Loss: 0.0156 | 0.0540
Epoch 265/300, seasonal_3 Loss: 0.0156 | 0.0523
Epoch 266/300, seasonal_3 Loss: 0.0156 | 0.0516
Epoch 267/300, seasonal_3 Loss: 0.0155 | 0.0538
Epoch 268/300, seasonal_3 Loss: 0.0155 | 0.0555
Epoch 269/300, seasonal_3 Loss: 0.0154 | 0.0558
Epoch 270/300, seasonal_3 Loss: 0.0154 | 0.0571
Epoch 271/300, seasonal_3 Loss: 0.0154 | 0.0571
Epoch 272/300, seasonal_3 Loss: 0.0153 | 0.0573
Epoch 273/300, seasonal_3 Loss: 0.0153 | 0.0560
Epoch 274/300, seasonal_3 Loss: 0.0153 | 0.0559
Epoch 275/300, seasonal_3 Loss: 0.0153 | 0.0551
Epoch 276/300, seasonal_3 Loss: 0.0153 | 0.0553
Epoch 277/300, seasonal_3 Loss: 0.0153 | 0.0537
Epoch 278/300, seasonal_3 Loss: 0.0152 | 0.0517
Epoch 279/300, seasonal_3 Loss: 0.0152 | 0.0489
Epoch 280/300, seasonal_3 Loss: 0.0152 | 0.0499
Epoch 281/300, seasonal_3 Loss: 0.0152 | 0.0536
Epoch 282/300, seasonal_3 Loss: 0.0152 | 0.0592
Epoch 283/300, seasonal_3 Loss: 0.0153 | 0.0637
Epoch 284/300, seasonal_3 Loss: 0.0153 | 0.0601
Epoch 285/300, seasonal_3 Loss: 0.0154 | 0.0564
Epoch 286/300, seasonal_3 Loss: 0.0153 | 0.0588
Epoch 287/300, seasonal_3 Loss: 0.0154 | 0.0605
Epoch 288/300, seasonal_3 Loss: 0.0154 | 0.0598
Epoch 289/300, seasonal_3 Loss: 0.0154 | 0.0512
Epoch 290/300, seasonal_3 Loss: 0.0154 | 0.0434
Epoch 291/300, seasonal_3 Loss: 0.0153 | 0.0446
Epoch 292/300, seasonal_3 Loss: 0.0152 | 0.0493
Epoch 293/300, seasonal_3 Loss: 0.0151 | 0.0551
Epoch 294/300, seasonal_3 Loss: 0.0151 | 0.0591
Epoch 295/300, seasonal_3 Loss: 0.0151 | 0.0603
Epoch 296/300, seasonal_3 Loss: 0.0151 | 0.0603
Epoch 297/300, seasonal_3 Loss: 0.0150 | 0.0599
Epoch 298/300, seasonal_3 Loss: 0.0150 | 0.0582
Epoch 299/300, seasonal_3 Loss: 0.0150 | 0.0561
Epoch 300/300, seasonal_3 Loss: 0.0149 | 0.0545
Training resid component with params: {'observation_period_num': 8, 'train_rates': 0.9042617893974346, 'learning_rate': 0.0009818737100685557, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8731808613274266}
Epoch 1/300, resid Loss: 1.5503 | 1.6587
Epoch 2/300, resid Loss: 0.8481 | 1.6033
Epoch 3/300, resid Loss: 0.8571 | 1.5955
Epoch 4/300, resid Loss: 0.8767 | 1.5885
Epoch 5/300, resid Loss: 0.8955 | 1.6046
Epoch 6/300, resid Loss: 0.9191 | 1.5944
Epoch 7/300, resid Loss: 0.9248 | 1.5874
Epoch 8/300, resid Loss: 0.9097 | 1.5911
Epoch 9/300, resid Loss: 0.9411 | 1.5872
Epoch 10/300, resid Loss: 1.0136 | 1.5972
Epoch 11/300, resid Loss: 1.0722 | 1.6482
Epoch 12/300, resid Loss: 1.0909 | 1.7954
Epoch 13/300, resid Loss: 1.0584 | 2.0063
Epoch 14/300, resid Loss: 0.9871 | 2.0228
Epoch 15/300, resid Loss: 0.9747 | 2.0395
Epoch 16/300, resid Loss: 0.9679 | 2.0567
Epoch 17/300, resid Loss: 0.9552 | 2.1065
Epoch 18/300, resid Loss: 0.9411 | 2.1044
Epoch 19/300, resid Loss: 0.9401 | 2.1107
Epoch 20/300, resid Loss: 0.9375 | 2.1156
Epoch 21/300, resid Loss: 0.9296 | 2.1401
Epoch 22/300, resid Loss: 0.9235 | 2.1400
Epoch 23/300, resid Loss: 0.9230 | 2.1421
Epoch 24/300, resid Loss: 0.9218 | 2.1442
Epoch 25/300, resid Loss: 0.9159 | 2.1579
Epoch 26/300, resid Loss: 0.9129 | 2.1594
Epoch 27/300, resid Loss: 0.9124 | 2.1599
Epoch 28/300, resid Loss: 0.9119 | 2.1608
Epoch 29/300, resid Loss: 0.9073 | 2.1691
Epoch 30/300, resid Loss: 0.9057 | 2.1712
Epoch 31/300, resid Loss: 0.9053 | 2.1712
Epoch 32/300, resid Loss: 0.9051 | 2.1716
Epoch 33/300, resid Loss: 0.9013 | 2.1769
Epoch 34/300, resid Loss: 0.9004 | 2.1790
Epoch 35/300, resid Loss: 0.9001 | 2.1789
Epoch 36/300, resid Loss: 0.9000 | 2.1791
Epoch 37/300, resid Loss: 0.8969 | 2.1826
Epoch 38/300, resid Loss: 0.8964 | 2.1845
Epoch 39/300, resid Loss: 0.8962 | 2.1846
Epoch 40/300, resid Loss: 0.8961 | 2.1846
Epoch 41/300, resid Loss: 0.8935 | 2.1869
Epoch 42/300, resid Loss: 0.8932 | 2.1886
Epoch 43/300, resid Loss: 0.8931 | 2.1887
Epoch 44/300, resid Loss: 0.8930 | 2.1887
Epoch 45/300, resid Loss: 0.8908 | 2.1903
Epoch 46/300, resid Loss: 0.8907 | 2.1918
Epoch 47/300, resid Loss: 0.8906 | 2.1920
Epoch 48/300, resid Loss: 0.8906 | 2.1920
Epoch 49/300, resid Loss: 0.8886 | 2.1931
Epoch 50/300, resid Loss: 0.8886 | 2.1943
Epoch 51/300, resid Loss: 0.8885 | 2.1946
Epoch 52/300, resid Loss: 0.8885 | 2.1945
Epoch 53/300, resid Loss: 0.8868 | 2.1953
Epoch 54/300, resid Loss: 0.8868 | 2.1964
Epoch 55/300, resid Loss: 0.8868 | 2.1966
Epoch 56/300, resid Loss: 0.8868 | 2.1967
Epoch 57/300, resid Loss: 0.8853 | 2.1972
Epoch 58/300, resid Loss: 0.8853 | 2.1981
Epoch 59/300, resid Loss: 0.8853 | 2.1983
Epoch 60/300, resid Loss: 0.8853 | 2.1984
Epoch 61/300, resid Loss: 0.8840 | 2.1988
Epoch 62/300, resid Loss: 0.8841 | 2.1995
Epoch 63/300, resid Loss: 0.8841 | 2.1998
Epoch 64/300, resid Loss: 0.8841 | 2.1999
Epoch 65/300, resid Loss: 0.8829 | 2.2002
Epoch 66/300, resid Loss: 0.8830 | 2.2008
Epoch 67/300, resid Loss: 0.8830 | 2.2010
Epoch 68/300, resid Loss: 0.8830 | 2.2011
Epoch 69/300, resid Loss: 0.8820 | 2.2014
Epoch 70/300, resid Loss: 0.8820 | 2.2018
Epoch 71/300, resid Loss: 0.8820 | 2.2021
Epoch 72/300, resid Loss: 0.8821 | 2.2021
Epoch 73/300, resid Loss: 0.8812 | 2.2024
Epoch 74/300, resid Loss: 0.8812 | 2.2027
Epoch 75/300, resid Loss: 0.8812 | 2.2030
Epoch 76/300, resid Loss: 0.8812 | 2.2031
Epoch 77/300, resid Loss: 0.8804 | 2.2032
Epoch 78/300, resid Loss: 0.8805 | 2.2036
Epoch 79/300, resid Loss: 0.8805 | 2.2037
Epoch 80/300, resid Loss: 0.8805 | 2.2038
Epoch 81/300, resid Loss: 0.8798 | 2.2040
Epoch 82/300, resid Loss: 0.8798 | 2.2043
Epoch 83/300, resid Loss: 0.8798 | 2.2044
Epoch 84/300, resid Loss: 0.8799 | 2.2045
Epoch 85/300, resid Loss: 0.8792 | 2.2047
Epoch 86/300, resid Loss: 0.8793 | 2.2049
Epoch 87/300, resid Loss: 0.8793 | 2.2050
Epoch 88/300, resid Loss: 0.8793 | 2.2051
Epoch 89/300, resid Loss: 0.8787 | 2.2052
Epoch 90/300, resid Loss: 0.8788 | 2.2054
Epoch 91/300, resid Loss: 0.8788 | 2.2056
Epoch 92/300, resid Loss: 0.8788 | 2.2057
Epoch 93/300, resid Loss: 0.8783 | 2.2058
Epoch 94/300, resid Loss: 0.8783 | 2.2059
Epoch 95/300, resid Loss: 0.8783 | 2.2060
Epoch 96/300, resid Loss: 0.8783 | 2.2061
Epoch 97/300, resid Loss: 0.8779 | 2.2062
Epoch 98/300, resid Loss: 0.8779 | 2.2064
Epoch 99/300, resid Loss: 0.8779 | 2.2065
Epoch 100/300, resid Loss: 0.8779 | 2.2065
Epoch 101/300, resid Loss: 0.8775 | 2.2066
Epoch 102/300, resid Loss: 0.8776 | 2.2067
Epoch 103/300, resid Loss: 0.8776 | 2.2068
Epoch 104/300, resid Loss: 0.8776 | 2.2069
Epoch 105/300, resid Loss: 0.8772 | 2.2070
Epoch 106/300, resid Loss: 0.8772 | 2.2071
Epoch 107/300, resid Loss: 0.8772 | 2.2072
Epoch 108/300, resid Loss: 0.8772 | 2.2072
Epoch 109/300, resid Loss: 0.8769 | 2.2073
Epoch 110/300, resid Loss: 0.8770 | 2.2074
Epoch 111/300, resid Loss: 0.8770 | 2.2075
Epoch 112/300, resid Loss: 0.8770 | 2.2075
Epoch 113/300, resid Loss: 0.8767 | 2.2076
Epoch 114/300, resid Loss: 0.8767 | 2.2076
Epoch 115/300, resid Loss: 0.8767 | 2.2077
Epoch 116/300, resid Loss: 0.8767 | 2.2078
Epoch 117/300, resid Loss: 0.8765 | 2.2078
Epoch 118/300, resid Loss: 0.8765 | 2.2079
Epoch 119/300, resid Loss: 0.8765 | 2.2079
Epoch 120/300, resid Loss: 0.8765 | 2.2080
Epoch 121/300, resid Loss: 0.8763 | 2.2080
Epoch 122/300, resid Loss: 0.8763 | 2.2081
Epoch 123/300, resid Loss: 0.8763 | 2.2081
Epoch 124/300, resid Loss: 0.8763 | 2.2082
Epoch 125/300, resid Loss: 0.8761 | 2.2082
Epoch 126/300, resid Loss: 0.8761 | 2.2083
Epoch 127/300, resid Loss: 0.8761 | 2.2083
Epoch 128/300, resid Loss: 0.8761 | 2.2084
Epoch 129/300, resid Loss: 0.8760 | 2.2084
Epoch 130/300, resid Loss: 0.8760 | 2.2084
Epoch 131/300, resid Loss: 0.8760 | 2.2085
Epoch 132/300, resid Loss: 0.8760 | 2.2085
Epoch 133/300, resid Loss: 0.8758 | 2.2086
Epoch 134/300, resid Loss: 0.8758 | 2.2086
Epoch 135/300, resid Loss: 0.8758 | 2.2086
Epoch 136/300, resid Loss: 0.8758 | 2.2087
Epoch 137/300, resid Loss: 0.8757 | 2.2087
Epoch 138/300, resid Loss: 0.8757 | 2.2087
Epoch 139/300, resid Loss: 0.8757 | 2.2088
Epoch 140/300, resid Loss: 0.8757 | 2.2088
Epoch 141/300, resid Loss: 0.8756 | 2.2088
Epoch 142/300, resid Loss: 0.8756 | 2.2088
Epoch 143/300, resid Loss: 0.8756 | 2.2089
Epoch 144/300, resid Loss: 0.8756 | 2.2089
Epoch 145/300, resid Loss: 0.8755 | 2.2089
Epoch 146/300, resid Loss: 0.8755 | 2.2089
Epoch 147/300, resid Loss: 0.8755 | 2.2090
Epoch 148/300, resid Loss: 0.8755 | 2.2090
Epoch 149/300, resid Loss: 0.8754 | 2.2090
Epoch 150/300, resid Loss: 0.8754 | 2.2090
Epoch 151/300, resid Loss: 0.8754 | 2.2091
Epoch 152/300, resid Loss: 0.8754 | 2.2091
Epoch 153/300, resid Loss: 0.8753 | 2.2091
Epoch 154/300, resid Loss: 0.8753 | 2.2091
Epoch 155/300, resid Loss: 0.8753 | 2.2091
Epoch 156/300, resid Loss: 0.8753 | 2.2091
Epoch 157/300, resid Loss: 0.8753 | 2.2092
Epoch 158/300, resid Loss: 0.8753 | 2.2092
Epoch 159/300, resid Loss: 0.8753 | 2.2092
Epoch 160/300, resid Loss: 0.8753 | 2.2092
Epoch 161/300, resid Loss: 0.8752 | 2.2092
Epoch 162/300, resid Loss: 0.8752 | 2.2092
Epoch 163/300, resid Loss: 0.8752 | 2.2093
Epoch 164/300, resid Loss: 0.8752 | 2.2093
Epoch 165/300, resid Loss: 0.8752 | 2.2093
Epoch 166/300, resid Loss: 0.8752 | 2.2093
Epoch 167/300, resid Loss: 0.8752 | 2.2093
Epoch 168/300, resid Loss: 0.8752 | 2.2093
Epoch 169/300, resid Loss: 0.8751 | 2.2093
Epoch 170/300, resid Loss: 0.8751 | 2.2093
Epoch 171/300, resid Loss: 0.8751 | 2.2094
Epoch 172/300, resid Loss: 0.8751 | 2.2094
Epoch 173/300, resid Loss: 0.8751 | 2.2094
Epoch 174/300, resid Loss: 0.8751 | 2.2094
Epoch 175/300, resid Loss: 0.8751 | 2.2094
Epoch 176/300, resid Loss: 0.8751 | 2.2094
Epoch 177/300, resid Loss: 0.8750 | 2.2094
Epoch 178/300, resid Loss: 0.8750 | 2.2094
Epoch 179/300, resid Loss: 0.8750 | 2.2094
Epoch 180/300, resid Loss: 0.8750 | 2.2094
Epoch 181/300, resid Loss: 0.8750 | 2.2094
Epoch 182/300, resid Loss: 0.8750 | 2.2095
Epoch 183/300, resid Loss: 0.8750 | 2.2095
Epoch 184/300, resid Loss: 0.8750 | 2.2095
Epoch 185/300, resid Loss: 0.8750 | 2.2095
Epoch 186/300, resid Loss: 0.8750 | 2.2095
Epoch 187/300, resid Loss: 0.8750 | 2.2095
Epoch 188/300, resid Loss: 0.8750 | 2.2095
Epoch 189/300, resid Loss: 0.8750 | 2.2095
Epoch 190/300, resid Loss: 0.8750 | 2.2095
Epoch 191/300, resid Loss: 0.8750 | 2.2095
Epoch 192/300, resid Loss: 0.8750 | 2.2095
Epoch 193/300, resid Loss: 0.8749 | 2.2095
Epoch 194/300, resid Loss: 0.8749 | 2.2095
Epoch 195/300, resid Loss: 0.8749 | 2.2095
Epoch 196/300, resid Loss: 0.8749 | 2.2095
Epoch 197/300, resid Loss: 0.8749 | 2.2096
Epoch 198/300, resid Loss: 0.8749 | 2.2096
Epoch 199/300, resid Loss: 0.8749 | 2.2096
Epoch 200/300, resid Loss: 0.8749 | 2.2096
Epoch 201/300, resid Loss: 0.8749 | 2.2096
Epoch 202/300, resid Loss: 0.8749 | 2.2096
Epoch 203/300, resid Loss: 0.8749 | 2.2096
Epoch 204/300, resid Loss: 0.8749 | 2.2096
Epoch 205/300, resid Loss: 0.8749 | 2.2096
Epoch 206/300, resid Loss: 0.8749 | 2.2096
Epoch 207/300, resid Loss: 0.8749 | 2.2096
Epoch 208/300, resid Loss: 0.8749 | 2.2096
Epoch 209/300, resid Loss: 0.8749 | 2.2096
Epoch 210/300, resid Loss: 0.8749 | 2.2096
Epoch 211/300, resid Loss: 0.8749 | 2.2096
Epoch 212/300, resid Loss: 0.8749 | 2.2096
Epoch 213/300, resid Loss: 0.8749 | 2.2096
Epoch 214/300, resid Loss: 0.8749 | 2.2096
Epoch 215/300, resid Loss: 0.8749 | 2.2096
Epoch 216/300, resid Loss: 0.8749 | 2.2096
Epoch 217/300, resid Loss: 0.8749 | 2.2096
Epoch 218/300, resid Loss: 0.8749 | 2.2096
Epoch 219/300, resid Loss: 0.8749 | 2.2096
Epoch 220/300, resid Loss: 0.8749 | 2.2096
Epoch 221/300, resid Loss: 0.8748 | 2.2096
Epoch 222/300, resid Loss: 0.8748 | 2.2096
Epoch 223/300, resid Loss: 0.8748 | 2.2096
Epoch 224/300, resid Loss: 0.8748 | 2.2096
Epoch 225/300, resid Loss: 0.8748 | 2.2096
Epoch 226/300, resid Loss: 0.8748 | 2.2096
Epoch 227/300, resid Loss: 0.8748 | 2.2096
Epoch 228/300, resid Loss: 0.8748 | 2.2096
Epoch 229/300, resid Loss: 0.8748 | 2.2096
Epoch 230/300, resid Loss: 0.8748 | 2.2097
Epoch 231/300, resid Loss: 0.8748 | 2.2097
Epoch 232/300, resid Loss: 0.8748 | 2.2097
Epoch 233/300, resid Loss: 0.8748 | 2.2097
Epoch 234/300, resid Loss: 0.8748 | 2.2097
Epoch 235/300, resid Loss: 0.8748 | 2.2097
Epoch 236/300, resid Loss: 0.8748 | 2.2097
Epoch 237/300, resid Loss: 0.8748 | 2.2097
Epoch 238/300, resid Loss: 0.8748 | 2.2097
Epoch 239/300, resid Loss: 0.8748 | 2.2097
Epoch 240/300, resid Loss: 0.8748 | 2.2097
Epoch 241/300, resid Loss: 0.8748 | 2.2097
Epoch 242/300, resid Loss: 0.8748 | 2.2097
Epoch 243/300, resid Loss: 0.8748 | 2.2097
Epoch 244/300, resid Loss: 0.8748 | 2.2097
Epoch 245/300, resid Loss: 0.8748 | 2.2097
Epoch 246/300, resid Loss: 0.8748 | 2.2097
Epoch 247/300, resid Loss: 0.8748 | 2.2097
Epoch 248/300, resid Loss: 0.8748 | 2.2097
Epoch 249/300, resid Loss: 0.8748 | 2.2097
Epoch 250/300, resid Loss: 0.8748 | 2.2097
Epoch 251/300, resid Loss: 0.8748 | 2.2097
Epoch 252/300, resid Loss: 0.8748 | 2.2097
Epoch 253/300, resid Loss: 0.8748 | 2.2097
Epoch 254/300, resid Loss: 0.8748 | 2.2097
Epoch 255/300, resid Loss: 0.8748 | 2.2097
Epoch 256/300, resid Loss: 0.8748 | 2.2097
Epoch 257/300, resid Loss: 0.8748 | 2.2097
Epoch 258/300, resid Loss: 0.8748 | 2.2097
Epoch 259/300, resid Loss: 0.8748 | 2.2097
Epoch 260/300, resid Loss: 0.8748 | 2.2097
Epoch 261/300, resid Loss: 0.8748 | 2.2097
Epoch 262/300, resid Loss: 0.8748 | 2.2097
Epoch 263/300, resid Loss: 0.8748 | 2.2097
Epoch 264/300, resid Loss: 0.8748 | 2.2097
Epoch 265/300, resid Loss: 0.8748 | 2.2097
Epoch 266/300, resid Loss: 0.8748 | 2.2097
Epoch 267/300, resid Loss: 0.8748 | 2.2097
Epoch 268/300, resid Loss: 0.8748 | 2.2097
Epoch 269/300, resid Loss: 0.8748 | 2.2097
Epoch 270/300, resid Loss: 0.8748 | 2.2097
Epoch 271/300, resid Loss: 0.8748 | 2.2097
Epoch 272/300, resid Loss: 0.8748 | 2.2097
Epoch 273/300, resid Loss: 0.8748 | 2.2097
Epoch 274/300, resid Loss: 0.8748 | 2.2097
Epoch 275/300, resid Loss: 0.8748 | 2.2097
Epoch 276/300, resid Loss: 0.8748 | 2.2097
Epoch 277/300, resid Loss: 0.8748 | 2.2097
Epoch 278/300, resid Loss: 0.8748 | 2.2097
Epoch 279/300, resid Loss: 0.8748 | 2.2097
Epoch 280/300, resid Loss: 0.8748 | 2.2097
Epoch 281/300, resid Loss: 0.8748 | 2.2097
Epoch 282/300, resid Loss: 0.8748 | 2.2097
Epoch 283/300, resid Loss: 0.8748 | 2.2097
Epoch 284/300, resid Loss: 0.8748 | 2.2097
Epoch 285/300, resid Loss: 0.8748 | 2.2097
Epoch 286/300, resid Loss: 0.8748 | 2.2097
Epoch 287/300, resid Loss: 0.8748 | 2.2097
Epoch 288/300, resid Loss: 0.8748 | 2.2097
Epoch 289/300, resid Loss: 0.8748 | 2.2097
Early stopping for resid
Runtime (seconds): 15428.417731285095
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[204.04031]
[-0.19219063]
[1.7891377]
[3.937466]
[-0.6348924]
[-2.8634531]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 566.1351325849537
RMSE: 23.793594360351562
MAE: 23.793594360351562
R-squared: nan
[206.0764]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
