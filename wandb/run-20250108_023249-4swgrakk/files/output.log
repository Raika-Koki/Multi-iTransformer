[32m[I 2025-01-08 02:32:50,995][0m A new study created in memory with name: no-name-7cf68d5d-53ee-4bca-b850-b8864f012778[0m
[32m[I 2025-01-08 02:35:36,379][0m Trial 0 finished with value: 0.3254859449072955 and parameters: {'observation_period_num': 116, 'train_rates': 0.9348235012866017, 'learning_rate': 0.0004410777808566247, 'batch_size': 58, 'step_size': 11, 'gamma': 0.9383670775650337}. Best is trial 0 with value: 0.3254859449072955.[0m
[32m[I 2025-01-08 02:38:26,063][0m Trial 1 finished with value: 1.1189779474828314 and parameters: {'observation_period_num': 149, 'train_rates': 0.6643944683512679, 'learning_rate': 6.798497229290411e-05, 'batch_size': 185, 'step_size': 2, 'gamma': 0.849301474064517}. Best is trial 0 with value: 0.3254859449072955.[0m
[32m[I 2025-01-08 02:42:45,240][0m Trial 2 finished with value: 0.2923804724576489 and parameters: {'observation_period_num': 190, 'train_rates': 0.8395143048855298, 'learning_rate': 0.00015225610150829217, 'batch_size': 207, 'step_size': 14, 'gamma': 0.8478255624877051}. Best is trial 2 with value: 0.2923804724576489.[0m
[32m[I 2025-01-08 02:46:51,791][0m Trial 3 finished with value: 0.7132554372684848 and parameters: {'observation_period_num': 206, 'train_rates': 0.6701508179320614, 'learning_rate': 0.00032129511333227806, 'batch_size': 227, 'step_size': 11, 'gamma': 0.8141849696436907}. Best is trial 2 with value: 0.2923804724576489.[0m
[32m[I 2025-01-08 02:50:22,242][0m Trial 4 finished with value: 0.42174000133254486 and parameters: {'observation_period_num': 159, 'train_rates': 0.8226266951532301, 'learning_rate': 1.3199237240094083e-05, 'batch_size': 106, 'step_size': 8, 'gamma': 0.9871134505942818}. Best is trial 2 with value: 0.2923804724576489.[0m
[32m[I 2025-01-08 02:54:17,513][0m Trial 5 finished with value: 0.7142741896447212 and parameters: {'observation_period_num': 197, 'train_rates': 0.669036872721055, 'learning_rate': 2.1165000709494158e-05, 'batch_size': 130, 'step_size': 4, 'gamma': 0.9728436225831278}. Best is trial 2 with value: 0.2923804724576489.[0m
[32m[I 2025-01-08 02:56:19,230][0m Trial 6 finished with value: 0.8055353358492151 and parameters: {'observation_period_num': 110, 'train_rates': 0.7002488774534743, 'learning_rate': 0.000838847870456547, 'batch_size': 204, 'step_size': 14, 'gamma': 0.761332266732723}. Best is trial 2 with value: 0.2923804724576489.[0m
[32m[I 2025-01-08 03:02:19,886][0m Trial 7 finished with value: 0.21027050912380219 and parameters: {'observation_period_num': 227, 'train_rates': 0.9777530505440457, 'learning_rate': 2.0315215559238778e-05, 'batch_size': 88, 'step_size': 6, 'gamma': 0.877069909196484}. Best is trial 7 with value: 0.21027050912380219.[0m
[32m[I 2025-01-08 03:07:57,124][0m Trial 8 finished with value: 0.1303865055864056 and parameters: {'observation_period_num': 198, 'train_rates': 0.9829515026557609, 'learning_rate': 3.493007708083664e-05, 'batch_size': 25, 'step_size': 13, 'gamma': 0.9798976066353187}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:09:20,694][0m Trial 9 finished with value: 0.7997446537017823 and parameters: {'observation_period_num': 73, 'train_rates': 0.6672648046406329, 'learning_rate': 0.000300153707802127, 'batch_size': 119, 'step_size': 1, 'gamma': 0.8888898646409409}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:13:03,471][0m Trial 10 finished with value: 0.29407456175847485 and parameters: {'observation_period_num': 5, 'train_rates': 0.9074754140694793, 'learning_rate': 1.8758078908543277e-06, 'batch_size': 17, 'step_size': 12, 'gamma': 0.917808148909794}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:19:52,437][0m Trial 11 finished with value: 0.3744305670261383 and parameters: {'observation_period_num': 250, 'train_rates': 0.9790323259808575, 'learning_rate': 8.233958771490757e-06, 'batch_size': 63, 'step_size': 7, 'gamma': 0.7892743921091833}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:27:01,840][0m Trial 12 finished with value: 0.25921682935012014 and parameters: {'observation_period_num': 251, 'train_rates': 0.9792411763385153, 'learning_rate': 4.0716171534690665e-06, 'batch_size': 30, 'step_size': 5, 'gamma': 0.9354467473178336}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:32:19,909][0m Trial 13 finished with value: 0.1885344876785471 and parameters: {'observation_period_num': 216, 'train_rates': 0.8922014580884166, 'learning_rate': 5.034979550805588e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8923537675470887}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:36:15,553][0m Trial 14 finished with value: 0.27254465137867584 and parameters: {'observation_period_num': 171, 'train_rates': 0.8617076571592183, 'learning_rate': 5.8772582966578635e-05, 'batch_size': 163, 'step_size': 10, 'gamma': 0.9570789036658384}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:40:52,973][0m Trial 15 finished with value: 0.49076544245084125 and parameters: {'observation_period_num': 210, 'train_rates': 0.7437547479792108, 'learning_rate': 5.575752473598048e-05, 'batch_size': 59, 'step_size': 15, 'gamma': 0.9072503493220826}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:42:50,021][0m Trial 16 finished with value: 0.18048315647305274 and parameters: {'observation_period_num': 86, 'train_rates': 0.8983522841542112, 'learning_rate': 0.0001295690841294043, 'batch_size': 85, 'step_size': 9, 'gamma': 0.8360904194360328}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:44:35,927][0m Trial 17 finished with value: 0.4465543063591015 and parameters: {'observation_period_num': 67, 'train_rates': 0.7750759058830036, 'learning_rate': 0.00013081019114455835, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8250730579710289}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:46:26,854][0m Trial 18 finished with value: 0.516284802224901 and parameters: {'observation_period_num': 80, 'train_rates': 0.9378255048746951, 'learning_rate': 5.5425685146815236e-06, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8498336412415196}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:47:08,775][0m Trial 19 finished with value: 1.0663888290856756 and parameters: {'observation_period_num': 41, 'train_rates': 0.6023662432749028, 'learning_rate': 0.00013418984189991507, 'batch_size': 252, 'step_size': 12, 'gamma': 0.8010754127665896}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:49:24,804][0m Trial 20 finished with value: 0.21349259140893354 and parameters: {'observation_period_num': 96, 'train_rates': 0.8774892124169951, 'learning_rate': 3.197845806458897e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.7656666084939067}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:52:34,459][0m Trial 21 finished with value: 0.20507947556423337 and parameters: {'observation_period_num': 133, 'train_rates': 0.9059714651713694, 'learning_rate': 3.7123950089518544e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9028209842403477}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 03:56:59,883][0m Trial 22 finished with value: 0.2196803716096011 and parameters: {'observation_period_num': 183, 'train_rates': 0.9372078573597237, 'learning_rate': 9.268914267160536e-05, 'batch_size': 156, 'step_size': 8, 'gamma': 0.8679887490264547}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:02:08,482][0m Trial 23 finished with value: 0.2917310511063396 and parameters: {'observation_period_num': 224, 'train_rates': 0.8010262517791921, 'learning_rate': 0.00024085999782898204, 'batch_size': 71, 'step_size': 10, 'gamma': 0.8292856320469648}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:05:25,347][0m Trial 24 finished with value: 0.24949911543142014 and parameters: {'observation_period_num': 140, 'train_rates': 0.8866441312377052, 'learning_rate': 1.4816785909780803e-05, 'batch_size': 102, 'step_size': 7, 'gamma': 0.9626843068784905}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:06:26,461][0m Trial 25 finished with value: 0.43306806683540344 and parameters: {'observation_period_num': 45, 'train_rates': 0.9509731176329745, 'learning_rate': 3.7728231337593635e-05, 'batch_size': 151, 'step_size': 4, 'gamma': 0.7850726522954713}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:11:45,952][0m Trial 26 finished with value: 0.8826478186860142 and parameters: {'observation_period_num': 219, 'train_rates': 0.8495289818440612, 'learning_rate': 0.0006982348313118357, 'batch_size': 47, 'step_size': 12, 'gamma': 0.9347552173173597}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:15:55,874][0m Trial 27 finished with value: 0.18464710926398253 and parameters: {'observation_period_num': 175, 'train_rates': 0.902386079958315, 'learning_rate': 9.851921144907102e-05, 'batch_size': 77, 'step_size': 10, 'gamma': 0.8606917824831608}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:19:53,400][0m Trial 28 finished with value: 0.17524777725338936 and parameters: {'observation_period_num': 165, 'train_rates': 0.914773988360964, 'learning_rate': 0.00018188326332929213, 'batch_size': 120, 'step_size': 11, 'gamma': 0.8622163973289179}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:22:36,431][0m Trial 29 finished with value: 0.2670392435562762 and parameters: {'observation_period_num': 119, 'train_rates': 0.9447011566004404, 'learning_rate': 0.00047791331419933994, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8320764588326309}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:27:12,017][0m Trial 30 finished with value: 0.15266150910535764 and parameters: {'observation_period_num': 159, 'train_rates': 0.9588126823440052, 'learning_rate': 0.00017951238596029416, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8808877761770145}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:31:40,727][0m Trial 31 finished with value: 0.19609737587631296 and parameters: {'observation_period_num': 161, 'train_rates': 0.9615537988784338, 'learning_rate': 0.0002024598457313092, 'batch_size': 23, 'step_size': 13, 'gamma': 0.8831446496115087}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:35:16,280][0m Trial 32 finished with value: 0.3484047129430484 and parameters: {'observation_period_num': 145, 'train_rates': 0.9179050920933253, 'learning_rate': 0.000427114472542179, 'batch_size': 52, 'step_size': 13, 'gamma': 0.8427060077817466}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:37:31,222][0m Trial 33 finished with value: 0.20167835266657277 and parameters: {'observation_period_num': 100, 'train_rates': 0.926068472493072, 'learning_rate': 9.186912261111585e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9195594493390997}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:41:24,713][0m Trial 34 finished with value: 0.18374305963516235 and parameters: {'observation_period_num': 159, 'train_rates': 0.9613038554238714, 'learning_rate': 0.00019451753121638878, 'batch_size': 145, 'step_size': 14, 'gamma': 0.8607475597062579}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:45:59,456][0m Trial 35 finished with value: 0.22833226056216838 and parameters: {'observation_period_num': 193, 'train_rates': 0.8691078476877161, 'learning_rate': 0.0005279846406363564, 'batch_size': 174, 'step_size': 12, 'gamma': 0.8099840649141189}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:48:34,053][0m Trial 36 finished with value: 0.43103157770382006 and parameters: {'observation_period_num': 126, 'train_rates': 0.8168806454046397, 'learning_rate': 0.00032050953632847503, 'batch_size': 138, 'step_size': 14, 'gamma': 0.9894297691224646}. Best is trial 8 with value: 0.1303865055864056.[0m
[32m[I 2025-01-08 04:53:00,060][0m Trial 37 finished with value: 0.09331005997955799 and parameters: {'observation_period_num': 152, 'train_rates': 0.9873252024326341, 'learning_rate': 0.00016442124044266365, 'batch_size': 18, 'step_size': 13, 'gamma': 0.9463415063255836}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 04:58:00,601][0m Trial 38 finished with value: 0.09933154029505593 and parameters: {'observation_period_num': 169, 'train_rates': 0.987807602660063, 'learning_rate': 7.310226077650519e-05, 'batch_size': 16, 'step_size': 13, 'gamma': 0.9493714100659225}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:03:20,380][0m Trial 39 finished with value: 0.09758658300746571 and parameters: {'observation_period_num': 181, 'train_rates': 0.9884798180124861, 'learning_rate': 2.260365073428808e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9511630148605182}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:08:03,038][0m Trial 40 finished with value: 0.14248239142554148 and parameters: {'observation_period_num': 177, 'train_rates': 0.9825408685551588, 'learning_rate': 1.3177862242527558e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9487280799739928}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:13:00,194][0m Trial 41 finished with value: 0.11794423573725932 and parameters: {'observation_period_num': 186, 'train_rates': 0.9869874562943031, 'learning_rate': 2.326123631875591e-05, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9514485531577379}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:18:35,704][0m Trial 42 finished with value: 0.1238590677579244 and parameters: {'observation_period_num': 204, 'train_rates': 0.9686319791251057, 'learning_rate': 2.5717555107955707e-05, 'batch_size': 29, 'step_size': 14, 'gamma': 0.9731044980128043}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:24:00,326][0m Trial 43 finished with value: 0.13989402905393106 and parameters: {'observation_period_num': 201, 'train_rates': 0.9625755945379317, 'learning_rate': 2.2625406453507283e-05, 'batch_size': 45, 'step_size': 15, 'gamma': 0.9703332296182534}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:31:03,435][0m Trial 44 finished with value: 0.1027596367256982 and parameters: {'observation_period_num': 236, 'train_rates': 0.987406278844962, 'learning_rate': 1.8346290077653877e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9526948339889086}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:38:13,292][0m Trial 45 finished with value: 0.12613928743771144 and parameters: {'observation_period_num': 241, 'train_rates': 0.9846550982412053, 'learning_rate': 9.35111205011454e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9463423774192972}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:42:05,881][0m Trial 46 finished with value: 0.21037593483924866 and parameters: {'observation_period_num': 150, 'train_rates': 0.9888156542014415, 'learning_rate': 1.0367974795903459e-05, 'batch_size': 63, 'step_size': 14, 'gamma': 0.9273756848165897}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:46:49,682][0m Trial 47 finished with value: 0.2022698238492012 and parameters: {'observation_period_num': 186, 'train_rates': 0.9317932073807067, 'learning_rate': 1.5117022067296154e-05, 'batch_size': 38, 'step_size': 13, 'gamma': 0.9493495259421778}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:53:41,530][0m Trial 48 finished with value: 0.22034886185552033 and parameters: {'observation_period_num': 234, 'train_rates': 0.9483334405275149, 'learning_rate': 5.075965555236785e-06, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9663600437644784}. Best is trial 37 with value: 0.09331005997955799.[0m
[32m[I 2025-01-08 05:58:34,331][0m Trial 49 finished with value: 0.1514624740396227 and parameters: {'observation_period_num': 188, 'train_rates': 0.9700393700164576, 'learning_rate': 7.769244503239195e-05, 'batch_size': 52, 'step_size': 14, 'gamma': 0.9797260482882361}. Best is trial 37 with value: 0.09331005997955799.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.3638 | 0.3686
Epoch 2/300, Loss: 0.3043 | 0.2646
Epoch 3/300, Loss: 0.3231 | 0.3805
Epoch 4/300, Loss: 0.3131 | 0.2345
Epoch 5/300, Loss: 0.2816 | 0.2694
Epoch 6/300, Loss: 0.2466 | 0.2739
Epoch 7/300, Loss: 0.2425 | 0.2751
Epoch 8/300, Loss: 0.2353 | 0.2793
Epoch 9/300, Loss: 0.2367 | 0.2611
Epoch 10/300, Loss: 0.2492 | 0.3314
Epoch 11/300, Loss: 0.2208 | 0.2994
Epoch 12/300, Loss: 0.2301 | 0.3052
Epoch 13/300, Loss: 0.2418 | 0.3353
Epoch 14/300, Loss: 0.2266 | 0.2627
Epoch 15/300, Loss: 0.2109 | 0.2237
Epoch 16/300, Loss: 0.1890 | 0.2333
Epoch 17/300, Loss: 0.1899 | 0.3318
Epoch 18/300, Loss: 0.1875 | 0.2451
Epoch 19/300, Loss: 0.1909 | 0.2471
Epoch 20/300, Loss: 0.1681 | 0.1834
Epoch 21/300, Loss: 0.1636 | 0.2029
Epoch 22/300, Loss: 0.1654 | 0.2598
Epoch 23/300, Loss: 0.1752 | 0.2124
Epoch 24/300, Loss: 0.1805 | 0.1910
Epoch 25/300, Loss: 0.1750 | 0.1934
Epoch 26/300, Loss: 0.1683 | 0.2479
Epoch 27/300, Loss: 0.1620 | 0.1947
Epoch 28/300, Loss: 0.1566 | 0.1705
Epoch 29/300, Loss: 0.1506 | 0.1797
Epoch 30/300, Loss: 0.1476 | 0.3046
Epoch 31/300, Loss: 0.1480 | 0.1792
Epoch 32/300, Loss: 0.1472 | 0.1896
Epoch 33/300, Loss: 0.1449 | 0.1802
Epoch 34/300, Loss: 0.1373 | 0.1689
Epoch 35/300, Loss: 0.1421 | 0.1695
Epoch 36/300, Loss: 0.1393 | 0.1815
Epoch 37/300, Loss: 0.1321 | 0.2805
Epoch 38/300, Loss: 0.1278 | 0.1781
Epoch 39/300, Loss: 0.1315 | 0.1625
Epoch 40/300, Loss: 0.1283 | 0.2553
Epoch 41/300, Loss: 0.1273 | 0.1582
Epoch 42/300, Loss: 0.1263 | 0.1668
Epoch 43/300, Loss: 0.1229 | 0.1622
Epoch 44/300, Loss: 0.1196 | 0.2086
Epoch 45/300, Loss: 0.1216 | 0.2269
Epoch 46/300, Loss: 0.1173 | 0.2165
Epoch 47/300, Loss: 0.1097 | 0.1504
Epoch 48/300, Loss: 0.1047 | 0.1646
Epoch 49/300, Loss: 0.1045 | 0.1644
Epoch 50/300, Loss: 0.1026 | 0.1763
Epoch 51/300, Loss: 0.1045 | 0.1405
Epoch 52/300, Loss: 0.1057 | 0.1646
Epoch 53/300, Loss: 0.1048 | 0.1706
Epoch 54/300, Loss: 0.1024 | 0.1758
Epoch 55/300, Loss: 0.1015 | 0.2276
Epoch 56/300, Loss: 0.1038 | 0.1630
Epoch 57/300, Loss: 0.0998 | 0.1385
Epoch 58/300, Loss: 0.0983 | 0.1754
Epoch 59/300, Loss: 0.0996 | 0.1481
Epoch 60/300, Loss: 0.0996 | 0.1473
Epoch 61/300, Loss: 0.0999 | 0.1674
Epoch 62/300, Loss: 0.1056 | 0.1471
Epoch 63/300, Loss: 0.1049 | 0.1422
Epoch 64/300, Loss: 0.0930 | 0.1395
Epoch 65/300, Loss: 0.0906 | 0.1322
Epoch 66/300, Loss: 0.0980 | 0.1545
Epoch 67/300, Loss: 0.0954 | 0.1780
Epoch 68/300, Loss: 0.0871 | 0.1205
Epoch 69/300, Loss: 0.0861 | 0.1540
Epoch 70/300, Loss: 0.0855 | 0.1546
Epoch 71/300, Loss: 0.0815 | 0.1307
Epoch 72/300, Loss: 0.0792 | 0.1596
Epoch 73/300, Loss: 0.0794 | 0.1326
Epoch 74/300, Loss: 0.0810 | 0.1463
Epoch 75/300, Loss: 0.0920 | 0.1364
Epoch 76/300, Loss: 0.0829 | 0.1231
Epoch 77/300, Loss: 0.0773 | 0.1442
Epoch 78/300, Loss: 0.0815 | 0.1284
Epoch 79/300, Loss: 0.0813 | 0.1688
Epoch 80/300, Loss: 0.0785 | 0.1119
Epoch 81/300, Loss: 0.0786 | 0.1253
Epoch 82/300, Loss: 0.0738 | 0.1400
Epoch 83/300, Loss: 0.0752 | 0.1307
Epoch 84/300, Loss: 0.0723 | 0.1088
Epoch 85/300, Loss: 0.0697 | 0.1238
Epoch 86/300, Loss: 0.0685 | 0.1518
Epoch 87/300, Loss: 0.0660 | 0.1249
Epoch 88/300, Loss: 0.0700 | 0.1112
Epoch 89/300, Loss: 0.0708 | 0.1233
Epoch 90/300, Loss: 0.0705 | 0.1527
Epoch 91/300, Loss: 0.0680 | 0.1415
Epoch 92/300, Loss: 0.0701 | 0.1155
Epoch 93/300, Loss: 0.0766 | 0.1310
Epoch 94/300, Loss: 0.0675 | 0.1556
Epoch 95/300, Loss: 0.0663 | 0.1209
Epoch 96/300, Loss: 0.0638 | 0.1034
Epoch 97/300, Loss: 0.0632 | 0.1261
Epoch 98/300, Loss: 0.0685 | 0.1575
Epoch 99/300, Loss: 0.0638 | 0.1188
Epoch 100/300, Loss: 0.0659 | 0.1049
Epoch 101/300, Loss: 0.0598 | 0.1181
Epoch 102/300, Loss: 0.0586 | 0.1255
Epoch 103/300, Loss: 0.0602 | 0.1151
Epoch 104/300, Loss: 0.0627 | 0.1220
Epoch 105/300, Loss: 0.0607 | 0.1184
Epoch 106/300, Loss: 0.0591 | 0.1118
Epoch 107/300, Loss: 0.0571 | 0.1356
Epoch 108/300, Loss: 0.0636 | 0.1246
Epoch 109/300, Loss: 0.0562 | 0.1033
Epoch 110/300, Loss: 0.0545 | 0.1082
Epoch 111/300, Loss: 0.0584 | 0.1397
Epoch 112/300, Loss: 0.0614 | 0.1295
Epoch 113/300, Loss: 0.0620 | 0.0977
Epoch 114/300, Loss: 0.0586 | 0.1190
Epoch 115/300, Loss: 0.0567 | 0.1294
Epoch 116/300, Loss: 0.0613 | 0.1183
Epoch 117/300, Loss: 0.0584 | 0.1038
Epoch 118/300, Loss: 0.0544 | 0.1237
Epoch 119/300, Loss: 0.0534 | 0.1138
Epoch 120/300, Loss: 0.0508 | 0.1140
Epoch 121/300, Loss: 0.0494 | 0.1068
Epoch 122/300, Loss: 0.0474 | 0.1116
Epoch 123/300, Loss: 0.0480 | 0.1145
Epoch 124/300, Loss: 0.0495 | 0.1126
Epoch 125/300, Loss: 0.0563 | 0.1097
Epoch 126/300, Loss: 0.0483 | 0.1069
Epoch 127/300, Loss: 0.0460 | 0.1053
Epoch 128/300, Loss: 0.0462 | 0.1187
Epoch 129/300, Loss: 0.0456 | 0.1032
Epoch 130/300, Loss: 0.0452 | 0.1015
Epoch 131/300, Loss: 0.0456 | 0.1144
Epoch 132/300, Loss: 0.0451 | 0.1381
Epoch 133/300, Loss: 0.0466 | 0.1092
Epoch 134/300, Loss: 0.0470 | 0.1115
Epoch 135/300, Loss: 0.0466 | 0.1167
Epoch 136/300, Loss: 0.0494 | 0.1281
Epoch 137/300, Loss: 0.0557 | 0.1049
Epoch 138/300, Loss: 0.0492 | 0.1159
Epoch 139/300, Loss: 0.0467 | 0.1209
Epoch 140/300, Loss: 0.0445 | 0.1162
Epoch 141/300, Loss: 0.0444 | 0.1076
Epoch 142/300, Loss: 0.0514 | 0.1141
Epoch 143/300, Loss: 0.0450 | 0.1145
Epoch 144/300, Loss: 0.0432 | 0.1133
Epoch 145/300, Loss: 0.0422 | 0.1038
Epoch 146/300, Loss: 0.0410 | 0.1108
Epoch 147/300, Loss: 0.0400 | 0.1085
Epoch 148/300, Loss: 0.0385 | 0.1175
Epoch 149/300, Loss: 0.0384 | 0.1056
Epoch 150/300, Loss: 0.0391 | 0.1075
Epoch 151/300, Loss: 0.0503 | 0.1095
Epoch 152/300, Loss: 0.0420 | 0.1173
Epoch 153/300, Loss: 0.0419 | 0.1199
Epoch 154/300, Loss: 0.0428 | 0.1062
Epoch 155/300, Loss: 0.0420 | 0.1052
Epoch 156/300, Loss: 0.0402 | 0.1157
Epoch 157/300, Loss: 0.0398 | 0.1155
Epoch 158/300, Loss: 0.0403 | 0.1015
Epoch 159/300, Loss: 0.0411 | 0.1126
Epoch 160/300, Loss: 0.0385 | 0.1187
Epoch 161/300, Loss: 0.0395 | 0.1084
Epoch 162/300, Loss: 0.0380 | 0.1047
Epoch 163/300, Loss: 0.0382 | 0.1162
Epoch 164/300, Loss: 0.0437 | 0.1114
Epoch 165/300, Loss: 0.0389 | 0.1038
Epoch 166/300, Loss: 0.0379 | 0.1056
Epoch 167/300, Loss: 0.0412 | 0.1106
Epoch 168/300, Loss: 0.0383 | 0.1092
Epoch 169/300, Loss: 0.0370 | 0.1017
Epoch 170/300, Loss: 0.0367 | 0.1079
Epoch 171/300, Loss: 0.0349 | 0.1064
Epoch 172/300, Loss: 0.0353 | 0.1068
Epoch 173/300, Loss: 0.0342 | 0.1077
Epoch 174/300, Loss: 0.0336 | 0.1077
Epoch 175/300, Loss: 0.0324 | 0.1071
Epoch 176/300, Loss: 0.0318 | 0.1121
Epoch 177/300, Loss: 0.0316 | 0.1073
Epoch 178/300, Loss: 0.0344 | 0.1073
Epoch 179/300, Loss: 0.0338 | 0.1065
Epoch 180/300, Loss: 0.0331 | 0.1129
Epoch 181/300, Loss: 0.0331 | 0.1136
Epoch 182/300, Loss: 0.0320 | 0.0995
Epoch 183/300, Loss: 0.0319 | 0.1104
Epoch 184/300, Loss: 0.0312 | 0.1128
Epoch 185/300, Loss: 0.0311 | 0.1116
Epoch 186/300, Loss: 0.0309 | 0.1041
Epoch 187/300, Loss: 0.0308 | 0.1071
Epoch 188/300, Loss: 0.0325 | 0.1130
Epoch 189/300, Loss: 0.0336 | 0.1188
Epoch 190/300, Loss: 0.0339 | 0.1056
Epoch 191/300, Loss: 0.0329 | 0.1048
Epoch 192/300, Loss: 0.0324 | 0.1090
Epoch 193/300, Loss: 0.0305 | 0.1059
Epoch 194/300, Loss: 0.0300 | 0.1046
Epoch 195/300, Loss: 0.0294 | 0.1066
Epoch 196/300, Loss: 0.0288 | 0.1103
Epoch 197/300, Loss: 0.0300 | 0.1078
Epoch 198/300, Loss: 0.0352 | 0.1070
Epoch 199/300, Loss: 0.0340 | 0.1014
Epoch 200/300, Loss: 0.0292 | 0.1110
Epoch 201/300, Loss: 0.0299 | 0.1103
Epoch 202/300, Loss: 0.0275 | 0.1027
Epoch 203/300, Loss: 0.0277 | 0.1091
Epoch 204/300, Loss: 0.0273 | 0.1089
Epoch 205/300, Loss: 0.0271 | 0.1052
Epoch 206/300, Loss: 0.0269 | 0.1048
Epoch 207/300, Loss: 0.0273 | 0.1089
Epoch 208/300, Loss: 0.0272 | 0.1115
Epoch 209/300, Loss: 0.0274 | 0.1075
Epoch 210/300, Loss: 0.0271 | 0.1007
Epoch 211/300, Loss: 0.0270 | 0.1058
Epoch 212/300, Loss: 0.0274 | 0.1120
Epoch 213/300, Loss: 0.0265 | 0.1083
Epoch 214/300, Loss: 0.0262 | 0.1016
Epoch 215/300, Loss: 0.0258 | 0.1034
Epoch 216/300, Loss: 0.0261 | 0.1098
Epoch 217/300, Loss: 0.0260 | 0.1069
Epoch 218/300, Loss: 0.0260 | 0.1030
Epoch 219/300, Loss: 0.0258 | 0.1077
Epoch 220/300, Loss: 0.0253 | 0.1075
Epoch 221/300, Loss: 0.0253 | 0.1036
Epoch 222/300, Loss: 0.0250 | 0.1037
Epoch 223/300, Loss: 0.0251 | 0.1044
Epoch 224/300, Loss: 0.0251 | 0.1055
Epoch 225/300, Loss: 0.0250 | 0.1014
Epoch 226/300, Loss: 0.0243 | 0.1022
Epoch 227/300, Loss: 0.0243 | 0.1024
Epoch 228/300, Loss: 0.0242 | 0.1064
Epoch 229/300, Loss: 0.0277 | 0.1039
Epoch 230/300, Loss: 0.0240 | 0.1033
Epoch 231/300, Loss: 0.0238 | 0.1048
Epoch 232/300, Loss: 0.0237 | 0.1061
Epoch 233/300, Loss: 0.0237 | 0.1068
Epoch 234/300, Loss: 0.0236 | 0.1046
Epoch 235/300, Loss: 0.0231 | 0.1024
Epoch 236/300, Loss: 0.0231 | 0.1031
Epoch 237/300, Loss: 0.0232 | 0.1048
Epoch 238/300, Loss: 0.0231 | 0.1042
Epoch 239/300, Loss: 0.0232 | 0.1025
Epoch 240/300, Loss: 0.0229 | 0.1017
Epoch 241/300, Loss: 0.0227 | 0.1009
Epoch 242/300, Loss: 0.0243 | 0.1011
Epoch 243/300, Loss: 0.0292 | 0.0977
Epoch 244/300, Loss: 0.0264 | 0.1037
Epoch 245/300, Loss: 0.0245 | 0.1012
Epoch 246/300, Loss: 0.0219 | 0.0999
Epoch 247/300, Loss: 0.0216 | 0.1003
Epoch 248/300, Loss: 0.0217 | 0.1011
Epoch 249/300, Loss: 0.0219 | 0.1010
Epoch 250/300, Loss: 0.0217 | 0.1000
Epoch 251/300, Loss: 0.0213 | 0.1004
Epoch 252/300, Loss: 0.0215 | 0.1003
Epoch 253/300, Loss: 0.0216 | 0.1029
Epoch 254/300, Loss: 0.0214 | 0.1035
Epoch 255/300, Loss: 0.0213 | 0.0962
Epoch 256/300, Loss: 0.0213 | 0.0989
Epoch 257/300, Loss: 0.0211 | 0.0991
Epoch 258/300, Loss: 0.0210 | 0.1036
Epoch 259/300, Loss: 0.0214 | 0.0965
Epoch 260/300, Loss: 0.0209 | 0.0999
Epoch 261/300, Loss: 0.0213 | 0.1010
Epoch 262/300, Loss: 0.0209 | 0.1016
Epoch 263/300, Loss: 0.0208 | 0.0998
Epoch 264/300, Loss: 0.0209 | 0.0980
Epoch 265/300, Loss: 0.0204 | 0.0989
Epoch 266/300, Loss: 0.0208 | 0.1032
Epoch 267/300, Loss: 0.0206 | 0.0987
Epoch 268/300, Loss: 0.0203 | 0.1020
Epoch 269/300, Loss: 0.0206 | 0.1017
Epoch 270/300, Loss: 0.0204 | 0.1022
Epoch 271/300, Loss: 0.0204 | 0.1008
Epoch 272/300, Loss: 0.0202 | 0.1019
Epoch 273/300, Loss: 0.0201 | 0.1024
Epoch 274/300, Loss: 0.0203 | 0.1021
Epoch 275/300, Loss: 0.0201 | 0.1013
Epoch 276/300, Loss: 0.0199 | 0.0989
Epoch 277/300, Loss: 0.0201 | 0.1000
Epoch 278/300, Loss: 0.0201 | 0.1007
Epoch 279/300, Loss: 0.0195 | 0.1006
Epoch 280/300, Loss: 0.0201 | 0.0999
Epoch 281/300, Loss: 0.0195 | 0.0999
Epoch 282/300, Loss: 0.0197 | 0.1053
Epoch 283/300, Loss: 0.0194 | 0.1009
Epoch 284/300, Loss: 0.0194 | 0.1039
Epoch 285/300, Loss: 0.0191 | 0.1032
Epoch 286/300, Loss: 0.0195 | 0.1049
Epoch 287/300, Loss: 0.0193 | 0.1026
Epoch 288/300, Loss: 0.0192 | 0.1034
Epoch 289/300, Loss: 0.0191 | 0.1019
Epoch 290/300, Loss: 0.0190 | 0.1024
Epoch 291/300, Loss: 0.0191 | 0.1006
Epoch 292/300, Loss: 0.0190 | 0.1003
Epoch 293/300, Loss: 0.0187 | 0.1003
Epoch 294/300, Loss: 0.0187 | 0.1006
Epoch 295/300, Loss: 0.0187 | 0.0978
Epoch 296/300, Loss: 0.0186 | 0.0996
Epoch 297/300, Loss: 0.0186 | 0.1004
Epoch 298/300, Loss: 0.0187 | 0.0992
Epoch 299/300, Loss: 0.0186 | 0.1005
Epoch 300/300, Loss: 0.0184 | 0.0998
Runtime (seconds): 797.8440623283386
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1066.7137143053114
RMSE: 32.66058349609375
MAE: 32.66058349609375
R-squared: nan
[225.53943]
