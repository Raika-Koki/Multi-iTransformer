ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 02:00:56,798][0m A new study created in memory with name: no-name-47243010-ad0c-49b9-a2f5-dc3afc8a1d1f[0m
[32m[I 2025-01-03 02:02:50,086][0m Trial 0 finished with value: 1.399693180214275 and parameters: {'observation_period_num': 96, 'train_rates': 0.6223440941958583, 'learning_rate': 0.0005406911537289773, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9583990324892326}. Best is trial 0 with value: 1.399693180214275.[0m
[32m[I 2025-01-03 02:07:33,710][0m Trial 1 finished with value: 1.0699091386864554 and parameters: {'observation_period_num': 212, 'train_rates': 0.7495348069729935, 'learning_rate': 1.2955611842083592e-05, 'batch_size': 254, 'step_size': 9, 'gamma': 0.8934342502030952}. Best is trial 1 with value: 1.0699091386864554.[0m
[32m[I 2025-01-03 02:11:20,171][0m Trial 2 finished with value: 0.7445764653764817 and parameters: {'observation_period_num': 182, 'train_rates': 0.7168862140573247, 'learning_rate': 2.670651155529881e-05, 'batch_size': 90, 'step_size': 13, 'gamma': 0.8470705256306676}. Best is trial 2 with value: 0.7445764653764817.[0m
[32m[I 2025-01-03 02:12:29,029][0m Trial 3 finished with value: 0.1856834590435028 and parameters: {'observation_period_num': 51, 'train_rates': 0.9623193077625825, 'learning_rate': 7.825855762956138e-05, 'batch_size': 207, 'step_size': 5, 'gamma': 0.8256009790499736}. Best is trial 3 with value: 0.1856834590435028.[0m
[32m[I 2025-01-03 02:15:28,427][0m Trial 4 finished with value: 0.11817803233861923 and parameters: {'observation_period_num': 127, 'train_rates': 0.9786565365924293, 'learning_rate': 9.788171219511917e-05, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8450245321913493}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:16:19,632][0m Trial 5 finished with value: 0.38781291246414185 and parameters: {'observation_period_num': 35, 'train_rates': 0.9868217955872756, 'learning_rate': 7.048068440926991e-06, 'batch_size': 239, 'step_size': 13, 'gamma': 0.859755732464992}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:21:15,849][0m Trial 6 finished with value: 0.13351383805274963 and parameters: {'observation_period_num': 191, 'train_rates': 0.9896439857981894, 'learning_rate': 0.000172804962134651, 'batch_size': 123, 'step_size': 7, 'gamma': 0.7522882010178776}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:26:50,778][0m Trial 7 finished with value: 0.4587262272834778 and parameters: {'observation_period_num': 212, 'train_rates': 0.9409263171660345, 'learning_rate': 1.8626930697189645e-06, 'batch_size': 54, 'step_size': 10, 'gamma': 0.9119197429828989}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:31:58,354][0m Trial 8 finished with value: 1.110757819196796 and parameters: {'observation_period_num': 221, 'train_rates': 0.801136574570315, 'learning_rate': 1.4308975181694602e-06, 'batch_size': 188, 'step_size': 8, 'gamma': 0.9572986905989891}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:35:26,880][0m Trial 9 finished with value: 1.4146570175344293 and parameters: {'observation_period_num': 159, 'train_rates': 0.8031305346534181, 'learning_rate': 1.0267280054291383e-06, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8877937352896268}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:37:48,740][0m Trial 10 finished with value: 0.9125491878291866 and parameters: {'observation_period_num': 108, 'train_rates': 0.882944548493924, 'learning_rate': 0.000831418850947676, 'batch_size': 152, 'step_size': 3, 'gamma': 0.7947154663996964}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:41:05,805][0m Trial 11 finished with value: 0.213454850998364 and parameters: {'observation_period_num': 139, 'train_rates': 0.892134223147564, 'learning_rate': 0.0001175218772406514, 'batch_size': 118, 'step_size': 6, 'gamma': 0.7602146568878956}. Best is trial 4 with value: 0.11817803233861923.[0m
Early stopping at epoch 97
[32m[I 2025-01-03 02:47:21,781][0m Trial 12 finished with value: 0.2970399574056409 and parameters: {'observation_period_num': 252, 'train_rates': 0.8827181532058389, 'learning_rate': 0.0002611740231687018, 'batch_size': 145, 'step_size': 2, 'gamma': 0.7593501545960102}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:49:09,799][0m Trial 13 finished with value: 0.15280069519057232 and parameters: {'observation_period_num': 77, 'train_rates': 0.9188658801308561, 'learning_rate': 0.00010476768234778175, 'batch_size': 103, 'step_size': 7, 'gamma': 0.802899566560014}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:53:18,219][0m Trial 14 finished with value: 0.12237334996461868 and parameters: {'observation_period_num': 163, 'train_rates': 0.9893086367119371, 'learning_rate': 0.00025765039074913027, 'batch_size': 169, 'step_size': 11, 'gamma': 0.7931661105289702}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:56:00,810][0m Trial 15 finished with value: 0.3700913115153237 and parameters: {'observation_period_num': 128, 'train_rates': 0.8440395823503685, 'learning_rate': 5.394239254990803e-05, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8008889845336421}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 02:56:20,523][0m Trial 16 finished with value: 0.8833585892479391 and parameters: {'observation_period_num': 6, 'train_rates': 0.6121604478939525, 'learning_rate': 0.0004362879052497734, 'batch_size': 178, 'step_size': 15, 'gamma': 0.8256270589306449}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:00:22,136][0m Trial 17 finished with value: 0.16252308844321936 and parameters: {'observation_period_num': 164, 'train_rates': 0.9288504093724989, 'learning_rate': 3.268753353555553e-05, 'batch_size': 72, 'step_size': 11, 'gamma': 0.9209186912736744}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:02:56,884][0m Trial 18 finished with value: 0.7463927132430628 and parameters: {'observation_period_num': 130, 'train_rates': 0.6907774221537777, 'learning_rate': 0.0002723887447218623, 'batch_size': 223, 'step_size': 13, 'gamma': 0.836115407623373}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:04:42,113][0m Trial 19 finished with value: 0.70028488619724 and parameters: {'observation_period_num': 82, 'train_rates': 0.8470441510222624, 'learning_rate': 6.436445758931823e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.7838166043014887}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:08:35,057][0m Trial 20 finished with value: 0.1441219598054886 and parameters: {'observation_period_num': 157, 'train_rates': 0.9552307141750987, 'learning_rate': 4.7154096711275005e-05, 'batch_size': 195, 'step_size': 14, 'gamma': 0.9875904928915762}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:13:29,923][0m Trial 21 finished with value: 0.14294733107089996 and parameters: {'observation_period_num': 192, 'train_rates': 0.9898669485640712, 'learning_rate': 0.0001973397770233779, 'batch_size': 127, 'step_size': 8, 'gamma': 0.7512182380009392}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:18:14,661][0m Trial 22 finished with value: 0.16479168832302094 and parameters: {'observation_period_num': 184, 'train_rates': 0.9859750629770232, 'learning_rate': 0.00016365880910474933, 'batch_size': 127, 'step_size': 4, 'gamma': 0.7775999209753315}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:24:35,559][0m Trial 23 finished with value: 0.18925037829328603 and parameters: {'observation_period_num': 246, 'train_rates': 0.9050629563234797, 'learning_rate': 0.0003824005525884946, 'batch_size': 99, 'step_size': 11, 'gamma': 0.8121078487529507}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:28:10,159][0m Trial 24 finished with value: 0.4473001956939697 and parameters: {'observation_period_num': 145, 'train_rates': 0.9581563476449643, 'learning_rate': 0.0008495880876224116, 'batch_size': 162, 'step_size': 9, 'gamma': 0.7797778908804215}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:30:34,310][0m Trial 25 finished with value: 0.31288663724168614 and parameters: {'observation_period_num': 111, 'train_rates': 0.8586834033354922, 'learning_rate': 0.0001245930987262864, 'batch_size': 140, 'step_size': 6, 'gamma': 0.8635347992417562}. Best is trial 4 with value: 0.11817803233861923.[0m
Early stopping at epoch 62
[32m[I 2025-01-03 03:33:18,858][0m Trial 26 finished with value: 0.39754045118836207 and parameters: {'observation_period_num': 177, 'train_rates': 0.931200955590448, 'learning_rate': 6.76091314212069e-05, 'batch_size': 113, 'step_size': 1, 'gamma': 0.8159298293672443}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:38:36,421][0m Trial 27 finished with value: 0.1842463573282079 and parameters: {'observation_period_num': 201, 'train_rates': 0.9533714482399216, 'learning_rate': 1.5932617722278154e-05, 'batch_size': 76, 'step_size': 12, 'gamma': 0.7695134509286637}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:44:43,518][0m Trial 28 finished with value: 0.11948574334383011 and parameters: {'observation_period_num': 229, 'train_rates': 0.969338664568007, 'learning_rate': 0.0001879621766948129, 'batch_size': 175, 'step_size': 9, 'gamma': 0.8403749046366176}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:49:55,584][0m Trial 29 finished with value: 0.7491126797482993 and parameters: {'observation_period_num': 232, 'train_rates': 0.7622213480985206, 'learning_rate': 0.0005872886421744911, 'batch_size': 183, 'step_size': 15, 'gamma': 0.8436221796421556}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:51:41,091][0m Trial 30 finished with value: 0.8529906739150325 and parameters: {'observation_period_num': 97, 'train_rates': 0.6525856684151986, 'learning_rate': 0.0002957153021868494, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8765255421972619}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 03:58:03,257][0m Trial 31 finished with value: 0.13785779476165771 and parameters: {'observation_period_num': 235, 'train_rates': 0.9710510192593587, 'learning_rate': 0.00017546843607095446, 'batch_size': 137, 'step_size': 7, 'gamma': 0.7912956716860452}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:02:59,959][0m Trial 32 finished with value: 0.16676830757280875 and parameters: {'observation_period_num': 198, 'train_rates': 0.9160313811042318, 'learning_rate': 8.982806942181092e-05, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8548324840736826}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:07:17,970][0m Trial 33 finished with value: 0.2427319437265396 and parameters: {'observation_period_num': 171, 'train_rates': 0.9682663721935294, 'learning_rate': 0.0005527000223972229, 'batch_size': 149, 'step_size': 12, 'gamma': 0.8790117916366501}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:12:49,460][0m Trial 34 finished with value: 0.23737703263759613 and parameters: {'observation_period_num': 212, 'train_rates': 0.9400749578696079, 'learning_rate': 3.3667742783119606e-05, 'batch_size': 199, 'step_size': 9, 'gamma': 0.8327994765381946}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:16:34,771][0m Trial 35 finished with value: 0.1256009191274643 and parameters: {'observation_period_num': 149, 'train_rates': 0.987000348978449, 'learning_rate': 0.0001514727287907261, 'batch_size': 231, 'step_size': 14, 'gamma': 0.9072053870155933}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:20:18,426][0m Trial 36 finished with value: 0.15267930924892426 and parameters: {'observation_period_num': 149, 'train_rates': 0.9688420747751486, 'learning_rate': 7.058060794357362e-05, 'batch_size': 256, 'step_size': 14, 'gamma': 0.903058310724655}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:22:41,179][0m Trial 37 finished with value: 0.4527364027690128 and parameters: {'observation_period_num': 114, 'train_rates': 0.8231267877864994, 'learning_rate': 1.9526208293701524e-05, 'batch_size': 226, 'step_size': 14, 'gamma': 0.9340655654208421}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:24:10,678][0m Trial 38 finished with value: 0.7424107597053395 and parameters: {'observation_period_num': 74, 'train_rates': 0.7594631964489772, 'learning_rate': 0.00023907533625638019, 'batch_size': 230, 'step_size': 13, 'gamma': 0.8963503241429878}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:27:25,787][0m Trial 39 finished with value: 0.15507016249241368 and parameters: {'observation_period_num': 125, 'train_rates': 0.9453141407887151, 'learning_rate': 0.00013135699061021268, 'batch_size': 33, 'step_size': 12, 'gamma': 0.9356690931040713}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:28:25,967][0m Trial 40 finished with value: 0.20518946745610156 and parameters: {'observation_period_num': 46, 'train_rates': 0.9005856924044873, 'learning_rate': 4.793190446717119e-05, 'batch_size': 247, 'step_size': 10, 'gamma': 0.8455284914148489}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:34:22,464][0m Trial 41 finished with value: 0.11928068846464157 and parameters: {'observation_period_num': 221, 'train_rates': 0.9839894872781374, 'learning_rate': 0.0004098727513330001, 'batch_size': 167, 'step_size': 8, 'gamma': 0.8137406766153364}. Best is trial 4 with value: 0.11817803233861923.[0m
[32m[I 2025-01-03 04:40:11,139][0m Trial 42 finished with value: 0.10971615463495255 and parameters: {'observation_period_num': 219, 'train_rates': 0.9763868659433601, 'learning_rate': 0.0003981821204048228, 'batch_size': 216, 'step_size': 13, 'gamma': 0.8211817606098862}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 04:46:17,562][0m Trial 43 finished with value: 0.12221898883581161 and parameters: {'observation_period_num': 227, 'train_rates': 0.9702849382391954, 'learning_rate': 0.00036404970063652853, 'batch_size': 168, 'step_size': 8, 'gamma': 0.8144696841300281}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 04:52:11,647][0m Trial 44 finished with value: 0.13993635773658752 and parameters: {'observation_period_num': 224, 'train_rates': 0.969292776550244, 'learning_rate': 0.00040014393134597364, 'batch_size': 189, 'step_size': 6, 'gamma': 0.8184854143112881}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 04:57:27,977][0m Trial 45 finished with value: 0.14833995699882507 and parameters: {'observation_period_num': 206, 'train_rates': 0.9209988097510603, 'learning_rate': 0.0006349267785273213, 'batch_size': 218, 'step_size': 8, 'gamma': 0.8542789325207866}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 05:02:53,379][0m Trial 46 finished with value: 0.6449530402482566 and parameters: {'observation_period_num': 221, 'train_rates': 0.87045450252059, 'learning_rate': 0.0009240032860831379, 'batch_size': 159, 'step_size': 5, 'gamma': 0.807744506452146}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 05:09:11,042][0m Trial 47 finished with value: 0.13365548849105835 and parameters: {'observation_period_num': 239, 'train_rates': 0.9413167410671468, 'learning_rate': 0.0003827379380635726, 'batch_size': 201, 'step_size': 7, 'gamma': 0.8267115503768085}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 05:15:40,992][0m Trial 48 finished with value: 0.19563092345871577 and parameters: {'observation_period_num': 251, 'train_rates': 0.9091896458808899, 'learning_rate': 0.0007029436159255775, 'batch_size': 179, 'step_size': 8, 'gamma': 0.8372593803523257}. Best is trial 42 with value: 0.10971615463495255.[0m
[32m[I 2025-01-03 05:21:33,765][0m Trial 49 finished with value: 0.1399574875831604 and parameters: {'observation_period_num': 221, 'train_rates': 0.9710522963435054, 'learning_rate': 0.0004864304622387312, 'batch_size': 214, 'step_size': 9, 'gamma': 0.862154053089785}. Best is trial 42 with value: 0.10971615463495255.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 05:21:33,771][0m A new study created in memory with name: no-name-5bb8bff2-fbea-4d30-9f47-ac214986dfc0[0m
[32m[I 2025-01-03 05:27:03,313][0m Trial 0 finished with value: 0.13486211001873016 and parameters: {'observation_period_num': 207, 'train_rates': 0.9830377482033366, 'learning_rate': 0.0001310089916565096, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9490449743810146}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:28:59,858][0m Trial 1 finished with value: 0.43708583430498227 and parameters: {'observation_period_num': 78, 'train_rates': 0.9156527675021003, 'learning_rate': 7.4274648511820055e-06, 'batch_size': 53, 'step_size': 5, 'gamma': 0.7815287437558481}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:32:05,678][0m Trial 2 finished with value: 0.47699656377872224 and parameters: {'observation_period_num': 143, 'train_rates': 0.7832567508490005, 'learning_rate': 0.0002492142384425912, 'batch_size': 185, 'step_size': 9, 'gamma': 0.8784312005977265}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:35:07,845][0m Trial 3 finished with value: 0.23446723700793257 and parameters: {'observation_period_num': 122, 'train_rates': 0.8794726704105122, 'learning_rate': 7.901911571707855e-05, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8419679184177911}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:39:24,759][0m Trial 4 finished with value: 1.0907014837208397 and parameters: {'observation_period_num': 200, 'train_rates': 0.7243907579053366, 'learning_rate': 4.01861010466887e-06, 'batch_size': 93, 'step_size': 8, 'gamma': 0.9746151264932175}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:42:34,796][0m Trial 5 finished with value: 0.9631872171203032 and parameters: {'observation_period_num': 146, 'train_rates': 0.7849149153934778, 'learning_rate': 3.12010928445094e-06, 'batch_size': 122, 'step_size': 15, 'gamma': 0.9582317461047465}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:47:45,726][0m Trial 6 finished with value: 0.6163041591644287 and parameters: {'observation_period_num': 194, 'train_rates': 0.9860550088054588, 'learning_rate': 2.228154328707762e-06, 'batch_size': 245, 'step_size': 13, 'gamma': 0.9076033545968625}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:52:23,668][0m Trial 7 finished with value: 0.99658526151587 and parameters: {'observation_period_num': 210, 'train_rates': 0.7371442788937104, 'learning_rate': 7.832897548344288e-06, 'batch_size': 75, 'step_size': 15, 'gamma': 0.8195031187584718}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 05:57:06,864][0m Trial 8 finished with value: 0.3038306740463757 and parameters: {'observation_period_num': 193, 'train_rates': 0.8896117923055692, 'learning_rate': 0.0001518351210808565, 'batch_size': 164, 'step_size': 1, 'gamma': 0.9203115135910824}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 06:02:00,445][0m Trial 9 finished with value: 1.4881527348045718 and parameters: {'observation_period_num': 233, 'train_rates': 0.6643637002791412, 'learning_rate': 1.1442848619149157e-06, 'batch_size': 65, 'step_size': 13, 'gamma': 0.849807570377073}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 06:02:59,177][0m Trial 10 finished with value: 0.9200585087751463 and parameters: {'observation_period_num': 59, 'train_rates': 0.6064636744485408, 'learning_rate': 0.0007189522289830617, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9874157320959829}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 06:06:10,036][0m Trial 11 finished with value: 0.225918296886527 and parameters: {'observation_period_num': 99, 'train_rates': 0.8873858662111715, 'learning_rate': 4.5558347561246195e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.7558688170764973}. Best is trial 0 with value: 0.13486211001873016.[0m
[32m[I 2025-01-03 06:09:55,335][0m Trial 12 finished with value: 0.12850211907264797 and parameters: {'observation_period_num': 8, 'train_rates': 0.9854939766103304, 'learning_rate': 3.07728022168804e-05, 'batch_size': 18, 'step_size': 4, 'gamma': 0.7526725272311005}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:10:33,501][0m Trial 13 finished with value: 0.18340758979320526 and parameters: {'observation_period_num': 17, 'train_rates': 0.9853652854469053, 'learning_rate': 2.2099496816927617e-05, 'batch_size': 114, 'step_size': 4, 'gamma': 0.9282203140058563}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:11:04,581][0m Trial 14 finished with value: 0.41335438191890717 and parameters: {'observation_period_num': 19, 'train_rates': 0.9304216346011085, 'learning_rate': 2.1263533310249165e-05, 'batch_size': 162, 'step_size': 4, 'gamma': 0.7926371059794859}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:17:46,897][0m Trial 15 finished with value: 0.1441953182220459 and parameters: {'observation_period_num': 252, 'train_rates': 0.9440767896998439, 'learning_rate': 0.0003822985986046686, 'batch_size': 204, 'step_size': 7, 'gamma': 0.8827538707067992}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:18:48,782][0m Trial 16 finished with value: 0.30608582396872874 and parameters: {'observation_period_num': 45, 'train_rates': 0.8406502469938172, 'learning_rate': 8.625212795352144e-05, 'batch_size': 97, 'step_size': 2, 'gamma': 0.9436934195887067}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:22:28,756][0m Trial 17 finished with value: 0.5912521221632554 and parameters: {'observation_period_num': 162, 'train_rates': 0.8347160993911096, 'learning_rate': 3.8620668306118324e-05, 'batch_size': 137, 'step_size': 3, 'gamma': 0.8986997622204247}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:25:05,416][0m Trial 18 finished with value: 0.28855154935906574 and parameters: {'observation_period_num': 105, 'train_rates': 0.957046231280861, 'learning_rate': 1.2185241364103994e-05, 'batch_size': 48, 'step_size': 6, 'gamma': 0.8130804567380477}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:28:57,291][0m Trial 19 finished with value: 0.36187225083510083 and parameters: {'observation_period_num': 171, 'train_rates': 0.8361041613359197, 'learning_rate': 0.00010888849566311516, 'batch_size': 152, 'step_size': 11, 'gamma': 0.75012454434183}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:29:54,957][0m Trial 20 finished with value: 0.3684422969818115 and parameters: {'observation_period_num': 40, 'train_rates': 0.9881087571103941, 'learning_rate': 0.0008933514921383472, 'batch_size': 201, 'step_size': 3, 'gamma': 0.854179346320922}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:36:17,119][0m Trial 21 finished with value: 0.1538044661283493 and parameters: {'observation_period_num': 241, 'train_rates': 0.93931478187904, 'learning_rate': 0.00030329188463870264, 'batch_size': 201, 'step_size': 7, 'gamma': 0.8788192158456714}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:43:03,422][0m Trial 22 finished with value: 0.144216388463974 and parameters: {'observation_period_num': 249, 'train_rates': 0.9567997600910585, 'learning_rate': 0.0004089565181585731, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8976882600507694}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:48:38,890][0m Trial 23 finished with value: 0.19488914991324802 and parameters: {'observation_period_num': 221, 'train_rates': 0.9031902740899157, 'learning_rate': 0.00019262589099814333, 'batch_size': 201, 'step_size': 6, 'gamma': 0.9504208326251236}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:53:11,671][0m Trial 24 finished with value: 0.18299253284931183 and parameters: {'observation_period_num': 181, 'train_rates': 0.9465160903274235, 'learning_rate': 0.00043667292123585863, 'batch_size': 256, 'step_size': 3, 'gamma': 0.826894416987791}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 06:59:29,094][0m Trial 25 finished with value: 0.35161682963371277 and parameters: {'observation_period_num': 251, 'train_rates': 0.867376912753112, 'learning_rate': 5.7644605371424314e-05, 'batch_size': 179, 'step_size': 8, 'gamma': 0.8654482813381499}. Best is trial 12 with value: 0.12850211907264797.[0m
Early stopping at epoch 97
[32m[I 2025-01-03 07:05:29,404][0m Trial 26 finished with value: 0.2620888650417328 and parameters: {'observation_period_num': 231, 'train_rates': 0.96378279906456, 'learning_rate': 0.00013344670949094444, 'batch_size': 137, 'step_size': 2, 'gamma': 0.7843197557481437}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:07:16,190][0m Trial 27 finished with value: 0.17773668004269422 and parameters: {'observation_period_num': 76, 'train_rates': 0.9162281530876839, 'learning_rate': 2.4392523743342017e-05, 'batch_size': 97, 'step_size': 10, 'gamma': 0.9336465684796605}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:13:16,331][0m Trial 28 finished with value: 0.3524966589782549 and parameters: {'observation_period_num': 217, 'train_rates': 0.9663800292623766, 'learning_rate': 0.0005204096156321434, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9714207165919946}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:15:27,639][0m Trial 29 finished with value: 0.19943680231146485 and parameters: {'observation_period_num': 94, 'train_rates': 0.9136445374267279, 'learning_rate': 5.9244309008079274e-05, 'batch_size': 70, 'step_size': 5, 'gamma': 0.7669386119056055}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:18:13,187][0m Trial 30 finished with value: 0.3228717717019158 and parameters: {'observation_period_num': 128, 'train_rates': 0.8591325104286773, 'learning_rate': 0.0002618225467916029, 'batch_size': 183, 'step_size': 4, 'gamma': 0.8018337644205444}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:24:46,361][0m Trial 31 finished with value: 0.15823635458946228 and parameters: {'observation_period_num': 245, 'train_rates': 0.9361086001568663, 'learning_rate': 0.0004600070294734986, 'batch_size': 225, 'step_size': 11, 'gamma': 0.9012833409596139}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:31:36,124][0m Trial 32 finished with value: 0.13363583385944366 and parameters: {'observation_period_num': 250, 'train_rates': 0.9589206528390619, 'learning_rate': 0.0003380696910361271, 'batch_size': 221, 'step_size': 9, 'gamma': 0.8900239843892181}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:37:42,885][0m Trial 33 finished with value: 0.13373440504074097 and parameters: {'observation_period_num': 226, 'train_rates': 0.9714640490045944, 'learning_rate': 0.0001840209763961321, 'batch_size': 218, 'step_size': 7, 'gamma': 0.8821052761871486}. Best is trial 12 with value: 0.12850211907264797.[0m
[32m[I 2025-01-03 07:43:17,242][0m Trial 34 finished with value: 0.12701399624347687 and parameters: {'observation_period_num': 210, 'train_rates': 0.9735559420275615, 'learning_rate': 0.00019836179663919193, 'batch_size': 234, 'step_size': 9, 'gamma': 0.8375872441262052}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 07:48:59,155][0m Trial 35 finished with value: 0.16688400506973267 and parameters: {'observation_period_num': 224, 'train_rates': 0.917483585699268, 'learning_rate': 0.00018060403915221476, 'batch_size': 245, 'step_size': 9, 'gamma': 0.8333833427827129}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 07:52:44,316][0m Trial 36 finished with value: 0.4382166564464569 and parameters: {'observation_period_num': 151, 'train_rates': 0.9694488822534516, 'learning_rate': 1.2366077353583105e-05, 'batch_size': 219, 'step_size': 11, 'gamma': 0.864469750518367}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 07:58:06,996][0m Trial 37 finished with value: 0.14542321860790253 and parameters: {'observation_period_num': 202, 'train_rates': 0.9720035463547358, 'learning_rate': 8.507795542568171e-05, 'batch_size': 240, 'step_size': 8, 'gamma': 0.8823691714744161}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 08:01:59,858][0m Trial 38 finished with value: 0.8086217732316604 and parameters: {'observation_period_num': 185, 'train_rates': 0.7406262279756619, 'learning_rate': 0.00019788514287523198, 'batch_size': 215, 'step_size': 10, 'gamma': 0.9115967675141297}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 08:07:23,960][0m Trial 39 finished with value: 0.4547138443192932 and parameters: {'observation_period_num': 232, 'train_rates': 0.7981609027099682, 'learning_rate': 0.000600071654918674, 'batch_size': 256, 'step_size': 7, 'gamma': 0.7710221670594383}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 08:13:05,556][0m Trial 40 finished with value: 0.12926480174064636 and parameters: {'observation_period_num': 212, 'train_rates': 0.9882766074049326, 'learning_rate': 0.00012359493752834476, 'batch_size': 234, 'step_size': 12, 'gamma': 0.8922090524182021}. Best is trial 34 with value: 0.12701399624347687.[0m
[32m[I 2025-01-03 08:18:50,381][0m Trial 41 finished with value: 0.11187892407178879 and parameters: {'observation_period_num': 215, 'train_rates': 0.9757300504919704, 'learning_rate': 0.0002796328748352246, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8930159786690934}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:24:07,986][0m Trial 42 finished with value: 0.13306692242622375 and parameters: {'observation_period_num': 200, 'train_rates': 0.9873988907203287, 'learning_rate': 0.0003003545566626849, 'batch_size': 230, 'step_size': 13, 'gamma': 0.8458490887768292}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:29:41,214][0m Trial 43 finished with value: 0.13761316239833832 and parameters: {'observation_period_num': 206, 'train_rates': 0.9843625964899212, 'learning_rate': 0.00011390785714334432, 'batch_size': 246, 'step_size': 13, 'gamma': 0.8458127328148023}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:34:10,092][0m Trial 44 finished with value: 0.15233321487903595 and parameters: {'observation_period_num': 184, 'train_rates': 0.9255980753162607, 'learning_rate': 0.00023812096802438375, 'batch_size': 233, 'step_size': 14, 'gamma': 0.8655585004034949}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:38:17,797][0m Trial 45 finished with value: 0.32483917474746704 and parameters: {'observation_period_num': 161, 'train_rates': 0.9854988189168817, 'learning_rate': 0.000828253494987226, 'batch_size': 193, 'step_size': 12, 'gamma': 0.8098476801834016}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:43:07,871][0m Trial 46 finished with value: 0.23170839763405332 and parameters: {'observation_period_num': 196, 'train_rates': 0.8917834945756548, 'learning_rate': 6.732039012011243e-05, 'batch_size': 236, 'step_size': 14, 'gamma': 0.8533292443637749}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:47:28,282][0m Trial 47 finished with value: 1.2353435519216456 and parameters: {'observation_period_num': 213, 'train_rates': 0.649651945982972, 'learning_rate': 3.384921803895779e-05, 'batch_size': 177, 'step_size': 12, 'gamma': 0.8385836680834013}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:50:50,540][0m Trial 48 finished with value: 0.2923433780670166 and parameters: {'observation_period_num': 138, 'train_rates': 0.9482285684847266, 'learning_rate': 1.5332440728249673e-05, 'batch_size': 210, 'step_size': 12, 'gamma': 0.9164480255021331}. Best is trial 41 with value: 0.11187892407178879.[0m
[32m[I 2025-01-03 08:53:28,380][0m Trial 49 finished with value: 0.4153871706553868 and parameters: {'observation_period_num': 115, 'train_rates': 0.9015057987231572, 'learning_rate': 4.526968518880502e-06, 'batch_size': 80, 'step_size': 15, 'gamma': 0.8260107445307182}. Best is trial 41 with value: 0.11187892407178879.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 08:53:28,388][0m A new study created in memory with name: no-name-f0ed9704-6c61-480f-b5cc-635e401a0145[0m
Early stopping at epoch 70
[32m[I 2025-01-03 08:58:08,490][0m Trial 0 finished with value: 0.7518849608798822 and parameters: {'observation_period_num': 250, 'train_rates': 0.9290876784116064, 'learning_rate': 2.7964191459991455e-05, 'batch_size': 140, 'step_size': 1, 'gamma': 0.8391754086728676}. Best is trial 0 with value: 0.7518849608798822.[0m
[32m[I 2025-01-03 08:58:30,037][0m Trial 1 finished with value: 0.635754791638462 and parameters: {'observation_period_num': 6, 'train_rates': 0.6235310398716853, 'learning_rate': 0.00012923897326438716, 'batch_size': 163, 'step_size': 9, 'gamma': 0.8845009678747096}. Best is trial 1 with value: 0.635754791638462.[0m
[32m[I 2025-01-03 09:01:11,339][0m Trial 2 finished with value: 0.5595139718112356 and parameters: {'observation_period_num': 122, 'train_rates': 0.8511351212816267, 'learning_rate': 4.8844561730854255e-06, 'batch_size': 80, 'step_size': 5, 'gamma': 0.9390789349825652}. Best is trial 2 with value: 0.5595139718112356.[0m
[32m[I 2025-01-03 09:04:57,422][0m Trial 3 finished with value: 1.5383752215768873 and parameters: {'observation_period_num': 176, 'train_rates': 0.755273839549931, 'learning_rate': 1.514889520521043e-06, 'batch_size': 62, 'step_size': 7, 'gamma': 0.8935835193896517}. Best is trial 2 with value: 0.5595139718112356.[0m
[32m[I 2025-01-03 09:05:54,263][0m Trial 4 finished with value: 0.3522099637375853 and parameters: {'observation_period_num': 40, 'train_rates': 0.8927207875326009, 'learning_rate': 7.232798257966476e-06, 'batch_size': 104, 'step_size': 13, 'gamma': 0.915886408351985}. Best is trial 4 with value: 0.3522099637375853.[0m
[32m[I 2025-01-03 09:08:35,772][0m Trial 5 finished with value: 0.8102711145688568 and parameters: {'observation_period_num': 109, 'train_rates': 0.7252968516878326, 'learning_rate': 6.09234784013076e-06, 'batch_size': 23, 'step_size': 10, 'gamma': 0.8556175170461136}. Best is trial 4 with value: 0.3522099637375853.[0m
[32m[I 2025-01-03 09:11:03,707][0m Trial 6 finished with value: 0.7676048816620619 and parameters: {'observation_period_num': 118, 'train_rates': 0.8159613111448292, 'learning_rate': 2.9394787282987985e-05, 'batch_size': 191, 'step_size': 2, 'gamma': 0.8220371832095735}. Best is trial 4 with value: 0.3522099637375853.[0m
[32m[I 2025-01-03 09:13:16,011][0m Trial 7 finished with value: 0.8492790271379553 and parameters: {'observation_period_num': 111, 'train_rates': 0.7638144845744637, 'learning_rate': 0.0006114525083297704, 'batch_size': 158, 'step_size': 7, 'gamma': 0.7928968668689786}. Best is trial 4 with value: 0.3522099637375853.[0m
[32m[I 2025-01-03 09:18:53,422][0m Trial 8 finished with value: 0.1576101928949356 and parameters: {'observation_period_num': 215, 'train_rates': 0.9388404471082847, 'learning_rate': 3.8811332529771566e-05, 'batch_size': 88, 'step_size': 10, 'gamma': 0.7945499095522451}. Best is trial 8 with value: 0.1576101928949356.[0m
[32m[I 2025-01-03 09:20:51,584][0m Trial 9 finished with value: 1.3592822415282928 and parameters: {'observation_period_num': 110, 'train_rates': 0.6495128097641721, 'learning_rate': 3.999076934773183e-06, 'batch_size': 156, 'step_size': 11, 'gamma': 0.9542131850906778}. Best is trial 8 with value: 0.1576101928949356.[0m
[32m[I 2025-01-03 09:27:06,323][0m Trial 10 finished with value: 0.12276338040828705 and parameters: {'observation_period_num': 231, 'train_rates': 0.9813805029990186, 'learning_rate': 0.00014334430544715837, 'batch_size': 235, 'step_size': 15, 'gamma': 0.7617324679200497}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 09:34:06,656][0m Trial 11 finished with value: 0.12851260602474213 and parameters: {'observation_period_num': 251, 'train_rates': 0.983997473405244, 'learning_rate': 0.00014944684332503976, 'batch_size': 234, 'step_size': 15, 'gamma': 0.7598212388759435}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 09:40:29,149][0m Trial 12 finished with value: 0.1308085322380066 and parameters: {'observation_period_num': 236, 'train_rates': 0.9709306047775554, 'learning_rate': 0.000247364449546511, 'batch_size': 256, 'step_size': 15, 'gamma': 0.75925157968693}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 09:45:09,236][0m Trial 13 finished with value: 0.13451042771339417 and parameters: {'observation_period_num': 182, 'train_rates': 0.9848969928376688, 'learning_rate': 0.00011917001362731771, 'batch_size': 250, 'step_size': 15, 'gamma': 0.7669932103042014}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 09:49:31,446][0m Trial 14 finished with value: 0.3356076569116416 and parameters: {'observation_period_num': 186, 'train_rates': 0.8709661183338555, 'learning_rate': 0.0008217600687484011, 'batch_size': 213, 'step_size': 13, 'gamma': 0.7513809752702392}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 09:54:54,388][0m Trial 15 finished with value: 0.16558364931589517 and parameters: {'observation_period_num': 212, 'train_rates': 0.9145803986709764, 'learning_rate': 0.00025079110433176625, 'batch_size': 217, 'step_size': 13, 'gamma': 0.7970820382583245}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 09:58:45,884][0m Trial 16 finished with value: 0.14971254765987396 and parameters: {'observation_period_num': 155, 'train_rates': 0.9713055250168722, 'learning_rate': 7.31801255055241e-05, 'batch_size': 223, 'step_size': 12, 'gamma': 0.8130040906165105}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:05:00,266][0m Trial 17 finished with value: 0.47945393522580465 and parameters: {'observation_period_num': 252, 'train_rates': 0.8502054116185804, 'learning_rate': 1.5015728550872018e-05, 'batch_size': 191, 'step_size': 15, 'gamma': 0.9842754610817572}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:06:20,875][0m Trial 18 finished with value: 0.7598388559566248 and parameters: {'observation_period_num': 72, 'train_rates': 0.6893987020198091, 'learning_rate': 0.0003630858027026492, 'batch_size': 189, 'step_size': 4, 'gamma': 0.7808406289561753}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:11:31,167][0m Trial 19 finished with value: 0.46465974133496923 and parameters: {'observation_period_num': 218, 'train_rates': 0.806460493565617, 'learning_rate': 9.356774203365877e-05, 'batch_size': 237, 'step_size': 14, 'gamma': 0.8525290261004528}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:15:20,007][0m Trial 20 finished with value: 0.15940448641777039 and parameters: {'observation_period_num': 153, 'train_rates': 0.9465273040270319, 'learning_rate': 6.440539300809372e-05, 'batch_size': 203, 'step_size': 11, 'gamma': 0.8214582164709149}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:21:39,848][0m Trial 21 finished with value: 0.12493668496608734 and parameters: {'observation_period_num': 232, 'train_rates': 0.9894545630834546, 'learning_rate': 0.00026101906926005074, 'batch_size': 249, 'step_size': 15, 'gamma': 0.7508665345081529}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:27:53,716][0m Trial 22 finished with value: 0.12298424541950226 and parameters: {'observation_period_num': 227, 'train_rates': 0.9866610273013147, 'learning_rate': 0.00020385894463234133, 'batch_size': 235, 'step_size': 14, 'gamma': 0.7732616128100254}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:32:59,151][0m Trial 23 finished with value: 0.1874429097760728 and parameters: {'observation_period_num': 201, 'train_rates': 0.900697144520501, 'learning_rate': 0.0004222337399109416, 'batch_size': 254, 'step_size': 13, 'gamma': 0.7732586079263164}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:39:04,329][0m Trial 24 finished with value: 0.12489008903503418 and parameters: {'observation_period_num': 230, 'train_rates': 0.9496005147701229, 'learning_rate': 0.00023197672715018187, 'batch_size': 223, 'step_size': 14, 'gamma': 0.7820900486524861}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:42:57,745][0m Trial 25 finished with value: 0.1321287900209427 and parameters: {'observation_period_num': 160, 'train_rates': 0.9452781783706776, 'learning_rate': 0.00018099421373995786, 'batch_size': 181, 'step_size': 12, 'gamma': 0.8087236181936763}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:49:05,890][0m Trial 26 finished with value: 0.1253935843706131 and parameters: {'observation_period_num': 229, 'train_rates': 0.9540862287973628, 'learning_rate': 0.0005187050689900653, 'batch_size': 229, 'step_size': 14, 'gamma': 0.7767407183533968}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:54:02,980][0m Trial 27 finished with value: 0.27276697134779343 and parameters: {'observation_period_num': 201, 'train_rates': 0.8875196364270902, 'learning_rate': 4.954693321915792e-05, 'batch_size': 211, 'step_size': 12, 'gamma': 0.8359675833435034}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 10:57:29,187][0m Trial 28 finished with value: 0.25575320591341777 and parameters: {'observation_period_num': 141, 'train_rates': 0.9245329320017314, 'learning_rate': 1.6874843377242678e-05, 'batch_size': 124, 'step_size': 14, 'gamma': 0.7828083008728944}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 11:02:00,512][0m Trial 29 finished with value: 1.1091861113052537 and parameters: {'observation_period_num': 193, 'train_rates': 0.8354508541017026, 'learning_rate': 0.0009251788118357578, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8355500009622985}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 11:08:09,212][0m Trial 30 finished with value: 0.16137815256641336 and parameters: {'observation_period_num': 238, 'train_rates': 0.919419596320145, 'learning_rate': 0.00019960703667171935, 'batch_size': 168, 'step_size': 11, 'gamma': 0.8015043670132033}. Best is trial 10 with value: 0.12276338040828705.[0m
[32m[I 2025-01-03 11:14:18,411][0m Trial 31 finished with value: 0.11449102312326431 and parameters: {'observation_period_num': 226, 'train_rates': 0.9890916405381694, 'learning_rate': 0.0003115004457165443, 'batch_size': 246, 'step_size': 14, 'gamma': 0.7536981106442717}. Best is trial 31 with value: 0.11449102312326431.[0m
[32m[I 2025-01-03 11:20:11,101][0m Trial 32 finished with value: 0.12660720944404602 and parameters: {'observation_period_num': 221, 'train_rates': 0.9645794910443121, 'learning_rate': 0.0002815429955894123, 'batch_size': 237, 'step_size': 14, 'gamma': 0.7714042513816067}. Best is trial 31 with value: 0.11449102312326431.[0m
[32m[I 2025-01-03 11:25:30,511][0m Trial 33 finished with value: 0.13568785786628723 and parameters: {'observation_period_num': 205, 'train_rates': 0.9546491964019505, 'learning_rate': 0.0001369569727389983, 'batch_size': 203, 'step_size': 14, 'gamma': 0.7860963575249248}. Best is trial 31 with value: 0.11449102312326431.[0m
[32m[I 2025-01-03 11:29:41,133][0m Trial 34 finished with value: 0.1604989618062973 and parameters: {'observation_period_num': 169, 'train_rates': 0.9351040485949557, 'learning_rate': 9.654044277566178e-05, 'batch_size': 239, 'step_size': 12, 'gamma': 0.8873904006901391}. Best is trial 31 with value: 0.11449102312326431.[0m
[32m[I 2025-01-03 11:35:34,874][0m Trial 35 finished with value: 0.2261927217245102 and parameters: {'observation_period_num': 235, 'train_rates': 0.8788140387551828, 'learning_rate': 0.00036554437821338827, 'batch_size': 222, 'step_size': 9, 'gamma': 0.7659562526069483}. Best is trial 31 with value: 0.11449102312326431.[0m
[32m[I 2025-01-03 11:41:51,411][0m Trial 36 finished with value: 0.17646186239269362 and parameters: {'observation_period_num': 243, 'train_rates': 0.9083295531894348, 'learning_rate': 0.0006505899319435256, 'batch_size': 203, 'step_size': 5, 'gamma': 0.8719560356051386}. Best is trial 31 with value: 0.11449102312326431.[0m
[32m[I 2025-01-03 11:42:14,355][0m Trial 37 finished with value: 0.11023779958486557 and parameters: {'observation_period_num': 7, 'train_rates': 0.9887151595271816, 'learning_rate': 6.649393866856025e-05, 'batch_size': 241, 'step_size': 13, 'gamma': 0.9136028277717149}. Best is trial 37 with value: 0.11023779958486557.[0m
[32m[I 2025-01-03 11:44:57,980][0m Trial 38 finished with value: 0.08822236187530286 and parameters: {'observation_period_num': 29, 'train_rates': 0.9889658330371552, 'learning_rate': 5.4605206002228943e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9144915065563395}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:46:15,353][0m Trial 39 finished with value: 0.6145875576705063 and parameters: {'observation_period_num': 29, 'train_rates': 0.7785367599627044, 'learning_rate': 1.9631773764106548e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.9096030228005885}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:48:15,830][0m Trial 40 finished with value: 0.7902855717500482 and parameters: {'observation_period_num': 8, 'train_rates': 0.6237497533153509, 'learning_rate': 4.2555841109416174e-05, 'batch_size': 28, 'step_size': 13, 'gamma': 0.9340839186167292}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:50:20,686][0m Trial 41 finished with value: 0.10908658057451248 and parameters: {'observation_period_num': 78, 'train_rates': 0.9888752424907458, 'learning_rate': 6.7805679422774e-05, 'batch_size': 52, 'step_size': 13, 'gamma': 0.9040092614256864}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:52:41,510][0m Trial 42 finished with value: 0.2578803229499871 and parameters: {'observation_period_num': 86, 'train_rates': 0.9755425967936944, 'learning_rate': 2.362119729204468e-05, 'batch_size': 38, 'step_size': 1, 'gamma': 0.9057323546644942}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:54:06,624][0m Trial 43 finished with value: 0.11940631727283856 and parameters: {'observation_period_num': 55, 'train_rates': 0.9636814405243861, 'learning_rate': 6.643231800991275e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9268745185474287}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:55:26,278][0m Trial 44 finished with value: 0.20668410791934116 and parameters: {'observation_period_num': 53, 'train_rates': 0.9645175030108237, 'learning_rate': 1.0323383564088134e-05, 'batch_size': 72, 'step_size': 11, 'gamma': 0.9277408812445379}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 11:59:08,847][0m Trial 45 finished with value: 0.12747322705884775 and parameters: {'observation_period_num': 21, 'train_rates': 0.9304229836609405, 'learning_rate': 3.022376072746567e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9526949290523065}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 12:00:27,158][0m Trial 46 finished with value: 0.13689329800675215 and parameters: {'observation_period_num': 55, 'train_rates': 0.9645196110007109, 'learning_rate': 5.254284341720879e-05, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8966100406453396}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 12:02:43,677][0m Trial 47 finished with value: 0.34751859307289124 and parameters: {'observation_period_num': 89, 'train_rates': 0.9892713839259912, 'learning_rate': 2.1759905654518455e-06, 'batch_size': 54, 'step_size': 12, 'gamma': 0.9209833823406389}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 12:04:26,235][0m Trial 48 finished with value: 0.10785541406699589 and parameters: {'observation_period_num': 42, 'train_rates': 0.940071818320517, 'learning_rate': 7.34661742432993e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.9456466092383259}. Best is trial 38 with value: 0.08822236187530286.[0m
[32m[I 2025-01-03 12:05:57,936][0m Trial 49 finished with value: 0.6768554320389574 and parameters: {'observation_period_num': 38, 'train_rates': 0.731944708781525, 'learning_rate': 9.003172081158786e-05, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9479821566254338}. Best is trial 38 with value: 0.08822236187530286.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-03 12:05:57,943][0m A new study created in memory with name: no-name-0eb9432e-f52c-43d3-a172-344327f9454d[0m
[32m[I 2025-01-03 12:10:59,800][0m Trial 0 finished with value: 0.15315374096526818 and parameters: {'observation_period_num': 195, 'train_rates': 0.9508174513727131, 'learning_rate': 4.6120876728360394e-05, 'batch_size': 82, 'step_size': 11, 'gamma': 0.7748221310672468}. Best is trial 0 with value: 0.15315374096526818.[0m
[32m[I 2025-01-03 12:11:24,888][0m Trial 1 finished with value: 0.1101038008928299 and parameters: {'observation_period_num': 15, 'train_rates': 0.9813294099965122, 'learning_rate': 0.00015985581300624352, 'batch_size': 223, 'step_size': 8, 'gamma': 0.7918879698045019}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:16:32,691][0m Trial 2 finished with value: 0.3051607385277748 and parameters: {'observation_period_num': 211, 'train_rates': 0.8643104485425761, 'learning_rate': 0.00027913876970841713, 'batch_size': 186, 'step_size': 13, 'gamma': 0.8283349556468678}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:18:18,650][0m Trial 3 finished with value: 1.0801390080005595 and parameters: {'observation_period_num': 88, 'train_rates': 0.7650713461673775, 'learning_rate': 7.026720202987333e-06, 'batch_size': 138, 'step_size': 12, 'gamma': 0.8528224830513541}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:20:12,255][0m Trial 4 finished with value: 0.43959598622538826 and parameters: {'observation_period_num': 90, 'train_rates': 0.8079667845966171, 'learning_rate': 0.0003147835118041988, 'batch_size': 114, 'step_size': 12, 'gamma': 0.7555956115709018}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:21:52,650][0m Trial 5 finished with value: 0.8533665557680692 and parameters: {'observation_period_num': 89, 'train_rates': 0.6799747016512172, 'learning_rate': 0.00010191155534863606, 'batch_size': 123, 'step_size': 4, 'gamma': 0.8183781559452079}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:25:18,980][0m Trial 6 finished with value: 0.5211534089333302 and parameters: {'observation_period_num': 53, 'train_rates': 0.9491181233274505, 'learning_rate': 3.320691848936179e-06, 'batch_size': 20, 'step_size': 3, 'gamma': 0.7706403155200807}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:27:18,165][0m Trial 7 finished with value: 0.6891025963415031 and parameters: {'observation_period_num': 82, 'train_rates': 0.804076075766087, 'learning_rate': 1.487959801593988e-06, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9739101018046312}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:30:17,349][0m Trial 8 finished with value: 0.9729085001871994 and parameters: {'observation_period_num': 133, 'train_rates': 0.8159536512902371, 'learning_rate': 1.9281664451667153e-06, 'batch_size': 85, 'step_size': 12, 'gamma': 0.836966773200724}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:36:40,633][0m Trial 9 finished with value: 0.4627760461463782 and parameters: {'observation_period_num': 232, 'train_rates': 0.9041695209785068, 'learning_rate': 0.0004278946275706808, 'batch_size': 22, 'step_size': 7, 'gamma': 0.8254148668983545}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:36:55,937][0m Trial 10 finished with value: 1.1527218590580226 and parameters: {'observation_period_num': 9, 'train_rates': 0.6117622034462564, 'learning_rate': 1.5956923617043006e-05, 'batch_size': 251, 'step_size': 7, 'gamma': 0.9189478349842464}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:41:24,897][0m Trial 11 finished with value: 0.15392112731933594 and parameters: {'observation_period_num': 174, 'train_rates': 0.988684941872192, 'learning_rate': 6.844583770219243e-05, 'batch_size': 249, 'step_size': 15, 'gamma': 0.7898133141479783}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:45:36,818][0m Trial 12 finished with value: 0.14898881316184998 and parameters: {'observation_period_num': 165, 'train_rates': 0.9885295165222108, 'learning_rate': 4.036926596809154e-05, 'batch_size': 195, 'step_size': 9, 'gamma': 0.8986657493812722}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:49:06,009][0m Trial 13 finished with value: 0.5391751814420056 and parameters: {'observation_period_num': 149, 'train_rates': 0.8882085281355567, 'learning_rate': 0.000889529357806797, 'batch_size': 206, 'step_size': 9, 'gamma': 0.893837153672724}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:49:40,708][0m Trial 14 finished with value: 0.1887194812297821 and parameters: {'observation_period_num': 21, 'train_rates': 0.9870161471570774, 'learning_rate': 2.0509933353477744e-05, 'batch_size': 202, 'step_size': 6, 'gamma': 0.9307572019327891}. Best is trial 1 with value: 0.1101038008928299.[0m
Early stopping at epoch 93
[32m[I 2025-01-03 12:52:52,983][0m Trial 15 finished with value: 1.044645334115022 and parameters: {'observation_period_num': 163, 'train_rates': 0.7359164686533564, 'learning_rate': 0.00013358362361087523, 'batch_size': 171, 'step_size': 1, 'gamma': 0.8783079736519488}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 12:55:35,184][0m Trial 16 finished with value: 0.32841387391090393 and parameters: {'observation_period_num': 119, 'train_rates': 0.9326875120153536, 'learning_rate': 9.539241350211085e-06, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9596117394820265}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:01:51,924][0m Trial 17 finished with value: 0.22320995080298273 and parameters: {'observation_period_num': 251, 'train_rates': 0.8723606465242633, 'learning_rate': 0.0001609132190571595, 'batch_size': 160, 'step_size': 8, 'gamma': 0.9017286129089146}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:02:46,825][0m Trial 18 finished with value: 0.38907385127687893 and parameters: {'observation_period_num': 44, 'train_rates': 0.851478090450452, 'learning_rate': 4.056320621540598e-05, 'batch_size': 227, 'step_size': 9, 'gamma': 0.7993282990198465}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:05:27,211][0m Trial 19 finished with value: 1.2578387709407064 and parameters: {'observation_period_num': 117, 'train_rates': 0.9187645615237936, 'learning_rate': 0.0009797323214805335, 'batch_size': 157, 'step_size': 15, 'gamma': 0.8643896164180408}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:10:16,260][0m Trial 20 finished with value: 0.20970337092876434 and parameters: {'observation_period_num': 188, 'train_rates': 0.9645581081788683, 'learning_rate': 2.2653337139711003e-05, 'batch_size': 234, 'step_size': 9, 'gamma': 0.9363238058142923}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:15:23,582][0m Trial 21 finished with value: 0.1339950156575851 and parameters: {'observation_period_num': 195, 'train_rates': 0.9526090613396639, 'learning_rate': 4.956531019400175e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.7882267083088736}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:21:08,652][0m Trial 22 finished with value: 0.12576189637184143 and parameters: {'observation_period_num': 210, 'train_rates': 0.98790335852811, 'learning_rate': 6.319655226093216e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8009653741273876}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:27:07,125][0m Trial 23 finished with value: 0.16877327453006397 and parameters: {'observation_period_num': 231, 'train_rates': 0.9192992375062624, 'learning_rate': 7.969668909635595e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.8017716645212971}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:32:46,817][0m Trial 24 finished with value: 0.1389841999153833 and parameters: {'observation_period_num': 212, 'train_rates': 0.9597596197720609, 'learning_rate': 0.00018725652442751943, 'batch_size': 54, 'step_size': 14, 'gamma': 0.7543684572226457}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:37:22,796][0m Trial 25 finished with value: 0.3352155828184181 and parameters: {'observation_period_num': 195, 'train_rates': 0.8445485430383417, 'learning_rate': 6.26849736757517e-05, 'batch_size': 105, 'step_size': 10, 'gamma': 0.7844463015442538}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:40:53,404][0m Trial 26 finished with value: 0.3200825648899708 and parameters: {'observation_period_num': 142, 'train_rates': 0.897484859958981, 'learning_rate': 9.918690698235374e-06, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8093361006808296}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:47:37,821][0m Trial 27 finished with value: 0.8661278269507668 and parameters: {'observation_period_num': 252, 'train_rates': 0.9349604720247218, 'learning_rate': 0.000593119655760957, 'batch_size': 98, 'step_size': 11, 'gamma': 0.8495377280875005}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:53:31,076][0m Trial 28 finished with value: 0.14609006275464823 and parameters: {'observation_period_num': 218, 'train_rates': 0.9596149183269942, 'learning_rate': 2.6749124694140732e-05, 'batch_size': 62, 'step_size': 13, 'gamma': 0.7714395583339042}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 13:58:22,010][0m Trial 29 finished with value: 0.1280023945229394 and parameters: {'observation_period_num': 182, 'train_rates': 0.9648612552988824, 'learning_rate': 0.00019694049688106228, 'batch_size': 40, 'step_size': 10, 'gamma': 0.7709085777925243}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:02:03,066][0m Trial 30 finished with value: 0.8344209726812801 and parameters: {'observation_period_num': 173, 'train_rates': 0.7075955090021895, 'learning_rate': 0.00017384135732037917, 'batch_size': 37, 'step_size': 8, 'gamma': 0.7717964444894472}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:07:00,480][0m Trial 31 finished with value: 0.13172576309375042 and parameters: {'observation_period_num': 190, 'train_rates': 0.9617849885852727, 'learning_rate': 0.00010969688483954746, 'batch_size': 78, 'step_size': 10, 'gamma': 0.7839779468137378}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:11:53,161][0m Trial 32 finished with value: 0.14396951595942178 and parameters: {'observation_period_num': 183, 'train_rates': 0.9785012726232338, 'learning_rate': 0.0002508372202603755, 'batch_size': 40, 'step_size': 10, 'gamma': 0.8109519334390108}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:17:18,265][0m Trial 33 finished with value: 0.15644230997429215 and parameters: {'observation_period_num': 209, 'train_rates': 0.934812694172356, 'learning_rate': 0.00010535584755812157, 'batch_size': 137, 'step_size': 10, 'gamma': 0.763834627912561}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:23:24,783][0m Trial 34 finished with value: 0.20444387613340867 and parameters: {'observation_period_num': 225, 'train_rates': 0.9687944790735594, 'learning_rate': 0.00039164983754596085, 'batch_size': 66, 'step_size': 13, 'gamma': 0.8366103151436967}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:28:22,379][0m Trial 35 finished with value: 0.2411790276507297 and parameters: {'observation_period_num': 202, 'train_rates': 0.8794398110090462, 'learning_rate': 0.0002657236446686783, 'batch_size': 92, 'step_size': 6, 'gamma': 0.7529047463506698}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:30:44,269][0m Trial 36 finished with value: 0.18669456186688552 and parameters: {'observation_period_num': 104, 'train_rates': 0.9173090960968905, 'learning_rate': 9.20097542182286e-05, 'batch_size': 125, 'step_size': 8, 'gamma': 0.7851951747172304}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:32:26,229][0m Trial 37 finished with value: 0.3083637600246164 and parameters: {'observation_period_num': 71, 'train_rates': 0.8318808967891771, 'learning_rate': 0.00012466789309015735, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8002258765826162}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:36:39,562][0m Trial 38 finished with value: 2.0318737147766868 and parameters: {'observation_period_num': 148, 'train_rates': 0.9425283773836151, 'learning_rate': 0.0005154545422038556, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8206046959402681}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:41:27,841][0m Trial 39 finished with value: 0.9171483648740328 and parameters: {'observation_period_num': 239, 'train_rates': 0.640658407866019, 'learning_rate': 0.00021110416558892635, 'batch_size': 114, 'step_size': 12, 'gamma': 0.7790720786634892}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:43:12,272][0m Trial 40 finished with value: 0.7343361540800996 and parameters: {'observation_period_num': 40, 'train_rates': 0.752844664073439, 'learning_rate': 5.5399351383048285e-05, 'batch_size': 31, 'step_size': 11, 'gamma': 0.8376170848914749}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:48:30,332][0m Trial 41 finished with value: 0.13962723747935407 and parameters: {'observation_period_num': 202, 'train_rates': 0.9532620120512201, 'learning_rate': 3.5099889876692486e-05, 'batch_size': 71, 'step_size': 11, 'gamma': 0.79637244193205}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:53:14,790][0m Trial 42 finished with value: 0.14332970179302593 and parameters: {'observation_period_num': 188, 'train_rates': 0.9487285481670686, 'learning_rate': 8.825929565939634e-05, 'batch_size': 83, 'step_size': 8, 'gamma': 0.7637577862364436}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 14:57:16,587][0m Trial 43 finished with value: 0.1339428573846817 and parameters: {'observation_period_num': 160, 'train_rates': 0.9759700146995807, 'learning_rate': 4.8975469410345e-05, 'batch_size': 77, 'step_size': 9, 'gamma': 0.7849263702933701}. Best is trial 1 with value: 0.1101038008928299.[0m
[32m[I 2025-01-03 15:01:23,739][0m Trial 44 finished with value: 0.10522338339397984 and parameters: {'observation_period_num': 160, 'train_rates': 0.9779906767611074, 'learning_rate': 0.00013483099593761814, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8106359802236661}. Best is trial 44 with value: 0.10522338339397984.[0m
[32m[I 2025-01-03 15:05:52,283][0m Trial 45 finished with value: 0.19253323882594145 and parameters: {'observation_period_num': 178, 'train_rates': 0.904213836217112, 'learning_rate': 0.0003643799406028754, 'batch_size': 50, 'step_size': 7, 'gamma': 0.8126671727718067}. Best is trial 44 with value: 0.10522338339397984.[0m
[32m[I 2025-01-03 15:09:38,544][0m Trial 46 finished with value: 0.5826106954439981 and parameters: {'observation_period_num': 158, 'train_rates': 0.7836923198653527, 'learning_rate': 0.000128881999502945, 'batch_size': 29, 'step_size': 6, 'gamma': 0.849410572265442}. Best is trial 44 with value: 0.10522338339397984.[0m
[32m[I 2025-01-03 15:11:13,273][0m Trial 47 finished with value: 0.11098414659500122 and parameters: {'observation_period_num': 67, 'train_rates': 0.9744271197874858, 'learning_rate': 0.0002588597546426146, 'batch_size': 180, 'step_size': 10, 'gamma': 0.8330849431696593}. Best is trial 44 with value: 0.10522338339397984.[0m
[32m[I 2025-01-03 15:12:45,964][0m Trial 48 finished with value: 0.1636907011270523 and parameters: {'observation_period_num': 65, 'train_rates': 0.9853342614591075, 'learning_rate': 0.0006011190199945731, 'batch_size': 190, 'step_size': 8, 'gamma': 0.832453972152152}. Best is trial 44 with value: 0.10522338339397984.[0m
[32m[I 2025-01-03 15:13:15,252][0m Trial 49 finished with value: 0.11699075271954408 and parameters: {'observation_period_num': 13, 'train_rates': 0.9246066195119319, 'learning_rate': 0.00029855723505744895, 'batch_size': 171, 'step_size': 9, 'gamma': 0.8247692832450071}. Best is trial 44 with value: 0.10522338339397984.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-03 15:13:15,260][0m A new study created in memory with name: no-name-fb6940f6-bca4-4ca5-af2f-7c927bde2da4[0m
[32m[I 2025-01-03 15:14:07,371][0m Trial 0 finished with value: 0.26953744648162986 and parameters: {'observation_period_num': 34, 'train_rates': 0.9171863368408308, 'learning_rate': 1.085885203363614e-05, 'batch_size': 103, 'step_size': 7, 'gamma': 0.922332314927499}. Best is trial 0 with value: 0.26953744648162986.[0m
Early stopping at epoch 75
[32m[I 2025-01-03 15:14:37,725][0m Trial 1 finished with value: 1.5594087184897731 and parameters: {'observation_period_num': 25, 'train_rates': 0.8426329973065236, 'learning_rate': 5.033499444361674e-06, 'batch_size': 127, 'step_size': 2, 'gamma': 0.7507547195185057}. Best is trial 0 with value: 0.26953744648162986.[0m
[32m[I 2025-01-03 15:17:58,576][0m Trial 2 finished with value: 0.8592498563584827 and parameters: {'observation_period_num': 158, 'train_rates': 0.7745019729656657, 'learning_rate': 0.0005736651272946029, 'batch_size': 176, 'step_size': 13, 'gamma': 0.9193479475221076}. Best is trial 0 with value: 0.26953744648162986.[0m
[32m[I 2025-01-03 15:19:15,229][0m Trial 3 finished with value: 0.851915451361422 and parameters: {'observation_period_num': 72, 'train_rates': 0.6041056564772234, 'learning_rate': 0.0001383040484482491, 'batch_size': 98, 'step_size': 6, 'gamma': 0.8235332326937854}. Best is trial 0 with value: 0.26953744648162986.[0m
[32m[I 2025-01-03 15:22:58,812][0m Trial 4 finished with value: 0.6873900691668192 and parameters: {'observation_period_num': 170, 'train_rates': 0.798353068419684, 'learning_rate': 1.762543427251037e-05, 'batch_size': 238, 'step_size': 11, 'gamma': 0.9354452946771522}. Best is trial 0 with value: 0.26953744648162986.[0m
[32m[I 2025-01-03 15:28:10,202][0m Trial 5 finished with value: 1.6478715919586548 and parameters: {'observation_period_num': 202, 'train_rates': 0.9397146735444174, 'learning_rate': 1.8203970379760978e-06, 'batch_size': 116, 'step_size': 2, 'gamma': 0.8371688456952205}. Best is trial 0 with value: 0.26953744648162986.[0m
[32m[I 2025-01-03 15:31:30,973][0m Trial 6 finished with value: 0.14370940724537837 and parameters: {'observation_period_num': 139, 'train_rates': 0.9250161996116981, 'learning_rate': 0.00014595003445733924, 'batch_size': 167, 'step_size': 9, 'gamma': 0.857252189268777}. Best is trial 6 with value: 0.14370940724537837.[0m
[32m[I 2025-01-03 15:35:09,971][0m Trial 7 finished with value: 0.8524252187886667 and parameters: {'observation_period_num': 178, 'train_rates': 0.7037721571404804, 'learning_rate': 0.0001258445122941718, 'batch_size': 212, 'step_size': 5, 'gamma': 0.918831501130579}. Best is trial 6 with value: 0.14370940724537837.[0m
[32m[I 2025-01-03 15:38:48,123][0m Trial 8 finished with value: 1.8780928211447634 and parameters: {'observation_period_num': 168, 'train_rates': 0.7090554670032696, 'learning_rate': 0.000679390242186363, 'batch_size': 32, 'step_size': 13, 'gamma': 0.7622506206308811}. Best is trial 6 with value: 0.14370940724537837.[0m
[32m[I 2025-01-03 15:41:59,013][0m Trial 9 finished with value: 0.9726553909156633 and parameters: {'observation_period_num': 172, 'train_rates': 0.6109098953371419, 'learning_rate': 0.00032551632590412094, 'batch_size': 141, 'step_size': 13, 'gamma': 0.8248304711293926}. Best is trial 6 with value: 0.14370940724537837.[0m
[32m[I 2025-01-03 15:44:57,808][0m Trial 10 finished with value: 0.12044012326170021 and parameters: {'observation_period_num': 105, 'train_rates': 0.9751070379565482, 'learning_rate': 5.588227984342401e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.8731505106898378}. Best is trial 10 with value: 0.12044012326170021.[0m
[32m[I 2025-01-03 15:49:24,484][0m Trial 11 finished with value: 0.11007708890570535 and parameters: {'observation_period_num': 95, 'train_rates': 0.9874840147760537, 'learning_rate': 6.973472474047547e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8707590452581023}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 15:52:04,367][0m Trial 12 finished with value: 0.27274485071500143 and parameters: {'observation_period_num': 88, 'train_rates': 0.9741231782430652, 'learning_rate': 5.029660988371462e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.975799783600377}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 15:54:25,116][0m Trial 13 finished with value: 0.11840026080608368 and parameters: {'observation_period_num': 96, 'train_rates': 0.9891110254199934, 'learning_rate': 4.410003732657577e-05, 'batch_size': 66, 'step_size': 11, 'gamma': 0.8862581488725829}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 15:55:49,302][0m Trial 14 finished with value: 0.24526156626600126 and parameters: {'observation_period_num': 59, 'train_rates': 0.8764379728264512, 'learning_rate': 2.2836094939602322e-05, 'batch_size': 71, 'step_size': 15, 'gamma': 0.8914154465246429}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 15:58:25,726][0m Trial 15 finished with value: 0.4626541371067996 and parameters: {'observation_period_num': 114, 'train_rates': 0.8667563272670082, 'learning_rate': 6.666685570094566e-06, 'batch_size': 64, 'step_size': 12, 'gamma': 0.7853573254248496}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:05:11,635][0m Trial 16 finished with value: 0.11167329549789429 and parameters: {'observation_period_num': 241, 'train_rates': 0.9860421167298414, 'learning_rate': 5.6480884294637015e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.9678964297692754}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:11:41,958][0m Trial 17 finished with value: 0.22136882150715048 and parameters: {'observation_period_num': 250, 'train_rates': 0.9023357289610989, 'learning_rate': 0.000245590826529651, 'batch_size': 56, 'step_size': 4, 'gamma': 0.9760552759085822}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:18:29,428][0m Trial 18 finished with value: 0.11468726127993228 and parameters: {'observation_period_num': 235, 'train_rates': 0.9527400663331149, 'learning_rate': 8.78011244284378e-05, 'batch_size': 21, 'step_size': 8, 'gamma': 0.9591326712044042}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:19:36,438][0m Trial 19 finished with value: 1.2651094976224397 and parameters: {'observation_period_num': 51, 'train_rates': 0.8035362237196618, 'learning_rate': 1.140734344922217e-06, 'batch_size': 93, 'step_size': 8, 'gamma': 0.7927511207751166}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:25:02,223][0m Trial 20 finished with value: 0.336552667018117 and parameters: {'observation_period_num': 218, 'train_rates': 0.8502597504424074, 'learning_rate': 2.5697160101355216e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9480612146141029}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:32:04,658][0m Trial 21 finished with value: 0.13694642260149642 and parameters: {'observation_period_num': 247, 'train_rates': 0.9474457791621584, 'learning_rate': 7.935457002737717e-05, 'batch_size': 23, 'step_size': 8, 'gamma': 0.9627198981750072}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:38:29,846][0m Trial 22 finished with value: 0.1136129699074305 and parameters: {'observation_period_num': 216, 'train_rates': 0.9574265443757377, 'learning_rate': 9.592830358458817e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.989423741629327}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:44:02,407][0m Trial 23 finished with value: 0.15257540345191956 and parameters: {'observation_period_num': 201, 'train_rates': 0.98980816018254, 'learning_rate': 0.00027799062875289983, 'batch_size': 49, 'step_size': 9, 'gamma': 0.9883813121753561}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:47:17,734][0m Trial 24 finished with value: 0.19106158018112182 and parameters: {'observation_period_num': 136, 'train_rates': 0.8936733646631042, 'learning_rate': 3.52966477584722e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.9873371478650385}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:53:49,126][0m Trial 25 finished with value: 0.13083718910261435 and parameters: {'observation_period_num': 218, 'train_rates': 0.9605451245035073, 'learning_rate': 1.582010562283971e-05, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9003914338338003}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 16:59:45,308][0m Trial 26 finished with value: 0.12225483936160358 and parameters: {'observation_period_num': 223, 'train_rates': 0.9265403283774402, 'learning_rate': 9.292633656669433e-05, 'batch_size': 42, 'step_size': 10, 'gamma': 0.9459233273581686}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:04:20,533][0m Trial 27 finished with value: 0.4826134874285022 and parameters: {'observation_period_num': 195, 'train_rates': 0.8280944034102862, 'learning_rate': 0.00016351678618693814, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8512128526780609}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:05:46,528][0m Trial 28 finished with value: 0.9633111384892659 and parameters: {'observation_period_num': 9, 'train_rates': 0.7506190239005298, 'learning_rate': 7.325824349930484e-05, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9647004560827719}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:09:52,390][0m Trial 29 finished with value: 0.15472513615372646 and parameters: {'observation_period_num': 121, 'train_rates': 0.9114485956621965, 'learning_rate': 7.905704595713634e-06, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9072736810270386}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:13:38,859][0m Trial 30 finished with value: 0.2360834573154096 and parameters: {'observation_period_num': 152, 'train_rates': 0.9517987469059712, 'learning_rate': 1.217596466662338e-05, 'batch_size': 115, 'step_size': 6, 'gamma': 0.9348009287682087}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:20:10,974][0m Trial 31 finished with value: 0.11117725159094585 and parameters: {'observation_period_num': 235, 'train_rates': 0.9547267950119105, 'learning_rate': 3.195985700381187e-05, 'batch_size': 38, 'step_size': 8, 'gamma': 0.9603983435706974}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:26:21,250][0m Trial 32 finished with value: 0.16866699631134288 and parameters: {'observation_period_num': 231, 'train_rates': 0.9234782972625825, 'learning_rate': 3.157920137111708e-05, 'batch_size': 39, 'step_size': 7, 'gamma': 0.9725233018278615}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:31:29,435][0m Trial 33 finished with value: 0.2756727337837219 and parameters: {'observation_period_num': 193, 'train_rates': 0.966640350851162, 'learning_rate': 4.158670456945402e-06, 'batch_size': 56, 'step_size': 9, 'gamma': 0.9366517983182041}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:37:32,057][0m Trial 34 finished with value: 0.18986656215596706 and parameters: {'observation_period_num': 239, 'train_rates': 0.8962704486740698, 'learning_rate': 5.669345788972007e-05, 'batch_size': 138, 'step_size': 5, 'gamma': 0.9512011064639642}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:44:14,904][0m Trial 35 finished with value: 0.16700803111125898 and parameters: {'observation_period_num': 252, 'train_rates': 0.9429857492382517, 'learning_rate': 0.00019110152441307414, 'batch_size': 80, 'step_size': 11, 'gamma': 0.9238881494884467}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:50:22,027][0m Trial 36 finished with value: 1.7788486733581081 and parameters: {'observation_period_num': 218, 'train_rates': 0.9758508967778712, 'learning_rate': 0.000407719475866637, 'batch_size': 34, 'step_size': 8, 'gamma': 0.9854220643740599}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:52:19,829][0m Trial 37 finished with value: 0.13025248050689697 and parameters: {'observation_period_num': 83, 'train_rates': 0.9896894618484539, 'learning_rate': 0.00011708754484086486, 'batch_size': 162, 'step_size': 12, 'gamma': 0.7952044558995526}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 17:56:50,290][0m Trial 38 finished with value: 0.2644339767353373 and parameters: {'observation_period_num': 185, 'train_rates': 0.8774636525112627, 'learning_rate': 1.8075920187904243e-05, 'batch_size': 58, 'step_size': 10, 'gamma': 0.9182701496009829}. Best is trial 11 with value: 0.11007708890570535.[0m
Early stopping at epoch 88
[32m[I 2025-01-03 18:01:36,076][0m Trial 39 finished with value: 0.6347175240516663 and parameters: {'observation_period_num': 207, 'train_rates': 0.9333523174447919, 'learning_rate': 3.882179651203174e-05, 'batch_size': 197, 'step_size': 1, 'gamma': 0.8732001587654307}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:02:29,168][0m Trial 40 finished with value: 2.298509976550694 and parameters: {'observation_period_num': 47, 'train_rates': 0.6408963412795541, 'learning_rate': 0.000840357794416792, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9705986946630546}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:09:22,629][0m Trial 41 finished with value: 0.12913016981294012 and parameters: {'observation_period_num': 234, 'train_rates': 0.9570852175999104, 'learning_rate': 9.78306898726452e-05, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9579280098921533}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:15:51,358][0m Trial 42 finished with value: 0.11721870962417487 and parameters: {'observation_period_num': 234, 'train_rates': 0.9394381056002343, 'learning_rate': 6.58288180317866e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.939916938895081}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:20:11,015][0m Trial 43 finished with value: 0.1990407812145521 and parameters: {'observation_period_num': 158, 'train_rates': 0.9622592692983601, 'learning_rate': 0.0001948732642534803, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9548246843345887}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:25:37,466][0m Trial 44 finished with value: 0.14376790821552277 and parameters: {'observation_period_num': 213, 'train_rates': 0.9199268585795789, 'learning_rate': 0.00012365164316107397, 'batch_size': 245, 'step_size': 9, 'gamma': 0.9790569735931655}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:31:52,665][0m Trial 45 finished with value: 0.11128691285848617 and parameters: {'observation_period_num': 227, 'train_rates': 0.9782378233317293, 'learning_rate': 2.9273747517460646e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.9272033154794431}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:36:41,726][0m Trial 46 finished with value: 0.13672252496083578 and parameters: {'observation_period_num': 182, 'train_rates': 0.9721748864119685, 'learning_rate': 4.255292190789534e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.9236722374389108}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:42:51,506][0m Trial 47 finished with value: 0.14199985563755035 and parameters: {'observation_period_num': 227, 'train_rates': 0.9783063668751106, 'learning_rate': 2.5672165364194154e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.8521668596910281}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:47:38,426][0m Trial 48 finished with value: 0.6631266450689685 and parameters: {'observation_period_num': 209, 'train_rates': 0.7738380266225218, 'learning_rate': 1.490646527247037e-05, 'batch_size': 90, 'step_size': 11, 'gamma': 0.9064070922607259}. Best is trial 11 with value: 0.11007708890570535.[0m
[32m[I 2025-01-03 18:52:53,100][0m Trial 49 finished with value: 0.9555545056229858 and parameters: {'observation_period_num': 243, 'train_rates': 0.6911980926145705, 'learning_rate': 2.1781567374202705e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.884038821099679}. Best is trial 11 with value: 0.11007708890570535.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-03 18:52:53,106][0m A new study created in memory with name: no-name-931a4729-1f51-4ba3-8852-3408c07b7c6c[0m
[32m[I 2025-01-03 18:56:28,274][0m Trial 0 finished with value: 0.9175167121778715 and parameters: {'observation_period_num': 182, 'train_rates': 0.6748123915599024, 'learning_rate': 0.0005938441508542608, 'batch_size': 130, 'step_size': 3, 'gamma': 0.9148024564180601}. Best is trial 0 with value: 0.9175167121778715.[0m
[32m[I 2025-01-03 18:57:16,966][0m Trial 1 finished with value: 1.2400462694696543 and parameters: {'observation_period_num': 41, 'train_rates': 0.6668839026195797, 'learning_rate': 2.199863468786906e-06, 'batch_size': 119, 'step_size': 8, 'gamma': 0.8952409857241413}. Best is trial 0 with value: 0.9175167121778715.[0m
[32m[I 2025-01-03 18:58:42,049][0m Trial 2 finished with value: 0.6034038340799782 and parameters: {'observation_period_num': 69, 'train_rates': 0.7812711607317273, 'learning_rate': 4.280903886686318e-05, 'batch_size': 171, 'step_size': 15, 'gamma': 0.9878705033737307}. Best is trial 2 with value: 0.6034038340799782.[0m
[32m[I 2025-01-03 19:00:51,957][0m Trial 3 finished with value: 0.9814961850643158 and parameters: {'observation_period_num': 126, 'train_rates': 0.6085926490375715, 'learning_rate': 0.0003826150273819184, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8447543169817543}. Best is trial 2 with value: 0.6034038340799782.[0m
[32m[I 2025-01-03 19:05:30,298][0m Trial 4 finished with value: 0.3189425648827302 and parameters: {'observation_period_num': 195, 'train_rates': 0.8347126537915206, 'learning_rate': 7.680195865327083e-05, 'batch_size': 60, 'step_size': 11, 'gamma': 0.8562845688139674}. Best is trial 4 with value: 0.3189425648827302.[0m
[32m[I 2025-01-03 19:07:18,549][0m Trial 5 finished with value: 0.2987547622250003 and parameters: {'observation_period_num': 77, 'train_rates': 0.9120186481133463, 'learning_rate': 0.0004518640837858104, 'batch_size': 111, 'step_size': 2, 'gamma': 0.8309568694164992}. Best is trial 5 with value: 0.2987547622250003.[0m
[32m[I 2025-01-03 19:10:42,067][0m Trial 6 finished with value: 1.1724584780543683 and parameters: {'observation_period_num': 166, 'train_rates': 0.702226379845318, 'learning_rate': 6.757145324073554e-06, 'batch_size': 64, 'step_size': 3, 'gamma': 0.9342448504470402}. Best is trial 5 with value: 0.2987547622250003.[0m
[32m[I 2025-01-03 19:12:03,541][0m Trial 7 finished with value: 0.9960763774249913 and parameters: {'observation_period_num': 39, 'train_rates': 0.6359689999112665, 'learning_rate': 0.0002317161059401307, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8978058363964997}. Best is trial 5 with value: 0.2987547622250003.[0m
[32m[I 2025-01-03 19:13:57,656][0m Trial 8 finished with value: 0.32980085482681376 and parameters: {'observation_period_num': 85, 'train_rates': 0.8415878557593534, 'learning_rate': 4.7189297183178936e-05, 'batch_size': 89, 'step_size': 7, 'gamma': 0.9035328842940031}. Best is trial 5 with value: 0.2987547622250003.[0m
[32m[I 2025-01-03 19:15:55,295][0m Trial 9 finished with value: 1.3361645027359514 and parameters: {'observation_period_num': 91, 'train_rates': 0.8525875517783958, 'learning_rate': 1.4680651269471024e-06, 'batch_size': 166, 'step_size': 5, 'gamma': 0.8700616790043847}. Best is trial 5 with value: 0.2987547622250003.[0m
Early stopping at epoch 49
[32m[I 2025-01-03 19:19:17,810][0m Trial 10 finished with value: 0.7506557703018188 and parameters: {'observation_period_num': 244, 'train_rates': 0.988769056893622, 'learning_rate': 0.0009657887850397466, 'batch_size': 234, 'step_size': 1, 'gamma': 0.7658654722982841}. Best is trial 5 with value: 0.2987547622250003.[0m
[32m[I 2025-01-03 19:26:07,877][0m Trial 11 finished with value: 0.1297334668790402 and parameters: {'observation_period_num': 229, 'train_rates': 0.9461833095848228, 'learning_rate': 0.00010326217305915478, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8089559757696038}. Best is trial 11 with value: 0.1297334668790402.[0m
[32m[I 2025-01-03 19:33:21,328][0m Trial 12 finished with value: 0.19457708658843204 and parameters: {'observation_period_num': 251, 'train_rates': 0.9680589151155219, 'learning_rate': 0.00015581305171844832, 'batch_size': 24, 'step_size': 12, 'gamma': 0.8000318044191717}. Best is trial 11 with value: 0.1297334668790402.[0m
[32m[I 2025-01-03 19:40:47,673][0m Trial 13 finished with value: 0.12867024115153722 and parameters: {'observation_period_num': 250, 'train_rates': 0.9744255907009642, 'learning_rate': 9.701750764041076e-05, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7828890953609874}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 19:47:11,126][0m Trial 14 finished with value: 0.17458101639560625 and parameters: {'observation_period_num': 221, 'train_rates': 0.9255011585457567, 'learning_rate': 1.5099218354575807e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7549655222212219}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 19:52:45,846][0m Trial 15 finished with value: 0.1498377374788322 and parameters: {'observation_period_num': 213, 'train_rates': 0.9265423312021509, 'learning_rate': 0.00011731718263180087, 'batch_size': 72, 'step_size': 13, 'gamma': 0.8096090770644385}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 19:56:04,542][0m Trial 16 finished with value: 0.7558279899514724 and parameters: {'observation_period_num': 149, 'train_rates': 0.7710182215025152, 'learning_rate': 1.6231297374343294e-05, 'batch_size': 45, 'step_size': 10, 'gamma': 0.7861099332081793}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:01:36,973][0m Trial 17 finished with value: 0.3731625589600491 and parameters: {'observation_period_num': 221, 'train_rates': 0.8889897035430254, 'learning_rate': 2.0037738803546168e-05, 'batch_size': 220, 'step_size': 13, 'gamma': 0.8150769140023963}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:04:34,368][0m Trial 18 finished with value: 0.4963177524085315 and parameters: {'observation_period_num': 124, 'train_rates': 0.962493754148558, 'learning_rate': 5.48431930251414e-06, 'batch_size': 85, 'step_size': 10, 'gamma': 0.7807894952007958}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:09:06,661][0m Trial 19 finished with value: 0.7126575008179378 and parameters: {'observation_period_num': 198, 'train_rates': 0.7381509343346286, 'learning_rate': 8.04667883221664e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.8249229591680526}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:09:29,757][0m Trial 20 finished with value: 0.19595567433814692 and parameters: {'observation_period_num': 9, 'train_rates': 0.8748486032658342, 'learning_rate': 0.0002243275974276588, 'batch_size': 195, 'step_size': 13, 'gamma': 0.7504019263422246}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:15:29,150][0m Trial 21 finished with value: 0.15403348300606012 and parameters: {'observation_period_num': 227, 'train_rates': 0.9354542662741119, 'learning_rate': 0.00011020120165652797, 'batch_size': 70, 'step_size': 13, 'gamma': 0.7906870922461734}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:21:02,299][0m Trial 22 finished with value: 0.14241192741713055 and parameters: {'observation_period_num': 209, 'train_rates': 0.9482645059345497, 'learning_rate': 6.450531116765658e-05, 'batch_size': 51, 'step_size': 10, 'gamma': 0.8052172277971144}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:28:08,078][0m Trial 23 finished with value: 0.15320788323879242 and parameters: {'observation_period_num': 250, 'train_rates': 0.9896786637813683, 'learning_rate': 2.8935051718799335e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.7773139079410185}. Best is trial 13 with value: 0.12867024115153722.[0m
[32m[I 2025-01-03 20:33:00,984][0m Trial 24 finished with value: 0.10924561724175502 and parameters: {'observation_period_num': 170, 'train_rates': 0.9510232077405274, 'learning_rate': 5.6803480664591135e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.84127239286545}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 20:37:34,295][0m Trial 25 finished with value: 0.3211531612215583 and parameters: {'observation_period_num': 169, 'train_rates': 0.8953635241698676, 'learning_rate': 0.00020609558373337142, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8434837417717546}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 20:43:47,545][0m Trial 26 finished with value: 0.34180981522739523 and parameters: {'observation_period_num': 234, 'train_rates': 0.8140802567713142, 'learning_rate': 3.225207067603652e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8683767378097345}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 20:47:21,800][0m Trial 27 finished with value: 0.4468479451940795 and parameters: {'observation_period_num': 152, 'train_rates': 0.8763318491743628, 'learning_rate': 1.189232402657839e-05, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8366400877217773}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 20:52:24,971][0m Trial 28 finished with value: 0.12298224645632284 and parameters: {'observation_period_num': 191, 'train_rates': 0.9609753584498187, 'learning_rate': 5.440738032175804e-05, 'batch_size': 37, 'step_size': 12, 'gamma': 0.876189148475523}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 20:57:13,369][0m Trial 29 finished with value: 0.15297050732705328 and parameters: {'observation_period_num': 181, 'train_rates': 0.9740038022575, 'learning_rate': 8.859329272741302e-06, 'batch_size': 37, 'step_size': 12, 'gamma': 0.9283105581239095}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:00:46,548][0m Trial 30 finished with value: 0.2480321155404145 and parameters: {'observation_period_num': 148, 'train_rates': 0.9057429165756035, 'learning_rate': 2.745300769576758e-05, 'batch_size': 145, 'step_size': 14, 'gamma': 0.8802776676048705}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:06:01,684][0m Trial 31 finished with value: 0.11349110055143816 and parameters: {'observation_period_num': 194, 'train_rates': 0.9506530037568867, 'learning_rate': 4.582218995090699e-05, 'batch_size': 36, 'step_size': 12, 'gamma': 0.8541772402312703}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:10:54,892][0m Trial 32 finished with value: 0.12821613172538407 and parameters: {'observation_period_num': 188, 'train_rates': 0.9527277174545417, 'learning_rate': 6.689424940289512e-05, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8824587633356448}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:15:45,847][0m Trial 33 finished with value: 0.13930387168690778 and parameters: {'observation_period_num': 190, 'train_rates': 0.9502690095796883, 'learning_rate': 5.308244747421326e-05, 'batch_size': 79, 'step_size': 9, 'gamma': 0.8809941874321663}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:20:01,200][0m Trial 34 finished with value: 0.1901568711972704 and parameters: {'observation_period_num': 170, 'train_rates': 0.9268587760004613, 'learning_rate': 3.8114971275807505e-05, 'batch_size': 56, 'step_size': 12, 'gamma': 0.9532524395843543}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:23:11,929][0m Trial 35 finished with value: 0.31149884964537433 and parameters: {'observation_period_num': 137, 'train_rates': 0.8646085246118764, 'learning_rate': 2.3564102387266935e-05, 'batch_size': 115, 'step_size': 14, 'gamma': 0.8542526935557817}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:25:42,624][0m Trial 36 finished with value: 0.3144857915182077 and parameters: {'observation_period_num': 107, 'train_rates': 0.8150116484820594, 'learning_rate': 5.8645149255757844e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.915577332981386}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:30:52,898][0m Trial 37 finished with value: 0.458356042180145 and parameters: {'observation_period_num': 202, 'train_rates': 0.9173568719582197, 'learning_rate': 3.964948044146996e-06, 'batch_size': 100, 'step_size': 15, 'gamma': 0.8592076977058016}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:34:54,966][0m Trial 38 finished with value: 0.2276219449260018 and parameters: {'observation_period_num': 164, 'train_rates': 0.9014224362803236, 'learning_rate': 0.0002995950047235326, 'batch_size': 60, 'step_size': 11, 'gamma': 0.8257878686427741}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:39:28,166][0m Trial 39 finished with value: 0.16464002430438995 and parameters: {'observation_period_num': 182, 'train_rates': 0.9538365090659698, 'learning_rate': 4.04389301784351e-05, 'batch_size': 130, 'step_size': 7, 'gamma': 0.8867238576779964}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:43:28,390][0m Trial 40 finished with value: 0.9150282711865594 and parameters: {'observation_period_num': 186, 'train_rates': 0.709159366430911, 'learning_rate': 0.00015123516100195336, 'batch_size': 34, 'step_size': 9, 'gamma': 0.9119243817995709}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:49:01,676][0m Trial 41 finished with value: 0.11883899196982384 and parameters: {'observation_period_num': 206, 'train_rates': 0.9738011483101813, 'learning_rate': 7.940206268734816e-05, 'batch_size': 51, 'step_size': 12, 'gamma': 0.851520814743263}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:53:40,096][0m Trial 42 finished with value: 0.1407555639743805 and parameters: {'observation_period_num': 174, 'train_rates': 0.9887099337330698, 'learning_rate': 6.443453671678722e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8669439955705457}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 21:59:00,828][0m Trial 43 finished with value: 0.1447314858619421 and parameters: {'observation_period_num': 204, 'train_rates': 0.940720105916803, 'learning_rate': 4.558539205060444e-05, 'batch_size': 64, 'step_size': 11, 'gamma': 0.8531274616623966}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 22:03:17,687][0m Trial 44 finished with value: 0.1421367049636975 and parameters: {'observation_period_num': 158, 'train_rates': 0.9748164749828523, 'learning_rate': 0.0001464965178914829, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8894921876053334}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 22:08:37,856][0m Trial 45 finished with value: 0.1170100852703819 and parameters: {'observation_period_num': 194, 'train_rates': 0.962385949216537, 'learning_rate': 8.4116317699448e-05, 'batch_size': 41, 'step_size': 12, 'gamma': 0.8427389129793074}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 22:12:58,051][0m Trial 46 finished with value: 1.1856649421543055 and parameters: {'observation_period_num': 214, 'train_rates': 0.6206694470367132, 'learning_rate': 0.0006258694545977935, 'batch_size': 42, 'step_size': 8, 'gamma': 0.8392829293155213}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 22:16:10,194][0m Trial 47 finished with value: 0.20811843872070312 and parameters: {'observation_period_num': 135, 'train_rates': 0.9143848668827826, 'learning_rate': 8.878500010754852e-05, 'batch_size': 255, 'step_size': 11, 'gamma': 0.8475226806753832}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 22:22:34,420][0m Trial 48 finished with value: 0.17756031995469873 and parameters: {'observation_period_num': 235, 'train_rates': 0.9635702747612291, 'learning_rate': 0.000358191748132878, 'batch_size': 75, 'step_size': 13, 'gamma': 0.8203354867961853}. Best is trial 24 with value: 0.10924561724175502.[0m
[32m[I 2025-01-03 22:28:00,122][0m Trial 49 finished with value: 0.14802232523297154 and parameters: {'observation_period_num': 200, 'train_rates': 0.9377627667045195, 'learning_rate': 2.0352363646052043e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8328400803869069}. Best is trial 24 with value: 0.10924561724175502.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 219, 'train_rates': 0.9763868659433601, 'learning_rate': 0.0003981821204048228, 'batch_size': 216, 'step_size': 13, 'gamma': 0.8211817606098862}
Epoch 1/300, trend Loss: 1.0078 | 0.9269
Epoch 2/300, trend Loss: 0.9678 | 0.5920
Epoch 3/300, trend Loss: 0.7426 | 0.5142
Epoch 4/300, trend Loss: 0.6710 | 0.5846
Epoch 5/300, trend Loss: 0.5380 | 0.4669
Epoch 6/300, trend Loss: 0.5058 | 0.3969
Epoch 7/300, trend Loss: 0.5793 | 0.4774
Epoch 8/300, trend Loss: 0.5347 | 0.3745
Epoch 9/300, trend Loss: 0.6179 | 0.4279
Epoch 10/300, trend Loss: 0.5162 | 0.3928
Epoch 11/300, trend Loss: 0.5750 | 0.5703
Epoch 12/300, trend Loss: 0.4804 | 0.3809
Epoch 13/300, trend Loss: 0.4310 | 0.3535
Epoch 14/300, trend Loss: 0.3905 | 0.4081
Epoch 15/300, trend Loss: 0.3384 | 0.3164
Epoch 16/300, trend Loss: 0.3072 | 0.3128
Epoch 17/300, trend Loss: 0.3573 | 0.3167
Epoch 18/300, trend Loss: 0.2985 | 0.2813
Epoch 19/300, trend Loss: 0.3004 | 0.2759
Epoch 20/300, trend Loss: 0.2644 | 0.2596
Epoch 21/300, trend Loss: 0.2549 | 0.2633
Epoch 22/300, trend Loss: 0.2410 | 0.2429
Epoch 23/300, trend Loss: 0.2276 | 0.2318
Epoch 24/300, trend Loss: 0.2213 | 0.2219
Epoch 25/300, trend Loss: 0.2168 | 0.2170
Epoch 26/300, trend Loss: 0.2113 | 0.2094
Epoch 27/300, trend Loss: 0.2067 | 0.2065
Epoch 28/300, trend Loss: 0.2013 | 0.2036
Epoch 29/300, trend Loss: 0.1961 | 0.1976
Epoch 30/300, trend Loss: 0.1915 | 0.1963
Epoch 31/300, trend Loss: 0.1883 | 0.1913
Epoch 32/300, trend Loss: 0.1867 | 0.1885
Epoch 33/300, trend Loss: 0.1846 | 0.1839
Epoch 34/300, trend Loss: 0.1830 | 0.1851
Epoch 35/300, trend Loss: 0.1800 | 0.1806
Epoch 36/300, trend Loss: 0.1765 | 0.1783
Epoch 37/300, trend Loss: 0.1741 | 0.1747
Epoch 38/300, trend Loss: 0.1730 | 0.1742
Epoch 39/300, trend Loss: 0.1741 | 0.1720
Epoch 40/300, trend Loss: 0.1770 | 0.1685
Epoch 41/300, trend Loss: 0.1741 | 0.1689
Epoch 42/300, trend Loss: 0.1741 | 0.1656
Epoch 43/300, trend Loss: 0.1709 | 0.1657
Epoch 44/300, trend Loss: 0.1692 | 0.1613
Epoch 45/300, trend Loss: 0.1678 | 0.1651
Epoch 46/300, trend Loss: 0.1656 | 0.1592
Epoch 47/300, trend Loss: 0.1634 | 0.1633
Epoch 48/300, trend Loss: 0.1592 | 0.1575
Epoch 49/300, trend Loss: 0.1579 | 0.1575
Epoch 50/300, trend Loss: 0.1569 | 0.1556
Epoch 51/300, trend Loss: 0.1553 | 0.1546
Epoch 52/300, trend Loss: 0.1546 | 0.1526
Epoch 53/300, trend Loss: 0.1538 | 0.1516
Epoch 54/300, trend Loss: 0.1524 | 0.1517
Epoch 55/300, trend Loss: 0.1521 | 0.1499
Epoch 56/300, trend Loss: 0.1519 | 0.1500
Epoch 57/300, trend Loss: 0.1507 | 0.1482
Epoch 58/300, trend Loss: 0.1506 | 0.1471
Epoch 59/300, trend Loss: 0.1491 | 0.1458
Epoch 60/300, trend Loss: 0.1491 | 0.1450
Epoch 61/300, trend Loss: 0.1485 | 0.1449
Epoch 62/300, trend Loss: 0.1477 | 0.1435
Epoch 63/300, trend Loss: 0.1469 | 0.1427
Epoch 64/300, trend Loss: 0.1465 | 0.1417
Epoch 65/300, trend Loss: 0.1457 | 0.1412
Epoch 66/300, trend Loss: 0.1450 | 0.1399
Epoch 67/300, trend Loss: 0.1448 | 0.1400
Epoch 68/300, trend Loss: 0.1445 | 0.1392
Epoch 69/300, trend Loss: 0.1442 | 0.1377
Epoch 70/300, trend Loss: 0.1430 | 0.1376
Epoch 71/300, trend Loss: 0.1429 | 0.1374
Epoch 72/300, trend Loss: 0.1428 | 0.1370
Epoch 73/300, trend Loss: 0.1422 | 0.1366
Epoch 74/300, trend Loss: 0.1422 | 0.1355
Epoch 75/300, trend Loss: 0.1410 | 0.1355
Epoch 76/300, trend Loss: 0.1406 | 0.1349
Epoch 77/300, trend Loss: 0.1412 | 0.1339
Epoch 78/300, trend Loss: 0.1402 | 0.1335
Epoch 79/300, trend Loss: 0.1399 | 0.1332
Epoch 80/300, trend Loss: 0.1394 | 0.1324
Epoch 81/300, trend Loss: 0.1389 | 0.1323
Epoch 82/300, trend Loss: 0.1390 | 0.1318
Epoch 83/300, trend Loss: 0.1386 | 0.1308
Epoch 84/300, trend Loss: 0.1373 | 0.1306
Epoch 85/300, trend Loss: 0.1378 | 0.1304
Epoch 86/300, trend Loss: 0.1374 | 0.1300
Epoch 87/300, trend Loss: 0.1379 | 0.1293
Epoch 88/300, trend Loss: 0.1370 | 0.1287
Epoch 89/300, trend Loss: 0.1368 | 0.1288
Epoch 90/300, trend Loss: 0.1365 | 0.1287
Epoch 91/300, trend Loss: 0.1363 | 0.1286
Epoch 92/300, trend Loss: 0.1357 | 0.1282
Epoch 93/300, trend Loss: 0.1362 | 0.1276
Epoch 94/300, trend Loss: 0.1355 | 0.1271
Epoch 95/300, trend Loss: 0.1360 | 0.1266
Epoch 96/300, trend Loss: 0.1349 | 0.1260
Epoch 97/300, trend Loss: 0.1345 | 0.1259
Epoch 98/300, trend Loss: 0.1338 | 0.1259
Epoch 99/300, trend Loss: 0.1340 | 0.1259
Epoch 100/300, trend Loss: 0.1340 | 0.1254
Epoch 101/300, trend Loss: 0.1337 | 0.1252
Epoch 102/300, trend Loss: 0.1341 | 0.1247
Epoch 103/300, trend Loss: 0.1337 | 0.1246
Epoch 104/300, trend Loss: 0.1333 | 0.1243
Epoch 105/300, trend Loss: 0.1336 | 0.1244
Epoch 106/300, trend Loss: 0.1333 | 0.1243
Epoch 107/300, trend Loss: 0.1331 | 0.1240
Epoch 108/300, trend Loss: 0.1329 | 0.1235
Epoch 109/300, trend Loss: 0.1329 | 0.1235
Epoch 110/300, trend Loss: 0.1322 | 0.1233
Epoch 111/300, trend Loss: 0.1329 | 0.1231
Epoch 112/300, trend Loss: 0.1325 | 0.1230
Epoch 113/300, trend Loss: 0.1322 | 0.1227
Epoch 114/300, trend Loss: 0.1320 | 0.1227
Epoch 115/300, trend Loss: 0.1321 | 0.1225
Epoch 116/300, trend Loss: 0.1317 | 0.1223
Epoch 117/300, trend Loss: 0.1311 | 0.1220
Epoch 118/300, trend Loss: 0.1312 | 0.1221
Epoch 119/300, trend Loss: 0.1308 | 0.1222
Epoch 120/300, trend Loss: 0.1312 | 0.1219
Epoch 121/300, trend Loss: 0.1305 | 0.1217
Epoch 122/300, trend Loss: 0.1307 | 0.1216
Epoch 123/300, trend Loss: 0.1310 | 0.1216
Epoch 124/300, trend Loss: 0.1310 | 0.1215
Epoch 125/300, trend Loss: 0.1302 | 0.1214
Epoch 126/300, trend Loss: 0.1305 | 0.1213
Epoch 127/300, trend Loss: 0.1303 | 0.1212
Epoch 128/300, trend Loss: 0.1305 | 0.1209
Epoch 129/300, trend Loss: 0.1303 | 0.1208
Epoch 130/300, trend Loss: 0.1310 | 0.1208
Epoch 131/300, trend Loss: 0.1293 | 0.1206
Epoch 132/300, trend Loss: 0.1296 | 0.1203
Epoch 133/300, trend Loss: 0.1302 | 0.1202
Epoch 134/300, trend Loss: 0.1302 | 0.1201
Epoch 135/300, trend Loss: 0.1296 | 0.1199
Epoch 136/300, trend Loss: 0.1286 | 0.1198
Epoch 137/300, trend Loss: 0.1295 | 0.1198
Epoch 138/300, trend Loss: 0.1300 | 0.1198
Epoch 139/300, trend Loss: 0.1290 | 0.1197
Epoch 140/300, trend Loss: 0.1293 | 0.1196
Epoch 141/300, trend Loss: 0.1298 | 0.1196
Epoch 142/300, trend Loss: 0.1286 | 0.1195
Epoch 143/300, trend Loss: 0.1291 | 0.1195
Epoch 144/300, trend Loss: 0.1293 | 0.1195
Epoch 145/300, trend Loss: 0.1292 | 0.1196
Epoch 146/300, trend Loss: 0.1293 | 0.1195
Epoch 147/300, trend Loss: 0.1294 | 0.1194
Epoch 148/300, trend Loss: 0.1288 | 0.1193
Epoch 149/300, trend Loss: 0.1282 | 0.1192
Epoch 150/300, trend Loss: 0.1285 | 0.1190
Epoch 151/300, trend Loss: 0.1284 | 0.1190
Epoch 152/300, trend Loss: 0.1283 | 0.1190
Epoch 153/300, trend Loss: 0.1286 | 0.1189
Epoch 154/300, trend Loss: 0.1289 | 0.1189
Epoch 155/300, trend Loss: 0.1287 | 0.1188
Epoch 156/300, trend Loss: 0.1285 | 0.1187
Epoch 157/300, trend Loss: 0.1280 | 0.1186
Epoch 158/300, trend Loss: 0.1281 | 0.1185
Epoch 159/300, trend Loss: 0.1279 | 0.1185
Epoch 160/300, trend Loss: 0.1282 | 0.1185
Epoch 161/300, trend Loss: 0.1284 | 0.1184
Epoch 162/300, trend Loss: 0.1282 | 0.1185
Epoch 163/300, trend Loss: 0.1281 | 0.1185
Epoch 164/300, trend Loss: 0.1286 | 0.1185
Epoch 165/300, trend Loss: 0.1282 | 0.1185
Epoch 166/300, trend Loss: 0.1292 | 0.1185
Epoch 167/300, trend Loss: 0.1280 | 0.1184
Epoch 168/300, trend Loss: 0.1281 | 0.1184
Epoch 169/300, trend Loss: 0.1279 | 0.1183
Epoch 170/300, trend Loss: 0.1285 | 0.1182
Epoch 171/300, trend Loss: 0.1288 | 0.1182
Epoch 172/300, trend Loss: 0.1278 | 0.1182
Epoch 173/300, trend Loss: 0.1279 | 0.1181
Epoch 174/300, trend Loss: 0.1283 | 0.1181
Epoch 175/300, trend Loss: 0.1284 | 0.1180
Epoch 176/300, trend Loss: 0.1280 | 0.1179
Epoch 177/300, trend Loss: 0.1279 | 0.1179
Epoch 178/300, trend Loss: 0.1280 | 0.1179
Epoch 179/300, trend Loss: 0.1278 | 0.1179
Epoch 180/300, trend Loss: 0.1279 | 0.1179
Epoch 181/300, trend Loss: 0.1278 | 0.1179
Epoch 182/300, trend Loss: 0.1272 | 0.1179
Epoch 183/300, trend Loss: 0.1281 | 0.1179
Epoch 184/300, trend Loss: 0.1276 | 0.1179
Epoch 185/300, trend Loss: 0.1273 | 0.1179
Epoch 186/300, trend Loss: 0.1274 | 0.1178
Epoch 187/300, trend Loss: 0.1276 | 0.1178
Epoch 188/300, trend Loss: 0.1277 | 0.1178
Epoch 189/300, trend Loss: 0.1280 | 0.1178
Epoch 190/300, trend Loss: 0.1283 | 0.1177
Epoch 191/300, trend Loss: 0.1279 | 0.1177
Epoch 192/300, trend Loss: 0.1271 | 0.1176
Epoch 193/300, trend Loss: 0.1279 | 0.1176
Epoch 194/300, trend Loss: 0.1276 | 0.1176
Epoch 195/300, trend Loss: 0.1276 | 0.1176
Epoch 196/300, trend Loss: 0.1277 | 0.1175
Epoch 197/300, trend Loss: 0.1269 | 0.1175
Epoch 198/300, trend Loss: 0.1275 | 0.1174
Epoch 199/300, trend Loss: 0.1278 | 0.1175
Epoch 200/300, trend Loss: 0.1265 | 0.1175
Epoch 201/300, trend Loss: 0.1274 | 0.1174
Epoch 202/300, trend Loss: 0.1279 | 0.1174
Epoch 203/300, trend Loss: 0.1277 | 0.1174
Epoch 204/300, trend Loss: 0.1270 | 0.1173
Epoch 205/300, trend Loss: 0.1274 | 0.1173
Epoch 206/300, trend Loss: 0.1277 | 0.1173
Epoch 207/300, trend Loss: 0.1283 | 0.1173
Epoch 208/300, trend Loss: 0.1271 | 0.1173
Epoch 209/300, trend Loss: 0.1274 | 0.1173
Epoch 210/300, trend Loss: 0.1276 | 0.1173
Epoch 211/300, trend Loss: 0.1277 | 0.1173
Epoch 212/300, trend Loss: 0.1271 | 0.1173
Epoch 213/300, trend Loss: 0.1278 | 0.1172
Epoch 214/300, trend Loss: 0.1272 | 0.1172
Epoch 215/300, trend Loss: 0.1274 | 0.1172
Epoch 216/300, trend Loss: 0.1274 | 0.1172
Epoch 217/300, trend Loss: 0.1277 | 0.1172
Epoch 218/300, trend Loss: 0.1270 | 0.1172
Epoch 219/300, trend Loss: 0.1276 | 0.1171
Epoch 220/300, trend Loss: 0.1268 | 0.1171
Epoch 221/300, trend Loss: 0.1270 | 0.1171
Epoch 222/300, trend Loss: 0.1275 | 0.1171
Epoch 223/300, trend Loss: 0.1273 | 0.1171
Epoch 224/300, trend Loss: 0.1268 | 0.1171
Epoch 225/300, trend Loss: 0.1275 | 0.1171
Epoch 226/300, trend Loss: 0.1265 | 0.1171
Epoch 227/300, trend Loss: 0.1275 | 0.1171
Epoch 228/300, trend Loss: 0.1270 | 0.1171
Epoch 229/300, trend Loss: 0.1277 | 0.1171
Epoch 230/300, trend Loss: 0.1269 | 0.1171
Epoch 231/300, trend Loss: 0.1270 | 0.1171
Epoch 232/300, trend Loss: 0.1275 | 0.1171
Epoch 233/300, trend Loss: 0.1273 | 0.1170
Epoch 234/300, trend Loss: 0.1275 | 0.1170
Epoch 235/300, trend Loss: 0.1265 | 0.1170
Epoch 236/300, trend Loss: 0.1273 | 0.1170
Epoch 237/300, trend Loss: 0.1265 | 0.1170
Epoch 238/300, trend Loss: 0.1274 | 0.1170
Epoch 239/300, trend Loss: 0.1274 | 0.1170
Epoch 240/300, trend Loss: 0.1274 | 0.1170
Epoch 241/300, trend Loss: 0.1263 | 0.1170
Epoch 242/300, trend Loss: 0.1273 | 0.1170
Epoch 243/300, trend Loss: 0.1270 | 0.1170
Epoch 244/300, trend Loss: 0.1266 | 0.1170
Epoch 245/300, trend Loss: 0.1273 | 0.1170
Epoch 246/300, trend Loss: 0.1272 | 0.1170
Epoch 247/300, trend Loss: 0.1272 | 0.1170
Epoch 248/300, trend Loss: 0.1270 | 0.1170
Epoch 249/300, trend Loss: 0.1268 | 0.1170
Epoch 250/300, trend Loss: 0.1271 | 0.1170
Epoch 251/300, trend Loss: 0.1270 | 0.1170
Epoch 252/300, trend Loss: 0.1272 | 0.1170
Epoch 253/300, trend Loss: 0.1273 | 0.1169
Epoch 254/300, trend Loss: 0.1267 | 0.1169
Epoch 255/300, trend Loss: 0.1270 | 0.1169
Epoch 256/300, trend Loss: 0.1264 | 0.1169
Epoch 257/300, trend Loss: 0.1272 | 0.1169
Epoch 258/300, trend Loss: 0.1272 | 0.1169
Epoch 259/300, trend Loss: 0.1268 | 0.1169
Epoch 260/300, trend Loss: 0.1269 | 0.1169
Epoch 261/300, trend Loss: 0.1276 | 0.1169
Epoch 262/300, trend Loss: 0.1268 | 0.1169
Epoch 263/300, trend Loss: 0.1272 | 0.1169
Epoch 264/300, trend Loss: 0.1272 | 0.1169
Epoch 265/300, trend Loss: 0.1270 | 0.1169
Epoch 266/300, trend Loss: 0.1270 | 0.1169
Epoch 267/300, trend Loss: 0.1265 | 0.1169
Epoch 268/300, trend Loss: 0.1268 | 0.1169
Epoch 269/300, trend Loss: 0.1268 | 0.1169
Epoch 270/300, trend Loss: 0.1271 | 0.1169
Epoch 271/300, trend Loss: 0.1271 | 0.1169
Epoch 272/300, trend Loss: 0.1260 | 0.1169
Epoch 273/300, trend Loss: 0.1268 | 0.1169
Epoch 274/300, trend Loss: 0.1268 | 0.1169
Epoch 275/300, trend Loss: 0.1270 | 0.1169
Epoch 276/300, trend Loss: 0.1269 | 0.1169
Epoch 277/300, trend Loss: 0.1276 | 0.1169
Epoch 278/300, trend Loss: 0.1273 | 0.1169
Epoch 279/300, trend Loss: 0.1269 | 0.1169
Epoch 280/300, trend Loss: 0.1272 | 0.1169
Epoch 281/300, trend Loss: 0.1269 | 0.1169
Epoch 282/300, trend Loss: 0.1267 | 0.1169
Epoch 283/300, trend Loss: 0.1272 | 0.1169
Epoch 284/300, trend Loss: 0.1268 | 0.1169
Epoch 285/300, trend Loss: 0.1269 | 0.1169
Epoch 286/300, trend Loss: 0.1269 | 0.1169
Epoch 287/300, trend Loss: 0.1268 | 0.1169
Epoch 288/300, trend Loss: 0.1267 | 0.1169
Epoch 289/300, trend Loss: 0.1273 | 0.1169
Epoch 290/300, trend Loss: 0.1271 | 0.1169
Epoch 291/300, trend Loss: 0.1269 | 0.1169
Epoch 292/300, trend Loss: 0.1273 | 0.1169
Epoch 293/300, trend Loss: 0.1264 | 0.1169
Epoch 294/300, trend Loss: 0.1269 | 0.1169
Epoch 295/300, trend Loss: 0.1270 | 0.1169
Epoch 296/300, trend Loss: 0.1270 | 0.1169
Epoch 297/300, trend Loss: 0.1264 | 0.1169
Epoch 298/300, trend Loss: 0.1272 | 0.1169
Epoch 299/300, trend Loss: 0.1268 | 0.1169
Epoch 300/300, trend Loss: 0.1266 | 0.1169
Training seasonal_0 component with params: {'observation_period_num': 215, 'train_rates': 0.9757300504919704, 'learning_rate': 0.0002796328748352246, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8930159786690934}
Epoch 1/300, seasonal_0 Loss: 0.9807 | 0.9744
Epoch 2/300, seasonal_0 Loss: 0.7830 | 0.6492
Epoch 3/300, seasonal_0 Loss: 0.6251 | 0.6079
Epoch 4/300, seasonal_0 Loss: 0.5672 | 0.5430
Epoch 5/300, seasonal_0 Loss: 0.6812 | 0.6573
Epoch 6/300, seasonal_0 Loss: 0.5261 | 0.4750
Epoch 7/300, seasonal_0 Loss: 0.4107 | 0.4479
Epoch 8/300, seasonal_0 Loss: 0.3702 | 0.4090
Epoch 9/300, seasonal_0 Loss: 0.3742 | 0.3972
Epoch 10/300, seasonal_0 Loss: 0.3741 | 0.4502
Epoch 11/300, seasonal_0 Loss: 0.3148 | 0.3387
Epoch 12/300, seasonal_0 Loss: 0.3442 | 0.3070
Epoch 13/300, seasonal_0 Loss: 0.2853 | 0.2745
Epoch 14/300, seasonal_0 Loss: 0.2534 | 0.2689
Epoch 15/300, seasonal_0 Loss: 0.2261 | 0.2341
Epoch 16/300, seasonal_0 Loss: 0.2161 | 0.2323
Epoch 17/300, seasonal_0 Loss: 0.2201 | 0.2167
Epoch 18/300, seasonal_0 Loss: 0.2280 | 0.2294
Epoch 19/300, seasonal_0 Loss: 0.2439 | 0.2017
Epoch 20/300, seasonal_0 Loss: 0.2184 | 0.2031
Epoch 21/300, seasonal_0 Loss: 0.2273 | 0.2004
Epoch 22/300, seasonal_0 Loss: 0.2154 | 0.2121
Epoch 23/300, seasonal_0 Loss: 0.2257 | 0.2005
Epoch 24/300, seasonal_0 Loss: 0.2485 | 0.2042
Epoch 25/300, seasonal_0 Loss: 0.2411 | 0.1987
Epoch 26/300, seasonal_0 Loss: 0.2507 | 0.2090
Epoch 27/300, seasonal_0 Loss: 0.2062 | 0.2114
Epoch 28/300, seasonal_0 Loss: 0.1985 | 0.1839
Epoch 29/300, seasonal_0 Loss: 0.1890 | 0.1806
Epoch 30/300, seasonal_0 Loss: 0.1838 | 0.1724
Epoch 31/300, seasonal_0 Loss: 0.1847 | 0.1783
Epoch 32/300, seasonal_0 Loss: 0.1853 | 0.1660
Epoch 33/300, seasonal_0 Loss: 0.1807 | 0.1745
Epoch 34/300, seasonal_0 Loss: 0.1751 | 0.1640
Epoch 35/300, seasonal_0 Loss: 0.1725 | 0.1634
Epoch 36/300, seasonal_0 Loss: 0.1694 | 0.1625
Epoch 37/300, seasonal_0 Loss: 0.1677 | 0.1563
Epoch 38/300, seasonal_0 Loss: 0.1700 | 0.1554
Epoch 39/300, seasonal_0 Loss: 0.1667 | 0.1560
Epoch 40/300, seasonal_0 Loss: 0.1663 | 0.1500
Epoch 41/300, seasonal_0 Loss: 0.1626 | 0.1541
Epoch 42/300, seasonal_0 Loss: 0.1594 | 0.1483
Epoch 43/300, seasonal_0 Loss: 0.1566 | 0.1486
Epoch 44/300, seasonal_0 Loss: 0.1565 | 0.1472
Epoch 45/300, seasonal_0 Loss: 0.1561 | 0.1450
Epoch 46/300, seasonal_0 Loss: 0.1562 | 0.1463
Epoch 47/300, seasonal_0 Loss: 0.1575 | 0.1433
Epoch 48/300, seasonal_0 Loss: 0.1582 | 0.1452
Epoch 49/300, seasonal_0 Loss: 0.1562 | 0.1416
Epoch 50/300, seasonal_0 Loss: 0.1546 | 0.1421
Epoch 51/300, seasonal_0 Loss: 0.1522 | 0.1397
Epoch 52/300, seasonal_0 Loss: 0.1505 | 0.1413
Epoch 53/300, seasonal_0 Loss: 0.1496 | 0.1373
Epoch 54/300, seasonal_0 Loss: 0.1496 | 0.1383
Epoch 55/300, seasonal_0 Loss: 0.1481 | 0.1353
Epoch 56/300, seasonal_0 Loss: 0.1474 | 0.1362
Epoch 57/300, seasonal_0 Loss: 0.1459 | 0.1338
Epoch 58/300, seasonal_0 Loss: 0.1450 | 0.1345
Epoch 59/300, seasonal_0 Loss: 0.1441 | 0.1323
Epoch 60/300, seasonal_0 Loss: 0.1441 | 0.1330
Epoch 61/300, seasonal_0 Loss: 0.1423 | 0.1314
Epoch 62/300, seasonal_0 Loss: 0.1417 | 0.1308
Epoch 63/300, seasonal_0 Loss: 0.1425 | 0.1302
Epoch 64/300, seasonal_0 Loss: 0.1404 | 0.1291
Epoch 65/300, seasonal_0 Loss: 0.1401 | 0.1289
Epoch 66/300, seasonal_0 Loss: 0.1395 | 0.1281
Epoch 67/300, seasonal_0 Loss: 0.1386 | 0.1282
Epoch 68/300, seasonal_0 Loss: 0.1380 | 0.1262
Epoch 69/300, seasonal_0 Loss: 0.1373 | 0.1260
Epoch 70/300, seasonal_0 Loss: 0.1368 | 0.1255
Epoch 71/300, seasonal_0 Loss: 0.1360 | 0.1246
Epoch 72/300, seasonal_0 Loss: 0.1360 | 0.1239
Epoch 73/300, seasonal_0 Loss: 0.1363 | 0.1244
Epoch 74/300, seasonal_0 Loss: 0.1351 | 0.1229
Epoch 75/300, seasonal_0 Loss: 0.1342 | 0.1229
Epoch 76/300, seasonal_0 Loss: 0.1344 | 0.1222
Epoch 77/300, seasonal_0 Loss: 0.1336 | 0.1215
Epoch 78/300, seasonal_0 Loss: 0.1340 | 0.1223
Epoch 79/300, seasonal_0 Loss: 0.1345 | 0.1205
Epoch 80/300, seasonal_0 Loss: 0.1351 | 0.1221
Epoch 81/300, seasonal_0 Loss: 0.1358 | 0.1205
Epoch 82/300, seasonal_0 Loss: 0.1361 | 0.1223
Epoch 83/300, seasonal_0 Loss: 0.1358 | 0.1192
Epoch 84/300, seasonal_0 Loss: 0.1344 | 0.1217
Epoch 85/300, seasonal_0 Loss: 0.1323 | 0.1183
Epoch 86/300, seasonal_0 Loss: 0.1307 | 0.1192
Epoch 87/300, seasonal_0 Loss: 0.1298 | 0.1173
Epoch 88/300, seasonal_0 Loss: 0.1293 | 0.1190
Epoch 89/300, seasonal_0 Loss: 0.1281 | 0.1174
Epoch 90/300, seasonal_0 Loss: 0.1275 | 0.1178
Epoch 91/300, seasonal_0 Loss: 0.1283 | 0.1166
Epoch 92/300, seasonal_0 Loss: 0.1276 | 0.1164
Epoch 93/300, seasonal_0 Loss: 0.1265 | 0.1166
Epoch 94/300, seasonal_0 Loss: 0.1265 | 0.1161
Epoch 95/300, seasonal_0 Loss: 0.1257 | 0.1163
Epoch 96/300, seasonal_0 Loss: 0.1260 | 0.1153
Epoch 97/300, seasonal_0 Loss: 0.1260 | 0.1149
Epoch 98/300, seasonal_0 Loss: 0.1259 | 0.1148
Epoch 99/300, seasonal_0 Loss: 0.1256 | 0.1154
Epoch 100/300, seasonal_0 Loss: 0.1250 | 0.1146
Epoch 101/300, seasonal_0 Loss: 0.1244 | 0.1145
Epoch 102/300, seasonal_0 Loss: 0.1247 | 0.1144
Epoch 103/300, seasonal_0 Loss: 0.1245 | 0.1143
Epoch 104/300, seasonal_0 Loss: 0.1241 | 0.1134
Epoch 105/300, seasonal_0 Loss: 0.1236 | 0.1137
Epoch 106/300, seasonal_0 Loss: 0.1233 | 0.1130
Epoch 107/300, seasonal_0 Loss: 0.1237 | 0.1135
Epoch 108/300, seasonal_0 Loss: 0.1231 | 0.1130
Epoch 109/300, seasonal_0 Loss: 0.1231 | 0.1135
Epoch 110/300, seasonal_0 Loss: 0.1227 | 0.1129
Epoch 111/300, seasonal_0 Loss: 0.1229 | 0.1132
Epoch 112/300, seasonal_0 Loss: 0.1224 | 0.1123
Epoch 113/300, seasonal_0 Loss: 0.1219 | 0.1125
Epoch 114/300, seasonal_0 Loss: 0.1214 | 0.1119
Epoch 115/300, seasonal_0 Loss: 0.1215 | 0.1122
Epoch 116/300, seasonal_0 Loss: 0.1218 | 0.1120
Epoch 117/300, seasonal_0 Loss: 0.1217 | 0.1119
Epoch 118/300, seasonal_0 Loss: 0.1213 | 0.1116
Epoch 119/300, seasonal_0 Loss: 0.1209 | 0.1115
Epoch 120/300, seasonal_0 Loss: 0.1205 | 0.1116
Epoch 121/300, seasonal_0 Loss: 0.1200 | 0.1116
Epoch 122/300, seasonal_0 Loss: 0.1198 | 0.1111
Epoch 123/300, seasonal_0 Loss: 0.1199 | 0.1108
Epoch 124/300, seasonal_0 Loss: 0.1196 | 0.1106
Epoch 125/300, seasonal_0 Loss: 0.1197 | 0.1106
Epoch 126/300, seasonal_0 Loss: 0.1198 | 0.1104
Epoch 127/300, seasonal_0 Loss: 0.1190 | 0.1110
Epoch 128/300, seasonal_0 Loss: 0.1187 | 0.1102
Epoch 129/300, seasonal_0 Loss: 0.1182 | 0.1100
Epoch 130/300, seasonal_0 Loss: 0.1183 | 0.1098
Epoch 131/300, seasonal_0 Loss: 0.1192 | 0.1099
Epoch 132/300, seasonal_0 Loss: 0.1179 | 0.1106
Epoch 133/300, seasonal_0 Loss: 0.1178 | 0.1097
Epoch 134/300, seasonal_0 Loss: 0.1185 | 0.1097
Epoch 135/300, seasonal_0 Loss: 0.1180 | 0.1098
Epoch 136/300, seasonal_0 Loss: 0.1174 | 0.1099
Epoch 137/300, seasonal_0 Loss: 0.1179 | 0.1100
Epoch 138/300, seasonal_0 Loss: 0.1178 | 0.1093
Epoch 139/300, seasonal_0 Loss: 0.1174 | 0.1096
Epoch 140/300, seasonal_0 Loss: 0.1170 | 0.1094
Epoch 141/300, seasonal_0 Loss: 0.1172 | 0.1090
Epoch 142/300, seasonal_0 Loss: 0.1174 | 0.1090
Epoch 143/300, seasonal_0 Loss: 0.1163 | 0.1088
Epoch 144/300, seasonal_0 Loss: 0.1160 | 0.1084
Epoch 145/300, seasonal_0 Loss: 0.1170 | 0.1083
Epoch 146/300, seasonal_0 Loss: 0.1162 | 0.1086
Epoch 147/300, seasonal_0 Loss: 0.1162 | 0.1088
Epoch 148/300, seasonal_0 Loss: 0.1160 | 0.1084
Epoch 149/300, seasonal_0 Loss: 0.1162 | 0.1079
Epoch 150/300, seasonal_0 Loss: 0.1161 | 0.1077
Epoch 151/300, seasonal_0 Loss: 0.1160 | 0.1081
Epoch 152/300, seasonal_0 Loss: 0.1156 | 0.1081
Epoch 153/300, seasonal_0 Loss: 0.1155 | 0.1073
Epoch 154/300, seasonal_0 Loss: 0.1159 | 0.1073
Epoch 155/300, seasonal_0 Loss: 0.1150 | 0.1075
Epoch 156/300, seasonal_0 Loss: 0.1152 | 0.1082
Epoch 157/300, seasonal_0 Loss: 0.1155 | 0.1076
Epoch 158/300, seasonal_0 Loss: 0.1145 | 0.1071
Epoch 159/300, seasonal_0 Loss: 0.1144 | 0.1078
Epoch 160/300, seasonal_0 Loss: 0.1148 | 0.1081
Epoch 161/300, seasonal_0 Loss: 0.1147 | 0.1073
Epoch 162/300, seasonal_0 Loss: 0.1140 | 0.1069
Epoch 163/300, seasonal_0 Loss: 0.1146 | 0.1068
Epoch 164/300, seasonal_0 Loss: 0.1141 | 0.1070
Epoch 165/300, seasonal_0 Loss: 0.1145 | 0.1070
Epoch 166/300, seasonal_0 Loss: 0.1143 | 0.1069
Epoch 167/300, seasonal_0 Loss: 0.1140 | 0.1068
Epoch 168/300, seasonal_0 Loss: 0.1135 | 0.1068
Epoch 169/300, seasonal_0 Loss: 0.1134 | 0.1068
Epoch 170/300, seasonal_0 Loss: 0.1139 | 0.1063
Epoch 171/300, seasonal_0 Loss: 0.1131 | 0.1064
Epoch 172/300, seasonal_0 Loss: 0.1132 | 0.1066
Epoch 173/300, seasonal_0 Loss: 0.1131 | 0.1071
Epoch 174/300, seasonal_0 Loss: 0.1132 | 0.1072
Epoch 175/300, seasonal_0 Loss: 0.1128 | 0.1069
Epoch 176/300, seasonal_0 Loss: 0.1129 | 0.1068
Epoch 177/300, seasonal_0 Loss: 0.1136 | 0.1068
Epoch 178/300, seasonal_0 Loss: 0.1134 | 0.1068
Epoch 179/300, seasonal_0 Loss: 0.1128 | 0.1066
Epoch 180/300, seasonal_0 Loss: 0.1131 | 0.1063
Epoch 181/300, seasonal_0 Loss: 0.1129 | 0.1065
Epoch 182/300, seasonal_0 Loss: 0.1126 | 0.1064
Epoch 183/300, seasonal_0 Loss: 0.1122 | 0.1066
Epoch 184/300, seasonal_0 Loss: 0.1126 | 0.1062
Epoch 185/300, seasonal_0 Loss: 0.1124 | 0.1061
Epoch 186/300, seasonal_0 Loss: 0.1136 | 0.1060
Epoch 187/300, seasonal_0 Loss: 0.1128 | 0.1058
Epoch 188/300, seasonal_0 Loss: 0.1126 | 0.1059
Epoch 189/300, seasonal_0 Loss: 0.1125 | 0.1059
Epoch 190/300, seasonal_0 Loss: 0.1120 | 0.1059
Epoch 191/300, seasonal_0 Loss: 0.1119 | 0.1056
Epoch 192/300, seasonal_0 Loss: 0.1120 | 0.1055
Epoch 193/300, seasonal_0 Loss: 0.1119 | 0.1058
Epoch 194/300, seasonal_0 Loss: 0.1120 | 0.1061
Epoch 195/300, seasonal_0 Loss: 0.1124 | 0.1060
Epoch 196/300, seasonal_0 Loss: 0.1120 | 0.1061
Epoch 197/300, seasonal_0 Loss: 0.1120 | 0.1060
Epoch 198/300, seasonal_0 Loss: 0.1116 | 0.1056
Epoch 199/300, seasonal_0 Loss: 0.1117 | 0.1054
Epoch 200/300, seasonal_0 Loss: 0.1117 | 0.1055
Epoch 201/300, seasonal_0 Loss: 0.1112 | 0.1053
Epoch 202/300, seasonal_0 Loss: 0.1124 | 0.1053
Epoch 203/300, seasonal_0 Loss: 0.1108 | 0.1053
Epoch 204/300, seasonal_0 Loss: 0.1109 | 0.1051
Epoch 205/300, seasonal_0 Loss: 0.1119 | 0.1051
Epoch 206/300, seasonal_0 Loss: 0.1122 | 0.1050
Epoch 207/300, seasonal_0 Loss: 0.1113 | 0.1048
Epoch 208/300, seasonal_0 Loss: 0.1110 | 0.1048
Epoch 209/300, seasonal_0 Loss: 0.1108 | 0.1048
Epoch 210/300, seasonal_0 Loss: 0.1110 | 0.1047
Epoch 211/300, seasonal_0 Loss: 0.1114 | 0.1050
Epoch 212/300, seasonal_0 Loss: 0.1110 | 0.1051
Epoch 213/300, seasonal_0 Loss: 0.1116 | 0.1048
Epoch 214/300, seasonal_0 Loss: 0.1111 | 0.1046
Epoch 215/300, seasonal_0 Loss: 0.1105 | 0.1047
Epoch 216/300, seasonal_0 Loss: 0.1108 | 0.1048
Epoch 217/300, seasonal_0 Loss: 0.1111 | 0.1049
Epoch 218/300, seasonal_0 Loss: 0.1108 | 0.1048
Epoch 219/300, seasonal_0 Loss: 0.1104 | 0.1047
Epoch 220/300, seasonal_0 Loss: 0.1101 | 0.1046
Epoch 221/300, seasonal_0 Loss: 0.1109 | 0.1044
Epoch 222/300, seasonal_0 Loss: 0.1103 | 0.1047
Epoch 223/300, seasonal_0 Loss: 0.1100 | 0.1050
Epoch 224/300, seasonal_0 Loss: 0.1099 | 0.1046
Epoch 225/300, seasonal_0 Loss: 0.1106 | 0.1043
Epoch 226/300, seasonal_0 Loss: 0.1105 | 0.1044
Epoch 227/300, seasonal_0 Loss: 0.1096 | 0.1044
Epoch 228/300, seasonal_0 Loss: 0.1104 | 0.1042
Epoch 229/300, seasonal_0 Loss: 0.1100 | 0.1042
Epoch 230/300, seasonal_0 Loss: 0.1108 | 0.1044
Epoch 231/300, seasonal_0 Loss: 0.1100 | 0.1047
Epoch 232/300, seasonal_0 Loss: 0.1103 | 0.1046
Epoch 233/300, seasonal_0 Loss: 0.1100 | 0.1045
Epoch 234/300, seasonal_0 Loss: 0.1107 | 0.1044
Epoch 235/300, seasonal_0 Loss: 0.1100 | 0.1043
Epoch 236/300, seasonal_0 Loss: 0.1100 | 0.1044
Epoch 237/300, seasonal_0 Loss: 0.1099 | 0.1043
Epoch 238/300, seasonal_0 Loss: 0.1097 | 0.1043
Epoch 239/300, seasonal_0 Loss: 0.1097 | 0.1042
Epoch 240/300, seasonal_0 Loss: 0.1094 | 0.1041
Epoch 241/300, seasonal_0 Loss: 0.1097 | 0.1041
Epoch 242/300, seasonal_0 Loss: 0.1093 | 0.1043
Epoch 243/300, seasonal_0 Loss: 0.1100 | 0.1044
Epoch 244/300, seasonal_0 Loss: 0.1092 | 0.1043
Epoch 245/300, seasonal_0 Loss: 0.1104 | 0.1041
Epoch 246/300, seasonal_0 Loss: 0.1098 | 0.1040
Epoch 247/300, seasonal_0 Loss: 0.1096 | 0.1040
Epoch 248/300, seasonal_0 Loss: 0.1097 | 0.1040
Epoch 249/300, seasonal_0 Loss: 0.1097 | 0.1039
Epoch 250/300, seasonal_0 Loss: 0.1097 | 0.1039
Epoch 251/300, seasonal_0 Loss: 0.1097 | 0.1040
Epoch 252/300, seasonal_0 Loss: 0.1102 | 0.1039
Epoch 253/300, seasonal_0 Loss: 0.1095 | 0.1039
Epoch 254/300, seasonal_0 Loss: 0.1092 | 0.1039
Epoch 255/300, seasonal_0 Loss: 0.1091 | 0.1038
Epoch 256/300, seasonal_0 Loss: 0.1093 | 0.1038
Epoch 257/300, seasonal_0 Loss: 0.1099 | 0.1040
Epoch 258/300, seasonal_0 Loss: 0.1097 | 0.1039
Epoch 259/300, seasonal_0 Loss: 0.1089 | 0.1037
Epoch 260/300, seasonal_0 Loss: 0.1091 | 0.1038
Epoch 261/300, seasonal_0 Loss: 0.1094 | 0.1038
Epoch 262/300, seasonal_0 Loss: 0.1092 | 0.1038
Epoch 263/300, seasonal_0 Loss: 0.1087 | 0.1037
Epoch 264/300, seasonal_0 Loss: 0.1093 | 0.1037
Epoch 265/300, seasonal_0 Loss: 0.1094 | 0.1037
Epoch 266/300, seasonal_0 Loss: 0.1088 | 0.1038
Epoch 267/300, seasonal_0 Loss: 0.1096 | 0.1038
Epoch 268/300, seasonal_0 Loss: 0.1090 | 0.1038
Epoch 269/300, seasonal_0 Loss: 0.1095 | 0.1039
Epoch 270/300, seasonal_0 Loss: 0.1092 | 0.1039
Epoch 271/300, seasonal_0 Loss: 0.1093 | 0.1038
Epoch 272/300, seasonal_0 Loss: 0.1091 | 0.1038
Epoch 273/300, seasonal_0 Loss: 0.1087 | 0.1039
Epoch 274/300, seasonal_0 Loss: 0.1087 | 0.1039
Epoch 275/300, seasonal_0 Loss: 0.1094 | 0.1038
Epoch 276/300, seasonal_0 Loss: 0.1093 | 0.1039
Epoch 277/300, seasonal_0 Loss: 0.1093 | 0.1038
Epoch 278/300, seasonal_0 Loss: 0.1090 | 0.1038
Epoch 279/300, seasonal_0 Loss: 0.1092 | 0.1037
Epoch 280/300, seasonal_0 Loss: 0.1092 | 0.1038
Epoch 281/300, seasonal_0 Loss: 0.1086 | 0.1037
Epoch 282/300, seasonal_0 Loss: 0.1089 | 0.1037
Epoch 283/300, seasonal_0 Loss: 0.1092 | 0.1037
Epoch 284/300, seasonal_0 Loss: 0.1089 | 0.1036
Epoch 285/300, seasonal_0 Loss: 0.1081 | 0.1036
Epoch 286/300, seasonal_0 Loss: 0.1086 | 0.1036
Epoch 287/300, seasonal_0 Loss: 0.1095 | 0.1036
Epoch 288/300, seasonal_0 Loss: 0.1088 | 0.1037
Epoch 289/300, seasonal_0 Loss: 0.1097 | 0.1038
Epoch 290/300, seasonal_0 Loss: 0.1083 | 0.1037
Epoch 291/300, seasonal_0 Loss: 0.1087 | 0.1037
Epoch 292/300, seasonal_0 Loss: 0.1088 | 0.1036
Epoch 293/300, seasonal_0 Loss: 0.1091 | 0.1036
Epoch 294/300, seasonal_0 Loss: 0.1083 | 0.1036
Epoch 295/300, seasonal_0 Loss: 0.1090 | 0.1036
Epoch 296/300, seasonal_0 Loss: 0.1082 | 0.1036
Epoch 297/300, seasonal_0 Loss: 0.1090 | 0.1036
Epoch 298/300, seasonal_0 Loss: 0.1090 | 0.1036
Epoch 299/300, seasonal_0 Loss: 0.1095 | 0.1036
Epoch 300/300, seasonal_0 Loss: 0.1082 | 0.1035
Training seasonal_1 component with params: {'observation_period_num': 29, 'train_rates': 0.9889658330371552, 'learning_rate': 5.4605206002228943e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.9144915065563395}
Epoch 1/300, seasonal_1 Loss: 0.7295 | 0.7288
Epoch 2/300, seasonal_1 Loss: 0.5377 | 0.5487
Epoch 3/300, seasonal_1 Loss: 0.3970 | 0.4425
Epoch 4/300, seasonal_1 Loss: 0.3199 | 0.4044
Epoch 5/300, seasonal_1 Loss: 0.2806 | 0.3281
Epoch 6/300, seasonal_1 Loss: 0.2573 | 0.3331
Epoch 7/300, seasonal_1 Loss: 0.2479 | 0.3365
Epoch 8/300, seasonal_1 Loss: 0.2369 | 0.2678
Epoch 9/300, seasonal_1 Loss: 0.2247 | 0.2448
Epoch 10/300, seasonal_1 Loss: 0.2290 | 0.2503
Epoch 11/300, seasonal_1 Loss: 0.2198 | 0.2677
Epoch 12/300, seasonal_1 Loss: 0.2108 | 0.2416
Epoch 13/300, seasonal_1 Loss: 0.2101 | 0.2930
Epoch 14/300, seasonal_1 Loss: 0.2011 | 0.2213
Epoch 15/300, seasonal_1 Loss: 0.2011 | 0.2597
Epoch 16/300, seasonal_1 Loss: 0.2085 | 0.2126
Epoch 17/300, seasonal_1 Loss: 0.2089 | 0.2372
Epoch 18/300, seasonal_1 Loss: 0.1838 | 0.2000
Epoch 19/300, seasonal_1 Loss: 0.1789 | 0.2119
Epoch 20/300, seasonal_1 Loss: 0.1731 | 0.1917
Epoch 21/300, seasonal_1 Loss: 0.1829 | 0.1865
Epoch 22/300, seasonal_1 Loss: 0.1649 | 0.1676
Epoch 23/300, seasonal_1 Loss: 0.1598 | 0.1744
Epoch 24/300, seasonal_1 Loss: 0.1632 | 0.1642
Epoch 25/300, seasonal_1 Loss: 0.1612 | 0.1893
Epoch 26/300, seasonal_1 Loss: 0.1592 | 0.1670
Epoch 27/300, seasonal_1 Loss: 0.1676 | 0.1586
Epoch 28/300, seasonal_1 Loss: 0.1621 | 0.1521
Epoch 29/300, seasonal_1 Loss: 0.1587 | 0.1671
Epoch 30/300, seasonal_1 Loss: 0.1593 | 0.1762
Epoch 31/300, seasonal_1 Loss: 0.1581 | 0.1624
Epoch 32/300, seasonal_1 Loss: 0.1557 | 0.1578
Epoch 33/300, seasonal_1 Loss: 0.1534 | 0.1413
Epoch 34/300, seasonal_1 Loss: 0.1523 | 0.1688
Epoch 35/300, seasonal_1 Loss: 0.1480 | 0.1438
Epoch 36/300, seasonal_1 Loss: 0.1526 | 0.1672
Epoch 37/300, seasonal_1 Loss: 0.1464 | 0.1705
Epoch 38/300, seasonal_1 Loss: 0.1446 | 0.1604
Epoch 39/300, seasonal_1 Loss: 0.1416 | 0.1492
Epoch 40/300, seasonal_1 Loss: 0.1457 | 0.1313
Epoch 41/300, seasonal_1 Loss: 0.1382 | 0.1494
Epoch 42/300, seasonal_1 Loss: 0.1337 | 0.1502
Epoch 43/300, seasonal_1 Loss: 0.1356 | 0.1450
Epoch 44/300, seasonal_1 Loss: 0.1326 | 0.1224
Epoch 45/300, seasonal_1 Loss: 0.1332 | 0.1293
Epoch 46/300, seasonal_1 Loss: 0.1375 | 0.1247
Epoch 47/300, seasonal_1 Loss: 0.1312 | 0.1458
Epoch 48/300, seasonal_1 Loss: 0.1270 | 0.1403
Epoch 49/300, seasonal_1 Loss: 0.1264 | 0.1516
Epoch 50/300, seasonal_1 Loss: 0.1230 | 0.1409
Epoch 51/300, seasonal_1 Loss: 0.1291 | 0.1231
Epoch 52/300, seasonal_1 Loss: 0.1272 | 0.1392
Epoch 53/300, seasonal_1 Loss: 0.1289 | 0.1674
Epoch 54/300, seasonal_1 Loss: 0.1261 | 0.1377
Epoch 55/300, seasonal_1 Loss: 0.1180 | 0.1112
Epoch 56/300, seasonal_1 Loss: 0.1173 | 0.1189
Epoch 57/300, seasonal_1 Loss: 0.1157 | 0.1137
Epoch 58/300, seasonal_1 Loss: 0.1135 | 0.1257
Epoch 59/300, seasonal_1 Loss: 0.1130 | 0.1203
Epoch 60/300, seasonal_1 Loss: 0.1126 | 0.1141
Epoch 61/300, seasonal_1 Loss: 0.1153 | 0.1173
Epoch 62/300, seasonal_1 Loss: 0.1170 | 0.1202
Epoch 63/300, seasonal_1 Loss: 0.1153 | 0.1137
Epoch 64/300, seasonal_1 Loss: 0.1131 | 0.1101
Epoch 65/300, seasonal_1 Loss: 0.1105 | 0.1085
Epoch 66/300, seasonal_1 Loss: 0.1086 | 0.1096
Epoch 67/300, seasonal_1 Loss: 0.1062 | 0.0944
Epoch 68/300, seasonal_1 Loss: 0.1029 | 0.0927
Epoch 69/300, seasonal_1 Loss: 0.1017 | 0.1023
Epoch 70/300, seasonal_1 Loss: 0.1007 | 0.1127
Epoch 71/300, seasonal_1 Loss: 0.0998 | 0.0958
Epoch 72/300, seasonal_1 Loss: 0.0999 | 0.0936
Epoch 73/300, seasonal_1 Loss: 0.1002 | 0.0980
Epoch 74/300, seasonal_1 Loss: 0.0975 | 0.0996
Epoch 75/300, seasonal_1 Loss: 0.0959 | 0.0935
Epoch 76/300, seasonal_1 Loss: 0.0971 | 0.1000
Epoch 77/300, seasonal_1 Loss: 0.0960 | 0.0928
Epoch 78/300, seasonal_1 Loss: 0.0966 | 0.0945
Epoch 79/300, seasonal_1 Loss: 0.0955 | 0.0919
Epoch 80/300, seasonal_1 Loss: 0.0947 | 0.0991
Epoch 81/300, seasonal_1 Loss: 0.0931 | 0.0923
Epoch 82/300, seasonal_1 Loss: 0.0926 | 0.0872
Epoch 83/300, seasonal_1 Loss: 0.0909 | 0.0876
Epoch 84/300, seasonal_1 Loss: 0.0912 | 0.0918
Epoch 85/300, seasonal_1 Loss: 0.0900 | 0.0950
Epoch 86/300, seasonal_1 Loss: 0.0902 | 0.0860
Epoch 87/300, seasonal_1 Loss: 0.0892 | 0.0847
Epoch 88/300, seasonal_1 Loss: 0.0886 | 0.0863
Epoch 89/300, seasonal_1 Loss: 0.0884 | 0.0920
Epoch 90/300, seasonal_1 Loss: 0.0884 | 0.0832
Epoch 91/300, seasonal_1 Loss: 0.0901 | 0.0906
Epoch 92/300, seasonal_1 Loss: 0.0894 | 0.0900
Epoch 93/300, seasonal_1 Loss: 0.0890 | 0.0883
Epoch 94/300, seasonal_1 Loss: 0.0874 | 0.0824
Epoch 95/300, seasonal_1 Loss: 0.0872 | 0.0862
Epoch 96/300, seasonal_1 Loss: 0.0849 | 0.0851
Epoch 97/300, seasonal_1 Loss: 0.0856 | 0.0861
Epoch 98/300, seasonal_1 Loss: 0.0844 | 0.0827
Epoch 99/300, seasonal_1 Loss: 0.0852 | 0.0850
Epoch 100/300, seasonal_1 Loss: 0.0837 | 0.0809
Epoch 101/300, seasonal_1 Loss: 0.0842 | 0.0827
Epoch 102/300, seasonal_1 Loss: 0.0831 | 0.0858
Epoch 103/300, seasonal_1 Loss: 0.0842 | 0.0850
Epoch 104/300, seasonal_1 Loss: 0.0833 | 0.0803
Epoch 105/300, seasonal_1 Loss: 0.0830 | 0.0793
Epoch 106/300, seasonal_1 Loss: 0.0838 | 0.0902
Epoch 107/300, seasonal_1 Loss: 0.0820 | 0.0854
Epoch 108/300, seasonal_1 Loss: 0.0818 | 0.0795
Epoch 109/300, seasonal_1 Loss: 0.0807 | 0.0781
Epoch 110/300, seasonal_1 Loss: 0.0805 | 0.0822
Epoch 111/300, seasonal_1 Loss: 0.0806 | 0.0866
Epoch 112/300, seasonal_1 Loss: 0.0799 | 0.0795
Epoch 113/300, seasonal_1 Loss: 0.0805 | 0.0779
Epoch 114/300, seasonal_1 Loss: 0.0786 | 0.0785
Epoch 115/300, seasonal_1 Loss: 0.0786 | 0.0820
Epoch 116/300, seasonal_1 Loss: 0.0781 | 0.0801
Epoch 117/300, seasonal_1 Loss: 0.0779 | 0.0775
Epoch 118/300, seasonal_1 Loss: 0.0778 | 0.0780
Epoch 119/300, seasonal_1 Loss: 0.0775 | 0.0783
Epoch 120/300, seasonal_1 Loss: 0.0763 | 0.0811
Epoch 121/300, seasonal_1 Loss: 0.0766 | 0.0775
Epoch 122/300, seasonal_1 Loss: 0.0760 | 0.0777
Epoch 123/300, seasonal_1 Loss: 0.0760 | 0.0763
Epoch 124/300, seasonal_1 Loss: 0.0752 | 0.0758
Epoch 125/300, seasonal_1 Loss: 0.0751 | 0.0808
Epoch 126/300, seasonal_1 Loss: 0.0754 | 0.0757
Epoch 127/300, seasonal_1 Loss: 0.0753 | 0.0772
Epoch 128/300, seasonal_1 Loss: 0.0747 | 0.0749
Epoch 129/300, seasonal_1 Loss: 0.0742 | 0.0775
Epoch 130/300, seasonal_1 Loss: 0.0736 | 0.0757
Epoch 131/300, seasonal_1 Loss: 0.0737 | 0.0751
Epoch 132/300, seasonal_1 Loss: 0.0734 | 0.0764
Epoch 133/300, seasonal_1 Loss: 0.0731 | 0.0747
Epoch 134/300, seasonal_1 Loss: 0.0732 | 0.0755
Epoch 135/300, seasonal_1 Loss: 0.0724 | 0.0751
Epoch 136/300, seasonal_1 Loss: 0.0727 | 0.0754
Epoch 137/300, seasonal_1 Loss: 0.0724 | 0.0740
Epoch 138/300, seasonal_1 Loss: 0.0722 | 0.0751
Epoch 139/300, seasonal_1 Loss: 0.0714 | 0.0742
Epoch 140/300, seasonal_1 Loss: 0.0718 | 0.0747
Epoch 141/300, seasonal_1 Loss: 0.0715 | 0.0737
Epoch 142/300, seasonal_1 Loss: 0.0716 | 0.0730
Epoch 143/300, seasonal_1 Loss: 0.0717 | 0.0735
Epoch 144/300, seasonal_1 Loss: 0.0718 | 0.0753
Epoch 145/300, seasonal_1 Loss: 0.0709 | 0.0734
Epoch 146/300, seasonal_1 Loss: 0.0708 | 0.0730
Epoch 147/300, seasonal_1 Loss: 0.0703 | 0.0725
Epoch 148/300, seasonal_1 Loss: 0.0697 | 0.0744
Epoch 149/300, seasonal_1 Loss: 0.0698 | 0.0739
Epoch 150/300, seasonal_1 Loss: 0.0697 | 0.0730
Epoch 151/300, seasonal_1 Loss: 0.0697 | 0.0722
Epoch 152/300, seasonal_1 Loss: 0.0690 | 0.0729
Epoch 153/300, seasonal_1 Loss: 0.0698 | 0.0732
Epoch 154/300, seasonal_1 Loss: 0.0692 | 0.0735
Epoch 155/300, seasonal_1 Loss: 0.0694 | 0.0728
Epoch 156/300, seasonal_1 Loss: 0.0687 | 0.0731
Epoch 157/300, seasonal_1 Loss: 0.0686 | 0.0723
Epoch 158/300, seasonal_1 Loss: 0.0681 | 0.0722
Epoch 159/300, seasonal_1 Loss: 0.0686 | 0.0720
Epoch 160/300, seasonal_1 Loss: 0.0684 | 0.0728
Epoch 161/300, seasonal_1 Loss: 0.0680 | 0.0724
Epoch 162/300, seasonal_1 Loss: 0.0679 | 0.0726
Epoch 163/300, seasonal_1 Loss: 0.0678 | 0.0709
Epoch 164/300, seasonal_1 Loss: 0.0674 | 0.0727
Epoch 165/300, seasonal_1 Loss: 0.0677 | 0.0715
Epoch 166/300, seasonal_1 Loss: 0.0679 | 0.0713
Epoch 167/300, seasonal_1 Loss: 0.0671 | 0.0709
Epoch 168/300, seasonal_1 Loss: 0.0676 | 0.0711
Epoch 169/300, seasonal_1 Loss: 0.0672 | 0.0717
Epoch 170/300, seasonal_1 Loss: 0.0670 | 0.0720
Epoch 171/300, seasonal_1 Loss: 0.0670 | 0.0724
Epoch 172/300, seasonal_1 Loss: 0.0665 | 0.0705
Epoch 173/300, seasonal_1 Loss: 0.0666 | 0.0708
Epoch 174/300, seasonal_1 Loss: 0.0663 | 0.0716
Epoch 175/300, seasonal_1 Loss: 0.0661 | 0.0709
Epoch 176/300, seasonal_1 Loss: 0.0666 | 0.0711
Epoch 177/300, seasonal_1 Loss: 0.0664 | 0.0715
Epoch 178/300, seasonal_1 Loss: 0.0663 | 0.0715
Epoch 179/300, seasonal_1 Loss: 0.0664 | 0.0706
Epoch 180/300, seasonal_1 Loss: 0.0662 | 0.0710
Epoch 181/300, seasonal_1 Loss: 0.0664 | 0.0709
Epoch 182/300, seasonal_1 Loss: 0.0660 | 0.0711
Epoch 183/300, seasonal_1 Loss: 0.0664 | 0.0707
Epoch 184/300, seasonal_1 Loss: 0.0655 | 0.0712
Epoch 185/300, seasonal_1 Loss: 0.0657 | 0.0703
Epoch 186/300, seasonal_1 Loss: 0.0657 | 0.0705
Epoch 187/300, seasonal_1 Loss: 0.0648 | 0.0706
Epoch 188/300, seasonal_1 Loss: 0.0654 | 0.0703
Epoch 189/300, seasonal_1 Loss: 0.0651 | 0.0712
Epoch 190/300, seasonal_1 Loss: 0.0649 | 0.0706
Epoch 191/300, seasonal_1 Loss: 0.0646 | 0.0704
Epoch 192/300, seasonal_1 Loss: 0.0649 | 0.0700
Epoch 193/300, seasonal_1 Loss: 0.0646 | 0.0706
Epoch 194/300, seasonal_1 Loss: 0.0645 | 0.0704
Epoch 195/300, seasonal_1 Loss: 0.0643 | 0.0716
Epoch 196/300, seasonal_1 Loss: 0.0640 | 0.0698
Epoch 197/300, seasonal_1 Loss: 0.0646 | 0.0702
Epoch 198/300, seasonal_1 Loss: 0.0650 | 0.0703
Epoch 199/300, seasonal_1 Loss: 0.0643 | 0.0704
Epoch 200/300, seasonal_1 Loss: 0.0643 | 0.0701
Epoch 201/300, seasonal_1 Loss: 0.0643 | 0.0706
Epoch 202/300, seasonal_1 Loss: 0.0638 | 0.0714
Epoch 203/300, seasonal_1 Loss: 0.0642 | 0.0706
Epoch 204/300, seasonal_1 Loss: 0.0639 | 0.0703
Epoch 205/300, seasonal_1 Loss: 0.0638 | 0.0706
Epoch 206/300, seasonal_1 Loss: 0.0637 | 0.0702
Epoch 207/300, seasonal_1 Loss: 0.0636 | 0.0698
Epoch 208/300, seasonal_1 Loss: 0.0637 | 0.0702
Epoch 209/300, seasonal_1 Loss: 0.0638 | 0.0696
Epoch 210/300, seasonal_1 Loss: 0.0638 | 0.0695
Epoch 211/300, seasonal_1 Loss: 0.0636 | 0.0701
Epoch 212/300, seasonal_1 Loss: 0.0633 | 0.0703
Epoch 213/300, seasonal_1 Loss: 0.0633 | 0.0706
Epoch 214/300, seasonal_1 Loss: 0.0643 | 0.0700
Epoch 215/300, seasonal_1 Loss: 0.0639 | 0.0698
Epoch 216/300, seasonal_1 Loss: 0.0635 | 0.0703
Epoch 217/300, seasonal_1 Loss: 0.0630 | 0.0697
Epoch 218/300, seasonal_1 Loss: 0.0632 | 0.0702
Epoch 219/300, seasonal_1 Loss: 0.0633 | 0.0707
Epoch 220/300, seasonal_1 Loss: 0.0627 | 0.0709
Epoch 221/300, seasonal_1 Loss: 0.0630 | 0.0703
Epoch 222/300, seasonal_1 Loss: 0.0630 | 0.0704
Epoch 223/300, seasonal_1 Loss: 0.0629 | 0.0697
Epoch 224/300, seasonal_1 Loss: 0.0624 | 0.0701
Epoch 225/300, seasonal_1 Loss: 0.0627 | 0.0701
Epoch 226/300, seasonal_1 Loss: 0.0626 | 0.0708
Epoch 227/300, seasonal_1 Loss: 0.0628 | 0.0704
Epoch 228/300, seasonal_1 Loss: 0.0626 | 0.0699
Epoch 229/300, seasonal_1 Loss: 0.0628 | 0.0706
Epoch 230/300, seasonal_1 Loss: 0.0628 | 0.0710
Epoch 231/300, seasonal_1 Loss: 0.0623 | 0.0697
Epoch 232/300, seasonal_1 Loss: 0.0624 | 0.0700
Epoch 233/300, seasonal_1 Loss: 0.0625 | 0.0701
Epoch 234/300, seasonal_1 Loss: 0.0623 | 0.0703
Epoch 235/300, seasonal_1 Loss: 0.0625 | 0.0700
Epoch 236/300, seasonal_1 Loss: 0.0623 | 0.0702
Epoch 237/300, seasonal_1 Loss: 0.0621 | 0.0695
Epoch 238/300, seasonal_1 Loss: 0.0618 | 0.0701
Epoch 239/300, seasonal_1 Loss: 0.0626 | 0.0696
Epoch 240/300, seasonal_1 Loss: 0.0624 | 0.0695
Epoch 241/300, seasonal_1 Loss: 0.0618 | 0.0694
Epoch 242/300, seasonal_1 Loss: 0.0624 | 0.0699
Epoch 243/300, seasonal_1 Loss: 0.0619 | 0.0696
Epoch 244/300, seasonal_1 Loss: 0.0622 | 0.0696
Epoch 245/300, seasonal_1 Loss: 0.0617 | 0.0695
Epoch 246/300, seasonal_1 Loss: 0.0616 | 0.0694
Epoch 247/300, seasonal_1 Loss: 0.0618 | 0.0695
Epoch 248/300, seasonal_1 Loss: 0.0618 | 0.0694
Epoch 249/300, seasonal_1 Loss: 0.0612 | 0.0702
Epoch 250/300, seasonal_1 Loss: 0.0618 | 0.0702
Epoch 251/300, seasonal_1 Loss: 0.0616 | 0.0697
Epoch 252/300, seasonal_1 Loss: 0.0616 | 0.0700
Epoch 253/300, seasonal_1 Loss: 0.0622 | 0.0705
Epoch 254/300, seasonal_1 Loss: 0.0615 | 0.0703
Epoch 255/300, seasonal_1 Loss: 0.0617 | 0.0702
Epoch 256/300, seasonal_1 Loss: 0.0618 | 0.0699
Epoch 257/300, seasonal_1 Loss: 0.0617 | 0.0699
Epoch 258/300, seasonal_1 Loss: 0.0615 | 0.0694
Epoch 259/300, seasonal_1 Loss: 0.0613 | 0.0695
Epoch 260/300, seasonal_1 Loss: 0.0615 | 0.0695
Epoch 261/300, seasonal_1 Loss: 0.0615 | 0.0692
Epoch 262/300, seasonal_1 Loss: 0.0613 | 0.0696
Epoch 263/300, seasonal_1 Loss: 0.0610 | 0.0701
Epoch 264/300, seasonal_1 Loss: 0.0616 | 0.0695
Epoch 265/300, seasonal_1 Loss: 0.0612 | 0.0696
Epoch 266/300, seasonal_1 Loss: 0.0610 | 0.0698
Epoch 267/300, seasonal_1 Loss: 0.0609 | 0.0701
Epoch 268/300, seasonal_1 Loss: 0.0613 | 0.0700
Epoch 269/300, seasonal_1 Loss: 0.0613 | 0.0700
Epoch 270/300, seasonal_1 Loss: 0.0606 | 0.0698
Epoch 271/300, seasonal_1 Loss: 0.0617 | 0.0703
Epoch 272/300, seasonal_1 Loss: 0.0612 | 0.0704
Epoch 273/300, seasonal_1 Loss: 0.0611 | 0.0700
Epoch 274/300, seasonal_1 Loss: 0.0615 | 0.0702
Epoch 275/300, seasonal_1 Loss: 0.0614 | 0.0701
Epoch 276/300, seasonal_1 Loss: 0.0610 | 0.0700
Epoch 277/300, seasonal_1 Loss: 0.0610 | 0.0698
Epoch 278/300, seasonal_1 Loss: 0.0612 | 0.0696
Epoch 279/300, seasonal_1 Loss: 0.0613 | 0.0697
Epoch 280/300, seasonal_1 Loss: 0.0611 | 0.0697
Epoch 281/300, seasonal_1 Loss: 0.0614 | 0.0699
Epoch 282/300, seasonal_1 Loss: 0.0607 | 0.0700
Epoch 283/300, seasonal_1 Loss: 0.0610 | 0.0701
Epoch 284/300, seasonal_1 Loss: 0.0605 | 0.0701
Epoch 285/300, seasonal_1 Loss: 0.0609 | 0.0704
Epoch 286/300, seasonal_1 Loss: 0.0608 | 0.0701
Epoch 287/300, seasonal_1 Loss: 0.0606 | 0.0699
Epoch 288/300, seasonal_1 Loss: 0.0608 | 0.0703
Epoch 289/300, seasonal_1 Loss: 0.0608 | 0.0701
Epoch 290/300, seasonal_1 Loss: 0.0607 | 0.0697
Epoch 291/300, seasonal_1 Loss: 0.0607 | 0.0693
Epoch 292/300, seasonal_1 Loss: 0.0610 | 0.0696
Epoch 293/300, seasonal_1 Loss: 0.0607 | 0.0700
Epoch 294/300, seasonal_1 Loss: 0.0605 | 0.0703
Epoch 295/300, seasonal_1 Loss: 0.0607 | 0.0704
Epoch 296/300, seasonal_1 Loss: 0.0608 | 0.0703
Epoch 297/300, seasonal_1 Loss: 0.0604 | 0.0698
Epoch 298/300, seasonal_1 Loss: 0.0603 | 0.0693
Epoch 299/300, seasonal_1 Loss: 0.0604 | 0.0699
Epoch 300/300, seasonal_1 Loss: 0.0605 | 0.0698
Training seasonal_2 component with params: {'observation_period_num': 160, 'train_rates': 0.9779906767611074, 'learning_rate': 0.00013483099593761814, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8106359802236661}
Epoch 1/300, seasonal_2 Loss: 0.8470 | 0.7096
Epoch 2/300, seasonal_2 Loss: 0.7809 | 0.5555
Epoch 3/300, seasonal_2 Loss: 0.5517 | 0.4772
Epoch 4/300, seasonal_2 Loss: 0.5369 | 0.4452
Epoch 5/300, seasonal_2 Loss: 0.4662 | 0.4100
Epoch 6/300, seasonal_2 Loss: 0.4278 | 0.4030
Epoch 7/300, seasonal_2 Loss: 0.4164 | 0.4087
Epoch 8/300, seasonal_2 Loss: 0.3720 | 0.3796
Epoch 9/300, seasonal_2 Loss: 0.4624 | 0.3228
Epoch 10/300, seasonal_2 Loss: 0.5855 | 0.3670
Epoch 11/300, seasonal_2 Loss: 0.3960 | 0.3299
Epoch 12/300, seasonal_2 Loss: 0.3424 | 0.3064
Epoch 13/300, seasonal_2 Loss: 0.2832 | 0.2760
Epoch 14/300, seasonal_2 Loss: 0.2846 | 0.2567
Epoch 15/300, seasonal_2 Loss: 0.2636 | 0.2478
Epoch 16/300, seasonal_2 Loss: 0.2443 | 0.2393
Epoch 17/300, seasonal_2 Loss: 0.2226 | 0.2251
Epoch 18/300, seasonal_2 Loss: 0.2175 | 0.2229
Epoch 19/300, seasonal_2 Loss: 0.2107 | 0.2144
Epoch 20/300, seasonal_2 Loss: 0.2023 | 0.2106
Epoch 21/300, seasonal_2 Loss: 0.2009 | 0.2041
Epoch 22/300, seasonal_2 Loss: 0.1953 | 0.2032
Epoch 23/300, seasonal_2 Loss: 0.1919 | 0.2014
Epoch 24/300, seasonal_2 Loss: 0.1879 | 0.1935
Epoch 25/300, seasonal_2 Loss: 0.1846 | 0.1933
Epoch 26/300, seasonal_2 Loss: 0.1809 | 0.1909
Epoch 27/300, seasonal_2 Loss: 0.1774 | 0.1899
Epoch 28/300, seasonal_2 Loss: 0.1766 | 0.1845
Epoch 29/300, seasonal_2 Loss: 0.1743 | 0.1859
Epoch 30/300, seasonal_2 Loss: 0.1721 | 0.1815
Epoch 31/300, seasonal_2 Loss: 0.1711 | 0.1792
Epoch 32/300, seasonal_2 Loss: 0.1710 | 0.1768
Epoch 33/300, seasonal_2 Loss: 0.1695 | 0.1768
Epoch 34/300, seasonal_2 Loss: 0.1690 | 0.1751
Epoch 35/300, seasonal_2 Loss: 0.1673 | 0.1739
Epoch 36/300, seasonal_2 Loss: 0.1667 | 0.1708
Epoch 37/300, seasonal_2 Loss: 0.1646 | 0.1683
Epoch 38/300, seasonal_2 Loss: 0.1635 | 0.1703
Epoch 39/300, seasonal_2 Loss: 0.1624 | 0.1676
Epoch 40/300, seasonal_2 Loss: 0.1612 | 0.1687
Epoch 41/300, seasonal_2 Loss: 0.1602 | 0.1668
Epoch 42/300, seasonal_2 Loss: 0.1597 | 0.1686
Epoch 43/300, seasonal_2 Loss: 0.1587 | 0.1672
Epoch 44/300, seasonal_2 Loss: 0.1585 | 0.1678
Epoch 45/300, seasonal_2 Loss: 0.1581 | 0.1641
Epoch 46/300, seasonal_2 Loss: 0.1571 | 0.1646
Epoch 47/300, seasonal_2 Loss: 0.1559 | 0.1614
Epoch 48/300, seasonal_2 Loss: 0.1555 | 0.1622
Epoch 49/300, seasonal_2 Loss: 0.1548 | 0.1617
Epoch 50/300, seasonal_2 Loss: 0.1549 | 0.1627
Epoch 51/300, seasonal_2 Loss: 0.1545 | 0.1603
Epoch 52/300, seasonal_2 Loss: 0.1536 | 0.1594
Epoch 53/300, seasonal_2 Loss: 0.1534 | 0.1588
Epoch 54/300, seasonal_2 Loss: 0.1532 | 0.1587
Epoch 55/300, seasonal_2 Loss: 0.1522 | 0.1572
Epoch 56/300, seasonal_2 Loss: 0.1519 | 0.1573
Epoch 57/300, seasonal_2 Loss: 0.1520 | 0.1568
Epoch 58/300, seasonal_2 Loss: 0.1510 | 0.1574
Epoch 59/300, seasonal_2 Loss: 0.1518 | 0.1575
Epoch 60/300, seasonal_2 Loss: 0.1513 | 0.1562
Epoch 61/300, seasonal_2 Loss: 0.1501 | 0.1550
Epoch 62/300, seasonal_2 Loss: 0.1501 | 0.1551
Epoch 63/300, seasonal_2 Loss: 0.1493 | 0.1546
Epoch 64/300, seasonal_2 Loss: 0.1492 | 0.1543
Epoch 65/300, seasonal_2 Loss: 0.1490 | 0.1542
Epoch 66/300, seasonal_2 Loss: 0.1488 | 0.1535
Epoch 67/300, seasonal_2 Loss: 0.1493 | 0.1548
Epoch 68/300, seasonal_2 Loss: 0.1485 | 0.1547
Epoch 69/300, seasonal_2 Loss: 0.1475 | 0.1540
Epoch 70/300, seasonal_2 Loss: 0.1482 | 0.1537
Epoch 71/300, seasonal_2 Loss: 0.1477 | 0.1538
Epoch 72/300, seasonal_2 Loss: 0.1470 | 0.1528
Epoch 73/300, seasonal_2 Loss: 0.1472 | 0.1520
Epoch 74/300, seasonal_2 Loss: 0.1472 | 0.1519
Epoch 75/300, seasonal_2 Loss: 0.1464 | 0.1523
Epoch 76/300, seasonal_2 Loss: 0.1460 | 0.1508
Epoch 77/300, seasonal_2 Loss: 0.1467 | 0.1513
Epoch 78/300, seasonal_2 Loss: 0.1468 | 0.1506
Epoch 79/300, seasonal_2 Loss: 0.1464 | 0.1496
Epoch 80/300, seasonal_2 Loss: 0.1457 | 0.1501
Epoch 81/300, seasonal_2 Loss: 0.1462 | 0.1503
Epoch 82/300, seasonal_2 Loss: 0.1456 | 0.1509
Epoch 83/300, seasonal_2 Loss: 0.1459 | 0.1500
Epoch 84/300, seasonal_2 Loss: 0.1455 | 0.1493
Epoch 85/300, seasonal_2 Loss: 0.1457 | 0.1487
Epoch 86/300, seasonal_2 Loss: 0.1451 | 0.1484
Epoch 87/300, seasonal_2 Loss: 0.1448 | 0.1484
Epoch 88/300, seasonal_2 Loss: 0.1456 | 0.1500
Epoch 89/300, seasonal_2 Loss: 0.1449 | 0.1500
Epoch 90/300, seasonal_2 Loss: 0.1443 | 0.1504
Epoch 91/300, seasonal_2 Loss: 0.1450 | 0.1495
Epoch 92/300, seasonal_2 Loss: 0.1448 | 0.1496
Epoch 93/300, seasonal_2 Loss: 0.1447 | 0.1507
Epoch 94/300, seasonal_2 Loss: 0.1444 | 0.1502
Epoch 95/300, seasonal_2 Loss: 0.1448 | 0.1495
Epoch 96/300, seasonal_2 Loss: 0.1440 | 0.1498
Epoch 97/300, seasonal_2 Loss: 0.1441 | 0.1487
Epoch 98/300, seasonal_2 Loss: 0.1444 | 0.1476
Epoch 99/300, seasonal_2 Loss: 0.1433 | 0.1482
Epoch 100/300, seasonal_2 Loss: 0.1437 | 0.1482
Epoch 101/300, seasonal_2 Loss: 0.1438 | 0.1484
Epoch 102/300, seasonal_2 Loss: 0.1433 | 0.1484
Epoch 103/300, seasonal_2 Loss: 0.1431 | 0.1480
Epoch 104/300, seasonal_2 Loss: 0.1429 | 0.1477
Epoch 105/300, seasonal_2 Loss: 0.1437 | 0.1474
Epoch 106/300, seasonal_2 Loss: 0.1439 | 0.1470
Epoch 107/300, seasonal_2 Loss: 0.1437 | 0.1469
Epoch 108/300, seasonal_2 Loss: 0.1433 | 0.1471
Epoch 109/300, seasonal_2 Loss: 0.1432 | 0.1472
Epoch 110/300, seasonal_2 Loss: 0.1432 | 0.1470
Epoch 111/300, seasonal_2 Loss: 0.1432 | 0.1470
Epoch 112/300, seasonal_2 Loss: 0.1431 | 0.1468
Epoch 113/300, seasonal_2 Loss: 0.1434 | 0.1469
Epoch 114/300, seasonal_2 Loss: 0.1434 | 0.1469
Epoch 115/300, seasonal_2 Loss: 0.1430 | 0.1468
Epoch 116/300, seasonal_2 Loss: 0.1432 | 0.1467
Epoch 117/300, seasonal_2 Loss: 0.1427 | 0.1469
Epoch 118/300, seasonal_2 Loss: 0.1425 | 0.1472
Epoch 119/300, seasonal_2 Loss: 0.1427 | 0.1472
Epoch 120/300, seasonal_2 Loss: 0.1425 | 0.1470
Epoch 121/300, seasonal_2 Loss: 0.1429 | 0.1467
Epoch 122/300, seasonal_2 Loss: 0.1427 | 0.1468
Epoch 123/300, seasonal_2 Loss: 0.1427 | 0.1466
Epoch 124/300, seasonal_2 Loss: 0.1427 | 0.1465
Epoch 125/300, seasonal_2 Loss: 0.1424 | 0.1466
Epoch 126/300, seasonal_2 Loss: 0.1428 | 0.1465
Epoch 127/300, seasonal_2 Loss: 0.1425 | 0.1463
Epoch 128/300, seasonal_2 Loss: 0.1427 | 0.1462
Epoch 129/300, seasonal_2 Loss: 0.1432 | 0.1464
Epoch 130/300, seasonal_2 Loss: 0.1427 | 0.1462
Epoch 131/300, seasonal_2 Loss: 0.1429 | 0.1463
Epoch 132/300, seasonal_2 Loss: 0.1426 | 0.1463
Epoch 133/300, seasonal_2 Loss: 0.1423 | 0.1462
Epoch 134/300, seasonal_2 Loss: 0.1422 | 0.1463
Epoch 135/300, seasonal_2 Loss: 0.1430 | 0.1462
Epoch 136/300, seasonal_2 Loss: 0.1425 | 0.1462
Epoch 137/300, seasonal_2 Loss: 0.1430 | 0.1463
Epoch 138/300, seasonal_2 Loss: 0.1424 | 0.1463
Epoch 139/300, seasonal_2 Loss: 0.1433 | 0.1463
Epoch 140/300, seasonal_2 Loss: 0.1424 | 0.1464
Epoch 141/300, seasonal_2 Loss: 0.1425 | 0.1463
Epoch 142/300, seasonal_2 Loss: 0.1422 | 0.1462
Epoch 143/300, seasonal_2 Loss: 0.1425 | 0.1461
Epoch 144/300, seasonal_2 Loss: 0.1425 | 0.1460
Epoch 145/300, seasonal_2 Loss: 0.1417 | 0.1460
Epoch 146/300, seasonal_2 Loss: 0.1419 | 0.1460
Epoch 147/300, seasonal_2 Loss: 0.1426 | 0.1459
Epoch 148/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 149/300, seasonal_2 Loss: 0.1425 | 0.1460
Epoch 150/300, seasonal_2 Loss: 0.1422 | 0.1459
Epoch 151/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 152/300, seasonal_2 Loss: 0.1427 | 0.1459
Epoch 153/300, seasonal_2 Loss: 0.1421 | 0.1458
Epoch 154/300, seasonal_2 Loss: 0.1426 | 0.1458
Epoch 155/300, seasonal_2 Loss: 0.1424 | 0.1458
Epoch 156/300, seasonal_2 Loss: 0.1423 | 0.1457
Epoch 157/300, seasonal_2 Loss: 0.1428 | 0.1458
Epoch 158/300, seasonal_2 Loss: 0.1424 | 0.1459
Epoch 159/300, seasonal_2 Loss: 0.1420 | 0.1459
Epoch 160/300, seasonal_2 Loss: 0.1427 | 0.1459
Epoch 161/300, seasonal_2 Loss: 0.1424 | 0.1459
Epoch 162/300, seasonal_2 Loss: 0.1419 | 0.1458
Epoch 163/300, seasonal_2 Loss: 0.1424 | 0.1458
Epoch 164/300, seasonal_2 Loss: 0.1421 | 0.1458
Epoch 165/300, seasonal_2 Loss: 0.1420 | 0.1458
Epoch 166/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 167/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 168/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 169/300, seasonal_2 Loss: 0.1421 | 0.1458
Epoch 170/300, seasonal_2 Loss: 0.1424 | 0.1458
Epoch 171/300, seasonal_2 Loss: 0.1423 | 0.1459
Epoch 172/300, seasonal_2 Loss: 0.1422 | 0.1459
Epoch 173/300, seasonal_2 Loss: 0.1424 | 0.1459
Epoch 174/300, seasonal_2 Loss: 0.1420 | 0.1459
Epoch 175/300, seasonal_2 Loss: 0.1426 | 0.1459
Epoch 176/300, seasonal_2 Loss: 0.1426 | 0.1459
Epoch 177/300, seasonal_2 Loss: 0.1425 | 0.1459
Epoch 178/300, seasonal_2 Loss: 0.1423 | 0.1459
Epoch 179/300, seasonal_2 Loss: 0.1418 | 0.1459
Epoch 180/300, seasonal_2 Loss: 0.1427 | 0.1459
Epoch 181/300, seasonal_2 Loss: 0.1426 | 0.1458
Epoch 182/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 183/300, seasonal_2 Loss: 0.1421 | 0.1458
Epoch 184/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 185/300, seasonal_2 Loss: 0.1422 | 0.1458
Epoch 186/300, seasonal_2 Loss: 0.1422 | 0.1458
Epoch 187/300, seasonal_2 Loss: 0.1420 | 0.1458
Epoch 188/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 189/300, seasonal_2 Loss: 0.1420 | 0.1458
Epoch 190/300, seasonal_2 Loss: 0.1421 | 0.1458
Epoch 191/300, seasonal_2 Loss: 0.1416 | 0.1458
Epoch 192/300, seasonal_2 Loss: 0.1415 | 0.1458
Epoch 193/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 194/300, seasonal_2 Loss: 0.1427 | 0.1458
Epoch 195/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 196/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 197/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 198/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 199/300, seasonal_2 Loss: 0.1417 | 0.1457
Epoch 200/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 201/300, seasonal_2 Loss: 0.1417 | 0.1458
Epoch 202/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 203/300, seasonal_2 Loss: 0.1428 | 0.1458
Epoch 204/300, seasonal_2 Loss: 0.1420 | 0.1458
Epoch 205/300, seasonal_2 Loss: 0.1418 | 0.1458
Epoch 206/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 207/300, seasonal_2 Loss: 0.1417 | 0.1458
Epoch 208/300, seasonal_2 Loss: 0.1419 | 0.1458
Epoch 209/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 210/300, seasonal_2 Loss: 0.1420 | 0.1458
Epoch 211/300, seasonal_2 Loss: 0.1422 | 0.1458
Epoch 212/300, seasonal_2 Loss: 0.1419 | 0.1458
Epoch 213/300, seasonal_2 Loss: 0.1423 | 0.1458
Epoch 214/300, seasonal_2 Loss: 0.1424 | 0.1458
Epoch 215/300, seasonal_2 Loss: 0.1422 | 0.1458
Epoch 216/300, seasonal_2 Loss: 0.1417 | 0.1458
Epoch 217/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 218/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 219/300, seasonal_2 Loss: 0.1424 | 0.1458
Epoch 220/300, seasonal_2 Loss: 0.1424 | 0.1458
Epoch 221/300, seasonal_2 Loss: 0.1413 | 0.1458
Epoch 222/300, seasonal_2 Loss: 0.1425 | 0.1458
Epoch 223/300, seasonal_2 Loss: 0.1420 | 0.1458
Epoch 224/300, seasonal_2 Loss: 0.1422 | 0.1458
Epoch 225/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 226/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 227/300, seasonal_2 Loss: 0.1426 | 0.1457
Epoch 228/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 229/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 230/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 231/300, seasonal_2 Loss: 0.1427 | 0.1457
Epoch 232/300, seasonal_2 Loss: 0.1421 | 0.1457
Epoch 233/300, seasonal_2 Loss: 0.1417 | 0.1457
Epoch 234/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 235/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 236/300, seasonal_2 Loss: 0.1423 | 0.1457
Epoch 237/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 238/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 239/300, seasonal_2 Loss: 0.1421 | 0.1457
Epoch 240/300, seasonal_2 Loss: 0.1423 | 0.1457
Epoch 241/300, seasonal_2 Loss: 0.1431 | 0.1457
Epoch 242/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 243/300, seasonal_2 Loss: 0.1425 | 0.1457
Epoch 244/300, seasonal_2 Loss: 0.1423 | 0.1457
Epoch 245/300, seasonal_2 Loss: 0.1428 | 0.1457
Epoch 246/300, seasonal_2 Loss: 0.1419 | 0.1457
Epoch 247/300, seasonal_2 Loss: 0.1426 | 0.1457
Epoch 248/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 249/300, seasonal_2 Loss: 0.1423 | 0.1457
Epoch 250/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 251/300, seasonal_2 Loss: 0.1417 | 0.1457
Epoch 252/300, seasonal_2 Loss: 0.1421 | 0.1457
Epoch 253/300, seasonal_2 Loss: 0.1421 | 0.1457
Epoch 254/300, seasonal_2 Loss: 0.1421 | 0.1457
Epoch 255/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 256/300, seasonal_2 Loss: 0.1417 | 0.1457
Epoch 257/300, seasonal_2 Loss: 0.1423 | 0.1457
Epoch 258/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 259/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 260/300, seasonal_2 Loss: 0.1425 | 0.1457
Epoch 261/300, seasonal_2 Loss: 0.1422 | 0.1457
Epoch 262/300, seasonal_2 Loss: 0.1425 | 0.1457
Epoch 263/300, seasonal_2 Loss: 0.1426 | 0.1457
Epoch 264/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 265/300, seasonal_2 Loss: 0.1426 | 0.1457
Epoch 266/300, seasonal_2 Loss: 0.1420 | 0.1457
Epoch 267/300, seasonal_2 Loss: 0.1416 | 0.1457
Epoch 268/300, seasonal_2 Loss: 0.1423 | 0.1457
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 95, 'train_rates': 0.9874840147760537, 'learning_rate': 6.973472474047547e-05, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8707590452581023}
Epoch 1/300, seasonal_3 Loss: 0.5506 | 0.4672
Epoch 2/300, seasonal_3 Loss: 0.4333 | 0.3794
Epoch 3/300, seasonal_3 Loss: 0.3460 | 0.3609
Epoch 4/300, seasonal_3 Loss: 0.3134 | 0.3171
Epoch 5/300, seasonal_3 Loss: 0.2676 | 0.2685
Epoch 6/300, seasonal_3 Loss: 0.2397 | 0.2497
Epoch 7/300, seasonal_3 Loss: 0.2233 | 0.2013
Epoch 8/300, seasonal_3 Loss: 0.2130 | 0.2197
Epoch 9/300, seasonal_3 Loss: 0.2165 | 0.2171
Epoch 10/300, seasonal_3 Loss: 0.2078 | 0.2272
Epoch 11/300, seasonal_3 Loss: 0.1988 | 0.1821
Epoch 12/300, seasonal_3 Loss: 0.1852 | 0.1838
Epoch 13/300, seasonal_3 Loss: 0.1799 | 0.1800
Epoch 14/300, seasonal_3 Loss: 0.1763 | 0.1640
Epoch 15/300, seasonal_3 Loss: 0.1755 | 0.1715
Epoch 16/300, seasonal_3 Loss: 0.1774 | 0.1944
Epoch 17/300, seasonal_3 Loss: 0.1783 | 0.1761
Epoch 18/300, seasonal_3 Loss: 0.1749 | 0.2001
Epoch 19/300, seasonal_3 Loss: 0.1716 | 0.1484
Epoch 20/300, seasonal_3 Loss: 0.1681 | 0.1639
Epoch 21/300, seasonal_3 Loss: 0.1719 | 0.1880
Epoch 22/300, seasonal_3 Loss: 0.1651 | 0.1716
Epoch 23/300, seasonal_3 Loss: 0.1645 | 0.2130
Epoch 24/300, seasonal_3 Loss: 0.1626 | 0.1769
Epoch 25/300, seasonal_3 Loss: 0.1592 | 0.1970
Epoch 26/300, seasonal_3 Loss: 0.1583 | 0.1666
Epoch 27/300, seasonal_3 Loss: 0.1517 | 0.1638
Epoch 28/300, seasonal_3 Loss: 0.1510 | 0.1432
Epoch 29/300, seasonal_3 Loss: 0.1472 | 0.1637
Epoch 30/300, seasonal_3 Loss: 0.1462 | 0.1678
Epoch 31/300, seasonal_3 Loss: 0.1447 | 0.1514
Epoch 32/300, seasonal_3 Loss: 0.1412 | 0.1639
Epoch 33/300, seasonal_3 Loss: 0.1395 | 0.1341
Epoch 34/300, seasonal_3 Loss: 0.1350 | 0.1319
Epoch 35/300, seasonal_3 Loss: 0.1355 | 0.1378
Epoch 36/300, seasonal_3 Loss: 0.1406 | 0.1437
Epoch 37/300, seasonal_3 Loss: 0.1331 | 0.1274
Epoch 38/300, seasonal_3 Loss: 0.1340 | 0.1850
Epoch 39/300, seasonal_3 Loss: 0.1297 | 0.1595
Epoch 40/300, seasonal_3 Loss: 0.1272 | 0.1240
Epoch 41/300, seasonal_3 Loss: 0.1247 | 0.1355
Epoch 42/300, seasonal_3 Loss: 0.1240 | 0.1283
Epoch 43/300, seasonal_3 Loss: 0.1238 | 0.1284
Epoch 44/300, seasonal_3 Loss: 0.1217 | 0.1390
Epoch 45/300, seasonal_3 Loss: 0.1207 | 0.1389
Epoch 46/300, seasonal_3 Loss: 0.1269 | 0.1702
Epoch 47/300, seasonal_3 Loss: 0.1280 | 0.1569
Epoch 48/300, seasonal_3 Loss: 0.1282 | 0.1163
Epoch 49/300, seasonal_3 Loss: 0.1233 | 0.1152
Epoch 50/300, seasonal_3 Loss: 0.1194 | 0.1347
Epoch 51/300, seasonal_3 Loss: 0.1184 | 0.1591
Epoch 52/300, seasonal_3 Loss: 0.1134 | 0.1282
Epoch 53/300, seasonal_3 Loss: 0.1099 | 0.1153
Epoch 54/300, seasonal_3 Loss: 0.1076 | 0.1129
Epoch 55/300, seasonal_3 Loss: 0.1074 | 0.1108
Epoch 56/300, seasonal_3 Loss: 0.1063 | 0.1249
Epoch 57/300, seasonal_3 Loss: 0.1056 | 0.1130
Epoch 58/300, seasonal_3 Loss: 0.1048 | 0.1125
Epoch 59/300, seasonal_3 Loss: 0.1027 | 0.1147
Epoch 60/300, seasonal_3 Loss: 0.1017 | 0.1164
Epoch 61/300, seasonal_3 Loss: 0.1012 | 0.1047
Epoch 62/300, seasonal_3 Loss: 0.1003 | 0.1072
Epoch 63/300, seasonal_3 Loss: 0.0999 | 0.1125
Epoch 64/300, seasonal_3 Loss: 0.0994 | 0.1192
Epoch 65/300, seasonal_3 Loss: 0.0998 | 0.1105
Epoch 66/300, seasonal_3 Loss: 0.0984 | 0.1047
Epoch 67/300, seasonal_3 Loss: 0.0978 | 0.0990
Epoch 68/300, seasonal_3 Loss: 0.0980 | 0.1002
Epoch 69/300, seasonal_3 Loss: 0.0969 | 0.1071
Epoch 70/300, seasonal_3 Loss: 0.0977 | 0.1202
Epoch 71/300, seasonal_3 Loss: 0.0981 | 0.1242
Epoch 72/300, seasonal_3 Loss: 0.0980 | 0.1022
Epoch 73/300, seasonal_3 Loss: 0.0968 | 0.0977
Epoch 74/300, seasonal_3 Loss: 0.0968 | 0.0990
Epoch 75/300, seasonal_3 Loss: 0.0954 | 0.1193
Epoch 76/300, seasonal_3 Loss: 0.0941 | 0.1115
Epoch 77/300, seasonal_3 Loss: 0.0943 | 0.0987
Epoch 78/300, seasonal_3 Loss: 0.0932 | 0.0974
Epoch 79/300, seasonal_3 Loss: 0.0926 | 0.1033
Epoch 80/300, seasonal_3 Loss: 0.0914 | 0.1112
Epoch 81/300, seasonal_3 Loss: 0.0915 | 0.1014
Epoch 82/300, seasonal_3 Loss: 0.0911 | 0.0967
Epoch 83/300, seasonal_3 Loss: 0.0909 | 0.0978
Epoch 84/300, seasonal_3 Loss: 0.0895 | 0.1042
Epoch 85/300, seasonal_3 Loss: 0.0897 | 0.1082
Epoch 86/300, seasonal_3 Loss: 0.0895 | 0.0987
Epoch 87/300, seasonal_3 Loss: 0.0892 | 0.1013
Epoch 88/300, seasonal_3 Loss: 0.0893 | 0.1043
Epoch 89/300, seasonal_3 Loss: 0.0886 | 0.1010
Epoch 90/300, seasonal_3 Loss: 0.0876 | 0.0991
Epoch 91/300, seasonal_3 Loss: 0.0879 | 0.0956
Epoch 92/300, seasonal_3 Loss: 0.0874 | 0.1030
Epoch 93/300, seasonal_3 Loss: 0.0869 | 0.0999
Epoch 94/300, seasonal_3 Loss: 0.0863 | 0.0997
Epoch 95/300, seasonal_3 Loss: 0.0869 | 0.0998
Epoch 96/300, seasonal_3 Loss: 0.0865 | 0.0975
Epoch 97/300, seasonal_3 Loss: 0.0869 | 0.0972
Epoch 98/300, seasonal_3 Loss: 0.0865 | 0.0954
Epoch 99/300, seasonal_3 Loss: 0.0862 | 0.0969
Epoch 100/300, seasonal_3 Loss: 0.0867 | 0.1047
Epoch 101/300, seasonal_3 Loss: 0.0860 | 0.0995
Epoch 102/300, seasonal_3 Loss: 0.0857 | 0.0976
Epoch 103/300, seasonal_3 Loss: 0.0858 | 0.0929
Epoch 104/300, seasonal_3 Loss: 0.0856 | 0.0962
Epoch 105/300, seasonal_3 Loss: 0.0851 | 0.1017
Epoch 106/300, seasonal_3 Loss: 0.0847 | 0.0993
Epoch 107/300, seasonal_3 Loss: 0.0851 | 0.0948
Epoch 108/300, seasonal_3 Loss: 0.0845 | 0.0935
Epoch 109/300, seasonal_3 Loss: 0.0846 | 0.0980
Epoch 110/300, seasonal_3 Loss: 0.0841 | 0.0991
Epoch 111/300, seasonal_3 Loss: 0.0844 | 0.0962
Epoch 112/300, seasonal_3 Loss: 0.0843 | 0.0976
Epoch 113/300, seasonal_3 Loss: 0.0843 | 0.0938
Epoch 114/300, seasonal_3 Loss: 0.0840 | 0.0948
Epoch 115/300, seasonal_3 Loss: 0.0837 | 0.0986
Epoch 116/300, seasonal_3 Loss: 0.0835 | 0.0959
Epoch 117/300, seasonal_3 Loss: 0.0837 | 0.0951
Epoch 118/300, seasonal_3 Loss: 0.0837 | 0.0937
Epoch 119/300, seasonal_3 Loss: 0.0827 | 0.0975
Epoch 120/300, seasonal_3 Loss: 0.0834 | 0.0968
Epoch 121/300, seasonal_3 Loss: 0.0826 | 0.0955
Epoch 122/300, seasonal_3 Loss: 0.0830 | 0.0944
Epoch 123/300, seasonal_3 Loss: 0.0827 | 0.0954
Epoch 124/300, seasonal_3 Loss: 0.0826 | 0.0941
Epoch 125/300, seasonal_3 Loss: 0.0831 | 0.0975
Epoch 126/300, seasonal_3 Loss: 0.0820 | 0.0955
Epoch 127/300, seasonal_3 Loss: 0.0822 | 0.0956
Epoch 128/300, seasonal_3 Loss: 0.0825 | 0.0968
Epoch 129/300, seasonal_3 Loss: 0.0821 | 0.0954
Epoch 130/300, seasonal_3 Loss: 0.0814 | 0.0945
Epoch 131/300, seasonal_3 Loss: 0.0816 | 0.0938
Epoch 132/300, seasonal_3 Loss: 0.0814 | 0.0938
Epoch 133/300, seasonal_3 Loss: 0.0819 | 0.0934
Epoch 134/300, seasonal_3 Loss: 0.0815 | 0.0937
Epoch 135/300, seasonal_3 Loss: 0.0817 | 0.0942
Epoch 136/300, seasonal_3 Loss: 0.0822 | 0.0932
Epoch 137/300, seasonal_3 Loss: 0.0813 | 0.0926
Epoch 138/300, seasonal_3 Loss: 0.0817 | 0.0939
Epoch 139/300, seasonal_3 Loss: 0.0818 | 0.0933
Epoch 140/300, seasonal_3 Loss: 0.0816 | 0.0934
Epoch 141/300, seasonal_3 Loss: 0.0814 | 0.0941
Epoch 142/300, seasonal_3 Loss: 0.0820 | 0.0927
Epoch 143/300, seasonal_3 Loss: 0.0815 | 0.0944
Epoch 144/300, seasonal_3 Loss: 0.0811 | 0.0941
Epoch 145/300, seasonal_3 Loss: 0.0809 | 0.0939
Epoch 146/300, seasonal_3 Loss: 0.0809 | 0.0920
Epoch 147/300, seasonal_3 Loss: 0.0809 | 0.0930
Epoch 148/300, seasonal_3 Loss: 0.0809 | 0.0942
Epoch 149/300, seasonal_3 Loss: 0.0816 | 0.0929
Epoch 150/300, seasonal_3 Loss: 0.0810 | 0.0937
Epoch 151/300, seasonal_3 Loss: 0.0809 | 0.0931
Epoch 152/300, seasonal_3 Loss: 0.0810 | 0.0925
Epoch 153/300, seasonal_3 Loss: 0.0811 | 0.0940
Epoch 154/300, seasonal_3 Loss: 0.0812 | 0.0933
Epoch 155/300, seasonal_3 Loss: 0.0809 | 0.0935
Epoch 156/300, seasonal_3 Loss: 0.0808 | 0.0937
Epoch 157/300, seasonal_3 Loss: 0.0809 | 0.0931
Epoch 158/300, seasonal_3 Loss: 0.0804 | 0.0933
Epoch 159/300, seasonal_3 Loss: 0.0805 | 0.0931
Epoch 160/300, seasonal_3 Loss: 0.0805 | 0.0932
Epoch 161/300, seasonal_3 Loss: 0.0806 | 0.0936
Epoch 162/300, seasonal_3 Loss: 0.0803 | 0.0933
Epoch 163/300, seasonal_3 Loss: 0.0808 | 0.0933
Epoch 164/300, seasonal_3 Loss: 0.0799 | 0.0929
Epoch 165/300, seasonal_3 Loss: 0.0806 | 0.0918
Epoch 166/300, seasonal_3 Loss: 0.0802 | 0.0931
Epoch 167/300, seasonal_3 Loss: 0.0802 | 0.0928
Epoch 168/300, seasonal_3 Loss: 0.0806 | 0.0915
Epoch 169/300, seasonal_3 Loss: 0.0802 | 0.0925
Epoch 170/300, seasonal_3 Loss: 0.0809 | 0.0919
Epoch 171/300, seasonal_3 Loss: 0.0799 | 0.0924
Epoch 172/300, seasonal_3 Loss: 0.0807 | 0.0930
Epoch 173/300, seasonal_3 Loss: 0.0798 | 0.0925
Epoch 174/300, seasonal_3 Loss: 0.0805 | 0.0918
Epoch 175/300, seasonal_3 Loss: 0.0805 | 0.0918
Epoch 176/300, seasonal_3 Loss: 0.0810 | 0.0919
Epoch 177/300, seasonal_3 Loss: 0.0802 | 0.0920
Epoch 178/300, seasonal_3 Loss: 0.0808 | 0.0922
Epoch 179/300, seasonal_3 Loss: 0.0806 | 0.0921
Epoch 180/300, seasonal_3 Loss: 0.0800 | 0.0921
Epoch 181/300, seasonal_3 Loss: 0.0809 | 0.0917
Epoch 182/300, seasonal_3 Loss: 0.0799 | 0.0926
Epoch 183/300, seasonal_3 Loss: 0.0803 | 0.0929
Epoch 184/300, seasonal_3 Loss: 0.0799 | 0.0919
Epoch 185/300, seasonal_3 Loss: 0.0801 | 0.0934
Epoch 186/300, seasonal_3 Loss: 0.0807 | 0.0922
Epoch 187/300, seasonal_3 Loss: 0.0795 | 0.0919
Epoch 188/300, seasonal_3 Loss: 0.0803 | 0.0921
Epoch 189/300, seasonal_3 Loss: 0.0802 | 0.0925
Epoch 190/300, seasonal_3 Loss: 0.0800 | 0.0928
Epoch 191/300, seasonal_3 Loss: 0.0802 | 0.0923
Epoch 192/300, seasonal_3 Loss: 0.0790 | 0.0918
Epoch 193/300, seasonal_3 Loss: 0.0807 | 0.0922
Epoch 194/300, seasonal_3 Loss: 0.0800 | 0.0924
Epoch 195/300, seasonal_3 Loss: 0.0805 | 0.0916
Epoch 196/300, seasonal_3 Loss: 0.0798 | 0.0919
Epoch 197/300, seasonal_3 Loss: 0.0792 | 0.0921
Epoch 198/300, seasonal_3 Loss: 0.0802 | 0.0925
Epoch 199/300, seasonal_3 Loss: 0.0796 | 0.0919
Epoch 200/300, seasonal_3 Loss: 0.0798 | 0.0919
Epoch 201/300, seasonal_3 Loss: 0.0803 | 0.0921
Epoch 202/300, seasonal_3 Loss: 0.0797 | 0.0927
Epoch 203/300, seasonal_3 Loss: 0.0801 | 0.0925
Epoch 204/300, seasonal_3 Loss: 0.0795 | 0.0918
Epoch 205/300, seasonal_3 Loss: 0.0797 | 0.0917
Epoch 206/300, seasonal_3 Loss: 0.0800 | 0.0915
Epoch 207/300, seasonal_3 Loss: 0.0799 | 0.0912
Epoch 208/300, seasonal_3 Loss: 0.0793 | 0.0916
Epoch 209/300, seasonal_3 Loss: 0.0797 | 0.0916
Epoch 210/300, seasonal_3 Loss: 0.0805 | 0.0915
Epoch 211/300, seasonal_3 Loss: 0.0795 | 0.0920
Epoch 212/300, seasonal_3 Loss: 0.0798 | 0.0915
Epoch 213/300, seasonal_3 Loss: 0.0799 | 0.0918
Epoch 214/300, seasonal_3 Loss: 0.0800 | 0.0913
Epoch 215/300, seasonal_3 Loss: 0.0802 | 0.0915
Epoch 216/300, seasonal_3 Loss: 0.0796 | 0.0914
Epoch 217/300, seasonal_3 Loss: 0.0800 | 0.0916
Epoch 218/300, seasonal_3 Loss: 0.0799 | 0.0920
Epoch 219/300, seasonal_3 Loss: 0.0800 | 0.0920
Epoch 220/300, seasonal_3 Loss: 0.0807 | 0.0916
Epoch 221/300, seasonal_3 Loss: 0.0803 | 0.0917
Epoch 222/300, seasonal_3 Loss: 0.0799 | 0.0920
Epoch 223/300, seasonal_3 Loss: 0.0796 | 0.0920
Epoch 224/300, seasonal_3 Loss: 0.0797 | 0.0921
Epoch 225/300, seasonal_3 Loss: 0.0797 | 0.0920
Epoch 226/300, seasonal_3 Loss: 0.0798 | 0.0920
Epoch 227/300, seasonal_3 Loss: 0.0799 | 0.0918
Epoch 228/300, seasonal_3 Loss: 0.0801 | 0.0918
Epoch 229/300, seasonal_3 Loss: 0.0791 | 0.0919
Epoch 230/300, seasonal_3 Loss: 0.0797 | 0.0920
Epoch 231/300, seasonal_3 Loss: 0.0807 | 0.0920
Epoch 232/300, seasonal_3 Loss: 0.0791 | 0.0917
Epoch 233/300, seasonal_3 Loss: 0.0795 | 0.0918
Epoch 234/300, seasonal_3 Loss: 0.0791 | 0.0919
Epoch 235/300, seasonal_3 Loss: 0.0802 | 0.0918
Epoch 236/300, seasonal_3 Loss: 0.0797 | 0.0918
Epoch 237/300, seasonal_3 Loss: 0.0795 | 0.0918
Epoch 238/300, seasonal_3 Loss: 0.0801 | 0.0916
Epoch 239/300, seasonal_3 Loss: 0.0791 | 0.0917
Epoch 240/300, seasonal_3 Loss: 0.0805 | 0.0916
Epoch 241/300, seasonal_3 Loss: 0.0796 | 0.0918
Epoch 242/300, seasonal_3 Loss: 0.0792 | 0.0920
Epoch 243/300, seasonal_3 Loss: 0.0793 | 0.0920
Epoch 244/300, seasonal_3 Loss: 0.0797 | 0.0921
Epoch 245/300, seasonal_3 Loss: 0.0799 | 0.0922
Epoch 246/300, seasonal_3 Loss: 0.0798 | 0.0920
Epoch 247/300, seasonal_3 Loss: 0.0802 | 0.0921
Epoch 248/300, seasonal_3 Loss: 0.0792 | 0.0920
Epoch 249/300, seasonal_3 Loss: 0.0790 | 0.0920
Epoch 250/300, seasonal_3 Loss: 0.0791 | 0.0921
Epoch 251/300, seasonal_3 Loss: 0.0791 | 0.0920
Epoch 252/300, seasonal_3 Loss: 0.0799 | 0.0921
Epoch 253/300, seasonal_3 Loss: 0.0798 | 0.0920
Epoch 254/300, seasonal_3 Loss: 0.0800 | 0.0921
Epoch 255/300, seasonal_3 Loss: 0.0797 | 0.0921
Epoch 256/300, seasonal_3 Loss: 0.0789 | 0.0919
Epoch 257/300, seasonal_3 Loss: 0.0792 | 0.0920
Epoch 258/300, seasonal_3 Loss: 0.0797 | 0.0920
Epoch 259/300, seasonal_3 Loss: 0.0796 | 0.0921
Epoch 260/300, seasonal_3 Loss: 0.0793 | 0.0919
Epoch 261/300, seasonal_3 Loss: 0.0795 | 0.0920
Epoch 262/300, seasonal_3 Loss: 0.0799 | 0.0921
Epoch 263/300, seasonal_3 Loss: 0.0795 | 0.0920
Epoch 264/300, seasonal_3 Loss: 0.0807 | 0.0918
Epoch 265/300, seasonal_3 Loss: 0.0794 | 0.0918
Epoch 266/300, seasonal_3 Loss: 0.0791 | 0.0918
Epoch 267/300, seasonal_3 Loss: 0.0795 | 0.0917
Epoch 268/300, seasonal_3 Loss: 0.0793 | 0.0917
Epoch 269/300, seasonal_3 Loss: 0.0796 | 0.0918
Epoch 270/300, seasonal_3 Loss: 0.0795 | 0.0917
Epoch 271/300, seasonal_3 Loss: 0.0786 | 0.0916
Epoch 272/300, seasonal_3 Loss: 0.0791 | 0.0916
Epoch 273/300, seasonal_3 Loss: 0.0792 | 0.0917
Epoch 274/300, seasonal_3 Loss: 0.0792 | 0.0918
Epoch 275/300, seasonal_3 Loss: 0.0792 | 0.0918
Epoch 276/300, seasonal_3 Loss: 0.0792 | 0.0918
Epoch 277/300, seasonal_3 Loss: 0.0804 | 0.0917
Epoch 278/300, seasonal_3 Loss: 0.0791 | 0.0918
Epoch 279/300, seasonal_3 Loss: 0.0798 | 0.0918
Epoch 280/300, seasonal_3 Loss: 0.0801 | 0.0917
Epoch 281/300, seasonal_3 Loss: 0.0786 | 0.0917
Epoch 282/300, seasonal_3 Loss: 0.0794 | 0.0918
Epoch 283/300, seasonal_3 Loss: 0.0789 | 0.0918
Epoch 284/300, seasonal_3 Loss: 0.0797 | 0.0918
Epoch 285/300, seasonal_3 Loss: 0.0798 | 0.0918
Epoch 286/300, seasonal_3 Loss: 0.0793 | 0.0918
Epoch 287/300, seasonal_3 Loss: 0.0792 | 0.0917
Epoch 288/300, seasonal_3 Loss: 0.0793 | 0.0917
Epoch 289/300, seasonal_3 Loss: 0.0791 | 0.0918
Epoch 290/300, seasonal_3 Loss: 0.0796 | 0.0918
Epoch 291/300, seasonal_3 Loss: 0.0794 | 0.0919
Epoch 292/300, seasonal_3 Loss: 0.0798 | 0.0919
Epoch 293/300, seasonal_3 Loss: 0.0792 | 0.0919
Epoch 294/300, seasonal_3 Loss: 0.0801 | 0.0918
Epoch 295/300, seasonal_3 Loss: 0.0788 | 0.0918
Epoch 296/300, seasonal_3 Loss: 0.0797 | 0.0918
Epoch 297/300, seasonal_3 Loss: 0.0800 | 0.0917
Epoch 298/300, seasonal_3 Loss: 0.0794 | 0.0917
Epoch 299/300, seasonal_3 Loss: 0.0796 | 0.0917
Epoch 300/300, seasonal_3 Loss: 0.0790 | 0.0917
Training resid component with params: {'observation_period_num': 170, 'train_rates': 0.9510232077405274, 'learning_rate': 5.6803480664591135e-05, 'batch_size': 18, 'step_size': 10, 'gamma': 0.84127239286545}
Epoch 1/300, resid Loss: 0.5672 | 0.7242
Epoch 2/300, resid Loss: 0.4206 | 0.6458
Epoch 3/300, resid Loss: 0.3363 | 0.3865
Epoch 4/300, resid Loss: 0.2762 | 0.3615
Epoch 5/300, resid Loss: 0.2541 | 0.3479
Epoch 6/300, resid Loss: 0.2390 | 0.4621
Epoch 7/300, resid Loss: 0.2313 | 0.3421
Epoch 8/300, resid Loss: 0.2307 | 0.3729
Epoch 9/300, resid Loss: 0.2300 | 0.2856
Epoch 10/300, resid Loss: 0.2224 | 0.4593
Epoch 11/300, resid Loss: 0.2271 | 0.3384
Epoch 12/300, resid Loss: 0.2066 | 0.2765
Epoch 13/300, resid Loss: 0.2041 | 0.2306
Epoch 14/300, resid Loss: 0.1853 | 0.2475
Epoch 15/300, resid Loss: 0.1791 | 0.2149
Epoch 16/300, resid Loss: 0.1768 | 0.2388
Epoch 17/300, resid Loss: 0.1804 | 0.2229
Epoch 18/300, resid Loss: 0.1764 | 0.2464
Epoch 19/300, resid Loss: 0.1736 | 0.2001
Epoch 20/300, resid Loss: 0.1720 | 0.2187
Epoch 21/300, resid Loss: 0.1811 | 0.2174
Epoch 22/300, resid Loss: 0.1716 | 0.2120
Epoch 23/300, resid Loss: 0.1651 | 0.1671
Epoch 24/300, resid Loss: 0.1575 | 0.1636
Epoch 25/300, resid Loss: 0.1572 | 0.2007
Epoch 26/300, resid Loss: 0.1521 | 0.1972
Epoch 27/300, resid Loss: 0.1499 | 0.1624
Epoch 28/300, resid Loss: 0.1468 | 0.1622
Epoch 29/300, resid Loss: 0.1465 | 0.1978
Epoch 30/300, resid Loss: 0.1446 | 0.1956
Epoch 31/300, resid Loss: 0.1474 | 0.1694
Epoch 32/300, resid Loss: 0.1465 | 0.1639
Epoch 33/300, resid Loss: 0.1445 | 0.1603
Epoch 34/300, resid Loss: 0.1420 | 0.1749
Epoch 35/300, resid Loss: 0.1381 | 0.1483
Epoch 36/300, resid Loss: 0.1387 | 0.1435
Epoch 37/300, resid Loss: 0.1352 | 0.1493
Epoch 38/300, resid Loss: 0.1329 | 0.1522
Epoch 39/300, resid Loss: 0.1311 | 0.1425
Epoch 40/300, resid Loss: 0.1306 | 0.1351
Epoch 41/300, resid Loss: 0.1310 | 0.1456
Epoch 42/300, resid Loss: 0.1295 | 0.1472
Epoch 43/300, resid Loss: 0.1270 | 0.1377
Epoch 44/300, resid Loss: 0.1264 | 0.1350
Epoch 45/300, resid Loss: 0.1259 | 0.1338
Epoch 46/300, resid Loss: 0.1254 | 0.1418
Epoch 47/300, resid Loss: 0.1240 | 0.1324
Epoch 48/300, resid Loss: 0.1229 | 0.1399
Epoch 49/300, resid Loss: 0.1219 | 0.1393
Epoch 50/300, resid Loss: 0.1221 | 0.1353
Epoch 51/300, resid Loss: 0.1216 | 0.1295
Epoch 52/300, resid Loss: 0.1213 | 0.1310
Epoch 53/300, resid Loss: 0.1202 | 0.1343
Epoch 54/300, resid Loss: 0.1193 | 0.1324
Epoch 55/300, resid Loss: 0.1185 | 0.1301
Epoch 56/300, resid Loss: 0.1184 | 0.1264
Epoch 57/300, resid Loss: 0.1172 | 0.1269
Epoch 58/300, resid Loss: 0.1163 | 0.1288
Epoch 59/300, resid Loss: 0.1163 | 0.1274
Epoch 60/300, resid Loss: 0.1162 | 0.1263
Epoch 61/300, resid Loss: 0.1151 | 0.1226
Epoch 62/300, resid Loss: 0.1149 | 0.1267
Epoch 63/300, resid Loss: 0.1144 | 0.1282
Epoch 64/300, resid Loss: 0.1139 | 0.1248
Epoch 65/300, resid Loss: 0.1134 | 0.1235
Epoch 66/300, resid Loss: 0.1139 | 0.1242
Epoch 67/300, resid Loss: 0.1125 | 0.1250
Epoch 68/300, resid Loss: 0.1135 | 0.1224
Epoch 69/300, resid Loss: 0.1124 | 0.1228
Epoch 70/300, resid Loss: 0.1117 | 0.1212
Epoch 71/300, resid Loss: 0.1112 | 0.1225
Epoch 72/300, resid Loss: 0.1111 | 0.1230
Epoch 73/300, resid Loss: 0.1111 | 0.1218
Epoch 74/300, resid Loss: 0.1112 | 0.1215
Epoch 75/300, resid Loss: 0.1114 | 0.1216
Epoch 76/300, resid Loss: 0.1107 | 0.1219
Epoch 77/300, resid Loss: 0.1104 | 0.1213
Epoch 78/300, resid Loss: 0.1100 | 0.1207
Epoch 79/300, resid Loss: 0.1102 | 0.1216
Epoch 80/300, resid Loss: 0.1096 | 0.1207
Epoch 81/300, resid Loss: 0.1092 | 0.1213
Epoch 82/300, resid Loss: 0.1090 | 0.1209
Epoch 83/300, resid Loss: 0.1086 | 0.1196
Epoch 84/300, resid Loss: 0.1090 | 0.1193
Epoch 85/300, resid Loss: 0.1086 | 0.1202
Epoch 86/300, resid Loss: 0.1087 | 0.1194
Epoch 87/300, resid Loss: 0.1078 | 0.1201
Epoch 88/300, resid Loss: 0.1081 | 0.1194
Epoch 89/300, resid Loss: 0.1087 | 0.1202
Epoch 90/300, resid Loss: 0.1077 | 0.1187
Epoch 91/300, resid Loss: 0.1076 | 0.1206
Epoch 92/300, resid Loss: 0.1082 | 0.1198
Epoch 93/300, resid Loss: 0.1075 | 0.1196
Epoch 94/300, resid Loss: 0.1077 | 0.1190
Epoch 95/300, resid Loss: 0.1076 | 0.1190
Epoch 96/300, resid Loss: 0.1075 | 0.1189
Epoch 97/300, resid Loss: 0.1069 | 0.1186
Epoch 98/300, resid Loss: 0.1072 | 0.1184
Epoch 99/300, resid Loss: 0.1068 | 0.1182
Epoch 100/300, resid Loss: 0.1074 | 0.1180
Epoch 101/300, resid Loss: 0.1069 | 0.1183
Epoch 102/300, resid Loss: 0.1070 | 0.1182
Epoch 103/300, resid Loss: 0.1067 | 0.1189
Epoch 104/300, resid Loss: 0.1061 | 0.1179
Epoch 105/300, resid Loss: 0.1063 | 0.1178
Epoch 106/300, resid Loss: 0.1061 | 0.1178
Epoch 107/300, resid Loss: 0.1064 | 0.1184
Epoch 108/300, resid Loss: 0.1065 | 0.1180
Epoch 109/300, resid Loss: 0.1061 | 0.1183
Epoch 110/300, resid Loss: 0.1063 | 0.1176
Epoch 111/300, resid Loss: 0.1061 | 0.1177
Epoch 112/300, resid Loss: 0.1060 | 0.1173
Epoch 113/300, resid Loss: 0.1054 | 0.1176
Epoch 114/300, resid Loss: 0.1057 | 0.1169
Epoch 115/300, resid Loss: 0.1056 | 0.1178
Epoch 116/300, resid Loss: 0.1064 | 0.1174
Epoch 117/300, resid Loss: 0.1054 | 0.1169
Epoch 118/300, resid Loss: 0.1059 | 0.1168
Epoch 119/300, resid Loss: 0.1054 | 0.1170
Epoch 120/300, resid Loss: 0.1062 | 0.1168
Epoch 121/300, resid Loss: 0.1058 | 0.1172
Epoch 122/300, resid Loss: 0.1059 | 0.1167
Epoch 123/300, resid Loss: 0.1052 | 0.1168
Epoch 124/300, resid Loss: 0.1054 | 0.1168
Epoch 125/300, resid Loss: 0.1043 | 0.1168
Epoch 126/300, resid Loss: 0.1048 | 0.1168
Epoch 127/300, resid Loss: 0.1050 | 0.1170
Epoch 128/300, resid Loss: 0.1050 | 0.1171
Epoch 129/300, resid Loss: 0.1046 | 0.1170
Epoch 130/300, resid Loss: 0.1051 | 0.1165
Epoch 131/300, resid Loss: 0.1052 | 0.1165
Epoch 132/300, resid Loss: 0.1056 | 0.1167
Epoch 133/300, resid Loss: 0.1048 | 0.1170
Epoch 134/300, resid Loss: 0.1052 | 0.1171
Epoch 135/300, resid Loss: 0.1046 | 0.1168
Epoch 136/300, resid Loss: 0.1053 | 0.1167
Epoch 137/300, resid Loss: 0.1049 | 0.1165
Epoch 138/300, resid Loss: 0.1044 | 0.1163
Epoch 139/300, resid Loss: 0.1050 | 0.1163
Epoch 140/300, resid Loss: 0.1042 | 0.1162
Epoch 141/300, resid Loss: 0.1045 | 0.1161
Epoch 142/300, resid Loss: 0.1045 | 0.1162
Epoch 143/300, resid Loss: 0.1044 | 0.1164
Epoch 144/300, resid Loss: 0.1053 | 0.1166
Epoch 145/300, resid Loss: 0.1049 | 0.1165
Epoch 146/300, resid Loss: 0.1052 | 0.1165
Epoch 147/300, resid Loss: 0.1041 | 0.1163
Epoch 148/300, resid Loss: 0.1047 | 0.1162
Epoch 149/300, resid Loss: 0.1040 | 0.1162
Epoch 150/300, resid Loss: 0.1045 | 0.1164
Epoch 151/300, resid Loss: 0.1046 | 0.1163
Epoch 152/300, resid Loss: 0.1047 | 0.1162
Epoch 153/300, resid Loss: 0.1051 | 0.1161
Epoch 154/300, resid Loss: 0.1041 | 0.1163
Epoch 155/300, resid Loss: 0.1042 | 0.1165
Epoch 156/300, resid Loss: 0.1049 | 0.1164
Epoch 157/300, resid Loss: 0.1050 | 0.1164
Epoch 158/300, resid Loss: 0.1050 | 0.1163
Epoch 159/300, resid Loss: 0.1043 | 0.1162
Epoch 160/300, resid Loss: 0.1048 | 0.1164
Epoch 161/300, resid Loss: 0.1043 | 0.1165
Epoch 162/300, resid Loss: 0.1042 | 0.1166
Epoch 163/300, resid Loss: 0.1039 | 0.1164
Epoch 164/300, resid Loss: 0.1044 | 0.1165
Epoch 165/300, resid Loss: 0.1045 | 0.1164
Epoch 166/300, resid Loss: 0.1045 | 0.1165
Epoch 167/300, resid Loss: 0.1044 | 0.1163
Epoch 168/300, resid Loss: 0.1040 | 0.1162
Epoch 169/300, resid Loss: 0.1040 | 0.1163
Epoch 170/300, resid Loss: 0.1047 | 0.1162
Epoch 171/300, resid Loss: 0.1048 | 0.1162
Epoch 172/300, resid Loss: 0.1045 | 0.1161
Epoch 173/300, resid Loss: 0.1042 | 0.1161
Epoch 174/300, resid Loss: 0.1045 | 0.1162
Epoch 175/300, resid Loss: 0.1044 | 0.1162
Epoch 176/300, resid Loss: 0.1043 | 0.1162
Epoch 177/300, resid Loss: 0.1040 | 0.1162
Epoch 178/300, resid Loss: 0.1045 | 0.1162
Epoch 179/300, resid Loss: 0.1049 | 0.1163
Epoch 180/300, resid Loss: 0.1038 | 0.1163
Epoch 181/300, resid Loss: 0.1044 | 0.1162
Epoch 182/300, resid Loss: 0.1040 | 0.1161
Epoch 183/300, resid Loss: 0.1044 | 0.1161
Epoch 184/300, resid Loss: 0.1042 | 0.1161
Epoch 185/300, resid Loss: 0.1044 | 0.1161
Epoch 186/300, resid Loss: 0.1047 | 0.1161
Epoch 187/300, resid Loss: 0.1043 | 0.1161
Epoch 188/300, resid Loss: 0.1046 | 0.1161
Epoch 189/300, resid Loss: 0.1039 | 0.1161
Epoch 190/300, resid Loss: 0.1045 | 0.1161
Epoch 191/300, resid Loss: 0.1046 | 0.1161
Epoch 192/300, resid Loss: 0.1039 | 0.1161
Epoch 193/300, resid Loss: 0.1044 | 0.1161
Epoch 194/300, resid Loss: 0.1040 | 0.1161
Epoch 195/300, resid Loss: 0.1041 | 0.1161
Epoch 196/300, resid Loss: 0.1045 | 0.1161
Epoch 197/300, resid Loss: 0.1046 | 0.1161
Epoch 198/300, resid Loss: 0.1046 | 0.1161
Epoch 199/300, resid Loss: 0.1046 | 0.1161
Epoch 200/300, resid Loss: 0.1040 | 0.1160
Epoch 201/300, resid Loss: 0.1040 | 0.1161
Epoch 202/300, resid Loss: 0.1040 | 0.1161
Epoch 203/300, resid Loss: 0.1045 | 0.1161
Epoch 204/300, resid Loss: 0.1041 | 0.1161
Epoch 205/300, resid Loss: 0.1038 | 0.1161
Epoch 206/300, resid Loss: 0.1042 | 0.1161
Epoch 207/300, resid Loss: 0.1042 | 0.1161
Epoch 208/300, resid Loss: 0.1039 | 0.1161
Epoch 209/300, resid Loss: 0.1042 | 0.1161
Epoch 210/300, resid Loss: 0.1033 | 0.1161
Epoch 211/300, resid Loss: 0.1039 | 0.1161
Epoch 212/300, resid Loss: 0.1043 | 0.1161
Epoch 213/300, resid Loss: 0.1036 | 0.1161
Epoch 214/300, resid Loss: 0.1042 | 0.1162
Epoch 215/300, resid Loss: 0.1035 | 0.1162
Epoch 216/300, resid Loss: 0.1045 | 0.1162
Epoch 217/300, resid Loss: 0.1036 | 0.1162
Epoch 218/300, resid Loss: 0.1040 | 0.1162
Epoch 219/300, resid Loss: 0.1038 | 0.1162
Epoch 220/300, resid Loss: 0.1044 | 0.1162
Epoch 221/300, resid Loss: 0.1047 | 0.1161
Epoch 222/300, resid Loss: 0.1046 | 0.1161
Epoch 223/300, resid Loss: 0.1048 | 0.1162
Epoch 224/300, resid Loss: 0.1035 | 0.1161
Epoch 225/300, resid Loss: 0.1042 | 0.1161
Epoch 226/300, resid Loss: 0.1041 | 0.1161
Epoch 227/300, resid Loss: 0.1042 | 0.1161
Epoch 228/300, resid Loss: 0.1043 | 0.1161
Epoch 229/300, resid Loss: 0.1044 | 0.1161
Epoch 230/300, resid Loss: 0.1043 | 0.1161
Epoch 231/300, resid Loss: 0.1043 | 0.1161
Epoch 232/300, resid Loss: 0.1041 | 0.1161
Epoch 233/300, resid Loss: 0.1040 | 0.1161
Epoch 234/300, resid Loss: 0.1046 | 0.1161
Epoch 235/300, resid Loss: 0.1041 | 0.1161
Epoch 236/300, resid Loss: 0.1043 | 0.1161
Epoch 237/300, resid Loss: 0.1034 | 0.1161
Epoch 238/300, resid Loss: 0.1032 | 0.1161
Epoch 239/300, resid Loss: 0.1049 | 0.1161
Epoch 240/300, resid Loss: 0.1045 | 0.1161
Epoch 241/300, resid Loss: 0.1038 | 0.1161
Epoch 242/300, resid Loss: 0.1045 | 0.1161
Epoch 243/300, resid Loss: 0.1038 | 0.1161
Epoch 244/300, resid Loss: 0.1040 | 0.1161
Epoch 245/300, resid Loss: 0.1054 | 0.1161
Epoch 246/300, resid Loss: 0.1039 | 0.1161
Epoch 247/300, resid Loss: 0.1037 | 0.1161
Epoch 248/300, resid Loss: 0.1043 | 0.1161
Epoch 249/300, resid Loss: 0.1044 | 0.1161
Epoch 250/300, resid Loss: 0.1037 | 0.1161
Epoch 251/300, resid Loss: 0.1041 | 0.1161
Epoch 252/300, resid Loss: 0.1043 | 0.1161
Epoch 253/300, resid Loss: 0.1046 | 0.1161
Epoch 254/300, resid Loss: 0.1043 | 0.1161
Epoch 255/300, resid Loss: 0.1044 | 0.1161
Epoch 256/300, resid Loss: 0.1038 | 0.1161
Epoch 257/300, resid Loss: 0.1043 | 0.1161
Epoch 258/300, resid Loss: 0.1035 | 0.1161
Epoch 259/300, resid Loss: 0.1041 | 0.1161
Epoch 260/300, resid Loss: 0.1044 | 0.1161
Epoch 261/300, resid Loss: 0.1045 | 0.1161
Epoch 262/300, resid Loss: 0.1039 | 0.1161
Epoch 263/300, resid Loss: 0.1047 | 0.1161
Epoch 264/300, resid Loss: 0.1043 | 0.1161
Epoch 265/300, resid Loss: 0.1045 | 0.1161
Epoch 266/300, resid Loss: 0.1046 | 0.1161
Epoch 267/300, resid Loss: 0.1034 | 0.1161
Epoch 268/300, resid Loss: 0.1047 | 0.1161
Epoch 269/300, resid Loss: 0.1041 | 0.1161
Epoch 270/300, resid Loss: 0.1044 | 0.1161
Epoch 271/300, resid Loss: 0.1048 | 0.1161
Epoch 272/300, resid Loss: 0.1047 | 0.1161
Epoch 273/300, resid Loss: 0.1044 | 0.1161
Epoch 274/300, resid Loss: 0.1042 | 0.1161
Epoch 275/300, resid Loss: 0.1043 | 0.1161
Epoch 276/300, resid Loss: 0.1041 | 0.1161
Epoch 277/300, resid Loss: 0.1041 | 0.1161
Epoch 278/300, resid Loss: 0.1036 | 0.1161
Epoch 279/300, resid Loss: 0.1045 | 0.1161
Epoch 280/300, resid Loss: 0.1043 | 0.1161
Epoch 281/300, resid Loss: 0.1043 | 0.1161
Epoch 282/300, resid Loss: 0.1039 | 0.1161
Epoch 283/300, resid Loss: 0.1036 | 0.1161
Epoch 284/300, resid Loss: 0.1036 | 0.1161
Epoch 285/300, resid Loss: 0.1047 | 0.1161
Epoch 286/300, resid Loss: 0.1043 | 0.1161
Epoch 287/300, resid Loss: 0.1038 | 0.1161
Epoch 288/300, resid Loss: 0.1044 | 0.1161
Epoch 289/300, resid Loss: 0.1033 | 0.1161
Epoch 290/300, resid Loss: 0.1042 | 0.1161
Epoch 291/300, resid Loss: 0.1037 | 0.1161
Epoch 292/300, resid Loss: 0.1042 | 0.1161
Epoch 293/300, resid Loss: 0.1042 | 0.1161
Epoch 294/300, resid Loss: 0.1038 | 0.1161
Epoch 295/300, resid Loss: 0.1045 | 0.1161
Epoch 296/300, resid Loss: 0.1042 | 0.1161
Epoch 297/300, resid Loss: 0.1043 | 0.1161
Epoch 298/300, resid Loss: 0.1038 | 0.1161
Epoch 299/300, resid Loss: 0.1040 | 0.1161
Epoch 300/300, resid Loss: 0.1042 | 0.1161
Runtime (seconds): 4760.247079133987
0.0003981821204048228
[161.17496]
[-2.0845027]
[3.911602]
[11.872657]
[1.1894798]
[17.454685]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 12.968065219232813
RMSE: 3.6011199951171875
MAE: 3.6011199951171875
R-squared: nan
[193.51888]
File amzn_stock_price_prediction_by_Transformer.png exists. Logging to WandB.
