[32m[I 2025-02-06 15:39:01,737][0m A new study created in memory with name: no-name-7193c3aa-4874-4b98-a3b4-34f99aa893e7[0m
[32m[I 2025-02-06 15:41:22,806][0m Trial 0 finished with value: 0.19169299569226764 and parameters: {'observation_period_num': 239, 'train_rates': 0.7852802688429413, 'learning_rate': 6.168387693427368e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9467111041731233}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:41:45,202][0m Trial 1 finished with value: 0.3318152023680249 and parameters: {'observation_period_num': 69, 'train_rates': 0.6258639925778325, 'learning_rate': 1.6778343729928958e-05, 'batch_size': 222, 'step_size': 14, 'gamma': 0.8849952171091899}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:42:21,830][0m Trial 2 finished with value: 0.9061004327876228 and parameters: {'observation_period_num': 193, 'train_rates': 0.7400756045424657, 'learning_rate': 2.6329967719301814e-06, 'batch_size': 142, 'step_size': 13, 'gamma': 0.8143349177715493}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:42:48,058][0m Trial 3 finished with value: 1.326961723240939 and parameters: {'observation_period_num': 98, 'train_rates': 0.8060916865457954, 'learning_rate': 1.4559350166082477e-06, 'batch_size': 221, 'step_size': 3, 'gamma': 0.9155166751743271}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:43:20,664][0m Trial 4 finished with value: 0.43889903396875063 and parameters: {'observation_period_num': 203, 'train_rates': 0.6206547012278761, 'learning_rate': 0.00013327277958382665, 'batch_size': 144, 'step_size': 4, 'gamma': 0.8114728254553052}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:43:47,836][0m Trial 5 finished with value: 0.19397801160812378 and parameters: {'observation_period_num': 126, 'train_rates': 0.9230494579639509, 'learning_rate': 0.00010817934711178664, 'batch_size': 227, 'step_size': 12, 'gamma': 0.9511842474831469}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:44:11,021][0m Trial 6 finished with value: 0.3104267960904054 and parameters: {'observation_period_num': 185, 'train_rates': 0.6559688209728511, 'learning_rate': 0.0006127401774867839, 'batch_size': 214, 'step_size': 6, 'gamma': 0.9266202423509519}. Best is trial 0 with value: 0.19169299569226764.[0m
[32m[I 2025-02-06 15:45:16,165][0m Trial 7 finished with value: 0.16522077534427035 and parameters: {'observation_period_num': 168, 'train_rates': 0.9335820546405568, 'learning_rate': 4.8315330615781226e-05, 'batch_size': 90, 'step_size': 1, 'gamma': 0.9809893233213746}. Best is trial 7 with value: 0.16522077534427035.[0m
[32m[I 2025-02-06 15:45:58,780][0m Trial 8 finished with value: 0.27666821158849275 and parameters: {'observation_period_num': 209, 'train_rates': 0.9255521651079179, 'learning_rate': 4.6061257527146924e-05, 'batch_size': 138, 'step_size': 6, 'gamma': 0.9045292198856741}. Best is trial 7 with value: 0.16522077534427035.[0m
[32m[I 2025-02-06 15:46:43,617][0m Trial 9 finished with value: 0.5236870111631495 and parameters: {'observation_period_num': 149, 'train_rates': 0.8819912588848893, 'learning_rate': 4.90652665291426e-06, 'batch_size': 129, 'step_size': 8, 'gamma': 0.9387452292968526}. Best is trial 7 with value: 0.16522077534427035.[0m
[32m[I 2025-02-06 15:49:09,966][0m Trial 10 finished with value: 0.06593949875785309 and parameters: {'observation_period_num': 47, 'train_rates': 0.9650827945217765, 'learning_rate': 0.0008903377027419595, 'batch_size': 41, 'step_size': 1, 'gamma': 0.9869880244456225}. Best is trial 10 with value: 0.06593949875785309.[0m
[32m[I 2025-02-06 15:51:59,252][0m Trial 11 finished with value: 0.10048282891511917 and parameters: {'observation_period_num': 25, 'train_rates': 0.9890575535843114, 'learning_rate': 0.0008869978554158069, 'batch_size': 36, 'step_size': 1, 'gamma': 0.9824006065475632}. Best is trial 10 with value: 0.06593949875785309.[0m
[32m[I 2025-02-06 15:56:09,275][0m Trial 12 finished with value: 0.03858797401189804 and parameters: {'observation_period_num': 7, 'train_rates': 0.9665969069592619, 'learning_rate': 0.0008991218023547251, 'batch_size': 24, 'step_size': 1, 'gamma': 0.985932646378447}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 15:57:39,500][0m Trial 13 finished with value: 0.04620050109814906 and parameters: {'observation_period_num': 5, 'train_rates': 0.9770030728181266, 'learning_rate': 0.0003846756916321684, 'batch_size': 68, 'step_size': 3, 'gamma': 0.763010826952077}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 15:58:53,457][0m Trial 14 finished with value: 0.04001073479893301 and parameters: {'observation_period_num': 8, 'train_rates': 0.8548956688205773, 'learning_rate': 0.00028742640536531935, 'batch_size': 77, 'step_size': 4, 'gamma': 0.7788915736391719}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 15:59:59,746][0m Trial 15 finished with value: 0.08711777993698339 and parameters: {'observation_period_num': 74, 'train_rates': 0.8580758322762686, 'learning_rate': 0.0002543720264417517, 'batch_size': 85, 'step_size': 5, 'gamma': 0.8339432808927483}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:04:56,534][0m Trial 16 finished with value: 0.06086083617709909 and parameters: {'observation_period_num': 19, 'train_rates': 0.834085224585287, 'learning_rate': 1.6416160276158126e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.7653982797836962}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:06:10,005][0m Trial 17 finished with value: 0.16757370333625932 and parameters: {'observation_period_num': 51, 'train_rates': 0.7068684382212572, 'learning_rate': 0.00024446481915254236, 'batch_size': 66, 'step_size': 3, 'gamma': 0.8553646223298437}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:07:00,781][0m Trial 18 finished with value: 0.09574718182575118 and parameters: {'observation_period_num': 106, 'train_rates': 0.8870965303528072, 'learning_rate': 0.00014981270001051183, 'batch_size': 113, 'step_size': 8, 'gamma': 0.7878626349764908}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:07:31,502][0m Trial 19 finished with value: 0.2164529192827723 and parameters: {'observation_period_num': 42, 'train_rates': 0.7820385505958045, 'learning_rate': 0.0004414772791143344, 'batch_size': 191, 'step_size': 2, 'gamma': 0.8646896787174532}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:09:03,410][0m Trial 20 finished with value: 0.15614738694991276 and parameters: {'observation_period_num': 80, 'train_rates': 0.8922071486415332, 'learning_rate': 1.6277443731503307e-05, 'batch_size': 61, 'step_size': 5, 'gamma': 0.8885929218622961}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:10:39,047][0m Trial 21 finished with value: 0.0497884526848793 and parameters: {'observation_period_num': 11, 'train_rates': 0.985275827358924, 'learning_rate': 0.0003616488214282153, 'batch_size': 64, 'step_size': 3, 'gamma': 0.7551400360605168}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:11:40,472][0m Trial 22 finished with value: 0.03984847163770631 and parameters: {'observation_period_num': 10, 'train_rates': 0.9427283211675478, 'learning_rate': 0.0004804939329760993, 'batch_size': 97, 'step_size': 4, 'gamma': 0.781357178299901}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:12:36,684][0m Trial 23 finished with value: 0.045888281877288654 and parameters: {'observation_period_num': 32, 'train_rates': 0.940029138299908, 'learning_rate': 0.0009908004945639824, 'batch_size': 111, 'step_size': 6, 'gamma': 0.7835552206836046}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:13:10,700][0m Trial 24 finished with value: 0.07547570346395578 and parameters: {'observation_period_num': 56, 'train_rates': 0.8323473658742762, 'learning_rate': 0.00023978064367509888, 'batch_size': 169, 'step_size': 4, 'gamma': 0.8317249613364956}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:14:15,834][0m Trial 25 finished with value: 0.06127425905888098 and parameters: {'observation_period_num': 28, 'train_rates': 0.9077193454704813, 'learning_rate': 0.0005546521788214081, 'batch_size': 91, 'step_size': 2, 'gamma': 0.7894581225760409}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:15:09,568][0m Trial 26 finished with value: 0.06271891944931351 and parameters: {'observation_period_num': 9, 'train_rates': 0.8390181979997002, 'learning_rate': 8.481061926136611e-05, 'batch_size': 108, 'step_size': 5, 'gamma': 0.8079393089786521}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:15:36,637][0m Trial 27 finished with value: 0.18036602437496185 and parameters: {'observation_period_num': 91, 'train_rates': 0.9532875168365026, 'learning_rate': 0.00016837560599304384, 'batch_size': 251, 'step_size': 7, 'gamma': 0.8484476363909889}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:20:39,210][0m Trial 28 finished with value: 0.07044099462310924 and parameters: {'observation_period_num': 63, 'train_rates': 0.8797179273457818, 'learning_rate': 0.0006129860393830779, 'batch_size': 18, 'step_size': 2, 'gamma': 0.7784002682253375}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:22:29,749][0m Trial 29 finished with value: 0.18732570985267902 and parameters: {'observation_period_num': 36, 'train_rates': 0.774402452974562, 'learning_rate': 2.8475281403918882e-05, 'batch_size': 46, 'step_size': 11, 'gamma': 0.9569065796790447}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:23:37,209][0m Trial 30 finished with value: 0.1158572793006897 and parameters: {'observation_period_num': 135, 'train_rates': 0.858291935729292, 'learning_rate': 7.529550035554303e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.8268787442614199}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:24:31,269][0m Trial 31 finished with value: 0.17966401912796665 and parameters: {'observation_period_num': 247, 'train_rates': 0.9483591305035175, 'learning_rate': 0.0009856052446292193, 'batch_size': 108, 'step_size': 6, 'gamma': 0.7919414581897906}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:25:23,379][0m Trial 32 finished with value: 0.0671409434080124 and parameters: {'observation_period_num': 29, 'train_rates': 0.9409948169987686, 'learning_rate': 0.00033035772659690665, 'batch_size': 119, 'step_size': 4, 'gamma': 0.7534824398899633}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:26:00,941][0m Trial 33 finished with value: 0.03940219586365856 and parameters: {'observation_period_num': 7, 'train_rates': 0.9144488276783705, 'learning_rate': 0.0007168165876988191, 'batch_size': 161, 'step_size': 15, 'gamma': 0.7761987926547665}. Best is trial 12 with value: 0.03858797401189804.[0m
[32m[I 2025-02-06 16:26:36,041][0m Trial 34 finished with value: 0.03595570388894815 and parameters: {'observation_period_num': 5, 'train_rates': 0.9042877900792238, 'learning_rate': 0.0006040982338919941, 'batch_size': 176, 'step_size': 14, 'gamma': 0.7724361827900819}. Best is trial 34 with value: 0.03595570388894815.[0m
[32m[I 2025-02-06 16:27:13,896][0m Trial 35 finished with value: 0.040540911934592506 and parameters: {'observation_period_num': 19, 'train_rates': 0.9113372859765552, 'learning_rate': 0.0005690425400273456, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8011117195409536}. Best is trial 34 with value: 0.03595570388894815.[0m
[32m[I 2025-02-06 16:27:54,097][0m Trial 36 finished with value: 0.05682758241891861 and parameters: {'observation_period_num': 39, 'train_rates': 0.9644064892844993, 'learning_rate': 0.0006054941554278292, 'batch_size': 162, 'step_size': 14, 'gamma': 0.8797891023413932}. Best is trial 34 with value: 0.03595570388894815.[0m
[32m[I 2025-02-06 16:28:27,339][0m Trial 37 finished with value: 0.35771683922835756 and parameters: {'observation_period_num': 61, 'train_rates': 0.904695522745081, 'learning_rate': 8.567441950559937e-06, 'batch_size': 185, 'step_size': 15, 'gamma': 0.819429518420915}. Best is trial 34 with value: 0.03595570388894815.[0m
[32m[I 2025-02-06 16:28:59,133][0m Trial 38 finished with value: 1.9333650645563158 and parameters: {'observation_period_num': 5, 'train_rates': 0.8027435758286761, 'learning_rate': 1.1340869690244415e-06, 'batch_size': 193, 'step_size': 13, 'gamma': 0.7697054088124171}. Best is trial 34 with value: 0.03595570388894815.[0m
[32m[I 2025-02-06 16:29:24,220][0m Trial 39 finished with value: 0.31538030207157136 and parameters: {'observation_period_num': 227, 'train_rates': 0.7433999158776288, 'learning_rate': 0.00016046543311537296, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8995648560998759}. Best is trial 34 with value: 0.03595570388894815.[0m
[32m[I 2025-02-06 16:30:05,111][0m Trial 40 finished with value: 0.0349704883992672 and parameters: {'observation_period_num': 19, 'train_rates': 0.9605297615133249, 'learning_rate': 0.0004491782157003435, 'batch_size': 151, 'step_size': 11, 'gamma': 0.8004698376268211}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:30:48,167][0m Trial 41 finished with value: 0.03872954577929648 and parameters: {'observation_period_num': 20, 'train_rates': 0.923765030437877, 'learning_rate': 0.00044443066374618854, 'batch_size': 150, 'step_size': 11, 'gamma': 0.7993798901301596}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:31:31,526][0m Trial 42 finished with value: 0.041474971339687125 and parameters: {'observation_period_num': 22, 'train_rates': 0.922258542074521, 'learning_rate': 0.0007384144181505658, 'batch_size': 144, 'step_size': 11, 'gamma': 0.8096872437202134}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:32:08,979][0m Trial 43 finished with value: 0.06681346893310547 and parameters: {'observation_period_num': 38, 'train_rates': 0.9553512524715225, 'learning_rate': 0.00040722309387280803, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8035113590280916}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:32:50,189][0m Trial 44 finished with value: 0.04664450138807297 and parameters: {'observation_period_num': 21, 'train_rates': 0.9668837034030277, 'learning_rate': 0.0002056241616444205, 'batch_size': 155, 'step_size': 14, 'gamma': 0.796634276813525}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:33:37,736][0m Trial 45 finished with value: 0.1429629348359018 and parameters: {'observation_period_num': 47, 'train_rates': 0.9282561181150133, 'learning_rate': 0.0007163785406954954, 'batch_size': 128, 'step_size': 12, 'gamma': 0.9601227704651578}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:34:09,634][0m Trial 46 finished with value: 0.3609821066275405 and parameters: {'observation_period_num': 165, 'train_rates': 0.650267318956502, 'learning_rate': 0.00010133323008508449, 'batch_size': 152, 'step_size': 12, 'gamma': 0.7711467639704722}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:34:45,091][0m Trial 47 finished with value: 0.10323437912713575 and parameters: {'observation_period_num': 114, 'train_rates': 0.8724084808667685, 'learning_rate': 0.00032890723522294087, 'batch_size': 179, 'step_size': 10, 'gamma': 0.842628436298809}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:35:19,120][0m Trial 48 finished with value: 0.04446585476398468 and parameters: {'observation_period_num': 16, 'train_rates': 0.9774738319329938, 'learning_rate': 0.0007687954575580666, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9309357145979371}. Best is trial 40 with value: 0.0349704883992672.[0m
[32m[I 2025-02-06 16:35:52,791][0m Trial 49 finished with value: 0.7397986901650568 and parameters: {'observation_period_num': 80, 'train_rates': 0.6024309635904859, 'learning_rate': 3.68874440705555e-06, 'batch_size': 136, 'step_size': 11, 'gamma': 0.819669264260424}. Best is trial 40 with value: 0.0349704883992672.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3735 | 0.2947
Epoch 2/300, Loss: 0.2131 | 0.2247
Epoch 3/300, Loss: 0.2715 | 0.3244
Epoch 4/300, Loss: 0.3600 | 1.0756
Epoch 5/300, Loss: 0.2210 | 0.2998
Epoch 6/300, Loss: 0.1785 | 0.4116
Epoch 7/300, Loss: 0.1530 | 0.2212
Epoch 8/300, Loss: 0.1293 | 0.2052
Epoch 9/300, Loss: 0.1252 | 0.1434
Epoch 10/300, Loss: 0.1467 | 0.1325
Epoch 11/300, Loss: 0.1443 | 0.1344
Epoch 12/300, Loss: 0.1810 | 0.1418
Epoch 13/300, Loss: 0.1256 | 0.1167
Epoch 14/300, Loss: 0.1246 | 0.1176
Epoch 15/300, Loss: 0.1309 | 0.1064
Epoch 16/300, Loss: 0.1167 | 0.1737
Epoch 17/300, Loss: 0.1109 | 0.1587
Epoch 18/300, Loss: 0.1032 | 0.0920
Epoch 19/300, Loss: 0.1014 | 0.0869
Epoch 20/300, Loss: 0.0979 | 0.0814
Epoch 21/300, Loss: 0.0971 | 0.0786
Epoch 22/300, Loss: 0.0949 | 0.0783
Epoch 23/300, Loss: 0.0951 | 0.0758
Epoch 24/300, Loss: 0.0908 | 0.0723
Epoch 25/300, Loss: 0.0900 | 0.0713
Epoch 26/300, Loss: 0.0897 | 0.0729
Epoch 27/300, Loss: 0.0893 | 0.0732
Epoch 28/300, Loss: 0.0873 | 0.0695
Epoch 29/300, Loss: 0.0868 | 0.0676
Epoch 30/300, Loss: 0.0870 | 0.0697
Epoch 31/300, Loss: 0.0874 | 0.0729
Epoch 32/300, Loss: 0.0862 | 0.0706
Epoch 33/300, Loss: 0.0849 | 0.0654
Epoch 34/300, Loss: 0.0866 | 0.0760
Epoch 35/300, Loss: 0.0857 | 0.0659
Epoch 36/300, Loss: 0.0840 | 0.0636
Epoch 37/300, Loss: 0.0829 | 0.0622
Epoch 38/300, Loss: 0.0825 | 0.0620
Epoch 39/300, Loss: 0.0818 | 0.0632
Epoch 40/300, Loss: 0.0816 | 0.0627
Epoch 41/300, Loss: 0.0811 | 0.0600
Epoch 42/300, Loss: 0.0807 | 0.0602
Epoch 43/300, Loss: 0.0803 | 0.0597
Epoch 44/300, Loss: 0.0798 | 0.0596
Epoch 45/300, Loss: 0.0795 | 0.0600
Epoch 46/300, Loss: 0.0791 | 0.0586
Epoch 47/300, Loss: 0.0788 | 0.0581
Epoch 48/300, Loss: 0.0784 | 0.0578
Epoch 49/300, Loss: 0.0781 | 0.0577
Epoch 50/300, Loss: 0.0777 | 0.0572
Epoch 51/300, Loss: 0.0774 | 0.0567
Epoch 52/300, Loss: 0.0771 | 0.0564
Epoch 53/300, Loss: 0.0767 | 0.0562
Epoch 54/300, Loss: 0.0764 | 0.0558
Epoch 55/300, Loss: 0.0761 | 0.0555
Epoch 56/300, Loss: 0.0758 | 0.0551
Epoch 57/300, Loss: 0.0756 | 0.0549
Epoch 58/300, Loss: 0.0753 | 0.0545
Epoch 59/300, Loss: 0.0750 | 0.0541
Epoch 60/300, Loss: 0.0747 | 0.0537
Epoch 61/300, Loss: 0.0744 | 0.0533
Epoch 62/300, Loss: 0.0742 | 0.0528
Epoch 63/300, Loss: 0.0740 | 0.0525
Epoch 64/300, Loss: 0.0738 | 0.0522
Epoch 65/300, Loss: 0.0736 | 0.0518
Epoch 66/300, Loss: 0.0734 | 0.0515
Epoch 67/300, Loss: 0.0733 | 0.0512
Epoch 68/300, Loss: 0.0732 | 0.0510
Epoch 69/300, Loss: 0.0729 | 0.0508
Epoch 70/300, Loss: 0.0727 | 0.0507
Epoch 71/300, Loss: 0.0724 | 0.0505
Epoch 72/300, Loss: 0.0722 | 0.0504
Epoch 73/300, Loss: 0.0721 | 0.0503
Epoch 74/300, Loss: 0.0720 | 0.0501
Epoch 75/300, Loss: 0.0718 | 0.0499
Epoch 76/300, Loss: 0.0717 | 0.0497
Epoch 77/300, Loss: 0.0716 | 0.0495
Epoch 78/300, Loss: 0.0714 | 0.0493
Epoch 79/300, Loss: 0.0713 | 0.0492
Epoch 80/300, Loss: 0.0712 | 0.0490
Epoch 81/300, Loss: 0.0711 | 0.0489
Epoch 82/300, Loss: 0.0710 | 0.0488
Epoch 83/300, Loss: 0.0709 | 0.0486
Epoch 84/300, Loss: 0.0708 | 0.0485
Epoch 85/300, Loss: 0.0707 | 0.0484
Epoch 86/300, Loss: 0.0706 | 0.0483
Epoch 87/300, Loss: 0.0706 | 0.0482
Epoch 88/300, Loss: 0.0705 | 0.0481
Epoch 89/300, Loss: 0.0704 | 0.0480
Epoch 90/300, Loss: 0.0703 | 0.0479
Epoch 91/300, Loss: 0.0703 | 0.0478
Epoch 92/300, Loss: 0.0702 | 0.0478
Epoch 93/300, Loss: 0.0701 | 0.0477
Epoch 94/300, Loss: 0.0701 | 0.0476
Epoch 95/300, Loss: 0.0700 | 0.0475
Epoch 96/300, Loss: 0.0699 | 0.0474
Epoch 97/300, Loss: 0.0699 | 0.0474
Epoch 98/300, Loss: 0.0698 | 0.0473
Epoch 99/300, Loss: 0.0698 | 0.0473
Epoch 100/300, Loss: 0.0697 | 0.0472
Epoch 101/300, Loss: 0.0697 | 0.0472
Epoch 102/300, Loss: 0.0697 | 0.0471
Epoch 103/300, Loss: 0.0696 | 0.0471
Epoch 104/300, Loss: 0.0696 | 0.0470
Epoch 105/300, Loss: 0.0695 | 0.0469
Epoch 106/300, Loss: 0.0695 | 0.0469
Epoch 107/300, Loss: 0.0695 | 0.0469
Epoch 108/300, Loss: 0.0694 | 0.0468
Epoch 109/300, Loss: 0.0694 | 0.0468
Epoch 110/300, Loss: 0.0694 | 0.0467
Epoch 111/300, Loss: 0.0693 | 0.0467
Epoch 112/300, Loss: 0.0693 | 0.0467
Epoch 113/300, Loss: 0.0693 | 0.0466
Epoch 114/300, Loss: 0.0692 | 0.0466
Epoch 115/300, Loss: 0.0692 | 0.0466
Epoch 116/300, Loss: 0.0692 | 0.0465
Epoch 117/300, Loss: 0.0691 | 0.0465
Epoch 118/300, Loss: 0.0691 | 0.0465
Epoch 119/300, Loss: 0.0691 | 0.0464
Epoch 120/300, Loss: 0.0691 | 0.0464
Epoch 121/300, Loss: 0.0691 | 0.0464
Epoch 122/300, Loss: 0.0690 | 0.0464
Epoch 123/300, Loss: 0.0690 | 0.0463
Epoch 124/300, Loss: 0.0690 | 0.0463
Epoch 125/300, Loss: 0.0690 | 0.0463
Epoch 126/300, Loss: 0.0690 | 0.0463
Epoch 127/300, Loss: 0.0689 | 0.0463
Epoch 128/300, Loss: 0.0689 | 0.0462
Epoch 129/300, Loss: 0.0689 | 0.0462
Epoch 130/300, Loss: 0.0689 | 0.0462
Epoch 131/300, Loss: 0.0689 | 0.0462
Epoch 132/300, Loss: 0.0689 | 0.0462
Epoch 133/300, Loss: 0.0688 | 0.0462
Epoch 134/300, Loss: 0.0688 | 0.0461
Epoch 135/300, Loss: 0.0688 | 0.0461
Epoch 136/300, Loss: 0.0688 | 0.0461
Epoch 137/300, Loss: 0.0688 | 0.0461
Epoch 138/300, Loss: 0.0688 | 0.0461
Epoch 139/300, Loss: 0.0688 | 0.0461
Epoch 140/300, Loss: 0.0688 | 0.0461
Epoch 141/300, Loss: 0.0688 | 0.0460
Epoch 142/300, Loss: 0.0687 | 0.0460
Epoch 143/300, Loss: 0.0687 | 0.0460
Epoch 144/300, Loss: 0.0687 | 0.0460
Epoch 145/300, Loss: 0.0687 | 0.0460
Epoch 146/300, Loss: 0.0687 | 0.0460
Epoch 147/300, Loss: 0.0687 | 0.0460
Epoch 148/300, Loss: 0.0687 | 0.0460
Epoch 149/300, Loss: 0.0687 | 0.0460
Epoch 150/300, Loss: 0.0687 | 0.0460
Epoch 151/300, Loss: 0.0687 | 0.0459
Epoch 152/300, Loss: 0.0687 | 0.0459
Epoch 153/300, Loss: 0.0687 | 0.0459
Epoch 154/300, Loss: 0.0686 | 0.0459
Epoch 155/300, Loss: 0.0686 | 0.0459
Epoch 156/300, Loss: 0.0686 | 0.0459
Epoch 157/300, Loss: 0.0686 | 0.0459
Epoch 158/300, Loss: 0.0686 | 0.0459
Epoch 159/300, Loss: 0.0686 | 0.0459
Epoch 160/300, Loss: 0.0686 | 0.0459
Epoch 161/300, Loss: 0.0686 | 0.0459
Epoch 162/300, Loss: 0.0686 | 0.0459
Epoch 163/300, Loss: 0.0686 | 0.0459
Epoch 164/300, Loss: 0.0686 | 0.0459
Epoch 165/300, Loss: 0.0686 | 0.0459
Epoch 166/300, Loss: 0.0686 | 0.0458
Epoch 167/300, Loss: 0.0686 | 0.0458
Epoch 168/300, Loss: 0.0686 | 0.0458
Epoch 169/300, Loss: 0.0686 | 0.0458
Epoch 170/300, Loss: 0.0686 | 0.0458
Epoch 171/300, Loss: 0.0686 | 0.0458
Epoch 172/300, Loss: 0.0686 | 0.0458
Epoch 173/300, Loss: 0.0686 | 0.0458
Epoch 174/300, Loss: 0.0686 | 0.0458
Epoch 175/300, Loss: 0.0686 | 0.0458
Epoch 176/300, Loss: 0.0686 | 0.0458
Epoch 177/300, Loss: 0.0685 | 0.0458
Epoch 178/300, Loss: 0.0685 | 0.0458
Epoch 179/300, Loss: 0.0685 | 0.0458
Epoch 180/300, Loss: 0.0685 | 0.0458
Epoch 181/300, Loss: 0.0685 | 0.0458
Epoch 182/300, Loss: 0.0685 | 0.0458
Epoch 183/300, Loss: 0.0685 | 0.0458
Epoch 184/300, Loss: 0.0685 | 0.0458
Epoch 185/300, Loss: 0.0685 | 0.0458
Epoch 186/300, Loss: 0.0685 | 0.0458
Epoch 187/300, Loss: 0.0685 | 0.0458
Epoch 188/300, Loss: 0.0685 | 0.0458
Epoch 189/300, Loss: 0.0685 | 0.0458
Epoch 190/300, Loss: 0.0685 | 0.0458
Epoch 191/300, Loss: 0.0685 | 0.0458
Epoch 192/300, Loss: 0.0685 | 0.0458
Epoch 193/300, Loss: 0.0685 | 0.0458
Epoch 194/300, Loss: 0.0685 | 0.0458
Epoch 195/300, Loss: 0.0685 | 0.0458
Epoch 196/300, Loss: 0.0685 | 0.0458
Epoch 197/300, Loss: 0.0685 | 0.0458
Epoch 198/300, Loss: 0.0685 | 0.0458
Epoch 199/300, Loss: 0.0685 | 0.0458
Epoch 200/300, Loss: 0.0685 | 0.0458
Epoch 201/300, Loss: 0.0685 | 0.0458
Epoch 202/300, Loss: 0.0685 | 0.0458
Epoch 203/300, Loss: 0.0685 | 0.0458
Epoch 204/300, Loss: 0.0685 | 0.0458
Epoch 205/300, Loss: 0.0685 | 0.0458
Epoch 206/300, Loss: 0.0685 | 0.0458
Epoch 207/300, Loss: 0.0685 | 0.0457
Epoch 208/300, Loss: 0.0685 | 0.0457
Epoch 209/300, Loss: 0.0685 | 0.0457
Epoch 210/300, Loss: 0.0685 | 0.0457
Epoch 211/300, Loss: 0.0685 | 0.0457
Epoch 212/300, Loss: 0.0685 | 0.0457
Epoch 213/300, Loss: 0.0685 | 0.0457
Epoch 214/300, Loss: 0.0685 | 0.0457
Epoch 215/300, Loss: 0.0685 | 0.0457
Epoch 216/300, Loss: 0.0685 | 0.0457
Epoch 217/300, Loss: 0.0685 | 0.0457
Epoch 218/300, Loss: 0.0685 | 0.0457
Epoch 219/300, Loss: 0.0685 | 0.0457
Epoch 220/300, Loss: 0.0685 | 0.0457
Epoch 221/300, Loss: 0.0685 | 0.0457
Epoch 222/300, Loss: 0.0685 | 0.0457
Epoch 223/300, Loss: 0.0685 | 0.0457
Epoch 224/300, Loss: 0.0685 | 0.0457
Epoch 225/300, Loss: 0.0685 | 0.0457
Epoch 226/300, Loss: 0.0685 | 0.0457
Epoch 227/300, Loss: 0.0685 | 0.0457
Epoch 228/300, Loss: 0.0685 | 0.0457
Epoch 229/300, Loss: 0.0685 | 0.0457
Epoch 230/300, Loss: 0.0685 | 0.0457
Epoch 231/300, Loss: 0.0685 | 0.0457
Epoch 232/300, Loss: 0.0685 | 0.0457
Epoch 233/300, Loss: 0.0685 | 0.0457
Epoch 234/300, Loss: 0.0685 | 0.0457
Epoch 235/300, Loss: 0.0685 | 0.0457
Epoch 236/300, Loss: 0.0685 | 0.0457
Epoch 237/300, Loss: 0.0685 | 0.0457
Epoch 238/300, Loss: 0.0685 | 0.0457
Epoch 239/300, Loss: 0.0685 | 0.0457
Epoch 240/300, Loss: 0.0685 | 0.0457
Epoch 241/300, Loss: 0.0685 | 0.0457
Epoch 242/300, Loss: 0.0685 | 0.0457
Epoch 243/300, Loss: 0.0685 | 0.0457
Epoch 244/300, Loss: 0.0685 | 0.0457
Epoch 245/300, Loss: 0.0685 | 0.0457
Epoch 246/300, Loss: 0.0685 | 0.0457
Epoch 247/300, Loss: 0.0685 | 0.0457
Epoch 248/300, Loss: 0.0685 | 0.0457
Epoch 249/300, Loss: 0.0685 | 0.0457
Epoch 250/300, Loss: 0.0685 | 0.0457
Epoch 251/300, Loss: 0.0685 | 0.0457
Epoch 252/300, Loss: 0.0685 | 0.0457
Epoch 253/300, Loss: 0.0685 | 0.0457
Epoch 254/300, Loss: 0.0685 | 0.0457
Epoch 255/300, Loss: 0.0685 | 0.0457
Epoch 256/300, Loss: 0.0685 | 0.0457
Epoch 257/300, Loss: 0.0685 | 0.0457
Epoch 258/300, Loss: 0.0685 | 0.0457
Epoch 259/300, Loss: 0.0685 | 0.0457
Epoch 260/300, Loss: 0.0685 | 0.0457
Epoch 261/300, Loss: 0.0685 | 0.0457
Epoch 262/300, Loss: 0.0685 | 0.0457
Epoch 263/300, Loss: 0.0685 | 0.0457
Epoch 264/300, Loss: 0.0685 | 0.0457
Epoch 265/300, Loss: 0.0685 | 0.0457
Epoch 266/300, Loss: 0.0685 | 0.0457
Epoch 267/300, Loss: 0.0685 | 0.0457
Epoch 268/300, Loss: 0.0685 | 0.0457
Epoch 269/300, Loss: 0.0685 | 0.0457
Epoch 270/300, Loss: 0.0685 | 0.0457
Epoch 271/300, Loss: 0.0685 | 0.0457
Epoch 272/300, Loss: 0.0685 | 0.0457
Epoch 273/300, Loss: 0.0685 | 0.0457
Epoch 274/300, Loss: 0.0685 | 0.0457
Epoch 275/300, Loss: 0.0685 | 0.0457
Epoch 276/300, Loss: 0.0685 | 0.0457
Epoch 277/300, Loss: 0.0685 | 0.0457
Epoch 278/300, Loss: 0.0685 | 0.0457
Epoch 279/300, Loss: 0.0685 | 0.0457
Epoch 280/300, Loss: 0.0685 | 0.0457
Epoch 281/300, Loss: 0.0685 | 0.0457
Epoch 282/300, Loss: 0.0685 | 0.0457
Epoch 283/300, Loss: 0.0685 | 0.0457
Epoch 284/300, Loss: 0.0685 | 0.0457
Epoch 285/300, Loss: 0.0685 | 0.0457
Epoch 286/300, Loss: 0.0685 | 0.0457
Epoch 287/300, Loss: 0.0685 | 0.0457
Epoch 288/300, Loss: 0.0685 | 0.0457
Epoch 289/300, Loss: 0.0685 | 0.0457
Epoch 290/300, Loss: 0.0685 | 0.0457
Epoch 291/300, Loss: 0.0685 | 0.0457
Epoch 292/300, Loss: 0.0685 | 0.0457
Epoch 293/300, Loss: 0.0685 | 0.0457
Epoch 294/300, Loss: 0.0685 | 0.0457
Epoch 295/300, Loss: 0.0685 | 0.0457
Epoch 296/300, Loss: 0.0685 | 0.0457
Epoch 297/300, Loss: 0.0685 | 0.0457
Epoch 298/300, Loss: 0.0685 | 0.0457
Epoch 299/300, Loss: 0.0685 | 0.0457
Epoch 300/300, Loss: 0.0685 | 0.0457
Runtime (seconds): 124.82827377319336
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 86.5414346465841
RMSE: 9.302764892578125
MAE: 9.302764892578125
R-squared: nan
[203.07277]
