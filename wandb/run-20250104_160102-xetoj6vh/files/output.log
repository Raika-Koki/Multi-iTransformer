ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 16:01:03,183][0m A new study created in memory with name: no-name-33633517-480f-4fe1-928d-8be206d6d68c[0m
[32m[I 2025-01-04 16:02:50,309][0m Trial 0 finished with value: 0.19736350254844065 and parameters: {'observation_period_num': 26, 'train_rates': 0.6228421215987795, 'learning_rate': 6.586406037718635e-06, 'batch_size': 42, 'step_size': 13, 'gamma': 0.834210362150338}. Best is trial 0 with value: 0.19736350254844065.[0m
[32m[I 2025-01-04 16:03:26,515][0m Trial 1 finished with value: 0.1521730610315711 and parameters: {'observation_period_num': 190, 'train_rates': 0.6497332490588051, 'learning_rate': 0.0002673546209979522, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9401941827223343}. Best is trial 1 with value: 0.1521730610315711.[0m
[32m[I 2025-01-04 16:04:01,443][0m Trial 2 finished with value: 0.8402023315429688 and parameters: {'observation_period_num': 139, 'train_rates': 0.9537190460627434, 'learning_rate': 7.501327833966811e-06, 'batch_size': 176, 'step_size': 1, 'gamma': 0.9329155157865053}. Best is trial 1 with value: 0.1521730610315711.[0m
[32m[I 2025-01-04 16:06:21,557][0m Trial 3 finished with value: 0.2611469481759063 and parameters: {'observation_period_num': 26, 'train_rates': 0.6133152130209388, 'learning_rate': 2.272574631668815e-06, 'batch_size': 30, 'step_size': 12, 'gamma': 0.8779836014374445}. Best is trial 1 with value: 0.1521730610315711.[0m
[32m[I 2025-01-04 16:06:47,274][0m Trial 4 finished with value: 0.6517206778479228 and parameters: {'observation_period_num': 187, 'train_rates': 0.744021314186371, 'learning_rate': 1.2398131745816088e-05, 'batch_size': 205, 'step_size': 4, 'gamma': 0.7527222515074403}. Best is trial 1 with value: 0.1521730610315711.[0m
[32m[I 2025-01-04 16:07:59,824][0m Trial 5 finished with value: 0.10186101074635283 and parameters: {'observation_period_num': 110, 'train_rates': 0.7303592558988291, 'learning_rate': 0.0006737908438906488, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8005289571066272}. Best is trial 5 with value: 0.10186101074635283.[0m
[32m[I 2025-01-04 16:09:24,413][0m Trial 6 finished with value: 0.1119238567827998 and parameters: {'observation_period_num': 100, 'train_rates': 0.8343353541209395, 'learning_rate': 0.00010873604365393452, 'batch_size': 61, 'step_size': 11, 'gamma': 0.9859817206441903}. Best is trial 5 with value: 0.10186101074635283.[0m
[32m[I 2025-01-04 16:10:00,490][0m Trial 7 finished with value: 0.07729034083037217 and parameters: {'observation_period_num': 155, 'train_rates': 0.8820308754119586, 'learning_rate': 0.000885565028684006, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8314994632635375}. Best is trial 7 with value: 0.07729034083037217.[0m
[32m[I 2025-01-04 16:12:35,209][0m Trial 8 finished with value: 0.17624116332327586 and parameters: {'observation_period_num': 230, 'train_rates': 0.922828199423567, 'learning_rate': 2.4188669949307445e-05, 'batch_size': 34, 'step_size': 2, 'gamma': 0.916871844515212}. Best is trial 7 with value: 0.07729034083037217.[0m
[32m[I 2025-01-04 16:15:52,789][0m Trial 9 finished with value: 0.8281281259137944 and parameters: {'observation_period_num': 243, 'train_rates': 0.673310906175846, 'learning_rate': 2.945703524749095e-06, 'batch_size': 21, 'step_size': 1, 'gamma': 0.8814432114435737}. Best is trial 7 with value: 0.07729034083037217.[0m
[32m[I 2025-01-04 16:16:16,052][0m Trial 10 finished with value: 0.11165931904427359 and parameters: {'observation_period_num': 69, 'train_rates': 0.8517416866786752, 'learning_rate': 9.911411061928902e-05, 'batch_size': 248, 'step_size': 6, 'gamma': 0.8094316540985373}. Best is trial 7 with value: 0.07729034083037217.[0m
[32m[I 2025-01-04 16:17:03,709][0m Trial 11 finished with value: 0.0888496647988047 and parameters: {'observation_period_num': 139, 'train_rates': 0.748878375675477, 'learning_rate': 0.0009180009852465251, 'batch_size': 101, 'step_size': 7, 'gamma': 0.7848277415207946}. Best is trial 7 with value: 0.07729034083037217.[0m
[32m[I 2025-01-04 16:17:51,687][0m Trial 12 finished with value: 0.0748496266741019 and parameters: {'observation_period_num': 154, 'train_rates': 0.8840444243648274, 'learning_rate': 0.000933444988115049, 'batch_size': 113, 'step_size': 6, 'gamma': 0.7520043897476376}. Best is trial 12 with value: 0.0748496266741019.[0m
[32m[I 2025-01-04 16:18:26,891][0m Trial 13 finished with value: 0.13858878138390454 and parameters: {'observation_period_num': 181, 'train_rates': 0.8890426222638363, 'learning_rate': 0.0002905465263742698, 'batch_size': 161, 'step_size': 5, 'gamma': 0.7500036748104387}. Best is trial 12 with value: 0.0748496266741019.[0m
[32m[I 2025-01-04 16:19:22,048][0m Trial 14 finished with value: 0.16396798193454742 and parameters: {'observation_period_num': 154, 'train_rates': 0.9845849323349175, 'learning_rate': 7.582483765256379e-05, 'batch_size': 107, 'step_size': 4, 'gamma': 0.8414745840919033}. Best is trial 12 with value: 0.0748496266741019.[0m
[32m[I 2025-01-04 16:19:56,082][0m Trial 15 finished with value: 0.054344629454943866 and parameters: {'observation_period_num': 82, 'train_rates': 0.8058744270524008, 'learning_rate': 0.0003072797652603205, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8425824793993344}. Best is trial 15 with value: 0.054344629454943866.[0m
[32m[I 2025-01-04 16:20:23,388][0m Trial 16 finished with value: 0.058255366510600363 and parameters: {'observation_period_num': 69, 'train_rates': 0.7970241274921281, 'learning_rate': 0.0003153750861787121, 'batch_size': 207, 'step_size': 15, 'gamma': 0.7764515627810288}. Best is trial 15 with value: 0.054344629454943866.[0m
[32m[I 2025-01-04 16:20:49,120][0m Trial 17 finished with value: 0.059380969038712476 and parameters: {'observation_period_num': 56, 'train_rates': 0.7883738732605617, 'learning_rate': 0.00023967049773066686, 'batch_size': 227, 'step_size': 14, 'gamma': 0.7858317114127288}. Best is trial 15 with value: 0.054344629454943866.[0m
[32m[I 2025-01-04 16:21:16,986][0m Trial 18 finished with value: 0.07945192013320337 and parameters: {'observation_period_num': 73, 'train_rates': 0.8049902742806418, 'learning_rate': 5.850941779729974e-05, 'batch_size': 195, 'step_size': 15, 'gamma': 0.8614526414216147}. Best is trial 15 with value: 0.054344629454943866.[0m
[32m[I 2025-01-04 16:21:50,151][0m Trial 19 finished with value: 0.14659898457439424 and parameters: {'observation_period_num': 98, 'train_rates': 0.7004824811746605, 'learning_rate': 3.0219017316804772e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.8937145648949117}. Best is trial 15 with value: 0.054344629454943866.[0m
[32m[I 2025-01-04 16:22:18,924][0m Trial 20 finished with value: 0.035170183771848676 and parameters: {'observation_period_num': 10, 'train_rates': 0.787806510521287, 'learning_rate': 0.000381030821363715, 'batch_size': 204, 'step_size': 15, 'gamma': 0.814622848144625}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:22:47,627][0m Trial 21 finished with value: 0.06745391368961641 and parameters: {'observation_period_num': 45, 'train_rates': 0.7860795121749433, 'learning_rate': 0.00043061175302348354, 'batch_size': 202, 'step_size': 15, 'gamma': 0.8170701075068648}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:23:13,757][0m Trial 22 finished with value: 0.0396184919644957 and parameters: {'observation_period_num': 10, 'train_rates': 0.8205414804129654, 'learning_rate': 0.00015791605576794798, 'batch_size': 230, 'step_size': 13, 'gamma': 0.854291262037063}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:23:39,093][0m Trial 23 finished with value: 0.0446465999842429 and parameters: {'observation_period_num': 11, 'train_rates': 0.8356053595962508, 'learning_rate': 0.0001511346537664213, 'batch_size': 245, 'step_size': 12, 'gamma': 0.8522606384903163}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:24:02,142][0m Trial 24 finished with value: 0.04334346276703575 and parameters: {'observation_period_num': 7, 'train_rates': 0.8556953319862205, 'learning_rate': 0.00015154693111429698, 'batch_size': 254, 'step_size': 13, 'gamma': 0.8598383135864777}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:24:29,321][0m Trial 25 finished with value: 0.05154804937718879 and parameters: {'observation_period_num': 5, 'train_rates': 0.8576310097824122, 'learning_rate': 4.696411903055969e-05, 'batch_size': 229, 'step_size': 13, 'gamma': 0.8635377239855517}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:24:53,556][0m Trial 26 finished with value: 0.04814803068126951 and parameters: {'observation_period_num': 38, 'train_rates': 0.7624711721895182, 'learning_rate': 0.00014428690421492758, 'batch_size': 228, 'step_size': 13, 'gamma': 0.9067765677586532}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:25:15,595][0m Trial 27 finished with value: 0.05546776605088536 and parameters: {'observation_period_num': 23, 'train_rates': 0.7064523197128755, 'learning_rate': 0.0005371564313904421, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8198649198951781}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:25:50,009][0m Trial 28 finished with value: 0.06304973390604883 and parameters: {'observation_period_num': 47, 'train_rates': 0.9233423949496099, 'learning_rate': 0.00017058071207446464, 'batch_size': 186, 'step_size': 11, 'gamma': 0.8856649243019256}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:26:15,635][0m Trial 29 finished with value: 0.06825477853417397 and parameters: {'observation_period_num': 27, 'train_rates': 0.8310610350340685, 'learning_rate': 3.429489984534166e-05, 'batch_size': 229, 'step_size': 14, 'gamma': 0.8372986492651795}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:26:42,361][0m Trial 30 finished with value: 0.13082799396731637 and parameters: {'observation_period_num': 9, 'train_rates': 0.7758530242698813, 'learning_rate': 1.7253129100896466e-05, 'batch_size': 216, 'step_size': 12, 'gamma': 0.8001700472190291}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:27:06,754][0m Trial 31 finished with value: 0.039805225380088974 and parameters: {'observation_period_num': 7, 'train_rates': 0.828329401730558, 'learning_rate': 0.00018229664299290155, 'batch_size': 244, 'step_size': 12, 'gamma': 0.8553435622214993}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:27:30,236][0m Trial 32 finished with value: 0.046260843632009 and parameters: {'observation_period_num': 22, 'train_rates': 0.8199602893599323, 'learning_rate': 0.0001930367071521713, 'batch_size': 254, 'step_size': 13, 'gamma': 0.8613836851607106}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:27:55,368][0m Trial 33 finished with value: 0.040285737972848026 and parameters: {'observation_period_num': 36, 'train_rates': 0.8646950864461189, 'learning_rate': 0.00042191244416945315, 'batch_size': 240, 'step_size': 11, 'gamma': 0.823960600137241}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:28:28,560][0m Trial 34 finished with value: 0.03690278009110498 and parameters: {'observation_period_num': 34, 'train_rates': 0.9040140894085579, 'learning_rate': 0.00047159583417146964, 'batch_size': 178, 'step_size': 9, 'gamma': 0.8281279207283515}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:29:01,709][0m Trial 35 finished with value: 0.0397622637071852 and parameters: {'observation_period_num': 51, 'train_rates': 0.9189437921546464, 'learning_rate': 0.0005185511022119981, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8479197264642278}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:29:36,799][0m Trial 36 finished with value: 0.05435258104250981 and parameters: {'observation_period_num': 54, 'train_rates': 0.9327798763714159, 'learning_rate': 0.0005983663050285704, 'batch_size': 173, 'step_size': 9, 'gamma': 0.7989287391408464}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:30:24,537][0m Trial 37 finished with value: 0.04310843348503113 and parameters: {'observation_period_num': 28, 'train_rates': 0.9641558800426556, 'learning_rate': 0.00042373753849261767, 'batch_size': 131, 'step_size': 8, 'gamma': 0.9486710399063168}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:30:58,202][0m Trial 38 finished with value: 2.5457992553710938 and parameters: {'observation_period_num': 59, 'train_rates': 0.9122303956338456, 'learning_rate': 1.2116630239649763e-06, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8316551430415403}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:31:31,230][0m Trial 39 finished with value: 0.06985199451446533 and parameters: {'observation_period_num': 86, 'train_rates': 0.9475799594104989, 'learning_rate': 0.0006476659668289821, 'batch_size': 189, 'step_size': 8, 'gamma': 0.8743163226237051}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:32:04,357][0m Trial 40 finished with value: 0.10852556384850612 and parameters: {'observation_period_num': 119, 'train_rates': 0.9051029524737007, 'learning_rate': 0.00010464048347111953, 'batch_size': 171, 'step_size': 10, 'gamma': 0.773471982899828}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:32:33,864][0m Trial 41 finished with value: 0.04024461471746045 and parameters: {'observation_period_num': 20, 'train_rates': 0.8945142237424948, 'learning_rate': 0.00023550359041876328, 'batch_size': 214, 'step_size': 12, 'gamma': 0.8445340638748691}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:33:01,254][0m Trial 42 finished with value: 0.039459757531616596 and parameters: {'observation_period_num': 37, 'train_rates': 0.871307109477894, 'learning_rate': 0.0003766431428965488, 'batch_size': 213, 'step_size': 9, 'gamma': 0.8486899524966665}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:33:28,210][0m Trial 43 finished with value: 0.04372885172304354 and parameters: {'observation_period_num': 39, 'train_rates': 0.8696870249426142, 'learning_rate': 0.00040384024245186304, 'batch_size': 216, 'step_size': 7, 'gamma': 0.8190332305152911}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:34:01,284][0m Trial 44 finished with value: 0.049577269703149796 and parameters: {'observation_period_num': 31, 'train_rates': 0.9543515639509992, 'learning_rate': 0.0006160581549794126, 'batch_size': 197, 'step_size': 9, 'gamma': 0.8293010170574564}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:35:10,689][0m Trial 45 finished with value: 0.0359827799160147 and parameters: {'observation_period_num': 18, 'train_rates': 0.8770023117864005, 'learning_rate': 0.0009909092167874442, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8095096669570608}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:36:12,823][0m Trial 46 finished with value: 0.06215108298704404 and parameters: {'observation_period_num': 19, 'train_rates': 0.635017204092873, 'learning_rate': 0.0008108874198525871, 'batch_size': 72, 'step_size': 7, 'gamma': 0.809505184935926}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:38:02,763][0m Trial 47 finished with value: 0.10833544408281644 and parameters: {'observation_period_num': 207, 'train_rates': 0.8493388997873872, 'learning_rate': 0.000308262619673194, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8071770005725165}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:39:10,488][0m Trial 48 finished with value: 0.3422246441497641 and parameters: {'observation_period_num': 61, 'train_rates': 0.8778370285247841, 'learning_rate': 7.732721980269707e-06, 'batch_size': 83, 'step_size': 7, 'gamma': 0.7653619207740852}. Best is trial 20 with value: 0.035170183771848676.[0m
[32m[I 2025-01-04 16:39:55,252][0m Trial 49 finished with value: 0.027156092243531285 and parameters: {'observation_period_num': 15, 'train_rates': 0.8140686602762969, 'learning_rate': 0.0007921628563370255, 'batch_size': 123, 'step_size': 5, 'gamma': 0.7903514646206479}. Best is trial 49 with value: 0.027156092243531285.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 16:39:55,263][0m A new study created in memory with name: no-name-b11b55fc-9898-4666-adb3-38572a4bb331[0m
[32m[I 2025-01-04 16:40:25,011][0m Trial 0 finished with value: 0.09412768265504516 and parameters: {'observation_period_num': 137, 'train_rates': 0.8106104459210345, 'learning_rate': 0.00011617306262919827, 'batch_size': 185, 'step_size': 9, 'gamma': 0.8656212364250458}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:41:22,721][0m Trial 1 finished with value: 0.22710910175234225 and parameters: {'observation_period_num': 210, 'train_rates': 0.9261088165517082, 'learning_rate': 1.8199871163464476e-05, 'batch_size': 95, 'step_size': 9, 'gamma': 0.7968915316565457}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:41:46,717][0m Trial 2 finished with value: 0.5836764284082361 and parameters: {'observation_period_num': 237, 'train_rates': 0.7413350056363173, 'learning_rate': 4.965753105534482e-06, 'batch_size': 209, 'step_size': 12, 'gamma': 0.9346984480158903}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:42:32,373][0m Trial 3 finished with value: 0.21037111541739217 and parameters: {'observation_period_num': 13, 'train_rates': 0.6327854977502455, 'learning_rate': 4.238541587711313e-06, 'batch_size': 100, 'step_size': 8, 'gamma': 0.852753973142375}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:43:04,218][0m Trial 4 finished with value: 0.17952382117835908 and parameters: {'observation_period_num': 152, 'train_rates': 0.673977439501062, 'learning_rate': 5.781878671344516e-05, 'batch_size': 148, 'step_size': 3, 'gamma': 0.9534336682080465}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:43:29,855][0m Trial 5 finished with value: 0.3302956565700728 and parameters: {'observation_period_num': 65, 'train_rates': 0.8996684662708286, 'learning_rate': 1.706066179024793e-05, 'batch_size': 251, 'step_size': 3, 'gamma': 0.8882550824066406}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:43:55,315][0m Trial 6 finished with value: 0.695543325940768 and parameters: {'observation_period_num': 184, 'train_rates': 0.8485725463981011, 'learning_rate': 4.0071357650794646e-06, 'batch_size': 217, 'step_size': 8, 'gamma': 0.8634638304069613}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:44:24,669][0m Trial 7 finished with value: 0.18738380074501038 and parameters: {'observation_period_num': 125, 'train_rates': 0.9508622665490087, 'learning_rate': 4.789591348582854e-05, 'batch_size': 223, 'step_size': 14, 'gamma': 0.8007218277967839}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:45:01,611][0m Trial 8 finished with value: 0.23667482866181266 and parameters: {'observation_period_num': 160, 'train_rates': 0.6136382000138405, 'learning_rate': 0.0007196289058622598, 'batch_size': 120, 'step_size': 12, 'gamma': 0.9001734814455523}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:45:26,269][0m Trial 9 finished with value: 0.7768687605857849 and parameters: {'observation_period_num': 152, 'train_rates': 0.9407384840612812, 'learning_rate': 6.65005773032049e-06, 'batch_size': 242, 'step_size': 8, 'gamma': 0.798804021965803}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:49:43,562][0m Trial 10 finished with value: 0.13427987138282355 and parameters: {'observation_period_num': 91, 'train_rates': 0.777384025345818, 'learning_rate': 0.0003048355696600256, 'batch_size': 19, 'step_size': 1, 'gamma': 0.9855144543213789}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:54:14,724][0m Trial 11 finished with value: 0.10273444345341874 and parameters: {'observation_period_num': 92, 'train_rates': 0.7848736971074337, 'learning_rate': 0.00021780412348022004, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9653369404448858}. Best is trial 0 with value: 0.09412768265504516.[0m
[32m[I 2025-01-04 16:57:55,121][0m Trial 12 finished with value: 0.061727013347812595 and parameters: {'observation_period_num': 91, 'train_rates': 0.8216269920247128, 'learning_rate': 0.00014167877270865255, 'batch_size': 23, 'step_size': 5, 'gamma': 0.8374452700439724}. Best is trial 12 with value: 0.061727013347812595.[0m
[32m[I 2025-01-04 16:58:29,266][0m Trial 13 finished with value: 0.0670583466411505 and parameters: {'observation_period_num': 41, 'train_rates': 0.8586958423498562, 'learning_rate': 0.00012415291496087455, 'batch_size': 170, 'step_size': 6, 'gamma': 0.7522983784165684}. Best is trial 12 with value: 0.061727013347812595.[0m
[32m[I 2025-01-04 17:00:06,026][0m Trial 14 finished with value: 0.031461743432588465 and parameters: {'observation_period_num': 21, 'train_rates': 0.8700568428104042, 'learning_rate': 0.0009959666210000977, 'batch_size': 57, 'step_size': 5, 'gamma': 0.750189710903528}. Best is trial 14 with value: 0.031461743432588465.[0m
[32m[I 2025-01-04 17:01:33,792][0m Trial 15 finished with value: 0.03233620242645898 and parameters: {'observation_period_num': 9, 'train_rates': 0.7325385848596365, 'learning_rate': 0.000987298627940025, 'batch_size': 56, 'step_size': 5, 'gamma': 0.7509159044906658}. Best is trial 14 with value: 0.031461743432588465.[0m
[32m[I 2025-01-04 17:02:43,968][0m Trial 16 finished with value: 0.03067718700251796 and parameters: {'observation_period_num': 6, 'train_rates': 0.7314993471356174, 'learning_rate': 0.0009830195765333388, 'batch_size': 71, 'step_size': 5, 'gamma': 0.7519883050200994}. Best is trial 16 with value: 0.03067718700251796.[0m
[32m[I 2025-01-04 17:03:58,259][0m Trial 17 finished with value: 0.07891768705228279 and parameters: {'observation_period_num': 43, 'train_rates': 0.7011752789046312, 'learning_rate': 0.000455413502208687, 'batch_size': 64, 'step_size': 3, 'gamma': 0.7748709501930187}. Best is trial 16 with value: 0.03067718700251796.[0m
[32m[I 2025-01-04 17:05:43,455][0m Trial 18 finished with value: 0.5698764324188232 and parameters: {'observation_period_num': 35, 'train_rates': 0.9875033903083537, 'learning_rate': 1.0908571644015502e-06, 'batch_size': 57, 'step_size': 6, 'gamma': 0.8212544000312628}. Best is trial 16 with value: 0.03067718700251796.[0m
[32m[I 2025-01-04 17:06:44,416][0m Trial 19 finished with value: 0.04832332303196612 and parameters: {'observation_period_num': 66, 'train_rates': 0.8772712115874448, 'learning_rate': 0.0005770754050075867, 'batch_size': 93, 'step_size': 5, 'gamma': 0.782334806041334}. Best is trial 16 with value: 0.03067718700251796.[0m
[32m[I 2025-01-04 17:08:00,715][0m Trial 20 finished with value: 0.03917768104489349 and parameters: {'observation_period_num': 8, 'train_rates': 0.7536822568778756, 'learning_rate': 0.0003000883913902997, 'batch_size': 67, 'step_size': 3, 'gamma': 0.8250806334278078}. Best is trial 16 with value: 0.03067718700251796.[0m
[32m[I 2025-01-04 17:09:47,634][0m Trial 21 finished with value: 0.02835588823807867 and parameters: {'observation_period_num': 6, 'train_rates': 0.7100998910289729, 'learning_rate': 0.0009858622658821053, 'batch_size': 45, 'step_size': 5, 'gamma': 0.751237745001437}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:11:39,970][0m Trial 22 finished with value: 0.08661727216838183 and parameters: {'observation_period_num': 32, 'train_rates': 0.6826935713448032, 'learning_rate': 0.0009462424670455008, 'batch_size': 41, 'step_size': 6, 'gamma': 0.7714891751349318}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:12:38,413][0m Trial 23 finished with value: 0.0782086824066937 and parameters: {'observation_period_num': 61, 'train_rates': 0.7095526120282407, 'learning_rate': 0.0004153939544441563, 'batch_size': 83, 'step_size': 7, 'gamma': 0.7634481530389692}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:13:17,384][0m Trial 24 finished with value: 0.07037101484439223 and parameters: {'observation_period_num': 8, 'train_rates': 0.6488575676435138, 'learning_rate': 0.00018928249917853044, 'batch_size': 122, 'step_size': 4, 'gamma': 0.7852409162957658}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:14:24,316][0m Trial 25 finished with value: 0.03678677002309074 and parameters: {'observation_period_num': 27, 'train_rates': 0.7573849888471252, 'learning_rate': 0.00048551011974249655, 'batch_size': 76, 'step_size': 10, 'gamma': 0.7505111074388227}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:16:39,407][0m Trial 26 finished with value: 0.05026417401060462 and parameters: {'observation_period_num': 55, 'train_rates': 0.8275085595074673, 'learning_rate': 0.0007463244966845161, 'batch_size': 39, 'step_size': 2, 'gamma': 0.8128916048503997}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:18:56,101][0m Trial 27 finished with value: 0.06073826659996451 and parameters: {'observation_period_num': 24, 'train_rates': 0.7218885382768574, 'learning_rate': 7.030828943726966e-05, 'batch_size': 35, 'step_size': 4, 'gamma': 0.7676538619004813}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:19:41,503][0m Trial 28 finished with value: 0.07466853526567326 and parameters: {'observation_period_num': 106, 'train_rates': 0.7767133564070433, 'learning_rate': 0.00028702023777203917, 'batch_size': 115, 'step_size': 4, 'gamma': 0.7907935165307456}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:20:20,272][0m Trial 29 finished with value: 0.0565199230814522 and parameters: {'observation_period_num': 45, 'train_rates': 0.8110028195370579, 'learning_rate': 0.00010215400404127026, 'batch_size': 139, 'step_size': 7, 'gamma': 0.8364313380650066}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:22:04,763][0m Trial 30 finished with value: 0.07464776726655004 and parameters: {'observation_period_num': 76, 'train_rates': 0.8980996746922183, 'learning_rate': 0.0004998748738914078, 'batch_size': 53, 'step_size': 10, 'gamma': 0.9191073204191444}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:23:45,869][0m Trial 31 finished with value: 0.04026976493038949 and parameters: {'observation_period_num': 17, 'train_rates': 0.7328971397898382, 'learning_rate': 0.0008714445732219023, 'batch_size': 48, 'step_size': 5, 'gamma': 0.7505090803803991}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:24:47,529][0m Trial 32 finished with value: 0.03413289366386784 and parameters: {'observation_period_num': 5, 'train_rates': 0.6738283778531412, 'learning_rate': 0.0009798359644175361, 'batch_size': 77, 'step_size': 5, 'gamma': 0.7630203985877478}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:25:36,654][0m Trial 33 finished with value: 0.05404641422067295 and parameters: {'observation_period_num': 24, 'train_rates': 0.6887906376440374, 'learning_rate': 0.00035542582863523484, 'batch_size': 100, 'step_size': 7, 'gamma': 0.7796561469094803}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:27:45,927][0m Trial 34 finished with value: 0.20674036356138498 and parameters: {'observation_period_num': 227, 'train_rates': 0.6548169001604773, 'learning_rate': 0.000642182074472218, 'batch_size': 32, 'step_size': 6, 'gamma': 0.8057041339475551}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:29:09,555][0m Trial 35 finished with value: 0.10022607442249168 and parameters: {'observation_period_num': 48, 'train_rates': 0.7412410901499359, 'learning_rate': 2.726497671600607e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.7616215041815919}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:30:11,286][0m Trial 36 finished with value: 0.06137904971837997 and parameters: {'observation_period_num': 19, 'train_rates': 0.8009085820450885, 'learning_rate': 0.000202859096955479, 'batch_size': 86, 'step_size': 2, 'gamma': 0.7909647933901217}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:30:58,282][0m Trial 37 finished with value: 0.055726771108569315 and parameters: {'observation_period_num': 30, 'train_rates': 0.7202407291703802, 'learning_rate': 0.0009477641997958157, 'batch_size': 107, 'step_size': 4, 'gamma': 0.774834977530599}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:31:31,766][0m Trial 38 finished with value: 0.12100986383463207 and parameters: {'observation_period_num': 192, 'train_rates': 0.7593842087113676, 'learning_rate': 0.0005318796207772784, 'batch_size': 161, 'step_size': 7, 'gamma': 0.7589805442939067}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:32:32,676][0m Trial 39 finished with value: 0.059773416926361296 and parameters: {'observation_period_num': 5, 'train_rates': 0.6044503222027227, 'learning_rate': 0.0006614131059400062, 'batch_size': 72, 'step_size': 2, 'gamma': 0.8881430079861301}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:34:22,863][0m Trial 40 finished with value: 0.18190308634217803 and parameters: {'observation_period_num': 76, 'train_rates': 0.8509016083818433, 'learning_rate': 9.910905171229557e-06, 'batch_size': 48, 'step_size': 9, 'gamma': 0.8079954742920119}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:35:24,081][0m Trial 41 finished with value: 0.03248402276968466 and parameters: {'observation_period_num': 7, 'train_rates': 0.6616908216968413, 'learning_rate': 0.0008785768364310914, 'batch_size': 75, 'step_size': 5, 'gamma': 0.7655672700229982}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:37:47,181][0m Trial 42 finished with value: 0.04334146628132588 and parameters: {'observation_period_num': 19, 'train_rates': 0.652314554843571, 'learning_rate': 0.0009870294820647338, 'batch_size': 31, 'step_size': 5, 'gamma': 0.769094226560363}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:38:43,271][0m Trial 43 finished with value: 0.04731752425603877 and parameters: {'observation_period_num': 18, 'train_rates': 0.7045695546725104, 'learning_rate': 0.0006440098022019047, 'batch_size': 89, 'step_size': 4, 'gamma': 0.7505066191427813}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:39:48,136][0m Trial 44 finished with value: 0.10568898162291293 and parameters: {'observation_period_num': 130, 'train_rates': 0.6291681624496075, 'learning_rate': 0.0003468954870516642, 'batch_size': 67, 'step_size': 6, 'gamma': 0.7913726246129865}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:40:12,785][0m Trial 45 finished with value: 0.07464251254113664 and parameters: {'observation_period_num': 36, 'train_rates': 0.6700585471654809, 'learning_rate': 0.00023375285085538733, 'batch_size': 202, 'step_size': 14, 'gamma': 0.7615393942266135}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:41:50,018][0m Trial 46 finished with value: 1.0966810773794216 and parameters: {'observation_period_num': 51, 'train_rates': 0.6256477792283007, 'learning_rate': 1.0106393013810092e-06, 'batch_size': 45, 'step_size': 3, 'gamma': 0.7792699637039904}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:44:38,866][0m Trial 47 finished with value: 0.03828421386948845 and parameters: {'observation_period_num': 15, 'train_rates': 0.6979459368382032, 'learning_rate': 0.0006915771332703667, 'batch_size': 28, 'step_size': 5, 'gamma': 0.7977990736020151}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:46:00,108][0m Trial 48 finished with value: 0.029206517498015772 and parameters: {'observation_period_num': 5, 'train_rates': 0.7240541982587547, 'learning_rate': 0.0004757565449156765, 'batch_size': 60, 'step_size': 8, 'gamma': 0.8760625278893539}. Best is trial 21 with value: 0.02835588823807867.[0m
[32m[I 2025-01-04 17:47:28,213][0m Trial 49 finished with value: 0.05043059442791024 and parameters: {'observation_period_num': 40, 'train_rates': 0.7696307239089584, 'learning_rate': 0.00045622384057490787, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8704304182606872}. Best is trial 21 with value: 0.02835588823807867.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 17:47:28,224][0m A new study created in memory with name: no-name-040d8e5b-8f01-4a89-ab28-167f11c5d194[0m
[32m[I 2025-01-04 17:48:01,244][0m Trial 0 finished with value: 0.1334702902483045 and parameters: {'observation_period_num': 219, 'train_rates': 0.893175271880414, 'learning_rate': 0.0007628677374496262, 'batch_size': 172, 'step_size': 14, 'gamma': 0.8218260354359065}. Best is trial 0 with value: 0.1334702902483045.[0m
[32m[I 2025-01-04 17:48:43,327][0m Trial 1 finished with value: 0.10314840278333837 and parameters: {'observation_period_num': 67, 'train_rates': 0.7085773505084618, 'learning_rate': 5.5232981411262596e-05, 'batch_size': 119, 'step_size': 8, 'gamma': 0.8812939350233561}. Best is trial 1 with value: 0.10314840278333837.[0m
[32m[I 2025-01-04 17:49:26,520][0m Trial 2 finished with value: 0.6268986282787945 and parameters: {'observation_period_num': 150, 'train_rates': 0.8143407010262915, 'learning_rate': 3.3124232131785046e-06, 'batch_size': 124, 'step_size': 5, 'gamma': 0.7653731775190014}. Best is trial 1 with value: 0.10314840278333837.[0m
[32m[I 2025-01-04 17:50:10,389][0m Trial 3 finished with value: 0.2514486297297834 and parameters: {'observation_period_num': 194, 'train_rates': 0.9028997250161459, 'learning_rate': 1.004640754129753e-05, 'batch_size': 127, 'step_size': 6, 'gamma': 0.9674183581196591}. Best is trial 1 with value: 0.10314840278333837.[0m
Early stopping at epoch 98
[32m[I 2025-01-04 17:50:50,897][0m Trial 4 finished with value: 0.8650332518014082 and parameters: {'observation_period_num': 102, 'train_rates': 0.6313962159401549, 'learning_rate': 2.209952001498021e-06, 'batch_size': 108, 'step_size': 1, 'gamma': 0.8620922507641324}. Best is trial 1 with value: 0.10314840278333837.[0m
[32m[I 2025-01-04 17:51:09,461][0m Trial 5 finished with value: 0.4953636498405383 and parameters: {'observation_period_num': 211, 'train_rates': 0.6207622259821213, 'learning_rate': 3.592823726586392e-05, 'batch_size': 252, 'step_size': 4, 'gamma': 0.7730064992175987}. Best is trial 1 with value: 0.10314840278333837.[0m
Early stopping at epoch 44
[32m[I 2025-01-04 17:51:22,268][0m Trial 6 finished with value: 0.39238807559013367 and parameters: {'observation_period_num': 173, 'train_rates': 0.9291372639063054, 'learning_rate': 0.00019122045227535728, 'batch_size': 224, 'step_size': 1, 'gamma': 0.7614549773847894}. Best is trial 1 with value: 0.10314840278333837.[0m
[32m[I 2025-01-04 17:51:56,859][0m Trial 7 finished with value: 0.7218003273010254 and parameters: {'observation_period_num': 251, 'train_rates': 0.9899338647688942, 'learning_rate': 3.34871536970417e-06, 'batch_size': 172, 'step_size': 3, 'gamma': 0.8978549601030248}. Best is trial 1 with value: 0.10314840278333837.[0m
[32m[I 2025-01-04 17:52:26,376][0m Trial 8 finished with value: 0.04011307029327374 and parameters: {'observation_period_num': 6, 'train_rates': 0.6722779204109186, 'learning_rate': 0.00021369327641134, 'batch_size': 167, 'step_size': 7, 'gamma': 0.8909880414582925}. Best is trial 8 with value: 0.04011307029327374.[0m
[32m[I 2025-01-04 17:54:17,864][0m Trial 9 finished with value: 0.12839034388336953 and parameters: {'observation_period_num': 193, 'train_rates': 0.9716661084493459, 'learning_rate': 1.972831233342015e-05, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9288967732427293}. Best is trial 8 with value: 0.04011307029327374.[0m
[32m[I 2025-01-04 17:57:48,227][0m Trial 10 finished with value: 0.03573808512490267 and parameters: {'observation_period_num': 11, 'train_rates': 0.7334477981816871, 'learning_rate': 0.00023172693597453426, 'batch_size': 23, 'step_size': 13, 'gamma': 0.9801795626885496}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 17:59:24,414][0m Trial 11 finished with value: 0.043487905566580595 and parameters: {'observation_period_num': 5, 'train_rates': 0.7288956627826515, 'learning_rate': 0.00027502733022444556, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9888904322658872}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:03:23,462][0m Trial 12 finished with value: 0.0376661794125961 and parameters: {'observation_period_num': 12, 'train_rates': 0.7218595724064025, 'learning_rate': 0.0001294022125422101, 'batch_size': 20, 'step_size': 11, 'gamma': 0.936787621566984}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:07:42,067][0m Trial 13 finished with value: 0.06594812240218743 and parameters: {'observation_period_num': 53, 'train_rates': 0.7795842490336168, 'learning_rate': 0.00010501123560593281, 'batch_size': 19, 'step_size': 11, 'gamma': 0.94186254568102}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:12:26,602][0m Trial 14 finished with value: 0.03997248095114218 and parameters: {'observation_period_num': 50, 'train_rates': 0.8136912827451921, 'learning_rate': 0.0009856405995145455, 'batch_size': 18, 'step_size': 11, 'gamma': 0.9427358012475101}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:13:31,151][0m Trial 15 finished with value: 0.1114310705486466 and parameters: {'observation_period_num': 118, 'train_rates': 0.7301688076598668, 'learning_rate': 0.0004220803208362018, 'batch_size': 75, 'step_size': 11, 'gamma': 0.986691044616126}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:14:35,607][0m Trial 16 finished with value: 0.054269472958069845 and parameters: {'observation_period_num': 30, 'train_rates': 0.7834598875397771, 'learning_rate': 8.355010868124934e-05, 'batch_size': 82, 'step_size': 15, 'gamma': 0.9179451629568558}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:16:10,342][0m Trial 17 finished with value: 0.14704863894088516 and parameters: {'observation_period_num': 91, 'train_rates': 0.6759570448052731, 'learning_rate': 1.2661299870081296e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.9611013417125613}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:17:07,563][0m Trial 18 finished with value: 0.048064458895173065 and parameters: {'observation_period_num': 79, 'train_rates': 0.8412003200668616, 'learning_rate': 0.0001231895815209762, 'batch_size': 94, 'step_size': 12, 'gamma': 0.8334214116047035}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:19:16,585][0m Trial 19 finished with value: 0.04565705844243422 and parameters: {'observation_period_num': 27, 'train_rates': 0.7516600487490694, 'learning_rate': 0.00043355236801316077, 'batch_size': 38, 'step_size': 9, 'gamma': 0.9635772012229502}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:20:31,749][0m Trial 20 finished with value: 0.05755293200600822 and parameters: {'observation_period_num': 36, 'train_rates': 0.6732446469624271, 'learning_rate': 2.91604740316177e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.919675919609863}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:25:55,077][0m Trial 21 finished with value: 0.04999019250273704 and parameters: {'observation_period_num': 54, 'train_rates': 0.8363365646530972, 'learning_rate': 0.0009004394728872198, 'batch_size': 16, 'step_size': 11, 'gamma': 0.9389826394428662}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:29:20,702][0m Trial 22 finished with value: 0.03904313789838451 and parameters: {'observation_period_num': 24, 'train_rates': 0.7584854018326519, 'learning_rate': 0.000438043974527336, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9555901008455268}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:31:44,065][0m Trial 23 finished with value: 0.04990653798499044 and parameters: {'observation_period_num': 21, 'train_rates': 0.7582465541443194, 'learning_rate': 0.00040061592205732806, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9667230890363253}. Best is trial 10 with value: 0.03573808512490267.[0m
[32m[I 2025-01-04 18:32:46,772][0m Trial 24 finished with value: 0.02891533955345663 and parameters: {'observation_period_num': 7, 'train_rates': 0.6980957238699996, 'learning_rate': 0.00015362779372765734, 'batch_size': 78, 'step_size': 13, 'gamma': 0.9848488422929899}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:33:51,083][0m Trial 25 finished with value: 0.07323408311130293 and parameters: {'observation_period_num': 65, 'train_rates': 0.7047695249763251, 'learning_rate': 6.218044102452078e-05, 'batch_size': 74, 'step_size': 13, 'gamma': 0.985633396702879}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:34:39,153][0m Trial 26 finished with value: 0.03686177370914121 and parameters: {'observation_period_num': 7, 'train_rates': 0.6467274392248141, 'learning_rate': 0.00014873468764229375, 'batch_size': 96, 'step_size': 12, 'gamma': 0.9073980317627748}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:35:27,988][0m Trial 27 finished with value: 0.5906026545003157 and parameters: {'observation_period_num': 38, 'train_rates': 0.6443736351536897, 'learning_rate': 1.0051172240806862e-06, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8487271105330347}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:36:00,985][0m Trial 28 finished with value: 0.1472458377698042 and parameters: {'observation_period_num': 116, 'train_rates': 0.6137630448010027, 'learning_rate': 0.00017917944016246172, 'batch_size': 136, 'step_size': 9, 'gamma': 0.9071441992580115}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:37:12,608][0m Trial 29 finished with value: 0.09072347236967232 and parameters: {'observation_period_num': 82, 'train_rates': 0.6566439852291287, 'learning_rate': 6.039436476902691e-05, 'batch_size': 63, 'step_size': 14, 'gamma': 0.8027885393954448}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:37:43,881][0m Trial 30 finished with value: 0.11462280628122774 and parameters: {'observation_period_num': 145, 'train_rates': 0.6898903689219787, 'learning_rate': 0.0005736710494369219, 'batch_size': 153, 'step_size': 12, 'gamma': 0.8671295809056018}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:38:10,840][0m Trial 31 finished with value: 0.0670302813874986 and parameters: {'observation_period_num': 18, 'train_rates': 0.7231620716203769, 'learning_rate': 0.0001501756297491276, 'batch_size': 200, 'step_size': 12, 'gamma': 0.9768663669930795}. Best is trial 24 with value: 0.02891533955345663.[0m
[32m[I 2025-01-04 18:39:07,132][0m Trial 32 finished with value: 0.028129781156542077 and parameters: {'observation_period_num': 10, 'train_rates': 0.7000764669856001, 'learning_rate': 0.00026984471152186615, 'batch_size': 88, 'step_size': 10, 'gamma': 0.9517658142664869}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:39:52,106][0m Trial 33 finished with value: 0.047642602214375426 and parameters: {'observation_period_num': 43, 'train_rates': 0.689479158991435, 'learning_rate': 0.00028809152322967836, 'batch_size': 110, 'step_size': 10, 'gamma': 0.9537583323940867}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:40:44,736][0m Trial 34 finished with value: 0.1769772894608797 and parameters: {'observation_period_num': 71, 'train_rates': 0.6035855525060038, 'learning_rate': 4.082608944425137e-05, 'batch_size': 85, 'step_size': 14, 'gamma': 0.9750491137136822}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:41:26,388][0m Trial 35 finished with value: 0.047025047343715216 and parameters: {'observation_period_num': 5, 'train_rates': 0.6500164259085636, 'learning_rate': 8.660927656298606e-05, 'batch_size': 116, 'step_size': 9, 'gamma': 0.9173025280839441}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:42:00,082][0m Trial 36 finished with value: 0.07237467220068255 and parameters: {'observation_period_num': 61, 'train_rates': 0.7032953396497119, 'learning_rate': 0.0002839590818512656, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9483887566842987}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:42:45,974][0m Trial 37 finished with value: 0.07286293416551229 and parameters: {'observation_period_num': 36, 'train_rates': 0.6308785935222874, 'learning_rate': 0.0005972008701460529, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8846204520196308}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:43:25,831][0m Trial 38 finished with value: 0.04148626859269796 and parameters: {'observation_period_num': 21, 'train_rates': 0.746571446995645, 'learning_rate': 0.00024966329466675867, 'batch_size': 129, 'step_size': 12, 'gamma': 0.9759188850596838}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:44:35,837][0m Trial 39 finished with value: 0.06002550098236183 and parameters: {'observation_period_num': 46, 'train_rates': 0.6628197864270287, 'learning_rate': 5.016359140164771e-05, 'batch_size': 66, 'step_size': 7, 'gamma': 0.9303581561029725}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:45:27,207][0m Trial 40 finished with value: 0.19670946952043006 and parameters: {'observation_period_num': 247, 'train_rates': 0.6854124011739922, 'learning_rate': 0.0006652463272052285, 'batch_size': 88, 'step_size': 13, 'gamma': 0.906193429315526}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:47:22,856][0m Trial 41 finished with value: 0.03262543187586851 and parameters: {'observation_period_num': 14, 'train_rates': 0.7153258758808532, 'learning_rate': 0.0001363830327235123, 'batch_size': 42, 'step_size': 10, 'gamma': 0.9318242855313125}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:49:23,465][0m Trial 42 finished with value: 0.048454814428734665 and parameters: {'observation_period_num': 16, 'train_rates': 0.7101067561680348, 'learning_rate': 0.00017956514539710654, 'batch_size': 40, 'step_size': 8, 'gamma': 0.9529716703876621}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:50:45,595][0m Trial 43 finished with value: 0.04215862169770762 and parameters: {'observation_period_num': 7, 'train_rates': 0.6391508078300576, 'learning_rate': 8.508776189264078e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.8763670063129572}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:53:01,975][0m Trial 44 finished with value: 0.054939722505890416 and parameters: {'observation_period_num': 34, 'train_rates': 0.7430375445299766, 'learning_rate': 0.0003189320892700698, 'batch_size': 36, 'step_size': 12, 'gamma': 0.9282804050174431}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:54:09,221][0m Trial 45 finished with value: 0.15662126346495733 and parameters: {'observation_period_num': 173, 'train_rates': 0.774632314727972, 'learning_rate': 2.655652095455177e-05, 'batch_size': 73, 'step_size': 5, 'gamma': 0.897892544112343}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:54:56,123][0m Trial 46 finished with value: 0.03298942643093925 and parameters: {'observation_period_num': 15, 'train_rates': 0.7058565670269183, 'learning_rate': 0.00015013693845153953, 'batch_size': 104, 'step_size': 8, 'gamma': 0.9748828053233379}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:55:41,376][0m Trial 47 finished with value: 0.12200705910037304 and parameters: {'observation_period_num': 18, 'train_rates': 0.7036003672760671, 'learning_rate': 0.00020587551315395252, 'batch_size': 111, 'step_size': 7, 'gamma': 0.973692855584692}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:57:19,757][0m Trial 48 finished with value: 0.040659894169145845 and parameters: {'observation_period_num': 29, 'train_rates': 0.807993984869987, 'learning_rate': 0.00010676451163867168, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9846589242901522}. Best is trial 32 with value: 0.028129781156542077.[0m
[32m[I 2025-01-04 18:57:41,835][0m Trial 49 finished with value: 0.16438692433692012 and parameters: {'observation_period_num': 44, 'train_rates': 0.734766247360193, 'learning_rate': 1.708415643103747e-05, 'batch_size': 253, 'step_size': 6, 'gamma': 0.9685050923602084}. Best is trial 32 with value: 0.028129781156542077.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 18:57:41,845][0m A new study created in memory with name: no-name-55a76e63-52e0-4621-92cb-04a62b79505d[0m
[32m[I 2025-01-04 18:58:05,512][0m Trial 0 finished with value: 0.16132718104808058 and parameters: {'observation_period_num': 171, 'train_rates': 0.802994636845207, 'learning_rate': 5.0796457214270147e-05, 'batch_size': 234, 'step_size': 11, 'gamma': 0.883787338703144}. Best is trial 0 with value: 0.16132718104808058.[0m
[32m[I 2025-01-04 18:58:33,223][0m Trial 1 finished with value: 0.13860332856715565 and parameters: {'observation_period_num': 220, 'train_rates': 0.8703322004677239, 'learning_rate': 0.0008281851580017055, 'batch_size': 216, 'step_size': 7, 'gamma': 0.7611119949446682}. Best is trial 1 with value: 0.13860332856715565.[0m
[32m[I 2025-01-04 19:01:49,080][0m Trial 2 finished with value: 0.3658075030845932 and parameters: {'observation_period_num': 122, 'train_rates': 0.6094895692349213, 'learning_rate': 9.742888040896467e-06, 'batch_size': 21, 'step_size': 13, 'gamma': 0.8398434399061029}. Best is trial 1 with value: 0.13860332856715565.[0m
[32m[I 2025-01-04 19:03:23,863][0m Trial 3 finished with value: 0.12086191024435194 and parameters: {'observation_period_num': 219, 'train_rates': 0.8613387608375991, 'learning_rate': 0.000342703696217898, 'batch_size': 54, 'step_size': 10, 'gamma': 0.8712955431642779}. Best is trial 3 with value: 0.12086191024435194.[0m
[32m[I 2025-01-04 19:03:55,936][0m Trial 4 finished with value: 0.2870776653289795 and parameters: {'observation_period_num': 132, 'train_rates': 0.9824178144642801, 'learning_rate': 2.1328193172579786e-05, 'batch_size': 204, 'step_size': 6, 'gamma': 0.8521790445200675}. Best is trial 3 with value: 0.12086191024435194.[0m
[32m[I 2025-01-04 19:04:16,789][0m Trial 5 finished with value: 0.193121053059089 and parameters: {'observation_period_num': 159, 'train_rates': 0.6475759050298299, 'learning_rate': 0.00010923118130440736, 'batch_size': 238, 'step_size': 12, 'gamma': 0.8016470584061957}. Best is trial 3 with value: 0.12086191024435194.[0m
[32m[I 2025-01-04 19:04:46,072][0m Trial 6 finished with value: 0.4462288409367671 and parameters: {'observation_period_num': 175, 'train_rates': 0.8968704083430792, 'learning_rate': 1.028323228892609e-05, 'batch_size': 199, 'step_size': 10, 'gamma': 0.7811154731217071}. Best is trial 3 with value: 0.12086191024435194.[0m
[32m[I 2025-01-04 19:05:16,997][0m Trial 7 finished with value: 1.39269591805752 and parameters: {'observation_period_num': 103, 'train_rates': 0.7393370081519473, 'learning_rate': 1.9269944006626855e-06, 'batch_size': 175, 'step_size': 4, 'gamma': 0.7722698686455016}. Best is trial 3 with value: 0.12086191024435194.[0m
[32m[I 2025-01-04 19:06:04,087][0m Trial 8 finished with value: 0.568808312518594 and parameters: {'observation_period_num': 58, 'train_rates': 0.6869109048835786, 'learning_rate': 1.754532441557425e-06, 'batch_size': 104, 'step_size': 8, 'gamma': 0.8600495572562347}. Best is trial 3 with value: 0.12086191024435194.[0m
[32m[I 2025-01-04 19:07:10,956][0m Trial 9 finished with value: 0.0497339298017323 and parameters: {'observation_period_num': 32, 'train_rates': 0.786404854384414, 'learning_rate': 2.3700291960998245e-05, 'batch_size': 78, 'step_size': 13, 'gamma': 0.9116017753676809}. Best is trial 9 with value: 0.0497339298017323.[0m
[32m[I 2025-01-04 19:07:55,562][0m Trial 10 finished with value: 0.033467250048286386 and parameters: {'observation_period_num': 14, 'train_rates': 0.7674739721625041, 'learning_rate': 0.00010199264100304597, 'batch_size': 122, 'step_size': 15, 'gamma': 0.9593234805606172}. Best is trial 10 with value: 0.033467250048286386.[0m
[32m[I 2025-01-04 19:08:43,727][0m Trial 11 finished with value: 0.032972475759980434 and parameters: {'observation_period_num': 5, 'train_rates': 0.7692149887253933, 'learning_rate': 0.00014430366979103254, 'batch_size': 110, 'step_size': 15, 'gamma': 0.964922469246081}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:09:21,214][0m Trial 12 finished with value: 0.040232539950114374 and parameters: {'observation_period_num': 5, 'train_rates': 0.7304654506178496, 'learning_rate': 0.0001372259142947063, 'batch_size': 135, 'step_size': 15, 'gamma': 0.9845310033772097}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:10:00,171][0m Trial 13 finished with value: 0.0576091927145758 and parameters: {'observation_period_num': 70, 'train_rates': 0.7851964556797332, 'learning_rate': 0.00019750713459551502, 'batch_size': 136, 'step_size': 15, 'gamma': 0.9775559026179708}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:10:45,488][0m Trial 14 finished with value: 0.04257980276533353 and parameters: {'observation_period_num': 13, 'train_rates': 0.6984737584577521, 'learning_rate': 5.9866247851468576e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.9413458261612325}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:11:17,580][0m Trial 15 finished with value: 0.039200229488717694 and parameters: {'observation_period_num': 54, 'train_rates': 0.8032610187274564, 'learning_rate': 0.0005532515435442434, 'batch_size': 170, 'step_size': 3, 'gamma': 0.9466364915890165}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:12:16,440][0m Trial 16 finished with value: 0.07702403670288827 and parameters: {'observation_period_num': 87, 'train_rates': 0.936249082641142, 'learning_rate': 0.0002386136553046459, 'batch_size': 97, 'step_size': 13, 'gamma': 0.9367957527181465}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:13:46,626][0m Trial 17 finished with value: 0.05585866058018149 and parameters: {'observation_period_num': 34, 'train_rates': 0.8316190133084677, 'learning_rate': 8.968887193190695e-05, 'batch_size': 60, 'step_size': 1, 'gamma': 0.9232467206877499}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:14:21,341][0m Trial 18 finished with value: 0.22716913420243326 and parameters: {'observation_period_num': 41, 'train_rates': 0.7599907401675503, 'learning_rate': 9.361528976409407e-06, 'batch_size': 158, 'step_size': 9, 'gamma': 0.9639681563921542}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:14:59,503][0m Trial 19 finished with value: 0.09208694781296746 and parameters: {'observation_period_num': 81, 'train_rates': 0.6946776596064106, 'learning_rate': 0.000367640799352152, 'batch_size': 127, 'step_size': 14, 'gamma': 0.9025437957576834}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:15:53,135][0m Trial 20 finished with value: 0.23377960247887394 and parameters: {'observation_period_num': 247, 'train_rates': 0.6514712888744983, 'learning_rate': 3.170461879876954e-05, 'batch_size': 79, 'step_size': 5, 'gamma': 0.9625064093740567}. Best is trial 11 with value: 0.032972475759980434.[0m
[32m[I 2025-01-04 19:16:27,149][0m Trial 21 finished with value: 0.03007990906453731 and parameters: {'observation_period_num': 23, 'train_rates': 0.8234112092881752, 'learning_rate': 0.0006341779342396675, 'batch_size': 166, 'step_size': 2, 'gamma': 0.9543082215589881}. Best is trial 21 with value: 0.03007990906453731.[0m
[32m[I 2025-01-04 19:17:03,450][0m Trial 22 finished with value: 0.02931077321178265 and parameters: {'observation_period_num': 20, 'train_rates': 0.8282015664518593, 'learning_rate': 0.000986808650647801, 'batch_size': 157, 'step_size': 1, 'gamma': 0.9615809693700785}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:17:39,494][0m Trial 23 finished with value: 0.03290947913959786 and parameters: {'observation_period_num': 25, 'train_rates': 0.8449085509787988, 'learning_rate': 0.0008689203768284674, 'batch_size': 157, 'step_size': 1, 'gamma': 0.987834344445973}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:18:15,813][0m Trial 24 finished with value: 0.037133850925361984 and parameters: {'observation_period_num': 40, 'train_rates': 0.8337883467463258, 'learning_rate': 0.0009056295629070885, 'batch_size': 160, 'step_size': 1, 'gamma': 0.9880414633113581}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:18:50,650][0m Trial 25 finished with value: 0.05080958970113477 and parameters: {'observation_period_num': 57, 'train_rates': 0.9077107154795402, 'learning_rate': 0.0005387261030328992, 'batch_size': 185, 'step_size': 2, 'gamma': 0.9222930230591131}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:19:28,972][0m Trial 26 finished with value: 0.052775568994602944 and parameters: {'observation_period_num': 103, 'train_rates': 0.8376503507220324, 'learning_rate': 0.0005422332366140442, 'batch_size': 149, 'step_size': 3, 'gamma': 0.897391244268072}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:20:02,662][0m Trial 27 finished with value: 0.036573346704244614 and parameters: {'observation_period_num': 25, 'train_rates': 0.9376588971691932, 'learning_rate': 0.0009197247558972633, 'batch_size': 191, 'step_size': 2, 'gamma': 0.9447830057638402}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:20:43,042][0m Trial 28 finished with value: 0.03823606866192885 and parameters: {'observation_period_num': 50, 'train_rates': 0.8785788681603589, 'learning_rate': 0.0002867268310601109, 'batch_size': 148, 'step_size': 4, 'gamma': 0.9771069137199518}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:21:08,986][0m Trial 29 finished with value: 0.14233364267793133 and parameters: {'observation_period_num': 73, 'train_rates': 0.8193914843870016, 'learning_rate': 0.0004867974298442549, 'batch_size': 227, 'step_size': 1, 'gamma': 0.8857649614587224}. Best is trial 22 with value: 0.02931077321178265.[0m
[32m[I 2025-01-04 19:21:45,573][0m Trial 30 finished with value: 0.029048933500975923 and parameters: {'observation_period_num': 28, 'train_rates': 0.8526368373987463, 'learning_rate': 0.00097788240434487, 'batch_size': 163, 'step_size': 3, 'gamma': 0.9310768326250987}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:22:08,598][0m Trial 31 finished with value: 0.04031474171054391 and parameters: {'observation_period_num': 24, 'train_rates': 0.8535586956111892, 'learning_rate': 0.0007239475987123342, 'batch_size': 254, 'step_size': 2, 'gamma': 0.9269280192471495}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:22:41,279][0m Trial 32 finished with value: 0.02977808569566295 and parameters: {'observation_period_num': 22, 'train_rates': 0.8093235624062399, 'learning_rate': 0.0008767167257760995, 'batch_size': 173, 'step_size': 3, 'gamma': 0.9520806689215121}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:23:13,698][0m Trial 33 finished with value: 0.04917931772304542 and parameters: {'observation_period_num': 44, 'train_rates': 0.8124191468975586, 'learning_rate': 0.00038638281591264095, 'batch_size': 178, 'step_size': 5, 'gamma': 0.95427315218451}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:23:44,699][0m Trial 34 finished with value: 0.04058103644027697 and parameters: {'observation_period_num': 65, 'train_rates': 0.8731370055172479, 'learning_rate': 0.0009650821605846895, 'batch_size': 206, 'step_size': 3, 'gamma': 0.9325301659959608}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:24:14,958][0m Trial 35 finished with value: 0.043130231315785264 and parameters: {'observation_period_num': 23, 'train_rates': 0.9199579665713169, 'learning_rate': 0.00020370554691887335, 'batch_size': 218, 'step_size': 7, 'gamma': 0.911336808119413}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:24:50,486][0m Trial 36 finished with value: 0.05972952534508261 and parameters: {'observation_period_num': 92, 'train_rates': 0.8877470591989887, 'learning_rate': 0.0006102215917173318, 'batch_size': 166, 'step_size': 4, 'gamma': 0.8200319927420151}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:29:16,145][0m Trial 37 finished with value: 0.18556866664735097 and parameters: {'observation_period_num': 172, 'train_rates': 0.7964024800835872, 'learning_rate': 0.00037348790767167636, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8794258602611412}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:29:46,064][0m Trial 38 finished with value: 0.08699351503284433 and parameters: {'observation_period_num': 132, 'train_rates': 0.8591075462859327, 'learning_rate': 0.0002778873753196186, 'batch_size': 195, 'step_size': 2, 'gamma': 0.973073535051576}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:30:21,665][0m Trial 39 finished with value: 0.18930051201376422 and parameters: {'observation_period_num': 154, 'train_rates': 0.7412049140740263, 'learning_rate': 0.0006713229751826944, 'batch_size': 143, 'step_size': 5, 'gamma': 0.9507133590332394}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:30:55,069][0m Trial 40 finished with value: 0.8996357321739197 and parameters: {'observation_period_num': 205, 'train_rates': 0.9571270430026613, 'learning_rate': 2.8480562406758827e-06, 'batch_size': 180, 'step_size': 3, 'gamma': 0.9147372613743517}. Best is trial 30 with value: 0.029048933500975923.[0m
[32m[I 2025-01-04 19:31:31,880][0m Trial 41 finished with value: 0.027187503572251345 and parameters: {'observation_period_num': 18, 'train_rates': 0.8236652673147264, 'learning_rate': 0.0009784092776204502, 'batch_size': 154, 'step_size': 1, 'gamma': 0.9695362749394331}. Best is trial 41 with value: 0.027187503572251345.[0m
[32m[I 2025-01-04 19:32:17,459][0m Trial 42 finished with value: 0.03051010791266115 and parameters: {'observation_period_num': 19, 'train_rates': 0.8113356410512753, 'learning_rate': 0.0009738776578070162, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9677639383395861}. Best is trial 41 with value: 0.027187503572251345.[0m
[32m[I 2025-01-04 19:32:51,166][0m Trial 43 finished with value: 0.033606709653063664 and parameters: {'observation_period_num': 32, 'train_rates': 0.8651086201399073, 'learning_rate': 0.00043396828372652195, 'batch_size': 170, 'step_size': 4, 'gamma': 0.9548892898362702}. Best is trial 41 with value: 0.027187503572251345.[0m
Early stopping at epoch 48
[32m[I 2025-01-04 19:33:05,682][0m Trial 44 finished with value: 0.11318572570029864 and parameters: {'observation_period_num': 14, 'train_rates': 0.8253708641487495, 'learning_rate': 0.0006895090086929956, 'batch_size': 209, 'step_size': 1, 'gamma': 0.7507497916550743}. Best is trial 41 with value: 0.027187503572251345.[0m
[32m[I 2025-01-04 19:33:35,904][0m Trial 45 finished with value: 0.027014113939583487 and parameters: {'observation_period_num': 6, 'train_rates': 0.7809809320208144, 'learning_rate': 0.0006941066410276456, 'batch_size': 188, 'step_size': 3, 'gamma': 0.9758549682493445}. Best is trial 45 with value: 0.027014113939583487.[0m
[32m[I 2025-01-04 19:34:06,073][0m Trial 46 finished with value: 0.1514984451986234 and parameters: {'observation_period_num': 8, 'train_rates': 0.7781802532171143, 'learning_rate': 6.364154340969675e-06, 'batch_size': 189, 'step_size': 3, 'gamma': 0.9759292916458084}. Best is trial 45 with value: 0.027014113939583487.[0m
[32m[I 2025-01-04 19:34:44,123][0m Trial 47 finished with value: 0.049982640757368135 and parameters: {'observation_period_num': 47, 'train_rates': 0.753131014059215, 'learning_rate': 0.00029806978354829415, 'batch_size': 139, 'step_size': 6, 'gamma': 0.8418642695672475}. Best is trial 45 with value: 0.027014113939583487.[0m
[32m[I 2025-01-04 19:35:12,360][0m Trial 48 finished with value: 0.031514496934806804 and parameters: {'observation_period_num': 6, 'train_rates': 0.7892168934766122, 'learning_rate': 0.00045001569485096464, 'batch_size': 198, 'step_size': 4, 'gamma': 0.9714183713991775}. Best is trial 45 with value: 0.027014113939583487.[0m
[32m[I 2025-01-04 19:35:46,503][0m Trial 49 finished with value: 0.11736491119439622 and parameters: {'observation_period_num': 116, 'train_rates': 0.8009323919306404, 'learning_rate': 0.0001863908046910422, 'batch_size': 152, 'step_size': 1, 'gamma': 0.9385286896148316}. Best is trial 45 with value: 0.027014113939583487.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 19:35:46,513][0m A new study created in memory with name: no-name-834841c2-09a8-4853-b3f1-3a4314acf276[0m
[32m[I 2025-01-04 19:38:15,113][0m Trial 0 finished with value: 0.4606684451637743 and parameters: {'observation_period_num': 107, 'train_rates': 0.9013454676467842, 'learning_rate': 1.1485843309374952e-06, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9197877755115147}. Best is trial 0 with value: 0.4606684451637743.[0m
[32m[I 2025-01-04 19:39:50,216][0m Trial 1 finished with value: 0.1640437627701383 and parameters: {'observation_period_num': 76, 'train_rates': 0.9473041624660583, 'learning_rate': 5.0969661546325606e-05, 'batch_size': 61, 'step_size': 3, 'gamma': 0.7620406037987941}. Best is trial 1 with value: 0.1640437627701383.[0m
[32m[I 2025-01-04 19:41:23,934][0m Trial 2 finished with value: 0.12722099253109523 and parameters: {'observation_period_num': 235, 'train_rates': 0.9537408083297119, 'learning_rate': 0.00015024651527746922, 'batch_size': 58, 'step_size': 13, 'gamma': 0.8516891439455315}. Best is trial 2 with value: 0.12722099253109523.[0m
[32m[I 2025-01-04 19:42:20,124][0m Trial 3 finished with value: 0.10242690428577621 and parameters: {'observation_period_num': 208, 'train_rates': 0.7889475556004024, 'learning_rate': 0.00016789703844590932, 'batch_size': 90, 'step_size': 6, 'gamma': 0.9630545899064105}. Best is trial 3 with value: 0.10242690428577621.[0m
[32m[I 2025-01-04 19:42:47,766][0m Trial 4 finished with value: 0.17992306234425673 and parameters: {'observation_period_num': 239, 'train_rates': 0.6218495301484239, 'learning_rate': 0.0004953721987951258, 'batch_size': 157, 'step_size': 12, 'gamma': 0.8936149604910641}. Best is trial 3 with value: 0.10242690428577621.[0m
[32m[I 2025-01-04 19:45:40,760][0m Trial 5 finished with value: 0.4370833965515366 and parameters: {'observation_period_num': 147, 'train_rates': 0.6717053287946503, 'learning_rate': 1.2058762399186801e-06, 'batch_size': 25, 'step_size': 4, 'gamma': 0.957025464909624}. Best is trial 3 with value: 0.10242690428577621.[0m
[32m[I 2025-01-04 19:46:15,140][0m Trial 6 finished with value: 0.06089824662935099 and parameters: {'observation_period_num': 128, 'train_rates': 0.8208060461916207, 'learning_rate': 0.0004452992205105809, 'batch_size': 163, 'step_size': 8, 'gamma': 0.8922948948787848}. Best is trial 6 with value: 0.06089824662935099.[0m
[32m[I 2025-01-04 19:46:34,085][0m Trial 7 finished with value: 0.5787117995713886 and parameters: {'observation_period_num': 165, 'train_rates': 0.6254940301730358, 'learning_rate': 5.806302073597388e-06, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9421978365630295}. Best is trial 6 with value: 0.06089824662935099.[0m
[32m[I 2025-01-04 19:50:31,592][0m Trial 8 finished with value: 0.10080216393612747 and parameters: {'observation_period_num': 97, 'train_rates': 0.7031506816309352, 'learning_rate': 0.0005744272544889932, 'batch_size': 19, 'step_size': 5, 'gamma': 0.7758157090890794}. Best is trial 6 with value: 0.06089824662935099.[0m
[32m[I 2025-01-04 19:52:07,375][0m Trial 9 finished with value: 0.17460227012634277 and parameters: {'observation_period_num': 40, 'train_rates': 0.9853423449639892, 'learning_rate': 9.988075218112854e-06, 'batch_size': 63, 'step_size': 3, 'gamma': 0.9327368043030912}. Best is trial 6 with value: 0.06089824662935099.[0m
Early stopping at epoch 69
[32m[I 2025-01-04 19:52:30,204][0m Trial 10 finished with value: 0.2905123423006665 and parameters: {'observation_period_num': 21, 'train_rates': 0.8320336882388336, 'learning_rate': 5.4223222239737734e-05, 'batch_size': 178, 'step_size': 1, 'gamma': 0.8371885667483844}. Best is trial 6 with value: 0.06089824662935099.[0m
[32m[I 2025-01-04 19:52:54,399][0m Trial 11 finished with value: 0.07266733493999104 and parameters: {'observation_period_num': 98, 'train_rates': 0.7377004568460662, 'learning_rate': 0.0009952283783071427, 'batch_size': 213, 'step_size': 9, 'gamma': 0.7920829589124688}. Best is trial 6 with value: 0.06089824662935099.[0m
[32m[I 2025-01-04 19:53:20,563][0m Trial 12 finished with value: 0.10672587766916716 and parameters: {'observation_period_num': 167, 'train_rates': 0.7873472638479799, 'learning_rate': 0.0007615661175407203, 'batch_size': 212, 'step_size': 9, 'gamma': 0.815259906139005}. Best is trial 6 with value: 0.06089824662935099.[0m
[32m[I 2025-01-04 19:54:02,894][0m Trial 13 finished with value: 0.057499691057645554 and parameters: {'observation_period_num': 75, 'train_rates': 0.7369088716227474, 'learning_rate': 0.000209545603945808, 'batch_size': 123, 'step_size': 9, 'gamma': 0.8848383802121337}. Best is trial 13 with value: 0.057499691057645554.[0m
[32m[I 2025-01-04 19:54:52,267][0m Trial 14 finished with value: 0.0400103601802896 and parameters: {'observation_period_num': 50, 'train_rates': 0.8524703788039092, 'learning_rate': 0.0001839569914156264, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8816134780070509}. Best is trial 14 with value: 0.0400103601802896.[0m
[32m[I 2025-01-04 19:55:46,904][0m Trial 15 finished with value: 0.04239560291171074 and parameters: {'observation_period_num': 55, 'train_rates': 0.874416056733798, 'learning_rate': 0.00015469702965212619, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8742344925820733}. Best is trial 14 with value: 0.0400103601802896.[0m
[32m[I 2025-01-04 19:56:41,928][0m Trial 16 finished with value: 0.05737033051570142 and parameters: {'observation_period_num': 6, 'train_rates': 0.8824582164584487, 'learning_rate': 2.10265778212934e-05, 'batch_size': 107, 'step_size': 12, 'gamma': 0.8579527815535817}. Best is trial 14 with value: 0.0400103601802896.[0m
[32m[I 2025-01-04 19:57:37,349][0m Trial 17 finished with value: 0.03958502661719519 and parameters: {'observation_period_num': 50, 'train_rates': 0.8459814507511378, 'learning_rate': 8.360497140311223e-05, 'batch_size': 99, 'step_size': 11, 'gamma': 0.9821273585587443}. Best is trial 17 with value: 0.03958502661719519.[0m
[32m[I 2025-01-04 19:58:17,557][0m Trial 18 finished with value: 0.05478484858129475 and parameters: {'observation_period_num': 46, 'train_rates': 0.8475286828481794, 'learning_rate': 7.035619674961205e-05, 'batch_size': 142, 'step_size': 14, 'gamma': 0.9747216256902757}. Best is trial 17 with value: 0.03958502661719519.[0m
[32m[I 2025-01-04 19:59:24,021][0m Trial 19 finished with value: 0.06832461258662599 and parameters: {'observation_period_num': 71, 'train_rates': 0.9223828892421059, 'learning_rate': 2.502623292086153e-05, 'batch_size': 87, 'step_size': 11, 'gamma': 0.9863743714371759}. Best is trial 17 with value: 0.03958502661719519.[0m
[32m[I 2025-01-04 20:00:05,647][0m Trial 20 finished with value: 0.09240772854265249 and parameters: {'observation_period_num': 23, 'train_rates': 0.7661099559548868, 'learning_rate': 1.2345627768797077e-05, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9144163152510434}. Best is trial 17 with value: 0.03958502661719519.[0m
[32m[I 2025-01-04 20:01:02,537][0m Trial 21 finished with value: 0.04200368370897699 and parameters: {'observation_period_num': 48, 'train_rates': 0.8610376972972369, 'learning_rate': 0.00010129171112007566, 'batch_size': 100, 'step_size': 11, 'gamma': 0.8264668055757052}. Best is trial 17 with value: 0.03958502661719519.[0m
[32m[I 2025-01-04 20:02:12,034][0m Trial 22 finished with value: 0.0429030124338117 and parameters: {'observation_period_num': 57, 'train_rates': 0.8553543667563942, 'learning_rate': 9.378769274221208e-05, 'batch_size': 79, 'step_size': 13, 'gamma': 0.8308821311823018}. Best is trial 17 with value: 0.03958502661719519.[0m
[32m[I 2025-01-04 20:03:03,680][0m Trial 23 finished with value: 0.03481342560637777 and parameters: {'observation_period_num': 33, 'train_rates': 0.8169367278012084, 'learning_rate': 0.00029287745618948867, 'batch_size': 108, 'step_size': 7, 'gamma': 0.8101237186946256}. Best is trial 23 with value: 0.03481342560637777.[0m
[32m[I 2025-01-04 20:03:50,909][0m Trial 24 finished with value: 0.03310073847233346 and parameters: {'observation_period_num': 26, 'train_rates': 0.8199210731265427, 'learning_rate': 0.0003058953966054744, 'batch_size': 118, 'step_size': 7, 'gamma': 0.7946458244629028}. Best is trial 24 with value: 0.03310073847233346.[0m
[32m[I 2025-01-04 20:04:27,335][0m Trial 25 finished with value: 0.03255528599931085 and parameters: {'observation_period_num': 11, 'train_rates': 0.815852527641924, 'learning_rate': 0.0003099640628324646, 'batch_size': 150, 'step_size': 7, 'gamma': 0.7998721032957493}. Best is trial 25 with value: 0.03255528599931085.[0m
[32m[I 2025-01-04 20:04:57,147][0m Trial 26 finished with value: 0.036383450382121214 and parameters: {'observation_period_num': 10, 'train_rates': 0.8017209908855145, 'learning_rate': 0.0003070857934726354, 'batch_size': 189, 'step_size': 7, 'gamma': 0.8008445777984887}. Best is trial 25 with value: 0.03255528599931085.[0m
[32m[I 2025-01-04 20:05:34,314][0m Trial 27 finished with value: 0.047338277484252025 and parameters: {'observation_period_num': 30, 'train_rates': 0.7503967544157152, 'learning_rate': 0.00033495218642429144, 'batch_size': 145, 'step_size': 7, 'gamma': 0.7545767343621915}. Best is trial 25 with value: 0.03255528599931085.[0m
[32m[I 2025-01-04 20:06:17,060][0m Trial 28 finished with value: 0.031493014843588164 and parameters: {'observation_period_num': 5, 'train_rates': 0.8080624678771803, 'learning_rate': 0.0003031162656558069, 'batch_size': 126, 'step_size': 7, 'gamma': 0.7848645980034555}. Best is trial 28 with value: 0.031493014843588164.[0m
[32m[I 2025-01-04 20:06:52,244][0m Trial 29 finished with value: 0.03368448810246975 and parameters: {'observation_period_num': 6, 'train_rates': 0.9066129072501066, 'learning_rate': 0.0007150230737171629, 'batch_size': 169, 'step_size': 5, 'gamma': 0.7810654751059759}. Best is trial 28 with value: 0.031493014843588164.[0m
[32m[I 2025-01-04 20:07:20,574][0m Trial 30 finished with value: 0.22977439805888109 and parameters: {'observation_period_num': 110, 'train_rates': 0.7705123137052321, 'learning_rate': 3.64973898289109e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.7800304023139548}. Best is trial 28 with value: 0.031493014843588164.[0m
[32m[I 2025-01-04 20:08:00,364][0m Trial 31 finished with value: 0.03351479543577114 and parameters: {'observation_period_num': 7, 'train_rates': 0.9011922513302044, 'learning_rate': 0.0009396726603812719, 'batch_size': 155, 'step_size': 5, 'gamma': 0.7790314031208375}. Best is trial 28 with value: 0.031493014843588164.[0m
[32m[I 2025-01-04 20:08:44,306][0m Trial 32 finished with value: 0.030822387799919648 and parameters: {'observation_period_num': 21, 'train_rates': 0.8980646736651398, 'learning_rate': 0.0009050480267046533, 'batch_size': 132, 'step_size': 5, 'gamma': 0.7646190891799658}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:09:31,770][0m Trial 33 finished with value: 0.03455541032739883 and parameters: {'observation_period_num': 21, 'train_rates': 0.9294688448978536, 'learning_rate': 0.000349861538047764, 'batch_size': 125, 'step_size': 8, 'gamma': 0.7511003273652366}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:10:09,944][0m Trial 34 finished with value: 0.07456592137197263 and parameters: {'observation_period_num': 72, 'train_rates': 0.8062714888879975, 'learning_rate': 0.00026484634306504453, 'batch_size': 140, 'step_size': 3, 'gamma': 0.7704787874453897}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:11:24,284][0m Trial 35 finished with value: 0.03153266317037754 and parameters: {'observation_period_num': 25, 'train_rates': 0.8920715825746621, 'learning_rate': 0.0005087185783466924, 'batch_size': 77, 'step_size': 6, 'gamma': 0.7944439357956942}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:13:02,854][0m Trial 36 finished with value: 0.11589472944086249 and parameters: {'observation_period_num': 221, 'train_rates': 0.9438962024678976, 'learning_rate': 0.0004954974259487496, 'batch_size': 56, 'step_size': 4, 'gamma': 0.7633187942766417}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:15:06,025][0m Trial 37 finished with value: 0.07693347583214442 and parameters: {'observation_period_num': 89, 'train_rates': 0.9635919656095568, 'learning_rate': 0.0006043943592370679, 'batch_size': 47, 'step_size': 2, 'gamma': 0.8048584288569555}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:16:14,925][0m Trial 38 finished with value: 0.08872684100701374 and parameters: {'observation_period_num': 188, 'train_rates': 0.885257932775095, 'learning_rate': 0.00012170864309210498, 'batch_size': 77, 'step_size': 6, 'gamma': 0.8473186949557207}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:18:17,158][0m Trial 39 finished with value: 0.0803025975103744 and parameters: {'observation_period_num': 62, 'train_rates': 0.6959076779321339, 'learning_rate': 0.0004509510034851094, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8196729604645216}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:19:41,374][0m Trial 40 finished with value: 0.4376002580238967 and parameters: {'observation_period_num': 36, 'train_rates': 0.901692846102505, 'learning_rate': 1.959440774159189e-06, 'batch_size': 69, 'step_size': 8, 'gamma': 0.7672266744684807}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:20:18,231][0m Trial 41 finished with value: 0.048352619118851387 and parameters: {'observation_period_num': 22, 'train_rates': 0.8334775873750561, 'learning_rate': 0.0002345266011236576, 'batch_size': 153, 'step_size': 4, 'gamma': 0.7968083361758543}. Best is trial 32 with value: 0.030822387799919648.[0m
[32m[I 2025-01-04 20:21:04,229][0m Trial 42 finished with value: 0.028441652942162294 and parameters: {'observation_period_num': 16, 'train_rates': 0.7834365500910144, 'learning_rate': 0.0003969604373374693, 'batch_size': 120, 'step_size': 7, 'gamma': 0.7954024805642811}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:21:45,433][0m Trial 43 finished with value: 0.03425281279181179 and parameters: {'observation_period_num': 17, 'train_rates': 0.7867560711408105, 'learning_rate': 0.0004203839920896798, 'batch_size': 132, 'step_size': 5, 'gamma': 0.7921595235509293}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:22:15,648][0m Trial 44 finished with value: 0.14759873774400942 and parameters: {'observation_period_num': 252, 'train_rates': 0.7678401972022452, 'learning_rate': 0.0007903025994977138, 'batch_size': 170, 'step_size': 8, 'gamma': 0.7857178093712527}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:23:08,934][0m Trial 45 finished with value: 0.05300755101256072 and parameters: {'observation_period_num': 37, 'train_rates': 0.725664944071148, 'learning_rate': 0.0005987705937798912, 'batch_size': 94, 'step_size': 4, 'gamma': 0.7612111830247769}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:23:46,327][0m Trial 46 finished with value: 0.15542412008542836 and parameters: {'observation_period_num': 125, 'train_rates': 0.602903954020646, 'learning_rate': 0.0001434277224561219, 'batch_size': 118, 'step_size': 7, 'gamma': 0.842673009786796}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:24:29,893][0m Trial 47 finished with value: 0.03702009096741676 and parameters: {'observation_period_num': 15, 'train_rates': 0.9762279306058745, 'learning_rate': 0.00020196927196057302, 'batch_size': 149, 'step_size': 6, 'gamma': 0.8196061817661348}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:24:56,979][0m Trial 48 finished with value: 0.049303792896236126 and parameters: {'observation_period_num': 39, 'train_rates': 0.6691907479541473, 'learning_rate': 0.0003921001125349161, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8078026331840736}. Best is trial 42 with value: 0.028441652942162294.[0m
[32m[I 2025-01-04 20:25:38,008][0m Trial 49 finished with value: 0.05563613340258598 and parameters: {'observation_period_num': 85, 'train_rates': 0.834520703965093, 'learning_rate': 0.0006728565466627282, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8626578487212659}. Best is trial 42 with value: 0.028441652942162294.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 20:25:38,019][0m A new study created in memory with name: no-name-242b46a0-4c4a-4643-860e-f6a0c5cf279f[0m
[32m[I 2025-01-04 20:27:20,663][0m Trial 0 finished with value: 0.13715931475560225 and parameters: {'observation_period_num': 29, 'train_rates': 0.6157832592587212, 'learning_rate': 5.283517283880061e-05, 'batch_size': 42, 'step_size': 8, 'gamma': 0.7850467453550641}. Best is trial 0 with value: 0.13715931475560225.[0m
[32m[I 2025-01-04 20:27:53,818][0m Trial 1 finished with value: 0.05661633831415795 and parameters: {'observation_period_num': 38, 'train_rates': 0.7200391379458881, 'learning_rate': 0.0002033173353784637, 'batch_size': 156, 'step_size': 8, 'gamma': 0.7869357635894235}. Best is trial 1 with value: 0.05661633831415795.[0m
[32m[I 2025-01-04 20:28:19,933][0m Trial 2 finished with value: 0.33699596595097253 and parameters: {'observation_period_num': 40, 'train_rates': 0.7547518323901029, 'learning_rate': 8.518235417974752e-06, 'batch_size': 200, 'step_size': 14, 'gamma': 0.8376259379960144}. Best is trial 1 with value: 0.05661633831415795.[0m
[32m[I 2025-01-04 20:28:50,830][0m Trial 3 finished with value: 0.15662623941898346 and parameters: {'observation_period_num': 197, 'train_rates': 0.9892252342202418, 'learning_rate': 0.0002705487163873867, 'batch_size': 199, 'step_size': 12, 'gamma': 0.9752637052303447}. Best is trial 1 with value: 0.05661633831415795.[0m
[32m[I 2025-01-04 20:29:20,010][0m Trial 4 finished with value: 0.558212399482727 and parameters: {'observation_period_num': 164, 'train_rates': 0.9323617061974907, 'learning_rate': 3.6091784392576716e-06, 'batch_size': 211, 'step_size': 14, 'gamma': 0.9099878088326209}. Best is trial 1 with value: 0.05661633831415795.[0m
[32m[I 2025-01-04 20:29:47,350][0m Trial 5 finished with value: 0.8294178247451782 and parameters: {'observation_period_num': 128, 'train_rates': 0.9727007916276151, 'learning_rate': 1.7813144923292134e-06, 'batch_size': 245, 'step_size': 8, 'gamma': 0.9496518153747081}. Best is trial 1 with value: 0.05661633831415795.[0m
[32m[I 2025-01-04 20:30:41,964][0m Trial 6 finished with value: 0.0327401320763769 and parameters: {'observation_period_num': 23, 'train_rates': 0.8724896982914475, 'learning_rate': 0.00013786510751802067, 'batch_size': 104, 'step_size': 12, 'gamma': 0.9780744616811488}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:31:18,210][0m Trial 7 finished with value: 0.23745206533166982 and parameters: {'observation_period_num': 90, 'train_rates': 0.8406594737347857, 'learning_rate': 2.505602269280817e-05, 'batch_size': 160, 'step_size': 4, 'gamma': 0.8527444758408363}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:32:05,018][0m Trial 8 finished with value: 0.42422534348601004 and parameters: {'observation_period_num': 187, 'train_rates': 0.8935286364844748, 'learning_rate': 4.1897840712669564e-06, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9173937304895479}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:32:32,854][0m Trial 9 finished with value: 0.06550947184982996 and parameters: {'observation_period_num': 63, 'train_rates': 0.7725291519227391, 'learning_rate': 0.00020635607669330634, 'batch_size': 202, 'step_size': 12, 'gamma': 0.759694654012276}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:33:44,295][0m Trial 10 finished with value: 0.1767149352121587 and parameters: {'observation_period_num': 248, 'train_rates': 0.8492102687438817, 'learning_rate': 0.0007816343590162868, 'batch_size': 70, 'step_size': 1, 'gamma': 0.9877948953563005}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:34:25,495][0m Trial 11 finished with value: 0.05658616306629546 and parameters: {'observation_period_num': 5, 'train_rates': 0.6855405802110051, 'learning_rate': 0.00011086857896697734, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8177958137377765}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:35:13,400][0m Trial 12 finished with value: 0.06445084849738666 and parameters: {'observation_period_num': 6, 'train_rates': 0.642874964577226, 'learning_rate': 5.452009462976371e-05, 'batch_size': 97, 'step_size': 5, 'gamma': 0.8241445449335056}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:39:09,844][0m Trial 13 finished with value: 0.08630476872120624 and parameters: {'observation_period_num': 87, 'train_rates': 0.6928203240968556, 'learning_rate': 9.306085831405795e-05, 'batch_size': 19, 'step_size': 5, 'gamma': 0.8938377612473282}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:40:07,301][0m Trial 14 finished with value: 0.07068780635017902 and parameters: {'observation_period_num': 12, 'train_rates': 0.8260647513542142, 'learning_rate': 1.7328787969621753e-05, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8773684531874297}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:40:43,349][0m Trial 15 finished with value: 0.08157273075842447 and parameters: {'observation_period_num': 76, 'train_rates': 0.6765011837190176, 'learning_rate': 0.000980035124150339, 'batch_size': 135, 'step_size': 3, 'gamma': 0.81403354221167}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:42:02,547][0m Trial 16 finished with value: 0.06754543063190817 and parameters: {'observation_period_num': 116, 'train_rates': 0.8853007427581, 'learning_rate': 0.00010133780098234013, 'batch_size': 69, 'step_size': 6, 'gamma': 0.9406067724362273}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:42:35,642][0m Trial 17 finished with value: 0.04112069220763856 and parameters: {'observation_period_num': 55, 'train_rates': 0.8000799296751944, 'learning_rate': 0.0003315500542282706, 'batch_size': 167, 'step_size': 15, 'gamma': 0.8603997635447656}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:43:08,134][0m Trial 18 finished with value: 0.05685147062400943 and parameters: {'observation_period_num': 57, 'train_rates': 0.7974628400602239, 'learning_rate': 0.00044778008112912685, 'batch_size': 169, 'step_size': 15, 'gamma': 0.86669294313673}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:43:32,443][0m Trial 19 finished with value: 0.11771168967910633 and parameters: {'observation_period_num': 106, 'train_rates': 0.889985863497739, 'learning_rate': 0.00044839786454404514, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9553887883614773}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:44:02,933][0m Trial 20 finished with value: 0.11314958993058938 and parameters: {'observation_period_num': 160, 'train_rates': 0.8045601975923432, 'learning_rate': 0.000378779198790899, 'batch_size': 175, 'step_size': 13, 'gamma': 0.9164109726706858}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:44:47,597][0m Trial 21 finished with value: 0.04619003655413524 and parameters: {'observation_period_num': 24, 'train_rates': 0.741372741830798, 'learning_rate': 0.00011901570260031076, 'batch_size': 118, 'step_size': 10, 'gamma': 0.806616968870692}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:45:38,215][0m Trial 22 finished with value: 0.0569355886567522 and parameters: {'observation_period_num': 49, 'train_rates': 0.7492791783451608, 'learning_rate': 0.00014032493797070444, 'batch_size': 99, 'step_size': 10, 'gamma': 0.7914438793231782}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:46:51,878][0m Trial 23 finished with value: 0.046264209546101324 and parameters: {'observation_period_num': 27, 'train_rates': 0.7319870307215194, 'learning_rate': 4.9938348904365064e-05, 'batch_size': 67, 'step_size': 10, 'gamma': 0.8543222875906871}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:47:32,398][0m Trial 24 finished with value: 0.04610467933540799 and parameters: {'observation_period_num': 68, 'train_rates': 0.8658302895610989, 'learning_rate': 0.00018283174073598793, 'batch_size': 145, 'step_size': 13, 'gamma': 0.8904791953637013}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:48:13,722][0m Trial 25 finished with value: 0.07884001399915998 and parameters: {'observation_period_num': 69, 'train_rates': 0.9301903773477376, 'learning_rate': 0.0006526419129194582, 'batch_size': 145, 'step_size': 13, 'gamma': 0.890692814245847}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:48:45,646][0m Trial 26 finished with value: 0.1415396052598953 and parameters: {'observation_period_num': 96, 'train_rates': 0.868897137285654, 'learning_rate': 0.00025270680772298973, 'batch_size': 182, 'step_size': 13, 'gamma': 0.9310714698508511}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:49:12,616][0m Trial 27 finished with value: 0.0776384100317955 and parameters: {'observation_period_num': 53, 'train_rates': 0.9280973471921993, 'learning_rate': 6.509308047661715e-05, 'batch_size': 230, 'step_size': 15, 'gamma': 0.9697742752749491}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:49:50,750][0m Trial 28 finished with value: 0.1400653812840674 and parameters: {'observation_period_num': 146, 'train_rates': 0.8289068854263983, 'learning_rate': 2.8835177482336866e-05, 'batch_size': 140, 'step_size': 12, 'gamma': 0.8928557949040044}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:51:45,495][0m Trial 29 finished with value: 0.07464024306179207 and parameters: {'observation_period_num': 77, 'train_rates': 0.7768883249765278, 'learning_rate': 0.0003761593985730203, 'batch_size': 43, 'step_size': 14, 'gamma': 0.7520375519301329}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:52:51,728][0m Trial 30 finished with value: 0.041303321648548 and parameters: {'observation_period_num': 31, 'train_rates': 0.8556350504086269, 'learning_rate': 0.00015376513010643706, 'batch_size': 84, 'step_size': 11, 'gamma': 0.8666192417009793}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:54:38,537][0m Trial 31 finished with value: 0.05578854173984168 and parameters: {'observation_period_num': 31, 'train_rates': 0.8731879121240251, 'learning_rate': 0.0001805152551907617, 'batch_size': 52, 'step_size': 11, 'gamma': 0.851901628921715}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:55:46,356][0m Trial 32 finished with value: 0.049805347108307516 and parameters: {'observation_period_num': 45, 'train_rates': 0.9117670955540329, 'learning_rate': 6.681863345794591e-05, 'batch_size': 87, 'step_size': 9, 'gamma': 0.8713945486501385}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:56:23,190][0m Trial 33 finished with value: 0.033257923630672856 and parameters: {'observation_period_num': 25, 'train_rates': 0.8139599005768449, 'learning_rate': 0.0002658289575783237, 'batch_size': 156, 'step_size': 13, 'gamma': 0.833188237725973}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:57:14,999][0m Trial 34 finished with value: 0.03396118251229083 and parameters: {'observation_period_num': 20, 'train_rates': 0.805110433736532, 'learning_rate': 0.0002905430099621705, 'batch_size': 106, 'step_size': 11, 'gamma': 0.8281506660623142}. Best is trial 6 with value: 0.0327401320763769.[0m
[32m[I 2025-01-04 20:58:04,384][0m Trial 35 finished with value: 0.03170564688839282 and parameters: {'observation_period_num': 18, 'train_rates': 0.81734111148961, 'learning_rate': 0.0002858158648640722, 'batch_size': 110, 'step_size': 14, 'gamma': 0.8352000759185388}. Best is trial 35 with value: 0.03170564688839282.[0m
[32m[I 2025-01-04 20:58:53,583][0m Trial 36 finished with value: 0.028444795883618867 and parameters: {'observation_period_num': 18, 'train_rates': 0.8231226589909498, 'learning_rate': 0.0005954083924805665, 'batch_size': 112, 'step_size': 12, 'gamma': 0.8367417197461807}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 20:59:36,198][0m Trial 37 finished with value: 0.037362713693526756 and parameters: {'observation_period_num': 38, 'train_rates': 0.8242715465561086, 'learning_rate': 0.0006244740858404233, 'batch_size': 132, 'step_size': 14, 'gamma': 0.8435059556568781}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:00:22,029][0m Trial 38 finished with value: 0.4149403351324576 and parameters: {'observation_period_num': 11, 'train_rates': 0.7705798862001569, 'learning_rate': 1.0156451188293073e-06, 'batch_size': 115, 'step_size': 12, 'gamma': 0.7995508028685142}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:00:55,092][0m Trial 39 finished with value: 0.043155375868082047 and parameters: {'observation_period_num': 40, 'train_rates': 0.953261799469755, 'learning_rate': 0.0005412629374803264, 'batch_size': 189, 'step_size': 14, 'gamma': 0.7763607876199979}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:01:32,169][0m Trial 40 finished with value: 0.16088014633664283 and parameters: {'observation_period_num': 18, 'train_rates': 0.9070269200081411, 'learning_rate': 1.2288078470431192e-05, 'batch_size': 158, 'step_size': 9, 'gamma': 0.8336207415610377}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:02:21,759][0m Trial 41 finished with value: 0.030495805886382395 and parameters: {'observation_period_num': 21, 'train_rates': 0.8127807188244621, 'learning_rate': 0.00027391942877558396, 'batch_size': 110, 'step_size': 11, 'gamma': 0.8316623223141291}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:03:09,140][0m Trial 42 finished with value: 0.25419355170231434 and parameters: {'observation_period_num': 249, 'train_rates': 0.8381899436470643, 'learning_rate': 0.0008280488030080759, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8401951663507293}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:03:52,032][0m Trial 43 finished with value: 0.036877931238671814 and parameters: {'observation_period_num': 34, 'train_rates': 0.8170630318018025, 'learning_rate': 0.0002164871117822851, 'batch_size': 128, 'step_size': 13, 'gamma': 0.7722321588926602}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:04:27,239][0m Trial 44 finished with value: 0.13194688144253522 and parameters: {'observation_period_num': 200, 'train_rates': 0.7766354950209603, 'learning_rate': 0.00026878500278867495, 'batch_size': 151, 'step_size': 12, 'gamma': 0.8171882002978031}. Best is trial 36 with value: 0.028444795883618867.[0m
[32m[I 2025-01-04 21:05:36,565][0m Trial 45 finished with value: 0.02591657207919917 and parameters: {'observation_period_num': 5, 'train_rates': 0.852310534138155, 'learning_rate': 0.0006173887266949908, 'batch_size': 81, 'step_size': 9, 'gamma': 0.804171883426389}. Best is trial 45 with value: 0.02591657207919917.[0m
[32m[I 2025-01-04 21:06:41,797][0m Trial 46 finished with value: 0.022810876461562147 and parameters: {'observation_period_num': 7, 'train_rates': 0.8491356115543863, 'learning_rate': 0.0005661197363980873, 'batch_size': 84, 'step_size': 9, 'gamma': 0.8025465174431337}. Best is trial 46 with value: 0.022810876461562147.[0m
[32m[I 2025-01-04 21:07:47,186][0m Trial 47 finished with value: 0.02195585610119484 and parameters: {'observation_period_num': 6, 'train_rates': 0.8487578370143609, 'learning_rate': 0.0009562218919099258, 'batch_size': 86, 'step_size': 7, 'gamma': 0.7970804811102472}. Best is trial 47 with value: 0.02195585610119484.[0m
[32m[I 2025-01-04 21:08:54,526][0m Trial 48 finished with value: 0.021199350126271423 and parameters: {'observation_period_num': 6, 'train_rates': 0.8487170940076432, 'learning_rate': 0.000980050418692611, 'batch_size': 81, 'step_size': 7, 'gamma': 0.7899606003514623}. Best is trial 48 with value: 0.021199350126271423.[0m
[32m[I 2025-01-04 21:10:02,832][0m Trial 49 finished with value: 0.023186337536458525 and parameters: {'observation_period_num': 7, 'train_rates': 0.8505914776102276, 'learning_rate': 0.0008767726027527985, 'batch_size': 82, 'step_size': 7, 'gamma': 0.7900764087290525}. Best is trial 48 with value: 0.021199350126271423.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 15, 'train_rates': 0.8140686602762969, 'learning_rate': 0.0007921628563370255, 'batch_size': 123, 'step_size': 5, 'gamma': 0.7903514646206479}
Epoch 1/300, trend Loss: 0.6459 | 0.2738
Epoch 2/300, trend Loss: 0.1623 | 0.1834
Epoch 3/300, trend Loss: 0.1367 | 0.0886
Epoch 4/300, trend Loss: 0.1284 | 0.0798
Epoch 5/300, trend Loss: 0.1306 | 0.1033
Epoch 6/300, trend Loss: 0.1492 | 0.1829
Epoch 7/300, trend Loss: 0.1395 | 0.1795
Epoch 8/300, trend Loss: 0.1552 | 0.1454
Epoch 9/300, trend Loss: 0.1318 | 0.0870
Epoch 10/300, trend Loss: 0.1103 | 0.0649
Epoch 11/300, trend Loss: 0.1048 | 0.0692
Epoch 12/300, trend Loss: 0.1051 | 0.0603
Epoch 13/300, trend Loss: 0.0972 | 0.0551
Epoch 14/300, trend Loss: 0.0946 | 0.0546
Epoch 15/300, trend Loss: 0.0939 | 0.0525
Epoch 16/300, trend Loss: 0.0928 | 0.0550
Epoch 17/300, trend Loss: 0.0929 | 0.0556
Epoch 18/300, trend Loss: 0.0933 | 0.0528
Epoch 19/300, trend Loss: 0.0939 | 0.0514
Epoch 20/300, trend Loss: 0.0947 | 0.0504
Epoch 21/300, trend Loss: 0.0933 | 0.0524
Epoch 22/300, trend Loss: 0.0915 | 0.0527
Epoch 23/300, trend Loss: 0.0923 | 0.0511
Epoch 24/300, trend Loss: 0.0941 | 0.0498
Epoch 25/300, trend Loss: 0.0906 | 0.0481
Epoch 26/300, trend Loss: 0.0866 | 0.0483
Epoch 27/300, trend Loss: 0.0858 | 0.0484
Epoch 28/300, trend Loss: 0.0853 | 0.0483
Epoch 29/300, trend Loss: 0.0849 | 0.0479
Epoch 30/300, trend Loss: 0.0845 | 0.0476
Epoch 31/300, trend Loss: 0.0842 | 0.0474
Epoch 32/300, trend Loss: 0.0840 | 0.0473
Epoch 33/300, trend Loss: 0.0838 | 0.0470
Epoch 34/300, trend Loss: 0.0836 | 0.0469
Epoch 35/300, trend Loss: 0.0834 | 0.0467
Epoch 36/300, trend Loss: 0.0832 | 0.0465
Epoch 37/300, trend Loss: 0.0831 | 0.0464
Epoch 38/300, trend Loss: 0.0829 | 0.0463
Epoch 39/300, trend Loss: 0.0828 | 0.0462
Epoch 40/300, trend Loss: 0.0827 | 0.0461
Epoch 41/300, trend Loss: 0.0826 | 0.0460
Epoch 42/300, trend Loss: 0.0825 | 0.0459
Epoch 43/300, trend Loss: 0.0824 | 0.0458
Epoch 44/300, trend Loss: 0.0823 | 0.0457
Epoch 45/300, trend Loss: 0.0822 | 0.0456
Epoch 46/300, trend Loss: 0.0821 | 0.0456
Epoch 47/300, trend Loss: 0.0821 | 0.0455
Epoch 48/300, trend Loss: 0.0820 | 0.0455
Epoch 49/300, trend Loss: 0.0820 | 0.0454
Epoch 50/300, trend Loss: 0.0819 | 0.0454
Epoch 51/300, trend Loss: 0.0819 | 0.0453
Epoch 52/300, trend Loss: 0.0818 | 0.0453
Epoch 53/300, trend Loss: 0.0818 | 0.0453
Epoch 54/300, trend Loss: 0.0817 | 0.0452
Epoch 55/300, trend Loss: 0.0817 | 0.0452
Epoch 56/300, trend Loss: 0.0817 | 0.0452
Epoch 57/300, trend Loss: 0.0817 | 0.0451
Epoch 58/300, trend Loss: 0.0816 | 0.0451
Epoch 59/300, trend Loss: 0.0816 | 0.0451
Epoch 60/300, trend Loss: 0.0816 | 0.0451
Epoch 61/300, trend Loss: 0.0816 | 0.0451
Epoch 62/300, trend Loss: 0.0815 | 0.0450
Epoch 63/300, trend Loss: 0.0815 | 0.0450
Epoch 64/300, trend Loss: 0.0815 | 0.0450
Epoch 65/300, trend Loss: 0.0815 | 0.0450
Epoch 66/300, trend Loss: 0.0815 | 0.0450
Epoch 67/300, trend Loss: 0.0815 | 0.0450
Epoch 68/300, trend Loss: 0.0815 | 0.0450
Epoch 69/300, trend Loss: 0.0815 | 0.0450
Epoch 70/300, trend Loss: 0.0814 | 0.0449
Epoch 71/300, trend Loss: 0.0814 | 0.0449
Epoch 72/300, trend Loss: 0.0814 | 0.0449
Epoch 73/300, trend Loss: 0.0814 | 0.0449
Epoch 74/300, trend Loss: 0.0814 | 0.0449
Epoch 75/300, trend Loss: 0.0814 | 0.0449
Epoch 76/300, trend Loss: 0.0814 | 0.0449
Epoch 77/300, trend Loss: 0.0814 | 0.0449
Epoch 78/300, trend Loss: 0.0814 | 0.0449
Epoch 79/300, trend Loss: 0.0814 | 0.0449
Epoch 80/300, trend Loss: 0.0814 | 0.0449
Epoch 81/300, trend Loss: 0.0814 | 0.0449
Epoch 82/300, trend Loss: 0.0814 | 0.0449
Epoch 83/300, trend Loss: 0.0814 | 0.0449
Epoch 84/300, trend Loss: 0.0814 | 0.0449
Epoch 85/300, trend Loss: 0.0814 | 0.0449
Epoch 86/300, trend Loss: 0.0814 | 0.0449
Epoch 87/300, trend Loss: 0.0814 | 0.0449
Epoch 88/300, trend Loss: 0.0814 | 0.0449
Epoch 89/300, trend Loss: 0.0814 | 0.0449
Epoch 90/300, trend Loss: 0.0814 | 0.0449
Epoch 91/300, trend Loss: 0.0814 | 0.0449
Epoch 92/300, trend Loss: 0.0814 | 0.0449
Epoch 93/300, trend Loss: 0.0814 | 0.0449
Epoch 94/300, trend Loss: 0.0814 | 0.0449
Epoch 95/300, trend Loss: 0.0814 | 0.0449
Epoch 96/300, trend Loss: 0.0814 | 0.0449
Epoch 97/300, trend Loss: 0.0814 | 0.0449
Epoch 98/300, trend Loss: 0.0814 | 0.0449
Epoch 99/300, trend Loss: 0.0814 | 0.0449
Epoch 100/300, trend Loss: 0.0814 | 0.0449
Epoch 101/300, trend Loss: 0.0813 | 0.0449
Epoch 102/300, trend Loss: 0.0813 | 0.0449
Epoch 103/300, trend Loss: 0.0813 | 0.0449
Epoch 104/300, trend Loss: 0.0813 | 0.0449
Epoch 105/300, trend Loss: 0.0813 | 0.0448
Epoch 106/300, trend Loss: 0.0813 | 0.0448
Epoch 107/300, trend Loss: 0.0813 | 0.0448
Epoch 108/300, trend Loss: 0.0813 | 0.0448
Epoch 109/300, trend Loss: 0.0813 | 0.0448
Epoch 110/300, trend Loss: 0.0813 | 0.0448
Epoch 111/300, trend Loss: 0.0813 | 0.0448
Epoch 112/300, trend Loss: 0.0813 | 0.0448
Epoch 113/300, trend Loss: 0.0813 | 0.0448
Epoch 114/300, trend Loss: 0.0813 | 0.0448
Epoch 115/300, trend Loss: 0.0813 | 0.0448
Epoch 116/300, trend Loss: 0.0813 | 0.0448
Epoch 117/300, trend Loss: 0.0813 | 0.0448
Epoch 118/300, trend Loss: 0.0813 | 0.0448
Epoch 119/300, trend Loss: 0.0813 | 0.0448
Epoch 120/300, trend Loss: 0.0813 | 0.0448
Epoch 121/300, trend Loss: 0.0813 | 0.0448
Epoch 122/300, trend Loss: 0.0813 | 0.0448
Epoch 123/300, trend Loss: 0.0813 | 0.0448
Epoch 124/300, trend Loss: 0.0813 | 0.0448
Epoch 125/300, trend Loss: 0.0813 | 0.0448
Epoch 126/300, trend Loss: 0.0813 | 0.0448
Epoch 127/300, trend Loss: 0.0813 | 0.0448
Epoch 128/300, trend Loss: 0.0813 | 0.0448
Epoch 129/300, trend Loss: 0.0813 | 0.0448
Epoch 130/300, trend Loss: 0.0813 | 0.0448
Epoch 131/300, trend Loss: 0.0813 | 0.0448
Epoch 132/300, trend Loss: 0.0813 | 0.0448
Epoch 133/300, trend Loss: 0.0813 | 0.0448
Epoch 134/300, trend Loss: 0.0813 | 0.0448
Epoch 135/300, trend Loss: 0.0813 | 0.0448
Epoch 136/300, trend Loss: 0.0813 | 0.0448
Epoch 137/300, trend Loss: 0.0813 | 0.0448
Epoch 138/300, trend Loss: 0.0813 | 0.0448
Epoch 139/300, trend Loss: 0.0813 | 0.0448
Epoch 140/300, trend Loss: 0.0813 | 0.0448
Epoch 141/300, trend Loss: 0.0813 | 0.0448
Epoch 142/300, trend Loss: 0.0813 | 0.0448
Epoch 143/300, trend Loss: 0.0813 | 0.0448
Epoch 144/300, trend Loss: 0.0813 | 0.0448
Epoch 145/300, trend Loss: 0.0813 | 0.0448
Epoch 146/300, trend Loss: 0.0813 | 0.0448
Epoch 147/300, trend Loss: 0.0813 | 0.0448
Epoch 148/300, trend Loss: 0.0813 | 0.0448
Epoch 149/300, trend Loss: 0.0813 | 0.0448
Epoch 150/300, trend Loss: 0.0813 | 0.0448
Epoch 151/300, trend Loss: 0.0813 | 0.0448
Epoch 152/300, trend Loss: 0.0813 | 0.0448
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 6, 'train_rates': 0.7100998910289729, 'learning_rate': 0.0009858622658821053, 'batch_size': 45, 'step_size': 5, 'gamma': 0.751237745001437}
Epoch 1/300, seasonal_0 Loss: 0.2941 | 0.2523
Epoch 2/300, seasonal_0 Loss: 0.1231 | 0.0861
Epoch 3/300, seasonal_0 Loss: 0.1093 | 0.2025
Epoch 4/300, seasonal_0 Loss: 0.1091 | 0.0821
Epoch 5/300, seasonal_0 Loss: 0.1206 | 0.0653
Epoch 6/300, seasonal_0 Loss: 0.0982 | 0.0699
Epoch 7/300, seasonal_0 Loss: 0.0948 | 0.0672
Epoch 8/300, seasonal_0 Loss: 0.0938 | 0.0690
Epoch 9/300, seasonal_0 Loss: 0.0942 | 0.1236
Epoch 10/300, seasonal_0 Loss: 0.0970 | 0.1138
Epoch 11/300, seasonal_0 Loss: 0.0944 | 0.0889
Epoch 12/300, seasonal_0 Loss: 0.0910 | 0.0505
Epoch 13/300, seasonal_0 Loss: 0.0857 | 0.0624
Epoch 14/300, seasonal_0 Loss: 0.0849 | 0.0427
Epoch 15/300, seasonal_0 Loss: 0.0840 | 0.0562
Epoch 16/300, seasonal_0 Loss: 0.0835 | 0.0408
Epoch 17/300, seasonal_0 Loss: 0.0829 | 0.0468
Epoch 18/300, seasonal_0 Loss: 0.0822 | 0.0399
Epoch 19/300, seasonal_0 Loss: 0.0812 | 0.0459
Epoch 20/300, seasonal_0 Loss: 0.0808 | 0.0399
Epoch 21/300, seasonal_0 Loss: 0.0801 | 0.0446
Epoch 22/300, seasonal_0 Loss: 0.0799 | 0.0414
Epoch 23/300, seasonal_0 Loss: 0.0794 | 0.0421
Epoch 24/300, seasonal_0 Loss: 0.0789 | 0.0405
Epoch 25/300, seasonal_0 Loss: 0.0786 | 0.0404
Epoch 26/300, seasonal_0 Loss: 0.0782 | 0.0400
Epoch 27/300, seasonal_0 Loss: 0.0779 | 0.0398
Epoch 28/300, seasonal_0 Loss: 0.0776 | 0.0395
Epoch 29/300, seasonal_0 Loss: 0.0774 | 0.0398
Epoch 30/300, seasonal_0 Loss: 0.0772 | 0.0394
Epoch 31/300, seasonal_0 Loss: 0.0769 | 0.0398
Epoch 32/300, seasonal_0 Loss: 0.0768 | 0.0394
Epoch 33/300, seasonal_0 Loss: 0.0766 | 0.0391
Epoch 34/300, seasonal_0 Loss: 0.0765 | 0.0394
Epoch 35/300, seasonal_0 Loss: 0.0763 | 0.0390
Epoch 36/300, seasonal_0 Loss: 0.0762 | 0.0392
Epoch 37/300, seasonal_0 Loss: 0.0761 | 0.0389
Epoch 38/300, seasonal_0 Loss: 0.0759 | 0.0387
Epoch 39/300, seasonal_0 Loss: 0.0758 | 0.0388
Epoch 40/300, seasonal_0 Loss: 0.0757 | 0.0386
Epoch 41/300, seasonal_0 Loss: 0.0756 | 0.0386
Epoch 42/300, seasonal_0 Loss: 0.0756 | 0.0384
Epoch 43/300, seasonal_0 Loss: 0.0755 | 0.0384
Epoch 44/300, seasonal_0 Loss: 0.0754 | 0.0383
Epoch 45/300, seasonal_0 Loss: 0.0754 | 0.0382
Epoch 46/300, seasonal_0 Loss: 0.0753 | 0.0382
Epoch 47/300, seasonal_0 Loss: 0.0753 | 0.0382
Epoch 48/300, seasonal_0 Loss: 0.0752 | 0.0382
Epoch 49/300, seasonal_0 Loss: 0.0752 | 0.0382
Epoch 50/300, seasonal_0 Loss: 0.0751 | 0.0382
Epoch 51/300, seasonal_0 Loss: 0.0751 | 0.0381
Epoch 52/300, seasonal_0 Loss: 0.0750 | 0.0381
Epoch 53/300, seasonal_0 Loss: 0.0750 | 0.0381
Epoch 54/300, seasonal_0 Loss: 0.0750 | 0.0380
Epoch 55/300, seasonal_0 Loss: 0.0750 | 0.0380
Epoch 56/300, seasonal_0 Loss: 0.0749 | 0.0380
Epoch 57/300, seasonal_0 Loss: 0.0749 | 0.0380
Epoch 58/300, seasonal_0 Loss: 0.0749 | 0.0379
Epoch 59/300, seasonal_0 Loss: 0.0749 | 0.0379
Epoch 60/300, seasonal_0 Loss: 0.0749 | 0.0379
Epoch 61/300, seasonal_0 Loss: 0.0748 | 0.0379
Epoch 62/300, seasonal_0 Loss: 0.0748 | 0.0379
Epoch 63/300, seasonal_0 Loss: 0.0748 | 0.0379
Epoch 64/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 65/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 66/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 67/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 68/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 69/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 70/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 71/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 72/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 73/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 74/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 75/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 76/300, seasonal_0 Loss: 0.0748 | 0.0378
Epoch 77/300, seasonal_0 Loss: 0.0747 | 0.0378
Epoch 78/300, seasonal_0 Loss: 0.0747 | 0.0378
Epoch 79/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 80/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 81/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 82/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 83/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 84/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 85/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 86/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 87/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 88/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 89/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 90/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 91/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 92/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 93/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 94/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 95/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 96/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 97/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 98/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 99/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 100/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 101/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 102/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 103/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 104/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 105/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 106/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 107/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 108/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 109/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 110/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 111/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 112/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 113/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 114/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 115/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 116/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 117/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 118/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 119/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 120/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 121/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 122/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 123/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 124/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 125/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 126/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 127/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 128/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 129/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 130/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 131/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 132/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 133/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 134/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 135/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 136/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 137/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 138/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 139/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 140/300, seasonal_0 Loss: 0.0747 | 0.0377
Epoch 141/300, seasonal_0 Loss: 0.0747 | 0.0377
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 10, 'train_rates': 0.7000764669856001, 'learning_rate': 0.00026984471152186615, 'batch_size': 88, 'step_size': 10, 'gamma': 0.9517658142664869}
Epoch 1/300, seasonal_1 Loss: 0.7294 | 0.2602
Epoch 2/300, seasonal_1 Loss: 0.2097 | 0.1788
Epoch 3/300, seasonal_1 Loss: 0.1672 | 0.1507
Epoch 4/300, seasonal_1 Loss: 0.1403 | 0.1329
Epoch 5/300, seasonal_1 Loss: 0.1304 | 0.1231
Epoch 6/300, seasonal_1 Loss: 0.1263 | 0.1169
Epoch 7/300, seasonal_1 Loss: 0.1264 | 0.1114
Epoch 8/300, seasonal_1 Loss: 0.1256 | 0.1012
Epoch 9/300, seasonal_1 Loss: 0.1215 | 0.1000
Epoch 10/300, seasonal_1 Loss: 0.1183 | 0.0989
Epoch 11/300, seasonal_1 Loss: 0.1159 | 0.0931
Epoch 12/300, seasonal_1 Loss: 0.1136 | 0.0925
Epoch 13/300, seasonal_1 Loss: 0.1110 | 0.0914
Epoch 14/300, seasonal_1 Loss: 0.1088 | 0.0886
Epoch 15/300, seasonal_1 Loss: 0.1068 | 0.0833
Epoch 16/300, seasonal_1 Loss: 0.1046 | 0.0739
Epoch 17/300, seasonal_1 Loss: 0.1025 | 0.0687
Epoch 18/300, seasonal_1 Loss: 0.1004 | 0.0743
Epoch 19/300, seasonal_1 Loss: 0.0994 | 0.0833
Epoch 20/300, seasonal_1 Loss: 0.0985 | 0.0827
Epoch 21/300, seasonal_1 Loss: 0.0972 | 0.0828
Epoch 22/300, seasonal_1 Loss: 0.0962 | 0.0821
Epoch 23/300, seasonal_1 Loss: 0.0949 | 0.0783
Epoch 24/300, seasonal_1 Loss: 0.0932 | 0.0772
Epoch 25/300, seasonal_1 Loss: 0.0917 | 0.0748
Epoch 26/300, seasonal_1 Loss: 0.0903 | 0.0747
Epoch 27/300, seasonal_1 Loss: 0.0895 | 0.0692
Epoch 28/300, seasonal_1 Loss: 0.0886 | 0.0636
Epoch 29/300, seasonal_1 Loss: 0.0877 | 0.0578
Epoch 30/300, seasonal_1 Loss: 0.0867 | 0.0534
Epoch 31/300, seasonal_1 Loss: 0.0857 | 0.0508
Epoch 32/300, seasonal_1 Loss: 0.0848 | 0.0467
Epoch 33/300, seasonal_1 Loss: 0.0839 | 0.0443
Epoch 34/300, seasonal_1 Loss: 0.0831 | 0.0421
Epoch 35/300, seasonal_1 Loss: 0.0825 | 0.0407
Epoch 36/300, seasonal_1 Loss: 0.0820 | 0.0401
Epoch 37/300, seasonal_1 Loss: 0.0816 | 0.0397
Epoch 38/300, seasonal_1 Loss: 0.0812 | 0.0390
Epoch 39/300, seasonal_1 Loss: 0.0808 | 0.0386
Epoch 40/300, seasonal_1 Loss: 0.0805 | 0.0388
Epoch 41/300, seasonal_1 Loss: 0.0802 | 0.0387
Epoch 42/300, seasonal_1 Loss: 0.0799 | 0.0376
Epoch 43/300, seasonal_1 Loss: 0.0793 | 0.0371
Epoch 44/300, seasonal_1 Loss: 0.0789 | 0.0368
Epoch 45/300, seasonal_1 Loss: 0.0786 | 0.0367
Epoch 46/300, seasonal_1 Loss: 0.0785 | 0.0366
Epoch 47/300, seasonal_1 Loss: 0.0784 | 0.0366
Epoch 48/300, seasonal_1 Loss: 0.0781 | 0.0368
Epoch 49/300, seasonal_1 Loss: 0.0775 | 0.0372
Epoch 50/300, seasonal_1 Loss: 0.0770 | 0.0377
Epoch 51/300, seasonal_1 Loss: 0.0765 | 0.0363
Epoch 52/300, seasonal_1 Loss: 0.0762 | 0.0352
Epoch 53/300, seasonal_1 Loss: 0.0765 | 0.0352
Epoch 54/300, seasonal_1 Loss: 0.0765 | 0.0352
Epoch 55/300, seasonal_1 Loss: 0.0758 | 0.0338
Epoch 56/300, seasonal_1 Loss: 0.0746 | 0.0331
Epoch 57/300, seasonal_1 Loss: 0.0742 | 0.0332
Epoch 58/300, seasonal_1 Loss: 0.0736 | 0.0335
Epoch 59/300, seasonal_1 Loss: 0.0730 | 0.0339
Epoch 60/300, seasonal_1 Loss: 0.0726 | 0.0329
Epoch 61/300, seasonal_1 Loss: 0.0722 | 0.0321
Epoch 62/300, seasonal_1 Loss: 0.0721 | 0.0320
Epoch 63/300, seasonal_1 Loss: 0.0716 | 0.0321
Epoch 64/300, seasonal_1 Loss: 0.0712 | 0.0316
Epoch 65/300, seasonal_1 Loss: 0.0710 | 0.0317
Epoch 66/300, seasonal_1 Loss: 0.0706 | 0.0318
Epoch 67/300, seasonal_1 Loss: 0.0704 | 0.0313
Epoch 68/300, seasonal_1 Loss: 0.0701 | 0.0310
Epoch 69/300, seasonal_1 Loss: 0.0698 | 0.0311
Epoch 70/300, seasonal_1 Loss: 0.0696 | 0.0310
Epoch 71/300, seasonal_1 Loss: 0.0693 | 0.0307
Epoch 72/300, seasonal_1 Loss: 0.0691 | 0.0304
Epoch 73/300, seasonal_1 Loss: 0.0690 | 0.0307
Epoch 74/300, seasonal_1 Loss: 0.0689 | 0.0306
Epoch 75/300, seasonal_1 Loss: 0.0688 | 0.0301
Epoch 76/300, seasonal_1 Loss: 0.0687 | 0.0301
Epoch 77/300, seasonal_1 Loss: 0.0691 | 0.0305
Epoch 78/300, seasonal_1 Loss: 0.0717 | 0.0329
Epoch 79/300, seasonal_1 Loss: 0.0707 | 0.0365
Epoch 80/300, seasonal_1 Loss: 0.0704 | 0.0524
Epoch 81/300, seasonal_1 Loss: 0.0739 | 0.2036
Epoch 82/300, seasonal_1 Loss: 0.0992 | 0.0465
Epoch 83/300, seasonal_1 Loss: 0.0795 | 0.0469
Epoch 84/300, seasonal_1 Loss: 0.0740 | 0.0341
Epoch 85/300, seasonal_1 Loss: 0.0712 | 0.0299
Epoch 86/300, seasonal_1 Loss: 0.0692 | 0.0515
Epoch 87/300, seasonal_1 Loss: 0.0711 | 0.0360
Epoch 88/300, seasonal_1 Loss: 0.0685 | 0.0492
Epoch 89/300, seasonal_1 Loss: 0.0703 | 0.0374
Epoch 90/300, seasonal_1 Loss: 0.0679 | 0.0372
Epoch 91/300, seasonal_1 Loss: 0.0678 | 0.0291
Epoch 92/300, seasonal_1 Loss: 0.0668 | 0.0304
Epoch 93/300, seasonal_1 Loss: 0.0668 | 0.0297
Epoch 94/300, seasonal_1 Loss: 0.0665 | 0.0284
Epoch 95/300, seasonal_1 Loss: 0.0662 | 0.0289
Epoch 96/300, seasonal_1 Loss: 0.0663 | 0.0282
Epoch 97/300, seasonal_1 Loss: 0.0662 | 0.0282
Epoch 98/300, seasonal_1 Loss: 0.0661 | 0.0277
Epoch 99/300, seasonal_1 Loss: 0.0658 | 0.0273
Epoch 100/300, seasonal_1 Loss: 0.0656 | 0.0272
Epoch 101/300, seasonal_1 Loss: 0.0654 | 0.0272
Epoch 102/300, seasonal_1 Loss: 0.0654 | 0.0273
Epoch 103/300, seasonal_1 Loss: 0.0653 | 0.0276
Epoch 104/300, seasonal_1 Loss: 0.0653 | 0.0279
Epoch 105/300, seasonal_1 Loss: 0.0654 | 0.0285
Epoch 106/300, seasonal_1 Loss: 0.0655 | 0.0283
Epoch 107/300, seasonal_1 Loss: 0.0655 | 0.0279
Epoch 108/300, seasonal_1 Loss: 0.0649 | 0.0274
Epoch 109/300, seasonal_1 Loss: 0.0648 | 0.0278
Epoch 110/300, seasonal_1 Loss: 0.0648 | 0.0275
Epoch 111/300, seasonal_1 Loss: 0.0647 | 0.0271
Epoch 112/300, seasonal_1 Loss: 0.0648 | 0.0272
Epoch 113/300, seasonal_1 Loss: 0.0647 | 0.0272
Epoch 114/300, seasonal_1 Loss: 0.0647 | 0.0273
Epoch 115/300, seasonal_1 Loss: 0.0649 | 0.0272
Epoch 116/300, seasonal_1 Loss: 0.0653 | 0.0278
Epoch 117/300, seasonal_1 Loss: 0.0697 | 0.0363
Epoch 118/300, seasonal_1 Loss: 0.0678 | 0.0324
Epoch 119/300, seasonal_1 Loss: 0.0681 | 0.0293
Epoch 120/300, seasonal_1 Loss: 0.0692 | 0.0310
Epoch 121/300, seasonal_1 Loss: 0.0661 | 0.0280
Epoch 122/300, seasonal_1 Loss: 0.0662 | 0.0310
Epoch 123/300, seasonal_1 Loss: 0.0648 | 0.0275
Epoch 124/300, seasonal_1 Loss: 0.0648 | 0.0293
Epoch 125/300, seasonal_1 Loss: 0.0652 | 0.0286
Epoch 126/300, seasonal_1 Loss: 0.0645 | 0.0276
Epoch 127/300, seasonal_1 Loss: 0.0642 | 0.0279
Epoch 128/300, seasonal_1 Loss: 0.0640 | 0.0277
Epoch 129/300, seasonal_1 Loss: 0.0639 | 0.0270
Epoch 130/300, seasonal_1 Loss: 0.0638 | 0.0274
Epoch 131/300, seasonal_1 Loss: 0.0636 | 0.0266
Epoch 132/300, seasonal_1 Loss: 0.0636 | 0.0276
Epoch 133/300, seasonal_1 Loss: 0.0633 | 0.0267
Epoch 134/300, seasonal_1 Loss: 0.0635 | 0.0278
Epoch 135/300, seasonal_1 Loss: 0.0631 | 0.0266
Epoch 136/300, seasonal_1 Loss: 0.0634 | 0.0281
Epoch 137/300, seasonal_1 Loss: 0.0629 | 0.0265
Epoch 138/300, seasonal_1 Loss: 0.0633 | 0.0273
Epoch 139/300, seasonal_1 Loss: 0.0629 | 0.0277
Epoch 140/300, seasonal_1 Loss: 0.0630 | 0.0270
Epoch 141/300, seasonal_1 Loss: 0.0629 | 0.0326
Epoch 142/300, seasonal_1 Loss: 0.0629 | 0.0276
Epoch 143/300, seasonal_1 Loss: 0.0633 | 0.0455
Epoch 144/300, seasonal_1 Loss: 0.0631 | 0.0278
Epoch 145/300, seasonal_1 Loss: 0.0632 | 0.0368
Epoch 146/300, seasonal_1 Loss: 0.0626 | 0.0273
Epoch 147/300, seasonal_1 Loss: 0.0632 | 0.0305
Epoch 148/300, seasonal_1 Loss: 0.0625 | 0.0290
Epoch 149/300, seasonal_1 Loss: 0.0630 | 0.0276
Epoch 150/300, seasonal_1 Loss: 0.0628 | 0.0382
Epoch 151/300, seasonal_1 Loss: 0.0633 | 0.0270
Epoch 152/300, seasonal_1 Loss: 0.0648 | 0.0595
Epoch 153/300, seasonal_1 Loss: 0.0654 | 0.0272
Epoch 154/300, seasonal_1 Loss: 0.0651 | 0.0340
Epoch 155/300, seasonal_1 Loss: 0.0647 | 0.0321
Epoch 156/300, seasonal_1 Loss: 0.0642 | 0.0263
Epoch 157/300, seasonal_1 Loss: 0.0634 | 0.0303
Epoch 158/300, seasonal_1 Loss: 0.0637 | 0.0270
Epoch 159/300, seasonal_1 Loss: 0.0636 | 0.0362
Epoch 160/300, seasonal_1 Loss: 0.0640 | 0.0259
Epoch 161/300, seasonal_1 Loss: 0.0646 | 0.0294
Epoch 162/300, seasonal_1 Loss: 0.0666 | 0.0297
Epoch 163/300, seasonal_1 Loss: 0.0635 | 0.0321
Epoch 164/300, seasonal_1 Loss: 0.0672 | 0.0524
Epoch 165/300, seasonal_1 Loss: 0.0645 | 0.0274
Epoch 166/300, seasonal_1 Loss: 0.0658 | 0.0296
Epoch 167/300, seasonal_1 Loss: 0.0641 | 0.0351
Epoch 168/300, seasonal_1 Loss: 0.0644 | 0.0301
Epoch 169/300, seasonal_1 Loss: 0.0655 | 0.0412
Epoch 170/300, seasonal_1 Loss: 0.0644 | 0.0268
Epoch 171/300, seasonal_1 Loss: 0.0636 | 0.0275
Epoch 172/300, seasonal_1 Loss: 0.0628 | 0.0277
Epoch 173/300, seasonal_1 Loss: 0.0626 | 0.0271
Epoch 174/300, seasonal_1 Loss: 0.0621 | 0.0297
Epoch 175/300, seasonal_1 Loss: 0.0618 | 0.0273
Epoch 176/300, seasonal_1 Loss: 0.0614 | 0.0297
Epoch 177/300, seasonal_1 Loss: 0.0612 | 0.0272
Epoch 178/300, seasonal_1 Loss: 0.0611 | 0.0283
Epoch 179/300, seasonal_1 Loss: 0.0608 | 0.0271
Epoch 180/300, seasonal_1 Loss: 0.0606 | 0.0272
Epoch 181/300, seasonal_1 Loss: 0.0604 | 0.0274
Epoch 182/300, seasonal_1 Loss: 0.0604 | 0.0268
Epoch 183/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 184/300, seasonal_1 Loss: 0.0602 | 0.0267
Epoch 185/300, seasonal_1 Loss: 0.0600 | 0.0277
Epoch 186/300, seasonal_1 Loss: 0.0599 | 0.0266
Epoch 187/300, seasonal_1 Loss: 0.0598 | 0.0268
Epoch 188/300, seasonal_1 Loss: 0.0597 | 0.0267
Epoch 189/300, seasonal_1 Loss: 0.0597 | 0.0264
Epoch 190/300, seasonal_1 Loss: 0.0596 | 0.0269
Epoch 191/300, seasonal_1 Loss: 0.0596 | 0.0264
Epoch 192/300, seasonal_1 Loss: 0.0595 | 0.0268
Epoch 193/300, seasonal_1 Loss: 0.0595 | 0.0262
Epoch 194/300, seasonal_1 Loss: 0.0595 | 0.0263
Epoch 195/300, seasonal_1 Loss: 0.0594 | 0.0260
Epoch 196/300, seasonal_1 Loss: 0.0594 | 0.0260
Epoch 197/300, seasonal_1 Loss: 0.0594 | 0.0262
Epoch 198/300, seasonal_1 Loss: 0.0595 | 0.0260
Epoch 199/300, seasonal_1 Loss: 0.0596 | 0.0264
Epoch 200/300, seasonal_1 Loss: 0.0596 | 0.0260
Epoch 201/300, seasonal_1 Loss: 0.0599 | 0.0260
Epoch 202/300, seasonal_1 Loss: 0.0602 | 0.0259
Epoch 203/300, seasonal_1 Loss: 0.0607 | 0.0256
Epoch 204/300, seasonal_1 Loss: 0.0613 | 0.0257
Epoch 205/300, seasonal_1 Loss: 0.0609 | 0.0265
Epoch 206/300, seasonal_1 Loss: 0.0607 | 0.0266
Epoch 207/300, seasonal_1 Loss: 0.0604 | 0.0262
Epoch 208/300, seasonal_1 Loss: 0.0603 | 0.0267
Epoch 209/300, seasonal_1 Loss: 0.0600 | 0.0260
Epoch 210/300, seasonal_1 Loss: 0.0599 | 0.0262
Epoch 211/300, seasonal_1 Loss: 0.0596 | 0.0267
Epoch 212/300, seasonal_1 Loss: 0.0596 | 0.0258
Epoch 213/300, seasonal_1 Loss: 0.0595 | 0.0281
Epoch 214/300, seasonal_1 Loss: 0.0594 | 0.0258
Epoch 215/300, seasonal_1 Loss: 0.0592 | 0.0281
Epoch 216/300, seasonal_1 Loss: 0.0591 | 0.0262
Epoch 217/300, seasonal_1 Loss: 0.0590 | 0.0264
Epoch 218/300, seasonal_1 Loss: 0.0589 | 0.0279
Epoch 219/300, seasonal_1 Loss: 0.0590 | 0.0261
Epoch 220/300, seasonal_1 Loss: 0.0589 | 0.0286
Epoch 221/300, seasonal_1 Loss: 0.0591 | 0.0261
Epoch 222/300, seasonal_1 Loss: 0.0590 | 0.0265
Epoch 223/300, seasonal_1 Loss: 0.0590 | 0.0265
Epoch 224/300, seasonal_1 Loss: 0.0590 | 0.0260
Epoch 225/300, seasonal_1 Loss: 0.0591 | 0.0271
Epoch 226/300, seasonal_1 Loss: 0.0591 | 0.0259
Epoch 227/300, seasonal_1 Loss: 0.0590 | 0.0260
Epoch 228/300, seasonal_1 Loss: 0.0589 | 0.0257
Epoch 229/300, seasonal_1 Loss: 0.0587 | 0.0257
Epoch 230/300, seasonal_1 Loss: 0.0586 | 0.0263
Epoch 231/300, seasonal_1 Loss: 0.0585 | 0.0261
Epoch 232/300, seasonal_1 Loss: 0.0584 | 0.0260
Epoch 233/300, seasonal_1 Loss: 0.0584 | 0.0259
Epoch 234/300, seasonal_1 Loss: 0.0583 | 0.0260
Epoch 235/300, seasonal_1 Loss: 0.0583 | 0.0261
Epoch 236/300, seasonal_1 Loss: 0.0583 | 0.0265
Epoch 237/300, seasonal_1 Loss: 0.0582 | 0.0263
Epoch 238/300, seasonal_1 Loss: 0.0582 | 0.0265
Epoch 239/300, seasonal_1 Loss: 0.0581 | 0.0263
Epoch 240/300, seasonal_1 Loss: 0.0580 | 0.0263
Epoch 241/300, seasonal_1 Loss: 0.0580 | 0.0266
Epoch 242/300, seasonal_1 Loss: 0.0580 | 0.0265
Epoch 243/300, seasonal_1 Loss: 0.0579 | 0.0266
Epoch 244/300, seasonal_1 Loss: 0.0578 | 0.0265
Epoch 245/300, seasonal_1 Loss: 0.0578 | 0.0263
Epoch 246/300, seasonal_1 Loss: 0.0577 | 0.0262
Epoch 247/300, seasonal_1 Loss: 0.0577 | 0.0263
Epoch 248/300, seasonal_1 Loss: 0.0577 | 0.0262
Epoch 249/300, seasonal_1 Loss: 0.0577 | 0.0264
Epoch 250/300, seasonal_1 Loss: 0.0577 | 0.0261
Epoch 251/300, seasonal_1 Loss: 0.0576 | 0.0260
Epoch 252/300, seasonal_1 Loss: 0.0576 | 0.0258
Epoch 253/300, seasonal_1 Loss: 0.0576 | 0.0257
Epoch 254/300, seasonal_1 Loss: 0.0575 | 0.0259
Epoch 255/300, seasonal_1 Loss: 0.0575 | 0.0258
Epoch 256/300, seasonal_1 Loss: 0.0575 | 0.0257
Epoch 257/300, seasonal_1 Loss: 0.0575 | 0.0255
Epoch 258/300, seasonal_1 Loss: 0.0574 | 0.0254
Epoch 259/300, seasonal_1 Loss: 0.0574 | 0.0254
Epoch 260/300, seasonal_1 Loss: 0.0573 | 0.0254
Epoch 261/300, seasonal_1 Loss: 0.0573 | 0.0255
Epoch 262/300, seasonal_1 Loss: 0.0573 | 0.0253
Epoch 263/300, seasonal_1 Loss: 0.0573 | 0.0253
Epoch 264/300, seasonal_1 Loss: 0.0572 | 0.0252
Epoch 265/300, seasonal_1 Loss: 0.0572 | 0.0251
Epoch 266/300, seasonal_1 Loss: 0.0572 | 0.0253
Epoch 267/300, seasonal_1 Loss: 0.0572 | 0.0251
Epoch 268/300, seasonal_1 Loss: 0.0572 | 0.0254
Epoch 269/300, seasonal_1 Loss: 0.0572 | 0.0251
Epoch 270/300, seasonal_1 Loss: 0.0572 | 0.0254
Epoch 271/300, seasonal_1 Loss: 0.0572 | 0.0253
Epoch 272/300, seasonal_1 Loss: 0.0572 | 0.0254
Epoch 273/300, seasonal_1 Loss: 0.0572 | 0.0257
Epoch 274/300, seasonal_1 Loss: 0.0572 | 0.0255
Epoch 275/300, seasonal_1 Loss: 0.0571 | 0.0261
Epoch 276/300, seasonal_1 Loss: 0.0571 | 0.0256
Epoch 277/300, seasonal_1 Loss: 0.0570 | 0.0261
Epoch 278/300, seasonal_1 Loss: 0.0570 | 0.0258
Epoch 279/300, seasonal_1 Loss: 0.0569 | 0.0257
Epoch 280/300, seasonal_1 Loss: 0.0569 | 0.0260
Epoch 281/300, seasonal_1 Loss: 0.0569 | 0.0255
Epoch 282/300, seasonal_1 Loss: 0.0569 | 0.0258
Epoch 283/300, seasonal_1 Loss: 0.0569 | 0.0255
Epoch 284/300, seasonal_1 Loss: 0.0569 | 0.0255
Epoch 285/300, seasonal_1 Loss: 0.0569 | 0.0255
Epoch 286/300, seasonal_1 Loss: 0.0569 | 0.0254
Epoch 287/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 288/300, seasonal_1 Loss: 0.0569 | 0.0254
Epoch 289/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 290/300, seasonal_1 Loss: 0.0569 | 0.0255
Epoch 291/300, seasonal_1 Loss: 0.0569 | 0.0256
Epoch 292/300, seasonal_1 Loss: 0.0568 | 0.0255
Epoch 293/300, seasonal_1 Loss: 0.0568 | 0.0253
Epoch 294/300, seasonal_1 Loss: 0.0567 | 0.0254
Epoch 295/300, seasonal_1 Loss: 0.0567 | 0.0254
Epoch 296/300, seasonal_1 Loss: 0.0567 | 0.0256
Epoch 297/300, seasonal_1 Loss: 0.0566 | 0.0257
Epoch 298/300, seasonal_1 Loss: 0.0565 | 0.0258
Epoch 299/300, seasonal_1 Loss: 0.0564 | 0.0259
Epoch 300/300, seasonal_1 Loss: 0.0564 | 0.0258
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.7809809320208144, 'learning_rate': 0.0006941066410276456, 'batch_size': 188, 'step_size': 3, 'gamma': 0.9758549682493445}
Epoch 1/300, seasonal_2 Loss: 0.5288 | 0.2595
Epoch 2/300, seasonal_2 Loss: 0.1765 | 0.1429
Epoch 3/300, seasonal_2 Loss: 0.1735 | 0.1523
Epoch 4/300, seasonal_2 Loss: 0.1735 | 0.1331
Epoch 5/300, seasonal_2 Loss: 0.1525 | 0.1196
Epoch 6/300, seasonal_2 Loss: 0.1327 | 0.0807
Epoch 7/300, seasonal_2 Loss: 0.1260 | 0.0872
Epoch 8/300, seasonal_2 Loss: 0.1191 | 0.0696
Epoch 9/300, seasonal_2 Loss: 0.1085 | 0.0618
Epoch 10/300, seasonal_2 Loss: 0.1046 | 0.0590
Epoch 11/300, seasonal_2 Loss: 0.1009 | 0.0574
Epoch 12/300, seasonal_2 Loss: 0.1017 | 0.0575
Epoch 13/300, seasonal_2 Loss: 0.1023 | 0.0572
Epoch 14/300, seasonal_2 Loss: 0.1006 | 0.0621
Epoch 15/300, seasonal_2 Loss: 0.0997 | 0.0560
Epoch 16/300, seasonal_2 Loss: 0.0999 | 0.0538
Epoch 17/300, seasonal_2 Loss: 0.1015 | 0.0545
Epoch 18/300, seasonal_2 Loss: 0.1037 | 0.0549
Epoch 19/300, seasonal_2 Loss: 0.1064 | 0.0621
Epoch 20/300, seasonal_2 Loss: 0.1092 | 0.0707
Epoch 21/300, seasonal_2 Loss: 0.1082 | 0.0520
Epoch 22/300, seasonal_2 Loss: 0.1018 | 0.0661
Epoch 23/300, seasonal_2 Loss: 0.1111 | 0.0643
Epoch 24/300, seasonal_2 Loss: 0.1033 | 0.0718
Epoch 25/300, seasonal_2 Loss: 0.1195 | 0.1136
Epoch 26/300, seasonal_2 Loss: 0.1248 | 0.0542
Epoch 27/300, seasonal_2 Loss: 0.1002 | 0.0633
Epoch 28/300, seasonal_2 Loss: 0.0980 | 0.0516
Epoch 29/300, seasonal_2 Loss: 0.0929 | 0.0445
Epoch 30/300, seasonal_2 Loss: 0.0881 | 0.0464
Epoch 31/300, seasonal_2 Loss: 0.0873 | 0.0513
Epoch 32/300, seasonal_2 Loss: 0.0847 | 0.0456
Epoch 33/300, seasonal_2 Loss: 0.0843 | 0.0422
Epoch 34/300, seasonal_2 Loss: 0.0808 | 0.0414
Epoch 35/300, seasonal_2 Loss: 0.0801 | 0.0425
Epoch 36/300, seasonal_2 Loss: 0.0792 | 0.0397
Epoch 37/300, seasonal_2 Loss: 0.0788 | 0.0398
Epoch 38/300, seasonal_2 Loss: 0.0783 | 0.0394
Epoch 39/300, seasonal_2 Loss: 0.0778 | 0.0393
Epoch 40/300, seasonal_2 Loss: 0.0779 | 0.0383
Epoch 41/300, seasonal_2 Loss: 0.0787 | 0.0394
Epoch 42/300, seasonal_2 Loss: 0.0813 | 0.0405
Epoch 43/300, seasonal_2 Loss: 0.0824 | 0.0422
Epoch 44/300, seasonal_2 Loss: 0.0784 | 0.0367
Epoch 45/300, seasonal_2 Loss: 0.0763 | 0.0372
Epoch 46/300, seasonal_2 Loss: 0.0760 | 0.0361
Epoch 47/300, seasonal_2 Loss: 0.0754 | 0.0358
Epoch 48/300, seasonal_2 Loss: 0.0746 | 0.0357
Epoch 49/300, seasonal_2 Loss: 0.0738 | 0.0365
Epoch 50/300, seasonal_2 Loss: 0.0734 | 0.0349
Epoch 51/300, seasonal_2 Loss: 0.0732 | 0.0357
Epoch 52/300, seasonal_2 Loss: 0.0729 | 0.0343
Epoch 53/300, seasonal_2 Loss: 0.0726 | 0.0349
Epoch 54/300, seasonal_2 Loss: 0.0723 | 0.0338
Epoch 55/300, seasonal_2 Loss: 0.0718 | 0.0340
Epoch 56/300, seasonal_2 Loss: 0.0713 | 0.0337
Epoch 57/300, seasonal_2 Loss: 0.0709 | 0.0337
Epoch 58/300, seasonal_2 Loss: 0.0705 | 0.0328
Epoch 59/300, seasonal_2 Loss: 0.0701 | 0.0325
Epoch 60/300, seasonal_2 Loss: 0.0697 | 0.0322
Epoch 61/300, seasonal_2 Loss: 0.0693 | 0.0320
Epoch 62/300, seasonal_2 Loss: 0.0690 | 0.0316
Epoch 63/300, seasonal_2 Loss: 0.0687 | 0.0313
Epoch 64/300, seasonal_2 Loss: 0.0684 | 0.0310
Epoch 65/300, seasonal_2 Loss: 0.0681 | 0.0306
Epoch 66/300, seasonal_2 Loss: 0.0679 | 0.0303
Epoch 67/300, seasonal_2 Loss: 0.0676 | 0.0303
Epoch 68/300, seasonal_2 Loss: 0.0674 | 0.0301
Epoch 69/300, seasonal_2 Loss: 0.0671 | 0.0301
Epoch 70/300, seasonal_2 Loss: 0.0668 | 0.0299
Epoch 71/300, seasonal_2 Loss: 0.0665 | 0.0297
Epoch 72/300, seasonal_2 Loss: 0.0662 | 0.0294
Epoch 73/300, seasonal_2 Loss: 0.0660 | 0.0296
Epoch 74/300, seasonal_2 Loss: 0.0661 | 0.0294
Epoch 75/300, seasonal_2 Loss: 0.0665 | 0.0296
Epoch 76/300, seasonal_2 Loss: 0.0664 | 0.0299
Epoch 77/300, seasonal_2 Loss: 0.0656 | 0.0295
Epoch 78/300, seasonal_2 Loss: 0.0654 | 0.0287
Epoch 79/300, seasonal_2 Loss: 0.0655 | 0.0302
Epoch 80/300, seasonal_2 Loss: 0.0658 | 0.0293
Epoch 81/300, seasonal_2 Loss: 0.0659 | 0.0300
Epoch 82/300, seasonal_2 Loss: 0.0661 | 0.0298
Epoch 83/300, seasonal_2 Loss: 0.0670 | 0.0307
Epoch 84/300, seasonal_2 Loss: 0.0698 | 0.0287
Epoch 85/300, seasonal_2 Loss: 0.0698 | 0.0322
Epoch 86/300, seasonal_2 Loss: 0.0686 | 0.0320
Epoch 87/300, seasonal_2 Loss: 0.0665 | 0.0295
Epoch 88/300, seasonal_2 Loss: 0.0651 | 0.0279
Epoch 89/300, seasonal_2 Loss: 0.0651 | 0.0285
Epoch 90/300, seasonal_2 Loss: 0.0651 | 0.0277
Epoch 91/300, seasonal_2 Loss: 0.0650 | 0.0286
Epoch 92/300, seasonal_2 Loss: 0.0655 | 0.0296
Epoch 93/300, seasonal_2 Loss: 0.0668 | 0.0322
Epoch 94/300, seasonal_2 Loss: 0.0648 | 0.0274
Epoch 95/300, seasonal_2 Loss: 0.0645 | 0.0273
Epoch 96/300, seasonal_2 Loss: 0.0643 | 0.0292
Epoch 97/300, seasonal_2 Loss: 0.0637 | 0.0271
Epoch 98/300, seasonal_2 Loss: 0.0636 | 0.0285
Epoch 99/300, seasonal_2 Loss: 0.0634 | 0.0279
Epoch 100/300, seasonal_2 Loss: 0.0628 | 0.0283
Epoch 101/300, seasonal_2 Loss: 0.0626 | 0.0282
Epoch 102/300, seasonal_2 Loss: 0.0627 | 0.0301
Epoch 103/300, seasonal_2 Loss: 0.0628 | 0.0291
Epoch 104/300, seasonal_2 Loss: 0.0628 | 0.0282
Epoch 105/300, seasonal_2 Loss: 0.0626 | 0.0274
Epoch 106/300, seasonal_2 Loss: 0.0624 | 0.0266
Epoch 107/300, seasonal_2 Loss: 0.0622 | 0.0262
Epoch 108/300, seasonal_2 Loss: 0.0626 | 0.0267
Epoch 109/300, seasonal_2 Loss: 0.0630 | 0.0270
Epoch 110/300, seasonal_2 Loss: 0.0632 | 0.0265
Epoch 111/300, seasonal_2 Loss: 0.0634 | 0.0280
Epoch 112/300, seasonal_2 Loss: 0.0635 | 0.0277
Epoch 113/300, seasonal_2 Loss: 0.0644 | 0.0271
Epoch 114/300, seasonal_2 Loss: 0.0641 | 0.0269
Epoch 115/300, seasonal_2 Loss: 0.0653 | 0.0273
Epoch 116/300, seasonal_2 Loss: 0.0684 | 0.0283
Epoch 117/300, seasonal_2 Loss: 0.0650 | 0.0269
Epoch 118/300, seasonal_2 Loss: 0.0663 | 0.0278
Epoch 119/300, seasonal_2 Loss: 0.0663 | 0.0294
Epoch 120/300, seasonal_2 Loss: 0.0641 | 0.0279
Epoch 121/300, seasonal_2 Loss: 0.0634 | 0.0294
Epoch 122/300, seasonal_2 Loss: 0.0622 | 0.0263
Epoch 123/300, seasonal_2 Loss: 0.0623 | 0.0257
Epoch 124/300, seasonal_2 Loss: 0.0620 | 0.0263
Epoch 125/300, seasonal_2 Loss: 0.0618 | 0.0256
Epoch 126/300, seasonal_2 Loss: 0.0614 | 0.0256
Epoch 127/300, seasonal_2 Loss: 0.0611 | 0.0256
Epoch 128/300, seasonal_2 Loss: 0.0608 | 0.0251
Epoch 129/300, seasonal_2 Loss: 0.0606 | 0.0254
Epoch 130/300, seasonal_2 Loss: 0.0604 | 0.0248
Epoch 131/300, seasonal_2 Loss: 0.0603 | 0.0253
Epoch 132/300, seasonal_2 Loss: 0.0602 | 0.0245
Epoch 133/300, seasonal_2 Loss: 0.0601 | 0.0250
Epoch 134/300, seasonal_2 Loss: 0.0600 | 0.0243
Epoch 135/300, seasonal_2 Loss: 0.0599 | 0.0248
Epoch 136/300, seasonal_2 Loss: 0.0599 | 0.0242
Epoch 137/300, seasonal_2 Loss: 0.0599 | 0.0247
Epoch 138/300, seasonal_2 Loss: 0.0599 | 0.0241
Epoch 139/300, seasonal_2 Loss: 0.0600 | 0.0246
Epoch 140/300, seasonal_2 Loss: 0.0603 | 0.0245
Epoch 141/300, seasonal_2 Loss: 0.0608 | 0.0252
Epoch 142/300, seasonal_2 Loss: 0.0617 | 0.0241
Epoch 143/300, seasonal_2 Loss: 0.0618 | 0.0266
Epoch 144/300, seasonal_2 Loss: 0.0615 | 0.0257
Epoch 145/300, seasonal_2 Loss: 0.0624 | 0.0250
Epoch 146/300, seasonal_2 Loss: 0.0618 | 0.0250
Epoch 147/300, seasonal_2 Loss: 0.0600 | 0.0248
Epoch 148/300, seasonal_2 Loss: 0.0595 | 0.0248
Epoch 149/300, seasonal_2 Loss: 0.0594 | 0.0241
Epoch 150/300, seasonal_2 Loss: 0.0594 | 0.0244
Epoch 151/300, seasonal_2 Loss: 0.0594 | 0.0240
Epoch 152/300, seasonal_2 Loss: 0.0593 | 0.0242
Epoch 153/300, seasonal_2 Loss: 0.0592 | 0.0239
Epoch 154/300, seasonal_2 Loss: 0.0592 | 0.0243
Epoch 155/300, seasonal_2 Loss: 0.0595 | 0.0243
Epoch 156/300, seasonal_2 Loss: 0.0598 | 0.0241
Epoch 157/300, seasonal_2 Loss: 0.0597 | 0.0238
Epoch 158/300, seasonal_2 Loss: 0.0593 | 0.0241
Epoch 159/300, seasonal_2 Loss: 0.0590 | 0.0241
Epoch 160/300, seasonal_2 Loss: 0.0590 | 0.0242
Epoch 161/300, seasonal_2 Loss: 0.0591 | 0.0240
Epoch 162/300, seasonal_2 Loss: 0.0592 | 0.0239
Epoch 163/300, seasonal_2 Loss: 0.0591 | 0.0235
Epoch 164/300, seasonal_2 Loss: 0.0589 | 0.0236
Epoch 165/300, seasonal_2 Loss: 0.0588 | 0.0236
Epoch 166/300, seasonal_2 Loss: 0.0588 | 0.0237
Epoch 167/300, seasonal_2 Loss: 0.0588 | 0.0235
Epoch 168/300, seasonal_2 Loss: 0.0587 | 0.0236
Epoch 169/300, seasonal_2 Loss: 0.0586 | 0.0237
Epoch 170/300, seasonal_2 Loss: 0.0586 | 0.0238
Epoch 171/300, seasonal_2 Loss: 0.0586 | 0.0237
Epoch 172/300, seasonal_2 Loss: 0.0587 | 0.0236
Epoch 173/300, seasonal_2 Loss: 0.0587 | 0.0234
Epoch 174/300, seasonal_2 Loss: 0.0586 | 0.0234
Epoch 175/300, seasonal_2 Loss: 0.0584 | 0.0233
Epoch 176/300, seasonal_2 Loss: 0.0584 | 0.0234
Epoch 177/300, seasonal_2 Loss: 0.0584 | 0.0233
Epoch 178/300, seasonal_2 Loss: 0.0584 | 0.0233
Epoch 179/300, seasonal_2 Loss: 0.0583 | 0.0234
Epoch 180/300, seasonal_2 Loss: 0.0583 | 0.0235
Epoch 181/300, seasonal_2 Loss: 0.0583 | 0.0235
Epoch 182/300, seasonal_2 Loss: 0.0583 | 0.0235
Epoch 183/300, seasonal_2 Loss: 0.0584 | 0.0233
Epoch 184/300, seasonal_2 Loss: 0.0583 | 0.0232
Epoch 185/300, seasonal_2 Loss: 0.0582 | 0.0232
Epoch 186/300, seasonal_2 Loss: 0.0581 | 0.0232
Epoch 187/300, seasonal_2 Loss: 0.0581 | 0.0232
Epoch 188/300, seasonal_2 Loss: 0.0581 | 0.0232
Epoch 189/300, seasonal_2 Loss: 0.0581 | 0.0232
Epoch 190/300, seasonal_2 Loss: 0.0581 | 0.0233
Epoch 191/300, seasonal_2 Loss: 0.0580 | 0.0233
Epoch 192/300, seasonal_2 Loss: 0.0580 | 0.0233
Epoch 193/300, seasonal_2 Loss: 0.0581 | 0.0232
Epoch 194/300, seasonal_2 Loss: 0.0581 | 0.0232
Epoch 195/300, seasonal_2 Loss: 0.0580 | 0.0231
Epoch 196/300, seasonal_2 Loss: 0.0579 | 0.0231
Epoch 197/300, seasonal_2 Loss: 0.0579 | 0.0231
Epoch 198/300, seasonal_2 Loss: 0.0579 | 0.0231
Epoch 199/300, seasonal_2 Loss: 0.0579 | 0.0231
Epoch 200/300, seasonal_2 Loss: 0.0579 | 0.0231
Epoch 201/300, seasonal_2 Loss: 0.0578 | 0.0232
Epoch 202/300, seasonal_2 Loss: 0.0578 | 0.0232
Epoch 203/300, seasonal_2 Loss: 0.0578 | 0.0232
Epoch 204/300, seasonal_2 Loss: 0.0578 | 0.0231
Epoch 205/300, seasonal_2 Loss: 0.0578 | 0.0231
Epoch 206/300, seasonal_2 Loss: 0.0578 | 0.0230
Epoch 207/300, seasonal_2 Loss: 0.0577 | 0.0230
Epoch 208/300, seasonal_2 Loss: 0.0577 | 0.0230
Epoch 209/300, seasonal_2 Loss: 0.0577 | 0.0230
Epoch 210/300, seasonal_2 Loss: 0.0577 | 0.0230
Epoch 211/300, seasonal_2 Loss: 0.0576 | 0.0231
Epoch 212/300, seasonal_2 Loss: 0.0576 | 0.0231
Epoch 213/300, seasonal_2 Loss: 0.0576 | 0.0231
Epoch 214/300, seasonal_2 Loss: 0.0576 | 0.0230
Epoch 215/300, seasonal_2 Loss: 0.0576 | 0.0230
Epoch 216/300, seasonal_2 Loss: 0.0576 | 0.0230
Epoch 217/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 218/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 219/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 220/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 221/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 222/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 223/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 224/300, seasonal_2 Loss: 0.0575 | 0.0230
Epoch 225/300, seasonal_2 Loss: 0.0574 | 0.0230
Epoch 226/300, seasonal_2 Loss: 0.0574 | 0.0229
Epoch 227/300, seasonal_2 Loss: 0.0574 | 0.0230
Epoch 228/300, seasonal_2 Loss: 0.0574 | 0.0229
Epoch 229/300, seasonal_2 Loss: 0.0574 | 0.0230
Epoch 230/300, seasonal_2 Loss: 0.0574 | 0.0229
Epoch 231/300, seasonal_2 Loss: 0.0573 | 0.0230
Epoch 232/300, seasonal_2 Loss: 0.0573 | 0.0229
Epoch 233/300, seasonal_2 Loss: 0.0573 | 0.0230
Epoch 234/300, seasonal_2 Loss: 0.0573 | 0.0229
Epoch 235/300, seasonal_2 Loss: 0.0573 | 0.0229
Epoch 236/300, seasonal_2 Loss: 0.0573 | 0.0229
Epoch 237/300, seasonal_2 Loss: 0.0573 | 0.0229
Epoch 238/300, seasonal_2 Loss: 0.0573 | 0.0229
Epoch 239/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 240/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 241/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 242/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 243/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 244/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 245/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 246/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 247/300, seasonal_2 Loss: 0.0572 | 0.0229
Epoch 248/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 249/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 250/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 251/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 252/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 253/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 254/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 255/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 256/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 257/300, seasonal_2 Loss: 0.0571 | 0.0229
Epoch 258/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 259/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 260/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 261/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 262/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 263/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 264/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 265/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 266/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 267/300, seasonal_2 Loss: 0.0570 | 0.0229
Epoch 268/300, seasonal_2 Loss: 0.0570 | 0.0228
Epoch 269/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 270/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 271/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 272/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 273/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 274/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 275/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 276/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 277/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 278/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 279/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 280/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 281/300, seasonal_2 Loss: 0.0569 | 0.0228
Epoch 282/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 283/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 284/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 285/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 286/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 287/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 288/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 289/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 290/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 291/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 292/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 293/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 294/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 295/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 296/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 297/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 298/300, seasonal_2 Loss: 0.0568 | 0.0228
Epoch 299/300, seasonal_2 Loss: 0.0567 | 0.0228
Epoch 300/300, seasonal_2 Loss: 0.0567 | 0.0228
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.7834365500910144, 'learning_rate': 0.0003969604373374693, 'batch_size': 120, 'step_size': 7, 'gamma': 0.7954024805642811}
Epoch 1/300, seasonal_3 Loss: 0.3167 | 0.2260
Epoch 2/300, seasonal_3 Loss: 0.1688 | 0.2683
Epoch 3/300, seasonal_3 Loss: 0.2150 | 0.1386
Epoch 4/300, seasonal_3 Loss: 0.1548 | 0.1210
Epoch 5/300, seasonal_3 Loss: 0.1392 | 0.1226
Epoch 6/300, seasonal_3 Loss: 0.1570 | 0.1205
Epoch 7/300, seasonal_3 Loss: 0.1379 | 0.0930
Epoch 8/300, seasonal_3 Loss: 0.1200 | 0.0775
Epoch 9/300, seasonal_3 Loss: 0.1120 | 0.0842
Epoch 10/300, seasonal_3 Loss: 0.1150 | 0.0730
Epoch 11/300, seasonal_3 Loss: 0.1076 | 0.0648
Epoch 12/300, seasonal_3 Loss: 0.1005 | 0.0610
Epoch 13/300, seasonal_3 Loss: 0.0997 | 0.0595
Epoch 14/300, seasonal_3 Loss: 0.0990 | 0.0596
Epoch 15/300, seasonal_3 Loss: 0.0981 | 0.0565
Epoch 16/300, seasonal_3 Loss: 0.0971 | 0.0554
Epoch 17/300, seasonal_3 Loss: 0.0959 | 0.0556
Epoch 18/300, seasonal_3 Loss: 0.0949 | 0.0552
Epoch 19/300, seasonal_3 Loss: 0.0936 | 0.0546
Epoch 20/300, seasonal_3 Loss: 0.0925 | 0.0522
Epoch 21/300, seasonal_3 Loss: 0.0911 | 0.0504
Epoch 22/300, seasonal_3 Loss: 0.0900 | 0.0502
Epoch 23/300, seasonal_3 Loss: 0.0893 | 0.0496
Epoch 24/300, seasonal_3 Loss: 0.0887 | 0.0489
Epoch 25/300, seasonal_3 Loss: 0.0882 | 0.0481
Epoch 26/300, seasonal_3 Loss: 0.0876 | 0.0481
Epoch 27/300, seasonal_3 Loss: 0.0874 | 0.0476
Epoch 28/300, seasonal_3 Loss: 0.0870 | 0.0470
Epoch 29/300, seasonal_3 Loss: 0.0866 | 0.0468
Epoch 30/300, seasonal_3 Loss: 0.0862 | 0.0463
Epoch 31/300, seasonal_3 Loss: 0.0856 | 0.0460
Epoch 32/300, seasonal_3 Loss: 0.0850 | 0.0458
Epoch 33/300, seasonal_3 Loss: 0.0845 | 0.0460
Epoch 34/300, seasonal_3 Loss: 0.0841 | 0.0457
Epoch 35/300, seasonal_3 Loss: 0.0838 | 0.0453
Epoch 36/300, seasonal_3 Loss: 0.0833 | 0.0447
Epoch 37/300, seasonal_3 Loss: 0.0828 | 0.0443
Epoch 38/300, seasonal_3 Loss: 0.0824 | 0.0440
Epoch 39/300, seasonal_3 Loss: 0.0821 | 0.0437
Epoch 40/300, seasonal_3 Loss: 0.0818 | 0.0435
Epoch 41/300, seasonal_3 Loss: 0.0816 | 0.0432
Epoch 42/300, seasonal_3 Loss: 0.0814 | 0.0430
Epoch 43/300, seasonal_3 Loss: 0.0811 | 0.0427
Epoch 44/300, seasonal_3 Loss: 0.0809 | 0.0426
Epoch 45/300, seasonal_3 Loss: 0.0808 | 0.0424
Epoch 46/300, seasonal_3 Loss: 0.0806 | 0.0422
Epoch 47/300, seasonal_3 Loss: 0.0804 | 0.0420
Epoch 48/300, seasonal_3 Loss: 0.0803 | 0.0418
Epoch 49/300, seasonal_3 Loss: 0.0801 | 0.0417
Epoch 50/300, seasonal_3 Loss: 0.0800 | 0.0415
Epoch 51/300, seasonal_3 Loss: 0.0799 | 0.0414
Epoch 52/300, seasonal_3 Loss: 0.0798 | 0.0413
Epoch 53/300, seasonal_3 Loss: 0.0797 | 0.0412
Epoch 54/300, seasonal_3 Loss: 0.0796 | 0.0411
Epoch 55/300, seasonal_3 Loss: 0.0795 | 0.0410
Epoch 56/300, seasonal_3 Loss: 0.0794 | 0.0409
Epoch 57/300, seasonal_3 Loss: 0.0793 | 0.0408
Epoch 58/300, seasonal_3 Loss: 0.0793 | 0.0407
Epoch 59/300, seasonal_3 Loss: 0.0792 | 0.0407
Epoch 60/300, seasonal_3 Loss: 0.0791 | 0.0406
Epoch 61/300, seasonal_3 Loss: 0.0791 | 0.0405
Epoch 62/300, seasonal_3 Loss: 0.0790 | 0.0405
Epoch 63/300, seasonal_3 Loss: 0.0790 | 0.0404
Epoch 64/300, seasonal_3 Loss: 0.0789 | 0.0404
Epoch 65/300, seasonal_3 Loss: 0.0789 | 0.0403
Epoch 66/300, seasonal_3 Loss: 0.0788 | 0.0403
Epoch 67/300, seasonal_3 Loss: 0.0788 | 0.0402
Epoch 68/300, seasonal_3 Loss: 0.0787 | 0.0402
Epoch 69/300, seasonal_3 Loss: 0.0787 | 0.0402
Epoch 70/300, seasonal_3 Loss: 0.0787 | 0.0401
Epoch 71/300, seasonal_3 Loss: 0.0787 | 0.0401
Epoch 72/300, seasonal_3 Loss: 0.0786 | 0.0401
Epoch 73/300, seasonal_3 Loss: 0.0786 | 0.0400
Epoch 74/300, seasonal_3 Loss: 0.0786 | 0.0400
Epoch 75/300, seasonal_3 Loss: 0.0785 | 0.0400
Epoch 76/300, seasonal_3 Loss: 0.0785 | 0.0400
Epoch 77/300, seasonal_3 Loss: 0.0785 | 0.0399
Epoch 78/300, seasonal_3 Loss: 0.0785 | 0.0399
Epoch 79/300, seasonal_3 Loss: 0.0785 | 0.0399
Epoch 80/300, seasonal_3 Loss: 0.0785 | 0.0399
Epoch 81/300, seasonal_3 Loss: 0.0784 | 0.0399
Epoch 82/300, seasonal_3 Loss: 0.0784 | 0.0398
Epoch 83/300, seasonal_3 Loss: 0.0784 | 0.0398
Epoch 84/300, seasonal_3 Loss: 0.0784 | 0.0398
Epoch 85/300, seasonal_3 Loss: 0.0784 | 0.0398
Epoch 86/300, seasonal_3 Loss: 0.0784 | 0.0398
Epoch 87/300, seasonal_3 Loss: 0.0784 | 0.0398
Epoch 88/300, seasonal_3 Loss: 0.0783 | 0.0398
Epoch 89/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 90/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 91/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 92/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 93/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 94/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 95/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 96/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 97/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 98/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 99/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 100/300, seasonal_3 Loss: 0.0783 | 0.0397
Epoch 101/300, seasonal_3 Loss: 0.0782 | 0.0397
Epoch 102/300, seasonal_3 Loss: 0.0782 | 0.0397
Epoch 103/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 104/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 105/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 106/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 107/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 108/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 109/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 110/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 111/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 112/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 113/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 114/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 115/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 116/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 117/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 118/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 119/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 120/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 121/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 122/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 123/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 124/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 125/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 126/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 127/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 128/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 129/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 130/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 131/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 132/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 133/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 134/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 135/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 136/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 137/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 138/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 139/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 140/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 141/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 142/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 143/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 144/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 145/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 146/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 147/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 148/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 149/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 150/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 151/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 152/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 153/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 154/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 155/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 156/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 157/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 158/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 159/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 160/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 161/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 162/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 163/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 164/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 165/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 166/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 167/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 168/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 169/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 170/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 171/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 172/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 173/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 174/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 175/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 176/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 177/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 178/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 179/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 180/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 181/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 182/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 183/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 184/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 185/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 186/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 187/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 188/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 189/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 190/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 191/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 192/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 193/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 194/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 195/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 196/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 197/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 198/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 199/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 200/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 201/300, seasonal_3 Loss: 0.0782 | 0.0396
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.8487170940076432, 'learning_rate': 0.000980050418692611, 'batch_size': 81, 'step_size': 7, 'gamma': 0.7899606003514623}
Epoch 1/300, resid Loss: 0.3846 | 0.0995
Epoch 2/300, resid Loss: 0.1237 | 0.0710
Epoch 3/300, resid Loss: 0.1087 | 0.0646
Epoch 4/300, resid Loss: 0.1106 | 0.0676
Epoch 5/300, resid Loss: 0.1037 | 0.0574
Epoch 6/300, resid Loss: 0.0985 | 0.0511
Epoch 7/300, resid Loss: 0.0963 | 0.0490
Epoch 8/300, resid Loss: 0.0922 | 0.0509
Epoch 9/300, resid Loss: 0.0913 | 0.0478
Epoch 10/300, resid Loss: 0.0887 | 0.0477
Epoch 11/300, resid Loss: 0.0881 | 0.0475
Epoch 12/300, resid Loss: 0.0858 | 0.0465
Epoch 13/300, resid Loss: 0.0816 | 0.0418
Epoch 14/300, resid Loss: 0.0782 | 0.0440
Epoch 15/300, resid Loss: 0.0771 | 0.0392
Epoch 16/300, resid Loss: 0.0761 | 0.0394
Epoch 17/300, resid Loss: 0.0751 | 0.0371
Epoch 18/300, resid Loss: 0.0745 | 0.0367
Epoch 19/300, resid Loss: 0.0741 | 0.0360
Epoch 20/300, resid Loss: 0.0741 | 0.0355
Epoch 21/300, resid Loss: 0.0743 | 0.0350
Epoch 22/300, resid Loss: 0.0742 | 0.0369
Epoch 23/300, resid Loss: 0.0750 | 0.0359
Epoch 24/300, resid Loss: 0.0754 | 0.0372
Epoch 25/300, resid Loss: 0.0767 | 0.0397
Epoch 26/300, resid Loss: 0.0765 | 0.0372
Epoch 27/300, resid Loss: 0.0750 | 0.0371
Epoch 28/300, resid Loss: 0.0734 | 0.0370
Epoch 29/300, resid Loss: 0.0726 | 0.0353
Epoch 30/300, resid Loss: 0.0718 | 0.0342
Epoch 31/300, resid Loss: 0.0703 | 0.0340
Epoch 32/300, resid Loss: 0.0693 | 0.0343
Epoch 33/300, resid Loss: 0.0687 | 0.0346
Epoch 34/300, resid Loss: 0.0683 | 0.0354
Epoch 35/300, resid Loss: 0.0680 | 0.0364
Epoch 36/300, resid Loss: 0.0680 | 0.0368
Epoch 37/300, resid Loss: 0.0679 | 0.0352
Epoch 38/300, resid Loss: 0.0672 | 0.0340
Epoch 39/300, resid Loss: 0.0666 | 0.0332
Epoch 40/300, resid Loss: 0.0662 | 0.0325
Epoch 41/300, resid Loss: 0.0659 | 0.0315
Epoch 42/300, resid Loss: 0.0655 | 0.0312
Epoch 43/300, resid Loss: 0.0654 | 0.0308
Epoch 44/300, resid Loss: 0.0652 | 0.0303
Epoch 45/300, resid Loss: 0.0650 | 0.0301
Epoch 46/300, resid Loss: 0.0648 | 0.0299
Epoch 47/300, resid Loss: 0.0648 | 0.0296
Epoch 48/300, resid Loss: 0.0646 | 0.0294
Epoch 49/300, resid Loss: 0.0644 | 0.0293
Epoch 50/300, resid Loss: 0.0643 | 0.0293
Epoch 51/300, resid Loss: 0.0642 | 0.0291
Epoch 52/300, resid Loss: 0.0641 | 0.0291
Epoch 53/300, resid Loss: 0.0640 | 0.0290
Epoch 54/300, resid Loss: 0.0639 | 0.0290
Epoch 55/300, resid Loss: 0.0639 | 0.0289
Epoch 56/300, resid Loss: 0.0638 | 0.0289
Epoch 57/300, resid Loss: 0.0639 | 0.0289
Epoch 58/300, resid Loss: 0.0640 | 0.0288
Epoch 59/300, resid Loss: 0.0639 | 0.0286
Epoch 60/300, resid Loss: 0.0637 | 0.0285
Epoch 61/300, resid Loss: 0.0636 | 0.0284
Epoch 62/300, resid Loss: 0.0634 | 0.0284
Epoch 63/300, resid Loss: 0.0631 | 0.0283
Epoch 64/300, resid Loss: 0.0630 | 0.0283
Epoch 65/300, resid Loss: 0.0629 | 0.0282
Epoch 66/300, resid Loss: 0.0628 | 0.0281
Epoch 67/300, resid Loss: 0.0627 | 0.0281
Epoch 68/300, resid Loss: 0.0626 | 0.0280
Epoch 69/300, resid Loss: 0.0625 | 0.0280
Epoch 70/300, resid Loss: 0.0625 | 0.0279
Epoch 71/300, resid Loss: 0.0624 | 0.0279
Epoch 72/300, resid Loss: 0.0624 | 0.0278
Epoch 73/300, resid Loss: 0.0623 | 0.0278
Epoch 74/300, resid Loss: 0.0623 | 0.0278
Epoch 75/300, resid Loss: 0.0623 | 0.0278
Epoch 76/300, resid Loss: 0.0622 | 0.0277
Epoch 77/300, resid Loss: 0.0622 | 0.0277
Epoch 78/300, resid Loss: 0.0622 | 0.0277
Epoch 79/300, resid Loss: 0.0622 | 0.0277
Epoch 80/300, resid Loss: 0.0621 | 0.0276
Epoch 81/300, resid Loss: 0.0621 | 0.0276
Epoch 82/300, resid Loss: 0.0621 | 0.0276
Epoch 83/300, resid Loss: 0.0621 | 0.0276
Epoch 84/300, resid Loss: 0.0621 | 0.0276
Epoch 85/300, resid Loss: 0.0620 | 0.0276
Epoch 86/300, resid Loss: 0.0620 | 0.0276
Epoch 87/300, resid Loss: 0.0620 | 0.0275
Epoch 88/300, resid Loss: 0.0620 | 0.0275
Epoch 89/300, resid Loss: 0.0620 | 0.0275
Epoch 90/300, resid Loss: 0.0620 | 0.0275
Epoch 91/300, resid Loss: 0.0620 | 0.0275
Epoch 92/300, resid Loss: 0.0620 | 0.0275
Epoch 93/300, resid Loss: 0.0620 | 0.0275
Epoch 94/300, resid Loss: 0.0619 | 0.0275
Epoch 95/300, resid Loss: 0.0619 | 0.0275
Epoch 96/300, resid Loss: 0.0619 | 0.0275
Epoch 97/300, resid Loss: 0.0619 | 0.0275
Epoch 98/300, resid Loss: 0.0619 | 0.0275
Epoch 99/300, resid Loss: 0.0619 | 0.0274
Epoch 100/300, resid Loss: 0.0619 | 0.0274
Epoch 101/300, resid Loss: 0.0619 | 0.0274
Epoch 102/300, resid Loss: 0.0619 | 0.0274
Epoch 103/300, resid Loss: 0.0619 | 0.0274
Epoch 104/300, resid Loss: 0.0619 | 0.0274
Epoch 105/300, resid Loss: 0.0619 | 0.0274
Epoch 106/300, resid Loss: 0.0619 | 0.0274
Epoch 107/300, resid Loss: 0.0619 | 0.0274
Epoch 108/300, resid Loss: 0.0619 | 0.0274
Epoch 109/300, resid Loss: 0.0619 | 0.0274
Epoch 110/300, resid Loss: 0.0619 | 0.0274
Epoch 111/300, resid Loss: 0.0619 | 0.0274
Epoch 112/300, resid Loss: 0.0618 | 0.0274
Epoch 113/300, resid Loss: 0.0618 | 0.0274
Epoch 114/300, resid Loss: 0.0618 | 0.0274
Epoch 115/300, resid Loss: 0.0618 | 0.0274
Epoch 116/300, resid Loss: 0.0618 | 0.0274
Epoch 117/300, resid Loss: 0.0618 | 0.0274
Epoch 118/300, resid Loss: 0.0618 | 0.0274
Epoch 119/300, resid Loss: 0.0618 | 0.0274
Epoch 120/300, resid Loss: 0.0618 | 0.0274
Epoch 121/300, resid Loss: 0.0618 | 0.0274
Epoch 122/300, resid Loss: 0.0618 | 0.0274
Epoch 123/300, resid Loss: 0.0618 | 0.0274
Epoch 124/300, resid Loss: 0.0618 | 0.0274
Epoch 125/300, resid Loss: 0.0618 | 0.0274
Epoch 126/300, resid Loss: 0.0618 | 0.0274
Epoch 127/300, resid Loss: 0.0618 | 0.0274
Epoch 128/300, resid Loss: 0.0618 | 0.0274
Epoch 129/300, resid Loss: 0.0618 | 0.0274
Epoch 130/300, resid Loss: 0.0618 | 0.0274
Epoch 131/300, resid Loss: 0.0618 | 0.0274
Epoch 132/300, resid Loss: 0.0618 | 0.0274
Epoch 133/300, resid Loss: 0.0618 | 0.0274
Epoch 134/300, resid Loss: 0.0618 | 0.0274
Epoch 135/300, resid Loss: 0.0618 | 0.0274
Epoch 136/300, resid Loss: 0.0618 | 0.0274
Epoch 137/300, resid Loss: 0.0618 | 0.0274
Epoch 138/300, resid Loss: 0.0618 | 0.0274
Epoch 139/300, resid Loss: 0.0618 | 0.0274
Epoch 140/300, resid Loss: 0.0618 | 0.0274
Epoch 141/300, resid Loss: 0.0618 | 0.0274
Epoch 142/300, resid Loss: 0.0618 | 0.0274
Epoch 143/300, resid Loss: 0.0618 | 0.0274
Epoch 144/300, resid Loss: 0.0618 | 0.0274
Epoch 145/300, resid Loss: 0.0618 | 0.0274
Epoch 146/300, resid Loss: 0.0618 | 0.0274
Epoch 147/300, resid Loss: 0.0618 | 0.0274
Epoch 148/300, resid Loss: 0.0618 | 0.0274
Epoch 149/300, resid Loss: 0.0618 | 0.0274
Epoch 150/300, resid Loss: 0.0618 | 0.0274
Epoch 151/300, resid Loss: 0.0618 | 0.0274
Epoch 152/300, resid Loss: 0.0618 | 0.0274
Epoch 153/300, resid Loss: 0.0618 | 0.0274
Epoch 154/300, resid Loss: 0.0618 | 0.0274
Epoch 155/300, resid Loss: 0.0618 | 0.0274
Epoch 156/300, resid Loss: 0.0618 | 0.0274
Epoch 157/300, resid Loss: 0.0618 | 0.0274
Epoch 158/300, resid Loss: 0.0618 | 0.0274
Epoch 159/300, resid Loss: 0.0618 | 0.0274
Epoch 160/300, resid Loss: 0.0618 | 0.0274
Epoch 161/300, resid Loss: 0.0618 | 0.0274
Epoch 162/300, resid Loss: 0.0618 | 0.0274
Epoch 163/300, resid Loss: 0.0618 | 0.0274
Epoch 164/300, resid Loss: 0.0618 | 0.0274
Epoch 165/300, resid Loss: 0.0618 | 0.0274
Epoch 166/300, resid Loss: 0.0618 | 0.0274
Epoch 167/300, resid Loss: 0.0618 | 0.0274
Epoch 168/300, resid Loss: 0.0618 | 0.0274
Epoch 169/300, resid Loss: 0.0618 | 0.0274
Epoch 170/300, resid Loss: 0.0618 | 0.0274
Epoch 171/300, resid Loss: 0.0618 | 0.0274
Epoch 172/300, resid Loss: 0.0618 | 0.0274
Epoch 173/300, resid Loss: 0.0618 | 0.0274
Epoch 174/300, resid Loss: 0.0618 | 0.0274
Epoch 175/300, resid Loss: 0.0618 | 0.0274
Epoch 176/300, resid Loss: 0.0618 | 0.0274
Epoch 177/300, resid Loss: 0.0618 | 0.0274
Epoch 178/300, resid Loss: 0.0618 | 0.0274
Epoch 179/300, resid Loss: 0.0618 | 0.0274
Epoch 180/300, resid Loss: 0.0618 | 0.0274
Epoch 181/300, resid Loss: 0.0618 | 0.0274
Epoch 182/300, resid Loss: 0.0618 | 0.0274
Epoch 183/300, resid Loss: 0.0618 | 0.0274
Epoch 184/300, resid Loss: 0.0618 | 0.0274
Epoch 185/300, resid Loss: 0.0618 | 0.0274
Epoch 186/300, resid Loss: 0.0618 | 0.0274
Epoch 187/300, resid Loss: 0.0618 | 0.0274
Epoch 188/300, resid Loss: 0.0618 | 0.0274
Epoch 189/300, resid Loss: 0.0618 | 0.0274
Epoch 190/300, resid Loss: 0.0618 | 0.0274
Epoch 191/300, resid Loss: 0.0618 | 0.0274
Epoch 192/300, resid Loss: 0.0618 | 0.0274
Epoch 193/300, resid Loss: 0.0618 | 0.0274
Epoch 194/300, resid Loss: 0.0618 | 0.0274
Epoch 195/300, resid Loss: 0.0618 | 0.0274
Epoch 196/300, resid Loss: 0.0618 | 0.0274
Epoch 197/300, resid Loss: 0.0618 | 0.0274
Epoch 198/300, resid Loss: 0.0618 | 0.0274
Epoch 199/300, resid Loss: 0.0618 | 0.0274
Epoch 200/300, resid Loss: 0.0618 | 0.0274
Epoch 201/300, resid Loss: 0.0618 | 0.0274
Epoch 202/300, resid Loss: 0.0618 | 0.0274
Epoch 203/300, resid Loss: 0.0618 | 0.0274
Epoch 204/300, resid Loss: 0.0618 | 0.0274
Epoch 205/300, resid Loss: 0.0618 | 0.0274
Epoch 206/300, resid Loss: 0.0618 | 0.0274
Epoch 207/300, resid Loss: 0.0618 | 0.0274
Epoch 208/300, resid Loss: 0.0618 | 0.0274
Epoch 209/300, resid Loss: 0.0618 | 0.0274
Epoch 210/300, resid Loss: 0.0618 | 0.0274
Epoch 211/300, resid Loss: 0.0618 | 0.0274
Epoch 212/300, resid Loss: 0.0618 | 0.0274
Epoch 213/300, resid Loss: 0.0618 | 0.0274
Epoch 214/300, resid Loss: 0.0618 | 0.0274
Epoch 215/300, resid Loss: 0.0618 | 0.0274
Epoch 216/300, resid Loss: 0.0618 | 0.0274
Epoch 217/300, resid Loss: 0.0618 | 0.0274
Epoch 218/300, resid Loss: 0.0618 | 0.0274
Epoch 219/300, resid Loss: 0.0618 | 0.0274
Epoch 220/300, resid Loss: 0.0618 | 0.0274
Epoch 221/300, resid Loss: 0.0618 | 0.0274
Epoch 222/300, resid Loss: 0.0618 | 0.0274
Epoch 223/300, resid Loss: 0.0618 | 0.0274
Epoch 224/300, resid Loss: 0.0618 | 0.0274
Epoch 225/300, resid Loss: 0.0618 | 0.0274
Epoch 226/300, resid Loss: 0.0618 | 0.0274
Early stopping for resid
Runtime (seconds): 715.1852049827576
0.0007921628563370255
[209.87123]
[1.5310036]
[0.3933588]
[6.0874844]
[-0.0791955]
[6.860465]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 105.3836832055822
RMSE: 10.265655517578125
MAE: 10.265655517578125
R-squared: nan
[224.66434]
