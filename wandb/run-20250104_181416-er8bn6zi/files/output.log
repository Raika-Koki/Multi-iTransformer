ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 18:14:17,851][0m A new study created in memory with name: no-name-92511239-15ab-42ca-8ba9-3724066328bb[0m
[32m[I 2025-01-04 18:14:43,597][0m Trial 0 finished with value: 0.1962861870816731 and parameters: {'observation_period_num': 227, 'train_rates': 0.859666741956286, 'learning_rate': 0.00023632096847215852, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8863004629765656}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:15:17,841][0m Trial 1 finished with value: 0.7702713169182424 and parameters: {'observation_period_num': 161, 'train_rates': 0.6787350244280336, 'learning_rate': 3.31570304167803e-06, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8018226766276668}. Best is trial 0 with value: 0.1962861870816731.[0m
Early stopping at epoch 95
[32m[I 2025-01-04 18:16:32,587][0m Trial 2 finished with value: 2.4123885742000404 and parameters: {'observation_period_num': 20, 'train_rates': 0.6249464719518907, 'learning_rate': 1.9319876593709345e-06, 'batch_size': 56, 'step_size': 2, 'gamma': 0.7947055120683445}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:17:05,432][0m Trial 3 finished with value: 0.36737921437638443 and parameters: {'observation_period_num': 133, 'train_rates': 0.756316142831463, 'learning_rate': 6.483277663740107e-05, 'batch_size': 153, 'step_size': 2, 'gamma': 0.8846987930218112}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:17:26,112][0m Trial 4 finished with value: 0.3757854896975914 and parameters: {'observation_period_num': 230, 'train_rates': 0.6241817267570352, 'learning_rate': 0.00010663866782744567, 'batch_size': 217, 'step_size': 9, 'gamma': 0.9751934421359225}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:18:01,272][0m Trial 5 finished with value: 0.23887543020941687 and parameters: {'observation_period_num': 126, 'train_rates': 0.7506405659046202, 'learning_rate': 0.00019129625072452814, 'batch_size': 145, 'step_size': 15, 'gamma': 0.8042578414428894}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:19:18,192][0m Trial 6 finished with value: 0.20695449306436306 and parameters: {'observation_period_num': 130, 'train_rates': 0.6623353537845679, 'learning_rate': 4.1988937296721586e-05, 'batch_size': 57, 'step_size': 9, 'gamma': 0.903805409251607}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:19:45,453][0m Trial 7 finished with value: 0.3247148920222699 and parameters: {'observation_period_num': 109, 'train_rates': 0.6868236945221456, 'learning_rate': 1.4075504844953282e-05, 'batch_size': 191, 'step_size': 11, 'gamma': 0.946132520746438}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:20:13,098][0m Trial 8 finished with value: 0.2105196110642136 and parameters: {'observation_period_num': 130, 'train_rates': 0.6475709865804856, 'learning_rate': 0.00021874814418087004, 'batch_size': 167, 'step_size': 11, 'gamma': 0.7783069307102827}. Best is trial 0 with value: 0.1962861870816731.[0m
[32m[I 2025-01-04 18:21:59,990][0m Trial 9 finished with value: 0.08797531779045643 and parameters: {'observation_period_num': 69, 'train_rates': 0.8804328056165402, 'learning_rate': 0.00035834025301623184, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9352949141822748}. Best is trial 9 with value: 0.08797531779045643.[0m
[32m[I 2025-01-04 18:28:06,353][0m Trial 10 finished with value: 0.02947060708646421 and parameters: {'observation_period_num': 28, 'train_rates': 0.9816470062588084, 'learning_rate': 0.0006885203505168741, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9328294039111883}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:33:47,681][0m Trial 11 finished with value: 0.0436943316587595 and parameters: {'observation_period_num': 25, 'train_rates': 0.9663751213515706, 'learning_rate': 0.0007689257410699405, 'batch_size': 17, 'step_size': 4, 'gamma': 0.9421805888134099}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:37:56,611][0m Trial 12 finished with value: 0.04592447882381881 and parameters: {'observation_period_num': 8, 'train_rates': 0.9861757748564881, 'learning_rate': 0.000827904668247034, 'batch_size': 24, 'step_size': 5, 'gamma': 0.8444373574845339}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:39:03,144][0m Trial 13 finished with value: 0.06259072571992874 and parameters: {'observation_period_num': 56, 'train_rates': 0.984654614707265, 'learning_rate': 0.0009123538554615229, 'batch_size': 93, 'step_size': 5, 'gamma': 0.9831900886813092}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:43:55,262][0m Trial 14 finished with value: 0.06601245692152469 and parameters: {'observation_period_num': 65, 'train_rates': 0.9157747149294269, 'learning_rate': 9.930734639006932e-06, 'batch_size': 19, 'step_size': 5, 'gamma': 0.9266732962115537}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:44:55,305][0m Trial 15 finished with value: 0.04499436172016094 and parameters: {'observation_period_num': 38, 'train_rates': 0.9408790094129414, 'learning_rate': 0.0005131352678994951, 'batch_size': 98, 'step_size': 7, 'gamma': 0.8436907495092566}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:45:50,313][0m Trial 16 finished with value: 0.0738737390812078 and parameters: {'observation_period_num': 87, 'train_rates': 0.8443116028107414, 'learning_rate': 9.887305899785453e-05, 'batch_size': 98, 'step_size': 1, 'gamma': 0.9548096837358129}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:47:03,563][0m Trial 17 finished with value: 0.1258765790279549 and parameters: {'observation_period_num': 178, 'train_rates': 0.9272292594945548, 'learning_rate': 1.9956614008177794e-05, 'batch_size': 77, 'step_size': 4, 'gamma': 0.9116800176784623}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:49:40,983][0m Trial 18 finished with value: 0.04615534300772617 and parameters: {'observation_period_num': 36, 'train_rates': 0.8219477219561506, 'learning_rate': 0.0004620780909024402, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8531364695061306}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:50:30,577][0m Trial 19 finished with value: 0.23186306733834117 and parameters: {'observation_period_num': 88, 'train_rates': 0.953614664803468, 'learning_rate': 5.657118132709445e-06, 'batch_size': 124, 'step_size': 3, 'gamma': 0.9707637558290245}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:50:55,867][0m Trial 20 finished with value: 0.048497515117847605 and parameters: {'observation_period_num': 33, 'train_rates': 0.8960953154438267, 'learning_rate': 0.0008833648196294821, 'batch_size': 250, 'step_size': 6, 'gamma': 0.9128059658899653}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:51:49,526][0m Trial 21 finished with value: 0.047758926048966276 and parameters: {'observation_period_num': 45, 'train_rates': 0.9440561669643502, 'learning_rate': 0.0004548733722749416, 'batch_size': 110, 'step_size': 8, 'gamma': 0.8340622067718507}. Best is trial 10 with value: 0.02947060708646421.[0m
[32m[I 2025-01-04 18:53:13,746][0m Trial 22 finished with value: 0.02901498037503987 and parameters: {'observation_period_num': 11, 'train_rates': 0.9582006760222093, 'learning_rate': 0.0004768183997334518, 'batch_size': 71, 'step_size': 7, 'gamma': 0.8266566200720801}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 18:55:53,989][0m Trial 23 finished with value: 0.044576626271009445 and parameters: {'observation_period_num': 7, 'train_rates': 0.9879842271080886, 'learning_rate': 0.00014374484109015698, 'batch_size': 38, 'step_size': 4, 'gamma': 0.7532357546236385}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 18:57:13,239][0m Trial 24 finished with value: 0.03252250534044394 and parameters: {'observation_period_num': 20, 'train_rates': 0.8972584539296273, 'learning_rate': 0.0003066744041394569, 'batch_size': 73, 'step_size': 6, 'gamma': 0.8228708559339133}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 18:58:31,773][0m Trial 25 finished with value: 0.060289668501354754 and parameters: {'observation_period_num': 81, 'train_rates': 0.9111732401871421, 'learning_rate': 0.0002552716507398352, 'batch_size': 72, 'step_size': 7, 'gamma': 0.8221781106793601}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:00:34,633][0m Trial 26 finished with value: 0.030806226232512432 and parameters: {'observation_period_num': 6, 'train_rates': 0.8808656716925047, 'learning_rate': 5.248701253098535e-05, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8677551410847593}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:02:32,597][0m Trial 27 finished with value: 0.05818473053633744 and parameters: {'observation_period_num': 50, 'train_rates': 0.8058390770924985, 'learning_rate': 3.072641731467125e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8684575960234225}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:04:01,059][0m Trial 28 finished with value: 0.03573884357782928 and parameters: {'observation_period_num': 7, 'train_rates': 0.865735958353322, 'learning_rate': 6.267701936967497e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8624993074100125}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:05:05,251][0m Trial 29 finished with value: 0.08360646293443792 and parameters: {'observation_period_num': 173, 'train_rates': 0.8472707938899421, 'learning_rate': 0.00015056329920987206, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8937466390593941}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:07:17,636][0m Trial 30 finished with value: 0.11401653585006606 and parameters: {'observation_period_num': 249, 'train_rates': 0.9608813993641092, 'learning_rate': 3.2739683514919326e-05, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8787583077198986}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:08:08,045][0m Trial 31 finished with value: 0.036589991859484965 and parameters: {'observation_period_num': 18, 'train_rates': 0.8956965138093405, 'learning_rate': 0.00031498811477576223, 'batch_size': 119, 'step_size': 6, 'gamma': 0.8196219886843255}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:09:29,511][0m Trial 32 finished with value: 0.03699809244523446 and parameters: {'observation_period_num': 22, 'train_rates': 0.8772553982207973, 'learning_rate': 0.0005555287569557169, 'batch_size': 69, 'step_size': 7, 'gamma': 0.8241300337326857}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:12:37,458][0m Trial 33 finished with value: 0.056761879362721944 and parameters: {'observation_period_num': 55, 'train_rates': 0.9309758344110347, 'learning_rate': 0.00026933569366039615, 'batch_size': 30, 'step_size': 10, 'gamma': 0.7879502670481298}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:14:26,294][0m Trial 34 finished with value: 0.034620867340037456 and parameters: {'observation_period_num': 27, 'train_rates': 0.9114683757444078, 'learning_rate': 9.929285181044435e-05, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8590958641965264}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:15:28,175][0m Trial 35 finished with value: 0.6396928578615189 and parameters: {'observation_period_num': 5, 'train_rates': 0.7884932964565456, 'learning_rate': 1.6238697010943535e-06, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8081267860408501}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:21:24,387][0m Trial 36 finished with value: 0.07343309438403915 and parameters: {'observation_period_num': 69, 'train_rates': 0.9647612675684346, 'learning_rate': 5.7857066808112385e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7651043430888663}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:22:49,620][0m Trial 37 finished with value: 0.08946731372221613 and parameters: {'observation_period_num': 105, 'train_rates': 0.8337505841315135, 'learning_rate': 0.0006229096582867061, 'batch_size': 62, 'step_size': 6, 'gamma': 0.8876479666969296}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:24:34,264][0m Trial 38 finished with value: 0.13661330414718792 and parameters: {'observation_period_num': 204, 'train_rates': 0.7785076728722595, 'learning_rate': 0.0001573161325808777, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8753879952710767}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:25:07,946][0m Trial 39 finished with value: 0.33527396246790886 and parameters: {'observation_period_num': 20, 'train_rates': 0.7178326758241549, 'learning_rate': 4.230468660422107e-06, 'batch_size': 161, 'step_size': 10, 'gamma': 0.8374640206118058}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:25:51,960][0m Trial 40 finished with value: 0.07921032811102179 and parameters: {'observation_period_num': 144, 'train_rates': 0.896648533242791, 'learning_rate': 0.00038576212741605145, 'batch_size': 129, 'step_size': 3, 'gamma': 0.8093026963411779}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:27:37,686][0m Trial 41 finished with value: 0.040395924548723705 and parameters: {'observation_period_num': 35, 'train_rates': 0.9182978457993362, 'learning_rate': 9.536329607410808e-05, 'batch_size': 54, 'step_size': 8, 'gamma': 0.8520740065840982}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:30:17,123][0m Trial 42 finished with value: 0.032678956572476985 and parameters: {'observation_period_num': 25, 'train_rates': 0.865592860305257, 'learning_rate': 7.743848618193615e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.8636125186249168}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:33:00,660][0m Trial 43 finished with value: 0.03523318980474424 and parameters: {'observation_period_num': 17, 'train_rates': 0.8634611383308571, 'learning_rate': 4.800143140485606e-05, 'batch_size': 33, 'step_size': 9, 'gamma': 0.795718519988737}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:36:08,655][0m Trial 44 finished with value: 0.054473587751304835 and parameters: {'observation_period_num': 44, 'train_rates': 0.877649435500967, 'learning_rate': 2.2796017385877748e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.8989963475296769}. Best is trial 22 with value: 0.02901498037503987.[0m
[32m[I 2025-01-04 19:37:47,402][0m Trial 45 finished with value: 0.026692780752976736 and parameters: {'observation_period_num': 16, 'train_rates': 0.9746970381277509, 'learning_rate': 0.00023436469191064942, 'batch_size': 62, 'step_size': 5, 'gamma': 0.8298060488040724}. Best is trial 45 with value: 0.026692780752976736.[0m
[32m[I 2025-01-04 19:38:57,397][0m Trial 46 finished with value: 0.04652245342731476 and parameters: {'observation_period_num': 15, 'train_rates': 0.9711027091042667, 'learning_rate': 0.00018377384370918046, 'batch_size': 88, 'step_size': 5, 'gamma': 0.8291800833628926}. Best is trial 45 with value: 0.026692780752976736.[0m
[32m[I 2025-01-04 19:40:22,416][0m Trial 47 finished with value: 0.060491145898898445 and parameters: {'observation_period_num': 59, 'train_rates': 0.9410337753786844, 'learning_rate': 0.00036815959964082745, 'batch_size': 69, 'step_size': 5, 'gamma': 0.7793389788803184}. Best is trial 45 with value: 0.026692780752976736.[0m
[32m[I 2025-01-04 19:41:07,907][0m Trial 48 finished with value: 0.06325168162584305 and parameters: {'observation_period_num': 44, 'train_rates': 0.9704849595197504, 'learning_rate': 0.0006527484588030758, 'batch_size': 139, 'step_size': 4, 'gamma': 0.8126121560934817}. Best is trial 45 with value: 0.026692780752976736.[0m
[32m[I 2025-01-04 19:42:05,931][0m Trial 49 finished with value: 0.04733383024318351 and parameters: {'observation_period_num': 30, 'train_rates': 0.9508020548615765, 'learning_rate': 0.00023733330868921964, 'batch_size': 106, 'step_size': 6, 'gamma': 0.8406126422750053}. Best is trial 45 with value: 0.026692780752976736.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 19:42:05,941][0m A new study created in memory with name: no-name-d5453bdd-1d52-424e-839b-4fcd94c9c837[0m
[32m[I 2025-01-04 19:43:30,351][0m Trial 0 finished with value: 0.09116766071399453 and parameters: {'observation_period_num': 117, 'train_rates': 0.7898057304414756, 'learning_rate': 0.0009504941549870388, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8404036937704367}. Best is trial 0 with value: 0.09116766071399453.[0m
[32m[I 2025-01-04 19:44:00,370][0m Trial 1 finished with value: 0.08268658535207732 and parameters: {'observation_period_num': 36, 'train_rates': 0.7980586449781533, 'learning_rate': 4.177517116969199e-05, 'batch_size': 191, 'step_size': 4, 'gamma': 0.9706624461149337}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:44:22,221][0m Trial 2 finished with value: 0.4517167327099277 and parameters: {'observation_period_num': 103, 'train_rates': 0.7202024832828513, 'learning_rate': 9.733008831876694e-06, 'batch_size': 236, 'step_size': 4, 'gamma': 0.9500491746828252}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:44:56,016][0m Trial 3 finished with value: 0.23797409816552664 and parameters: {'observation_period_num': 182, 'train_rates': 0.6693732172945283, 'learning_rate': 0.00010386207178341813, 'batch_size': 134, 'step_size': 7, 'gamma': 0.9276960650983523}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:49:02,308][0m Trial 4 finished with value: 0.09698702599087806 and parameters: {'observation_period_num': 131, 'train_rates': 0.9744001135284159, 'learning_rate': 0.0001955581426549467, 'batch_size': 23, 'step_size': 13, 'gamma': 0.9110311210687322}. Best is trial 1 with value: 0.08268658535207732.[0m
Early stopping at epoch 64
[32m[I 2025-01-04 19:51:05,938][0m Trial 5 finished with value: 0.1630877884680283 and parameters: {'observation_period_num': 32, 'train_rates': 0.6821597312885217, 'learning_rate': 0.0009982978615037444, 'batch_size': 24, 'step_size': 1, 'gamma': 0.7763435897937704}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:51:41,134][0m Trial 6 finished with value: 0.3075840906240046 and parameters: {'observation_period_num': 181, 'train_rates': 0.7692182754771295, 'learning_rate': 1.699943779619702e-05, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9695635914666705}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:52:30,748][0m Trial 7 finished with value: 0.11550495981338965 and parameters: {'observation_period_num': 44, 'train_rates': 0.8275501479853683, 'learning_rate': 2.346924129842142e-05, 'batch_size': 110, 'step_size': 7, 'gamma': 0.8075725033640514}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:54:05,021][0m Trial 8 finished with value: 0.10916248708963394 and parameters: {'observation_period_num': 251, 'train_rates': 0.9811708698855532, 'learning_rate': 0.0003245615029478209, 'batch_size': 60, 'step_size': 11, 'gamma': 0.7903812297121812}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:55:24,891][0m Trial 9 finished with value: 0.09362736651422204 and parameters: {'observation_period_num': 167, 'train_rates': 0.8938924003063531, 'learning_rate': 0.0007963476803362646, 'batch_size': 68, 'step_size': 1, 'gamma': 0.8782821513365902}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:55:51,216][0m Trial 10 finished with value: 0.5912198251799533 and parameters: {'observation_period_num': 68, 'train_rates': 0.8551399525993686, 'learning_rate': 1.4458617300684797e-06, 'batch_size': 234, 'step_size': 4, 'gamma': 0.9865670586669487}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:56:23,968][0m Trial 11 finished with value: 0.5516871540103017 and parameters: {'observation_period_num': 10, 'train_rates': 0.777110246824378, 'learning_rate': 2.3517358050892803e-06, 'batch_size': 166, 'step_size': 15, 'gamma': 0.8454997656164953}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:56:46,988][0m Trial 12 finished with value: 0.22426022290659706 and parameters: {'observation_period_num': 87, 'train_rates': 0.603116747105215, 'learning_rate': 6.262379168876354e-05, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8458494609255359}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:57:16,731][0m Trial 13 finished with value: 0.49722517753133966 and parameters: {'observation_period_num': 120, 'train_rates': 0.8964365604606, 'learning_rate': 4.765046271267329e-06, 'batch_size': 201, 'step_size': 4, 'gamma': 0.8933832502246839}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:58:05,071][0m Trial 14 finished with value: 0.3745474326733824 and parameters: {'observation_period_num': 237, 'train_rates': 0.7508240880482591, 'learning_rate': 4.910784307358526e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.7510946315491009}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:58:33,031][0m Trial 15 finished with value: 0.08357180092585226 and parameters: {'observation_period_num': 147, 'train_rates': 0.8169172742880076, 'learning_rate': 0.00034636835369383193, 'batch_size': 192, 'step_size': 6, 'gamma': 0.8314106148912656}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:59:02,838][0m Trial 16 finished with value: 0.09245413987436027 and parameters: {'observation_period_num': 151, 'train_rates': 0.8474877928092556, 'learning_rate': 0.00027498065441554955, 'batch_size': 197, 'step_size': 5, 'gamma': 0.8140618067276836}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 19:59:35,954][0m Trial 17 finished with value: 0.10020324136777843 and parameters: {'observation_period_num': 210, 'train_rates': 0.9189279388905706, 'learning_rate': 0.00012909586296518515, 'batch_size': 175, 'step_size': 6, 'gamma': 0.866488947881128}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 20:00:00,189][0m Trial 18 finished with value: 0.3079845490261941 and parameters: {'observation_period_num': 83, 'train_rates': 0.8200488247314273, 'learning_rate': 1.1915483460787283e-05, 'batch_size': 248, 'step_size': 3, 'gamma': 0.9356014653979963}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 20:00:24,576][0m Trial 19 finished with value: 0.3091525336941957 and parameters: {'observation_period_num': 55, 'train_rates': 0.7333297333711383, 'learning_rate': 4.451034158168457e-05, 'batch_size': 218, 'step_size': 2, 'gamma': 0.9065758102056082}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 20:01:02,959][0m Trial 20 finished with value: 0.141655493758708 and parameters: {'observation_period_num': 14, 'train_rates': 0.6874645283225047, 'learning_rate': 0.00042632792797728117, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8206158985593323}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 20:02:01,000][0m Trial 21 finished with value: 0.13042827242094537 and parameters: {'observation_period_num': 138, 'train_rates': 0.8039097717916119, 'learning_rate': 0.0005507193234585824, 'batch_size': 88, 'step_size': 11, 'gamma': 0.8402065535201405}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 20:02:33,366][0m Trial 22 finished with value: 0.08827153615139682 and parameters: {'observation_period_num': 107, 'train_rates': 0.7964227632572585, 'learning_rate': 0.0001520688248082714, 'batch_size': 169, 'step_size': 6, 'gamma': 0.8606007010793948}. Best is trial 1 with value: 0.08268658535207732.[0m
[32m[I 2025-01-04 20:03:07,708][0m Trial 23 finished with value: 0.07996431697424644 and parameters: {'observation_period_num': 97, 'train_rates': 0.8719131738768473, 'learning_rate': 0.00010620095313913062, 'batch_size': 170, 'step_size': 6, 'gamma': 0.8702718490729318}. Best is trial 23 with value: 0.07996431697424644.[0m
[32m[I 2025-01-04 20:03:34,825][0m Trial 24 finished with value: 0.07992580214825769 and parameters: {'observation_period_num': 65, 'train_rates': 0.8673461433650574, 'learning_rate': 8.498276734357545e-05, 'batch_size': 218, 'step_size': 5, 'gamma': 0.8847967077804498}. Best is trial 24 with value: 0.07992580214825769.[0m
[32m[I 2025-01-04 20:04:03,815][0m Trial 25 finished with value: 0.10608665658607742 and parameters: {'observation_period_num': 68, 'train_rates': 0.9236445663634087, 'learning_rate': 7.024515812628002e-05, 'batch_size': 214, 'step_size': 3, 'gamma': 0.888926525439921}. Best is trial 24 with value: 0.07992580214825769.[0m
[32m[I 2025-01-04 20:04:41,879][0m Trial 26 finished with value: 0.060147847279463665 and parameters: {'observation_period_num': 31, 'train_rates': 0.8619276349952731, 'learning_rate': 2.8533066277341782e-05, 'batch_size': 152, 'step_size': 5, 'gamma': 0.9580449871355415}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:05:19,669][0m Trial 27 finished with value: 0.08369790344703489 and parameters: {'observation_period_num': 87, 'train_rates': 0.8713106116713618, 'learning_rate': 2.7241179701407853e-05, 'batch_size': 153, 'step_size': 8, 'gamma': 0.9202035616032455}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:06:00,684][0m Trial 28 finished with value: 0.0724858542283376 and parameters: {'observation_period_num': 59, 'train_rates': 0.9471905768139056, 'learning_rate': 9.476803486023176e-05, 'batch_size': 150, 'step_size': 5, 'gamma': 0.9475067133694276}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:06:51,913][0m Trial 29 finished with value: 0.13636248769431278 and parameters: {'observation_period_num': 25, 'train_rates': 0.9507794942114481, 'learning_rate': 8.504026574120183e-06, 'batch_size': 117, 'step_size': 5, 'gamma': 0.9520624530712994}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:07:33,233][0m Trial 30 finished with value: 0.07506983975569408 and parameters: {'observation_period_num': 52, 'train_rates': 0.9423936078307205, 'learning_rate': 7.690327363459162e-05, 'batch_size': 148, 'step_size': 2, 'gamma': 0.9435567153121807}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:08:15,093][0m Trial 31 finished with value: 0.07442730186241013 and parameters: {'observation_period_num': 57, 'train_rates': 0.9373941814410927, 'learning_rate': 8.021895344604512e-05, 'batch_size': 143, 'step_size': 2, 'gamma': 0.9405120281287491}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:08:56,438][0m Trial 32 finished with value: 0.07926534861326218 and parameters: {'observation_period_num': 47, 'train_rates': 0.9493042052080511, 'learning_rate': 3.760445409951463e-05, 'batch_size': 149, 'step_size': 2, 'gamma': 0.9518504433722631}. Best is trial 26 with value: 0.060147847279463665.[0m
[32m[I 2025-01-04 20:09:44,235][0m Trial 33 finished with value: 0.048609761591953564 and parameters: {'observation_period_num': 23, 'train_rates': 0.9461912980415417, 'learning_rate': 0.00016302767335106582, 'batch_size': 130, 'step_size': 2, 'gamma': 0.9706115447922157}. Best is trial 33 with value: 0.048609761591953564.[0m
[32m[I 2025-01-04 20:10:30,298][0m Trial 34 finished with value: 0.04799266434029529 and parameters: {'observation_period_num': 28, 'train_rates': 0.9028262508704182, 'learning_rate': 0.00018271733983991656, 'batch_size': 126, 'step_size': 1, 'gamma': 0.9668953008597574}. Best is trial 34 with value: 0.04799266434029529.[0m
[32m[I 2025-01-04 20:11:22,256][0m Trial 35 finished with value: 0.04035014659166336 and parameters: {'observation_period_num': 21, 'train_rates': 0.9673127899158217, 'learning_rate': 0.0001912584583862285, 'batch_size': 120, 'step_size': 3, 'gamma': 0.9692806721085312}. Best is trial 35 with value: 0.04035014659166336.[0m
[32m[I 2025-01-04 20:12:10,758][0m Trial 36 finished with value: 0.033966095857449356 and parameters: {'observation_period_num': 5, 'train_rates': 0.9007871682128195, 'learning_rate': 0.00020777161845908666, 'batch_size': 126, 'step_size': 2, 'gamma': 0.989757249106557}. Best is trial 36 with value: 0.033966095857449356.[0m
[32m[I 2025-01-04 20:13:03,337][0m Trial 37 finished with value: 0.04926728829741478 and parameters: {'observation_period_num': 7, 'train_rates': 0.988378978212258, 'learning_rate': 0.00020234298529015707, 'batch_size': 125, 'step_size': 1, 'gamma': 0.9834206616760017}. Best is trial 36 with value: 0.033966095857449356.[0m
[32m[I 2025-01-04 20:14:15,720][0m Trial 38 finished with value: 0.03341812161462648 and parameters: {'observation_period_num': 23, 'train_rates': 0.9046294586466822, 'learning_rate': 0.00020396177069751287, 'batch_size': 80, 'step_size': 3, 'gamma': 0.970784245825082}. Best is trial 38 with value: 0.03341812161462648.[0m
[32m[I 2025-01-04 20:15:23,070][0m Trial 39 finished with value: 0.04623304942951483 and parameters: {'observation_period_num': 35, 'train_rates': 0.9010497628481645, 'learning_rate': 0.0002332074440033886, 'batch_size': 85, 'step_size': 3, 'gamma': 0.9768774196659953}. Best is trial 38 with value: 0.03341812161462648.[0m
[32m[I 2025-01-04 20:16:39,030][0m Trial 40 finished with value: 0.07556447156527069 and parameters: {'observation_period_num': 41, 'train_rates': 0.9695285301763757, 'learning_rate': 0.0006296938983644096, 'batch_size': 80, 'step_size': 3, 'gamma': 0.9871663911734218}. Best is trial 38 with value: 0.03341812161462648.[0m
[32m[I 2025-01-04 20:18:43,334][0m Trial 41 finished with value: 0.032770832608825656 and parameters: {'observation_period_num': 22, 'train_rates': 0.9205944169624524, 'learning_rate': 0.00023389398818654124, 'batch_size': 46, 'step_size': 1, 'gamma': 0.9663087611318047}. Best is trial 41 with value: 0.032770832608825656.[0m
[32m[I 2025-01-04 20:20:59,404][0m Trial 42 finished with value: 0.03601738760197485 and parameters: {'observation_period_num': 16, 'train_rates': 0.9118432777826457, 'learning_rate': 0.0002519118038985969, 'batch_size': 42, 'step_size': 3, 'gamma': 0.9784986967433945}. Best is trial 41 with value: 0.032770832608825656.[0m
[32m[I 2025-01-04 20:23:18,604][0m Trial 43 finished with value: 0.03889618635329665 and parameters: {'observation_period_num': 6, 'train_rates': 0.9669216556540162, 'learning_rate': 0.00047874943312644546, 'batch_size': 43, 'step_size': 4, 'gamma': 0.9654419430333131}. Best is trial 41 with value: 0.032770832608825656.[0m
[32m[I 2025-01-04 20:25:52,052][0m Trial 44 finished with value: 0.034490929856172456 and parameters: {'observation_period_num': 14, 'train_rates': 0.9160183278059679, 'learning_rate': 0.0005033226967755454, 'batch_size': 37, 'step_size': 4, 'gamma': 0.9616834626448048}. Best is trial 41 with value: 0.032770832608825656.[0m
[32m[I 2025-01-04 20:28:08,726][0m Trial 45 finished with value: 0.02804566066024641 and parameters: {'observation_period_num': 14, 'train_rates': 0.9208349152267251, 'learning_rate': 0.000745373184793514, 'batch_size': 42, 'step_size': 1, 'gamma': 0.9271526289858901}. Best is trial 45 with value: 0.02804566066024641.[0m
[32m[I 2025-01-04 20:33:51,066][0m Trial 46 finished with value: 0.031088083721043772 and parameters: {'observation_period_num': 5, 'train_rates': 0.8840456215747082, 'learning_rate': 0.000810594750688268, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9177624969297115}. Best is trial 45 with value: 0.02804566066024641.[0m
[32m[I 2025-01-04 20:38:35,933][0m Trial 47 finished with value: 0.04453694225131404 and parameters: {'observation_period_num': 36, 'train_rates': 0.8845209004197823, 'learning_rate': 0.0006785626460370503, 'batch_size': 19, 'step_size': 1, 'gamma': 0.929663234692112}. Best is trial 45 with value: 0.02804566066024641.[0m
[32m[I 2025-01-04 20:40:05,662][0m Trial 48 finished with value: 0.05485424631580109 and parameters: {'observation_period_num': 43, 'train_rates': 0.8468509498763368, 'learning_rate': 0.0008291932293644839, 'batch_size': 61, 'step_size': 1, 'gamma': 0.9141555147359516}. Best is trial 45 with value: 0.02804566066024641.[0m
[32m[I 2025-01-04 20:43:16,669][0m Trial 49 finished with value: 0.027985783562283307 and parameters: {'observation_period_num': 5, 'train_rates': 0.8849797772061458, 'learning_rate': 0.00033929423154687864, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9029610260108089}. Best is trial 49 with value: 0.027985783562283307.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 20:43:16,680][0m A new study created in memory with name: no-name-785c2386-b39b-4e94-9825-d92f4072c533[0m
[32m[I 2025-01-04 20:43:37,807][0m Trial 0 finished with value: 0.25719140325273787 and parameters: {'observation_period_num': 199, 'train_rates': 0.6187728207157336, 'learning_rate': 0.0003674293962284414, 'batch_size': 228, 'step_size': 6, 'gamma': 0.902923577807868}. Best is trial 0 with value: 0.25719140325273787.[0m
[32m[I 2025-01-04 20:47:48,332][0m Trial 1 finished with value: 0.16309055361631966 and parameters: {'observation_period_num': 12, 'train_rates': 0.7227001883102959, 'learning_rate': 2.736777560526363e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.7632810455882437}. Best is trial 1 with value: 0.16309055361631966.[0m
[32m[I 2025-01-04 20:48:46,405][0m Trial 2 finished with value: 1.0059245489176036 and parameters: {'observation_period_num': 66, 'train_rates': 0.9288624253534574, 'learning_rate': 1.688715123026583e-06, 'batch_size': 101, 'step_size': 1, 'gamma': 0.9348412698538623}. Best is trial 1 with value: 0.16309055361631966.[0m
[32m[I 2025-01-04 20:49:57,039][0m Trial 3 finished with value: 0.14422115225058335 and parameters: {'observation_period_num': 26, 'train_rates': 0.9690869473797291, 'learning_rate': 9.66007992900902e-06, 'batch_size': 86, 'step_size': 9, 'gamma': 0.7569098707873666}. Best is trial 3 with value: 0.14422115225058335.[0m
[32m[I 2025-01-04 20:50:21,184][0m Trial 4 finished with value: 0.04746596324012468 and parameters: {'observation_period_num': 17, 'train_rates': 0.7931761543077658, 'learning_rate': 0.0002357677588528414, 'batch_size': 252, 'step_size': 8, 'gamma': 0.8667672358750952}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:55:07,418][0m Trial 5 finished with value: 0.18802054063903206 and parameters: {'observation_period_num': 46, 'train_rates': 0.7036922564986421, 'learning_rate': 8.847360464593894e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9465509823174735}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:56:10,064][0m Trial 6 finished with value: 0.2050688365437337 and parameters: {'observation_period_num': 10, 'train_rates': 0.7434828134530146, 'learning_rate': 7.725898306184494e-06, 'batch_size': 84, 'step_size': 9, 'gamma': 0.9405996103057632}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:56:35,975][0m Trial 7 finished with value: 0.2942590071528002 and parameters: {'observation_period_num': 241, 'train_rates': 0.7801969933450604, 'learning_rate': 1.73655706164047e-05, 'batch_size': 211, 'step_size': 8, 'gamma': 0.8285690662732522}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:56:57,753][0m Trial 8 finished with value: 0.8232458032581998 and parameters: {'observation_period_num': 239, 'train_rates': 0.7436228485347942, 'learning_rate': 3.306729922969751e-06, 'batch_size': 233, 'step_size': 6, 'gamma': 0.7982706276044765}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:57:20,852][0m Trial 9 finished with value: 0.39578981112550804 and parameters: {'observation_period_num': 222, 'train_rates': 0.6046791598465185, 'learning_rate': 6.138608541650167e-05, 'batch_size': 188, 'step_size': 14, 'gamma': 0.9671442793730372}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:57:56,963][0m Trial 10 finished with value: 0.08514846560707234 and parameters: {'observation_period_num': 116, 'train_rates': 0.8704673224331934, 'learning_rate': 0.0008774967394019384, 'batch_size': 159, 'step_size': 13, 'gamma': 0.8539904709734983}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:58:32,960][0m Trial 11 finished with value: 0.07470873103422278 and parameters: {'observation_period_num': 107, 'train_rates': 0.8509911096960068, 'learning_rate': 0.0007494227782113461, 'batch_size': 156, 'step_size': 13, 'gamma': 0.8698821923241133}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:59:09,324][0m Trial 12 finished with value: 0.06335439714461355 and parameters: {'observation_period_num': 111, 'train_rates': 0.8498629328258472, 'learning_rate': 0.00022409596657580272, 'batch_size': 155, 'step_size': 12, 'gamma': 0.8889357952453478}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 20:59:32,426][0m Trial 13 finished with value: 0.12113325495819946 and parameters: {'observation_period_num': 161, 'train_rates': 0.8464474870330341, 'learning_rate': 0.00019564470751976532, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9000131867578581}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 21:00:15,529][0m Trial 14 finished with value: 0.06317174502266378 and parameters: {'observation_period_num': 85, 'train_rates': 0.8156536593191714, 'learning_rate': 0.00015876544792098926, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8890454757108152}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 21:01:01,509][0m Trial 15 finished with value: 0.07403950826121881 and parameters: {'observation_period_num': 67, 'train_rates': 0.7970177307203125, 'learning_rate': 7.630286662695565e-05, 'batch_size': 119, 'step_size': 10, 'gamma': 0.8297028297358062}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 21:02:17,588][0m Trial 16 finished with value: 0.17471705423668027 and parameters: {'observation_period_num': 81, 'train_rates': 0.6714188309766345, 'learning_rate': 0.00014789346426223143, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8483858546314176}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 21:02:45,618][0m Trial 17 finished with value: 0.09524419614156648 and parameters: {'observation_period_num': 161, 'train_rates': 0.7974306256823595, 'learning_rate': 0.0003276288510921272, 'batch_size': 188, 'step_size': 3, 'gamma': 0.9164249052782728}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 21:03:34,131][0m Trial 18 finished with value: 0.060895328455315104 and parameters: {'observation_period_num': 46, 'train_rates': 0.908301485107532, 'learning_rate': 4.7250111485847494e-05, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9811584899950422}. Best is trial 4 with value: 0.04746596324012468.[0m
[32m[I 2025-01-04 21:05:12,969][0m Trial 19 finished with value: 0.046136940217666546 and parameters: {'observation_period_num': 39, 'train_rates': 0.9182407923010727, 'learning_rate': 4.233843584971611e-05, 'batch_size': 58, 'step_size': 5, 'gamma': 0.9844115494195008}. Best is trial 19 with value: 0.046136940217666546.[0m
[32m[I 2025-01-04 21:07:05,124][0m Trial 20 finished with value: 0.09723706888173943 and parameters: {'observation_period_num': 51, 'train_rates': 0.9770837451323853, 'learning_rate': 2.8743498434260515e-05, 'batch_size': 54, 'step_size': 4, 'gamma': 0.7935847511429289}. Best is trial 19 with value: 0.046136940217666546.[0m
[32m[I 2025-01-04 21:09:13,467][0m Trial 21 finished with value: 0.04679506434046704 and parameters: {'observation_period_num': 36, 'train_rates': 0.9055944644200394, 'learning_rate': 4.7618162465644965e-05, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9892049152703813}. Best is trial 19 with value: 0.046136940217666546.[0m
[32m[I 2025-01-04 21:11:25,327][0m Trial 22 finished with value: 0.05255340039730072 and parameters: {'observation_period_num': 30, 'train_rates': 0.910297771175228, 'learning_rate': 1.5035111663368994e-05, 'batch_size': 43, 'step_size': 5, 'gamma': 0.9834757219955983}. Best is trial 19 with value: 0.046136940217666546.[0m
[32m[I 2025-01-04 21:14:12,135][0m Trial 23 finished with value: 0.028012618930502373 and parameters: {'observation_period_num': 9, 'train_rates': 0.9403743313311494, 'learning_rate': 9.70008209197557e-05, 'batch_size': 35, 'step_size': 7, 'gamma': 0.9633591904986404}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:16:52,716][0m Trial 24 finished with value: 0.05305509814980683 and parameters: {'observation_period_num': 38, 'train_rates': 0.9461539821056884, 'learning_rate': 4.139970755379417e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.961075063367422}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:18:10,615][0m Trial 25 finished with value: 0.1304286633800847 and parameters: {'observation_period_num': 155, 'train_rates': 0.8890298400289994, 'learning_rate': 8.710631486192625e-05, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9860714812692349}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:20:44,786][0m Trial 26 finished with value: 0.06756425075278692 and parameters: {'observation_period_num': 66, 'train_rates': 0.943626869201008, 'learning_rate': 1.9634703468180658e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.9615991813181961}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:22:03,918][0m Trial 27 finished with value: 0.0702224525209433 and parameters: {'observation_period_num': 5, 'train_rates': 0.8939752834415812, 'learning_rate': 5.69432292024647e-06, 'batch_size': 72, 'step_size': 7, 'gamma': 0.9253951233174411}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:25:38,669][0m Trial 28 finished with value: 0.06654125895906002 and parameters: {'observation_period_num': 92, 'train_rates': 0.9836936321200633, 'learning_rate': 0.00011868569707314642, 'batch_size': 27, 'step_size': 5, 'gamma': 0.9526458144184392}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:26:36,378][0m Trial 29 finished with value: 0.08967971056699753 and parameters: {'observation_period_num': 135, 'train_rates': 0.9459881381217831, 'learning_rate': 4.409515723809997e-05, 'batch_size': 102, 'step_size': 7, 'gamma': 0.989445772222767}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:28:19,388][0m Trial 30 finished with value: 0.08792633574345324 and parameters: {'observation_period_num': 196, 'train_rates': 0.9267990203769646, 'learning_rate': 2.7256354954181934e-05, 'batch_size': 53, 'step_size': 5, 'gamma': 0.9679692802667557}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:29:43,128][0m Trial 31 finished with value: 0.1098531373903494 and parameters: {'observation_period_num': 25, 'train_rates': 0.8798539150462725, 'learning_rate': 0.0005092181979476093, 'batch_size': 67, 'step_size': 8, 'gamma': 0.9143748255852452}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:31:30,971][0m Trial 32 finished with value: 0.14542619623243808 and parameters: {'observation_period_num': 21, 'train_rates': 0.6523449671393358, 'learning_rate': 0.00038666986644566345, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8762112182995985}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:32:30,556][0m Trial 33 finished with value: 0.06182189500761892 and parameters: {'observation_period_num': 57, 'train_rates': 0.8279031850815157, 'learning_rate': 0.00027118092974730116, 'batch_size': 91, 'step_size': 8, 'gamma': 0.9731412916214379}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:35:56,812][0m Trial 34 finished with value: 0.1658038916184178 and parameters: {'observation_period_num': 11, 'train_rates': 0.7585521245673046, 'learning_rate': 0.00011240731244521428, 'batch_size': 24, 'step_size': 6, 'gamma': 0.9303044294595585}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:36:52,855][0m Trial 35 finished with value: 0.07059685872948689 and parameters: {'observation_period_num': 35, 'train_rates': 0.9607498852567544, 'learning_rate': 5.947354093196464e-05, 'batch_size': 108, 'step_size': 7, 'gamma': 0.9514864815921227}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:37:36,325][0m Trial 36 finished with value: 0.04066658283719856 and parameters: {'observation_period_num': 6, 'train_rates': 0.9094313668255553, 'learning_rate': 0.000516977796013107, 'batch_size': 141, 'step_size': 1, 'gamma': 0.9392806660061223}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:38:17,996][0m Trial 37 finished with value: 0.040938721193621554 and parameters: {'observation_period_num': 6, 'train_rates': 0.9185976744294114, 'learning_rate': 0.0005350616170798306, 'batch_size': 145, 'step_size': 1, 'gamma': 0.94485037179978}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:39:01,405][0m Trial 38 finished with value: 0.04918899974297909 and parameters: {'observation_period_num': 18, 'train_rates': 0.9286170280063232, 'learning_rate': 0.00045207589999163, 'batch_size': 137, 'step_size': 1, 'gamma': 0.940001717481145}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:39:35,409][0m Trial 39 finished with value: 0.04149465545258941 and parameters: {'observation_period_num': 8, 'train_rates': 0.8724477112339011, 'learning_rate': 0.0006764654212395508, 'batch_size': 175, 'step_size': 1, 'gamma': 0.9492783847386087}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:40:09,160][0m Trial 40 finished with value: 0.05072732180044913 and parameters: {'observation_period_num': 14, 'train_rates': 0.8701794564412131, 'learning_rate': 0.0006728325003414731, 'batch_size': 182, 'step_size': 1, 'gamma': 0.9166875221892584}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:40:46,310][0m Trial 41 finished with value: 0.03751089423894882 and parameters: {'observation_period_num': 7, 'train_rates': 0.9592653458912863, 'learning_rate': 0.0005305860551272705, 'batch_size': 171, 'step_size': 2, 'gamma': 0.9467458375620873}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:41:23,160][0m Trial 42 finished with value: 0.03486143425107002 and parameters: {'observation_period_num': 5, 'train_rates': 0.9587839174371431, 'learning_rate': 0.0009984286178553912, 'batch_size': 173, 'step_size': 2, 'gamma': 0.9454555011493666}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:42:02,338][0m Trial 43 finished with value: 0.04793858528137207 and parameters: {'observation_period_num': 24, 'train_rates': 0.9890919813281972, 'learning_rate': 0.0008972237347021173, 'batch_size': 167, 'step_size': 2, 'gamma': 0.9274958490887594}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:42:35,330][0m Trial 44 finished with value: 0.04290769249200821 and parameters: {'observation_period_num': 7, 'train_rates': 0.9662909581672621, 'learning_rate': 0.0005380584035635069, 'batch_size': 203, 'step_size': 2, 'gamma': 0.9367348475454131}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:43:19,104][0m Trial 45 finished with value: 0.047323036938905716 and parameters: {'observation_period_num': 24, 'train_rates': 0.9541252287724493, 'learning_rate': 0.0003215146091996906, 'batch_size': 143, 'step_size': 3, 'gamma': 0.9020792634488511}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:44:00,942][0m Trial 46 finished with value: 0.05342065534748874 and parameters: {'observation_period_num': 55, 'train_rates': 0.9372894452867232, 'learning_rate': 0.000575531852687604, 'batch_size': 146, 'step_size': 3, 'gamma': 0.957400276203985}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:44:34,108][0m Trial 47 finished with value: 0.03808950260281563 and parameters: {'observation_period_num': 20, 'train_rates': 0.9666500486038553, 'learning_rate': 0.0009307076968797473, 'batch_size': 200, 'step_size': 2, 'gamma': 0.943705283265619}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:45:03,162][0m Trial 48 finished with value: 0.0736851841211319 and parameters: {'observation_period_num': 65, 'train_rates': 0.9683780817968448, 'learning_rate': 0.000963797582829828, 'batch_size': 223, 'step_size': 2, 'gamma': 0.9728973364167752}. Best is trial 23 with value: 0.028012618930502373.[0m
[32m[I 2025-01-04 21:45:36,158][0m Trial 49 finished with value: 0.07844412326812744 and parameters: {'observation_period_num': 45, 'train_rates': 0.9897338658111262, 'learning_rate': 0.0001858313671899151, 'batch_size': 204, 'step_size': 3, 'gamma': 0.9113026644771622}. Best is trial 23 with value: 0.028012618930502373.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 21:45:36,168][0m A new study created in memory with name: no-name-bb6cfb0a-908e-4747-ac82-35de75728451[0m
[32m[I 2025-01-04 21:48:45,631][0m Trial 0 finished with value: 0.11410586370362176 and parameters: {'observation_period_num': 250, 'train_rates': 0.9404061270220353, 'learning_rate': 0.0003116495295894832, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8259634520242479}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 21:50:51,023][0m Trial 1 finished with value: 0.12878931714626757 and parameters: {'observation_period_num': 52, 'train_rates': 0.8732469934386169, 'learning_rate': 4.296426353156423e-06, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8687318112100197}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 21:53:53,930][0m Trial 2 finished with value: 0.14225598535052053 and parameters: {'observation_period_num': 136, 'train_rates': 0.961697318088049, 'learning_rate': 0.00037604125271540137, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8894721920896531}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 21:55:31,289][0m Trial 3 finished with value: 0.1707269858327701 and parameters: {'observation_period_num': 22, 'train_rates': 0.7630241230420175, 'learning_rate': 0.00024982986571056906, 'batch_size': 51, 'step_size': 1, 'gamma': 0.9592008958800884}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 21:55:56,075][0m Trial 4 finished with value: 0.5518953800201416 and parameters: {'observation_period_num': 55, 'train_rates': 0.9285438264381644, 'learning_rate': 3.20736251669556e-06, 'batch_size': 250, 'step_size': 7, 'gamma': 0.7804429412495563}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 22:00:00,151][0m Trial 5 finished with value: 0.21716665272254085 and parameters: {'observation_period_num': 13, 'train_rates': 0.7509007032952759, 'learning_rate': 3.7195699524905326e-06, 'batch_size': 20, 'step_size': 9, 'gamma': 0.764994510526806}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 22:00:22,974][0m Trial 6 finished with value: 0.31000202058292015 and parameters: {'observation_period_num': 206, 'train_rates': 0.6269809974678833, 'learning_rate': 0.0008222977890549671, 'batch_size': 215, 'step_size': 6, 'gamma': 0.9032467208953482}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 22:00:57,202][0m Trial 7 finished with value: 0.2660374936908906 and parameters: {'observation_period_num': 186, 'train_rates': 0.872480459861208, 'learning_rate': 7.865545840610099e-06, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8474253720835495}. Best is trial 0 with value: 0.11410586370362176.[0m
[32m[I 2025-01-04 22:01:49,704][0m Trial 8 finished with value: 0.05784386468758761 and parameters: {'observation_period_num': 53, 'train_rates': 0.916552625204534, 'learning_rate': 6.868272844062893e-05, 'batch_size': 115, 'step_size': 5, 'gamma': 0.9599222494948162}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:02:21,302][0m Trial 9 finished with value: 0.2372275311334998 and parameters: {'observation_period_num': 166, 'train_rates': 0.7162062667959429, 'learning_rate': 0.00048305191368339806, 'batch_size': 161, 'step_size': 15, 'gamma': 0.8343638361215178}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:03:16,682][0m Trial 10 finished with value: 0.0832577785135088 and parameters: {'observation_period_num': 103, 'train_rates': 0.8375803830919284, 'learning_rate': 4.6446482732620115e-05, 'batch_size': 97, 'step_size': 2, 'gamma': 0.9793932664325522}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:04:11,702][0m Trial 11 finished with value: 0.08766141828635464 and parameters: {'observation_period_num': 94, 'train_rates': 0.8393039448940184, 'learning_rate': 4.974586929747893e-05, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9880974659032159}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:05:05,792][0m Trial 12 finished with value: 0.07596788633494608 and parameters: {'observation_period_num': 98, 'train_rates': 0.8264797098195111, 'learning_rate': 4.516105673995203e-05, 'batch_size': 102, 'step_size': 4, 'gamma': 0.9391310286279171}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:06:01,218][0m Trial 13 finished with value: 0.12035632133483887 and parameters: {'observation_period_num': 79, 'train_rates': 0.9837622337995314, 'learning_rate': 1.7735071449974042e-05, 'batch_size': 109, 'step_size': 4, 'gamma': 0.933380529342436}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:06:44,767][0m Trial 14 finished with value: 0.07526758499443531 and parameters: {'observation_period_num': 132, 'train_rates': 0.9052660977252286, 'learning_rate': 0.00011005717069761905, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9344940284347973}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:07:24,287][0m Trial 15 finished with value: 0.07886162362177007 and parameters: {'observation_period_num': 137, 'train_rates': 0.9029051650470225, 'learning_rate': 0.0001343256446030532, 'batch_size': 149, 'step_size': 5, 'gamma': 0.9209794650665077}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:07:53,915][0m Trial 16 finished with value: 0.09246170917432085 and parameters: {'observation_period_num': 158, 'train_rates': 0.9008377437256678, 'learning_rate': 0.00010756417676949949, 'batch_size': 203, 'step_size': 4, 'gamma': 0.9521459536785684}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:09:00,659][0m Trial 17 finished with value: 0.3115483528345556 and parameters: {'observation_period_num': 60, 'train_rates': 0.69189530957667, 'learning_rate': 1.382750101645938e-05, 'batch_size': 69, 'step_size': 3, 'gamma': 0.909370713551101}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:09:41,841][0m Trial 18 finished with value: 0.5123564667642434 and parameters: {'observation_period_num': 121, 'train_rates': 0.8012356291213572, 'learning_rate': 1.476546980434038e-06, 'batch_size': 132, 'step_size': 8, 'gamma': 0.9734822013981611}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:10:10,861][0m Trial 19 finished with value: 0.107179066313178 and parameters: {'observation_period_num': 218, 'train_rates': 0.8842987955749685, 'learning_rate': 0.00010340342740523596, 'batch_size': 187, 'step_size': 6, 'gamma': 0.8867317200215457}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:11:00,140][0m Trial 20 finished with value: 0.07919129729270935 and parameters: {'observation_period_num': 29, 'train_rates': 0.9884982783965237, 'learning_rate': 1.9211908513911274e-05, 'batch_size': 129, 'step_size': 12, 'gamma': 0.9546106744851459}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:12:15,730][0m Trial 21 finished with value: 0.06843178041202769 and parameters: {'observation_period_num': 86, 'train_rates': 0.7965989871398027, 'learning_rate': 5.5592770232105566e-05, 'batch_size': 69, 'step_size': 5, 'gamma': 0.9317666320384522}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:13:26,927][0m Trial 22 finished with value: 0.06833571334542397 and parameters: {'observation_period_num': 75, 'train_rates': 0.7867233604327817, 'learning_rate': 8.248351806054247e-05, 'batch_size': 73, 'step_size': 5, 'gamma': 0.9257407125240594}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:14:36,710][0m Trial 23 finished with value: 0.24090279173532514 and parameters: {'observation_period_num': 74, 'train_rates': 0.7727641604633682, 'learning_rate': 2.8971477726319482e-05, 'batch_size': 74, 'step_size': 6, 'gamma': 0.9179877390117385}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:15:39,678][0m Trial 24 finished with value: 0.16618436141762621 and parameters: {'observation_period_num': 33, 'train_rates': 0.6760679341146147, 'learning_rate': 6.849402496733432e-05, 'batch_size': 74, 'step_size': 5, 'gamma': 0.8685848985917526}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:17:05,614][0m Trial 25 finished with value: 0.06094519750160329 and parameters: {'observation_period_num': 44, 'train_rates': 0.8015165240138079, 'learning_rate': 0.00017803979353338864, 'batch_size': 60, 'step_size': 8, 'gamma': 0.9555771064477089}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:18:04,883][0m Trial 26 finished with value: 0.17819953478910788 and parameters: {'observation_period_num': 42, 'train_rates': 0.7306891986403101, 'learning_rate': 0.00018898341460295854, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9692352140436368}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:19:30,096][0m Trial 27 finished with value: 0.1254764116752101 and parameters: {'observation_period_num': 7, 'train_rates': 0.6103925922326056, 'learning_rate': 0.0006665081755978737, 'batch_size': 51, 'step_size': 10, 'gamma': 0.9525865986451393}. Best is trial 8 with value: 0.05784386468758761.[0m
[32m[I 2025-01-04 22:20:17,564][0m Trial 28 finished with value: 0.046855776367667024 and parameters: {'observation_period_num': 61, 'train_rates': 0.8160819305298114, 'learning_rate': 0.00019415716778227, 'batch_size': 115, 'step_size': 8, 'gamma': 0.9649482074238626}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:21:04,652][0m Trial 29 finished with value: 0.1118304527675112 and parameters: {'observation_period_num': 249, 'train_rates': 0.9380198881749401, 'learning_rate': 0.00018596434894775234, 'batch_size': 119, 'step_size': 12, 'gamma': 0.7956519578902932}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:21:42,687][0m Trial 30 finished with value: 0.09733307078195531 and parameters: {'observation_period_num': 112, 'train_rates': 0.8530674850407461, 'learning_rate': 0.00023769396636697467, 'batch_size': 151, 'step_size': 9, 'gamma': 0.9886822061660543}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:22:45,176][0m Trial 31 finished with value: 0.05505373927679929 and parameters: {'observation_period_num': 64, 'train_rates': 0.8098137901590238, 'learning_rate': 7.49950825020752e-05, 'batch_size': 86, 'step_size': 8, 'gamma': 0.9661409108309926}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:23:32,417][0m Trial 32 finished with value: 0.05736673235130484 and parameters: {'observation_period_num': 65, 'train_rates': 0.810835048058132, 'learning_rate': 0.00033959364171947147, 'batch_size': 116, 'step_size': 8, 'gamma': 0.9652864663761442}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:24:19,856][0m Trial 33 finished with value: 0.0885827725854787 and parameters: {'observation_period_num': 67, 'train_rates': 0.8095832051539265, 'learning_rate': 0.0003710998448605262, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9670113238802218}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:25:16,595][0m Trial 34 finished with value: 0.22995651423365 and parameters: {'observation_period_num': 50, 'train_rates': 0.7461768252443164, 'learning_rate': 0.0005780468897479966, 'batch_size': 89, 'step_size': 9, 'gamma': 0.9481049611907277}. Best is trial 28 with value: 0.046855776367667024.[0m
[32m[I 2025-01-04 22:26:02,720][0m Trial 35 finished with value: 0.039239475993733654 and parameters: {'observation_period_num': 23, 'train_rates': 0.8574132840212747, 'learning_rate': 0.00031320747931742546, 'batch_size': 120, 'step_size': 7, 'gamma': 0.9749537065548626}. Best is trial 35 with value: 0.039239475993733654.[0m
[32m[I 2025-01-04 22:26:42,466][0m Trial 36 finished with value: 0.04117059459594016 and parameters: {'observation_period_num': 24, 'train_rates': 0.8612828612609491, 'learning_rate': 0.00035617534261328667, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9788570870537927}. Best is trial 35 with value: 0.039239475993733654.[0m
[32m[I 2025-01-04 22:27:16,156][0m Trial 37 finished with value: 0.04451059840207723 and parameters: {'observation_period_num': 19, 'train_rates': 0.8646470873612611, 'learning_rate': 0.0008743954876101278, 'batch_size': 177, 'step_size': 7, 'gamma': 0.9812522090357412}. Best is trial 35 with value: 0.039239475993733654.[0m
[32m[I 2025-01-04 22:27:50,085][0m Trial 38 finished with value: 0.03708429647499461 and parameters: {'observation_period_num': 20, 'train_rates': 0.867697938136333, 'learning_rate': 0.0009943031329490005, 'batch_size': 177, 'step_size': 6, 'gamma': 0.9892667318035683}. Best is trial 38 with value: 0.03708429647499461.[0m
[32m[I 2025-01-04 22:28:23,945][0m Trial 39 finished with value: 0.05704652820713818 and parameters: {'observation_period_num': 24, 'train_rates': 0.8690688873489347, 'learning_rate': 0.0009705122562404173, 'batch_size': 180, 'step_size': 7, 'gamma': 0.9809001912289145}. Best is trial 38 with value: 0.03708429647499461.[0m
[32m[I 2025-01-04 22:28:51,661][0m Trial 40 finished with value: 0.040978405461476665 and parameters: {'observation_period_num': 12, 'train_rates': 0.8603735058416655, 'learning_rate': 0.000480461667472389, 'batch_size': 228, 'step_size': 6, 'gamma': 0.9871383800631967}. Best is trial 38 with value: 0.03708429647499461.[0m
[32m[I 2025-01-04 22:29:16,680][0m Trial 41 finished with value: 0.036764050682794674 and parameters: {'observation_period_num': 15, 'train_rates': 0.8594698909255369, 'learning_rate': 0.0005041596036810447, 'batch_size': 242, 'step_size': 6, 'gamma': 0.9896975254794574}. Best is trial 41 with value: 0.036764050682794674.[0m
[32m[I 2025-01-04 22:29:42,002][0m Trial 42 finished with value: 0.03358664035884516 and parameters: {'observation_period_num': 5, 'train_rates': 0.8844072809369177, 'learning_rate': 0.00044701541562591264, 'batch_size': 240, 'step_size': 6, 'gamma': 0.9898455579930374}. Best is trial 42 with value: 0.03358664035884516.[0m
[32m[I 2025-01-04 22:30:07,151][0m Trial 43 finished with value: 0.02998092600680662 and parameters: {'observation_period_num': 9, 'train_rates': 0.888009781880768, 'learning_rate': 0.000506448284899147, 'batch_size': 251, 'step_size': 6, 'gamma': 0.9893639066652541}. Best is trial 43 with value: 0.02998092600680662.[0m
[32m[I 2025-01-04 22:30:33,891][0m Trial 44 finished with value: 0.0346871018409729 and parameters: {'observation_period_num': 5, 'train_rates': 0.9621030994537845, 'learning_rate': 0.0006723675577342761, 'batch_size': 255, 'step_size': 6, 'gamma': 0.9765486936416679}. Best is trial 43 with value: 0.02998092600680662.[0m
[32m[I 2025-01-04 22:31:00,385][0m Trial 45 finished with value: 0.03738707676529884 and parameters: {'observation_period_num': 7, 'train_rates': 0.9663742571368826, 'learning_rate': 0.0006663746784357373, 'batch_size': 255, 'step_size': 6, 'gamma': 0.9429095117343697}. Best is trial 43 with value: 0.02998092600680662.[0m
[32m[I 2025-01-04 22:31:27,073][0m Trial 46 finished with value: 0.08648788183927536 and parameters: {'observation_period_num': 38, 'train_rates': 0.9481901704359704, 'learning_rate': 0.00045269592832117875, 'batch_size': 238, 'step_size': 3, 'gamma': 0.9899396793110069}. Best is trial 43 with value: 0.02998092600680662.[0m
[32m[I 2025-01-04 22:31:52,597][0m Trial 47 finished with value: 0.03560473147902761 and parameters: {'observation_period_num': 6, 'train_rates': 0.8872089007359816, 'learning_rate': 0.0006859272914392517, 'batch_size': 241, 'step_size': 6, 'gamma': 0.8119379024460343}. Best is trial 43 with value: 0.02998092600680662.[0m
[32m[I 2025-01-04 22:32:17,449][0m Trial 48 finished with value: 0.05445157166139308 and parameters: {'observation_period_num': 5, 'train_rates': 0.8859447243993757, 'learning_rate': 0.0005714850579442413, 'batch_size': 241, 'step_size': 3, 'gamma': 0.7919433146275385}. Best is trial 43 with value: 0.02998092600680662.[0m
Early stopping at epoch 66
[32m[I 2025-01-04 22:32:37,497][0m Trial 49 finished with value: 0.0996199424814257 and parameters: {'observation_period_num': 38, 'train_rates': 0.9201771943206991, 'learning_rate': 0.0007484772077412339, 'batch_size': 215, 'step_size': 1, 'gamma': 0.8134350129706535}. Best is trial 43 with value: 0.02998092600680662.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 22:32:37,507][0m A new study created in memory with name: no-name-6c262830-fe70-4578-875e-53ff687f8e62[0m
[32m[I 2025-01-04 22:34:53,999][0m Trial 0 finished with value: 0.21037161529152526 and parameters: {'observation_period_num': 128, 'train_rates': 0.6419608393749356, 'learning_rate': 0.0007036151979906486, 'batch_size': 31, 'step_size': 13, 'gamma': 0.8967758460387172}. Best is trial 0 with value: 0.21037161529152526.[0m
[32m[I 2025-01-04 22:35:33,157][0m Trial 1 finished with value: 0.14648013444323288 and parameters: {'observation_period_num': 223, 'train_rates': 0.9165484726639928, 'learning_rate': 2.5621123026847926e-05, 'batch_size': 144, 'step_size': 15, 'gamma': 0.80259495344297}. Best is trial 1 with value: 0.14648013444323288.[0m
[32m[I 2025-01-04 22:35:58,149][0m Trial 2 finished with value: 0.31294819712638855 and parameters: {'observation_period_num': 190, 'train_rates': 0.9179928134695218, 'learning_rate': 7.688420369322801e-06, 'batch_size': 242, 'step_size': 15, 'gamma': 0.8960916859571167}. Best is trial 1 with value: 0.14648013444323288.[0m
[32m[I 2025-01-04 22:36:31,002][0m Trial 3 finished with value: 0.10045313090085983 and parameters: {'observation_period_num': 186, 'train_rates': 0.9786401571430474, 'learning_rate': 0.0009306347942922358, 'batch_size': 191, 'step_size': 10, 'gamma': 0.8320333925189792}. Best is trial 3 with value: 0.10045313090085983.[0m
[32m[I 2025-01-04 22:37:24,988][0m Trial 4 finished with value: 0.32796863371409674 and parameters: {'observation_period_num': 121, 'train_rates': 0.748472679914783, 'learning_rate': 1.5663996357386704e-05, 'batch_size': 90, 'step_size': 11, 'gamma': 0.8781435272618183}. Best is trial 3 with value: 0.10045313090085983.[0m
[32m[I 2025-01-04 22:37:48,508][0m Trial 5 finished with value: 0.7950596720416372 and parameters: {'observation_period_num': 72, 'train_rates': 0.6443345044236826, 'learning_rate': 3.5162053091616948e-06, 'batch_size': 217, 'step_size': 7, 'gamma': 0.7592083278438975}. Best is trial 3 with value: 0.10045313090085983.[0m
[32m[I 2025-01-04 22:38:15,568][0m Trial 6 finished with value: 0.08624123455209529 and parameters: {'observation_period_num': 72, 'train_rates': 0.8703345822936859, 'learning_rate': 0.0003624902684652193, 'batch_size': 214, 'step_size': 1, 'gamma': 0.9123245465513866}. Best is trial 6 with value: 0.08624123455209529.[0m
[32m[I 2025-01-04 22:38:42,307][0m Trial 7 finished with value: 0.06638778217377202 and parameters: {'observation_period_num': 36, 'train_rates': 0.8619700138251596, 'learning_rate': 0.00028145910613031186, 'batch_size': 224, 'step_size': 3, 'gamma': 0.7954342789326224}. Best is trial 7 with value: 0.06638778217377202.[0m
[32m[I 2025-01-04 22:39:09,963][0m Trial 8 finished with value: 0.7786914496875006 and parameters: {'observation_period_num': 32, 'train_rates': 0.6273944803994514, 'learning_rate': 2.6356623794619993e-06, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8777532645650294}. Best is trial 7 with value: 0.06638778217377202.[0m
[32m[I 2025-01-04 22:40:49,273][0m Trial 9 finished with value: 0.25956527336880014 and parameters: {'observation_period_num': 162, 'train_rates': 0.7680331513009693, 'learning_rate': 0.0009165985118213512, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8409789409027382}. Best is trial 7 with value: 0.06638778217377202.[0m
[32m[I 2025-01-04 22:41:12,428][0m Trial 10 finished with value: 0.04528401859515295 and parameters: {'observation_period_num': 16, 'train_rates': 0.8271911876704701, 'learning_rate': 0.00016842110440159447, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9660268146878754}. Best is trial 10 with value: 0.04528401859515295.[0m
[32m[I 2025-01-04 22:41:35,237][0m Trial 11 finished with value: 0.04268864098166374 and parameters: {'observation_period_num': 6, 'train_rates': 0.8375360726565544, 'learning_rate': 0.00010550626847414502, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9887032514453499}. Best is trial 11 with value: 0.04268864098166374.[0m
[32m[I 2025-01-04 22:41:58,843][0m Trial 12 finished with value: 0.04143013851631672 and parameters: {'observation_period_num': 5, 'train_rates': 0.8153815070680471, 'learning_rate': 0.00010222586849735695, 'batch_size': 253, 'step_size': 6, 'gamma': 0.9796897145729558}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:42:42,157][0m Trial 13 finished with value: 0.19374298572074622 and parameters: {'observation_period_num': 78, 'train_rates': 0.7106900868731074, 'learning_rate': 7.680622006294021e-05, 'batch_size': 114, 'step_size': 6, 'gamma': 0.9894315778411504}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:43:15,927][0m Trial 14 finished with value: 0.05214243360225939 and parameters: {'observation_period_num': 6, 'train_rates': 0.8045845636415894, 'learning_rate': 6.41869910579598e-05, 'batch_size': 169, 'step_size': 4, 'gamma': 0.9431262657890286}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:43:37,827][0m Trial 15 finished with value: 0.1913593692807771 and parameters: {'observation_period_num': 50, 'train_rates': 0.7129463453106235, 'learning_rate': 7.225754486363016e-05, 'batch_size': 253, 'step_size': 8, 'gamma': 0.9448481953200248}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:44:08,028][0m Trial 16 finished with value: 0.07916892989969894 and parameters: {'observation_period_num': 99, 'train_rates': 0.8436336318363128, 'learning_rate': 0.0001488923508218951, 'batch_size': 194, 'step_size': 3, 'gamma': 0.9810104232942409}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:45:24,204][0m Trial 17 finished with value: 0.33936520990494373 and parameters: {'observation_period_num': 51, 'train_rates': 0.9172092526103843, 'learning_rate': 1.1479891594455756e-06, 'batch_size': 75, 'step_size': 9, 'gamma': 0.941738350962482}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:46:03,155][0m Trial 18 finished with value: 0.05934281835278261 and parameters: {'observation_period_num': 5, 'train_rates': 0.7905900281376862, 'learning_rate': 4.635281741974969e-05, 'batch_size': 144, 'step_size': 6, 'gamma': 0.9264200500225427}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:46:29,969][0m Trial 19 finished with value: 0.20506520569324493 and parameters: {'observation_period_num': 250, 'train_rates': 0.9740522083648627, 'learning_rate': 1.8684557632219523e-05, 'batch_size': 232, 'step_size': 8, 'gamma': 0.9613623408470173}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:46:57,190][0m Trial 20 finished with value: 0.2294365446790155 and parameters: {'observation_period_num': 99, 'train_rates': 0.7093105267517191, 'learning_rate': 0.00015254187603795584, 'batch_size': 195, 'step_size': 3, 'gamma': 0.9676299317456307}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:47:21,095][0m Trial 21 finished with value: 0.052096304577374145 and parameters: {'observation_period_num': 24, 'train_rates': 0.820102595585847, 'learning_rate': 0.00015678759463479846, 'batch_size': 250, 'step_size': 5, 'gamma': 0.9749305296521893}. Best is trial 12 with value: 0.04143013851631672.[0m
[32m[I 2025-01-04 22:47:46,155][0m Trial 22 finished with value: 0.034609070932899895 and parameters: {'observation_period_num': 8, 'train_rates': 0.8837471809806181, 'learning_rate': 0.0003551394722236175, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9558229117765169}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:48:15,485][0m Trial 23 finished with value: 0.04492613682392198 and parameters: {'observation_period_num': 47, 'train_rates': 0.9023410969083159, 'learning_rate': 0.0003972817034444386, 'batch_size': 215, 'step_size': 7, 'gamma': 0.9871455668216037}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:48:42,449][0m Trial 24 finished with value: 0.06672638298099877 and parameters: {'observation_period_num': 20, 'train_rates': 0.8806502684056989, 'learning_rate': 3.7312781653434786e-05, 'batch_size': 233, 'step_size': 5, 'gamma': 0.9533713347189788}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:49:14,028][0m Trial 25 finished with value: 0.07404277473688126 and parameters: {'observation_period_num': 58, 'train_rates': 0.9535688431442068, 'learning_rate': 0.00010251343793474302, 'batch_size': 202, 'step_size': 7, 'gamma': 0.9240283672959895}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:49:38,007][0m Trial 26 finished with value: 0.18141633568774046 and parameters: {'observation_period_num': 32, 'train_rates': 0.7668115708516473, 'learning_rate': 0.0003168651197322712, 'batch_size': 233, 'step_size': 4, 'gamma': 0.9298609406633169}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:50:11,728][0m Trial 27 finished with value: 0.0597779888642226 and parameters: {'observation_period_num': 98, 'train_rates': 0.8509401028850614, 'learning_rate': 0.000225105983721735, 'batch_size': 164, 'step_size': 8, 'gamma': 0.9633229750154085}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:50:33,517][0m Trial 28 finished with value: 0.10189264356348908 and parameters: {'observation_period_num': 154, 'train_rates': 0.7991880837712274, 'learning_rate': 0.0005611198216631838, 'batch_size': 254, 'step_size': 2, 'gamma': 0.9072916062236034}. Best is trial 22 with value: 0.034609070932899895.[0m
[32m[I 2025-01-04 22:53:02,692][0m Trial 29 finished with value: 0.03154617417986092 and parameters: {'observation_period_num': 5, 'train_rates': 0.8846415737042319, 'learning_rate': 0.00047407521803251854, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9897998026552572}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 22:56:10,046][0m Trial 30 finished with value: 0.09024187747377327 and parameters: {'observation_period_num': 63, 'train_rates': 0.8906049765158562, 'learning_rate': 0.0005712091128073273, 'batch_size': 29, 'step_size': 9, 'gamma': 0.8580707989636096}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 22:57:01,543][0m Trial 31 finished with value: 0.034618509036523325 and parameters: {'observation_period_num': 6, 'train_rates': 0.9450535501066458, 'learning_rate': 0.0001016152467955447, 'batch_size': 120, 'step_size': 6, 'gamma': 0.989862593725986}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:02:32,058][0m Trial 32 finished with value: 0.059093035983316826 and parameters: {'observation_period_num': 38, 'train_rates': 0.9320835831942821, 'learning_rate': 0.0004944224988752345, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9744075574739973}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:04:12,544][0m Trial 33 finished with value: 0.04578668277149331 and parameters: {'observation_period_num': 23, 'train_rates': 0.9502482488536652, 'learning_rate': 0.00022367754385016456, 'batch_size': 59, 'step_size': 6, 'gamma': 0.9530483810018822}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:05:05,856][0m Trial 34 finished with value: 0.05747150400510201 and parameters: {'observation_period_num': 16, 'train_rates': 0.9383094047296627, 'learning_rate': 2.366430514711023e-05, 'batch_size': 112, 'step_size': 4, 'gamma': 0.9750197103799221}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:06:00,405][0m Trial 35 finished with value: 0.04226442098719967 and parameters: {'observation_period_num': 5, 'train_rates': 0.9015537250245587, 'learning_rate': 4.543282078458546e-05, 'batch_size': 109, 'step_size': 7, 'gamma': 0.9541396107317925}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:06:43,743][0m Trial 36 finished with value: 0.22698050737380981 and parameters: {'observation_period_num': 206, 'train_rates': 0.9895962752793808, 'learning_rate': 9.80124677981907e-06, 'batch_size': 138, 'step_size': 9, 'gamma': 0.988532479895911}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:07:57,642][0m Trial 37 finished with value: 0.07477925850794866 and parameters: {'observation_period_num': 83, 'train_rates': 0.8870902652430652, 'learning_rate': 0.0007284927719933521, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8924505255966941}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:08:59,889][0m Trial 38 finished with value: 0.043657409549142524 and parameters: {'observation_period_num': 42, 'train_rates': 0.8665825921569767, 'learning_rate': 0.00010772627126556737, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9372851690352106}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:09:32,613][0m Trial 39 finished with value: 0.08673439919948578 and parameters: {'observation_period_num': 132, 'train_rates': 0.9632471861493839, 'learning_rate': 0.00023320086591194965, 'batch_size': 183, 'step_size': 10, 'gamma': 0.7989071489908279}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:10:19,857][0m Trial 40 finished with value: 0.03624990962335629 and parameters: {'observation_period_num': 29, 'train_rates': 0.9306376193502993, 'learning_rate': 0.00044709562904076663, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9113827236140745}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:11:10,036][0m Trial 41 finished with value: 0.05051877790567826 and parameters: {'observation_period_num': 26, 'train_rates': 0.9333030193951626, 'learning_rate': 0.0009791923239702606, 'batch_size': 120, 'step_size': 7, 'gamma': 0.9159442465032084}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:11:49,767][0m Trial 42 finished with value: 0.033599704353191595 and parameters: {'observation_period_num': 16, 'train_rates': 0.9160881546246925, 'learning_rate': 0.0004370272240190297, 'batch_size': 154, 'step_size': 8, 'gamma': 0.7519792193840481}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:12:29,187][0m Trial 43 finished with value: 0.03330684024840593 and parameters: {'observation_period_num': 18, 'train_rates': 0.9149300541142096, 'learning_rate': 0.00041711909334897056, 'batch_size': 151, 'step_size': 8, 'gamma': 0.7549773614829166}. Best is trial 29 with value: 0.03154617417986092.[0m
[32m[I 2025-01-04 23:13:08,737][0m Trial 44 finished with value: 0.025936868279538255 and parameters: {'observation_period_num': 17, 'train_rates': 0.9042201225099494, 'learning_rate': 0.0007272198878977273, 'batch_size': 152, 'step_size': 12, 'gamma': 0.7538948756514965}. Best is trial 44 with value: 0.025936868279538255.[0m
[32m[I 2025-01-04 23:13:46,728][0m Trial 45 finished with value: 0.0572829186592413 and parameters: {'observation_period_num': 61, 'train_rates': 0.9048901178315887, 'learning_rate': 0.0006926703803215482, 'batch_size': 156, 'step_size': 14, 'gamma': 0.7539586705399299}. Best is trial 44 with value: 0.025936868279538255.[0m
[32m[I 2025-01-04 23:14:26,192][0m Trial 46 finished with value: 0.03528358250765539 and parameters: {'observation_period_num': 16, 'train_rates': 0.8606395121355924, 'learning_rate': 0.00028567251510597366, 'batch_size': 151, 'step_size': 11, 'gamma': 0.775413846581539}. Best is trial 44 with value: 0.025936868279538255.[0m
[32m[I 2025-01-04 23:15:05,387][0m Trial 47 finished with value: 0.044916023846623326 and parameters: {'observation_period_num': 38, 'train_rates': 0.9170600013094437, 'learning_rate': 0.00037704746075308416, 'batch_size': 155, 'step_size': 12, 'gamma': 0.7783448815627751}. Best is trial 44 with value: 0.025936868279538255.[0m
[32m[I 2025-01-04 23:15:39,242][0m Trial 48 finished with value: 0.031284450339611435 and parameters: {'observation_period_num': 19, 'train_rates': 0.8753894584050607, 'learning_rate': 0.000663200116730098, 'batch_size': 177, 'step_size': 10, 'gamma': 0.7640693421894408}. Best is trial 44 with value: 0.025936868279538255.[0m
[32m[I 2025-01-04 23:16:11,722][0m Trial 49 finished with value: 0.04875344544906596 and parameters: {'observation_period_num': 72, 'train_rates': 0.8394436838398126, 'learning_rate': 0.0007288713419926478, 'batch_size': 179, 'step_size': 10, 'gamma': 0.8192722811590507}. Best is trial 44 with value: 0.025936868279538255.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 23:16:11,732][0m A new study created in memory with name: no-name-6af9a4fc-a373-4788-bf52-494bd4c6ddd8[0m
[32m[I 2025-01-04 23:16:34,462][0m Trial 0 finished with value: 1.2002079177905618 and parameters: {'observation_period_num': 139, 'train_rates': 0.7382574352189619, 'learning_rate': 3.367903037387372e-06, 'batch_size': 251, 'step_size': 3, 'gamma': 0.8398863726548487}. Best is trial 0 with value: 1.2002079177905618.[0m
Early stopping at epoch 82
[32m[I 2025-01-04 23:17:12,880][0m Trial 1 finished with value: 0.4073726534843445 and parameters: {'observation_period_num': 216, 'train_rates': 0.9196777621759628, 'learning_rate': 2.6980868853883194e-05, 'batch_size': 125, 'step_size': 2, 'gamma': 0.7575699790722038}. Best is trial 1 with value: 0.4073726534843445.[0m
[32m[I 2025-01-04 23:17:38,029][0m Trial 2 finished with value: 0.13876742124557495 and parameters: {'observation_period_num': 191, 'train_rates': 0.9291426148539967, 'learning_rate': 0.00023209513686083926, 'batch_size': 248, 'step_size': 15, 'gamma': 0.9683749527661222}. Best is trial 2 with value: 0.13876742124557495.[0m
[32m[I 2025-01-04 23:18:14,979][0m Trial 3 finished with value: 0.1317703094548331 and parameters: {'observation_period_num': 173, 'train_rates': 0.7757957212377194, 'learning_rate': 0.0005273406562779936, 'batch_size': 139, 'step_size': 8, 'gamma': 0.9724365094458199}. Best is trial 3 with value: 0.1317703094548331.[0m
Early stopping at epoch 48
[32m[I 2025-01-04 23:18:34,911][0m Trial 4 finished with value: 1.5165038108825684 and parameters: {'observation_period_num': 207, 'train_rates': 0.9586237687492796, 'learning_rate': 6.73882173832248e-06, 'batch_size': 154, 'step_size': 1, 'gamma': 0.7893773308625149}. Best is trial 3 with value: 0.1317703094548331.[0m
[32m[I 2025-01-04 23:19:04,044][0m Trial 5 finished with value: 0.08772885975512591 and parameters: {'observation_period_num': 226, 'train_rates': 0.919598844605354, 'learning_rate': 0.0005389910427620304, 'batch_size': 197, 'step_size': 10, 'gamma': 0.7978575295128435}. Best is trial 5 with value: 0.08772885975512591.[0m
[32m[I 2025-01-04 23:19:28,808][0m Trial 6 finished with value: 0.5836445093154907 and parameters: {'observation_period_num': 193, 'train_rates': 0.9230773157997856, 'learning_rate': 3.984167675467042e-06, 'batch_size': 249, 'step_size': 15, 'gamma': 0.7895227046382268}. Best is trial 5 with value: 0.08772885975512591.[0m
[32m[I 2025-01-04 23:19:51,965][0m Trial 7 finished with value: 0.18822000580457784 and parameters: {'observation_period_num': 30, 'train_rates': 0.845766783306001, 'learning_rate': 6.938978409653463e-06, 'batch_size': 250, 'step_size': 15, 'gamma': 0.9041909259028256}. Best is trial 5 with value: 0.08772885975512591.[0m
[32m[I 2025-01-04 23:20:26,563][0m Trial 8 finished with value: 0.08326975432665724 and parameters: {'observation_period_num': 115, 'train_rates': 0.8128939175106169, 'learning_rate': 0.00013747613540367166, 'batch_size': 161, 'step_size': 14, 'gamma': 0.820171317839807}. Best is trial 8 with value: 0.08326975432665724.[0m
[32m[I 2025-01-04 23:20:51,357][0m Trial 9 finished with value: 0.24496045014380438 and parameters: {'observation_period_num': 198, 'train_rates': 0.6433523617968017, 'learning_rate': 0.000410712566182138, 'batch_size': 191, 'step_size': 14, 'gamma': 0.9817199791586916}. Best is trial 8 with value: 0.08326975432665724.[0m
[32m[I 2025-01-04 23:22:37,483][0m Trial 10 finished with value: 0.1737224339903763 and parameters: {'observation_period_num': 64, 'train_rates': 0.6821268826364556, 'learning_rate': 7.637156095961718e-05, 'batch_size': 43, 'step_size': 11, 'gamma': 0.884475299063054}. Best is trial 8 with value: 0.08326975432665724.[0m
[32m[I 2025-01-04 23:23:07,098][0m Trial 11 finished with value: 0.06923990993089574 and parameters: {'observation_period_num': 89, 'train_rates': 0.8377336979609937, 'learning_rate': 0.00011724972833272052, 'batch_size': 190, 'step_size': 10, 'gamma': 0.8423872074368423}. Best is trial 11 with value: 0.06923990993089574.[0m
[32m[I 2025-01-04 23:24:10,052][0m Trial 12 finished with value: 0.06387092687896809 and parameters: {'observation_period_num': 91, 'train_rates': 0.84206148606952, 'learning_rate': 9.274921414050077e-05, 'batch_size': 86, 'step_size': 7, 'gamma': 0.8426483388559686}. Best is trial 12 with value: 0.06387092687896809.[0m
[32m[I 2025-01-04 23:25:13,378][0m Trial 13 finished with value: 0.07857123871573883 and parameters: {'observation_period_num': 85, 'train_rates': 0.8528276575426815, 'learning_rate': 3.4362745663394326e-05, 'batch_size': 86, 'step_size': 6, 'gamma': 0.8540866284274143}. Best is trial 12 with value: 0.06387092687896809.[0m
[32m[I 2025-01-04 23:26:20,818][0m Trial 14 finished with value: 0.05280487153605364 and parameters: {'observation_period_num': 17, 'train_rates': 0.8624102361620106, 'learning_rate': 3.786184021755684e-05, 'batch_size': 83, 'step_size': 6, 'gamma': 0.9101176991848239}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:31:22,942][0m Trial 15 finished with value: 0.15470000938481132 and parameters: {'observation_period_num': 5, 'train_rates': 0.7473357926982206, 'learning_rate': 3.263917880521128e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.924459591682982}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:32:24,595][0m Trial 16 finished with value: 0.4636142723700579 and parameters: {'observation_period_num': 43, 'train_rates': 0.8539796615675678, 'learning_rate': 1.1849464818305611e-06, 'batch_size': 90, 'step_size': 6, 'gamma': 0.9361198135343872}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:33:34,266][0m Trial 17 finished with value: 0.059280436617486616 and parameters: {'observation_period_num': 5, 'train_rates': 0.8848290770090398, 'learning_rate': 1.6840412214224382e-05, 'batch_size': 82, 'step_size': 8, 'gamma': 0.8855948846101266}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:35:21,590][0m Trial 18 finished with value: 0.05831530364670964 and parameters: {'observation_period_num': 8, 'train_rates': 0.8925280111262904, 'learning_rate': 1.645784799178352e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8852774264563846}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:37:14,553][0m Trial 19 finished with value: 0.062061842530965805 and parameters: {'observation_period_num': 41, 'train_rates': 0.9854433700868396, 'learning_rate': 1.3344385095028094e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.9447350248711074}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:38:56,592][0m Trial 20 finished with value: 0.08457276165880115 and parameters: {'observation_period_num': 251, 'train_rates': 0.8884749136739845, 'learning_rate': 4.961211389115359e-05, 'batch_size': 50, 'step_size': 4, 'gamma': 0.9162950704782671}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:39:51,590][0m Trial 21 finished with value: 0.05959256358487024 and parameters: {'observation_period_num': 5, 'train_rates': 0.8809107778393104, 'learning_rate': 1.5576665714970784e-05, 'batch_size': 105, 'step_size': 8, 'gamma': 0.8985141127986696}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:41:08,902][0m Trial 22 finished with value: 0.08506670689990803 and parameters: {'observation_period_num': 24, 'train_rates': 0.7955323949184154, 'learning_rate': 1.2882499058775624e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.874252698049511}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:45:56,515][0m Trial 23 finished with value: 0.05602734511984246 and parameters: {'observation_period_num': 60, 'train_rates': 0.9034593923891419, 'learning_rate': 1.9960197722309078e-05, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8880894979233025}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:50:31,274][0m Trial 24 finished with value: 0.10135264840067887 and parameters: {'observation_period_num': 59, 'train_rates': 0.971903533836929, 'learning_rate': 7.2717331085328435e-06, 'batch_size': 21, 'step_size': 5, 'gamma': 0.8652575399410456}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:53:19,248][0m Trial 25 finished with value: 0.06408811957569596 and parameters: {'observation_period_num': 61, 'train_rates': 0.9443727576483476, 'learning_rate': 4.7807355240316674e-05, 'batch_size': 34, 'step_size': 3, 'gamma': 0.94875273902292}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:54:44,627][0m Trial 26 finished with value: 0.18357804558567098 and parameters: {'observation_period_num': 25, 'train_rates': 0.899113804377664, 'learning_rate': 2.087247980223311e-06, 'batch_size': 67, 'step_size': 6, 'gamma': 0.9038680735959087}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:55:31,398][0m Trial 27 finished with value: 0.09938920708989375 and parameters: {'observation_period_num': 124, 'train_rates': 0.8132504963517518, 'learning_rate': 0.00020022867529656874, 'batch_size': 113, 'step_size': 1, 'gamma': 0.9280947722667975}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:58:20,017][0m Trial 28 finished with value: 0.0868207873268561 and parameters: {'observation_period_num': 148, 'train_rates': 0.8786276354387168, 'learning_rate': 2.249075455034475e-05, 'batch_size': 31, 'step_size': 4, 'gamma': 0.8885982679880071}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-04 23:59:36,905][0m Trial 29 finished with value: 0.25688280334955527 and parameters: {'observation_period_num': 73, 'train_rates': 0.7258618708221605, 'learning_rate': 5.104480522994085e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8201245642749118}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:02:08,501][0m Trial 30 finished with value: 0.1671818345785141 and parameters: {'observation_period_num': 44, 'train_rates': 0.951491283889442, 'learning_rate': 3.389432330767049e-06, 'batch_size': 38, 'step_size': 7, 'gamma': 0.8650597472575091}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:03:27,480][0m Trial 31 finished with value: 0.06052602268755436 and parameters: {'observation_period_num': 13, 'train_rates': 0.9036804303291379, 'learning_rate': 1.629567522196468e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8827806004662871}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:04:21,844][0m Trial 32 finished with value: 0.09306777115972316 and parameters: {'observation_period_num': 22, 'train_rates': 0.8752942371949712, 'learning_rate': 9.542569786609775e-06, 'batch_size': 103, 'step_size': 5, 'gamma': 0.910538340944815}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:05:31,845][0m Trial 33 finished with value: 0.061095871753534495 and parameters: {'observation_period_num': 36, 'train_rates': 0.8658402638520084, 'learning_rate': 2.1317271040335615e-05, 'batch_size': 79, 'step_size': 9, 'gamma': 0.8921119476686136}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:06:15,046][0m Trial 34 finished with value: 0.16805260666633692 and parameters: {'observation_period_num': 17, 'train_rates': 0.8206985187900234, 'learning_rate': 2.8762338273437263e-05, 'batch_size': 130, 'step_size': 2, 'gamma': 0.8597069736377576}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:08:00,892][0m Trial 35 finished with value: 0.10124271959066392 and parameters: {'observation_period_num': 52, 'train_rates': 0.9087659563983219, 'learning_rate': 5.062014873536223e-06, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9573916868065907}. Best is trial 14 with value: 0.05280487153605364.[0m
[32m[I 2025-01-05 00:09:00,791][0m Trial 36 finished with value: 0.03179062719619463 and parameters: {'observation_period_num': 5, 'train_rates': 0.9361041806612609, 'learning_rate': 0.0008924454046804217, 'batch_size': 100, 'step_size': 11, 'gamma': 0.7544088787253677}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:09:50,230][0m Trial 37 finished with value: 0.07917099481537229 and parameters: {'observation_period_num': 147, 'train_rates': 0.9326546521671272, 'learning_rate': 0.0007556992959530447, 'batch_size': 117, 'step_size': 12, 'gamma': 0.810348806264786}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:10:31,181][0m Trial 38 finished with value: 0.08551091700792313 and parameters: {'observation_period_num': 104, 'train_rates': 0.9683227058996772, 'learning_rate': 0.0003043811785165728, 'batch_size': 153, 'step_size': 12, 'gamma': 0.7756540142012486}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:11:31,684][0m Trial 39 finished with value: 0.0774982955419656 and parameters: {'observation_period_num': 74, 'train_rates': 0.9315381859900875, 'learning_rate': 6.761065758223702e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.7640091720066475}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:14:27,589][0m Trial 40 finished with value: 0.09783773257661221 and parameters: {'observation_period_num': 165, 'train_rates': 0.783772584590674, 'learning_rate': 0.0008627049319804375, 'batch_size': 27, 'step_size': 2, 'gamma': 0.8282581835089047}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:16:11,100][0m Trial 41 finished with value: 0.05979717873813563 and parameters: {'observation_period_num': 9, 'train_rates': 0.9120603472702491, 'learning_rate': 9.231936386466821e-06, 'batch_size': 55, 'step_size': 9, 'gamma': 0.8734172906274476}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:17:23,913][0m Trial 42 finished with value: 0.055553749263382114 and parameters: {'observation_period_num': 32, 'train_rates': 0.8932878957394113, 'learning_rate': 1.901503606025934e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9180401155384632}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:19:32,776][0m Trial 43 finished with value: 0.07124753047216248 and parameters: {'observation_period_num': 32, 'train_rates': 0.9474375022342423, 'learning_rate': 0.00018827236770168653, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9271136000626665}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:20:15,804][0m Trial 44 finished with value: 0.6114376244362462 and parameters: {'observation_period_num': 53, 'train_rates': 0.9281374304742563, 'learning_rate': 2.0435814256723265e-06, 'batch_size': 138, 'step_size': 5, 'gamma': 0.9165792221931}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:20:43,293][0m Trial 45 finished with value: 0.21689629108828498 and parameters: {'observation_period_num': 30, 'train_rates': 0.8630812146499285, 'learning_rate': 4.905292505793548e-06, 'batch_size': 212, 'step_size': 11, 'gamma': 0.8507964132462863}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:22:12,750][0m Trial 46 finished with value: 0.147070996438628 and parameters: {'observation_period_num': 18, 'train_rates': 0.8974144023755912, 'learning_rate': 1.0505935139816863e-05, 'batch_size': 63, 'step_size': 4, 'gamma': 0.752525130084455}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:22:57,601][0m Trial 47 finished with value: 0.06673966505025562 and parameters: {'observation_period_num': 45, 'train_rates': 0.8238569328208727, 'learning_rate': 3.808164822685829e-05, 'batch_size': 120, 'step_size': 6, 'gamma': 0.8960201318003643}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:23:54,900][0m Trial 48 finished with value: 0.07131165155628597 and parameters: {'observation_period_num': 70, 'train_rates': 0.8376566132221043, 'learning_rate': 2.032942630594038e-05, 'batch_size': 95, 'step_size': 7, 'gamma': 0.9681162635887931}. Best is trial 36 with value: 0.03179062719619463.[0m
[32m[I 2025-01-05 00:25:13,559][0m Trial 49 finished with value: 0.05504313624137408 and parameters: {'observation_period_num': 33, 'train_rates': 0.9212026735738252, 'learning_rate': 9.723550397944086e-05, 'batch_size': 74, 'step_size': 10, 'gamma': 0.9374924485062195}. Best is trial 36 with value: 0.03179062719619463.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 16, 'train_rates': 0.9746970381277509, 'learning_rate': 0.00023436469191064942, 'batch_size': 62, 'step_size': 5, 'gamma': 0.8298060488040724}
Epoch 1/300, trend Loss: 0.2278 | 0.1445
Epoch 2/300, trend Loss: 0.1488 | 0.1012
Epoch 3/300, trend Loss: 0.1262 | 0.0776
Epoch 4/300, trend Loss: 0.1161 | 0.0704
Epoch 5/300, trend Loss: 0.1101 | 0.0666
Epoch 6/300, trend Loss: 0.1049 | 0.0641
Epoch 7/300, trend Loss: 0.1051 | 0.0629
Epoch 8/300, trend Loss: 0.1042 | 0.0616
Epoch 9/300, trend Loss: 0.1035 | 0.0676
Epoch 10/300, trend Loss: 0.1019 | 0.0761
Epoch 11/300, trend Loss: 0.0983 | 0.0817
Epoch 12/300, trend Loss: 0.1025 | 0.0809
Epoch 13/300, trend Loss: 0.1059 | 0.0639
Epoch 14/300, trend Loss: 0.1071 | 0.0614
Epoch 15/300, trend Loss: 0.0948 | 0.0548
Epoch 16/300, trend Loss: 0.0896 | 0.0542
Epoch 17/300, trend Loss: 0.0887 | 0.0533
Epoch 18/300, trend Loss: 0.0873 | 0.0527
Epoch 19/300, trend Loss: 0.0857 | 0.0529
Epoch 20/300, trend Loss: 0.0841 | 0.0512
Epoch 21/300, trend Loss: 0.0823 | 0.0497
Epoch 22/300, trend Loss: 0.0813 | 0.0483
Epoch 23/300, trend Loss: 0.0806 | 0.0472
Epoch 24/300, trend Loss: 0.0802 | 0.0472
Epoch 25/300, trend Loss: 0.0794 | 0.0464
Epoch 26/300, trend Loss: 0.0791 | 0.0461
Epoch 27/300, trend Loss: 0.0788 | 0.0452
Epoch 28/300, trend Loss: 0.0786 | 0.0447
Epoch 29/300, trend Loss: 0.0782 | 0.0437
Epoch 30/300, trend Loss: 0.0782 | 0.0435
Epoch 31/300, trend Loss: 0.0777 | 0.0425
Epoch 32/300, trend Loss: 0.0778 | 0.0425
Epoch 33/300, trend Loss: 0.0775 | 0.0424
Epoch 34/300, trend Loss: 0.0777 | 0.0432
Epoch 35/300, trend Loss: 0.0776 | 0.0433
Epoch 36/300, trend Loss: 0.0775 | 0.0467
Epoch 37/300, trend Loss: 0.0770 | 0.0466
Epoch 38/300, trend Loss: 0.0764 | 0.0464
Epoch 39/300, trend Loss: 0.0762 | 0.0460
Epoch 40/300, trend Loss: 0.0760 | 0.0444
Epoch 41/300, trend Loss: 0.0758 | 0.0422
Epoch 42/300, trend Loss: 0.0756 | 0.0416
Epoch 43/300, trend Loss: 0.0750 | 0.0412
Epoch 44/300, trend Loss: 0.0743 | 0.0406
Epoch 45/300, trend Loss: 0.0737 | 0.0405
Epoch 46/300, trend Loss: 0.0732 | 0.0404
Epoch 47/300, trend Loss: 0.0729 | 0.0404
Epoch 48/300, trend Loss: 0.0728 | 0.0404
Epoch 49/300, trend Loss: 0.0727 | 0.0404
Epoch 50/300, trend Loss: 0.0726 | 0.0403
Epoch 51/300, trend Loss: 0.0726 | 0.0403
Epoch 52/300, trend Loss: 0.0725 | 0.0402
Epoch 53/300, trend Loss: 0.0724 | 0.0402
Epoch 54/300, trend Loss: 0.0723 | 0.0401
Epoch 55/300, trend Loss: 0.0723 | 0.0400
Epoch 56/300, trend Loss: 0.0722 | 0.0400
Epoch 57/300, trend Loss: 0.0722 | 0.0399
Epoch 58/300, trend Loss: 0.0721 | 0.0399
Epoch 59/300, trend Loss: 0.0721 | 0.0398
Epoch 60/300, trend Loss: 0.0720 | 0.0398
Epoch 61/300, trend Loss: 0.0720 | 0.0398
Epoch 62/300, trend Loss: 0.0720 | 0.0397
Epoch 63/300, trend Loss: 0.0719 | 0.0397
Epoch 64/300, trend Loss: 0.0719 | 0.0397
Epoch 65/300, trend Loss: 0.0719 | 0.0396
Epoch 66/300, trend Loss: 0.0719 | 0.0396
Epoch 67/300, trend Loss: 0.0718 | 0.0396
Epoch 68/300, trend Loss: 0.0718 | 0.0396
Epoch 69/300, trend Loss: 0.0718 | 0.0395
Epoch 70/300, trend Loss: 0.0718 | 0.0395
Epoch 71/300, trend Loss: 0.0717 | 0.0395
Epoch 72/300, trend Loss: 0.0717 | 0.0395
Epoch 73/300, trend Loss: 0.0717 | 0.0395
Epoch 74/300, trend Loss: 0.0717 | 0.0395
Epoch 75/300, trend Loss: 0.0717 | 0.0394
Epoch 76/300, trend Loss: 0.0717 | 0.0394
Epoch 77/300, trend Loss: 0.0717 | 0.0394
Epoch 78/300, trend Loss: 0.0717 | 0.0394
Epoch 79/300, trend Loss: 0.0716 | 0.0394
Epoch 80/300, trend Loss: 0.0716 | 0.0394
Epoch 81/300, trend Loss: 0.0716 | 0.0394
Epoch 82/300, trend Loss: 0.0716 | 0.0394
Epoch 83/300, trend Loss: 0.0716 | 0.0394
Epoch 84/300, trend Loss: 0.0716 | 0.0394
Epoch 85/300, trend Loss: 0.0716 | 0.0393
Epoch 86/300, trend Loss: 0.0716 | 0.0393
Epoch 87/300, trend Loss: 0.0716 | 0.0393
Epoch 88/300, trend Loss: 0.0716 | 0.0393
Epoch 89/300, trend Loss: 0.0716 | 0.0393
Epoch 90/300, trend Loss: 0.0716 | 0.0393
Epoch 91/300, trend Loss: 0.0716 | 0.0393
Epoch 92/300, trend Loss: 0.0716 | 0.0393
Epoch 93/300, trend Loss: 0.0715 | 0.0393
Epoch 94/300, trend Loss: 0.0715 | 0.0393
Epoch 95/300, trend Loss: 0.0715 | 0.0393
Epoch 96/300, trend Loss: 0.0715 | 0.0393
Epoch 97/300, trend Loss: 0.0715 | 0.0393
Epoch 98/300, trend Loss: 0.0715 | 0.0393
Epoch 99/300, trend Loss: 0.0715 | 0.0393
Epoch 100/300, trend Loss: 0.0715 | 0.0393
Epoch 101/300, trend Loss: 0.0715 | 0.0393
Epoch 102/300, trend Loss: 0.0715 | 0.0393
Epoch 103/300, trend Loss: 0.0715 | 0.0393
Epoch 104/300, trend Loss: 0.0715 | 0.0393
Epoch 105/300, trend Loss: 0.0715 | 0.0393
Epoch 106/300, trend Loss: 0.0715 | 0.0393
Epoch 107/300, trend Loss: 0.0715 | 0.0393
Epoch 108/300, trend Loss: 0.0715 | 0.0393
Epoch 109/300, trend Loss: 0.0715 | 0.0393
Epoch 110/300, trend Loss: 0.0715 | 0.0393
Epoch 111/300, trend Loss: 0.0715 | 0.0393
Epoch 112/300, trend Loss: 0.0715 | 0.0392
Epoch 113/300, trend Loss: 0.0715 | 0.0392
Epoch 114/300, trend Loss: 0.0715 | 0.0392
Epoch 115/300, trend Loss: 0.0715 | 0.0392
Epoch 116/300, trend Loss: 0.0715 | 0.0392
Epoch 117/300, trend Loss: 0.0715 | 0.0392
Epoch 118/300, trend Loss: 0.0715 | 0.0392
Epoch 119/300, trend Loss: 0.0715 | 0.0392
Epoch 120/300, trend Loss: 0.0715 | 0.0392
Epoch 121/300, trend Loss: 0.0715 | 0.0392
Epoch 122/300, trend Loss: 0.0715 | 0.0392
Epoch 123/300, trend Loss: 0.0715 | 0.0392
Epoch 124/300, trend Loss: 0.0715 | 0.0392
Epoch 125/300, trend Loss: 0.0715 | 0.0392
Epoch 126/300, trend Loss: 0.0715 | 0.0392
Epoch 127/300, trend Loss: 0.0715 | 0.0392
Epoch 128/300, trend Loss: 0.0715 | 0.0392
Epoch 129/300, trend Loss: 0.0715 | 0.0392
Epoch 130/300, trend Loss: 0.0715 | 0.0392
Epoch 131/300, trend Loss: 0.0715 | 0.0392
Epoch 132/300, trend Loss: 0.0715 | 0.0392
Epoch 133/300, trend Loss: 0.0715 | 0.0392
Epoch 134/300, trend Loss: 0.0715 | 0.0392
Epoch 135/300, trend Loss: 0.0715 | 0.0392
Epoch 136/300, trend Loss: 0.0715 | 0.0392
Epoch 137/300, trend Loss: 0.0715 | 0.0392
Epoch 138/300, trend Loss: 0.0715 | 0.0392
Epoch 139/300, trend Loss: 0.0715 | 0.0392
Epoch 140/300, trend Loss: 0.0715 | 0.0392
Epoch 141/300, trend Loss: 0.0715 | 0.0392
Epoch 142/300, trend Loss: 0.0715 | 0.0392
Epoch 143/300, trend Loss: 0.0715 | 0.0392
Epoch 144/300, trend Loss: 0.0715 | 0.0392
Epoch 145/300, trend Loss: 0.0715 | 0.0392
Epoch 146/300, trend Loss: 0.0715 | 0.0392
Epoch 147/300, trend Loss: 0.0715 | 0.0392
Epoch 148/300, trend Loss: 0.0715 | 0.0392
Epoch 149/300, trend Loss: 0.0715 | 0.0392
Epoch 150/300, trend Loss: 0.0715 | 0.0392
Epoch 151/300, trend Loss: 0.0715 | 0.0392
Epoch 152/300, trend Loss: 0.0715 | 0.0392
Epoch 153/300, trend Loss: 0.0715 | 0.0392
Epoch 154/300, trend Loss: 0.0715 | 0.0392
Epoch 155/300, trend Loss: 0.0715 | 0.0392
Epoch 156/300, trend Loss: 0.0715 | 0.0392
Epoch 157/300, trend Loss: 0.0715 | 0.0392
Epoch 158/300, trend Loss: 0.0715 | 0.0392
Epoch 159/300, trend Loss: 0.0715 | 0.0392
Epoch 160/300, trend Loss: 0.0715 | 0.0392
Epoch 161/300, trend Loss: 0.0715 | 0.0392
Epoch 162/300, trend Loss: 0.0715 | 0.0392
Epoch 163/300, trend Loss: 0.0715 | 0.0392
Epoch 164/300, trend Loss: 0.0715 | 0.0392
Epoch 165/300, trend Loss: 0.0715 | 0.0392
Epoch 166/300, trend Loss: 0.0715 | 0.0392
Epoch 167/300, trend Loss: 0.0715 | 0.0392
Epoch 168/300, trend Loss: 0.0715 | 0.0392
Epoch 169/300, trend Loss: 0.0715 | 0.0392
Epoch 170/300, trend Loss: 0.0715 | 0.0392
Epoch 171/300, trend Loss: 0.0715 | 0.0392
Epoch 172/300, trend Loss: 0.0715 | 0.0392
Epoch 173/300, trend Loss: 0.0715 | 0.0392
Epoch 174/300, trend Loss: 0.0715 | 0.0392
Epoch 175/300, trend Loss: 0.0715 | 0.0392
Epoch 176/300, trend Loss: 0.0715 | 0.0392
Epoch 177/300, trend Loss: 0.0715 | 0.0392
Epoch 178/300, trend Loss: 0.0715 | 0.0392
Epoch 179/300, trend Loss: 0.0715 | 0.0392
Epoch 180/300, trend Loss: 0.0715 | 0.0392
Epoch 181/300, trend Loss: 0.0715 | 0.0392
Epoch 182/300, trend Loss: 0.0715 | 0.0392
Epoch 183/300, trend Loss: 0.0715 | 0.0392
Epoch 184/300, trend Loss: 0.0715 | 0.0392
Epoch 185/300, trend Loss: 0.0715 | 0.0392
Epoch 186/300, trend Loss: 0.0715 | 0.0392
Epoch 187/300, trend Loss: 0.0715 | 0.0392
Epoch 188/300, trend Loss: 0.0715 | 0.0392
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.8849797772061458, 'learning_rate': 0.00033929423154687864, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9029610260108089}
Epoch 1/300, seasonal_0 Loss: 0.2057 | 0.0974
Epoch 2/300, seasonal_0 Loss: 0.1249 | 0.0747
Epoch 3/300, seasonal_0 Loss: 0.1145 | 0.0813
Epoch 4/300, seasonal_0 Loss: 0.1109 | 0.0846
Epoch 5/300, seasonal_0 Loss: 0.1048 | 0.0720
Epoch 6/300, seasonal_0 Loss: 0.0971 | 0.0646
Epoch 7/300, seasonal_0 Loss: 0.0922 | 0.0599
Epoch 8/300, seasonal_0 Loss: 0.0896 | 0.0589
Epoch 9/300, seasonal_0 Loss: 0.0875 | 0.0594
Epoch 10/300, seasonal_0 Loss: 0.0858 | 0.0608
Epoch 11/300, seasonal_0 Loss: 0.0841 | 0.0624
Epoch 12/300, seasonal_0 Loss: 0.0823 | 0.0614
Epoch 13/300, seasonal_0 Loss: 0.0806 | 0.0574
Epoch 14/300, seasonal_0 Loss: 0.0791 | 0.0526
Epoch 15/300, seasonal_0 Loss: 0.0779 | 0.0479
Epoch 16/300, seasonal_0 Loss: 0.0769 | 0.0436
Epoch 17/300, seasonal_0 Loss: 0.0761 | 0.0404
Epoch 18/300, seasonal_0 Loss: 0.0754 | 0.0381
Epoch 19/300, seasonal_0 Loss: 0.0748 | 0.0367
Epoch 20/300, seasonal_0 Loss: 0.0743 | 0.0358
Epoch 21/300, seasonal_0 Loss: 0.0739 | 0.0351
Epoch 22/300, seasonal_0 Loss: 0.0734 | 0.0347
Epoch 23/300, seasonal_0 Loss: 0.0731 | 0.0343
Epoch 24/300, seasonal_0 Loss: 0.0728 | 0.0341
Epoch 25/300, seasonal_0 Loss: 0.0725 | 0.0338
Epoch 26/300, seasonal_0 Loss: 0.0722 | 0.0336
Epoch 27/300, seasonal_0 Loss: 0.0720 | 0.0333
Epoch 28/300, seasonal_0 Loss: 0.0718 | 0.0331
Epoch 29/300, seasonal_0 Loss: 0.0716 | 0.0328
Epoch 30/300, seasonal_0 Loss: 0.0715 | 0.0326
Epoch 31/300, seasonal_0 Loss: 0.0713 | 0.0325
Epoch 32/300, seasonal_0 Loss: 0.0712 | 0.0324
Epoch 33/300, seasonal_0 Loss: 0.0711 | 0.0324
Epoch 34/300, seasonal_0 Loss: 0.0710 | 0.0323
Epoch 35/300, seasonal_0 Loss: 0.0709 | 0.0322
Epoch 36/300, seasonal_0 Loss: 0.0708 | 0.0322
Epoch 37/300, seasonal_0 Loss: 0.0707 | 0.0322
Epoch 38/300, seasonal_0 Loss: 0.0706 | 0.0322
Epoch 39/300, seasonal_0 Loss: 0.0706 | 0.0322
Epoch 40/300, seasonal_0 Loss: 0.0705 | 0.0322
Epoch 41/300, seasonal_0 Loss: 0.0705 | 0.0321
Epoch 42/300, seasonal_0 Loss: 0.0704 | 0.0321
Epoch 43/300, seasonal_0 Loss: 0.0704 | 0.0321
Epoch 44/300, seasonal_0 Loss: 0.0703 | 0.0320
Epoch 45/300, seasonal_0 Loss: 0.0703 | 0.0320
Epoch 46/300, seasonal_0 Loss: 0.0702 | 0.0320
Epoch 47/300, seasonal_0 Loss: 0.0702 | 0.0320
Epoch 48/300, seasonal_0 Loss: 0.0701 | 0.0320
Epoch 49/300, seasonal_0 Loss: 0.0701 | 0.0320
Epoch 50/300, seasonal_0 Loss: 0.0701 | 0.0320
Epoch 51/300, seasonal_0 Loss: 0.0700 | 0.0320
Epoch 52/300, seasonal_0 Loss: 0.0700 | 0.0320
Epoch 53/300, seasonal_0 Loss: 0.0700 | 0.0320
Epoch 54/300, seasonal_0 Loss: 0.0699 | 0.0320
Epoch 55/300, seasonal_0 Loss: 0.0699 | 0.0320
Epoch 56/300, seasonal_0 Loss: 0.0699 | 0.0320
Epoch 57/300, seasonal_0 Loss: 0.0699 | 0.0320
Epoch 58/300, seasonal_0 Loss: 0.0699 | 0.0320
Epoch 59/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 60/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 61/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 62/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 63/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 64/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 65/300, seasonal_0 Loss: 0.0698 | 0.0320
Epoch 66/300, seasonal_0 Loss: 0.0698 | 0.0321
Epoch 67/300, seasonal_0 Loss: 0.0698 | 0.0321
Epoch 68/300, seasonal_0 Loss: 0.0698 | 0.0321
Epoch 69/300, seasonal_0 Loss: 0.0697 | 0.0321
Epoch 70/300, seasonal_0 Loss: 0.0697 | 0.0321
Epoch 71/300, seasonal_0 Loss: 0.0697 | 0.0321
Epoch 72/300, seasonal_0 Loss: 0.0697 | 0.0321
Epoch 73/300, seasonal_0 Loss: 0.0697 | 0.0321
Epoch 74/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 75/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 76/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 77/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 78/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 79/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 80/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 81/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 82/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 83/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 84/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 85/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 86/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 87/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 88/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 89/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 90/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 91/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 92/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 93/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 94/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 95/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 96/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 97/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 98/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 99/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 100/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 101/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 102/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 103/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 104/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 105/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 106/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 107/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 108/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 109/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 110/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 111/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 112/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 113/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 114/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 115/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 116/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 117/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 118/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 119/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 120/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 121/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 122/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 123/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 124/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 125/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 126/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 127/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 128/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 129/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 130/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 131/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 132/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 133/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 134/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 135/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 136/300, seasonal_0 Loss: 0.0697 | 0.0320
Epoch 137/300, seasonal_0 Loss: 0.0697 | 0.0320
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 9, 'train_rates': 0.9403743313311494, 'learning_rate': 9.70008209197557e-05, 'batch_size': 35, 'step_size': 7, 'gamma': 0.9633591904986404}
Epoch 1/300, seasonal_1 Loss: 0.2419 | 0.1183
Epoch 2/300, seasonal_1 Loss: 0.1372 | 0.0882
Epoch 3/300, seasonal_1 Loss: 0.1226 | 0.0740
Epoch 4/300, seasonal_1 Loss: 0.1147 | 0.0679
Epoch 5/300, seasonal_1 Loss: 0.1110 | 0.0664
Epoch 6/300, seasonal_1 Loss: 0.1097 | 0.0816
Epoch 7/300, seasonal_1 Loss: 0.1080 | 0.0772
Epoch 8/300, seasonal_1 Loss: 0.1029 | 0.0638
Epoch 9/300, seasonal_1 Loss: 0.0975 | 0.0596
Epoch 10/300, seasonal_1 Loss: 0.0943 | 0.0570
Epoch 11/300, seasonal_1 Loss: 0.0920 | 0.0545
Epoch 12/300, seasonal_1 Loss: 0.0902 | 0.0524
Epoch 13/300, seasonal_1 Loss: 0.0887 | 0.0505
Epoch 14/300, seasonal_1 Loss: 0.0875 | 0.0491
Epoch 15/300, seasonal_1 Loss: 0.0864 | 0.0477
Epoch 16/300, seasonal_1 Loss: 0.0853 | 0.0466
Epoch 17/300, seasonal_1 Loss: 0.0841 | 0.0457
Epoch 18/300, seasonal_1 Loss: 0.0832 | 0.0450
Epoch 19/300, seasonal_1 Loss: 0.0822 | 0.0442
Epoch 20/300, seasonal_1 Loss: 0.0812 | 0.0434
Epoch 21/300, seasonal_1 Loss: 0.0803 | 0.0428
Epoch 22/300, seasonal_1 Loss: 0.0794 | 0.0422
Epoch 23/300, seasonal_1 Loss: 0.0786 | 0.0415
Epoch 24/300, seasonal_1 Loss: 0.0778 | 0.0409
Epoch 25/300, seasonal_1 Loss: 0.0772 | 0.0404
Epoch 26/300, seasonal_1 Loss: 0.0765 | 0.0399
Epoch 27/300, seasonal_1 Loss: 0.0758 | 0.0392
Epoch 28/300, seasonal_1 Loss: 0.0751 | 0.0386
Epoch 29/300, seasonal_1 Loss: 0.0743 | 0.0382
Epoch 30/300, seasonal_1 Loss: 0.0736 | 0.0374
Epoch 31/300, seasonal_1 Loss: 0.0729 | 0.0368
Epoch 32/300, seasonal_1 Loss: 0.0723 | 0.0363
Epoch 33/300, seasonal_1 Loss: 0.0718 | 0.0364
Epoch 34/300, seasonal_1 Loss: 0.0714 | 0.0361
Epoch 35/300, seasonal_1 Loss: 0.0709 | 0.0359
Epoch 36/300, seasonal_1 Loss: 0.0705 | 0.0362
Epoch 37/300, seasonal_1 Loss: 0.0701 | 0.0358
Epoch 38/300, seasonal_1 Loss: 0.0696 | 0.0356
Epoch 39/300, seasonal_1 Loss: 0.0692 | 0.0354
Epoch 40/300, seasonal_1 Loss: 0.0688 | 0.0356
Epoch 41/300, seasonal_1 Loss: 0.0685 | 0.0351
Epoch 42/300, seasonal_1 Loss: 0.0680 | 0.0349
Epoch 43/300, seasonal_1 Loss: 0.0677 | 0.0349
Epoch 44/300, seasonal_1 Loss: 0.0674 | 0.0344
Epoch 45/300, seasonal_1 Loss: 0.0670 | 0.0343
Epoch 46/300, seasonal_1 Loss: 0.0666 | 0.0342
Epoch 47/300, seasonal_1 Loss: 0.0663 | 0.0342
Epoch 48/300, seasonal_1 Loss: 0.0661 | 0.0338
Epoch 49/300, seasonal_1 Loss: 0.0657 | 0.0338
Epoch 50/300, seasonal_1 Loss: 0.0655 | 0.0337
Epoch 51/300, seasonal_1 Loss: 0.0652 | 0.0334
Epoch 52/300, seasonal_1 Loss: 0.0650 | 0.0333
Epoch 53/300, seasonal_1 Loss: 0.0647 | 0.0332
Epoch 54/300, seasonal_1 Loss: 0.0645 | 0.0332
Epoch 55/300, seasonal_1 Loss: 0.0643 | 0.0328
Epoch 56/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 57/300, seasonal_1 Loss: 0.0639 | 0.0323
Epoch 58/300, seasonal_1 Loss: 0.0637 | 0.0319
Epoch 59/300, seasonal_1 Loss: 0.0635 | 0.0317
Epoch 60/300, seasonal_1 Loss: 0.0632 | 0.0315
Epoch 61/300, seasonal_1 Loss: 0.0629 | 0.0311
Epoch 62/300, seasonal_1 Loss: 0.0626 | 0.0307
Epoch 63/300, seasonal_1 Loss: 0.0624 | 0.0306
Epoch 64/300, seasonal_1 Loss: 0.0621 | 0.0303
Epoch 65/300, seasonal_1 Loss: 0.0618 | 0.0300
Epoch 66/300, seasonal_1 Loss: 0.0615 | 0.0299
Epoch 67/300, seasonal_1 Loss: 0.0613 | 0.0298
Epoch 68/300, seasonal_1 Loss: 0.0610 | 0.0296
Epoch 69/300, seasonal_1 Loss: 0.0607 | 0.0294
Epoch 70/300, seasonal_1 Loss: 0.0605 | 0.0293
Epoch 71/300, seasonal_1 Loss: 0.0602 | 0.0291
Epoch 72/300, seasonal_1 Loss: 0.0599 | 0.0289
Epoch 73/300, seasonal_1 Loss: 0.0596 | 0.0288
Epoch 74/300, seasonal_1 Loss: 0.0594 | 0.0287
Epoch 75/300, seasonal_1 Loss: 0.0591 | 0.0285
Epoch 76/300, seasonal_1 Loss: 0.0588 | 0.0282
Epoch 77/300, seasonal_1 Loss: 0.0585 | 0.0281
Epoch 78/300, seasonal_1 Loss: 0.0583 | 0.0278
Epoch 79/300, seasonal_1 Loss: 0.0581 | 0.0277
Epoch 80/300, seasonal_1 Loss: 0.0579 | 0.0274
Epoch 81/300, seasonal_1 Loss: 0.0577 | 0.0275
Epoch 82/300, seasonal_1 Loss: 0.0575 | 0.0270
Epoch 83/300, seasonal_1 Loss: 0.0574 | 0.0272
Epoch 84/300, seasonal_1 Loss: 0.0572 | 0.0267
Epoch 85/300, seasonal_1 Loss: 0.0573 | 0.0270
Epoch 86/300, seasonal_1 Loss: 0.0570 | 0.0264
Epoch 87/300, seasonal_1 Loss: 0.0570 | 0.0268
Epoch 88/300, seasonal_1 Loss: 0.0567 | 0.0263
Epoch 89/300, seasonal_1 Loss: 0.0570 | 0.0268
Epoch 90/300, seasonal_1 Loss: 0.0566 | 0.0263
Epoch 91/300, seasonal_1 Loss: 0.0567 | 0.0267
Epoch 92/300, seasonal_1 Loss: 0.0565 | 0.0266
Epoch 93/300, seasonal_1 Loss: 0.0568 | 0.0272
Epoch 94/300, seasonal_1 Loss: 0.0563 | 0.0268
Epoch 95/300, seasonal_1 Loss: 0.0564 | 0.0273
Epoch 96/300, seasonal_1 Loss: 0.0559 | 0.0271
Epoch 97/300, seasonal_1 Loss: 0.0562 | 0.0276
Epoch 98/300, seasonal_1 Loss: 0.0556 | 0.0272
Epoch 99/300, seasonal_1 Loss: 0.0558 | 0.0278
Epoch 100/300, seasonal_1 Loss: 0.0553 | 0.0275
Epoch 101/300, seasonal_1 Loss: 0.0554 | 0.0278
Epoch 102/300, seasonal_1 Loss: 0.0550 | 0.0275
Epoch 103/300, seasonal_1 Loss: 0.0551 | 0.0280
Epoch 104/300, seasonal_1 Loss: 0.0548 | 0.0277
Epoch 105/300, seasonal_1 Loss: 0.0548 | 0.0279
Epoch 106/300, seasonal_1 Loss: 0.0545 | 0.0279
Epoch 107/300, seasonal_1 Loss: 0.0545 | 0.0280
Epoch 108/300, seasonal_1 Loss: 0.0542 | 0.0278
Epoch 109/300, seasonal_1 Loss: 0.0542 | 0.0279
Epoch 110/300, seasonal_1 Loss: 0.0540 | 0.0280
Epoch 111/300, seasonal_1 Loss: 0.0539 | 0.0280
Epoch 112/300, seasonal_1 Loss: 0.0537 | 0.0279
Epoch 113/300, seasonal_1 Loss: 0.0537 | 0.0280
Epoch 114/300, seasonal_1 Loss: 0.0535 | 0.0279
Epoch 115/300, seasonal_1 Loss: 0.0534 | 0.0279
Epoch 116/300, seasonal_1 Loss: 0.0533 | 0.0278
Epoch 117/300, seasonal_1 Loss: 0.0532 | 0.0280
Epoch 118/300, seasonal_1 Loss: 0.0530 | 0.0278
Epoch 119/300, seasonal_1 Loss: 0.0529 | 0.0279
Epoch 120/300, seasonal_1 Loss: 0.0528 | 0.0279
Epoch 121/300, seasonal_1 Loss: 0.0527 | 0.0279
Epoch 122/300, seasonal_1 Loss: 0.0525 | 0.0279
Epoch 123/300, seasonal_1 Loss: 0.0524 | 0.0280
Epoch 124/300, seasonal_1 Loss: 0.0522 | 0.0282
Epoch 125/300, seasonal_1 Loss: 0.0521 | 0.0282
Epoch 126/300, seasonal_1 Loss: 0.0518 | 0.0283
Epoch 127/300, seasonal_1 Loss: 0.0516 | 0.0286
Epoch 128/300, seasonal_1 Loss: 0.0513 | 0.0286
Epoch 129/300, seasonal_1 Loss: 0.0509 | 0.0288
Epoch 130/300, seasonal_1 Loss: 0.0504 | 0.0290
Epoch 131/300, seasonal_1 Loss: 0.0496 | 0.0289
Epoch 132/300, seasonal_1 Loss: 0.0488 | 0.0283
Epoch 133/300, seasonal_1 Loss: 0.0479 | 0.0278
Epoch 134/300, seasonal_1 Loss: 0.0473 | 0.0274
Epoch 135/300, seasonal_1 Loss: 0.0468 | 0.0272
Epoch 136/300, seasonal_1 Loss: 0.0464 | 0.0271
Epoch 137/300, seasonal_1 Loss: 0.0461 | 0.0269
Epoch 138/300, seasonal_1 Loss: 0.0458 | 0.0268
Epoch 139/300, seasonal_1 Loss: 0.0455 | 0.0267
Epoch 140/300, seasonal_1 Loss: 0.0453 | 0.0267
Epoch 141/300, seasonal_1 Loss: 0.0451 | 0.0265
Epoch 142/300, seasonal_1 Loss: 0.0451 | 0.0278
Epoch 143/300, seasonal_1 Loss: 0.0455 | 0.0267
Epoch 144/300, seasonal_1 Loss: 0.0450 | 0.0276
Epoch 145/300, seasonal_1 Loss: 0.0454 | 0.0270
Epoch 146/300, seasonal_1 Loss: 0.0444 | 0.0266
Epoch 147/300, seasonal_1 Loss: 0.0443 | 0.0266
Epoch 148/300, seasonal_1 Loss: 0.0441 | 0.0266
Epoch 149/300, seasonal_1 Loss: 0.0440 | 0.0267
Epoch 150/300, seasonal_1 Loss: 0.0440 | 0.0267
Epoch 151/300, seasonal_1 Loss: 0.0439 | 0.0268
Epoch 152/300, seasonal_1 Loss: 0.0440 | 0.0268
Epoch 153/300, seasonal_1 Loss: 0.0438 | 0.0269
Epoch 154/300, seasonal_1 Loss: 0.0438 | 0.0269
Epoch 155/300, seasonal_1 Loss: 0.0436 | 0.0270
Epoch 156/300, seasonal_1 Loss: 0.0436 | 0.0270
Epoch 157/300, seasonal_1 Loss: 0.0434 | 0.0271
Epoch 158/300, seasonal_1 Loss: 0.0435 | 0.0271
Epoch 159/300, seasonal_1 Loss: 0.0433 | 0.0272
Epoch 160/300, seasonal_1 Loss: 0.0433 | 0.0273
Epoch 161/300, seasonal_1 Loss: 0.0432 | 0.0274
Epoch 162/300, seasonal_1 Loss: 0.0432 | 0.0276
Epoch 163/300, seasonal_1 Loss: 0.0431 | 0.0277
Epoch 164/300, seasonal_1 Loss: 0.0431 | 0.0279
Epoch 165/300, seasonal_1 Loss: 0.0430 | 0.0281
Epoch 166/300, seasonal_1 Loss: 0.0431 | 0.0279
Epoch 167/300, seasonal_1 Loss: 0.0429 | 0.0279
Epoch 168/300, seasonal_1 Loss: 0.0429 | 0.0276
Epoch 169/300, seasonal_1 Loss: 0.0427 | 0.0276
Epoch 170/300, seasonal_1 Loss: 0.0427 | 0.0274
Epoch 171/300, seasonal_1 Loss: 0.0425 | 0.0275
Epoch 172/300, seasonal_1 Loss: 0.0425 | 0.0273
Epoch 173/300, seasonal_1 Loss: 0.0424 | 0.0274
Epoch 174/300, seasonal_1 Loss: 0.0423 | 0.0272
Epoch 175/300, seasonal_1 Loss: 0.0422 | 0.0274
Epoch 176/300, seasonal_1 Loss: 0.0422 | 0.0272
Epoch 177/300, seasonal_1 Loss: 0.0421 | 0.0273
Epoch 178/300, seasonal_1 Loss: 0.0421 | 0.0271
Epoch 179/300, seasonal_1 Loss: 0.0420 | 0.0272
Epoch 180/300, seasonal_1 Loss: 0.0419 | 0.0271
Epoch 181/300, seasonal_1 Loss: 0.0418 | 0.0271
Epoch 182/300, seasonal_1 Loss: 0.0418 | 0.0270
Epoch 183/300, seasonal_1 Loss: 0.0417 | 0.0271
Epoch 184/300, seasonal_1 Loss: 0.0417 | 0.0270
Epoch 185/300, seasonal_1 Loss: 0.0416 | 0.0271
Epoch 186/300, seasonal_1 Loss: 0.0416 | 0.0270
Epoch 187/300, seasonal_1 Loss: 0.0415 | 0.0270
Epoch 188/300, seasonal_1 Loss: 0.0415 | 0.0270
Epoch 189/300, seasonal_1 Loss: 0.0414 | 0.0270
Epoch 190/300, seasonal_1 Loss: 0.0414 | 0.0269
Epoch 191/300, seasonal_1 Loss: 0.0413 | 0.0270
Epoch 192/300, seasonal_1 Loss: 0.0413 | 0.0269
Epoch 193/300, seasonal_1 Loss: 0.0412 | 0.0270
Epoch 194/300, seasonal_1 Loss: 0.0412 | 0.0269
Epoch 195/300, seasonal_1 Loss: 0.0411 | 0.0269
Epoch 196/300, seasonal_1 Loss: 0.0411 | 0.0269
Epoch 197/300, seasonal_1 Loss: 0.0410 | 0.0269
Epoch 198/300, seasonal_1 Loss: 0.0410 | 0.0269
Epoch 199/300, seasonal_1 Loss: 0.0409 | 0.0269
Epoch 200/300, seasonal_1 Loss: 0.0409 | 0.0269
Epoch 201/300, seasonal_1 Loss: 0.0408 | 0.0269
Epoch 202/300, seasonal_1 Loss: 0.0408 | 0.0269
Epoch 203/300, seasonal_1 Loss: 0.0408 | 0.0269
Epoch 204/300, seasonal_1 Loss: 0.0407 | 0.0269
Epoch 205/300, seasonal_1 Loss: 0.0407 | 0.0269
Epoch 206/300, seasonal_1 Loss: 0.0406 | 0.0269
Epoch 207/300, seasonal_1 Loss: 0.0406 | 0.0269
Epoch 208/300, seasonal_1 Loss: 0.0406 | 0.0269
Epoch 209/300, seasonal_1 Loss: 0.0405 | 0.0270
Epoch 210/300, seasonal_1 Loss: 0.0405 | 0.0270
Epoch 211/300, seasonal_1 Loss: 0.0404 | 0.0270
Epoch 212/300, seasonal_1 Loss: 0.0404 | 0.0270
Epoch 213/300, seasonal_1 Loss: 0.0404 | 0.0270
Epoch 214/300, seasonal_1 Loss: 0.0403 | 0.0270
Epoch 215/300, seasonal_1 Loss: 0.0403 | 0.0271
Epoch 216/300, seasonal_1 Loss: 0.0403 | 0.0271
Epoch 217/300, seasonal_1 Loss: 0.0402 | 0.0271
Epoch 218/300, seasonal_1 Loss: 0.0402 | 0.0271
Epoch 219/300, seasonal_1 Loss: 0.0402 | 0.0271
Epoch 220/300, seasonal_1 Loss: 0.0401 | 0.0271
Epoch 221/300, seasonal_1 Loss: 0.0401 | 0.0272
Epoch 222/300, seasonal_1 Loss: 0.0400 | 0.0272
Epoch 223/300, seasonal_1 Loss: 0.0400 | 0.0272
Epoch 224/300, seasonal_1 Loss: 0.0400 | 0.0272
Epoch 225/300, seasonal_1 Loss: 0.0399 | 0.0273
Epoch 226/300, seasonal_1 Loss: 0.0399 | 0.0273
Epoch 227/300, seasonal_1 Loss: 0.0399 | 0.0274
Epoch 228/300, seasonal_1 Loss: 0.0398 | 0.0274
Epoch 229/300, seasonal_1 Loss: 0.0398 | 0.0275
Epoch 230/300, seasonal_1 Loss: 0.0398 | 0.0275
Epoch 231/300, seasonal_1 Loss: 0.0397 | 0.0275
Epoch 232/300, seasonal_1 Loss: 0.0397 | 0.0275
Epoch 233/300, seasonal_1 Loss: 0.0397 | 0.0275
Epoch 234/300, seasonal_1 Loss: 0.0397 | 0.0275
Epoch 235/300, seasonal_1 Loss: 0.0396 | 0.0276
Epoch 236/300, seasonal_1 Loss: 0.0396 | 0.0275
Epoch 237/300, seasonal_1 Loss: 0.0396 | 0.0275
Epoch 238/300, seasonal_1 Loss: 0.0396 | 0.0275
Epoch 239/300, seasonal_1 Loss: 0.0395 | 0.0275
Epoch 240/300, seasonal_1 Loss: 0.0395 | 0.0275
Epoch 241/300, seasonal_1 Loss: 0.0395 | 0.0274
Epoch 242/300, seasonal_1 Loss: 0.0394 | 0.0275
Epoch 243/300, seasonal_1 Loss: 0.0394 | 0.0274
Epoch 244/300, seasonal_1 Loss: 0.0394 | 0.0274
Epoch 245/300, seasonal_1 Loss: 0.0394 | 0.0274
Epoch 246/300, seasonal_1 Loss: 0.0393 | 0.0275
Epoch 247/300, seasonal_1 Loss: 0.0393 | 0.0274
Epoch 248/300, seasonal_1 Loss: 0.0393 | 0.0275
Epoch 249/300, seasonal_1 Loss: 0.0392 | 0.0275
Epoch 250/300, seasonal_1 Loss: 0.0392 | 0.0275
Epoch 251/300, seasonal_1 Loss: 0.0392 | 0.0275
Epoch 252/300, seasonal_1 Loss: 0.0392 | 0.0275
Epoch 253/300, seasonal_1 Loss: 0.0391 | 0.0275
Epoch 254/300, seasonal_1 Loss: 0.0391 | 0.0276
Epoch 255/300, seasonal_1 Loss: 0.0391 | 0.0276
Epoch 256/300, seasonal_1 Loss: 0.0390 | 0.0276
Epoch 257/300, seasonal_1 Loss: 0.0390 | 0.0276
Epoch 258/300, seasonal_1 Loss: 0.0390 | 0.0276
Epoch 259/300, seasonal_1 Loss: 0.0390 | 0.0276
Epoch 260/300, seasonal_1 Loss: 0.0389 | 0.0277
Epoch 261/300, seasonal_1 Loss: 0.0389 | 0.0277
Epoch 262/300, seasonal_1 Loss: 0.0389 | 0.0277
Epoch 263/300, seasonal_1 Loss: 0.0389 | 0.0277
Epoch 264/300, seasonal_1 Loss: 0.0388 | 0.0277
Epoch 265/300, seasonal_1 Loss: 0.0388 | 0.0277
Epoch 266/300, seasonal_1 Loss: 0.0388 | 0.0277
Epoch 267/300, seasonal_1 Loss: 0.0388 | 0.0277
Epoch 268/300, seasonal_1 Loss: 0.0388 | 0.0278
Epoch 269/300, seasonal_1 Loss: 0.0387 | 0.0277
Epoch 270/300, seasonal_1 Loss: 0.0387 | 0.0278
Epoch 271/300, seasonal_1 Loss: 0.0387 | 0.0278
Epoch 272/300, seasonal_1 Loss: 0.0387 | 0.0278
Epoch 273/300, seasonal_1 Loss: 0.0387 | 0.0278
Epoch 274/300, seasonal_1 Loss: 0.0387 | 0.0278
Epoch 275/300, seasonal_1 Loss: 0.0386 | 0.0278
Epoch 276/300, seasonal_1 Loss: 0.0386 | 0.0278
Epoch 277/300, seasonal_1 Loss: 0.0386 | 0.0278
Epoch 278/300, seasonal_1 Loss: 0.0386 | 0.0278
Epoch 279/300, seasonal_1 Loss: 0.0386 | 0.0278
Epoch 280/300, seasonal_1 Loss: 0.0385 | 0.0279
Epoch 281/300, seasonal_1 Loss: 0.0385 | 0.0279
Epoch 282/300, seasonal_1 Loss: 0.0385 | 0.0279
Epoch 283/300, seasonal_1 Loss: 0.0385 | 0.0279
Epoch 284/300, seasonal_1 Loss: 0.0384 | 0.0279
Epoch 285/300, seasonal_1 Loss: 0.0384 | 0.0279
Epoch 286/300, seasonal_1 Loss: 0.0384 | 0.0279
Epoch 287/300, seasonal_1 Loss: 0.0384 | 0.0279
Epoch 288/300, seasonal_1 Loss: 0.0384 | 0.0279
Epoch 289/300, seasonal_1 Loss: 0.0383 | 0.0279
Epoch 290/300, seasonal_1 Loss: 0.0383 | 0.0279
Epoch 291/300, seasonal_1 Loss: 0.0383 | 0.0279
Epoch 292/300, seasonal_1 Loss: 0.0383 | 0.0280
Epoch 293/300, seasonal_1 Loss: 0.0382 | 0.0280
Epoch 294/300, seasonal_1 Loss: 0.0382 | 0.0280
Epoch 295/300, seasonal_1 Loss: 0.0382 | 0.0280
Epoch 296/300, seasonal_1 Loss: 0.0382 | 0.0280
Epoch 297/300, seasonal_1 Loss: 0.0382 | 0.0280
Epoch 298/300, seasonal_1 Loss: 0.0381 | 0.0280
Epoch 299/300, seasonal_1 Loss: 0.0381 | 0.0280
Epoch 300/300, seasonal_1 Loss: 0.0381 | 0.0280
Training seasonal_2 component with params: {'observation_period_num': 9, 'train_rates': 0.888009781880768, 'learning_rate': 0.000506448284899147, 'batch_size': 251, 'step_size': 6, 'gamma': 0.9893639066652541}
Epoch 1/300, seasonal_2 Loss: 0.4262 | 0.1628
Epoch 2/300, seasonal_2 Loss: 0.2151 | 0.1631
Epoch 3/300, seasonal_2 Loss: 0.1487 | 0.1372
Epoch 4/300, seasonal_2 Loss: 0.1528 | 0.0931
Epoch 5/300, seasonal_2 Loss: 0.1408 | 0.0796
Epoch 6/300, seasonal_2 Loss: 0.1336 | 0.0788
Epoch 7/300, seasonal_2 Loss: 0.1291 | 0.0735
Epoch 8/300, seasonal_2 Loss: 0.1199 | 0.0672
Epoch 9/300, seasonal_2 Loss: 0.1379 | 0.0700
Epoch 10/300, seasonal_2 Loss: 0.1632 | 0.0996
Epoch 11/300, seasonal_2 Loss: 0.2043 | 0.1264
Epoch 12/300, seasonal_2 Loss: 0.1573 | 0.0919
Epoch 13/300, seasonal_2 Loss: 0.1766 | 0.0735
Epoch 14/300, seasonal_2 Loss: 0.1736 | 0.0875
Epoch 15/300, seasonal_2 Loss: 0.1384 | 0.0712
Epoch 16/300, seasonal_2 Loss: 0.1217 | 0.0669
Epoch 17/300, seasonal_2 Loss: 0.1119 | 0.0602
Epoch 18/300, seasonal_2 Loss: 0.1265 | 0.0815
Epoch 19/300, seasonal_2 Loss: 0.1267 | 0.0993
Epoch 20/300, seasonal_2 Loss: 0.1130 | 0.0650
Epoch 21/300, seasonal_2 Loss: 0.1080 | 0.0541
Epoch 22/300, seasonal_2 Loss: 0.1015 | 0.0590
Epoch 23/300, seasonal_2 Loss: 0.0988 | 0.0553
Epoch 24/300, seasonal_2 Loss: 0.0955 | 0.0485
Epoch 25/300, seasonal_2 Loss: 0.0946 | 0.0526
Epoch 26/300, seasonal_2 Loss: 0.0937 | 0.0555
Epoch 27/300, seasonal_2 Loss: 0.0903 | 0.0495
Epoch 28/300, seasonal_2 Loss: 0.0899 | 0.0435
Epoch 29/300, seasonal_2 Loss: 0.0902 | 0.0476
Epoch 30/300, seasonal_2 Loss: 0.0904 | 0.0468
Epoch 31/300, seasonal_2 Loss: 0.0883 | 0.0444
Epoch 32/300, seasonal_2 Loss: 0.0894 | 0.0628
Epoch 33/300, seasonal_2 Loss: 0.0907 | 0.0506
Epoch 34/300, seasonal_2 Loss: 0.0884 | 0.0430
Epoch 35/300, seasonal_2 Loss: 0.0857 | 0.0409
Epoch 36/300, seasonal_2 Loss: 0.0909 | 0.0401
Epoch 37/300, seasonal_2 Loss: 0.0931 | 0.0485
Epoch 38/300, seasonal_2 Loss: 0.0954 | 0.0482
Epoch 39/300, seasonal_2 Loss: 0.0891 | 0.0401
Epoch 40/300, seasonal_2 Loss: 0.1076 | 0.0543
Epoch 41/300, seasonal_2 Loss: 0.1040 | 0.0507
Epoch 42/300, seasonal_2 Loss: 0.0984 | 0.0833
Epoch 43/300, seasonal_2 Loss: 0.1080 | 0.0505
Epoch 44/300, seasonal_2 Loss: 0.0940 | 0.0403
Epoch 45/300, seasonal_2 Loss: 0.0874 | 0.0415
Epoch 46/300, seasonal_2 Loss: 0.0861 | 0.0382
Epoch 47/300, seasonal_2 Loss: 0.0818 | 0.0385
Epoch 48/300, seasonal_2 Loss: 0.0803 | 0.0412
Epoch 49/300, seasonal_2 Loss: 0.0792 | 0.0360
Epoch 50/300, seasonal_2 Loss: 0.0779 | 0.0363
Epoch 51/300, seasonal_2 Loss: 0.0775 | 0.0360
Epoch 52/300, seasonal_2 Loss: 0.0768 | 0.0370
Epoch 53/300, seasonal_2 Loss: 0.0762 | 0.0366
Epoch 54/300, seasonal_2 Loss: 0.0760 | 0.0354
Epoch 55/300, seasonal_2 Loss: 0.0756 | 0.0350
Epoch 56/300, seasonal_2 Loss: 0.0754 | 0.0347
Epoch 57/300, seasonal_2 Loss: 0.0745 | 0.0334
Epoch 58/300, seasonal_2 Loss: 0.0741 | 0.0337
Epoch 59/300, seasonal_2 Loss: 0.0744 | 0.0346
Epoch 60/300, seasonal_2 Loss: 0.0751 | 0.0355
Epoch 61/300, seasonal_2 Loss: 0.0765 | 0.0354
Epoch 62/300, seasonal_2 Loss: 0.0761 | 0.0336
Epoch 63/300, seasonal_2 Loss: 0.0761 | 0.0351
Epoch 64/300, seasonal_2 Loss: 0.0766 | 0.0345
Epoch 65/300, seasonal_2 Loss: 0.0779 | 0.0453
Epoch 66/300, seasonal_2 Loss: 0.0827 | 0.0580
Epoch 67/300, seasonal_2 Loss: 0.0817 | 0.0377
Epoch 68/300, seasonal_2 Loss: 0.0748 | 0.0345
Epoch 69/300, seasonal_2 Loss: 0.0823 | 0.0360
Epoch 70/300, seasonal_2 Loss: 0.0822 | 0.0360
Epoch 71/300, seasonal_2 Loss: 0.0793 | 0.0344
Epoch 72/300, seasonal_2 Loss: 0.0761 | 0.0418
Epoch 73/300, seasonal_2 Loss: 0.0845 | 0.0450
Epoch 74/300, seasonal_2 Loss: 0.0831 | 0.0436
Epoch 75/300, seasonal_2 Loss: 0.0876 | 0.0451
Epoch 76/300, seasonal_2 Loss: 0.0838 | 0.0429
Epoch 77/300, seasonal_2 Loss: 0.0795 | 0.0376
Epoch 78/300, seasonal_2 Loss: 0.0780 | 0.0417
Epoch 79/300, seasonal_2 Loss: 0.0786 | 0.0367
Epoch 80/300, seasonal_2 Loss: 0.0761 | 0.0337
Epoch 81/300, seasonal_2 Loss: 0.0737 | 0.0329
Epoch 82/300, seasonal_2 Loss: 0.0714 | 0.0315
Epoch 83/300, seasonal_2 Loss: 0.0708 | 0.0341
Epoch 84/300, seasonal_2 Loss: 0.0696 | 0.0318
Epoch 85/300, seasonal_2 Loss: 0.0689 | 0.0339
Epoch 86/300, seasonal_2 Loss: 0.0688 | 0.0308
Epoch 87/300, seasonal_2 Loss: 0.0686 | 0.0300
Epoch 88/300, seasonal_2 Loss: 0.0682 | 0.0317
Epoch 89/300, seasonal_2 Loss: 0.0674 | 0.0297
Epoch 90/300, seasonal_2 Loss: 0.0679 | 0.0315
Epoch 91/300, seasonal_2 Loss: 0.0678 | 0.0315
Epoch 92/300, seasonal_2 Loss: 0.0671 | 0.0311
Epoch 93/300, seasonal_2 Loss: 0.0665 | 0.0302
Epoch 94/300, seasonal_2 Loss: 0.0661 | 0.0296
Epoch 95/300, seasonal_2 Loss: 0.0654 | 0.0289
Epoch 96/300, seasonal_2 Loss: 0.0653 | 0.0316
Epoch 97/300, seasonal_2 Loss: 0.0661 | 0.0303
Epoch 98/300, seasonal_2 Loss: 0.0669 | 0.0298
Epoch 99/300, seasonal_2 Loss: 0.0675 | 0.0345
Epoch 100/300, seasonal_2 Loss: 0.0689 | 0.0308
Epoch 101/300, seasonal_2 Loss: 0.0699 | 0.0316
Epoch 102/300, seasonal_2 Loss: 0.0674 | 0.0328
Epoch 103/300, seasonal_2 Loss: 0.0713 | 0.0313
Epoch 104/300, seasonal_2 Loss: 0.0745 | 0.0344
Epoch 105/300, seasonal_2 Loss: 0.0733 | 0.0301
Epoch 106/300, seasonal_2 Loss: 0.0692 | 0.0329
Epoch 107/300, seasonal_2 Loss: 0.0673 | 0.0331
Epoch 108/300, seasonal_2 Loss: 0.0712 | 0.0352
Epoch 109/300, seasonal_2 Loss: 0.0701 | 0.0286
Epoch 110/300, seasonal_2 Loss: 0.0678 | 0.0319
Epoch 111/300, seasonal_2 Loss: 0.0666 | 0.0306
Epoch 112/300, seasonal_2 Loss: 0.0667 | 0.0296
Epoch 113/300, seasonal_2 Loss: 0.0688 | 0.0358
Epoch 114/300, seasonal_2 Loss: 0.0692 | 0.0394
Epoch 115/300, seasonal_2 Loss: 0.0679 | 0.0287
Epoch 116/300, seasonal_2 Loss: 0.0652 | 0.0278
Epoch 117/300, seasonal_2 Loss: 0.0650 | 0.0302
Epoch 118/300, seasonal_2 Loss: 0.0668 | 0.0283
Epoch 119/300, seasonal_2 Loss: 0.0646 | 0.0286
Epoch 120/300, seasonal_2 Loss: 0.0629 | 0.0291
Epoch 121/300, seasonal_2 Loss: 0.0627 | 0.0300
Epoch 122/300, seasonal_2 Loss: 0.0634 | 0.0263
Epoch 123/300, seasonal_2 Loss: 0.0677 | 0.0330
Epoch 124/300, seasonal_2 Loss: 0.0697 | 0.0351
Epoch 125/300, seasonal_2 Loss: 0.0677 | 0.0276
Epoch 126/300, seasonal_2 Loss: 0.0689 | 0.0307
Epoch 127/300, seasonal_2 Loss: 0.0668 | 0.0295
Epoch 128/300, seasonal_2 Loss: 0.0651 | 0.0403
Epoch 129/300, seasonal_2 Loss: 0.0647 | 0.0258
Epoch 130/300, seasonal_2 Loss: 0.0621 | 0.0256
Epoch 131/300, seasonal_2 Loss: 0.0634 | 0.0246
Epoch 132/300, seasonal_2 Loss: 0.0608 | 0.0241
Epoch 133/300, seasonal_2 Loss: 0.0613 | 0.0270
Epoch 134/300, seasonal_2 Loss: 0.0639 | 0.0312
Epoch 135/300, seasonal_2 Loss: 0.0619 | 0.0287
Epoch 136/300, seasonal_2 Loss: 0.0630 | 0.0267
Epoch 137/300, seasonal_2 Loss: 0.0617 | 0.0256
Epoch 138/300, seasonal_2 Loss: 0.0642 | 0.0349
Epoch 139/300, seasonal_2 Loss: 0.0624 | 0.0270
Epoch 140/300, seasonal_2 Loss: 0.0606 | 0.0249
Epoch 141/300, seasonal_2 Loss: 0.0609 | 0.0307
Epoch 142/300, seasonal_2 Loss: 0.0613 | 0.0258
Epoch 143/300, seasonal_2 Loss: 0.0628 | 0.0305
Epoch 144/300, seasonal_2 Loss: 0.0647 | 0.0272
Epoch 145/300, seasonal_2 Loss: 0.0638 | 0.0285
Epoch 146/300, seasonal_2 Loss: 0.0633 | 0.0262
Epoch 147/300, seasonal_2 Loss: 0.0607 | 0.0242
Epoch 148/300, seasonal_2 Loss: 0.0599 | 0.0275
Epoch 149/300, seasonal_2 Loss: 0.0607 | 0.0266
Epoch 150/300, seasonal_2 Loss: 0.0584 | 0.0239
Epoch 151/300, seasonal_2 Loss: 0.0576 | 0.0237
Epoch 152/300, seasonal_2 Loss: 0.0577 | 0.0248
Epoch 153/300, seasonal_2 Loss: 0.0577 | 0.0237
Epoch 154/300, seasonal_2 Loss: 0.0570 | 0.0238
Epoch 155/300, seasonal_2 Loss: 0.0572 | 0.0271
Epoch 156/300, seasonal_2 Loss: 0.0582 | 0.0267
Epoch 157/300, seasonal_2 Loss: 0.0582 | 0.0247
Epoch 158/300, seasonal_2 Loss: 0.0572 | 0.0241
Epoch 159/300, seasonal_2 Loss: 0.0561 | 0.0249
Epoch 160/300, seasonal_2 Loss: 0.0569 | 0.0243
Epoch 161/300, seasonal_2 Loss: 0.0589 | 0.0266
Epoch 162/300, seasonal_2 Loss: 0.0594 | 0.0256
Epoch 163/300, seasonal_2 Loss: 0.0579 | 0.0255
Epoch 164/300, seasonal_2 Loss: 0.0566 | 0.0228
Epoch 165/300, seasonal_2 Loss: 0.0570 | 0.0243
Epoch 166/300, seasonal_2 Loss: 0.0571 | 0.0237
Epoch 167/300, seasonal_2 Loss: 0.0563 | 0.0227
Epoch 168/300, seasonal_2 Loss: 0.0554 | 0.0236
Epoch 169/300, seasonal_2 Loss: 0.0567 | 0.0253
Epoch 170/300, seasonal_2 Loss: 0.0574 | 0.0289
Epoch 171/300, seasonal_2 Loss: 0.0566 | 0.0294
Epoch 172/300, seasonal_2 Loss: 0.0558 | 0.0230
Epoch 173/300, seasonal_2 Loss: 0.0560 | 0.0254
Epoch 174/300, seasonal_2 Loss: 0.0574 | 0.0242
Epoch 175/300, seasonal_2 Loss: 0.0563 | 0.0233
Epoch 176/300, seasonal_2 Loss: 0.0556 | 0.0249
Epoch 177/300, seasonal_2 Loss: 0.0577 | 0.0258
Epoch 178/300, seasonal_2 Loss: 0.0575 | 0.0279
Epoch 179/300, seasonal_2 Loss: 0.0570 | 0.0269
Epoch 180/300, seasonal_2 Loss: 0.0588 | 0.0292
Epoch 181/300, seasonal_2 Loss: 0.0578 | 0.0228
Epoch 182/300, seasonal_2 Loss: 0.0582 | 0.0270
Epoch 183/300, seasonal_2 Loss: 0.0587 | 0.0281
Epoch 184/300, seasonal_2 Loss: 0.0575 | 0.0236
Epoch 185/300, seasonal_2 Loss: 0.0558 | 0.0265
Epoch 186/300, seasonal_2 Loss: 0.0559 | 0.0227
Epoch 187/300, seasonal_2 Loss: 0.0561 | 0.0250
Epoch 188/300, seasonal_2 Loss: 0.0559 | 0.0235
Epoch 189/300, seasonal_2 Loss: 0.0550 | 0.0232
Epoch 190/300, seasonal_2 Loss: 0.0559 | 0.0257
Epoch 191/300, seasonal_2 Loss: 0.0562 | 0.0243
Epoch 192/300, seasonal_2 Loss: 0.0563 | 0.0253
Epoch 193/300, seasonal_2 Loss: 0.0570 | 0.0258
Epoch 194/300, seasonal_2 Loss: 0.0565 | 0.0236
Epoch 195/300, seasonal_2 Loss: 0.0559 | 0.0249
Epoch 196/300, seasonal_2 Loss: 0.0555 | 0.0240
Epoch 197/300, seasonal_2 Loss: 0.0547 | 0.0246
Epoch 198/300, seasonal_2 Loss: 0.0560 | 0.0272
Epoch 199/300, seasonal_2 Loss: 0.0563 | 0.0251
Epoch 200/300, seasonal_2 Loss: 0.0554 | 0.0240
Epoch 201/300, seasonal_2 Loss: 0.0551 | 0.0244
Epoch 202/300, seasonal_2 Loss: 0.0547 | 0.0237
Epoch 203/300, seasonal_2 Loss: 0.0538 | 0.0238
Epoch 204/300, seasonal_2 Loss: 0.0534 | 0.0240
Epoch 205/300, seasonal_2 Loss: 0.0536 | 0.0264
Epoch 206/300, seasonal_2 Loss: 0.0539 | 0.0247
Epoch 207/300, seasonal_2 Loss: 0.0530 | 0.0228
Epoch 208/300, seasonal_2 Loss: 0.0535 | 0.0233
Epoch 209/300, seasonal_2 Loss: 0.0542 | 0.0226
Epoch 210/300, seasonal_2 Loss: 0.0531 | 0.0239
Epoch 211/300, seasonal_2 Loss: 0.0529 | 0.0250
Epoch 212/300, seasonal_2 Loss: 0.0524 | 0.0245
Epoch 213/300, seasonal_2 Loss: 0.0524 | 0.0232
Epoch 214/300, seasonal_2 Loss: 0.0526 | 0.0238
Epoch 215/300, seasonal_2 Loss: 0.0525 | 0.0233
Epoch 216/300, seasonal_2 Loss: 0.0520 | 0.0236
Epoch 217/300, seasonal_2 Loss: 0.0520 | 0.0242
Epoch 218/300, seasonal_2 Loss: 0.0518 | 0.0240
Epoch 219/300, seasonal_2 Loss: 0.0517 | 0.0231
Epoch 220/300, seasonal_2 Loss: 0.0517 | 0.0239
Epoch 221/300, seasonal_2 Loss: 0.0518 | 0.0232
Epoch 222/300, seasonal_2 Loss: 0.0515 | 0.0237
Epoch 223/300, seasonal_2 Loss: 0.0517 | 0.0258
Epoch 224/300, seasonal_2 Loss: 0.0517 | 0.0241
Epoch 225/300, seasonal_2 Loss: 0.0513 | 0.0242
Epoch 226/300, seasonal_2 Loss: 0.0509 | 0.0242
Epoch 227/300, seasonal_2 Loss: 0.0518 | 0.0250
Epoch 228/300, seasonal_2 Loss: 0.0525 | 0.0260
Epoch 229/300, seasonal_2 Loss: 0.0524 | 0.0238
Epoch 230/300, seasonal_2 Loss: 0.0512 | 0.0239
Epoch 231/300, seasonal_2 Loss: 0.0521 | 0.0261
Epoch 232/300, seasonal_2 Loss: 0.0542 | 0.0256
Epoch 233/300, seasonal_2 Loss: 0.0538 | 0.0265
Epoch 234/300, seasonal_2 Loss: 0.0521 | 0.0260
Epoch 235/300, seasonal_2 Loss: 0.0530 | 0.0243
Epoch 236/300, seasonal_2 Loss: 0.0549 | 0.0263
Epoch 237/300, seasonal_2 Loss: 0.0541 | 0.0239
Epoch 238/300, seasonal_2 Loss: 0.0524 | 0.0260
Epoch 239/300, seasonal_2 Loss: 0.0538 | 0.0254
Epoch 240/300, seasonal_2 Loss: 0.0555 | 0.0264
Epoch 241/300, seasonal_2 Loss: 0.0546 | 0.0298
Epoch 242/300, seasonal_2 Loss: 0.0524 | 0.0239
Epoch 243/300, seasonal_2 Loss: 0.0530 | 0.0257
Epoch 244/300, seasonal_2 Loss: 0.0546 | 0.0240
Epoch 245/300, seasonal_2 Loss: 0.0515 | 0.0236
Epoch 246/300, seasonal_2 Loss: 0.0508 | 0.0245
Epoch 247/300, seasonal_2 Loss: 0.0525 | 0.0254
Epoch 248/300, seasonal_2 Loss: 0.0520 | 0.0260
Epoch 249/300, seasonal_2 Loss: 0.0513 | 0.0255
Epoch 250/300, seasonal_2 Loss: 0.0516 | 0.0251
Epoch 251/300, seasonal_2 Loss: 0.0504 | 0.0237
Epoch 252/300, seasonal_2 Loss: 0.0501 | 0.0253
Epoch 253/300, seasonal_2 Loss: 0.0512 | 0.0238
Epoch 254/300, seasonal_2 Loss: 0.0501 | 0.0241
Epoch 255/300, seasonal_2 Loss: 0.0496 | 0.0253
Epoch 256/300, seasonal_2 Loss: 0.0500 | 0.0246
Epoch 257/300, seasonal_2 Loss: 0.0496 | 0.0241
Epoch 258/300, seasonal_2 Loss: 0.0494 | 0.0243
Epoch 259/300, seasonal_2 Loss: 0.0495 | 0.0251
Epoch 260/300, seasonal_2 Loss: 0.0493 | 0.0240
Epoch 261/300, seasonal_2 Loss: 0.0489 | 0.0244
Epoch 262/300, seasonal_2 Loss: 0.0494 | 0.0247
Epoch 263/300, seasonal_2 Loss: 0.0488 | 0.0236
Epoch 264/300, seasonal_2 Loss: 0.0487 | 0.0242
Epoch 265/300, seasonal_2 Loss: 0.0487 | 0.0238
Epoch 266/300, seasonal_2 Loss: 0.0487 | 0.0259
Epoch 267/300, seasonal_2 Loss: 0.0484 | 0.0254
Epoch 268/300, seasonal_2 Loss: 0.0483 | 0.0244
Epoch 269/300, seasonal_2 Loss: 0.0482 | 0.0238
Epoch 270/300, seasonal_2 Loss: 0.0483 | 0.0242
Epoch 271/300, seasonal_2 Loss: 0.0483 | 0.0236
Epoch 272/300, seasonal_2 Loss: 0.0480 | 0.0246
Epoch 273/300, seasonal_2 Loss: 0.0481 | 0.0257
Epoch 274/300, seasonal_2 Loss: 0.0481 | 0.0259
Epoch 275/300, seasonal_2 Loss: 0.0479 | 0.0250
Epoch 276/300, seasonal_2 Loss: 0.0478 | 0.0244
Epoch 277/300, seasonal_2 Loss: 0.0479 | 0.0245
Epoch 278/300, seasonal_2 Loss: 0.0479 | 0.0247
Epoch 279/300, seasonal_2 Loss: 0.0479 | 0.0242
Epoch 280/300, seasonal_2 Loss: 0.0477 | 0.0252
Epoch 281/300, seasonal_2 Loss: 0.0479 | 0.0260
Epoch 282/300, seasonal_2 Loss: 0.0478 | 0.0255
Epoch 283/300, seasonal_2 Loss: 0.0477 | 0.0259
Epoch 284/300, seasonal_2 Loss: 0.0473 | 0.0246
Epoch 285/300, seasonal_2 Loss: 0.0476 | 0.0263
Epoch 286/300, seasonal_2 Loss: 0.0479 | 0.0253
Epoch 287/300, seasonal_2 Loss: 0.0477 | 0.0246
Epoch 288/300, seasonal_2 Loss: 0.0474 | 0.0255
Epoch 289/300, seasonal_2 Loss: 0.0486 | 0.0260
Epoch 290/300, seasonal_2 Loss: 0.0487 | 0.0257
Epoch 291/300, seasonal_2 Loss: 0.0486 | 0.0283
Epoch 292/300, seasonal_2 Loss: 0.0483 | 0.0263
Epoch 293/300, seasonal_2 Loss: 0.0500 | 0.0294
Epoch 294/300, seasonal_2 Loss: 0.0508 | 0.0259
Epoch 295/300, seasonal_2 Loss: 0.0493 | 0.0247
Epoch 296/300, seasonal_2 Loss: 0.0490 | 0.0271
Epoch 297/300, seasonal_2 Loss: 0.0510 | 0.0270
Epoch 298/300, seasonal_2 Loss: 0.0499 | 0.0257
Epoch 299/300, seasonal_2 Loss: 0.0495 | 0.0286
Epoch 300/300, seasonal_2 Loss: 0.0495 | 0.0284
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.9042201225099494, 'learning_rate': 0.0007272198878977273, 'batch_size': 152, 'step_size': 12, 'gamma': 0.7538948756514965}
Epoch 1/300, seasonal_3 Loss: 0.5086 | 0.2915
Epoch 2/300, seasonal_3 Loss: 0.1804 | 0.1428
Epoch 3/300, seasonal_3 Loss: 0.1528 | 0.0898
Epoch 4/300, seasonal_3 Loss: 0.1404 | 0.0749
Epoch 5/300, seasonal_3 Loss: 0.1380 | 0.0738
Epoch 6/300, seasonal_3 Loss: 0.1281 | 0.0723
Epoch 7/300, seasonal_3 Loss: 0.1194 | 0.0693
Epoch 8/300, seasonal_3 Loss: 0.1091 | 0.0624
Epoch 9/300, seasonal_3 Loss: 0.1089 | 0.0680
Epoch 10/300, seasonal_3 Loss: 0.1128 | 0.0813
Epoch 11/300, seasonal_3 Loss: 0.1134 | 0.0831
Epoch 12/300, seasonal_3 Loss: 0.1084 | 0.0883
Epoch 13/300, seasonal_3 Loss: 0.1197 | 0.0972
Epoch 14/300, seasonal_3 Loss: 0.1214 | 0.1343
Epoch 15/300, seasonal_3 Loss: 0.1433 | 0.0690
Epoch 16/300, seasonal_3 Loss: 0.1561 | 0.1016
Epoch 17/300, seasonal_3 Loss: 0.1512 | 0.0631
Epoch 18/300, seasonal_3 Loss: 0.1215 | 0.0665
Epoch 19/300, seasonal_3 Loss: 0.1280 | 0.1228
Epoch 20/300, seasonal_3 Loss: 0.1285 | 0.1046
Epoch 21/300, seasonal_3 Loss: 0.1097 | 0.1010
Epoch 22/300, seasonal_3 Loss: 0.1042 | 0.0674
Epoch 23/300, seasonal_3 Loss: 0.0954 | 0.0492
Epoch 24/300, seasonal_3 Loss: 0.0942 | 0.0507
Epoch 25/300, seasonal_3 Loss: 0.0917 | 0.0475
Epoch 26/300, seasonal_3 Loss: 0.0897 | 0.0470
Epoch 27/300, seasonal_3 Loss: 0.0863 | 0.0515
Epoch 28/300, seasonal_3 Loss: 0.0880 | 0.0764
Epoch 29/300, seasonal_3 Loss: 0.0887 | 0.0725
Epoch 30/300, seasonal_3 Loss: 0.0845 | 0.0508
Epoch 31/300, seasonal_3 Loss: 0.0819 | 0.0436
Epoch 32/300, seasonal_3 Loss: 0.0821 | 0.0433
Epoch 33/300, seasonal_3 Loss: 0.0808 | 0.0471
Epoch 34/300, seasonal_3 Loss: 0.0803 | 0.0516
Epoch 35/300, seasonal_3 Loss: 0.0798 | 0.0480
Epoch 36/300, seasonal_3 Loss: 0.0791 | 0.0436
Epoch 37/300, seasonal_3 Loss: 0.0787 | 0.0424
Epoch 38/300, seasonal_3 Loss: 0.0785 | 0.0435
Epoch 39/300, seasonal_3 Loss: 0.0780 | 0.0440
Epoch 40/300, seasonal_3 Loss: 0.0777 | 0.0432
Epoch 41/300, seasonal_3 Loss: 0.0774 | 0.0423
Epoch 42/300, seasonal_3 Loss: 0.0773 | 0.0421
Epoch 43/300, seasonal_3 Loss: 0.0774 | 0.0428
Epoch 44/300, seasonal_3 Loss: 0.0782 | 0.0421
Epoch 45/300, seasonal_3 Loss: 0.0779 | 0.0411
Epoch 46/300, seasonal_3 Loss: 0.0763 | 0.0410
Epoch 47/300, seasonal_3 Loss: 0.0756 | 0.0406
Epoch 48/300, seasonal_3 Loss: 0.0752 | 0.0404
Epoch 49/300, seasonal_3 Loss: 0.0748 | 0.0402
Epoch 50/300, seasonal_3 Loss: 0.0746 | 0.0398
Epoch 51/300, seasonal_3 Loss: 0.0744 | 0.0396
Epoch 52/300, seasonal_3 Loss: 0.0742 | 0.0394
Epoch 53/300, seasonal_3 Loss: 0.0740 | 0.0392
Epoch 54/300, seasonal_3 Loss: 0.0738 | 0.0390
Epoch 55/300, seasonal_3 Loss: 0.0736 | 0.0388
Epoch 56/300, seasonal_3 Loss: 0.0735 | 0.0387
Epoch 57/300, seasonal_3 Loss: 0.0733 | 0.0386
Epoch 58/300, seasonal_3 Loss: 0.0732 | 0.0385
Epoch 59/300, seasonal_3 Loss: 0.0730 | 0.0383
Epoch 60/300, seasonal_3 Loss: 0.0729 | 0.0382
Epoch 61/300, seasonal_3 Loss: 0.0728 | 0.0380
Epoch 62/300, seasonal_3 Loss: 0.0726 | 0.0380
Epoch 63/300, seasonal_3 Loss: 0.0725 | 0.0379
Epoch 64/300, seasonal_3 Loss: 0.0724 | 0.0378
Epoch 65/300, seasonal_3 Loss: 0.0723 | 0.0377
Epoch 66/300, seasonal_3 Loss: 0.0722 | 0.0376
Epoch 67/300, seasonal_3 Loss: 0.0721 | 0.0375
Epoch 68/300, seasonal_3 Loss: 0.0720 | 0.0374
Epoch 69/300, seasonal_3 Loss: 0.0719 | 0.0373
Epoch 70/300, seasonal_3 Loss: 0.0718 | 0.0373
Epoch 71/300, seasonal_3 Loss: 0.0717 | 0.0372
Epoch 72/300, seasonal_3 Loss: 0.0716 | 0.0371
Epoch 73/300, seasonal_3 Loss: 0.0715 | 0.0370
Epoch 74/300, seasonal_3 Loss: 0.0715 | 0.0369
Epoch 75/300, seasonal_3 Loss: 0.0714 | 0.0369
Epoch 76/300, seasonal_3 Loss: 0.0713 | 0.0368
Epoch 77/300, seasonal_3 Loss: 0.0713 | 0.0368
Epoch 78/300, seasonal_3 Loss: 0.0712 | 0.0367
Epoch 79/300, seasonal_3 Loss: 0.0711 | 0.0366
Epoch 80/300, seasonal_3 Loss: 0.0711 | 0.0366
Epoch 81/300, seasonal_3 Loss: 0.0710 | 0.0365
Epoch 82/300, seasonal_3 Loss: 0.0709 | 0.0365
Epoch 83/300, seasonal_3 Loss: 0.0709 | 0.0364
Epoch 84/300, seasonal_3 Loss: 0.0708 | 0.0364
Epoch 85/300, seasonal_3 Loss: 0.0708 | 0.0363
Epoch 86/300, seasonal_3 Loss: 0.0707 | 0.0363
Epoch 87/300, seasonal_3 Loss: 0.0707 | 0.0363
Epoch 88/300, seasonal_3 Loss: 0.0706 | 0.0362
Epoch 89/300, seasonal_3 Loss: 0.0706 | 0.0362
Epoch 90/300, seasonal_3 Loss: 0.0705 | 0.0361
Epoch 91/300, seasonal_3 Loss: 0.0705 | 0.0361
Epoch 92/300, seasonal_3 Loss: 0.0704 | 0.0361
Epoch 93/300, seasonal_3 Loss: 0.0704 | 0.0360
Epoch 94/300, seasonal_3 Loss: 0.0704 | 0.0360
Epoch 95/300, seasonal_3 Loss: 0.0703 | 0.0360
Epoch 96/300, seasonal_3 Loss: 0.0703 | 0.0359
Epoch 97/300, seasonal_3 Loss: 0.0703 | 0.0359
Epoch 98/300, seasonal_3 Loss: 0.0702 | 0.0359
Epoch 99/300, seasonal_3 Loss: 0.0702 | 0.0359
Epoch 100/300, seasonal_3 Loss: 0.0702 | 0.0358
Epoch 101/300, seasonal_3 Loss: 0.0701 | 0.0358
Epoch 102/300, seasonal_3 Loss: 0.0701 | 0.0358
Epoch 103/300, seasonal_3 Loss: 0.0701 | 0.0358
Epoch 104/300, seasonal_3 Loss: 0.0701 | 0.0357
Epoch 105/300, seasonal_3 Loss: 0.0700 | 0.0357
Epoch 106/300, seasonal_3 Loss: 0.0700 | 0.0357
Epoch 107/300, seasonal_3 Loss: 0.0700 | 0.0357
Epoch 108/300, seasonal_3 Loss: 0.0700 | 0.0357
Epoch 109/300, seasonal_3 Loss: 0.0699 | 0.0357
Epoch 110/300, seasonal_3 Loss: 0.0699 | 0.0356
Epoch 111/300, seasonal_3 Loss: 0.0699 | 0.0356
Epoch 112/300, seasonal_3 Loss: 0.0699 | 0.0356
Epoch 113/300, seasonal_3 Loss: 0.0699 | 0.0356
Epoch 114/300, seasonal_3 Loss: 0.0699 | 0.0356
Epoch 115/300, seasonal_3 Loss: 0.0698 | 0.0356
Epoch 116/300, seasonal_3 Loss: 0.0698 | 0.0355
Epoch 117/300, seasonal_3 Loss: 0.0698 | 0.0355
Epoch 118/300, seasonal_3 Loss: 0.0698 | 0.0355
Epoch 119/300, seasonal_3 Loss: 0.0698 | 0.0355
Epoch 120/300, seasonal_3 Loss: 0.0698 | 0.0355
Epoch 121/300, seasonal_3 Loss: 0.0697 | 0.0355
Epoch 122/300, seasonal_3 Loss: 0.0697 | 0.0355
Epoch 123/300, seasonal_3 Loss: 0.0697 | 0.0355
Epoch 124/300, seasonal_3 Loss: 0.0697 | 0.0355
Epoch 125/300, seasonal_3 Loss: 0.0697 | 0.0354
Epoch 126/300, seasonal_3 Loss: 0.0697 | 0.0354
Epoch 127/300, seasonal_3 Loss: 0.0697 | 0.0354
Epoch 128/300, seasonal_3 Loss: 0.0697 | 0.0354
Epoch 129/300, seasonal_3 Loss: 0.0697 | 0.0354
Epoch 130/300, seasonal_3 Loss: 0.0697 | 0.0354
Epoch 131/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 132/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 133/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 134/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 135/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 136/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 137/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 138/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 139/300, seasonal_3 Loss: 0.0696 | 0.0354
Epoch 140/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 141/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 142/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 143/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 144/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 145/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 146/300, seasonal_3 Loss: 0.0696 | 0.0353
Epoch 147/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 148/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 149/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 150/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 151/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 152/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 153/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 154/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 155/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 156/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 157/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 158/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 159/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 160/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 161/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 162/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 163/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 164/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 165/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 166/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 167/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 168/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 169/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 170/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 171/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 172/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 173/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 174/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 175/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 176/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 177/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 178/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 179/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 180/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 181/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 182/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 183/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 184/300, seasonal_3 Loss: 0.0695 | 0.0353
Epoch 185/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 186/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 187/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 188/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 189/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 190/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 191/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 192/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 193/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 194/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 195/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 196/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 197/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 198/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 199/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 200/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 201/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 202/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 203/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 204/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 205/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 206/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 207/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 208/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 209/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 210/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 211/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 212/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 213/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 214/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 215/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 216/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 217/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 218/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 219/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 220/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 221/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 222/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 223/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 224/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 225/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 226/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 227/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 228/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 229/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 230/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 231/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 232/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 233/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 234/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 235/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 236/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 237/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 238/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 239/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 240/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 241/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 242/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 243/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 244/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 245/300, seasonal_3 Loss: 0.0695 | 0.0352
Epoch 246/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 247/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 248/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 249/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 250/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 251/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 252/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 253/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 254/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 255/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 256/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 257/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 258/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 259/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 260/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 261/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 262/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 263/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 264/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 265/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 266/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 267/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 268/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 269/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 270/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 271/300, seasonal_3 Loss: 0.0694 | 0.0352
Epoch 272/300, seasonal_3 Loss: 0.0694 | 0.0352
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9361041806612609, 'learning_rate': 0.0008924454046804217, 'batch_size': 100, 'step_size': 11, 'gamma': 0.7544088787253677}
Epoch 1/300, resid Loss: 0.3772 | 0.1046
Epoch 2/300, resid Loss: 0.1411 | 0.1092
Epoch 3/300, resid Loss: 0.1197 | 0.0879
Epoch 4/300, resid Loss: 0.1166 | 0.0823
Epoch 5/300, resid Loss: 0.1259 | 0.0730
Epoch 6/300, resid Loss: 0.1305 | 0.0801
Epoch 7/300, resid Loss: 0.1084 | 0.0579
Epoch 8/300, resid Loss: 0.0997 | 0.0541
Epoch 9/300, resid Loss: 0.0962 | 0.0528
Epoch 10/300, resid Loss: 0.0959 | 0.0552
Epoch 11/300, resid Loss: 0.0963 | 0.0575
Epoch 12/300, resid Loss: 0.0976 | 0.0532
Epoch 13/300, resid Loss: 0.0964 | 0.0498
Epoch 14/300, resid Loss: 0.0927 | 0.0475
Epoch 15/300, resid Loss: 0.0937 | 0.0447
Epoch 16/300, resid Loss: 0.0986 | 0.0471
Epoch 17/300, resid Loss: 0.0981 | 0.0460
Epoch 18/300, resid Loss: 0.0984 | 0.0416
Epoch 19/300, resid Loss: 0.0901 | 0.0411
Epoch 20/300, resid Loss: 0.0843 | 0.0411
Epoch 21/300, resid Loss: 0.0818 | 0.0394
Epoch 22/300, resid Loss: 0.0810 | 0.0393
Epoch 23/300, resid Loss: 0.0802 | 0.0379
Epoch 24/300, resid Loss: 0.0797 | 0.0395
Epoch 25/300, resid Loss: 0.0795 | 0.0397
Epoch 26/300, resid Loss: 0.0795 | 0.0405
Epoch 27/300, resid Loss: 0.0810 | 0.0406
Epoch 28/300, resid Loss: 0.0817 | 0.0429
Epoch 29/300, resid Loss: 0.0865 | 0.0453
Epoch 30/300, resid Loss: 0.0863 | 0.0534
Epoch 31/300, resid Loss: 0.0889 | 0.0549
Epoch 32/300, resid Loss: 0.0844 | 0.0519
Epoch 33/300, resid Loss: 0.0821 | 0.0525
Epoch 34/300, resid Loss: 0.0806 | 0.0490
Epoch 35/300, resid Loss: 0.0810 | 0.0455
Epoch 36/300, resid Loss: 0.0825 | 0.0396
Epoch 37/300, resid Loss: 0.0874 | 0.0381
Epoch 38/300, resid Loss: 0.0877 | 0.0393
Epoch 39/300, resid Loss: 0.0862 | 0.0387
Epoch 40/300, resid Loss: 0.0855 | 0.0392
Epoch 41/300, resid Loss: 0.0789 | 0.0377
Epoch 42/300, resid Loss: 0.0756 | 0.0378
Epoch 43/300, resid Loss: 0.0764 | 0.0385
Epoch 44/300, resid Loss: 0.0757 | 0.0387
Epoch 45/300, resid Loss: 0.0739 | 0.0380
Epoch 46/300, resid Loss: 0.0716 | 0.0360
Epoch 47/300, resid Loss: 0.0707 | 0.0354
Epoch 48/300, resid Loss: 0.0705 | 0.0352
Epoch 49/300, resid Loss: 0.0703 | 0.0352
Epoch 50/300, resid Loss: 0.0701 | 0.0350
Epoch 51/300, resid Loss: 0.0698 | 0.0348
Epoch 52/300, resid Loss: 0.0696 | 0.0348
Epoch 53/300, resid Loss: 0.0694 | 0.0347
Epoch 54/300, resid Loss: 0.0693 | 0.0346
Epoch 55/300, resid Loss: 0.0691 | 0.0346
Epoch 56/300, resid Loss: 0.0689 | 0.0346
Epoch 57/300, resid Loss: 0.0689 | 0.0346
Epoch 58/300, resid Loss: 0.0688 | 0.0347
Epoch 59/300, resid Loss: 0.0688 | 0.0347
Epoch 60/300, resid Loss: 0.0688 | 0.0347
Epoch 61/300, resid Loss: 0.0687 | 0.0346
Epoch 62/300, resid Loss: 0.0686 | 0.0341
Epoch 63/300, resid Loss: 0.0683 | 0.0341
Epoch 64/300, resid Loss: 0.0681 | 0.0340
Epoch 65/300, resid Loss: 0.0679 | 0.0339
Epoch 66/300, resid Loss: 0.0677 | 0.0338
Epoch 67/300, resid Loss: 0.0676 | 0.0336
Epoch 68/300, resid Loss: 0.0675 | 0.0336
Epoch 69/300, resid Loss: 0.0674 | 0.0335
Epoch 70/300, resid Loss: 0.0673 | 0.0335
Epoch 71/300, resid Loss: 0.0672 | 0.0334
Epoch 72/300, resid Loss: 0.0671 | 0.0334
Epoch 73/300, resid Loss: 0.0670 | 0.0332
Epoch 74/300, resid Loss: 0.0669 | 0.0331
Epoch 75/300, resid Loss: 0.0668 | 0.0331
Epoch 76/300, resid Loss: 0.0667 | 0.0330
Epoch 77/300, resid Loss: 0.0666 | 0.0330
Epoch 78/300, resid Loss: 0.0666 | 0.0329
Epoch 79/300, resid Loss: 0.0665 | 0.0328
Epoch 80/300, resid Loss: 0.0664 | 0.0328
Epoch 81/300, resid Loss: 0.0664 | 0.0328
Epoch 82/300, resid Loss: 0.0663 | 0.0327
Epoch 83/300, resid Loss: 0.0662 | 0.0327
Epoch 84/300, resid Loss: 0.0662 | 0.0326
Epoch 85/300, resid Loss: 0.0661 | 0.0326
Epoch 86/300, resid Loss: 0.0660 | 0.0326
Epoch 87/300, resid Loss: 0.0660 | 0.0325
Epoch 88/300, resid Loss: 0.0659 | 0.0325
Epoch 89/300, resid Loss: 0.0659 | 0.0325
Epoch 90/300, resid Loss: 0.0658 | 0.0324
Epoch 91/300, resid Loss: 0.0658 | 0.0324
Epoch 92/300, resid Loss: 0.0658 | 0.0324
Epoch 93/300, resid Loss: 0.0657 | 0.0324
Epoch 94/300, resid Loss: 0.0657 | 0.0324
Epoch 95/300, resid Loss: 0.0656 | 0.0323
Epoch 96/300, resid Loss: 0.0656 | 0.0323
Epoch 97/300, resid Loss: 0.0656 | 0.0323
Epoch 98/300, resid Loss: 0.0655 | 0.0323
Epoch 99/300, resid Loss: 0.0655 | 0.0323
Epoch 100/300, resid Loss: 0.0655 | 0.0322
Epoch 101/300, resid Loss: 0.0654 | 0.0322
Epoch 102/300, resid Loss: 0.0654 | 0.0322
Epoch 103/300, resid Loss: 0.0654 | 0.0322
Epoch 104/300, resid Loss: 0.0654 | 0.0322
Epoch 105/300, resid Loss: 0.0653 | 0.0322
Epoch 106/300, resid Loss: 0.0653 | 0.0322
Epoch 107/300, resid Loss: 0.0653 | 0.0322
Epoch 108/300, resid Loss: 0.0653 | 0.0321
Epoch 109/300, resid Loss: 0.0653 | 0.0321
Epoch 110/300, resid Loss: 0.0652 | 0.0321
Epoch 111/300, resid Loss: 0.0652 | 0.0321
Epoch 112/300, resid Loss: 0.0652 | 0.0321
Epoch 113/300, resid Loss: 0.0652 | 0.0321
Epoch 114/300, resid Loss: 0.0652 | 0.0321
Epoch 115/300, resid Loss: 0.0652 | 0.0321
Epoch 116/300, resid Loss: 0.0651 | 0.0321
Epoch 117/300, resid Loss: 0.0651 | 0.0321
Epoch 118/300, resid Loss: 0.0651 | 0.0321
Epoch 119/300, resid Loss: 0.0651 | 0.0320
Epoch 120/300, resid Loss: 0.0651 | 0.0320
Epoch 121/300, resid Loss: 0.0651 | 0.0320
Epoch 122/300, resid Loss: 0.0651 | 0.0320
Epoch 123/300, resid Loss: 0.0651 | 0.0320
Epoch 124/300, resid Loss: 0.0650 | 0.0320
Epoch 125/300, resid Loss: 0.0650 | 0.0320
Epoch 126/300, resid Loss: 0.0650 | 0.0320
Epoch 127/300, resid Loss: 0.0650 | 0.0320
Epoch 128/300, resid Loss: 0.0650 | 0.0320
Epoch 129/300, resid Loss: 0.0650 | 0.0320
Epoch 130/300, resid Loss: 0.0650 | 0.0320
Epoch 131/300, resid Loss: 0.0650 | 0.0320
Epoch 132/300, resid Loss: 0.0650 | 0.0320
Epoch 133/300, resid Loss: 0.0650 | 0.0320
Epoch 134/300, resid Loss: 0.0650 | 0.0320
Epoch 135/300, resid Loss: 0.0650 | 0.0320
Epoch 136/300, resid Loss: 0.0650 | 0.0320
Epoch 137/300, resid Loss: 0.0650 | 0.0320
Epoch 138/300, resid Loss: 0.0649 | 0.0320
Epoch 139/300, resid Loss: 0.0649 | 0.0320
Epoch 140/300, resid Loss: 0.0649 | 0.0319
Epoch 141/300, resid Loss: 0.0649 | 0.0319
Epoch 142/300, resid Loss: 0.0649 | 0.0319
Epoch 143/300, resid Loss: 0.0649 | 0.0319
Epoch 144/300, resid Loss: 0.0649 | 0.0319
Epoch 145/300, resid Loss: 0.0649 | 0.0319
Epoch 146/300, resid Loss: 0.0649 | 0.0319
Epoch 147/300, resid Loss: 0.0649 | 0.0319
Epoch 148/300, resid Loss: 0.0649 | 0.0319
Epoch 149/300, resid Loss: 0.0649 | 0.0319
Epoch 150/300, resid Loss: 0.0649 | 0.0319
Epoch 151/300, resid Loss: 0.0649 | 0.0319
Epoch 152/300, resid Loss: 0.0649 | 0.0319
Epoch 153/300, resid Loss: 0.0649 | 0.0319
Epoch 154/300, resid Loss: 0.0649 | 0.0319
Epoch 155/300, resid Loss: 0.0649 | 0.0319
Epoch 156/300, resid Loss: 0.0649 | 0.0319
Epoch 157/300, resid Loss: 0.0649 | 0.0319
Epoch 158/300, resid Loss: 0.0649 | 0.0319
Epoch 159/300, resid Loss: 0.0649 | 0.0319
Epoch 160/300, resid Loss: 0.0649 | 0.0319
Epoch 161/300, resid Loss: 0.0649 | 0.0319
Epoch 162/300, resid Loss: 0.0649 | 0.0319
Epoch 163/300, resid Loss: 0.0649 | 0.0319
Epoch 164/300, resid Loss: 0.0649 | 0.0319
Epoch 165/300, resid Loss: 0.0649 | 0.0319
Epoch 166/300, resid Loss: 0.0649 | 0.0319
Epoch 167/300, resid Loss: 0.0649 | 0.0319
Epoch 168/300, resid Loss: 0.0649 | 0.0319
Epoch 169/300, resid Loss: 0.0649 | 0.0319
Epoch 170/300, resid Loss: 0.0649 | 0.0319
Epoch 171/300, resid Loss: 0.0649 | 0.0319
Epoch 172/300, resid Loss: 0.0649 | 0.0319
Epoch 173/300, resid Loss: 0.0649 | 0.0319
Epoch 174/300, resid Loss: 0.0649 | 0.0319
Epoch 175/300, resid Loss: 0.0649 | 0.0319
Epoch 176/300, resid Loss: 0.0649 | 0.0319
Epoch 177/300, resid Loss: 0.0649 | 0.0319
Epoch 178/300, resid Loss: 0.0649 | 0.0319
Epoch 179/300, resid Loss: 0.0649 | 0.0319
Epoch 180/300, resid Loss: 0.0649 | 0.0319
Epoch 181/300, resid Loss: 0.0649 | 0.0319
Epoch 182/300, resid Loss: 0.0649 | 0.0319
Epoch 183/300, resid Loss: 0.0649 | 0.0319
Epoch 184/300, resid Loss: 0.0649 | 0.0319
Epoch 185/300, resid Loss: 0.0649 | 0.0319
Epoch 186/300, resid Loss: 0.0648 | 0.0319
Epoch 187/300, resid Loss: 0.0648 | 0.0319
Epoch 188/300, resid Loss: 0.0648 | 0.0319
Epoch 189/300, resid Loss: 0.0648 | 0.0319
Epoch 190/300, resid Loss: 0.0648 | 0.0319
Epoch 191/300, resid Loss: 0.0648 | 0.0319
Epoch 192/300, resid Loss: 0.0648 | 0.0319
Epoch 193/300, resid Loss: 0.0648 | 0.0319
Epoch 194/300, resid Loss: 0.0648 | 0.0319
Epoch 195/300, resid Loss: 0.0648 | 0.0319
Epoch 196/300, resid Loss: 0.0648 | 0.0319
Epoch 197/300, resid Loss: 0.0648 | 0.0319
Epoch 198/300, resid Loss: 0.0648 | 0.0319
Epoch 199/300, resid Loss: 0.0648 | 0.0319
Epoch 200/300, resid Loss: 0.0648 | 0.0319
Epoch 201/300, resid Loss: 0.0648 | 0.0319
Epoch 202/300, resid Loss: 0.0648 | 0.0319
Epoch 203/300, resid Loss: 0.0648 | 0.0319
Epoch 204/300, resid Loss: 0.0648 | 0.0319
Epoch 205/300, resid Loss: 0.0648 | 0.0319
Epoch 206/300, resid Loss: 0.0648 | 0.0319
Epoch 207/300, resid Loss: 0.0648 | 0.0319
Epoch 208/300, resid Loss: 0.0648 | 0.0319
Epoch 209/300, resid Loss: 0.0648 | 0.0319
Epoch 210/300, resid Loss: 0.0648 | 0.0319
Epoch 211/300, resid Loss: 0.0648 | 0.0319
Epoch 212/300, resid Loss: 0.0648 | 0.0319
Epoch 213/300, resid Loss: 0.0648 | 0.0319
Epoch 214/300, resid Loss: 0.0648 | 0.0319
Epoch 215/300, resid Loss: 0.0648 | 0.0319
Epoch 216/300, resid Loss: 0.0648 | 0.0319
Epoch 217/300, resid Loss: 0.0648 | 0.0319
Epoch 218/300, resid Loss: 0.0648 | 0.0319
Epoch 219/300, resid Loss: 0.0648 | 0.0319
Epoch 220/300, resid Loss: 0.0648 | 0.0319
Epoch 221/300, resid Loss: 0.0648 | 0.0319
Epoch 222/300, resid Loss: 0.0648 | 0.0319
Epoch 223/300, resid Loss: 0.0648 | 0.0319
Epoch 224/300, resid Loss: 0.0648 | 0.0319
Epoch 225/300, resid Loss: 0.0648 | 0.0319
Epoch 226/300, resid Loss: 0.0648 | 0.0319
Epoch 227/300, resid Loss: 0.0648 | 0.0319
Epoch 228/300, resid Loss: 0.0648 | 0.0319
Epoch 229/300, resid Loss: 0.0648 | 0.0319
Epoch 230/300, resid Loss: 0.0648 | 0.0319
Epoch 231/300, resid Loss: 0.0648 | 0.0319
Epoch 232/300, resid Loss: 0.0648 | 0.0319
Epoch 233/300, resid Loss: 0.0648 | 0.0319
Epoch 234/300, resid Loss: 0.0648 | 0.0319
Epoch 235/300, resid Loss: 0.0648 | 0.0319
Epoch 236/300, resid Loss: 0.0648 | 0.0319
Epoch 237/300, resid Loss: 0.0648 | 0.0319
Epoch 238/300, resid Loss: 0.0648 | 0.0319
Epoch 239/300, resid Loss: 0.0648 | 0.0319
Epoch 240/300, resid Loss: 0.0648 | 0.0319
Epoch 241/300, resid Loss: 0.0648 | 0.0319
Epoch 242/300, resid Loss: 0.0648 | 0.0319
Epoch 243/300, resid Loss: 0.0648 | 0.0319
Epoch 244/300, resid Loss: 0.0648 | 0.0319
Epoch 245/300, resid Loss: 0.0648 | 0.0319
Epoch 246/300, resid Loss: 0.0648 | 0.0319
Epoch 247/300, resid Loss: 0.0648 | 0.0319
Epoch 248/300, resid Loss: 0.0648 | 0.0319
Epoch 249/300, resid Loss: 0.0648 | 0.0319
Epoch 250/300, resid Loss: 0.0648 | 0.0319
Epoch 251/300, resid Loss: 0.0648 | 0.0319
Epoch 252/300, resid Loss: 0.0648 | 0.0319
Epoch 253/300, resid Loss: 0.0648 | 0.0319
Epoch 254/300, resid Loss: 0.0648 | 0.0319
Epoch 255/300, resid Loss: 0.0648 | 0.0319
Epoch 256/300, resid Loss: 0.0648 | 0.0319
Epoch 257/300, resid Loss: 0.0648 | 0.0319
Epoch 258/300, resid Loss: 0.0648 | 0.0319
Epoch 259/300, resid Loss: 0.0648 | 0.0319
Epoch 260/300, resid Loss: 0.0648 | 0.0319
Epoch 261/300, resid Loss: 0.0648 | 0.0319
Epoch 262/300, resid Loss: 0.0648 | 0.0319
Epoch 263/300, resid Loss: 0.0648 | 0.0319
Epoch 264/300, resid Loss: 0.0648 | 0.0319
Epoch 265/300, resid Loss: 0.0648 | 0.0319
Epoch 266/300, resid Loss: 0.0648 | 0.0319
Epoch 267/300, resid Loss: 0.0648 | 0.0319
Early stopping for resid
Runtime (seconds): 1278.4888150691986
0.00023436469191064942
[160.6018]
[-4.8126345]
[1.9205942]
[15.250691]
[1.3659892]
[19.991333]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 130.46759428083897
RMSE: 11.4222412109375
MAE: 11.4222412109375
R-squared: nan
[194.31776]
