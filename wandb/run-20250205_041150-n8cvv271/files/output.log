[32m[I 2025-02-05 04:11:55,574][0m A new study created in memory with name: no-name-8cb8182c-65d4-49d8-90bc-b9250d938d04[0m
[32m[I 2025-02-05 04:12:26,588][0m Trial 0 finished with value: 0.3767206492403182 and parameters: {'observation_period_num': 99, 'train_rates': 0.7401721128929952, 'learning_rate': 0.0001011271753774168, 'batch_size': 173, 'step_size': 1, 'gamma': 0.9329255281844516}. Best is trial 0 with value: 0.3767206492403182.[0m
[32m[I 2025-02-05 04:13:02,028][0m Trial 1 finished with value: 0.12260464433811408 and parameters: {'observation_period_num': 172, 'train_rates': 0.7976410823332915, 'learning_rate': 0.0006234644869218946, 'batch_size': 155, 'step_size': 3, 'gamma': 0.8979824305751269}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:14:42,108][0m Trial 2 finished with value: 0.1899776039097239 and parameters: {'observation_period_num': 81, 'train_rates': 0.7522304396696085, 'learning_rate': 6.16186339970202e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8914637903915203}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:15:13,163][0m Trial 3 finished with value: 0.7816085957415938 and parameters: {'observation_period_num': 238, 'train_rates': 0.7040918198381152, 'learning_rate': 3.7775686766341684e-06, 'batch_size': 154, 'step_size': 4, 'gamma': 0.9574479223422292}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:15:39,697][0m Trial 4 finished with value: 0.29217631067176136 and parameters: {'observation_period_num': 193, 'train_rates': 0.686407527003633, 'learning_rate': 0.0005963605791238144, 'batch_size': 179, 'step_size': 8, 'gamma': 0.8868325658247709}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:18:38,611][0m Trial 5 finished with value: 0.14138797059052252 and parameters: {'observation_period_num': 235, 'train_rates': 0.8097769271484846, 'learning_rate': 2.573172988157089e-05, 'batch_size': 27, 'step_size': 11, 'gamma': 0.8990991373793011}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:19:09,199][0m Trial 6 finished with value: 0.37861962936542654 and parameters: {'observation_period_num': 221, 'train_rates': 0.8031266970880568, 'learning_rate': 2.2235868807188616e-05, 'batch_size': 172, 'step_size': 9, 'gamma': 0.805169501817196}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:20:36,757][0m Trial 7 finished with value: 0.38025491228609376 and parameters: {'observation_period_num': 232, 'train_rates': 0.8793769912854668, 'learning_rate': 5.361200431016061e-06, 'batch_size': 59, 'step_size': 2, 'gamma': 0.9764711852911345}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:21:08,975][0m Trial 8 finished with value: 0.2299453680214591 and parameters: {'observation_period_num': 6, 'train_rates': 0.6231854173597408, 'learning_rate': 0.0002247510021470848, 'batch_size': 150, 'step_size': 3, 'gamma': 0.7674382507336891}. Best is trial 1 with value: 0.12260464433811408.[0m
Early stopping at epoch 84
[32m[I 2025-02-05 04:21:37,612][0m Trial 9 finished with value: 1.6565400191312711 and parameters: {'observation_period_num': 56, 'train_rates': 0.7093409460094727, 'learning_rate': 1.4301203832318844e-06, 'batch_size': 148, 'step_size': 1, 'gamma': 0.8869852770500564}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:22:02,334][0m Trial 10 finished with value: 0.18602748215198517 and parameters: {'observation_period_num': 156, 'train_rates': 0.964121860219774, 'learning_rate': 0.0008171520058536991, 'batch_size': 256, 'step_size': 6, 'gamma': 0.830188223967922}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:22:58,492][0m Trial 11 finished with value: 0.19673281806909634 and parameters: {'observation_period_num': 172, 'train_rates': 0.8205182755996268, 'learning_rate': 1.949389881692914e-05, 'batch_size': 94, 'step_size': 12, 'gamma': 0.9252714732039258}. Best is trial 1 with value: 0.12260464433811408.[0m
[32m[I 2025-02-05 04:23:55,022][0m Trial 12 finished with value: 0.10175660618786085 and parameters: {'observation_period_num': 137, 'train_rates': 0.8539711596190674, 'learning_rate': 0.00018151330495960757, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8552683680408755}. Best is trial 12 with value: 0.10175660618786085.[0m
[32m[I 2025-02-05 04:24:53,620][0m Trial 13 finished with value: 0.15244049773691526 and parameters: {'observation_period_num': 135, 'train_rates': 0.8920570007460871, 'learning_rate': 0.00031081595661232775, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8426747046016141}. Best is trial 12 with value: 0.10175660618786085.[0m
[32m[I 2025-02-05 04:25:20,777][0m Trial 14 finished with value: 0.11383685752560821 and parameters: {'observation_period_num': 116, 'train_rates': 0.8612663272921892, 'learning_rate': 0.0001763306520452347, 'batch_size': 219, 'step_size': 6, 'gamma': 0.8561391853365572}. Best is trial 12 with value: 0.10175660618786085.[0m
[32m[I 2025-02-05 04:25:46,318][0m Trial 15 finished with value: 0.15067983080040326 and parameters: {'observation_period_num': 110, 'train_rates': 0.8881175966610165, 'learning_rate': 0.00012060528129010823, 'batch_size': 232, 'step_size': 6, 'gamma': 0.8478470231295804}. Best is trial 12 with value: 0.10175660618786085.[0m
[32m[I 2025-02-05 04:26:17,293][0m Trial 16 finished with value: 0.09342139214277267 and parameters: {'observation_period_num': 59, 'train_rates': 0.9888454443101895, 'learning_rate': 0.00022633283679084263, 'batch_size': 210, 'step_size': 12, 'gamma': 0.7832770663868841}. Best is trial 16 with value: 0.09342139214277267.[0m
[32m[I 2025-02-05 04:27:13,643][0m Trial 17 finished with value: 0.04305644333362579 and parameters: {'observation_period_num': 47, 'train_rates': 0.9854685252563019, 'learning_rate': 0.00038258048443573283, 'batch_size': 111, 'step_size': 13, 'gamma': 0.7582130316819843}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:27:46,542][0m Trial 18 finished with value: 0.08435507118701935 and parameters: {'observation_period_num': 26, 'train_rates': 0.9888969837032913, 'learning_rate': 6.0980013668942154e-05, 'batch_size': 200, 'step_size': 13, 'gamma': 0.7524282162011591}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:28:39,016][0m Trial 19 finished with value: 0.05638543129810747 and parameters: {'observation_period_num': 11, 'train_rates': 0.9282804609624469, 'learning_rate': 5.7866318137315406e-05, 'batch_size': 118, 'step_size': 14, 'gamma': 0.7549874879091858}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:29:30,208][0m Trial 20 finished with value: 0.2732218719866811 and parameters: {'observation_period_num': 36, 'train_rates': 0.9332778079278259, 'learning_rate': 9.446983677870001e-06, 'batch_size': 122, 'step_size': 10, 'gamma': 0.8012530316079503}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:30:18,887][0m Trial 21 finished with value: 0.05907752698698601 and parameters: {'observation_period_num': 11, 'train_rates': 0.9480771166936104, 'learning_rate': 5.549773094737521e-05, 'batch_size': 125, 'step_size': 14, 'gamma': 0.7529920312763517}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:31:10,607][0m Trial 22 finished with value: 0.06601262478916733 and parameters: {'observation_period_num': 7, 'train_rates': 0.9362055763183845, 'learning_rate': 5.023571521954169e-05, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7514627243815234}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:32:33,675][0m Trial 23 finished with value: 0.11255575759479633 and parameters: {'observation_period_num': 38, 'train_rates': 0.92914444510511, 'learning_rate': 1.2831502650254322e-05, 'batch_size': 70, 'step_size': 14, 'gamma': 0.7804507661430897}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:33:22,306][0m Trial 24 finished with value: 0.13749036903564746 and parameters: {'observation_period_num': 72, 'train_rates': 0.9551865278085725, 'learning_rate': 4.2320915123829465e-05, 'batch_size': 126, 'step_size': 14, 'gamma': 0.8118605154552817}. Best is trial 17 with value: 0.04305644333362579.[0m
[32m[I 2025-02-05 04:34:17,271][0m Trial 25 finished with value: 0.03828468721267168 and parameters: {'observation_period_num': 22, 'train_rates': 0.9043941575335956, 'learning_rate': 0.0003773209921215147, 'batch_size': 108, 'step_size': 13, 'gamma': 0.7818673909448912}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:35:36,937][0m Trial 26 finished with value: 0.06646937290279883 and parameters: {'observation_period_num': 48, 'train_rates': 0.9128872470971803, 'learning_rate': 0.00039700099425788213, 'batch_size': 72, 'step_size': 10, 'gamma': 0.7836952394986227}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:36:30,062][0m Trial 27 finished with value: 0.0651659114115457 and parameters: {'observation_period_num': 86, 'train_rates': 0.8448265109931944, 'learning_rate': 0.00011110877747056249, 'batch_size': 103, 'step_size': 13, 'gamma': 0.8239801970271501}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:37:48,016][0m Trial 28 finished with value: 0.05365198615204446 and parameters: {'observation_period_num': 23, 'train_rates': 0.908586309502646, 'learning_rate': 0.000981580520892131, 'batch_size': 76, 'step_size': 11, 'gamma': 0.7732151258481432}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:42:44,851][0m Trial 29 finished with value: 0.04235677506602319 and parameters: {'observation_period_num': 24, 'train_rates': 0.9044380492616946, 'learning_rate': 0.0009479716286502137, 'batch_size': 19, 'step_size': 11, 'gamma': 0.7932530131736489}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:46:30,502][0m Trial 30 finished with value: 0.15282343155209055 and parameters: {'observation_period_num': 96, 'train_rates': 0.8360289110269075, 'learning_rate': 0.00044470598173372274, 'batch_size': 23, 'step_size': 9, 'gamma': 0.7964163142558301}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:48:51,986][0m Trial 31 finished with value: 0.04645084097700299 and parameters: {'observation_period_num': 26, 'train_rates': 0.9099093727401945, 'learning_rate': 0.0009586415962175797, 'batch_size': 40, 'step_size': 11, 'gamma': 0.7732176890000697}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:51:12,712][0m Trial 32 finished with value: 0.1338235080242157 and parameters: {'observation_period_num': 33, 'train_rates': 0.8668547144513281, 'learning_rate': 0.0005991789678560897, 'batch_size': 39, 'step_size': 11, 'gamma': 0.7925718378989222}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:53:41,700][0m Trial 33 finished with value: 0.08576579425822604 and parameters: {'observation_period_num': 68, 'train_rates': 0.9696043420200798, 'learning_rate': 0.00034458508422316686, 'batch_size': 40, 'step_size': 13, 'gamma': 0.8169246128567051}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 04:59:06,602][0m Trial 34 finished with value: 0.04344969534236127 and parameters: {'observation_period_num': 40, 'train_rates': 0.9072709180361267, 'learning_rate': 0.0009125015143464435, 'batch_size': 17, 'step_size': 12, 'gamma': 0.7672968396872485}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:00:33,936][0m Trial 35 finished with value: 0.1968549433171301 and parameters: {'observation_period_num': 49, 'train_rates': 0.7718038206040798, 'learning_rate': 0.0005843465494256634, 'batch_size': 58, 'step_size': 12, 'gamma': 0.7645346711420545}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:06:14,814][0m Trial 36 finished with value: 0.12083146212906237 and parameters: {'observation_period_num': 78, 'train_rates': 0.8955842025779458, 'learning_rate': 0.00046899890071719325, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7915955233859143}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:06:44,867][0m Trial 37 finished with value: 0.19431160765637226 and parameters: {'observation_period_num': 44, 'train_rates': 0.7662351054506731, 'learning_rate': 0.0007128563319574157, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8354206865883842}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:08:31,322][0m Trial 38 finished with value: 0.061854729221926794 and parameters: {'observation_period_num': 61, 'train_rates': 0.9754079137180665, 'learning_rate': 0.0002854152749926554, 'batch_size': 56, 'step_size': 8, 'gamma': 0.809248221908746}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:09:39,462][0m Trial 39 finished with value: 0.05152020233034401 and parameters: {'observation_period_num': 21, 'train_rates': 0.8229729428909007, 'learning_rate': 8.548019820177667e-05, 'batch_size': 82, 'step_size': 13, 'gamma': 0.8685063387511284}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:10:15,258][0m Trial 40 finished with value: 0.14642963664921538 and parameters: {'observation_period_num': 51, 'train_rates': 0.6287084954861725, 'learning_rate': 0.0005245595405640383, 'batch_size': 137, 'step_size': 9, 'gamma': 0.7649443011055478}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:12:35,429][0m Trial 41 finished with value: 0.05909676478740356 and parameters: {'observation_period_num': 23, 'train_rates': 0.9112273030641022, 'learning_rate': 0.0009364056366493779, 'batch_size': 41, 'step_size': 11, 'gamma': 0.7739409709048937}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:15:25,772][0m Trial 42 finished with value: 0.047000226277019344 and parameters: {'observation_period_num': 20, 'train_rates': 0.8703495357773224, 'learning_rate': 0.0008660964995197262, 'batch_size': 32, 'step_size': 12, 'gamma': 0.7651766327986202}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:21:12,937][0m Trial 43 finished with value: 0.04284567118815656 and parameters: {'observation_period_num': 34, 'train_rates': 0.9094972515130696, 'learning_rate': 0.00065147041278051, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7872327455923408}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:26:23,577][0m Trial 44 finished with value: 0.16138632581211054 and parameters: {'observation_period_num': 90, 'train_rates': 0.945918852206657, 'learning_rate': 0.00015036427567828058, 'batch_size': 18, 'step_size': 8, 'gamma': 0.7850525680487316}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:26:58,061][0m Trial 45 finished with value: 0.19299778663035896 and parameters: {'observation_period_num': 38, 'train_rates': 0.7906605488200844, 'learning_rate': 0.0002276166689832934, 'batch_size': 163, 'step_size': 7, 'gamma': 0.9253690098796679}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:29:48,445][0m Trial 46 finished with value: 0.34327294429817223 and parameters: {'observation_period_num': 252, 'train_rates': 0.7281111677683212, 'learning_rate': 0.0006645660746182015, 'batch_size': 26, 'step_size': 5, 'gamma': 0.8023226535632908}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:31:33,207][0m Trial 47 finished with value: 0.22764270193874836 and parameters: {'observation_period_num': 195, 'train_rates': 0.8786960586222632, 'learning_rate': 0.00029852318296734904, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9023904983356378}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:32:39,438][0m Trial 48 finished with value: 0.2235879160365776 and parameters: {'observation_period_num': 72, 'train_rates': 0.894829287007675, 'learning_rate': 0.0004111309946586885, 'batch_size': 85, 'step_size': 9, 'gamma': 0.96901302791845}. Best is trial 25 with value: 0.03828468721267168.[0m
[32m[I 2025-02-05 05:35:54,614][0m Trial 49 finished with value: 0.4264678758218748 and parameters: {'observation_period_num': 61, 'train_rates': 0.962724745111468, 'learning_rate': 1.864715846893311e-06, 'batch_size': 30, 'step_size': 10, 'gamma': 0.8196814229856905}. Best is trial 25 with value: 0.03828468721267168.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4552 | 0.3824
Epoch 2/300, Loss: 0.1968 | 0.3030
Epoch 3/300, Loss: 0.2120 | 0.1941
Epoch 4/300, Loss: 0.1520 | 0.1556
Epoch 5/300, Loss: 0.1435 | 0.1359
Epoch 6/300, Loss: 0.1325 | 0.1547
Epoch 7/300, Loss: 0.1253 | 0.1141
Epoch 8/300, Loss: 0.1243 | 0.1592
Epoch 9/300, Loss: 0.1217 | 0.2037
Epoch 10/300, Loss: 0.1540 | 0.1554
Epoch 11/300, Loss: 0.1334 | 0.1124
Epoch 12/300, Loss: 0.1203 | 0.0972
Epoch 13/300, Loss: 0.1095 | 0.0846
Epoch 14/300, Loss: 0.1185 | 0.1232
Epoch 15/300, Loss: 0.1559 | 0.4268
Epoch 16/300, Loss: 0.1443 | 0.2311
Epoch 17/300, Loss: 0.1163 | 0.1013
Epoch 18/300, Loss: 0.1295 | 0.0843
Epoch 19/300, Loss: 0.1095 | 0.0822
Epoch 20/300, Loss: 0.1073 | 0.1107
Epoch 21/300, Loss: 0.1025 | 0.0837
Epoch 22/300, Loss: 0.1050 | 0.0623
Epoch 23/300, Loss: 0.1029 | 0.0592
Epoch 24/300, Loss: 0.1017 | 0.0619
Epoch 25/300, Loss: 0.1026 | 0.0628
Epoch 26/300, Loss: 0.1037 | 0.1063
Epoch 27/300, Loss: 0.1016 | 0.1425
Epoch 28/300, Loss: 0.1134 | 0.0795
Epoch 29/300, Loss: 0.1042 | 0.0774
Epoch 30/300, Loss: 0.0895 | 0.0658
Epoch 31/300, Loss: 0.0841 | 0.0791
Epoch 32/300, Loss: 0.0827 | 0.0700
Epoch 33/300, Loss: 0.0814 | 0.0567
Epoch 34/300, Loss: 0.0808 | 0.0535
Epoch 35/300, Loss: 0.0797 | 0.0558
Epoch 36/300, Loss: 0.0793 | 0.0604
Epoch 37/300, Loss: 0.0789 | 0.0598
Epoch 38/300, Loss: 0.0783 | 0.0553
Epoch 39/300, Loss: 0.0778 | 0.0524
Epoch 40/300, Loss: 0.0773 | 0.0511
Epoch 41/300, Loss: 0.0770 | 0.0512
Epoch 42/300, Loss: 0.0766 | 0.0514
Epoch 43/300, Loss: 0.0761 | 0.0512
Epoch 44/300, Loss: 0.0757 | 0.0507
Epoch 45/300, Loss: 0.0753 | 0.0500
Epoch 46/300, Loss: 0.0749 | 0.0492
Epoch 47/300, Loss: 0.0744 | 0.0484
Epoch 48/300, Loss: 0.0741 | 0.0478
Epoch 49/300, Loss: 0.0737 | 0.0474
Epoch 50/300, Loss: 0.0733 | 0.0471
Epoch 51/300, Loss: 0.0730 | 0.0466
Epoch 52/300, Loss: 0.0726 | 0.0461
Epoch 53/300, Loss: 0.0722 | 0.0456
Epoch 54/300, Loss: 0.0719 | 0.0451
Epoch 55/300, Loss: 0.0716 | 0.0446
Epoch 56/300, Loss: 0.0712 | 0.0442
Epoch 57/300, Loss: 0.0709 | 0.0437
Epoch 58/300, Loss: 0.0706 | 0.0433
Epoch 59/300, Loss: 0.0703 | 0.0428
Epoch 60/300, Loss: 0.0699 | 0.0426
Epoch 61/300, Loss: 0.0697 | 0.0422
Epoch 62/300, Loss: 0.0695 | 0.0419
Epoch 63/300, Loss: 0.0692 | 0.0416
Epoch 64/300, Loss: 0.0690 | 0.0413
Epoch 65/300, Loss: 0.0688 | 0.0410
Epoch 66/300, Loss: 0.0685 | 0.0408
Epoch 67/300, Loss: 0.0684 | 0.0406
Epoch 68/300, Loss: 0.0682 | 0.0403
Epoch 69/300, Loss: 0.0680 | 0.0401
Epoch 70/300, Loss: 0.0678 | 0.0399
Epoch 71/300, Loss: 0.0677 | 0.0397
Epoch 72/300, Loss: 0.0675 | 0.0394
Epoch 73/300, Loss: 0.0673 | 0.0393
Epoch 74/300, Loss: 0.0671 | 0.0391
Epoch 75/300, Loss: 0.0670 | 0.0390
Epoch 76/300, Loss: 0.0669 | 0.0388
Epoch 77/300, Loss: 0.0668 | 0.0387
Epoch 78/300, Loss: 0.0666 | 0.0385
Epoch 79/300, Loss: 0.0665 | 0.0384
Epoch 80/300, Loss: 0.0664 | 0.0383
Epoch 81/300, Loss: 0.0663 | 0.0382
Epoch 82/300, Loss: 0.0662 | 0.0381
Epoch 83/300, Loss: 0.0661 | 0.0379
Epoch 84/300, Loss: 0.0660 | 0.0378
Epoch 85/300, Loss: 0.0659 | 0.0377
Epoch 86/300, Loss: 0.0658 | 0.0376
Epoch 87/300, Loss: 0.0657 | 0.0375
Epoch 88/300, Loss: 0.0656 | 0.0375
Epoch 89/300, Loss: 0.0655 | 0.0374
Epoch 90/300, Loss: 0.0655 | 0.0373
Epoch 91/300, Loss: 0.0654 | 0.0372
Epoch 92/300, Loss: 0.0653 | 0.0371
Epoch 93/300, Loss: 0.0652 | 0.0371
Epoch 94/300, Loss: 0.0652 | 0.0370
Epoch 95/300, Loss: 0.0651 | 0.0369
Epoch 96/300, Loss: 0.0650 | 0.0369
Epoch 97/300, Loss: 0.0650 | 0.0368
Epoch 98/300, Loss: 0.0649 | 0.0367
Epoch 99/300, Loss: 0.0649 | 0.0367
Epoch 100/300, Loss: 0.0648 | 0.0366
Epoch 101/300, Loss: 0.0648 | 0.0366
Epoch 102/300, Loss: 0.0647 | 0.0365
Epoch 103/300, Loss: 0.0647 | 0.0365
Epoch 104/300, Loss: 0.0646 | 0.0364
Epoch 105/300, Loss: 0.0645 | 0.0364
Epoch 106/300, Loss: 0.0645 | 0.0363
Epoch 107/300, Loss: 0.0645 | 0.0363
Epoch 108/300, Loss: 0.0644 | 0.0363
Epoch 109/300, Loss: 0.0644 | 0.0362
Epoch 110/300, Loss: 0.0644 | 0.0362
Epoch 111/300, Loss: 0.0643 | 0.0361
Epoch 112/300, Loss: 0.0643 | 0.0361
Epoch 113/300, Loss: 0.0642 | 0.0361
Epoch 114/300, Loss: 0.0642 | 0.0360
Epoch 115/300, Loss: 0.0642 | 0.0360
Epoch 116/300, Loss: 0.0641 | 0.0360
Epoch 117/300, Loss: 0.0641 | 0.0359
Epoch 118/300, Loss: 0.0641 | 0.0359
Epoch 119/300, Loss: 0.0640 | 0.0359
Epoch 120/300, Loss: 0.0640 | 0.0359
Epoch 121/300, Loss: 0.0640 | 0.0358
Epoch 122/300, Loss: 0.0640 | 0.0358
Epoch 123/300, Loss: 0.0639 | 0.0358
Epoch 124/300, Loss: 0.0639 | 0.0358
Epoch 125/300, Loss: 0.0639 | 0.0358
Epoch 126/300, Loss: 0.0639 | 0.0357
Epoch 127/300, Loss: 0.0639 | 0.0357
Epoch 128/300, Loss: 0.0638 | 0.0357
Epoch 129/300, Loss: 0.0638 | 0.0357
Epoch 130/300, Loss: 0.0638 | 0.0357
Epoch 131/300, Loss: 0.0638 | 0.0356
Epoch 132/300, Loss: 0.0638 | 0.0356
Epoch 133/300, Loss: 0.0637 | 0.0356
Epoch 134/300, Loss: 0.0637 | 0.0356
Epoch 135/300, Loss: 0.0637 | 0.0356
Epoch 136/300, Loss: 0.0637 | 0.0356
Epoch 137/300, Loss: 0.0637 | 0.0355
Epoch 138/300, Loss: 0.0637 | 0.0355
Epoch 139/300, Loss: 0.0636 | 0.0355
Epoch 140/300, Loss: 0.0636 | 0.0355
Epoch 141/300, Loss: 0.0636 | 0.0355
Epoch 142/300, Loss: 0.0636 | 0.0355
Epoch 143/300, Loss: 0.0636 | 0.0355
Epoch 144/300, Loss: 0.0636 | 0.0355
Epoch 145/300, Loss: 0.0636 | 0.0354
Epoch 146/300, Loss: 0.0636 | 0.0354
Epoch 147/300, Loss: 0.0635 | 0.0354
Epoch 148/300, Loss: 0.0635 | 0.0354
Epoch 149/300, Loss: 0.0635 | 0.0354
Epoch 150/300, Loss: 0.0635 | 0.0354
Epoch 151/300, Loss: 0.0635 | 0.0354
Epoch 152/300, Loss: 0.0635 | 0.0354
Epoch 153/300, Loss: 0.0635 | 0.0354
Epoch 154/300, Loss: 0.0635 | 0.0354
Epoch 155/300, Loss: 0.0635 | 0.0354
Epoch 156/300, Loss: 0.0635 | 0.0353
Epoch 157/300, Loss: 0.0634 | 0.0353
Epoch 158/300, Loss: 0.0634 | 0.0353
Epoch 159/300, Loss: 0.0634 | 0.0353
Epoch 160/300, Loss: 0.0634 | 0.0353
Epoch 161/300, Loss: 0.0634 | 0.0353
Epoch 162/300, Loss: 0.0634 | 0.0353
Epoch 163/300, Loss: 0.0634 | 0.0353
Epoch 164/300, Loss: 0.0634 | 0.0353
Epoch 165/300, Loss: 0.0634 | 0.0353
Epoch 166/300, Loss: 0.0634 | 0.0353
Epoch 167/300, Loss: 0.0634 | 0.0353
Epoch 168/300, Loss: 0.0634 | 0.0353
Epoch 169/300, Loss: 0.0634 | 0.0353
Epoch 170/300, Loss: 0.0634 | 0.0353
Epoch 171/300, Loss: 0.0634 | 0.0353
Epoch 172/300, Loss: 0.0634 | 0.0353
Epoch 173/300, Loss: 0.0634 | 0.0353
Epoch 174/300, Loss: 0.0633 | 0.0353
Epoch 175/300, Loss: 0.0633 | 0.0352
Epoch 176/300, Loss: 0.0633 | 0.0352
Epoch 177/300, Loss: 0.0633 | 0.0352
Epoch 178/300, Loss: 0.0633 | 0.0352
Epoch 179/300, Loss: 0.0633 | 0.0352
Epoch 180/300, Loss: 0.0633 | 0.0352
Epoch 181/300, Loss: 0.0633 | 0.0352
Epoch 182/300, Loss: 0.0633 | 0.0352
Epoch 183/300, Loss: 0.0633 | 0.0352
Epoch 184/300, Loss: 0.0633 | 0.0352
Epoch 185/300, Loss: 0.0633 | 0.0352
Epoch 186/300, Loss: 0.0633 | 0.0352
Epoch 187/300, Loss: 0.0633 | 0.0352
Epoch 188/300, Loss: 0.0633 | 0.0352
Epoch 189/300, Loss: 0.0633 | 0.0352
Epoch 190/300, Loss: 0.0633 | 0.0352
Epoch 191/300, Loss: 0.0633 | 0.0352
Epoch 192/300, Loss: 0.0633 | 0.0352
Epoch 193/300, Loss: 0.0633 | 0.0352
Epoch 194/300, Loss: 0.0633 | 0.0352
Epoch 195/300, Loss: 0.0633 | 0.0352
Epoch 196/300, Loss: 0.0633 | 0.0352
Epoch 197/300, Loss: 0.0633 | 0.0352
Epoch 198/300, Loss: 0.0633 | 0.0352
Epoch 199/300, Loss: 0.0633 | 0.0352
Epoch 200/300, Loss: 0.0633 | 0.0352
Epoch 201/300, Loss: 0.0633 | 0.0352
Epoch 202/300, Loss: 0.0633 | 0.0352
Epoch 203/300, Loss: 0.0633 | 0.0352
Epoch 204/300, Loss: 0.0633 | 0.0352
Epoch 205/300, Loss: 0.0633 | 0.0352
Epoch 206/300, Loss: 0.0633 | 0.0352
Epoch 207/300, Loss: 0.0633 | 0.0352
Epoch 208/300, Loss: 0.0633 | 0.0352
Epoch 209/300, Loss: 0.0633 | 0.0352
Epoch 210/300, Loss: 0.0633 | 0.0352
Epoch 211/300, Loss: 0.0633 | 0.0352
Epoch 212/300, Loss: 0.0633 | 0.0352
Epoch 213/300, Loss: 0.0633 | 0.0352
Epoch 214/300, Loss: 0.0633 | 0.0352
Epoch 215/300, Loss: 0.0633 | 0.0352
Epoch 216/300, Loss: 0.0633 | 0.0352
Epoch 217/300, Loss: 0.0633 | 0.0352
Epoch 218/300, Loss: 0.0633 | 0.0352
Epoch 219/300, Loss: 0.0633 | 0.0352
Epoch 220/300, Loss: 0.0633 | 0.0352
Epoch 221/300, Loss: 0.0633 | 0.0352
Epoch 222/300, Loss: 0.0632 | 0.0352
Epoch 223/300, Loss: 0.0632 | 0.0352
Epoch 224/300, Loss: 0.0632 | 0.0352
Epoch 225/300, Loss: 0.0632 | 0.0352
Epoch 226/300, Loss: 0.0632 | 0.0352
Epoch 227/300, Loss: 0.0632 | 0.0352
Epoch 228/300, Loss: 0.0632 | 0.0352
Epoch 229/300, Loss: 0.0632 | 0.0352
Epoch 230/300, Loss: 0.0632 | 0.0352
Epoch 231/300, Loss: 0.0632 | 0.0352
Epoch 232/300, Loss: 0.0632 | 0.0352
Epoch 233/300, Loss: 0.0632 | 0.0352
Epoch 234/300, Loss: 0.0632 | 0.0352
Epoch 235/300, Loss: 0.0632 | 0.0352
Epoch 236/300, Loss: 0.0632 | 0.0352
Epoch 237/300, Loss: 0.0632 | 0.0352
Epoch 238/300, Loss: 0.0632 | 0.0352
Epoch 239/300, Loss: 0.0632 | 0.0352
Epoch 240/300, Loss: 0.0632 | 0.0352
Epoch 241/300, Loss: 0.0632 | 0.0352
Epoch 242/300, Loss: 0.0632 | 0.0352
Epoch 243/300, Loss: 0.0632 | 0.0352
Epoch 244/300, Loss: 0.0632 | 0.0352
Epoch 245/300, Loss: 0.0632 | 0.0352
Epoch 246/300, Loss: 0.0632 | 0.0352
Epoch 247/300, Loss: 0.0632 | 0.0352
Epoch 248/300, Loss: 0.0632 | 0.0352
Epoch 249/300, Loss: 0.0632 | 0.0352
Epoch 250/300, Loss: 0.0632 | 0.0352
Epoch 251/300, Loss: 0.0632 | 0.0351
Epoch 252/300, Loss: 0.0632 | 0.0351
Epoch 253/300, Loss: 0.0632 | 0.0351
Epoch 254/300, Loss: 0.0632 | 0.0351
Epoch 255/300, Loss: 0.0632 | 0.0351
Epoch 256/300, Loss: 0.0632 | 0.0351
Epoch 257/300, Loss: 0.0632 | 0.0351
Epoch 258/300, Loss: 0.0632 | 0.0351
Epoch 259/300, Loss: 0.0632 | 0.0351
Epoch 260/300, Loss: 0.0632 | 0.0351
Epoch 261/300, Loss: 0.0632 | 0.0351
Epoch 262/300, Loss: 0.0632 | 0.0351
Epoch 263/300, Loss: 0.0632 | 0.0351
Epoch 264/300, Loss: 0.0632 | 0.0351
Epoch 265/300, Loss: 0.0632 | 0.0351
Epoch 266/300, Loss: 0.0632 | 0.0351
Epoch 267/300, Loss: 0.0632 | 0.0351
Epoch 268/300, Loss: 0.0632 | 0.0351
Epoch 269/300, Loss: 0.0632 | 0.0351
Epoch 270/300, Loss: 0.0632 | 0.0351
Epoch 271/300, Loss: 0.0632 | 0.0351
Epoch 272/300, Loss: 0.0632 | 0.0351
Epoch 273/300, Loss: 0.0632 | 0.0351
Epoch 274/300, Loss: 0.0632 | 0.0351
Epoch 275/300, Loss: 0.0632 | 0.0351
Epoch 276/300, Loss: 0.0632 | 0.0351
Epoch 277/300, Loss: 0.0632 | 0.0351
Epoch 278/300, Loss: 0.0632 | 0.0351
Epoch 279/300, Loss: 0.0632 | 0.0351
Epoch 280/300, Loss: 0.0632 | 0.0351
Epoch 281/300, Loss: 0.0632 | 0.0351
Epoch 282/300, Loss: 0.0632 | 0.0351
Epoch 283/300, Loss: 0.0632 | 0.0351
Epoch 284/300, Loss: 0.0632 | 0.0351
Epoch 285/300, Loss: 0.0632 | 0.0351
Epoch 286/300, Loss: 0.0632 | 0.0351
Epoch 287/300, Loss: 0.0632 | 0.0351
Epoch 288/300, Loss: 0.0632 | 0.0351
Epoch 289/300, Loss: 0.0632 | 0.0351
Epoch 290/300, Loss: 0.0632 | 0.0351
Epoch 291/300, Loss: 0.0632 | 0.0351
Epoch 292/300, Loss: 0.0632 | 0.0351
Epoch 293/300, Loss: 0.0632 | 0.0351
Epoch 294/300, Loss: 0.0632 | 0.0351
Epoch 295/300, Loss: 0.0632 | 0.0351
Epoch 296/300, Loss: 0.0632 | 0.0351
Epoch 297/300, Loss: 0.0632 | 0.0351
Epoch 298/300, Loss: 0.0632 | 0.0351
Epoch 299/300, Loss: 0.0632 | 0.0351
Epoch 300/300, Loss: 0.0632 | 0.0351
Runtime (seconds): 164.1622884273529
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 133.04926917050034
RMSE: 11.534698486328125
MAE: 11.534698486328125
R-squared: nan
[174.9953]
