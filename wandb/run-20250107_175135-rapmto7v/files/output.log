ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-07 17:51:40,134][0m A new study created in memory with name: no-name-91191710-d60a-4966-bfd4-7c5fbe8dce48[0m
[32m[I 2025-01-07 17:52:09,832][0m Trial 0 finished with value: 0.12729993486468166 and parameters: {'observation_period_num': 193, 'train_rates': 0.8647687565106197, 'learning_rate': 0.0009482764283201054, 'batch_size': 224, 'step_size': 6, 'gamma': 0.8577587810509681}. Best is trial 0 with value: 0.12729993486468166.[0m
[32m[I 2025-01-07 17:52:44,285][0m Trial 1 finished with value: 1.8866320923131692 and parameters: {'observation_period_num': 31, 'train_rates': 0.8418658241279693, 'learning_rate': 1.0923601784334923e-06, 'batch_size': 180, 'step_size': 1, 'gamma': 0.9701354507169362}. Best is trial 0 with value: 0.12729993486468166.[0m
[32m[I 2025-01-07 17:53:24,944][0m Trial 2 finished with value: 0.1518422392079767 and parameters: {'observation_period_num': 118, 'train_rates': 0.7873010583551301, 'learning_rate': 0.0007557608315515149, 'batch_size': 202, 'step_size': 11, 'gamma': 0.9021800714836375}. Best is trial 0 with value: 0.12729993486468166.[0m
[32m[I 2025-01-07 17:53:52,416][0m Trial 3 finished with value: 0.13087132097684695 and parameters: {'observation_period_num': 81, 'train_rates': 0.6056934017027173, 'learning_rate': 0.0001795119666771332, 'batch_size': 251, 'step_size': 15, 'gamma': 0.9878211662145167}. Best is trial 0 with value: 0.12729993486468166.[0m
[32m[I 2025-01-07 17:54:33,072][0m Trial 4 finished with value: 0.20386731624603271 and parameters: {'observation_period_num': 152, 'train_rates': 0.9535783561403433, 'learning_rate': 4.757114094416288e-05, 'batch_size': 158, 'step_size': 11, 'gamma': 0.7806523592283071}. Best is trial 0 with value: 0.12729993486468166.[0m
[32m[I 2025-01-07 17:55:27,006][0m Trial 5 finished with value: 0.071495229423773 and parameters: {'observation_period_num': 20, 'train_rates': 0.7067563377033834, 'learning_rate': 2.592092575479858e-05, 'batch_size': 105, 'step_size': 8, 'gamma': 0.8673398685269436}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 17:56:17,762][0m Trial 6 finished with value: 0.11945902880069481 and parameters: {'observation_period_num': 136, 'train_rates': 0.7691612739594534, 'learning_rate': 4.20835953052555e-05, 'batch_size': 106, 'step_size': 13, 'gamma': 0.8650923091145812}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 17:56:54,931][0m Trial 7 finished with value: 0.8662987046283135 and parameters: {'observation_period_num': 234, 'train_rates': 0.8305628932771258, 'learning_rate': 1.0737812561503655e-06, 'batch_size': 247, 'step_size': 13, 'gamma': 0.7521330941215741}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 17:57:49,348][0m Trial 8 finished with value: 0.16049816958101623 and parameters: {'observation_period_num': 149, 'train_rates': 0.642160842887354, 'learning_rate': 0.0002724996489164654, 'batch_size': 92, 'step_size': 11, 'gamma': 0.9347305761097866}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 17:58:23,480][0m Trial 9 finished with value: 0.07976493550817909 and parameters: {'observation_period_num': 84, 'train_rates': 0.755857993738206, 'learning_rate': 0.00018599896552175753, 'batch_size': 251, 'step_size': 6, 'gamma': 0.8704788260631935}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 18:00:12,433][0m Trial 10 finished with value: 0.301076896297626 and parameters: {'observation_period_num': 7, 'train_rates': 0.6957093507446438, 'learning_rate': 6.422626530293812e-06, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8061022343576041}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 18:00:53,154][0m Trial 11 finished with value: 0.3820010862947231 and parameters: {'observation_period_num': 66, 'train_rates': 0.7137691117992427, 'learning_rate': 1.0712794211180286e-05, 'batch_size': 122, 'step_size': 6, 'gamma': 0.828718901770834}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 18:02:12,934][0m Trial 12 finished with value: 0.07328574471308086 and parameters: {'observation_period_num': 62, 'train_rates': 0.7081471079875942, 'learning_rate': 0.0001688390348842263, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9042682953494204}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 18:05:57,303][0m Trial 13 finished with value: 0.07213914536840771 and parameters: {'observation_period_num': 47, 'train_rates': 0.6927096568354751, 'learning_rate': 1.543926898246096e-05, 'batch_size': 21, 'step_size': 8, 'gamma': 0.9234061553215285}. Best is trial 5 with value: 0.071495229423773.[0m
[32m[I 2025-01-07 18:09:46,722][0m Trial 14 finished with value: 0.05313996255702494 and parameters: {'observation_period_num': 5, 'train_rates': 0.6516449849981706, 'learning_rate': 8.686703272373095e-06, 'batch_size': 20, 'step_size': 9, 'gamma': 0.9327463042807848}. Best is trial 14 with value: 0.05313996255702494.[0m
[32m[I 2025-01-07 18:10:49,889][0m Trial 15 finished with value: 0.5943826510722472 and parameters: {'observation_period_num': 5, 'train_rates': 0.6434948909259011, 'learning_rate': 3.954804942102972e-06, 'batch_size': 76, 'step_size': 4, 'gamma': 0.8318526427131755}. Best is trial 14 with value: 0.05313996255702494.[0m
[32m[I 2025-01-07 18:14:44,128][0m Trial 16 finished with value: 0.24377141214550935 and parameters: {'observation_period_num': 105, 'train_rates': 0.6548832031860115, 'learning_rate': 2.902459869856596e-06, 'batch_size': 19, 'step_size': 9, 'gamma': 0.9470639185292692}. Best is trial 14 with value: 0.05313996255702494.[0m
[32m[I 2025-01-07 18:15:22,543][0m Trial 17 finished with value: 0.20001607492098825 and parameters: {'observation_period_num': 19, 'train_rates': 0.6089449584406701, 'learning_rate': 2.6606360772579575e-05, 'batch_size': 146, 'step_size': 9, 'gamma': 0.8881687269002956}. Best is trial 14 with value: 0.05313996255702494.[0m
[32m[I 2025-01-07 18:16:56,997][0m Trial 18 finished with value: 0.059277719019243384 and parameters: {'observation_period_num': 42, 'train_rates': 0.7441837487025267, 'learning_rate': 7.618729766643913e-05, 'batch_size': 54, 'step_size': 3, 'gamma': 0.961212496525509}. Best is trial 14 with value: 0.05313996255702494.[0m
[32m[I 2025-01-07 18:19:04,668][0m Trial 19 finished with value: 0.04151283828024235 and parameters: {'observation_period_num': 43, 'train_rates': 0.9012587969195562, 'learning_rate': 6.973467758116624e-05, 'batch_size': 44, 'step_size': 4, 'gamma': 0.9594165213774282}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:21:30,802][0m Trial 20 finished with value: 0.05677484693350615 and parameters: {'observation_period_num': 98, 'train_rates': 0.9246180102649516, 'learning_rate': 7.213350845081243e-05, 'batch_size': 38, 'step_size': 4, 'gamma': 0.9876933711265433}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:24:06,551][0m Trial 21 finished with value: 0.1958510217002847 and parameters: {'observation_period_num': 98, 'train_rates': 0.9384986685828781, 'learning_rate': 8.882266087870414e-05, 'batch_size': 36, 'step_size': 5, 'gamma': 0.9859833884490025}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:25:25,326][0m Trial 22 finished with value: 0.09511005421799998 and parameters: {'observation_period_num': 55, 'train_rates': 0.9038171312495891, 'learning_rate': 1.3967565255787573e-05, 'batch_size': 72, 'step_size': 4, 'gamma': 0.953403866653912}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:30:51,979][0m Trial 23 finished with value: 0.09627044137035097 and parameters: {'observation_period_num': 169, 'train_rates': 0.9876342546690747, 'learning_rate': 7.280143129660948e-05, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9202640336181102}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:32:00,884][0m Trial 24 finished with value: 0.19742161949149897 and parameters: {'observation_period_num': 77, 'train_rates': 0.8942332880061203, 'learning_rate': 7.44397527892854e-06, 'batch_size': 81, 'step_size': 3, 'gamma': 0.9760828933778489}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:34:20,783][0m Trial 25 finished with value: 0.047106487379104146 and parameters: {'observation_period_num': 39, 'train_rates': 0.9183946398189348, 'learning_rate': 0.00035682936445016316, 'batch_size': 40, 'step_size': 4, 'gamma': 0.93880101380523}. Best is trial 19 with value: 0.04151283828024235.[0m
[32m[I 2025-01-07 18:36:37,513][0m Trial 26 finished with value: 0.036244773863321506 and parameters: {'observation_period_num': 28, 'train_rates': 0.8215237535462842, 'learning_rate': 0.00043372115019091746, 'batch_size': 38, 'step_size': 10, 'gamma': 0.9395704209638772}. Best is trial 26 with value: 0.036244773863321506.[0m
[32m[I 2025-01-07 18:38:06,259][0m Trial 27 finished with value: 0.04124476734432392 and parameters: {'observation_period_num': 36, 'train_rates': 0.82467404763952, 'learning_rate': 0.0004873963087866858, 'batch_size': 60, 'step_size': 10, 'gamma': 0.9438349349365301}. Best is trial 26 with value: 0.036244773863321506.[0m
[32m[I 2025-01-07 18:39:31,659][0m Trial 28 finished with value: 0.041215658649126165 and parameters: {'observation_period_num': 29, 'train_rates': 0.8193423858678329, 'learning_rate': 0.00047294972485141377, 'batch_size': 63, 'step_size': 12, 'gamma': 0.908386844161477}. Best is trial 26 with value: 0.036244773863321506.[0m
[32m[I 2025-01-07 18:40:13,024][0m Trial 29 finished with value: 0.165925007642192 and parameters: {'observation_period_num': 205, 'train_rates': 0.8270453575833002, 'learning_rate': 0.0009637100680849578, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9031977647609524}. Best is trial 26 with value: 0.036244773863321506.[0m
[32m[I 2025-01-07 18:41:09,245][0m Trial 30 finished with value: 0.03409751266709173 and parameters: {'observation_period_num': 29, 'train_rates': 0.8040733464207492, 'learning_rate': 0.0005678175364004451, 'batch_size': 96, 'step_size': 10, 'gamma': 0.8831070405310034}. Best is trial 30 with value: 0.03409751266709173.[0m
[32m[I 2025-01-07 18:42:09,718][0m Trial 31 finished with value: 0.03552810034801807 and parameters: {'observation_period_num': 26, 'train_rates': 0.861468965639682, 'learning_rate': 0.0003983620670739637, 'batch_size': 96, 'step_size': 10, 'gamma': 0.8820645243678233}. Best is trial 30 with value: 0.03409751266709173.[0m
[32m[I 2025-01-07 18:43:05,285][0m Trial 32 finished with value: 0.02804950074377385 and parameters: {'observation_period_num': 21, 'train_rates': 0.8652083052613442, 'learning_rate': 0.0005762744748391957, 'batch_size': 102, 'step_size': 12, 'gamma': 0.8856383910768304}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:43:56,712][0m Trial 33 finished with value: 0.030541384422028337 and parameters: {'observation_period_num': 21, 'train_rates': 0.8660290631398521, 'learning_rate': 0.0006569170150502795, 'batch_size': 111, 'step_size': 10, 'gamma': 0.8823859210090932}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:44:48,696][0m Trial 34 finished with value: 0.0448314683844909 and parameters: {'observation_period_num': 21, 'train_rates': 0.8456485354363099, 'learning_rate': 0.0007510302742132915, 'batch_size': 111, 'step_size': 15, 'gamma': 0.8827536037378466}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:45:26,038][0m Trial 35 finished with value: 0.04146113185940877 and parameters: {'observation_period_num': 56, 'train_rates': 0.8730218395494527, 'learning_rate': 0.0006669200263361821, 'batch_size': 168, 'step_size': 12, 'gamma': 0.8396200731642309}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:46:06,867][0m Trial 36 finished with value: 0.06272299922321857 and parameters: {'observation_period_num': 70, 'train_rates': 0.8673438939031771, 'learning_rate': 0.000249484347297733, 'batch_size': 142, 'step_size': 10, 'gamma': 0.8865606484110967}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:47:05,636][0m Trial 37 finished with value: 0.036390879717085434 and parameters: {'observation_period_num': 18, 'train_rates': 0.7860282021383792, 'learning_rate': 0.00012131775554358241, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8481200255717434}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:47:37,779][0m Trial 38 finished with value: 0.09058902712253036 and parameters: {'observation_period_num': 50, 'train_rates': 0.8047735151083423, 'learning_rate': 0.0006657870613017437, 'batch_size': 201, 'step_size': 14, 'gamma': 0.857526903673371}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:48:49,355][0m Trial 39 finished with value: 0.07423989159219405 and parameters: {'observation_period_num': 85, 'train_rates': 0.8519915369077059, 'learning_rate': 0.00032140870033771054, 'batch_size': 90, 'step_size': 11, 'gamma': 0.8824417650872568}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:49:50,615][0m Trial 40 finished with value: 0.058473151490510064 and parameters: {'observation_period_num': 119, 'train_rates': 0.8869405400179228, 'learning_rate': 0.00020580093391889986, 'batch_size': 114, 'step_size': 7, 'gamma': 0.8734891763567383}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:50:54,840][0m Trial 41 finished with value: 0.035004617754408114 and parameters: {'observation_period_num': 26, 'train_rates': 0.7999127102315456, 'learning_rate': 0.00043777018631809233, 'batch_size': 100, 'step_size': 10, 'gamma': 0.8947226556581439}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:51:59,304][0m Trial 42 finished with value: 0.03322702846374394 and parameters: {'observation_period_num': 17, 'train_rates': 0.7940562352282349, 'learning_rate': 0.0009749939466120811, 'batch_size': 100, 'step_size': 10, 'gamma': 0.8947696001019783}. Best is trial 32 with value: 0.02804950074377385.[0m
[32m[I 2025-01-07 18:52:56,702][0m Trial 43 finished with value: 0.02777959550669613 and parameters: {'observation_period_num': 14, 'train_rates': 0.7931929475147242, 'learning_rate': 0.0009245364438999779, 'batch_size': 125, 'step_size': 8, 'gamma': 0.9156659980956351}. Best is trial 43 with value: 0.02777959550669613.[0m
[32m[I 2025-01-07 18:53:53,799][0m Trial 44 finished with value: 0.027039336962135216 and parameters: {'observation_period_num': 13, 'train_rates': 0.7738617552635233, 'learning_rate': 0.0008575358526368714, 'batch_size': 126, 'step_size': 8, 'gamma': 0.9156225054834012}. Best is trial 44 with value: 0.027039336962135216.[0m
[32m[I 2025-01-07 18:54:50,571][0m Trial 45 finished with value: 0.027640060795255515 and parameters: {'observation_period_num': 11, 'train_rates': 0.7710334249050181, 'learning_rate': 0.000877073476982698, 'batch_size': 124, 'step_size': 7, 'gamma': 0.915230524441568}. Best is trial 44 with value: 0.027039336962135216.[0m
[32m[I 2025-01-07 18:55:44,557][0m Trial 46 finished with value: 0.028540048748254776 and parameters: {'observation_period_num': 10, 'train_rates': 0.7331338698762424, 'learning_rate': 0.0009838544315471135, 'batch_size': 131, 'step_size': 7, 'gamma': 0.9215904977654414}. Best is trial 44 with value: 0.027039336962135216.[0m
[32m[I 2025-01-07 18:56:31,592][0m Trial 47 finished with value: 0.17999042972256352 and parameters: {'observation_period_num': 250, 'train_rates': 0.7351549834048312, 'learning_rate': 0.0008801693919585207, 'batch_size': 159, 'step_size': 7, 'gamma': 0.929070260151701}. Best is trial 44 with value: 0.027039336962135216.[0m
[32m[I 2025-01-07 18:57:28,294][0m Trial 48 finished with value: 0.031447942926347436 and parameters: {'observation_period_num': 12, 'train_rates': 0.7765998824253254, 'learning_rate': 0.0009990408200266007, 'batch_size': 130, 'step_size': 8, 'gamma': 0.914190489021738}. Best is trial 44 with value: 0.027039336962135216.[0m
[32m[I 2025-01-07 18:58:15,030][0m Trial 49 finished with value: 0.03323531647693816 and parameters: {'observation_period_num': 5, 'train_rates': 0.7531816382768225, 'learning_rate': 0.0002664580381904033, 'batch_size': 179, 'step_size': 8, 'gamma': 0.9180559554393982}. Best is trial 44 with value: 0.027039336962135216.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-07 18:58:15,040][0m A new study created in memory with name: no-name-633b6e1e-af68-42bd-94c6-6d9d85077cb5[0m
[32m[I 2025-01-07 18:59:28,722][0m Trial 0 finished with value: 0.09835458546876907 and parameters: {'observation_period_num': 114, 'train_rates': 0.9787548252714315, 'learning_rate': 3.426132863137562e-05, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9133532786837191}. Best is trial 0 with value: 0.09835458546876907.[0m
[32m[I 2025-01-07 19:00:29,704][0m Trial 1 finished with value: 0.15225154681083483 and parameters: {'observation_period_num': 156, 'train_rates': 0.8607140759839941, 'learning_rate': 2.411274928257081e-05, 'batch_size': 116, 'step_size': 15, 'gamma': 0.833886216311827}. Best is trial 0 with value: 0.09835458546876907.[0m
[32m[I 2025-01-07 19:01:33,252][0m Trial 2 finished with value: 0.19828260480611667 and parameters: {'observation_period_num': 88, 'train_rates': 0.693409280260527, 'learning_rate': 1.7322694753982054e-05, 'batch_size': 82, 'step_size': 15, 'gamma': 0.8034552270292559}. Best is trial 0 with value: 0.09835458546876907.[0m
[32m[I 2025-01-07 19:02:32,876][0m Trial 3 finished with value: 1.0139346971075134 and parameters: {'observation_period_num': 201, 'train_rates': 0.8532267934157891, 'learning_rate': 1.1659554758803854e-06, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8956263495500681}. Best is trial 0 with value: 0.09835458546876907.[0m
[32m[I 2025-01-07 19:03:32,198][0m Trial 4 finished with value: 0.2976382374763489 and parameters: {'observation_period_num': 119, 'train_rates': 0.9284613586438843, 'learning_rate': 1.7925227367756023e-05, 'batch_size': 222, 'step_size': 7, 'gamma': 0.9388491963463969}. Best is trial 0 with value: 0.09835458546876907.[0m
[32m[I 2025-01-07 19:04:30,490][0m Trial 5 finished with value: 0.08512829447664866 and parameters: {'observation_period_num': 117, 'train_rates': 0.8556912254470911, 'learning_rate': 0.0006589614742190054, 'batch_size': 173, 'step_size': 15, 'gamma': 0.8594639050134545}. Best is trial 5 with value: 0.08512829447664866.[0m
[32m[I 2025-01-07 19:05:21,458][0m Trial 6 finished with value: 0.3891337843654634 and parameters: {'observation_period_num': 192, 'train_rates': 0.6377699797086279, 'learning_rate': 0.00011414241065791093, 'batch_size': 137, 'step_size': 3, 'gamma': 0.9713047836056659}. Best is trial 5 with value: 0.08512829447664866.[0m
[32m[I 2025-01-07 19:06:11,506][0m Trial 7 finished with value: 0.5515336341117588 and parameters: {'observation_period_num': 155, 'train_rates': 0.7079106263146714, 'learning_rate': 1.027209316162544e-05, 'batch_size': 181, 'step_size': 9, 'gamma': 0.8009942948202138}. Best is trial 5 with value: 0.08512829447664866.[0m
[32m[I 2025-01-07 19:07:20,158][0m Trial 8 finished with value: 0.1963476330107674 and parameters: {'observation_period_num': 219, 'train_rates': 0.834317477204883, 'learning_rate': 3.75751981255822e-05, 'batch_size': 115, 'step_size': 13, 'gamma': 0.8688905764830526}. Best is trial 5 with value: 0.08512829447664866.[0m
[32m[I 2025-01-07 19:08:42,751][0m Trial 9 finished with value: 0.2534561643045243 and parameters: {'observation_period_num': 22, 'train_rates': 0.9502139135397083, 'learning_rate': 8.618221898729603e-06, 'batch_size': 98, 'step_size': 9, 'gamma': 0.8640299341922622}. Best is trial 5 with value: 0.08512829447664866.[0m
Early stopping at epoch 66
[32m[I 2025-01-07 19:10:25,861][0m Trial 10 finished with value: 0.10269409849961826 and parameters: {'observation_period_num': 29, 'train_rates': 0.7618544981219332, 'learning_rate': 0.0008179162521167385, 'batch_size': 42, 'step_size': 1, 'gamma': 0.766569112529295}. Best is trial 5 with value: 0.08512829447664866.[0m
[32m[I 2025-01-07 19:11:18,276][0m Trial 11 finished with value: 0.07284343242645264 and parameters: {'observation_period_num': 82, 'train_rates': 0.9711127870215389, 'learning_rate': 0.000993417117261961, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9132997838340964}. Best is trial 11 with value: 0.07284343242645264.[0m
[32m[I 2025-01-07 19:12:08,440][0m Trial 12 finished with value: 0.07322835551019301 and parameters: {'observation_period_num': 69, 'train_rates': 0.9012166690592024, 'learning_rate': 0.0008959501821216763, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9469838146528069}. Best is trial 11 with value: 0.07284343242645264.[0m
[32m[I 2025-01-07 19:12:58,601][0m Trial 13 finished with value: 0.14937204122543335 and parameters: {'observation_period_num': 66, 'train_rates': 0.9130511762130076, 'learning_rate': 0.00035364875438977783, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9852098720698614}. Best is trial 11 with value: 0.07284343242645264.[0m
[32m[I 2025-01-07 19:13:52,077][0m Trial 14 finished with value: 0.134254589676857 and parameters: {'observation_period_num': 70, 'train_rates': 0.989134795281294, 'learning_rate': 0.00018018722354180917, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9382529857952012}. Best is trial 11 with value: 0.07284343242645264.[0m
[32m[I 2025-01-07 19:14:43,716][0m Trial 15 finished with value: 0.10515979478057001 and parameters: {'observation_period_num': 56, 'train_rates': 0.9019692596488756, 'learning_rate': 0.0001611440380598083, 'batch_size': 212, 'step_size': 12, 'gamma': 0.9374567572239907}. Best is trial 11 with value: 0.07284343242645264.[0m
[32m[I 2025-01-07 19:15:31,612][0m Trial 16 finished with value: 0.06112097892218145 and parameters: {'observation_period_num': 13, 'train_rates': 0.7895058637791199, 'learning_rate': 0.00038531956604293953, 'batch_size': 214, 'step_size': 7, 'gamma': 0.8976015611793238}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:16:02,886][0m Trial 17 finished with value: 0.06459313797654573 and parameters: {'observation_period_num': 12, 'train_rates': 0.78109387650665, 'learning_rate': 0.0003510951633474654, 'batch_size': 213, 'step_size': 7, 'gamma': 0.9020603210731315}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:16:34,451][0m Trial 18 finished with value: 0.0840931411832571 and parameters: {'observation_period_num': 5, 'train_rates': 0.7831557670032735, 'learning_rate': 6.197148398247635e-05, 'batch_size': 204, 'step_size': 7, 'gamma': 0.8904915272616191}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:17:07,732][0m Trial 19 finished with value: 0.08326985118522619 and parameters: {'observation_period_num': 38, 'train_rates': 0.7367127671308613, 'learning_rate': 0.00036752397027400076, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8293468944558852}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:17:35,782][0m Trial 20 finished with value: 0.569719906383329 and parameters: {'observation_period_num': 14, 'train_rates': 0.6221737058986453, 'learning_rate': 2.6573691227680083e-06, 'batch_size': 227, 'step_size': 5, 'gamma': 0.8934159052689401}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:18:08,440][0m Trial 21 finished with value: 0.07286396941968373 and parameters: {'observation_period_num': 46, 'train_rates': 0.8122469233736298, 'learning_rate': 0.0003169240358036029, 'batch_size': 234, 'step_size': 8, 'gamma': 0.9116109191930096}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:18:41,670][0m Trial 22 finished with value: 0.07711293320615806 and parameters: {'observation_period_num': 87, 'train_rates': 0.8025933794189779, 'learning_rate': 0.0004889850134482856, 'batch_size': 190, 'step_size': 7, 'gamma': 0.9112014178567047}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:19:09,086][0m Trial 23 finished with value: 0.2078534774765758 and parameters: {'observation_period_num': 249, 'train_rates': 0.7572935892341498, 'learning_rate': 0.0001934838125283801, 'batch_size': 232, 'step_size': 9, 'gamma': 0.9565968291243673}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:19:53,325][0m Trial 24 finished with value: 0.13995584053765037 and parameters: {'observation_period_num': 38, 'train_rates': 0.6884758332158696, 'learning_rate': 8.789811436956103e-05, 'batch_size': 200, 'step_size': 4, 'gamma': 0.9163130911914615}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:20:26,887][0m Trial 25 finished with value: 0.10537151057464966 and parameters: {'observation_period_num': 89, 'train_rates': 0.7425042640910751, 'learning_rate': 0.00026965209767728255, 'batch_size': 238, 'step_size': 6, 'gamma': 0.8486899757827446}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:21:01,760][0m Trial 26 finished with value: 0.193934045943552 and parameters: {'observation_period_num': 148, 'train_rates': 0.661010451287608, 'learning_rate': 0.000965989522473028, 'batch_size': 156, 'step_size': 10, 'gamma': 0.8800956172362491}. Best is trial 16 with value: 0.06112097892218145.[0m
[32m[I 2025-01-07 19:21:41,082][0m Trial 27 finished with value: 0.06076534664129267 and parameters: {'observation_period_num': 25, 'train_rates': 0.8276272274645708, 'learning_rate': 0.0005669262123193001, 'batch_size': 159, 'step_size': 2, 'gamma': 0.9246981302522261}. Best is trial 27 with value: 0.06076534664129267.[0m
[32m[I 2025-01-07 19:22:17,415][0m Trial 28 finished with value: 0.060237155182857736 and parameters: {'observation_period_num': 7, 'train_rates': 0.8241667500653823, 'learning_rate': 0.0005175435964423788, 'batch_size': 165, 'step_size': 1, 'gamma': 0.9622637928714274}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:22:54,981][0m Trial 29 finished with value: 0.06717941809815971 and parameters: {'observation_period_num': 48, 'train_rates': 0.823617358849108, 'learning_rate': 0.0005279121717806172, 'batch_size': 159, 'step_size': 1, 'gamma': 0.9885915441529446}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:23:44,290][0m Trial 30 finished with value: 0.0759450825439258 and parameters: {'observation_period_num': 26, 'train_rates': 0.8724435713346151, 'learning_rate': 5.557554381408361e-05, 'batch_size': 119, 'step_size': 2, 'gamma': 0.9586146944007055}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:24:16,151][0m Trial 31 finished with value: 0.06820854011973644 and parameters: {'observation_period_num': 9, 'train_rates': 0.7748013230846292, 'learning_rate': 0.0005109611095718335, 'batch_size': 192, 'step_size': 3, 'gamma': 0.9191058134566857}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:24:52,092][0m Trial 32 finished with value: 0.06820531895126754 and parameters: {'observation_period_num': 5, 'train_rates': 0.7916604273405965, 'learning_rate': 0.00024702551249852653, 'batch_size': 163, 'step_size': 2, 'gamma': 0.9261247227326597}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:25:24,358][0m Trial 33 finished with value: 0.09382826744424187 and parameters: {'observation_period_num': 26, 'train_rates': 0.8312937463332614, 'learning_rate': 0.0001026560110281037, 'batch_size': 218, 'step_size': 3, 'gamma': 0.896781984890268}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:26:01,202][0m Trial 34 finished with value: 0.07959998163449414 and parameters: {'observation_period_num': 20, 'train_rates': 0.7221121333770563, 'learning_rate': 0.0004370531563903202, 'batch_size': 147, 'step_size': 4, 'gamma': 0.9722732947151397}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:27:24,975][0m Trial 35 finished with value: 0.07233178480104967 and parameters: {'observation_period_num': 39, 'train_rates': 0.8756959335964786, 'learning_rate': 0.0002142991951113312, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9306881946789197}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:28:02,287][0m Trial 36 finished with value: 0.09641223046760838 and parameters: {'observation_period_num': 99, 'train_rates': 0.8445308457023458, 'learning_rate': 0.00013530877003861645, 'batch_size': 181, 'step_size': 2, 'gamma': 0.9530938609199472}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:28:54,084][0m Trial 37 finished with value: 0.09146050052584974 and parameters: {'observation_period_num': 58, 'train_rates': 0.8008478762139764, 'learning_rate': 0.000605480916248937, 'batch_size': 123, 'step_size': 1, 'gamma': 0.881789717637137}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:29:39,644][0m Trial 38 finished with value: 0.07099955715239048 and parameters: {'observation_period_num': 35, 'train_rates': 0.7685896493163084, 'learning_rate': 0.0003385537962177956, 'batch_size': 130, 'step_size': 8, 'gamma': 0.905461376802131}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:35:18,877][0m Trial 39 finished with value: 0.06826969912859815 and parameters: {'observation_period_num': 18, 'train_rates': 0.8848959383517171, 'learning_rate': 0.000620858141400761, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8430142456502187}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:36:14,989][0m Trial 40 finished with value: 0.10565939927030177 and parameters: {'observation_period_num': 52, 'train_rates': 0.8555314541821627, 'learning_rate': 7.015269004899879e-05, 'batch_size': 103, 'step_size': 8, 'gamma': 0.9660497235068725}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:36:51,356][0m Trial 41 finished with value: 0.06714413104192266 and parameters: {'observation_period_num': 49, 'train_rates': 0.8178166024584411, 'learning_rate': 0.0006403251509804212, 'batch_size': 162, 'step_size': 1, 'gamma': 0.9891523897441465}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:37:27,358][0m Trial 42 finished with value: 0.08507767802802846 and parameters: {'observation_period_num': 134, 'train_rates': 0.8184271774158454, 'learning_rate': 0.0007599563128863233, 'batch_size': 170, 'step_size': 2, 'gamma': 0.979471322121839}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:38:05,436][0m Trial 43 finished with value: 0.16128308417031129 and parameters: {'observation_period_num': 184, 'train_rates': 0.8406843960227749, 'learning_rate': 0.00026889613044442487, 'batch_size': 150, 'step_size': 1, 'gamma': 0.9454041006942769}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:38:45,897][0m Trial 44 finished with value: 0.06710452983696225 and parameters: {'observation_period_num': 17, 'train_rates': 0.7899272938533286, 'learning_rate': 0.0006318080802415834, 'batch_size': 138, 'step_size': 3, 'gamma': 0.9647019486247899}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:39:26,269][0m Trial 45 finished with value: 0.461010714236657 and parameters: {'observation_period_num': 16, 'train_rates': 0.750118808706772, 'learning_rate': 4.046760976151213e-06, 'batch_size': 135, 'step_size': 3, 'gamma': 0.9264143194523412}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:40:16,664][0m Trial 46 finished with value: 0.06758476379713055 and parameters: {'observation_period_num': 28, 'train_rates': 0.785407052760263, 'learning_rate': 0.0003730935381216998, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9026206446849802}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:41:16,781][0m Trial 47 finished with value: 0.1018144882355745 and parameters: {'observation_period_num': 6, 'train_rates': 0.717893929272481, 'learning_rate': 3.278556922811898e-05, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8789813521037578}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:41:50,214][0m Trial 48 finished with value: 0.07414267074871023 and parameters: {'observation_period_num': 32, 'train_rates': 0.8022474052583622, 'learning_rate': 0.00014569738434668918, 'batch_size': 183, 'step_size': 3, 'gamma': 0.9652438769825896}. Best is trial 28 with value: 0.060237155182857736.[0m
[32m[I 2025-01-07 19:42:20,120][0m Trial 49 finished with value: 0.15557676259933587 and parameters: {'observation_period_num': 176, 'train_rates': 0.7720405019830303, 'learning_rate': 0.0008158449209372203, 'batch_size': 206, 'step_size': 14, 'gamma': 0.7578711659960181}. Best is trial 28 with value: 0.060237155182857736.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-07 19:42:20,129][0m A new study created in memory with name: no-name-4afd76f3-99dc-4ece-af69-26e1b5db2c63[0m
[32m[I 2025-01-07 19:43:00,603][0m Trial 0 finished with value: 0.09755724668502808 and parameters: {'observation_period_num': 121, 'train_rates': 0.8979039825008619, 'learning_rate': 0.00013192177677383721, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9072115160528285}. Best is trial 0 with value: 0.09755724668502808.[0m
[32m[I 2025-01-07 19:43:55,470][0m Trial 1 finished with value: 0.8834792757831446 and parameters: {'observation_period_num': 247, 'train_rates': 0.75698651351779, 'learning_rate': 1.4141571669005252e-06, 'batch_size': 86, 'step_size': 5, 'gamma': 0.8649215570504011}. Best is trial 0 with value: 0.09755724668502808.[0m
[32m[I 2025-01-07 19:44:24,945][0m Trial 2 finished with value: 0.12266073263052737 and parameters: {'observation_period_num': 173, 'train_rates': 0.8340213655110326, 'learning_rate': 9.7409151586773e-05, 'batch_size': 232, 'step_size': 12, 'gamma': 0.8695770429036127}. Best is trial 0 with value: 0.09755724668502808.[0m
[32m[I 2025-01-07 19:45:08,500][0m Trial 3 finished with value: 0.11365789693334828 and parameters: {'observation_period_num': 87, 'train_rates': 0.8878643955477177, 'learning_rate': 0.00043887446912106433, 'batch_size': 147, 'step_size': 1, 'gamma': 0.9106735803199494}. Best is trial 0 with value: 0.09755724668502808.[0m
[32m[I 2025-01-07 19:46:03,825][0m Trial 4 finished with value: 0.2489000054606829 and parameters: {'observation_period_num': 216, 'train_rates': 0.7091649400181028, 'learning_rate': 0.0007665859524199486, 'batch_size': 86, 'step_size': 11, 'gamma': 0.8270801461782933}. Best is trial 0 with value: 0.09755724668502808.[0m
[32m[I 2025-01-07 19:46:30,182][0m Trial 5 finished with value: 0.3365923604577087 and parameters: {'observation_period_num': 223, 'train_rates': 0.606513047813136, 'learning_rate': 6.647596877090526e-05, 'batch_size': 218, 'step_size': 9, 'gamma': 0.860154326814538}. Best is trial 0 with value: 0.09755724668502808.[0m
[32m[I 2025-01-07 19:47:06,538][0m Trial 6 finished with value: 0.04394241749052552 and parameters: {'observation_period_num': 23, 'train_rates': 0.8117713827382244, 'learning_rate': 7.725338491855299e-05, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9873578828806693}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:47:48,347][0m Trial 7 finished with value: 0.654396750498563 and parameters: {'observation_period_num': 218, 'train_rates': 0.6959404533327276, 'learning_rate': 1.9884781554081485e-06, 'batch_size': 117, 'step_size': 15, 'gamma': 0.8396928695989784}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:48:37,929][0m Trial 8 finished with value: 0.046977484646515966 and parameters: {'observation_period_num': 59, 'train_rates': 0.8924494596922008, 'learning_rate': 0.00021622887520977017, 'batch_size': 128, 'step_size': 1, 'gamma': 0.9735181953600586}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:50:26,222][0m Trial 9 finished with value: 0.10141995778585293 and parameters: {'observation_period_num': 72, 'train_rates': 0.7046409939271593, 'learning_rate': 0.00022023858508540554, 'batch_size': 44, 'step_size': 10, 'gamma': 0.8548125180450801}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:51:05,013][0m Trial 10 finished with value: 0.21988199651241302 and parameters: {'observation_period_num': 11, 'train_rates': 0.9753386721359094, 'learning_rate': 1.2125875539438301e-05, 'batch_size': 196, 'step_size': 6, 'gamma': 0.7514166249812083}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:51:42,666][0m Trial 11 finished with value: 0.06348576329379196 and parameters: {'observation_period_num': 12, 'train_rates': 0.8307686727785557, 'learning_rate': 2.0345999471974868e-05, 'batch_size': 170, 'step_size': 1, 'gamma': 0.9897111280145274}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:52:31,776][0m Trial 12 finished with value: 0.13298332691192627 and parameters: {'observation_period_num': 49, 'train_rates': 0.9515675895055148, 'learning_rate': 4.282667465320568e-05, 'batch_size': 182, 'step_size': 4, 'gamma': 0.9851290851158517}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:53:25,492][0m Trial 13 finished with value: 0.14458275249027663 and parameters: {'observation_period_num': 46, 'train_rates': 0.8945687372588046, 'learning_rate': 8.32871436559377e-06, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9485886648314464}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:55:48,057][0m Trial 14 finished with value: 0.11383207365521775 and parameters: {'observation_period_num': 117, 'train_rates': 0.774170194339171, 'learning_rate': 0.0002925960269936437, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9473270779785335}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:56:25,215][0m Trial 15 finished with value: 0.05423651119546764 and parameters: {'observation_period_num': 39, 'train_rates': 0.8311083306864823, 'learning_rate': 0.000978280105139822, 'batch_size': 243, 'step_size': 14, 'gamma': 0.9532245991683921}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:57:06,489][0m Trial 16 finished with value: 0.13032490886174716 and parameters: {'observation_period_num': 78, 'train_rates': 0.9324076243523467, 'learning_rate': 3.966027779937418e-05, 'batch_size': 163, 'step_size': 8, 'gamma': 0.9114328544016319}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:57:39,303][0m Trial 17 finished with value: 0.2129671281223656 and parameters: {'observation_period_num': 154, 'train_rates': 0.8672712221669912, 'learning_rate': 0.0001533049352343746, 'batch_size': 209, 'step_size': 3, 'gamma': 0.8021897393591361}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:58:52,924][0m Trial 18 finished with value: 0.23401360563351192 and parameters: {'observation_period_num': 97, 'train_rates': 0.7896710107743382, 'learning_rate': 4.044040053279079e-06, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9665984821594673}. Best is trial 6 with value: 0.04394241749052552.[0m
[32m[I 2025-01-07 19:59:36,059][0m Trial 19 finished with value: 0.04206593146712597 and parameters: {'observation_period_num': 30, 'train_rates': 0.7374630400509419, 'learning_rate': 0.00037702074794856744, 'batch_size': 125, 'step_size': 7, 'gamma': 0.9221628174965915}. Best is trial 19 with value: 0.04206593146712597.[0m
[32m[I 2025-01-07 20:00:02,884][0m Trial 20 finished with value: 0.04118488545526056 and parameters: {'observation_period_num': 5, 'train_rates': 0.6380380234681637, 'learning_rate': 0.0004946516412054451, 'batch_size': 254, 'step_size': 7, 'gamma': 0.9324194949184763}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:00:33,882][0m Trial 21 finished with value: 0.06275493941373295 and parameters: {'observation_period_num': 5, 'train_rates': 0.6123338118231844, 'learning_rate': 0.00047735415789109645, 'batch_size': 254, 'step_size': 7, 'gamma': 0.9285516385353593}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:01:08,929][0m Trial 22 finished with value: 0.047096284042144644 and parameters: {'observation_period_num': 35, 'train_rates': 0.7419029982141325, 'learning_rate': 0.00045580662102882944, 'batch_size': 189, 'step_size': 8, 'gamma': 0.8963767754659051}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:01:55,978][0m Trial 23 finished with value: 0.06240894277272058 and parameters: {'observation_period_num': 23, 'train_rates': 0.6491723545096544, 'learning_rate': 6.399961119284023e-05, 'batch_size': 104, 'step_size': 6, 'gamma': 0.9278127911703902}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:02:28,987][0m Trial 24 finished with value: 0.06231657048884016 and parameters: {'observation_period_num': 29, 'train_rates': 0.6696839479514171, 'learning_rate': 0.0003137599863232441, 'batch_size': 165, 'step_size': 9, 'gamma': 0.8912716167234481}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:03:00,199][0m Trial 25 finished with value: 0.08668224377535778 and parameters: {'observation_period_num': 62, 'train_rates': 0.7349952213656388, 'learning_rate': 0.0007531982307873843, 'batch_size': 223, 'step_size': 6, 'gamma': 0.9319636271975392}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:03:29,288][0m Trial 26 finished with value: 0.12777602924396128 and parameters: {'observation_period_num': 102, 'train_rates': 0.6485323227083098, 'learning_rate': 0.00012752417914350567, 'batch_size': 206, 'step_size': 7, 'gamma': 0.9668809524749719}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:04:09,686][0m Trial 27 finished with value: 0.10555391701368186 and parameters: {'observation_period_num': 153, 'train_rates': 0.8004645783411823, 'learning_rate': 7.262732426546538e-05, 'batch_size': 140, 'step_size': 9, 'gamma': 0.9414831704198193}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:05:18,360][0m Trial 28 finished with value: 0.10910920606064146 and parameters: {'observation_period_num': 25, 'train_rates': 0.6747763815060847, 'learning_rate': 1.651970693901037e-05, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8860113674980324}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:05:57,928][0m Trial 29 finished with value: 0.04902584318233573 and parameters: {'observation_period_num': 58, 'train_rates': 0.8013892101218384, 'learning_rate': 0.00013800663037171572, 'batch_size': 148, 'step_size': 11, 'gamma': 0.9206202920229903}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:10:17,705][0m Trial 30 finished with value: 0.17680429173158382 and parameters: {'observation_period_num': 126, 'train_rates': 0.632774582575279, 'learning_rate': 2.8930445941142963e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9617504830638572}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:11:08,941][0m Trial 31 finished with value: 0.04748574941283479 and parameters: {'observation_period_num': 5, 'train_rates': 0.8581829823887459, 'learning_rate': 0.0002605625964481142, 'batch_size': 132, 'step_size': 3, 'gamma': 0.9753524861220053}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:12:02,667][0m Trial 32 finished with value: 0.04944377535333236 and parameters: {'observation_period_num': 58, 'train_rates': 0.9171798065309257, 'learning_rate': 0.00020807155840723993, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9736246161663815}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:12:57,951][0m Trial 33 finished with value: 0.041636215831092796 and parameters: {'observation_period_num': 22, 'train_rates': 0.7630499456151206, 'learning_rate': 0.0005110878003251524, 'batch_size': 105, 'step_size': 2, 'gamma': 0.9814393165498747}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:13:55,271][0m Trial 34 finished with value: 0.04253812804101752 and parameters: {'observation_period_num': 28, 'train_rates': 0.7511588338534746, 'learning_rate': 0.0005632009776710866, 'batch_size': 98, 'step_size': 4, 'gamma': 0.9401384074060748}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:14:51,100][0m Trial 35 finished with value: 0.050009376650545556 and parameters: {'observation_period_num': 43, 'train_rates': 0.7599973930724184, 'learning_rate': 0.0005547549168580655, 'batch_size': 103, 'step_size': 2, 'gamma': 0.878427612927095}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:15:55,161][0m Trial 36 finished with value: 0.04400478048300921 and parameters: {'observation_period_num': 22, 'train_rates': 0.7251663005945836, 'learning_rate': 0.0006923939134952451, 'batch_size': 84, 'step_size': 4, 'gamma': 0.9086990541585224}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:16:57,589][0m Trial 37 finished with value: 0.1595839178737472 and parameters: {'observation_period_num': 195, 'train_rates': 0.7660083260637165, 'learning_rate': 0.00043517103289287883, 'batch_size': 86, 'step_size': 4, 'gamma': 0.9341205917578288}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:17:55,500][0m Trial 38 finished with value: 0.08462983891287626 and parameters: {'observation_period_num': 75, 'train_rates': 0.6776800480561053, 'learning_rate': 0.0009800134349452643, 'batch_size': 99, 'step_size': 2, 'gamma': 0.9548634557701963}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:18:43,571][0m Trial 39 finished with value: 0.05888040471010982 and parameters: {'observation_period_num': 35, 'train_rates': 0.721616301394047, 'learning_rate': 0.0003496235114108268, 'batch_size': 152, 'step_size': 2, 'gamma': 0.9181128603654837}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:20:00,201][0m Trial 40 finished with value: 0.1582550407392893 and parameters: {'observation_period_num': 247, 'train_rates': 0.6915606817987179, 'learning_rate': 0.000646592278284841, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9041889448725267}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:20:54,777][0m Trial 41 finished with value: 0.04574924966682008 and parameters: {'observation_period_num': 19, 'train_rates': 0.8140956664417255, 'learning_rate': 9.31721023734106e-05, 'batch_size': 115, 'step_size': 10, 'gamma': 0.9815996776226024}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:21:56,526][0m Trial 42 finished with value: 0.042158672598584385 and parameters: {'observation_period_num': 16, 'train_rates': 0.746456319839164, 'learning_rate': 0.00019678213991151608, 'batch_size': 94, 'step_size': 6, 'gamma': 0.942204774467593}. Best is trial 20 with value: 0.04118488545526056.[0m
[32m[I 2025-01-07 20:23:04,098][0m Trial 43 finished with value: 0.037742130173332736 and parameters: {'observation_period_num': 15, 'train_rates': 0.7465675899505981, 'learning_rate': 0.0001774511415414142, 'batch_size': 87, 'step_size': 6, 'gamma': 0.9363408956845395}. Best is trial 43 with value: 0.037742130173332736.[0m
[32m[I 2025-01-07 20:24:46,151][0m Trial 44 finished with value: 0.032405527097424204 and parameters: {'observation_period_num': 14, 'train_rates': 0.7149665068271523, 'learning_rate': 0.00017509166760447238, 'batch_size': 55, 'step_size': 6, 'gamma': 0.9613131907363522}. Best is trial 44 with value: 0.032405527097424204.[0m
[32m[I 2025-01-07 20:26:38,080][0m Trial 45 finished with value: 0.06287614230451913 and parameters: {'observation_period_num': 48, 'train_rates': 0.7205110193387969, 'learning_rate': 0.00014770348609928756, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9632289622504183}. Best is trial 44 with value: 0.032405527097424204.[0m
[32m[I 2025-01-07 20:28:24,009][0m Trial 46 finished with value: 0.02858283390710271 and parameters: {'observation_period_num': 10, 'train_rates': 0.7753632743210644, 'learning_rate': 0.0003557334080925216, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9572422378739834}. Best is trial 46 with value: 0.02858283390710271.[0m
[32m[I 2025-01-07 20:30:10,671][0m Trial 47 finished with value: 0.02844311827654714 and parameters: {'observation_period_num': 12, 'train_rates': 0.7724306457794774, 'learning_rate': 0.00010528801897096439, 'batch_size': 51, 'step_size': 5, 'gamma': 0.9565286584761599}. Best is trial 47 with value: 0.02844311827654714.[0m
[32m[I 2025-01-07 20:32:05,382][0m Trial 48 finished with value: 0.3534699963004906 and parameters: {'observation_period_num': 7, 'train_rates': 0.7856573867170523, 'learning_rate': 1.0239134452586773e-06, 'batch_size': 55, 'step_size': 6, 'gamma': 0.9565430855191038}. Best is trial 47 with value: 0.02844311827654714.[0m
[32m[I 2025-01-07 20:35:00,664][0m Trial 49 finished with value: 0.06193912559668544 and parameters: {'observation_period_num': 14, 'train_rates': 0.7056425032476481, 'learning_rate': 9.472424240374162e-05, 'batch_size': 27, 'step_size': 5, 'gamma': 0.9500442360611757}. Best is trial 47 with value: 0.02844311827654714.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-07 20:35:00,675][0m A new study created in memory with name: no-name-1853fc73-9a26-47d6-a695-cae838188aa6[0m
[32m[I 2025-01-07 20:37:14,954][0m Trial 0 finished with value: 0.1982359448486791 and parameters: {'observation_period_num': 127, 'train_rates': 0.6617241091476395, 'learning_rate': 0.0006962719859659578, 'batch_size': 32, 'step_size': 4, 'gamma': 0.8545785982042287}. Best is trial 0 with value: 0.1982359448486791.[0m
[32m[I 2025-01-07 20:37:45,138][0m Trial 1 finished with value: 2.187642698445596 and parameters: {'observation_period_num': 162, 'train_rates': 0.8699607270180494, 'learning_rate': 1.3099890448591773e-06, 'batch_size': 193, 'step_size': 3, 'gamma': 0.8007746312022465}. Best is trial 0 with value: 0.1982359448486791.[0m
[32m[I 2025-01-07 20:38:13,245][0m Trial 2 finished with value: 0.09691771301259468 and parameters: {'observation_period_num': 96, 'train_rates': 0.8922267819612246, 'learning_rate': 8.212377589143609e-05, 'batch_size': 249, 'step_size': 6, 'gamma': 0.9574897479992981}. Best is trial 2 with value: 0.09691771301259468.[0m
[32m[I 2025-01-07 20:40:01,850][0m Trial 3 finished with value: 0.1275079090825536 and parameters: {'observation_period_num': 177, 'train_rates': 0.8019844703117397, 'learning_rate': 0.0007542122514402383, 'batch_size': 45, 'step_size': 11, 'gamma': 0.8284648900411429}. Best is trial 2 with value: 0.09691771301259468.[0m
[32m[I 2025-01-07 20:42:03,741][0m Trial 4 finished with value: 0.22375639488822535 and parameters: {'observation_period_num': 90, 'train_rates': 0.9006261596705452, 'learning_rate': 5.506249871007951e-06, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8360339293436906}. Best is trial 2 with value: 0.09691771301259468.[0m
[32m[I 2025-01-07 20:42:37,640][0m Trial 5 finished with value: 0.05197239798658034 and parameters: {'observation_period_num': 87, 'train_rates': 0.8697510021207575, 'learning_rate': 0.0003271750980188172, 'batch_size': 176, 'step_size': 15, 'gamma': 0.752413793696514}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:43:04,886][0m Trial 6 finished with value: 0.0925835736661461 and parameters: {'observation_period_num': 135, 'train_rates': 0.8926517531788378, 'learning_rate': 0.0006306155679146114, 'batch_size': 211, 'step_size': 14, 'gamma': 0.8264543515541497}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:43:27,053][0m Trial 7 finished with value: 0.21527193530400593 and parameters: {'observation_period_num': 60, 'train_rates': 0.6112249509632133, 'learning_rate': 0.00024007366031628466, 'batch_size': 238, 'step_size': 2, 'gamma': 0.8618962413214107}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:43:57,150][0m Trial 8 finished with value: 0.08935805809933965 and parameters: {'observation_period_num': 113, 'train_rates': 0.8556905641935115, 'learning_rate': 0.0002582675440663553, 'batch_size': 196, 'step_size': 15, 'gamma': 0.9043704610744542}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:45:05,366][0m Trial 9 finished with value: 0.08814567349032859 and parameters: {'observation_period_num': 165, 'train_rates': 0.8760955439816964, 'learning_rate': 6.368912906637387e-05, 'batch_size': 77, 'step_size': 12, 'gamma': 0.8172933655076902}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:45:46,469][0m Trial 10 finished with value: 0.37023407220840454 and parameters: {'observation_period_num': 251, 'train_rates': 0.9890297469064308, 'learning_rate': 1.3919648953863285e-05, 'batch_size': 141, 'step_size': 8, 'gamma': 0.7627169226014375}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:46:36,984][0m Trial 11 finished with value: 0.05926400947641665 and parameters: {'observation_period_num': 13, 'train_rates': 0.7426655962201085, 'learning_rate': 4.68961044275448e-05, 'batch_size': 101, 'step_size': 12, 'gamma': 0.7568606424038204}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:47:17,701][0m Trial 12 finished with value: 0.07236208912528706 and parameters: {'observation_period_num': 5, 'train_rates': 0.7143715251448003, 'learning_rate': 1.874482493887842e-05, 'batch_size': 128, 'step_size': 13, 'gamma': 0.7582005463942273}. Best is trial 5 with value: 0.05197239798658034.[0m
[32m[I 2025-01-07 20:47:56,807][0m Trial 13 finished with value: 0.04900405811911626 and parameters: {'observation_period_num': 6, 'train_rates': 0.7617700535311848, 'learning_rate': 0.00011036242934553063, 'batch_size': 134, 'step_size': 10, 'gamma': 0.7825173950571385}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:48:29,544][0m Trial 14 finished with value: 0.055013710739020304 and parameters: {'observation_period_num': 45, 'train_rates': 0.7829182611017522, 'learning_rate': 0.00018957978222499763, 'batch_size': 170, 'step_size': 9, 'gamma': 0.7909123767408006}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:49:11,008][0m Trial 15 finished with value: 0.0671084001660347 and parameters: {'observation_period_num': 51, 'train_rates': 0.9637405621593191, 'learning_rate': 0.00012763491138461684, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8911553484721384}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:49:58,058][0m Trial 16 finished with value: 0.09993060384255543 and parameters: {'observation_period_num': 224, 'train_rates': 0.8108254540756169, 'learning_rate': 0.00037960557531754494, 'batch_size': 109, 'step_size': 15, 'gamma': 0.785662772389548}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:50:29,598][0m Trial 17 finished with value: 0.11807796801142681 and parameters: {'observation_period_num': 34, 'train_rates': 0.7323869580536408, 'learning_rate': 2.4457745062659863e-05, 'batch_size': 168, 'step_size': 10, 'gamma': 0.776106357560538}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:51:32,622][0m Trial 18 finished with value: 0.23523233680242903 and parameters: {'observation_period_num': 84, 'train_rates': 0.9380606115791223, 'learning_rate': 7.529033733885105e-06, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9253269385444921}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:52:17,496][0m Trial 19 finished with value: 0.07126484719352631 and parameters: {'observation_period_num': 74, 'train_rates': 0.8356465198044785, 'learning_rate': 0.00011636705813856709, 'batch_size': 123, 'step_size': 13, 'gamma': 0.9782598128062905}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:53:30,930][0m Trial 20 finished with value: 0.049590275279785455 and parameters: {'observation_period_num': 26, 'train_rates': 0.7728586593651038, 'learning_rate': 4.026847790063678e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8010599202495184}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:54:46,566][0m Trial 21 finished with value: 0.06103468377164803 and parameters: {'observation_period_num': 25, 'train_rates': 0.7653168519923506, 'learning_rate': 3.163083251032757e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.8029326783561218}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:55:18,220][0m Trial 22 finished with value: 0.10175684788947033 and parameters: {'observation_period_num': 30, 'train_rates': 0.68763374704468, 'learning_rate': 4.381613284759588e-05, 'batch_size': 162, 'step_size': 10, 'gamma': 0.7526953266683788}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 20:55:44,636][0m Trial 23 finished with value: 0.04904573365441851 and parameters: {'observation_period_num': 62, 'train_rates': 0.8208266718218913, 'learning_rate': 0.00036626237122983237, 'batch_size': 221, 'step_size': 13, 'gamma': 0.7745408384131035}. Best is trial 13 with value: 0.04900405811911626.[0m
[32m[I 2025-01-07 21:00:59,528][0m Trial 24 finished with value: 0.04651897376679991 and parameters: {'observation_period_num': 59, 'train_rates': 0.8279215547031777, 'learning_rate': 9.903602732975512e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.7781882450762071}. Best is trial 24 with value: 0.04651897376679991.[0m
[32m[I 2025-01-07 21:06:18,418][0m Trial 25 finished with value: 0.18237699125438864 and parameters: {'observation_period_num': 63, 'train_rates': 0.8407411775406903, 'learning_rate': 0.00013571653244544158, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8459430051663855}. Best is trial 24 with value: 0.04651897376679991.[0m
[32m[I 2025-01-07 21:06:44,628][0m Trial 26 finished with value: 0.03382065718708275 and parameters: {'observation_period_num': 7, 'train_rates': 0.8116120717679831, 'learning_rate': 0.00043563678293666997, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8778703233150966}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:07:08,218][0m Trial 27 finished with value: 0.059334967100414734 and parameters: {'observation_period_num': 9, 'train_rates': 0.7537473759415161, 'learning_rate': 8.040421434985928e-05, 'batch_size': 256, 'step_size': 9, 'gamma': 0.8744468209950121}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:07:34,644][0m Trial 28 finished with value: 0.04082850414414366 and parameters: {'observation_period_num': 42, 'train_rates': 0.7958980188834248, 'learning_rate': 0.0009649407199012671, 'batch_size': 227, 'step_size': 14, 'gamma': 0.9133932411048709}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:07:58,165][0m Trial 29 finished with value: 0.18435147474989028 and parameters: {'observation_period_num': 113, 'train_rates': 0.6682296939050802, 'learning_rate': 0.0005941510749756588, 'batch_size': 233, 'step_size': 14, 'gamma': 0.9065882543159225}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:08:26,429][0m Trial 30 finished with value: 0.048744114755436004 and parameters: {'observation_period_num': 43, 'train_rates': 0.7951614553110133, 'learning_rate': 0.0009531288178611792, 'batch_size': 205, 'step_size': 14, 'gamma': 0.9388759447797839}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:08:54,743][0m Trial 31 finished with value: 0.04679597954132727 and parameters: {'observation_period_num': 45, 'train_rates': 0.7980431748368815, 'learning_rate': 0.0009702047260855967, 'batch_size': 207, 'step_size': 14, 'gamma': 0.9331999627698129}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:09:25,180][0m Trial 32 finished with value: 0.040826353530296455 and parameters: {'observation_period_num': 43, 'train_rates': 0.8356207131285727, 'learning_rate': 0.0004940467921270632, 'batch_size': 187, 'step_size': 14, 'gamma': 0.879408379840391}. Best is trial 26 with value: 0.03382065718708275.[0m
[32m[I 2025-01-07 21:09:57,613][0m Trial 33 finished with value: 0.031413957318573285 and parameters: {'observation_period_num': 23, 'train_rates': 0.8311011550644702, 'learning_rate': 0.0004681552259895976, 'batch_size': 186, 'step_size': 13, 'gamma': 0.8775136517830913}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:10:30,797][0m Trial 34 finished with value: 0.03402704541147451 and parameters: {'observation_period_num': 21, 'train_rates': 0.9126027597011913, 'learning_rate': 0.0004523589470521807, 'batch_size': 185, 'step_size': 13, 'gamma': 0.8751554380511046}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:11:04,664][0m Trial 35 finished with value: 0.03858897249365962 and parameters: {'observation_period_num': 22, 'train_rates': 0.9267092765410401, 'learning_rate': 0.00043023675989047706, 'batch_size': 186, 'step_size': 4, 'gamma': 0.8788319668040838}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:11:36,172][0m Trial 36 finished with value: 1.900107192993164 and parameters: {'observation_period_num': 21, 'train_rates': 0.9286502585432945, 'learning_rate': 1.7050004122071792e-06, 'batch_size': 196, 'step_size': 4, 'gamma': 0.8565454030749713}. Best is trial 33 with value: 0.031413957318573285.[0m
Early stopping at epoch 99
[32m[I 2025-01-07 21:12:07,438][0m Trial 37 finished with value: 0.22178443701102815 and parameters: {'observation_period_num': 189, 'train_rates': 0.9163909333914273, 'learning_rate': 0.0004674604445483229, 'batch_size': 184, 'step_size': 1, 'gamma': 0.8871532351429957}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:12:46,734][0m Trial 38 finished with value: 0.05876318737864494 and parameters: {'observation_period_num': 20, 'train_rates': 0.9499152010539604, 'learning_rate': 0.00019310734326537615, 'batch_size': 155, 'step_size': 5, 'gamma': 0.8457929607238661}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:13:14,900][0m Trial 39 finished with value: 0.09491207238138798 and parameters: {'observation_period_num': 106, 'train_rates': 0.9030458445749442, 'learning_rate': 0.00027756390319667623, 'batch_size': 244, 'step_size': 5, 'gamma': 0.8705678629135097}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:13:43,535][0m Trial 40 finished with value: 0.08275805481455543 and parameters: {'observation_period_num': 73, 'train_rates': 0.8741946156730073, 'learning_rate': 0.00018759308284883403, 'batch_size': 216, 'step_size': 3, 'gamma': 0.8936658993350663}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:14:16,151][0m Trial 41 finished with value: 0.11251488604280345 and parameters: {'observation_period_num': 137, 'train_rates': 0.8611629597719601, 'learning_rate': 0.0004915456468305478, 'batch_size': 183, 'step_size': 13, 'gamma': 0.8772112793481073}. Best is trial 33 with value: 0.031413957318573285.[0m
[32m[I 2025-01-07 21:14:46,622][0m Trial 42 finished with value: 0.029017489110668673 and parameters: {'observation_period_num': 17, 'train_rates': 0.8443458768660566, 'learning_rate': 0.0005936039625606572, 'batch_size': 196, 'step_size': 15, 'gamma': 0.882872114503122}. Best is trial 42 with value: 0.029017489110668673.[0m
[32m[I 2025-01-07 21:15:18,616][0m Trial 43 finished with value: 0.03359268665423288 and parameters: {'observation_period_num': 15, 'train_rates': 0.8842806188231308, 'learning_rate': 0.0006567389445351964, 'batch_size': 199, 'step_size': 15, 'gamma': 0.8609205215379904}. Best is trial 42 with value: 0.029017489110668673.[0m
[32m[I 2025-01-07 21:15:50,705][0m Trial 44 finished with value: 0.04853506259047068 and parameters: {'observation_period_num': 32, 'train_rates': 0.8888179675019863, 'learning_rate': 0.0007045244106522208, 'batch_size': 199, 'step_size': 15, 'gamma': 0.8646625363010084}. Best is trial 42 with value: 0.029017489110668673.[0m
[32m[I 2025-01-07 21:16:19,336][0m Trial 45 finished with value: 0.027267902015412154 and parameters: {'observation_period_num': 13, 'train_rates': 0.851670642600215, 'learning_rate': 0.0006817369799399096, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8293367507330445}. Best is trial 45 with value: 0.027267902015412154.[0m
[32m[I 2025-01-07 21:16:48,472][0m Trial 46 finished with value: 0.0318817866388627 and parameters: {'observation_period_num': 5, 'train_rates': 0.8446972635113495, 'learning_rate': 0.0006766594725892489, 'batch_size': 225, 'step_size': 15, 'gamma': 0.8203504619863694}. Best is trial 45 with value: 0.027267902015412154.[0m
[32m[I 2025-01-07 21:17:15,755][0m Trial 47 finished with value: 0.029562438974316877 and parameters: {'observation_period_num': 12, 'train_rates': 0.8536234626658545, 'learning_rate': 0.0006549856301192208, 'batch_size': 242, 'step_size': 15, 'gamma': 0.8260334324810416}. Best is trial 45 with value: 0.027267902015412154.[0m
[32m[I 2025-01-07 21:17:44,767][0m Trial 48 finished with value: 0.04077002059184868 and parameters: {'observation_period_num': 34, 'train_rates': 0.8655224605666678, 'learning_rate': 0.00028094853354801165, 'batch_size': 238, 'step_size': 15, 'gamma': 0.8169703215277373}. Best is trial 45 with value: 0.027267902015412154.[0m
[32m[I 2025-01-07 21:18:13,168][0m Trial 49 finished with value: 0.032078893749140934 and parameters: {'observation_period_num': 5, 'train_rates': 0.8511025346242256, 'learning_rate': 0.0007549598238983265, 'batch_size': 249, 'step_size': 15, 'gamma': 0.8302687877742826}. Best is trial 45 with value: 0.027267902015412154.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-07 21:18:13,178][0m A new study created in memory with name: no-name-7f3505c1-f2cf-4a8f-95f7-fde099d5039c[0m
[32m[I 2025-01-07 21:18:50,350][0m Trial 0 finished with value: 0.17980183812438463 and parameters: {'observation_period_num': 44, 'train_rates': 0.7904384463513973, 'learning_rate': 9.873434384675843e-06, 'batch_size': 148, 'step_size': 15, 'gamma': 0.8031310482908803}. Best is trial 0 with value: 0.17980183812438463.[0m
[32m[I 2025-01-07 21:19:21,258][0m Trial 1 finished with value: 0.08355145156383514 and parameters: {'observation_period_num': 91, 'train_rates': 0.9794238455270907, 'learning_rate': 0.00035572628743295087, 'batch_size': 220, 'step_size': 4, 'gamma': 0.8473945108130092}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:19:46,628][0m Trial 2 finished with value: 0.4044190087950771 and parameters: {'observation_period_num': 93, 'train_rates': 0.6844117742607115, 'learning_rate': 3.2780082400295376e-05, 'batch_size': 239, 'step_size': 3, 'gamma': 0.8158254519482722}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:20:46,999][0m Trial 3 finished with value: 0.40251633028189343 and parameters: {'observation_period_num': 120, 'train_rates': 0.9197331580329259, 'learning_rate': 5.057753036919948e-06, 'batch_size': 95, 'step_size': 3, 'gamma': 0.9410436608150672}. Best is trial 1 with value: 0.08355145156383514.[0m
Early stopping at epoch 93
[32m[I 2025-01-07 21:21:37,568][0m Trial 4 finished with value: 1.7666798830032349 and parameters: {'observation_period_num': 82, 'train_rates': 0.82429735797724, 'learning_rate': 2.488096188023867e-06, 'batch_size': 101, 'step_size': 1, 'gamma': 0.8914113657794513}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:22:17,429][0m Trial 5 finished with value: 0.8061552129008553 and parameters: {'observation_period_num': 129, 'train_rates': 0.7898824394728153, 'learning_rate': 1.2575451279824237e-06, 'batch_size': 135, 'step_size': 15, 'gamma': 0.8329926459254379}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:22:46,829][0m Trial 6 finished with value: 0.727334520673691 and parameters: {'observation_period_num': 235, 'train_rates': 0.7120647840549471, 'learning_rate': 4.228516701297579e-06, 'batch_size': 166, 'step_size': 13, 'gamma': 0.7502572118556976}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:23:18,868][0m Trial 7 finished with value: 0.279227614402771 and parameters: {'observation_period_num': 13, 'train_rates': 0.9447149836846822, 'learning_rate': 1.349008931689082e-05, 'batch_size': 207, 'step_size': 2, 'gamma': 0.9568031055514187}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:23:51,609][0m Trial 8 finished with value: 0.3174073155881104 and parameters: {'observation_period_num': 165, 'train_rates': 0.8489595968409069, 'learning_rate': 4.331567366908769e-05, 'batch_size': 175, 'step_size': 3, 'gamma': 0.7765132934976378}. Best is trial 1 with value: 0.08355145156383514.[0m
[32m[I 2025-01-07 21:24:53,300][0m Trial 9 finished with value: 0.06060444548499161 and parameters: {'observation_period_num': 129, 'train_rates': 0.8605643120225167, 'learning_rate': 0.00017427204098837245, 'batch_size': 87, 'step_size': 14, 'gamma': 0.817831058485289}. Best is trial 9 with value: 0.06060444548499161.[0m
[32m[I 2025-01-07 21:27:16,200][0m Trial 10 finished with value: 0.21458112151775818 and parameters: {'observation_period_num': 202, 'train_rates': 0.6378836975470404, 'learning_rate': 0.0007166897382674939, 'batch_size': 29, 'step_size': 10, 'gamma': 0.8855582814717802}. Best is trial 9 with value: 0.06060444548499161.[0m
[32m[I 2025-01-07 21:30:29,165][0m Trial 11 finished with value: 0.08911786222773971 and parameters: {'observation_period_num': 159, 'train_rates': 0.9765905098854392, 'learning_rate': 0.0002888334844276239, 'batch_size': 29, 'step_size': 7, 'gamma': 0.861628174880877}. Best is trial 9 with value: 0.06060444548499161.[0m
[32m[I 2025-01-07 21:30:56,782][0m Trial 12 finished with value: 0.07552997172801487 and parameters: {'observation_period_num': 66, 'train_rates': 0.888444634136096, 'learning_rate': 0.00014532728407017778, 'batch_size': 256, 'step_size': 7, 'gamma': 0.850275791694104}. Best is trial 9 with value: 0.06060444548499161.[0m
[32m[I 2025-01-07 21:32:10,280][0m Trial 13 finished with value: 0.04702603198447317 and parameters: {'observation_period_num': 52, 'train_rates': 0.8906786615492405, 'learning_rate': 0.00011512417319820697, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9135471845525192}. Best is trial 13 with value: 0.04702603198447317.[0m
[32m[I 2025-01-07 21:33:31,629][0m Trial 14 finished with value: 0.03580516764083824 and parameters: {'observation_period_num': 23, 'train_rates': 0.8794792760085143, 'learning_rate': 9.82404085641666e-05, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9263225828847754}. Best is trial 14 with value: 0.03580516764083824.[0m
[32m[I 2025-01-07 21:34:43,774][0m Trial 15 finished with value: 0.03222701245465794 and parameters: {'observation_period_num': 7, 'train_rates': 0.749124434041911, 'learning_rate': 7.819482106898894e-05, 'batch_size': 70, 'step_size': 10, 'gamma': 0.9276106657453577}. Best is trial 15 with value: 0.03222701245465794.[0m
[32m[I 2025-01-07 21:36:19,906][0m Trial 16 finished with value: 0.029869958562027954 and parameters: {'observation_period_num': 8, 'train_rates': 0.7426145623496941, 'learning_rate': 7.462177865734465e-05, 'batch_size': 52, 'step_size': 10, 'gamma': 0.975307108897348}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:37:48,424][0m Trial 17 finished with value: 0.03802640324162897 and parameters: {'observation_period_num': 19, 'train_rates': 0.7296588442108028, 'learning_rate': 5.858197209765842e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.9890667396980269}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:38:33,367][0m Trial 18 finished with value: 0.06041647978127003 and parameters: {'observation_period_num': 5, 'train_rates': 0.742265907249519, 'learning_rate': 1.6020979438499698e-05, 'batch_size': 116, 'step_size': 12, 'gamma': 0.983464503682153}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:39:53,721][0m Trial 19 finished with value: 0.07097174244106941 and parameters: {'observation_period_num': 36, 'train_rates': 0.6366172289405082, 'learning_rate': 0.0009458567624653308, 'batch_size': 56, 'step_size': 9, 'gamma': 0.9616126234487264}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:41:40,478][0m Trial 20 finished with value: 0.08143269224494953 and parameters: {'observation_period_num': 66, 'train_rates': 0.7597071574095268, 'learning_rate': 2.1976579868051417e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.9061162791046524}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:42:52,189][0m Trial 21 finished with value: 0.0798417094231495 and parameters: {'observation_period_num': 28, 'train_rates': 0.6860752262818272, 'learning_rate': 8.663130161329709e-05, 'batch_size': 66, 'step_size': 10, 'gamma': 0.9295725597228841}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:43:41,645][0m Trial 22 finished with value: 0.03581807879309585 and parameters: {'observation_period_num': 5, 'train_rates': 0.8144320896779195, 'learning_rate': 6.051403816783668e-05, 'batch_size': 116, 'step_size': 12, 'gamma': 0.9606769655769049}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:47:04,036][0m Trial 23 finished with value: 0.050029811312617356 and parameters: {'observation_period_num': 34, 'train_rates': 0.7527906592373492, 'learning_rate': 0.00022977593497858518, 'batch_size': 24, 'step_size': 9, 'gamma': 0.9299973080953862}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:48:06,796][0m Trial 24 finished with value: 0.08010482561329137 and parameters: {'observation_period_num': 59, 'train_rates': 0.6900822328709053, 'learning_rate': 0.00046840727430300496, 'batch_size': 76, 'step_size': 11, 'gamma': 0.9446633773290444}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:49:52,183][0m Trial 25 finished with value: 0.03672502562403679 and parameters: {'observation_period_num': 26, 'train_rates': 0.7746735434909379, 'learning_rate': 9.347768927336946e-05, 'batch_size': 48, 'step_size': 5, 'gamma': 0.9089050147198388}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:50:43,979][0m Trial 26 finished with value: 0.0691199077408451 and parameters: {'observation_period_num': 52, 'train_rates': 0.838527666263704, 'learning_rate': 2.8171626794162066e-05, 'batch_size': 108, 'step_size': 9, 'gamma': 0.8775721058596743}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:54:43,619][0m Trial 27 finished with value: 0.1812874078584154 and parameters: {'observation_period_num': 104, 'train_rates': 0.6074136644561753, 'learning_rate': 5.807913228320893e-05, 'batch_size': 17, 'step_size': 11, 'gamma': 0.9738400123241143}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:56:05,134][0m Trial 28 finished with value: 0.030617069453001022 and parameters: {'observation_period_num': 5, 'train_rates': 0.8813001939270604, 'learning_rate': 0.00018283854725060147, 'batch_size': 70, 'step_size': 13, 'gamma': 0.9211468829093146}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:56:46,629][0m Trial 29 finished with value: 0.043553345627029526 and parameters: {'observation_period_num': 46, 'train_rates': 0.8029501322980311, 'learning_rate': 0.0004508776969904512, 'batch_size': 132, 'step_size': 13, 'gamma': 0.8983200713525626}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 21:58:43,846][0m Trial 30 finished with value: 0.08563896810801902 and parameters: {'observation_period_num': 78, 'train_rates': 0.7190910229829974, 'learning_rate': 0.00020643297491862192, 'batch_size': 40, 'step_size': 14, 'gamma': 0.943761312031242}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:00:10,895][0m Trial 31 finished with value: 0.02992007241934348 and parameters: {'observation_period_num': 18, 'train_rates': 0.8930559687420205, 'learning_rate': 0.00010848969789065276, 'batch_size': 67, 'step_size': 10, 'gamma': 0.922035793658329}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:01:28,861][0m Trial 32 finished with value: 0.031888284477867 and parameters: {'observation_period_num': 6, 'train_rates': 0.9195467313396326, 'learning_rate': 0.00012989773084120165, 'batch_size': 88, 'step_size': 8, 'gamma': 0.9173860934354889}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:02:49,502][0m Trial 33 finished with value: 0.04181398482633238 and parameters: {'observation_period_num': 39, 'train_rates': 0.9185667227716215, 'learning_rate': 0.00014667445692496736, 'batch_size': 88, 'step_size': 8, 'gamma': 0.9158594555844467}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:05:27,087][0m Trial 34 finished with value: 0.0387715075617831 and parameters: {'observation_period_num': 19, 'train_rates': 0.9385294093106464, 'learning_rate': 0.0003200198544577089, 'batch_size': 40, 'step_size': 7, 'gamma': 0.869890643235371}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:06:30,924][0m Trial 35 finished with value: 0.05401046574115753 and parameters: {'observation_period_num': 42, 'train_rates': 0.9880236707800859, 'learning_rate': 4.332882035825946e-05, 'batch_size': 101, 'step_size': 8, 'gamma': 0.9722867730367878}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:07:44,199][0m Trial 36 finished with value: 0.04850522082831178 and parameters: {'observation_period_num': 25, 'train_rates': 0.9620488267111783, 'learning_rate': 0.00013613813414784393, 'batch_size': 82, 'step_size': 12, 'gamma': 0.9503081265232386}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:09:23,228][0m Trial 37 finished with value: 0.07198691738741046 and parameters: {'observation_period_num': 82, 'train_rates': 0.9316839413206366, 'learning_rate': 0.00022377654475352166, 'batch_size': 57, 'step_size': 15, 'gamma': 0.8967557634459222}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:10:01,259][0m Trial 38 finished with value: 0.34665502686889804 and parameters: {'observation_period_num': 246, 'train_rates': 0.909797215771204, 'learning_rate': 7.864663572159609e-06, 'batch_size': 149, 'step_size': 5, 'gamma': 0.9373236945491018}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:10:52,160][0m Trial 39 finished with value: 0.07867024065856647 and parameters: {'observation_period_num': 106, 'train_rates': 0.8600494450869485, 'learning_rate': 3.795585123588746e-05, 'batch_size': 111, 'step_size': 13, 'gamma': 0.9720456511903273}. Best is trial 16 with value: 0.029869958562027954.[0m
[32m[I 2025-01-07 22:11:53,309][0m Trial 40 finished with value: 0.0270708389328541 and parameters: {'observation_period_num': 16, 'train_rates': 0.8993884190522747, 'learning_rate': 0.0004811866209471049, 'batch_size': 95, 'step_size': 9, 'gamma': 0.879334387655515}. Best is trial 40 with value: 0.0270708389328541.[0m
[32m[I 2025-01-07 22:12:55,348][0m Trial 41 finished with value: 0.03781926453423997 and parameters: {'observation_period_num': 18, 'train_rates': 0.9021281299440858, 'learning_rate': 0.0006785907159188929, 'batch_size': 97, 'step_size': 9, 'gamma': 0.8545612167019393}. Best is trial 40 with value: 0.0270708389328541.[0m
[32m[I 2025-01-07 22:14:11,377][0m Trial 42 finished with value: 0.025266903017283134 and parameters: {'observation_period_num': 16, 'train_rates': 0.8749265381820085, 'learning_rate': 0.00041040585728405594, 'batch_size': 89, 'step_size': 6, 'gamma': 0.8713587604854824}. Best is trial 42 with value: 0.025266903017283134.[0m
[32m[I 2025-01-07 22:15:08,288][0m Trial 43 finished with value: 0.03190032917077356 and parameters: {'observation_period_num': 32, 'train_rates': 0.8723078841986152, 'learning_rate': 0.0004981681103030085, 'batch_size': 126, 'step_size': 6, 'gamma': 0.8325794939501482}. Best is trial 42 with value: 0.025266903017283134.[0m
[32m[I 2025-01-07 22:16:34,818][0m Trial 44 finished with value: 0.07627039560391201 and parameters: {'observation_period_num': 157, 'train_rates': 0.8278874913093661, 'learning_rate': 0.0002754733397631596, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8826221752313783}. Best is trial 42 with value: 0.025266903017283134.[0m
[32m[I 2025-01-07 22:17:19,472][0m Trial 45 finished with value: 0.03362225741147995 and parameters: {'observation_period_num': 15, 'train_rates': 0.9579315431697176, 'learning_rate': 0.0006232703109656434, 'batch_size': 146, 'step_size': 6, 'gamma': 0.8676169141932248}. Best is trial 42 with value: 0.025266903017283134.[0m
[32m[I 2025-01-07 22:17:53,679][0m Trial 46 finished with value: 0.06638797013547973 and parameters: {'observation_period_num': 68, 'train_rates': 0.8567864975578, 'learning_rate': 0.0009595661195172533, 'batch_size': 213, 'step_size': 11, 'gamma': 0.8367070722280303}. Best is trial 42 with value: 0.025266903017283134.[0m
[32m[I 2025-01-07 22:18:27,519][0m Trial 47 finished with value: 0.1109698535221967 and parameters: {'observation_period_num': 146, 'train_rates': 0.7763879945788614, 'learning_rate': 0.0004227848941590006, 'batch_size': 193, 'step_size': 14, 'gamma': 0.8097241214987546}. Best is trial 42 with value: 0.025266903017283134.[0m
Early stopping at epoch 45
[32m[I 2025-01-07 22:18:55,658][0m Trial 48 finished with value: 2.0450621001370304 and parameters: {'observation_period_num': 212, 'train_rates': 0.8959005513849702, 'learning_rate': 1.0506661590586084e-06, 'batch_size': 95, 'step_size': 1, 'gamma': 0.7882339701034791}. Best is trial 42 with value: 0.025266903017283134.[0m
[32m[I 2025-01-07 22:21:33,865][0m Trial 49 finished with value: 0.09390180453637748 and parameters: {'observation_period_num': 50, 'train_rates': 0.8739674433359339, 'learning_rate': 0.00032269834640022524, 'batch_size': 35, 'step_size': 7, 'gamma': 0.8906892214269329}. Best is trial 42 with value: 0.025266903017283134.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-07 22:21:33,876][0m A new study created in memory with name: no-name-58290513-4663-4a0a-a519-259e1a42a01a[0m
[32m[I 2025-01-07 22:22:24,944][0m Trial 0 finished with value: 0.08153949911153413 and parameters: {'observation_period_num': 85, 'train_rates': 0.7427671908139142, 'learning_rate': 0.00017585227250818686, 'batch_size': 150, 'step_size': 14, 'gamma': 0.9695398660457696}. Best is trial 0 with value: 0.08153949911153413.[0m
[32m[I 2025-01-07 22:22:55,889][0m Trial 1 finished with value: 0.4539925882201286 and parameters: {'observation_period_num': 167, 'train_rates': 0.774917837411065, 'learning_rate': 6.076510480334778e-06, 'batch_size': 199, 'step_size': 14, 'gamma': 0.9503002196907513}. Best is trial 0 with value: 0.08153949911153413.[0m
[32m[I 2025-01-07 22:24:57,043][0m Trial 2 finished with value: 0.15194956385394903 and parameters: {'observation_period_num': 66, 'train_rates': 0.6434296681546364, 'learning_rate': 9.596774811175859e-06, 'batch_size': 36, 'step_size': 13, 'gamma': 0.7865826549230976}. Best is trial 0 with value: 0.08153949911153413.[0m
[32m[I 2025-01-07 22:26:32,288][0m Trial 3 finished with value: 0.18800633833851926 and parameters: {'observation_period_num': 224, 'train_rates': 0.8582575959780852, 'learning_rate': 0.0006292716680588403, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9597719731863412}. Best is trial 0 with value: 0.08153949911153413.[0m
[32m[I 2025-01-07 22:27:22,228][0m Trial 4 finished with value: 0.5165275512588223 and parameters: {'observation_period_num': 187, 'train_rates': 0.7618265067981458, 'learning_rate': 4.2286677543793975e-06, 'batch_size': 109, 'step_size': 3, 'gamma': 0.987568109602089}. Best is trial 0 with value: 0.08153949911153413.[0m
[32m[I 2025-01-07 22:28:18,736][0m Trial 5 finished with value: 0.056651053227739785 and parameters: {'observation_period_num': 47, 'train_rates': 0.7391764846419213, 'learning_rate': 0.0004139895295599677, 'batch_size': 114, 'step_size': 6, 'gamma': 0.8373259281525138}. Best is trial 5 with value: 0.056651053227739785.[0m
[32m[I 2025-01-07 22:29:20,410][0m Trial 6 finished with value: 0.7329013240273281 and parameters: {'observation_period_num': 74, 'train_rates': 0.8709529270470553, 'learning_rate': 3.2216291174300524e-06, 'batch_size': 113, 'step_size': 3, 'gamma': 0.8861476771536713}. Best is trial 5 with value: 0.056651053227739785.[0m
[32m[I 2025-01-07 22:30:02,647][0m Trial 7 finished with value: 0.04825947433710098 and parameters: {'observation_period_num': 47, 'train_rates': 0.7974859692228871, 'learning_rate': 0.0005504238030079874, 'batch_size': 231, 'step_size': 8, 'gamma': 0.8172660590961831}. Best is trial 7 with value: 0.04825947433710098.[0m
[32m[I 2025-01-07 22:30:52,024][0m Trial 8 finished with value: 0.14544552973201197 and parameters: {'observation_period_num': 106, 'train_rates': 0.7605730227903832, 'learning_rate': 0.00022337714538654183, 'batch_size': 209, 'step_size': 8, 'gamma': 0.9816803706919559}. Best is trial 7 with value: 0.04825947433710098.[0m
[32m[I 2025-01-07 22:33:49,801][0m Trial 9 finished with value: 0.04604705813277549 and parameters: {'observation_period_num': 28, 'train_rates': 0.8592615628052025, 'learning_rate': 0.0008141269751344813, 'batch_size': 31, 'step_size': 6, 'gamma': 0.822242284771681}. Best is trial 9 with value: 0.04604705813277549.[0m
[32m[I 2025-01-07 22:35:30,548][0m Trial 10 finished with value: 0.04291258752346039 and parameters: {'observation_period_num': 17, 'train_rates': 0.9812526646485423, 'learning_rate': 4.907494827218389e-05, 'batch_size': 67, 'step_size': 11, 'gamma': 0.7520075097210679}. Best is trial 10 with value: 0.04291258752346039.[0m
[32m[I 2025-01-07 22:40:28,061][0m Trial 11 finished with value: 0.029565282541313277 and parameters: {'observation_period_num': 10, 'train_rates': 0.9775012188230972, 'learning_rate': 4.628140585217005e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.7533763917124181}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:41:57,258][0m Trial 12 finished with value: 0.05064588785171509 and parameters: {'observation_period_num': 6, 'train_rates': 0.9828824568989989, 'learning_rate': 3.6240476651141813e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.7527894896134716}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:43:21,320][0m Trial 13 finished with value: 0.05368518829345703 and parameters: {'observation_period_num': 5, 'train_rates': 0.977499786450303, 'learning_rate': 4.805074991955843e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.7528782256890011}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:47:43,707][0m Trial 14 finished with value: 0.07463394947836746 and parameters: {'observation_period_num': 134, 'train_rates': 0.9297522263201939, 'learning_rate': 8.644029387925237e-05, 'batch_size': 22, 'step_size': 11, 'gamma': 0.8880443970991745}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:48:38,193][0m Trial 15 finished with value: 1.0932098092319809 and parameters: {'observation_period_num': 131, 'train_rates': 0.9243119461821541, 'learning_rate': 1.0654981400451412e-06, 'batch_size': 154, 'step_size': 10, 'gamma': 0.7846915472585311}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:50:08,261][0m Trial 16 finished with value: 0.12652889935121145 and parameters: {'observation_period_num': 36, 'train_rates': 0.9249472653239368, 'learning_rate': 1.7217571728160255e-05, 'batch_size': 84, 'step_size': 9, 'gamma': 0.7874053035917844}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:54:52,144][0m Trial 17 finished with value: 0.12495759707504549 and parameters: {'observation_period_num': 93, 'train_rates': 0.6165114369648723, 'learning_rate': 7.285780106208533e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9181374196832435}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:56:59,746][0m Trial 18 finished with value: 0.06499804886242816 and parameters: {'observation_period_num': 22, 'train_rates': 0.9555544741936001, 'learning_rate': 1.8669178626924108e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8503704727638759}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 22:58:07,892][0m Trial 19 finished with value: 0.11639091441082576 and parameters: {'observation_period_num': 247, 'train_rates': 0.8836118759625062, 'learning_rate': 0.00014486072169143913, 'batch_size': 89, 'step_size': 9, 'gamma': 0.7726436466319148}. Best is trial 11 with value: 0.029565282541313277.[0m
Early stopping at epoch 65
[32m[I 2025-01-07 22:59:22,580][0m Trial 20 finished with value: 0.24861862119069822 and parameters: {'observation_period_num': 55, 'train_rates': 0.8228237923336077, 'learning_rate': 2.5564862902353847e-05, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8037572130556433}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 23:02:01,269][0m Trial 21 finished with value: 0.041294651357143115 and parameters: {'observation_period_num': 30, 'train_rates': 0.9087344297273455, 'learning_rate': 0.0008851038493855724, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8137240215294321}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 23:03:47,843][0m Trial 22 finished with value: 0.04459000982601067 and parameters: {'observation_period_num': 24, 'train_rates': 0.9508088914081174, 'learning_rate': 0.0003130438946858252, 'batch_size': 60, 'step_size': 7, 'gamma': 0.764719404880793}. Best is trial 11 with value: 0.029565282541313277.[0m
[32m[I 2025-01-07 23:06:48,860][0m Trial 23 finished with value: 0.02743147046783486 and parameters: {'observation_period_num': 6, 'train_rates': 0.8995714328793115, 'learning_rate': 8.616515369116292e-05, 'batch_size': 32, 'step_size': 10, 'gamma': 0.8051262312182261}. Best is trial 23 with value: 0.02743147046783486.[0m
[32m[I 2025-01-07 23:11:29,511][0m Trial 24 finished with value: 0.0255660138121157 and parameters: {'observation_period_num': 5, 'train_rates': 0.8882732833366698, 'learning_rate': 9.619234404021868e-05, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8594921608746808}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:15:53,276][0m Trial 25 finished with value: 0.09973871025193735 and parameters: {'observation_period_num': 108, 'train_rates': 0.8237399606525482, 'learning_rate': 0.00010212813998249587, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8571533901809086}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:16:34,967][0m Trial 26 finished with value: 0.11950103978943878 and parameters: {'observation_period_num': 61, 'train_rates': 0.6937231350563676, 'learning_rate': 5.890686651554537e-05, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8749746910669591}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:17:44,053][0m Trial 27 finished with value: 0.030782628089274924 and parameters: {'observation_period_num': 8, 'train_rates': 0.8815221659014444, 'learning_rate': 0.00012946487991893793, 'batch_size': 91, 'step_size': 10, 'gamma': 0.9141709414838094}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:20:16,563][0m Trial 28 finished with value: 0.07164324188190298 and parameters: {'observation_period_num': 42, 'train_rates': 0.9030541587845446, 'learning_rate': 3.0540765284106263e-05, 'batch_size': 40, 'step_size': 12, 'gamma': 0.8423586228474815}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:25:41,385][0m Trial 29 finished with value: 0.16616810567884424 and parameters: {'observation_period_num': 155, 'train_rates': 0.8332370968024552, 'learning_rate': 0.00025718646406476363, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8004282637832278}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:26:35,913][0m Trial 30 finished with value: 0.07701928903245264 and parameters: {'observation_period_num': 80, 'train_rates': 0.9501901596136189, 'learning_rate': 0.00018014600230082113, 'batch_size': 139, 'step_size': 13, 'gamma': 0.830979181592122}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:27:51,707][0m Trial 31 finished with value: 0.03338887772192589 and parameters: {'observation_period_num': 5, 'train_rates': 0.8941413902117977, 'learning_rate': 0.00013141463978765005, 'batch_size': 89, 'step_size': 10, 'gamma': 0.9112808655132973}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:29:47,765][0m Trial 32 finished with value: 0.030552836282468722 and parameters: {'observation_period_num': 17, 'train_rates': 0.8710816475482283, 'learning_rate': 9.609475778757258e-05, 'batch_size': 48, 'step_size': 9, 'gamma': 0.9319077158384429}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:32:10,559][0m Trial 33 finished with value: 0.05412247014955399 and parameters: {'observation_period_num': 36, 'train_rates': 0.8497736719170496, 'learning_rate': 1.1749151241701672e-05, 'batch_size': 37, 'step_size': 9, 'gamma': 0.942301232848314}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:34:04,226][0m Trial 34 finished with value: 0.045531856568992043 and parameters: {'observation_period_num': 19, 'train_rates': 0.7927396189193145, 'learning_rate': 9.134499712281595e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.9427479263959623}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:37:29,057][0m Trial 35 finished with value: 0.05408140865620226 and parameters: {'observation_period_num': 56, 'train_rates': 0.9339855133457576, 'learning_rate': 4.136718877299866e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8712167311622152}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:39:28,130][0m Trial 36 finished with value: 0.036145671173485355 and parameters: {'observation_period_num': 19, 'train_rates': 0.8403977855133574, 'learning_rate': 6.383875381052589e-05, 'batch_size': 56, 'step_size': 8, 'gamma': 0.899359583854934}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:40:44,223][0m Trial 37 finished with value: 0.09426581258556657 and parameters: {'observation_period_num': 70, 'train_rates': 0.9069351379422369, 'learning_rate': 2.1488536341144208e-05, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9282774916731401}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:41:42,905][0m Trial 38 finished with value: 0.13540026162927216 and parameters: {'observation_period_num': 198, 'train_rates': 0.8657630976613494, 'learning_rate': 0.0003435841032561855, 'batch_size': 183, 'step_size': 4, 'gamma': 0.9615538158231435}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:44:55,956][0m Trial 39 finished with value: 0.11716860103600099 and parameters: {'observation_period_num': 47, 'train_rates': 0.710535966284379, 'learning_rate': 1.0175013285739896e-05, 'batch_size': 26, 'step_size': 6, 'gamma': 0.8587708308318622}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:45:55,627][0m Trial 40 finished with value: 0.06499992650562683 and parameters: {'observation_period_num': 93, 'train_rates': 0.8026410735518695, 'learning_rate': 0.00020529627859550893, 'batch_size': 125, 'step_size': 9, 'gamma': 0.7988481126558982}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:48:17,655][0m Trial 41 finished with value: 0.02630212166553272 and parameters: {'observation_period_num': 14, 'train_rates': 0.8820417106314876, 'learning_rate': 0.000113971648017999, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9265281808118189}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:50:45,718][0m Trial 42 finished with value: 0.06754441005555359 and parameters: {'observation_period_num': 32, 'train_rates': 0.9620752616776992, 'learning_rate': 0.00010642988263478607, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9304813227600162}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:53:53,480][0m Trial 43 finished with value: 0.03022136615133301 and parameters: {'observation_period_num': 16, 'train_rates': 0.8695530703268273, 'learning_rate': 8.031870307284807e-05, 'batch_size': 31, 'step_size': 8, 'gamma': 0.892782181104059}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:57:07,439][0m Trial 44 finished with value: 0.042636869438723024 and parameters: {'observation_period_num': 47, 'train_rates': 0.8949853988569707, 'learning_rate': 6.724207300276589e-05, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8949161277114797}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:58:48,155][0m Trial 45 finished with value: 0.040245807419220604 and parameters: {'observation_period_num': 15, 'train_rates': 0.9152778769394431, 'learning_rate': 4.712127028169461e-05, 'batch_size': 68, 'step_size': 11, 'gamma': 0.8809588638782356}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-07 23:59:35,845][0m Trial 46 finished with value: 0.08023867011070251 and parameters: {'observation_period_num': 5, 'train_rates': 0.937405390920053, 'learning_rate': 3.096106203708078e-05, 'batch_size': 239, 'step_size': 8, 'gamma': 0.9035388082936724}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-08 00:01:06,454][0m Trial 47 finished with value: 0.03692276593507596 and parameters: {'observation_period_num': 35, 'train_rates': 0.8541820723019213, 'learning_rate': 0.00016619163720683612, 'batch_size': 61, 'step_size': 12, 'gamma': 0.8632757604487018}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-08 00:02:28,677][0m Trial 48 finished with value: 0.07625370887444191 and parameters: {'observation_period_num': 26, 'train_rates': 0.9671608077061118, 'learning_rate': 5.812838373202082e-05, 'batch_size': 75, 'step_size': 5, 'gamma': 0.7721260588246516}. Best is trial 24 with value: 0.0255660138121157.[0m
[32m[I 2025-01-08 00:06:02,793][0m Trial 49 finished with value: 0.03553540676727261 and parameters: {'observation_period_num': 14, 'train_rates': 0.8795145101613979, 'learning_rate': 0.0005390812741777835, 'batch_size': 26, 'step_size': 13, 'gamma': 0.8342061726643522}. Best is trial 24 with value: 0.0255660138121157.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 13, 'train_rates': 0.7738617552635233, 'learning_rate': 0.0008575358526368714, 'batch_size': 126, 'step_size': 8, 'gamma': 0.9156225054834012}
Epoch 1/300, trend Loss: 0.3841 | 0.2155
Epoch 2/300, trend Loss: 0.1609 | 0.2095
Epoch 3/300, trend Loss: 0.1364 | 0.2420
Epoch 4/300, trend Loss: 0.1451 | 0.1360
Epoch 5/300, trend Loss: 0.1509 | 0.1230
Epoch 6/300, trend Loss: 0.1182 | 0.0835
Epoch 7/300, trend Loss: 0.1387 | 0.0819
Epoch 8/300, trend Loss: 0.1174 | 0.1014
Epoch 9/300, trend Loss: 0.1093 | 0.0714
Epoch 10/300, trend Loss: 0.1511 | 0.1879
Epoch 11/300, trend Loss: 0.1269 | 0.1042
Epoch 12/300, trend Loss: 0.1157 | 0.0645
Epoch 13/300, trend Loss: 0.1069 | 0.0835
Epoch 14/300, trend Loss: 0.1014 | 0.0773
Epoch 15/300, trend Loss: 0.0975 | 0.0881
Epoch 16/300, trend Loss: 0.0949 | 0.0815
Epoch 17/300, trend Loss: 0.0947 | 0.1156
Epoch 18/300, trend Loss: 0.1005 | 0.0507
Epoch 19/300, trend Loss: 0.1125 | 0.0653
Epoch 20/300, trend Loss: 0.0994 | 0.0459
Epoch 21/300, trend Loss: 0.0894 | 0.0476
Epoch 22/300, trend Loss: 0.0987 | 0.0456
Epoch 23/300, trend Loss: 0.0946 | 0.0453
Epoch 24/300, trend Loss: 0.1024 | 0.0699
Epoch 25/300, trend Loss: 0.1198 | 0.0524
Epoch 26/300, trend Loss: 0.1075 | 0.0539
Epoch 27/300, trend Loss: 0.1059 | 0.0486
Epoch 28/300, trend Loss: 0.0932 | 0.0907
Epoch 29/300, trend Loss: 0.0924 | 0.0518
Epoch 30/300, trend Loss: 0.0923 | 0.0540
Epoch 31/300, trend Loss: 0.0831 | 0.0385
Epoch 32/300, trend Loss: 0.0766 | 0.0380
Epoch 33/300, trend Loss: 0.0752 | 0.0422
Epoch 34/300, trend Loss: 0.0744 | 0.0404
Epoch 35/300, trend Loss: 0.0733 | 0.0386
Epoch 36/300, trend Loss: 0.0726 | 0.0413
Epoch 37/300, trend Loss: 0.0727 | 0.0362
Epoch 38/300, trend Loss: 0.0727 | 0.0374
Epoch 39/300, trend Loss: 0.0713 | 0.0407
Epoch 40/300, trend Loss: 0.0727 | 0.0337
Epoch 41/300, trend Loss: 0.0727 | 0.0498
Epoch 42/300, trend Loss: 0.0739 | 0.0357
Epoch 43/300, trend Loss: 0.0734 | 0.0503
Epoch 44/300, trend Loss: 0.0735 | 0.0370
Epoch 45/300, trend Loss: 0.0725 | 0.0438
Epoch 46/300, trend Loss: 0.0722 | 0.0370
Epoch 47/300, trend Loss: 0.0722 | 0.0349
Epoch 48/300, trend Loss: 0.0742 | 0.0358
Epoch 49/300, trend Loss: 0.0746 | 0.0418
Epoch 50/300, trend Loss: 0.0703 | 0.0327
Epoch 51/300, trend Loss: 0.0698 | 0.0344
Epoch 52/300, trend Loss: 0.0678 | 0.0361
Epoch 53/300, trend Loss: 0.0682 | 0.0319
Epoch 54/300, trend Loss: 0.0687 | 0.0359
Epoch 55/300, trend Loss: 0.0681 | 0.0314
Epoch 56/300, trend Loss: 0.0680 | 0.0316
Epoch 57/300, trend Loss: 0.0684 | 0.0325
Epoch 58/300, trend Loss: 0.0682 | 0.0312
Epoch 59/300, trend Loss: 0.0693 | 0.0336
Epoch 60/300, trend Loss: 0.0691 | 0.0344
Epoch 61/300, trend Loss: 0.0725 | 0.0380
Epoch 62/300, trend Loss: 0.0728 | 0.0334
Epoch 63/300, trend Loss: 0.0777 | 0.0389
Epoch 64/300, trend Loss: 0.0801 | 0.0367
Epoch 65/300, trend Loss: 0.0777 | 0.0332
Epoch 66/300, trend Loss: 0.0757 | 0.0422
Epoch 67/300, trend Loss: 0.0775 | 0.0355
Epoch 68/300, trend Loss: 0.0780 | 0.0343
Epoch 69/300, trend Loss: 0.0800 | 0.0338
Epoch 70/300, trend Loss: 0.0851 | 0.0380
Epoch 71/300, trend Loss: 0.0819 | 0.0391
Epoch 72/300, trend Loss: 0.0713 | 0.0358
Epoch 73/300, trend Loss: 0.0712 | 0.0368
Epoch 74/300, trend Loss: 0.0704 | 0.0348
Epoch 75/300, trend Loss: 0.0679 | 0.0350
Epoch 76/300, trend Loss: 0.0651 | 0.0292
Epoch 77/300, trend Loss: 0.0659 | 0.0295
Epoch 78/300, trend Loss: 0.0641 | 0.0354
Epoch 79/300, trend Loss: 0.0641 | 0.0292
Epoch 80/300, trend Loss: 0.0629 | 0.0294
Epoch 81/300, trend Loss: 0.0622 | 0.0297
Epoch 82/300, trend Loss: 0.0626 | 0.0278
Epoch 83/300, trend Loss: 0.0624 | 0.0290
Epoch 84/300, trend Loss: 0.0621 | 0.0275
Epoch 85/300, trend Loss: 0.0616 | 0.0283
Epoch 86/300, trend Loss: 0.0612 | 0.0282
Epoch 87/300, trend Loss: 0.0611 | 0.0272
Epoch 88/300, trend Loss: 0.0612 | 0.0278
Epoch 89/300, trend Loss: 0.0610 | 0.0269
Epoch 90/300, trend Loss: 0.0608 | 0.0265
Epoch 91/300, trend Loss: 0.0607 | 0.0268
Epoch 92/300, trend Loss: 0.0605 | 0.0265
Epoch 93/300, trend Loss: 0.0604 | 0.0263
Epoch 94/300, trend Loss: 0.0603 | 0.0263
Epoch 95/300, trend Loss: 0.0602 | 0.0262
Epoch 96/300, trend Loss: 0.0601 | 0.0261
Epoch 97/300, trend Loss: 0.0599 | 0.0260
Epoch 98/300, trend Loss: 0.0598 | 0.0259
Epoch 99/300, trend Loss: 0.0595 | 0.0258
Epoch 100/300, trend Loss: 0.0594 | 0.0258
Epoch 101/300, trend Loss: 0.0592 | 0.0258
Epoch 102/300, trend Loss: 0.0591 | 0.0256
Epoch 103/300, trend Loss: 0.0591 | 0.0255
Epoch 104/300, trend Loss: 0.0590 | 0.0254
Epoch 105/300, trend Loss: 0.0590 | 0.0253
Epoch 106/300, trend Loss: 0.0589 | 0.0252
Epoch 107/300, trend Loss: 0.0592 | 0.0253
Epoch 108/300, trend Loss: 0.0596 | 0.0256
Epoch 109/300, trend Loss: 0.0611 | 0.0253
Epoch 110/300, trend Loss: 0.0617 | 0.0274
Epoch 111/300, trend Loss: 0.0603 | 0.0260
Epoch 112/300, trend Loss: 0.0597 | 0.0269
Epoch 113/300, trend Loss: 0.0592 | 0.0261
Epoch 114/300, trend Loss: 0.0588 | 0.0253
Epoch 115/300, trend Loss: 0.0587 | 0.0250
Epoch 116/300, trend Loss: 0.0584 | 0.0248
Epoch 117/300, trend Loss: 0.0581 | 0.0243
Epoch 118/300, trend Loss: 0.0579 | 0.0241
Epoch 119/300, trend Loss: 0.0577 | 0.0240
Epoch 120/300, trend Loss: 0.0575 | 0.0236
Epoch 121/300, trend Loss: 0.0574 | 0.0235
Epoch 122/300, trend Loss: 0.0572 | 0.0234
Epoch 123/300, trend Loss: 0.0571 | 0.0232
Epoch 124/300, trend Loss: 0.0570 | 0.0232
Epoch 125/300, trend Loss: 0.0569 | 0.0231
Epoch 126/300, trend Loss: 0.0568 | 0.0231
Epoch 127/300, trend Loss: 0.0568 | 0.0230
Epoch 128/300, trend Loss: 0.0568 | 0.0230
Epoch 129/300, trend Loss: 0.0568 | 0.0230
Epoch 130/300, trend Loss: 0.0569 | 0.0231
Epoch 131/300, trend Loss: 0.0572 | 0.0233
Epoch 132/300, trend Loss: 0.0576 | 0.0236
Epoch 133/300, trend Loss: 0.0583 | 0.0233
Epoch 134/300, trend Loss: 0.0582 | 0.0239
Epoch 135/300, trend Loss: 0.0572 | 0.0231
Epoch 136/300, trend Loss: 0.0568 | 0.0236
Epoch 137/300, trend Loss: 0.0567 | 0.0234
Epoch 138/300, trend Loss: 0.0568 | 0.0229
Epoch 139/300, trend Loss: 0.0569 | 0.0230
Epoch 140/300, trend Loss: 0.0566 | 0.0227
Epoch 141/300, trend Loss: 0.0563 | 0.0227
Epoch 142/300, trend Loss: 0.0560 | 0.0226
Epoch 143/300, trend Loss: 0.0560 | 0.0226
Epoch 144/300, trend Loss: 0.0560 | 0.0225
Epoch 145/300, trend Loss: 0.0560 | 0.0225
Epoch 146/300, trend Loss: 0.0559 | 0.0225
Epoch 147/300, trend Loss: 0.0558 | 0.0225
Epoch 148/300, trend Loss: 0.0558 | 0.0225
Epoch 149/300, trend Loss: 0.0560 | 0.0225
Epoch 150/300, trend Loss: 0.0560 | 0.0224
Epoch 151/300, trend Loss: 0.0558 | 0.0224
Epoch 152/300, trend Loss: 0.0556 | 0.0224
Epoch 153/300, trend Loss: 0.0555 | 0.0223
Epoch 154/300, trend Loss: 0.0556 | 0.0223
Epoch 155/300, trend Loss: 0.0555 | 0.0223
Epoch 156/300, trend Loss: 0.0555 | 0.0223
Epoch 157/300, trend Loss: 0.0554 | 0.0223
Epoch 158/300, trend Loss: 0.0554 | 0.0223
Epoch 159/300, trend Loss: 0.0554 | 0.0223
Epoch 160/300, trend Loss: 0.0554 | 0.0223
Epoch 161/300, trend Loss: 0.0553 | 0.0222
Epoch 162/300, trend Loss: 0.0552 | 0.0222
Epoch 163/300, trend Loss: 0.0552 | 0.0222
Epoch 164/300, trend Loss: 0.0552 | 0.0222
Epoch 165/300, trend Loss: 0.0551 | 0.0222
Epoch 166/300, trend Loss: 0.0551 | 0.0222
Epoch 167/300, trend Loss: 0.0551 | 0.0222
Epoch 168/300, trend Loss: 0.0551 | 0.0222
Epoch 169/300, trend Loss: 0.0550 | 0.0222
Epoch 170/300, trend Loss: 0.0550 | 0.0222
Epoch 171/300, trend Loss: 0.0550 | 0.0222
Epoch 172/300, trend Loss: 0.0549 | 0.0222
Epoch 173/300, trend Loss: 0.0549 | 0.0222
Epoch 174/300, trend Loss: 0.0549 | 0.0222
Epoch 175/300, trend Loss: 0.0549 | 0.0222
Epoch 176/300, trend Loss: 0.0549 | 0.0222
Epoch 177/300, trend Loss: 0.0548 | 0.0221
Epoch 178/300, trend Loss: 0.0548 | 0.0222
Epoch 179/300, trend Loss: 0.0548 | 0.0221
Epoch 180/300, trend Loss: 0.0548 | 0.0222
Epoch 181/300, trend Loss: 0.0547 | 0.0221
Epoch 182/300, trend Loss: 0.0547 | 0.0221
Epoch 183/300, trend Loss: 0.0547 | 0.0221
Epoch 184/300, trend Loss: 0.0547 | 0.0221
Epoch 185/300, trend Loss: 0.0547 | 0.0221
Epoch 186/300, trend Loss: 0.0546 | 0.0221
Epoch 187/300, trend Loss: 0.0546 | 0.0221
Epoch 188/300, trend Loss: 0.0546 | 0.0221
Epoch 189/300, trend Loss: 0.0546 | 0.0221
Epoch 190/300, trend Loss: 0.0546 | 0.0221
Epoch 191/300, trend Loss: 0.0546 | 0.0221
Epoch 192/300, trend Loss: 0.0545 | 0.0221
Epoch 193/300, trend Loss: 0.0545 | 0.0221
Epoch 194/300, trend Loss: 0.0545 | 0.0221
Epoch 195/300, trend Loss: 0.0545 | 0.0221
Epoch 196/300, trend Loss: 0.0545 | 0.0221
Epoch 197/300, trend Loss: 0.0545 | 0.0221
Epoch 198/300, trend Loss: 0.0544 | 0.0221
Epoch 199/300, trend Loss: 0.0544 | 0.0221
Epoch 200/300, trend Loss: 0.0544 | 0.0221
Epoch 201/300, trend Loss: 0.0544 | 0.0221
Epoch 202/300, trend Loss: 0.0544 | 0.0221
Epoch 203/300, trend Loss: 0.0544 | 0.0221
Epoch 204/300, trend Loss: 0.0544 | 0.0221
Epoch 205/300, trend Loss: 0.0543 | 0.0221
Epoch 206/300, trend Loss: 0.0543 | 0.0221
Epoch 207/300, trend Loss: 0.0543 | 0.0221
Epoch 208/300, trend Loss: 0.0543 | 0.0221
Epoch 209/300, trend Loss: 0.0543 | 0.0221
Epoch 210/300, trend Loss: 0.0543 | 0.0221
Epoch 211/300, trend Loss: 0.0543 | 0.0221
Epoch 212/300, trend Loss: 0.0543 | 0.0221
Epoch 213/300, trend Loss: 0.0542 | 0.0221
Epoch 214/300, trend Loss: 0.0542 | 0.0221
Epoch 215/300, trend Loss: 0.0542 | 0.0221
Epoch 216/300, trend Loss: 0.0542 | 0.0221
Epoch 217/300, trend Loss: 0.0542 | 0.0221
Epoch 218/300, trend Loss: 0.0542 | 0.0221
Epoch 219/300, trend Loss: 0.0542 | 0.0221
Epoch 220/300, trend Loss: 0.0542 | 0.0221
Epoch 221/300, trend Loss: 0.0542 | 0.0221
Epoch 222/300, trend Loss: 0.0541 | 0.0221
Epoch 223/300, trend Loss: 0.0541 | 0.0221
Epoch 224/300, trend Loss: 0.0541 | 0.0221
Epoch 225/300, trend Loss: 0.0541 | 0.0221
Epoch 226/300, trend Loss: 0.0541 | 0.0221
Epoch 227/300, trend Loss: 0.0541 | 0.0221
Epoch 228/300, trend Loss: 0.0541 | 0.0221
Epoch 229/300, trend Loss: 0.0541 | 0.0221
Epoch 230/300, trend Loss: 0.0541 | 0.0221
Epoch 231/300, trend Loss: 0.0541 | 0.0221
Epoch 232/300, trend Loss: 0.0541 | 0.0221
Epoch 233/300, trend Loss: 0.0540 | 0.0221
Epoch 234/300, trend Loss: 0.0540 | 0.0221
Epoch 235/300, trend Loss: 0.0540 | 0.0221
Epoch 236/300, trend Loss: 0.0540 | 0.0221
Epoch 237/300, trend Loss: 0.0540 | 0.0221
Epoch 238/300, trend Loss: 0.0540 | 0.0221
Epoch 239/300, trend Loss: 0.0540 | 0.0221
Epoch 240/300, trend Loss: 0.0540 | 0.0222
Epoch 241/300, trend Loss: 0.0540 | 0.0222
Epoch 242/300, trend Loss: 0.0540 | 0.0222
Epoch 243/300, trend Loss: 0.0540 | 0.0222
Epoch 244/300, trend Loss: 0.0540 | 0.0222
Epoch 245/300, trend Loss: 0.0540 | 0.0222
Epoch 246/300, trend Loss: 0.0540 | 0.0222
Epoch 247/300, trend Loss: 0.0539 | 0.0222
Epoch 248/300, trend Loss: 0.0539 | 0.0222
Epoch 249/300, trend Loss: 0.0539 | 0.0222
Epoch 250/300, trend Loss: 0.0539 | 0.0222
Epoch 251/300, trend Loss: 0.0539 | 0.0222
Epoch 252/300, trend Loss: 0.0539 | 0.0222
Epoch 253/300, trend Loss: 0.0539 | 0.0222
Epoch 254/300, trend Loss: 0.0539 | 0.0222
Epoch 255/300, trend Loss: 0.0539 | 0.0222
Epoch 256/300, trend Loss: 0.0539 | 0.0222
Epoch 257/300, trend Loss: 0.0539 | 0.0222
Epoch 258/300, trend Loss: 0.0539 | 0.0222
Epoch 259/300, trend Loss: 0.0539 | 0.0222
Epoch 260/300, trend Loss: 0.0539 | 0.0222
Epoch 261/300, trend Loss: 0.0539 | 0.0222
Epoch 262/300, trend Loss: 0.0539 | 0.0222
Epoch 263/300, trend Loss: 0.0539 | 0.0222
Epoch 264/300, trend Loss: 0.0539 | 0.0222
Epoch 265/300, trend Loss: 0.0539 | 0.0222
Epoch 266/300, trend Loss: 0.0538 | 0.0222
Epoch 267/300, trend Loss: 0.0538 | 0.0222
Epoch 268/300, trend Loss: 0.0538 | 0.0222
Epoch 269/300, trend Loss: 0.0538 | 0.0222
Epoch 270/300, trend Loss: 0.0538 | 0.0222
Epoch 271/300, trend Loss: 0.0538 | 0.0222
Epoch 272/300, trend Loss: 0.0538 | 0.0222
Epoch 273/300, trend Loss: 0.0538 | 0.0222
Epoch 274/300, trend Loss: 0.0538 | 0.0222
Epoch 275/300, trend Loss: 0.0538 | 0.0222
Epoch 276/300, trend Loss: 0.0538 | 0.0222
Epoch 277/300, trend Loss: 0.0538 | 0.0222
Epoch 278/300, trend Loss: 0.0538 | 0.0222
Epoch 279/300, trend Loss: 0.0538 | 0.0222
Epoch 280/300, trend Loss: 0.0538 | 0.0222
Epoch 281/300, trend Loss: 0.0538 | 0.0222
Epoch 282/300, trend Loss: 0.0538 | 0.0222
Epoch 283/300, trend Loss: 0.0538 | 0.0222
Epoch 284/300, trend Loss: 0.0538 | 0.0222
Epoch 285/300, trend Loss: 0.0538 | 0.0222
Epoch 286/300, trend Loss: 0.0538 | 0.0222
Epoch 287/300, trend Loss: 0.0538 | 0.0222
Epoch 288/300, trend Loss: 0.0538 | 0.0222
Epoch 289/300, trend Loss: 0.0538 | 0.0222
Epoch 290/300, trend Loss: 0.0538 | 0.0222
Epoch 291/300, trend Loss: 0.0538 | 0.0222
Epoch 292/300, trend Loss: 0.0538 | 0.0222
Epoch 293/300, trend Loss: 0.0538 | 0.0222
Epoch 294/300, trend Loss: 0.0537 | 0.0222
Epoch 295/300, trend Loss: 0.0537 | 0.0222
Epoch 296/300, trend Loss: 0.0537 | 0.0222
Epoch 297/300, trend Loss: 0.0537 | 0.0222
Epoch 298/300, trend Loss: 0.0537 | 0.0222
Epoch 299/300, trend Loss: 0.0537 | 0.0222
Epoch 300/300, trend Loss: 0.0537 | 0.0222
Training seasonal_0 component with params: {'observation_period_num': 7, 'train_rates': 0.8241667500653823, 'learning_rate': 0.0005175435964423788, 'batch_size': 165, 'step_size': 1, 'gamma': 0.9622637928714274}
Epoch 1/300, seasonal_0 Loss: 0.4892 | 0.2207
Epoch 2/300, seasonal_0 Loss: 0.1848 | 0.1621
Epoch 3/300, seasonal_0 Loss: 0.1559 | 0.1481
Epoch 4/300, seasonal_0 Loss: 0.1512 | 0.1786
Epoch 5/300, seasonal_0 Loss: 0.1445 | 0.1099
Epoch 6/300, seasonal_0 Loss: 0.1266 | 0.1017
Epoch 7/300, seasonal_0 Loss: 0.1254 | 0.0998
Epoch 8/300, seasonal_0 Loss: 0.1204 | 0.0925
Epoch 9/300, seasonal_0 Loss: 0.1176 | 0.0884
Epoch 10/300, seasonal_0 Loss: 0.1154 | 0.0877
Epoch 11/300, seasonal_0 Loss: 0.1139 | 0.0882
Epoch 12/300, seasonal_0 Loss: 0.1116 | 0.0836
Epoch 13/300, seasonal_0 Loss: 0.1104 | 0.0844
Epoch 14/300, seasonal_0 Loss: 0.1114 | 0.0841
Epoch 15/300, seasonal_0 Loss: 0.1134 | 0.0844
Epoch 16/300, seasonal_0 Loss: 0.1157 | 0.0849
Epoch 17/300, seasonal_0 Loss: 0.1182 | 0.0850
Epoch 18/300, seasonal_0 Loss: 0.1147 | 0.0820
Epoch 19/300, seasonal_0 Loss: 0.1081 | 0.0824
Epoch 20/300, seasonal_0 Loss: 0.1147 | 0.0812
Epoch 21/300, seasonal_0 Loss: 0.1136 | 0.0800
Epoch 22/300, seasonal_0 Loss: 0.1043 | 0.0795
Epoch 23/300, seasonal_0 Loss: 0.1041 | 0.0795
Epoch 24/300, seasonal_0 Loss: 0.1027 | 0.0787
Epoch 25/300, seasonal_0 Loss: 0.1023 | 0.0782
Epoch 26/300, seasonal_0 Loss: 0.1017 | 0.0780
Epoch 27/300, seasonal_0 Loss: 0.1014 | 0.0777
Epoch 28/300, seasonal_0 Loss: 0.1011 | 0.0773
Epoch 29/300, seasonal_0 Loss: 0.1008 | 0.0769
Epoch 30/300, seasonal_0 Loss: 0.1005 | 0.0768
Epoch 31/300, seasonal_0 Loss: 0.1003 | 0.0763
Epoch 32/300, seasonal_0 Loss: 0.1001 | 0.0763
Epoch 33/300, seasonal_0 Loss: 0.0999 | 0.0758
Epoch 34/300, seasonal_0 Loss: 0.0996 | 0.0759
Epoch 35/300, seasonal_0 Loss: 0.0995 | 0.0753
Epoch 36/300, seasonal_0 Loss: 0.0993 | 0.0755
Epoch 37/300, seasonal_0 Loss: 0.0991 | 0.0749
Epoch 38/300, seasonal_0 Loss: 0.0989 | 0.0751
Epoch 39/300, seasonal_0 Loss: 0.0988 | 0.0747
Epoch 40/300, seasonal_0 Loss: 0.0986 | 0.0747
Epoch 41/300, seasonal_0 Loss: 0.0985 | 0.0744
Epoch 42/300, seasonal_0 Loss: 0.0984 | 0.0744
Epoch 43/300, seasonal_0 Loss: 0.0983 | 0.0742
Epoch 44/300, seasonal_0 Loss: 0.0981 | 0.0741
Epoch 45/300, seasonal_0 Loss: 0.0980 | 0.0740
Epoch 46/300, seasonal_0 Loss: 0.0979 | 0.0739
Epoch 47/300, seasonal_0 Loss: 0.0979 | 0.0738
Epoch 48/300, seasonal_0 Loss: 0.0978 | 0.0737
Epoch 49/300, seasonal_0 Loss: 0.0977 | 0.0736
Epoch 50/300, seasonal_0 Loss: 0.0976 | 0.0735
Epoch 51/300, seasonal_0 Loss: 0.0976 | 0.0735
Epoch 52/300, seasonal_0 Loss: 0.0975 | 0.0734
Epoch 53/300, seasonal_0 Loss: 0.0974 | 0.0733
Epoch 54/300, seasonal_0 Loss: 0.0974 | 0.0733
Epoch 55/300, seasonal_0 Loss: 0.0973 | 0.0732
Epoch 56/300, seasonal_0 Loss: 0.0973 | 0.0732
Epoch 57/300, seasonal_0 Loss: 0.0972 | 0.0731
Epoch 58/300, seasonal_0 Loss: 0.0972 | 0.0731
Epoch 59/300, seasonal_0 Loss: 0.0972 | 0.0730
Epoch 60/300, seasonal_0 Loss: 0.0971 | 0.0730
Epoch 61/300, seasonal_0 Loss: 0.0971 | 0.0729
Epoch 62/300, seasonal_0 Loss: 0.0971 | 0.0729
Epoch 63/300, seasonal_0 Loss: 0.0970 | 0.0729
Epoch 64/300, seasonal_0 Loss: 0.0970 | 0.0728
Epoch 65/300, seasonal_0 Loss: 0.0970 | 0.0728
Epoch 66/300, seasonal_0 Loss: 0.0969 | 0.0728
Epoch 67/300, seasonal_0 Loss: 0.0969 | 0.0728
Epoch 68/300, seasonal_0 Loss: 0.0969 | 0.0728
Epoch 69/300, seasonal_0 Loss: 0.0969 | 0.0727
Epoch 70/300, seasonal_0 Loss: 0.0969 | 0.0727
Epoch 71/300, seasonal_0 Loss: 0.0969 | 0.0727
Epoch 72/300, seasonal_0 Loss: 0.0968 | 0.0727
Epoch 73/300, seasonal_0 Loss: 0.0968 | 0.0727
Epoch 74/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 75/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 76/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 77/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 78/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 79/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 80/300, seasonal_0 Loss: 0.0968 | 0.0726
Epoch 81/300, seasonal_0 Loss: 0.0967 | 0.0726
Epoch 82/300, seasonal_0 Loss: 0.0967 | 0.0726
Epoch 83/300, seasonal_0 Loss: 0.0967 | 0.0726
Epoch 84/300, seasonal_0 Loss: 0.0967 | 0.0726
Epoch 85/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 86/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 87/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 88/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 89/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 90/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 91/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 92/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 93/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 94/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 95/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 96/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 97/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 98/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 99/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 100/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 101/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 102/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 103/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 104/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 105/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 106/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 107/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 108/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 109/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 110/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 111/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 112/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 113/300, seasonal_0 Loss: 0.0967 | 0.0725
Epoch 114/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 115/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 116/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 117/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 118/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 119/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 120/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 121/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 122/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 123/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 124/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 125/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 126/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 127/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 128/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 129/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 130/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 131/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 132/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 133/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 134/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 135/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 136/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 137/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 138/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 139/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 140/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 141/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 142/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 143/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 144/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 145/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 146/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 147/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 148/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 149/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 150/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 151/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 152/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 153/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 154/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 155/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 156/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 157/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 158/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 159/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 160/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 161/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 162/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 163/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 164/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 165/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 166/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 167/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 168/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 169/300, seasonal_0 Loss: 0.0966 | 0.0725
Epoch 170/300, seasonal_0 Loss: 0.0966 | 0.0725
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 12, 'train_rates': 0.7724306457794774, 'learning_rate': 0.00010528801897096439, 'batch_size': 51, 'step_size': 5, 'gamma': 0.9565286584761599}
Epoch 1/300, seasonal_1 Loss: 0.2760 | 0.2561
Epoch 2/300, seasonal_1 Loss: 0.1533 | 0.1648
Epoch 3/300, seasonal_1 Loss: 0.1401 | 0.1197
Epoch 4/300, seasonal_1 Loss: 0.1229 | 0.0984
Epoch 5/300, seasonal_1 Loss: 0.1149 | 0.0938
Epoch 6/300, seasonal_1 Loss: 0.1130 | 0.0920
Epoch 7/300, seasonal_1 Loss: 0.1150 | 0.0854
Epoch 8/300, seasonal_1 Loss: 0.1176 | 0.0786
Epoch 9/300, seasonal_1 Loss: 0.1151 | 0.0738
Epoch 10/300, seasonal_1 Loss: 0.1100 | 0.0704
Epoch 11/300, seasonal_1 Loss: 0.1067 | 0.0682
Epoch 12/300, seasonal_1 Loss: 0.1056 | 0.0672
Epoch 13/300, seasonal_1 Loss: 0.1051 | 0.0672
Epoch 14/300, seasonal_1 Loss: 0.1047 | 0.0697
Epoch 15/300, seasonal_1 Loss: 0.1037 | 0.0657
Epoch 16/300, seasonal_1 Loss: 0.1014 | 0.0621
Epoch 17/300, seasonal_1 Loss: 0.0989 | 0.0632
Epoch 18/300, seasonal_1 Loss: 0.0968 | 0.0628
Epoch 19/300, seasonal_1 Loss: 0.0953 | 0.0643
Epoch 20/300, seasonal_1 Loss: 0.0943 | 0.0655
Epoch 21/300, seasonal_1 Loss: 0.0938 | 0.0698
Epoch 22/300, seasonal_1 Loss: 0.0938 | 0.0694
Epoch 23/300, seasonal_1 Loss: 0.0928 | 0.0650
Epoch 24/300, seasonal_1 Loss: 0.0907 | 0.0616
Epoch 25/300, seasonal_1 Loss: 0.0885 | 0.0570
Epoch 26/300, seasonal_1 Loss: 0.0862 | 0.0543
Epoch 27/300, seasonal_1 Loss: 0.0846 | 0.0511
Epoch 28/300, seasonal_1 Loss: 0.0831 | 0.0486
Epoch 29/300, seasonal_1 Loss: 0.0819 | 0.0472
Epoch 30/300, seasonal_1 Loss: 0.0811 | 0.0455
Epoch 31/300, seasonal_1 Loss: 0.0803 | 0.0445
Epoch 32/300, seasonal_1 Loss: 0.0797 | 0.0433
Epoch 33/300, seasonal_1 Loss: 0.0791 | 0.0424
Epoch 34/300, seasonal_1 Loss: 0.0785 | 0.0419
Epoch 35/300, seasonal_1 Loss: 0.0781 | 0.0412
Epoch 36/300, seasonal_1 Loss: 0.0776 | 0.0408
Epoch 37/300, seasonal_1 Loss: 0.0771 | 0.0403
Epoch 38/300, seasonal_1 Loss: 0.0766 | 0.0398
Epoch 39/300, seasonal_1 Loss: 0.0762 | 0.0394
Epoch 40/300, seasonal_1 Loss: 0.0758 | 0.0390
Epoch 41/300, seasonal_1 Loss: 0.0754 | 0.0388
Epoch 42/300, seasonal_1 Loss: 0.0750 | 0.0385
Epoch 43/300, seasonal_1 Loss: 0.0747 | 0.0382
Epoch 44/300, seasonal_1 Loss: 0.0743 | 0.0380
Epoch 45/300, seasonal_1 Loss: 0.0741 | 0.0378
Epoch 46/300, seasonal_1 Loss: 0.0738 | 0.0376
Epoch 47/300, seasonal_1 Loss: 0.0736 | 0.0374
Epoch 48/300, seasonal_1 Loss: 0.0733 | 0.0372
Epoch 49/300, seasonal_1 Loss: 0.0731 | 0.0371
Epoch 50/300, seasonal_1 Loss: 0.0729 | 0.0369
Epoch 51/300, seasonal_1 Loss: 0.0726 | 0.0369
Epoch 52/300, seasonal_1 Loss: 0.0724 | 0.0366
Epoch 53/300, seasonal_1 Loss: 0.0722 | 0.0364
Epoch 54/300, seasonal_1 Loss: 0.0719 | 0.0363
Epoch 55/300, seasonal_1 Loss: 0.0716 | 0.0361
Epoch 56/300, seasonal_1 Loss: 0.0712 | 0.0360
Epoch 57/300, seasonal_1 Loss: 0.0709 | 0.0358
Epoch 58/300, seasonal_1 Loss: 0.0706 | 0.0356
Epoch 59/300, seasonal_1 Loss: 0.0703 | 0.0356
Epoch 60/300, seasonal_1 Loss: 0.0701 | 0.0355
Epoch 61/300, seasonal_1 Loss: 0.0699 | 0.0354
Epoch 62/300, seasonal_1 Loss: 0.0697 | 0.0353
Epoch 63/300, seasonal_1 Loss: 0.0696 | 0.0356
Epoch 64/300, seasonal_1 Loss: 0.0697 | 0.0361
Epoch 65/300, seasonal_1 Loss: 0.0700 | 0.0358
Epoch 66/300, seasonal_1 Loss: 0.0700 | 0.0357
Epoch 67/300, seasonal_1 Loss: 0.0699 | 0.0353
Epoch 68/300, seasonal_1 Loss: 0.0696 | 0.0353
Epoch 69/300, seasonal_1 Loss: 0.0694 | 0.0351
Epoch 70/300, seasonal_1 Loss: 0.0691 | 0.0350
Epoch 71/300, seasonal_1 Loss: 0.0689 | 0.0349
Epoch 72/300, seasonal_1 Loss: 0.0686 | 0.0348
Epoch 73/300, seasonal_1 Loss: 0.0684 | 0.0348
Epoch 74/300, seasonal_1 Loss: 0.0682 | 0.0347
Epoch 75/300, seasonal_1 Loss: 0.0680 | 0.0347
Epoch 76/300, seasonal_1 Loss: 0.0679 | 0.0346
Epoch 77/300, seasonal_1 Loss: 0.0679 | 0.0346
Epoch 78/300, seasonal_1 Loss: 0.0679 | 0.0346
Epoch 79/300, seasonal_1 Loss: 0.0680 | 0.0350
Epoch 80/300, seasonal_1 Loss: 0.0684 | 0.0354
Epoch 81/300, seasonal_1 Loss: 0.0689 | 0.0359
Epoch 82/300, seasonal_1 Loss: 0.0693 | 0.0355
Epoch 83/300, seasonal_1 Loss: 0.0689 | 0.0339
Epoch 84/300, seasonal_1 Loss: 0.0682 | 0.0364
Epoch 85/300, seasonal_1 Loss: 0.0682 | 0.0388
Epoch 86/300, seasonal_1 Loss: 0.0681 | 0.0337
Epoch 87/300, seasonal_1 Loss: 0.0675 | 0.0378
Epoch 88/300, seasonal_1 Loss: 0.0677 | 0.0329
Epoch 89/300, seasonal_1 Loss: 0.0672 | 0.0365
Epoch 90/300, seasonal_1 Loss: 0.0669 | 0.0327
Epoch 91/300, seasonal_1 Loss: 0.0663 | 0.0346
Epoch 92/300, seasonal_1 Loss: 0.0659 | 0.0326
Epoch 93/300, seasonal_1 Loss: 0.0655 | 0.0334
Epoch 94/300, seasonal_1 Loss: 0.0653 | 0.0328
Epoch 95/300, seasonal_1 Loss: 0.0651 | 0.0330
Epoch 96/300, seasonal_1 Loss: 0.0650 | 0.0328
Epoch 97/300, seasonal_1 Loss: 0.0649 | 0.0329
Epoch 98/300, seasonal_1 Loss: 0.0648 | 0.0328
Epoch 99/300, seasonal_1 Loss: 0.0647 | 0.0328
Epoch 100/300, seasonal_1 Loss: 0.0646 | 0.0327
Epoch 101/300, seasonal_1 Loss: 0.0645 | 0.0327
Epoch 102/300, seasonal_1 Loss: 0.0644 | 0.0326
Epoch 103/300, seasonal_1 Loss: 0.0643 | 0.0326
Epoch 104/300, seasonal_1 Loss: 0.0642 | 0.0325
Epoch 105/300, seasonal_1 Loss: 0.0641 | 0.0325
Epoch 106/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 107/300, seasonal_1 Loss: 0.0639 | 0.0325
Epoch 108/300, seasonal_1 Loss: 0.0638 | 0.0325
Epoch 109/300, seasonal_1 Loss: 0.0637 | 0.0324
Epoch 110/300, seasonal_1 Loss: 0.0637 | 0.0324
Epoch 111/300, seasonal_1 Loss: 0.0636 | 0.0324
Epoch 112/300, seasonal_1 Loss: 0.0635 | 0.0324
Epoch 113/300, seasonal_1 Loss: 0.0635 | 0.0324
Epoch 114/300, seasonal_1 Loss: 0.0634 | 0.0323
Epoch 115/300, seasonal_1 Loss: 0.0633 | 0.0323
Epoch 116/300, seasonal_1 Loss: 0.0632 | 0.0323
Epoch 117/300, seasonal_1 Loss: 0.0632 | 0.0323
Epoch 118/300, seasonal_1 Loss: 0.0631 | 0.0323
Epoch 119/300, seasonal_1 Loss: 0.0631 | 0.0322
Epoch 120/300, seasonal_1 Loss: 0.0630 | 0.0322
Epoch 121/300, seasonal_1 Loss: 0.0630 | 0.0322
Epoch 122/300, seasonal_1 Loss: 0.0629 | 0.0322
Epoch 123/300, seasonal_1 Loss: 0.0629 | 0.0322
Epoch 124/300, seasonal_1 Loss: 0.0628 | 0.0321
Epoch 125/300, seasonal_1 Loss: 0.0628 | 0.0321
Epoch 126/300, seasonal_1 Loss: 0.0628 | 0.0319
Epoch 127/300, seasonal_1 Loss: 0.0627 | 0.0318
Epoch 128/300, seasonal_1 Loss: 0.0627 | 0.0319
Epoch 129/300, seasonal_1 Loss: 0.0627 | 0.0315
Epoch 130/300, seasonal_1 Loss: 0.0627 | 0.0314
Epoch 131/300, seasonal_1 Loss: 0.0628 | 0.0310
Epoch 132/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 133/300, seasonal_1 Loss: 0.0629 | 0.0308
Epoch 134/300, seasonal_1 Loss: 0.0630 | 0.0303
Epoch 135/300, seasonal_1 Loss: 0.0630 | 0.0302
Epoch 136/300, seasonal_1 Loss: 0.0631 | 0.0300
Epoch 137/300, seasonal_1 Loss: 0.0632 | 0.0300
Epoch 138/300, seasonal_1 Loss: 0.0632 | 0.0300
Epoch 139/300, seasonal_1 Loss: 0.0633 | 0.0302
Epoch 140/300, seasonal_1 Loss: 0.0636 | 0.0307
Epoch 141/300, seasonal_1 Loss: 0.0641 | 0.0342
Epoch 142/300, seasonal_1 Loss: 0.0663 | 0.0291
Epoch 143/300, seasonal_1 Loss: 0.0637 | 0.0320
Epoch 144/300, seasonal_1 Loss: 0.0656 | 0.0287
Epoch 145/300, seasonal_1 Loss: 0.0636 | 0.0325
Epoch 146/300, seasonal_1 Loss: 0.0657 | 0.0287
Epoch 147/300, seasonal_1 Loss: 0.0636 | 0.0324
Epoch 148/300, seasonal_1 Loss: 0.0652 | 0.0283
Epoch 149/300, seasonal_1 Loss: 0.0633 | 0.0320
Epoch 150/300, seasonal_1 Loss: 0.0645 | 0.0282
Epoch 151/300, seasonal_1 Loss: 0.0631 | 0.0317
Epoch 152/300, seasonal_1 Loss: 0.0638 | 0.0288
Epoch 153/300, seasonal_1 Loss: 0.0629 | 0.0313
Epoch 154/300, seasonal_1 Loss: 0.0633 | 0.0297
Epoch 155/300, seasonal_1 Loss: 0.0628 | 0.0309
Epoch 156/300, seasonal_1 Loss: 0.0629 | 0.0302
Epoch 157/300, seasonal_1 Loss: 0.0626 | 0.0304
Epoch 158/300, seasonal_1 Loss: 0.0625 | 0.0303
Epoch 159/300, seasonal_1 Loss: 0.0624 | 0.0301
Epoch 160/300, seasonal_1 Loss: 0.0623 | 0.0300
Epoch 161/300, seasonal_1 Loss: 0.0622 | 0.0299
Epoch 162/300, seasonal_1 Loss: 0.0621 | 0.0298
Epoch 163/300, seasonal_1 Loss: 0.0620 | 0.0296
Epoch 164/300, seasonal_1 Loss: 0.0619 | 0.0295
Epoch 165/300, seasonal_1 Loss: 0.0618 | 0.0294
Epoch 166/300, seasonal_1 Loss: 0.0617 | 0.0292
Epoch 167/300, seasonal_1 Loss: 0.0616 | 0.0291
Epoch 168/300, seasonal_1 Loss: 0.0615 | 0.0290
Epoch 169/300, seasonal_1 Loss: 0.0615 | 0.0289
Epoch 170/300, seasonal_1 Loss: 0.0614 | 0.0288
Epoch 171/300, seasonal_1 Loss: 0.0613 | 0.0288
Epoch 172/300, seasonal_1 Loss: 0.0613 | 0.0287
Epoch 173/300, seasonal_1 Loss: 0.0612 | 0.0286
Epoch 174/300, seasonal_1 Loss: 0.0612 | 0.0285
Epoch 175/300, seasonal_1 Loss: 0.0611 | 0.0285
Epoch 176/300, seasonal_1 Loss: 0.0611 | 0.0284
Epoch 177/300, seasonal_1 Loss: 0.0611 | 0.0284
Epoch 178/300, seasonal_1 Loss: 0.0610 | 0.0284
Epoch 179/300, seasonal_1 Loss: 0.0610 | 0.0283
Epoch 180/300, seasonal_1 Loss: 0.0609 | 0.0283
Epoch 181/300, seasonal_1 Loss: 0.0609 | 0.0283
Epoch 182/300, seasonal_1 Loss: 0.0609 | 0.0282
Epoch 183/300, seasonal_1 Loss: 0.0609 | 0.0282
Epoch 184/300, seasonal_1 Loss: 0.0608 | 0.0282
Epoch 185/300, seasonal_1 Loss: 0.0608 | 0.0282
Epoch 186/300, seasonal_1 Loss: 0.0608 | 0.0281
Epoch 187/300, seasonal_1 Loss: 0.0607 | 0.0281
Epoch 188/300, seasonal_1 Loss: 0.0607 | 0.0281
Epoch 189/300, seasonal_1 Loss: 0.0607 | 0.0281
Epoch 190/300, seasonal_1 Loss: 0.0607 | 0.0281
Epoch 191/300, seasonal_1 Loss: 0.0607 | 0.0281
Epoch 192/300, seasonal_1 Loss: 0.0606 | 0.0281
Epoch 193/300, seasonal_1 Loss: 0.0606 | 0.0280
Epoch 194/300, seasonal_1 Loss: 0.0606 | 0.0280
Epoch 195/300, seasonal_1 Loss: 0.0606 | 0.0280
Epoch 196/300, seasonal_1 Loss: 0.0606 | 0.0280
Epoch 197/300, seasonal_1 Loss: 0.0605 | 0.0280
Epoch 198/300, seasonal_1 Loss: 0.0605 | 0.0280
Epoch 199/300, seasonal_1 Loss: 0.0605 | 0.0280
Epoch 200/300, seasonal_1 Loss: 0.0605 | 0.0280
Epoch 201/300, seasonal_1 Loss: 0.0605 | 0.0280
Epoch 202/300, seasonal_1 Loss: 0.0605 | 0.0280
Epoch 203/300, seasonal_1 Loss: 0.0604 | 0.0280
Epoch 204/300, seasonal_1 Loss: 0.0604 | 0.0280
Epoch 205/300, seasonal_1 Loss: 0.0604 | 0.0280
Epoch 206/300, seasonal_1 Loss: 0.0604 | 0.0280
Epoch 207/300, seasonal_1 Loss: 0.0604 | 0.0279
Epoch 208/300, seasonal_1 Loss: 0.0604 | 0.0279
Epoch 209/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 210/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 211/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 212/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 213/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 214/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 215/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 216/300, seasonal_1 Loss: 0.0603 | 0.0279
Epoch 217/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 218/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 219/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 220/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 221/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 222/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 223/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 224/300, seasonal_1 Loss: 0.0602 | 0.0279
Epoch 225/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 226/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 227/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 228/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 229/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 230/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 231/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 232/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 233/300, seasonal_1 Loss: 0.0601 | 0.0279
Epoch 234/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 235/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 236/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 237/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 238/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 239/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 240/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 241/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 242/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 243/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 244/300, seasonal_1 Loss: 0.0600 | 0.0279
Epoch 245/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 246/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 247/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 248/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 249/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 250/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 251/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 252/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 253/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 254/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 255/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 256/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 257/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 258/300, seasonal_1 Loss: 0.0599 | 0.0279
Epoch 259/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 260/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 261/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 262/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 263/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 264/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 265/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 266/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 267/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 268/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 269/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 270/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 271/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 272/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 273/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 274/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 275/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 276/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 277/300, seasonal_1 Loss: 0.0598 | 0.0279
Epoch 278/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 279/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 280/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 281/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 282/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 283/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 284/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 285/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 286/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 287/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 288/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 289/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 290/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 291/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 292/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 293/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 294/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 295/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 296/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 297/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 298/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 299/300, seasonal_1 Loss: 0.0597 | 0.0279
Epoch 300/300, seasonal_1 Loss: 0.0597 | 0.0279
Training seasonal_2 component with params: {'observation_period_num': 13, 'train_rates': 0.851670642600215, 'learning_rate': 0.0006817369799399096, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8293367507330445}
Epoch 1/300, seasonal_2 Loss: 0.4464 | 0.2731
Epoch 2/300, seasonal_2 Loss: 0.2272 | 0.1902
Epoch 3/300, seasonal_2 Loss: 0.2291 | 0.1604
Epoch 4/300, seasonal_2 Loss: 0.2042 | 0.4134
Epoch 5/300, seasonal_2 Loss: 0.2019 | 0.1978
Epoch 6/300, seasonal_2 Loss: 0.1654 | 0.1633
Epoch 7/300, seasonal_2 Loss: 0.1363 | 0.1141
Epoch 8/300, seasonal_2 Loss: 0.1174 | 0.0902
Epoch 9/300, seasonal_2 Loss: 0.1161 | 0.0842
Epoch 10/300, seasonal_2 Loss: 0.1142 | 0.1092
Epoch 11/300, seasonal_2 Loss: 0.1191 | 0.0806
Epoch 12/300, seasonal_2 Loss: 0.1157 | 0.0906
Epoch 13/300, seasonal_2 Loss: 0.1118 | 0.0718
Epoch 14/300, seasonal_2 Loss: 0.1005 | 0.0621
Epoch 15/300, seasonal_2 Loss: 0.1030 | 0.0659
Epoch 16/300, seasonal_2 Loss: 0.0968 | 0.0581
Epoch 17/300, seasonal_2 Loss: 0.0952 | 0.0579
Epoch 18/300, seasonal_2 Loss: 0.1000 | 0.0566
Epoch 19/300, seasonal_2 Loss: 0.0972 | 0.0551
Epoch 20/300, seasonal_2 Loss: 0.0917 | 0.0667
Epoch 21/300, seasonal_2 Loss: 0.0924 | 0.0569
Epoch 22/300, seasonal_2 Loss: 0.0893 | 0.0514
Epoch 23/300, seasonal_2 Loss: 0.0910 | 0.0826
Epoch 24/300, seasonal_2 Loss: 0.0949 | 0.0485
Epoch 25/300, seasonal_2 Loss: 0.0920 | 0.0929
Epoch 26/300, seasonal_2 Loss: 0.0928 | 0.0552
Epoch 27/300, seasonal_2 Loss: 0.0876 | 0.0476
Epoch 28/300, seasonal_2 Loss: 0.0859 | 0.0573
Epoch 29/300, seasonal_2 Loss: 0.0861 | 0.0495
Epoch 30/300, seasonal_2 Loss: 0.0835 | 0.0528
Epoch 31/300, seasonal_2 Loss: 0.0824 | 0.0426
Epoch 32/300, seasonal_2 Loss: 0.0809 | 0.0422
Epoch 33/300, seasonal_2 Loss: 0.0812 | 0.0490
Epoch 34/300, seasonal_2 Loss: 0.0804 | 0.0409
Epoch 35/300, seasonal_2 Loss: 0.0787 | 0.0407
Epoch 36/300, seasonal_2 Loss: 0.0786 | 0.0447
Epoch 37/300, seasonal_2 Loss: 0.0790 | 0.0444
Epoch 38/300, seasonal_2 Loss: 0.0831 | 0.0465
Epoch 39/300, seasonal_2 Loss: 0.0878 | 0.0440
Epoch 40/300, seasonal_2 Loss: 0.0824 | 0.0470
Epoch 41/300, seasonal_2 Loss: 0.0911 | 0.0441
Epoch 42/300, seasonal_2 Loss: 0.0878 | 0.0432
Epoch 43/300, seasonal_2 Loss: 0.0855 | 0.0597
Epoch 44/300, seasonal_2 Loss: 0.0879 | 0.0395
Epoch 45/300, seasonal_2 Loss: 0.0803 | 0.0415
Epoch 46/300, seasonal_2 Loss: 0.0848 | 0.0437
Epoch 47/300, seasonal_2 Loss: 0.0793 | 0.0414
Epoch 48/300, seasonal_2 Loss: 0.0794 | 0.0435
Epoch 49/300, seasonal_2 Loss: 0.0770 | 0.0410
Epoch 50/300, seasonal_2 Loss: 0.0765 | 0.0395
Epoch 51/300, seasonal_2 Loss: 0.0761 | 0.0434
Epoch 52/300, seasonal_2 Loss: 0.0751 | 0.0400
Epoch 53/300, seasonal_2 Loss: 0.0742 | 0.0370
Epoch 54/300, seasonal_2 Loss: 0.0741 | 0.0391
Epoch 55/300, seasonal_2 Loss: 0.0727 | 0.0380
Epoch 56/300, seasonal_2 Loss: 0.0723 | 0.0362
Epoch 57/300, seasonal_2 Loss: 0.0723 | 0.0400
Epoch 58/300, seasonal_2 Loss: 0.0719 | 0.0358
Epoch 59/300, seasonal_2 Loss: 0.0713 | 0.0361
Epoch 60/300, seasonal_2 Loss: 0.0710 | 0.0372
Epoch 61/300, seasonal_2 Loss: 0.0708 | 0.0357
Epoch 62/300, seasonal_2 Loss: 0.0705 | 0.0357
Epoch 63/300, seasonal_2 Loss: 0.0703 | 0.0361
Epoch 64/300, seasonal_2 Loss: 0.0700 | 0.0351
Epoch 65/300, seasonal_2 Loss: 0.0699 | 0.0361
Epoch 66/300, seasonal_2 Loss: 0.0697 | 0.0352
Epoch 67/300, seasonal_2 Loss: 0.0695 | 0.0352
Epoch 68/300, seasonal_2 Loss: 0.0693 | 0.0352
Epoch 69/300, seasonal_2 Loss: 0.0691 | 0.0349
Epoch 70/300, seasonal_2 Loss: 0.0688 | 0.0349
Epoch 71/300, seasonal_2 Loss: 0.0687 | 0.0349
Epoch 72/300, seasonal_2 Loss: 0.0685 | 0.0346
Epoch 73/300, seasonal_2 Loss: 0.0684 | 0.0349
Epoch 74/300, seasonal_2 Loss: 0.0682 | 0.0345
Epoch 75/300, seasonal_2 Loss: 0.0680 | 0.0347
Epoch 76/300, seasonal_2 Loss: 0.0678 | 0.0344
Epoch 77/300, seasonal_2 Loss: 0.0676 | 0.0344
Epoch 78/300, seasonal_2 Loss: 0.0674 | 0.0342
Epoch 79/300, seasonal_2 Loss: 0.0673 | 0.0343
Epoch 80/300, seasonal_2 Loss: 0.0671 | 0.0340
Epoch 81/300, seasonal_2 Loss: 0.0669 | 0.0341
Epoch 82/300, seasonal_2 Loss: 0.0668 | 0.0338
Epoch 83/300, seasonal_2 Loss: 0.0666 | 0.0339
Epoch 84/300, seasonal_2 Loss: 0.0663 | 0.0335
Epoch 85/300, seasonal_2 Loss: 0.0662 | 0.0334
Epoch 86/300, seasonal_2 Loss: 0.0660 | 0.0331
Epoch 87/300, seasonal_2 Loss: 0.0658 | 0.0332
Epoch 88/300, seasonal_2 Loss: 0.0656 | 0.0327
Epoch 89/300, seasonal_2 Loss: 0.0654 | 0.0329
Epoch 90/300, seasonal_2 Loss: 0.0652 | 0.0323
Epoch 91/300, seasonal_2 Loss: 0.0651 | 0.0327
Epoch 92/300, seasonal_2 Loss: 0.0649 | 0.0322
Epoch 93/300, seasonal_2 Loss: 0.0649 | 0.0327
Epoch 94/300, seasonal_2 Loss: 0.0649 | 0.0318
Epoch 95/300, seasonal_2 Loss: 0.0650 | 0.0327
Epoch 96/300, seasonal_2 Loss: 0.0652 | 0.0312
Epoch 97/300, seasonal_2 Loss: 0.0658 | 0.0328
Epoch 98/300, seasonal_2 Loss: 0.0672 | 0.0312
Epoch 99/300, seasonal_2 Loss: 0.0683 | 0.0310
Epoch 100/300, seasonal_2 Loss: 0.0708 | 0.0392
Epoch 101/300, seasonal_2 Loss: 0.0721 | 0.0323
Epoch 102/300, seasonal_2 Loss: 0.0735 | 0.0520
Epoch 103/300, seasonal_2 Loss: 0.0712 | 0.0338
Epoch 104/300, seasonal_2 Loss: 0.0693 | 0.0467
Epoch 105/300, seasonal_2 Loss: 0.0668 | 0.0313
Epoch 106/300, seasonal_2 Loss: 0.0647 | 0.0374
Epoch 107/300, seasonal_2 Loss: 0.0635 | 0.0297
Epoch 108/300, seasonal_2 Loss: 0.0629 | 0.0311
Epoch 109/300, seasonal_2 Loss: 0.0628 | 0.0299
Epoch 110/300, seasonal_2 Loss: 0.0628 | 0.0304
Epoch 111/300, seasonal_2 Loss: 0.0627 | 0.0301
Epoch 112/300, seasonal_2 Loss: 0.0625 | 0.0295
Epoch 113/300, seasonal_2 Loss: 0.0623 | 0.0291
Epoch 114/300, seasonal_2 Loss: 0.0622 | 0.0287
Epoch 115/300, seasonal_2 Loss: 0.0621 | 0.0285
Epoch 116/300, seasonal_2 Loss: 0.0621 | 0.0284
Epoch 117/300, seasonal_2 Loss: 0.0620 | 0.0283
Epoch 118/300, seasonal_2 Loss: 0.0619 | 0.0284
Epoch 119/300, seasonal_2 Loss: 0.0618 | 0.0284
Epoch 120/300, seasonal_2 Loss: 0.0618 | 0.0285
Epoch 121/300, seasonal_2 Loss: 0.0617 | 0.0284
Epoch 122/300, seasonal_2 Loss: 0.0617 | 0.0282
Epoch 123/300, seasonal_2 Loss: 0.0616 | 0.0280
Epoch 124/300, seasonal_2 Loss: 0.0615 | 0.0278
Epoch 125/300, seasonal_2 Loss: 0.0615 | 0.0277
Epoch 126/300, seasonal_2 Loss: 0.0614 | 0.0276
Epoch 127/300, seasonal_2 Loss: 0.0614 | 0.0277
Epoch 128/300, seasonal_2 Loss: 0.0613 | 0.0278
Epoch 129/300, seasonal_2 Loss: 0.0613 | 0.0278
Epoch 130/300, seasonal_2 Loss: 0.0612 | 0.0277
Epoch 131/300, seasonal_2 Loss: 0.0612 | 0.0276
Epoch 132/300, seasonal_2 Loss: 0.0611 | 0.0274
Epoch 133/300, seasonal_2 Loss: 0.0611 | 0.0273
Epoch 134/300, seasonal_2 Loss: 0.0611 | 0.0273
Epoch 135/300, seasonal_2 Loss: 0.0610 | 0.0274
Epoch 136/300, seasonal_2 Loss: 0.0610 | 0.0274
Epoch 137/300, seasonal_2 Loss: 0.0609 | 0.0273
Epoch 138/300, seasonal_2 Loss: 0.0609 | 0.0272
Epoch 139/300, seasonal_2 Loss: 0.0608 | 0.0272
Epoch 140/300, seasonal_2 Loss: 0.0608 | 0.0272
Epoch 141/300, seasonal_2 Loss: 0.0608 | 0.0271
Epoch 142/300, seasonal_2 Loss: 0.0607 | 0.0271
Epoch 143/300, seasonal_2 Loss: 0.0607 | 0.0271
Epoch 144/300, seasonal_2 Loss: 0.0607 | 0.0270
Epoch 145/300, seasonal_2 Loss: 0.0606 | 0.0270
Epoch 146/300, seasonal_2 Loss: 0.0606 | 0.0270
Epoch 147/300, seasonal_2 Loss: 0.0606 | 0.0270
Epoch 148/300, seasonal_2 Loss: 0.0606 | 0.0269
Epoch 149/300, seasonal_2 Loss: 0.0605 | 0.0269
Epoch 150/300, seasonal_2 Loss: 0.0605 | 0.0269
Epoch 151/300, seasonal_2 Loss: 0.0605 | 0.0269
Epoch 152/300, seasonal_2 Loss: 0.0604 | 0.0268
Epoch 153/300, seasonal_2 Loss: 0.0604 | 0.0268
Epoch 154/300, seasonal_2 Loss: 0.0604 | 0.0268
Epoch 155/300, seasonal_2 Loss: 0.0604 | 0.0268
Epoch 156/300, seasonal_2 Loss: 0.0603 | 0.0268
Epoch 157/300, seasonal_2 Loss: 0.0603 | 0.0267
Epoch 158/300, seasonal_2 Loss: 0.0603 | 0.0267
Epoch 159/300, seasonal_2 Loss: 0.0603 | 0.0267
Epoch 160/300, seasonal_2 Loss: 0.0602 | 0.0267
Epoch 161/300, seasonal_2 Loss: 0.0602 | 0.0267
Epoch 162/300, seasonal_2 Loss: 0.0602 | 0.0266
Epoch 163/300, seasonal_2 Loss: 0.0602 | 0.0266
Epoch 164/300, seasonal_2 Loss: 0.0602 | 0.0266
Epoch 165/300, seasonal_2 Loss: 0.0601 | 0.0266
Epoch 166/300, seasonal_2 Loss: 0.0601 | 0.0266
Epoch 167/300, seasonal_2 Loss: 0.0601 | 0.0266
Epoch 168/300, seasonal_2 Loss: 0.0601 | 0.0266
Epoch 169/300, seasonal_2 Loss: 0.0601 | 0.0265
Epoch 170/300, seasonal_2 Loss: 0.0601 | 0.0265
Epoch 171/300, seasonal_2 Loss: 0.0600 | 0.0265
Epoch 172/300, seasonal_2 Loss: 0.0600 | 0.0265
Epoch 173/300, seasonal_2 Loss: 0.0600 | 0.0265
Epoch 174/300, seasonal_2 Loss: 0.0600 | 0.0265
Epoch 175/300, seasonal_2 Loss: 0.0600 | 0.0265
Epoch 176/300, seasonal_2 Loss: 0.0600 | 0.0265
Epoch 177/300, seasonal_2 Loss: 0.0600 | 0.0264
Epoch 178/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 179/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 180/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 181/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 182/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 183/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 184/300, seasonal_2 Loss: 0.0599 | 0.0264
Epoch 185/300, seasonal_2 Loss: 0.0598 | 0.0264
Epoch 186/300, seasonal_2 Loss: 0.0598 | 0.0264
Epoch 187/300, seasonal_2 Loss: 0.0598 | 0.0264
Epoch 188/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 189/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 190/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 191/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 192/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 193/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 194/300, seasonal_2 Loss: 0.0598 | 0.0263
Epoch 195/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 196/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 197/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 198/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 199/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 200/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 201/300, seasonal_2 Loss: 0.0597 | 0.0263
Epoch 202/300, seasonal_2 Loss: 0.0597 | 0.0262
Epoch 203/300, seasonal_2 Loss: 0.0597 | 0.0262
Epoch 204/300, seasonal_2 Loss: 0.0597 | 0.0262
Epoch 205/300, seasonal_2 Loss: 0.0597 | 0.0262
Epoch 206/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 207/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 208/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 209/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 210/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 211/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 212/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 213/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 214/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 215/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 216/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 217/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 218/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 219/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 220/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 221/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 222/300, seasonal_2 Loss: 0.0596 | 0.0262
Epoch 223/300, seasonal_2 Loss: 0.0595 | 0.0262
Epoch 224/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 225/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 226/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 227/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 228/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 229/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 230/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 231/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 232/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 233/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 234/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 235/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 236/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 237/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 238/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 239/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 240/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 241/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 242/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 243/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 244/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 245/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 246/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 247/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 248/300, seasonal_2 Loss: 0.0595 | 0.0261
Epoch 249/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 250/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 251/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 252/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 253/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 254/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 255/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 256/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 257/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 258/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 259/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 260/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 261/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 262/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 263/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 264/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 265/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 266/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 267/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 268/300, seasonal_2 Loss: 0.0594 | 0.0261
Epoch 269/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 270/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 271/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 272/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 273/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 274/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 275/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 276/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 277/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 278/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 279/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 280/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 281/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 282/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 283/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 284/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 285/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 286/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 287/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 288/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 289/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 290/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 291/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 292/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 293/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 294/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 295/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 296/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 297/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 298/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 299/300, seasonal_2 Loss: 0.0594 | 0.0260
Epoch 300/300, seasonal_2 Loss: 0.0594 | 0.0260
Training seasonal_3 component with params: {'observation_period_num': 16, 'train_rates': 0.8749265381820085, 'learning_rate': 0.00041040585728405594, 'batch_size': 89, 'step_size': 6, 'gamma': 0.8713587604854824}
Epoch 1/300, seasonal_3 Loss: 0.3657 | 0.1921
Epoch 2/300, seasonal_3 Loss: 0.1642 | 0.1295
Epoch 3/300, seasonal_3 Loss: 0.1542 | 0.1147
Epoch 4/300, seasonal_3 Loss: 0.1341 | 0.1360
Epoch 5/300, seasonal_3 Loss: 0.1276 | 0.1339
Epoch 6/300, seasonal_3 Loss: 0.1239 | 0.1435
Epoch 7/300, seasonal_3 Loss: 0.1262 | 0.1726
Epoch 8/300, seasonal_3 Loss: 0.1243 | 0.1309
Epoch 9/300, seasonal_3 Loss: 0.1110 | 0.1072
Epoch 10/300, seasonal_3 Loss: 0.1030 | 0.0732
Epoch 11/300, seasonal_3 Loss: 0.1081 | 0.0663
Epoch 12/300, seasonal_3 Loss: 0.1188 | 0.0653
Epoch 13/300, seasonal_3 Loss: 0.1066 | 0.0585
Epoch 14/300, seasonal_3 Loss: 0.0959 | 0.0541
Epoch 15/300, seasonal_3 Loss: 0.0998 | 0.0644
Epoch 16/300, seasonal_3 Loss: 0.0976 | 0.0578
Epoch 17/300, seasonal_3 Loss: 0.0890 | 0.0504
Epoch 18/300, seasonal_3 Loss: 0.0861 | 0.0514
Epoch 19/300, seasonal_3 Loss: 0.0861 | 0.0548
Epoch 20/300, seasonal_3 Loss: 0.0846 | 0.0502
Epoch 21/300, seasonal_3 Loss: 0.0812 | 0.0470
Epoch 22/300, seasonal_3 Loss: 0.0797 | 0.0437
Epoch 23/300, seasonal_3 Loss: 0.0793 | 0.0409
Epoch 24/300, seasonal_3 Loss: 0.0804 | 0.0460
Epoch 25/300, seasonal_3 Loss: 0.0813 | 0.0536
Epoch 26/300, seasonal_3 Loss: 0.0807 | 0.0520
Epoch 27/300, seasonal_3 Loss: 0.0778 | 0.0442
Epoch 28/300, seasonal_3 Loss: 0.0757 | 0.0390
Epoch 29/300, seasonal_3 Loss: 0.0746 | 0.0370
Epoch 30/300, seasonal_3 Loss: 0.0735 | 0.0364
Epoch 31/300, seasonal_3 Loss: 0.0726 | 0.0366
Epoch 32/300, seasonal_3 Loss: 0.0722 | 0.0370
Epoch 33/300, seasonal_3 Loss: 0.0724 | 0.0372
Epoch 34/300, seasonal_3 Loss: 0.0732 | 0.0376
Epoch 35/300, seasonal_3 Loss: 0.0743 | 0.0365
Epoch 36/300, seasonal_3 Loss: 0.0741 | 0.0352
Epoch 37/300, seasonal_3 Loss: 0.0732 | 0.0344
Epoch 38/300, seasonal_3 Loss: 0.0724 | 0.0342
Epoch 39/300, seasonal_3 Loss: 0.0713 | 0.0339
Epoch 40/300, seasonal_3 Loss: 0.0704 | 0.0345
Epoch 41/300, seasonal_3 Loss: 0.0701 | 0.0358
Epoch 42/300, seasonal_3 Loss: 0.0706 | 0.0366
Epoch 43/300, seasonal_3 Loss: 0.0715 | 0.0377
Epoch 44/300, seasonal_3 Loss: 0.0717 | 0.0372
Epoch 45/300, seasonal_3 Loss: 0.0706 | 0.0358
Epoch 46/300, seasonal_3 Loss: 0.0697 | 0.0342
Epoch 47/300, seasonal_3 Loss: 0.0691 | 0.0333
Epoch 48/300, seasonal_3 Loss: 0.0688 | 0.0328
Epoch 49/300, seasonal_3 Loss: 0.0685 | 0.0323
Epoch 50/300, seasonal_3 Loss: 0.0682 | 0.0322
Epoch 51/300, seasonal_3 Loss: 0.0678 | 0.0321
Epoch 52/300, seasonal_3 Loss: 0.0676 | 0.0320
Epoch 53/300, seasonal_3 Loss: 0.0673 | 0.0320
Epoch 54/300, seasonal_3 Loss: 0.0671 | 0.0319
Epoch 55/300, seasonal_3 Loss: 0.0669 | 0.0318
Epoch 56/300, seasonal_3 Loss: 0.0668 | 0.0318
Epoch 57/300, seasonal_3 Loss: 0.0666 | 0.0316
Epoch 58/300, seasonal_3 Loss: 0.0665 | 0.0317
Epoch 59/300, seasonal_3 Loss: 0.0664 | 0.0316
Epoch 60/300, seasonal_3 Loss: 0.0662 | 0.0315
Epoch 61/300, seasonal_3 Loss: 0.0660 | 0.0316
Epoch 62/300, seasonal_3 Loss: 0.0660 | 0.0316
Epoch 63/300, seasonal_3 Loss: 0.0659 | 0.0316
Epoch 64/300, seasonal_3 Loss: 0.0659 | 0.0317
Epoch 65/300, seasonal_3 Loss: 0.0659 | 0.0316
Epoch 66/300, seasonal_3 Loss: 0.0657 | 0.0315
Epoch 67/300, seasonal_3 Loss: 0.0655 | 0.0313
Epoch 68/300, seasonal_3 Loss: 0.0653 | 0.0312
Epoch 69/300, seasonal_3 Loss: 0.0651 | 0.0311
Epoch 70/300, seasonal_3 Loss: 0.0650 | 0.0310
Epoch 71/300, seasonal_3 Loss: 0.0649 | 0.0309
Epoch 72/300, seasonal_3 Loss: 0.0647 | 0.0309
Epoch 73/300, seasonal_3 Loss: 0.0646 | 0.0307
Epoch 74/300, seasonal_3 Loss: 0.0645 | 0.0307
Epoch 75/300, seasonal_3 Loss: 0.0644 | 0.0306
Epoch 76/300, seasonal_3 Loss: 0.0643 | 0.0306
Epoch 77/300, seasonal_3 Loss: 0.0642 | 0.0305
Epoch 78/300, seasonal_3 Loss: 0.0641 | 0.0305
Epoch 79/300, seasonal_3 Loss: 0.0640 | 0.0304
Epoch 80/300, seasonal_3 Loss: 0.0640 | 0.0304
Epoch 81/300, seasonal_3 Loss: 0.0639 | 0.0303
Epoch 82/300, seasonal_3 Loss: 0.0638 | 0.0303
Epoch 83/300, seasonal_3 Loss: 0.0638 | 0.0303
Epoch 84/300, seasonal_3 Loss: 0.0638 | 0.0302
Epoch 85/300, seasonal_3 Loss: 0.0637 | 0.0302
Epoch 86/300, seasonal_3 Loss: 0.0637 | 0.0301
Epoch 87/300, seasonal_3 Loss: 0.0636 | 0.0301
Epoch 88/300, seasonal_3 Loss: 0.0636 | 0.0301
Epoch 89/300, seasonal_3 Loss: 0.0635 | 0.0300
Epoch 90/300, seasonal_3 Loss: 0.0635 | 0.0300
Epoch 91/300, seasonal_3 Loss: 0.0635 | 0.0300
Epoch 92/300, seasonal_3 Loss: 0.0634 | 0.0300
Epoch 93/300, seasonal_3 Loss: 0.0634 | 0.0299
Epoch 94/300, seasonal_3 Loss: 0.0634 | 0.0299
Epoch 95/300, seasonal_3 Loss: 0.0633 | 0.0299
Epoch 96/300, seasonal_3 Loss: 0.0633 | 0.0299
Epoch 97/300, seasonal_3 Loss: 0.0633 | 0.0298
Epoch 98/300, seasonal_3 Loss: 0.0633 | 0.0298
Epoch 99/300, seasonal_3 Loss: 0.0632 | 0.0298
Epoch 100/300, seasonal_3 Loss: 0.0632 | 0.0298
Epoch 101/300, seasonal_3 Loss: 0.0632 | 0.0298
Epoch 102/300, seasonal_3 Loss: 0.0632 | 0.0297
Epoch 103/300, seasonal_3 Loss: 0.0631 | 0.0297
Epoch 104/300, seasonal_3 Loss: 0.0631 | 0.0297
Epoch 105/300, seasonal_3 Loss: 0.0631 | 0.0297
Epoch 106/300, seasonal_3 Loss: 0.0631 | 0.0297
Epoch 107/300, seasonal_3 Loss: 0.0631 | 0.0297
Epoch 108/300, seasonal_3 Loss: 0.0631 | 0.0296
Epoch 109/300, seasonal_3 Loss: 0.0630 | 0.0296
Epoch 110/300, seasonal_3 Loss: 0.0630 | 0.0296
Epoch 111/300, seasonal_3 Loss: 0.0630 | 0.0296
Epoch 112/300, seasonal_3 Loss: 0.0630 | 0.0296
Epoch 113/300, seasonal_3 Loss: 0.0630 | 0.0296
Epoch 114/300, seasonal_3 Loss: 0.0630 | 0.0296
Epoch 115/300, seasonal_3 Loss: 0.0629 | 0.0296
Epoch 116/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 117/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 118/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 119/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 120/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 121/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 122/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 123/300, seasonal_3 Loss: 0.0629 | 0.0295
Epoch 124/300, seasonal_3 Loss: 0.0628 | 0.0295
Epoch 125/300, seasonal_3 Loss: 0.0628 | 0.0295
Epoch 126/300, seasonal_3 Loss: 0.0628 | 0.0295
Epoch 127/300, seasonal_3 Loss: 0.0628 | 0.0295
Epoch 128/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 129/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 130/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 131/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 132/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 133/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 134/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 135/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 136/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 137/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 138/300, seasonal_3 Loss: 0.0628 | 0.0294
Epoch 139/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 140/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 141/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 142/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 143/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 144/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 145/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 146/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 147/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 148/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 149/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 150/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 151/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 152/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 153/300, seasonal_3 Loss: 0.0627 | 0.0294
Epoch 154/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 155/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 156/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 157/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 158/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 159/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 160/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 161/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 162/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 163/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 164/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 165/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 166/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 167/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 168/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 169/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 170/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 171/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 172/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 173/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 174/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 175/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 176/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 177/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 178/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 179/300, seasonal_3 Loss: 0.0627 | 0.0293
Epoch 180/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 181/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 182/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 183/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 184/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 185/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 186/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 187/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 188/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 189/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 190/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 191/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 192/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 193/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 194/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 195/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 196/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 197/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 198/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 199/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 200/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 201/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 202/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 203/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 204/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 205/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 206/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 207/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 208/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 209/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 210/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 211/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 212/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 213/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 214/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 215/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 216/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 217/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 218/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 219/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 220/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 221/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 222/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 223/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 224/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 225/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 226/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 227/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 228/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 229/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 230/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 231/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 232/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 233/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 234/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 235/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 236/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 237/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 238/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 239/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 240/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 241/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 242/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 243/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 244/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 245/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 246/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 247/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 248/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 249/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 250/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 251/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 252/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 253/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 254/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 255/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 256/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 257/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 258/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 259/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 260/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 261/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 262/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 263/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 264/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 265/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 266/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 267/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 268/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 269/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 270/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 271/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 272/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 273/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 274/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 275/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 276/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 277/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 278/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 279/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 280/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 281/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 282/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 283/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 284/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 285/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 286/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 287/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 288/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 289/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 290/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 291/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 292/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 293/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 294/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 295/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 296/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 297/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 298/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 299/300, seasonal_3 Loss: 0.0626 | 0.0293
Epoch 300/300, seasonal_3 Loss: 0.0626 | 0.0293
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8882732833366698, 'learning_rate': 9.619234404021868e-05, 'batch_size': 20, 'step_size': 9, 'gamma': 0.8594921608746808}
Epoch 1/300, resid Loss: 0.1834 | 0.1030
Epoch 2/300, resid Loss: 0.1160 | 0.0784
Epoch 3/300, resid Loss: 0.1037 | 0.0681
Epoch 4/300, resid Loss: 0.0982 | 0.0631
Epoch 5/300, resid Loss: 0.0954 | 0.0579
Epoch 6/300, resid Loss: 0.0923 | 0.0553
Epoch 7/300, resid Loss: 0.0893 | 0.0519
Epoch 8/300, resid Loss: 0.0864 | 0.0492
Epoch 9/300, resid Loss: 0.0841 | 0.0468
Epoch 10/300, resid Loss: 0.0819 | 0.0452
Epoch 11/300, resid Loss: 0.0799 | 0.0434
Epoch 12/300, resid Loss: 0.0785 | 0.0423
Epoch 13/300, resid Loss: 0.0773 | 0.0415
Epoch 14/300, resid Loss: 0.0762 | 0.0406
Epoch 15/300, resid Loss: 0.0747 | 0.0394
Epoch 16/300, resid Loss: 0.0737 | 0.0384
Epoch 17/300, resid Loss: 0.0726 | 0.0373
Epoch 18/300, resid Loss: 0.0716 | 0.0364
Epoch 19/300, resid Loss: 0.0706 | 0.0352
Epoch 20/300, resid Loss: 0.0700 | 0.0347
Epoch 21/300, resid Loss: 0.0691 | 0.0342
Epoch 22/300, resid Loss: 0.0684 | 0.0339
Epoch 23/300, resid Loss: 0.0679 | 0.0336
Epoch 24/300, resid Loss: 0.0673 | 0.0328
Epoch 25/300, resid Loss: 0.0668 | 0.0327
Epoch 26/300, resid Loss: 0.0664 | 0.0325
Epoch 27/300, resid Loss: 0.0661 | 0.0323
Epoch 28/300, resid Loss: 0.0657 | 0.0319
Epoch 29/300, resid Loss: 0.0654 | 0.0317
Epoch 30/300, resid Loss: 0.0652 | 0.0316
Epoch 31/300, resid Loss: 0.0650 | 0.0314
Epoch 32/300, resid Loss: 0.0647 | 0.0312
Epoch 33/300, resid Loss: 0.0644 | 0.0308
Epoch 34/300, resid Loss: 0.0641 | 0.0306
Epoch 35/300, resid Loss: 0.0639 | 0.0304
Epoch 36/300, resid Loss: 0.0637 | 0.0303
Epoch 37/300, resid Loss: 0.0633 | 0.0298
Epoch 38/300, resid Loss: 0.0631 | 0.0296
Epoch 39/300, resid Loss: 0.0629 | 0.0295
Epoch 40/300, resid Loss: 0.0627 | 0.0295
Epoch 41/300, resid Loss: 0.0625 | 0.0295
Epoch 42/300, resid Loss: 0.0622 | 0.0288
Epoch 43/300, resid Loss: 0.0620 | 0.0287
Epoch 44/300, resid Loss: 0.0618 | 0.0287
Epoch 45/300, resid Loss: 0.0616 | 0.0286
Epoch 46/300, resid Loss: 0.0613 | 0.0282
Epoch 47/300, resid Loss: 0.0612 | 0.0281
Epoch 48/300, resid Loss: 0.0611 | 0.0281
Epoch 49/300, resid Loss: 0.0609 | 0.0280
Epoch 50/300, resid Loss: 0.0608 | 0.0280
Epoch 51/300, resid Loss: 0.0606 | 0.0279
Epoch 52/300, resid Loss: 0.0604 | 0.0278
Epoch 53/300, resid Loss: 0.0603 | 0.0277
Epoch 54/300, resid Loss: 0.0602 | 0.0277
Epoch 55/300, resid Loss: 0.0600 | 0.0276
Epoch 56/300, resid Loss: 0.0599 | 0.0276
Epoch 57/300, resid Loss: 0.0598 | 0.0276
Epoch 58/300, resid Loss: 0.0597 | 0.0276
Epoch 59/300, resid Loss: 0.0596 | 0.0275
Epoch 60/300, resid Loss: 0.0594 | 0.0277
Epoch 61/300, resid Loss: 0.0594 | 0.0277
Epoch 62/300, resid Loss: 0.0593 | 0.0277
Epoch 63/300, resid Loss: 0.0592 | 0.0277
Epoch 64/300, resid Loss: 0.0591 | 0.0279
Epoch 65/300, resid Loss: 0.0590 | 0.0278
Epoch 66/300, resid Loss: 0.0589 | 0.0278
Epoch 67/300, resid Loss: 0.0588 | 0.0277
Epoch 68/300, resid Loss: 0.0588 | 0.0277
Epoch 69/300, resid Loss: 0.0586 | 0.0277
Epoch 70/300, resid Loss: 0.0586 | 0.0276
Epoch 71/300, resid Loss: 0.0585 | 0.0275
Epoch 72/300, resid Loss: 0.0584 | 0.0275
Epoch 73/300, resid Loss: 0.0583 | 0.0273
Epoch 74/300, resid Loss: 0.0583 | 0.0273
Epoch 75/300, resid Loss: 0.0582 | 0.0272
Epoch 76/300, resid Loss: 0.0582 | 0.0272
Epoch 77/300, resid Loss: 0.0581 | 0.0271
Epoch 78/300, resid Loss: 0.0580 | 0.0270
Epoch 79/300, resid Loss: 0.0580 | 0.0270
Epoch 80/300, resid Loss: 0.0579 | 0.0269
Epoch 81/300, resid Loss: 0.0579 | 0.0269
Epoch 82/300, resid Loss: 0.0578 | 0.0269
Epoch 83/300, resid Loss: 0.0577 | 0.0268
Epoch 84/300, resid Loss: 0.0577 | 0.0268
Epoch 85/300, resid Loss: 0.0577 | 0.0268
Epoch 86/300, resid Loss: 0.0576 | 0.0267
Epoch 87/300, resid Loss: 0.0576 | 0.0267
Epoch 88/300, resid Loss: 0.0575 | 0.0267
Epoch 89/300, resid Loss: 0.0575 | 0.0267
Epoch 90/300, resid Loss: 0.0575 | 0.0266
Epoch 91/300, resid Loss: 0.0574 | 0.0266
Epoch 92/300, resid Loss: 0.0574 | 0.0266
Epoch 93/300, resid Loss: 0.0573 | 0.0266
Epoch 94/300, resid Loss: 0.0573 | 0.0266
Epoch 95/300, resid Loss: 0.0573 | 0.0266
Epoch 96/300, resid Loss: 0.0572 | 0.0265
Epoch 97/300, resid Loss: 0.0572 | 0.0265
Epoch 98/300, resid Loss: 0.0572 | 0.0265
Epoch 99/300, resid Loss: 0.0572 | 0.0265
Epoch 100/300, resid Loss: 0.0571 | 0.0263
Epoch 101/300, resid Loss: 0.0571 | 0.0263
Epoch 102/300, resid Loss: 0.0571 | 0.0263
Epoch 103/300, resid Loss: 0.0571 | 0.0263
Epoch 104/300, resid Loss: 0.0570 | 0.0263
Epoch 105/300, resid Loss: 0.0570 | 0.0261
Epoch 106/300, resid Loss: 0.0570 | 0.0261
Epoch 107/300, resid Loss: 0.0570 | 0.0261
Epoch 108/300, resid Loss: 0.0570 | 0.0260
Epoch 109/300, resid Loss: 0.0570 | 0.0258
Epoch 110/300, resid Loss: 0.0569 | 0.0258
Epoch 111/300, resid Loss: 0.0569 | 0.0258
Epoch 112/300, resid Loss: 0.0569 | 0.0258
Epoch 113/300, resid Loss: 0.0569 | 0.0258
Epoch 114/300, resid Loss: 0.0569 | 0.0257
Epoch 115/300, resid Loss: 0.0569 | 0.0256
Epoch 116/300, resid Loss: 0.0569 | 0.0256
Epoch 117/300, resid Loss: 0.0568 | 0.0256
Epoch 118/300, resid Loss: 0.0568 | 0.0256
Epoch 119/300, resid Loss: 0.0568 | 0.0256
Epoch 120/300, resid Loss: 0.0568 | 0.0256
Epoch 121/300, resid Loss: 0.0568 | 0.0256
Epoch 122/300, resid Loss: 0.0568 | 0.0256
Epoch 123/300, resid Loss: 0.0568 | 0.0256
Epoch 124/300, resid Loss: 0.0567 | 0.0256
Epoch 125/300, resid Loss: 0.0567 | 0.0256
Epoch 126/300, resid Loss: 0.0567 | 0.0256
Epoch 127/300, resid Loss: 0.0567 | 0.0256
Epoch 128/300, resid Loss: 0.0567 | 0.0256
Epoch 129/300, resid Loss: 0.0567 | 0.0256
Epoch 130/300, resid Loss: 0.0567 | 0.0256
Epoch 131/300, resid Loss: 0.0567 | 0.0256
Epoch 132/300, resid Loss: 0.0566 | 0.0256
Epoch 133/300, resid Loss: 0.0566 | 0.0256
Epoch 134/300, resid Loss: 0.0566 | 0.0256
Epoch 135/300, resid Loss: 0.0566 | 0.0256
Epoch 136/300, resid Loss: 0.0566 | 0.0256
Epoch 137/300, resid Loss: 0.0566 | 0.0256
Epoch 138/300, resid Loss: 0.0566 | 0.0256
Epoch 139/300, resid Loss: 0.0566 | 0.0256
Epoch 140/300, resid Loss: 0.0566 | 0.0256
Epoch 141/300, resid Loss: 0.0565 | 0.0256
Epoch 142/300, resid Loss: 0.0565 | 0.0256
Epoch 143/300, resid Loss: 0.0565 | 0.0256
Epoch 144/300, resid Loss: 0.0565 | 0.0256
Epoch 145/300, resid Loss: 0.0565 | 0.0256
Epoch 146/300, resid Loss: 0.0565 | 0.0256
Epoch 147/300, resid Loss: 0.0565 | 0.0256
Epoch 148/300, resid Loss: 0.0565 | 0.0256
Epoch 149/300, resid Loss: 0.0565 | 0.0256
Epoch 150/300, resid Loss: 0.0565 | 0.0256
Epoch 151/300, resid Loss: 0.0564 | 0.0256
Epoch 152/300, resid Loss: 0.0564 | 0.0256
Epoch 153/300, resid Loss: 0.0564 | 0.0256
Epoch 154/300, resid Loss: 0.0564 | 0.0256
Epoch 155/300, resid Loss: 0.0564 | 0.0256
Epoch 156/300, resid Loss: 0.0564 | 0.0256
Epoch 157/300, resid Loss: 0.0564 | 0.0256
Epoch 158/300, resid Loss: 0.0564 | 0.0256
Epoch 159/300, resid Loss: 0.0564 | 0.0256
Epoch 160/300, resid Loss: 0.0564 | 0.0256
Epoch 161/300, resid Loss: 0.0564 | 0.0255
Epoch 162/300, resid Loss: 0.0564 | 0.0255
Epoch 163/300, resid Loss: 0.0564 | 0.0255
Epoch 164/300, resid Loss: 0.0564 | 0.0255
Epoch 165/300, resid Loss: 0.0564 | 0.0255
Epoch 166/300, resid Loss: 0.0563 | 0.0255
Epoch 167/300, resid Loss: 0.0563 | 0.0255
Epoch 168/300, resid Loss: 0.0563 | 0.0255
Epoch 169/300, resid Loss: 0.0563 | 0.0255
Epoch 170/300, resid Loss: 0.0563 | 0.0255
Epoch 171/300, resid Loss: 0.0563 | 0.0255
Epoch 172/300, resid Loss: 0.0563 | 0.0255
Epoch 173/300, resid Loss: 0.0563 | 0.0255
Epoch 174/300, resid Loss: 0.0563 | 0.0255
Epoch 175/300, resid Loss: 0.0563 | 0.0255
Epoch 176/300, resid Loss: 0.0563 | 0.0255
Epoch 177/300, resid Loss: 0.0563 | 0.0255
Epoch 178/300, resid Loss: 0.0563 | 0.0255
Epoch 179/300, resid Loss: 0.0563 | 0.0255
Epoch 180/300, resid Loss: 0.0563 | 0.0255
Epoch 181/300, resid Loss: 0.0563 | 0.0255
Epoch 182/300, resid Loss: 0.0563 | 0.0255
Epoch 183/300, resid Loss: 0.0563 | 0.0255
Epoch 184/300, resid Loss: 0.0563 | 0.0255
Epoch 185/300, resid Loss: 0.0563 | 0.0255
Epoch 186/300, resid Loss: 0.0563 | 0.0255
Epoch 187/300, resid Loss: 0.0563 | 0.0255
Epoch 188/300, resid Loss: 0.0563 | 0.0255
Epoch 189/300, resid Loss: 0.0563 | 0.0255
Epoch 190/300, resid Loss: 0.0563 | 0.0255
Epoch 191/300, resid Loss: 0.0563 | 0.0255
Epoch 192/300, resid Loss: 0.0563 | 0.0255
Epoch 193/300, resid Loss: 0.0563 | 0.0255
Epoch 194/300, resid Loss: 0.0563 | 0.0255
Epoch 195/300, resid Loss: 0.0563 | 0.0255
Epoch 196/300, resid Loss: 0.0563 | 0.0255
Epoch 197/300, resid Loss: 0.0563 | 0.0255
Epoch 198/300, resid Loss: 0.0563 | 0.0255
Epoch 199/300, resid Loss: 0.0563 | 0.0255
Epoch 200/300, resid Loss: 0.0563 | 0.0255
Epoch 201/300, resid Loss: 0.0563 | 0.0255
Epoch 202/300, resid Loss: 0.0563 | 0.0255
Epoch 203/300, resid Loss: 0.0563 | 0.0255
Epoch 204/300, resid Loss: 0.0562 | 0.0255
Epoch 205/300, resid Loss: 0.0562 | 0.0255
Epoch 206/300, resid Loss: 0.0562 | 0.0255
Epoch 207/300, resid Loss: 0.0562 | 0.0255
Epoch 208/300, resid Loss: 0.0562 | 0.0255
Epoch 209/300, resid Loss: 0.0562 | 0.0255
Epoch 210/300, resid Loss: 0.0562 | 0.0255
Epoch 211/300, resid Loss: 0.0562 | 0.0255
Epoch 212/300, resid Loss: 0.0562 | 0.0255
Epoch 213/300, resid Loss: 0.0562 | 0.0255
Epoch 214/300, resid Loss: 0.0562 | 0.0255
Epoch 215/300, resid Loss: 0.0562 | 0.0255
Epoch 216/300, resid Loss: 0.0562 | 0.0255
Epoch 217/300, resid Loss: 0.0562 | 0.0255
Epoch 218/300, resid Loss: 0.0562 | 0.0255
Epoch 219/300, resid Loss: 0.0562 | 0.0255
Epoch 220/300, resid Loss: 0.0562 | 0.0255
Epoch 221/300, resid Loss: 0.0562 | 0.0255
Epoch 222/300, resid Loss: 0.0562 | 0.0255
Epoch 223/300, resid Loss: 0.0562 | 0.0255
Epoch 224/300, resid Loss: 0.0562 | 0.0255
Epoch 225/300, resid Loss: 0.0562 | 0.0255
Epoch 226/300, resid Loss: 0.0562 | 0.0255
Epoch 227/300, resid Loss: 0.0562 | 0.0255
Epoch 228/300, resid Loss: 0.0562 | 0.0255
Epoch 229/300, resid Loss: 0.0562 | 0.0255
Epoch 230/300, resid Loss: 0.0562 | 0.0255
Epoch 231/300, resid Loss: 0.0562 | 0.0255
Epoch 232/300, resid Loss: 0.0562 | 0.0255
Epoch 233/300, resid Loss: 0.0562 | 0.0255
Epoch 234/300, resid Loss: 0.0562 | 0.0255
Epoch 235/300, resid Loss: 0.0562 | 0.0255
Epoch 236/300, resid Loss: 0.0562 | 0.0255
Epoch 237/300, resid Loss: 0.0562 | 0.0255
Epoch 238/300, resid Loss: 0.0562 | 0.0255
Epoch 239/300, resid Loss: 0.0562 | 0.0255
Epoch 240/300, resid Loss: 0.0562 | 0.0255
Epoch 241/300, resid Loss: 0.0562 | 0.0255
Epoch 242/300, resid Loss: 0.0562 | 0.0255
Epoch 243/300, resid Loss: 0.0562 | 0.0255
Epoch 244/300, resid Loss: 0.0562 | 0.0255
Epoch 245/300, resid Loss: 0.0562 | 0.0255
Epoch 246/300, resid Loss: 0.0562 | 0.0255
Epoch 247/300, resid Loss: 0.0562 | 0.0255
Epoch 248/300, resid Loss: 0.0562 | 0.0255
Epoch 249/300, resid Loss: 0.0562 | 0.0255
Epoch 250/300, resid Loss: 0.0562 | 0.0255
Epoch 251/300, resid Loss: 0.0562 | 0.0255
Epoch 252/300, resid Loss: 0.0562 | 0.0255
Epoch 253/300, resid Loss: 0.0562 | 0.0255
Epoch 254/300, resid Loss: 0.0562 | 0.0255
Epoch 255/300, resid Loss: 0.0562 | 0.0255
Epoch 256/300, resid Loss: 0.0562 | 0.0255
Epoch 257/300, resid Loss: 0.0562 | 0.0255
Epoch 258/300, resid Loss: 0.0562 | 0.0255
Epoch 259/300, resid Loss: 0.0562 | 0.0255
Epoch 260/300, resid Loss: 0.0562 | 0.0255
Epoch 261/300, resid Loss: 0.0562 | 0.0255
Epoch 262/300, resid Loss: 0.0562 | 0.0255
Epoch 263/300, resid Loss: 0.0562 | 0.0255
Epoch 264/300, resid Loss: 0.0562 | 0.0255
Epoch 265/300, resid Loss: 0.0562 | 0.0255
Epoch 266/300, resid Loss: 0.0562 | 0.0255
Epoch 267/300, resid Loss: 0.0562 | 0.0255
Epoch 268/300, resid Loss: 0.0562 | 0.0255
Epoch 269/300, resid Loss: 0.0562 | 0.0255
Epoch 270/300, resid Loss: 0.0562 | 0.0255
Epoch 271/300, resid Loss: 0.0562 | 0.0255
Epoch 272/300, resid Loss: 0.0562 | 0.0255
Epoch 273/300, resid Loss: 0.0562 | 0.0255
Epoch 274/300, resid Loss: 0.0562 | 0.0255
Epoch 275/300, resid Loss: 0.0562 | 0.0255
Epoch 276/300, resid Loss: 0.0562 | 0.0255
Epoch 277/300, resid Loss: 0.0562 | 0.0255
Epoch 278/300, resid Loss: 0.0562 | 0.0255
Epoch 279/300, resid Loss: 0.0562 | 0.0255
Epoch 280/300, resid Loss: 0.0562 | 0.0255
Epoch 281/300, resid Loss: 0.0562 | 0.0255
Epoch 282/300, resid Loss: 0.0562 | 0.0255
Epoch 283/300, resid Loss: 0.0562 | 0.0255
Epoch 284/300, resid Loss: 0.0562 | 0.0255
Epoch 285/300, resid Loss: 0.0562 | 0.0255
Epoch 286/300, resid Loss: 0.0562 | 0.0255
Epoch 287/300, resid Loss: 0.0562 | 0.0255
Epoch 288/300, resid Loss: 0.0562 | 0.0255
Epoch 289/300, resid Loss: 0.0562 | 0.0255
Epoch 290/300, resid Loss: 0.0562 | 0.0255
Epoch 291/300, resid Loss: 0.0562 | 0.0255
Epoch 292/300, resid Loss: 0.0562 | 0.0255
Epoch 293/300, resid Loss: 0.0562 | 0.0255
Epoch 294/300, resid Loss: 0.0562 | 0.0255
Epoch 295/300, resid Loss: 0.0562 | 0.0255
Epoch 296/300, resid Loss: 0.0562 | 0.0255
Epoch 297/300, resid Loss: 0.0562 | 0.0255
Epoch 298/300, resid Loss: 0.0562 | 0.0255
Epoch 299/300, resid Loss: 0.0562 | 0.0255
Epoch 300/300, resid Loss: 0.0562 | 0.0255
Runtime (seconds): 1628.243089914322
0.0008575358526368714
[218.45541]
[-0.23803939]
[-3.252703]
[3.440359]
[0.30068964]
[11.249765]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 24.74567575403489
RMSE: 4.9745025634765625
MAE: 4.9745025634765625
R-squared: nan
[229.95549]
