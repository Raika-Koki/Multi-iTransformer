ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-08 13:08:16,765][0m A new study created in memory with name: no-name-5cffed53-ec29-4688-9b1d-bd5732323038[0m
[32m[I 2025-02-08 13:12:32,590][0m Trial 0 finished with value: 0.5435553675727226 and parameters: {'observation_period_num': 231, 'train_rates': 0.7851728504128653, 'learning_rate': 0.0003320870235149974, 'batch_size': 54, 'step_size': 7, 'gamma': 0.8027675670662362}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:13:16,734][0m Trial 1 finished with value: 0.7395526062811317 and parameters: {'observation_period_num': 45, 'train_rates': 0.8768040006356905, 'learning_rate': 2.0074844054500888e-05, 'batch_size': 235, 'step_size': 9, 'gamma': 0.8979773051164688}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:15:58,346][0m Trial 2 finished with value: 1.0603037387785683 and parameters: {'observation_period_num': 173, 'train_rates': 0.6395870092738573, 'learning_rate': 0.00019126116957690712, 'batch_size': 50, 'step_size': 8, 'gamma': 0.8642349645700727}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:16:36,158][0m Trial 3 finished with value: 0.8392526964503011 and parameters: {'observation_period_num': 39, 'train_rates': 0.782633653388313, 'learning_rate': 3.776347037993432e-05, 'batch_size': 193, 'step_size': 15, 'gamma': 0.854610035144069}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:17:07,802][0m Trial 4 finished with value: 1.0325421466097713 and parameters: {'observation_period_num': 36, 'train_rates': 0.6152158897203874, 'learning_rate': 5.4403091785886096e-05, 'batch_size': 188, 'step_size': 10, 'gamma': 0.9286440180467457}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:17:50,200][0m Trial 5 finished with value: 0.8135684742144361 and parameters: {'observation_period_num': 55, 'train_rates': 0.620473147873385, 'learning_rate': 9.462897728028551e-05, 'batch_size': 198, 'step_size': 15, 'gamma': 0.9190952982966891}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:18:16,296][0m Trial 6 finished with value: 0.7189673581077565 and parameters: {'observation_period_num': 19, 'train_rates': 0.7518547080048157, 'learning_rate': 0.0006431399637007542, 'batch_size': 181, 'step_size': 6, 'gamma': 0.7582488503437479}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:21:43,554][0m Trial 7 finished with value: 1.1147445365912194 and parameters: {'observation_period_num': 172, 'train_rates': 0.7872037621416781, 'learning_rate': 2.446057185654013e-06, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8618453729135116}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:25:13,394][0m Trial 8 finished with value: 0.9370774067582691 and parameters: {'observation_period_num': 207, 'train_rates': 0.6953422839076172, 'learning_rate': 0.0001466651176842944, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8136589012399724}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:27:24,707][0m Trial 9 finished with value: 0.7139427729360812 and parameters: {'observation_period_num': 78, 'train_rates': 0.7539492419211553, 'learning_rate': 0.00012033150069623244, 'batch_size': 31, 'step_size': 12, 'gamma': 0.756275157442976}. Best is trial 0 with value: 0.5435553675727226.[0m
[32m[I 2025-02-08 13:32:29,488][0m Trial 10 finished with value: 0.5006659626960754 and parameters: {'observation_period_num': 234, 'train_rates': 0.9785924043752284, 'learning_rate': 0.0008737358228069066, 'batch_size': 104, 'step_size': 2, 'gamma': 0.9762108529224213}. Best is trial 10 with value: 0.5006659626960754.[0m
[32m[I 2025-02-08 13:37:30,998][0m Trial 11 finished with value: 0.2891277074813843 and parameters: {'observation_period_num': 234, 'train_rates': 0.9674763563284569, 'learning_rate': 0.00046851255887749894, 'batch_size': 108, 'step_size': 1, 'gamma': 0.9831612704017578}. Best is trial 11 with value: 0.2891277074813843.[0m
[32m[I 2025-02-08 13:42:40,575][0m Trial 12 finished with value: 0.3190086781978607 and parameters: {'observation_period_num': 237, 'train_rates': 0.9826420625905944, 'learning_rate': 0.0007536835801336504, 'batch_size': 112, 'step_size': 1, 'gamma': 0.9853758437938133}. Best is trial 11 with value: 0.2891277074813843.[0m
[32m[I 2025-02-08 13:45:11,374][0m Trial 13 finished with value: 0.7454578280448914 and parameters: {'observation_period_num': 131, 'train_rates': 0.9884730395515061, 'learning_rate': 8.487991864942911e-06, 'batch_size': 122, 'step_size': 1, 'gamma': 0.9861842740094963}. Best is trial 11 with value: 0.2891277074813843.[0m
[32m[I 2025-02-08 13:50:25,232][0m Trial 14 finished with value: 0.2577028463245819 and parameters: {'observation_period_num': 249, 'train_rates': 0.9045088001389682, 'learning_rate': 0.00040864548539001173, 'batch_size': 96, 'step_size': 4, 'gamma': 0.9565861277956662}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 13:54:20,472][0m Trial 15 finished with value: 0.2851264866971478 and parameters: {'observation_period_num': 198, 'train_rates': 0.8948235363003316, 'learning_rate': 0.000302967918992926, 'batch_size': 86, 'step_size': 4, 'gamma': 0.9505239229031516}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 13:57:55,429][0m Trial 16 finished with value: 0.2879464651675934 and parameters: {'observation_period_num': 190, 'train_rates': 0.8813706322948461, 'learning_rate': 0.00027990005282140553, 'batch_size': 154, 'step_size': 4, 'gamma': 0.9337074372364351}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:00:04,741][0m Trial 17 finished with value: 0.5758221145948706 and parameters: {'observation_period_num': 122, 'train_rates': 0.8798796926442348, 'learning_rate': 1.5862362570123876e-05, 'batch_size': 80, 'step_size': 4, 'gamma': 0.9478439101046203}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:02:33,741][0m Trial 18 finished with value: 0.4575078281909908 and parameters: {'observation_period_num': 133, 'train_rates': 0.9226539440476333, 'learning_rate': 6.834597020532631e-05, 'batch_size': 145, 'step_size': 4, 'gamma': 0.9000791646912951}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:07:38,653][0m Trial 19 finished with value: 1.269345670211606 and parameters: {'observation_period_num': 251, 'train_rates': 0.8485693360552624, 'learning_rate': 1.4712646101081887e-06, 'batch_size': 84, 'step_size': 5, 'gamma': 0.9494630656400773}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:11:51,063][0m Trial 20 finished with value: 0.27914588482756364 and parameters: {'observation_period_num': 204, 'train_rates': 0.9311702559114637, 'learning_rate': 0.00028145443904080853, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8931462433182853}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:16:02,252][0m Trial 21 finished with value: 0.293262300033902 and parameters: {'observation_period_num': 206, 'train_rates': 0.9220804565329447, 'learning_rate': 0.0002298476339691428, 'batch_size': 91, 'step_size': 3, 'gamma': 0.8931576897062192}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:18:58,391][0m Trial 22 finished with value: 0.658454987368766 and parameters: {'observation_period_num': 165, 'train_rates': 0.8312580875315223, 'learning_rate': 0.0003936514013742687, 'batch_size': 69, 'step_size': 3, 'gamma': 0.9615547835329065}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:23:08,682][0m Trial 23 finished with value: 0.2603639138643968 and parameters: {'observation_period_num': 208, 'train_rates': 0.9348886060397, 'learning_rate': 0.00018179226210105216, 'batch_size': 132, 'step_size': 5, 'gamma': 0.9204655567307871}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:27:33,831][0m Trial 24 finished with value: 0.8074731000445106 and parameters: {'observation_period_num': 215, 'train_rates': 0.9361118276468859, 'learning_rate': 0.0009484087794326741, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8842693821181384}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:32:55,366][0m Trial 25 finished with value: 0.514461684671248 and parameters: {'observation_period_num': 252, 'train_rates': 0.940494393581865, 'learning_rate': 8.891559955687509e-05, 'batch_size': 159, 'step_size': 6, 'gamma': 0.8411217794648896}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:36:29,764][0m Trial 26 finished with value: 0.32654839754104614 and parameters: {'observation_period_num': 184, 'train_rates': 0.9511283008049443, 'learning_rate': 0.0001949803641842222, 'batch_size': 167, 'step_size': 3, 'gamma': 0.9198919942260354}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:39:05,462][0m Trial 27 finished with value: 0.5476304761001042 and parameters: {'observation_period_num': 148, 'train_rates': 0.8355905139987494, 'learning_rate': 0.0005754401760742927, 'batch_size': 135, 'step_size': 6, 'gamma': 0.9019218969204044}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:40:53,937][0m Trial 28 finished with value: 0.811135466235715 and parameters: {'observation_period_num': 108, 'train_rates': 0.8960809210202425, 'learning_rate': 4.409185215357211e-05, 'batch_size': 219, 'step_size': 2, 'gamma': 0.8827449584736937}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:45:08,513][0m Trial 29 finished with value: 0.916358232498169 and parameters: {'observation_period_num': 216, 'train_rates': 0.8233561664519817, 'learning_rate': 6.1811775013718125e-06, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8334160667232499}. Best is trial 14 with value: 0.2577028463245819.[0m
[32m[I 2025-02-08 14:49:35,811][0m Trial 30 finished with value: 0.24991622309272105 and parameters: {'observation_period_num': 221, 'train_rates': 0.9050344472256345, 'learning_rate': 0.00038486024665412757, 'batch_size': 119, 'step_size': 5, 'gamma': 0.9129096741858707}. Best is trial 30 with value: 0.24991622309272105.[0m
[32m[I 2025-02-08 14:54:07,439][0m Trial 31 finished with value: 0.2598984550971251 and parameters: {'observation_period_num': 220, 'train_rates': 0.9194505508118652, 'learning_rate': 0.00043073310356438367, 'batch_size': 99, 'step_size': 5, 'gamma': 0.915152761209881}. Best is trial 30 with value: 0.24991622309272105.[0m
[32m[I 2025-02-08 14:58:38,018][0m Trial 32 finished with value: 0.2833614631026399 and parameters: {'observation_period_num': 222, 'train_rates': 0.9070093855489781, 'learning_rate': 0.00043578465749164396, 'batch_size': 118, 'step_size': 5, 'gamma': 0.9136747848809917}. Best is trial 30 with value: 0.24991622309272105.[0m
[32m[I 2025-02-08 15:03:13,106][0m Trial 33 finished with value: 0.3344003800823036 and parameters: {'observation_period_num': 230, 'train_rates': 0.8688553115409088, 'learning_rate': 0.00015932476649300133, 'batch_size': 97, 'step_size': 8, 'gamma': 0.9645594052046428}. Best is trial 30 with value: 0.24991622309272105.[0m
[32m[I 2025-02-08 15:08:05,686][0m Trial 34 finished with value: 0.5658028550557236 and parameters: {'observation_period_num': 245, 'train_rates': 0.8519742964085981, 'learning_rate': 2.7807638527916346e-05, 'batch_size': 141, 'step_size': 9, 'gamma': 0.9375404520668993}. Best is trial 30 with value: 0.24991622309272105.[0m
[32m[I 2025-02-08 15:11:39,419][0m Trial 35 finished with value: 0.2512433970574852 and parameters: {'observation_period_num': 187, 'train_rates': 0.9500638783155136, 'learning_rate': 0.0003874052098239676, 'batch_size': 120, 'step_size': 7, 'gamma': 0.913052174214587}. Best is trial 30 with value: 0.24991622309272105.[0m
[32m[I 2025-02-08 15:14:51,355][0m Trial 36 finished with value: 0.24341853524817794 and parameters: {'observation_period_num': 156, 'train_rates': 0.9567152552287824, 'learning_rate': 0.00038855783057221836, 'batch_size': 58, 'step_size': 7, 'gamma': 0.9077498280065192}. Best is trial 36 with value: 0.24341853524817794.[0m
[32m[I 2025-02-08 15:18:11,510][0m Trial 37 finished with value: 0.3724730197838911 and parameters: {'observation_period_num': 158, 'train_rates': 0.9547847484608801, 'learning_rate': 0.000521646261893918, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8784503322617322}. Best is trial 36 with value: 0.24341853524817794.[0m
[32m[I 2025-02-08 15:21:52,858][0m Trial 38 finished with value: 0.22543504471714432 and parameters: {'observation_period_num': 183, 'train_rates': 0.9602507347860669, 'learning_rate': 0.00010339503593823275, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9358239932702179}. Best is trial 38 with value: 0.22543504471714432.[0m
[32m[I 2025-02-08 15:25:30,075][0m Trial 39 finished with value: 0.2479849473351524 and parameters: {'observation_period_num': 180, 'train_rates': 0.9623032323961539, 'learning_rate': 8.027243502691337e-05, 'batch_size': 64, 'step_size': 12, 'gamma': 0.9357656584932765}. Best is trial 38 with value: 0.22543504471714432.[0m
[32m[I 2025-02-08 15:29:55,594][0m Trial 40 finished with value: 0.21061379611492156 and parameters: {'observation_period_num': 156, 'train_rates': 0.9681098663524768, 'learning_rate': 8.10918956356609e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.9361337095098745}. Best is trial 40 with value: 0.21061379611492156.[0m
[32m[I 2025-02-08 15:34:21,344][0m Trial 41 finished with value: 0.2253826567903161 and parameters: {'observation_period_num': 149, 'train_rates': 0.9658902534655361, 'learning_rate': 6.543621549996197e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.9342360400971811}. Best is trial 40 with value: 0.21061379611492156.[0m
[32m[I 2025-02-08 15:37:25,983][0m Trial 42 finished with value: 0.21005300638523508 and parameters: {'observation_period_num': 145, 'train_rates': 0.9668006007311533, 'learning_rate': 6.123191422353913e-05, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9390324819491779}. Best is trial 42 with value: 0.21005300638523508.[0m
[32m[I 2025-02-08 15:41:02,187][0m Trial 43 finished with value: 1.0830291089952968 and parameters: {'observation_period_num': 147, 'train_rates': 0.6743030535894343, 'learning_rate': 2.4997186113588287e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9700729949156848}. Best is trial 42 with value: 0.21005300638523508.[0m
[32m[I 2025-02-08 15:43:46,613][0m Trial 44 finished with value: 0.2219619850317637 and parameters: {'observation_period_num': 105, 'train_rates': 0.9685544637932922, 'learning_rate': 4.781793649838532e-05, 'batch_size': 30, 'step_size': 14, 'gamma': 0.93900319464007}. Best is trial 42 with value: 0.21005300638523508.[0m
[32m[I 2025-02-08 15:46:05,736][0m Trial 45 finished with value: 0.15879644453525543 and parameters: {'observation_period_num': 93, 'train_rates': 0.9893290932556534, 'learning_rate': 4.884421633530267e-05, 'batch_size': 36, 'step_size': 14, 'gamma': 0.7741557301391672}. Best is trial 45 with value: 0.15879644453525543.[0m
[32m[I 2025-02-08 15:48:29,097][0m Trial 46 finished with value: 0.15742777999151836 and parameters: {'observation_period_num': 91, 'train_rates': 0.9847509072588054, 'learning_rate': 5.281710633533444e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.7788667762303993}. Best is trial 46 with value: 0.15742777999151836.[0m
[32m[I 2025-02-08 15:50:48,776][0m Trial 47 finished with value: 0.15096546884845286 and parameters: {'observation_period_num': 85, 'train_rates': 0.9825389747955425, 'learning_rate': 4.382536360015831e-05, 'batch_size': 36, 'step_size': 15, 'gamma': 0.77611872403775}. Best is trial 47 with value: 0.15096546884845286.[0m
[32m[I 2025-02-08 15:52:46,643][0m Trial 48 finished with value: 0.2829543352127075 and parameters: {'observation_period_num': 76, 'train_rates': 0.9881040145976735, 'learning_rate': 1.6836727050924124e-05, 'batch_size': 43, 'step_size': 15, 'gamma': 0.7733126340947637}. Best is trial 47 with value: 0.15096546884845286.[0m
[32m[I 2025-02-08 15:55:09,915][0m Trial 49 finished with value: 0.16597952899234047 and parameters: {'observation_period_num': 80, 'train_rates': 0.9798808549357453, 'learning_rate': 3.132168331976742e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.787259082339884}. Best is trial 47 with value: 0.15096546884845286.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-08 15:55:09,922][0m A new study created in memory with name: no-name-a6323aed-bccb-4d55-8d12-9a51402ffc53[0m
[32m[I 2025-02-08 15:55:40,403][0m Trial 0 finished with value: 0.4577423861588944 and parameters: {'observation_period_num': 9, 'train_rates': 0.8210942383434638, 'learning_rate': 0.000164900863067287, 'batch_size': 155, 'step_size': 2, 'gamma': 0.9430672567495528}. Best is trial 0 with value: 0.4577423861588944.[0m
[32m[I 2025-02-08 15:59:19,672][0m Trial 1 finished with value: 1.328722596168518 and parameters: {'observation_period_num': 186, 'train_rates': 0.9691698926051382, 'learning_rate': 2.4385684924908055e-06, 'batch_size': 254, 'step_size': 14, 'gamma': 0.9878913129030168}. Best is trial 0 with value: 0.4577423861588944.[0m
[32m[I 2025-02-08 16:02:29,800][0m Trial 2 finished with value: 0.43654508668662123 and parameters: {'observation_period_num': 178, 'train_rates': 0.8296474840031849, 'learning_rate': 7.033257175692786e-05, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8875338060857993}. Best is trial 2 with value: 0.43654508668662123.[0m
[32m[I 2025-02-08 16:07:47,616][0m Trial 3 finished with value: 0.3581196218729019 and parameters: {'observation_period_num': 251, 'train_rates': 0.9187642663016351, 'learning_rate': 3.8676675765033646e-05, 'batch_size': 115, 'step_size': 12, 'gamma': 0.9808586219273213}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:09:14,379][0m Trial 4 finished with value: 1.2385302458181742 and parameters: {'observation_period_num': 85, 'train_rates': 0.6689239207289375, 'learning_rate': 0.0001451577231132225, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9463205247226263}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:13:27,821][0m Trial 5 finished with value: 0.6582157294710571 and parameters: {'observation_period_num': 219, 'train_rates': 0.8462977184837612, 'learning_rate': 2.1534247422025326e-05, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8352607996054734}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:16:33,200][0m Trial 6 finished with value: 0.6042708258989246 and parameters: {'observation_period_num': 175, 'train_rates': 0.8764529504703937, 'learning_rate': 4.3405811473191263e-05, 'batch_size': 162, 'step_size': 4, 'gamma': 0.8895908361798786}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:17:55,416][0m Trial 7 finished with value: 1.3566169611045293 and parameters: {'observation_period_num': 92, 'train_rates': 0.7265835737854373, 'learning_rate': 4.189460834897893e-06, 'batch_size': 141, 'step_size': 10, 'gamma': 0.9523148270359363}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:21:04,435][0m Trial 8 finished with value: 0.5972410181294316 and parameters: {'observation_period_num': 171, 'train_rates': 0.87640142891243, 'learning_rate': 0.0006461082935942807, 'batch_size': 106, 'step_size': 5, 'gamma': 0.9835893551685723}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:25:30,436][0m Trial 9 finished with value: 1.2673600912094116 and parameters: {'observation_period_num': 217, 'train_rates': 0.9761810923399143, 'learning_rate': 3.7008813475582263e-06, 'batch_size': 200, 'step_size': 1, 'gamma': 0.9811552578040454}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:30:50,354][0m Trial 10 finished with value: 0.8330277876687846 and parameters: {'observation_period_num': 251, 'train_rates': 0.7456147179992257, 'learning_rate': 1.2753363949751934e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7525199541081348}. Best is trial 3 with value: 0.3581196218729019.[0m
[32m[I 2025-02-08 16:33:04,381][0m Trial 11 finished with value: 0.3074548274408782 and parameters: {'observation_period_num': 128, 'train_rates': 0.9190438619632526, 'learning_rate': 7.914100292151148e-05, 'batch_size': 100, 'step_size': 15, 'gamma': 0.8684921051485527}. Best is trial 11 with value: 0.3074548274408782.[0m
[32m[I 2025-02-08 16:35:14,709][0m Trial 12 finished with value: 0.5536170192062855 and parameters: {'observation_period_num': 119, 'train_rates': 0.9296665814454511, 'learning_rate': 0.0007047452149143837, 'batch_size': 85, 'step_size': 11, 'gamma': 0.8276226929491232}. Best is trial 11 with value: 0.3074548274408782.[0m
[32m[I 2025-02-08 16:36:08,856][0m Trial 13 finished with value: 0.8899538516998291 and parameters: {'observation_period_num': 53, 'train_rates': 0.927871579910314, 'learning_rate': 9.909145562967283e-06, 'batch_size': 195, 'step_size': 15, 'gamma': 0.8542048233815318}. Best is trial 11 with value: 0.3074548274408782.[0m
[32m[I 2025-02-08 16:38:10,924][0m Trial 14 finished with value: 1.0088805546614237 and parameters: {'observation_period_num': 138, 'train_rates': 0.6070897719845344, 'learning_rate': 0.00014100825678526173, 'batch_size': 99, 'step_size': 12, 'gamma': 0.7936169827477546}. Best is trial 11 with value: 0.3074548274408782.[0m
[32m[I 2025-02-08 16:40:54,592][0m Trial 15 finished with value: 0.2942405414911127 and parameters: {'observation_period_num': 140, 'train_rates': 0.9103038413176705, 'learning_rate': 5.335166030865675e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9126439010538719}. Best is trial 15 with value: 0.2942405414911127.[0m
[32m[I 2025-02-08 16:43:21,359][0m Trial 16 finished with value: 0.7114360501989723 and parameters: {'observation_period_num': 135, 'train_rates': 0.7733217668831874, 'learning_rate': 0.0002472279203172797, 'batch_size': 53, 'step_size': 7, 'gamma': 0.9103672377170733}. Best is trial 15 with value: 0.2942405414911127.[0m
[32m[I 2025-02-08 16:45:21,358][0m Trial 17 finished with value: 0.20554834604263306 and parameters: {'observation_period_num': 107, 'train_rates': 0.9883859765226244, 'learning_rate': 7.145178281004205e-05, 'batch_size': 81, 'step_size': 5, 'gamma': 0.9110048568253512}. Best is trial 17 with value: 0.20554834604263306.[0m
[32m[I 2025-02-08 16:48:13,714][0m Trial 18 finished with value: 0.21505073927066945 and parameters: {'observation_period_num': 59, 'train_rates': 0.9815943314241345, 'learning_rate': 0.00030370257590868607, 'batch_size': 29, 'step_size': 6, 'gamma': 0.9182004337973942}. Best is trial 17 with value: 0.20554834604263306.[0m
[32m[I 2025-02-08 16:53:19,062][0m Trial 19 finished with value: 0.2612586959203084 and parameters: {'observation_period_num': 43, 'train_rates': 0.9745631032300903, 'learning_rate': 0.0003893345796131041, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9230586489917675}. Best is trial 17 with value: 0.20554834604263306.[0m
[32m[I 2025-02-08 16:55:50,397][0m Trial 20 finished with value: 0.6888492261541301 and parameters: {'observation_period_num': 75, 'train_rates': 0.9800747420289991, 'learning_rate': 0.0009718167698184749, 'batch_size': 33, 'step_size': 6, 'gamma': 0.89338942554649}. Best is trial 17 with value: 0.20554834604263306.[0m
[32m[I 2025-02-08 16:59:48,948][0m Trial 21 finished with value: 0.17699102063973746 and parameters: {'observation_period_num': 27, 'train_rates': 0.9877336907855799, 'learning_rate': 0.00027967116655799586, 'batch_size': 21, 'step_size': 3, 'gamma': 0.9235144343912718}. Best is trial 21 with value: 0.17699102063973746.[0m
[32m[I 2025-02-08 17:00:54,989][0m Trial 22 finished with value: 0.2629027014805211 and parameters: {'observation_period_num': 25, 'train_rates': 0.9508902209381803, 'learning_rate': 0.00030989277219867987, 'batch_size': 77, 'step_size': 3, 'gamma': 0.9315071595279127}. Best is trial 21 with value: 0.17699102063973746.[0m
[32m[I 2025-02-08 17:03:08,356][0m Trial 23 finished with value: 0.2812200004379906 and parameters: {'observation_period_num': 60, 'train_rates': 0.8848991004139317, 'learning_rate': 8.68697613908385e-05, 'batch_size': 35, 'step_size': 3, 'gamma': 0.9585989190127128}. Best is trial 21 with value: 0.17699102063973746.[0m
[32m[I 2025-02-08 17:05:33,476][0m Trial 24 finished with value: 0.39027525561636894 and parameters: {'observation_period_num': 109, 'train_rates': 0.9392121430160854, 'learning_rate': 0.0004208402213917052, 'batch_size': 33, 'step_size': 5, 'gamma': 0.9031768254425958}. Best is trial 21 with value: 0.17699102063973746.[0m
[32m[I 2025-02-08 17:06:43,487][0m Trial 25 finished with value: 0.15648534893989563 and parameters: {'observation_period_num': 32, 'train_rates': 0.988444807986931, 'learning_rate': 0.00019833952909232067, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8751645873885653}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:07:46,848][0m Trial 26 finished with value: 0.2723519787284898 and parameters: {'observation_period_num': 33, 'train_rates': 0.9450418335705734, 'learning_rate': 0.00013409148384019843, 'batch_size': 79, 'step_size': 9, 'gamma': 0.8696523801030446}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:09:08,690][0m Trial 27 finished with value: 1.0907821655273438 and parameters: {'observation_period_num': 6, 'train_rates': 0.988177074328675, 'learning_rate': 1.1169884083066108e-06, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8375296894646849}. Best is trial 25 with value: 0.15648534893989563.[0m
Early stopping at epoch 54
[32m[I 2025-02-08 17:09:30,794][0m Trial 28 finished with value: 1.5863176885160428 and parameters: {'observation_period_num': 26, 'train_rates': 0.8948692954195998, 'learning_rate': 1.9171971616046574e-05, 'batch_size': 126, 'step_size': 1, 'gamma': 0.8078498571638517}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:10:02,754][0m Trial 29 finished with value: 0.5084059150426979 and parameters: {'observation_period_num': 17, 'train_rates': 0.8480994305127694, 'learning_rate': 0.00021488526624709704, 'batch_size': 150, 'step_size': 2, 'gamma': 0.9315811229612078}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:11:56,963][0m Trial 30 finished with value: 0.37717093899846077 and parameters: {'observation_period_num': 102, 'train_rates': 0.9523625074109183, 'learning_rate': 9.810192116563405e-05, 'batch_size': 85, 'step_size': 5, 'gamma': 0.8572969601336252}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:13:44,985][0m Trial 31 finished with value: 0.1665625274181366 and parameters: {'observation_period_num': 69, 'train_rates': 0.986335800106204, 'learning_rate': 0.00020368414348851627, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8785993989337706}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:15:25,282][0m Trial 32 finished with value: 0.2306273912912921 and parameters: {'observation_period_num': 75, 'train_rates': 0.9607027060005028, 'learning_rate': 0.00019605527140230267, 'batch_size': 51, 'step_size': 7, 'gamma': 0.8886172536843306}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:17:16,392][0m Trial 33 finished with value: 0.3458191603422165 and parameters: {'observation_period_num': 39, 'train_rates': 0.9570401104896569, 'learning_rate': 0.0004361901063311915, 'batch_size': 45, 'step_size': 3, 'gamma': 0.878036657873971}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:18:21,578][0m Trial 34 finished with value: 0.6470499335658932 and parameters: {'observation_period_num': 68, 'train_rates': 0.8057365561417558, 'learning_rate': 5.7793008593432525e-05, 'batch_size': 250, 'step_size': 6, 'gamma': 0.9019880987488589}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:19:33,233][0m Trial 35 finished with value: 0.23637494444847107 and parameters: {'observation_period_num': 48, 'train_rates': 0.9890289482482778, 'learning_rate': 2.9741883776233316e-05, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9687159154557884}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:21:10,744][0m Trial 36 finished with value: 0.24693747047040102 and parameters: {'observation_period_num': 87, 'train_rates': 0.9034102528147001, 'learning_rate': 0.00012541617838946685, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9376499257663856}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:23:32,486][0m Trial 37 finished with value: 0.9483442140950097 and parameters: {'observation_period_num': 153, 'train_rates': 0.6917622251400346, 'learning_rate': 0.0006086161572527723, 'batch_size': 92, 'step_size': 2, 'gamma': 0.8556681231406216}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:24:17,419][0m Trial 38 finished with value: 0.2807457745075226 and parameters: {'observation_period_num': 15, 'train_rates': 0.9620862985019908, 'learning_rate': 0.0001628549039681086, 'batch_size': 117, 'step_size': 6, 'gamma': 0.8721475623996238}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:26:22,388][0m Trial 39 finished with value: 0.36939532625675203 and parameters: {'observation_period_num': 106, 'train_rates': 0.8687320692574153, 'learning_rate': 3.184887257682143e-05, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8998743045202382}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:27:46,093][0m Trial 40 finished with value: 0.3107535365571831 and parameters: {'observation_period_num': 70, 'train_rates': 0.9318779987640138, 'learning_rate': 9.194954773282558e-05, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8819811866476356}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:30:58,137][0m Trial 41 finished with value: 0.2062551294054304 and parameters: {'observation_period_num': 60, 'train_rates': 0.985740386612308, 'learning_rate': 0.00027206572722457254, 'batch_size': 26, 'step_size': 6, 'gamma': 0.9188494735050725}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:34:23,378][0m Trial 42 finished with value: 0.2479552924633026 and parameters: {'observation_period_num': 94, 'train_rates': 0.9681287046090687, 'learning_rate': 0.00024451157020283573, 'batch_size': 24, 'step_size': 7, 'gamma': 0.9203545267410076}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:36:28,220][0m Trial 43 finished with value: 0.7556681420543406 and parameters: {'observation_period_num': 32, 'train_rates': 0.965513541105897, 'learning_rate': 0.0005130834164959586, 'batch_size': 40, 'step_size': 4, 'gamma': 0.9473262492501724}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:38:08,445][0m Trial 44 finished with value: 0.33549491975499296 and parameters: {'observation_period_num': 81, 'train_rates': 0.9398121393867738, 'learning_rate': 0.0003213051186035226, 'batch_size': 53, 'step_size': 8, 'gamma': 0.9569874374328404}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:41:41,913][0m Trial 45 finished with value: 1.0117474667472985 and parameters: {'observation_period_num': 50, 'train_rates': 0.9098066681427686, 'learning_rate': 0.000972860592732923, 'batch_size': 22, 'step_size': 5, 'gamma': 0.9283648325075282}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:42:45,171][0m Trial 46 finished with value: 0.38198623061180115 and parameters: {'observation_period_num': 60, 'train_rates': 0.988512252295461, 'learning_rate': 0.00017985584729986824, 'batch_size': 181, 'step_size': 3, 'gamma': 0.8446854760537489}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:44:06,071][0m Trial 47 finished with value: 0.8876981076919358 and parameters: {'observation_period_num': 23, 'train_rates': 0.6143670059050864, 'learning_rate': 0.00012524012908199662, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9072821825921783}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:46:09,718][0m Trial 48 finished with value: 0.4033139851028563 and parameters: {'observation_period_num': 121, 'train_rates': 0.9221493726220982, 'learning_rate': 4.886369960194184e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.8904906950937603}. Best is trial 25 with value: 0.15648534893989563.[0m
[32m[I 2025-02-08 17:47:07,281][0m Trial 49 finished with value: 0.2161957025527954 and parameters: {'observation_period_num': 36, 'train_rates': 0.972143761788901, 'learning_rate': 0.0002544471622527665, 'batch_size': 90, 'step_size': 8, 'gamma': 0.8222674264813737}. Best is trial 25 with value: 0.15648534893989563.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-08 17:47:07,458][0m A new study created in memory with name: no-name-2e36c854-95a8-4b4a-b9b4-176885c18838[0m
[32m[I 2025-02-08 17:50:43,478][0m Trial 0 finished with value: 0.33429282903671265 and parameters: {'observation_period_num': 183, 'train_rates': 0.9739909517459032, 'learning_rate': 0.0007686917514004862, 'batch_size': 184, 'step_size': 10, 'gamma': 0.7663662431735137}. Best is trial 0 with value: 0.33429282903671265.[0m
[32m[I 2025-02-08 17:52:15,169][0m Trial 1 finished with value: 0.3003624757895103 and parameters: {'observation_period_num': 90, 'train_rates': 0.8821604959258882, 'learning_rate': 0.0003533245164660738, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8695221176694607}. Best is trial 1 with value: 0.3003624757895103.[0m
Early stopping at epoch 86
[32m[I 2025-02-08 17:54:04,306][0m Trial 2 finished with value: 1.1801141292759867 and parameters: {'observation_period_num': 97, 'train_rates': 0.9540711572659769, 'learning_rate': 1.1997375389896226e-05, 'batch_size': 41, 'step_size': 1, 'gamma': 0.874107423116377}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 17:55:28,625][0m Trial 3 finished with value: 1.1491026057888138 and parameters: {'observation_period_num': 98, 'train_rates': 0.6546399087957011, 'learning_rate': 0.0003087697345495952, 'batch_size': 112, 'step_size': 15, 'gamma': 0.9727588218385139}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 17:57:38,070][0m Trial 4 finished with value: 0.6421477262962592 and parameters: {'observation_period_num': 132, 'train_rates': 0.7737838272552892, 'learning_rate': 0.00046137113644962875, 'batch_size': 205, 'step_size': 9, 'gamma': 0.9494415469399682}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 18:00:53,023][0m Trial 5 finished with value: 0.8444473159880223 and parameters: {'observation_period_num': 200, 'train_rates': 0.6796788135282616, 'learning_rate': 0.00011991955293416066, 'batch_size': 173, 'step_size': 5, 'gamma': 0.9120392265225671}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 18:02:10,886][0m Trial 6 finished with value: 0.9639955765629417 and parameters: {'observation_period_num': 73, 'train_rates': 0.7803495222181538, 'learning_rate': 0.0007226735443595295, 'batch_size': 61, 'step_size': 8, 'gamma': 0.8595609721457946}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 18:06:14,357][0m Trial 7 finished with value: 0.7476491781773867 and parameters: {'observation_period_num': 216, 'train_rates': 0.8381601315613748, 'learning_rate': 3.6207467777331146e-05, 'batch_size': 215, 'step_size': 6, 'gamma': 0.8973019551158421}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 18:08:19,940][0m Trial 8 finished with value: 1.0023893247235496 and parameters: {'observation_period_num': 145, 'train_rates': 0.6607515291230582, 'learning_rate': 0.00010008196269590903, 'batch_size': 234, 'step_size': 11, 'gamma': 0.9171804186072491}. Best is trial 1 with value: 0.3003624757895103.[0m
[32m[I 2025-02-08 18:13:20,507][0m Trial 9 finished with value: 0.27552669171024774 and parameters: {'observation_period_num': 243, 'train_rates': 0.9062894231153085, 'learning_rate': 0.00018272128990178796, 'batch_size': 143, 'step_size': 5, 'gamma': 0.9459631457666899}. Best is trial 9 with value: 0.27552669171024774.[0m
Early stopping at epoch 46
[32m[I 2025-02-08 18:15:43,027][0m Trial 10 finished with value: 2.5673396083670603 and parameters: {'observation_period_num': 246, 'train_rates': 0.8952395740887822, 'learning_rate': 1.4354378151727652e-06, 'batch_size': 112, 'step_size': 1, 'gamma': 0.8072624890111892}. Best is trial 9 with value: 0.27552669171024774.[0m
[32m[I 2025-02-08 18:16:16,825][0m Trial 11 finished with value: 0.27505080458305614 and parameters: {'observation_period_num': 27, 'train_rates': 0.8920857105561927, 'learning_rate': 0.00014082826431644955, 'batch_size': 147, 'step_size': 15, 'gamma': 0.8344222706750334}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:16:58,515][0m Trial 12 finished with value: 0.6969341977658975 and parameters: {'observation_period_num': 37, 'train_rates': 0.9141396634765915, 'learning_rate': 4.427497542557534e-05, 'batch_size': 141, 'step_size': 4, 'gamma': 0.8130094316585165}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:17:49,556][0m Trial 13 finished with value: 0.6242183499892239 and parameters: {'observation_period_num': 5, 'train_rates': 0.8288293510287001, 'learning_rate': 6.824857556666977e-06, 'batch_size': 90, 'step_size': 12, 'gamma': 0.9881038118437298}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:20:31,752][0m Trial 14 finished with value: 0.778686012270586 and parameters: {'observation_period_num': 175, 'train_rates': 0.7280267078989575, 'learning_rate': 0.00012848890637665595, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8284452421796151}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:25:16,738][0m Trial 15 finished with value: 0.7056180265370537 and parameters: {'observation_period_num': 247, 'train_rates': 0.84322147856361, 'learning_rate': 1.3233404747171172e-05, 'batch_size': 88, 'step_size': 13, 'gamma': 0.7706335000189246}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:26:09,137][0m Trial 16 finished with value: 0.3069793546020086 and parameters: {'observation_period_num': 47, 'train_rates': 0.9318683730946256, 'learning_rate': 0.00022268950543126442, 'batch_size': 119, 'step_size': 3, 'gamma': 0.9336245732448493}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:29:10,729][0m Trial 17 finished with value: 0.6635868549346924 and parameters: {'observation_period_num': 155, 'train_rates': 0.989138170566501, 'learning_rate': 8.073825806242027e-05, 'batch_size': 254, 'step_size': 3, 'gamma': 0.8403916624274596}. Best is trial 11 with value: 0.27505080458305614.[0m
[32m[I 2025-02-08 18:32:49,852][0m Trial 18 finished with value: 0.23046986884045823 and parameters: {'observation_period_num': 19, 'train_rates': 0.8907311352022653, 'learning_rate': 4.719223100012589e-05, 'batch_size': 21, 'step_size': 13, 'gamma': 0.7856887690503724}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:35:06,903][0m Trial 19 finished with value: 0.8723125345487561 and parameters: {'observation_period_num': 13, 'train_rates': 0.6086517553129912, 'learning_rate': 1.989633190888301e-05, 'batch_size': 25, 'step_size': 13, 'gamma': 0.7881779066931679}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:36:11,525][0m Trial 20 finished with value: 1.0964038496897406 and parameters: {'observation_period_num': 45, 'train_rates': 0.8625513638508228, 'learning_rate': 4.194601742846633e-06, 'batch_size': 72, 'step_size': 14, 'gamma': 0.7524622796982784}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:36:45,287][0m Trial 21 finished with value: 0.42623251323637207 and parameters: {'observation_period_num': 24, 'train_rates': 0.922343393504812, 'learning_rate': 5.469205224604592e-05, 'batch_size': 157, 'step_size': 12, 'gamma': 0.7943898129086941}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:37:45,910][0m Trial 22 finished with value: 0.47696790276864254 and parameters: {'observation_period_num': 65, 'train_rates': 0.8116734967993714, 'learning_rate': 0.00025340259556404617, 'batch_size': 185, 'step_size': 14, 'gamma': 0.84126565285413}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:39:44,545][0m Trial 23 finished with value: 0.29739084218939144 and parameters: {'observation_period_num': 114, 'train_rates': 0.8734304973508737, 'learning_rate': 0.00016964279398589947, 'batch_size': 130, 'step_size': 10, 'gamma': 0.8278608926753563}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:41:05,354][0m Trial 24 finished with value: 0.3068704931127945 and parameters: {'observation_period_num': 73, 'train_rates': 0.9484601890119752, 'learning_rate': 7.34154657753749e-05, 'batch_size': 97, 'step_size': 7, 'gamma': 0.892586958410648}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:42:38,750][0m Trial 25 finished with value: 0.29093154331746574 and parameters: {'observation_period_num': 28, 'train_rates': 0.8933687980888785, 'learning_rate': 2.9285433207812862e-05, 'batch_size': 51, 'step_size': 14, 'gamma': 0.95653271005977}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:47:06,379][0m Trial 26 finished with value: 0.7709083988931444 and parameters: {'observation_period_num': 222, 'train_rates': 0.7534605638919075, 'learning_rate': 0.00017145217355982826, 'batch_size': 24, 'step_size': 5, 'gamma': 0.7869153706764317}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:47:57,605][0m Trial 27 finished with value: 0.4863789268235719 and parameters: {'observation_period_num': 57, 'train_rates': 0.8062917357635214, 'learning_rate': 0.0005165549320394895, 'batch_size': 156, 'step_size': 15, 'gamma': 0.8464512778553266}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:49:53,557][0m Trial 28 finished with value: 0.6988519779881652 and parameters: {'observation_period_num': 119, 'train_rates': 0.8621322484205483, 'learning_rate': 2.284114196575714e-05, 'batch_size': 199, 'step_size': 12, 'gamma': 0.8110125525099642}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:53:23,618][0m Trial 29 finished with value: 0.5734254717826843 and parameters: {'observation_period_num': 178, 'train_rates': 0.9643578994704809, 'learning_rate': 6.10841446311012e-05, 'batch_size': 175, 'step_size': 10, 'gamma': 0.7632616952223288}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:56:17,954][0m Trial 30 finished with value: 0.385303309158637 and parameters: {'observation_period_num': 156, 'train_rates': 0.907313972716668, 'learning_rate': 0.0008387875115822493, 'batch_size': 129, 'step_size': 9, 'gamma': 0.775238574966029}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:57:40,252][0m Trial 31 finished with value: 0.35329971499741075 and parameters: {'observation_period_num': 25, 'train_rates': 0.8910463426124389, 'learning_rate': 1.7025218493021395e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9549145560469579}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 18:59:42,362][0m Trial 32 finished with value: 0.45951763915093563 and parameters: {'observation_period_num': 28, 'train_rates': 0.9381979053910087, 'learning_rate': 8.862189177825949e-06, 'batch_size': 40, 'step_size': 13, 'gamma': 0.9583645582994407}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 19:03:51,327][0m Trial 33 finished with value: 0.3077392037957907 and parameters: {'observation_period_num': 86, 'train_rates': 0.8830613299858219, 'learning_rate': 3.051576912465852e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9335359069472132}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 19:05:22,880][0m Trial 34 finished with value: 0.35853348166507604 and parameters: {'observation_period_num': 14, 'train_rates': 0.8530238805348206, 'learning_rate': 3.4297585784855824e-05, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8791891203608103}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 19:07:29,032][0m Trial 35 finished with value: 0.46992968510595157 and parameters: {'observation_period_num': 52, 'train_rates': 0.9045786999161293, 'learning_rate': 0.0004088224693495479, 'batch_size': 37, 'step_size': 11, 'gamma': 0.9334591094416912}. Best is trial 18 with value: 0.23046986884045823.[0m
[32m[I 2025-02-08 19:08:09,482][0m Trial 36 finished with value: 0.2102983444929123 and parameters: {'observation_period_num': 33, 'train_rates': 0.9747952350201505, 'learning_rate': 0.0001487697016605843, 'batch_size': 149, 'step_size': 15, 'gamma': 0.9660214211664905}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:09:39,616][0m Trial 37 finished with value: 0.2961091995239258 and parameters: {'observation_period_num': 87, 'train_rates': 0.9784918881118766, 'learning_rate': 0.00016557525664071988, 'batch_size': 149, 'step_size': 15, 'gamma': 0.9836552133457833}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:10:28,092][0m Trial 38 finished with value: 0.25727947579141247 and parameters: {'observation_period_num': 5, 'train_rates': 0.9616641612256799, 'learning_rate': 0.00026990473057083447, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8630838994668144}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:11:18,077][0m Trial 39 finished with value: 0.2687356948852539 and parameters: {'observation_period_num': 8, 'train_rates': 0.9594607385742484, 'learning_rate': 0.0002925791901301966, 'batch_size': 103, 'step_size': 11, 'gamma': 0.8622191296483751}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:12:07,866][0m Trial 40 finished with value: 0.23249418169510463 and parameters: {'observation_period_num': 6, 'train_rates': 0.9598643945574735, 'learning_rate': 0.00025913687344644346, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8551136679696413}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:13:10,155][0m Trial 41 finished with value: 0.6364372484385967 and parameters: {'observation_period_num': 9, 'train_rates': 0.9620077116490425, 'learning_rate': 0.0005988340058008828, 'batch_size': 81, 'step_size': 11, 'gamma': 0.8689853879037034}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:14:00,269][0m Trial 42 finished with value: 0.2889875509909221 and parameters: {'observation_period_num': 39, 'train_rates': 0.9522664027694658, 'learning_rate': 0.0003479637516580201, 'batch_size': 103, 'step_size': 12, 'gamma': 0.855357896618758}. Best is trial 36 with value: 0.2102983444929123.[0m
[32m[I 2025-02-08 19:14:46,633][0m Trial 43 finished with value: 0.20141611993312836 and parameters: {'observation_period_num': 5, 'train_rates': 0.9813264464778264, 'learning_rate': 0.00028162592468789047, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8856458573497588}. Best is trial 43 with value: 0.20141611993312836.[0m
[32m[I 2025-02-08 19:15:30,405][0m Trial 44 finished with value: 0.1995111107826233 and parameters: {'observation_period_num': 19, 'train_rates': 0.984291500462808, 'learning_rate': 0.00010235366050669441, 'batch_size': 125, 'step_size': 13, 'gamma': 0.882858428468449}. Best is trial 44 with value: 0.1995111107826233.[0m
[32m[I 2025-02-08 19:16:14,137][0m Trial 45 finished with value: 0.19988976418972015 and parameters: {'observation_period_num': 20, 'train_rates': 0.9899901038507666, 'learning_rate': 9.356006799611088e-05, 'batch_size': 124, 'step_size': 13, 'gamma': 0.9062056734521633}. Best is trial 44 with value: 0.1995111107826233.[0m
[32m[I 2025-02-08 19:16:59,813][0m Trial 46 finished with value: 0.19989262521266937 and parameters: {'observation_period_num': 37, 'train_rates': 0.9888942264798904, 'learning_rate': 9.778259870913336e-05, 'batch_size': 128, 'step_size': 13, 'gamma': 0.9090763115148518}. Best is trial 44 with value: 0.1995111107826233.[0m
[32m[I 2025-02-08 19:17:44,808][0m Trial 47 finished with value: 0.20614536106586456 and parameters: {'observation_period_num': 34, 'train_rates': 0.9880525950903756, 'learning_rate': 9.391453622800339e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9030733824218641}. Best is trial 44 with value: 0.1995111107826233.[0m
[32m[I 2025-02-08 19:18:51,875][0m Trial 48 finished with value: 0.1931181252002716 and parameters: {'observation_period_num': 61, 'train_rates': 0.9856129269576481, 'learning_rate': 9.288695925078009e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9067493424897287}. Best is trial 48 with value: 0.1931181252002716.[0m
[32m[I 2025-02-08 19:19:57,140][0m Trial 49 finished with value: 0.3105105368458495 and parameters: {'observation_period_num': 62, 'train_rates': 0.9324215967101144, 'learning_rate': 0.00010858092860528926, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8834660065566192}. Best is trial 48 with value: 0.1931181252002716.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-08 19:19:57,148][0m A new study created in memory with name: no-name-d7a51cbd-8a9a-47da-b8b1-3390e1fd2f25[0m
[32m[I 2025-02-08 19:23:05,559][0m Trial 0 finished with value: 0.4978041995464443 and parameters: {'observation_period_num': 155, 'train_rates': 0.9191772795481333, 'learning_rate': 2.0093325195478095e-05, 'batch_size': 46, 'step_size': 8, 'gamma': 0.8454262465192453}. Best is trial 0 with value: 0.4978041995464443.[0m
[32m[I 2025-02-08 19:27:26,416][0m Trial 1 finished with value: 0.7322929486443724 and parameters: {'observation_period_num': 231, 'train_rates': 0.8551818552093376, 'learning_rate': 5.981277462228054e-05, 'batch_size': 177, 'step_size': 4, 'gamma': 0.8288379307712197}. Best is trial 0 with value: 0.4978041995464443.[0m
[32m[I 2025-02-08 19:30:18,036][0m Trial 2 finished with value: 0.6503122760568346 and parameters: {'observation_period_num': 160, 'train_rates': 0.8751504315507888, 'learning_rate': 2.5594347194419823e-05, 'batch_size': 207, 'step_size': 6, 'gamma': 0.9215112752733996}. Best is trial 0 with value: 0.4978041995464443.[0m
[32m[I 2025-02-08 19:32:17,349][0m Trial 3 finished with value: 0.27993238644103785 and parameters: {'observation_period_num': 108, 'train_rates': 0.939513366132021, 'learning_rate': 8.969488724943604e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.875972712972599}. Best is trial 3 with value: 0.27993238644103785.[0m
Early stopping at epoch 52
[32m[I 2025-02-08 19:34:29,805][0m Trial 4 finished with value: 2.1823152505434478 and parameters: {'observation_period_num': 215, 'train_rates': 0.8580249330336545, 'learning_rate': 1.0613153903475518e-06, 'batch_size': 108, 'step_size': 1, 'gamma': 0.8177135957997388}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:34:57,991][0m Trial 5 finished with value: 1.2436909002527803 and parameters: {'observation_period_num': 8, 'train_rates': 0.8296242576532392, 'learning_rate': 5.563599522391394e-06, 'batch_size': 168, 'step_size': 11, 'gamma': 0.7689501848383595}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:37:51,521][0m Trial 6 finished with value: 0.8735957475488677 and parameters: {'observation_period_num': 102, 'train_rates': 0.6664755327640106, 'learning_rate': 4.255927503571501e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.7521785500188524}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:40:11,776][0m Trial 7 finished with value: 1.3022278933505023 and parameters: {'observation_period_num': 151, 'train_rates': 0.7460354946950498, 'learning_rate': 4.789841169258224e-06, 'batch_size': 229, 'step_size': 4, 'gamma': 0.9748412405793725}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:43:39,772][0m Trial 8 finished with value: 1.3083260183533032 and parameters: {'observation_period_num': 216, 'train_rates': 0.6453278113638322, 'learning_rate': 2.294063995608362e-05, 'batch_size': 147, 'step_size': 12, 'gamma': 0.7560898636468301}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:45:57,510][0m Trial 9 finished with value: 1.085236095994018 and parameters: {'observation_period_num': 159, 'train_rates': 0.6157290298005254, 'learning_rate': 0.0001462417269728094, 'batch_size': 71, 'step_size': 2, 'gamma': 0.786111768553659}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:47:35,109][0m Trial 10 finished with value: 0.9628180861473083 and parameters: {'observation_period_num': 86, 'train_rates': 0.980304670275714, 'learning_rate': 0.0009647182602987977, 'batch_size': 101, 'step_size': 14, 'gamma': 0.8918296804143326}. Best is trial 3 with value: 0.27993238644103785.[0m
[32m[I 2025-02-08 19:49:39,280][0m Trial 11 finished with value: 0.16284852288663387 and parameters: {'observation_period_num': 60, 'train_rates': 0.9836918505385573, 'learning_rate': 0.00022257386220546392, 'batch_size': 41, 'step_size': 9, 'gamma': 0.86014725732381}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 19:50:49,372][0m Trial 12 finished with value: 0.18079586327075958 and parameters: {'observation_period_num': 47, 'train_rates': 0.9891895058894447, 'learning_rate': 0.00032200108459690466, 'batch_size': 75, 'step_size': 9, 'gamma': 0.8940615145444404}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 19:54:00,220][0m Trial 13 finished with value: 0.449398973892475 and parameters: {'observation_period_num': 40, 'train_rates': 0.9803300729561741, 'learning_rate': 0.0003502234579322945, 'batch_size': 26, 'step_size': 11, 'gamma': 0.9300481745431424}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 19:55:04,306][0m Trial 14 finished with value: 0.7422404851831984 and parameters: {'observation_period_num': 54, 'train_rates': 0.7530881480261861, 'learning_rate': 0.0003482970253424967, 'batch_size': 67, 'step_size': 10, 'gamma': 0.9244195271952378}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 19:56:30,124][0m Trial 15 finished with value: 1.083094103695595 and parameters: {'observation_period_num': 52, 'train_rates': 0.9498879997150311, 'learning_rate': 0.0009802399080182777, 'batch_size': 58, 'step_size': 14, 'gamma': 0.8576989153123301}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 19:57:12,558][0m Trial 16 finished with value: 0.2773129412106105 and parameters: {'observation_period_num': 19, 'train_rates': 0.9047452866510004, 'learning_rate': 0.0002342505526180055, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8940289583429064}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 19:58:54,640][0m Trial 17 finished with value: 0.4915795664128881 and parameters: {'observation_period_num': 78, 'train_rates': 0.7975986284234032, 'learning_rate': 0.00014868662438016916, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9695827083832055}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:00:13,137][0m Trial 18 finished with value: 0.2188364714384079 and parameters: {'observation_period_num': 70, 'train_rates': 0.9855104076278894, 'learning_rate': 0.00054188181812416, 'batch_size': 136, 'step_size': 13, 'gamma': 0.8034214480571694}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:02:23,268][0m Trial 19 finished with value: 0.2590646888337918 and parameters: {'observation_period_num': 120, 'train_rates': 0.9058377697627157, 'learning_rate': 0.00011775577086282263, 'batch_size': 82, 'step_size': 8, 'gamma': 0.9534668181693472}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:05:36,100][0m Trial 20 finished with value: 0.7545422180360105 and parameters: {'observation_period_num': 27, 'train_rates': 0.7158349073912988, 'learning_rate': 1.1179964657888702e-05, 'batch_size': 20, 'step_size': 10, 'gamma': 0.8981924144365944}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:06:51,229][0m Trial 21 finished with value: 0.2561303675174713 and parameters: {'observation_period_num': 66, 'train_rates': 0.9854842004371361, 'learning_rate': 0.0004607763027795739, 'batch_size': 133, 'step_size': 15, 'gamma': 0.7962141848064767}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:08:24,626][0m Trial 22 finished with value: 0.3785776487168144 and parameters: {'observation_period_num': 86, 'train_rates': 0.9528707511753262, 'learning_rate': 0.0005699543192096848, 'batch_size': 124, 'step_size': 13, 'gamma': 0.8189511970356088}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:09:08,189][0m Trial 23 finished with value: 0.24564294517040253 and parameters: {'observation_period_num': 38, 'train_rates': 0.9574119888824258, 'learning_rate': 0.00019143039377700054, 'batch_size': 174, 'step_size': 12, 'gamma': 0.8434973936526646}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:10:20,479][0m Trial 24 finished with value: 0.1695052534341812 and parameters: {'observation_period_num': 66, 'train_rates': 0.9892180495724568, 'learning_rate': 0.000611150196134737, 'batch_size': 254, 'step_size': 7, 'gamma': 0.8690393252644346}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:10:41,450][0m Trial 25 finished with value: 0.2827373147010803 and parameters: {'observation_period_num': 5, 'train_rates': 0.9210724485091739, 'learning_rate': 0.0002575919477801612, 'batch_size': 255, 'step_size': 7, 'gamma': 0.86472021892911}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:12:56,418][0m Trial 26 finished with value: 0.5684858730322198 and parameters: {'observation_period_num': 129, 'train_rates': 0.8884067325613091, 'learning_rate': 7.321783574226631e-05, 'batch_size': 206, 'step_size': 4, 'gamma': 0.8755555561062582}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:14:43,732][0m Trial 27 finished with value: 1.1145743446723195 and parameters: {'observation_period_num': 48, 'train_rates': 0.8199878994408556, 'learning_rate': 0.000688041306758531, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8953418712122371}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:16:23,747][0m Trial 28 finished with value: 0.26086729991695157 and parameters: {'observation_period_num': 93, 'train_rates': 0.9358462568596965, 'learning_rate': 0.00027117474355375075, 'batch_size': 85, 'step_size': 7, 'gamma': 0.8486306603696799}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:17:55,844][0m Trial 29 finished with value: 0.2836852746705214 and parameters: {'observation_period_num': 63, 'train_rates': 0.9173548073316817, 'learning_rate': 3.968819434796e-05, 'batch_size': 53, 'step_size': 9, 'gamma': 0.9138627882196081}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:18:28,279][0m Trial 30 finished with value: 0.9868373274803162 and parameters: {'observation_period_num': 29, 'train_rates': 0.9598145023979443, 'learning_rate': 1.2625529411689868e-05, 'batch_size': 247, 'step_size': 5, 'gamma': 0.9428522295926633}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:19:43,563][0m Trial 31 finished with value: 0.23693124949932098 and parameters: {'observation_period_num': 69, 'train_rates': 0.985108555006518, 'learning_rate': 0.000519959766700554, 'batch_size': 201, 'step_size': 10, 'gamma': 0.8351623253732365}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:21:00,776][0m Trial 32 finished with value: 0.20752020180225372 and parameters: {'observation_period_num': 69, 'train_rates': 0.9882642613627899, 'learning_rate': 0.00041334518518714873, 'batch_size': 145, 'step_size': 7, 'gamma': 0.7986804874044785}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:23:13,930][0m Trial 33 finished with value: 0.2521083652973175 and parameters: {'observation_period_num': 124, 'train_rates': 0.960826632568796, 'learning_rate': 0.00036572553389418685, 'batch_size': 155, 'step_size': 7, 'gamma': 0.8673445663022922}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:24:57,688][0m Trial 34 finished with value: 0.34209057336370347 and parameters: {'observation_period_num': 100, 'train_rates': 0.9289465141354483, 'learning_rate': 0.0007769932384438917, 'batch_size': 187, 'step_size': 8, 'gamma': 0.9072934134718964}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:27:50,468][0m Trial 35 finished with value: 0.23955698162317277 and parameters: {'observation_period_num': 143, 'train_rates': 0.8935129863515263, 'learning_rate': 0.00010117694363575273, 'batch_size': 38, 'step_size': 5, 'gamma': 0.8793773371346109}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:31:15,855][0m Trial 36 finished with value: 0.41609055123444216 and parameters: {'observation_period_num': 188, 'train_rates': 0.8653277598092287, 'learning_rate': 0.0001985399044435384, 'batch_size': 225, 'step_size': 7, 'gamma': 0.8274430450471212}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:32:15,988][0m Trial 37 finished with value: 1.8511353731155396 and parameters: {'observation_period_num': 55, 'train_rates': 0.9680783859471974, 'learning_rate': 1.0815113134464882e-06, 'batch_size': 101, 'step_size': 8, 'gamma': 0.8521442939601838}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:35:49,966][0m Trial 38 finished with value: 0.29128486717619545 and parameters: {'observation_period_num': 180, 'train_rates': 0.9412127065586955, 'learning_rate': 0.00038247563645222416, 'batch_size': 71, 'step_size': 5, 'gamma': 0.8045124514677485}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:37:41,952][0m Trial 39 finished with value: 0.32130023209672226 and parameters: {'observation_period_num': 112, 'train_rates': 0.8864917599584259, 'learning_rate': 0.0001432449985901762, 'batch_size': 158, 'step_size': 9, 'gamma': 0.8815020094377821}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:38:55,789][0m Trial 40 finished with value: 0.9390786878142346 and parameters: {'observation_period_num': 79, 'train_rates': 0.8463137306316646, 'learning_rate': 6.149997963412035e-05, 'batch_size': 227, 'step_size': 3, 'gamma': 0.7726269200276363}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:40:10,955][0m Trial 41 finished with value: 0.28950199484825134 and parameters: {'observation_period_num': 70, 'train_rates': 0.9699874859924713, 'learning_rate': 0.000632453997009312, 'batch_size': 192, 'step_size': 11, 'gamma': 0.8040717101380535}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:40:57,499][0m Trial 42 finished with value: 0.25492045283317566 and parameters: {'observation_period_num': 39, 'train_rates': 0.9888548084386448, 'learning_rate': 0.00047056216902852967, 'batch_size': 132, 'step_size': 6, 'gamma': 0.7790125016070187}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:41:32,851][0m Trial 43 finished with value: 0.4061984995315815 and parameters: {'observation_period_num': 18, 'train_rates': 0.9407847090622562, 'learning_rate': 0.0007787267684301532, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8175147460797676}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:43:05,160][0m Trial 44 finished with value: 0.1795949637889862 and parameters: {'observation_period_num': 80, 'train_rates': 0.9896159306159776, 'learning_rate': 0.0002903796290254365, 'batch_size': 88, 'step_size': 8, 'gamma': 0.8369157226035969}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:44:04,213][0m Trial 45 finished with value: 0.2717215120792389 and parameters: {'observation_period_num': 45, 'train_rates': 0.9709491734579917, 'learning_rate': 0.00027920469716235103, 'batch_size': 90, 'step_size': 8, 'gamma': 0.8344459000966822}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:45:54,593][0m Trial 46 finished with value: 0.23070650128037648 and parameters: {'observation_period_num': 96, 'train_rates': 0.920059021252841, 'learning_rate': 0.00018165164698342827, 'batch_size': 58, 'step_size': 10, 'gamma': 0.8622676683442975}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:46:58,195][0m Trial 47 finished with value: 0.23209841222297856 and parameters: {'observation_period_num': 58, 'train_rates': 0.9437360533599323, 'learning_rate': 0.0003064154844055596, 'batch_size': 112, 'step_size': 7, 'gamma': 0.8858367253535846}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:48:31,408][0m Trial 48 finished with value: 0.279947966337204 and parameters: {'observation_period_num': 82, 'train_rates': 0.9700926065281288, 'learning_rate': 0.0004234432263705697, 'batch_size': 100, 'step_size': 9, 'gamma': 0.7603561794853934}. Best is trial 11 with value: 0.16284852288663387.[0m
[32m[I 2025-02-08 20:50:40,577][0m Trial 49 finished with value: 0.2225651890039444 and parameters: {'observation_period_num': 111, 'train_rates': 0.9897709322456559, 'learning_rate': 8.214092015918616e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.846316812208214}. Best is trial 11 with value: 0.16284852288663387.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-08 20:50:40,584][0m A new study created in memory with name: no-name-9a3abe33-b643-42f4-9819-55988c53e0a9[0m
[32m[I 2025-02-08 20:54:58,956][0m Trial 0 finished with value: 1.4195308287938435 and parameters: {'observation_period_num': 245, 'train_rates': 0.7122912286715447, 'learning_rate': 1.702757318588084e-05, 'batch_size': 236, 'step_size': 7, 'gamma': 0.8862998457313117}. Best is trial 0 with value: 1.4195308287938435.[0m
[32m[I 2025-02-08 20:58:06,542][0m Trial 1 finished with value: 1.770229800514408 and parameters: {'observation_period_num': 189, 'train_rates': 0.7418464350777885, 'learning_rate': 0.000835025651450674, 'batch_size': 63, 'step_size': 15, 'gamma': 0.802391591571348}. Best is trial 0 with value: 1.4195308287938435.[0m
[32m[I 2025-02-08 21:02:27,202][0m Trial 2 finished with value: 2.132866193837133 and parameters: {'observation_period_num': 244, 'train_rates': 0.7328793058385656, 'learning_rate': 2.1796445339158916e-06, 'batch_size': 198, 'step_size': 7, 'gamma': 0.7912649177695189}. Best is trial 0 with value: 1.4195308287938435.[0m
[32m[I 2025-02-08 21:05:34,308][0m Trial 3 finished with value: 0.22864769001801807 and parameters: {'observation_period_num': 99, 'train_rates': 0.9582002623543329, 'learning_rate': 1.7011004668484773e-05, 'batch_size': 26, 'step_size': 9, 'gamma': 0.9761435813585474}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:10:39,443][0m Trial 4 finished with value: 0.7761209930364902 and parameters: {'observation_period_num': 239, 'train_rates': 0.9045212894676621, 'learning_rate': 4.2829216916296175e-06, 'batch_size': 38, 'step_size': 9, 'gamma': 0.7800905721239781}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:12:06,960][0m Trial 5 finished with value: 0.8391682741326322 and parameters: {'observation_period_num': 98, 'train_rates': 0.6988677900203951, 'learning_rate': 0.00045406104935803585, 'batch_size': 171, 'step_size': 2, 'gamma': 0.8616027122547809}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:13:02,246][0m Trial 6 finished with value: 0.9434751527057053 and parameters: {'observation_period_num': 61, 'train_rates': 0.7521569938234379, 'learning_rate': 3.551177989432586e-05, 'batch_size': 157, 'step_size': 11, 'gamma': 0.8413182383421954}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:13:40,050][0m Trial 7 finished with value: 1.2965061555785338 and parameters: {'observation_period_num': 31, 'train_rates': 0.7379098279141111, 'learning_rate': 9.276044100727158e-06, 'batch_size': 114, 'step_size': 7, 'gamma': 0.8551051857938905}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:14:35,351][0m Trial 8 finished with value: 0.4647928456013853 and parameters: {'observation_period_num': 53, 'train_rates': 0.8939573108999804, 'learning_rate': 2.2247719916052823e-05, 'batch_size': 133, 'step_size': 15, 'gamma': 0.9568893739302581}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:15:27,682][0m Trial 9 finished with value: 0.3959857943170358 and parameters: {'observation_period_num': 43, 'train_rates': 0.8690802967311118, 'learning_rate': 2.5981915599733974e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9783373141775299}. Best is trial 3 with value: 0.22864769001801807.[0m
[32m[I 2025-02-08 21:18:56,553][0m Trial 10 finished with value: 0.16705565433949232 and parameters: {'observation_period_num': 143, 'train_rates': 0.9718398484138855, 'learning_rate': 0.0001352255611810027, 'batch_size': 23, 'step_size': 3, 'gamma': 0.9296541588457005}. Best is trial 10 with value: 0.16705565433949232.[0m
[32m[I 2025-02-08 21:22:16,842][0m Trial 11 finished with value: 0.15314788980917496 and parameters: {'observation_period_num': 144, 'train_rates': 0.9885697483885277, 'learning_rate': 0.00012781651961228478, 'batch_size': 27, 'step_size': 2, 'gamma': 0.9284083222404288}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:25:28,433][0m Trial 12 finished with value: 0.3477124571800232 and parameters: {'observation_period_num': 157, 'train_rates': 0.9890999230539133, 'learning_rate': 0.00015062108792742015, 'batch_size': 73, 'step_size': 1, 'gamma': 0.9179086941992285}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:28:50,052][0m Trial 13 finished with value: 0.40900381484021575 and parameters: {'observation_period_num': 155, 'train_rates': 0.8317811169094116, 'learning_rate': 0.0001056500398671281, 'batch_size': 22, 'step_size': 3, 'gamma': 0.923365510523149}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:31:44,190][0m Trial 14 finished with value: 0.9368201186046449 and parameters: {'observation_period_num': 192, 'train_rates': 0.6122485731486421, 'learning_rate': 0.00010994471267305488, 'batch_size': 59, 'step_size': 4, 'gamma': 0.9281961363958152}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:36:34,036][0m Trial 15 finished with value: 0.23321334104384145 and parameters: {'observation_period_num': 126, 'train_rates': 0.9344718833780693, 'learning_rate': 0.00021197109939729636, 'batch_size': 16, 'step_size': 3, 'gamma': 0.8879905718107779}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:40:42,247][0m Trial 16 finished with value: 0.20030111074447632 and parameters: {'observation_period_num': 196, 'train_rates': 0.988323841107044, 'learning_rate': 6.184171290499473e-05, 'batch_size': 93, 'step_size': 5, 'gamma': 0.9450560342913733}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:42:57,115][0m Trial 17 finished with value: 0.450438847283801 and parameters: {'observation_period_num': 127, 'train_rates': 0.8238768958483835, 'learning_rate': 0.00033298748079807266, 'batch_size': 52, 'step_size': 5, 'gamma': 0.8937536099989816}. Best is trial 11 with value: 0.15314788980917496.[0m
Early stopping at epoch 64
[32m[I 2025-02-08 21:44:02,098][0m Trial 18 finished with value: 0.9298694220335124 and parameters: {'observation_period_num': 93, 'train_rates': 0.9375056158016164, 'learning_rate': 5.7226819806358985e-05, 'batch_size': 95, 'step_size': 1, 'gamma': 0.8258205909586567}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:44:23,419][0m Trial 19 finished with value: 0.5876730752853768 and parameters: {'observation_period_num': 7, 'train_rates': 0.8691017788377527, 'learning_rate': 0.0009143330814359869, 'batch_size': 244, 'step_size': 12, 'gamma': 0.9527614907513534}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:47:00,793][0m Trial 20 finished with value: 0.9617703886536079 and parameters: {'observation_period_num': 160, 'train_rates': 0.6480727083129043, 'learning_rate': 5.9394051851193235e-05, 'batch_size': 40, 'step_size': 5, 'gamma': 0.758510841722829}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:51:13,978][0m Trial 21 finished with value: 0.19255521893501282 and parameters: {'observation_period_num': 201, 'train_rates': 0.9899456419749099, 'learning_rate': 6.450336755319643e-05, 'batch_size': 86, 'step_size': 5, 'gamma': 0.9461507952599661}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:55:35,199][0m Trial 22 finished with value: 0.25977008132373586 and parameters: {'observation_period_num': 211, 'train_rates': 0.9568910472825359, 'learning_rate': 0.00027002502782370256, 'batch_size': 78, 'step_size': 3, 'gamma': 0.9873484311988905}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 21:58:50,315][0m Trial 23 finished with value: 0.31826768310602044 and parameters: {'observation_period_num': 172, 'train_rates': 0.9134182229215704, 'learning_rate': 9.363603523488826e-05, 'batch_size': 119, 'step_size': 4, 'gamma': 0.8994971655224336}. Best is trial 11 with value: 0.15314788980917496.[0m
[32m[I 2025-02-08 22:03:47,477][0m Trial 24 finished with value: 0.13705164194107056 and parameters: {'observation_period_num': 220, 'train_rates': 0.9879225756721891, 'learning_rate': 0.00018516222876211605, 'batch_size': 45, 'step_size': 2, 'gamma': 0.9349928127071232}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:08:46,193][0m Trial 25 finished with value: 0.3522685610840463 and parameters: {'observation_period_num': 223, 'train_rates': 0.9572595326878065, 'learning_rate': 0.00019067745678977906, 'batch_size': 44, 'step_size': 1, 'gamma': 0.9093999007971514}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:13:20,823][0m Trial 26 finished with value: 0.4329600461193773 and parameters: {'observation_period_num': 139, 'train_rates': 0.9351328850075363, 'learning_rate': 0.0005176233579430639, 'batch_size': 17, 'step_size': 2, 'gamma': 0.9255287951452305}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:15:29,632][0m Trial 27 finished with value: 0.8612237531359833 and parameters: {'observation_period_num': 114, 'train_rates': 0.7868146812401547, 'learning_rate': 0.00045232453447835013, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9658270799043001}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:16:56,765][0m Trial 28 finished with value: 0.3471017212227539 and parameters: {'observation_period_num': 79, 'train_rates': 0.8501103138138953, 'learning_rate': 0.00014084403204612072, 'batch_size': 61, 'step_size': 6, 'gamma': 0.9426510048811871}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:20:14,072][0m Trial 29 finished with value: 0.722809910774231 and parameters: {'observation_period_num': 170, 'train_rates': 0.9641974888858709, 'learning_rate': 3.8998410352439876e-05, 'batch_size': 210, 'step_size': 4, 'gamma': 0.8717304187816572}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:24:38,408][0m Trial 30 finished with value: 0.44603887304196377 and parameters: {'observation_period_num': 220, 'train_rates': 0.8952730842054938, 'learning_rate': 0.00019364872610700524, 'batch_size': 109, 'step_size': 2, 'gamma': 0.8736935335342877}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:27:29,837][0m Trial 31 finished with value: 0.18099920451641083 and parameters: {'observation_period_num': 144, 'train_rates': 0.9863980083907503, 'learning_rate': 7.631518052496168e-05, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9392489107404689}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:30:24,468][0m Trial 32 finished with value: 0.1790140997382659 and parameters: {'observation_period_num': 139, 'train_rates': 0.9721263196090502, 'learning_rate': 9.270262316073053e-05, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9329510165367687}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:34:05,322][0m Trial 33 finished with value: 0.24992250325158238 and parameters: {'observation_period_num': 182, 'train_rates': 0.9308566501355601, 'learning_rate': 0.0003071697862624409, 'batch_size': 51, 'step_size': 6, 'gamma': 0.9085007394790728}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:36:37,527][0m Trial 34 finished with value: 0.35756931712752893 and parameters: {'observation_period_num': 113, 'train_rates': 0.9668439698116152, 'learning_rate': 4.090359896707309e-05, 'batch_size': 32, 'step_size': 1, 'gamma': 0.9624726731119385}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:39:43,854][0m Trial 35 finished with value: 0.4158245305220286 and parameters: {'observation_period_num': 142, 'train_rates': 0.9201585596574486, 'learning_rate': 1.2725396014756003e-05, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9303837633017074}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:41:15,728][0m Trial 36 finished with value: 0.2249393058674676 and parameters: {'observation_period_num': 79, 'train_rates': 0.9465996175568128, 'learning_rate': 0.00013544266596176823, 'batch_size': 66, 'step_size': 8, 'gamma': 0.9123393869843597}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:46:52,032][0m Trial 37 finished with value: 0.8675703801465838 and parameters: {'observation_period_num': 248, 'train_rates': 0.9672057727182884, 'learning_rate': 0.0006475124371254568, 'batch_size': 49, 'step_size': 2, 'gamma': 0.9711915607564443}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:48:45,887][0m Trial 38 finished with value: 1.0643696040987112 and parameters: {'observation_period_num': 112, 'train_rates': 0.9023454074148539, 'learning_rate': 6.7733435880821564e-06, 'batch_size': 177, 'step_size': 6, 'gamma': 0.8816074745422864}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:52:29,875][0m Trial 39 finished with value: 1.9992052247115306 and parameters: {'observation_period_num': 231, 'train_rates': 0.685553611662009, 'learning_rate': 1.0286522013839286e-06, 'batch_size': 219, 'step_size': 3, 'gamma': 0.8997935575813235}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:55:50,458][0m Trial 40 finished with value: 1.0912701453759064 and parameters: {'observation_period_num': 174, 'train_rates': 0.7687953542934745, 'learning_rate': 0.00032606568018730525, 'batch_size': 29, 'step_size': 10, 'gamma': 0.8462256455647478}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 22:58:51,128][0m Trial 41 finished with value: 0.16696055233478546 and parameters: {'observation_period_num': 149, 'train_rates': 0.9755580408292979, 'learning_rate': 8.562231089356342e-05, 'batch_size': 70, 'step_size': 3, 'gamma': 0.9369856996732553}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:01:41,269][0m Trial 42 finished with value: 0.22016362648412405 and parameters: {'observation_period_num': 133, 'train_rates': 0.9708027837800874, 'learning_rate': 8.444444887191035e-05, 'batch_size': 44, 'step_size': 2, 'gamma': 0.932970579067755}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:04:49,779][0m Trial 43 finished with value: 0.20980199996162863 and parameters: {'observation_period_num': 155, 'train_rates': 0.9760663954409207, 'learning_rate': 0.00014585391345046066, 'batch_size': 64, 'step_size': 1, 'gamma': 0.9554564263728856}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:07:54,120][0m Trial 44 finished with value: 0.2228485932238668 and parameters: {'observation_period_num': 121, 'train_rates': 0.9511590139962899, 'learning_rate': 4.723898591402798e-05, 'batch_size': 26, 'step_size': 3, 'gamma': 0.9166459453993601}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:10:33,872][0m Trial 45 finished with value: 0.2784374611718314 and parameters: {'observation_period_num': 148, 'train_rates': 0.8779235830204604, 'learning_rate': 0.0002269892483208169, 'batch_size': 147, 'step_size': 4, 'gamma': 0.9365715401312286}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:13:41,485][0m Trial 46 finished with value: 0.27866173282737644 and parameters: {'observation_period_num': 164, 'train_rates': 0.9210279873803898, 'learning_rate': 9.458963181390405e-05, 'batch_size': 103, 'step_size': 8, 'gamma': 0.9886664359911288}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:15:38,212][0m Trial 47 finished with value: 0.21939803312222164 and parameters: {'observation_period_num': 97, 'train_rates': 0.9475777765108039, 'learning_rate': 0.00011703885521468431, 'batch_size': 55, 'step_size': 2, 'gamma': 0.9523348455615768}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:20:37,943][0m Trial 48 finished with value: 0.13798567881950966 and parameters: {'observation_period_num': 185, 'train_rates': 0.9813719073737962, 'learning_rate': 2.075258071166777e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9730125649937608}. Best is trial 24 with value: 0.13705164194107056.[0m
[32m[I 2025-02-08 23:24:54,450][0m Trial 49 finished with value: 0.1623316719647377 and parameters: {'observation_period_num': 182, 'train_rates': 0.9888584849511827, 'learning_rate': 2.555232528837365e-05, 'batch_size': 25, 'step_size': 4, 'gamma': 0.9792144291789971}. Best is trial 24 with value: 0.13705164194107056.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-08 23:24:54,458][0m A new study created in memory with name: no-name-9c4a0ff9-b5fc-49bf-b3bf-9512094a18f3[0m
[32m[I 2025-02-08 23:26:28,604][0m Trial 0 finished with value: 1.8723356939774356 and parameters: {'observation_period_num': 110, 'train_rates': 0.6974736997646585, 'learning_rate': 1.844411000830128e-06, 'batch_size': 225, 'step_size': 8, 'gamma': 0.9272233337964478}. Best is trial 0 with value: 1.8723356939774356.[0m
[32m[I 2025-02-08 23:26:46,841][0m Trial 1 finished with value: 0.7816214809710077 and parameters: {'observation_period_num': 5, 'train_rates': 0.6796253241791146, 'learning_rate': 0.00018061499681750894, 'batch_size': 245, 'step_size': 7, 'gamma': 0.7956732977977411}. Best is trial 1 with value: 0.7816214809710077.[0m
[32m[I 2025-02-08 23:27:17,744][0m Trial 2 finished with value: 0.8162060445046035 and parameters: {'observation_period_num': 31, 'train_rates': 0.7277955654591322, 'learning_rate': 0.0002235927247577743, 'batch_size': 183, 'step_size': 6, 'gamma': 0.7909178627666237}. Best is trial 1 with value: 0.7816214809710077.[0m
[32m[I 2025-02-08 23:28:08,391][0m Trial 3 finished with value: 0.20039063692092896 and parameters: {'observation_period_num': 42, 'train_rates': 0.9808847823887767, 'learning_rate': 0.0003259974202916212, 'batch_size': 124, 'step_size': 14, 'gamma': 0.9077057017575352}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:29:05,085][0m Trial 4 finished with value: 0.48410252820361743 and parameters: {'observation_period_num': 62, 'train_rates': 0.8099932883781645, 'learning_rate': 0.0004644727990934139, 'batch_size': 255, 'step_size': 7, 'gamma': 0.765791622643332}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:31:03,356][0m Trial 5 finished with value: 0.5548716397563211 and parameters: {'observation_period_num': 123, 'train_rates': 0.7925506570989186, 'learning_rate': 0.00033304367569936684, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8398902364174055}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:34:06,984][0m Trial 6 finished with value: 1.8095783441047357 and parameters: {'observation_period_num': 186, 'train_rates': 0.7335995745963968, 'learning_rate': 1.2181997301185142e-06, 'batch_size': 158, 'step_size': 11, 'gamma': 0.9655126493338306}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:38:38,979][0m Trial 7 finished with value: 0.7948264023709332 and parameters: {'observation_period_num': 235, 'train_rates': 0.7475832615790936, 'learning_rate': 1.9254973236573626e-05, 'batch_size': 35, 'step_size': 11, 'gamma': 0.7679198936952711}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:40:12,854][0m Trial 8 finished with value: 0.9512522492943138 and parameters: {'observation_period_num': 115, 'train_rates': 0.6734409276193211, 'learning_rate': 0.0008913513546355199, 'batch_size': 208, 'step_size': 13, 'gamma': 0.7769061563243899}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:40:56,593][0m Trial 9 finished with value: 0.6445807523843718 and parameters: {'observation_period_num': 6, 'train_rates': 0.7359433169918402, 'learning_rate': 9.117135108640781e-05, 'batch_size': 95, 'step_size': 13, 'gamma': 0.8060931535061737}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:42:00,308][0m Trial 10 finished with value: 0.8501404293558814 and parameters: {'observation_period_num': 63, 'train_rates': 0.9090211645085933, 'learning_rate': 2.4464235911588904e-05, 'batch_size': 111, 'step_size': 2, 'gamma': 0.9039763135735813}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:43:16,514][0m Trial 11 finished with value: 0.32761910557746887 and parameters: {'observation_period_num': 67, 'train_rates': 0.9857260512669328, 'learning_rate': 0.0006936085408950335, 'batch_size': 143, 'step_size': 4, 'gamma': 0.8874406906266034}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:44:35,717][0m Trial 12 finished with value: 0.4694213271141052 and parameters: {'observation_period_num': 70, 'train_rates': 0.9899898368442749, 'learning_rate': 7.035448550908312e-05, 'batch_size': 141, 'step_size': 3, 'gamma': 0.8733158359043}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:47:57,658][0m Trial 13 finished with value: 0.790982723236084 and parameters: {'observation_period_num': 162, 'train_rates': 0.9880351293650048, 'learning_rate': 0.0009955070794116514, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9130757860506633}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:49:11,831][0m Trial 14 finished with value: 0.9399527941292863 and parameters: {'observation_period_num': 71, 'train_rates': 0.9148868870669115, 'learning_rate': 4.658705875385291e-06, 'batch_size': 124, 'step_size': 15, 'gamma': 0.9874165331061046}. Best is trial 3 with value: 0.20039063692092896.[0m
Early stopping at epoch 91
[32m[I 2025-02-08 23:49:42,235][0m Trial 15 finished with value: 1.4189847066562475 and parameters: {'observation_period_num': 37, 'train_rates': 0.6021823365836094, 'learning_rate': 8.807431635428407e-05, 'batch_size': 170, 'step_size': 1, 'gamma': 0.869597573107822}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:51:28,564][0m Trial 16 finished with value: 0.32293293986258026 and parameters: {'observation_period_num': 102, 'train_rates': 0.9200759760415336, 'learning_rate': 0.00046228058334353686, 'batch_size': 135, 'step_size': 10, 'gamma': 0.8650132444217382}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:54:26,914][0m Trial 17 finished with value: 0.855613194498015 and parameters: {'observation_period_num': 154, 'train_rates': 0.9132797143029918, 'learning_rate': 7.828902006617187e-06, 'batch_size': 75, 'step_size': 10, 'gamma': 0.8448755066356853}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:55:58,488][0m Trial 18 finished with value: 0.3570595688498806 and parameters: {'observation_period_num': 95, 'train_rates': 0.8783087507903259, 'learning_rate': 0.00016877066311220084, 'batch_size': 189, 'step_size': 15, 'gamma': 0.9384580213343061}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-08 23:59:18,735][0m Trial 19 finished with value: 0.33881902142395426 and parameters: {'observation_period_num': 92, 'train_rates': 0.8599508785958745, 'learning_rate': 4.529876110880486e-05, 'batch_size': 22, 'step_size': 10, 'gamma': 0.8297178613983626}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:02:18,519][0m Trial 20 finished with value: 0.7045792890222449 and parameters: {'observation_period_num': 154, 'train_rates': 0.9459515867252966, 'learning_rate': 0.0004870221303508833, 'batch_size': 108, 'step_size': 12, 'gamma': 0.9517652287509812}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:03:05,148][0m Trial 21 finished with value: 0.46147146821022034 and parameters: {'observation_period_num': 41, 'train_rates': 0.9527181873831448, 'learning_rate': 0.0006274573877106308, 'batch_size': 146, 'step_size': 5, 'gamma': 0.8864144731345613}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:04:39,290][0m Trial 22 finished with value: 0.3049311304533923 and parameters: {'observation_period_num': 88, 'train_rates': 0.9531380073365063, 'learning_rate': 0.00022502943536956702, 'batch_size': 127, 'step_size': 9, 'gamma': 0.8920347226896599}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:06:08,834][0m Trial 23 finished with value: 0.34546234757505 and parameters: {'observation_period_num': 88, 'train_rates': 0.8696607497565968, 'learning_rate': 0.0002921417696590953, 'batch_size': 124, 'step_size': 9, 'gamma': 0.8600208114713491}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:07:38,203][0m Trial 24 finished with value: 0.29294042927878244 and parameters: {'observation_period_num': 43, 'train_rates': 0.9496060524498527, 'learning_rate': 0.00014949530544014245, 'batch_size': 56, 'step_size': 9, 'gamma': 0.8946061150258603}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:09:17,468][0m Trial 25 finished with value: 0.34781460226721067 and parameters: {'observation_period_num': 26, 'train_rates': 0.9466927745714886, 'learning_rate': 0.00011812497717932, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9000638560244306}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:10:24,694][0m Trial 26 finished with value: 0.44803881691677994 and parameters: {'observation_period_num': 47, 'train_rates': 0.8238588368921127, 'learning_rate': 3.6914905390014194e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.9249526285411096}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:12:59,808][0m Trial 27 finished with value: 0.29545852447312976 and parameters: {'observation_period_num': 134, 'train_rates': 0.9555354608079057, 'learning_rate': 0.00014995888917370635, 'batch_size': 95, 'step_size': 9, 'gamma': 0.8903198318973524}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:17:02,050][0m Trial 28 finished with value: 0.4130762831880412 and parameters: {'observation_period_num': 207, 'train_rates': 0.8811338418741645, 'learning_rate': 5.204221183915871e-05, 'batch_size': 92, 'step_size': 6, 'gamma': 0.9117408599237043}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:19:43,770][0m Trial 29 finished with value: 0.4286715721134015 and parameters: {'observation_period_num': 135, 'train_rates': 0.837891383134716, 'learning_rate': 0.00012276203561499668, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9347481593026912}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:22:26,784][0m Trial 30 finished with value: 0.60271526660238 and parameters: {'observation_period_num': 140, 'train_rates': 0.9604209270886263, 'learning_rate': 1.4164391799040726e-05, 'batch_size': 101, 'step_size': 12, 'gamma': 0.956813651525975}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:23:57,613][0m Trial 31 finished with value: 0.32286354899406433 and parameters: {'observation_period_num': 82, 'train_rates': 0.9641324832167859, 'learning_rate': 0.00029797531286403206, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8875641877305684}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:24:58,902][0m Trial 32 finished with value: 0.31423209221274767 and parameters: {'observation_period_num': 19, 'train_rates': 0.9359535601132742, 'learning_rate': 0.00017369659738010668, 'batch_size': 82, 'step_size': 8, 'gamma': 0.9199515633857248}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:26:34,697][0m Trial 33 finished with value: 0.2757340944475598 and parameters: {'observation_period_num': 49, 'train_rates': 0.8919840544118198, 'learning_rate': 0.00023762773400409022, 'batch_size': 49, 'step_size': 6, 'gamma': 0.8954923673013757}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:28:24,742][0m Trial 34 finished with value: 0.28194790211329507 and parameters: {'observation_period_num': 51, 'train_rates': 0.8932801955866675, 'learning_rate': 0.00013960589734926513, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8787912162422755}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:30:07,942][0m Trial 35 finished with value: 0.3113288829889012 and parameters: {'observation_period_num': 52, 'train_rates': 0.8965938886788954, 'learning_rate': 7.039104524403962e-05, 'batch_size': 46, 'step_size': 7, 'gamma': 0.8267535367278978}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:33:46,049][0m Trial 36 finished with value: 0.5829436631074973 and parameters: {'observation_period_num': 14, 'train_rates': 0.7861186915107666, 'learning_rate': 0.00025590647522152107, 'batch_size': 19, 'step_size': 5, 'gamma': 0.8513375377283463}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:34:54,740][0m Trial 37 finished with value: 0.6204593387481413 and parameters: {'observation_period_num': 55, 'train_rates': 0.8428707804663026, 'learning_rate': 0.00040340309921509294, 'batch_size': 67, 'step_size': 6, 'gamma': 0.9343937370295539}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:35:29,461][0m Trial 38 finished with value: 0.3306172225210402 and parameters: {'observation_period_num': 32, 'train_rates': 0.8895250088587974, 'learning_rate': 0.00023624739341474864, 'batch_size': 234, 'step_size': 7, 'gamma': 0.878786526335889}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:37:14,428][0m Trial 39 finished with value: 0.724622868853264 and parameters: {'observation_period_num': 6, 'train_rates': 0.7659199004931142, 'learning_rate': 9.721626417814063e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9032841721467297}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:39:19,014][0m Trial 40 finished with value: 0.6940677756588208 and parameters: {'observation_period_num': 110, 'train_rates': 0.9233567258305576, 'learning_rate': 0.0006447298925199561, 'batch_size': 59, 'step_size': 5, 'gamma': 0.8586983228963506}. Best is trial 3 with value: 0.20039063692092896.[0m
[32m[I 2025-02-09 00:42:52,841][0m Trial 41 finished with value: 0.175631582736969 and parameters: {'observation_period_num': 181, 'train_rates': 0.9710162656026622, 'learning_rate': 0.00014546329805374848, 'batch_size': 86, 'step_size': 8, 'gamma': 0.8977533789367159}. Best is trial 41 with value: 0.175631582736969.[0m
[32m[I 2025-02-09 00:48:43,461][0m Trial 42 finished with value: 0.35232979193162384 and parameters: {'observation_period_num': 246, 'train_rates': 0.9674478356933782, 'learning_rate': 0.00036319410342522445, 'batch_size': 28, 'step_size': 7, 'gamma': 0.908828035499747}. Best is trial 41 with value: 0.175631582736969.[0m
[32m[I 2025-02-09 00:52:48,538][0m Trial 43 finished with value: 0.30969475507736205 and parameters: {'observation_period_num': 201, 'train_rates': 0.9274733740277683, 'learning_rate': 0.00013215399466567698, 'batch_size': 80, 'step_size': 8, 'gamma': 0.8801100597537385}. Best is trial 41 with value: 0.175631582736969.[0m
[32m[I 2025-02-09 00:56:21,257][0m Trial 44 finished with value: 0.18891821893644922 and parameters: {'observation_period_num': 175, 'train_rates': 0.9711987492213587, 'learning_rate': 6.524278898950269e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.8979196584807307}. Best is trial 41 with value: 0.175631582736969.[0m
[32m[I 2025-02-09 01:00:10,629][0m Trial 45 finished with value: 0.15939933314161786 and parameters: {'observation_period_num': 178, 'train_rates': 0.9790328105067794, 'learning_rate': 6.129374536809053e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.9203090949766259}. Best is trial 45 with value: 0.15939933314161786.[0m
[32m[I 2025-02-09 01:03:48,125][0m Trial 46 finished with value: 0.2892697961891399 and parameters: {'observation_period_num': 181, 'train_rates': 0.9693896407376466, 'learning_rate': 2.7114424350777107e-05, 'batch_size': 67, 'step_size': 12, 'gamma': 0.9448646785006897}. Best is trial 45 with value: 0.15939933314161786.[0m
[32m[I 2025-02-09 01:07:37,214][0m Trial 47 finished with value: 0.16478325269724192 and parameters: {'observation_period_num': 175, 'train_rates': 0.9798202337111294, 'learning_rate': 7.365042777024319e-05, 'batch_size': 30, 'step_size': 14, 'gamma': 0.9228442102112777}. Best is trial 45 with value: 0.15939933314161786.[0m
[32m[I 2025-02-09 01:11:40,698][0m Trial 48 finished with value: 0.16897441123922666 and parameters: {'observation_period_num': 182, 'train_rates': 0.9786898933172471, 'learning_rate': 7.408720409640251e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.9231042059408024}. Best is trial 45 with value: 0.15939933314161786.[0m
[32m[I 2025-02-09 01:15:50,544][0m Trial 49 finished with value: 0.18038302137480153 and parameters: {'observation_period_num': 179, 'train_rates': 0.9723272137385885, 'learning_rate': 5.742018724354494e-05, 'batch_size': 24, 'step_size': 14, 'gamma': 0.974583891383333}. Best is trial 45 with value: 0.15939933314161786.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 85, 'train_rates': 0.9825389747955425, 'learning_rate': 4.382536360015831e-05, 'batch_size': 36, 'step_size': 15, 'gamma': 0.77611872403775}
Epoch 1/300, trend Loss: 0.9118 | 1.4724
Epoch 2/300, trend Loss: 0.6744 | 1.2106
Epoch 3/300, trend Loss: 0.5615 | 1.0954
Epoch 4/300, trend Loss: 0.4785 | 0.9779
Epoch 5/300, trend Loss: 0.4146 | 0.8952
Epoch 6/300, trend Loss: 0.3745 | 0.8382
Epoch 7/300, trend Loss: 0.3447 | 0.7629
Epoch 8/300, trend Loss: 0.3189 | 0.6996
Epoch 9/300, trend Loss: 0.3039 | 0.6411
Epoch 10/300, trend Loss: 0.2939 | 0.6341
Epoch 11/300, trend Loss: 0.2807 | 0.6025
Epoch 12/300, trend Loss: 0.2770 | 0.5586
Epoch 13/300, trend Loss: 0.2706 | 0.5265
Epoch 14/300, trend Loss: 0.2895 | 0.5407
Epoch 15/300, trend Loss: 0.2699 | 0.4892
Epoch 16/300, trend Loss: 0.2715 | 0.4732
Epoch 17/300, trend Loss: 0.2530 | 0.4658
Epoch 18/300, trend Loss: 0.2393 | 0.4575
Epoch 19/300, trend Loss: 0.2341 | 0.4207
Epoch 20/300, trend Loss: 0.2270 | 0.4129
Epoch 21/300, trend Loss: 0.2216 | 0.4102
Epoch 22/300, trend Loss: 0.2168 | 0.3863
Epoch 23/300, trend Loss: 0.2134 | 0.3816
Epoch 24/300, trend Loss: 0.2096 | 0.3721
Epoch 25/300, trend Loss: 0.2076 | 0.3607
Epoch 26/300, trend Loss: 0.2047 | 0.3499
Epoch 27/300, trend Loss: 0.2029 | 0.3506
Epoch 28/300, trend Loss: 0.1997 | 0.3380
Epoch 29/300, trend Loss: 0.1976 | 0.3284
Epoch 30/300, trend Loss: 0.1951 | 0.3305
Epoch 31/300, trend Loss: 0.1931 | 0.3180
Epoch 32/300, trend Loss: 0.1925 | 0.3099
Epoch 33/300, trend Loss: 0.1909 | 0.3030
Epoch 34/300, trend Loss: 0.1892 | 0.3082
Epoch 35/300, trend Loss: 0.1869 | 0.2996
Epoch 36/300, trend Loss: 0.1863 | 0.2897
Epoch 37/300, trend Loss: 0.1854 | 0.2830
Epoch 38/300, trend Loss: 0.1840 | 0.2865
Epoch 39/300, trend Loss: 0.1825 | 0.2845
Epoch 40/300, trend Loss: 0.1811 | 0.2722
Epoch 41/300, trend Loss: 0.1806 | 0.2705
Epoch 42/300, trend Loss: 0.1791 | 0.2711
Epoch 43/300, trend Loss: 0.1785 | 0.2670
Epoch 44/300, trend Loss: 0.1766 | 0.2604
Epoch 45/300, trend Loss: 0.1767 | 0.2576
Epoch 46/300, trend Loss: 0.1756 | 0.2590
Epoch 47/300, trend Loss: 0.1732 | 0.2521
Epoch 48/300, trend Loss: 0.1741 | 0.2508
Epoch 49/300, trend Loss: 0.1731 | 0.2493
Epoch 50/300, trend Loss: 0.1718 | 0.2448
Epoch 51/300, trend Loss: 0.1708 | 0.2444
Epoch 52/300, trend Loss: 0.1714 | 0.2460
Epoch 53/300, trend Loss: 0.1704 | 0.2372
Epoch 54/300, trend Loss: 0.1695 | 0.2339
Epoch 55/300, trend Loss: 0.1691 | 0.2398
Epoch 56/300, trend Loss: 0.1683 | 0.2341
Epoch 57/300, trend Loss: 0.1674 | 0.2302
Epoch 58/300, trend Loss: 0.1671 | 0.2318
Epoch 59/300, trend Loss: 0.1667 | 0.2323
Epoch 60/300, trend Loss: 0.1660 | 0.2268
Epoch 61/300, trend Loss: 0.1655 | 0.2265
Epoch 62/300, trend Loss: 0.1648 | 0.2282
Epoch 63/300, trend Loss: 0.1649 | 0.2227
Epoch 64/300, trend Loss: 0.1637 | 0.2213
Epoch 65/300, trend Loss: 0.1631 | 0.2230
Epoch 66/300, trend Loss: 0.1631 | 0.2204
Epoch 67/300, trend Loss: 0.1628 | 0.2184
Epoch 68/300, trend Loss: 0.1625 | 0.2168
Epoch 69/300, trend Loss: 0.1619 | 0.2173
Epoch 70/300, trend Loss: 0.1616 | 0.2158
Epoch 71/300, trend Loss: 0.1611 | 0.2148
Epoch 72/300, trend Loss: 0.1612 | 0.2143
Epoch 73/300, trend Loss: 0.1615 | 0.2126
Epoch 74/300, trend Loss: 0.1604 | 0.2137
Epoch 75/300, trend Loss: 0.1601 | 0.2105
Epoch 76/300, trend Loss: 0.1599 | 0.2105
Epoch 77/300, trend Loss: 0.1595 | 0.2098
Epoch 78/300, trend Loss: 0.1600 | 0.2095
Epoch 79/300, trend Loss: 0.1591 | 0.2086
Epoch 80/300, trend Loss: 0.1585 | 0.2079
Epoch 81/300, trend Loss: 0.1583 | 0.2065
Epoch 82/300, trend Loss: 0.1584 | 0.2066
Epoch 83/300, trend Loss: 0.1586 | 0.2056
Epoch 84/300, trend Loss: 0.1579 | 0.2049
Epoch 85/300, trend Loss: 0.1578 | 0.2038
Epoch 86/300, trend Loss: 0.1583 | 0.2039
Epoch 87/300, trend Loss: 0.1569 | 0.2021
Epoch 88/300, trend Loss: 0.1573 | 0.2029
Epoch 89/300, trend Loss: 0.1567 | 0.2024
Epoch 90/300, trend Loss: 0.1569 | 0.2014
Epoch 91/300, trend Loss: 0.1567 | 0.2014
Epoch 92/300, trend Loss: 0.1569 | 0.2009
Epoch 93/300, trend Loss: 0.1569 | 0.2003
Epoch 94/300, trend Loss: 0.1559 | 0.1999
Epoch 95/300, trend Loss: 0.1564 | 0.2001
Epoch 96/300, trend Loss: 0.1565 | 0.1990
Epoch 97/300, trend Loss: 0.1555 | 0.1986
Epoch 98/300, trend Loss: 0.1566 | 0.1994
Epoch 99/300, trend Loss: 0.1553 | 0.1989
Epoch 100/300, trend Loss: 0.1552 | 0.1989
Epoch 101/300, trend Loss: 0.1554 | 0.1978
Epoch 102/300, trend Loss: 0.1551 | 0.1982
Epoch 103/300, trend Loss: 0.1555 | 0.1976
Epoch 104/300, trend Loss: 0.1552 | 0.1973
Epoch 105/300, trend Loss: 0.1550 | 0.1971
Epoch 106/300, trend Loss: 0.1550 | 0.1964
Epoch 107/300, trend Loss: 0.1547 | 0.1967
Epoch 108/300, trend Loss: 0.1546 | 0.1962
Epoch 109/300, trend Loss: 0.1544 | 0.1961
Epoch 110/300, trend Loss: 0.1545 | 0.1956
Epoch 111/300, trend Loss: 0.1545 | 0.1955
Epoch 112/300, trend Loss: 0.1546 | 0.1950
Epoch 113/300, trend Loss: 0.1540 | 0.1948
Epoch 114/300, trend Loss: 0.1540 | 0.1947
Epoch 115/300, trend Loss: 0.1543 | 0.1948
Epoch 116/300, trend Loss: 0.1539 | 0.1945
Epoch 117/300, trend Loss: 0.1536 | 0.1942
Epoch 118/300, trend Loss: 0.1534 | 0.1941
Epoch 119/300, trend Loss: 0.1539 | 0.1943
Epoch 120/300, trend Loss: 0.1534 | 0.1940
Epoch 121/300, trend Loss: 0.1537 | 0.1935
Epoch 122/300, trend Loss: 0.1538 | 0.1934
Epoch 123/300, trend Loss: 0.1534 | 0.1935
Epoch 124/300, trend Loss: 0.1533 | 0.1936
Epoch 125/300, trend Loss: 0.1532 | 0.1932
Epoch 126/300, trend Loss: 0.1530 | 0.1930
Epoch 127/300, trend Loss: 0.1533 | 0.1931
Epoch 128/300, trend Loss: 0.1530 | 0.1930
Epoch 129/300, trend Loss: 0.1528 | 0.1928
Epoch 130/300, trend Loss: 0.1533 | 0.1926
Epoch 131/300, trend Loss: 0.1537 | 0.1926
Epoch 132/300, trend Loss: 0.1530 | 0.1926
Epoch 133/300, trend Loss: 0.1531 | 0.1926
Epoch 134/300, trend Loss: 0.1533 | 0.1925
Epoch 135/300, trend Loss: 0.1536 | 0.1923
Epoch 136/300, trend Loss: 0.1541 | 0.1922
Epoch 137/300, trend Loss: 0.1536 | 0.1921
Epoch 138/300, trend Loss: 0.1530 | 0.1921
Epoch 139/300, trend Loss: 0.1532 | 0.1921
Epoch 140/300, trend Loss: 0.1532 | 0.1919
Epoch 141/300, trend Loss: 0.1531 | 0.1917
Epoch 142/300, trend Loss: 0.1529 | 0.1916
Epoch 143/300, trend Loss: 0.1531 | 0.1913
Epoch 144/300, trend Loss: 0.1537 | 0.1913
Epoch 145/300, trend Loss: 0.1532 | 0.1912
Epoch 146/300, trend Loss: 0.1533 | 0.1912
Epoch 147/300, trend Loss: 0.1527 | 0.1910
Epoch 148/300, trend Loss: 0.1528 | 0.1908
Epoch 149/300, trend Loss: 0.1526 | 0.1907
Epoch 150/300, trend Loss: 0.1527 | 0.1908
Epoch 151/300, trend Loss: 0.1524 | 0.1907
Epoch 152/300, trend Loss: 0.1529 | 0.1907
Epoch 153/300, trend Loss: 0.1526 | 0.1906
Epoch 154/300, trend Loss: 0.1518 | 0.1904
Epoch 155/300, trend Loss: 0.1527 | 0.1904
Epoch 156/300, trend Loss: 0.1527 | 0.1904
Epoch 157/300, trend Loss: 0.1524 | 0.1903
Epoch 158/300, trend Loss: 0.1525 | 0.1903
Epoch 159/300, trend Loss: 0.1525 | 0.1902
Epoch 160/300, trend Loss: 0.1525 | 0.1901
Epoch 161/300, trend Loss: 0.1527 | 0.1902
Epoch 162/300, trend Loss: 0.1522 | 0.1902
Epoch 163/300, trend Loss: 0.1528 | 0.1901
Epoch 164/300, trend Loss: 0.1520 | 0.1901
Epoch 165/300, trend Loss: 0.1525 | 0.1900
Epoch 166/300, trend Loss: 0.1524 | 0.1900
Epoch 167/300, trend Loss: 0.1518 | 0.1900
Epoch 168/300, trend Loss: 0.1519 | 0.1901
Epoch 169/300, trend Loss: 0.1522 | 0.1902
Epoch 170/300, trend Loss: 0.1520 | 0.1902
Epoch 171/300, trend Loss: 0.1524 | 0.1902
Epoch 172/300, trend Loss: 0.1525 | 0.1901
Epoch 173/300, trend Loss: 0.1522 | 0.1900
Epoch 174/300, trend Loss: 0.1520 | 0.1901
Epoch 175/300, trend Loss: 0.1519 | 0.1900
Epoch 176/300, trend Loss: 0.1524 | 0.1899
Epoch 177/300, trend Loss: 0.1521 | 0.1899
Epoch 178/300, trend Loss: 0.1528 | 0.1899
Epoch 179/300, trend Loss: 0.1524 | 0.1899
Epoch 180/300, trend Loss: 0.1523 | 0.1898
Epoch 181/300, trend Loss: 0.1521 | 0.1898
Epoch 182/300, trend Loss: 0.1524 | 0.1898
Epoch 183/300, trend Loss: 0.1522 | 0.1898
Epoch 184/300, trend Loss: 0.1521 | 0.1898
Epoch 185/300, trend Loss: 0.1520 | 0.1898
Epoch 186/300, trend Loss: 0.1523 | 0.1897
Epoch 187/300, trend Loss: 0.1520 | 0.1897
Epoch 188/300, trend Loss: 0.1525 | 0.1897
Epoch 189/300, trend Loss: 0.1525 | 0.1897
Epoch 190/300, trend Loss: 0.1518 | 0.1897
Epoch 191/300, trend Loss: 0.1520 | 0.1896
Epoch 192/300, trend Loss: 0.1525 | 0.1897
Epoch 193/300, trend Loss: 0.1524 | 0.1896
Epoch 194/300, trend Loss: 0.1520 | 0.1896
Epoch 195/300, trend Loss: 0.1528 | 0.1896
Epoch 196/300, trend Loss: 0.1519 | 0.1896
Epoch 197/300, trend Loss: 0.1526 | 0.1896
Epoch 198/300, trend Loss: 0.1524 | 0.1896
Epoch 199/300, trend Loss: 0.1523 | 0.1896
Epoch 200/300, trend Loss: 0.1524 | 0.1896
Epoch 201/300, trend Loss: 0.1523 | 0.1896
Epoch 202/300, trend Loss: 0.1523 | 0.1896
Epoch 203/300, trend Loss: 0.1517 | 0.1896
Epoch 204/300, trend Loss: 0.1519 | 0.1895
Epoch 205/300, trend Loss: 0.1526 | 0.1896
Epoch 206/300, trend Loss: 0.1521 | 0.1895
Epoch 207/300, trend Loss: 0.1523 | 0.1895
Epoch 208/300, trend Loss: 0.1521 | 0.1895
Epoch 209/300, trend Loss: 0.1529 | 0.1895
Epoch 210/300, trend Loss: 0.1520 | 0.1895
Epoch 211/300, trend Loss: 0.1526 | 0.1895
Epoch 212/300, trend Loss: 0.1523 | 0.1895
Epoch 213/300, trend Loss: 0.1519 | 0.1895
Epoch 214/300, trend Loss: 0.1520 | 0.1895
Epoch 215/300, trend Loss: 0.1525 | 0.1895
Epoch 216/300, trend Loss: 0.1516 | 0.1895
Epoch 217/300, trend Loss: 0.1533 | 0.1895
Epoch 218/300, trend Loss: 0.1524 | 0.1895
Epoch 219/300, trend Loss: 0.1521 | 0.1895
Epoch 220/300, trend Loss: 0.1518 | 0.1895
Epoch 221/300, trend Loss: 0.1525 | 0.1895
Epoch 222/300, trend Loss: 0.1528 | 0.1895
Epoch 223/300, trend Loss: 0.1521 | 0.1895
Epoch 224/300, trend Loss: 0.1522 | 0.1895
Epoch 225/300, trend Loss: 0.1519 | 0.1895
Epoch 226/300, trend Loss: 0.1524 | 0.1895
Epoch 227/300, trend Loss: 0.1521 | 0.1895
Epoch 228/300, trend Loss: 0.1520 | 0.1895
Epoch 229/300, trend Loss: 0.1524 | 0.1895
Epoch 230/300, trend Loss: 0.1522 | 0.1895
Epoch 231/300, trend Loss: 0.1524 | 0.1895
Epoch 232/300, trend Loss: 0.1518 | 0.1894
Epoch 233/300, trend Loss: 0.1523 | 0.1894
Epoch 234/300, trend Loss: 0.1523 | 0.1894
Epoch 235/300, trend Loss: 0.1522 | 0.1894
Epoch 236/300, trend Loss: 0.1526 | 0.1894
Epoch 237/300, trend Loss: 0.1512 | 0.1894
Epoch 238/300, trend Loss: 0.1518 | 0.1894
Epoch 239/300, trend Loss: 0.1525 | 0.1894
Epoch 240/300, trend Loss: 0.1529 | 0.1894
Epoch 241/300, trend Loss: 0.1519 | 0.1894
Epoch 242/300, trend Loss: 0.1515 | 0.1894
Epoch 243/300, trend Loss: 0.1520 | 0.1894
Epoch 244/300, trend Loss: 0.1517 | 0.1894
Epoch 245/300, trend Loss: 0.1517 | 0.1894
Epoch 246/300, trend Loss: 0.1524 | 0.1894
Epoch 247/300, trend Loss: 0.1522 | 0.1894
Epoch 248/300, trend Loss: 0.1525 | 0.1894
Epoch 249/300, trend Loss: 0.1517 | 0.1894
Epoch 250/300, trend Loss: 0.1522 | 0.1894
Epoch 251/300, trend Loss: 0.1522 | 0.1894
Epoch 252/300, trend Loss: 0.1518 | 0.1894
Epoch 253/300, trend Loss: 0.1521 | 0.1894
Epoch 254/300, trend Loss: 0.1520 | 0.1894
Epoch 255/300, trend Loss: 0.1522 | 0.1894
Epoch 256/300, trend Loss: 0.1522 | 0.1894
Epoch 257/300, trend Loss: 0.1525 | 0.1894
Epoch 258/300, trend Loss: 0.1523 | 0.1894
Epoch 259/300, trend Loss: 0.1524 | 0.1894
Epoch 260/300, trend Loss: 0.1524 | 0.1894
Epoch 261/300, trend Loss: 0.1517 | 0.1894
Epoch 262/300, trend Loss: 0.1523 | 0.1894
Epoch 263/300, trend Loss: 0.1527 | 0.1894
Epoch 264/300, trend Loss: 0.1520 | 0.1894
Epoch 265/300, trend Loss: 0.1522 | 0.1894
Epoch 266/300, trend Loss: 0.1524 | 0.1894
Epoch 267/300, trend Loss: 0.1524 | 0.1894
Epoch 268/300, trend Loss: 0.1513 | 0.1894
Epoch 269/300, trend Loss: 0.1523 | 0.1894
Epoch 270/300, trend Loss: 0.1519 | 0.1894
Epoch 271/300, trend Loss: 0.1526 | 0.1894
Epoch 272/300, trend Loss: 0.1523 | 0.1894
Epoch 273/300, trend Loss: 0.1522 | 0.1894
Epoch 274/300, trend Loss: 0.1518 | 0.1894
Epoch 275/300, trend Loss: 0.1518 | 0.1894
Epoch 276/300, trend Loss: 0.1521 | 0.1894
Epoch 277/300, trend Loss: 0.1528 | 0.1894
Epoch 278/300, trend Loss: 0.1524 | 0.1894
Epoch 279/300, trend Loss: 0.1520 | 0.1894
Epoch 280/300, trend Loss: 0.1521 | 0.1894
Epoch 281/300, trend Loss: 0.1521 | 0.1894
Epoch 282/300, trend Loss: 0.1518 | 0.1894
Epoch 283/300, trend Loss: 0.1528 | 0.1894
Epoch 284/300, trend Loss: 0.1527 | 0.1894
Epoch 285/300, trend Loss: 0.1516 | 0.1894
Epoch 286/300, trend Loss: 0.1527 | 0.1894
Epoch 287/300, trend Loss: 0.1524 | 0.1894
Epoch 288/300, trend Loss: 0.1516 | 0.1894
Epoch 289/300, trend Loss: 0.1526 | 0.1894
Epoch 290/300, trend Loss: 0.1518 | 0.1894
Epoch 291/300, trend Loss: 0.1519 | 0.1894
Epoch 292/300, trend Loss: 0.1521 | 0.1894
Epoch 293/300, trend Loss: 0.1517 | 0.1894
Epoch 294/300, trend Loss: 0.1520 | 0.1894
Epoch 295/300, trend Loss: 0.1520 | 0.1894
Epoch 296/300, trend Loss: 0.1522 | 0.1894
Epoch 297/300, trend Loss: 0.1507 | 0.1894
Epoch 298/300, trend Loss: 0.1523 | 0.1894
Epoch 299/300, trend Loss: 0.1520 | 0.1894
Epoch 300/300, trend Loss: 0.1521 | 0.1894
Training seasonal_0 component with params: {'observation_period_num': 32, 'train_rates': 0.988444807986931, 'learning_rate': 0.00019833952909232067, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8751645873885653}
Epoch 1/300, seasonal_0 Loss: 0.8612 | 1.0804
Epoch 2/300, seasonal_0 Loss: 0.7266 | 0.8845
Epoch 3/300, seasonal_0 Loss: 0.6063 | 0.8173
Epoch 4/300, seasonal_0 Loss: 0.5666 | 0.7071
Epoch 5/300, seasonal_0 Loss: 0.4693 | 0.6066
Epoch 6/300, seasonal_0 Loss: 0.4929 | 0.6279
Epoch 7/300, seasonal_0 Loss: 0.3766 | 0.5198
Epoch 8/300, seasonal_0 Loss: 0.3764 | 0.5181
Epoch 9/300, seasonal_0 Loss: 0.4069 | 0.4729
Epoch 10/300, seasonal_0 Loss: 0.3800 | 0.4445
Epoch 11/300, seasonal_0 Loss: 0.3617 | 0.4286
Epoch 12/300, seasonal_0 Loss: 0.3591 | 0.3936
Epoch 13/300, seasonal_0 Loss: 0.3388 | 0.3911
Epoch 14/300, seasonal_0 Loss: 0.3353 | 0.3932
Epoch 15/300, seasonal_0 Loss: 0.3053 | 0.3652
Epoch 16/300, seasonal_0 Loss: 0.3171 | 0.3574
Epoch 17/300, seasonal_0 Loss: 0.2906 | 0.3332
Epoch 18/300, seasonal_0 Loss: 0.2600 | 0.3186
Epoch 19/300, seasonal_0 Loss: 0.2609 | 0.3184
Epoch 20/300, seasonal_0 Loss: 0.2509 | 0.2997
Epoch 21/300, seasonal_0 Loss: 0.2527 | 0.2948
Epoch 22/300, seasonal_0 Loss: 0.2415 | 0.2900
Epoch 23/300, seasonal_0 Loss: 0.2285 | 0.2776
Epoch 24/300, seasonal_0 Loss: 0.2416 | 0.2837
Epoch 25/300, seasonal_0 Loss: 0.2317 | 0.2691
Epoch 26/300, seasonal_0 Loss: 0.2244 | 0.2773
Epoch 27/300, seasonal_0 Loss: 0.2246 | 0.2640
Epoch 28/300, seasonal_0 Loss: 0.2224 | 0.2644
Epoch 29/300, seasonal_0 Loss: 0.2147 | 0.2506
Epoch 30/300, seasonal_0 Loss: 0.2024 | 0.2493
Epoch 31/300, seasonal_0 Loss: 0.2024 | 0.2435
Epoch 32/300, seasonal_0 Loss: 0.1962 | 0.2422
Epoch 33/300, seasonal_0 Loss: 0.1920 | 0.2380
Epoch 34/300, seasonal_0 Loss: 0.1879 | 0.2339
Epoch 35/300, seasonal_0 Loss: 0.1856 | 0.2299
Epoch 36/300, seasonal_0 Loss: 0.1846 | 0.2260
Epoch 37/300, seasonal_0 Loss: 0.1825 | 0.2282
Epoch 38/300, seasonal_0 Loss: 0.1814 | 0.2223
Epoch 39/300, seasonal_0 Loss: 0.1818 | 0.2216
Epoch 40/300, seasonal_0 Loss: 0.1809 | 0.2161
Epoch 41/300, seasonal_0 Loss: 0.1778 | 0.2166
Epoch 42/300, seasonal_0 Loss: 0.1790 | 0.2137
Epoch 43/300, seasonal_0 Loss: 0.1777 | 0.2141
Epoch 44/300, seasonal_0 Loss: 0.1751 | 0.2096
Epoch 45/300, seasonal_0 Loss: 0.1770 | 0.2064
Epoch 46/300, seasonal_0 Loss: 0.1736 | 0.2071
Epoch 47/300, seasonal_0 Loss: 0.1730 | 0.2053
Epoch 48/300, seasonal_0 Loss: 0.1731 | 0.2057
Epoch 49/300, seasonal_0 Loss: 0.1687 | 0.1998
Epoch 50/300, seasonal_0 Loss: 0.1681 | 0.1992
Epoch 51/300, seasonal_0 Loss: 0.1683 | 0.2003
Epoch 52/300, seasonal_0 Loss: 0.1689 | 0.1995
Epoch 53/300, seasonal_0 Loss: 0.1659 | 0.1954
Epoch 54/300, seasonal_0 Loss: 0.1677 | 0.1949
Epoch 55/300, seasonal_0 Loss: 0.1661 | 0.1953
Epoch 56/300, seasonal_0 Loss: 0.1644 | 0.1943
Epoch 57/300, seasonal_0 Loss: 0.1622 | 0.1896
Epoch 58/300, seasonal_0 Loss: 0.1611 | 0.1892
Epoch 59/300, seasonal_0 Loss: 0.1604 | 0.1898
Epoch 60/300, seasonal_0 Loss: 0.1603 | 0.1902
Epoch 61/300, seasonal_0 Loss: 0.1580 | 0.1850
Epoch 62/300, seasonal_0 Loss: 0.1591 | 0.1875
Epoch 63/300, seasonal_0 Loss: 0.1585 | 0.1856
Epoch 64/300, seasonal_0 Loss: 0.1590 | 0.1837
Epoch 65/300, seasonal_0 Loss: 0.1568 | 0.1820
Epoch 66/300, seasonal_0 Loss: 0.1556 | 0.1820
Epoch 67/300, seasonal_0 Loss: 0.1552 | 0.1835
Epoch 68/300, seasonal_0 Loss: 0.1543 | 0.1810
Epoch 69/300, seasonal_0 Loss: 0.1521 | 0.1802
Epoch 70/300, seasonal_0 Loss: 0.1513 | 0.1793
Epoch 71/300, seasonal_0 Loss: 0.1515 | 0.1793
Epoch 72/300, seasonal_0 Loss: 0.1508 | 0.1769
Epoch 73/300, seasonal_0 Loss: 0.1500 | 0.1778
Epoch 74/300, seasonal_0 Loss: 0.1489 | 0.1762
Epoch 75/300, seasonal_0 Loss: 0.1487 | 0.1763
Epoch 76/300, seasonal_0 Loss: 0.1478 | 0.1751
Epoch 77/300, seasonal_0 Loss: 0.1476 | 0.1752
Epoch 78/300, seasonal_0 Loss: 0.1474 | 0.1740
Epoch 79/300, seasonal_0 Loss: 0.1464 | 0.1730
Epoch 80/300, seasonal_0 Loss: 0.1455 | 0.1743
Epoch 81/300, seasonal_0 Loss: 0.1462 | 0.1738
Epoch 82/300, seasonal_0 Loss: 0.1457 | 0.1727
Epoch 83/300, seasonal_0 Loss: 0.1449 | 0.1721
Epoch 84/300, seasonal_0 Loss: 0.1444 | 0.1719
Epoch 85/300, seasonal_0 Loss: 0.1445 | 0.1713
Epoch 86/300, seasonal_0 Loss: 0.1439 | 0.1713
Epoch 87/300, seasonal_0 Loss: 0.1438 | 0.1718
Epoch 88/300, seasonal_0 Loss: 0.1430 | 0.1701
Epoch 89/300, seasonal_0 Loss: 0.1434 | 0.1694
Epoch 90/300, seasonal_0 Loss: 0.1418 | 0.1686
Epoch 91/300, seasonal_0 Loss: 0.1428 | 0.1678
Epoch 92/300, seasonal_0 Loss: 0.1425 | 0.1680
Epoch 93/300, seasonal_0 Loss: 0.1423 | 0.1674
Epoch 94/300, seasonal_0 Loss: 0.1418 | 0.1677
Epoch 95/300, seasonal_0 Loss: 0.1413 | 0.1678
Epoch 96/300, seasonal_0 Loss: 0.1416 | 0.1669
Epoch 97/300, seasonal_0 Loss: 0.1410 | 0.1667
Epoch 98/300, seasonal_0 Loss: 0.1396 | 0.1670
Epoch 99/300, seasonal_0 Loss: 0.1402 | 0.1663
Epoch 100/300, seasonal_0 Loss: 0.1401 | 0.1666
Epoch 101/300, seasonal_0 Loss: 0.1403 | 0.1656
Epoch 102/300, seasonal_0 Loss: 0.1397 | 0.1654
Epoch 103/300, seasonal_0 Loss: 0.1391 | 0.1647
Epoch 104/300, seasonal_0 Loss: 0.1397 | 0.1650
Epoch 105/300, seasonal_0 Loss: 0.1397 | 0.1650
Epoch 106/300, seasonal_0 Loss: 0.1397 | 0.1647
Epoch 107/300, seasonal_0 Loss: 0.1391 | 0.1647
Epoch 108/300, seasonal_0 Loss: 0.1388 | 0.1644
Epoch 109/300, seasonal_0 Loss: 0.1390 | 0.1644
Epoch 110/300, seasonal_0 Loss: 0.1381 | 0.1636
Epoch 111/300, seasonal_0 Loss: 0.1379 | 0.1630
Epoch 112/300, seasonal_0 Loss: 0.1382 | 0.1628
Epoch 113/300, seasonal_0 Loss: 0.1385 | 0.1623
Epoch 114/300, seasonal_0 Loss: 0.1372 | 0.1621
Epoch 115/300, seasonal_0 Loss: 0.1376 | 0.1622
Epoch 116/300, seasonal_0 Loss: 0.1367 | 0.1623
Epoch 117/300, seasonal_0 Loss: 0.1378 | 0.1628
Epoch 118/300, seasonal_0 Loss: 0.1377 | 0.1627
Epoch 119/300, seasonal_0 Loss: 0.1374 | 0.1621
Epoch 120/300, seasonal_0 Loss: 0.1368 | 0.1614
Epoch 121/300, seasonal_0 Loss: 0.1372 | 0.1612
Epoch 122/300, seasonal_0 Loss: 0.1367 | 0.1613
Epoch 123/300, seasonal_0 Loss: 0.1367 | 0.1612
Epoch 124/300, seasonal_0 Loss: 0.1364 | 0.1614
Epoch 125/300, seasonal_0 Loss: 0.1369 | 0.1614
Epoch 126/300, seasonal_0 Loss: 0.1358 | 0.1609
Epoch 127/300, seasonal_0 Loss: 0.1356 | 0.1613
Epoch 128/300, seasonal_0 Loss: 0.1366 | 0.1614
Epoch 129/300, seasonal_0 Loss: 0.1359 | 0.1614
Epoch 130/300, seasonal_0 Loss: 0.1362 | 0.1609
Epoch 131/300, seasonal_0 Loss: 0.1361 | 0.1607
Epoch 132/300, seasonal_0 Loss: 0.1362 | 0.1607
Epoch 133/300, seasonal_0 Loss: 0.1363 | 0.1602
Epoch 134/300, seasonal_0 Loss: 0.1355 | 0.1602
Epoch 135/300, seasonal_0 Loss: 0.1361 | 0.1597
Epoch 136/300, seasonal_0 Loss: 0.1353 | 0.1601
Epoch 137/300, seasonal_0 Loss: 0.1350 | 0.1599
Epoch 138/300, seasonal_0 Loss: 0.1352 | 0.1600
Epoch 139/300, seasonal_0 Loss: 0.1350 | 0.1602
Epoch 140/300, seasonal_0 Loss: 0.1354 | 0.1600
Epoch 141/300, seasonal_0 Loss: 0.1351 | 0.1599
Epoch 142/300, seasonal_0 Loss: 0.1357 | 0.1597
Epoch 143/300, seasonal_0 Loss: 0.1356 | 0.1597
Epoch 144/300, seasonal_0 Loss: 0.1357 | 0.1593
Epoch 145/300, seasonal_0 Loss: 0.1353 | 0.1594
Epoch 146/300, seasonal_0 Loss: 0.1347 | 0.1595
Epoch 147/300, seasonal_0 Loss: 0.1345 | 0.1596
Epoch 148/300, seasonal_0 Loss: 0.1351 | 0.1593
Epoch 149/300, seasonal_0 Loss: 0.1355 | 0.1591
Epoch 150/300, seasonal_0 Loss: 0.1348 | 0.1589
Epoch 151/300, seasonal_0 Loss: 0.1347 | 0.1589
Epoch 152/300, seasonal_0 Loss: 0.1353 | 0.1590
Epoch 153/300, seasonal_0 Loss: 0.1346 | 0.1588
Epoch 154/300, seasonal_0 Loss: 0.1345 | 0.1587
Epoch 155/300, seasonal_0 Loss: 0.1342 | 0.1585
Epoch 156/300, seasonal_0 Loss: 0.1344 | 0.1584
Epoch 157/300, seasonal_0 Loss: 0.1344 | 0.1586
Epoch 158/300, seasonal_0 Loss: 0.1349 | 0.1585
Epoch 159/300, seasonal_0 Loss: 0.1343 | 0.1586
Epoch 160/300, seasonal_0 Loss: 0.1347 | 0.1587
Epoch 161/300, seasonal_0 Loss: 0.1343 | 0.1586
Epoch 162/300, seasonal_0 Loss: 0.1337 | 0.1584
Epoch 163/300, seasonal_0 Loss: 0.1337 | 0.1584
Epoch 164/300, seasonal_0 Loss: 0.1343 | 0.1584
Epoch 165/300, seasonal_0 Loss: 0.1344 | 0.1584
Epoch 166/300, seasonal_0 Loss: 0.1338 | 0.1584
Epoch 167/300, seasonal_0 Loss: 0.1338 | 0.1582
Epoch 168/300, seasonal_0 Loss: 0.1335 | 0.1583
Epoch 169/300, seasonal_0 Loss: 0.1346 | 0.1583
Epoch 170/300, seasonal_0 Loss: 0.1349 | 0.1584
Epoch 171/300, seasonal_0 Loss: 0.1336 | 0.1583
Epoch 172/300, seasonal_0 Loss: 0.1338 | 0.1582
Epoch 173/300, seasonal_0 Loss: 0.1338 | 0.1581
Epoch 174/300, seasonal_0 Loss: 0.1337 | 0.1580
Epoch 175/300, seasonal_0 Loss: 0.1338 | 0.1579
Epoch 176/300, seasonal_0 Loss: 0.1338 | 0.1578
Epoch 177/300, seasonal_0 Loss: 0.1344 | 0.1579
Epoch 178/300, seasonal_0 Loss: 0.1346 | 0.1579
Epoch 179/300, seasonal_0 Loss: 0.1340 | 0.1579
Epoch 180/300, seasonal_0 Loss: 0.1334 | 0.1578
Epoch 181/300, seasonal_0 Loss: 0.1335 | 0.1577
Epoch 182/300, seasonal_0 Loss: 0.1336 | 0.1576
Epoch 183/300, seasonal_0 Loss: 0.1330 | 0.1577
Epoch 184/300, seasonal_0 Loss: 0.1338 | 0.1577
Epoch 185/300, seasonal_0 Loss: 0.1342 | 0.1578
Epoch 186/300, seasonal_0 Loss: 0.1335 | 0.1578
Epoch 187/300, seasonal_0 Loss: 0.1338 | 0.1578
Epoch 188/300, seasonal_0 Loss: 0.1329 | 0.1577
Epoch 189/300, seasonal_0 Loss: 0.1337 | 0.1577
Epoch 190/300, seasonal_0 Loss: 0.1339 | 0.1577
Epoch 191/300, seasonal_0 Loss: 0.1337 | 0.1577
Epoch 192/300, seasonal_0 Loss: 0.1335 | 0.1576
Epoch 193/300, seasonal_0 Loss: 0.1331 | 0.1576
Epoch 194/300, seasonal_0 Loss: 0.1332 | 0.1577
Epoch 195/300, seasonal_0 Loss: 0.1333 | 0.1576
Epoch 196/300, seasonal_0 Loss: 0.1339 | 0.1577
Epoch 197/300, seasonal_0 Loss: 0.1332 | 0.1576
Epoch 198/300, seasonal_0 Loss: 0.1338 | 0.1575
Epoch 199/300, seasonal_0 Loss: 0.1331 | 0.1576
Epoch 200/300, seasonal_0 Loss: 0.1331 | 0.1576
Epoch 201/300, seasonal_0 Loss: 0.1338 | 0.1576
Epoch 202/300, seasonal_0 Loss: 0.1343 | 0.1576
Epoch 203/300, seasonal_0 Loss: 0.1333 | 0.1575
Epoch 204/300, seasonal_0 Loss: 0.1330 | 0.1576
Epoch 205/300, seasonal_0 Loss: 0.1332 | 0.1575
Epoch 206/300, seasonal_0 Loss: 0.1330 | 0.1575
Epoch 207/300, seasonal_0 Loss: 0.1336 | 0.1576
Epoch 208/300, seasonal_0 Loss: 0.1337 | 0.1576
Epoch 209/300, seasonal_0 Loss: 0.1330 | 0.1576
Epoch 210/300, seasonal_0 Loss: 0.1340 | 0.1576
Epoch 211/300, seasonal_0 Loss: 0.1343 | 0.1576
Epoch 212/300, seasonal_0 Loss: 0.1339 | 0.1575
Epoch 213/300, seasonal_0 Loss: 0.1343 | 0.1576
Epoch 214/300, seasonal_0 Loss: 0.1337 | 0.1576
Epoch 215/300, seasonal_0 Loss: 0.1330 | 0.1576
Epoch 216/300, seasonal_0 Loss: 0.1342 | 0.1576
Epoch 217/300, seasonal_0 Loss: 0.1336 | 0.1575
Epoch 218/300, seasonal_0 Loss: 0.1339 | 0.1575
Epoch 219/300, seasonal_0 Loss: 0.1337 | 0.1575
Epoch 220/300, seasonal_0 Loss: 0.1335 | 0.1575
Epoch 221/300, seasonal_0 Loss: 0.1336 | 0.1575
Epoch 222/300, seasonal_0 Loss: 0.1333 | 0.1575
Epoch 223/300, seasonal_0 Loss: 0.1336 | 0.1575
Epoch 224/300, seasonal_0 Loss: 0.1344 | 0.1575
Epoch 225/300, seasonal_0 Loss: 0.1332 | 0.1575
Epoch 226/300, seasonal_0 Loss: 0.1329 | 0.1575
Epoch 227/300, seasonal_0 Loss: 0.1330 | 0.1575
Epoch 228/300, seasonal_0 Loss: 0.1339 | 0.1575
Epoch 229/300, seasonal_0 Loss: 0.1331 | 0.1575
Epoch 230/300, seasonal_0 Loss: 0.1339 | 0.1575
Epoch 231/300, seasonal_0 Loss: 0.1332 | 0.1575
Epoch 232/300, seasonal_0 Loss: 0.1338 | 0.1575
Epoch 233/300, seasonal_0 Loss: 0.1333 | 0.1575
Epoch 234/300, seasonal_0 Loss: 0.1337 | 0.1574
Epoch 235/300, seasonal_0 Loss: 0.1335 | 0.1574
Epoch 236/300, seasonal_0 Loss: 0.1334 | 0.1574
Epoch 237/300, seasonal_0 Loss: 0.1340 | 0.1574
Epoch 238/300, seasonal_0 Loss: 0.1325 | 0.1574
Epoch 239/300, seasonal_0 Loss: 0.1329 | 0.1574
Epoch 240/300, seasonal_0 Loss: 0.1335 | 0.1574
Epoch 241/300, seasonal_0 Loss: 0.1340 | 0.1574
Epoch 242/300, seasonal_0 Loss: 0.1329 | 0.1574
Epoch 243/300, seasonal_0 Loss: 0.1336 | 0.1574
Epoch 244/300, seasonal_0 Loss: 0.1335 | 0.1574
Epoch 245/300, seasonal_0 Loss: 0.1335 | 0.1574
Epoch 246/300, seasonal_0 Loss: 0.1330 | 0.1574
Epoch 247/300, seasonal_0 Loss: 0.1331 | 0.1574
Epoch 248/300, seasonal_0 Loss: 0.1334 | 0.1574
Epoch 249/300, seasonal_0 Loss: 0.1339 | 0.1574
Epoch 250/300, seasonal_0 Loss: 0.1342 | 0.1574
Epoch 251/300, seasonal_0 Loss: 0.1338 | 0.1574
Epoch 252/300, seasonal_0 Loss: 0.1338 | 0.1574
Epoch 253/300, seasonal_0 Loss: 0.1331 | 0.1574
Epoch 254/300, seasonal_0 Loss: 0.1338 | 0.1574
Epoch 255/300, seasonal_0 Loss: 0.1337 | 0.1574
Epoch 256/300, seasonal_0 Loss: 0.1334 | 0.1573
Epoch 257/300, seasonal_0 Loss: 0.1345 | 0.1573
Epoch 258/300, seasonal_0 Loss: 0.1333 | 0.1573
Epoch 259/300, seasonal_0 Loss: 0.1333 | 0.1573
Epoch 260/300, seasonal_0 Loss: 0.1341 | 0.1573
Epoch 261/300, seasonal_0 Loss: 0.1338 | 0.1573
Epoch 262/300, seasonal_0 Loss: 0.1329 | 0.1573
Epoch 263/300, seasonal_0 Loss: 0.1340 | 0.1573
Epoch 264/300, seasonal_0 Loss: 0.1328 | 0.1573
Epoch 265/300, seasonal_0 Loss: 0.1334 | 0.1573
Epoch 266/300, seasonal_0 Loss: 0.1343 | 0.1573
Epoch 267/300, seasonal_0 Loss: 0.1337 | 0.1573
Epoch 268/300, seasonal_0 Loss: 0.1337 | 0.1573
Epoch 269/300, seasonal_0 Loss: 0.1333 | 0.1573
Epoch 270/300, seasonal_0 Loss: 0.1331 | 0.1573
Epoch 271/300, seasonal_0 Loss: 0.1335 | 0.1573
Epoch 272/300, seasonal_0 Loss: 0.1341 | 0.1573
Epoch 273/300, seasonal_0 Loss: 0.1332 | 0.1573
Epoch 274/300, seasonal_0 Loss: 0.1339 | 0.1573
Epoch 275/300, seasonal_0 Loss: 0.1341 | 0.1573
Epoch 276/300, seasonal_0 Loss: 0.1330 | 0.1573
Epoch 277/300, seasonal_0 Loss: 0.1333 | 0.1573
Epoch 278/300, seasonal_0 Loss: 0.1332 | 0.1573
Epoch 279/300, seasonal_0 Loss: 0.1336 | 0.1573
Epoch 280/300, seasonal_0 Loss: 0.1330 | 0.1573
Epoch 281/300, seasonal_0 Loss: 0.1337 | 0.1573
Epoch 282/300, seasonal_0 Loss: 0.1329 | 0.1573
Epoch 283/300, seasonal_0 Loss: 0.1337 | 0.1573
Epoch 284/300, seasonal_0 Loss: 0.1335 | 0.1573
Epoch 285/300, seasonal_0 Loss: 0.1328 | 0.1573
Epoch 286/300, seasonal_0 Loss: 0.1329 | 0.1573
Epoch 287/300, seasonal_0 Loss: 0.1340 | 0.1573
Epoch 288/300, seasonal_0 Loss: 0.1341 | 0.1573
Epoch 289/300, seasonal_0 Loss: 0.1328 | 0.1573
Epoch 290/300, seasonal_0 Loss: 0.1333 | 0.1573
Epoch 291/300, seasonal_0 Loss: 0.1327 | 0.1573
Epoch 292/300, seasonal_0 Loss: 0.1338 | 0.1573
Epoch 293/300, seasonal_0 Loss: 0.1335 | 0.1573
Epoch 294/300, seasonal_0 Loss: 0.1328 | 0.1573
Epoch 295/300, seasonal_0 Loss: 0.1333 | 0.1573
Epoch 296/300, seasonal_0 Loss: 0.1336 | 0.1573
Epoch 297/300, seasonal_0 Loss: 0.1334 | 0.1573
Epoch 298/300, seasonal_0 Loss: 0.1331 | 0.1573
Epoch 299/300, seasonal_0 Loss: 0.1334 | 0.1573
Epoch 300/300, seasonal_0 Loss: 0.1328 | 0.1573
Training seasonal_1 component with params: {'observation_period_num': 61, 'train_rates': 0.9856129269576481, 'learning_rate': 9.288695925078009e-05, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9067493424897287}
Epoch 1/300, seasonal_1 Loss: 1.1374 | 1.9489
Epoch 2/300, seasonal_1 Loss: 0.7365 | 1.2429
Epoch 3/300, seasonal_1 Loss: 0.6011 | 0.9969
Epoch 4/300, seasonal_1 Loss: 0.5166 | 0.9292
Epoch 5/300, seasonal_1 Loss: 0.4596 | 0.8797
Epoch 6/300, seasonal_1 Loss: 0.4294 | 0.8352
Epoch 7/300, seasonal_1 Loss: 0.4020 | 0.7997
Epoch 8/300, seasonal_1 Loss: 0.4059 | 0.7443
Epoch 9/300, seasonal_1 Loss: 0.3786 | 0.7144
Epoch 10/300, seasonal_1 Loss: 0.3898 | 0.7124
Epoch 11/300, seasonal_1 Loss: 0.3917 | 0.6625
Epoch 12/300, seasonal_1 Loss: 0.3694 | 0.6574
Epoch 13/300, seasonal_1 Loss: 0.3467 | 0.6259
Epoch 14/300, seasonal_1 Loss: 0.3283 | 0.6096
Epoch 15/300, seasonal_1 Loss: 0.3047 | 0.5871
Epoch 16/300, seasonal_1 Loss: 0.2970 | 0.5706
Epoch 17/300, seasonal_1 Loss: 0.2846 | 0.5478
Epoch 18/300, seasonal_1 Loss: 0.2861 | 0.5431
Epoch 19/300, seasonal_1 Loss: 0.2855 | 0.5189
Epoch 20/300, seasonal_1 Loss: 0.2747 | 0.5078
Epoch 21/300, seasonal_1 Loss: 0.2726 | 0.5003
Epoch 22/300, seasonal_1 Loss: 0.2572 | 0.4867
Epoch 23/300, seasonal_1 Loss: 0.2551 | 0.4766
Epoch 24/300, seasonal_1 Loss: 0.2669 | 0.4639
Epoch 25/300, seasonal_1 Loss: 0.2587 | 0.4538
Epoch 26/300, seasonal_1 Loss: 0.2634 | 0.4527
Epoch 27/300, seasonal_1 Loss: 0.2512 | 0.4433
Epoch 28/300, seasonal_1 Loss: 0.2492 | 0.4319
Epoch 29/300, seasonal_1 Loss: 0.2522 | 0.4249
Epoch 30/300, seasonal_1 Loss: 0.2372 | 0.4154
Epoch 31/300, seasonal_1 Loss: 0.2336 | 0.4133
Epoch 32/300, seasonal_1 Loss: 0.2360 | 0.4020
Epoch 33/300, seasonal_1 Loss: 0.2336 | 0.3956
Epoch 34/300, seasonal_1 Loss: 0.2326 | 0.3891
Epoch 35/300, seasonal_1 Loss: 0.2249 | 0.3865
Epoch 36/300, seasonal_1 Loss: 0.2232 | 0.3792
Epoch 37/300, seasonal_1 Loss: 0.2267 | 0.3746
Epoch 38/300, seasonal_1 Loss: 0.2182 | 0.3684
Epoch 39/300, seasonal_1 Loss: 0.2158 | 0.3653
Epoch 40/300, seasonal_1 Loss: 0.2098 | 0.3594
Epoch 41/300, seasonal_1 Loss: 0.2089 | 0.3575
Epoch 42/300, seasonal_1 Loss: 0.2090 | 0.3485
Epoch 43/300, seasonal_1 Loss: 0.2062 | 0.3468
Epoch 44/300, seasonal_1 Loss: 0.2051 | 0.3406
Epoch 45/300, seasonal_1 Loss: 0.2037 | 0.3377
Epoch 46/300, seasonal_1 Loss: 0.2023 | 0.3328
Epoch 47/300, seasonal_1 Loss: 0.2011 | 0.3311
Epoch 48/300, seasonal_1 Loss: 0.2004 | 0.3272
Epoch 49/300, seasonal_1 Loss: 0.1984 | 0.3247
Epoch 50/300, seasonal_1 Loss: 0.1963 | 0.3207
Epoch 51/300, seasonal_1 Loss: 0.1954 | 0.3173
Epoch 52/300, seasonal_1 Loss: 0.1942 | 0.3128
Epoch 53/300, seasonal_1 Loss: 0.1925 | 0.3123
Epoch 54/300, seasonal_1 Loss: 0.1913 | 0.3075
Epoch 55/300, seasonal_1 Loss: 0.1906 | 0.3051
Epoch 56/300, seasonal_1 Loss: 0.1896 | 0.3030
Epoch 57/300, seasonal_1 Loss: 0.1893 | 0.3000
Epoch 58/300, seasonal_1 Loss: 0.1888 | 0.2985
Epoch 59/300, seasonal_1 Loss: 0.1877 | 0.2943
Epoch 60/300, seasonal_1 Loss: 0.1873 | 0.2932
Epoch 61/300, seasonal_1 Loss: 0.1863 | 0.2912
Epoch 62/300, seasonal_1 Loss: 0.1854 | 0.2882
Epoch 63/300, seasonal_1 Loss: 0.1858 | 0.2864
Epoch 64/300, seasonal_1 Loss: 0.1846 | 0.2838
Epoch 65/300, seasonal_1 Loss: 0.1834 | 0.2835
Epoch 66/300, seasonal_1 Loss: 0.1825 | 0.2799
Epoch 67/300, seasonal_1 Loss: 0.1818 | 0.2782
Epoch 68/300, seasonal_1 Loss: 0.1815 | 0.2770
Epoch 69/300, seasonal_1 Loss: 0.1813 | 0.2749
Epoch 70/300, seasonal_1 Loss: 0.1801 | 0.2727
Epoch 71/300, seasonal_1 Loss: 0.1797 | 0.2722
Epoch 72/300, seasonal_1 Loss: 0.1792 | 0.2701
Epoch 73/300, seasonal_1 Loss: 0.1789 | 0.2683
Epoch 74/300, seasonal_1 Loss: 0.1788 | 0.2670
Epoch 75/300, seasonal_1 Loss: 0.1774 | 0.2662
Epoch 76/300, seasonal_1 Loss: 0.1778 | 0.2641
Epoch 77/300, seasonal_1 Loss: 0.1764 | 0.2624
Epoch 78/300, seasonal_1 Loss: 0.1758 | 0.2618
Epoch 79/300, seasonal_1 Loss: 0.1757 | 0.2602
Epoch 80/300, seasonal_1 Loss: 0.1759 | 0.2588
Epoch 81/300, seasonal_1 Loss: 0.1755 | 0.2569
Epoch 82/300, seasonal_1 Loss: 0.1745 | 0.2568
Epoch 83/300, seasonal_1 Loss: 0.1744 | 0.2551
Epoch 84/300, seasonal_1 Loss: 0.1738 | 0.2533
Epoch 85/300, seasonal_1 Loss: 0.1736 | 0.2527
Epoch 86/300, seasonal_1 Loss: 0.1734 | 0.2514
Epoch 87/300, seasonal_1 Loss: 0.1722 | 0.2505
Epoch 88/300, seasonal_1 Loss: 0.1717 | 0.2500
Epoch 89/300, seasonal_1 Loss: 0.1722 | 0.2482
Epoch 90/300, seasonal_1 Loss: 0.1719 | 0.2481
Epoch 91/300, seasonal_1 Loss: 0.1710 | 0.2464
Epoch 92/300, seasonal_1 Loss: 0.1708 | 0.2451
Epoch 93/300, seasonal_1 Loss: 0.1706 | 0.2451
Epoch 94/300, seasonal_1 Loss: 0.1704 | 0.2439
Epoch 95/300, seasonal_1 Loss: 0.1704 | 0.2428
Epoch 96/300, seasonal_1 Loss: 0.1697 | 0.2420
Epoch 97/300, seasonal_1 Loss: 0.1692 | 0.2413
Epoch 98/300, seasonal_1 Loss: 0.1686 | 0.2400
Epoch 99/300, seasonal_1 Loss: 0.1689 | 0.2393
Epoch 100/300, seasonal_1 Loss: 0.1679 | 0.2379
Epoch 101/300, seasonal_1 Loss: 0.1678 | 0.2371
Epoch 102/300, seasonal_1 Loss: 0.1681 | 0.2363
Epoch 103/300, seasonal_1 Loss: 0.1675 | 0.2361
Epoch 104/300, seasonal_1 Loss: 0.1668 | 0.2354
Epoch 105/300, seasonal_1 Loss: 0.1668 | 0.2346
Epoch 106/300, seasonal_1 Loss: 0.1663 | 0.2340
Epoch 107/300, seasonal_1 Loss: 0.1670 | 0.2333
Epoch 108/300, seasonal_1 Loss: 0.1662 | 0.2322
Epoch 109/300, seasonal_1 Loss: 0.1658 | 0.2313
Epoch 110/300, seasonal_1 Loss: 0.1658 | 0.2309
Epoch 111/300, seasonal_1 Loss: 0.1652 | 0.2306
Epoch 112/300, seasonal_1 Loss: 0.1652 | 0.2302
Epoch 113/300, seasonal_1 Loss: 0.1645 | 0.2301
Epoch 114/300, seasonal_1 Loss: 0.1651 | 0.2288
Epoch 115/300, seasonal_1 Loss: 0.1646 | 0.2284
Epoch 116/300, seasonal_1 Loss: 0.1643 | 0.2285
Epoch 117/300, seasonal_1 Loss: 0.1642 | 0.2270
Epoch 118/300, seasonal_1 Loss: 0.1645 | 0.2269
Epoch 119/300, seasonal_1 Loss: 0.1641 | 0.2260
Epoch 120/300, seasonal_1 Loss: 0.1646 | 0.2249
Epoch 121/300, seasonal_1 Loss: 0.1640 | 0.2254
Epoch 122/300, seasonal_1 Loss: 0.1639 | 0.2249
Epoch 123/300, seasonal_1 Loss: 0.1632 | 0.2246
Epoch 124/300, seasonal_1 Loss: 0.1629 | 0.2240
Epoch 125/300, seasonal_1 Loss: 0.1629 | 0.2241
Epoch 126/300, seasonal_1 Loss: 0.1630 | 0.2237
Epoch 127/300, seasonal_1 Loss: 0.1624 | 0.2233
Epoch 128/300, seasonal_1 Loss: 0.1625 | 0.2230
Epoch 129/300, seasonal_1 Loss: 0.1623 | 0.2224
Epoch 130/300, seasonal_1 Loss: 0.1619 | 0.2221
Epoch 131/300, seasonal_1 Loss: 0.1615 | 0.2218
Epoch 132/300, seasonal_1 Loss: 0.1617 | 0.2219
Epoch 133/300, seasonal_1 Loss: 0.1614 | 0.2213
Epoch 134/300, seasonal_1 Loss: 0.1614 | 0.2203
Epoch 135/300, seasonal_1 Loss: 0.1611 | 0.2202
Epoch 136/300, seasonal_1 Loss: 0.1612 | 0.2201
Epoch 137/300, seasonal_1 Loss: 0.1613 | 0.2195
Epoch 138/300, seasonal_1 Loss: 0.1609 | 0.2192
Epoch 139/300, seasonal_1 Loss: 0.1609 | 0.2191
Epoch 140/300, seasonal_1 Loss: 0.1610 | 0.2184
Epoch 141/300, seasonal_1 Loss: 0.1602 | 0.2184
Epoch 142/300, seasonal_1 Loss: 0.1604 | 0.2182
Epoch 143/300, seasonal_1 Loss: 0.1607 | 0.2178
Epoch 144/300, seasonal_1 Loss: 0.1597 | 0.2176
Epoch 145/300, seasonal_1 Loss: 0.1596 | 0.2169
Epoch 146/300, seasonal_1 Loss: 0.1607 | 0.2168
Epoch 147/300, seasonal_1 Loss: 0.1604 | 0.2167
Epoch 148/300, seasonal_1 Loss: 0.1602 | 0.2162
Epoch 149/300, seasonal_1 Loss: 0.1600 | 0.2161
Epoch 150/300, seasonal_1 Loss: 0.1595 | 0.2161
Epoch 151/300, seasonal_1 Loss: 0.1597 | 0.2154
Epoch 152/300, seasonal_1 Loss: 0.1595 | 0.2156
Epoch 153/300, seasonal_1 Loss: 0.1595 | 0.2153
Epoch 154/300, seasonal_1 Loss: 0.1593 | 0.2146
Epoch 155/300, seasonal_1 Loss: 0.1594 | 0.2143
Epoch 156/300, seasonal_1 Loss: 0.1595 | 0.2139
Epoch 157/300, seasonal_1 Loss: 0.1585 | 0.2138
Epoch 158/300, seasonal_1 Loss: 0.1594 | 0.2137
Epoch 159/300, seasonal_1 Loss: 0.1588 | 0.2136
Epoch 160/300, seasonal_1 Loss: 0.1585 | 0.2133
Epoch 161/300, seasonal_1 Loss: 0.1591 | 0.2131
Epoch 162/300, seasonal_1 Loss: 0.1585 | 0.2129
Epoch 163/300, seasonal_1 Loss: 0.1588 | 0.2129
Epoch 164/300, seasonal_1 Loss: 0.1588 | 0.2128
Epoch 165/300, seasonal_1 Loss: 0.1590 | 0.2124
Epoch 166/300, seasonal_1 Loss: 0.1581 | 0.2118
Epoch 167/300, seasonal_1 Loss: 0.1582 | 0.2118
Epoch 168/300, seasonal_1 Loss: 0.1582 | 0.2116
Epoch 169/300, seasonal_1 Loss: 0.1581 | 0.2117
Epoch 170/300, seasonal_1 Loss: 0.1582 | 0.2117
Epoch 171/300, seasonal_1 Loss: 0.1580 | 0.2112
Epoch 172/300, seasonal_1 Loss: 0.1577 | 0.2110
Epoch 173/300, seasonal_1 Loss: 0.1580 | 0.2106
Epoch 174/300, seasonal_1 Loss: 0.1575 | 0.2105
Epoch 175/300, seasonal_1 Loss: 0.1582 | 0.2109
Epoch 176/300, seasonal_1 Loss: 0.1575 | 0.2107
Epoch 177/300, seasonal_1 Loss: 0.1572 | 0.2106
Epoch 178/300, seasonal_1 Loss: 0.1570 | 0.2106
Epoch 179/300, seasonal_1 Loss: 0.1577 | 0.2103
Epoch 180/300, seasonal_1 Loss: 0.1575 | 0.2100
Epoch 181/300, seasonal_1 Loss: 0.1572 | 0.2095
Epoch 182/300, seasonal_1 Loss: 0.1576 | 0.2093
Epoch 183/300, seasonal_1 Loss: 0.1577 | 0.2091
Epoch 184/300, seasonal_1 Loss: 0.1572 | 0.2091
Epoch 185/300, seasonal_1 Loss: 0.1568 | 0.2091
Epoch 186/300, seasonal_1 Loss: 0.1572 | 0.2088
Epoch 187/300, seasonal_1 Loss: 0.1570 | 0.2086
Epoch 188/300, seasonal_1 Loss: 0.1566 | 0.2084
Epoch 189/300, seasonal_1 Loss: 0.1565 | 0.2083
Epoch 190/300, seasonal_1 Loss: 0.1568 | 0.2083
Epoch 191/300, seasonal_1 Loss: 0.1564 | 0.2083
Epoch 192/300, seasonal_1 Loss: 0.1570 | 0.2083
Epoch 193/300, seasonal_1 Loss: 0.1560 | 0.2082
Epoch 194/300, seasonal_1 Loss: 0.1573 | 0.2079
Epoch 195/300, seasonal_1 Loss: 0.1565 | 0.2077
Epoch 196/300, seasonal_1 Loss: 0.1564 | 0.2076
Epoch 197/300, seasonal_1 Loss: 0.1566 | 0.2077
Epoch 198/300, seasonal_1 Loss: 0.1567 | 0.2075
Epoch 199/300, seasonal_1 Loss: 0.1566 | 0.2074
Epoch 200/300, seasonal_1 Loss: 0.1562 | 0.2072
Epoch 201/300, seasonal_1 Loss: 0.1564 | 0.2072
Epoch 202/300, seasonal_1 Loss: 0.1562 | 0.2071
Epoch 203/300, seasonal_1 Loss: 0.1561 | 0.2068
Epoch 204/300, seasonal_1 Loss: 0.1555 | 0.2066
Epoch 205/300, seasonal_1 Loss: 0.1561 | 0.2068
Epoch 206/300, seasonal_1 Loss: 0.1563 | 0.2067
Epoch 207/300, seasonal_1 Loss: 0.1562 | 0.2065
Epoch 208/300, seasonal_1 Loss: 0.1552 | 0.2065
Epoch 209/300, seasonal_1 Loss: 0.1556 | 0.2064
Epoch 210/300, seasonal_1 Loss: 0.1564 | 0.2063
Epoch 211/300, seasonal_1 Loss: 0.1562 | 0.2064
Epoch 212/300, seasonal_1 Loss: 0.1565 | 0.2063
Epoch 213/300, seasonal_1 Loss: 0.1559 | 0.2062
Epoch 214/300, seasonal_1 Loss: 0.1566 | 0.2059
Epoch 215/300, seasonal_1 Loss: 0.1553 | 0.2059
Epoch 216/300, seasonal_1 Loss: 0.1563 | 0.2058
Epoch 217/300, seasonal_1 Loss: 0.1559 | 0.2057
Epoch 218/300, seasonal_1 Loss: 0.1554 | 0.2056
Epoch 219/300, seasonal_1 Loss: 0.1558 | 0.2055
Epoch 220/300, seasonal_1 Loss: 0.1550 | 0.2055
Epoch 221/300, seasonal_1 Loss: 0.1549 | 0.2055
Epoch 222/300, seasonal_1 Loss: 0.1556 | 0.2055
Epoch 223/300, seasonal_1 Loss: 0.1552 | 0.2055
Epoch 224/300, seasonal_1 Loss: 0.1554 | 0.2056
Epoch 225/300, seasonal_1 Loss: 0.1564 | 0.2055
Epoch 226/300, seasonal_1 Loss: 0.1555 | 0.2055
Epoch 227/300, seasonal_1 Loss: 0.1560 | 0.2054
Epoch 228/300, seasonal_1 Loss: 0.1555 | 0.2053
Epoch 229/300, seasonal_1 Loss: 0.1555 | 0.2053
Epoch 230/300, seasonal_1 Loss: 0.1554 | 0.2051
Epoch 231/300, seasonal_1 Loss: 0.1553 | 0.2050
Epoch 232/300, seasonal_1 Loss: 0.1550 | 0.2049
Epoch 233/300, seasonal_1 Loss: 0.1561 | 0.2049
Epoch 234/300, seasonal_1 Loss: 0.1555 | 0.2048
Epoch 235/300, seasonal_1 Loss: 0.1547 | 0.2048
Epoch 236/300, seasonal_1 Loss: 0.1550 | 0.2047
Epoch 237/300, seasonal_1 Loss: 0.1549 | 0.2047
Epoch 238/300, seasonal_1 Loss: 0.1549 | 0.2047
Epoch 239/300, seasonal_1 Loss: 0.1558 | 0.2045
Epoch 240/300, seasonal_1 Loss: 0.1550 | 0.2044
Epoch 241/300, seasonal_1 Loss: 0.1555 | 0.2043
Epoch 242/300, seasonal_1 Loss: 0.1549 | 0.2043
Epoch 243/300, seasonal_1 Loss: 0.1553 | 0.2042
Epoch 244/300, seasonal_1 Loss: 0.1555 | 0.2042
Epoch 245/300, seasonal_1 Loss: 0.1554 | 0.2043
Epoch 246/300, seasonal_1 Loss: 0.1555 | 0.2043
Epoch 247/300, seasonal_1 Loss: 0.1560 | 0.2043
Epoch 248/300, seasonal_1 Loss: 0.1550 | 0.2043
Epoch 249/300, seasonal_1 Loss: 0.1553 | 0.2043
Epoch 250/300, seasonal_1 Loss: 0.1550 | 0.2042
Epoch 251/300, seasonal_1 Loss: 0.1544 | 0.2042
Epoch 252/300, seasonal_1 Loss: 0.1548 | 0.2042
Epoch 253/300, seasonal_1 Loss: 0.1553 | 0.2042
Epoch 254/300, seasonal_1 Loss: 0.1548 | 0.2042
Epoch 255/300, seasonal_1 Loss: 0.1549 | 0.2042
Epoch 256/300, seasonal_1 Loss: 0.1552 | 0.2041
Epoch 257/300, seasonal_1 Loss: 0.1547 | 0.2041
Epoch 258/300, seasonal_1 Loss: 0.1556 | 0.2041
Epoch 259/300, seasonal_1 Loss: 0.1549 | 0.2040
Epoch 260/300, seasonal_1 Loss: 0.1547 | 0.2040
Epoch 261/300, seasonal_1 Loss: 0.1550 | 0.2040
Epoch 262/300, seasonal_1 Loss: 0.1548 | 0.2040
Epoch 263/300, seasonal_1 Loss: 0.1545 | 0.2039
Epoch 264/300, seasonal_1 Loss: 0.1543 | 0.2039
Epoch 265/300, seasonal_1 Loss: 0.1550 | 0.2039
Epoch 266/300, seasonal_1 Loss: 0.1550 | 0.2039
Epoch 267/300, seasonal_1 Loss: 0.1551 | 0.2039
Epoch 268/300, seasonal_1 Loss: 0.1552 | 0.2039
Epoch 269/300, seasonal_1 Loss: 0.1550 | 0.2039
Epoch 270/300, seasonal_1 Loss: 0.1546 | 0.2038
Epoch 271/300, seasonal_1 Loss: 0.1548 | 0.2038
Epoch 272/300, seasonal_1 Loss: 0.1544 | 0.2038
Epoch 273/300, seasonal_1 Loss: 0.1550 | 0.2037
Epoch 274/300, seasonal_1 Loss: 0.1547 | 0.2037
Epoch 275/300, seasonal_1 Loss: 0.1547 | 0.2037
Epoch 276/300, seasonal_1 Loss: 0.1548 | 0.2037
Epoch 277/300, seasonal_1 Loss: 0.1551 | 0.2038
Epoch 278/300, seasonal_1 Loss: 0.1548 | 0.2038
Epoch 279/300, seasonal_1 Loss: 0.1542 | 0.2038
Epoch 280/300, seasonal_1 Loss: 0.1549 | 0.2037
Epoch 281/300, seasonal_1 Loss: 0.1543 | 0.2037
Epoch 282/300, seasonal_1 Loss: 0.1544 | 0.2037
Epoch 283/300, seasonal_1 Loss: 0.1545 | 0.2037
Epoch 284/300, seasonal_1 Loss: 0.1551 | 0.2037
Epoch 285/300, seasonal_1 Loss: 0.1554 | 0.2036
Epoch 286/300, seasonal_1 Loss: 0.1544 | 0.2036
Epoch 287/300, seasonal_1 Loss: 0.1544 | 0.2035
Epoch 288/300, seasonal_1 Loss: 0.1547 | 0.2035
Epoch 289/300, seasonal_1 Loss: 0.1554 | 0.2034
Epoch 290/300, seasonal_1 Loss: 0.1550 | 0.2034
Epoch 291/300, seasonal_1 Loss: 0.1547 | 0.2034
Epoch 292/300, seasonal_1 Loss: 0.1548 | 0.2034
Epoch 293/300, seasonal_1 Loss: 0.1544 | 0.2034
Epoch 294/300, seasonal_1 Loss: 0.1547 | 0.2034
Epoch 295/300, seasonal_1 Loss: 0.1542 | 0.2033
Epoch 296/300, seasonal_1 Loss: 0.1542 | 0.2033
Epoch 297/300, seasonal_1 Loss: 0.1546 | 0.2033
Epoch 298/300, seasonal_1 Loss: 0.1551 | 0.2033
Epoch 299/300, seasonal_1 Loss: 0.1553 | 0.2033
Epoch 300/300, seasonal_1 Loss: 0.1548 | 0.2033
Training seasonal_2 component with params: {'observation_period_num': 60, 'train_rates': 0.9836918505385573, 'learning_rate': 0.00022257386220546392, 'batch_size': 41, 'step_size': 9, 'gamma': 0.86014725732381}
Epoch 1/300, seasonal_2 Loss: 0.8103 | 1.0333
Epoch 2/300, seasonal_2 Loss: 0.7260 | 0.9250
Epoch 3/300, seasonal_2 Loss: 0.7262 | 0.7585
Epoch 4/300, seasonal_2 Loss: 0.6366 | 0.7133
Epoch 5/300, seasonal_2 Loss: 0.5972 | 0.7774
Epoch 6/300, seasonal_2 Loss: 0.5004 | 0.6344
Epoch 7/300, seasonal_2 Loss: 0.4345 | 0.6340
Epoch 8/300, seasonal_2 Loss: 0.3923 | 0.6223
Epoch 9/300, seasonal_2 Loss: 0.3530 | 0.4610
Epoch 10/300, seasonal_2 Loss: 0.4737 | 0.4580
Epoch 11/300, seasonal_2 Loss: 0.3913 | 0.4441
Epoch 12/300, seasonal_2 Loss: 0.3607 | 0.4931
Epoch 13/300, seasonal_2 Loss: 0.3954 | 0.4689
Epoch 14/300, seasonal_2 Loss: 0.3524 | 0.3930
Epoch 15/300, seasonal_2 Loss: 0.4070 | 0.4429
Epoch 16/300, seasonal_2 Loss: 0.3481 | 0.4120
Epoch 17/300, seasonal_2 Loss: 0.3191 | 0.4352
Epoch 18/300, seasonal_2 Loss: 0.2980 | 0.3382
Epoch 19/300, seasonal_2 Loss: 0.2728 | 0.3459
Epoch 20/300, seasonal_2 Loss: 0.2699 | 0.3487
Epoch 21/300, seasonal_2 Loss: 0.2575 | 0.3062
Epoch 22/300, seasonal_2 Loss: 0.2570 | 0.3311
Epoch 23/300, seasonal_2 Loss: 0.2386 | 0.3067
Epoch 24/300, seasonal_2 Loss: 0.2310 | 0.2858
Epoch 25/300, seasonal_2 Loss: 0.2189 | 0.2845
Epoch 26/300, seasonal_2 Loss: 0.2165 | 0.2814
Epoch 27/300, seasonal_2 Loss: 0.2109 | 0.2683
Epoch 28/300, seasonal_2 Loss: 0.2043 | 0.2584
Epoch 29/300, seasonal_2 Loss: 0.2016 | 0.2648
Epoch 30/300, seasonal_2 Loss: 0.1990 | 0.2771
Epoch 31/300, seasonal_2 Loss: 0.1966 | 0.2458
Epoch 32/300, seasonal_2 Loss: 0.1964 | 0.2644
Epoch 33/300, seasonal_2 Loss: 0.1985 | 0.2585
Epoch 34/300, seasonal_2 Loss: 0.1924 | 0.2366
Epoch 35/300, seasonal_2 Loss: 0.1973 | 0.2566
Epoch 36/300, seasonal_2 Loss: 0.1930 | 0.2488
Epoch 37/300, seasonal_2 Loss: 0.1901 | 0.2289
Epoch 38/300, seasonal_2 Loss: 0.1852 | 0.2286
Epoch 39/300, seasonal_2 Loss: 0.1943 | 0.2505
Epoch 40/300, seasonal_2 Loss: 0.1810 | 0.2182
Epoch 41/300, seasonal_2 Loss: 0.1804 | 0.2107
Epoch 42/300, seasonal_2 Loss: 0.1779 | 0.2338
Epoch 43/300, seasonal_2 Loss: 0.1795 | 0.2226
Epoch 44/300, seasonal_2 Loss: 0.1769 | 0.2073
Epoch 45/300, seasonal_2 Loss: 0.1722 | 0.2200
Epoch 46/300, seasonal_2 Loss: 0.1739 | 0.2302
Epoch 47/300, seasonal_2 Loss: 0.1674 | 0.1964
Epoch 48/300, seasonal_2 Loss: 0.1649 | 0.2126
Epoch 49/300, seasonal_2 Loss: 0.1648 | 0.2052
Epoch 50/300, seasonal_2 Loss: 0.1591 | 0.1998
Epoch 51/300, seasonal_2 Loss: 0.1557 | 0.1942
Epoch 52/300, seasonal_2 Loss: 0.1553 | 0.1980
Epoch 53/300, seasonal_2 Loss: 0.1523 | 0.1922
Epoch 54/300, seasonal_2 Loss: 0.1509 | 0.1878
Epoch 55/300, seasonal_2 Loss: 0.1499 | 0.1886
Epoch 56/300, seasonal_2 Loss: 0.1489 | 0.1903
Epoch 57/300, seasonal_2 Loss: 0.1484 | 0.1857
Epoch 58/300, seasonal_2 Loss: 0.1465 | 0.1822
Epoch 59/300, seasonal_2 Loss: 0.1461 | 0.1868
Epoch 60/300, seasonal_2 Loss: 0.1448 | 0.1791
Epoch 61/300, seasonal_2 Loss: 0.1436 | 0.1836
Epoch 62/300, seasonal_2 Loss: 0.1428 | 0.1799
Epoch 63/300, seasonal_2 Loss: 0.1433 | 0.1805
Epoch 64/300, seasonal_2 Loss: 0.1412 | 0.1781
Epoch 65/300, seasonal_2 Loss: 0.1402 | 0.1755
Epoch 66/300, seasonal_2 Loss: 0.1390 | 0.1728
Epoch 67/300, seasonal_2 Loss: 0.1383 | 0.1762
Epoch 68/300, seasonal_2 Loss: 0.1381 | 0.1745
Epoch 69/300, seasonal_2 Loss: 0.1375 | 0.1712
Epoch 70/300, seasonal_2 Loss: 0.1368 | 0.1692
Epoch 71/300, seasonal_2 Loss: 0.1356 | 0.1772
Epoch 72/300, seasonal_2 Loss: 0.1352 | 0.1707
Epoch 73/300, seasonal_2 Loss: 0.1349 | 0.1646
Epoch 74/300, seasonal_2 Loss: 0.1340 | 0.1702
Epoch 75/300, seasonal_2 Loss: 0.1330 | 0.1683
Epoch 76/300, seasonal_2 Loss: 0.1327 | 0.1651
Epoch 77/300, seasonal_2 Loss: 0.1316 | 0.1636
Epoch 78/300, seasonal_2 Loss: 0.1313 | 0.1681
Epoch 79/300, seasonal_2 Loss: 0.1311 | 0.1694
Epoch 80/300, seasonal_2 Loss: 0.1306 | 0.1621
Epoch 81/300, seasonal_2 Loss: 0.1300 | 0.1655
Epoch 82/300, seasonal_2 Loss: 0.1294 | 0.1625
Epoch 83/300, seasonal_2 Loss: 0.1285 | 0.1624
Epoch 84/300, seasonal_2 Loss: 0.1282 | 0.1645
Epoch 85/300, seasonal_2 Loss: 0.1280 | 0.1613
Epoch 86/300, seasonal_2 Loss: 0.1272 | 0.1621
Epoch 87/300, seasonal_2 Loss: 0.1266 | 0.1613
Epoch 88/300, seasonal_2 Loss: 0.1269 | 0.1601
Epoch 89/300, seasonal_2 Loss: 0.1258 | 0.1605
Epoch 90/300, seasonal_2 Loss: 0.1264 | 0.1606
Epoch 91/300, seasonal_2 Loss: 0.1259 | 0.1618
Epoch 92/300, seasonal_2 Loss: 0.1255 | 0.1588
Epoch 93/300, seasonal_2 Loss: 0.1248 | 0.1587
Epoch 94/300, seasonal_2 Loss: 0.1240 | 0.1623
Epoch 95/300, seasonal_2 Loss: 0.1241 | 0.1586
Epoch 96/300, seasonal_2 Loss: 0.1243 | 0.1577
Epoch 97/300, seasonal_2 Loss: 0.1227 | 0.1565
Epoch 98/300, seasonal_2 Loss: 0.1242 | 0.1579
Epoch 99/300, seasonal_2 Loss: 0.1234 | 0.1565
Epoch 100/300, seasonal_2 Loss: 0.1224 | 0.1585
Epoch 101/300, seasonal_2 Loss: 0.1227 | 0.1587
Epoch 102/300, seasonal_2 Loss: 0.1219 | 0.1565
Epoch 103/300, seasonal_2 Loss: 0.1232 | 0.1584
Epoch 104/300, seasonal_2 Loss: 0.1211 | 0.1568
Epoch 105/300, seasonal_2 Loss: 0.1210 | 0.1575
Epoch 106/300, seasonal_2 Loss: 0.1213 | 0.1554
Epoch 107/300, seasonal_2 Loss: 0.1218 | 0.1556
Epoch 108/300, seasonal_2 Loss: 0.1216 | 0.1556
Epoch 109/300, seasonal_2 Loss: 0.1202 | 0.1552
Epoch 110/300, seasonal_2 Loss: 0.1205 | 0.1540
Epoch 111/300, seasonal_2 Loss: 0.1201 | 0.1566
Epoch 112/300, seasonal_2 Loss: 0.1192 | 0.1563
Epoch 113/300, seasonal_2 Loss: 0.1196 | 0.1549
Epoch 114/300, seasonal_2 Loss: 0.1193 | 0.1569
Epoch 115/300, seasonal_2 Loss: 0.1196 | 0.1554
Epoch 116/300, seasonal_2 Loss: 0.1203 | 0.1548
Epoch 117/300, seasonal_2 Loss: 0.1197 | 0.1549
Epoch 118/300, seasonal_2 Loss: 0.1190 | 0.1545
Epoch 119/300, seasonal_2 Loss: 0.1191 | 0.1538
Epoch 120/300, seasonal_2 Loss: 0.1193 | 0.1542
Epoch 121/300, seasonal_2 Loss: 0.1183 | 0.1522
Epoch 122/300, seasonal_2 Loss: 0.1192 | 0.1540
Epoch 123/300, seasonal_2 Loss: 0.1180 | 0.1534
Epoch 124/300, seasonal_2 Loss: 0.1182 | 0.1536
Epoch 125/300, seasonal_2 Loss: 0.1185 | 0.1532
Epoch 126/300, seasonal_2 Loss: 0.1179 | 0.1535
Epoch 127/300, seasonal_2 Loss: 0.1187 | 0.1535
Epoch 128/300, seasonal_2 Loss: 0.1183 | 0.1534
Epoch 129/300, seasonal_2 Loss: 0.1172 | 0.1544
Epoch 130/300, seasonal_2 Loss: 0.1187 | 0.1542
Epoch 131/300, seasonal_2 Loss: 0.1177 | 0.1539
Epoch 132/300, seasonal_2 Loss: 0.1176 | 0.1539
Epoch 133/300, seasonal_2 Loss: 0.1179 | 0.1542
Epoch 134/300, seasonal_2 Loss: 0.1177 | 0.1534
Epoch 135/300, seasonal_2 Loss: 0.1177 | 0.1531
Epoch 136/300, seasonal_2 Loss: 0.1177 | 0.1531
Epoch 137/300, seasonal_2 Loss: 0.1179 | 0.1526
Epoch 138/300, seasonal_2 Loss: 0.1180 | 0.1533
Epoch 139/300, seasonal_2 Loss: 0.1178 | 0.1525
Epoch 140/300, seasonal_2 Loss: 0.1181 | 0.1532
Epoch 141/300, seasonal_2 Loss: 0.1171 | 0.1533
Epoch 142/300, seasonal_2 Loss: 0.1172 | 0.1526
Epoch 143/300, seasonal_2 Loss: 0.1163 | 0.1522
Epoch 144/300, seasonal_2 Loss: 0.1162 | 0.1531
Epoch 145/300, seasonal_2 Loss: 0.1167 | 0.1535
Epoch 146/300, seasonal_2 Loss: 0.1167 | 0.1530
Epoch 147/300, seasonal_2 Loss: 0.1168 | 0.1531
Epoch 148/300, seasonal_2 Loss: 0.1161 | 0.1528
Epoch 149/300, seasonal_2 Loss: 0.1161 | 0.1521
Epoch 150/300, seasonal_2 Loss: 0.1158 | 0.1523
Epoch 151/300, seasonal_2 Loss: 0.1174 | 0.1525
Epoch 152/300, seasonal_2 Loss: 0.1161 | 0.1533
Epoch 153/300, seasonal_2 Loss: 0.1161 | 0.1530
Epoch 154/300, seasonal_2 Loss: 0.1167 | 0.1529
Epoch 155/300, seasonal_2 Loss: 0.1164 | 0.1533
Epoch 156/300, seasonal_2 Loss: 0.1164 | 0.1530
Epoch 157/300, seasonal_2 Loss: 0.1158 | 0.1531
Epoch 158/300, seasonal_2 Loss: 0.1162 | 0.1526
Epoch 159/300, seasonal_2 Loss: 0.1160 | 0.1526
Epoch 160/300, seasonal_2 Loss: 0.1157 | 0.1523
Epoch 161/300, seasonal_2 Loss: 0.1158 | 0.1526
Epoch 162/300, seasonal_2 Loss: 0.1160 | 0.1529
Epoch 163/300, seasonal_2 Loss: 0.1165 | 0.1530
Epoch 164/300, seasonal_2 Loss: 0.1157 | 0.1522
Epoch 165/300, seasonal_2 Loss: 0.1164 | 0.1522
Epoch 166/300, seasonal_2 Loss: 0.1157 | 0.1521
Epoch 167/300, seasonal_2 Loss: 0.1152 | 0.1522
Epoch 168/300, seasonal_2 Loss: 0.1162 | 0.1518
Epoch 169/300, seasonal_2 Loss: 0.1165 | 0.1515
Epoch 170/300, seasonal_2 Loss: 0.1158 | 0.1514
Epoch 171/300, seasonal_2 Loss: 0.1162 | 0.1515
Epoch 172/300, seasonal_2 Loss: 0.1158 | 0.1517
Epoch 173/300, seasonal_2 Loss: 0.1167 | 0.1516
Epoch 174/300, seasonal_2 Loss: 0.1153 | 0.1517
Epoch 175/300, seasonal_2 Loss: 0.1156 | 0.1521
Epoch 176/300, seasonal_2 Loss: 0.1165 | 0.1522
Epoch 177/300, seasonal_2 Loss: 0.1163 | 0.1520
Epoch 178/300, seasonal_2 Loss: 0.1160 | 0.1518
Epoch 179/300, seasonal_2 Loss: 0.1155 | 0.1522
Epoch 180/300, seasonal_2 Loss: 0.1160 | 0.1523
Epoch 181/300, seasonal_2 Loss: 0.1154 | 0.1520
Epoch 182/300, seasonal_2 Loss: 0.1154 | 0.1519
Epoch 183/300, seasonal_2 Loss: 0.1164 | 0.1517
Epoch 184/300, seasonal_2 Loss: 0.1153 | 0.1515
Epoch 185/300, seasonal_2 Loss: 0.1162 | 0.1517
Epoch 186/300, seasonal_2 Loss: 0.1158 | 0.1521
Epoch 187/300, seasonal_2 Loss: 0.1158 | 0.1521
Epoch 188/300, seasonal_2 Loss: 0.1153 | 0.1519
Epoch 189/300, seasonal_2 Loss: 0.1151 | 0.1516
Epoch 190/300, seasonal_2 Loss: 0.1162 | 0.1515
Epoch 191/300, seasonal_2 Loss: 0.1163 | 0.1515
Epoch 192/300, seasonal_2 Loss: 0.1154 | 0.1514
Epoch 193/300, seasonal_2 Loss: 0.1154 | 0.1514
Epoch 194/300, seasonal_2 Loss: 0.1161 | 0.1514
Epoch 195/300, seasonal_2 Loss: 0.1164 | 0.1516
Epoch 196/300, seasonal_2 Loss: 0.1159 | 0.1515
Epoch 197/300, seasonal_2 Loss: 0.1156 | 0.1513
Epoch 198/300, seasonal_2 Loss: 0.1159 | 0.1510
Epoch 199/300, seasonal_2 Loss: 0.1164 | 0.1511
Epoch 200/300, seasonal_2 Loss: 0.1153 | 0.1512
Epoch 201/300, seasonal_2 Loss: 0.1159 | 0.1513
Epoch 202/300, seasonal_2 Loss: 0.1161 | 0.1513
Epoch 203/300, seasonal_2 Loss: 0.1149 | 0.1511
Epoch 204/300, seasonal_2 Loss: 0.1148 | 0.1511
Epoch 205/300, seasonal_2 Loss: 0.1159 | 0.1511
Epoch 206/300, seasonal_2 Loss: 0.1152 | 0.1512
Epoch 207/300, seasonal_2 Loss: 0.1150 | 0.1511
Epoch 208/300, seasonal_2 Loss: 0.1156 | 0.1512
Epoch 209/300, seasonal_2 Loss: 0.1159 | 0.1512
Epoch 210/300, seasonal_2 Loss: 0.1152 | 0.1512
Epoch 211/300, seasonal_2 Loss: 0.1155 | 0.1512
Epoch 212/300, seasonal_2 Loss: 0.1158 | 0.1513
Epoch 213/300, seasonal_2 Loss: 0.1154 | 0.1514
Epoch 214/300, seasonal_2 Loss: 0.1163 | 0.1515
Epoch 215/300, seasonal_2 Loss: 0.1147 | 0.1515
Epoch 216/300, seasonal_2 Loss: 0.1146 | 0.1516
Epoch 217/300, seasonal_2 Loss: 0.1159 | 0.1516
Epoch 218/300, seasonal_2 Loss: 0.1155 | 0.1516
Epoch 219/300, seasonal_2 Loss: 0.1161 | 0.1516
Epoch 220/300, seasonal_2 Loss: 0.1153 | 0.1516
Epoch 221/300, seasonal_2 Loss: 0.1154 | 0.1517
Epoch 222/300, seasonal_2 Loss: 0.1154 | 0.1518
Epoch 223/300, seasonal_2 Loss: 0.1148 | 0.1518
Epoch 224/300, seasonal_2 Loss: 0.1151 | 0.1516
Epoch 225/300, seasonal_2 Loss: 0.1151 | 0.1516
Epoch 226/300, seasonal_2 Loss: 0.1158 | 0.1516
Epoch 227/300, seasonal_2 Loss: 0.1150 | 0.1515
Epoch 228/300, seasonal_2 Loss: 0.1148 | 0.1515
Epoch 229/300, seasonal_2 Loss: 0.1157 | 0.1516
Epoch 230/300, seasonal_2 Loss: 0.1154 | 0.1516
Epoch 231/300, seasonal_2 Loss: 0.1156 | 0.1515
Epoch 232/300, seasonal_2 Loss: 0.1162 | 0.1516
Epoch 233/300, seasonal_2 Loss: 0.1156 | 0.1515
Epoch 234/300, seasonal_2 Loss: 0.1148 | 0.1514
Epoch 235/300, seasonal_2 Loss: 0.1151 | 0.1515
Epoch 236/300, seasonal_2 Loss: 0.1149 | 0.1515
Epoch 237/300, seasonal_2 Loss: 0.1147 | 0.1514
Epoch 238/300, seasonal_2 Loss: 0.1159 | 0.1514
Epoch 239/300, seasonal_2 Loss: 0.1153 | 0.1513
Epoch 240/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 241/300, seasonal_2 Loss: 0.1154 | 0.1513
Epoch 242/300, seasonal_2 Loss: 0.1146 | 0.1513
Epoch 243/300, seasonal_2 Loss: 0.1145 | 0.1513
Epoch 244/300, seasonal_2 Loss: 0.1154 | 0.1513
Epoch 245/300, seasonal_2 Loss: 0.1154 | 0.1513
Epoch 246/300, seasonal_2 Loss: 0.1161 | 0.1513
Epoch 247/300, seasonal_2 Loss: 0.1154 | 0.1513
Epoch 248/300, seasonal_2 Loss: 0.1159 | 0.1513
Epoch 249/300, seasonal_2 Loss: 0.1151 | 0.1513
Epoch 250/300, seasonal_2 Loss: 0.1145 | 0.1513
Epoch 251/300, seasonal_2 Loss: 0.1150 | 0.1513
Epoch 252/300, seasonal_2 Loss: 0.1147 | 0.1513
Epoch 253/300, seasonal_2 Loss: 0.1162 | 0.1513
Epoch 254/300, seasonal_2 Loss: 0.1149 | 0.1513
Epoch 255/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 256/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 257/300, seasonal_2 Loss: 0.1148 | 0.1513
Epoch 258/300, seasonal_2 Loss: 0.1151 | 0.1514
Epoch 259/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 260/300, seasonal_2 Loss: 0.1149 | 0.1513
Epoch 261/300, seasonal_2 Loss: 0.1153 | 0.1513
Epoch 262/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 263/300, seasonal_2 Loss: 0.1156 | 0.1514
Epoch 264/300, seasonal_2 Loss: 0.1152 | 0.1514
Epoch 265/300, seasonal_2 Loss: 0.1161 | 0.1513
Epoch 266/300, seasonal_2 Loss: 0.1154 | 0.1513
Epoch 267/300, seasonal_2 Loss: 0.1149 | 0.1513
Epoch 268/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 269/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 270/300, seasonal_2 Loss: 0.1158 | 0.1513
Epoch 271/300, seasonal_2 Loss: 0.1153 | 0.1513
Epoch 272/300, seasonal_2 Loss: 0.1153 | 0.1513
Epoch 273/300, seasonal_2 Loss: 0.1157 | 0.1513
Epoch 274/300, seasonal_2 Loss: 0.1158 | 0.1513
Epoch 275/300, seasonal_2 Loss: 0.1159 | 0.1513
Epoch 276/300, seasonal_2 Loss: 0.1161 | 0.1513
Epoch 277/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 278/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 279/300, seasonal_2 Loss: 0.1145 | 0.1513
Epoch 280/300, seasonal_2 Loss: 0.1163 | 0.1513
Epoch 281/300, seasonal_2 Loss: 0.1149 | 0.1513
Epoch 282/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 283/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 284/300, seasonal_2 Loss: 0.1152 | 0.1513
Epoch 285/300, seasonal_2 Loss: 0.1146 | 0.1513
Epoch 286/300, seasonal_2 Loss: 0.1143 | 0.1513
Epoch 287/300, seasonal_2 Loss: 0.1147 | 0.1513
Epoch 288/300, seasonal_2 Loss: 0.1153 | 0.1513
Epoch 289/300, seasonal_2 Loss: 0.1160 | 0.1513
Epoch 290/300, seasonal_2 Loss: 0.1161 | 0.1513
Epoch 291/300, seasonal_2 Loss: 0.1157 | 0.1513
Epoch 292/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 293/300, seasonal_2 Loss: 0.1153 | 0.1513
Epoch 294/300, seasonal_2 Loss: 0.1148 | 0.1513
Epoch 295/300, seasonal_2 Loss: 0.1148 | 0.1513
Epoch 296/300, seasonal_2 Loss: 0.1161 | 0.1513
Epoch 297/300, seasonal_2 Loss: 0.1156 | 0.1513
Epoch 298/300, seasonal_2 Loss: 0.1155 | 0.1513
Epoch 299/300, seasonal_2 Loss: 0.1144 | 0.1513
Epoch 300/300, seasonal_2 Loss: 0.1155 | 0.1513
Training seasonal_3 component with params: {'observation_period_num': 220, 'train_rates': 0.9879225756721891, 'learning_rate': 0.00018516222876211605, 'batch_size': 45, 'step_size': 2, 'gamma': 0.9349928127071232}
Epoch 1/300, seasonal_3 Loss: 0.8759 | 1.0493
Epoch 2/300, seasonal_3 Loss: 0.7238 | 0.9073
Epoch 3/300, seasonal_3 Loss: 0.5345 | 0.7426
Epoch 4/300, seasonal_3 Loss: 0.5247 | 0.6539
Epoch 5/300, seasonal_3 Loss: 0.4445 | 0.6026
Epoch 6/300, seasonal_3 Loss: 0.3908 | 0.5892
Epoch 7/300, seasonal_3 Loss: 0.3903 | 0.4797
Epoch 8/300, seasonal_3 Loss: 0.3384 | 0.4659
Epoch 9/300, seasonal_3 Loss: 0.3438 | 0.4426
Epoch 10/300, seasonal_3 Loss: 0.3697 | 0.4235
Epoch 11/300, seasonal_3 Loss: 0.3207 | 0.4005
Epoch 12/300, seasonal_3 Loss: 0.2800 | 0.3758
Epoch 13/300, seasonal_3 Loss: 0.2759 | 0.3735
Epoch 14/300, seasonal_3 Loss: 0.2505 | 0.3419
Epoch 15/300, seasonal_3 Loss: 0.2357 | 0.3265
Epoch 16/300, seasonal_3 Loss: 0.2323 | 0.3142
Epoch 17/300, seasonal_3 Loss: 0.2188 | 0.3042
Epoch 18/300, seasonal_3 Loss: 0.2157 | 0.2952
Epoch 19/300, seasonal_3 Loss: 0.2106 | 0.2847
Epoch 20/300, seasonal_3 Loss: 0.2070 | 0.2770
Epoch 21/300, seasonal_3 Loss: 0.2039 | 0.2716
Epoch 22/300, seasonal_3 Loss: 0.2015 | 0.2644
Epoch 23/300, seasonal_3 Loss: 0.1979 | 0.2586
Epoch 24/300, seasonal_3 Loss: 0.1956 | 0.2527
Epoch 25/300, seasonal_3 Loss: 0.1937 | 0.2496
Epoch 26/300, seasonal_3 Loss: 0.1920 | 0.2434
Epoch 27/300, seasonal_3 Loss: 0.1900 | 0.2401
Epoch 28/300, seasonal_3 Loss: 0.1882 | 0.2358
Epoch 29/300, seasonal_3 Loss: 0.1873 | 0.2336
Epoch 30/300, seasonal_3 Loss: 0.1847 | 0.2302
Epoch 31/300, seasonal_3 Loss: 0.1834 | 0.2273
Epoch 32/300, seasonal_3 Loss: 0.1826 | 0.2236
Epoch 33/300, seasonal_3 Loss: 0.1812 | 0.2227
Epoch 34/300, seasonal_3 Loss: 0.1803 | 0.2193
Epoch 35/300, seasonal_3 Loss: 0.1789 | 0.2182
Epoch 36/300, seasonal_3 Loss: 0.1784 | 0.2158
Epoch 37/300, seasonal_3 Loss: 0.1771 | 0.2141
Epoch 38/300, seasonal_3 Loss: 0.1759 | 0.2128
Epoch 39/300, seasonal_3 Loss: 0.1757 | 0.2114
Epoch 40/300, seasonal_3 Loss: 0.1755 | 0.2096
Epoch 41/300, seasonal_3 Loss: 0.1744 | 0.2074
Epoch 42/300, seasonal_3 Loss: 0.1733 | 0.2062
Epoch 43/300, seasonal_3 Loss: 0.1731 | 0.2056
Epoch 44/300, seasonal_3 Loss: 0.1721 | 0.2046
Epoch 45/300, seasonal_3 Loss: 0.1721 | 0.2030
Epoch 46/300, seasonal_3 Loss: 0.1713 | 0.2020
Epoch 47/300, seasonal_3 Loss: 0.1715 | 0.2020
Epoch 48/300, seasonal_3 Loss: 0.1704 | 0.2010
Epoch 49/300, seasonal_3 Loss: 0.1699 | 0.2000
Epoch 50/300, seasonal_3 Loss: 0.1697 | 0.1995
Epoch 51/300, seasonal_3 Loss: 0.1695 | 0.1991
Epoch 52/300, seasonal_3 Loss: 0.1688 | 0.1978
Epoch 53/300, seasonal_3 Loss: 0.1683 | 0.1972
Epoch 54/300, seasonal_3 Loss: 0.1682 | 0.1969
Epoch 55/300, seasonal_3 Loss: 0.1677 | 0.1961
Epoch 56/300, seasonal_3 Loss: 0.1678 | 0.1960
Epoch 57/300, seasonal_3 Loss: 0.1673 | 0.1952
Epoch 58/300, seasonal_3 Loss: 0.1671 | 0.1952
Epoch 59/300, seasonal_3 Loss: 0.1668 | 0.1947
Epoch 60/300, seasonal_3 Loss: 0.1674 | 0.1942
Epoch 61/300, seasonal_3 Loss: 0.1665 | 0.1938
Epoch 62/300, seasonal_3 Loss: 0.1661 | 0.1933
Epoch 63/300, seasonal_3 Loss: 0.1660 | 0.1925
Epoch 64/300, seasonal_3 Loss: 0.1660 | 0.1921
Epoch 65/300, seasonal_3 Loss: 0.1662 | 0.1919
Epoch 66/300, seasonal_3 Loss: 0.1665 | 0.1918
Epoch 67/300, seasonal_3 Loss: 0.1656 | 0.1912
Epoch 68/300, seasonal_3 Loss: 0.1654 | 0.1913
Epoch 69/300, seasonal_3 Loss: 0.1649 | 0.1912
Epoch 70/300, seasonal_3 Loss: 0.1659 | 0.1910
Epoch 71/300, seasonal_3 Loss: 0.1656 | 0.1909
Epoch 72/300, seasonal_3 Loss: 0.1655 | 0.1907
Epoch 73/300, seasonal_3 Loss: 0.1648 | 0.1904
Epoch 74/300, seasonal_3 Loss: 0.1652 | 0.1902
Epoch 75/300, seasonal_3 Loss: 0.1648 | 0.1900
Epoch 76/300, seasonal_3 Loss: 0.1655 | 0.1899
Epoch 77/300, seasonal_3 Loss: 0.1648 | 0.1900
Epoch 78/300, seasonal_3 Loss: 0.1658 | 0.1899
Epoch 79/300, seasonal_3 Loss: 0.1651 | 0.1898
Epoch 80/300, seasonal_3 Loss: 0.1637 | 0.1897
Epoch 81/300, seasonal_3 Loss: 0.1647 | 0.1896
Epoch 82/300, seasonal_3 Loss: 0.1652 | 0.1894
Epoch 83/300, seasonal_3 Loss: 0.1647 | 0.1893
Epoch 84/300, seasonal_3 Loss: 0.1649 | 0.1892
Epoch 85/300, seasonal_3 Loss: 0.1646 | 0.1891
Epoch 86/300, seasonal_3 Loss: 0.1648 | 0.1891
Epoch 87/300, seasonal_3 Loss: 0.1638 | 0.1889
Epoch 88/300, seasonal_3 Loss: 0.1644 | 0.1889
Epoch 89/300, seasonal_3 Loss: 0.1641 | 0.1889
Epoch 90/300, seasonal_3 Loss: 0.1635 | 0.1888
Epoch 91/300, seasonal_3 Loss: 0.1637 | 0.1888
Epoch 92/300, seasonal_3 Loss: 0.1645 | 0.1887
Epoch 93/300, seasonal_3 Loss: 0.1636 | 0.1887
Epoch 94/300, seasonal_3 Loss: 0.1640 | 0.1886
Epoch 95/300, seasonal_3 Loss: 0.1633 | 0.1886
Epoch 96/300, seasonal_3 Loss: 0.1646 | 0.1886
Epoch 97/300, seasonal_3 Loss: 0.1636 | 0.1885
Epoch 98/300, seasonal_3 Loss: 0.1631 | 0.1885
Epoch 99/300, seasonal_3 Loss: 0.1644 | 0.1885
Epoch 100/300, seasonal_3 Loss: 0.1636 | 0.1885
Epoch 101/300, seasonal_3 Loss: 0.1636 | 0.1885
Epoch 102/300, seasonal_3 Loss: 0.1649 | 0.1884
Epoch 103/300, seasonal_3 Loss: 0.1647 | 0.1884
Epoch 104/300, seasonal_3 Loss: 0.1637 | 0.1883
Epoch 105/300, seasonal_3 Loss: 0.1641 | 0.1883
Epoch 106/300, seasonal_3 Loss: 0.1633 | 0.1883
Epoch 107/300, seasonal_3 Loss: 0.1636 | 0.1883
Epoch 108/300, seasonal_3 Loss: 0.1633 | 0.1883
Epoch 109/300, seasonal_3 Loss: 0.1636 | 0.1883
Epoch 110/300, seasonal_3 Loss: 0.1635 | 0.1883
Epoch 111/300, seasonal_3 Loss: 0.1634 | 0.1882
Epoch 112/300, seasonal_3 Loss: 0.1642 | 0.1882
Epoch 113/300, seasonal_3 Loss: 0.1646 | 0.1883
Epoch 114/300, seasonal_3 Loss: 0.1639 | 0.1882
Epoch 115/300, seasonal_3 Loss: 0.1641 | 0.1882
Epoch 116/300, seasonal_3 Loss: 0.1636 | 0.1882
Epoch 117/300, seasonal_3 Loss: 0.1636 | 0.1882
Epoch 118/300, seasonal_3 Loss: 0.1638 | 0.1882
Epoch 119/300, seasonal_3 Loss: 0.1640 | 0.1882
Epoch 120/300, seasonal_3 Loss: 0.1637 | 0.1882
Epoch 121/300, seasonal_3 Loss: 0.1635 | 0.1882
Epoch 122/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 123/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 124/300, seasonal_3 Loss: 0.1638 | 0.1881
Epoch 125/300, seasonal_3 Loss: 0.1636 | 0.1881
Epoch 126/300, seasonal_3 Loss: 0.1643 | 0.1881
Epoch 127/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 128/300, seasonal_3 Loss: 0.1642 | 0.1881
Epoch 129/300, seasonal_3 Loss: 0.1637 | 0.1881
Epoch 130/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 131/300, seasonal_3 Loss: 0.1643 | 0.1881
Epoch 132/300, seasonal_3 Loss: 0.1633 | 0.1881
Epoch 133/300, seasonal_3 Loss: 0.1642 | 0.1881
Epoch 134/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 135/300, seasonal_3 Loss: 0.1631 | 0.1881
Epoch 136/300, seasonal_3 Loss: 0.1629 | 0.1881
Epoch 137/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 138/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 139/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 140/300, seasonal_3 Loss: 0.1643 | 0.1881
Epoch 141/300, seasonal_3 Loss: 0.1630 | 0.1881
Epoch 142/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 143/300, seasonal_3 Loss: 0.1638 | 0.1881
Epoch 144/300, seasonal_3 Loss: 0.1631 | 0.1881
Epoch 145/300, seasonal_3 Loss: 0.1634 | 0.1881
Epoch 146/300, seasonal_3 Loss: 0.1639 | 0.1881
Epoch 147/300, seasonal_3 Loss: 0.1630 | 0.1881
Epoch 148/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 149/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 150/300, seasonal_3 Loss: 0.1638 | 0.1881
Epoch 151/300, seasonal_3 Loss: 0.1636 | 0.1881
Epoch 152/300, seasonal_3 Loss: 0.1632 | 0.1881
Epoch 153/300, seasonal_3 Loss: 0.1636 | 0.1881
Epoch 154/300, seasonal_3 Loss: 0.1637 | 0.1881
Epoch 155/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 156/300, seasonal_3 Loss: 0.1638 | 0.1881
Epoch 157/300, seasonal_3 Loss: 0.1639 | 0.1881
Epoch 158/300, seasonal_3 Loss: 0.1639 | 0.1881
Epoch 159/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 160/300, seasonal_3 Loss: 0.1629 | 0.1881
Epoch 161/300, seasonal_3 Loss: 0.1631 | 0.1881
Epoch 162/300, seasonal_3 Loss: 0.1634 | 0.1881
Epoch 163/300, seasonal_3 Loss: 0.1633 | 0.1881
Epoch 164/300, seasonal_3 Loss: 0.1634 | 0.1881
Epoch 165/300, seasonal_3 Loss: 0.1644 | 0.1881
Epoch 166/300, seasonal_3 Loss: 0.1637 | 0.1881
Epoch 167/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 168/300, seasonal_3 Loss: 0.1635 | 0.1881
Epoch 169/300, seasonal_3 Loss: 0.1638 | 0.1881
Epoch 170/300, seasonal_3 Loss: 0.1637 | 0.1881
Epoch 171/300, seasonal_3 Loss: 0.1638 | 0.1881
Epoch 172/300, seasonal_3 Loss: 0.1634 | 0.1881
Epoch 173/300, seasonal_3 Loss: 0.1647 | 0.1881
Epoch 174/300, seasonal_3 Loss: 0.1630 | 0.1881
Epoch 175/300, seasonal_3 Loss: 0.1643 | 0.1881
Epoch 176/300, seasonal_3 Loss: 0.1641 | 0.1881
Epoch 177/300, seasonal_3 Loss: 0.1636 | 0.1881
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 178, 'train_rates': 0.9790328105067794, 'learning_rate': 6.129374536809053e-05, 'batch_size': 44, 'step_size': 11, 'gamma': 0.9203090949766259}
Epoch 1/300, resid Loss: 0.9187 | 1.4078
Epoch 2/300, resid Loss: 0.6457 | 1.1009
Epoch 3/300, resid Loss: 0.5192 | 1.0198
Epoch 4/300, resid Loss: 0.4302 | 0.8783
Epoch 5/300, resid Loss: 0.3965 | 0.8118
Epoch 6/300, resid Loss: 0.3766 | 0.7904
Epoch 7/300, resid Loss: 0.3450 | 0.7019
Epoch 8/300, resid Loss: 0.3196 | 0.6590
Epoch 9/300, resid Loss: 0.3059 | 0.6313
Epoch 10/300, resid Loss: 0.2849 | 0.5902
Epoch 11/300, resid Loss: 0.2704 | 0.5531
Epoch 12/300, resid Loss: 0.2676 | 0.5341
Epoch 13/300, resid Loss: 0.2663 | 0.5172
Epoch 14/300, resid Loss: 0.2594 | 0.5113
Epoch 15/300, resid Loss: 0.2685 | 0.4786
Epoch 16/300, resid Loss: 0.2708 | 0.4656
Epoch 17/300, resid Loss: 0.2599 | 0.4521
Epoch 18/300, resid Loss: 0.2754 | 0.4399
Epoch 19/300, resid Loss: 0.2655 | 0.4364
Epoch 20/300, resid Loss: 0.2317 | 0.4100
Epoch 21/300, resid Loss: 0.2269 | 0.3923
Epoch 22/300, resid Loss: 0.2199 | 0.3825
Epoch 23/300, resid Loss: 0.2107 | 0.3643
Epoch 24/300, resid Loss: 0.2074 | 0.3618
Epoch 25/300, resid Loss: 0.2061 | 0.3436
Epoch 26/300, resid Loss: 0.2008 | 0.3332
Epoch 27/300, resid Loss: 0.2000 | 0.3254
Epoch 28/300, resid Loss: 0.1983 | 0.3244
Epoch 29/300, resid Loss: 0.1929 | 0.3137
Epoch 30/300, resid Loss: 0.1949 | 0.3073
Epoch 31/300, resid Loss: 0.1919 | 0.3210
Epoch 32/300, resid Loss: 0.1917 | 0.3159
Epoch 33/300, resid Loss: 0.1983 | 0.2882
Epoch 34/300, resid Loss: 0.1983 | 0.3185
Epoch 35/300, resid Loss: 0.1900 | 0.2897
Epoch 36/300, resid Loss: 0.1921 | 0.2896
Epoch 37/300, resid Loss: 0.1922 | 0.2691
Epoch 38/300, resid Loss: 0.2091 | 0.2937
Epoch 39/300, resid Loss: 0.1915 | 0.2769
Epoch 40/300, resid Loss: 0.2040 | 0.2705
Epoch 41/300, resid Loss: 0.2210 | 0.2964
Epoch 42/300, resid Loss: 0.1812 | 0.2618
Epoch 43/300, resid Loss: 0.1801 | 0.2542
Epoch 44/300, resid Loss: 0.1850 | 0.2687
Epoch 45/300, resid Loss: 0.1740 | 0.2530
Epoch 46/300, resid Loss: 0.1724 | 0.2425
Epoch 47/300, resid Loss: 0.1680 | 0.2433
Epoch 48/300, resid Loss: 0.1643 | 0.2400
Epoch 49/300, resid Loss: 0.1631 | 0.2322
Epoch 50/300, resid Loss: 0.1618 | 0.2317
Epoch 51/300, resid Loss: 0.1598 | 0.2267
Epoch 52/300, resid Loss: 0.1588 | 0.2234
Epoch 53/300, resid Loss: 0.1584 | 0.2236
Epoch 54/300, resid Loss: 0.1575 | 0.2189
Epoch 55/300, resid Loss: 0.1565 | 0.2149
Epoch 56/300, resid Loss: 0.1563 | 0.2165
Epoch 57/300, resid Loss: 0.1548 | 0.2203
Epoch 58/300, resid Loss: 0.1548 | 0.2092
Epoch 59/300, resid Loss: 0.1561 | 0.2106
Epoch 60/300, resid Loss: 0.1561 | 0.2187
Epoch 61/300, resid Loss: 0.1572 | 0.2140
Epoch 62/300, resid Loss: 0.1563 | 0.2046
Epoch 63/300, resid Loss: 0.1588 | 0.2076
Epoch 64/300, resid Loss: 0.1569 | 0.2145
Epoch 65/300, resid Loss: 0.1576 | 0.2096
Epoch 66/300, resid Loss: 0.1597 | 0.1968
Epoch 67/300, resid Loss: 0.1636 | 0.2276
Epoch 68/300, resid Loss: 0.1515 | 0.2007
Epoch 69/300, resid Loss: 0.1561 | 0.1936
Epoch 70/300, resid Loss: 0.1513 | 0.2024
Epoch 71/300, resid Loss: 0.1487 | 0.1971
Epoch 72/300, resid Loss: 0.1480 | 0.1918
Epoch 73/300, resid Loss: 0.1469 | 0.1912
Epoch 74/300, resid Loss: 0.1497 | 0.1962
Epoch 75/300, resid Loss: 0.1449 | 0.1888
Epoch 76/300, resid Loss: 0.1457 | 0.1865
Epoch 77/300, resid Loss: 0.1437 | 0.1905
Epoch 78/300, resid Loss: 0.1440 | 0.1848
Epoch 79/300, resid Loss: 0.1432 | 0.1822
Epoch 80/300, resid Loss: 0.1425 | 0.1849
Epoch 81/300, resid Loss: 0.1408 | 0.1831
Epoch 82/300, resid Loss: 0.1405 | 0.1778
Epoch 83/300, resid Loss: 0.1391 | 0.1793
Epoch 84/300, resid Loss: 0.1393 | 0.1813
Epoch 85/300, resid Loss: 0.1383 | 0.1765
Epoch 86/300, resid Loss: 0.1380 | 0.1742
Epoch 87/300, resid Loss: 0.1380 | 0.1813
Epoch 88/300, resid Loss: 0.1375 | 0.1751
Epoch 89/300, resid Loss: 0.1373 | 0.1725
Epoch 90/300, resid Loss: 0.1365 | 0.1735
Epoch 91/300, resid Loss: 0.1359 | 0.1730
Epoch 92/300, resid Loss: 0.1352 | 0.1698
Epoch 93/300, resid Loss: 0.1346 | 0.1710
Epoch 94/300, resid Loss: 0.1344 | 0.1704
Epoch 95/300, resid Loss: 0.1341 | 0.1695
Epoch 96/300, resid Loss: 0.1348 | 0.1675
Epoch 97/300, resid Loss: 0.1330 | 0.1681
Epoch 98/300, resid Loss: 0.1331 | 0.1682
Epoch 99/300, resid Loss: 0.1318 | 0.1665
Epoch 100/300, resid Loss: 0.1322 | 0.1669
Epoch 101/300, resid Loss: 0.1320 | 0.1665
Epoch 102/300, resid Loss: 0.1316 | 0.1659
Epoch 103/300, resid Loss: 0.1312 | 0.1633
Epoch 104/300, resid Loss: 0.1315 | 0.1651
Epoch 105/300, resid Loss: 0.1297 | 0.1651
Epoch 106/300, resid Loss: 0.1304 | 0.1614
Epoch 107/300, resid Loss: 0.1297 | 0.1633
Epoch 108/300, resid Loss: 0.1296 | 0.1648
Epoch 109/300, resid Loss: 0.1304 | 0.1638
Epoch 110/300, resid Loss: 0.1304 | 0.1589
Epoch 111/300, resid Loss: 0.1309 | 0.1669
Epoch 112/300, resid Loss: 0.1291 | 0.1618
Epoch 113/300, resid Loss: 0.1291 | 0.1580
Epoch 114/300, resid Loss: 0.1294 | 0.1637
Epoch 115/300, resid Loss: 0.1284 | 0.1611
Epoch 116/300, resid Loss: 0.1287 | 0.1581
Epoch 117/300, resid Loss: 0.1272 | 0.1578
Epoch 118/300, resid Loss: 0.1270 | 0.1611
Epoch 119/300, resid Loss: 0.1263 | 0.1574
Epoch 120/300, resid Loss: 0.1253 | 0.1572
Epoch 121/300, resid Loss: 0.1260 | 0.1570
Epoch 122/300, resid Loss: 0.1249 | 0.1555
Epoch 123/300, resid Loss: 0.1247 | 0.1547
Epoch 124/300, resid Loss: 0.1247 | 0.1555
Epoch 125/300, resid Loss: 0.1242 | 0.1558
Epoch 126/300, resid Loss: 0.1228 | 0.1543
Epoch 127/300, resid Loss: 0.1246 | 0.1547
Epoch 128/300, resid Loss: 0.1242 | 0.1555
Epoch 129/300, resid Loss: 0.1237 | 0.1541
Epoch 130/300, resid Loss: 0.1225 | 0.1538
Epoch 131/300, resid Loss: 0.1226 | 0.1536
Epoch 132/300, resid Loss: 0.1228 | 0.1527
Epoch 133/300, resid Loss: 0.1225 | 0.1528
Epoch 134/300, resid Loss: 0.1220 | 0.1514
Epoch 135/300, resid Loss: 0.1225 | 0.1506
Epoch 136/300, resid Loss: 0.1218 | 0.1527
Epoch 137/300, resid Loss: 0.1212 | 0.1510
Epoch 138/300, resid Loss: 0.1211 | 0.1502
Epoch 139/300, resid Loss: 0.1214 | 0.1512
Epoch 140/300, resid Loss: 0.1209 | 0.1510
Epoch 141/300, resid Loss: 0.1209 | 0.1510
Epoch 142/300, resid Loss: 0.1207 | 0.1500
Epoch 143/300, resid Loss: 0.1201 | 0.1502
Epoch 144/300, resid Loss: 0.1204 | 0.1493
Epoch 145/300, resid Loss: 0.1199 | 0.1507
Epoch 146/300, resid Loss: 0.1210 | 0.1504
Epoch 147/300, resid Loss: 0.1204 | 0.1485
Epoch 148/300, resid Loss: 0.1199 | 0.1480
Epoch 149/300, resid Loss: 0.1195 | 0.1484
Epoch 150/300, resid Loss: 0.1186 | 0.1481
Epoch 151/300, resid Loss: 0.1196 | 0.1476
Epoch 152/300, resid Loss: 0.1195 | 0.1480
Epoch 153/300, resid Loss: 0.1190 | 0.1476
Epoch 154/300, resid Loss: 0.1190 | 0.1474
Epoch 155/300, resid Loss: 0.1186 | 0.1481
Epoch 156/300, resid Loss: 0.1187 | 0.1473
Epoch 157/300, resid Loss: 0.1181 | 0.1470
Epoch 158/300, resid Loss: 0.1180 | 0.1475
Epoch 159/300, resid Loss: 0.1186 | 0.1468
Epoch 160/300, resid Loss: 0.1176 | 0.1462
Epoch 161/300, resid Loss: 0.1169 | 0.1468
Epoch 162/300, resid Loss: 0.1177 | 0.1468
Epoch 163/300, resid Loss: 0.1170 | 0.1450
Epoch 164/300, resid Loss: 0.1166 | 0.1457
Epoch 165/300, resid Loss: 0.1172 | 0.1459
Epoch 166/300, resid Loss: 0.1166 | 0.1455
Epoch 167/300, resid Loss: 0.1165 | 0.1457
Epoch 168/300, resid Loss: 0.1169 | 0.1464
Epoch 169/300, resid Loss: 0.1168 | 0.1448
Epoch 170/300, resid Loss: 0.1171 | 0.1451
Epoch 171/300, resid Loss: 0.1162 | 0.1455
Epoch 172/300, resid Loss: 0.1171 | 0.1451
Epoch 173/300, resid Loss: 0.1161 | 0.1442
Epoch 174/300, resid Loss: 0.1159 | 0.1443
Epoch 175/300, resid Loss: 0.1159 | 0.1442
Epoch 176/300, resid Loss: 0.1162 | 0.1435
Epoch 177/300, resid Loss: 0.1157 | 0.1429
Epoch 178/300, resid Loss: 0.1157 | 0.1443
Epoch 179/300, resid Loss: 0.1160 | 0.1437
Epoch 180/300, resid Loss: 0.1152 | 0.1439
Epoch 181/300, resid Loss: 0.1150 | 0.1439
Epoch 182/300, resid Loss: 0.1155 | 0.1437
Epoch 183/300, resid Loss: 0.1156 | 0.1432
Epoch 184/300, resid Loss: 0.1153 | 0.1427
Epoch 185/300, resid Loss: 0.1142 | 0.1431
Epoch 186/300, resid Loss: 0.1149 | 0.1430
Epoch 187/300, resid Loss: 0.1149 | 0.1428
Epoch 188/300, resid Loss: 0.1138 | 0.1428
Epoch 189/300, resid Loss: 0.1154 | 0.1426
Epoch 190/300, resid Loss: 0.1145 | 0.1428
Epoch 191/300, resid Loss: 0.1148 | 0.1423
Epoch 192/300, resid Loss: 0.1144 | 0.1427
Epoch 193/300, resid Loss: 0.1146 | 0.1432
Epoch 194/300, resid Loss: 0.1143 | 0.1425
Epoch 195/300, resid Loss: 0.1139 | 0.1427
Epoch 196/300, resid Loss: 0.1145 | 0.1422
Epoch 197/300, resid Loss: 0.1138 | 0.1424
Epoch 198/300, resid Loss: 0.1138 | 0.1426
Epoch 199/300, resid Loss: 0.1138 | 0.1425
Epoch 200/300, resid Loss: 0.1134 | 0.1421
Epoch 201/300, resid Loss: 0.1139 | 0.1414
Epoch 202/300, resid Loss: 0.1138 | 0.1418
Epoch 203/300, resid Loss: 0.1140 | 0.1416
Epoch 204/300, resid Loss: 0.1131 | 0.1416
Epoch 205/300, resid Loss: 0.1134 | 0.1417
Epoch 206/300, resid Loss: 0.1133 | 0.1416
Epoch 207/300, resid Loss: 0.1136 | 0.1415
Epoch 208/300, resid Loss: 0.1132 | 0.1411
Epoch 209/300, resid Loss: 0.1137 | 0.1412
Epoch 210/300, resid Loss: 0.1132 | 0.1413
Epoch 211/300, resid Loss: 0.1134 | 0.1411
Epoch 212/300, resid Loss: 0.1126 | 0.1408
Epoch 213/300, resid Loss: 0.1133 | 0.1413
Epoch 214/300, resid Loss: 0.1134 | 0.1416
Epoch 215/300, resid Loss: 0.1127 | 0.1408
Epoch 216/300, resid Loss: 0.1126 | 0.1408
Epoch 217/300, resid Loss: 0.1130 | 0.1414
Epoch 218/300, resid Loss: 0.1128 | 0.1413
Epoch 219/300, resid Loss: 0.1131 | 0.1414
Epoch 220/300, resid Loss: 0.1128 | 0.1409
Epoch 221/300, resid Loss: 0.1129 | 0.1409
Epoch 222/300, resid Loss: 0.1118 | 0.1406
Epoch 223/300, resid Loss: 0.1125 | 0.1403
Epoch 224/300, resid Loss: 0.1121 | 0.1404
Epoch 225/300, resid Loss: 0.1131 | 0.1410
Epoch 226/300, resid Loss: 0.1123 | 0.1406
Epoch 227/300, resid Loss: 0.1120 | 0.1406
Epoch 228/300, resid Loss: 0.1117 | 0.1408
Epoch 229/300, resid Loss: 0.1117 | 0.1406
Epoch 230/300, resid Loss: 0.1123 | 0.1404
Epoch 231/300, resid Loss: 0.1115 | 0.1408
Epoch 232/300, resid Loss: 0.1118 | 0.1408
Epoch 233/300, resid Loss: 0.1125 | 0.1405
Epoch 234/300, resid Loss: 0.1113 | 0.1407
Epoch 235/300, resid Loss: 0.1113 | 0.1406
Epoch 236/300, resid Loss: 0.1116 | 0.1406
Epoch 237/300, resid Loss: 0.1118 | 0.1405
Epoch 238/300, resid Loss: 0.1114 | 0.1399
Epoch 239/300, resid Loss: 0.1121 | 0.1397
Epoch 240/300, resid Loss: 0.1117 | 0.1399
Epoch 241/300, resid Loss: 0.1113 | 0.1400
Epoch 242/300, resid Loss: 0.1113 | 0.1394
Epoch 243/300, resid Loss: 0.1115 | 0.1397
Epoch 244/300, resid Loss: 0.1112 | 0.1396
Epoch 245/300, resid Loss: 0.1114 | 0.1399
Epoch 246/300, resid Loss: 0.1114 | 0.1398
Epoch 247/300, resid Loss: 0.1109 | 0.1396
Epoch 248/300, resid Loss: 0.1113 | 0.1397
Epoch 249/300, resid Loss: 0.1119 | 0.1396
Epoch 250/300, resid Loss: 0.1108 | 0.1395
Epoch 251/300, resid Loss: 0.1101 | 0.1394
Epoch 252/300, resid Loss: 0.1118 | 0.1396
Epoch 253/300, resid Loss: 0.1108 | 0.1395
Epoch 254/300, resid Loss: 0.1108 | 0.1398
Epoch 255/300, resid Loss: 0.1112 | 0.1396
Epoch 256/300, resid Loss: 0.1105 | 0.1394
Epoch 257/300, resid Loss: 0.1109 | 0.1392
Epoch 258/300, resid Loss: 0.1107 | 0.1392
Epoch 259/300, resid Loss: 0.1113 | 0.1392
Epoch 260/300, resid Loss: 0.1107 | 0.1393
Epoch 261/300, resid Loss: 0.1102 | 0.1390
Epoch 262/300, resid Loss: 0.1111 | 0.1387
Epoch 263/300, resid Loss: 0.1106 | 0.1388
Epoch 264/300, resid Loss: 0.1104 | 0.1390
Epoch 265/300, resid Loss: 0.1105 | 0.1389
Epoch 266/300, resid Loss: 0.1106 | 0.1387
Epoch 267/300, resid Loss: 0.1112 | 0.1386
Epoch 268/300, resid Loss: 0.1106 | 0.1388
Epoch 269/300, resid Loss: 0.1105 | 0.1390
Epoch 270/300, resid Loss: 0.1105 | 0.1389
Epoch 271/300, resid Loss: 0.1112 | 0.1388
Epoch 272/300, resid Loss: 0.1106 | 0.1389
Epoch 273/300, resid Loss: 0.1104 | 0.1386
Epoch 274/300, resid Loss: 0.1097 | 0.1385
Epoch 275/300, resid Loss: 0.1111 | 0.1385
Epoch 276/300, resid Loss: 0.1106 | 0.1386
Epoch 277/300, resid Loss: 0.1110 | 0.1385
Epoch 278/300, resid Loss: 0.1113 | 0.1384
Epoch 279/300, resid Loss: 0.1107 | 0.1385
Epoch 280/300, resid Loss: 0.1106 | 0.1386
Epoch 281/300, resid Loss: 0.1113 | 0.1384
Epoch 282/300, resid Loss: 0.1099 | 0.1382
Epoch 283/300, resid Loss: 0.1108 | 0.1381
Epoch 284/300, resid Loss: 0.1116 | 0.1383
Epoch 285/300, resid Loss: 0.1102 | 0.1387
Epoch 286/300, resid Loss: 0.1107 | 0.1384
Epoch 287/300, resid Loss: 0.1098 | 0.1381
Epoch 288/300, resid Loss: 0.1110 | 0.1382
Epoch 289/300, resid Loss: 0.1102 | 0.1381
Epoch 290/300, resid Loss: 0.1096 | 0.1382
Epoch 291/300, resid Loss: 0.1103 | 0.1383
Epoch 292/300, resid Loss: 0.1101 | 0.1385
Epoch 293/300, resid Loss: 0.1116 | 0.1383
Epoch 294/300, resid Loss: 0.1109 | 0.1382
Epoch 295/300, resid Loss: 0.1102 | 0.1381
Epoch 296/300, resid Loss: 0.1110 | 0.1383
Epoch 297/300, resid Loss: 0.1097 | 0.1382
Epoch 298/300, resid Loss: 0.1097 | 0.1381
Epoch 299/300, resid Loss: 0.1105 | 0.1382
Epoch 300/300, resid Loss: 0.1104 | 0.1382
Runtime (seconds): 2375.4652395248413
4.382536360015831e-05
[141.45596]
[-1.4828086]
[-4.850233]
[11.211973]
[2.4064045]
[8.398207]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 246.0014380107168
RMSE: 15.684432983398438
MAE: 15.684432983398438
R-squared: nan
[157.13951]
