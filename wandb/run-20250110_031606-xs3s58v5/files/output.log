ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-10 03:16:12,299][0m A new study created in memory with name: no-name-7c5aa979-c214-4720-b971-d9288418bff4[0m
[32m[I 2025-01-10 03:18:50,031][0m Trial 0 finished with value: 0.054061506082490746 and parameters: {'observation_period_num': 25, 'train_rates': 0.8077784571791246, 'learning_rate': 3.425993227283801e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8453978205150912}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:19:34,120][0m Trial 1 finished with value: 1.054358316379818 and parameters: {'observation_period_num': 52, 'train_rates': 0.684560174559984, 'learning_rate': 2.2054045262177554e-06, 'batch_size': 106, 'step_size': 7, 'gamma': 0.9225075235611536}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:21:04,968][0m Trial 2 finished with value: 0.4911759633940595 and parameters: {'observation_period_num': 150, 'train_rates': 0.6261378127184288, 'learning_rate': 2.0285766079455328e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.7979351231937903}. Best is trial 0 with value: 0.054061506082490746.[0m
Early stopping at epoch 50
[32m[I 2025-01-10 03:21:37,844][0m Trial 3 finished with value: 1.7510982536810291 and parameters: {'observation_period_num': 176, 'train_rates': 0.7943351962380114, 'learning_rate': 1.5238179550361543e-06, 'batch_size': 78, 'step_size': 1, 'gamma': 0.7914209065323565}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:22:03,064][0m Trial 4 finished with value: 0.9236805311446328 and parameters: {'observation_period_num': 91, 'train_rates': 0.7451327701564157, 'learning_rate': 2.3017087985607492e-06, 'batch_size': 199, 'step_size': 10, 'gamma': 0.933767353272393}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:24:40,399][0m Trial 5 finished with value: 0.4238095580341019 and parameters: {'observation_period_num': 136, 'train_rates': 0.7332587968302764, 'learning_rate': 4.204224389507661e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.8085949510618236}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:25:35,507][0m Trial 6 finished with value: 0.38492916396255233 and parameters: {'observation_period_num': 187, 'train_rates': 0.7855051263804957, 'learning_rate': 4.3753822283458927e-05, 'batch_size': 87, 'step_size': 4, 'gamma': 0.8556401591138066}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:26:02,578][0m Trial 7 finished with value: 1.3180199645608806 and parameters: {'observation_period_num': 203, 'train_rates': 0.6183122095147444, 'learning_rate': 1.2627284444213034e-06, 'batch_size': 161, 'step_size': 12, 'gamma': 0.8181370543746138}. Best is trial 0 with value: 0.054061506082490746.[0m
Early stopping at epoch 80
[32m[I 2025-01-10 03:26:26,358][0m Trial 8 finished with value: 0.8165600909743198 and parameters: {'observation_period_num': 25, 'train_rates': 0.8236671285481968, 'learning_rate': 9.206852194981672e-06, 'batch_size': 193, 'step_size': 2, 'gamma': 0.7698595082140277}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:26:58,803][0m Trial 9 finished with value: 1.0693463600845852 and parameters: {'observation_period_num': 192, 'train_rates': 0.8926936981960782, 'learning_rate': 3.0659071357526768e-06, 'batch_size': 167, 'step_size': 7, 'gamma': 0.7946984996645589}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:27:22,927][0m Trial 10 finished with value: 0.1576739251613617 and parameters: {'observation_period_num': 247, 'train_rates': 0.9883262508720664, 'learning_rate': 0.0002708803654386316, 'batch_size': 247, 'step_size': 15, 'gamma': 0.9853023778231549}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:27:46,956][0m Trial 11 finished with value: 0.47096914052963257 and parameters: {'observation_period_num': 230, 'train_rates': 0.96829124471833, 'learning_rate': 0.0003291967765371895, 'batch_size': 240, 'step_size': 15, 'gamma': 0.9805937418465775}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:28:13,116][0m Trial 12 finished with value: 0.18730047345161438 and parameters: {'observation_period_num': 89, 'train_rates': 0.9889252551185025, 'learning_rate': 0.0003602022169289176, 'batch_size': 248, 'step_size': 15, 'gamma': 0.8708603215944634}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:28:54,166][0m Trial 13 finished with value: 0.12473938134167525 and parameters: {'observation_period_num': 252, 'train_rates': 0.9012166262968281, 'learning_rate': 0.0007951363823108708, 'batch_size': 130, 'step_size': 12, 'gamma': 0.8621047759066784}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:29:39,398][0m Trial 14 finished with value: 0.05790688224338197 and parameters: {'observation_period_num': 15, 'train_rates': 0.874251066478905, 'learning_rate': 0.00010762227187175544, 'batch_size': 127, 'step_size': 12, 'gamma': 0.8563627111797619}. Best is trial 0 with value: 0.054061506082490746.[0m
[32m[I 2025-01-10 03:31:18,724][0m Trial 15 finished with value: 0.03499922809138332 and parameters: {'observation_period_num': 7, 'train_rates': 0.8560458043336157, 'learning_rate': 0.00010640202724283596, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8940436324153704}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:36:13,651][0m Trial 16 finished with value: 0.0704352515642173 and parameters: {'observation_period_num': 55, 'train_rates': 0.820760920991872, 'learning_rate': 0.00010949530576464705, 'batch_size': 17, 'step_size': 10, 'gamma': 0.9081668855793479}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:37:48,959][0m Trial 17 finished with value: 0.23236433397500944 and parameters: {'observation_period_num': 50, 'train_rates': 0.8523383856987341, 'learning_rate': 1.102273182506774e-05, 'batch_size': 55, 'step_size': 13, 'gamma': 0.8938950063781912}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:39:06,409][0m Trial 18 finished with value: 0.08186420709604308 and parameters: {'observation_period_num': 99, 'train_rates': 0.9119802623857548, 'learning_rate': 9.38389645191777e-05, 'batch_size': 71, 'step_size': 10, 'gamma': 0.949241108513944}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:40:53,558][0m Trial 19 finished with value: 0.4575350821349816 and parameters: {'observation_period_num': 32, 'train_rates': 0.7538847631916592, 'learning_rate': 5.771071288423528e-06, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8344502301062925}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:41:51,779][0m Trial 20 finished with value: 0.17721069681233373 and parameters: {'observation_period_num': 6, 'train_rates': 0.9410500812122099, 'learning_rate': 1.840249190109072e-05, 'batch_size': 102, 'step_size': 9, 'gamma': 0.8906195473977717}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:42:36,989][0m Trial 21 finished with value: 0.06056842133584104 and parameters: {'observation_period_num': 6, 'train_rates': 0.860481255256007, 'learning_rate': 9.183261735759352e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.8326338311926031}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:44:03,522][0m Trial 22 finished with value: 0.05686505690448044 and parameters: {'observation_period_num': 71, 'train_rates': 0.8656993297322586, 'learning_rate': 0.00019622268988697808, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8770591011926485}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:45:33,211][0m Trial 23 finished with value: 0.04950473607178948 and parameters: {'observation_period_num': 71, 'train_rates': 0.825510897346572, 'learning_rate': 0.0001817777249604725, 'batch_size': 57, 'step_size': 11, 'gamma': 0.884022834268351}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:48:49,680][0m Trial 24 finished with value: 0.043354437664503184 and parameters: {'observation_period_num': 36, 'train_rates': 0.8257714904840812, 'learning_rate': 5.460468976922241e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.9040667566677921}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:52:28,687][0m Trial 25 finished with value: 0.1503079760730784 and parameters: {'observation_period_num': 114, 'train_rates': 0.8378141895917448, 'learning_rate': 0.0007268699411817897, 'batch_size': 23, 'step_size': 8, 'gamma': 0.9475031764194304}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:53:23,548][0m Trial 26 finished with value: 0.1258095260945846 and parameters: {'observation_period_num': 72, 'train_rates': 0.6989191239476564, 'learning_rate': 6.427798359616838e-05, 'batch_size': 86, 'step_size': 11, 'gamma': 0.9075323840953606}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:55:02,966][0m Trial 27 finished with value: 0.0458574849216975 and parameters: {'observation_period_num': 40, 'train_rates': 0.7787209777957808, 'learning_rate': 0.00018364200191261032, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8960699457108168}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:57:10,200][0m Trial 28 finished with value: 0.03796756671981652 and parameters: {'observation_period_num': 39, 'train_rates': 0.7896707409483263, 'learning_rate': 0.00048213846937965907, 'batch_size': 39, 'step_size': 14, 'gamma': 0.9118139860455062}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 03:59:54,526][0m Trial 29 finished with value: 0.056856561056225774 and parameters: {'observation_period_num': 26, 'train_rates': 0.9292977143321526, 'learning_rate': 0.0005035005822911902, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9522710307540673}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:01:59,044][0m Trial 30 finished with value: 0.0667097476283474 and parameters: {'observation_period_num': 36, 'train_rates': 0.7104112498738009, 'learning_rate': 6.296570963808036e-05, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9231061384925255}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:07:04,661][0m Trial 31 finished with value: 0.04059758044169821 and parameters: {'observation_period_num': 44, 'train_rates': 0.7942792808132164, 'learning_rate': 0.00017433186575137037, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9053397044825164}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:11:16,979][0m Trial 32 finished with value: 0.06589724079667618 and parameters: {'observation_period_num': 57, 'train_rates': 0.7723290889278894, 'learning_rate': 0.00047810442183166994, 'batch_size': 19, 'step_size': 13, 'gamma': 0.9117728186603707}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:12:30,894][0m Trial 33 finished with value: 0.036534325730820695 and parameters: {'observation_period_num': 18, 'train_rates': 0.7976612404712315, 'learning_rate': 0.0001494988563050061, 'batch_size': 70, 'step_size': 15, 'gamma': 0.9295418799278193}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:13:15,211][0m Trial 34 finished with value: 0.1449136749947468 and parameters: {'observation_period_num': 18, 'train_rates': 0.662367070419338, 'learning_rate': 2.5527968343063034e-05, 'batch_size': 104, 'step_size': 15, 'gamma': 0.926807719477353}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:14:28,931][0m Trial 35 finished with value: 0.0497089064506128 and parameters: {'observation_period_num': 50, 'train_rates': 0.8026530203002525, 'learning_rate': 0.00014649408634973292, 'batch_size': 70, 'step_size': 14, 'gamma': 0.9606904059531218}. Best is trial 15 with value: 0.03499922809138332.[0m
[32m[I 2025-01-10 04:16:33,314][0m Trial 36 finished with value: 0.03073009474010303 and parameters: {'observation_period_num': 5, 'train_rates': 0.754098660383412, 'learning_rate': 0.00027065242699770236, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9402084486286051}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:17:24,716][0m Trial 37 finished with value: 0.139017443751266 and parameters: {'observation_period_num': 157, 'train_rates': 0.7581930279207567, 'learning_rate': 0.0005918645817244174, 'batch_size': 95, 'step_size': 6, 'gamma': 0.9651448751126266}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:18:35,936][0m Trial 38 finished with value: 0.04113593006492833 and parameters: {'observation_period_num': 8, 'train_rates': 0.7235589003693578, 'learning_rate': 0.0009817102135988333, 'batch_size': 68, 'step_size': 5, 'gamma': 0.9413812252757943}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:20:24,751][0m Trial 39 finished with value: 0.05299058041098167 and parameters: {'observation_period_num': 19, 'train_rates': 0.6659746693593473, 'learning_rate': 0.00029052703679927056, 'batch_size': 41, 'step_size': 8, 'gamma': 0.9219280478887845}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:20:59,025][0m Trial 40 finished with value: 0.05155019385527711 and parameters: {'observation_period_num': 28, 'train_rates': 0.7421803087621648, 'learning_rate': 0.0004102373225031516, 'batch_size': 150, 'step_size': 6, 'gamma': 0.929278787027835}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:22:02,282][0m Trial 41 finished with value: 0.049649232386113844 and parameters: {'observation_period_num': 43, 'train_rates': 0.7994170485571428, 'learning_rate': 0.00023810393070804599, 'batch_size': 81, 'step_size': 4, 'gamma': 0.9369959599338363}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:24:27,203][0m Trial 42 finished with value: 0.03568080151453614 and parameters: {'observation_period_num': 20, 'train_rates': 0.7680714725999154, 'learning_rate': 0.000143632692086047, 'batch_size': 34, 'step_size': 15, 'gamma': 0.916819181641796}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:26:14,272][0m Trial 43 finished with value: 0.03460692908390716 and parameters: {'observation_period_num': 5, 'train_rates': 0.7632444433492764, 'learning_rate': 0.00013591521958223425, 'batch_size': 46, 'step_size': 15, 'gamma': 0.9690362329035954}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:27:48,442][0m Trial 44 finished with value: 0.03824961875323896 and parameters: {'observation_period_num': 16, 'train_rates': 0.7609780703930582, 'learning_rate': 0.00011555843288005776, 'batch_size': 52, 'step_size': 15, 'gamma': 0.9649682658101298}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:29:04,792][0m Trial 45 finished with value: 0.05825112703202579 and parameters: {'observation_period_num': 7, 'train_rates': 0.7174313108797836, 'learning_rate': 4.444933701787839e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9763247042933701}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:31:22,165][0m Trial 46 finished with value: 0.04025694850695782 and parameters: {'observation_period_num': 19, 'train_rates': 0.6927586191737535, 'learning_rate': 0.0001353399414180291, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9752507429502261}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:32:26,666][0m Trial 47 finished with value: 0.27499814178397064 and parameters: {'observation_period_num': 64, 'train_rates': 0.7372697825979597, 'learning_rate': 6.928759497345321e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.9565000527320481}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:33:11,204][0m Trial 48 finished with value: 0.07688517996766528 and parameters: {'observation_period_num': 83, 'train_rates': 0.7662358453652457, 'learning_rate': 0.00024457563846272417, 'batch_size': 113, 'step_size': 12, 'gamma': 0.9883697540255478}. Best is trial 36 with value: 0.03073009474010303.[0m
[32m[I 2025-01-10 04:33:36,297][0m Trial 49 finished with value: 0.2983824001981857 and parameters: {'observation_period_num': 27, 'train_rates': 0.8073505887561392, 'learning_rate': 3.458285044222564e-05, 'batch_size': 220, 'step_size': 15, 'gamma': 0.7514553844328702}. Best is trial 36 with value: 0.03073009474010303.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-10 04:33:36,307][0m A new study created in memory with name: no-name-40f6d0a9-bfa2-40bb-bc94-caf1fe3f42c6[0m
[32m[I 2025-01-10 04:34:35,742][0m Trial 0 finished with value: 0.11966803669929504 and parameters: {'observation_period_num': 107, 'train_rates': 0.9325561670571747, 'learning_rate': 0.000979059199073812, 'batch_size': 250, 'step_size': 11, 'gamma': 0.9317214444095148}. Best is trial 0 with value: 0.11966803669929504.[0m
[32m[I 2025-01-10 04:35:33,162][0m Trial 1 finished with value: 0.040829300130013936 and parameters: {'observation_period_num': 13, 'train_rates': 0.7635218589835294, 'learning_rate': 0.0005332008709752599, 'batch_size': 185, 'step_size': 3, 'gamma': 0.8579757950347906}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:36:32,877][0m Trial 2 finished with value: 0.17523804306983948 and parameters: {'observation_period_num': 252, 'train_rates': 0.9528960495069474, 'learning_rate': 0.00017558526343735103, 'batch_size': 224, 'step_size': 1, 'gamma': 0.9877476939428188}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:37:25,961][0m Trial 3 finished with value: 0.20483908106532647 and parameters: {'observation_period_num': 102, 'train_rates': 0.6025998165077403, 'learning_rate': 0.00016961819364432588, 'batch_size': 137, 'step_size': 8, 'gamma': 0.9062267101030842}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:38:16,877][0m Trial 4 finished with value: 0.11391701621248929 and parameters: {'observation_period_num': 84, 'train_rates': 0.6535877046951506, 'learning_rate': 0.00012837672204879877, 'batch_size': 228, 'step_size': 7, 'gamma': 0.8990921005426467}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:40:14,290][0m Trial 5 finished with value: 0.28267629887038914 and parameters: {'observation_period_num': 80, 'train_rates': 0.6047622242084758, 'learning_rate': 6.31093685346066e-06, 'batch_size': 59, 'step_size': 8, 'gamma': 0.9526779350209883}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:42:22,467][0m Trial 6 finished with value: 0.11131694041844549 and parameters: {'observation_period_num': 103, 'train_rates': 0.642976616642012, 'learning_rate': 0.00012922889993183996, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8863054293645938}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:43:12,857][0m Trial 7 finished with value: 0.2197578660983896 and parameters: {'observation_period_num': 204, 'train_rates': 0.6609368163592776, 'learning_rate': 6.767401792425776e-05, 'batch_size': 159, 'step_size': 15, 'gamma': 0.8096109530592417}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:45:40,328][0m Trial 8 finished with value: 0.19528849644411475 and parameters: {'observation_period_num': 143, 'train_rates': 0.7006663607719237, 'learning_rate': 3.556505211339611e-05, 'batch_size': 48, 'step_size': 2, 'gamma': 0.8602558785423994}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:46:46,348][0m Trial 9 finished with value: 0.5574792854329373 and parameters: {'observation_period_num': 98, 'train_rates': 0.9178322896278082, 'learning_rate': 1.9040605680705219e-06, 'batch_size': 149, 'step_size': 6, 'gamma': 0.7959996848495382}. Best is trial 1 with value: 0.040829300130013936.[0m
[32m[I 2025-01-10 04:47:44,817][0m Trial 10 finished with value: 0.034472671237417915 and parameters: {'observation_period_num': 6, 'train_rates': 0.7976594299284364, 'learning_rate': 0.0005803763755550475, 'batch_size': 187, 'step_size': 12, 'gamma': 0.8269877352052354}. Best is trial 10 with value: 0.034472671237417915.[0m
[32m[I 2025-01-10 04:48:44,269][0m Trial 11 finished with value: 0.050175704241661394 and parameters: {'observation_period_num': 27, 'train_rates': 0.7874139492640728, 'learning_rate': 0.0007369931575286963, 'batch_size': 190, 'step_size': 12, 'gamma': 0.8348018991501577}. Best is trial 10 with value: 0.034472671237417915.[0m
[32m[I 2025-01-10 04:50:04,748][0m Trial 12 finished with value: 0.02720414660871029 and parameters: {'observation_period_num': 6, 'train_rates': 0.8054931885145741, 'learning_rate': 0.00041093637665892745, 'batch_size': 105, 'step_size': 11, 'gamma': 0.7609401212442125}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 04:51:30,119][0m Trial 13 finished with value: 0.04513221906233409 and parameters: {'observation_period_num': 51, 'train_rates': 0.8714548932279694, 'learning_rate': 0.0003325508133094422, 'batch_size': 102, 'step_size': 11, 'gamma': 0.7557684003225067}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 04:52:59,041][0m Trial 14 finished with value: 0.044455222247085924 and parameters: {'observation_period_num': 5, 'train_rates': 0.8544808061359436, 'learning_rate': 2.2196652387469285e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.7518573174966072}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 04:54:15,362][0m Trial 15 finished with value: 0.0642515943300995 and parameters: {'observation_period_num': 48, 'train_rates': 0.7272891839121549, 'learning_rate': 0.00031578192552246444, 'batch_size': 102, 'step_size': 10, 'gamma': 0.7855526377668317}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 04:55:12,731][0m Trial 16 finished with value: 0.1675617437051461 and parameters: {'observation_period_num': 158, 'train_rates': 0.8384308494359461, 'learning_rate': 1.1627805810698655e-05, 'batch_size': 185, 'step_size': 15, 'gamma': 0.8276810960541172}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 04:56:49,739][0m Trial 17 finished with value: 0.043748536054532564 and parameters: {'observation_period_num': 44, 'train_rates': 0.803193567866597, 'learning_rate': 4.98392263087734e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.7769211965326606}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:03:24,671][0m Trial 18 finished with value: 0.13830213248729706 and parameters: {'observation_period_num': 183, 'train_rates': 0.9843202972521639, 'learning_rate': 0.0004031196161904117, 'batch_size': 22, 'step_size': 9, 'gamma': 0.8316858040339494}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:04:31,125][0m Trial 19 finished with value: 0.17250554511944452 and parameters: {'observation_period_num': 57, 'train_rates': 0.747372055645974, 'learning_rate': 3.9260378291425405e-06, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8093557359545737}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:05:29,498][0m Trial 20 finished with value: 0.06211116537451744 and parameters: {'observation_period_num': 32, 'train_rates': 0.823415230587521, 'learning_rate': 1.977786983645241e-05, 'batch_size': 172, 'step_size': 10, 'gamma': 0.7647954237817839}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:06:26,440][0m Trial 21 finished with value: 0.03604886254614538 and parameters: {'observation_period_num': 6, 'train_rates': 0.774042300683469, 'learning_rate': 0.0005728519568131037, 'batch_size': 204, 'step_size': 5, 'gamma': 0.8587505533645347}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:07:27,871][0m Trial 22 finished with value: 0.036236416269093755 and parameters: {'observation_period_num': 5, 'train_rates': 0.8916721011262729, 'learning_rate': 0.0009253175843704787, 'batch_size': 218, 'step_size': 5, 'gamma': 0.8485667798719332}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:08:25,194][0m Trial 23 finished with value: 0.06746123425745698 and parameters: {'observation_period_num': 72, 'train_rates': 0.7823673049372961, 'learning_rate': 0.0002548240484740879, 'batch_size': 203, 'step_size': 6, 'gamma': 0.8771385764465851}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:09:17,621][0m Trial 24 finished with value: 0.06605897775873491 and parameters: {'observation_period_num': 31, 'train_rates': 0.6997008791401665, 'learning_rate': 8.708135135239611e-05, 'batch_size': 244, 'step_size': 9, 'gamma': 0.807110931418139}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:10:31,150][0m Trial 25 finished with value: 0.05092177493415602 and parameters: {'observation_period_num': 23, 'train_rates': 0.8064821683731197, 'learning_rate': 0.0005553547373101186, 'batch_size': 122, 'step_size': 14, 'gamma': 0.9137295022297852}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:11:25,361][0m Trial 26 finished with value: 0.05976274922907909 and parameters: {'observation_period_num': 60, 'train_rates': 0.7410667224096268, 'learning_rate': 0.00022657638348724902, 'batch_size': 166, 'step_size': 11, 'gamma': 0.8389253669373385}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:12:24,544][0m Trial 27 finished with value: 0.03785164151565138 and parameters: {'observation_period_num': 33, 'train_rates': 0.8296173858272443, 'learning_rate': 0.00041903652715498337, 'batch_size': 201, 'step_size': 5, 'gamma': 0.872146489585526}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:13:27,222][0m Trial 28 finished with value: 0.12229164938131969 and parameters: {'observation_period_num': 129, 'train_rates': 0.7739099143088753, 'learning_rate': 0.0005870635292101143, 'batch_size': 142, 'step_size': 12, 'gamma': 0.8163039662684467}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:14:27,298][0m Trial 29 finished with value: 0.04515875632621649 and parameters: {'observation_period_num': 19, 'train_rates': 0.8820574901611703, 'learning_rate': 0.0006490134676669997, 'batch_size': 249, 'step_size': 10, 'gamma': 0.9380264816792582}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:16:16,619][0m Trial 30 finished with value: 0.09128888684497545 and parameters: {'observation_period_num': 72, 'train_rates': 0.8484787114834582, 'learning_rate': 0.00096034472154622, 'batch_size': 77, 'step_size': 7, 'gamma': 0.7805295189339908}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:17:15,916][0m Trial 31 finished with value: 0.03951274182219974 and parameters: {'observation_period_num': 7, 'train_rates': 0.8964358221068591, 'learning_rate': 0.0009424820467401196, 'batch_size': 220, 'step_size': 4, 'gamma': 0.8526827619248338}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:18:17,149][0m Trial 32 finished with value: 0.04197105962417221 and parameters: {'observation_period_num': 13, 'train_rates': 0.9240145855333197, 'learning_rate': 0.0009546463358250912, 'batch_size': 209, 'step_size': 3, 'gamma': 0.8573436951862721}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:19:09,044][0m Trial 33 finished with value: 0.09154921880541855 and parameters: {'observation_period_num': 39, 'train_rates': 0.7177017968188623, 'learning_rate': 0.0004612294537665742, 'batch_size': 233, 'step_size': 5, 'gamma': 0.8470907178886536}. Best is trial 12 with value: 0.02720414660871029.[0m
Early stopping at epoch 59
[32m[I 2025-01-10 05:19:43,431][0m Trial 34 finished with value: 0.09917431832130613 and parameters: {'observation_period_num': 10, 'train_rates': 0.7635346341401268, 'learning_rate': 0.00020147582286347395, 'batch_size': 179, 'step_size': 1, 'gamma': 0.7949555968842325}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:20:36,653][0m Trial 35 finished with value: 0.16254967018196234 and parameters: {'observation_period_num': 243, 'train_rates': 0.8132092890739342, 'learning_rate': 0.00013784387783369125, 'batch_size': 211, 'step_size': 7, 'gamma': 0.895439577037425}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:21:54,852][0m Trial 36 finished with value: 0.036423075006612196 and parameters: {'observation_period_num': 16, 'train_rates': 0.9492649370645634, 'learning_rate': 0.0002846445166069551, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9889935515342485}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:22:56,953][0m Trial 37 finished with value: 0.06973455039778165 and parameters: {'observation_period_num': 66, 'train_rates': 0.8950213565266115, 'learning_rate': 0.00010075264164332573, 'batch_size': 194, 'step_size': 5, 'gamma': 0.9162219888136816}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:23:55,233][0m Trial 38 finished with value: 0.04511268899580579 and parameters: {'observation_period_num': 40, 'train_rates': 0.8609979044932734, 'learning_rate': 0.0006140144210115815, 'batch_size': 229, 'step_size': 9, 'gamma': 0.8234927446573237}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:24:56,012][0m Trial 39 finished with value: 0.08324562497437 and parameters: {'observation_period_num': 91, 'train_rates': 0.7555707384041849, 'learning_rate': 0.00044238455387822555, 'batch_size': 154, 'step_size': 2, 'gamma': 0.8817125067948034}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:25:51,445][0m Trial 40 finished with value: 0.10001239403065154 and parameters: {'observation_period_num': 115, 'train_rates': 0.793624767608621, 'learning_rate': 0.00016309272553673943, 'batch_size': 241, 'step_size': 8, 'gamma': 0.9565012106729008}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:27:08,508][0m Trial 41 finished with value: 0.038584140414344284 and parameters: {'observation_period_num': 22, 'train_rates': 0.9087725008925207, 'learning_rate': 0.0003145647264675753, 'batch_size': 127, 'step_size': 3, 'gamma': 0.860141443488564}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:28:22,980][0m Trial 42 finished with value: 0.03579896315932274 and parameters: {'observation_period_num': 5, 'train_rates': 0.9482413148036405, 'learning_rate': 0.0007416435787908529, 'batch_size': 135, 'step_size': 4, 'gamma': 0.9731823870812629}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:29:53,672][0m Trial 43 finished with value: 0.04219834879040718 and parameters: {'observation_period_num': 6, 'train_rates': 0.9722977755573026, 'learning_rate': 0.0007231617663387015, 'batch_size': 114, 'step_size': 4, 'gamma': 0.9610894725580732}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:30:55,329][0m Trial 44 finished with value: 0.050667401403188705 and parameters: {'observation_period_num': 20, 'train_rates': 0.9500360851393288, 'learning_rate': 0.0007867712854913834, 'batch_size': 218, 'step_size': 5, 'gamma': 0.9742368922836939}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:32:06,320][0m Trial 45 finished with value: 0.038117646638836176 and parameters: {'observation_period_num': 5, 'train_rates': 0.9360001871529261, 'learning_rate': 0.0005006653283801319, 'batch_size': 141, 'step_size': 6, 'gamma': 0.8903039624110255}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:33:01,637][0m Trial 46 finished with value: 0.10301255034458916 and parameters: {'observation_period_num': 50, 'train_rates': 0.6902536931230427, 'learning_rate': 0.0003654698366542227, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8413377188416067}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:34:06,020][0m Trial 47 finished with value: 0.06957871466875076 and parameters: {'observation_period_num': 32, 'train_rates': 0.9717302383822664, 'learning_rate': 0.0008118414491906486, 'batch_size': 255, 'step_size': 2, 'gamma': 0.8678373197174422}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:35:54,833][0m Trial 48 finished with value: 0.3713036533173597 and parameters: {'observation_period_num': 21, 'train_rates': 0.8831816437989769, 'learning_rate': 1.3279694776450196e-06, 'batch_size': 81, 'step_size': 4, 'gamma': 0.7680789578757357}. Best is trial 12 with value: 0.02720414660871029.[0m
[32m[I 2025-01-10 05:36:50,872][0m Trial 49 finished with value: 0.048794147999663105 and parameters: {'observation_period_num': 40, 'train_rates': 0.7718337214298197, 'learning_rate': 0.0001884967770218176, 'batch_size': 176, 'step_size': 7, 'gamma': 0.9026871146324519}. Best is trial 12 with value: 0.02720414660871029.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-10 05:36:50,884][0m A new study created in memory with name: no-name-8eb0c483-8ff3-46ce-b668-955421494bb5[0m
[32m[I 2025-01-10 05:37:45,996][0m Trial 0 finished with value: 0.09868786019998937 and parameters: {'observation_period_num': 45, 'train_rates': 0.7102286240120476, 'learning_rate': 1.5777593162709256e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8447563617823273}. Best is trial 0 with value: 0.09868786019998937.[0m
[32m[I 2025-01-10 05:38:37,003][0m Trial 1 finished with value: 0.10748290460653118 and parameters: {'observation_period_num': 43, 'train_rates': 0.6146663083013216, 'learning_rate': 3.094880061477014e-05, 'batch_size': 154, 'step_size': 13, 'gamma': 0.8563141888231216}. Best is trial 0 with value: 0.09868786019998937.[0m
[32m[I 2025-01-10 05:39:41,942][0m Trial 2 finished with value: 0.055774081498384476 and parameters: {'observation_period_num': 41, 'train_rates': 0.9772545408509565, 'learning_rate': 0.0008542078053930389, 'batch_size': 231, 'step_size': 3, 'gamma': 0.7959001209738477}. Best is trial 2 with value: 0.055774081498384476.[0m
[32m[I 2025-01-10 05:40:41,218][0m Trial 3 finished with value: 0.057023383107061135 and parameters: {'observation_period_num': 14, 'train_rates': 0.8465983008478188, 'learning_rate': 7.005241558893807e-06, 'batch_size': 197, 'step_size': 4, 'gamma': 0.9761266681082831}. Best is trial 2 with value: 0.055774081498384476.[0m
[32m[I 2025-01-10 05:41:47,091][0m Trial 4 finished with value: 0.3825025563405839 and parameters: {'observation_period_num': 21, 'train_rates': 0.6909198341339566, 'learning_rate': 1.1835869675830982e-06, 'batch_size': 123, 'step_size': 12, 'gamma': 0.8750101934899309}. Best is trial 2 with value: 0.055774081498384476.[0m
[32m[I 2025-01-10 05:43:54,866][0m Trial 5 finished with value: 0.08755489807945115 and parameters: {'observation_period_num': 118, 'train_rates': 0.9216905921563341, 'learning_rate': 0.0005695511449277628, 'batch_size': 71, 'step_size': 6, 'gamma': 0.779484859437607}. Best is trial 2 with value: 0.055774081498384476.[0m
[32m[I 2025-01-10 05:45:05,252][0m Trial 6 finished with value: 0.027367564463397352 and parameters: {'observation_period_num': 19, 'train_rates': 0.9163480962951636, 'learning_rate': 0.000715476798233295, 'batch_size': 144, 'step_size': 5, 'gamma': 0.9288591660124619}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:46:08,858][0m Trial 7 finished with value: 0.11597189623937916 and parameters: {'observation_period_num': 81, 'train_rates': 0.9142596831640504, 'learning_rate': 0.0006222465676950966, 'batch_size': 157, 'step_size': 14, 'gamma': 0.9716518772714915}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:47:31,260][0m Trial 8 finished with value: 0.11457969247956475 and parameters: {'observation_period_num': 20, 'train_rates': 0.6245013564855609, 'learning_rate': 5.9523232937819514e-06, 'batch_size': 83, 'step_size': 5, 'gamma': 0.9395319494632561}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:48:53,427][0m Trial 9 finished with value: 0.05831169227082935 and parameters: {'observation_period_num': 9, 'train_rates': 0.621942174977032, 'learning_rate': 0.000104755574262235, 'batch_size': 87, 'step_size': 3, 'gamma': 0.9356330898470968}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:54:38,445][0m Trial 10 finished with value: 0.11535717968244028 and parameters: {'observation_period_num': 197, 'train_rates': 0.8089288557992241, 'learning_rate': 0.00010967029862226369, 'batch_size': 22, 'step_size': 1, 'gamma': 0.8986395861088818}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:55:41,776][0m Trial 11 finished with value: 0.1211557537317276 and parameters: {'observation_period_num': 93, 'train_rates': 0.9875639656707431, 'learning_rate': 0.0009908892305072942, 'batch_size': 250, 'step_size': 9, 'gamma': 0.7546902905459089}. Best is trial 6 with value: 0.027367564463397352.[0m
Early stopping at epoch 55
[32m[I 2025-01-10 05:56:16,073][0m Trial 12 finished with value: 0.24810053408145905 and parameters: {'observation_period_num': 182, 'train_rates': 0.9807107571324845, 'learning_rate': 0.00019645656718198513, 'batch_size': 217, 'step_size': 1, 'gamma': 0.808239271851204}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:57:12,876][0m Trial 13 finished with value: 0.12986692780280018 and parameters: {'observation_period_num': 240, 'train_rates': 0.9077652289424587, 'learning_rate': 0.000254064305013563, 'batch_size': 206, 'step_size': 7, 'gamma': 0.8181098940932786}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:58:09,414][0m Trial 14 finished with value: 0.0508506083548469 and parameters: {'observation_period_num': 68, 'train_rates': 0.8451205641714209, 'learning_rate': 0.000376035962581574, 'batch_size': 253, 'step_size': 3, 'gamma': 0.9133785939453555}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 05:59:28,985][0m Trial 15 finished with value: 0.05678285028564163 and parameters: {'observation_period_num': 79, 'train_rates': 0.8593953426964265, 'learning_rate': 0.00029661592210575093, 'batch_size': 109, 'step_size': 8, 'gamma': 0.9092467491098783}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:00:23,238][0m Trial 16 finished with value: 0.0874456162047055 and parameters: {'observation_period_num': 119, 'train_rates': 0.7714238109674117, 'learning_rate': 7.746455218054878e-05, 'batch_size': 183, 'step_size': 3, 'gamma': 0.9361658950817874}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:06:00,724][0m Trial 17 finished with value: 0.16971650025887483 and parameters: {'observation_period_num': 155, 'train_rates': 0.7717205588206364, 'learning_rate': 0.0003944401052862891, 'batch_size': 22, 'step_size': 11, 'gamma': 0.899714542279717}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:06:59,185][0m Trial 18 finished with value: 0.0785024057881942 and parameters: {'observation_period_num': 53, 'train_rates': 0.8727325789898512, 'learning_rate': 4.701806675136867e-05, 'batch_size': 242, 'step_size': 5, 'gamma': 0.9569684927728109}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:09:33,138][0m Trial 19 finished with value: 0.054452186769880284 and parameters: {'observation_period_num': 71, 'train_rates': 0.8219680945521421, 'learning_rate': 0.0001597579341094762, 'batch_size': 53, 'step_size': 2, 'gamma': 0.8786572533945359}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:10:47,623][0m Trial 20 finished with value: 0.07961577978092811 and parameters: {'observation_period_num': 101, 'train_rates': 0.9395625254282951, 'learning_rate': 4.399661652726126e-05, 'batch_size': 132, 'step_size': 6, 'gamma': 0.9183357900015998}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:13:35,678][0m Trial 21 finished with value: 0.05641441224586396 and parameters: {'observation_period_num': 62, 'train_rates': 0.8184975775479709, 'learning_rate': 0.00035996575603491156, 'batch_size': 50, 'step_size': 2, 'gamma': 0.8744141499414685}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:16:09,634][0m Trial 22 finished with value: 0.07252184461418301 and parameters: {'observation_period_num': 68, 'train_rates': 0.875656226237227, 'learning_rate': 0.00017619112591307605, 'batch_size': 56, 'step_size': 4, 'gamma': 0.8830585457385793}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:17:22,434][0m Trial 23 finished with value: 0.13986277246526235 and parameters: {'observation_period_num': 155, 'train_rates': 0.7495271177280761, 'learning_rate': 0.0001495029574922489, 'batch_size': 106, 'step_size': 2, 'gamma': 0.8409115832926929}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:18:22,503][0m Trial 24 finished with value: 0.03352021019807573 and parameters: {'observation_period_num': 32, 'train_rates': 0.8313287810921894, 'learning_rate': 0.0004765824944701812, 'batch_size': 183, 'step_size': 5, 'gamma': 0.914300891726541}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:19:25,741][0m Trial 25 finished with value: 0.04442828466925459 and parameters: {'observation_period_num': 31, 'train_rates': 0.8894286534368145, 'learning_rate': 0.0004913199365962838, 'batch_size': 184, 'step_size': 7, 'gamma': 0.925242975479459}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:20:31,394][0m Trial 26 finished with value: 0.04088582098484039 and parameters: {'observation_period_num': 6, 'train_rates': 0.9464602025220442, 'learning_rate': 0.0006127815745856587, 'batch_size': 182, 'step_size': 7, 'gamma': 0.953902280137224}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:21:39,437][0m Trial 27 finished with value: 0.03153927996754646 and parameters: {'observation_period_num': 5, 'train_rates': 0.9486775460315098, 'learning_rate': 0.000885901090893017, 'batch_size': 172, 'step_size': 10, 'gamma': 0.9477266998503403}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:22:50,794][0m Trial 28 finished with value: 0.06776390224695206 and parameters: {'observation_period_num': 37, 'train_rates': 0.9567930519884036, 'learning_rate': 0.0008845894180282838, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9515347748669174}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:23:46,981][0m Trial 29 finished with value: 0.05950843621421298 and parameters: {'observation_period_num': 33, 'train_rates': 0.7012634784680303, 'learning_rate': 2.53584463614928e-05, 'batch_size': 170, 'step_size': 9, 'gamma': 0.9892734307199905}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:24:57,277][0m Trial 30 finished with value: 0.07347815370601816 and parameters: {'observation_period_num': 51, 'train_rates': 0.9027450150226501, 'learning_rate': 9.420814529905673e-06, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9684926861772668}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:26:03,491][0m Trial 31 finished with value: 0.04665867577706064 and parameters: {'observation_period_num': 5, 'train_rates': 0.9407753741018802, 'learning_rate': 0.0006415534933071434, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9547067525674264}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:27:08,128][0m Trial 32 finished with value: 0.05910908803343773 and parameters: {'observation_period_num': 23, 'train_rates': 0.955509374644271, 'learning_rate': 0.0009905228198777694, 'batch_size': 194, 'step_size': 8, 'gamma': 0.9435799317302743}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:28:12,560][0m Trial 33 finished with value: 0.03252605456858873 and parameters: {'observation_period_num': 8, 'train_rates': 0.9323504500305024, 'learning_rate': 0.00029096672094857664, 'batch_size': 164, 'step_size': 6, 'gamma': 0.8956642914830697}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:29:18,770][0m Trial 34 finished with value: 0.04296062465956701 and parameters: {'observation_period_num': 48, 'train_rates': 0.9228646608064881, 'learning_rate': 0.0002561701858344595, 'batch_size': 159, 'step_size': 6, 'gamma': 0.8970702734378887}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:30:23,633][0m Trial 35 finished with value: 0.34166157245635986 and parameters: {'observation_period_num': 28, 'train_rates': 0.9658766505107438, 'learning_rate': 2.163009825542268e-06, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9211762287605231}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:31:30,536][0m Trial 36 finished with value: 0.06625525378867199 and parameters: {'observation_period_num': 41, 'train_rates': 0.7325543042161909, 'learning_rate': 0.00044076044642513764, 'batch_size': 123, 'step_size': 12, 'gamma': 0.8557304674590499}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:32:38,261][0m Trial 37 finished with value: 0.030771138163250007 and parameters: {'observation_period_num': 18, 'train_rates': 0.8864432388137693, 'learning_rate': 0.0007271633645638999, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8901797742125234}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:33:44,693][0m Trial 38 finished with value: 0.04701900102001481 and parameters: {'observation_period_num': 15, 'train_rates': 0.883980799351807, 'learning_rate': 0.0006963088447838144, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8875700932977317}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:35:06,451][0m Trial 39 finished with value: 0.035150912531592825 and parameters: {'observation_period_num': 22, 'train_rates': 0.9244293728070102, 'learning_rate': 0.00024934611101656834, 'batch_size': 116, 'step_size': 4, 'gamma': 0.8629276742112983}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:36:01,419][0m Trial 40 finished with value: 0.22205288467234116 and parameters: {'observation_period_num': 247, 'train_rates': 0.6641301203773438, 'learning_rate': 0.0006951613852019115, 'batch_size': 136, 'step_size': 8, 'gamma': 0.8497404751290245}. Best is trial 6 with value: 0.027367564463397352.[0m
[32m[I 2025-01-10 06:37:01,674][0m Trial 41 finished with value: 0.027322742133006707 and parameters: {'observation_period_num': 13, 'train_rates': 0.8394732978027012, 'learning_rate': 0.0004614785229564763, 'batch_size': 165, 'step_size': 5, 'gamma': 0.9271553016246026}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:38:05,235][0m Trial 42 finished with value: 0.03230234093754716 and parameters: {'observation_period_num': 5, 'train_rates': 0.8553909334819825, 'learning_rate': 0.0007099915600312108, 'batch_size': 165, 'step_size': 4, 'gamma': 0.9272465610307876}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:39:10,334][0m Trial 43 finished with value: 0.030450801397490113 and parameters: {'observation_period_num': 19, 'train_rates': 0.8531500918558436, 'learning_rate': 0.0008156451283892048, 'batch_size': 152, 'step_size': 4, 'gamma': 0.9279262676833685}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:40:18,473][0m Trial 44 finished with value: 0.10839308187365532 and parameters: {'observation_period_num': 54, 'train_rates': 0.8965239530956998, 'learning_rate': 0.000965452145317172, 'batch_size': 147, 'step_size': 15, 'gamma': 0.9710676797410931}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:41:26,379][0m Trial 45 finished with value: 0.03311310825173301 and parameters: {'observation_period_num': 20, 'train_rates': 0.7975825236098677, 'learning_rate': 0.000515529007434293, 'batch_size': 128, 'step_size': 5, 'gamma': 0.9319974163943525}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:42:51,994][0m Trial 46 finished with value: 0.03689765097272448 and parameters: {'observation_period_num': 40, 'train_rates': 0.8426744641963204, 'learning_rate': 0.00011548051036285343, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9438506539345093}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:43:50,224][0m Trial 47 finished with value: 0.05696962773799896 and parameters: {'observation_period_num': 88, 'train_rates': 0.8645037596272162, 'learning_rate': 0.00034162596931065645, 'batch_size': 194, 'step_size': 4, 'gamma': 0.9064988434047323}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:44:59,575][0m Trial 48 finished with value: 0.0346793012537119 and parameters: {'observation_period_num': 23, 'train_rates': 0.910785952196813, 'learning_rate': 0.0007559396990355469, 'batch_size': 154, 'step_size': 10, 'gamma': 0.8290028521480066}. Best is trial 41 with value: 0.027322742133006707.[0m
[32m[I 2025-01-10 06:45:56,650][0m Trial 49 finished with value: 0.03905761821668631 and parameters: {'observation_period_num': 16, 'train_rates': 0.7880819949744794, 'learning_rate': 7.118155164305258e-05, 'batch_size': 203, 'step_size': 3, 'gamma': 0.9858916180870582}. Best is trial 41 with value: 0.027322742133006707.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-10 06:45:56,660][0m A new study created in memory with name: no-name-816c3973-e5bd-4422-8f39-822f4a21cadd[0m
[32m[I 2025-01-10 06:47:08,481][0m Trial 0 finished with value: 0.05673617124557495 and parameters: {'observation_period_num': 20, 'train_rates': 0.9843044158542631, 'learning_rate': 2.9792463855965308e-05, 'batch_size': 159, 'step_size': 9, 'gamma': 0.9800506801955484}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 06:48:09,418][0m Trial 1 finished with value: 0.387004017829895 and parameters: {'observation_period_num': 159, 'train_rates': 0.9633567839786576, 'learning_rate': 2.918722948240429e-06, 'batch_size': 228, 'step_size': 8, 'gamma': 0.9821974687532432}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 06:51:19,627][0m Trial 2 finished with value: 0.1073418642361493 and parameters: {'observation_period_num': 35, 'train_rates': 0.6200167565444737, 'learning_rate': 0.00014656607697193742, 'batch_size': 35, 'step_size': 6, 'gamma': 0.756580830828981}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 06:55:25,441][0m Trial 3 finished with value: 0.06162060285259408 and parameters: {'observation_period_num': 64, 'train_rates': 0.8817987039634183, 'learning_rate': 0.0006382528645152197, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9339125829822493}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 06:56:23,260][0m Trial 4 finished with value: 0.1406598687171936 and parameters: {'observation_period_num': 193, 'train_rates': 0.9262968157380838, 'learning_rate': 6.000271997715569e-05, 'batch_size': 223, 'step_size': 9, 'gamma': 0.9809955346718763}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 06:57:48,318][0m Trial 5 finished with value: 0.5827698277645423 and parameters: {'observation_period_num': 225, 'train_rates': 0.9555852571780051, 'learning_rate': 1.0343710619584732e-06, 'batch_size': 106, 'step_size': 8, 'gamma': 0.8891296936840141}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 06:58:41,477][0m Trial 6 finished with value: 0.13791429017922968 and parameters: {'observation_period_num': 177, 'train_rates': 0.7528847981729778, 'learning_rate': 0.00011701171098166293, 'batch_size': 183, 'step_size': 5, 'gamma': 0.8266054097362973}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 07:00:53,401][0m Trial 7 finished with value: 0.29372739212888394 and parameters: {'observation_period_num': 53, 'train_rates': 0.8315948027806612, 'learning_rate': 1.8164957671655427e-06, 'batch_size': 64, 'step_size': 4, 'gamma': 0.8611695082953705}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 07:03:30,427][0m Trial 8 finished with value: 0.12357718189921948 and parameters: {'observation_period_num': 94, 'train_rates': 0.9621614566427815, 'learning_rate': 0.00011380777047074882, 'batch_size': 59, 'step_size': 4, 'gamma': 0.9788597332474571}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 07:04:28,862][0m Trial 9 finished with value: 0.13585028462032855 and parameters: {'observation_period_num': 100, 'train_rates': 0.8350679383830238, 'learning_rate': 0.00040433548941083794, 'batch_size': 233, 'step_size': 12, 'gamma': 0.9165714415433884}. Best is trial 0 with value: 0.05673617124557495.[0m
Early stopping at epoch 53
[32m[I 2025-01-10 07:05:01,252][0m Trial 10 finished with value: 0.2018375640457615 and parameters: {'observation_period_num': 6, 'train_rates': 0.7449064946128963, 'learning_rate': 1.3740306795367882e-05, 'batch_size': 150, 'step_size': 1, 'gamma': 0.8028396916462773}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 07:06:19,450][0m Trial 11 finished with value: 0.05690239671241032 and parameters: {'observation_period_num': 60, 'train_rates': 0.8771736603387931, 'learning_rate': 1.864618315169121e-05, 'batch_size': 118, 'step_size': 15, 'gamma': 0.9331018679845223}. Best is trial 0 with value: 0.05673617124557495.[0m
[32m[I 2025-01-10 07:07:32,334][0m Trial 12 finished with value: 0.04958882330950484 and parameters: {'observation_period_num': 17, 'train_rates': 0.8868495089306103, 'learning_rate': 1.1493313480061564e-05, 'batch_size': 132, 'step_size': 15, 'gamma': 0.9367590026167915}. Best is trial 12 with value: 0.04958882330950484.[0m
[32m[I 2025-01-10 07:08:41,105][0m Trial 13 finished with value: 0.04950873181223869 and parameters: {'observation_period_num': 15, 'train_rates': 0.9874578379849049, 'learning_rate': 8.179153182768628e-06, 'batch_size': 164, 'step_size': 12, 'gamma': 0.9518977503265021}. Best is trial 13 with value: 0.04950873181223869.[0m
[32m[I 2025-01-10 07:09:39,641][0m Trial 14 finished with value: 0.2363699453579981 and parameters: {'observation_period_num': 115, 'train_rates': 0.8841629166877875, 'learning_rate': 5.392937823237921e-06, 'batch_size': 193, 'step_size': 12, 'gamma': 0.9036388025571916}. Best is trial 13 with value: 0.04950873181223869.[0m
[32m[I 2025-01-10 07:11:18,566][0m Trial 15 finished with value: 0.04800160870151228 and parameters: {'observation_period_num': 7, 'train_rates': 0.9225078617236626, 'learning_rate': 7.69488484636159e-06, 'batch_size': 95, 'step_size': 12, 'gamma': 0.936082947621135}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:12:49,961][0m Trial 16 finished with value: 0.2753190755319315 and parameters: {'observation_period_num': 139, 'train_rates': 0.6670871777834599, 'learning_rate': 6.381384524691171e-06, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8628717479919481}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:14:24,406][0m Trial 17 finished with value: 0.13626744237638289 and parameters: {'observation_period_num': 89, 'train_rates': 0.9134802443761676, 'learning_rate': 5.536360100996769e-06, 'batch_size': 96, 'step_size': 11, 'gamma': 0.939736967604743}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:15:21,135][0m Trial 18 finished with value: 0.23744941185945753 and parameters: {'observation_period_num': 246, 'train_rates': 0.8073118768566411, 'learning_rate': 4.003557654443759e-05, 'batch_size': 173, 'step_size': 13, 'gamma': 0.9575371871659271}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:16:37,072][0m Trial 19 finished with value: 0.10406142307652368 and parameters: {'observation_period_num': 40, 'train_rates': 0.9353517861630875, 'learning_rate': 7.522816938792158e-06, 'batch_size': 133, 'step_size': 10, 'gamma': 0.8856787142193584}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:17:31,236][0m Trial 20 finished with value: 0.338430418074131 and parameters: {'observation_period_num': 74, 'train_rates': 0.7603027457345836, 'learning_rate': 2.7316246702939625e-06, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8358398553287096}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:18:47,223][0m Trial 21 finished with value: 0.04859470652705496 and parameters: {'observation_period_num': 5, 'train_rates': 0.9062866279540112, 'learning_rate': 1.3022603413173904e-05, 'batch_size': 125, 'step_size': 15, 'gamma': 0.950214396615514}. Best is trial 15 with value: 0.04800160870151228.[0m
[32m[I 2025-01-10 07:20:38,276][0m Trial 22 finished with value: 0.04072878137230873 and parameters: {'observation_period_num': 7, 'train_rates': 0.982777790665324, 'learning_rate': 2.146488874895407e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.9584448798115897}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:22:23,970][0m Trial 23 finished with value: 0.044053539877765564 and parameters: {'observation_period_num': 35, 'train_rates': 0.9216725975580594, 'learning_rate': 2.2209035010142802e-05, 'batch_size': 84, 'step_size': 14, 'gamma': 0.9606398108675615}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:24:14,196][0m Trial 24 finished with value: 0.048166104267127086 and parameters: {'observation_period_num': 36, 'train_rates': 0.8421180901294655, 'learning_rate': 3.43724461655841e-05, 'batch_size': 79, 'step_size': 14, 'gamma': 0.9147351773323034}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:27:06,785][0m Trial 25 finished with value: 0.06566225562644544 and parameters: {'observation_period_num': 39, 'train_rates': 0.939421080634872, 'learning_rate': 6.576579704701993e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.963651413545532}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:35:06,060][0m Trial 26 finished with value: 0.06643070990685374 and parameters: {'observation_period_num': 74, 'train_rates': 0.9890906863092165, 'learning_rate': 2.2274334803686454e-05, 'batch_size': 19, 'step_size': 10, 'gamma': 0.9131844794347755}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:36:34,092][0m Trial 27 finished with value: 0.047305759813454105 and parameters: {'observation_period_num': 29, 'train_rates': 0.7836873179860818, 'learning_rate': 0.00022698704530553076, 'batch_size': 98, 'step_size': 14, 'gamma': 0.9683270870341951}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:38:04,168][0m Trial 28 finished with value: 0.2584931982878376 and parameters: {'observation_period_num': 132, 'train_rates': 0.6986312937885655, 'learning_rate': 0.0002607470619895983, 'batch_size': 81, 'step_size': 14, 'gamma': 0.9899512873279434}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:39:05,307][0m Trial 29 finished with value: 0.05219884219204451 and parameters: {'observation_period_num': 35, 'train_rates': 0.7889089375362224, 'learning_rate': 0.0008657319086522149, 'batch_size': 150, 'step_size': 14, 'gamma': 0.9708489939048524}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:40:16,336][0m Trial 30 finished with value: 0.04524806356162168 and parameters: {'observation_period_num': 25, 'train_rates': 0.6960484169939685, 'learning_rate': 5.579202576464937e-05, 'batch_size': 108, 'step_size': 10, 'gamma': 0.9620153405865604}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:41:25,486][0m Trial 31 finished with value: 0.042820496026510946 and parameters: {'observation_period_num': 26, 'train_rates': 0.6831519803177425, 'learning_rate': 7.002373139898896e-05, 'batch_size': 107, 'step_size': 10, 'gamma': 0.9668687042188753}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:42:32,916][0m Trial 32 finished with value: 0.06729234443634682 and parameters: {'observation_period_num': 52, 'train_rates': 0.6891778619963347, 'learning_rate': 5.9461754114188845e-05, 'batch_size': 112, 'step_size': 7, 'gamma': 0.9580801760566494}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:43:22,978][0m Trial 33 finished with value: 0.10413402849738311 and parameters: {'observation_period_num': 26, 'train_rates': 0.6365015259676418, 'learning_rate': 2.8438435832001244e-05, 'batch_size': 255, 'step_size': 9, 'gamma': 0.9727915952239384}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:45:18,314][0m Trial 34 finished with value: 0.05012772251434045 and parameters: {'observation_period_num': 23, 'train_rates': 0.727668293289047, 'learning_rate': 4.510016544000096e-05, 'batch_size': 67, 'step_size': 10, 'gamma': 0.9235760068314965}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:46:42,475][0m Trial 35 finished with value: 0.17638053627801942 and parameters: {'observation_period_num': 76, 'train_rates': 0.6023863125515078, 'learning_rate': 7.314268552604142e-05, 'batch_size': 86, 'step_size': 11, 'gamma': 0.9867912725101757}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:47:49,150][0m Trial 36 finished with value: 0.06817022214835955 and parameters: {'observation_period_num': 47, 'train_rates': 0.6600371912855225, 'learning_rate': 9.264186848072045e-05, 'batch_size': 116, 'step_size': 7, 'gamma': 0.750945784455735}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:50:43,977][0m Trial 37 finished with value: 0.11781131629964477 and parameters: {'observation_period_num': 66, 'train_rates': 0.7196984557116567, 'learning_rate': 0.00018383758659813553, 'batch_size': 43, 'step_size': 9, 'gamma': 0.7724117968241131}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:51:41,332][0m Trial 38 finished with value: 0.08502538649134656 and parameters: {'observation_period_num': 20, 'train_rates': 0.6721316165921752, 'learning_rate': 2.3516894848202112e-05, 'batch_size': 143, 'step_size': 11, 'gamma': 0.8945433807086742}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:53:51,179][0m Trial 39 finished with value: 0.11948137764615574 and parameters: {'observation_period_num': 203, 'train_rates': 0.9686989187864894, 'learning_rate': 1.6512781179150938e-05, 'batch_size': 70, 'step_size': 8, 'gamma': 0.9481474751246307}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:54:59,689][0m Trial 40 finished with value: 0.4744087425422293 and parameters: {'observation_period_num': 164, 'train_rates': 0.6351318262189689, 'learning_rate': 4.064779138119256e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9262483084591823}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:56:19,216][0m Trial 41 finished with value: 0.04114809993856637 and parameters: {'observation_period_num': 28, 'train_rates': 0.7795757244076367, 'learning_rate': 0.0002264336268915663, 'batch_size': 106, 'step_size': 14, 'gamma': 0.9700621565887726}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:57:29,083][0m Trial 42 finished with value: 0.09432315663187749 and parameters: {'observation_period_num': 50, 'train_rates': 0.7074550597806927, 'learning_rate': 0.0003594344335526181, 'batch_size': 108, 'step_size': 13, 'gamma': 0.9810627138674921}. Best is trial 22 with value: 0.04072878137230873.[0m
[32m[I 2025-01-10 07:59:03,902][0m Trial 43 finished with value: 0.03343949011237208 and parameters: {'observation_period_num': 15, 'train_rates': 0.8567678947561301, 'learning_rate': 0.00016215054062851053, 'batch_size': 92, 'step_size': 1, 'gamma': 0.9459049752030102}. Best is trial 43 with value: 0.03343949011237208.[0m
[32m[I 2025-01-10 08:00:42,468][0m Trial 44 finished with value: 0.026407904548650725 and parameters: {'observation_period_num': 13, 'train_rates': 0.8578898096584916, 'learning_rate': 0.00014918749520693214, 'batch_size': 87, 'step_size': 2, 'gamma': 0.9449242060567072}. Best is trial 44 with value: 0.026407904548650725.[0m
[32m[I 2025-01-10 08:03:29,115][0m Trial 45 finished with value: 0.027858536752978426 and parameters: {'observation_period_num': 5, 'train_rates': 0.8484028644519519, 'learning_rate': 0.00013287779812329233, 'batch_size': 51, 'step_size': 1, 'gamma': 0.9480539185733563}. Best is trial 44 with value: 0.026407904548650725.[0m
[32m[I 2025-01-10 08:06:20,585][0m Trial 46 finished with value: 0.02659959283819631 and parameters: {'observation_period_num': 12, 'train_rates': 0.8575856272164104, 'learning_rate': 0.00013380497900765504, 'batch_size': 50, 'step_size': 1, 'gamma': 0.9460920996030171}. Best is trial 44 with value: 0.026407904548650725.[0m
[32m[I 2025-01-10 08:12:34,441][0m Trial 47 finished with value: 0.02266118690662139 and parameters: {'observation_period_num': 13, 'train_rates': 0.8563005615627018, 'learning_rate': 0.0001332019475426905, 'batch_size': 23, 'step_size': 1, 'gamma': 0.9423563005108274}. Best is trial 47 with value: 0.02266118690662139.[0m
[32m[I 2025-01-10 08:20:11,515][0m Trial 48 finished with value: 0.021794593516792277 and parameters: {'observation_period_num': 14, 'train_rates': 0.8619700754466173, 'learning_rate': 0.00013582259781636994, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9439938673358765}. Best is trial 48 with value: 0.021794593516792277.[0m
[32m[I 2025-01-10 08:26:23,578][0m Trial 49 finished with value: 0.0241343982133594 and parameters: {'observation_period_num': 15, 'train_rates': 0.8249353645960479, 'learning_rate': 0.00011244572030566932, 'batch_size': 22, 'step_size': 2, 'gamma': 0.9003744520234347}. Best is trial 48 with value: 0.021794593516792277.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-10 08:26:23,588][0m A new study created in memory with name: no-name-72a3aacf-d171-437a-8a48-0025c4596efd[0m
[32m[I 2025-01-10 08:29:15,178][0m Trial 0 finished with value: 0.13377271817280695 and parameters: {'observation_period_num': 204, 'train_rates': 0.8820257733155396, 'learning_rate': 6.32575652246658e-05, 'batch_size': 48, 'step_size': 10, 'gamma': 0.9551647576192686}. Best is trial 0 with value: 0.13377271817280695.[0m
[32m[I 2025-01-10 08:30:08,679][0m Trial 1 finished with value: 0.07327041921653146 and parameters: {'observation_period_num': 63, 'train_rates': 0.7363655834272489, 'learning_rate': 7.576916715610325e-05, 'batch_size': 204, 'step_size': 14, 'gamma': 0.7823519188920207}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:34:02,560][0m Trial 2 finished with value: 0.10004794942673112 and parameters: {'observation_period_num': 39, 'train_rates': 0.6648735637485618, 'learning_rate': 0.00018095649068384915, 'batch_size': 30, 'step_size': 3, 'gamma': 0.8797393775471323}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:35:49,695][0m Trial 3 finished with value: 0.10947489859215144 and parameters: {'observation_period_num': 115, 'train_rates': 0.642317282541202, 'learning_rate': 0.00026668616389349613, 'batch_size': 63, 'step_size': 5, 'gamma': 0.982974242296324}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:36:41,329][0m Trial 4 finished with value: 0.3797943514688672 and parameters: {'observation_period_num': 138, 'train_rates': 0.7331004157209833, 'learning_rate': 2.724549896655004e-06, 'batch_size': 239, 'step_size': 12, 'gamma': 0.9330839521243227}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:37:32,023][0m Trial 5 finished with value: 0.47100951339377733 and parameters: {'observation_period_num': 194, 'train_rates': 0.7345158981940744, 'learning_rate': 4.337472641529902e-06, 'batch_size': 232, 'step_size': 9, 'gamma': 0.8095362453757728}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:38:31,834][0m Trial 6 finished with value: 0.28894362710771104 and parameters: {'observation_period_num': 236, 'train_rates': 0.806912077616016, 'learning_rate': 1.0478649386247933e-05, 'batch_size': 152, 'step_size': 6, 'gamma': 0.7566611986005221}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:39:38,030][0m Trial 7 finished with value: 0.08336131862576726 and parameters: {'observation_period_num': 74, 'train_rates': 0.7086567347467609, 'learning_rate': 0.0006546081548120345, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8500044619277098}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:40:39,207][0m Trial 8 finished with value: 0.5839034914970398 and parameters: {'observation_period_num': 90, 'train_rates': 0.9659022819612503, 'learning_rate': 1.2426436055817624e-06, 'batch_size': 217, 'step_size': 4, 'gamma': 0.9237474228914904}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:41:40,097][0m Trial 9 finished with value: 0.09857354313135147 and parameters: {'observation_period_num': 41, 'train_rates': 0.9365830033430526, 'learning_rate': 1.4346089796122645e-05, 'batch_size': 249, 'step_size': 14, 'gamma': 0.7771865090845483}. Best is trial 1 with value: 0.07327041921653146.[0m
[32m[I 2025-01-10 08:42:41,125][0m Trial 10 finished with value: 0.03413685226780024 and parameters: {'observation_period_num': 14, 'train_rates': 0.8267616060868839, 'learning_rate': 5.6376060813373736e-05, 'batch_size': 170, 'step_size': 15, 'gamma': 0.8232544482764373}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:43:44,226][0m Trial 11 finished with value: 0.03925476331151862 and parameters: {'observation_period_num': 32, 'train_rates': 0.8404370455807976, 'learning_rate': 5.063583678458167e-05, 'batch_size': 175, 'step_size': 15, 'gamma': 0.818972145379347}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:44:43,580][0m Trial 12 finished with value: 0.03806210004118525 and parameters: {'observation_period_num': 14, 'train_rates': 0.8363632419993504, 'learning_rate': 2.949070490618233e-05, 'batch_size': 165, 'step_size': 15, 'gamma': 0.8375568797594343}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:46:04,966][0m Trial 13 finished with value: 0.04722896360885101 and parameters: {'observation_period_num': 17, 'train_rates': 0.8737384283719238, 'learning_rate': 1.9314890672283467e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8695660981932956}. Best is trial 10 with value: 0.03413685226780024.[0m
Early stopping at epoch 78
[32m[I 2025-01-10 08:46:51,719][0m Trial 14 finished with value: 0.06636546807629722 and parameters: {'observation_period_num': 7, 'train_rates': 0.7860983250102724, 'learning_rate': 0.00018149533901387543, 'batch_size': 174, 'step_size': 1, 'gamma': 0.8306097630766014}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:48:14,851][0m Trial 15 finished with value: 0.07561689290880251 and parameters: {'observation_period_num': 140, 'train_rates': 0.9065512818031363, 'learning_rate': 2.8531297362097404e-05, 'batch_size': 107, 'step_size': 11, 'gamma': 0.891452812797154}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:49:12,813][0m Trial 16 finished with value: 0.19263454770755603 and parameters: {'observation_period_num': 106, 'train_rates': 0.7951400142991656, 'learning_rate': 6.926056020184326e-06, 'batch_size': 184, 'step_size': 15, 'gamma': 0.8432289669170372}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:50:16,693][0m Trial 17 finished with value: 0.04626182136494802 and parameters: {'observation_period_num': 67, 'train_rates': 0.8384009975844072, 'learning_rate': 8.961354202894962e-05, 'batch_size': 150, 'step_size': 13, 'gamma': 0.8066978890684315}. Best is trial 10 with value: 0.03413685226780024.[0m
[32m[I 2025-01-10 08:52:06,028][0m Trial 18 finished with value: 0.018147286027669907 and parameters: {'observation_period_num': 8, 'train_rates': 0.9890331019567238, 'learning_rate': 0.0006642510936570916, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8593517235714349}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 08:53:53,407][0m Trial 19 finished with value: 0.05342985689640045 and parameters: {'observation_period_num': 45, 'train_rates': 0.9838396948194739, 'learning_rate': 0.00046611050594099883, 'batch_size': 87, 'step_size': 8, 'gamma': 0.8961917484957964}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 08:55:08,347][0m Trial 20 finished with value: 0.2931984480657534 and parameters: {'observation_period_num': 169, 'train_rates': 0.6066277257880166, 'learning_rate': 0.0008045412913580182, 'batch_size': 85, 'step_size': 8, 'gamma': 0.8651356939994156}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 08:56:22,126][0m Trial 21 finished with value: 0.03808543581477145 and parameters: {'observation_period_num': 13, 'train_rates': 0.9340747208474038, 'learning_rate': 3.774356339828028e-05, 'batch_size': 135, 'step_size': 13, 'gamma': 0.8483469762099567}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 08:57:23,887][0m Trial 22 finished with value: 0.030441309333712394 and parameters: {'observation_period_num': 22, 'train_rates': 0.8470512359832211, 'learning_rate': 0.00011175148691250552, 'batch_size': 157, 'step_size': 10, 'gamma': 0.7976556415417799}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 08:58:20,221][0m Trial 23 finished with value: 0.05559704209026921 and parameters: {'observation_period_num': 54, 'train_rates': 0.7685161485448155, 'learning_rate': 0.00013572886171251414, 'batch_size': 197, 'step_size': 10, 'gamma': 0.7978214233525044}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:00:03,514][0m Trial 24 finished with value: 0.0673521993489101 and parameters: {'observation_period_num': 86, 'train_rates': 0.8785755139077832, 'learning_rate': 0.0003625497780909865, 'batch_size': 84, 'step_size': 10, 'gamma': 0.7611422016816842}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:01:17,103][0m Trial 25 finished with value: 0.033899765050048984 and parameters: {'observation_period_num': 28, 'train_rates': 0.9383158489108572, 'learning_rate': 0.000985563783308474, 'batch_size': 131, 'step_size': 7, 'gamma': 0.7814876211924134}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:02:40,343][0m Trial 26 finished with value: 0.03142731898697093 and parameters: {'observation_period_num': 33, 'train_rates': 0.9342096909335138, 'learning_rate': 0.0009618451846468194, 'batch_size': 117, 'step_size': 6, 'gamma': 0.7865312070244805}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:04:05,019][0m Trial 27 finished with value: 0.044984668256744506 and parameters: {'observation_period_num': 51, 'train_rates': 0.9568196024723594, 'learning_rate': 0.0004919750390771573, 'batch_size': 109, 'step_size': 7, 'gamma': 0.791177766478468}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:06:21,451][0m Trial 28 finished with value: 0.03175508632109715 and parameters: {'observation_period_num': 26, 'train_rates': 0.915575108726029, 'learning_rate': 0.00030216528805702557, 'batch_size': 66, 'step_size': 9, 'gamma': 0.7640216886994177}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:09:13,381][0m Trial 29 finished with value: 0.08597253262996674 and parameters: {'observation_period_num': 160, 'train_rates': 0.9819427205884504, 'learning_rate': 0.00012566716673691853, 'batch_size': 51, 'step_size': 9, 'gamma': 0.9183580074899615}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:13:52,327][0m Trial 30 finished with value: 0.07553843200932281 and parameters: {'observation_period_num': 87, 'train_rates': 0.9068616860043099, 'learning_rate': 0.000594213184636606, 'batch_size': 31, 'step_size': 3, 'gamma': 0.8084328348562263}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:16:05,058][0m Trial 31 finished with value: 0.031023158872629696 and parameters: {'observation_period_num': 28, 'train_rates': 0.9067771291819052, 'learning_rate': 0.00026158976656272194, 'batch_size': 67, 'step_size': 9, 'gamma': 0.7663679866082783}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:17:31,929][0m Trial 32 finished with value: 0.02672319180871311 and parameters: {'observation_period_num': 5, 'train_rates': 0.8711741402305438, 'learning_rate': 0.00022454378338943723, 'batch_size': 96, 'step_size': 7, 'gamma': 0.7507425066455223}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:19:35,319][0m Trial 33 finished with value: 0.02091251239640477 and parameters: {'observation_period_num': 7, 'train_rates': 0.8626819722818992, 'learning_rate': 0.00022870422299853198, 'batch_size': 71, 'step_size': 11, 'gamma': 0.7520005021770815}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:21:08,431][0m Trial 34 finished with value: 0.028589444601779705 and parameters: {'observation_period_num': 6, 'train_rates': 0.8673568486649045, 'learning_rate': 8.39865468359141e-05, 'batch_size': 97, 'step_size': 11, 'gamma': 0.7762330363103492}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:22:49,508][0m Trial 35 finished with value: 0.03344575180248781 and parameters: {'observation_period_num': 7, 'train_rates': 0.8620053469754734, 'learning_rate': 0.00020870060558165446, 'batch_size': 87, 'step_size': 11, 'gamma': 0.9870557554666181}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:24:10,788][0m Trial 36 finished with value: 0.057095275160044694 and parameters: {'observation_period_num': 60, 'train_rates': 0.7629380974690515, 'learning_rate': 7.750351635193306e-05, 'batch_size': 97, 'step_size': 11, 'gamma': 0.9563742081427264}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:30:45,750][0m Trial 37 finished with value: 0.18292307360904422 and parameters: {'observation_period_num': 236, 'train_rates': 0.813209488959104, 'learning_rate': 0.00037040738838327314, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7538979084704702}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:33:50,903][0m Trial 38 finished with value: 0.021185184053315466 and parameters: {'observation_period_num': 6, 'train_rates': 0.8903900714904429, 'learning_rate': 0.000189401089215155, 'batch_size': 48, 'step_size': 7, 'gamma': 0.7723535541342176}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:36:47,967][0m Trial 39 finished with value: 0.04097693729198585 and parameters: {'observation_period_num': 44, 'train_rates': 0.8987479679780286, 'learning_rate': 0.00020639645965204025, 'batch_size': 50, 'step_size': 7, 'gamma': 0.7504008303173655}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:38:50,987][0m Trial 40 finished with value: 0.09992322223175557 and parameters: {'observation_period_num': 221, 'train_rates': 0.9601950800996836, 'learning_rate': 0.0001622721700968871, 'batch_size': 72, 'step_size': 5, 'gamma': 0.7716566508753084}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:41:36,102][0m Trial 41 finished with value: 0.027773995533658773 and parameters: {'observation_period_num': 23, 'train_rates': 0.8612687535807985, 'learning_rate': 9.997166372262154e-05, 'batch_size': 52, 'step_size': 8, 'gamma': 0.7744215204721159}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:45:44,537][0m Trial 42 finished with value: 0.02154801752961551 and parameters: {'observation_period_num': 5, 'train_rates': 0.886011061622873, 'learning_rate': 0.0002498344985282065, 'batch_size': 35, 'step_size': 8, 'gamma': 0.7519544617756302}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:50:09,618][0m Trial 43 finished with value: 0.040092587664087374 and parameters: {'observation_period_num': 39, 'train_rates': 0.8874101215914114, 'learning_rate': 0.0004244181139703568, 'batch_size': 33, 'step_size': 5, 'gamma': 0.7610678669314475}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:56:32,960][0m Trial 44 finished with value: 0.03834499703662696 and parameters: {'observation_period_num': 18, 'train_rates': 0.7132123685471756, 'learning_rate': 0.00025705732359879933, 'batch_size': 19, 'step_size': 7, 'gamma': 0.7532023247944744}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 09:58:33,197][0m Trial 45 finished with value: 0.08799223732088972 and parameters: {'observation_period_num': 72, 'train_rates': 0.8868292235531127, 'learning_rate': 0.0006187816748684398, 'batch_size': 73, 'step_size': 6, 'gamma': 0.9701391296637489}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 10:02:16,980][0m Trial 46 finished with value: 0.01914674175916136 and parameters: {'observation_period_num': 5, 'train_rates': 0.8534110481556829, 'learning_rate': 0.00030491126879670884, 'batch_size': 39, 'step_size': 9, 'gamma': 0.7887568853741025}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 10:05:57,373][0m Trial 47 finished with value: 0.05749223688777921 and parameters: {'observation_period_num': 37, 'train_rates': 0.9205624173203807, 'learning_rate': 0.00034054883703232077, 'batch_size': 41, 'step_size': 9, 'gamma': 0.8166745583312403}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 10:09:16,601][0m Trial 48 finished with value: 0.051420705638395535 and parameters: {'observation_period_num': 51, 'train_rates': 0.8091207357165509, 'learning_rate': 0.0007011386001823731, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8608015845564819}. Best is trial 18 with value: 0.018147286027669907.[0m
[32m[I 2025-01-10 10:17:22,644][0m Trial 49 finished with value: 0.15096717176446878 and parameters: {'observation_period_num': 109, 'train_rates': 0.8236947743613023, 'learning_rate': 0.00016539767060152393, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7934604707874903}. Best is trial 18 with value: 0.018147286027669907.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-10 10:17:22,654][0m A new study created in memory with name: no-name-97527f0c-350c-4e13-96d3-be1f14518b03[0m
[32m[I 2025-01-10 10:18:24,113][0m Trial 0 finished with value: 0.11161546066720435 and parameters: {'observation_period_num': 106, 'train_rates': 0.8350884387754711, 'learning_rate': 4.202245077366276e-05, 'batch_size': 154, 'step_size': 5, 'gamma': 0.8460301821203546}. Best is trial 0 with value: 0.11161546066720435.[0m
[32m[I 2025-01-10 10:19:20,810][0m Trial 1 finished with value: 0.5298097755863587 and parameters: {'observation_period_num': 174, 'train_rates': 0.7542643982515903, 'learning_rate': 2.0161128052071125e-06, 'batch_size': 170, 'step_size': 1, 'gamma': 0.9859252870636116}. Best is trial 0 with value: 0.11161546066720435.[0m
[32m[I 2025-01-10 10:20:20,717][0m Trial 2 finished with value: 0.17761497105224222 and parameters: {'observation_period_num': 146, 'train_rates': 0.8874409447720928, 'learning_rate': 1.5184494344743137e-05, 'batch_size': 184, 'step_size': 11, 'gamma': 0.7624775146247749}. Best is trial 0 with value: 0.11161546066720435.[0m
[32m[I 2025-01-10 10:22:02,132][0m Trial 3 finished with value: 0.3851623342070781 and parameters: {'observation_period_num': 96, 'train_rates': 0.8756956027820342, 'learning_rate': 1.996977080730541e-06, 'batch_size': 85, 'step_size': 5, 'gamma': 0.771267787325379}. Best is trial 0 with value: 0.11161546066720435.[0m
[32m[I 2025-01-10 10:23:00,347][0m Trial 4 finished with value: 0.0730889055916329 and parameters: {'observation_period_num': 28, 'train_rates': 0.8674942533481456, 'learning_rate': 8.67542597612673e-06, 'batch_size': 220, 'step_size': 10, 'gamma': 0.9786595906066795}. Best is trial 4 with value: 0.0730889055916329.[0m
[32m[I 2025-01-10 10:26:00,132][0m Trial 5 finished with value: 0.3856525854414352 and parameters: {'observation_period_num': 220, 'train_rates': 0.794104053239771, 'learning_rate': 1.4353093397953887e-06, 'batch_size': 43, 'step_size': 8, 'gamma': 0.8061603205178406}. Best is trial 4 with value: 0.0730889055916329.[0m
[32m[I 2025-01-10 10:27:55,752][0m Trial 6 finished with value: 0.39803075029661783 and parameters: {'observation_period_num': 192, 'train_rates': 0.8624227086892887, 'learning_rate': 1.2770301841195869e-06, 'batch_size': 71, 'step_size': 15, 'gamma': 0.8508001122516377}. Best is trial 4 with value: 0.0730889055916329.[0m
[32m[I 2025-01-10 10:29:29,708][0m Trial 7 finished with value: 0.16777584878828902 and parameters: {'observation_period_num': 240, 'train_rates': 0.6943318610510454, 'learning_rate': 6.465605968208523e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.880250245660532}. Best is trial 4 with value: 0.0730889055916329.[0m
[32m[I 2025-01-10 10:34:11,670][0m Trial 8 finished with value: 0.09818815849172975 and parameters: {'observation_period_num': 119, 'train_rates': 0.6882543952956622, 'learning_rate': 0.00014298141925187216, 'batch_size': 24, 'step_size': 5, 'gamma': 0.7622131835351568}. Best is trial 4 with value: 0.0730889055916329.[0m
[32m[I 2025-01-10 10:35:05,628][0m Trial 9 finished with value: 0.927573496713413 and parameters: {'observation_period_num': 180, 'train_rates': 0.7712004711391396, 'learning_rate': 1.1402937079962393e-06, 'batch_size': 168, 'step_size': 2, 'gamma': 0.8571344096536261}. Best is trial 4 with value: 0.0730889055916329.[0m
[32m[I 2025-01-10 10:36:09,969][0m Trial 10 finished with value: 0.054342497140169144 and parameters: {'observation_period_num': 7, 'train_rates': 0.9898894394979236, 'learning_rate': 9.552085172917245e-06, 'batch_size': 248, 'step_size': 12, 'gamma': 0.9801832638646054}. Best is trial 10 with value: 0.054342497140169144.[0m
[32m[I 2025-01-10 10:37:13,932][0m Trial 11 finished with value: 0.11658354103565216 and parameters: {'observation_period_num': 7, 'train_rates': 0.9574471017311569, 'learning_rate': 7.876225815045154e-06, 'batch_size': 250, 'step_size': 12, 'gamma': 0.980574757197317}. Best is trial 10 with value: 0.054342497140169144.[0m
[32m[I 2025-01-10 10:38:18,674][0m Trial 12 finished with value: 0.05246047303080559 and parameters: {'observation_period_num': 5, 'train_rates': 0.9891300819386958, 'learning_rate': 1.058237383231404e-05, 'batch_size': 248, 'step_size': 11, 'gamma': 0.9344288089266202}. Best is trial 12 with value: 0.05246047303080559.[0m
[32m[I 2025-01-10 10:39:22,085][0m Trial 13 finished with value: 0.15039345622062683 and parameters: {'observation_period_num': 55, 'train_rates': 0.9803921494143325, 'learning_rate': 0.0005433389119548733, 'batch_size': 243, 'step_size': 14, 'gamma': 0.9262719377317868}. Best is trial 12 with value: 0.05246047303080559.[0m
[32m[I 2025-01-10 10:40:22,407][0m Trial 14 finished with value: 0.2313896268606186 and parameters: {'observation_period_num': 62, 'train_rates': 0.9339269656069731, 'learning_rate': 5.295708574393585e-06, 'batch_size': 208, 'step_size': 13, 'gamma': 0.9290730300853844}. Best is trial 12 with value: 0.05246047303080559.[0m
[32m[I 2025-01-10 10:41:10,501][0m Trial 15 finished with value: 0.09531014409922335 and parameters: {'observation_period_num': 6, 'train_rates': 0.6278743689120011, 'learning_rate': 1.7186195856288498e-05, 'batch_size': 212, 'step_size': 9, 'gamma': 0.9330231813514906}. Best is trial 12 with value: 0.05246047303080559.[0m
[32m[I 2025-01-10 10:42:33,906][0m Trial 16 finished with value: 0.11941910837439523 and parameters: {'observation_period_num': 63, 'train_rates': 0.9284655515098276, 'learning_rate': 4.250090910423274e-06, 'batch_size': 111, 'step_size': 12, 'gamma': 0.9555150709506272}. Best is trial 12 with value: 0.05246047303080559.[0m
[32m[I 2025-01-10 10:43:49,518][0m Trial 17 finished with value: 0.03416730463504791 and parameters: {'observation_period_num': 37, 'train_rates': 0.9849528315453956, 'learning_rate': 0.00011389932649866666, 'batch_size': 126, 'step_size': 10, 'gamma': 0.8921052946591268}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:45:06,347][0m Trial 18 finished with value: 0.044853365464535146 and parameters: {'observation_period_num': 44, 'train_rates': 0.9232577098239403, 'learning_rate': 0.0004996020776509785, 'batch_size': 127, 'step_size': 7, 'gamma': 0.8917558371104094}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:46:23,346][0m Trial 19 finished with value: 0.06760971000991188 and parameters: {'observation_period_num': 86, 'train_rates': 0.9276588342914198, 'learning_rate': 0.0006836604551553084, 'batch_size': 125, 'step_size': 7, 'gamma': 0.888779039101407}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:47:46,618][0m Trial 20 finished with value: 0.045573322888230905 and parameters: {'observation_period_num': 42, 'train_rates': 0.912044990000038, 'learning_rate': 0.0002732211031829428, 'batch_size': 113, 'step_size': 3, 'gamma': 0.8201348807286747}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:49:10,392][0m Trial 21 finished with value: 0.047783751622340935 and parameters: {'observation_period_num': 46, 'train_rates': 0.9075066330034625, 'learning_rate': 0.0002696548052559182, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8150161276951754}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:50:15,215][0m Trial 22 finished with value: 0.0386567322054395 and parameters: {'observation_period_num': 34, 'train_rates': 0.8335223774615642, 'learning_rate': 0.00015255449722292817, 'batch_size': 138, 'step_size': 6, 'gamma': 0.8932099666703067}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:51:19,905][0m Trial 23 finished with value: 0.05294155534471574 and parameters: {'observation_period_num': 81, 'train_rates': 0.8304751432462181, 'learning_rate': 0.00010315576822888656, 'batch_size': 143, 'step_size': 7, 'gamma': 0.897599103811246}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:52:26,062][0m Trial 24 finished with value: 0.0392556526900305 and parameters: {'observation_period_num': 32, 'train_rates': 0.8313911484794415, 'learning_rate': 0.00025857503399846787, 'batch_size': 142, 'step_size': 9, 'gamma': 0.9099449971165259}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:53:25,571][0m Trial 25 finished with value: 0.04656274733133614 and parameters: {'observation_period_num': 32, 'train_rates': 0.7150244800645296, 'learning_rate': 0.00018267855399906567, 'batch_size': 148, 'step_size': 9, 'gamma': 0.9089749667903227}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:54:56,256][0m Trial 26 finished with value: 0.053365933349226964 and parameters: {'observation_period_num': 74, 'train_rates': 0.824799307553322, 'learning_rate': 7.786013247015747e-05, 'batch_size': 92, 'step_size': 9, 'gamma': 0.8674810211553506}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:55:49,777][0m Trial 27 finished with value: 0.123062225050946 and parameters: {'observation_period_num': 140, 'train_rates': 0.7428682974230614, 'learning_rate': 0.0009717784335115194, 'batch_size': 180, 'step_size': 6, 'gamma': 0.9048084879128331}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:56:55,920][0m Trial 28 finished with value: 0.04121505685596385 and parameters: {'observation_period_num': 35, 'train_rates': 0.7994543067207599, 'learning_rate': 3.289834227626262e-05, 'batch_size': 131, 'step_size': 10, 'gamma': 0.9536518114756014}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:57:48,807][0m Trial 29 finished with value: 0.2387765242568915 and parameters: {'observation_period_num': 106, 'train_rates': 0.6077193073961572, 'learning_rate': 4.7809144090329684e-05, 'batch_size': 152, 'step_size': 4, 'gamma': 0.8337600198787536}. Best is trial 17 with value: 0.03416730463504791.[0m
[32m[I 2025-01-10 10:58:48,308][0m Trial 30 finished with value: 0.032297936567120406 and parameters: {'observation_period_num': 28, 'train_rates': 0.8521842566948757, 'learning_rate': 0.0003074427440322099, 'batch_size': 198, 'step_size': 8, 'gamma': 0.8736488262234063}. Best is trial 30 with value: 0.032297936567120406.[0m
[32m[I 2025-01-10 10:59:46,782][0m Trial 31 finished with value: 0.03083999642845372 and parameters: {'observation_period_num': 23, 'train_rates': 0.8416757955846104, 'learning_rate': 0.00027266558565206483, 'batch_size': 190, 'step_size': 8, 'gamma': 0.8686063079924085}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:00:42,755][0m Trial 32 finished with value: 0.032080918617558754 and parameters: {'observation_period_num': 21, 'train_rates': 0.7915473559111654, 'learning_rate': 0.0003709481263419348, 'batch_size': 196, 'step_size': 6, 'gamma': 0.8701280055604675}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:01:41,063][0m Trial 33 finished with value: 0.037997031075279374 and parameters: {'observation_period_num': 20, 'train_rates': 0.7917836585825929, 'learning_rate': 0.0003986222577526206, 'batch_size': 190, 'step_size': 8, 'gamma': 0.8677268433964667}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:02:34,265][0m Trial 34 finished with value: 0.0353394084840372 and parameters: {'observation_period_num': 21, 'train_rates': 0.7463789128298612, 'learning_rate': 0.0009829583595617514, 'batch_size': 200, 'step_size': 8, 'gamma': 0.8386550700486581}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:03:35,435][0m Trial 35 finished with value: 0.046271329942327105 and parameters: {'observation_period_num': 66, 'train_rates': 0.8885572166342594, 'learning_rate': 0.00034231879784149126, 'batch_size': 167, 'step_size': 6, 'gamma': 0.8740111739710285}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:04:33,303][0m Trial 36 finished with value: 0.047719823793646254 and parameters: {'observation_period_num': 52, 'train_rates': 0.8559339634425867, 'learning_rate': 0.00012485789535028203, 'batch_size': 227, 'step_size': 10, 'gamma': 0.7956441917606976}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:05:28,062][0m Trial 37 finished with value: 0.05271230648233466 and parameters: {'observation_period_num': 20, 'train_rates': 0.7759025086090269, 'learning_rate': 0.00021200966345154138, 'batch_size': 232, 'step_size': 4, 'gamma': 0.8551459698347595}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:06:26,924][0m Trial 38 finished with value: 0.13717033586523555 and parameters: {'observation_period_num': 94, 'train_rates': 0.883709967842323, 'learning_rate': 9.230493060517806e-05, 'batch_size': 195, 'step_size': 11, 'gamma': 0.880328127000561}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:07:26,307][0m Trial 39 finished with value: 0.11517899308452884 and parameters: {'observation_period_num': 122, 'train_rates': 0.854161957743245, 'learning_rate': 5.606194630906685e-05, 'batch_size': 165, 'step_size': 10, 'gamma': 0.8407755188338515}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:08:22,795][0m Trial 40 finished with value: 0.11401805116376683 and parameters: {'observation_period_num': 165, 'train_rates': 0.8059588099985122, 'learning_rate': 0.0006810646534949938, 'batch_size': 184, 'step_size': 5, 'gamma': 0.8618962790640873}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:09:19,005][0m Trial 41 finished with value: 0.03542658633997907 and parameters: {'observation_period_num': 19, 'train_rates': 0.7489874820459544, 'learning_rate': 0.000857110282587768, 'batch_size': 198, 'step_size': 8, 'gamma': 0.8435236345305799}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:10:10,448][0m Trial 42 finished with value: 0.044014384379697374 and parameters: {'observation_period_num': 20, 'train_rates': 0.7223260880955814, 'learning_rate': 0.0004208504571942302, 'batch_size': 212, 'step_size': 8, 'gamma': 0.8310953273821405}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:11:01,535][0m Trial 43 finished with value: 0.0389411446520273 and parameters: {'observation_period_num': 20, 'train_rates': 0.653086017527747, 'learning_rate': 0.0006286866797055686, 'batch_size': 177, 'step_size': 6, 'gamma': 0.8802809493574368}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:11:57,976][0m Trial 44 finished with value: 0.05817656547448339 and parameters: {'observation_period_num': 51, 'train_rates': 0.767101195125456, 'learning_rate': 0.00035233096876834596, 'batch_size': 202, 'step_size': 7, 'gamma': 0.7919960826329168}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:12:49,258][0m Trial 45 finished with value: 0.19094421882387047 and parameters: {'observation_period_num': 219, 'train_rates': 0.7336566411788396, 'learning_rate': 0.00020537460134685895, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8519034814445653}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:13:44,145][0m Trial 46 finished with value: 0.07648739797058027 and parameters: {'observation_period_num': 27, 'train_rates': 0.6906203009947911, 'learning_rate': 0.0008360834118044374, 'batch_size': 159, 'step_size': 11, 'gamma': 0.9180503612749183}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:14:51,304][0m Trial 47 finished with value: 0.04175932705402374 and parameters: {'observation_period_num': 12, 'train_rates': 0.9592394388442333, 'learning_rate': 0.0004886543715247837, 'batch_size': 187, 'step_size': 8, 'gamma': 0.8243664159392263}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:15:45,796][0m Trial 48 finished with value: 0.061672293953899435 and parameters: {'observation_period_num': 74, 'train_rates': 0.7856919157267221, 'learning_rate': 0.00013416000017900593, 'batch_size': 236, 'step_size': 8, 'gamma': 0.8738110457767023}. Best is trial 31 with value: 0.03083999642845372.[0m
[32m[I 2025-01-10 11:16:44,889][0m Trial 49 finished with value: 0.08027910813689232 and parameters: {'observation_period_num': 40, 'train_rates': 0.8102714475358134, 'learning_rate': 2.0864338806394224e-05, 'batch_size': 175, 'step_size': 6, 'gamma': 0.8453403225053484}. Best is trial 31 with value: 0.03083999642845372.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.754098660383412, 'learning_rate': 0.00027065242699770236, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9402084486286051}
Epoch 1/300, trend Loss: 0.3575 | 0.3939
Epoch 2/300, trend Loss: 0.1644 | 0.1918
Epoch 3/300, trend Loss: 0.1334 | 0.1076
Epoch 4/300, trend Loss: 0.1220 | 0.0850
Epoch 5/300, trend Loss: 0.1161 | 0.0762
Epoch 6/300, trend Loss: 0.1121 | 0.0711
Epoch 7/300, trend Loss: 0.1097 | 0.0675
Epoch 8/300, trend Loss: 0.1083 | 0.0651
Epoch 9/300, trend Loss: 0.1071 | 0.0642
Epoch 10/300, trend Loss: 0.1059 | 0.0643
Epoch 11/300, trend Loss: 0.1046 | 0.0657
Epoch 12/300, trend Loss: 0.1030 | 0.0692
Epoch 13/300, trend Loss: 0.1011 | 0.0726
Epoch 14/300, trend Loss: 0.0993 | 0.0787
Epoch 15/300, trend Loss: 0.0976 | 0.0846
Epoch 16/300, trend Loss: 0.0964 | 0.0895
Epoch 17/300, trend Loss: 0.0958 | 0.0910
Epoch 18/300, trend Loss: 0.0957 | 0.0881
Epoch 19/300, trend Loss: 0.0956 | 0.0817
Epoch 20/300, trend Loss: 0.0950 | 0.0739
Epoch 21/300, trend Loss: 0.0941 | 0.0670
Epoch 22/300, trend Loss: 0.0929 | 0.0616
Epoch 23/300, trend Loss: 0.0917 | 0.0571
Epoch 24/300, trend Loss: 0.0903 | 0.0535
Epoch 25/300, trend Loss: 0.0889 | 0.0509
Epoch 26/300, trend Loss: 0.0876 | 0.0487
Epoch 27/300, trend Loss: 0.0862 | 0.0470
Epoch 28/300, trend Loss: 0.0850 | 0.0458
Epoch 29/300, trend Loss: 0.0840 | 0.0450
Epoch 30/300, trend Loss: 0.0832 | 0.0443
Epoch 31/300, trend Loss: 0.0825 | 0.0436
Epoch 32/300, trend Loss: 0.0819 | 0.0431
Epoch 33/300, trend Loss: 0.0814 | 0.0426
Epoch 34/300, trend Loss: 0.0808 | 0.0421
Epoch 35/300, trend Loss: 0.0802 | 0.0416
Epoch 36/300, trend Loss: 0.0797 | 0.0411
Epoch 37/300, trend Loss: 0.0792 | 0.0406
Epoch 38/300, trend Loss: 0.0787 | 0.0399
Epoch 39/300, trend Loss: 0.0782 | 0.0393
Epoch 40/300, trend Loss: 0.0777 | 0.0387
Epoch 41/300, trend Loss: 0.0773 | 0.0380
Epoch 42/300, trend Loss: 0.0769 | 0.0373
Epoch 43/300, trend Loss: 0.0766 | 0.0368
Epoch 44/300, trend Loss: 0.0763 | 0.0362
Epoch 45/300, trend Loss: 0.0760 | 0.0358
Epoch 46/300, trend Loss: 0.0759 | 0.0353
Epoch 47/300, trend Loss: 0.0757 | 0.0350
Epoch 48/300, trend Loss: 0.0755 | 0.0350
Epoch 49/300, trend Loss: 0.0750 | 0.0351
Epoch 50/300, trend Loss: 0.0747 | 0.0348
Epoch 51/300, trend Loss: 0.0747 | 0.0345
Epoch 52/300, trend Loss: 0.0748 | 0.0345
Epoch 53/300, trend Loss: 0.0749 | 0.0342
Epoch 54/300, trend Loss: 0.0747 | 0.0335
Epoch 55/300, trend Loss: 0.0740 | 0.0331
Epoch 56/300, trend Loss: 0.0737 | 0.0331
Epoch 57/300, trend Loss: 0.0736 | 0.0331
Epoch 58/300, trend Loss: 0.0734 | 0.0329
Epoch 59/300, trend Loss: 0.0733 | 0.0330
Epoch 60/300, trend Loss: 0.0731 | 0.0331
Epoch 61/300, trend Loss: 0.0730 | 0.0329
Epoch 62/300, trend Loss: 0.0729 | 0.0330
Epoch 63/300, trend Loss: 0.0728 | 0.0331
Epoch 64/300, trend Loss: 0.0726 | 0.0329
Epoch 65/300, trend Loss: 0.0725 | 0.0329
Epoch 66/300, trend Loss: 0.0724 | 0.0328
Epoch 67/300, trend Loss: 0.0723 | 0.0326
Epoch 68/300, trend Loss: 0.0722 | 0.0325
Epoch 69/300, trend Loss: 0.0721 | 0.0324
Epoch 70/300, trend Loss: 0.0720 | 0.0321
Epoch 71/300, trend Loss: 0.0719 | 0.0321
Epoch 72/300, trend Loss: 0.0719 | 0.0320
Epoch 73/300, trend Loss: 0.0718 | 0.0318
Epoch 74/300, trend Loss: 0.0718 | 0.0318
Epoch 75/300, trend Loss: 0.0717 | 0.0318
Epoch 76/300, trend Loss: 0.0717 | 0.0317
Epoch 77/300, trend Loss: 0.0717 | 0.0315
Epoch 78/300, trend Loss: 0.0717 | 0.0314
Epoch 79/300, trend Loss: 0.0716 | 0.0314
Epoch 80/300, trend Loss: 0.0716 | 0.0313
Epoch 81/300, trend Loss: 0.0716 | 0.0313
Epoch 82/300, trend Loss: 0.0717 | 0.0315
Epoch 83/300, trend Loss: 0.0718 | 0.0316
Epoch 84/300, trend Loss: 0.0720 | 0.0320
Epoch 85/300, trend Loss: 0.0720 | 0.0318
Epoch 86/300, trend Loss: 0.0718 | 0.0313
Epoch 87/300, trend Loss: 0.0714 | 0.0311
Epoch 88/300, trend Loss: 0.0712 | 0.0310
Epoch 89/300, trend Loss: 0.0710 | 0.0308
Epoch 90/300, trend Loss: 0.0709 | 0.0308
Epoch 91/300, trend Loss: 0.0707 | 0.0306
Epoch 92/300, trend Loss: 0.0706 | 0.0306
Epoch 93/300, trend Loss: 0.0705 | 0.0305
Epoch 94/300, trend Loss: 0.0704 | 0.0304
Epoch 95/300, trend Loss: 0.0703 | 0.0303
Epoch 96/300, trend Loss: 0.0702 | 0.0303
Epoch 97/300, trend Loss: 0.0701 | 0.0302
Epoch 98/300, trend Loss: 0.0700 | 0.0301
Epoch 99/300, trend Loss: 0.0699 | 0.0301
Epoch 100/300, trend Loss: 0.0698 | 0.0300
Epoch 101/300, trend Loss: 0.0697 | 0.0300
Epoch 102/300, trend Loss: 0.0697 | 0.0299
Epoch 103/300, trend Loss: 0.0696 | 0.0299
Epoch 104/300, trend Loss: 0.0695 | 0.0298
Epoch 105/300, trend Loss: 0.0694 | 0.0298
Epoch 106/300, trend Loss: 0.0694 | 0.0297
Epoch 107/300, trend Loss: 0.0693 | 0.0297
Epoch 108/300, trend Loss: 0.0692 | 0.0297
Epoch 109/300, trend Loss: 0.0692 | 0.0296
Epoch 110/300, trend Loss: 0.0691 | 0.0296
Epoch 111/300, trend Loss: 0.0690 | 0.0295
Epoch 112/300, trend Loss: 0.0690 | 0.0295
Epoch 113/300, trend Loss: 0.0689 | 0.0295
Epoch 114/300, trend Loss: 0.0689 | 0.0294
Epoch 115/300, trend Loss: 0.0688 | 0.0294
Epoch 116/300, trend Loss: 0.0688 | 0.0294
Epoch 117/300, trend Loss: 0.0687 | 0.0293
Epoch 118/300, trend Loss: 0.0686 | 0.0293
Epoch 119/300, trend Loss: 0.0686 | 0.0293
Epoch 120/300, trend Loss: 0.0686 | 0.0292
Epoch 121/300, trend Loss: 0.0685 | 0.0292
Epoch 122/300, trend Loss: 0.0685 | 0.0292
Epoch 123/300, trend Loss: 0.0684 | 0.0292
Epoch 124/300, trend Loss: 0.0684 | 0.0291
Epoch 125/300, trend Loss: 0.0683 | 0.0291
Epoch 126/300, trend Loss: 0.0683 | 0.0291
Epoch 127/300, trend Loss: 0.0682 | 0.0291
Epoch 128/300, trend Loss: 0.0682 | 0.0291
Epoch 129/300, trend Loss: 0.0682 | 0.0290
Epoch 130/300, trend Loss: 0.0681 | 0.0290
Epoch 131/300, trend Loss: 0.0681 | 0.0290
Epoch 132/300, trend Loss: 0.0680 | 0.0290
Epoch 133/300, trend Loss: 0.0680 | 0.0290
Epoch 134/300, trend Loss: 0.0680 | 0.0290
Epoch 135/300, trend Loss: 0.0679 | 0.0290
Epoch 136/300, trend Loss: 0.0679 | 0.0290
Epoch 137/300, trend Loss: 0.0679 | 0.0289
Epoch 138/300, trend Loss: 0.0678 | 0.0289
Epoch 139/300, trend Loss: 0.0678 | 0.0289
Epoch 140/300, trend Loss: 0.0677 | 0.0289
Epoch 141/300, trend Loss: 0.0677 | 0.0289
Epoch 142/300, trend Loss: 0.0677 | 0.0289
Epoch 143/300, trend Loss: 0.0676 | 0.0289
Epoch 144/300, trend Loss: 0.0676 | 0.0289
Epoch 145/300, trend Loss: 0.0676 | 0.0289
Epoch 146/300, trend Loss: 0.0676 | 0.0289
Epoch 147/300, trend Loss: 0.0675 | 0.0288
Epoch 148/300, trend Loss: 0.0675 | 0.0288
Epoch 149/300, trend Loss: 0.0675 | 0.0288
Epoch 150/300, trend Loss: 0.0675 | 0.0288
Epoch 151/300, trend Loss: 0.0674 | 0.0288
Epoch 152/300, trend Loss: 0.0674 | 0.0288
Epoch 153/300, trend Loss: 0.0674 | 0.0288
Epoch 154/300, trend Loss: 0.0674 | 0.0288
Epoch 155/300, trend Loss: 0.0674 | 0.0288
Epoch 156/300, trend Loss: 0.0673 | 0.0288
Epoch 157/300, trend Loss: 0.0673 | 0.0288
Epoch 158/300, trend Loss: 0.0673 | 0.0288
Epoch 159/300, trend Loss: 0.0673 | 0.0288
Epoch 160/300, trend Loss: 0.0673 | 0.0288
Epoch 161/300, trend Loss: 0.0672 | 0.0288
Epoch 162/300, trend Loss: 0.0672 | 0.0288
Epoch 163/300, trend Loss: 0.0672 | 0.0288
Epoch 164/300, trend Loss: 0.0672 | 0.0288
Epoch 165/300, trend Loss: 0.0672 | 0.0288
Epoch 166/300, trend Loss: 0.0672 | 0.0288
Epoch 167/300, trend Loss: 0.0672 | 0.0288
Epoch 168/300, trend Loss: 0.0672 | 0.0288
Epoch 169/300, trend Loss: 0.0672 | 0.0288
Epoch 170/300, trend Loss: 0.0671 | 0.0288
Epoch 171/300, trend Loss: 0.0671 | 0.0288
Epoch 172/300, trend Loss: 0.0671 | 0.0287
Epoch 173/300, trend Loss: 0.0671 | 0.0287
Epoch 174/300, trend Loss: 0.0671 | 0.0287
Epoch 175/300, trend Loss: 0.0671 | 0.0287
Epoch 176/300, trend Loss: 0.0671 | 0.0287
Epoch 177/300, trend Loss: 0.0671 | 0.0287
Epoch 178/300, trend Loss: 0.0671 | 0.0287
Epoch 179/300, trend Loss: 0.0671 | 0.0287
Epoch 180/300, trend Loss: 0.0671 | 0.0287
Epoch 181/300, trend Loss: 0.0671 | 0.0287
Epoch 182/300, trend Loss: 0.0671 | 0.0287
Epoch 183/300, trend Loss: 0.0671 | 0.0287
Epoch 184/300, trend Loss: 0.0671 | 0.0287
Epoch 185/300, trend Loss: 0.0671 | 0.0287
Epoch 186/300, trend Loss: 0.0670 | 0.0287
Epoch 187/300, trend Loss: 0.0670 | 0.0287
Epoch 188/300, trend Loss: 0.0670 | 0.0287
Epoch 189/300, trend Loss: 0.0670 | 0.0286
Epoch 190/300, trend Loss: 0.0670 | 0.0286
Epoch 191/300, trend Loss: 0.0669 | 0.0286
Epoch 192/300, trend Loss: 0.0669 | 0.0286
Epoch 193/300, trend Loss: 0.0669 | 0.0286
Epoch 194/300, trend Loss: 0.0669 | 0.0286
Epoch 195/300, trend Loss: 0.0669 | 0.0286
Epoch 196/300, trend Loss: 0.0668 | 0.0286
Epoch 197/300, trend Loss: 0.0668 | 0.0286
Epoch 198/300, trend Loss: 0.0668 | 0.0286
Epoch 199/300, trend Loss: 0.0668 | 0.0286
Epoch 200/300, trend Loss: 0.0668 | 0.0286
Epoch 201/300, trend Loss: 0.0668 | 0.0286
Epoch 202/300, trend Loss: 0.0668 | 0.0286
Epoch 203/300, trend Loss: 0.0668 | 0.0286
Epoch 204/300, trend Loss: 0.0668 | 0.0286
Epoch 205/300, trend Loss: 0.0667 | 0.0286
Epoch 206/300, trend Loss: 0.0667 | 0.0286
Epoch 207/300, trend Loss: 0.0667 | 0.0286
Epoch 208/300, trend Loss: 0.0667 | 0.0286
Epoch 209/300, trend Loss: 0.0667 | 0.0285
Epoch 210/300, trend Loss: 0.0667 | 0.0285
Epoch 211/300, trend Loss: 0.0667 | 0.0285
Epoch 212/300, trend Loss: 0.0667 | 0.0285
Epoch 213/300, trend Loss: 0.0667 | 0.0285
Epoch 214/300, trend Loss: 0.0667 | 0.0285
Epoch 215/300, trend Loss: 0.0667 | 0.0285
Epoch 216/300, trend Loss: 0.0667 | 0.0285
Epoch 217/300, trend Loss: 0.0667 | 0.0285
Epoch 218/300, trend Loss: 0.0666 | 0.0285
Epoch 219/300, trend Loss: 0.0666 | 0.0285
Epoch 220/300, trend Loss: 0.0666 | 0.0285
Epoch 221/300, trend Loss: 0.0666 | 0.0285
Epoch 222/300, trend Loss: 0.0666 | 0.0285
Epoch 223/300, trend Loss: 0.0666 | 0.0285
Epoch 224/300, trend Loss: 0.0666 | 0.0285
Epoch 225/300, trend Loss: 0.0666 | 0.0285
Epoch 226/300, trend Loss: 0.0666 | 0.0285
Epoch 227/300, trend Loss: 0.0666 | 0.0285
Epoch 228/300, trend Loss: 0.0666 | 0.0285
Epoch 229/300, trend Loss: 0.0666 | 0.0285
Epoch 230/300, trend Loss: 0.0666 | 0.0285
Epoch 231/300, trend Loss: 0.0666 | 0.0285
Epoch 232/300, trend Loss: 0.0666 | 0.0285
Epoch 233/300, trend Loss: 0.0666 | 0.0285
Epoch 234/300, trend Loss: 0.0666 | 0.0285
Epoch 235/300, trend Loss: 0.0666 | 0.0285
Epoch 236/300, trend Loss: 0.0665 | 0.0285
Epoch 237/300, trend Loss: 0.0665 | 0.0285
Epoch 238/300, trend Loss: 0.0665 | 0.0285
Epoch 239/300, trend Loss: 0.0665 | 0.0286
Epoch 240/300, trend Loss: 0.0665 | 0.0286
Epoch 241/300, trend Loss: 0.0665 | 0.0286
Epoch 242/300, trend Loss: 0.0665 | 0.0286
Epoch 243/300, trend Loss: 0.0665 | 0.0286
Epoch 244/300, trend Loss: 0.0665 | 0.0286
Epoch 245/300, trend Loss: 0.0665 | 0.0286
Epoch 246/300, trend Loss: 0.0665 | 0.0286
Epoch 247/300, trend Loss: 0.0665 | 0.0286
Epoch 248/300, trend Loss: 0.0665 | 0.0286
Epoch 249/300, trend Loss: 0.0665 | 0.0286
Epoch 250/300, trend Loss: 0.0665 | 0.0286
Epoch 251/300, trend Loss: 0.0665 | 0.0285
Epoch 252/300, trend Loss: 0.0665 | 0.0285
Epoch 253/300, trend Loss: 0.0665 | 0.0285
Epoch 254/300, trend Loss: 0.0665 | 0.0285
Epoch 255/300, trend Loss: 0.0665 | 0.0285
Epoch 256/300, trend Loss: 0.0665 | 0.0285
Epoch 257/300, trend Loss: 0.0665 | 0.0285
Epoch 258/300, trend Loss: 0.0665 | 0.0285
Epoch 259/300, trend Loss: 0.0665 | 0.0285
Epoch 260/300, trend Loss: 0.0665 | 0.0285
Epoch 261/300, trend Loss: 0.0665 | 0.0285
Epoch 262/300, trend Loss: 0.0665 | 0.0285
Epoch 263/300, trend Loss: 0.0665 | 0.0285
Epoch 264/300, trend Loss: 0.0665 | 0.0285
Epoch 265/300, trend Loss: 0.0665 | 0.0285
Epoch 266/300, trend Loss: 0.0665 | 0.0285
Epoch 267/300, trend Loss: 0.0664 | 0.0285
Epoch 268/300, trend Loss: 0.0664 | 0.0285
Epoch 269/300, trend Loss: 0.0664 | 0.0285
Epoch 270/300, trend Loss: 0.0664 | 0.0285
Epoch 271/300, trend Loss: 0.0664 | 0.0285
Epoch 272/300, trend Loss: 0.0664 | 0.0285
Epoch 273/300, trend Loss: 0.0664 | 0.0285
Epoch 274/300, trend Loss: 0.0664 | 0.0285
Epoch 275/300, trend Loss: 0.0664 | 0.0285
Epoch 276/300, trend Loss: 0.0664 | 0.0285
Epoch 277/300, trend Loss: 0.0664 | 0.0285
Epoch 278/300, trend Loss: 0.0664 | 0.0285
Epoch 279/300, trend Loss: 0.0664 | 0.0285
Epoch 280/300, trend Loss: 0.0664 | 0.0285
Epoch 281/300, trend Loss: 0.0664 | 0.0285
Epoch 282/300, trend Loss: 0.0664 | 0.0285
Epoch 283/300, trend Loss: 0.0664 | 0.0285
Epoch 284/300, trend Loss: 0.0664 | 0.0285
Epoch 285/300, trend Loss: 0.0664 | 0.0285
Epoch 286/300, trend Loss: 0.0664 | 0.0285
Epoch 287/300, trend Loss: 0.0664 | 0.0285
Epoch 288/300, trend Loss: 0.0664 | 0.0285
Epoch 289/300, trend Loss: 0.0664 | 0.0285
Epoch 290/300, trend Loss: 0.0664 | 0.0285
Epoch 291/300, trend Loss: 0.0664 | 0.0285
Epoch 292/300, trend Loss: 0.0664 | 0.0285
Epoch 293/300, trend Loss: 0.0664 | 0.0285
Epoch 294/300, trend Loss: 0.0664 | 0.0285
Epoch 295/300, trend Loss: 0.0664 | 0.0285
Epoch 296/300, trend Loss: 0.0664 | 0.0285
Epoch 297/300, trend Loss: 0.0664 | 0.0285
Epoch 298/300, trend Loss: 0.0664 | 0.0285
Epoch 299/300, trend Loss: 0.0664 | 0.0285
Epoch 300/300, trend Loss: 0.0664 | 0.0285
Training seasonal_0 component with params: {'observation_period_num': 6, 'train_rates': 0.8054931885145741, 'learning_rate': 0.00041093637665892745, 'batch_size': 105, 'step_size': 11, 'gamma': 0.7609401212442125}
Epoch 1/300, seasonal_0 Loss: 1.5992 | 0.5124
Epoch 2/300, seasonal_0 Loss: 0.2169 | 0.1310
Epoch 3/300, seasonal_0 Loss: 0.1762 | 0.1319
Epoch 4/300, seasonal_0 Loss: 0.1618 | 0.1071
Epoch 5/300, seasonal_0 Loss: 0.1357 | 0.0793
Epoch 6/300, seasonal_0 Loss: 0.1170 | 0.0832
Epoch 7/300, seasonal_0 Loss: 0.1348 | 0.0824
Epoch 8/300, seasonal_0 Loss: 0.1224 | 0.0646
Epoch 9/300, seasonal_0 Loss: 0.1051 | 0.0577
Epoch 10/300, seasonal_0 Loss: 0.1051 | 0.0625
Epoch 11/300, seasonal_0 Loss: 0.1111 | 0.0645
Epoch 12/300, seasonal_0 Loss: 0.1032 | 0.0532
Epoch 13/300, seasonal_0 Loss: 0.0975 | 0.0548
Epoch 14/300, seasonal_0 Loss: 0.0945 | 0.0500
Epoch 15/300, seasonal_0 Loss: 0.0917 | 0.0483
Epoch 16/300, seasonal_0 Loss: 0.0894 | 0.0474
Epoch 17/300, seasonal_0 Loss: 0.0866 | 0.0482
Epoch 18/300, seasonal_0 Loss: 0.0880 | 0.0689
Epoch 19/300, seasonal_0 Loss: 0.0904 | 0.0644
Epoch 20/300, seasonal_0 Loss: 0.1028 | 0.0791
Epoch 21/300, seasonal_0 Loss: 0.1219 | 0.0765
Epoch 22/300, seasonal_0 Loss: 0.1112 | 0.0552
Epoch 23/300, seasonal_0 Loss: 0.1309 | 0.0724
Epoch 24/300, seasonal_0 Loss: 0.1281 | 0.0552
Epoch 25/300, seasonal_0 Loss: 0.1245 | 0.0594
Epoch 26/300, seasonal_0 Loss: 0.1020 | 0.0623
Epoch 27/300, seasonal_0 Loss: 0.1042 | 0.0695
Epoch 28/300, seasonal_0 Loss: 0.1058 | 0.0568
Epoch 29/300, seasonal_0 Loss: 0.1001 | 0.0596
Epoch 30/300, seasonal_0 Loss: 0.1124 | 0.0603
Epoch 31/300, seasonal_0 Loss: 0.1284 | 0.0745
Epoch 32/300, seasonal_0 Loss: 0.1494 | 0.0697
Epoch 33/300, seasonal_0 Loss: 0.1248 | 0.0797
Epoch 34/300, seasonal_0 Loss: 0.1046 | 0.0566
Epoch 35/300, seasonal_0 Loss: 0.0989 | 0.0893
Epoch 36/300, seasonal_0 Loss: 0.1018 | 0.0636
Epoch 37/300, seasonal_0 Loss: 0.1002 | 0.0781
Epoch 38/300, seasonal_0 Loss: 0.0980 | 0.0477
Epoch 39/300, seasonal_0 Loss: 0.0895 | 0.0627
Epoch 40/300, seasonal_0 Loss: 0.0997 | 0.0543
Epoch 41/300, seasonal_0 Loss: 0.0833 | 0.0424
Epoch 42/300, seasonal_0 Loss: 0.0855 | 0.0415
Epoch 43/300, seasonal_0 Loss: 0.0833 | 0.0456
Epoch 44/300, seasonal_0 Loss: 0.0820 | 0.0420
Epoch 45/300, seasonal_0 Loss: 0.0811 | 0.0497
Epoch 46/300, seasonal_0 Loss: 0.0812 | 0.0519
Epoch 47/300, seasonal_0 Loss: 0.0818 | 0.0469
Epoch 48/300, seasonal_0 Loss: 0.0822 | 0.0427
Epoch 49/300, seasonal_0 Loss: 0.0806 | 0.0445
Epoch 50/300, seasonal_0 Loss: 0.0813 | 0.0435
Epoch 51/300, seasonal_0 Loss: 0.0931 | 0.0489
Epoch 52/300, seasonal_0 Loss: 0.0932 | 0.0445
Epoch 53/300, seasonal_0 Loss: 0.0910 | 0.0480
Epoch 54/300, seasonal_0 Loss: 0.1099 | 0.0434
Epoch 55/300, seasonal_0 Loss: 0.1065 | 0.0513
Epoch 56/300, seasonal_0 Loss: 0.0953 | 0.0490
Epoch 57/300, seasonal_0 Loss: 0.0791 | 0.0479
Epoch 58/300, seasonal_0 Loss: 0.0777 | 0.0385
Epoch 59/300, seasonal_0 Loss: 0.0732 | 0.0406
Epoch 60/300, seasonal_0 Loss: 0.0715 | 0.0421
Epoch 61/300, seasonal_0 Loss: 0.0700 | 0.0409
Epoch 62/300, seasonal_0 Loss: 0.0692 | 0.0412
Epoch 63/300, seasonal_0 Loss: 0.0682 | 0.0382
Epoch 64/300, seasonal_0 Loss: 0.0680 | 0.0376
Epoch 65/300, seasonal_0 Loss: 0.0677 | 0.0373
Epoch 66/300, seasonal_0 Loss: 0.0676 | 0.0374
Epoch 67/300, seasonal_0 Loss: 0.0674 | 0.0373
Epoch 68/300, seasonal_0 Loss: 0.0672 | 0.0371
Epoch 69/300, seasonal_0 Loss: 0.0671 | 0.0369
Epoch 70/300, seasonal_0 Loss: 0.0670 | 0.0367
Epoch 71/300, seasonal_0 Loss: 0.0669 | 0.0365
Epoch 72/300, seasonal_0 Loss: 0.0668 | 0.0364
Epoch 73/300, seasonal_0 Loss: 0.0667 | 0.0366
Epoch 74/300, seasonal_0 Loss: 0.0666 | 0.0367
Epoch 75/300, seasonal_0 Loss: 0.0665 | 0.0368
Epoch 76/300, seasonal_0 Loss: 0.0664 | 0.0368
Epoch 77/300, seasonal_0 Loss: 0.0663 | 0.0368
Epoch 78/300, seasonal_0 Loss: 0.0662 | 0.0366
Epoch 79/300, seasonal_0 Loss: 0.0661 | 0.0363
Epoch 80/300, seasonal_0 Loss: 0.0661 | 0.0360
Epoch 81/300, seasonal_0 Loss: 0.0660 | 0.0359
Epoch 82/300, seasonal_0 Loss: 0.0659 | 0.0359
Epoch 83/300, seasonal_0 Loss: 0.0659 | 0.0360
Epoch 84/300, seasonal_0 Loss: 0.0658 | 0.0367
Epoch 85/300, seasonal_0 Loss: 0.0657 | 0.0370
Epoch 86/300, seasonal_0 Loss: 0.0657 | 0.0371
Epoch 87/300, seasonal_0 Loss: 0.0657 | 0.0369
Epoch 88/300, seasonal_0 Loss: 0.0656 | 0.0365
Epoch 89/300, seasonal_0 Loss: 0.0655 | 0.0358
Epoch 90/300, seasonal_0 Loss: 0.0655 | 0.0357
Epoch 91/300, seasonal_0 Loss: 0.0654 | 0.0358
Epoch 92/300, seasonal_0 Loss: 0.0654 | 0.0361
Epoch 93/300, seasonal_0 Loss: 0.0653 | 0.0364
Epoch 94/300, seasonal_0 Loss: 0.0653 | 0.0365
Epoch 95/300, seasonal_0 Loss: 0.0653 | 0.0364
Epoch 96/300, seasonal_0 Loss: 0.0652 | 0.0363
Epoch 97/300, seasonal_0 Loss: 0.0652 | 0.0363
Epoch 98/300, seasonal_0 Loss: 0.0652 | 0.0364
Epoch 99/300, seasonal_0 Loss: 0.0651 | 0.0364
Epoch 100/300, seasonal_0 Loss: 0.0651 | 0.0364
Epoch 101/300, seasonal_0 Loss: 0.0650 | 0.0364
Epoch 102/300, seasonal_0 Loss: 0.0650 | 0.0364
Epoch 103/300, seasonal_0 Loss: 0.0650 | 0.0364
Epoch 104/300, seasonal_0 Loss: 0.0650 | 0.0364
Epoch 105/300, seasonal_0 Loss: 0.0649 | 0.0364
Epoch 106/300, seasonal_0 Loss: 0.0649 | 0.0364
Epoch 107/300, seasonal_0 Loss: 0.0649 | 0.0364
Epoch 108/300, seasonal_0 Loss: 0.0649 | 0.0364
Epoch 109/300, seasonal_0 Loss: 0.0648 | 0.0364
Epoch 110/300, seasonal_0 Loss: 0.0648 | 0.0364
Epoch 111/300, seasonal_0 Loss: 0.0648 | 0.0364
Epoch 112/300, seasonal_0 Loss: 0.0648 | 0.0364
Epoch 113/300, seasonal_0 Loss: 0.0647 | 0.0364
Epoch 114/300, seasonal_0 Loss: 0.0647 | 0.0364
Epoch 115/300, seasonal_0 Loss: 0.0647 | 0.0365
Epoch 116/300, seasonal_0 Loss: 0.0647 | 0.0365
Epoch 117/300, seasonal_0 Loss: 0.0647 | 0.0364
Epoch 118/300, seasonal_0 Loss: 0.0646 | 0.0365
Epoch 119/300, seasonal_0 Loss: 0.0646 | 0.0365
Epoch 120/300, seasonal_0 Loss: 0.0646 | 0.0365
Epoch 121/300, seasonal_0 Loss: 0.0646 | 0.0365
Epoch 122/300, seasonal_0 Loss: 0.0646 | 0.0365
Epoch 123/300, seasonal_0 Loss: 0.0646 | 0.0365
Epoch 124/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 125/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 126/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 127/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 128/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 129/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 130/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 131/300, seasonal_0 Loss: 0.0645 | 0.0365
Epoch 132/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 133/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 134/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 135/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 136/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 137/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 138/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 139/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 140/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 141/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 142/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 143/300, seasonal_0 Loss: 0.0644 | 0.0365
Epoch 144/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 145/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 146/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 147/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 148/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 149/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 150/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 151/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 152/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 153/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 154/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 155/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 156/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 157/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 158/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 159/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 160/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 161/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 162/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 163/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 164/300, seasonal_0 Loss: 0.0643 | 0.0365
Epoch 165/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 166/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 167/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 168/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 169/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 170/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 171/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 172/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 173/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 174/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 175/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 176/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 177/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 178/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 179/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 180/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 181/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 182/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 183/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 184/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 185/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 186/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 187/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 188/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 189/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 190/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 191/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 192/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 193/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 194/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 195/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 196/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 197/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 198/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 199/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 200/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 201/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 202/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 203/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 204/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 205/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 206/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 207/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 208/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 209/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 210/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 211/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 212/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 213/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 214/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 215/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 216/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 217/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 218/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 219/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 220/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 221/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 222/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 223/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 224/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 225/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 226/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 227/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 228/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 229/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 230/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 231/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 232/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 233/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 234/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 235/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 236/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 237/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 238/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 239/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 240/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 241/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 242/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 243/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 244/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 245/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 246/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 247/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 248/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 249/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 250/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 251/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 252/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 253/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 254/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 255/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 256/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 257/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 258/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 259/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 260/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 261/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 262/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 263/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 264/300, seasonal_0 Loss: 0.0642 | 0.0365
Epoch 265/300, seasonal_0 Loss: 0.0642 | 0.0365
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 13, 'train_rates': 0.8394732978027012, 'learning_rate': 0.0004614785229564763, 'batch_size': 165, 'step_size': 5, 'gamma': 0.9271553016246026}
Epoch 1/300, seasonal_1 Loss: 1.6590 | 0.4382
Epoch 2/300, seasonal_1 Loss: 0.3698 | 0.3422
Epoch 3/300, seasonal_1 Loss: 0.2319 | 0.2560
Epoch 4/300, seasonal_1 Loss: 0.2257 | 0.1934
Epoch 5/300, seasonal_1 Loss: 0.3433 | 0.2438
Epoch 6/300, seasonal_1 Loss: 0.2740 | 0.2866
Epoch 7/300, seasonal_1 Loss: 0.2509 | 0.2755
Epoch 8/300, seasonal_1 Loss: 0.2636 | 0.2256
Epoch 9/300, seasonal_1 Loss: 0.1837 | 0.1874
Epoch 10/300, seasonal_1 Loss: 0.1622 | 0.1338
Epoch 11/300, seasonal_1 Loss: 0.1419 | 0.1065
Epoch 12/300, seasonal_1 Loss: 0.1275 | 0.1337
Epoch 13/300, seasonal_1 Loss: 0.1244 | 0.1590
Epoch 14/300, seasonal_1 Loss: 0.1418 | 0.0813
Epoch 15/300, seasonal_1 Loss: 0.1603 | 0.1401
Epoch 16/300, seasonal_1 Loss: 0.1339 | 0.0723
Epoch 17/300, seasonal_1 Loss: 0.1430 | 0.0661
Epoch 18/300, seasonal_1 Loss: 0.1290 | 0.0640
Epoch 19/300, seasonal_1 Loss: 0.1682 | 0.1223
Epoch 20/300, seasonal_1 Loss: 0.1922 | 0.0886
Epoch 21/300, seasonal_1 Loss: 0.2205 | 0.1288
Epoch 22/300, seasonal_1 Loss: 0.3712 | 0.3075
Epoch 23/300, seasonal_1 Loss: 0.3601 | 0.2583
Epoch 24/300, seasonal_1 Loss: 0.2588 | 0.0903
Epoch 25/300, seasonal_1 Loss: 0.1447 | 0.0836
Epoch 26/300, seasonal_1 Loss: 0.1434 | 0.0940
Epoch 27/300, seasonal_1 Loss: 0.1616 | 0.0923
Epoch 28/300, seasonal_1 Loss: 0.2343 | 0.1326
Epoch 29/300, seasonal_1 Loss: 0.2382 | 0.1973
Epoch 30/300, seasonal_1 Loss: 0.1571 | 0.1192
Epoch 31/300, seasonal_1 Loss: 0.1534 | 0.0871
Epoch 32/300, seasonal_1 Loss: 0.1442 | 0.1694
Epoch 33/300, seasonal_1 Loss: 0.1361 | 0.0949
Epoch 34/300, seasonal_1 Loss: 0.1360 | 0.0719
Epoch 35/300, seasonal_1 Loss: 0.1005 | 0.0753
Epoch 36/300, seasonal_1 Loss: 0.1036 | 0.0788
Epoch 37/300, seasonal_1 Loss: 0.0903 | 0.0532
Epoch 38/300, seasonal_1 Loss: 0.0824 | 0.0456
Epoch 39/300, seasonal_1 Loss: 0.0833 | 0.0662
Epoch 40/300, seasonal_1 Loss: 0.0825 | 0.0425
Epoch 41/300, seasonal_1 Loss: 0.0811 | 0.0456
Epoch 42/300, seasonal_1 Loss: 0.0823 | 0.0574
Epoch 43/300, seasonal_1 Loss: 0.0802 | 0.0393
Epoch 44/300, seasonal_1 Loss: 0.0810 | 0.0641
Epoch 45/300, seasonal_1 Loss: 0.0822 | 0.0455
Epoch 46/300, seasonal_1 Loss: 0.0864 | 0.0424
Epoch 47/300, seasonal_1 Loss: 0.0862 | 0.0558
Epoch 48/300, seasonal_1 Loss: 0.0812 | 0.0381
Epoch 49/300, seasonal_1 Loss: 0.0766 | 0.0554
Epoch 50/300, seasonal_1 Loss: 0.0770 | 0.0424
Epoch 51/300, seasonal_1 Loss: 0.0775 | 0.0391
Epoch 52/300, seasonal_1 Loss: 0.0746 | 0.0450
Epoch 53/300, seasonal_1 Loss: 0.0745 | 0.0349
Epoch 54/300, seasonal_1 Loss: 0.0713 | 0.0426
Epoch 55/300, seasonal_1 Loss: 0.0716 | 0.0354
Epoch 56/300, seasonal_1 Loss: 0.0717 | 0.0393
Epoch 57/300, seasonal_1 Loss: 0.0705 | 0.0362
Epoch 58/300, seasonal_1 Loss: 0.0706 | 0.0343
Epoch 59/300, seasonal_1 Loss: 0.0709 | 0.0407
Epoch 60/300, seasonal_1 Loss: 0.0686 | 0.0328
Epoch 61/300, seasonal_1 Loss: 0.0678 | 0.0374
Epoch 62/300, seasonal_1 Loss: 0.0677 | 0.0319
Epoch 63/300, seasonal_1 Loss: 0.0677 | 0.0342
Epoch 64/300, seasonal_1 Loss: 0.0681 | 0.0323
Epoch 65/300, seasonal_1 Loss: 0.0676 | 0.0339
Epoch 66/300, seasonal_1 Loss: 0.0660 | 0.0347
Epoch 67/300, seasonal_1 Loss: 0.0659 | 0.0307
Epoch 68/300, seasonal_1 Loss: 0.0652 | 0.0328
Epoch 69/300, seasonal_1 Loss: 0.0649 | 0.0307
Epoch 70/300, seasonal_1 Loss: 0.0650 | 0.0343
Epoch 71/300, seasonal_1 Loss: 0.0673 | 0.0332
Epoch 72/300, seasonal_1 Loss: 0.0660 | 0.0318
Epoch 73/300, seasonal_1 Loss: 0.0644 | 0.0314
Epoch 74/300, seasonal_1 Loss: 0.0640 | 0.0303
Epoch 75/300, seasonal_1 Loss: 0.0633 | 0.0304
Epoch 76/300, seasonal_1 Loss: 0.0630 | 0.0295
Epoch 77/300, seasonal_1 Loss: 0.0630 | 0.0302
Epoch 78/300, seasonal_1 Loss: 0.0633 | 0.0295
Epoch 79/300, seasonal_1 Loss: 0.0639 | 0.0319
Epoch 80/300, seasonal_1 Loss: 0.0630 | 0.0304
Epoch 81/300, seasonal_1 Loss: 0.0638 | 0.0299
Epoch 82/300, seasonal_1 Loss: 0.0626 | 0.0298
Epoch 83/300, seasonal_1 Loss: 0.0620 | 0.0298
Epoch 84/300, seasonal_1 Loss: 0.0620 | 0.0285
Epoch 85/300, seasonal_1 Loss: 0.0614 | 0.0282
Epoch 86/300, seasonal_1 Loss: 0.0612 | 0.0283
Epoch 87/300, seasonal_1 Loss: 0.0608 | 0.0277
Epoch 88/300, seasonal_1 Loss: 0.0607 | 0.0273
Epoch 89/300, seasonal_1 Loss: 0.0605 | 0.0271
Epoch 90/300, seasonal_1 Loss: 0.0605 | 0.0273
Epoch 91/300, seasonal_1 Loss: 0.0606 | 0.0273
Epoch 92/300, seasonal_1 Loss: 0.0625 | 0.0272
Epoch 93/300, seasonal_1 Loss: 0.0638 | 0.0322
Epoch 94/300, seasonal_1 Loss: 0.0614 | 0.0278
Epoch 95/300, seasonal_1 Loss: 0.0614 | 0.0267
Epoch 96/300, seasonal_1 Loss: 0.0613 | 0.0278
Epoch 97/300, seasonal_1 Loss: 0.0606 | 0.0266
Epoch 98/300, seasonal_1 Loss: 0.0600 | 0.0278
Epoch 99/300, seasonal_1 Loss: 0.0599 | 0.0264
Epoch 100/300, seasonal_1 Loss: 0.0605 | 0.0287
Epoch 101/300, seasonal_1 Loss: 0.0612 | 0.0255
Epoch 102/300, seasonal_1 Loss: 0.0615 | 0.0279
Epoch 103/300, seasonal_1 Loss: 0.0609 | 0.0261
Epoch 104/300, seasonal_1 Loss: 0.0596 | 0.0276
Epoch 105/300, seasonal_1 Loss: 0.0594 | 0.0259
Epoch 106/300, seasonal_1 Loss: 0.0597 | 0.0275
Epoch 107/300, seasonal_1 Loss: 0.0595 | 0.0258
Epoch 108/300, seasonal_1 Loss: 0.0603 | 0.0280
Epoch 109/300, seasonal_1 Loss: 0.0601 | 0.0266
Epoch 110/300, seasonal_1 Loss: 0.0592 | 0.0275
Epoch 111/300, seasonal_1 Loss: 0.0591 | 0.0259
Epoch 112/300, seasonal_1 Loss: 0.0601 | 0.0269
Epoch 113/300, seasonal_1 Loss: 0.0589 | 0.0256
Epoch 114/300, seasonal_1 Loss: 0.0588 | 0.0270
Epoch 115/300, seasonal_1 Loss: 0.0584 | 0.0260
Epoch 116/300, seasonal_1 Loss: 0.0582 | 0.0267
Epoch 117/300, seasonal_1 Loss: 0.0585 | 0.0255
Epoch 118/300, seasonal_1 Loss: 0.0586 | 0.0261
Epoch 119/300, seasonal_1 Loss: 0.0582 | 0.0256
Epoch 120/300, seasonal_1 Loss: 0.0581 | 0.0263
Epoch 121/300, seasonal_1 Loss: 0.0578 | 0.0260
Epoch 122/300, seasonal_1 Loss: 0.0578 | 0.0258
Epoch 123/300, seasonal_1 Loss: 0.0581 | 0.0253
Epoch 124/300, seasonal_1 Loss: 0.0577 | 0.0255
Epoch 125/300, seasonal_1 Loss: 0.0577 | 0.0257
Epoch 126/300, seasonal_1 Loss: 0.0576 | 0.0260
Epoch 127/300, seasonal_1 Loss: 0.0574 | 0.0257
Epoch 128/300, seasonal_1 Loss: 0.0576 | 0.0253
Epoch 129/300, seasonal_1 Loss: 0.0575 | 0.0252
Epoch 130/300, seasonal_1 Loss: 0.0573 | 0.0254
Epoch 131/300, seasonal_1 Loss: 0.0573 | 0.0257
Epoch 132/300, seasonal_1 Loss: 0.0572 | 0.0256
Epoch 133/300, seasonal_1 Loss: 0.0572 | 0.0254
Epoch 134/300, seasonal_1 Loss: 0.0572 | 0.0251
Epoch 135/300, seasonal_1 Loss: 0.0571 | 0.0252
Epoch 136/300, seasonal_1 Loss: 0.0570 | 0.0254
Epoch 137/300, seasonal_1 Loss: 0.0570 | 0.0255
Epoch 138/300, seasonal_1 Loss: 0.0569 | 0.0253
Epoch 139/300, seasonal_1 Loss: 0.0569 | 0.0251
Epoch 140/300, seasonal_1 Loss: 0.0569 | 0.0251
Epoch 141/300, seasonal_1 Loss: 0.0568 | 0.0252
Epoch 142/300, seasonal_1 Loss: 0.0568 | 0.0253
Epoch 143/300, seasonal_1 Loss: 0.0567 | 0.0253
Epoch 144/300, seasonal_1 Loss: 0.0567 | 0.0251
Epoch 145/300, seasonal_1 Loss: 0.0567 | 0.0251
Epoch 146/300, seasonal_1 Loss: 0.0566 | 0.0251
Epoch 147/300, seasonal_1 Loss: 0.0566 | 0.0252
Epoch 148/300, seasonal_1 Loss: 0.0566 | 0.0252
Epoch 149/300, seasonal_1 Loss: 0.0566 | 0.0251
Epoch 150/300, seasonal_1 Loss: 0.0565 | 0.0251
Epoch 151/300, seasonal_1 Loss: 0.0565 | 0.0251
Epoch 152/300, seasonal_1 Loss: 0.0565 | 0.0251
Epoch 153/300, seasonal_1 Loss: 0.0565 | 0.0251
Epoch 154/300, seasonal_1 Loss: 0.0564 | 0.0251
Epoch 155/300, seasonal_1 Loss: 0.0564 | 0.0251
Epoch 156/300, seasonal_1 Loss: 0.0564 | 0.0251
Epoch 157/300, seasonal_1 Loss: 0.0564 | 0.0251
Epoch 158/300, seasonal_1 Loss: 0.0564 | 0.0251
Epoch 159/300, seasonal_1 Loss: 0.0563 | 0.0251
Epoch 160/300, seasonal_1 Loss: 0.0563 | 0.0250
Epoch 161/300, seasonal_1 Loss: 0.0563 | 0.0250
Epoch 162/300, seasonal_1 Loss: 0.0563 | 0.0250
Epoch 163/300, seasonal_1 Loss: 0.0563 | 0.0250
Epoch 164/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 165/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 166/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 167/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 168/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 169/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 170/300, seasonal_1 Loss: 0.0562 | 0.0250
Epoch 171/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 172/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 173/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 174/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 175/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 176/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 177/300, seasonal_1 Loss: 0.0561 | 0.0250
Epoch 178/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 179/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 180/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 181/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 182/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 183/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 184/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 185/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 186/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 187/300, seasonal_1 Loss: 0.0560 | 0.0250
Epoch 188/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 189/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 190/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 191/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 192/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 193/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 194/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 195/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 196/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 197/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 198/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 199/300, seasonal_1 Loss: 0.0559 | 0.0250
Epoch 200/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 201/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 202/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 203/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 204/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 205/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 206/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 207/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 208/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 209/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 210/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 211/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 212/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 213/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 214/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 215/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 216/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 217/300, seasonal_1 Loss: 0.0558 | 0.0250
Epoch 218/300, seasonal_1 Loss: 0.0557 | 0.0250
Epoch 219/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 220/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 221/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 222/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 223/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 224/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 225/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 226/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 227/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 228/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 229/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 230/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 231/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 232/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 233/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 234/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 235/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 236/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 237/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 238/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 239/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 240/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 241/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 242/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 243/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 244/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 245/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 246/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 247/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 248/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 249/300, seasonal_1 Loss: 0.0557 | 0.0249
Epoch 250/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 251/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 252/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 253/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 254/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 255/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 256/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 257/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 258/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 259/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 260/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 261/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 262/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 263/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 264/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 265/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 266/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 267/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 268/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 269/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 270/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 271/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 272/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 273/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 274/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 275/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 276/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 277/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 278/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 279/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 280/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 281/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 282/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 283/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 284/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 285/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 286/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 287/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 288/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 289/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 290/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 291/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 292/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 293/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 294/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 295/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 296/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 297/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 298/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 299/300, seasonal_1 Loss: 0.0556 | 0.0249
Epoch 300/300, seasonal_1 Loss: 0.0556 | 0.0249
Training seasonal_2 component with params: {'observation_period_num': 14, 'train_rates': 0.8619700754466173, 'learning_rate': 0.00013582259781636994, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9439938673358765}
Epoch 1/300, seasonal_2 Loss: 0.2619 | 0.0776
Epoch 2/300, seasonal_2 Loss: 0.1038 | 0.0740
Epoch 3/300, seasonal_2 Loss: 0.0954 | 0.0921
Epoch 4/300, seasonal_2 Loss: 0.1037 | 0.0719
Epoch 5/300, seasonal_2 Loss: 0.0987 | 0.0521
Epoch 6/300, seasonal_2 Loss: 0.0819 | 0.0585
Epoch 7/300, seasonal_2 Loss: 0.0785 | 0.0509
Epoch 8/300, seasonal_2 Loss: 0.0760 | 0.0465
Epoch 9/300, seasonal_2 Loss: 0.0724 | 0.0430
Epoch 10/300, seasonal_2 Loss: 0.0666 | 0.0447
Epoch 11/300, seasonal_2 Loss: 0.0632 | 0.0437
Epoch 12/300, seasonal_2 Loss: 0.0620 | 0.0421
Epoch 13/300, seasonal_2 Loss: 0.0610 | 0.0414
Epoch 14/300, seasonal_2 Loss: 0.0606 | 0.0419
Epoch 15/300, seasonal_2 Loss: 0.0584 | 0.0401
Epoch 16/300, seasonal_2 Loss: 0.0568 | 0.0376
Epoch 17/300, seasonal_2 Loss: 0.0566 | 0.0348
Epoch 18/300, seasonal_2 Loss: 0.0555 | 0.0351
Epoch 19/300, seasonal_2 Loss: 0.0547 | 0.0347
Epoch 20/300, seasonal_2 Loss: 0.0541 | 0.0349
Epoch 21/300, seasonal_2 Loss: 0.0536 | 0.0341
Epoch 22/300, seasonal_2 Loss: 0.0531 | 0.0340
Epoch 23/300, seasonal_2 Loss: 0.0527 | 0.0338
Epoch 24/300, seasonal_2 Loss: 0.0525 | 0.0330
Epoch 25/300, seasonal_2 Loss: 0.0523 | 0.0326
Epoch 26/300, seasonal_2 Loss: 0.0522 | 0.0327
Epoch 27/300, seasonal_2 Loss: 0.0520 | 0.0328
Epoch 28/300, seasonal_2 Loss: 0.0518 | 0.0328
Epoch 29/300, seasonal_2 Loss: 0.0516 | 0.0330
Epoch 30/300, seasonal_2 Loss: 0.0514 | 0.0330
Epoch 31/300, seasonal_2 Loss: 0.0514 | 0.0327
Epoch 32/300, seasonal_2 Loss: 0.0515 | 0.0321
Epoch 33/300, seasonal_2 Loss: 0.0516 | 0.0314
Epoch 34/300, seasonal_2 Loss: 0.0516 | 0.0305
Epoch 35/300, seasonal_2 Loss: 0.0516 | 0.0293
Epoch 36/300, seasonal_2 Loss: 0.0515 | 0.0279
Epoch 37/300, seasonal_2 Loss: 0.0513 | 0.0267
Epoch 38/300, seasonal_2 Loss: 0.0511 | 0.0259
Epoch 39/300, seasonal_2 Loss: 0.0506 | 0.0254
Epoch 40/300, seasonal_2 Loss: 0.0501 | 0.0251
Epoch 41/300, seasonal_2 Loss: 0.0496 | 0.0252
Epoch 42/300, seasonal_2 Loss: 0.0492 | 0.0256
Epoch 43/300, seasonal_2 Loss: 0.0489 | 0.0259
Epoch 44/300, seasonal_2 Loss: 0.0487 | 0.0262
Epoch 45/300, seasonal_2 Loss: 0.0486 | 0.0264
Epoch 46/300, seasonal_2 Loss: 0.0485 | 0.0266
Epoch 47/300, seasonal_2 Loss: 0.0484 | 0.0267
Epoch 48/300, seasonal_2 Loss: 0.0483 | 0.0268
Epoch 49/300, seasonal_2 Loss: 0.0482 | 0.0269
Epoch 50/300, seasonal_2 Loss: 0.0481 | 0.0271
Epoch 51/300, seasonal_2 Loss: 0.0481 | 0.0272
Epoch 52/300, seasonal_2 Loss: 0.0480 | 0.0273
Epoch 53/300, seasonal_2 Loss: 0.0480 | 0.0274
Epoch 54/300, seasonal_2 Loss: 0.0479 | 0.0275
Epoch 55/300, seasonal_2 Loss: 0.0479 | 0.0275
Epoch 56/300, seasonal_2 Loss: 0.0478 | 0.0275
Epoch 57/300, seasonal_2 Loss: 0.0478 | 0.0275
Epoch 58/300, seasonal_2 Loss: 0.0478 | 0.0275
Epoch 59/300, seasonal_2 Loss: 0.0477 | 0.0274
Epoch 60/300, seasonal_2 Loss: 0.0477 | 0.0274
Epoch 61/300, seasonal_2 Loss: 0.0477 | 0.0274
Epoch 62/300, seasonal_2 Loss: 0.0476 | 0.0273
Epoch 63/300, seasonal_2 Loss: 0.0476 | 0.0273
Epoch 64/300, seasonal_2 Loss: 0.0476 | 0.0273
Epoch 65/300, seasonal_2 Loss: 0.0476 | 0.0273
Epoch 66/300, seasonal_2 Loss: 0.0476 | 0.0273
Epoch 67/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 68/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 69/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 70/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 71/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 72/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 73/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 74/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 75/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 76/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 77/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 78/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 79/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 80/300, seasonal_2 Loss: 0.0475 | 0.0272
Epoch 81/300, seasonal_2 Loss: 0.0474 | 0.0272
Epoch 82/300, seasonal_2 Loss: 0.0474 | 0.0272
Epoch 83/300, seasonal_2 Loss: 0.0474 | 0.0272
Epoch 84/300, seasonal_2 Loss: 0.0474 | 0.0272
Epoch 85/300, seasonal_2 Loss: 0.0474 | 0.0272
Epoch 86/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 87/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 88/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 89/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 90/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 91/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 92/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 93/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 94/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 95/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 96/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 97/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 98/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 99/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 100/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 101/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 102/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 103/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 104/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 105/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 106/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 107/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 108/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 109/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 110/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 111/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 112/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 113/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 114/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 115/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 116/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 117/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 118/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 119/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 120/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 121/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 122/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 123/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 124/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 125/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 126/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 127/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 128/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 129/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 130/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 131/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 132/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 133/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 134/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 135/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 136/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 137/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 138/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 139/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 140/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 141/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 142/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 143/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 144/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 145/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 146/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 147/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 148/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 149/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 150/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 151/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 152/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 153/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 154/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 155/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 156/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 157/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 158/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 159/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 160/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 161/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 162/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 163/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 164/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 165/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 166/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 167/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 168/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 169/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 170/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 171/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 172/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 173/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 174/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 175/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 176/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 177/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 178/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 179/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 180/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 181/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 182/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 183/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 184/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 185/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 186/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 187/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 188/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 189/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 190/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 191/300, seasonal_2 Loss: 0.0474 | 0.0271
Epoch 192/300, seasonal_2 Loss: 0.0474 | 0.0271
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 8, 'train_rates': 0.9890331019567238, 'learning_rate': 0.0006642510936570916, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8593517235714349}
Epoch 1/300, seasonal_3 Loss: 1.7015 | 0.7035
Epoch 2/300, seasonal_3 Loss: 0.4057 | 0.3629
Epoch 3/300, seasonal_3 Loss: 0.2792 | 0.2140
Epoch 4/300, seasonal_3 Loss: 0.2269 | 0.1379
Epoch 5/300, seasonal_3 Loss: 0.1537 | 0.1656
Epoch 6/300, seasonal_3 Loss: 0.1466 | 0.0878
Epoch 7/300, seasonal_3 Loss: 0.1280 | 0.0908
Epoch 8/300, seasonal_3 Loss: 0.1484 | 0.0616
Epoch 9/300, seasonal_3 Loss: 0.1293 | 0.0735
Epoch 10/300, seasonal_3 Loss: 0.1273 | 0.2584
Epoch 11/300, seasonal_3 Loss: 0.1603 | 0.1061
Epoch 12/300, seasonal_3 Loss: 0.1579 | 0.0659
Epoch 13/300, seasonal_3 Loss: 0.1582 | 0.1013
Epoch 14/300, seasonal_3 Loss: 0.1705 | 0.1150
Epoch 15/300, seasonal_3 Loss: 0.2257 | 0.1048
Epoch 16/300, seasonal_3 Loss: 0.2207 | 0.1031
Epoch 17/300, seasonal_3 Loss: 0.1798 | 0.1033
Epoch 18/300, seasonal_3 Loss: 0.2066 | 0.1110
Epoch 19/300, seasonal_3 Loss: 0.1824 | 0.0829
Epoch 20/300, seasonal_3 Loss: 0.1484 | 0.0916
Epoch 21/300, seasonal_3 Loss: 0.1928 | 0.0891
Epoch 22/300, seasonal_3 Loss: 0.1440 | 0.0621
Epoch 23/300, seasonal_3 Loss: 0.1560 | 0.0683
Epoch 24/300, seasonal_3 Loss: 0.1995 | 0.1299
Epoch 25/300, seasonal_3 Loss: 0.2455 | 0.1233
Epoch 26/300, seasonal_3 Loss: 0.1764 | 0.0855
Epoch 27/300, seasonal_3 Loss: 0.1319 | 0.0769
Epoch 28/300, seasonal_3 Loss: 0.2099 | 0.1503
Epoch 29/300, seasonal_3 Loss: 0.1615 | 0.0400
Epoch 30/300, seasonal_3 Loss: 0.1151 | 0.0609
Epoch 31/300, seasonal_3 Loss: 0.1047 | 0.0589
Epoch 32/300, seasonal_3 Loss: 0.1014 | 0.0554
Epoch 33/300, seasonal_3 Loss: 0.1171 | 0.0487
Epoch 34/300, seasonal_3 Loss: 0.1197 | 0.0421
Epoch 35/300, seasonal_3 Loss: 0.1318 | 0.0804
Epoch 36/300, seasonal_3 Loss: 0.1344 | 0.0603
Epoch 37/300, seasonal_3 Loss: 0.1393 | 0.0454
Epoch 38/300, seasonal_3 Loss: 0.1088 | 0.0421
Epoch 39/300, seasonal_3 Loss: 0.1177 | 0.0481
Epoch 40/300, seasonal_3 Loss: 0.0840 | 0.0507
Epoch 41/300, seasonal_3 Loss: 0.0804 | 0.0538
Epoch 42/300, seasonal_3 Loss: 0.0808 | 0.0653
Epoch 43/300, seasonal_3 Loss: 0.0848 | 0.0643
Epoch 44/300, seasonal_3 Loss: 0.0897 | 0.0399
Epoch 45/300, seasonal_3 Loss: 0.0742 | 0.0359
Epoch 46/300, seasonal_3 Loss: 0.0841 | 0.0456
Epoch 47/300, seasonal_3 Loss: 0.0798 | 0.0436
Epoch 48/300, seasonal_3 Loss: 0.0809 | 0.0411
Epoch 49/300, seasonal_3 Loss: 0.0909 | 0.0359
Epoch 50/300, seasonal_3 Loss: 0.0944 | 0.0442
Epoch 51/300, seasonal_3 Loss: 0.0828 | 0.0432
Epoch 52/300, seasonal_3 Loss: 0.0807 | 0.0486
Epoch 53/300, seasonal_3 Loss: 0.0997 | 0.0909
Epoch 54/300, seasonal_3 Loss: 0.0819 | 0.0500
Epoch 55/300, seasonal_3 Loss: 0.0888 | 0.1857
Epoch 56/300, seasonal_3 Loss: 0.0752 | 0.0685
Epoch 57/300, seasonal_3 Loss: 0.0777 | 0.0456
Epoch 58/300, seasonal_3 Loss: 0.0843 | 0.0383
Epoch 59/300, seasonal_3 Loss: 0.0700 | 0.0308
Epoch 60/300, seasonal_3 Loss: 0.0634 | 0.0279
Epoch 61/300, seasonal_3 Loss: 0.0605 | 0.0283
Epoch 62/300, seasonal_3 Loss: 0.0603 | 0.0289
Epoch 63/300, seasonal_3 Loss: 0.0630 | 0.0270
Epoch 64/300, seasonal_3 Loss: 0.0646 | 0.0285
Epoch 65/300, seasonal_3 Loss: 0.0639 | 0.0360
Epoch 66/300, seasonal_3 Loss: 0.0642 | 0.0305
Epoch 67/300, seasonal_3 Loss: 0.0766 | 0.0440
Epoch 68/300, seasonal_3 Loss: 0.0953 | 0.0501
Epoch 69/300, seasonal_3 Loss: 0.1006 | 0.0283
Epoch 70/300, seasonal_3 Loss: 0.0828 | 0.0303
Epoch 71/300, seasonal_3 Loss: 0.0713 | 0.0344
Epoch 72/300, seasonal_3 Loss: 0.0717 | 0.0381
Epoch 73/300, seasonal_3 Loss: 0.0701 | 0.0188
Epoch 74/300, seasonal_3 Loss: 0.0649 | 0.0205
Epoch 75/300, seasonal_3 Loss: 0.0718 | 0.0197
Epoch 76/300, seasonal_3 Loss: 0.0689 | 0.0250
Epoch 77/300, seasonal_3 Loss: 0.0618 | 0.0246
Epoch 78/300, seasonal_3 Loss: 0.0573 | 0.0178
Epoch 79/300, seasonal_3 Loss: 0.0612 | 0.0130
Epoch 80/300, seasonal_3 Loss: 0.0541 | 0.0152
Epoch 81/300, seasonal_3 Loss: 0.0546 | 0.0141
Epoch 82/300, seasonal_3 Loss: 0.0539 | 0.0144
Epoch 83/300, seasonal_3 Loss: 0.0531 | 0.0146
Epoch 84/300, seasonal_3 Loss: 0.0530 | 0.0171
Epoch 85/300, seasonal_3 Loss: 0.0523 | 0.0175
Epoch 86/300, seasonal_3 Loss: 0.0517 | 0.0158
Epoch 87/300, seasonal_3 Loss: 0.0525 | 0.0155
Epoch 88/300, seasonal_3 Loss: 0.0557 | 0.0178
Epoch 89/300, seasonal_3 Loss: 0.0535 | 0.0167
Epoch 90/300, seasonal_3 Loss: 0.0530 | 0.0168
Epoch 91/300, seasonal_3 Loss: 0.0511 | 0.0135
Epoch 92/300, seasonal_3 Loss: 0.0502 | 0.0126
Epoch 93/300, seasonal_3 Loss: 0.0498 | 0.0132
Epoch 94/300, seasonal_3 Loss: 0.0495 | 0.0138
Epoch 95/300, seasonal_3 Loss: 0.0495 | 0.0142
Epoch 96/300, seasonal_3 Loss: 0.0497 | 0.0143
Epoch 97/300, seasonal_3 Loss: 0.0495 | 0.0138
Epoch 98/300, seasonal_3 Loss: 0.0491 | 0.0134
Epoch 99/300, seasonal_3 Loss: 0.0490 | 0.0127
Epoch 100/300, seasonal_3 Loss: 0.0497 | 0.0127
Epoch 101/300, seasonal_3 Loss: 0.0496 | 0.0129
Epoch 102/300, seasonal_3 Loss: 0.0489 | 0.0139
Epoch 103/300, seasonal_3 Loss: 0.0490 | 0.0149
Epoch 104/300, seasonal_3 Loss: 0.0490 | 0.0143
Epoch 105/300, seasonal_3 Loss: 0.0484 | 0.0133
Epoch 106/300, seasonal_3 Loss: 0.0481 | 0.0128
Epoch 107/300, seasonal_3 Loss: 0.0482 | 0.0127
Epoch 108/300, seasonal_3 Loss: 0.0481 | 0.0130
Epoch 109/300, seasonal_3 Loss: 0.0478 | 0.0136
Epoch 110/300, seasonal_3 Loss: 0.0477 | 0.0142
Epoch 111/300, seasonal_3 Loss: 0.0477 | 0.0138
Epoch 112/300, seasonal_3 Loss: 0.0475 | 0.0135
Epoch 113/300, seasonal_3 Loss: 0.0473 | 0.0132
Epoch 114/300, seasonal_3 Loss: 0.0473 | 0.0129
Epoch 115/300, seasonal_3 Loss: 0.0473 | 0.0130
Epoch 116/300, seasonal_3 Loss: 0.0472 | 0.0132
Epoch 117/300, seasonal_3 Loss: 0.0471 | 0.0136
Epoch 118/300, seasonal_3 Loss: 0.0470 | 0.0137
Epoch 119/300, seasonal_3 Loss: 0.0470 | 0.0136
Epoch 120/300, seasonal_3 Loss: 0.0470 | 0.0136
Epoch 121/300, seasonal_3 Loss: 0.0469 | 0.0134
Epoch 122/300, seasonal_3 Loss: 0.0467 | 0.0130
Epoch 123/300, seasonal_3 Loss: 0.0468 | 0.0128
Epoch 124/300, seasonal_3 Loss: 0.0468 | 0.0129
Epoch 125/300, seasonal_3 Loss: 0.0466 | 0.0132
Epoch 126/300, seasonal_3 Loss: 0.0465 | 0.0137
Epoch 127/300, seasonal_3 Loss: 0.0467 | 0.0137
Epoch 128/300, seasonal_3 Loss: 0.0467 | 0.0135
Epoch 129/300, seasonal_3 Loss: 0.0464 | 0.0131
Epoch 130/300, seasonal_3 Loss: 0.0464 | 0.0127
Epoch 131/300, seasonal_3 Loss: 0.0464 | 0.0128
Epoch 132/300, seasonal_3 Loss: 0.0463 | 0.0134
Epoch 133/300, seasonal_3 Loss: 0.0462 | 0.0137
Epoch 134/300, seasonal_3 Loss: 0.0463 | 0.0136
Epoch 135/300, seasonal_3 Loss: 0.0462 | 0.0134
Epoch 136/300, seasonal_3 Loss: 0.0461 | 0.0130
Epoch 137/300, seasonal_3 Loss: 0.0461 | 0.0129
Epoch 138/300, seasonal_3 Loss: 0.0460 | 0.0132
Epoch 139/300, seasonal_3 Loss: 0.0459 | 0.0135
Epoch 140/300, seasonal_3 Loss: 0.0459 | 0.0136
Epoch 141/300, seasonal_3 Loss: 0.0459 | 0.0134
Epoch 142/300, seasonal_3 Loss: 0.0458 | 0.0132
Epoch 143/300, seasonal_3 Loss: 0.0458 | 0.0131
Epoch 144/300, seasonal_3 Loss: 0.0458 | 0.0132
Epoch 145/300, seasonal_3 Loss: 0.0457 | 0.0134
Epoch 146/300, seasonal_3 Loss: 0.0457 | 0.0135
Epoch 147/300, seasonal_3 Loss: 0.0457 | 0.0135
Epoch 148/300, seasonal_3 Loss: 0.0456 | 0.0133
Epoch 149/300, seasonal_3 Loss: 0.0456 | 0.0132
Epoch 150/300, seasonal_3 Loss: 0.0456 | 0.0132
Epoch 151/300, seasonal_3 Loss: 0.0455 | 0.0134
Epoch 152/300, seasonal_3 Loss: 0.0455 | 0.0135
Epoch 153/300, seasonal_3 Loss: 0.0455 | 0.0135
Epoch 154/300, seasonal_3 Loss: 0.0455 | 0.0134
Epoch 155/300, seasonal_3 Loss: 0.0454 | 0.0132
Epoch 156/300, seasonal_3 Loss: 0.0454 | 0.0132
Epoch 157/300, seasonal_3 Loss: 0.0454 | 0.0133
Epoch 158/300, seasonal_3 Loss: 0.0453 | 0.0134
Epoch 159/300, seasonal_3 Loss: 0.0453 | 0.0135
Epoch 160/300, seasonal_3 Loss: 0.0453 | 0.0134
Epoch 161/300, seasonal_3 Loss: 0.0453 | 0.0133
Epoch 162/300, seasonal_3 Loss: 0.0453 | 0.0133
Epoch 163/300, seasonal_3 Loss: 0.0452 | 0.0134
Epoch 164/300, seasonal_3 Loss: 0.0452 | 0.0134
Epoch 165/300, seasonal_3 Loss: 0.0452 | 0.0134
Epoch 166/300, seasonal_3 Loss: 0.0452 | 0.0134
Epoch 167/300, seasonal_3 Loss: 0.0452 | 0.0133
Epoch 168/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 169/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 170/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 171/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 172/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 173/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 174/300, seasonal_3 Loss: 0.0451 | 0.0134
Epoch 175/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 176/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 177/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 178/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 179/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 180/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 181/300, seasonal_3 Loss: 0.0450 | 0.0134
Epoch 182/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 183/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 184/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 185/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 186/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 187/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 188/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 189/300, seasonal_3 Loss: 0.0449 | 0.0134
Epoch 190/300, seasonal_3 Loss: 0.0449 | 0.0135
Epoch 191/300, seasonal_3 Loss: 0.0449 | 0.0135
Epoch 192/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 193/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 194/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 195/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 196/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 197/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 198/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 199/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 200/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 201/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 202/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 203/300, seasonal_3 Loss: 0.0448 | 0.0135
Epoch 204/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 205/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 206/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 207/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 208/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 209/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 210/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 211/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 212/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 213/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 214/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 215/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 216/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 217/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 218/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 219/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 220/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 221/300, seasonal_3 Loss: 0.0447 | 0.0135
Epoch 222/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 223/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 224/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 225/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 226/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 227/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 228/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 229/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 230/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 231/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 232/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 233/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 234/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 235/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 236/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 237/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 238/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 239/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 240/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 241/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 242/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 243/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 244/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 245/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 246/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 247/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 248/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 249/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 250/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 251/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 252/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 253/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 254/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 255/300, seasonal_3 Loss: 0.0446 | 0.0135
Epoch 256/300, seasonal_3 Loss: 0.0445 | 0.0135
Epoch 257/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 258/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 259/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 260/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 261/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 262/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 263/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 264/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 265/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 266/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 267/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 268/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 269/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 270/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 271/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 272/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 273/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 274/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 275/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 276/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 277/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 278/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 279/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 280/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 281/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 282/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 283/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 284/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 285/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 286/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 287/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 288/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 289/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 290/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 291/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 292/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 293/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 294/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 295/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 296/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 297/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 298/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 299/300, seasonal_3 Loss: 0.0445 | 0.0136
Epoch 300/300, seasonal_3 Loss: 0.0445 | 0.0136
Training resid component with params: {'observation_period_num': 23, 'train_rates': 0.8416757955846104, 'learning_rate': 0.00027266558565206483, 'batch_size': 190, 'step_size': 8, 'gamma': 0.8686063079924085}
Epoch 1/300, resid Loss: 1.4877 | 0.4488
Epoch 2/300, resid Loss: 0.3716 | 0.3366
Epoch 3/300, resid Loss: 0.2401 | 0.1915
Epoch 4/300, resid Loss: 0.2657 | 0.2346
Epoch 5/300, resid Loss: 0.2130 | 0.2972
Epoch 6/300, resid Loss: 0.1996 | 0.2599
Epoch 7/300, resid Loss: 0.2423 | 0.6110
Epoch 8/300, resid Loss: 0.3323 | 0.3012
Epoch 9/300, resid Loss: 0.2668 | 0.2625
Epoch 10/300, resid Loss: 0.2156 | 0.1546
Epoch 11/300, resid Loss: 0.1772 | 0.1309
Epoch 12/300, resid Loss: 0.3031 | 0.3713
Epoch 13/300, resid Loss: 0.4251 | 0.2995
Epoch 14/300, resid Loss: 0.3732 | 0.1676
Epoch 15/300, resid Loss: 0.3026 | 0.2674
Epoch 16/300, resid Loss: 0.2092 | 0.5548
Epoch 17/300, resid Loss: 0.3141 | 0.2270
Epoch 18/300, resid Loss: 0.1790 | 0.1761
Epoch 19/300, resid Loss: 0.1448 | 0.1118
Epoch 20/300, resid Loss: 0.1163 | 0.0758
Epoch 21/300, resid Loss: 0.1051 | 0.0761
Epoch 22/300, resid Loss: 0.1023 | 0.0616
Epoch 23/300, resid Loss: 0.1032 | 0.0665
Epoch 24/300, resid Loss: 0.1022 | 0.0609
Epoch 25/300, resid Loss: 0.1072 | 0.0576
Epoch 26/300, resid Loss: 0.1231 | 0.0725
Epoch 27/300, resid Loss: 0.1053 | 0.0601
Epoch 28/300, resid Loss: 0.0937 | 0.0786
Epoch 29/300, resid Loss: 0.0950 | 0.0684
Epoch 30/300, resid Loss: 0.0890 | 0.0527
Epoch 31/300, resid Loss: 0.0882 | 0.0489
Epoch 32/300, resid Loss: 0.0842 | 0.0662
Epoch 33/300, resid Loss: 0.0805 | 0.0503
Epoch 34/300, resid Loss: 0.0793 | 0.0590
Epoch 35/300, resid Loss: 0.0796 | 0.0461
Epoch 36/300, resid Loss: 0.0793 | 0.0442
Epoch 37/300, resid Loss: 0.0798 | 0.0656
Epoch 38/300, resid Loss: 0.0804 | 0.0454
Epoch 39/300, resid Loss: 0.0774 | 0.0530
Epoch 40/300, resid Loss: 0.0771 | 0.0436
Epoch 41/300, resid Loss: 0.0751 | 0.0435
Epoch 42/300, resid Loss: 0.0739 | 0.0437
Epoch 43/300, resid Loss: 0.0730 | 0.0447
Epoch 44/300, resid Loss: 0.0720 | 0.0423
Epoch 45/300, resid Loss: 0.0718 | 0.0392
Epoch 46/300, resid Loss: 0.0721 | 0.0463
Epoch 47/300, resid Loss: 0.0710 | 0.0394
Epoch 48/300, resid Loss: 0.0711 | 0.0412
Epoch 49/300, resid Loss: 0.0701 | 0.0401
Epoch 50/300, resid Loss: 0.0704 | 0.0372
Epoch 51/300, resid Loss: 0.0706 | 0.0479
Epoch 52/300, resid Loss: 0.0725 | 0.0379
Epoch 53/300, resid Loss: 0.0715 | 0.0398
Epoch 54/300, resid Loss: 0.0705 | 0.0379
Epoch 55/300, resid Loss: 0.0709 | 0.0420
Epoch 56/300, resid Loss: 0.0700 | 0.0383
Epoch 57/300, resid Loss: 0.0696 | 0.0388
Epoch 58/300, resid Loss: 0.0719 | 0.0456
Epoch 59/300, resid Loss: 0.0735 | 0.0404
Epoch 60/300, resid Loss: 0.0817 | 0.0816
Epoch 61/300, resid Loss: 0.0802 | 0.0434
Epoch 62/300, resid Loss: 0.0856 | 0.1054
Epoch 63/300, resid Loss: 0.0813 | 0.0429
Epoch 64/300, resid Loss: 0.0820 | 0.0894
Epoch 65/300, resid Loss: 0.0752 | 0.0407
Epoch 66/300, resid Loss: 0.0737 | 0.0693
Epoch 67/300, resid Loss: 0.0701 | 0.0402
Epoch 68/300, resid Loss: 0.0691 | 0.0591
Epoch 69/300, resid Loss: 0.0668 | 0.0382
Epoch 70/300, resid Loss: 0.0661 | 0.0524
Epoch 71/300, resid Loss: 0.0651 | 0.0375
Epoch 72/300, resid Loss: 0.0646 | 0.0469
Epoch 73/300, resid Loss: 0.0641 | 0.0376
Epoch 74/300, resid Loss: 0.0637 | 0.0434
Epoch 75/300, resid Loss: 0.0634 | 0.0377
Epoch 76/300, resid Loss: 0.0632 | 0.0409
Epoch 77/300, resid Loss: 0.0630 | 0.0379
Epoch 78/300, resid Loss: 0.0628 | 0.0396
Epoch 79/300, resid Loss: 0.0626 | 0.0379
Epoch 80/300, resid Loss: 0.0624 | 0.0385
Epoch 81/300, resid Loss: 0.0623 | 0.0376
Epoch 82/300, resid Loss: 0.0621 | 0.0378
Epoch 83/300, resid Loss: 0.0620 | 0.0375
Epoch 84/300, resid Loss: 0.0618 | 0.0373
Epoch 85/300, resid Loss: 0.0615 | 0.0370
Epoch 86/300, resid Loss: 0.0614 | 0.0368
Epoch 87/300, resid Loss: 0.0612 | 0.0367
Epoch 88/300, resid Loss: 0.0611 | 0.0366
Epoch 89/300, resid Loss: 0.0609 | 0.0365
Epoch 90/300, resid Loss: 0.0608 | 0.0363
Epoch 91/300, resid Loss: 0.0607 | 0.0362
Epoch 92/300, resid Loss: 0.0606 | 0.0362
Epoch 93/300, resid Loss: 0.0604 | 0.0362
Epoch 94/300, resid Loss: 0.0603 | 0.0361
Epoch 95/300, resid Loss: 0.0602 | 0.0359
Epoch 96/300, resid Loss: 0.0602 | 0.0358
Epoch 97/300, resid Loss: 0.0601 | 0.0358
Epoch 98/300, resid Loss: 0.0600 | 0.0358
Epoch 99/300, resid Loss: 0.0599 | 0.0357
Epoch 100/300, resid Loss: 0.0598 | 0.0356
Epoch 101/300, resid Loss: 0.0597 | 0.0356
Epoch 102/300, resid Loss: 0.0597 | 0.0356
Epoch 103/300, resid Loss: 0.0596 | 0.0355
Epoch 104/300, resid Loss: 0.0595 | 0.0355
Epoch 105/300, resid Loss: 0.0595 | 0.0354
Epoch 106/300, resid Loss: 0.0594 | 0.0354
Epoch 107/300, resid Loss: 0.0594 | 0.0354
Epoch 108/300, resid Loss: 0.0593 | 0.0353
Epoch 109/300, resid Loss: 0.0593 | 0.0353
Epoch 110/300, resid Loss: 0.0592 | 0.0353
Epoch 111/300, resid Loss: 0.0592 | 0.0352
Epoch 112/300, resid Loss: 0.0591 | 0.0352
Epoch 113/300, resid Loss: 0.0591 | 0.0352
Epoch 114/300, resid Loss: 0.0590 | 0.0352
Epoch 115/300, resid Loss: 0.0590 | 0.0351
Epoch 116/300, resid Loss: 0.0589 | 0.0351
Epoch 117/300, resid Loss: 0.0589 | 0.0351
Epoch 118/300, resid Loss: 0.0589 | 0.0351
Epoch 119/300, resid Loss: 0.0588 | 0.0350
Epoch 120/300, resid Loss: 0.0588 | 0.0350
Epoch 121/300, resid Loss: 0.0588 | 0.0350
Epoch 122/300, resid Loss: 0.0587 | 0.0350
Epoch 123/300, resid Loss: 0.0587 | 0.0350
Epoch 124/300, resid Loss: 0.0587 | 0.0350
Epoch 125/300, resid Loss: 0.0586 | 0.0349
Epoch 126/300, resid Loss: 0.0586 | 0.0349
Epoch 127/300, resid Loss: 0.0586 | 0.0349
Epoch 128/300, resid Loss: 0.0585 | 0.0349
Epoch 129/300, resid Loss: 0.0585 | 0.0349
Epoch 130/300, resid Loss: 0.0585 | 0.0349
Epoch 131/300, resid Loss: 0.0585 | 0.0348
Epoch 132/300, resid Loss: 0.0584 | 0.0348
Epoch 133/300, resid Loss: 0.0584 | 0.0348
Epoch 134/300, resid Loss: 0.0584 | 0.0348
Epoch 135/300, resid Loss: 0.0584 | 0.0348
Epoch 136/300, resid Loss: 0.0584 | 0.0348
Epoch 137/300, resid Loss: 0.0583 | 0.0348
Epoch 138/300, resid Loss: 0.0583 | 0.0348
Epoch 139/300, resid Loss: 0.0583 | 0.0348
Epoch 140/300, resid Loss: 0.0583 | 0.0347
Epoch 141/300, resid Loss: 0.0583 | 0.0347
Epoch 142/300, resid Loss: 0.0582 | 0.0347
Epoch 143/300, resid Loss: 0.0582 | 0.0347
Epoch 144/300, resid Loss: 0.0582 | 0.0347
Epoch 145/300, resid Loss: 0.0582 | 0.0347
Epoch 146/300, resid Loss: 0.0582 | 0.0347
Epoch 147/300, resid Loss: 0.0582 | 0.0347
Epoch 148/300, resid Loss: 0.0581 | 0.0347
Epoch 149/300, resid Loss: 0.0581 | 0.0347
Epoch 150/300, resid Loss: 0.0581 | 0.0347
Epoch 151/300, resid Loss: 0.0581 | 0.0347
Epoch 152/300, resid Loss: 0.0581 | 0.0346
Epoch 153/300, resid Loss: 0.0581 | 0.0346
Epoch 154/300, resid Loss: 0.0581 | 0.0346
Epoch 155/300, resid Loss: 0.0581 | 0.0346
Epoch 156/300, resid Loss: 0.0580 | 0.0346
Epoch 157/300, resid Loss: 0.0580 | 0.0346
Epoch 158/300, resid Loss: 0.0580 | 0.0346
Epoch 159/300, resid Loss: 0.0580 | 0.0346
Epoch 160/300, resid Loss: 0.0580 | 0.0346
Epoch 161/300, resid Loss: 0.0580 | 0.0346
Epoch 162/300, resid Loss: 0.0580 | 0.0346
Epoch 163/300, resid Loss: 0.0580 | 0.0346
Epoch 164/300, resid Loss: 0.0580 | 0.0346
Epoch 165/300, resid Loss: 0.0580 | 0.0346
Epoch 166/300, resid Loss: 0.0580 | 0.0346
Epoch 167/300, resid Loss: 0.0579 | 0.0346
Epoch 168/300, resid Loss: 0.0579 | 0.0346
Epoch 169/300, resid Loss: 0.0579 | 0.0346
Epoch 170/300, resid Loss: 0.0579 | 0.0346
Epoch 171/300, resid Loss: 0.0579 | 0.0346
Epoch 172/300, resid Loss: 0.0579 | 0.0346
Epoch 173/300, resid Loss: 0.0579 | 0.0345
Epoch 174/300, resid Loss: 0.0579 | 0.0345
Epoch 175/300, resid Loss: 0.0579 | 0.0345
Epoch 176/300, resid Loss: 0.0579 | 0.0345
Epoch 177/300, resid Loss: 0.0579 | 0.0345
Epoch 178/300, resid Loss: 0.0579 | 0.0345
Epoch 179/300, resid Loss: 0.0579 | 0.0345
Epoch 180/300, resid Loss: 0.0579 | 0.0345
Epoch 181/300, resid Loss: 0.0579 | 0.0345
Epoch 182/300, resid Loss: 0.0578 | 0.0345
Epoch 183/300, resid Loss: 0.0578 | 0.0345
Epoch 184/300, resid Loss: 0.0578 | 0.0345
Epoch 185/300, resid Loss: 0.0578 | 0.0345
Epoch 186/300, resid Loss: 0.0578 | 0.0345
Epoch 187/300, resid Loss: 0.0578 | 0.0345
Epoch 188/300, resid Loss: 0.0578 | 0.0345
Epoch 189/300, resid Loss: 0.0578 | 0.0345
Epoch 190/300, resid Loss: 0.0578 | 0.0345
Epoch 191/300, resid Loss: 0.0578 | 0.0345
Epoch 192/300, resid Loss: 0.0578 | 0.0345
Epoch 193/300, resid Loss: 0.0578 | 0.0345
Epoch 194/300, resid Loss: 0.0578 | 0.0345
Epoch 195/300, resid Loss: 0.0578 | 0.0345
Epoch 196/300, resid Loss: 0.0578 | 0.0345
Epoch 197/300, resid Loss: 0.0578 | 0.0345
Epoch 198/300, resid Loss: 0.0578 | 0.0345
Epoch 199/300, resid Loss: 0.0578 | 0.0345
Epoch 200/300, resid Loss: 0.0578 | 0.0345
Epoch 201/300, resid Loss: 0.0578 | 0.0345
Epoch 202/300, resid Loss: 0.0578 | 0.0345
Epoch 203/300, resid Loss: 0.0578 | 0.0345
Epoch 204/300, resid Loss: 0.0578 | 0.0345
Epoch 205/300, resid Loss: 0.0578 | 0.0345
Epoch 206/300, resid Loss: 0.0578 | 0.0345
Epoch 207/300, resid Loss: 0.0578 | 0.0345
Epoch 208/300, resid Loss: 0.0578 | 0.0345
Epoch 209/300, resid Loss: 0.0578 | 0.0345
Epoch 210/300, resid Loss: 0.0578 | 0.0345
Epoch 211/300, resid Loss: 0.0577 | 0.0345
Epoch 212/300, resid Loss: 0.0577 | 0.0345
Epoch 213/300, resid Loss: 0.0577 | 0.0345
Epoch 214/300, resid Loss: 0.0577 | 0.0345
Epoch 215/300, resid Loss: 0.0577 | 0.0345
Epoch 216/300, resid Loss: 0.0577 | 0.0345
Epoch 217/300, resid Loss: 0.0577 | 0.0345
Epoch 218/300, resid Loss: 0.0577 | 0.0345
Epoch 219/300, resid Loss: 0.0577 | 0.0345
Epoch 220/300, resid Loss: 0.0577 | 0.0345
Epoch 221/300, resid Loss: 0.0577 | 0.0345
Epoch 222/300, resid Loss: 0.0577 | 0.0345
Epoch 223/300, resid Loss: 0.0577 | 0.0345
Epoch 224/300, resid Loss: 0.0577 | 0.0345
Epoch 225/300, resid Loss: 0.0577 | 0.0345
Epoch 226/300, resid Loss: 0.0577 | 0.0345
Epoch 227/300, resid Loss: 0.0577 | 0.0345
Epoch 228/300, resid Loss: 0.0577 | 0.0345
Epoch 229/300, resid Loss: 0.0577 | 0.0345
Epoch 230/300, resid Loss: 0.0577 | 0.0345
Epoch 231/300, resid Loss: 0.0577 | 0.0345
Epoch 232/300, resid Loss: 0.0577 | 0.0345
Epoch 233/300, resid Loss: 0.0577 | 0.0345
Epoch 234/300, resid Loss: 0.0577 | 0.0345
Epoch 235/300, resid Loss: 0.0577 | 0.0345
Epoch 236/300, resid Loss: 0.0577 | 0.0345
Epoch 237/300, resid Loss: 0.0577 | 0.0345
Epoch 238/300, resid Loss: 0.0577 | 0.0345
Epoch 239/300, resid Loss: 0.0577 | 0.0345
Epoch 240/300, resid Loss: 0.0577 | 0.0345
Epoch 241/300, resid Loss: 0.0577 | 0.0345
Epoch 242/300, resid Loss: 0.0577 | 0.0345
Epoch 243/300, resid Loss: 0.0577 | 0.0345
Epoch 244/300, resid Loss: 0.0577 | 0.0345
Epoch 245/300, resid Loss: 0.0577 | 0.0345
Epoch 246/300, resid Loss: 0.0577 | 0.0345
Epoch 247/300, resid Loss: 0.0577 | 0.0345
Epoch 248/300, resid Loss: 0.0577 | 0.0345
Epoch 249/300, resid Loss: 0.0577 | 0.0345
Epoch 250/300, resid Loss: 0.0577 | 0.0345
Epoch 251/300, resid Loss: 0.0577 | 0.0345
Epoch 252/300, resid Loss: 0.0577 | 0.0345
Epoch 253/300, resid Loss: 0.0577 | 0.0345
Epoch 254/300, resid Loss: 0.0577 | 0.0345
Epoch 255/300, resid Loss: 0.0577 | 0.0345
Epoch 256/300, resid Loss: 0.0577 | 0.0345
Epoch 257/300, resid Loss: 0.0577 | 0.0345
Epoch 258/300, resid Loss: 0.0577 | 0.0345
Epoch 259/300, resid Loss: 0.0577 | 0.0345
Epoch 260/300, resid Loss: 0.0577 | 0.0344
Epoch 261/300, resid Loss: 0.0577 | 0.0344
Epoch 262/300, resid Loss: 0.0577 | 0.0344
Epoch 263/300, resid Loss: 0.0577 | 0.0344
Epoch 264/300, resid Loss: 0.0577 | 0.0344
Epoch 265/300, resid Loss: 0.0577 | 0.0344
Epoch 266/300, resid Loss: 0.0577 | 0.0344
Epoch 267/300, resid Loss: 0.0577 | 0.0344
Epoch 268/300, resid Loss: 0.0577 | 0.0344
Epoch 269/300, resid Loss: 0.0577 | 0.0344
Epoch 270/300, resid Loss: 0.0577 | 0.0344
Epoch 271/300, resid Loss: 0.0577 | 0.0344
Epoch 272/300, resid Loss: 0.0577 | 0.0344
Epoch 273/300, resid Loss: 0.0577 | 0.0344
Epoch 274/300, resid Loss: 0.0577 | 0.0344
Epoch 275/300, resid Loss: 0.0577 | 0.0344
Epoch 276/300, resid Loss: 0.0577 | 0.0344
Epoch 277/300, resid Loss: 0.0577 | 0.0344
Epoch 278/300, resid Loss: 0.0577 | 0.0344
Epoch 279/300, resid Loss: 0.0577 | 0.0344
Epoch 280/300, resid Loss: 0.0577 | 0.0344
Epoch 281/300, resid Loss: 0.0577 | 0.0344
Epoch 282/300, resid Loss: 0.0577 | 0.0344
Epoch 283/300, resid Loss: 0.0577 | 0.0344
Epoch 284/300, resid Loss: 0.0577 | 0.0344
Epoch 285/300, resid Loss: 0.0577 | 0.0344
Epoch 286/300, resid Loss: 0.0577 | 0.0344
Epoch 287/300, resid Loss: 0.0577 | 0.0344
Epoch 288/300, resid Loss: 0.0577 | 0.0344
Epoch 289/300, resid Loss: 0.0577 | 0.0344
Epoch 290/300, resid Loss: 0.0577 | 0.0344
Epoch 291/300, resid Loss: 0.0577 | 0.0344
Epoch 292/300, resid Loss: 0.0577 | 0.0344
Epoch 293/300, resid Loss: 0.0577 | 0.0344
Epoch 294/300, resid Loss: 0.0577 | 0.0344
Epoch 295/300, resid Loss: 0.0577 | 0.0344
Epoch 296/300, resid Loss: 0.0577 | 0.0344
Epoch 297/300, resid Loss: 0.0577 | 0.0344
Epoch 298/300, resid Loss: 0.0577 | 0.0344
Epoch 299/300, resid Loss: 0.0577 | 0.0344
Epoch 300/300, resid Loss: 0.0577 | 0.0344
Runtime (seconds): 6171.892032384872
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[210.61905]
[1.838596]
[0.72884154]
[4.588685]
[-0.10989618]
[6.3127537]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 79.06709088385105
RMSE: 8.8919677734375
MAE: 8.8919677734375
R-squared: nan
[223.97803]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
