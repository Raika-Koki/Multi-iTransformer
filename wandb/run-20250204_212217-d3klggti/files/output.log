[32m[I 2025-02-04 21:22:21,470][0m A new study created in memory with name: no-name-bc3cc383-323d-441a-a464-ee097e03e2bd[0m
[32m[I 2025-02-04 21:24:13,890][0m Trial 0 finished with value: 0.6952415704727173 and parameters: {'observation_period_num': 233, 'train_rates': 0.9871526222160514, 'learning_rate': 1.5740006399588796e-06, 'batch_size': 51, 'step_size': 8, 'gamma': 0.7776500819183672}. Best is trial 0 with value: 0.6952415704727173.[0m
[32m[I 2025-02-04 21:27:24,051][0m Trial 1 finished with value: 0.09819695999224981 and parameters: {'observation_period_num': 12, 'train_rates': 0.8165175952290196, 'learning_rate': 3.6862534152520277e-06, 'batch_size': 28, 'step_size': 12, 'gamma': 0.7877350368758875}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:28:44,173][0m Trial 2 finished with value: 0.38489905909100414 and parameters: {'observation_period_num': 217, 'train_rates': 0.8081307355517966, 'learning_rate': 3.1443079294139018e-06, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9594838472018268}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:29:05,394][0m Trial 3 finished with value: 0.21820037942225493 and parameters: {'observation_period_num': 36, 'train_rates': 0.627812153819506, 'learning_rate': 0.00014153134498620404, 'batch_size': 247, 'step_size': 5, 'gamma': 0.7655390530212549}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:30:04,620][0m Trial 4 finished with value: 0.34526472643357653 and parameters: {'observation_period_num': 17, 'train_rates': 0.7166593832563553, 'learning_rate': 2.649498421481278e-06, 'batch_size': 86, 'step_size': 13, 'gamma': 0.9654153120608918}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:31:25,956][0m Trial 5 finished with value: 0.17723579922764723 and parameters: {'observation_period_num': 219, 'train_rates': 0.7878529797615201, 'learning_rate': 0.0002139204938253474, 'batch_size': 60, 'step_size': 9, 'gamma': 0.9226406760406307}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:32:27,690][0m Trial 6 finished with value: 0.22557414073766793 and parameters: {'observation_period_num': 73, 'train_rates': 0.6453213722377296, 'learning_rate': 0.0009874826972359934, 'batch_size': 76, 'step_size': 13, 'gamma': 0.9671828180768586}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:32:53,382][0m Trial 7 finished with value: 0.40943545710582 and parameters: {'observation_period_num': 230, 'train_rates': 0.9047125016881421, 'learning_rate': 2.2927006019553646e-05, 'batch_size': 241, 'step_size': 3, 'gamma': 0.9370969153431387}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:33:42,296][0m Trial 8 finished with value: 1.3352857103625548 and parameters: {'observation_period_num': 40, 'train_rates': 0.6465891552461732, 'learning_rate': 1.0324947379287303e-06, 'batch_size': 96, 'step_size': 10, 'gamma': 0.9219098548375771}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:34:12,397][0m Trial 9 finished with value: 0.46051216075758544 and parameters: {'observation_period_num': 219, 'train_rates': 0.6932766082465945, 'learning_rate': 4.250841210622469e-05, 'batch_size': 161, 'step_size': 11, 'gamma': 0.7546505114556302}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:38:31,307][0m Trial 10 finished with value: 0.11188936903899088 and parameters: {'observation_period_num': 135, 'train_rates': 0.8481207101277214, 'learning_rate': 8.09325202768673e-06, 'batch_size': 20, 'step_size': 15, 'gamma': 0.8271108475910558}. Best is trial 1 with value: 0.09819695999224981.[0m
[32m[I 2025-02-04 21:43:50,986][0m Trial 11 finished with value: 0.0921673525279669 and parameters: {'observation_period_num': 134, 'train_rates': 0.8382118014484107, 'learning_rate': 1.1795440251915417e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8290629173859886}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:48:46,146][0m Trial 12 finished with value: 0.11873844091314822 and parameters: {'observation_period_num': 139, 'train_rates': 0.8865974325089638, 'learning_rate': 9.618486494019936e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8365635025642784}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:49:22,397][0m Trial 13 finished with value: 0.5159040360980564 and parameters: {'observation_period_num': 92, 'train_rates': 0.7644927477788023, 'learning_rate': 7.891137482217008e-06, 'batch_size': 146, 'step_size': 12, 'gamma': 0.8132599615672853}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:50:12,180][0m Trial 14 finished with value: 0.24410836135197167 and parameters: {'observation_period_num': 174, 'train_rates': 0.9291088296533547, 'learning_rate': 2.544171511223262e-05, 'batch_size': 119, 'step_size': 15, 'gamma': 0.8814116947676105}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:50:43,200][0m Trial 15 finished with value: 0.5746082670057368 and parameters: {'observation_period_num': 174, 'train_rates': 0.8356668820837654, 'learning_rate': 4.974695106577816e-06, 'batch_size': 186, 'step_size': 8, 'gamma': 0.8690838733822089}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:53:05,482][0m Trial 16 finished with value: 0.2161246989257329 and parameters: {'observation_period_num': 97, 'train_rates': 0.7434143326031757, 'learning_rate': 6.522960549000451e-05, 'batch_size': 34, 'step_size': 13, 'gamma': 0.8011549748338026}. Best is trial 11 with value: 0.0921673525279669.[0m
Early stopping at epoch 75
[32m[I 2025-02-04 21:53:41,457][0m Trial 17 finished with value: 0.721636993644383 and parameters: {'observation_period_num': 167, 'train_rates': 0.8474934761748399, 'learning_rate': 1.4622769985659562e-05, 'batch_size': 118, 'step_size': 1, 'gamma': 0.8583232416498202}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:54:13,813][0m Trial 18 finished with value: 0.8896145820617676 and parameters: {'observation_period_num': 65, 'train_rates': 0.9556150169583426, 'learning_rate': 3.286320405641557e-06, 'batch_size': 207, 'step_size': 11, 'gamma': 0.7963721179782761}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:56:52,591][0m Trial 19 finished with value: 0.2116968119062665 and parameters: {'observation_period_num': 118, 'train_rates': 0.8844626723553457, 'learning_rate': 0.00011123744989617574, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8915166093992761}. Best is trial 11 with value: 0.0921673525279669.[0m
[32m[I 2025-02-04 21:57:49,229][0m Trial 20 finished with value: 0.03221534891544204 and parameters: {'observation_period_num': 9, 'train_rates': 0.806524416648855, 'learning_rate': 0.00048041640231690186, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8413088336278589}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 21:58:43,920][0m Trial 21 finished with value: 0.032399923872688544 and parameters: {'observation_period_num': 9, 'train_rates': 0.8050796147844328, 'learning_rate': 0.0006707545210852664, 'batch_size': 102, 'step_size': 7, 'gamma': 0.8454916330082282}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 21:59:33,048][0m Trial 22 finished with value: 0.18980530388881825 and parameters: {'observation_period_num': 48, 'train_rates': 0.7758950756576267, 'learning_rate': 0.0009950510767826619, 'batch_size': 112, 'step_size': 6, 'gamma': 0.8451591358567571}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:00:12,219][0m Trial 23 finished with value: 0.14983955463942358 and parameters: {'observation_period_num': 10, 'train_rates': 0.7287134550932182, 'learning_rate': 0.00034219672924466364, 'batch_size': 141, 'step_size': 7, 'gamma': 0.8224522491442806}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:01:08,109][0m Trial 24 finished with value: 0.09073079092715813 and parameters: {'observation_period_num': 113, 'train_rates': 0.8704443655639696, 'learning_rate': 0.0004500253507065138, 'batch_size': 102, 'step_size': 3, 'gamma': 0.840864540341031}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:02:02,213][0m Trial 25 finished with value: 0.06623261466801884 and parameters: {'observation_period_num': 63, 'train_rates': 0.8854930273031707, 'learning_rate': 0.00044836506146788004, 'batch_size': 107, 'step_size': 3, 'gamma': 0.9021379906128209}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:02:38,225][0m Trial 26 finished with value: 0.04469841140558116 and parameters: {'observation_period_num': 29, 'train_rates': 0.9178622259617981, 'learning_rate': 0.000516091845031033, 'batch_size': 170, 'step_size': 3, 'gamma': 0.9145032194512913}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:03:16,574][0m Trial 27 finished with value: 0.0488225482404232 and parameters: {'observation_period_num': 29, 'train_rates': 0.9109029244149038, 'learning_rate': 0.00025955615970188684, 'batch_size': 163, 'step_size': 5, 'gamma': 0.8577701005245454}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:03:43,714][0m Trial 28 finished with value: 0.1723105934354177 and parameters: {'observation_period_num': 6, 'train_rates': 0.6920335007972083, 'learning_rate': 0.0006142687853267852, 'batch_size': 195, 'step_size': 1, 'gamma': 0.9088518345726282}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:04:23,498][0m Trial 29 finished with value: 0.07710075378417969 and parameters: {'observation_period_num': 52, 'train_rates': 0.9892280377340461, 'learning_rate': 0.00014302566122997268, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9858190460301123}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:05:11,914][0m Trial 30 finished with value: 0.03596301619998283 and parameters: {'observation_period_num': 24, 'train_rates': 0.9510150826567945, 'learning_rate': 0.0006442739380017691, 'batch_size': 130, 'step_size': 4, 'gamma': 0.8815265236408935}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:06:00,921][0m Trial 31 finished with value: 0.03928545139291707 and parameters: {'observation_period_num': 25, 'train_rates': 0.9538209939851084, 'learning_rate': 0.0006400219069738092, 'batch_size': 131, 'step_size': 4, 'gamma': 0.8801808097261571}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:06:49,893][0m Trial 32 finished with value: 0.03376677632331848 and parameters: {'observation_period_num': 21, 'train_rates': 0.9574410801584203, 'learning_rate': 0.0007082230223442395, 'batch_size': 131, 'step_size': 4, 'gamma': 0.875102959437061}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:07:57,332][0m Trial 33 finished with value: 0.03341912081929938 and parameters: {'observation_period_num': 5, 'train_rates': 0.8141629746244975, 'learning_rate': 0.00028678773955274927, 'batch_size': 82, 'step_size': 6, 'gamma': 0.8571638330101116}. Best is trial 20 with value: 0.03221534891544204.[0m
[32m[I 2025-02-04 22:09:08,828][0m Trial 34 finished with value: 0.031046665196276742 and parameters: {'observation_period_num': 8, 'train_rates': 0.8120046948615617, 'learning_rate': 0.00022611573946750024, 'batch_size': 76, 'step_size': 7, 'gamma': 0.8628241845093478}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:10:20,697][0m Trial 35 finished with value: 0.03261034245438436 and parameters: {'observation_period_num': 6, 'train_rates': 0.8097182476415922, 'learning_rate': 0.00023095108904466885, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8548216558877931}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:12:01,345][0m Trial 36 finished with value: 0.04597219749000566 and parameters: {'observation_period_num': 40, 'train_rates': 0.8012273994762201, 'learning_rate': 0.00017547886168801076, 'batch_size': 52, 'step_size': 8, 'gamma': 0.7796061736034288}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:13:07,754][0m Trial 37 finished with value: 0.3339249596354507 and parameters: {'observation_period_num': 251, 'train_rates': 0.7514147637151503, 'learning_rate': 9.753749513169745e-05, 'batch_size': 71, 'step_size': 9, 'gamma': 0.8466912104448059}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:14:07,228][0m Trial 38 finished with value: 0.07960470488494868 and parameters: {'observation_period_num': 80, 'train_rates': 0.7958028562228797, 'learning_rate': 7.849265554507384e-05, 'batch_size': 89, 'step_size': 7, 'gamma': 0.8074458390621615}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:15:36,194][0m Trial 39 finished with value: 0.06690870123366786 and parameters: {'observation_period_num': 53, 'train_rates': 0.8240590950813745, 'learning_rate': 0.00035683743474077456, 'batch_size': 60, 'step_size': 6, 'gamma': 0.8628807005370336}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:17:30,472][0m Trial 40 finished with value: 0.16572982624860738 and parameters: {'observation_period_num': 15, 'train_rates': 0.7753975722606731, 'learning_rate': 0.00020323380030341142, 'batch_size': 45, 'step_size': 9, 'gamma': 0.8961110138918965}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:18:36,529][0m Trial 41 finished with value: 0.03501741069045225 and parameters: {'observation_period_num': 5, 'train_rates': 0.810950571611853, 'learning_rate': 0.00023987123044653835, 'batch_size': 85, 'step_size': 6, 'gamma': 0.8558389928477537}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:19:52,494][0m Trial 42 finished with value: 0.06314551493798817 and parameters: {'observation_period_num': 40, 'train_rates': 0.8635603263367911, 'learning_rate': 0.00031328785743195615, 'batch_size': 75, 'step_size': 8, 'gamma': 0.8167777935742666}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:20:53,081][0m Trial 43 finished with value: 0.05923824184215986 and parameters: {'observation_period_num': 15, 'train_rates': 0.823169039382782, 'learning_rate': 5.564679938658373e-05, 'batch_size': 92, 'step_size': 5, 'gamma': 0.8489282352678037}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:22:10,721][0m Trial 44 finished with value: 0.18449105795833373 and parameters: {'observation_period_num': 33, 'train_rates': 0.7879490182971718, 'learning_rate': 0.0001431344415368041, 'batch_size': 69, 'step_size': 7, 'gamma': 0.8333871012719609}. Best is trial 34 with value: 0.031046665196276742.[0m
[32m[I 2025-02-04 22:23:17,120][0m Trial 45 finished with value: 0.02936860054731369 and parameters: {'observation_period_num': 6, 'train_rates': 0.8221145512427499, 'learning_rate': 0.0007935635857708477, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8715651331579344}. Best is trial 45 with value: 0.02936860054731369.[0m
[32m[I 2025-02-04 22:25:07,255][0m Trial 46 finished with value: 0.17445081448793867 and parameters: {'observation_period_num': 17, 'train_rates': 0.7770227155400238, 'learning_rate': 0.000800851654837764, 'batch_size': 47, 'step_size': 5, 'gamma': 0.9370434737532747}. Best is trial 45 with value: 0.02936860054731369.[0m
[32m[I 2025-02-04 22:25:55,945][0m Trial 47 finished with value: 0.3193799817740028 and parameters: {'observation_period_num': 200, 'train_rates': 0.7568522604052241, 'learning_rate': 0.00042592748196133953, 'batch_size': 100, 'step_size': 8, 'gamma': 0.8699279683987324}. Best is trial 45 with value: 0.02936860054731369.[0m
[32m[I 2025-02-04 22:27:26,943][0m Trial 48 finished with value: 0.12280570283219532 and parameters: {'observation_period_num': 46, 'train_rates': 0.8525792581213406, 'learning_rate': 0.0008878212516294622, 'batch_size': 61, 'step_size': 10, 'gamma': 0.8902399226353178}. Best is trial 45 with value: 0.02936860054731369.[0m
[32m[I 2025-02-04 22:28:15,914][0m Trial 49 finished with value: 0.11329093375619817 and parameters: {'observation_period_num': 62, 'train_rates': 0.8249627267905334, 'learning_rate': 3.8576066996914714e-05, 'batch_size': 119, 'step_size': 9, 'gamma': 0.8304982337658795}. Best is trial 45 with value: 0.02936860054731369.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3734 | 0.1458
Epoch 2/300, Loss: 0.1436 | 0.1017
Epoch 3/300, Loss: 0.1307 | 0.0869
Epoch 4/300, Loss: 0.1283 | 0.0812
Epoch 5/300, Loss: 0.1308 | 0.1062
Epoch 6/300, Loss: 0.1325 | 0.1238
Epoch 7/300, Loss: 0.1360 | 0.2071
Epoch 8/300, Loss: 0.1147 | 0.0967
Epoch 9/300, Loss: 0.1090 | 0.0695
Epoch 10/300, Loss: 0.1182 | 0.0951
Epoch 11/300, Loss: 0.1061 | 0.0597
Epoch 12/300, Loss: 0.0963 | 0.0525
Epoch 13/300, Loss: 0.1046 | 0.0624
Epoch 14/300, Loss: 0.0967 | 0.0534
Epoch 15/300, Loss: 0.0903 | 0.0573
Epoch 16/300, Loss: 0.0901 | 0.0519
Epoch 17/300, Loss: 0.0864 | 0.0470
Epoch 18/300, Loss: 0.0842 | 0.0483
Epoch 19/300, Loss: 0.0830 | 0.0440
Epoch 20/300, Loss: 0.0811 | 0.0452
Epoch 21/300, Loss: 0.0808 | 0.0432
Epoch 22/300, Loss: 0.0808 | 0.0422
Epoch 23/300, Loss: 0.0784 | 0.0399
Epoch 24/300, Loss: 0.0778 | 0.0397
Epoch 25/300, Loss: 0.0772 | 0.0393
Epoch 26/300, Loss: 0.0765 | 0.0389
Epoch 27/300, Loss: 0.0761 | 0.0387
Epoch 28/300, Loss: 0.0766 | 0.0387
Epoch 29/300, Loss: 0.0787 | 0.0394
Epoch 30/300, Loss: 0.0804 | 0.0391
Epoch 31/300, Loss: 0.0797 | 0.0381
Epoch 32/300, Loss: 0.0783 | 0.0389
Epoch 33/300, Loss: 0.0767 | 0.0379
Epoch 34/300, Loss: 0.0758 | 0.0380
Epoch 35/300, Loss: 0.0754 | 0.0372
Epoch 36/300, Loss: 0.0745 | 0.0373
Epoch 37/300, Loss: 0.0738 | 0.0374
Epoch 38/300, Loss: 0.0734 | 0.0369
Epoch 39/300, Loss: 0.0733 | 0.0371
Epoch 40/300, Loss: 0.0734 | 0.0369
Epoch 41/300, Loss: 0.0733 | 0.0367
Epoch 42/300, Loss: 0.0728 | 0.0366
Epoch 43/300, Loss: 0.0724 | 0.0369
Epoch 44/300, Loss: 0.0722 | 0.0366
Epoch 45/300, Loss: 0.0717 | 0.0362
Epoch 46/300, Loss: 0.0716 | 0.0359
Epoch 47/300, Loss: 0.0719 | 0.0353
Epoch 48/300, Loss: 0.0714 | 0.0351
Epoch 49/300, Loss: 0.0713 | 0.0350
Epoch 50/300, Loss: 0.0711 | 0.0349
Epoch 51/300, Loss: 0.0709 | 0.0350
Epoch 52/300, Loss: 0.0707 | 0.0350
Epoch 53/300, Loss: 0.0706 | 0.0350
Epoch 54/300, Loss: 0.0703 | 0.0349
Epoch 55/300, Loss: 0.0701 | 0.0348
Epoch 56/300, Loss: 0.0699 | 0.0348
Epoch 57/300, Loss: 0.0696 | 0.0347
Epoch 58/300, Loss: 0.0694 | 0.0347
Epoch 59/300, Loss: 0.0692 | 0.0347
Epoch 60/300, Loss: 0.0690 | 0.0346
Epoch 61/300, Loss: 0.0688 | 0.0347
Epoch 62/300, Loss: 0.0688 | 0.0348
Epoch 63/300, Loss: 0.0686 | 0.0348
Epoch 64/300, Loss: 0.0684 | 0.0347
Epoch 65/300, Loss: 0.0682 | 0.0346
Epoch 66/300, Loss: 0.0679 | 0.0345
Epoch 67/300, Loss: 0.0677 | 0.0344
Epoch 68/300, Loss: 0.0675 | 0.0343
Epoch 69/300, Loss: 0.0674 | 0.0342
Epoch 70/300, Loss: 0.0673 | 0.0341
Epoch 71/300, Loss: 0.0672 | 0.0340
Epoch 72/300, Loss: 0.0671 | 0.0339
Epoch 73/300, Loss: 0.0670 | 0.0338
Epoch 74/300, Loss: 0.0670 | 0.0337
Epoch 75/300, Loss: 0.0670 | 0.0337
Epoch 76/300, Loss: 0.0669 | 0.0336
Epoch 77/300, Loss: 0.0668 | 0.0335
Epoch 78/300, Loss: 0.0667 | 0.0334
Epoch 79/300, Loss: 0.0665 | 0.0333
Epoch 80/300, Loss: 0.0664 | 0.0332
Epoch 81/300, Loss: 0.0663 | 0.0331
Epoch 82/300, Loss: 0.0662 | 0.0330
Epoch 83/300, Loss: 0.0662 | 0.0330
Epoch 84/300, Loss: 0.0661 | 0.0329
Epoch 85/300, Loss: 0.0661 | 0.0328
Epoch 86/300, Loss: 0.0661 | 0.0328
Epoch 87/300, Loss: 0.0660 | 0.0328
Epoch 88/300, Loss: 0.0659 | 0.0328
Epoch 89/300, Loss: 0.0658 | 0.0328
Epoch 90/300, Loss: 0.0657 | 0.0327
Epoch 91/300, Loss: 0.0657 | 0.0327
Epoch 92/300, Loss: 0.0656 | 0.0327
Epoch 93/300, Loss: 0.0656 | 0.0327
Epoch 94/300, Loss: 0.0655 | 0.0326
Epoch 95/300, Loss: 0.0655 | 0.0326
Epoch 96/300, Loss: 0.0655 | 0.0326
Epoch 97/300, Loss: 0.0654 | 0.0325
Epoch 98/300, Loss: 0.0654 | 0.0325
Epoch 99/300, Loss: 0.0654 | 0.0325
Epoch 100/300, Loss: 0.0653 | 0.0324
Epoch 101/300, Loss: 0.0653 | 0.0324
Epoch 102/300, Loss: 0.0653 | 0.0324
Epoch 103/300, Loss: 0.0652 | 0.0324
Epoch 104/300, Loss: 0.0652 | 0.0323
Epoch 105/300, Loss: 0.0652 | 0.0323
Epoch 106/300, Loss: 0.0651 | 0.0323
Epoch 107/300, Loss: 0.0651 | 0.0323
Epoch 108/300, Loss: 0.0651 | 0.0323
Epoch 109/300, Loss: 0.0651 | 0.0322
Epoch 110/300, Loss: 0.0651 | 0.0322
Epoch 111/300, Loss: 0.0650 | 0.0322
Epoch 112/300, Loss: 0.0650 | 0.0322
Epoch 113/300, Loss: 0.0650 | 0.0322
Epoch 114/300, Loss: 0.0650 | 0.0322
Epoch 115/300, Loss: 0.0650 | 0.0321
Epoch 116/300, Loss: 0.0649 | 0.0321
Epoch 117/300, Loss: 0.0649 | 0.0321
Epoch 118/300, Loss: 0.0649 | 0.0321
Epoch 119/300, Loss: 0.0649 | 0.0321
Epoch 120/300, Loss: 0.0649 | 0.0321
Epoch 121/300, Loss: 0.0649 | 0.0321
Epoch 122/300, Loss: 0.0649 | 0.0321
Epoch 123/300, Loss: 0.0648 | 0.0320
Epoch 124/300, Loss: 0.0648 | 0.0320
Epoch 125/300, Loss: 0.0648 | 0.0320
Epoch 126/300, Loss: 0.0648 | 0.0320
Epoch 127/300, Loss: 0.0648 | 0.0320
Epoch 128/300, Loss: 0.0648 | 0.0320
Epoch 129/300, Loss: 0.0648 | 0.0320
Epoch 130/300, Loss: 0.0648 | 0.0320
Epoch 131/300, Loss: 0.0648 | 0.0320
Epoch 132/300, Loss: 0.0648 | 0.0320
Epoch 133/300, Loss: 0.0647 | 0.0320
Epoch 134/300, Loss: 0.0647 | 0.0319
Epoch 135/300, Loss: 0.0647 | 0.0319
Epoch 136/300, Loss: 0.0647 | 0.0319
Epoch 137/300, Loss: 0.0647 | 0.0319
Epoch 138/300, Loss: 0.0647 | 0.0319
Epoch 139/300, Loss: 0.0647 | 0.0319
Epoch 140/300, Loss: 0.0647 | 0.0319
Epoch 141/300, Loss: 0.0647 | 0.0319
Epoch 142/300, Loss: 0.0647 | 0.0319
Epoch 143/300, Loss: 0.0647 | 0.0319
Epoch 144/300, Loss: 0.0647 | 0.0319
Epoch 145/300, Loss: 0.0647 | 0.0319
Epoch 146/300, Loss: 0.0647 | 0.0319
Epoch 147/300, Loss: 0.0647 | 0.0319
Epoch 148/300, Loss: 0.0647 | 0.0319
Epoch 149/300, Loss: 0.0646 | 0.0319
Epoch 150/300, Loss: 0.0646 | 0.0319
Epoch 151/300, Loss: 0.0646 | 0.0319
Epoch 152/300, Loss: 0.0646 | 0.0319
Epoch 153/300, Loss: 0.0646 | 0.0319
Epoch 154/300, Loss: 0.0646 | 0.0319
Epoch 155/300, Loss: 0.0646 | 0.0319
Epoch 156/300, Loss: 0.0646 | 0.0318
Epoch 157/300, Loss: 0.0646 | 0.0318
Epoch 158/300, Loss: 0.0646 | 0.0318
Epoch 159/300, Loss: 0.0646 | 0.0318
Epoch 160/300, Loss: 0.0646 | 0.0318
Epoch 161/300, Loss: 0.0646 | 0.0318
Epoch 162/300, Loss: 0.0646 | 0.0318
Epoch 163/300, Loss: 0.0646 | 0.0318
Epoch 164/300, Loss: 0.0646 | 0.0318
Epoch 165/300, Loss: 0.0646 | 0.0318
Epoch 166/300, Loss: 0.0646 | 0.0318
Epoch 167/300, Loss: 0.0646 | 0.0318
Epoch 168/300, Loss: 0.0646 | 0.0318
Epoch 169/300, Loss: 0.0646 | 0.0318
Epoch 170/300, Loss: 0.0646 | 0.0318
Epoch 171/300, Loss: 0.0646 | 0.0318
Epoch 172/300, Loss: 0.0646 | 0.0318
Epoch 173/300, Loss: 0.0646 | 0.0318
Epoch 174/300, Loss: 0.0646 | 0.0318
Epoch 175/300, Loss: 0.0646 | 0.0318
Epoch 176/300, Loss: 0.0646 | 0.0318
Epoch 177/300, Loss: 0.0646 | 0.0318
Epoch 178/300, Loss: 0.0646 | 0.0318
Epoch 179/300, Loss: 0.0646 | 0.0318
Epoch 180/300, Loss: 0.0646 | 0.0318
Epoch 181/300, Loss: 0.0646 | 0.0318
Epoch 182/300, Loss: 0.0646 | 0.0318
Epoch 183/300, Loss: 0.0646 | 0.0318
Epoch 184/300, Loss: 0.0646 | 0.0318
Epoch 185/300, Loss: 0.0646 | 0.0318
Epoch 186/300, Loss: 0.0646 | 0.0318
Epoch 187/300, Loss: 0.0646 | 0.0318
Epoch 188/300, Loss: 0.0646 | 0.0318
Epoch 189/300, Loss: 0.0646 | 0.0318
Epoch 190/300, Loss: 0.0646 | 0.0318
Epoch 191/300, Loss: 0.0646 | 0.0318
Epoch 192/300, Loss: 0.0646 | 0.0318
Epoch 193/300, Loss: 0.0646 | 0.0318
Epoch 194/300, Loss: 0.0646 | 0.0318
Epoch 195/300, Loss: 0.0646 | 0.0318
Epoch 196/300, Loss: 0.0646 | 0.0318
Epoch 197/300, Loss: 0.0646 | 0.0318
Epoch 198/300, Loss: 0.0646 | 0.0318
Epoch 199/300, Loss: 0.0646 | 0.0318
Epoch 200/300, Loss: 0.0646 | 0.0318
Epoch 201/300, Loss: 0.0646 | 0.0318
Epoch 202/300, Loss: 0.0646 | 0.0318
Epoch 203/300, Loss: 0.0646 | 0.0318
Epoch 204/300, Loss: 0.0646 | 0.0318
Epoch 205/300, Loss: 0.0646 | 0.0318
Epoch 206/300, Loss: 0.0646 | 0.0318
Epoch 207/300, Loss: 0.0646 | 0.0318
Epoch 208/300, Loss: 0.0646 | 0.0318
Epoch 209/300, Loss: 0.0646 | 0.0318
Epoch 210/300, Loss: 0.0646 | 0.0318
Epoch 211/300, Loss: 0.0646 | 0.0318
Epoch 212/300, Loss: 0.0646 | 0.0318
Epoch 213/300, Loss: 0.0646 | 0.0318
Epoch 214/300, Loss: 0.0646 | 0.0318
Epoch 215/300, Loss: 0.0646 | 0.0318
Epoch 216/300, Loss: 0.0646 | 0.0318
Epoch 217/300, Loss: 0.0646 | 0.0318
Epoch 218/300, Loss: 0.0646 | 0.0318
Epoch 219/300, Loss: 0.0646 | 0.0318
Epoch 220/300, Loss: 0.0646 | 0.0318
Epoch 221/300, Loss: 0.0646 | 0.0318
Epoch 222/300, Loss: 0.0646 | 0.0318
Epoch 223/300, Loss: 0.0646 | 0.0318
Epoch 224/300, Loss: 0.0646 | 0.0318
Epoch 225/300, Loss: 0.0646 | 0.0318
Epoch 226/300, Loss: 0.0646 | 0.0318
Epoch 227/300, Loss: 0.0646 | 0.0318
Epoch 228/300, Loss: 0.0646 | 0.0318
Epoch 229/300, Loss: 0.0646 | 0.0318
Epoch 230/300, Loss: 0.0646 | 0.0318
Epoch 231/300, Loss: 0.0646 | 0.0318
Epoch 232/300, Loss: 0.0646 | 0.0318
Epoch 233/300, Loss: 0.0646 | 0.0318
Epoch 234/300, Loss: 0.0646 | 0.0318
Epoch 235/300, Loss: 0.0646 | 0.0318
Epoch 236/300, Loss: 0.0646 | 0.0318
Epoch 237/300, Loss: 0.0646 | 0.0318
Epoch 238/300, Loss: 0.0646 | 0.0318
Epoch 239/300, Loss: 0.0646 | 0.0318
Epoch 240/300, Loss: 0.0646 | 0.0318
Epoch 241/300, Loss: 0.0646 | 0.0318
Epoch 242/300, Loss: 0.0646 | 0.0318
Epoch 243/300, Loss: 0.0646 | 0.0318
Epoch 244/300, Loss: 0.0646 | 0.0318
Epoch 245/300, Loss: 0.0646 | 0.0318
Epoch 246/300, Loss: 0.0646 | 0.0318
Epoch 247/300, Loss: 0.0646 | 0.0318
Epoch 248/300, Loss: 0.0646 | 0.0318
Epoch 249/300, Loss: 0.0646 | 0.0318
Epoch 250/300, Loss: 0.0646 | 0.0318
Epoch 251/300, Loss: 0.0646 | 0.0318
Epoch 252/300, Loss: 0.0646 | 0.0318
Epoch 253/300, Loss: 0.0645 | 0.0318
Epoch 254/300, Loss: 0.0645 | 0.0318
Epoch 255/300, Loss: 0.0645 | 0.0318
Epoch 256/300, Loss: 0.0645 | 0.0318
Epoch 257/300, Loss: 0.0645 | 0.0318
Epoch 258/300, Loss: 0.0645 | 0.0318
Epoch 259/300, Loss: 0.0645 | 0.0318
Epoch 260/300, Loss: 0.0645 | 0.0318
Epoch 261/300, Loss: 0.0645 | 0.0318
Epoch 262/300, Loss: 0.0645 | 0.0318
Epoch 263/300, Loss: 0.0645 | 0.0318
Epoch 264/300, Loss: 0.0645 | 0.0318
Epoch 265/300, Loss: 0.0645 | 0.0318
Epoch 266/300, Loss: 0.0645 | 0.0318
Epoch 267/300, Loss: 0.0645 | 0.0318
Epoch 268/300, Loss: 0.0645 | 0.0318
Epoch 269/300, Loss: 0.0645 | 0.0318
Epoch 270/300, Loss: 0.0645 | 0.0318
Epoch 271/300, Loss: 0.0645 | 0.0318
Epoch 272/300, Loss: 0.0645 | 0.0318
Epoch 273/300, Loss: 0.0645 | 0.0318
Epoch 274/300, Loss: 0.0645 | 0.0318
Epoch 275/300, Loss: 0.0645 | 0.0318
Epoch 276/300, Loss: 0.0645 | 0.0318
Epoch 277/300, Loss: 0.0645 | 0.0318
Epoch 278/300, Loss: 0.0645 | 0.0318
Epoch 279/300, Loss: 0.0645 | 0.0318
Epoch 280/300, Loss: 0.0645 | 0.0318
Epoch 281/300, Loss: 0.0645 | 0.0318
Epoch 282/300, Loss: 0.0645 | 0.0318
Epoch 283/300, Loss: 0.0645 | 0.0318
Epoch 284/300, Loss: 0.0645 | 0.0318
Epoch 285/300, Loss: 0.0645 | 0.0318
Epoch 286/300, Loss: 0.0645 | 0.0318
Epoch 287/300, Loss: 0.0645 | 0.0318
Epoch 288/300, Loss: 0.0645 | 0.0318
Epoch 289/300, Loss: 0.0645 | 0.0318
Epoch 290/300, Loss: 0.0645 | 0.0318
Epoch 291/300, Loss: 0.0645 | 0.0318
Epoch 292/300, Loss: 0.0645 | 0.0318
Epoch 293/300, Loss: 0.0645 | 0.0318
Epoch 294/300, Loss: 0.0645 | 0.0318
Epoch 295/300, Loss: 0.0645 | 0.0318
Epoch 296/300, Loss: 0.0645 | 0.0318
Epoch 297/300, Loss: 0.0645 | 0.0318
Epoch 298/300, Loss: 0.0645 | 0.0318
Epoch 299/300, Loss: 0.0645 | 0.0318
Epoch 300/300, Loss: 0.0645 | 0.0318
Runtime (seconds): 196.98167943954468
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 9.071828536922112
RMSE: 3.0119476318359375
MAE: 3.0119476318359375
R-squared: nan
[167.28485]
