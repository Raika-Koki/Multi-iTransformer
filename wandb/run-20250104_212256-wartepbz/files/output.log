ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 21:22:57,735][0m A new study created in memory with name: no-name-09d11d98-6cc4-4eb0-8d9b-60d5f86bab77[0m
[32m[I 2025-01-04 21:23:23,200][0m Trial 0 finished with value: 0.06852266811626159 and parameters: {'observation_period_num': 76, 'train_rates': 0.8727773584006286, 'learning_rate': 0.00012326923354935202, 'batch_size': 236, 'step_size': 13, 'gamma': 0.94705835589062}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:23:47,256][0m Trial 1 finished with value: 0.16753705063971078 and parameters: {'observation_period_num': 239, 'train_rates': 0.7885561238785082, 'learning_rate': 7.072245923718137e-05, 'batch_size': 217, 'step_size': 9, 'gamma': 0.9337562468464325}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:26:25,839][0m Trial 2 finished with value: 0.13074376854120484 and parameters: {'observation_period_num': 221, 'train_rates': 0.945632828519607, 'learning_rate': 0.0002848588004114732, 'batch_size': 34, 'step_size': 8, 'gamma': 0.9203052009369503}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:28:20,902][0m Trial 3 finished with value: 0.12342371016295038 and parameters: {'observation_period_num': 88, 'train_rates': 0.8367724899857996, 'learning_rate': 0.00032718884170024174, 'batch_size': 46, 'step_size': 13, 'gamma': 0.9556719791194634}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:29:10,703][0m Trial 4 finished with value: 0.1252434355020523 and parameters: {'observation_period_num': 231, 'train_rates': 0.7796673677880663, 'learning_rate': 0.00012252197515641902, 'batch_size': 99, 'step_size': 11, 'gamma': 0.9817256186787062}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:29:39,454][0m Trial 5 finished with value: 0.6731655378243179 and parameters: {'observation_period_num': 65, 'train_rates': 0.7151113941522402, 'learning_rate': 4.192405223482656e-06, 'batch_size': 182, 'step_size': 6, 'gamma': 0.7669909746531492}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:31:17,239][0m Trial 6 finished with value: 0.30074838408663696 and parameters: {'observation_period_num': 197, 'train_rates': 0.7167295521955891, 'learning_rate': 9.451892035458234e-06, 'batch_size': 47, 'step_size': 10, 'gamma': 0.8506835412438412}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:32:17,407][0m Trial 7 finished with value: 0.4783755945448844 and parameters: {'observation_period_num': 171, 'train_rates': 0.946058954207285, 'learning_rate': 8.456472932467007e-06, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8752365729869693}. Best is trial 0 with value: 0.06852266811626159.[0m
Early stopping at epoch 60
[32m[I 2025-01-04 21:32:49,360][0m Trial 8 finished with value: 0.2714818925523633 and parameters: {'observation_period_num': 166, 'train_rates': 0.8629550077852222, 'learning_rate': 0.0004277606569596185, 'batch_size': 107, 'step_size': 1, 'gamma': 0.800536686071797}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:33:15,331][0m Trial 9 finished with value: 0.1602607509548511 and parameters: {'observation_period_num': 190, 'train_rates': 0.842392290216134, 'learning_rate': 5.8204456686478456e-05, 'batch_size': 225, 'step_size': 14, 'gamma': 0.8807848770789808}. Best is trial 0 with value: 0.06852266811626159.[0m
[32m[I 2025-01-04 21:33:36,134][0m Trial 10 finished with value: 0.05816311761395413 and parameters: {'observation_period_num': 8, 'train_rates': 0.6297651397791588, 'learning_rate': 0.000962125340791603, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9856623107328568}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:33:57,083][0m Trial 11 finished with value: 0.05960511121684344 and parameters: {'observation_period_num': 8, 'train_rates': 0.611126243728843, 'learning_rate': 0.0009642399531683981, 'batch_size': 255, 'step_size': 15, 'gamma': 0.9774749901250247}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:34:24,965][0m Trial 12 finished with value: 0.06244534540158124 and parameters: {'observation_period_num': 7, 'train_rates': 0.612655123295157, 'learning_rate': 0.0008208731904897409, 'batch_size': 179, 'step_size': 15, 'gamma': 0.9866409588623026}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:34:46,223][0m Trial 13 finished with value: 0.5998518914745857 and parameters: {'observation_period_num': 10, 'train_rates': 0.6011212608809684, 'learning_rate': 1.1345932044609882e-06, 'batch_size': 248, 'step_size': 15, 'gamma': 0.9144151459432894}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:35:13,645][0m Trial 14 finished with value: 0.09042841854349057 and parameters: {'observation_period_num': 47, 'train_rates': 0.6675979405160837, 'learning_rate': 0.000978596105454377, 'batch_size': 181, 'step_size': 12, 'gamma': 0.9863508260050846}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:35:33,526][0m Trial 15 finished with value: 0.128053707591294 and parameters: {'observation_period_num': 109, 'train_rates': 0.6637702197163605, 'learning_rate': 0.0002624672854989128, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8339430847431633}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:36:05,963][0m Trial 16 finished with value: 0.125146459161575 and parameters: {'observation_period_num': 35, 'train_rates': 0.659703922389045, 'learning_rate': 2.9747559083651352e-05, 'batch_size': 152, 'step_size': 11, 'gamma': 0.8959847333067507}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:36:31,946][0m Trial 17 finished with value: 0.147405361587351 and parameters: {'observation_period_num': 114, 'train_rates': 0.7291071528666155, 'learning_rate': 0.0005989620951430648, 'batch_size': 203, 'step_size': 15, 'gamma': 0.9492044503826462}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:37:02,518][0m Trial 18 finished with value: 0.12621092749673798 and parameters: {'observation_period_num': 36, 'train_rates': 0.6280124546701469, 'learning_rate': 0.00016879701514612808, 'batch_size': 154, 'step_size': 5, 'gamma': 0.9650554816559762}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:37:28,589][0m Trial 19 finished with value: 0.2490356328188504 and parameters: {'observation_period_num': 137, 'train_rates': 0.7564884149217851, 'learning_rate': 2.7504074331699138e-05, 'batch_size': 199, 'step_size': 13, 'gamma': 0.9077288822481766}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:37:48,987][0m Trial 20 finished with value: 0.0748784443218741 and parameters: {'observation_period_num': 23, 'train_rates': 0.6798358064676723, 'learning_rate': 0.0005799717969671602, 'batch_size': 256, 'step_size': 2, 'gamma': 0.8193689032576182}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:38:12,368][0m Trial 21 finished with value: 0.06135124747048725 and parameters: {'observation_period_num': 5, 'train_rates': 0.6194152506350322, 'learning_rate': 0.0008656348975396617, 'batch_size': 216, 'step_size': 15, 'gamma': 0.9875339741177445}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:38:35,691][0m Trial 22 finished with value: 0.08655449134837322 and parameters: {'observation_period_num': 57, 'train_rates': 0.6372053300781221, 'learning_rate': 0.0008542478520677577, 'batch_size': 228, 'step_size': 14, 'gamma': 0.9686035065207922}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:39:00,981][0m Trial 23 finished with value: 0.06291879393738699 and parameters: {'observation_period_num': 21, 'train_rates': 0.7014200609313999, 'learning_rate': 0.0003886940354642498, 'batch_size': 212, 'step_size': 12, 'gamma': 0.9335286621253136}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:39:21,888][0m Trial 24 finished with value: 0.0612883242655869 and parameters: {'observation_period_num': 5, 'train_rates': 0.6369110415404258, 'learning_rate': 0.00019411792725660323, 'batch_size': 240, 'step_size': 15, 'gamma': 0.9693176303380286}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:39:42,731][0m Trial 25 finished with value: 0.09113282423071394 and parameters: {'observation_period_num': 42, 'train_rates': 0.6535583547258905, 'learning_rate': 0.0001849893655156215, 'batch_size': 239, 'step_size': 14, 'gamma': 0.9642472811562222}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:40:04,753][0m Trial 26 finished with value: 0.10933477259410096 and parameters: {'observation_period_num': 91, 'train_rates': 0.6827281122909357, 'learning_rate': 6.455868185093345e-05, 'batch_size': 237, 'step_size': 12, 'gamma': 0.9327974846331445}. Best is trial 10 with value: 0.05816311761395413.[0m
[32m[I 2025-01-04 21:40:54,020][0m Trial 27 finished with value: 0.04815401881933212 and parameters: {'observation_period_num': 27, 'train_rates': 0.9860400235314692, 'learning_rate': 0.0004896279427236789, 'batch_size': 127, 'step_size': 14, 'gamma': 0.9679261711164638}. Best is trial 27 with value: 0.04815401881933212.[0m
[32m[I 2025-01-04 21:41:50,049][0m Trial 28 finished with value: 0.03947421535849571 and parameters: {'observation_period_num': 30, 'train_rates': 0.9828437887762521, 'learning_rate': 0.0005348714851893169, 'batch_size': 111, 'step_size': 10, 'gamma': 0.8946998431496879}. Best is trial 28 with value: 0.03947421535849571.[0m
[32m[I 2025-01-04 21:42:41,768][0m Trial 29 finished with value: 0.09090493619441986 and parameters: {'observation_period_num': 65, 'train_rates': 0.9855107730923256, 'learning_rate': 0.00010686267848948268, 'batch_size': 120, 'step_size': 9, 'gamma': 0.8906376041656154}. Best is trial 28 with value: 0.03947421535849571.[0m
[32m[I 2025-01-04 21:43:52,404][0m Trial 30 finished with value: 0.07717381878332659 and parameters: {'observation_period_num': 89, 'train_rates': 0.904118785799136, 'learning_rate': 0.0004954793745992613, 'batch_size': 80, 'step_size': 10, 'gamma': 0.8533294618743147}. Best is trial 28 with value: 0.03947421535849571.[0m
[32m[I 2025-01-04 21:44:35,927][0m Trial 31 finished with value: 0.027797969058156013 and parameters: {'observation_period_num': 27, 'train_rates': 0.9857341957200046, 'learning_rate': 0.0006197761295501707, 'batch_size': 148, 'step_size': 13, 'gamma': 0.9523339235405086}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:45:23,259][0m Trial 32 finished with value: 0.05462011694908142 and parameters: {'observation_period_num': 31, 'train_rates': 0.988587973065226, 'learning_rate': 0.000533227402486095, 'batch_size': 133, 'step_size': 13, 'gamma': 0.944002679279901}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:46:08,639][0m Trial 33 finished with value: 0.04657186567783356 and parameters: {'observation_period_num': 28, 'train_rates': 0.9883702582923986, 'learning_rate': 0.00023234398857956866, 'batch_size': 138, 'step_size': 11, 'gamma': 0.9346456732613876}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:46:52,308][0m Trial 34 finished with value: 0.07477462291717529 and parameters: {'observation_period_num': 74, 'train_rates': 0.9580500941946836, 'learning_rate': 0.0002245761373903904, 'batch_size': 143, 'step_size': 9, 'gamma': 0.9242787662836982}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:48:02,156][0m Trial 35 finished with value: 0.08222138294137564 and parameters: {'observation_period_num': 52, 'train_rates': 0.8974467484586836, 'learning_rate': 0.00035350281541631103, 'batch_size': 83, 'step_size': 11, 'gamma': 0.9002963118960478}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:48:50,558][0m Trial 36 finished with value: 0.04098580280478116 and parameters: {'observation_period_num': 25, 'train_rates': 0.9081789842009429, 'learning_rate': 9.390377111375033e-05, 'batch_size': 125, 'step_size': 8, 'gamma': 0.9467945817423876}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:49:25,872][0m Trial 37 finished with value: 0.13088684502136277 and parameters: {'observation_period_num': 74, 'train_rates': 0.91363063354143, 'learning_rate': 0.0001141910941198024, 'batch_size': 168, 'step_size': 8, 'gamma': 0.9457526136204081}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:50:16,043][0m Trial 38 finished with value: 0.0732617912075992 and parameters: {'observation_period_num': 58, 'train_rates': 0.9267778157701904, 'learning_rate': 3.766482182769246e-05, 'batch_size': 117, 'step_size': 8, 'gamma': 0.9197767532781786}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:50:55,077][0m Trial 39 finished with value: 0.053957365453243256 and parameters: {'observation_period_num': 24, 'train_rates': 0.9577282651441569, 'learning_rate': 8.697811374990599e-05, 'batch_size': 162, 'step_size': 10, 'gamma': 0.8608849930328758}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:52:06,731][0m Trial 40 finished with value: 0.08825962307552497 and parameters: {'observation_period_num': 99, 'train_rates': 0.9664402291010861, 'learning_rate': 4.4432304247886166e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9385516877128038}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:52:52,379][0m Trial 41 finished with value: 0.039141198730852346 and parameters: {'observation_period_num': 46, 'train_rates': 0.9308014689437146, 'learning_rate': 0.0003291955466272763, 'batch_size': 129, 'step_size': 10, 'gamma': 0.954421850237228}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:53:50,510][0m Trial 42 finished with value: 0.04603842666169795 and parameters: {'observation_period_num': 44, 'train_rates': 0.9305921548073307, 'learning_rate': 0.00015275877175344667, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9580501731727523}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:54:46,911][0m Trial 43 finished with value: 0.051983944237069354 and parameters: {'observation_period_num': 65, 'train_rates': 0.8735093196580328, 'learning_rate': 0.00013975403006026785, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9545834992434546}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:56:07,355][0m Trial 44 finished with value: 0.08647163005338775 and parameters: {'observation_period_num': 147, 'train_rates': 0.929482038315228, 'learning_rate': 0.00031274241157668096, 'batch_size': 70, 'step_size': 7, 'gamma': 0.9552211208933344}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:56:56,807][0m Trial 45 finished with value: 0.1535497532959757 and parameters: {'observation_period_num': 46, 'train_rates': 0.8152913577490516, 'learning_rate': 1.477773532854411e-05, 'batch_size': 111, 'step_size': 10, 'gamma': 0.7828681662480305}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:57:44,518][0m Trial 46 finished with value: 0.048024457206443556 and parameters: {'observation_period_num': 39, 'train_rates': 0.9408277811870631, 'learning_rate': 0.00015289863656097887, 'batch_size': 125, 'step_size': 6, 'gamma': 0.9093705277912568}. Best is trial 31 with value: 0.027797969058156013.[0m
[32m[I 2025-01-04 21:59:20,367][0m Trial 47 finished with value: 0.027404077706879443 and parameters: {'observation_period_num': 17, 'train_rates': 0.9697992514871054, 'learning_rate': 0.0002900878964956585, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9284884961277896}. Best is trial 47 with value: 0.027404077706879443.[0m
[32m[I 2025-01-04 22:03:19,139][0m Trial 48 finished with value: 0.02843527903716112 and parameters: {'observation_period_num': 20, 'train_rates': 0.8816675074896604, 'learning_rate': 0.000641569515611755, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8837580839487045}. Best is trial 47 with value: 0.027404077706879443.[0m
[32m[I 2025-01-04 22:07:28,516][0m Trial 49 finished with value: 0.03303684848334703 and parameters: {'observation_period_num': 15, 'train_rates': 0.8783479455754369, 'learning_rate': 0.0006063002022677374, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8824074978620606}. Best is trial 47 with value: 0.027404077706879443.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 22:07:28,529][0m A new study created in memory with name: no-name-67d5625a-eeeb-41ac-908a-6db2f69cdff5[0m
[32m[I 2025-01-04 22:07:59,765][0m Trial 0 finished with value: 0.09348002770978413 and parameters: {'observation_period_num': 104, 'train_rates': 0.906377662965144, 'learning_rate': 0.0008546706218487578, 'batch_size': 195, 'step_size': 12, 'gamma': 0.8607158221808091}. Best is trial 0 with value: 0.09348002770978413.[0m
[32m[I 2025-01-04 22:08:26,411][0m Trial 1 finished with value: 0.11346305161714554 and parameters: {'observation_period_num': 158, 'train_rates': 0.939760710276839, 'learning_rate': 0.00011857931173085758, 'batch_size': 233, 'step_size': 15, 'gamma': 0.9116119074964432}. Best is trial 0 with value: 0.09348002770978413.[0m
[32m[I 2025-01-04 22:09:00,198][0m Trial 2 finished with value: 0.5326134977133378 and parameters: {'observation_period_num': 110, 'train_rates': 0.9194156337983266, 'learning_rate': 3.225312821682748e-06, 'batch_size': 180, 'step_size': 10, 'gamma': 0.8938917381711401}. Best is trial 0 with value: 0.09348002770978413.[0m
[32m[I 2025-01-04 22:10:40,163][0m Trial 3 finished with value: 0.1694282585704149 and parameters: {'observation_period_num': 177, 'train_rates': 0.7096753950122362, 'learning_rate': 0.0004393756810761536, 'batch_size': 45, 'step_size': 6, 'gamma': 0.8156188612663796}. Best is trial 0 with value: 0.09348002770978413.[0m
[32m[I 2025-01-04 22:11:31,535][0m Trial 4 finished with value: 0.5959116807624475 and parameters: {'observation_period_num': 41, 'train_rates': 0.9542956995724963, 'learning_rate': 1.5273494403531487e-06, 'batch_size': 124, 'step_size': 8, 'gamma': 0.9853867924032906}. Best is trial 0 with value: 0.09348002770978413.[0m
[32m[I 2025-01-04 22:11:59,642][0m Trial 5 finished with value: 0.06310068984350113 and parameters: {'observation_period_num': 17, 'train_rates': 0.6272627341900106, 'learning_rate': 0.0006294983061803411, 'batch_size': 170, 'step_size': 5, 'gamma': 0.9215044913164374}. Best is trial 5 with value: 0.06310068984350113.[0m
[32m[I 2025-01-04 22:12:34,611][0m Trial 6 finished with value: 0.2827509045600891 and parameters: {'observation_period_num': 79, 'train_rates': 0.9713945710046358, 'learning_rate': 1.454440962793022e-05, 'batch_size': 175, 'step_size': 10, 'gamma': 0.8114664017683804}. Best is trial 5 with value: 0.06310068984350113.[0m
[32m[I 2025-01-04 22:13:40,614][0m Trial 7 finished with value: 0.4868803672930774 and parameters: {'observation_period_num': 102, 'train_rates': 0.9523315285301117, 'learning_rate': 6.137003775815189e-06, 'batch_size': 90, 'step_size': 5, 'gamma': 0.8434199909103568}. Best is trial 5 with value: 0.06310068984350113.[0m
[32m[I 2025-01-04 22:14:05,730][0m Trial 8 finished with value: 0.14353898725745534 and parameters: {'observation_period_num': 196, 'train_rates': 0.6920901285427833, 'learning_rate': 0.00045215273745733575, 'batch_size': 194, 'step_size': 5, 'gamma': 0.7889419523557901}. Best is trial 5 with value: 0.06310068984350113.[0m
[32m[I 2025-01-04 22:14:36,799][0m Trial 9 finished with value: 0.2231309711933136 and parameters: {'observation_period_num': 216, 'train_rates': 0.9471133413123229, 'learning_rate': 4.785667215332798e-05, 'batch_size': 197, 'step_size': 7, 'gamma': 0.8076153622631146}. Best is trial 5 with value: 0.06310068984350113.[0m
[32m[I 2025-01-04 22:14:58,064][0m Trial 10 finished with value: 0.10368098856966824 and parameters: {'observation_period_num': 8, 'train_rates': 0.6156803495640228, 'learning_rate': 0.00013577924210836812, 'batch_size': 250, 'step_size': 2, 'gamma': 0.945115459202223}. Best is trial 5 with value: 0.06310068984350113.[0m
[32m[I 2025-01-04 22:15:40,615][0m Trial 11 finished with value: 0.04386237384751439 and parameters: {'observation_period_num': 53, 'train_rates': 0.8345344744023372, 'learning_rate': 0.000728322651193019, 'batch_size': 132, 'step_size': 14, 'gamma': 0.7520034280234988}. Best is trial 11 with value: 0.04386237384751439.[0m
Early stopping at epoch 65
[32m[I 2025-01-04 22:16:12,511][0m Trial 12 finished with value: 0.07832379887501399 and parameters: {'observation_period_num': 5, 'train_rates': 0.8200405439497482, 'learning_rate': 0.0001927526373378293, 'batch_size': 118, 'step_size': 1, 'gamma': 0.7541035938988271}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:17:24,818][0m Trial 13 finished with value: 0.05238920585223659 and parameters: {'observation_period_num': 54, 'train_rates': 0.8323195317213963, 'learning_rate': 0.0007287228866301814, 'batch_size': 75, 'step_size': 15, 'gamma': 0.937010114881602}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:18:51,232][0m Trial 14 finished with value: 0.051028077688909344 and parameters: {'observation_period_num': 61, 'train_rates': 0.8393999907266414, 'learning_rate': 5.048129023971215e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9673658236857042}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:22:09,301][0m Trial 15 finished with value: 0.06167410233805717 and parameters: {'observation_period_num': 63, 'train_rates': 0.8773117531434435, 'learning_rate': 3.417948097580097e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.986710224529369}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:23:18,366][0m Trial 16 finished with value: 0.23221037385976484 and parameters: {'observation_period_num': 152, 'train_rates': 0.7729008098873132, 'learning_rate': 1.125596093388073e-05, 'batch_size': 72, 'step_size': 13, 'gamma': 0.8826211064199084}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:24:08,338][0m Trial 17 finished with value: 0.10538290973220553 and parameters: {'observation_period_num': 76, 'train_rates': 0.7518271379282447, 'learning_rate': 5.3114647705938166e-05, 'batch_size': 102, 'step_size': 11, 'gamma': 0.7592527200461326}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:24:42,265][0m Trial 18 finished with value: 0.25277647480368615 and parameters: {'observation_period_num': 251, 'train_rates': 0.8521569366463294, 'learning_rate': 1.4376380389087744e-05, 'batch_size': 154, 'step_size': 14, 'gamma': 0.9648294211228565}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:26:09,359][0m Trial 19 finished with value: 0.06845935807296087 and parameters: {'observation_period_num': 129, 'train_rates': 0.8014710014214789, 'learning_rate': 8.14702314595307e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.8584733481386426}. Best is trial 11 with value: 0.04386237384751439.[0m
[32m[I 2025-01-04 22:30:13,116][0m Trial 20 finished with value: 0.042526244304906154 and parameters: {'observation_period_num': 37, 'train_rates': 0.8632574758771567, 'learning_rate': 0.00022307170156016535, 'batch_size': 22, 'step_size': 13, 'gamma': 0.8344332943440831}. Best is trial 20 with value: 0.042526244304906154.[0m
[32m[I 2025-01-04 22:34:28,112][0m Trial 21 finished with value: 0.03971764276502654 and parameters: {'observation_period_num': 34, 'train_rates': 0.8686001506251398, 'learning_rate': 0.0002574774315877415, 'batch_size': 21, 'step_size': 13, 'gamma': 0.7807297693437681}. Best is trial 21 with value: 0.03971764276502654.[0m
[32m[I 2025-01-04 22:37:24,302][0m Trial 22 finished with value: 0.04895582983931552 and parameters: {'observation_period_num': 32, 'train_rates': 0.8737842776637291, 'learning_rate': 0.0002738096758889175, 'batch_size': 31, 'step_size': 12, 'gamma': 0.7820857155076555}. Best is trial 21 with value: 0.03971764276502654.[0m
[32m[I 2025-01-04 22:42:11,267][0m Trial 23 finished with value: 0.04480189537121491 and parameters: {'observation_period_num': 33, 'train_rates': 0.8870522643416573, 'learning_rate': 0.0002719593056832776, 'batch_size': 19, 'step_size': 12, 'gamma': 0.7798057502751137}. Best is trial 21 with value: 0.03971764276502654.[0m
[32m[I 2025-01-04 22:42:48,682][0m Trial 24 finished with value: 0.06110177532020694 and parameters: {'observation_period_num': 81, 'train_rates': 0.7724818532940212, 'learning_rate': 0.0003419621660493341, 'batch_size': 142, 'step_size': 14, 'gamma': 0.8381099122375685}. Best is trial 21 with value: 0.03971764276502654.[0m
[32m[I 2025-01-04 22:45:13,817][0m Trial 25 finished with value: 0.055720758161969954 and parameters: {'observation_period_num': 30, 'train_rates': 0.8532673499617747, 'learning_rate': 0.00015740775786559933, 'batch_size': 37, 'step_size': 11, 'gamma': 0.7690627966192094}. Best is trial 21 with value: 0.03971764276502654.[0m
[32m[I 2025-01-04 22:50:31,984][0m Trial 26 finished with value: 0.035312081334123974 and parameters: {'observation_period_num': 43, 'train_rates': 0.8099512364710608, 'learning_rate': 0.0008642397151374273, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7956965109571998}. Best is trial 26 with value: 0.035312081334123974.[0m
[32m[I 2025-01-04 22:55:24,481][0m Trial 27 finished with value: 0.1333501013184366 and parameters: {'observation_period_num': 92, 'train_rates': 0.7410730659297208, 'learning_rate': 0.00023738372748256204, 'batch_size': 16, 'step_size': 15, 'gamma': 0.829350541882987}. Best is trial 26 with value: 0.035312081334123974.[0m
[32m[I 2025-01-04 22:57:07,467][0m Trial 28 finished with value: 0.08614643450197637 and parameters: {'observation_period_num': 121, 'train_rates': 0.8055611679164812, 'learning_rate': 0.0009988556900150034, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7974450441143123}. Best is trial 26 with value: 0.035312081334123974.[0m
[32m[I 2025-01-04 22:58:11,777][0m Trial 29 finished with value: 0.04856276817703515 and parameters: {'observation_period_num': 43, 'train_rates': 0.9084776948438222, 'learning_rate': 9.175788248709048e-05, 'batch_size': 90, 'step_size': 11, 'gamma': 0.859481053709018}. Best is trial 26 with value: 0.035312081334123974.[0m
[32m[I 2025-01-04 23:00:19,240][0m Trial 30 finished with value: 0.0392309380341557 and parameters: {'observation_period_num': 21, 'train_rates': 0.8925486816623296, 'learning_rate': 0.0004762723184016009, 'batch_size': 44, 'step_size': 13, 'gamma': 0.8233837905325548}. Best is trial 26 with value: 0.035312081334123974.[0m
[32m[I 2025-01-04 23:02:47,584][0m Trial 31 finished with value: 0.03614676972808961 and parameters: {'observation_period_num': 18, 'train_rates': 0.8947643338645144, 'learning_rate': 0.0005082988178998035, 'batch_size': 38, 'step_size': 13, 'gamma': 0.826420617460032}. Best is trial 26 with value: 0.035312081334123974.[0m
[32m[I 2025-01-04 23:05:08,939][0m Trial 32 finished with value: 0.0316771591703097 and parameters: {'observation_period_num': 18, 'train_rates': 0.9009436540501763, 'learning_rate': 0.00047724735725639497, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8226679451205914}. Best is trial 32 with value: 0.0316771591703097.[0m
[32m[I 2025-01-04 23:07:23,323][0m Trial 33 finished with value: 0.030066130682826042 and parameters: {'observation_period_num': 20, 'train_rates': 0.9864733587814274, 'learning_rate': 0.0005821813535846929, 'batch_size': 45, 'step_size': 15, 'gamma': 0.8179532740970937}. Best is trial 33 with value: 0.030066130682826042.[0m
[32m[I 2025-01-04 23:08:55,742][0m Trial 34 finished with value: 0.04018204286694527 and parameters: {'observation_period_num': 18, 'train_rates': 0.9837843302292621, 'learning_rate': 0.0005414505543224136, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8005125099703659}. Best is trial 33 with value: 0.030066130682826042.[0m
[32m[I 2025-01-04 23:10:47,373][0m Trial 35 finished with value: 0.030039330670500502 and parameters: {'observation_period_num': 5, 'train_rates': 0.9250931976262937, 'learning_rate': 0.0008850397661873524, 'batch_size': 52, 'step_size': 15, 'gamma': 0.8487874204480693}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:11:55,663][0m Trial 36 finished with value: 0.04014430262533755 and parameters: {'observation_period_num': 6, 'train_rates': 0.9216110081745997, 'learning_rate': 0.0009663500499066556, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8813071493469956}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:13:49,661][0m Trial 37 finished with value: 0.06306939888580711 and parameters: {'observation_period_num': 51, 'train_rates': 0.9222963267770804, 'learning_rate': 0.00036603339524704184, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8507331882676812}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:15:33,477][0m Trial 38 finished with value: 0.03926153892628205 and parameters: {'observation_period_num': 24, 'train_rates': 0.9724766354855272, 'learning_rate': 0.0006986745593161658, 'batch_size': 58, 'step_size': 14, 'gamma': 0.874988281834142}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:16:32,335][0m Trial 39 finished with value: 0.06167143890086342 and parameters: {'observation_period_num': 69, 'train_rates': 0.941239311883925, 'learning_rate': 0.0008738115985533302, 'batch_size': 100, 'step_size': 3, 'gamma': 0.817938462175616}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:19:26,419][0m Trial 40 finished with value: 0.0713830441236496 and parameters: {'observation_period_num': 46, 'train_rates': 0.9887874704679313, 'learning_rate': 9.633795037390739e-05, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9124009116488897}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:21:47,513][0m Trial 41 finished with value: 0.034127964071564744 and parameters: {'observation_period_num': 16, 'train_rates': 0.9028195897875201, 'learning_rate': 0.0005626634681166219, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8447879717397389}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:23:41,299][0m Trial 42 finished with value: 0.22694753331912532 and parameters: {'observation_period_num': 5, 'train_rates': 0.9247620694966784, 'learning_rate': 1.1281710042331845e-06, 'batch_size': 51, 'step_size': 15, 'gamma': 0.8445038893324708}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:25:02,379][0m Trial 43 finished with value: 0.04603044824166731 and parameters: {'observation_period_num': 18, 'train_rates': 0.9627259860862183, 'learning_rate': 0.0005477470031658831, 'batch_size': 74, 'step_size': 14, 'gamma': 0.8030972625242546}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:26:45,636][0m Trial 44 finished with value: 0.0557854906994764 and parameters: {'observation_period_num': 26, 'train_rates': 0.657494189041312, 'learning_rate': 0.00033534971861122383, 'batch_size': 44, 'step_size': 15, 'gamma': 0.8655739199663447}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:27:14,763][0m Trial 45 finished with value: 0.04397390257153246 and parameters: {'observation_period_num': 41, 'train_rates': 0.9074214512815708, 'learning_rate': 0.0006862351909119368, 'batch_size': 213, 'step_size': 12, 'gamma': 0.815210530657116}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:30:25,796][0m Trial 46 finished with value: 0.0430834057295452 and parameters: {'observation_period_num': 15, 'train_rates': 0.9374585504003664, 'learning_rate': 0.000389827600672762, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8493034593519201}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:31:23,241][0m Trial 47 finished with value: 0.06380331818770646 and parameters: {'observation_period_num': 56, 'train_rates': 0.95441453035553, 'learning_rate': 0.00014599033098472582, 'batch_size': 106, 'step_size': 8, 'gamma': 0.8915193068229531}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:32:21,916][0m Trial 48 finished with value: 0.041679470668022654 and parameters: {'observation_period_num': 12, 'train_rates': 0.6997819308469374, 'learning_rate': 0.0006125321037825195, 'batch_size': 83, 'step_size': 15, 'gamma': 0.8102781522305337}. Best is trial 35 with value: 0.030039330670500502.[0m
[32m[I 2025-01-04 23:35:09,252][0m Trial 49 finished with value: 0.2935726195573807 and parameters: {'observation_period_num': 69, 'train_rates': 0.9690945164080156, 'learning_rate': 2.759097744152838e-06, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8345356889182188}. Best is trial 35 with value: 0.030039330670500502.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 23:35:09,260][0m A new study created in memory with name: no-name-2c7cb260-a12d-4a7a-987e-fcf83c8a1a19[0m
[32m[I 2025-01-04 23:35:35,482][0m Trial 0 finished with value: 0.09385841920350989 and parameters: {'observation_period_num': 70, 'train_rates': 0.7503583506263604, 'learning_rate': 8.265721721004625e-05, 'batch_size': 201, 'step_size': 11, 'gamma': 0.7580333839192024}. Best is trial 0 with value: 0.09385841920350989.[0m
[32m[I 2025-01-04 23:37:17,619][0m Trial 1 finished with value: 0.0690943040243991 and parameters: {'observation_period_num': 34, 'train_rates': 0.8681721867296872, 'learning_rate': 0.0008146163129655276, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8588512388503244}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:38:41,000][0m Trial 2 finished with value: 0.5025068521499634 and parameters: {'observation_period_num': 221, 'train_rates': 0.9764808646936477, 'learning_rate': 1.3541102919151374e-06, 'batch_size': 68, 'step_size': 6, 'gamma': 0.9293827854782084}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:41:17,135][0m Trial 3 finished with value: 0.2809495706738475 and parameters: {'observation_period_num': 37, 'train_rates': 0.7006997868680866, 'learning_rate': 1.9590195789476006e-06, 'batch_size': 30, 'step_size': 3, 'gamma': 0.9086430407797678}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:41:51,350][0m Trial 4 finished with value: 0.49045089456638685 and parameters: {'observation_period_num': 172, 'train_rates': 0.8551656979609602, 'learning_rate': 5.885256443895671e-06, 'batch_size': 160, 'step_size': 6, 'gamma': 0.8823245515770936}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:42:16,863][0m Trial 5 finished with value: 1.2789109580392684 and parameters: {'observation_period_num': 7, 'train_rates': 0.811125055537726, 'learning_rate': 1.0557296888709138e-06, 'batch_size': 231, 'step_size': 8, 'gamma': 0.9615365326875496}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:42:48,564][0m Trial 6 finished with value: 0.19824934446072268 and parameters: {'observation_period_num': 120, 'train_rates': 0.6744034486999456, 'learning_rate': 0.0006386388336434458, 'batch_size': 149, 'step_size': 8, 'gamma': 0.912739392184896}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:43:35,751][0m Trial 7 finished with value: 0.9994311959937562 and parameters: {'observation_period_num': 92, 'train_rates': 0.8198691485140253, 'learning_rate': 1.1803523152593277e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.8957879563162134}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:44:21,402][0m Trial 8 finished with value: 0.4154048558014735 and parameters: {'observation_period_num': 74, 'train_rates': 0.8206263815145858, 'learning_rate': 7.728856517643131e-06, 'batch_size': 123, 'step_size': 6, 'gamma': 0.7577408597516428}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:44:50,920][0m Trial 9 finished with value: 0.576418185579604 and parameters: {'observation_period_num': 148, 'train_rates': 0.9016502603292654, 'learning_rate': 1.9565390393589184e-06, 'batch_size': 208, 'step_size': 11, 'gamma': 0.9493176946305032}. Best is trial 1 with value: 0.0690943040243991.[0m
[32m[I 2025-01-04 23:47:35,498][0m Trial 10 finished with value: 0.05855132765340162 and parameters: {'observation_period_num': 10, 'train_rates': 0.603546859237403, 'learning_rate': 0.000775061651850362, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8297532058576377}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-04 23:50:07,847][0m Trial 11 finished with value: 0.06477541742580277 and parameters: {'observation_period_num': 11, 'train_rates': 0.6003811345192922, 'learning_rate': 0.0009675913392702768, 'batch_size': 28, 'step_size': 15, 'gamma': 0.8233258565075756}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-04 23:51:01,410][0m Trial 12 finished with value: 0.05923749796413588 and parameters: {'observation_period_num': 9, 'train_rates': 0.6042592249613086, 'learning_rate': 0.00021464804627109046, 'batch_size': 81, 'step_size': 15, 'gamma': 0.8219082073468996}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-04 23:51:54,424][0m Trial 13 finished with value: 0.07970210419814219 and parameters: {'observation_period_num': 48, 'train_rates': 0.615961097194001, 'learning_rate': 0.00023034853470013363, 'batch_size': 83, 'step_size': 15, 'gamma': 0.8156196996389019}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-04 23:52:42,110][0m Trial 14 finished with value: 0.16468905130458192 and parameters: {'observation_period_num': 246, 'train_rates': 0.6617968013274975, 'learning_rate': 0.00020160282153890968, 'batch_size': 90, 'step_size': 13, 'gamma': 0.8206163062745182}. Best is trial 10 with value: 0.05855132765340162.[0m
Early stopping at epoch 94
[32m[I 2025-01-04 23:54:23,559][0m Trial 15 finished with value: 0.06894857905671038 and parameters: {'observation_period_num': 6, 'train_rates': 0.7362011570626213, 'learning_rate': 0.00024346934487015482, 'batch_size': 46, 'step_size': 1, 'gamma': 0.8489509804906124}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-04 23:58:32,452][0m Trial 16 finished with value: 0.11831780416940724 and parameters: {'observation_period_num': 113, 'train_rates': 0.6442724438290873, 'learning_rate': 6.604224588946741e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.797230734839189}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-04 23:59:20,152][0m Trial 17 finished with value: 0.22422187339875005 and parameters: {'observation_period_num': 186, 'train_rates': 0.7201504253771781, 'learning_rate': 2.375842167939913e-05, 'batch_size': 97, 'step_size': 15, 'gamma': 0.7849446177097028}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-05 00:00:36,071][0m Trial 18 finished with value: 0.08324123086712065 and parameters: {'observation_period_num': 61, 'train_rates': 0.6340414998186581, 'learning_rate': 0.00041028875744672895, 'batch_size': 59, 'step_size': 12, 'gamma': 0.8457568687991195}. Best is trial 10 with value: 0.05855132765340162.[0m
[32m[I 2025-01-05 00:01:24,175][0m Trial 19 finished with value: 0.045864916409402635 and parameters: {'observation_period_num': 31, 'train_rates': 0.7749267991700448, 'learning_rate': 8.601852380140492e-05, 'batch_size': 108, 'step_size': 11, 'gamma': 0.7935736710900347}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:01:59,386][0m Trial 20 finished with value: 0.13246050477027893 and parameters: {'observation_period_num': 95, 'train_rates': 0.946367025972145, 'learning_rate': 2.9700131765992686e-05, 'batch_size': 174, 'step_size': 10, 'gamma': 0.9893208811099743}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:02:49,626][0m Trial 21 finished with value: 0.04612899533754518 and parameters: {'observation_period_num': 29, 'train_rates': 0.7648480456590216, 'learning_rate': 7.642553504967521e-05, 'batch_size': 106, 'step_size': 14, 'gamma': 0.788718844085004}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:03:37,658][0m Trial 22 finished with value: 0.04838969343638217 and parameters: {'observation_period_num': 33, 'train_rates': 0.7590503268916033, 'learning_rate': 7.869890010297338e-05, 'batch_size': 107, 'step_size': 13, 'gamma': 0.7796632513544428}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:04:21,201][0m Trial 23 finished with value: 0.05789951602090425 and parameters: {'observation_period_num': 46, 'train_rates': 0.766808674191067, 'learning_rate': 8.55241067569438e-05, 'batch_size': 121, 'step_size': 12, 'gamma': 0.783563397255704}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:05:00,427][0m Trial 24 finished with value: 0.0639980473424211 and parameters: {'observation_period_num': 35, 'train_rates': 0.7766649056263585, 'learning_rate': 4.770497649681247e-05, 'batch_size': 138, 'step_size': 13, 'gamma': 0.7814506136675461}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:05:49,849][0m Trial 25 finished with value: 0.06017609153825201 and parameters: {'observation_period_num': 76, 'train_rates': 0.788755137936932, 'learning_rate': 0.00011920602287479327, 'batch_size': 108, 'step_size': 10, 'gamma': 0.7538594972327356}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:06:19,036][0m Trial 26 finished with value: 0.267584903063546 and parameters: {'observation_period_num': 92, 'train_rates': 0.7005182093408799, 'learning_rate': 1.6279714320889587e-05, 'batch_size': 175, 'step_size': 14, 'gamma': 0.7981528396465822}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:07:12,105][0m Trial 27 finished with value: 0.0476219030657165 and parameters: {'observation_period_num': 28, 'train_rates': 0.8467319230031334, 'learning_rate': 4.7416617474449164e-05, 'batch_size': 105, 'step_size': 12, 'gamma': 0.770557477617588}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:07:56,415][0m Trial 28 finished with value: 0.1847853131864554 and parameters: {'observation_period_num': 58, 'train_rates': 0.8960579192966824, 'learning_rate': 1.4044104297211235e-05, 'batch_size': 132, 'step_size': 11, 'gamma': 0.8058869059569647}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:09:12,694][0m Trial 29 finished with value: 0.08372605854974073 and parameters: {'observation_period_num': 139, 'train_rates': 0.8492932069071475, 'learning_rate': 4.538906061423732e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.7684813682955213}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:09:47,921][0m Trial 30 finished with value: 0.05564387359061039 and parameters: {'observation_period_num': 25, 'train_rates': 0.7425169594906111, 'learning_rate': 9.950721621664052e-05, 'batch_size': 150, 'step_size': 10, 'gamma': 0.7698998226917809}. Best is trial 19 with value: 0.045864916409402635.[0m
[32m[I 2025-01-05 00:10:39,480][0m Trial 31 finished with value: 0.03704158999151464 and parameters: {'observation_period_num': 22, 'train_rates': 0.797714135044274, 'learning_rate': 0.00014301134997875116, 'batch_size': 103, 'step_size': 12, 'gamma': 0.768014670451788}. Best is trial 31 with value: 0.03704158999151464.[0m
[32m[I 2025-01-05 00:11:33,528][0m Trial 32 finished with value: 0.038787680637293646 and parameters: {'observation_period_num': 24, 'train_rates': 0.8052242512227177, 'learning_rate': 0.0001402638222868412, 'batch_size': 99, 'step_size': 12, 'gamma': 0.7508259464988806}. Best is trial 31 with value: 0.03704158999151464.[0m
[32m[I 2025-01-05 00:12:40,778][0m Trial 33 finished with value: 0.047897298634052274 and parameters: {'observation_period_num': 58, 'train_rates': 0.7981960205357982, 'learning_rate': 0.00013876779432303555, 'batch_size': 78, 'step_size': 9, 'gamma': 0.7555185157597791}. Best is trial 31 with value: 0.03704158999151464.[0m
[32m[I 2025-01-05 00:14:25,577][0m Trial 34 finished with value: 0.04445238578422316 and parameters: {'observation_period_num': 23, 'train_rates': 0.8320795492419882, 'learning_rate': 0.0003671063951293156, 'batch_size': 51, 'step_size': 14, 'gamma': 0.7974123358595915}. Best is trial 31 with value: 0.03704158999151464.[0m
[32m[I 2025-01-05 00:16:27,764][0m Trial 35 finished with value: 0.06312141219049008 and parameters: {'observation_period_num': 46, 'train_rates': 0.8352335834076356, 'learning_rate': 0.0003667046960791631, 'batch_size': 43, 'step_size': 14, 'gamma': 0.7511551736170087}. Best is trial 31 with value: 0.03704158999151464.[0m
[32m[I 2025-01-05 00:17:57,817][0m Trial 36 finished with value: 0.036018467161717804 and parameters: {'observation_period_num': 20, 'train_rates': 0.8827450808008898, 'learning_rate': 0.0004394404894903354, 'batch_size': 63, 'step_size': 9, 'gamma': 0.8743141593062987}. Best is trial 36 with value: 0.036018467161717804.[0m
[32m[I 2025-01-05 00:19:32,202][0m Trial 37 finished with value: 0.05157599329468416 and parameters: {'observation_period_num': 23, 'train_rates': 0.8784288383953263, 'learning_rate': 0.00043159623254151254, 'batch_size': 59, 'step_size': 7, 'gamma': 0.872896231052325}. Best is trial 36 with value: 0.036018467161717804.[0m
[32m[I 2025-01-05 00:21:43,989][0m Trial 38 finished with value: 0.08303190357443216 and parameters: {'observation_period_num': 78, 'train_rates': 0.9281817044186603, 'learning_rate': 0.0005547226039216724, 'batch_size': 43, 'step_size': 5, 'gamma': 0.8843911844099585}. Best is trial 36 with value: 0.036018467161717804.[0m
[32m[I 2025-01-05 00:23:03,712][0m Trial 39 finished with value: 0.11535846925265081 and parameters: {'observation_period_num': 194, 'train_rates': 0.8661302761561995, 'learning_rate': 0.0003000492715126009, 'batch_size': 65, 'step_size': 9, 'gamma': 0.8529250468653682}. Best is trial 36 with value: 0.036018467161717804.[0m
[32m[I 2025-01-05 00:24:27,009][0m Trial 40 finished with value: 0.049526747316122055 and parameters: {'observation_period_num': 51, 'train_rates': 0.9814675937341503, 'learning_rate': 0.00015668227448619956, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9191388655408276}. Best is trial 36 with value: 0.036018467161717804.[0m
[32m[I 2025-01-05 00:25:25,510][0m Trial 41 finished with value: 0.03126312678992955 and parameters: {'observation_period_num': 19, 'train_rates': 0.8035699972938782, 'learning_rate': 0.0006174670437640473, 'batch_size': 92, 'step_size': 11, 'gamma': 0.8360274512186385}. Best is trial 41 with value: 0.03126312678992955.[0m
[32m[I 2025-01-05 00:26:25,669][0m Trial 42 finished with value: 0.0313909611740403 and parameters: {'observation_period_num': 19, 'train_rates': 0.8072719550833388, 'learning_rate': 0.0005711052636577055, 'batch_size': 91, 'step_size': 12, 'gamma': 0.8388457966560714}. Best is trial 41 with value: 0.03126312678992955.[0m
[32m[I 2025-01-05 00:27:24,250][0m Trial 43 finished with value: 0.045282186689507384 and parameters: {'observation_period_num': 41, 'train_rates': 0.80898624793354, 'learning_rate': 0.0006216983868319167, 'batch_size': 91, 'step_size': 12, 'gamma': 0.8387512242146969}. Best is trial 41 with value: 0.03126312678992955.[0m
[32m[I 2025-01-05 00:28:06,130][0m Trial 44 finished with value: 0.03296508892838444 and parameters: {'observation_period_num': 18, 'train_rates': 0.7903840117597938, 'learning_rate': 0.000959490933449829, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8943399467156447}. Best is trial 41 with value: 0.03126312678992955.[0m
[32m[I 2025-01-05 00:28:29,680][0m Trial 45 finished with value: 0.04065822987215312 and parameters: {'observation_period_num': 12, 'train_rates': 0.7886240253694536, 'learning_rate': 0.0008585213472703696, 'batch_size': 252, 'step_size': 8, 'gamma': 0.8648130588618548}. Best is trial 41 with value: 0.03126312678992955.[0m
[32m[I 2025-01-05 00:29:10,205][0m Trial 46 finished with value: 0.034102315131478204 and parameters: {'observation_period_num': 17, 'train_rates': 0.8293657318135629, 'learning_rate': 0.0005548465093042428, 'batch_size': 136, 'step_size': 10, 'gamma': 0.8984391453938089}. Best is trial 41 with value: 0.03126312678992955.[0m
[32m[I 2025-01-05 00:29:54,439][0m Trial 47 finished with value: 0.02672421423706906 and parameters: {'observation_period_num': 17, 'train_rates': 0.8890777208359977, 'learning_rate': 0.0005780688306891082, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8989667503112131}. Best is trial 47 with value: 0.02672421423706906.[0m
[32m[I 2025-01-05 00:30:32,013][0m Trial 48 finished with value: 0.049488724263098374 and parameters: {'observation_period_num': 65, 'train_rates': 0.9280828669409611, 'learning_rate': 0.0006617989969930425, 'batch_size': 165, 'step_size': 5, 'gamma': 0.894433433974363}. Best is trial 47 with value: 0.02672421423706906.[0m
[32m[I 2025-01-05 00:31:14,313][0m Trial 49 finished with value: 0.03196154240258904 and parameters: {'observation_period_num': 5, 'train_rates': 0.8252209736971164, 'learning_rate': 0.000911812999421313, 'batch_size': 135, 'step_size': 7, 'gamma': 0.9353210328014863}. Best is trial 47 with value: 0.02672421423706906.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-05 00:31:14,324][0m A new study created in memory with name: no-name-3f708d12-7229-4b5b-8c6a-92aa5038db0a[0m
[32m[I 2025-01-05 00:32:01,475][0m Trial 0 finished with value: 0.3608780670722164 and parameters: {'observation_period_num': 23, 'train_rates': 0.6635050093221007, 'learning_rate': 1.3042832318104225e-05, 'batch_size': 100, 'step_size': 2, 'gamma': 0.8414208330846384}. Best is trial 0 with value: 0.3608780670722164.[0m
[32m[I 2025-01-05 00:33:02,210][0m Trial 1 finished with value: 0.0388625512456792 and parameters: {'observation_period_num': 46, 'train_rates': 0.7990088064298603, 'learning_rate': 0.0006131436615176352, 'batch_size': 86, 'step_size': 7, 'gamma': 0.8429604114812862}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:34:15,866][0m Trial 2 finished with value: 0.5480275750160217 and parameters: {'observation_period_num': 114, 'train_rates': 0.9837084086680306, 'learning_rate': 1.985727380789607e-06, 'batch_size': 82, 'step_size': 9, 'gamma': 0.8500730403054333}. Best is trial 1 with value: 0.0388625512456792.[0m
Early stopping at epoch 59
[32m[I 2025-01-05 00:34:32,986][0m Trial 3 finished with value: 0.4115860779874174 and parameters: {'observation_period_num': 216, 'train_rates': 0.6125342837619229, 'learning_rate': 0.0002683060644820991, 'batch_size': 158, 'step_size': 1, 'gamma': 0.7972670644739919}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:35:52,521][0m Trial 4 finished with value: 0.11713035933781361 and parameters: {'observation_period_num': 181, 'train_rates': 0.9503876011980676, 'learning_rate': 0.000230910476233044, 'batch_size': 70, 'step_size': 8, 'gamma': 0.8443799274889873}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:36:27,481][0m Trial 5 finished with value: 0.0934588281933836 and parameters: {'observation_period_num': 105, 'train_rates': 0.7205529313735501, 'learning_rate': 0.0006381779622348726, 'batch_size': 139, 'step_size': 5, 'gamma': 0.926411641956885}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:37:02,587][0m Trial 6 finished with value: 0.17693958119197253 and parameters: {'observation_period_num': 234, 'train_rates': 0.7774729755513514, 'learning_rate': 7.864989469502346e-05, 'batch_size': 146, 'step_size': 7, 'gamma': 0.8164280069297636}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:37:31,768][0m Trial 7 finished with value: 0.20625277332938774 and parameters: {'observation_period_num': 120, 'train_rates': 0.8848424336830024, 'learning_rate': 1.229662309487046e-05, 'batch_size': 208, 'step_size': 15, 'gamma': 0.911788282049355}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:38:02,909][0m Trial 8 finished with value: 0.1516023576259613 and parameters: {'observation_period_num': 119, 'train_rates': 0.9834660148200877, 'learning_rate': 3.173732636345879e-05, 'batch_size': 210, 'step_size': 8, 'gamma': 0.9809521607594884}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:39:05,502][0m Trial 9 finished with value: 0.5567997429558035 and parameters: {'observation_period_num': 48, 'train_rates': 0.6975904279986033, 'learning_rate': 2.0308116682485035e-06, 'batch_size': 77, 'step_size': 8, 'gamma': 0.7627598377433153}. Best is trial 1 with value: 0.0388625512456792.[0m
[32m[I 2025-01-05 00:44:32,684][0m Trial 10 finished with value: 0.037809005183518346 and parameters: {'observation_period_num': 59, 'train_rates': 0.8505121710501768, 'learning_rate': 0.0009718401603793287, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8988418220469941}. Best is trial 10 with value: 0.037809005183518346.[0m
[32m[I 2025-01-05 00:49:05,994][0m Trial 11 finished with value: 0.05051549032149992 and parameters: {'observation_period_num': 62, 'train_rates': 0.8432420630763718, 'learning_rate': 0.0009460704267831631, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8993383996608086}. Best is trial 10 with value: 0.037809005183518346.[0m
[32m[I 2025-01-05 00:53:47,346][0m Trial 12 finished with value: 0.028841641878611164 and parameters: {'observation_period_num': 11, 'train_rates': 0.7998260751937984, 'learning_rate': 0.0002121035385809063, 'batch_size': 18, 'step_size': 11, 'gamma': 0.8835083563726771}. Best is trial 12 with value: 0.028841641878611164.[0m
[32m[I 2025-01-05 00:58:41,023][0m Trial 13 finished with value: 0.026791094758566593 and parameters: {'observation_period_num': 11, 'train_rates': 0.8483740197640682, 'learning_rate': 0.00014433466833882743, 'batch_size': 18, 'step_size': 12, 'gamma': 0.9534653961768141}. Best is trial 13 with value: 0.026791094758566593.[0m
[32m[I 2025-01-05 01:00:46,539][0m Trial 14 finished with value: 0.023848629196303605 and parameters: {'observation_period_num': 8, 'train_rates': 0.9072501957954616, 'learning_rate': 0.00011616561472843723, 'batch_size': 45, 'step_size': 11, 'gamma': 0.9647189238838725}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:02:40,602][0m Trial 15 finished with value: 0.026092472280759144 and parameters: {'observation_period_num': 5, 'train_rates': 0.9079415143775834, 'learning_rate': 7.799626669400004e-05, 'batch_size': 50, 'step_size': 15, 'gamma': 0.9821999100588645}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:04:29,829][0m Trial 16 finished with value: 0.05882952980726604 and parameters: {'observation_period_num': 81, 'train_rates': 0.912142956394819, 'learning_rate': 4.746079803399551e-05, 'batch_size': 52, 'step_size': 15, 'gamma': 0.9894888704999781}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:05:17,947][0m Trial 17 finished with value: 0.22639743495716277 and parameters: {'observation_period_num': 158, 'train_rates': 0.9183427373898208, 'learning_rate': 1.1971397248421188e-05, 'batch_size': 117, 'step_size': 14, 'gamma': 0.9515340528466094}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:07:17,807][0m Trial 18 finished with value: 0.12164722738532245 and parameters: {'observation_period_num': 87, 'train_rates': 0.9314022801207535, 'learning_rate': 8.413935016134732e-05, 'batch_size': 47, 'step_size': 10, 'gamma': 0.9513411076337356}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:07:51,372][0m Trial 19 finished with value: 0.4224175799109064 and parameters: {'observation_period_num': 151, 'train_rates': 0.8820625898786513, 'learning_rate': 4.805060202747228e-06, 'batch_size': 167, 'step_size': 13, 'gamma': 0.9644581148367978}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:08:44,940][0m Trial 20 finished with value: 0.07822596906300854 and parameters: {'observation_period_num': 31, 'train_rates': 0.9536178132740596, 'learning_rate': 2.0761580040468557e-05, 'batch_size': 113, 'step_size': 11, 'gamma': 0.9381588790340372}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:10:51,812][0m Trial 21 finished with value: 0.04337164040705467 and parameters: {'observation_period_num': 13, 'train_rates': 0.84990947489196, 'learning_rate': 9.133458489803273e-05, 'batch_size': 43, 'step_size': 13, 'gamma': 0.9701968703692408}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:11:17,016][0m Trial 22 finished with value: 0.04112313041696325 and parameters: {'observation_period_num': 30, 'train_rates': 0.8689072739746334, 'learning_rate': 0.00012960367282360968, 'batch_size': 246, 'step_size': 11, 'gamma': 0.92795675937505}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:13:23,703][0m Trial 23 finished with value: 0.027810994610403875 and parameters: {'observation_period_num': 6, 'train_rates': 0.7634595668093122, 'learning_rate': 4.5403145977847735e-05, 'batch_size': 40, 'step_size': 14, 'gamma': 0.9596701627060372}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:14:47,794][0m Trial 24 finished with value: 0.04848942569381482 and parameters: {'observation_period_num': 36, 'train_rates': 0.8097837380922561, 'learning_rate': 0.000389320470470013, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9857243283691064}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:17:24,442][0m Trial 25 finished with value: 0.08721045418672897 and parameters: {'observation_period_num': 91, 'train_rates': 0.8933193036593178, 'learning_rate': 0.0001493422633535735, 'batch_size': 35, 'step_size': 15, 'gamma': 0.937672143979941}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:18:49,767][0m Trial 26 finished with value: 0.045153715784048164 and parameters: {'observation_period_num': 62, 'train_rates': 0.8299543535542739, 'learning_rate': 5.8471286811789965e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.9709780933718576}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:19:42,359][0m Trial 27 finished with value: 0.030751678246904064 and parameters: {'observation_period_num': 8, 'train_rates': 0.7489585452041396, 'learning_rate': 0.00014401945269658676, 'batch_size': 98, 'step_size': 12, 'gamma': 0.9163028660938732}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:23:11,991][0m Trial 28 finished with value: 0.05189308778621083 and parameters: {'observation_period_num': 68, 'train_rates': 0.9406857436210019, 'learning_rate': 3.1309730027605297e-05, 'batch_size': 27, 'step_size': 14, 'gamma': 0.8775851253618518}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:24:10,212][0m Trial 29 finished with value: 0.03433982387104405 and parameters: {'observation_period_num': 27, 'train_rates': 0.9053425038377986, 'learning_rate': 0.000390568444601634, 'batch_size': 99, 'step_size': 5, 'gamma': 0.9449383769957151}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:25:49,250][0m Trial 30 finished with value: 0.0511900922831367 and parameters: {'observation_period_num': 46, 'train_rates': 0.8716107538087111, 'learning_rate': 1.892263022138671e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9736495898151423}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:28:11,533][0m Trial 31 finished with value: 0.030916201178715026 and parameters: {'observation_period_num': 5, 'train_rates': 0.756318613251393, 'learning_rate': 5.107385182747123e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9619416909727984}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:30:40,162][0m Trial 32 finished with value: 0.035035720125629305 and parameters: {'observation_period_num': 20, 'train_rates': 0.8134763230683528, 'learning_rate': 3.5109376298620294e-05, 'batch_size': 35, 'step_size': 14, 'gamma': 0.9572037712873421}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:31:40,871][0m Trial 33 finished with value: 0.03716233648971221 and parameters: {'observation_period_num': 25, 'train_rates': 0.7753659818429239, 'learning_rate': 0.00010965081831443643, 'batch_size': 85, 'step_size': 13, 'gamma': 0.9867799531307755}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:33:30,181][0m Trial 34 finished with value: 0.05843518218596873 and parameters: {'observation_period_num': 37, 'train_rates': 0.6479407333130576, 'learning_rate': 7.099234878033153e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.8636259518127116}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:34:57,794][0m Trial 35 finished with value: 0.024903365727706055 and parameters: {'observation_period_num': 5, 'train_rates': 0.9673918263882373, 'learning_rate': 0.00018530792100682726, 'batch_size': 69, 'step_size': 12, 'gamma': 0.9298173472917821}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:36:20,743][0m Trial 36 finished with value: 0.05667312450560869 and parameters: {'observation_period_num': 48, 'train_rates': 0.9651120618118307, 'learning_rate': 0.0002939669667626851, 'batch_size': 71, 'step_size': 9, 'gamma': 0.9283562220033834}. Best is trial 14 with value: 0.023848629196303605.[0m
[32m[I 2025-01-05 01:37:15,804][0m Trial 37 finished with value: 0.022265607491135597 and parameters: {'observation_period_num': 20, 'train_rates': 0.987381978164134, 'learning_rate': 0.000210758885676157, 'batch_size': 112, 'step_size': 12, 'gamma': 0.9408787578946637}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:38:03,008][0m Trial 38 finished with value: 0.08698815107345581 and parameters: {'observation_period_num': 205, 'train_rates': 0.9586666836937392, 'learning_rate': 0.00022114857641819364, 'batch_size': 121, 'step_size': 9, 'gamma': 0.9136975771658261}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:39:09,477][0m Trial 39 finished with value: 0.12979134917259216 and parameters: {'observation_period_num': 137, 'train_rates': 0.9893027550076007, 'learning_rate': 0.0004686182150317901, 'batch_size': 92, 'step_size': 5, 'gamma': 0.8952390289169901}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:39:58,498][0m Trial 40 finished with value: 0.026030337437987328 and parameters: {'observation_period_num': 22, 'train_rates': 0.9735677693590914, 'learning_rate': 0.0005917654740061899, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9383693985860916}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:40:39,522][0m Trial 41 finished with value: 0.029341518878936768 and parameters: {'observation_period_num': 19, 'train_rates': 0.9739306718755387, 'learning_rate': 0.0006563722496653534, 'batch_size': 157, 'step_size': 6, 'gamma': 0.9405659455201185}. Best is trial 37 with value: 0.022265607491135597.[0m
Early stopping at epoch 58
[32m[I 2025-01-05 01:41:01,132][0m Trial 42 finished with value: 0.21747911845644316 and parameters: {'observation_period_num': 40, 'train_rates': 0.9341390023558799, 'learning_rate': 0.00018430347506770035, 'batch_size': 176, 'step_size': 1, 'gamma': 0.8106382929438416}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:41:46,037][0m Trial 43 finished with value: 0.03440367802977562 and parameters: {'observation_period_num': 20, 'train_rates': 0.9646407605621032, 'learning_rate': 0.00030821471226294525, 'batch_size': 138, 'step_size': 3, 'gamma': 0.9219432523812493}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:42:40,605][0m Trial 44 finished with value: 0.1637108325958252 and parameters: {'observation_period_num': 246, 'train_rates': 0.9878831911786317, 'learning_rate': 0.000610268065163622, 'batch_size': 110, 'step_size': 3, 'gamma': 0.9743918644452849}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:43:52,767][0m Trial 45 finished with value: 1.0359600450934434 and parameters: {'observation_period_num': 74, 'train_rates': 0.9288763139985005, 'learning_rate': 1.075044984479999e-06, 'batch_size': 79, 'step_size': 7, 'gamma': 0.759795528550353}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:44:40,661][0m Trial 46 finished with value: 0.06199051632035163 and parameters: {'observation_period_num': 48, 'train_rates': 0.9467032692017856, 'learning_rate': 0.00027211399174169543, 'batch_size': 130, 'step_size': 11, 'gamma': 0.931883355482383}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:46:10,084][0m Trial 47 finished with value: 0.04270872869053666 and parameters: {'observation_period_num': 56, 'train_rates': 0.8986085732669322, 'learning_rate': 0.0001704145438674945, 'batch_size': 62, 'step_size': 3, 'gamma': 0.9067670724419049}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:46:43,226][0m Trial 48 finished with value: 0.10994920879602432 and parameters: {'observation_period_num': 99, 'train_rates': 0.9719852737523766, 'learning_rate': 0.00010096249793357206, 'batch_size': 186, 'step_size': 6, 'gamma': 0.888159732779454}. Best is trial 37 with value: 0.022265607491135597.[0m
[32m[I 2025-01-05 01:47:24,760][0m Trial 49 finished with value: 0.03035263679290222 and parameters: {'observation_period_num': 19, 'train_rates': 0.9197183022592528, 'learning_rate': 0.0005211722804320979, 'batch_size': 148, 'step_size': 9, 'gamma': 0.8325066427223448}. Best is trial 37 with value: 0.022265607491135597.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 01:47:24,770][0m A new study created in memory with name: no-name-7d605bb4-81d0-4a1c-9d1d-2fe612e30882[0m
[32m[I 2025-01-05 01:49:32,358][0m Trial 0 finished with value: 0.12013667656315698 and parameters: {'observation_period_num': 248, 'train_rates': 0.9701343542080637, 'learning_rate': 1.7972383820391563e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9390165988713229}. Best is trial 0 with value: 0.12013667656315698.[0m
[32m[I 2025-01-05 01:50:04,385][0m Trial 1 finished with value: 0.04847225883963556 and parameters: {'observation_period_num': 69, 'train_rates': 0.8397780427071779, 'learning_rate': 0.00024959338978697645, 'batch_size': 183, 'step_size': 2, 'gamma': 0.9614726127964711}. Best is trial 1 with value: 0.04847225883963556.[0m
[32m[I 2025-01-05 01:50:48,830][0m Trial 2 finished with value: 0.023275670522869562 and parameters: {'observation_period_num': 11, 'train_rates': 0.8476500194098459, 'learning_rate': 0.0007373980961353831, 'batch_size': 129, 'step_size': 9, 'gamma': 0.768137220851108}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:51:16,476][0m Trial 3 finished with value: 0.1847797371741964 and parameters: {'observation_period_num': 95, 'train_rates': 0.6022986205495957, 'learning_rate': 4.8439978447546e-05, 'batch_size': 165, 'step_size': 14, 'gamma': 0.8422201198907394}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:53:53,892][0m Trial 4 finished with value: 0.09601947559444292 and parameters: {'observation_period_num': 160, 'train_rates': 0.828778217919332, 'learning_rate': 1.9689400339843762e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7674797691733869}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:54:20,582][0m Trial 5 finished with value: 0.11671427637338638 and parameters: {'observation_period_num': 216, 'train_rates': 0.9884522017929331, 'learning_rate': 9.609949369768282e-05, 'batch_size': 239, 'step_size': 13, 'gamma': 0.9496716772603506}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:54:45,713][0m Trial 6 finished with value: 0.09953667968511581 and parameters: {'observation_period_num': 196, 'train_rates': 0.9150358862849945, 'learning_rate': 0.00035827966707518136, 'batch_size': 245, 'step_size': 13, 'gamma': 0.7845944083715881}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:55:08,678][0m Trial 7 finished with value: 0.9503127307273878 and parameters: {'observation_period_num': 94, 'train_rates': 0.6008097765438002, 'learning_rate': 0.00013883837166175046, 'batch_size': 200, 'step_size': 12, 'gamma': 0.8302436625690828}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:55:29,084][0m Trial 8 finished with value: 1.3429382417039073 and parameters: {'observation_period_num': 244, 'train_rates': 0.6166223242410945, 'learning_rate': 3.0093029756805563e-06, 'batch_size': 229, 'step_size': 5, 'gamma': 0.80209080145793}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:56:17,286][0m Trial 9 finished with value: 0.4971697097239287 and parameters: {'observation_period_num': 44, 'train_rates': 0.84987780449995, 'learning_rate': 1.882798420342051e-06, 'batch_size': 114, 'step_size': 1, 'gamma': 0.983358371007268}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:57:05,319][0m Trial 10 finished with value: 0.030404259682428547 and parameters: {'observation_period_num': 7, 'train_rates': 0.7218852224382878, 'learning_rate': 0.0007555844382955531, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8945654599743109}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:57:51,739][0m Trial 11 finished with value: 0.026447158943437323 and parameters: {'observation_period_num': 6, 'train_rates': 0.7320377946560065, 'learning_rate': 0.0008204534527267031, 'batch_size': 110, 'step_size': 9, 'gamma': 0.8884474771515001}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:58:55,307][0m Trial 12 finished with value: 0.028365238931719165 and parameters: {'observation_period_num': 12, 'train_rates': 0.7464324987370885, 'learning_rate': 0.0008139726641273501, 'batch_size': 80, 'step_size': 9, 'gamma': 0.889376548628668}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 01:59:28,849][0m Trial 13 finished with value: 0.06033191357487291 and parameters: {'observation_period_num': 44, 'train_rates': 0.7118357950892638, 'learning_rate': 0.0007738092990949381, 'batch_size': 151, 'step_size': 6, 'gamma': 0.750469818948409}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:00:33,641][0m Trial 14 finished with value: 0.2805525733830449 and parameters: {'observation_period_num': 131, 'train_rates': 0.7783899201742688, 'learning_rate': 5.716485723457424e-06, 'batch_size': 78, 'step_size': 10, 'gamma': 0.9130464523008379}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:01:19,891][0m Trial 15 finished with value: 0.03732736267149448 and parameters: {'observation_period_num': 32, 'train_rates': 0.8974479552191528, 'learning_rate': 0.00030451981107734803, 'batch_size': 126, 'step_size': 4, 'gamma': 0.8559776817771327}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:02:33,126][0m Trial 16 finished with value: 0.07388052510560593 and parameters: {'observation_period_num': 74, 'train_rates': 0.6997447166141872, 'learning_rate': 8.515395235000817e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.804642551944281}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:03:10,020][0m Trial 17 finished with value: 0.08081010944116702 and parameters: {'observation_period_num': 133, 'train_rates': 0.7912930958090523, 'learning_rate': 0.00032398112189412564, 'batch_size': 143, 'step_size': 15, 'gamma': 0.8699789526932948}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:03:55,035][0m Trial 18 finished with value: 0.18104467822104386 and parameters: {'observation_period_num': 28, 'train_rates': 0.6684479879508644, 'learning_rate': 9.477523483623295e-06, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8240483055490256}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:04:26,345][0m Trial 19 finished with value: 0.09118918897412671 and parameters: {'observation_period_num': 84, 'train_rates': 0.8865928790241293, 'learning_rate': 0.00017917150854526747, 'batch_size': 183, 'step_size': 7, 'gamma': 0.9215738500448736}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:05:20,354][0m Trial 20 finished with value: 0.13914092334429012 and parameters: {'observation_period_num': 59, 'train_rates': 0.6651597482100997, 'learning_rate': 5.0499598663129e-05, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8837254347143879}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:06:32,116][0m Trial 21 finished with value: 0.024064959370467848 and parameters: {'observation_period_num': 6, 'train_rates': 0.7555571464225433, 'learning_rate': 0.000932643225996185, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8967631028622249}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:08:18,681][0m Trial 22 finished with value: 0.028664467008694242 and parameters: {'observation_period_num': 10, 'train_rates': 0.7543396281354241, 'learning_rate': 0.000507995414051248, 'batch_size': 47, 'step_size': 9, 'gamma': 0.9108197980515037}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:09:01,659][0m Trial 23 finished with value: 0.02893663669625918 and parameters: {'observation_period_num': 34, 'train_rates': 0.799795772119015, 'learning_rate': 0.0009699250602284452, 'batch_size': 129, 'step_size': 10, 'gamma': 0.8647495576322642}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:13:27,172][0m Trial 24 finished with value: 0.06562810844723703 and parameters: {'observation_period_num': 55, 'train_rates': 0.7511884861689344, 'learning_rate': 0.0004880264306143186, 'batch_size': 18, 'step_size': 7, 'gamma': 0.9277120543159225}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:14:22,282][0m Trial 25 finished with value: 0.027881911007487273 and parameters: {'observation_period_num': 5, 'train_rates': 0.8140974635419044, 'learning_rate': 0.00017202237145554222, 'batch_size': 97, 'step_size': 8, 'gamma': 0.8484341853047287}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:15:36,991][0m Trial 26 finished with value: 0.12474215785280252 and parameters: {'observation_period_num': 113, 'train_rates': 0.6706102857460423, 'learning_rate': 0.000450151640727479, 'batch_size': 60, 'step_size': 10, 'gamma': 0.8953896309206598}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:16:13,118][0m Trial 27 finished with value: 0.030334526668576632 and parameters: {'observation_period_num': 24, 'train_rates': 0.8549509471067219, 'learning_rate': 0.000638989026196553, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8797947531152805}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:16:58,541][0m Trial 28 finished with value: 0.08881738979742523 and parameters: {'observation_period_num': 163, 'train_rates': 0.774692700153461, 'learning_rate': 0.0009724373841268357, 'batch_size': 112, 'step_size': 6, 'gamma': 0.8143095900701474}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:18:29,405][0m Trial 29 finished with value: 0.13277522698501867 and parameters: {'observation_period_num': 26, 'train_rates': 0.9443494008478426, 'learning_rate': 0.00025136835892728814, 'batch_size': 65, 'step_size': 8, 'gamma': 0.9455097370042846}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:19:07,191][0m Trial 30 finished with value: 0.06373918166166635 and parameters: {'observation_period_num': 52, 'train_rates': 0.7286064078151291, 'learning_rate': 9.774504377154137e-05, 'batch_size': 133, 'step_size': 6, 'gamma': 0.9017729479630645}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:20:02,038][0m Trial 31 finished with value: 0.02835294478427386 and parameters: {'observation_period_num': 5, 'train_rates': 0.8264027751605338, 'learning_rate': 0.00018390681037821281, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8413144617583663}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:21:00,016][0m Trial 32 finished with value: 0.027864024846893664 and parameters: {'observation_period_num': 21, 'train_rates': 0.8033389082056146, 'learning_rate': 0.0005175101230990454, 'batch_size': 92, 'step_size': 9, 'gamma': 0.854688568618728}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:21:48,400][0m Trial 33 finished with value: 0.029594496930561338 and parameters: {'observation_period_num': 23, 'train_rates': 0.8689842154054949, 'learning_rate': 0.0004937331728728997, 'batch_size': 121, 'step_size': 9, 'gamma': 0.932208082086518}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:23:35,008][0m Trial 34 finished with value: 0.0727158631039265 and parameters: {'observation_period_num': 69, 'train_rates': 0.807118927702529, 'learning_rate': 0.0006008747913353362, 'batch_size': 48, 'step_size': 12, 'gamma': 0.7835589181096855}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:24:31,388][0m Trial 35 finished with value: 0.08803838774048049 and parameters: {'observation_period_num': 38, 'train_rates': 0.7713101584430729, 'learning_rate': 2.2998803400425487e-05, 'batch_size': 92, 'step_size': 10, 'gamma': 0.8623244389882173}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:25:36,474][0m Trial 36 finished with value: 0.04070298137531391 and parameters: {'observation_period_num': 21, 'train_rates': 0.6909918766626297, 'learning_rate': 0.0003102220926257973, 'batch_size': 74, 'step_size': 12, 'gamma': 0.975250805353927}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:26:08,294][0m Trial 37 finished with value: 0.058007653772830965 and parameters: {'observation_period_num': 64, 'train_rates': 0.8270684755103571, 'learning_rate': 6.0583999207702404e-05, 'batch_size': 176, 'step_size': 11, 'gamma': 0.9571290919719684}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:26:40,511][0m Trial 38 finished with value: 2.209199316508436 and parameters: {'observation_period_num': 47, 'train_rates': 0.6393503970512697, 'learning_rate': 1.0451108911297267e-06, 'batch_size': 144, 'step_size': 7, 'gamma': 0.7830600040839993}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:28:45,542][0m Trial 39 finished with value: 0.13642002845624365 and parameters: {'observation_period_num': 100, 'train_rates': 0.7383192831102601, 'learning_rate': 3.1958978817114514e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8369828386729451}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:29:20,976][0m Trial 40 finished with value: 0.08193122813927717 and parameters: {'observation_period_num': 174, 'train_rates': 0.9165845558925988, 'learning_rate': 0.0004053627215753364, 'batch_size': 167, 'step_size': 13, 'gamma': 0.753983268256057}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:30:19,170][0m Trial 41 finished with value: 0.027973868115381762 and parameters: {'observation_period_num': 15, 'train_rates': 0.8129477774978586, 'learning_rate': 0.00018144911015670866, 'batch_size': 93, 'step_size': 8, 'gamma': 0.8527371414480316}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:31:08,517][0m Trial 42 finished with value: 0.026258727062873365 and parameters: {'observation_period_num': 15, 'train_rates': 0.843219526385464, 'learning_rate': 0.000999369482747138, 'batch_size': 117, 'step_size': 10, 'gamma': 0.8509467694202656}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:31:57,781][0m Trial 43 finished with value: 0.035313459860671095 and parameters: {'observation_period_num': 37, 'train_rates': 0.8387663975965957, 'learning_rate': 0.0009713460026262965, 'batch_size': 115, 'step_size': 10, 'gamma': 0.879284298563879}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:32:27,217][0m Trial 44 finished with value: 0.04313534174541958 and parameters: {'observation_period_num': 18, 'train_rates': 0.8755675282654454, 'learning_rate': 0.000688204504258201, 'batch_size': 213, 'step_size': 11, 'gamma': 0.9038015517540431}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:33:16,416][0m Trial 45 finished with value: 0.024827707298339562 and parameters: {'observation_period_num': 17, 'train_rates': 0.7884302132190373, 'learning_rate': 0.000633785696791559, 'batch_size': 109, 'step_size': 9, 'gamma': 0.825368928787962}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:33:51,300][0m Trial 46 finished with value: 0.14504270136356354 and parameters: {'observation_period_num': 242, 'train_rates': 0.7697798403215813, 'learning_rate': 0.0006719632073755364, 'batch_size': 140, 'step_size': 10, 'gamma': 0.7673510083188814}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:34:41,896][0m Trial 47 finished with value: 0.02903779925071393 and parameters: {'observation_period_num': 5, 'train_rates': 0.8536442281472733, 'learning_rate': 0.00023768706448695618, 'batch_size': 113, 'step_size': 12, 'gamma': 0.7952625082512764}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:35:14,841][0m Trial 48 finished with value: 0.04384591485380281 and parameters: {'observation_period_num': 43, 'train_rates': 0.7580318403694545, 'learning_rate': 0.0003656846955243802, 'batch_size': 158, 'step_size': 14, 'gamma': 0.8215879054962223}. Best is trial 2 with value: 0.023275670522869562.[0m
[32m[I 2025-01-05 02:35:55,979][0m Trial 49 finished with value: 0.39901780822781574 and parameters: {'observation_period_num': 202, 'train_rates': 0.7872713147376392, 'learning_rate': 1.0370534653817496e-05, 'batch_size': 121, 'step_size': 5, 'gamma': 0.8311282789741822}. Best is trial 2 with value: 0.023275670522869562.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 02:35:55,990][0m A new study created in memory with name: no-name-36f173fb-8daa-4572-bde2-a022a0ad2f3b[0m
[32m[I 2025-01-05 02:36:25,228][0m Trial 0 finished with value: 0.12065186536351884 and parameters: {'observation_period_num': 40, 'train_rates': 0.6915486760679076, 'learning_rate': 2.47526562906483e-05, 'batch_size': 171, 'step_size': 9, 'gamma': 0.8260637397606204}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:37:06,666][0m Trial 1 finished with value: 0.15136862829691025 and parameters: {'observation_period_num': 174, 'train_rates': 0.6311757355886869, 'learning_rate': 0.0003084908866436375, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9464355268741698}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:37:45,209][0m Trial 2 finished with value: 0.27035147624174377 and parameters: {'observation_period_num': 163, 'train_rates': 0.8491073836833101, 'learning_rate': 3.5594573936083024e-05, 'batch_size': 146, 'step_size': 3, 'gamma': 0.8677458452662182}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:38:10,219][0m Trial 3 finished with value: 0.26838794888162903 and parameters: {'observation_period_num': 185, 'train_rates': 0.7055697507071816, 'learning_rate': 0.000519204508173098, 'batch_size': 197, 'step_size': 10, 'gamma': 0.9222035508070565}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:38:39,079][0m Trial 4 finished with value: 0.2859690204616194 and parameters: {'observation_period_num': 96, 'train_rates': 0.7199286278971422, 'learning_rate': 1.0117699134948032e-05, 'batch_size': 176, 'step_size': 7, 'gamma': 0.9554106389202437}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:39:05,384][0m Trial 5 finished with value: 0.18807066639811257 and parameters: {'observation_period_num': 244, 'train_rates': 0.7587788302337266, 'learning_rate': 0.0002617946383155188, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8528110477808621}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:39:29,015][0m Trial 6 finished with value: 0.39465170014860496 and parameters: {'observation_period_num': 112, 'train_rates': 0.6268965526254295, 'learning_rate': 0.00012743662232656512, 'batch_size': 207, 'step_size': 5, 'gamma': 0.9444307374655443}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:40:38,773][0m Trial 7 finished with value: 0.47731754085086037 and parameters: {'observation_period_num': 133, 'train_rates': 0.9465033589524324, 'learning_rate': 3.0946068219014866e-06, 'batch_size': 83, 'step_size': 11, 'gamma': 0.8656456749381918}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:42:04,164][0m Trial 8 finished with value: 0.15035550822229946 and parameters: {'observation_period_num': 234, 'train_rates': 0.8753153039183887, 'learning_rate': 2.4254214886253802e-05, 'batch_size': 60, 'step_size': 9, 'gamma': 0.8892204770807319}. Best is trial 0 with value: 0.12065186536351884.[0m
[32m[I 2025-01-05 02:42:29,369][0m Trial 9 finished with value: 0.07317653351075241 and parameters: {'observation_period_num': 53, 'train_rates': 0.6833422889728666, 'learning_rate': 6.849106971867191e-05, 'batch_size': 215, 'step_size': 15, 'gamma': 0.9596487007507113}. Best is trial 9 with value: 0.07317653351075241.[0m
[32m[I 2025-01-05 02:42:53,129][0m Trial 10 finished with value: 0.7980443238164503 and parameters: {'observation_period_num': 5, 'train_rates': 0.8071429755068482, 'learning_rate': 1.1317279315092e-06, 'batch_size': 247, 'step_size': 15, 'gamma': 0.7545498318064666}. Best is trial 9 with value: 0.07317653351075241.[0m
Early stopping at epoch 74
[32m[I 2025-01-05 02:46:29,819][0m Trial 11 finished with value: 0.10900317500778336 and parameters: {'observation_period_num': 30, 'train_rates': 0.6985656345343076, 'learning_rate': 6.392223306828214e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.7947636597177063}. Best is trial 9 with value: 0.07317653351075241.[0m
Early stopping at epoch 88
[32m[I 2025-01-05 02:49:45,541][0m Trial 12 finished with value: 0.17957988470973235 and parameters: {'observation_period_num': 62, 'train_rates': 0.6002728232160508, 'learning_rate': 8.81098261501916e-05, 'batch_size': 19, 'step_size': 1, 'gamma': 0.7947227134024873}. Best is trial 9 with value: 0.07317653351075241.[0m
[32m[I 2025-01-05 02:50:05,432][0m Trial 13 finished with value: 0.07089709205505175 and parameters: {'observation_period_num': 8, 'train_rates': 0.6689885971300215, 'learning_rate': 7.795799845756452e-05, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9868860396638516}. Best is trial 13 with value: 0.07089709205505175.[0m
[32m[I 2025-01-05 02:50:25,387][0m Trial 14 finished with value: 0.3369816070930524 and parameters: {'observation_period_num': 75, 'train_rates': 0.6637825953767562, 'learning_rate': 8.477801981150841e-06, 'batch_size': 253, 'step_size': 6, 'gamma': 0.986858864081396}. Best is trial 13 with value: 0.07089709205505175.[0m
[32m[I 2025-01-05 02:50:48,917][0m Trial 15 finished with value: 0.061577878329488966 and parameters: {'observation_period_num': 15, 'train_rates': 0.770332043147314, 'learning_rate': 0.0008126041028408349, 'batch_size': 228, 'step_size': 4, 'gamma': 0.984737842181091}. Best is trial 15 with value: 0.061577878329488966.[0m
[32m[I 2025-01-05 02:51:13,015][0m Trial 16 finished with value: 0.029391385905299407 and parameters: {'observation_period_num': 12, 'train_rates': 0.7733978160723193, 'learning_rate': 0.0007627469257603658, 'batch_size': 235, 'step_size': 4, 'gamma': 0.9838894010560439}. Best is trial 16 with value: 0.029391385905299407.[0m
[32m[I 2025-01-05 02:51:36,822][0m Trial 17 finished with value: 0.06378679807989969 and parameters: {'observation_period_num': 82, 'train_rates': 0.7723203744811543, 'learning_rate': 0.0008877581399370966, 'batch_size': 229, 'step_size': 3, 'gamma': 0.9054648355662196}. Best is trial 16 with value: 0.029391385905299407.[0m
[32m[I 2025-01-05 02:52:17,690][0m Trial 18 finished with value: 0.0341997864888981 and parameters: {'observation_period_num': 32, 'train_rates': 0.8358229900363101, 'learning_rate': 0.0009265089361985604, 'batch_size': 138, 'step_size': 3, 'gamma': 0.919391963771505}. Best is trial 16 with value: 0.029391385905299407.[0m
[32m[I 2025-01-05 02:53:08,073][0m Trial 19 finished with value: 0.05381766418774289 and parameters: {'observation_period_num': 44, 'train_rates': 0.9138167440653677, 'learning_rate': 0.0002183550133973318, 'batch_size': 120, 'step_size': 2, 'gamma': 0.9200887732311354}. Best is trial 16 with value: 0.029391385905299407.[0m
[32m[I 2025-01-05 02:53:44,699][0m Trial 20 finished with value: 0.06711330294609069 and parameters: {'observation_period_num': 135, 'train_rates': 0.8226563358375727, 'learning_rate': 0.0004342784588975258, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8911737466495698}. Best is trial 16 with value: 0.029391385905299407.[0m
[32m[I 2025-01-05 02:54:32,924][0m Trial 21 finished with value: 0.043738156056671 and parameters: {'observation_period_num': 35, 'train_rates': 0.9084344169550455, 'learning_rate': 0.0001894515571352084, 'batch_size': 121, 'step_size': 2, 'gamma': 0.9268659717125355}. Best is trial 16 with value: 0.029391385905299407.[0m
[32m[I 2025-01-05 02:55:42,723][0m Trial 22 finished with value: 0.02198999933898449 and parameters: {'observation_period_num': 29, 'train_rates': 0.9791351857336593, 'learning_rate': 0.0008694013886418433, 'batch_size': 89, 'step_size': 3, 'gamma': 0.927533561677686}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 02:57:08,218][0m Trial 23 finished with value: 0.04393688945666603 and parameters: {'observation_period_num': 23, 'train_rates': 0.9608314144567801, 'learning_rate': 0.0009103512121597824, 'batch_size': 70, 'step_size': 4, 'gamma': 0.9667989700098026}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 02:59:31,427][0m Trial 24 finished with value: 0.0756585132330656 and parameters: {'observation_period_num': 64, 'train_rates': 0.9765295441735239, 'learning_rate': 0.0004879697267774452, 'batch_size': 41, 'step_size': 3, 'gamma': 0.9339277306761606}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:00:23,852][0m Trial 25 finished with value: 0.07178626525843435 and parameters: {'observation_period_num': 95, 'train_rates': 0.7425276092953013, 'learning_rate': 0.0005242759590508999, 'batch_size': 93, 'step_size': 7, 'gamma': 0.8943960363026797}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:00:57,845][0m Trial 26 finished with value: 0.03801585010491138 and parameters: {'observation_period_num': 24, 'train_rates': 0.85322380058469, 'learning_rate': 0.00015067677846165658, 'batch_size': 173, 'step_size': 4, 'gamma': 0.9677463444924869}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:01:45,247][0m Trial 27 finished with value: 0.04610107848305761 and parameters: {'observation_period_num': 53, 'train_rates': 0.9164391552121213, 'learning_rate': 0.0009999231152119628, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9096153642328919}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:03:25,593][0m Trial 28 finished with value: 0.07071756740272668 and parameters: {'observation_period_num': 85, 'train_rates': 0.8001941232113243, 'learning_rate': 0.00034466379050086457, 'batch_size': 50, 'step_size': 5, 'gamma': 0.9379545999215507}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:04:03,392][0m Trial 29 finished with value: 0.03919837780359412 and parameters: {'observation_period_num': 50, 'train_rates': 0.8773244904452258, 'learning_rate': 0.0006361004612335464, 'batch_size': 157, 'step_size': 7, 'gamma': 0.8382845129132392}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:04:33,441][0m Trial 30 finished with value: 0.12854516629515023 and parameters: {'observation_period_num': 212, 'train_rates': 0.8416233092639257, 'learning_rate': 0.0002947697110438678, 'batch_size': 183, 'step_size': 1, 'gamma': 0.9708716442941318}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:05:09,831][0m Trial 31 finished with value: 0.037033908021495775 and parameters: {'observation_period_num': 34, 'train_rates': 0.8663732585974848, 'learning_rate': 0.00014177549754621963, 'batch_size': 164, 'step_size': 4, 'gamma': 0.9713808984687435}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:06:09,355][0m Trial 32 finished with value: 0.05603586581159145 and parameters: {'observation_period_num': 39, 'train_rates': 0.9357474616161919, 'learning_rate': 0.0006346392630588691, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9497801819763217}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:06:58,430][0m Trial 33 finished with value: 0.03496239334344864 and parameters: {'observation_period_num': 23, 'train_rates': 0.9875411092546558, 'learning_rate': 0.00034279896276036775, 'batch_size': 129, 'step_size': 4, 'gamma': 0.9683643911946255}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:07:45,729][0m Trial 34 finished with value: 0.02740670181810856 and parameters: {'observation_period_num': 13, 'train_rates': 0.9847455617270128, 'learning_rate': 0.00045067029610566757, 'batch_size': 135, 'step_size': 6, 'gamma': 0.9121556998260276}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:08:55,226][0m Trial 35 finished with value: 0.029860114540766786 and parameters: {'observation_period_num': 7, 'train_rates': 0.9635103923731149, 'learning_rate': 0.0006049221728140572, 'batch_size': 86, 'step_size': 6, 'gamma': 0.8747997347788654}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:09:51,475][0m Trial 36 finished with value: 0.06966552702942863 and parameters: {'observation_period_num': 5, 'train_rates': 0.9569146279933785, 'learning_rate': 3.455826072165456e-05, 'batch_size': 110, 'step_size': 8, 'gamma': 0.8782307829321271}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:11:07,054][0m Trial 37 finished with value: 0.022264860570430756 and parameters: {'observation_period_num': 16, 'train_rates': 0.9748416062550758, 'learning_rate': 0.00039011636192507463, 'batch_size': 80, 'step_size': 6, 'gamma': 0.8596756550461547}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:12:23,888][0m Trial 38 finished with value: 0.12844671308994293 and parameters: {'observation_period_num': 166, 'train_rates': 0.9888816529826757, 'learning_rate': 0.00038556383993213866, 'batch_size': 75, 'step_size': 8, 'gamma': 0.842768487990823}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:13:16,338][0m Trial 39 finished with value: 0.17504762047428196 and parameters: {'observation_period_num': 150, 'train_rates': 0.894852514194728, 'learning_rate': 1.8834517059596622e-05, 'batch_size': 107, 'step_size': 11, 'gamma': 0.8155597331015383}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:13:49,188][0m Trial 40 finished with value: 0.07183428108692169 and parameters: {'observation_period_num': 66, 'train_rates': 0.9367238991574909, 'learning_rate': 0.0002264522654223402, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8587092673626097}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:14:58,722][0m Trial 41 finished with value: 0.0285586539345483 and parameters: {'observation_period_num': 16, 'train_rates': 0.9674908274166343, 'learning_rate': 0.0006113919602413885, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8708581952508341}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:16:25,180][0m Trial 42 finished with value: 0.040440976320199634 and parameters: {'observation_period_num': 17, 'train_rates': 0.9322409359936318, 'learning_rate': 0.0006510633471235222, 'batch_size': 68, 'step_size': 6, 'gamma': 0.9046314808276084}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:17:10,377][0m Trial 43 finished with value: 0.0322142094373703 and parameters: {'observation_period_num': 17, 'train_rates': 0.9728872660717851, 'learning_rate': 0.00028429971318646015, 'batch_size': 142, 'step_size': 9, 'gamma': 0.8257578771172853}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:19:29,231][0m Trial 44 finished with value: 0.04918600096186595 and parameters: {'observation_period_num': 44, 'train_rates': 0.954200809737278, 'learning_rate': 0.00043113977302020154, 'batch_size': 42, 'step_size': 7, 'gamma': 0.8791185432994305}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:20:23,997][0m Trial 45 finished with value: 0.11436194656303351 and parameters: {'observation_period_num': 198, 'train_rates': 0.7319907654972595, 'learning_rate': 0.00017728126902872855, 'batch_size': 86, 'step_size': 5, 'gamma': 0.8505751636748138}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:21:55,880][0m Trial 46 finished with value: 0.09460944977480096 and parameters: {'observation_period_num': 120, 'train_rates': 0.8914137442571681, 'learning_rate': 0.0007014761376859996, 'batch_size': 60, 'step_size': 6, 'gamma': 0.8648122888919132}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:22:58,846][0m Trial 47 finished with value: 0.3063020706176758 and parameters: {'observation_period_num': 15, 'train_rates': 0.9897015394381529, 'learning_rate': 3.3475533358121582e-06, 'batch_size': 99, 'step_size': 10, 'gamma': 0.8858453003226098}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:23:46,760][0m Trial 48 finished with value: 0.053207867820627794 and parameters: {'observation_period_num': 57, 'train_rates': 0.7847030265764036, 'learning_rate': 0.00011009632034073636, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9000052275750294}. Best is trial 22 with value: 0.02198999933898449.[0m
[32m[I 2025-01-05 03:25:05,177][0m Trial 49 finished with value: 0.046338088012167385 and parameters: {'observation_period_num': 28, 'train_rates': 0.942570699149735, 'learning_rate': 0.00048176600578801444, 'batch_size': 76, 'step_size': 5, 'gamma': 0.7995390285435078}. Best is trial 22 with value: 0.02198999933898449.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 17, 'train_rates': 0.9697992514871054, 'learning_rate': 0.0002900878964956585, 'batch_size': 62, 'step_size': 8, 'gamma': 0.9284884961277896}
Epoch 1/300, trend Loss: 0.3520 | 0.2086
Epoch 2/300, trend Loss: 0.1422 | 0.1212
Epoch 3/300, trend Loss: 0.1185 | 0.0928
Epoch 4/300, trend Loss: 0.1065 | 0.1039
Epoch 5/300, trend Loss: 0.1061 | 0.0806
Epoch 6/300, trend Loss: 0.0969 | 0.0762
Epoch 7/300, trend Loss: 0.0967 | 0.0733
Epoch 8/300, trend Loss: 0.0952 | 0.0714
Epoch 9/300, trend Loss: 0.0969 | 0.0854
Epoch 10/300, trend Loss: 0.1009 | 0.1250
Epoch 11/300, trend Loss: 0.1020 | 0.1061
Epoch 12/300, trend Loss: 0.0949 | 0.0693
Epoch 13/300, trend Loss: 0.0916 | 0.0591
Epoch 14/300, trend Loss: 0.0888 | 0.0548
Epoch 15/300, trend Loss: 0.0850 | 0.0519
Epoch 16/300, trend Loss: 0.0837 | 0.0495
Epoch 17/300, trend Loss: 0.0825 | 0.0479
Epoch 18/300, trend Loss: 0.0830 | 0.0477
Epoch 19/300, trend Loss: 0.0814 | 0.0472
Epoch 20/300, trend Loss: 0.0786 | 0.0468
Epoch 21/300, trend Loss: 0.0777 | 0.0467
Epoch 22/300, trend Loss: 0.0766 | 0.0480
Epoch 23/300, trend Loss: 0.0772 | 0.0483
Epoch 24/300, trend Loss: 0.0754 | 0.0509
Epoch 25/300, trend Loss: 0.0745 | 0.0566
Epoch 26/300, trend Loss: 0.0752 | 0.0541
Epoch 27/300, trend Loss: 0.0874 | 0.0819
Epoch 28/300, trend Loss: 0.0892 | 0.0470
Epoch 29/300, trend Loss: 0.0762 | 0.0424
Epoch 30/300, trend Loss: 0.0726 | 0.0409
Epoch 31/300, trend Loss: 0.0705 | 0.0395
Epoch 32/300, trend Loss: 0.0686 | 0.0384
Epoch 33/300, trend Loss: 0.0672 | 0.0374
Epoch 34/300, trend Loss: 0.0661 | 0.0373
Epoch 35/300, trend Loss: 0.0649 | 0.0370
Epoch 36/300, trend Loss: 0.0637 | 0.0365
Epoch 37/300, trend Loss: 0.0628 | 0.0356
Epoch 38/300, trend Loss: 0.0621 | 0.0347
Epoch 39/300, trend Loss: 0.0614 | 0.0336
Epoch 40/300, trend Loss: 0.0609 | 0.0325
Epoch 41/300, trend Loss: 0.0605 | 0.0309
Epoch 42/300, trend Loss: 0.0606 | 0.0299
Epoch 43/300, trend Loss: 0.0607 | 0.0292
Epoch 44/300, trend Loss: 0.0609 | 0.0292
Epoch 45/300, trend Loss: 0.0610 | 0.0292
Epoch 46/300, trend Loss: 0.0607 | 0.0289
Epoch 47/300, trend Loss: 0.0602 | 0.0288
Epoch 48/300, trend Loss: 0.0599 | 0.0288
Epoch 49/300, trend Loss: 0.0597 | 0.0301
Epoch 50/300, trend Loss: 0.0594 | 0.0296
Epoch 51/300, trend Loss: 0.0588 | 0.0293
Epoch 52/300, trend Loss: 0.0582 | 0.0293
Epoch 53/300, trend Loss: 0.0580 | 0.0306
Epoch 54/300, trend Loss: 0.0578 | 0.0308
Epoch 55/300, trend Loss: 0.0573 | 0.0314
Epoch 56/300, trend Loss: 0.0568 | 0.0316
Epoch 57/300, trend Loss: 0.0566 | 0.0329
Epoch 58/300, trend Loss: 0.0568 | 0.0339
Epoch 59/300, trend Loss: 0.0575 | 0.0354
Epoch 60/300, trend Loss: 0.0587 | 0.0356
Epoch 61/300, trend Loss: 0.0603 | 0.0336
Epoch 62/300, trend Loss: 0.0623 | 0.0314
Epoch 63/300, trend Loss: 0.0651 | 0.0334
Epoch 64/300, trend Loss: 0.0657 | 0.0355
Epoch 65/300, trend Loss: 0.0655 | 0.0366
Epoch 66/300, trend Loss: 0.0677 | 0.0383
Epoch 67/300, trend Loss: 0.0661 | 0.0345
Epoch 68/300, trend Loss: 0.0640 | 0.0335
Epoch 69/300, trend Loss: 0.0631 | 0.0329
Epoch 70/300, trend Loss: 0.0604 | 0.0312
Epoch 71/300, trend Loss: 0.0577 | 0.0305
Epoch 72/300, trend Loss: 0.0559 | 0.0303
Epoch 73/300, trend Loss: 0.0550 | 0.0303
Epoch 74/300, trend Loss: 0.0547 | 0.0300
Epoch 75/300, trend Loss: 0.0550 | 0.0298
Epoch 76/300, trend Loss: 0.0554 | 0.0299
Epoch 77/300, trend Loss: 0.0560 | 0.0318
Epoch 78/300, trend Loss: 0.0559 | 0.0331
Epoch 79/300, trend Loss: 0.0547 | 0.0328
Epoch 80/300, trend Loss: 0.0537 | 0.0321
Epoch 81/300, trend Loss: 0.0532 | 0.0310
Epoch 82/300, trend Loss: 0.0530 | 0.0308
Epoch 83/300, trend Loss: 0.0532 | 0.0309
Epoch 84/300, trend Loss: 0.0536 | 0.0307
Epoch 85/300, trend Loss: 0.0541 | 0.0299
Epoch 86/300, trend Loss: 0.0547 | 0.0296
Epoch 87/300, trend Loss: 0.0551 | 0.0295
Epoch 88/300, trend Loss: 0.0561 | 0.0316
Epoch 89/300, trend Loss: 0.0559 | 0.0309
Epoch 90/300, trend Loss: 0.0561 | 0.0299
Epoch 91/300, trend Loss: 0.0572 | 0.0296
Epoch 92/300, trend Loss: 0.0592 | 0.0291
Epoch 93/300, trend Loss: 0.0620 | 0.0339
Epoch 94/300, trend Loss: 0.0652 | 0.0491
Epoch 95/300, trend Loss: 0.0661 | 0.0652
Epoch 96/300, trend Loss: 0.0628 | 0.0664
Epoch 97/300, trend Loss: 0.0625 | 0.0333
Epoch 98/300, trend Loss: 0.0690 | 0.0461
Epoch 99/300, trend Loss: 0.0688 | 0.0305
Epoch 100/300, trend Loss: 0.0587 | 0.0288
Epoch 101/300, trend Loss: 0.0569 | 0.0274
Epoch 102/300, trend Loss: 0.0556 | 0.0275
Epoch 103/300, trend Loss: 0.0537 | 0.0274
Epoch 104/300, trend Loss: 0.0518 | 0.0273
Epoch 105/300, trend Loss: 0.0505 | 0.0272
Epoch 106/300, trend Loss: 0.0498 | 0.0267
Epoch 107/300, trend Loss: 0.0498 | 0.0260
Epoch 108/300, trend Loss: 0.0499 | 0.0253
Epoch 109/300, trend Loss: 0.0499 | 0.0248
Epoch 110/300, trend Loss: 0.0495 | 0.0246
Epoch 111/300, trend Loss: 0.0490 | 0.0245
Epoch 112/300, trend Loss: 0.0489 | 0.0246
Epoch 113/300, trend Loss: 0.0492 | 0.0251
Epoch 114/300, trend Loss: 0.0496 | 0.0257
Epoch 115/300, trend Loss: 0.0498 | 0.0261
Epoch 116/300, trend Loss: 0.0496 | 0.0263
Epoch 117/300, trend Loss: 0.0494 | 0.0264
Epoch 118/300, trend Loss: 0.0493 | 0.0265
Epoch 119/300, trend Loss: 0.0491 | 0.0264
Epoch 120/300, trend Loss: 0.0489 | 0.0260
Epoch 121/300, trend Loss: 0.0486 | 0.0255
Epoch 122/300, trend Loss: 0.0484 | 0.0253
Epoch 123/300, trend Loss: 0.0482 | 0.0252
Epoch 124/300, trend Loss: 0.0480 | 0.0252
Epoch 125/300, trend Loss: 0.0478 | 0.0251
Epoch 126/300, trend Loss: 0.0477 | 0.0251
Epoch 127/300, trend Loss: 0.0477 | 0.0251
Epoch 128/300, trend Loss: 0.0477 | 0.0251
Epoch 129/300, trend Loss: 0.0476 | 0.0252
Epoch 130/300, trend Loss: 0.0476 | 0.0252
Epoch 131/300, trend Loss: 0.0474 | 0.0252
Epoch 132/300, trend Loss: 0.0473 | 0.0251
Epoch 133/300, trend Loss: 0.0472 | 0.0250
Epoch 134/300, trend Loss: 0.0471 | 0.0248
Epoch 135/300, trend Loss: 0.0470 | 0.0247
Epoch 136/300, trend Loss: 0.0469 | 0.0246
Epoch 137/300, trend Loss: 0.0468 | 0.0244
Epoch 138/300, trend Loss: 0.0468 | 0.0243
Epoch 139/300, trend Loss: 0.0467 | 0.0243
Epoch 140/300, trend Loss: 0.0466 | 0.0242
Epoch 141/300, trend Loss: 0.0467 | 0.0241
Epoch 142/300, trend Loss: 0.0468 | 0.0241
Epoch 143/300, trend Loss: 0.0471 | 0.0243
Epoch 144/300, trend Loss: 0.0476 | 0.0252
Epoch 145/300, trend Loss: 0.0485 | 0.0248
Epoch 146/300, trend Loss: 0.0485 | 0.0248
Epoch 147/300, trend Loss: 0.0474 | 0.0253
Epoch 148/300, trend Loss: 0.0468 | 0.0254
Epoch 149/300, trend Loss: 0.0465 | 0.0252
Epoch 150/300, trend Loss: 0.0464 | 0.0251
Epoch 151/300, trend Loss: 0.0463 | 0.0251
Epoch 152/300, trend Loss: 0.0461 | 0.0251
Epoch 153/300, trend Loss: 0.0460 | 0.0250
Epoch 154/300, trend Loss: 0.0460 | 0.0250
Epoch 155/300, trend Loss: 0.0460 | 0.0251
Epoch 156/300, trend Loss: 0.0460 | 0.0251
Epoch 157/300, trend Loss: 0.0460 | 0.0252
Epoch 158/300, trend Loss: 0.0460 | 0.0253
Epoch 159/300, trend Loss: 0.0459 | 0.0253
Epoch 160/300, trend Loss: 0.0457 | 0.0253
Epoch 161/300, trend Loss: 0.0456 | 0.0252
Epoch 162/300, trend Loss: 0.0456 | 0.0251
Epoch 163/300, trend Loss: 0.0455 | 0.0250
Epoch 164/300, trend Loss: 0.0454 | 0.0250
Epoch 165/300, trend Loss: 0.0454 | 0.0249
Epoch 166/300, trend Loss: 0.0453 | 0.0248
Epoch 167/300, trend Loss: 0.0452 | 0.0248
Epoch 168/300, trend Loss: 0.0452 | 0.0248
Epoch 169/300, trend Loss: 0.0451 | 0.0249
Epoch 170/300, trend Loss: 0.0451 | 0.0249
Epoch 171/300, trend Loss: 0.0451 | 0.0249
Epoch 172/300, trend Loss: 0.0450 | 0.0249
Epoch 173/300, trend Loss: 0.0450 | 0.0249
Epoch 174/300, trend Loss: 0.0449 | 0.0248
Epoch 175/300, trend Loss: 0.0449 | 0.0248
Epoch 176/300, trend Loss: 0.0448 | 0.0248
Epoch 177/300, trend Loss: 0.0448 | 0.0247
Epoch 178/300, trend Loss: 0.0448 | 0.0247
Epoch 179/300, trend Loss: 0.0447 | 0.0247
Epoch 180/300, trend Loss: 0.0447 | 0.0247
Epoch 181/300, trend Loss: 0.0446 | 0.0247
Epoch 182/300, trend Loss: 0.0446 | 0.0247
Epoch 183/300, trend Loss: 0.0446 | 0.0247
Epoch 184/300, trend Loss: 0.0446 | 0.0248
Epoch 185/300, trend Loss: 0.0446 | 0.0248
Epoch 186/300, trend Loss: 0.0446 | 0.0249
Epoch 187/300, trend Loss: 0.0446 | 0.0249
Epoch 188/300, trend Loss: 0.0446 | 0.0250
Epoch 189/300, trend Loss: 0.0446 | 0.0250
Epoch 190/300, trend Loss: 0.0445 | 0.0251
Epoch 191/300, trend Loss: 0.0445 | 0.0251
Epoch 192/300, trend Loss: 0.0444 | 0.0252
Epoch 193/300, trend Loss: 0.0444 | 0.0252
Epoch 194/300, trend Loss: 0.0443 | 0.0253
Epoch 195/300, trend Loss: 0.0443 | 0.0254
Epoch 196/300, trend Loss: 0.0442 | 0.0254
Epoch 197/300, trend Loss: 0.0442 | 0.0254
Epoch 198/300, trend Loss: 0.0442 | 0.0253
Epoch 199/300, trend Loss: 0.0442 | 0.0253
Epoch 200/300, trend Loss: 0.0442 | 0.0252
Epoch 201/300, trend Loss: 0.0442 | 0.0251
Epoch 202/300, trend Loss: 0.0441 | 0.0251
Epoch 203/300, trend Loss: 0.0441 | 0.0250
Epoch 204/300, trend Loss: 0.0441 | 0.0250
Epoch 205/300, trend Loss: 0.0440 | 0.0250
Epoch 206/300, trend Loss: 0.0440 | 0.0250
Epoch 207/300, trend Loss: 0.0439 | 0.0251
Epoch 208/300, trend Loss: 0.0439 | 0.0251
Epoch 209/300, trend Loss: 0.0439 | 0.0251
Epoch 210/300, trend Loss: 0.0439 | 0.0252
Epoch 211/300, trend Loss: 0.0439 | 0.0252
Epoch 212/300, trend Loss: 0.0439 | 0.0252
Epoch 213/300, trend Loss: 0.0439 | 0.0252
Epoch 214/300, trend Loss: 0.0438 | 0.0253
Epoch 215/300, trend Loss: 0.0438 | 0.0253
Epoch 216/300, trend Loss: 0.0437 | 0.0253
Epoch 217/300, trend Loss: 0.0437 | 0.0253
Epoch 218/300, trend Loss: 0.0437 | 0.0253
Epoch 219/300, trend Loss: 0.0436 | 0.0253
Epoch 220/300, trend Loss: 0.0436 | 0.0253
Epoch 221/300, trend Loss: 0.0436 | 0.0253
Epoch 222/300, trend Loss: 0.0436 | 0.0253
Epoch 223/300, trend Loss: 0.0436 | 0.0253
Epoch 224/300, trend Loss: 0.0435 | 0.0253
Epoch 225/300, trend Loss: 0.0435 | 0.0253
Epoch 226/300, trend Loss: 0.0435 | 0.0253
Epoch 227/300, trend Loss: 0.0435 | 0.0253
Epoch 228/300, trend Loss: 0.0435 | 0.0253
Epoch 229/300, trend Loss: 0.0435 | 0.0253
Epoch 230/300, trend Loss: 0.0434 | 0.0253
Epoch 231/300, trend Loss: 0.0434 | 0.0253
Epoch 232/300, trend Loss: 0.0434 | 0.0254
Epoch 233/300, trend Loss: 0.0434 | 0.0254
Epoch 234/300, trend Loss: 0.0434 | 0.0254
Epoch 235/300, trend Loss: 0.0434 | 0.0254
Epoch 236/300, trend Loss: 0.0434 | 0.0254
Epoch 237/300, trend Loss: 0.0433 | 0.0254
Epoch 238/300, trend Loss: 0.0433 | 0.0254
Epoch 239/300, trend Loss: 0.0433 | 0.0254
Epoch 240/300, trend Loss: 0.0433 | 0.0254
Epoch 241/300, trend Loss: 0.0433 | 0.0254
Epoch 242/300, trend Loss: 0.0433 | 0.0254
Epoch 243/300, trend Loss: 0.0433 | 0.0254
Epoch 244/300, trend Loss: 0.0432 | 0.0254
Epoch 245/300, trend Loss: 0.0432 | 0.0254
Epoch 246/300, trend Loss: 0.0432 | 0.0254
Epoch 247/300, trend Loss: 0.0432 | 0.0254
Epoch 248/300, trend Loss: 0.0432 | 0.0254
Epoch 249/300, trend Loss: 0.0432 | 0.0254
Epoch 250/300, trend Loss: 0.0432 | 0.0254
Epoch 251/300, trend Loss: 0.0432 | 0.0254
Epoch 252/300, trend Loss: 0.0432 | 0.0254
Epoch 253/300, trend Loss: 0.0431 | 0.0254
Epoch 254/300, trend Loss: 0.0431 | 0.0254
Epoch 255/300, trend Loss: 0.0431 | 0.0255
Epoch 256/300, trend Loss: 0.0431 | 0.0255
Epoch 257/300, trend Loss: 0.0431 | 0.0255
Epoch 258/300, trend Loss: 0.0431 | 0.0255
Epoch 259/300, trend Loss: 0.0431 | 0.0255
Epoch 260/300, trend Loss: 0.0431 | 0.0255
Epoch 261/300, trend Loss: 0.0431 | 0.0255
Epoch 262/300, trend Loss: 0.0431 | 0.0255
Epoch 263/300, trend Loss: 0.0430 | 0.0255
Epoch 264/300, trend Loss: 0.0430 | 0.0255
Epoch 265/300, trend Loss: 0.0430 | 0.0255
Epoch 266/300, trend Loss: 0.0430 | 0.0255
Epoch 267/300, trend Loss: 0.0430 | 0.0255
Epoch 268/300, trend Loss: 0.0430 | 0.0255
Epoch 269/300, trend Loss: 0.0430 | 0.0255
Epoch 270/300, trend Loss: 0.0430 | 0.0255
Epoch 271/300, trend Loss: 0.0430 | 0.0255
Epoch 272/300, trend Loss: 0.0430 | 0.0255
Epoch 273/300, trend Loss: 0.0430 | 0.0255
Epoch 274/300, trend Loss: 0.0430 | 0.0255
Epoch 275/300, trend Loss: 0.0430 | 0.0255
Epoch 276/300, trend Loss: 0.0429 | 0.0255
Epoch 277/300, trend Loss: 0.0429 | 0.0255
Epoch 278/300, trend Loss: 0.0429 | 0.0255
Epoch 279/300, trend Loss: 0.0429 | 0.0255
Epoch 280/300, trend Loss: 0.0429 | 0.0255
Epoch 281/300, trend Loss: 0.0429 | 0.0255
Epoch 282/300, trend Loss: 0.0429 | 0.0255
Epoch 283/300, trend Loss: 0.0429 | 0.0255
Epoch 284/300, trend Loss: 0.0429 | 0.0255
Epoch 285/300, trend Loss: 0.0429 | 0.0255
Epoch 286/300, trend Loss: 0.0429 | 0.0255
Epoch 287/300, trend Loss: 0.0429 | 0.0256
Epoch 288/300, trend Loss: 0.0429 | 0.0256
Epoch 289/300, trend Loss: 0.0429 | 0.0256
Epoch 290/300, trend Loss: 0.0429 | 0.0256
Epoch 291/300, trend Loss: 0.0429 | 0.0256
Epoch 292/300, trend Loss: 0.0429 | 0.0256
Epoch 293/300, trend Loss: 0.0428 | 0.0256
Epoch 294/300, trend Loss: 0.0428 | 0.0256
Epoch 295/300, trend Loss: 0.0428 | 0.0256
Epoch 296/300, trend Loss: 0.0428 | 0.0256
Epoch 297/300, trend Loss: 0.0428 | 0.0256
Epoch 298/300, trend Loss: 0.0428 | 0.0256
Epoch 299/300, trend Loss: 0.0428 | 0.0256
Epoch 300/300, trend Loss: 0.0428 | 0.0256
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.9250931976262937, 'learning_rate': 0.0008850397661873524, 'batch_size': 52, 'step_size': 15, 'gamma': 0.8487874204480693}
Epoch 1/300, seasonal_0 Loss: 0.2734 | 0.1032
Epoch 2/300, seasonal_0 Loss: 0.1106 | 0.0830
Epoch 3/300, seasonal_0 Loss: 0.0981 | 0.0676
Epoch 4/300, seasonal_0 Loss: 0.0942 | 0.0626
Epoch 5/300, seasonal_0 Loss: 0.0912 | 0.0586
Epoch 6/300, seasonal_0 Loss: 0.0883 | 0.0498
Epoch 7/300, seasonal_0 Loss: 0.0892 | 0.0542
Epoch 8/300, seasonal_0 Loss: 0.0848 | 0.0496
Epoch 9/300, seasonal_0 Loss: 0.0866 | 0.0595
Epoch 10/300, seasonal_0 Loss: 0.0842 | 0.0543
Epoch 11/300, seasonal_0 Loss: 0.0785 | 0.0495
Epoch 12/300, seasonal_0 Loss: 0.0761 | 0.0446
Epoch 13/300, seasonal_0 Loss: 0.0746 | 0.0426
Epoch 14/300, seasonal_0 Loss: 0.0735 | 0.0420
Epoch 15/300, seasonal_0 Loss: 0.0721 | 0.0411
Epoch 16/300, seasonal_0 Loss: 0.0697 | 0.0380
Epoch 17/300, seasonal_0 Loss: 0.0688 | 0.0393
Epoch 18/300, seasonal_0 Loss: 0.0679 | 0.0356
Epoch 19/300, seasonal_0 Loss: 0.0668 | 0.0362
Epoch 20/300, seasonal_0 Loss: 0.0663 | 0.0358
Epoch 21/300, seasonal_0 Loss: 0.0662 | 0.0362
Epoch 22/300, seasonal_0 Loss: 0.0653 | 0.0361
Epoch 23/300, seasonal_0 Loss: 0.0653 | 0.0366
Epoch 24/300, seasonal_0 Loss: 0.0637 | 0.0360
Epoch 25/300, seasonal_0 Loss: 0.0632 | 0.0340
Epoch 26/300, seasonal_0 Loss: 0.0619 | 0.0356
Epoch 27/300, seasonal_0 Loss: 0.0618 | 0.0360
Epoch 28/300, seasonal_0 Loss: 0.0620 | 0.0319
Epoch 29/300, seasonal_0 Loss: 0.0628 | 0.0335
Epoch 30/300, seasonal_0 Loss: 0.0645 | 0.0338
Epoch 31/300, seasonal_0 Loss: 0.0617 | 0.0325
Epoch 32/300, seasonal_0 Loss: 0.0593 | 0.0344
Epoch 33/300, seasonal_0 Loss: 0.0616 | 0.0383
Epoch 34/300, seasonal_0 Loss: 0.0596 | 0.0309
Epoch 35/300, seasonal_0 Loss: 0.0580 | 0.0347
Epoch 36/300, seasonal_0 Loss: 0.0583 | 0.0374
Epoch 37/300, seasonal_0 Loss: 0.0599 | 0.0319
Epoch 38/300, seasonal_0 Loss: 0.0572 | 0.0355
Epoch 39/300, seasonal_0 Loss: 0.0567 | 0.0369
Epoch 40/300, seasonal_0 Loss: 0.0575 | 0.0325
Epoch 41/300, seasonal_0 Loss: 0.0562 | 0.0290
Epoch 42/300, seasonal_0 Loss: 0.0591 | 0.0321
Epoch 43/300, seasonal_0 Loss: 0.0572 | 0.0326
Epoch 44/300, seasonal_0 Loss: 0.0559 | 0.0316
Epoch 45/300, seasonal_0 Loss: 0.0560 | 0.0366
Epoch 46/300, seasonal_0 Loss: 0.0571 | 0.0357
Epoch 47/300, seasonal_0 Loss: 0.0569 | 0.0333
Epoch 48/300, seasonal_0 Loss: 0.0554 | 0.0341
Epoch 49/300, seasonal_0 Loss: 0.0548 | 0.0332
Epoch 50/300, seasonal_0 Loss: 0.0598 | 0.0369
Epoch 51/300, seasonal_0 Loss: 0.0570 | 0.0351
Epoch 52/300, seasonal_0 Loss: 0.0540 | 0.0358
Epoch 53/300, seasonal_0 Loss: 0.0544 | 0.0350
Epoch 54/300, seasonal_0 Loss: 0.0532 | 0.0312
Epoch 55/300, seasonal_0 Loss: 0.0523 | 0.0304
Epoch 56/300, seasonal_0 Loss: 0.0521 | 0.0311
Epoch 57/300, seasonal_0 Loss: 0.0517 | 0.0302
Epoch 58/300, seasonal_0 Loss: 0.0515 | 0.0305
Epoch 59/300, seasonal_0 Loss: 0.0512 | 0.0301
Epoch 60/300, seasonal_0 Loss: 0.0513 | 0.0311
Epoch 61/300, seasonal_0 Loss: 0.0515 | 0.0310
Epoch 62/300, seasonal_0 Loss: 0.0519 | 0.0307
Epoch 63/300, seasonal_0 Loss: 0.0511 | 0.0313
Epoch 64/300, seasonal_0 Loss: 0.0505 | 0.0306
Epoch 65/300, seasonal_0 Loss: 0.0501 | 0.0312
Epoch 66/300, seasonal_0 Loss: 0.0496 | 0.0302
Epoch 67/300, seasonal_0 Loss: 0.0488 | 0.0308
Epoch 68/300, seasonal_0 Loss: 0.0491 | 0.0307
Epoch 69/300, seasonal_0 Loss: 0.0512 | 0.0303
Epoch 70/300, seasonal_0 Loss: 0.0502 | 0.0302
Epoch 71/300, seasonal_0 Loss: 0.0498 | 0.0306
Epoch 72/300, seasonal_0 Loss: 0.0509 | 0.0313
Epoch 73/300, seasonal_0 Loss: 0.0497 | 0.0308
Epoch 74/300, seasonal_0 Loss: 0.0502 | 0.0299
Epoch 75/300, seasonal_0 Loss: 0.0476 | 0.0296
Epoch 76/300, seasonal_0 Loss: 0.0464 | 0.0297
Epoch 77/300, seasonal_0 Loss: 0.0493 | 0.0292
Epoch 78/300, seasonal_0 Loss: 0.0484 | 0.0303
Epoch 79/300, seasonal_0 Loss: 0.0479 | 0.0304
Epoch 80/300, seasonal_0 Loss: 0.0478 | 0.0308
Epoch 81/300, seasonal_0 Loss: 0.0479 | 0.0318
Epoch 82/300, seasonal_0 Loss: 0.0444 | 0.0309
Epoch 83/300, seasonal_0 Loss: 0.0494 | 0.0317
Epoch 84/300, seasonal_0 Loss: 0.0589 | 0.0356
Epoch 85/300, seasonal_0 Loss: 0.0563 | 0.0315
Epoch 86/300, seasonal_0 Loss: 0.0520 | 0.0291
Epoch 87/300, seasonal_0 Loss: 0.0505 | 0.0320
Epoch 88/300, seasonal_0 Loss: 0.0492 | 0.0300
Epoch 89/300, seasonal_0 Loss: 0.0486 | 0.0308
Epoch 90/300, seasonal_0 Loss: 0.0480 | 0.0307
Epoch 91/300, seasonal_0 Loss: 0.0477 | 0.0310
Epoch 92/300, seasonal_0 Loss: 0.0477 | 0.0310
Epoch 93/300, seasonal_0 Loss: 0.0477 | 0.0312
Epoch 94/300, seasonal_0 Loss: 0.0476 | 0.0314
Epoch 95/300, seasonal_0 Loss: 0.0473 | 0.0315
Epoch 96/300, seasonal_0 Loss: 0.0472 | 0.0318
Epoch 97/300, seasonal_0 Loss: 0.0471 | 0.0318
Epoch 98/300, seasonal_0 Loss: 0.0470 | 0.0321
Epoch 99/300, seasonal_0 Loss: 0.0469 | 0.0324
Epoch 100/300, seasonal_0 Loss: 0.0473 | 0.0329
Epoch 101/300, seasonal_0 Loss: 0.0471 | 0.0331
Epoch 102/300, seasonal_0 Loss: 0.0472 | 0.0338
Epoch 103/300, seasonal_0 Loss: 0.0468 | 0.0343
Epoch 104/300, seasonal_0 Loss: 0.0465 | 0.0349
Epoch 105/300, seasonal_0 Loss: 0.0459 | 0.0348
Epoch 106/300, seasonal_0 Loss: 0.0451 | 0.0351
Epoch 107/300, seasonal_0 Loss: 0.0446 | 0.0358
Epoch 108/300, seasonal_0 Loss: 0.0441 | 0.0359
Epoch 109/300, seasonal_0 Loss: 0.0439 | 0.0343
Epoch 110/300, seasonal_0 Loss: 0.0471 | 0.0357
Epoch 111/300, seasonal_0 Loss: 0.0439 | 0.0349
Epoch 112/300, seasonal_0 Loss: 0.0461 | 0.0331
Epoch 113/300, seasonal_0 Loss: 0.0449 | 0.0322
Epoch 114/300, seasonal_0 Loss: 0.0438 | 0.0338
Epoch 115/300, seasonal_0 Loss: 0.0437 | 0.0330
Epoch 116/300, seasonal_0 Loss: 0.0428 | 0.0331
Epoch 117/300, seasonal_0 Loss: 0.0429 | 0.0331
Epoch 118/300, seasonal_0 Loss: 0.0420 | 0.0337
Epoch 119/300, seasonal_0 Loss: 0.0418 | 0.0338
Epoch 120/300, seasonal_0 Loss: 0.0416 | 0.0340
Epoch 121/300, seasonal_0 Loss: 0.0415 | 0.0339
Epoch 122/300, seasonal_0 Loss: 0.0415 | 0.0338
Epoch 123/300, seasonal_0 Loss: 0.0415 | 0.0342
Epoch 124/300, seasonal_0 Loss: 0.0414 | 0.0342
Epoch 125/300, seasonal_0 Loss: 0.0414 | 0.0344
Epoch 126/300, seasonal_0 Loss: 0.0414 | 0.0344
Epoch 127/300, seasonal_0 Loss: 0.0415 | 0.0345
Epoch 128/300, seasonal_0 Loss: 0.0415 | 0.0345
Epoch 129/300, seasonal_0 Loss: 0.0417 | 0.0341
Epoch 130/300, seasonal_0 Loss: 0.0421 | 0.0339
Epoch 131/300, seasonal_0 Loss: 0.0423 | 0.0340
Epoch 132/300, seasonal_0 Loss: 0.0424 | 0.0342
Epoch 133/300, seasonal_0 Loss: 0.0426 | 0.0340
Epoch 134/300, seasonal_0 Loss: 0.0422 | 0.0339
Epoch 135/300, seasonal_0 Loss: 0.0418 | 0.0336
Epoch 136/300, seasonal_0 Loss: 0.0415 | 0.0338
Epoch 137/300, seasonal_0 Loss: 0.0410 | 0.0330
Epoch 138/300, seasonal_0 Loss: 0.0408 | 0.0325
Epoch 139/300, seasonal_0 Loss: 0.0445 | 0.0324
Epoch 140/300, seasonal_0 Loss: 0.0418 | 0.0314
Epoch 141/300, seasonal_0 Loss: 0.0413 | 0.0320
Epoch 142/300, seasonal_0 Loss: 0.0415 | 0.0320
Epoch 143/300, seasonal_0 Loss: 0.0416 | 0.0328
Epoch 144/300, seasonal_0 Loss: 0.0419 | 0.0345
Epoch 145/300, seasonal_0 Loss: 0.0416 | 0.0337
Epoch 146/300, seasonal_0 Loss: 0.0410 | 0.0328
Epoch 147/300, seasonal_0 Loss: 0.0404 | 0.0326
Epoch 148/300, seasonal_0 Loss: 0.0400 | 0.0332
Epoch 149/300, seasonal_0 Loss: 0.0397 | 0.0337
Epoch 150/300, seasonal_0 Loss: 0.0395 | 0.0340
Epoch 151/300, seasonal_0 Loss: 0.0394 | 0.0340
Epoch 152/300, seasonal_0 Loss: 0.0393 | 0.0337
Epoch 153/300, seasonal_0 Loss: 0.0391 | 0.0335
Epoch 154/300, seasonal_0 Loss: 0.0390 | 0.0335
Epoch 155/300, seasonal_0 Loss: 0.0389 | 0.0336
Epoch 156/300, seasonal_0 Loss: 0.0389 | 0.0337
Epoch 157/300, seasonal_0 Loss: 0.0388 | 0.0339
Epoch 158/300, seasonal_0 Loss: 0.0387 | 0.0341
Epoch 159/300, seasonal_0 Loss: 0.0386 | 0.0341
Epoch 160/300, seasonal_0 Loss: 0.0385 | 0.0343
Epoch 161/300, seasonal_0 Loss: 0.0385 | 0.0342
Epoch 162/300, seasonal_0 Loss: 0.0392 | 0.0345
Epoch 163/300, seasonal_0 Loss: 0.0390 | 0.0337
Epoch 164/300, seasonal_0 Loss: 0.0387 | 0.0342
Epoch 165/300, seasonal_0 Loss: 0.0386 | 0.0346
Epoch 166/300, seasonal_0 Loss: 0.0385 | 0.0345
Epoch 167/300, seasonal_0 Loss: 0.0384 | 0.0348
Epoch 168/300, seasonal_0 Loss: 0.0384 | 0.0348
Epoch 169/300, seasonal_0 Loss: 0.0383 | 0.0350
Epoch 170/300, seasonal_0 Loss: 0.0382 | 0.0351
Epoch 171/300, seasonal_0 Loss: 0.0382 | 0.0352
Epoch 172/300, seasonal_0 Loss: 0.0381 | 0.0353
Epoch 173/300, seasonal_0 Loss: 0.0381 | 0.0354
Epoch 174/300, seasonal_0 Loss: 0.0380 | 0.0354
Epoch 175/300, seasonal_0 Loss: 0.0380 | 0.0355
Epoch 176/300, seasonal_0 Loss: 0.0379 | 0.0356
Epoch 177/300, seasonal_0 Loss: 0.0379 | 0.0356
Epoch 178/300, seasonal_0 Loss: 0.0379 | 0.0357
Epoch 179/300, seasonal_0 Loss: 0.0378 | 0.0358
Epoch 180/300, seasonal_0 Loss: 0.0378 | 0.0359
Epoch 181/300, seasonal_0 Loss: 0.0377 | 0.0359
Epoch 182/300, seasonal_0 Loss: 0.0377 | 0.0360
Epoch 183/300, seasonal_0 Loss: 0.0376 | 0.0360
Epoch 184/300, seasonal_0 Loss: 0.0376 | 0.0361
Epoch 185/300, seasonal_0 Loss: 0.0375 | 0.0362
Epoch 186/300, seasonal_0 Loss: 0.0375 | 0.0362
Epoch 187/300, seasonal_0 Loss: 0.0374 | 0.0363
Epoch 188/300, seasonal_0 Loss: 0.0374 | 0.0364
Epoch 189/300, seasonal_0 Loss: 0.0373 | 0.0364
Epoch 190/300, seasonal_0 Loss: 0.0373 | 0.0365
Epoch 191/300, seasonal_0 Loss: 0.0372 | 0.0366
Epoch 192/300, seasonal_0 Loss: 0.0372 | 0.0366
Epoch 193/300, seasonal_0 Loss: 0.0373 | 0.0367
Epoch 194/300, seasonal_0 Loss: 0.0372 | 0.0366
Epoch 195/300, seasonal_0 Loss: 0.0371 | 0.0369
Epoch 196/300, seasonal_0 Loss: 0.0370 | 0.0369
Epoch 197/300, seasonal_0 Loss: 0.0369 | 0.0370
Epoch 198/300, seasonal_0 Loss: 0.0368 | 0.0371
Epoch 199/300, seasonal_0 Loss: 0.0368 | 0.0371
Epoch 200/300, seasonal_0 Loss: 0.0367 | 0.0372
Epoch 201/300, seasonal_0 Loss: 0.0367 | 0.0370
Epoch 202/300, seasonal_0 Loss: 0.0369 | 0.0373
Epoch 203/300, seasonal_0 Loss: 0.0367 | 0.0373
Epoch 204/300, seasonal_0 Loss: 0.0365 | 0.0373
Epoch 205/300, seasonal_0 Loss: 0.0364 | 0.0374
Epoch 206/300, seasonal_0 Loss: 0.0363 | 0.0375
Epoch 207/300, seasonal_0 Loss: 0.0363 | 0.0373
Epoch 208/300, seasonal_0 Loss: 0.0364 | 0.0376
Epoch 209/300, seasonal_0 Loss: 0.0362 | 0.0376
Epoch 210/300, seasonal_0 Loss: 0.0361 | 0.0377
Epoch 211/300, seasonal_0 Loss: 0.0363 | 0.0373
Epoch 212/300, seasonal_0 Loss: 0.0365 | 0.0375
Epoch 213/300, seasonal_0 Loss: 0.0361 | 0.0377
Epoch 214/300, seasonal_0 Loss: 0.0359 | 0.0378
Epoch 215/300, seasonal_0 Loss: 0.0358 | 0.0379
Epoch 216/300, seasonal_0 Loss: 0.0357 | 0.0379
Epoch 217/300, seasonal_0 Loss: 0.0356 | 0.0379
Epoch 218/300, seasonal_0 Loss: 0.0355 | 0.0379
Epoch 219/300, seasonal_0 Loss: 0.0355 | 0.0380
Epoch 220/300, seasonal_0 Loss: 0.0360 | 0.0375
Epoch 221/300, seasonal_0 Loss: 0.0361 | 0.0377
Epoch 222/300, seasonal_0 Loss: 0.0357 | 0.0378
Epoch 223/300, seasonal_0 Loss: 0.0356 | 0.0379
Epoch 224/300, seasonal_0 Loss: 0.0355 | 0.0380
Epoch 225/300, seasonal_0 Loss: 0.0354 | 0.0381
Epoch 226/300, seasonal_0 Loss: 0.0352 | 0.0381
Epoch 227/300, seasonal_0 Loss: 0.0351 | 0.0381
Epoch 228/300, seasonal_0 Loss: 0.0350 | 0.0381
Epoch 229/300, seasonal_0 Loss: 0.0349 | 0.0381
Epoch 230/300, seasonal_0 Loss: 0.0349 | 0.0381
Epoch 231/300, seasonal_0 Loss: 0.0349 | 0.0379
Epoch 232/300, seasonal_0 Loss: 0.0352 | 0.0381
Epoch 233/300, seasonal_0 Loss: 0.0348 | 0.0381
Epoch 234/300, seasonal_0 Loss: 0.0347 | 0.0382
Epoch 235/300, seasonal_0 Loss: 0.0349 | 0.0378
Epoch 236/300, seasonal_0 Loss: 0.0354 | 0.0379
Epoch 237/300, seasonal_0 Loss: 0.0351 | 0.0380
Epoch 238/300, seasonal_0 Loss: 0.0348 | 0.0382
Epoch 239/300, seasonal_0 Loss: 0.0345 | 0.0382
Epoch 240/300, seasonal_0 Loss: 0.0344 | 0.0381
Epoch 241/300, seasonal_0 Loss: 0.0344 | 0.0382
Epoch 242/300, seasonal_0 Loss: 0.0344 | 0.0380
Epoch 243/300, seasonal_0 Loss: 0.0346 | 0.0382
Epoch 244/300, seasonal_0 Loss: 0.0344 | 0.0380
Epoch 245/300, seasonal_0 Loss: 0.0345 | 0.0382
Epoch 246/300, seasonal_0 Loss: 0.0343 | 0.0380
Epoch 247/300, seasonal_0 Loss: 0.0345 | 0.0382
Epoch 248/300, seasonal_0 Loss: 0.0342 | 0.0381
Epoch 249/300, seasonal_0 Loss: 0.0342 | 0.0382
Epoch 250/300, seasonal_0 Loss: 0.0342 | 0.0380
Epoch 251/300, seasonal_0 Loss: 0.0344 | 0.0382
Epoch 252/300, seasonal_0 Loss: 0.0341 | 0.0382
Epoch 253/300, seasonal_0 Loss: 0.0340 | 0.0382
Epoch 254/300, seasonal_0 Loss: 0.0340 | 0.0382
Epoch 255/300, seasonal_0 Loss: 0.0340 | 0.0382
Epoch 256/300, seasonal_0 Loss: 0.0340 | 0.0382
Epoch 257/300, seasonal_0 Loss: 0.0340 | 0.0383
Epoch 258/300, seasonal_0 Loss: 0.0340 | 0.0381
Epoch 259/300, seasonal_0 Loss: 0.0341 | 0.0383
Epoch 260/300, seasonal_0 Loss: 0.0339 | 0.0382
Epoch 261/300, seasonal_0 Loss: 0.0340 | 0.0383
Epoch 262/300, seasonal_0 Loss: 0.0339 | 0.0382
Epoch 263/300, seasonal_0 Loss: 0.0339 | 0.0383
Epoch 264/300, seasonal_0 Loss: 0.0338 | 0.0382
Epoch 265/300, seasonal_0 Loss: 0.0338 | 0.0383
Epoch 266/300, seasonal_0 Loss: 0.0338 | 0.0383
Epoch 267/300, seasonal_0 Loss: 0.0337 | 0.0383
Epoch 268/300, seasonal_0 Loss: 0.0337 | 0.0383
Epoch 269/300, seasonal_0 Loss: 0.0337 | 0.0383
Epoch 270/300, seasonal_0 Loss: 0.0337 | 0.0383
Epoch 271/300, seasonal_0 Loss: 0.0337 | 0.0383
Epoch 272/300, seasonal_0 Loss: 0.0336 | 0.0383
Epoch 273/300, seasonal_0 Loss: 0.0336 | 0.0384
Epoch 274/300, seasonal_0 Loss: 0.0336 | 0.0384
Epoch 275/300, seasonal_0 Loss: 0.0336 | 0.0384
Epoch 276/300, seasonal_0 Loss: 0.0336 | 0.0384
Epoch 277/300, seasonal_0 Loss: 0.0336 | 0.0384
Epoch 278/300, seasonal_0 Loss: 0.0336 | 0.0384
Epoch 279/300, seasonal_0 Loss: 0.0335 | 0.0384
Epoch 280/300, seasonal_0 Loss: 0.0335 | 0.0384
Epoch 281/300, seasonal_0 Loss: 0.0335 | 0.0384
Epoch 282/300, seasonal_0 Loss: 0.0335 | 0.0384
Epoch 283/300, seasonal_0 Loss: 0.0335 | 0.0384
Epoch 284/300, seasonal_0 Loss: 0.0335 | 0.0385
Epoch 285/300, seasonal_0 Loss: 0.0335 | 0.0385
Epoch 286/300, seasonal_0 Loss: 0.0335 | 0.0385
Epoch 287/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 288/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 289/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 290/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 291/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 292/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 293/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 294/300, seasonal_0 Loss: 0.0334 | 0.0385
Epoch 295/300, seasonal_0 Loss: 0.0334 | 0.0386
Epoch 296/300, seasonal_0 Loss: 0.0333 | 0.0386
Epoch 297/300, seasonal_0 Loss: 0.0333 | 0.0386
Epoch 298/300, seasonal_0 Loss: 0.0333 | 0.0386
Epoch 299/300, seasonal_0 Loss: 0.0333 | 0.0386
Epoch 300/300, seasonal_0 Loss: 0.0333 | 0.0386
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8890777208359977, 'learning_rate': 0.0005780688306891082, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8989667503112131}
Epoch 1/300, seasonal_1 Loss: 0.3699 | 0.1847
Epoch 2/300, seasonal_1 Loss: 0.1788 | 0.1599
Epoch 3/300, seasonal_1 Loss: 0.1537 | 0.1538
Epoch 4/300, seasonal_1 Loss: 0.1545 | 0.1273
Epoch 5/300, seasonal_1 Loss: 0.1540 | 0.3336
Epoch 6/300, seasonal_1 Loss: 0.1543 | 0.2145
Epoch 7/300, seasonal_1 Loss: 0.1344 | 0.1269
Epoch 8/300, seasonal_1 Loss: 0.1388 | 0.0994
Epoch 9/300, seasonal_1 Loss: 0.1190 | 0.0922
Epoch 10/300, seasonal_1 Loss: 0.1207 | 0.0801
Epoch 11/300, seasonal_1 Loss: 0.1063 | 0.0918
Epoch 12/300, seasonal_1 Loss: 0.1032 | 0.0683
Epoch 13/300, seasonal_1 Loss: 0.1127 | 0.0873
Epoch 14/300, seasonal_1 Loss: 0.1119 | 0.0628
Epoch 15/300, seasonal_1 Loss: 0.0945 | 0.0585
Epoch 16/300, seasonal_1 Loss: 0.0946 | 0.0586
Epoch 17/300, seasonal_1 Loss: 0.0914 | 0.0587
Epoch 18/300, seasonal_1 Loss: 0.0920 | 0.0597
Epoch 19/300, seasonal_1 Loss: 0.0920 | 0.0699
Epoch 20/300, seasonal_1 Loss: 0.0915 | 0.0585
Epoch 21/300, seasonal_1 Loss: 0.0914 | 0.0610
Epoch 22/300, seasonal_1 Loss: 0.0970 | 0.0994
Epoch 23/300, seasonal_1 Loss: 0.1036 | 0.1245
Epoch 24/300, seasonal_1 Loss: 0.1075 | 0.1007
Epoch 25/300, seasonal_1 Loss: 0.0939 | 0.0768
Epoch 26/300, seasonal_1 Loss: 0.0855 | 0.0499
Epoch 27/300, seasonal_1 Loss: 0.0900 | 0.0564
Epoch 28/300, seasonal_1 Loss: 0.0843 | 0.0462
Epoch 29/300, seasonal_1 Loss: 0.0825 | 0.0692
Epoch 30/300, seasonal_1 Loss: 0.0853 | 0.0774
Epoch 31/300, seasonal_1 Loss: 0.0874 | 0.0458
Epoch 32/300, seasonal_1 Loss: 0.0773 | 0.0414
Epoch 33/300, seasonal_1 Loss: 0.0768 | 0.0401
Epoch 34/300, seasonal_1 Loss: 0.0740 | 0.0454
Epoch 35/300, seasonal_1 Loss: 0.0737 | 0.0443
Epoch 36/300, seasonal_1 Loss: 0.0735 | 0.0385
Epoch 37/300, seasonal_1 Loss: 0.0722 | 0.0391
Epoch 38/300, seasonal_1 Loss: 0.0712 | 0.0422
Epoch 39/300, seasonal_1 Loss: 0.0708 | 0.0408
Epoch 40/300, seasonal_1 Loss: 0.0705 | 0.0381
Epoch 41/300, seasonal_1 Loss: 0.0696 | 0.0361
Epoch 42/300, seasonal_1 Loss: 0.0686 | 0.0354
Epoch 43/300, seasonal_1 Loss: 0.0682 | 0.0356
Epoch 44/300, seasonal_1 Loss: 0.0685 | 0.0358
Epoch 45/300, seasonal_1 Loss: 0.0688 | 0.0356
Epoch 46/300, seasonal_1 Loss: 0.0686 | 0.0357
Epoch 47/300, seasonal_1 Loss: 0.0681 | 0.0371
Epoch 48/300, seasonal_1 Loss: 0.0679 | 0.0393
Epoch 49/300, seasonal_1 Loss: 0.0682 | 0.0388
Epoch 50/300, seasonal_1 Loss: 0.0678 | 0.0348
Epoch 51/300, seasonal_1 Loss: 0.0672 | 0.0333
Epoch 52/300, seasonal_1 Loss: 0.0678 | 0.0347
Epoch 53/300, seasonal_1 Loss: 0.0685 | 0.0381
Epoch 54/300, seasonal_1 Loss: 0.0684 | 0.0370
Epoch 55/300, seasonal_1 Loss: 0.0668 | 0.0335
Epoch 56/300, seasonal_1 Loss: 0.0657 | 0.0324
Epoch 57/300, seasonal_1 Loss: 0.0656 | 0.0328
Epoch 58/300, seasonal_1 Loss: 0.0672 | 0.0333
Epoch 59/300, seasonal_1 Loss: 0.0697 | 0.0332
Epoch 60/300, seasonal_1 Loss: 0.0724 | 0.0346
Epoch 61/300, seasonal_1 Loss: 0.0756 | 0.0351
Epoch 62/300, seasonal_1 Loss: 0.0747 | 0.0355
Epoch 63/300, seasonal_1 Loss: 0.0702 | 0.0349
Epoch 64/300, seasonal_1 Loss: 0.0691 | 0.0364
Epoch 65/300, seasonal_1 Loss: 0.0677 | 0.0416
Epoch 66/300, seasonal_1 Loss: 0.0681 | 0.0394
Epoch 67/300, seasonal_1 Loss: 0.0674 | 0.0356
Epoch 68/300, seasonal_1 Loss: 0.0648 | 0.0344
Epoch 69/300, seasonal_1 Loss: 0.0636 | 0.0319
Epoch 70/300, seasonal_1 Loss: 0.0630 | 0.0307
Epoch 71/300, seasonal_1 Loss: 0.0626 | 0.0316
Epoch 72/300, seasonal_1 Loss: 0.0623 | 0.0324
Epoch 73/300, seasonal_1 Loss: 0.0622 | 0.0312
Epoch 74/300, seasonal_1 Loss: 0.0620 | 0.0312
Epoch 75/300, seasonal_1 Loss: 0.0619 | 0.0318
Epoch 76/300, seasonal_1 Loss: 0.0618 | 0.0310
Epoch 77/300, seasonal_1 Loss: 0.0617 | 0.0309
Epoch 78/300, seasonal_1 Loss: 0.0616 | 0.0312
Epoch 79/300, seasonal_1 Loss: 0.0615 | 0.0308
Epoch 80/300, seasonal_1 Loss: 0.0614 | 0.0306
Epoch 81/300, seasonal_1 Loss: 0.0613 | 0.0308
Epoch 82/300, seasonal_1 Loss: 0.0612 | 0.0305
Epoch 83/300, seasonal_1 Loss: 0.0611 | 0.0304
Epoch 84/300, seasonal_1 Loss: 0.0610 | 0.0305
Epoch 85/300, seasonal_1 Loss: 0.0609 | 0.0303
Epoch 86/300, seasonal_1 Loss: 0.0609 | 0.0301
Epoch 87/300, seasonal_1 Loss: 0.0608 | 0.0302
Epoch 88/300, seasonal_1 Loss: 0.0607 | 0.0300
Epoch 89/300, seasonal_1 Loss: 0.0606 | 0.0299
Epoch 90/300, seasonal_1 Loss: 0.0605 | 0.0299
Epoch 91/300, seasonal_1 Loss: 0.0605 | 0.0298
Epoch 92/300, seasonal_1 Loss: 0.0604 | 0.0297
Epoch 93/300, seasonal_1 Loss: 0.0603 | 0.0297
Epoch 94/300, seasonal_1 Loss: 0.0603 | 0.0296
Epoch 95/300, seasonal_1 Loss: 0.0602 | 0.0295
Epoch 96/300, seasonal_1 Loss: 0.0601 | 0.0295
Epoch 97/300, seasonal_1 Loss: 0.0601 | 0.0294
Epoch 98/300, seasonal_1 Loss: 0.0600 | 0.0293
Epoch 99/300, seasonal_1 Loss: 0.0599 | 0.0293
Epoch 100/300, seasonal_1 Loss: 0.0599 | 0.0292
Epoch 101/300, seasonal_1 Loss: 0.0598 | 0.0292
Epoch 102/300, seasonal_1 Loss: 0.0597 | 0.0292
Epoch 103/300, seasonal_1 Loss: 0.0597 | 0.0291
Epoch 104/300, seasonal_1 Loss: 0.0596 | 0.0291
Epoch 105/300, seasonal_1 Loss: 0.0596 | 0.0290
Epoch 106/300, seasonal_1 Loss: 0.0595 | 0.0290
Epoch 107/300, seasonal_1 Loss: 0.0595 | 0.0290
Epoch 108/300, seasonal_1 Loss: 0.0594 | 0.0289
Epoch 109/300, seasonal_1 Loss: 0.0594 | 0.0289
Epoch 110/300, seasonal_1 Loss: 0.0593 | 0.0289
Epoch 111/300, seasonal_1 Loss: 0.0593 | 0.0289
Epoch 112/300, seasonal_1 Loss: 0.0592 | 0.0288
Epoch 113/300, seasonal_1 Loss: 0.0592 | 0.0288
Epoch 114/300, seasonal_1 Loss: 0.0591 | 0.0288
Epoch 115/300, seasonal_1 Loss: 0.0591 | 0.0288
Epoch 116/300, seasonal_1 Loss: 0.0590 | 0.0287
Epoch 117/300, seasonal_1 Loss: 0.0590 | 0.0287
Epoch 118/300, seasonal_1 Loss: 0.0590 | 0.0287
Epoch 119/300, seasonal_1 Loss: 0.0589 | 0.0287
Epoch 120/300, seasonal_1 Loss: 0.0589 | 0.0287
Epoch 121/300, seasonal_1 Loss: 0.0589 | 0.0286
Epoch 122/300, seasonal_1 Loss: 0.0588 | 0.0286
Epoch 123/300, seasonal_1 Loss: 0.0588 | 0.0286
Epoch 124/300, seasonal_1 Loss: 0.0588 | 0.0286
Epoch 125/300, seasonal_1 Loss: 0.0587 | 0.0286
Epoch 126/300, seasonal_1 Loss: 0.0587 | 0.0285
Epoch 127/300, seasonal_1 Loss: 0.0587 | 0.0285
Epoch 128/300, seasonal_1 Loss: 0.0586 | 0.0285
Epoch 129/300, seasonal_1 Loss: 0.0586 | 0.0285
Epoch 130/300, seasonal_1 Loss: 0.0586 | 0.0285
Epoch 131/300, seasonal_1 Loss: 0.0586 | 0.0285
Epoch 132/300, seasonal_1 Loss: 0.0585 | 0.0284
Epoch 133/300, seasonal_1 Loss: 0.0585 | 0.0284
Epoch 134/300, seasonal_1 Loss: 0.0585 | 0.0284
Epoch 135/300, seasonal_1 Loss: 0.0585 | 0.0284
Epoch 136/300, seasonal_1 Loss: 0.0584 | 0.0284
Epoch 137/300, seasonal_1 Loss: 0.0584 | 0.0284
Epoch 138/300, seasonal_1 Loss: 0.0584 | 0.0284
Epoch 139/300, seasonal_1 Loss: 0.0584 | 0.0283
Epoch 140/300, seasonal_1 Loss: 0.0583 | 0.0283
Epoch 141/300, seasonal_1 Loss: 0.0583 | 0.0283
Epoch 142/300, seasonal_1 Loss: 0.0583 | 0.0283
Epoch 143/300, seasonal_1 Loss: 0.0583 | 0.0283
Epoch 144/300, seasonal_1 Loss: 0.0583 | 0.0283
Epoch 145/300, seasonal_1 Loss: 0.0582 | 0.0283
Epoch 146/300, seasonal_1 Loss: 0.0582 | 0.0283
Epoch 147/300, seasonal_1 Loss: 0.0582 | 0.0282
Epoch 148/300, seasonal_1 Loss: 0.0582 | 0.0282
Epoch 149/300, seasonal_1 Loss: 0.0582 | 0.0282
Epoch 150/300, seasonal_1 Loss: 0.0582 | 0.0282
Epoch 151/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 152/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 153/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 154/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 155/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 156/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 157/300, seasonal_1 Loss: 0.0581 | 0.0282
Epoch 158/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 159/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 160/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 161/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 162/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 163/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 164/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 165/300, seasonal_1 Loss: 0.0580 | 0.0281
Epoch 166/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 167/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 168/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 169/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 170/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 171/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 172/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 173/300, seasonal_1 Loss: 0.0579 | 0.0281
Epoch 174/300, seasonal_1 Loss: 0.0579 | 0.0280
Epoch 175/300, seasonal_1 Loss: 0.0579 | 0.0280
Epoch 176/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 177/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 178/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 179/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 180/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 181/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 182/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 183/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 184/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 185/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 186/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 187/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 188/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 189/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 190/300, seasonal_1 Loss: 0.0578 | 0.0280
Epoch 191/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 192/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 193/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 194/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 195/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 196/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 197/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 198/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 199/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 200/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 201/300, seasonal_1 Loss: 0.0577 | 0.0280
Epoch 202/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 203/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 204/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 205/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 206/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 207/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 208/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 209/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 210/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 211/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 212/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 213/300, seasonal_1 Loss: 0.0577 | 0.0279
Epoch 214/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 215/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 216/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 217/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 218/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 219/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 220/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 221/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 222/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 223/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 224/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 225/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 226/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 227/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 228/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 229/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 230/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 231/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 232/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 233/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 234/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 235/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 236/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 237/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 238/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 239/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 240/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 241/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 242/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 243/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 244/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 245/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 246/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 247/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 248/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 249/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 250/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 251/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 252/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 253/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 254/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 255/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 256/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 257/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 258/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 259/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 260/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 261/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 262/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 263/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 264/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 265/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 266/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 267/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 268/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 269/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 270/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 271/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 272/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 273/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 274/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 275/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 276/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 277/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 278/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 279/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 280/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 281/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 282/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 283/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 284/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 285/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 286/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 287/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 288/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 289/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 290/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 291/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 292/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 293/300, seasonal_1 Loss: 0.0576 | 0.0279
Epoch 294/300, seasonal_1 Loss: 0.0575 | 0.0279
Epoch 295/300, seasonal_1 Loss: 0.0575 | 0.0279
Epoch 296/300, seasonal_1 Loss: 0.0575 | 0.0279
Epoch 297/300, seasonal_1 Loss: 0.0575 | 0.0279
Epoch 298/300, seasonal_1 Loss: 0.0575 | 0.0279
Epoch 299/300, seasonal_1 Loss: 0.0575 | 0.0279
Epoch 300/300, seasonal_1 Loss: 0.0575 | 0.0279
Training seasonal_2 component with params: {'observation_period_num': 20, 'train_rates': 0.987381978164134, 'learning_rate': 0.000210758885676157, 'batch_size': 112, 'step_size': 12, 'gamma': 0.9408787578946637}
Epoch 1/300, seasonal_2 Loss: 0.4043 | 0.2763
Epoch 2/300, seasonal_2 Loss: 0.2335 | 0.2221
Epoch 3/300, seasonal_2 Loss: 0.1823 | 0.1927
Epoch 4/300, seasonal_2 Loss: 0.1904 | 0.1429
Epoch 5/300, seasonal_2 Loss: 0.2597 | 0.4404
Epoch 6/300, seasonal_2 Loss: 0.1998 | 0.3938
Epoch 7/300, seasonal_2 Loss: 0.1586 | 0.1886
Epoch 8/300, seasonal_2 Loss: 0.1691 | 0.1767
Epoch 9/300, seasonal_2 Loss: 0.1270 | 0.1495
Epoch 10/300, seasonal_2 Loss: 0.1165 | 0.1391
Epoch 11/300, seasonal_2 Loss: 0.1129 | 0.1309
Epoch 12/300, seasonal_2 Loss: 0.1082 | 0.1183
Epoch 13/300, seasonal_2 Loss: 0.1052 | 0.1115
Epoch 14/300, seasonal_2 Loss: 0.1030 | 0.1067
Epoch 15/300, seasonal_2 Loss: 0.1018 | 0.1073
Epoch 16/300, seasonal_2 Loss: 0.1024 | 0.1173
Epoch 17/300, seasonal_2 Loss: 0.1033 | 0.1354
Epoch 18/300, seasonal_2 Loss: 0.1014 | 0.1391
Epoch 19/300, seasonal_2 Loss: 0.0958 | 0.1131
Epoch 20/300, seasonal_2 Loss: 0.0914 | 0.0760
Epoch 21/300, seasonal_2 Loss: 0.0940 | 0.0636
Epoch 22/300, seasonal_2 Loss: 0.0976 | 0.0690
Epoch 23/300, seasonal_2 Loss: 0.0934 | 0.0630
Epoch 24/300, seasonal_2 Loss: 0.0882 | 0.0593
Epoch 25/300, seasonal_2 Loss: 0.0888 | 0.0586
Epoch 26/300, seasonal_2 Loss: 0.0904 | 0.0622
Epoch 27/300, seasonal_2 Loss: 0.0885 | 0.0610
Epoch 28/300, seasonal_2 Loss: 0.0861 | 0.0586
Epoch 29/300, seasonal_2 Loss: 0.0886 | 0.0559
Epoch 30/300, seasonal_2 Loss: 0.0957 | 0.0602
Epoch 31/300, seasonal_2 Loss: 0.0920 | 0.0626
Epoch 32/300, seasonal_2 Loss: 0.0888 | 0.0639
Epoch 33/300, seasonal_2 Loss: 0.0893 | 0.0664
Epoch 34/300, seasonal_2 Loss: 0.0943 | 0.0931
Epoch 35/300, seasonal_2 Loss: 0.1044 | 0.1580
Epoch 36/300, seasonal_2 Loss: 0.1023 | 0.1527
Epoch 37/300, seasonal_2 Loss: 0.0927 | 0.0860
Epoch 38/300, seasonal_2 Loss: 0.0871 | 0.0568
Epoch 39/300, seasonal_2 Loss: 0.0943 | 0.0663
Epoch 40/300, seasonal_2 Loss: 0.0907 | 0.0640
Epoch 41/300, seasonal_2 Loss: 0.0813 | 0.0557
Epoch 42/300, seasonal_2 Loss: 0.0886 | 0.0559
Epoch 43/300, seasonal_2 Loss: 0.0843 | 0.0516
Epoch 44/300, seasonal_2 Loss: 0.0807 | 0.0501
Epoch 45/300, seasonal_2 Loss: 0.0825 | 0.0526
Epoch 46/300, seasonal_2 Loss: 0.0789 | 0.0547
Epoch 47/300, seasonal_2 Loss: 0.0740 | 0.0477
Epoch 48/300, seasonal_2 Loss: 0.0723 | 0.0464
Epoch 49/300, seasonal_2 Loss: 0.0732 | 0.0474
Epoch 50/300, seasonal_2 Loss: 0.0750 | 0.0511
Epoch 51/300, seasonal_2 Loss: 0.0755 | 0.0537
Epoch 52/300, seasonal_2 Loss: 0.0740 | 0.0510
Epoch 53/300, seasonal_2 Loss: 0.0714 | 0.0467
Epoch 54/300, seasonal_2 Loss: 0.0707 | 0.0442
Epoch 55/300, seasonal_2 Loss: 0.0746 | 0.0553
Epoch 56/300, seasonal_2 Loss: 0.0734 | 0.0424
Epoch 57/300, seasonal_2 Loss: 0.0678 | 0.0419
Epoch 58/300, seasonal_2 Loss: 0.0663 | 0.0410
Epoch 59/300, seasonal_2 Loss: 0.0671 | 0.0412
Epoch 60/300, seasonal_2 Loss: 0.0679 | 0.0404
Epoch 61/300, seasonal_2 Loss: 0.0673 | 0.0418
Epoch 62/300, seasonal_2 Loss: 0.0685 | 0.0497
Epoch 63/300, seasonal_2 Loss: 0.0696 | 0.0536
Epoch 64/300, seasonal_2 Loss: 0.0676 | 0.0461
Epoch 65/300, seasonal_2 Loss: 0.0657 | 0.0395
Epoch 66/300, seasonal_2 Loss: 0.0686 | 0.0443
Epoch 67/300, seasonal_2 Loss: 0.0727 | 0.0478
Epoch 68/300, seasonal_2 Loss: 0.0696 | 0.0376
Epoch 69/300, seasonal_2 Loss: 0.0661 | 0.0368
Epoch 70/300, seasonal_2 Loss: 0.0641 | 0.0356
Epoch 71/300, seasonal_2 Loss: 0.0632 | 0.0341
Epoch 72/300, seasonal_2 Loss: 0.0628 | 0.0336
Epoch 73/300, seasonal_2 Loss: 0.0623 | 0.0332
Epoch 74/300, seasonal_2 Loss: 0.0615 | 0.0334
Epoch 75/300, seasonal_2 Loss: 0.0611 | 0.0342
Epoch 76/300, seasonal_2 Loss: 0.0617 | 0.0344
Epoch 77/300, seasonal_2 Loss: 0.0630 | 0.0340
Epoch 78/300, seasonal_2 Loss: 0.0654 | 0.0388
Epoch 79/300, seasonal_2 Loss: 0.0667 | 0.0402
Epoch 80/300, seasonal_2 Loss: 0.0653 | 0.0334
Epoch 81/300, seasonal_2 Loss: 0.0653 | 0.0328
Epoch 82/300, seasonal_2 Loss: 0.0668 | 0.0387
Epoch 83/300, seasonal_2 Loss: 0.0697 | 0.0461
Epoch 84/300, seasonal_2 Loss: 0.0696 | 0.0476
Epoch 85/300, seasonal_2 Loss: 0.0669 | 0.0432
Epoch 86/300, seasonal_2 Loss: 0.0640 | 0.0360
Epoch 87/300, seasonal_2 Loss: 0.0649 | 0.0336
Epoch 88/300, seasonal_2 Loss: 0.0682 | 0.0408
Epoch 89/300, seasonal_2 Loss: 0.0738 | 0.0382
Epoch 90/300, seasonal_2 Loss: 0.0710 | 0.0299
Epoch 91/300, seasonal_2 Loss: 0.0655 | 0.0288
Epoch 92/300, seasonal_2 Loss: 0.0647 | 0.0315
Epoch 93/300, seasonal_2 Loss: 0.0646 | 0.0333
Epoch 94/300, seasonal_2 Loss: 0.0618 | 0.0311
Epoch 95/300, seasonal_2 Loss: 0.0593 | 0.0279
Epoch 96/300, seasonal_2 Loss: 0.0601 | 0.0280
Epoch 97/300, seasonal_2 Loss: 0.0619 | 0.0300
Epoch 98/300, seasonal_2 Loss: 0.0616 | 0.0274
Epoch 99/300, seasonal_2 Loss: 0.0604 | 0.0261
Epoch 100/300, seasonal_2 Loss: 0.0604 | 0.0262
Epoch 101/300, seasonal_2 Loss: 0.0604 | 0.0274
Epoch 102/300, seasonal_2 Loss: 0.0603 | 0.0291
Epoch 103/300, seasonal_2 Loss: 0.0608 | 0.0305
Epoch 104/300, seasonal_2 Loss: 0.0637 | 0.0299
Epoch 105/300, seasonal_2 Loss: 0.0634 | 0.0258
Epoch 106/300, seasonal_2 Loss: 0.0600 | 0.0235
Epoch 107/300, seasonal_2 Loss: 0.0593 | 0.0290
Epoch 108/300, seasonal_2 Loss: 0.0581 | 0.0246
Epoch 109/300, seasonal_2 Loss: 0.0569 | 0.0217
Epoch 110/300, seasonal_2 Loss: 0.0580 | 0.0245
Epoch 111/300, seasonal_2 Loss: 0.0584 | 0.0273
Epoch 112/300, seasonal_2 Loss: 0.0575 | 0.0255
Epoch 113/300, seasonal_2 Loss: 0.0567 | 0.0240
Epoch 114/300, seasonal_2 Loss: 0.0563 | 0.0238
Epoch 115/300, seasonal_2 Loss: 0.0566 | 0.0250
Epoch 116/300, seasonal_2 Loss: 0.0578 | 0.0240
Epoch 117/300, seasonal_2 Loss: 0.0594 | 0.0236
Epoch 118/300, seasonal_2 Loss: 0.0599 | 0.0234
Epoch 119/300, seasonal_2 Loss: 0.0582 | 0.0242
Epoch 120/300, seasonal_2 Loss: 0.0567 | 0.0253
Epoch 121/300, seasonal_2 Loss: 0.0565 | 0.0242
Epoch 122/300, seasonal_2 Loss: 0.0568 | 0.0218
Epoch 123/300, seasonal_2 Loss: 0.0568 | 0.0207
Epoch 124/300, seasonal_2 Loss: 0.0559 | 0.0205
Epoch 125/300, seasonal_2 Loss: 0.0548 | 0.0203
Epoch 126/300, seasonal_2 Loss: 0.0542 | 0.0201
Epoch 127/300, seasonal_2 Loss: 0.0542 | 0.0203
Epoch 128/300, seasonal_2 Loss: 0.0540 | 0.0210
Epoch 129/300, seasonal_2 Loss: 0.0535 | 0.0214
Epoch 130/300, seasonal_2 Loss: 0.0529 | 0.0213
Epoch 131/300, seasonal_2 Loss: 0.0529 | 0.0216
Epoch 132/300, seasonal_2 Loss: 0.0541 | 0.0240
Epoch 133/300, seasonal_2 Loss: 0.0553 | 0.0256
Epoch 134/300, seasonal_2 Loss: 0.0553 | 0.0232
Epoch 135/300, seasonal_2 Loss: 0.0549 | 0.0235
Epoch 136/300, seasonal_2 Loss: 0.0552 | 0.0269
Epoch 137/300, seasonal_2 Loss: 0.0568 | 0.0301
Epoch 138/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 139/300, seasonal_2 Loss: 0.0570 | 0.0224
Epoch 140/300, seasonal_2 Loss: 0.0548 | 0.0227
Epoch 141/300, seasonal_2 Loss: 0.0546 | 0.0228
Epoch 142/300, seasonal_2 Loss: 0.0549 | 0.0210
Epoch 143/300, seasonal_2 Loss: 0.0549 | 0.0220
Epoch 144/300, seasonal_2 Loss: 0.0540 | 0.0216
Epoch 145/300, seasonal_2 Loss: 0.0533 | 0.0210
Epoch 146/300, seasonal_2 Loss: 0.0531 | 0.0209
Epoch 147/300, seasonal_2 Loss: 0.0531 | 0.0208
Epoch 148/300, seasonal_2 Loss: 0.0528 | 0.0201
Epoch 149/300, seasonal_2 Loss: 0.0523 | 0.0194
Epoch 150/300, seasonal_2 Loss: 0.0519 | 0.0195
Epoch 151/300, seasonal_2 Loss: 0.0519 | 0.0200
Epoch 152/300, seasonal_2 Loss: 0.0518 | 0.0201
Epoch 153/300, seasonal_2 Loss: 0.0517 | 0.0203
Epoch 154/300, seasonal_2 Loss: 0.0515 | 0.0206
Epoch 155/300, seasonal_2 Loss: 0.0515 | 0.0208
Epoch 156/300, seasonal_2 Loss: 0.0514 | 0.0207
Epoch 157/300, seasonal_2 Loss: 0.0514 | 0.0204
Epoch 158/300, seasonal_2 Loss: 0.0514 | 0.0203
Epoch 159/300, seasonal_2 Loss: 0.0515 | 0.0206
Epoch 160/300, seasonal_2 Loss: 0.0515 | 0.0206
Epoch 161/300, seasonal_2 Loss: 0.0512 | 0.0200
Epoch 162/300, seasonal_2 Loss: 0.0510 | 0.0192
Epoch 163/300, seasonal_2 Loss: 0.0508 | 0.0187
Epoch 164/300, seasonal_2 Loss: 0.0507 | 0.0188
Epoch 165/300, seasonal_2 Loss: 0.0506 | 0.0192
Epoch 166/300, seasonal_2 Loss: 0.0504 | 0.0191
Epoch 167/300, seasonal_2 Loss: 0.0501 | 0.0187
Epoch 168/300, seasonal_2 Loss: 0.0499 | 0.0186
Epoch 169/300, seasonal_2 Loss: 0.0499 | 0.0193
Epoch 170/300, seasonal_2 Loss: 0.0499 | 0.0199
Epoch 171/300, seasonal_2 Loss: 0.0498 | 0.0197
Epoch 172/300, seasonal_2 Loss: 0.0497 | 0.0192
Epoch 173/300, seasonal_2 Loss: 0.0497 | 0.0192
Epoch 174/300, seasonal_2 Loss: 0.0498 | 0.0197
Epoch 175/300, seasonal_2 Loss: 0.0499 | 0.0197
Epoch 176/300, seasonal_2 Loss: 0.0498 | 0.0189
Epoch 177/300, seasonal_2 Loss: 0.0499 | 0.0193
Epoch 178/300, seasonal_2 Loss: 0.0504 | 0.0210
Epoch 179/300, seasonal_2 Loss: 0.0507 | 0.0220
Epoch 180/300, seasonal_2 Loss: 0.0505 | 0.0208
Epoch 181/300, seasonal_2 Loss: 0.0502 | 0.0194
Epoch 182/300, seasonal_2 Loss: 0.0503 | 0.0202
Epoch 183/300, seasonal_2 Loss: 0.0509 | 0.0202
Epoch 184/300, seasonal_2 Loss: 0.0512 | 0.0185
Epoch 185/300, seasonal_2 Loss: 0.0516 | 0.0189
Epoch 186/300, seasonal_2 Loss: 0.0518 | 0.0207
Epoch 187/300, seasonal_2 Loss: 0.0520 | 0.0214
Epoch 188/300, seasonal_2 Loss: 0.0522 | 0.0192
Epoch 189/300, seasonal_2 Loss: 0.0525 | 0.0187
Epoch 190/300, seasonal_2 Loss: 0.0534 | 0.0212
Epoch 191/300, seasonal_2 Loss: 0.0554 | 0.0236
Epoch 192/300, seasonal_2 Loss: 0.0564 | 0.0226
Epoch 193/300, seasonal_2 Loss: 0.0529 | 0.0220
Epoch 194/300, seasonal_2 Loss: 0.0499 | 0.0196
Epoch 195/300, seasonal_2 Loss: 0.0499 | 0.0191
Epoch 196/300, seasonal_2 Loss: 0.0510 | 0.0197
Epoch 197/300, seasonal_2 Loss: 0.0511 | 0.0184
Epoch 198/300, seasonal_2 Loss: 0.0498 | 0.0175
Epoch 199/300, seasonal_2 Loss: 0.0490 | 0.0181
Epoch 200/300, seasonal_2 Loss: 0.0494 | 0.0188
Epoch 201/300, seasonal_2 Loss: 0.0498 | 0.0189
Epoch 202/300, seasonal_2 Loss: 0.0493 | 0.0187
Epoch 203/300, seasonal_2 Loss: 0.0487 | 0.0186
Epoch 204/300, seasonal_2 Loss: 0.0485 | 0.0182
Epoch 205/300, seasonal_2 Loss: 0.0484 | 0.0177
Epoch 206/300, seasonal_2 Loss: 0.0483 | 0.0174
Epoch 207/300, seasonal_2 Loss: 0.0482 | 0.0172
Epoch 208/300, seasonal_2 Loss: 0.0480 | 0.0171
Epoch 209/300, seasonal_2 Loss: 0.0479 | 0.0170
Epoch 210/300, seasonal_2 Loss: 0.0478 | 0.0170
Epoch 211/300, seasonal_2 Loss: 0.0478 | 0.0171
Epoch 212/300, seasonal_2 Loss: 0.0477 | 0.0171
Epoch 213/300, seasonal_2 Loss: 0.0477 | 0.0171
Epoch 214/300, seasonal_2 Loss: 0.0476 | 0.0171
Epoch 215/300, seasonal_2 Loss: 0.0476 | 0.0171
Epoch 216/300, seasonal_2 Loss: 0.0475 | 0.0171
Epoch 217/300, seasonal_2 Loss: 0.0475 | 0.0171
Epoch 218/300, seasonal_2 Loss: 0.0475 | 0.0171
Epoch 219/300, seasonal_2 Loss: 0.0474 | 0.0170
Epoch 220/300, seasonal_2 Loss: 0.0474 | 0.0170
Epoch 221/300, seasonal_2 Loss: 0.0473 | 0.0170
Epoch 222/300, seasonal_2 Loss: 0.0473 | 0.0169
Epoch 223/300, seasonal_2 Loss: 0.0473 | 0.0169
Epoch 224/300, seasonal_2 Loss: 0.0472 | 0.0169
Epoch 225/300, seasonal_2 Loss: 0.0472 | 0.0168
Epoch 226/300, seasonal_2 Loss: 0.0471 | 0.0168
Epoch 227/300, seasonal_2 Loss: 0.0471 | 0.0167
Epoch 228/300, seasonal_2 Loss: 0.0471 | 0.0167
Epoch 229/300, seasonal_2 Loss: 0.0470 | 0.0167
Epoch 230/300, seasonal_2 Loss: 0.0470 | 0.0167
Epoch 231/300, seasonal_2 Loss: 0.0470 | 0.0167
Epoch 232/300, seasonal_2 Loss: 0.0470 | 0.0166
Epoch 233/300, seasonal_2 Loss: 0.0469 | 0.0166
Epoch 234/300, seasonal_2 Loss: 0.0469 | 0.0166
Epoch 235/300, seasonal_2 Loss: 0.0469 | 0.0166
Epoch 236/300, seasonal_2 Loss: 0.0468 | 0.0166
Epoch 237/300, seasonal_2 Loss: 0.0468 | 0.0166
Epoch 238/300, seasonal_2 Loss: 0.0467 | 0.0167
Epoch 239/300, seasonal_2 Loss: 0.0467 | 0.0167
Epoch 240/300, seasonal_2 Loss: 0.0467 | 0.0168
Epoch 241/300, seasonal_2 Loss: 0.0467 | 0.0169
Epoch 242/300, seasonal_2 Loss: 0.0466 | 0.0168
Epoch 243/300, seasonal_2 Loss: 0.0466 | 0.0169
Epoch 244/300, seasonal_2 Loss: 0.0466 | 0.0169
Epoch 245/300, seasonal_2 Loss: 0.0466 | 0.0169
Epoch 246/300, seasonal_2 Loss: 0.0465 | 0.0168
Epoch 247/300, seasonal_2 Loss: 0.0465 | 0.0168
Epoch 248/300, seasonal_2 Loss: 0.0465 | 0.0170
Epoch 249/300, seasonal_2 Loss: 0.0466 | 0.0172
Epoch 250/300, seasonal_2 Loss: 0.0465 | 0.0171
Epoch 251/300, seasonal_2 Loss: 0.0464 | 0.0167
Epoch 252/300, seasonal_2 Loss: 0.0465 | 0.0166
Epoch 253/300, seasonal_2 Loss: 0.0467 | 0.0170
Epoch 254/300, seasonal_2 Loss: 0.0467 | 0.0165
Epoch 255/300, seasonal_2 Loss: 0.0466 | 0.0162
Epoch 256/300, seasonal_2 Loss: 0.0468 | 0.0172
Epoch 257/300, seasonal_2 Loss: 0.0470 | 0.0180
Epoch 258/300, seasonal_2 Loss: 0.0468 | 0.0171
Epoch 259/300, seasonal_2 Loss: 0.0466 | 0.0165
Epoch 260/300, seasonal_2 Loss: 0.0467 | 0.0171
Epoch 261/300, seasonal_2 Loss: 0.0465 | 0.0168
Epoch 262/300, seasonal_2 Loss: 0.0463 | 0.0169
Epoch 263/300, seasonal_2 Loss: 0.0464 | 0.0176
Epoch 264/300, seasonal_2 Loss: 0.0464 | 0.0176
Epoch 265/300, seasonal_2 Loss: 0.0463 | 0.0174
Epoch 266/300, seasonal_2 Loss: 0.0462 | 0.0172
Epoch 267/300, seasonal_2 Loss: 0.0462 | 0.0169
Epoch 268/300, seasonal_2 Loss: 0.0462 | 0.0167
Epoch 269/300, seasonal_2 Loss: 0.0461 | 0.0167
Epoch 270/300, seasonal_2 Loss: 0.0460 | 0.0166
Epoch 271/300, seasonal_2 Loss: 0.0460 | 0.0164
Epoch 272/300, seasonal_2 Loss: 0.0459 | 0.0163
Epoch 273/300, seasonal_2 Loss: 0.0459 | 0.0163
Epoch 274/300, seasonal_2 Loss: 0.0458 | 0.0163
Epoch 275/300, seasonal_2 Loss: 0.0458 | 0.0165
Epoch 276/300, seasonal_2 Loss: 0.0458 | 0.0166
Epoch 277/300, seasonal_2 Loss: 0.0458 | 0.0168
Epoch 278/300, seasonal_2 Loss: 0.0458 | 0.0168
Epoch 279/300, seasonal_2 Loss: 0.0458 | 0.0168
Epoch 280/300, seasonal_2 Loss: 0.0457 | 0.0168
Epoch 281/300, seasonal_2 Loss: 0.0457 | 0.0167
Epoch 282/300, seasonal_2 Loss: 0.0456 | 0.0167
Epoch 283/300, seasonal_2 Loss: 0.0456 | 0.0165
Epoch 284/300, seasonal_2 Loss: 0.0455 | 0.0165
Epoch 285/300, seasonal_2 Loss: 0.0455 | 0.0164
Epoch 286/300, seasonal_2 Loss: 0.0455 | 0.0165
Epoch 287/300, seasonal_2 Loss: 0.0455 | 0.0166
Epoch 288/300, seasonal_2 Loss: 0.0456 | 0.0167
Epoch 289/300, seasonal_2 Loss: 0.0456 | 0.0168
Epoch 290/300, seasonal_2 Loss: 0.0456 | 0.0168
Epoch 291/300, seasonal_2 Loss: 0.0455 | 0.0167
Epoch 292/300, seasonal_2 Loss: 0.0454 | 0.0166
Epoch 293/300, seasonal_2 Loss: 0.0454 | 0.0166
Epoch 294/300, seasonal_2 Loss: 0.0453 | 0.0166
Epoch 295/300, seasonal_2 Loss: 0.0453 | 0.0165
Epoch 296/300, seasonal_2 Loss: 0.0453 | 0.0166
Epoch 297/300, seasonal_2 Loss: 0.0452 | 0.0166
Epoch 298/300, seasonal_2 Loss: 0.0452 | 0.0166
Epoch 299/300, seasonal_2 Loss: 0.0452 | 0.0166
Epoch 300/300, seasonal_2 Loss: 0.0452 | 0.0166
Training seasonal_3 component with params: {'observation_period_num': 11, 'train_rates': 0.8476500194098459, 'learning_rate': 0.0007373980961353831, 'batch_size': 129, 'step_size': 9, 'gamma': 0.768137220851108}
Epoch 1/300, seasonal_3 Loss: 0.3863 | 0.1796
Epoch 2/300, seasonal_3 Loss: 0.1508 | 0.1394
Epoch 3/300, seasonal_3 Loss: 0.1447 | 0.0976
Epoch 4/300, seasonal_3 Loss: 0.1231 | 0.0716
Epoch 5/300, seasonal_3 Loss: 0.1212 | 0.0637
Epoch 6/300, seasonal_3 Loss: 0.1142 | 0.1786
Epoch 7/300, seasonal_3 Loss: 0.1094 | 0.1146
Epoch 8/300, seasonal_3 Loss: 0.1098 | 0.1007
Epoch 9/300, seasonal_3 Loss: 0.1404 | 0.0899
Epoch 10/300, seasonal_3 Loss: 0.1065 | 0.0586
Epoch 11/300, seasonal_3 Loss: 0.0963 | 0.0530
Epoch 12/300, seasonal_3 Loss: 0.1034 | 0.0722
Epoch 13/300, seasonal_3 Loss: 0.1080 | 0.0617
Epoch 14/300, seasonal_3 Loss: 0.0933 | 0.0458
Epoch 15/300, seasonal_3 Loss: 0.0871 | 0.0443
Epoch 16/300, seasonal_3 Loss: 0.0859 | 0.0449
Epoch 17/300, seasonal_3 Loss: 0.0842 | 0.0456
Epoch 18/300, seasonal_3 Loss: 0.0833 | 0.0415
Epoch 19/300, seasonal_3 Loss: 0.0832 | 0.0412
Epoch 20/300, seasonal_3 Loss: 0.0858 | 0.0410
Epoch 21/300, seasonal_3 Loss: 0.0830 | 0.0453
Epoch 22/300, seasonal_3 Loss: 0.0830 | 0.0481
Epoch 23/300, seasonal_3 Loss: 0.0815 | 0.0436
Epoch 24/300, seasonal_3 Loss: 0.0808 | 0.0451
Epoch 25/300, seasonal_3 Loss: 0.0811 | 0.0468
Epoch 26/300, seasonal_3 Loss: 0.0810 | 0.0467
Epoch 27/300, seasonal_3 Loss: 0.0783 | 0.0415
Epoch 28/300, seasonal_3 Loss: 0.0752 | 0.0364
Epoch 29/300, seasonal_3 Loss: 0.0745 | 0.0353
Epoch 30/300, seasonal_3 Loss: 0.0745 | 0.0352
Epoch 31/300, seasonal_3 Loss: 0.0739 | 0.0355
Epoch 32/300, seasonal_3 Loss: 0.0734 | 0.0359
Epoch 33/300, seasonal_3 Loss: 0.0731 | 0.0357
Epoch 34/300, seasonal_3 Loss: 0.0727 | 0.0349
Epoch 35/300, seasonal_3 Loss: 0.0723 | 0.0344
Epoch 36/300, seasonal_3 Loss: 0.0720 | 0.0340
Epoch 37/300, seasonal_3 Loss: 0.0718 | 0.0340
Epoch 38/300, seasonal_3 Loss: 0.0716 | 0.0339
Epoch 39/300, seasonal_3 Loss: 0.0713 | 0.0337
Epoch 40/300, seasonal_3 Loss: 0.0711 | 0.0335
Epoch 41/300, seasonal_3 Loss: 0.0710 | 0.0333
Epoch 42/300, seasonal_3 Loss: 0.0708 | 0.0335
Epoch 43/300, seasonal_3 Loss: 0.0707 | 0.0335
Epoch 44/300, seasonal_3 Loss: 0.0706 | 0.0334
Epoch 45/300, seasonal_3 Loss: 0.0706 | 0.0334
Epoch 46/300, seasonal_3 Loss: 0.0705 | 0.0334
Epoch 47/300, seasonal_3 Loss: 0.0704 | 0.0333
Epoch 48/300, seasonal_3 Loss: 0.0703 | 0.0332
Epoch 49/300, seasonal_3 Loss: 0.0702 | 0.0328
Epoch 50/300, seasonal_3 Loss: 0.0699 | 0.0325
Epoch 51/300, seasonal_3 Loss: 0.0695 | 0.0323
Epoch 52/300, seasonal_3 Loss: 0.0692 | 0.0322
Epoch 53/300, seasonal_3 Loss: 0.0690 | 0.0321
Epoch 54/300, seasonal_3 Loss: 0.0689 | 0.0321
Epoch 55/300, seasonal_3 Loss: 0.0688 | 0.0320
Epoch 56/300, seasonal_3 Loss: 0.0687 | 0.0320
Epoch 57/300, seasonal_3 Loss: 0.0686 | 0.0319
Epoch 58/300, seasonal_3 Loss: 0.0686 | 0.0319
Epoch 59/300, seasonal_3 Loss: 0.0685 | 0.0318
Epoch 60/300, seasonal_3 Loss: 0.0684 | 0.0318
Epoch 61/300, seasonal_3 Loss: 0.0684 | 0.0317
Epoch 62/300, seasonal_3 Loss: 0.0683 | 0.0317
Epoch 63/300, seasonal_3 Loss: 0.0682 | 0.0316
Epoch 64/300, seasonal_3 Loss: 0.0681 | 0.0315
Epoch 65/300, seasonal_3 Loss: 0.0680 | 0.0314
Epoch 66/300, seasonal_3 Loss: 0.0679 | 0.0313
Epoch 67/300, seasonal_3 Loss: 0.0678 | 0.0313
Epoch 68/300, seasonal_3 Loss: 0.0677 | 0.0312
Epoch 69/300, seasonal_3 Loss: 0.0676 | 0.0311
Epoch 70/300, seasonal_3 Loss: 0.0676 | 0.0311
Epoch 71/300, seasonal_3 Loss: 0.0675 | 0.0311
Epoch 72/300, seasonal_3 Loss: 0.0675 | 0.0310
Epoch 73/300, seasonal_3 Loss: 0.0674 | 0.0310
Epoch 74/300, seasonal_3 Loss: 0.0674 | 0.0309
Epoch 75/300, seasonal_3 Loss: 0.0673 | 0.0309
Epoch 76/300, seasonal_3 Loss: 0.0673 | 0.0309
Epoch 77/300, seasonal_3 Loss: 0.0672 | 0.0308
Epoch 78/300, seasonal_3 Loss: 0.0672 | 0.0308
Epoch 79/300, seasonal_3 Loss: 0.0672 | 0.0308
Epoch 80/300, seasonal_3 Loss: 0.0671 | 0.0307
Epoch 81/300, seasonal_3 Loss: 0.0671 | 0.0307
Epoch 82/300, seasonal_3 Loss: 0.0670 | 0.0307
Epoch 83/300, seasonal_3 Loss: 0.0670 | 0.0306
Epoch 84/300, seasonal_3 Loss: 0.0670 | 0.0306
Epoch 85/300, seasonal_3 Loss: 0.0670 | 0.0306
Epoch 86/300, seasonal_3 Loss: 0.0669 | 0.0306
Epoch 87/300, seasonal_3 Loss: 0.0669 | 0.0305
Epoch 88/300, seasonal_3 Loss: 0.0669 | 0.0305
Epoch 89/300, seasonal_3 Loss: 0.0669 | 0.0305
Epoch 90/300, seasonal_3 Loss: 0.0668 | 0.0305
Epoch 91/300, seasonal_3 Loss: 0.0668 | 0.0305
Epoch 92/300, seasonal_3 Loss: 0.0668 | 0.0304
Epoch 93/300, seasonal_3 Loss: 0.0668 | 0.0304
Epoch 94/300, seasonal_3 Loss: 0.0668 | 0.0304
Epoch 95/300, seasonal_3 Loss: 0.0667 | 0.0304
Epoch 96/300, seasonal_3 Loss: 0.0667 | 0.0304
Epoch 97/300, seasonal_3 Loss: 0.0667 | 0.0304
Epoch 98/300, seasonal_3 Loss: 0.0667 | 0.0304
Epoch 99/300, seasonal_3 Loss: 0.0667 | 0.0303
Epoch 100/300, seasonal_3 Loss: 0.0667 | 0.0303
Epoch 101/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 102/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 103/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 104/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 105/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 106/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 107/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 108/300, seasonal_3 Loss: 0.0666 | 0.0303
Epoch 109/300, seasonal_3 Loss: 0.0666 | 0.0302
Epoch 110/300, seasonal_3 Loss: 0.0666 | 0.0302
Epoch 111/300, seasonal_3 Loss: 0.0666 | 0.0302
Epoch 112/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 113/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 114/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 115/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 116/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 117/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 118/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 119/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 120/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 121/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 122/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 123/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 124/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 125/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 126/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 127/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 128/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 129/300, seasonal_3 Loss: 0.0665 | 0.0302
Epoch 130/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 131/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 132/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 133/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 134/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 135/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 136/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 137/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 138/300, seasonal_3 Loss: 0.0665 | 0.0301
Epoch 139/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 140/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 141/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 142/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 143/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 144/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 145/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 146/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 147/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 148/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 149/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 150/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 151/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 152/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 153/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 154/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 155/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 156/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 157/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 158/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 159/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 160/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 161/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 162/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 163/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 164/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 165/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 166/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 167/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 168/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 169/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 170/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 171/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 172/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 173/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 174/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 175/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 176/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 177/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 178/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 179/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 180/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 181/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 182/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 183/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 184/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 185/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 186/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 187/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 188/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 189/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 190/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 191/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 192/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 193/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 194/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 195/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 196/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 197/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 198/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 199/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 200/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 201/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 202/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 203/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 204/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 205/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 206/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 207/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 208/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 209/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 210/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 211/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 212/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 213/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 214/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 215/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 216/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 217/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 218/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 219/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 220/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 221/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 222/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 223/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 224/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 225/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 226/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 227/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 228/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 229/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 230/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 231/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 232/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 233/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 234/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 235/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 236/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 237/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 238/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 239/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 240/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 241/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 242/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 243/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 244/300, seasonal_3 Loss: 0.0664 | 0.0301
Epoch 245/300, seasonal_3 Loss: 0.0664 | 0.0301
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 29, 'train_rates': 0.9791351857336593, 'learning_rate': 0.0008694013886418433, 'batch_size': 89, 'step_size': 3, 'gamma': 0.927533561677686}
Epoch 1/300, resid Loss: 0.3243 | 0.1820
Epoch 2/300, resid Loss: 0.1531 | 0.1248
Epoch 3/300, resid Loss: 0.1150 | 0.0908
Epoch 4/300, resid Loss: 0.1048 | 0.1999
Epoch 5/300, resid Loss: 0.1245 | 0.0951
Epoch 6/300, resid Loss: 0.0962 | 0.0663
Epoch 7/300, resid Loss: 0.0869 | 0.0658
Epoch 8/300, resid Loss: 0.0848 | 0.0782
Epoch 9/300, resid Loss: 0.0896 | 0.0628
Epoch 10/300, resid Loss: 0.0905 | 0.0810
Epoch 11/300, resid Loss: 0.0800 | 0.0569
Epoch 12/300, resid Loss: 0.0770 | 0.0645
Epoch 13/300, resid Loss: 0.0770 | 0.0814
Epoch 14/300, resid Loss: 0.0773 | 0.0775
Epoch 15/300, resid Loss: 0.0741 | 0.0735
Epoch 16/300, resid Loss: 0.0731 | 0.0708
Epoch 17/300, resid Loss: 0.0732 | 0.0730
Epoch 18/300, resid Loss: 0.0763 | 0.0712
Epoch 19/300, resid Loss: 0.0802 | 0.0514
Epoch 20/300, resid Loss: 0.0858 | 0.0528
Epoch 21/300, resid Loss: 0.0842 | 0.0537
Epoch 22/300, resid Loss: 0.0777 | 0.0499
Epoch 23/300, resid Loss: 0.0738 | 0.0465
Epoch 24/300, resid Loss: 0.0725 | 0.0527
Epoch 25/300, resid Loss: 0.0710 | 0.0453
Epoch 26/300, resid Loss: 0.0672 | 0.0464
Epoch 27/300, resid Loss: 0.0659 | 0.0444
Epoch 28/300, resid Loss: 0.0664 | 0.0458
Epoch 29/300, resid Loss: 0.0676 | 0.0441
Epoch 30/300, resid Loss: 0.0696 | 0.0454
Epoch 31/300, resid Loss: 0.0731 | 0.0474
Epoch 32/300, resid Loss: 0.0774 | 0.0554
Epoch 33/300, resid Loss: 0.0844 | 0.0996
Epoch 34/300, resid Loss: 0.0916 | 0.0648
Epoch 35/300, resid Loss: 0.0920 | 0.0770
Epoch 36/300, resid Loss: 0.1020 | 0.0652
Epoch 37/300, resid Loss: 0.0871 | 0.1601
Epoch 38/300, resid Loss: 0.0977 | 0.1411
Epoch 39/300, resid Loss: 0.0974 | 0.2197
Epoch 40/300, resid Loss: 0.0807 | 0.0562
Epoch 41/300, resid Loss: 0.0690 | 0.0409
Epoch 42/300, resid Loss: 0.0652 | 0.0394
Epoch 43/300, resid Loss: 0.0611 | 0.0407
Epoch 44/300, resid Loss: 0.0595 | 0.0410
Epoch 45/300, resid Loss: 0.0585 | 0.0401
Epoch 46/300, resid Loss: 0.0581 | 0.0389
Epoch 47/300, resid Loss: 0.0581 | 0.0381
Epoch 48/300, resid Loss: 0.0583 | 0.0376
Epoch 49/300, resid Loss: 0.0588 | 0.0375
Epoch 50/300, resid Loss: 0.0592 | 0.0377
Epoch 51/300, resid Loss: 0.0596 | 0.0377
Epoch 52/300, resid Loss: 0.0597 | 0.0377
Epoch 53/300, resid Loss: 0.0589 | 0.0377
Epoch 54/300, resid Loss: 0.0579 | 0.0378
Epoch 55/300, resid Loss: 0.0570 | 0.0380
Epoch 56/300, resid Loss: 0.0566 | 0.0382
Epoch 57/300, resid Loss: 0.0564 | 0.0379
Epoch 58/300, resid Loss: 0.0560 | 0.0370
Epoch 59/300, resid Loss: 0.0555 | 0.0362
Epoch 60/300, resid Loss: 0.0552 | 0.0356
Epoch 61/300, resid Loss: 0.0550 | 0.0353
Epoch 62/300, resid Loss: 0.0549 | 0.0351
Epoch 63/300, resid Loss: 0.0548 | 0.0350
Epoch 64/300, resid Loss: 0.0547 | 0.0349
Epoch 65/300, resid Loss: 0.0546 | 0.0349
Epoch 66/300, resid Loss: 0.0545 | 0.0348
Epoch 67/300, resid Loss: 0.0544 | 0.0347
Epoch 68/300, resid Loss: 0.0543 | 0.0346
Epoch 69/300, resid Loss: 0.0542 | 0.0345
Epoch 70/300, resid Loss: 0.0541 | 0.0344
Epoch 71/300, resid Loss: 0.0541 | 0.0343
Epoch 72/300, resid Loss: 0.0540 | 0.0342
Epoch 73/300, resid Loss: 0.0540 | 0.0341
Epoch 74/300, resid Loss: 0.0539 | 0.0341
Epoch 75/300, resid Loss: 0.0538 | 0.0340
Epoch 76/300, resid Loss: 0.0538 | 0.0339
Epoch 77/300, resid Loss: 0.0537 | 0.0339
Epoch 78/300, resid Loss: 0.0537 | 0.0338
Epoch 79/300, resid Loss: 0.0536 | 0.0337
Epoch 80/300, resid Loss: 0.0536 | 0.0337
Epoch 81/300, resid Loss: 0.0535 | 0.0336
Epoch 82/300, resid Loss: 0.0535 | 0.0336
Epoch 83/300, resid Loss: 0.0535 | 0.0335
Epoch 84/300, resid Loss: 0.0534 | 0.0335
Epoch 85/300, resid Loss: 0.0534 | 0.0334
Epoch 86/300, resid Loss: 0.0533 | 0.0334
Epoch 87/300, resid Loss: 0.0533 | 0.0334
Epoch 88/300, resid Loss: 0.0533 | 0.0333
Epoch 89/300, resid Loss: 0.0532 | 0.0333
Epoch 90/300, resid Loss: 0.0532 | 0.0333
Epoch 91/300, resid Loss: 0.0532 | 0.0332
Epoch 92/300, resid Loss: 0.0532 | 0.0332
Epoch 93/300, resid Loss: 0.0531 | 0.0332
Epoch 94/300, resid Loss: 0.0531 | 0.0331
Epoch 95/300, resid Loss: 0.0531 | 0.0331
Epoch 96/300, resid Loss: 0.0531 | 0.0331
Epoch 97/300, resid Loss: 0.0530 | 0.0331
Epoch 98/300, resid Loss: 0.0530 | 0.0330
Epoch 99/300, resid Loss: 0.0530 | 0.0330
Epoch 100/300, resid Loss: 0.0530 | 0.0330
Epoch 101/300, resid Loss: 0.0530 | 0.0330
Epoch 102/300, resid Loss: 0.0529 | 0.0330
Epoch 103/300, resid Loss: 0.0529 | 0.0329
Epoch 104/300, resid Loss: 0.0529 | 0.0329
Epoch 105/300, resid Loss: 0.0529 | 0.0329
Epoch 106/300, resid Loss: 0.0529 | 0.0329
Epoch 107/300, resid Loss: 0.0529 | 0.0329
Epoch 108/300, resid Loss: 0.0528 | 0.0329
Epoch 109/300, resid Loss: 0.0528 | 0.0329
Epoch 110/300, resid Loss: 0.0528 | 0.0328
Epoch 111/300, resid Loss: 0.0528 | 0.0328
Epoch 112/300, resid Loss: 0.0528 | 0.0328
Epoch 113/300, resid Loss: 0.0528 | 0.0328
Epoch 114/300, resid Loss: 0.0528 | 0.0328
Epoch 115/300, resid Loss: 0.0528 | 0.0328
Epoch 116/300, resid Loss: 0.0528 | 0.0328
Epoch 117/300, resid Loss: 0.0527 | 0.0328
Epoch 118/300, resid Loss: 0.0527 | 0.0328
Epoch 119/300, resid Loss: 0.0527 | 0.0327
Epoch 120/300, resid Loss: 0.0527 | 0.0327
Epoch 121/300, resid Loss: 0.0527 | 0.0327
Epoch 122/300, resid Loss: 0.0527 | 0.0327
Epoch 123/300, resid Loss: 0.0527 | 0.0327
Epoch 124/300, resid Loss: 0.0527 | 0.0327
Epoch 125/300, resid Loss: 0.0527 | 0.0327
Epoch 126/300, resid Loss: 0.0527 | 0.0327
Epoch 127/300, resid Loss: 0.0527 | 0.0327
Epoch 128/300, resid Loss: 0.0527 | 0.0327
Epoch 129/300, resid Loss: 0.0527 | 0.0327
Epoch 130/300, resid Loss: 0.0527 | 0.0327
Epoch 131/300, resid Loss: 0.0526 | 0.0327
Epoch 132/300, resid Loss: 0.0526 | 0.0327
Epoch 133/300, resid Loss: 0.0526 | 0.0327
Epoch 134/300, resid Loss: 0.0526 | 0.0326
Epoch 135/300, resid Loss: 0.0526 | 0.0326
Epoch 136/300, resid Loss: 0.0526 | 0.0326
Epoch 137/300, resid Loss: 0.0526 | 0.0326
Epoch 138/300, resid Loss: 0.0526 | 0.0326
Epoch 139/300, resid Loss: 0.0526 | 0.0326
Epoch 140/300, resid Loss: 0.0526 | 0.0326
Epoch 141/300, resid Loss: 0.0526 | 0.0326
Epoch 142/300, resid Loss: 0.0526 | 0.0326
Epoch 143/300, resid Loss: 0.0526 | 0.0326
Epoch 144/300, resid Loss: 0.0526 | 0.0326
Epoch 145/300, resid Loss: 0.0526 | 0.0326
Epoch 146/300, resid Loss: 0.0526 | 0.0326
Epoch 147/300, resid Loss: 0.0526 | 0.0326
Epoch 148/300, resid Loss: 0.0526 | 0.0326
Epoch 149/300, resid Loss: 0.0526 | 0.0326
Epoch 150/300, resid Loss: 0.0526 | 0.0326
Epoch 151/300, resid Loss: 0.0526 | 0.0326
Epoch 152/300, resid Loss: 0.0526 | 0.0326
Epoch 153/300, resid Loss: 0.0526 | 0.0326
Epoch 154/300, resid Loss: 0.0526 | 0.0326
Epoch 155/300, resid Loss: 0.0526 | 0.0326
Epoch 156/300, resid Loss: 0.0526 | 0.0326
Epoch 157/300, resid Loss: 0.0526 | 0.0326
Epoch 158/300, resid Loss: 0.0526 | 0.0326
Epoch 159/300, resid Loss: 0.0526 | 0.0326
Epoch 160/300, resid Loss: 0.0526 | 0.0326
Epoch 161/300, resid Loss: 0.0526 | 0.0326
Epoch 162/300, resid Loss: 0.0526 | 0.0326
Epoch 163/300, resid Loss: 0.0526 | 0.0326
Epoch 164/300, resid Loss: 0.0526 | 0.0326
Epoch 165/300, resid Loss: 0.0526 | 0.0326
Epoch 166/300, resid Loss: 0.0526 | 0.0326
Epoch 167/300, resid Loss: 0.0526 | 0.0326
Epoch 168/300, resid Loss: 0.0526 | 0.0326
Epoch 169/300, resid Loss: 0.0526 | 0.0326
Epoch 170/300, resid Loss: 0.0526 | 0.0326
Epoch 171/300, resid Loss: 0.0525 | 0.0326
Epoch 172/300, resid Loss: 0.0525 | 0.0326
Epoch 173/300, resid Loss: 0.0525 | 0.0326
Epoch 174/300, resid Loss: 0.0525 | 0.0326
Epoch 175/300, resid Loss: 0.0525 | 0.0326
Epoch 176/300, resid Loss: 0.0525 | 0.0326
Epoch 177/300, resid Loss: 0.0525 | 0.0326
Epoch 178/300, resid Loss: 0.0525 | 0.0326
Epoch 179/300, resid Loss: 0.0525 | 0.0326
Epoch 180/300, resid Loss: 0.0525 | 0.0326
Epoch 181/300, resid Loss: 0.0525 | 0.0326
Epoch 182/300, resid Loss: 0.0525 | 0.0326
Epoch 183/300, resid Loss: 0.0525 | 0.0326
Epoch 184/300, resid Loss: 0.0525 | 0.0326
Epoch 185/300, resid Loss: 0.0525 | 0.0326
Epoch 186/300, resid Loss: 0.0525 | 0.0326
Epoch 187/300, resid Loss: 0.0525 | 0.0326
Epoch 188/300, resid Loss: 0.0525 | 0.0326
Epoch 189/300, resid Loss: 0.0525 | 0.0326
Epoch 190/300, resid Loss: 0.0525 | 0.0326
Epoch 191/300, resid Loss: 0.0525 | 0.0326
Epoch 192/300, resid Loss: 0.0525 | 0.0326
Epoch 193/300, resid Loss: 0.0525 | 0.0326
Epoch 194/300, resid Loss: 0.0525 | 0.0326
Epoch 195/300, resid Loss: 0.0525 | 0.0326
Epoch 196/300, resid Loss: 0.0525 | 0.0326
Epoch 197/300, resid Loss: 0.0525 | 0.0326
Epoch 198/300, resid Loss: 0.0525 | 0.0326
Epoch 199/300, resid Loss: 0.0525 | 0.0326
Epoch 200/300, resid Loss: 0.0525 | 0.0326
Epoch 201/300, resid Loss: 0.0525 | 0.0326
Epoch 202/300, resid Loss: 0.0525 | 0.0326
Epoch 203/300, resid Loss: 0.0525 | 0.0326
Epoch 204/300, resid Loss: 0.0525 | 0.0326
Epoch 205/300, resid Loss: 0.0525 | 0.0326
Epoch 206/300, resid Loss: 0.0525 | 0.0326
Epoch 207/300, resid Loss: 0.0525 | 0.0326
Epoch 208/300, resid Loss: 0.0525 | 0.0326
Epoch 209/300, resid Loss: 0.0525 | 0.0326
Epoch 210/300, resid Loss: 0.0525 | 0.0326
Epoch 211/300, resid Loss: 0.0525 | 0.0326
Epoch 212/300, resid Loss: 0.0525 | 0.0326
Epoch 213/300, resid Loss: 0.0525 | 0.0326
Epoch 214/300, resid Loss: 0.0525 | 0.0326
Epoch 215/300, resid Loss: 0.0525 | 0.0326
Epoch 216/300, resid Loss: 0.0525 | 0.0326
Epoch 217/300, resid Loss: 0.0525 | 0.0326
Epoch 218/300, resid Loss: 0.0525 | 0.0326
Epoch 219/300, resid Loss: 0.0525 | 0.0326
Epoch 220/300, resid Loss: 0.0525 | 0.0326
Epoch 221/300, resid Loss: 0.0525 | 0.0326
Epoch 222/300, resid Loss: 0.0525 | 0.0326
Epoch 223/300, resid Loss: 0.0525 | 0.0325
Epoch 224/300, resid Loss: 0.0525 | 0.0325
Epoch 225/300, resid Loss: 0.0525 | 0.0325
Epoch 226/300, resid Loss: 0.0525 | 0.0325
Epoch 227/300, resid Loss: 0.0525 | 0.0325
Epoch 228/300, resid Loss: 0.0525 | 0.0325
Epoch 229/300, resid Loss: 0.0525 | 0.0325
Epoch 230/300, resid Loss: 0.0525 | 0.0325
Epoch 231/300, resid Loss: 0.0525 | 0.0325
Epoch 232/300, resid Loss: 0.0525 | 0.0325
Epoch 233/300, resid Loss: 0.0525 | 0.0325
Epoch 234/300, resid Loss: 0.0525 | 0.0325
Epoch 235/300, resid Loss: 0.0525 | 0.0325
Epoch 236/300, resid Loss: 0.0525 | 0.0325
Epoch 237/300, resid Loss: 0.0525 | 0.0325
Epoch 238/300, resid Loss: 0.0525 | 0.0325
Epoch 239/300, resid Loss: 0.0525 | 0.0325
Epoch 240/300, resid Loss: 0.0525 | 0.0325
Epoch 241/300, resid Loss: 0.0525 | 0.0325
Epoch 242/300, resid Loss: 0.0525 | 0.0325
Epoch 243/300, resid Loss: 0.0525 | 0.0325
Epoch 244/300, resid Loss: 0.0525 | 0.0325
Epoch 245/300, resid Loss: 0.0525 | 0.0325
Epoch 246/300, resid Loss: 0.0525 | 0.0325
Epoch 247/300, resid Loss: 0.0525 | 0.0325
Epoch 248/300, resid Loss: 0.0525 | 0.0325
Epoch 249/300, resid Loss: 0.0525 | 0.0325
Epoch 250/300, resid Loss: 0.0525 | 0.0325
Epoch 251/300, resid Loss: 0.0525 | 0.0325
Epoch 252/300, resid Loss: 0.0525 | 0.0325
Epoch 253/300, resid Loss: 0.0525 | 0.0325
Epoch 254/300, resid Loss: 0.0525 | 0.0325
Epoch 255/300, resid Loss: 0.0525 | 0.0325
Early stopping for resid
Runtime (seconds): 1197.6594104766846
0.0002900878964956585
[215.94217]
[1.3306216]
[0.32927126]
[5.7583137]
[0.0289526]
[6.73584]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 23.08634163835086
RMSE: 4.8048248291015625
MAE: 4.8048248291015625
R-squared: nan
[230.12517]
