[32m[I 2025-02-06 08:20:17,896][0m A new study created in memory with name: no-name-805581be-8e0f-47d8-ba0d-5ca155585a22[0m
[32m[I 2025-02-06 08:25:48,618][0m Trial 0 finished with value: 0.1402604066169084 and parameters: {'observation_period_num': 142, 'train_rates': 0.9529192019514786, 'learning_rate': 6.223490452440855e-05, 'batch_size': 17, 'step_size': 3, 'gamma': 0.8247600042076408}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:26:36,455][0m Trial 1 finished with value: 0.20894107493980768 and parameters: {'observation_period_num': 222, 'train_rates': 0.801372327563222, 'learning_rate': 7.500431729075499e-05, 'batch_size': 110, 'step_size': 9, 'gamma': 0.7853733678537257}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:27:49,121][0m Trial 2 finished with value: 0.18613121503177174 and parameters: {'observation_period_num': 173, 'train_rates': 0.9338381986467672, 'learning_rate': 2.819139915644845e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.9808554228928721}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:28:24,238][0m Trial 3 finished with value: 0.23459706272791225 and parameters: {'observation_period_num': 72, 'train_rates': 0.7327138742859405, 'learning_rate': 0.0005485390291700351, 'batch_size': 144, 'step_size': 15, 'gamma': 0.9627137944361778}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:30:09,483][0m Trial 4 finished with value: 0.2013405568406477 and parameters: {'observation_period_num': 44, 'train_rates': 0.6905924967208864, 'learning_rate': 3.148381727838148e-05, 'batch_size': 45, 'step_size': 12, 'gamma': 0.7850688869883452}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:31:01,269][0m Trial 5 finished with value: 0.47599291801452637 and parameters: {'observation_period_num': 133, 'train_rates': 0.9670018985099894, 'learning_rate': 1.1493044617078463e-05, 'batch_size': 116, 'step_size': 11, 'gamma': 0.7649520052504684}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:31:29,691][0m Trial 6 finished with value: 0.3617936670780182 and parameters: {'observation_period_num': 239, 'train_rates': 0.927931938870407, 'learning_rate': 2.6460826954198374e-05, 'batch_size': 217, 'step_size': 11, 'gamma': 0.9644897843059925}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:32:25,369][0m Trial 7 finished with value: 0.5107982158660889 and parameters: {'observation_period_num': 154, 'train_rates': 0.9628932894226212, 'learning_rate': 5.831236970821829e-06, 'batch_size': 107, 'step_size': 9, 'gamma': 0.9112414034436338}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:33:01,164][0m Trial 8 finished with value: 0.6577329277992249 and parameters: {'observation_period_num': 196, 'train_rates': 0.881969022950382, 'learning_rate': 4.477094543784326e-06, 'batch_size': 156, 'step_size': 7, 'gamma': 0.8489176828280811}. Best is trial 0 with value: 0.1402604066169084.[0m
[32m[I 2025-02-06 08:33:33,193][0m Trial 9 finished with value: 0.10355651168071706 and parameters: {'observation_period_num': 100, 'train_rates': 0.800784689581183, 'learning_rate': 0.0007635397338104335, 'batch_size': 175, 'step_size': 8, 'gamma': 0.8016489316097264}. Best is trial 9 with value: 0.10355651168071706.[0m
[32m[I 2025-02-06 08:33:55,161][0m Trial 10 finished with value: 0.18233365545984848 and parameters: {'observation_period_num': 88, 'train_rates': 0.6188415564072797, 'learning_rate': 0.0009631806903720798, 'batch_size': 252, 'step_size': 3, 'gamma': 0.8970948173254242}. Best is trial 9 with value: 0.10355651168071706.[0m
Early stopping at epoch 70
[32m[I 2025-02-06 08:34:16,480][0m Trial 11 finished with value: 0.35030503326695733 and parameters: {'observation_period_num': 102, 'train_rates': 0.8500185293215107, 'learning_rate': 0.00019242747801860268, 'batch_size': 194, 'step_size': 1, 'gamma': 0.8320453649171373}. Best is trial 9 with value: 0.10355651168071706.[0m
[32m[I 2025-02-06 08:37:58,139][0m Trial 12 finished with value: 0.7311099003971636 and parameters: {'observation_period_num': 119, 'train_rates': 0.7743673811249661, 'learning_rate': 1.092603509720543e-06, 'batch_size': 22, 'step_size': 5, 'gamma': 0.8166987508224847}. Best is trial 9 with value: 0.10355651168071706.[0m
[32m[I 2025-02-06 08:38:30,491][0m Trial 13 finished with value: 0.06354954609111564 and parameters: {'observation_period_num': 33, 'train_rates': 0.8516751203666525, 'learning_rate': 0.00017943191239862909, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8074939027986983}. Best is trial 13 with value: 0.06354954609111564.[0m
[32m[I 2025-02-06 08:39:02,537][0m Trial 14 finished with value: 0.06333216179324233 and parameters: {'observation_period_num': 18, 'train_rates': 0.8451008821445783, 'learning_rate': 0.0002746585238039606, 'batch_size': 181, 'step_size': 6, 'gamma': 0.7505035871966319}. Best is trial 14 with value: 0.06333216179324233.[0m
[32m[I 2025-02-06 08:39:29,753][0m Trial 15 finished with value: 0.055458340166430725 and parameters: {'observation_period_num': 15, 'train_rates': 0.8721789937804174, 'learning_rate': 0.00030471423811460126, 'batch_size': 222, 'step_size': 6, 'gamma': 0.7569297022842171}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:39:56,793][0m Trial 16 finished with value: 0.07638131963134787 and parameters: {'observation_period_num': 14, 'train_rates': 0.876562064457454, 'learning_rate': 0.0002722845112298046, 'batch_size': 236, 'step_size': 4, 'gamma': 0.7545958539341197}. Best is trial 15 with value: 0.055458340166430725.[0m
Early stopping at epoch 48
[32m[I 2025-02-06 08:40:11,944][0m Trial 17 finished with value: 0.33205648916108266 and parameters: {'observation_period_num': 56, 'train_rates': 0.8926967880438744, 'learning_rate': 0.0003387527318722987, 'batch_size': 216, 'step_size': 1, 'gamma': 0.7518087844613235}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:40:39,857][0m Trial 18 finished with value: 0.0651326977071308 and parameters: {'observation_period_num': 8, 'train_rates': 0.8237611803883238, 'learning_rate': 7.84607493427847e-05, 'batch_size': 215, 'step_size': 6, 'gamma': 0.8665777150843217}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:41:12,929][0m Trial 19 finished with value: 0.20922799258813507 and parameters: {'observation_period_num': 30, 'train_rates': 0.7523680532341704, 'learning_rate': 0.00012293010751891973, 'batch_size': 165, 'step_size': 4, 'gamma': 0.779818285689877}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:41:35,381][0m Trial 20 finished with value: 0.18152130111175424 and parameters: {'observation_period_num': 63, 'train_rates': 0.7094122282540668, 'learning_rate': 0.0003786649494587778, 'batch_size': 254, 'step_size': 8, 'gamma': 0.9110557603836659}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:42:07,237][0m Trial 21 finished with value: 0.06635352946092597 and parameters: {'observation_period_num': 31, 'train_rates': 0.8471980856619559, 'learning_rate': 0.0001793135161977852, 'batch_size': 188, 'step_size': 6, 'gamma': 0.8027281931093252}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:42:40,709][0m Trial 22 finished with value: 0.06571826444860415 and parameters: {'observation_period_num': 5, 'train_rates': 0.8965998045389505, 'learning_rate': 0.0001395631815876124, 'batch_size': 188, 'step_size': 6, 'gamma': 0.767529738237388}. Best is trial 15 with value: 0.055458340166430725.[0m
[32m[I 2025-02-06 08:43:23,740][0m Trial 23 finished with value: 0.04803222591379081 and parameters: {'observation_period_num': 37, 'train_rates': 0.8310793093142368, 'learning_rate': 0.0004642850542082363, 'batch_size': 133, 'step_size': 5, 'gamma': 0.7927166172401178}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:44:06,252][0m Trial 24 finished with value: 0.09827143438388965 and parameters: {'observation_period_num': 76, 'train_rates': 0.8320065876932926, 'learning_rate': 0.00045074569057301783, 'batch_size': 130, 'step_size': 3, 'gamma': 0.7508634953512433}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:44:47,379][0m Trial 25 finished with value: 0.05395266005108433 and parameters: {'observation_period_num': 46, 'train_rates': 0.9157373670037233, 'learning_rate': 0.0005995002878780379, 'batch_size': 143, 'step_size': 5, 'gamma': 0.7824421914685654}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:45:55,576][0m Trial 26 finished with value: 0.06093440473274893 and parameters: {'observation_period_num': 61, 'train_rates': 0.921750243744994, 'learning_rate': 0.0006549573306592652, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8451776896988189}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:46:37,451][0m Trial 27 finished with value: 0.08074928681765284 and parameters: {'observation_period_num': 45, 'train_rates': 0.9049438034509804, 'learning_rate': 0.0009720394391247477, 'batch_size': 142, 'step_size': 2, 'gamma': 0.7890760810417425}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:47:40,537][0m Trial 28 finished with value: 0.06595268816902088 and parameters: {'observation_period_num': 46, 'train_rates': 0.8718883508578834, 'learning_rate': 0.0005044213568437698, 'batch_size': 89, 'step_size': 5, 'gamma': 0.7706700728864604}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:49:15,019][0m Trial 29 finished with value: 0.1396968751488363 and parameters: {'observation_period_num': 87, 'train_rates': 0.9767984235756234, 'learning_rate': 5.351859374920962e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.8222241397048011}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:49:51,568][0m Trial 30 finished with value: 0.0924109545621005 and parameters: {'observation_period_num': 25, 'train_rates': 0.8144923717437912, 'learning_rate': 0.00011337969593750927, 'batch_size': 160, 'step_size': 2, 'gamma': 0.8802851840786797}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:50:59,155][0m Trial 31 finished with value: 0.06340918821253966 and parameters: {'observation_period_num': 60, 'train_rates': 0.9224191105858508, 'learning_rate': 0.0006240946136849179, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8403839829281909}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:51:46,639][0m Trial 32 finished with value: 0.05737371454315801 and parameters: {'observation_period_num': 45, 'train_rates': 0.937044990541566, 'learning_rate': 0.0002950175713104414, 'batch_size': 128, 'step_size': 5, 'gamma': 0.8616577561539165}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:52:38,572][0m Trial 33 finished with value: 0.09305965900421143 and parameters: {'observation_period_num': 42, 'train_rates': 0.9858226580400907, 'learning_rate': 0.00026715707801659, 'batch_size': 125, 'step_size': 5, 'gamma': 0.7925157805630463}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:53:36,109][0m Trial 34 finished with value: 0.15750994012190428 and parameters: {'observation_period_num': 77, 'train_rates': 0.9407518148791447, 'learning_rate': 4.559372033994411e-05, 'batch_size': 103, 'step_size': 5, 'gamma': 0.8576308563902395}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:54:13,394][0m Trial 35 finished with value: 0.1881079316373885 and parameters: {'observation_period_num': 23, 'train_rates': 0.785811869786137, 'learning_rate': 8.87888040283271e-05, 'batch_size': 146, 'step_size': 9, 'gamma': 0.9350617445404831}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:55:04,353][0m Trial 36 finished with value: 0.20617027569861307 and parameters: {'observation_period_num': 192, 'train_rates': 0.9512654383707219, 'learning_rate': 0.0004109167045236139, 'batch_size': 120, 'step_size': 15, 'gamma': 0.7764297383031631}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:55:45,303][0m Trial 37 finished with value: 0.2339472960256323 and parameters: {'observation_period_num': 46, 'train_rates': 0.86574536194956, 'learning_rate': 1.923924073632828e-05, 'batch_size': 143, 'step_size': 7, 'gamma': 0.8118422990606377}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:57:08,046][0m Trial 38 finished with value: 0.16785030545932905 and parameters: {'observation_period_num': 115, 'train_rates': 0.9075525270103568, 'learning_rate': 0.00023035407250055867, 'batch_size': 67, 'step_size': 7, 'gamma': 0.8777251437867087}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:57:39,341][0m Trial 39 finished with value: 0.2652300298213959 and parameters: {'observation_period_num': 149, 'train_rates': 0.9523606234705871, 'learning_rate': 0.0005256842841947446, 'batch_size': 205, 'step_size': 3, 'gamma': 0.7960302953955349}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:58:14,664][0m Trial 40 finished with value: 0.058095863007335546 and parameters: {'observation_period_num': 36, 'train_rates': 0.9150353378705132, 'learning_rate': 0.0007661701105127441, 'batch_size': 169, 'step_size': 8, 'gamma': 0.8282080226331883}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:58:50,655][0m Trial 41 finished with value: 0.04954339721398391 and parameters: {'observation_period_num': 37, 'train_rates': 0.914255934178482, 'learning_rate': 0.000735290128766425, 'batch_size': 171, 'step_size': 8, 'gamma': 0.8320344559414734}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:59:18,320][0m Trial 42 finished with value: 0.04836166277527809 and parameters: {'observation_period_num': 18, 'train_rates': 0.9394268035604809, 'learning_rate': 0.000407395826276862, 'batch_size': 233, 'step_size': 12, 'gamma': 0.7797462726470014}. Best is trial 23 with value: 0.04803222591379081.[0m
[32m[I 2025-02-06 08:59:49,884][0m Trial 43 finished with value: 0.03807268856016136 and parameters: {'observation_period_num': 17, 'train_rates': 0.8862061231948274, 'learning_rate': 0.000996441815730305, 'batch_size': 200, 'step_size': 13, 'gamma': 0.7828133800720823}. Best is trial 43 with value: 0.03807268856016136.[0m
[32m[I 2025-02-06 09:00:14,878][0m Trial 44 finished with value: 0.2346773072821168 and parameters: {'observation_period_num': 249, 'train_rates': 0.8937712355858044, 'learning_rate': 0.0009591612465588465, 'batch_size': 235, 'step_size': 13, 'gamma': 0.7840744761819334}. Best is trial 43 with value: 0.03807268856016136.[0m
[32m[I 2025-02-06 09:00:47,942][0m Trial 45 finished with value: 0.034583285450935364 and parameters: {'observation_period_num': 5, 'train_rates': 0.9634708160848692, 'learning_rate': 0.0006236658050961512, 'batch_size': 201, 'step_size': 13, 'gamma': 0.8140384250644763}. Best is trial 45 with value: 0.034583285450935364.[0m
[32m[I 2025-02-06 09:01:21,310][0m Trial 46 finished with value: 0.040336865931749344 and parameters: {'observation_period_num': 20, 'train_rates': 0.9705987814092445, 'learning_rate': 0.000768511223167687, 'batch_size': 204, 'step_size': 13, 'gamma': 0.834158084690671}. Best is trial 45 with value: 0.034583285450935364.[0m
[32m[I 2025-02-06 09:01:54,637][0m Trial 47 finished with value: 0.03625570982694626 and parameters: {'observation_period_num': 7, 'train_rates': 0.9688001971649761, 'learning_rate': 0.0009973719029734488, 'batch_size': 206, 'step_size': 13, 'gamma': 0.8125047900006849}. Best is trial 45 with value: 0.034583285450935364.[0m
[32m[I 2025-02-06 09:02:27,580][0m Trial 48 finished with value: 1.4382884502410889 and parameters: {'observation_period_num': 8, 'train_rates': 0.9682171892944429, 'learning_rate': 1.4714904639617695e-06, 'batch_size': 194, 'step_size': 14, 'gamma': 0.816818053000485}. Best is trial 45 with value: 0.034583285450935364.[0m
[32m[I 2025-02-06 09:03:01,051][0m Trial 49 finished with value: 0.03453616425395012 and parameters: {'observation_period_num': 5, 'train_rates': 0.9658958334936608, 'learning_rate': 0.0009901191430005357, 'batch_size': 204, 'step_size': 12, 'gamma': 0.8070169163981237}. Best is trial 49 with value: 0.03453616425395012.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6767 | 0.3443
Epoch 2/300, Loss: 0.2214 | 0.1467
Epoch 3/300, Loss: 0.1497 | 0.1393
Epoch 4/300, Loss: 0.2072 | 0.1610
Epoch 5/300, Loss: 0.2563 | 0.3153
Epoch 6/300, Loss: 0.1972 | 0.4585
Epoch 7/300, Loss: 0.1606 | 0.1489
Epoch 8/300, Loss: 0.1559 | 0.1680
Epoch 9/300, Loss: 0.1476 | 0.0985
Epoch 10/300, Loss: 0.1231 | 0.1405
Epoch 11/300, Loss: 0.1401 | 0.1151
Epoch 12/300, Loss: 0.1543 | 0.1625
Epoch 13/300, Loss: 0.1242 | 0.0908
Epoch 14/300, Loss: 0.1236 | 0.1140
Epoch 15/300, Loss: 0.1413 | 0.0726
Epoch 16/300, Loss: 0.1064 | 0.0949
Epoch 17/300, Loss: 0.1273 | 0.0823
Epoch 18/300, Loss: 0.1207 | 0.1153
Epoch 19/300, Loss: 0.1262 | 0.0677
Epoch 20/300, Loss: 0.1017 | 0.0834
Epoch 21/300, Loss: 0.1059 | 0.0656
Epoch 22/300, Loss: 0.1095 | 0.1101
Epoch 23/300, Loss: 0.1162 | 0.0706
Epoch 24/300, Loss: 0.1217 | 0.1248
Epoch 25/300, Loss: 0.1313 | 0.1055
Epoch 26/300, Loss: 0.1399 | 0.0894
Epoch 27/300, Loss: 0.1527 | 0.1723
Epoch 28/300, Loss: 0.1472 | 0.0828
Epoch 29/300, Loss: 0.1304 | 0.1032
Epoch 30/300, Loss: 0.1269 | 0.0871
Epoch 31/300, Loss: 0.1148 | 0.1572
Epoch 32/300, Loss: 0.1012 | 0.0701
Epoch 33/300, Loss: 0.0903 | 0.0732
Epoch 34/300, Loss: 0.0874 | 0.0616
Epoch 35/300, Loss: 0.0876 | 0.0726
Epoch 36/300, Loss: 0.0875 | 0.0585
Epoch 37/300, Loss: 0.0872 | 0.0708
Epoch 38/300, Loss: 0.0840 | 0.0559
Epoch 39/300, Loss: 0.0825 | 0.0633
Epoch 40/300, Loss: 0.0809 | 0.0552
Epoch 41/300, Loss: 0.0801 | 0.0589
Epoch 42/300, Loss: 0.0794 | 0.0541
Epoch 43/300, Loss: 0.0787 | 0.0570
Epoch 44/300, Loss: 0.0781 | 0.0532
Epoch 45/300, Loss: 0.0777 | 0.0545
Epoch 46/300, Loss: 0.0774 | 0.0526
Epoch 47/300, Loss: 0.0771 | 0.0534
Epoch 48/300, Loss: 0.0769 | 0.0519
Epoch 49/300, Loss: 0.0766 | 0.0527
Epoch 50/300, Loss: 0.0764 | 0.0518
Epoch 51/300, Loss: 0.0761 | 0.0520
Epoch 52/300, Loss: 0.0759 | 0.0514
Epoch 53/300, Loss: 0.0757 | 0.0515
Epoch 54/300, Loss: 0.0755 | 0.0509
Epoch 55/300, Loss: 0.0753 | 0.0510
Epoch 56/300, Loss: 0.0751 | 0.0505
Epoch 57/300, Loss: 0.0749 | 0.0505
Epoch 58/300, Loss: 0.0748 | 0.0501
Epoch 59/300, Loss: 0.0746 | 0.0500
Epoch 60/300, Loss: 0.0744 | 0.0497
Epoch 61/300, Loss: 0.0743 | 0.0496
Epoch 62/300, Loss: 0.0741 | 0.0493
Epoch 63/300, Loss: 0.0740 | 0.0492
Epoch 64/300, Loss: 0.0738 | 0.0489
Epoch 65/300, Loss: 0.0737 | 0.0488
Epoch 66/300, Loss: 0.0735 | 0.0486
Epoch 67/300, Loss: 0.0734 | 0.0484
Epoch 68/300, Loss: 0.0733 | 0.0483
Epoch 69/300, Loss: 0.0731 | 0.0481
Epoch 70/300, Loss: 0.0730 | 0.0479
Epoch 71/300, Loss: 0.0729 | 0.0477
Epoch 72/300, Loss: 0.0728 | 0.0476
Epoch 73/300, Loss: 0.0726 | 0.0475
Epoch 74/300, Loss: 0.0725 | 0.0473
Epoch 75/300, Loss: 0.0724 | 0.0472
Epoch 76/300, Loss: 0.0723 | 0.0470
Epoch 77/300, Loss: 0.0722 | 0.0469
Epoch 78/300, Loss: 0.0721 | 0.0467
Epoch 79/300, Loss: 0.0720 | 0.0466
Epoch 80/300, Loss: 0.0719 | 0.0465
Epoch 81/300, Loss: 0.0718 | 0.0464
Epoch 82/300, Loss: 0.0717 | 0.0463
Epoch 83/300, Loss: 0.0716 | 0.0462
Epoch 84/300, Loss: 0.0716 | 0.0461
Epoch 85/300, Loss: 0.0715 | 0.0460
Epoch 86/300, Loss: 0.0714 | 0.0459
Epoch 87/300, Loss: 0.0713 | 0.0458
Epoch 88/300, Loss: 0.0712 | 0.0457
Epoch 89/300, Loss: 0.0712 | 0.0456
Epoch 90/300, Loss: 0.0711 | 0.0455
Epoch 91/300, Loss: 0.0710 | 0.0454
Epoch 92/300, Loss: 0.0709 | 0.0454
Epoch 93/300, Loss: 0.0709 | 0.0453
Epoch 94/300, Loss: 0.0708 | 0.0452
Epoch 95/300, Loss: 0.0708 | 0.0451
Epoch 96/300, Loss: 0.0707 | 0.0451
Epoch 97/300, Loss: 0.0706 | 0.0450
Epoch 98/300, Loss: 0.0706 | 0.0449
Epoch 99/300, Loss: 0.0705 | 0.0449
Epoch 100/300, Loss: 0.0705 | 0.0448
Epoch 101/300, Loss: 0.0704 | 0.0447
Epoch 102/300, Loss: 0.0704 | 0.0447
Epoch 103/300, Loss: 0.0703 | 0.0446
Epoch 104/300, Loss: 0.0703 | 0.0446
Epoch 105/300, Loss: 0.0702 | 0.0445
Epoch 106/300, Loss: 0.0702 | 0.0445
Epoch 107/300, Loss: 0.0702 | 0.0444
Epoch 108/300, Loss: 0.0701 | 0.0444
Epoch 109/300, Loss: 0.0701 | 0.0443
Epoch 110/300, Loss: 0.0700 | 0.0443
Epoch 111/300, Loss: 0.0700 | 0.0443
Epoch 112/300, Loss: 0.0700 | 0.0442
Epoch 113/300, Loss: 0.0699 | 0.0442
Epoch 114/300, Loss: 0.0699 | 0.0441
Epoch 115/300, Loss: 0.0698 | 0.0441
Epoch 116/300, Loss: 0.0698 | 0.0441
Epoch 117/300, Loss: 0.0698 | 0.0440
Epoch 118/300, Loss: 0.0697 | 0.0440
Epoch 119/300, Loss: 0.0697 | 0.0440
Epoch 120/300, Loss: 0.0697 | 0.0439
Epoch 121/300, Loss: 0.0697 | 0.0439
Epoch 122/300, Loss: 0.0696 | 0.0439
Epoch 123/300, Loss: 0.0696 | 0.0438
Epoch 124/300, Loss: 0.0696 | 0.0438
Epoch 125/300, Loss: 0.0696 | 0.0438
Epoch 126/300, Loss: 0.0695 | 0.0438
Epoch 127/300, Loss: 0.0695 | 0.0437
Epoch 128/300, Loss: 0.0695 | 0.0437
Epoch 129/300, Loss: 0.0695 | 0.0437
Epoch 130/300, Loss: 0.0694 | 0.0437
Epoch 131/300, Loss: 0.0694 | 0.0436
Epoch 132/300, Loss: 0.0694 | 0.0436
Epoch 133/300, Loss: 0.0694 | 0.0436
Epoch 134/300, Loss: 0.0693 | 0.0436
Epoch 135/300, Loss: 0.0693 | 0.0436
Epoch 136/300, Loss: 0.0693 | 0.0435
Epoch 137/300, Loss: 0.0693 | 0.0435
Epoch 138/300, Loss: 0.0693 | 0.0435
Epoch 139/300, Loss: 0.0693 | 0.0435
Epoch 140/300, Loss: 0.0692 | 0.0435
Epoch 141/300, Loss: 0.0692 | 0.0434
Epoch 142/300, Loss: 0.0692 | 0.0434
Epoch 143/300, Loss: 0.0692 | 0.0434
Epoch 144/300, Loss: 0.0692 | 0.0434
Epoch 145/300, Loss: 0.0692 | 0.0434
Epoch 146/300, Loss: 0.0692 | 0.0434
Epoch 147/300, Loss: 0.0691 | 0.0434
Epoch 148/300, Loss: 0.0691 | 0.0433
Epoch 149/300, Loss: 0.0691 | 0.0433
Epoch 150/300, Loss: 0.0691 | 0.0433
Epoch 151/300, Loss: 0.0691 | 0.0433
Epoch 152/300, Loss: 0.0691 | 0.0433
Epoch 153/300, Loss: 0.0691 | 0.0433
Epoch 154/300, Loss: 0.0691 | 0.0433
Epoch 155/300, Loss: 0.0690 | 0.0433
Epoch 156/300, Loss: 0.0690 | 0.0433
Epoch 157/300, Loss: 0.0690 | 0.0433
Epoch 158/300, Loss: 0.0690 | 0.0432
Epoch 159/300, Loss: 0.0690 | 0.0432
Epoch 160/300, Loss: 0.0690 | 0.0432
Epoch 161/300, Loss: 0.0690 | 0.0432
Epoch 162/300, Loss: 0.0690 | 0.0432
Epoch 163/300, Loss: 0.0690 | 0.0432
Epoch 164/300, Loss: 0.0690 | 0.0432
Epoch 165/300, Loss: 0.0690 | 0.0432
Epoch 166/300, Loss: 0.0690 | 0.0432
Epoch 167/300, Loss: 0.0689 | 0.0432
Epoch 168/300, Loss: 0.0689 | 0.0432
Epoch 169/300, Loss: 0.0689 | 0.0432
Epoch 170/300, Loss: 0.0689 | 0.0432
Epoch 171/300, Loss: 0.0689 | 0.0431
Epoch 172/300, Loss: 0.0689 | 0.0431
Epoch 173/300, Loss: 0.0689 | 0.0431
Epoch 174/300, Loss: 0.0689 | 0.0431
Epoch 175/300, Loss: 0.0689 | 0.0431
Epoch 176/300, Loss: 0.0689 | 0.0431
Epoch 177/300, Loss: 0.0689 | 0.0431
Epoch 178/300, Loss: 0.0689 | 0.0431
Epoch 179/300, Loss: 0.0689 | 0.0431
Epoch 180/300, Loss: 0.0689 | 0.0431
Epoch 181/300, Loss: 0.0689 | 0.0431
Epoch 182/300, Loss: 0.0689 | 0.0431
Epoch 183/300, Loss: 0.0689 | 0.0431
Epoch 184/300, Loss: 0.0689 | 0.0431
Epoch 185/300, Loss: 0.0689 | 0.0431
Epoch 186/300, Loss: 0.0688 | 0.0431
Epoch 187/300, Loss: 0.0688 | 0.0431
Epoch 188/300, Loss: 0.0688 | 0.0431
Epoch 189/300, Loss: 0.0688 | 0.0431
Epoch 190/300, Loss: 0.0688 | 0.0431
Epoch 191/300, Loss: 0.0688 | 0.0431
Epoch 192/300, Loss: 0.0688 | 0.0431
Epoch 193/300, Loss: 0.0688 | 0.0431
Epoch 194/300, Loss: 0.0688 | 0.0430
Epoch 195/300, Loss: 0.0688 | 0.0430
Epoch 196/300, Loss: 0.0688 | 0.0430
Epoch 197/300, Loss: 0.0688 | 0.0430
Epoch 198/300, Loss: 0.0688 | 0.0430
Epoch 199/300, Loss: 0.0688 | 0.0430
Epoch 200/300, Loss: 0.0688 | 0.0430
Epoch 201/300, Loss: 0.0688 | 0.0430
Epoch 202/300, Loss: 0.0688 | 0.0430
Epoch 203/300, Loss: 0.0688 | 0.0430
Epoch 204/300, Loss: 0.0688 | 0.0430
Epoch 205/300, Loss: 0.0688 | 0.0430
Epoch 206/300, Loss: 0.0688 | 0.0430
Epoch 207/300, Loss: 0.0688 | 0.0430
Epoch 208/300, Loss: 0.0688 | 0.0430
Epoch 209/300, Loss: 0.0688 | 0.0430
Epoch 210/300, Loss: 0.0688 | 0.0430
Epoch 211/300, Loss: 0.0688 | 0.0430
Epoch 212/300, Loss: 0.0688 | 0.0430
Epoch 213/300, Loss: 0.0688 | 0.0430
Epoch 214/300, Loss: 0.0688 | 0.0430
Epoch 215/300, Loss: 0.0688 | 0.0430
Epoch 216/300, Loss: 0.0688 | 0.0430
Epoch 217/300, Loss: 0.0688 | 0.0430
Epoch 218/300, Loss: 0.0688 | 0.0430
Epoch 219/300, Loss: 0.0688 | 0.0430
Epoch 220/300, Loss: 0.0688 | 0.0430
Epoch 221/300, Loss: 0.0688 | 0.0430
Epoch 222/300, Loss: 0.0688 | 0.0430
Epoch 223/300, Loss: 0.0688 | 0.0430
Epoch 224/300, Loss: 0.0688 | 0.0430
Epoch 225/300, Loss: 0.0688 | 0.0430
Epoch 226/300, Loss: 0.0688 | 0.0430
Epoch 227/300, Loss: 0.0688 | 0.0430
Epoch 228/300, Loss: 0.0688 | 0.0430
Epoch 229/300, Loss: 0.0688 | 0.0430
Epoch 230/300, Loss: 0.0688 | 0.0430
Epoch 231/300, Loss: 0.0688 | 0.0430
Epoch 232/300, Loss: 0.0688 | 0.0430
Epoch 233/300, Loss: 0.0688 | 0.0430
Epoch 234/300, Loss: 0.0688 | 0.0430
Epoch 235/300, Loss: 0.0688 | 0.0430
Epoch 236/300, Loss: 0.0688 | 0.0430
Epoch 237/300, Loss: 0.0688 | 0.0430
Epoch 238/300, Loss: 0.0688 | 0.0430
Epoch 239/300, Loss: 0.0687 | 0.0430
Epoch 240/300, Loss: 0.0687 | 0.0430
Epoch 241/300, Loss: 0.0687 | 0.0430
Epoch 242/300, Loss: 0.0687 | 0.0430
Epoch 243/300, Loss: 0.0687 | 0.0430
Epoch 244/300, Loss: 0.0687 | 0.0430
Epoch 245/300, Loss: 0.0687 | 0.0430
Epoch 246/300, Loss: 0.0687 | 0.0430
Epoch 247/300, Loss: 0.0687 | 0.0430
Epoch 248/300, Loss: 0.0687 | 0.0430
Epoch 249/300, Loss: 0.0687 | 0.0430
Epoch 250/300, Loss: 0.0687 | 0.0430
Epoch 251/300, Loss: 0.0687 | 0.0430
Epoch 252/300, Loss: 0.0687 | 0.0430
Epoch 253/300, Loss: 0.0687 | 0.0430
Epoch 254/300, Loss: 0.0687 | 0.0430
Epoch 255/300, Loss: 0.0687 | 0.0430
Epoch 256/300, Loss: 0.0687 | 0.0430
Epoch 257/300, Loss: 0.0687 | 0.0430
Epoch 258/300, Loss: 0.0687 | 0.0430
Epoch 259/300, Loss: 0.0687 | 0.0430
Epoch 260/300, Loss: 0.0687 | 0.0430
Epoch 261/300, Loss: 0.0687 | 0.0430
Epoch 262/300, Loss: 0.0687 | 0.0430
Epoch 263/300, Loss: 0.0687 | 0.0430
Epoch 264/300, Loss: 0.0687 | 0.0430
Epoch 265/300, Loss: 0.0687 | 0.0430
Epoch 266/300, Loss: 0.0687 | 0.0430
Epoch 267/300, Loss: 0.0687 | 0.0430
Epoch 268/300, Loss: 0.0687 | 0.0430
Epoch 269/300, Loss: 0.0687 | 0.0430
Epoch 270/300, Loss: 0.0687 | 0.0430
Epoch 271/300, Loss: 0.0687 | 0.0430
Epoch 272/300, Loss: 0.0687 | 0.0430
Epoch 273/300, Loss: 0.0687 | 0.0430
Epoch 274/300, Loss: 0.0687 | 0.0430
Epoch 275/300, Loss: 0.0687 | 0.0430
Epoch 276/300, Loss: 0.0687 | 0.0430
Epoch 277/300, Loss: 0.0687 | 0.0430
Epoch 278/300, Loss: 0.0687 | 0.0430
Epoch 279/300, Loss: 0.0687 | 0.0430
Epoch 280/300, Loss: 0.0687 | 0.0430
Epoch 281/300, Loss: 0.0687 | 0.0430
Epoch 282/300, Loss: 0.0687 | 0.0430
Epoch 283/300, Loss: 0.0687 | 0.0430
Epoch 284/300, Loss: 0.0687 | 0.0430
Epoch 285/300, Loss: 0.0687 | 0.0430
Epoch 286/300, Loss: 0.0687 | 0.0430
Epoch 287/300, Loss: 0.0687 | 0.0430
Epoch 288/300, Loss: 0.0687 | 0.0430
Epoch 289/300, Loss: 0.0687 | 0.0430
Epoch 290/300, Loss: 0.0687 | 0.0430
Epoch 291/300, Loss: 0.0687 | 0.0430
Epoch 292/300, Loss: 0.0687 | 0.0430
Epoch 293/300, Loss: 0.0687 | 0.0430
Epoch 294/300, Loss: 0.0687 | 0.0430
Epoch 295/300, Loss: 0.0687 | 0.0430
Epoch 296/300, Loss: 0.0687 | 0.0430
Epoch 297/300, Loss: 0.0687 | 0.0430
Epoch 298/300, Loss: 0.0687 | 0.0430
Epoch 299/300, Loss: 0.0687 | 0.0430
Epoch 300/300, Loss: 0.0687 | 0.0430
Runtime (seconds): 99.01363754272461
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 35.21966812224127
RMSE: 5.9346160888671875
MAE: 5.9346160888671875
R-squared: nan
[188.47539]
