Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Epoch 1/200, trend Loss: 1.6196 | 0.5648
Epoch 2/200, trend Loss: 0.4695 | 0.4621
Epoch 3/200, trend Loss: 0.6255 | 0.7394
Epoch 4/200, trend Loss: 0.5424 | 0.5070
Epoch 5/200, trend Loss: 0.6541 | 0.8128
Epoch 6/200, trend Loss: 0.4419 | 0.3675
Epoch 7/200, trend Loss: 0.4073 | 0.3416
Epoch 8/200, trend Loss: 0.4119 | 0.7465
Epoch 9/200, trend Loss: 0.2760 | 0.3337
Epoch 10/200, trend Loss: 0.2383 | 0.3014
Epoch 11/200, trend Loss: 0.2124 | 0.2860
Epoch 12/200, trend Loss: 0.1987 | 0.2576
Epoch 13/200, trend Loss: 0.1891 | 0.2408
Epoch 14/200, trend Loss: 0.1812 | 0.2303
Epoch 15/200, trend Loss: 0.1747 | 0.2224
Epoch 16/200, trend Loss: 0.1703 | 0.2161
Epoch 17/200, trend Loss: 0.1669 | 0.2103
Epoch 18/200, trend Loss: 0.1637 | 0.2062
Epoch 19/200, trend Loss: 0.1614 | 0.2032
Epoch 20/200, trend Loss: 0.1595 | 0.2002
Epoch 21/200, trend Loss: 0.1578 | 0.1980
Epoch 22/200, trend Loss: 0.1565 | 0.1964
Epoch 23/200, trend Loss: 0.1554 | 0.1947
Epoch 24/200, trend Loss: 0.1544 | 0.1935
Epoch 25/200, trend Loss: 0.1536 | 0.1925
Epoch 26/200, trend Loss: 0.1530 | 0.1916
Epoch 27/200, trend Loss: 0.1523 | 0.1908
Epoch 28/200, trend Loss: 0.1519 | 0.1903
Epoch 29/200, trend Loss: 0.1515 | 0.1897
Epoch 30/200, trend Loss: 0.1511 | 0.1893
Epoch 31/200, trend Loss: 0.1509 | 0.1890
Epoch 32/200, trend Loss: 0.1506 | 0.1886
Epoch 33/200, trend Loss: 0.1504 | 0.1884
Epoch 34/200, trend Loss: 0.1502 | 0.1882
Epoch 35/200, trend Loss: 0.1501 | 0.1880
Epoch 36/200, trend Loss: 0.1500 | 0.1878
Epoch 37/200, trend Loss: 0.1499 | 0.1877
Epoch 38/200, trend Loss: 0.1498 | 0.1876
Epoch 39/200, trend Loss: 0.1497 | 0.1875
Epoch 40/200, trend Loss: 0.1497 | 0.1874
Epoch 41/200, trend Loss: 0.1496 | 0.1874
Epoch 42/200, trend Loss: 0.1496 | 0.1873
Epoch 43/200, trend Loss: 0.1495 | 0.1873
Epoch 44/200, trend Loss: 0.1495 | 0.1872
Epoch 45/200, trend Loss: 0.1495 | 0.1872
Epoch 46/200, trend Loss: 0.1494 | 0.1872
Epoch 47/200, trend Loss: 0.1494 | 0.1871
Epoch 48/200, trend Loss: 0.1494 | 0.1871
Epoch 49/200, trend Loss: 0.1494 | 0.1871
Epoch 50/200, trend Loss: 0.1494 | 0.1871
Epoch 51/200, trend Loss: 0.1494 | 0.1871
Epoch 52/200, trend Loss: 0.1494 | 0.1871
Epoch 53/200, trend Loss: 0.1494 | 0.1871
Epoch 54/200, trend Loss: 0.1493 | 0.1871
Epoch 55/200, trend Loss: 0.1493 | 0.1870
Epoch 56/200, trend Loss: 0.1493 | 0.1870
Epoch 57/200, trend Loss: 0.1493 | 0.1870
Epoch 58/200, trend Loss: 0.1493 | 0.1870
Epoch 59/200, trend Loss: 0.1493 | 0.1870
Epoch 60/200, trend Loss: 0.1493 | 0.1870
Epoch 61/200, trend Loss: 0.1493 | 0.1870
Epoch 62/200, trend Loss: 0.1493 | 0.1870
Epoch 63/200, trend Loss: 0.1493 | 0.1870
Epoch 64/200, trend Loss: 0.1493 | 0.1870
Epoch 65/200, trend Loss: 0.1493 | 0.1870
Epoch 66/200, trend Loss: 0.1493 | 0.1870
Epoch 67/200, trend Loss: 0.1493 | 0.1870
Epoch 68/200, trend Loss: 0.1493 | 0.1870
Epoch 69/200, trend Loss: 0.1493 | 0.1870
Epoch 70/200, trend Loss: 0.1493 | 0.1870
Epoch 71/200, trend Loss: 0.1493 | 0.1870
Epoch 72/200, trend Loss: 0.1493 | 0.1870
Epoch 73/200, trend Loss: 0.1493 | 0.1870
Epoch 74/200, trend Loss: 0.1493 | 0.1870
Epoch 75/200, trend Loss: 0.1493 | 0.1870
Epoch 76/200, trend Loss: 0.1493 | 0.1870
Epoch 77/200, trend Loss: 0.1493 | 0.1870
Epoch 78/200, trend Loss: 0.1493 | 0.1870
Early stopping for trend
Epoch 1/200, seasonal_0 Loss: 0.4929 | 0.6551
Epoch 2/200, seasonal_0 Loss: 0.2770 | 0.4820
Epoch 3/200, seasonal_0 Loss: 0.2690 | 0.4301
Epoch 4/200, seasonal_0 Loss: 0.2692 | 0.4079
Epoch 5/200, seasonal_0 Loss: 0.2520 | 0.3596
Epoch 6/200, seasonal_0 Loss: 0.2099 | 0.3552
Epoch 7/200, seasonal_0 Loss: 0.1815 | 0.3562
Epoch 8/200, seasonal_0 Loss: 0.1828 | 0.3283
Epoch 9/200, seasonal_0 Loss: 0.1826 | 0.3277
Epoch 10/200, seasonal_0 Loss: 0.1766 | 0.3078
Epoch 11/200, seasonal_0 Loss: 0.1752 | 0.2975
Epoch 12/200, seasonal_0 Loss: 0.1490 | 0.2919
Epoch 13/200, seasonal_0 Loss: 0.1389 | 0.2892
Epoch 14/200, seasonal_0 Loss: 0.1313 | 0.2850
Epoch 15/200, seasonal_0 Loss: 0.1290 | 0.2796
Epoch 16/200, seasonal_0 Loss: 0.1260 | 0.2761
Epoch 17/200, seasonal_0 Loss: 0.1233 | 0.2727
Epoch 18/200, seasonal_0 Loss: 0.1217 | 0.2696
Epoch 19/200, seasonal_0 Loss: 0.1200 | 0.2670
Epoch 20/200, seasonal_0 Loss: 0.1185 | 0.2646
Epoch 21/200, seasonal_0 Loss: 0.1174 | 0.2626
Epoch 22/200, seasonal_0 Loss: 0.1163 | 0.2609
Epoch 23/200, seasonal_0 Loss: 0.1155 | 0.2591
Epoch 24/200, seasonal_0 Loss: 0.1147 | 0.2578
Epoch 25/200, seasonal_0 Loss: 0.1140 | 0.2566
Epoch 26/200, seasonal_0 Loss: 0.1135 | 0.2554
Epoch 27/200, seasonal_0 Loss: 0.1129 | 0.2544
Epoch 28/200, seasonal_0 Loss: 0.1125 | 0.2536
Epoch 29/200, seasonal_0 Loss: 0.1121 | 0.2528
Epoch 30/200, seasonal_0 Loss: 0.1117 | 0.2521
Epoch 31/200, seasonal_0 Loss: 0.1114 | 0.2515
Epoch 32/200, seasonal_0 Loss: 0.1111 | 0.2509
Epoch 33/200, seasonal_0 Loss: 0.1109 | 0.2505
Epoch 34/200, seasonal_0 Loss: 0.1107 | 0.2501
Epoch 35/200, seasonal_0 Loss: 0.1105 | 0.2497
Epoch 36/200, seasonal_0 Loss: 0.1103 | 0.2493
Epoch 37/200, seasonal_0 Loss: 0.1102 | 0.2491
Epoch 38/200, seasonal_0 Loss: 0.1100 | 0.2488
Epoch 39/200, seasonal_0 Loss: 0.1099 | 0.2485
Epoch 40/200, seasonal_0 Loss: 0.1098 | 0.2483
Epoch 41/200, seasonal_0 Loss: 0.1097 | 0.2481
Epoch 42/200, seasonal_0 Loss: 0.1096 | 0.2480
Epoch 43/200, seasonal_0 Loss: 0.1095 | 0.2478
Epoch 44/200, seasonal_0 Loss: 0.1095 | 0.2477
Epoch 45/200, seasonal_0 Loss: 0.1094 | 0.2476
Epoch 46/200, seasonal_0 Loss: 0.1094 | 0.2475
Epoch 47/200, seasonal_0 Loss: 0.1093 | 0.2474
Epoch 48/200, seasonal_0 Loss: 0.1093 | 0.2473
Epoch 49/200, seasonal_0 Loss: 0.1092 | 0.2472
Epoch 50/200, seasonal_0 Loss: 0.1092 | 0.2472
Epoch 51/200, seasonal_0 Loss: 0.1092 | 0.2471
Epoch 52/200, seasonal_0 Loss: 0.1092 | 0.2471
Epoch 53/200, seasonal_0 Loss: 0.1091 | 0.2470
Epoch 54/200, seasonal_0 Loss: 0.1091 | 0.2470
Epoch 55/200, seasonal_0 Loss: 0.1091 | 0.2469
Epoch 56/200, seasonal_0 Loss: 0.1091 | 0.2469
Epoch 57/200, seasonal_0 Loss: 0.1091 | 0.2469
Epoch 58/200, seasonal_0 Loss: 0.1091 | 0.2468
Epoch 59/200, seasonal_0 Loss: 0.1090 | 0.2468
Epoch 60/200, seasonal_0 Loss: 0.1090 | 0.2468
Epoch 61/200, seasonal_0 Loss: 0.1090 | 0.2468
Epoch 62/200, seasonal_0 Loss: 0.1090 | 0.2468
Epoch 63/200, seasonal_0 Loss: 0.1090 | 0.2468
Epoch 64/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 65/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 66/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 67/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 68/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 69/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 70/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 71/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 72/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 73/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 74/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 75/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 76/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 77/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 78/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 79/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 80/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 81/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 82/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 83/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 84/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 85/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 86/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 87/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 88/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 89/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 90/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 91/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 92/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 93/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 94/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 95/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 96/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 97/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 98/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 99/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 100/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 101/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 102/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 103/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 104/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 105/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 106/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 107/200, seasonal_0 Loss: 0.1090 | 0.2467
Epoch 108/200, seasonal_0 Loss: 0.1090 | 0.2467
Early stopping for seasonal_0
Epoch 1/200, seasonal_1 Loss: 0.7247 | 1.3798
Epoch 2/200, seasonal_1 Loss: 0.5040 | 1.2420
Epoch 3/200, seasonal_1 Loss: 0.4531 | 1.0024
Epoch 4/200, seasonal_1 Loss: 0.3920 | 0.9830
Epoch 5/200, seasonal_1 Loss: 0.3701 | 0.7950
Epoch 6/200, seasonal_1 Loss: 0.3397 | 0.8254
Epoch 7/200, seasonal_1 Loss: 0.3288 | 0.6799
Epoch 8/200, seasonal_1 Loss: 0.3060 | 0.7737
Epoch 9/200, seasonal_1 Loss: 0.3057 | 0.6235
Epoch 10/200, seasonal_1 Loss: 0.2885 | 0.7054
Epoch 11/200, seasonal_1 Loss: 0.2870 | 0.5814
Epoch 12/200, seasonal_1 Loss: 0.2706 | 0.6545
Epoch 13/200, seasonal_1 Loss: 0.2655 | 0.5559
Epoch 14/200, seasonal_1 Loss: 0.2506 | 0.6114
Epoch 15/200, seasonal_1 Loss: 0.2433 | 0.5385
Epoch 16/200, seasonal_1 Loss: 0.2309 | 0.5658
Epoch 17/200, seasonal_1 Loss: 0.2222 | 0.5335
Epoch 18/200, seasonal_1 Loss: 0.2156 | 0.5342
Epoch 19/200, seasonal_1 Loss: 0.2099 | 0.5210
Epoch 20/200, seasonal_1 Loss: 0.2052 | 0.5158
Epoch 21/200, seasonal_1 Loss: 0.2006 | 0.5065
Epoch 22/200, seasonal_1 Loss: 0.1965 | 0.5015
Epoch 23/200, seasonal_1 Loss: 0.1927 | 0.4949
Epoch 24/200, seasonal_1 Loss: 0.1892 | 0.4895
Epoch 25/200, seasonal_1 Loss: 0.1857 | 0.4843
Epoch 26/200, seasonal_1 Loss: 0.1825 | 0.4785
Epoch 27/200, seasonal_1 Loss: 0.1792 | 0.4743
Epoch 28/200, seasonal_1 Loss: 0.1762 | 0.4683
Epoch 29/200, seasonal_1 Loss: 0.1732 | 0.4650
Epoch 30/200, seasonal_1 Loss: 0.1707 | 0.4597
Epoch 31/200, seasonal_1 Loss: 0.1682 | 0.4569
Epoch 32/200, seasonal_1 Loss: 0.1658 | 0.4514
Epoch 33/200, seasonal_1 Loss: 0.1634 | 0.4492
Epoch 34/200, seasonal_1 Loss: 0.1611 | 0.4433
Epoch 35/200, seasonal_1 Loss: 0.1588 | 0.4421
Epoch 36/200, seasonal_1 Loss: 0.1568 | 0.4348
Epoch 37/200, seasonal_1 Loss: 0.1548 | 0.4368
Epoch 38/200, seasonal_1 Loss: 0.1532 | 0.4271
Epoch 39/200, seasonal_1 Loss: 0.1512 | 0.4322
Epoch 40/200, seasonal_1 Loss: 0.1499 | 0.4181
Epoch 41/200, seasonal_1 Loss: 0.1480 | 0.4305
Epoch 42/200, seasonal_1 Loss: 0.1477 | 0.4065
Epoch 43/200, seasonal_1 Loss: 0.1464 | 0.4375
Epoch 44/200, seasonal_1 Loss: 0.1482 | 0.3913
Epoch 45/200, seasonal_1 Loss: 0.1482 | 0.4518
Epoch 46/200, seasonal_1 Loss: 0.1532 | 0.3782
Epoch 47/200, seasonal_1 Loss: 0.1549 | 0.4750
Epoch 48/200, seasonal_1 Loss: 0.1616 | 0.3699
Epoch 49/200, seasonal_1 Loss: 0.1611 | 0.4761
Epoch 50/200, seasonal_1 Loss: 0.1614 | 0.3729
Epoch 51/200, seasonal_1 Loss: 0.1504 | 0.4207
Epoch 52/200, seasonal_1 Loss: 0.1395 | 0.3966
Epoch 53/200, seasonal_1 Loss: 0.1345 | 0.3944
Epoch 54/200, seasonal_1 Loss: 0.1318 | 0.3974
Epoch 55/200, seasonal_1 Loss: 0.1314 | 0.3904
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/main.py", line 1044, in <module>
    models[comp], train_loss, valid_loss = train(model, train_data, valid_data, optimizer, criterion, scheduler, params['batch_size'], params['observation_period_num'])
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 43, in train
    total_loss_train += loss.item() * data.size(0)
                        ^^^^^^^^^^^
KeyboardInterrupt
