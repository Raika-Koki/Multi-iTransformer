[*********************100%%**********************]  1 of 1 completed
Date
2019-12-31     78.597268
2020-01-02     78.442337
2020-01-03     78.286206
2020-01-06     78.128777
2020-01-07     77.969960
                 ...
2023-05-24    173.117850
2023-05-25    173.456800
2023-05-26    173.795646
2023-05-30    174.134451
2023-05-31    174.473266
Name: trend, Length: 860, dtype: float64
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
{'AAPL': Date
2019-12-31     78.597268
2020-01-02     78.442337
2020-01-03     78.286206
2020-01-06     78.128777
2020-01-07     77.969960
                 ...
2023-05-24    173.117850
2023-05-25    173.456800
2023-05-26    173.795646
2023-05-30    174.134451
2023-05-31    174.473266
Name: trend, Length: 860, dtype: float64, 'GOOGL': Date
2019-12-31     66.804077
2020-01-02     68.264961
2020-01-03     67.907845
2020-01-06     69.717865
2020-01-07     69.583206
                 ...
2023-05-24    120.601372
2023-05-25    123.175003
2023-05-26    124.302208
2023-05-30    123.364532
2023-05-31    122.566513
Name: Adj Close, Length: 860, dtype: float64, 'META': Date
2019-12-31    204.633881
2020-01-02    209.150269
2020-01-03    208.043610
2020-01-06    211.961807
2020-01-07    212.420410
                 ...
2023-05-24    248.461914
2023-05-25    251.931473
2023-05-26    261.253387
2023-05-30    261.731934
2023-05-31    263.925354
Name: Adj Close, Length: 860, dtype: float64, 'AMZN': Date
2019-12-31     92.391998
2020-01-02     94.900497
2020-01-03     93.748497
2020-01-06     95.143997
2020-01-07     95.343002
                 ...
2023-05-24    116.750000
2023-05-25    115.000000
2023-05-26    120.110001
2023-05-30    121.660004
2023-05-31    120.580002
Name: Adj Close, Length: 860, dtype: float64, 'MSFT': Date
2019-12-31    151.139694
2020-01-02    153.938232
2020-01-03    152.021439
2020-01-06    152.414337
2020-01-07    151.024689
                 ...
2023-05-24    310.853638
2023-05-25    322.808411
2023-05-26    329.711884
2023-05-30    328.047882
2023-05-31    325.254822
Name: Adj Close, Length: 860, dtype: float64}
Dataset created successfully.
/home/raikakoki/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
Epoch 1/100, Training Loss: 0.4516, Validation Loss: 0.8078
Epoch 2/100, Training Loss: 0.3818, Validation Loss: 0.2598
Epoch 3/100, Training Loss: 0.1552, Validation Loss: 0.1937
Epoch 4/100, Training Loss: 0.1336, Validation Loss: 0.1657
Epoch 5/100, Training Loss: 0.0940, Validation Loss: 0.1340
Epoch 6/100, Training Loss: 0.0883, Validation Loss: 0.0949
Epoch 7/100, Training Loss: 0.0788, Validation Loss: 0.1007
Epoch 8/100, Training Loss: 0.0615, Validation Loss: 0.0744
Epoch 9/100, Training Loss: 0.0546, Validation Loss: 0.0954
Epoch 10/100, Training Loss: 0.0564, Validation Loss: 0.0753
Epoch 11/100, Training Loss: 0.0431, Validation Loss: 0.0670
Epoch 12/100, Training Loss: 0.0639, Validation Loss: 0.0484
Epoch 13/100, Training Loss: 0.0420, Validation Loss: 0.0374
Epoch 14/100, Training Loss: 0.0433, Validation Loss: 0.0764
Epoch 15/100, Training Loss: 0.0352, Validation Loss: 0.0378
Epoch 16/100, Training Loss: 0.0463, Validation Loss: 0.0602
Epoch 17/100, Training Loss: 0.0473, Validation Loss: 0.0255
Epoch 18/100, Training Loss: 0.0451, Validation Loss: 0.0862
Epoch 19/100, Training Loss: 0.0473, Validation Loss: 0.0274
Epoch 20/100, Training Loss: 0.0316, Validation Loss: 0.0841
Epoch 21/100, Training Loss: 0.0445, Validation Loss: 0.0363
Epoch 22/100, Training Loss: 0.0534, Validation Loss: 0.1763
Epoch 23/100, Training Loss: 0.0881, Validation Loss: 0.0637
Epoch 24/100, Training Loss: 0.0604, Validation Loss: 0.1261
Epoch 25/100, Training Loss: 0.0456, Validation Loss: 0.0408
Epoch 26/100, Training Loss: 0.0339, Validation Loss: 0.0612
Epoch 27/100, Training Loss: 0.0275, Validation Loss: 0.0406
Epoch 28/100, Training Loss: 0.0257, Validation Loss: 0.0455
Epoch 29/100, Training Loss: 0.0231, Validation Loss: 0.0359
Epoch 30/100, Training Loss: 0.0224, Validation Loss: 0.0394
Epoch 31/100, Training Loss: 0.0212, Validation Loss: 0.0324
Epoch 32/100, Training Loss: 0.0207, Validation Loss: 0.0356
Epoch 33/100, Training Loss: 0.0200, Validation Loss: 0.0303
Epoch 34/100, Training Loss: 0.0196, Validation Loss: 0.0329
Epoch 35/100, Training Loss: 0.0191, Validation Loss: 0.0290
Epoch 36/100, Training Loss: 0.0188, Validation Loss: 0.0308
Epoch 37/100, Training Loss: 0.0184, Validation Loss: 0.0281
Epoch 38/100, Training Loss: 0.0181, Validation Loss: 0.0291
Epoch 39/100, Training Loss: 0.0179, Validation Loss: 0.0274
Epoch 40/100, Training Loss: 0.0175, Validation Loss: 0.0276
Epoch 41/100, Training Loss: 0.0175, Validation Loss: 0.0269
Epoch 42/100, Training Loss: 0.0170, Validation Loss: 0.0264
Epoch 43/100, Training Loss: 0.0172, Validation Loss: 0.0266
Epoch 44/100, Training Loss: 0.0167, Validation Loss: 0.0255
Epoch 45/100, Training Loss: 0.0172, Validation Loss: 0.0265
Epoch 46/100, Training Loss: 0.0167, Validation Loss: 0.0249
Epoch 47/100, Training Loss: 0.0177, Validation Loss: 0.0266
Epoch 48/100, Training Loss: 0.0175, Validation Loss: 0.0248
Epoch 49/100, Training Loss: 0.0199, Validation Loss: 0.0273
Epoch 50/100, Training Loss: 0.0206, Validation Loss: 0.0252
Epoch 51/100, Training Loss: 0.0257, Validation Loss: 0.0283
Epoch 52/100, Training Loss: 0.0266, Validation Loss: 0.0248
Epoch 53/100, Training Loss: 0.0306, Validation Loss: 0.0274
Epoch 54/100, Training Loss: 0.0257, Validation Loss: 0.0238
Epoch 55/100, Training Loss: 0.0231, Validation Loss: 0.0256
Epoch 56/100, Training Loss: 0.0183, Validation Loss: 0.0239
Epoch 57/100, Training Loss: 0.0169, Validation Loss: 0.0244
Epoch 58/100, Training Loss: 0.0158, Validation Loss: 0.0238
Epoch 59/100, Training Loss: 0.0155, Validation Loss: 0.0237
Epoch 60/100, Training Loss: 0.0154, Validation Loss: 0.0234
Epoch 61/100, Training Loss: 0.0153, Validation Loss: 0.0233
Epoch 62/100, Training Loss: 0.0152, Validation Loss: 0.0231
Epoch 63/100, Training Loss: 0.0151, Validation Loss: 0.0229
Epoch 64/100, Training Loss: 0.0151, Validation Loss: 0.0228
Epoch 65/100, Training Loss: 0.0150, Validation Loss: 0.0227
Epoch 66/100, Training Loss: 0.0150, Validation Loss: 0.0226
Epoch 67/100, Training Loss: 0.0149, Validation Loss: 0.0225
Epoch 68/100, Training Loss: 0.0149, Validation Loss: 0.0224
Epoch 69/100, Training Loss: 0.0148, Validation Loss: 0.0223
Epoch 70/100, Training Loss: 0.0148, Validation Loss: 0.0222
Epoch 71/100, Training Loss: 0.0147, Validation Loss: 0.0221
Epoch 72/100, Training Loss: 0.0147, Validation Loss: 0.0220
Epoch 73/100, Training Loss: 0.0147, Validation Loss: 0.0220
Epoch 74/100, Training Loss: 0.0146, Validation Loss: 0.0219
Epoch 75/100, Training Loss: 0.0146, Validation Loss: 0.0218
Epoch 76/100, Training Loss: 0.0146, Validation Loss: 0.0218
Epoch 77/100, Training Loss: 0.0145, Validation Loss: 0.0217
Epoch 78/100, Training Loss: 0.0145, Validation Loss: 0.0217
Epoch 79/100, Training Loss: 0.0145, Validation Loss: 0.0216
Epoch 80/100, Training Loss: 0.0145, Validation Loss: 0.0216
Epoch 81/100, Training Loss: 0.0144, Validation Loss: 0.0215
Epoch 82/100, Training Loss: 0.0144, Validation Loss: 0.0215
Epoch 83/100, Training Loss: 0.0144, Validation Loss: 0.0214
Epoch 84/100, Training Loss: 0.0144, Validation Loss: 0.0214
Epoch 85/100, Training Loss: 0.0144, Validation Loss: 0.0214
Epoch 86/100, Training Loss: 0.0143, Validation Loss: 0.0213
Epoch 87/100, Training Loss: 0.0143, Validation Loss: 0.0213
Epoch 88/100, Training Loss: 0.0143, Validation Loss: 0.0213
Epoch 89/100, Training Loss: 0.0143, Validation Loss: 0.0212
Epoch 90/100, Training Loss: 0.0143, Validation Loss: 0.0212
Epoch 91/100, Training Loss: 0.0143, Validation Loss: 0.0212
Epoch 92/100, Training Loss: 0.0142, Validation Loss: 0.0211
Epoch 93/100, Training Loss: 0.0142, Validation Loss: 0.0211
Epoch 94/100, Training Loss: 0.0142, Validation Loss: 0.0211
Epoch 95/100, Training Loss: 0.0142, Validation Loss: 0.0211
Epoch 96/100, Training Loss: 0.0142, Validation Loss: 0.0211
Epoch 97/100, Training Loss: 0.0142, Validation Loss: 0.0210
Epoch 98/100, Training Loss: 0.0142, Validation Loss: 0.0210
Epoch 99/100, Training Loss: 0.0142, Validation Loss: 0.0210
Epoch 100/100, Training Loss: 0.0141, Validation Loss: 0.0210
/mnt/c/Users/RAIKA KOKI/B4研究/Multi_iTransformer/STLdemo.py:261: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_selected_stock_price = predicted_selected_stock_price.cpu().numpy().flatten() * std_list[0] + mean_list[0]  # Using AAPL normalization factors
['2023-05-17', '2023-05-18', '2023-05-19', '2023-05-22', '2023-05-23', '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-30', '2023-05-31']
[168.13521862]
[171.41883967 171.75953144 172.09966959 172.43936827 172.77873215
 173.11785009 173.45679984 173.79564626 174.13445098 168.13521862]
[171.41883967 171.75953144 172.09966959 172.43936827 172.77873215
 173.11785009 173.45679984 173.79564626 174.13445098 174.4732657 ]
/mnt/c/Users/RAIKA KOKI/B4研究/Multi_iTransformer/STLdemo.py:293: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()
