ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 08:21:22,426][0m A new study created in memory with name: no-name-c293fcaa-3934-4cff-8dee-99422177b41e[0m
[32m[I 2025-01-04 08:23:50,721][0m Trial 0 finished with value: 0.11919381832329658 and parameters: {'observation_period_num': 187, 'train_rates': 0.8469613685528796, 'learning_rate': 0.00012033323022831028, 'batch_size': 34, 'step_size': 9, 'gamma': 0.8765236021557457}. Best is trial 0 with value: 0.11919381832329658.[0m
[32m[I 2025-01-04 08:25:13,018][0m Trial 1 finished with value: 0.07128450848057727 and parameters: {'observation_period_num': 54, 'train_rates': 0.7046011627438092, 'learning_rate': 0.00043155276352852397, 'batch_size': 58, 'step_size': 6, 'gamma': 0.9007055640363639}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:26:27,397][0m Trial 2 finished with value: 0.09822400120815428 and parameters: {'observation_period_num': 191, 'train_rates': 0.9634903821859269, 'learning_rate': 0.0002955505567338559, 'batch_size': 77, 'step_size': 10, 'gamma': 0.7894607015980534}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:27:23,950][0m Trial 3 finished with value: 0.44195274453041916 and parameters: {'observation_period_num': 213, 'train_rates': 0.9138965445664908, 'learning_rate': 1.6677629021849228e-06, 'batch_size': 98, 'step_size': 5, 'gamma': 0.9778484111013277}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:28:13,951][0m Trial 4 finished with value: 0.35372982609090464 and parameters: {'observation_period_num': 128, 'train_rates': 0.8604546260468124, 'learning_rate': 1.81837814782424e-06, 'batch_size': 110, 'step_size': 9, 'gamma': 0.963152766506048}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:31:56,054][0m Trial 5 finished with value: 0.10729684404752872 and parameters: {'observation_period_num': 138, 'train_rates': 0.9520900092524021, 'learning_rate': 2.6465664727298014e-05, 'batch_size': 25, 'step_size': 11, 'gamma': 0.9230749185907832}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:32:58,223][0m Trial 6 finished with value: 0.07564167604807101 and parameters: {'observation_period_num': 67, 'train_rates': 0.7841499398678438, 'learning_rate': 1.2536740743568985e-05, 'batch_size': 83, 'step_size': 15, 'gamma': 0.9198064034337247}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:33:30,853][0m Trial 7 finished with value: 0.6701262213681873 and parameters: {'observation_period_num': 27, 'train_rates': 0.8703073620541915, 'learning_rate': 1.173047152340193e-06, 'batch_size': 203, 'step_size': 9, 'gamma': 0.8563545748009231}. Best is trial 1 with value: 0.07128450848057727.[0m
[32m[I 2025-01-04 08:34:35,265][0m Trial 8 finished with value: 0.037179186525623256 and parameters: {'observation_period_num': 34, 'train_rates': 0.7814662739383021, 'learning_rate': 0.0003665647911925675, 'batch_size': 81, 'step_size': 14, 'gamma': 0.7904534220407139}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:35:02,794][0m Trial 9 finished with value: 0.5464539786088302 and parameters: {'observation_period_num': 186, 'train_rates': 0.6861965904212898, 'learning_rate': 5.734061698659146e-06, 'batch_size': 193, 'step_size': 6, 'gamma': 0.9051534362971725}. Best is trial 8 with value: 0.037179186525623256.[0m
Early stopping at epoch 56
[32m[I 2025-01-04 08:35:20,768][0m Trial 10 finished with value: 0.15844699803084264 and parameters: {'observation_period_num': 6, 'train_rates': 0.6045724394938283, 'learning_rate': 9.933327146544461e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.7507037788773907}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:36:50,929][0m Trial 11 finished with value: 0.07915579189741334 and parameters: {'observation_period_num': 74, 'train_rates': 0.7461202637330828, 'learning_rate': 0.0006881484498089452, 'batch_size': 55, 'step_size': 14, 'gamma': 0.8313073457665799}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:37:29,238][0m Trial 12 finished with value: 0.06434633050837534 and parameters: {'observation_period_num': 55, 'train_rates': 0.7120337672174463, 'learning_rate': 0.0008265510510513907, 'batch_size': 135, 'step_size': 4, 'gamma': 0.8152151720396043}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:38:02,034][0m Trial 13 finished with value: 0.15886020416879468 and parameters: {'observation_period_num': 106, 'train_rates': 0.6389925022315959, 'learning_rate': 0.00016242619934010724, 'batch_size': 142, 'step_size': 2, 'gamma': 0.8040886541650303}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:38:32,431][0m Trial 14 finished with value: 0.041366946963679714 and parameters: {'observation_period_num': 35, 'train_rates': 0.7929342116821781, 'learning_rate': 0.000983415194960802, 'batch_size': 246, 'step_size': 12, 'gamma': 0.7619621965257588}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:39:02,854][0m Trial 15 finished with value: 0.0515542114888759 and parameters: {'observation_period_num': 9, 'train_rates': 0.7934428615002715, 'learning_rate': 5.8714228848635416e-05, 'batch_size': 252, 'step_size': 13, 'gamma': 0.7543853418060212}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:39:30,626][0m Trial 16 finished with value: 0.16672148029246028 and parameters: {'observation_period_num': 251, 'train_rates': 0.8136580613562183, 'learning_rate': 0.00024927209663991366, 'batch_size': 254, 'step_size': 12, 'gamma': 0.7796063080548287}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:40:01,627][0m Trial 17 finished with value: 0.09008500765722531 and parameters: {'observation_period_num': 97, 'train_rates': 0.7723295515530291, 'learning_rate': 0.0006953426776922469, 'batch_size': 189, 'step_size': 15, 'gamma': 0.8364495215335643}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:40:31,418][0m Trial 18 finished with value: 0.06744793444055933 and parameters: {'observation_period_num': 35, 'train_rates': 0.7370543252381636, 'learning_rate': 5.5160314721588506e-05, 'batch_size': 224, 'step_size': 12, 'gamma': 0.7722730278848099}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:41:14,952][0m Trial 19 finished with value: 0.05019739490805888 and parameters: {'observation_period_num': 92, 'train_rates': 0.821921242581422, 'learning_rate': 0.0003170127203920447, 'batch_size': 124, 'step_size': 13, 'gamma': 0.8045544409241524}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:41:51,392][0m Trial 20 finished with value: 0.130816865039159 and parameters: {'observation_period_num': 145, 'train_rates': 0.9115449380160333, 'learning_rate': 0.000903089972980099, 'batch_size': 163, 'step_size': 11, 'gamma': 0.8573866059882237}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:42:39,948][0m Trial 21 finished with value: 0.06019583570455618 and parameters: {'observation_period_num': 85, 'train_rates': 0.8219070915537019, 'learning_rate': 0.0003459644624208613, 'batch_size': 113, 'step_size': 13, 'gamma': 0.8015863498245609}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:43:18,983][0m Trial 22 finished with value: 0.03878339746823678 and parameters: {'observation_period_num': 32, 'train_rates': 0.7551480423093055, 'learning_rate': 0.00018713206861114146, 'batch_size': 132, 'step_size': 14, 'gamma': 0.7705721447121532}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:43:52,720][0m Trial 23 finished with value: 0.0378762723072275 and parameters: {'observation_period_num': 33, 'train_rates': 0.7561357730101476, 'learning_rate': 0.00017720615156970789, 'batch_size': 162, 'step_size': 15, 'gamma': 0.7700262369250649}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:44:23,234][0m Trial 24 finished with value: 0.05029915300289505 and parameters: {'observation_period_num': 28, 'train_rates': 0.666096808186826, 'learning_rate': 0.00015990296927965593, 'batch_size': 167, 'step_size': 15, 'gamma': 0.7855163766538199}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:44:58,542][0m Trial 25 finished with value: 0.06259887786079739 and parameters: {'observation_period_num': 52, 'train_rates': 0.7507080908593211, 'learning_rate': 6.224183579817435e-05, 'batch_size': 146, 'step_size': 14, 'gamma': 0.8243367960415023}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:45:53,546][0m Trial 26 finished with value: 0.09786882251501083 and parameters: {'observation_period_num': 114, 'train_rates': 0.7243296632509106, 'learning_rate': 3.088837457232381e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.7724183109514551}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:46:25,862][0m Trial 27 finished with value: 0.03906402598928522 and parameters: {'observation_period_num': 19, 'train_rates': 0.76074683694116, 'learning_rate': 0.00019719502399167136, 'batch_size': 178, 'step_size': 15, 'gamma': 0.7884033984526745}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:47:55,217][0m Trial 28 finished with value: 0.07865370462504809 and parameters: {'observation_period_num': 45, 'train_rates': 0.6923915378213358, 'learning_rate': 8.359176433554221e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8450170876438692}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:48:41,617][0m Trial 29 finished with value: 0.0626778976487896 and parameters: {'observation_period_num': 70, 'train_rates': 0.8859269910847543, 'learning_rate': 0.0004835180953477032, 'batch_size': 122, 'step_size': 10, 'gamma': 0.8724687205972514}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:51:00,842][0m Trial 30 finished with value: 0.0884078548504756 and parameters: {'observation_period_num': 158, 'train_rates': 0.8371304884659647, 'learning_rate': 1.4172428775213235e-05, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8174792038382689}. Best is trial 8 with value: 0.037179186525623256.[0m
[32m[I 2025-01-04 08:51:33,403][0m Trial 31 finished with value: 0.033910283735278025 and parameters: {'observation_period_num': 17, 'train_rates': 0.7647588518816681, 'learning_rate': 0.00016510556712725642, 'batch_size': 176, 'step_size': 15, 'gamma': 0.7924618979965227}. Best is trial 31 with value: 0.033910283735278025.[0m
[32m[I 2025-01-04 08:52:03,403][0m Trial 32 finished with value: 0.043002227240282555 and parameters: {'observation_period_num': 16, 'train_rates': 0.7652143414313425, 'learning_rate': 0.0001151635530648398, 'batch_size': 210, 'step_size': 13, 'gamma': 0.7665528267782125}. Best is trial 31 with value: 0.033910283735278025.[0m
[32m[I 2025-01-04 08:52:34,455][0m Trial 33 finished with value: 0.048467409004372415 and parameters: {'observation_period_num': 45, 'train_rates': 0.7320916393246403, 'learning_rate': 0.00046360181757181223, 'batch_size': 178, 'step_size': 15, 'gamma': 0.7944674152223321}. Best is trial 31 with value: 0.033910283735278025.[0m
[32m[I 2025-01-04 08:53:11,585][0m Trial 34 finished with value: 0.0506039227048556 and parameters: {'observation_period_num': 67, 'train_rates': 0.8024038194057136, 'learning_rate': 0.00021035926870158417, 'batch_size': 152, 'step_size': 14, 'gamma': 0.7762067952106112}. Best is trial 31 with value: 0.033910283735278025.[0m
[32m[I 2025-01-04 08:54:23,048][0m Trial 35 finished with value: 0.051900048986308935 and parameters: {'observation_period_num': 40, 'train_rates': 0.6686154191947659, 'learning_rate': 0.0001303717624061646, 'batch_size': 66, 'step_size': 12, 'gamma': 0.8085850748984468}. Best is trial 31 with value: 0.033910283735278025.[0m
[32m[I 2025-01-04 08:55:08,598][0m Trial 36 finished with value: 0.023952013062157294 and parameters: {'observation_period_num': 18, 'train_rates': 0.8452595592178976, 'learning_rate': 0.00047768912986868953, 'batch_size': 130, 'step_size': 11, 'gamma': 0.7502916090404769}. Best is trial 36 with value: 0.023952013062157294.[0m
[32m[I 2025-01-04 08:56:03,001][0m Trial 37 finished with value: 0.02567053393199561 and parameters: {'observation_period_num': 21, 'train_rates': 0.8407762224096509, 'learning_rate': 0.0004814908880849333, 'batch_size': 104, 'step_size': 10, 'gamma': 0.7501718463902659}. Best is trial 36 with value: 0.023952013062157294.[0m
[32m[I 2025-01-04 08:57:00,609][0m Trial 38 finished with value: 0.024106672864632105 and parameters: {'observation_period_num': 5, 'train_rates': 0.8426615702953895, 'learning_rate': 0.0005356731336149491, 'batch_size': 99, 'step_size': 8, 'gamma': 0.7513627971891745}. Best is trial 36 with value: 0.023952013062157294.[0m
[32m[I 2025-01-04 08:57:56,574][0m Trial 39 finished with value: 0.020742346134733657 and parameters: {'observation_period_num': 5, 'train_rates': 0.853109244237127, 'learning_rate': 0.0005572030996297379, 'batch_size': 101, 'step_size': 8, 'gamma': 0.751870581939154}. Best is trial 39 with value: 0.020742346134733657.[0m
[32m[I 2025-01-04 08:58:49,063][0m Trial 40 finished with value: 0.023715768262085855 and parameters: {'observation_period_num': 18, 'train_rates': 0.8920399576234708, 'learning_rate': 0.0004399602897722265, 'batch_size': 109, 'step_size': 8, 'gamma': 0.7508158460873633}. Best is trial 39 with value: 0.020742346134733657.[0m
[32m[I 2025-01-04 08:59:47,517][0m Trial 41 finished with value: 0.02570362301719305 and parameters: {'observation_period_num': 5, 'train_rates': 0.8959427198142691, 'learning_rate': 0.0005861752095949796, 'batch_size': 101, 'step_size': 8, 'gamma': 0.7511542686920389}. Best is trial 39 with value: 0.020742346134733657.[0m
[32m[I 2025-01-04 09:00:42,742][0m Trial 42 finished with value: 0.025238238909050054 and parameters: {'observation_period_num': 21, 'train_rates': 0.8546284472227469, 'learning_rate': 0.0004999805917458672, 'batch_size': 101, 'step_size': 8, 'gamma': 0.7598864147933362}. Best is trial 39 with value: 0.020742346134733657.[0m
[32m[I 2025-01-04 09:01:44,486][0m Trial 43 finished with value: 0.04612870456011438 and parameters: {'observation_period_num': 56, 'train_rates': 0.9539024672104249, 'learning_rate': 0.000293057625563256, 'batch_size': 97, 'step_size': 8, 'gamma': 0.7636882647501333}. Best is trial 39 with value: 0.020742346134733657.[0m
[32m[I 2025-01-04 09:03:06,562][0m Trial 44 finished with value: 0.015185908414423466 and parameters: {'observation_period_num': 7, 'train_rates': 0.9863390432074738, 'learning_rate': 0.0006061438854803278, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9643488539682092}. Best is trial 44 with value: 0.015185908414423466.[0m
[32m[I 2025-01-04 09:04:35,381][0m Trial 45 finished with value: 0.058595750480890274 and parameters: {'observation_period_num': 5, 'train_rates': 0.9828442213958715, 'learning_rate': 3.5849227999924617e-06, 'batch_size': 69, 'step_size': 6, 'gamma': 0.956353283181557}. Best is trial 44 with value: 0.015185908414423466.[0m
[32m[I 2025-01-04 09:05:42,939][0m Trial 46 finished with value: 0.029242914589777798 and parameters: {'observation_period_num': 14, 'train_rates': 0.9277023773651667, 'learning_rate': 0.0006879542339034372, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9871759575632988}. Best is trial 44 with value: 0.015185908414423466.[0m
[32m[I 2025-01-04 09:06:31,765][0m Trial 47 finished with value: 0.07071587979181172 and parameters: {'observation_period_num': 56, 'train_rates': 0.8741886199702351, 'learning_rate': 0.0003587507248663803, 'batch_size': 115, 'step_size': 9, 'gamma': 0.9440822792652288}. Best is trial 44 with value: 0.015185908414423466.[0m
[32m[I 2025-01-04 09:07:50,635][0m Trial 48 finished with value: 0.04050965116105296 and parameters: {'observation_period_num': 23, 'train_rates': 0.9399218771692894, 'learning_rate': 0.0009504044531392273, 'batch_size': 76, 'step_size': 5, 'gamma': 0.887440612314062}. Best is trial 44 with value: 0.015185908414423466.[0m
[32m[I 2025-01-04 09:08:42,980][0m Trial 49 finished with value: 0.03407866880297661 and parameters: {'observation_period_num': 6, 'train_rates': 0.9778345324224454, 'learning_rate': 0.00025100109243252983, 'batch_size': 124, 'step_size': 7, 'gamma': 0.9110003278650685}. Best is trial 44 with value: 0.015185908414423466.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 09:08:42,990][0m A new study created in memory with name: no-name-6c908a52-9870-4fe5-82f4-85fb0de04ed5[0m
[32m[I 2025-01-04 09:12:15,322][0m Trial 0 finished with value: 0.05840399925993663 and parameters: {'observation_period_num': 30, 'train_rates': 0.7438253233321689, 'learning_rate': 0.0004914416496843906, 'batch_size': 184, 'step_size': 10, 'gamma': 0.7782646828375275}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 09:22:44,273][0m Trial 1 finished with value: 0.09649496967041934 and parameters: {'observation_period_num': 97, 'train_rates': 0.7918821874667767, 'learning_rate': 0.00037008030242963093, 'batch_size': 44, 'step_size': 13, 'gamma': 0.8853719948753417}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 09:26:28,625][0m Trial 2 finished with value: 0.12681225326809067 and parameters: {'observation_period_num': 39, 'train_rates': 0.8518993892529161, 'learning_rate': 3.6919631958391136e-06, 'batch_size': 235, 'step_size': 5, 'gamma': 0.8728964272462214}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 09:35:28,470][0m Trial 3 finished with value: 2.05696329058739 and parameters: {'observation_period_num': 58, 'train_rates': 0.8240636172751548, 'learning_rate': 0.0006896160728707937, 'batch_size': 54, 'step_size': 4, 'gamma': 0.7992157823749324}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 09:45:58,055][0m Trial 4 finished with value: 0.08271618002380421 and parameters: {'observation_period_num': 38, 'train_rates': 0.8952660043927245, 'learning_rate': 9.319255314603975e-05, 'batch_size': 49, 'step_size': 13, 'gamma': 0.9519865495747972}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 09:52:44,084][0m Trial 5 finished with value: 0.12513485878821046 and parameters: {'observation_period_num': 235, 'train_rates': 0.9046815866722924, 'learning_rate': 0.0005876632873410159, 'batch_size': 73, 'step_size': 8, 'gamma': 0.8740632931559158}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 09:56:19,223][0m Trial 6 finished with value: 0.3431200002832637 and parameters: {'observation_period_num': 165, 'train_rates': 0.8327221600429549, 'learning_rate': 1.1724660595715764e-06, 'batch_size': 253, 'step_size': 10, 'gamma': 0.934275001630851}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 10:00:15,312][0m Trial 7 finished with value: 0.17408464334692275 and parameters: {'observation_period_num': 126, 'train_rates': 0.6904145688990336, 'learning_rate': 0.0007212041336275405, 'batch_size': 117, 'step_size': 13, 'gamma': 0.9173665538216942}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 10:06:57,317][0m Trial 8 finished with value: 0.14578895792365074 and parameters: {'observation_period_num': 118, 'train_rates': 0.9154682603513615, 'learning_rate': 2.311865815650046e-06, 'batch_size': 78, 'step_size': 11, 'gamma': 0.8231247950671371}. Best is trial 0 with value: 0.05840399925993663.[0m
[32m[I 2025-01-04 10:10:32,938][0m Trial 9 finished with value: 0.04963985630934855 and parameters: {'observation_period_num': 22, 'train_rates': 0.7809147013791513, 'learning_rate': 5.447298374257553e-05, 'batch_size': 199, 'step_size': 6, 'gamma': 0.805166771345719}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:13:36,900][0m Trial 10 finished with value: 0.4410322547878414 and parameters: {'observation_period_num': 190, 'train_rates': 0.600959947397754, 'learning_rate': 2.007135821720785e-05, 'batch_size': 179, 'step_size': 2, 'gamma': 0.8255549302406296}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:17:10,025][0m Trial 11 finished with value: 0.04991288734383361 and parameters: {'observation_period_num': 6, 'train_rates': 0.7373969493614635, 'learning_rate': 8.279045296882747e-05, 'batch_size': 186, 'step_size': 7, 'gamma': 0.7550658202159592}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:21:23,445][0m Trial 12 finished with value: 0.05044768005609512 and parameters: {'observation_period_num': 5, 'train_rates': 0.9866403432543156, 'learning_rate': 4.250924090240823e-05, 'batch_size': 199, 'step_size': 7, 'gamma': 0.7511745523603158}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:25:00,530][0m Trial 13 finished with value: 0.177144384849926 and parameters: {'observation_period_num': 83, 'train_rates': 0.7034964155750127, 'learning_rate': 0.00011960144460181177, 'batch_size': 151, 'step_size': 6, 'gamma': 0.7527545263624225}. Best is trial 9 with value: 0.04963985630934855.[0m
Early stopping at epoch 65
[32m[I 2025-01-04 10:27:20,292][0m Trial 14 finished with value: 0.11831624455566157 and parameters: {'observation_period_num': 8, 'train_rates': 0.765966415365905, 'learning_rate': 1.1741097549463704e-05, 'batch_size': 215, 'step_size': 1, 'gamma': 0.8311588643863101}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:30:56,485][0m Trial 15 finished with value: 0.1506319903876202 and parameters: {'observation_period_num': 68, 'train_rates': 0.6519714641802401, 'learning_rate': 0.00013581960152704171, 'batch_size': 144, 'step_size': 3, 'gamma': 0.7920497560550742}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:35:09,711][0m Trial 16 finished with value: 0.2069237616565205 and parameters: {'observation_period_num': 159, 'train_rates': 0.7362942126283806, 'learning_rate': 5.675866895393718e-05, 'batch_size': 114, 'step_size': 8, 'gamma': 0.8425089056576046}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:38:29,830][0m Trial 17 finished with value: 0.13145889265474325 and parameters: {'observation_period_num': 59, 'train_rates': 0.6661281141933038, 'learning_rate': 9.928769207018939e-06, 'batch_size': 162, 'step_size': 5, 'gamma': 0.9783992727578781}. Best is trial 9 with value: 0.04963985630934855.[0m
[32m[I 2025-01-04 10:42:01,390][0m Trial 18 finished with value: 0.036003719363361594 and parameters: {'observation_period_num': 5, 'train_rates': 0.789743754020081, 'learning_rate': 0.00027006554845270295, 'batch_size': 217, 'step_size': 15, 'gamma': 0.778372333803797}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 10:45:29,803][0m Trial 19 finished with value: 0.1536635877063185 and parameters: {'observation_period_num': 94, 'train_rates': 0.7857632966318246, 'learning_rate': 0.00027524180622054016, 'batch_size': 217, 'step_size': 15, 'gamma': 0.8002468333673703}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 10:49:01,417][0m Trial 20 finished with value: 0.2463333080939161 and parameters: {'observation_period_num': 240, 'train_rates': 0.8640670588942313, 'learning_rate': 0.00019518265784902045, 'batch_size': 249, 'step_size': 11, 'gamma': 0.8471701173636839}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 10:52:26,046][0m Trial 21 finished with value: 0.05477438772599377 and parameters: {'observation_period_num': 22, 'train_rates': 0.7430086305021998, 'learning_rate': 5.8547336736537465e-05, 'batch_size': 199, 'step_size': 7, 'gamma': 0.771522123631972}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 10:56:09,162][0m Trial 22 finished with value: 0.049508936042266506 and parameters: {'observation_period_num': 5, 'train_rates': 0.8106509755159454, 'learning_rate': 2.7792840950897007e-05, 'batch_size': 225, 'step_size': 8, 'gamma': 0.7717020713531735}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 10:59:53,029][0m Trial 23 finished with value: 0.05788656410829 and parameters: {'observation_period_num': 37, 'train_rates': 0.8069526422194556, 'learning_rate': 2.7530251943338007e-05, 'batch_size': 229, 'step_size': 9, 'gamma': 0.8061229752275659}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:25:47,646][0m Trial 24 finished with value: 0.22508394054096678 and parameters: {'observation_period_num': 52, 'train_rates': 0.873295836791816, 'learning_rate': 1.4812780250887127e-05, 'batch_size': 19, 'step_size': 15, 'gamma': 0.7765525948775603}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:29:18,559][0m Trial 25 finished with value: 0.13359477407565074 and parameters: {'observation_period_num': 26, 'train_rates': 0.7705888176789522, 'learning_rate': 5.671971812361326e-06, 'batch_size': 213, 'step_size': 5, 'gamma': 0.7775668255585244}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:33:10,205][0m Trial 26 finished with value: 0.07493225209517022 and parameters: {'observation_period_num': 72, 'train_rates': 0.822653422788702, 'learning_rate': 3.724407684927387e-05, 'batch_size': 165, 'step_size': 9, 'gamma': 0.8118371264580415}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:37:22,228][0m Trial 27 finished with value: 0.08726540952920914 and parameters: {'observation_period_num': 24, 'train_rates': 0.949058203116628, 'learning_rate': 0.00020655286978619352, 'batch_size': 231, 'step_size': 12, 'gamma': 0.8480654106290408}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:41:50,720][0m Trial 28 finished with value: 0.054950927116623964 and parameters: {'observation_period_num': 45, 'train_rates': 0.8457278597451493, 'learning_rate': 2.5060283953711605e-05, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7866505628648001}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:45:25,042][0m Trial 29 finished with value: 0.08870363796781178 and parameters: {'observation_period_num': 20, 'train_rates': 0.7221352278998505, 'learning_rate': 7.965274830069366e-06, 'batch_size': 188, 'step_size': 9, 'gamma': 0.7618359410651657}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:48:56,857][0m Trial 30 finished with value: 0.1399358798664199 and parameters: {'observation_period_num': 109, 'train_rates': 0.797117284079836, 'learning_rate': 0.00044723913453137215, 'batch_size': 242, 'step_size': 10, 'gamma': 0.7698801949852845}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:52:39,884][0m Trial 31 finished with value: 0.044409267000249913 and parameters: {'observation_period_num': 7, 'train_rates': 0.7740868180452319, 'learning_rate': 7.776703207397233e-05, 'batch_size': 186, 'step_size': 7, 'gamma': 0.7617726713450518}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:56:10,696][0m Trial 32 finished with value: 0.047847295800844826 and parameters: {'observation_period_num': 5, 'train_rates': 0.7649911954244705, 'learning_rate': 6.717122358229287e-05, 'batch_size': 209, 'step_size': 8, 'gamma': 0.7854233430809295}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 11:59:44,799][0m Trial 33 finished with value: 0.04265809785365389 and parameters: {'observation_period_num': 6, 'train_rates': 0.7585158952481765, 'learning_rate': 0.00018974012696610412, 'batch_size': 220, 'step_size': 8, 'gamma': 0.7849671371384934}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:03:13,632][0m Trial 34 finished with value: 0.06015253026454701 and parameters: {'observation_period_num': 37, 'train_rates': 0.7607049505274616, 'learning_rate': 0.00028966002308874767, 'batch_size': 207, 'step_size': 11, 'gamma': 0.7862618754711838}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:06:39,610][0m Trial 35 finished with value: 0.1343038095669313 and parameters: {'observation_period_num': 49, 'train_rates': 0.7007520971410259, 'learning_rate': 0.00015685764166270658, 'batch_size': 178, 'step_size': 4, 'gamma': 0.8170352139305795}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:10:30,840][0m Trial 36 finished with value: 0.04960600064801318 and parameters: {'observation_period_num': 14, 'train_rates': 0.7617786901820895, 'learning_rate': 9.127283588997615e-05, 'batch_size': 171, 'step_size': 14, 'gamma': 0.9034818571920761}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:13:51,681][0m Trial 37 finished with value: 0.07174627654126921 and parameters: {'observation_period_num': 34, 'train_rates': 0.7196648908935848, 'learning_rate': 0.0003428978095751461, 'batch_size': 239, 'step_size': 10, 'gamma': 0.7890158529372403}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:17:21,621][0m Trial 38 finished with value: 2.016533069660009 and parameters: {'observation_period_num': 144, 'train_rates': 0.7937563532485558, 'learning_rate': 0.0009301944187734605, 'batch_size': 195, 'step_size': 8, 'gamma': 0.8678706815320936}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:20:30,921][0m Trial 39 finished with value: 0.2559875792041819 and parameters: {'observation_period_num': 72, 'train_rates': 0.6753938856541958, 'learning_rate': 0.00022055698381242048, 'batch_size': 252, 'step_size': 9, 'gamma': 0.7620847967026622}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:24:01,963][0m Trial 40 finished with value: 0.17088046137775695 and parameters: {'observation_period_num': 206, 'train_rates': 0.8266083333831649, 'learning_rate': 7.777732798196668e-05, 'batch_size': 222, 'step_size': 7, 'gamma': 0.8576176435824032}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:27:48,674][0m Trial 41 finished with value: 0.04009554013609886 and parameters: {'observation_period_num': 15, 'train_rates': 0.8008030455399747, 'learning_rate': 0.00011901198962292101, 'batch_size': 234, 'step_size': 8, 'gamma': 0.7681039094822261}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:31:16,669][0m Trial 42 finished with value: 0.05199406739695336 and parameters: {'observation_period_num': 19, 'train_rates': 0.7500792954988449, 'learning_rate': 0.00011753391871001066, 'batch_size': 207, 'step_size': 8, 'gamma': 0.79848112559746}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:35:00,275][0m Trial 43 finished with value: 0.04855198951768669 and parameters: {'observation_period_num': 32, 'train_rates': 0.8418789157986856, 'learning_rate': 0.0005365230389871575, 'batch_size': 232, 'step_size': 6, 'gamma': 0.7627991026685067}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:38:35,654][0m Trial 44 finished with value: 0.0576432652897754 and parameters: {'observation_period_num': 16, 'train_rates': 0.7791284137481012, 'learning_rate': 6.806183042304219e-05, 'batch_size': 245, 'step_size': 5, 'gamma': 0.7818411496185677}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:42:14,158][0m Trial 45 finished with value: 0.08834078118746413 and parameters: {'observation_period_num': 60, 'train_rates': 0.819413764776515, 'learning_rate': 0.00015705000635511375, 'batch_size': 207, 'step_size': 12, 'gamma': 0.7501161491102879}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:47:06,113][0m Trial 46 finished with value: 0.08273599978083417 and parameters: {'observation_period_num': 44, 'train_rates': 0.7229410403937028, 'learning_rate': 0.0004033629993245288, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8866820371096701}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:50:47,260][0m Trial 47 finished with value: 0.04632607926925023 and parameters: {'observation_period_num': 12, 'train_rates': 0.7960884602545124, 'learning_rate': 4.0967714825141356e-05, 'batch_size': 256, 'step_size': 10, 'gamma': 0.7979921483880333}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:54:40,972][0m Trial 48 finished with value: 0.054952748832018936 and parameters: {'observation_period_num': 29, 'train_rates': 0.8836808404298353, 'learning_rate': 4.365617447946164e-05, 'batch_size': 253, 'step_size': 12, 'gamma': 0.7978327161595656}. Best is trial 18 with value: 0.036003719363361594.[0m
[32m[I 2025-01-04 12:58:26,187][0m Trial 49 finished with value: 0.05440755703651586 and parameters: {'observation_period_num': 13, 'train_rates': 0.7967848492665507, 'learning_rate': 0.00011327804479838376, 'batch_size': 233, 'step_size': 13, 'gamma': 0.8338121883110945}. Best is trial 18 with value: 0.036003719363361594.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 12:58:26,195][0m A new study created in memory with name: no-name-5578ca43-7e2c-474a-b621-31be2ad36523[0m
[32m[I 2025-01-04 13:02:32,374][0m Trial 0 finished with value: 0.17972268164157867 and parameters: {'observation_period_num': 216, 'train_rates': 0.9621137678150196, 'learning_rate': 8.182247745237736e-05, 'batch_size': 155, 'step_size': 2, 'gamma': 0.9500368080977597}. Best is trial 0 with value: 0.17972268164157867.[0m
[32m[I 2025-01-04 13:06:58,172][0m Trial 1 finished with value: 0.14468766461909383 and parameters: {'observation_period_num': 37, 'train_rates': 0.6385510832554273, 'learning_rate': 4.758157947761077e-06, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8839125443981187}. Best is trial 1 with value: 0.14468766461909383.[0m
[32m[I 2025-01-04 13:15:06,603][0m Trial 2 finished with value: 0.5091875390327478 and parameters: {'observation_period_num': 94, 'train_rates': 0.6755898125314748, 'learning_rate': 0.0008157303956175825, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8563221153049526}. Best is trial 1 with value: 0.14468766461909383.[0m
[32m[I 2025-01-04 13:21:25,530][0m Trial 3 finished with value: 0.2550877593457699 and parameters: {'observation_period_num': 217, 'train_rates': 0.8890757280690369, 'learning_rate': 2.067356676469825e-06, 'batch_size': 76, 'step_size': 5, 'gamma': 0.8760957024625087}. Best is trial 1 with value: 0.14468766461909383.[0m
[32m[I 2025-01-04 13:25:14,953][0m Trial 4 finished with value: 0.12626983244327983 and parameters: {'observation_period_num': 52, 'train_rates': 0.7293045430319389, 'learning_rate': 1.0730744398314918e-05, 'batch_size': 150, 'step_size': 7, 'gamma': 0.7868581977230047}. Best is trial 4 with value: 0.12626983244327983.[0m
[32m[I 2025-01-04 13:29:37,147][0m Trial 5 finished with value: 0.18670547880439936 and parameters: {'observation_period_num': 212, 'train_rates': 0.8822774434674017, 'learning_rate': 9.612441846788743e-06, 'batch_size': 122, 'step_size': 12, 'gamma': 0.7886997920271199}. Best is trial 4 with value: 0.12626983244327983.[0m
[32m[I 2025-01-04 13:39:07,103][0m Trial 6 finished with value: 0.1525602576546458 and parameters: {'observation_period_num': 165, 'train_rates': 0.8461744236519242, 'learning_rate': 0.00010270819920932303, 'batch_size': 50, 'step_size': 11, 'gamma': 0.8672752063101354}. Best is trial 4 with value: 0.12626983244327983.[0m
[32m[I 2025-01-04 13:42:28,843][0m Trial 7 finished with value: 0.1523394763287419 and parameters: {'observation_period_num': 192, 'train_rates': 0.6445726260380132, 'learning_rate': 0.0005668939936180215, 'batch_size': 142, 'step_size': 15, 'gamma': 0.7671982522427626}. Best is trial 4 with value: 0.12626983244327983.[0m
[32m[I 2025-01-04 13:51:31,471][0m Trial 8 finished with value: 0.1019023490101099 and parameters: {'observation_period_num': 61, 'train_rates': 0.8270539005695825, 'learning_rate': 2.770690966650253e-06, 'batch_size': 54, 'step_size': 6, 'gamma': 0.8031957608490065}. Best is trial 8 with value: 0.1019023490101099.[0m
[32m[I 2025-01-04 13:56:12,231][0m Trial 9 finished with value: 0.1885280190219342 and parameters: {'observation_period_num': 168, 'train_rates': 0.8726219359031882, 'learning_rate': 0.0003552374281397077, 'batch_size': 114, 'step_size': 15, 'gamma': 0.9488412962973392}. Best is trial 8 with value: 0.1019023490101099.[0m
Early stopping at epoch 92
[32m[I 2025-01-04 14:11:16,537][0m Trial 10 finished with value: 0.5745674170588124 and parameters: {'observation_period_num': 107, 'train_rates': 0.771866230316438, 'learning_rate': 1.0178474672983419e-06, 'batch_size': 28, 'step_size': 1, 'gamma': 0.8270965177367284}. Best is trial 8 with value: 0.1019023490101099.[0m
[32m[I 2025-01-04 14:14:49,432][0m Trial 11 finished with value: 0.05811193090391486 and parameters: {'observation_period_num': 10, 'train_rates': 0.7518338062268819, 'learning_rate': 1.6167922369671682e-05, 'batch_size': 215, 'step_size': 8, 'gamma': 0.8076855829981167}. Best is trial 11 with value: 0.05811193090391486.[0m
[32m[I 2025-01-04 14:18:21,113][0m Trial 12 finished with value: 0.051164663916033304 and parameters: {'observation_period_num': 26, 'train_rates': 0.7714777299777227, 'learning_rate': 2.976572074792022e-05, 'batch_size': 227, 'step_size': 9, 'gamma': 0.814549915053799}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:21:53,213][0m Trial 13 finished with value: 0.05299197113352645 and parameters: {'observation_period_num': 21, 'train_rates': 0.7353658122611845, 'learning_rate': 3.7059037269319025e-05, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8294206571608944}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:25:18,983][0m Trial 14 finished with value: 0.05716108811139222 and parameters: {'observation_period_num': 6, 'train_rates': 0.7105363838146904, 'learning_rate': 5.3439316162652686e-05, 'batch_size': 254, 'step_size': 10, 'gamma': 0.9169038376783701}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:28:53,182][0m Trial 15 finished with value: 0.10646882650589647 and parameters: {'observation_period_num': 84, 'train_rates': 0.8037596604327796, 'learning_rate': 0.00019810115783305233, 'batch_size': 194, 'step_size': 10, 'gamma': 0.8421680394461399}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:32:13,155][0m Trial 16 finished with value: 0.3856929285136287 and parameters: {'observation_period_num': 139, 'train_rates': 0.6936205918482387, 'learning_rate': 3.283243945112613e-05, 'batch_size': 243, 'step_size': 4, 'gamma': 0.8255374505196565}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:35:38,917][0m Trial 17 finished with value: 0.2691535719393893 and parameters: {'observation_period_num': 252, 'train_rates': 0.776670112115494, 'learning_rate': 1.7345325748733496e-05, 'batch_size': 182, 'step_size': 8, 'gamma': 0.7614611126664046}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:39:37,243][0m Trial 18 finished with value: 0.05552711337804794 and parameters: {'observation_period_num': 33, 'train_rates': 0.9525351721781625, 'learning_rate': 3.548843470465797e-05, 'batch_size': 223, 'step_size': 9, 'gamma': 0.8990527773825967}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:42:45,559][0m Trial 19 finished with value: 0.21481592134929334 and parameters: {'observation_period_num': 72, 'train_rates': 0.6184092602053156, 'learning_rate': 0.0001598548220675012, 'batch_size': 179, 'step_size': 12, 'gamma': 0.9879866032368172}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:46:04,338][0m Trial 20 finished with value: 0.30829809846416595 and parameters: {'observation_period_num': 122, 'train_rates': 0.7373606648608351, 'learning_rate': 7.047129376879265e-06, 'batch_size': 223, 'step_size': 3, 'gamma': 0.8353791386860203}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:50:02,124][0m Trial 21 finished with value: 0.07678505033254623 and parameters: {'observation_period_num': 32, 'train_rates': 0.9508861641238999, 'learning_rate': 3.884747807656518e-05, 'batch_size': 222, 'step_size': 9, 'gamma': 0.9022161741998642}. Best is trial 12 with value: 0.051164663916033304.[0m
[32m[I 2025-01-04 14:53:43,700][0m Trial 22 finished with value: 0.049299996868531044 and parameters: {'observation_period_num': 27, 'train_rates': 0.7977183841313646, 'learning_rate': 2.085829874604775e-05, 'batch_size': 209, 'step_size': 9, 'gamma': 0.9033059196293536}. Best is trial 22 with value: 0.049299996868531044.[0m
[32m[I 2025-01-04 14:57:20,137][0m Trial 23 finished with value: 0.04070473635348223 and parameters: {'observation_period_num': 20, 'train_rates': 0.7990263593682542, 'learning_rate': 2.1433575937092776e-05, 'batch_size': 198, 'step_size': 13, 'gamma': 0.9292364684309299}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:01:07,851][0m Trial 24 finished with value: 0.06407725019785374 and parameters: {'observation_period_num': 50, 'train_rates': 0.841494097620666, 'learning_rate': 1.8027325857223517e-05, 'batch_size': 191, 'step_size': 14, 'gamma': 0.9237559340750068}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:04:44,303][0m Trial 25 finished with value: 0.14442298489232216 and parameters: {'observation_period_num': 70, 'train_rates': 0.7957674673506916, 'learning_rate': 6.710783008134546e-05, 'batch_size': 203, 'step_size': 13, 'gamma': 0.9398570090945426}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:08:47,533][0m Trial 26 finished with value: 0.07886707616236174 and parameters: {'observation_period_num': 7, 'train_rates': 0.8208001756968283, 'learning_rate': 2.031319251956159e-05, 'batch_size': 170, 'step_size': 11, 'gamma': 0.9759855193814553}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:12:40,964][0m Trial 27 finished with value: 0.11684230715036392 and parameters: {'observation_period_num': 45, 'train_rates': 0.9187240374057244, 'learning_rate': 5.343197752845215e-06, 'batch_size': 254, 'step_size': 7, 'gamma': 0.9673110901207315}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:16:25,717][0m Trial 28 finished with value: 0.04624053438504537 and parameters: {'observation_period_num': 24, 'train_rates': 0.769627878569542, 'learning_rate': 0.00013378899376068778, 'batch_size': 165, 'step_size': 13, 'gamma': 0.9180978462334101}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:20:27,120][0m Trial 29 finished with value: 0.14138960882578747 and parameters: {'observation_period_num': 86, 'train_rates': 0.8592889211470389, 'learning_rate': 0.00010086398260714885, 'batch_size': 174, 'step_size': 14, 'gamma': 0.9344995520348837}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:24:41,486][0m Trial 30 finished with value: 0.08089710772037506 and parameters: {'observation_period_num': 63, 'train_rates': 0.9170063802402888, 'learning_rate': 0.0002306784122520923, 'batch_size': 160, 'step_size': 14, 'gamma': 0.9034476288920951}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:28:23,130][0m Trial 31 finished with value: 0.04796479539545767 and parameters: {'observation_period_num': 24, 'train_rates': 0.7774883344313321, 'learning_rate': 5.568472543367771e-05, 'batch_size': 204, 'step_size': 13, 'gamma': 0.9219533618770435}. Best is trial 23 with value: 0.04070473635348223.[0m
[32m[I 2025-01-04 15:32:02,108][0m Trial 32 finished with value: 0.03431245894836528 and parameters: {'observation_period_num': 23, 'train_rates': 0.8090220292351398, 'learning_rate': 0.00012068648759107733, 'batch_size': 204, 'step_size': 13, 'gamma': 0.8900497390985385}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 15:35:37,787][0m Trial 33 finished with value: 0.0860910289719572 and parameters: {'observation_period_num': 41, 'train_rates': 0.7616613028067981, 'learning_rate': 0.00014079386085126026, 'batch_size': 198, 'step_size': 13, 'gamma': 0.8882986490408138}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 15:39:46,548][0m Trial 34 finished with value: 0.03755214715199749 and parameters: {'observation_period_num': 16, 'train_rates': 0.8135315413943485, 'learning_rate': 6.451765637732072e-05, 'batch_size': 158, 'step_size': 13, 'gamma': 0.9519034735166461}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 15:44:02,714][0m Trial 35 finished with value: 0.05875667026982857 and parameters: {'observation_period_num': 18, 'train_rates': 0.8140846641952643, 'learning_rate': 0.0002672529211092339, 'batch_size': 133, 'step_size': 15, 'gamma': 0.9624534686756037}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 15:48:07,141][0m Trial 36 finished with value: 0.07608882213632266 and parameters: {'observation_period_num': 51, 'train_rates': 0.8367626797583512, 'learning_rate': 0.0003937999307848409, 'batch_size': 158, 'step_size': 12, 'gamma': 0.8621695999726436}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 15:52:49,082][0m Trial 37 finished with value: 0.044550753479197513 and parameters: {'observation_period_num': 7, 'train_rates': 0.6674650979365007, 'learning_rate': 0.00010304033798027977, 'batch_size': 95, 'step_size': 11, 'gamma': 0.9534509155522662}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 15:57:43,178][0m Trial 38 finished with value: 0.039976007226097746 and parameters: {'observation_period_num': 6, 'train_rates': 0.6559261715455458, 'learning_rate': 8.488685303987761e-05, 'batch_size': 94, 'step_size': 11, 'gamma': 0.9520780520878153}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:04:00,146][0m Trial 39 finished with value: 0.0895899477385983 and parameters: {'observation_period_num': 42, 'train_rates': 0.8999672804309092, 'learning_rate': 8.301691524277625e-05, 'batch_size': 93, 'step_size': 11, 'gamma': 0.9363924150085596}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:11:26,399][0m Trial 40 finished with value: 0.10931538790464401 and parameters: {'observation_period_num': 99, 'train_rates': 0.9867136337442621, 'learning_rate': 1.1251668447594664e-05, 'batch_size': 76, 'step_size': 14, 'gamma': 0.8846401376233284}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:15:37,708][0m Trial 41 finished with value: 1.7312369532541398 and parameters: {'observation_period_num': 5, 'train_rates': 0.6670518549507856, 'learning_rate': 0.0009442514133482645, 'batch_size': 114, 'step_size': 11, 'gamma': 0.9564394953344031}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:20:06,951][0m Trial 42 finished with value: 0.08743883527277671 and parameters: {'observation_period_num': 14, 'train_rates': 0.6038231113771311, 'learning_rate': 8.445383574787372e-05, 'batch_size': 96, 'step_size': 12, 'gamma': 0.9811129921964351}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:26:25,665][0m Trial 43 finished with value: 0.13427955188309423 and parameters: {'observation_period_num': 34, 'train_rates': 0.6783918431940774, 'learning_rate': 5.7842096007170814e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.9406309012829016}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:30:08,241][0m Trial 44 finished with value: 0.07640823886345421 and parameters: {'observation_period_num': 16, 'train_rates': 0.6568468547274833, 'learning_rate': 0.00011684622251549434, 'batch_size': 143, 'step_size': 12, 'gamma': 0.9520298150814112}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:34:00,420][0m Trial 45 finished with value: 0.18082946537121156 and parameters: {'observation_period_num': 59, 'train_rates': 0.7145438375983169, 'learning_rate': 0.00031572966521036585, 'batch_size': 130, 'step_size': 13, 'gamma': 0.975323490988201}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:38:01,022][0m Trial 46 finished with value: 0.06066816253199193 and parameters: {'observation_period_num': 5, 'train_rates': 0.6339534982748523, 'learning_rate': 4.4538321672064866e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.9462082344403471}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 16:45:28,663][0m Trial 47 finished with value: 2.0649440769858205 and parameters: {'observation_period_num': 38, 'train_rates': 0.8538490727177607, 'learning_rate': 0.0005914403359667625, 'batch_size': 68, 'step_size': 11, 'gamma': 0.9635909229300523}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 17:05:11,261][0m Trial 48 finished with value: 0.23103144289087063 and parameters: {'observation_period_num': 158, 'train_rates': 0.6978328667266334, 'learning_rate': 2.593731956526233e-05, 'batch_size': 21, 'step_size': 10, 'gamma': 0.9295368021772776}. Best is trial 32 with value: 0.03431245894836528.[0m
[32m[I 2025-01-04 17:16:24,962][0m Trial 49 finished with value: 0.09165355475164774 and parameters: {'observation_period_num': 19, 'train_rates': 0.8703266810034982, 'learning_rate': 0.00018365717562570015, 'batch_size': 45, 'step_size': 12, 'gamma': 0.8741824087741068}. Best is trial 32 with value: 0.03431245894836528.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 17:16:24,971][0m A new study created in memory with name: no-name-2e2509e6-1bd2-442d-9db4-1fefbad78a52[0m
[32m[I 2025-01-04 17:20:31,927][0m Trial 0 finished with value: 0.17140386226986135 and parameters: {'observation_period_num': 120, 'train_rates': 0.8615531264437394, 'learning_rate': 1.1151256324777564e-05, 'batch_size': 150, 'step_size': 14, 'gamma': 0.8652076677470282}. Best is trial 0 with value: 0.17140386226986135.[0m
[32m[I 2025-01-04 17:26:39,661][0m Trial 1 finished with value: 0.14076574002164643 and parameters: {'observation_period_num': 23, 'train_rates': 0.6095852051021952, 'learning_rate': 0.0001756008538239881, 'batch_size': 68, 'step_size': 1, 'gamma': 0.8991626708748369}. Best is trial 1 with value: 0.14076574002164643.[0m
[32m[I 2025-01-04 17:35:05,311][0m Trial 2 finished with value: 0.10435936951383888 and parameters: {'observation_period_num': 110, 'train_rates': 0.9132163342926759, 'learning_rate': 1.1757696111153379e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.8500818444271606}. Best is trial 2 with value: 0.10435936951383888.[0m
[32m[I 2025-01-04 17:38:20,607][0m Trial 3 finished with value: 0.19319796053538646 and parameters: {'observation_period_num': 204, 'train_rates': 0.7579363588317914, 'learning_rate': 0.0001486945237757422, 'batch_size': 242, 'step_size': 4, 'gamma': 0.773104213503178}. Best is trial 2 with value: 0.10435936951383888.[0m
[32m[I 2025-01-04 17:41:18,202][0m Trial 4 finished with value: 0.4219734460362186 and parameters: {'observation_period_num': 147, 'train_rates': 0.6125454671857301, 'learning_rate': 0.000309621709825428, 'batch_size': 201, 'step_size': 3, 'gamma': 0.8820864217998182}. Best is trial 2 with value: 0.10435936951383888.[0m
[32m[I 2025-01-04 17:44:32,078][0m Trial 5 finished with value: 0.4196023185062301 and parameters: {'observation_period_num': 95, 'train_rates': 0.6894712077211222, 'learning_rate': 1.6028750173619525e-06, 'batch_size': 231, 'step_size': 11, 'gamma': 0.9549138045001244}. Best is trial 2 with value: 0.10435936951383888.[0m
[32m[I 2025-01-04 17:47:46,540][0m Trial 6 finished with value: 0.20822276813440196 and parameters: {'observation_period_num': 47, 'train_rates': 0.6394054188618098, 'learning_rate': 1.6083472752558817e-06, 'batch_size': 225, 'step_size': 12, 'gamma': 0.9001775412852505}. Best is trial 2 with value: 0.10435936951383888.[0m
[32m[I 2025-01-04 17:55:01,486][0m Trial 7 finished with value: 0.13123837759777834 and parameters: {'observation_period_num': 127, 'train_rates': 0.7674826279409077, 'learning_rate': 6.561963256173817e-06, 'batch_size': 62, 'step_size': 6, 'gamma': 0.9203168582603136}. Best is trial 2 with value: 0.10435936951383888.[0m
[32m[I 2025-01-04 18:00:36,563][0m Trial 8 finished with value: 0.03595054894685745 and parameters: {'observation_period_num': 21, 'train_rates': 0.9770406456231986, 'learning_rate': 0.00045299479456401107, 'batch_size': 102, 'step_size': 8, 'gamma': 0.9506944230764285}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:04:04,576][0m Trial 9 finished with value: 0.35893389478630683 and parameters: {'observation_period_num': 249, 'train_rates': 0.6926545685640629, 'learning_rate': 0.0006059825558618216, 'batch_size': 136, 'step_size': 3, 'gamma': 0.8766824690411541}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:09:10,935][0m Trial 10 finished with value: 0.06480834633111954 and parameters: {'observation_period_num': 57, 'train_rates': 0.9879770904759907, 'learning_rate': 5.412941094074465e-05, 'batch_size': 111, 'step_size': 9, 'gamma': 0.9726993213745938}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:14:20,879][0m Trial 11 finished with value: 0.05962168797850609 and parameters: {'observation_period_num': 8, 'train_rates': 0.9715034815035767, 'learning_rate': 7.067358339518483e-05, 'batch_size': 117, 'step_size': 9, 'gamma': 0.987363404701641}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:20:17,950][0m Trial 12 finished with value: 2.2843103408813477 and parameters: {'observation_period_num': 8, 'train_rates': 0.97670397746921, 'learning_rate': 0.0008741716415136305, 'batch_size': 95, 'step_size': 8, 'gamma': 0.9849533928365581}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:36:34,278][0m Trial 13 finished with value: 0.12958636583080368 and parameters: {'observation_period_num': 71, 'train_rates': 0.890805979486929, 'learning_rate': 4.285394395418461e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.9500731197240654}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:40:48,287][0m Trial 14 finished with value: 0.03710799946172817 and parameters: {'observation_period_num': 9, 'train_rates': 0.9373406485880179, 'learning_rate': 8.45835694016852e-05, 'batch_size': 175, 'step_size': 10, 'gamma': 0.8246443628953442}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:44:25,594][0m Trial 15 finished with value: 0.17205795193415868 and parameters: {'observation_period_num': 171, 'train_rates': 0.8370924387292491, 'learning_rate': 0.0003232821796189439, 'batch_size': 179, 'step_size': 11, 'gamma': 0.8182786760649693}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:48:38,526][0m Trial 16 finished with value: 0.04753395206317669 and parameters: {'observation_period_num': 35, 'train_rates': 0.9299123764403048, 'learning_rate': 0.00013208835584758478, 'batch_size': 165, 'step_size': 6, 'gamma': 0.8121666695802834}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:52:17,802][0m Trial 17 finished with value: 0.10346167702550833 and parameters: {'observation_period_num': 86, 'train_rates': 0.8189732806039735, 'learning_rate': 2.399686505380357e-05, 'batch_size': 187, 'step_size': 10, 'gamma': 0.7606966629939282}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 18:56:50,508][0m Trial 18 finished with value: 0.07630698944994076 and parameters: {'observation_period_num': 60, 'train_rates': 0.9423290719972608, 'learning_rate': 0.0003994596732474328, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8331716246406763}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:03:00,809][0m Trial 19 finished with value: 2.144742131928313 and parameters: {'observation_period_num': 30, 'train_rates': 0.8829044686063025, 'learning_rate': 0.0009953950633496073, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8002647157202519}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:30:55,454][0m Trial 20 finished with value: 0.17140620646925717 and parameters: {'observation_period_num': 163, 'train_rates': 0.9448840047817199, 'learning_rate': 9.326880373748962e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9297194464938421}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:35:10,354][0m Trial 21 finished with value: 0.04200519589441163 and parameters: {'observation_period_num': 34, 'train_rates': 0.9282764162955882, 'learning_rate': 0.0001372377906440187, 'batch_size': 162, 'step_size': 7, 'gamma': 0.7949727109249344}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:39:05,034][0m Trial 22 finished with value: 0.051842910093320926 and parameters: {'observation_period_num': 6, 'train_rates': 0.9038333899378524, 'learning_rate': 2.3927327700481808e-05, 'batch_size': 208, 'step_size': 8, 'gamma': 0.7929352332971493}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:43:27,169][0m Trial 23 finished with value: 0.06994584202766418 and parameters: {'observation_period_num': 39, 'train_rates': 0.9524577161026002, 'learning_rate': 0.0002011061910045698, 'batch_size': 159, 'step_size': 9, 'gamma': 0.842056902139246}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:48:10,555][0m Trial 24 finished with value: 0.07519880921575071 and parameters: {'observation_period_num': 78, 'train_rates': 0.8658023699638216, 'learning_rate': 0.0004885174738991929, 'batch_size': 117, 'step_size': 7, 'gamma': 0.7894669599391494}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:52:28,657][0m Trial 25 finished with value: 0.04291762411594391 and parameters: {'observation_period_num': 25, 'train_rates': 0.9890429698491647, 'learning_rate': 0.00010521729448362331, 'batch_size': 187, 'step_size': 7, 'gamma': 0.8192078037465377}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 19:56:41,961][0m Trial 26 finished with value: 0.06933993641116204 and parameters: {'observation_period_num': 50, 'train_rates': 0.9248944363895557, 'learning_rate': 0.0002752423459794253, 'batch_size': 166, 'step_size': 10, 'gamma': 0.7641553017345583}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:00:10,396][0m Trial 27 finished with value: 0.08109494368378208 and parameters: {'observation_period_num': 97, 'train_rates': 0.8054648330505146, 'learning_rate': 6.403578603441611e-05, 'batch_size': 210, 'step_size': 5, 'gamma': 0.8542021562018437}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:04:39,615][0m Trial 28 finished with value: 0.05994448915326218 and parameters: {'observation_period_num': 67, 'train_rates': 0.8468557380726436, 'learning_rate': 3.1757618469252334e-05, 'batch_size': 128, 'step_size': 8, 'gamma': 0.7786326939153696}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:08:50,730][0m Trial 29 finished with value: 0.03883758327487777 and parameters: {'observation_period_num': 25, 'train_rates': 0.8717947167458162, 'learning_rate': 0.0002120007073687675, 'batch_size': 155, 'step_size': 13, 'gamma': 0.7520766622691866}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:14:33,895][0m Trial 30 finished with value: 0.1470700306660038 and parameters: {'observation_period_num': 18, 'train_rates': 0.87932769832532, 'learning_rate': 0.0006641928203831643, 'batch_size': 90, 'step_size': 14, 'gamma': 0.8680824823626191}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:18:48,922][0m Trial 31 finished with value: 0.051977126575392205 and parameters: {'observation_period_num': 41, 'train_rates': 0.904415656543933, 'learning_rate': 0.0002650871622224729, 'batch_size': 149, 'step_size': 13, 'gamma': 0.7803025746224803}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:23:18,620][0m Trial 32 finished with value: 0.041108958423137665 and parameters: {'observation_period_num': 22, 'train_rates': 0.9604270692550394, 'learning_rate': 0.00016840087136084969, 'batch_size': 153, 'step_size': 11, 'gamma': 0.7545937256115117}. Best is trial 8 with value: 0.03595054894685745.[0m
[32m[I 2025-01-04 20:27:57,493][0m Trial 33 finished with value: 0.034782569855451584 and parameters: {'observation_period_num': 19, 'train_rates': 0.9632795713627718, 'learning_rate': 0.00020833606354539545, 'batch_size': 147, 'step_size': 13, 'gamma': 0.7546629377872369}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 20:33:15,649][0m Trial 34 finished with value: 0.05346813534989076 and parameters: {'observation_period_num': 5, 'train_rates': 0.9597822251855074, 'learning_rate': 0.00020298363104340273, 'batch_size': 105, 'step_size': 15, 'gamma': 0.7504342280258383}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 20:40:09,218][0m Trial 35 finished with value: 0.16894095616137728 and parameters: {'observation_period_num': 111, 'train_rates': 0.9175762407824187, 'learning_rate': 9.385259226647863e-05, 'batch_size': 76, 'step_size': 13, 'gamma': 0.9009351608121986}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 20:44:10,183][0m Trial 36 finished with value: 2.089404410340008 and parameters: {'observation_period_num': 20, 'train_rates': 0.8683864191828926, 'learning_rate': 0.0004538129039999138, 'batch_size': 177, 'step_size': 13, 'gamma': 0.8296702605425724}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 20:48:19,925][0m Trial 37 finished with value: 0.07448080311948006 and parameters: {'observation_period_num': 50, 'train_rates': 0.7747093743314585, 'learning_rate': 1.1186905638381299e-05, 'batch_size': 132, 'step_size': 12, 'gamma': 0.7711990385336366}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 20:52:05,172][0m Trial 38 finished with value: 0.256882007420063 and parameters: {'observation_period_num': 219, 'train_rates': 0.9014464323016258, 'learning_rate': 3.2890167535166294e-06, 'batch_size': 201, 'step_size': 10, 'gamma': 0.9187248471163262}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 20:56:23,829][0m Trial 39 finished with value: 0.06729307631959937 and parameters: {'observation_period_num': 20, 'train_rates': 0.8463259693750393, 'learning_rate': 0.0002755353473333247, 'batch_size': 142, 'step_size': 14, 'gamma': 0.8856066165217849}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:00:58,218][0m Trial 40 finished with value: 0.18183554557269205 and parameters: {'observation_period_num': 145, 'train_rates': 0.9406866935559902, 'learning_rate': 0.000641601056495997, 'batch_size': 126, 'step_size': 12, 'gamma': 0.807208167133537}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:05:25,938][0m Trial 41 finished with value: 0.03542088717222214 and parameters: {'observation_period_num': 18, 'train_rates': 0.9621450277878264, 'learning_rate': 0.00017498204509076555, 'batch_size': 151, 'step_size': 11, 'gamma': 0.7509480741824189}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:09:48,992][0m Trial 42 finished with value: 0.03985244035720825 and parameters: {'observation_period_num': 41, 'train_rates': 0.9758007930636791, 'learning_rate': 0.00019617929645212028, 'batch_size': 176, 'step_size': 11, 'gamma': 0.7679500365772298}. Best is trial 33 with value: 0.034782569855451584.[0m
Early stopping at epoch 56
[32m[I 2025-01-04 21:12:22,191][0m Trial 43 finished with value: 0.11855330318212509 and parameters: {'observation_period_num': 15, 'train_rates': 0.9574095198033813, 'learning_rate': 0.00037097385710818225, 'batch_size': 148, 'step_size': 1, 'gamma': 0.780907638846975}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:21:24,852][0m Trial 44 finished with value: 0.19805149286799828 and parameters: {'observation_period_num': 57, 'train_rates': 0.7227011288181645, 'learning_rate': 0.00010892682776068762, 'batch_size': 49, 'step_size': 13, 'gamma': 0.7501866662548583}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:25:36,321][0m Trial 45 finished with value: 0.06942896544933319 and parameters: {'observation_period_num': 28, 'train_rates': 0.9891014409991459, 'learning_rate': 4.802329891944513e-05, 'batch_size': 193, 'step_size': 10, 'gamma': 0.9519675211614196}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:29:41,478][0m Trial 46 finished with value: 0.06272584944963455 and parameters: {'observation_period_num': 47, 'train_rates': 0.9675766874647312, 'learning_rate': 0.00022833042000900008, 'batch_size': 247, 'step_size': 9, 'gamma': 0.7642200754864291}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:35:05,935][0m Trial 47 finished with value: 0.03741632650295893 and parameters: {'observation_period_num': 12, 'train_rates': 0.9357037691875818, 'learning_rate': 0.0001462091977308265, 'batch_size': 99, 'step_size': 11, 'gamma': 0.8579447354130514}. Best is trial 33 with value: 0.034782569855451584.[0m
[32m[I 2025-01-04 21:40:24,136][0m Trial 48 finished with value: 0.033586397061222475 and parameters: {'observation_period_num': 14, 'train_rates': 0.9354504448830356, 'learning_rate': 7.270224616218108e-05, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8558525468357027}. Best is trial 48 with value: 0.033586397061222475.[0m
[32m[I 2025-01-04 21:44:13,249][0m Trial 49 finished with value: 0.36561072857382243 and parameters: {'observation_period_num': 68, 'train_rates': 0.645171516427476, 'learning_rate': 3.6916925631768964e-05, 'batch_size': 119, 'step_size': 11, 'gamma': 0.892509297811897}. Best is trial 48 with value: 0.033586397061222475.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 21:44:13,258][0m A new study created in memory with name: no-name-1d826625-c070-4634-95ea-02f1ca933cbf[0m
[32m[I 2025-01-04 21:47:35,168][0m Trial 0 finished with value: 0.6061677321470285 and parameters: {'observation_period_num': 248, 'train_rates': 0.6740742706511704, 'learning_rate': 8.911517019196054e-06, 'batch_size': 141, 'step_size': 6, 'gamma': 0.8722648431858057}. Best is trial 0 with value: 0.6061677321470285.[0m
[32m[I 2025-01-04 21:55:28,317][0m Trial 1 finished with value: 0.3351552247169958 and parameters: {'observation_period_num': 94, 'train_rates': 0.648872463278847, 'learning_rate': 1.6739339498373444e-06, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8884952678087241}. Best is trial 1 with value: 0.3351552247169958.[0m
[32m[I 2025-01-04 21:58:39,374][0m Trial 2 finished with value: 1.7804962518482894 and parameters: {'observation_period_num': 48, 'train_rates': 0.6750487715131077, 'learning_rate': 0.0008253488727797772, 'batch_size': 201, 'step_size': 9, 'gamma': 0.766867286214585}. Best is trial 1 with value: 0.3351552247169958.[0m
[32m[I 2025-01-04 22:01:56,821][0m Trial 3 finished with value: 0.5831741121682253 and parameters: {'observation_period_num': 98, 'train_rates': 0.7147017550459711, 'learning_rate': 1.5277479462849108e-06, 'batch_size': 222, 'step_size': 3, 'gamma': 0.8566008057976359}. Best is trial 1 with value: 0.3351552247169958.[0m
[32m[I 2025-01-04 22:11:34,075][0m Trial 4 finished with value: 0.04813094064593315 and parameters: {'observation_period_num': 41, 'train_rates': 0.9826760580314071, 'learning_rate': 3.6735991333198574e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.9484496157089273}. Best is trial 4 with value: 0.04813094064593315.[0m
[32m[I 2025-01-04 22:17:44,614][0m Trial 5 finished with value: 0.07354888959384676 and parameters: {'observation_period_num': 42, 'train_rates': 0.8620648766736747, 'learning_rate': 0.00011839646351682115, 'batch_size': 82, 'step_size': 6, 'gamma': 0.9139288822667737}. Best is trial 4 with value: 0.04813094064593315.[0m
[32m[I 2025-01-04 22:25:21,950][0m Trial 6 finished with value: 0.09792300174511957 and parameters: {'observation_period_num': 89, 'train_rates': 0.771959426196255, 'learning_rate': 0.0002622417972305932, 'batch_size': 60, 'step_size': 7, 'gamma': 0.9295574458465282}. Best is trial 4 with value: 0.04813094064593315.[0m
[32m[I 2025-01-04 22:30:25,768][0m Trial 7 finished with value: 0.06455836445093155 and parameters: {'observation_period_num': 40, 'train_rates': 0.9696527646669498, 'learning_rate': 6.752753015467818e-06, 'batch_size': 110, 'step_size': 12, 'gamma': 0.9478685936047266}. Best is trial 4 with value: 0.04813094064593315.[0m
[32m[I 2025-01-04 22:40:34,965][0m Trial 8 finished with value: 0.23979826482752578 and parameters: {'observation_period_num': 166, 'train_rates': 0.6999013867593071, 'learning_rate': 0.00022828157702886953, 'batch_size': 41, 'step_size': 15, 'gamma': 0.9657194792520511}. Best is trial 4 with value: 0.04813094064593315.[0m
[32m[I 2025-01-04 22:45:25,029][0m Trial 9 finished with value: 0.2599019666662999 and parameters: {'observation_period_num': 224, 'train_rates': 0.8304312791202582, 'learning_rate': 0.0001841548331490415, 'batch_size': 95, 'step_size': 7, 'gamma': 0.9376040389215217}. Best is trial 4 with value: 0.04813094064593315.[0m
[32m[I 2025-01-04 23:19:05,601][0m Trial 10 finished with value: 0.030224697132195746 and parameters: {'observation_period_num': 11, 'train_rates': 0.9857882190347789, 'learning_rate': 3.6084960669910486e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7909888658624766}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-04 23:52:30,078][0m Trial 11 finished with value: 0.030245201662182808 and parameters: {'observation_period_num': 7, 'train_rates': 0.9739063633669026, 'learning_rate': 2.7047261914606645e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8024887890697489}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 00:12:24,587][0m Trial 12 finished with value: 0.030923055182468847 and parameters: {'observation_period_num': 8, 'train_rates': 0.9081459518505147, 'learning_rate': 2.7224678723212194e-05, 'batch_size': 26, 'step_size': 15, 'gamma': 0.7852217733780524}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 00:41:05,338][0m Trial 13 finished with value: 0.03027786028438381 and parameters: {'observation_period_num': 5, 'train_rates': 0.9146551502376995, 'learning_rate': 3.591868134478759e-05, 'batch_size': 18, 'step_size': 11, 'gamma': 0.8107486850461626}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 00:45:25,643][0m Trial 14 finished with value: 0.14635997987472948 and parameters: {'observation_period_num': 137, 'train_rates': 0.9314215514299458, 'learning_rate': 1.1411700340376456e-05, 'batch_size': 154, 'step_size': 10, 'gamma': 0.8243501601913263}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 00:49:11,881][0m Trial 15 finished with value: 0.15364395987133764 and parameters: {'observation_period_num': 182, 'train_rates': 0.8639656530790404, 'learning_rate': 6.887698421083251e-05, 'batch_size': 173, 'step_size': 13, 'gamma': 0.7510044452860964}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 01:12:59,686][0m Trial 16 finished with value: 0.08378665291649454 and parameters: {'observation_period_num': 77, 'train_rates': 0.7754082462681954, 'learning_rate': 3.5304933250947412e-06, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8253665644246391}. Best is trial 10 with value: 0.030224697132195746.[0m
Early stopping at epoch 58
[32m[I 2025-01-05 01:16:02,292][0m Trial 17 finished with value: 0.09229096056933099 and parameters: {'observation_period_num': 6, 'train_rates': 0.9522180810142241, 'learning_rate': 1.249206970670752e-05, 'batch_size': 110, 'step_size': 1, 'gamma': 0.7954700948199765}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 01:19:47,641][0m Trial 18 finished with value: 0.07343046665647343 and parameters: {'observation_period_num': 66, 'train_rates': 0.8869862518013609, 'learning_rate': 2.299034975245361e-05, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8470599560649755}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 01:27:04,323][0m Trial 19 finished with value: 0.1344921737909317 and parameters: {'observation_period_num': 131, 'train_rates': 0.9893180262387943, 'learning_rate': 6.761892493170355e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.7823206302441312}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 01:40:42,820][0m Trial 20 finished with value: 0.041567722846130285 and parameters: {'observation_period_num': 25, 'train_rates': 0.805692422669755, 'learning_rate': 0.0006527726883933676, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8381659953144713}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 02:04:39,489][0m Trial 21 finished with value: 0.03801383473910391 and parameters: {'observation_period_num': 7, 'train_rates': 0.9405104040446097, 'learning_rate': 4.2491174116650694e-05, 'batch_size': 22, 'step_size': 10, 'gamma': 0.8090007731536909}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 02:30:05,265][0m Trial 22 finished with value: 0.09950761951090441 and parameters: {'observation_period_num': 62, 'train_rates': 0.9193883942833979, 'learning_rate': 1.8810704071221085e-05, 'batch_size': 20, 'step_size': 11, 'gamma': 0.8094031676212492}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 02:35:46,823][0m Trial 23 finished with value: 0.12435966203593063 and parameters: {'observation_period_num': 23, 'train_rates': 0.602015356211828, 'learning_rate': 7.845015935749618e-05, 'batch_size': 73, 'step_size': 14, 'gamma': 0.7657617138213614}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 02:47:14,740][0m Trial 24 finished with value: 0.04199796104803681 and parameters: {'observation_period_num': 27, 'train_rates': 0.8974086640511387, 'learning_rate': 3.7009337719979024e-06, 'batch_size': 45, 'step_size': 12, 'gamma': 0.8074105549172387}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 02:52:21,511][0m Trial 25 finished with value: 0.1511215047808144 and parameters: {'observation_period_num': 111, 'train_rates': 0.9555525871540984, 'learning_rate': 4.185754179543359e-05, 'batch_size': 112, 'step_size': 14, 'gamma': 0.8882261713675498}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 03:22:36,844][0m Trial 26 finished with value: 0.10092903952319901 and parameters: {'observation_period_num': 59, 'train_rates': 0.8669815383476575, 'learning_rate': 1.6599834094345148e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8279781299667004}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 03:36:22,345][0m Trial 27 finished with value: 0.03788498133420944 and parameters: {'observation_period_num': 18, 'train_rates': 0.9574037898000681, 'learning_rate': 5.531478877087953e-06, 'batch_size': 39, 'step_size': 12, 'gamma': 0.7750357904571971}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 03:43:59,622][0m Trial 28 finished with value: 0.14634633420282983 and parameters: {'observation_period_num': 158, 'train_rates': 0.9274769452241926, 'learning_rate': 0.00012897658847504692, 'batch_size': 68, 'step_size': 4, 'gamma': 0.794731221693152}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 03:48:19,843][0m Trial 29 finished with value: 0.17462879419326782 and parameters: {'observation_period_num': 238, 'train_rates': 0.9826496531461952, 'learning_rate': 4.662178348634873e-05, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8645058975605014}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 03:53:26,693][0m Trial 30 finished with value: 0.18568619045333362 and parameters: {'observation_period_num': 201, 'train_rates': 0.8194608981100879, 'learning_rate': 8.118705672004766e-06, 'batch_size': 93, 'step_size': 11, 'gamma': 0.7520808319601097}. Best is trial 10 with value: 0.030224697132195746.[0m
[32m[I 2025-01-05 04:09:13,753][0m Trial 31 finished with value: 0.028147861553147597 and parameters: {'observation_period_num': 8, 'train_rates': 0.9061011009151809, 'learning_rate': 2.6959537630967378e-05, 'batch_size': 33, 'step_size': 15, 'gamma': 0.7892395780754781}. Best is trial 31 with value: 0.028147861553147597.[0m
[32m[I 2025-01-05 04:23:20,579][0m Trial 32 finished with value: 0.07011747029092577 and parameters: {'observation_period_num': 31, 'train_rates': 0.8892845306371289, 'learning_rate': 1.5690792342018233e-05, 'batch_size': 36, 'step_size': 14, 'gamma': 0.7998359343275068}. Best is trial 31 with value: 0.028147861553147597.[0m
[32m[I 2025-01-05 04:33:18,776][0m Trial 33 finished with value: 0.022010530867091902 and parameters: {'observation_period_num': 5, 'train_rates': 0.8457425581149951, 'learning_rate': 2.6705678124906986e-05, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8781192026251187}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 04:42:27,638][0m Trial 34 finished with value: 0.08272225974680303 and parameters: {'observation_period_num': 54, 'train_rates': 0.8431279051299425, 'learning_rate': 2.695646789920659e-05, 'batch_size': 54, 'step_size': 15, 'gamma': 0.8826854340480718}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 04:51:29,364][0m Trial 35 finished with value: 0.04773236634327641 and parameters: {'observation_period_num': 33, 'train_rates': 0.7834864809805662, 'learning_rate': 5.6421589297495405e-05, 'batch_size': 52, 'step_size': 13, 'gamma': 0.9027442505119403}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 05:03:57,777][0m Trial 36 finished with value: 0.05299942289934026 and parameters: {'observation_period_num': 18, 'train_rates': 0.7481627613233548, 'learning_rate': 0.00010199299458026929, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8724195371571576}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 05:13:01,853][0m Trial 37 finished with value: 0.09759965777089916 and parameters: {'observation_period_num': 45, 'train_rates': 0.9667230788723893, 'learning_rate': 1.0848814648415201e-06, 'batch_size': 61, 'step_size': 13, 'gamma': 0.9868672576191652}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 05:17:03,595][0m Trial 38 finished with value: 0.11170609295368195 and parameters: {'observation_period_num': 80, 'train_rates': 0.9398045418860813, 'learning_rate': 1.0490539988231294e-05, 'batch_size': 184, 'step_size': 15, 'gamma': 0.8498495217002976}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 05:22:53,358][0m Trial 39 finished with value: 0.08307806547739609 and parameters: {'observation_period_num': 112, 'train_rates': 0.8449353361584719, 'learning_rate': 2.099237838842398e-05, 'batch_size': 85, 'step_size': 14, 'gamma': 0.7656176018716118}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 05:38:06,372][0m Trial 40 finished with value: 0.05145607178865326 and parameters: {'observation_period_num': 43, 'train_rates': 0.8802233801589703, 'learning_rate': 4.741120816447014e-06, 'batch_size': 33, 'step_size': 5, 'gamma': 0.876291447093856}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 05:49:19,398][0m Trial 41 finished with value: 0.02662393793357703 and parameters: {'observation_period_num': 7, 'train_rates': 0.9166260630024027, 'learning_rate': 3.3621312213897804e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8206469550640364}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 06:00:09,545][0m Trial 42 finished with value: 0.04081165546720678 and parameters: {'observation_period_num': 16, 'train_rates': 0.9066418761888897, 'learning_rate': 2.8643811258939424e-05, 'batch_size': 48, 'step_size': 12, 'gamma': 0.9101739320835622}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 06:05:03,103][0m Trial 43 finished with value: 0.042318131774663925 and parameters: {'observation_period_num': 32, 'train_rates': 0.9748850000514263, 'learning_rate': 3.4060538160820685e-05, 'batch_size': 125, 'step_size': 13, 'gamma': 0.7833629749540556}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 06:23:37,774][0m Trial 44 finished with value: 0.05521617657194535 and parameters: {'observation_period_num': 15, 'train_rates': 0.9491891957378192, 'learning_rate': 1.5656969805946236e-05, 'batch_size': 29, 'step_size': 15, 'gamma': 0.834004277746778}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 06:32:00,888][0m Trial 45 finished with value: 0.04279020471521751 and parameters: {'observation_period_num': 35, 'train_rates': 0.9183503667492663, 'learning_rate': 8.780928446492781e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.7953981548019184}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 06:43:23,370][0m Trial 46 finished with value: 0.06504607945680618 and parameters: {'observation_period_num': 53, 'train_rates': 0.9864658056434653, 'learning_rate': 5.559731874879883e-05, 'batch_size': 48, 'step_size': 14, 'gamma': 0.8604911985189846}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 07:02:36,341][0m Trial 47 finished with value: 0.06435982012475024 and parameters: {'observation_period_num': 13, 'train_rates': 0.8667679960314462, 'learning_rate': 0.0001266774730428536, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8176819172065329}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 07:08:38,300][0m Trial 48 finished with value: 0.07121294908542228 and parameters: {'observation_period_num': 73, 'train_rates': 0.8448521269776843, 'learning_rate': 2.7804021728319238e-05, 'batch_size': 82, 'step_size': 12, 'gamma': 0.8440438951041445}. Best is trial 33 with value: 0.022010530867091902.[0m
[32m[I 2025-01-05 07:18:39,760][0m Trial 49 finished with value: 0.03739251224575816 and parameters: {'observation_period_num': 7, 'train_rates': 0.9372064633855408, 'learning_rate': 8.237181418449793e-06, 'batch_size': 54, 'step_size': 13, 'gamma': 0.7766372013346917}. Best is trial 33 with value: 0.022010530867091902.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 07:18:39,769][0m A new study created in memory with name: no-name-0bf20909-d090-49e6-8928-961c61b9ed48[0m
[32m[I 2025-01-05 07:24:42,375][0m Trial 0 finished with value: 0.2122978150844574 and parameters: {'observation_period_num': 112, 'train_rates': 0.9315671942900987, 'learning_rate': 3.5420181246300886e-06, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8459836858005348}. Best is trial 0 with value: 0.2122978150844574.[0m
[32m[I 2025-01-05 07:28:42,399][0m Trial 1 finished with value: 0.11909143536568186 and parameters: {'observation_period_num': 75, 'train_rates': 0.6668957373302243, 'learning_rate': 0.0002905222909579636, 'batch_size': 111, 'step_size': 13, 'gamma': 0.7720838073624431}. Best is trial 1 with value: 0.11909143536568186.[0m
[32m[I 2025-01-05 07:33:20,698][0m Trial 2 finished with value: 0.0730821223357531 and parameters: {'observation_period_num': 24, 'train_rates': 0.6528958999298666, 'learning_rate': 1.1985137380682005e-05, 'batch_size': 93, 'step_size': 15, 'gamma': 0.8984698395860496}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 07:38:19,255][0m Trial 3 finished with value: 0.6190547302876279 and parameters: {'observation_period_num': 149, 'train_rates': 0.6150449902180338, 'learning_rate': 1.0807819039845335e-06, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8752362391383228}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 07:47:59,010][0m Trial 4 finished with value: 1.912998717675677 and parameters: {'observation_period_num': 188, 'train_rates': 0.7459028854542458, 'learning_rate': 0.0007475145652747833, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9791104083366757}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 07:51:57,008][0m Trial 5 finished with value: 0.08735645562410355 and parameters: {'observation_period_num': 58, 'train_rates': 0.9293826307962032, 'learning_rate': 3.371223928235493e-05, 'batch_size': 222, 'step_size': 12, 'gamma': 0.8524592127352073}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 07:55:37,877][0m Trial 6 finished with value: 0.15826435897756239 and parameters: {'observation_period_num': 150, 'train_rates': 0.8343828805135032, 'learning_rate': 9.79593942351035e-05, 'batch_size': 225, 'step_size': 15, 'gamma': 0.8311835267101996}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 07:58:42,212][0m Trial 7 finished with value: 0.8953962210079898 and parameters: {'observation_period_num': 195, 'train_rates': 0.6665132716335034, 'learning_rate': 1.827931893148501e-06, 'batch_size': 193, 'step_size': 10, 'gamma': 0.7584717938436437}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 08:02:37,362][0m Trial 8 finished with value: 0.19610381126403809 and parameters: {'observation_period_num': 240, 'train_rates': 0.9802200870657206, 'learning_rate': 1.1141567579509791e-05, 'batch_size': 221, 'step_size': 12, 'gamma': 0.9826329649238283}. Best is trial 2 with value: 0.0730821223357531.[0m
Early stopping at epoch 62
[32m[I 2025-01-05 08:04:53,339][0m Trial 9 finished with value: 0.4282312411814928 and parameters: {'observation_period_num': 15, 'train_rates': 0.7495693556663067, 'learning_rate': 2.8962446896154335e-06, 'batch_size': 197, 'step_size': 1, 'gamma': 0.818330012033875}. Best is trial 2 with value: 0.0730821223357531.[0m
[32m[I 2025-01-05 08:33:06,985][0m Trial 10 finished with value: 0.029563067761740384 and parameters: {'observation_period_num': 10, 'train_rates': 0.822008167983812, 'learning_rate': 1.16866276873551e-05, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9097293508954614}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 08:58:32,769][0m Trial 11 finished with value: 0.03594658046743823 and parameters: {'observation_period_num': 12, 'train_rates': 0.8248344604541664, 'learning_rate': 1.6548921070553258e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9135891740029382}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 09:24:17,912][0m Trial 12 finished with value: 0.030659065137363294 and parameters: {'observation_period_num': 5, 'train_rates': 0.8359975375187528, 'learning_rate': 2.0942353683966402e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.9203322646244677}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 09:54:19,789][0m Trial 13 finished with value: 0.11061216236063928 and parameters: {'observation_period_num': 67, 'train_rates': 0.8629448078695986, 'learning_rate': 4.946301396378103e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9369753321611688}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:04:05,147][0m Trial 14 finished with value: 0.06038516845840674 and parameters: {'observation_period_num': 43, 'train_rates': 0.785625619125356, 'learning_rate': 7.647718789090045e-06, 'batch_size': 48, 'step_size': 9, 'gamma': 0.9405499363096366}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:07:53,283][0m Trial 15 finished with value: 0.16757865600368899 and parameters: {'observation_period_num': 95, 'train_rates': 0.7424145451217268, 'learning_rate': 8.55682838159991e-05, 'batch_size': 147, 'step_size': 5, 'gamma': 0.9389522040722691}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:17:52,842][0m Trial 16 finished with value: 0.03535222606294868 and parameters: {'observation_period_num': 7, 'train_rates': 0.8773870518269968, 'learning_rate': 5.287645258170226e-06, 'batch_size': 51, 'step_size': 6, 'gamma': 0.8907720620868858}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:21:59,983][0m Trial 17 finished with value: 0.05066304175002921 and parameters: {'observation_period_num': 39, 'train_rates': 0.7872716069812631, 'learning_rate': 2.198609211378553e-05, 'batch_size': 129, 'step_size': 9, 'gamma': 0.9162084228024088}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:26:01,768][0m Trial 18 finished with value: 0.09490084996214136 and parameters: {'observation_period_num': 76, 'train_rates': 0.8889065567984329, 'learning_rate': 0.0001606859288177698, 'batch_size': 161, 'step_size': 2, 'gamma': 0.9601293559693018}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:32:49,347][0m Trial 19 finished with value: 0.1670917982267121 and parameters: {'observation_period_num': 103, 'train_rates': 0.7101280743452905, 'learning_rate': 3.984910174483119e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.8030272042093892}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:36:29,300][0m Trial 20 finished with value: 0.09786676391012655 and parameters: {'observation_period_num': 46, 'train_rates': 0.8105965460490983, 'learning_rate': 6.013179988505106e-06, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8884809070922686}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 10:49:07,631][0m Trial 21 finished with value: 0.03414263192540671 and parameters: {'observation_period_num': 6, 'train_rates': 0.8930345967360107, 'learning_rate': 6.182626728917032e-06, 'batch_size': 41, 'step_size': 6, 'gamma': 0.8718780521497012}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 11:05:04,724][0m Trial 22 finished with value: 0.055549596631672325 and parameters: {'observation_period_num': 28, 'train_rates': 0.9299983239103645, 'learning_rate': 1.9682200489413597e-05, 'batch_size': 33, 'step_size': 6, 'gamma': 0.8628639282734213}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 11:12:32,822][0m Trial 23 finished with value: 0.037279133941630294 and parameters: {'observation_period_num': 7, 'train_rates': 0.8509166515236343, 'learning_rate': 7.981473544101027e-06, 'batch_size': 68, 'step_size': 3, 'gamma': 0.9139190150285227}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 11:28:33,963][0m Trial 24 finished with value: 0.04689722205587575 and parameters: {'observation_period_num': 34, 'train_rates': 0.8942124909147467, 'learning_rate': 3.0817623225310263e-06, 'batch_size': 32, 'step_size': 7, 'gamma': 0.8710925622700618}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 11:56:24,535][0m Trial 25 finished with value: 0.09147938517565579 and parameters: {'observation_period_num': 51, 'train_rates': 0.966552087689103, 'learning_rate': 6.577562515259316e-05, 'batch_size': 19, 'step_size': 8, 'gamma': 0.9585110836396674}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:04:51,407][0m Trial 26 finished with value: 0.10046355026767 and parameters: {'observation_period_num': 83, 'train_rates': 0.9087876186059948, 'learning_rate': 2.5627342357249613e-05, 'batch_size': 61, 'step_size': 5, 'gamma': 0.9047581436264879}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:09:46,128][0m Trial 27 finished with value: 0.03491106871599039 and parameters: {'observation_period_num': 5, 'train_rates': 0.8487857425862224, 'learning_rate': 1.2292012063072902e-05, 'batch_size': 107, 'step_size': 10, 'gamma': 0.9285401552215216}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:22:38,853][0m Trial 28 finished with value: 0.0550602272980743 and parameters: {'observation_period_num': 23, 'train_rates': 0.7788279556251626, 'learning_rate': 1.9430471135543216e-06, 'batch_size': 36, 'step_size': 6, 'gamma': 0.8848177408709517}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:28:29,333][0m Trial 29 finished with value: 0.18957789109373002 and parameters: {'observation_period_num': 115, 'train_rates': 0.8141898727522091, 'learning_rate': 5.1342325831884285e-06, 'batch_size': 82, 'step_size': 4, 'gamma': 0.8412289559038615}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:36:22,744][0m Trial 30 finished with value: 0.12782649853033945 and parameters: {'observation_period_num': 58, 'train_rates': 0.9559392403800339, 'learning_rate': 4.504184634853821e-06, 'batch_size': 69, 'step_size': 9, 'gamma': 0.955514265895945}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:41:16,551][0m Trial 31 finished with value: 0.0518926529803553 and parameters: {'observation_period_num': 28, 'train_rates': 0.8499127990868661, 'learning_rate': 1.1852092330570157e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9230013760854565}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:45:59,785][0m Trial 32 finished with value: 0.03717541372822841 and parameters: {'observation_period_num': 8, 'train_rates': 0.870526350085799, 'learning_rate': 1.4084650084248317e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.9296002415687001}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 12:51:17,775][0m Trial 33 finished with value: 0.048902129105637585 and parameters: {'observation_period_num': 27, 'train_rates': 0.9088478285441903, 'learning_rate': 8.261473342289787e-06, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8994529805056746}. Best is trial 10 with value: 0.029563067761740384.[0m
[32m[I 2025-01-05 13:06:12,169][0m Trial 34 finished with value: 0.02687414020292019 and parameters: {'observation_period_num': 5, 'train_rates': 0.8399774755763061, 'learning_rate': 2.7236333093780394e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.8619796514024894}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 13:20:53,661][0m Trial 35 finished with value: 0.13755309932960166 and parameters: {'observation_period_num': 135, 'train_rates': 0.8018337843149356, 'learning_rate': 2.9923767345018125e-05, 'batch_size': 31, 'step_size': 5, 'gamma': 0.8573815291546}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 13:30:36,102][0m Trial 36 finished with value: 0.04625123507768205 and parameters: {'observation_period_num': 19, 'train_rates': 0.754322301026684, 'learning_rate': 5.166683959160664e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.80036914983184}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 13:36:38,580][0m Trial 37 finished with value: 0.0679486649731795 and parameters: {'observation_period_num': 66, 'train_rates': 0.8336402574217001, 'learning_rate': 0.000522280434812537, 'batch_size': 82, 'step_size': 3, 'gamma': 0.8745772391717546}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 13:47:16,048][0m Trial 38 finished with value: 0.19035996073414843 and parameters: {'observation_period_num': 169, 'train_rates': 0.723031567058161, 'learning_rate': 0.0001624986142694112, 'batch_size': 40, 'step_size': 6, 'gamma': 0.8483505002585039}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 14:06:24,700][0m Trial 39 finished with value: 0.07158008580450463 and parameters: {'observation_period_num': 39, 'train_rates': 0.7671784714096425, 'learning_rate': 2.1807655747886116e-05, 'batch_size': 24, 'step_size': 7, 'gamma': 0.8307444105110928}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 14:12:54,053][0m Trial 40 finished with value: 0.5684212082219355 and parameters: {'observation_period_num': 245, 'train_rates': 0.6196700997102371, 'learning_rate': 1.7108322470935262e-06, 'batch_size': 58, 'step_size': 5, 'gamma': 0.8828205731945004}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 14:24:46,262][0m Trial 41 finished with value: 0.0317325932555832 and parameters: {'observation_period_num': 5, 'train_rates': 0.8481430027904119, 'learning_rate': 1.1231133997461535e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9049359071145745}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 14:36:26,208][0m Trial 42 finished with value: 0.04197027788451705 and parameters: {'observation_period_num': 19, 'train_rates': 0.833753185987607, 'learning_rate': 9.056844455892704e-06, 'batch_size': 42, 'step_size': 13, 'gamma': 0.8668388805513009}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 14:56:58,045][0m Trial 43 finished with value: 0.047210629033300026 and parameters: {'observation_period_num': 19, 'train_rates': 0.9071790804027696, 'learning_rate': 1.6630080300070573e-05, 'batch_size': 25, 'step_size': 11, 'gamma': 0.9051465199609932}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 15:03:10,422][0m Trial 44 finished with value: 0.2074348211288452 and parameters: {'observation_period_num': 215, 'train_rates': 0.8613314684140406, 'learning_rate': 3.6313447295870317e-06, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9000733404803928}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 15:12:02,200][0m Trial 45 finished with value: 0.065605001306379 and parameters: {'observation_period_num': 53, 'train_rates': 0.8142903786223571, 'learning_rate': 3.207457027729952e-05, 'batch_size': 54, 'step_size': 8, 'gamma': 0.9464319158254699}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 15:44:11,282][0m Trial 46 finished with value: 0.08024179422958738 and parameters: {'observation_period_num': 34, 'train_rates': 0.9409223908610324, 'learning_rate': 4.271197952906756e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8783574296604106}. Best is trial 34 with value: 0.02687414020292019.[0m
[32m[I 2025-01-05 15:56:31,917][0m Trial 47 finished with value: 0.02685410169339398 and parameters: {'observation_period_num': 5, 'train_rates': 0.8886684749169751, 'learning_rate': 9.981037655905798e-06, 'batch_size': 42, 'step_size': 12, 'gamma': 0.839522011664329}. Best is trial 47 with value: 0.02685410169339398.[0m
[32m[I 2025-01-05 16:15:29,408][0m Trial 48 finished with value: 0.0826993726980938 and parameters: {'observation_period_num': 20, 'train_rates': 0.8789999097149275, 'learning_rate': 1.0221137324559782e-05, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8345559919928541}. Best is trial 47 with value: 0.02685410169339398.[0m
[32m[I 2025-01-05 16:19:21,372][0m Trial 49 finished with value: 0.11203459235791867 and parameters: {'observation_period_num': 63, 'train_rates': 0.7973079525537495, 'learning_rate': 1.6202968141255513e-05, 'batch_size': 164, 'step_size': 14, 'gamma': 0.8173429486204749}. Best is trial 47 with value: 0.02685410169339398.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 7, 'train_rates': 0.9863390432074738, 'learning_rate': 0.0006061438854803278, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9643488539682092}
Epoch 1/300, trend Loss: 0.3498 | 0.2435
Epoch 2/300, trend Loss: 0.1456 | 0.0924
Epoch 3/300, trend Loss: 0.1177 | 0.0695
Epoch 4/300, trend Loss: 0.1094 | 0.0702
Epoch 5/300, trend Loss: 0.1113 | 0.0921
Epoch 6/300, trend Loss: 0.1107 | 0.0583
Epoch 7/300, trend Loss: 0.1015 | 0.0582
Epoch 8/300, trend Loss: 0.0995 | 0.0564
Epoch 9/300, trend Loss: 0.0980 | 0.0536
Epoch 10/300, trend Loss: 0.0964 | 0.0530
Epoch 11/300, trend Loss: 0.0950 | 0.0532
Epoch 12/300, trend Loss: 0.0939 | 0.0538
Epoch 13/300, trend Loss: 0.0916 | 0.0504
Epoch 14/300, trend Loss: 0.0890 | 0.0486
Epoch 15/300, trend Loss: 0.0878 | 0.0479
Epoch 16/300, trend Loss: 0.0869 | 0.0474
Epoch 17/300, trend Loss: 0.0865 | 0.0472
Epoch 18/300, trend Loss: 0.0866 | 0.0477
Epoch 19/300, trend Loss: 0.0874 | 0.0492
Epoch 20/300, trend Loss: 0.0880 | 0.0509
Epoch 21/300, trend Loss: 0.0880 | 0.0531
Epoch 22/300, trend Loss: 0.0890 | 0.0600
Epoch 23/300, trend Loss: 0.0924 | 0.0816
Epoch 24/300, trend Loss: 0.0940 | 0.0812
Epoch 25/300, trend Loss: 0.1020 | 0.0724
Epoch 26/300, trend Loss: 0.1006 | 0.0480
Epoch 27/300, trend Loss: 0.0957 | 0.0498
Epoch 28/300, trend Loss: 0.0914 | 0.0483
Epoch 29/300, trend Loss: 0.0888 | 0.0487
Epoch 30/300, trend Loss: 0.0856 | 0.0479
Epoch 31/300, trend Loss: 0.0850 | 0.0452
Epoch 32/300, trend Loss: 0.0864 | 0.0473
Epoch 33/300, trend Loss: 0.0896 | 0.0414
Epoch 34/300, trend Loss: 0.0982 | 0.0578
Epoch 35/300, trend Loss: 0.0960 | 0.0479
Epoch 36/300, trend Loss: 0.1161 | 0.1061
Epoch 37/300, trend Loss: 0.1179 | 0.1443
Epoch 38/300, trend Loss: 0.1052 | 0.0747
Epoch 39/300, trend Loss: 0.1000 | 0.0778
Epoch 40/300, trend Loss: 0.0910 | 0.0432
Epoch 41/300, trend Loss: 0.0886 | 0.0490
Epoch 42/300, trend Loss: 0.0802 | 0.0422
Epoch 43/300, trend Loss: 0.0767 | 0.0430
Epoch 44/300, trend Loss: 0.0770 | 0.0427
Epoch 45/300, trend Loss: 0.0785 | 0.0414
Epoch 46/300, trend Loss: 0.0803 | 0.0402
Epoch 47/300, trend Loss: 0.0821 | 0.0406
Epoch 48/300, trend Loss: 0.0821 | 0.0394
Epoch 49/300, trend Loss: 0.0791 | 0.0382
Epoch 50/300, trend Loss: 0.0756 | 0.0385
Epoch 51/300, trend Loss: 0.0728 | 0.0384
Epoch 52/300, trend Loss: 0.0711 | 0.0375
Epoch 53/300, trend Loss: 0.0707 | 0.0366
Epoch 54/300, trend Loss: 0.0721 | 0.0395
Epoch 55/300, trend Loss: 0.0732 | 0.0397
Epoch 56/300, trend Loss: 0.0716 | 0.0390
Epoch 57/300, trend Loss: 0.0708 | 0.0408
Epoch 58/300, trend Loss: 0.0710 | 0.0393
Epoch 59/300, trend Loss: 0.0727 | 0.0388
Epoch 60/300, trend Loss: 0.0729 | 0.0387
Epoch 61/300, trend Loss: 0.0716 | 0.0397
Epoch 62/300, trend Loss: 0.0704 | 0.0381
Epoch 63/300, trend Loss: 0.0697 | 0.0378
Epoch 64/300, trend Loss: 0.0718 | 0.0414
Epoch 65/300, trend Loss: 0.0734 | 0.0403
Epoch 66/300, trend Loss: 0.0730 | 0.0369
Epoch 67/300, trend Loss: 0.0712 | 0.0373
Epoch 68/300, trend Loss: 0.0732 | 0.0460
Epoch 69/300, trend Loss: 0.0744 | 0.0507
Epoch 70/300, trend Loss: 0.0739 | 0.0460
Epoch 71/300, trend Loss: 0.0714 | 0.0341
Epoch 72/300, trend Loss: 0.0703 | 0.0343
Epoch 73/300, trend Loss: 0.0698 | 0.0367
Epoch 74/300, trend Loss: 0.0673 | 0.0321
Epoch 75/300, trend Loss: 0.0657 | 0.0325
Epoch 76/300, trend Loss: 0.0650 | 0.0335
Epoch 77/300, trend Loss: 0.0641 | 0.0331
Epoch 78/300, trend Loss: 0.0636 | 0.0314
Epoch 79/300, trend Loss: 0.0638 | 0.0301
Epoch 80/300, trend Loss: 0.0645 | 0.0306
Epoch 81/300, trend Loss: 0.0644 | 0.0309
Epoch 82/300, trend Loss: 0.0638 | 0.0308
Epoch 83/300, trend Loss: 0.0631 | 0.0302
Epoch 84/300, trend Loss: 0.0624 | 0.0298
Epoch 85/300, trend Loss: 0.0620 | 0.0297
Epoch 86/300, trend Loss: 0.0617 | 0.0297
Epoch 87/300, trend Loss: 0.0624 | 0.0314
Epoch 88/300, trend Loss: 0.0628 | 0.0325
Epoch 89/300, trend Loss: 0.0616 | 0.0313
Epoch 90/300, trend Loss: 0.0605 | 0.0302
Epoch 91/300, trend Loss: 0.0608 | 0.0285
Epoch 92/300, trend Loss: 0.0622 | 0.0280
Epoch 93/300, trend Loss: 0.0627 | 0.0282
Epoch 94/300, trend Loss: 0.0616 | 0.0278
Epoch 95/300, trend Loss: 0.0613 | 0.0270
Epoch 96/300, trend Loss: 0.0615 | 0.0272
Epoch 97/300, trend Loss: 0.0614 | 0.0262
Epoch 98/300, trend Loss: 0.0609 | 0.0263
Epoch 99/300, trend Loss: 0.0612 | 0.0267
Epoch 100/300, trend Loss: 0.0600 | 0.0256
Epoch 101/300, trend Loss: 0.0588 | 0.0238
Epoch 102/300, trend Loss: 0.0589 | 0.0227
Epoch 103/300, trend Loss: 0.0603 | 0.0239
Epoch 104/300, trend Loss: 0.0609 | 0.0253
Epoch 105/300, trend Loss: 0.0599 | 0.0251
Epoch 106/300, trend Loss: 0.0593 | 0.0250
Epoch 107/300, trend Loss: 0.0594 | 0.0245
Epoch 108/300, trend Loss: 0.0592 | 0.0242
Epoch 109/300, trend Loss: 0.0582 | 0.0228
Epoch 110/300, trend Loss: 0.0576 | 0.0219
Epoch 111/300, trend Loss: 0.0578 | 0.0219
Epoch 112/300, trend Loss: 0.0586 | 0.0224
Epoch 113/300, trend Loss: 0.0599 | 0.0231
Epoch 114/300, trend Loss: 0.0615 | 0.0239
Epoch 115/300, trend Loss: 0.0630 | 0.0278
Epoch 116/300, trend Loss: 0.0629 | 0.0271
Epoch 117/300, trend Loss: 0.0630 | 0.0253
Epoch 118/300, trend Loss: 0.0619 | 0.0225
Epoch 119/300, trend Loss: 0.0597 | 0.0224
Epoch 120/300, trend Loss: 0.0581 | 0.0217
Epoch 121/300, trend Loss: 0.0576 | 0.0205
Epoch 122/300, trend Loss: 0.0575 | 0.0215
Epoch 123/300, trend Loss: 0.0577 | 0.0238
Epoch 124/300, trend Loss: 0.0579 | 0.0237
Epoch 125/300, trend Loss: 0.0578 | 0.0231
Epoch 126/300, trend Loss: 0.0570 | 0.0202
Epoch 127/300, trend Loss: 0.0563 | 0.0188
Epoch 128/300, trend Loss: 0.0558 | 0.0191
Epoch 129/300, trend Loss: 0.0555 | 0.0196
Epoch 130/300, trend Loss: 0.0551 | 0.0201
Epoch 131/300, trend Loss: 0.0549 | 0.0203
Epoch 132/300, trend Loss: 0.0550 | 0.0202
Epoch 133/300, trend Loss: 0.0553 | 0.0204
Epoch 134/300, trend Loss: 0.0555 | 0.0201
Epoch 135/300, trend Loss: 0.0555 | 0.0192
Epoch 136/300, trend Loss: 0.0555 | 0.0180
Epoch 137/300, trend Loss: 0.0559 | 0.0175
Epoch 138/300, trend Loss: 0.0561 | 0.0177
Epoch 139/300, trend Loss: 0.0557 | 0.0185
Epoch 140/300, trend Loss: 0.0551 | 0.0186
Epoch 141/300, trend Loss: 0.0548 | 0.0190
Epoch 142/300, trend Loss: 0.0546 | 0.0193
Epoch 143/300, trend Loss: 0.0545 | 0.0197
Epoch 144/300, trend Loss: 0.0545 | 0.0198
Epoch 145/300, trend Loss: 0.0540 | 0.0192
Epoch 146/300, trend Loss: 0.0533 | 0.0184
Epoch 147/300, trend Loss: 0.0534 | 0.0184
Epoch 148/300, trend Loss: 0.0542 | 0.0193
Epoch 149/300, trend Loss: 0.0550 | 0.0194
Epoch 150/300, trend Loss: 0.0545 | 0.0197
Epoch 151/300, trend Loss: 0.0541 | 0.0197
Epoch 152/300, trend Loss: 0.0543 | 0.0199
Epoch 153/300, trend Loss: 0.0558 | 0.0210
Epoch 154/300, trend Loss: 0.0557 | 0.0203
Epoch 155/300, trend Loss: 0.0596 | 0.0255
Epoch 156/300, trend Loss: 0.0601 | 0.0250
Epoch 157/300, trend Loss: 0.0564 | 0.0217
Epoch 158/300, trend Loss: 0.0576 | 0.0243
Epoch 159/300, trend Loss: 0.0538 | 0.0215
Epoch 160/300, trend Loss: 0.0565 | 0.0234
Epoch 161/300, trend Loss: 0.0537 | 0.0209
Epoch 162/300, trend Loss: 0.0531 | 0.0191
Epoch 163/300, trend Loss: 0.0521 | 0.0174
Epoch 164/300, trend Loss: 0.0517 | 0.0170
Epoch 165/300, trend Loss: 0.0513 | 0.0169
Epoch 166/300, trend Loss: 0.0509 | 0.0171
Epoch 167/300, trend Loss: 0.0510 | 0.0175
Epoch 168/300, trend Loss: 0.0515 | 0.0182
Epoch 169/300, trend Loss: 0.0525 | 0.0197
Epoch 170/300, trend Loss: 0.0540 | 0.0189
Epoch 171/300, trend Loss: 0.0532 | 0.0187
Epoch 172/300, trend Loss: 0.0519 | 0.0174
Epoch 173/300, trend Loss: 0.0516 | 0.0183
Epoch 174/300, trend Loss: 0.0514 | 0.0185
Epoch 175/300, trend Loss: 0.0512 | 0.0185
Epoch 176/300, trend Loss: 0.0509 | 0.0182
Epoch 177/300, trend Loss: 0.0507 | 0.0180
Epoch 178/300, trend Loss: 0.0508 | 0.0179
Epoch 179/300, trend Loss: 0.0509 | 0.0175
Epoch 180/300, trend Loss: 0.0508 | 0.0168
Epoch 181/300, trend Loss: 0.0506 | 0.0166
Epoch 182/300, trend Loss: 0.0504 | 0.0172
Epoch 183/300, trend Loss: 0.0502 | 0.0180
Epoch 184/300, trend Loss: 0.0503 | 0.0186
Epoch 185/300, trend Loss: 0.0506 | 0.0189
Epoch 186/300, trend Loss: 0.0507 | 0.0193
Epoch 187/300, trend Loss: 0.0511 | 0.0197
Epoch 188/300, trend Loss: 0.0514 | 0.0192
Epoch 189/300, trend Loss: 0.0513 | 0.0187
Epoch 190/300, trend Loss: 0.0503 | 0.0175
Epoch 191/300, trend Loss: 0.0499 | 0.0176
Epoch 192/300, trend Loss: 0.0501 | 0.0176
Epoch 193/300, trend Loss: 0.0505 | 0.0175
Epoch 194/300, trend Loss: 0.0506 | 0.0182
Epoch 195/300, trend Loss: 0.0504 | 0.0173
Epoch 196/300, trend Loss: 0.0500 | 0.0173
Epoch 197/300, trend Loss: 0.0500 | 0.0169
Epoch 198/300, trend Loss: 0.0499 | 0.0161
Epoch 199/300, trend Loss: 0.0497 | 0.0152
Epoch 200/300, trend Loss: 0.0495 | 0.0154
Epoch 201/300, trend Loss: 0.0494 | 0.0164
Epoch 202/300, trend Loss: 0.0494 | 0.0179
Epoch 203/300, trend Loss: 0.0497 | 0.0196
Epoch 204/300, trend Loss: 0.0503 | 0.0198
Epoch 205/300, trend Loss: 0.0502 | 0.0184
Epoch 206/300, trend Loss: 0.0492 | 0.0180
Epoch 207/300, trend Loss: 0.0490 | 0.0183
Epoch 208/300, trend Loss: 0.0490 | 0.0178
Epoch 209/300, trend Loss: 0.0489 | 0.0176
Epoch 210/300, trend Loss: 0.0489 | 0.0172
Epoch 211/300, trend Loss: 0.0488 | 0.0174
Epoch 212/300, trend Loss: 0.0487 | 0.0166
Epoch 213/300, trend Loss: 0.0484 | 0.0161
Epoch 214/300, trend Loss: 0.0482 | 0.0159
Epoch 215/300, trend Loss: 0.0482 | 0.0160
Epoch 216/300, trend Loss: 0.0483 | 0.0166
Epoch 217/300, trend Loss: 0.0485 | 0.0180
Epoch 218/300, trend Loss: 0.0488 | 0.0192
Epoch 219/300, trend Loss: 0.0487 | 0.0188
Epoch 220/300, trend Loss: 0.0482 | 0.0181
Epoch 221/300, trend Loss: 0.0481 | 0.0180
Epoch 222/300, trend Loss: 0.0482 | 0.0177
Epoch 223/300, trend Loss: 0.0483 | 0.0178
Epoch 224/300, trend Loss: 0.0483 | 0.0177
Epoch 225/300, trend Loss: 0.0482 | 0.0172
Epoch 226/300, trend Loss: 0.0479 | 0.0166
Epoch 227/300, trend Loss: 0.0478 | 0.0162
Epoch 228/300, trend Loss: 0.0478 | 0.0161
Epoch 229/300, trend Loss: 0.0479 | 0.0168
Epoch 230/300, trend Loss: 0.0480 | 0.0182
Epoch 231/300, trend Loss: 0.0481 | 0.0192
Epoch 232/300, trend Loss: 0.0480 | 0.0191
Epoch 233/300, trend Loss: 0.0478 | 0.0186
Epoch 234/300, trend Loss: 0.0477 | 0.0184
Epoch 235/300, trend Loss: 0.0478 | 0.0181
Epoch 236/300, trend Loss: 0.0478 | 0.0178
Epoch 237/300, trend Loss: 0.0477 | 0.0175
Epoch 238/300, trend Loss: 0.0476 | 0.0170
Epoch 239/300, trend Loss: 0.0476 | 0.0165
Epoch 240/300, trend Loss: 0.0475 | 0.0162
Epoch 241/300, trend Loss: 0.0476 | 0.0162
Epoch 242/300, trend Loss: 0.0476 | 0.0167
Epoch 243/300, trend Loss: 0.0476 | 0.0180
Epoch 244/300, trend Loss: 0.0475 | 0.0192
Epoch 245/300, trend Loss: 0.0475 | 0.0192
Epoch 246/300, trend Loss: 0.0473 | 0.0186
Epoch 247/300, trend Loss: 0.0472 | 0.0182
Epoch 248/300, trend Loss: 0.0472 | 0.0181
Epoch 249/300, trend Loss: 0.0473 | 0.0178
Epoch 250/300, trend Loss: 0.0472 | 0.0174
Epoch 251/300, trend Loss: 0.0471 | 0.0168
Epoch 252/300, trend Loss: 0.0469 | 0.0164
Epoch 253/300, trend Loss: 0.0469 | 0.0161
Epoch 254/300, trend Loss: 0.0470 | 0.0163
Epoch 255/300, trend Loss: 0.0471 | 0.0172
Epoch 256/300, trend Loss: 0.0471 | 0.0182
Epoch 257/300, trend Loss: 0.0471 | 0.0188
Epoch 258/300, trend Loss: 0.0469 | 0.0186
Epoch 259/300, trend Loss: 0.0467 | 0.0184
Epoch 260/300, trend Loss: 0.0466 | 0.0182
Epoch 261/300, trend Loss: 0.0466 | 0.0179
Epoch 262/300, trend Loss: 0.0467 | 0.0176
Epoch 263/300, trend Loss: 0.0466 | 0.0172
Epoch 264/300, trend Loss: 0.0465 | 0.0168
Epoch 265/300, trend Loss: 0.0465 | 0.0165
Epoch 266/300, trend Loss: 0.0464 | 0.0164
Epoch 267/300, trend Loss: 0.0465 | 0.0168
Epoch 268/300, trend Loss: 0.0465 | 0.0175
Epoch 269/300, trend Loss: 0.0465 | 0.0181
Epoch 270/300, trend Loss: 0.0464 | 0.0183
Epoch 271/300, trend Loss: 0.0463 | 0.0181
Epoch 272/300, trend Loss: 0.0463 | 0.0179
Epoch 273/300, trend Loss: 0.0463 | 0.0176
Epoch 274/300, trend Loss: 0.0463 | 0.0174
Epoch 275/300, trend Loss: 0.0463 | 0.0171
Epoch 276/300, trend Loss: 0.0463 | 0.0168
Epoch 277/300, trend Loss: 0.0462 | 0.0165
Epoch 278/300, trend Loss: 0.0462 | 0.0162
Epoch 279/300, trend Loss: 0.0462 | 0.0161
Epoch 280/300, trend Loss: 0.0462 | 0.0164
Epoch 281/300, trend Loss: 0.0462 | 0.0169
Epoch 282/300, trend Loss: 0.0462 | 0.0174
Epoch 283/300, trend Loss: 0.0462 | 0.0179
Epoch 284/300, trend Loss: 0.0461 | 0.0181
Epoch 285/300, trend Loss: 0.0461 | 0.0179
Epoch 286/300, trend Loss: 0.0460 | 0.0177
Epoch 287/300, trend Loss: 0.0460 | 0.0175
Epoch 288/300, trend Loss: 0.0460 | 0.0172
Epoch 289/300, trend Loss: 0.0460 | 0.0170
Epoch 290/300, trend Loss: 0.0460 | 0.0168
Epoch 291/300, trend Loss: 0.0459 | 0.0165
Epoch 292/300, trend Loss: 0.0459 | 0.0163
Epoch 293/300, trend Loss: 0.0459 | 0.0162
Epoch 294/300, trend Loss: 0.0459 | 0.0163
Epoch 295/300, trend Loss: 0.0459 | 0.0166
Epoch 296/300, trend Loss: 0.0459 | 0.0170
Epoch 297/300, trend Loss: 0.0459 | 0.0175
Epoch 298/300, trend Loss: 0.0458 | 0.0177
Epoch 299/300, trend Loss: 0.0458 | 0.0177
Epoch 300/300, trend Loss: 0.0457 | 0.0176
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.789743754020081, 'learning_rate': 0.00027006554845270295, 'batch_size': 217, 'step_size': 15, 'gamma': 0.778372333803797}
Epoch 1/300, seasonal_0 Loss: 2.2297 | 1.1627
Epoch 2/300, seasonal_0 Loss: 0.4941 | 0.7036
Epoch 3/300, seasonal_0 Loss: 0.2468 | 0.2411
Epoch 4/300, seasonal_0 Loss: 0.2237 | 0.1408
Epoch 5/300, seasonal_0 Loss: 0.1696 | 0.1223
Epoch 6/300, seasonal_0 Loss: 0.2073 | 0.3097
Epoch 7/300, seasonal_0 Loss: 0.2900 | 0.1947
Epoch 8/300, seasonal_0 Loss: 0.3867 | 0.4999
Epoch 9/300, seasonal_0 Loss: 0.3648 | 0.2338
Epoch 10/300, seasonal_0 Loss: 0.3321 | 0.1612
Epoch 11/300, seasonal_0 Loss: 0.2880 | 0.1593
Epoch 12/300, seasonal_0 Loss: 0.2192 | 0.2001
Epoch 13/300, seasonal_0 Loss: 0.1617 | 0.1550
Epoch 14/300, seasonal_0 Loss: 0.1583 | 0.1073
Epoch 15/300, seasonal_0 Loss: 0.1376 | 0.0985
Epoch 16/300, seasonal_0 Loss: 0.1299 | 0.1162
Epoch 17/300, seasonal_0 Loss: 0.1264 | 0.0873
Epoch 18/300, seasonal_0 Loss: 0.1155 | 0.0822
Epoch 19/300, seasonal_0 Loss: 0.1132 | 0.0788
Epoch 20/300, seasonal_0 Loss: 0.1055 | 0.0689
Epoch 21/300, seasonal_0 Loss: 0.1027 | 0.0677
Epoch 22/300, seasonal_0 Loss: 0.1004 | 0.0624
Epoch 23/300, seasonal_0 Loss: 0.0982 | 0.0655
Epoch 24/300, seasonal_0 Loss: 0.0978 | 0.0639
Epoch 25/300, seasonal_0 Loss: 0.0975 | 0.0606
Epoch 26/300, seasonal_0 Loss: 0.0950 | 0.0658
Epoch 27/300, seasonal_0 Loss: 0.0954 | 0.0564
Epoch 28/300, seasonal_0 Loss: 0.0939 | 0.0579
Epoch 29/300, seasonal_0 Loss: 0.0922 | 0.0539
Epoch 30/300, seasonal_0 Loss: 0.0917 | 0.0626
Epoch 31/300, seasonal_0 Loss: 0.0927 | 0.0561
Epoch 32/300, seasonal_0 Loss: 0.0913 | 0.0564
Epoch 33/300, seasonal_0 Loss: 0.0911 | 0.0521
Epoch 34/300, seasonal_0 Loss: 0.0911 | 0.0492
Epoch 35/300, seasonal_0 Loss: 0.0894 | 0.0596
Epoch 36/300, seasonal_0 Loss: 0.0892 | 0.0481
Epoch 37/300, seasonal_0 Loss: 0.0938 | 0.1034
Epoch 38/300, seasonal_0 Loss: 0.1009 | 0.0623
Epoch 39/300, seasonal_0 Loss: 0.0943 | 0.0581
Epoch 40/300, seasonal_0 Loss: 0.0940 | 0.0508
Epoch 41/300, seasonal_0 Loss: 0.0912 | 0.0492
Epoch 42/300, seasonal_0 Loss: 0.0875 | 0.0476
Epoch 43/300, seasonal_0 Loss: 0.0868 | 0.0507
Epoch 44/300, seasonal_0 Loss: 0.0859 | 0.0451
Epoch 45/300, seasonal_0 Loss: 0.0862 | 0.0563
Epoch 46/300, seasonal_0 Loss: 0.0882 | 0.0484
Epoch 47/300, seasonal_0 Loss: 0.0866 | 0.0477
Epoch 48/300, seasonal_0 Loss: 0.0903 | 0.0557
Epoch 49/300, seasonal_0 Loss: 0.0874 | 0.0490
Epoch 50/300, seasonal_0 Loss: 0.0885 | 0.0525
Epoch 51/300, seasonal_0 Loss: 0.0879 | 0.0521
Epoch 52/300, seasonal_0 Loss: 0.0882 | 0.0510
Epoch 53/300, seasonal_0 Loss: 0.0865 | 0.0476
Epoch 54/300, seasonal_0 Loss: 0.0856 | 0.0502
Epoch 55/300, seasonal_0 Loss: 0.0841 | 0.0454
Epoch 56/300, seasonal_0 Loss: 0.0838 | 0.0471
Epoch 57/300, seasonal_0 Loss: 0.0842 | 0.0449
Epoch 58/300, seasonal_0 Loss: 0.0835 | 0.0469
Epoch 59/300, seasonal_0 Loss: 0.0848 | 0.0460
Epoch 60/300, seasonal_0 Loss: 0.0849 | 0.0475
Epoch 61/300, seasonal_0 Loss: 0.0849 | 0.0483
Epoch 62/300, seasonal_0 Loss: 0.0835 | 0.0442
Epoch 63/300, seasonal_0 Loss: 0.0839 | 0.0467
Epoch 64/300, seasonal_0 Loss: 0.0835 | 0.0450
Epoch 65/300, seasonal_0 Loss: 0.0838 | 0.0476
Epoch 66/300, seasonal_0 Loss: 0.0837 | 0.0442
Epoch 67/300, seasonal_0 Loss: 0.0850 | 0.0503
Epoch 68/300, seasonal_0 Loss: 0.0847 | 0.0437
Epoch 69/300, seasonal_0 Loss: 0.0850 | 0.0549
Epoch 70/300, seasonal_0 Loss: 0.0834 | 0.0450
Epoch 71/300, seasonal_0 Loss: 0.0840 | 0.0559
Epoch 72/300, seasonal_0 Loss: 0.0830 | 0.0450
Epoch 73/300, seasonal_0 Loss: 0.0831 | 0.0547
Epoch 74/300, seasonal_0 Loss: 0.0820 | 0.0443
Epoch 75/300, seasonal_0 Loss: 0.0819 | 0.0517
Epoch 76/300, seasonal_0 Loss: 0.0805 | 0.0437
Epoch 77/300, seasonal_0 Loss: 0.0799 | 0.0478
Epoch 78/300, seasonal_0 Loss: 0.0793 | 0.0434
Epoch 79/300, seasonal_0 Loss: 0.0791 | 0.0447
Epoch 80/300, seasonal_0 Loss: 0.0789 | 0.0433
Epoch 81/300, seasonal_0 Loss: 0.0787 | 0.0436
Epoch 82/300, seasonal_0 Loss: 0.0786 | 0.0431
Epoch 83/300, seasonal_0 Loss: 0.0785 | 0.0432
Epoch 84/300, seasonal_0 Loss: 0.0784 | 0.0429
Epoch 85/300, seasonal_0 Loss: 0.0784 | 0.0429
Epoch 86/300, seasonal_0 Loss: 0.0783 | 0.0428
Epoch 87/300, seasonal_0 Loss: 0.0782 | 0.0427
Epoch 88/300, seasonal_0 Loss: 0.0782 | 0.0426
Epoch 89/300, seasonal_0 Loss: 0.0781 | 0.0425
Epoch 90/300, seasonal_0 Loss: 0.0780 | 0.0424
Epoch 91/300, seasonal_0 Loss: 0.0779 | 0.0424
Epoch 92/300, seasonal_0 Loss: 0.0779 | 0.0423
Epoch 93/300, seasonal_0 Loss: 0.0778 | 0.0422
Epoch 94/300, seasonal_0 Loss: 0.0778 | 0.0421
Epoch 95/300, seasonal_0 Loss: 0.0777 | 0.0421
Epoch 96/300, seasonal_0 Loss: 0.0777 | 0.0420
Epoch 97/300, seasonal_0 Loss: 0.0776 | 0.0419
Epoch 98/300, seasonal_0 Loss: 0.0775 | 0.0419
Epoch 99/300, seasonal_0 Loss: 0.0775 | 0.0418
Epoch 100/300, seasonal_0 Loss: 0.0774 | 0.0418
Epoch 101/300, seasonal_0 Loss: 0.0774 | 0.0417
Epoch 102/300, seasonal_0 Loss: 0.0773 | 0.0417
Epoch 103/300, seasonal_0 Loss: 0.0773 | 0.0416
Epoch 104/300, seasonal_0 Loss: 0.0773 | 0.0416
Epoch 105/300, seasonal_0 Loss: 0.0772 | 0.0415
Epoch 106/300, seasonal_0 Loss: 0.0772 | 0.0415
Epoch 107/300, seasonal_0 Loss: 0.0771 | 0.0415
Epoch 108/300, seasonal_0 Loss: 0.0771 | 0.0414
Epoch 109/300, seasonal_0 Loss: 0.0770 | 0.0414
Epoch 110/300, seasonal_0 Loss: 0.0770 | 0.0413
Epoch 111/300, seasonal_0 Loss: 0.0770 | 0.0413
Epoch 112/300, seasonal_0 Loss: 0.0769 | 0.0413
Epoch 113/300, seasonal_0 Loss: 0.0769 | 0.0412
Epoch 114/300, seasonal_0 Loss: 0.0768 | 0.0412
Epoch 115/300, seasonal_0 Loss: 0.0768 | 0.0412
Epoch 116/300, seasonal_0 Loss: 0.0768 | 0.0411
Epoch 117/300, seasonal_0 Loss: 0.0768 | 0.0411
Epoch 118/300, seasonal_0 Loss: 0.0767 | 0.0411
Epoch 119/300, seasonal_0 Loss: 0.0767 | 0.0410
Epoch 120/300, seasonal_0 Loss: 0.0767 | 0.0410
Epoch 121/300, seasonal_0 Loss: 0.0766 | 0.0410
Epoch 122/300, seasonal_0 Loss: 0.0766 | 0.0409
Epoch 123/300, seasonal_0 Loss: 0.0766 | 0.0409
Epoch 124/300, seasonal_0 Loss: 0.0765 | 0.0409
Epoch 125/300, seasonal_0 Loss: 0.0765 | 0.0409
Epoch 126/300, seasonal_0 Loss: 0.0765 | 0.0408
Epoch 127/300, seasonal_0 Loss: 0.0765 | 0.0408
Epoch 128/300, seasonal_0 Loss: 0.0764 | 0.0408
Epoch 129/300, seasonal_0 Loss: 0.0764 | 0.0408
Epoch 130/300, seasonal_0 Loss: 0.0764 | 0.0408
Epoch 131/300, seasonal_0 Loss: 0.0764 | 0.0407
Epoch 132/300, seasonal_0 Loss: 0.0764 | 0.0407
Epoch 133/300, seasonal_0 Loss: 0.0763 | 0.0407
Epoch 134/300, seasonal_0 Loss: 0.0763 | 0.0407
Epoch 135/300, seasonal_0 Loss: 0.0763 | 0.0407
Epoch 136/300, seasonal_0 Loss: 0.0763 | 0.0406
Epoch 137/300, seasonal_0 Loss: 0.0762 | 0.0406
Epoch 138/300, seasonal_0 Loss: 0.0762 | 0.0406
Epoch 139/300, seasonal_0 Loss: 0.0762 | 0.0406
Epoch 140/300, seasonal_0 Loss: 0.0762 | 0.0406
Epoch 141/300, seasonal_0 Loss: 0.0762 | 0.0406
Epoch 142/300, seasonal_0 Loss: 0.0762 | 0.0405
Epoch 143/300, seasonal_0 Loss: 0.0761 | 0.0405
Epoch 144/300, seasonal_0 Loss: 0.0761 | 0.0405
Epoch 145/300, seasonal_0 Loss: 0.0761 | 0.0405
Epoch 146/300, seasonal_0 Loss: 0.0761 | 0.0405
Epoch 147/300, seasonal_0 Loss: 0.0761 | 0.0405
Epoch 148/300, seasonal_0 Loss: 0.0761 | 0.0405
Epoch 149/300, seasonal_0 Loss: 0.0761 | 0.0404
Epoch 150/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 151/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 152/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 153/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 154/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 155/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 156/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 157/300, seasonal_0 Loss: 0.0760 | 0.0404
Epoch 158/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 159/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 160/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 161/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 162/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 163/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 164/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 165/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 166/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 167/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 168/300, seasonal_0 Loss: 0.0759 | 0.0403
Epoch 169/300, seasonal_0 Loss: 0.0758 | 0.0403
Epoch 170/300, seasonal_0 Loss: 0.0758 | 0.0403
Epoch 171/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 172/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 173/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 174/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 175/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 176/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 177/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 178/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 179/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 180/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 181/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 182/300, seasonal_0 Loss: 0.0758 | 0.0402
Epoch 183/300, seasonal_0 Loss: 0.0757 | 0.0402
Epoch 184/300, seasonal_0 Loss: 0.0757 | 0.0402
Epoch 185/300, seasonal_0 Loss: 0.0757 | 0.0402
Epoch 186/300, seasonal_0 Loss: 0.0757 | 0.0402
Epoch 187/300, seasonal_0 Loss: 0.0757 | 0.0402
Epoch 188/300, seasonal_0 Loss: 0.0757 | 0.0402
Epoch 189/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 190/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 191/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 192/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 193/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 194/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 195/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 196/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 197/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 198/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 199/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 200/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 201/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 202/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 203/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 204/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 205/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 206/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 207/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 208/300, seasonal_0 Loss: 0.0757 | 0.0401
Epoch 209/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 210/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 211/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 212/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 213/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 214/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 215/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 216/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 217/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 218/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 219/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 220/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 221/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 222/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 223/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 224/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 225/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 226/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 227/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 228/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 229/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 230/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 231/300, seasonal_0 Loss: 0.0756 | 0.0401
Epoch 232/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 233/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 234/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 235/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 236/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 237/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 238/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 239/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 240/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 241/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 242/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 243/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 244/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 245/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 246/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 247/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 248/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 249/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 250/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 251/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 252/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 253/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 254/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 255/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 256/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 257/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 258/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 259/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 260/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 261/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 262/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 263/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 264/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 265/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 266/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 267/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 268/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 269/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 270/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 271/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 272/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 273/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 274/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 275/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 276/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 277/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 278/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 279/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 280/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 281/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 282/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 283/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 284/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 285/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 286/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 287/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 288/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 289/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 290/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 291/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 292/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 293/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 294/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 295/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 296/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 297/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 298/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 299/300, seasonal_0 Loss: 0.0756 | 0.0400
Epoch 300/300, seasonal_0 Loss: 0.0756 | 0.0400
Training seasonal_1 component with params: {'observation_period_num': 23, 'train_rates': 0.8090220292351398, 'learning_rate': 0.00012068648759107733, 'batch_size': 204, 'step_size': 13, 'gamma': 0.8900497390985385}
Epoch 1/300, seasonal_1 Loss: 1.4239 | 0.3960
Epoch 2/300, seasonal_1 Loss: 0.3042 | 0.6094
Epoch 3/300, seasonal_1 Loss: 0.2825 | 0.2814
Epoch 4/300, seasonal_1 Loss: 0.2177 | 0.1967
Epoch 5/300, seasonal_1 Loss: 0.2530 | 0.2800
Epoch 6/300, seasonal_1 Loss: 0.2291 | 0.1911
Epoch 7/300, seasonal_1 Loss: 0.1671 | 0.1492
Epoch 8/300, seasonal_1 Loss: 0.1785 | 0.1477
Epoch 9/300, seasonal_1 Loss: 0.1522 | 0.0985
Epoch 10/300, seasonal_1 Loss: 0.1239 | 0.1020
Epoch 11/300, seasonal_1 Loss: 0.1306 | 0.1000
Epoch 12/300, seasonal_1 Loss: 0.1592 | 0.1100
Epoch 13/300, seasonal_1 Loss: 0.1911 | 0.3339
Epoch 14/300, seasonal_1 Loss: 0.1872 | 0.1041
Epoch 15/300, seasonal_1 Loss: 0.1755 | 0.0989
Epoch 16/300, seasonal_1 Loss: 0.2351 | 0.0763
Epoch 17/300, seasonal_1 Loss: 0.1500 | 0.1436
Epoch 18/300, seasonal_1 Loss: 0.1571 | 0.1341
Epoch 19/300, seasonal_1 Loss: 0.1495 | 0.1161
Epoch 20/300, seasonal_1 Loss: 0.1738 | 0.1738
Epoch 21/300, seasonal_1 Loss: 0.2108 | 0.1727
Epoch 22/300, seasonal_1 Loss: 0.1155 | 0.0769
Epoch 23/300, seasonal_1 Loss: 0.1074 | 0.0730
Epoch 24/300, seasonal_1 Loss: 0.1011 | 0.0629
Epoch 25/300, seasonal_1 Loss: 0.0962 | 0.0590
Epoch 26/300, seasonal_1 Loss: 0.0923 | 0.0635
Epoch 27/300, seasonal_1 Loss: 0.0888 | 0.0562
Epoch 28/300, seasonal_1 Loss: 0.0889 | 0.0537
Epoch 29/300, seasonal_1 Loss: 0.0852 | 0.0559
Epoch 30/300, seasonal_1 Loss: 0.0901 | 0.0778
Epoch 31/300, seasonal_1 Loss: 0.0914 | 0.0586
Epoch 32/300, seasonal_1 Loss: 0.0943 | 0.0674
Epoch 33/300, seasonal_1 Loss: 0.0974 | 0.0725
Epoch 34/300, seasonal_1 Loss: 0.0912 | 0.0854
Epoch 35/300, seasonal_1 Loss: 0.1060 | 0.0857
Epoch 36/300, seasonal_1 Loss: 0.0987 | 0.0660
Epoch 37/300, seasonal_1 Loss: 0.0957 | 0.0536
Epoch 38/300, seasonal_1 Loss: 0.0906 | 0.0625
Epoch 39/300, seasonal_1 Loss: 0.0894 | 0.0588
Epoch 40/300, seasonal_1 Loss: 0.0847 | 0.0529
Epoch 41/300, seasonal_1 Loss: 0.0888 | 0.0709
Epoch 42/300, seasonal_1 Loss: 0.0898 | 0.0593
Epoch 43/300, seasonal_1 Loss: 0.0956 | 0.0827
Epoch 44/300, seasonal_1 Loss: 0.0897 | 0.0580
Epoch 45/300, seasonal_1 Loss: 0.0936 | 0.0933
Epoch 46/300, seasonal_1 Loss: 0.0905 | 0.0567
Epoch 47/300, seasonal_1 Loss: 0.0900 | 0.0827
Epoch 48/300, seasonal_1 Loss: 0.0837 | 0.0560
Epoch 49/300, seasonal_1 Loss: 0.0852 | 0.0770
Epoch 50/300, seasonal_1 Loss: 0.0818 | 0.0500
Epoch 51/300, seasonal_1 Loss: 0.0827 | 0.0653
Epoch 52/300, seasonal_1 Loss: 0.0800 | 0.0542
Epoch 53/300, seasonal_1 Loss: 0.0819 | 0.0600
Epoch 54/300, seasonal_1 Loss: 0.0759 | 0.0470
Epoch 55/300, seasonal_1 Loss: 0.0803 | 0.0585
Epoch 56/300, seasonal_1 Loss: 0.0791 | 0.0483
Epoch 57/300, seasonal_1 Loss: 0.0796 | 0.0552
Epoch 58/300, seasonal_1 Loss: 0.0779 | 0.0492
Epoch 59/300, seasonal_1 Loss: 0.0810 | 0.0572
Epoch 60/300, seasonal_1 Loss: 0.0834 | 0.0455
Epoch 61/300, seasonal_1 Loss: 0.0785 | 0.0588
Epoch 62/300, seasonal_1 Loss: 0.0786 | 0.0479
Epoch 63/300, seasonal_1 Loss: 0.0808 | 0.0639
Epoch 64/300, seasonal_1 Loss: 0.0828 | 0.0464
Epoch 65/300, seasonal_1 Loss: 0.0800 | 0.0599
Epoch 66/300, seasonal_1 Loss: 0.0766 | 0.0470
Epoch 67/300, seasonal_1 Loss: 0.0787 | 0.0631
Epoch 68/300, seasonal_1 Loss: 0.0782 | 0.0457
Epoch 69/300, seasonal_1 Loss: 0.0772 | 0.0582
Epoch 70/300, seasonal_1 Loss: 0.0745 | 0.0475
Epoch 71/300, seasonal_1 Loss: 0.0766 | 0.0608
Epoch 72/300, seasonal_1 Loss: 0.0753 | 0.0461
Epoch 73/300, seasonal_1 Loss: 0.0752 | 0.0612
Epoch 74/300, seasonal_1 Loss: 0.0729 | 0.0458
Epoch 75/300, seasonal_1 Loss: 0.0750 | 0.0646
Epoch 76/300, seasonal_1 Loss: 0.0736 | 0.0457
Epoch 77/300, seasonal_1 Loss: 0.0738 | 0.0621
Epoch 78/300, seasonal_1 Loss: 0.0723 | 0.0462
Epoch 79/300, seasonal_1 Loss: 0.0728 | 0.0644
Epoch 80/300, seasonal_1 Loss: 0.0712 | 0.0451
Epoch 81/300, seasonal_1 Loss: 0.0712 | 0.0620
Epoch 82/300, seasonal_1 Loss: 0.0700 | 0.0450
Epoch 83/300, seasonal_1 Loss: 0.0702 | 0.0614
Epoch 84/300, seasonal_1 Loss: 0.0692 | 0.0448
Epoch 85/300, seasonal_1 Loss: 0.0694 | 0.0608
Epoch 86/300, seasonal_1 Loss: 0.0683 | 0.0439
Epoch 87/300, seasonal_1 Loss: 0.0680 | 0.0601
Epoch 88/300, seasonal_1 Loss: 0.0671 | 0.0440
Epoch 89/300, seasonal_1 Loss: 0.0669 | 0.0575
Epoch 90/300, seasonal_1 Loss: 0.0664 | 0.0436
Epoch 91/300, seasonal_1 Loss: 0.0662 | 0.0561
Epoch 92/300, seasonal_1 Loss: 0.0652 | 0.0431
Epoch 93/300, seasonal_1 Loss: 0.0653 | 0.0549
Epoch 94/300, seasonal_1 Loss: 0.0645 | 0.0431
Epoch 95/300, seasonal_1 Loss: 0.0643 | 0.0519
Epoch 96/300, seasonal_1 Loss: 0.0636 | 0.0429
Epoch 97/300, seasonal_1 Loss: 0.0636 | 0.0509
Epoch 98/300, seasonal_1 Loss: 0.0634 | 0.0427
Epoch 99/300, seasonal_1 Loss: 0.0629 | 0.0496
Epoch 100/300, seasonal_1 Loss: 0.0628 | 0.0421
Epoch 101/300, seasonal_1 Loss: 0.0622 | 0.0483
Epoch 102/300, seasonal_1 Loss: 0.0623 | 0.0425
Epoch 103/300, seasonal_1 Loss: 0.0616 | 0.0471
Epoch 104/300, seasonal_1 Loss: 0.0620 | 0.0419
Epoch 105/300, seasonal_1 Loss: 0.0613 | 0.0462
Epoch 106/300, seasonal_1 Loss: 0.0613 | 0.0420
Epoch 107/300, seasonal_1 Loss: 0.0611 | 0.0455
Epoch 108/300, seasonal_1 Loss: 0.0610 | 0.0419
Epoch 109/300, seasonal_1 Loss: 0.0608 | 0.0441
Epoch 110/300, seasonal_1 Loss: 0.0605 | 0.0419
Epoch 111/300, seasonal_1 Loss: 0.0607 | 0.0441
Epoch 112/300, seasonal_1 Loss: 0.0603 | 0.0419
Epoch 113/300, seasonal_1 Loss: 0.0605 | 0.0431
Epoch 114/300, seasonal_1 Loss: 0.0600 | 0.0418
Epoch 115/300, seasonal_1 Loss: 0.0602 | 0.0430
Epoch 116/300, seasonal_1 Loss: 0.0600 | 0.0421
Epoch 117/300, seasonal_1 Loss: 0.0600 | 0.0425
Epoch 118/300, seasonal_1 Loss: 0.0601 | 0.0416
Epoch 119/300, seasonal_1 Loss: 0.0596 | 0.0424
Epoch 120/300, seasonal_1 Loss: 0.0599 | 0.0419
Epoch 121/300, seasonal_1 Loss: 0.0595 | 0.0420
Epoch 122/300, seasonal_1 Loss: 0.0596 | 0.0415
Epoch 123/300, seasonal_1 Loss: 0.0594 | 0.0419
Epoch 124/300, seasonal_1 Loss: 0.0594 | 0.0418
Epoch 125/300, seasonal_1 Loss: 0.0594 | 0.0418
Epoch 126/300, seasonal_1 Loss: 0.0592 | 0.0413
Epoch 127/300, seasonal_1 Loss: 0.0590 | 0.0415
Epoch 128/300, seasonal_1 Loss: 0.0589 | 0.0417
Epoch 129/300, seasonal_1 Loss: 0.0589 | 0.0416
Epoch 130/300, seasonal_1 Loss: 0.0587 | 0.0413
Epoch 131/300, seasonal_1 Loss: 0.0587 | 0.0412
Epoch 132/300, seasonal_1 Loss: 0.0585 | 0.0413
Epoch 133/300, seasonal_1 Loss: 0.0585 | 0.0414
Epoch 134/300, seasonal_1 Loss: 0.0584 | 0.0413
Epoch 135/300, seasonal_1 Loss: 0.0583 | 0.0411
Epoch 136/300, seasonal_1 Loss: 0.0582 | 0.0411
Epoch 137/300, seasonal_1 Loss: 0.0581 | 0.0412
Epoch 138/300, seasonal_1 Loss: 0.0581 | 0.0412
Epoch 139/300, seasonal_1 Loss: 0.0580 | 0.0411
Epoch 140/300, seasonal_1 Loss: 0.0580 | 0.0409
Epoch 141/300, seasonal_1 Loss: 0.0578 | 0.0410
Epoch 142/300, seasonal_1 Loss: 0.0578 | 0.0411
Epoch 143/300, seasonal_1 Loss: 0.0578 | 0.0411
Epoch 144/300, seasonal_1 Loss: 0.0577 | 0.0409
Epoch 145/300, seasonal_1 Loss: 0.0576 | 0.0409
Epoch 146/300, seasonal_1 Loss: 0.0575 | 0.0410
Epoch 147/300, seasonal_1 Loss: 0.0575 | 0.0410
Epoch 148/300, seasonal_1 Loss: 0.0574 | 0.0409
Epoch 149/300, seasonal_1 Loss: 0.0574 | 0.0408
Epoch 150/300, seasonal_1 Loss: 0.0573 | 0.0409
Epoch 151/300, seasonal_1 Loss: 0.0573 | 0.0409
Epoch 152/300, seasonal_1 Loss: 0.0572 | 0.0409
Epoch 153/300, seasonal_1 Loss: 0.0572 | 0.0407
Epoch 154/300, seasonal_1 Loss: 0.0571 | 0.0408
Epoch 155/300, seasonal_1 Loss: 0.0570 | 0.0408
Epoch 156/300, seasonal_1 Loss: 0.0570 | 0.0408
Epoch 157/300, seasonal_1 Loss: 0.0569 | 0.0407
Epoch 158/300, seasonal_1 Loss: 0.0569 | 0.0407
Epoch 159/300, seasonal_1 Loss: 0.0568 | 0.0407
Epoch 160/300, seasonal_1 Loss: 0.0568 | 0.0408
Epoch 161/300, seasonal_1 Loss: 0.0568 | 0.0407
Epoch 162/300, seasonal_1 Loss: 0.0567 | 0.0407
Epoch 163/300, seasonal_1 Loss: 0.0567 | 0.0407
Epoch 164/300, seasonal_1 Loss: 0.0566 | 0.0407
Epoch 165/300, seasonal_1 Loss: 0.0566 | 0.0407
Epoch 166/300, seasonal_1 Loss: 0.0565 | 0.0406
Epoch 167/300, seasonal_1 Loss: 0.0565 | 0.0406
Epoch 168/300, seasonal_1 Loss: 0.0565 | 0.0407
Epoch 169/300, seasonal_1 Loss: 0.0564 | 0.0407
Epoch 170/300, seasonal_1 Loss: 0.0564 | 0.0406
Epoch 171/300, seasonal_1 Loss: 0.0563 | 0.0406
Epoch 172/300, seasonal_1 Loss: 0.0563 | 0.0406
Epoch 173/300, seasonal_1 Loss: 0.0563 | 0.0406
Epoch 174/300, seasonal_1 Loss: 0.0562 | 0.0406
Epoch 175/300, seasonal_1 Loss: 0.0562 | 0.0406
Epoch 176/300, seasonal_1 Loss: 0.0562 | 0.0406
Epoch 177/300, seasonal_1 Loss: 0.0561 | 0.0406
Epoch 178/300, seasonal_1 Loss: 0.0561 | 0.0406
Epoch 179/300, seasonal_1 Loss: 0.0560 | 0.0406
Epoch 180/300, seasonal_1 Loss: 0.0560 | 0.0406
Epoch 181/300, seasonal_1 Loss: 0.0560 | 0.0406
Epoch 182/300, seasonal_1 Loss: 0.0559 | 0.0406
Epoch 183/300, seasonal_1 Loss: 0.0559 | 0.0405
Epoch 184/300, seasonal_1 Loss: 0.0559 | 0.0405
Epoch 185/300, seasonal_1 Loss: 0.0559 | 0.0405
Epoch 186/300, seasonal_1 Loss: 0.0558 | 0.0405
Epoch 187/300, seasonal_1 Loss: 0.0558 | 0.0405
Epoch 188/300, seasonal_1 Loss: 0.0558 | 0.0405
Epoch 189/300, seasonal_1 Loss: 0.0557 | 0.0405
Epoch 190/300, seasonal_1 Loss: 0.0557 | 0.0405
Epoch 191/300, seasonal_1 Loss: 0.0557 | 0.0405
Epoch 192/300, seasonal_1 Loss: 0.0556 | 0.0405
Epoch 193/300, seasonal_1 Loss: 0.0556 | 0.0405
Epoch 194/300, seasonal_1 Loss: 0.0556 | 0.0405
Epoch 195/300, seasonal_1 Loss: 0.0556 | 0.0405
Epoch 196/300, seasonal_1 Loss: 0.0555 | 0.0405
Epoch 197/300, seasonal_1 Loss: 0.0555 | 0.0405
Epoch 198/300, seasonal_1 Loss: 0.0555 | 0.0405
Epoch 199/300, seasonal_1 Loss: 0.0555 | 0.0405
Epoch 200/300, seasonal_1 Loss: 0.0554 | 0.0405
Epoch 201/300, seasonal_1 Loss: 0.0554 | 0.0405
Epoch 202/300, seasonal_1 Loss: 0.0554 | 0.0405
Epoch 203/300, seasonal_1 Loss: 0.0554 | 0.0405
Epoch 204/300, seasonal_1 Loss: 0.0553 | 0.0405
Epoch 205/300, seasonal_1 Loss: 0.0553 | 0.0405
Epoch 206/300, seasonal_1 Loss: 0.0553 | 0.0405
Epoch 207/300, seasonal_1 Loss: 0.0553 | 0.0405
Epoch 208/300, seasonal_1 Loss: 0.0552 | 0.0404
Epoch 209/300, seasonal_1 Loss: 0.0552 | 0.0404
Epoch 210/300, seasonal_1 Loss: 0.0552 | 0.0404
Epoch 211/300, seasonal_1 Loss: 0.0552 | 0.0404
Epoch 212/300, seasonal_1 Loss: 0.0551 | 0.0404
Epoch 213/300, seasonal_1 Loss: 0.0551 | 0.0404
Epoch 214/300, seasonal_1 Loss: 0.0551 | 0.0404
Epoch 215/300, seasonal_1 Loss: 0.0551 | 0.0404
Epoch 216/300, seasonal_1 Loss: 0.0551 | 0.0404
Epoch 217/300, seasonal_1 Loss: 0.0550 | 0.0404
Epoch 218/300, seasonal_1 Loss: 0.0550 | 0.0404
Epoch 219/300, seasonal_1 Loss: 0.0550 | 0.0404
Epoch 220/300, seasonal_1 Loss: 0.0550 | 0.0404
Epoch 221/300, seasonal_1 Loss: 0.0550 | 0.0404
Epoch 222/300, seasonal_1 Loss: 0.0549 | 0.0404
Epoch 223/300, seasonal_1 Loss: 0.0549 | 0.0404
Epoch 224/300, seasonal_1 Loss: 0.0549 | 0.0404
Epoch 225/300, seasonal_1 Loss: 0.0549 | 0.0404
Epoch 226/300, seasonal_1 Loss: 0.0549 | 0.0404
Epoch 227/300, seasonal_1 Loss: 0.0549 | 0.0404
Epoch 228/300, seasonal_1 Loss: 0.0548 | 0.0404
Epoch 229/300, seasonal_1 Loss: 0.0548 | 0.0404
Epoch 230/300, seasonal_1 Loss: 0.0548 | 0.0404
Epoch 231/300, seasonal_1 Loss: 0.0548 | 0.0404
Epoch 232/300, seasonal_1 Loss: 0.0548 | 0.0404
Epoch 233/300, seasonal_1 Loss: 0.0548 | 0.0404
Epoch 234/300, seasonal_1 Loss: 0.0547 | 0.0404
Epoch 235/300, seasonal_1 Loss: 0.0547 | 0.0404
Epoch 236/300, seasonal_1 Loss: 0.0547 | 0.0404
Epoch 237/300, seasonal_1 Loss: 0.0547 | 0.0404
Epoch 238/300, seasonal_1 Loss: 0.0547 | 0.0404
Epoch 239/300, seasonal_1 Loss: 0.0547 | 0.0404
Epoch 240/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 241/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 242/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 243/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 244/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 245/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 246/300, seasonal_1 Loss: 0.0546 | 0.0404
Epoch 247/300, seasonal_1 Loss: 0.0545 | 0.0404
Epoch 248/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 249/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 250/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 251/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 252/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 253/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 254/300, seasonal_1 Loss: 0.0545 | 0.0403
Epoch 255/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 256/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 257/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 258/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 259/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 260/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 261/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 262/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 263/300, seasonal_1 Loss: 0.0544 | 0.0403
Epoch 264/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 265/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 266/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 267/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 268/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 269/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 270/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 271/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 272/300, seasonal_1 Loss: 0.0543 | 0.0403
Epoch 273/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 274/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 275/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 276/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 277/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 278/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 279/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 280/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 281/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 282/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 283/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 284/300, seasonal_1 Loss: 0.0542 | 0.0403
Epoch 285/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 286/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 287/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 288/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 289/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 290/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 291/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 292/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 293/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 294/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 295/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 296/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 297/300, seasonal_1 Loss: 0.0541 | 0.0403
Epoch 298/300, seasonal_1 Loss: 0.0540 | 0.0403
Epoch 299/300, seasonal_1 Loss: 0.0540 | 0.0403
Epoch 300/300, seasonal_1 Loss: 0.0540 | 0.0403
Training seasonal_2 component with params: {'observation_period_num': 14, 'train_rates': 0.9354504448830356, 'learning_rate': 7.270224616218108e-05, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8558525468357027}
Epoch 1/300, seasonal_2 Loss: 0.5740 | 0.2744
Epoch 2/300, seasonal_2 Loss: 0.1853 | 0.1432
Epoch 3/300, seasonal_2 Loss: 0.1719 | 0.1884
Epoch 4/300, seasonal_2 Loss: 0.1737 | 0.1348
Epoch 5/300, seasonal_2 Loss: 0.1489 | 0.1044
Epoch 6/300, seasonal_2 Loss: 0.1640 | 0.1038
Epoch 7/300, seasonal_2 Loss: 0.1417 | 0.0879
Epoch 8/300, seasonal_2 Loss: 0.1328 | 0.0819
Epoch 9/300, seasonal_2 Loss: 0.1205 | 0.0789
Epoch 10/300, seasonal_2 Loss: 0.1007 | 0.0707
Epoch 11/300, seasonal_2 Loss: 0.1002 | 0.0949
Epoch 12/300, seasonal_2 Loss: 0.1119 | 0.0935
Epoch 13/300, seasonal_2 Loss: 0.1044 | 0.0927
Epoch 14/300, seasonal_2 Loss: 0.1085 | 0.1227
Epoch 15/300, seasonal_2 Loss: 0.1261 | 0.1139
Epoch 16/300, seasonal_2 Loss: 0.1320 | 0.2230
Epoch 17/300, seasonal_2 Loss: 0.1968 | 0.5300
Epoch 18/300, seasonal_2 Loss: 0.1475 | 0.1574
Epoch 19/300, seasonal_2 Loss: 0.0977 | 0.0712
Epoch 20/300, seasonal_2 Loss: 0.1010 | 0.0739
Epoch 21/300, seasonal_2 Loss: 0.0909 | 0.0577
Epoch 22/300, seasonal_2 Loss: 0.0989 | 0.0644
Epoch 23/300, seasonal_2 Loss: 0.0949 | 0.0695
Epoch 24/300, seasonal_2 Loss: 0.0989 | 0.0792
Epoch 25/300, seasonal_2 Loss: 0.1101 | 0.0794
Epoch 26/300, seasonal_2 Loss: 0.1188 | 0.0651
Epoch 27/300, seasonal_2 Loss: 0.0798 | 0.0483
Epoch 28/300, seasonal_2 Loss: 0.0771 | 0.0467
Epoch 29/300, seasonal_2 Loss: 0.0782 | 0.0562
Epoch 30/300, seasonal_2 Loss: 0.0781 | 0.0690
Epoch 31/300, seasonal_2 Loss: 0.0770 | 0.0513
Epoch 32/300, seasonal_2 Loss: 0.0747 | 0.0441
Epoch 33/300, seasonal_2 Loss: 0.0820 | 0.0461
Epoch 34/300, seasonal_2 Loss: 0.0861 | 0.0926
Epoch 35/300, seasonal_2 Loss: 0.0855 | 0.0855
Epoch 36/300, seasonal_2 Loss: 0.0896 | 0.0456
Epoch 37/300, seasonal_2 Loss: 0.0787 | 0.0461
Epoch 38/300, seasonal_2 Loss: 0.0824 | 0.0487
Epoch 39/300, seasonal_2 Loss: 0.0804 | 0.0680
Epoch 40/300, seasonal_2 Loss: 0.0788 | 0.0594
Epoch 41/300, seasonal_2 Loss: 0.0788 | 0.0457
Epoch 42/300, seasonal_2 Loss: 0.0757 | 0.0420
Epoch 43/300, seasonal_2 Loss: 0.0811 | 0.0510
Epoch 44/300, seasonal_2 Loss: 0.0773 | 0.0491
Epoch 45/300, seasonal_2 Loss: 0.0702 | 0.0423
Epoch 46/300, seasonal_2 Loss: 0.0685 | 0.0413
Epoch 47/300, seasonal_2 Loss: 0.0697 | 0.0480
Epoch 48/300, seasonal_2 Loss: 0.0710 | 0.0455
Epoch 49/300, seasonal_2 Loss: 0.0736 | 0.0406
Epoch 50/300, seasonal_2 Loss: 0.0751 | 0.0412
Epoch 51/300, seasonal_2 Loss: 0.0696 | 0.0414
Epoch 52/300, seasonal_2 Loss: 0.0679 | 0.0474
Epoch 53/300, seasonal_2 Loss: 0.0711 | 0.0466
Epoch 54/300, seasonal_2 Loss: 0.0688 | 0.0396
Epoch 55/300, seasonal_2 Loss: 0.0687 | 0.0379
Epoch 56/300, seasonal_2 Loss: 0.0695 | 0.0384
Epoch 57/300, seasonal_2 Loss: 0.0690 | 0.0425
Epoch 58/300, seasonal_2 Loss: 0.0718 | 0.0432
Epoch 59/300, seasonal_2 Loss: 0.0697 | 0.0403
Epoch 60/300, seasonal_2 Loss: 0.0665 | 0.0380
Epoch 61/300, seasonal_2 Loss: 0.0681 | 0.0381
Epoch 62/300, seasonal_2 Loss: 0.0670 | 0.0374
Epoch 63/300, seasonal_2 Loss: 0.0667 | 0.0401
Epoch 64/300, seasonal_2 Loss: 0.0707 | 0.0434
Epoch 65/300, seasonal_2 Loss: 0.0722 | 0.0381
Epoch 66/300, seasonal_2 Loss: 0.0666 | 0.0366
Epoch 67/300, seasonal_2 Loss: 0.0671 | 0.0419
Epoch 68/300, seasonal_2 Loss: 0.0723 | 0.0392
Epoch 69/300, seasonal_2 Loss: 0.0731 | 0.0369
Epoch 70/300, seasonal_2 Loss: 0.0677 | 0.0367
Epoch 71/300, seasonal_2 Loss: 0.0636 | 0.0378
Epoch 72/300, seasonal_2 Loss: 0.0659 | 0.0395
Epoch 73/300, seasonal_2 Loss: 0.0707 | 0.0397
Epoch 74/300, seasonal_2 Loss: 0.0672 | 0.0398
Epoch 75/300, seasonal_2 Loss: 0.0640 | 0.0360
Epoch 76/300, seasonal_2 Loss: 0.0645 | 0.0357
Epoch 77/300, seasonal_2 Loss: 0.0661 | 0.0370
Epoch 78/300, seasonal_2 Loss: 0.0632 | 0.0369
Epoch 79/300, seasonal_2 Loss: 0.0600 | 0.0363
Epoch 80/300, seasonal_2 Loss: 0.0616 | 0.0356
Epoch 81/300, seasonal_2 Loss: 0.0602 | 0.0350
Epoch 82/300, seasonal_2 Loss: 0.0588 | 0.0341
Epoch 83/300, seasonal_2 Loss: 0.0588 | 0.0332
Epoch 84/300, seasonal_2 Loss: 0.0590 | 0.0341
Epoch 85/300, seasonal_2 Loss: 0.0589 | 0.0349
Epoch 86/300, seasonal_2 Loss: 0.0581 | 0.0342
Epoch 87/300, seasonal_2 Loss: 0.0577 | 0.0334
Epoch 88/300, seasonal_2 Loss: 0.0577 | 0.0327
Epoch 89/300, seasonal_2 Loss: 0.0575 | 0.0326
Epoch 90/300, seasonal_2 Loss: 0.0577 | 0.0322
Epoch 91/300, seasonal_2 Loss: 0.0577 | 0.0330
Epoch 92/300, seasonal_2 Loss: 0.0568 | 0.0329
Epoch 93/300, seasonal_2 Loss: 0.0565 | 0.0323
Epoch 94/300, seasonal_2 Loss: 0.0561 | 0.0320
Epoch 95/300, seasonal_2 Loss: 0.0560 | 0.0319
Epoch 96/300, seasonal_2 Loss: 0.0560 | 0.0317
Epoch 97/300, seasonal_2 Loss: 0.0559 | 0.0316
Epoch 98/300, seasonal_2 Loss: 0.0557 | 0.0316
Epoch 99/300, seasonal_2 Loss: 0.0556 | 0.0316
Epoch 100/300, seasonal_2 Loss: 0.0555 | 0.0315
Epoch 101/300, seasonal_2 Loss: 0.0555 | 0.0315
Epoch 102/300, seasonal_2 Loss: 0.0558 | 0.0318
Epoch 103/300, seasonal_2 Loss: 0.0563 | 0.0316
Epoch 104/300, seasonal_2 Loss: 0.0555 | 0.0313
Epoch 105/300, seasonal_2 Loss: 0.0552 | 0.0310
Epoch 106/300, seasonal_2 Loss: 0.0551 | 0.0307
Epoch 107/300, seasonal_2 Loss: 0.0555 | 0.0315
Epoch 108/300, seasonal_2 Loss: 0.0558 | 0.0305
Epoch 109/300, seasonal_2 Loss: 0.0552 | 0.0310
Epoch 110/300, seasonal_2 Loss: 0.0549 | 0.0313
Epoch 111/300, seasonal_2 Loss: 0.0552 | 0.0309
Epoch 112/300, seasonal_2 Loss: 0.0547 | 0.0306
Epoch 113/300, seasonal_2 Loss: 0.0546 | 0.0306
Epoch 114/300, seasonal_2 Loss: 0.0547 | 0.0305
Epoch 115/300, seasonal_2 Loss: 0.0549 | 0.0309
Epoch 116/300, seasonal_2 Loss: 0.0551 | 0.0303
Epoch 117/300, seasonal_2 Loss: 0.0544 | 0.0308
Epoch 118/300, seasonal_2 Loss: 0.0546 | 0.0309
Epoch 119/300, seasonal_2 Loss: 0.0544 | 0.0306
Epoch 120/300, seasonal_2 Loss: 0.0540 | 0.0304
Epoch 121/300, seasonal_2 Loss: 0.0540 | 0.0303
Epoch 122/300, seasonal_2 Loss: 0.0540 | 0.0303
Epoch 123/300, seasonal_2 Loss: 0.0541 | 0.0302
Epoch 124/300, seasonal_2 Loss: 0.0539 | 0.0303
Epoch 125/300, seasonal_2 Loss: 0.0537 | 0.0305
Epoch 126/300, seasonal_2 Loss: 0.0539 | 0.0305
Epoch 127/300, seasonal_2 Loss: 0.0538 | 0.0304
Epoch 128/300, seasonal_2 Loss: 0.0536 | 0.0301
Epoch 129/300, seasonal_2 Loss: 0.0535 | 0.0300
Epoch 130/300, seasonal_2 Loss: 0.0535 | 0.0300
Epoch 131/300, seasonal_2 Loss: 0.0536 | 0.0300
Epoch 132/300, seasonal_2 Loss: 0.0535 | 0.0300
Epoch 133/300, seasonal_2 Loss: 0.0533 | 0.0302
Epoch 134/300, seasonal_2 Loss: 0.0534 | 0.0303
Epoch 135/300, seasonal_2 Loss: 0.0534 | 0.0302
Epoch 136/300, seasonal_2 Loss: 0.0532 | 0.0301
Epoch 137/300, seasonal_2 Loss: 0.0531 | 0.0299
Epoch 138/300, seasonal_2 Loss: 0.0531 | 0.0299
Epoch 139/300, seasonal_2 Loss: 0.0531 | 0.0299
Epoch 140/300, seasonal_2 Loss: 0.0530 | 0.0299
Epoch 141/300, seasonal_2 Loss: 0.0529 | 0.0300
Epoch 142/300, seasonal_2 Loss: 0.0529 | 0.0300
Epoch 143/300, seasonal_2 Loss: 0.0529 | 0.0300
Epoch 144/300, seasonal_2 Loss: 0.0528 | 0.0299
Epoch 145/300, seasonal_2 Loss: 0.0528 | 0.0298
Epoch 146/300, seasonal_2 Loss: 0.0527 | 0.0298
Epoch 147/300, seasonal_2 Loss: 0.0527 | 0.0298
Epoch 148/300, seasonal_2 Loss: 0.0527 | 0.0298
Epoch 149/300, seasonal_2 Loss: 0.0526 | 0.0299
Epoch 150/300, seasonal_2 Loss: 0.0526 | 0.0299
Epoch 151/300, seasonal_2 Loss: 0.0526 | 0.0298
Epoch 152/300, seasonal_2 Loss: 0.0525 | 0.0298
Epoch 153/300, seasonal_2 Loss: 0.0525 | 0.0298
Epoch 154/300, seasonal_2 Loss: 0.0525 | 0.0298
Epoch 155/300, seasonal_2 Loss: 0.0524 | 0.0297
Epoch 156/300, seasonal_2 Loss: 0.0524 | 0.0298
Epoch 157/300, seasonal_2 Loss: 0.0524 | 0.0298
Epoch 158/300, seasonal_2 Loss: 0.0524 | 0.0297
Epoch 159/300, seasonal_2 Loss: 0.0523 | 0.0297
Epoch 160/300, seasonal_2 Loss: 0.0523 | 0.0297
Epoch 161/300, seasonal_2 Loss: 0.0523 | 0.0297
Epoch 162/300, seasonal_2 Loss: 0.0523 | 0.0297
Epoch 163/300, seasonal_2 Loss: 0.0522 | 0.0297
Epoch 164/300, seasonal_2 Loss: 0.0522 | 0.0297
Epoch 165/300, seasonal_2 Loss: 0.0522 | 0.0297
Epoch 166/300, seasonal_2 Loss: 0.0522 | 0.0297
Epoch 167/300, seasonal_2 Loss: 0.0522 | 0.0297
Epoch 168/300, seasonal_2 Loss: 0.0521 | 0.0297
Epoch 169/300, seasonal_2 Loss: 0.0521 | 0.0297
Epoch 170/300, seasonal_2 Loss: 0.0521 | 0.0296
Epoch 171/300, seasonal_2 Loss: 0.0521 | 0.0296
Epoch 172/300, seasonal_2 Loss: 0.0521 | 0.0296
Epoch 173/300, seasonal_2 Loss: 0.0520 | 0.0296
Epoch 174/300, seasonal_2 Loss: 0.0520 | 0.0296
Epoch 175/300, seasonal_2 Loss: 0.0520 | 0.0296
Epoch 176/300, seasonal_2 Loss: 0.0520 | 0.0296
Epoch 177/300, seasonal_2 Loss: 0.0520 | 0.0296
Epoch 178/300, seasonal_2 Loss: 0.0520 | 0.0296
Epoch 179/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 180/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 181/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 182/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 183/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 184/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 185/300, seasonal_2 Loss: 0.0519 | 0.0296
Epoch 186/300, seasonal_2 Loss: 0.0518 | 0.0296
Epoch 187/300, seasonal_2 Loss: 0.0518 | 0.0296
Epoch 188/300, seasonal_2 Loss: 0.0518 | 0.0296
Epoch 189/300, seasonal_2 Loss: 0.0518 | 0.0295
Epoch 190/300, seasonal_2 Loss: 0.0518 | 0.0295
Epoch 191/300, seasonal_2 Loss: 0.0518 | 0.0295
Epoch 192/300, seasonal_2 Loss: 0.0518 | 0.0295
Epoch 193/300, seasonal_2 Loss: 0.0518 | 0.0295
Epoch 194/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 195/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 196/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 197/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 198/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 199/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 200/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 201/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 202/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 203/300, seasonal_2 Loss: 0.0517 | 0.0295
Epoch 204/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 205/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 206/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 207/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 208/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 209/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 210/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 211/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 212/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 213/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 214/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 215/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 216/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 217/300, seasonal_2 Loss: 0.0516 | 0.0295
Epoch 218/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 219/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 220/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 221/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 222/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 223/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 224/300, seasonal_2 Loss: 0.0515 | 0.0295
Epoch 225/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 226/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 227/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 228/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 229/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 230/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 231/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 232/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 233/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 234/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 235/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 236/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 237/300, seasonal_2 Loss: 0.0515 | 0.0294
Epoch 238/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 239/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 240/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 241/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 242/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 243/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 244/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 245/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 246/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 247/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 248/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 249/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 250/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 251/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 252/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 253/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 254/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 255/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 256/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 257/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 258/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 259/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 260/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 261/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 262/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 263/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 264/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 265/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 266/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 267/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 268/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 269/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 270/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 271/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 272/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 273/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 274/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 275/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 276/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 277/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 278/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 279/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 280/300, seasonal_2 Loss: 0.0514 | 0.0294
Epoch 281/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 282/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 283/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 284/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 285/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 286/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 287/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 288/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 289/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 290/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 291/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 292/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 293/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 294/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 295/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 296/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 297/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 298/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 299/300, seasonal_2 Loss: 0.0513 | 0.0294
Epoch 300/300, seasonal_2 Loss: 0.0513 | 0.0294
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.8457425581149951, 'learning_rate': 2.6705678124906986e-05, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8781192026251187}
Epoch 1/300, seasonal_3 Loss: 0.3142 | 0.1061
Epoch 2/300, seasonal_3 Loss: 0.1211 | 0.0858
Epoch 3/300, seasonal_3 Loss: 0.1099 | 0.0669
Epoch 4/300, seasonal_3 Loss: 0.1044 | 0.0629
Epoch 5/300, seasonal_3 Loss: 0.1014 | 0.0574
Epoch 6/300, seasonal_3 Loss: 0.0987 | 0.0522
Epoch 7/300, seasonal_3 Loss: 0.0953 | 0.0489
Epoch 8/300, seasonal_3 Loss: 0.0962 | 0.0567
Epoch 9/300, seasonal_3 Loss: 0.0896 | 0.0453
Epoch 10/300, seasonal_3 Loss: 0.0942 | 0.0482
Epoch 11/300, seasonal_3 Loss: 0.0938 | 0.0428
Epoch 12/300, seasonal_3 Loss: 0.0997 | 0.0628
Epoch 13/300, seasonal_3 Loss: 0.0924 | 0.0448
Epoch 14/300, seasonal_3 Loss: 0.0956 | 0.0410
Epoch 15/300, seasonal_3 Loss: 0.0872 | 0.0421
Epoch 16/300, seasonal_3 Loss: 0.0843 | 0.0440
Epoch 17/300, seasonal_3 Loss: 0.0839 | 0.0461
Epoch 18/300, seasonal_3 Loss: 0.0872 | 0.0564
Epoch 19/300, seasonal_3 Loss: 0.0873 | 0.0478
Epoch 20/300, seasonal_3 Loss: 0.0815 | 0.0374
Epoch 21/300, seasonal_3 Loss: 0.0832 | 0.0470
Epoch 22/300, seasonal_3 Loss: 0.0805 | 0.0360
Epoch 23/300, seasonal_3 Loss: 0.0845 | 0.0437
Epoch 24/300, seasonal_3 Loss: 0.0798 | 0.0348
Epoch 25/300, seasonal_3 Loss: 0.0815 | 0.0383
Epoch 26/300, seasonal_3 Loss: 0.0777 | 0.0335
Epoch 27/300, seasonal_3 Loss: 0.0801 | 0.0357
Epoch 28/300, seasonal_3 Loss: 0.0767 | 0.0337
Epoch 29/300, seasonal_3 Loss: 0.0783 | 0.0348
Epoch 30/300, seasonal_3 Loss: 0.0769 | 0.0336
Epoch 31/300, seasonal_3 Loss: 0.0782 | 0.0350
Epoch 32/300, seasonal_3 Loss: 0.0775 | 0.0336
Epoch 33/300, seasonal_3 Loss: 0.0783 | 0.0343
Epoch 34/300, seasonal_3 Loss: 0.0791 | 0.0323
Epoch 35/300, seasonal_3 Loss: 0.0766 | 0.0324
Epoch 36/300, seasonal_3 Loss: 0.0807 | 0.0343
Epoch 37/300, seasonal_3 Loss: 0.0751 | 0.0332
Epoch 38/300, seasonal_3 Loss: 0.0796 | 0.0372
Epoch 39/300, seasonal_3 Loss: 0.0717 | 0.0336
Epoch 40/300, seasonal_3 Loss: 0.0806 | 0.0382
Epoch 41/300, seasonal_3 Loss: 0.0761 | 0.0343
Epoch 42/300, seasonal_3 Loss: 0.0760 | 0.0320
Epoch 43/300, seasonal_3 Loss: 0.0745 | 0.0315
Epoch 44/300, seasonal_3 Loss: 0.0755 | 0.0315
Epoch 45/300, seasonal_3 Loss: 0.0746 | 0.0323
Epoch 46/300, seasonal_3 Loss: 0.0739 | 0.0347
Epoch 47/300, seasonal_3 Loss: 0.0758 | 0.0433
Epoch 48/300, seasonal_3 Loss: 0.0767 | 0.0379
Epoch 49/300, seasonal_3 Loss: 0.0782 | 0.0337
Epoch 50/300, seasonal_3 Loss: 0.0781 | 0.0323
Epoch 51/300, seasonal_3 Loss: 0.0781 | 0.0333
Epoch 52/300, seasonal_3 Loss: 0.0758 | 0.0352
Epoch 53/300, seasonal_3 Loss: 0.0736 | 0.0402
Epoch 54/300, seasonal_3 Loss: 0.0734 | 0.0421
Epoch 55/300, seasonal_3 Loss: 0.0723 | 0.0389
Epoch 56/300, seasonal_3 Loss: 0.0711 | 0.0349
Epoch 57/300, seasonal_3 Loss: 0.0716 | 0.0347
Epoch 58/300, seasonal_3 Loss: 0.0739 | 0.0319
Epoch 59/300, seasonal_3 Loss: 0.0708 | 0.0307
Epoch 60/300, seasonal_3 Loss: 0.0671 | 0.0300
Epoch 61/300, seasonal_3 Loss: 0.0653 | 0.0297
Epoch 62/300, seasonal_3 Loss: 0.0648 | 0.0292
Epoch 63/300, seasonal_3 Loss: 0.0640 | 0.0284
Epoch 64/300, seasonal_3 Loss: 0.0642 | 0.0279
Epoch 65/300, seasonal_3 Loss: 0.0655 | 0.0283
Epoch 66/300, seasonal_3 Loss: 0.0664 | 0.0299
Epoch 67/300, seasonal_3 Loss: 0.0656 | 0.0302
Epoch 68/300, seasonal_3 Loss: 0.0640 | 0.0293
Epoch 69/300, seasonal_3 Loss: 0.0630 | 0.0287
Epoch 70/300, seasonal_3 Loss: 0.0628 | 0.0280
Epoch 71/300, seasonal_3 Loss: 0.0627 | 0.0274
Epoch 72/300, seasonal_3 Loss: 0.0625 | 0.0269
Epoch 73/300, seasonal_3 Loss: 0.0622 | 0.0265
Epoch 74/300, seasonal_3 Loss: 0.0622 | 0.0263
Epoch 75/300, seasonal_3 Loss: 0.0628 | 0.0264
Epoch 76/300, seasonal_3 Loss: 0.0635 | 0.0264
Epoch 77/300, seasonal_3 Loss: 0.0634 | 0.0266
Epoch 78/300, seasonal_3 Loss: 0.0628 | 0.0270
Epoch 79/300, seasonal_3 Loss: 0.0620 | 0.0280
Epoch 80/300, seasonal_3 Loss: 0.0615 | 0.0282
Epoch 81/300, seasonal_3 Loss: 0.0616 | 0.0273
Epoch 82/300, seasonal_3 Loss: 0.0616 | 0.0268
Epoch 83/300, seasonal_3 Loss: 0.0615 | 0.0275
Epoch 84/300, seasonal_3 Loss: 0.0614 | 0.0274
Epoch 85/300, seasonal_3 Loss: 0.0615 | 0.0263
Epoch 86/300, seasonal_3 Loss: 0.0618 | 0.0252
Epoch 87/300, seasonal_3 Loss: 0.0635 | 0.0258
Epoch 88/300, seasonal_3 Loss: 0.0631 | 0.0253
Epoch 89/300, seasonal_3 Loss: 0.0608 | 0.0256
Epoch 90/300, seasonal_3 Loss: 0.0601 | 0.0255
Epoch 91/300, seasonal_3 Loss: 0.0602 | 0.0253
Epoch 92/300, seasonal_3 Loss: 0.0602 | 0.0251
Epoch 93/300, seasonal_3 Loss: 0.0595 | 0.0259
Epoch 94/300, seasonal_3 Loss: 0.0589 | 0.0257
Epoch 95/300, seasonal_3 Loss: 0.0590 | 0.0254
Epoch 96/300, seasonal_3 Loss: 0.0594 | 0.0247
Epoch 97/300, seasonal_3 Loss: 0.0597 | 0.0241
Epoch 98/300, seasonal_3 Loss: 0.0595 | 0.0238
Epoch 99/300, seasonal_3 Loss: 0.0591 | 0.0244
Epoch 100/300, seasonal_3 Loss: 0.0588 | 0.0252
Epoch 101/300, seasonal_3 Loss: 0.0585 | 0.0246
Epoch 102/300, seasonal_3 Loss: 0.0589 | 0.0238
Epoch 103/300, seasonal_3 Loss: 0.0596 | 0.0238
Epoch 104/300, seasonal_3 Loss: 0.0593 | 0.0240
Epoch 105/300, seasonal_3 Loss: 0.0588 | 0.0243
Epoch 106/300, seasonal_3 Loss: 0.0582 | 0.0243
Epoch 107/300, seasonal_3 Loss: 0.0585 | 0.0239
Epoch 108/300, seasonal_3 Loss: 0.0594 | 0.0230
Epoch 109/300, seasonal_3 Loss: 0.0594 | 0.0229
Epoch 110/300, seasonal_3 Loss: 0.0588 | 0.0230
Epoch 111/300, seasonal_3 Loss: 0.0581 | 0.0235
Epoch 112/300, seasonal_3 Loss: 0.0579 | 0.0234
Epoch 113/300, seasonal_3 Loss: 0.0586 | 0.0228
Epoch 114/300, seasonal_3 Loss: 0.0610 | 0.0233
Epoch 115/300, seasonal_3 Loss: 0.0593 | 0.0226
Epoch 116/300, seasonal_3 Loss: 0.0572 | 0.0228
Epoch 117/300, seasonal_3 Loss: 0.0571 | 0.0225
Epoch 118/300, seasonal_3 Loss: 0.0571 | 0.0224
Epoch 119/300, seasonal_3 Loss: 0.0567 | 0.0221
Epoch 120/300, seasonal_3 Loss: 0.0563 | 0.0220
Epoch 121/300, seasonal_3 Loss: 0.0563 | 0.0219
Epoch 122/300, seasonal_3 Loss: 0.0565 | 0.0218
Epoch 123/300, seasonal_3 Loss: 0.0566 | 0.0219
Epoch 124/300, seasonal_3 Loss: 0.0564 | 0.0221
Epoch 125/300, seasonal_3 Loss: 0.0561 | 0.0222
Epoch 126/300, seasonal_3 Loss: 0.0560 | 0.0222
Epoch 127/300, seasonal_3 Loss: 0.0562 | 0.0220
Epoch 128/300, seasonal_3 Loss: 0.0561 | 0.0219
Epoch 129/300, seasonal_3 Loss: 0.0558 | 0.0219
Epoch 130/300, seasonal_3 Loss: 0.0557 | 0.0221
Epoch 131/300, seasonal_3 Loss: 0.0558 | 0.0221
Epoch 132/300, seasonal_3 Loss: 0.0559 | 0.0217
Epoch 133/300, seasonal_3 Loss: 0.0562 | 0.0221
Epoch 134/300, seasonal_3 Loss: 0.0564 | 0.0221
Epoch 135/300, seasonal_3 Loss: 0.0559 | 0.0219
Epoch 136/300, seasonal_3 Loss: 0.0558 | 0.0219
Epoch 137/300, seasonal_3 Loss: 0.0557 | 0.0217
Epoch 138/300, seasonal_3 Loss: 0.0555 | 0.0217
Epoch 139/300, seasonal_3 Loss: 0.0553 | 0.0216
Epoch 140/300, seasonal_3 Loss: 0.0552 | 0.0216
Epoch 141/300, seasonal_3 Loss: 0.0552 | 0.0215
Epoch 142/300, seasonal_3 Loss: 0.0553 | 0.0215
Epoch 143/300, seasonal_3 Loss: 0.0552 | 0.0218
Epoch 144/300, seasonal_3 Loss: 0.0551 | 0.0221
Epoch 145/300, seasonal_3 Loss: 0.0551 | 0.0218
Epoch 146/300, seasonal_3 Loss: 0.0552 | 0.0216
Epoch 147/300, seasonal_3 Loss: 0.0552 | 0.0215
Epoch 148/300, seasonal_3 Loss: 0.0550 | 0.0214
Epoch 149/300, seasonal_3 Loss: 0.0548 | 0.0214
Epoch 150/300, seasonal_3 Loss: 0.0548 | 0.0214
Epoch 151/300, seasonal_3 Loss: 0.0548 | 0.0213
Epoch 152/300, seasonal_3 Loss: 0.0547 | 0.0216
Epoch 153/300, seasonal_3 Loss: 0.0546 | 0.0218
Epoch 154/300, seasonal_3 Loss: 0.0545 | 0.0217
Epoch 155/300, seasonal_3 Loss: 0.0545 | 0.0215
Epoch 156/300, seasonal_3 Loss: 0.0546 | 0.0213
Epoch 157/300, seasonal_3 Loss: 0.0545 | 0.0213
Epoch 158/300, seasonal_3 Loss: 0.0544 | 0.0212
Epoch 159/300, seasonal_3 Loss: 0.0543 | 0.0212
Epoch 160/300, seasonal_3 Loss: 0.0543 | 0.0212
Epoch 161/300, seasonal_3 Loss: 0.0542 | 0.0212
Epoch 162/300, seasonal_3 Loss: 0.0542 | 0.0214
Epoch 163/300, seasonal_3 Loss: 0.0541 | 0.0214
Epoch 164/300, seasonal_3 Loss: 0.0540 | 0.0213
Epoch 165/300, seasonal_3 Loss: 0.0540 | 0.0212
Epoch 166/300, seasonal_3 Loss: 0.0541 | 0.0211
Epoch 167/300, seasonal_3 Loss: 0.0540 | 0.0211
Epoch 168/300, seasonal_3 Loss: 0.0539 | 0.0211
Epoch 169/300, seasonal_3 Loss: 0.0539 | 0.0210
Epoch 170/300, seasonal_3 Loss: 0.0538 | 0.0211
Epoch 171/300, seasonal_3 Loss: 0.0538 | 0.0212
Epoch 172/300, seasonal_3 Loss: 0.0538 | 0.0212
Epoch 173/300, seasonal_3 Loss: 0.0537 | 0.0211
Epoch 174/300, seasonal_3 Loss: 0.0537 | 0.0211
Epoch 175/300, seasonal_3 Loss: 0.0537 | 0.0210
Epoch 176/300, seasonal_3 Loss: 0.0537 | 0.0210
Epoch 177/300, seasonal_3 Loss: 0.0536 | 0.0210
Epoch 178/300, seasonal_3 Loss: 0.0536 | 0.0210
Epoch 179/300, seasonal_3 Loss: 0.0535 | 0.0210
Epoch 180/300, seasonal_3 Loss: 0.0535 | 0.0210
Epoch 181/300, seasonal_3 Loss: 0.0535 | 0.0210
Epoch 182/300, seasonal_3 Loss: 0.0534 | 0.0210
Epoch 183/300, seasonal_3 Loss: 0.0534 | 0.0210
Epoch 184/300, seasonal_3 Loss: 0.0534 | 0.0209
Epoch 185/300, seasonal_3 Loss: 0.0534 | 0.0209
Epoch 186/300, seasonal_3 Loss: 0.0534 | 0.0209
Epoch 187/300, seasonal_3 Loss: 0.0533 | 0.0209
Epoch 188/300, seasonal_3 Loss: 0.0533 | 0.0209
Epoch 189/300, seasonal_3 Loss: 0.0533 | 0.0209
Epoch 190/300, seasonal_3 Loss: 0.0533 | 0.0209
Epoch 191/300, seasonal_3 Loss: 0.0532 | 0.0209
Epoch 192/300, seasonal_3 Loss: 0.0532 | 0.0209
Epoch 193/300, seasonal_3 Loss: 0.0532 | 0.0209
Epoch 194/300, seasonal_3 Loss: 0.0532 | 0.0209
Epoch 195/300, seasonal_3 Loss: 0.0532 | 0.0209
Epoch 196/300, seasonal_3 Loss: 0.0531 | 0.0209
Epoch 197/300, seasonal_3 Loss: 0.0531 | 0.0209
Epoch 198/300, seasonal_3 Loss: 0.0531 | 0.0209
Epoch 199/300, seasonal_3 Loss: 0.0531 | 0.0209
Epoch 200/300, seasonal_3 Loss: 0.0531 | 0.0208
Epoch 201/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 202/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 203/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 204/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 205/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 206/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 207/300, seasonal_3 Loss: 0.0530 | 0.0208
Epoch 208/300, seasonal_3 Loss: 0.0529 | 0.0208
Epoch 209/300, seasonal_3 Loss: 0.0529 | 0.0208
Epoch 210/300, seasonal_3 Loss: 0.0529 | 0.0208
Epoch 211/300, seasonal_3 Loss: 0.0529 | 0.0208
Epoch 212/300, seasonal_3 Loss: 0.0529 | 0.0208
Epoch 213/300, seasonal_3 Loss: 0.0529 | 0.0208
Epoch 214/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 215/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 216/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 217/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 218/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 219/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 220/300, seasonal_3 Loss: 0.0528 | 0.0208
Epoch 221/300, seasonal_3 Loss: 0.0527 | 0.0208
Epoch 222/300, seasonal_3 Loss: 0.0527 | 0.0208
Epoch 223/300, seasonal_3 Loss: 0.0527 | 0.0208
Epoch 224/300, seasonal_3 Loss: 0.0527 | 0.0208
Epoch 225/300, seasonal_3 Loss: 0.0527 | 0.0208
Epoch 226/300, seasonal_3 Loss: 0.0527 | 0.0208
Epoch 227/300, seasonal_3 Loss: 0.0527 | 0.0207
Epoch 228/300, seasonal_3 Loss: 0.0527 | 0.0207
Epoch 229/300, seasonal_3 Loss: 0.0527 | 0.0207
Epoch 230/300, seasonal_3 Loss: 0.0527 | 0.0207
Epoch 231/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 232/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 233/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 234/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 235/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 236/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 237/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 238/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 239/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 240/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 241/300, seasonal_3 Loss: 0.0526 | 0.0207
Epoch 242/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 243/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 244/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 245/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 246/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 247/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 248/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 249/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 250/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 251/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 252/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 253/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 254/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 255/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 256/300, seasonal_3 Loss: 0.0525 | 0.0207
Epoch 257/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 258/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 259/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 260/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 261/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 262/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 263/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 264/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 265/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 266/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 267/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 268/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 269/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 270/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 271/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 272/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 273/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 274/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 275/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 276/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 277/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 278/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 279/300, seasonal_3 Loss: 0.0524 | 0.0207
Epoch 280/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 281/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 282/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 283/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 284/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 285/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 286/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 287/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 288/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 289/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 290/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 291/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 292/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 293/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 294/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 295/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 296/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 297/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 298/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 299/300, seasonal_3 Loss: 0.0523 | 0.0207
Epoch 300/300, seasonal_3 Loss: 0.0523 | 0.0207
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8886684749169751, 'learning_rate': 9.981037655905798e-06, 'batch_size': 42, 'step_size': 12, 'gamma': 0.839522011664329}
Epoch 1/300, resid Loss: 0.1893 | 0.1206
Epoch 2/300, resid Loss: 0.1233 | 0.0862
Epoch 3/300, resid Loss: 0.1085 | 0.0765
Epoch 4/300, resid Loss: 0.1028 | 0.0708
Epoch 5/300, resid Loss: 0.1010 | 0.0662
Epoch 6/300, resid Loss: 0.0992 | 0.0630
Epoch 7/300, resid Loss: 0.0979 | 0.0673
Epoch 8/300, resid Loss: 0.0983 | 0.0625
Epoch 9/300, resid Loss: 0.0955 | 0.0606
Epoch 10/300, resid Loss: 0.0933 | 0.0588
Epoch 11/300, resid Loss: 0.0905 | 0.0562
Epoch 12/300, resid Loss: 0.0877 | 0.0536
Epoch 13/300, resid Loss: 0.0855 | 0.0550
Epoch 14/300, resid Loss: 0.0850 | 0.0602
Epoch 15/300, resid Loss: 0.0858 | 0.0553
Epoch 16/300, resid Loss: 0.0846 | 0.0493
Epoch 17/300, resid Loss: 0.0829 | 0.0463
Epoch 18/300, resid Loss: 0.0811 | 0.0444
Epoch 19/300, resid Loss: 0.0798 | 0.0430
Epoch 20/300, resid Loss: 0.0786 | 0.0412
Epoch 21/300, resid Loss: 0.0778 | 0.0396
Epoch 22/300, resid Loss: 0.0766 | 0.0384
Epoch 23/300, resid Loss: 0.0758 | 0.0377
Epoch 24/300, resid Loss: 0.0750 | 0.0371
Epoch 25/300, resid Loss: 0.0744 | 0.0369
Epoch 26/300, resid Loss: 0.0740 | 0.0365
Epoch 27/300, resid Loss: 0.0736 | 0.0363
Epoch 28/300, resid Loss: 0.0731 | 0.0363
Epoch 29/300, resid Loss: 0.0727 | 0.0363
Epoch 30/300, resid Loss: 0.0723 | 0.0366
Epoch 31/300, resid Loss: 0.0723 | 0.0371
Epoch 32/300, resid Loss: 0.0722 | 0.0373
Epoch 33/300, resid Loss: 0.0719 | 0.0378
Epoch 34/300, resid Loss: 0.0718 | 0.0382
Epoch 35/300, resid Loss: 0.0716 | 0.0383
Epoch 36/300, resid Loss: 0.0713 | 0.0383
Epoch 37/300, resid Loss: 0.0710 | 0.0358
Epoch 38/300, resid Loss: 0.0710 | 0.0359
Epoch 39/300, resid Loss: 0.0711 | 0.0358
Epoch 40/300, resid Loss: 0.0710 | 0.0353
Epoch 41/300, resid Loss: 0.0711 | 0.0348
Epoch 42/300, resid Loss: 0.0712 | 0.0343
Epoch 43/300, resid Loss: 0.0709 | 0.0353
Epoch 44/300, resid Loss: 0.0711 | 0.0358
Epoch 45/300, resid Loss: 0.0706 | 0.0359
Epoch 46/300, resid Loss: 0.0703 | 0.0353
Epoch 47/300, resid Loss: 0.0701 | 0.0347
Epoch 48/300, resid Loss: 0.0698 | 0.0341
Epoch 49/300, resid Loss: 0.0696 | 0.0338
Epoch 50/300, resid Loss: 0.0697 | 0.0336
Epoch 51/300, resid Loss: 0.0699 | 0.0334
Epoch 52/300, resid Loss: 0.0696 | 0.0332
Epoch 53/300, resid Loss: 0.0692 | 0.0331
Epoch 54/300, resid Loss: 0.0687 | 0.0329
Epoch 55/300, resid Loss: 0.0683 | 0.0335
Epoch 56/300, resid Loss: 0.0680 | 0.0334
Epoch 57/300, resid Loss: 0.0676 | 0.0332
Epoch 58/300, resid Loss: 0.0673 | 0.0330
Epoch 59/300, resid Loss: 0.0671 | 0.0328
Epoch 60/300, resid Loss: 0.0669 | 0.0327
Epoch 61/300, resid Loss: 0.0666 | 0.0327
Epoch 62/300, resid Loss: 0.0665 | 0.0325
Epoch 63/300, resid Loss: 0.0663 | 0.0324
Epoch 64/300, resid Loss: 0.0661 | 0.0323
Epoch 65/300, resid Loss: 0.0660 | 0.0323
Epoch 66/300, resid Loss: 0.0659 | 0.0324
Epoch 67/300, resid Loss: 0.0658 | 0.0321
Epoch 68/300, resid Loss: 0.0658 | 0.0321
Epoch 69/300, resid Loss: 0.0658 | 0.0321
Epoch 70/300, resid Loss: 0.0657 | 0.0322
Epoch 71/300, resid Loss: 0.0657 | 0.0322
Epoch 72/300, resid Loss: 0.0656 | 0.0322
Epoch 73/300, resid Loss: 0.0656 | 0.0318
Epoch 74/300, resid Loss: 0.0655 | 0.0318
Epoch 75/300, resid Loss: 0.0654 | 0.0317
Epoch 76/300, resid Loss: 0.0653 | 0.0316
Epoch 77/300, resid Loss: 0.0652 | 0.0316
Epoch 78/300, resid Loss: 0.0650 | 0.0315
Epoch 79/300, resid Loss: 0.0649 | 0.0315
Epoch 80/300, resid Loss: 0.0648 | 0.0314
Epoch 81/300, resid Loss: 0.0647 | 0.0313
Epoch 82/300, resid Loss: 0.0646 | 0.0313
Epoch 83/300, resid Loss: 0.0646 | 0.0312
Epoch 84/300, resid Loss: 0.0645 | 0.0312
Epoch 85/300, resid Loss: 0.0644 | 0.0316
Epoch 86/300, resid Loss: 0.0644 | 0.0315
Epoch 87/300, resid Loss: 0.0643 | 0.0314
Epoch 88/300, resid Loss: 0.0642 | 0.0313
Epoch 89/300, resid Loss: 0.0640 | 0.0312
Epoch 90/300, resid Loss: 0.0639 | 0.0310
Epoch 91/300, resid Loss: 0.0637 | 0.0309
Epoch 92/300, resid Loss: 0.0636 | 0.0308
Epoch 93/300, resid Loss: 0.0635 | 0.0307
Epoch 94/300, resid Loss: 0.0634 | 0.0306
Epoch 95/300, resid Loss: 0.0633 | 0.0306
Epoch 96/300, resid Loss: 0.0632 | 0.0306
Epoch 97/300, resid Loss: 0.0632 | 0.0305
Epoch 98/300, resid Loss: 0.0631 | 0.0305
Epoch 99/300, resid Loss: 0.0631 | 0.0305
Epoch 100/300, resid Loss: 0.0631 | 0.0305
Epoch 101/300, resid Loss: 0.0630 | 0.0304
Epoch 102/300, resid Loss: 0.0630 | 0.0304
Epoch 103/300, resid Loss: 0.0629 | 0.0303
Epoch 104/300, resid Loss: 0.0628 | 0.0303
Epoch 105/300, resid Loss: 0.0627 | 0.0302
Epoch 106/300, resid Loss: 0.0627 | 0.0302
Epoch 107/300, resid Loss: 0.0626 | 0.0301
Epoch 108/300, resid Loss: 0.0625 | 0.0301
Epoch 109/300, resid Loss: 0.0625 | 0.0300
Epoch 110/300, resid Loss: 0.0624 | 0.0300
Epoch 111/300, resid Loss: 0.0624 | 0.0300
Epoch 112/300, resid Loss: 0.0623 | 0.0299
Epoch 113/300, resid Loss: 0.0623 | 0.0299
Epoch 114/300, resid Loss: 0.0623 | 0.0299
Epoch 115/300, resid Loss: 0.0622 | 0.0299
Epoch 116/300, resid Loss: 0.0622 | 0.0298
Epoch 117/300, resid Loss: 0.0621 | 0.0298
Epoch 118/300, resid Loss: 0.0621 | 0.0298
Epoch 119/300, resid Loss: 0.0621 | 0.0297
Epoch 120/300, resid Loss: 0.0620 | 0.0297
Epoch 121/300, resid Loss: 0.0620 | 0.0297
Epoch 122/300, resid Loss: 0.0620 | 0.0297
Epoch 123/300, resid Loss: 0.0619 | 0.0296
Epoch 124/300, resid Loss: 0.0619 | 0.0296
Epoch 125/300, resid Loss: 0.0619 | 0.0296
Epoch 126/300, resid Loss: 0.0618 | 0.0296
Epoch 127/300, resid Loss: 0.0618 | 0.0296
Epoch 128/300, resid Loss: 0.0618 | 0.0295
Epoch 129/300, resid Loss: 0.0618 | 0.0295
Epoch 130/300, resid Loss: 0.0617 | 0.0295
Epoch 131/300, resid Loss: 0.0617 | 0.0295
Epoch 132/300, resid Loss: 0.0617 | 0.0294
Epoch 133/300, resid Loss: 0.0617 | 0.0294
Epoch 134/300, resid Loss: 0.0616 | 0.0294
Epoch 135/300, resid Loss: 0.0616 | 0.0294
Epoch 136/300, resid Loss: 0.0616 | 0.0294
Epoch 137/300, resid Loss: 0.0616 | 0.0294
Epoch 138/300, resid Loss: 0.0615 | 0.0293
Epoch 139/300, resid Loss: 0.0615 | 0.0293
Epoch 140/300, resid Loss: 0.0615 | 0.0293
Epoch 141/300, resid Loss: 0.0615 | 0.0293
Epoch 142/300, resid Loss: 0.0615 | 0.0293
Epoch 143/300, resid Loss: 0.0614 | 0.0293
Epoch 144/300, resid Loss: 0.0614 | 0.0292
Epoch 145/300, resid Loss: 0.0614 | 0.0292
Epoch 146/300, resid Loss: 0.0614 | 0.0292
Epoch 147/300, resid Loss: 0.0614 | 0.0292
Epoch 148/300, resid Loss: 0.0614 | 0.0292
Epoch 149/300, resid Loss: 0.0613 | 0.0292
Epoch 150/300, resid Loss: 0.0613 | 0.0292
Epoch 151/300, resid Loss: 0.0613 | 0.0292
Epoch 152/300, resid Loss: 0.0613 | 0.0291
Epoch 153/300, resid Loss: 0.0613 | 0.0291
Epoch 154/300, resid Loss: 0.0613 | 0.0291
Epoch 155/300, resid Loss: 0.0612 | 0.0291
Epoch 156/300, resid Loss: 0.0612 | 0.0291
Epoch 157/300, resid Loss: 0.0612 | 0.0291
Epoch 158/300, resid Loss: 0.0612 | 0.0291
Epoch 159/300, resid Loss: 0.0612 | 0.0291
Epoch 160/300, resid Loss: 0.0612 | 0.0291
Epoch 161/300, resid Loss: 0.0612 | 0.0290
Epoch 162/300, resid Loss: 0.0612 | 0.0290
Epoch 163/300, resid Loss: 0.0611 | 0.0290
Epoch 164/300, resid Loss: 0.0611 | 0.0290
Epoch 165/300, resid Loss: 0.0611 | 0.0290
Epoch 166/300, resid Loss: 0.0611 | 0.0290
Epoch 167/300, resid Loss: 0.0611 | 0.0290
Epoch 168/300, resid Loss: 0.0611 | 0.0290
Epoch 169/300, resid Loss: 0.0611 | 0.0290
Epoch 170/300, resid Loss: 0.0611 | 0.0290
Epoch 171/300, resid Loss: 0.0611 | 0.0290
Epoch 172/300, resid Loss: 0.0611 | 0.0290
Epoch 173/300, resid Loss: 0.0611 | 0.0290
Epoch 174/300, resid Loss: 0.0610 | 0.0290
Epoch 175/300, resid Loss: 0.0610 | 0.0289
Epoch 176/300, resid Loss: 0.0610 | 0.0289
Epoch 177/300, resid Loss: 0.0610 | 0.0289
Epoch 178/300, resid Loss: 0.0610 | 0.0289
Epoch 179/300, resid Loss: 0.0610 | 0.0289
Epoch 180/300, resid Loss: 0.0610 | 0.0289
Epoch 181/300, resid Loss: 0.0610 | 0.0289
Epoch 182/300, resid Loss: 0.0610 | 0.0289
Epoch 183/300, resid Loss: 0.0610 | 0.0289
Epoch 184/300, resid Loss: 0.0610 | 0.0289
Epoch 185/300, resid Loss: 0.0610 | 0.0289
Epoch 186/300, resid Loss: 0.0610 | 0.0289
Epoch 187/300, resid Loss: 0.0610 | 0.0289
Epoch 188/300, resid Loss: 0.0610 | 0.0289
Epoch 189/300, resid Loss: 0.0610 | 0.0289
Epoch 190/300, resid Loss: 0.0609 | 0.0289
Epoch 191/300, resid Loss: 0.0609 | 0.0289
Epoch 192/300, resid Loss: 0.0609 | 0.0289
Epoch 193/300, resid Loss: 0.0609 | 0.0289
Epoch 194/300, resid Loss: 0.0609 | 0.0289
Epoch 195/300, resid Loss: 0.0609 | 0.0289
Epoch 196/300, resid Loss: 0.0609 | 0.0289
Epoch 197/300, resid Loss: 0.0609 | 0.0289
Epoch 198/300, resid Loss: 0.0609 | 0.0289
Epoch 199/300, resid Loss: 0.0609 | 0.0289
Epoch 200/300, resid Loss: 0.0609 | 0.0289
Epoch 201/300, resid Loss: 0.0609 | 0.0288
Epoch 202/300, resid Loss: 0.0609 | 0.0288
Epoch 203/300, resid Loss: 0.0609 | 0.0288
Epoch 204/300, resid Loss: 0.0609 | 0.0288
Epoch 205/300, resid Loss: 0.0609 | 0.0288
Epoch 206/300, resid Loss: 0.0609 | 0.0288
Epoch 207/300, resid Loss: 0.0609 | 0.0288
Epoch 208/300, resid Loss: 0.0609 | 0.0288
Epoch 209/300, resid Loss: 0.0609 | 0.0288
Epoch 210/300, resid Loss: 0.0609 | 0.0288
Epoch 211/300, resid Loss: 0.0609 | 0.0288
Epoch 212/300, resid Loss: 0.0609 | 0.0288
Epoch 213/300, resid Loss: 0.0609 | 0.0288
Epoch 214/300, resid Loss: 0.0609 | 0.0288
Epoch 215/300, resid Loss: 0.0609 | 0.0288
Epoch 216/300, resid Loss: 0.0609 | 0.0288
Epoch 217/300, resid Loss: 0.0609 | 0.0288
Epoch 218/300, resid Loss: 0.0609 | 0.0288
Epoch 219/300, resid Loss: 0.0609 | 0.0288
Epoch 220/300, resid Loss: 0.0609 | 0.0288
Epoch 221/300, resid Loss: 0.0609 | 0.0288
Epoch 222/300, resid Loss: 0.0609 | 0.0288
Epoch 223/300, resid Loss: 0.0608 | 0.0288
Epoch 224/300, resid Loss: 0.0608 | 0.0288
Epoch 225/300, resid Loss: 0.0608 | 0.0288
Epoch 226/300, resid Loss: 0.0608 | 0.0288
Epoch 227/300, resid Loss: 0.0608 | 0.0288
Epoch 228/300, resid Loss: 0.0608 | 0.0288
Epoch 229/300, resid Loss: 0.0608 | 0.0288
Epoch 230/300, resid Loss: 0.0608 | 0.0288
Epoch 231/300, resid Loss: 0.0608 | 0.0288
Epoch 232/300, resid Loss: 0.0608 | 0.0288
Epoch 233/300, resid Loss: 0.0608 | 0.0288
Epoch 234/300, resid Loss: 0.0608 | 0.0288
Epoch 235/300, resid Loss: 0.0608 | 0.0288
Epoch 236/300, resid Loss: 0.0608 | 0.0288
Epoch 237/300, resid Loss: 0.0608 | 0.0288
Epoch 238/300, resid Loss: 0.0608 | 0.0288
Epoch 239/300, resid Loss: 0.0608 | 0.0288
Epoch 240/300, resid Loss: 0.0608 | 0.0288
Epoch 241/300, resid Loss: 0.0608 | 0.0288
Epoch 242/300, resid Loss: 0.0608 | 0.0288
Epoch 243/300, resid Loss: 0.0608 | 0.0288
Epoch 244/300, resid Loss: 0.0608 | 0.0288
Epoch 245/300, resid Loss: 0.0608 | 0.0288
Epoch 246/300, resid Loss: 0.0608 | 0.0288
Epoch 247/300, resid Loss: 0.0608 | 0.0288
Epoch 248/300, resid Loss: 0.0608 | 0.0288
Epoch 249/300, resid Loss: 0.0608 | 0.0288
Epoch 250/300, resid Loss: 0.0608 | 0.0288
Epoch 251/300, resid Loss: 0.0608 | 0.0288
Epoch 252/300, resid Loss: 0.0608 | 0.0288
Epoch 253/300, resid Loss: 0.0608 | 0.0288
Epoch 254/300, resid Loss: 0.0608 | 0.0288
Epoch 255/300, resid Loss: 0.0608 | 0.0288
Epoch 256/300, resid Loss: 0.0608 | 0.0288
Epoch 257/300, resid Loss: 0.0608 | 0.0288
Epoch 258/300, resid Loss: 0.0608 | 0.0288
Epoch 259/300, resid Loss: 0.0608 | 0.0288
Epoch 260/300, resid Loss: 0.0608 | 0.0288
Epoch 261/300, resid Loss: 0.0608 | 0.0288
Epoch 262/300, resid Loss: 0.0608 | 0.0288
Epoch 263/300, resid Loss: 0.0608 | 0.0288
Epoch 264/300, resid Loss: 0.0608 | 0.0288
Epoch 265/300, resid Loss: 0.0608 | 0.0288
Epoch 266/300, resid Loss: 0.0608 | 0.0288
Epoch 267/300, resid Loss: 0.0608 | 0.0288
Epoch 268/300, resid Loss: 0.0608 | 0.0288
Epoch 269/300, resid Loss: 0.0608 | 0.0288
Epoch 270/300, resid Loss: 0.0608 | 0.0288
Epoch 271/300, resid Loss: 0.0608 | 0.0288
Epoch 272/300, resid Loss: 0.0608 | 0.0288
Epoch 273/300, resid Loss: 0.0608 | 0.0288
Epoch 274/300, resid Loss: 0.0608 | 0.0288
Epoch 275/300, resid Loss: 0.0608 | 0.0288
Epoch 276/300, resid Loss: 0.0608 | 0.0288
Epoch 277/300, resid Loss: 0.0608 | 0.0288
Epoch 278/300, resid Loss: 0.0608 | 0.0288
Epoch 279/300, resid Loss: 0.0608 | 0.0288
Epoch 280/300, resid Loss: 0.0608 | 0.0288
Epoch 281/300, resid Loss: 0.0608 | 0.0288
Epoch 282/300, resid Loss: 0.0608 | 0.0288
Epoch 283/300, resid Loss: 0.0608 | 0.0288
Epoch 284/300, resid Loss: 0.0608 | 0.0288
Epoch 285/300, resid Loss: 0.0608 | 0.0288
Epoch 286/300, resid Loss: 0.0608 | 0.0288
Epoch 287/300, resid Loss: 0.0608 | 0.0288
Epoch 288/300, resid Loss: 0.0608 | 0.0288
Epoch 289/300, resid Loss: 0.0608 | 0.0288
Epoch 290/300, resid Loss: 0.0608 | 0.0288
Epoch 291/300, resid Loss: 0.0608 | 0.0288
Epoch 292/300, resid Loss: 0.0608 | 0.0288
Epoch 293/300, resid Loss: 0.0608 | 0.0288
Epoch 294/300, resid Loss: 0.0608 | 0.0288
Epoch 295/300, resid Loss: 0.0608 | 0.0288
Epoch 296/300, resid Loss: 0.0608 | 0.0288
Epoch 297/300, resid Loss: 0.0608 | 0.0288
Epoch 298/300, resid Loss: 0.0608 | 0.0288
Epoch 299/300, resid Loss: 0.0608 | 0.0288
Epoch 300/300, resid Loss: 0.0608 | 0.0288
Runtime (seconds): 6600.260457754135
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[216.04756]
[1.4527943]
[0.92100215]
[4.597019]
[-0.18542594]
[6.5103884]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 12.43731033266522
RMSE: 3.5266571044921875
MAE: 3.5266571044921875
R-squared: nan
[229.34334]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
