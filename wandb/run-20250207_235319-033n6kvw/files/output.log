ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-07 23:53:25,231][0m A new study created in memory with name: no-name-c3e8add0-27eb-41d8-8194-cdcbe30261c1[0m
[32m[I 2025-02-07 23:53:53,029][0m Trial 0 finished with value: 0.14967329800128937 and parameters: {'observation_period_num': 83, 'train_rates': 0.9888426239773056, 'learning_rate': 0.0001422073313788437, 'batch_size': 237, 'step_size': 11, 'gamma': 0.8246186348378314}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:54:21,546][0m Trial 1 finished with value: 0.9052247967450849 and parameters: {'observation_period_num': 224, 'train_rates': 0.7736985899168918, 'learning_rate': 2.8612760374098016e-06, 'batch_size': 181, 'step_size': 9, 'gamma': 0.7514408900666378}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:55:04,226][0m Trial 2 finished with value: 0.2699182124255778 and parameters: {'observation_period_num': 202, 'train_rates': 0.8353353944278936, 'learning_rate': 9.173446813674371e-05, 'batch_size': 123, 'step_size': 4, 'gamma': 0.7820824030904472}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:55:31,725][0m Trial 3 finished with value: 0.9283502697944641 and parameters: {'observation_period_num': 107, 'train_rates': 0.969820284428922, 'learning_rate': 3.352441126847901e-06, 'batch_size': 242, 'step_size': 14, 'gamma': 0.8203181413447025}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:56:45,200][0m Trial 4 finished with value: 0.2616787299849704 and parameters: {'observation_period_num': 71, 'train_rates': 0.662457800489642, 'learning_rate': 1.4204727138385567e-05, 'batch_size': 63, 'step_size': 1, 'gamma': 0.9824928138086747}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:57:42,533][0m Trial 5 finished with value: 0.5540038047552999 and parameters: {'observation_period_num': 154, 'train_rates': 0.8091084759685543, 'learning_rate': 3.64457127987051e-06, 'batch_size': 91, 'step_size': 3, 'gamma': 0.9517793410736495}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:58:06,571][0m Trial 6 finished with value: 0.4786951124233051 and parameters: {'observation_period_num': 15, 'train_rates': 0.8141178688633588, 'learning_rate': 1.8023153406480243e-06, 'batch_size': 256, 'step_size': 10, 'gamma': 0.9734616335220927}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:59:26,272][0m Trial 7 finished with value: 0.45485701835990167 and parameters: {'observation_period_num': 164, 'train_rates': 0.8008843504163111, 'learning_rate': 1.978908046983445e-06, 'batch_size': 63, 'step_size': 13, 'gamma': 0.9867671497193311}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-07 23:59:53,149][0m Trial 8 finished with value: 0.24902097296745626 and parameters: {'observation_period_num': 52, 'train_rates': 0.7343331480753685, 'learning_rate': 0.0005278690011757914, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9127576655898715}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-08 00:00:43,379][0m Trial 9 finished with value: 0.3558778301272053 and parameters: {'observation_period_num': 202, 'train_rates': 0.9032049872110424, 'learning_rate': 1.3452762517034644e-05, 'batch_size': 113, 'step_size': 10, 'gamma': 0.7948613553575036}. Best is trial 0 with value: 0.14967329800128937.[0m
[32m[I 2025-02-08 00:01:18,955][0m Trial 10 finished with value: 0.10993567854166031 and parameters: {'observation_period_num': 111, 'train_rates': 0.9848560902547057, 'learning_rate': 0.0009889079584215392, 'batch_size': 178, 'step_size': 6, 'gamma': 0.8594536441072118}. Best is trial 10 with value: 0.10993567854166031.[0m
[32m[I 2025-02-08 00:01:54,467][0m Trial 11 finished with value: 0.12891066074371338 and parameters: {'observation_period_num': 105, 'train_rates': 0.9827323321761927, 'learning_rate': 0.0008892623977580874, 'batch_size': 185, 'step_size': 6, 'gamma': 0.8626553653432021}. Best is trial 10 with value: 0.10993567854166031.[0m
[32m[I 2025-02-08 00:02:30,273][0m Trial 12 finished with value: 0.09374771472588878 and parameters: {'observation_period_num': 128, 'train_rates': 0.9244382085652728, 'learning_rate': 0.0007871653865201139, 'batch_size': 167, 'step_size': 6, 'gamma': 0.8706334040776967}. Best is trial 12 with value: 0.09374771472588878.[0m
[32m[I 2025-02-08 00:03:05,964][0m Trial 13 finished with value: 0.12394517039259274 and parameters: {'observation_period_num': 149, 'train_rates': 0.8977319841945226, 'learning_rate': 0.0002752362961505762, 'batch_size': 159, 'step_size': 7, 'gamma': 0.8830057079222854}. Best is trial 12 with value: 0.09374771472588878.[0m
[32m[I 2025-02-08 00:03:44,373][0m Trial 14 finished with value: 0.15910141587257384 and parameters: {'observation_period_num': 116, 'train_rates': 0.9121336886640767, 'learning_rate': 6.092297554280988e-05, 'batch_size': 159, 'step_size': 6, 'gamma': 0.8677019899450036}. Best is trial 12 with value: 0.09374771472588878.[0m
[32m[I 2025-02-08 00:08:13,988][0m Trial 15 finished with value: 0.045832625230153405 and parameters: {'observation_period_num': 34, 'train_rates': 0.9234013553516431, 'learning_rate': 0.0003181720993454402, 'batch_size': 21, 'step_size': 4, 'gamma': 0.9102909710302616}. Best is trial 15 with value: 0.045832625230153405.[0m
[32m[I 2025-02-08 00:11:05,268][0m Trial 16 finished with value: 0.03700465807805256 and parameters: {'observation_period_num': 15, 'train_rates': 0.8670853268220997, 'learning_rate': 0.0002690255145462724, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9221219167540043}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:15:38,347][0m Trial 17 finished with value: 0.12028246873153814 and parameters: {'observation_period_num': 5, 'train_rates': 0.6170614973381078, 'learning_rate': 0.00027345398786781706, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9221384799705681}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:20:05,734][0m Trial 18 finished with value: 0.05280605794517632 and parameters: {'observation_period_num': 39, 'train_rates': 0.8555438783602755, 'learning_rate': 2.7798856000212967e-05, 'batch_size': 20, 'step_size': 3, 'gamma': 0.9182642473674677}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:21:57,192][0m Trial 19 finished with value: 0.10423080098606477 and parameters: {'observation_period_num': 37, 'train_rates': 0.8644576174970577, 'learning_rate': 0.00029826933525766103, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9438145850741978}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:23:55,039][0m Trial 20 finished with value: 0.2383685399155266 and parameters: {'observation_period_num': 69, 'train_rates': 0.7368871256675096, 'learning_rate': 5.5152110069454185e-05, 'batch_size': 41, 'step_size': 2, 'gamma': 0.8923134337840457}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:27:13,195][0m Trial 21 finished with value: 0.05080074300565342 and parameters: {'observation_period_num': 28, 'train_rates': 0.8622864058773424, 'learning_rate': 2.1564952048590028e-05, 'batch_size': 28, 'step_size': 4, 'gamma': 0.9155448455410599}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:28:25,177][0m Trial 22 finished with value: 0.14990896627872805 and parameters: {'observation_period_num': 23, 'train_rates': 0.946462865430283, 'learning_rate': 8.039671822300625e-06, 'batch_size': 84, 'step_size': 4, 'gamma': 0.944414785895967}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:30:57,717][0m Trial 23 finished with value: 0.09617253223888013 and parameters: {'observation_period_num': 51, 'train_rates': 0.8776200568587497, 'learning_rate': 0.000144700927325581, 'batch_size': 36, 'step_size': 5, 'gamma': 0.8978706367058199}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:32:05,644][0m Trial 24 finished with value: 0.06389632750779325 and parameters: {'observation_period_num': 30, 'train_rates': 0.8429523773449531, 'learning_rate': 2.355670852203301e-05, 'batch_size': 81, 'step_size': 8, 'gamma': 0.9373327889889733}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:35:21,839][0m Trial 25 finished with value: 0.07017280807886174 and parameters: {'observation_period_num': 79, 'train_rates': 0.934423685518031, 'learning_rate': 4.786903423525831e-05, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9650387240917612}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:36:55,273][0m Trial 26 finished with value: 0.05733324587345123 and parameters: {'observation_period_num': 60, 'train_rates': 0.883563126773027, 'learning_rate': 0.0001435927949674902, 'batch_size': 60, 'step_size': 3, 'gamma': 0.9041472473699527}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:37:45,330][0m Trial 27 finished with value: 0.156331825138874 and parameters: {'observation_period_num': 13, 'train_rates': 0.761176825751782, 'learning_rate': 0.00038473679117017955, 'batch_size': 102, 'step_size': 5, 'gamma': 0.9243825585785826}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:39:36,775][0m Trial 28 finished with value: 0.17443372958986014 and parameters: {'observation_period_num': 5, 'train_rates': 0.8387415756522105, 'learning_rate': 7.323403931044329e-06, 'batch_size': 49, 'step_size': 2, 'gamma': 0.8465264445281211}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:40:20,933][0m Trial 29 finished with value: 0.1302899238934 and parameters: {'observation_period_num': 90, 'train_rates': 0.9422282337210826, 'learning_rate': 9.985564550082518e-05, 'batch_size': 140, 'step_size': 8, 'gamma': 0.841973345074505}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:41:34,594][0m Trial 30 finished with value: 0.04667595309369704 and parameters: {'observation_period_num': 42, 'train_rates': 0.8835581891323602, 'learning_rate': 0.00020530735314093753, 'batch_size': 76, 'step_size': 4, 'gamma': 0.8839070953455036}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:44:32,593][0m Trial 31 finished with value: 0.06406574823005601 and parameters: {'observation_period_num': 39, 'train_rates': 0.8824798262503432, 'learning_rate': 0.00019638762249201384, 'batch_size': 31, 'step_size': 5, 'gamma': 0.8787389206747931}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:45:56,845][0m Trial 32 finished with value: 0.04475057343470639 and parameters: {'observation_period_num': 30, 'train_rates': 0.9505744427588996, 'learning_rate': 0.0005579022633031862, 'batch_size': 71, 'step_size': 4, 'gamma': 0.9316711128294812}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:47:17,780][0m Trial 33 finished with value: 0.10457359819576659 and parameters: {'observation_period_num': 89, 'train_rates': 0.9597605971871574, 'learning_rate': 0.0005609427372728165, 'batch_size': 73, 'step_size': 3, 'gamma': 0.9307560541408136}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:49:12,343][0m Trial 34 finished with value: 0.15527688794665867 and parameters: {'observation_period_num': 54, 'train_rates': 0.9228723230061473, 'learning_rate': 0.0004430011077332301, 'batch_size': 50, 'step_size': 7, 'gamma': 0.9556575099186758}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:50:12,730][0m Trial 35 finished with value: 0.09596250392496586 and parameters: {'observation_period_num': 22, 'train_rates': 0.9565099068217893, 'learning_rate': 0.0002019861394068454, 'batch_size': 102, 'step_size': 1, 'gamma': 0.8957104719418507}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:51:44,564][0m Trial 36 finished with value: 0.060362602287734095 and parameters: {'observation_period_num': 40, 'train_rates': 0.9096298429192103, 'learning_rate': 8.592026615445804e-05, 'batch_size': 63, 'step_size': 5, 'gamma': 0.8193083405732365}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:52:33,470][0m Trial 37 finished with value: 0.15632222592830658 and parameters: {'observation_period_num': 65, 'train_rates': 0.9632051502699908, 'learning_rate': 0.0006223792539004209, 'batch_size': 125, 'step_size': 15, 'gamma': 0.9062368667885305}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:52:59,346][0m Trial 38 finished with value: 0.08284689006548167 and parameters: {'observation_period_num': 80, 'train_rates': 0.8200422188541016, 'learning_rate': 0.00019872701971984167, 'batch_size': 221, 'step_size': 2, 'gamma': 0.9599241812723273}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:54:11,021][0m Trial 39 finished with value: 0.1833336171457323 and parameters: {'observation_period_num': 233, 'train_rates': 0.8919002262782374, 'learning_rate': 0.0003376406826539371, 'batch_size': 74, 'step_size': 4, 'gamma': 0.9295664018787726}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:55:05,604][0m Trial 40 finished with value: 0.19962457119006116 and parameters: {'observation_period_num': 45, 'train_rates': 0.7831786978936683, 'learning_rate': 0.00010945531284065728, 'batch_size': 97, 'step_size': 3, 'gamma': 0.8863046994789144}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 00:58:34,313][0m Trial 41 finished with value: 0.3046942066766135 and parameters: {'observation_period_num': 23, 'train_rates': 0.8507238186789474, 'learning_rate': 1.1297257480593139e-06, 'batch_size': 26, 'step_size': 4, 'gamma': 0.9122795273019979}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:00:56,649][0m Trial 42 finished with value: 0.04249850553028362 and parameters: {'observation_period_num': 28, 'train_rates': 0.8677897373527267, 'learning_rate': 3.7926542822657065e-05, 'batch_size': 40, 'step_size': 4, 'gamma': 0.9390559835988467}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:03:02,502][0m Trial 43 finished with value: 0.0742360049949109 and parameters: {'observation_period_num': 14, 'train_rates': 0.8253490272507418, 'learning_rate': 0.0006382136763996433, 'batch_size': 42, 'step_size': 7, 'gamma': 0.9710876747394093}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:04:39,713][0m Trial 44 finished with value: 0.17028177946363346 and parameters: {'observation_period_num': 186, 'train_rates': 0.9272309190767376, 'learning_rate': 0.00045649423005775393, 'batch_size': 57, 'step_size': 5, 'gamma': 0.9413303296169001}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:06:01,103][0m Trial 45 finished with value: 0.06293961211917846 and parameters: {'observation_period_num': 58, 'train_rates': 0.9153531898889892, 'learning_rate': 4.252457656748041e-05, 'batch_size': 71, 'step_size': 3, 'gamma': 0.979393915987641}. Best is trial 16 with value: 0.03700465807805256.[0m
Early stopping at epoch 95
[32m[I 2025-02-08 01:08:27,176][0m Trial 46 finished with value: 0.05753183364868164 and parameters: {'observation_period_num': 16, 'train_rates': 0.9888400435050163, 'learning_rate': 0.0002489386735675071, 'batch_size': 40, 'step_size': 2, 'gamma': 0.7624397232594083}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:10:15,726][0m Trial 47 finished with value: 0.047847082778330774 and parameters: {'observation_period_num': 32, 'train_rates': 0.8730314743181822, 'learning_rate': 7.140493524815227e-05, 'batch_size': 52, 'step_size': 9, 'gamma': 0.931412596440011}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:14:30,118][0m Trial 48 finished with value: 0.16510572277083255 and parameters: {'observation_period_num': 48, 'train_rates': 0.687971118196634, 'learning_rate': 0.0007405670837299811, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9506848432963506}. Best is trial 16 with value: 0.03700465807805256.[0m
[32m[I 2025-02-08 01:15:34,441][0m Trial 49 finished with value: 0.08062280339413676 and parameters: {'observation_period_num': 99, 'train_rates': 0.8989837523531076, 'learning_rate': 0.00012940223543161594, 'batch_size': 88, 'step_size': 12, 'gamma': 0.8774731888233989}. Best is trial 16 with value: 0.03700465807805256.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-08 01:15:34,451][0m A new study created in memory with name: no-name-ca08a6bb-461e-4d4d-9345-0e3425b483ea[0m
[32m[I 2025-02-08 01:16:08,082][0m Trial 0 finished with value: 0.20828620357126554 and parameters: {'observation_period_num': 55, 'train_rates': 0.7785263129207095, 'learning_rate': 0.0008630090457699392, 'batch_size': 165, 'step_size': 6, 'gamma': 0.9369131959276567}. Best is trial 0 with value: 0.20828620357126554.[0m
[32m[I 2025-02-08 01:16:29,124][0m Trial 1 finished with value: 0.4288744337224642 and parameters: {'observation_period_num': 106, 'train_rates': 0.6326932607195623, 'learning_rate': 7.841089935737424e-06, 'batch_size': 243, 'step_size': 15, 'gamma': 0.9341632713685925}. Best is trial 0 with value: 0.20828620357126554.[0m
[32m[I 2025-02-08 01:17:03,290][0m Trial 2 finished with value: 0.29994606902992577 and parameters: {'observation_period_num': 158, 'train_rates': 0.6907289729460828, 'learning_rate': 0.00028659265889028913, 'batch_size': 140, 'step_size': 5, 'gamma': 0.774465016841613}. Best is trial 0 with value: 0.20828620357126554.[0m
[32m[I 2025-02-08 01:17:46,002][0m Trial 3 finished with value: 0.15068260934778246 and parameters: {'observation_period_num': 164, 'train_rates': 0.9188176650066082, 'learning_rate': 0.00025642341894841695, 'batch_size': 137, 'step_size': 14, 'gamma': 0.8591476349246827}. Best is trial 3 with value: 0.15068260934778246.[0m
[32m[I 2025-02-08 01:18:09,828][0m Trial 4 finished with value: 0.14930002676024906 and parameters: {'observation_period_num': 105, 'train_rates': 0.7968219881578339, 'learning_rate': 6.415149965858305e-05, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8699222129518009}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:18:41,604][0m Trial 5 finished with value: 0.38840091012810407 and parameters: {'observation_period_num': 160, 'train_rates': 0.7831846358388056, 'learning_rate': 0.00011340687636427868, 'batch_size': 178, 'step_size': 1, 'gamma': 0.9332472749324694}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:19:14,890][0m Trial 6 finished with value: 0.18006285806851727 and parameters: {'observation_period_num': 31, 'train_rates': 0.6179555433559473, 'learning_rate': 0.00019403360712024107, 'batch_size': 146, 'step_size': 2, 'gamma': 0.8428411377197765}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:19:40,310][0m Trial 7 finished with value: 0.251578007120895 and parameters: {'observation_period_num': 119, 'train_rates': 0.6312419436281876, 'learning_rate': 0.0006104292146673918, 'batch_size': 191, 'step_size': 15, 'gamma': 0.9226342315132792}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:20:25,105][0m Trial 8 finished with value: 1.2691142559051514 and parameters: {'observation_period_num': 36, 'train_rates': 0.9535725216303521, 'learning_rate': 1.482441871701199e-06, 'batch_size': 144, 'step_size': 3, 'gamma': 0.8440487663288195}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:21:01,072][0m Trial 9 finished with value: 0.29064257912541147 and parameters: {'observation_period_num': 93, 'train_rates': 0.7189035962470021, 'learning_rate': 1.6646088896327016e-05, 'batch_size': 144, 'step_size': 13, 'gamma': 0.9622137559374648}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:23:09,715][0m Trial 10 finished with value: 0.1501389224638884 and parameters: {'observation_period_num': 249, 'train_rates': 0.8719168882361022, 'learning_rate': 5.006052122925458e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.7834868771340177}. Best is trial 4 with value: 0.14930002676024906.[0m
[32m[I 2025-02-08 01:26:08,911][0m Trial 11 finished with value: 0.13368644158261409 and parameters: {'observation_period_num': 233, 'train_rates': 0.8735412749480527, 'learning_rate': 4.255466449006213e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.7500569452172765}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:27:40,836][0m Trial 12 finished with value: 0.1666086408611081 and parameters: {'observation_period_num': 251, 'train_rates': 0.8472796326831774, 'learning_rate': 4.5745825094219114e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.7560164821164926}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:28:45,099][0m Trial 13 finished with value: 0.384020616658952 and parameters: {'observation_period_num': 203, 'train_rates': 0.8503210819561555, 'learning_rate': 8.075962737004383e-06, 'batch_size': 81, 'step_size': 11, 'gamma': 0.8142607323774247}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:29:11,338][0m Trial 14 finished with value: 0.17991838966672485 and parameters: {'observation_period_num': 74, 'train_rates': 0.8975371003018412, 'learning_rate': 2.5798300399415178e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.8786345495148856}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:30:10,334][0m Trial 15 finished with value: 0.13738949174353665 and parameters: {'observation_period_num': 199, 'train_rates': 0.8085487591381703, 'learning_rate': 7.366832539378509e-05, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8961598605307322}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:31:07,156][0m Trial 16 finished with value: 0.30708207207706134 and parameters: {'observation_period_num': 212, 'train_rates': 0.8160722451766761, 'learning_rate': 1.0399888149260291e-05, 'batch_size': 91, 'step_size': 8, 'gamma': 0.8944929412636516}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:35:47,072][0m Trial 17 finished with value: 0.383534810441063 and parameters: {'observation_period_num': 213, 'train_rates': 0.7279829886230829, 'learning_rate': 2.6100521633008818e-06, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9789865333956296}. Best is trial 11 with value: 0.13368644158261409.[0m
[32m[I 2025-02-08 01:36:46,389][0m Trial 18 finished with value: 0.12823987007141113 and parameters: {'observation_period_num': 187, 'train_rates': 0.9773235442157149, 'learning_rate': 0.00013094481679954068, 'batch_size': 100, 'step_size': 9, 'gamma': 0.814757658503013}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:37:39,296][0m Trial 19 finished with value: 0.32402586936950684 and parameters: {'observation_period_num': 186, 'train_rates': 0.9860762300643926, 'learning_rate': 2.507205479096652e-05, 'batch_size': 115, 'step_size': 6, 'gamma': 0.8072297934524529}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:39:21,545][0m Trial 20 finished with value: 0.15878851818186895 and parameters: {'observation_period_num': 231, 'train_rates': 0.938775076310189, 'learning_rate': 0.00014351763788890122, 'batch_size': 54, 'step_size': 9, 'gamma': 0.8099623741560307}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:40:20,788][0m Trial 21 finished with value: 0.13875994086265564 and parameters: {'observation_period_num': 185, 'train_rates': 0.9720075048552198, 'learning_rate': 0.00010040945035861984, 'batch_size': 98, 'step_size': 12, 'gamma': 0.9008491411261399}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:45:35,461][0m Trial 22 finished with value: 0.17002692324913118 and parameters: {'observation_period_num': 143, 'train_rates': 0.896401996805718, 'learning_rate': 0.0004109259937269606, 'batch_size': 17, 'step_size': 9, 'gamma': 0.7555476263917352}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:46:49,374][0m Trial 23 finished with value: 0.14271266683066403 and parameters: {'observation_period_num': 226, 'train_rates': 0.8121061206520553, 'learning_rate': 8.305114077835211e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.8293681070429864}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:47:33,439][0m Trial 24 finished with value: 0.3468298278696594 and parameters: {'observation_period_num': 185, 'train_rates': 0.743922137512823, 'learning_rate': 4.129786326430689e-05, 'batch_size': 110, 'step_size': 7, 'gamma': 0.7946439284666595}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:49:33,033][0m Trial 25 finished with value: 0.20838807515891983 and parameters: {'observation_period_num': 198, 'train_rates': 0.8536563576105554, 'learning_rate': 1.7413419042782062e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.7737337662596189}. Best is trial 18 with value: 0.12823987007141113.[0m
[32m[I 2025-02-08 01:50:25,196][0m Trial 26 finished with value: 0.04577329884734916 and parameters: {'observation_period_num': 7, 'train_rates': 0.9213954955964155, 'learning_rate': 0.00015772043998937515, 'batch_size': 116, 'step_size': 13, 'gamma': 0.9047597914823435}. Best is trial 26 with value: 0.04577329884734916.[0m
[32m[I 2025-02-08 01:50:56,609][0m Trial 27 finished with value: 0.053479768335819244 and parameters: {'observation_period_num': 8, 'train_rates': 0.9368188005129873, 'learning_rate': 0.00016835128318108667, 'batch_size': 205, 'step_size': 13, 'gamma': 0.825815710365738}. Best is trial 26 with value: 0.04577329884734916.[0m
[32m[I 2025-02-08 01:51:25,467][0m Trial 28 finished with value: 0.04266219213604927 and parameters: {'observation_period_num': 5, 'train_rates': 0.9297534764183352, 'learning_rate': 0.0004313129096536761, 'batch_size': 212, 'step_size': 13, 'gamma': 0.8276196402980247}. Best is trial 28 with value: 0.04266219213604927.[0m
[32m[I 2025-02-08 01:51:54,573][0m Trial 29 finished with value: 0.03909984690447648 and parameters: {'observation_period_num': 8, 'train_rates': 0.9239309820745015, 'learning_rate': 0.0008828982363217428, 'batch_size': 220, 'step_size': 13, 'gamma': 0.8378389084898721}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:52:21,518][0m Trial 30 finished with value: 0.04003087371420949 and parameters: {'observation_period_num': 5, 'train_rates': 0.909599362324478, 'learning_rate': 0.0008976230096368273, 'batch_size': 225, 'step_size': 14, 'gamma': 0.8531279742875327}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:52:51,279][0m Trial 31 finished with value: 0.045753481690546834 and parameters: {'observation_period_num': 5, 'train_rates': 0.9090068324814574, 'learning_rate': 0.0009830822720365998, 'batch_size': 219, 'step_size': 14, 'gamma': 0.8524017401935846}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:53:19,860][0m Trial 32 finished with value: 0.05870145559310913 and parameters: {'observation_period_num': 30, 'train_rates': 0.8998668078354582, 'learning_rate': 0.0009035146665041997, 'batch_size': 224, 'step_size': 15, 'gamma': 0.8496443822843249}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:53:49,411][0m Trial 33 finished with value: 0.0724758580327034 and parameters: {'observation_period_num': 55, 'train_rates': 0.9592722173305366, 'learning_rate': 0.0005744955875037469, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8713064650516406}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:54:19,024][0m Trial 34 finished with value: 0.04142697900533676 and parameters: {'observation_period_num': 19, 'train_rates': 0.9309899974579073, 'learning_rate': 0.0009839888065462207, 'batch_size': 218, 'step_size': 14, 'gamma': 0.8327493793657698}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:54:47,425][0m Trial 35 finished with value: 0.07386500388383865 and parameters: {'observation_period_num': 45, 'train_rates': 0.9363750440286008, 'learning_rate': 0.00036305186712166584, 'batch_size': 234, 'step_size': 15, 'gamma': 0.8283410099333633}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:55:17,902][0m Trial 36 finished with value: 0.05589657502291216 and parameters: {'observation_period_num': 22, 'train_rates': 0.8805082676917408, 'learning_rate': 0.0006080278091685605, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8629895961958322}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:55:47,546][0m Trial 37 finished with value: 0.19993104338645934 and parameters: {'observation_period_num': 59, 'train_rates': 0.6590073246557179, 'learning_rate': 0.00030989487467791984, 'batch_size': 168, 'step_size': 15, 'gamma': 0.8359633792407879}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:56:21,377][0m Trial 38 finished with value: 0.045554313842856554 and parameters: {'observation_period_num': 18, 'train_rates': 0.9261122802941509, 'learning_rate': 0.0004859436424595675, 'batch_size': 189, 'step_size': 12, 'gamma': 0.8828013398314172}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:56:49,260][0m Trial 39 finished with value: 0.08874277770519257 and parameters: {'observation_period_num': 72, 'train_rates': 0.9556950123520991, 'learning_rate': 0.0007236295220589896, 'batch_size': 243, 'step_size': 14, 'gamma': 0.7983323499594877}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:57:17,865][0m Trial 40 finished with value: 0.0727567925238709 and parameters: {'observation_period_num': 41, 'train_rates': 0.8363655428971207, 'learning_rate': 0.0002565064539277473, 'batch_size': 208, 'step_size': 4, 'gamma': 0.8404490485646458}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:57:50,990][0m Trial 41 finished with value: 0.04364694126656857 and parameters: {'observation_period_num': 20, 'train_rates': 0.9211876891486921, 'learning_rate': 0.0004889494717182253, 'batch_size': 187, 'step_size': 12, 'gamma': 0.8753435337372093}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:58:24,539][0m Trial 42 finished with value: 0.04370072002454502 and parameters: {'observation_period_num': 20, 'train_rates': 0.9165902398713257, 'learning_rate': 0.0009512622015752688, 'batch_size': 192, 'step_size': 14, 'gamma': 0.8600913655809507}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:58:52,163][0m Trial 43 finished with value: 0.04961055889725685 and parameters: {'observation_period_num': 22, 'train_rates': 0.9502612349109704, 'learning_rate': 0.0004630815739064002, 'batch_size': 241, 'step_size': 13, 'gamma': 0.8817091422296911}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 01:59:25,239][0m Trial 44 finished with value: 0.0615963809301748 and parameters: {'observation_period_num': 48, 'train_rates': 0.883530764755262, 'learning_rate': 0.0002120782158766641, 'batch_size': 175, 'step_size': 11, 'gamma': 0.8483928624929257}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 02:00:06,543][0m Trial 45 finished with value: 0.08788525313138962 and parameters: {'observation_period_num': 29, 'train_rates': 0.966827347169356, 'learning_rate': 0.0007274459765890985, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9130898883547126}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 02:00:33,739][0m Trial 46 finished with value: 0.09099609524907311 and parameters: {'observation_period_num': 79, 'train_rates': 0.8732046165027078, 'learning_rate': 0.0003164146663260744, 'batch_size': 215, 'step_size': 13, 'gamma': 0.8239438711812164}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 02:01:04,510][0m Trial 47 finished with value: 0.040864232927560806 and parameters: {'observation_period_num': 14, 'train_rates': 0.9408809068122693, 'learning_rate': 0.0006570056664986071, 'batch_size': 198, 'step_size': 12, 'gamma': 0.8704933997425682}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 02:01:37,841][0m Trial 48 finished with value: 0.06527131795883179 and parameters: {'observation_period_num': 59, 'train_rates': 0.9886942656639941, 'learning_rate': 0.000732380140578776, 'batch_size': 201, 'step_size': 14, 'gamma': 0.8593794700173478}. Best is trial 29 with value: 0.03909984690447648.[0m
[32m[I 2025-02-08 02:02:01,470][0m Trial 49 finished with value: 0.1908443016013668 and parameters: {'observation_period_num': 36, 'train_rates': 0.765531178767915, 'learning_rate': 0.00022227566382343046, 'batch_size': 233, 'step_size': 11, 'gamma': 0.8411171895984477}. Best is trial 29 with value: 0.03909984690447648.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-08 02:02:01,480][0m A new study created in memory with name: no-name-08b19183-c6a6-43a9-a8ed-acded6a7f85e[0m
[32m[I 2025-02-08 02:02:31,367][0m Trial 0 finished with value: 0.1293044193717074 and parameters: {'observation_period_num': 81, 'train_rates': 0.8740090677879678, 'learning_rate': 3.133108586727271e-05, 'batch_size': 201, 'step_size': 15, 'gamma': 0.9175896928382824}. Best is trial 0 with value: 0.1293044193717074.[0m
[32m[I 2025-02-08 02:02:58,198][0m Trial 1 finished with value: 0.4699346927548844 and parameters: {'observation_period_num': 129, 'train_rates': 0.7697437815223656, 'learning_rate': 8.187591124123069e-05, 'batch_size': 207, 'step_size': 1, 'gamma': 0.9153546642948667}. Best is trial 0 with value: 0.1293044193717074.[0m
[32m[I 2025-02-08 02:04:41,586][0m Trial 2 finished with value: 0.3018202563570235 and parameters: {'observation_period_num': 230, 'train_rates': 0.7581649193515989, 'learning_rate': 3.200130887609217e-05, 'batch_size': 45, 'step_size': 7, 'gamma': 0.8410608229322991}. Best is trial 0 with value: 0.1293044193717074.[0m
[32m[I 2025-02-08 02:05:11,645][0m Trial 3 finished with value: 0.2886812061025119 and parameters: {'observation_period_num': 209, 'train_rates': 0.7662347711723869, 'learning_rate': 0.00016299337867428057, 'batch_size': 171, 'step_size': 2, 'gamma': 0.931782873972532}. Best is trial 0 with value: 0.1293044193717074.[0m
[32m[I 2025-02-08 02:05:36,974][0m Trial 4 finished with value: 0.5322783076972292 and parameters: {'observation_period_num': 70, 'train_rates': 0.8816650811168766, 'learning_rate': 5.70268477298501e-06, 'batch_size': 232, 'step_size': 7, 'gamma': 0.8614029795627924}. Best is trial 0 with value: 0.1293044193717074.[0m
[32m[I 2025-02-08 02:06:34,619][0m Trial 5 finished with value: 0.8231561848750482 and parameters: {'observation_period_num': 63, 'train_rates': 0.6410266786151161, 'learning_rate': 1.2510768561386702e-06, 'batch_size': 80, 'step_size': 14, 'gamma': 0.8428294483725245}. Best is trial 0 with value: 0.1293044193717074.[0m
[32m[I 2025-02-08 02:08:17,817][0m Trial 6 finished with value: 0.11783867921148028 and parameters: {'observation_period_num': 98, 'train_rates': 0.8780281964717576, 'learning_rate': 0.0005199075755108963, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8559412093787917}. Best is trial 6 with value: 0.11783867921148028.[0m
[32m[I 2025-02-08 02:09:36,447][0m Trial 7 finished with value: 0.3530580612520377 and parameters: {'observation_period_num': 10, 'train_rates': 0.7357126154599638, 'learning_rate': 1.4174423923014669e-06, 'batch_size': 65, 'step_size': 6, 'gamma': 0.9163240396754992}. Best is trial 6 with value: 0.11783867921148028.[0m
[32m[I 2025-02-08 02:10:11,140][0m Trial 8 finished with value: 0.6971582797578144 and parameters: {'observation_period_num': 236, 'train_rates': 0.8555812860156121, 'learning_rate': 2.568640498440898e-06, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8970079101292847}. Best is trial 6 with value: 0.11783867921148028.[0m
[32m[I 2025-02-08 02:11:15,367][0m Trial 9 finished with value: 0.1039990683019549 and parameters: {'observation_period_num': 169, 'train_rates': 0.8350089106490632, 'learning_rate': 5.107629388415595e-05, 'batch_size': 82, 'step_size': 9, 'gamma': 0.9219015580191828}. Best is trial 9 with value: 0.1039990683019549.[0m
[32m[I 2025-02-08 02:12:09,178][0m Trial 10 finished with value: 0.26594310998916626 and parameters: {'observation_period_num': 168, 'train_rates': 0.9733753991764448, 'learning_rate': 1.1501223425618558e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9878757713591106}. Best is trial 9 with value: 0.1039990683019549.[0m
[32m[I 2025-02-08 02:17:55,815][0m Trial 11 finished with value: 0.1230363785289228 and parameters: {'observation_period_num': 150, 'train_rates': 0.9546356475294309, 'learning_rate': 0.0008665098457257932, 'batch_size': 16, 'step_size': 5, 'gamma': 0.7867074217129684}. Best is trial 9 with value: 0.1039990683019549.[0m
[32m[I 2025-02-08 02:18:42,578][0m Trial 12 finished with value: 0.10048476234078407 and parameters: {'observation_period_num': 189, 'train_rates': 0.8398578490230971, 'learning_rate': 0.0009021260117885876, 'batch_size': 111, 'step_size': 4, 'gamma': 0.8068751502654825}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:19:28,048][0m Trial 13 finished with value: 0.10506520651175942 and parameters: {'observation_period_num': 184, 'train_rates': 0.8211695365711722, 'learning_rate': 0.00021317749991313435, 'batch_size': 117, 'step_size': 10, 'gamma': 0.7560620359725353}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:20:15,038][0m Trial 14 finished with value: 0.2360730294555169 and parameters: {'observation_period_num': 194, 'train_rates': 0.7063321735747826, 'learning_rate': 0.0003698363606461792, 'batch_size': 98, 'step_size': 9, 'gamma': 0.7932834778371728}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:20:53,363][0m Trial 15 finished with value: 0.1264826997939963 and parameters: {'observation_period_num': 141, 'train_rates': 0.8186940450576414, 'learning_rate': 6.923213139016759e-05, 'batch_size': 141, 'step_size': 4, 'gamma': 0.962113942876957}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:21:45,968][0m Trial 16 finished with value: 0.3667870079423045 and parameters: {'observation_period_num': 164, 'train_rates': 0.6746182349119316, 'learning_rate': 1.4306460113961825e-05, 'batch_size': 88, 'step_size': 13, 'gamma': 0.8109363251850398}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:22:31,286][0m Trial 17 finished with value: 0.13743293635985432 and parameters: {'observation_period_num': 110, 'train_rates': 0.9284879467604106, 'learning_rate': 9.592281486908082e-05, 'batch_size': 132, 'step_size': 3, 'gamma': 0.8846625656842668}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:22:59,268][0m Trial 18 finished with value: 0.39953084953369633 and parameters: {'observation_period_num': 219, 'train_rates': 0.6043954081843307, 'learning_rate': 2.852054602945607e-05, 'batch_size': 164, 'step_size': 8, 'gamma': 0.8215266066026367}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:26:28,905][0m Trial 19 finished with value: 0.21777270209382882 and parameters: {'observation_period_num': 247, 'train_rates': 0.9189755145413236, 'learning_rate': 0.0007893443722446411, 'batch_size': 25, 'step_size': 12, 'gamma': 0.7527403824969108}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:27:38,743][0m Trial 20 finished with value: 0.19641806559373381 and parameters: {'observation_period_num': 191, 'train_rates': 0.8320099986704697, 'learning_rate': 0.00036897214484470585, 'batch_size': 74, 'step_size': 9, 'gamma': 0.9458665521220494}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:28:28,035][0m Trial 21 finished with value: 0.1035248001475306 and parameters: {'observation_period_num': 179, 'train_rates': 0.8179428923219877, 'learning_rate': 0.0001690530606606525, 'batch_size': 106, 'step_size': 10, 'gamma': 0.7571192302657149}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:29:17,939][0m Trial 22 finished with value: 0.10743391029949118 and parameters: {'observation_period_num': 169, 'train_rates': 0.8061403535956243, 'learning_rate': 0.00016876454250350418, 'batch_size': 104, 'step_size': 9, 'gamma': 0.7752367984496171}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:30:00,809][0m Trial 23 finished with value: 0.2054108040056367 and parameters: {'observation_period_num': 206, 'train_rates': 0.8497907415994969, 'learning_rate': 4.613343750135999e-05, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8076848387078265}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:30:35,249][0m Trial 24 finished with value: 0.11506188502202042 and parameters: {'observation_period_num': 179, 'train_rates': 0.7890210248591206, 'learning_rate': 0.00026782530124672176, 'batch_size': 150, 'step_size': 11, 'gamma': 0.7760602615809186}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:31:38,103][0m Trial 25 finished with value: 0.10763130125102646 and parameters: {'observation_period_num': 153, 'train_rates': 0.9223939337190601, 'learning_rate': 0.00011198998969903086, 'batch_size': 90, 'step_size': 5, 'gamma': 0.8841759118769082}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:33:36,803][0m Trial 26 finished with value: 0.16291423565463015 and parameters: {'observation_period_num': 118, 'train_rates': 0.8998174129299448, 'learning_rate': 0.0009252927261169169, 'batch_size': 46, 'step_size': 8, 'gamma': 0.8288098024825796}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:34:50,109][0m Trial 27 finished with value: 0.3400093036992797 and parameters: {'observation_period_num': 137, 'train_rates': 0.7331275271619083, 'learning_rate': 1.6222936908427568e-05, 'batch_size': 65, 'step_size': 10, 'gamma': 0.7978045151538339}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:35:30,859][0m Trial 28 finished with value: 0.12195591633663373 and parameters: {'observation_period_num': 203, 'train_rates': 0.7885971637653945, 'learning_rate': 0.00040198755203740686, 'batch_size': 125, 'step_size': 12, 'gamma': 0.7734513810071578}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:36:03,053][0m Trial 29 finished with value: 0.11811361330693786 and parameters: {'observation_period_num': 89, 'train_rates': 0.8459107617317057, 'learning_rate': 5.42203534839907e-05, 'batch_size': 181, 'step_size': 14, 'gamma': 0.764078060562079}. Best is trial 12 with value: 0.10048476234078407.[0m
[32m[I 2025-02-08 02:36:59,068][0m Trial 30 finished with value: 0.06075813591371487 and parameters: {'observation_period_num': 39, 'train_rates': 0.8594232427103784, 'learning_rate': 0.00013653852615636505, 'batch_size': 103, 'step_size': 6, 'gamma': 0.9729412842201566}. Best is trial 30 with value: 0.06075813591371487.[0m
[32m[I 2025-02-08 02:37:54,703][0m Trial 31 finished with value: 0.04241293328848198 and parameters: {'observation_period_num': 22, 'train_rates': 0.8602460933180784, 'learning_rate': 0.00012122498030282298, 'batch_size': 102, 'step_size': 6, 'gamma': 0.9467023056152564}. Best is trial 31 with value: 0.04241293328848198.[0m
[32m[I 2025-02-08 02:38:50,507][0m Trial 32 finished with value: 0.03762139106727191 and parameters: {'observation_period_num': 17, 'train_rates': 0.8661508565208573, 'learning_rate': 0.0001189008328562521, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9764428000601195}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:39:32,552][0m Trial 33 finished with value: 0.04161077932314535 and parameters: {'observation_period_num': 5, 'train_rates': 0.8854521554423334, 'learning_rate': 0.00011149048352037519, 'batch_size': 142, 'step_size': 1, 'gamma': 0.9889591705123617}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:40:02,186][0m Trial 34 finished with value: 0.043523979872541946 and parameters: {'observation_period_num': 5, 'train_rates': 0.8689774351179044, 'learning_rate': 0.0001155956742008196, 'batch_size': 205, 'step_size': 1, 'gamma': 0.9887460410989287}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:40:28,917][0m Trial 35 finished with value: 0.06558134455661305 and parameters: {'observation_period_num': 8, 'train_rates': 0.8967981621699616, 'learning_rate': 7.94599773267678e-05, 'batch_size': 240, 'step_size': 1, 'gamma': 0.9807382335817062}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:40:59,829][0m Trial 36 finished with value: 0.26186999678611755 and parameters: {'observation_period_num': 31, 'train_rates': 0.9485319903510951, 'learning_rate': 3.001948314728097e-05, 'batch_size': 212, 'step_size': 1, 'gamma': 0.9563301408848569}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:41:33,596][0m Trial 37 finished with value: 0.06336442488603868 and parameters: {'observation_period_num': 32, 'train_rates': 0.8937638637345109, 'learning_rate': 0.00010992178885477173, 'batch_size': 186, 'step_size': 2, 'gamma': 0.9662481324483144}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:42:01,125][0m Trial 38 finished with value: 0.07485498464863065 and parameters: {'observation_period_num': 56, 'train_rates': 0.8779905099734419, 'learning_rate': 0.00021855215022762735, 'batch_size': 218, 'step_size': 2, 'gamma': 0.9396380608306475}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:42:26,983][0m Trial 39 finished with value: 0.07107104700239723 and parameters: {'observation_period_num': 19, 'train_rates': 0.8671725594972228, 'learning_rate': 3.8642938546611875e-05, 'batch_size': 251, 'step_size': 3, 'gamma': 0.9875110492406511}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:43:08,525][0m Trial 40 finished with value: 0.12356251035816967 and parameters: {'observation_period_num': 48, 'train_rates': 0.9121390357769682, 'learning_rate': 2.009647827756042e-05, 'batch_size': 141, 'step_size': 3, 'gamma': 0.9508205106806942}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:43:40,300][0m Trial 41 finished with value: 0.04951445957339263 and parameters: {'observation_period_num': 36, 'train_rates': 0.8651222168950289, 'learning_rate': 0.00012615721371808273, 'batch_size': 186, 'step_size': 6, 'gamma': 0.9741783829534386}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:44:11,255][0m Trial 42 finished with value: 0.06075356577483701 and parameters: {'observation_period_num': 20, 'train_rates': 0.8795226344833686, 'learning_rate': 7.4611955839526e-05, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9723043508913938}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:44:40,408][0m Trial 43 finished with value: 0.16147480905056 and parameters: {'observation_period_num': 73, 'train_rates': 0.9890977208432206, 'learning_rate': 0.0002598608055973223, 'batch_size': 226, 'step_size': 5, 'gamma': 0.9783963389594247}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:45:13,619][0m Trial 44 finished with value: 0.07211892306804657 and parameters: {'observation_period_num': 19, 'train_rates': 0.9420690158959649, 'learning_rate': 0.00013738534434516345, 'batch_size': 194, 'step_size': 2, 'gamma': 0.9256823672770245}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:45:53,101][0m Trial 45 finished with value: 0.09343590152145613 and parameters: {'observation_period_num': 6, 'train_rates': 0.8635701512172408, 'learning_rate': 7.763726997505944e-06, 'batch_size': 152, 'step_size': 6, 'gamma': 0.9363491796278678}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:46:24,025][0m Trial 46 finished with value: 0.20942095076293887 and parameters: {'observation_period_num': 42, 'train_rates': 0.7750498613943175, 'learning_rate': 6.242468578074992e-05, 'batch_size': 179, 'step_size': 7, 'gamma': 0.904690408742439}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:46:59,993][0m Trial 47 finished with value: 0.05861646693925888 and parameters: {'observation_period_num': 55, 'train_rates': 0.892316725823116, 'learning_rate': 0.00048328488277862744, 'batch_size': 166, 'step_size': 1, 'gamma': 0.9877628362985188}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:47:30,478][0m Trial 48 finished with value: 0.060586347517824005 and parameters: {'observation_period_num': 27, 'train_rates': 0.8070841743848065, 'learning_rate': 9.32584886782918e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.9570630649253584}. Best is trial 32 with value: 0.03762139106727191.[0m
[32m[I 2025-02-08 02:48:00,318][0m Trial 49 finished with value: 0.08166519107658472 and parameters: {'observation_period_num': 65, 'train_rates': 0.9075787299055286, 'learning_rate': 0.0002969714724841065, 'batch_size': 204, 'step_size': 4, 'gamma': 0.9682623096424887}. Best is trial 32 with value: 0.03762139106727191.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-08 02:48:00,329][0m A new study created in memory with name: no-name-c641d1b2-497a-47f2-9aee-867813b458d2[0m
[32m[I 2025-02-08 02:48:51,592][0m Trial 0 finished with value: 0.9284538326773254 and parameters: {'observation_period_num': 207, 'train_rates': 0.9424696921890116, 'learning_rate': 3.9640292447281445e-06, 'batch_size': 112, 'step_size': 5, 'gamma': 0.7724473930598063}. Best is trial 0 with value: 0.9284538326773254.[0m
[32m[I 2025-02-08 02:49:44,284][0m Trial 1 finished with value: 0.5293719899564532 and parameters: {'observation_period_num': 20, 'train_rates': 0.8648888185808115, 'learning_rate': 1.2099021348785989e-06, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8288375411927661}. Best is trial 1 with value: 0.5293719899564532.[0m
[32m[I 2025-02-08 02:50:34,281][0m Trial 2 finished with value: 0.14883016244546318 and parameters: {'observation_period_num': 37, 'train_rates': 0.6897685798024935, 'learning_rate': 0.000350211456848768, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9054942319629051}. Best is trial 2 with value: 0.14883016244546318.[0m
[32m[I 2025-02-08 02:50:57,044][0m Trial 3 finished with value: 0.8977681197156416 and parameters: {'observation_period_num': 212, 'train_rates': 0.6885238302606136, 'learning_rate': 4.212796457254851e-06, 'batch_size': 227, 'step_size': 3, 'gamma': 0.9593900304299096}. Best is trial 2 with value: 0.14883016244546318.[0m
[32m[I 2025-02-08 02:51:24,996][0m Trial 4 finished with value: 0.14786433840885102 and parameters: {'observation_period_num': 168, 'train_rates': 0.8292434457820285, 'learning_rate': 0.00048223252322359745, 'batch_size': 197, 'step_size': 15, 'gamma': 0.8431891013377484}. Best is trial 4 with value: 0.14786433840885102.[0m
[32m[I 2025-02-08 02:51:53,112][0m Trial 5 finished with value: 1.066886116582227 and parameters: {'observation_period_num': 232, 'train_rates': 0.7568785487146685, 'learning_rate': 1.5497169462660425e-06, 'batch_size': 182, 'step_size': 11, 'gamma': 0.8005226720724187}. Best is trial 4 with value: 0.14786433840885102.[0m
[32m[I 2025-02-08 02:52:15,873][0m Trial 6 finished with value: 0.3131853314963254 and parameters: {'observation_period_num': 88, 'train_rates': 0.7090783392697302, 'learning_rate': 4.078489541372869e-05, 'batch_size': 228, 'step_size': 9, 'gamma': 0.8112599174175928}. Best is trial 4 with value: 0.14786433840885102.[0m
[32m[I 2025-02-08 02:53:09,651][0m Trial 7 finished with value: 0.1469412331792002 and parameters: {'observation_period_num': 203, 'train_rates': 0.8525580161541639, 'learning_rate': 0.00013817552992947426, 'batch_size': 103, 'step_size': 15, 'gamma': 0.8596162694927185}. Best is trial 7 with value: 0.1469412331792002.[0m
[32m[I 2025-02-08 02:53:42,862][0m Trial 8 finished with value: 0.1769250245272044 and parameters: {'observation_period_num': 20, 'train_rates': 0.77255062358741, 'learning_rate': 0.00017620194783904257, 'batch_size': 187, 'step_size': 3, 'gamma': 0.9537495460776947}. Best is trial 7 with value: 0.1469412331792002.[0m
[32m[I 2025-02-08 02:54:13,742][0m Trial 9 finished with value: 0.17216706581990726 and parameters: {'observation_period_num': 161, 'train_rates': 0.7878367351371275, 'learning_rate': 0.00011531994998020894, 'batch_size': 172, 'step_size': 14, 'gamma': 0.8445512570063164}. Best is trial 7 with value: 0.1469412331792002.[0m
[32m[I 2025-02-08 02:56:33,518][0m Trial 10 finished with value: 0.1418425498040099 and parameters: {'observation_period_num': 108, 'train_rates': 0.9667686668515786, 'learning_rate': 0.0009836757540728886, 'batch_size': 42, 'step_size': 6, 'gamma': 0.8890758813012835}. Best is trial 10 with value: 0.1418425498040099.[0m
[32m[I 2025-02-08 03:01:54,994][0m Trial 11 finished with value: 0.10847904203936111 and parameters: {'observation_period_num': 113, 'train_rates': 0.9851681424591684, 'learning_rate': 0.0009862576985476578, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8915871181992133}. Best is trial 11 with value: 0.10847904203936111.[0m
[32m[I 2025-02-08 03:05:08,953][0m Trial 12 finished with value: 0.11664151184020503 and parameters: {'observation_period_num': 100, 'train_rates': 0.9783495724536962, 'learning_rate': 0.0009109401083950994, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9043688514762699}. Best is trial 11 with value: 0.10847904203936111.[0m
[32m[I 2025-02-08 03:10:36,410][0m Trial 13 finished with value: 0.07658849720203091 and parameters: {'observation_period_num': 73, 'train_rates': 0.9158892825244126, 'learning_rate': 0.0009169952714245973, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9241560396152229}. Best is trial 13 with value: 0.07658849720203091.[0m
[32m[I 2025-02-08 03:12:09,680][0m Trial 14 finished with value: 0.11286025504794037 and parameters: {'observation_period_num': 61, 'train_rates': 0.9019798611686295, 'learning_rate': 3.164909621173923e-05, 'batch_size': 61, 'step_size': 2, 'gamma': 0.9279928954901615}. Best is trial 13 with value: 0.07658849720203091.[0m
[32m[I 2025-02-08 03:13:20,233][0m Trial 15 finished with value: 0.28880792166575914 and parameters: {'observation_period_num': 134, 'train_rates': 0.614722877430466, 'learning_rate': 0.0003128823790532778, 'batch_size': 60, 'step_size': 4, 'gamma': 0.9341872660389762}. Best is trial 13 with value: 0.07658849720203091.[0m
[32m[I 2025-02-08 03:18:14,282][0m Trial 16 finished with value: 0.059511885015254326 and parameters: {'observation_period_num': 67, 'train_rates': 0.9188167722426162, 'learning_rate': 6.64992693780982e-05, 'batch_size': 19, 'step_size': 1, 'gamma': 0.9825790197210653}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:19:25,343][0m Trial 17 finished with value: 0.16051110697602763 and parameters: {'observation_period_num': 68, 'train_rates': 0.8933718813376249, 'learning_rate': 1.2916523537108952e-05, 'batch_size': 80, 'step_size': 2, 'gamma': 0.9738477509484486}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:20:09,365][0m Trial 18 finished with value: 0.08016328288027015 and parameters: {'observation_period_num': 58, 'train_rates': 0.913676295487151, 'learning_rate': 6.222833803816205e-05, 'batch_size': 138, 'step_size': 1, 'gamma': 0.9813649803789786}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:21:50,275][0m Trial 19 finished with value: 0.3769251020817921 and parameters: {'observation_period_num': 142, 'train_rates': 0.8250584824134904, 'learning_rate': 1.1656034165949438e-05, 'batch_size': 51, 'step_size': 1, 'gamma': 0.9436595591078574}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:22:32,140][0m Trial 20 finished with value: 0.18500214461670367 and parameters: {'observation_period_num': 79, 'train_rates': 0.9256087033226131, 'learning_rate': 1.2274149224332675e-05, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9899541941136848}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:23:16,259][0m Trial 21 finished with value: 0.07470732947391291 and parameters: {'observation_period_num': 58, 'train_rates': 0.8976587012271745, 'learning_rate': 6.580195276513284e-05, 'batch_size': 134, 'step_size': 1, 'gamma': 0.988566816790719}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:23:39,722][0m Trial 22 finished with value: 0.10238604422545794 and parameters: {'observation_period_num': 42, 'train_rates': 0.8645382492466707, 'learning_rate': 7.28088392187831e-05, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9656599956914044}. Best is trial 16 with value: 0.059511885015254326.[0m
[32m[I 2025-02-08 03:24:56,661][0m Trial 23 finished with value: 0.033225424156138085 and parameters: {'observation_period_num': 7, 'train_rates': 0.9445671229927642, 'learning_rate': 0.00022682566760523005, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9198333568446444}. Best is trial 23 with value: 0.033225424156138085.[0m
[32m[I 2025-02-08 03:26:12,442][0m Trial 24 finished with value: 0.03724561209653641 and parameters: {'observation_period_num': 13, 'train_rates': 0.9517747564653112, 'learning_rate': 7.050347999179391e-05, 'batch_size': 79, 'step_size': 4, 'gamma': 0.9862784147543932}. Best is trial 23 with value: 0.033225424156138085.[0m
[32m[I 2025-02-08 03:27:37,867][0m Trial 25 finished with value: 0.02596437460872241 and parameters: {'observation_period_num': 6, 'train_rates': 0.9514408695662091, 'learning_rate': 0.00020173945227506946, 'batch_size': 71, 'step_size': 4, 'gamma': 0.948684393644014}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:28:53,961][0m Trial 26 finished with value: 0.03280153770891713 and parameters: {'observation_period_num': 6, 'train_rates': 0.951988291762162, 'learning_rate': 0.00018791744717880707, 'batch_size': 79, 'step_size': 4, 'gamma': 0.9189396931870263}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:30:16,194][0m Trial 27 finished with value: 0.03658755496144295 and parameters: {'observation_period_num': 9, 'train_rates': 0.9886801637177312, 'learning_rate': 0.00018105602541490133, 'batch_size': 78, 'step_size': 7, 'gamma': 0.9131180969366747}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:31:25,429][0m Trial 28 finished with value: 0.04047527329515148 and parameters: {'observation_period_num': 39, 'train_rates': 0.948504795803331, 'learning_rate': 0.0004596982740901735, 'batch_size': 89, 'step_size': 4, 'gamma': 0.886024308645981}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:32:56,210][0m Trial 29 finished with value: 0.04411678776765863 and parameters: {'observation_period_num': 30, 'train_rates': 0.9427528292480413, 'learning_rate': 0.00023524822785102407, 'batch_size': 65, 'step_size': 5, 'gamma': 0.9452903343712947}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:33:42,405][0m Trial 30 finished with value: 0.14101129318600675 and parameters: {'observation_period_num': 5, 'train_rates': 0.8806276281059879, 'learning_rate': 2.0914604420502682e-05, 'batch_size': 130, 'step_size': 5, 'gamma': 0.7527830129478184}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:35:06,114][0m Trial 31 finished with value: 0.02857918292284012 and parameters: {'observation_period_num': 5, 'train_rates': 0.9883373600613453, 'learning_rate': 0.00019773502801447094, 'batch_size': 74, 'step_size': 7, 'gamma': 0.9100000186692918}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:35:59,877][0m Trial 32 finished with value: 0.07931412863903321 and parameters: {'observation_period_num': 26, 'train_rates': 0.9603600338278178, 'learning_rate': 0.0001107818476467902, 'batch_size': 116, 'step_size': 3, 'gamma': 0.8686050579045173}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:38:13,479][0m Trial 33 finished with value: 0.10025352169070724 and parameters: {'observation_period_num': 44, 'train_rates': 0.9423500981966096, 'learning_rate': 0.0005444452794254344, 'batch_size': 43, 'step_size': 11, 'gamma': 0.9187732067649989}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:39:20,062][0m Trial 34 finished with value: 0.03279313715505721 and parameters: {'observation_period_num': 5, 'train_rates': 0.9335096601787791, 'learning_rate': 0.00023326784909360532, 'batch_size': 91, 'step_size': 7, 'gamma': 0.906373093441477}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:40:10,947][0m Trial 35 finished with value: 0.039785797787564144 and parameters: {'observation_period_num': 22, 'train_rates': 0.8452463394472692, 'learning_rate': 0.0003209070993707304, 'batch_size': 117, 'step_size': 8, 'gamma': 0.9013120594419846}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:41:16,640][0m Trial 36 finished with value: 0.0636482909321785 and parameters: {'observation_period_num': 44, 'train_rates': 0.9686230698245899, 'learning_rate': 0.0001295430403662881, 'batch_size': 94, 'step_size': 7, 'gamma': 0.874918390291926}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:42:09,678][0m Trial 37 finished with value: 0.07270768763763565 and parameters: {'observation_period_num': 30, 'train_rates': 0.8759755901349036, 'learning_rate': 0.0005688885630943391, 'batch_size': 108, 'step_size': 11, 'gamma': 0.9387625557816613}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:43:36,001][0m Trial 38 finished with value: 0.03426465007566636 and parameters: {'observation_period_num': 18, 'train_rates': 0.9368748969248354, 'learning_rate': 0.00024861809295370147, 'batch_size': 68, 'step_size': 10, 'gamma': 0.9543981996016125}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:44:32,166][0m Trial 39 finished with value: 0.182253476891498 and parameters: {'observation_period_num': 249, 'train_rates': 0.8208240579240801, 'learning_rate': 0.00040127285949937243, 'batch_size': 91, 'step_size': 7, 'gamma': 0.9077671076748683}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:45:05,554][0m Trial 40 finished with value: 0.3381694549099538 and parameters: {'observation_period_num': 186, 'train_rates': 0.7396154811635699, 'learning_rate': 4.4044618058778494e-05, 'batch_size': 150, 'step_size': 9, 'gamma': 0.8549817396281246}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:46:29,430][0m Trial 41 finished with value: 0.04526085406541824 and parameters: {'observation_period_num': 9, 'train_rates': 0.9888218567468383, 'learning_rate': 0.00020139536098441152, 'batch_size': 74, 'step_size': 3, 'gamma': 0.9211317383636572}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:48:35,732][0m Trial 42 finished with value: 0.033047890721179134 and parameters: {'observation_period_num': 5, 'train_rates': 0.9328599417407529, 'learning_rate': 9.744928271131186e-05, 'batch_size': 47, 'step_size': 5, 'gamma': 0.8965766839556785}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:50:26,423][0m Trial 43 finished with value: 0.0626281747250881 and parameters: {'observation_period_num': 49, 'train_rates': 0.9648920004139747, 'learning_rate': 9.823756615204375e-05, 'batch_size': 53, 'step_size': 5, 'gamma': 0.8980007311487125}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:52:47,674][0m Trial 44 finished with value: 0.036766535306994155 and parameters: {'observation_period_num': 28, 'train_rates': 0.9290782584877175, 'learning_rate': 0.00016571681285711647, 'batch_size': 41, 'step_size': 6, 'gamma': 0.8774154928544886}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:54:57,670][0m Trial 45 finished with value: 0.13532371144631683 and parameters: {'observation_period_num': 17, 'train_rates': 0.6436736402406518, 'learning_rate': 0.0002967687475022555, 'batch_size': 35, 'step_size': 7, 'gamma': 0.817517570750388}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:56:01,465][0m Trial 46 finished with value: 0.0367119237780571 and parameters: {'observation_period_num': 34, 'train_rates': 0.9693053130096978, 'learning_rate': 0.0006591174952333388, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8860736015952668}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 03:57:50,326][0m Trial 47 finished with value: 0.09902727489173413 and parameters: {'observation_period_num': 52, 'train_rates': 0.9141734616561541, 'learning_rate': 0.00015272909289681136, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9299821284107196}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 04:01:03,693][0m Trial 48 finished with value: 0.13159748511090222 and parameters: {'observation_period_num': 20, 'train_rates': 0.8834356098785623, 'learning_rate': 2.7854617529790625e-06, 'batch_size': 29, 'step_size': 9, 'gamma': 0.9107518018232339}. Best is trial 25 with value: 0.02596437460872241.[0m
[32m[I 2025-02-08 04:01:43,284][0m Trial 49 finished with value: 0.11452566087245941 and parameters: {'observation_period_num': 90, 'train_rates': 0.9578806897413167, 'learning_rate': 9.23439618848945e-05, 'batch_size': 164, 'step_size': 5, 'gamma': 0.9607232701532104}. Best is trial 25 with value: 0.02596437460872241.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-08 04:01:43,295][0m A new study created in memory with name: no-name-3319b6fa-8b80-462d-a919-6c8f202edaba[0m
[32m[I 2025-02-08 04:02:13,193][0m Trial 0 finished with value: 1.0969563722610474 and parameters: {'observation_period_num': 252, 'train_rates': 0.959987958466181, 'learning_rate': 1.2389670394139798e-06, 'batch_size': 207, 'step_size': 6, 'gamma': 0.7662246251558479}. Best is trial 0 with value: 1.0969563722610474.[0m
[32m[I 2025-02-08 04:02:52,900][0m Trial 1 finished with value: 0.10894367103435978 and parameters: {'observation_period_num': 111, 'train_rates': 0.9435144720072639, 'learning_rate': 6.510266944922848e-05, 'batch_size': 152, 'step_size': 13, 'gamma': 0.976967966171526}. Best is trial 1 with value: 0.10894367103435978.[0m
[32m[I 2025-02-08 04:03:38,373][0m Trial 2 finished with value: 0.16832732297959757 and parameters: {'observation_period_num': 24, 'train_rates': 0.60346731004398, 'learning_rate': 0.00010167278422288559, 'batch_size': 102, 'step_size': 5, 'gamma': 0.8285525914363123}. Best is trial 1 with value: 0.10894367103435978.[0m
[32m[I 2025-02-08 04:04:19,567][0m Trial 3 finished with value: 0.351038829733928 and parameters: {'observation_period_num': 43, 'train_rates': 0.7861985666663913, 'learning_rate': 1.5502987361259388e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.8463472477974732}. Best is trial 1 with value: 0.10894367103435978.[0m
[32m[I 2025-02-08 04:04:49,374][0m Trial 4 finished with value: 0.49747978446977587 and parameters: {'observation_period_num': 140, 'train_rates': 0.664661759360748, 'learning_rate': 3.0614775696760404e-06, 'batch_size': 159, 'step_size': 12, 'gamma': 0.9485913933465894}. Best is trial 1 with value: 0.10894367103435978.[0m
[32m[I 2025-02-08 04:05:18,470][0m Trial 5 finished with value: 0.6951068937778473 and parameters: {'observation_period_num': 126, 'train_rates': 0.8942842217567419, 'learning_rate': 5.8202605816636216e-06, 'batch_size': 198, 'step_size': 6, 'gamma': 0.7643161555942848}. Best is trial 1 with value: 0.10894367103435978.[0m
[32m[I 2025-02-08 04:05:44,555][0m Trial 6 finished with value: 0.07020875126928897 and parameters: {'observation_period_num': 57, 'train_rates': 0.8218558929194534, 'learning_rate': 0.0007024614899496589, 'batch_size': 238, 'step_size': 9, 'gamma': 0.9316368718643497}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:06:55,104][0m Trial 7 finished with value: 0.32103387453002513 and parameters: {'observation_period_num': 171, 'train_rates': 0.6078681314544561, 'learning_rate': 2.647199615482654e-05, 'batch_size': 62, 'step_size': 9, 'gamma': 0.7520301637579063}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:09:13,500][0m Trial 8 finished with value: 0.18523514225899454 and parameters: {'observation_period_num': 192, 'train_rates': 0.9701099542193858, 'learning_rate': 1.1278297824507494e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.8383689898081371}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:10:08,195][0m Trial 9 finished with value: 0.2219925862980992 and parameters: {'observation_period_num': 143, 'train_rates': 0.6815845038233395, 'learning_rate': 0.0008742365581879346, 'batch_size': 85, 'step_size': 14, 'gamma': 0.8665306316243545}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:10:32,441][0m Trial 10 finished with value: 0.08871362534938035 and parameters: {'observation_period_num': 68, 'train_rates': 0.8135965960029357, 'learning_rate': 0.0005447697879855016, 'batch_size': 244, 'step_size': 1, 'gamma': 0.9229071993698584}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:10:56,750][0m Trial 11 finished with value: 0.07876932001455711 and parameters: {'observation_period_num': 61, 'train_rates': 0.8076103422560506, 'learning_rate': 0.0009669347275237749, 'batch_size': 254, 'step_size': 1, 'gamma': 0.9214305606442985}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:11:21,464][0m Trial 12 finished with value: 0.16319189219314692 and parameters: {'observation_period_num': 78, 'train_rates': 0.808968874332727, 'learning_rate': 0.0002467038012500705, 'batch_size': 254, 'step_size': 1, 'gamma': 0.9051552687915437}. Best is trial 6 with value: 0.07020875126928897.[0m
[32m[I 2025-02-08 04:11:49,864][0m Trial 13 finished with value: 0.04052355361024016 and parameters: {'observation_period_num': 13, 'train_rates': 0.8712422987352856, 'learning_rate': 0.0002657281600935234, 'batch_size': 215, 'step_size': 10, 'gamma': 0.9061428992985548}. Best is trial 13 with value: 0.04052355361024016.[0m
[32m[I 2025-02-08 04:12:20,484][0m Trial 14 finished with value: 0.03468996791455608 and parameters: {'observation_period_num': 10, 'train_rates': 0.8593155835185087, 'learning_rate': 0.00022796356739517902, 'batch_size': 202, 'step_size': 10, 'gamma': 0.9867621950634765}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:12:53,018][0m Trial 15 finished with value: 0.03900811008323496 and parameters: {'observation_period_num': 12, 'train_rates': 0.8940817589451298, 'learning_rate': 0.00018757198517040536, 'batch_size': 192, 'step_size': 11, 'gamma': 0.9839994381097524}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:13:27,106][0m Trial 16 finished with value: 0.04594058465499144 and parameters: {'observation_period_num': 7, 'train_rates': 0.8899566132074427, 'learning_rate': 8.466636328713024e-05, 'batch_size': 177, 'step_size': 11, 'gamma': 0.9825198243695303}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:14:01,273][0m Trial 17 finished with value: 0.07711292848903306 and parameters: {'observation_period_num': 86, 'train_rates': 0.8637260269645544, 'learning_rate': 0.00021932892878252948, 'batch_size': 171, 'step_size': 8, 'gamma': 0.956255454523502}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:14:43,229][0m Trial 18 finished with value: 0.1877729568979229 and parameters: {'observation_period_num': 32, 'train_rates': 0.7484965921042624, 'learning_rate': 4.235217092657988e-05, 'batch_size': 127, 'step_size': 12, 'gamma': 0.9866785561200817}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:15:09,747][0m Trial 19 finished with value: 0.22560275392606854 and parameters: {'observation_period_num': 105, 'train_rates': 0.7479541068963517, 'learning_rate': 0.0001843747081937717, 'batch_size': 195, 'step_size': 8, 'gamma': 0.8009495405051896}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:15:37,277][0m Trial 20 finished with value: 0.18185248970985413 and parameters: {'observation_period_num': 228, 'train_rates': 0.9258272200321576, 'learning_rate': 0.00042416739752743223, 'batch_size': 220, 'step_size': 11, 'gamma': 0.9576019755112317}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:16:05,167][0m Trial 21 finished with value: 0.04226366734915278 and parameters: {'observation_period_num': 7, 'train_rates': 0.8575240797210736, 'learning_rate': 0.00013638133810696355, 'batch_size': 213, 'step_size': 10, 'gamma': 0.8947037657652835}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:16:40,145][0m Trial 22 finished with value: 0.05059119313955307 and parameters: {'observation_period_num': 30, 'train_rates': 0.9071178392037553, 'learning_rate': 0.00038649706409061825, 'batch_size': 182, 'step_size': 10, 'gamma': 0.8836222603113748}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:17:08,966][0m Trial 23 finished with value: 0.036642716048977945 and parameters: {'observation_period_num': 11, 'train_rates': 0.8534807278120388, 'learning_rate': 0.00024189146638760642, 'batch_size': 226, 'step_size': 13, 'gamma': 0.9596888588129144}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:17:35,405][0m Trial 24 finished with value: 0.08079458878853836 and parameters: {'observation_period_num': 46, 'train_rates': 0.8479055407666232, 'learning_rate': 4.6093803362315975e-05, 'batch_size': 230, 'step_size': 15, 'gamma': 0.9674006411486745}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:18:03,656][0m Trial 25 finished with value: 0.19407361968933784 and parameters: {'observation_period_num': 39, 'train_rates': 0.7559514286671002, 'learning_rate': 0.00010715371833286747, 'batch_size': 191, 'step_size': 13, 'gamma': 0.9418529151502352}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:18:33,520][0m Trial 26 finished with value: 0.04996078461408615 and parameters: {'observation_period_num': 21, 'train_rates': 0.9257307866262175, 'learning_rate': 0.0003432283434812464, 'batch_size': 225, 'step_size': 13, 'gamma': 0.9859005727444701}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:19:06,628][0m Trial 27 finished with value: 0.22483647974044407 and parameters: {'observation_period_num': 90, 'train_rates': 0.7741451138324213, 'learning_rate': 0.0001619557002126049, 'batch_size': 162, 'step_size': 12, 'gamma': 0.9619931949858249}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:19:47,486][0m Trial 28 finished with value: 0.0795783909010489 and parameters: {'observation_period_num': 51, 'train_rates': 0.835363417345807, 'learning_rate': 2.332947143633965e-05, 'batch_size': 141, 'step_size': 14, 'gamma': 0.9435521809005214}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:20:16,637][0m Trial 29 finished with value: 0.20644575357437134 and parameters: {'observation_period_num': 247, 'train_rates': 0.986606168656237, 'learning_rate': 5.052784175249989e-05, 'batch_size': 206, 'step_size': 7, 'gamma': 0.9742162196948015}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:24:14,922][0m Trial 30 finished with value: 0.2865765204683679 and parameters: {'observation_period_num': 8, 'train_rates': 0.7109570470296739, 'learning_rate': 1.4033491148416694e-06, 'batch_size': 20, 'step_size': 4, 'gamma': 0.7962940717596706}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:24:41,723][0m Trial 31 finished with value: 0.04242846541345641 and parameters: {'observation_period_num': 19, 'train_rates': 0.8835531142707814, 'learning_rate': 0.00026299920185365833, 'batch_size': 218, 'step_size': 10, 'gamma': 0.9088932460867813}. Best is trial 14 with value: 0.03468996791455608.[0m
[32m[I 2025-02-08 04:25:11,091][0m Trial 32 finished with value: 0.032432264663006 and parameters: {'observation_period_num': 5, 'train_rates': 0.8709579568052846, 'learning_rate': 0.0005153596688280365, 'batch_size': 206, 'step_size': 11, 'gamma': 0.9892732605242742}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:25:45,204][0m Trial 33 finished with value: 0.07021544873714447 and parameters: {'observation_period_num': 31, 'train_rates': 0.9405148327563542, 'learning_rate': 0.0005369616813628502, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9713123837129386}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:26:16,882][0m Trial 34 finished with value: 0.05326717823397282 and parameters: {'observation_period_num': 22, 'train_rates': 0.9122351087797868, 'learning_rate': 7.38636088892185e-05, 'batch_size': 203, 'step_size': 9, 'gamma': 0.9879972309216645}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:26:55,283][0m Trial 35 finished with value: 0.054979063090554045 and parameters: {'observation_period_num': 44, 'train_rates': 0.8324511587606677, 'learning_rate': 0.00012910471865921976, 'batch_size': 145, 'step_size': 12, 'gamma': 0.9691962848970415}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:27:22,811][0m Trial 36 finished with value: 0.052634116262197495 and parameters: {'observation_period_num': 33, 'train_rates': 0.9444475547923847, 'learning_rate': 0.0005015846448646963, 'batch_size': 236, 'step_size': 14, 'gamma': 0.9344710056813748}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:27:58,974][0m Trial 37 finished with value: 0.08073661327984671 and parameters: {'observation_period_num': 73, 'train_rates': 0.8756882248208143, 'learning_rate': 0.0003330324800533295, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9574986979840833}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:28:40,650][0m Trial 38 finished with value: 0.19745215238851793 and parameters: {'observation_period_num': 56, 'train_rates': 0.7843867224950589, 'learning_rate': 0.0007060819548384825, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9890516680568532}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:29:07,619][0m Trial 39 finished with value: 0.04662971552256699 and parameters: {'observation_period_num': 20, 'train_rates': 0.8454056510869873, 'learning_rate': 0.00018592576032399266, 'batch_size': 241, 'step_size': 7, 'gamma': 0.9479998951865993}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:30:00,846][0m Trial 40 finished with value: 0.23357677101667693 and parameters: {'observation_period_num': 117, 'train_rates': 0.9004989357513439, 'learning_rate': 8.031361890789892e-06, 'batch_size': 107, 'step_size': 9, 'gamma': 0.9749642689328722}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:30:30,473][0m Trial 41 finished with value: 0.04102349963637522 and parameters: {'observation_period_num': 5, 'train_rates': 0.8693984879817181, 'learning_rate': 0.00028955317901171665, 'batch_size': 209, 'step_size': 10, 'gamma': 0.8753272395669668}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:30:58,158][0m Trial 42 finished with value: 0.04930195642608992 and parameters: {'observation_period_num': 17, 'train_rates': 0.8795790138036217, 'learning_rate': 0.00010393983978298272, 'batch_size': 225, 'step_size': 11, 'gamma': 0.9264694506058958}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:31:28,223][0m Trial 43 finished with value: 0.059893861135107376 and parameters: {'observation_period_num': 42, 'train_rates': 0.8302799504690164, 'learning_rate': 0.00020703811761881718, 'batch_size': 201, 'step_size': 12, 'gamma': 0.8542516954337718}. Best is trial 32 with value: 0.032432264663006.[0m
[32m[I 2025-02-08 04:32:00,804][0m Trial 44 finished with value: 0.03148018376481149 and parameters: {'observation_period_num': 14, 'train_rates': 0.8528527798676642, 'learning_rate': 0.00073495641954545, 'batch_size': 182, 'step_size': 10, 'gamma': 0.9779346876394431}. Best is trial 44 with value: 0.03148018376481149.[0m
[32m[I 2025-02-08 04:32:28,727][0m Trial 45 finished with value: 0.1260418480715236 and parameters: {'observation_period_num': 149, 'train_rates': 0.802866445241651, 'learning_rate': 0.0007034724734701362, 'batch_size': 189, 'step_size': 13, 'gamma': 0.9786665351477642}. Best is trial 44 with value: 0.03148018376481149.[0m
[32m[I 2025-02-08 04:33:04,912][0m Trial 46 finished with value: 0.3225486672864919 and parameters: {'observation_period_num': 198, 'train_rates': 0.8578634362565413, 'learning_rate': 0.0008003311667974875, 'batch_size': 157, 'step_size': 9, 'gamma': 0.9534020703254111}. Best is trial 44 with value: 0.03148018376481149.[0m
[32m[I 2025-02-08 04:33:40,631][0m Trial 47 finished with value: 0.07905805086993788 and parameters: {'observation_period_num': 63, 'train_rates': 0.91406705123918, 'learning_rate': 0.0005496161575896638, 'batch_size': 176, 'step_size': 4, 'gamma': 0.9664751747717839}. Best is trial 44 with value: 0.03148018376481149.[0m
[32m[I 2025-02-08 04:34:04,061][0m Trial 48 finished with value: 0.04922262845775344 and parameters: {'observation_period_num': 27, 'train_rates': 0.8491335691222989, 'learning_rate': 0.0009878873335739328, 'batch_size': 250, 'step_size': 11, 'gamma': 0.9348528804658445}. Best is trial 44 with value: 0.03148018376481149.[0m
[32m[I 2025-02-08 04:34:45,725][0m Trial 49 finished with value: 0.032798316329717636 and parameters: {'observation_period_num': 14, 'train_rates': 0.9573195996966558, 'learning_rate': 0.00048378407830812096, 'batch_size': 152, 'step_size': 8, 'gamma': 0.9778911952620509}. Best is trial 44 with value: 0.03148018376481149.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-08 04:34:45,735][0m A new study created in memory with name: no-name-a60ec6ca-749c-4874-a801-b00c0a3baf08[0m
[32m[I 2025-02-08 04:35:08,922][0m Trial 0 finished with value: 0.34597694778114285 and parameters: {'observation_period_num': 199, 'train_rates': 0.6844650228231389, 'learning_rate': 7.755455077432778e-05, 'batch_size': 223, 'step_size': 5, 'gamma': 0.9020255106829657}. Best is trial 0 with value: 0.34597694778114285.[0m
[32m[I 2025-02-08 04:39:42,666][0m Trial 1 finished with value: 0.19420156069099903 and parameters: {'observation_period_num': 114, 'train_rates': 0.9045080211176846, 'learning_rate': 4.231562065383561e-06, 'batch_size': 20, 'step_size': 14, 'gamma': 0.7682406757023962}. Best is trial 1 with value: 0.19420156069099903.[0m
[32m[I 2025-02-08 04:40:17,979][0m Trial 2 finished with value: 0.21002027245987606 and parameters: {'observation_period_num': 113, 'train_rates': 0.6193407103131763, 'learning_rate': 0.00023444811852107233, 'batch_size': 130, 'step_size': 14, 'gamma': 0.9122763984866334}. Best is trial 1 with value: 0.19420156069099903.[0m
[32m[I 2025-02-08 04:41:52,189][0m Trial 3 finished with value: 0.05457897451790896 and parameters: {'observation_period_num': 30, 'train_rates': 0.812559747422646, 'learning_rate': 0.0004408196926103934, 'batch_size': 56, 'step_size': 2, 'gamma': 0.8106000336704395}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:42:29,413][0m Trial 4 finished with value: 0.4046383798122406 and parameters: {'observation_period_num': 246, 'train_rates': 0.9839944619822465, 'learning_rate': 1.1525380599936467e-05, 'batch_size': 166, 'step_size': 15, 'gamma': 0.8063937552854761}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:43:04,727][0m Trial 5 finished with value: 0.24956691603402834 and parameters: {'observation_period_num': 231, 'train_rates': 0.9322794584428592, 'learning_rate': 7.955944509201336e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.9862299105390526}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:43:48,147][0m Trial 6 finished with value: 0.10453400612437636 and parameters: {'observation_period_num': 77, 'train_rates': 0.9078989129670101, 'learning_rate': 0.00018974296806643267, 'batch_size': 135, 'step_size': 15, 'gamma': 0.9304837677696524}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:44:38,400][0m Trial 7 finished with value: 0.20326590005476564 and parameters: {'observation_period_num': 24, 'train_rates': 0.7510977034692107, 'learning_rate': 1.6514138932516765e-05, 'batch_size': 104, 'step_size': 11, 'gamma': 0.9332176127632013}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:45:15,257][0m Trial 8 finished with value: 0.20582361007109284 and parameters: {'observation_period_num': 75, 'train_rates': 0.6729374337136542, 'learning_rate': 8.026085767253954e-05, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9689113401319707}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:45:48,223][0m Trial 9 finished with value: 0.8643074647183294 and parameters: {'observation_period_num': 79, 'train_rates': 0.7463348100894573, 'learning_rate': 2.3829400046976794e-06, 'batch_size': 165, 'step_size': 3, 'gamma': 0.8101685212339876}. Best is trial 3 with value: 0.05457897451790896.[0m
[32m[I 2025-02-08 04:49:21,976][0m Trial 10 finished with value: 0.0354925130346888 and parameters: {'observation_period_num': 7, 'train_rates': 0.8392861992559854, 'learning_rate': 0.000891726187985168, 'batch_size': 25, 'step_size': 1, 'gamma': 0.8474365356691985}. Best is trial 10 with value: 0.0354925130346888.[0m
[32m[I 2025-02-08 04:52:31,390][0m Trial 11 finished with value: 0.03544931836237547 and parameters: {'observation_period_num': 5, 'train_rates': 0.8199332927454955, 'learning_rate': 0.0009250761221052209, 'batch_size': 28, 'step_size': 1, 'gamma': 0.8501782204719363}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 04:56:56,400][0m Trial 12 finished with value: 0.04335062206532088 and parameters: {'observation_period_num': 23, 'train_rates': 0.8287710807137367, 'learning_rate': 0.000711700093200876, 'batch_size': 20, 'step_size': 1, 'gamma': 0.861842708249383}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 04:58:15,834][0m Trial 13 finished with value: 0.1382982529754452 and parameters: {'observation_period_num': 170, 'train_rates': 0.853456931163664, 'learning_rate': 0.0009261432260697072, 'batch_size': 67, 'step_size': 5, 'gamma': 0.8563540571445554}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 04:59:32,050][0m Trial 14 finished with value: 0.1580510140417821 and parameters: {'observation_period_num': 11, 'train_rates': 0.7615414467445489, 'learning_rate': 0.0002787216569443268, 'batch_size': 68, 'step_size': 4, 'gamma': 0.8441066020124223}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:01:24,017][0m Trial 15 finished with value: 0.11426091758120391 and parameters: {'observation_period_num': 54, 'train_rates': 0.8639951032999219, 'learning_rate': 0.0009694324256705075, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8882015069476584}. Best is trial 11 with value: 0.03544931836237547.[0m
Early stopping at epoch 69
[32m[I 2025-02-08 05:02:03,417][0m Trial 16 finished with value: 0.601976290517602 and parameters: {'observation_period_num': 141, 'train_rates': 0.7856672316047814, 'learning_rate': 2.984207581410326e-05, 'batch_size': 93, 'step_size': 1, 'gamma': 0.8288478668141028}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:06:59,119][0m Trial 17 finished with value: 0.1767745128303182 and parameters: {'observation_period_num': 47, 'train_rates': 0.7198831017314536, 'learning_rate': 0.00014017526652536656, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7637120217493569}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:07:24,003][0m Trial 18 finished with value: 0.058286598765514265 and parameters: {'observation_period_num': 5, 'train_rates': 0.879917919024, 'learning_rate': 0.0004009699242154204, 'batch_size': 240, 'step_size': 3, 'gamma': 0.8759734096538274}. Best is trial 11 with value: 0.03544931836237547.[0m
Early stopping at epoch 49
[32m[I 2025-02-08 05:07:56,325][0m Trial 19 finished with value: 0.465665340423584 and parameters: {'observation_period_num': 55, 'train_rates': 0.9810115233640864, 'learning_rate': 3.6906991511102296e-05, 'batch_size': 96, 'step_size': 1, 'gamma': 0.7829365653414874}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:09:54,830][0m Trial 20 finished with value: 0.11948432198740168 and parameters: {'observation_period_num': 99, 'train_rates': 0.798624506458357, 'learning_rate': 0.0004405943601656387, 'batch_size': 43, 'step_size': 11, 'gamma': 0.8434645218414379}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:12:39,766][0m Trial 21 finished with value: 0.05096451630599277 and parameters: {'observation_period_num': 40, 'train_rates': 0.8382991244129585, 'learning_rate': 0.0007074572533153754, 'batch_size': 33, 'step_size': 1, 'gamma': 0.8689262365481177}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:13:51,165][0m Trial 22 finished with value: 0.03694190029451187 and parameters: {'observation_period_num': 10, 'train_rates': 0.8250653131993064, 'learning_rate': 0.0006757548684138672, 'batch_size': 77, 'step_size': 3, 'gamma': 0.8318459221839342}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:15:00,629][0m Trial 23 finished with value: 0.16878358022311316 and parameters: {'observation_period_num': 6, 'train_rates': 0.7865147140561385, 'learning_rate': 0.0004441996635567483, 'batch_size': 77, 'step_size': 4, 'gamma': 0.8330887569025548}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:17:26,649][0m Trial 24 finished with value: 0.062048141288196114 and parameters: {'observation_period_num': 65, 'train_rates': 0.8937585081113305, 'learning_rate': 0.0001635409032369371, 'batch_size': 39, 'step_size': 3, 'gamma': 0.7896021687383843}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:18:36,322][0m Trial 25 finished with value: 0.13068973132461872 and parameters: {'observation_period_num': 148, 'train_rates': 0.9435633999133308, 'learning_rate': 0.0005798352242007082, 'batch_size': 81, 'step_size': 2, 'gamma': 0.8877590537057568}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:20:12,808][0m Trial 26 finished with value: 0.05275078488909823 and parameters: {'observation_period_num': 34, 'train_rates': 0.826607063358601, 'learning_rate': 0.00029649437320595964, 'batch_size': 56, 'step_size': 2, 'gamma': 0.8232862903502128}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:23:09,976][0m Trial 27 finished with value: 0.04790693653833547 and parameters: {'observation_period_num': 24, 'train_rates': 0.8600685467477257, 'learning_rate': 0.000975105861103343, 'batch_size': 31, 'step_size': 4, 'gamma': 0.8517288455575307}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:23:53,348][0m Trial 28 finished with value: 0.900134246168114 and parameters: {'observation_period_num': 91, 'train_rates': 0.7067523663724756, 'learning_rate': 1.1321135676342141e-06, 'batch_size': 113, 'step_size': 6, 'gamma': 0.7836610608245047}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:24:21,782][0m Trial 29 finished with value: 0.3100309241533656 and parameters: {'observation_period_num': 179, 'train_rates': 0.7726289591193933, 'learning_rate': 0.00011100367198795128, 'batch_size': 181, 'step_size': 5, 'gamma': 0.8904864553389727}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:25:43,991][0m Trial 30 finished with value: 0.1997501132083915 and parameters: {'observation_period_num': 208, 'train_rates': 0.8139232745164559, 'learning_rate': 4.466101780775794e-05, 'batch_size': 62, 'step_size': 2, 'gamma': 0.9035254055646899}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:30:39,726][0m Trial 31 finished with value: 0.038983911793694453 and parameters: {'observation_period_num': 20, 'train_rates': 0.8362797322505068, 'learning_rate': 0.0006465492247626562, 'batch_size': 18, 'step_size': 1, 'gamma': 0.8642450993697743}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:33:39,822][0m Trial 32 finished with value: 0.03721081383427859 and parameters: {'observation_period_num': 13, 'train_rates': 0.8460394127921403, 'learning_rate': 0.0006310695833159663, 'batch_size': 30, 'step_size': 1, 'gamma': 0.8713735023268856}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:36:22,504][0m Trial 33 finished with value: 0.04704875108124553 and parameters: {'observation_period_num': 42, 'train_rates': 0.8818098224593035, 'learning_rate': 0.0002752309840223613, 'batch_size': 34, 'step_size': 3, 'gamma': 0.8357560173512656}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:37:35,657][0m Trial 34 finished with value: 0.06242399107836281 and parameters: {'observation_period_num': 12, 'train_rates': 0.9191720347900629, 'learning_rate': 0.00038064099691447964, 'batch_size': 82, 'step_size': 2, 'gamma': 0.8162502430133005}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:39:24,660][0m Trial 35 finished with value: 0.05148202533887319 and parameters: {'observation_period_num': 35, 'train_rates': 0.8043847052515726, 'learning_rate': 0.0005643764528002728, 'batch_size': 48, 'step_size': 1, 'gamma': 0.8751802356850228}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:40:14,292][0m Trial 36 finished with value: 0.5126062723614225 and parameters: {'observation_period_num': 5, 'train_rates': 0.848835626378174, 'learning_rate': 6.610120541433783e-06, 'batch_size': 114, 'step_size': 2, 'gamma': 0.84449501451278}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:43:48,190][0m Trial 37 finished with value: 0.08667046579325365 and parameters: {'observation_period_num': 114, 'train_rates': 0.8736974778098987, 'learning_rate': 0.00021799104970376723, 'batch_size': 25, 'step_size': 3, 'gamma': 0.7962023369300532}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:44:14,401][0m Trial 38 finished with value: 0.15324889182278817 and parameters: {'observation_period_num': 58, 'train_rates': 0.6381018318342337, 'learning_rate': 0.0009803760892594694, 'batch_size': 195, 'step_size': 5, 'gamma': 0.9174352228348501}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:45:55,862][0m Trial 39 finished with value: 0.160810341395564 and parameters: {'observation_period_num': 29, 'train_rates': 0.738516067532153, 'learning_rate': 0.00012531947418881792, 'batch_size': 51, 'step_size': 11, 'gamma': 0.7516109609513892}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:48:23,968][0m Trial 40 finished with value: 0.13293436683954732 and parameters: {'observation_period_num': 67, 'train_rates': 0.9466904789308881, 'learning_rate': 0.00035626793196515735, 'batch_size': 39, 'step_size': 4, 'gamma': 0.9450617656814575}. Best is trial 11 with value: 0.03544931836237547.[0m
[32m[I 2025-02-08 05:52:47,697][0m Trial 41 finished with value: 0.034820548801706944 and parameters: {'observation_period_num': 16, 'train_rates': 0.8160901019268822, 'learning_rate': 0.000608169918442636, 'batch_size': 20, 'step_size': 1, 'gamma': 0.8620190761820923}. Best is trial 41 with value: 0.034820548801706944.[0m
Early stopping at epoch 87
[32m[I 2025-02-08 05:53:23,428][0m Trial 42 finished with value: 0.07585665446474611 and parameters: {'observation_period_num': 18, 'train_rates': 0.8210293766460962, 'learning_rate': 0.0005459880916034368, 'batch_size': 147, 'step_size': 1, 'gamma': 0.8532075856854567}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 05:56:38,095][0m Trial 43 finished with value: 0.17131118263988762 and parameters: {'observation_period_num': 19, 'train_rates': 0.7699834269078785, 'learning_rate': 0.0007696696890407611, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8747404737269538}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 05:58:04,835][0m Trial 44 finished with value: 0.06576832619083411 and parameters: {'observation_period_num': 45, 'train_rates': 0.8057913802875428, 'learning_rate': 0.00020706163403221424, 'batch_size': 61, 'step_size': 2, 'gamma': 0.8037646540166129}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 06:03:37,631][0m Trial 45 finished with value: 0.043060572981311565 and parameters: {'observation_period_num': 31, 'train_rates': 0.8443638235165734, 'learning_rate': 0.0004962239926207647, 'batch_size': 16, 'step_size': 1, 'gamma': 0.8227741124647405}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 06:04:57,110][0m Trial 46 finished with value: 0.03891414682407981 and parameters: {'observation_period_num': 14, 'train_rates': 0.8952331117723935, 'learning_rate': 0.0007038462019187393, 'batch_size': 72, 'step_size': 3, 'gamma': 0.8990709141311358}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 06:06:54,323][0m Trial 47 finished with value: 0.18390665355237942 and parameters: {'observation_period_num': 25, 'train_rates': 0.7894985929552856, 'learning_rate': 5.9721811643337656e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8609198186267177}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 06:10:16,379][0m Trial 48 finished with value: 0.04456721040613876 and parameters: {'observation_period_num': 5, 'train_rates': 0.8634280913488422, 'learning_rate': 1.529147037059158e-05, 'batch_size': 27, 'step_size': 13, 'gamma': 0.8408970248906613}. Best is trial 41 with value: 0.034820548801706944.[0m
[32m[I 2025-02-08 06:11:16,830][0m Trial 49 finished with value: 0.06073883491403916 and parameters: {'observation_period_num': 50, 'train_rates': 0.82479256521739, 'learning_rate': 0.0002773137245073883, 'batch_size': 92, 'step_size': 2, 'gamma': 0.8834364687592448}. Best is trial 41 with value: 0.034820548801706944.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 15, 'train_rates': 0.8670853268220997, 'learning_rate': 0.0002690255145462724, 'batch_size': 32, 'step_size': 3, 'gamma': 0.9221219167540043}
Epoch 1/300, trend Loss: 0.2187 | 0.1251
Epoch 2/300, trend Loss: 0.1249 | 0.0862
Epoch 3/300, trend Loss: 0.1097 | 0.0714
Epoch 4/300, trend Loss: 0.1038 | 0.0656
Epoch 5/300, trend Loss: 0.0998 | 0.0632
Epoch 6/300, trend Loss: 0.0963 | 0.0603
Epoch 7/300, trend Loss: 0.0917 | 0.0581
Epoch 8/300, trend Loss: 0.0876 | 0.0519
Epoch 9/300, trend Loss: 0.0850 | 0.0535
Epoch 10/300, trend Loss: 0.0830 | 0.0538
Epoch 11/300, trend Loss: 0.0815 | 0.0508
Epoch 12/300, trend Loss: 0.0799 | 0.0500
Epoch 13/300, trend Loss: 0.0804 | 0.0495
Epoch 14/300, trend Loss: 0.0780 | 0.0458
Epoch 15/300, trend Loss: 0.0737 | 0.0443
Epoch 16/300, trend Loss: 0.0724 | 0.0438
Epoch 17/300, trend Loss: 0.0716 | 0.0440
Epoch 18/300, trend Loss: 0.0708 | 0.0446
Epoch 19/300, trend Loss: 0.0699 | 0.0444
Epoch 20/300, trend Loss: 0.0690 | 0.0442
Epoch 21/300, trend Loss: 0.0682 | 0.0440
Epoch 22/300, trend Loss: 0.0679 | 0.0429
Epoch 23/300, trend Loss: 0.0671 | 0.0416
Epoch 24/300, trend Loss: 0.0662 | 0.0420
Epoch 25/300, trend Loss: 0.0660 | 0.0433
Epoch 26/300, trend Loss: 0.0656 | 0.0431
Epoch 27/300, trend Loss: 0.0653 | 0.0447
Epoch 28/300, trend Loss: 0.0652 | 0.0459
Epoch 29/300, trend Loss: 0.0649 | 0.0451
Epoch 30/300, trend Loss: 0.0645 | 0.0442
Epoch 31/300, trend Loss: 0.0641 | 0.0430
Epoch 32/300, trend Loss: 0.0637 | 0.0420
Epoch 33/300, trend Loss: 0.0632 | 0.0412
Epoch 34/300, trend Loss: 0.0628 | 0.0405
Epoch 35/300, trend Loss: 0.0624 | 0.0398
Epoch 36/300, trend Loss: 0.0620 | 0.0392
Epoch 37/300, trend Loss: 0.0617 | 0.0382
Epoch 38/300, trend Loss: 0.0615 | 0.0373
Epoch 39/300, trend Loss: 0.0612 | 0.0363
Epoch 40/300, trend Loss: 0.0610 | 0.0349
Epoch 41/300, trend Loss: 0.0608 | 0.0340
Epoch 42/300, trend Loss: 0.0606 | 0.0327
Epoch 43/300, trend Loss: 0.0605 | 0.0315
Epoch 44/300, trend Loss: 0.0604 | 0.0308
Epoch 45/300, trend Loss: 0.0602 | 0.0301
Epoch 46/300, trend Loss: 0.0601 | 0.0296
Epoch 47/300, trend Loss: 0.0599 | 0.0294
Epoch 48/300, trend Loss: 0.0596 | 0.0292
Epoch 49/300, trend Loss: 0.0594 | 0.0293
Epoch 50/300, trend Loss: 0.0592 | 0.0294
Epoch 51/300, trend Loss: 0.0590 | 0.0297
Epoch 52/300, trend Loss: 0.0589 | 0.0301
Epoch 53/300, trend Loss: 0.0588 | 0.0302
Epoch 54/300, trend Loss: 0.0587 | 0.0304
Epoch 55/300, trend Loss: 0.0587 | 0.0304
Epoch 56/300, trend Loss: 0.0586 | 0.0304
Epoch 57/300, trend Loss: 0.0585 | 0.0299
Epoch 58/300, trend Loss: 0.0585 | 0.0295
Epoch 59/300, trend Loss: 0.0583 | 0.0293
Epoch 60/300, trend Loss: 0.0582 | 0.0291
Epoch 61/300, trend Loss: 0.0582 | 0.0292
Epoch 62/300, trend Loss: 0.0580 | 0.0292
Epoch 63/300, trend Loss: 0.0580 | 0.0293
Epoch 64/300, trend Loss: 0.0579 | 0.0292
Epoch 65/300, trend Loss: 0.0578 | 0.0290
Epoch 66/300, trend Loss: 0.0578 | 0.0288
Epoch 67/300, trend Loss: 0.0579 | 0.0286
Epoch 68/300, trend Loss: 0.0579 | 0.0285
Epoch 69/300, trend Loss: 0.0579 | 0.0285
Epoch 70/300, trend Loss: 0.0579 | 0.0285
Epoch 71/300, trend Loss: 0.0579 | 0.0285
Epoch 72/300, trend Loss: 0.0578 | 0.0286
Epoch 73/300, trend Loss: 0.0578 | 0.0287
Epoch 74/300, trend Loss: 0.0576 | 0.0286
Epoch 75/300, trend Loss: 0.0575 | 0.0288
Epoch 76/300, trend Loss: 0.0574 | 0.0289
Epoch 77/300, trend Loss: 0.0573 | 0.0289
Epoch 78/300, trend Loss: 0.0572 | 0.0290
Epoch 79/300, trend Loss: 0.0571 | 0.0292
Epoch 80/300, trend Loss: 0.0570 | 0.0291
Epoch 81/300, trend Loss: 0.0569 | 0.0293
Epoch 82/300, trend Loss: 0.0568 | 0.0294
Epoch 83/300, trend Loss: 0.0568 | 0.0293
Epoch 84/300, trend Loss: 0.0567 | 0.0294
Epoch 85/300, trend Loss: 0.0566 | 0.0294
Epoch 86/300, trend Loss: 0.0566 | 0.0294
Epoch 87/300, trend Loss: 0.0565 | 0.0294
Epoch 88/300, trend Loss: 0.0565 | 0.0294
Epoch 89/300, trend Loss: 0.0564 | 0.0293
Epoch 90/300, trend Loss: 0.0564 | 0.0293
Epoch 91/300, trend Loss: 0.0564 | 0.0293
Epoch 92/300, trend Loss: 0.0563 | 0.0293
Epoch 93/300, trend Loss: 0.0563 | 0.0293
Epoch 94/300, trend Loss: 0.0563 | 0.0292
Epoch 95/300, trend Loss: 0.0563 | 0.0292
Epoch 96/300, trend Loss: 0.0562 | 0.0292
Epoch 97/300, trend Loss: 0.0562 | 0.0292
Epoch 98/300, trend Loss: 0.0562 | 0.0292
Epoch 99/300, trend Loss: 0.0562 | 0.0292
Epoch 100/300, trend Loss: 0.0561 | 0.0292
Epoch 101/300, trend Loss: 0.0561 | 0.0292
Epoch 102/300, trend Loss: 0.0561 | 0.0292
Epoch 103/300, trend Loss: 0.0561 | 0.0292
Epoch 104/300, trend Loss: 0.0561 | 0.0291
Epoch 105/300, trend Loss: 0.0561 | 0.0291
Epoch 106/300, trend Loss: 0.0560 | 0.0291
Epoch 107/300, trend Loss: 0.0560 | 0.0291
Epoch 108/300, trend Loss: 0.0560 | 0.0291
Epoch 109/300, trend Loss: 0.0560 | 0.0291
Epoch 110/300, trend Loss: 0.0560 | 0.0290
Epoch 111/300, trend Loss: 0.0560 | 0.0290
Epoch 112/300, trend Loss: 0.0560 | 0.0290
Epoch 113/300, trend Loss: 0.0560 | 0.0290
Epoch 114/300, trend Loss: 0.0560 | 0.0290
Epoch 115/300, trend Loss: 0.0559 | 0.0290
Epoch 116/300, trend Loss: 0.0559 | 0.0289
Epoch 117/300, trend Loss: 0.0559 | 0.0289
Epoch 118/300, trend Loss: 0.0559 | 0.0289
Epoch 119/300, trend Loss: 0.0559 | 0.0289
Epoch 120/300, trend Loss: 0.0559 | 0.0289
Epoch 121/300, trend Loss: 0.0559 | 0.0289
Epoch 122/300, trend Loss: 0.0559 | 0.0289
Epoch 123/300, trend Loss: 0.0559 | 0.0289
Epoch 124/300, trend Loss: 0.0559 | 0.0289
Epoch 125/300, trend Loss: 0.0559 | 0.0289
Epoch 126/300, trend Loss: 0.0559 | 0.0288
Epoch 127/300, trend Loss: 0.0559 | 0.0288
Epoch 128/300, trend Loss: 0.0559 | 0.0288
Epoch 129/300, trend Loss: 0.0559 | 0.0288
Epoch 130/300, trend Loss: 0.0559 | 0.0288
Epoch 131/300, trend Loss: 0.0559 | 0.0288
Epoch 132/300, trend Loss: 0.0559 | 0.0288
Epoch 133/300, trend Loss: 0.0559 | 0.0288
Epoch 134/300, trend Loss: 0.0559 | 0.0288
Epoch 135/300, trend Loss: 0.0559 | 0.0288
Epoch 136/300, trend Loss: 0.0559 | 0.0288
Epoch 137/300, trend Loss: 0.0559 | 0.0288
Epoch 138/300, trend Loss: 0.0559 | 0.0288
Epoch 139/300, trend Loss: 0.0558 | 0.0288
Epoch 140/300, trend Loss: 0.0558 | 0.0288
Epoch 141/300, trend Loss: 0.0558 | 0.0288
Epoch 142/300, trend Loss: 0.0558 | 0.0288
Epoch 143/300, trend Loss: 0.0558 | 0.0288
Epoch 144/300, trend Loss: 0.0558 | 0.0288
Epoch 145/300, trend Loss: 0.0558 | 0.0288
Epoch 146/300, trend Loss: 0.0558 | 0.0288
Epoch 147/300, trend Loss: 0.0558 | 0.0287
Epoch 148/300, trend Loss: 0.0558 | 0.0287
Epoch 149/300, trend Loss: 0.0558 | 0.0287
Epoch 150/300, trend Loss: 0.0558 | 0.0287
Epoch 151/300, trend Loss: 0.0558 | 0.0287
Epoch 152/300, trend Loss: 0.0558 | 0.0287
Epoch 153/300, trend Loss: 0.0558 | 0.0287
Epoch 154/300, trend Loss: 0.0558 | 0.0287
Epoch 155/300, trend Loss: 0.0558 | 0.0287
Epoch 156/300, trend Loss: 0.0558 | 0.0287
Epoch 157/300, trend Loss: 0.0558 | 0.0287
Epoch 158/300, trend Loss: 0.0558 | 0.0287
Epoch 159/300, trend Loss: 0.0558 | 0.0287
Epoch 160/300, trend Loss: 0.0558 | 0.0287
Epoch 161/300, trend Loss: 0.0558 | 0.0287
Epoch 162/300, trend Loss: 0.0558 | 0.0287
Epoch 163/300, trend Loss: 0.0558 | 0.0287
Epoch 164/300, trend Loss: 0.0558 | 0.0287
Epoch 165/300, trend Loss: 0.0558 | 0.0287
Epoch 166/300, trend Loss: 0.0558 | 0.0287
Epoch 167/300, trend Loss: 0.0558 | 0.0287
Epoch 168/300, trend Loss: 0.0558 | 0.0287
Epoch 169/300, trend Loss: 0.0558 | 0.0287
Epoch 170/300, trend Loss: 0.0558 | 0.0287
Epoch 171/300, trend Loss: 0.0558 | 0.0287
Epoch 172/300, trend Loss: 0.0558 | 0.0287
Epoch 173/300, trend Loss: 0.0558 | 0.0287
Epoch 174/300, trend Loss: 0.0558 | 0.0287
Epoch 175/300, trend Loss: 0.0558 | 0.0287
Epoch 176/300, trend Loss: 0.0558 | 0.0287
Epoch 177/300, trend Loss: 0.0558 | 0.0287
Epoch 178/300, trend Loss: 0.0558 | 0.0287
Epoch 179/300, trend Loss: 0.0558 | 0.0287
Epoch 180/300, trend Loss: 0.0558 | 0.0287
Epoch 181/300, trend Loss: 0.0558 | 0.0287
Epoch 182/300, trend Loss: 0.0558 | 0.0287
Epoch 183/300, trend Loss: 0.0558 | 0.0287
Epoch 184/300, trend Loss: 0.0558 | 0.0287
Epoch 185/300, trend Loss: 0.0558 | 0.0287
Epoch 186/300, trend Loss: 0.0558 | 0.0287
Epoch 187/300, trend Loss: 0.0558 | 0.0287
Epoch 188/300, trend Loss: 0.0558 | 0.0287
Epoch 189/300, trend Loss: 0.0558 | 0.0287
Epoch 190/300, trend Loss: 0.0558 | 0.0287
Epoch 191/300, trend Loss: 0.0558 | 0.0287
Epoch 192/300, trend Loss: 0.0558 | 0.0287
Epoch 193/300, trend Loss: 0.0558 | 0.0287
Epoch 194/300, trend Loss: 0.0558 | 0.0287
Epoch 195/300, trend Loss: 0.0558 | 0.0287
Epoch 196/300, trend Loss: 0.0558 | 0.0287
Epoch 197/300, trend Loss: 0.0558 | 0.0287
Epoch 198/300, trend Loss: 0.0558 | 0.0287
Epoch 199/300, trend Loss: 0.0558 | 0.0287
Epoch 200/300, trend Loss: 0.0558 | 0.0287
Epoch 201/300, trend Loss: 0.0558 | 0.0287
Epoch 202/300, trend Loss: 0.0558 | 0.0287
Epoch 203/300, trend Loss: 0.0558 | 0.0287
Epoch 204/300, trend Loss: 0.0558 | 0.0287
Epoch 205/300, trend Loss: 0.0558 | 0.0287
Epoch 206/300, trend Loss: 0.0558 | 0.0287
Epoch 207/300, trend Loss: 0.0558 | 0.0287
Epoch 208/300, trend Loss: 0.0558 | 0.0287
Epoch 209/300, trend Loss: 0.0558 | 0.0287
Epoch 210/300, trend Loss: 0.0558 | 0.0287
Epoch 211/300, trend Loss: 0.0558 | 0.0287
Epoch 212/300, trend Loss: 0.0558 | 0.0287
Epoch 213/300, trend Loss: 0.0558 | 0.0287
Epoch 214/300, trend Loss: 0.0558 | 0.0287
Epoch 215/300, trend Loss: 0.0558 | 0.0287
Epoch 216/300, trend Loss: 0.0558 | 0.0287
Epoch 217/300, trend Loss: 0.0558 | 0.0287
Epoch 218/300, trend Loss: 0.0558 | 0.0287
Epoch 219/300, trend Loss: 0.0558 | 0.0287
Epoch 220/300, trend Loss: 0.0558 | 0.0287
Epoch 221/300, trend Loss: 0.0558 | 0.0287
Epoch 222/300, trend Loss: 0.0558 | 0.0287
Epoch 223/300, trend Loss: 0.0558 | 0.0287
Epoch 224/300, trend Loss: 0.0558 | 0.0287
Epoch 225/300, trend Loss: 0.0558 | 0.0287
Epoch 226/300, trend Loss: 0.0558 | 0.0287
Epoch 227/300, trend Loss: 0.0558 | 0.0287
Epoch 228/300, trend Loss: 0.0558 | 0.0287
Epoch 229/300, trend Loss: 0.0558 | 0.0287
Epoch 230/300, trend Loss: 0.0558 | 0.0287
Epoch 231/300, trend Loss: 0.0558 | 0.0287
Epoch 232/300, trend Loss: 0.0558 | 0.0287
Epoch 233/300, trend Loss: 0.0558 | 0.0287
Epoch 234/300, trend Loss: 0.0558 | 0.0287
Epoch 235/300, trend Loss: 0.0558 | 0.0287
Epoch 236/300, trend Loss: 0.0558 | 0.0287
Epoch 237/300, trend Loss: 0.0558 | 0.0287
Epoch 238/300, trend Loss: 0.0558 | 0.0287
Epoch 239/300, trend Loss: 0.0558 | 0.0287
Epoch 240/300, trend Loss: 0.0558 | 0.0287
Epoch 241/300, trend Loss: 0.0558 | 0.0287
Epoch 242/300, trend Loss: 0.0558 | 0.0287
Epoch 243/300, trend Loss: 0.0558 | 0.0287
Epoch 244/300, trend Loss: 0.0558 | 0.0287
Epoch 245/300, trend Loss: 0.0558 | 0.0287
Epoch 246/300, trend Loss: 0.0558 | 0.0287
Epoch 247/300, trend Loss: 0.0558 | 0.0287
Epoch 248/300, trend Loss: 0.0558 | 0.0287
Epoch 249/300, trend Loss: 0.0558 | 0.0287
Epoch 250/300, trend Loss: 0.0558 | 0.0287
Epoch 251/300, trend Loss: 0.0558 | 0.0287
Epoch 252/300, trend Loss: 0.0558 | 0.0287
Epoch 253/300, trend Loss: 0.0558 | 0.0287
Epoch 254/300, trend Loss: 0.0558 | 0.0287
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.9239309820745015, 'learning_rate': 0.0008828982363217428, 'batch_size': 220, 'step_size': 13, 'gamma': 0.8378389084898721}
Epoch 1/300, seasonal_0 Loss: 0.7126 | 0.3009
Epoch 2/300, seasonal_0 Loss: 0.2135 | 0.2034
Epoch 3/300, seasonal_0 Loss: 0.1993 | 0.4673
Epoch 4/300, seasonal_0 Loss: 0.1838 | 0.2092
Epoch 5/300, seasonal_0 Loss: 0.1569 | 0.1485
Epoch 6/300, seasonal_0 Loss: 0.1448 | 0.1143
Epoch 7/300, seasonal_0 Loss: 0.1243 | 0.1145
Epoch 8/300, seasonal_0 Loss: 0.1479 | 0.1199
Epoch 9/300, seasonal_0 Loss: 0.1804 | 0.1311
Epoch 10/300, seasonal_0 Loss: 0.1409 | 0.1042
Epoch 11/300, seasonal_0 Loss: 0.1319 | 0.1237
Epoch 12/300, seasonal_0 Loss: 0.1328 | 0.0928
Epoch 13/300, seasonal_0 Loss: 0.1138 | 0.0860
Epoch 14/300, seasonal_0 Loss: 0.1280 | 0.0888
Epoch 15/300, seasonal_0 Loss: 0.1148 | 0.1065
Epoch 16/300, seasonal_0 Loss: 0.1207 | 0.0857
Epoch 17/300, seasonal_0 Loss: 0.1074 | 0.1413
Epoch 18/300, seasonal_0 Loss: 0.1082 | 0.1119
Epoch 19/300, seasonal_0 Loss: 0.1018 | 0.0760
Epoch 20/300, seasonal_0 Loss: 0.1002 | 0.1006
Epoch 21/300, seasonal_0 Loss: 0.0997 | 0.0706
Epoch 22/300, seasonal_0 Loss: 0.0991 | 0.1334
Epoch 23/300, seasonal_0 Loss: 0.0976 | 0.0737
Epoch 24/300, seasonal_0 Loss: 0.0906 | 0.0686
Epoch 25/300, seasonal_0 Loss: 0.0874 | 0.0613
Epoch 26/300, seasonal_0 Loss: 0.0874 | 0.0632
Epoch 27/300, seasonal_0 Loss: 0.0858 | 0.0716
Epoch 28/300, seasonal_0 Loss: 0.0851 | 0.0561
Epoch 29/300, seasonal_0 Loss: 0.0840 | 0.0555
Epoch 30/300, seasonal_0 Loss: 0.0829 | 0.0659
Epoch 31/300, seasonal_0 Loss: 0.0823 | 0.0542
Epoch 32/300, seasonal_0 Loss: 0.0806 | 0.0519
Epoch 33/300, seasonal_0 Loss: 0.0804 | 0.0553
Epoch 34/300, seasonal_0 Loss: 0.0793 | 0.0527
Epoch 35/300, seasonal_0 Loss: 0.0787 | 0.0505
Epoch 36/300, seasonal_0 Loss: 0.0785 | 0.0522
Epoch 37/300, seasonal_0 Loss: 0.0777 | 0.0515
Epoch 38/300, seasonal_0 Loss: 0.0772 | 0.0494
Epoch 39/300, seasonal_0 Loss: 0.0767 | 0.0493
Epoch 40/300, seasonal_0 Loss: 0.0763 | 0.0496
Epoch 41/300, seasonal_0 Loss: 0.0759 | 0.0494
Epoch 42/300, seasonal_0 Loss: 0.0757 | 0.0488
Epoch 43/300, seasonal_0 Loss: 0.0757 | 0.0494
Epoch 44/300, seasonal_0 Loss: 0.0755 | 0.0496
Epoch 45/300, seasonal_0 Loss: 0.0752 | 0.0486
Epoch 46/300, seasonal_0 Loss: 0.0753 | 0.0481
Epoch 47/300, seasonal_0 Loss: 0.0758 | 0.0477
Epoch 48/300, seasonal_0 Loss: 0.0755 | 0.0474
Epoch 49/300, seasonal_0 Loss: 0.0745 | 0.0476
Epoch 50/300, seasonal_0 Loss: 0.0749 | 0.0465
Epoch 51/300, seasonal_0 Loss: 0.0753 | 0.0476
Epoch 52/300, seasonal_0 Loss: 0.0755 | 0.0477
Epoch 53/300, seasonal_0 Loss: 0.0761 | 0.0495
Epoch 54/300, seasonal_0 Loss: 0.0791 | 0.0496
Epoch 55/300, seasonal_0 Loss: 0.0834 | 0.0498
Epoch 56/300, seasonal_0 Loss: 0.0783 | 0.0478
Epoch 57/300, seasonal_0 Loss: 0.0781 | 0.0480
Epoch 58/300, seasonal_0 Loss: 0.0752 | 0.0475
Epoch 59/300, seasonal_0 Loss: 0.0740 | 0.0476
Epoch 60/300, seasonal_0 Loss: 0.0742 | 0.0451
Epoch 61/300, seasonal_0 Loss: 0.0743 | 0.0449
Epoch 62/300, seasonal_0 Loss: 0.0731 | 0.0452
Epoch 63/300, seasonal_0 Loss: 0.0728 | 0.0440
Epoch 64/300, seasonal_0 Loss: 0.0727 | 0.0449
Epoch 65/300, seasonal_0 Loss: 0.0720 | 0.0441
Epoch 66/300, seasonal_0 Loss: 0.0715 | 0.0424
Epoch 67/300, seasonal_0 Loss: 0.0706 | 0.0428
Epoch 68/300, seasonal_0 Loss: 0.0704 | 0.0419
Epoch 69/300, seasonal_0 Loss: 0.0700 | 0.0418
Epoch 70/300, seasonal_0 Loss: 0.0698 | 0.0420
Epoch 71/300, seasonal_0 Loss: 0.0697 | 0.0413
Epoch 72/300, seasonal_0 Loss: 0.0695 | 0.0413
Epoch 73/300, seasonal_0 Loss: 0.0694 | 0.0417
Epoch 74/300, seasonal_0 Loss: 0.0695 | 0.0418
Epoch 75/300, seasonal_0 Loss: 0.0698 | 0.0419
Epoch 76/300, seasonal_0 Loss: 0.0700 | 0.0413
Epoch 77/300, seasonal_0 Loss: 0.0697 | 0.0408
Epoch 78/300, seasonal_0 Loss: 0.0692 | 0.0405
Epoch 79/300, seasonal_0 Loss: 0.0688 | 0.0401
Epoch 80/300, seasonal_0 Loss: 0.0687 | 0.0399
Epoch 81/300, seasonal_0 Loss: 0.0686 | 0.0399
Epoch 82/300, seasonal_0 Loss: 0.0685 | 0.0396
Epoch 83/300, seasonal_0 Loss: 0.0684 | 0.0397
Epoch 84/300, seasonal_0 Loss: 0.0682 | 0.0398
Epoch 85/300, seasonal_0 Loss: 0.0682 | 0.0403
Epoch 86/300, seasonal_0 Loss: 0.0685 | 0.0397
Epoch 87/300, seasonal_0 Loss: 0.0687 | 0.0392
Epoch 88/300, seasonal_0 Loss: 0.0683 | 0.0387
Epoch 89/300, seasonal_0 Loss: 0.0679 | 0.0385
Epoch 90/300, seasonal_0 Loss: 0.0677 | 0.0383
Epoch 91/300, seasonal_0 Loss: 0.0677 | 0.0383
Epoch 92/300, seasonal_0 Loss: 0.0679 | 0.0381
Epoch 93/300, seasonal_0 Loss: 0.0679 | 0.0397
Epoch 94/300, seasonal_0 Loss: 0.0674 | 0.0388
Epoch 95/300, seasonal_0 Loss: 0.0672 | 0.0383
Epoch 96/300, seasonal_0 Loss: 0.0669 | 0.0380
Epoch 97/300, seasonal_0 Loss: 0.0668 | 0.0378
Epoch 98/300, seasonal_0 Loss: 0.0666 | 0.0377
Epoch 99/300, seasonal_0 Loss: 0.0665 | 0.0376
Epoch 100/300, seasonal_0 Loss: 0.0664 | 0.0376
Epoch 101/300, seasonal_0 Loss: 0.0663 | 0.0374
Epoch 102/300, seasonal_0 Loss: 0.0662 | 0.0374
Epoch 103/300, seasonal_0 Loss: 0.0661 | 0.0372
Epoch 104/300, seasonal_0 Loss: 0.0660 | 0.0370
Epoch 105/300, seasonal_0 Loss: 0.0659 | 0.0369
Epoch 106/300, seasonal_0 Loss: 0.0659 | 0.0367
Epoch 107/300, seasonal_0 Loss: 0.0658 | 0.0366
Epoch 108/300, seasonal_0 Loss: 0.0657 | 0.0364
Epoch 109/300, seasonal_0 Loss: 0.0657 | 0.0364
Epoch 110/300, seasonal_0 Loss: 0.0656 | 0.0362
Epoch 111/300, seasonal_0 Loss: 0.0655 | 0.0363
Epoch 112/300, seasonal_0 Loss: 0.0655 | 0.0362
Epoch 113/300, seasonal_0 Loss: 0.0654 | 0.0364
Epoch 114/300, seasonal_0 Loss: 0.0653 | 0.0365
Epoch 115/300, seasonal_0 Loss: 0.0653 | 0.0367
Epoch 116/300, seasonal_0 Loss: 0.0654 | 0.0367
Epoch 117/300, seasonal_0 Loss: 0.0656 | 0.0368
Epoch 118/300, seasonal_0 Loss: 0.0656 | 0.0366
Epoch 119/300, seasonal_0 Loss: 0.0658 | 0.0357
Epoch 120/300, seasonal_0 Loss: 0.0661 | 0.0368
Epoch 121/300, seasonal_0 Loss: 0.0664 | 0.0355
Epoch 122/300, seasonal_0 Loss: 0.0670 | 0.0381
Epoch 123/300, seasonal_0 Loss: 0.0674 | 0.0360
Epoch 124/300, seasonal_0 Loss: 0.0674 | 0.0406
Epoch 125/300, seasonal_0 Loss: 0.0668 | 0.0364
Epoch 126/300, seasonal_0 Loss: 0.0660 | 0.0403
Epoch 127/300, seasonal_0 Loss: 0.0654 | 0.0356
Epoch 128/300, seasonal_0 Loss: 0.0650 | 0.0377
Epoch 129/300, seasonal_0 Loss: 0.0648 | 0.0356
Epoch 130/300, seasonal_0 Loss: 0.0646 | 0.0367
Epoch 131/300, seasonal_0 Loss: 0.0645 | 0.0359
Epoch 132/300, seasonal_0 Loss: 0.0644 | 0.0361
Epoch 133/300, seasonal_0 Loss: 0.0644 | 0.0357
Epoch 134/300, seasonal_0 Loss: 0.0643 | 0.0357
Epoch 135/300, seasonal_0 Loss: 0.0643 | 0.0357
Epoch 136/300, seasonal_0 Loss: 0.0642 | 0.0357
Epoch 137/300, seasonal_0 Loss: 0.0642 | 0.0357
Epoch 138/300, seasonal_0 Loss: 0.0642 | 0.0357
Epoch 139/300, seasonal_0 Loss: 0.0641 | 0.0356
Epoch 140/300, seasonal_0 Loss: 0.0641 | 0.0356
Epoch 141/300, seasonal_0 Loss: 0.0641 | 0.0355
Epoch 142/300, seasonal_0 Loss: 0.0641 | 0.0355
Epoch 143/300, seasonal_0 Loss: 0.0640 | 0.0355
Epoch 144/300, seasonal_0 Loss: 0.0640 | 0.0355
Epoch 145/300, seasonal_0 Loss: 0.0640 | 0.0355
Epoch 146/300, seasonal_0 Loss: 0.0640 | 0.0354
Epoch 147/300, seasonal_0 Loss: 0.0639 | 0.0354
Epoch 148/300, seasonal_0 Loss: 0.0639 | 0.0354
Epoch 149/300, seasonal_0 Loss: 0.0639 | 0.0354
Epoch 150/300, seasonal_0 Loss: 0.0639 | 0.0354
Epoch 151/300, seasonal_0 Loss: 0.0638 | 0.0353
Epoch 152/300, seasonal_0 Loss: 0.0638 | 0.0353
Epoch 153/300, seasonal_0 Loss: 0.0638 | 0.0353
Epoch 154/300, seasonal_0 Loss: 0.0638 | 0.0353
Epoch 155/300, seasonal_0 Loss: 0.0638 | 0.0353
Epoch 156/300, seasonal_0 Loss: 0.0637 | 0.0353
Epoch 157/300, seasonal_0 Loss: 0.0637 | 0.0352
Epoch 158/300, seasonal_0 Loss: 0.0637 | 0.0352
Epoch 159/300, seasonal_0 Loss: 0.0637 | 0.0352
Epoch 160/300, seasonal_0 Loss: 0.0637 | 0.0352
Epoch 161/300, seasonal_0 Loss: 0.0636 | 0.0352
Epoch 162/300, seasonal_0 Loss: 0.0636 | 0.0352
Epoch 163/300, seasonal_0 Loss: 0.0636 | 0.0352
Epoch 164/300, seasonal_0 Loss: 0.0636 | 0.0352
Epoch 165/300, seasonal_0 Loss: 0.0636 | 0.0351
Epoch 166/300, seasonal_0 Loss: 0.0636 | 0.0351
Epoch 167/300, seasonal_0 Loss: 0.0636 | 0.0351
Epoch 168/300, seasonal_0 Loss: 0.0635 | 0.0351
Epoch 169/300, seasonal_0 Loss: 0.0635 | 0.0351
Epoch 170/300, seasonal_0 Loss: 0.0635 | 0.0351
Epoch 171/300, seasonal_0 Loss: 0.0635 | 0.0351
Epoch 172/300, seasonal_0 Loss: 0.0635 | 0.0351
Epoch 173/300, seasonal_0 Loss: 0.0635 | 0.0351
Epoch 174/300, seasonal_0 Loss: 0.0635 | 0.0350
Epoch 175/300, seasonal_0 Loss: 0.0635 | 0.0350
Epoch 176/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 177/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 178/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 179/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 180/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 181/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 182/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 183/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 184/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 185/300, seasonal_0 Loss: 0.0634 | 0.0350
Epoch 186/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 187/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 188/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 189/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 190/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 191/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 192/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 193/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 194/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 195/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 196/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 197/300, seasonal_0 Loss: 0.0633 | 0.0349
Epoch 198/300, seasonal_0 Loss: 0.0632 | 0.0349
Epoch 199/300, seasonal_0 Loss: 0.0632 | 0.0349
Epoch 200/300, seasonal_0 Loss: 0.0632 | 0.0349
Epoch 201/300, seasonal_0 Loss: 0.0632 | 0.0349
Epoch 202/300, seasonal_0 Loss: 0.0632 | 0.0349
Epoch 203/300, seasonal_0 Loss: 0.0632 | 0.0349
Epoch 204/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 205/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 206/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 207/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 208/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 209/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 210/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 211/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 212/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 213/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 214/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 215/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 216/300, seasonal_0 Loss: 0.0632 | 0.0348
Epoch 217/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 218/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 219/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 220/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 221/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 222/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 223/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 224/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 225/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 226/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 227/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 228/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 229/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 230/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 231/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 232/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 233/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 234/300, seasonal_0 Loss: 0.0631 | 0.0348
Epoch 235/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 236/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 237/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 238/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 239/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 240/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 241/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 242/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 243/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 244/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 245/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 246/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 247/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 248/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 249/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 250/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 251/300, seasonal_0 Loss: 0.0631 | 0.0347
Epoch 252/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 253/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 254/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 255/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 256/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 257/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 258/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 259/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 260/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 261/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 262/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 263/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 264/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 265/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 266/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 267/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 268/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 269/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 270/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 271/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 272/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 273/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 274/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 275/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 276/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 277/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 278/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 279/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 280/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 281/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 282/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 283/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 284/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 285/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 286/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 287/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 288/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 289/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 290/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 291/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 292/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 293/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 294/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 295/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 296/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 297/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 298/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 299/300, seasonal_0 Loss: 0.0630 | 0.0347
Epoch 300/300, seasonal_0 Loss: 0.0630 | 0.0347
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.8661508565208573, 'learning_rate': 0.0001189008328562521, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9764428000601195}
Epoch 1/300, seasonal_1 Loss: 0.3032 | 0.2546
Epoch 2/300, seasonal_1 Loss: 0.1756 | 0.2167
Epoch 3/300, seasonal_1 Loss: 0.1564 | 0.1742
Epoch 4/300, seasonal_1 Loss: 0.1433 | 0.1444
Epoch 5/300, seasonal_1 Loss: 0.1328 | 0.1198
Epoch 6/300, seasonal_1 Loss: 0.1317 | 0.1144
Epoch 7/300, seasonal_1 Loss: 0.1365 | 0.1239
Epoch 8/300, seasonal_1 Loss: 0.1363 | 0.1009
Epoch 9/300, seasonal_1 Loss: 0.1274 | 0.1013
Epoch 10/300, seasonal_1 Loss: 0.1250 | 0.1126
Epoch 11/300, seasonal_1 Loss: 0.1335 | 0.1341
Epoch 12/300, seasonal_1 Loss: 0.1409 | 0.1386
Epoch 13/300, seasonal_1 Loss: 0.1225 | 0.1022
Epoch 14/300, seasonal_1 Loss: 0.1060 | 0.0748
Epoch 15/300, seasonal_1 Loss: 0.1066 | 0.0756
Epoch 16/300, seasonal_1 Loss: 0.1064 | 0.0731
Epoch 17/300, seasonal_1 Loss: 0.1026 | 0.0687
Epoch 18/300, seasonal_1 Loss: 0.1001 | 0.0665
Epoch 19/300, seasonal_1 Loss: 0.0983 | 0.0648
Epoch 20/300, seasonal_1 Loss: 0.0972 | 0.0635
Epoch 21/300, seasonal_1 Loss: 0.0964 | 0.0624
Epoch 22/300, seasonal_1 Loss: 0.0956 | 0.0614
Epoch 23/300, seasonal_1 Loss: 0.0948 | 0.0607
Epoch 24/300, seasonal_1 Loss: 0.0942 | 0.0608
Epoch 25/300, seasonal_1 Loss: 0.0940 | 0.0621
Epoch 26/300, seasonal_1 Loss: 0.0942 | 0.0669
Epoch 27/300, seasonal_1 Loss: 0.0946 | 0.0735
Epoch 28/300, seasonal_1 Loss: 0.0945 | 0.0778
Epoch 29/300, seasonal_1 Loss: 0.0931 | 0.0738
Epoch 30/300, seasonal_1 Loss: 0.0905 | 0.0626
Epoch 31/300, seasonal_1 Loss: 0.0880 | 0.0550
Epoch 32/300, seasonal_1 Loss: 0.0889 | 0.0555
Epoch 33/300, seasonal_1 Loss: 0.0953 | 0.0683
Epoch 34/300, seasonal_1 Loss: 0.0951 | 0.0618
Epoch 35/300, seasonal_1 Loss: 0.0901 | 0.0544
Epoch 36/300, seasonal_1 Loss: 0.0870 | 0.0508
Epoch 37/300, seasonal_1 Loss: 0.0844 | 0.0488
Epoch 38/300, seasonal_1 Loss: 0.0877 | 0.0529
Epoch 39/300, seasonal_1 Loss: 0.0961 | 0.0669
Epoch 40/300, seasonal_1 Loss: 0.0978 | 0.0827
Epoch 41/300, seasonal_1 Loss: 0.0879 | 0.0699
Epoch 42/300, seasonal_1 Loss: 0.0834 | 0.0467
Epoch 43/300, seasonal_1 Loss: 0.0826 | 0.0472
Epoch 44/300, seasonal_1 Loss: 0.0825 | 0.0488
Epoch 45/300, seasonal_1 Loss: 0.0812 | 0.0460
Epoch 46/300, seasonal_1 Loss: 0.0793 | 0.0433
Epoch 47/300, seasonal_1 Loss: 0.0784 | 0.0458
Epoch 48/300, seasonal_1 Loss: 0.0786 | 0.0495
Epoch 49/300, seasonal_1 Loss: 0.0792 | 0.0486
Epoch 50/300, seasonal_1 Loss: 0.0794 | 0.0459
Epoch 51/300, seasonal_1 Loss: 0.0788 | 0.0446
Epoch 52/300, seasonal_1 Loss: 0.0773 | 0.0432
Epoch 53/300, seasonal_1 Loss: 0.0757 | 0.0416
Epoch 54/300, seasonal_1 Loss: 0.0753 | 0.0402
Epoch 55/300, seasonal_1 Loss: 0.0756 | 0.0399
Epoch 56/300, seasonal_1 Loss: 0.0761 | 0.0404
Epoch 57/300, seasonal_1 Loss: 0.0762 | 0.0404
Epoch 58/300, seasonal_1 Loss: 0.0755 | 0.0396
Epoch 59/300, seasonal_1 Loss: 0.0745 | 0.0393
Epoch 60/300, seasonal_1 Loss: 0.0739 | 0.0406
Epoch 61/300, seasonal_1 Loss: 0.0742 | 0.0421
Epoch 62/300, seasonal_1 Loss: 0.0755 | 0.0425
Epoch 63/300, seasonal_1 Loss: 0.0778 | 0.0433
Epoch 64/300, seasonal_1 Loss: 0.0795 | 0.0453
Epoch 65/300, seasonal_1 Loss: 0.0779 | 0.0467
Epoch 66/300, seasonal_1 Loss: 0.0748 | 0.0438
Epoch 67/300, seasonal_1 Loss: 0.0745 | 0.0390
Epoch 68/300, seasonal_1 Loss: 0.0769 | 0.0410
Epoch 69/300, seasonal_1 Loss: 0.0803 | 0.0515
Epoch 70/300, seasonal_1 Loss: 0.0809 | 0.0501
Epoch 71/300, seasonal_1 Loss: 0.0759 | 0.0389
Epoch 72/300, seasonal_1 Loss: 0.0734 | 0.0454
Epoch 73/300, seasonal_1 Loss: 0.0740 | 0.0483
Epoch 74/300, seasonal_1 Loss: 0.0766 | 0.0450
Epoch 75/300, seasonal_1 Loss: 0.0760 | 0.0435
Epoch 76/300, seasonal_1 Loss: 0.0721 | 0.0416
Epoch 77/300, seasonal_1 Loss: 0.0706 | 0.0372
Epoch 78/300, seasonal_1 Loss: 0.0708 | 0.0362
Epoch 79/300, seasonal_1 Loss: 0.0708 | 0.0361
Epoch 80/300, seasonal_1 Loss: 0.0704 | 0.0363
Epoch 81/300, seasonal_1 Loss: 0.0698 | 0.0367
Epoch 82/300, seasonal_1 Loss: 0.0695 | 0.0370
Epoch 83/300, seasonal_1 Loss: 0.0694 | 0.0373
Epoch 84/300, seasonal_1 Loss: 0.0695 | 0.0374
Epoch 85/300, seasonal_1 Loss: 0.0692 | 0.0371
Epoch 86/300, seasonal_1 Loss: 0.0686 | 0.0364
Epoch 87/300, seasonal_1 Loss: 0.0683 | 0.0355
Epoch 88/300, seasonal_1 Loss: 0.0683 | 0.0349
Epoch 89/300, seasonal_1 Loss: 0.0684 | 0.0346
Epoch 90/300, seasonal_1 Loss: 0.0683 | 0.0344
Epoch 91/300, seasonal_1 Loss: 0.0679 | 0.0343
Epoch 92/300, seasonal_1 Loss: 0.0674 | 0.0343
Epoch 93/300, seasonal_1 Loss: 0.0672 | 0.0345
Epoch 94/300, seasonal_1 Loss: 0.0673 | 0.0349
Epoch 95/300, seasonal_1 Loss: 0.0676 | 0.0354
Epoch 96/300, seasonal_1 Loss: 0.0675 | 0.0357
Epoch 97/300, seasonal_1 Loss: 0.0670 | 0.0354
Epoch 98/300, seasonal_1 Loss: 0.0664 | 0.0345
Epoch 99/300, seasonal_1 Loss: 0.0664 | 0.0336
Epoch 100/300, seasonal_1 Loss: 0.0667 | 0.0332
Epoch 101/300, seasonal_1 Loss: 0.0671 | 0.0332
Epoch 102/300, seasonal_1 Loss: 0.0669 | 0.0332
Epoch 103/300, seasonal_1 Loss: 0.0662 | 0.0328
Epoch 104/300, seasonal_1 Loss: 0.0655 | 0.0326
Epoch 105/300, seasonal_1 Loss: 0.0654 | 0.0332
Epoch 106/300, seasonal_1 Loss: 0.0658 | 0.0340
Epoch 107/300, seasonal_1 Loss: 0.0664 | 0.0348
Epoch 108/300, seasonal_1 Loss: 0.0663 | 0.0354
Epoch 109/300, seasonal_1 Loss: 0.0655 | 0.0354
Epoch 110/300, seasonal_1 Loss: 0.0649 | 0.0341
Epoch 111/300, seasonal_1 Loss: 0.0652 | 0.0328
Epoch 112/300, seasonal_1 Loss: 0.0657 | 0.0323
Epoch 113/300, seasonal_1 Loss: 0.0657 | 0.0325
Epoch 114/300, seasonal_1 Loss: 0.0652 | 0.0322
Epoch 115/300, seasonal_1 Loss: 0.0644 | 0.0317
Epoch 116/300, seasonal_1 Loss: 0.0639 | 0.0323
Epoch 117/300, seasonal_1 Loss: 0.0638 | 0.0327
Epoch 118/300, seasonal_1 Loss: 0.0644 | 0.0328
Epoch 119/300, seasonal_1 Loss: 0.0647 | 0.0334
Epoch 120/300, seasonal_1 Loss: 0.0641 | 0.0338
Epoch 121/300, seasonal_1 Loss: 0.0634 | 0.0332
Epoch 122/300, seasonal_1 Loss: 0.0634 | 0.0319
Epoch 123/300, seasonal_1 Loss: 0.0637 | 0.0317
Epoch 124/300, seasonal_1 Loss: 0.0637 | 0.0321
Epoch 125/300, seasonal_1 Loss: 0.0634 | 0.0318
Epoch 126/300, seasonal_1 Loss: 0.0629 | 0.0312
Epoch 127/300, seasonal_1 Loss: 0.0626 | 0.0314
Epoch 128/300, seasonal_1 Loss: 0.0625 | 0.0315
Epoch 129/300, seasonal_1 Loss: 0.0626 | 0.0314
Epoch 130/300, seasonal_1 Loss: 0.0627 | 0.0315
Epoch 131/300, seasonal_1 Loss: 0.0623 | 0.0316
Epoch 132/300, seasonal_1 Loss: 0.0618 | 0.0313
Epoch 133/300, seasonal_1 Loss: 0.0618 | 0.0308
Epoch 134/300, seasonal_1 Loss: 0.0620 | 0.0306
Epoch 135/300, seasonal_1 Loss: 0.0622 | 0.0307
Epoch 136/300, seasonal_1 Loss: 0.0620 | 0.0306
Epoch 137/300, seasonal_1 Loss: 0.0616 | 0.0304
Epoch 138/300, seasonal_1 Loss: 0.0614 | 0.0309
Epoch 139/300, seasonal_1 Loss: 0.0614 | 0.0314
Epoch 140/300, seasonal_1 Loss: 0.0615 | 0.0311
Epoch 141/300, seasonal_1 Loss: 0.0615 | 0.0306
Epoch 142/300, seasonal_1 Loss: 0.0614 | 0.0305
Epoch 143/300, seasonal_1 Loss: 0.0612 | 0.0304
Epoch 144/300, seasonal_1 Loss: 0.0609 | 0.0302
Epoch 145/300, seasonal_1 Loss: 0.0608 | 0.0299
Epoch 146/300, seasonal_1 Loss: 0.0610 | 0.0299
Epoch 147/300, seasonal_1 Loss: 0.0613 | 0.0301
Epoch 148/300, seasonal_1 Loss: 0.0613 | 0.0303
Epoch 149/300, seasonal_1 Loss: 0.0612 | 0.0302
Epoch 150/300, seasonal_1 Loss: 0.0611 | 0.0303
Epoch 151/300, seasonal_1 Loss: 0.0610 | 0.0308
Epoch 152/300, seasonal_1 Loss: 0.0610 | 0.0311
Epoch 153/300, seasonal_1 Loss: 0.0609 | 0.0310
Epoch 154/300, seasonal_1 Loss: 0.0606 | 0.0309
Epoch 155/300, seasonal_1 Loss: 0.0603 | 0.0306
Epoch 156/300, seasonal_1 Loss: 0.0601 | 0.0302
Epoch 157/300, seasonal_1 Loss: 0.0602 | 0.0299
Epoch 158/300, seasonal_1 Loss: 0.0606 | 0.0301
Epoch 159/300, seasonal_1 Loss: 0.0612 | 0.0305
Epoch 160/300, seasonal_1 Loss: 0.0614 | 0.0304
Epoch 161/300, seasonal_1 Loss: 0.0612 | 0.0304
Epoch 162/300, seasonal_1 Loss: 0.0612 | 0.0314
Epoch 163/300, seasonal_1 Loss: 0.0613 | 0.0324
Epoch 164/300, seasonal_1 Loss: 0.0613 | 0.0318
Epoch 165/300, seasonal_1 Loss: 0.0607 | 0.0304
Epoch 166/300, seasonal_1 Loss: 0.0601 | 0.0299
Epoch 167/300, seasonal_1 Loss: 0.0601 | 0.0301
Epoch 168/300, seasonal_1 Loss: 0.0605 | 0.0304
Epoch 169/300, seasonal_1 Loss: 0.0609 | 0.0303
Epoch 170/300, seasonal_1 Loss: 0.0612 | 0.0299
Epoch 171/300, seasonal_1 Loss: 0.0611 | 0.0299
Epoch 172/300, seasonal_1 Loss: 0.0610 | 0.0303
Epoch 173/300, seasonal_1 Loss: 0.0608 | 0.0308
Epoch 174/300, seasonal_1 Loss: 0.0606 | 0.0311
Epoch 175/300, seasonal_1 Loss: 0.0603 | 0.0310
Epoch 176/300, seasonal_1 Loss: 0.0601 | 0.0309
Epoch 177/300, seasonal_1 Loss: 0.0600 | 0.0307
Epoch 178/300, seasonal_1 Loss: 0.0598 | 0.0304
Epoch 179/300, seasonal_1 Loss: 0.0595 | 0.0303
Epoch 180/300, seasonal_1 Loss: 0.0593 | 0.0304
Epoch 181/300, seasonal_1 Loss: 0.0591 | 0.0306
Epoch 182/300, seasonal_1 Loss: 0.0593 | 0.0308
Epoch 183/300, seasonal_1 Loss: 0.0597 | 0.0310
Epoch 184/300, seasonal_1 Loss: 0.0603 | 0.0311
Epoch 185/300, seasonal_1 Loss: 0.0606 | 0.0309
Epoch 186/300, seasonal_1 Loss: 0.0607 | 0.0305
Epoch 187/300, seasonal_1 Loss: 0.0606 | 0.0301
Epoch 188/300, seasonal_1 Loss: 0.0604 | 0.0299
Epoch 189/300, seasonal_1 Loss: 0.0600 | 0.0296
Epoch 190/300, seasonal_1 Loss: 0.0591 | 0.0293
Epoch 191/300, seasonal_1 Loss: 0.0589 | 0.0294
Epoch 192/300, seasonal_1 Loss: 0.0597 | 0.0298
Epoch 193/300, seasonal_1 Loss: 0.0604 | 0.0302
Epoch 194/300, seasonal_1 Loss: 0.0605 | 0.0305
Epoch 195/300, seasonal_1 Loss: 0.0598 | 0.0303
Epoch 196/300, seasonal_1 Loss: 0.0588 | 0.0301
Epoch 197/300, seasonal_1 Loss: 0.0582 | 0.0299
Epoch 198/300, seasonal_1 Loss: 0.0582 | 0.0300
Epoch 199/300, seasonal_1 Loss: 0.0584 | 0.0300
Epoch 200/300, seasonal_1 Loss: 0.0585 | 0.0300
Epoch 201/300, seasonal_1 Loss: 0.0584 | 0.0299
Epoch 202/300, seasonal_1 Loss: 0.0581 | 0.0297
Epoch 203/300, seasonal_1 Loss: 0.0576 | 0.0294
Epoch 204/300, seasonal_1 Loss: 0.0574 | 0.0293
Epoch 205/300, seasonal_1 Loss: 0.0574 | 0.0293
Epoch 206/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 207/300, seasonal_1 Loss: 0.0575 | 0.0294
Epoch 208/300, seasonal_1 Loss: 0.0573 | 0.0294
Epoch 209/300, seasonal_1 Loss: 0.0571 | 0.0294
Epoch 210/300, seasonal_1 Loss: 0.0570 | 0.0295
Epoch 211/300, seasonal_1 Loss: 0.0570 | 0.0294
Epoch 212/300, seasonal_1 Loss: 0.0570 | 0.0294
Epoch 213/300, seasonal_1 Loss: 0.0570 | 0.0293
Epoch 214/300, seasonal_1 Loss: 0.0569 | 0.0292
Epoch 215/300, seasonal_1 Loss: 0.0568 | 0.0292
Epoch 216/300, seasonal_1 Loss: 0.0567 | 0.0291
Epoch 217/300, seasonal_1 Loss: 0.0567 | 0.0291
Epoch 218/300, seasonal_1 Loss: 0.0567 | 0.0291
Epoch 219/300, seasonal_1 Loss: 0.0566 | 0.0292
Epoch 220/300, seasonal_1 Loss: 0.0566 | 0.0292
Epoch 221/300, seasonal_1 Loss: 0.0565 | 0.0292
Epoch 222/300, seasonal_1 Loss: 0.0565 | 0.0292
Epoch 223/300, seasonal_1 Loss: 0.0564 | 0.0292
Epoch 224/300, seasonal_1 Loss: 0.0564 | 0.0292
Epoch 225/300, seasonal_1 Loss: 0.0563 | 0.0292
Epoch 226/300, seasonal_1 Loss: 0.0563 | 0.0291
Epoch 227/300, seasonal_1 Loss: 0.0563 | 0.0291
Epoch 228/300, seasonal_1 Loss: 0.0562 | 0.0291
Epoch 229/300, seasonal_1 Loss: 0.0562 | 0.0290
Epoch 230/300, seasonal_1 Loss: 0.0561 | 0.0290
Epoch 231/300, seasonal_1 Loss: 0.0561 | 0.0291
Epoch 232/300, seasonal_1 Loss: 0.0561 | 0.0291
Epoch 233/300, seasonal_1 Loss: 0.0560 | 0.0291
Epoch 234/300, seasonal_1 Loss: 0.0560 | 0.0291
Epoch 235/300, seasonal_1 Loss: 0.0560 | 0.0291
Epoch 236/300, seasonal_1 Loss: 0.0559 | 0.0291
Epoch 237/300, seasonal_1 Loss: 0.0559 | 0.0291
Epoch 238/300, seasonal_1 Loss: 0.0558 | 0.0291
Epoch 239/300, seasonal_1 Loss: 0.0558 | 0.0291
Epoch 240/300, seasonal_1 Loss: 0.0558 | 0.0291
Epoch 241/300, seasonal_1 Loss: 0.0557 | 0.0290
Epoch 242/300, seasonal_1 Loss: 0.0557 | 0.0290
Epoch 243/300, seasonal_1 Loss: 0.0557 | 0.0290
Epoch 244/300, seasonal_1 Loss: 0.0556 | 0.0290
Epoch 245/300, seasonal_1 Loss: 0.0556 | 0.0290
Epoch 246/300, seasonal_1 Loss: 0.0556 | 0.0290
Epoch 247/300, seasonal_1 Loss: 0.0555 | 0.0290
Epoch 248/300, seasonal_1 Loss: 0.0555 | 0.0291
Epoch 249/300, seasonal_1 Loss: 0.0555 | 0.0291
Epoch 250/300, seasonal_1 Loss: 0.0554 | 0.0291
Epoch 251/300, seasonal_1 Loss: 0.0554 | 0.0291
Epoch 252/300, seasonal_1 Loss: 0.0553 | 0.0291
Epoch 253/300, seasonal_1 Loss: 0.0553 | 0.0291
Epoch 254/300, seasonal_1 Loss: 0.0553 | 0.0291
Epoch 255/300, seasonal_1 Loss: 0.0552 | 0.0290
Epoch 256/300, seasonal_1 Loss: 0.0552 | 0.0290
Epoch 257/300, seasonal_1 Loss: 0.0552 | 0.0290
Epoch 258/300, seasonal_1 Loss: 0.0552 | 0.0290
Epoch 259/300, seasonal_1 Loss: 0.0551 | 0.0290
Epoch 260/300, seasonal_1 Loss: 0.0551 | 0.0290
Epoch 261/300, seasonal_1 Loss: 0.0551 | 0.0290
Epoch 262/300, seasonal_1 Loss: 0.0550 | 0.0291
Epoch 263/300, seasonal_1 Loss: 0.0550 | 0.0291
Epoch 264/300, seasonal_1 Loss: 0.0550 | 0.0291
Epoch 265/300, seasonal_1 Loss: 0.0549 | 0.0291
Epoch 266/300, seasonal_1 Loss: 0.0549 | 0.0291
Epoch 267/300, seasonal_1 Loss: 0.0549 | 0.0291
Epoch 268/300, seasonal_1 Loss: 0.0548 | 0.0291
Epoch 269/300, seasonal_1 Loss: 0.0548 | 0.0291
Epoch 270/300, seasonal_1 Loss: 0.0548 | 0.0291
Epoch 271/300, seasonal_1 Loss: 0.0548 | 0.0291
Epoch 272/300, seasonal_1 Loss: 0.0547 | 0.0291
Epoch 273/300, seasonal_1 Loss: 0.0547 | 0.0291
Epoch 274/300, seasonal_1 Loss: 0.0547 | 0.0291
Epoch 275/300, seasonal_1 Loss: 0.0547 | 0.0291
Epoch 276/300, seasonal_1 Loss: 0.0546 | 0.0291
Epoch 277/300, seasonal_1 Loss: 0.0546 | 0.0291
Epoch 278/300, seasonal_1 Loss: 0.0546 | 0.0291
Epoch 279/300, seasonal_1 Loss: 0.0545 | 0.0291
Epoch 280/300, seasonal_1 Loss: 0.0545 | 0.0291
Epoch 281/300, seasonal_1 Loss: 0.0545 | 0.0291
Epoch 282/300, seasonal_1 Loss: 0.0545 | 0.0291
Epoch 283/300, seasonal_1 Loss: 0.0544 | 0.0291
Epoch 284/300, seasonal_1 Loss: 0.0544 | 0.0291
Epoch 285/300, seasonal_1 Loss: 0.0544 | 0.0291
Epoch 286/300, seasonal_1 Loss: 0.0544 | 0.0291
Epoch 287/300, seasonal_1 Loss: 0.0543 | 0.0291
Epoch 288/300, seasonal_1 Loss: 0.0543 | 0.0291
Epoch 289/300, seasonal_1 Loss: 0.0543 | 0.0291
Epoch 290/300, seasonal_1 Loss: 0.0543 | 0.0291
Epoch 291/300, seasonal_1 Loss: 0.0542 | 0.0291
Epoch 292/300, seasonal_1 Loss: 0.0542 | 0.0291
Epoch 293/300, seasonal_1 Loss: 0.0542 | 0.0292
Epoch 294/300, seasonal_1 Loss: 0.0542 | 0.0292
Epoch 295/300, seasonal_1 Loss: 0.0541 | 0.0292
Epoch 296/300, seasonal_1 Loss: 0.0541 | 0.0292
Epoch 297/300, seasonal_1 Loss: 0.0541 | 0.0292
Epoch 298/300, seasonal_1 Loss: 0.0541 | 0.0292
Epoch 299/300, seasonal_1 Loss: 0.0540 | 0.0292
Epoch 300/300, seasonal_1 Loss: 0.0540 | 0.0292
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9514408695662091, 'learning_rate': 0.00020173945227506946, 'batch_size': 71, 'step_size': 4, 'gamma': 0.948684393644014}
Epoch 1/300, seasonal_2 Loss: 0.2113 | 0.1535
Epoch 2/300, seasonal_2 Loss: 0.1340 | 0.1149
Epoch 3/300, seasonal_2 Loss: 0.1230 | 0.1188
Epoch 4/300, seasonal_2 Loss: 0.1135 | 0.1084
Epoch 5/300, seasonal_2 Loss: 0.1137 | 0.0919
Epoch 6/300, seasonal_2 Loss: 0.1176 | 0.0979
Epoch 7/300, seasonal_2 Loss: 0.1162 | 0.0789
Epoch 8/300, seasonal_2 Loss: 0.1013 | 0.0728
Epoch 9/300, seasonal_2 Loss: 0.0961 | 0.0695
Epoch 10/300, seasonal_2 Loss: 0.0954 | 0.0663
Epoch 11/300, seasonal_2 Loss: 0.0928 | 0.0632
Epoch 12/300, seasonal_2 Loss: 0.0908 | 0.0591
Epoch 13/300, seasonal_2 Loss: 0.0880 | 0.0556
Epoch 14/300, seasonal_2 Loss: 0.0863 | 0.0518
Epoch 15/300, seasonal_2 Loss: 0.0842 | 0.0496
Epoch 16/300, seasonal_2 Loss: 0.0832 | 0.0478
Epoch 17/300, seasonal_2 Loss: 0.0816 | 0.0463
Epoch 18/300, seasonal_2 Loss: 0.0803 | 0.0451
Epoch 19/300, seasonal_2 Loss: 0.0788 | 0.0439
Epoch 20/300, seasonal_2 Loss: 0.0775 | 0.0432
Epoch 21/300, seasonal_2 Loss: 0.0769 | 0.0425
Epoch 22/300, seasonal_2 Loss: 0.0769 | 0.0427
Epoch 23/300, seasonal_2 Loss: 0.0767 | 0.0408
Epoch 24/300, seasonal_2 Loss: 0.0754 | 0.0392
Epoch 25/300, seasonal_2 Loss: 0.0742 | 0.0381
Epoch 26/300, seasonal_2 Loss: 0.0734 | 0.0373
Epoch 27/300, seasonal_2 Loss: 0.0726 | 0.0369
Epoch 28/300, seasonal_2 Loss: 0.0721 | 0.0365
Epoch 29/300, seasonal_2 Loss: 0.0714 | 0.0366
Epoch 30/300, seasonal_2 Loss: 0.0711 | 0.0365
Epoch 31/300, seasonal_2 Loss: 0.0707 | 0.0367
Epoch 32/300, seasonal_2 Loss: 0.0704 | 0.0366
Epoch 33/300, seasonal_2 Loss: 0.0701 | 0.0366
Epoch 34/300, seasonal_2 Loss: 0.0698 | 0.0365
Epoch 35/300, seasonal_2 Loss: 0.0697 | 0.0363
Epoch 36/300, seasonal_2 Loss: 0.0696 | 0.0361
Epoch 37/300, seasonal_2 Loss: 0.0695 | 0.0359
Epoch 38/300, seasonal_2 Loss: 0.0691 | 0.0356
Epoch 39/300, seasonal_2 Loss: 0.0684 | 0.0355
Epoch 40/300, seasonal_2 Loss: 0.0677 | 0.0352
Epoch 41/300, seasonal_2 Loss: 0.0671 | 0.0354
Epoch 42/300, seasonal_2 Loss: 0.0668 | 0.0356
Epoch 43/300, seasonal_2 Loss: 0.0664 | 0.0357
Epoch 44/300, seasonal_2 Loss: 0.0662 | 0.0355
Epoch 45/300, seasonal_2 Loss: 0.0660 | 0.0352
Epoch 46/300, seasonal_2 Loss: 0.0660 | 0.0352
Epoch 47/300, seasonal_2 Loss: 0.0660 | 0.0352
Epoch 48/300, seasonal_2 Loss: 0.0658 | 0.0344
Epoch 49/300, seasonal_2 Loss: 0.0654 | 0.0341
Epoch 50/300, seasonal_2 Loss: 0.0651 | 0.0339
Epoch 51/300, seasonal_2 Loss: 0.0650 | 0.0337
Epoch 52/300, seasonal_2 Loss: 0.0650 | 0.0336
Epoch 53/300, seasonal_2 Loss: 0.0649 | 0.0334
Epoch 54/300, seasonal_2 Loss: 0.0647 | 0.0332
Epoch 55/300, seasonal_2 Loss: 0.0646 | 0.0330
Epoch 56/300, seasonal_2 Loss: 0.0644 | 0.0329
Epoch 57/300, seasonal_2 Loss: 0.0642 | 0.0327
Epoch 58/300, seasonal_2 Loss: 0.0641 | 0.0327
Epoch 59/300, seasonal_2 Loss: 0.0639 | 0.0324
Epoch 60/300, seasonal_2 Loss: 0.0638 | 0.0324
Epoch 61/300, seasonal_2 Loss: 0.0636 | 0.0321
Epoch 62/300, seasonal_2 Loss: 0.0635 | 0.0321
Epoch 63/300, seasonal_2 Loss: 0.0634 | 0.0319
Epoch 64/300, seasonal_2 Loss: 0.0634 | 0.0318
Epoch 65/300, seasonal_2 Loss: 0.0633 | 0.0317
Epoch 66/300, seasonal_2 Loss: 0.0633 | 0.0317
Epoch 67/300, seasonal_2 Loss: 0.0633 | 0.0318
Epoch 68/300, seasonal_2 Loss: 0.0633 | 0.0319
Epoch 69/300, seasonal_2 Loss: 0.0632 | 0.0319
Epoch 70/300, seasonal_2 Loss: 0.0631 | 0.0320
Epoch 71/300, seasonal_2 Loss: 0.0630 | 0.0320
Epoch 72/300, seasonal_2 Loss: 0.0628 | 0.0320
Epoch 73/300, seasonal_2 Loss: 0.0627 | 0.0318
Epoch 74/300, seasonal_2 Loss: 0.0625 | 0.0317
Epoch 75/300, seasonal_2 Loss: 0.0624 | 0.0315
Epoch 76/300, seasonal_2 Loss: 0.0623 | 0.0314
Epoch 77/300, seasonal_2 Loss: 0.0621 | 0.0311
Epoch 78/300, seasonal_2 Loss: 0.0620 | 0.0310
Epoch 79/300, seasonal_2 Loss: 0.0619 | 0.0308
Epoch 80/300, seasonal_2 Loss: 0.0619 | 0.0307
Epoch 81/300, seasonal_2 Loss: 0.0618 | 0.0305
Epoch 82/300, seasonal_2 Loss: 0.0617 | 0.0304
Epoch 83/300, seasonal_2 Loss: 0.0616 | 0.0303
Epoch 84/300, seasonal_2 Loss: 0.0615 | 0.0302
Epoch 85/300, seasonal_2 Loss: 0.0614 | 0.0302
Epoch 86/300, seasonal_2 Loss: 0.0614 | 0.0301
Epoch 87/300, seasonal_2 Loss: 0.0613 | 0.0301
Epoch 88/300, seasonal_2 Loss: 0.0612 | 0.0300
Epoch 89/300, seasonal_2 Loss: 0.0611 | 0.0299
Epoch 90/300, seasonal_2 Loss: 0.0610 | 0.0298
Epoch 91/300, seasonal_2 Loss: 0.0609 | 0.0298
Epoch 92/300, seasonal_2 Loss: 0.0608 | 0.0298
Epoch 93/300, seasonal_2 Loss: 0.0608 | 0.0298
Epoch 94/300, seasonal_2 Loss: 0.0608 | 0.0298
Epoch 95/300, seasonal_2 Loss: 0.0607 | 0.0299
Epoch 96/300, seasonal_2 Loss: 0.0607 | 0.0300
Epoch 97/300, seasonal_2 Loss: 0.0608 | 0.0300
Epoch 98/300, seasonal_2 Loss: 0.0609 | 0.0301
Epoch 99/300, seasonal_2 Loss: 0.0610 | 0.0301
Epoch 100/300, seasonal_2 Loss: 0.0612 | 0.0301
Epoch 101/300, seasonal_2 Loss: 0.0614 | 0.0301
Epoch 102/300, seasonal_2 Loss: 0.0616 | 0.0303
Epoch 103/300, seasonal_2 Loss: 0.0617 | 0.0309
Epoch 104/300, seasonal_2 Loss: 0.0616 | 0.0312
Epoch 105/300, seasonal_2 Loss: 0.0614 | 0.0315
Epoch 106/300, seasonal_2 Loss: 0.0611 | 0.0313
Epoch 107/300, seasonal_2 Loss: 0.0609 | 0.0311
Epoch 108/300, seasonal_2 Loss: 0.0608 | 0.0309
Epoch 109/300, seasonal_2 Loss: 0.0608 | 0.0307
Epoch 110/300, seasonal_2 Loss: 0.0609 | 0.0303
Epoch 111/300, seasonal_2 Loss: 0.0610 | 0.0297
Epoch 112/300, seasonal_2 Loss: 0.0613 | 0.0293
Epoch 113/300, seasonal_2 Loss: 0.0616 | 0.0290
Epoch 114/300, seasonal_2 Loss: 0.0619 | 0.0291
Epoch 115/300, seasonal_2 Loss: 0.0618 | 0.0291
Epoch 116/300, seasonal_2 Loss: 0.0615 | 0.0291
Epoch 117/300, seasonal_2 Loss: 0.0610 | 0.0290
Epoch 118/300, seasonal_2 Loss: 0.0605 | 0.0288
Epoch 119/300, seasonal_2 Loss: 0.0601 | 0.0288
Epoch 120/300, seasonal_2 Loss: 0.0598 | 0.0286
Epoch 121/300, seasonal_2 Loss: 0.0595 | 0.0285
Epoch 122/300, seasonal_2 Loss: 0.0594 | 0.0284
Epoch 123/300, seasonal_2 Loss: 0.0593 | 0.0284
Epoch 124/300, seasonal_2 Loss: 0.0593 | 0.0283
Epoch 125/300, seasonal_2 Loss: 0.0592 | 0.0283
Epoch 126/300, seasonal_2 Loss: 0.0592 | 0.0282
Epoch 127/300, seasonal_2 Loss: 0.0592 | 0.0282
Epoch 128/300, seasonal_2 Loss: 0.0591 | 0.0281
Epoch 129/300, seasonal_2 Loss: 0.0591 | 0.0281
Epoch 130/300, seasonal_2 Loss: 0.0591 | 0.0280
Epoch 131/300, seasonal_2 Loss: 0.0591 | 0.0280
Epoch 132/300, seasonal_2 Loss: 0.0590 | 0.0280
Epoch 133/300, seasonal_2 Loss: 0.0590 | 0.0279
Epoch 134/300, seasonal_2 Loss: 0.0590 | 0.0279
Epoch 135/300, seasonal_2 Loss: 0.0589 | 0.0279
Epoch 136/300, seasonal_2 Loss: 0.0589 | 0.0279
Epoch 137/300, seasonal_2 Loss: 0.0588 | 0.0279
Epoch 138/300, seasonal_2 Loss: 0.0588 | 0.0279
Epoch 139/300, seasonal_2 Loss: 0.0588 | 0.0279
Epoch 140/300, seasonal_2 Loss: 0.0588 | 0.0279
Epoch 141/300, seasonal_2 Loss: 0.0589 | 0.0279
Epoch 142/300, seasonal_2 Loss: 0.0589 | 0.0278
Epoch 143/300, seasonal_2 Loss: 0.0590 | 0.0278
Epoch 144/300, seasonal_2 Loss: 0.0590 | 0.0277
Epoch 145/300, seasonal_2 Loss: 0.0590 | 0.0276
Epoch 146/300, seasonal_2 Loss: 0.0591 | 0.0276
Epoch 147/300, seasonal_2 Loss: 0.0590 | 0.0276
Epoch 148/300, seasonal_2 Loss: 0.0590 | 0.0275
Epoch 149/300, seasonal_2 Loss: 0.0589 | 0.0275
Epoch 150/300, seasonal_2 Loss: 0.0587 | 0.0275
Epoch 151/300, seasonal_2 Loss: 0.0586 | 0.0275
Epoch 152/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 153/300, seasonal_2 Loss: 0.0584 | 0.0275
Epoch 154/300, seasonal_2 Loss: 0.0583 | 0.0275
Epoch 155/300, seasonal_2 Loss: 0.0583 | 0.0275
Epoch 156/300, seasonal_2 Loss: 0.0583 | 0.0275
Epoch 157/300, seasonal_2 Loss: 0.0583 | 0.0275
Epoch 158/300, seasonal_2 Loss: 0.0583 | 0.0275
Epoch 159/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 160/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 161/300, seasonal_2 Loss: 0.0582 | 0.0274
Epoch 162/300, seasonal_2 Loss: 0.0581 | 0.0274
Epoch 163/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 164/300, seasonal_2 Loss: 0.0581 | 0.0273
Epoch 165/300, seasonal_2 Loss: 0.0580 | 0.0273
Epoch 166/300, seasonal_2 Loss: 0.0580 | 0.0273
Epoch 167/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 168/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 169/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 170/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 171/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 172/300, seasonal_2 Loss: 0.0580 | 0.0272
Epoch 173/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 174/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 175/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 176/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 177/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 178/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 179/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 180/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 181/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 182/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 183/300, seasonal_2 Loss: 0.0579 | 0.0271
Epoch 184/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 185/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 186/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 187/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 188/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 189/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 190/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 191/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 192/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 193/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 194/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 195/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 196/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 197/300, seasonal_2 Loss: 0.0578 | 0.0270
Epoch 198/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 199/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 200/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 201/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 202/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 203/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 204/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 205/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 206/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 207/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 208/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 209/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 210/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 211/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 212/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 213/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 214/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 215/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 216/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 217/300, seasonal_2 Loss: 0.0577 | 0.0269
Epoch 218/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 219/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 220/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 221/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 222/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 223/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 224/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 225/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 226/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 227/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 228/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 229/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 230/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 231/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 232/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 233/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 234/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 235/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 236/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 237/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 238/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 239/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 240/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 241/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 242/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 243/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 244/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 245/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 246/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 247/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 248/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 249/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 250/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 251/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 252/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 253/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 254/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 255/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 256/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 257/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 258/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 259/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 260/300, seasonal_2 Loss: 0.0576 | 0.0268
Epoch 261/300, seasonal_2 Loss: 0.0575 | 0.0268
Epoch 262/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 263/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 264/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 265/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 266/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 267/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 268/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 269/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 270/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 271/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 272/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 273/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 274/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 275/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 276/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 277/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 278/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 279/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 280/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 281/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 282/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 283/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 284/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 285/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 286/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 287/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 288/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 289/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 290/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 291/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 292/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 293/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 294/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 295/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 296/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 297/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 298/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 299/300, seasonal_2 Loss: 0.0575 | 0.0267
Epoch 300/300, seasonal_2 Loss: 0.0575 | 0.0267
Training seasonal_3 component with params: {'observation_period_num': 14, 'train_rates': 0.8528527798676642, 'learning_rate': 0.00073495641954545, 'batch_size': 182, 'step_size': 10, 'gamma': 0.9779346876394431}
Epoch 1/300, seasonal_3 Loss: 0.4382 | 0.3010
Epoch 2/300, seasonal_3 Loss: 0.2035 | 0.1646
Epoch 3/300, seasonal_3 Loss: 0.2846 | 0.1972
Epoch 4/300, seasonal_3 Loss: 0.2175 | 0.3739
Epoch 5/300, seasonal_3 Loss: 0.2308 | 0.1904
Epoch 6/300, seasonal_3 Loss: 0.1642 | 0.1302
Epoch 7/300, seasonal_3 Loss: 0.1484 | 0.1289
Epoch 8/300, seasonal_3 Loss: 0.1299 | 0.0871
Epoch 9/300, seasonal_3 Loss: 0.1096 | 0.0742
Epoch 10/300, seasonal_3 Loss: 0.1088 | 0.0820
Epoch 11/300, seasonal_3 Loss: 0.1125 | 0.0842
Epoch 12/300, seasonal_3 Loss: 0.1134 | 0.0698
Epoch 13/300, seasonal_3 Loss: 0.1135 | 0.0902
Epoch 14/300, seasonal_3 Loss: 0.1069 | 0.0884
Epoch 15/300, seasonal_3 Loss: 0.1001 | 0.0643
Epoch 16/300, seasonal_3 Loss: 0.1082 | 0.0870
Epoch 17/300, seasonal_3 Loss: 0.1116 | 0.0662
Epoch 18/300, seasonal_3 Loss: 0.1006 | 0.0761
Epoch 19/300, seasonal_3 Loss: 0.0959 | 0.0625
Epoch 20/300, seasonal_3 Loss: 0.0910 | 0.0557
Epoch 21/300, seasonal_3 Loss: 0.0911 | 0.0573
Epoch 22/300, seasonal_3 Loss: 0.0942 | 0.0541
Epoch 23/300, seasonal_3 Loss: 0.0916 | 0.0515
Epoch 24/300, seasonal_3 Loss: 0.0858 | 0.0624
Epoch 25/300, seasonal_3 Loss: 0.0854 | 0.0677
Epoch 26/300, seasonal_3 Loss: 0.0858 | 0.0556
Epoch 27/300, seasonal_3 Loss: 0.0828 | 0.0490
Epoch 28/300, seasonal_3 Loss: 0.0806 | 0.0436
Epoch 29/300, seasonal_3 Loss: 0.0800 | 0.0424
Epoch 30/300, seasonal_3 Loss: 0.0782 | 0.0417
Epoch 31/300, seasonal_3 Loss: 0.0785 | 0.0459
Epoch 32/300, seasonal_3 Loss: 0.0815 | 0.0549
Epoch 33/300, seasonal_3 Loss: 0.0795 | 0.0451
Epoch 34/300, seasonal_3 Loss: 0.0766 | 0.0411
Epoch 35/300, seasonal_3 Loss: 0.0765 | 0.0408
Epoch 36/300, seasonal_3 Loss: 0.0754 | 0.0404
Epoch 37/300, seasonal_3 Loss: 0.0755 | 0.0410
Epoch 38/300, seasonal_3 Loss: 0.0782 | 0.0477
Epoch 39/300, seasonal_3 Loss: 0.0802 | 0.0610
Epoch 40/300, seasonal_3 Loss: 0.0756 | 0.0443
Epoch 41/300, seasonal_3 Loss: 0.0749 | 0.0407
Epoch 42/300, seasonal_3 Loss: 0.0766 | 0.0466
Epoch 43/300, seasonal_3 Loss: 0.0768 | 0.0516
Epoch 44/300, seasonal_3 Loss: 0.0782 | 0.0438
Epoch 45/300, seasonal_3 Loss: 0.0777 | 0.0442
Epoch 46/300, seasonal_3 Loss: 0.0730 | 0.0478
Epoch 47/300, seasonal_3 Loss: 0.0759 | 0.0434
Epoch 48/300, seasonal_3 Loss: 0.0730 | 0.0431
Epoch 49/300, seasonal_3 Loss: 0.0698 | 0.0392
Epoch 50/300, seasonal_3 Loss: 0.0693 | 0.0372
Epoch 51/300, seasonal_3 Loss: 0.0696 | 0.0378
Epoch 52/300, seasonal_3 Loss: 0.0688 | 0.0408
Epoch 53/300, seasonal_3 Loss: 0.0685 | 0.0401
Epoch 54/300, seasonal_3 Loss: 0.0678 | 0.0368
Epoch 55/300, seasonal_3 Loss: 0.0676 | 0.0359
Epoch 56/300, seasonal_3 Loss: 0.0699 | 0.0392
Epoch 57/300, seasonal_3 Loss: 0.0701 | 0.0356
Epoch 58/300, seasonal_3 Loss: 0.0681 | 0.0375
Epoch 59/300, seasonal_3 Loss: 0.0677 | 0.0373
Epoch 60/300, seasonal_3 Loss: 0.0677 | 0.0405
Epoch 61/300, seasonal_3 Loss: 0.0672 | 0.0483
Epoch 62/300, seasonal_3 Loss: 0.0672 | 0.0379
Epoch 63/300, seasonal_3 Loss: 0.0656 | 0.0348
Epoch 64/300, seasonal_3 Loss: 0.0653 | 0.0363
Epoch 65/300, seasonal_3 Loss: 0.0661 | 0.0380
Epoch 66/300, seasonal_3 Loss: 0.0655 | 0.0363
Epoch 67/300, seasonal_3 Loss: 0.0646 | 0.0357
Epoch 68/300, seasonal_3 Loss: 0.0669 | 0.0370
Epoch 69/300, seasonal_3 Loss: 0.0668 | 0.0364
Epoch 70/300, seasonal_3 Loss: 0.0682 | 0.0538
Epoch 71/300, seasonal_3 Loss: 0.0672 | 0.0385
Epoch 72/300, seasonal_3 Loss: 0.0681 | 0.0377
Epoch 73/300, seasonal_3 Loss: 0.0677 | 0.0517
Epoch 74/300, seasonal_3 Loss: 0.0737 | 0.0433
Epoch 75/300, seasonal_3 Loss: 0.0707 | 0.0529
Epoch 76/300, seasonal_3 Loss: 0.0802 | 0.0460
Epoch 77/300, seasonal_3 Loss: 0.0746 | 0.0413
Epoch 78/300, seasonal_3 Loss: 0.0821 | 0.0386
Epoch 79/300, seasonal_3 Loss: 0.0693 | 0.0384
Epoch 80/300, seasonal_3 Loss: 0.0668 | 0.0349
Epoch 81/300, seasonal_3 Loss: 0.0653 | 0.0441
Epoch 82/300, seasonal_3 Loss: 0.0686 | 0.0532
Epoch 83/300, seasonal_3 Loss: 0.0705 | 0.0378
Epoch 84/300, seasonal_3 Loss: 0.0643 | 0.0430
Epoch 85/300, seasonal_3 Loss: 0.0632 | 0.0371
Epoch 86/300, seasonal_3 Loss: 0.0621 | 0.0360
Epoch 87/300, seasonal_3 Loss: 0.0612 | 0.0328
Epoch 88/300, seasonal_3 Loss: 0.0606 | 0.0310
Epoch 89/300, seasonal_3 Loss: 0.0610 | 0.0343
Epoch 90/300, seasonal_3 Loss: 0.0619 | 0.0334
Epoch 91/300, seasonal_3 Loss: 0.0617 | 0.0340
Epoch 92/300, seasonal_3 Loss: 0.0624 | 0.0462
Epoch 93/300, seasonal_3 Loss: 0.0619 | 0.0347
Epoch 94/300, seasonal_3 Loss: 0.0622 | 0.0368
Epoch 95/300, seasonal_3 Loss: 0.0644 | 0.0411
Epoch 96/300, seasonal_3 Loss: 0.0652 | 0.0368
Epoch 97/300, seasonal_3 Loss: 0.0657 | 0.0372
Epoch 98/300, seasonal_3 Loss: 0.0697 | 0.0562
Epoch 99/300, seasonal_3 Loss: 0.0693 | 0.0356
Epoch 100/300, seasonal_3 Loss: 0.0681 | 0.0365
Epoch 101/300, seasonal_3 Loss: 0.0817 | 0.0458
Epoch 102/300, seasonal_3 Loss: 0.0706 | 0.0387
Epoch 103/300, seasonal_3 Loss: 0.0696 | 0.0348
Epoch 104/300, seasonal_3 Loss: 0.0742 | 0.0450
Epoch 105/300, seasonal_3 Loss: 0.0729 | 0.1022
Epoch 106/300, seasonal_3 Loss: 0.0798 | 0.0413
Epoch 107/300, seasonal_3 Loss: 0.0946 | 0.0605
Epoch 108/300, seasonal_3 Loss: 0.0945 | 0.0516
Epoch 109/300, seasonal_3 Loss: 0.0820 | 0.0499
Epoch 110/300, seasonal_3 Loss: 0.0795 | 0.0450
Epoch 111/300, seasonal_3 Loss: 0.0752 | 0.0357
Epoch 112/300, seasonal_3 Loss: 0.0710 | 0.0353
Epoch 113/300, seasonal_3 Loss: 0.0689 | 0.0331
Epoch 114/300, seasonal_3 Loss: 0.0622 | 0.0341
Epoch 115/300, seasonal_3 Loss: 0.0742 | 0.0516
Epoch 116/300, seasonal_3 Loss: 0.0739 | 0.0462
Epoch 117/300, seasonal_3 Loss: 0.0669 | 0.0378
Epoch 118/300, seasonal_3 Loss: 0.0613 | 0.0331
Epoch 119/300, seasonal_3 Loss: 0.0587 | 0.0308
Epoch 120/300, seasonal_3 Loss: 0.0581 | 0.0336
Epoch 121/300, seasonal_3 Loss: 0.0598 | 0.0320
Epoch 122/300, seasonal_3 Loss: 0.0575 | 0.0316
Epoch 123/300, seasonal_3 Loss: 0.0603 | 0.0347
Epoch 124/300, seasonal_3 Loss: 0.0585 | 0.0305
Epoch 125/300, seasonal_3 Loss: 0.0563 | 0.0297
Epoch 126/300, seasonal_3 Loss: 0.0550 | 0.0309
Epoch 127/300, seasonal_3 Loss: 0.0548 | 0.0317
Epoch 128/300, seasonal_3 Loss: 0.0539 | 0.0295
Epoch 129/300, seasonal_3 Loss: 0.0535 | 0.0284
Epoch 130/300, seasonal_3 Loss: 0.0535 | 0.0284
Epoch 131/300, seasonal_3 Loss: 0.0533 | 0.0293
Epoch 132/300, seasonal_3 Loss: 0.0531 | 0.0297
Epoch 133/300, seasonal_3 Loss: 0.0528 | 0.0283
Epoch 134/300, seasonal_3 Loss: 0.0526 | 0.0280
Epoch 135/300, seasonal_3 Loss: 0.0524 | 0.0282
Epoch 136/300, seasonal_3 Loss: 0.0521 | 0.0289
Epoch 137/300, seasonal_3 Loss: 0.0519 | 0.0294
Epoch 138/300, seasonal_3 Loss: 0.0518 | 0.0303
Epoch 139/300, seasonal_3 Loss: 0.0519 | 0.0287
Epoch 140/300, seasonal_3 Loss: 0.0520 | 0.0292
Epoch 141/300, seasonal_3 Loss: 0.0521 | 0.0293
Epoch 142/300, seasonal_3 Loss: 0.0516 | 0.0296
Epoch 143/300, seasonal_3 Loss: 0.0514 | 0.0286
Epoch 144/300, seasonal_3 Loss: 0.0516 | 0.0295
Epoch 145/300, seasonal_3 Loss: 0.0520 | 0.0297
Epoch 146/300, seasonal_3 Loss: 0.0517 | 0.0308
Epoch 147/300, seasonal_3 Loss: 0.0519 | 0.0294
Epoch 148/300, seasonal_3 Loss: 0.0528 | 0.0295
Epoch 149/300, seasonal_3 Loss: 0.0556 | 0.0374
Epoch 150/300, seasonal_3 Loss: 0.0611 | 0.0345
Epoch 151/300, seasonal_3 Loss: 0.0569 | 0.0319
Epoch 152/300, seasonal_3 Loss: 0.0545 | 0.0314
Epoch 153/300, seasonal_3 Loss: 0.0525 | 0.0301
Epoch 154/300, seasonal_3 Loss: 0.0524 | 0.0337
Epoch 155/300, seasonal_3 Loss: 0.0521 | 0.0337
Epoch 156/300, seasonal_3 Loss: 0.0513 | 0.0330
Epoch 157/300, seasonal_3 Loss: 0.0515 | 0.0303
Epoch 158/300, seasonal_3 Loss: 0.0509 | 0.0297
Epoch 159/300, seasonal_3 Loss: 0.0507 | 0.0300
Epoch 160/300, seasonal_3 Loss: 0.0503 | 0.0314
Epoch 161/300, seasonal_3 Loss: 0.0508 | 0.0327
Epoch 162/300, seasonal_3 Loss: 0.0527 | 0.0348
Epoch 163/300, seasonal_3 Loss: 0.0560 | 0.0349
Epoch 164/300, seasonal_3 Loss: 0.0541 | 0.0313
Epoch 165/300, seasonal_3 Loss: 0.0545 | 0.0360
Epoch 166/300, seasonal_3 Loss: 0.0565 | 0.0337
Epoch 167/300, seasonal_3 Loss: 0.0572 | 0.0355
Epoch 168/300, seasonal_3 Loss: 0.0559 | 0.0440
Epoch 169/300, seasonal_3 Loss: 0.0561 | 0.0321
Epoch 170/300, seasonal_3 Loss: 0.0536 | 0.0348
Epoch 171/300, seasonal_3 Loss: 0.0539 | 0.0350
Epoch 172/300, seasonal_3 Loss: 0.0535 | 0.0314
Epoch 173/300, seasonal_3 Loss: 0.0519 | 0.0373
Epoch 174/300, seasonal_3 Loss: 0.0553 | 0.0355
Epoch 175/300, seasonal_3 Loss: 0.0560 | 0.0335
Epoch 176/300, seasonal_3 Loss: 0.0530 | 0.0368
Epoch 177/300, seasonal_3 Loss: 0.0519 | 0.0410
Epoch 178/300, seasonal_3 Loss: 0.0515 | 0.0323
Epoch 179/300, seasonal_3 Loss: 0.0484 | 0.0410
Epoch 180/300, seasonal_3 Loss: 0.0515 | 0.0368
Epoch 181/300, seasonal_3 Loss: 0.0531 | 0.0393
Epoch 182/300, seasonal_3 Loss: 0.0496 | 0.0394
Epoch 183/300, seasonal_3 Loss: 0.0478 | 0.0340
Epoch 184/300, seasonal_3 Loss: 0.0526 | 0.0330
Epoch 185/300, seasonal_3 Loss: 0.0460 | 0.0360
Epoch 186/300, seasonal_3 Loss: 0.0496 | 0.0474
Epoch 187/300, seasonal_3 Loss: 0.0556 | 0.0392
Epoch 188/300, seasonal_3 Loss: 0.0544 | 0.0341
Epoch 189/300, seasonal_3 Loss: 0.0512 | 0.0318
Epoch 190/300, seasonal_3 Loss: 0.0503 | 0.0359
Epoch 191/300, seasonal_3 Loss: 0.0498 | 0.0351
Epoch 192/300, seasonal_3 Loss: 0.0455 | 0.0458
Epoch 193/300, seasonal_3 Loss: 0.0443 | 0.0429
Epoch 194/300, seasonal_3 Loss: 0.0449 | 0.0438
Epoch 195/300, seasonal_3 Loss: 0.0448 | 0.0509
Epoch 196/300, seasonal_3 Loss: 0.0469 | 0.0949
Epoch 197/300, seasonal_3 Loss: 0.0602 | 0.0440
Epoch 198/300, seasonal_3 Loss: 0.0509 | 0.0355
Epoch 199/300, seasonal_3 Loss: 0.0557 | 0.0457
Epoch 200/300, seasonal_3 Loss: 0.0482 | 0.0399
Epoch 201/300, seasonal_3 Loss: 0.0472 | 0.0380
Epoch 202/300, seasonal_3 Loss: 0.0468 | 0.0355
Epoch 203/300, seasonal_3 Loss: 0.0508 | 0.0358
Epoch 204/300, seasonal_3 Loss: 0.0440 | 0.0330
Epoch 205/300, seasonal_3 Loss: 0.0434 | 0.0397
Epoch 206/300, seasonal_3 Loss: 0.0434 | 0.0379
Epoch 207/300, seasonal_3 Loss: 0.0448 | 0.0343
Epoch 208/300, seasonal_3 Loss: 0.0471 | 0.0363
Epoch 209/300, seasonal_3 Loss: 0.0515 | 0.0333
Epoch 210/300, seasonal_3 Loss: 0.0510 | 0.0352
Epoch 211/300, seasonal_3 Loss: 0.0505 | 0.0383
Epoch 212/300, seasonal_3 Loss: 0.0504 | 0.0345
Epoch 213/300, seasonal_3 Loss: 0.0493 | 0.0325
Epoch 214/300, seasonal_3 Loss: 0.0496 | 0.0373
Epoch 215/300, seasonal_3 Loss: 0.0507 | 0.0354
Epoch 216/300, seasonal_3 Loss: 0.0511 | 0.0344
Epoch 217/300, seasonal_3 Loss: 0.0492 | 0.0330
Epoch 218/300, seasonal_3 Loss: 0.0479 | 0.0321
Epoch 219/300, seasonal_3 Loss: 0.0437 | 0.0352
Epoch 220/300, seasonal_3 Loss: 0.0480 | 0.0433
Epoch 221/300, seasonal_3 Loss: 0.0479 | 0.0387
Epoch 222/300, seasonal_3 Loss: 0.0451 | 0.0366
Epoch 223/300, seasonal_3 Loss: 0.0420 | 0.0353
Epoch 224/300, seasonal_3 Loss: 0.0418 | 0.0347
Epoch 225/300, seasonal_3 Loss: 0.0429 | 0.0376
Epoch 226/300, seasonal_3 Loss: 0.0436 | 0.0342
Epoch 227/300, seasonal_3 Loss: 0.0422 | 0.0334
Epoch 228/300, seasonal_3 Loss: 0.0403 | 0.0339
Epoch 229/300, seasonal_3 Loss: 0.0407 | 0.0338
Epoch 230/300, seasonal_3 Loss: 0.0418 | 0.0385
Epoch 231/300, seasonal_3 Loss: 0.0422 | 0.0348
Epoch 232/300, seasonal_3 Loss: 0.0399 | 0.0339
Epoch 233/300, seasonal_3 Loss: 0.0404 | 0.0330
Epoch 234/300, seasonal_3 Loss: 0.0409 | 0.0334
Epoch 235/300, seasonal_3 Loss: 0.0390 | 0.0338
Epoch 236/300, seasonal_3 Loss: 0.0389 | 0.0336
Epoch 237/300, seasonal_3 Loss: 0.0387 | 0.0334
Epoch 238/300, seasonal_3 Loss: 0.0393 | 0.0360
Epoch 239/300, seasonal_3 Loss: 0.0377 | 0.0337
Epoch 240/300, seasonal_3 Loss: 0.0380 | 0.0342
Epoch 241/300, seasonal_3 Loss: 0.0389 | 0.0334
Epoch 242/300, seasonal_3 Loss: 0.0385 | 0.0344
Epoch 243/300, seasonal_3 Loss: 0.0377 | 0.0343
Epoch 244/300, seasonal_3 Loss: 0.0382 | 0.0341
Epoch 245/300, seasonal_3 Loss: 0.0380 | 0.0339
Epoch 246/300, seasonal_3 Loss: 0.0375 | 0.0364
Epoch 247/300, seasonal_3 Loss: 0.0375 | 0.0332
Epoch 248/300, seasonal_3 Loss: 0.0372 | 0.0339
Epoch 249/300, seasonal_3 Loss: 0.0373 | 0.0331
Epoch 250/300, seasonal_3 Loss: 0.0363 | 0.0340
Epoch 251/300, seasonal_3 Loss: 0.0368 | 0.0358
Epoch 252/300, seasonal_3 Loss: 0.0375 | 0.0352
Epoch 253/300, seasonal_3 Loss: 0.0371 | 0.0334
Epoch 254/300, seasonal_3 Loss: 0.0368 | 0.0346
Epoch 255/300, seasonal_3 Loss: 0.0370 | 0.0332
Epoch 256/300, seasonal_3 Loss: 0.0364 | 0.0349
Epoch 257/300, seasonal_3 Loss: 0.0363 | 0.0338
Epoch 258/300, seasonal_3 Loss: 0.0354 | 0.0345
Epoch 259/300, seasonal_3 Loss: 0.0356 | 0.0370
Epoch 260/300, seasonal_3 Loss: 0.0360 | 0.0353
Epoch 261/300, seasonal_3 Loss: 0.0355 | 0.0347
Epoch 262/300, seasonal_3 Loss: 0.0359 | 0.0340
Epoch 263/300, seasonal_3 Loss: 0.0367 | 0.0341
Epoch 264/300, seasonal_3 Loss: 0.0358 | 0.0346
Epoch 265/300, seasonal_3 Loss: 0.0355 | 0.0346
Epoch 266/300, seasonal_3 Loss: 0.0356 | 0.0344
Epoch 267/300, seasonal_3 Loss: 0.0350 | 0.0346
Epoch 268/300, seasonal_3 Loss: 0.0349 | 0.0359
Epoch 269/300, seasonal_3 Loss: 0.0349 | 0.0343
Epoch 270/300, seasonal_3 Loss: 0.0347 | 0.0356
Epoch 271/300, seasonal_3 Loss: 0.0357 | 0.0342
Epoch 272/300, seasonal_3 Loss: 0.0353 | 0.0358
Epoch 273/300, seasonal_3 Loss: 0.0351 | 0.0358
Epoch 274/300, seasonal_3 Loss: 0.0359 | 0.0363
Epoch 275/300, seasonal_3 Loss: 0.0354 | 0.0349
Epoch 276/300, seasonal_3 Loss: 0.0348 | 0.0364
Epoch 277/300, seasonal_3 Loss: 0.0345 | 0.0365
Epoch 278/300, seasonal_3 Loss: 0.0344 | 0.0345
Epoch 279/300, seasonal_3 Loss: 0.0343 | 0.0344
Epoch 280/300, seasonal_3 Loss: 0.0340 | 0.0345
Epoch 281/300, seasonal_3 Loss: 0.0338 | 0.0351
Epoch 282/300, seasonal_3 Loss: 0.0339 | 0.0357
Epoch 283/300, seasonal_3 Loss: 0.0336 | 0.0350
Epoch 284/300, seasonal_3 Loss: 0.0336 | 0.0379
Epoch 285/300, seasonal_3 Loss: 0.0335 | 0.0364
Epoch 286/300, seasonal_3 Loss: 0.0331 | 0.0349
Epoch 287/300, seasonal_3 Loss: 0.0335 | 0.0370
Epoch 288/300, seasonal_3 Loss: 0.0348 | 0.0346
Epoch 289/300, seasonal_3 Loss: 0.0341 | 0.0354
Epoch 290/300, seasonal_3 Loss: 0.0335 | 0.0362
Epoch 291/300, seasonal_3 Loss: 0.0342 | 0.0356
Epoch 292/300, seasonal_3 Loss: 0.0338 | 0.0368
Epoch 293/300, seasonal_3 Loss: 0.0333 | 0.0358
Epoch 294/300, seasonal_3 Loss: 0.0333 | 0.0355
Epoch 295/300, seasonal_3 Loss: 0.0330 | 0.0350
Epoch 296/300, seasonal_3 Loss: 0.0335 | 0.0345
Epoch 297/300, seasonal_3 Loss: 0.0334 | 0.0354
Epoch 298/300, seasonal_3 Loss: 0.0327 | 0.0355
Epoch 299/300, seasonal_3 Loss: 0.0329 | 0.0364
Epoch 300/300, seasonal_3 Loss: 0.0326 | 0.0366
Training resid component with params: {'observation_period_num': 16, 'train_rates': 0.8160901019268822, 'learning_rate': 0.000608169918442636, 'batch_size': 20, 'step_size': 1, 'gamma': 0.8620190761820923}
Epoch 1/300, resid Loss: 0.1957 | 0.1282
Epoch 2/300, resid Loss: 0.1173 | 0.0899
Epoch 3/300, resid Loss: 0.1132 | 0.0740
Epoch 4/300, resid Loss: 0.1050 | 0.0654
Epoch 5/300, resid Loss: 0.0949 | 0.0572
Epoch 6/300, resid Loss: 0.0890 | 0.0543
Epoch 7/300, resid Loss: 0.0851 | 0.0510
Epoch 8/300, resid Loss: 0.0823 | 0.0516
Epoch 9/300, resid Loss: 0.0804 | 0.0508
Epoch 10/300, resid Loss: 0.0793 | 0.0494
Epoch 11/300, resid Loss: 0.0786 | 0.0475
Epoch 12/300, resid Loss: 0.0780 | 0.0462
Epoch 13/300, resid Loss: 0.0777 | 0.0458
Epoch 14/300, resid Loss: 0.0776 | 0.0459
Epoch 15/300, resid Loss: 0.0774 | 0.0465
Epoch 16/300, resid Loss: 0.0771 | 0.0474
Epoch 17/300, resid Loss: 0.0767 | 0.0482
Epoch 18/300, resid Loss: 0.0764 | 0.0486
Epoch 19/300, resid Loss: 0.0761 | 0.0487
Epoch 20/300, resid Loss: 0.0760 | 0.0486
Epoch 21/300, resid Loss: 0.0758 | 0.0485
Epoch 22/300, resid Loss: 0.0758 | 0.0484
Epoch 23/300, resid Loss: 0.0757 | 0.0483
Epoch 24/300, resid Loss: 0.0757 | 0.0482
Epoch 25/300, resid Loss: 0.0756 | 0.0482
Epoch 26/300, resid Loss: 0.0756 | 0.0482
Epoch 27/300, resid Loss: 0.0756 | 0.0482
Epoch 28/300, resid Loss: 0.0756 | 0.0481
Epoch 29/300, resid Loss: 0.0755 | 0.0481
Epoch 30/300, resid Loss: 0.0755 | 0.0481
Epoch 31/300, resid Loss: 0.0755 | 0.0481
Epoch 32/300, resid Loss: 0.0755 | 0.0481
Epoch 33/300, resid Loss: 0.0755 | 0.0481
Epoch 34/300, resid Loss: 0.0755 | 0.0481
Epoch 35/300, resid Loss: 0.0755 | 0.0481
Epoch 36/300, resid Loss: 0.0755 | 0.0481
Epoch 37/300, resid Loss: 0.0755 | 0.0481
Epoch 38/300, resid Loss: 0.0755 | 0.0481
Epoch 39/300, resid Loss: 0.0755 | 0.0481
Epoch 40/300, resid Loss: 0.0755 | 0.0481
Epoch 41/300, resid Loss: 0.0755 | 0.0481
Epoch 42/300, resid Loss: 0.0755 | 0.0481
Epoch 43/300, resid Loss: 0.0755 | 0.0481
Epoch 44/300, resid Loss: 0.0755 | 0.0481
Epoch 45/300, resid Loss: 0.0755 | 0.0481
Epoch 46/300, resid Loss: 0.0755 | 0.0481
Epoch 47/300, resid Loss: 0.0755 | 0.0481
Epoch 48/300, resid Loss: 0.0755 | 0.0481
Epoch 49/300, resid Loss: 0.0755 | 0.0481
Epoch 50/300, resid Loss: 0.0755 | 0.0481
Epoch 51/300, resid Loss: 0.0755 | 0.0481
Epoch 52/300, resid Loss: 0.0755 | 0.0481
Epoch 53/300, resid Loss: 0.0755 | 0.0481
Epoch 54/300, resid Loss: 0.0755 | 0.0481
Epoch 55/300, resid Loss: 0.0755 | 0.0481
Epoch 56/300, resid Loss: 0.0755 | 0.0481
Epoch 57/300, resid Loss: 0.0755 | 0.0481
Epoch 58/300, resid Loss: 0.0755 | 0.0481
Epoch 59/300, resid Loss: 0.0755 | 0.0481
Epoch 60/300, resid Loss: 0.0755 | 0.0481
Epoch 61/300, resid Loss: 0.0755 | 0.0481
Epoch 62/300, resid Loss: 0.0755 | 0.0481
Epoch 63/300, resid Loss: 0.0755 | 0.0481
Epoch 64/300, resid Loss: 0.0755 | 0.0481
Epoch 65/300, resid Loss: 0.0755 | 0.0481
Epoch 66/300, resid Loss: 0.0755 | 0.0481
Epoch 67/300, resid Loss: 0.0755 | 0.0481
Epoch 68/300, resid Loss: 0.0755 | 0.0481
Epoch 69/300, resid Loss: 0.0755 | 0.0481
Epoch 70/300, resid Loss: 0.0755 | 0.0481
Epoch 71/300, resid Loss: 0.0755 | 0.0481
Epoch 72/300, resid Loss: 0.0755 | 0.0481
Epoch 73/300, resid Loss: 0.0755 | 0.0481
Epoch 74/300, resid Loss: 0.0755 | 0.0481
Early stopping for resid
Runtime (seconds): 1230.3449783325195
0.0002690255145462724
[154.23557]
[-1.4463565]
[-2.7693603]
[11.007325]
[3.1485105]
[8.80383]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 16.97838343679905
RMSE: 4.1204833984375
MAE: 4.1204833984375
R-squared: nan
[172.97952]
