[32m[I 2025-01-07 09:06:28,688][0m A new study created in memory with name: no-name-736f83cb-acc9-43ad-a1ea-b66a9d92a1ad[0m
[32m[I 2025-01-07 09:12:06,982][0m Trial 0 finished with value: 0.1336759328842163 and parameters: {'observation_period_num': 216, 'train_rates': 0.9878491290980508, 'learning_rate': 0.0004611451338806193, 'batch_size': 185, 'step_size': 10, 'gamma': 0.8749672809297262}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:14:00,513][0m Trial 1 finished with value: 0.9517815803211619 and parameters: {'observation_period_num': 108, 'train_rates': 0.6355025937352998, 'learning_rate': 1.4733617033309698e-05, 'batch_size': 110, 'step_size': 9, 'gamma': 0.9689256612742314}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:16:37,292][0m Trial 2 finished with value: 0.9479005793014942 and parameters: {'observation_period_num': 142, 'train_rates': 0.6165146898930206, 'learning_rate': 7.053137761047805e-05, 'batch_size': 143, 'step_size': 2, 'gamma': 0.9412130568777671}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:18:19,093][0m Trial 3 finished with value: 0.6374247219640884 and parameters: {'observation_period_num': 77, 'train_rates': 0.8527508742092689, 'learning_rate': 0.0005821222037104917, 'batch_size': 88, 'step_size': 2, 'gamma': 0.9710192359051721}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:24:15,805][0m Trial 4 finished with value: 0.19032452533165908 and parameters: {'observation_period_num': 235, 'train_rates': 0.9010350892844619, 'learning_rate': 4.212577762055517e-05, 'batch_size': 74, 'step_size': 15, 'gamma': 0.7825715784477367}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:25:38,945][0m Trial 5 finished with value: 0.6189193590216233 and parameters: {'observation_period_num': 66, 'train_rates': 0.8042375552091243, 'learning_rate': 0.0006580512571816472, 'batch_size': 106, 'step_size': 14, 'gamma': 0.9661289626519459}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:30:13,007][0m Trial 6 finished with value: 0.8732580922631656 and parameters: {'observation_period_num': 206, 'train_rates': 0.692168130548344, 'learning_rate': 1.78618174902663e-06, 'batch_size': 26, 'step_size': 13, 'gamma': 0.9363034290569114}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:32:01,564][0m Trial 7 finished with value: 1.3213790068278455 and parameters: {'observation_period_num': 101, 'train_rates': 0.6602178708649522, 'learning_rate': 4.564000741408314e-06, 'batch_size': 131, 'step_size': 4, 'gamma': 0.9809947904123307}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:33:23,302][0m Trial 8 finished with value: 0.32933482108329304 and parameters: {'observation_period_num': 62, 'train_rates': 0.8959804614059961, 'learning_rate': 0.0007411321174228643, 'batch_size': 117, 'step_size': 7, 'gamma': 0.7557676449792529}. Best is trial 0 with value: 0.1336759328842163.[0m
Early stopping at epoch 79
[32m[I 2025-01-07 09:33:55,158][0m Trial 9 finished with value: 0.741853674252828 and parameters: {'observation_period_num': 29, 'train_rates': 0.8110801880667977, 'learning_rate': 5.684692939463755e-05, 'batch_size': 148, 'step_size': 1, 'gamma': 0.8369488901802558}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:38:03,024][0m Trial 10 finished with value: 0.14185945689678192 and parameters: {'observation_period_num': 166, 'train_rates': 0.9861378364342078, 'learning_rate': 0.00018128682870899602, 'batch_size': 233, 'step_size': 11, 'gamma': 0.8797003765722733}. Best is trial 0 with value: 0.1336759328842163.[0m
[32m[I 2025-01-07 09:42:19,114][0m Trial 11 finished with value: 0.13082830607891083 and parameters: {'observation_period_num': 171, 'train_rates': 0.9855346635867108, 'learning_rate': 0.00019713802587956961, 'batch_size': 230, 'step_size': 10, 'gamma': 0.8781856165517165}. Best is trial 11 with value: 0.13082830607891083.[0m
[32m[I 2025-01-07 09:46:54,687][0m Trial 12 finished with value: 0.1224745362997055 and parameters: {'observation_period_num': 181, 'train_rates': 0.9858952684120376, 'learning_rate': 0.00025321670349992656, 'batch_size': 221, 'step_size': 7, 'gamma': 0.8773599049891521}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 09:51:10,191][0m Trial 13 finished with value: 0.21978458762168884 and parameters: {'observation_period_num': 176, 'train_rates': 0.932784080622027, 'learning_rate': 0.0001641783836820472, 'batch_size': 247, 'step_size': 6, 'gamma': 0.8364511430157059}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 09:54:56,906][0m Trial 14 finished with value: 0.5346892137233525 and parameters: {'observation_period_num': 182, 'train_rates': 0.737649211061908, 'learning_rate': 0.00017717311822125885, 'batch_size': 210, 'step_size': 6, 'gamma': 0.9027684268260635}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 09:58:19,997][0m Trial 15 finished with value: 0.4195100963115692 and parameters: {'observation_period_num': 142, 'train_rates': 0.9459065513119418, 'learning_rate': 1.715061107987829e-05, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8321595804047807}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 10:04:30,352][0m Trial 16 finished with value: 0.22921625731173795 and parameters: {'observation_period_num': 250, 'train_rates': 0.860516579072042, 'learning_rate': 0.00023817563002848422, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9158646379512514}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 10:09:06,780][0m Trial 17 finished with value: 0.7400511438640633 and parameters: {'observation_period_num': 209, 'train_rates': 0.7522911279614184, 'learning_rate': 8.280939688090512e-05, 'batch_size': 208, 'step_size': 4, 'gamma': 0.8125405657027013}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 10:12:47,336][0m Trial 18 finished with value: 0.3011724811452658 and parameters: {'observation_period_num': 153, 'train_rates': 0.9314809354429638, 'learning_rate': 2.0637569808619335e-05, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8939171563118837}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 10:15:22,135][0m Trial 19 finished with value: 0.17121095955371857 and parameters: {'observation_period_num': 113, 'train_rates': 0.9659835088741083, 'learning_rate': 0.0003177290815158958, 'batch_size': 219, 'step_size': 8, 'gamma': 0.8561353903575363}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 10:19:52,524][0m Trial 20 finished with value: 0.20986299939498215 and parameters: {'observation_period_num': 191, 'train_rates': 0.8796623906978164, 'learning_rate': 0.00010641042158502823, 'batch_size': 164, 'step_size': 5, 'gamma': 0.9307969470691003}. Best is trial 12 with value: 0.1224745362997055.[0m
[32m[I 2025-01-07 10:25:47,059][0m Trial 21 finished with value: 0.11283180117607117 and parameters: {'observation_period_num': 222, 'train_rates': 0.9894383416211037, 'learning_rate': 0.0003222326896920157, 'batch_size': 185, 'step_size': 10, 'gamma': 0.8727714452610752}. Best is trial 21 with value: 0.11283180117607117.[0m
[32m[I 2025-01-07 10:31:28,182][0m Trial 22 finished with value: 0.8038761019706726 and parameters: {'observation_period_num': 221, 'train_rates': 0.939858592614069, 'learning_rate': 0.0009512125383020813, 'batch_size': 199, 'step_size': 11, 'gamma': 0.8605736650716149}. Best is trial 21 with value: 0.11283180117607117.[0m
[32m[I 2025-01-07 10:36:34,901][0m Trial 23 finished with value: 0.11041165888309479 and parameters: {'observation_period_num': 197, 'train_rates': 0.9826434401473204, 'learning_rate': 0.0003082274892454046, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8937687838010115}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 10:42:56,369][0m Trial 24 finished with value: 0.15777833824013843 and parameters: {'observation_period_num': 251, 'train_rates': 0.9147503924444818, 'learning_rate': 0.0003700530616598239, 'batch_size': 195, 'step_size': 7, 'gamma': 0.9034398196799612}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 10:47:52,400][0m Trial 25 finished with value: 0.2040562629699707 and parameters: {'observation_period_num': 195, 'train_rates': 0.9593987589322553, 'learning_rate': 0.00011069462154974124, 'batch_size': 160, 'step_size': 9, 'gamma': 0.852411751916799}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 10:53:18,273][0m Trial 26 finished with value: 0.8751130939289263 and parameters: {'observation_period_num': 230, 'train_rates': 0.8277853510200369, 'learning_rate': 6.851663082368579e-06, 'batch_size': 236, 'step_size': 12, 'gamma': 0.8027224759755808}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 10:57:40,705][0m Trial 27 finished with value: 0.5032950830823593 and parameters: {'observation_period_num': 201, 'train_rates': 0.7632446049859538, 'learning_rate': 0.000342196622633278, 'batch_size': 216, 'step_size': 9, 'gamma': 0.8901121416499743}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 11:01:33,928][0m Trial 28 finished with value: 0.27783605456352234 and parameters: {'observation_period_num': 160, 'train_rates': 0.9568532620867034, 'learning_rate': 2.9467066348733996e-05, 'batch_size': 191, 'step_size': 7, 'gamma': 0.9133233511137896}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 11:07:30,713][0m Trial 29 finished with value: 0.13050386309623718 and parameters: {'observation_period_num': 223, 'train_rates': 0.9880812432293361, 'learning_rate': 0.0004417779122343778, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8750531039704466}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 11:10:16,780][0m Trial 30 finished with value: 0.19127165410807942 and parameters: {'observation_period_num': 128, 'train_rates': 0.915787535897317, 'learning_rate': 0.00012277379101506685, 'batch_size': 226, 'step_size': 9, 'gamma': 0.9501866733707346}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 11:16:01,516][0m Trial 31 finished with value: 0.11232797801494598 and parameters: {'observation_period_num': 218, 'train_rates': 0.9898041990507292, 'learning_rate': 0.00044071963648146633, 'batch_size': 170, 'step_size': 11, 'gamma': 0.8707750756025924}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 11:22:17,123][0m Trial 32 finished with value: 0.1415145993232727 and parameters: {'observation_period_num': 239, 'train_rates': 0.9658381404456365, 'learning_rate': 0.00030615696042577176, 'batch_size': 185, 'step_size': 12, 'gamma': 0.8630146112202312}. Best is trial 23 with value: 0.11041165888309479.[0m
[32m[I 2025-01-07 11:28:05,121][0m Trial 33 finished with value: 0.09906656295061111 and parameters: {'observation_period_num': 217, 'train_rates': 0.9896138826033688, 'learning_rate': 0.0005031146592020774, 'batch_size': 205, 'step_size': 8, 'gamma': 0.8451722806676317}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 11:33:32,830][0m Trial 34 finished with value: 0.13038338720798492 and parameters: {'observation_period_num': 211, 'train_rates': 0.9706112885009337, 'learning_rate': 0.0005368937479221093, 'batch_size': 204, 'step_size': 10, 'gamma': 0.8235368306571488}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 11:39:25,162][0m Trial 35 finished with value: 0.27753399633096926 and parameters: {'observation_period_num': 237, 'train_rates': 0.8790388397321558, 'learning_rate': 0.0006451470819808354, 'batch_size': 151, 'step_size': 13, 'gamma': 0.8475161377829801}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 11:45:01,081][0m Trial 36 finished with value: 0.16490875825083634 and parameters: {'observation_period_num': 222, 'train_rates': 0.9195926737483523, 'learning_rate': 0.00044463475695363645, 'batch_size': 183, 'step_size': 9, 'gamma': 0.7988966689538575}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 11:49:53,940][0m Trial 37 finished with value: 0.7047848105430603 and parameters: {'observation_period_num': 194, 'train_rates': 0.9474720044460437, 'learning_rate': 0.0009694357404076356, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9214687877160043}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 11:55:13,531][0m Trial 38 finished with value: 0.23005275122377256 and parameters: {'observation_period_num': 216, 'train_rates': 0.8903093668829273, 'learning_rate': 4.381467338030146e-05, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8910778682941926}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:01:05,418][0m Trial 39 finished with value: 0.35611018256458177 and parameters: {'observation_period_num': 238, 'train_rates': 0.856838996820639, 'learning_rate': 8.320815036312071e-06, 'batch_size': 58, 'step_size': 13, 'gamma': 0.8460382083897495}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:04:56,500][0m Trial 40 finished with value: 1.7486742378688747 and parameters: {'observation_period_num': 205, 'train_rates': 0.6011837458314508, 'learning_rate': 1.5333943492574478e-06, 'batch_size': 124, 'step_size': 10, 'gamma': 0.7834479544031496}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:09:30,335][0m Trial 41 finished with value: 0.1280098408460617 and parameters: {'observation_period_num': 182, 'train_rates': 0.9851298309023863, 'learning_rate': 0.00025638385014976005, 'batch_size': 216, 'step_size': 6, 'gamma': 0.8706596547085063}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:14:14,416][0m Trial 42 finished with value: 0.1352432817220688 and parameters: {'observation_period_num': 188, 'train_rates': 0.9701514296380881, 'learning_rate': 0.0005326504669856499, 'batch_size': 194, 'step_size': 7, 'gamma': 0.8818800116853587}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:18:03,908][0m Trial 43 finished with value: 0.7079283595085144 and parameters: {'observation_period_num': 154, 'train_rates': 0.9845702182552709, 'learning_rate': 0.000791229920203417, 'batch_size': 155, 'step_size': 8, 'gamma': 0.8974953118332326}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:23:57,242][0m Trial 44 finished with value: 0.20885175466537476 and parameters: {'observation_period_num': 225, 'train_rates': 0.9512625648320988, 'learning_rate': 0.00023995224374069533, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8614591032655411}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:28:06,898][0m Trial 45 finished with value: 0.6696673145950782 and parameters: {'observation_period_num': 202, 'train_rates': 0.6793804513357653, 'learning_rate': 0.00016335466895184286, 'batch_size': 100, 'step_size': 5, 'gamma': 0.8854583967122017}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:28:43,342][0m Trial 46 finished with value: 0.1919599175453186 and parameters: {'observation_period_num': 25, 'train_rates': 0.9253767769262212, 'learning_rate': 7.924276892770433e-05, 'batch_size': 224, 'step_size': 9, 'gamma': 0.871201620961807}. Best is trial 33 with value: 0.09906656295061111.[0m
[32m[I 2025-01-07 12:29:01,944][0m Trial 47 finished with value: 0.08107035607099533 and parameters: {'observation_period_num': 8, 'train_rates': 0.9898254345721945, 'learning_rate': 0.0004534812000800014, 'batch_size': 244, 'step_size': 8, 'gamma': 0.8262559405320543}. Best is trial 47 with value: 0.08107035607099533.[0m
[32m[I 2025-01-07 12:29:21,089][0m Trial 48 finished with value: 0.19612842798233032 and parameters: {'observation_period_num': 9, 'train_rates': 0.9713878252589535, 'learning_rate': 0.0006579921856210057, 'batch_size': 241, 'step_size': 10, 'gamma': 0.8246113890220785}. Best is trial 47 with value: 0.08107035607099533.[0m
[32m[I 2025-01-07 12:31:14,439][0m Trial 49 finished with value: 0.18263618520250285 and parameters: {'observation_period_num': 85, 'train_rates': 0.9050045937854245, 'learning_rate': 0.00014736625793438087, 'batch_size': 143, 'step_size': 8, 'gamma': 0.8414278932567395}. Best is trial 47 with value: 0.08107035607099533.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.9187 | 1.2703
Epoch 2/300, Loss: 0.8446 | 0.9957
Epoch 3/300, Loss: 0.7069 | 0.9006
Epoch 4/300, Loss: 0.5891 | 0.8657
Epoch 5/300, Loss: 0.4610 | 0.7325
Epoch 6/300, Loss: 0.4044 | 0.6120
Epoch 7/300, Loss: 0.4593 | 0.5744
Epoch 8/300, Loss: 0.4690 | 0.6265
Epoch 9/300, Loss: 0.4377 | 0.6955
Epoch 10/300, Loss: 0.5265 | 0.6687
Epoch 11/300, Loss: 0.3972 | 0.5680
Epoch 12/300, Loss: 0.3470 | 0.5071
Epoch 13/300, Loss: 0.3305 | 0.5198
Epoch 14/300, Loss: 0.3274 | 0.5449
Epoch 15/300, Loss: 0.3056 | 0.4591
Epoch 16/300, Loss: 0.2659 | 0.4152
Epoch 17/300, Loss: 0.2617 | 0.4059
Epoch 18/300, Loss: 0.2276 | 0.3762
Epoch 19/300, Loss: 0.2122 | 0.3528
Epoch 20/300, Loss: 0.2042 | 0.3380
Epoch 21/300, Loss: 0.2015 | 0.3226
Epoch 22/300, Loss: 0.1994 | 0.3120
Epoch 23/300, Loss: 0.1979 | 0.2982
Epoch 24/300, Loss: 0.2001 | 0.2890
Epoch 25/300, Loss: 0.1884 | 0.2805
Epoch 26/300, Loss: 0.1787 | 0.2711
Epoch 27/300, Loss: 0.1707 | 0.2621
Epoch 28/300, Loss: 0.1666 | 0.2549
Epoch 29/300, Loss: 0.1629 | 0.2486
Epoch 30/300, Loss: 0.1603 | 0.2438
Epoch 31/300, Loss: 0.1569 | 0.2375
Epoch 32/300, Loss: 0.1545 | 0.2331
Epoch 33/300, Loss: 0.1530 | 0.2287
Epoch 34/300, Loss: 0.1509 | 0.2246
Epoch 35/300, Loss: 0.1496 | 0.2207
Epoch 36/300, Loss: 0.1475 | 0.2180
Epoch 37/300, Loss: 0.1455 | 0.2148
Epoch 38/300, Loss: 0.1447 | 0.2118
Epoch 39/300, Loss: 0.1435 | 0.2086
Epoch 40/300, Loss: 0.1425 | 0.2060
Epoch 41/300, Loss: 0.1403 | 0.2044
Epoch 42/300, Loss: 0.1397 | 0.2022
Epoch 43/300, Loss: 0.1393 | 0.2005
Epoch 44/300, Loss: 0.1379 | 0.1984
Epoch 45/300, Loss: 0.1365 | 0.1967
Epoch 46/300, Loss: 0.1364 | 0.1948
Epoch 47/300, Loss: 0.1351 | 0.1929
Epoch 48/300, Loss: 0.1344 | 0.1912
Epoch 49/300, Loss: 0.1338 | 0.1897
Epoch 50/300, Loss: 0.1328 | 0.1884
Epoch 51/300, Loss: 0.1318 | 0.1872
Epoch 52/300, Loss: 0.1310 | 0.1856
Epoch 53/300, Loss: 0.1306 | 0.1846
Epoch 54/300, Loss: 0.1303 | 0.1833
Epoch 55/300, Loss: 0.1294 | 0.1820
Epoch 56/300, Loss: 0.1292 | 0.1812
Epoch 57/300, Loss: 0.1287 | 0.1805
Epoch 58/300, Loss: 0.1290 | 0.1797
Epoch 59/300, Loss: 0.1279 | 0.1789
Epoch 60/300, Loss: 0.1276 | 0.1778
Epoch 61/300, Loss: 0.1273 | 0.1770
Epoch 62/300, Loss: 0.1267 | 0.1762
Epoch 63/300, Loss: 0.1261 | 0.1755
Epoch 64/300, Loss: 0.1255 | 0.1747
Epoch 65/300, Loss: 0.1253 | 0.1741
Epoch 66/300, Loss: 0.1245 | 0.1735
Epoch 67/300, Loss: 0.1246 | 0.1729
Epoch 68/300, Loss: 0.1242 | 0.1721
Epoch 69/300, Loss: 0.1240 | 0.1713
Epoch 70/300, Loss: 0.1235 | 0.1708
Epoch 71/300, Loss: 0.1232 | 0.1703
Epoch 72/300, Loss: 0.1235 | 0.1698
Epoch 73/300, Loss: 0.1229 | 0.1695
Epoch 74/300, Loss: 0.1227 | 0.1692
Epoch 75/300, Loss: 0.1229 | 0.1688
Epoch 76/300, Loss: 0.1219 | 0.1683
Epoch 77/300, Loss: 0.1220 | 0.1679
Epoch 78/300, Loss: 0.1218 | 0.1676
Epoch 79/300, Loss: 0.1216 | 0.1674
Epoch 80/300, Loss: 0.1219 | 0.1671
Epoch 81/300, Loss: 0.1209 | 0.1666
Epoch 82/300, Loss: 0.1215 | 0.1663
Epoch 83/300, Loss: 0.1210 | 0.1659
Epoch 84/300, Loss: 0.1203 | 0.1656
Epoch 85/300, Loss: 0.1206 | 0.1654
Epoch 86/300, Loss: 0.1200 | 0.1652
Epoch 87/300, Loss: 0.1200 | 0.1649
Epoch 88/300, Loss: 0.1206 | 0.1646
Epoch 89/300, Loss: 0.1198 | 0.1643
Epoch 90/300, Loss: 0.1204 | 0.1641
Epoch 91/300, Loss: 0.1202 | 0.1638
Epoch 92/300, Loss: 0.1193 | 0.1635
Epoch 93/300, Loss: 0.1197 | 0.1634
Epoch 94/300, Loss: 0.1196 | 0.1631
Epoch 95/300, Loss: 0.1192 | 0.1629
Epoch 96/300, Loss: 0.1197 | 0.1628
Epoch 97/300, Loss: 0.1192 | 0.1627
Epoch 98/300, Loss: 0.1188 | 0.1626
Epoch 99/300, Loss: 0.1190 | 0.1625
Epoch 100/300, Loss: 0.1196 | 0.1624
Epoch 101/300, Loss: 0.1188 | 0.1622
Epoch 102/300, Loss: 0.1185 | 0.1621
Epoch 103/300, Loss: 0.1185 | 0.1619
Epoch 104/300, Loss: 0.1189 | 0.1618
Epoch 105/300, Loss: 0.1191 | 0.1617
Epoch 106/300, Loss: 0.1184 | 0.1615
Epoch 107/300, Loss: 0.1183 | 0.1614
Epoch 108/300, Loss: 0.1182 | 0.1612
Epoch 109/300, Loss: 0.1184 | 0.1611
Epoch 110/300, Loss: 0.1185 | 0.1610
Epoch 111/300, Loss: 0.1187 | 0.1609
Epoch 112/300, Loss: 0.1179 | 0.1609
Epoch 113/300, Loss: 0.1181 | 0.1608
Epoch 114/300, Loss: 0.1179 | 0.1608
Epoch 115/300, Loss: 0.1176 | 0.1607
Epoch 116/300, Loss: 0.1180 | 0.1606
Epoch 117/300, Loss: 0.1179 | 0.1605
Epoch 118/300, Loss: 0.1176 | 0.1604
Epoch 119/300, Loss: 0.1181 | 0.1603
Epoch 120/300, Loss: 0.1175 | 0.1603
Epoch 121/300, Loss: 0.1181 | 0.1602
Epoch 122/300, Loss: 0.1179 | 0.1601
Epoch 123/300, Loss: 0.1182 | 0.1601
Epoch 124/300, Loss: 0.1179 | 0.1600
Epoch 125/300, Loss: 0.1172 | 0.1599
Epoch 126/300, Loss: 0.1171 | 0.1599
Epoch 127/300, Loss: 0.1173 | 0.1599
Epoch 128/300, Loss: 0.1173 | 0.1598
Epoch 129/300, Loss: 0.1179 | 0.1598
Epoch 130/300, Loss: 0.1172 | 0.1598
Epoch 131/300, Loss: 0.1168 | 0.1597
Epoch 132/300, Loss: 0.1175 | 0.1597
Epoch 133/300, Loss: 0.1175 | 0.1596
Epoch 134/300, Loss: 0.1171 | 0.1596
Epoch 135/300, Loss: 0.1178 | 0.1596
Epoch 136/300, Loss: 0.1174 | 0.1595
Epoch 137/300, Loss: 0.1174 | 0.1595
Epoch 138/300, Loss: 0.1173 | 0.1595
Epoch 139/300, Loss: 0.1171 | 0.1594
Epoch 140/300, Loss: 0.1172 | 0.1594
Epoch 141/300, Loss: 0.1178 | 0.1594
Epoch 142/300, Loss: 0.1179 | 0.1594
Epoch 143/300, Loss: 0.1171 | 0.1593
Epoch 144/300, Loss: 0.1171 | 0.1593
Epoch 145/300, Loss: 0.1171 | 0.1593
Epoch 146/300, Loss: 0.1175 | 0.1593
Epoch 147/300, Loss: 0.1173 | 0.1593
Epoch 148/300, Loss: 0.1174 | 0.1592
Epoch 149/300, Loss: 0.1171 | 0.1592
Epoch 150/300, Loss: 0.1175 | 0.1592
Epoch 151/300, Loss: 0.1178 | 0.1592
Epoch 152/300, Loss: 0.1180 | 0.1591
Epoch 153/300, Loss: 0.1172 | 0.1591
Epoch 154/300, Loss: 0.1168 | 0.1591
Epoch 155/300, Loss: 0.1175 | 0.1591
Epoch 156/300, Loss: 0.1180 | 0.1591
Epoch 157/300, Loss: 0.1169 | 0.1591
Epoch 158/300, Loss: 0.1171 | 0.1591
Epoch 159/300, Loss: 0.1178 | 0.1591
Epoch 160/300, Loss: 0.1176 | 0.1590
Epoch 161/300, Loss: 0.1174 | 0.1590
Epoch 162/300, Loss: 0.1177 | 0.1590
Epoch 163/300, Loss: 0.1171 | 0.1590
Epoch 164/300, Loss: 0.1179 | 0.1590
Epoch 165/300, Loss: 0.1178 | 0.1590
Epoch 166/300, Loss: 0.1171 | 0.1590
Epoch 167/300, Loss: 0.1177 | 0.1590
Epoch 168/300, Loss: 0.1178 | 0.1590
Epoch 169/300, Loss: 0.1170 | 0.1590
Epoch 170/300, Loss: 0.1172 | 0.1590
Epoch 171/300, Loss: 0.1174 | 0.1590
Epoch 172/300, Loss: 0.1165 | 0.1589
Epoch 173/300, Loss: 0.1177 | 0.1589
Epoch 174/300, Loss: 0.1170 | 0.1589
Epoch 175/300, Loss: 0.1172 | 0.1589
Epoch 176/300, Loss: 0.1174 | 0.1589
Epoch 177/300, Loss: 0.1174 | 0.1589
Epoch 178/300, Loss: 0.1175 | 0.1589
Epoch 179/300, Loss: 0.1170 | 0.1589
Epoch 180/300, Loss: 0.1171 | 0.1589
Epoch 181/300, Loss: 0.1171 | 0.1589
Epoch 182/300, Loss: 0.1170 | 0.1589
Epoch 183/300, Loss: 0.1174 | 0.1589
Epoch 184/300, Loss: 0.1167 | 0.1589
Epoch 185/300, Loss: 0.1170 | 0.1589
Epoch 186/300, Loss: 0.1171 | 0.1589
Epoch 187/300, Loss: 0.1168 | 0.1589
Epoch 188/300, Loss: 0.1167 | 0.1589
Epoch 189/300, Loss: 0.1167 | 0.1589
Epoch 190/300, Loss: 0.1164 | 0.1589
Epoch 191/300, Loss: 0.1169 | 0.1589
Epoch 192/300, Loss: 0.1167 | 0.1589
Epoch 193/300, Loss: 0.1164 | 0.1589
Epoch 194/300, Loss: 0.1170 | 0.1589
Epoch 195/300, Loss: 0.1171 | 0.1589
Epoch 196/300, Loss: 0.1170 | 0.1589
Epoch 197/300, Loss: 0.1174 | 0.1588
Epoch 198/300, Loss: 0.1171 | 0.1588
Epoch 199/300, Loss: 0.1171 | 0.1588
Epoch 200/300, Loss: 0.1170 | 0.1588
Epoch 201/300, Loss: 0.1174 | 0.1588
Epoch 202/300, Loss: 0.1165 | 0.1588
Epoch 203/300, Loss: 0.1166 | 0.1588
Epoch 204/300, Loss: 0.1171 | 0.1588
Epoch 205/300, Loss: 0.1170 | 0.1588
Epoch 206/300, Loss: 0.1171 | 0.1588
Epoch 207/300, Loss: 0.1170 | 0.1588
Epoch 208/300, Loss: 0.1171 | 0.1588
Epoch 209/300, Loss: 0.1169 | 0.1588
Epoch 210/300, Loss: 0.1171 | 0.1588
Epoch 211/300, Loss: 0.1169 | 0.1588
Epoch 212/300, Loss: 0.1171 | 0.1588
Epoch 213/300, Loss: 0.1167 | 0.1588
Epoch 214/300, Loss: 0.1168 | 0.1588
Epoch 215/300, Loss: 0.1174 | 0.1588
Epoch 216/300, Loss: 0.1166 | 0.1588
Epoch 217/300, Loss: 0.1169 | 0.1588
Epoch 218/300, Loss: 0.1171 | 0.1588
Epoch 219/300, Loss: 0.1174 | 0.1588
Epoch 220/300, Loss: 0.1165 | 0.1588
Epoch 221/300, Loss: 0.1165 | 0.1588
Epoch 222/300, Loss: 0.1174 | 0.1588
Epoch 223/300, Loss: 0.1167 | 0.1588
Epoch 224/300, Loss: 0.1174 | 0.1588
Epoch 225/300, Loss: 0.1168 | 0.1588
Epoch 226/300, Loss: 0.1167 | 0.1588
Epoch 227/300, Loss: 0.1168 | 0.1588
Epoch 228/300, Loss: 0.1169 | 0.1588
Epoch 229/300, Loss: 0.1165 | 0.1588
Epoch 230/300, Loss: 0.1167 | 0.1588
Epoch 231/300, Loss: 0.1168 | 0.1588
Epoch 232/300, Loss: 0.1169 | 0.1588
Epoch 233/300, Loss: 0.1174 | 0.1588
Epoch 234/300, Loss: 0.1174 | 0.1588
Epoch 235/300, Loss: 0.1173 | 0.1588
Epoch 236/300, Loss: 0.1168 | 0.1588
Epoch 237/300, Loss: 0.1173 | 0.1588
Epoch 238/300, Loss: 0.1174 | 0.1588
Epoch 239/300, Loss: 0.1165 | 0.1588
Epoch 240/300, Loss: 0.1175 | 0.1588
Epoch 241/300, Loss: 0.1167 | 0.1588
Epoch 242/300, Loss: 0.1166 | 0.1588
Epoch 243/300, Loss: 0.1170 | 0.1588
Epoch 244/300, Loss: 0.1172 | 0.1588
Epoch 245/300, Loss: 0.1171 | 0.1588
Epoch 246/300, Loss: 0.1171 | 0.1588
Epoch 247/300, Loss: 0.1170 | 0.1588
Epoch 248/300, Loss: 0.1174 | 0.1588
Epoch 249/300, Loss: 0.1173 | 0.1588
Epoch 250/300, Loss: 0.1170 | 0.1588
Epoch 251/300, Loss: 0.1171 | 0.1588
Epoch 252/300, Loss: 0.1172 | 0.1588
Early stopping
Runtime (seconds): 45.81175994873047
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 2352.118272718275
RMSE: 48.49864196777344
MAE: 48.49864196777344
R-squared: nan
[199.63136]
