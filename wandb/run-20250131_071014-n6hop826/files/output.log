ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-31 07:10:18,553][0m A new study created in memory with name: no-name-b7c04703-69fe-4664-b993-5d915bb78af6[0m
[32m[I 2025-01-31 07:10:51,853][0m Trial 0 finished with value: 0.3413707600808997 and parameters: {'observation_period_num': 62, 'train_rates': 0.6234142858844994, 'learning_rate': 0.0001284409907504021, 'batch_size': 137, 'step_size': 9, 'gamma': 0.7945538829089729}. Best is trial 0 with value: 0.3413707600808997.[0m
[32m[I 2025-01-31 07:11:58,929][0m Trial 1 finished with value: 0.3119851888754429 and parameters: {'observation_period_num': 151, 'train_rates': 0.930409168447122, 'learning_rate': 1.6831461673124737e-05, 'batch_size': 82, 'step_size': 13, 'gamma': 0.7642945979297462}. Best is trial 1 with value: 0.3119851888754429.[0m
[32m[I 2025-01-31 07:12:24,960][0m Trial 2 finished with value: 0.20269660651683807 and parameters: {'observation_period_num': 9, 'train_rates': 0.9296228221204015, 'learning_rate': 0.0009109529227099805, 'batch_size': 248, 'step_size': 1, 'gamma': 0.8821597550125662}. Best is trial 2 with value: 0.20269660651683807.[0m
[32m[I 2025-01-31 07:13:53,817][0m Trial 3 finished with value: 0.08024553102630064 and parameters: {'observation_period_num': 24, 'train_rates': 0.7138759133977792, 'learning_rate': 0.00024226280047769705, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8218457885911888}. Best is trial 3 with value: 0.08024553102630064.[0m
[32m[I 2025-01-31 07:14:26,005][0m Trial 4 finished with value: 0.17763401885470315 and parameters: {'observation_period_num': 102, 'train_rates': 0.7267430766660353, 'learning_rate': 5.430717480923883e-05, 'batch_size': 154, 'step_size': 6, 'gamma': 0.8522294886013111}. Best is trial 3 with value: 0.08024553102630064.[0m
[32m[I 2025-01-31 07:15:39,733][0m Trial 5 finished with value: 0.07108611515853472 and parameters: {'observation_period_num': 9, 'train_rates': 0.7693464857709562, 'learning_rate': 0.0002649343264916662, 'batch_size': 68, 'step_size': 6, 'gamma': 0.8882623631786818}. Best is trial 5 with value: 0.07108611515853472.[0m
[32m[I 2025-01-31 07:16:11,689][0m Trial 6 finished with value: 0.7237882614135742 and parameters: {'observation_period_num': 195, 'train_rates': 0.9569128477963089, 'learning_rate': 6.199108130703361e-06, 'batch_size': 181, 'step_size': 5, 'gamma': 0.8992952871374735}. Best is trial 5 with value: 0.07108611515853472.[0m
[32m[I 2025-01-31 07:16:35,958][0m Trial 7 finished with value: 0.4581562280654907 and parameters: {'observation_period_num': 129, 'train_rates': 0.9392192323459247, 'learning_rate': 3.165913431865388e-05, 'batch_size': 251, 'step_size': 5, 'gamma': 0.7944899297332289}. Best is trial 5 with value: 0.07108611515853472.[0m
[32m[I 2025-01-31 07:17:05,804][0m Trial 8 finished with value: 0.10864161369179709 and parameters: {'observation_period_num': 52, 'train_rates': 0.627529676473302, 'learning_rate': 0.00044010744081252794, 'batch_size': 154, 'step_size': 8, 'gamma': 0.8811015881648936}. Best is trial 5 with value: 0.07108611515853472.[0m
[32m[I 2025-01-31 07:17:33,998][0m Trial 9 finished with value: 0.20589192234174422 and parameters: {'observation_period_num': 233, 'train_rates': 0.7328379593506069, 'learning_rate': 4.9411491143394955e-05, 'batch_size': 179, 'step_size': 4, 'gamma': 0.9891120544428729}. Best is trial 5 with value: 0.07108611515853472.[0m
[32m[I 2025-01-31 07:22:32,221][0m Trial 10 finished with value: 0.24152301144507402 and parameters: {'observation_period_num': 87, 'train_rates': 0.8428279312851896, 'learning_rate': 1.4365836460148437e-06, 'batch_size': 17, 'step_size': 15, 'gamma': 0.939378759942319}. Best is trial 5 with value: 0.07108611515853472.[0m
[32m[I 2025-01-31 07:23:54,605][0m Trial 11 finished with value: 0.06890926739232814 and parameters: {'observation_period_num': 9, 'train_rates': 0.7815355656156522, 'learning_rate': 0.00021917944635807142, 'batch_size': 61, 'step_size': 11, 'gamma': 0.8301974210436309}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:24:51,160][0m Trial 12 finished with value: 0.10404235112570948 and parameters: {'observation_period_num': 37, 'train_rates': 0.8194560273027517, 'learning_rate': 0.00013336643336909606, 'batch_size': 94, 'step_size': 12, 'gamma': 0.91982609502685}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:25:46,423][0m Trial 13 finished with value: 0.0765454564243555 and parameters: {'observation_period_num': 6, 'train_rates': 0.7787383023502006, 'learning_rate': 0.0008148708914995898, 'batch_size': 93, 'step_size': 11, 'gamma': 0.8472216205540308}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:30:05,268][0m Trial 14 finished with value: 0.14463632935658097 and parameters: {'observation_period_num': 76, 'train_rates': 0.8610683644082769, 'learning_rate': 0.00021579843308633517, 'batch_size': 20, 'step_size': 7, 'gamma': 0.94974860315906}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:31:19,148][0m Trial 15 finished with value: 0.18015299410468733 and parameters: {'observation_period_num': 167, 'train_rates': 0.7712434182551217, 'learning_rate': 8.378998426203405e-05, 'batch_size': 64, 'step_size': 3, 'gamma': 0.8301265666836075}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:32:01,343][0m Trial 16 finished with value: 0.18513701625160303 and parameters: {'observation_period_num': 43, 'train_rates': 0.680692879482695, 'learning_rate': 1.5705919059165285e-05, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8106616025041284}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:34:00,633][0m Trial 17 finished with value: 0.17688050794769342 and parameters: {'observation_period_num': 106, 'train_rates': 0.8752766027476895, 'learning_rate': 0.00031898266807964, 'batch_size': 44, 'step_size': 15, 'gamma': 0.7538181100264201}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:35:43,766][0m Trial 18 finished with value: 0.20826006386297755 and parameters: {'observation_period_num': 242, 'train_rates': 0.8013983142685143, 'learning_rate': 0.0004442704084695979, 'batch_size': 46, 'step_size': 10, 'gamma': 0.8619494276107954}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:36:22,895][0m Trial 19 finished with value: 0.5568632340963284 and parameters: {'observation_period_num': 26, 'train_rates': 0.6784282392398456, 'learning_rate': 4.086917066285492e-06, 'batch_size': 118, 'step_size': 2, 'gamma': 0.9071103541868689}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:37:26,884][0m Trial 20 finished with value: 0.10435886262387295 and parameters: {'observation_period_num': 70, 'train_rates': 0.7593328014152978, 'learning_rate': 0.0001254946900753705, 'batch_size': 72, 'step_size': 13, 'gamma': 0.9551195547406109}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:38:16,515][0m Trial 21 finished with value: 0.07519987165578641 and parameters: {'observation_period_num': 6, 'train_rates': 0.7828653477566428, 'learning_rate': 0.0006196706313505873, 'batch_size': 101, 'step_size': 11, 'gamma': 0.8447167922172242}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:39:03,257][0m Trial 22 finished with value: 0.0821320097789418 and parameters: {'observation_period_num': 5, 'train_rates': 0.8180602339830746, 'learning_rate': 0.0005820423963427884, 'batch_size': 107, 'step_size': 7, 'gamma': 0.8411440789082602}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:40:58,420][0m Trial 23 finished with value: 0.09926020068173506 and parameters: {'observation_period_num': 31, 'train_rates': 0.7486794804193024, 'learning_rate': 0.00022047506492729182, 'batch_size': 38, 'step_size': 13, 'gamma': 0.8739051299852842}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:42:05,500][0m Trial 24 finished with value: 0.13381525088517918 and parameters: {'observation_period_num': 51, 'train_rates': 0.8799666777608308, 'learning_rate': 0.0005827692285235847, 'batch_size': 77, 'step_size': 9, 'gamma': 0.8055689798384724}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:42:42,399][0m Trial 25 finished with value: 0.07436895706120425 and parameters: {'observation_period_num': 23, 'train_rates': 0.6830855256245919, 'learning_rate': 0.0009441757994037372, 'batch_size': 127, 'step_size': 10, 'gamma': 0.7813313459430952}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:43:06,960][0m Trial 26 finished with value: 0.09609862832587843 and parameters: {'observation_period_num': 30, 'train_rates': 0.68038113143369, 'learning_rate': 0.0004001683219405431, 'batch_size': 190, 'step_size': 7, 'gamma': 0.7793477205505682}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:43:29,188][0m Trial 27 finished with value: 0.14143793229274973 and parameters: {'observation_period_num': 94, 'train_rates': 0.6510315639930483, 'learning_rate': 0.000993722666805093, 'batch_size': 209, 'step_size': 8, 'gamma': 0.7758687649209077}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:44:01,486][0m Trial 28 finished with value: 0.10634259665583043 and parameters: {'observation_period_num': 58, 'train_rates': 0.7077068575704895, 'learning_rate': 0.00017195552572824263, 'batch_size': 150, 'step_size': 10, 'gamma': 0.8138903801483395}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:44:35,556][0m Trial 29 finished with value: 0.16452972686802095 and parameters: {'observation_period_num': 124, 'train_rates': 0.6043797995243563, 'learning_rate': 6.947766948828116e-05, 'batch_size': 129, 'step_size': 10, 'gamma': 0.788487013367837}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:45:45,298][0m Trial 30 finished with value: 0.15014311600927055 and parameters: {'observation_period_num': 74, 'train_rates': 0.6467230099395368, 'learning_rate': 0.00011039070893175659, 'batch_size': 61, 'step_size': 8, 'gamma': 0.828668979676833}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:46:42,329][0m Trial 31 finished with value: 0.08473995923505802 and parameters: {'observation_period_num': 15, 'train_rates': 0.7930283985019834, 'learning_rate': 0.0003127293168156141, 'batch_size': 92, 'step_size': 11, 'gamma': 0.8950991119151797}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:47:24,234][0m Trial 32 finished with value: 0.09693462422410072 and parameters: {'observation_period_num': 21, 'train_rates': 0.8351950520360572, 'learning_rate': 0.0006282212607637864, 'batch_size': 131, 'step_size': 12, 'gamma': 0.8624857428083894}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:48:11,527][0m Trial 33 finished with value: 0.08767804336152352 and parameters: {'observation_period_num': 41, 'train_rates': 0.7440618426171094, 'learning_rate': 0.0007121509810359932, 'batch_size': 105, 'step_size': 14, 'gamma': 0.8436261357962749}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:49:22,963][0m Trial 34 finished with value: 0.12200497556477785 and parameters: {'observation_period_num': 5, 'train_rates': 0.9024150905242451, 'learning_rate': 0.0003052390768107563, 'batch_size': 80, 'step_size': 9, 'gamma': 0.7651812339554609}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:49:57,344][0m Trial 35 finished with value: 0.07901890645567462 and parameters: {'observation_period_num': 20, 'train_rates': 0.7156636501796038, 'learning_rate': 0.0009750031980788124, 'batch_size': 141, 'step_size': 12, 'gamma': 0.8300077474274863}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:52:36,670][0m Trial 36 finished with value: 0.1030160210574006 and parameters: {'observation_period_num': 58, 'train_rates': 0.7896301295124716, 'learning_rate': 0.00017057917664838648, 'batch_size': 31, 'step_size': 6, 'gamma': 0.8645275861872657}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:53:55,305][0m Trial 37 finished with value: 0.07815471987126904 and parameters: {'observation_period_num': 18, 'train_rates': 0.7000473574474926, 'learning_rate': 0.00047608461718792717, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8866292789178654}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:54:24,641][0m Trial 38 finished with value: 0.262660668965913 and parameters: {'observation_period_num': 170, 'train_rates': 0.7621683722168174, 'learning_rate': 1.9413051932076283e-05, 'batch_size': 168, 'step_size': 13, 'gamma': 0.8018914451006484}. Best is trial 11 with value: 0.06890926739232814.[0m
[32m[I 2025-01-31 07:55:15,106][0m Trial 39 finished with value: 0.04858642816543579 and parameters: {'observation_period_num': 43, 'train_rates': 0.9813194081568906, 'learning_rate': 0.0002871270328349659, 'batch_size': 122, 'step_size': 5, 'gamma': 0.9160255811770451}. Best is trial 39 with value: 0.04858642816543579.[0m
[32m[I 2025-01-31 07:55:43,511][0m Trial 40 finished with value: 0.23218080525054682 and parameters: {'observation_period_num': 45, 'train_rates': 0.921328880141597, 'learning_rate': 3.009005228743623e-05, 'batch_size': 223, 'step_size': 5, 'gamma': 0.9202374360604115}. Best is trial 39 with value: 0.04858642816543579.[0m
[32m[I 2025-01-31 07:56:34,077][0m Trial 41 finished with value: 0.3194369077682495 and parameters: {'observation_period_num': 28, 'train_rates': 0.9726100380369506, 'learning_rate': 0.0002780338810916149, 'batch_size': 122, 'step_size': 4, 'gamma': 0.9113428442096371}. Best is trial 39 with value: 0.04858642816543579.[0m
[32m[I 2025-01-31 07:57:28,371][0m Trial 42 finished with value: 0.09046624737325219 and parameters: {'observation_period_num': 12, 'train_rates': 0.8107798820520556, 'learning_rate': 0.0004074132307111059, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9300762955169801}. Best is trial 39 with value: 0.04858642816543579.[0m
[32m[I 2025-01-31 07:58:12,706][0m Trial 43 finished with value: 0.04801463335752487 and parameters: {'observation_period_num': 36, 'train_rates': 0.9874969324050649, 'learning_rate': 0.00018337655424356704, 'batch_size': 140, 'step_size': 4, 'gamma': 0.8942606944583541}. Best is trial 43 with value: 0.04801463335752487.[0m
[32m[I 2025-01-31 07:58:57,264][0m Trial 44 finished with value: 0.06005385145545006 and parameters: {'observation_period_num': 37, 'train_rates': 0.97831268736421, 'learning_rate': 9.941917988271235e-05, 'batch_size': 142, 'step_size': 4, 'gamma': 0.8917069343197961}. Best is trial 43 with value: 0.04801463335752487.[0m
[32m[I 2025-01-31 07:59:33,545][0m Trial 45 finished with value: 0.08813704550266266 and parameters: {'observation_period_num': 68, 'train_rates': 0.9863743494878155, 'learning_rate': 9.871381046264555e-05, 'batch_size': 169, 'step_size': 4, 'gamma': 0.8940676347702489}. Best is trial 43 with value: 0.04801463335752487.[0m
Early stopping at epoch 96
[32m[I 2025-01-31 08:00:14,651][0m Trial 46 finished with value: 0.3787812361040631 and parameters: {'observation_period_num': 81, 'train_rates': 0.9486207886867286, 'learning_rate': 0.00016989355765947582, 'batch_size': 138, 'step_size': 1, 'gamma': 0.8819933098014918}. Best is trial 43 with value: 0.04801463335752487.[0m
[32m[I 2025-01-31 08:00:55,345][0m Trial 47 finished with value: 0.25145071744918823 and parameters: {'observation_period_num': 38, 'train_rates': 0.9594960890902485, 'learning_rate': 7.16198955453771e-05, 'batch_size': 148, 'step_size': 5, 'gamma': 0.9717617280263533}. Best is trial 43 with value: 0.04801463335752487.[0m
[32m[I 2025-01-31 08:01:29,950][0m Trial 48 finished with value: 0.25463892254564496 and parameters: {'observation_period_num': 56, 'train_rates': 0.9224031060943333, 'learning_rate': 3.538895431051761e-05, 'batch_size': 170, 'step_size': 3, 'gamma': 0.9060584293813014}. Best is trial 43 with value: 0.04801463335752487.[0m
[32m[I 2025-01-31 08:02:08,484][0m Trial 49 finished with value: 0.11151058226823807 and parameters: {'observation_period_num': 113, 'train_rates': 0.9879932818554751, 'learning_rate': 0.0001523803678560622, 'batch_size': 160, 'step_size': 3, 'gamma': 0.9323368872111163}. Best is trial 43 with value: 0.04801463335752487.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-31 08:02:08,496][0m A new study created in memory with name: no-name-8ccaf36f-6bd9-44fc-9229-cce918d07539[0m
[32m[I 2025-01-31 08:02:39,815][0m Trial 0 finished with value: 0.09951462420280528 and parameters: {'observation_period_num': 6, 'train_rates': 0.8466807790849076, 'learning_rate': 0.0007618564300516848, 'batch_size': 185, 'step_size': 13, 'gamma': 0.8716311377242694}. Best is trial 0 with value: 0.09951462420280528.[0m
[32m[I 2025-01-31 08:03:42,199][0m Trial 1 finished with value: 0.1238794035811334 and parameters: {'observation_period_num': 53, 'train_rates': 0.8451268519543988, 'learning_rate': 0.0005153125197405737, 'batch_size': 87, 'step_size': 7, 'gamma': 0.9218596416635395}. Best is trial 0 with value: 0.09951462420280528.[0m
[32m[I 2025-01-31 08:06:03,478][0m Trial 2 finished with value: 0.27478942970064146 and parameters: {'observation_period_num': 137, 'train_rates': 0.9526475954775981, 'learning_rate': 0.0008905269150406564, 'batch_size': 39, 'step_size': 1, 'gamma': 0.8896405587859804}. Best is trial 0 with value: 0.09951462420280528.[0m
[32m[I 2025-01-31 08:07:23,928][0m Trial 3 finished with value: 0.11508383662993885 and parameters: {'observation_period_num': 8, 'train_rates': 0.788731023272367, 'learning_rate': 9.133729300634202e-06, 'batch_size': 63, 'step_size': 9, 'gamma': 0.9734788014516291}. Best is trial 0 with value: 0.09951462420280528.[0m
[32m[I 2025-01-31 08:08:22,082][0m Trial 4 finished with value: 0.14016361005395372 and parameters: {'observation_period_num': 120, 'train_rates': 0.84920444914076, 'learning_rate': 0.00010257073145485021, 'batch_size': 91, 'step_size': 9, 'gamma': 0.8792934181641523}. Best is trial 0 with value: 0.09951462420280528.[0m
[32m[I 2025-01-31 08:09:44,143][0m Trial 5 finished with value: 0.06782565654275265 and parameters: {'observation_period_num': 11, 'train_rates': 0.6165709450076631, 'learning_rate': 0.00019738613798179718, 'batch_size': 52, 'step_size': 6, 'gamma': 0.8986414024476028}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:10:14,368][0m Trial 6 finished with value: 0.23299826940229115 and parameters: {'observation_period_num': 10, 'train_rates': 0.9178270344869128, 'learning_rate': 1.5489922448068736e-05, 'batch_size': 206, 'step_size': 5, 'gamma': 0.9309916041423036}. Best is trial 5 with value: 0.06782565654275265.[0m
Early stopping at epoch 50
[32m[I 2025-01-31 08:10:31,609][0m Trial 7 finished with value: 0.19373706220525555 and parameters: {'observation_period_num': 5, 'train_rates': 0.8850077164385379, 'learning_rate': 0.00024529206275996653, 'batch_size': 180, 'step_size': 1, 'gamma': 0.7681051213715134}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:13:59,010][0m Trial 8 finished with value: 0.1279209178244833 and parameters: {'observation_period_num': 91, 'train_rates': 0.6272675521813118, 'learning_rate': 7.636604994317471e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.9194383115747744}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:14:46,621][0m Trial 9 finished with value: 0.1346216815568152 and parameters: {'observation_period_num': 119, 'train_rates': 0.7035894090713678, 'learning_rate': 0.00011829656546117563, 'batch_size': 97, 'step_size': 8, 'gamma': 0.7588716024737112}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:15:05,021][0m Trial 10 finished with value: 0.7926042048075727 and parameters: {'observation_period_num': 228, 'train_rates': 0.6152963804742779, 'learning_rate': 1.3782305839735478e-06, 'batch_size': 253, 'step_size': 14, 'gamma': 0.8237630047778332}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:15:39,220][0m Trial 11 finished with value: 0.09895366125179632 and parameters: {'observation_period_num': 54, 'train_rates': 0.7714852095581731, 'learning_rate': 0.0008952836862229769, 'batch_size': 155, 'step_size': 14, 'gamma': 0.8333998393226032}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:16:13,766][0m Trial 12 finished with value: 0.10718206083912507 and parameters: {'observation_period_num': 61, 'train_rates': 0.7160969393694449, 'learning_rate': 0.00026216326151508106, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8265495056115084}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:16:50,951][0m Trial 13 finished with value: 0.20850894707086623 and parameters: {'observation_period_num': 54, 'train_rates': 0.704430467151939, 'learning_rate': 3.6280288372625954e-05, 'batch_size': 128, 'step_size': 4, 'gamma': 0.8196432940182757}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:17:27,074][0m Trial 14 finished with value: 0.17924045675896405 and parameters: {'observation_period_num': 180, 'train_rates': 0.7551060235224252, 'learning_rate': 0.00028240514399673766, 'batch_size': 134, 'step_size': 15, 'gamma': 0.8492354185733171}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:17:57,855][0m Trial 15 finished with value: 0.10593435403043434 and parameters: {'observation_period_num': 77, 'train_rates': 0.6755431882354428, 'learning_rate': 0.00040806039776280345, 'batch_size': 156, 'step_size': 11, 'gamma': 0.79761861702679}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:18:19,188][0m Trial 16 finished with value: 0.14415386256982954 and parameters: {'observation_period_num': 36, 'train_rates': 0.7619682907353661, 'learning_rate': 4.026068777248716e-05, 'batch_size': 247, 'step_size': 3, 'gamma': 0.9662536466098879}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:19:30,517][0m Trial 17 finished with value: 0.5368808681381745 and parameters: {'observation_period_num': 155, 'train_rates': 0.6538554747769922, 'learning_rate': 2.567710550399021e-06, 'batch_size': 60, 'step_size': 10, 'gamma': 0.8992746446751685}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:19:55,889][0m Trial 18 finished with value: 0.1744721498693863 and parameters: {'observation_period_num': 90, 'train_rates': 0.8088508692389895, 'learning_rate': 0.00014458042718990671, 'batch_size': 218, 'step_size': 3, 'gamma': 0.8430211065522548}. Best is trial 5 with value: 0.06782565654275265.[0m
[32m[I 2025-01-31 08:20:48,188][0m Trial 19 finished with value: 0.05538659542798996 and parameters: {'observation_period_num': 38, 'train_rates': 0.9754246720097935, 'learning_rate': 0.0009311312661619799, 'batch_size': 114, 'step_size': 7, 'gamma': 0.7840590599220012}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:21:42,758][0m Trial 20 finished with value: 0.07672866433858871 and parameters: {'observation_period_num': 36, 'train_rates': 0.9798972445497379, 'learning_rate': 6.22283241054172e-05, 'batch_size': 112, 'step_size': 7, 'gamma': 0.7902066239984706}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:22:36,998][0m Trial 21 finished with value: 0.5077512860298157 and parameters: {'observation_period_num': 34, 'train_rates': 0.9725643979383898, 'learning_rate': 1.3744153166256389e-05, 'batch_size': 113, 'step_size': 7, 'gamma': 0.7850147473032644}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:24:03,299][0m Trial 22 finished with value: 0.05919327214360237 and parameters: {'observation_period_num': 31, 'train_rates': 0.9851690037159333, 'learning_rate': 5.819519802541247e-05, 'batch_size': 69, 'step_size': 6, 'gamma': 0.7893681608271134}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:25:34,290][0m Trial 23 finished with value: 0.18494869888361012 and parameters: {'observation_period_num': 29, 'train_rates': 0.9332329665438094, 'learning_rate': 0.00018038506406579843, 'batch_size': 63, 'step_size': 5, 'gamma': 0.8073904263755297}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:27:47,500][0m Trial 24 finished with value: 0.15961902847449327 and parameters: {'observation_period_num': 78, 'train_rates': 0.8991025530512197, 'learning_rate': 0.0004937908954418742, 'batch_size': 41, 'step_size': 5, 'gamma': 0.7724383477506661}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:28:59,831][0m Trial 25 finished with value: 0.31134132184880847 and parameters: {'observation_period_num': 248, 'train_rates': 0.9478589949916905, 'learning_rate': 2.608854613202213e-05, 'batch_size': 75, 'step_size': 8, 'gamma': 0.9478458900826479}. Best is trial 19 with value: 0.05538659542798996.[0m
[32m[I 2025-01-31 08:35:03,150][0m Trial 26 finished with value: 0.03201435706936396 and parameters: {'observation_period_num': 31, 'train_rates': 0.9823823015798168, 'learning_rate': 7.005241597981609e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8525009481265459}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:40:58,243][0m Trial 27 finished with value: 0.3225816840475256 and parameters: {'observation_period_num': 106, 'train_rates': 0.9847393607180959, 'learning_rate': 3.427296995579845e-06, 'batch_size': 16, 'step_size': 3, 'gamma': 0.751343879778519}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:43:20,091][0m Trial 28 finished with value: 0.3092244084713594 and parameters: {'observation_period_num': 191, 'train_rates': 0.8848517066462095, 'learning_rate': 6.9028419364784185e-06, 'batch_size': 36, 'step_size': 9, 'gamma': 0.862567069450543}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:44:12,601][0m Trial 29 finished with value: 0.35388287901878357 and parameters: {'observation_period_num': 71, 'train_rates': 0.9646719583402774, 'learning_rate': 5.6650556541403235e-05, 'batch_size': 112, 'step_size': 6, 'gamma': 0.8097396256949656}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:45:22,012][0m Trial 30 finished with value: 0.21593567970813365 and parameters: {'observation_period_num': 26, 'train_rates': 0.9222605349795425, 'learning_rate': 2.6131601832977172e-05, 'batch_size': 83, 'step_size': 4, 'gamma': 0.8646654258805475}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:47:09,089][0m Trial 31 finished with value: 0.10900641399505945 and parameters: {'observation_period_num': 20, 'train_rates': 0.8599850410842502, 'learning_rate': 9.136220242441661e-05, 'batch_size': 50, 'step_size': 6, 'gamma': 0.9083886594975141}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:50:45,886][0m Trial 32 finished with value: 0.032033784314990044 and parameters: {'observation_period_num': 48, 'train_rates': 0.9891457241107255, 'learning_rate': 0.0004311362083342693, 'batch_size': 27, 'step_size': 7, 'gamma': 0.7778163744111748}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:53:54,543][0m Trial 33 finished with value: 0.2475791034827891 and parameters: {'observation_period_num': 49, 'train_rates': 0.9478338474311218, 'learning_rate': 0.0005615456143231131, 'batch_size': 30, 'step_size': 7, 'gamma': 0.7823179291719727}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:55:16,898][0m Trial 34 finished with value: 0.03722267225384712 and parameters: {'observation_period_num': 45, 'train_rates': 0.9888149812192909, 'learning_rate': 0.0009877378737380852, 'batch_size': 73, 'step_size': 8, 'gamma': 0.7996147239088285}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 08:58:45,682][0m Trial 35 finished with value: 0.2564154736539151 and parameters: {'observation_period_num': 65, 'train_rates': 0.9512596192049954, 'learning_rate': 0.0006561451694827407, 'batch_size': 27, 'step_size': 10, 'gamma': 0.8084173375669226}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:00:52,111][0m Trial 36 finished with value: 0.15706929144112516 and parameters: {'observation_period_num': 42, 'train_rates': 0.9117452487798878, 'learning_rate': 0.0009570838927293265, 'batch_size': 44, 'step_size': 8, 'gamma': 0.7679153442076079}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:01:50,572][0m Trial 37 finished with value: 0.23900537271032019 and parameters: {'observation_period_num': 89, 'train_rates': 0.9368489660418211, 'learning_rate': 0.00036085785465718375, 'batch_size': 97, 'step_size': 10, 'gamma': 0.8784872987461685}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:03:09,041][0m Trial 38 finished with value: 0.26170056506439493 and parameters: {'observation_period_num': 17, 'train_rates': 0.9634737570657165, 'learning_rate': 0.0006818257208564663, 'batch_size': 76, 'step_size': 9, 'gamma': 0.8527039834710597}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:04:41,316][0m Trial 39 finished with value: 0.11323725758212627 and parameters: {'observation_period_num': 47, 'train_rates': 0.826243148136836, 'learning_rate': 0.00034909244647980583, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7968227165389506}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:08:57,868][0m Trial 40 finished with value: 0.08263988246520361 and parameters: {'observation_period_num': 145, 'train_rates': 0.9896379322672635, 'learning_rate': 0.0009749435424449999, 'batch_size': 22, 'step_size': 8, 'gamma': 0.7759557193148512}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:10:26,025][0m Trial 41 finished with value: 0.039103660732507706 and parameters: {'observation_period_num': 22, 'train_rates': 0.9871626797353589, 'learning_rate': 0.000520371777062106, 'batch_size': 68, 'step_size': 5, 'gamma': 0.7606545411891914}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:11:34,429][0m Trial 42 finished with value: 0.25424977019429207 and parameters: {'observation_period_num': 19, 'train_rates': 0.9632857362863073, 'learning_rate': 0.0005051057306932325, 'batch_size': 87, 'step_size': 5, 'gamma': 0.7565582750138761}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:14:09,148][0m Trial 43 finished with value: 0.1661260523155187 and parameters: {'observation_period_num': 6, 'train_rates': 0.9303633441893566, 'learning_rate': 0.00019814468377743984, 'batch_size': 37, 'step_size': 7, 'gamma': 0.7676377806804835}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:15:07,870][0m Trial 44 finished with value: 0.0777529925107956 and parameters: {'observation_period_num': 61, 'train_rates': 0.9897806584140908, 'learning_rate': 0.0007539065088519136, 'batch_size': 104, 'step_size': 4, 'gamma': 0.7597731094480957}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:17:04,236][0m Trial 45 finished with value: 0.18222242222621096 and parameters: {'observation_period_num': 109, 'train_rates': 0.9055900816259369, 'learning_rate': 0.0004174112371708382, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8001780410680635}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:17:49,597][0m Trial 46 finished with value: 0.12291427189607104 and parameters: {'observation_period_num': 41, 'train_rates': 0.8700692958649884, 'learning_rate': 0.00013277225677582426, 'batch_size': 122, 'step_size': 9, 'gamma': 0.7786263062792739}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:19:09,996][0m Trial 47 finished with value: 0.2847256450490518 and parameters: {'observation_period_num': 56, 'train_rates': 0.9623000422865767, 'learning_rate': 0.0006587537554834815, 'batch_size': 73, 'step_size': 5, 'gamma': 0.835697383829064}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:19:46,041][0m Trial 48 finished with value: 0.21209919452667236 and parameters: {'observation_period_num': 16, 'train_rates': 0.9441412333964612, 'learning_rate': 0.0002804135394627499, 'batch_size': 169, 'step_size': 6, 'gamma': 0.8153008996759762}. Best is trial 26 with value: 0.03201435706936396.[0m
[32m[I 2025-01-31 09:23:02,441][0m Trial 49 finished with value: 0.3984996042190454 and parameters: {'observation_period_num': 73, 'train_rates': 0.9730287927821286, 'learning_rate': 0.00033825724849936953, 'batch_size': 29, 'step_size': 8, 'gamma': 0.7586826376542112}. Best is trial 26 with value: 0.03201435706936396.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-31 09:23:02,451][0m A new study created in memory with name: no-name-8fe7fab8-8fd6-41f2-9fb8-d805f65344bd[0m
[32m[I 2025-01-31 09:23:31,486][0m Trial 0 finished with value: 0.3220124913228525 and parameters: {'observation_period_num': 96, 'train_rates': 0.9224234843691106, 'learning_rate': 2.3826926181738408e-05, 'batch_size': 204, 'step_size': 14, 'gamma': 0.765462180986901}. Best is trial 0 with value: 0.3220124913228525.[0m
[32m[I 2025-01-31 09:24:00,044][0m Trial 1 finished with value: 0.15766934066771365 and parameters: {'observation_period_num': 164, 'train_rates': 0.6878553503095214, 'learning_rate': 0.00017911614618236634, 'batch_size': 170, 'step_size': 12, 'gamma': 0.879455221468875}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:27:03,805][0m Trial 2 finished with value: 0.23754808359912463 and parameters: {'observation_period_num': 45, 'train_rates': 0.9497218850311966, 'learning_rate': 3.060775132466717e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.7516776926943407}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:29:35,353][0m Trial 3 finished with value: 0.3253261144514437 and parameters: {'observation_period_num': 96, 'train_rates': 0.9528579554856167, 'learning_rate': 0.00010235783565325048, 'batch_size': 37, 'step_size': 7, 'gamma': 0.9596989070231182}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:30:02,588][0m Trial 4 finished with value: 0.4044571586617573 and parameters: {'observation_period_num': 89, 'train_rates': 0.635781855410387, 'learning_rate': 4.915846652244347e-06, 'batch_size': 171, 'step_size': 10, 'gamma': 0.936682276403717}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:30:38,859][0m Trial 5 finished with value: 0.5745846858391395 and parameters: {'observation_period_num': 185, 'train_rates': 0.6242945205765039, 'learning_rate': 7.522508130949705e-06, 'batch_size': 120, 'step_size': 6, 'gamma': 0.8309271676177297}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:30:58,624][0m Trial 6 finished with value: 0.7424416707408044 and parameters: {'observation_period_num': 210, 'train_rates': 0.6609195180019372, 'learning_rate': 2.9744369330190996e-06, 'batch_size': 234, 'step_size': 14, 'gamma': 0.8658622314805565}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:31:31,067][0m Trial 7 finished with value: 0.43553245067596436 and parameters: {'observation_period_num': 189, 'train_rates': 0.9572552039330795, 'learning_rate': 0.00028752076422346667, 'batch_size': 184, 'step_size': 2, 'gamma': 0.8016507066646565}. Best is trial 1 with value: 0.15766934066771365.[0m
[32m[I 2025-01-31 09:32:03,016][0m Trial 8 finished with value: 0.14169737696647644 and parameters: {'observation_period_num': 192, 'train_rates': 0.9843853946605181, 'learning_rate': 0.00010406699785957652, 'batch_size': 186, 'step_size': 14, 'gamma': 0.9788695702283728}. Best is trial 8 with value: 0.14169737696647644.[0m
[32m[I 2025-01-31 09:32:42,283][0m Trial 9 finished with value: 0.35457807324314844 and parameters: {'observation_period_num': 28, 'train_rates': 0.8208666776983177, 'learning_rate': 3.0394323122938103e-06, 'batch_size': 138, 'step_size': 9, 'gamma': 0.8656246362939017}. Best is trial 8 with value: 0.14169737696647644.[0m
[32m[I 2025-01-31 09:33:04,853][0m Trial 10 finished with value: 0.18576536760097598 and parameters: {'observation_period_num': 250, 'train_rates': 0.8483240320965149, 'learning_rate': 0.000885434494446735, 'batch_size': 252, 'step_size': 1, 'gamma': 0.9792596734000016}. Best is trial 8 with value: 0.14169737696647644.[0m
[32m[I 2025-01-31 09:33:48,017][0m Trial 11 finished with value: 0.14341178020605674 and parameters: {'observation_period_num': 154, 'train_rates': 0.7352145194689217, 'learning_rate': 0.00012409211777155904, 'batch_size': 114, 'step_size': 11, 'gamma': 0.9157994408716104}. Best is trial 8 with value: 0.14169737696647644.[0m
[32m[I 2025-01-31 09:34:39,816][0m Trial 12 finished with value: 0.12431415192228049 and parameters: {'observation_period_num': 148, 'train_rates': 0.7320413507293501, 'learning_rate': 8.205002159922511e-05, 'batch_size': 90, 'step_size': 11, 'gamma': 0.9236623441104648}. Best is trial 12 with value: 0.12431415192228049.[0m
[32m[I 2025-01-31 09:35:39,706][0m Trial 13 finished with value: 0.1324429851770401 and parameters: {'observation_period_num': 133, 'train_rates': 0.7608186816251086, 'learning_rate': 6.088938560927674e-05, 'batch_size': 81, 'step_size': 12, 'gamma': 0.9884507266480553}. Best is trial 12 with value: 0.12431415192228049.[0m
[32m[I 2025-01-31 09:36:45,485][0m Trial 14 finished with value: 0.21840269579026933 and parameters: {'observation_period_num': 133, 'train_rates': 0.7643185533799839, 'learning_rate': 1.5026510217099876e-05, 'batch_size': 74, 'step_size': 5, 'gamma': 0.915633500925209}. Best is trial 12 with value: 0.12431415192228049.[0m
[32m[I 2025-01-31 09:37:48,181][0m Trial 15 finished with value: 0.11981123521119585 and parameters: {'observation_period_num': 118, 'train_rates': 0.7256527736967024, 'learning_rate': 5.537527846072881e-05, 'batch_size': 76, 'step_size': 12, 'gamma': 0.938407955558366}. Best is trial 15 with value: 0.11981123521119585.[0m
[32m[I 2025-01-31 09:38:47,435][0m Trial 16 finished with value: 0.10294835646342337 and parameters: {'observation_period_num': 70, 'train_rates': 0.7038324800640571, 'learning_rate': 0.0003767619535525387, 'batch_size': 80, 'step_size': 8, 'gamma': 0.9031256244865639}. Best is trial 16 with value: 0.10294835646342337.[0m
[32m[I 2025-01-31 09:40:00,650][0m Trial 17 finished with value: 0.09547541353043977 and parameters: {'observation_period_num': 56, 'train_rates': 0.7039320117741726, 'learning_rate': 0.000543731047298613, 'batch_size': 62, 'step_size': 4, 'gamma': 0.8959515408949195}. Best is trial 17 with value: 0.09547541353043977.[0m
[32m[I 2025-01-31 09:44:07,348][0m Trial 18 finished with value: 0.10695549279031619 and parameters: {'observation_period_num': 56, 'train_rates': 0.6815945138232169, 'learning_rate': 0.0008379428847764308, 'batch_size': 18, 'step_size': 4, 'gamma': 0.8856657254599621}. Best is trial 17 with value: 0.09547541353043977.[0m
[32m[I 2025-01-31 09:46:04,892][0m Trial 19 finished with value: 0.12383428666936724 and parameters: {'observation_period_num': 66, 'train_rates': 0.8686248114329644, 'learning_rate': 0.0004007379254315055, 'batch_size': 45, 'step_size': 3, 'gamma': 0.8401436371821474}. Best is trial 17 with value: 0.09547541353043977.[0m
[32m[I 2025-01-31 09:47:39,853][0m Trial 20 finished with value: 0.08030581194907427 and parameters: {'observation_period_num': 8, 'train_rates': 0.8057941673230121, 'learning_rate': 0.0004333312836192513, 'batch_size': 55, 'step_size': 8, 'gamma': 0.8887222725759408}. Best is trial 20 with value: 0.08030581194907427.[0m
[32m[I 2025-01-31 09:49:09,066][0m Trial 21 finished with value: 0.07327638767222784 and parameters: {'observation_period_num': 9, 'train_rates': 0.7858988615820706, 'learning_rate': 0.0003768744585484831, 'batch_size': 57, 'step_size': 8, 'gamma': 0.89263860842219}. Best is trial 21 with value: 0.07327638767222784.[0m
[32m[I 2025-01-31 09:50:39,237][0m Trial 22 finished with value: 0.07848783767368604 and parameters: {'observation_period_num': 8, 'train_rates': 0.8119908632100079, 'learning_rate': 0.0005418709791676661, 'batch_size': 58, 'step_size': 7, 'gamma': 0.843047218093314}. Best is trial 21 with value: 0.07327638767222784.[0m
[32m[I 2025-01-31 09:52:22,800][0m Trial 23 finished with value: 0.07454997401892845 and parameters: {'observation_period_num': 7, 'train_rates': 0.7938819114998192, 'learning_rate': 0.00019466601871601926, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8481719028425265}. Best is trial 21 with value: 0.07327638767222784.[0m
[32m[I 2025-01-31 09:53:16,167][0m Trial 24 finished with value: 0.653899054504911 and parameters: {'observation_period_num': 7, 'train_rates': 0.8735917315189133, 'learning_rate': 1.0666073996496376e-06, 'batch_size': 103, 'step_size': 6, 'gamma': 0.8416342581929436}. Best is trial 21 with value: 0.07327638767222784.[0m
[32m[I 2025-01-31 09:57:09,246][0m Trial 25 finished with value: 0.09354414650475357 and parameters: {'observation_period_num': 28, 'train_rates': 0.7710080476094121, 'learning_rate': 0.0002687822745520038, 'batch_size': 21, 'step_size': 7, 'gamma': 0.8112195889340328}. Best is trial 21 with value: 0.07327638767222784.[0m
[32m[I 2025-01-31 09:57:46,735][0m Trial 26 finished with value: 0.10070411409325307 and parameters: {'observation_period_num': 32, 'train_rates': 0.8275132159513232, 'learning_rate': 0.00021214875455429391, 'batch_size': 145, 'step_size': 6, 'gamma': 0.8532678440988359}. Best is trial 21 with value: 0.07327638767222784.[0m
[32m[I 2025-01-31 09:59:15,480][0m Trial 27 finished with value: 0.06953529339361439 and parameters: {'observation_period_num': 6, 'train_rates': 0.7885740545813038, 'learning_rate': 0.0006339623822353624, 'batch_size': 57, 'step_size': 9, 'gamma': 0.8112806178754011}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:00:04,700][0m Trial 28 finished with value: 0.08691325566228132 and parameters: {'observation_period_num': 25, 'train_rates': 0.7802632241759594, 'learning_rate': 0.0009706310376574668, 'batch_size': 104, 'step_size': 9, 'gamma': 0.7883626168960455}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:01:50,772][0m Trial 29 finished with value: 0.1564073826802868 and parameters: {'observation_period_num': 85, 'train_rates': 0.9011116640387377, 'learning_rate': 0.00017612962332221382, 'batch_size': 51, 'step_size': 9, 'gamma': 0.7721440822322013}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:04:20,287][0m Trial 30 finished with value: 0.11855660400099376 and parameters: {'observation_period_num': 46, 'train_rates': 0.848738497905244, 'learning_rate': 2.023392343777763e-05, 'batch_size': 35, 'step_size': 10, 'gamma': 0.8270109679581816}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:05:46,749][0m Trial 31 finished with value: 0.07340472025209058 and parameters: {'observation_period_num': 7, 'train_rates': 0.7966761694136029, 'learning_rate': 0.0006150402360349121, 'batch_size': 59, 'step_size': 7, 'gamma': 0.8217991931596718}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:07:01,101][0m Trial 32 finished with value: 0.08240405001087588 and parameters: {'observation_period_num': 19, 'train_rates': 0.7842903972006371, 'learning_rate': 0.000672775739669252, 'batch_size': 68, 'step_size': 8, 'gamma': 0.8147291081774046}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:07:55,246][0m Trial 33 finished with value: 0.10125119315818719 and parameters: {'observation_period_num': 39, 'train_rates': 0.7467779606131049, 'learning_rate': 0.00016130045889302936, 'batch_size': 92, 'step_size': 5, 'gamma': 0.7941656886969242}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:10:19,299][0m Trial 34 finished with value: 0.07427242997013303 and parameters: {'observation_period_num': 5, 'train_rates': 0.7944951146492617, 'learning_rate': 0.00029799099177490795, 'batch_size': 35, 'step_size': 8, 'gamma': 0.7815851953880555}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:13:05,692][0m Trial 35 finished with value: 0.11428050246548553 and parameters: {'observation_period_num': 44, 'train_rates': 0.8353361839854868, 'learning_rate': 0.00035046043572971346, 'batch_size': 31, 'step_size': 10, 'gamma': 0.7717252596473059}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:18:07,961][0m Trial 36 finished with value: 0.1331221892616745 and parameters: {'observation_period_num': 22, 'train_rates': 0.8952417077754034, 'learning_rate': 0.0005006535555237241, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7573769738496727}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:20:19,181][0m Trial 37 finished with value: 0.11574704851150924 and parameters: {'observation_period_num': 75, 'train_rates': 0.7988890460494897, 'learning_rate': 4.032024795991204e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.7828261802671305}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:21:02,472][0m Trial 38 finished with value: 0.1458884893128505 and parameters: {'observation_period_num': 114, 'train_rates': 0.8719312457018549, 'learning_rate': 0.0002825845356202605, 'batch_size': 126, 'step_size': 8, 'gamma': 0.8134828173504448}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:21:36,652][0m Trial 39 finished with value: 0.09468570733750226 and parameters: {'observation_period_num': 55, 'train_rates': 0.759411679410153, 'learning_rate': 0.0007077276864037806, 'batch_size': 157, 'step_size': 6, 'gamma': 0.8295270308345781}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:22:35,048][0m Trial 40 finished with value: 0.0958696376867353 and parameters: {'observation_period_num': 18, 'train_rates': 0.8409443392607526, 'learning_rate': 0.0006255429044418109, 'batch_size': 92, 'step_size': 13, 'gamma': 0.872479477119255}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:24:32,601][0m Trial 41 finished with value: 0.07967246165300937 and parameters: {'observation_period_num': 5, 'train_rates': 0.80870058429398, 'learning_rate': 0.00023026756230076363, 'batch_size': 44, 'step_size': 7, 'gamma': 0.8583384828422865}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:25:55,520][0m Trial 42 finished with value: 0.07880264407356298 and parameters: {'observation_period_num': 16, 'train_rates': 0.7846105361306375, 'learning_rate': 0.00013525279179001583, 'batch_size': 61, 'step_size': 8, 'gamma': 0.8193274140818241}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:28:36,824][0m Trial 43 finished with value: 0.08760367611743855 and parameters: {'observation_period_num': 33, 'train_rates': 0.7932666631741312, 'learning_rate': 0.0003497146314189314, 'batch_size': 31, 'step_size': 5, 'gamma': 0.7988464210175789}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:30:28,173][0m Trial 44 finished with value: 0.09868683399500025 and parameters: {'observation_period_num': 40, 'train_rates': 0.8166883366452762, 'learning_rate': 0.00018642161385112895, 'batch_size': 46, 'step_size': 7, 'gamma': 0.7805760798819065}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:31:39,231][0m Trial 45 finished with value: 0.07931018700346469 and parameters: {'observation_period_num': 16, 'train_rates': 0.7419617142724145, 'learning_rate': 8.142009810625706e-05, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8466540977388076}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:32:02,762][0m Trial 46 finished with value: 0.10005940666293867 and parameters: {'observation_period_num': 49, 'train_rates': 0.7152696504800686, 'learning_rate': 0.0002703116868597707, 'batch_size': 208, 'step_size': 9, 'gamma': 0.8027345714188613}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:34:48,044][0m Trial 47 finished with value: 0.22832214039501952 and parameters: {'observation_period_num': 245, 'train_rates': 0.7522886084043421, 'learning_rate': 0.0009209144325783074, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8713495024800901}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:36:25,539][0m Trial 48 finished with value: 0.08532316359985795 and parameters: {'observation_period_num': 32, 'train_rates': 0.773700466168749, 'learning_rate': 0.00047795894895365427, 'batch_size': 51, 'step_size': 6, 'gamma': 0.762042047272986}. Best is trial 27 with value: 0.06953529339361439.[0m
[32m[I 2025-01-31 10:37:25,669][0m Trial 49 finished with value: 0.1722859813994968 and parameters: {'observation_period_num': 228, 'train_rates': 0.8576640473607404, 'learning_rate': 0.00010178402784127506, 'batch_size': 84, 'step_size': 7, 'gamma': 0.8273857224998004}. Best is trial 27 with value: 0.06953529339361439.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-31 10:37:25,679][0m A new study created in memory with name: no-name-e6002e7d-2994-4969-9091-af59eb818e5f[0m
[32m[I 2025-01-31 10:40:19,447][0m Trial 0 finished with value: 0.10764904837047712 and parameters: {'observation_period_num': 12, 'train_rates': 0.834181518628685, 'learning_rate': 9.216055071324863e-06, 'batch_size': 30, 'step_size': 14, 'gamma': 0.978710942921875}. Best is trial 0 with value: 0.10764904837047712.[0m
[32m[I 2025-01-31 10:41:53,131][0m Trial 1 finished with value: 0.21824348966280618 and parameters: {'observation_period_num': 83, 'train_rates': 0.9310015599852182, 'learning_rate': 0.0003107308986797835, 'batch_size': 60, 'step_size': 15, 'gamma': 0.7585797448279475}. Best is trial 0 with value: 0.10764904837047712.[0m
[32m[I 2025-01-31 10:42:17,416][0m Trial 2 finished with value: 0.2048629402708845 and parameters: {'observation_period_num': 208, 'train_rates': 0.6786699449049138, 'learning_rate': 0.0002572973467707065, 'batch_size': 204, 'step_size': 12, 'gamma': 0.9804851091488175}. Best is trial 0 with value: 0.10764904837047712.[0m
[32m[I 2025-01-31 10:42:51,841][0m Trial 3 finished with value: 0.6227314028251975 and parameters: {'observation_period_num': 92, 'train_rates': 0.730352586071728, 'learning_rate': 2.3068948909483785e-06, 'batch_size': 149, 'step_size': 7, 'gamma': 0.8015290934365863}. Best is trial 0 with value: 0.10764904837047712.[0m
[32m[I 2025-01-31 10:43:26,355][0m Trial 4 finished with value: 0.3371743857860565 and parameters: {'observation_period_num': 111, 'train_rates': 0.9886633197362669, 'learning_rate': 1.3090765761037813e-05, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8283888185544692}. Best is trial 0 with value: 0.10764904837047712.[0m
[32m[I 2025-01-31 10:43:59,800][0m Trial 5 finished with value: 0.10085576768349676 and parameters: {'observation_period_num': 57, 'train_rates': 0.6445308465188055, 'learning_rate': 0.00035279129043747293, 'batch_size': 138, 'step_size': 3, 'gamma': 0.9101943743952936}. Best is trial 5 with value: 0.10085576768349676.[0m
[32m[I 2025-01-31 10:44:25,265][0m Trial 6 finished with value: 0.839604897610332 and parameters: {'observation_period_num': 157, 'train_rates': 0.6089533369926431, 'learning_rate': 1.3480320608379317e-06, 'batch_size': 175, 'step_size': 13, 'gamma': 0.8791847530976057}. Best is trial 5 with value: 0.10085576768349676.[0m
[32m[I 2025-01-31 10:44:51,541][0m Trial 7 finished with value: 0.13106461355607665 and parameters: {'observation_period_num': 35, 'train_rates': 0.8857765108053952, 'learning_rate': 0.0004254476878840618, 'batch_size': 230, 'step_size': 2, 'gamma': 0.9300299931471868}. Best is trial 5 with value: 0.10085576768349676.[0m
[32m[I 2025-01-31 10:46:32,709][0m Trial 8 finished with value: 0.24568539202489234 and parameters: {'observation_period_num': 111, 'train_rates': 0.6962316541375153, 'learning_rate': 3.7420457597867026e-06, 'batch_size': 44, 'step_size': 15, 'gamma': 0.9436284961055024}. Best is trial 5 with value: 0.10085576768349676.[0m
[32m[I 2025-01-31 10:47:13,451][0m Trial 9 finished with value: 0.7355113074749331 and parameters: {'observation_period_num': 157, 'train_rates': 0.9435271659392062, 'learning_rate': 9.410203426943015e-06, 'batch_size': 140, 'step_size': 4, 'gamma': 0.8350501202646067}. Best is trial 5 with value: 0.10085576768349676.[0m
[32m[I 2025-01-31 10:47:57,199][0m Trial 10 finished with value: 0.33054052789138394 and parameters: {'observation_period_num': 55, 'train_rates': 0.6048807466691225, 'learning_rate': 7.465000645058629e-05, 'batch_size': 98, 'step_size': 1, 'gamma': 0.8945909120531076}. Best is trial 5 with value: 0.10085576768349676.[0m
[32m[I 2025-01-31 10:48:53,790][0m Trial 11 finished with value: 0.09977955566775391 and parameters: {'observation_period_num': 12, 'train_rates': 0.8310841835190768, 'learning_rate': 6.0963837942803454e-05, 'batch_size': 96, 'step_size': 10, 'gamma': 0.9880620727129819}. Best is trial 11 with value: 0.09977955566775391.[0m
[32m[I 2025-01-31 10:49:49,177][0m Trial 12 finished with value: 0.08576064732753569 and parameters: {'observation_period_num': 6, 'train_rates': 0.7801262402910718, 'learning_rate': 6.362800635692778e-05, 'batch_size': 93, 'step_size': 10, 'gamma': 0.9260504319574586}. Best is trial 12 with value: 0.08576064732753569.[0m
[32m[I 2025-01-31 10:50:44,640][0m Trial 13 finished with value: 0.08423028769729389 and parameters: {'observation_period_num': 7, 'train_rates': 0.7723248375354298, 'learning_rate': 8.225966561082245e-05, 'batch_size': 94, 'step_size': 10, 'gamma': 0.9494428456944721}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:51:33,936][0m Trial 14 finished with value: 0.18841195390552687 and parameters: {'observation_period_num': 232, 'train_rates': 0.7571079903704176, 'learning_rate': 0.00010122592283588125, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9442316625777287}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:52:40,344][0m Trial 15 finished with value: 0.10033296114381622 and parameters: {'observation_period_num': 8, 'train_rates': 0.7748783776508794, 'learning_rate': 2.7728549338063038e-05, 'batch_size': 78, 'step_size': 10, 'gamma': 0.9182744467293866}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:53:30,403][0m Trial 16 finished with value: 0.12256990017639714 and parameters: {'observation_period_num': 50, 'train_rates': 0.8267973647982226, 'learning_rate': 3.326988881543204e-05, 'batch_size': 109, 'step_size': 11, 'gamma': 0.8530594086952352}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:54:44,150][0m Trial 17 finished with value: 0.147509245243555 and parameters: {'observation_period_num': 151, 'train_rates': 0.7283669738390925, 'learning_rate': 0.0009177838503207311, 'batch_size': 63, 'step_size': 8, 'gamma': 0.9576288646235788}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:55:28,492][0m Trial 18 finished with value: 0.09794578265221969 and parameters: {'observation_period_num': 33, 'train_rates': 0.805330934221618, 'learning_rate': 0.00013008482382081995, 'batch_size': 120, 'step_size': 5, 'gamma': 0.8924699364344874}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:56:04,462][0m Trial 19 finished with value: 0.1502732318565456 and parameters: {'observation_period_num': 76, 'train_rates': 0.8746292542835122, 'learning_rate': 0.00014820097268576434, 'batch_size': 163, 'step_size': 8, 'gamma': 0.962835118862787}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 10:59:52,966][0m Trial 20 finished with value: 0.15374358672314978 and parameters: {'observation_period_num': 183, 'train_rates': 0.754113715218521, 'learning_rate': 3.8226591974625694e-05, 'batch_size': 20, 'step_size': 12, 'gamma': 0.9110300201338888}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 11:00:37,249][0m Trial 21 finished with value: 0.09593013311043763 and parameters: {'observation_period_num': 34, 'train_rates': 0.7996615999788151, 'learning_rate': 0.00014186441506197005, 'batch_size': 120, 'step_size': 5, 'gamma': 0.8827371770720462}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 11:01:21,646][0m Trial 22 finished with value: 0.136582524697671 and parameters: {'observation_period_num': 29, 'train_rates': 0.7916777475644922, 'learning_rate': 1.9539042947825045e-05, 'batch_size': 118, 'step_size': 7, 'gamma': 0.8739504735959696}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 11:02:31,911][0m Trial 23 finished with value: 0.12558446216849642 and parameters: {'observation_period_num': 27, 'train_rates': 0.8778063369780436, 'learning_rate': 5.8617207549256696e-05, 'batch_size': 78, 'step_size': 10, 'gamma': 0.9327440204502028}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 11:03:33,060][0m Trial 24 finished with value: 0.10225136447831241 and parameters: {'observation_period_num': 63, 'train_rates': 0.7101487890178173, 'learning_rate': 0.0002052714072590435, 'batch_size': 77, 'step_size': 5, 'gamma': 0.8965110903756666}. Best is trial 13 with value: 0.08423028769729389.[0m
[32m[I 2025-01-31 11:04:17,571][0m Trial 25 finished with value: 0.08344084192923017 and parameters: {'observation_period_num': 6, 'train_rates': 0.808144668346283, 'learning_rate': 0.0005135303882600146, 'batch_size': 124, 'step_size': 9, 'gamma': 0.8631055743495988}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:06:05,430][0m Trial 26 finished with value: 0.10108966569578837 and parameters: {'observation_period_num': 6, 'train_rates': 0.8503202325533203, 'learning_rate': 0.0006233181178904853, 'batch_size': 50, 'step_size': 9, 'gamma': 0.856744362038762}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:06:26,820][0m Trial 27 finished with value: 0.1081254056537117 and parameters: {'observation_period_num': 50, 'train_rates': 0.7604678904107864, 'learning_rate': 0.0008506189693464732, 'batch_size': 254, 'step_size': 11, 'gamma': 0.9614225492018098}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:07:02,544][0m Trial 28 finished with value: 0.17867099097152064 and parameters: {'observation_period_num': 251, 'train_rates': 0.667654455634356, 'learning_rate': 0.0005121167601582289, 'batch_size': 128, 'step_size': 9, 'gamma': 0.8098140399983212}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:07:40,139][0m Trial 29 finished with value: 0.21438370381883526 and parameters: {'observation_period_num': 21, 'train_rates': 0.8515067184940849, 'learning_rate': 6.46304330822979e-06, 'batch_size': 155, 'step_size': 13, 'gamma': 0.8616862744462113}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:10:05,313][0m Trial 30 finished with value: 0.08881657501292783 and parameters: {'observation_period_num': 5, 'train_rates': 0.737687433703677, 'learning_rate': 1.7866350628842056e-05, 'batch_size': 33, 'step_size': 11, 'gamma': 0.8389695140553173}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:12:45,112][0m Trial 31 finished with value: 0.09478175560153036 and parameters: {'observation_period_num': 6, 'train_rates': 0.7843259251158964, 'learning_rate': 1.7811690684215814e-05, 'batch_size': 31, 'step_size': 11, 'gamma': 0.839679171016761}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:13:56,491][0m Trial 32 finished with value: 0.09706501279245405 and parameters: {'observation_period_num': 22, 'train_rates': 0.7333405121166858, 'learning_rate': 4.440877995103961e-05, 'batch_size': 69, 'step_size': 13, 'gamma': 0.7778903787035274}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:16:21,621][0m Trial 33 finished with value: 0.10489798632452105 and parameters: {'observation_period_num': 40, 'train_rates': 0.8123447452677468, 'learning_rate': 0.00021813849941806294, 'batch_size': 35, 'step_size': 12, 'gamma': 0.8145553153921246}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:21:12,451][0m Trial 34 finished with value: 0.17041813728831404 and parameters: {'observation_period_num': 68, 'train_rates': 0.7376954129844915, 'learning_rate': 5.644297995848247e-06, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7938043296369038}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:21:58,163][0m Trial 35 finished with value: 0.18759958454822284 and parameters: {'observation_period_num': 97, 'train_rates': 0.7033619878276516, 'learning_rate': 1.9609934134827414e-05, 'batch_size': 104, 'step_size': 10, 'gamma': 0.8442704780903463}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:22:51,353][0m Trial 36 finished with value: 0.1238382183439991 and parameters: {'observation_period_num': 16, 'train_rates': 0.6716552257585191, 'learning_rate': 1.1185810683643529e-05, 'batch_size': 89, 'step_size': 14, 'gamma': 0.8252405700958321}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:24:21,151][0m Trial 37 finished with value: 0.10269388653237574 and parameters: {'observation_period_num': 47, 'train_rates': 0.7702570697791042, 'learning_rate': 9.038886991103728e-05, 'batch_size': 55, 'step_size': 9, 'gamma': 0.751208047402087}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:24:53,433][0m Trial 38 finished with value: 0.20757457178453326 and parameters: {'observation_period_num': 81, 'train_rates': 0.9012389855413317, 'learning_rate': 2.4169974711287162e-05, 'batch_size': 182, 'step_size': 11, 'gamma': 0.9767674060270473}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:25:24,934][0m Trial 39 finished with value: 0.12314759864719636 and parameters: {'observation_period_num': 139, 'train_rates': 0.6514718930681385, 'learning_rate': 0.00029252747727505915, 'batch_size': 144, 'step_size': 7, 'gamma': 0.864942286923211}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:26:09,488][0m Trial 40 finished with value: 0.11792650988691278 and parameters: {'observation_period_num': 18, 'train_rates': 0.8490526620413815, 'learning_rate': 5.2038022655516886e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.9256244054061603}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:28:21,401][0m Trial 41 finished with value: 0.09650513614628133 and parameters: {'observation_period_num': 5, 'train_rates': 0.7826983243849311, 'learning_rate': 1.678865270399626e-05, 'batch_size': 38, 'step_size': 11, 'gamma': 0.8415922335508678}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:30:56,499][0m Trial 42 finished with value: 0.09815774074896419 and parameters: {'observation_period_num': 5, 'train_rates': 0.7443806755844681, 'learning_rate': 1.4438709994600058e-05, 'batch_size': 31, 'step_size': 12, 'gamma': 0.844743003982598}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:31:23,712][0m Trial 43 finished with value: 0.5611392113582744 and parameters: {'observation_period_num': 41, 'train_rates': 0.8121267085347524, 'learning_rate': 2.660344395494066e-06, 'batch_size': 206, 'step_size': 14, 'gamma': 0.8252713979004372}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:33:08,175][0m Trial 44 finished with value: 0.1029483656010994 and parameters: {'observation_period_num': 21, 'train_rates': 0.7159523197577672, 'learning_rate': 2.8871840701963877e-05, 'batch_size': 45, 'step_size': 10, 'gamma': 0.7900113129286888}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:34:05,535][0m Trial 45 finished with value: 0.15698354915837104 and parameters: {'observation_period_num': 18, 'train_rates': 0.7829881617703993, 'learning_rate': 6.679112014559958e-06, 'batch_size': 89, 'step_size': 12, 'gamma': 0.8861199899762913}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:37:11,323][0m Trial 46 finished with value: 0.16774746833711968 and parameters: {'observation_period_num': 191, 'train_rates': 0.6914130070911837, 'learning_rate': 6.915219809317914e-05, 'batch_size': 23, 'step_size': 11, 'gamma': 0.904774437630642}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:38:30,803][0m Trial 47 finished with value: 0.1254652974285788 and parameters: {'observation_period_num': 98, 'train_rates': 0.8225332354644439, 'learning_rate': 4.123992468005053e-05, 'batch_size': 64, 'step_size': 9, 'gamma': 0.9469278351204031}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:39:18,169][0m Trial 48 finished with value: 0.09888417834891434 and parameters: {'observation_period_num': 39, 'train_rates': 0.7540227960411324, 'learning_rate': 9.731398119646492e-05, 'batch_size': 108, 'step_size': 13, 'gamma': 0.87487097453181}. Best is trial 25 with value: 0.08344084192923017.[0m
[32m[I 2025-01-31 11:39:51,754][0m Trial 49 finished with value: 0.43012355758073134 and parameters: {'observation_period_num': 64, 'train_rates': 0.7928669123676508, 'learning_rate': 4.429792725488056e-06, 'batch_size': 163, 'step_size': 10, 'gamma': 0.835779062095843}. Best is trial 25 with value: 0.08344084192923017.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-31 11:39:51,764][0m A new study created in memory with name: no-name-fc6cde74-e404-4444-8b4a-b63d7e53dce5[0m
[32m[I 2025-01-31 11:40:40,034][0m Trial 0 finished with value: 0.33616261081402876 and parameters: {'observation_period_num': 149, 'train_rates': 0.6019142148758082, 'learning_rate': 2.6153551708422527e-05, 'batch_size': 85, 'step_size': 10, 'gamma': 0.7600087753071287}. Best is trial 0 with value: 0.33616261081402876.[0m
[32m[I 2025-01-31 11:41:06,258][0m Trial 1 finished with value: 0.4770597633070983 and parameters: {'observation_period_num': 160, 'train_rates': 0.6826235061510493, 'learning_rate': 3.87981760637832e-06, 'batch_size': 179, 'step_size': 13, 'gamma': 0.9561588357283621}. Best is trial 0 with value: 0.33616261081402876.[0m
[32m[I 2025-01-31 11:45:14,651][0m Trial 2 finished with value: 0.10479644876943917 and parameters: {'observation_period_num': 50, 'train_rates': 0.7951034305539536, 'learning_rate': 0.0006957182932354061, 'batch_size': 20, 'step_size': 9, 'gamma': 0.946042995038477}. Best is trial 2 with value: 0.10479644876943917.[0m
Early stopping at epoch 97
[32m[I 2025-01-31 11:46:31,193][0m Trial 3 finished with value: 0.45007691112743026 and parameters: {'observation_period_num': 88, 'train_rates': 0.969965170300118, 'learning_rate': 0.00015079506042585907, 'batch_size': 74, 'step_size': 1, 'gamma': 0.8778168809183814}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:47:36,589][0m Trial 4 finished with value: 0.43582555907113213 and parameters: {'observation_period_num': 245, 'train_rates': 0.7931336385730237, 'learning_rate': 2.4998316601947945e-06, 'batch_size': 74, 'step_size': 12, 'gamma': 0.8901132565137685}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:48:02,042][0m Trial 5 finished with value: 0.22659033500566717 and parameters: {'observation_period_num': 181, 'train_rates': 0.7338994033392615, 'learning_rate': 3.909364585970386e-05, 'batch_size': 198, 'step_size': 4, 'gamma': 0.9265182185578396}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:48:24,248][0m Trial 6 finished with value: 0.33605929450894295 and parameters: {'observation_period_num': 10, 'train_rates': 0.6740811107826576, 'learning_rate': 1.7196906067504545e-06, 'batch_size': 234, 'step_size': 12, 'gamma': 0.9424457318339902}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:48:54,456][0m Trial 7 finished with value: 0.32152774930000305 and parameters: {'observation_period_num': 74, 'train_rates': 0.9584670909540094, 'learning_rate': 5.569238748234361e-05, 'batch_size': 205, 'step_size': 15, 'gamma': 0.9724288559336846}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:49:50,555][0m Trial 8 finished with value: 0.25300368798226447 and parameters: {'observation_period_num': 42, 'train_rates': 0.8443366649066988, 'learning_rate': 4.440288839234079e-06, 'batch_size': 97, 'step_size': 11, 'gamma': 0.8490882996581267}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:50:10,766][0m Trial 9 finished with value: 0.36919389494789495 and parameters: {'observation_period_num': 132, 'train_rates': 0.6062627459833124, 'learning_rate': 1.0185431791092088e-05, 'batch_size': 227, 'step_size': 14, 'gamma': 0.8301049846167172}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:54:07,444][0m Trial 10 finished with value: 0.13792355509798218 and parameters: {'observation_period_num': 71, 'train_rates': 0.8581551377453238, 'learning_rate': 0.0007920830972531082, 'batch_size': 22, 'step_size': 7, 'gamma': 0.9868816908532925}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 11:58:27,963][0m Trial 11 finished with value: 0.1568629206581549 and parameters: {'observation_period_num': 84, 'train_rates': 0.8699161307265803, 'learning_rate': 0.0009320875947972543, 'batch_size': 20, 'step_size': 7, 'gamma': 0.9820022580162435}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 12:01:12,630][0m Trial 12 finished with value: 0.12378379728375788 and parameters: {'observation_period_num': 26, 'train_rates': 0.8865650037606244, 'learning_rate': 0.0008987280789444942, 'batch_size': 33, 'step_size': 7, 'gamma': 0.9187428066562525}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 12:01:56,152][0m Trial 13 finished with value: 0.1280064875872372 and parameters: {'observation_period_num': 5, 'train_rates': 0.9009650472526969, 'learning_rate': 0.00023572506426441836, 'batch_size': 135, 'step_size': 9, 'gamma': 0.908332738798539}. Best is trial 2 with value: 0.10479644876943917.[0m
[32m[I 2025-01-31 12:03:58,601][0m Trial 14 finished with value: 0.09307214004659889 and parameters: {'observation_period_num': 31, 'train_rates': 0.7932935190162393, 'learning_rate': 0.00031709737205379843, 'batch_size': 41, 'step_size': 5, 'gamma': 0.9194299720041025}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:05:31,613][0m Trial 15 finished with value: 0.09963941809505904 and parameters: {'observation_period_num': 53, 'train_rates': 0.7884934678331997, 'learning_rate': 0.00023760890361518535, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8109093436816507}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:06:14,146][0m Trial 16 finished with value: 0.13229153696419432 and parameters: {'observation_period_num': 102, 'train_rates': 0.768332327474166, 'learning_rate': 0.00020994536775511866, 'batch_size': 121, 'step_size': 4, 'gamma': 0.7960323076668844}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:07:44,707][0m Trial 17 finished with value: 0.11188119842803904 and parameters: {'observation_period_num': 46, 'train_rates': 0.7304554840498252, 'learning_rate': 9.090464436449574e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8099761731534783}. Best is trial 14 with value: 0.09307214004659889.[0m
Early stopping at epoch 48
[32m[I 2025-01-31 12:08:01,675][0m Trial 18 finished with value: 0.25556846343644773 and parameters: {'observation_period_num': 112, 'train_rates': 0.8221998198208463, 'learning_rate': 0.00029572455251791, 'batch_size': 157, 'step_size': 1, 'gamma': 0.7584199310178913}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:09:53,245][0m Trial 19 finished with value: 0.26125924183904187 and parameters: {'observation_period_num': 213, 'train_rates': 0.930354455816442, 'learning_rate': 0.0003678079635519712, 'batch_size': 48, 'step_size': 5, 'gamma': 0.8421286302363347}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:10:38,600][0m Trial 20 finished with value: 0.35926592350006104 and parameters: {'observation_period_num': 60, 'train_rates': 0.738688562705552, 'learning_rate': 1.6995948318811162e-05, 'batch_size': 112, 'step_size': 2, 'gamma': 0.7896053825763144}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:12:17,409][0m Trial 21 finished with value: 0.11092645493926297 and parameters: {'observation_period_num': 38, 'train_rates': 0.8053763785167212, 'learning_rate': 0.00043507730305596256, 'batch_size': 52, 'step_size': 6, 'gamma': 0.8952234233197073}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:14:16,821][0m Trial 22 finished with value: 0.10118906739551355 and parameters: {'observation_period_num': 26, 'train_rates': 0.7677772152122052, 'learning_rate': 0.00010601809223689243, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9451211290299003}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:15:33,584][0m Trial 23 finished with value: 0.0982208846605939 and parameters: {'observation_period_num': 18, 'train_rates': 0.7633455042515637, 'learning_rate': 0.00010516876778740171, 'batch_size': 64, 'step_size': 3, 'gamma': 0.8613385284815828}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:16:47,927][0m Trial 24 finished with value: 0.10212991673792346 and parameters: {'observation_period_num': 22, 'train_rates': 0.6962843236080088, 'learning_rate': 7.409623897358862e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8661393621481965}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:17:42,271][0m Trial 25 finished with value: 0.12048821000898459 and parameters: {'observation_period_num': 62, 'train_rates': 0.8278498921752507, 'learning_rate': 0.00014301315546088225, 'batch_size': 99, 'step_size': 3, 'gamma': 0.8542115711789924}. Best is trial 14 with value: 0.09307214004659889.[0m
[32m[I 2025-01-31 12:18:55,275][0m Trial 26 finished with value: 0.08352563685516061 and parameters: {'observation_period_num': 27, 'train_rates': 0.7615355653444568, 'learning_rate': 0.0004882872583233361, 'batch_size': 69, 'step_size': 5, 'gamma': 0.8122188175542439}. Best is trial 26 with value: 0.08352563685516061.[0m
[32m[I 2025-01-31 12:19:58,460][0m Trial 27 finished with value: 0.06454671587914335 and parameters: {'observation_period_num': 9, 'train_rates': 0.7136762595200775, 'learning_rate': 0.0004589693453720936, 'batch_size': 75, 'step_size': 6, 'gamma': 0.8266764586037684}. Best is trial 27 with value: 0.06454671587914335.[0m
[32m[I 2025-01-31 12:20:50,864][0m Trial 28 finished with value: 0.06786732937176387 and parameters: {'observation_period_num': 7, 'train_rates': 0.7023647855286407, 'learning_rate': 0.0004997257031706104, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8311077449670916}. Best is trial 27 with value: 0.06454671587914335.[0m
[32m[I 2025-01-31 12:21:44,958][0m Trial 29 finished with value: 0.06642731712854724 and parameters: {'observation_period_num': 12, 'train_rates': 0.6471930331191968, 'learning_rate': 0.0005330463312936547, 'batch_size': 85, 'step_size': 6, 'gamma': 0.7810768981825083}. Best is trial 27 with value: 0.06454671587914335.[0m
[32m[I 2025-01-31 12:22:30,331][0m Trial 30 finished with value: 0.06494670618719202 and parameters: {'observation_period_num': 7, 'train_rates': 0.6242394806769554, 'learning_rate': 0.0005421030787738433, 'batch_size': 99, 'step_size': 8, 'gamma': 0.774489064544531}. Best is trial 27 with value: 0.06454671587914335.[0m
[32m[I 2025-01-31 12:23:19,483][0m Trial 31 finished with value: 0.060991504238180914 and parameters: {'observation_period_num': 5, 'train_rates': 0.631694572051324, 'learning_rate': 0.000588946841599191, 'batch_size': 92, 'step_size': 8, 'gamma': 0.7741042729215036}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:23:59,804][0m Trial 32 finished with value: 0.06524074015175 and parameters: {'observation_period_num': 7, 'train_rates': 0.6312515925447166, 'learning_rate': 0.0005944352398727656, 'batch_size': 116, 'step_size': 8, 'gamma': 0.7745377250651297}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:24:28,125][0m Trial 33 finished with value: 0.1754861811425748 and parameters: {'observation_period_num': 169, 'train_rates': 0.6009185613254187, 'learning_rate': 0.0006732784673087452, 'batch_size': 153, 'step_size': 8, 'gamma': 0.7779687688458539}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:25:07,856][0m Trial 34 finished with value: 0.09592522864931086 and parameters: {'observation_period_num': 40, 'train_rates': 0.6333437113344885, 'learning_rate': 0.00017204054827902202, 'batch_size': 110, 'step_size': 10, 'gamma': 0.7710569045782335}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:25:45,281][0m Trial 35 finished with value: 0.07211366868371447 and parameters: {'observation_period_num': 6, 'train_rates': 0.6556710596645104, 'learning_rate': 0.0005918206981122575, 'batch_size': 128, 'step_size': 8, 'gamma': 0.7567095838046235}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:26:14,504][0m Trial 36 finished with value: 0.1750923101715356 and parameters: {'observation_period_num': 135, 'train_rates': 0.6250918298456363, 'learning_rate': 0.0003376052838803218, 'batch_size': 157, 'step_size': 10, 'gamma': 0.768153232106318}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:27:05,374][0m Trial 37 finished with value: 0.22055536485810318 and parameters: {'observation_period_num': 203, 'train_rates': 0.6666282075419139, 'learning_rate': 3.259785529280988e-05, 'batch_size': 84, 'step_size': 8, 'gamma': 0.7974201065515378}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:27:41,045][0m Trial 38 finished with value: 0.07447571614586296 and parameters: {'observation_period_num': 19, 'train_rates': 0.702947212111241, 'learning_rate': 0.0009913360534724515, 'batch_size': 143, 'step_size': 11, 'gamma': 0.8288296614060214}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:28:22,185][0m Trial 39 finished with value: 0.22960967504742394 and parameters: {'observation_period_num': 53, 'train_rates': 0.6251885226451517, 'learning_rate': 1.9576815677690884e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.7506377996100922}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:29:19,794][0m Trial 40 finished with value: 0.8378197261038028 and parameters: {'observation_period_num': 105, 'train_rates': 0.6842909125559329, 'learning_rate': 1.1161334617748157e-06, 'batch_size': 78, 'step_size': 7, 'gamma': 0.7896335569436145}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:29:57,039][0m Trial 41 finished with value: 0.0808528108626777 and parameters: {'observation_period_num': 16, 'train_rates': 0.6480718121881248, 'learning_rate': 0.0005812823062245363, 'batch_size': 120, 'step_size': 6, 'gamma': 0.7793594930812199}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:30:45,776][0m Trial 42 finished with value: 0.10327309475662151 and parameters: {'observation_period_num': 36, 'train_rates': 0.6422157184844675, 'learning_rate': 0.000408775486626962, 'batch_size': 92, 'step_size': 6, 'gamma': 0.7686909915380729}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:31:39,795][0m Trial 43 finished with value: 0.06146392759680748 and parameters: {'observation_period_num': 5, 'train_rates': 0.6184895798228882, 'learning_rate': 0.0006505702484835437, 'batch_size': 81, 'step_size': 9, 'gamma': 0.7802855360944677}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:31:57,693][0m Trial 44 finished with value: 0.3098025530404438 and parameters: {'observation_period_num': 250, 'train_rates': 0.6160795295332369, 'learning_rate': 0.0006836867204736659, 'batch_size': 254, 'step_size': 9, 'gamma': 0.8202496320005054}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:32:43,281][0m Trial 45 finished with value: 0.08393611172559085 and parameters: {'observation_period_num': 32, 'train_rates': 0.6673855225236769, 'learning_rate': 0.0001410887406056247, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8042212021996066}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:33:45,939][0m Trial 46 finished with value: 0.07083864654527931 and parameters: {'observation_period_num': 15, 'train_rates': 0.7209446444835199, 'learning_rate': 0.0002731998724954416, 'batch_size': 76, 'step_size': 8, 'gamma': 0.7850785078447314}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:34:13,323][0m Trial 47 finished with value: 0.5600347044416096 and parameters: {'observation_period_num': 70, 'train_rates': 0.6010887739594591, 'learning_rate': 5.5947028707270215e-06, 'batch_size': 171, 'step_size': 10, 'gamma': 0.7668419046600673}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:34:51,890][0m Trial 48 finished with value: 0.06633557912889664 and parameters: {'observation_period_num': 5, 'train_rates': 0.6785930769004811, 'learning_rate': 0.0008150286218436495, 'batch_size': 124, 'step_size': 7, 'gamma': 0.750807400917355}. Best is trial 31 with value: 0.060991504238180914.[0m
[32m[I 2025-01-31 12:35:24,119][0m Trial 49 finished with value: 0.1034561607489761 and parameters: {'observation_period_num': 50, 'train_rates': 0.6153915004271407, 'learning_rate': 0.00021055106548102654, 'batch_size': 139, 'step_size': 12, 'gamma': 0.8816750102701766}. Best is trial 31 with value: 0.060991504238180914.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-31 12:35:24,334][0m A new study created in memory with name: no-name-51d279e4-342c-4dda-846f-d41d31c41310[0m
[32m[I 2025-01-31 12:38:16,059][0m Trial 0 finished with value: 0.24081938182231372 and parameters: {'observation_period_num': 144, 'train_rates': 0.6372538786653139, 'learning_rate': 1.2343109031586758e-05, 'batch_size': 24, 'step_size': 3, 'gamma': 0.8698682364182394}. Best is trial 0 with value: 0.24081938182231372.[0m
[32m[I 2025-01-31 12:38:36,003][0m Trial 1 finished with value: 0.3225885540844718 and parameters: {'observation_period_num': 92, 'train_rates': 0.6339812745758631, 'learning_rate': 1.5239372764860772e-05, 'batch_size': 230, 'step_size': 8, 'gamma': 0.844760474388595}. Best is trial 0 with value: 0.24081938182231372.[0m
[32m[I 2025-01-31 12:40:00,501][0m Trial 2 finished with value: 0.2009631400364776 and parameters: {'observation_period_num': 17, 'train_rates': 0.85353545977091, 'learning_rate': 3.3805408640195344e-06, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7940035654682425}. Best is trial 2 with value: 0.2009631400364776.[0m
[32m[I 2025-01-31 12:42:00,794][0m Trial 3 finished with value: 0.5413461921415923 and parameters: {'observation_period_num': 192, 'train_rates': 0.8022718655553249, 'learning_rate': 1.4808524806433074e-06, 'batch_size': 40, 'step_size': 8, 'gamma': 0.8327630594425661}. Best is trial 2 with value: 0.2009631400364776.[0m
[32m[I 2025-01-31 12:42:32,131][0m Trial 4 finished with value: 0.2881523241238161 and parameters: {'observation_period_num': 133, 'train_rates': 0.6100214326541658, 'learning_rate': 5.6498234216372513e-05, 'batch_size': 141, 'step_size': 1, 'gamma': 0.9439177660783418}. Best is trial 2 with value: 0.2009631400364776.[0m
[32m[I 2025-01-31 12:43:10,178][0m Trial 5 finished with value: 0.10734710097312927 and parameters: {'observation_period_num': 65, 'train_rates': 0.9872929399002706, 'learning_rate': 0.0002954765319368648, 'batch_size': 159, 'step_size': 3, 'gamma': 0.7708263755546132}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:46:21,696][0m Trial 6 finished with value: 0.16450381380910017 and parameters: {'observation_period_num': 158, 'train_rates': 0.8863072517032331, 'learning_rate': 3.16396521001321e-05, 'batch_size': 27, 'step_size': 12, 'gamma': 0.9581728413149956}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:46:51,836][0m Trial 7 finished with value: 0.1494567264821015 and parameters: {'observation_period_num': 177, 'train_rates': 0.7245622129877088, 'learning_rate': 0.0005155307262180842, 'batch_size': 161, 'step_size': 4, 'gamma': 0.7880929436693157}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:47:37,029][0m Trial 8 finished with value: 0.3781396357194046 and parameters: {'observation_period_num': 84, 'train_rates': 0.8865002705366591, 'learning_rate': 7.133758356846987e-06, 'batch_size': 124, 'step_size': 4, 'gamma': 0.9295446667928493}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:49:13,671][0m Trial 9 finished with value: 0.5992746414190315 and parameters: {'observation_period_num': 30, 'train_rates': 0.9719058141916367, 'learning_rate': 1.4718566092140097e-06, 'batch_size': 61, 'step_size': 9, 'gamma': 0.9286087410581272}. Best is trial 5 with value: 0.10734710097312927.[0m
Early stopping at epoch 48
[32m[I 2025-01-31 12:49:25,937][0m Trial 10 finished with value: 0.3582295775413513 and parameters: {'observation_period_num': 232, 'train_rates': 0.9872464853487499, 'learning_rate': 0.0009972824789952186, 'batch_size': 245, 'step_size': 1, 'gamma': 0.7515234078336068}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:49:53,595][0m Trial 11 finished with value: 0.11675358849948801 and parameters: {'observation_period_num': 90, 'train_rates': 0.707375640467714, 'learning_rate': 0.0006805579938657071, 'batch_size': 175, 'step_size': 5, 'gamma': 0.7548247025460548}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:50:20,991][0m Trial 12 finished with value: 0.13837583392605227 and parameters: {'observation_period_num': 75, 'train_rates': 0.7549491965550563, 'learning_rate': 0.000185047100972907, 'batch_size': 184, 'step_size': 6, 'gamma': 0.7505628381704257}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:50:46,765][0m Trial 13 finished with value: 0.11282905015585706 and parameters: {'observation_period_num': 54, 'train_rates': 0.6867113683387932, 'learning_rate': 0.00015409415195672289, 'batch_size': 198, 'step_size': 6, 'gamma': 0.7958246781561289}. Best is trial 5 with value: 0.10734710097312927.[0m
[32m[I 2025-01-31 12:51:10,656][0m Trial 14 finished with value: 0.10162463910143141 and parameters: {'observation_period_num': 45, 'train_rates': 0.6826112104843917, 'learning_rate': 0.00014767084798319262, 'batch_size': 208, 'step_size': 10, 'gamma': 0.8062383700684173}. Best is trial 14 with value: 0.10162463910143141.[0m
[32m[I 2025-01-31 12:51:35,469][0m Trial 15 finished with value: 0.09503171562963963 and parameters: {'observation_period_num': 7, 'train_rates': 0.7807485948846009, 'learning_rate': 0.0001538667310441467, 'batch_size': 224, 'step_size': 11, 'gamma': 0.8254732617840442}. Best is trial 15 with value: 0.09503171562963963.[0m
[32m[I 2025-01-31 12:52:02,599][0m Trial 16 finished with value: 0.10621259755605077 and parameters: {'observation_period_num': 8, 'train_rates': 0.783211113311861, 'learning_rate': 7.811502530135185e-05, 'batch_size': 209, 'step_size': 15, 'gamma': 0.8270939958267002}. Best is trial 15 with value: 0.09503171562963963.[0m
[32m[I 2025-01-31 12:52:43,462][0m Trial 17 finished with value: 0.09103026327333952 and parameters: {'observation_period_num': 41, 'train_rates': 0.673719626697956, 'learning_rate': 9.41569879164566e-05, 'batch_size': 110, 'step_size': 11, 'gamma': 0.8680543072441287}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:53:35,388][0m Trial 18 finished with value: 0.09541026272989334 and parameters: {'observation_period_num': 8, 'train_rates': 0.8086167028169909, 'learning_rate': 5.645917757944341e-05, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8943146364318665}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:54:24,121][0m Trial 19 finished with value: 0.12266560165516173 and parameters: {'observation_period_num': 107, 'train_rates': 0.7378514390799076, 'learning_rate': 0.0003152712254856928, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8891401355486379}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:55:08,813][0m Trial 20 finished with value: 0.11582629243959434 and parameters: {'observation_period_num': 38, 'train_rates': 0.6645192893152884, 'learning_rate': 2.5904037343760366e-05, 'batch_size': 106, 'step_size': 11, 'gamma': 0.8663202650692045}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:56:06,382][0m Trial 21 finished with value: 0.09544652917077354 and parameters: {'observation_period_num': 5, 'train_rates': 0.823293619650759, 'learning_rate': 7.177606926576531e-05, 'batch_size': 93, 'step_size': 11, 'gamma': 0.8961550492295545}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:56:46,981][0m Trial 22 finished with value: 0.10306653604135141 and parameters: {'observation_period_num': 31, 'train_rates': 0.7720614085678221, 'learning_rate': 4.448609384930455e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.8854851866529134}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:57:54,232][0m Trial 23 finished with value: 0.10030188145382064 and parameters: {'observation_period_num': 25, 'train_rates': 0.8280246859025149, 'learning_rate': 0.0001149652456294912, 'batch_size': 80, 'step_size': 9, 'gamma': 0.9065037643258104}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:58:45,525][0m Trial 24 finished with value: 0.17953627568726638 and parameters: {'observation_period_num': 60, 'train_rates': 0.9154124389089999, 'learning_rate': 0.00026149025527118263, 'batch_size': 114, 'step_size': 13, 'gamma': 0.8609058935048219}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 12:59:22,893][0m Trial 25 finished with value: 0.1588284382925314 and parameters: {'observation_period_num': 110, 'train_rates': 0.8506141036803833, 'learning_rate': 2.4841403003430525e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9801295438818858}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 13:00:28,349][0m Trial 26 finished with value: 0.1012767332889179 and parameters: {'observation_period_num': 44, 'train_rates': 0.8043878469415395, 'learning_rate': 0.00010134576632963002, 'batch_size': 80, 'step_size': 11, 'gamma': 0.8151423974801053}. Best is trial 17 with value: 0.09103026327333952.[0m
[32m[I 2025-01-31 13:01:55,883][0m Trial 27 finished with value: 0.08585051849673865 and parameters: {'observation_period_num': 10, 'train_rates': 0.7595041075922699, 'learning_rate': 4.4083005118813195e-05, 'batch_size': 57, 'step_size': 7, 'gamma': 0.8487492777588364}. Best is trial 27 with value: 0.08585051849673865.[0m
[32m[I 2025-01-31 13:02:57,911][0m Trial 28 finished with value: 0.3056467488744205 and parameters: {'observation_period_num': 252, 'train_rates': 0.7540511596724959, 'learning_rate': 1.521158312909228e-05, 'batch_size': 74, 'step_size': 7, 'gamma': 0.8566218904875879}. Best is trial 27 with value: 0.08585051849673865.[0m
[32m[I 2025-01-31 13:05:10,297][0m Trial 29 finished with value: 0.12874988732130632 and parameters: {'observation_period_num': 22, 'train_rates': 0.6468101932263507, 'learning_rate': 7.366173694256914e-06, 'batch_size': 33, 'step_size': 9, 'gamma': 0.8434801821355888}. Best is trial 27 with value: 0.08585051849673865.[0m
[32m[I 2025-01-31 13:06:59,816][0m Trial 30 finished with value: 0.10956453906326759 and parameters: {'observation_period_num': 67, 'train_rates': 0.7159209924812237, 'learning_rate': 3.913978904110799e-05, 'batch_size': 43, 'step_size': 7, 'gamma': 0.8754605424172848}. Best is trial 27 with value: 0.08585051849673865.[0m
[32m[I 2025-01-31 13:12:03,546][0m Trial 31 finished with value: 0.07284746735815541 and parameters: {'observation_period_num': 5, 'train_rates': 0.7701960213765681, 'learning_rate': 6.373835854834866e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8454534499631499}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:13:37,374][0m Trial 32 finished with value: 0.07806347406457362 and parameters: {'observation_period_num': 6, 'train_rates': 0.7615289362564455, 'learning_rate': 0.00010101555496033875, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8404155827342173}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:18:30,830][0m Trial 33 finished with value: 0.10247511408395238 and parameters: {'observation_period_num': 44, 'train_rates': 0.7525162163027693, 'learning_rate': 1.979766823687355e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8446737514820712}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:19:49,703][0m Trial 34 finished with value: 0.14165145027975848 and parameters: {'observation_period_num': 22, 'train_rates': 0.6092264993474723, 'learning_rate': 9.729319526904984e-06, 'batch_size': 53, 'step_size': 10, 'gamma': 0.8562891305764043}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:21:20,508][0m Trial 35 finished with value: 0.07700224818200926 and parameters: {'observation_period_num': 17, 'train_rates': 0.7014155373834281, 'learning_rate': 9.659183686064248e-05, 'batch_size': 51, 'step_size': 8, 'gamma': 0.8424940429254011}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:22:55,867][0m Trial 36 finished with value: 0.09471762681611672 and parameters: {'observation_period_num': 23, 'train_rates': 0.7037561914682964, 'learning_rate': 5.283317548844654e-05, 'batch_size': 48, 'step_size': 9, 'gamma': 0.8378637578778105}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:26:09,145][0m Trial 37 finished with value: 0.2289275045007428 and parameters: {'observation_period_num': 54, 'train_rates': 0.7368632426567356, 'learning_rate': 3.6605425850252265e-06, 'batch_size': 24, 'step_size': 8, 'gamma': 0.8112107604030839}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:27:23,739][0m Trial 38 finished with value: 0.07392834195601089 and parameters: {'observation_period_num': 13, 'train_rates': 0.7620187267574279, 'learning_rate': 0.00022404303667216606, 'batch_size': 66, 'step_size': 8, 'gamma': 0.8268442345450326}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:29:19,321][0m Trial 39 finished with value: 0.14993316050099337 and parameters: {'observation_period_num': 106, 'train_rates': 0.6448446377036724, 'learning_rate': 0.00043494851936197226, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8225103919066172}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:33:10,353][0m Trial 40 finished with value: 0.14785826293228493 and parameters: {'observation_period_num': 160, 'train_rates': 0.7272913341359861, 'learning_rate': 0.00023805292245634464, 'batch_size': 19, 'step_size': 8, 'gamma': 0.7804213552411109}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:34:24,907][0m Trial 41 finished with value: 0.08745548985176754 and parameters: {'observation_period_num': 15, 'train_rates': 0.7715435285299641, 'learning_rate': 7.245905925533377e-05, 'batch_size': 65, 'step_size': 6, 'gamma': 0.8501512606963184}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:35:54,296][0m Trial 42 finished with value: 0.09980574684312284 and parameters: {'observation_period_num': 16, 'train_rates': 0.7591306148038275, 'learning_rate': 3.637768475446125e-05, 'batch_size': 55, 'step_size': 7, 'gamma': 0.8331762443750552}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:37:11,118][0m Trial 43 finished with value: 0.09971444566946218 and parameters: {'observation_period_num': 35, 'train_rates': 0.792307012740328, 'learning_rate': 0.00012382998141591197, 'batch_size': 66, 'step_size': 5, 'gamma': 0.7988623403410622}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:38:11,646][0m Trial 44 finished with value: 0.6750752900828835 and parameters: {'observation_period_num': 17, 'train_rates': 0.8380202730414845, 'learning_rate': 1.005248837134317e-06, 'batch_size': 88, 'step_size': 9, 'gamma': 0.8384942238030213}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:40:08,503][0m Trial 45 finished with value: 0.08613140197324284 and parameters: {'observation_period_num': 30, 'train_rates': 0.6954925383841909, 'learning_rate': 0.0002004191699779721, 'batch_size': 39, 'step_size': 10, 'gamma': 0.8736959567652938}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:41:37,814][0m Trial 46 finished with value: 0.14788083684261286 and parameters: {'observation_period_num': 211, 'train_rates': 0.7390987401578187, 'learning_rate': 0.0003283047674900586, 'batch_size': 50, 'step_size': 8, 'gamma': 0.8180332481050576}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:42:42,464][0m Trial 47 finished with value: 0.10659345000112144 and parameters: {'observation_period_num': 71, 'train_rates': 0.7148806422773113, 'learning_rate': 0.000471950780603645, 'batch_size': 72, 'step_size': 5, 'gamma': 0.8488983818225018}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:45:10,557][0m Trial 48 finished with value: 0.16531641804613173 and parameters: {'observation_period_num': 124, 'train_rates': 0.813604616562608, 'learning_rate': 8.672877186589435e-05, 'batch_size': 33, 'step_size': 2, 'gamma': 0.7826527777868286}. Best is trial 31 with value: 0.07284746735815541.[0m
[32m[I 2025-01-31 13:46:42,628][0m Trial 49 finished with value: 0.13175583018859227 and parameters: {'observation_period_num': 55, 'train_rates': 0.8706626591183286, 'learning_rate': 6.021806312245117e-05, 'batch_size': 58, 'step_size': 12, 'gamma': 0.802296795866897}. Best is trial 31 with value: 0.07284746735815541.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_SBUX_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 36, 'train_rates': 0.9874969324050649, 'learning_rate': 0.00018337655424356704, 'batch_size': 140, 'step_size': 4, 'gamma': 0.8942606944583541}
Epoch 1/300, trend Loss: 0.6669 | 0.3090
Epoch 2/300, trend Loss: 0.4041 | 0.1929
Epoch 3/300, trend Loss: 0.3008 | 0.2469
Epoch 4/300, trend Loss: 0.2852 | 0.4021
Epoch 5/300, trend Loss: 0.2319 | 0.1682
Epoch 6/300, trend Loss: 0.1973 | 0.1373
Epoch 7/300, trend Loss: 0.1967 | 0.1451
Epoch 8/300, trend Loss: 0.1617 | 0.1410
Epoch 9/300, trend Loss: 0.1506 | 0.1316
Epoch 10/300, trend Loss: 0.1445 | 0.1281
Epoch 11/300, trend Loss: 0.1402 | 0.1227
Epoch 12/300, trend Loss: 0.1356 | 0.1142
Epoch 13/300, trend Loss: 0.1317 | 0.1104
Epoch 14/300, trend Loss: 0.1296 | 0.1108
Epoch 15/300, trend Loss: 0.1263 | 0.1046
Epoch 16/300, trend Loss: 0.1242 | 0.0983
Epoch 17/300, trend Loss: 0.1221 | 0.0935
Epoch 18/300, trend Loss: 0.1200 | 0.0927
Epoch 19/300, trend Loss: 0.1187 | 0.0924
Epoch 20/300, trend Loss: 0.1167 | 0.0904
Epoch 21/300, trend Loss: 0.1151 | 0.0859
Epoch 22/300, trend Loss: 0.1140 | 0.0838
Epoch 23/300, trend Loss: 0.1126 | 0.0826
Epoch 24/300, trend Loss: 0.1115 | 0.0825
Epoch 25/300, trend Loss: 0.1100 | 0.0812
Epoch 26/300, trend Loss: 0.1092 | 0.0792
Epoch 27/300, trend Loss: 0.1084 | 0.0780
Epoch 28/300, trend Loss: 0.1075 | 0.0774
Epoch 29/300, trend Loss: 0.1066 | 0.0768
Epoch 30/300, trend Loss: 0.1058 | 0.0758
Epoch 31/300, trend Loss: 0.1052 | 0.0751
Epoch 32/300, trend Loss: 0.1046 | 0.0745
Epoch 33/300, trend Loss: 0.1039 | 0.0740
Epoch 34/300, trend Loss: 0.1034 | 0.0735
Epoch 35/300, trend Loss: 0.1029 | 0.0730
Epoch 36/300, trend Loss: 0.1024 | 0.0726
Epoch 37/300, trend Loss: 0.1019 | 0.0722
Epoch 38/300, trend Loss: 0.1015 | 0.0719
Epoch 39/300, trend Loss: 0.1011 | 0.0716
Epoch 40/300, trend Loss: 0.1007 | 0.0712
Epoch 41/300, trend Loss: 0.1003 | 0.0710
Epoch 42/300, trend Loss: 0.1000 | 0.0707
Epoch 43/300, trend Loss: 0.0997 | 0.0705
Epoch 44/300, trend Loss: 0.0994 | 0.0702
Epoch 45/300, trend Loss: 0.0991 | 0.0700
Epoch 46/300, trend Loss: 0.0988 | 0.0698
Epoch 47/300, trend Loss: 0.0986 | 0.0696
Epoch 48/300, trend Loss: 0.0983 | 0.0694
Epoch 49/300, trend Loss: 0.0981 | 0.0693
Epoch 50/300, trend Loss: 0.0979 | 0.0691
Epoch 51/300, trend Loss: 0.0977 | 0.0690
Epoch 52/300, trend Loss: 0.0975 | 0.0688
Epoch 53/300, trend Loss: 0.0973 | 0.0687
Epoch 54/300, trend Loss: 0.0971 | 0.0686
Epoch 55/300, trend Loss: 0.0970 | 0.0685
Epoch 56/300, trend Loss: 0.0968 | 0.0684
Epoch 57/300, trend Loss: 0.0967 | 0.0683
Epoch 58/300, trend Loss: 0.0965 | 0.0682
Epoch 59/300, trend Loss: 0.0964 | 0.0681
Epoch 60/300, trend Loss: 0.0963 | 0.0680
Epoch 61/300, trend Loss: 0.0961 | 0.0679
Epoch 62/300, trend Loss: 0.0960 | 0.0678
Epoch 63/300, trend Loss: 0.0959 | 0.0677
Epoch 64/300, trend Loss: 0.0958 | 0.0677
Epoch 65/300, trend Loss: 0.0957 | 0.0676
Epoch 66/300, trend Loss: 0.0956 | 0.0675
Epoch 67/300, trend Loss: 0.0955 | 0.0675
Epoch 68/300, trend Loss: 0.0955 | 0.0674
Epoch 69/300, trend Loss: 0.0954 | 0.0674
Epoch 70/300, trend Loss: 0.0953 | 0.0673
Epoch 71/300, trend Loss: 0.0952 | 0.0673
Epoch 72/300, trend Loss: 0.0952 | 0.0672
Epoch 73/300, trend Loss: 0.0951 | 0.0672
Epoch 74/300, trend Loss: 0.0951 | 0.0671
Epoch 75/300, trend Loss: 0.0950 | 0.0671
Epoch 76/300, trend Loss: 0.0949 | 0.0671
Epoch 77/300, trend Loss: 0.0949 | 0.0670
Epoch 78/300, trend Loss: 0.0948 | 0.0670
Epoch 79/300, trend Loss: 0.0948 | 0.0670
Epoch 80/300, trend Loss: 0.0947 | 0.0669
Epoch 81/300, trend Loss: 0.0947 | 0.0669
Epoch 82/300, trend Loss: 0.0947 | 0.0669
Epoch 83/300, trend Loss: 0.0946 | 0.0669
Epoch 84/300, trend Loss: 0.0946 | 0.0668
Epoch 85/300, trend Loss: 0.0946 | 0.0668
Epoch 86/300, trend Loss: 0.0945 | 0.0668
Epoch 87/300, trend Loss: 0.0945 | 0.0668
Epoch 88/300, trend Loss: 0.0945 | 0.0668
Epoch 89/300, trend Loss: 0.0944 | 0.0667
Epoch 90/300, trend Loss: 0.0944 | 0.0667
Epoch 91/300, trend Loss: 0.0944 | 0.0667
Epoch 92/300, trend Loss: 0.0944 | 0.0667
Epoch 93/300, trend Loss: 0.0943 | 0.0667
Epoch 94/300, trend Loss: 0.0943 | 0.0667
Epoch 95/300, trend Loss: 0.0943 | 0.0666
Epoch 96/300, trend Loss: 0.0943 | 0.0666
Epoch 97/300, trend Loss: 0.0943 | 0.0666
Epoch 98/300, trend Loss: 0.0942 | 0.0666
Epoch 99/300, trend Loss: 0.0942 | 0.0666
Epoch 100/300, trend Loss: 0.0942 | 0.0666
Epoch 101/300, trend Loss: 0.0942 | 0.0666
Epoch 102/300, trend Loss: 0.0942 | 0.0666
Epoch 103/300, trend Loss: 0.0942 | 0.0666
Epoch 104/300, trend Loss: 0.0942 | 0.0665
Epoch 105/300, trend Loss: 0.0942 | 0.0665
Epoch 106/300, trend Loss: 0.0941 | 0.0665
Epoch 107/300, trend Loss: 0.0941 | 0.0665
Epoch 108/300, trend Loss: 0.0941 | 0.0665
Epoch 109/300, trend Loss: 0.0941 | 0.0665
Epoch 110/300, trend Loss: 0.0941 | 0.0665
Epoch 111/300, trend Loss: 0.0941 | 0.0665
Epoch 112/300, trend Loss: 0.0941 | 0.0665
Epoch 113/300, trend Loss: 0.0941 | 0.0665
Epoch 114/300, trend Loss: 0.0941 | 0.0665
Epoch 115/300, trend Loss: 0.0941 | 0.0665
Epoch 116/300, trend Loss: 0.0941 | 0.0665
Epoch 117/300, trend Loss: 0.0940 | 0.0665
Epoch 118/300, trend Loss: 0.0940 | 0.0665
Epoch 119/300, trend Loss: 0.0940 | 0.0665
Epoch 120/300, trend Loss: 0.0940 | 0.0665
Epoch 121/300, trend Loss: 0.0940 | 0.0665
Epoch 122/300, trend Loss: 0.0940 | 0.0664
Epoch 123/300, trend Loss: 0.0940 | 0.0664
Epoch 124/300, trend Loss: 0.0940 | 0.0664
Epoch 125/300, trend Loss: 0.0940 | 0.0664
Epoch 126/300, trend Loss: 0.0940 | 0.0664
Epoch 127/300, trend Loss: 0.0940 | 0.0664
Epoch 128/300, trend Loss: 0.0940 | 0.0664
Epoch 129/300, trend Loss: 0.0940 | 0.0664
Epoch 130/300, trend Loss: 0.0940 | 0.0664
Epoch 131/300, trend Loss: 0.0940 | 0.0664
Epoch 132/300, trend Loss: 0.0940 | 0.0664
Epoch 133/300, trend Loss: 0.0940 | 0.0664
Epoch 134/300, trend Loss: 0.0940 | 0.0664
Epoch 135/300, trend Loss: 0.0940 | 0.0664
Epoch 136/300, trend Loss: 0.0940 | 0.0664
Epoch 137/300, trend Loss: 0.0940 | 0.0664
Epoch 138/300, trend Loss: 0.0940 | 0.0664
Epoch 139/300, trend Loss: 0.0940 | 0.0664
Epoch 140/300, trend Loss: 0.0940 | 0.0664
Epoch 141/300, trend Loss: 0.0940 | 0.0664
Epoch 142/300, trend Loss: 0.0940 | 0.0664
Epoch 143/300, trend Loss: 0.0940 | 0.0664
Epoch 144/300, trend Loss: 0.0940 | 0.0664
Epoch 145/300, trend Loss: 0.0940 | 0.0664
Epoch 146/300, trend Loss: 0.0940 | 0.0664
Epoch 147/300, trend Loss: 0.0940 | 0.0664
Epoch 148/300, trend Loss: 0.0940 | 0.0664
Epoch 149/300, trend Loss: 0.0940 | 0.0664
Epoch 150/300, trend Loss: 0.0939 | 0.0664
Epoch 151/300, trend Loss: 0.0939 | 0.0664
Epoch 152/300, trend Loss: 0.0939 | 0.0664
Epoch 153/300, trend Loss: 0.0939 | 0.0664
Epoch 154/300, trend Loss: 0.0939 | 0.0664
Epoch 155/300, trend Loss: 0.0939 | 0.0664
Epoch 156/300, trend Loss: 0.0939 | 0.0664
Epoch 157/300, trend Loss: 0.0939 | 0.0664
Epoch 158/300, trend Loss: 0.0939 | 0.0664
Epoch 159/300, trend Loss: 0.0939 | 0.0664
Epoch 160/300, trend Loss: 0.0939 | 0.0664
Epoch 161/300, trend Loss: 0.0939 | 0.0664
Epoch 162/300, trend Loss: 0.0939 | 0.0664
Epoch 163/300, trend Loss: 0.0939 | 0.0664
Epoch 164/300, trend Loss: 0.0939 | 0.0664
Epoch 165/300, trend Loss: 0.0939 | 0.0664
Epoch 166/300, trend Loss: 0.0939 | 0.0664
Epoch 167/300, trend Loss: 0.0939 | 0.0664
Epoch 168/300, trend Loss: 0.0939 | 0.0664
Epoch 169/300, trend Loss: 0.0939 | 0.0664
Epoch 170/300, trend Loss: 0.0939 | 0.0664
Epoch 171/300, trend Loss: 0.0939 | 0.0664
Epoch 172/300, trend Loss: 0.0939 | 0.0664
Epoch 173/300, trend Loss: 0.0939 | 0.0664
Epoch 174/300, trend Loss: 0.0939 | 0.0664
Epoch 175/300, trend Loss: 0.0939 | 0.0664
Epoch 176/300, trend Loss: 0.0939 | 0.0664
Epoch 177/300, trend Loss: 0.0939 | 0.0664
Epoch 178/300, trend Loss: 0.0939 | 0.0664
Epoch 179/300, trend Loss: 0.0939 | 0.0664
Epoch 180/300, trend Loss: 0.0939 | 0.0664
Epoch 181/300, trend Loss: 0.0939 | 0.0664
Epoch 182/300, trend Loss: 0.0939 | 0.0664
Epoch 183/300, trend Loss: 0.0939 | 0.0664
Epoch 184/300, trend Loss: 0.0939 | 0.0664
Epoch 185/300, trend Loss: 0.0939 | 0.0664
Epoch 186/300, trend Loss: 0.0939 | 0.0664
Epoch 187/300, trend Loss: 0.0939 | 0.0664
Epoch 188/300, trend Loss: 0.0939 | 0.0664
Epoch 189/300, trend Loss: 0.0939 | 0.0664
Epoch 190/300, trend Loss: 0.0939 | 0.0664
Epoch 191/300, trend Loss: 0.0939 | 0.0664
Epoch 192/300, trend Loss: 0.0939 | 0.0664
Epoch 193/300, trend Loss: 0.0939 | 0.0664
Epoch 194/300, trend Loss: 0.0939 | 0.0664
Epoch 195/300, trend Loss: 0.0939 | 0.0664
Epoch 196/300, trend Loss: 0.0939 | 0.0664
Epoch 197/300, trend Loss: 0.0939 | 0.0664
Epoch 198/300, trend Loss: 0.0939 | 0.0664
Epoch 199/300, trend Loss: 0.0939 | 0.0664
Epoch 200/300, trend Loss: 0.0939 | 0.0664
Epoch 201/300, trend Loss: 0.0939 | 0.0664
Epoch 202/300, trend Loss: 0.0939 | 0.0664
Epoch 203/300, trend Loss: 0.0939 | 0.0664
Epoch 204/300, trend Loss: 0.0939 | 0.0664
Epoch 205/300, trend Loss: 0.0939 | 0.0664
Epoch 206/300, trend Loss: 0.0939 | 0.0664
Epoch 207/300, trend Loss: 0.0939 | 0.0664
Epoch 208/300, trend Loss: 0.0939 | 0.0664
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 31, 'train_rates': 0.9823823015798168, 'learning_rate': 7.005241597981609e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8525009481265459}
Epoch 1/300, seasonal_0 Loss: 0.2526 | 0.1614
Epoch 2/300, seasonal_0 Loss: 0.1631 | 0.1213
Epoch 3/300, seasonal_0 Loss: 0.1385 | 0.0962
Epoch 4/300, seasonal_0 Loss: 0.1276 | 0.0790
Epoch 5/300, seasonal_0 Loss: 0.1214 | 0.0699
Epoch 6/300, seasonal_0 Loss: 0.1155 | 0.0651
Epoch 7/300, seasonal_0 Loss: 0.1115 | 0.0628
Epoch 8/300, seasonal_0 Loss: 0.1084 | 0.0615
Epoch 9/300, seasonal_0 Loss: 0.1055 | 0.0599
Epoch 10/300, seasonal_0 Loss: 0.1030 | 0.0622
Epoch 11/300, seasonal_0 Loss: 0.0996 | 0.0574
Epoch 12/300, seasonal_0 Loss: 0.0971 | 0.0551
Epoch 13/300, seasonal_0 Loss: 0.0951 | 0.0567
Epoch 14/300, seasonal_0 Loss: 0.0929 | 0.0532
Epoch 15/300, seasonal_0 Loss: 0.0913 | 0.0512
Epoch 16/300, seasonal_0 Loss: 0.0899 | 0.0521
Epoch 17/300, seasonal_0 Loss: 0.0885 | 0.0502
Epoch 18/300, seasonal_0 Loss: 0.0875 | 0.0488
Epoch 19/300, seasonal_0 Loss: 0.0864 | 0.0492
Epoch 20/300, seasonal_0 Loss: 0.0855 | 0.0481
Epoch 21/300, seasonal_0 Loss: 0.0846 | 0.0472
Epoch 22/300, seasonal_0 Loss: 0.0837 | 0.0472
Epoch 23/300, seasonal_0 Loss: 0.0830 | 0.0465
Epoch 24/300, seasonal_0 Loss: 0.0823 | 0.0460
Epoch 25/300, seasonal_0 Loss: 0.0815 | 0.0456
Epoch 26/300, seasonal_0 Loss: 0.0809 | 0.0452
Epoch 27/300, seasonal_0 Loss: 0.0804 | 0.0449
Epoch 28/300, seasonal_0 Loss: 0.0797 | 0.0445
Epoch 29/300, seasonal_0 Loss: 0.0793 | 0.0442
Epoch 30/300, seasonal_0 Loss: 0.0788 | 0.0441
Epoch 31/300, seasonal_0 Loss: 0.0782 | 0.0438
Epoch 32/300, seasonal_0 Loss: 0.0779 | 0.0436
Epoch 33/300, seasonal_0 Loss: 0.0775 | 0.0436
Epoch 34/300, seasonal_0 Loss: 0.0770 | 0.0434
Epoch 35/300, seasonal_0 Loss: 0.0767 | 0.0432
Epoch 36/300, seasonal_0 Loss: 0.0764 | 0.0431
Epoch 37/300, seasonal_0 Loss: 0.0760 | 0.0429
Epoch 38/300, seasonal_0 Loss: 0.0757 | 0.0427
Epoch 39/300, seasonal_0 Loss: 0.0755 | 0.0426
Epoch 40/300, seasonal_0 Loss: 0.0752 | 0.0425
Epoch 41/300, seasonal_0 Loss: 0.0749 | 0.0423
Epoch 42/300, seasonal_0 Loss: 0.0747 | 0.0421
Epoch 43/300, seasonal_0 Loss: 0.0744 | 0.0421
Epoch 44/300, seasonal_0 Loss: 0.0742 | 0.0419
Epoch 45/300, seasonal_0 Loss: 0.0741 | 0.0417
Epoch 46/300, seasonal_0 Loss: 0.0738 | 0.0419
Epoch 47/300, seasonal_0 Loss: 0.0737 | 0.0417
Epoch 48/300, seasonal_0 Loss: 0.0735 | 0.0416
Epoch 49/300, seasonal_0 Loss: 0.0733 | 0.0418
Epoch 50/300, seasonal_0 Loss: 0.0732 | 0.0417
Epoch 51/300, seasonal_0 Loss: 0.0731 | 0.0416
Epoch 52/300, seasonal_0 Loss: 0.0729 | 0.0416
Epoch 53/300, seasonal_0 Loss: 0.0728 | 0.0415
Epoch 54/300, seasonal_0 Loss: 0.0727 | 0.0414
Epoch 55/300, seasonal_0 Loss: 0.0726 | 0.0412
Epoch 56/300, seasonal_0 Loss: 0.0725 | 0.0411
Epoch 57/300, seasonal_0 Loss: 0.0724 | 0.0411
Epoch 58/300, seasonal_0 Loss: 0.0723 | 0.0407
Epoch 59/300, seasonal_0 Loss: 0.0722 | 0.0407
Epoch 60/300, seasonal_0 Loss: 0.0721 | 0.0406
Epoch 61/300, seasonal_0 Loss: 0.0720 | 0.0403
Epoch 62/300, seasonal_0 Loss: 0.0719 | 0.0403
Epoch 63/300, seasonal_0 Loss: 0.0718 | 0.0402
Epoch 64/300, seasonal_0 Loss: 0.0717 | 0.0400
Epoch 65/300, seasonal_0 Loss: 0.0717 | 0.0400
Epoch 66/300, seasonal_0 Loss: 0.0716 | 0.0399
Epoch 67/300, seasonal_0 Loss: 0.0715 | 0.0399
Epoch 68/300, seasonal_0 Loss: 0.0715 | 0.0398
Epoch 69/300, seasonal_0 Loss: 0.0714 | 0.0398
Epoch 70/300, seasonal_0 Loss: 0.0714 | 0.0398
Epoch 71/300, seasonal_0 Loss: 0.0713 | 0.0397
Epoch 72/300, seasonal_0 Loss: 0.0713 | 0.0397
Epoch 73/300, seasonal_0 Loss: 0.0712 | 0.0397
Epoch 74/300, seasonal_0 Loss: 0.0712 | 0.0397
Epoch 75/300, seasonal_0 Loss: 0.0711 | 0.0397
Epoch 76/300, seasonal_0 Loss: 0.0711 | 0.0397
Epoch 77/300, seasonal_0 Loss: 0.0710 | 0.0397
Epoch 78/300, seasonal_0 Loss: 0.0710 | 0.0396
Epoch 79/300, seasonal_0 Loss: 0.0710 | 0.0396
Epoch 80/300, seasonal_0 Loss: 0.0709 | 0.0396
Epoch 81/300, seasonal_0 Loss: 0.0709 | 0.0396
Epoch 82/300, seasonal_0 Loss: 0.0709 | 0.0396
Epoch 83/300, seasonal_0 Loss: 0.0708 | 0.0396
Epoch 84/300, seasonal_0 Loss: 0.0708 | 0.0396
Epoch 85/300, seasonal_0 Loss: 0.0708 | 0.0396
Epoch 86/300, seasonal_0 Loss: 0.0708 | 0.0396
Epoch 87/300, seasonal_0 Loss: 0.0707 | 0.0395
Epoch 88/300, seasonal_0 Loss: 0.0707 | 0.0395
Epoch 89/300, seasonal_0 Loss: 0.0707 | 0.0395
Epoch 90/300, seasonal_0 Loss: 0.0707 | 0.0395
Epoch 91/300, seasonal_0 Loss: 0.0706 | 0.0395
Epoch 92/300, seasonal_0 Loss: 0.0706 | 0.0395
Epoch 93/300, seasonal_0 Loss: 0.0706 | 0.0395
Epoch 94/300, seasonal_0 Loss: 0.0706 | 0.0395
Epoch 95/300, seasonal_0 Loss: 0.0706 | 0.0395
Epoch 96/300, seasonal_0 Loss: 0.0706 | 0.0395
Epoch 97/300, seasonal_0 Loss: 0.0706 | 0.0396
Epoch 98/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 99/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 100/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 101/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 102/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 103/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 104/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 105/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 106/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 107/300, seasonal_0 Loss: 0.0705 | 0.0396
Epoch 108/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 109/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 110/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 111/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 112/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 113/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 114/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 115/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 116/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 117/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 118/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 119/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 120/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 121/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 122/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 123/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 124/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 125/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 126/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 127/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 128/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 129/300, seasonal_0 Loss: 0.0704 | 0.0396
Epoch 130/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 131/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 132/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 133/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 134/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 135/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 136/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 137/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 138/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 139/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 140/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 141/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 142/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 143/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 144/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 145/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 146/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 147/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 148/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 149/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 150/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 151/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 152/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 153/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 154/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 155/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 156/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 157/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 158/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 159/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 160/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 161/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 162/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 163/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 164/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 165/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 166/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 167/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 168/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 169/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 170/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 171/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 172/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 173/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 174/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 175/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 176/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 177/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 178/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 179/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 180/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 181/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 182/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 183/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 184/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 185/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 186/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 187/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 188/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 189/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 190/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 191/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 192/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 193/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 194/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 195/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 196/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 197/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 198/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 199/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 200/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 201/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 202/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 203/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 204/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 205/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 206/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 207/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 208/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 209/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 210/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 211/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 212/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 213/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 214/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 215/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 216/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 217/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 218/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 219/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 220/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 221/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 222/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 223/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 224/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 225/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 226/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 227/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 228/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 229/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 230/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 231/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 232/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 233/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 234/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 235/300, seasonal_0 Loss: 0.0703 | 0.0396
Epoch 236/300, seasonal_0 Loss: 0.0703 | 0.0396
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.7885740545813038, 'learning_rate': 0.0006339623822353624, 'batch_size': 57, 'step_size': 9, 'gamma': 0.8112806178754011}
Epoch 1/300, seasonal_1 Loss: 0.2531 | 0.1405
Epoch 2/300, seasonal_1 Loss: 0.1327 | 0.1472
Epoch 3/300, seasonal_1 Loss: 0.1252 | 0.1247
Epoch 4/300, seasonal_1 Loss: 0.1261 | 0.1083
Epoch 5/300, seasonal_1 Loss: 0.1168 | 0.1240
Epoch 6/300, seasonal_1 Loss: 0.1052 | 0.1204
Epoch 7/300, seasonal_1 Loss: 0.0991 | 0.1377
Epoch 8/300, seasonal_1 Loss: 0.1025 | 0.1107
Epoch 9/300, seasonal_1 Loss: 0.1003 | 0.1453
Epoch 10/300, seasonal_1 Loss: 0.1021 | 0.1006
Epoch 11/300, seasonal_1 Loss: 0.0955 | 0.0988
Epoch 12/300, seasonal_1 Loss: 0.0914 | 0.1009
Epoch 13/300, seasonal_1 Loss: 0.0920 | 0.1029
Epoch 14/300, seasonal_1 Loss: 0.0930 | 0.0966
Epoch 15/300, seasonal_1 Loss: 0.0901 | 0.0928
Epoch 16/300, seasonal_1 Loss: 0.0864 | 0.0895
Epoch 17/300, seasonal_1 Loss: 0.0837 | 0.0879
Epoch 18/300, seasonal_1 Loss: 0.0806 | 0.0847
Epoch 19/300, seasonal_1 Loss: 0.0797 | 0.0855
Epoch 20/300, seasonal_1 Loss: 0.0805 | 0.0853
Epoch 21/300, seasonal_1 Loss: 0.0802 | 0.0848
Epoch 22/300, seasonal_1 Loss: 0.0807 | 0.0843
Epoch 23/300, seasonal_1 Loss: 0.0797 | 0.0837
Epoch 24/300, seasonal_1 Loss: 0.0793 | 0.0827
Epoch 25/300, seasonal_1 Loss: 0.0784 | 0.0826
Epoch 26/300, seasonal_1 Loss: 0.0773 | 0.0808
Epoch 27/300, seasonal_1 Loss: 0.0762 | 0.0814
Epoch 28/300, seasonal_1 Loss: 0.0752 | 0.0799
Epoch 29/300, seasonal_1 Loss: 0.0744 | 0.0800
Epoch 30/300, seasonal_1 Loss: 0.0736 | 0.0788
Epoch 31/300, seasonal_1 Loss: 0.0729 | 0.0799
Epoch 32/300, seasonal_1 Loss: 0.0726 | 0.0781
Epoch 33/300, seasonal_1 Loss: 0.0721 | 0.0794
Epoch 34/300, seasonal_1 Loss: 0.0723 | 0.0781
Epoch 35/300, seasonal_1 Loss: 0.0717 | 0.0789
Epoch 36/300, seasonal_1 Loss: 0.0710 | 0.0777
Epoch 37/300, seasonal_1 Loss: 0.0700 | 0.0775
Epoch 38/300, seasonal_1 Loss: 0.0694 | 0.0766
Epoch 39/300, seasonal_1 Loss: 0.0691 | 0.0767
Epoch 40/300, seasonal_1 Loss: 0.0690 | 0.0763
Epoch 41/300, seasonal_1 Loss: 0.0691 | 0.0765
Epoch 42/300, seasonal_1 Loss: 0.0692 | 0.0766
Epoch 43/300, seasonal_1 Loss: 0.0692 | 0.0766
Epoch 44/300, seasonal_1 Loss: 0.0690 | 0.0765
Epoch 45/300, seasonal_1 Loss: 0.0687 | 0.0765
Epoch 46/300, seasonal_1 Loss: 0.0685 | 0.0763
Epoch 47/300, seasonal_1 Loss: 0.0686 | 0.0763
Epoch 48/300, seasonal_1 Loss: 0.0684 | 0.0764
Epoch 49/300, seasonal_1 Loss: 0.0680 | 0.0765
Epoch 50/300, seasonal_1 Loss: 0.0676 | 0.0765
Epoch 51/300, seasonal_1 Loss: 0.0674 | 0.0764
Epoch 52/300, seasonal_1 Loss: 0.0673 | 0.0764
Epoch 53/300, seasonal_1 Loss: 0.0671 | 0.0764
Epoch 54/300, seasonal_1 Loss: 0.0669 | 0.0764
Epoch 55/300, seasonal_1 Loss: 0.0666 | 0.0759
Epoch 56/300, seasonal_1 Loss: 0.0665 | 0.0759
Epoch 57/300, seasonal_1 Loss: 0.0663 | 0.0759
Epoch 58/300, seasonal_1 Loss: 0.0661 | 0.0759
Epoch 59/300, seasonal_1 Loss: 0.0659 | 0.0759
Epoch 60/300, seasonal_1 Loss: 0.0657 | 0.0757
Epoch 61/300, seasonal_1 Loss: 0.0656 | 0.0757
Epoch 62/300, seasonal_1 Loss: 0.0654 | 0.0757
Epoch 63/300, seasonal_1 Loss: 0.0653 | 0.0757
Epoch 64/300, seasonal_1 Loss: 0.0651 | 0.0757
Epoch 65/300, seasonal_1 Loss: 0.0650 | 0.0756
Epoch 66/300, seasonal_1 Loss: 0.0649 | 0.0756
Epoch 67/300, seasonal_1 Loss: 0.0648 | 0.0756
Epoch 68/300, seasonal_1 Loss: 0.0646 | 0.0756
Epoch 69/300, seasonal_1 Loss: 0.0645 | 0.0756
Epoch 70/300, seasonal_1 Loss: 0.0645 | 0.0756
Epoch 71/300, seasonal_1 Loss: 0.0644 | 0.0756
Epoch 72/300, seasonal_1 Loss: 0.0643 | 0.0756
Epoch 73/300, seasonal_1 Loss: 0.0642 | 0.0756
Epoch 74/300, seasonal_1 Loss: 0.0642 | 0.0756
Epoch 75/300, seasonal_1 Loss: 0.0641 | 0.0755
Epoch 76/300, seasonal_1 Loss: 0.0640 | 0.0755
Epoch 77/300, seasonal_1 Loss: 0.0640 | 0.0755
Epoch 78/300, seasonal_1 Loss: 0.0640 | 0.0755
Epoch 79/300, seasonal_1 Loss: 0.0640 | 0.0755
Epoch 80/300, seasonal_1 Loss: 0.0639 | 0.0755
Epoch 81/300, seasonal_1 Loss: 0.0639 | 0.0754
Epoch 82/300, seasonal_1 Loss: 0.0638 | 0.0755
Epoch 83/300, seasonal_1 Loss: 0.0638 | 0.0755
Epoch 84/300, seasonal_1 Loss: 0.0637 | 0.0754
Epoch 85/300, seasonal_1 Loss: 0.0636 | 0.0754
Epoch 86/300, seasonal_1 Loss: 0.0635 | 0.0753
Epoch 87/300, seasonal_1 Loss: 0.0634 | 0.0753
Epoch 88/300, seasonal_1 Loss: 0.0633 | 0.0752
Epoch 89/300, seasonal_1 Loss: 0.0632 | 0.0752
Epoch 90/300, seasonal_1 Loss: 0.0631 | 0.0751
Epoch 91/300, seasonal_1 Loss: 0.0631 | 0.0750
Epoch 92/300, seasonal_1 Loss: 0.0630 | 0.0750
Epoch 93/300, seasonal_1 Loss: 0.0630 | 0.0749
Epoch 94/300, seasonal_1 Loss: 0.0629 | 0.0749
Epoch 95/300, seasonal_1 Loss: 0.0628 | 0.0749
Epoch 96/300, seasonal_1 Loss: 0.0628 | 0.0748
Epoch 97/300, seasonal_1 Loss: 0.0628 | 0.0748
Epoch 98/300, seasonal_1 Loss: 0.0627 | 0.0748
Epoch 99/300, seasonal_1 Loss: 0.0627 | 0.0747
Epoch 100/300, seasonal_1 Loss: 0.0627 | 0.0748
Epoch 101/300, seasonal_1 Loss: 0.0627 | 0.0747
Epoch 102/300, seasonal_1 Loss: 0.0626 | 0.0747
Epoch 103/300, seasonal_1 Loss: 0.0626 | 0.0747
Epoch 104/300, seasonal_1 Loss: 0.0626 | 0.0747
Epoch 105/300, seasonal_1 Loss: 0.0626 | 0.0746
Epoch 106/300, seasonal_1 Loss: 0.0626 | 0.0746
Epoch 107/300, seasonal_1 Loss: 0.0626 | 0.0746
Epoch 108/300, seasonal_1 Loss: 0.0627 | 0.0746
Epoch 109/300, seasonal_1 Loss: 0.0627 | 0.0746
Epoch 110/300, seasonal_1 Loss: 0.0627 | 0.0746
Epoch 111/300, seasonal_1 Loss: 0.0627 | 0.0746
Epoch 112/300, seasonal_1 Loss: 0.0626 | 0.0746
Epoch 113/300, seasonal_1 Loss: 0.0626 | 0.0746
Epoch 114/300, seasonal_1 Loss: 0.0625 | 0.0746
Epoch 115/300, seasonal_1 Loss: 0.0624 | 0.0745
Epoch 116/300, seasonal_1 Loss: 0.0623 | 0.0745
Epoch 117/300, seasonal_1 Loss: 0.0623 | 0.0745
Epoch 118/300, seasonal_1 Loss: 0.0623 | 0.0745
Epoch 119/300, seasonal_1 Loss: 0.0622 | 0.0745
Epoch 120/300, seasonal_1 Loss: 0.0622 | 0.0745
Epoch 121/300, seasonal_1 Loss: 0.0622 | 0.0745
Epoch 122/300, seasonal_1 Loss: 0.0622 | 0.0744
Epoch 123/300, seasonal_1 Loss: 0.0621 | 0.0744
Epoch 124/300, seasonal_1 Loss: 0.0621 | 0.0744
Epoch 125/300, seasonal_1 Loss: 0.0621 | 0.0744
Epoch 126/300, seasonal_1 Loss: 0.0621 | 0.0744
Epoch 127/300, seasonal_1 Loss: 0.0621 | 0.0744
Epoch 128/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 129/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 130/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 131/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 132/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 133/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 134/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 135/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 136/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 137/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 138/300, seasonal_1 Loss: 0.0620 | 0.0744
Epoch 139/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 140/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 141/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 142/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 143/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 144/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 145/300, seasonal_1 Loss: 0.0619 | 0.0744
Epoch 146/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 147/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 148/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 149/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 150/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 151/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 152/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 153/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 154/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 155/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 156/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 157/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 158/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 159/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 160/300, seasonal_1 Loss: 0.0619 | 0.0743
Epoch 161/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 162/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 163/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 164/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 165/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 166/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 167/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 168/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 169/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 170/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 171/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 172/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 173/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 174/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 175/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 176/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 177/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 178/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 179/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 180/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 181/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 182/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 183/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 184/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 185/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 186/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 187/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 188/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 189/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 190/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 191/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 192/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 193/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 194/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 195/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 196/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 197/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 198/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 199/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 200/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 201/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 202/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 203/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 204/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 205/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 206/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 207/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 208/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 209/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 210/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 211/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 212/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 213/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 214/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 215/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 216/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 217/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 218/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 219/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 220/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 221/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 222/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 223/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 224/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 225/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 226/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 227/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 228/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 229/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 230/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 231/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 232/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 233/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 234/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 235/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 236/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 237/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 238/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 239/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 240/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 241/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 242/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 243/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 244/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 245/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 246/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 247/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 248/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 249/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 250/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 251/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 252/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 253/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 254/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 255/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 256/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 257/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 258/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 259/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 260/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 261/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 262/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 263/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 264/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 265/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 266/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 267/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 268/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 269/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 270/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 271/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 272/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 273/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 274/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 275/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 276/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 277/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 278/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 279/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 280/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 281/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 282/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 283/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 284/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 285/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 286/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 287/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 288/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 289/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 290/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 291/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 292/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 293/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 294/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 295/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 296/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 297/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 298/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 299/300, seasonal_1 Loss: 0.0618 | 0.0743
Epoch 300/300, seasonal_1 Loss: 0.0618 | 0.0743
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.808144668346283, 'learning_rate': 0.0005135303882600146, 'batch_size': 124, 'step_size': 9, 'gamma': 0.8631055743495988}
Epoch 1/300, seasonal_2 Loss: 0.2553 | 0.1618
Epoch 2/300, seasonal_2 Loss: 0.1404 | 0.1380
Epoch 3/300, seasonal_2 Loss: 0.1444 | 0.1312
Epoch 4/300, seasonal_2 Loss: 0.1295 | 0.1280
Epoch 5/300, seasonal_2 Loss: 0.1244 | 0.1260
Epoch 6/300, seasonal_2 Loss: 0.1225 | 0.1234
Epoch 7/300, seasonal_2 Loss: 0.1300 | 0.1273
Epoch 8/300, seasonal_2 Loss: 0.1196 | 0.1227
Epoch 9/300, seasonal_2 Loss: 0.1114 | 0.1174
Epoch 10/300, seasonal_2 Loss: 0.1092 | 0.1127
Epoch 11/300, seasonal_2 Loss: 0.1052 | 0.1175
Epoch 12/300, seasonal_2 Loss: 0.1058 | 0.1366
Epoch 13/300, seasonal_2 Loss: 0.1057 | 0.1172
Epoch 14/300, seasonal_2 Loss: 0.1134 | 0.1084
Epoch 15/300, seasonal_2 Loss: 0.1132 | 0.1063
Epoch 16/300, seasonal_2 Loss: 0.1089 | 0.1080
Epoch 17/300, seasonal_2 Loss: 0.1136 | 0.1181
Epoch 18/300, seasonal_2 Loss: 0.1080 | 0.1079
Epoch 19/300, seasonal_2 Loss: 0.1002 | 0.1050
Epoch 20/300, seasonal_2 Loss: 0.1082 | 0.1063
Epoch 21/300, seasonal_2 Loss: 0.0920 | 0.1004
Epoch 22/300, seasonal_2 Loss: 0.0898 | 0.1033
Epoch 23/300, seasonal_2 Loss: 0.0906 | 0.1030
Epoch 24/300, seasonal_2 Loss: 0.0910 | 0.1018
Epoch 25/300, seasonal_2 Loss: 0.0873 | 0.0965
Epoch 26/300, seasonal_2 Loss: 0.0864 | 0.0963
Epoch 27/300, seasonal_2 Loss: 0.0853 | 0.0966
Epoch 28/300, seasonal_2 Loss: 0.0860 | 0.0963
Epoch 29/300, seasonal_2 Loss: 0.0841 | 0.0951
Epoch 30/300, seasonal_2 Loss: 0.0854 | 0.0947
Epoch 31/300, seasonal_2 Loss: 0.0834 | 0.0943
Epoch 32/300, seasonal_2 Loss: 0.0831 | 0.0951
Epoch 33/300, seasonal_2 Loss: 0.0831 | 0.0947
Epoch 34/300, seasonal_2 Loss: 0.0816 | 0.0934
Epoch 35/300, seasonal_2 Loss: 0.0821 | 0.0931
Epoch 36/300, seasonal_2 Loss: 0.0810 | 0.0928
Epoch 37/300, seasonal_2 Loss: 0.0809 | 0.0937
Epoch 38/300, seasonal_2 Loss: 0.0816 | 0.0939
Epoch 39/300, seasonal_2 Loss: 0.0820 | 0.0943
Epoch 40/300, seasonal_2 Loss: 0.0837 | 0.0951
Epoch 41/300, seasonal_2 Loss: 0.0847 | 0.0943
Epoch 42/300, seasonal_2 Loss: 0.0839 | 0.0953
Epoch 43/300, seasonal_2 Loss: 0.0832 | 0.0961
Epoch 44/300, seasonal_2 Loss: 0.0836 | 0.0940
Epoch 45/300, seasonal_2 Loss: 0.0846 | 0.0928
Epoch 46/300, seasonal_2 Loss: 0.0858 | 0.0938
Epoch 47/300, seasonal_2 Loss: 0.0899 | 0.0976
Epoch 48/300, seasonal_2 Loss: 0.0926 | 0.0983
Epoch 49/300, seasonal_2 Loss: 0.0905 | 0.0957
Epoch 50/300, seasonal_2 Loss: 0.0857 | 0.0925
Epoch 51/300, seasonal_2 Loss: 0.0838 | 0.0921
Epoch 52/300, seasonal_2 Loss: 0.0786 | 0.0912
Epoch 53/300, seasonal_2 Loss: 0.0767 | 0.0901
Epoch 54/300, seasonal_2 Loss: 0.0759 | 0.0889
Epoch 55/300, seasonal_2 Loss: 0.0754 | 0.0892
Epoch 56/300, seasonal_2 Loss: 0.0755 | 0.0888
Epoch 57/300, seasonal_2 Loss: 0.0756 | 0.0889
Epoch 58/300, seasonal_2 Loss: 0.0757 | 0.0884
Epoch 59/300, seasonal_2 Loss: 0.0755 | 0.0880
Epoch 60/300, seasonal_2 Loss: 0.0751 | 0.0877
Epoch 61/300, seasonal_2 Loss: 0.0744 | 0.0879
Epoch 62/300, seasonal_2 Loss: 0.0738 | 0.0881
Epoch 63/300, seasonal_2 Loss: 0.0737 | 0.0881
Epoch 64/300, seasonal_2 Loss: 0.0736 | 0.0879
Epoch 65/300, seasonal_2 Loss: 0.0735 | 0.0877
Epoch 66/300, seasonal_2 Loss: 0.0734 | 0.0876
Epoch 67/300, seasonal_2 Loss: 0.0731 | 0.0874
Epoch 68/300, seasonal_2 Loss: 0.0729 | 0.0873
Epoch 69/300, seasonal_2 Loss: 0.0728 | 0.0871
Epoch 70/300, seasonal_2 Loss: 0.0726 | 0.0871
Epoch 71/300, seasonal_2 Loss: 0.0725 | 0.0871
Epoch 72/300, seasonal_2 Loss: 0.0724 | 0.0870
Epoch 73/300, seasonal_2 Loss: 0.0723 | 0.0869
Epoch 74/300, seasonal_2 Loss: 0.0722 | 0.0868
Epoch 75/300, seasonal_2 Loss: 0.0721 | 0.0868
Epoch 76/300, seasonal_2 Loss: 0.0720 | 0.0867
Epoch 77/300, seasonal_2 Loss: 0.0720 | 0.0867
Epoch 78/300, seasonal_2 Loss: 0.0719 | 0.0866
Epoch 79/300, seasonal_2 Loss: 0.0718 | 0.0865
Epoch 80/300, seasonal_2 Loss: 0.0717 | 0.0865
Epoch 81/300, seasonal_2 Loss: 0.0716 | 0.0864
Epoch 82/300, seasonal_2 Loss: 0.0715 | 0.0863
Epoch 83/300, seasonal_2 Loss: 0.0715 | 0.0862
Epoch 84/300, seasonal_2 Loss: 0.0714 | 0.0862
Epoch 85/300, seasonal_2 Loss: 0.0713 | 0.0861
Epoch 86/300, seasonal_2 Loss: 0.0713 | 0.0861
Epoch 87/300, seasonal_2 Loss: 0.0712 | 0.0860
Epoch 88/300, seasonal_2 Loss: 0.0711 | 0.0860
Epoch 89/300, seasonal_2 Loss: 0.0711 | 0.0859
Epoch 90/300, seasonal_2 Loss: 0.0710 | 0.0859
Epoch 91/300, seasonal_2 Loss: 0.0709 | 0.0859
Epoch 92/300, seasonal_2 Loss: 0.0709 | 0.0858
Epoch 93/300, seasonal_2 Loss: 0.0708 | 0.0858
Epoch 94/300, seasonal_2 Loss: 0.0708 | 0.0858
Epoch 95/300, seasonal_2 Loss: 0.0707 | 0.0858
Epoch 96/300, seasonal_2 Loss: 0.0707 | 0.0857
Epoch 97/300, seasonal_2 Loss: 0.0706 | 0.0857
Epoch 98/300, seasonal_2 Loss: 0.0706 | 0.0857
Epoch 99/300, seasonal_2 Loss: 0.0705 | 0.0856
Epoch 100/300, seasonal_2 Loss: 0.0704 | 0.0856
Epoch 101/300, seasonal_2 Loss: 0.0704 | 0.0855
Epoch 102/300, seasonal_2 Loss: 0.0703 | 0.0855
Epoch 103/300, seasonal_2 Loss: 0.0703 | 0.0855
Epoch 104/300, seasonal_2 Loss: 0.0702 | 0.0854
Epoch 105/300, seasonal_2 Loss: 0.0702 | 0.0854
Epoch 106/300, seasonal_2 Loss: 0.0701 | 0.0853
Epoch 107/300, seasonal_2 Loss: 0.0701 | 0.0853
Epoch 108/300, seasonal_2 Loss: 0.0700 | 0.0853
Epoch 109/300, seasonal_2 Loss: 0.0700 | 0.0853
Epoch 110/300, seasonal_2 Loss: 0.0700 | 0.0852
Epoch 111/300, seasonal_2 Loss: 0.0699 | 0.0852
Epoch 112/300, seasonal_2 Loss: 0.0699 | 0.0852
Epoch 113/300, seasonal_2 Loss: 0.0698 | 0.0852
Epoch 114/300, seasonal_2 Loss: 0.0698 | 0.0852
Epoch 115/300, seasonal_2 Loss: 0.0698 | 0.0851
Epoch 116/300, seasonal_2 Loss: 0.0697 | 0.0851
Epoch 117/300, seasonal_2 Loss: 0.0697 | 0.0851
Epoch 118/300, seasonal_2 Loss: 0.0696 | 0.0850
Epoch 119/300, seasonal_2 Loss: 0.0696 | 0.0850
Epoch 120/300, seasonal_2 Loss: 0.0696 | 0.0850
Epoch 121/300, seasonal_2 Loss: 0.0695 | 0.0850
Epoch 122/300, seasonal_2 Loss: 0.0695 | 0.0849
Epoch 123/300, seasonal_2 Loss: 0.0695 | 0.0849
Epoch 124/300, seasonal_2 Loss: 0.0694 | 0.0849
Epoch 125/300, seasonal_2 Loss: 0.0694 | 0.0849
Epoch 126/300, seasonal_2 Loss: 0.0694 | 0.0848
Epoch 127/300, seasonal_2 Loss: 0.0693 | 0.0848
Epoch 128/300, seasonal_2 Loss: 0.0693 | 0.0848
Epoch 129/300, seasonal_2 Loss: 0.0693 | 0.0848
Epoch 130/300, seasonal_2 Loss: 0.0693 | 0.0848
Epoch 131/300, seasonal_2 Loss: 0.0692 | 0.0847
Epoch 132/300, seasonal_2 Loss: 0.0692 | 0.0847
Epoch 133/300, seasonal_2 Loss: 0.0692 | 0.0847
Epoch 134/300, seasonal_2 Loss: 0.0692 | 0.0847
Epoch 135/300, seasonal_2 Loss: 0.0691 | 0.0847
Epoch 136/300, seasonal_2 Loss: 0.0691 | 0.0847
Epoch 137/300, seasonal_2 Loss: 0.0691 | 0.0846
Epoch 138/300, seasonal_2 Loss: 0.0691 | 0.0846
Epoch 139/300, seasonal_2 Loss: 0.0691 | 0.0846
Epoch 140/300, seasonal_2 Loss: 0.0690 | 0.0846
Epoch 141/300, seasonal_2 Loss: 0.0690 | 0.0846
Epoch 142/300, seasonal_2 Loss: 0.0690 | 0.0846
Epoch 143/300, seasonal_2 Loss: 0.0690 | 0.0846
Epoch 144/300, seasonal_2 Loss: 0.0690 | 0.0846
Epoch 145/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 146/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 147/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 148/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 149/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 150/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 151/300, seasonal_2 Loss: 0.0689 | 0.0845
Epoch 152/300, seasonal_2 Loss: 0.0688 | 0.0845
Epoch 153/300, seasonal_2 Loss: 0.0688 | 0.0845
Epoch 154/300, seasonal_2 Loss: 0.0688 | 0.0844
Epoch 155/300, seasonal_2 Loss: 0.0688 | 0.0844
Epoch 156/300, seasonal_2 Loss: 0.0688 | 0.0844
Epoch 157/300, seasonal_2 Loss: 0.0688 | 0.0844
Epoch 158/300, seasonal_2 Loss: 0.0688 | 0.0844
Epoch 159/300, seasonal_2 Loss: 0.0688 | 0.0844
Epoch 160/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 161/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 162/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 163/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 164/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 165/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 166/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 167/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 168/300, seasonal_2 Loss: 0.0687 | 0.0844
Epoch 169/300, seasonal_2 Loss: 0.0687 | 0.0843
Epoch 170/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 171/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 172/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 173/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 174/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 175/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 176/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 177/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 178/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 179/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 180/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 181/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 182/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 183/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 184/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 185/300, seasonal_2 Loss: 0.0686 | 0.0843
Epoch 186/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 187/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 188/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 189/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 190/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 191/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 192/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 193/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 194/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 195/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 196/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 197/300, seasonal_2 Loss: 0.0685 | 0.0843
Epoch 198/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 199/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 200/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 201/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 202/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 203/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 204/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 205/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 206/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 207/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 208/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 209/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 210/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 211/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 212/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 213/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 214/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 215/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 216/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 217/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 218/300, seasonal_2 Loss: 0.0685 | 0.0842
Epoch 219/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 220/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 221/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 222/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 223/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 224/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 225/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 226/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 227/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 228/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 229/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 230/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 231/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 232/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 233/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 234/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 235/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 236/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 237/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 238/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 239/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 240/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 241/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 242/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 243/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 244/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 245/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 246/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 247/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 248/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 249/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 250/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 251/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 252/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 253/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 254/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 255/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 256/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 257/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 258/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 259/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 260/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 261/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 262/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 263/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 264/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 265/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 266/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 267/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 268/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 269/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 270/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 271/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 272/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 273/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 274/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 275/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 276/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 277/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 278/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 279/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 280/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 281/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 282/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 283/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 284/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 285/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 286/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 287/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 288/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 289/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 290/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 291/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 292/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 293/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 294/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 295/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 296/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 297/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 298/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 299/300, seasonal_2 Loss: 0.0684 | 0.0842
Epoch 300/300, seasonal_2 Loss: 0.0684 | 0.0842
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.631694572051324, 'learning_rate': 0.000588946841599191, 'batch_size': 92, 'step_size': 8, 'gamma': 0.7741042729215036}
Epoch 1/300, seasonal_3 Loss: 0.3562 | 0.1669
Epoch 2/300, seasonal_3 Loss: 0.1752 | 0.1636
Epoch 3/300, seasonal_3 Loss: 0.1514 | 0.1606
Epoch 4/300, seasonal_3 Loss: 0.1335 | 0.1448
Epoch 5/300, seasonal_3 Loss: 0.1246 | 0.1423
Epoch 6/300, seasonal_3 Loss: 0.1215 | 0.1275
Epoch 7/300, seasonal_3 Loss: 0.1209 | 0.1610
Epoch 8/300, seasonal_3 Loss: 0.1272 | 0.1281
Epoch 9/300, seasonal_3 Loss: 0.1369 | 0.1126
Epoch 10/300, seasonal_3 Loss: 0.1191 | 0.1267
Epoch 11/300, seasonal_3 Loss: 0.1120 | 0.1081
Epoch 12/300, seasonal_3 Loss: 0.1076 | 0.0952
Epoch 13/300, seasonal_3 Loss: 0.1063 | 0.0900
Epoch 14/300, seasonal_3 Loss: 0.1049 | 0.0878
Epoch 15/300, seasonal_3 Loss: 0.1026 | 0.0829
Epoch 16/300, seasonal_3 Loss: 0.1014 | 0.0808
Epoch 17/300, seasonal_3 Loss: 0.1002 | 0.0785
Epoch 18/300, seasonal_3 Loss: 0.0992 | 0.0770
Epoch 19/300, seasonal_3 Loss: 0.0983 | 0.0757
Epoch 20/300, seasonal_3 Loss: 0.0976 | 0.0749
Epoch 21/300, seasonal_3 Loss: 0.0970 | 0.0739
Epoch 22/300, seasonal_3 Loss: 0.0968 | 0.0736
Epoch 23/300, seasonal_3 Loss: 0.0968 | 0.0731
Epoch 24/300, seasonal_3 Loss: 0.0960 | 0.0724
Epoch 25/300, seasonal_3 Loss: 0.0952 | 0.0722
Epoch 26/300, seasonal_3 Loss: 0.0947 | 0.0724
Epoch 27/300, seasonal_3 Loss: 0.0945 | 0.0716
Epoch 28/300, seasonal_3 Loss: 0.0942 | 0.0721
Epoch 29/300, seasonal_3 Loss: 0.0942 | 0.0711
Epoch 30/300, seasonal_3 Loss: 0.0938 | 0.0714
Epoch 31/300, seasonal_3 Loss: 0.0935 | 0.0707
Epoch 32/300, seasonal_3 Loss: 0.0933 | 0.0719
Epoch 33/300, seasonal_3 Loss: 0.0932 | 0.0703
Epoch 34/300, seasonal_3 Loss: 0.0929 | 0.0708
Epoch 35/300, seasonal_3 Loss: 0.0930 | 0.0699
Epoch 36/300, seasonal_3 Loss: 0.0928 | 0.0708
Epoch 37/300, seasonal_3 Loss: 0.0927 | 0.0701
Epoch 38/300, seasonal_3 Loss: 0.0923 | 0.0705
Epoch 39/300, seasonal_3 Loss: 0.0921 | 0.0697
Epoch 40/300, seasonal_3 Loss: 0.0920 | 0.0703
Epoch 41/300, seasonal_3 Loss: 0.0916 | 0.0702
Epoch 42/300, seasonal_3 Loss: 0.0915 | 0.0700
Epoch 43/300, seasonal_3 Loss: 0.0911 | 0.0697
Epoch 44/300, seasonal_3 Loss: 0.0910 | 0.0696
Epoch 45/300, seasonal_3 Loss: 0.0907 | 0.0697
Epoch 46/300, seasonal_3 Loss: 0.0908 | 0.0696
Epoch 47/300, seasonal_3 Loss: 0.0906 | 0.0695
Epoch 48/300, seasonal_3 Loss: 0.0905 | 0.0694
Epoch 49/300, seasonal_3 Loss: 0.0904 | 0.0694
Epoch 50/300, seasonal_3 Loss: 0.0905 | 0.0693
Epoch 51/300, seasonal_3 Loss: 0.0906 | 0.0694
Epoch 52/300, seasonal_3 Loss: 0.0905 | 0.0694
Epoch 53/300, seasonal_3 Loss: 0.0904 | 0.0696
Epoch 54/300, seasonal_3 Loss: 0.0905 | 0.0696
Epoch 55/300, seasonal_3 Loss: 0.0903 | 0.0694
Epoch 56/300, seasonal_3 Loss: 0.0901 | 0.0692
Epoch 57/300, seasonal_3 Loss: 0.0899 | 0.0691
Epoch 58/300, seasonal_3 Loss: 0.0896 | 0.0689
Epoch 59/300, seasonal_3 Loss: 0.0895 | 0.0689
Epoch 60/300, seasonal_3 Loss: 0.0894 | 0.0689
Epoch 61/300, seasonal_3 Loss: 0.0893 | 0.0688
Epoch 62/300, seasonal_3 Loss: 0.0892 | 0.0688
Epoch 63/300, seasonal_3 Loss: 0.0892 | 0.0688
Epoch 64/300, seasonal_3 Loss: 0.0891 | 0.0687
Epoch 65/300, seasonal_3 Loss: 0.0891 | 0.0687
Epoch 66/300, seasonal_3 Loss: 0.0890 | 0.0687
Epoch 67/300, seasonal_3 Loss: 0.0890 | 0.0687
Epoch 68/300, seasonal_3 Loss: 0.0890 | 0.0686
Epoch 69/300, seasonal_3 Loss: 0.0889 | 0.0686
Epoch 70/300, seasonal_3 Loss: 0.0889 | 0.0686
Epoch 71/300, seasonal_3 Loss: 0.0889 | 0.0686
Epoch 72/300, seasonal_3 Loss: 0.0888 | 0.0686
Epoch 73/300, seasonal_3 Loss: 0.0888 | 0.0685
Epoch 74/300, seasonal_3 Loss: 0.0888 | 0.0685
Epoch 75/300, seasonal_3 Loss: 0.0887 | 0.0685
Epoch 76/300, seasonal_3 Loss: 0.0887 | 0.0685
Epoch 77/300, seasonal_3 Loss: 0.0887 | 0.0685
Epoch 78/300, seasonal_3 Loss: 0.0887 | 0.0685
Epoch 79/300, seasonal_3 Loss: 0.0887 | 0.0685
Epoch 80/300, seasonal_3 Loss: 0.0886 | 0.0685
Epoch 81/300, seasonal_3 Loss: 0.0886 | 0.0685
Epoch 82/300, seasonal_3 Loss: 0.0886 | 0.0684
Epoch 83/300, seasonal_3 Loss: 0.0886 | 0.0684
Epoch 84/300, seasonal_3 Loss: 0.0886 | 0.0684
Epoch 85/300, seasonal_3 Loss: 0.0886 | 0.0684
Epoch 86/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 87/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 88/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 89/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 90/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 91/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 92/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 93/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 94/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 95/300, seasonal_3 Loss: 0.0885 | 0.0684
Epoch 96/300, seasonal_3 Loss: 0.0884 | 0.0684
Epoch 97/300, seasonal_3 Loss: 0.0884 | 0.0684
Epoch 98/300, seasonal_3 Loss: 0.0884 | 0.0684
Epoch 99/300, seasonal_3 Loss: 0.0884 | 0.0684
Epoch 100/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 101/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 102/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 103/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 104/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 105/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 106/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 107/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 108/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 109/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 110/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 111/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 112/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 113/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 114/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 115/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 116/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 117/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 118/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 119/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 120/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 121/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 122/300, seasonal_3 Loss: 0.0884 | 0.0683
Epoch 123/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 124/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 125/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 126/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 127/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 128/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 129/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 130/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 131/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 132/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 133/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 134/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 135/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 136/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 137/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 138/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 139/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 140/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 141/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 142/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 143/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 144/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 145/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 146/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 147/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 148/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 149/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 150/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 151/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 152/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 153/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 154/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 155/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 156/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 157/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 158/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 159/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 160/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 161/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 162/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 163/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 164/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 165/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 166/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 167/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 168/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 169/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 170/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 171/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 172/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 173/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 174/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 175/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 176/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 177/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 178/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 179/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 180/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 181/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 182/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 183/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 184/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 185/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 186/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 187/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 188/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 189/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 190/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 191/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 192/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 193/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 194/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 195/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 196/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 197/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 198/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 199/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 200/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 201/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 202/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 203/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 204/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 205/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 206/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 207/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 208/300, seasonal_3 Loss: 0.0883 | 0.0683
Epoch 209/300, seasonal_3 Loss: 0.0883 | 0.0683
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.7701960213765681, 'learning_rate': 6.373835854834866e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8454534499631499}
Epoch 1/300, resid Loss: 0.3133 | 0.1769
Epoch 2/300, resid Loss: 0.1446 | 0.1568
Epoch 3/300, resid Loss: 0.1329 | 0.1415
Epoch 4/300, resid Loss: 0.1241 | 0.1286
Epoch 5/300, resid Loss: 0.1175 | 0.1194
Epoch 6/300, resid Loss: 0.1132 | 0.1136
Epoch 7/300, resid Loss: 0.1103 | 0.1103
Epoch 8/300, resid Loss: 0.1079 | 0.1073
Epoch 9/300, resid Loss: 0.1058 | 0.1046
Epoch 10/300, resid Loss: 0.1038 | 0.1020
Epoch 11/300, resid Loss: 0.1017 | 0.1007
Epoch 12/300, resid Loss: 0.0998 | 0.0984
Epoch 13/300, resid Loss: 0.0980 | 0.0963
Epoch 14/300, resid Loss: 0.0961 | 0.0942
Epoch 15/300, resid Loss: 0.0943 | 0.0923
Epoch 16/300, resid Loss: 0.0925 | 0.0911
Epoch 17/300, resid Loss: 0.0910 | 0.0898
Epoch 18/300, resid Loss: 0.0897 | 0.0888
Epoch 19/300, resid Loss: 0.0886 | 0.0880
Epoch 20/300, resid Loss: 0.0876 | 0.0873
Epoch 21/300, resid Loss: 0.0866 | 0.0867
Epoch 22/300, resid Loss: 0.0858 | 0.0863
Epoch 23/300, resid Loss: 0.0851 | 0.0859
Epoch 24/300, resid Loss: 0.0845 | 0.0855
Epoch 25/300, resid Loss: 0.0839 | 0.0852
Epoch 26/300, resid Loss: 0.0832 | 0.0847
Epoch 27/300, resid Loss: 0.0826 | 0.0844
Epoch 28/300, resid Loss: 0.0822 | 0.0841
Epoch 29/300, resid Loss: 0.0817 | 0.0838
Epoch 30/300, resid Loss: 0.0812 | 0.0834
Epoch 31/300, resid Loss: 0.0805 | 0.0829
Epoch 32/300, resid Loss: 0.0801 | 0.0826
Epoch 33/300, resid Loss: 0.0796 | 0.0822
Epoch 34/300, resid Loss: 0.0790 | 0.0818
Epoch 35/300, resid Loss: 0.0785 | 0.0814
Epoch 36/300, resid Loss: 0.0778 | 0.0806
Epoch 37/300, resid Loss: 0.0773 | 0.0802
Epoch 38/300, resid Loss: 0.0768 | 0.0798
Epoch 39/300, resid Loss: 0.0764 | 0.0795
Epoch 40/300, resid Loss: 0.0761 | 0.0792
Epoch 41/300, resid Loss: 0.0756 | 0.0787
Epoch 42/300, resid Loss: 0.0754 | 0.0785
Epoch 43/300, resid Loss: 0.0752 | 0.0784
Epoch 44/300, resid Loss: 0.0750 | 0.0782
Epoch 45/300, resid Loss: 0.0748 | 0.0781
Epoch 46/300, resid Loss: 0.0746 | 0.0779
Epoch 47/300, resid Loss: 0.0744 | 0.0777
Epoch 48/300, resid Loss: 0.0743 | 0.0776
Epoch 49/300, resid Loss: 0.0741 | 0.0775
Epoch 50/300, resid Loss: 0.0739 | 0.0774
Epoch 51/300, resid Loss: 0.0737 | 0.0772
Epoch 52/300, resid Loss: 0.0736 | 0.0771
Epoch 53/300, resid Loss: 0.0735 | 0.0771
Epoch 54/300, resid Loss: 0.0733 | 0.0770
Epoch 55/300, resid Loss: 0.0732 | 0.0769
Epoch 56/300, resid Loss: 0.0730 | 0.0768
Epoch 57/300, resid Loss: 0.0729 | 0.0767
Epoch 58/300, resid Loss: 0.0728 | 0.0766
Epoch 59/300, resid Loss: 0.0727 | 0.0765
Epoch 60/300, resid Loss: 0.0726 | 0.0764
Epoch 61/300, resid Loss: 0.0724 | 0.0764
Epoch 62/300, resid Loss: 0.0723 | 0.0763
Epoch 63/300, resid Loss: 0.0722 | 0.0762
Epoch 64/300, resid Loss: 0.0721 | 0.0761
Epoch 65/300, resid Loss: 0.0720 | 0.0761
Epoch 66/300, resid Loss: 0.0719 | 0.0760
Epoch 67/300, resid Loss: 0.0718 | 0.0759
Epoch 68/300, resid Loss: 0.0717 | 0.0759
Epoch 69/300, resid Loss: 0.0716 | 0.0758
Epoch 70/300, resid Loss: 0.0715 | 0.0757
Epoch 71/300, resid Loss: 0.0714 | 0.0757
Epoch 72/300, resid Loss: 0.0713 | 0.0756
Epoch 73/300, resid Loss: 0.0712 | 0.0756
Epoch 74/300, resid Loss: 0.0711 | 0.0755
Epoch 75/300, resid Loss: 0.0711 | 0.0754
Epoch 76/300, resid Loss: 0.0710 | 0.0754
Epoch 77/300, resid Loss: 0.0709 | 0.0754
Epoch 78/300, resid Loss: 0.0708 | 0.0753
Epoch 79/300, resid Loss: 0.0708 | 0.0753
Epoch 80/300, resid Loss: 0.0707 | 0.0752
Epoch 81/300, resid Loss: 0.0706 | 0.0752
Epoch 82/300, resid Loss: 0.0706 | 0.0752
Epoch 83/300, resid Loss: 0.0705 | 0.0752
Epoch 84/300, resid Loss: 0.0705 | 0.0751
Epoch 85/300, resid Loss: 0.0704 | 0.0751
Epoch 86/300, resid Loss: 0.0704 | 0.0751
Epoch 87/300, resid Loss: 0.0703 | 0.0750
Epoch 88/300, resid Loss: 0.0703 | 0.0750
Epoch 89/300, resid Loss: 0.0702 | 0.0750
Epoch 90/300, resid Loss: 0.0702 | 0.0749
Epoch 91/300, resid Loss: 0.0702 | 0.0750
Epoch 92/300, resid Loss: 0.0701 | 0.0749
Epoch 93/300, resid Loss: 0.0701 | 0.0749
Epoch 94/300, resid Loss: 0.0701 | 0.0749
Epoch 95/300, resid Loss: 0.0700 | 0.0748
Epoch 96/300, resid Loss: 0.0700 | 0.0748
Epoch 97/300, resid Loss: 0.0700 | 0.0748
Epoch 98/300, resid Loss: 0.0700 | 0.0748
Epoch 99/300, resid Loss: 0.0699 | 0.0748
Epoch 100/300, resid Loss: 0.0699 | 0.0748
Epoch 101/300, resid Loss: 0.0699 | 0.0748
Epoch 102/300, resid Loss: 0.0699 | 0.0748
Epoch 103/300, resid Loss: 0.0698 | 0.0748
Epoch 104/300, resid Loss: 0.0698 | 0.0748
Epoch 105/300, resid Loss: 0.0698 | 0.0747
Epoch 106/300, resid Loss: 0.0698 | 0.0748
Epoch 107/300, resid Loss: 0.0697 | 0.0748
Epoch 108/300, resid Loss: 0.0697 | 0.0748
Epoch 109/300, resid Loss: 0.0697 | 0.0748
Epoch 110/300, resid Loss: 0.0697 | 0.0748
Epoch 111/300, resid Loss: 0.0697 | 0.0748
Epoch 112/300, resid Loss: 0.0696 | 0.0748
Epoch 113/300, resid Loss: 0.0696 | 0.0748
Epoch 114/300, resid Loss: 0.0696 | 0.0748
Epoch 115/300, resid Loss: 0.0696 | 0.0748
Epoch 116/300, resid Loss: 0.0696 | 0.0748
Epoch 117/300, resid Loss: 0.0696 | 0.0748
Epoch 118/300, resid Loss: 0.0696 | 0.0748
Epoch 119/300, resid Loss: 0.0695 | 0.0748
Epoch 120/300, resid Loss: 0.0695 | 0.0748
Epoch 121/300, resid Loss: 0.0695 | 0.0748
Epoch 122/300, resid Loss: 0.0695 | 0.0748
Epoch 123/300, resid Loss: 0.0695 | 0.0748
Epoch 124/300, resid Loss: 0.0695 | 0.0748
Epoch 125/300, resid Loss: 0.0695 | 0.0748
Epoch 126/300, resid Loss: 0.0694 | 0.0748
Epoch 127/300, resid Loss: 0.0694 | 0.0748
Epoch 128/300, resid Loss: 0.0694 | 0.0748
Epoch 129/300, resid Loss: 0.0694 | 0.0748
Epoch 130/300, resid Loss: 0.0694 | 0.0748
Epoch 131/300, resid Loss: 0.0694 | 0.0748
Epoch 132/300, resid Loss: 0.0694 | 0.0748
Epoch 133/300, resid Loss: 0.0694 | 0.0748
Epoch 134/300, resid Loss: 0.0694 | 0.0748
Epoch 135/300, resid Loss: 0.0694 | 0.0748
Epoch 136/300, resid Loss: 0.0693 | 0.0748
Epoch 137/300, resid Loss: 0.0693 | 0.0748
Epoch 138/300, resid Loss: 0.0693 | 0.0748
Epoch 139/300, resid Loss: 0.0693 | 0.0748
Epoch 140/300, resid Loss: 0.0693 | 0.0748
Epoch 141/300, resid Loss: 0.0693 | 0.0748
Epoch 142/300, resid Loss: 0.0693 | 0.0748
Epoch 143/300, resid Loss: 0.0693 | 0.0748
Epoch 144/300, resid Loss: 0.0693 | 0.0748
Epoch 145/300, resid Loss: 0.0693 | 0.0748
Epoch 146/300, resid Loss: 0.0693 | 0.0748
Epoch 147/300, resid Loss: 0.0693 | 0.0748
Epoch 148/300, resid Loss: 0.0692 | 0.0748
Epoch 149/300, resid Loss: 0.0692 | 0.0748
Epoch 150/300, resid Loss: 0.0692 | 0.0748
Epoch 151/300, resid Loss: 0.0692 | 0.0748
Epoch 152/300, resid Loss: 0.0692 | 0.0748
Epoch 153/300, resid Loss: 0.0692 | 0.0748
Epoch 154/300, resid Loss: 0.0692 | 0.0748
Epoch 155/300, resid Loss: 0.0692 | 0.0748
Epoch 156/300, resid Loss: 0.0692 | 0.0747
Epoch 157/300, resid Loss: 0.0692 | 0.0747
Epoch 158/300, resid Loss: 0.0692 | 0.0747
Epoch 159/300, resid Loss: 0.0692 | 0.0747
Epoch 160/300, resid Loss: 0.0692 | 0.0747
Epoch 161/300, resid Loss: 0.0692 | 0.0747
Epoch 162/300, resid Loss: 0.0692 | 0.0747
Epoch 163/300, resid Loss: 0.0692 | 0.0747
Epoch 164/300, resid Loss: 0.0692 | 0.0747
Epoch 165/300, resid Loss: 0.0692 | 0.0747
Epoch 166/300, resid Loss: 0.0692 | 0.0747
Epoch 167/300, resid Loss: 0.0692 | 0.0747
Epoch 168/300, resid Loss: 0.0692 | 0.0747
Epoch 169/300, resid Loss: 0.0692 | 0.0747
Epoch 170/300, resid Loss: 0.0692 | 0.0747
Epoch 171/300, resid Loss: 0.0692 | 0.0747
Epoch 172/300, resid Loss: 0.0691 | 0.0747
Epoch 173/300, resid Loss: 0.0691 | 0.0747
Epoch 174/300, resid Loss: 0.0691 | 0.0747
Epoch 175/300, resid Loss: 0.0691 | 0.0747
Epoch 176/300, resid Loss: 0.0691 | 0.0747
Epoch 177/300, resid Loss: 0.0691 | 0.0747
Epoch 178/300, resid Loss: 0.0691 | 0.0747
Epoch 179/300, resid Loss: 0.0691 | 0.0747
Epoch 180/300, resid Loss: 0.0691 | 0.0747
Epoch 181/300, resid Loss: 0.0691 | 0.0747
Epoch 182/300, resid Loss: 0.0691 | 0.0746
Epoch 183/300, resid Loss: 0.0691 | 0.0746
Epoch 184/300, resid Loss: 0.0691 | 0.0746
Epoch 185/300, resid Loss: 0.0691 | 0.0746
Epoch 186/300, resid Loss: 0.0691 | 0.0746
Epoch 187/300, resid Loss: 0.0691 | 0.0746
Epoch 188/300, resid Loss: 0.0691 | 0.0746
Epoch 189/300, resid Loss: 0.0691 | 0.0746
Epoch 190/300, resid Loss: 0.0691 | 0.0746
Epoch 191/300, resid Loss: 0.0691 | 0.0746
Epoch 192/300, resid Loss: 0.0691 | 0.0746
Epoch 193/300, resid Loss: 0.0691 | 0.0746
Epoch 194/300, resid Loss: 0.0691 | 0.0746
Epoch 195/300, resid Loss: 0.0691 | 0.0746
Epoch 196/300, resid Loss: 0.0691 | 0.0746
Epoch 197/300, resid Loss: 0.0691 | 0.0746
Epoch 198/300, resid Loss: 0.0691 | 0.0746
Epoch 199/300, resid Loss: 0.0691 | 0.0746
Epoch 200/300, resid Loss: 0.0691 | 0.0746
Epoch 201/300, resid Loss: 0.0691 | 0.0746
Epoch 202/300, resid Loss: 0.0691 | 0.0746
Epoch 203/300, resid Loss: 0.0691 | 0.0746
Epoch 204/300, resid Loss: 0.0691 | 0.0746
Epoch 205/300, resid Loss: 0.0691 | 0.0746
Epoch 206/300, resid Loss: 0.0691 | 0.0746
Epoch 207/300, resid Loss: 0.0691 | 0.0746
Epoch 208/300, resid Loss: 0.0691 | 0.0746
Epoch 209/300, resid Loss: 0.0691 | 0.0746
Epoch 210/300, resid Loss: 0.0691 | 0.0746
Epoch 211/300, resid Loss: 0.0691 | 0.0746
Epoch 212/300, resid Loss: 0.0691 | 0.0746
Epoch 213/300, resid Loss: 0.0691 | 0.0746
Epoch 214/300, resid Loss: 0.0691 | 0.0746
Epoch 215/300, resid Loss: 0.0691 | 0.0746
Epoch 216/300, resid Loss: 0.0691 | 0.0746
Epoch 217/300, resid Loss: 0.0691 | 0.0746
Epoch 218/300, resid Loss: 0.0691 | 0.0746
Epoch 219/300, resid Loss: 0.0691 | 0.0746
Epoch 220/300, resid Loss: 0.0691 | 0.0746
Epoch 221/300, resid Loss: 0.0691 | 0.0746
Epoch 222/300, resid Loss: 0.0691 | 0.0746
Epoch 223/300, resid Loss: 0.0691 | 0.0746
Epoch 224/300, resid Loss: 0.0691 | 0.0746
Epoch 225/300, resid Loss: 0.0691 | 0.0746
Epoch 226/300, resid Loss: 0.0691 | 0.0746
Epoch 227/300, resid Loss: 0.0691 | 0.0746
Epoch 228/300, resid Loss: 0.0691 | 0.0746
Epoch 229/300, resid Loss: 0.0691 | 0.0746
Epoch 230/300, resid Loss: 0.0691 | 0.0746
Epoch 231/300, resid Loss: 0.0691 | 0.0746
Epoch 232/300, resid Loss: 0.0691 | 0.0746
Epoch 233/300, resid Loss: 0.0691 | 0.0746
Epoch 234/300, resid Loss: 0.0691 | 0.0746
Epoch 235/300, resid Loss: 0.0691 | 0.0746
Epoch 236/300, resid Loss: 0.0691 | 0.0746
Epoch 237/300, resid Loss: 0.0691 | 0.0746
Epoch 238/300, resid Loss: 0.0691 | 0.0746
Epoch 239/300, resid Loss: 0.0691 | 0.0746
Epoch 240/300, resid Loss: 0.0691 | 0.0746
Epoch 241/300, resid Loss: 0.0691 | 0.0746
Epoch 242/300, resid Loss: 0.0691 | 0.0746
Epoch 243/300, resid Loss: 0.0691 | 0.0746
Epoch 244/300, resid Loss: 0.0691 | 0.0746
Epoch 245/300, resid Loss: 0.0691 | 0.0746
Epoch 246/300, resid Loss: 0.0691 | 0.0746
Epoch 247/300, resid Loss: 0.0691 | 0.0746
Epoch 248/300, resid Loss: 0.0691 | 0.0746
Epoch 249/300, resid Loss: 0.0691 | 0.0746
Epoch 250/300, resid Loss: 0.0691 | 0.0746
Epoch 251/300, resid Loss: 0.0691 | 0.0746
Epoch 252/300, resid Loss: 0.0691 | 0.0746
Epoch 253/300, resid Loss: 0.0691 | 0.0746
Epoch 254/300, resid Loss: 0.0691 | 0.0746
Epoch 255/300, resid Loss: 0.0691 | 0.0746
Epoch 256/300, resid Loss: 0.0691 | 0.0746
Epoch 257/300, resid Loss: 0.0691 | 0.0746
Epoch 258/300, resid Loss: 0.0691 | 0.0746
Epoch 259/300, resid Loss: 0.0691 | 0.0746
Epoch 260/300, resid Loss: 0.0691 | 0.0746
Epoch 261/300, resid Loss: 0.0691 | 0.0746
Epoch 262/300, resid Loss: 0.0691 | 0.0746
Epoch 263/300, resid Loss: 0.0691 | 0.0746
Epoch 264/300, resid Loss: 0.0691 | 0.0746
Epoch 265/300, resid Loss: 0.0691 | 0.0746
Epoch 266/300, resid Loss: 0.0691 | 0.0746
Epoch 267/300, resid Loss: 0.0691 | 0.0746
Epoch 268/300, resid Loss: 0.0691 | 0.0746
Epoch 269/300, resid Loss: 0.0691 | 0.0746
Epoch 270/300, resid Loss: 0.0691 | 0.0746
Epoch 271/300, resid Loss: 0.0691 | 0.0746
Epoch 272/300, resid Loss: 0.0691 | 0.0746
Epoch 273/300, resid Loss: 0.0691 | 0.0746
Epoch 274/300, resid Loss: 0.0691 | 0.0746
Epoch 275/300, resid Loss: 0.0691 | 0.0746
Epoch 276/300, resid Loss: 0.0691 | 0.0746
Epoch 277/300, resid Loss: 0.0691 | 0.0746
Epoch 278/300, resid Loss: 0.0691 | 0.0746
Epoch 279/300, resid Loss: 0.0691 | 0.0746
Epoch 280/300, resid Loss: 0.0691 | 0.0746
Epoch 281/300, resid Loss: 0.0691 | 0.0746
Epoch 282/300, resid Loss: 0.0691 | 0.0746
Epoch 283/300, resid Loss: 0.0691 | 0.0746
Epoch 284/300, resid Loss: 0.0691 | 0.0746
Epoch 285/300, resid Loss: 0.0691 | 0.0746
Epoch 286/300, resid Loss: 0.0691 | 0.0746
Epoch 287/300, resid Loss: 0.0691 | 0.0746
Epoch 288/300, resid Loss: 0.0691 | 0.0746
Epoch 289/300, resid Loss: 0.0691 | 0.0746
Epoch 290/300, resid Loss: 0.0691 | 0.0746
Epoch 291/300, resid Loss: 0.0691 | 0.0746
Epoch 292/300, resid Loss: 0.0691 | 0.0746
Epoch 293/300, resid Loss: 0.0691 | 0.0746
Epoch 294/300, resid Loss: 0.0691 | 0.0746
Epoch 295/300, resid Loss: 0.0691 | 0.0746
Epoch 296/300, resid Loss: 0.0691 | 0.0746
Epoch 297/300, resid Loss: 0.0691 | 0.0746
Epoch 298/300, resid Loss: 0.0691 | 0.0746
Epoch 299/300, resid Loss: 0.0691 | 0.0746
Epoch 300/300, resid Loss: 0.0691 | 0.0746
Runtime (seconds): 2340.809276819229
0.00018337655424356704
[102.57353]
[4.9442925]
[-1.2110839]
[0.8530185]
[-3.4320273]
[-1.9907249]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1.117263451218605
RMSE: 1.0570068359375
MAE: 1.0570068359375
R-squared: nan
[101.73701]
