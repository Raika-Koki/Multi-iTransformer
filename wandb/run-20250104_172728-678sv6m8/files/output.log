[32m[I 2025-01-04 17:27:29,592][0m A new study created in memory with name: no-name-491488f3-14ea-4ad0-9a0b-3a98f063dfda[0m
[32m[I 2025-01-04 17:28:14,249][0m Trial 0 finished with value: 0.19458337169269035 and parameters: {'observation_period_num': 10, 'train_rates': 0.704447646792244, 'learning_rate': 1.7660323260077306e-05, 'batch_size': 212, 'step_size': 8, 'gamma': 0.9506628212406208}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:29:08,024][0m Trial 1 finished with value: 0.2221703190232119 and parameters: {'observation_period_num': 53, 'train_rates': 0.7831745832136859, 'learning_rate': 2.9786781117591404e-05, 'batch_size': 142, 'step_size': 7, 'gamma': 0.9765102895278571}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:30:12,475][0m Trial 2 finished with value: 0.4132658664777153 and parameters: {'observation_period_num': 245, 'train_rates': 0.6646717548148746, 'learning_rate': 5.273014490320064e-05, 'batch_size': 101, 'step_size': 3, 'gamma': 0.7992272222307952}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:31:01,834][0m Trial 3 finished with value: 0.27130775399125584 and parameters: {'observation_period_num': 100, 'train_rates': 0.7874270960963667, 'learning_rate': 1.3602252155084664e-05, 'batch_size': 251, 'step_size': 15, 'gamma': 0.9115666279937031}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:31:51,261][0m Trial 4 finished with value: 0.27144457495036817 and parameters: {'observation_period_num': 155, 'train_rates': 0.7089475354696841, 'learning_rate': 0.00012388644521498833, 'batch_size': 200, 'step_size': 3, 'gamma': 0.9244321470735539}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:32:39,789][0m Trial 5 finished with value: 0.7145424801009622 and parameters: {'observation_period_num': 149, 'train_rates': 0.7490829777761969, 'learning_rate': 9.656325678829252e-06, 'batch_size': 178, 'step_size': 5, 'gamma': 0.8364592189576519}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:33:35,094][0m Trial 6 finished with value: 0.4468979446445023 and parameters: {'observation_period_num': 168, 'train_rates': 0.712002015846537, 'learning_rate': 1.2845343278595645e-05, 'batch_size': 198, 'step_size': 14, 'gamma': 0.7637582868899485}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:34:27,911][0m Trial 7 finished with value: 0.37722615864120584 and parameters: {'observation_period_num': 214, 'train_rates': 0.6269074339707045, 'learning_rate': 0.00010234042923800338, 'batch_size': 131, 'step_size': 10, 'gamma': 0.795922449178408}. Best is trial 0 with value: 0.19458337169269035.[0m
[32m[I 2025-01-04 17:35:23,028][0m Trial 8 finished with value: 0.0637487613988158 and parameters: {'observation_period_num': 36, 'train_rates': 0.791479975095098, 'learning_rate': 0.0005357354730950551, 'batch_size': 226, 'step_size': 7, 'gamma': 0.9221746043920894}. Best is trial 8 with value: 0.0637487613988158.[0m
[32m[I 2025-01-04 17:36:20,560][0m Trial 9 finished with value: 0.1385711279626076 and parameters: {'observation_period_num': 233, 'train_rates': 0.9236031316105275, 'learning_rate': 0.00014092005568337645, 'batch_size': 201, 'step_size': 3, 'gamma': 0.8761072301277165}. Best is trial 8 with value: 0.0637487613988158.[0m
[32m[I 2025-01-04 17:39:33,139][0m Trial 10 finished with value: 0.10233635965026229 and parameters: {'observation_period_num': 68, 'train_rates': 0.8870661134941287, 'learning_rate': 0.0009064782362931275, 'batch_size': 22, 'step_size': 12, 'gamma': 0.8773679676990762}. Best is trial 8 with value: 0.0637487613988158.[0m
[32m[I 2025-01-04 17:41:50,824][0m Trial 11 finished with value: 0.0977906970372207 and parameters: {'observation_period_num': 75, 'train_rates': 0.8805551468184046, 'learning_rate': 0.0009024239290772411, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8649491371977}. Best is trial 8 with value: 0.0637487613988158.[0m
[32m[I 2025-01-04 17:45:25,911][0m Trial 12 finished with value: 0.036212689031592825 and parameters: {'observation_period_num': 8, 'train_rates': 0.8581384137597103, 'learning_rate': 0.0008082931030390402, 'batch_size': 19, 'step_size': 11, 'gamma': 0.912540967533718}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:46:33,528][0m Trial 13 finished with value: 0.1924924999475479 and parameters: {'observation_period_num': 14, 'train_rates': 0.97987496087198, 'learning_rate': 1.2491159829623095e-06, 'batch_size': 71, 'step_size': 6, 'gamma': 0.9246674520651834}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:47:10,271][0m Trial 14 finished with value: 0.0800030714283321 and parameters: {'observation_period_num': 35, 'train_rates': 0.8449874916401862, 'learning_rate': 0.0003167583262130953, 'batch_size': 247, 'step_size': 9, 'gamma': 0.9899518753463381}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:48:14,648][0m Trial 15 finished with value: 0.12958612751262458 and parameters: {'observation_period_num': 104, 'train_rates': 0.8321398897781681, 'learning_rate': 0.0003677140549351749, 'batch_size': 64, 'step_size': 13, 'gamma': 0.8968125402894217}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:48:53,752][0m Trial 16 finished with value: 0.04740449507365522 and parameters: {'observation_period_num': 33, 'train_rates': 0.8338360615705165, 'learning_rate': 0.00037682440766624277, 'batch_size': 157, 'step_size': 1, 'gamma': 0.9556739798624193}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:49:34,426][0m Trial 17 finished with value: 0.07961060851812363 and parameters: {'observation_period_num': 96, 'train_rates': 0.9589516625477091, 'learning_rate': 0.00023308818265574226, 'batch_size': 155, 'step_size': 1, 'gamma': 0.9579296325794378}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:50:29,892][0m Trial 18 finished with value: 0.19143849181015274 and parameters: {'observation_period_num': 8, 'train_rates': 0.8401338067722774, 'learning_rate': 2.8500054441079e-06, 'batch_size': 112, 'step_size': 1, 'gamma': 0.9507266358694301}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:51:29,018][0m Trial 19 finished with value: 0.07217166757099198 and parameters: {'observation_period_num': 37, 'train_rates': 0.903167399084379, 'learning_rate': 5.2200812251310484e-05, 'batch_size': 167, 'step_size': 10, 'gamma': 0.8444628306258561}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:52:54,856][0m Trial 20 finished with value: 0.09953051914150517 and parameters: {'observation_period_num': 186, 'train_rates': 0.8266088930205113, 'learning_rate': 0.0005519152266584889, 'batch_size': 82, 'step_size': 5, 'gamma': 0.8975145002741833}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:53:49,544][0m Trial 21 finished with value: 0.18697208883885288 and parameters: {'observation_period_num': 40, 'train_rates': 0.7470331872786157, 'learning_rate': 0.00049840367440873, 'batch_size': 238, 'step_size': 8, 'gamma': 0.9391971593116605}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:55:02,415][0m Trial 22 finished with value: 0.07441804609170147 and parameters: {'observation_period_num': 70, 'train_rates': 0.8054992632942338, 'learning_rate': 0.00017783194198934808, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9712114649018799}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:55:43,427][0m Trial 23 finished with value: 0.17326578180726301 and parameters: {'observation_period_num': 27, 'train_rates': 0.7632126216467322, 'learning_rate': 0.0006186323648252125, 'batch_size': 118, 'step_size': 10, 'gamma': 0.9290931278093765}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:56:37,588][0m Trial 24 finished with value: 0.1131164500195729 and parameters: {'observation_period_num': 123, 'train_rates': 0.8658791612342646, 'learning_rate': 0.0003203931413424479, 'batch_size': 225, 'step_size': 5, 'gamma': 0.9015453737201526}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:57:27,209][0m Trial 25 finished with value: 0.05947160720825195 and parameters: {'observation_period_num': 51, 'train_rates': 0.9433703401968442, 'learning_rate': 0.0009983290173447006, 'batch_size': 184, 'step_size': 7, 'gamma': 0.9127270932499111}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:58:14,407][0m Trial 26 finished with value: 0.0725562605787726 and parameters: {'observation_period_num': 62, 'train_rates': 0.9118102368767256, 'learning_rate': 0.0009500609837178131, 'batch_size': 172, 'step_size': 12, 'gamma': 0.8562857513768165}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 17:59:15,890][0m Trial 27 finished with value: 0.13653263738078456 and parameters: {'observation_period_num': 86, 'train_rates': 0.9459961116908779, 'learning_rate': 7.242804221232067e-05, 'batch_size': 151, 'step_size': 2, 'gamma': 0.8834690220002825}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:00:13,080][0m Trial 28 finished with value: 0.092897929251194 and parameters: {'observation_period_num': 51, 'train_rates': 0.9895969359572343, 'learning_rate': 0.00021725716173935156, 'batch_size': 184, 'step_size': 9, 'gamma': 0.972722152773216}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:01:21,087][0m Trial 29 finished with value: 0.05282286960536089 and parameters: {'observation_period_num': 17, 'train_rates': 0.8671653507490067, 'learning_rate': 2.708878239684465e-05, 'batch_size': 89, 'step_size': 4, 'gamma': 0.9420850759545203}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:03:12,574][0m Trial 30 finished with value: 0.046457902475265024 and parameters: {'observation_period_num': 19, 'train_rates': 0.8685812057773077, 'learning_rate': 2.145340967925461e-05, 'batch_size': 39, 'step_size': 4, 'gamma': 0.946932859720601}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:05:02,571][0m Trial 31 finished with value: 0.04032714768602133 and parameters: {'observation_period_num': 5, 'train_rates': 0.8109955453462326, 'learning_rate': 3.117879383477919e-05, 'batch_size': 36, 'step_size': 4, 'gamma': 0.9424360455499323}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:06:50,020][0m Trial 32 finished with value: 0.06608750294649156 and parameters: {'observation_period_num': 5, 'train_rates': 0.808902268564631, 'learning_rate': 5.845502589834162e-06, 'batch_size': 37, 'step_size': 2, 'gamma': 0.9570082555204517}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:08:26,611][0m Trial 33 finished with value: 0.045453816924216975 and parameters: {'observation_period_num': 24, 'train_rates': 0.8598700184164758, 'learning_rate': 2.4573558034537124e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.9431790345595068}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:12:46,690][0m Trial 34 finished with value: 0.04238048601266626 and parameters: {'observation_period_num': 19, 'train_rates': 0.8688130578582702, 'learning_rate': 2.1769072067675162e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9339798956189442}. Best is trial 12 with value: 0.036212689031592825.[0m
[32m[I 2025-01-04 18:17:21,483][0m Trial 35 finished with value: 0.02626246703080476 and parameters: {'observation_period_num': 5, 'train_rates': 0.8939272578199177, 'learning_rate': 4.106788016267291e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9864111829829905}. Best is trial 35 with value: 0.02626246703080476.[0m
[32m[I 2025-01-04 18:21:34,564][0m Trial 36 finished with value: 0.025961237085506947 and parameters: {'observation_period_num': 5, 'train_rates': 0.8951241604153682, 'learning_rate': 4.3953264668006726e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9757009520010053}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:23:36,175][0m Trial 37 finished with value: 0.030279416027943037 and parameters: {'observation_period_num': 7, 'train_rates': 0.8944967767918871, 'learning_rate': 4.779913641244138e-05, 'batch_size': 48, 'step_size': 6, 'gamma': 0.9893157222453326}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:28:01,643][0m Trial 38 finished with value: 0.07512271012130536 and parameters: {'observation_period_num': 50, 'train_rates': 0.8952684474832722, 'learning_rate': 4.2887722685960525e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9879675006864556}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:29:44,271][0m Trial 39 finished with value: 0.09081033205017826 and parameters: {'observation_period_num': 135, 'train_rates': 0.9303901469811733, 'learning_rate': 8.549005962765429e-05, 'batch_size': 55, 'step_size': 7, 'gamma': 0.9755001663436897}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:32:30,218][0m Trial 40 finished with value: 0.06111283672143178 and parameters: {'observation_period_num': 45, 'train_rates': 0.9145658491281015, 'learning_rate': 6.89179144637559e-06, 'batch_size': 27, 'step_size': 8, 'gamma': 0.9666102098705253}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:34:10,066][0m Trial 41 finished with value: 0.16586638707943946 and parameters: {'observation_period_num': 7, 'train_rates': 0.7712491666055892, 'learning_rate': 4.261376977228222e-05, 'batch_size': 42, 'step_size': 6, 'gamma': 0.9846400876124345}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:36:47,911][0m Trial 42 finished with value: 0.05338986441087377 and parameters: {'observation_period_num': 27, 'train_rates': 0.8118601940709229, 'learning_rate': 1.5078357227958203e-05, 'batch_size': 30, 'step_size': 6, 'gamma': 0.9681316130739732}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:38:06,782][0m Trial 43 finished with value: 0.03763470534039171 and parameters: {'observation_period_num': 8, 'train_rates': 0.8968259530755018, 'learning_rate': 6.593051153603726e-05, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9803064989170585}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:39:08,261][0m Trial 44 finished with value: 0.05958221604426702 and parameters: {'observation_period_num': 22, 'train_rates': 0.8924679215888118, 'learning_rate': 6.811977152940799e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.980715375372296}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:40:34,701][0m Trial 45 finished with value: 0.0677222504994823 and parameters: {'observation_period_num': 59, 'train_rates': 0.9646780342891628, 'learning_rate': 0.00013385586575896536, 'batch_size': 51, 'step_size': 14, 'gamma': 0.8275356168017615}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:42:02,823][0m Trial 46 finished with value: 0.13510412660431978 and parameters: {'observation_period_num': 185, 'train_rates': 0.9267872935273777, 'learning_rate': 1.0148981946954424e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9637501040597759}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:42:59,734][0m Trial 47 finished with value: 0.03841598134707002 and parameters: {'observation_period_num': 15, 'train_rates': 0.8842426772230242, 'learning_rate': 0.0001108436080724262, 'batch_size': 94, 'step_size': 14, 'gamma': 0.9792023673524899}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:45:05,932][0m Trial 48 finished with value: 0.19056821360273637 and parameters: {'observation_period_num': 33, 'train_rates': 0.6020178182007463, 'learning_rate': 3.6557325945521085e-05, 'batch_size': 25, 'step_size': 8, 'gamma': 0.7642586813320671}. Best is trial 36 with value: 0.025961237085506947.[0m
[32m[I 2025-01-04 18:49:09,489][0m Trial 49 finished with value: 0.06719055323119563 and parameters: {'observation_period_num': 27, 'train_rates': 0.8554704192266922, 'learning_rate': 5.890453492251341e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.9843570610038525}. Best is trial 36 with value: 0.025961237085506947.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AMZN_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1825 | 0.1020
Epoch 2/300, Loss: 0.1347 | 0.0741
Epoch 3/300, Loss: 0.1234 | 0.0652
Epoch 4/300, Loss: 0.1164 | 0.0601
Epoch 5/300, Loss: 0.1115 | 0.0571
Epoch 6/300, Loss: 0.1080 | 0.0548
Epoch 7/300, Loss: 0.1051 | 0.0528
Epoch 8/300, Loss: 0.1027 | 0.0515
Epoch 9/300, Loss: 0.1005 | 0.0509
Epoch 10/300, Loss: 0.0983 | 0.0509
Epoch 11/300, Loss: 0.0965 | 0.0507
Epoch 12/300, Loss: 0.0946 | 0.0503
Epoch 13/300, Loss: 0.0928 | 0.0500
Epoch 14/300, Loss: 0.0910 | 0.0493
Epoch 15/300, Loss: 0.0895 | 0.0487
Epoch 16/300, Loss: 0.0881 | 0.0485
Epoch 17/300, Loss: 0.0869 | 0.0481
Epoch 18/300, Loss: 0.0859 | 0.0477
Epoch 19/300, Loss: 0.0849 | 0.0472
Epoch 20/300, Loss: 0.0841 | 0.0466
Epoch 21/300, Loss: 0.0833 | 0.0460
Epoch 22/300, Loss: 0.0825 | 0.0453
Epoch 23/300, Loss: 0.0818 | 0.0446
Epoch 24/300, Loss: 0.0811 | 0.0439
Epoch 25/300, Loss: 0.0803 | 0.0431
Epoch 26/300, Loss: 0.0796 | 0.0424
Epoch 27/300, Loss: 0.0790 | 0.0418
Epoch 28/300, Loss: 0.0783 | 0.0408
Epoch 29/300, Loss: 0.0776 | 0.0400
Epoch 30/300, Loss: 0.0769 | 0.0392
Epoch 31/300, Loss: 0.0763 | 0.0383
Epoch 32/300, Loss: 0.0756 | 0.0376
Epoch 33/300, Loss: 0.0749 | 0.0369
Epoch 34/300, Loss: 0.0742 | 0.0362
Epoch 35/300, Loss: 0.0737 | 0.0357
Epoch 36/300, Loss: 0.0732 | 0.0352
Epoch 37/300, Loss: 0.0728 | 0.0348
Epoch 38/300, Loss: 0.0724 | 0.0345
Epoch 39/300, Loss: 0.0721 | 0.0343
Epoch 40/300, Loss: 0.0718 | 0.0339
Epoch 41/300, Loss: 0.0715 | 0.0338
Epoch 42/300, Loss: 0.0712 | 0.0337
Epoch 43/300, Loss: 0.0709 | 0.0333
Epoch 44/300, Loss: 0.0706 | 0.0332
Epoch 45/300, Loss: 0.0704 | 0.0330
Epoch 46/300, Loss: 0.0701 | 0.0327
Epoch 47/300, Loss: 0.0698 | 0.0326
Epoch 48/300, Loss: 0.0696 | 0.0325
Epoch 49/300, Loss: 0.0693 | 0.0322
Epoch 50/300, Loss: 0.0690 | 0.0321
Epoch 51/300, Loss: 0.0688 | 0.0321
Epoch 52/300, Loss: 0.0686 | 0.0318
Epoch 53/300, Loss: 0.0683 | 0.0318
Epoch 54/300, Loss: 0.0681 | 0.0318
Epoch 55/300, Loss: 0.0679 | 0.0314
Epoch 56/300, Loss: 0.0677 | 0.0313
Epoch 57/300, Loss: 0.0675 | 0.0312
Epoch 58/300, Loss: 0.0673 | 0.0308
Epoch 59/300, Loss: 0.0671 | 0.0307
Epoch 60/300, Loss: 0.0669 | 0.0306
Epoch 61/300, Loss: 0.0667 | 0.0302
Epoch 62/300, Loss: 0.0665 | 0.0301
Epoch 63/300, Loss: 0.0663 | 0.0300
Epoch 64/300, Loss: 0.0661 | 0.0297
Epoch 65/300, Loss: 0.0659 | 0.0296
Epoch 66/300, Loss: 0.0657 | 0.0295
Epoch 67/300, Loss: 0.0655 | 0.0293
Epoch 68/300, Loss: 0.0653 | 0.0292
Epoch 69/300, Loss: 0.0651 | 0.0291
Epoch 70/300, Loss: 0.0649 | 0.0289
Epoch 71/300, Loss: 0.0647 | 0.0288
Epoch 72/300, Loss: 0.0645 | 0.0287
Epoch 73/300, Loss: 0.0643 | 0.0285
Epoch 74/300, Loss: 0.0641 | 0.0284
Epoch 75/300, Loss: 0.0640 | 0.0283
Epoch 76/300, Loss: 0.0638 | 0.0282
Epoch 77/300, Loss: 0.0636 | 0.0281
Epoch 78/300, Loss: 0.0634 | 0.0280
Epoch 79/300, Loss: 0.0632 | 0.0279
Epoch 80/300, Loss: 0.0630 | 0.0278
Epoch 81/300, Loss: 0.0629 | 0.0277
Epoch 82/300, Loss: 0.0627 | 0.0275
Epoch 83/300, Loss: 0.0625 | 0.0274
Epoch 84/300, Loss: 0.0624 | 0.0273
Epoch 85/300, Loss: 0.0622 | 0.0271
Epoch 86/300, Loss: 0.0620 | 0.0270
Epoch 87/300, Loss: 0.0619 | 0.0269
Epoch 88/300, Loss: 0.0617 | 0.0268
Epoch 89/300, Loss: 0.0616 | 0.0268
Epoch 90/300, Loss: 0.0614 | 0.0268
Epoch 91/300, Loss: 0.0613 | 0.0267
Epoch 92/300, Loss: 0.0611 | 0.0268
Epoch 93/300, Loss: 0.0610 | 0.0268
Epoch 94/300, Loss: 0.0608 | 0.0268
Epoch 95/300, Loss: 0.0607 | 0.0268
Epoch 96/300, Loss: 0.0606 | 0.0268
Epoch 97/300, Loss: 0.0604 | 0.0269
Epoch 98/300, Loss: 0.0603 | 0.0269
Epoch 99/300, Loss: 0.0602 | 0.0269
Epoch 100/300, Loss: 0.0601 | 0.0269
Epoch 101/300, Loss: 0.0600 | 0.0269
Epoch 102/300, Loss: 0.0598 | 0.0268
Epoch 103/300, Loss: 0.0597 | 0.0268
Epoch 104/300, Loss: 0.0596 | 0.0267
Epoch 105/300, Loss: 0.0595 | 0.0267
Epoch 106/300, Loss: 0.0594 | 0.0266
Epoch 107/300, Loss: 0.0593 | 0.0265
Epoch 108/300, Loss: 0.0592 | 0.0265
Epoch 109/300, Loss: 0.0590 | 0.0264
Epoch 110/300, Loss: 0.0589 | 0.0263
Epoch 111/300, Loss: 0.0588 | 0.0262
Epoch 112/300, Loss: 0.0587 | 0.0261
Epoch 113/300, Loss: 0.0586 | 0.0261
Epoch 114/300, Loss: 0.0585 | 0.0260
Epoch 115/300, Loss: 0.0584 | 0.0259
Epoch 116/300, Loss: 0.0583 | 0.0258
Epoch 117/300, Loss: 0.0582 | 0.0257
Epoch 118/300, Loss: 0.0581 | 0.0256
Epoch 119/300, Loss: 0.0580 | 0.0256
Epoch 120/300, Loss: 0.0579 | 0.0255
Epoch 121/300, Loss: 0.0578 | 0.0255
Epoch 122/300, Loss: 0.0577 | 0.0256
Epoch 123/300, Loss: 0.0576 | 0.0256
Epoch 124/300, Loss: 0.0575 | 0.0258
Epoch 125/300, Loss: 0.0575 | 0.0259
Epoch 126/300, Loss: 0.0574 | 0.0259
Epoch 127/300, Loss: 0.0573 | 0.0259
Epoch 128/300, Loss: 0.0572 | 0.0258
Epoch 129/300, Loss: 0.0571 | 0.0257
Epoch 130/300, Loss: 0.0570 | 0.0256
Epoch 131/300, Loss: 0.0569 | 0.0255
Epoch 132/300, Loss: 0.0568 | 0.0255
Epoch 133/300, Loss: 0.0566 | 0.0254
Epoch 134/300, Loss: 0.0565 | 0.0254
Epoch 135/300, Loss: 0.0564 | 0.0253
Epoch 136/300, Loss: 0.0563 | 0.0253
Epoch 137/300, Loss: 0.0561 | 0.0252
Epoch 138/300, Loss: 0.0560 | 0.0252
Epoch 139/300, Loss: 0.0559 | 0.0252
Epoch 140/300, Loss: 0.0558 | 0.0252
Epoch 141/300, Loss: 0.0557 | 0.0251
Epoch 142/300, Loss: 0.0555 | 0.0252
Epoch 143/300, Loss: 0.0554 | 0.0251
Epoch 144/300, Loss: 0.0553 | 0.0252
Epoch 145/300, Loss: 0.0552 | 0.0252
Epoch 146/300, Loss: 0.0550 | 0.0252
Epoch 147/300, Loss: 0.0549 | 0.0253
Epoch 148/300, Loss: 0.0547 | 0.0254
Epoch 149/300, Loss: 0.0545 | 0.0255
Epoch 150/300, Loss: 0.0544 | 0.0256
Epoch 151/300, Loss: 0.0541 | 0.0257
Epoch 152/300, Loss: 0.0539 | 0.0259
Epoch 153/300, Loss: 0.0535 | 0.0260
Epoch 154/300, Loss: 0.0531 | 0.0262
Epoch 155/300, Loss: 0.0527 | 0.0265
Epoch 156/300, Loss: 0.0521 | 0.0267
Epoch 157/300, Loss: 0.0515 | 0.0269
Epoch 158/300, Loss: 0.0509 | 0.0270
Epoch 159/300, Loss: 0.0503 | 0.0270
Epoch 160/300, Loss: 0.0498 | 0.0270
Epoch 161/300, Loss: 0.0493 | 0.0268
Epoch 162/300, Loss: 0.0489 | 0.0267
Epoch 163/300, Loss: 0.0486 | 0.0266
Epoch 164/300, Loss: 0.0483 | 0.0264
Epoch 165/300, Loss: 0.0480 | 0.0263
Epoch 166/300, Loss: 0.0477 | 0.0262
Epoch 167/300, Loss: 0.0475 | 0.0261
Epoch 168/300, Loss: 0.0473 | 0.0260
Epoch 169/300, Loss: 0.0472 | 0.0262
Epoch 170/300, Loss: 0.0471 | 0.0264
Epoch 171/300, Loss: 0.0486 | 0.0356
Epoch 172/300, Loss: 0.0488 | 0.0293
Epoch 173/300, Loss: 0.0475 | 0.0276
Epoch 174/300, Loss: 0.0474 | 0.0282
Epoch 175/300, Loss: 0.0465 | 0.0262
Epoch 176/300, Loss: 0.0463 | 0.0265
Epoch 177/300, Loss: 0.0462 | 0.0262
Epoch 178/300, Loss: 0.0461 | 0.0262
Epoch 179/300, Loss: 0.0460 | 0.0261
Epoch 180/300, Loss: 0.0459 | 0.0261
Epoch 181/300, Loss: 0.0458 | 0.0260
Epoch 182/300, Loss: 0.0457 | 0.0260
Epoch 183/300, Loss: 0.0457 | 0.0259
Epoch 184/300, Loss: 0.0456 | 0.0259
Epoch 185/300, Loss: 0.0455 | 0.0259
Epoch 186/300, Loss: 0.0454 | 0.0259
Epoch 187/300, Loss: 0.0454 | 0.0258
Epoch 188/300, Loss: 0.0453 | 0.0259
Epoch 189/300, Loss: 0.0453 | 0.0257
Epoch 190/300, Loss: 0.0453 | 0.0262
Epoch 191/300, Loss: 0.0454 | 0.0272
Epoch 192/300, Loss: 0.0460 | 0.0276
Epoch 193/300, Loss: 0.0476 | 0.0382
Epoch 194/300, Loss: 0.0482 | 0.0278
Epoch 195/300, Loss: 0.0452 | 0.0258
Epoch 196/300, Loss: 0.0450 | 0.0256
Epoch 197/300, Loss: 0.0449 | 0.0255
Epoch 198/300, Loss: 0.0448 | 0.0255
Epoch 199/300, Loss: 0.0447 | 0.0254
Epoch 200/300, Loss: 0.0447 | 0.0255
Epoch 201/300, Loss: 0.0446 | 0.0255
Epoch 202/300, Loss: 0.0446 | 0.0254
Epoch 203/300, Loss: 0.0445 | 0.0255
Epoch 204/300, Loss: 0.0445 | 0.0255
Epoch 205/300, Loss: 0.0445 | 0.0254
Epoch 206/300, Loss: 0.0444 | 0.0255
Epoch 207/300, Loss: 0.0444 | 0.0255
Epoch 208/300, Loss: 0.0443 | 0.0254
Epoch 209/300, Loss: 0.0443 | 0.0255
Epoch 210/300, Loss: 0.0442 | 0.0255
Epoch 211/300, Loss: 0.0442 | 0.0254
Epoch 212/300, Loss: 0.0442 | 0.0255
Epoch 213/300, Loss: 0.0441 | 0.0255
Epoch 214/300, Loss: 0.0441 | 0.0254
Epoch 215/300, Loss: 0.0440 | 0.0254
Epoch 216/300, Loss: 0.0440 | 0.0255
Epoch 217/300, Loss: 0.0439 | 0.0254
Epoch 218/300, Loss: 0.0439 | 0.0254
Epoch 219/300, Loss: 0.0439 | 0.0254
Epoch 220/300, Loss: 0.0438 | 0.0254
Epoch 221/300, Loss: 0.0438 | 0.0254
Epoch 222/300, Loss: 0.0437 | 0.0254
Epoch 223/300, Loss: 0.0437 | 0.0254
Epoch 224/300, Loss: 0.0437 | 0.0253
Epoch 225/300, Loss: 0.0437 | 0.0256
Epoch 226/300, Loss: 0.0440 | 0.0262
Epoch 227/300, Loss: 0.0447 | 0.0263
Epoch 228/300, Loss: 0.0437 | 0.0252
Epoch 229/300, Loss: 0.0437 | 0.0252
Epoch 230/300, Loss: 0.0434 | 0.0252
Epoch 231/300, Loss: 0.0434 | 0.0252
Epoch 232/300, Loss: 0.0434 | 0.0252
Epoch 233/300, Loss: 0.0433 | 0.0252
Epoch 234/300, Loss: 0.0433 | 0.0252
Epoch 235/300, Loss: 0.0433 | 0.0252
Epoch 236/300, Loss: 0.0432 | 0.0252
Epoch 237/300, Loss: 0.0432 | 0.0252
Epoch 238/300, Loss: 0.0432 | 0.0253
Epoch 239/300, Loss: 0.0432 | 0.0252
Epoch 240/300, Loss: 0.0432 | 0.0254
Epoch 241/300, Loss: 0.0431 | 0.0252
Epoch 242/300, Loss: 0.0431 | 0.0254
Epoch 243/300, Loss: 0.0430 | 0.0252
Epoch 244/300, Loss: 0.0430 | 0.0254
Epoch 245/300, Loss: 0.0429 | 0.0253
Epoch 246/300, Loss: 0.0430 | 0.0254
Epoch 247/300, Loss: 0.0429 | 0.0253
Epoch 248/300, Loss: 0.0429 | 0.0254
Epoch 249/300, Loss: 0.0428 | 0.0253
Epoch 250/300, Loss: 0.0428 | 0.0254
Epoch 251/300, Loss: 0.0428 | 0.0253
Epoch 252/300, Loss: 0.0428 | 0.0255
Epoch 253/300, Loss: 0.0427 | 0.0254
Epoch 254/300, Loss: 0.0427 | 0.0255
Epoch 255/300, Loss: 0.0426 | 0.0254
Epoch 256/300, Loss: 0.0426 | 0.0255
Epoch 257/300, Loss: 0.0426 | 0.0255
Epoch 258/300, Loss: 0.0426 | 0.0256
Epoch 259/300, Loss: 0.0425 | 0.0255
Epoch 260/300, Loss: 0.0425 | 0.0256
Epoch 261/300, Loss: 0.0424 | 0.0255
Epoch 262/300, Loss: 0.0424 | 0.0256
Epoch 263/300, Loss: 0.0424 | 0.0256
Epoch 264/300, Loss: 0.0424 | 0.0257
Epoch 265/300, Loss: 0.0423 | 0.0256
Epoch 266/300, Loss: 0.0423 | 0.0257
Epoch 267/300, Loss: 0.0423 | 0.0256
Epoch 268/300, Loss: 0.0423 | 0.0258
Epoch 269/300, Loss: 0.0422 | 0.0257
Epoch 270/300, Loss: 0.0422 | 0.0258
Epoch 271/300, Loss: 0.0422 | 0.0257
Epoch 272/300, Loss: 0.0421 | 0.0258
Epoch 273/300, Loss: 0.0421 | 0.0258
Epoch 274/300, Loss: 0.0421 | 0.0259
Epoch 275/300, Loss: 0.0420 | 0.0258
Epoch 276/300, Loss: 0.0420 | 0.0259
Epoch 277/300, Loss: 0.0420 | 0.0259
Epoch 278/300, Loss: 0.0420 | 0.0260
Epoch 279/300, Loss: 0.0419 | 0.0259
Epoch 280/300, Loss: 0.0419 | 0.0260
Epoch 281/300, Loss: 0.0419 | 0.0260
Epoch 282/300, Loss: 0.0419 | 0.0260
Epoch 283/300, Loss: 0.0418 | 0.0260
Epoch 284/300, Loss: 0.0418 | 0.0261
Epoch 285/300, Loss: 0.0418 | 0.0260
Epoch 286/300, Loss: 0.0418 | 0.0261
Epoch 287/300, Loss: 0.0417 | 0.0261
Epoch 288/300, Loss: 0.0417 | 0.0262
Epoch 289/300, Loss: 0.0417 | 0.0262
Epoch 290/300, Loss: 0.0417 | 0.0262
Epoch 291/300, Loss: 0.0416 | 0.0262
Epoch 292/300, Loss: 0.0416 | 0.0263
Epoch 293/300, Loss: 0.0416 | 0.0262
Epoch 294/300, Loss: 0.0416 | 0.0263
Epoch 295/300, Loss: 0.0416 | 0.0263
Epoch 296/300, Loss: 0.0415 | 0.0263
Epoch 297/300, Loss: 0.0415 | 0.0263
Epoch 298/300, Loss: 0.0415 | 0.0264
Epoch 299/300, Loss: 0.0415 | 0.0264
Epoch 300/300, Loss: 0.0415 | 0.0264
Runtime (seconds): 753.2078127861023
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 12.42009628773667
RMSE: 3.5242156982421875
MAE: 3.5242156982421875
R-squared: nan
[197.92578]
