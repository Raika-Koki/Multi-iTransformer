ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-06 23:44:06,637][0m A new study created in memory with name: no-name-e16597ef-0c1f-487d-bca2-c928599fdf24[0m
[32m[I 2025-02-06 23:45:01,313][0m Trial 0 finished with value: 0.3751915662424333 and parameters: {'observation_period_num': 49, 'train_rates': 0.8159782107254148, 'learning_rate': 9.928501046541278e-06, 'batch_size': 99, 'step_size': 2, 'gamma': 0.8895097777196562}. Best is trial 0 with value: 0.3751915662424333.[0m
[32m[I 2025-02-06 23:45:37,537][0m Trial 1 finished with value: 0.26577674944088653 and parameters: {'observation_period_num': 176, 'train_rates': 0.8710050594037537, 'learning_rate': 1.916878498201895e-05, 'batch_size': 166, 'step_size': 13, 'gamma': 0.8699592447851464}. Best is trial 1 with value: 0.26577674944088653.[0m
[32m[I 2025-02-06 23:46:16,298][0m Trial 2 finished with value: 0.24705834265903764 and parameters: {'observation_period_num': 129, 'train_rates': 0.7004610100495272, 'learning_rate': 0.00028267398950681914, 'batch_size': 129, 'step_size': 5, 'gamma': 0.9127485787427815}. Best is trial 2 with value: 0.24705834265903764.[0m
[32m[I 2025-02-06 23:47:25,801][0m Trial 3 finished with value: 0.7380784992068331 and parameters: {'observation_period_num': 228, 'train_rates': 0.7737245960242782, 'learning_rate': 2.097353416058815e-06, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8873643508638702}. Best is trial 2 with value: 0.24705834265903764.[0m
[32m[I 2025-02-06 23:48:34,646][0m Trial 4 finished with value: 0.2242032628821673 and parameters: {'observation_period_num': 19, 'train_rates': 0.7146484328169933, 'learning_rate': 4.484762333469703e-06, 'batch_size': 71, 'step_size': 11, 'gamma': 0.9413678897543633}. Best is trial 4 with value: 0.2242032628821673.[0m
[32m[I 2025-02-06 23:49:31,385][0m Trial 5 finished with value: 0.30684496938637945 and parameters: {'observation_period_num': 226, 'train_rates': 0.6795215351753378, 'learning_rate': 4.8905066420010456e-05, 'batch_size': 80, 'step_size': 8, 'gamma': 0.93011967247759}. Best is trial 4 with value: 0.2242032628821673.[0m
[32m[I 2025-02-06 23:53:10,452][0m Trial 6 finished with value: 0.3324153026502019 and parameters: {'observation_period_num': 199, 'train_rates': 0.7252126258872617, 'learning_rate': 0.0005508810137311712, 'batch_size': 21, 'step_size': 14, 'gamma': 0.9144761506266006}. Best is trial 4 with value: 0.2242032628821673.[0m
[32m[I 2025-02-06 23:53:36,296][0m Trial 7 finished with value: 0.206303673076913 and parameters: {'observation_period_num': 235, 'train_rates': 0.7837287137069171, 'learning_rate': 9.996182022574857e-05, 'batch_size': 199, 'step_size': 6, 'gamma': 0.9318296552542473}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-06 23:54:55,724][0m Trial 8 finished with value: 0.2961696684360504 and parameters: {'observation_period_num': 230, 'train_rates': 0.9767628874485007, 'learning_rate': 6.634411274029315e-06, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9589298359820677}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-06 23:57:44,154][0m Trial 9 finished with value: 0.38035607029459756 and parameters: {'observation_period_num': 95, 'train_rates': 0.766529698900104, 'learning_rate': 1.3685678809083767e-05, 'batch_size': 29, 'step_size': 5, 'gamma': 0.7585369529727604}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-06 23:58:05,101][0m Trial 10 finished with value: 0.45235483437614304 and parameters: {'observation_period_num': 148, 'train_rates': 0.6079300748121839, 'learning_rate': 9.039919141279391e-05, 'batch_size': 235, 'step_size': 2, 'gamma': 0.9896130995659522}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-06 23:58:35,373][0m Trial 11 finished with value: 1.5478698021070962 and parameters: {'observation_period_num': 5, 'train_rates': 0.8623915646108906, 'learning_rate': 1.3637054980888917e-06, 'batch_size': 199, 'step_size': 11, 'gamma': 0.837370876698125}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-06 23:58:56,345][0m Trial 12 finished with value: 0.2523973160708437 and parameters: {'observation_period_num': 83, 'train_rates': 0.6250146762985753, 'learning_rate': 0.00014153629933094327, 'batch_size': 253, 'step_size': 10, 'gamma': 0.9595817485508855}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-06 23:59:32,789][0m Trial 13 finished with value: 0.3759866445383766 and parameters: {'observation_period_num': 31, 'train_rates': 0.8367176445564303, 'learning_rate': 4.210839413396943e-06, 'batch_size': 173, 'step_size': 11, 'gamma': 0.8162325128213959}. Best is trial 7 with value: 0.206303673076913.[0m
[32m[I 2025-02-07 00:00:18,966][0m Trial 14 finished with value: 0.12319272535440584 and parameters: {'observation_period_num': 88, 'train_rates': 0.9239870366064238, 'learning_rate': 5.003794288302601e-05, 'batch_size': 127, 'step_size': 6, 'gamma': 0.943683589877348}. Best is trial 14 with value: 0.12319272535440584.[0m
[32m[I 2025-02-07 00:01:02,655][0m Trial 15 finished with value: 0.11529934231905227 and parameters: {'observation_period_num': 90, 'train_rates': 0.934742287448169, 'learning_rate': 3.778709992656499e-05, 'batch_size': 138, 'step_size': 5, 'gamma': 0.9823159316547927}. Best is trial 15 with value: 0.11529934231905227.[0m
[32m[I 2025-02-07 00:01:51,476][0m Trial 16 finished with value: 0.2133553158353876 and parameters: {'observation_period_num': 87, 'train_rates': 0.9530860928298941, 'learning_rate': 4.056151281655224e-05, 'batch_size': 127, 'step_size': 4, 'gamma': 0.9886501224175496}. Best is trial 15 with value: 0.11529934231905227.[0m
[32m[I 2025-02-07 00:02:31,026][0m Trial 17 finished with value: 0.1385450259498928 and parameters: {'observation_period_num': 59, 'train_rates': 0.9207182257814321, 'learning_rate': 3.6646468024341106e-05, 'batch_size': 155, 'step_size': 3, 'gamma': 0.9676295141734682}. Best is trial 15 with value: 0.11529934231905227.[0m
Early stopping at epoch 84
[32m[I 2025-02-07 00:03:19,787][0m Trial 18 finished with value: 0.15701585131533005 and parameters: {'observation_period_num': 110, 'train_rates': 0.9045600825083447, 'learning_rate': 0.0008869746525184557, 'batch_size': 100, 'step_size': 1, 'gamma': 0.8449261946717486}. Best is trial 15 with value: 0.11529934231905227.[0m
[32m[I 2025-02-07 00:04:12,455][0m Trial 19 finished with value: 0.11970583824296072 and parameters: {'observation_period_num': 159, 'train_rates': 0.931256245340922, 'learning_rate': 0.00023997063197831077, 'batch_size': 111, 'step_size': 7, 'gamma': 0.7555882325068213}. Best is trial 15 with value: 0.11529934231905227.[0m
[32m[I 2025-02-07 00:06:21,588][0m Trial 20 finished with value: 0.15155743062496185 and parameters: {'observation_period_num': 156, 'train_rates': 0.9875478851121987, 'learning_rate': 0.0003114026382430789, 'batch_size': 45, 'step_size': 8, 'gamma': 0.7501480979118325}. Best is trial 15 with value: 0.11529934231905227.[0m
[32m[I 2025-02-07 00:07:14,755][0m Trial 21 finished with value: 0.11069374265415328 and parameters: {'observation_period_num': 122, 'train_rates': 0.93089988767616, 'learning_rate': 0.0001792345718342752, 'batch_size': 114, 'step_size': 6, 'gamma': 0.7955845940177679}. Best is trial 21 with value: 0.11069374265415328.[0m
[32m[I 2025-02-07 00:08:09,931][0m Trial 22 finished with value: 0.0994267190922983 and parameters: {'observation_period_num': 125, 'train_rates': 0.8873542328458442, 'learning_rate': 0.0001778399318972114, 'batch_size': 103, 'step_size': 7, 'gamma': 0.7869647900249955}. Best is trial 22 with value: 0.0994267190922983.[0m
[32m[I 2025-02-07 00:08:48,413][0m Trial 23 finished with value: 0.15965684005854333 and parameters: {'observation_period_num': 127, 'train_rates': 0.8841100964548606, 'learning_rate': 0.00014888598876366043, 'batch_size': 151, 'step_size': 4, 'gamma': 0.7900342850256088}. Best is trial 22 with value: 0.0994267190922983.[0m
[32m[I 2025-02-07 00:09:43,443][0m Trial 24 finished with value: 0.15163623914122581 and parameters: {'observation_period_num': 117, 'train_rates': 0.9495670073735449, 'learning_rate': 7.475325941371232e-05, 'batch_size': 108, 'step_size': 6, 'gamma': 0.7858736784179948}. Best is trial 22 with value: 0.0994267190922983.[0m
[32m[I 2025-02-07 00:10:15,674][0m Trial 25 finished with value: 0.3547928219563083 and parameters: {'observation_period_num': 63, 'train_rates': 0.8951893850172946, 'learning_rate': 2.3713807157593308e-05, 'batch_size': 196, 'step_size': 4, 'gamma': 0.7970385277464613}. Best is trial 22 with value: 0.0994267190922983.[0m
[32m[I 2025-02-07 00:10:54,449][0m Trial 26 finished with value: 0.13821707416974843 and parameters: {'observation_period_num': 184, 'train_rates': 0.8489334498132826, 'learning_rate': 0.0005268709128728122, 'batch_size': 144, 'step_size': 8, 'gamma': 0.811991419266727}. Best is trial 22 with value: 0.0994267190922983.[0m
[32m[I 2025-02-07 00:12:49,309][0m Trial 27 finished with value: 0.13000096470838593 and parameters: {'observation_period_num': 143, 'train_rates': 0.955969719247838, 'learning_rate': 0.00019382003285865157, 'batch_size': 49, 'step_size': 5, 'gamma': 0.7736146450991557}. Best is trial 22 with value: 0.0994267190922983.[0m
[32m[I 2025-02-07 00:13:49,103][0m Trial 28 finished with value: 0.09633138806625145 and parameters: {'observation_period_num': 105, 'train_rates': 0.8220073240058791, 'learning_rate': 0.0004219949273042817, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8321509554069455}. Best is trial 28 with value: 0.09633138806625145.[0m
[32m[I 2025-02-07 00:14:48,610][0m Trial 29 finished with value: 0.11562920033216917 and parameters: {'observation_period_num': 107, 'train_rates': 0.8100062388922726, 'learning_rate': 0.0004707758669464436, 'batch_size': 89, 'step_size': 10, 'gamma': 0.8427006479702307}. Best is trial 28 with value: 0.09633138806625145.[0m
[32m[I 2025-02-07 00:15:47,829][0m Trial 30 finished with value: 0.08514902440912694 and parameters: {'observation_period_num': 74, 'train_rates': 0.8261872977118678, 'learning_rate': 0.0009799411724474595, 'batch_size': 92, 'step_size': 9, 'gamma': 0.8228565696337764}. Best is trial 30 with value: 0.08514902440912694.[0m
[32m[I 2025-02-07 00:16:38,979][0m Trial 31 finished with value: 0.07348139059792468 and parameters: {'observation_period_num': 69, 'train_rates': 0.8300513956029529, 'learning_rate': 0.0009801918958150223, 'batch_size': 113, 'step_size': 9, 'gamma': 0.8214499497202794}. Best is trial 31 with value: 0.07348139059792468.[0m
[32m[I 2025-02-07 00:17:39,464][0m Trial 32 finished with value: 0.08843606737728166 and parameters: {'observation_period_num': 68, 'train_rates': 0.8254156991935897, 'learning_rate': 0.000986506758862147, 'batch_size': 89, 'step_size': 9, 'gamma': 0.824692306505176}. Best is trial 31 with value: 0.07348139059792468.[0m
[32m[I 2025-02-07 00:19:15,793][0m Trial 33 finished with value: 0.062172324229509406 and parameters: {'observation_period_num': 44, 'train_rates': 0.8126289386824145, 'learning_rate': 0.0009896464676528937, 'batch_size': 56, 'step_size': 12, 'gamma': 0.8261562700557118}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:20:46,837][0m Trial 34 finished with value: 0.20561455951659804 and parameters: {'observation_period_num': 45, 'train_rates': 0.7508021275492829, 'learning_rate': 0.0007634236379129798, 'batch_size': 56, 'step_size': 13, 'gamma': 0.8647783218578059}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:22:11,461][0m Trial 35 finished with value: 0.10250684850628722 and parameters: {'observation_period_num': 44, 'train_rates': 0.805951391046264, 'learning_rate': 0.0008968520215234121, 'batch_size': 62, 'step_size': 12, 'gamma': 0.8622255094565254}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:24:35,591][0m Trial 36 finished with value: 0.18377126626125195 and parameters: {'observation_period_num': 69, 'train_rates': 0.8581766298443467, 'learning_rate': 0.00035783568792360635, 'batch_size': 38, 'step_size': 15, 'gamma': 0.8174673459041016}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:25:30,910][0m Trial 37 finished with value: 0.16604827370161448 and parameters: {'observation_period_num': 30, 'train_rates': 0.7479399450935613, 'learning_rate': 0.0006447388214356776, 'batch_size': 93, 'step_size': 12, 'gamma': 0.8920442816938197}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:26:39,546][0m Trial 38 finished with value: 0.12605983511064991 and parameters: {'observation_period_num': 73, 'train_rates': 0.8286657266215641, 'learning_rate': 0.0009719184104842691, 'batch_size': 78, 'step_size': 9, 'gamma': 0.8545109444953094}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:28:00,051][0m Trial 39 finished with value: 0.21460627150160588 and parameters: {'observation_period_num': 54, 'train_rates': 0.792138734402082, 'learning_rate': 0.0006305821868819478, 'batch_size': 65, 'step_size': 10, 'gamma': 0.8295704734407242}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:28:46,110][0m Trial 40 finished with value: 0.16818682869374296 and parameters: {'observation_period_num': 21, 'train_rates': 0.7778908042501004, 'learning_rate': 0.0002910132526376645, 'batch_size': 117, 'step_size': 13, 'gamma': 0.8105000096930606}. Best is trial 33 with value: 0.062172324229509406.[0m
[32m[I 2025-02-07 00:29:47,899][0m Trial 41 finished with value: 0.060946028108200226 and parameters: {'observation_period_num': 75, 'train_rates': 0.8259858724194213, 'learning_rate': 0.0004172551920801695, 'batch_size': 85, 'step_size': 9, 'gamma': 0.8244677100794633}. Best is trial 41 with value: 0.060946028108200226.[0m
[32m[I 2025-02-07 00:30:53,481][0m Trial 42 finished with value: 0.10342218602420493 and parameters: {'observation_period_num': 75, 'train_rates': 0.8442256167912215, 'learning_rate': 0.0009918004099620356, 'batch_size': 84, 'step_size': 9, 'gamma': 0.8819196546302265}. Best is trial 41 with value: 0.060946028108200226.[0m
[32m[I 2025-02-07 00:32:06,468][0m Trial 43 finished with value: 0.054867689171290486 and parameters: {'observation_period_num': 36, 'train_rates': 0.8147081551982027, 'learning_rate': 0.0006469440997437411, 'batch_size': 73, 'step_size': 12, 'gamma': 0.8213100358314186}. Best is trial 43 with value: 0.054867689171290486.[0m
[32m[I 2025-02-07 00:34:59,941][0m Trial 44 finished with value: 0.032093087775689186 and parameters: {'observation_period_num': 8, 'train_rates': 0.798702233718772, 'learning_rate': 0.0005953105169070539, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8019475470230012}. Best is trial 44 with value: 0.032093087775689186.[0m
[32m[I 2025-02-07 00:38:54,668][0m Trial 45 finished with value: 0.04130575243941172 and parameters: {'observation_period_num': 6, 'train_rates': 0.8008564798509906, 'learning_rate': 0.0004189965482741551, 'batch_size': 22, 'step_size': 15, 'gamma': 0.7736730781263044}. Best is trial 44 with value: 0.032093087775689186.[0m
[32m[I 2025-02-07 00:44:07,102][0m Trial 46 finished with value: 0.16037761137808754 and parameters: {'observation_period_num': 5, 'train_rates': 0.7640370621896792, 'learning_rate': 0.00040119181531126014, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7742505395870756}. Best is trial 44 with value: 0.032093087775689186.[0m
[32m[I 2025-02-07 00:46:38,607][0m Trial 47 finished with value: 0.1528096596521387 and parameters: {'observation_period_num': 17, 'train_rates': 0.7220009005566106, 'learning_rate': 0.0005801839784391292, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8016275719932984}. Best is trial 44 with value: 0.032093087775689186.[0m
[32m[I 2025-02-07 00:49:50,371][0m Trial 48 finished with value: 0.1591064318857695 and parameters: {'observation_period_num': 36, 'train_rates': 0.6748639390758, 'learning_rate': 0.00011832453556565205, 'batch_size': 24, 'step_size': 14, 'gamma': 0.7676654059180346}. Best is trial 44 with value: 0.032093087775689186.[0m
[32m[I 2025-02-07 00:52:04,589][0m Trial 49 finished with value: 0.04937660625447397 and parameters: {'observation_period_num': 17, 'train_rates': 0.8046602432950505, 'learning_rate': 0.00023778161705877496, 'batch_size': 39, 'step_size': 15, 'gamma': 0.8050233721288431}. Best is trial 44 with value: 0.032093087775689186.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-07 00:52:04,601][0m A new study created in memory with name: no-name-50aa6d6f-a344-4440-863a-58aa63484580[0m
[32m[I 2025-02-07 00:53:31,181][0m Trial 0 finished with value: 0.08522691568079936 and parameters: {'observation_period_num': 59, 'train_rates': 0.9196612537511717, 'learning_rate': 5.450297056163612e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.7802214660509076}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:53:56,941][0m Trial 1 finished with value: 0.12280660609941224 and parameters: {'observation_period_num': 62, 'train_rates': 0.8721954010410893, 'learning_rate': 4.785136991974248e-05, 'batch_size': 243, 'step_size': 15, 'gamma': 0.7986998470146279}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:54:32,964][0m Trial 2 finished with value: 0.9116069159265292 and parameters: {'observation_period_num': 199, 'train_rates': 0.936013337238781, 'learning_rate': 1.5166687159586356e-06, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9819808697507693}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:55:14,552][0m Trial 3 finished with value: 0.3185289266789071 and parameters: {'observation_period_num': 208, 'train_rates': 0.7666348972267718, 'learning_rate': 0.0007667367506454443, 'batch_size': 123, 'step_size': 12, 'gamma': 0.8836093145019641}. Best is trial 0 with value: 0.08522691568079936.[0m
Early stopping at epoch 84
[32m[I 2025-02-07 00:57:44,022][0m Trial 4 finished with value: 0.8860755862934249 and parameters: {'observation_period_num': 127, 'train_rates': 0.9010918448042633, 'learning_rate': 1.78389130856704e-06, 'batch_size': 31, 'step_size': 1, 'gamma': 0.8399160298436857}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:58:07,583][0m Trial 5 finished with value: 0.31729978869273207 and parameters: {'observation_period_num': 37, 'train_rates': 0.6020551094102942, 'learning_rate': 1.59649637395893e-05, 'batch_size': 202, 'step_size': 15, 'gamma': 0.9608272308879705}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:58:30,889][0m Trial 6 finished with value: 0.5436108146452342 and parameters: {'observation_period_num': 245, 'train_rates': 0.8907959280185178, 'learning_rate': 7.341807004538505e-06, 'batch_size': 245, 'step_size': 12, 'gamma': 0.8220384946651951}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:59:00,972][0m Trial 7 finished with value: 0.180143725938904 and parameters: {'observation_period_num': 21, 'train_rates': 0.7875195555043298, 'learning_rate': 0.0009931482704085628, 'batch_size': 188, 'step_size': 8, 'gamma': 0.8926211879353002}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 00:59:55,780][0m Trial 8 finished with value: 0.16549727468854852 and parameters: {'observation_period_num': 12, 'train_rates': 0.7710048529817959, 'learning_rate': 0.00039692633557284715, 'batch_size': 96, 'step_size': 3, 'gamma': 0.9389925646799518}. Best is trial 0 with value: 0.08522691568079936.[0m
Early stopping at epoch 94
[32m[I 2025-02-07 01:00:25,904][0m Trial 9 finished with value: 2.779193639755249 and parameters: {'observation_period_num': 126, 'train_rates': 0.9327972805245309, 'learning_rate': 3.184873568448756e-06, 'batch_size': 196, 'step_size': 2, 'gamma': 0.8046764634735398}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:04:38,112][0m Trial 10 finished with value: 0.08744032295613453 and parameters: {'observation_period_num': 97, 'train_rates': 0.979799747130628, 'learning_rate': 8.330886718819016e-05, 'batch_size': 23, 'step_size': 5, 'gamma': 0.7525038024296183}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:08:21,924][0m Trial 11 finished with value: 0.08857250430931647 and parameters: {'observation_period_num': 96, 'train_rates': 0.9834711586492205, 'learning_rate': 9.335267698949502e-05, 'batch_size': 26, 'step_size': 5, 'gamma': 0.756734636764455}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:09:53,905][0m Trial 12 finished with value: 0.10244419936039677 and parameters: {'observation_period_num': 80, 'train_rates': 0.9748240666484144, 'learning_rate': 0.00011216806983800355, 'batch_size': 64, 'step_size': 5, 'gamma': 0.7634348340481929}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:11:11,002][0m Trial 13 finished with value: 0.10055286884664348 and parameters: {'observation_period_num': 164, 'train_rates': 0.8504477469184886, 'learning_rate': 0.0001908044024309903, 'batch_size': 69, 'step_size': 10, 'gamma': 0.7795882264164092}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:12:27,533][0m Trial 14 finished with value: 0.2569846429760464 and parameters: {'observation_period_num': 98, 'train_rates': 0.6891943204761659, 'learning_rate': 2.886906350706397e-05, 'batch_size': 60, 'step_size': 6, 'gamma': 0.8558633748479925}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:13:28,491][0m Trial 15 finished with value: 0.33665465348449647 and parameters: {'observation_period_num': 53, 'train_rates': 0.8510103634737486, 'learning_rate': 1.572603490400478e-05, 'batch_size': 89, 'step_size': 4, 'gamma': 0.7532805154781229}. Best is trial 0 with value: 0.08522691568079936.[0m
[32m[I 2025-02-07 01:18:32,130][0m Trial 16 finished with value: 0.0704875396367926 and parameters: {'observation_period_num': 78, 'train_rates': 0.9441296125143093, 'learning_rate': 5.3322240924984365e-05, 'batch_size': 19, 'step_size': 9, 'gamma': 0.7866777281793598}. Best is trial 16 with value: 0.0704875396367926.[0m
[32m[I 2025-02-07 01:19:15,133][0m Trial 17 finished with value: 0.10213835745567253 and parameters: {'observation_period_num': 156, 'train_rates': 0.823743615465234, 'learning_rate': 0.00026947937543038756, 'batch_size': 126, 'step_size': 10, 'gamma': 0.7942893698103406}. Best is trial 16 with value: 0.0704875396367926.[0m
[32m[I 2025-02-07 01:20:37,536][0m Trial 18 finished with value: 0.20155725321784523 and parameters: {'observation_period_num': 65, 'train_rates': 0.7219079647143588, 'learning_rate': 2.8645734964350357e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9048522858483125}. Best is trial 16 with value: 0.0704875396367926.[0m
[32m[I 2025-02-07 01:21:44,388][0m Trial 19 finished with value: 0.06544475257396698 and parameters: {'observation_period_num': 31, 'train_rates': 0.9316867908380776, 'learning_rate': 4.975933644049422e-05, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8321772840838055}. Best is trial 19 with value: 0.06544475257396698.[0m
[32m[I 2025-02-07 01:22:25,882][0m Trial 20 finished with value: 0.2985735371520248 and parameters: {'observation_period_num': 34, 'train_rates': 0.9460699056972791, 'learning_rate': 7.3499871298310225e-06, 'batch_size': 153, 'step_size': 13, 'gamma': 0.8274870848645434}. Best is trial 19 with value: 0.06544475257396698.[0m
[32m[I 2025-02-07 01:23:26,407][0m Trial 21 finished with value: 0.060962229786861326 and parameters: {'observation_period_num': 7, 'train_rates': 0.9160933807500328, 'learning_rate': 5.553294991797446e-05, 'batch_size': 100, 'step_size': 10, 'gamma': 0.8612132578231058}. Best is trial 21 with value: 0.060962229786861326.[0m
[32m[I 2025-02-07 01:24:28,222][0m Trial 22 finished with value: 0.08322770393466296 and parameters: {'observation_period_num': 6, 'train_rates': 0.950718128285084, 'learning_rate': 1.6396354335095313e-05, 'batch_size': 97, 'step_size': 11, 'gamma': 0.8636244163278622}. Best is trial 21 with value: 0.060962229786861326.[0m
[32m[I 2025-02-07 01:25:23,066][0m Trial 23 finished with value: 0.0582991730210916 and parameters: {'observation_period_num': 38, 'train_rates': 0.8911276776078726, 'learning_rate': 0.00014354091888025566, 'batch_size': 108, 'step_size': 13, 'gamma': 0.8491872259149855}. Best is trial 23 with value: 0.0582991730210916.[0m
[32m[I 2025-02-07 01:26:11,316][0m Trial 24 finished with value: 0.04921138317724126 and parameters: {'observation_period_num': 34, 'train_rates': 0.8814020972348875, 'learning_rate': 0.0001577066722869596, 'batch_size': 118, 'step_size': 13, 'gamma': 0.9093043147515486}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:26:58,997][0m Trial 25 finished with value: 0.05807866628402001 and parameters: {'observation_period_num': 42, 'train_rates': 0.8219852175324629, 'learning_rate': 0.00016825923656289701, 'batch_size': 116, 'step_size': 14, 'gamma': 0.9204029809476949}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:27:40,081][0m Trial 26 finished with value: 0.05909624598564142 and parameters: {'observation_period_num': 40, 'train_rates': 0.8279206059341215, 'learning_rate': 0.0001606604858380149, 'batch_size': 139, 'step_size': 14, 'gamma': 0.9211261517549887}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:28:31,190][0m Trial 27 finished with value: 0.12434311989007246 and parameters: {'observation_period_num': 47, 'train_rates': 0.875419474083545, 'learning_rate': 0.00045595194178825514, 'batch_size': 115, 'step_size': 14, 'gamma': 0.928211476253757}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:29:09,551][0m Trial 28 finished with value: 0.12671129641005363 and parameters: {'observation_period_num': 80, 'train_rates': 0.8186381873104047, 'learning_rate': 0.0002201598044583338, 'batch_size': 147, 'step_size': 13, 'gamma': 0.9531898391034522}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:29:40,759][0m Trial 29 finished with value: 0.243763750543197 and parameters: {'observation_period_num': 115, 'train_rates': 0.7364487729033229, 'learning_rate': 0.0004441563456865064, 'batch_size': 163, 'step_size': 14, 'gamma': 0.9117236701092877}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:30:29,670][0m Trial 30 finished with value: 0.10064975054269705 and parameters: {'observation_period_num': 151, 'train_rates': 0.8493887111548432, 'learning_rate': 0.00013247606633879413, 'batch_size': 113, 'step_size': 13, 'gamma': 0.8795357789484649}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:31:10,315][0m Trial 31 finished with value: 0.055572326376772764 and parameters: {'observation_period_num': 44, 'train_rates': 0.8239762529903488, 'learning_rate': 0.00015116650235266556, 'batch_size': 141, 'step_size': 14, 'gamma': 0.9181473044979491}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:31:41,531][0m Trial 32 finished with value: 0.10368381780604799 and parameters: {'observation_period_num': 62, 'train_rates': 0.8025536753940092, 'learning_rate': 0.0002836258496819351, 'batch_size': 179, 'step_size': 15, 'gamma': 0.90630491389835}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:32:24,881][0m Trial 33 finished with value: 0.052317846950709304 and parameters: {'observation_period_num': 29, 'train_rates': 0.8810232008147614, 'learning_rate': 8.256784090906501e-05, 'batch_size': 139, 'step_size': 15, 'gamma': 0.9393168301352333}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:32:52,030][0m Trial 34 finished with value: 0.06485936501577719 and parameters: {'observation_period_num': 21, 'train_rates': 0.863169892434006, 'learning_rate': 7.564144243232265e-05, 'batch_size': 224, 'step_size': 15, 'gamma': 0.9804348535021191}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:33:35,064][0m Trial 35 finished with value: 0.08491163896886926 and parameters: {'observation_period_num': 65, 'train_rates': 0.8295257436021161, 'learning_rate': 0.0005900918230710723, 'batch_size': 130, 'step_size': 14, 'gamma': 0.9431465561197013}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:34:09,519][0m Trial 36 finished with value: 0.19036663583080685 and parameters: {'observation_period_num': 20, 'train_rates': 0.7904441663157746, 'learning_rate': 0.00033184126715036003, 'batch_size': 165, 'step_size': 12, 'gamma': 0.8919370459466722}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:34:51,316][0m Trial 37 finished with value: 0.06268954972426097 and parameters: {'observation_period_num': 49, 'train_rates': 0.9074475172646768, 'learning_rate': 7.268579445804986e-05, 'batch_size': 144, 'step_size': 15, 'gamma': 0.9690542625791413}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:36:01,591][0m Trial 38 finished with value: 0.05626502571546513 and parameters: {'observation_period_num': 25, 'train_rates': 0.882377729944758, 'learning_rate': 3.595905102852535e-05, 'batch_size': 81, 'step_size': 11, 'gamma': 0.9270677474216478}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:37:58,368][0m Trial 39 finished with value: 0.05145830421724133 and parameters: {'observation_period_num': 25, 'train_rates': 0.8779393095207453, 'learning_rate': 3.510358164069903e-05, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9401053584550504}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:39:48,547][0m Trial 40 finished with value: 0.2928204877148889 and parameters: {'observation_period_num': 205, 'train_rates': 0.8632064435157324, 'learning_rate': 2.1132904976888407e-05, 'batch_size': 48, 'step_size': 11, 'gamma': 0.9402503030662072}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:41:02,613][0m Trial 41 finished with value: 0.05978257972292784 and parameters: {'observation_period_num': 28, 'train_rates': 0.8881875427402677, 'learning_rate': 3.532313986312135e-05, 'batch_size': 78, 'step_size': 11, 'gamma': 0.9286687471232611}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:43:14,786][0m Trial 42 finished with value: 0.06986845065565671 and parameters: {'observation_period_num': 19, 'train_rates': 0.8786102325379785, 'learning_rate': 1.0088051345716772e-05, 'batch_size': 42, 'step_size': 12, 'gamma': 0.9548820523246743}. Best is trial 24 with value: 0.04921138317724126.[0m
[32m[I 2025-02-07 01:45:42,408][0m Trial 43 finished with value: 0.04259766269713006 and parameters: {'observation_period_num': 5, 'train_rates': 0.9045110968092512, 'learning_rate': 3.3497730376660215e-05, 'batch_size': 39, 'step_size': 13, 'gamma': 0.9701412287915591}. Best is trial 43 with value: 0.04259766269713006.[0m
[32m[I 2025-02-07 01:48:14,444][0m Trial 44 finished with value: 0.1938418218245109 and parameters: {'observation_period_num': 250, 'train_rates': 0.9069737595603431, 'learning_rate': 0.00010431346606596245, 'batch_size': 35, 'step_size': 13, 'gamma': 0.9737025345598048}. Best is trial 43 with value: 0.04259766269713006.[0m
[32m[I 2025-02-07 01:48:38,197][0m Trial 45 finished with value: 0.2842027024650509 and parameters: {'observation_period_num': 12, 'train_rates': 0.625576523783063, 'learning_rate': 2.107925826106286e-05, 'batch_size': 213, 'step_size': 15, 'gamma': 0.9651501437419945}. Best is trial 43 with value: 0.04259766269713006.[0m
[32m[I 2025-02-07 01:49:06,786][0m Trial 46 finished with value: 0.2148388110867378 and parameters: {'observation_period_num': 57, 'train_rates': 0.7689339005562104, 'learning_rate': 7.21893822521021e-05, 'batch_size': 186, 'step_size': 14, 'gamma': 0.9835236287929543}. Best is trial 43 with value: 0.04259766269713006.[0m
[32m[I 2025-02-07 01:49:45,806][0m Trial 47 finished with value: 0.6077312231063843 and parameters: {'observation_period_num': 186, 'train_rates': 0.9675574437606383, 'learning_rate': 3.482913289660817e-06, 'batch_size': 153, 'step_size': 12, 'gamma': 0.9451493189883036}. Best is trial 43 with value: 0.04259766269713006.[0m
[32m[I 2025-02-07 01:51:34,874][0m Trial 48 finished with value: 0.07851516323931076 and parameters: {'observation_period_num': 75, 'train_rates': 0.8525972472020807, 'learning_rate': 4.3281581359928765e-05, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9892265479327554}. Best is trial 43 with value: 0.04259766269713006.[0m
[32m[I 2025-02-07 01:52:16,735][0m Trial 49 finished with value: 0.0478527621867565 and parameters: {'observation_period_num': 17, 'train_rates': 0.8057820839264855, 'learning_rate': 0.00011382766210602235, 'batch_size': 133, 'step_size': 13, 'gamma': 0.8791871046274324}. Best is trial 43 with value: 0.04259766269713006.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-07 01:52:16,745][0m A new study created in memory with name: no-name-8e31bcd6-ac28-4a5b-b3e2-bf02e6644f8a[0m
[32m[I 2025-02-07 01:53:45,251][0m Trial 0 finished with value: 0.1236918157471819 and parameters: {'observation_period_num': 126, 'train_rates': 0.912885578515721, 'learning_rate': 0.00042545310470790647, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7999831202364671}. Best is trial 0 with value: 0.1236918157471819.[0m
[32m[I 2025-02-07 01:54:16,658][0m Trial 1 finished with value: 0.2528338559961819 and parameters: {'observation_period_num': 140, 'train_rates': 0.7464070640976079, 'learning_rate': 0.00014374650599846344, 'batch_size': 175, 'step_size': 8, 'gamma': 0.9081117126133518}. Best is trial 0 with value: 0.1236918157471819.[0m
[32m[I 2025-02-07 01:55:10,713][0m Trial 2 finished with value: 0.09264033962698544 and parameters: {'observation_period_num': 118, 'train_rates': 0.9404260162676996, 'learning_rate': 0.00026286090323526096, 'batch_size': 108, 'step_size': 6, 'gamma': 0.8181340616019033}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 01:55:45,579][0m Trial 3 finished with value: 0.27062665306350664 and parameters: {'observation_period_num': 250, 'train_rates': 0.6637199215177474, 'learning_rate': 0.00015820103384705186, 'batch_size': 137, 'step_size': 12, 'gamma': 0.8432581964942198}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 01:56:16,142][0m Trial 4 finished with value: 0.1323336507167922 and parameters: {'observation_period_num': 181, 'train_rates': 0.8053401521517216, 'learning_rate': 0.00022959811920075817, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8501875752874679}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 01:57:24,272][0m Trial 5 finished with value: 0.37907827517381565 and parameters: {'observation_period_num': 154, 'train_rates': 0.7553130142964088, 'learning_rate': 1.615252184033389e-05, 'batch_size': 72, 'step_size': 3, 'gamma': 0.9342864225485205}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:01:18,759][0m Trial 6 finished with value: 0.3887385300817252 and parameters: {'observation_period_num': 213, 'train_rates': 0.8684508647135843, 'learning_rate': 4.636312666083401e-06, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9488482251224502}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:03:50,837][0m Trial 7 finished with value: 0.1447385177917538 and parameters: {'observation_period_num': 62, 'train_rates': 0.8288286806450867, 'learning_rate': 0.0007805700484669971, 'batch_size': 35, 'step_size': 7, 'gamma': 0.8987605908123721}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:04:36,004][0m Trial 8 finished with value: 0.5645868414988596 and parameters: {'observation_period_num': 72, 'train_rates': 0.6027250689529675, 'learning_rate': 7.558929372693448e-06, 'batch_size': 99, 'step_size': 7, 'gamma': 0.77950855961691}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:05:00,331][0m Trial 9 finished with value: 0.3153514562927367 and parameters: {'observation_period_num': 177, 'train_rates': 0.8069901287086447, 'learning_rate': 3.935996367595096e-05, 'batch_size': 233, 'step_size': 5, 'gamma': 0.8348096533754927}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:05:27,471][0m Trial 10 finished with value: 0.9540135264396667 and parameters: {'observation_period_num': 101, 'train_rates': 0.9841186036542359, 'learning_rate': 1.0010739634384811e-06, 'batch_size': 252, 'step_size': 11, 'gamma': 0.8054322001834736}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:06:36,285][0m Trial 11 finished with value: 0.1492154960287735 and parameters: {'observation_period_num': 120, 'train_rates': 0.9549903377607641, 'learning_rate': 0.0008989632283398078, 'batch_size': 85, 'step_size': 15, 'gamma': 0.7537129432182229}. Best is trial 2 with value: 0.09264033962698544.[0m
[32m[I 2025-02-07 02:07:28,942][0m Trial 12 finished with value: 0.03561269004854399 and parameters: {'observation_period_num': 12, 'train_rates': 0.8996743506578502, 'learning_rate': 0.000290452029327808, 'batch_size': 114, 'step_size': 11, 'gamma': 0.8004660051005349}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:08:17,591][0m Trial 13 finished with value: 0.06317836975448587 and parameters: {'observation_period_num': 16, 'train_rates': 0.9094147018602411, 'learning_rate': 6.337386557887183e-05, 'batch_size': 132, 'step_size': 10, 'gamma': 0.8196682770273143}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:08:57,535][0m Trial 14 finished with value: 0.051735354970921486 and parameters: {'observation_period_num': 7, 'train_rates': 0.8849304582680615, 'learning_rate': 5.9416524871620906e-05, 'batch_size': 145, 'step_size': 10, 'gamma': 0.9839110183263247}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:09:29,433][0m Trial 15 finished with value: 0.05158632030193718 and parameters: {'observation_period_num': 16, 'train_rates': 0.8702991720068532, 'learning_rate': 6.825707126977052e-05, 'batch_size': 187, 'step_size': 10, 'gamma': 0.9847850055796261}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:09:59,230][0m Trial 16 finished with value: 0.055492503121321576 and parameters: {'observation_period_num': 41, 'train_rates': 0.8531931049823156, 'learning_rate': 0.00010022160392916037, 'batch_size': 193, 'step_size': 15, 'gamma': 0.9882821217033235}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:10:26,118][0m Trial 17 finished with value: 0.2995448356400655 and parameters: {'observation_period_num': 39, 'train_rates': 0.7548471690418098, 'learning_rate': 1.8774412029919332e-05, 'batch_size': 213, 'step_size': 9, 'gamma': 0.873099438367833}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:10:57,619][0m Trial 18 finished with value: 0.17774872387112198 and parameters: {'observation_period_num': 78, 'train_rates': 0.7134151289536608, 'learning_rate': 0.0004545710715471956, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8769323985453498}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:11:25,659][0m Trial 19 finished with value: 0.19346839558819065 and parameters: {'observation_period_num': 37, 'train_rates': 0.8425600042849711, 'learning_rate': 2.256811762167034e-05, 'batch_size': 206, 'step_size': 13, 'gamma': 0.754790323970947}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:12:13,702][0m Trial 20 finished with value: 0.04185697248211731 and parameters: {'observation_period_num': 5, 'train_rates': 0.8982016825076508, 'learning_rate': 9.559309167837815e-05, 'batch_size': 125, 'step_size': 11, 'gamma': 0.9481484959860637}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:13:04,947][0m Trial 21 finished with value: 0.041615926053213036 and parameters: {'observation_period_num': 6, 'train_rates': 0.9067324852073589, 'learning_rate': 7.699218825923104e-05, 'batch_size': 117, 'step_size': 11, 'gamma': 0.956623463237803}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:13:59,890][0m Trial 22 finished with value: 0.035940075220925544 and parameters: {'observation_period_num': 5, 'train_rates': 0.9215336233865721, 'learning_rate': 0.0003205434579757745, 'batch_size': 109, 'step_size': 12, 'gamma': 0.9512072579110544}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:14:57,043][0m Trial 23 finished with value: 0.06266171485185623 and parameters: {'observation_period_num': 56, 'train_rates': 0.9820391014362713, 'learning_rate': 0.0003400512639391758, 'batch_size': 110, 'step_size': 12, 'gamma': 0.9242049735866433}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:16:57,293][0m Trial 24 finished with value: 0.09115295774406856 and parameters: {'observation_period_num': 92, 'train_rates': 0.9374909631189491, 'learning_rate': 0.0001760961025610594, 'batch_size': 49, 'step_size': 14, 'gamma': 0.959920779496769}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:18:06,419][0m Trial 25 finished with value: 0.06562368655284959 and parameters: {'observation_period_num': 30, 'train_rates': 0.924049305992375, 'learning_rate': 0.0006075890291685111, 'batch_size': 87, 'step_size': 9, 'gamma': 0.890749560172089}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:18:55,171][0m Trial 26 finished with value: 0.0656528373301383 and parameters: {'observation_period_num': 29, 'train_rates': 0.8850218793502703, 'learning_rate': 3.76120331331724e-05, 'batch_size': 119, 'step_size': 12, 'gamma': 0.9635258315206698}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:20:00,913][0m Trial 27 finished with value: 0.05443620933299391 and parameters: {'observation_period_num': 50, 'train_rates': 0.9598461253466559, 'learning_rate': 0.0002488350072308445, 'batch_size': 92, 'step_size': 11, 'gamma': 0.9177366489798722}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:20:37,267][0m Trial 28 finished with value: 0.06161295880491917 and parameters: {'observation_period_num': 81, 'train_rates': 0.8373802068220471, 'learning_rate': 0.00013193053918344842, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8549593193573888}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:22:12,044][0m Trial 29 finished with value: 0.03721922996599193 and parameters: {'observation_period_num': 22, 'train_rates': 0.9258231112341448, 'learning_rate': 0.0004614427930020177, 'batch_size': 61, 'step_size': 9, 'gamma': 0.7822455202480785}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:24:00,803][0m Trial 30 finished with value: 0.03758412250958451 and parameters: {'observation_period_num': 25, 'train_rates': 0.9629034201742125, 'learning_rate': 0.0004868381809728919, 'batch_size': 55, 'step_size': 8, 'gamma': 0.7868941877022136}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:25:39,103][0m Trial 31 finished with value: 0.03811432506788422 and parameters: {'observation_period_num': 26, 'train_rates': 0.962224980789509, 'learning_rate': 0.0005028758737105023, 'batch_size': 60, 'step_size': 8, 'gamma': 0.7826171231609256}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:27:03,997][0m Trial 32 finished with value: 0.06799083251026597 and parameters: {'observation_period_num': 55, 'train_rates': 0.9304038181900737, 'learning_rate': 0.00033511231037598136, 'batch_size': 69, 'step_size': 9, 'gamma': 0.7799375773623263}. Best is trial 12 with value: 0.03561269004854399.[0m
[32m[I 2025-02-07 02:29:06,034][0m Trial 33 finished with value: 0.03260182589292526 and parameters: {'observation_period_num': 20, 'train_rates': 0.9878045578301476, 'learning_rate': 0.0009516705217751463, 'batch_size': 50, 'step_size': 8, 'gamma': 0.7986319235120509}. Best is trial 33 with value: 0.03260182589292526.[0m
[32m[I 2025-02-07 02:35:10,411][0m Trial 34 finished with value: 0.04839889146387577 and parameters: {'observation_period_num': 47, 'train_rates': 0.9783334911905401, 'learning_rate': 0.0008899616091708119, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8069754063198545}. Best is trial 33 with value: 0.03260182589292526.[0m
[32m[I 2025-02-07 02:38:06,530][0m Trial 35 finished with value: 0.03518455646229976 and parameters: {'observation_period_num': 19, 'train_rates': 0.935262492422905, 'learning_rate': 0.0003151801032241371, 'batch_size': 33, 'step_size': 6, 'gamma': 0.8305242510779629}. Best is trial 33 with value: 0.03260182589292526.[0m
[32m[I 2025-02-07 02:40:16,254][0m Trial 36 finished with value: 0.0649356178501073 and parameters: {'observation_period_num': 65, 'train_rates': 0.9414618534902514, 'learning_rate': 0.0002141678455112269, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8263087679347564}. Best is trial 33 with value: 0.03260182589292526.[0m
[32m[I 2025-02-07 02:43:14,953][0m Trial 37 finished with value: 0.10723435133695602 and parameters: {'observation_period_num': 106, 'train_rates': 0.989373331520649, 'learning_rate': 0.0003204601037468409, 'batch_size': 33, 'step_size': 4, 'gamma': 0.8517642622742604}. Best is trial 33 with value: 0.03260182589292526.[0m
[32m[I 2025-02-07 02:44:21,188][0m Trial 38 finished with value: 0.13016379648010143 and parameters: {'observation_period_num': 211, 'train_rates': 0.7916190138754043, 'learning_rate': 0.00014955461875858792, 'batch_size': 75, 'step_size': 6, 'gamma': 0.7655474500323287}. Best is trial 33 with value: 0.03260182589292526.[0m
[32m[I 2025-02-07 02:47:22,285][0m Trial 39 finished with value: 0.028255501018145925 and parameters: {'observation_period_num': 16, 'train_rates': 0.8921571883161178, 'learning_rate': 0.0006838624503216884, 'batch_size': 31, 'step_size': 2, 'gamma': 0.8033283219156138}. Best is trial 39 with value: 0.028255501018145925.[0m
Early stopping at epoch 80
[32m[I 2025-02-07 02:49:23,504][0m Trial 40 finished with value: 0.12092824997204654 and parameters: {'observation_period_num': 158, 'train_rates': 0.8675995850289049, 'learning_rate': 0.0006891597141003866, 'batch_size': 35, 'step_size': 1, 'gamma': 0.7941862907821844}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 02:53:35,297][0m Trial 41 finished with value: 0.031631419307882 and parameters: {'observation_period_num': 15, 'train_rates': 0.8905619412848723, 'learning_rate': 0.0009877494652071705, 'batch_size': 22, 'step_size': 2, 'gamma': 0.8131341433962185}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 02:57:09,347][0m Trial 42 finished with value: 0.03312973163182825 and parameters: {'observation_period_num': 18, 'train_rates': 0.9003284611672047, 'learning_rate': 0.0009332247748362664, 'batch_size': 26, 'step_size': 3, 'gamma': 0.8127024919118891}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 03:00:29,911][0m Trial 43 finished with value: 0.047632508983102444 and parameters: {'observation_period_num': 36, 'train_rates': 0.8184727884881671, 'learning_rate': 0.0007227467447322748, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8154317784712415}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 03:02:36,995][0m Trial 44 finished with value: 0.17119908654633822 and parameters: {'observation_period_num': 20, 'train_rates': 0.7827273629642519, 'learning_rate': 0.0009554159970642164, 'batch_size': 41, 'step_size': 3, 'gamma': 0.8314858779246309}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 03:08:15,867][0m Trial 45 finished with value: 0.054254990340356364 and parameters: {'observation_period_num': 64, 'train_rates': 0.8855573298959061, 'learning_rate': 0.0005917758267751602, 'batch_size': 16, 'step_size': 2, 'gamma': 0.837604164944188}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 03:11:44,138][0m Trial 46 finished with value: 0.10762450752228918 and parameters: {'observation_period_num': 138, 'train_rates': 0.9497846536822621, 'learning_rate': 0.0009989646953868334, 'batch_size': 27, 'step_size': 3, 'gamma': 0.8131915271725974}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 03:13:14,011][0m Trial 47 finished with value: 0.6373568222744652 and parameters: {'observation_period_num': 247, 'train_rates': 0.6908580453512329, 'learning_rate': 9.49492380481497e-06, 'batch_size': 50, 'step_size': 4, 'gamma': 0.7670193825390976}. Best is trial 39 with value: 0.028255501018145925.[0m
[32m[I 2025-02-07 03:14:12,326][0m Trial 48 finished with value: 0.14585624597267555 and parameters: {'observation_period_num': 47, 'train_rates': 0.6156653136259774, 'learning_rate': 0.0006807106889106592, 'batch_size': 77, 'step_size': 2, 'gamma': 0.8607301955120492}. Best is trial 39 with value: 0.028255501018145925.[0m
Early stopping at epoch 67
[32m[I 2025-02-07 03:15:58,182][0m Trial 49 finished with value: 0.6211479289150466 and parameters: {'observation_period_num': 17, 'train_rates': 0.8577722859844978, 'learning_rate': 1.1070475260391892e-06, 'batch_size': 36, 'step_size': 1, 'gamma': 0.8427884064212318}. Best is trial 39 with value: 0.028255501018145925.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-07 03:15:58,191][0m A new study created in memory with name: no-name-f64cf5db-a8db-4b41-84c1-7dbaf8899a65[0m
[32m[I 2025-02-07 03:16:55,526][0m Trial 0 finished with value: 0.2527777874175974 and parameters: {'observation_period_num': 104, 'train_rates': 0.6551028554993905, 'learning_rate': 0.0007437779364254293, 'batch_size': 79, 'step_size': 9, 'gamma': 0.9058390008739092}. Best is trial 0 with value: 0.2527777874175974.[0m
[32m[I 2025-02-07 03:20:33,838][0m Trial 1 finished with value: 0.3756188238986863 and parameters: {'observation_period_num': 25, 'train_rates': 0.6211156434384357, 'learning_rate': 3.287810975082402e-06, 'batch_size': 20, 'step_size': 3, 'gamma': 0.8086916386542465}. Best is trial 0 with value: 0.2527777874175974.[0m
[32m[I 2025-02-07 03:21:19,703][0m Trial 2 finished with value: 0.12794192873155155 and parameters: {'observation_period_num': 11, 'train_rates': 0.6333365840669959, 'learning_rate': 0.0002420872721290963, 'batch_size': 103, 'step_size': 5, 'gamma': 0.9713020351080913}. Best is trial 2 with value: 0.12794192873155155.[0m
[32m[I 2025-02-07 03:24:38,873][0m Trial 3 finished with value: 0.05614613803304933 and parameters: {'observation_period_num': 30, 'train_rates': 0.8483550005986643, 'learning_rate': 1.22905780433611e-05, 'batch_size': 27, 'step_size': 5, 'gamma': 0.9649513858365575}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:25:05,083][0m Trial 4 finished with value: 0.13622591583704463 and parameters: {'observation_period_num': 228, 'train_rates': 0.8923560946253262, 'learning_rate': 0.0007583134695660318, 'batch_size': 234, 'step_size': 5, 'gamma': 0.8505032264749967}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:26:39,531][0m Trial 5 finished with value: 0.26313386040879044 and parameters: {'observation_period_num': 192, 'train_rates': 0.7704761230239635, 'learning_rate': 0.00010360520398381356, 'batch_size': 51, 'step_size': 10, 'gamma': 0.750232391733183}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:27:35,804][0m Trial 6 finished with value: 0.29111570369702977 and parameters: {'observation_period_num': 203, 'train_rates': 0.6979467401822064, 'learning_rate': 0.00016316057958670304, 'batch_size': 81, 'step_size': 15, 'gamma': 0.7712000349730631}. Best is trial 3 with value: 0.05614613803304933.[0m
Early stopping at epoch 97
[32m[I 2025-02-07 03:32:07,399][0m Trial 7 finished with value: 0.257850136421142 and parameters: {'observation_period_num': 77, 'train_rates': 0.7299993098558373, 'learning_rate': 5.61039099442953e-05, 'batch_size': 17, 'step_size': 1, 'gamma': 0.819797421180104}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:32:47,888][0m Trial 8 finished with value: 0.3687757443258728 and parameters: {'observation_period_num': 240, 'train_rates': 0.8730251636190365, 'learning_rate': 1.3063658754824907e-05, 'batch_size': 137, 'step_size': 10, 'gamma': 0.8099753056672435}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:33:15,851][0m Trial 9 finished with value: 0.5779168665803821 and parameters: {'observation_period_num': 65, 'train_rates': 0.7389364249592724, 'learning_rate': 3.2577449731155532e-06, 'batch_size': 194, 'step_size': 12, 'gamma': 0.9773529193698531}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:33:55,306][0m Trial 10 finished with value: 0.2685527801513672 and parameters: {'observation_period_num': 138, 'train_rates': 0.9894108542794223, 'learning_rate': 2.4538697940830592e-05, 'batch_size': 161, 'step_size': 6, 'gamma': 0.926362262327497}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:34:53,157][0m Trial 11 finished with value: 0.0679219616507005 and parameters: {'observation_period_num': 13, 'train_rates': 0.864656417136261, 'learning_rate': 1.0193018159327661e-05, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9884938588560193}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:35:43,818][0m Trial 12 finished with value: 0.16527221872757525 and parameters: {'observation_period_num': 45, 'train_rates': 0.8414091757736629, 'learning_rate': 1.2571512028616398e-05, 'batch_size': 115, 'step_size': 7, 'gamma': 0.9387802131611707}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:37:43,353][0m Trial 13 finished with value: 0.1850688177987007 and parameters: {'observation_period_num': 5, 'train_rates': 0.9400561360722837, 'learning_rate': 1.1171795854391066e-06, 'batch_size': 50, 'step_size': 3, 'gamma': 0.9896867927631908}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:38:16,151][0m Trial 14 finished with value: 0.711132153951002 and parameters: {'observation_period_num': 138, 'train_rates': 0.8077246747222285, 'learning_rate': 5.399958527054465e-06, 'batch_size': 164, 'step_size': 3, 'gamma': 0.885354018489962}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:39:58,482][0m Trial 15 finished with value: 0.14884506423765093 and parameters: {'observation_period_num': 86, 'train_rates': 0.9176817228262363, 'learning_rate': 9.250786675945592e-06, 'batch_size': 55, 'step_size': 8, 'gamma': 0.947186284925686}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:41:00,695][0m Trial 16 finished with value: 0.107077528585176 and parameters: {'observation_period_num': 42, 'train_rates': 0.8327874575828259, 'learning_rate': 3.904268947322623e-05, 'batch_size': 88, 'step_size': 1, 'gamma': 0.9542225036728348}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:41:46,214][0m Trial 17 finished with value: 1.0847378969192505 and parameters: {'observation_period_num': 113, 'train_rates': 0.9583490470924227, 'learning_rate': 1.1112103085798817e-06, 'batch_size': 131, 'step_size': 5, 'gamma': 0.906851689475159}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:42:12,187][0m Trial 18 finished with value: 0.10359035211580771 and parameters: {'observation_period_num': 46, 'train_rates': 0.8701499881965028, 'learning_rate': 2.5420399019085318e-05, 'batch_size': 244, 'step_size': 12, 'gamma': 0.8721799511038931}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:44:30,246][0m Trial 19 finished with value: 0.34409820383876416 and parameters: {'observation_period_num': 157, 'train_rates': 0.7855123636359438, 'learning_rate': 5.545655988680969e-06, 'batch_size': 36, 'step_size': 8, 'gamma': 0.9606811383642433}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:45:00,524][0m Trial 20 finished with value: 0.6858343608178536 and parameters: {'observation_period_num': 60, 'train_rates': 0.8401796936989105, 'learning_rate': 2.3393046898472347e-06, 'batch_size': 195, 'step_size': 4, 'gamma': 0.9176062189846828}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:45:26,100][0m Trial 21 finished with value: 0.13646638555371243 and parameters: {'observation_period_num': 33, 'train_rates': 0.8822537735498094, 'learning_rate': 2.2022820825512788e-05, 'batch_size': 255, 'step_size': 13, 'gamma': 0.8662100860124529}. Best is trial 3 with value: 0.05614613803304933.[0m
[32m[I 2025-02-07 03:45:54,883][0m Trial 22 finished with value: 0.051456359857824485 and parameters: {'observation_period_num': 24, 'train_rates': 0.8550665570276792, 'learning_rate': 6.451340731901765e-05, 'batch_size': 209, 'step_size': 12, 'gamma': 0.9892372847551065}. Best is trial 22 with value: 0.051456359857824485.[0m
[32m[I 2025-02-07 03:46:22,765][0m Trial 23 finished with value: 0.07958498668480427 and parameters: {'observation_period_num': 21, 'train_rates': 0.8080022210427299, 'learning_rate': 5.192160305583074e-05, 'batch_size': 215, 'step_size': 15, 'gamma': 0.9836433530457918}. Best is trial 22 with value: 0.051456359857824485.[0m
[32m[I 2025-02-07 03:47:00,409][0m Trial 24 finished with value: 0.04374674671682818 and parameters: {'observation_period_num': 6, 'train_rates': 0.9019433319858952, 'learning_rate': 0.00011108679102784955, 'batch_size': 163, 'step_size': 7, 'gamma': 0.9645052900577598}. Best is trial 24 with value: 0.04374674671682818.[0m
[32m[I 2025-02-07 03:47:36,158][0m Trial 25 finished with value: 0.05789759127717269 and parameters: {'observation_period_num': 61, 'train_rates': 0.9082182168694624, 'learning_rate': 0.0002832184973712214, 'batch_size': 172, 'step_size': 10, 'gamma': 0.936080956107103}. Best is trial 24 with value: 0.04374674671682818.[0m
[32m[I 2025-02-07 03:48:09,622][0m Trial 26 finished with value: 0.10751892626285553 and parameters: {'observation_period_num': 94, 'train_rates': 0.9373463037851639, 'learning_rate': 0.00013907286902170997, 'batch_size': 190, 'step_size': 7, 'gamma': 0.9633160307255447}. Best is trial 24 with value: 0.04374674671682818.[0m
[32m[I 2025-02-07 03:48:41,248][0m Trial 27 finished with value: 0.07290187478065491 and parameters: {'observation_period_num': 32, 'train_rates': 0.9887858390776862, 'learning_rate': 9.23917130892031e-05, 'batch_size': 218, 'step_size': 13, 'gamma': 0.9507531300190315}. Best is trial 24 with value: 0.04374674671682818.[0m
[32m[I 2025-02-07 03:49:22,074][0m Trial 28 finished with value: 0.03445959470488808 and parameters: {'observation_period_num': 6, 'train_rates': 0.8325559383882299, 'learning_rate': 0.00047313404957690183, 'batch_size': 144, 'step_size': 11, 'gamma': 0.9679892280662031}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:50:01,311][0m Trial 29 finished with value: 0.1589209002461776 and parameters: {'observation_period_num': 5, 'train_rates': 0.7691486896836698, 'learning_rate': 0.0004145022281861764, 'batch_size': 142, 'step_size': 11, 'gamma': 0.8887088199346138}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:50:29,039][0m Trial 30 finished with value: 0.26778772801554673 and parameters: {'observation_period_num': 113, 'train_rates': 0.6809153722582151, 'learning_rate': 0.0003504278201426956, 'batch_size': 176, 'step_size': 9, 'gamma': 0.9080790013898062}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:51:08,019][0m Trial 31 finished with value: 0.05523372977318992 and parameters: {'observation_period_num': 27, 'train_rates': 0.8288665250229712, 'learning_rate': 0.0009012002339393275, 'batch_size': 150, 'step_size': 14, 'gamma': 0.9674217557482702}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:51:48,398][0m Trial 32 finished with value: 0.0413220175566839 and parameters: {'observation_period_num': 24, 'train_rates': 0.8089226913631815, 'learning_rate': 0.0006323725305101875, 'batch_size': 144, 'step_size': 14, 'gamma': 0.9739268363646613}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:52:31,943][0m Trial 33 finished with value: 0.06459679777207582 and parameters: {'observation_period_num': 51, 'train_rates': 0.8022372667579587, 'learning_rate': 0.0005845613552399233, 'batch_size': 123, 'step_size': 13, 'gamma': 0.9292776108077417}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:53:06,875][0m Trial 34 finished with value: 0.159175323701889 and parameters: {'observation_period_num': 21, 'train_rates': 0.7444637274711029, 'learning_rate': 0.00020876881738775217, 'batch_size': 152, 'step_size': 11, 'gamma': 0.9715118838198745}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:53:35,923][0m Trial 35 finished with value: 0.11293051151906029 and parameters: {'observation_period_num': 72, 'train_rates': 0.8978591406772622, 'learning_rate': 0.0006142011671422799, 'batch_size': 210, 'step_size': 14, 'gamma': 0.945427602068563}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:54:10,261][0m Trial 36 finished with value: 0.04817493455974679 and parameters: {'observation_period_num': 18, 'train_rates': 0.8644959863253583, 'learning_rate': 8.725026628734719e-05, 'batch_size': 181, 'step_size': 11, 'gamma': 0.9767633318140354}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:54:43,117][0m Trial 37 finished with value: 0.05750504643061107 and parameters: {'observation_period_num': 11, 'train_rates': 0.8197099880059319, 'learning_rate': 9.635503155642549e-05, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8397215199724204}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:55:39,520][0m Trial 38 finished with value: 0.0788187969576668 and parameters: {'observation_period_num': 38, 'train_rates': 0.924101468956077, 'learning_rate': 0.0004211102184447774, 'batch_size': 105, 'step_size': 11, 'gamma': 0.9750112247840852}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:56:15,211][0m Trial 39 finished with value: 0.18866024322715808 and parameters: {'observation_period_num': 53, 'train_rates': 0.7769734668489284, 'learning_rate': 0.0001911377187680318, 'batch_size': 160, 'step_size': 8, 'gamma': 0.9224041113727088}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:56:52,845][0m Trial 40 finished with value: 0.13094688204259636 and parameters: {'observation_period_num': 15, 'train_rates': 0.6181786711503084, 'learning_rate': 0.0001390683922820048, 'batch_size': 128, 'step_size': 14, 'gamma': 0.7863033792650329}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:57:24,918][0m Trial 41 finished with value: 0.05230150802444059 and parameters: {'observation_period_num': 24, 'train_rates': 0.8560682096763701, 'learning_rate': 7.809658939971196e-05, 'batch_size': 184, 'step_size': 12, 'gamma': 0.9774929384895044}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:57:56,914][0m Trial 42 finished with value: 0.06609954481884356 and parameters: {'observation_period_num': 33, 'train_rates': 0.8941582632793396, 'learning_rate': 6.358251100104904e-05, 'batch_size': 201, 'step_size': 12, 'gamma': 0.9616278511006066}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:58:22,393][0m Trial 43 finished with value: 0.18105385824108955 and parameters: {'observation_period_num': 185, 'train_rates': 0.8556051286795971, 'learning_rate': 0.0009936025759009928, 'batch_size': 233, 'step_size': 10, 'gamma': 0.9877299478935032}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:59:03,026][0m Trial 44 finished with value: 0.049608587321280095 and parameters: {'observation_period_num': 5, 'train_rates': 0.8793363160164877, 'learning_rate': 4.3857061960791096e-05, 'batch_size': 149, 'step_size': 13, 'gamma': 0.9742283007159095}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 03:59:45,770][0m Trial 45 finished with value: 0.051524257408620336 and parameters: {'observation_period_num': 5, 'train_rates': 0.8856956776783487, 'learning_rate': 4.462219384077538e-05, 'batch_size': 142, 'step_size': 15, 'gamma': 0.9402969306884609}. Best is trial 28 with value: 0.03445959470488808.[0m
[32m[I 2025-02-07 04:00:36,486][0m Trial 46 finished with value: 0.0297774815971547 and parameters: {'observation_period_num': 13, 'train_rates': 0.8725648412054476, 'learning_rate': 0.0002486904614120312, 'batch_size': 112, 'step_size': 14, 'gamma': 0.9550522477946544}. Best is trial 46 with value: 0.0297774815971547.[0m
[32m[I 2025-02-07 04:01:30,954][0m Trial 47 finished with value: 0.03275320839881897 and parameters: {'observation_period_num': 17, 'train_rates': 0.9575271150911971, 'learning_rate': 0.00027289953352017824, 'batch_size': 114, 'step_size': 14, 'gamma': 0.953896938096258}. Best is trial 46 with value: 0.0297774815971547.[0m
[32m[I 2025-02-07 04:02:24,997][0m Trial 48 finished with value: 0.10404536128044128 and parameters: {'observation_period_num': 78, 'train_rates': 0.9633845533574681, 'learning_rate': 0.0002688000785932736, 'batch_size': 113, 'step_size': 14, 'gamma': 0.9545837925683297}. Best is trial 46 with value: 0.0297774815971547.[0m
[32m[I 2025-02-07 04:03:42,682][0m Trial 49 finished with value: 0.25975246074107977 and parameters: {'observation_period_num': 216, 'train_rates': 0.952926661724222, 'learning_rate': 0.0005134981660613949, 'batch_size': 71, 'step_size': 15, 'gamma': 0.9330555723609839}. Best is trial 46 with value: 0.0297774815971547.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-07 04:03:42,693][0m A new study created in memory with name: no-name-f52d57ee-35a0-474e-af50-14c8ba59158e[0m
[32m[I 2025-02-07 04:04:58,190][0m Trial 0 finished with value: 0.16144446520022407 and parameters: {'observation_period_num': 26, 'train_rates': 0.6835997793164594, 'learning_rate': 6.936580416331097e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8825126176246956}. Best is trial 0 with value: 0.16144446520022407.[0m
[32m[I 2025-02-07 04:05:19,429][0m Trial 1 finished with value: 0.5019041165573893 and parameters: {'observation_period_num': 184, 'train_rates': 0.6433851611817037, 'learning_rate': 1.92847689795368e-05, 'batch_size': 241, 'step_size': 6, 'gamma': 0.8865529767533802}. Best is trial 0 with value: 0.16144446520022407.[0m
[32m[I 2025-02-07 04:06:23,813][0m Trial 2 finished with value: 0.7174603996162643 and parameters: {'observation_period_num': 101, 'train_rates': 0.8246688407444769, 'learning_rate': 1.0053429568741995e-06, 'batch_size': 84, 'step_size': 7, 'gamma': 0.7829342156068383}. Best is trial 0 with value: 0.16144446520022407.[0m
[32m[I 2025-02-07 04:09:53,924][0m Trial 3 finished with value: 0.3084908000926837 and parameters: {'observation_period_num': 180, 'train_rates': 0.7834288963713397, 'learning_rate': 0.00024231747742374028, 'batch_size': 23, 'step_size': 6, 'gamma': 0.8755539618307109}. Best is trial 0 with value: 0.16144446520022407.[0m
[32m[I 2025-02-07 04:11:46,606][0m Trial 4 finished with value: 0.5673550474064429 and parameters: {'observation_period_num': 220, 'train_rates': 0.7143078230399129, 'learning_rate': 2.7134750355212972e-06, 'batch_size': 40, 'step_size': 13, 'gamma': 0.764166690669992}. Best is trial 0 with value: 0.16144446520022407.[0m
Early stopping at epoch 82
[32m[I 2025-02-07 04:12:07,961][0m Trial 5 finished with value: 2.1794469176336775 and parameters: {'observation_period_num': 60, 'train_rates': 0.8813962043043695, 'learning_rate': 1.379950716596626e-06, 'batch_size': 236, 'step_size': 2, 'gamma': 0.796776565391966}. Best is trial 0 with value: 0.16144446520022407.[0m
[32m[I 2025-02-07 04:12:35,932][0m Trial 6 finished with value: 0.20867576610367253 and parameters: {'observation_period_num': 198, 'train_rates': 0.8081101161147267, 'learning_rate': 0.0005079612423556756, 'batch_size': 202, 'step_size': 11, 'gamma': 0.9566162075974811}. Best is trial 0 with value: 0.16144446520022407.[0m
[32m[I 2025-02-07 04:13:03,149][0m Trial 7 finished with value: 0.15289210284009894 and parameters: {'observation_period_num': 95, 'train_rates': 0.9180670875036093, 'learning_rate': 3.198756277327682e-05, 'batch_size': 227, 'step_size': 6, 'gamma': 0.9801854104944093}. Best is trial 7 with value: 0.15289210284009894.[0m
[32m[I 2025-02-07 04:13:27,914][0m Trial 8 finished with value: 0.8945095282705391 and parameters: {'observation_period_num': 62, 'train_rates': 0.7184583393939731, 'learning_rate': 6.307344534091054e-06, 'batch_size': 225, 'step_size': 7, 'gamma': 0.7572964364151855}. Best is trial 7 with value: 0.15289210284009894.[0m
[32m[I 2025-02-07 04:14:09,660][0m Trial 9 finished with value: 0.45017601987549066 and parameters: {'observation_period_num': 130, 'train_rates': 0.8209265809136894, 'learning_rate': 8.210692484056577e-06, 'batch_size': 136, 'step_size': 5, 'gamma': 0.8375140181739059}. Best is trial 7 with value: 0.15289210284009894.[0m
[32m[I 2025-02-07 04:14:46,923][0m Trial 10 finished with value: 0.13162487745285034 and parameters: {'observation_period_num': 124, 'train_rates': 0.9796221522303263, 'learning_rate': 8.74957382461585e-05, 'batch_size': 171, 'step_size': 2, 'gamma': 0.9828990157155919}. Best is trial 10 with value: 0.13162487745285034.[0m
[32m[I 2025-02-07 04:15:22,432][0m Trial 11 finished with value: 0.18830394744873047 and parameters: {'observation_period_num': 134, 'train_rates': 0.978180764931596, 'learning_rate': 7.605835431166511e-05, 'batch_size': 175, 'step_size': 2, 'gamma': 0.9877014098119897}. Best is trial 10 with value: 0.13162487745285034.[0m
[32m[I 2025-02-07 04:16:03,424][0m Trial 12 finished with value: 0.34600454568862915 and parameters: {'observation_period_num': 96, 'train_rates': 0.979269289396791, 'learning_rate': 4.016008773261131e-05, 'batch_size': 155, 'step_size': 1, 'gamma': 0.9364697500377097}. Best is trial 10 with value: 0.13162487745285034.[0m
[32m[I 2025-02-07 04:16:58,103][0m Trial 13 finished with value: 0.1036431279956785 and parameters: {'observation_period_num': 151, 'train_rates': 0.9169318442092659, 'learning_rate': 0.0001404568435012791, 'batch_size': 106, 'step_size': 4, 'gamma': 0.9318172824591024}. Best is trial 13 with value: 0.1036431279956785.[0m
[32m[I 2025-02-07 04:17:48,665][0m Trial 14 finished with value: 0.09776318214156411 and parameters: {'observation_period_num': 154, 'train_rates': 0.9020596614093098, 'learning_rate': 0.00017627872441883659, 'batch_size': 111, 'step_size': 4, 'gamma': 0.9244776758198702}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:18:43,623][0m Trial 15 finished with value: 0.13331751476986745 and parameters: {'observation_period_num': 157, 'train_rates': 0.9011607015913452, 'learning_rate': 0.0008206160977671794, 'batch_size': 105, 'step_size': 4, 'gamma': 0.9200648788158552}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:19:27,097][0m Trial 16 finished with value: 0.21338226384726552 and parameters: {'observation_period_num': 247, 'train_rates': 0.8601626137028141, 'learning_rate': 0.00022213652122880936, 'batch_size': 124, 'step_size': 9, 'gamma': 0.919675606108441}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:20:29,127][0m Trial 17 finished with value: 0.12619833648204803 and parameters: {'observation_period_num': 155, 'train_rates': 0.9352686612445137, 'learning_rate': 0.0002090743138464678, 'batch_size': 91, 'step_size': 15, 'gamma': 0.8380000490649132}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:21:47,203][0m Trial 18 finished with value: 0.2719608877988466 and parameters: {'observation_period_num': 224, 'train_rates': 0.7635746304716441, 'learning_rate': 0.00038256130550981245, 'batch_size': 61, 'step_size': 4, 'gamma': 0.9055028723748734}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:22:34,767][0m Trial 19 finished with value: 0.13804976998463922 and parameters: {'observation_period_num': 151, 'train_rates': 0.8525088867822657, 'learning_rate': 0.00012526211010862082, 'batch_size': 117, 'step_size': 9, 'gamma': 0.9501227735276737}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:23:14,307][0m Trial 20 finished with value: 0.13071661546479824 and parameters: {'observation_period_num': 207, 'train_rates': 0.9377980670703288, 'learning_rate': 0.0009284693527581679, 'batch_size': 148, 'step_size': 4, 'gamma': 0.8437494345766123}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:24:20,556][0m Trial 21 finished with value: 0.13761757562557855 and parameters: {'observation_period_num': 164, 'train_rates': 0.9388270221248286, 'learning_rate': 0.00017481039474657152, 'batch_size': 87, 'step_size': 14, 'gamma': 0.8383989169199711}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:25:19,119][0m Trial 22 finished with value: 0.13945452582377654 and parameters: {'observation_period_num': 173, 'train_rates': 0.911591217644062, 'learning_rate': 0.0004067319589485224, 'batch_size': 95, 'step_size': 15, 'gamma': 0.8164473801508302}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:26:42,270][0m Trial 23 finished with value: 0.13967414580236207 and parameters: {'observation_period_num': 145, 'train_rates': 0.9465157152536854, 'learning_rate': 0.00012873168565718604, 'batch_size': 69, 'step_size': 10, 'gamma': 0.8542917288483692}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:27:33,196][0m Trial 24 finished with value: 0.1554384694106317 and parameters: {'observation_period_num': 113, 'train_rates': 0.8783951777466008, 'learning_rate': 4.1186659209448044e-05, 'batch_size': 112, 'step_size': 3, 'gamma': 0.9049793757603115}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:29:36,851][0m Trial 25 finished with value: 0.09785551882452435 and parameters: {'observation_period_num': 73, 'train_rates': 0.8441804388290166, 'learning_rate': 1.437212178058582e-05, 'batch_size': 43, 'step_size': 8, 'gamma': 0.8619560122975854}. Best is trial 14 with value: 0.09776318214156411.[0m
[32m[I 2025-02-07 04:35:06,234][0m Trial 26 finished with value: 0.053187165392915584 and parameters: {'observation_period_num': 64, 'train_rates': 0.8433964853695336, 'learning_rate': 1.5119911880354935e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.9601924430102532}. Best is trial 26 with value: 0.053187165392915584.[0m
[32m[I 2025-02-07 04:39:35,608][0m Trial 27 finished with value: 0.048998558633607664 and parameters: {'observation_period_num': 12, 'train_rates': 0.8468851429191248, 'learning_rate': 1.0232599397412356e-05, 'batch_size': 20, 'step_size': 8, 'gamma': 0.8979975052162259}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 04:44:46,514][0m Trial 28 finished with value: 0.17505624252471993 and parameters: {'observation_period_num': 6, 'train_rates': 0.7647160552219187, 'learning_rate': 4.9209721594203565e-06, 'batch_size': 16, 'step_size': 8, 'gamma': 0.964876860440265}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 04:47:03,116][0m Trial 29 finished with value: 0.19908737173576033 and parameters: {'observation_period_num': 36, 'train_rates': 0.78673660431233, 'learning_rate': 1.1009392513745759e-05, 'batch_size': 38, 'step_size': 12, 'gamma': 0.9423018522723978}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 04:48:18,071][0m Trial 30 finished with value: 0.32137270956356495 and parameters: {'observation_period_num': 24, 'train_rates': 0.6054478062710636, 'learning_rate': 3.0469354103069087e-06, 'batch_size': 59, 'step_size': 10, 'gamma': 0.8898495636932136}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 04:50:31,262][0m Trial 31 finished with value: 0.07924548298754591 and parameters: {'observation_period_num': 66, 'train_rates': 0.8374344957236279, 'learning_rate': 1.574034940642791e-05, 'batch_size': 40, 'step_size': 8, 'gamma': 0.8963132382889728}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 04:53:47,663][0m Trial 32 finished with value: 0.06526440534402024 and parameters: {'observation_period_num': 39, 'train_rates': 0.8795639044627528, 'learning_rate': 1.8540179110210277e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8941850451604895}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 04:56:52,193][0m Trial 33 finished with value: 0.052150640866052825 and parameters: {'observation_period_num': 39, 'train_rates': 0.8386529129465988, 'learning_rate': 2.2569425662038132e-05, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8955691572357262}. Best is trial 27 with value: 0.048998558633607664.[0m
[32m[I 2025-02-07 05:00:06,758][0m Trial 34 finished with value: 0.04821362208861571 and parameters: {'observation_period_num': 29, 'train_rates': 0.8671019459769416, 'learning_rate': 2.4005900239159505e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.873966371433001}. Best is trial 34 with value: 0.04821362208861571.[0m
[32m[I 2025-02-07 05:05:28,271][0m Trial 35 finished with value: 0.043842390333254315 and parameters: {'observation_period_num': 20, 'train_rates': 0.801881466360609, 'learning_rate': 2.7137581924992268e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.8753122823391885}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:06:38,943][0m Trial 36 finished with value: 0.18099644216871133 and parameters: {'observation_period_num': 5, 'train_rates': 0.7474737315134065, 'learning_rate': 2.6500178140628215e-05, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8755260089721959}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:08:22,652][0m Trial 37 finished with value: 0.049892239896761253 and parameters: {'observation_period_num': 23, 'train_rates': 0.8070763591141089, 'learning_rate': 5.1546547515332033e-05, 'batch_size': 52, 'step_size': 7, 'gamma': 0.8667580690039189}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:10:03,569][0m Trial 38 finished with value: 0.06241090362235694 and parameters: {'observation_period_num': 21, 'train_rates': 0.8066304000296642, 'learning_rate': 4.807590776965e-05, 'batch_size': 52, 'step_size': 5, 'gamma': 0.8184323755575105}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:11:34,767][0m Trial 39 finished with value: 0.18283492335492474 and parameters: {'observation_period_num': 81, 'train_rates': 0.6747638298591809, 'learning_rate': 4.9059891267225034e-05, 'batch_size': 50, 'step_size': 9, 'gamma': 0.8681280590347893}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:14:27,892][0m Trial 40 finished with value: 0.2435283150681831 and parameters: {'observation_period_num': 51, 'train_rates': 0.7243531774724582, 'learning_rate': 9.399422931250145e-06, 'batch_size': 28, 'step_size': 5, 'gamma': 0.8553039625276994}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:17:22,294][0m Trial 41 finished with value: 0.05722133339206864 and parameters: {'observation_period_num': 46, 'train_rates': 0.8036988346516506, 'learning_rate': 2.874160177421367e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8781722797355803}. Best is trial 35 with value: 0.043842390333254315.[0m
[32m[I 2025-02-07 05:22:52,228][0m Trial 42 finished with value: 0.039149412418742785 and parameters: {'observation_period_num': 17, 'train_rates': 0.8251580733646546, 'learning_rate': 2.0554711886799726e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9064393680863262}. Best is trial 42 with value: 0.039149412418742785.[0m
[32m[I 2025-02-07 05:27:59,064][0m Trial 43 finished with value: 0.06900070394688057 and parameters: {'observation_period_num': 21, 'train_rates': 0.8195435091565426, 'learning_rate': 5.5946130070972394e-06, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9102300451221973}. Best is trial 42 with value: 0.039149412418742785.[0m
[32m[I 2025-02-07 05:29:12,498][0m Trial 44 finished with value: 0.18073157394734712 and parameters: {'observation_period_num': 14, 'train_rates': 0.7791988812478584, 'learning_rate': 5.830056481996438e-05, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8786738601227085}. Best is trial 42 with value: 0.039149412418742785.[0m
[32m[I 2025-02-07 05:31:02,347][0m Trial 45 finished with value: 0.05168510396943301 and parameters: {'observation_period_num': 31, 'train_rates': 0.8673202401107936, 'learning_rate': 3.308430953508266e-05, 'batch_size': 50, 'step_size': 10, 'gamma': 0.8661100555352467}. Best is trial 42 with value: 0.039149412418742785.[0m
[32m[I 2025-02-07 05:33:43,411][0m Trial 46 finished with value: 0.030481836491683772 and parameters: {'observation_period_num': 13, 'train_rates': 0.8259861139647494, 'learning_rate': 9.446716545553754e-05, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8857143967075227}. Best is trial 46 with value: 0.030481836491683772.[0m
[32m[I 2025-02-07 05:35:56,124][0m Trial 47 finished with value: 0.18016912645176747 and parameters: {'observation_period_num': 49, 'train_rates': 0.7413962613225474, 'learning_rate': 8.548233450611262e-05, 'batch_size': 37, 'step_size': 5, 'gamma': 0.8844896528431244}. Best is trial 46 with value: 0.030481836491683772.[0m
[32m[I 2025-02-07 05:36:19,783][0m Trial 48 finished with value: 0.4686042789150687 and parameters: {'observation_period_num': 12, 'train_rates': 0.7922032429542303, 'learning_rate': 4.040956140606642e-06, 'batch_size': 252, 'step_size': 9, 'gamma': 0.9097854269476662}. Best is trial 46 with value: 0.030481836491683772.[0m
[32m[I 2025-02-07 05:39:54,551][0m Trial 49 finished with value: 0.1125617669038246 and parameters: {'observation_period_num': 87, 'train_rates': 0.8298010808857037, 'learning_rate': 8.731405273844296e-06, 'batch_size': 24, 'step_size': 8, 'gamma': 0.8523051578635387}. Best is trial 46 with value: 0.030481836491683772.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-07 05:39:54,564][0m A new study created in memory with name: no-name-798be1a7-8671-4f59-8166-b8623613f7fd[0m
[32m[I 2025-02-07 05:42:13,575][0m Trial 0 finished with value: 0.04817450791597366 and parameters: {'observation_period_num': 8, 'train_rates': 0.9881182614355224, 'learning_rate': 9.385317435374386e-05, 'batch_size': 44, 'step_size': 4, 'gamma': 0.7823049571484652}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:42:35,725][0m Trial 1 finished with value: 1.7744646486172007 and parameters: {'observation_period_num': 183, 'train_rates': 0.7384731503498925, 'learning_rate': 6.331859277965658e-06, 'batch_size': 248, 'step_size': 1, 'gamma': 0.9135931330535422}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:42:59,128][0m Trial 2 finished with value: 0.7486036583044683 and parameters: {'observation_period_num': 182, 'train_rates': 0.9052381111065854, 'learning_rate': 4.7787868289054e-06, 'batch_size': 253, 'step_size': 13, 'gamma': 0.8004094937227935}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:43:38,642][0m Trial 3 finished with value: 0.18343745640123077 and parameters: {'observation_period_num': 156, 'train_rates': 0.9319987970085466, 'learning_rate': 5.2889780203341374e-05, 'batch_size': 148, 'step_size': 14, 'gamma': 0.9215684345585995}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:44:05,670][0m Trial 4 finished with value: 0.7116178870201111 and parameters: {'observation_period_num': 93, 'train_rates': 0.9841235343950978, 'learning_rate': 2.2592253976697e-06, 'batch_size': 243, 'step_size': 11, 'gamma': 0.7673473411560722}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:44:33,959][0m Trial 5 finished with value: 0.23105806322906547 and parameters: {'observation_period_num': 22, 'train_rates': 0.61449588781646, 'learning_rate': 1.314246391195737e-05, 'batch_size': 173, 'step_size': 13, 'gamma': 0.8427113898283478}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:45:30,363][0m Trial 6 finished with value: 0.1663111803637257 and parameters: {'observation_period_num': 42, 'train_rates': 0.7174909273306355, 'learning_rate': 0.00023516480570627728, 'batch_size': 90, 'step_size': 9, 'gamma': 0.9871725897929041}. Best is trial 0 with value: 0.04817450791597366.[0m
Early stopping at epoch 69
[32m[I 2025-02-07 05:46:05,672][0m Trial 7 finished with value: 0.44342573570893773 and parameters: {'observation_period_num': 161, 'train_rates': 0.8083314135730326, 'learning_rate': 5.475350009565054e-05, 'batch_size': 105, 'step_size': 1, 'gamma': 0.8256522216746656}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:46:57,714][0m Trial 8 finished with value: 0.8833830055266371 and parameters: {'observation_period_num': 224, 'train_rates': 0.6045097883992412, 'learning_rate': 1.9635133186447616e-06, 'batch_size': 79, 'step_size': 12, 'gamma': 0.7871653956060798}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:50:34,511][0m Trial 9 finished with value: 0.1322164043136265 and parameters: {'observation_period_num': 238, 'train_rates': 0.8988501332797129, 'learning_rate': 3.891525576242922e-05, 'batch_size': 24, 'step_size': 6, 'gamma': 0.8854889498065832}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:56:01,866][0m Trial 10 finished with value: 0.06501521496316805 and parameters: {'observation_period_num': 83, 'train_rates': 0.8490635702972391, 'learning_rate': 0.0009742786215577398, 'batch_size': 16, 'step_size': 5, 'gamma': 0.755327784203912}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 05:59:31,276][0m Trial 11 finished with value: 0.08402480949696742 and parameters: {'observation_period_num': 81, 'train_rates': 0.8350934704576894, 'learning_rate': 0.0008934318310993609, 'batch_size': 25, 'step_size': 4, 'gamma': 0.7512022892868827}. Best is trial 0 with value: 0.04817450791597366.[0m
[32m[I 2025-02-07 06:01:35,067][0m Trial 12 finished with value: 0.025346247479319572 and parameters: {'observation_period_num': 5, 'train_rates': 0.9836321987401357, 'learning_rate': 0.0009519555487206153, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8104224188368452}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:03:19,069][0m Trial 13 finished with value: 0.046451669186353683 and parameters: {'observation_period_num': 6, 'train_rates': 0.985968455286064, 'learning_rate': 0.0001955273727625915, 'batch_size': 59, 'step_size': 3, 'gamma': 0.8246865181062879}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:05:02,463][0m Trial 14 finished with value: 0.070178821524407 and parameters: {'observation_period_num': 52, 'train_rates': 0.9355203274667697, 'learning_rate': 0.00023819684710396817, 'batch_size': 56, 'step_size': 7, 'gamma': 0.8412487520730613}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:05:55,089][0m Trial 15 finished with value: 0.13433685898780823 and parameters: {'observation_period_num': 114, 'train_rates': 0.9877499280253401, 'learning_rate': 0.0003104495423489689, 'batch_size': 118, 'step_size': 3, 'gamma': 0.8128823348728436}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:07:19,155][0m Trial 16 finished with value: 0.09960657291153752 and parameters: {'observation_period_num': 46, 'train_rates': 0.8703639384100512, 'learning_rate': 0.0004753217235444735, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8719169811130483}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:07:52,589][0m Trial 17 finished with value: 0.19366477797121923 and parameters: {'observation_period_num': 13, 'train_rates': 0.7363933506126098, 'learning_rate': 9.888929423036461e-05, 'batch_size': 158, 'step_size': 2, 'gamma': 0.8567847711514628}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:08:26,536][0m Trial 18 finished with value: 0.08350510158292625 and parameters: {'observation_period_num': 62, 'train_rates': 0.9365434313354053, 'learning_rate': 0.0001475044006321333, 'batch_size': 183, 'step_size': 7, 'gamma': 0.8956860353337679}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:09:10,742][0m Trial 19 finished with value: 0.17793542920308977 and parameters: {'observation_period_num': 31, 'train_rates': 0.7773261592064261, 'learning_rate': 0.0004927002997376311, 'batch_size': 121, 'step_size': 3, 'gamma': 0.9678998590489085}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:09:36,407][0m Trial 20 finished with value: 0.2399268742384655 and parameters: {'observation_period_num': 5, 'train_rates': 0.6586878296823583, 'learning_rate': 1.7375663328487924e-05, 'batch_size': 198, 'step_size': 5, 'gamma': 0.8135629777452866}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:11:42,719][0m Trial 21 finished with value: 0.05148677807301283 and parameters: {'observation_period_num': 8, 'train_rates': 0.9663154847674835, 'learning_rate': 0.00010991595509349186, 'batch_size': 48, 'step_size': 4, 'gamma': 0.7823232875803108}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:13:40,827][0m Trial 22 finished with value: 0.07848691512815288 and parameters: {'observation_period_num': 65, 'train_rates': 0.9579090975975076, 'learning_rate': 0.00047433622095991227, 'batch_size': 50, 'step_size': 3, 'gamma': 0.7877313349026406}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:15:47,937][0m Trial 23 finished with value: 0.055699319144090015 and parameters: {'observation_period_num': 34, 'train_rates': 0.8974293117612084, 'learning_rate': 7.270230738580825e-05, 'batch_size': 44, 'step_size': 5, 'gamma': 0.8306315628173371}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:16:57,513][0m Trial 24 finished with value: 0.2940721862188732 and parameters: {'observation_period_num': 108, 'train_rates': 0.954208765771099, 'learning_rate': 2.2285736431835165e-05, 'batch_size': 84, 'step_size': 7, 'gamma': 0.7995319964106056}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:18:01,255][0m Trial 25 finished with value: 0.0718262642621994 and parameters: {'observation_period_num': 21, 'train_rates': 0.9871946570672655, 'learning_rate': 0.00015492242155037825, 'batch_size': 99, 'step_size': 4, 'gamma': 0.7724543592904112}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:19:28,482][0m Trial 26 finished with value: 0.06997270891933065 and parameters: {'observation_period_num': 59, 'train_rates': 0.9216384915323992, 'learning_rate': 0.0006443757922058522, 'batch_size': 66, 'step_size': 2, 'gamma': 0.8621047781440391}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:21:56,716][0m Trial 27 finished with value: 0.03418528246227652 and parameters: {'observation_period_num': 5, 'train_rates': 0.8648346315225239, 'learning_rate': 0.0003146401883678537, 'batch_size': 37, 'step_size': 6, 'gamma': 0.8127065186067688}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:23:16,201][0m Trial 28 finished with value: 0.05861467794551478 and parameters: {'observation_period_num': 32, 'train_rates': 0.8684674675214444, 'learning_rate': 0.00029876632820175544, 'batch_size': 69, 'step_size': 9, 'gamma': 0.8183143130945567}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:25:37,419][0m Trial 29 finished with value: 0.16760742119239533 and parameters: {'observation_period_num': 7, 'train_rates': 0.7927262309204359, 'learning_rate': 0.0003464228643817688, 'batch_size': 37, 'step_size': 6, 'gamma': 0.8469238327593887}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:28:21,755][0m Trial 30 finished with value: 0.10364598834388693 and parameters: {'observation_period_num': 73, 'train_rates': 0.8771008898601698, 'learning_rate': 0.00016843390965915817, 'batch_size': 33, 'step_size': 8, 'gamma': 0.800815465028716}. Best is trial 12 with value: 0.025346247479319572.[0m
Early stopping at epoch 69
[32m[I 2025-02-07 06:29:31,716][0m Trial 31 finished with value: 0.17048650562763215 and parameters: {'observation_period_num': 28, 'train_rates': 0.9573765934818793, 'learning_rate': 9.178024901317105e-05, 'batch_size': 60, 'step_size': 1, 'gamma': 0.8295343801280047}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:32:06,774][0m Trial 32 finished with value: 0.03265528509527752 and parameters: {'observation_period_num': 5, 'train_rates': 0.9672122568561426, 'learning_rate': 0.0006765870111344533, 'batch_size': 39, 'step_size': 6, 'gamma': 0.8046536563499611}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:34:32,684][0m Trial 33 finished with value: 0.059717391413869336 and parameters: {'observation_period_num': 46, 'train_rates': 0.9120849297718696, 'learning_rate': 0.0006578407145795334, 'batch_size': 39, 'step_size': 6, 'gamma': 0.8024502174878902}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:39:45,599][0m Trial 34 finished with value: 0.041169395380549964 and parameters: {'observation_period_num': 16, 'train_rates': 0.9695270036076435, 'learning_rate': 0.000962333188299309, 'batch_size': 19, 'step_size': 8, 'gamma': 0.772625084669174}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:44:37,740][0m Trial 35 finished with value: 0.04161736370608358 and parameters: {'observation_period_num': 20, 'train_rates': 0.8282139401843928, 'learning_rate': 0.0009909467675700767, 'batch_size': 18, 'step_size': 10, 'gamma': 0.7716070736942279}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:47:15,865][0m Trial 36 finished with value: 0.16459744231786558 and parameters: {'observation_period_num': 134, 'train_rates': 0.9409525182360094, 'learning_rate': 0.0007286263959962395, 'batch_size': 36, 'step_size': 8, 'gamma': 0.767009387468964}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:50:14,329][0m Trial 37 finished with value: 0.07087890040129423 and parameters: {'observation_period_num': 38, 'train_rates': 0.8837774265294448, 'learning_rate': 7.670464834859089e-06, 'batch_size': 31, 'step_size': 10, 'gamma': 0.9353968402536732}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:50:41,452][0m Trial 38 finished with value: 0.20338496565818787 and parameters: {'observation_period_num': 201, 'train_rates': 0.9657945768573869, 'learning_rate': 0.00041719075022331165, 'batch_size': 232, 'step_size': 7, 'gamma': 0.7923410187826041}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:51:56,360][0m Trial 39 finished with value: 0.04117064012421502 and parameters: {'observation_period_num': 20, 'train_rates': 0.914445958489324, 'learning_rate': 0.0006253179471685012, 'batch_size': 77, 'step_size': 6, 'gamma': 0.7621984145567151}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:52:32,028][0m Trial 40 finished with value: 0.19521777850056457 and parameters: {'observation_period_num': 101, 'train_rates': 0.6929454130805407, 'learning_rate': 0.00029290068642717803, 'batch_size': 134, 'step_size': 15, 'gamma': 0.778873454194029}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:53:52,491][0m Trial 41 finished with value: 0.9470368292866921 and parameters: {'observation_period_num': 19, 'train_rates': 0.9169113365252303, 'learning_rate': 1.2839587429460307e-06, 'batch_size': 76, 'step_size': 6, 'gamma': 0.754975089860013}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 06:55:52,450][0m Trial 42 finished with value: 0.03633255009172541 and parameters: {'observation_period_num': 23, 'train_rates': 0.944080347772089, 'learning_rate': 0.0006437341781418512, 'batch_size': 49, 'step_size': 5, 'gamma': 0.7617347517629736}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:01:58,792][0m Trial 43 finished with value: 0.03507155054188394 and parameters: {'observation_period_num': 39, 'train_rates': 0.9736379777115107, 'learning_rate': 0.0007381374991761411, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8028318754133362}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:03:58,277][0m Trial 44 finished with value: 0.05385886902946073 and parameters: {'observation_period_num': 41, 'train_rates': 0.9494888442601084, 'learning_rate': 0.0005945016613065891, 'batch_size': 49, 'step_size': 5, 'gamma': 0.8089973493020953}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:04:59,607][0m Trial 45 finished with value: 0.1225333958864212 and parameters: {'observation_period_num': 146, 'train_rates': 0.9717443241526152, 'learning_rate': 0.0004382975059983582, 'batch_size': 95, 'step_size': 5, 'gamma': 0.842462268083475}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:08:10,564][0m Trial 46 finished with value: 0.05653372253455109 and parameters: {'observation_period_num': 54, 'train_rates': 0.9317320884775893, 'learning_rate': 0.00023238057471228419, 'batch_size': 30, 'step_size': 4, 'gamma': 0.7908825158871597}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:09:50,297][0m Trial 47 finished with value: 0.07956900944312413 and parameters: {'observation_period_num': 26, 'train_rates': 0.8528956537263742, 'learning_rate': 0.0007409592952500504, 'batch_size': 55, 'step_size': 6, 'gamma': 0.8346252940990158}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:13:04,316][0m Trial 48 finished with value: 0.1575146731389656 and parameters: {'observation_period_num': 179, 'train_rates': 0.8895951882650047, 'learning_rate': 0.0003522355869335916, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8186428776297103}. Best is trial 12 with value: 0.025346247479319572.[0m
[32m[I 2025-02-07 07:15:28,165][0m Trial 49 finished with value: 0.14387049029270807 and parameters: {'observation_period_num': 90, 'train_rates': 0.9427576809603987, 'learning_rate': 0.0007989404918510135, 'batch_size': 40, 'step_size': 5, 'gamma': 0.8759729819965345}. Best is trial 12 with value: 0.025346247479319572.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 8, 'train_rates': 0.798702233718772, 'learning_rate': 0.0005953105169070539, 'batch_size': 30, 'step_size': 14, 'gamma': 0.8019475470230012}
Epoch 1/300, trend Loss: 0.2161 | 0.0915
Epoch 2/300, trend Loss: 0.1192 | 0.0756
Epoch 3/300, trend Loss: 0.1099 | 0.0637
Epoch 4/300, trend Loss: 0.1031 | 0.0592
Epoch 5/300, trend Loss: 0.0967 | 0.0678
Epoch 6/300, trend Loss: 0.0938 | 0.0721
Epoch 7/300, trend Loss: 0.0911 | 0.0660
Epoch 8/300, trend Loss: 0.0879 | 0.0494
Epoch 9/300, trend Loss: 0.0850 | 0.0476
Epoch 10/300, trend Loss: 0.0823 | 0.0451
Epoch 11/300, trend Loss: 0.0798 | 0.0456
Epoch 12/300, trend Loss: 0.0779 | 0.0445
Epoch 13/300, trend Loss: 0.0766 | 0.0423
Epoch 14/300, trend Loss: 0.0750 | 0.0403
Epoch 15/300, trend Loss: 0.0741 | 0.0411
Epoch 16/300, trend Loss: 0.0727 | 0.0412
Epoch 17/300, trend Loss: 0.0714 | 0.0369
Epoch 18/300, trend Loss: 0.0714 | 0.0366
Epoch 19/300, trend Loss: 0.0688 | 0.0395
Epoch 20/300, trend Loss: 0.0694 | 0.0426
Epoch 21/300, trend Loss: 0.0685 | 0.0402
Epoch 22/300, trend Loss: 0.0664 | 0.0378
Epoch 23/300, trend Loss: 0.0670 | 0.0382
Epoch 24/300, trend Loss: 0.0659 | 0.0408
Epoch 25/300, trend Loss: 0.0655 | 0.0414
Epoch 26/300, trend Loss: 0.0648 | 0.0436
Epoch 27/300, trend Loss: 0.0649 | 0.0438
Epoch 28/300, trend Loss: 0.0658 | 0.0479
Epoch 29/300, trend Loss: 0.0604 | 0.0391
Epoch 30/300, trend Loss: 0.0586 | 0.0363
Epoch 31/300, trend Loss: 0.0661 | 0.0315
Epoch 32/300, trend Loss: 0.0625 | 0.0331
Epoch 33/300, trend Loss: 0.0616 | 0.0311
Epoch 34/300, trend Loss: 0.0610 | 0.0334
Epoch 35/300, trend Loss: 0.0614 | 0.0333
Epoch 36/300, trend Loss: 0.0618 | 0.0379
Epoch 37/300, trend Loss: 0.0619 | 0.0364
Epoch 38/300, trend Loss: 0.0615 | 0.0373
Epoch 39/300, trend Loss: 0.0592 | 0.0401
Epoch 40/300, trend Loss: 0.0614 | 0.0436
Epoch 41/300, trend Loss: 0.0614 | 0.0525
Epoch 42/300, trend Loss: 0.0613 | 0.0443
Epoch 43/300, trend Loss: 0.0676 | 0.0430
Epoch 44/300, trend Loss: 0.0635 | 0.0591
Epoch 45/300, trend Loss: 0.0609 | 0.0479
Epoch 46/300, trend Loss: 0.0765 | 0.0340
Epoch 47/300, trend Loss: 0.0583 | 0.0395
Epoch 48/300, trend Loss: 0.0614 | 0.0375
Epoch 49/300, trend Loss: 0.0597 | 0.0356
Epoch 50/300, trend Loss: 0.0515 | 0.0368
Epoch 51/300, trend Loss: 0.0516 | 0.0328
Epoch 52/300, trend Loss: 0.0499 | 0.0333
Epoch 53/300, trend Loss: 0.0509 | 0.0343
Epoch 54/300, trend Loss: 0.0490 | 0.0338
Epoch 55/300, trend Loss: 0.0487 | 0.0302
Epoch 56/300, trend Loss: 0.0474 | 0.0320
Epoch 57/300, trend Loss: 0.0469 | 0.0301
Epoch 58/300, trend Loss: 0.0465 | 0.0307
Epoch 59/300, trend Loss: 0.0461 | 0.0298
Epoch 60/300, trend Loss: 0.0459 | 0.0303
Epoch 61/300, trend Loss: 0.0454 | 0.0298
Epoch 62/300, trend Loss: 0.0454 | 0.0301
Epoch 63/300, trend Loss: 0.0448 | 0.0298
Epoch 64/300, trend Loss: 0.0447 | 0.0299
Epoch 65/300, trend Loss: 0.0441 | 0.0295
Epoch 66/300, trend Loss: 0.0440 | 0.0294
Epoch 67/300, trend Loss: 0.0436 | 0.0295
Epoch 68/300, trend Loss: 0.0434 | 0.0294
Epoch 69/300, trend Loss: 0.0432 | 0.0296
Epoch 70/300, trend Loss: 0.0430 | 0.0295
Epoch 71/300, trend Loss: 0.0427 | 0.0295
Epoch 72/300, trend Loss: 0.0425 | 0.0294
Epoch 73/300, trend Loss: 0.0422 | 0.0293
Epoch 74/300, trend Loss: 0.0422 | 0.0295
Epoch 75/300, trend Loss: 0.0419 | 0.0294
Epoch 76/300, trend Loss: 0.0419 | 0.0295
Epoch 77/300, trend Loss: 0.0415 | 0.0295
Epoch 78/300, trend Loss: 0.0416 | 0.0296
Epoch 79/300, trend Loss: 0.0412 | 0.0295
Epoch 80/300, trend Loss: 0.0412 | 0.0296
Epoch 81/300, trend Loss: 0.0408 | 0.0297
Epoch 82/300, trend Loss: 0.0407 | 0.0298
Epoch 83/300, trend Loss: 0.0403 | 0.0299
Epoch 84/300, trend Loss: 0.0403 | 0.0300
Epoch 85/300, trend Loss: 0.0400 | 0.0303
Epoch 86/300, trend Loss: 0.0400 | 0.0302
Epoch 87/300, trend Loss: 0.0397 | 0.0305
Epoch 88/300, trend Loss: 0.0396 | 0.0305
Epoch 89/300, trend Loss: 0.0394 | 0.0308
Epoch 90/300, trend Loss: 0.0394 | 0.0308
Epoch 91/300, trend Loss: 0.0391 | 0.0311
Epoch 92/300, trend Loss: 0.0391 | 0.0313
Epoch 93/300, trend Loss: 0.0389 | 0.0313
Epoch 94/300, trend Loss: 0.0390 | 0.0315
Epoch 95/300, trend Loss: 0.0388 | 0.0315
Epoch 96/300, trend Loss: 0.0388 | 0.0317
Epoch 97/300, trend Loss: 0.0385 | 0.0317
Epoch 98/300, trend Loss: 0.0384 | 0.0319
Epoch 99/300, trend Loss: 0.0382 | 0.0321
Epoch 100/300, trend Loss: 0.0381 | 0.0321
Epoch 101/300, trend Loss: 0.0380 | 0.0323
Epoch 102/300, trend Loss: 0.0379 | 0.0323
Epoch 103/300, trend Loss: 0.0378 | 0.0324
Epoch 104/300, trend Loss: 0.0377 | 0.0324
Epoch 105/300, trend Loss: 0.0376 | 0.0325
Epoch 106/300, trend Loss: 0.0375 | 0.0322
Epoch 107/300, trend Loss: 0.0374 | 0.0323
Epoch 108/300, trend Loss: 0.0373 | 0.0323
Epoch 109/300, trend Loss: 0.0372 | 0.0324
Epoch 110/300, trend Loss: 0.0370 | 0.0324
Epoch 111/300, trend Loss: 0.0369 | 0.0325
Epoch 112/300, trend Loss: 0.0367 | 0.0325
Epoch 113/300, trend Loss: 0.0366 | 0.0324
Epoch 114/300, trend Loss: 0.0366 | 0.0323
Epoch 115/300, trend Loss: 0.0365 | 0.0324
Epoch 116/300, trend Loss: 0.0364 | 0.0324
Epoch 117/300, trend Loss: 0.0363 | 0.0325
Epoch 118/300, trend Loss: 0.0362 | 0.0325
Epoch 119/300, trend Loss: 0.0361 | 0.0326
Epoch 120/300, trend Loss: 0.0361 | 0.0332
Epoch 121/300, trend Loss: 0.0360 | 0.0335
Epoch 122/300, trend Loss: 0.0359 | 0.0337
Epoch 123/300, trend Loss: 0.0358 | 0.0338
Epoch 124/300, trend Loss: 0.0357 | 0.0340
Epoch 125/300, trend Loss: 0.0356 | 0.0341
Epoch 126/300, trend Loss: 0.0355 | 0.0342
Epoch 127/300, trend Loss: 0.0355 | 0.0340
Epoch 128/300, trend Loss: 0.0355 | 0.0341
Epoch 129/300, trend Loss: 0.0354 | 0.0341
Epoch 130/300, trend Loss: 0.0354 | 0.0340
Epoch 131/300, trend Loss: 0.0354 | 0.0339
Epoch 132/300, trend Loss: 0.0354 | 0.0339
Epoch 133/300, trend Loss: 0.0353 | 0.0338
Epoch 134/300, trend Loss: 0.0353 | 0.0333
Epoch 135/300, trend Loss: 0.0353 | 0.0333
Epoch 136/300, trend Loss: 0.0352 | 0.0333
Epoch 137/300, trend Loss: 0.0351 | 0.0333
Epoch 138/300, trend Loss: 0.0351 | 0.0333
Epoch 139/300, trend Loss: 0.0350 | 0.0334
Epoch 140/300, trend Loss: 0.0349 | 0.0334
Epoch 141/300, trend Loss: 0.0348 | 0.0334
Epoch 142/300, trend Loss: 0.0348 | 0.0334
Epoch 143/300, trend Loss: 0.0348 | 0.0334
Epoch 144/300, trend Loss: 0.0347 | 0.0335
Epoch 145/300, trend Loss: 0.0347 | 0.0335
Epoch 146/300, trend Loss: 0.0346 | 0.0336
Epoch 147/300, trend Loss: 0.0346 | 0.0336
Epoch 148/300, trend Loss: 0.0346 | 0.0337
Epoch 149/300, trend Loss: 0.0345 | 0.0337
Epoch 150/300, trend Loss: 0.0345 | 0.0337
Epoch 151/300, trend Loss: 0.0345 | 0.0338
Epoch 152/300, trend Loss: 0.0345 | 0.0338
Epoch 153/300, trend Loss: 0.0344 | 0.0338
Epoch 154/300, trend Loss: 0.0344 | 0.0339
Epoch 155/300, trend Loss: 0.0344 | 0.0339
Epoch 156/300, trend Loss: 0.0344 | 0.0339
Epoch 157/300, trend Loss: 0.0343 | 0.0340
Epoch 158/300, trend Loss: 0.0343 | 0.0340
Epoch 159/300, trend Loss: 0.0343 | 0.0340
Epoch 160/300, trend Loss: 0.0343 | 0.0340
Epoch 161/300, trend Loss: 0.0343 | 0.0340
Epoch 162/300, trend Loss: 0.0342 | 0.0341
Epoch 163/300, trend Loss: 0.0342 | 0.0341
Epoch 164/300, trend Loss: 0.0342 | 0.0341
Epoch 165/300, trend Loss: 0.0342 | 0.0341
Epoch 166/300, trend Loss: 0.0342 | 0.0341
Epoch 167/300, trend Loss: 0.0342 | 0.0341
Epoch 168/300, trend Loss: 0.0341 | 0.0341
Epoch 169/300, trend Loss: 0.0341 | 0.0341
Epoch 170/300, trend Loss: 0.0341 | 0.0341
Epoch 171/300, trend Loss: 0.0341 | 0.0341
Epoch 172/300, trend Loss: 0.0341 | 0.0341
Epoch 173/300, trend Loss: 0.0341 | 0.0342
Epoch 174/300, trend Loss: 0.0340 | 0.0342
Epoch 175/300, trend Loss: 0.0340 | 0.0342
Epoch 176/300, trend Loss: 0.0340 | 0.0342
Epoch 177/300, trend Loss: 0.0340 | 0.0342
Epoch 178/300, trend Loss: 0.0340 | 0.0342
Epoch 179/300, trend Loss: 0.0340 | 0.0342
Epoch 180/300, trend Loss: 0.0339 | 0.0342
Epoch 181/300, trend Loss: 0.0339 | 0.0342
Epoch 182/300, trend Loss: 0.0339 | 0.0342
Epoch 183/300, trend Loss: 0.0339 | 0.0342
Epoch 184/300, trend Loss: 0.0339 | 0.0342
Epoch 185/300, trend Loss: 0.0339 | 0.0343
Epoch 186/300, trend Loss: 0.0339 | 0.0343
Epoch 187/300, trend Loss: 0.0339 | 0.0343
Epoch 188/300, trend Loss: 0.0338 | 0.0343
Epoch 189/300, trend Loss: 0.0338 | 0.0343
Epoch 190/300, trend Loss: 0.0338 | 0.0343
Epoch 191/300, trend Loss: 0.0338 | 0.0343
Epoch 192/300, trend Loss: 0.0338 | 0.0343
Epoch 193/300, trend Loss: 0.0338 | 0.0343
Epoch 194/300, trend Loss: 0.0338 | 0.0343
Epoch 195/300, trend Loss: 0.0338 | 0.0343
Epoch 196/300, trend Loss: 0.0338 | 0.0343
Epoch 197/300, trend Loss: 0.0338 | 0.0343
Epoch 198/300, trend Loss: 0.0337 | 0.0343
Epoch 199/300, trend Loss: 0.0337 | 0.0343
Epoch 200/300, trend Loss: 0.0337 | 0.0343
Epoch 201/300, trend Loss: 0.0337 | 0.0344
Epoch 202/300, trend Loss: 0.0337 | 0.0344
Epoch 203/300, trend Loss: 0.0337 | 0.0344
Epoch 204/300, trend Loss: 0.0337 | 0.0344
Epoch 205/300, trend Loss: 0.0337 | 0.0344
Epoch 206/300, trend Loss: 0.0337 | 0.0344
Epoch 207/300, trend Loss: 0.0337 | 0.0344
Epoch 208/300, trend Loss: 0.0337 | 0.0344
Epoch 209/300, trend Loss: 0.0337 | 0.0344
Epoch 210/300, trend Loss: 0.0337 | 0.0344
Epoch 211/300, trend Loss: 0.0337 | 0.0344
Epoch 212/300, trend Loss: 0.0337 | 0.0344
Epoch 213/300, trend Loss: 0.0337 | 0.0344
Epoch 214/300, trend Loss: 0.0336 | 0.0344
Epoch 215/300, trend Loss: 0.0336 | 0.0344
Epoch 216/300, trend Loss: 0.0336 | 0.0344
Epoch 217/300, trend Loss: 0.0336 | 0.0344
Epoch 218/300, trend Loss: 0.0336 | 0.0344
Epoch 219/300, trend Loss: 0.0336 | 0.0344
Epoch 220/300, trend Loss: 0.0336 | 0.0344
Epoch 221/300, trend Loss: 0.0336 | 0.0344
Epoch 222/300, trend Loss: 0.0336 | 0.0344
Epoch 223/300, trend Loss: 0.0336 | 0.0344
Epoch 224/300, trend Loss: 0.0336 | 0.0344
Epoch 225/300, trend Loss: 0.0336 | 0.0344
Epoch 226/300, trend Loss: 0.0336 | 0.0344
Epoch 227/300, trend Loss: 0.0336 | 0.0344
Epoch 228/300, trend Loss: 0.0336 | 0.0344
Epoch 229/300, trend Loss: 0.0336 | 0.0344
Epoch 230/300, trend Loss: 0.0336 | 0.0344
Epoch 231/300, trend Loss: 0.0336 | 0.0344
Epoch 232/300, trend Loss: 0.0336 | 0.0344
Epoch 233/300, trend Loss: 0.0336 | 0.0344
Epoch 234/300, trend Loss: 0.0336 | 0.0344
Epoch 235/300, trend Loss: 0.0336 | 0.0344
Epoch 236/300, trend Loss: 0.0336 | 0.0344
Epoch 237/300, trend Loss: 0.0336 | 0.0344
Epoch 238/300, trend Loss: 0.0336 | 0.0344
Epoch 239/300, trend Loss: 0.0336 | 0.0344
Epoch 240/300, trend Loss: 0.0336 | 0.0344
Epoch 241/300, trend Loss: 0.0335 | 0.0344
Epoch 242/300, trend Loss: 0.0335 | 0.0344
Epoch 243/300, trend Loss: 0.0335 | 0.0344
Epoch 244/300, trend Loss: 0.0335 | 0.0344
Epoch 245/300, trend Loss: 0.0335 | 0.0344
Epoch 246/300, trend Loss: 0.0335 | 0.0344
Epoch 247/300, trend Loss: 0.0335 | 0.0344
Epoch 248/300, trend Loss: 0.0335 | 0.0344
Epoch 249/300, trend Loss: 0.0335 | 0.0344
Epoch 250/300, trend Loss: 0.0335 | 0.0344
Epoch 251/300, trend Loss: 0.0335 | 0.0344
Epoch 252/300, trend Loss: 0.0335 | 0.0344
Epoch 253/300, trend Loss: 0.0335 | 0.0344
Epoch 254/300, trend Loss: 0.0335 | 0.0344
Epoch 255/300, trend Loss: 0.0335 | 0.0344
Epoch 256/300, trend Loss: 0.0335 | 0.0344
Epoch 257/300, trend Loss: 0.0335 | 0.0344
Epoch 258/300, trend Loss: 0.0335 | 0.0344
Epoch 259/300, trend Loss: 0.0335 | 0.0345
Epoch 260/300, trend Loss: 0.0335 | 0.0345
Epoch 261/300, trend Loss: 0.0335 | 0.0345
Epoch 262/300, trend Loss: 0.0335 | 0.0345
Epoch 263/300, trend Loss: 0.0335 | 0.0345
Epoch 264/300, trend Loss: 0.0335 | 0.0345
Epoch 265/300, trend Loss: 0.0335 | 0.0345
Epoch 266/300, trend Loss: 0.0335 | 0.0345
Epoch 267/300, trend Loss: 0.0335 | 0.0345
Epoch 268/300, trend Loss: 0.0335 | 0.0345
Epoch 269/300, trend Loss: 0.0335 | 0.0345
Epoch 270/300, trend Loss: 0.0335 | 0.0345
Epoch 271/300, trend Loss: 0.0335 | 0.0345
Epoch 272/300, trend Loss: 0.0335 | 0.0345
Epoch 273/300, trend Loss: 0.0335 | 0.0345
Epoch 274/300, trend Loss: 0.0335 | 0.0345
Epoch 275/300, trend Loss: 0.0335 | 0.0345
Epoch 276/300, trend Loss: 0.0335 | 0.0345
Epoch 277/300, trend Loss: 0.0335 | 0.0345
Epoch 278/300, trend Loss: 0.0335 | 0.0345
Epoch 279/300, trend Loss: 0.0335 | 0.0345
Epoch 280/300, trend Loss: 0.0335 | 0.0345
Epoch 281/300, trend Loss: 0.0335 | 0.0345
Epoch 282/300, trend Loss: 0.0335 | 0.0345
Epoch 283/300, trend Loss: 0.0335 | 0.0345
Epoch 284/300, trend Loss: 0.0335 | 0.0345
Epoch 285/300, trend Loss: 0.0335 | 0.0345
Epoch 286/300, trend Loss: 0.0335 | 0.0345
Epoch 287/300, trend Loss: 0.0335 | 0.0345
Epoch 288/300, trend Loss: 0.0335 | 0.0345
Epoch 289/300, trend Loss: 0.0335 | 0.0345
Epoch 290/300, trend Loss: 0.0335 | 0.0345
Epoch 291/300, trend Loss: 0.0335 | 0.0345
Epoch 292/300, trend Loss: 0.0335 | 0.0345
Epoch 293/300, trend Loss: 0.0335 | 0.0345
Epoch 294/300, trend Loss: 0.0335 | 0.0345
Epoch 295/300, trend Loss: 0.0335 | 0.0345
Epoch 296/300, trend Loss: 0.0335 | 0.0345
Epoch 297/300, trend Loss: 0.0335 | 0.0345
Epoch 298/300, trend Loss: 0.0335 | 0.0345
Epoch 299/300, trend Loss: 0.0335 | 0.0345
Epoch 300/300, trend Loss: 0.0335 | 0.0345
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.9045110968092512, 'learning_rate': 3.3497730376660215e-05, 'batch_size': 39, 'step_size': 13, 'gamma': 0.9701412287915591}
Epoch 1/300, seasonal_0 Loss: 0.2261 | 0.1984
Epoch 2/300, seasonal_0 Loss: 0.1457 | 0.1471
Epoch 3/300, seasonal_0 Loss: 0.1347 | 0.1285
Epoch 4/300, seasonal_0 Loss: 0.1282 | 0.1152
Epoch 5/300, seasonal_0 Loss: 0.1232 | 0.1054
Epoch 6/300, seasonal_0 Loss: 0.1192 | 0.0979
Epoch 7/300, seasonal_0 Loss: 0.1160 | 0.0922
Epoch 8/300, seasonal_0 Loss: 0.1135 | 0.0880
Epoch 9/300, seasonal_0 Loss: 0.1115 | 0.0851
Epoch 10/300, seasonal_0 Loss: 0.1099 | 0.0830
Epoch 11/300, seasonal_0 Loss: 0.1084 | 0.0813
Epoch 12/300, seasonal_0 Loss: 0.1070 | 0.0799
Epoch 13/300, seasonal_0 Loss: 0.1057 | 0.0786
Epoch 14/300, seasonal_0 Loss: 0.1043 | 0.0774
Epoch 15/300, seasonal_0 Loss: 0.1030 | 0.0763
Epoch 16/300, seasonal_0 Loss: 0.1017 | 0.0752
Epoch 17/300, seasonal_0 Loss: 0.1003 | 0.0739
Epoch 18/300, seasonal_0 Loss: 0.0990 | 0.0725
Epoch 19/300, seasonal_0 Loss: 0.0975 | 0.0710
Epoch 20/300, seasonal_0 Loss: 0.0961 | 0.0694
Epoch 21/300, seasonal_0 Loss: 0.0947 | 0.0679
Epoch 22/300, seasonal_0 Loss: 0.0934 | 0.0664
Epoch 23/300, seasonal_0 Loss: 0.0923 | 0.0650
Epoch 24/300, seasonal_0 Loss: 0.0912 | 0.0637
Epoch 25/300, seasonal_0 Loss: 0.0903 | 0.0625
Epoch 26/300, seasonal_0 Loss: 0.0895 | 0.0613
Epoch 27/300, seasonal_0 Loss: 0.0887 | 0.0602
Epoch 28/300, seasonal_0 Loss: 0.0879 | 0.0591
Epoch 29/300, seasonal_0 Loss: 0.0872 | 0.0581
Epoch 30/300, seasonal_0 Loss: 0.0865 | 0.0570
Epoch 31/300, seasonal_0 Loss: 0.0859 | 0.0560
Epoch 32/300, seasonal_0 Loss: 0.0852 | 0.0550
Epoch 33/300, seasonal_0 Loss: 0.0847 | 0.0542
Epoch 34/300, seasonal_0 Loss: 0.0841 | 0.0536
Epoch 35/300, seasonal_0 Loss: 0.0837 | 0.0529
Epoch 36/300, seasonal_0 Loss: 0.0832 | 0.0522
Epoch 37/300, seasonal_0 Loss: 0.0827 | 0.0517
Epoch 38/300, seasonal_0 Loss: 0.0822 | 0.0511
Epoch 39/300, seasonal_0 Loss: 0.0818 | 0.0507
Epoch 40/300, seasonal_0 Loss: 0.0813 | 0.0502
Epoch 41/300, seasonal_0 Loss: 0.0808 | 0.0498
Epoch 42/300, seasonal_0 Loss: 0.0803 | 0.0495
Epoch 43/300, seasonal_0 Loss: 0.0799 | 0.0493
Epoch 44/300, seasonal_0 Loss: 0.0794 | 0.0491
Epoch 45/300, seasonal_0 Loss: 0.0790 | 0.0488
Epoch 46/300, seasonal_0 Loss: 0.0787 | 0.0486
Epoch 47/300, seasonal_0 Loss: 0.0783 | 0.0482
Epoch 48/300, seasonal_0 Loss: 0.0780 | 0.0479
Epoch 49/300, seasonal_0 Loss: 0.0776 | 0.0476
Epoch 50/300, seasonal_0 Loss: 0.0773 | 0.0473
Epoch 51/300, seasonal_0 Loss: 0.0770 | 0.0471
Epoch 52/300, seasonal_0 Loss: 0.0767 | 0.0468
Epoch 53/300, seasonal_0 Loss: 0.0764 | 0.0464
Epoch 54/300, seasonal_0 Loss: 0.0762 | 0.0462
Epoch 55/300, seasonal_0 Loss: 0.0760 | 0.0460
Epoch 56/300, seasonal_0 Loss: 0.0758 | 0.0458
Epoch 57/300, seasonal_0 Loss: 0.0756 | 0.0457
Epoch 58/300, seasonal_0 Loss: 0.0754 | 0.0455
Epoch 59/300, seasonal_0 Loss: 0.0752 | 0.0454
Epoch 60/300, seasonal_0 Loss: 0.0750 | 0.0452
Epoch 61/300, seasonal_0 Loss: 0.0748 | 0.0452
Epoch 62/300, seasonal_0 Loss: 0.0746 | 0.0451
Epoch 63/300, seasonal_0 Loss: 0.0744 | 0.0450
Epoch 64/300, seasonal_0 Loss: 0.0742 | 0.0450
Epoch 65/300, seasonal_0 Loss: 0.0740 | 0.0450
Epoch 66/300, seasonal_0 Loss: 0.0738 | 0.0449
Epoch 67/300, seasonal_0 Loss: 0.0736 | 0.0448
Epoch 68/300, seasonal_0 Loss: 0.0735 | 0.0447
Epoch 69/300, seasonal_0 Loss: 0.0733 | 0.0445
Epoch 70/300, seasonal_0 Loss: 0.0731 | 0.0443
Epoch 71/300, seasonal_0 Loss: 0.0729 | 0.0440
Epoch 72/300, seasonal_0 Loss: 0.0727 | 0.0438
Epoch 73/300, seasonal_0 Loss: 0.0725 | 0.0437
Epoch 74/300, seasonal_0 Loss: 0.0723 | 0.0435
Epoch 75/300, seasonal_0 Loss: 0.0722 | 0.0433
Epoch 76/300, seasonal_0 Loss: 0.0720 | 0.0432
Epoch 77/300, seasonal_0 Loss: 0.0718 | 0.0431
Epoch 78/300, seasonal_0 Loss: 0.0717 | 0.0430
Epoch 79/300, seasonal_0 Loss: 0.0715 | 0.0430
Epoch 80/300, seasonal_0 Loss: 0.0714 | 0.0430
Epoch 81/300, seasonal_0 Loss: 0.0713 | 0.0429
Epoch 82/300, seasonal_0 Loss: 0.0712 | 0.0429
Epoch 83/300, seasonal_0 Loss: 0.0711 | 0.0430
Epoch 84/300, seasonal_0 Loss: 0.0710 | 0.0429
Epoch 85/300, seasonal_0 Loss: 0.0709 | 0.0427
Epoch 86/300, seasonal_0 Loss: 0.0709 | 0.0420
Epoch 87/300, seasonal_0 Loss: 0.0707 | 0.0419
Epoch 88/300, seasonal_0 Loss: 0.0706 | 0.0418
Epoch 89/300, seasonal_0 Loss: 0.0705 | 0.0417
Epoch 90/300, seasonal_0 Loss: 0.0704 | 0.0417
Epoch 91/300, seasonal_0 Loss: 0.0704 | 0.0416
Epoch 92/300, seasonal_0 Loss: 0.0703 | 0.0417
Epoch 93/300, seasonal_0 Loss: 0.0703 | 0.0417
Epoch 94/300, seasonal_0 Loss: 0.0701 | 0.0417
Epoch 95/300, seasonal_0 Loss: 0.0700 | 0.0417
Epoch 96/300, seasonal_0 Loss: 0.0698 | 0.0417
Epoch 97/300, seasonal_0 Loss: 0.0697 | 0.0417
Epoch 98/300, seasonal_0 Loss: 0.0696 | 0.0416
Epoch 99/300, seasonal_0 Loss: 0.0695 | 0.0417
Epoch 100/300, seasonal_0 Loss: 0.0693 | 0.0416
Epoch 101/300, seasonal_0 Loss: 0.0692 | 0.0416
Epoch 102/300, seasonal_0 Loss: 0.0691 | 0.0415
Epoch 103/300, seasonal_0 Loss: 0.0690 | 0.0415
Epoch 104/300, seasonal_0 Loss: 0.0689 | 0.0414
Epoch 105/300, seasonal_0 Loss: 0.0688 | 0.0415
Epoch 106/300, seasonal_0 Loss: 0.0687 | 0.0414
Epoch 107/300, seasonal_0 Loss: 0.0686 | 0.0414
Epoch 108/300, seasonal_0 Loss: 0.0686 | 0.0413
Epoch 109/300, seasonal_0 Loss: 0.0685 | 0.0413
Epoch 110/300, seasonal_0 Loss: 0.0684 | 0.0412
Epoch 111/300, seasonal_0 Loss: 0.0683 | 0.0412
Epoch 112/300, seasonal_0 Loss: 0.0682 | 0.0412
Epoch 113/300, seasonal_0 Loss: 0.0681 | 0.0411
Epoch 114/300, seasonal_0 Loss: 0.0681 | 0.0411
Epoch 115/300, seasonal_0 Loss: 0.0680 | 0.0410
Epoch 116/300, seasonal_0 Loss: 0.0679 | 0.0410
Epoch 117/300, seasonal_0 Loss: 0.0678 | 0.0410
Epoch 118/300, seasonal_0 Loss: 0.0678 | 0.0409
Epoch 119/300, seasonal_0 Loss: 0.0677 | 0.0408
Epoch 120/300, seasonal_0 Loss: 0.0676 | 0.0408
Epoch 121/300, seasonal_0 Loss: 0.0675 | 0.0408
Epoch 122/300, seasonal_0 Loss: 0.0675 | 0.0407
Epoch 123/300, seasonal_0 Loss: 0.0674 | 0.0407
Epoch 124/300, seasonal_0 Loss: 0.0673 | 0.0407
Epoch 125/300, seasonal_0 Loss: 0.0673 | 0.0406
Epoch 126/300, seasonal_0 Loss: 0.0672 | 0.0405
Epoch 127/300, seasonal_0 Loss: 0.0671 | 0.0405
Epoch 128/300, seasonal_0 Loss: 0.0671 | 0.0404
Epoch 129/300, seasonal_0 Loss: 0.0670 | 0.0404
Epoch 130/300, seasonal_0 Loss: 0.0669 | 0.0404
Epoch 131/300, seasonal_0 Loss: 0.0669 | 0.0403
Epoch 132/300, seasonal_0 Loss: 0.0668 | 0.0402
Epoch 133/300, seasonal_0 Loss: 0.0667 | 0.0402
Epoch 134/300, seasonal_0 Loss: 0.0667 | 0.0401
Epoch 135/300, seasonal_0 Loss: 0.0666 | 0.0401
Epoch 136/300, seasonal_0 Loss: 0.0666 | 0.0401
Epoch 137/300, seasonal_0 Loss: 0.0665 | 0.0401
Epoch 138/300, seasonal_0 Loss: 0.0664 | 0.0400
Epoch 139/300, seasonal_0 Loss: 0.0664 | 0.0399
Epoch 140/300, seasonal_0 Loss: 0.0663 | 0.0399
Epoch 141/300, seasonal_0 Loss: 0.0663 | 0.0399
Epoch 142/300, seasonal_0 Loss: 0.0662 | 0.0399
Epoch 143/300, seasonal_0 Loss: 0.0661 | 0.0399
Epoch 144/300, seasonal_0 Loss: 0.0661 | 0.0399
Epoch 145/300, seasonal_0 Loss: 0.0660 | 0.0398
Epoch 146/300, seasonal_0 Loss: 0.0660 | 0.0398
Epoch 147/300, seasonal_0 Loss: 0.0659 | 0.0398
Epoch 148/300, seasonal_0 Loss: 0.0658 | 0.0398
Epoch 149/300, seasonal_0 Loss: 0.0658 | 0.0398
Epoch 150/300, seasonal_0 Loss: 0.0657 | 0.0398
Epoch 151/300, seasonal_0 Loss: 0.0656 | 0.0398
Epoch 152/300, seasonal_0 Loss: 0.0656 | 0.0398
Epoch 153/300, seasonal_0 Loss: 0.0655 | 0.0398
Epoch 154/300, seasonal_0 Loss: 0.0655 | 0.0398
Epoch 155/300, seasonal_0 Loss: 0.0654 | 0.0398
Epoch 156/300, seasonal_0 Loss: 0.0654 | 0.0398
Epoch 157/300, seasonal_0 Loss: 0.0653 | 0.0398
Epoch 158/300, seasonal_0 Loss: 0.0653 | 0.0398
Epoch 159/300, seasonal_0 Loss: 0.0652 | 0.0398
Epoch 160/300, seasonal_0 Loss: 0.0651 | 0.0398
Epoch 161/300, seasonal_0 Loss: 0.0651 | 0.0397
Epoch 162/300, seasonal_0 Loss: 0.0650 | 0.0397
Epoch 163/300, seasonal_0 Loss: 0.0650 | 0.0397
Epoch 164/300, seasonal_0 Loss: 0.0649 | 0.0397
Epoch 165/300, seasonal_0 Loss: 0.0649 | 0.0397
Epoch 166/300, seasonal_0 Loss: 0.0648 | 0.0397
Epoch 167/300, seasonal_0 Loss: 0.0647 | 0.0396
Epoch 168/300, seasonal_0 Loss: 0.0647 | 0.0396
Epoch 169/300, seasonal_0 Loss: 0.0646 | 0.0396
Epoch 170/300, seasonal_0 Loss: 0.0646 | 0.0396
Epoch 171/300, seasonal_0 Loss: 0.0645 | 0.0396
Epoch 172/300, seasonal_0 Loss: 0.0645 | 0.0396
Epoch 173/300, seasonal_0 Loss: 0.0644 | 0.0395
Epoch 174/300, seasonal_0 Loss: 0.0644 | 0.0395
Epoch 175/300, seasonal_0 Loss: 0.0643 | 0.0395
Epoch 176/300, seasonal_0 Loss: 0.0643 | 0.0394
Epoch 177/300, seasonal_0 Loss: 0.0642 | 0.0395
Epoch 178/300, seasonal_0 Loss: 0.0642 | 0.0395
Epoch 179/300, seasonal_0 Loss: 0.0641 | 0.0394
Epoch 180/300, seasonal_0 Loss: 0.0641 | 0.0394
Epoch 181/300, seasonal_0 Loss: 0.0640 | 0.0394
Epoch 182/300, seasonal_0 Loss: 0.0640 | 0.0393
Epoch 183/300, seasonal_0 Loss: 0.0639 | 0.0394
Epoch 184/300, seasonal_0 Loss: 0.0639 | 0.0393
Epoch 185/300, seasonal_0 Loss: 0.0638 | 0.0393
Epoch 186/300, seasonal_0 Loss: 0.0638 | 0.0393
Epoch 187/300, seasonal_0 Loss: 0.0637 | 0.0393
Epoch 188/300, seasonal_0 Loss: 0.0637 | 0.0392
Epoch 189/300, seasonal_0 Loss: 0.0636 | 0.0392
Epoch 190/300, seasonal_0 Loss: 0.0636 | 0.0392
Epoch 191/300, seasonal_0 Loss: 0.0635 | 0.0392
Epoch 192/300, seasonal_0 Loss: 0.0635 | 0.0392
Epoch 193/300, seasonal_0 Loss: 0.0634 | 0.0392
Epoch 194/300, seasonal_0 Loss: 0.0634 | 0.0391
Epoch 195/300, seasonal_0 Loss: 0.0633 | 0.0391
Epoch 196/300, seasonal_0 Loss: 0.0633 | 0.0391
Epoch 197/300, seasonal_0 Loss: 0.0633 | 0.0391
Epoch 198/300, seasonal_0 Loss: 0.0632 | 0.0391
Epoch 199/300, seasonal_0 Loss: 0.0632 | 0.0391
Epoch 200/300, seasonal_0 Loss: 0.0631 | 0.0390
Epoch 201/300, seasonal_0 Loss: 0.0631 | 0.0390
Epoch 202/300, seasonal_0 Loss: 0.0630 | 0.0390
Epoch 203/300, seasonal_0 Loss: 0.0630 | 0.0390
Epoch 204/300, seasonal_0 Loss: 0.0630 | 0.0390
Epoch 205/300, seasonal_0 Loss: 0.0629 | 0.0390
Epoch 206/300, seasonal_0 Loss: 0.0629 | 0.0389
Epoch 207/300, seasonal_0 Loss: 0.0628 | 0.0389
Epoch 208/300, seasonal_0 Loss: 0.0628 | 0.0389
Epoch 209/300, seasonal_0 Loss: 0.0627 | 0.0389
Epoch 210/300, seasonal_0 Loss: 0.0627 | 0.0389
Epoch 211/300, seasonal_0 Loss: 0.0627 | 0.0389
Epoch 212/300, seasonal_0 Loss: 0.0626 | 0.0389
Epoch 213/300, seasonal_0 Loss: 0.0626 | 0.0388
Epoch 214/300, seasonal_0 Loss: 0.0625 | 0.0388
Epoch 215/300, seasonal_0 Loss: 0.0625 | 0.0388
Epoch 216/300, seasonal_0 Loss: 0.0625 | 0.0388
Epoch 217/300, seasonal_0 Loss: 0.0624 | 0.0388
Epoch 218/300, seasonal_0 Loss: 0.0624 | 0.0388
Epoch 219/300, seasonal_0 Loss: 0.0624 | 0.0387
Epoch 220/300, seasonal_0 Loss: 0.0623 | 0.0387
Epoch 221/300, seasonal_0 Loss: 0.0623 | 0.0387
Epoch 222/300, seasonal_0 Loss: 0.0622 | 0.0388
Epoch 223/300, seasonal_0 Loss: 0.0622 | 0.0387
Epoch 224/300, seasonal_0 Loss: 0.0622 | 0.0387
Epoch 225/300, seasonal_0 Loss: 0.0621 | 0.0387
Epoch 226/300, seasonal_0 Loss: 0.0621 | 0.0386
Epoch 227/300, seasonal_0 Loss: 0.0621 | 0.0386
Epoch 228/300, seasonal_0 Loss: 0.0620 | 0.0386
Epoch 229/300, seasonal_0 Loss: 0.0620 | 0.0387
Epoch 230/300, seasonal_0 Loss: 0.0619 | 0.0386
Epoch 231/300, seasonal_0 Loss: 0.0619 | 0.0386
Epoch 232/300, seasonal_0 Loss: 0.0619 | 0.0386
Epoch 233/300, seasonal_0 Loss: 0.0618 | 0.0386
Epoch 234/300, seasonal_0 Loss: 0.0618 | 0.0386
Epoch 235/300, seasonal_0 Loss: 0.0618 | 0.0386
Epoch 236/300, seasonal_0 Loss: 0.0617 | 0.0386
Epoch 237/300, seasonal_0 Loss: 0.0617 | 0.0386
Epoch 238/300, seasonal_0 Loss: 0.0617 | 0.0385
Epoch 239/300, seasonal_0 Loss: 0.0616 | 0.0385
Epoch 240/300, seasonal_0 Loss: 0.0616 | 0.0385
Epoch 241/300, seasonal_0 Loss: 0.0616 | 0.0385
Epoch 242/300, seasonal_0 Loss: 0.0615 | 0.0386
Epoch 243/300, seasonal_0 Loss: 0.0615 | 0.0385
Epoch 244/300, seasonal_0 Loss: 0.0615 | 0.0385
Epoch 245/300, seasonal_0 Loss: 0.0614 | 0.0385
Epoch 246/300, seasonal_0 Loss: 0.0614 | 0.0385
Epoch 247/300, seasonal_0 Loss: 0.0614 | 0.0385
Epoch 248/300, seasonal_0 Loss: 0.0613 | 0.0385
Epoch 249/300, seasonal_0 Loss: 0.0613 | 0.0385
Epoch 250/300, seasonal_0 Loss: 0.0613 | 0.0385
Epoch 251/300, seasonal_0 Loss: 0.0612 | 0.0385
Epoch 252/300, seasonal_0 Loss: 0.0612 | 0.0385
Epoch 253/300, seasonal_0 Loss: 0.0612 | 0.0385
Epoch 254/300, seasonal_0 Loss: 0.0611 | 0.0385
Epoch 255/300, seasonal_0 Loss: 0.0611 | 0.0386
Epoch 256/300, seasonal_0 Loss: 0.0611 | 0.0385
Epoch 257/300, seasonal_0 Loss: 0.0610 | 0.0385
Epoch 258/300, seasonal_0 Loss: 0.0610 | 0.0385
Epoch 259/300, seasonal_0 Loss: 0.0610 | 0.0385
Epoch 260/300, seasonal_0 Loss: 0.0609 | 0.0385
Epoch 261/300, seasonal_0 Loss: 0.0609 | 0.0386
Epoch 262/300, seasonal_0 Loss: 0.0609 | 0.0386
Epoch 263/300, seasonal_0 Loss: 0.0608 | 0.0386
Epoch 264/300, seasonal_0 Loss: 0.0608 | 0.0386
Epoch 265/300, seasonal_0 Loss: 0.0608 | 0.0386
Epoch 266/300, seasonal_0 Loss: 0.0608 | 0.0386
Epoch 267/300, seasonal_0 Loss: 0.0607 | 0.0387
Epoch 268/300, seasonal_0 Loss: 0.0607 | 0.0388
Epoch 269/300, seasonal_0 Loss: 0.0607 | 0.0388
Epoch 270/300, seasonal_0 Loss: 0.0606 | 0.0388
Epoch 271/300, seasonal_0 Loss: 0.0606 | 0.0388
Epoch 272/300, seasonal_0 Loss: 0.0606 | 0.0388
Epoch 273/300, seasonal_0 Loss: 0.0606 | 0.0389
Epoch 274/300, seasonal_0 Loss: 0.0605 | 0.0390
Epoch 275/300, seasonal_0 Loss: 0.0605 | 0.0390
Epoch 276/300, seasonal_0 Loss: 0.0605 | 0.0390
Epoch 277/300, seasonal_0 Loss: 0.0605 | 0.0391
Epoch 278/300, seasonal_0 Loss: 0.0604 | 0.0392
Epoch 279/300, seasonal_0 Loss: 0.0604 | 0.0393
Epoch 280/300, seasonal_0 Loss: 0.0604 | 0.0394
Epoch 281/300, seasonal_0 Loss: 0.0604 | 0.0394
Epoch 282/300, seasonal_0 Loss: 0.0604 | 0.0395
Epoch 283/300, seasonal_0 Loss: 0.0603 | 0.0396
Epoch 284/300, seasonal_0 Loss: 0.0603 | 0.0397
Epoch 285/300, seasonal_0 Loss: 0.0603 | 0.0398
Epoch 286/300, seasonal_0 Loss: 0.0603 | 0.0399
Epoch 287/300, seasonal_0 Loss: 0.0603 | 0.0400
Epoch 288/300, seasonal_0 Loss: 0.0603 | 0.0400
Epoch 289/300, seasonal_0 Loss: 0.0603 | 0.0401
Epoch 290/300, seasonal_0 Loss: 0.0603 | 0.0402
Epoch 291/300, seasonal_0 Loss: 0.0603 | 0.0404
Epoch 292/300, seasonal_0 Loss: 0.0603 | 0.0404
Epoch 293/300, seasonal_0 Loss: 0.0604 | 0.0405
Epoch 294/300, seasonal_0 Loss: 0.0604 | 0.0402
Epoch 295/300, seasonal_0 Loss: 0.0604 | 0.0401
Epoch 296/300, seasonal_0 Loss: 0.0605 | 0.0400
Epoch 297/300, seasonal_0 Loss: 0.0606 | 0.0399
Epoch 298/300, seasonal_0 Loss: 0.0606 | 0.0397
Epoch 299/300, seasonal_0 Loss: 0.0607 | 0.0395
Epoch 300/300, seasonal_0 Loss: 0.0608 | 0.0389
Training seasonal_1 component with params: {'observation_period_num': 16, 'train_rates': 0.8921571883161178, 'learning_rate': 0.0006838624503216884, 'batch_size': 31, 'step_size': 2, 'gamma': 0.8033283219156138}
Epoch 1/300, seasonal_1 Loss: 0.1888 | 0.1311
Epoch 2/300, seasonal_1 Loss: 0.1196 | 0.0901
Epoch 3/300, seasonal_1 Loss: 0.1077 | 0.0659
Epoch 4/300, seasonal_1 Loss: 0.0980 | 0.0576
Epoch 5/300, seasonal_1 Loss: 0.0929 | 0.0618
Epoch 6/300, seasonal_1 Loss: 0.0900 | 0.0516
Epoch 7/300, seasonal_1 Loss: 0.0848 | 0.0475
Epoch 8/300, seasonal_1 Loss: 0.0807 | 0.0447
Epoch 9/300, seasonal_1 Loss: 0.0794 | 0.0463
Epoch 10/300, seasonal_1 Loss: 0.0777 | 0.0473
Epoch 11/300, seasonal_1 Loss: 0.0762 | 0.0446
Epoch 12/300, seasonal_1 Loss: 0.0751 | 0.0431
Epoch 13/300, seasonal_1 Loss: 0.0743 | 0.0423
Epoch 14/300, seasonal_1 Loss: 0.0735 | 0.0413
Epoch 15/300, seasonal_1 Loss: 0.0728 | 0.0402
Epoch 16/300, seasonal_1 Loss: 0.0723 | 0.0392
Epoch 17/300, seasonal_1 Loss: 0.0719 | 0.0383
Epoch 18/300, seasonal_1 Loss: 0.0716 | 0.0376
Epoch 19/300, seasonal_1 Loss: 0.0714 | 0.0372
Epoch 20/300, seasonal_1 Loss: 0.0713 | 0.0371
Epoch 21/300, seasonal_1 Loss: 0.0712 | 0.0371
Epoch 22/300, seasonal_1 Loss: 0.0711 | 0.0373
Epoch 23/300, seasonal_1 Loss: 0.0709 | 0.0376
Epoch 24/300, seasonal_1 Loss: 0.0707 | 0.0377
Epoch 25/300, seasonal_1 Loss: 0.0704 | 0.0378
Epoch 26/300, seasonal_1 Loss: 0.0703 | 0.0378
Epoch 27/300, seasonal_1 Loss: 0.0702 | 0.0378
Epoch 28/300, seasonal_1 Loss: 0.0701 | 0.0378
Epoch 29/300, seasonal_1 Loss: 0.0700 | 0.0378
Epoch 30/300, seasonal_1 Loss: 0.0700 | 0.0378
Epoch 31/300, seasonal_1 Loss: 0.0699 | 0.0378
Epoch 32/300, seasonal_1 Loss: 0.0699 | 0.0377
Epoch 33/300, seasonal_1 Loss: 0.0699 | 0.0377
Epoch 34/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 35/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 36/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 37/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 38/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 39/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 40/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 41/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 42/300, seasonal_1 Loss: 0.0698 | 0.0377
Epoch 43/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 44/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 45/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 46/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 47/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 48/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 49/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 50/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 51/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 52/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 53/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 54/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 55/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 56/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 57/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 58/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 59/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 60/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 61/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 62/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 63/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 64/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 65/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 66/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 67/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 68/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 69/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 70/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 71/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 72/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 73/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 74/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 75/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 76/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 77/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 78/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 79/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 80/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 81/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 82/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 83/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 84/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 85/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 86/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 87/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 88/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 89/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 90/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 91/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 92/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 93/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 94/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 95/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 96/300, seasonal_1 Loss: 0.0697 | 0.0377
Epoch 97/300, seasonal_1 Loss: 0.0697 | 0.0377
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 13, 'train_rates': 0.8725648412054476, 'learning_rate': 0.0002486904614120312, 'batch_size': 112, 'step_size': 14, 'gamma': 0.9550522477946544}
Epoch 1/300, seasonal_2 Loss: 0.3985 | 0.2270
Epoch 2/300, seasonal_2 Loss: 0.1831 | 0.2004
Epoch 3/300, seasonal_2 Loss: 0.1626 | 0.1679
Epoch 4/300, seasonal_2 Loss: 0.1350 | 0.1378
Epoch 5/300, seasonal_2 Loss: 0.1276 | 0.1228
Epoch 6/300, seasonal_2 Loss: 0.1319 | 0.1044
Epoch 7/300, seasonal_2 Loss: 0.1390 | 0.1092
Epoch 8/300, seasonal_2 Loss: 0.1427 | 0.1042
Epoch 9/300, seasonal_2 Loss: 0.1233 | 0.0866
Epoch 10/300, seasonal_2 Loss: 0.1229 | 0.0833
Epoch 11/300, seasonal_2 Loss: 0.1188 | 0.0802
Epoch 12/300, seasonal_2 Loss: 0.1114 | 0.0964
Epoch 13/300, seasonal_2 Loss: 0.1136 | 0.1188
Epoch 14/300, seasonal_2 Loss: 0.1078 | 0.1075
Epoch 15/300, seasonal_2 Loss: 0.1009 | 0.0813
Epoch 16/300, seasonal_2 Loss: 0.0980 | 0.0662
Epoch 17/300, seasonal_2 Loss: 0.0986 | 0.0608
Epoch 18/300, seasonal_2 Loss: 0.1046 | 0.0651
Epoch 19/300, seasonal_2 Loss: 0.0978 | 0.0583
Epoch 20/300, seasonal_2 Loss: 0.0915 | 0.0555
Epoch 21/300, seasonal_2 Loss: 0.0920 | 0.0599
Epoch 22/300, seasonal_2 Loss: 0.0971 | 0.0602
Epoch 23/300, seasonal_2 Loss: 0.0930 | 0.0538
Epoch 24/300, seasonal_2 Loss: 0.0891 | 0.0523
Epoch 25/300, seasonal_2 Loss: 0.0879 | 0.0497
Epoch 26/300, seasonal_2 Loss: 0.0852 | 0.0498
Epoch 27/300, seasonal_2 Loss: 0.0872 | 0.0630
Epoch 28/300, seasonal_2 Loss: 0.0933 | 0.0845
Epoch 29/300, seasonal_2 Loss: 0.0917 | 0.0851
Epoch 30/300, seasonal_2 Loss: 0.0839 | 0.0586
Epoch 31/300, seasonal_2 Loss: 0.0807 | 0.0452
Epoch 32/300, seasonal_2 Loss: 0.0802 | 0.0428
Epoch 33/300, seasonal_2 Loss: 0.0831 | 0.0452
Epoch 34/300, seasonal_2 Loss: 0.0880 | 0.0498
Epoch 35/300, seasonal_2 Loss: 0.0881 | 0.0477
Epoch 36/300, seasonal_2 Loss: 0.0838 | 0.0442
Epoch 37/300, seasonal_2 Loss: 0.0842 | 0.0478
Epoch 38/300, seasonal_2 Loss: 0.0851 | 0.0642
Epoch 39/300, seasonal_2 Loss: 0.0822 | 0.0547
Epoch 40/300, seasonal_2 Loss: 0.0865 | 0.0495
Epoch 41/300, seasonal_2 Loss: 0.0872 | 0.0440
Epoch 42/300, seasonal_2 Loss: 0.0769 | 0.0425
Epoch 43/300, seasonal_2 Loss: 0.0758 | 0.0478
Epoch 44/300, seasonal_2 Loss: 0.0764 | 0.0479
Epoch 45/300, seasonal_2 Loss: 0.0745 | 0.0432
Epoch 46/300, seasonal_2 Loss: 0.0732 | 0.0391
Epoch 47/300, seasonal_2 Loss: 0.0732 | 0.0382
Epoch 48/300, seasonal_2 Loss: 0.0727 | 0.0379
Epoch 49/300, seasonal_2 Loss: 0.0727 | 0.0387
Epoch 50/300, seasonal_2 Loss: 0.0726 | 0.0404
Epoch 51/300, seasonal_2 Loss: 0.0726 | 0.0410
Epoch 52/300, seasonal_2 Loss: 0.0718 | 0.0402
Epoch 53/300, seasonal_2 Loss: 0.0712 | 0.0385
Epoch 54/300, seasonal_2 Loss: 0.0714 | 0.0371
Epoch 55/300, seasonal_2 Loss: 0.0713 | 0.0357
Epoch 56/300, seasonal_2 Loss: 0.0702 | 0.0347
Epoch 57/300, seasonal_2 Loss: 0.0690 | 0.0343
Epoch 58/300, seasonal_2 Loss: 0.0682 | 0.0339
Epoch 59/300, seasonal_2 Loss: 0.0682 | 0.0341
Epoch 60/300, seasonal_2 Loss: 0.0696 | 0.0348
Epoch 61/300, seasonal_2 Loss: 0.0718 | 0.0364
Epoch 62/300, seasonal_2 Loss: 0.0722 | 0.0407
Epoch 63/300, seasonal_2 Loss: 0.0694 | 0.0410
Epoch 64/300, seasonal_2 Loss: 0.0685 | 0.0390
Epoch 65/300, seasonal_2 Loss: 0.0720 | 0.0366
Epoch 66/300, seasonal_2 Loss: 0.0717 | 0.0338
Epoch 67/300, seasonal_2 Loss: 0.0683 | 0.0348
Epoch 68/300, seasonal_2 Loss: 0.0679 | 0.0361
Epoch 69/300, seasonal_2 Loss: 0.0680 | 0.0364
Epoch 70/300, seasonal_2 Loss: 0.0668 | 0.0369
Epoch 71/300, seasonal_2 Loss: 0.0658 | 0.0367
Epoch 72/300, seasonal_2 Loss: 0.0653 | 0.0354
Epoch 73/300, seasonal_2 Loss: 0.0653 | 0.0350
Epoch 74/300, seasonal_2 Loss: 0.0655 | 0.0353
Epoch 75/300, seasonal_2 Loss: 0.0663 | 0.0361
Epoch 76/300, seasonal_2 Loss: 0.0684 | 0.0380
Epoch 77/300, seasonal_2 Loss: 0.0746 | 0.0341
Epoch 78/300, seasonal_2 Loss: 0.0698 | 0.0348
Epoch 79/300, seasonal_2 Loss: 0.0693 | 0.0402
Epoch 80/300, seasonal_2 Loss: 0.0691 | 0.0347
Epoch 81/300, seasonal_2 Loss: 0.0700 | 0.0396
Epoch 82/300, seasonal_2 Loss: 0.0786 | 0.0388
Epoch 83/300, seasonal_2 Loss: 0.0701 | 0.0342
Epoch 84/300, seasonal_2 Loss: 0.0679 | 0.0412
Epoch 85/300, seasonal_2 Loss: 0.0679 | 0.0352
Epoch 86/300, seasonal_2 Loss: 0.0657 | 0.0313
Epoch 87/300, seasonal_2 Loss: 0.0689 | 0.0322
Epoch 88/300, seasonal_2 Loss: 0.0677 | 0.0326
Epoch 89/300, seasonal_2 Loss: 0.0658 | 0.0341
Epoch 90/300, seasonal_2 Loss: 0.0664 | 0.0357
Epoch 91/300, seasonal_2 Loss: 0.0654 | 0.0379
Epoch 92/300, seasonal_2 Loss: 0.0666 | 0.0359
Epoch 93/300, seasonal_2 Loss: 0.0640 | 0.0357
Epoch 94/300, seasonal_2 Loss: 0.0635 | 0.0358
Epoch 95/300, seasonal_2 Loss: 0.0658 | 0.0331
Epoch 96/300, seasonal_2 Loss: 0.0642 | 0.0313
Epoch 97/300, seasonal_2 Loss: 0.0629 | 0.0300
Epoch 98/300, seasonal_2 Loss: 0.0611 | 0.0298
Epoch 99/300, seasonal_2 Loss: 0.0605 | 0.0298
Epoch 100/300, seasonal_2 Loss: 0.0607 | 0.0311
Epoch 101/300, seasonal_2 Loss: 0.0618 | 0.0332
Epoch 102/300, seasonal_2 Loss: 0.0619 | 0.0346
Epoch 103/300, seasonal_2 Loss: 0.0613 | 0.0355
Epoch 104/300, seasonal_2 Loss: 0.0611 | 0.0354
Epoch 105/300, seasonal_2 Loss: 0.0612 | 0.0340
Epoch 106/300, seasonal_2 Loss: 0.0620 | 0.0334
Epoch 107/300, seasonal_2 Loss: 0.0628 | 0.0329
Epoch 108/300, seasonal_2 Loss: 0.0627 | 0.0307
Epoch 109/300, seasonal_2 Loss: 0.0627 | 0.0294
Epoch 110/300, seasonal_2 Loss: 0.0674 | 0.0341
Epoch 111/300, seasonal_2 Loss: 0.0648 | 0.0344
Epoch 112/300, seasonal_2 Loss: 0.0673 | 0.0393
Epoch 113/300, seasonal_2 Loss: 0.0609 | 0.0372
Epoch 114/300, seasonal_2 Loss: 0.0605 | 0.0303
Epoch 115/300, seasonal_2 Loss: 0.0622 | 0.0289
Epoch 116/300, seasonal_2 Loss: 0.0622 | 0.0295
Epoch 117/300, seasonal_2 Loss: 0.0610 | 0.0301
Epoch 118/300, seasonal_2 Loss: 0.0603 | 0.0319
Epoch 119/300, seasonal_2 Loss: 0.0606 | 0.0344
Epoch 120/300, seasonal_2 Loss: 0.0596 | 0.0373
Epoch 121/300, seasonal_2 Loss: 0.0598 | 0.0384
Epoch 122/300, seasonal_2 Loss: 0.0635 | 0.0346
Epoch 123/300, seasonal_2 Loss: 0.0641 | 0.0322
Epoch 124/300, seasonal_2 Loss: 0.0650 | 0.0304
Epoch 125/300, seasonal_2 Loss: 0.0626 | 0.0301
Epoch 126/300, seasonal_2 Loss: 0.0620 | 0.0307
Epoch 127/300, seasonal_2 Loss: 0.0625 | 0.0358
Epoch 128/300, seasonal_2 Loss: 0.0643 | 0.0346
Epoch 129/300, seasonal_2 Loss: 0.0665 | 0.0331
Epoch 130/300, seasonal_2 Loss: 0.0697 | 0.0365
Epoch 131/300, seasonal_2 Loss: 0.0719 | 0.0385
Epoch 132/300, seasonal_2 Loss: 0.0722 | 0.0388
Epoch 133/300, seasonal_2 Loss: 0.0742 | 0.0401
Epoch 134/300, seasonal_2 Loss: 0.0688 | 0.0410
Epoch 135/300, seasonal_2 Loss: 0.0622 | 0.0321
Epoch 136/300, seasonal_2 Loss: 0.0611 | 0.0362
Epoch 137/300, seasonal_2 Loss: 0.0623 | 0.0370
Epoch 138/300, seasonal_2 Loss: 0.0610 | 0.0329
Epoch 139/300, seasonal_2 Loss: 0.0597 | 0.0294
Epoch 140/300, seasonal_2 Loss: 0.0613 | 0.0295
Epoch 141/300, seasonal_2 Loss: 0.0581 | 0.0310
Epoch 142/300, seasonal_2 Loss: 0.0570 | 0.0303
Epoch 143/300, seasonal_2 Loss: 0.0569 | 0.0281
Epoch 144/300, seasonal_2 Loss: 0.0565 | 0.0275
Epoch 145/300, seasonal_2 Loss: 0.0562 | 0.0274
Epoch 146/300, seasonal_2 Loss: 0.0561 | 0.0274
Epoch 147/300, seasonal_2 Loss: 0.0563 | 0.0274
Epoch 148/300, seasonal_2 Loss: 0.0566 | 0.0272
Epoch 149/300, seasonal_2 Loss: 0.0566 | 0.0270
Epoch 150/300, seasonal_2 Loss: 0.0562 | 0.0271
Epoch 151/300, seasonal_2 Loss: 0.0555 | 0.0272
Epoch 152/300, seasonal_2 Loss: 0.0551 | 0.0274
Epoch 153/300, seasonal_2 Loss: 0.0549 | 0.0278
Epoch 154/300, seasonal_2 Loss: 0.0549 | 0.0284
Epoch 155/300, seasonal_2 Loss: 0.0548 | 0.0289
Epoch 156/300, seasonal_2 Loss: 0.0546 | 0.0288
Epoch 157/300, seasonal_2 Loss: 0.0543 | 0.0284
Epoch 158/300, seasonal_2 Loss: 0.0542 | 0.0277
Epoch 159/300, seasonal_2 Loss: 0.0542 | 0.0270
Epoch 160/300, seasonal_2 Loss: 0.0543 | 0.0266
Epoch 161/300, seasonal_2 Loss: 0.0544 | 0.0264
Epoch 162/300, seasonal_2 Loss: 0.0545 | 0.0265
Epoch 163/300, seasonal_2 Loss: 0.0546 | 0.0265
Epoch 164/300, seasonal_2 Loss: 0.0545 | 0.0268
Epoch 165/300, seasonal_2 Loss: 0.0546 | 0.0279
Epoch 166/300, seasonal_2 Loss: 0.0545 | 0.0275
Epoch 167/300, seasonal_2 Loss: 0.0542 | 0.0271
Epoch 168/300, seasonal_2 Loss: 0.0548 | 0.0275
Epoch 169/300, seasonal_2 Loss: 0.0560 | 0.0291
Epoch 170/300, seasonal_2 Loss: 0.0565 | 0.0278
Epoch 171/300, seasonal_2 Loss: 0.0553 | 0.0271
Epoch 172/300, seasonal_2 Loss: 0.0552 | 0.0271
Epoch 173/300, seasonal_2 Loss: 0.0550 | 0.0274
Epoch 174/300, seasonal_2 Loss: 0.0548 | 0.0280
Epoch 175/300, seasonal_2 Loss: 0.0551 | 0.0297
Epoch 176/300, seasonal_2 Loss: 0.0572 | 0.0355
Epoch 177/300, seasonal_2 Loss: 0.0595 | 0.0316
Epoch 178/300, seasonal_2 Loss: 0.0571 | 0.0320
Epoch 179/300, seasonal_2 Loss: 0.0605 | 0.0327
Epoch 180/300, seasonal_2 Loss: 0.0620 | 0.0295
Epoch 181/300, seasonal_2 Loss: 0.0599 | 0.0317
Epoch 182/300, seasonal_2 Loss: 0.0594 | 0.0302
Epoch 183/300, seasonal_2 Loss: 0.0577 | 0.0303
Epoch 184/300, seasonal_2 Loss: 0.0562 | 0.0297
Epoch 185/300, seasonal_2 Loss: 0.0554 | 0.0289
Epoch 186/300, seasonal_2 Loss: 0.0563 | 0.0299
Epoch 187/300, seasonal_2 Loss: 0.0569 | 0.0305
Epoch 188/300, seasonal_2 Loss: 0.0561 | 0.0315
Epoch 189/300, seasonal_2 Loss: 0.0559 | 0.0322
Epoch 190/300, seasonal_2 Loss: 0.0563 | 0.0351
Epoch 191/300, seasonal_2 Loss: 0.0558 | 0.0337
Epoch 192/300, seasonal_2 Loss: 0.0546 | 0.0301
Epoch 193/300, seasonal_2 Loss: 0.0546 | 0.0290
Epoch 194/300, seasonal_2 Loss: 0.0560 | 0.0294
Epoch 195/300, seasonal_2 Loss: 0.0557 | 0.0291
Epoch 196/300, seasonal_2 Loss: 0.0583 | 0.0296
Epoch 197/300, seasonal_2 Loss: 0.0583 | 0.0302
Epoch 198/300, seasonal_2 Loss: 0.0554 | 0.0301
Epoch 199/300, seasonal_2 Loss: 0.0535 | 0.0309
Epoch 200/300, seasonal_2 Loss: 0.0536 | 0.0284
Epoch 201/300, seasonal_2 Loss: 0.0539 | 0.0278
Epoch 202/300, seasonal_2 Loss: 0.0534 | 0.0279
Epoch 203/300, seasonal_2 Loss: 0.0531 | 0.0289
Epoch 204/300, seasonal_2 Loss: 0.0533 | 0.0297
Epoch 205/300, seasonal_2 Loss: 0.0535 | 0.0298
Epoch 206/300, seasonal_2 Loss: 0.0540 | 0.0276
Epoch 207/300, seasonal_2 Loss: 0.0532 | 0.0273
Epoch 208/300, seasonal_2 Loss: 0.0547 | 0.0274
Epoch 209/300, seasonal_2 Loss: 0.0547 | 0.0285
Epoch 210/300, seasonal_2 Loss: 0.0532 | 0.0294
Epoch 211/300, seasonal_2 Loss: 0.0522 | 0.0281
Epoch 212/300, seasonal_2 Loss: 0.0523 | 0.0279
Epoch 213/300, seasonal_2 Loss: 0.0529 | 0.0277
Epoch 214/300, seasonal_2 Loss: 0.0530 | 0.0280
Epoch 215/300, seasonal_2 Loss: 0.0530 | 0.0280
Epoch 216/300, seasonal_2 Loss: 0.0526 | 0.0280
Epoch 217/300, seasonal_2 Loss: 0.0522 | 0.0269
Epoch 218/300, seasonal_2 Loss: 0.0529 | 0.0274
Epoch 219/300, seasonal_2 Loss: 0.0522 | 0.0287
Epoch 220/300, seasonal_2 Loss: 0.0514 | 0.0275
Epoch 221/300, seasonal_2 Loss: 0.0511 | 0.0275
Epoch 222/300, seasonal_2 Loss: 0.0515 | 0.0272
Epoch 223/300, seasonal_2 Loss: 0.0519 | 0.0276
Epoch 224/300, seasonal_2 Loss: 0.0520 | 0.0270
Epoch 225/300, seasonal_2 Loss: 0.0516 | 0.0273
Epoch 226/300, seasonal_2 Loss: 0.0512 | 0.0270
Epoch 227/300, seasonal_2 Loss: 0.0515 | 0.0267
Epoch 228/300, seasonal_2 Loss: 0.0513 | 0.0268
Epoch 229/300, seasonal_2 Loss: 0.0507 | 0.0274
Epoch 230/300, seasonal_2 Loss: 0.0503 | 0.0280
Epoch 231/300, seasonal_2 Loss: 0.0505 | 0.0275
Epoch 232/300, seasonal_2 Loss: 0.0507 | 0.0278
Epoch 233/300, seasonal_2 Loss: 0.0506 | 0.0278
Epoch 234/300, seasonal_2 Loss: 0.0502 | 0.0271
Epoch 235/300, seasonal_2 Loss: 0.0500 | 0.0272
Epoch 236/300, seasonal_2 Loss: 0.0507 | 0.0272
Epoch 237/300, seasonal_2 Loss: 0.0509 | 0.0271
Epoch 238/300, seasonal_2 Loss: 0.0504 | 0.0274
Epoch 239/300, seasonal_2 Loss: 0.0498 | 0.0278
Epoch 240/300, seasonal_2 Loss: 0.0495 | 0.0280
Epoch 241/300, seasonal_2 Loss: 0.0497 | 0.0292
Epoch 242/300, seasonal_2 Loss: 0.0496 | 0.0298
Epoch 243/300, seasonal_2 Loss: 0.0486 | 0.0299
Epoch 244/300, seasonal_2 Loss: 0.0476 | 0.0306
Epoch 245/300, seasonal_2 Loss: 0.0467 | 0.0295
Epoch 246/300, seasonal_2 Loss: 0.0458 | 0.0285
Epoch 247/300, seasonal_2 Loss: 0.0449 | 0.0296
Epoch 248/300, seasonal_2 Loss: 0.0446 | 0.0310
Epoch 249/300, seasonal_2 Loss: 0.0456 | 0.0340
Epoch 250/300, seasonal_2 Loss: 0.0507 | 0.0315
Epoch 251/300, seasonal_2 Loss: 0.0494 | 0.0280
Epoch 252/300, seasonal_2 Loss: 0.0470 | 0.0283
Epoch 253/300, seasonal_2 Loss: 0.0450 | 0.0286
Epoch 254/300, seasonal_2 Loss: 0.0445 | 0.0293
Epoch 255/300, seasonal_2 Loss: 0.0447 | 0.0305
Epoch 256/300, seasonal_2 Loss: 0.0431 | 0.0291
Epoch 257/300, seasonal_2 Loss: 0.0424 | 0.0274
Epoch 258/300, seasonal_2 Loss: 0.0423 | 0.0275
Epoch 259/300, seasonal_2 Loss: 0.0424 | 0.0273
Epoch 260/300, seasonal_2 Loss: 0.0435 | 0.0278
Epoch 261/300, seasonal_2 Loss: 0.0421 | 0.0274
Epoch 262/300, seasonal_2 Loss: 0.0421 | 0.0274
Epoch 263/300, seasonal_2 Loss: 0.0422 | 0.0278
Epoch 264/300, seasonal_2 Loss: 0.0415 | 0.0291
Epoch 265/300, seasonal_2 Loss: 0.0413 | 0.0294
Epoch 266/300, seasonal_2 Loss: 0.0413 | 0.0298
Epoch 267/300, seasonal_2 Loss: 0.0415 | 0.0303
Epoch 268/300, seasonal_2 Loss: 0.0416 | 0.0302
Epoch 269/300, seasonal_2 Loss: 0.0420 | 0.0299
Epoch 270/300, seasonal_2 Loss: 0.0415 | 0.0292
Epoch 271/300, seasonal_2 Loss: 0.0414 | 0.0285
Epoch 272/300, seasonal_2 Loss: 0.0413 | 0.0277
Epoch 273/300, seasonal_2 Loss: 0.0413 | 0.0274
Epoch 274/300, seasonal_2 Loss: 0.0412 | 0.0272
Epoch 275/300, seasonal_2 Loss: 0.0413 | 0.0276
Epoch 276/300, seasonal_2 Loss: 0.0411 | 0.0280
Epoch 277/300, seasonal_2 Loss: 0.0418 | 0.0283
Epoch 278/300, seasonal_2 Loss: 0.0427 | 0.0282
Epoch 279/300, seasonal_2 Loss: 0.0427 | 0.0291
Epoch 280/300, seasonal_2 Loss: 0.0415 | 0.0304
Epoch 281/300, seasonal_2 Loss: 0.0409 | 0.0305
Epoch 282/300, seasonal_2 Loss: 0.0412 | 0.0304
Epoch 283/300, seasonal_2 Loss: 0.0417 | 0.0294
Epoch 284/300, seasonal_2 Loss: 0.0413 | 0.0289
Epoch 285/300, seasonal_2 Loss: 0.0407 | 0.0282
Epoch 286/300, seasonal_2 Loss: 0.0403 | 0.0280
Epoch 287/300, seasonal_2 Loss: 0.0403 | 0.0293
Epoch 288/300, seasonal_2 Loss: 0.0403 | 0.0311
Epoch 289/300, seasonal_2 Loss: 0.0401 | 0.0302
Epoch 290/300, seasonal_2 Loss: 0.0398 | 0.0294
Epoch 291/300, seasonal_2 Loss: 0.0397 | 0.0283
Epoch 292/300, seasonal_2 Loss: 0.0395 | 0.0279
Epoch 293/300, seasonal_2 Loss: 0.0396 | 0.0275
Epoch 294/300, seasonal_2 Loss: 0.0396 | 0.0276
Epoch 295/300, seasonal_2 Loss: 0.0397 | 0.0278
Epoch 296/300, seasonal_2 Loss: 0.0396 | 0.0282
Epoch 297/300, seasonal_2 Loss: 0.0393 | 0.0284
Epoch 298/300, seasonal_2 Loss: 0.0392 | 0.0287
Epoch 299/300, seasonal_2 Loss: 0.0392 | 0.0291
Epoch 300/300, seasonal_2 Loss: 0.0392 | 0.0299
Training seasonal_3 component with params: {'observation_period_num': 13, 'train_rates': 0.8259861139647494, 'learning_rate': 9.446716545553754e-05, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8857143967075227}
Epoch 1/300, seasonal_3 Loss: 0.3283 | 0.2041
Epoch 2/300, seasonal_3 Loss: 0.1519 | 0.1744
Epoch 3/300, seasonal_3 Loss: 0.1295 | 0.1212
Epoch 4/300, seasonal_3 Loss: 0.1202 | 0.1005
Epoch 5/300, seasonal_3 Loss: 0.1142 | 0.0887
Epoch 6/300, seasonal_3 Loss: 0.1097 | 0.0808
Epoch 7/300, seasonal_3 Loss: 0.1060 | 0.0750
Epoch 8/300, seasonal_3 Loss: 0.1030 | 0.0707
Epoch 9/300, seasonal_3 Loss: 0.1008 | 0.0676
Epoch 10/300, seasonal_3 Loss: 0.0992 | 0.0649
Epoch 11/300, seasonal_3 Loss: 0.0977 | 0.0626
Epoch 12/300, seasonal_3 Loss: 0.0960 | 0.0602
Epoch 13/300, seasonal_3 Loss: 0.0945 | 0.0586
Epoch 14/300, seasonal_3 Loss: 0.0930 | 0.0569
Epoch 15/300, seasonal_3 Loss: 0.0913 | 0.0545
Epoch 16/300, seasonal_3 Loss: 0.0897 | 0.0527
Epoch 17/300, seasonal_3 Loss: 0.0881 | 0.0511
Epoch 18/300, seasonal_3 Loss: 0.0867 | 0.0497
Epoch 19/300, seasonal_3 Loss: 0.0851 | 0.0477
Epoch 20/300, seasonal_3 Loss: 0.0840 | 0.0468
Epoch 21/300, seasonal_3 Loss: 0.0830 | 0.0461
Epoch 22/300, seasonal_3 Loss: 0.0822 | 0.0446
Epoch 23/300, seasonal_3 Loss: 0.0816 | 0.0439
Epoch 24/300, seasonal_3 Loss: 0.0811 | 0.0434
Epoch 25/300, seasonal_3 Loss: 0.0806 | 0.0429
Epoch 26/300, seasonal_3 Loss: 0.0798 | 0.0420
Epoch 27/300, seasonal_3 Loss: 0.0793 | 0.0418
Epoch 28/300, seasonal_3 Loss: 0.0788 | 0.0413
Epoch 29/300, seasonal_3 Loss: 0.0783 | 0.0407
Epoch 30/300, seasonal_3 Loss: 0.0779 | 0.0405
Epoch 31/300, seasonal_3 Loss: 0.0775 | 0.0401
Epoch 32/300, seasonal_3 Loss: 0.0773 | 0.0400
Epoch 33/300, seasonal_3 Loss: 0.0767 | 0.0397
Epoch 34/300, seasonal_3 Loss: 0.0763 | 0.0393
Epoch 35/300, seasonal_3 Loss: 0.0762 | 0.0394
Epoch 36/300, seasonal_3 Loss: 0.0758 | 0.0390
Epoch 37/300, seasonal_3 Loss: 0.0754 | 0.0389
Epoch 38/300, seasonal_3 Loss: 0.0752 | 0.0384
Epoch 39/300, seasonal_3 Loss: 0.0749 | 0.0384
Epoch 40/300, seasonal_3 Loss: 0.0745 | 0.0380
Epoch 41/300, seasonal_3 Loss: 0.0742 | 0.0383
Epoch 42/300, seasonal_3 Loss: 0.0739 | 0.0375
Epoch 43/300, seasonal_3 Loss: 0.0735 | 0.0380
Epoch 44/300, seasonal_3 Loss: 0.0733 | 0.0373
Epoch 45/300, seasonal_3 Loss: 0.0734 | 0.0387
Epoch 46/300, seasonal_3 Loss: 0.0735 | 0.0384
Epoch 47/300, seasonal_3 Loss: 0.0732 | 0.0375
Epoch 48/300, seasonal_3 Loss: 0.0730 | 0.0368
Epoch 49/300, seasonal_3 Loss: 0.0727 | 0.0369
Epoch 50/300, seasonal_3 Loss: 0.0720 | 0.0362
Epoch 51/300, seasonal_3 Loss: 0.0718 | 0.0362
Epoch 52/300, seasonal_3 Loss: 0.0713 | 0.0361
Epoch 53/300, seasonal_3 Loss: 0.0713 | 0.0361
Epoch 54/300, seasonal_3 Loss: 0.0709 | 0.0359
Epoch 55/300, seasonal_3 Loss: 0.0709 | 0.0357
Epoch 56/300, seasonal_3 Loss: 0.0706 | 0.0357
Epoch 57/300, seasonal_3 Loss: 0.0706 | 0.0354
Epoch 58/300, seasonal_3 Loss: 0.0705 | 0.0353
Epoch 59/300, seasonal_3 Loss: 0.0704 | 0.0352
Epoch 60/300, seasonal_3 Loss: 0.0703 | 0.0351
Epoch 61/300, seasonal_3 Loss: 0.0701 | 0.0351
Epoch 62/300, seasonal_3 Loss: 0.0700 | 0.0350
Epoch 63/300, seasonal_3 Loss: 0.0699 | 0.0349
Epoch 64/300, seasonal_3 Loss: 0.0697 | 0.0349
Epoch 65/300, seasonal_3 Loss: 0.0696 | 0.0350
Epoch 66/300, seasonal_3 Loss: 0.0695 | 0.0350
Epoch 67/300, seasonal_3 Loss: 0.0693 | 0.0349
Epoch 68/300, seasonal_3 Loss: 0.0691 | 0.0351
Epoch 69/300, seasonal_3 Loss: 0.0690 | 0.0351
Epoch 70/300, seasonal_3 Loss: 0.0689 | 0.0350
Epoch 71/300, seasonal_3 Loss: 0.0687 | 0.0351
Epoch 72/300, seasonal_3 Loss: 0.0686 | 0.0350
Epoch 73/300, seasonal_3 Loss: 0.0685 | 0.0348
Epoch 74/300, seasonal_3 Loss: 0.0684 | 0.0347
Epoch 75/300, seasonal_3 Loss: 0.0682 | 0.0347
Epoch 76/300, seasonal_3 Loss: 0.0682 | 0.0346
Epoch 77/300, seasonal_3 Loss: 0.0681 | 0.0345
Epoch 78/300, seasonal_3 Loss: 0.0680 | 0.0344
Epoch 79/300, seasonal_3 Loss: 0.0679 | 0.0343
Epoch 80/300, seasonal_3 Loss: 0.0678 | 0.0342
Epoch 81/300, seasonal_3 Loss: 0.0677 | 0.0341
Epoch 82/300, seasonal_3 Loss: 0.0676 | 0.0340
Epoch 83/300, seasonal_3 Loss: 0.0676 | 0.0340
Epoch 84/300, seasonal_3 Loss: 0.0675 | 0.0339
Epoch 85/300, seasonal_3 Loss: 0.0674 | 0.0339
Epoch 86/300, seasonal_3 Loss: 0.0674 | 0.0338
Epoch 87/300, seasonal_3 Loss: 0.0673 | 0.0338
Epoch 88/300, seasonal_3 Loss: 0.0672 | 0.0338
Epoch 89/300, seasonal_3 Loss: 0.0672 | 0.0338
Epoch 90/300, seasonal_3 Loss: 0.0671 | 0.0337
Epoch 91/300, seasonal_3 Loss: 0.0671 | 0.0337
Epoch 92/300, seasonal_3 Loss: 0.0670 | 0.0337
Epoch 93/300, seasonal_3 Loss: 0.0670 | 0.0337
Epoch 94/300, seasonal_3 Loss: 0.0669 | 0.0337
Epoch 95/300, seasonal_3 Loss: 0.0669 | 0.0337
Epoch 96/300, seasonal_3 Loss: 0.0669 | 0.0337
Epoch 97/300, seasonal_3 Loss: 0.0669 | 0.0337
Epoch 98/300, seasonal_3 Loss: 0.0668 | 0.0337
Epoch 99/300, seasonal_3 Loss: 0.0668 | 0.0336
Epoch 100/300, seasonal_3 Loss: 0.0668 | 0.0336
Epoch 101/300, seasonal_3 Loss: 0.0667 | 0.0336
Epoch 102/300, seasonal_3 Loss: 0.0667 | 0.0336
Epoch 103/300, seasonal_3 Loss: 0.0667 | 0.0336
Epoch 104/300, seasonal_3 Loss: 0.0667 | 0.0335
Epoch 105/300, seasonal_3 Loss: 0.0666 | 0.0335
Epoch 106/300, seasonal_3 Loss: 0.0666 | 0.0335
Epoch 107/300, seasonal_3 Loss: 0.0666 | 0.0335
Epoch 108/300, seasonal_3 Loss: 0.0666 | 0.0335
Epoch 109/300, seasonal_3 Loss: 0.0666 | 0.0335
Epoch 110/300, seasonal_3 Loss: 0.0665 | 0.0334
Epoch 111/300, seasonal_3 Loss: 0.0665 | 0.0334
Epoch 112/300, seasonal_3 Loss: 0.0665 | 0.0334
Epoch 113/300, seasonal_3 Loss: 0.0665 | 0.0334
Epoch 114/300, seasonal_3 Loss: 0.0664 | 0.0334
Epoch 115/300, seasonal_3 Loss: 0.0664 | 0.0334
Epoch 116/300, seasonal_3 Loss: 0.0664 | 0.0334
Epoch 117/300, seasonal_3 Loss: 0.0663 | 0.0334
Epoch 118/300, seasonal_3 Loss: 0.0663 | 0.0334
Epoch 119/300, seasonal_3 Loss: 0.0663 | 0.0334
Epoch 120/300, seasonal_3 Loss: 0.0663 | 0.0334
Epoch 121/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 122/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 123/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 124/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 125/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 126/300, seasonal_3 Loss: 0.0662 | 0.0334
Epoch 127/300, seasonal_3 Loss: 0.0661 | 0.0334
Epoch 128/300, seasonal_3 Loss: 0.0661 | 0.0334
Epoch 129/300, seasonal_3 Loss: 0.0661 | 0.0334
Epoch 130/300, seasonal_3 Loss: 0.0661 | 0.0334
Epoch 131/300, seasonal_3 Loss: 0.0661 | 0.0334
Epoch 132/300, seasonal_3 Loss: 0.0660 | 0.0334
Epoch 133/300, seasonal_3 Loss: 0.0660 | 0.0333
Epoch 134/300, seasonal_3 Loss: 0.0660 | 0.0333
Epoch 135/300, seasonal_3 Loss: 0.0660 | 0.0333
Epoch 136/300, seasonal_3 Loss: 0.0660 | 0.0333
Epoch 137/300, seasonal_3 Loss: 0.0660 | 0.0333
Epoch 138/300, seasonal_3 Loss: 0.0659 | 0.0333
Epoch 139/300, seasonal_3 Loss: 0.0659 | 0.0333
Epoch 140/300, seasonal_3 Loss: 0.0659 | 0.0333
Epoch 141/300, seasonal_3 Loss: 0.0659 | 0.0333
Epoch 142/300, seasonal_3 Loss: 0.0659 | 0.0333
Epoch 143/300, seasonal_3 Loss: 0.0659 | 0.0333
Epoch 144/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 145/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 146/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 147/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 148/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 149/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 150/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 151/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 152/300, seasonal_3 Loss: 0.0658 | 0.0333
Epoch 153/300, seasonal_3 Loss: 0.0657 | 0.0333
Epoch 154/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 155/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 156/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 157/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 158/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 159/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 160/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 161/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 162/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 163/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 164/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 165/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 166/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 167/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 168/300, seasonal_3 Loss: 0.0657 | 0.0332
Epoch 169/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 170/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 171/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 172/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 173/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 174/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 175/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 176/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 177/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 178/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 179/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 180/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 181/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 182/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 183/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 184/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 185/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 186/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 187/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 188/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 189/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 190/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 191/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 192/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 193/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 194/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 195/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 196/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 197/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 198/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 199/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 200/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 201/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 202/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 203/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 204/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 205/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 206/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 207/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 208/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 209/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 210/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 211/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 212/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 213/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 214/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 215/300, seasonal_3 Loss: 0.0656 | 0.0332
Epoch 216/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 217/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 218/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 219/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 220/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 221/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 222/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 223/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 224/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 225/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 226/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 227/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 228/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 229/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 230/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 231/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 232/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 233/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 234/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 235/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 236/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 237/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 238/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 239/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 240/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 241/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 242/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 243/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 244/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 245/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 246/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 247/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 248/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 249/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 250/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 251/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 252/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 253/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 254/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 255/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 256/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 257/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 258/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 259/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 260/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 261/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 262/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 263/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 264/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 265/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 266/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 267/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 268/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 269/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 270/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 271/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 272/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 273/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 274/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 275/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 276/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 277/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 278/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 279/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 280/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 281/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 282/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 283/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 284/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 285/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 286/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 287/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 288/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 289/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 290/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 291/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 292/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 293/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 294/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 295/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 296/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 297/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 298/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 299/300, seasonal_3 Loss: 0.0655 | 0.0332
Epoch 300/300, seasonal_3 Loss: 0.0655 | 0.0332
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9836321987401357, 'learning_rate': 0.0009519555487206153, 'batch_size': 50, 'step_size': 5, 'gamma': 0.8104224188368452}
Epoch 1/300, resid Loss: 0.2652 | 0.0884
Epoch 2/300, resid Loss: 0.1186 | 0.0795
Epoch 3/300, resid Loss: 0.1077 | 0.0615
Epoch 4/300, resid Loss: 0.0986 | 0.0554
Epoch 5/300, resid Loss: 0.0946 | 0.0531
Epoch 6/300, resid Loss: 0.0884 | 0.0543
Epoch 7/300, resid Loss: 0.0847 | 0.0532
Epoch 8/300, resid Loss: 0.0866 | 0.0561
Epoch 9/300, resid Loss: 0.0850 | 0.0511
Epoch 10/300, resid Loss: 0.0843 | 0.0496
Epoch 11/300, resid Loss: 0.0834 | 0.0597
Epoch 12/300, resid Loss: 0.0810 | 0.0569
Epoch 13/300, resid Loss: 0.0755 | 0.0441
Epoch 14/300, resid Loss: 0.0722 | 0.0445
Epoch 15/300, resid Loss: 0.0710 | 0.0436
Epoch 16/300, resid Loss: 0.0692 | 0.0425
Epoch 17/300, resid Loss: 0.0681 | 0.0406
Epoch 18/300, resid Loss: 0.0671 | 0.0386
Epoch 19/300, resid Loss: 0.0663 | 0.0378
Epoch 20/300, resid Loss: 0.0661 | 0.0367
Epoch 21/300, resid Loss: 0.0653 | 0.0373
Epoch 22/300, resid Loss: 0.0644 | 0.0360
Epoch 23/300, resid Loss: 0.0638 | 0.0349
Epoch 24/300, resid Loss: 0.0631 | 0.0347
Epoch 25/300, resid Loss: 0.0629 | 0.0335
Epoch 26/300, resid Loss: 0.0627 | 0.0354
Epoch 27/300, resid Loss: 0.0624 | 0.0361
Epoch 28/300, resid Loss: 0.0620 | 0.0379
Epoch 29/300, resid Loss: 0.0617 | 0.0314
Epoch 30/300, resid Loss: 0.0618 | 0.0311
Epoch 31/300, resid Loss: 0.0614 | 0.0300
Epoch 32/300, resid Loss: 0.0612 | 0.0301
Epoch 33/300, resid Loss: 0.0610 | 0.0306
Epoch 34/300, resid Loss: 0.0608 | 0.0302
Epoch 35/300, resid Loss: 0.0605 | 0.0302
Epoch 36/300, resid Loss: 0.0602 | 0.0304
Epoch 37/300, resid Loss: 0.0601 | 0.0304
Epoch 38/300, resid Loss: 0.0599 | 0.0303
Epoch 39/300, resid Loss: 0.0597 | 0.0304
Epoch 40/300, resid Loss: 0.0595 | 0.0303
Epoch 41/300, resid Loss: 0.0594 | 0.0302
Epoch 42/300, resid Loss: 0.0593 | 0.0301
Epoch 43/300, resid Loss: 0.0592 | 0.0301
Epoch 44/300, resid Loss: 0.0591 | 0.0300
Epoch 45/300, resid Loss: 0.0591 | 0.0299
Epoch 46/300, resid Loss: 0.0589 | 0.0304
Epoch 47/300, resid Loss: 0.0588 | 0.0304
Epoch 48/300, resid Loss: 0.0587 | 0.0304
Epoch 49/300, resid Loss: 0.0587 | 0.0300
Epoch 50/300, resid Loss: 0.0587 | 0.0300
Epoch 51/300, resid Loss: 0.0587 | 0.0292
Epoch 52/300, resid Loss: 0.0587 | 0.0292
Epoch 53/300, resid Loss: 0.0587 | 0.0291
Epoch 54/300, resid Loss: 0.0588 | 0.0290
Epoch 55/300, resid Loss: 0.0587 | 0.0289
Epoch 56/300, resid Loss: 0.0587 | 0.0291
Epoch 57/300, resid Loss: 0.0586 | 0.0291
Epoch 58/300, resid Loss: 0.0584 | 0.0291
Epoch 59/300, resid Loss: 0.0583 | 0.0297
Epoch 60/300, resid Loss: 0.0582 | 0.0296
Epoch 61/300, resid Loss: 0.0582 | 0.0304
Epoch 62/300, resid Loss: 0.0581 | 0.0303
Epoch 63/300, resid Loss: 0.0581 | 0.0303
Epoch 64/300, resid Loss: 0.0581 | 0.0306
Epoch 65/300, resid Loss: 0.0581 | 0.0306
Epoch 66/300, resid Loss: 0.0581 | 0.0302
Epoch 67/300, resid Loss: 0.0581 | 0.0303
Epoch 68/300, resid Loss: 0.0581 | 0.0304
Epoch 69/300, resid Loss: 0.0581 | 0.0300
Epoch 70/300, resid Loss: 0.0581 | 0.0300
Epoch 71/300, resid Loss: 0.0580 | 0.0298
Epoch 72/300, resid Loss: 0.0580 | 0.0298
Epoch 73/300, resid Loss: 0.0580 | 0.0298
Epoch 74/300, resid Loss: 0.0579 | 0.0297
Epoch 75/300, resid Loss: 0.0579 | 0.0297
Epoch 76/300, resid Loss: 0.0578 | 0.0297
Epoch 77/300, resid Loss: 0.0578 | 0.0297
Epoch 78/300, resid Loss: 0.0577 | 0.0297
Epoch 79/300, resid Loss: 0.0577 | 0.0297
Epoch 80/300, resid Loss: 0.0577 | 0.0297
Epoch 81/300, resid Loss: 0.0576 | 0.0297
Epoch 82/300, resid Loss: 0.0576 | 0.0297
Epoch 83/300, resid Loss: 0.0576 | 0.0296
Epoch 84/300, resid Loss: 0.0576 | 0.0297
Epoch 85/300, resid Loss: 0.0576 | 0.0296
Epoch 86/300, resid Loss: 0.0576 | 0.0297
Epoch 87/300, resid Loss: 0.0576 | 0.0296
Epoch 88/300, resid Loss: 0.0576 | 0.0296
Epoch 89/300, resid Loss: 0.0575 | 0.0296
Epoch 90/300, resid Loss: 0.0575 | 0.0296
Epoch 91/300, resid Loss: 0.0575 | 0.0296
Epoch 92/300, resid Loss: 0.0575 | 0.0296
Epoch 93/300, resid Loss: 0.0575 | 0.0296
Epoch 94/300, resid Loss: 0.0575 | 0.0296
Epoch 95/300, resid Loss: 0.0575 | 0.0296
Epoch 96/300, resid Loss: 0.0575 | 0.0296
Epoch 97/300, resid Loss: 0.0575 | 0.0296
Epoch 98/300, resid Loss: 0.0575 | 0.0296
Epoch 99/300, resid Loss: 0.0575 | 0.0296
Epoch 100/300, resid Loss: 0.0575 | 0.0296
Epoch 101/300, resid Loss: 0.0575 | 0.0296
Epoch 102/300, resid Loss: 0.0575 | 0.0296
Epoch 103/300, resid Loss: 0.0575 | 0.0296
Epoch 104/300, resid Loss: 0.0575 | 0.0296
Epoch 105/300, resid Loss: 0.0575 | 0.0296
Epoch 106/300, resid Loss: 0.0575 | 0.0296
Epoch 107/300, resid Loss: 0.0575 | 0.0296
Epoch 108/300, resid Loss: 0.0575 | 0.0296
Epoch 109/300, resid Loss: 0.0575 | 0.0296
Epoch 110/300, resid Loss: 0.0575 | 0.0296
Epoch 111/300, resid Loss: 0.0575 | 0.0296
Epoch 112/300, resid Loss: 0.0575 | 0.0296
Epoch 113/300, resid Loss: 0.0575 | 0.0296
Epoch 114/300, resid Loss: 0.0575 | 0.0296
Epoch 115/300, resid Loss: 0.0575 | 0.0296
Epoch 116/300, resid Loss: 0.0575 | 0.0296
Epoch 117/300, resid Loss: 0.0575 | 0.0295
Epoch 118/300, resid Loss: 0.0575 | 0.0295
Epoch 119/300, resid Loss: 0.0575 | 0.0295
Epoch 120/300, resid Loss: 0.0575 | 0.0295
Epoch 121/300, resid Loss: 0.0575 | 0.0295
Epoch 122/300, resid Loss: 0.0575 | 0.0295
Epoch 123/300, resid Loss: 0.0575 | 0.0295
Epoch 124/300, resid Loss: 0.0575 | 0.0295
Epoch 125/300, resid Loss: 0.0575 | 0.0295
Epoch 126/300, resid Loss: 0.0575 | 0.0295
Epoch 127/300, resid Loss: 0.0575 | 0.0295
Epoch 128/300, resid Loss: 0.0575 | 0.0295
Epoch 129/300, resid Loss: 0.0575 | 0.0295
Epoch 130/300, resid Loss: 0.0575 | 0.0295
Epoch 131/300, resid Loss: 0.0575 | 0.0295
Epoch 132/300, resid Loss: 0.0575 | 0.0295
Epoch 133/300, resid Loss: 0.0575 | 0.0295
Epoch 134/300, resid Loss: 0.0575 | 0.0295
Epoch 135/300, resid Loss: 0.0575 | 0.0295
Epoch 136/300, resid Loss: 0.0575 | 0.0295
Epoch 137/300, resid Loss: 0.0575 | 0.0295
Epoch 138/300, resid Loss: 0.0575 | 0.0295
Epoch 139/300, resid Loss: 0.0575 | 0.0295
Epoch 140/300, resid Loss: 0.0575 | 0.0295
Epoch 141/300, resid Loss: 0.0575 | 0.0295
Epoch 142/300, resid Loss: 0.0575 | 0.0295
Epoch 143/300, resid Loss: 0.0575 | 0.0295
Epoch 144/300, resid Loss: 0.0575 | 0.0295
Epoch 145/300, resid Loss: 0.0575 | 0.0295
Epoch 146/300, resid Loss: 0.0575 | 0.0295
Epoch 147/300, resid Loss: 0.0575 | 0.0295
Epoch 148/300, resid Loss: 0.0575 | 0.0295
Epoch 149/300, resid Loss: 0.0575 | 0.0295
Epoch 150/300, resid Loss: 0.0575 | 0.0295
Epoch 151/300, resid Loss: 0.0575 | 0.0295
Epoch 152/300, resid Loss: 0.0575 | 0.0295
Epoch 153/300, resid Loss: 0.0575 | 0.0295
Epoch 154/300, resid Loss: 0.0575 | 0.0295
Epoch 155/300, resid Loss: 0.0575 | 0.0295
Epoch 156/300, resid Loss: 0.0575 | 0.0295
Epoch 157/300, resid Loss: 0.0575 | 0.0295
Epoch 158/300, resid Loss: 0.0575 | 0.0295
Epoch 159/300, resid Loss: 0.0575 | 0.0295
Epoch 160/300, resid Loss: 0.0575 | 0.0295
Epoch 161/300, resid Loss: 0.0575 | 0.0295
Epoch 162/300, resid Loss: 0.0575 | 0.0295
Epoch 163/300, resid Loss: 0.0575 | 0.0295
Epoch 164/300, resid Loss: 0.0575 | 0.0295
Epoch 165/300, resid Loss: 0.0575 | 0.0295
Epoch 166/300, resid Loss: 0.0575 | 0.0295
Early stopping for resid
Runtime (seconds): 1942.3197121620178
0.0005953105169070539
[157.02866]
[-1.6963886]
[-4.834683]
[9.665462]
[2.6065125]
[8.1631975]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 24.578966731438413
RMSE: 4.9577178955078125
MAE: 4.9577178955078125
R-squared: nan
[170.93274]
