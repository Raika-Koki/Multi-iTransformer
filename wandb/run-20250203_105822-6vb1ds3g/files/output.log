[32m[I 2025-02-03 10:58:29,234][0m A new study created in memory with name: no-name-fba51fb5-6277-4e9b-b03a-509da5dea4a9[0m
[32m[I 2025-02-03 10:58:56,055][0m Trial 0 finished with value: 0.07675083011710097 and parameters: {'observation_period_num': 23, 'train_rates': 0.6205777136547805, 'learning_rate': 2.3323985050852664e-05, 'batch_size': 184, 'step_size': 15, 'gamma': 0.9364854761144269}. Best is trial 0 with value: 0.07675083011710097.[0m
[32m[I 2025-02-03 10:59:47,757][0m Trial 1 finished with value: 0.10144458137393936 and parameters: {'observation_period_num': 90, 'train_rates': 0.7785389550352602, 'learning_rate': 3.098305015458228e-05, 'batch_size': 106, 'step_size': 11, 'gamma': 0.7854097387431902}. Best is trial 0 with value: 0.07675083011710097.[0m
[32m[I 2025-02-03 11:00:11,047][0m Trial 2 finished with value: 0.1665211431123531 and parameters: {'observation_period_num': 207, 'train_rates': 0.6846384693283275, 'learning_rate': 9.287600707558744e-05, 'batch_size': 230, 'step_size': 5, 'gamma': 0.9218881026782425}. Best is trial 0 with value: 0.07675083011710097.[0m
[32m[I 2025-02-03 11:00:46,132][0m Trial 3 finished with value: 0.06071916361507334 and parameters: {'observation_period_num': 62, 'train_rates': 0.9158054203409546, 'learning_rate': 7.526850459757173e-05, 'batch_size': 177, 'step_size': 7, 'gamma': 0.8150306007746575}. Best is trial 3 with value: 0.06071916361507334.[0m
[32m[I 2025-02-03 11:04:34,020][0m Trial 4 finished with value: 0.09105251734068648 and parameters: {'observation_period_num': 87, 'train_rates': 0.8301051683124044, 'learning_rate': 2.293498809388267e-05, 'batch_size': 23, 'step_size': 4, 'gamma': 0.75185174805205}. Best is trial 3 with value: 0.06071916361507334.[0m
[32m[I 2025-02-03 11:05:11,138][0m Trial 5 finished with value: 0.055732622403990136 and parameters: {'observation_period_num': 121, 'train_rates': 0.8760180762432033, 'learning_rate': 0.0006329444551910253, 'batch_size': 160, 'step_size': 9, 'gamma': 0.9124980290849989}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:05:39,375][0m Trial 6 finished with value: 0.07213437166762225 and parameters: {'observation_period_num': 172, 'train_rates': 0.8658902242110583, 'learning_rate': 6.773055182397157e-05, 'batch_size': 207, 'step_size': 7, 'gamma': 0.9878757079827342}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:07:36,905][0m Trial 7 finished with value: 0.07645810309297113 and parameters: {'observation_period_num': 112, 'train_rates': 0.6703261566876831, 'learning_rate': 3.0074575413333274e-05, 'batch_size': 39, 'step_size': 11, 'gamma': 0.9739280297763212}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:08:05,796][0m Trial 8 finished with value: 0.08736703678344687 and parameters: {'observation_period_num': 11, 'train_rates': 0.8369867502435766, 'learning_rate': 1.1242263225680177e-05, 'batch_size': 206, 'step_size': 6, 'gamma': 0.8573755104960842}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:08:51,938][0m Trial 9 finished with value: 0.2792839080199495 and parameters: {'observation_period_num': 118, 'train_rates': 0.8992091588726923, 'learning_rate': 8.636259374273095e-06, 'batch_size': 133, 'step_size': 2, 'gamma': 0.9397993750212703}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:10:01,754][0m Trial 10 finished with value: 0.093928761780262 and parameters: {'observation_period_num': 168, 'train_rates': 0.9895660491587368, 'learning_rate': 0.0008887028181856069, 'batch_size': 87, 'step_size': 10, 'gamma': 0.8752605571943499}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:10:42,082][0m Trial 11 finished with value: 0.6328201977933039 and parameters: {'observation_period_num': 56, 'train_rates': 0.937060722907824, 'learning_rate': 1.079011316668412e-06, 'batch_size': 159, 'step_size': 9, 'gamma': 0.8292817620318207}. Best is trial 5 with value: 0.055732622403990136.[0m
[32m[I 2025-02-03 11:11:18,625][0m Trial 12 finished with value: 0.03602743101756101 and parameters: {'observation_period_num': 54, 'train_rates': 0.766811790331672, 'learning_rate': 0.0008006704308376379, 'batch_size': 155, 'step_size': 13, 'gamma': 0.819653230238956}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:11:59,591][0m Trial 13 finished with value: 0.07290745827757557 and parameters: {'observation_period_num': 153, 'train_rates': 0.7578967405162866, 'learning_rate': 0.0008724569170949015, 'batch_size': 132, 'step_size': 14, 'gamma': 0.8912561605438318}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:13:00,686][0m Trial 14 finished with value: 0.07747429963974907 and parameters: {'observation_period_num': 248, 'train_rates': 0.7689536082098282, 'learning_rate': 0.00029955715659250366, 'batch_size': 82, 'step_size': 13, 'gamma': 0.8464338044644284}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:13:23,515][0m Trial 15 finished with value: 0.045922529070767916 and parameters: {'observation_period_num': 63, 'train_rates': 0.7173722280010515, 'learning_rate': 0.00031381261128053895, 'batch_size': 252, 'step_size': 12, 'gamma': 0.9019615198026929}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:13:48,855][0m Trial 16 finished with value: 0.043477027243611834 and parameters: {'observation_period_num': 45, 'train_rates': 0.7224245680491623, 'learning_rate': 0.00024370500809903483, 'batch_size': 232, 'step_size': 13, 'gamma': 0.7996429091177331}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:14:12,234][0m Trial 17 finished with value: 0.042483244054883684 and parameters: {'observation_period_num': 36, 'train_rates': 0.7196733993122358, 'learning_rate': 0.00021716020371093073, 'batch_size': 248, 'step_size': 15, 'gamma': 0.7954007611914247}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:15:25,960][0m Trial 18 finished with value: 0.048209919441324564 and parameters: {'observation_period_num': 30, 'train_rates': 0.6107687350827817, 'learning_rate': 0.00015016917044971938, 'batch_size': 61, 'step_size': 15, 'gamma': 0.7644167866732358}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:15:46,919][0m Trial 19 finished with value: 0.038526109060433904 and parameters: {'observation_period_num': 6, 'train_rates': 0.6557194154484955, 'learning_rate': 0.0005063550281726299, 'batch_size': 252, 'step_size': 15, 'gamma': 0.7847767088313407}. Best is trial 12 with value: 0.03602743101756101.[0m
[32m[I 2025-02-03 11:16:31,705][0m Trial 20 finished with value: 0.03191021744565791 and parameters: {'observation_period_num': 5, 'train_rates': 0.6625285335858045, 'learning_rate': 0.0004433679017475469, 'batch_size': 110, 'step_size': 13, 'gamma': 0.8248447629057982}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:17:16,472][0m Trial 21 finished with value: 0.034728246466445095 and parameters: {'observation_period_num': 5, 'train_rates': 0.6481575324631861, 'learning_rate': 0.0005147794270650845, 'batch_size': 111, 'step_size': 13, 'gamma': 0.8328474717201443}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:17:59,490][0m Trial 22 finished with value: 0.03279581435231699 and parameters: {'observation_period_num': 10, 'train_rates': 0.6333771228240044, 'learning_rate': 0.00043560892307159484, 'batch_size': 111, 'step_size': 13, 'gamma': 0.8349422453545716}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:18:43,135][0m Trial 23 finished with value: 0.035347455197859215 and parameters: {'observation_period_num': 8, 'train_rates': 0.6400901095017324, 'learning_rate': 0.0004143974808168039, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8489786827032318}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:19:29,055][0m Trial 24 finished with value: 0.08755550956728672 and parameters: {'observation_period_num': 83, 'train_rates': 0.6007742089205406, 'learning_rate': 0.00011962064190474839, 'batch_size': 97, 'step_size': 12, 'gamma': 0.836472075011286}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:20:50,926][0m Trial 25 finished with value: 0.16307746160938116 and parameters: {'observation_period_num': 27, 'train_rates': 0.6951510470409537, 'learning_rate': 2.779379411394527e-06, 'batch_size': 60, 'step_size': 9, 'gamma': 0.8711071751111217}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:21:31,444][0m Trial 26 finished with value: 0.04495862020877455 and parameters: {'observation_period_num': 7, 'train_rates': 0.6442969089464726, 'learning_rate': 0.0001584108280000404, 'batch_size': 119, 'step_size': 13, 'gamma': 0.8119420028862393}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:22:41,563][0m Trial 27 finished with value: 0.04102790641440923 and parameters: {'observation_period_num': 75, 'train_rates': 0.6900575994933101, 'learning_rate': 0.00046850044582538345, 'batch_size': 70, 'step_size': 12, 'gamma': 0.8593767715806262}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:23:25,228][0m Trial 28 finished with value: 0.06377424838007498 and parameters: {'observation_period_num': 41, 'train_rates': 0.6322742898502102, 'learning_rate': 4.716180498509757e-05, 'batch_size': 111, 'step_size': 14, 'gamma': 0.8850845073114002}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:24:01,401][0m Trial 29 finished with value: 0.04637520162502996 and parameters: {'observation_period_num': 20, 'train_rates': 0.622573352620934, 'learning_rate': 0.00019404777664320743, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8328879040062435}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:24:34,612][0m Trial 30 finished with value: 0.06485222132098155 and parameters: {'observation_period_num': 103, 'train_rates': 0.662986924993557, 'learning_rate': 0.0004290105829234879, 'batch_size': 150, 'step_size': 14, 'gamma': 0.7724691045177622}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:25:16,608][0m Trial 31 finished with value: 0.03798796236515045 and parameters: {'observation_period_num': 19, 'train_rates': 0.6354110516189923, 'learning_rate': 0.00036393139642986784, 'batch_size': 119, 'step_size': 11, 'gamma': 0.837804919059571}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:26:11,935][0m Trial 32 finished with value: 0.036743805888078074 and parameters: {'observation_period_num': 30, 'train_rates': 0.7419068694739772, 'learning_rate': 0.0005758721784731454, 'batch_size': 97, 'step_size': 11, 'gamma': 0.8536484268396174}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:27:00,420][0m Trial 33 finished with value: 0.032843487972268166 and parameters: {'observation_period_num': 9, 'train_rates': 0.6724840446742699, 'learning_rate': 0.0003128454483906225, 'batch_size': 108, 'step_size': 12, 'gamma': 0.805335777503144}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:28:05,221][0m Trial 34 finished with value: 0.04347086801080649 and parameters: {'observation_period_num': 43, 'train_rates': 0.6718659122751445, 'learning_rate': 0.00011857974135856241, 'batch_size': 78, 'step_size': 14, 'gamma': 0.8071523617139414}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:28:54,368][0m Trial 35 finished with value: 0.04018212503125501 and parameters: {'observation_period_num': 70, 'train_rates': 0.7021533851679971, 'learning_rate': 0.0009908268686596344, 'batch_size': 104, 'step_size': 12, 'gamma': 0.785852014716666}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:30:38,534][0m Trial 36 finished with value: 0.052497051826445186 and parameters: {'observation_period_num': 22, 'train_rates': 0.6009484982480935, 'learning_rate': 0.000272206366769409, 'batch_size': 42, 'step_size': 13, 'gamma': 0.8187024194463648}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:31:32,266][0m Trial 37 finished with value: 0.09400804551399272 and parameters: {'observation_period_num': 136, 'train_rates': 0.6574637156483297, 'learning_rate': 6.433663710079526e-05, 'batch_size': 91, 'step_size': 10, 'gamma': 0.7994732787186661}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:32:10,546][0m Trial 38 finished with value: 0.041929064422018 and parameters: {'observation_period_num': 99, 'train_rates': 0.8111874455599097, 'learning_rate': 0.0005600005808149142, 'batch_size': 146, 'step_size': 8, 'gamma': 0.7719902611285214}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:32:38,972][0m Trial 39 finished with value: 0.12418437749147415 and parameters: {'observation_period_num': 208, 'train_rates': 0.6797486733864022, 'learning_rate': 8.887358173422897e-05, 'batch_size': 180, 'step_size': 14, 'gamma': 0.8243446392875268}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:33:24,188][0m Trial 40 finished with value: 0.03598329434742777 and parameters: {'observation_period_num': 48, 'train_rates': 0.7928454095223723, 'learning_rate': 0.0006713088615168765, 'batch_size': 127, 'step_size': 4, 'gamma': 0.8439741989727927}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:34:09,731][0m Trial 41 finished with value: 0.03591165609573259 and parameters: {'observation_period_num': 7, 'train_rates': 0.6412067461999401, 'learning_rate': 0.0003725619805476869, 'batch_size': 111, 'step_size': 12, 'gamma': 0.8607377109480124}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:34:50,921][0m Trial 42 finished with value: 0.04960008636317806 and parameters: {'observation_period_num': 17, 'train_rates': 0.6214987791505427, 'learning_rate': 0.0001981760630029216, 'batch_size': 121, 'step_size': 11, 'gamma': 0.8477284889019135}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:35:21,931][0m Trial 43 finished with value: 0.08067113598545664 and parameters: {'observation_period_num': 6, 'train_rates': 0.6519941283415365, 'learning_rate': 1.408612822784749e-05, 'batch_size': 168, 'step_size': 10, 'gamma': 0.8082803121802284}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:36:09,097][0m Trial 44 finished with value: 0.05497288745310572 and parameters: {'observation_period_num': 33, 'train_rates': 0.6768436257286083, 'learning_rate': 0.0004006445197852833, 'batch_size': 105, 'step_size': 13, 'gamma': 0.830627672155462}. Best is trial 20 with value: 0.03191021744565791.[0m
[32m[I 2025-02-03 11:36:48,215][0m Trial 45 finished with value: 0.030926626126625036 and parameters: {'observation_period_num': 17, 'train_rates': 0.7415173299400893, 'learning_rate': 0.0006858029620788539, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8672131559768638}. Best is trial 45 with value: 0.030926626126625036.[0m
[32m[I 2025-02-03 11:37:27,821][0m Trial 46 finished with value: 0.04334612000465844 and parameters: {'observation_period_num': 22, 'train_rates': 0.729937005822072, 'learning_rate': 0.0006904287974505249, 'batch_size': 142, 'step_size': 1, 'gamma': 0.9332851914355954}. Best is trial 45 with value: 0.030926626126625036.[0m
[32m[I 2025-02-03 11:38:07,293][0m Trial 47 finished with value: 0.06210545604191129 and parameters: {'observation_period_num': 62, 'train_rates': 0.7387608674831319, 'learning_rate': 0.0009822084092396502, 'batch_size': 138, 'step_size': 12, 'gamma': 0.8850480176277299}. Best is trial 45 with value: 0.030926626126625036.[0m
[32m[I 2025-02-03 11:38:35,419][0m Trial 48 finished with value: 0.03641493842092063 and parameters: {'observation_period_num': 16, 'train_rates': 0.7065702645981572, 'learning_rate': 0.0006790366612763603, 'batch_size': 202, 'step_size': 15, 'gamma': 0.8674980358430855}. Best is trial 45 with value: 0.030926626126625036.[0m
[32m[I 2025-02-03 11:39:31,206][0m Trial 49 finished with value: 0.03412944255066828 and parameters: {'observation_period_num': 37, 'train_rates': 0.7487000063935292, 'learning_rate': 0.00026001747837169087, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9029785094472035}. Best is trial 45 with value: 0.030926626126625036.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_BA_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.4964 | 0.1402
Epoch 2/300, Loss: 0.2533 | 0.2485
Epoch 3/300, Loss: 0.3252 | 0.3422
Epoch 4/300, Loss: 0.2147 | 0.0922
Epoch 5/300, Loss: 0.1390 | 0.1195
Epoch 6/300, Loss: 0.1402 | 0.0810
Epoch 7/300, Loss: 0.1317 | 0.0894
Epoch 8/300, Loss: 0.1869 | 0.1148
Epoch 9/300, Loss: 0.1636 | 0.0856
Epoch 10/300, Loss: 0.0911 | 0.1040
Epoch 11/300, Loss: 0.1005 | 0.1422
Epoch 12/300, Loss: 0.0893 | 0.0877
Epoch 13/300, Loss: 0.0740 | 0.0797
Epoch 14/300, Loss: 0.0731 | 0.0642
Epoch 15/300, Loss: 0.0658 | 0.0569
Epoch 16/300, Loss: 0.0690 | 0.0797
Epoch 17/300, Loss: 0.0638 | 0.0589
Epoch 18/300, Loss: 0.0606 | 0.0605
Epoch 19/300, Loss: 0.0642 | 0.0641
Epoch 20/300, Loss: 0.0638 | 0.0538
Epoch 21/300, Loss: 0.0652 | 0.0512
Epoch 22/300, Loss: 0.0661 | 0.0514
Epoch 23/300, Loss: 0.0606 | 0.0800
Epoch 24/300, Loss: 0.0626 | 0.0630
Epoch 25/300, Loss: 0.0581 | 0.0583
Epoch 26/300, Loss: 0.0660 | 0.1237
Epoch 27/300, Loss: 0.0684 | 0.0697
Epoch 28/300, Loss: 0.0668 | 0.0491
Epoch 29/300, Loss: 0.0504 | 0.0470
Epoch 30/300, Loss: 0.0485 | 0.0522
Epoch 31/300, Loss: 0.0527 | 0.0810
Epoch 32/300, Loss: 0.0578 | 0.0719
Epoch 33/300, Loss: 0.0551 | 0.0681
Epoch 34/300, Loss: 0.0547 | 0.0800
Epoch 35/300, Loss: 0.0781 | 0.0749
Epoch 36/300, Loss: 0.0597 | 0.0453
Epoch 37/300, Loss: 0.0489 | 0.0417
Epoch 38/300, Loss: 0.0471 | 0.0513
Epoch 39/300, Loss: 0.0481 | 0.0632
Epoch 40/300, Loss: 0.0596 | 0.0729
Epoch 41/300, Loss: 0.0551 | 0.0783
Epoch 42/300, Loss: 0.0710 | 0.0660
Epoch 43/300, Loss: 0.0551 | 0.0596
Epoch 44/300, Loss: 0.0739 | 0.0598
Epoch 45/300, Loss: 0.0517 | 0.0427
Epoch 46/300, Loss: 0.0480 | 0.0443
Epoch 47/300, Loss: 0.0479 | 0.0600
Epoch 48/300, Loss: 0.0710 | 0.0554
Epoch 49/300, Loss: 0.0520 | 0.0466
Epoch 50/300, Loss: 0.0479 | 0.0423
Epoch 51/300, Loss: 0.0416 | 0.0377
Epoch 52/300, Loss: 0.0403 | 0.0402
Epoch 53/300, Loss: 0.0407 | 0.0464
Epoch 54/300, Loss: 0.0529 | 0.0520
Epoch 55/300, Loss: 0.0458 | 0.0441
Epoch 56/300, Loss: 0.0512 | 0.0455
Epoch 57/300, Loss: 0.0452 | 0.0425
Epoch 58/300, Loss: 0.0530 | 0.0452
Epoch 59/300, Loss: 0.0423 | 0.0392
Epoch 60/300, Loss: 0.0437 | 0.0413
Epoch 61/300, Loss: 0.0415 | 0.0408
Epoch 62/300, Loss: 0.0477 | 0.0415
Epoch 63/300, Loss: 0.0390 | 0.0376
Epoch 64/300, Loss: 0.0389 | 0.0383
Epoch 65/300, Loss: 0.0377 | 0.0368
Epoch 66/300, Loss: 0.0405 | 0.0392
Epoch 67/300, Loss: 0.0371 | 0.0358
Epoch 68/300, Loss: 0.0379 | 0.0369
Epoch 69/300, Loss: 0.0359 | 0.0351
Epoch 70/300, Loss: 0.0368 | 0.0365
Epoch 71/300, Loss: 0.0352 | 0.0346
Epoch 72/300, Loss: 0.0362 | 0.0362
Epoch 73/300, Loss: 0.0344 | 0.0341
Epoch 74/300, Loss: 0.0344 | 0.0347
Epoch 75/300, Loss: 0.0332 | 0.0336
Epoch 76/300, Loss: 0.0332 | 0.0342
Epoch 77/300, Loss: 0.0325 | 0.0332
Epoch 78/300, Loss: 0.0323 | 0.0335
Epoch 79/300, Loss: 0.0316 | 0.0330
Epoch 80/300, Loss: 0.0313 | 0.0329
Epoch 81/300, Loss: 0.0308 | 0.0325
Epoch 82/300, Loss: 0.0306 | 0.0325
Epoch 83/300, Loss: 0.0302 | 0.0321
Epoch 84/300, Loss: 0.0300 | 0.0321
Epoch 85/300, Loss: 0.0297 | 0.0319
Epoch 86/300, Loss: 0.0295 | 0.0318
Epoch 87/300, Loss: 0.0293 | 0.0317
Epoch 88/300, Loss: 0.0292 | 0.0316
Epoch 89/300, Loss: 0.0290 | 0.0315
Epoch 90/300, Loss: 0.0288 | 0.0314
Epoch 91/300, Loss: 0.0287 | 0.0313
Epoch 92/300, Loss: 0.0286 | 0.0312
Epoch 93/300, Loss: 0.0284 | 0.0311
Epoch 94/300, Loss: 0.0283 | 0.0310
Epoch 95/300, Loss: 0.0282 | 0.0309
Epoch 96/300, Loss: 0.0281 | 0.0309
Epoch 97/300, Loss: 0.0280 | 0.0308
Epoch 98/300, Loss: 0.0279 | 0.0307
Epoch 99/300, Loss: 0.0278 | 0.0306
Epoch 100/300, Loss: 0.0277 | 0.0306
Epoch 101/300, Loss: 0.0276 | 0.0305
Epoch 102/300, Loss: 0.0275 | 0.0304
Epoch 103/300, Loss: 0.0274 | 0.0303
Epoch 104/300, Loss: 0.0273 | 0.0303
Epoch 105/300, Loss: 0.0273 | 0.0302
Epoch 106/300, Loss: 0.0272 | 0.0302
Epoch 107/300, Loss: 0.0271 | 0.0301
Epoch 108/300, Loss: 0.0270 | 0.0300
Epoch 109/300, Loss: 0.0269 | 0.0300
Epoch 110/300, Loss: 0.0269 | 0.0299
Epoch 111/300, Loss: 0.0268 | 0.0299
Epoch 112/300, Loss: 0.0267 | 0.0298
Epoch 113/300, Loss: 0.0266 | 0.0298
Epoch 114/300, Loss: 0.0266 | 0.0297
Epoch 115/300, Loss: 0.0265 | 0.0297
Epoch 116/300, Loss: 0.0265 | 0.0296
Epoch 117/300, Loss: 0.0264 | 0.0296
Epoch 118/300, Loss: 0.0263 | 0.0295
Epoch 119/300, Loss: 0.0263 | 0.0295
Epoch 120/300, Loss: 0.0262 | 0.0295
Epoch 121/300, Loss: 0.0262 | 0.0294
Epoch 122/300, Loss: 0.0261 | 0.0294
Epoch 123/300, Loss: 0.0261 | 0.0293
Epoch 124/300, Loss: 0.0260 | 0.0293
Epoch 125/300, Loss: 0.0260 | 0.0293
Epoch 126/300, Loss: 0.0259 | 0.0292
Epoch 127/300, Loss: 0.0259 | 0.0292
Epoch 128/300, Loss: 0.0258 | 0.0292
Epoch 129/300, Loss: 0.0258 | 0.0291
Epoch 130/300, Loss: 0.0257 | 0.0291
Epoch 131/300, Loss: 0.0257 | 0.0291
Epoch 132/300, Loss: 0.0256 | 0.0290
Epoch 133/300, Loss: 0.0256 | 0.0290
Epoch 134/300, Loss: 0.0256 | 0.0290
Epoch 135/300, Loss: 0.0255 | 0.0289
Epoch 136/300, Loss: 0.0255 | 0.0289
Epoch 137/300, Loss: 0.0255 | 0.0289
Epoch 138/300, Loss: 0.0254 | 0.0289
Epoch 139/300, Loss: 0.0254 | 0.0288
Epoch 140/300, Loss: 0.0253 | 0.0288
Epoch 141/300, Loss: 0.0253 | 0.0288
Epoch 142/300, Loss: 0.0253 | 0.0288
Epoch 143/300, Loss: 0.0252 | 0.0287
Epoch 144/300, Loss: 0.0252 | 0.0287
Epoch 145/300, Loss: 0.0252 | 0.0287
Epoch 146/300, Loss: 0.0251 | 0.0287
Epoch 147/300, Loss: 0.0251 | 0.0287
Epoch 148/300, Loss: 0.0251 | 0.0286
Epoch 149/300, Loss: 0.0251 | 0.0286
Epoch 150/300, Loss: 0.0250 | 0.0286
Epoch 151/300, Loss: 0.0250 | 0.0286
Epoch 152/300, Loss: 0.0250 | 0.0286
Epoch 153/300, Loss: 0.0250 | 0.0286
Epoch 154/300, Loss: 0.0249 | 0.0285
Epoch 155/300, Loss: 0.0249 | 0.0285
Epoch 156/300, Loss: 0.0249 | 0.0285
Epoch 157/300, Loss: 0.0249 | 0.0285
Epoch 158/300, Loss: 0.0248 | 0.0285
Epoch 159/300, Loss: 0.0248 | 0.0285
Epoch 160/300, Loss: 0.0248 | 0.0284
Epoch 161/300, Loss: 0.0248 | 0.0284
Epoch 162/300, Loss: 0.0248 | 0.0284
Epoch 163/300, Loss: 0.0247 | 0.0284
Epoch 164/300, Loss: 0.0247 | 0.0284
Epoch 165/300, Loss: 0.0247 | 0.0284
Epoch 166/300, Loss: 0.0247 | 0.0284
Epoch 167/300, Loss: 0.0247 | 0.0283
Epoch 168/300, Loss: 0.0246 | 0.0283
Epoch 169/300, Loss: 0.0246 | 0.0283
Epoch 170/300, Loss: 0.0246 | 0.0283
Epoch 171/300, Loss: 0.0246 | 0.0283
Epoch 172/300, Loss: 0.0246 | 0.0283
Epoch 173/300, Loss: 0.0246 | 0.0283
Epoch 174/300, Loss: 0.0245 | 0.0283
Epoch 175/300, Loss: 0.0245 | 0.0283
Epoch 176/300, Loss: 0.0245 | 0.0283
Epoch 177/300, Loss: 0.0245 | 0.0282
Epoch 178/300, Loss: 0.0245 | 0.0282
Epoch 179/300, Loss: 0.0245 | 0.0282
Epoch 180/300, Loss: 0.0245 | 0.0282
Epoch 181/300, Loss: 0.0244 | 0.0282
Epoch 182/300, Loss: 0.0244 | 0.0282
Epoch 183/300, Loss: 0.0244 | 0.0282
Epoch 184/300, Loss: 0.0244 | 0.0282
Epoch 185/300, Loss: 0.0244 | 0.0282
Epoch 186/300, Loss: 0.0244 | 0.0282
Epoch 187/300, Loss: 0.0244 | 0.0282
Epoch 188/300, Loss: 0.0243 | 0.0282
Epoch 189/300, Loss: 0.0243 | 0.0281
Epoch 190/300, Loss: 0.0243 | 0.0281
Epoch 191/300, Loss: 0.0243 | 0.0281
Epoch 192/300, Loss: 0.0243 | 0.0281
Epoch 193/300, Loss: 0.0243 | 0.0281
Epoch 194/300, Loss: 0.0243 | 0.0281
Epoch 195/300, Loss: 0.0243 | 0.0281
Epoch 196/300, Loss: 0.0243 | 0.0281
Epoch 197/300, Loss: 0.0243 | 0.0281
Epoch 198/300, Loss: 0.0242 | 0.0281
Epoch 199/300, Loss: 0.0242 | 0.0281
Epoch 200/300, Loss: 0.0242 | 0.0281
Epoch 201/300, Loss: 0.0242 | 0.0281
Epoch 202/300, Loss: 0.0242 | 0.0281
Epoch 203/300, Loss: 0.0242 | 0.0281
Epoch 204/300, Loss: 0.0242 | 0.0281
Epoch 205/300, Loss: 0.0242 | 0.0281
Epoch 206/300, Loss: 0.0242 | 0.0281
Epoch 207/300, Loss: 0.0242 | 0.0280
Epoch 208/300, Loss: 0.0242 | 0.0280
Epoch 209/300, Loss: 0.0242 | 0.0280
Epoch 210/300, Loss: 0.0242 | 0.0280
Epoch 211/300, Loss: 0.0241 | 0.0280
Epoch 212/300, Loss: 0.0241 | 0.0280
Epoch 213/300, Loss: 0.0241 | 0.0280
Epoch 214/300, Loss: 0.0241 | 0.0280
Epoch 215/300, Loss: 0.0241 | 0.0280
Epoch 216/300, Loss: 0.0241 | 0.0280
Epoch 217/300, Loss: 0.0241 | 0.0280
Epoch 218/300, Loss: 0.0241 | 0.0280
Epoch 219/300, Loss: 0.0241 | 0.0280
Epoch 220/300, Loss: 0.0241 | 0.0280
Epoch 221/300, Loss: 0.0241 | 0.0280
Epoch 222/300, Loss: 0.0241 | 0.0280
Epoch 223/300, Loss: 0.0241 | 0.0280
Epoch 224/300, Loss: 0.0241 | 0.0280
Epoch 225/300, Loss: 0.0241 | 0.0280
Epoch 226/300, Loss: 0.0241 | 0.0280
Epoch 227/300, Loss: 0.0241 | 0.0280
Epoch 228/300, Loss: 0.0240 | 0.0280
Epoch 229/300, Loss: 0.0240 | 0.0280
Epoch 230/300, Loss: 0.0240 | 0.0280
Epoch 231/300, Loss: 0.0240 | 0.0280
Epoch 232/300, Loss: 0.0240 | 0.0280
Epoch 233/300, Loss: 0.0240 | 0.0280
Epoch 234/300, Loss: 0.0240 | 0.0280
Epoch 235/300, Loss: 0.0240 | 0.0280
Epoch 236/300, Loss: 0.0240 | 0.0280
Epoch 237/300, Loss: 0.0240 | 0.0280
Epoch 238/300, Loss: 0.0240 | 0.0279
Epoch 239/300, Loss: 0.0240 | 0.0279
Epoch 240/300, Loss: 0.0240 | 0.0279
Epoch 241/300, Loss: 0.0240 | 0.0279
Epoch 242/300, Loss: 0.0240 | 0.0279
Epoch 243/300, Loss: 0.0240 | 0.0279
Epoch 244/300, Loss: 0.0240 | 0.0279
Epoch 245/300, Loss: 0.0240 | 0.0279
Epoch 246/300, Loss: 0.0240 | 0.0279
Epoch 247/300, Loss: 0.0240 | 0.0279
Epoch 248/300, Loss: 0.0240 | 0.0279
Epoch 249/300, Loss: 0.0240 | 0.0279
Epoch 250/300, Loss: 0.0240 | 0.0279
Epoch 251/300, Loss: 0.0240 | 0.0279
Epoch 252/300, Loss: 0.0240 | 0.0279
Epoch 253/300, Loss: 0.0240 | 0.0279
Epoch 254/300, Loss: 0.0240 | 0.0279
Epoch 255/300, Loss: 0.0240 | 0.0279
Epoch 256/300, Loss: 0.0240 | 0.0279
Epoch 257/300, Loss: 0.0239 | 0.0279
Epoch 258/300, Loss: 0.0239 | 0.0279
Epoch 259/300, Loss: 0.0239 | 0.0279
Epoch 260/300, Loss: 0.0239 | 0.0279
Epoch 261/300, Loss: 0.0239 | 0.0279
Epoch 262/300, Loss: 0.0239 | 0.0279
Epoch 263/300, Loss: 0.0239 | 0.0279
Epoch 264/300, Loss: 0.0239 | 0.0279
Epoch 265/300, Loss: 0.0239 | 0.0279
Epoch 266/300, Loss: 0.0239 | 0.0279
Epoch 267/300, Loss: 0.0239 | 0.0279
Epoch 268/300, Loss: 0.0239 | 0.0279
Epoch 269/300, Loss: 0.0239 | 0.0279
Epoch 270/300, Loss: 0.0239 | 0.0279
Epoch 271/300, Loss: 0.0239 | 0.0279
Epoch 272/300, Loss: 0.0239 | 0.0279
Epoch 273/300, Loss: 0.0239 | 0.0279
Epoch 274/300, Loss: 0.0239 | 0.0279
Epoch 275/300, Loss: 0.0239 | 0.0279
Epoch 276/300, Loss: 0.0239 | 0.0279
Epoch 277/300, Loss: 0.0239 | 0.0279
Epoch 278/300, Loss: 0.0239 | 0.0279
Epoch 279/300, Loss: 0.0239 | 0.0279
Epoch 280/300, Loss: 0.0239 | 0.0279
Epoch 281/300, Loss: 0.0239 | 0.0279
Epoch 282/300, Loss: 0.0239 | 0.0279
Epoch 283/300, Loss: 0.0239 | 0.0279
Epoch 284/300, Loss: 0.0239 | 0.0279
Epoch 285/300, Loss: 0.0239 | 0.0279
Epoch 286/300, Loss: 0.0239 | 0.0279
Epoch 287/300, Loss: 0.0239 | 0.0279
Epoch 288/300, Loss: 0.0239 | 0.0279
Epoch 289/300, Loss: 0.0239 | 0.0279
Epoch 290/300, Loss: 0.0239 | 0.0279
Epoch 291/300, Loss: 0.0239 | 0.0279
Epoch 292/300, Loss: 0.0239 | 0.0279
Epoch 293/300, Loss: 0.0239 | 0.0279
Epoch 294/300, Loss: 0.0239 | 0.0279
Epoch 295/300, Loss: 0.0239 | 0.0279
Epoch 296/300, Loss: 0.0239 | 0.0279
Epoch 297/300, Loss: 0.0239 | 0.0279
Epoch 298/300, Loss: 0.0239 | 0.0279
Epoch 299/300, Loss: 0.0239 | 0.0279
Epoch 300/300, Loss: 0.0239 | 0.0279
Runtime (seconds): 113.43809676170349
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 35.09083575592376
RMSE: 5.9237518310546875
MAE: 5.9237518310546875
R-squared: nan
[150.61624]
