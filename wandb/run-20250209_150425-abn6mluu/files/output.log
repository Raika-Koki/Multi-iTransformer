[32m[I 2025-02-09 15:04:30,274][0m A new study created in memory with name: no-name-0661da67-be03-439a-8d9e-c5b4e9b669dc[0m
[32m[I 2025-02-09 15:05:07,795][0m Trial 0 finished with value: 0.16217289005181457 and parameters: {'observation_period_num': 45, 'train_rates': 0.6201067527441684, 'learning_rate': 0.00043323093336371276, 'batch_size': 128, 'step_size': 3, 'gamma': 0.9602523369025431}. Best is trial 0 with value: 0.16217289005181457.[0m
[32m[I 2025-02-09 15:05:36,032][0m Trial 1 finished with value: 0.17489363016369186 and parameters: {'observation_period_num': 122, 'train_rates': 0.851352186876486, 'learning_rate': 0.0009473621209136522, 'batch_size': 216, 'step_size': 11, 'gamma': 0.9260578166820558}. Best is trial 0 with value: 0.16217289005181457.[0m
[32m[I 2025-02-09 15:07:48,009][0m Trial 2 finished with value: 0.21669504671663672 and parameters: {'observation_period_num': 163, 'train_rates': 0.9496386184675509, 'learning_rate': 2.4105275430742433e-05, 'batch_size': 43, 'step_size': 11, 'gamma': 0.9033315803801759}. Best is trial 0 with value: 0.16217289005181457.[0m
[32m[I 2025-02-09 15:08:55,009][0m Trial 3 finished with value: 0.4792468430526764 and parameters: {'observation_period_num': 216, 'train_rates': 0.9105613626203548, 'learning_rate': 4.276165718361279e-06, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9842949110564603}. Best is trial 0 with value: 0.16217289005181457.[0m
[32m[I 2025-02-09 15:09:28,967][0m Trial 4 finished with value: 0.15438021676960403 and parameters: {'observation_period_num': 42, 'train_rates': 0.683115266269669, 'learning_rate': 0.00059727436030013, 'batch_size': 145, 'step_size': 13, 'gamma': 0.8131006532950388}. Best is trial 4 with value: 0.15438021676960403.[0m
[32m[I 2025-02-09 15:10:18,345][0m Trial 5 finished with value: 0.7732291262725304 and parameters: {'observation_period_num': 176, 'train_rates': 0.8973710031488458, 'learning_rate': 2.4800336516479363e-06, 'batch_size': 120, 'step_size': 10, 'gamma': 0.8809377206485955}. Best is trial 4 with value: 0.15438021676960403.[0m
[32m[I 2025-02-09 15:10:55,715][0m Trial 6 finished with value: 0.3135945929170458 and parameters: {'observation_period_num': 175, 'train_rates': 0.6847336131937134, 'learning_rate': 0.00011088411976534343, 'batch_size': 137, 'step_size': 4, 'gamma': 0.8743844370992688}. Best is trial 4 with value: 0.15438021676960403.[0m
[32m[I 2025-02-09 15:11:16,441][0m Trial 7 finished with value: 0.49451984055452836 and parameters: {'observation_period_num': 201, 'train_rates': 0.6454613380753184, 'learning_rate': 7.763470402027991e-05, 'batch_size': 252, 'step_size': 3, 'gamma': 0.8247011802616361}. Best is trial 4 with value: 0.15438021676960403.[0m
[32m[I 2025-02-09 15:11:41,432][0m Trial 8 finished with value: 0.5198457606406097 and parameters: {'observation_period_num': 249, 'train_rates': 0.7589356584507487, 'learning_rate': 4.7153463394564536e-05, 'batch_size': 217, 'step_size': 5, 'gamma': 0.7619276708869518}. Best is trial 4 with value: 0.15438021676960403.[0m
Early stopping at epoch 99
[32m[I 2025-02-09 15:13:41,179][0m Trial 9 finished with value: 0.662566171793998 and parameters: {'observation_period_num': 225, 'train_rates': 0.7722564967597516, 'learning_rate': 7.303690080040147e-06, 'batch_size': 40, 'step_size': 1, 'gamma': 0.8721722311743001}. Best is trial 4 with value: 0.15438021676960403.[0m
[32m[I 2025-02-09 15:14:13,217][0m Trial 10 finished with value: 0.15153615980257323 and parameters: {'observation_period_num': 11, 'train_rates': 0.7148297230612037, 'learning_rate': 0.00028215500781622566, 'batch_size': 168, 'step_size': 15, 'gamma': 0.8073187260672031}. Best is trial 10 with value: 0.15153615980257323.[0m
[32m[I 2025-02-09 15:14:45,126][0m Trial 11 finished with value: 0.14520316510773146 and parameters: {'observation_period_num': 9, 'train_rates': 0.7105131316339852, 'learning_rate': 0.0002974201660593402, 'batch_size': 171, 'step_size': 15, 'gamma': 0.8088100910665257}. Best is trial 11 with value: 0.14520316510773146.[0m
[32m[I 2025-02-09 15:15:16,645][0m Trial 12 finished with value: 0.1455723719648373 and parameters: {'observation_period_num': 5, 'train_rates': 0.7296612184298116, 'learning_rate': 0.0002507646750338269, 'batch_size': 182, 'step_size': 15, 'gamma': 0.8137759449008423}. Best is trial 11 with value: 0.14520316510773146.[0m
[32m[I 2025-02-09 15:15:48,095][0m Trial 13 finished with value: 0.1029729563666224 and parameters: {'observation_period_num': 92, 'train_rates': 0.8328970150267019, 'learning_rate': 0.00021007779490827384, 'batch_size': 187, 'step_size': 8, 'gamma': 0.7529434502818851}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:16:19,380][0m Trial 14 finished with value: 0.13645392803862544 and parameters: {'observation_period_num': 100, 'train_rates': 0.8260698985612877, 'learning_rate': 0.00015900517265632713, 'batch_size': 193, 'step_size': 6, 'gamma': 0.7639275183471819}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:16:47,903][0m Trial 15 finished with value: 0.4466707019912087 and parameters: {'observation_period_num': 104, 'train_rates': 0.829673810373972, 'learning_rate': 1.835122656417349e-05, 'batch_size': 206, 'step_size': 7, 'gamma': 0.7554334169396244}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:17:11,879][0m Trial 16 finished with value: 0.11359856022255761 and parameters: {'observation_period_num': 85, 'train_rates': 0.831936042976904, 'learning_rate': 0.00012878741130466489, 'batch_size': 254, 'step_size': 8, 'gamma': 0.7797382088120932}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:17:39,079][0m Trial 17 finished with value: 0.23067593574523926 and parameters: {'observation_period_num': 72, 'train_rates': 0.9764775630273941, 'learning_rate': 7.112723009104096e-05, 'batch_size': 244, 'step_size': 9, 'gamma': 0.7777814548781051}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:18:04,971][0m Trial 18 finished with value: 0.3659952980167461 and parameters: {'observation_period_num': 77, 'train_rates': 0.8733455486868058, 'learning_rate': 1.4645465471112168e-05, 'batch_size': 236, 'step_size': 8, 'gamma': 0.8460661696419797}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:18:57,098][0m Trial 19 finished with value: 0.23221507226316637 and parameters: {'observation_period_num': 138, 'train_rates': 0.7965393333118715, 'learning_rate': 3.84697827529781e-05, 'batch_size': 101, 'step_size': 8, 'gamma': 0.7828421555671612}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:19:24,073][0m Trial 20 finished with value: 0.8220299685774403 and parameters: {'observation_period_num': 68, 'train_rates': 0.8002724668687181, 'learning_rate': 1.364604410729433e-06, 'batch_size': 228, 'step_size': 12, 'gamma': 0.8452246178868592}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:19:54,430][0m Trial 21 finished with value: 0.15253655513726086 and parameters: {'observation_period_num': 98, 'train_rates': 0.8287254859545456, 'learning_rate': 0.00011889596629444795, 'batch_size': 195, 'step_size': 6, 'gamma': 0.7840726633531647}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:20:30,932][0m Trial 22 finished with value: 0.16679252474509237 and parameters: {'observation_period_num': 128, 'train_rates': 0.8638787114946613, 'learning_rate': 0.00014031571624485971, 'batch_size': 158, 'step_size': 6, 'gamma': 0.7547822106150298}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:21:03,503][0m Trial 23 finished with value: 0.1381153225773523 and parameters: {'observation_period_num': 101, 'train_rates': 0.9182105193731431, 'learning_rate': 0.00014831925828180897, 'batch_size': 194, 'step_size': 9, 'gamma': 0.7853374426194958}. Best is trial 13 with value: 0.1029729563666224.[0m
[32m[I 2025-02-09 15:21:29,024][0m Trial 24 finished with value: 0.07207981671952056 and parameters: {'observation_period_num': 46, 'train_rates': 0.8170403981638286, 'learning_rate': 0.00022958612680834675, 'batch_size': 255, 'step_size': 7, 'gamma': 0.7687219644930046}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:21:52,211][0m Trial 25 finished with value: 0.18768871269929102 and parameters: {'observation_period_num': 41, 'train_rates': 0.7761045583294541, 'learning_rate': 0.0006332510623421973, 'batch_size': 256, 'step_size': 7, 'gamma': 0.7500615626267105}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:22:20,425][0m Trial 26 finished with value: 0.1287956483223859 and parameters: {'observation_period_num': 57, 'train_rates': 0.8905531402049319, 'learning_rate': 6.896520213682432e-05, 'batch_size': 228, 'step_size': 9, 'gamma': 0.7946838536988478}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:22:42,714][0m Trial 27 finished with value: 0.1783809391818115 and parameters: {'observation_period_num': 28, 'train_rates': 0.742164856961803, 'learning_rate': 0.00022806861828383274, 'batch_size': 256, 'step_size': 7, 'gamma': 0.8434109085496214}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:23:11,342][0m Trial 28 finished with value: 0.11156227676275637 and parameters: {'observation_period_num': 79, 'train_rates': 0.8116157448716732, 'learning_rate': 0.0004865773381590093, 'batch_size': 213, 'step_size': 10, 'gamma': 0.7782320093753358}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:23:39,171][0m Trial 29 finished with value: 0.1408797394145619 and parameters: {'observation_period_num': 60, 'train_rates': 0.8018181792749484, 'learning_rate': 0.00045365914890001577, 'batch_size': 212, 'step_size': 10, 'gamma': 0.9387715319564235}. Best is trial 24 with value: 0.07207981671952056.[0m
Early stopping at epoch 52
[32m[I 2025-02-09 15:23:55,126][0m Trial 30 finished with value: 0.3077031075954437 and parameters: {'observation_period_num': 50, 'train_rates': 0.9469389280649952, 'learning_rate': 0.0008711842403230422, 'batch_size': 232, 'step_size': 1, 'gamma': 0.7735257598220078}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:24:22,665][0m Trial 31 finished with value: 0.07950244668112771 and parameters: {'observation_period_num': 84, 'train_rates': 0.8468744738495506, 'learning_rate': 0.0003991378004570017, 'batch_size': 239, 'step_size': 10, 'gamma': 0.7969538393092196}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:24:50,743][0m Trial 32 finished with value: 0.12609285376099652 and parameters: {'observation_period_num': 118, 'train_rates': 0.8510752600836435, 'learning_rate': 0.0004161953253516722, 'batch_size': 218, 'step_size': 11, 'gamma': 0.8282658201758518}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:25:20,202][0m Trial 33 finished with value: 0.1637585469418102 and parameters: {'observation_period_num': 86, 'train_rates': 0.8736328378995071, 'learning_rate': 0.0008789893651295499, 'batch_size': 205, 'step_size': 10, 'gamma': 0.7935460549613534}. Best is trial 24 with value: 0.07207981671952056.[0m
[32m[I 2025-02-09 15:25:52,181][0m Trial 34 finished with value: 0.05062401873108588 and parameters: {'observation_period_num': 33, 'train_rates': 0.8465838804175433, 'learning_rate': 0.000413548350784767, 'batch_size': 180, 'step_size': 11, 'gamma': 0.7700052436381448}. Best is trial 34 with value: 0.05062401873108588.[0m
[32m[I 2025-02-09 15:30:48,409][0m Trial 35 finished with value: 0.05557390900364347 and parameters: {'observation_period_num': 30, 'train_rates': 0.8415493785689917, 'learning_rate': 0.00035406631829710265, 'batch_size': 18, 'step_size': 11, 'gamma': 0.765516442945047}. Best is trial 34 with value: 0.05062401873108588.[0m
[32m[I 2025-02-09 15:35:38,612][0m Trial 36 finished with value: 0.05238684713840484 and parameters: {'observation_period_num': 27, 'train_rates': 0.9245066821084896, 'learning_rate': 0.00038611145238622735, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7979570979140326}. Best is trial 34 with value: 0.05062401873108588.[0m
[32m[I 2025-02-09 15:41:44,974][0m Trial 37 finished with value: 0.04700556210084268 and parameters: {'observation_period_num': 27, 'train_rates': 0.9427760051855898, 'learning_rate': 0.0007594442432416584, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9011459513613458}. Best is trial 37 with value: 0.04700556210084268.[0m
[32m[I 2025-02-09 15:46:53,037][0m Trial 38 finished with value: 0.047884489289958386 and parameters: {'observation_period_num': 27, 'train_rates': 0.9399952264490514, 'learning_rate': 0.0006836589951794204, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8959686072544616}. Best is trial 37 with value: 0.04700556210084268.[0m
[32m[I 2025-02-09 15:48:48,479][0m Trial 39 finished with value: 0.06774507026587213 and parameters: {'observation_period_num': 25, 'train_rates': 0.9413324765679097, 'learning_rate': 0.000806808486599411, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9037874634224002}. Best is trial 37 with value: 0.04700556210084268.[0m
[32m[I 2025-02-09 15:53:11,089][0m Trial 40 finished with value: 0.055016291802221874 and parameters: {'observation_period_num': 21, 'train_rates': 0.9825317888886647, 'learning_rate': 0.0005458861602818176, 'batch_size': 23, 'step_size': 13, 'gamma': 0.8929387649923157}. Best is trial 37 with value: 0.04700556210084268.[0m
[32m[I 2025-02-09 15:58:43,974][0m Trial 41 finished with value: 0.07274003946424826 and parameters: {'observation_period_num': 17, 'train_rates': 0.9692221663743645, 'learning_rate': 0.0006150834014417613, 'batch_size': 18, 'step_size': 12, 'gamma': 0.8986128205388707}. Best is trial 37 with value: 0.04700556210084268.[0m
[32m[I 2025-02-09 16:00:30,684][0m Trial 42 finished with value: 0.0836691812637749 and parameters: {'observation_period_num': 36, 'train_rates': 0.9265797794886464, 'learning_rate': 0.0006510834232541049, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9251234323832395}. Best is trial 37 with value: 0.04700556210084268.[0m
[32m[I 2025-02-09 16:03:23,262][0m Trial 43 finished with value: 0.04553211892281121 and parameters: {'observation_period_num': 18, 'train_rates': 0.959018630750972, 'learning_rate': 0.0005598270980826411, 'batch_size': 35, 'step_size': 14, 'gamma': 0.8863214426943224}. Best is trial 43 with value: 0.04553211892281121.[0m
[32m[I 2025-02-09 16:06:11,353][0m Trial 44 finished with value: 0.12473177105641446 and parameters: {'observation_period_num': 56, 'train_rates': 0.8995605176883456, 'learning_rate': 0.00035341394421650095, 'batch_size': 34, 'step_size': 14, 'gamma': 0.91888843369462}. Best is trial 43 with value: 0.04553211892281121.[0m
[32m[I 2025-02-09 16:07:38,051][0m Trial 45 finished with value: 0.06254333122210069 and parameters: {'observation_period_num': 36, 'train_rates': 0.9631497354807556, 'learning_rate': 0.0006716432891730396, 'batch_size': 70, 'step_size': 14, 'gamma': 0.8583549317206489}. Best is trial 43 with value: 0.04553211892281121.[0m
[32m[I 2025-02-09 16:10:49,040][0m Trial 46 finished with value: 0.034492754268921115 and parameters: {'observation_period_num': 6, 'train_rates': 0.9314938145133671, 'learning_rate': 0.0002961233338569619, 'batch_size': 31, 'step_size': 12, 'gamma': 0.8864149915136639}. Best is trial 46 with value: 0.034492754268921115.[0m
[32m[I 2025-02-09 16:13:46,003][0m Trial 47 finished with value: 0.032422426056915095 and parameters: {'observation_period_num': 6, 'train_rates': 0.9439023099142982, 'learning_rate': 0.00018206342658245252, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8857528679648722}. Best is trial 47 with value: 0.032422426056915095.[0m
[32m[I 2025-02-09 16:15:00,016][0m Trial 48 finished with value: 0.11138973477994439 and parameters: {'observation_period_num': 5, 'train_rates': 0.6005585257680768, 'learning_rate': 0.0009597030944783635, 'batch_size': 61, 'step_size': 14, 'gamma': 0.8851752391330003}. Best is trial 47 with value: 0.032422426056915095.[0m
[32m[I 2025-02-09 16:18:07,662][0m Trial 49 finished with value: 0.049122899770736694 and parameters: {'observation_period_num': 14, 'train_rates': 0.989250765864517, 'learning_rate': 0.00018044981526347585, 'batch_size': 33, 'step_size': 15, 'gamma': 0.9418007927025448}. Best is trial 47 with value: 0.032422426056915095.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.1936 | 0.1387
Epoch 2/300, Loss: 0.1206 | 0.1075
Epoch 3/300, Loss: 0.1083 | 0.0861
Epoch 4/300, Loss: 0.1014 | 0.0755
Epoch 5/300, Loss: 0.0972 | 0.0705
Epoch 6/300, Loss: 0.0938 | 0.0648
Epoch 7/300, Loss: 0.0907 | 0.0603
Epoch 8/300, Loss: 0.0872 | 0.0581
Epoch 9/300, Loss: 0.0858 | 0.0545
Epoch 10/300, Loss: 0.0825 | 0.0521
Epoch 11/300, Loss: 0.0807 | 0.0503
Epoch 12/300, Loss: 0.0790 | 0.0490
Epoch 13/300, Loss: 0.0772 | 0.0473
Epoch 14/300, Loss: 0.0751 | 0.0452
Epoch 15/300, Loss: 0.0724 | 0.0442
Epoch 16/300, Loss: 0.0710 | 0.0424
Epoch 17/300, Loss: 0.0701 | 0.0412
Epoch 18/300, Loss: 0.0696 | 0.0403
Epoch 19/300, Loss: 0.0687 | 0.0400
Epoch 20/300, Loss: 0.0677 | 0.0400
Epoch 21/300, Loss: 0.0670 | 0.0400
Epoch 22/300, Loss: 0.0662 | 0.0399
Epoch 23/300, Loss: 0.0658 | 0.0396
Epoch 24/300, Loss: 0.0655 | 0.0397
Epoch 25/300, Loss: 0.0652 | 0.0393
Epoch 26/300, Loss: 0.0646 | 0.0383
Epoch 27/300, Loss: 0.0639 | 0.0375
Epoch 28/300, Loss: 0.0636 | 0.0371
Epoch 29/300, Loss: 0.0629 | 0.0392
Epoch 30/300, Loss: 0.0630 | 0.0390
Epoch 31/300, Loss: 0.0624 | 0.0380
Epoch 32/300, Loss: 0.0621 | 0.0381
Epoch 33/300, Loss: 0.0620 | 0.0380
Epoch 34/300, Loss: 0.0615 | 0.0379
Epoch 35/300, Loss: 0.0611 | 0.0375
Epoch 36/300, Loss: 0.0609 | 0.0354
Epoch 37/300, Loss: 0.0605 | 0.0356
Epoch 38/300, Loss: 0.0603 | 0.0358
Epoch 39/300, Loss: 0.0603 | 0.0363
Epoch 40/300, Loss: 0.0605 | 0.0351
Epoch 41/300, Loss: 0.0603 | 0.0338
Epoch 42/300, Loss: 0.0601 | 0.0336
Epoch 43/300, Loss: 0.0598 | 0.0329
Epoch 44/300, Loss: 0.0599 | 0.0328
Epoch 45/300, Loss: 0.0601 | 0.0337
Epoch 46/300, Loss: 0.0592 | 0.0317
Epoch 47/300, Loss: 0.0586 | 0.0315
Epoch 48/300, Loss: 0.0581 | 0.0313
Epoch 49/300, Loss: 0.0580 | 0.0312
Epoch 50/300, Loss: 0.0577 | 0.0314
Epoch 51/300, Loss: 0.0575 | 0.0313
Epoch 52/300, Loss: 0.0571 | 0.0309
Epoch 53/300, Loss: 0.0567 | 0.0308
Epoch 54/300, Loss: 0.0565 | 0.0305
Epoch 55/300, Loss: 0.0565 | 0.0307
Epoch 56/300, Loss: 0.0561 | 0.0300
Epoch 57/300, Loss: 0.0556 | 0.0311
Epoch 58/300, Loss: 0.0552 | 0.0306
Epoch 59/300, Loss: 0.0549 | 0.0301
Epoch 60/300, Loss: 0.0547 | 0.0300
Epoch 61/300, Loss: 0.0546 | 0.0300
Epoch 62/300, Loss: 0.0545 | 0.0302
Epoch 63/300, Loss: 0.0543 | 0.0305
Epoch 64/300, Loss: 0.0544 | 0.0309
Epoch 65/300, Loss: 0.0543 | 0.0306
Epoch 66/300, Loss: 0.0542 | 0.0313
Epoch 67/300, Loss: 0.0544 | 0.0331
Epoch 68/300, Loss: 0.0543 | 0.0320
Epoch 69/300, Loss: 0.0536 | 0.0302
Epoch 70/300, Loss: 0.0532 | 0.0301
Epoch 71/300, Loss: 0.0532 | 0.0303
Epoch 72/300, Loss: 0.0533 | 0.0305
Epoch 73/300, Loss: 0.0531 | 0.0304
Epoch 74/300, Loss: 0.0529 | 0.0305
Epoch 75/300, Loss: 0.0529 | 0.0311
Epoch 76/300, Loss: 0.0529 | 0.0320
Epoch 77/300, Loss: 0.0529 | 0.0317
Epoch 78/300, Loss: 0.0526 | 0.0307
Epoch 79/300, Loss: 0.0527 | 0.0311
Epoch 80/300, Loss: 0.0530 | 0.0323
Epoch 81/300, Loss: 0.0523 | 0.0304
Epoch 82/300, Loss: 0.0518 | 0.0303
Epoch 83/300, Loss: 0.0520 | 0.0309
Epoch 84/300, Loss: 0.0519 | 0.0311
Epoch 85/300, Loss: 0.0514 | 0.0305
Epoch 86/300, Loss: 0.0512 | 0.0303
Epoch 87/300, Loss: 0.0511 | 0.0307
Epoch 88/300, Loss: 0.0509 | 0.0303
Epoch 89/300, Loss: 0.0508 | 0.0301
Epoch 90/300, Loss: 0.0507 | 0.0304
Epoch 91/300, Loss: 0.0506 | 0.0305
Epoch 92/300, Loss: 0.0505 | 0.0301
Epoch 93/300, Loss: 0.0504 | 0.0300
Epoch 94/300, Loss: 0.0503 | 0.0300
Epoch 95/300, Loss: 0.0502 | 0.0299
Epoch 96/300, Loss: 0.0501 | 0.0299
Epoch 97/300, Loss: 0.0500 | 0.0301
Epoch 98/300, Loss: 0.0499 | 0.0302
Epoch 99/300, Loss: 0.0498 | 0.0300
Epoch 100/300, Loss: 0.0497 | 0.0302
Epoch 101/300, Loss: 0.0496 | 0.0304
Epoch 102/300, Loss: 0.0495 | 0.0305
Epoch 103/300, Loss: 0.0494 | 0.0307
Epoch 104/300, Loss: 0.0493 | 0.0311
Epoch 105/300, Loss: 0.0491 | 0.0317
Epoch 106/300, Loss: 0.0488 | 0.0327
Epoch 107/300, Loss: 0.0485 | 0.0352
Epoch 108/300, Loss: 0.0475 | 0.0387
Epoch 109/300, Loss: 0.0458 | 0.0362
Epoch 110/300, Loss: 0.0450 | 0.0378
Epoch 111/300, Loss: 0.0447 | 0.0351
Epoch 112/300, Loss: 0.0484 | 0.0491
Epoch 113/300, Loss: 0.0502 | 0.0357
Epoch 114/300, Loss: 0.0474 | 0.0332
Epoch 115/300, Loss: 0.0459 | 0.0327
Epoch 116/300, Loss: 0.0449 | 0.0339
Epoch 117/300, Loss: 0.0444 | 0.0337
Epoch 118/300, Loss: 0.0436 | 0.0333
Epoch 119/300, Loss: 0.0430 | 0.0341
Epoch 120/300, Loss: 0.0433 | 0.0324
Epoch 121/300, Loss: 0.0424 | 0.0314
Epoch 122/300, Loss: 0.0423 | 0.0307
Epoch 123/300, Loss: 0.0421 | 0.0303
Epoch 124/300, Loss: 0.0418 | 0.0301
Epoch 125/300, Loss: 0.0416 | 0.0312
Epoch 126/300, Loss: 0.0418 | 0.0297
Epoch 127/300, Loss: 0.0409 | 0.0297
Epoch 128/300, Loss: 0.0408 | 0.0295
Epoch 129/300, Loss: 0.0407 | 0.0295
Epoch 130/300, Loss: 0.0407 | 0.0294
Epoch 131/300, Loss: 0.0406 | 0.0302
Epoch 132/300, Loss: 0.0409 | 0.0293
Epoch 133/300, Loss: 0.0403 | 0.0295
Epoch 134/300, Loss: 0.0404 | 0.0293
Epoch 135/300, Loss: 0.0401 | 0.0295
Epoch 136/300, Loss: 0.0402 | 0.0293
Epoch 137/300, Loss: 0.0400 | 0.0295
Epoch 138/300, Loss: 0.0401 | 0.0292
Epoch 139/300, Loss: 0.0398 | 0.0294
Epoch 140/300, Loss: 0.0399 | 0.0292
Epoch 141/300, Loss: 0.0397 | 0.0294
Epoch 142/300, Loss: 0.0397 | 0.0292
Epoch 143/300, Loss: 0.0395 | 0.0293
Epoch 144/300, Loss: 0.0395 | 0.0292
Epoch 145/300, Loss: 0.0394 | 0.0293
Epoch 146/300, Loss: 0.0394 | 0.0292
Epoch 147/300, Loss: 0.0393 | 0.0293
Epoch 148/300, Loss: 0.0393 | 0.0293
Epoch 149/300, Loss: 0.0392 | 0.0293
Epoch 150/300, Loss: 0.0391 | 0.0292
Epoch 151/300, Loss: 0.0390 | 0.0293
Epoch 152/300, Loss: 0.0390 | 0.0292
Epoch 153/300, Loss: 0.0389 | 0.0294
Epoch 154/300, Loss: 0.0390 | 0.0292
Epoch 155/300, Loss: 0.0388 | 0.0294
Epoch 156/300, Loss: 0.0388 | 0.0293
Epoch 157/300, Loss: 0.0387 | 0.0294
Epoch 158/300, Loss: 0.0387 | 0.0293
Epoch 159/300, Loss: 0.0386 | 0.0294
Epoch 160/300, Loss: 0.0386 | 0.0293
Epoch 161/300, Loss: 0.0385 | 0.0294
Epoch 162/300, Loss: 0.0385 | 0.0294
Epoch 163/300, Loss: 0.0384 | 0.0294
Epoch 164/300, Loss: 0.0384 | 0.0294
Epoch 165/300, Loss: 0.0383 | 0.0294
Epoch 166/300, Loss: 0.0383 | 0.0294
Epoch 167/300, Loss: 0.0382 | 0.0295
Epoch 168/300, Loss: 0.0382 | 0.0294
Epoch 169/300, Loss: 0.0382 | 0.0295
Epoch 170/300, Loss: 0.0382 | 0.0295
Epoch 171/300, Loss: 0.0381 | 0.0295
Epoch 172/300, Loss: 0.0381 | 0.0295
Epoch 173/300, Loss: 0.0380 | 0.0296
Epoch 174/300, Loss: 0.0380 | 0.0296
Epoch 175/300, Loss: 0.0379 | 0.0296
Epoch 176/300, Loss: 0.0380 | 0.0296
Epoch 177/300, Loss: 0.0379 | 0.0296
Epoch 178/300, Loss: 0.0379 | 0.0296
Epoch 179/300, Loss: 0.0378 | 0.0297
Epoch 180/300, Loss: 0.0378 | 0.0297
Epoch 181/300, Loss: 0.0378 | 0.0297
Epoch 182/300, Loss: 0.0378 | 0.0297
Epoch 183/300, Loss: 0.0377 | 0.0297
Epoch 184/300, Loss: 0.0377 | 0.0297
Epoch 185/300, Loss: 0.0377 | 0.0297
Epoch 186/300, Loss: 0.0376 | 0.0298
Epoch 187/300, Loss: 0.0376 | 0.0298
Epoch 188/300, Loss: 0.0376 | 0.0298
Epoch 189/300, Loss: 0.0375 | 0.0298
Epoch 190/300, Loss: 0.0375 | 0.0299
Epoch 191/300, Loss: 0.0375 | 0.0299
Epoch 192/300, Loss: 0.0374 | 0.0299
Epoch 193/300, Loss: 0.0374 | 0.0299
Epoch 194/300, Loss: 0.0373 | 0.0300
Epoch 195/300, Loss: 0.0373 | 0.0300
Epoch 196/300, Loss: 0.0373 | 0.0300
Epoch 197/300, Loss: 0.0372 | 0.0300
Epoch 198/300, Loss: 0.0372 | 0.0301
Epoch 199/300, Loss: 0.0371 | 0.0301
Epoch 200/300, Loss: 0.0371 | 0.0301
Epoch 201/300, Loss: 0.0371 | 0.0301
Epoch 202/300, Loss: 0.0370 | 0.0301
Epoch 203/300, Loss: 0.0370 | 0.0301
Epoch 204/300, Loss: 0.0370 | 0.0301
Epoch 205/300, Loss: 0.0369 | 0.0301
Epoch 206/300, Loss: 0.0369 | 0.0301
Epoch 207/300, Loss: 0.0369 | 0.0301
Epoch 208/300, Loss: 0.0368 | 0.0301
Epoch 209/300, Loss: 0.0368 | 0.0301
Epoch 210/300, Loss: 0.0368 | 0.0301
Epoch 211/300, Loss: 0.0367 | 0.0302
Epoch 212/300, Loss: 0.0367 | 0.0301
Epoch 213/300, Loss: 0.0367 | 0.0302
Epoch 214/300, Loss: 0.0367 | 0.0301
Epoch 215/300, Loss: 0.0366 | 0.0301
Epoch 216/300, Loss: 0.0366 | 0.0301
Epoch 217/300, Loss: 0.0366 | 0.0301
Epoch 218/300, Loss: 0.0366 | 0.0301
Epoch 219/300, Loss: 0.0365 | 0.0301
Epoch 220/300, Loss: 0.0365 | 0.0301
Epoch 221/300, Loss: 0.0365 | 0.0301
Epoch 222/300, Loss: 0.0365 | 0.0301
Epoch 223/300, Loss: 0.0364 | 0.0301
Epoch 224/300, Loss: 0.0364 | 0.0301
Epoch 225/300, Loss: 0.0364 | 0.0301
Epoch 226/300, Loss: 0.0364 | 0.0301
Epoch 227/300, Loss: 0.0364 | 0.0301
Epoch 228/300, Loss: 0.0363 | 0.0301
Epoch 229/300, Loss: 0.0363 | 0.0301
Epoch 230/300, Loss: 0.0363 | 0.0301
Epoch 231/300, Loss: 0.0363 | 0.0302
Epoch 232/300, Loss: 0.0363 | 0.0302
Epoch 233/300, Loss: 0.0362 | 0.0302
Epoch 234/300, Loss: 0.0362 | 0.0302
Epoch 235/300, Loss: 0.0362 | 0.0302
Epoch 236/300, Loss: 0.0362 | 0.0302
Epoch 237/300, Loss: 0.0362 | 0.0302
Epoch 238/300, Loss: 0.0362 | 0.0302
Epoch 239/300, Loss: 0.0361 | 0.0302
Epoch 240/300, Loss: 0.0361 | 0.0302
Epoch 241/300, Loss: 0.0361 | 0.0302
Epoch 242/300, Loss: 0.0361 | 0.0302
Epoch 243/300, Loss: 0.0361 | 0.0302
Epoch 244/300, Loss: 0.0361 | 0.0302
Epoch 245/300, Loss: 0.0360 | 0.0302
Epoch 246/300, Loss: 0.0360 | 0.0302
Epoch 247/300, Loss: 0.0360 | 0.0302
Epoch 248/300, Loss: 0.0360 | 0.0302
Epoch 249/300, Loss: 0.0360 | 0.0302
Epoch 250/300, Loss: 0.0360 | 0.0302
Epoch 251/300, Loss: 0.0360 | 0.0302
Epoch 252/300, Loss: 0.0359 | 0.0302
Epoch 253/300, Loss: 0.0359 | 0.0302
Epoch 254/300, Loss: 0.0359 | 0.0302
Epoch 255/300, Loss: 0.0359 | 0.0303
Epoch 256/300, Loss: 0.0359 | 0.0303
Epoch 257/300, Loss: 0.0359 | 0.0303
Epoch 258/300, Loss: 0.0359 | 0.0303
Epoch 259/300, Loss: 0.0359 | 0.0303
Epoch 260/300, Loss: 0.0358 | 0.0303
Epoch 261/300, Loss: 0.0358 | 0.0303
Epoch 262/300, Loss: 0.0358 | 0.0303
Epoch 263/300, Loss: 0.0358 | 0.0303
Epoch 264/300, Loss: 0.0358 | 0.0303
Epoch 265/300, Loss: 0.0358 | 0.0303
Epoch 266/300, Loss: 0.0358 | 0.0303
Epoch 267/300, Loss: 0.0358 | 0.0303
Epoch 268/300, Loss: 0.0358 | 0.0303
Epoch 269/300, Loss: 0.0357 | 0.0303
Epoch 270/300, Loss: 0.0357 | 0.0303
Epoch 271/300, Loss: 0.0357 | 0.0303
Epoch 272/300, Loss: 0.0357 | 0.0303
Epoch 273/300, Loss: 0.0357 | 0.0303
Epoch 274/300, Loss: 0.0357 | 0.0303
Epoch 275/300, Loss: 0.0357 | 0.0303
Epoch 276/300, Loss: 0.0357 | 0.0303
Epoch 277/300, Loss: 0.0357 | 0.0303
Epoch 278/300, Loss: 0.0357 | 0.0303
Epoch 279/300, Loss: 0.0357 | 0.0303
Epoch 280/300, Loss: 0.0356 | 0.0303
Epoch 281/300, Loss: 0.0356 | 0.0303
Epoch 282/300, Loss: 0.0356 | 0.0303
Epoch 283/300, Loss: 0.0356 | 0.0303
Epoch 284/300, Loss: 0.0356 | 0.0303
Epoch 285/300, Loss: 0.0356 | 0.0303
Epoch 286/300, Loss: 0.0356 | 0.0303
Epoch 287/300, Loss: 0.0356 | 0.0303
Epoch 288/300, Loss: 0.0356 | 0.0303
Epoch 289/300, Loss: 0.0356 | 0.0303
Epoch 290/300, Loss: 0.0356 | 0.0303
Epoch 291/300, Loss: 0.0356 | 0.0303
Epoch 292/300, Loss: 0.0356 | 0.0303
Epoch 293/300, Loss: 0.0356 | 0.0303
Epoch 294/300, Loss: 0.0355 | 0.0303
Epoch 295/300, Loss: 0.0355 | 0.0303
Epoch 296/300, Loss: 0.0355 | 0.0303
Epoch 297/300, Loss: 0.0355 | 0.0303
Epoch 298/300, Loss: 0.0355 | 0.0303
Epoch 299/300, Loss: 0.0355 | 0.0303
Epoch 300/300, Loss: 0.0355 | 0.0303
Runtime (seconds): 525.4681677818298
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 406.42882383451797
RMSE: 20.160079956054688
MAE: 20.160079956054688
R-squared: nan
[207.30008]
