[32m[I 2025-01-06 20:26:37,082][0m A new study created in memory with name: no-name-cda0348b-80ca-4fe0-88be-5c1d0d90b2c1[0m
[32m[I 2025-01-06 20:31:43,481][0m Trial 0 finished with value: 0.14567828178405762 and parameters: {'observation_period_num': 206, 'train_rates': 0.9357247270583606, 'learning_rate': 0.0005270892626755293, 'batch_size': 224, 'step_size': 8, 'gamma': 0.798005292595787}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:34:50,005][0m Trial 1 finished with value: 0.1732489032049974 and parameters: {'observation_period_num': 136, 'train_rates': 0.9152718350862226, 'learning_rate': 8.487616489696208e-05, 'batch_size': 140, 'step_size': 3, 'gamma': 0.9434981932765015}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:37:28,400][0m Trial 2 finished with value: 0.725480854511261 and parameters: {'observation_period_num': 113, 'train_rates': 0.9875587204500218, 'learning_rate': 1.3899849253003186e-06, 'batch_size': 97, 'step_size': 15, 'gamma': 0.9063562679324383}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:43:28,755][0m Trial 3 finished with value: 0.29514890909194946 and parameters: {'observation_period_num': 236, 'train_rates': 0.9218575654899401, 'learning_rate': 1.7966556345167537e-05, 'batch_size': 224, 'step_size': 10, 'gamma': 0.906755844736102}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:44:42,174][0m Trial 4 finished with value: 0.24568643504583493 and parameters: {'observation_period_num': 61, 'train_rates': 0.8679694801500489, 'learning_rate': 0.0002991559152398871, 'batch_size': 252, 'step_size': 12, 'gamma': 0.9136752857383957}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:45:07,434][0m Trial 5 finished with value: 0.19930646849745443 and parameters: {'observation_period_num': 11, 'train_rates': 0.9130075893312566, 'learning_rate': 2.5149304183217086e-05, 'batch_size': 183, 'step_size': 3, 'gamma': 0.9815040596718149}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:46:18,163][0m Trial 6 finished with value: 0.5278368521092544 and parameters: {'observation_period_num': 59, 'train_rates': 0.8616122256007384, 'learning_rate': 2.2506929029811808e-05, 'batch_size': 228, 'step_size': 7, 'gamma': 0.8258746799896919}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:47:44,339][0m Trial 7 finished with value: 0.5082330550826052 and parameters: {'observation_period_num': 66, 'train_rates': 0.8285227749729044, 'learning_rate': 0.0006203120634669213, 'batch_size': 118, 'step_size': 3, 'gamma': 0.7807446566913234}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:50:42,108][0m Trial 8 finished with value: 0.6842610164591331 and parameters: {'observation_period_num': 140, 'train_rates': 0.7911269818338803, 'learning_rate': 0.0007596267978201148, 'batch_size': 241, 'step_size': 12, 'gamma': 0.7993897043730444}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:53:02,273][0m Trial 9 finished with value: 1.0202333381258208 and parameters: {'observation_period_num': 124, 'train_rates': 0.7445993232203617, 'learning_rate': 0.0007816678276952438, 'batch_size': 208, 'step_size': 13, 'gamma': 0.9109635476019967}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 20:58:20,732][0m Trial 10 finished with value: 1.013872469847019 and parameters: {'observation_period_num': 245, 'train_rates': 0.6169440296736263, 'learning_rate': 9.824142755804784e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.7515391012370441}. Best is trial 0 with value: 0.14567828178405762.[0m
[32m[I 2025-01-06 21:02:56,801][0m Trial 11 finished with value: 0.12406846135854721 and parameters: {'observation_period_num': 185, 'train_rates': 0.9813661808088591, 'learning_rate': 0.00012741364806169856, 'batch_size': 169, 'step_size': 5, 'gamma': 0.9846627434539342}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:07:53,542][0m Trial 12 finished with value: 0.13460026681423187 and parameters: {'observation_period_num': 193, 'train_rates': 0.986164911479397, 'learning_rate': 0.00016375820242614354, 'batch_size': 169, 'step_size': 6, 'gamma': 0.8453706237820523}. Best is trial 11 with value: 0.12406846135854721.[0m
Early stopping at epoch 79
[32m[I 2025-01-06 21:11:32,399][0m Trial 13 finished with value: 0.3495112955570221 and parameters: {'observation_period_num': 184, 'train_rates': 0.9851624100523446, 'learning_rate': 8.407124886921041e-05, 'batch_size': 158, 'step_size': 1, 'gamma': 0.8526530032476002}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:15:05,189][0m Trial 14 finished with value: 1.2570951175936966 and parameters: {'observation_period_num': 181, 'train_rates': 0.6702716543542583, 'learning_rate': 7.729020531553982e-06, 'batch_size': 79, 'step_size': 5, 'gamma': 0.8665685879574793}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:19:19,509][0m Trial 15 finished with value: 0.14913088083267212 and parameters: {'observation_period_num': 171, 'train_rates': 0.9882030787087478, 'learning_rate': 0.00023243601524814921, 'batch_size': 177, 'step_size': 5, 'gamma': 0.9866899551594044}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:23:58,990][0m Trial 16 finished with value: 0.6563657692500523 and parameters: {'observation_period_num': 215, 'train_rates': 0.763024225591033, 'learning_rate': 0.000170094373557242, 'batch_size': 186, 'step_size': 5, 'gamma': 0.8416106327466368}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:27:14,139][0m Trial 17 finished with value: 0.7796779229090764 and parameters: {'observation_period_num': 161, 'train_rates': 0.7122532023638547, 'learning_rate': 5.389206922425412e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.9506852633846569}. Best is trial 11 with value: 0.12406846135854721.[0m
Early stopping at epoch 88
[32m[I 2025-01-06 21:31:51,470][0m Trial 18 finished with value: 1.0640171766281128 and parameters: {'observation_period_num': 205, 'train_rates': 0.9494559463964967, 'learning_rate': 8.225667757815312e-06, 'batch_size': 145, 'step_size': 1, 'gamma': 0.8814152673572183}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:34:02,993][0m Trial 19 finished with value: 0.23705214974672897 and parameters: {'observation_period_num': 103, 'train_rates': 0.8713458559455814, 'learning_rate': 0.00017713550984521512, 'batch_size': 124, 'step_size': 6, 'gamma': 0.8246493635870085}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:37:25,560][0m Trial 20 finished with value: 0.39518317407918435 and parameters: {'observation_period_num': 155, 'train_rates': 0.8336318408215044, 'learning_rate': 4.274115174161913e-05, 'batch_size': 169, 'step_size': 9, 'gamma': 0.8785389931436329}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:42:33,330][0m Trial 21 finished with value: 0.14156082272529602 and parameters: {'observation_period_num': 208, 'train_rates': 0.9465037855734332, 'learning_rate': 0.000305500728267666, 'batch_size': 202, 'step_size': 8, 'gamma': 0.7931774946028506}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:48:23,890][0m Trial 22 finished with value: 0.14231501519680023 and parameters: {'observation_period_num': 225, 'train_rates': 0.9535393426764412, 'learning_rate': 0.00030977571018672695, 'batch_size': 199, 'step_size': 8, 'gamma': 0.7941517346880896}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:53:12,916][0m Trial 23 finished with value: 0.2752691518377375 and parameters: {'observation_period_num': 199, 'train_rates': 0.8925402137875217, 'learning_rate': 0.00013858773493935923, 'batch_size': 203, 'step_size': 4, 'gamma': 0.7587721510990466}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 21:59:40,224][0m Trial 24 finished with value: 0.13673682510852814 and parameters: {'observation_period_num': 248, 'train_rates': 0.9632453630220009, 'learning_rate': 0.0003594633114696666, 'batch_size': 160, 'step_size': 7, 'gamma': 0.82077019368243}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:06:12,896][0m Trial 25 finished with value: 0.1317872256040573 and parameters: {'observation_period_num': 251, 'train_rates': 0.9576583667639206, 'learning_rate': 0.0003807136714527077, 'batch_size': 158, 'step_size': 6, 'gamma': 0.8255298540519144}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:11:44,484][0m Trial 26 finished with value: 0.21509034513569564 and parameters: {'observation_period_num': 225, 'train_rates': 0.8985586301163925, 'learning_rate': 0.00013677490694595452, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8543146179819738}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:16:18,393][0m Trial 27 finished with value: 0.17817191779613495 and parameters: {'observation_period_num': 186, 'train_rates': 0.9689845680057942, 'learning_rate': 5.917679657226867e-05, 'batch_size': 146, 'step_size': 4, 'gamma': 0.8887950091204827}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:18:13,277][0m Trial 28 finished with value: 1.3180876932326395 and parameters: {'observation_period_num': 93, 'train_rates': 0.8357319170863239, 'learning_rate': 1.25492911724852e-06, 'batch_size': 129, 'step_size': 2, 'gamma': 0.9406278089167954}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:21:57,225][0m Trial 29 finished with value: 0.2663668096065521 and parameters: {'observation_period_num': 155, 'train_rates': 0.9435350007701392, 'learning_rate': 1.347883261761273e-05, 'batch_size': 159, 'step_size': 6, 'gamma': 0.9663474309871827}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:27:50,027][0m Trial 30 finished with value: 1.1953571616667102 and parameters: {'observation_period_num': 233, 'train_rates': 0.9302578648531704, 'learning_rate': 0.0009261963427323643, 'batch_size': 92, 'step_size': 9, 'gamma': 0.8128840715861456}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:34:25,582][0m Trial 31 finished with value: 0.14056256413459778 and parameters: {'observation_period_num': 252, 'train_rates': 0.9621620921515884, 'learning_rate': 0.00046351713333338593, 'batch_size': 163, 'step_size': 7, 'gamma': 0.83481972273506}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:41:05,089][0m Trial 32 finished with value: 0.16764290630817413 and parameters: {'observation_period_num': 252, 'train_rates': 0.9721087621261552, 'learning_rate': 0.00030275039755323687, 'batch_size': 147, 'step_size': 4, 'gamma': 0.8156972268488157}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:45:51,596][0m Trial 33 finished with value: 0.15492101860833046 and parameters: {'observation_period_num': 194, 'train_rates': 0.9288520947380348, 'learning_rate': 0.0004856988756237283, 'batch_size': 187, 'step_size': 6, 'gamma': 0.856769417174767}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:51:15,907][0m Trial 34 finished with value: 0.1851981819971748 and parameters: {'observation_period_num': 218, 'train_rates': 0.9077960270084612, 'learning_rate': 9.606834940512116e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.7709436605304438}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 22:57:33,307][0m Trial 35 finished with value: 0.14201726019382477 and parameters: {'observation_period_num': 237, 'train_rates': 0.9824979465873072, 'learning_rate': 0.000408274444952991, 'batch_size': 170, 'step_size': 7, 'gamma': 0.8411563778577522}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:03:23,428][0m Trial 36 finished with value: 1.262379788979888 and parameters: {'observation_period_num': 237, 'train_rates': 0.8827248175257301, 'learning_rate': 2.3354842811037335e-06, 'batch_size': 157, 'step_size': 5, 'gamma': 0.8056330208083774}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:07:27,444][0m Trial 37 finished with value: 0.13830839372598208 and parameters: {'observation_period_num': 171, 'train_rates': 0.9303453170935512, 'learning_rate': 0.00020027891490933828, 'batch_size': 108, 'step_size': 4, 'gamma': 0.9249960394352129}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:12:55,001][0m Trial 38 finished with value: 0.13773415982723236 and parameters: {'observation_period_num': 214, 'train_rates': 0.9628317398159556, 'learning_rate': 0.00012340267673838375, 'batch_size': 185, 'step_size': 7, 'gamma': 0.891727101512449}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:16:15,057][0m Trial 39 finished with value: 0.16808619968562957 and parameters: {'observation_period_num': 143, 'train_rates': 0.9123105048607059, 'learning_rate': 0.0005877491688027577, 'batch_size': 219, 'step_size': 10, 'gamma': 0.8269533401422845}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:16:48,684][0m Trial 40 finished with value: 0.323405041269095 and parameters: {'observation_period_num': 12, 'train_rates': 0.8566593120360362, 'learning_rate': 6.784214182460881e-05, 'batch_size': 134, 'step_size': 3, 'gamma': 0.8664051698494267}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:22:17,242][0m Trial 41 finished with value: 0.16600224375724792 and parameters: {'observation_period_num': 215, 'train_rates': 0.9619163054323999, 'learning_rate': 3.3879487988848425e-05, 'batch_size': 187, 'step_size': 7, 'gamma': 0.9698207727008009}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:28:17,081][0m Trial 42 finished with value: 0.14824123680591583 and parameters: {'observation_period_num': 228, 'train_rates': 0.9899591574364184, 'learning_rate': 0.00013139397718973283, 'batch_size': 177, 'step_size': 8, 'gamma': 0.8980900570162073}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:34:38,814][0m Trial 43 finished with value: 0.14907985925674438 and parameters: {'observation_period_num': 242, 'train_rates': 0.9685317215496135, 'learning_rate': 0.00023954547584588963, 'batch_size': 156, 'step_size': 6, 'gamma': 0.7803288971998595}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:39:19,278][0m Trial 44 finished with value: 0.13524188101291656 and parameters: {'observation_period_num': 191, 'train_rates': 0.9453123020446265, 'learning_rate': 0.00011086362082508295, 'batch_size': 211, 'step_size': 7, 'gamma': 0.922012790521302}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:44:09,756][0m Trial 45 finished with value: 0.15277829031611598 and parameters: {'observation_period_num': 195, 'train_rates': 0.9225137778243732, 'learning_rate': 0.00036805217738280815, 'batch_size': 212, 'step_size': 5, 'gamma': 0.9247641378699004}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:46:49,102][0m Trial 46 finished with value: 0.15641865134239197 and parameters: {'observation_period_num': 119, 'train_rates': 0.9409648812297315, 'learning_rate': 8.029426314347141e-05, 'batch_size': 234, 'step_size': 8, 'gamma': 0.9562441645691444}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:51:10,646][0m Trial 47 finished with value: 0.22158969938755035 and parameters: {'observation_period_num': 177, 'train_rates': 0.9785268106190359, 'learning_rate': 0.0006682418170592665, 'batch_size': 241, 'step_size': 14, 'gamma': 0.9369701916204635}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:55:27,692][0m Trial 48 finished with value: 0.41688846699453636 and parameters: {'observation_period_num': 193, 'train_rates': 0.7980929303619426, 'learning_rate': 0.0002278956133938272, 'batch_size': 196, 'step_size': 11, 'gamma': 0.8441487128245198}. Best is trial 11 with value: 0.12406846135854721.[0m
[32m[I 2025-01-06 23:58:06,731][0m Trial 49 finished with value: 0.9093338219801883 and parameters: {'observation_period_num': 148, 'train_rates': 0.6208265757784324, 'learning_rate': 0.00010868602031417166, 'batch_size': 220, 'step_size': 6, 'gamma': 0.9748277811086279}. Best is trial 11 with value: 0.12406846135854721.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 1.1130 | 1.1686
Epoch 2/300, Loss: 0.6999 | 0.7315
Epoch 3/300, Loss: 0.5674 | 0.6177
Epoch 4/300, Loss: 0.4580 | 0.5223
Epoch 5/300, Loss: 0.3907 | 0.4202
Epoch 6/300, Loss: 0.3684 | 0.3800
Epoch 7/300, Loss: 0.3698 | 0.3699
Epoch 8/300, Loss: 0.3920 | 0.3521
Epoch 9/300, Loss: 0.3475 | 0.3452
Epoch 10/300, Loss: 0.3268 | 0.3165
Epoch 11/300, Loss: 0.3436 | 0.2964
Epoch 12/300, Loss: 0.3762 | 0.2928
Epoch 13/300, Loss: 0.4572 | 0.4806
Epoch 14/300, Loss: 0.3347 | 0.3041
Epoch 15/300, Loss: 0.2624 | 0.2897
Epoch 16/300, Loss: 0.2435 | 0.2581
Epoch 17/300, Loss: 0.2422 | 0.2502
Epoch 18/300, Loss: 0.2369 | 0.2476
Epoch 19/300, Loss: 0.2177 | 0.2169
Epoch 20/300, Loss: 0.2041 | 0.2119
Epoch 21/300, Loss: 0.2015 | 0.2081
Epoch 22/300, Loss: 0.1968 | 0.2005
Epoch 23/300, Loss: 0.2018 | 0.2013
Epoch 24/300, Loss: 0.2064 | 0.2031
Epoch 25/300, Loss: 0.2152 | 0.1975
Epoch 26/300, Loss: 0.2277 | 0.1916
Epoch 27/300, Loss: 0.2197 | 0.1874
Epoch 28/300, Loss: 0.2511 | 0.1945
Epoch 29/300, Loss: 0.2131 | 0.1932
Epoch 30/300, Loss: 0.2163 | 0.1941
Epoch 31/300, Loss: 0.2280 | 0.1947
Epoch 32/300, Loss: 0.2244 | 0.1787
Epoch 33/300, Loss: 0.2257 | 0.1825
Epoch 34/300, Loss: 0.1940 | 0.1810
Epoch 35/300, Loss: 0.1840 | 0.1770
Epoch 36/300, Loss: 0.1850 | 0.1738
Epoch 37/300, Loss: 0.1899 | 0.1745
Epoch 38/300, Loss: 0.2035 | 0.1745
Epoch 39/300, Loss: 0.2032 | 0.1833
Epoch 40/300, Loss: 0.1986 | 0.1702
Epoch 41/300, Loss: 0.1836 | 0.1854
Epoch 42/300, Loss: 0.1730 | 0.1656
Epoch 43/300, Loss: 0.1682 | 0.1655
Epoch 44/300, Loss: 0.1645 | 0.1612
Epoch 45/300, Loss: 0.1626 | 0.1572
Epoch 46/300, Loss: 0.1611 | 0.1565
Epoch 47/300, Loss: 0.1607 | 0.1518
Epoch 48/300, Loss: 0.1604 | 0.1536
Epoch 49/300, Loss: 0.1602 | 0.1489
Epoch 50/300, Loss: 0.1610 | 0.1508
Epoch 51/300, Loss: 0.1609 | 0.1462
Epoch 52/300, Loss: 0.1632 | 0.1477
Epoch 53/300, Loss: 0.1612 | 0.1460
Epoch 54/300, Loss: 0.1664 | 0.1532
Epoch 55/300, Loss: 0.1741 | 0.1555
Epoch 56/300, Loss: 0.1848 | 0.1672
Epoch 57/300, Loss: 0.2053 | 0.1554
Epoch 58/300, Loss: 0.1915 | 0.1594
Epoch 59/300, Loss: 0.1956 | 0.1589
Epoch 60/300, Loss: 0.1749 | 0.1552
Epoch 61/300, Loss: 0.1656 | 0.1567
Epoch 62/300, Loss: 0.1607 | 0.1475
Epoch 63/300, Loss: 0.1567 | 0.1520
Epoch 64/300, Loss: 0.1549 | 0.1432
Epoch 65/300, Loss: 0.1543 | 0.1473
Epoch 66/300, Loss: 0.1533 | 0.1414
Epoch 67/300, Loss: 0.1529 | 0.1440
Epoch 68/300, Loss: 0.1508 | 0.1423
Epoch 69/300, Loss: 0.1493 | 0.1391
Epoch 70/300, Loss: 0.1481 | 0.1416
Epoch 71/300, Loss: 0.1481 | 0.1357
Epoch 72/300, Loss: 0.1498 | 0.1389
Epoch 73/300, Loss: 0.1531 | 0.1349
Epoch 74/300, Loss: 0.1631 | 0.1425
Epoch 75/300, Loss: 0.1657 | 0.1343
Epoch 76/300, Loss: 0.1694 | 0.1467
Epoch 77/300, Loss: 0.1646 | 0.1433
Epoch 78/300, Loss: 0.1582 | 0.1374
Epoch 79/300, Loss: 0.1540 | 0.1422
Epoch 80/300, Loss: 0.1499 | 0.1363
Epoch 81/300, Loss: 0.1506 | 0.1392
Epoch 82/300, Loss: 0.1476 | 0.1342
Epoch 83/300, Loss: 0.1465 | 0.1410
Epoch 84/300, Loss: 0.1438 | 0.1290
Epoch 85/300, Loss: 0.1413 | 0.1395
Epoch 86/300, Loss: 0.1414 | 0.1239
Epoch 87/300, Loss: 0.1446 | 0.1444
Epoch 88/300, Loss: 0.1505 | 0.1265
Epoch 89/300, Loss: 0.1453 | 0.1356
Epoch 90/300, Loss: 0.1413 | 0.1284
Epoch 91/300, Loss: 0.1395 | 0.1254
Epoch 92/300, Loss: 0.1444 | 0.1373
Epoch 93/300, Loss: 0.1577 | 0.1255
Epoch 94/300, Loss: 0.1611 | 0.1428
Epoch 95/300, Loss: 0.1653 | 0.1276
Epoch 96/300, Loss: 0.1561 | 0.1376
Epoch 97/300, Loss: 0.1527 | 0.1337
Epoch 98/300, Loss: 0.1481 | 0.1334
Epoch 99/300, Loss: 0.1434 | 0.1346
Epoch 100/300, Loss: 0.1402 | 0.1311
Epoch 101/300, Loss: 0.1407 | 0.1323
Epoch 102/300, Loss: 0.1416 | 0.1285
Epoch 103/300, Loss: 0.1437 | 0.1346
Epoch 104/300, Loss: 0.1428 | 0.1354
Epoch 105/300, Loss: 0.1376 | 0.1309
Epoch 106/300, Loss: 0.1319 | 0.1266
Epoch 107/300, Loss: 0.1289 | 0.1245
Epoch 108/300, Loss: 0.1274 | 0.1218
Epoch 109/300, Loss: 0.1278 | 0.1246
Epoch 110/300, Loss: 0.1279 | 0.1223
Epoch 111/300, Loss: 0.1276 | 0.1218
Epoch 112/300, Loss: 0.1272 | 0.1220
Epoch 113/300, Loss: 0.1267 | 0.1212
Epoch 114/300, Loss: 0.1278 | 0.1215
Epoch 115/300, Loss: 0.1285 | 0.1190
Epoch 116/300, Loss: 0.1294 | 0.1239
Epoch 117/300, Loss: 0.1293 | 0.1205
Epoch 118/300, Loss: 0.1285 | 0.1243
Epoch 119/300, Loss: 0.1280 | 0.1189
Epoch 120/300, Loss: 0.1270 | 0.1211
Epoch 121/300, Loss: 0.1267 | 0.1215
Epoch 122/300, Loss: 0.1278 | 0.1207
Epoch 123/300, Loss: 0.1271 | 0.1240
Epoch 124/300, Loss: 0.1248 | 0.1181
Epoch 125/300, Loss: 0.1221 | 0.1225
Epoch 126/300, Loss: 0.1196 | 0.1166
Epoch 127/300, Loss: 0.1192 | 0.1200
Epoch 128/300, Loss: 0.1192 | 0.1154
Epoch 129/300, Loss: 0.1197 | 0.1180
Epoch 130/300, Loss: 0.1196 | 0.1162
Epoch 131/300, Loss: 0.1192 | 0.1178
Epoch 132/300, Loss: 0.1186 | 0.1165
Epoch 133/300, Loss: 0.1194 | 0.1165
Epoch 134/300, Loss: 0.1200 | 0.1146
Epoch 135/300, Loss: 0.1208 | 0.1139
Epoch 136/300, Loss: 0.1231 | 0.1145
Epoch 137/300, Loss: 0.1231 | 0.1146
Epoch 138/300, Loss: 0.1253 | 0.1149
Epoch 139/300, Loss: 0.1251 | 0.1172
Epoch 140/300, Loss: 0.1240 | 0.1123
Epoch 141/300, Loss: 0.1213 | 0.1163
Epoch 142/300, Loss: 0.1213 | 0.1115
Epoch 143/300, Loss: 0.1201 | 0.1143
Epoch 144/300, Loss: 0.1190 | 0.1118
Epoch 145/300, Loss: 0.1174 | 0.1128
Epoch 146/300, Loss: 0.1162 | 0.1138
Epoch 147/300, Loss: 0.1164 | 0.1131
Epoch 148/300, Loss: 0.1167 | 0.1167
Epoch 149/300, Loss: 0.1178 | 0.1107
Epoch 150/300, Loss: 0.1173 | 0.1185
Epoch 151/300, Loss: 0.1174 | 0.1111
Epoch 152/300, Loss: 0.1161 | 0.1171
Epoch 153/300, Loss: 0.1162 | 0.1109
Epoch 154/300, Loss: 0.1148 | 0.1128
Epoch 155/300, Loss: 0.1133 | 0.1122
Epoch 156/300, Loss: 0.1127 | 0.1113
Epoch 157/300, Loss: 0.1112 | 0.1123
Epoch 158/300, Loss: 0.1105 | 0.1104
Epoch 159/300, Loss: 0.1113 | 0.1121
Epoch 160/300, Loss: 0.1130 | 0.1091
Epoch 161/300, Loss: 0.1177 | 0.1124
Epoch 162/300, Loss: 0.1188 | 0.1130
Epoch 163/300, Loss: 0.1229 | 0.1143
Epoch 164/300, Loss: 0.1192 | 0.1169
Epoch 165/300, Loss: 0.1167 | 0.1115
Epoch 166/300, Loss: 0.1145 | 0.1110
Epoch 167/300, Loss: 0.1132 | 0.1123
Epoch 168/300, Loss: 0.1111 | 0.1103
Epoch 169/300, Loss: 0.1105 | 0.1118
Epoch 170/300, Loss: 0.1104 | 0.1072
Epoch 171/300, Loss: 0.1111 | 0.1137
Epoch 172/300, Loss: 0.1103 | 0.1059
Epoch 173/300, Loss: 0.1112 | 0.1120
Epoch 174/300, Loss: 0.1091 | 0.1042
Epoch 175/300, Loss: 0.1094 | 0.1128
Epoch 176/300, Loss: 0.1082 | 0.1048
Epoch 177/300, Loss: 0.1074 | 0.1084
Epoch 178/300, Loss: 0.1072 | 0.1044
Epoch 179/300, Loss: 0.1060 | 0.1074
Epoch 180/300, Loss: 0.1054 | 0.1055
Epoch 181/300, Loss: 0.1063 | 0.1078
Epoch 182/300, Loss: 0.1062 | 0.1082
Epoch 183/300, Loss: 0.1049 | 0.1074
Epoch 184/300, Loss: 0.1052 | 0.1057
Epoch 185/300, Loss: 0.1042 | 0.1065
Epoch 186/300, Loss: 0.1045 | 0.1056
Epoch 187/300, Loss: 0.1032 | 0.1053
Epoch 188/300, Loss: 0.1037 | 0.1033
Epoch 189/300, Loss: 0.1028 | 0.1034
Epoch 190/300, Loss: 0.1023 | 0.1044
Epoch 191/300, Loss: 0.1025 | 0.1028
Epoch 192/300, Loss: 0.1021 | 0.1037
Epoch 193/300, Loss: 0.1013 | 0.1019
Epoch 194/300, Loss: 0.1004 | 0.1041
Epoch 195/300, Loss: 0.1001 | 0.1035
Epoch 196/300, Loss: 0.1008 | 0.1045
Epoch 197/300, Loss: 0.1005 | 0.1033
Epoch 198/300, Loss: 0.1004 | 0.1019
Epoch 199/300, Loss: 0.1000 | 0.1030
Epoch 200/300, Loss: 0.0995 | 0.1010
Epoch 201/300, Loss: 0.0990 | 0.1029
Epoch 202/300, Loss: 0.1009 | 0.1023
Epoch 203/300, Loss: 0.1049 | 0.1026
Epoch 204/300, Loss: 0.1084 | 0.1046
Epoch 205/300, Loss: 0.1090 | 0.1045
Epoch 206/300, Loss: 0.1042 | 0.1023
Epoch 207/300, Loss: 0.1009 | 0.1054
Epoch 208/300, Loss: 0.0996 | 0.1016
Epoch 209/300, Loss: 0.0988 | 0.1022
Epoch 210/300, Loss: 0.0985 | 0.1032
Epoch 211/300, Loss: 0.0987 | 0.1012
Epoch 212/300, Loss: 0.0991 | 0.1030
Epoch 213/300, Loss: 0.0996 | 0.1001
Epoch 214/300, Loss: 0.0987 | 0.1024
Epoch 215/300, Loss: 0.0990 | 0.1013
Epoch 216/300, Loss: 0.0981 | 0.1027
Epoch 217/300, Loss: 0.0983 | 0.1038
Epoch 218/300, Loss: 0.0986 | 0.1014
Epoch 219/300, Loss: 0.0988 | 0.1038
Epoch 220/300, Loss: 0.0980 | 0.1008
Epoch 221/300, Loss: 0.0982 | 0.1029
Epoch 222/300, Loss: 0.0969 | 0.1001
Epoch 223/300, Loss: 0.0977 | 0.1017
Epoch 224/300, Loss: 0.0971 | 0.0999
Epoch 225/300, Loss: 0.0984 | 0.1018
Epoch 226/300, Loss: 0.0970 | 0.1007
Epoch 227/300, Loss: 0.0959 | 0.1001
Epoch 228/300, Loss: 0.0956 | 0.0999
Epoch 229/300, Loss: 0.0951 | 0.1007
Epoch 230/300, Loss: 0.0951 | 0.0980
Epoch 231/300, Loss: 0.0957 | 0.1032
Epoch 232/300, Loss: 0.0960 | 0.0999
Epoch 233/300, Loss: 0.0970 | 0.1004
Epoch 234/300, Loss: 0.0953 | 0.0991
Epoch 235/300, Loss: 0.0929 | 0.0980
Epoch 236/300, Loss: 0.0924 | 0.0988
Epoch 237/300, Loss: 0.0917 | 0.0992
Epoch 238/300, Loss: 0.0911 | 0.0981
Epoch 239/300, Loss: 0.0911 | 0.0983
Epoch 240/300, Loss: 0.0910 | 0.0992
Epoch 241/300, Loss: 0.0916 | 0.0991
Epoch 242/300, Loss: 0.0937 | 0.0990
Epoch 243/300, Loss: 0.0947 | 0.0983
Epoch 244/300, Loss: 0.0968 | 0.0999
Epoch 245/300, Loss: 0.0950 | 0.0959
Epoch 246/300, Loss: 0.0935 | 0.0989
Epoch 247/300, Loss: 0.0923 | 0.0947
Epoch 248/300, Loss: 0.0922 | 0.0988
Epoch 249/300, Loss: 0.0916 | 0.0938
Epoch 250/300, Loss: 0.0923 | 0.0977
Epoch 251/300, Loss: 0.0914 | 0.0940
Epoch 252/300, Loss: 0.0919 | 0.0976
Epoch 253/300, Loss: 0.0913 | 0.0962
Epoch 254/300, Loss: 0.0915 | 0.0992
Epoch 255/300, Loss: 0.0909 | 0.0944
Epoch 256/300, Loss: 0.0900 | 0.0980
Epoch 257/300, Loss: 0.0894 | 0.0956
Epoch 258/300, Loss: 0.0899 | 0.0984
Epoch 259/300, Loss: 0.0895 | 0.0958
Epoch 260/300, Loss: 0.0900 | 0.0980
Epoch 261/300, Loss: 0.0890 | 0.0944
Epoch 262/300, Loss: 0.0892 | 0.0972
Epoch 263/300, Loss: 0.0895 | 0.0934
Epoch 264/300, Loss: 0.0881 | 0.0938
Epoch 265/300, Loss: 0.0873 | 0.0950
Epoch 266/300, Loss: 0.0874 | 0.0946
Epoch 267/300, Loss: 0.0875 | 0.0957
Epoch 268/300, Loss: 0.0882 | 0.0929
Epoch 269/300, Loss: 0.0886 | 0.0964
Epoch 270/300, Loss: 0.0875 | 0.0930
Epoch 271/300, Loss: 0.0873 | 0.0953
Epoch 272/300, Loss: 0.0870 | 0.0942
Epoch 273/300, Loss: 0.0867 | 0.0944
Epoch 274/300, Loss: 0.0867 | 0.0935
Epoch 275/300, Loss: 0.0860 | 0.0942
Epoch 276/300, Loss: 0.0864 | 0.0941
Epoch 277/300, Loss: 0.0871 | 0.0954
Epoch 278/300, Loss: 0.0871 | 0.0942
Epoch 279/300, Loss: 0.0866 | 0.0950
Epoch 280/300, Loss: 0.0856 | 0.0941
Epoch 281/300, Loss: 0.0859 | 0.0943
Epoch 282/300, Loss: 0.0847 | 0.0933
Epoch 283/300, Loss: 0.0843 | 0.0947
Epoch 284/300, Loss: 0.0845 | 0.0934
Epoch 285/300, Loss: 0.0839 | 0.0932
Epoch 286/300, Loss: 0.0839 | 0.0927
Epoch 287/300, Loss: 0.0842 | 0.0923
Epoch 288/300, Loss: 0.0843 | 0.0954
Epoch 289/300, Loss: 0.0849 | 0.0918
Epoch 290/300, Loss: 0.0843 | 0.0933
Epoch 291/300, Loss: 0.0853 | 0.0907
Epoch 292/300, Loss: 0.0850 | 0.0929
Epoch 293/300, Loss: 0.0846 | 0.0911
Epoch 294/300, Loss: 0.0847 | 0.0918
Epoch 295/300, Loss: 0.0843 | 0.0934
Epoch 296/300, Loss: 0.0837 | 0.0929
Epoch 297/300, Loss: 0.0831 | 0.0916
Epoch 298/300, Loss: 0.0829 | 0.0927
Epoch 299/300, Loss: 0.0836 | 0.0916
Epoch 300/300, Loss: 0.0834 | 0.0917
Runtime (seconds): 823.9071056842804
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 701.7826400408521
RMSE: 26.491180419921875
MAE: 26.491180419921875
R-squared: nan
[200.53882]
