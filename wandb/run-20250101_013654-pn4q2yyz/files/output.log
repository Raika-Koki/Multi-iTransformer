ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-01 01:36:55,136][0m A new study created in memory with name: no-name-c51c7428-e774-44ec-94df-4f1fdecdbc11[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
[32m[I 2025-01-01 01:37:29,857][0m Trial 0 finished with value: 0.21663805842399597 and parameters: {'observation_period_num': 252, 'train_rates': 0.9398014291583966, 'learning_rate': 1.0720461281156929e-05, 'batch_size': 226, 'step_size': 11, 'gamma': 0.8575560531849182}. Best is trial 0 with value: 0.21663805842399597.[0m
[32m[I 2025-01-01 01:38:10,923][0m Trial 1 finished with value: 0.12706915287171455 and parameters: {'observation_period_num': 19, 'train_rates': 0.8728865138192643, 'learning_rate': 5.073537337325846e-06, 'batch_size': 145, 'step_size': 5, 'gamma': 0.8481019335501648}. Best is trial 1 with value: 0.12706915287171455.[0m
[32m[I 2025-01-01 01:38:48,408][0m Trial 2 finished with value: 0.4946099030425009 and parameters: {'observation_period_num': 198, 'train_rates': 0.8734544568258232, 'learning_rate': 1.1115881891448406e-06, 'batch_size': 159, 'step_size': 14, 'gamma': 0.8391051414500539}. Best is trial 1 with value: 0.12706915287171455.[0m
[32m[I 2025-01-01 01:39:19,978][0m Trial 3 finished with value: 0.42924882088388716 and parameters: {'observation_period_num': 220, 'train_rates': 0.8977072070027258, 'learning_rate': 3.4070902065613743e-06, 'batch_size': 228, 'step_size': 2, 'gamma': 0.9409040875114991}. Best is trial 1 with value: 0.12706915287171455.[0m
[32m[I 2025-01-01 01:39:48,811][0m Trial 4 finished with value: 0.255808495678385 and parameters: {'observation_period_num': 42, 'train_rates': 0.7085740551318083, 'learning_rate': 8.762371211431502e-05, 'batch_size': 247, 'step_size': 1, 'gamma': 0.9024549241377272}. Best is trial 1 with value: 0.12706915287171455.[0m
[32m[I 2025-01-01 01:41:13,892][0m Trial 5 finished with value: 0.1940911298332584 and parameters: {'observation_period_num': 10, 'train_rates': 0.6583197837680054, 'learning_rate': 1.3980268198206506e-06, 'batch_size': 54, 'step_size': 14, 'gamma': 0.9737714534475665}. Best is trial 1 with value: 0.12706915287171455.[0m
Early stopping at epoch 63
[32m[I 2025-01-01 01:41:36,789][0m Trial 6 finished with value: 0.1490641859640558 and parameters: {'observation_period_num': 92, 'train_rates': 0.8552876627817634, 'learning_rate': 0.00042310314983620316, 'batch_size': 178, 'step_size': 1, 'gamma': 0.7927051208284679}. Best is trial 1 with value: 0.12706915287171455.[0m
[32m[I 2025-01-01 01:42:15,763][0m Trial 7 finished with value: 0.11673128604888916 and parameters: {'observation_period_num': 190, 'train_rates': 0.989239643499346, 'learning_rate': 0.0003066783143837664, 'batch_size': 167, 'step_size': 5, 'gamma': 0.9533775894366848}. Best is trial 7 with value: 0.11673128604888916.[0m
[32m[I 2025-01-01 01:43:07,323][0m Trial 8 finished with value: 0.2733895200781706 and parameters: {'observation_period_num': 229, 'train_rates': 0.6991509780417355, 'learning_rate': 0.0001472590698536633, 'batch_size': 88, 'step_size': 14, 'gamma': 0.957601723728998}. Best is trial 7 with value: 0.11673128604888916.[0m
[32m[I 2025-01-01 01:44:11,913][0m Trial 9 finished with value: 0.2501068385184239 and parameters: {'observation_period_num': 200, 'train_rates': 0.6966571295577814, 'learning_rate': 0.00022640818996496495, 'batch_size': 71, 'step_size': 7, 'gamma': 0.8735999725870638}. Best is trial 7 with value: 0.11673128604888916.[0m
[32m[I 2025-01-01 01:49:22,851][0m Trial 10 finished with value: 0.10087749063968658 and parameters: {'observation_period_num': 146, 'train_rates': 0.9769987290504265, 'learning_rate': 3.474040627071888e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9890827664316253}. Best is trial 10 with value: 0.10087749063968658.[0m
[32m[I 2025-01-01 01:54:32,089][0m Trial 11 finished with value: 0.08691539409859426 and parameters: {'observation_period_num': 153, 'train_rates': 0.9795524007477622, 'learning_rate': 0.0009487669963740203, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9897064070641203}. Best is trial 11 with value: 0.08691539409859426.[0m
[32m[I 2025-01-01 02:00:19,596][0m Trial 12 finished with value: 0.08006427519851261 and parameters: {'observation_period_num': 142, 'train_rates': 0.9779136187767241, 'learning_rate': 0.0008662655894844652, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9182744508407746}. Best is trial 12 with value: 0.08006427519851261.[0m
[32m[I 2025-01-01 02:05:18,922][0m Trial 13 finished with value: 0.07921894737023258 and parameters: {'observation_period_num': 146, 'train_rates': 0.7928706579905156, 'learning_rate': 0.0009642381337607953, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9138169871168044}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:06:08,496][0m Trial 14 finished with value: 0.12541010450632845 and parameters: {'observation_period_num': 98, 'train_rates': 0.7866024235336075, 'learning_rate': 0.0008187245488280514, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9146618651165809}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:08:02,838][0m Trial 15 finished with value: 0.09345192056506464 and parameters: {'observation_period_num': 101, 'train_rates': 0.7955021146716368, 'learning_rate': 7.071145390455634e-05, 'batch_size': 44, 'step_size': 9, 'gamma': 0.9062751510943949}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:08:52,785][0m Trial 16 finished with value: 0.0920929520621181 and parameters: {'observation_period_num': 159, 'train_rates': 0.806121886355948, 'learning_rate': 0.0005254589117200095, 'batch_size': 107, 'step_size': 12, 'gamma': 0.7527522544351333}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:10:48,482][0m Trial 17 finished with value: 0.23131216170364305 and parameters: {'observation_period_num': 123, 'train_rates': 0.7584066698569589, 'learning_rate': 1.9911456923671896e-05, 'batch_size': 42, 'step_size': 8, 'gamma': 0.9299733118230081}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:11:26,152][0m Trial 18 finished with value: 0.16928329008554382 and parameters: {'observation_period_num': 66, 'train_rates': 0.6315973801287524, 'learning_rate': 0.0001474219680585061, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8912280601511707}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:12:37,019][0m Trial 19 finished with value: 0.09250405072654266 and parameters: {'observation_period_num': 172, 'train_rates': 0.8325704674934884, 'learning_rate': 0.000984913988324598, 'batch_size': 74, 'step_size': 12, 'gamma': 0.8215179173504911}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:17:27,213][0m Trial 20 finished with value: 0.22488822304365927 and parameters: {'observation_period_num': 126, 'train_rates': 0.7381626387163549, 'learning_rate': 7.050910047831891e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.875616674120747}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:19:59,481][0m Trial 21 finished with value: 0.10452089372434115 and parameters: {'observation_period_num': 141, 'train_rates': 0.9324799858820575, 'learning_rate': 0.0005988821832216072, 'batch_size': 36, 'step_size': 3, 'gamma': 0.922050875350975}. Best is trial 13 with value: 0.07921894737023258.[0m
[32m[I 2025-01-01 02:25:38,547][0m Trial 22 finished with value: 0.07629192851440252 and parameters: {'observation_period_num': 170, 'train_rates': 0.938674512853455, 'learning_rate': 0.0002883790944245855, 'batch_size': 16, 'step_size': 7, 'gamma': 0.9868268041318542}. Best is trial 22 with value: 0.07629192851440252.[0m
[32m[I 2025-01-01 02:27:08,983][0m Trial 23 finished with value: 0.11467415706838592 and parameters: {'observation_period_num': 182, 'train_rates': 0.936430315554261, 'learning_rate': 0.00026291439941344367, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9531400102396861}. Best is trial 22 with value: 0.07629192851440252.[0m
[32m[I 2025-01-01 02:30:31,374][0m Trial 24 finished with value: 0.06966197302069844 and parameters: {'observation_period_num': 120, 'train_rates': 0.9165106412652865, 'learning_rate': 0.0004063517966799682, 'batch_size': 27, 'step_size': 9, 'gamma': 0.8891767651113578}. Best is trial 24 with value: 0.06966197302069844.[0m
[32m[I 2025-01-01 02:32:58,895][0m Trial 25 finished with value: 0.1037819334795039 and parameters: {'observation_period_num': 116, 'train_rates': 0.9097657333409542, 'learning_rate': 0.00015305456175103982, 'batch_size': 37, 'step_size': 9, 'gamma': 0.8860484906945751}. Best is trial 24 with value: 0.06966197302069844.[0m
[32m[I 2025-01-01 02:34:05,008][0m Trial 26 finished with value: 0.05764420797405805 and parameters: {'observation_period_num': 78, 'train_rates': 0.8408434302835129, 'learning_rate': 0.0004242824823721914, 'batch_size': 82, 'step_size': 10, 'gamma': 0.8159241014899361}. Best is trial 26 with value: 0.05764420797405805.[0m
[32m[I 2025-01-01 02:35:04,468][0m Trial 27 finished with value: 0.06768022399784913 and parameters: {'observation_period_num': 76, 'train_rates': 0.8371034216692808, 'learning_rate': 3.651226638692383e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.8002031839364061}. Best is trial 26 with value: 0.05764420797405805.[0m
[32m[I 2025-01-01 02:36:04,085][0m Trial 28 finished with value: 0.06486199713415569 and parameters: {'observation_period_num': 74, 'train_rates': 0.8314873592651574, 'learning_rate': 2.899731042359844e-05, 'batch_size': 90, 'step_size': 12, 'gamma': 0.7980099082637314}. Best is trial 26 with value: 0.05764420797405805.[0m
[32m[I 2025-01-01 02:37:03,926][0m Trial 29 finished with value: 0.08940397558907266 and parameters: {'observation_period_num': 65, 'train_rates': 0.8293206933873969, 'learning_rate': 1.2008313535630687e-05, 'batch_size': 92, 'step_size': 12, 'gamma': 0.7949357268876488}. Best is trial 26 with value: 0.05764420797405805.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-01 02:37:03,932][0m A new study created in memory with name: no-name-2b64228a-f3a2-4bdc-aa08-4f1c18da3cd7[0m
[32m[I 2025-01-01 02:41:25,815][0m Trial 0 finished with value: 0.3245100141768038 and parameters: {'observation_period_num': 173, 'train_rates': 0.7538149282964488, 'learning_rate': 0.00022227472133937637, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8026843836720662}. Best is trial 0 with value: 0.3245100141768038.[0m
[32m[I 2025-01-01 02:46:16,151][0m Trial 1 finished with value: 0.15740852057933807 and parameters: {'observation_period_num': 186, 'train_rates': 0.970328681163603, 'learning_rate': 2.1813131870141115e-05, 'batch_size': 115, 'step_size': 10, 'gamma': 0.9701714163831285}. Best is trial 1 with value: 0.15740852057933807.[0m
[32m[I 2025-01-01 02:50:14,361][0m Trial 2 finished with value: 0.29676172137260437 and parameters: {'observation_period_num': 202, 'train_rates': 0.9714881667333554, 'learning_rate': 1.6041599756520549e-06, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8661056655090057}. Best is trial 1 with value: 0.15740852057933807.[0m
[32m[I 2025-01-01 02:53:23,126][0m Trial 3 finished with value: 0.27836299486062377 and parameters: {'observation_period_num': 106, 'train_rates': 0.6367952846457837, 'learning_rate': 1.7113440764916085e-05, 'batch_size': 185, 'step_size': 13, 'gamma': 0.8293425698060557}. Best is trial 1 with value: 0.15740852057933807.[0m
[32m[I 2025-01-01 02:58:40,780][0m Trial 4 finished with value: 0.1528113464128624 and parameters: {'observation_period_num': 131, 'train_rates': 0.9637316697217693, 'learning_rate': 0.0001556753782148542, 'batch_size': 97, 'step_size': 4, 'gamma': 0.9851184977666784}. Best is trial 4 with value: 0.1528113464128624.[0m
[32m[I 2025-01-01 03:09:24,799][0m Trial 5 finished with value: 0.6743175176062017 and parameters: {'observation_period_num': 205, 'train_rates': 0.6114431280182744, 'learning_rate': 1.2769294673290247e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.8119298377335769}. Best is trial 4 with value: 0.1528113464128624.[0m
[32m[I 2025-01-01 03:12:41,459][0m Trial 6 finished with value: 0.35149977832575285 and parameters: {'observation_period_num': 241, 'train_rates': 0.7120603193668165, 'learning_rate': 7.043781864397123e-05, 'batch_size': 185, 'step_size': 14, 'gamma': 0.7758466711044119}. Best is trial 4 with value: 0.1528113464128624.[0m
Early stopping at epoch 71
[32m[I 2025-01-01 03:16:53,966][0m Trial 7 finished with value: 0.4613226056098938 and parameters: {'observation_period_num': 192, 'train_rates': 0.9758138962663709, 'learning_rate': 1.0953491155284163e-06, 'batch_size': 77, 'step_size': 1, 'gamma': 0.873152335875188}. Best is trial 4 with value: 0.1528113464128624.[0m
[32m[I 2025-01-01 03:21:42,988][0m Trial 8 finished with value: 0.26693553616092586 and parameters: {'observation_period_num': 50, 'train_rates': 0.6316607230926908, 'learning_rate': 0.00014508639073957467, 'batch_size': 77, 'step_size': 12, 'gamma': 0.9640079001902335}. Best is trial 4 with value: 0.1528113464128624.[0m
[32m[I 2025-01-01 03:28:53,287][0m Trial 9 finished with value: 0.32092385356588665 and parameters: {'observation_period_num': 171, 'train_rates': 0.7047561472214243, 'learning_rate': 0.00029619072291384906, 'batch_size': 52, 'step_size': 14, 'gamma': 0.8730131806893606}. Best is trial 4 with value: 0.1528113464128624.[0m
[32m[I 2025-01-01 03:32:58,390][0m Trial 10 finished with value: 0.0928686747185724 and parameters: {'observation_period_num': 99, 'train_rates': 0.8711945326937335, 'learning_rate': 0.0007862616360163566, 'batch_size': 149, 'step_size': 3, 'gamma': 0.9310847575000422}. Best is trial 10 with value: 0.0928686747185724.[0m
[32m[I 2025-01-01 03:37:01,610][0m Trial 11 finished with value: 1.7930428724784355 and parameters: {'observation_period_num': 103, 'train_rates': 0.8650489407864123, 'learning_rate': 0.0009707696967195965, 'batch_size': 149, 'step_size': 3, 'gamma': 0.9272038767729229}. Best is trial 10 with value: 0.0928686747185724.[0m
[32m[I 2025-01-01 03:41:06,088][0m Trial 12 finished with value: 1.7841565897970488 and parameters: {'observation_period_num': 45, 'train_rates': 0.8641160675903402, 'learning_rate': 0.0009406654914207717, 'batch_size': 152, 'step_size': 4, 'gamma': 0.9239608691220063}. Best is trial 10 with value: 0.0928686747185724.[0m
[32m[I 2025-01-01 03:44:59,037][0m Trial 13 finished with value: 0.12357091750623236 and parameters: {'observation_period_num': 132, 'train_rates': 0.8830203996782591, 'learning_rate': 0.0003454683276540448, 'batch_size': 248, 'step_size': 1, 'gamma': 0.9873786072004128}. Best is trial 10 with value: 0.0928686747185724.[0m
[32m[I 2025-01-01 03:48:45,632][0m Trial 14 finished with value: 0.07981729819331058 and parameters: {'observation_period_num': 78, 'train_rates': 0.8656748537999452, 'learning_rate': 0.000451379767300559, 'batch_size': 234, 'step_size': 1, 'gamma': 0.9311718822282535}. Best is trial 14 with value: 0.07981729819331058.[0m
[32m[I 2025-01-01 03:52:27,109][0m Trial 15 finished with value: 0.07090285794291686 and parameters: {'observation_period_num': 9, 'train_rates': 0.8119563658565878, 'learning_rate': 5.268948662237666e-06, 'batch_size': 204, 'step_size': 3, 'gamma': 0.9243022418203021}. Best is trial 15 with value: 0.07090285794291686.[0m
[32m[I 2025-01-01 03:56:07,524][0m Trial 16 finished with value: 0.06938231113838823 and parameters: {'observation_period_num': 14, 'train_rates': 0.8016648518277896, 'learning_rate': 5.0181902154039825e-06, 'batch_size': 204, 'step_size': 6, 'gamma': 0.9033449346113481}. Best is trial 16 with value: 0.06938231113838823.[0m
[32m[I 2025-01-01 03:59:49,431][0m Trial 17 finished with value: 0.06655439205012521 and parameters: {'observation_period_num': 6, 'train_rates': 0.7892630675336324, 'learning_rate': 4.064109136856726e-06, 'batch_size': 194, 'step_size': 6, 'gamma': 0.905644931281625}. Best is trial 17 with value: 0.06655439205012521.[0m
[32m[I 2025-01-01 04:03:43,113][0m Trial 18 finished with value: 0.06505580326995335 and parameters: {'observation_period_num': 5, 'train_rates': 0.7993573682149085, 'learning_rate': 3.4970461914235217e-06, 'batch_size': 179, 'step_size': 6, 'gamma': 0.895107568507326}. Best is trial 18 with value: 0.06505580326995335.[0m
[32m[I 2025-01-01 04:07:31,894][0m Trial 19 finished with value: 0.2635815553792885 and parameters: {'observation_period_num': 48, 'train_rates': 0.759258691759624, 'learning_rate': 2.9161968932466054e-06, 'batch_size': 169, 'step_size': 8, 'gamma': 0.8488146819571888}. Best is trial 18 with value: 0.06505580326995335.[0m
[32m[I 2025-01-01 04:12:06,564][0m Trial 20 finished with value: 0.05372634783387184 and parameters: {'observation_period_num': 31, 'train_rates': 0.914811846106646, 'learning_rate': 8.235784288703487e-06, 'batch_size': 130, 'step_size': 11, 'gamma': 0.8979923739759286}. Best is trial 20 with value: 0.05372634783387184.[0m
[32m[I 2025-01-01 04:16:55,440][0m Trial 21 finished with value: 0.05007074698122269 and parameters: {'observation_period_num': 27, 'train_rates': 0.9257007731034621, 'learning_rate': 9.51023635202683e-06, 'batch_size': 121, 'step_size': 11, 'gamma': 0.8961406045884175}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:21:32,156][0m Trial 22 finished with value: 0.05585361811993779 and parameters: {'observation_period_num': 30, 'train_rates': 0.9294375557862594, 'learning_rate': 9.617013358114241e-06, 'batch_size': 134, 'step_size': 11, 'gamma': 0.8876822604244984}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:26:14,866][0m Trial 23 finished with value: 0.06433693455024199 and parameters: {'observation_period_num': 33, 'train_rates': 0.9209386938328912, 'learning_rate': 9.710494118427796e-06, 'batch_size': 126, 'step_size': 11, 'gamma': 0.89353304848993}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:30:48,632][0m Trial 24 finished with value: 0.09636604335917308 and parameters: {'observation_period_num': 69, 'train_rates': 0.9220087471892046, 'learning_rate': 4.235465173141189e-05, 'batch_size': 127, 'step_size': 11, 'gamma': 0.8470491973745308}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:35:40,331][0m Trial 25 finished with value: 0.07439989839145478 and parameters: {'observation_period_num': 67, 'train_rates': 0.9187136071340868, 'learning_rate': 8.492946538846915e-06, 'batch_size': 109, 'step_size': 15, 'gamma': 0.8578177925580167}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:41:56,399][0m Trial 26 finished with value: 0.07856613277427611 and parameters: {'observation_period_num': 32, 'train_rates': 0.936514176546081, 'learning_rate': 3.208864671283795e-05, 'batch_size': 75, 'step_size': 10, 'gamma': 0.9526244568450989}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:46:08,111][0m Trial 27 finished with value: 0.06087123359674993 and parameters: {'observation_period_num': 28, 'train_rates': 0.890232409774848, 'learning_rate': 8.21895957506712e-06, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8885431393742171}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:50:16,011][0m Trial 28 finished with value: 0.09796941415681881 and parameters: {'observation_period_num': 82, 'train_rates': 0.8410164581280521, 'learning_rate': 5.274237059973048e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.9470491987870974}. Best is trial 21 with value: 0.05007074698122269.[0m
[32m[I 2025-01-01 04:55:30,768][0m Trial 29 finished with value: 0.11230581845430766 and parameters: {'observation_period_num': 57, 'train_rates': 0.9415716850223719, 'learning_rate': 2.196490263987671e-06, 'batch_size': 95, 'step_size': 9, 'gamma': 0.7558622246102136}. Best is trial 21 with value: 0.05007074698122269.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-01 04:55:30,774][0m A new study created in memory with name: no-name-163c8b3f-e5a3-4a58-81d0-d6ae143360e8[0m
Early stopping at epoch 64
[32m[I 2025-01-01 04:57:54,730][0m Trial 0 finished with value: 0.18905280551041553 and parameters: {'observation_period_num': 59, 'train_rates': 0.8001455870292253, 'learning_rate': 3.385442119984275e-05, 'batch_size': 253, 'step_size': 1, 'gamma': 0.8212876941331284}. Best is trial 0 with value: 0.18905280551041553.[0m
[32m[I 2025-01-01 05:12:21,094][0m Trial 1 finished with value: 0.13331783379764617 and parameters: {'observation_period_num': 113, 'train_rates': 0.8326364610777225, 'learning_rate': 4.1259230478879626e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9078932415635201}. Best is trial 1 with value: 0.13331783379764617.[0m
[32m[I 2025-01-01 05:15:56,213][0m Trial 2 finished with value: 0.1823769190056833 and parameters: {'observation_period_num': 245, 'train_rates': 0.8152785562950056, 'learning_rate': 3.355740875072813e-05, 'batch_size': 164, 'step_size': 8, 'gamma': 0.8430174984528478}. Best is trial 1 with value: 0.13331783379764617.[0m
[32m[I 2025-01-01 05:21:09,863][0m Trial 3 finished with value: 0.43609032803647396 and parameters: {'observation_period_num': 233, 'train_rates': 0.7231063137462513, 'learning_rate': 4.452476076432239e-06, 'batch_size': 71, 'step_size': 5, 'gamma': 0.7786064483885348}. Best is trial 1 with value: 0.13331783379764617.[0m
[32m[I 2025-01-01 05:24:29,559][0m Trial 4 finished with value: 0.4679916589818102 and parameters: {'observation_period_num': 189, 'train_rates': 0.6430745006229001, 'learning_rate': 9.467586120514461e-06, 'batch_size': 161, 'step_size': 3, 'gamma': 0.8173333372437103}. Best is trial 1 with value: 0.13331783379764617.[0m
[32m[I 2025-01-01 05:30:43,164][0m Trial 5 finished with value: 0.19329945397863416 and parameters: {'observation_period_num': 28, 'train_rates': 0.7531829854195617, 'learning_rate': 2.621685530482253e-06, 'batch_size': 65, 'step_size': 9, 'gamma': 0.9414442489896021}. Best is trial 1 with value: 0.13331783379764617.[0m
[32m[I 2025-01-01 05:47:27,636][0m Trial 6 finished with value: 0.06034186580713759 and parameters: {'observation_period_num': 26, 'train_rates': 0.9199791876559906, 'learning_rate': 0.0002791772046010881, 'batch_size': 27, 'step_size': 14, 'gamma': 0.8958214733133654}. Best is trial 6 with value: 0.06034186580713759.[0m
[32m[I 2025-01-01 05:51:22,877][0m Trial 7 finished with value: 0.23145747184753418 and parameters: {'observation_period_num': 187, 'train_rates': 0.9407384840997037, 'learning_rate': 1.2868656600054168e-05, 'batch_size': 164, 'step_size': 2, 'gamma': 0.826894889987603}. Best is trial 6 with value: 0.06034186580713759.[0m
[32m[I 2025-01-01 05:55:13,955][0m Trial 8 finished with value: 0.18673000294280703 and parameters: {'observation_period_num': 204, 'train_rates': 0.8535127745693003, 'learning_rate': 1.7078210711182107e-06, 'batch_size': 150, 'step_size': 8, 'gamma': 0.9695607837206471}. Best is trial 6 with value: 0.06034186580713759.[0m
[32m[I 2025-01-01 05:59:22,625][0m Trial 9 finished with value: 0.180073082447052 and parameters: {'observation_period_num': 122, 'train_rates': 0.9686243597128307, 'learning_rate': 1.169806379797431e-05, 'batch_size': 193, 'step_size': 4, 'gamma': 0.8049023171247031}. Best is trial 6 with value: 0.06034186580713759.[0m
[32m[I 2025-01-01 06:04:42,508][0m Trial 10 finished with value: 0.05081143599247264 and parameters: {'observation_period_num': 11, 'train_rates': 0.891060886420665, 'learning_rate': 0.0004848503078260573, 'batch_size': 93, 'step_size': 15, 'gamma': 0.8831898865333954}. Best is trial 10 with value: 0.05081143599247264.[0m
[32m[I 2025-01-01 06:10:02,906][0m Trial 11 finished with value: 0.0520583622210228 and parameters: {'observation_period_num': 18, 'train_rates': 0.9038816978763129, 'learning_rate': 0.0005148877446808312, 'batch_size': 94, 'step_size': 15, 'gamma': 0.8890995843799172}. Best is trial 10 with value: 0.05081143599247264.[0m
[32m[I 2025-01-01 06:14:57,727][0m Trial 12 finished with value: 1.830333762862734 and parameters: {'observation_period_num': 79, 'train_rates': 0.8879346972046093, 'learning_rate': 0.0008469193284052769, 'batch_size': 100, 'step_size': 15, 'gamma': 0.8737243985372652}. Best is trial 10 with value: 0.05081143599247264.[0m
[32m[I 2025-01-01 06:20:13,490][0m Trial 13 finished with value: 0.03924427926540375 and parameters: {'observation_period_num': 9, 'train_rates': 0.9863904184853886, 'learning_rate': 0.0001745306708282874, 'batch_size': 107, 'step_size': 12, 'gamma': 0.922794497151182}. Best is trial 13 with value: 0.03924427926540375.[0m
[32m[I 2025-01-01 06:25:15,968][0m Trial 14 finished with value: 0.10759842395782471 and parameters: {'observation_period_num': 68, 'train_rates': 0.9711770166378856, 'learning_rate': 0.00011852719583538458, 'batch_size': 116, 'step_size': 12, 'gamma': 0.936094909555177}. Best is trial 13 with value: 0.03924427926540375.[0m
[32m[I 2025-01-01 06:33:54,912][0m Trial 15 finished with value: 0.03319040685892105 and parameters: {'observation_period_num': 12, 'train_rates': 0.9885797675229907, 'learning_rate': 0.0001303452853256295, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9890051303776992}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 06:42:14,914][0m Trial 16 finished with value: 0.11335843801498413 and parameters: {'observation_period_num': 153, 'train_rates': 0.9833546104136031, 'learning_rate': 0.00011828529862356913, 'batch_size': 54, 'step_size': 11, 'gamma': 0.9852879658564018}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 06:45:47,081][0m Trial 17 finished with value: 0.29157712662997454 and parameters: {'observation_period_num': 54, 'train_rates': 0.6035785143164989, 'learning_rate': 0.00012100109041246101, 'batch_size': 126, 'step_size': 12, 'gamma': 0.9462200835052003}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 06:49:12,459][0m Trial 18 finished with value: 0.3199879122374035 and parameters: {'observation_period_num': 90, 'train_rates': 0.7409085473168728, 'learning_rate': 8.623840605788497e-05, 'batch_size': 204, 'step_size': 10, 'gamma': 0.9879227151138251}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 06:59:57,974][0m Trial 19 finished with value: 0.08338171941905073 and parameters: {'observation_period_num': 43, 'train_rates': 0.9374731608161613, 'learning_rate': 0.0002145051137362094, 'batch_size': 43, 'step_size': 6, 'gamma': 0.9139340708400339}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:05:37,643][0m Trial 20 finished with value: 0.10681657515749146 and parameters: {'observation_period_num': 97, 'train_rates': 0.8620699668055337, 'learning_rate': 5.961878635655714e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.9662831728312706}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:10:52,968][0m Trial 21 finished with value: 0.03831632807850838 and parameters: {'observation_period_num': 9, 'train_rates': 0.986591411621717, 'learning_rate': 0.00029520332722397715, 'batch_size': 115, 'step_size': 13, 'gamma': 0.8505371784587199}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:15:59,454][0m Trial 22 finished with value: 0.0938553735613823 and parameters: {'observation_period_num': 8, 'train_rates': 0.9862764019429879, 'learning_rate': 0.0002536145150090478, 'batch_size': 119, 'step_size': 13, 'gamma': 0.8484962467236195}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:20:39,066][0m Trial 23 finished with value: 0.0551861272007227 and parameters: {'observation_period_num': 42, 'train_rates': 0.9486120482425637, 'learning_rate': 0.0001952896345196714, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9229011074530288}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:26:30,267][0m Trial 24 finished with value: 1.9933511940938122 and parameters: {'observation_period_num': 8, 'train_rates': 0.9462387081200307, 'learning_rate': 0.000921472045613374, 'batch_size': 84, 'step_size': 13, 'gamma': 0.8557918756718129}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:31:38,902][0m Trial 25 finished with value: 0.043209634721279144 and parameters: {'observation_period_num': 34, 'train_rates': 0.9891063147323425, 'learning_rate': 0.000415829284562938, 'batch_size': 110, 'step_size': 10, 'gamma': 0.7727956696999756}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 07:58:22,092][0m Trial 26 finished with value: 0.0981004432386715 and parameters: {'observation_period_num': 145, 'train_rates': 0.9124059958387065, 'learning_rate': 6.84897477048655e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9602499306964329}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 08:06:40,463][0m Trial 27 finished with value: 0.09030788542590938 and parameters: {'observation_period_num': 51, 'train_rates': 0.8681030649194543, 'learning_rate': 2.0905189154218195e-05, 'batch_size': 53, 'step_size': 7, 'gamma': 0.8621302711738171}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 08:10:21,014][0m Trial 28 finished with value: 0.26132242421868357 and parameters: {'observation_period_num': 79, 'train_rates': 0.685088026387049, 'learning_rate': 0.0001516965466430094, 'batch_size': 139, 'step_size': 14, 'gamma': 0.7540854113508867}. Best is trial 15 with value: 0.03319040685892105.[0m
[32m[I 2025-01-01 08:13:54,553][0m Trial 29 finished with value: 0.24629645882556156 and parameters: {'observation_period_num': 63, 'train_rates': 0.777329253759078, 'learning_rate': 0.00035315455574349246, 'batch_size': 228, 'step_size': 11, 'gamma': 0.9302512008733211}. Best is trial 15 with value: 0.03319040685892105.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-01 08:13:54,558][0m A new study created in memory with name: no-name-90fb3afd-c178-496e-9947-6bfc5ffb7f4c[0m
Early stopping at epoch 97
[32m[I 2025-01-01 08:20:08,883][0m Trial 0 finished with value: 0.39789206269793037 and parameters: {'observation_period_num': 246, 'train_rates': 0.7664234676789726, 'learning_rate': 6.8416468366428e-05, 'batch_size': 60, 'step_size': 1, 'gamma': 0.8346898036366198}. Best is trial 0 with value: 0.39789206269793037.[0m
[32m[I 2025-01-01 08:25:02,596][0m Trial 1 finished with value: 0.11938762664794922 and parameters: {'observation_period_num': 181, 'train_rates': 0.9643684452287031, 'learning_rate': 6.568407871651761e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8965363136172615}. Best is trial 1 with value: 0.11938762664794922.[0m
[32m[I 2025-01-01 08:29:17,149][0m Trial 2 finished with value: 0.17462711395888492 and parameters: {'observation_period_num': 235, 'train_rates': 0.7868076098035052, 'learning_rate': 1.0586087844911961e-05, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8917026048820305}. Best is trial 1 with value: 0.11938762664794922.[0m
[32m[I 2025-01-01 08:32:29,131][0m Trial 3 finished with value: 0.4980966368142296 and parameters: {'observation_period_num': 236, 'train_rates': 0.6250748807542915, 'learning_rate': 0.0001808204910806152, 'batch_size': 168, 'step_size': 1, 'gamma': 0.895477708252971}. Best is trial 1 with value: 0.11938762664794922.[0m
[32m[I 2025-01-01 08:46:50,475][0m Trial 4 finished with value: 0.15230966451549038 and parameters: {'observation_period_num': 119, 'train_rates': 0.7949490997346331, 'learning_rate': 3.5564412672529375e-05, 'batch_size': 28, 'step_size': 3, 'gamma': 0.8195406654109931}. Best is trial 1 with value: 0.11938762664794922.[0m
[32m[I 2025-01-01 08:50:49,237][0m Trial 5 finished with value: 0.20451957407132001 and parameters: {'observation_period_num': 51, 'train_rates': 0.706758684403353, 'learning_rate': 0.0002980976613292986, 'batch_size': 128, 'step_size': 15, 'gamma': 0.9260751065077939}. Best is trial 1 with value: 0.11938762664794922.[0m
[32m[I 2025-01-01 08:54:49,690][0m Trial 6 finished with value: 0.08850502222776413 and parameters: {'observation_period_num': 60, 'train_rates': 0.9308736792303385, 'learning_rate': 0.0004391874308469184, 'batch_size': 232, 'step_size': 7, 'gamma': 0.9111111113381651}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:00:44,373][0m Trial 7 finished with value: 0.17486588313689388 and parameters: {'observation_period_num': 228, 'train_rates': 0.808964074132821, 'learning_rate': 6.858046248806017e-06, 'batch_size': 68, 'step_size': 2, 'gamma': 0.9845910875732856}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:12:08,721][0m Trial 8 finished with value: 1.9654365901289315 and parameters: {'observation_period_num': 191, 'train_rates': 0.9267879555686119, 'learning_rate': 0.0009900621661859212, 'batch_size': 38, 'step_size': 8, 'gamma': 0.8480836683278783}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:15:43,661][0m Trial 9 finished with value: 0.20347310415555056 and parameters: {'observation_period_num': 24, 'train_rates': 0.77469563473822, 'learning_rate': 6.993621787042284e-05, 'batch_size': 229, 'step_size': 4, 'gamma': 0.7896478484186297}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:19:39,608][0m Trial 10 finished with value: 0.12826868555388854 and parameters: {'observation_period_num': 86, 'train_rates': 0.8890939487947412, 'learning_rate': 2.9684168825862703e-06, 'batch_size': 252, 'step_size': 7, 'gamma': 0.961065579247249}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:23:45,865][0m Trial 11 finished with value: 1.9608725309371948 and parameters: {'observation_period_num': 149, 'train_rates': 0.9838623820032633, 'learning_rate': 0.000948876991731073, 'batch_size': 187, 'step_size': 12, 'gamma': 0.9309127448511489}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:27:49,460][0m Trial 12 finished with value: 0.11483217775821686 and parameters: {'observation_period_num': 169, 'train_rates': 0.9767256725682275, 'learning_rate': 0.00014134853192813702, 'batch_size': 183, 'step_size': 6, 'gamma': 0.8747512760710303}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:31:38,959][0m Trial 13 finished with value: 0.09116652154692556 and parameters: {'observation_period_num': 91, 'train_rates': 0.8824698740575143, 'learning_rate': 0.0002295690892626151, 'batch_size': 195, 'step_size': 5, 'gamma': 0.7629325929851908}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:35:23,093][0m Trial 14 finished with value: 0.0904601640773542 and parameters: {'observation_period_num': 92, 'train_rates': 0.8732889720800506, 'learning_rate': 0.0003986965396800353, 'batch_size': 220, 'step_size': 5, 'gamma': 0.7584895499600901}. Best is trial 6 with value: 0.08850502222776413.[0m
[32m[I 2025-01-01 09:39:20,851][0m Trial 15 finished with value: 0.07447372559601298 and parameters: {'observation_period_num': 58, 'train_rates': 0.856380167446757, 'learning_rate': 0.00044717434409619267, 'batch_size': 225, 'step_size': 9, 'gamma': 0.7538104383059109}. Best is trial 15 with value: 0.07447372559601298.[0m
[32m[I 2025-01-01 09:43:15,293][0m Trial 16 finished with value: 0.03663665539377641 and parameters: {'observation_period_num': 7, 'train_rates': 0.8447493378649096, 'learning_rate': 0.000497139546817353, 'batch_size': 247, 'step_size': 9, 'gamma': 0.7994409629029275}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 09:47:22,566][0m Trial 17 finished with value: 0.04879407026239603 and parameters: {'observation_period_num': 18, 'train_rates': 0.8268346717488889, 'learning_rate': 1.3669532736583125e-05, 'batch_size': 149, 'step_size': 10, 'gamma': 0.796691154805959}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 09:50:59,375][0m Trial 18 finished with value: 0.20322182292802862 and parameters: {'observation_period_num': 9, 'train_rates': 0.7071945012161367, 'learning_rate': 1.0512319451697496e-06, 'batch_size': 151, 'step_size': 13, 'gamma': 0.7975764914896503}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 09:55:23,955][0m Trial 19 finished with value: 0.04720100331535217 and parameters: {'observation_period_num': 28, 'train_rates': 0.8397211008727216, 'learning_rate': 1.3738897869242573e-05, 'batch_size': 123, 'step_size': 12, 'gamma': 0.7884861920068617}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 09:59:36,677][0m Trial 20 finished with value: 0.18578134746598102 and parameters: {'observation_period_num': 41, 'train_rates': 0.7235211864479529, 'learning_rate': 2.6270837887758526e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8588939276387011}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:03:52,432][0m Trial 21 finished with value: 0.043416166281604385 and parameters: {'observation_period_num': 5, 'train_rates': 0.8312239025387512, 'learning_rate': 1.271001350288076e-05, 'batch_size': 142, 'step_size': 11, 'gamma': 0.7935126330563993}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:09:23,218][0m Trial 22 finished with value: 0.04485499275902824 and parameters: {'observation_period_num': 5, 'train_rates': 0.838121309159936, 'learning_rate': 3.5311832635844994e-06, 'batch_size': 81, 'step_size': 13, 'gamma': 0.8158681268644699}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:14:39,050][0m Trial 23 finished with value: 0.1765109369309591 and parameters: {'observation_period_num': 6, 'train_rates': 0.752264279470851, 'learning_rate': 3.5478797458382073e-06, 'batch_size': 81, 'step_size': 14, 'gamma': 0.8211899278967094}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:19:51,126][0m Trial 24 finished with value: 0.09151425727337371 and parameters: {'observation_period_num': 70, 'train_rates': 0.9093712948501, 'learning_rate': 1.9062134656721125e-06, 'batch_size': 91, 'step_size': 11, 'gamma': 0.8130655766216688}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:28:35,322][0m Trial 25 finished with value: 0.05726751983165741 and parameters: {'observation_period_num': 33, 'train_rates': 0.8323545402466593, 'learning_rate': 6.098725890899711e-06, 'batch_size': 49, 'step_size': 15, 'gamma': 0.7774633032277319}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:32:32,966][0m Trial 26 finished with value: 0.0740193555324848 and parameters: {'observation_period_num': 40, 'train_rates': 0.8546323926579434, 'learning_rate': 2.5413697994380385e-05, 'batch_size': 207, 'step_size': 13, 'gamma': 0.8393739351125998}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:35:50,660][0m Trial 27 finished with value: 0.3882454150709613 and parameters: {'observation_period_num': 114, 'train_rates': 0.6428124424390933, 'learning_rate': 3.927381407609034e-06, 'batch_size': 156, 'step_size': 9, 'gamma': 0.7798120057305963}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:39:35,234][0m Trial 28 finished with value: 0.0722116389177822 and parameters: {'observation_period_num': 5, 'train_rates': 0.8162743963086397, 'learning_rate': 1.739024660151629e-06, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8071131297287752}. Best is trial 16 with value: 0.03663665539377641.[0m
[32m[I 2025-01-01 10:45:18,713][0m Trial 29 finished with value: 0.18511897407290412 and parameters: {'observation_period_num': 24, 'train_rates': 0.74815981331195, 'learning_rate': 7.638369367377042e-06, 'batch_size': 71, 'step_size': 14, 'gamma': 0.8335287192592303}. Best is trial 16 with value: 0.03663665539377641.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-01 10:45:18,719][0m A new study created in memory with name: no-name-dd157bd8-8826-42bc-8018-69172afcaa8b[0m
[32m[I 2025-01-01 10:51:19,262][0m Trial 0 finished with value: 0.3190646031467726 and parameters: {'observation_period_num': 189, 'train_rates': 0.7580533268405694, 'learning_rate': 5.1863000130614176e-05, 'batch_size': 65, 'step_size': 9, 'gamma': 0.8997770323982561}. Best is trial 0 with value: 0.3190646031467726.[0m
[32m[I 2025-01-01 11:01:55,516][0m Trial 1 finished with value: 0.3626040327004515 and parameters: {'observation_period_num': 220, 'train_rates': 0.7337242768445996, 'learning_rate': 1.173584245119114e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8392613078533817}. Best is trial 0 with value: 0.3190646031467726.[0m
[32m[I 2025-01-01 11:06:36,082][0m Trial 2 finished with value: 0.11767695844173431 and parameters: {'observation_period_num': 118, 'train_rates': 0.9618336461397218, 'learning_rate': 6.840387767968303e-05, 'batch_size': 124, 'step_size': 7, 'gamma': 0.9283249214286957}. Best is trial 2 with value: 0.11767695844173431.[0m
[32m[I 2025-01-01 11:10:33,003][0m Trial 3 finished with value: 0.1138473269072446 and parameters: {'observation_period_num': 114, 'train_rates': 0.9034754274915079, 'learning_rate': 7.535981342127453e-05, 'batch_size': 188, 'step_size': 2, 'gamma': 0.7982733408674971}. Best is trial 3 with value: 0.1138473269072446.[0m
[32m[I 2025-01-01 11:15:11,726][0m Trial 4 finished with value: 0.055103589568373766 and parameters: {'observation_period_num': 6, 'train_rates': 0.8544325640793514, 'learning_rate': 0.0005537853947692869, 'batch_size': 117, 'step_size': 4, 'gamma': 0.9707724163228691}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:25:42,085][0m Trial 5 finished with value: 0.28331350363217866 and parameters: {'observation_period_num': 135, 'train_rates': 0.7371717307197979, 'learning_rate': 1.2187080197790612e-05, 'batch_size': 37, 'step_size': 7, 'gamma': 0.945631336420713}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:29:03,853][0m Trial 6 finished with value: 0.3058894141600822 and parameters: {'observation_period_num': 169, 'train_rates': 0.7171367486815134, 'learning_rate': 5.26043781043764e-05, 'batch_size': 188, 'step_size': 9, 'gamma': 0.7672181997184716}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:33:01,534][0m Trial 7 finished with value: 0.2993743285888762 and parameters: {'observation_period_num': 124, 'train_rates': 0.9103606362703618, 'learning_rate': 2.255075328963792e-06, 'batch_size': 226, 'step_size': 3, 'gamma': 0.8214611579847749}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:39:22,071][0m Trial 8 finished with value: 0.28808154943790143 and parameters: {'observation_period_num': 121, 'train_rates': 0.7689417459531087, 'learning_rate': 9.809489043234648e-05, 'batch_size': 63, 'step_size': 10, 'gamma': 0.824400143168277}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:43:07,456][0m Trial 9 finished with value: 0.1361475116753763 and parameters: {'observation_period_num': 249, 'train_rates': 0.9048537605519695, 'learning_rate': 7.751744711815603e-05, 'batch_size': 200, 'step_size': 1, 'gamma': 0.9510863325603172}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:46:48,967][0m Trial 10 finished with value: 1.6817078766688494 and parameters: {'observation_period_num': 6, 'train_rates': 0.6146540559518541, 'learning_rate': 0.000946652149322798, 'batch_size': 124, 'step_size': 13, 'gamma': 0.989719430983537}. Best is trial 4 with value: 0.055103589568373766.[0m
Early stopping at epoch 47
[32m[I 2025-01-01 11:48:44,578][0m Trial 11 finished with value: 1.6476206124630743 and parameters: {'observation_period_num': 14, 'train_rates': 0.8484792212245921, 'learning_rate': 0.0007526822662542553, 'batch_size': 165, 'step_size': 1, 'gamma': 0.7513883728195091}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:52:26,518][0m Trial 12 finished with value: 0.08156754672527314 and parameters: {'observation_period_num': 72, 'train_rates': 0.850927906014285, 'learning_rate': 0.00033183578299308524, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8697321473929558}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 11:56:06,484][0m Trial 13 finished with value: 0.07023282787033322 and parameters: {'observation_period_num': 61, 'train_rates': 0.824784538365226, 'learning_rate': 0.0003173693859642211, 'batch_size': 256, 'step_size': 4, 'gamma': 0.8754055445870262}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 12:00:47,902][0m Trial 14 finished with value: 0.08329058274515529 and parameters: {'observation_period_num': 50, 'train_rates': 0.8157311642435696, 'learning_rate': 0.00025093312577772136, 'batch_size': 100, 'step_size': 6, 'gamma': 0.9879706774247198}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 12:04:49,744][0m Trial 15 finished with value: 0.09169337858406201 and parameters: {'observation_period_num': 50, 'train_rates': 0.8147111105861282, 'learning_rate': 0.00028021699508514727, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8881691903690008}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 12:09:17,603][0m Trial 16 finished with value: 0.20865328185260296 and parameters: {'observation_period_num': 65, 'train_rates': 0.688497079725185, 'learning_rate': 0.00037000070080541435, 'batch_size': 94, 'step_size': 4, 'gamma': 0.8611005013162654}. Best is trial 4 with value: 0.055103589568373766.[0m
[32m[I 2025-01-01 12:13:28,001][0m Trial 17 finished with value: 0.045078814029693604 and parameters: {'observation_period_num': 36, 'train_rates': 0.9766756424861124, 'learning_rate': 0.00014905410876565824, 'batch_size': 248, 'step_size': 5, 'gamma': 0.907702529108343}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:17:55,318][0m Trial 18 finished with value: 0.08540692180395126 and parameters: {'observation_period_num': 29, 'train_rates': 0.9631733773481862, 'learning_rate': 2.1376737570555304e-05, 'batch_size': 154, 'step_size': 11, 'gamma': 0.9204232649819959}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:23:11,543][0m Trial 19 finished with value: 0.1326470673084259 and parameters: {'observation_period_num': 84, 'train_rates': 0.9893197851534923, 'learning_rate': 0.00015512117743726487, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9645492033758278}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:26:58,665][0m Trial 20 finished with value: 0.17484242614001444 and parameters: {'observation_period_num': 30, 'train_rates': 0.8754525337478498, 'learning_rate': 1.1439543650653697e-06, 'batch_size': 223, 'step_size': 3, 'gamma': 0.9093019097410621}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:30:39,572][0m Trial 21 finished with value: 0.06647213761295591 and parameters: {'observation_period_num': 34, 'train_rates': 0.7989126599417393, 'learning_rate': 0.0006032717797044625, 'batch_size': 252, 'step_size': 5, 'gamma': 0.881153857489982}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:34:11,386][0m Trial 22 finished with value: 0.09784938077926636 and parameters: {'observation_period_num': 90, 'train_rates': 0.7818837524836821, 'learning_rate': 0.000659202193098398, 'batch_size': 236, 'step_size': 6, 'gamma': 0.9342490239354139}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:37:27,936][0m Trial 23 finished with value: 0.16299204678746199 and parameters: {'observation_period_num': 32, 'train_rates': 0.6730486709757592, 'learning_rate': 0.00015691495671982673, 'batch_size': 206, 'step_size': 5, 'gamma': 0.9663722687795968}. Best is trial 17 with value: 0.045078814029693604.[0m
[32m[I 2025-01-01 12:41:48,710][0m Trial 24 finished with value: 0.03495977446436882 and parameters: {'observation_period_num': 12, 'train_rates': 0.9444119624332082, 'learning_rate': 0.0005036204775221417, 'batch_size': 172, 'step_size': 8, 'gamma': 0.8528087979792369}. Best is trial 24 with value: 0.03495977446436882.[0m
[32m[I 2025-01-01 12:46:10,784][0m Trial 25 finished with value: 0.0344674289226532 and parameters: {'observation_period_num': 6, 'train_rates': 0.9457737179087906, 'learning_rate': 0.0001580962846713013, 'batch_size': 162, 'step_size': 8, 'gamma': 0.8475047397820581}. Best is trial 25 with value: 0.0344674289226532.[0m
[32m[I 2025-01-01 12:50:31,424][0m Trial 26 finished with value: 0.03352983214090325 and parameters: {'observation_period_num': 18, 'train_rates': 0.9412667183421158, 'learning_rate': 0.00014991764928175684, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8518307755459404}. Best is trial 26 with value: 0.03352983214090325.[0m
[32m[I 2025-01-01 12:54:43,410][0m Trial 27 finished with value: 0.039811352104112345 and parameters: {'observation_period_num': 17, 'train_rates': 0.9381128242710375, 'learning_rate': 3.0132303572581842e-05, 'batch_size': 167, 'step_size': 12, 'gamma': 0.8465401734472365}. Best is trial 26 with value: 0.03352983214090325.[0m
[32m[I 2025-01-01 12:58:53,095][0m Trial 28 finished with value: 0.07790517864635636 and parameters: {'observation_period_num': 90, 'train_rates': 0.9370737326192451, 'learning_rate': 0.00019060657858717907, 'batch_size': 169, 'step_size': 8, 'gamma': 0.7997215643908712}. Best is trial 26 with value: 0.03352983214090325.[0m
[32m[I 2025-01-01 13:02:55,907][0m Trial 29 finished with value: 0.12817952303744073 and parameters: {'observation_period_num': 161, 'train_rates': 0.9282790863186471, 'learning_rate': 2.9576403255282806e-05, 'batch_size': 182, 'step_size': 9, 'gamma': 0.8552351810673124}. Best is trial 26 with value: 0.03352983214090325.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-01 13:02:55,913][0m A new study created in memory with name: no-name-97d9d949-8aa2-4645-84cd-fca74a33c788[0m
[32m[I 2025-01-01 13:13:20,405][0m Trial 0 finished with value: 0.18281013416846778 and parameters: {'observation_period_num': 51, 'train_rates': 0.63062627924476, 'learning_rate': 0.00013626919304557375, 'batch_size': 35, 'step_size': 8, 'gamma': 0.8647371807809541}. Best is trial 0 with value: 0.18281013416846778.[0m
[32m[I 2025-01-01 13:18:53,171][0m Trial 1 finished with value: 0.22949320856660857 and parameters: {'observation_period_num': 80, 'train_rates': 0.6311100087468837, 'learning_rate': 5.325482693291322e-06, 'batch_size': 65, 'step_size': 2, 'gamma': 0.9881307014637318}. Best is trial 0 with value: 0.18281013416846778.[0m
[32m[I 2025-01-01 13:26:28,811][0m Trial 2 finished with value: 0.20572547603067745 and parameters: {'observation_period_num': 225, 'train_rates': 0.9053570983754777, 'learning_rate': 0.000380799955063386, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9446142200273289}. Best is trial 0 with value: 0.18281013416846778.[0m
[32m[I 2025-01-01 13:30:01,887][0m Trial 3 finished with value: 0.12694910932688197 and parameters: {'observation_period_num': 210, 'train_rates': 0.8285571387593537, 'learning_rate': 0.0007737713636579847, 'batch_size': 233, 'step_size': 14, 'gamma': 0.7994266404806248}. Best is trial 3 with value: 0.12694910932688197.[0m
[32m[I 2025-01-01 13:35:20,084][0m Trial 4 finished with value: 0.09413359254070623 and parameters: {'observation_period_num': 126, 'train_rates': 0.9104393411081254, 'learning_rate': 5.534558079830376e-05, 'batch_size': 87, 'step_size': 1, 'gamma': 0.9185415633738322}. Best is trial 4 with value: 0.09413359254070623.[0m
[32m[I 2025-01-01 13:38:38,724][0m Trial 5 finished with value: 0.1880286542134672 and parameters: {'observation_period_num': 61, 'train_rates': 0.6828916576831514, 'learning_rate': 0.000744638898408072, 'batch_size': 239, 'step_size': 9, 'gamma': 0.8301500591039745}. Best is trial 4 with value: 0.09413359254070623.[0m
[32m[I 2025-01-01 13:42:32,106][0m Trial 6 finished with value: 0.30175169450167105 and parameters: {'observation_period_num': 234, 'train_rates': 0.8929157545587322, 'learning_rate': 1.5727959097097663e-06, 'batch_size': 171, 'step_size': 7, 'gamma': 0.7801182690739109}. Best is trial 4 with value: 0.09413359254070623.[0m
[32m[I 2025-01-01 13:45:56,169][0m Trial 7 finished with value: 0.4324874155830454 and parameters: {'observation_period_num': 209, 'train_rates': 0.7542456364229414, 'learning_rate': 3.342163866976489e-05, 'batch_size': 203, 'step_size': 3, 'gamma': 0.8479981908446989}. Best is trial 4 with value: 0.09413359254070623.[0m
[32m[I 2025-01-01 13:51:22,279][0m Trial 8 finished with value: 0.0577226682931562 and parameters: {'observation_period_num': 11, 'train_rates': 0.9466275692495452, 'learning_rate': 1.8380080171457638e-05, 'batch_size': 94, 'step_size': 2, 'gamma': 0.8682800990910013}. Best is trial 8 with value: 0.0577226682931562.[0m
[32m[I 2025-01-01 13:54:21,931][0m Trial 9 finished with value: 0.5431772370020104 and parameters: {'observation_period_num': 224, 'train_rates': 0.6077191315633697, 'learning_rate': 4.983639351471714e-06, 'batch_size': 233, 'step_size': 11, 'gamma': 0.8881514755605806}. Best is trial 8 with value: 0.0577226682931562.[0m
[32m[I 2025-01-01 13:59:34,989][0m Trial 10 finished with value: 0.07258883863687515 and parameters: {'observation_period_num': 22, 'train_rates': 0.9849380281225124, 'learning_rate': 8.380248050654608e-06, 'batch_size': 113, 'step_size': 5, 'gamma': 0.7554567103590742}. Best is trial 8 with value: 0.0577226682931562.[0m
[32m[I 2025-01-01 14:04:36,171][0m Trial 11 finished with value: 0.05755780637264252 and parameters: {'observation_period_num': 15, 'train_rates': 0.9812620772159463, 'learning_rate': 1.087564273831643e-05, 'batch_size': 122, 'step_size': 5, 'gamma': 0.7606195151922548}. Best is trial 11 with value: 0.05755780637264252.[0m
[32m[I 2025-01-01 14:09:24,920][0m Trial 12 finished with value: 0.056165821850299835 and parameters: {'observation_period_num': 7, 'train_rates': 0.977039901584086, 'learning_rate': 1.36545689100142e-05, 'batch_size': 136, 'step_size': 5, 'gamma': 0.8181004515529416}. Best is trial 12 with value: 0.056165821850299835.[0m
[32m[I 2025-01-01 14:13:51,228][0m Trial 13 finished with value: 0.19282321631908417 and parameters: {'observation_period_num': 111, 'train_rates': 0.9896303544314247, 'learning_rate': 1.3507155532039277e-05, 'batch_size': 163, 'step_size': 5, 'gamma': 0.8036308926203415}. Best is trial 12 with value: 0.056165821850299835.[0m
[32m[I 2025-01-01 14:17:47,006][0m Trial 14 finished with value: 0.3208877550487363 and parameters: {'observation_period_num': 172, 'train_rates': 0.8236198773084957, 'learning_rate': 1.544480659248557e-06, 'batch_size': 138, 'step_size': 5, 'gamma': 0.7549336080254256}. Best is trial 12 with value: 0.056165821850299835.[0m
[32m[I 2025-01-01 14:22:15,668][0m Trial 15 finished with value: 0.05490816929809432 and parameters: {'observation_period_num': 36, 'train_rates': 0.863416149195303, 'learning_rate': 6.049441774485375e-05, 'batch_size': 130, 'step_size': 6, 'gamma': 0.8210143446037358}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:25:56,317][0m Trial 16 finished with value: 0.25717905297407656 and parameters: {'observation_period_num': 97, 'train_rates': 0.7660697267682921, 'learning_rate': 8.694291606117317e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8225057163899157}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:29:40,912][0m Trial 17 finished with value: 0.17368708995175872 and parameters: {'observation_period_num': 153, 'train_rates': 0.8666993675504018, 'learning_rate': 0.00016973902118133297, 'batch_size': 191, 'step_size': 10, 'gamma': 0.89630738066783}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:33:25,263][0m Trial 18 finished with value: 0.20005764781110702 and parameters: {'observation_period_num': 44, 'train_rates': 0.7202866462805823, 'learning_rate': 3.6088797562862824e-05, 'batch_size': 142, 'step_size': 4, 'gamma': 0.8169180466972662}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:38:08,224][0m Trial 19 finished with value: 0.11345663751874652 and parameters: {'observation_period_num': 74, 'train_rates': 0.8542587936626684, 'learning_rate': 2.9113645181464478e-06, 'batch_size': 104, 'step_size': 7, 'gamma': 0.8408692982925298}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:42:13,155][0m Trial 20 finished with value: 0.07076488435268402 and parameters: {'observation_period_num': 37, 'train_rates': 0.9386322630990119, 'learning_rate': 2.185555699759468e-05, 'batch_size': 206, 'step_size': 13, 'gamma': 0.7836425736923521}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:47:13,382][0m Trial 21 finished with value: 0.05755316284371585 and parameters: {'observation_period_num': 5, 'train_rates': 0.944647359974189, 'learning_rate': 9.972352279028755e-06, 'batch_size': 120, 'step_size': 6, 'gamma': 0.7785649126225792}. Best is trial 15 with value: 0.05490816929809432.[0m
[32m[I 2025-01-01 14:52:08,436][0m Trial 22 finished with value: 0.04234741053475839 and parameters: {'observation_period_num': 6, 'train_rates': 0.9444331851010117, 'learning_rate': 5.577912666428857e-05, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7865136050415347}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 14:58:37,596][0m Trial 23 finished with value: 0.04980782600618451 and parameters: {'observation_period_num': 29, 'train_rates': 0.9377343140291826, 'learning_rate': 6.884786064903658e-05, 'batch_size': 73, 'step_size': 8, 'gamma': 0.8013130755248437}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 15:04:33,577][0m Trial 24 finished with value: 0.06794631432579912 and parameters: {'observation_period_num': 33, 'train_rates': 0.8740289427588399, 'learning_rate': 0.00023800708367677424, 'batch_size': 76, 'step_size': 9, 'gamma': 0.7967556711155311}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 15:16:42,148][0m Trial 25 finished with value: 0.0946020665422581 and parameters: {'observation_period_num': 67, 'train_rates': 0.9279140852797656, 'learning_rate': 6.839645139534447e-05, 'batch_size': 37, 'step_size': 8, 'gamma': 0.8489936782542167}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 15:24:38,083][0m Trial 26 finished with value: 0.1032360564012312 and parameters: {'observation_period_num': 91, 'train_rates': 0.8148000808601145, 'learning_rate': 0.00010274157262026399, 'batch_size': 52, 'step_size': 10, 'gamma': 0.7729090463513524}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 15:48:20,658][0m Trial 27 finished with value: 0.07982287861908119 and parameters: {'observation_period_num': 31, 'train_rates': 0.8526091123306051, 'learning_rate': 5.07783986087598e-05, 'batch_size': 18, 'step_size': 6, 'gamma': 0.8037976048542721}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 15:53:43,060][0m Trial 28 finished with value: 0.08681558290353188 and parameters: {'observation_period_num': 53, 'train_rates': 0.8881150501497762, 'learning_rate': 0.0003494093494102033, 'batch_size': 86, 'step_size': 15, 'gamma': 0.8345703597783581}. Best is trial 22 with value: 0.04234741053475839.[0m
[32m[I 2025-01-01 15:58:48,349][0m Trial 29 finished with value: 0.08181007556391484 and parameters: {'observation_period_num': 48, 'train_rates': 0.9547011703690144, 'learning_rate': 0.00021767351610597606, 'batch_size': 103, 'step_size': 8, 'gamma': 0.8548860022253715}. Best is trial 22 with value: 0.04234741053475839.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 78, 'train_rates': 0.8408434302835129, 'learning_rate': 0.0004242824823721914, 'batch_size': 82, 'step_size': 10, 'gamma': 0.8159241014899361}
Epoch 1/300, trend Loss: 0.5408 | 0.2045
Epoch 2/300, trend Loss: 0.2293 | 0.1417
Epoch 3/300, trend Loss: 0.1646 | 0.1613
Epoch 4/300, trend Loss: 0.1542 | 0.1895
Epoch 5/300, trend Loss: 0.1371 | 0.0897
Epoch 6/300, trend Loss: 0.1284 | 0.1240
Epoch 7/300, trend Loss: 0.1112 | 0.1020
Epoch 8/300, trend Loss: 0.1080 | 0.1003
Epoch 9/300, trend Loss: 0.1043 | 0.0967
Epoch 10/300, trend Loss: 0.1024 | 0.0913
Epoch 11/300, trend Loss: 0.1148 | 0.0765
Epoch 12/300, trend Loss: 0.0927 | 0.0670
Epoch 13/300, trend Loss: 0.0887 | 0.0760
Epoch 14/300, trend Loss: 0.0903 | 0.0661
Epoch 15/300, trend Loss: 0.0963 | 0.0847
Epoch 16/300, trend Loss: 0.1065 | 0.0861
Epoch 17/300, trend Loss: 0.0974 | 0.0997
Epoch 18/300, trend Loss: 0.0895 | 0.1137
Epoch 19/300, trend Loss: 0.0968 | 0.1705
Epoch 20/300, trend Loss: 0.0996 | 0.1330
Epoch 21/300, trend Loss: 0.1004 | 0.3460
Epoch 22/300, trend Loss: 0.1131 | 0.3755
Epoch 23/300, trend Loss: 0.1298 | 0.2957
Epoch 24/300, trend Loss: 0.1005 | 0.1276
Epoch 25/300, trend Loss: 0.1111 | 0.0773
Epoch 26/300, trend Loss: 0.1198 | 0.1016
Epoch 27/300, trend Loss: 0.0940 | 0.0893
Epoch 28/300, trend Loss: 0.0874 | 0.1616
Epoch 29/300, trend Loss: 0.0937 | 0.0832
Epoch 30/300, trend Loss: 0.1247 | 0.0804
Epoch 31/300, trend Loss: 0.1092 | 0.2991
Epoch 32/300, trend Loss: 0.1292 | 0.0874
Epoch 33/300, trend Loss: 0.0909 | 0.0930
Epoch 34/300, trend Loss: 0.1251 | 0.0778
Epoch 35/300, trend Loss: 0.0783 | 0.0735
Epoch 36/300, trend Loss: 0.0839 | 0.1445
Epoch 37/300, trend Loss: 0.0805 | 0.0988
Epoch 38/300, trend Loss: 0.0893 | 0.0870
Epoch 39/300, trend Loss: 0.0834 | 0.0690
Epoch 40/300, trend Loss: 0.0729 | 0.0736
Epoch 41/300, trend Loss: 0.0705 | 0.0706
Epoch 42/300, trend Loss: 0.0728 | 0.0592
Epoch 43/300, trend Loss: 0.0687 | 0.0573
Epoch 44/300, trend Loss: 0.0635 | 0.0564
Epoch 45/300, trend Loss: 0.0614 | 0.0569
Epoch 46/300, trend Loss: 0.0608 | 0.0636
Epoch 47/300, trend Loss: 0.0593 | 0.0627
Epoch 48/300, trend Loss: 0.0574 | 0.0540
Epoch 49/300, trend Loss: 0.0575 | 0.0505
Epoch 50/300, trend Loss: 0.0579 | 0.0501
Epoch 51/300, trend Loss: 0.0575 | 0.0520
Epoch 52/300, trend Loss: 0.0561 | 0.0534
Epoch 53/300, trend Loss: 0.0551 | 0.0520
Epoch 54/300, trend Loss: 0.0548 | 0.0518
Epoch 55/300, trend Loss: 0.0543 | 0.0527
Epoch 56/300, trend Loss: 0.0538 | 0.0537
Epoch 57/300, trend Loss: 0.0533 | 0.0526
Epoch 58/300, trend Loss: 0.0529 | 0.0512
Epoch 59/300, trend Loss: 0.0526 | 0.0503
Epoch 60/300, trend Loss: 0.0523 | 0.0499
Epoch 61/300, trend Loss: 0.0521 | 0.0498
Epoch 62/300, trend Loss: 0.0519 | 0.0501
Epoch 63/300, trend Loss: 0.0517 | 0.0503
Epoch 64/300, trend Loss: 0.0515 | 0.0504
Epoch 65/300, trend Loss: 0.0513 | 0.0503
Epoch 66/300, trend Loss: 0.0510 | 0.0501
Epoch 67/300, trend Loss: 0.0509 | 0.0497
Epoch 68/300, trend Loss: 0.0507 | 0.0496
Epoch 69/300, trend Loss: 0.0505 | 0.0495
Epoch 70/300, trend Loss: 0.0504 | 0.0494
Epoch 71/300, trend Loss: 0.0502 | 0.0494
Epoch 72/300, trend Loss: 0.0501 | 0.0494
Epoch 73/300, trend Loss: 0.0500 | 0.0494
Epoch 74/300, trend Loss: 0.0498 | 0.0493
Epoch 75/300, trend Loss: 0.0497 | 0.0492
Epoch 76/300, trend Loss: 0.0496 | 0.0491
Epoch 77/300, trend Loss: 0.0495 | 0.0490
Epoch 78/300, trend Loss: 0.0494 | 0.0490
Epoch 79/300, trend Loss: 0.0493 | 0.0489
Epoch 80/300, trend Loss: 0.0491 | 0.0489
Epoch 81/300, trend Loss: 0.0490 | 0.0488
Epoch 82/300, trend Loss: 0.0489 | 0.0488
Epoch 83/300, trend Loss: 0.0489 | 0.0488
Epoch 84/300, trend Loss: 0.0488 | 0.0487
Epoch 85/300, trend Loss: 0.0487 | 0.0487
Epoch 86/300, trend Loss: 0.0486 | 0.0486
Epoch 87/300, trend Loss: 0.0485 | 0.0486
Epoch 88/300, trend Loss: 0.0484 | 0.0486
Epoch 89/300, trend Loss: 0.0484 | 0.0485
Epoch 90/300, trend Loss: 0.0483 | 0.0485
Epoch 91/300, trend Loss: 0.0482 | 0.0485
Epoch 92/300, trend Loss: 0.0482 | 0.0484
Epoch 93/300, trend Loss: 0.0481 | 0.0484
Epoch 94/300, trend Loss: 0.0480 | 0.0484
Epoch 95/300, trend Loss: 0.0480 | 0.0484
Epoch 96/300, trend Loss: 0.0479 | 0.0483
Epoch 97/300, trend Loss: 0.0479 | 0.0483
Epoch 98/300, trend Loss: 0.0478 | 0.0483
Epoch 99/300, trend Loss: 0.0478 | 0.0483
Epoch 100/300, trend Loss: 0.0477 | 0.0482
Epoch 101/300, trend Loss: 0.0477 | 0.0482
Epoch 102/300, trend Loss: 0.0476 | 0.0482
Epoch 103/300, trend Loss: 0.0476 | 0.0482
Epoch 104/300, trend Loss: 0.0475 | 0.0482
Epoch 105/300, trend Loss: 0.0475 | 0.0481
Epoch 106/300, trend Loss: 0.0474 | 0.0481
Epoch 107/300, trend Loss: 0.0474 | 0.0481
Epoch 108/300, trend Loss: 0.0474 | 0.0481
Epoch 109/300, trend Loss: 0.0473 | 0.0481
Epoch 110/300, trend Loss: 0.0473 | 0.0481
Epoch 111/300, trend Loss: 0.0473 | 0.0480
Epoch 112/300, trend Loss: 0.0472 | 0.0480
Epoch 113/300, trend Loss: 0.0472 | 0.0480
Epoch 114/300, trend Loss: 0.0472 | 0.0480
Epoch 115/300, trend Loss: 0.0471 | 0.0480
Epoch 116/300, trend Loss: 0.0471 | 0.0480
Epoch 117/300, trend Loss: 0.0471 | 0.0480
Epoch 118/300, trend Loss: 0.0471 | 0.0480
Epoch 119/300, trend Loss: 0.0470 | 0.0480
Epoch 120/300, trend Loss: 0.0470 | 0.0479
Epoch 121/300, trend Loss: 0.0470 | 0.0479
Epoch 122/300, trend Loss: 0.0470 | 0.0479
Epoch 123/300, trend Loss: 0.0469 | 0.0479
Epoch 124/300, trend Loss: 0.0469 | 0.0479
Epoch 125/300, trend Loss: 0.0469 | 0.0479
Epoch 126/300, trend Loss: 0.0469 | 0.0479
Epoch 127/300, trend Loss: 0.0469 | 0.0479
Epoch 128/300, trend Loss: 0.0468 | 0.0479
Epoch 129/300, trend Loss: 0.0468 | 0.0479
Epoch 130/300, trend Loss: 0.0468 | 0.0479
Epoch 131/300, trend Loss: 0.0468 | 0.0478
Epoch 132/300, trend Loss: 0.0468 | 0.0478
Epoch 133/300, trend Loss: 0.0467 | 0.0478
Epoch 134/300, trend Loss: 0.0467 | 0.0478
Epoch 135/300, trend Loss: 0.0467 | 0.0478
Epoch 136/300, trend Loss: 0.0467 | 0.0478
Epoch 137/300, trend Loss: 0.0467 | 0.0478
Epoch 138/300, trend Loss: 0.0467 | 0.0478
Epoch 139/300, trend Loss: 0.0467 | 0.0478
Epoch 140/300, trend Loss: 0.0466 | 0.0478
Epoch 141/300, trend Loss: 0.0466 | 0.0478
Epoch 142/300, trend Loss: 0.0466 | 0.0478
Epoch 143/300, trend Loss: 0.0466 | 0.0478
Epoch 144/300, trend Loss: 0.0466 | 0.0478
Epoch 145/300, trend Loss: 0.0466 | 0.0478
Epoch 146/300, trend Loss: 0.0466 | 0.0478
Epoch 147/300, trend Loss: 0.0466 | 0.0478
Epoch 148/300, trend Loss: 0.0466 | 0.0478
Epoch 149/300, trend Loss: 0.0466 | 0.0478
Epoch 150/300, trend Loss: 0.0465 | 0.0478
Epoch 151/300, trend Loss: 0.0465 | 0.0477
Epoch 152/300, trend Loss: 0.0465 | 0.0477
Epoch 153/300, trend Loss: 0.0465 | 0.0477
Epoch 154/300, trend Loss: 0.0465 | 0.0477
Epoch 155/300, trend Loss: 0.0465 | 0.0477
Epoch 156/300, trend Loss: 0.0465 | 0.0477
Epoch 157/300, trend Loss: 0.0465 | 0.0477
Epoch 158/300, trend Loss: 0.0465 | 0.0477
Epoch 159/300, trend Loss: 0.0465 | 0.0477
Epoch 160/300, trend Loss: 0.0465 | 0.0477
Epoch 161/300, trend Loss: 0.0465 | 0.0477
Epoch 162/300, trend Loss: 0.0465 | 0.0477
Epoch 163/300, trend Loss: 0.0465 | 0.0477
Epoch 164/300, trend Loss: 0.0464 | 0.0477
Epoch 165/300, trend Loss: 0.0464 | 0.0477
Epoch 166/300, trend Loss: 0.0464 | 0.0477
Epoch 167/300, trend Loss: 0.0464 | 0.0477
Epoch 168/300, trend Loss: 0.0464 | 0.0477
Epoch 169/300, trend Loss: 0.0464 | 0.0477
Epoch 170/300, trend Loss: 0.0464 | 0.0477
Epoch 171/300, trend Loss: 0.0464 | 0.0477
Epoch 172/300, trend Loss: 0.0464 | 0.0477
Epoch 173/300, trend Loss: 0.0464 | 0.0477
Epoch 174/300, trend Loss: 0.0464 | 0.0477
Epoch 175/300, trend Loss: 0.0464 | 0.0477
Epoch 176/300, trend Loss: 0.0464 | 0.0477
Epoch 177/300, trend Loss: 0.0464 | 0.0477
Epoch 178/300, trend Loss: 0.0464 | 0.0477
Epoch 179/300, trend Loss: 0.0464 | 0.0477
Epoch 180/300, trend Loss: 0.0464 | 0.0477
Epoch 181/300, trend Loss: 0.0464 | 0.0477
Epoch 182/300, trend Loss: 0.0464 | 0.0477
Epoch 183/300, trend Loss: 0.0464 | 0.0477
Epoch 184/300, trend Loss: 0.0464 | 0.0477
Epoch 185/300, trend Loss: 0.0464 | 0.0477
Epoch 186/300, trend Loss: 0.0464 | 0.0477
Epoch 187/300, trend Loss: 0.0464 | 0.0477
Epoch 188/300, trend Loss: 0.0464 | 0.0477
Epoch 189/300, trend Loss: 0.0464 | 0.0477
Epoch 190/300, trend Loss: 0.0464 | 0.0477
Epoch 191/300, trend Loss: 0.0463 | 0.0477
Epoch 192/300, trend Loss: 0.0463 | 0.0477
Epoch 193/300, trend Loss: 0.0463 | 0.0477
Epoch 194/300, trend Loss: 0.0463 | 0.0477
Epoch 195/300, trend Loss: 0.0463 | 0.0477
Epoch 196/300, trend Loss: 0.0463 | 0.0477
Epoch 197/300, trend Loss: 0.0463 | 0.0477
Epoch 198/300, trend Loss: 0.0463 | 0.0477
Epoch 199/300, trend Loss: 0.0463 | 0.0477
Epoch 200/300, trend Loss: 0.0463 | 0.0477
Epoch 201/300, trend Loss: 0.0463 | 0.0477
Epoch 202/300, trend Loss: 0.0463 | 0.0477
Epoch 203/300, trend Loss: 0.0463 | 0.0477
Epoch 204/300, trend Loss: 0.0463 | 0.0477
Epoch 205/300, trend Loss: 0.0463 | 0.0477
Epoch 206/300, trend Loss: 0.0463 | 0.0477
Epoch 207/300, trend Loss: 0.0463 | 0.0477
Epoch 208/300, trend Loss: 0.0463 | 0.0477
Epoch 209/300, trend Loss: 0.0463 | 0.0477
Epoch 210/300, trend Loss: 0.0463 | 0.0477
Epoch 211/300, trend Loss: 0.0463 | 0.0476
Epoch 212/300, trend Loss: 0.0463 | 0.0476
Epoch 213/300, trend Loss: 0.0463 | 0.0476
Epoch 214/300, trend Loss: 0.0463 | 0.0476
Epoch 215/300, trend Loss: 0.0463 | 0.0476
Epoch 216/300, trend Loss: 0.0463 | 0.0476
Epoch 217/300, trend Loss: 0.0463 | 0.0476
Epoch 218/300, trend Loss: 0.0463 | 0.0476
Epoch 219/300, trend Loss: 0.0463 | 0.0476
Epoch 220/300, trend Loss: 0.0463 | 0.0476
Epoch 221/300, trend Loss: 0.0463 | 0.0476
Epoch 222/300, trend Loss: 0.0463 | 0.0476
Epoch 223/300, trend Loss: 0.0463 | 0.0476
Epoch 224/300, trend Loss: 0.0463 | 0.0476
Epoch 225/300, trend Loss: 0.0463 | 0.0476
Epoch 226/300, trend Loss: 0.0463 | 0.0476
Epoch 227/300, trend Loss: 0.0463 | 0.0476
Epoch 228/300, trend Loss: 0.0463 | 0.0476
Epoch 229/300, trend Loss: 0.0463 | 0.0476
Epoch 230/300, trend Loss: 0.0463 | 0.0476
Epoch 231/300, trend Loss: 0.0463 | 0.0476
Epoch 232/300, trend Loss: 0.0463 | 0.0476
Epoch 233/300, trend Loss: 0.0463 | 0.0476
Epoch 234/300, trend Loss: 0.0463 | 0.0476
Epoch 235/300, trend Loss: 0.0463 | 0.0476
Epoch 236/300, trend Loss: 0.0463 | 0.0476
Epoch 237/300, trend Loss: 0.0463 | 0.0476
Epoch 238/300, trend Loss: 0.0463 | 0.0476
Epoch 239/300, trend Loss: 0.0463 | 0.0476
Epoch 240/300, trend Loss: 0.0463 | 0.0476
Epoch 241/300, trend Loss: 0.0463 | 0.0476
Epoch 242/300, trend Loss: 0.0463 | 0.0476
Epoch 243/300, trend Loss: 0.0463 | 0.0476
Epoch 244/300, trend Loss: 0.0463 | 0.0476
Epoch 245/300, trend Loss: 0.0463 | 0.0476
Epoch 246/300, trend Loss: 0.0463 | 0.0476
Epoch 247/300, trend Loss: 0.0463 | 0.0476
Epoch 248/300, trend Loss: 0.0463 | 0.0476
Epoch 249/300, trend Loss: 0.0463 | 0.0476
Epoch 250/300, trend Loss: 0.0463 | 0.0476
Epoch 251/300, trend Loss: 0.0463 | 0.0476
Epoch 252/300, trend Loss: 0.0463 | 0.0476
Epoch 253/300, trend Loss: 0.0463 | 0.0476
Epoch 254/300, trend Loss: 0.0463 | 0.0476
Epoch 255/300, trend Loss: 0.0463 | 0.0476
Epoch 256/300, trend Loss: 0.0463 | 0.0476
Epoch 257/300, trend Loss: 0.0463 | 0.0476
Epoch 258/300, trend Loss: 0.0463 | 0.0476
Epoch 259/300, trend Loss: 0.0463 | 0.0476
Epoch 260/300, trend Loss: 0.0463 | 0.0476
Epoch 261/300, trend Loss: 0.0463 | 0.0476
Epoch 262/300, trend Loss: 0.0463 | 0.0476
Epoch 263/300, trend Loss: 0.0463 | 0.0476
Epoch 264/300, trend Loss: 0.0463 | 0.0476
Epoch 265/300, trend Loss: 0.0463 | 0.0476
Epoch 266/300, trend Loss: 0.0463 | 0.0476
Epoch 267/300, trend Loss: 0.0463 | 0.0476
Epoch 268/300, trend Loss: 0.0463 | 0.0476
Epoch 269/300, trend Loss: 0.0463 | 0.0476
Epoch 270/300, trend Loss: 0.0463 | 0.0476
Epoch 271/300, trend Loss: 0.0463 | 0.0476
Epoch 272/300, trend Loss: 0.0463 | 0.0476
Epoch 273/300, trend Loss: 0.0463 | 0.0476
Epoch 274/300, trend Loss: 0.0463 | 0.0476
Epoch 275/300, trend Loss: 0.0463 | 0.0476
Epoch 276/300, trend Loss: 0.0463 | 0.0476
Epoch 277/300, trend Loss: 0.0463 | 0.0476
Epoch 278/300, trend Loss: 0.0463 | 0.0476
Epoch 279/300, trend Loss: 0.0463 | 0.0476
Epoch 280/300, trend Loss: 0.0463 | 0.0476
Epoch 281/300, trend Loss: 0.0463 | 0.0476
Epoch 282/300, trend Loss: 0.0463 | 0.0476
Epoch 283/300, trend Loss: 0.0463 | 0.0476
Epoch 284/300, trend Loss: 0.0463 | 0.0476
Epoch 285/300, trend Loss: 0.0463 | 0.0476
Epoch 286/300, trend Loss: 0.0463 | 0.0476
Epoch 287/300, trend Loss: 0.0463 | 0.0476
Epoch 288/300, trend Loss: 0.0463 | 0.0476
Epoch 289/300, trend Loss: 0.0463 | 0.0476
Epoch 290/300, trend Loss: 0.0463 | 0.0476
Epoch 291/300, trend Loss: 0.0463 | 0.0476
Epoch 292/300, trend Loss: 0.0463 | 0.0476
Epoch 293/300, trend Loss: 0.0463 | 0.0476
Epoch 294/300, trend Loss: 0.0463 | 0.0476
Epoch 295/300, trend Loss: 0.0463 | 0.0476
Epoch 296/300, trend Loss: 0.0463 | 0.0476
Epoch 297/300, trend Loss: 0.0463 | 0.0476
Epoch 298/300, trend Loss: 0.0463 | 0.0476
Epoch 299/300, trend Loss: 0.0463 | 0.0476
Epoch 300/300, trend Loss: 0.0463 | 0.0476
Training seasonal_0 component with params: {'observation_period_num': 27, 'train_rates': 0.9257007731034621, 'learning_rate': 9.51023635202683e-06, 'batch_size': 121, 'step_size': 11, 'gamma': 0.8961406045884175}
Epoch 1/300, seasonal_0 Loss: 0.5600 | 0.2921
Epoch 2/300, seasonal_0 Loss: 0.2969 | 0.3286
Epoch 3/300, seasonal_0 Loss: 0.2621 | 0.2723
Epoch 4/300, seasonal_0 Loss: 0.2037 | 0.1435
Epoch 5/300, seasonal_0 Loss: 0.2116 | 0.1487
Epoch 6/300, seasonal_0 Loss: 0.1755 | 0.1486
Epoch 7/300, seasonal_0 Loss: 0.2468 | 0.3311
Epoch 8/300, seasonal_0 Loss: 0.2261 | 0.1298
Epoch 9/300, seasonal_0 Loss: 0.1924 | 0.1142
Epoch 10/300, seasonal_0 Loss: 0.2525 | 0.1191
Epoch 11/300, seasonal_0 Loss: 0.1687 | 0.1348
Epoch 12/300, seasonal_0 Loss: 0.1408 | 0.1126
Epoch 13/300, seasonal_0 Loss: 0.1345 | 0.0994
Epoch 14/300, seasonal_0 Loss: 0.1303 | 0.0991
Epoch 15/300, seasonal_0 Loss: 0.1261 | 0.0907
Epoch 16/300, seasonal_0 Loss: 0.1212 | 0.0867
Epoch 17/300, seasonal_0 Loss: 0.1192 | 0.0880
Epoch 18/300, seasonal_0 Loss: 0.1170 | 0.0887
Epoch 19/300, seasonal_0 Loss: 0.1153 | 0.0828
Epoch 20/300, seasonal_0 Loss: 0.1135 | 0.0770
Epoch 21/300, seasonal_0 Loss: 0.1115 | 0.0743
Epoch 22/300, seasonal_0 Loss: 0.1099 | 0.0738
Epoch 23/300, seasonal_0 Loss: 0.1086 | 0.0758
Epoch 24/300, seasonal_0 Loss: 0.1076 | 0.0779
Epoch 25/300, seasonal_0 Loss: 0.1066 | 0.0767
Epoch 26/300, seasonal_0 Loss: 0.1052 | 0.0716
Epoch 27/300, seasonal_0 Loss: 0.1043 | 0.0667
Epoch 28/300, seasonal_0 Loss: 0.1051 | 0.0654
Epoch 29/300, seasonal_0 Loss: 0.1055 | 0.0672
Epoch 30/300, seasonal_0 Loss: 0.1028 | 0.0647
Epoch 31/300, seasonal_0 Loss: 0.1070 | 0.0858
Epoch 32/300, seasonal_0 Loss: 0.1150 | 0.1416
Epoch 33/300, seasonal_0 Loss: 0.1079 | 0.0958
Epoch 34/300, seasonal_0 Loss: 0.1111 | 0.0704
Epoch 35/300, seasonal_0 Loss: 0.1170 | 0.0715
Epoch 36/300, seasonal_0 Loss: 0.1035 | 0.0777
Epoch 37/300, seasonal_0 Loss: 0.1155 | 0.0958
Epoch 38/300, seasonal_0 Loss: 0.1057 | 0.0642
Epoch 39/300, seasonal_0 Loss: 0.1031 | 0.0621
Epoch 40/300, seasonal_0 Loss: 0.1135 | 0.0612
Epoch 41/300, seasonal_0 Loss: 0.0996 | 0.0703
Epoch 42/300, seasonal_0 Loss: 0.1003 | 0.0691
Epoch 43/300, seasonal_0 Loss: 0.0993 | 0.0609
Epoch 44/300, seasonal_0 Loss: 0.0946 | 0.0588
Epoch 45/300, seasonal_0 Loss: 0.0974 | 0.0595
Epoch 46/300, seasonal_0 Loss: 0.0923 | 0.0609
Epoch 47/300, seasonal_0 Loss: 0.0941 | 0.0608
Epoch 48/300, seasonal_0 Loss: 0.0914 | 0.0588
Epoch 49/300, seasonal_0 Loss: 0.0914 | 0.0581
Epoch 50/300, seasonal_0 Loss: 0.0906 | 0.0576
Epoch 51/300, seasonal_0 Loss: 0.0899 | 0.0582
Epoch 52/300, seasonal_0 Loss: 0.0898 | 0.0584
Epoch 53/300, seasonal_0 Loss: 0.0887 | 0.0576
Epoch 54/300, seasonal_0 Loss: 0.0886 | 0.0567
Epoch 55/300, seasonal_0 Loss: 0.0880 | 0.0564
Epoch 56/300, seasonal_0 Loss: 0.0879 | 0.0567
Epoch 57/300, seasonal_0 Loss: 0.0873 | 0.0569
Epoch 58/300, seasonal_0 Loss: 0.0869 | 0.0564
Epoch 59/300, seasonal_0 Loss: 0.0866 | 0.0558
Epoch 60/300, seasonal_0 Loss: 0.0863 | 0.0556
Epoch 61/300, seasonal_0 Loss: 0.0860 | 0.0557
Epoch 62/300, seasonal_0 Loss: 0.0856 | 0.0557
Epoch 63/300, seasonal_0 Loss: 0.0853 | 0.0555
Epoch 64/300, seasonal_0 Loss: 0.0850 | 0.0551
Epoch 65/300, seasonal_0 Loss: 0.0848 | 0.0550
Epoch 66/300, seasonal_0 Loss: 0.0845 | 0.0549
Epoch 67/300, seasonal_0 Loss: 0.0842 | 0.0548
Epoch 68/300, seasonal_0 Loss: 0.0840 | 0.0547
Epoch 69/300, seasonal_0 Loss: 0.0837 | 0.0545
Epoch 70/300, seasonal_0 Loss: 0.0835 | 0.0544
Epoch 71/300, seasonal_0 Loss: 0.0833 | 0.0543
Epoch 72/300, seasonal_0 Loss: 0.0830 | 0.0542
Epoch 73/300, seasonal_0 Loss: 0.0828 | 0.0541
Epoch 74/300, seasonal_0 Loss: 0.0826 | 0.0540
Epoch 75/300, seasonal_0 Loss: 0.0824 | 0.0539
Epoch 76/300, seasonal_0 Loss: 0.0822 | 0.0538
Epoch 77/300, seasonal_0 Loss: 0.0819 | 0.0537
Epoch 78/300, seasonal_0 Loss: 0.0817 | 0.0537
Epoch 79/300, seasonal_0 Loss: 0.0815 | 0.0536
Epoch 80/300, seasonal_0 Loss: 0.0814 | 0.0535
Epoch 81/300, seasonal_0 Loss: 0.0812 | 0.0534
Epoch 82/300, seasonal_0 Loss: 0.0810 | 0.0533
Epoch 83/300, seasonal_0 Loss: 0.0808 | 0.0533
Epoch 84/300, seasonal_0 Loss: 0.0806 | 0.0532
Epoch 85/300, seasonal_0 Loss: 0.0805 | 0.0532
Epoch 86/300, seasonal_0 Loss: 0.0803 | 0.0531
Epoch 87/300, seasonal_0 Loss: 0.0801 | 0.0530
Epoch 88/300, seasonal_0 Loss: 0.0800 | 0.0530
Epoch 89/300, seasonal_0 Loss: 0.0798 | 0.0529
Epoch 90/300, seasonal_0 Loss: 0.0796 | 0.0529
Epoch 91/300, seasonal_0 Loss: 0.0795 | 0.0528
Epoch 92/300, seasonal_0 Loss: 0.0793 | 0.0528
Epoch 93/300, seasonal_0 Loss: 0.0792 | 0.0527
Epoch 94/300, seasonal_0 Loss: 0.0791 | 0.0527
Epoch 95/300, seasonal_0 Loss: 0.0789 | 0.0526
Epoch 96/300, seasonal_0 Loss: 0.0788 | 0.0526
Epoch 97/300, seasonal_0 Loss: 0.0786 | 0.0525
Epoch 98/300, seasonal_0 Loss: 0.0785 | 0.0525
Epoch 99/300, seasonal_0 Loss: 0.0784 | 0.0524
Epoch 100/300, seasonal_0 Loss: 0.0782 | 0.0524
Epoch 101/300, seasonal_0 Loss: 0.0781 | 0.0524
Epoch 102/300, seasonal_0 Loss: 0.0780 | 0.0523
Epoch 103/300, seasonal_0 Loss: 0.0779 | 0.0523
Epoch 104/300, seasonal_0 Loss: 0.0778 | 0.0522
Epoch 105/300, seasonal_0 Loss: 0.0777 | 0.0522
Epoch 106/300, seasonal_0 Loss: 0.0775 | 0.0522
Epoch 107/300, seasonal_0 Loss: 0.0774 | 0.0521
Epoch 108/300, seasonal_0 Loss: 0.0773 | 0.0521
Epoch 109/300, seasonal_0 Loss: 0.0772 | 0.0521
Epoch 110/300, seasonal_0 Loss: 0.0771 | 0.0520
Epoch 111/300, seasonal_0 Loss: 0.0770 | 0.0520
Epoch 112/300, seasonal_0 Loss: 0.0769 | 0.0520
Epoch 113/300, seasonal_0 Loss: 0.0768 | 0.0519
Epoch 114/300, seasonal_0 Loss: 0.0767 | 0.0519
Epoch 115/300, seasonal_0 Loss: 0.0766 | 0.0519
Epoch 116/300, seasonal_0 Loss: 0.0765 | 0.0518
Epoch 117/300, seasonal_0 Loss: 0.0764 | 0.0518
Epoch 118/300, seasonal_0 Loss: 0.0763 | 0.0518
Epoch 119/300, seasonal_0 Loss: 0.0762 | 0.0518
Epoch 120/300, seasonal_0 Loss: 0.0762 | 0.0517
Epoch 121/300, seasonal_0 Loss: 0.0761 | 0.0517
Epoch 122/300, seasonal_0 Loss: 0.0760 | 0.0517
Epoch 123/300, seasonal_0 Loss: 0.0759 | 0.0517
Epoch 124/300, seasonal_0 Loss: 0.0758 | 0.0516
Epoch 125/300, seasonal_0 Loss: 0.0757 | 0.0516
Epoch 126/300, seasonal_0 Loss: 0.0757 | 0.0516
Epoch 127/300, seasonal_0 Loss: 0.0756 | 0.0516
Epoch 128/300, seasonal_0 Loss: 0.0755 | 0.0515
Epoch 129/300, seasonal_0 Loss: 0.0754 | 0.0515
Epoch 130/300, seasonal_0 Loss: 0.0754 | 0.0515
Epoch 131/300, seasonal_0 Loss: 0.0753 | 0.0515
Epoch 132/300, seasonal_0 Loss: 0.0752 | 0.0514
Epoch 133/300, seasonal_0 Loss: 0.0751 | 0.0514
Epoch 134/300, seasonal_0 Loss: 0.0751 | 0.0514
Epoch 135/300, seasonal_0 Loss: 0.0750 | 0.0514
Epoch 136/300, seasonal_0 Loss: 0.0749 | 0.0514
Epoch 137/300, seasonal_0 Loss: 0.0749 | 0.0513
Epoch 138/300, seasonal_0 Loss: 0.0748 | 0.0513
Epoch 139/300, seasonal_0 Loss: 0.0747 | 0.0513
Epoch 140/300, seasonal_0 Loss: 0.0747 | 0.0513
Epoch 141/300, seasonal_0 Loss: 0.0746 | 0.0513
Epoch 142/300, seasonal_0 Loss: 0.0746 | 0.0513
Epoch 143/300, seasonal_0 Loss: 0.0745 | 0.0512
Epoch 144/300, seasonal_0 Loss: 0.0744 | 0.0512
Epoch 145/300, seasonal_0 Loss: 0.0744 | 0.0512
Epoch 146/300, seasonal_0 Loss: 0.0743 | 0.0512
Epoch 147/300, seasonal_0 Loss: 0.0743 | 0.0512
Epoch 148/300, seasonal_0 Loss: 0.0742 | 0.0512
Epoch 149/300, seasonal_0 Loss: 0.0742 | 0.0511
Epoch 150/300, seasonal_0 Loss: 0.0741 | 0.0511
Epoch 151/300, seasonal_0 Loss: 0.0741 | 0.0511
Epoch 152/300, seasonal_0 Loss: 0.0740 | 0.0511
Epoch 153/300, seasonal_0 Loss: 0.0740 | 0.0511
Epoch 154/300, seasonal_0 Loss: 0.0739 | 0.0511
Epoch 155/300, seasonal_0 Loss: 0.0739 | 0.0511
Epoch 156/300, seasonal_0 Loss: 0.0738 | 0.0510
Epoch 157/300, seasonal_0 Loss: 0.0738 | 0.0510
Epoch 158/300, seasonal_0 Loss: 0.0737 | 0.0510
Epoch 159/300, seasonal_0 Loss: 0.0737 | 0.0510
Epoch 160/300, seasonal_0 Loss: 0.0736 | 0.0510
Epoch 161/300, seasonal_0 Loss: 0.0736 | 0.0510
Epoch 162/300, seasonal_0 Loss: 0.0736 | 0.0510
Epoch 163/300, seasonal_0 Loss: 0.0735 | 0.0510
Epoch 164/300, seasonal_0 Loss: 0.0735 | 0.0509
Epoch 165/300, seasonal_0 Loss: 0.0734 | 0.0509
Epoch 166/300, seasonal_0 Loss: 0.0734 | 0.0509
Epoch 167/300, seasonal_0 Loss: 0.0733 | 0.0509
Epoch 168/300, seasonal_0 Loss: 0.0733 | 0.0509
Epoch 169/300, seasonal_0 Loss: 0.0733 | 0.0509
Epoch 170/300, seasonal_0 Loss: 0.0732 | 0.0509
Epoch 171/300, seasonal_0 Loss: 0.0732 | 0.0509
Epoch 172/300, seasonal_0 Loss: 0.0732 | 0.0508
Epoch 173/300, seasonal_0 Loss: 0.0731 | 0.0508
Epoch 174/300, seasonal_0 Loss: 0.0731 | 0.0508
Epoch 175/300, seasonal_0 Loss: 0.0731 | 0.0508
Epoch 176/300, seasonal_0 Loss: 0.0730 | 0.0508
Epoch 177/300, seasonal_0 Loss: 0.0730 | 0.0508
Epoch 178/300, seasonal_0 Loss: 0.0730 | 0.0508
Epoch 179/300, seasonal_0 Loss: 0.0729 | 0.0508
Epoch 180/300, seasonal_0 Loss: 0.0729 | 0.0508
Epoch 181/300, seasonal_0 Loss: 0.0729 | 0.0508
Epoch 182/300, seasonal_0 Loss: 0.0728 | 0.0507
Epoch 183/300, seasonal_0 Loss: 0.0728 | 0.0507
Epoch 184/300, seasonal_0 Loss: 0.0728 | 0.0507
Epoch 185/300, seasonal_0 Loss: 0.0727 | 0.0507
Epoch 186/300, seasonal_0 Loss: 0.0727 | 0.0507
Epoch 187/300, seasonal_0 Loss: 0.0727 | 0.0507
Epoch 188/300, seasonal_0 Loss: 0.0727 | 0.0507
Epoch 189/300, seasonal_0 Loss: 0.0726 | 0.0507
Epoch 190/300, seasonal_0 Loss: 0.0726 | 0.0507
Epoch 191/300, seasonal_0 Loss: 0.0726 | 0.0507
Epoch 192/300, seasonal_0 Loss: 0.0725 | 0.0507
Epoch 193/300, seasonal_0 Loss: 0.0725 | 0.0507
Epoch 194/300, seasonal_0 Loss: 0.0725 | 0.0506
Epoch 195/300, seasonal_0 Loss: 0.0725 | 0.0506
Epoch 196/300, seasonal_0 Loss: 0.0724 | 0.0506
Epoch 197/300, seasonal_0 Loss: 0.0724 | 0.0506
Epoch 198/300, seasonal_0 Loss: 0.0724 | 0.0506
Epoch 199/300, seasonal_0 Loss: 0.0724 | 0.0506
Epoch 200/300, seasonal_0 Loss: 0.0724 | 0.0506
Epoch 201/300, seasonal_0 Loss: 0.0723 | 0.0506
Epoch 202/300, seasonal_0 Loss: 0.0723 | 0.0506
Epoch 203/300, seasonal_0 Loss: 0.0723 | 0.0506
Epoch 204/300, seasonal_0 Loss: 0.0723 | 0.0506
Epoch 205/300, seasonal_0 Loss: 0.0722 | 0.0506
Epoch 206/300, seasonal_0 Loss: 0.0722 | 0.0506
Epoch 207/300, seasonal_0 Loss: 0.0722 | 0.0506
Epoch 208/300, seasonal_0 Loss: 0.0722 | 0.0506
Epoch 209/300, seasonal_0 Loss: 0.0722 | 0.0506
Epoch 210/300, seasonal_0 Loss: 0.0721 | 0.0505
Epoch 211/300, seasonal_0 Loss: 0.0721 | 0.0505
Epoch 212/300, seasonal_0 Loss: 0.0721 | 0.0505
Epoch 213/300, seasonal_0 Loss: 0.0721 | 0.0505
Epoch 214/300, seasonal_0 Loss: 0.0721 | 0.0505
Epoch 215/300, seasonal_0 Loss: 0.0721 | 0.0505
Epoch 216/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 217/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 218/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 219/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 220/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 221/300, seasonal_0 Loss: 0.0720 | 0.0505
Epoch 222/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 223/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 224/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 225/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 226/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 227/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 228/300, seasonal_0 Loss: 0.0719 | 0.0505
Epoch 229/300, seasonal_0 Loss: 0.0718 | 0.0505
Epoch 230/300, seasonal_0 Loss: 0.0718 | 0.0505
Epoch 231/300, seasonal_0 Loss: 0.0718 | 0.0504
Epoch 232/300, seasonal_0 Loss: 0.0718 | 0.0504
Epoch 233/300, seasonal_0 Loss: 0.0718 | 0.0504
Epoch 234/300, seasonal_0 Loss: 0.0718 | 0.0504
Epoch 235/300, seasonal_0 Loss: 0.0718 | 0.0504
Epoch 236/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 237/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 238/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 239/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 240/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 241/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 242/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 243/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 244/300, seasonal_0 Loss: 0.0717 | 0.0504
Epoch 245/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 246/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 247/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 248/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 249/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 250/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 251/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 252/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 253/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 254/300, seasonal_0 Loss: 0.0716 | 0.0504
Epoch 255/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 256/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 257/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 258/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 259/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 260/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 261/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 262/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 263/300, seasonal_0 Loss: 0.0715 | 0.0504
Epoch 264/300, seasonal_0 Loss: 0.0715 | 0.0503
Epoch 265/300, seasonal_0 Loss: 0.0715 | 0.0503
Epoch 266/300, seasonal_0 Loss: 0.0715 | 0.0503
Epoch 267/300, seasonal_0 Loss: 0.0715 | 0.0503
Epoch 268/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 269/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 270/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 271/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 272/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 273/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 274/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 275/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 276/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 277/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 278/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 279/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 280/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 281/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 282/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 283/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 284/300, seasonal_0 Loss: 0.0714 | 0.0503
Epoch 285/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 286/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 287/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 288/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 289/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 290/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 291/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 292/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 293/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 294/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 295/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 296/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 297/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 298/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 299/300, seasonal_0 Loss: 0.0713 | 0.0503
Epoch 300/300, seasonal_0 Loss: 0.0713 | 0.0503
Training seasonal_1 component with params: {'observation_period_num': 12, 'train_rates': 0.9885797675229907, 'learning_rate': 0.0001303452853256295, 'batch_size': 55, 'step_size': 12, 'gamma': 0.9890051303776992}
Epoch 1/300, seasonal_1 Loss: 0.5576 | 0.1156
Epoch 2/300, seasonal_1 Loss: 0.1795 | 0.1190
Epoch 3/300, seasonal_1 Loss: 0.1455 | 0.1715
Epoch 4/300, seasonal_1 Loss: 0.1282 | 0.0707
Epoch 5/300, seasonal_1 Loss: 0.1045 | 0.0719
Epoch 6/300, seasonal_1 Loss: 0.0997 | 0.0773
Epoch 7/300, seasonal_1 Loss: 0.1109 | 0.0694
Epoch 8/300, seasonal_1 Loss: 0.1097 | 0.0596
Epoch 9/300, seasonal_1 Loss: 0.0926 | 0.0650
Epoch 10/300, seasonal_1 Loss: 0.0999 | 0.0930
Epoch 11/300, seasonal_1 Loss: 0.0980 | 0.0665
Epoch 12/300, seasonal_1 Loss: 0.0977 | 0.0730
Epoch 13/300, seasonal_1 Loss: 0.0947 | 0.0553
Epoch 14/300, seasonal_1 Loss: 0.0809 | 0.0564
Epoch 15/300, seasonal_1 Loss: 0.0790 | 0.0661
Epoch 16/300, seasonal_1 Loss: 0.0767 | 0.0499
Epoch 17/300, seasonal_1 Loss: 0.0738 | 0.0508
Epoch 18/300, seasonal_1 Loss: 0.0737 | 0.0499
Epoch 19/300, seasonal_1 Loss: 0.0718 | 0.0471
Epoch 20/300, seasonal_1 Loss: 0.0737 | 0.0448
Epoch 21/300, seasonal_1 Loss: 0.0693 | 0.0394
Epoch 22/300, seasonal_1 Loss: 0.0706 | 0.0413
Epoch 23/300, seasonal_1 Loss: 0.0773 | 0.0581
Epoch 24/300, seasonal_1 Loss: 0.0708 | 0.0502
Epoch 25/300, seasonal_1 Loss: 0.0833 | 0.0560
Epoch 26/300, seasonal_1 Loss: 0.0815 | 0.0473
Epoch 27/300, seasonal_1 Loss: 0.0857 | 0.0739
Epoch 28/300, seasonal_1 Loss: 0.0790 | 0.0445
Epoch 29/300, seasonal_1 Loss: 0.0693 | 0.0347
Epoch 30/300, seasonal_1 Loss: 0.0644 | 0.0333
Epoch 31/300, seasonal_1 Loss: 0.0652 | 0.0370
Epoch 32/300, seasonal_1 Loss: 0.0643 | 0.0412
Epoch 33/300, seasonal_1 Loss: 0.0913 | 0.1059
Epoch 34/300, seasonal_1 Loss: 0.0895 | 0.0612
Epoch 35/300, seasonal_1 Loss: 0.0731 | 0.0450
Epoch 36/300, seasonal_1 Loss: 0.0726 | 0.0563
Epoch 37/300, seasonal_1 Loss: 0.0654 | 0.0368
Epoch 38/300, seasonal_1 Loss: 0.0659 | 0.0330
Epoch 39/300, seasonal_1 Loss: 0.0653 | 0.0360
Epoch 40/300, seasonal_1 Loss: 0.0632 | 0.0352
Epoch 41/300, seasonal_1 Loss: 0.0642 | 0.0346
Epoch 42/300, seasonal_1 Loss: 0.0569 | 0.0300
Epoch 43/300, seasonal_1 Loss: 0.0564 | 0.0312
Epoch 44/300, seasonal_1 Loss: 0.0551 | 0.0344
Epoch 45/300, seasonal_1 Loss: 0.0562 | 0.0325
Epoch 46/300, seasonal_1 Loss: 0.0539 | 0.0354
Epoch 47/300, seasonal_1 Loss: 0.0569 | 0.0347
Epoch 48/300, seasonal_1 Loss: 0.0568 | 0.0396
Epoch 49/300, seasonal_1 Loss: 0.0566 | 0.0427
Epoch 50/300, seasonal_1 Loss: 0.0551 | 0.0477
Epoch 51/300, seasonal_1 Loss: 0.0518 | 0.0408
Epoch 52/300, seasonal_1 Loss: 0.0518 | 0.0381
Epoch 53/300, seasonal_1 Loss: 0.0578 | 0.0611
Epoch 54/300, seasonal_1 Loss: 0.0704 | 0.0406
Epoch 55/300, seasonal_1 Loss: 0.0647 | 0.0419
Epoch 56/300, seasonal_1 Loss: 0.0735 | 0.0533
Epoch 57/300, seasonal_1 Loss: 0.0688 | 0.0406
Epoch 58/300, seasonal_1 Loss: 0.0683 | 0.0451
Epoch 59/300, seasonal_1 Loss: 0.0551 | 0.0466
Epoch 60/300, seasonal_1 Loss: 0.0544 | 0.0367
Epoch 61/300, seasonal_1 Loss: 0.0592 | 0.0367
Epoch 62/300, seasonal_1 Loss: 0.0608 | 0.0464
Epoch 63/300, seasonal_1 Loss: 0.0609 | 0.0454
Epoch 64/300, seasonal_1 Loss: 0.0563 | 0.0362
Epoch 65/300, seasonal_1 Loss: 0.0555 | 0.0453
Epoch 66/300, seasonal_1 Loss: 0.0568 | 0.0547
Epoch 67/300, seasonal_1 Loss: 0.0594 | 0.0557
Epoch 68/300, seasonal_1 Loss: 0.0558 | 0.0383
Epoch 69/300, seasonal_1 Loss: 0.0563 | 0.0510
Epoch 70/300, seasonal_1 Loss: 0.0583 | 0.0510
Epoch 71/300, seasonal_1 Loss: 0.0491 | 0.0449
Epoch 72/300, seasonal_1 Loss: 0.0565 | 0.0731
Epoch 73/300, seasonal_1 Loss: 0.0557 | 0.0468
Epoch 74/300, seasonal_1 Loss: 0.1015 | 0.0724
Epoch 75/300, seasonal_1 Loss: 0.0868 | 0.0507
Epoch 76/300, seasonal_1 Loss: 0.0651 | 0.0385
Epoch 77/300, seasonal_1 Loss: 0.0608 | 0.0451
Epoch 78/300, seasonal_1 Loss: 0.0546 | 0.0359
Epoch 79/300, seasonal_1 Loss: 0.0535 | 0.0310
Epoch 80/300, seasonal_1 Loss: 0.0552 | 0.0461
Epoch 81/300, seasonal_1 Loss: 0.0555 | 0.0375
Epoch 82/300, seasonal_1 Loss: 0.0534 | 0.0362
Epoch 83/300, seasonal_1 Loss: 0.0560 | 0.0430
Epoch 84/300, seasonal_1 Loss: 0.0515 | 0.0350
Epoch 85/300, seasonal_1 Loss: 0.0558 | 0.0462
Epoch 86/300, seasonal_1 Loss: 0.0514 | 0.0358
Epoch 87/300, seasonal_1 Loss: 0.0497 | 0.0383
Epoch 88/300, seasonal_1 Loss: 0.0505 | 0.0367
Epoch 89/300, seasonal_1 Loss: 0.0529 | 0.0455
Epoch 90/300, seasonal_1 Loss: 0.0578 | 0.0442
Epoch 91/300, seasonal_1 Loss: 0.0588 | 0.0447
Epoch 92/300, seasonal_1 Loss: 0.0516 | 0.0387
Epoch 93/300, seasonal_1 Loss: 0.0506 | 0.0405
Epoch 94/300, seasonal_1 Loss: 0.0502 | 0.0390
Epoch 95/300, seasonal_1 Loss: 0.0464 | 0.0405
Epoch 96/300, seasonal_1 Loss: 0.0473 | 0.0536
Epoch 97/300, seasonal_1 Loss: 0.0441 | 0.0351
Epoch 98/300, seasonal_1 Loss: 0.0439 | 0.0435
Epoch 99/300, seasonal_1 Loss: 0.0409 | 0.0373
Epoch 100/300, seasonal_1 Loss: 0.0411 | 0.0430
Epoch 101/300, seasonal_1 Loss: 0.0428 | 0.0387
Epoch 102/300, seasonal_1 Loss: 0.0423 | 0.0353
Epoch 103/300, seasonal_1 Loss: 0.0434 | 0.0426
Epoch 104/300, seasonal_1 Loss: 0.0465 | 0.0395
Epoch 105/300, seasonal_1 Loss: 0.0448 | 0.0384
Epoch 106/300, seasonal_1 Loss: 0.0479 | 0.0820
Epoch 107/300, seasonal_1 Loss: 0.0397 | 0.0548
Epoch 108/300, seasonal_1 Loss: 0.0360 | 0.0458
Epoch 109/300, seasonal_1 Loss: 0.0367 | 0.0416
Epoch 110/300, seasonal_1 Loss: 0.0349 | 0.0350
Epoch 111/300, seasonal_1 Loss: 0.0400 | 0.0497
Epoch 112/300, seasonal_1 Loss: 0.0453 | 0.0456
Epoch 113/300, seasonal_1 Loss: 0.0443 | 0.0474
Epoch 114/300, seasonal_1 Loss: 0.0378 | 0.0493
Epoch 115/300, seasonal_1 Loss: 0.0370 | 0.0445
Epoch 116/300, seasonal_1 Loss: 0.0368 | 0.0456
Epoch 117/300, seasonal_1 Loss: 0.0399 | 0.0471
Epoch 118/300, seasonal_1 Loss: 0.0344 | 0.0491
Epoch 119/300, seasonal_1 Loss: 0.0360 | 0.0514
Epoch 120/300, seasonal_1 Loss: 0.0351 | 0.0438
Epoch 121/300, seasonal_1 Loss: 0.0306 | 0.0357
Epoch 122/300, seasonal_1 Loss: 0.0305 | 0.0424
Epoch 123/300, seasonal_1 Loss: 0.0309 | 0.0466
Epoch 124/300, seasonal_1 Loss: 0.0318 | 0.0477
Epoch 125/300, seasonal_1 Loss: 0.0329 | 0.0425
Epoch 126/300, seasonal_1 Loss: 0.0338 | 0.0494
Epoch 127/300, seasonal_1 Loss: 0.0340 | 0.0410
Epoch 128/300, seasonal_1 Loss: 0.0327 | 0.0419
Epoch 129/300, seasonal_1 Loss: 0.0335 | 0.0742
Epoch 130/300, seasonal_1 Loss: 0.0317 | 0.0687
Epoch 131/300, seasonal_1 Loss: 0.0333 | 0.0811
Epoch 132/300, seasonal_1 Loss: 0.0310 | 0.0745
Epoch 133/300, seasonal_1 Loss: 0.0305 | 0.0622
Epoch 134/300, seasonal_1 Loss: 0.0347 | 0.0374
Epoch 135/300, seasonal_1 Loss: 0.0341 | 0.0364
Epoch 136/300, seasonal_1 Loss: 0.0323 | 0.0345
Epoch 137/300, seasonal_1 Loss: 0.0336 | 0.0506
Epoch 138/300, seasonal_1 Loss: 0.0438 | 0.0402
Epoch 139/300, seasonal_1 Loss: 0.0388 | 0.0324
Epoch 140/300, seasonal_1 Loss: 0.0382 | 0.0474
Epoch 141/300, seasonal_1 Loss: 0.0395 | 0.0461
Epoch 142/300, seasonal_1 Loss: 0.0386 | 0.0628
Epoch 143/300, seasonal_1 Loss: 0.0403 | 0.0562
Epoch 144/300, seasonal_1 Loss: 0.0367 | 0.0409
Epoch 145/300, seasonal_1 Loss: 0.0330 | 0.0387
Epoch 146/300, seasonal_1 Loss: 0.0293 | 0.0428
Epoch 147/300, seasonal_1 Loss: 0.0367 | 0.0353
Epoch 148/300, seasonal_1 Loss: 0.0363 | 0.0380
Epoch 149/300, seasonal_1 Loss: 0.0356 | 0.0369
Epoch 150/300, seasonal_1 Loss: 0.0358 | 0.0406
Epoch 151/300, seasonal_1 Loss: 0.0368 | 0.0368
Epoch 152/300, seasonal_1 Loss: 0.0362 | 0.0376
Epoch 153/300, seasonal_1 Loss: 0.0363 | 0.0396
Epoch 154/300, seasonal_1 Loss: 0.0369 | 0.0354
Epoch 155/300, seasonal_1 Loss: 0.0333 | 0.0354
Epoch 156/300, seasonal_1 Loss: 0.0303 | 0.0323
Epoch 157/300, seasonal_1 Loss: 0.0363 | 0.0471
Epoch 158/300, seasonal_1 Loss: 0.0330 | 0.0453
Epoch 159/300, seasonal_1 Loss: 0.0483 | 0.0578
Epoch 160/300, seasonal_1 Loss: 0.0607 | 0.0474
Epoch 161/300, seasonal_1 Loss: 0.0481 | 0.0472
Epoch 162/300, seasonal_1 Loss: 0.0451 | 0.0394
Epoch 163/300, seasonal_1 Loss: 0.0371 | 0.0339
Epoch 164/300, seasonal_1 Loss: 0.0418 | 0.0408
Epoch 165/300, seasonal_1 Loss: 0.0389 | 0.0376
Epoch 166/300, seasonal_1 Loss: 0.0351 | 0.0393
Epoch 167/300, seasonal_1 Loss: 0.0340 | 0.0405
Epoch 168/300, seasonal_1 Loss: 0.0388 | 0.0361
Epoch 169/300, seasonal_1 Loss: 0.0333 | 0.0410
Epoch 170/300, seasonal_1 Loss: 0.0401 | 0.0309
Epoch 171/300, seasonal_1 Loss: 0.0379 | 0.0337
Epoch 172/300, seasonal_1 Loss: 0.0313 | 0.0334
Epoch 173/300, seasonal_1 Loss: 0.0282 | 0.0340
Epoch 174/300, seasonal_1 Loss: 0.0274 | 0.0327
Epoch 175/300, seasonal_1 Loss: 0.0370 | 0.0410
Epoch 176/300, seasonal_1 Loss: 0.0274 | 0.0372
Epoch 177/300, seasonal_1 Loss: 0.0272 | 0.0347
Epoch 178/300, seasonal_1 Loss: 0.0275 | 0.0395
Epoch 179/300, seasonal_1 Loss: 0.0266 | 0.0517
Epoch 180/300, seasonal_1 Loss: 0.0281 | 0.0739
Epoch 181/300, seasonal_1 Loss: 0.0272 | 0.0612
Epoch 182/300, seasonal_1 Loss: 0.0278 | 0.0402
Epoch 183/300, seasonal_1 Loss: 0.0311 | 0.0401
Epoch 184/300, seasonal_1 Loss: 0.0322 | 0.0363
Epoch 185/300, seasonal_1 Loss: 0.0322 | 0.0403
Epoch 186/300, seasonal_1 Loss: 0.0332 | 0.0314
Epoch 187/300, seasonal_1 Loss: 0.0304 | 0.0423
Epoch 188/300, seasonal_1 Loss: 0.0292 | 0.0317
Epoch 189/300, seasonal_1 Loss: 0.0287 | 0.0340
Epoch 190/300, seasonal_1 Loss: 0.0282 | 0.0347
Epoch 191/300, seasonal_1 Loss: 0.0291 | 0.0407
Epoch 192/300, seasonal_1 Loss: 0.0335 | 0.0450
Epoch 193/300, seasonal_1 Loss: 0.0318 | 0.0627
Epoch 194/300, seasonal_1 Loss: 0.0280 | 0.0365
Epoch 195/300, seasonal_1 Loss: 0.0264 | 0.0358
Epoch 196/300, seasonal_1 Loss: 0.0276 | 0.0322
Epoch 197/300, seasonal_1 Loss: 0.0273 | 0.0372
Epoch 198/300, seasonal_1 Loss: 0.0254 | 0.0340
Epoch 199/300, seasonal_1 Loss: 0.0265 | 0.0343
Epoch 200/300, seasonal_1 Loss: 0.0261 | 0.0321
Epoch 201/300, seasonal_1 Loss: 0.0269 | 0.0384
Epoch 202/300, seasonal_1 Loss: 0.0254 | 0.0329
Epoch 203/300, seasonal_1 Loss: 0.0251 | 0.0370
Epoch 204/300, seasonal_1 Loss: 0.0241 | 0.0343
Epoch 205/300, seasonal_1 Loss: 0.0253 | 0.0390
Epoch 206/300, seasonal_1 Loss: 0.0252 | 0.0316
Epoch 207/300, seasonal_1 Loss: 0.0249 | 0.0344
Epoch 208/300, seasonal_1 Loss: 0.0249 | 0.0333
Epoch 209/300, seasonal_1 Loss: 0.0258 | 0.0342
Epoch 210/300, seasonal_1 Loss: 0.0264 | 0.0334
Epoch 211/300, seasonal_1 Loss: 0.0262 | 0.0672
Epoch 212/300, seasonal_1 Loss: 0.0252 | 0.0346
Epoch 213/300, seasonal_1 Loss: 0.0249 | 0.0366
Epoch 214/300, seasonal_1 Loss: 0.0254 | 0.0328
Epoch 215/300, seasonal_1 Loss: 0.0264 | 0.0388
Epoch 216/300, seasonal_1 Loss: 0.0250 | 0.0320
Epoch 217/300, seasonal_1 Loss: 0.0244 | 0.0334
Epoch 218/300, seasonal_1 Loss: 0.0232 | 0.0344
Epoch 219/300, seasonal_1 Loss: 0.0248 | 0.0372
Epoch 220/300, seasonal_1 Loss: 0.0240 | 0.0350
Epoch 221/300, seasonal_1 Loss: 0.0230 | 0.0373
Epoch 222/300, seasonal_1 Loss: 0.0230 | 0.0340
Epoch 223/300, seasonal_1 Loss: 0.0228 | 0.0304
Epoch 224/300, seasonal_1 Loss: 0.0230 | 0.0302
Epoch 225/300, seasonal_1 Loss: 0.0227 | 0.0308
Epoch 226/300, seasonal_1 Loss: 0.0219 | 0.0321
Epoch 227/300, seasonal_1 Loss: 0.0243 | 0.0329
Epoch 228/300, seasonal_1 Loss: 0.0236 | 0.0327
Epoch 229/300, seasonal_1 Loss: 0.0242 | 0.0295
Epoch 230/300, seasonal_1 Loss: 0.0298 | 0.0385
Epoch 231/300, seasonal_1 Loss: 0.0363 | 0.0283
Epoch 232/300, seasonal_1 Loss: 0.0353 | 0.0418
Epoch 233/300, seasonal_1 Loss: 0.0348 | 0.0443
Epoch 234/300, seasonal_1 Loss: 0.0322 | 0.0510
Epoch 235/300, seasonal_1 Loss: 0.0274 | 0.0679
Epoch 236/300, seasonal_1 Loss: 0.0280 | 0.0396
Epoch 237/300, seasonal_1 Loss: 0.0272 | 0.0589
Epoch 238/300, seasonal_1 Loss: 0.0289 | 0.0411
Epoch 239/300, seasonal_1 Loss: 0.0271 | 0.0351
Epoch 240/300, seasonal_1 Loss: 0.0269 | 0.0335
Epoch 241/300, seasonal_1 Loss: 0.0252 | 0.0305
Epoch 242/300, seasonal_1 Loss: 0.0246 | 0.0334
Epoch 243/300, seasonal_1 Loss: 0.0247 | 0.0310
Epoch 244/300, seasonal_1 Loss: 0.0237 | 0.0323
Epoch 245/300, seasonal_1 Loss: 0.0236 | 0.0415
Epoch 246/300, seasonal_1 Loss: 0.0233 | 0.0565
Epoch 247/300, seasonal_1 Loss: 0.0261 | 0.0577
Epoch 248/300, seasonal_1 Loss: 0.0251 | 0.0332
Epoch 249/300, seasonal_1 Loss: 0.0241 | 0.0307
Epoch 250/300, seasonal_1 Loss: 0.0242 | 0.0320
Epoch 251/300, seasonal_1 Loss: 0.0233 | 0.0323
Epoch 252/300, seasonal_1 Loss: 0.0226 | 0.0297
Epoch 253/300, seasonal_1 Loss: 0.0220 | 0.0293
Epoch 254/300, seasonal_1 Loss: 0.0233 | 0.0308
Epoch 255/300, seasonal_1 Loss: 0.0212 | 0.0332
Epoch 256/300, seasonal_1 Loss: 0.0205 | 0.0315
Epoch 257/300, seasonal_1 Loss: 0.0204 | 0.0353
Epoch 258/300, seasonal_1 Loss: 0.0209 | 0.0347
Epoch 259/300, seasonal_1 Loss: 0.0207 | 0.0334
Epoch 260/300, seasonal_1 Loss: 0.0208 | 0.0337
Epoch 261/300, seasonal_1 Loss: 0.0213 | 0.0353
Epoch 262/300, seasonal_1 Loss: 0.0212 | 0.0358
Epoch 263/300, seasonal_1 Loss: 0.0223 | 0.0321
Epoch 264/300, seasonal_1 Loss: 0.0228 | 0.0405
Epoch 265/300, seasonal_1 Loss: 0.0389 | 0.0470
Epoch 266/300, seasonal_1 Loss: 0.0431 | 0.0293
Epoch 267/300, seasonal_1 Loss: 0.0356 | 0.0573
Epoch 268/300, seasonal_1 Loss: 0.0385 | 0.0295
Epoch 269/300, seasonal_1 Loss: 0.0359 | 0.0348
Epoch 270/300, seasonal_1 Loss: 0.0308 | 0.0466
Epoch 271/300, seasonal_1 Loss: 0.0463 | 0.0625
Epoch 272/300, seasonal_1 Loss: 0.0453 | 0.0314
Epoch 273/300, seasonal_1 Loss: 0.0382 | 0.0374
Epoch 274/300, seasonal_1 Loss: 0.0359 | 0.0316
Epoch 275/300, seasonal_1 Loss: 0.0335 | 0.0324
Epoch 276/300, seasonal_1 Loss: 0.0323 | 0.0335
Epoch 277/300, seasonal_1 Loss: 0.0316 | 0.0343
Epoch 278/300, seasonal_1 Loss: 0.0305 | 0.0427
Epoch 279/300, seasonal_1 Loss: 0.0341 | 0.0432
Epoch 280/300, seasonal_1 Loss: 0.0384 | 0.0429
Epoch 281/300, seasonal_1 Loss: 0.0370 | 0.0345
Epoch 282/300, seasonal_1 Loss: 0.0352 | 0.0349
Epoch 283/300, seasonal_1 Loss: 0.0346 | 0.0315
Epoch 284/300, seasonal_1 Loss: 0.0328 | 0.0305
Epoch 285/300, seasonal_1 Loss: 0.0305 | 0.0334
Epoch 286/300, seasonal_1 Loss: 0.0343 | 0.0322
Epoch 287/300, seasonal_1 Loss: 0.0330 | 0.0301
Epoch 288/300, seasonal_1 Loss: 0.0286 | 0.0283
Epoch 289/300, seasonal_1 Loss: 0.0235 | 0.0271
Epoch 290/300, seasonal_1 Loss: 0.0249 | 0.0494
Epoch 291/300, seasonal_1 Loss: 0.0393 | 0.0421
Epoch 292/300, seasonal_1 Loss: 0.0336 | 0.0329
Epoch 293/300, seasonal_1 Loss: 0.0316 | 0.0324
Epoch 294/300, seasonal_1 Loss: 0.0217 | 0.0326
Epoch 295/300, seasonal_1 Loss: 0.0343 | 0.0773
Epoch 296/300, seasonal_1 Loss: 0.0477 | 0.0328
Epoch 297/300, seasonal_1 Loss: 0.0384 | 0.0340
Epoch 298/300, seasonal_1 Loss: 0.0339 | 0.0272
Epoch 299/300, seasonal_1 Loss: 0.0321 | 0.0302
Epoch 300/300, seasonal_1 Loss: 0.0236 | 0.0439
Training seasonal_2 component with params: {'observation_period_num': 7, 'train_rates': 0.8447493378649096, 'learning_rate': 0.000497139546817353, 'batch_size': 247, 'step_size': 9, 'gamma': 0.7994409629029275}
Epoch 1/300, seasonal_2 Loss: 3.2022 | 0.4498
Epoch 2/300, seasonal_2 Loss: 0.4646 | 0.3514
Epoch 3/300, seasonal_2 Loss: 0.3908 | 0.2027
Epoch 4/300, seasonal_2 Loss: 0.3562 | 0.1537
Epoch 5/300, seasonal_2 Loss: 0.3592 | 0.2919
Epoch 6/300, seasonal_2 Loss: 0.5952 | 0.2699
Epoch 7/300, seasonal_2 Loss: 0.4045 | 0.2120
Epoch 8/300, seasonal_2 Loss: 0.2667 | 0.2132
Epoch 9/300, seasonal_2 Loss: 0.2716 | 0.1424
Epoch 10/300, seasonal_2 Loss: 0.2312 | 0.1131
Epoch 11/300, seasonal_2 Loss: 0.2469 | 0.1355
Epoch 12/300, seasonal_2 Loss: 0.2096 | 0.1452
Epoch 13/300, seasonal_2 Loss: 0.2340 | 0.2132
Epoch 14/300, seasonal_2 Loss: 0.2779 | 0.1664
Epoch 15/300, seasonal_2 Loss: 0.2577 | 0.1849
Epoch 16/300, seasonal_2 Loss: 0.1729 | 0.1339
Epoch 17/300, seasonal_2 Loss: 0.1372 | 0.0814
Epoch 18/300, seasonal_2 Loss: 0.1546 | 0.0894
Epoch 19/300, seasonal_2 Loss: 0.1553 | 0.1155
Epoch 20/300, seasonal_2 Loss: 0.1286 | 0.0650
Epoch 21/300, seasonal_2 Loss: 0.1301 | 0.1045
Epoch 22/300, seasonal_2 Loss: 0.1254 | 0.0873
Epoch 23/300, seasonal_2 Loss: 0.1171 | 0.0641
Epoch 24/300, seasonal_2 Loss: 0.1172 | 0.0673
Epoch 25/300, seasonal_2 Loss: 0.1182 | 0.0702
Epoch 26/300, seasonal_2 Loss: 0.1132 | 0.0811
Epoch 27/300, seasonal_2 Loss: 0.1053 | 0.0527
Epoch 28/300, seasonal_2 Loss: 0.0994 | 0.0627
Epoch 29/300, seasonal_2 Loss: 0.0989 | 0.0595
Epoch 30/300, seasonal_2 Loss: 0.1008 | 0.0643
Epoch 31/300, seasonal_2 Loss: 0.0973 | 0.0533
Epoch 32/300, seasonal_2 Loss: 0.0971 | 0.0584
Epoch 33/300, seasonal_2 Loss: 0.0950 | 0.0506
Epoch 34/300, seasonal_2 Loss: 0.0945 | 0.0586
Epoch 35/300, seasonal_2 Loss: 0.0928 | 0.0501
Epoch 36/300, seasonal_2 Loss: 0.0934 | 0.0599
Epoch 37/300, seasonal_2 Loss: 0.0928 | 0.0479
Epoch 38/300, seasonal_2 Loss: 0.0903 | 0.0557
Epoch 39/300, seasonal_2 Loss: 0.0920 | 0.0499
Epoch 40/300, seasonal_2 Loss: 0.0874 | 0.0524
Epoch 41/300, seasonal_2 Loss: 0.0869 | 0.0481
Epoch 42/300, seasonal_2 Loss: 0.0852 | 0.0504
Epoch 43/300, seasonal_2 Loss: 0.0845 | 0.0471
Epoch 44/300, seasonal_2 Loss: 0.0837 | 0.0484
Epoch 45/300, seasonal_2 Loss: 0.0831 | 0.0473
Epoch 46/300, seasonal_2 Loss: 0.0826 | 0.0467
Epoch 47/300, seasonal_2 Loss: 0.0821 | 0.0465
Epoch 48/300, seasonal_2 Loss: 0.0816 | 0.0462
Epoch 49/300, seasonal_2 Loss: 0.0813 | 0.0457
Epoch 50/300, seasonal_2 Loss: 0.0809 | 0.0457
Epoch 51/300, seasonal_2 Loss: 0.0805 | 0.0451
Epoch 52/300, seasonal_2 Loss: 0.0803 | 0.0451
Epoch 53/300, seasonal_2 Loss: 0.0800 | 0.0448
Epoch 54/300, seasonal_2 Loss: 0.0798 | 0.0445
Epoch 55/300, seasonal_2 Loss: 0.0795 | 0.0443
Epoch 56/300, seasonal_2 Loss: 0.0792 | 0.0444
Epoch 57/300, seasonal_2 Loss: 0.0790 | 0.0440
Epoch 58/300, seasonal_2 Loss: 0.0788 | 0.0439
Epoch 59/300, seasonal_2 Loss: 0.0786 | 0.0436
Epoch 60/300, seasonal_2 Loss: 0.0784 | 0.0439
Epoch 61/300, seasonal_2 Loss: 0.0782 | 0.0435
Epoch 62/300, seasonal_2 Loss: 0.0780 | 0.0434
Epoch 63/300, seasonal_2 Loss: 0.0779 | 0.0432
Epoch 64/300, seasonal_2 Loss: 0.0777 | 0.0435
Epoch 65/300, seasonal_2 Loss: 0.0776 | 0.0430
Epoch 66/300, seasonal_2 Loss: 0.0774 | 0.0429
Epoch 67/300, seasonal_2 Loss: 0.0773 | 0.0430
Epoch 68/300, seasonal_2 Loss: 0.0772 | 0.0430
Epoch 69/300, seasonal_2 Loss: 0.0771 | 0.0427
Epoch 70/300, seasonal_2 Loss: 0.0770 | 0.0427
Epoch 71/300, seasonal_2 Loss: 0.0769 | 0.0428
Epoch 72/300, seasonal_2 Loss: 0.0768 | 0.0426
Epoch 73/300, seasonal_2 Loss: 0.0767 | 0.0425
Epoch 74/300, seasonal_2 Loss: 0.0766 | 0.0425
Epoch 75/300, seasonal_2 Loss: 0.0765 | 0.0425
Epoch 76/300, seasonal_2 Loss: 0.0764 | 0.0424
Epoch 77/300, seasonal_2 Loss: 0.0763 | 0.0424
Epoch 78/300, seasonal_2 Loss: 0.0763 | 0.0423
Epoch 79/300, seasonal_2 Loss: 0.0762 | 0.0423
Epoch 80/300, seasonal_2 Loss: 0.0762 | 0.0422
Epoch 81/300, seasonal_2 Loss: 0.0761 | 0.0422
Epoch 82/300, seasonal_2 Loss: 0.0760 | 0.0422
Epoch 83/300, seasonal_2 Loss: 0.0760 | 0.0421
Epoch 84/300, seasonal_2 Loss: 0.0759 | 0.0421
Epoch 85/300, seasonal_2 Loss: 0.0759 | 0.0421
Epoch 86/300, seasonal_2 Loss: 0.0758 | 0.0420
Epoch 87/300, seasonal_2 Loss: 0.0758 | 0.0420
Epoch 88/300, seasonal_2 Loss: 0.0757 | 0.0420
Epoch 89/300, seasonal_2 Loss: 0.0757 | 0.0419
Epoch 90/300, seasonal_2 Loss: 0.0757 | 0.0419
Epoch 91/300, seasonal_2 Loss: 0.0756 | 0.0419
Epoch 92/300, seasonal_2 Loss: 0.0756 | 0.0419
Epoch 93/300, seasonal_2 Loss: 0.0756 | 0.0419
Epoch 94/300, seasonal_2 Loss: 0.0755 | 0.0418
Epoch 95/300, seasonal_2 Loss: 0.0755 | 0.0418
Epoch 96/300, seasonal_2 Loss: 0.0755 | 0.0418
Epoch 97/300, seasonal_2 Loss: 0.0754 | 0.0418
Epoch 98/300, seasonal_2 Loss: 0.0754 | 0.0418
Epoch 99/300, seasonal_2 Loss: 0.0754 | 0.0417
Epoch 100/300, seasonal_2 Loss: 0.0753 | 0.0417
Epoch 101/300, seasonal_2 Loss: 0.0753 | 0.0417
Epoch 102/300, seasonal_2 Loss: 0.0753 | 0.0417
Epoch 103/300, seasonal_2 Loss: 0.0753 | 0.0417
Epoch 104/300, seasonal_2 Loss: 0.0753 | 0.0417
Epoch 105/300, seasonal_2 Loss: 0.0752 | 0.0417
Epoch 106/300, seasonal_2 Loss: 0.0752 | 0.0416
Epoch 107/300, seasonal_2 Loss: 0.0752 | 0.0416
Epoch 108/300, seasonal_2 Loss: 0.0752 | 0.0416
Epoch 109/300, seasonal_2 Loss: 0.0752 | 0.0416
Epoch 110/300, seasonal_2 Loss: 0.0751 | 0.0416
Epoch 111/300, seasonal_2 Loss: 0.0751 | 0.0416
Epoch 112/300, seasonal_2 Loss: 0.0751 | 0.0416
Epoch 113/300, seasonal_2 Loss: 0.0751 | 0.0416
Epoch 114/300, seasonal_2 Loss: 0.0751 | 0.0416
Epoch 115/300, seasonal_2 Loss: 0.0751 | 0.0416
Epoch 116/300, seasonal_2 Loss: 0.0751 | 0.0415
Epoch 117/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 118/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 119/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 120/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 121/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 122/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 123/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 124/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 125/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 126/300, seasonal_2 Loss: 0.0750 | 0.0415
Epoch 127/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 128/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 129/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 130/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 131/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 132/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 133/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 134/300, seasonal_2 Loss: 0.0749 | 0.0415
Epoch 135/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 136/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 137/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 138/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 139/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 140/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 141/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 142/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 143/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 144/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 145/300, seasonal_2 Loss: 0.0749 | 0.0414
Epoch 146/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 147/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 148/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 149/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 150/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 151/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 152/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 153/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 154/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 155/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 156/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 157/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 158/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 159/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 160/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 161/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 162/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 163/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 164/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 165/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 166/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 167/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 168/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 169/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 170/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 171/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 172/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 173/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 174/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 175/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 176/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 177/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 178/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 179/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 180/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 181/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 182/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 183/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 184/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 185/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 186/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 187/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 188/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 189/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 190/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 191/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 192/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 193/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 194/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 195/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 196/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 197/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 198/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 199/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 200/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 201/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 202/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 203/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 204/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 205/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 206/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 207/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 208/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 209/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 210/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 211/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 212/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 213/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 214/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 215/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 216/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 217/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 218/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 219/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 220/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 221/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 222/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 223/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 224/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 225/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 226/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 227/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 228/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 229/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 230/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 231/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 232/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 233/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 234/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 235/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 236/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 237/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 238/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 239/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 240/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 241/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 242/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 243/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 244/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 245/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 246/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 247/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 248/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 249/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 250/300, seasonal_2 Loss: 0.0748 | 0.0414
Epoch 251/300, seasonal_2 Loss: 0.0748 | 0.0414
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 18, 'train_rates': 0.9412667183421158, 'learning_rate': 0.00014991764928175684, 'batch_size': 169, 'step_size': 11, 'gamma': 0.8518307755459404}
Epoch 1/300, seasonal_3 Loss: 0.9389 | 0.1772
Epoch 2/300, seasonal_3 Loss: 0.2223 | 0.1573
Epoch 3/300, seasonal_3 Loss: 0.2342 | 0.1400
Epoch 4/300, seasonal_3 Loss: 0.2123 | 0.1350
Epoch 5/300, seasonal_3 Loss: 0.2917 | 0.1248
Epoch 6/300, seasonal_3 Loss: 0.3450 | 0.4770
Epoch 7/300, seasonal_3 Loss: 0.3366 | 0.3591
Epoch 8/300, seasonal_3 Loss: 0.1900 | 0.1166
Epoch 9/300, seasonal_3 Loss: 0.1919 | 0.1011
Epoch 10/300, seasonal_3 Loss: 0.1953 | 0.0942
Epoch 11/300, seasonal_3 Loss: 0.2134 | 0.1081
Epoch 12/300, seasonal_3 Loss: 0.2800 | 0.1855
Epoch 13/300, seasonal_3 Loss: 0.3792 | 0.1879
Epoch 14/300, seasonal_3 Loss: 0.3397 | 0.1377
Epoch 15/300, seasonal_3 Loss: 0.2815 | 0.1327
Epoch 16/300, seasonal_3 Loss: 0.1625 | 0.0914
Epoch 17/300, seasonal_3 Loss: 0.2076 | 0.1081
Epoch 18/300, seasonal_3 Loss: 0.2023 | 0.2812
Epoch 19/300, seasonal_3 Loss: 0.1760 | 0.0917
Epoch 20/300, seasonal_3 Loss: 0.1375 | 0.0909
Epoch 21/300, seasonal_3 Loss: 0.1546 | 0.0711
Epoch 22/300, seasonal_3 Loss: 0.1204 | 0.0651
Epoch 23/300, seasonal_3 Loss: 0.1265 | 0.0843
Epoch 24/300, seasonal_3 Loss: 0.1203 | 0.1067
Epoch 25/300, seasonal_3 Loss: 0.1052 | 0.0771
Epoch 26/300, seasonal_3 Loss: 0.1062 | 0.0611
Epoch 27/300, seasonal_3 Loss: 0.1014 | 0.0659
Epoch 28/300, seasonal_3 Loss: 0.1009 | 0.0773
Epoch 29/300, seasonal_3 Loss: 0.1015 | 0.1036
Epoch 30/300, seasonal_3 Loss: 0.1075 | 0.0571
Epoch 31/300, seasonal_3 Loss: 0.1019 | 0.0605
Epoch 32/300, seasonal_3 Loss: 0.0945 | 0.0649
Epoch 33/300, seasonal_3 Loss: 0.0941 | 0.0665
Epoch 34/300, seasonal_3 Loss: 0.0907 | 0.0706
Epoch 35/300, seasonal_3 Loss: 0.0926 | 0.0507
Epoch 36/300, seasonal_3 Loss: 0.0871 | 0.0560
Epoch 37/300, seasonal_3 Loss: 0.0895 | 0.0758
Epoch 38/300, seasonal_3 Loss: 0.0916 | 0.0550
Epoch 39/300, seasonal_3 Loss: 0.0863 | 0.0531
Epoch 40/300, seasonal_3 Loss: 0.0898 | 0.0527
Epoch 41/300, seasonal_3 Loss: 0.0810 | 0.0509
Epoch 42/300, seasonal_3 Loss: 0.0843 | 0.0600
Epoch 43/300, seasonal_3 Loss: 0.0816 | 0.0493
Epoch 44/300, seasonal_3 Loss: 0.0866 | 0.0471
Epoch 45/300, seasonal_3 Loss: 0.0848 | 0.0526
Epoch 46/300, seasonal_3 Loss: 0.0782 | 0.0525
Epoch 47/300, seasonal_3 Loss: 0.0787 | 0.0475
Epoch 48/300, seasonal_3 Loss: 0.0752 | 0.0450
Epoch 49/300, seasonal_3 Loss: 0.0757 | 0.0464
Epoch 50/300, seasonal_3 Loss: 0.0743 | 0.0451
Epoch 51/300, seasonal_3 Loss: 0.0742 | 0.0482
Epoch 52/300, seasonal_3 Loss: 0.0733 | 0.0437
Epoch 53/300, seasonal_3 Loss: 0.0726 | 0.0414
Epoch 54/300, seasonal_3 Loss: 0.0722 | 0.0455
Epoch 55/300, seasonal_3 Loss: 0.0713 | 0.0434
Epoch 56/300, seasonal_3 Loss: 0.0710 | 0.0418
Epoch 57/300, seasonal_3 Loss: 0.0701 | 0.0438
Epoch 58/300, seasonal_3 Loss: 0.0703 | 0.0400
Epoch 59/300, seasonal_3 Loss: 0.0693 | 0.0417
Epoch 60/300, seasonal_3 Loss: 0.0692 | 0.0430
Epoch 61/300, seasonal_3 Loss: 0.0694 | 0.0398
Epoch 62/300, seasonal_3 Loss: 0.0687 | 0.0427
Epoch 63/300, seasonal_3 Loss: 0.0689 | 0.0389
Epoch 64/300, seasonal_3 Loss: 0.0682 | 0.0406
Epoch 65/300, seasonal_3 Loss: 0.0681 | 0.0426
Epoch 66/300, seasonal_3 Loss: 0.0678 | 0.0385
Epoch 67/300, seasonal_3 Loss: 0.0676 | 0.0410
Epoch 68/300, seasonal_3 Loss: 0.0683 | 0.0375
Epoch 69/300, seasonal_3 Loss: 0.0669 | 0.0389
Epoch 70/300, seasonal_3 Loss: 0.0672 | 0.0418
Epoch 71/300, seasonal_3 Loss: 0.0676 | 0.0383
Epoch 72/300, seasonal_3 Loss: 0.0662 | 0.0392
Epoch 73/300, seasonal_3 Loss: 0.0676 | 0.0364
Epoch 74/300, seasonal_3 Loss: 0.0658 | 0.0385
Epoch 75/300, seasonal_3 Loss: 0.0662 | 0.0391
Epoch 76/300, seasonal_3 Loss: 0.0655 | 0.0364
Epoch 77/300, seasonal_3 Loss: 0.0655 | 0.0376
Epoch 78/300, seasonal_3 Loss: 0.0654 | 0.0365
Epoch 79/300, seasonal_3 Loss: 0.0648 | 0.0385
Epoch 80/300, seasonal_3 Loss: 0.0649 | 0.0361
Epoch 81/300, seasonal_3 Loss: 0.0640 | 0.0363
Epoch 82/300, seasonal_3 Loss: 0.0642 | 0.0362
Epoch 83/300, seasonal_3 Loss: 0.0637 | 0.0371
Epoch 84/300, seasonal_3 Loss: 0.0640 | 0.0363
Epoch 85/300, seasonal_3 Loss: 0.0632 | 0.0355
Epoch 86/300, seasonal_3 Loss: 0.0634 | 0.0360
Epoch 87/300, seasonal_3 Loss: 0.0629 | 0.0360
Epoch 88/300, seasonal_3 Loss: 0.0630 | 0.0363
Epoch 89/300, seasonal_3 Loss: 0.0626 | 0.0351
Epoch 90/300, seasonal_3 Loss: 0.0626 | 0.0354
Epoch 91/300, seasonal_3 Loss: 0.0623 | 0.0354
Epoch 92/300, seasonal_3 Loss: 0.0622 | 0.0355
Epoch 93/300, seasonal_3 Loss: 0.0620 | 0.0349
Epoch 94/300, seasonal_3 Loss: 0.0619 | 0.0350
Epoch 95/300, seasonal_3 Loss: 0.0618 | 0.0350
Epoch 96/300, seasonal_3 Loss: 0.0616 | 0.0351
Epoch 97/300, seasonal_3 Loss: 0.0615 | 0.0347
Epoch 98/300, seasonal_3 Loss: 0.0613 | 0.0347
Epoch 99/300, seasonal_3 Loss: 0.0613 | 0.0346
Epoch 100/300, seasonal_3 Loss: 0.0611 | 0.0347
Epoch 101/300, seasonal_3 Loss: 0.0611 | 0.0345
Epoch 102/300, seasonal_3 Loss: 0.0609 | 0.0345
Epoch 103/300, seasonal_3 Loss: 0.0608 | 0.0344
Epoch 104/300, seasonal_3 Loss: 0.0607 | 0.0344
Epoch 105/300, seasonal_3 Loss: 0.0606 | 0.0343
Epoch 106/300, seasonal_3 Loss: 0.0605 | 0.0342
Epoch 107/300, seasonal_3 Loss: 0.0604 | 0.0342
Epoch 108/300, seasonal_3 Loss: 0.0604 | 0.0342
Epoch 109/300, seasonal_3 Loss: 0.0603 | 0.0341
Epoch 110/300, seasonal_3 Loss: 0.0602 | 0.0341
Epoch 111/300, seasonal_3 Loss: 0.0601 | 0.0341
Epoch 112/300, seasonal_3 Loss: 0.0600 | 0.0340
Epoch 113/300, seasonal_3 Loss: 0.0600 | 0.0340
Epoch 114/300, seasonal_3 Loss: 0.0599 | 0.0339
Epoch 115/300, seasonal_3 Loss: 0.0598 | 0.0339
Epoch 116/300, seasonal_3 Loss: 0.0598 | 0.0339
Epoch 117/300, seasonal_3 Loss: 0.0597 | 0.0339
Epoch 118/300, seasonal_3 Loss: 0.0596 | 0.0338
Epoch 119/300, seasonal_3 Loss: 0.0596 | 0.0338
Epoch 120/300, seasonal_3 Loss: 0.0595 | 0.0337
Epoch 121/300, seasonal_3 Loss: 0.0594 | 0.0337
Epoch 122/300, seasonal_3 Loss: 0.0594 | 0.0337
Epoch 123/300, seasonal_3 Loss: 0.0593 | 0.0337
Epoch 124/300, seasonal_3 Loss: 0.0593 | 0.0337
Epoch 125/300, seasonal_3 Loss: 0.0592 | 0.0337
Epoch 126/300, seasonal_3 Loss: 0.0592 | 0.0336
Epoch 127/300, seasonal_3 Loss: 0.0591 | 0.0336
Epoch 128/300, seasonal_3 Loss: 0.0591 | 0.0336
Epoch 129/300, seasonal_3 Loss: 0.0590 | 0.0336
Epoch 130/300, seasonal_3 Loss: 0.0590 | 0.0336
Epoch 131/300, seasonal_3 Loss: 0.0589 | 0.0336
Epoch 132/300, seasonal_3 Loss: 0.0589 | 0.0335
Epoch 133/300, seasonal_3 Loss: 0.0588 | 0.0335
Epoch 134/300, seasonal_3 Loss: 0.0588 | 0.0335
Epoch 135/300, seasonal_3 Loss: 0.0587 | 0.0335
Epoch 136/300, seasonal_3 Loss: 0.0587 | 0.0335
Epoch 137/300, seasonal_3 Loss: 0.0587 | 0.0335
Epoch 138/300, seasonal_3 Loss: 0.0586 | 0.0335
Epoch 139/300, seasonal_3 Loss: 0.0586 | 0.0335
Epoch 140/300, seasonal_3 Loss: 0.0585 | 0.0334
Epoch 141/300, seasonal_3 Loss: 0.0585 | 0.0334
Epoch 142/300, seasonal_3 Loss: 0.0585 | 0.0334
Epoch 143/300, seasonal_3 Loss: 0.0584 | 0.0334
Epoch 144/300, seasonal_3 Loss: 0.0584 | 0.0334
Epoch 145/300, seasonal_3 Loss: 0.0584 | 0.0334
Epoch 146/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 147/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 148/300, seasonal_3 Loss: 0.0583 | 0.0334
Epoch 149/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 150/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 151/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 152/300, seasonal_3 Loss: 0.0582 | 0.0333
Epoch 153/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 154/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 155/300, seasonal_3 Loss: 0.0581 | 0.0333
Epoch 156/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 157/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 158/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 159/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 160/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 161/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 162/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 163/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 164/300, seasonal_3 Loss: 0.0579 | 0.0333
Epoch 165/300, seasonal_3 Loss: 0.0578 | 0.0333
Epoch 166/300, seasonal_3 Loss: 0.0578 | 0.0333
Epoch 167/300, seasonal_3 Loss: 0.0578 | 0.0333
Epoch 168/300, seasonal_3 Loss: 0.0578 | 0.0333
Epoch 169/300, seasonal_3 Loss: 0.0578 | 0.0332
Epoch 170/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 171/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 172/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 173/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 174/300, seasonal_3 Loss: 0.0577 | 0.0332
Epoch 175/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 176/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 177/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 178/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 179/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 180/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 181/300, seasonal_3 Loss: 0.0576 | 0.0332
Epoch 182/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 183/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 184/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 185/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 186/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 187/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 188/300, seasonal_3 Loss: 0.0575 | 0.0332
Epoch 189/300, seasonal_3 Loss: 0.0574 | 0.0332
Epoch 190/300, seasonal_3 Loss: 0.0574 | 0.0332
Epoch 191/300, seasonal_3 Loss: 0.0574 | 0.0332
Epoch 192/300, seasonal_3 Loss: 0.0574 | 0.0332
Epoch 193/300, seasonal_3 Loss: 0.0574 | 0.0332
Epoch 194/300, seasonal_3 Loss: 0.0574 | 0.0331
Epoch 195/300, seasonal_3 Loss: 0.0574 | 0.0331
Epoch 196/300, seasonal_3 Loss: 0.0574 | 0.0331
Epoch 197/300, seasonal_3 Loss: 0.0574 | 0.0331
Epoch 198/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 199/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 200/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 201/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 202/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 203/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 204/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 205/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 206/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 207/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 208/300, seasonal_3 Loss: 0.0573 | 0.0331
Epoch 209/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 210/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 211/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 212/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 213/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 214/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 215/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 216/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 217/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 218/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 219/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 220/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 221/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 222/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 223/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 224/300, seasonal_3 Loss: 0.0572 | 0.0331
Epoch 225/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 226/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 227/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 228/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 229/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 230/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 231/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 232/300, seasonal_3 Loss: 0.0571 | 0.0331
Epoch 233/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 234/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 235/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 236/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 237/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 238/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 239/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 240/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 241/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 242/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 243/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 244/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 245/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 246/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 247/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 248/300, seasonal_3 Loss: 0.0571 | 0.0330
Epoch 249/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 250/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 251/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 252/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 253/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 254/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 255/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 256/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 257/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 258/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 259/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 260/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 261/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 262/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 263/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 264/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 265/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 266/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 267/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 268/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 269/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 270/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 271/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 272/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 273/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 274/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 275/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 276/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 277/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 278/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 279/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 280/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 281/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 282/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 283/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 284/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 285/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 286/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 287/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 288/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 289/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 290/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 291/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 292/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 293/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 294/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 295/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 296/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 297/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 298/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 299/300, seasonal_3 Loss: 0.0570 | 0.0330
Epoch 300/300, seasonal_3 Loss: 0.0570 | 0.0330
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.9444331851010117, 'learning_rate': 5.577912666428857e-05, 'batch_size': 125, 'step_size': 6, 'gamma': 0.7865136050415347}
Epoch 1/300, resid Loss: 0.5063 | 0.1364
Epoch 2/300, resid Loss: 0.1538 | 0.0994
Epoch 3/300, resid Loss: 0.1514 | 0.0990
Epoch 4/300, resid Loss: 0.1245 | 0.0808
Epoch 5/300, resid Loss: 0.1321 | 0.0744
Epoch 6/300, resid Loss: 0.1792 | 0.0949
Epoch 7/300, resid Loss: 0.2034 | 0.0834
Epoch 8/300, resid Loss: 0.2093 | 0.1365
Epoch 9/300, resid Loss: 0.1961 | 0.1123
Epoch 10/300, resid Loss: 0.1551 | 0.0817
Epoch 11/300, resid Loss: 0.1329 | 0.0768
Epoch 12/300, resid Loss: 0.2801 | 0.1747
Epoch 13/300, resid Loss: 0.1596 | 0.0767
Epoch 14/300, resid Loss: 0.1351 | 0.1478
Epoch 15/300, resid Loss: 0.1177 | 0.0843
Epoch 16/300, resid Loss: 0.1150 | 0.0680
Epoch 17/300, resid Loss: 0.1101 | 0.0677
Epoch 18/300, resid Loss: 0.1260 | 0.0841
Epoch 19/300, resid Loss: 0.1118 | 0.0658
Epoch 20/300, resid Loss: 0.1044 | 0.0746
Epoch 21/300, resid Loss: 0.1051 | 0.0744
Epoch 22/300, resid Loss: 0.0949 | 0.0611
Epoch 23/300, resid Loss: 0.0945 | 0.0605
Epoch 24/300, resid Loss: 0.0921 | 0.0597
Epoch 25/300, resid Loss: 0.0911 | 0.0594
Epoch 26/300, resid Loss: 0.0904 | 0.0588
Epoch 27/300, resid Loss: 0.0898 | 0.0584
Epoch 28/300, resid Loss: 0.0893 | 0.0581
Epoch 29/300, resid Loss: 0.0889 | 0.0577
Epoch 30/300, resid Loss: 0.0885 | 0.0574
Epoch 31/300, resid Loss: 0.0881 | 0.0571
Epoch 32/300, resid Loss: 0.0878 | 0.0568
Epoch 33/300, resid Loss: 0.0875 | 0.0565
Epoch 34/300, resid Loss: 0.0872 | 0.0563
Epoch 35/300, resid Loss: 0.0870 | 0.0560
Epoch 36/300, resid Loss: 0.0867 | 0.0558
Epoch 37/300, resid Loss: 0.0865 | 0.0556
Epoch 38/300, resid Loss: 0.0863 | 0.0554
Epoch 39/300, resid Loss: 0.0861 | 0.0552
Epoch 40/300, resid Loss: 0.0859 | 0.0550
Epoch 41/300, resid Loss: 0.0858 | 0.0548
Epoch 42/300, resid Loss: 0.0856 | 0.0547
Epoch 43/300, resid Loss: 0.0855 | 0.0545
Epoch 44/300, resid Loss: 0.0853 | 0.0544
Epoch 45/300, resid Loss: 0.0852 | 0.0543
Epoch 46/300, resid Loss: 0.0851 | 0.0541
Epoch 47/300, resid Loss: 0.0850 | 0.0540
Epoch 48/300, resid Loss: 0.0849 | 0.0539
Epoch 49/300, resid Loss: 0.0848 | 0.0538
Epoch 50/300, resid Loss: 0.0847 | 0.0537
Epoch 51/300, resid Loss: 0.0846 | 0.0536
Epoch 52/300, resid Loss: 0.0845 | 0.0536
Epoch 53/300, resid Loss: 0.0845 | 0.0535
Epoch 54/300, resid Loss: 0.0844 | 0.0534
Epoch 55/300, resid Loss: 0.0843 | 0.0533
Epoch 56/300, resid Loss: 0.0843 | 0.0533
Epoch 57/300, resid Loss: 0.0842 | 0.0532
Epoch 58/300, resid Loss: 0.0842 | 0.0532
Epoch 59/300, resid Loss: 0.0841 | 0.0531
Epoch 60/300, resid Loss: 0.0841 | 0.0531
Epoch 61/300, resid Loss: 0.0840 | 0.0530
Epoch 62/300, resid Loss: 0.0840 | 0.0530
Epoch 63/300, resid Loss: 0.0840 | 0.0529
Epoch 64/300, resid Loss: 0.0839 | 0.0529
Epoch 65/300, resid Loss: 0.0839 | 0.0529
Epoch 66/300, resid Loss: 0.0839 | 0.0528
Epoch 67/300, resid Loss: 0.0838 | 0.0528
Epoch 68/300, resid Loss: 0.0838 | 0.0528
Epoch 69/300, resid Loss: 0.0838 | 0.0528
Epoch 70/300, resid Loss: 0.0838 | 0.0527
Epoch 71/300, resid Loss: 0.0838 | 0.0527
Epoch 72/300, resid Loss: 0.0837 | 0.0527
Epoch 73/300, resid Loss: 0.0837 | 0.0527
Epoch 74/300, resid Loss: 0.0837 | 0.0527
Epoch 75/300, resid Loss: 0.0837 | 0.0526
Epoch 76/300, resid Loss: 0.0837 | 0.0526
Epoch 77/300, resid Loss: 0.0837 | 0.0526
Epoch 78/300, resid Loss: 0.0836 | 0.0526
Epoch 79/300, resid Loss: 0.0836 | 0.0526
Epoch 80/300, resid Loss: 0.0836 | 0.0526
Epoch 81/300, resid Loss: 0.0836 | 0.0526
Epoch 82/300, resid Loss: 0.0836 | 0.0525
Epoch 83/300, resid Loss: 0.0836 | 0.0525
Epoch 84/300, resid Loss: 0.0836 | 0.0525
Epoch 85/300, resid Loss: 0.0836 | 0.0525
Epoch 86/300, resid Loss: 0.0836 | 0.0525
Epoch 87/300, resid Loss: 0.0836 | 0.0525
Epoch 88/300, resid Loss: 0.0836 | 0.0525
Epoch 89/300, resid Loss: 0.0836 | 0.0525
Epoch 90/300, resid Loss: 0.0835 | 0.0525
Epoch 91/300, resid Loss: 0.0835 | 0.0525
Epoch 92/300, resid Loss: 0.0835 | 0.0525
Epoch 93/300, resid Loss: 0.0835 | 0.0525
Epoch 94/300, resid Loss: 0.0835 | 0.0525
Epoch 95/300, resid Loss: 0.0835 | 0.0525
Epoch 96/300, resid Loss: 0.0835 | 0.0525
Epoch 97/300, resid Loss: 0.0835 | 0.0525
Epoch 98/300, resid Loss: 0.0835 | 0.0525
Epoch 99/300, resid Loss: 0.0835 | 0.0524
Epoch 100/300, resid Loss: 0.0835 | 0.0524
Epoch 101/300, resid Loss: 0.0835 | 0.0524
Epoch 102/300, resid Loss: 0.0835 | 0.0524
Epoch 103/300, resid Loss: 0.0835 | 0.0524
Epoch 104/300, resid Loss: 0.0835 | 0.0524
Epoch 105/300, resid Loss: 0.0835 | 0.0524
Epoch 106/300, resid Loss: 0.0835 | 0.0524
Epoch 107/300, resid Loss: 0.0835 | 0.0524
Epoch 108/300, resid Loss: 0.0835 | 0.0524
Epoch 109/300, resid Loss: 0.0835 | 0.0524
Epoch 110/300, resid Loss: 0.0835 | 0.0524
Epoch 111/300, resid Loss: 0.0835 | 0.0524
Epoch 112/300, resid Loss: 0.0835 | 0.0524
Epoch 113/300, resid Loss: 0.0835 | 0.0524
Epoch 114/300, resid Loss: 0.0835 | 0.0524
Epoch 115/300, resid Loss: 0.0835 | 0.0524
Epoch 116/300, resid Loss: 0.0835 | 0.0524
Epoch 117/300, resid Loss: 0.0835 | 0.0524
Epoch 118/300, resid Loss: 0.0835 | 0.0524
Epoch 119/300, resid Loss: 0.0835 | 0.0524
Epoch 120/300, resid Loss: 0.0835 | 0.0524
Epoch 121/300, resid Loss: 0.0835 | 0.0524
Epoch 122/300, resid Loss: 0.0835 | 0.0524
Epoch 123/300, resid Loss: 0.0835 | 0.0524
Epoch 124/300, resid Loss: 0.0835 | 0.0524
Epoch 125/300, resid Loss: 0.0835 | 0.0524
Epoch 126/300, resid Loss: 0.0835 | 0.0524
Epoch 127/300, resid Loss: 0.0835 | 0.0524
Epoch 128/300, resid Loss: 0.0835 | 0.0524
Epoch 129/300, resid Loss: 0.0835 | 0.0524
Epoch 130/300, resid Loss: 0.0835 | 0.0524
Epoch 131/300, resid Loss: 0.0835 | 0.0524
Epoch 132/300, resid Loss: 0.0835 | 0.0524
Epoch 133/300, resid Loss: 0.0835 | 0.0524
Epoch 134/300, resid Loss: 0.0835 | 0.0524
Epoch 135/300, resid Loss: 0.0835 | 0.0524
Epoch 136/300, resid Loss: 0.0835 | 0.0524
Epoch 137/300, resid Loss: 0.0835 | 0.0524
Epoch 138/300, resid Loss: 0.0835 | 0.0524
Epoch 139/300, resid Loss: 0.0835 | 0.0524
Epoch 140/300, resid Loss: 0.0835 | 0.0524
Epoch 141/300, resid Loss: 0.0835 | 0.0524
Epoch 142/300, resid Loss: 0.0835 | 0.0524
Epoch 143/300, resid Loss: 0.0835 | 0.0524
Epoch 144/300, resid Loss: 0.0835 | 0.0524
Epoch 145/300, resid Loss: 0.0835 | 0.0524
Epoch 146/300, resid Loss: 0.0835 | 0.0524
Epoch 147/300, resid Loss: 0.0835 | 0.0524
Epoch 148/300, resid Loss: 0.0835 | 0.0524
Epoch 149/300, resid Loss: 0.0835 | 0.0524
Epoch 150/300, resid Loss: 0.0835 | 0.0524
Epoch 151/300, resid Loss: 0.0835 | 0.0524
Early stopping for resid
Runtime (seconds): 4451.48627448082
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:696: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[157.09900257]
[-5.39829002]
[2.81204804]
[13.43768971]
[3.51794255]
[22.49137188]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 194.05143756548682
RMSE: 13.930234655794095
MAE: 13.930234655794095
R-squared: nan
[193.95976473]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:738: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1]
