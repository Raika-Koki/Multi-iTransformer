ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-05 18:32:54,131][0m A new study created in memory with name: no-name-18845303-2fa3-4f3c-8145-51c4a7fb1117[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-05 18:33:29,239][0m Trial 0 finished with value: 0.2475061015242097 and parameters: {'observation_period_num': 151, 'train_rates': 0.8765085461074482, 'learning_rate': 1.7217926871946867e-05, 'batch_size': 192, 'step_size': 10, 'gamma': 0.7614558225958021}. Best is trial 0 with value: 0.2475061015242097.[0m
[32m[I 2025-01-05 18:34:04,759][0m Trial 1 finished with value: 0.03657986596226692 and parameters: {'observation_period_num': 9, 'train_rates': 0.9337019190212246, 'learning_rate': 0.0002292319347909306, 'batch_size': 208, 'step_size': 2, 'gamma': 0.9558940993612924}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:34:37,494][0m Trial 2 finished with value: 0.09045813232660294 and parameters: {'observation_period_num': 33, 'train_rates': 0.9498141026826481, 'learning_rate': 2.8223317868273763e-05, 'batch_size': 233, 'step_size': 4, 'gamma': 0.931013515452062}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:35:31,272][0m Trial 3 finished with value: 0.08773623134581571 and parameters: {'observation_period_num': 169, 'train_rates': 0.858728842126594, 'learning_rate': 8.970874043728893e-05, 'batch_size': 98, 'step_size': 5, 'gamma': 0.7990625646305612}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:36:12,778][0m Trial 4 finished with value: 0.0730940613782767 and parameters: {'observation_period_num': 102, 'train_rates': 0.9192179188394385, 'learning_rate': 0.0005353868344576262, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9238591200768165}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:36:53,505][0m Trial 5 finished with value: 0.13822053287023772 and parameters: {'observation_period_num': 209, 'train_rates': 0.841928548318505, 'learning_rate': 0.0005336558940128688, 'batch_size': 123, 'step_size': 14, 'gamma': 0.9329088128882613}. Best is trial 1 with value: 0.03657986596226692.[0m
Early stopping at epoch 73
[32m[I 2025-01-05 18:37:37,444][0m Trial 6 finished with value: 0.17721396869239087 and parameters: {'observation_period_num': 50, 'train_rates': 0.8082356389618995, 'learning_rate': 5.5695000034360394e-05, 'batch_size': 87, 'step_size': 1, 'gamma': 0.8455523975411748}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:39:21,935][0m Trial 7 finished with value: 0.1549877544682949 and parameters: {'observation_period_num': 193, 'train_rates': 0.8072726400489887, 'learning_rate': 0.00018924016043840236, 'batch_size': 45, 'step_size': 12, 'gamma': 0.9852442537141453}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:40:03,204][0m Trial 8 finished with value: 0.13979934442409214 and parameters: {'observation_period_num': 247, 'train_rates': 0.7712798471646367, 'learning_rate': 0.0003943449359493104, 'batch_size': 136, 'step_size': 4, 'gamma': 0.8128339057590868}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:40:32,505][0m Trial 9 finished with value: 0.13443957218160368 and parameters: {'observation_period_num': 19, 'train_rates': 0.6019287023340067, 'learning_rate': 0.0007422617825369274, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9210876434620641}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:41:15,301][0m Trial 10 finished with value: 0.3873053193092346 and parameters: {'observation_period_num': 88, 'train_rates': 0.9849323435557091, 'learning_rate': 2.2154294133878075e-06, 'batch_size': 194, 'step_size': 8, 'gamma': 0.9884985495534292}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:41:57,880][0m Trial 11 finished with value: 0.17727012925170826 and parameters: {'observation_period_num': 106, 'train_rates': 0.9266600296398244, 'learning_rate': 0.00021773126317296536, 'batch_size': 185, 'step_size': 1, 'gamma': 0.8874477439436144}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:42:35,367][0m Trial 12 finished with value: 0.35059733453014863 and parameters: {'observation_period_num': 75, 'train_rates': 0.7452453045461219, 'learning_rate': 8.293096659614549e-06, 'batch_size': 175, 'step_size': 6, 'gamma': 0.9543076256924946}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:43:21,042][0m Trial 13 finished with value: 0.05824419829206398 and parameters: {'observation_period_num': 9, 'train_rates': 0.9294062467678024, 'learning_rate': 0.00014655314804217867, 'batch_size': 161, 'step_size': 2, 'gamma': 0.8740382016215734}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:43:55,598][0m Trial 14 finished with value: 0.1855678721013682 and parameters: {'observation_period_num': 17, 'train_rates': 0.6950819700124516, 'learning_rate': 0.00014580836477440408, 'batch_size': 222, 'step_size': 2, 'gamma': 0.8783109653235961}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:44:38,221][0m Trial 15 finished with value: 0.06277901586145163 and parameters: {'observation_period_num': 60, 'train_rates': 0.8894495154301788, 'learning_rate': 7.91344503279493e-05, 'batch_size': 161, 'step_size': 7, 'gamma': 0.8461709376678821}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:45:20,523][0m Trial 16 finished with value: 0.977983832359314 and parameters: {'observation_period_num': 5, 'train_rates': 0.9754428019344928, 'learning_rate': 1.0200586341782152e-06, 'batch_size': 216, 'step_size': 3, 'gamma': 0.8949133981721489}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:45:46,893][0m Trial 17 finished with value: 0.6015136734910644 and parameters: {'observation_period_num': 45, 'train_rates': 0.686810139644629, 'learning_rate': 1.0673251813496017e-05, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8491247682104593}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:48:04,801][0m Trial 18 finished with value: 0.07285313521518962 and parameters: {'observation_period_num': 135, 'train_rates': 0.9070095816032022, 'learning_rate': 4.817038124966695e-05, 'batch_size': 42, 'step_size': 2, 'gamma': 0.9578336492112047}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:48:49,564][0m Trial 19 finished with value: 0.0688534677028656 and parameters: {'observation_period_num': 69, 'train_rates': 0.9511599510132324, 'learning_rate': 0.00026766495478103074, 'batch_size': 156, 'step_size': 6, 'gamma': 0.8182807674483696}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:49:27,617][0m Trial 20 finished with value: 0.11630365866448235 and parameters: {'observation_period_num': 120, 'train_rates': 0.8355875163853005, 'learning_rate': 0.00012145674582829472, 'batch_size': 208, 'step_size': 10, 'gamma': 0.9008109431345948}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:50:14,639][0m Trial 21 finished with value: 0.07457195058328296 and parameters: {'observation_period_num': 51, 'train_rates': 0.88730854296614, 'learning_rate': 7.615718400802707e-05, 'batch_size': 162, 'step_size': 7, 'gamma': 0.8416157887549282}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:51:22,567][0m Trial 22 finished with value: 0.040995717783566234 and parameters: {'observation_period_num': 8, 'train_rates': 0.9502039443616246, 'learning_rate': 0.0002987879537452403, 'batch_size': 103, 'step_size': 3, 'gamma': 0.8632622431471519}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:52:28,255][0m Trial 23 finished with value: 0.05489581802061626 and parameters: {'observation_period_num': 30, 'train_rates': 0.9521129650788162, 'learning_rate': 0.0003288091725504988, 'batch_size': 106, 'step_size': 2, 'gamma': 0.8616881922865288}. Best is trial 1 with value: 0.03657986596226692.[0m
[32m[I 2025-01-05 18:53:37,441][0m Trial 24 finished with value: 0.03503057733178139 and parameters: {'observation_period_num': 30, 'train_rates': 0.9847126008921929, 'learning_rate': 0.0008985147989540973, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8628426741232128}. Best is trial 24 with value: 0.03503057733178139.[0m
[32m[I 2025-01-05 18:55:15,146][0m Trial 25 finished with value: 0.032789357006549835 and parameters: {'observation_period_num': 27, 'train_rates': 0.9867385710559733, 'learning_rate': 0.0008718744085748928, 'batch_size': 65, 'step_size': 3, 'gamma': 0.8251159019291208}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 18:56:39,526][0m Trial 26 finished with value: 0.04204893857240677 and parameters: {'observation_period_num': 35, 'train_rates': 0.9724614771321565, 'learning_rate': 0.0008649544386050111, 'batch_size': 74, 'step_size': 5, 'gamma': 0.778496769506886}. Best is trial 25 with value: 0.032789357006549835.[0m
Early stopping at epoch 73
[32m[I 2025-01-05 18:57:51,460][0m Trial 27 finished with value: 0.1053522527217865 and parameters: {'observation_period_num': 82, 'train_rates': 0.9832212710262556, 'learning_rate': 0.0009796202281791423, 'batch_size': 65, 'step_size': 1, 'gamma': 0.8233694096262084}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:01:08,185][0m Trial 28 finished with value: 0.04538694606162608 and parameters: {'observation_period_num': 30, 'train_rates': 0.9069728824528812, 'learning_rate': 0.0006014340199145592, 'batch_size': 29, 'step_size': 3, 'gamma': 0.7923808905031391}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:02:35,321][0m Trial 29 finished with value: 0.10405163800245837 and parameters: {'observation_period_num': 153, 'train_rates': 0.8577109735370452, 'learning_rate': 0.00045085289029911363, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9058518047911401}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:03:28,622][0m Trial 30 finished with value: 0.07274191081523895 and parameters: {'observation_period_num': 65, 'train_rates': 0.9893101449000722, 'learning_rate': 0.0009858263904629619, 'batch_size': 119, 'step_size': 11, 'gamma': 0.7682497827804013}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:04:42,800][0m Trial 31 finished with value: 0.03797959805803096 and parameters: {'observation_period_num': 5, 'train_rates': 0.9523099322748799, 'learning_rate': 0.0002987091492496759, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8599642039601729}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:05:58,385][0m Trial 32 finished with value: 0.11174494932804789 and parameters: {'observation_period_num': 27, 'train_rates': 0.9522381901751685, 'learning_rate': 2.521741701361423e-05, 'batch_size': 82, 'step_size': 3, 'gamma': 0.8292763121399589}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:07:50,812][0m Trial 33 finished with value: 0.04366869840365618 and parameters: {'observation_period_num': 44, 'train_rates': 0.933930639584149, 'learning_rate': 0.00038794677881568795, 'batch_size': 53, 'step_size': 4, 'gamma': 0.8076504166695134}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:09:02,447][0m Trial 34 finished with value: 0.043527577036214106 and parameters: {'observation_period_num': 26, 'train_rates': 0.9649329577566703, 'learning_rate': 0.0006328228560361201, 'batch_size': 89, 'step_size': 4, 'gamma': 0.8328696883737137}. Best is trial 25 with value: 0.032789357006549835.[0m
[32m[I 2025-01-05 19:14:35,942][0m Trial 35 finished with value: 0.025755180052590965 and parameters: {'observation_period_num': 5, 'train_rates': 0.9049211237928657, 'learning_rate': 0.00022729458521905302, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8593371678519443}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:19:44,276][0m Trial 36 finished with value: 0.07453499307458321 and parameters: {'observation_period_num': 57, 'train_rates': 0.9080680870223491, 'learning_rate': 0.00010743932148772243, 'batch_size': 18, 'step_size': 6, 'gamma': 0.7898828938736564}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:22:35,426][0m Trial 37 finished with value: 0.04495748308571902 and parameters: {'observation_period_num': 39, 'train_rates': 0.8679615123029869, 'learning_rate': 0.00018083805890227125, 'batch_size': 32, 'step_size': 5, 'gamma': 0.9144016079732321}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:23:27,175][0m Trial 38 finished with value: 0.15941389149580246 and parameters: {'observation_period_num': 98, 'train_rates': 0.9319492388633398, 'learning_rate': 4.292835097500786e-05, 'batch_size': 119, 'step_size': 1, 'gamma': 0.9346002449215663}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:27:19,791][0m Trial 39 finished with value: 0.048960165336992094 and parameters: {'observation_period_num': 21, 'train_rates': 0.8268994905236945, 'learning_rate': 0.0007104043657574381, 'batch_size': 23, 'step_size': 4, 'gamma': 0.9662301131922578}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:28:05,541][0m Trial 40 finished with value: 0.11270862113345753 and parameters: {'observation_period_num': 190, 'train_rates': 0.900713149889381, 'learning_rate': 0.0005240204897051819, 'batch_size': 135, 'step_size': 7, 'gamma': 0.8788148086613993}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:30:01,094][0m Trial 41 finished with value: 0.0365167415126207 and parameters: {'observation_period_num': 13, 'train_rates': 0.9683231380001553, 'learning_rate': 0.00023564624950609558, 'batch_size': 53, 'step_size': 3, 'gamma': 0.8590623922494008}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:32:13,688][0m Trial 42 finished with value: 0.04092073827408827 and parameters: {'observation_period_num': 16, 'train_rates': 0.964761451602795, 'learning_rate': 0.0002032968246739598, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8564055318950883}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:33:53,066][0m Trial 43 finished with value: 0.045466484904439004 and parameters: {'observation_period_num': 41, 'train_rates': 0.9319159654785042, 'learning_rate': 0.00048027709164540105, 'batch_size': 59, 'step_size': 5, 'gamma': 0.8346442325820473}. Best is trial 35 with value: 0.025755180052590965.[0m
Early stopping at epoch 43
[32m[I 2025-01-05 19:35:00,139][0m Trial 44 finished with value: 0.4044252634048462 and parameters: {'observation_period_num': 237, 'train_rates': 0.9867270128155464, 'learning_rate': 1.8952687429281715e-05, 'batch_size': 38, 'step_size': 1, 'gamma': 0.7530523431566192}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:36:22,350][0m Trial 45 finished with value: 0.03914556657438764 and parameters: {'observation_period_num': 18, 'train_rates': 0.8765314831444636, 'learning_rate': 0.0002733381924910085, 'batch_size': 73, 'step_size': 4, 'gamma': 0.8076730133720957}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:37:30,531][0m Trial 46 finished with value: 0.06601664753336656 and parameters: {'observation_period_num': 54, 'train_rates': 0.967225896114934, 'learning_rate': 0.00016471908230361652, 'batch_size': 93, 'step_size': 8, 'gamma': 0.8845140233470159}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:39:07,995][0m Trial 47 finished with value: 0.1818584971016145 and parameters: {'observation_period_num': 15, 'train_rates': 0.7770804469848638, 'learning_rate': 0.00010451415290180356, 'batch_size': 54, 'step_size': 3, 'gamma': 0.8722359405128117}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:40:32,288][0m Trial 48 finished with value: 0.06383161622787191 and parameters: {'observation_period_num': 33, 'train_rates': 0.9267565025549769, 'learning_rate': 6.814035891581946e-05, 'batch_size': 72, 'step_size': 2, 'gamma': 0.8920280316457111}. Best is trial 35 with value: 0.025755180052590965.[0m
[32m[I 2025-01-05 19:41:16,230][0m Trial 49 finished with value: 0.2647697925567627 and parameters: {'observation_period_num': 13, 'train_rates': 0.9426199886935541, 'learning_rate': 4.534166003340318e-06, 'batch_size': 235, 'step_size': 14, 'gamma': 0.8509546053383013}. Best is trial 35 with value: 0.025755180052590965.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-05 19:41:16,240][0m A new study created in memory with name: no-name-fe76c532-0275-4cab-8bb7-ec38c62fd1a4[0m
[32m[I 2025-01-05 19:41:45,447][0m Trial 0 finished with value: 0.285467864323357 and parameters: {'observation_period_num': 200, 'train_rates': 0.6509448736958743, 'learning_rate': 0.00013731398697529817, 'batch_size': 218, 'step_size': 13, 'gamma': 0.9683420615261944}. Best is trial 0 with value: 0.285467864323357.[0m
[32m[I 2025-01-05 19:42:14,352][0m Trial 1 finished with value: 0.23117826344679754 and parameters: {'observation_period_num': 30, 'train_rates': 0.7472662211910921, 'learning_rate': 7.611899452940251e-05, 'batch_size': 203, 'step_size': 13, 'gamma': 0.9584944485149475}. Best is trial 1 with value: 0.23117826344679754.[0m
[32m[I 2025-01-05 19:44:50,547][0m Trial 2 finished with value: 0.09593186844659295 and parameters: {'observation_period_num': 78, 'train_rates': 0.8878572643417613, 'learning_rate': 7.163702992889005e-06, 'batch_size': 35, 'step_size': 10, 'gamma': 0.957756390466966}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:45:56,601][0m Trial 3 finished with value: 0.2551659471638485 and parameters: {'observation_period_num': 72, 'train_rates': 0.6687176926537886, 'learning_rate': 7.843841830820459e-05, 'batch_size': 73, 'step_size': 9, 'gamma': 0.78767095959201}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:46:34,449][0m Trial 4 finished with value: 0.8764094043545559 and parameters: {'observation_period_num': 196, 'train_rates': 0.6393742068713244, 'learning_rate': 1.464480023393301e-06, 'batch_size': 139, 'step_size': 15, 'gamma': 0.981462722661385}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:47:37,026][0m Trial 5 finished with value: 0.7656798775947078 and parameters: {'observation_period_num': 57, 'train_rates': 0.7488442173577136, 'learning_rate': 2.0810316071355795e-06, 'batch_size': 92, 'step_size': 7, 'gamma': 0.7623448157656698}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:49:00,309][0m Trial 6 finished with value: 0.723617356994548 and parameters: {'observation_period_num': 114, 'train_rates': 0.6011234366849215, 'learning_rate': 2.901284781569948e-06, 'batch_size': 53, 'step_size': 9, 'gamma': 0.8685536473821411}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:50:32,207][0m Trial 7 finished with value: 0.11458736409743626 and parameters: {'observation_period_num': 191, 'train_rates': 0.9633235401949097, 'learning_rate': 7.443518865533005e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.8033770590420126}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:51:49,184][0m Trial 8 finished with value: 0.26230426233806353 and parameters: {'observation_period_num': 198, 'train_rates': 0.6870743804352132, 'learning_rate': 0.0008399238685641735, 'batch_size': 61, 'step_size': 2, 'gamma': 0.89869627012653}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:52:31,169][0m Trial 9 finished with value: 0.2314240442689999 and parameters: {'observation_period_num': 178, 'train_rates': 0.9071607738054375, 'learning_rate': 9.433612275260441e-06, 'batch_size': 142, 'step_size': 5, 'gamma': 0.9613642415191616}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 19:57:20,836][0m Trial 10 finished with value: 0.19913354537677533 and parameters: {'observation_period_num': 122, 'train_rates': 0.8542059002518764, 'learning_rate': 1.1786732707857542e-05, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9071357567508136}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 20:02:12,769][0m Trial 11 finished with value: 0.11401609636165878 and parameters: {'observation_period_num': 246, 'train_rates': 0.9838035342099837, 'learning_rate': 1.4832563742083747e-05, 'batch_size': 19, 'step_size': 11, 'gamma': 0.8288226175514766}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 20:07:34,946][0m Trial 12 finished with value: 0.11529114045376002 and parameters: {'observation_period_num': 247, 'train_rates': 0.9842137832393132, 'learning_rate': 1.2169477007037004e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8459118024964288}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 20:08:24,107][0m Trial 13 finished with value: 0.3219924048532414 and parameters: {'observation_period_num': 88, 'train_rates': 0.8623655048886532, 'learning_rate': 5.348504931882572e-06, 'batch_size': 122, 'step_size': 6, 'gamma': 0.8306502869093997}. Best is trial 2 with value: 0.09593186844659295.[0m
[32m[I 2025-01-05 20:08:54,679][0m Trial 14 finished with value: 0.08944950252771378 and parameters: {'observation_period_num': 10, 'train_rates': 0.9210034459421457, 'learning_rate': 2.1725829300378392e-05, 'batch_size': 251, 'step_size': 11, 'gamma': 0.9010195016946654}. Best is trial 14 with value: 0.08944950252771378.[0m
[32m[I 2025-01-05 20:09:31,707][0m Trial 15 finished with value: 0.08603479463015976 and parameters: {'observation_period_num': 18, 'train_rates': 0.9077552705157548, 'learning_rate': 2.943228824534546e-05, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9260304639315773}. Best is trial 15 with value: 0.08603479463015976.[0m
[32m[I 2025-01-05 20:10:01,708][0m Trial 16 finished with value: 0.09743476519361138 and parameters: {'observation_period_num': 5, 'train_rates': 0.8045166052513039, 'learning_rate': 3.0495265894427795e-05, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9184375178985924}. Best is trial 15 with value: 0.08603479463015976.[0m
[32m[I 2025-01-05 20:10:40,780][0m Trial 17 finished with value: 0.06574098020792007 and parameters: {'observation_period_num': 31, 'train_rates': 0.9308905865560367, 'learning_rate': 0.00027749939178992055, 'batch_size': 251, 'step_size': 14, 'gamma': 0.9298988052915722}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:11:44,510][0m Trial 18 finished with value: 0.10706658263804884 and parameters: {'observation_period_num': 44, 'train_rates': 0.8260922760640663, 'learning_rate': 0.0005295765936062084, 'batch_size': 178, 'step_size': 14, 'gamma': 0.9320229844914398}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:13:02,704][0m Trial 19 finished with value: 0.08054701238870621 and parameters: {'observation_period_num': 147, 'train_rates': 0.9348438118451583, 'learning_rate': 0.0002577863377792225, 'batch_size': 222, 'step_size': 15, 'gamma': 0.8773095515422671}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:14:03,524][0m Trial 20 finished with value: 0.10261005163192749 and parameters: {'observation_period_num': 153, 'train_rates': 0.9330544985985308, 'learning_rate': 0.0002912710425408863, 'batch_size': 218, 'step_size': 4, 'gamma': 0.8763937645474298}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:15:15,874][0m Trial 21 finished with value: 0.12935228645801544 and parameters: {'observation_period_num': 150, 'train_rates': 0.9502626279020774, 'learning_rate': 0.0002092830562026728, 'batch_size': 233, 'step_size': 15, 'gamma': 0.9295626060517285}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:16:45,106][0m Trial 22 finished with value: 0.08590863171560936 and parameters: {'observation_period_num': 95, 'train_rates': 0.8838771095687861, 'learning_rate': 0.0003640110719705491, 'batch_size': 181, 'step_size': 13, 'gamma': 0.8858470555169681}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:17:49,209][0m Trial 23 finished with value: 0.09957176741016538 and parameters: {'observation_period_num': 110, 'train_rates': 0.8663824555163294, 'learning_rate': 0.00043597982011162154, 'batch_size': 182, 'step_size': 13, 'gamma': 0.8666158031369671}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:19:23,766][0m Trial 24 finished with value: 0.09028603881597519 and parameters: {'observation_period_num': 95, 'train_rates': 0.9450605434731674, 'learning_rate': 0.000919204187763053, 'batch_size': 192, 'step_size': 12, 'gamma': 0.8826714925250219}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:20:47,515][0m Trial 25 finished with value: 0.08342474649032465 and parameters: {'observation_period_num': 143, 'train_rates': 0.8887603858973079, 'learning_rate': 0.00016388552449356932, 'batch_size': 161, 'step_size': 14, 'gamma': 0.8552413478317749}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:21:56,374][0m Trial 26 finished with value: 0.10976053674699394 and parameters: {'observation_period_num': 154, 'train_rates': 0.831635063552818, 'learning_rate': 0.0001501057275332884, 'batch_size': 159, 'step_size': 14, 'gamma': 0.8424353348077471}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:23:20,451][0m Trial 27 finished with value: 0.28545279713508526 and parameters: {'observation_period_num': 142, 'train_rates': 0.7694538950073471, 'learning_rate': 0.00012169807952042232, 'batch_size': 226, 'step_size': 14, 'gamma': 0.8570845282880418}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:24:28,939][0m Trial 28 finished with value: 0.10038282777018406 and parameters: {'observation_period_num': 164, 'train_rates': 0.9025549868788124, 'learning_rate': 4.654760403472945e-05, 'batch_size': 111, 'step_size': 12, 'gamma': 0.8006408100081965}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:25:54,034][0m Trial 29 finished with value: 0.11343253403902054 and parameters: {'observation_period_num': 134, 'train_rates': 0.9632172251015942, 'learning_rate': 0.0002187660371458301, 'batch_size': 213, 'step_size': 14, 'gamma': 0.8182143555380201}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:26:59,366][0m Trial 30 finished with value: 0.1428164333767361 and parameters: {'observation_period_num': 231, 'train_rates': 0.8346510433231265, 'learning_rate': 0.0005628243760631321, 'batch_size': 236, 'step_size': 8, 'gamma': 0.9459236957476721}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:28:16,002][0m Trial 31 finished with value: 0.11303417847074311 and parameters: {'observation_period_num': 174, 'train_rates': 0.8851832866852638, 'learning_rate': 0.00029898127319428726, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8863803768312093}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:29:36,572][0m Trial 32 finished with value: 0.0811939129800153 and parameters: {'observation_period_num': 48, 'train_rates': 0.8879501503508758, 'learning_rate': 0.00038334849070283827, 'batch_size': 206, 'step_size': 13, 'gamma': 0.895329774997976}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:31:01,664][0m Trial 33 finished with value: 0.06677373498678207 and parameters: {'observation_period_num': 35, 'train_rates': 0.9326589906109448, 'learning_rate': 0.00013268063468230537, 'batch_size': 210, 'step_size': 11, 'gamma': 0.912774206667686}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:32:13,027][0m Trial 34 finished with value: 0.07315099622323135 and parameters: {'observation_period_num': 33, 'train_rates': 0.9268380010715203, 'learning_rate': 9.778280931890331e-05, 'batch_size': 202, 'step_size': 11, 'gamma': 0.947438578461539}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:33:35,515][0m Trial 35 finished with value: 0.07315459102392197 and parameters: {'observation_period_num': 39, 'train_rates': 0.932813773593345, 'learning_rate': 8.538980948489461e-05, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9868901582183169}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:34:50,576][0m Trial 36 finished with value: 0.06656412780284882 and parameters: {'observation_period_num': 31, 'train_rates': 0.9672207853278938, 'learning_rate': 9.127579089974256e-05, 'batch_size': 198, 'step_size': 10, 'gamma': 0.9887759010221367}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:36:09,996][0m Trial 37 finished with value: 0.14211854338645935 and parameters: {'observation_period_num': 64, 'train_rates': 0.9682130672667967, 'learning_rate': 5.108310323484325e-05, 'batch_size': 239, 'step_size': 9, 'gamma': 0.9701419776838965}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:37:15,189][0m Trial 38 finished with value: 0.07071619480848312 and parameters: {'observation_period_num': 28, 'train_rates': 0.9863158015030417, 'learning_rate': 8.715157118198663e-05, 'batch_size': 207, 'step_size': 10, 'gamma': 0.944785560710879}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:38:09,355][0m Trial 39 finished with value: 0.08785076439380646 and parameters: {'observation_period_num': 25, 'train_rates': 0.9894366437247188, 'learning_rate': 5.521683131906865e-05, 'batch_size': 240, 'step_size': 8, 'gamma': 0.9757973333639772}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:38:58,199][0m Trial 40 finished with value: 0.2302894903291738 and parameters: {'observation_period_num': 55, 'train_rates': 0.6970057735294433, 'learning_rate': 0.00012745789635937378, 'batch_size': 215, 'step_size': 10, 'gamma': 0.9483143327837488}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:39:58,173][0m Trial 41 finished with value: 0.07538557052612305 and parameters: {'observation_period_num': 33, 'train_rates': 0.9534107851414715, 'learning_rate': 9.55632303343784e-05, 'batch_size': 199, 'step_size': 10, 'gamma': 0.950936095317023}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:40:47,573][0m Trial 42 finished with value: 0.08412404814611972 and parameters: {'observation_period_num': 72, 'train_rates': 0.9211773038833605, 'learning_rate': 7.738589685624905e-05, 'batch_size': 173, 'step_size': 9, 'gamma': 0.9403642516171881}. Best is trial 17 with value: 0.06574098020792007.[0m
[32m[I 2025-01-05 20:42:13,970][0m Trial 43 finished with value: 0.05392056331038475 and parameters: {'observation_period_num': 23, 'train_rates': 0.9713785927351345, 'learning_rate': 0.0001759859123947022, 'batch_size': 146, 'step_size': 11, 'gamma': 0.915421492515324}. Best is trial 43 with value: 0.05392056331038475.[0m
[32m[I 2025-01-05 20:44:04,390][0m Trial 44 finished with value: 0.05346220650590292 and parameters: {'observation_period_num': 18, 'train_rates': 0.9656233087427852, 'learning_rate': 0.0001824390577364345, 'batch_size': 85, 'step_size': 7, 'gamma': 0.9163851310865607}. Best is trial 44 with value: 0.05346220650590292.[0m
[32m[I 2025-01-05 20:46:00,621][0m Trial 45 finished with value: 0.04414765955400221 and parameters: {'observation_period_num': 15, 'train_rates': 0.9670657911982976, 'learning_rate': 0.000641763897711025, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9090464857788871}. Best is trial 45 with value: 0.04414765955400221.[0m
[32m[I 2025-01-05 20:47:45,265][0m Trial 46 finished with value: 0.05455731406160023 and parameters: {'observation_period_num': 16, 'train_rates': 0.96869591727215, 'learning_rate': 0.00018940354172675367, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9176612385043192}. Best is trial 45 with value: 0.04414765955400221.[0m
[32m[I 2025-01-05 20:49:24,853][0m Trial 47 finished with value: 0.04955672941702817 and parameters: {'observation_period_num': 16, 'train_rates': 0.9618590194486717, 'learning_rate': 0.0006989921939918809, 'batch_size': 87, 'step_size': 7, 'gamma': 0.9204111029406395}. Best is trial 45 with value: 0.04414765955400221.[0m
[32m[I 2025-01-05 20:51:16,961][0m Trial 48 finished with value: 0.049238825088431096 and parameters: {'observation_period_num': 18, 'train_rates': 0.9705912008663836, 'learning_rate': 0.0006641487693556153, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9144662981802085}. Best is trial 45 with value: 0.04414765955400221.[0m
[32m[I 2025-01-05 20:52:50,870][0m Trial 49 finished with value: 0.06290147873692047 and parameters: {'observation_period_num': 5, 'train_rates': 0.9549951622972364, 'learning_rate': 0.0006980367847017763, 'batch_size': 86, 'step_size': 5, 'gamma': 0.904226565726655}. Best is trial 45 with value: 0.04414765955400221.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-05 20:52:50,881][0m A new study created in memory with name: no-name-fef217d2-4a93-4183-88d8-bcf4a04cd6c7[0m
[32m[I 2025-01-05 20:54:02,360][0m Trial 0 finished with value: 0.24919598329191406 and parameters: {'observation_period_num': 193, 'train_rates': 0.6522313190375727, 'learning_rate': 0.0001889370843508363, 'batch_size': 158, 'step_size': 12, 'gamma': 0.8180000623747758}. Best is trial 0 with value: 0.24919598329191406.[0m
[32m[I 2025-01-05 20:55:17,281][0m Trial 1 finished with value: 0.873247894656394 and parameters: {'observation_period_num': 244, 'train_rates': 0.7168427320137086, 'learning_rate': 1.025362808482652e-06, 'batch_size': 121, 'step_size': 10, 'gamma': 0.9410576441863521}. Best is trial 0 with value: 0.24919598329191406.[0m
[32m[I 2025-01-05 20:56:16,164][0m Trial 2 finished with value: 0.9586397028042123 and parameters: {'observation_period_num': 73, 'train_rates': 0.6587624722005033, 'learning_rate': 1.0062685115890803e-06, 'batch_size': 255, 'step_size': 10, 'gamma': 0.931369211206377}. Best is trial 0 with value: 0.24919598329191406.[0m
[32m[I 2025-01-05 20:57:29,454][0m Trial 3 finished with value: 0.9678663214991609 and parameters: {'observation_period_num': 150, 'train_rates': 0.6996286329466237, 'learning_rate': 1.35514687937424e-06, 'batch_size': 132, 'step_size': 7, 'gamma': 0.7690806966447946}. Best is trial 0 with value: 0.24919598329191406.[0m
[32m[I 2025-01-05 20:58:24,384][0m Trial 4 finished with value: 0.169619431856993 and parameters: {'observation_period_num': 28, 'train_rates': 0.7134787561934373, 'learning_rate': 0.0006102968002024862, 'batch_size': 236, 'step_size': 14, 'gamma': 0.8526229246627833}. Best is trial 4 with value: 0.169619431856993.[0m
[32m[I 2025-01-05 21:02:52,992][0m Trial 5 finished with value: 0.20209798241424007 and parameters: {'observation_period_num': 99, 'train_rates': 0.6231505723150919, 'learning_rate': 0.0005991896044354015, 'batch_size': 17, 'step_size': 7, 'gamma': 0.7879191846410515}. Best is trial 4 with value: 0.169619431856993.[0m
Early stopping at epoch 68
[32m[I 2025-01-05 21:03:25,691][0m Trial 6 finished with value: 0.2555379951457873 and parameters: {'observation_period_num': 160, 'train_rates': 0.8040817907824034, 'learning_rate': 0.0004643941405798749, 'batch_size': 236, 'step_size': 1, 'gamma': 0.8020684147168982}. Best is trial 4 with value: 0.169619431856993.[0m
[32m[I 2025-01-05 21:04:36,479][0m Trial 7 finished with value: 0.28321387846666357 and parameters: {'observation_period_num': 157, 'train_rates': 0.7744753114567061, 'learning_rate': 0.0005885412085525077, 'batch_size': 136, 'step_size': 2, 'gamma': 0.8066304641975959}. Best is trial 4 with value: 0.169619431856993.[0m
[32m[I 2025-01-05 21:05:56,974][0m Trial 8 finished with value: 0.43327094736882454 and parameters: {'observation_period_num': 26, 'train_rates': 0.858684706419715, 'learning_rate': 1.0576343545747262e-06, 'batch_size': 128, 'step_size': 7, 'gamma': 0.9739096355249773}. Best is trial 4 with value: 0.169619431856993.[0m
[32m[I 2025-01-05 21:08:22,151][0m Trial 9 finished with value: 0.18603490225701447 and parameters: {'observation_period_num': 50, 'train_rates': 0.656693772425359, 'learning_rate': 0.0004453187053121077, 'batch_size': 36, 'step_size': 9, 'gamma': 0.9847434379440035}. Best is trial 4 with value: 0.169619431856993.[0m
[32m[I 2025-01-05 21:09:16,768][0m Trial 10 finished with value: 0.13778623938560486 and parameters: {'observation_period_num': 103, 'train_rates': 0.9694036276589464, 'learning_rate': 2.8598061381673808e-05, 'batch_size': 199, 'step_size': 15, 'gamma': 0.8652963332239727}. Best is trial 10 with value: 0.13778623938560486.[0m
[32m[I 2025-01-05 21:10:36,696][0m Trial 11 finished with value: 0.07574126869440079 and parameters: {'observation_period_num': 6, 'train_rates': 0.9743861570826341, 'learning_rate': 2.4001290554209273e-05, 'batch_size': 202, 'step_size': 15, 'gamma': 0.8668526046855899}. Best is trial 11 with value: 0.07574126869440079.[0m
[32m[I 2025-01-05 21:11:50,494][0m Trial 12 finished with value: 0.17789408564567566 and parameters: {'observation_period_num': 101, 'train_rates': 0.9652790074392492, 'learning_rate': 1.6756995550970035e-05, 'batch_size': 196, 'step_size': 15, 'gamma': 0.8874665239058336}. Best is trial 11 with value: 0.07574126869440079.[0m
[32m[I 2025-01-05 21:13:16,460][0m Trial 13 finished with value: 0.07315807789564133 and parameters: {'observation_period_num': 12, 'train_rates': 0.9871078003716259, 'learning_rate': 2.166551359528613e-05, 'batch_size': 190, 'step_size': 13, 'gamma': 0.8815115974364335}. Best is trial 13 with value: 0.07315807789564133.[0m
[32m[I 2025-01-05 21:14:31,454][0m Trial 14 finished with value: 0.09685704251576899 and parameters: {'observation_period_num': 7, 'train_rates': 0.9096092146404103, 'learning_rate': 8.808004728489818e-06, 'batch_size': 184, 'step_size': 12, 'gamma': 0.895267663981454}. Best is trial 13 with value: 0.07315807789564133.[0m
[32m[I 2025-01-05 21:15:44,016][0m Trial 15 finished with value: 0.06516218185424805 and parameters: {'observation_period_num': 55, 'train_rates': 0.9889248755596869, 'learning_rate': 7.458824920009876e-05, 'batch_size': 89, 'step_size': 13, 'gamma': 0.9174449371671198}. Best is trial 15 with value: 0.06516218185424805.[0m
[32m[I 2025-01-05 21:17:04,460][0m Trial 16 finished with value: 0.0655267216810366 and parameters: {'observation_period_num': 60, 'train_rates': 0.9010449909798659, 'learning_rate': 0.00010137804791590782, 'batch_size': 77, 'step_size': 12, 'gamma': 0.9217422076745497}. Best is trial 15 with value: 0.06516218185424805.[0m
[32m[I 2025-01-05 21:18:22,119][0m Trial 17 finished with value: 0.06036251916814206 and parameters: {'observation_period_num': 65, 'train_rates': 0.9071702397847854, 'learning_rate': 8.505604849439751e-05, 'batch_size': 88, 'step_size': 4, 'gamma': 0.9222092210204368}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:19:43,218][0m Trial 18 finished with value: 0.06127305736250066 and parameters: {'observation_period_num': 78, 'train_rates': 0.9185385610982139, 'learning_rate': 8.147478718241211e-05, 'batch_size': 87, 'step_size': 4, 'gamma': 0.960257587410798}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:21:08,921][0m Trial 19 finished with value: 0.0646951351510851 and parameters: {'observation_period_num': 120, 'train_rates': 0.8526203164241577, 'learning_rate': 7.357867579987204e-05, 'batch_size': 72, 'step_size': 4, 'gamma': 0.9600401583917443}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:23:28,756][0m Trial 20 finished with value: 0.18466256962882147 and parameters: {'observation_period_num': 72, 'train_rates': 0.921849093485063, 'learning_rate': 3.973890361842723e-06, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9623218158901784}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:25:11,636][0m Trial 21 finished with value: 0.06765468422380196 and parameters: {'observation_period_num': 120, 'train_rates': 0.8610684983388548, 'learning_rate': 6.765097982200098e-05, 'batch_size': 88, 'step_size': 4, 'gamma': 0.9557695166572882}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:26:47,135][0m Trial 22 finished with value: 0.07372609889492966 and parameters: {'observation_period_num': 131, 'train_rates': 0.8525030901180041, 'learning_rate': 0.00018383958879804473, 'batch_size': 67, 'step_size': 4, 'gamma': 0.9885687720012402}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:28:23,124][0m Trial 23 finished with value: 0.07723958045244217 and parameters: {'observation_period_num': 86, 'train_rates': 0.8139309262182317, 'learning_rate': 4.520257840542091e-05, 'batch_size': 106, 'step_size': 5, 'gamma': 0.9514020180713187}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:30:32,764][0m Trial 24 finished with value: 0.07648880998189536 and parameters: {'observation_period_num': 118, 'train_rates': 0.9328340805756395, 'learning_rate': 0.0001918990861191927, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9098069388322277}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:32:04,289][0m Trial 25 finished with value: 0.07387087420114477 and parameters: {'observation_period_num': 190, 'train_rates': 0.8888629149472929, 'learning_rate': 0.0001227855853533129, 'batch_size': 101, 'step_size': 5, 'gamma': 0.938327805713855}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:35:16,400][0m Trial 26 finished with value: 0.06465409230629764 and parameters: {'observation_period_num': 84, 'train_rates': 0.82838199358505, 'learning_rate': 4.435468938858011e-05, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9692287928312011}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:38:27,333][0m Trial 27 finished with value: 0.24455542298762695 and parameters: {'observation_period_num': 37, 'train_rates': 0.7634615720478388, 'learning_rate': 1.1562331073869009e-05, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9052015084379171}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:43:22,839][0m Trial 28 finished with value: 0.06266198360494205 and parameters: {'observation_period_num': 78, 'train_rates': 0.9464326643509055, 'learning_rate': 4.510048002322112e-05, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8414670345678491}. Best is trial 17 with value: 0.06036251916814206.[0m
[32m[I 2025-01-05 21:44:47,891][0m Trial 29 finished with value: 0.056231867907376124 and parameters: {'observation_period_num': 44, 'train_rates': 0.9403070393164374, 'learning_rate': 0.0002490660153300479, 'batch_size': 153, 'step_size': 6, 'gamma': 0.8365276310600452}. Best is trial 29 with value: 0.056231867907376124.[0m
[32m[I 2025-01-05 21:45:58,547][0m Trial 30 finished with value: 0.05304928738396705 and parameters: {'observation_period_num': 41, 'train_rates': 0.8874210461453884, 'learning_rate': 0.0002825601644601505, 'batch_size': 171, 'step_size': 6, 'gamma': 0.8172992954659168}. Best is trial 30 with value: 0.05304928738396705.[0m
[32m[I 2025-01-05 21:47:25,368][0m Trial 31 finished with value: 0.07088062475988831 and parameters: {'observation_period_num': 45, 'train_rates': 0.8813627989992767, 'learning_rate': 0.0002533185271356172, 'batch_size': 164, 'step_size': 6, 'gamma': 0.8271483436644399}. Best is trial 30 with value: 0.05304928738396705.[0m
[32m[I 2025-01-05 21:48:39,369][0m Trial 32 finished with value: 0.0937676802277565 and parameters: {'observation_period_num': 248, 'train_rates': 0.939639884440838, 'learning_rate': 0.00029397234383547375, 'batch_size': 164, 'step_size': 8, 'gamma': 0.8329244813653806}. Best is trial 30 with value: 0.05304928738396705.[0m
[32m[I 2025-01-05 21:50:07,768][0m Trial 33 finished with value: 0.06090424209833145 and parameters: {'observation_period_num': 26, 'train_rates': 0.9527555687288353, 'learning_rate': 0.00013648502517652007, 'batch_size': 156, 'step_size': 6, 'gamma': 0.8098089119350844}. Best is trial 30 with value: 0.05304928738396705.[0m
[32m[I 2025-01-05 21:51:29,726][0m Trial 34 finished with value: 0.05848734825849533 and parameters: {'observation_period_num': 24, 'train_rates': 0.9538686085164687, 'learning_rate': 0.0001691841151183662, 'batch_size': 151, 'step_size': 9, 'gamma': 0.7925680450371341}. Best is trial 30 with value: 0.05304928738396705.[0m
[32m[I 2025-01-05 21:52:51,663][0m Trial 35 finished with value: 0.045857274363311336 and parameters: {'observation_period_num': 38, 'train_rates': 0.8927593281404798, 'learning_rate': 0.00032441922939960646, 'batch_size': 149, 'step_size': 10, 'gamma': 0.7572216519644546}. Best is trial 35 with value: 0.045857274363311336.[0m
[32m[I 2025-01-05 21:54:28,595][0m Trial 36 finished with value: 0.04500581493431872 and parameters: {'observation_period_num': 37, 'train_rates': 0.8756051416281717, 'learning_rate': 0.0007979501130918365, 'batch_size': 147, 'step_size': 10, 'gamma': 0.7500134511443804}. Best is trial 36 with value: 0.04500581493431872.[0m
[32m[I 2025-01-05 21:55:37,262][0m Trial 37 finished with value: 0.04066915033916575 and parameters: {'observation_period_num': 39, 'train_rates': 0.8811470936776697, 'learning_rate': 0.0009771925950017167, 'batch_size': 176, 'step_size': 10, 'gamma': 0.7548422976490132}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 21:56:50,262][0m Trial 38 finished with value: 0.10382302313119965 and parameters: {'observation_period_num': 228, 'train_rates': 0.8341458886439792, 'learning_rate': 0.0009744118012477135, 'batch_size': 173, 'step_size': 11, 'gamma': 0.7532904152858945}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 21:58:04,600][0m Trial 39 finished with value: 0.19242280756318292 and parameters: {'observation_period_num': 35, 'train_rates': 0.7790278580208777, 'learning_rate': 0.0003935559373802111, 'batch_size': 216, 'step_size': 10, 'gamma': 0.7734434127437723}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 21:59:21,582][0m Trial 40 finished with value: 0.16961389749644537 and parameters: {'observation_period_num': 15, 'train_rates': 0.7538058739256145, 'learning_rate': 0.0009689714173850087, 'batch_size': 142, 'step_size': 10, 'gamma': 0.7669362114202329}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 22:00:48,096][0m Trial 41 finished with value: 0.04752982428736901 and parameters: {'observation_period_num': 49, 'train_rates': 0.8775272368838949, 'learning_rate': 0.0006964551789679227, 'batch_size': 122, 'step_size': 8, 'gamma': 0.7514297386186923}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 22:02:08,829][0m Trial 42 finished with value: 0.053505706902686245 and parameters: {'observation_period_num': 55, 'train_rates': 0.8755013009517185, 'learning_rate': 0.0007746542818296096, 'batch_size': 119, 'step_size': 8, 'gamma': 0.7523722255728693}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 22:03:02,556][0m Trial 43 finished with value: 0.04299086704850197 and parameters: {'observation_period_num': 36, 'train_rates': 0.888831545999697, 'learning_rate': 0.0005718846664362348, 'batch_size': 176, 'step_size': 9, 'gamma': 0.7744363624920944}. Best is trial 37 with value: 0.04066915033916575.[0m
[32m[I 2025-01-05 22:04:24,790][0m Trial 44 finished with value: 0.03823716421177995 and parameters: {'observation_period_num': 20, 'train_rates': 0.8317706625659463, 'learning_rate': 0.0006149944215370888, 'batch_size': 120, 'step_size': 9, 'gamma': 0.778281763023794}. Best is trial 44 with value: 0.03823716421177995.[0m
[32m[I 2025-01-05 22:05:34,312][0m Trial 45 finished with value: 0.04079575202975076 and parameters: {'observation_period_num': 19, 'train_rates': 0.8347272868895965, 'learning_rate': 0.00040734377694516614, 'batch_size': 140, 'step_size': 11, 'gamma': 0.778756913729459}. Best is trial 44 with value: 0.03823716421177995.[0m
[32m[I 2025-01-05 22:06:42,551][0m Trial 46 finished with value: 0.044033978141741405 and parameters: {'observation_period_num': 19, 'train_rates': 0.8272062364733496, 'learning_rate': 0.0005180121018484038, 'batch_size': 218, 'step_size': 9, 'gamma': 0.7810839718891236}. Best is trial 44 with value: 0.03823716421177995.[0m
[32m[I 2025-01-05 22:07:56,144][0m Trial 47 finished with value: 0.0469356423561589 and parameters: {'observation_period_num': 22, 'train_rates': 0.7961048838478714, 'learning_rate': 0.000519114805894991, 'batch_size': 223, 'step_size': 11, 'gamma': 0.7822210050226794}. Best is trial 44 with value: 0.03823716421177995.[0m
[32m[I 2025-01-05 22:08:45,830][0m Trial 48 finished with value: 0.047422298103146056 and parameters: {'observation_period_num': 16, 'train_rates': 0.8328890169333929, 'learning_rate': 0.00041527495582719065, 'batch_size': 210, 'step_size': 9, 'gamma': 0.7975676737350247}. Best is trial 44 with value: 0.03823716421177995.[0m
[32m[I 2025-01-05 22:10:00,238][0m Trial 49 finished with value: 0.16448485763519222 and parameters: {'observation_period_num': 8, 'train_rates': 0.7405209713844959, 'learning_rate': 0.0006435586562900445, 'batch_size': 246, 'step_size': 11, 'gamma': 0.7804717270894314}. Best is trial 44 with value: 0.03823716421177995.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-05 22:10:00,248][0m A new study created in memory with name: no-name-2d9810f7-30d4-4c06-b25d-3d63314b3826[0m
[32m[I 2025-01-05 22:11:17,002][0m Trial 0 finished with value: 0.24214907265528077 and parameters: {'observation_period_num': 213, 'train_rates': 0.6596456613180104, 'learning_rate': 0.00039404725327735825, 'batch_size': 124, 'step_size': 4, 'gamma': 0.8211656716093851}. Best is trial 0 with value: 0.24214907265528077.[0m
[32m[I 2025-01-05 22:13:20,707][0m Trial 1 finished with value: 0.07333092437314552 and parameters: {'observation_period_num': 141, 'train_rates': 0.8438417852970724, 'learning_rate': 8.541642221597908e-05, 'batch_size': 49, 'step_size': 5, 'gamma': 0.9090507635565347}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:14:40,382][0m Trial 2 finished with value: 0.10747533574186523 and parameters: {'observation_period_num': 206, 'train_rates': 0.8629863993497022, 'learning_rate': 0.00017268885744960586, 'batch_size': 158, 'step_size': 8, 'gamma': 0.9566183581898609}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:15:29,832][0m Trial 3 finished with value: 0.29635106948782985 and parameters: {'observation_period_num': 54, 'train_rates': 0.7541083822627117, 'learning_rate': 1.027828311609653e-05, 'batch_size': 234, 'step_size': 15, 'gamma': 0.9597007587770375}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:17:21,321][0m Trial 4 finished with value: 0.29068484872769096 and parameters: {'observation_period_num': 44, 'train_rates': 0.7837927757174287, 'learning_rate': 7.522765704384363e-06, 'batch_size': 63, 'step_size': 9, 'gamma': 0.7518159208719789}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:18:31,783][0m Trial 5 finished with value: 0.2397945840007613 and parameters: {'observation_period_num': 141, 'train_rates': 0.6546081336899019, 'learning_rate': 0.00017691393525398163, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8470919148638149}. Best is trial 1 with value: 0.07333092437314552.[0m
Early stopping at epoch 57
[32m[I 2025-01-05 22:19:26,374][0m Trial 6 finished with value: 1.3531639515939686 and parameters: {'observation_period_num': 9, 'train_rates': 0.8044233470219314, 'learning_rate': 1.3046771973776382e-06, 'batch_size': 115, 'step_size': 1, 'gamma': 0.8370686772933791}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:21:00,197][0m Trial 7 finished with value: 0.09495717287063599 and parameters: {'observation_period_num': 180, 'train_rates': 0.9895561494528617, 'learning_rate': 0.00013338379874388136, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9671976717925459}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:21:59,852][0m Trial 8 finished with value: 1.235749124442545 and parameters: {'observation_period_num': 70, 'train_rates': 0.7610343533227182, 'learning_rate': 1.5516016596491255e-06, 'batch_size': 131, 'step_size': 5, 'gamma': 0.7715625199279588}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:23:41,096][0m Trial 9 finished with value: 0.24772375016354414 and parameters: {'observation_period_num': 200, 'train_rates': 0.7010760232285246, 'learning_rate': 0.00016194546213888986, 'batch_size': 64, 'step_size': 6, 'gamma': 0.7794187253042018}. Best is trial 1 with value: 0.07333092437314552.[0m
[32m[I 2025-01-05 22:29:06,103][0m Trial 10 finished with value: 0.05961572820633913 and parameters: {'observation_period_num': 119, 'train_rates': 0.920311379736536, 'learning_rate': 0.0009156835761742955, 'batch_size': 19, 'step_size': 12, 'gamma': 0.9024518144461304}. Best is trial 10 with value: 0.05961572820633913.[0m
[32m[I 2025-01-05 22:35:16,027][0m Trial 11 finished with value: 0.06025481776771497 and parameters: {'observation_period_num': 118, 'train_rates': 0.9137696298733132, 'learning_rate': 0.0009623684369934979, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8952583220336691}. Best is trial 10 with value: 0.05961572820633913.[0m
[32m[I 2025-01-05 22:41:45,425][0m Trial 12 finished with value: 0.05209354642843236 and parameters: {'observation_period_num': 101, 'train_rates': 0.9394291004183005, 'learning_rate': 0.0009107641124087098, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9060163185872391}. Best is trial 12 with value: 0.05209354642843236.[0m
[32m[I 2025-01-05 22:45:54,001][0m Trial 13 finished with value: 0.10116656041807598 and parameters: {'observation_period_num': 102, 'train_rates': 0.9875942212524802, 'learning_rate': 0.0009175270008440769, 'batch_size': 28, 'step_size': 12, 'gamma': 0.9143492409429453}. Best is trial 12 with value: 0.05209354642843236.[0m
[32m[I 2025-01-05 22:47:36,363][0m Trial 14 finished with value: 0.06598343161725197 and parameters: {'observation_period_num': 92, 'train_rates': 0.9170541909605485, 'learning_rate': 4.268376668063943e-05, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8743786150540453}. Best is trial 12 with value: 0.05209354642843236.[0m
[32m[I 2025-01-05 22:48:46,437][0m Trial 15 finished with value: 0.07736262679100037 and parameters: {'observation_period_num': 163, 'train_rates': 0.9354600259196263, 'learning_rate': 0.00048375467715677795, 'batch_size': 205, 'step_size': 15, 'gamma': 0.9278670616045599}. Best is trial 12 with value: 0.05209354642843236.[0m
[32m[I 2025-01-05 22:50:13,192][0m Trial 16 finished with value: 0.12448746609416875 and parameters: {'observation_period_num': 244, 'train_rates': 0.8700227361333799, 'learning_rate': 0.0003928735012936957, 'batch_size': 92, 'step_size': 11, 'gamma': 0.874264310889272}. Best is trial 12 with value: 0.05209354642843236.[0m
[32m[I 2025-01-05 22:53:00,365][0m Trial 17 finished with value: 0.0696951274563666 and parameters: {'observation_period_num': 86, 'train_rates': 0.9489778008846661, 'learning_rate': 2.265680533138175e-05, 'batch_size': 41, 'step_size': 10, 'gamma': 0.9397618661323557}. Best is trial 12 with value: 0.05209354642843236.[0m
[32m[I 2025-01-05 22:54:39,019][0m Trial 18 finished with value: 0.04322107623402889 and parameters: {'observation_period_num': 24, 'train_rates': 0.8890907615852437, 'learning_rate': 4.667068959196932e-05, 'batch_size': 85, 'step_size': 14, 'gamma': 0.8945525240651979}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 22:56:12,753][0m Trial 19 finished with value: 0.08746381449836031 and parameters: {'observation_period_num': 16, 'train_rates': 0.8811644750032354, 'learning_rate': 5.369331376411565e-06, 'batch_size': 89, 'step_size': 14, 'gamma': 0.8153392780729355}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 22:57:40,139][0m Trial 20 finished with value: 0.18251888633469227 and parameters: {'observation_period_num': 45, 'train_rates': 0.6016960729743561, 'learning_rate': 4.5726996918740805e-05, 'batch_size': 73, 'step_size': 14, 'gamma': 0.9863749535470183}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:03:11,784][0m Trial 21 finished with value: 0.20110096384464585 and parameters: {'observation_period_num': 116, 'train_rates': 0.8299192391129852, 'learning_rate': 0.0005371628582638595, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8930048809748656}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:05:44,924][0m Trial 22 finished with value: 0.06785653554457677 and parameters: {'observation_period_num': 75, 'train_rates': 0.892755986056597, 'learning_rate': 1.542370942421233e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8563941999455422}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:08:15,826][0m Trial 23 finished with value: 0.20507786201156733 and parameters: {'observation_period_num': 159, 'train_rates': 0.9534290475748458, 'learning_rate': 3.1570056077929238e-06, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8927111044156334}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:09:52,775][0m Trial 24 finished with value: 0.07410796892804068 and parameters: {'observation_period_num': 35, 'train_rates': 0.9621898888435305, 'learning_rate': 0.00026355988349862914, 'batch_size': 105, 'step_size': 10, 'gamma': 0.9363735993915757}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:11:30,703][0m Trial 25 finished with value: 0.06957914714608684 and parameters: {'observation_period_num': 132, 'train_rates': 0.904170209240179, 'learning_rate': 6.493043900997172e-05, 'batch_size': 75, 'step_size': 14, 'gamma': 0.9126762048395175}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:14:37,747][0m Trial 26 finished with value: 0.09145352516140116 and parameters: {'observation_period_num': 108, 'train_rates': 0.8286041814602945, 'learning_rate': 0.0006491090095423927, 'batch_size': 36, 'step_size': 13, 'gamma': 0.8666751843846808}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:21:02,817][0m Trial 27 finished with value: 0.055750651887937444 and parameters: {'observation_period_num': 65, 'train_rates': 0.9283155139865895, 'learning_rate': 0.0002805774182635557, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8828864310291539}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:22:21,894][0m Trial 28 finished with value: 0.05944320932030678 and parameters: {'observation_period_num': 29, 'train_rates': 0.9603244079616706, 'learning_rate': 8.873260895385306e-05, 'batch_size': 188, 'step_size': 11, 'gamma': 0.8766789696974479}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:24:29,825][0m Trial 29 finished with value: 0.05709516033306494 and parameters: {'observation_period_num': 63, 'train_rates': 0.8847612501265023, 'learning_rate': 0.0002995166717709304, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8134315261040362}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:26:10,872][0m Trial 30 finished with value: 0.05645777679309643 and parameters: {'observation_period_num': 24, 'train_rates': 0.9356384774322821, 'learning_rate': 3.239055828796581e-05, 'batch_size': 107, 'step_size': 11, 'gamma': 0.841368094063949}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:27:21,965][0m Trial 31 finished with value: 0.0571399622120208 and parameters: {'observation_period_num': 24, 'train_rates': 0.9272537661411057, 'learning_rate': 2.598565689736037e-05, 'batch_size': 119, 'step_size': 11, 'gamma': 0.8446155400591897}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:29:02,293][0m Trial 32 finished with value: 0.054498407150711026 and parameters: {'observation_period_num': 56, 'train_rates': 0.8467298816710145, 'learning_rate': 8.123775704220243e-05, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8292572203234826}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:30:17,734][0m Trial 33 finished with value: 0.07158791134332089 and parameters: {'observation_period_num': 81, 'train_rates': 0.8364148863405233, 'learning_rate': 8.516802475319428e-05, 'batch_size': 144, 'step_size': 9, 'gamma': 0.8057966830408722}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:32:20,945][0m Trial 34 finished with value: 0.05551773938069861 and parameters: {'observation_period_num': 58, 'train_rates': 0.8566458269987247, 'learning_rate': 0.00023167537880116292, 'batch_size': 52, 'step_size': 9, 'gamma': 0.8822606680837705}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:33:55,618][0m Trial 35 finished with value: 0.04861637887566589 and parameters: {'observation_period_num': 48, 'train_rates': 0.8520448195129351, 'learning_rate': 0.00012694780186705879, 'batch_size': 76, 'step_size': 8, 'gamma': 0.8276101692702466}. Best is trial 18 with value: 0.04322107623402889.[0m
[32m[I 2025-01-05 23:35:44,186][0m Trial 36 finished with value: 0.04131272524346447 and parameters: {'observation_period_num': 5, 'train_rates': 0.8172186732809892, 'learning_rate': 0.00011076687371628289, 'batch_size': 75, 'step_size': 7, 'gamma': 0.8281721221427548}. Best is trial 36 with value: 0.04131272524346447.[0m
[32m[I 2025-01-05 23:37:20,991][0m Trial 37 finished with value: 0.0680154903588823 and parameters: {'observation_period_num': 5, 'train_rates': 0.8039574251086905, 'learning_rate': 1.481811459728984e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.7989406518240885}. Best is trial 36 with value: 0.04131272524346447.[0m
[32m[I 2025-01-05 23:39:00,670][0m Trial 38 finished with value: 0.21701503584259435 and parameters: {'observation_period_num': 42, 'train_rates': 0.7715353601793755, 'learning_rate': 0.00012315897319754583, 'batch_size': 84, 'step_size': 3, 'gamma': 0.8591818626135234}. Best is trial 36 with value: 0.04131272524346447.[0m
[32m[I 2025-01-05 23:40:23,657][0m Trial 39 finished with value: 0.1977646345972001 and parameters: {'observation_period_num': 15, 'train_rates': 0.7198629635995702, 'learning_rate': 5.6597302518276204e-05, 'batch_size': 158, 'step_size': 7, 'gamma': 0.7906126707956453}. Best is trial 36 with value: 0.04131272524346447.[0m
[32m[I 2025-01-05 23:41:35,076][0m Trial 40 finished with value: 0.06523151238503506 and parameters: {'observation_period_num': 44, 'train_rates': 0.8063719960736843, 'learning_rate': 0.00011342518266793002, 'batch_size': 131, 'step_size': 5, 'gamma': 0.8304253066948654}. Best is trial 36 with value: 0.04131272524346447.[0m
[32m[I 2025-01-05 23:43:08,916][0m Trial 41 finished with value: 0.05780183006141145 and parameters: {'observation_period_num': 52, 'train_rates': 0.8577729319386034, 'learning_rate': 8.63515291355207e-05, 'batch_size': 105, 'step_size': 8, 'gamma': 0.8221659624824965}. Best is trial 36 with value: 0.04131272524346447.[0m
[32m[I 2025-01-05 23:45:15,263][0m Trial 42 finished with value: 0.04058816575161067 and parameters: {'observation_period_num': 31, 'train_rates': 0.8463223403248662, 'learning_rate': 0.00018177978073534285, 'batch_size': 58, 'step_size': 6, 'gamma': 0.8273552666005559}. Best is trial 42 with value: 0.04058816575161067.[0m
[32m[I 2025-01-05 23:47:34,959][0m Trial 43 finished with value: 0.04650976257968922 and parameters: {'observation_period_num': 35, 'train_rates': 0.8151613865474537, 'learning_rate': 0.0001777791646901579, 'batch_size': 54, 'step_size': 6, 'gamma': 0.8556241872248418}. Best is trial 42 with value: 0.04058816575161067.[0m
[32m[I 2025-01-05 23:49:35,986][0m Trial 44 finished with value: 0.149252249053414 and parameters: {'observation_period_num': 6, 'train_rates': 0.7380825341369162, 'learning_rate': 0.00020506143741871093, 'batch_size': 64, 'step_size': 6, 'gamma': 0.8510025612847564}. Best is trial 42 with value: 0.04058816575161067.[0m
[32m[I 2025-01-05 23:51:44,148][0m Trial 45 finished with value: 0.04840189315914328 and parameters: {'observation_period_num': 33, 'train_rates': 0.7904182047812301, 'learning_rate': 0.000178680465476888, 'batch_size': 55, 'step_size': 4, 'gamma': 0.8342072018474826}. Best is trial 42 with value: 0.04058816575161067.[0m
[32m[I 2025-01-05 23:52:46,330][0m Trial 46 finished with value: 0.20740986531705 and parameters: {'observation_period_num': 26, 'train_rates': 0.7872309045696487, 'learning_rate': 0.00016461672346627397, 'batch_size': 249, 'step_size': 3, 'gamma': 0.8619785047320228}. Best is trial 42 with value: 0.04058816575161067.[0m
[32m[I 2025-01-05 23:55:02,785][0m Trial 47 finished with value: 0.044054705606297005 and parameters: {'observation_period_num': 37, 'train_rates': 0.8230949490409554, 'learning_rate': 0.00018563549675919996, 'batch_size': 54, 'step_size': 4, 'gamma': 0.8413748434803883}. Best is trial 42 with value: 0.04058816575161067.[0m
[32m[I 2025-01-05 23:56:49,301][0m Trial 48 finished with value: 0.03400011573717973 and parameters: {'observation_period_num': 18, 'train_rates': 0.8176524260923114, 'learning_rate': 0.00032732087233840765, 'batch_size': 63, 'step_size': 6, 'gamma': 0.7746076969173901}. Best is trial 48 with value: 0.03400011573717973.[0m
[32m[I 2025-01-05 23:58:52,187][0m Trial 49 finished with value: 0.0735904349122905 and parameters: {'observation_period_num': 16, 'train_rates': 0.8193179425752847, 'learning_rate': 5.830082588895918e-05, 'batch_size': 67, 'step_size': 2, 'gamma': 0.7801478922106596}. Best is trial 48 with value: 0.03400011573717973.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 23:58:52,197][0m A new study created in memory with name: no-name-c3ef7351-7cfb-4ac2-9385-dc761211fd58[0m
[32m[I 2025-01-06 00:00:14,436][0m Trial 0 finished with value: 0.08593874424695969 and parameters: {'observation_period_num': 67, 'train_rates': 0.9470684358803264, 'learning_rate': 0.00031990663247104256, 'batch_size': 216, 'step_size': 1, 'gamma': 0.9443318381395198}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:01:44,679][0m Trial 1 finished with value: 0.1299640938172357 and parameters: {'observation_period_num': 196, 'train_rates': 0.7918599538124192, 'learning_rate': 5.184618242177326e-05, 'batch_size': 106, 'step_size': 8, 'gamma': 0.8937571264852047}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:02:52,543][0m Trial 2 finished with value: 0.24706847549281463 and parameters: {'observation_period_num': 138, 'train_rates': 0.7075534816732043, 'learning_rate': 0.0002091030311447493, 'batch_size': 180, 'step_size': 7, 'gamma': 0.8018830379629636}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:03:58,615][0m Trial 3 finished with value: 0.5673740910555788 and parameters: {'observation_period_num': 239, 'train_rates': 0.6921757251951781, 'learning_rate': 8.56583972620717e-06, 'batch_size': 213, 'step_size': 8, 'gamma': 0.8829257829473166}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:05:01,432][0m Trial 4 finished with value: 0.5623168024544914 and parameters: {'observation_period_num': 163, 'train_rates': 0.6557274352371936, 'learning_rate': 7.363738437941458e-06, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9578508069730507}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:06:34,451][0m Trial 5 finished with value: 0.1495433747768402 and parameters: {'observation_period_num': 97, 'train_rates': 0.942279432711639, 'learning_rate': 0.0003230786322983605, 'batch_size': 199, 'step_size': 1, 'gamma': 0.8863107338168551}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:07:54,763][0m Trial 6 finished with value: 0.6973483251449757 and parameters: {'observation_period_num': 138, 'train_rates': 0.7636704527324154, 'learning_rate': 4.79973221133693e-06, 'batch_size': 134, 'step_size': 3, 'gamma': 0.7993177003205468}. Best is trial 0 with value: 0.08593874424695969.[0m
[32m[I 2025-01-06 00:09:22,522][0m Trial 7 finished with value: 0.06041649264701898 and parameters: {'observation_period_num': 83, 'train_rates': 0.9281355892263864, 'learning_rate': 0.0007527167786633764, 'batch_size': 192, 'step_size': 3, 'gamma': 0.9022313716130943}. Best is trial 7 with value: 0.06041649264701898.[0m
[32m[I 2025-01-06 00:10:34,275][0m Trial 8 finished with value: 0.7075313806533814 and parameters: {'observation_period_num': 179, 'train_rates': 0.6701081831247572, 'learning_rate': 3.0260604668944156e-06, 'batch_size': 183, 'step_size': 15, 'gamma': 0.8214580828940042}. Best is trial 7 with value: 0.06041649264701898.[0m
[32m[I 2025-01-06 00:16:54,833][0m Trial 9 finished with value: 0.0625082066408093 and parameters: {'observation_period_num': 19, 'train_rates': 0.9344456911241232, 'learning_rate': 3.579922269180768e-06, 'batch_size': 17, 'step_size': 12, 'gamma': 0.8544335820793093}. Best is trial 7 with value: 0.06041649264701898.[0m
[32m[I 2025-01-06 00:18:54,146][0m Trial 10 finished with value: 0.06062172001687705 and parameters: {'observation_period_num': 28, 'train_rates': 0.8601200213991025, 'learning_rate': 5.897312883999215e-05, 'batch_size': 70, 'step_size': 4, 'gamma': 0.7590205318477633}. Best is trial 7 with value: 0.06041649264701898.[0m
[32m[I 2025-01-06 00:20:50,223][0m Trial 11 finished with value: 0.03230625545089157 and parameters: {'observation_period_num': 18, 'train_rates': 0.86993153941127, 'learning_rate': 0.000969600155117556, 'batch_size': 64, 'step_size': 4, 'gamma': 0.7545035627956045}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:23:03,683][0m Trial 12 finished with value: 0.08143150719368587 and parameters: {'observation_period_num': 62, 'train_rates': 0.8726240287370686, 'learning_rate': 0.0007663886756278461, 'batch_size': 54, 'step_size': 4, 'gamma': 0.9265700834487448}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:24:14,451][0m Trial 13 finished with value: 0.053943969020547794 and parameters: {'observation_period_num': 79, 'train_rates': 0.8654924640227102, 'learning_rate': 0.0008648520869941049, 'batch_size': 141, 'step_size': 11, 'gamma': 0.7513332933359899}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:25:30,342][0m Trial 14 finished with value: 0.04044743654239608 and parameters: {'observation_period_num': 8, 'train_rates': 0.8591961580187815, 'learning_rate': 0.0001447125719064192, 'batch_size': 130, 'step_size': 11, 'gamma': 0.7512983914041893}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:27:10,812][0m Trial 15 finished with value: 0.03659505886928171 and parameters: {'observation_period_num': 8, 'train_rates': 0.8249973039689674, 'learning_rate': 0.00012084138263445177, 'batch_size': 79, 'step_size': 10, 'gamma': 0.7720649406355987}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:28:26,993][0m Trial 16 finished with value: 0.2201818854902018 and parameters: {'observation_period_num': 50, 'train_rates': 0.7544325027906214, 'learning_rate': 2.148375902474682e-05, 'batch_size': 77, 'step_size': 13, 'gamma': 0.7861174856099626}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:32:43,864][0m Trial 17 finished with value: 0.23432776815182454 and parameters: {'observation_period_num': 39, 'train_rates': 0.987495535763156, 'learning_rate': 1.0917498114228416e-06, 'batch_size': 25, 'step_size': 10, 'gamma': 0.8424562383012343}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:34:02,490][0m Trial 18 finished with value: 0.19168577383628443 and parameters: {'observation_period_num': 111, 'train_rates': 0.6171629233095596, 'learning_rate': 0.00012205182977460528, 'batch_size': 97, 'step_size': 6, 'gamma': 0.980742171118318}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:36:10,682][0m Trial 19 finished with value: 0.061924457780627724 and parameters: {'observation_period_num': 14, 'train_rates': 0.824177195564585, 'learning_rate': 1.7947082845368985e-05, 'batch_size': 54, 'step_size': 9, 'gamma': 0.7748643443247276}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:38:48,400][0m Trial 20 finished with value: 0.04738071797465956 and parameters: {'observation_period_num': 42, 'train_rates': 0.8219648734766916, 'learning_rate': 6.987356919174474e-05, 'batch_size': 41, 'step_size': 14, 'gamma': 0.8235424958707295}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:40:19,385][0m Trial 21 finished with value: 0.0394588567985174 and parameters: {'observation_period_num': 5, 'train_rates': 0.8943962884682594, 'learning_rate': 0.00014389956811829355, 'batch_size': 124, 'step_size': 11, 'gamma': 0.7667276988186197}. Best is trial 11 with value: 0.03230625545089157.[0m
[32m[I 2025-01-06 00:41:53,100][0m Trial 22 finished with value: 0.02832679405124693 and parameters: {'observation_period_num': 5, 'train_rates': 0.8929205758252304, 'learning_rate': 0.0003945828479062341, 'batch_size': 103, 'step_size': 10, 'gamma': 0.7776411222693225}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:43:22,632][0m Trial 23 finished with value: 0.04278008701900641 and parameters: {'observation_period_num': 37, 'train_rates': 0.8169102398911361, 'learning_rate': 0.00039883930476309324, 'batch_size': 89, 'step_size': 10, 'gamma': 0.7822303567847187}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:44:56,195][0m Trial 24 finished with value: 0.05499047235302303 and parameters: {'observation_period_num': 57, 'train_rates': 0.9127774220139808, 'learning_rate': 0.0003489012202645984, 'batch_size': 110, 'step_size': 9, 'gamma': 0.8118252504498871}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:46:47,642][0m Trial 25 finished with value: 0.03107806108891964 and parameters: {'observation_period_num': 29, 'train_rates': 0.9889170640664943, 'learning_rate': 0.0005130605483681509, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8414847441579121}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:47:57,765][0m Trial 26 finished with value: 0.04335193708539009 and parameters: {'observation_period_num': 30, 'train_rates': 0.9859903134775662, 'learning_rate': 0.0005406116563866062, 'batch_size': 156, 'step_size': 5, 'gamma': 0.8395873490558098}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:50:11,565][0m Trial 27 finished with value: 0.10899190389248262 and parameters: {'observation_period_num': 113, 'train_rates': 0.9709675170711507, 'learning_rate': 0.0009095226020130407, 'batch_size': 59, 'step_size': 6, 'gamma': 0.8649139743221149}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:53:04,172][0m Trial 28 finished with value: 0.060796217620372774 and parameters: {'observation_period_num': 77, 'train_rates': 0.8994023833987251, 'learning_rate': 0.00021397398428225906, 'batch_size': 36, 'step_size': 2, 'gamma': 0.7925260304079115}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:54:06,550][0m Trial 29 finished with value: 0.05973156914114952 and parameters: {'observation_period_num': 65, 'train_rates': 0.9496207102408596, 'learning_rate': 0.0005101894323790952, 'batch_size': 162, 'step_size': 7, 'gamma': 0.8322996629658959}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:56:01,940][0m Trial 30 finished with value: 0.05255913248687398 and parameters: {'observation_period_num': 27, 'train_rates': 0.9651811363582594, 'learning_rate': 0.00027106401336339487, 'batch_size': 89, 'step_size': 5, 'gamma': 0.8070856338371327}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:57:27,790][0m Trial 31 finished with value: 0.029523699028679164 and parameters: {'observation_period_num': 5, 'train_rates': 0.835751196115403, 'learning_rate': 0.0004551161380217582, 'batch_size': 78, 'step_size': 9, 'gamma': 0.7734346034652768}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 00:59:02,332][0m Trial 32 finished with value: 0.1763161849485685 and parameters: {'observation_period_num': 23, 'train_rates': 0.7759472666426016, 'learning_rate': 0.0005528742197021452, 'batch_size': 109, 'step_size': 7, 'gamma': 0.7705043923921905}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:01:08,239][0m Trial 33 finished with value: 0.059612506753708945 and parameters: {'observation_period_num': 45, 'train_rates': 0.8860631591121794, 'learning_rate': 0.0005140234913595315, 'batch_size': 60, 'step_size': 9, 'gamma': 0.7873654022743033}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:02:28,323][0m Trial 34 finished with value: 0.2722065328381339 and parameters: {'observation_period_num': 238, 'train_rates': 0.7373950263415744, 'learning_rate': 0.00023575578612852107, 'batch_size': 116, 'step_size': 8, 'gamma': 0.7625974625257916}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:04:03,763][0m Trial 35 finished with value: 0.033035005719847035 and parameters: {'observation_period_num': 6, 'train_rates': 0.7982296515753877, 'learning_rate': 0.000990327979740557, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8024139952121385}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:05:33,574][0m Trial 36 finished with value: 0.04471018799058683 and parameters: {'observation_period_num': 22, 'train_rates': 0.8448466796023706, 'learning_rate': 7.496450272325483e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9104852515796769}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:08:40,899][0m Trial 37 finished with value: 0.05132508647330569 and parameters: {'observation_period_num': 45, 'train_rates': 0.9171106550954028, 'learning_rate': 3.788458273438648e-05, 'batch_size': 38, 'step_size': 8, 'gamma': 0.8192720402944935}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:10:03,469][0m Trial 38 finished with value: 0.08290608911351724 and parameters: {'observation_period_num': 212, 'train_rates': 0.8396469590288148, 'learning_rate': 0.00037036116926714553, 'batch_size': 103, 'step_size': 3, 'gamma': 0.8658093082321252}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:11:28,614][0m Trial 39 finished with value: 0.1136042797502051 and parameters: {'observation_period_num': 159, 'train_rates': 0.7980361813316128, 'learning_rate': 0.000629059559024327, 'batch_size': 252, 'step_size': 4, 'gamma': 0.7809463452838488}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:14:18,934][0m Trial 40 finished with value: 0.06637080386281013 and parameters: {'observation_period_num': 54, 'train_rates': 0.9551392432234909, 'learning_rate': 0.0001981984364009571, 'batch_size': 45, 'step_size': 6, 'gamma': 0.8812691025485768}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:16:18,609][0m Trial 41 finished with value: 0.034298387714303456 and parameters: {'observation_period_num': 15, 'train_rates': 0.7963828375663674, 'learning_rate': 0.0008834440467324879, 'batch_size': 66, 'step_size': 7, 'gamma': 0.8004274118368878}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:18:09,575][0m Trial 42 finished with value: 0.1429995145936708 and parameters: {'observation_period_num': 5, 'train_rates': 0.7279407165599974, 'learning_rate': 0.0009962901801874766, 'batch_size': 75, 'step_size': 9, 'gamma': 0.7965701542605258}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:20:01,812][0m Trial 43 finished with value: 0.04318688945238327 and parameters: {'observation_period_num': 33, 'train_rates': 0.8822367346718122, 'learning_rate': 0.00041864742685920523, 'batch_size': 82, 'step_size': 8, 'gamma': 0.8105427743670243}. Best is trial 22 with value: 0.02832679405124693.[0m
Early stopping at epoch 54
[32m[I 2025-01-06 01:21:10,154][0m Trial 44 finished with value: 0.21649471183633218 and parameters: {'observation_period_num': 19, 'train_rates': 0.7789831435284984, 'learning_rate': 0.0006483419216169142, 'batch_size': 100, 'step_size': 1, 'gamma': 0.7654047061574452}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:23:06,961][0m Trial 45 finished with value: 0.03243318416513084 and parameters: {'observation_period_num': 18, 'train_rates': 0.8484954847590651, 'learning_rate': 0.00031488924893284824, 'batch_size': 69, 'step_size': 7, 'gamma': 0.7516146914222275}. Best is trial 22 with value: 0.02832679405124693.[0m
Early stopping at epoch 96
[32m[I 2025-01-06 01:24:44,578][0m Trial 46 finished with value: 0.05948237205545107 and parameters: {'observation_period_num': 24, 'train_rates': 0.8536362340335995, 'learning_rate': 0.000298911167437444, 'batch_size': 121, 'step_size': 2, 'gamma': 0.7517072252972216}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:27:09,397][0m Trial 47 finished with value: 0.06509918719530106 and parameters: {'observation_period_num': 90, 'train_rates': 0.911103951798375, 'learning_rate': 0.0001729107491574726, 'batch_size': 51, 'step_size': 12, 'gamma': 0.7586461712361634}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:31:04,752][0m Trial 48 finished with value: 0.05183174639548126 and parameters: {'observation_period_num': 72, 'train_rates': 0.9343059958896462, 'learning_rate': 0.00010848832585267482, 'batch_size': 29, 'step_size': 6, 'gamma': 0.7744139032105852}. Best is trial 22 with value: 0.02832679405124693.[0m
[32m[I 2025-01-06 01:33:07,134][0m Trial 49 finished with value: 0.04602958431926899 and parameters: {'observation_period_num': 35, 'train_rates': 0.8707247753577729, 'learning_rate': 0.0002882475499635333, 'batch_size': 62, 'step_size': 4, 'gamma': 0.9403317018166352}. Best is trial 22 with value: 0.02832679405124693.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-06 01:33:07,144][0m A new study created in memory with name: no-name-de601ab0-4ad6-4c94-8f5b-19615dcaceb0[0m
[32m[I 2025-01-06 01:34:34,529][0m Trial 0 finished with value: 0.2320906718962319 and parameters: {'observation_period_num': 23, 'train_rates': 0.6523298834172749, 'learning_rate': 3.7196270741347626e-06, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9850082333173209}. Best is trial 0 with value: 0.2320906718962319.[0m
[32m[I 2025-01-06 01:36:16,317][0m Trial 1 finished with value: 0.10364940760124151 and parameters: {'observation_period_num': 71, 'train_rates': 0.784349210315545, 'learning_rate': 6.249513093382597e-05, 'batch_size': 62, 'step_size': 3, 'gamma': 0.8554561476888051}. Best is trial 1 with value: 0.10364940760124151.[0m
[32m[I 2025-01-06 01:37:42,577][0m Trial 2 finished with value: 0.10152459179653842 and parameters: {'observation_period_num': 136, 'train_rates': 0.9398293112608251, 'learning_rate': 0.00044531128376686595, 'batch_size': 117, 'step_size': 14, 'gamma': 0.9892227743195937}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:40:27,768][0m Trial 3 finished with value: 0.23972207656610045 and parameters: {'observation_period_num': 125, 'train_rates': 0.7446139863665655, 'learning_rate': 9.930242949091961e-05, 'batch_size': 32, 'step_size': 1, 'gamma': 0.9873382152970698}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:41:37,163][0m Trial 4 finished with value: 0.23186074250910899 and parameters: {'observation_period_num': 126, 'train_rates': 0.6704678246008173, 'learning_rate': 9.295805734223972e-05, 'batch_size': 229, 'step_size': 15, 'gamma': 0.825278823904696}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:42:28,735][0m Trial 5 finished with value: 0.2671845274860034 and parameters: {'observation_period_num': 117, 'train_rates': 0.6053773877017284, 'learning_rate': 0.00018561666428496327, 'batch_size': 180, 'step_size': 2, 'gamma': 0.8827017642172531}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:43:37,747][0m Trial 6 finished with value: 0.16212983354043908 and parameters: {'observation_period_num': 18, 'train_rates': 0.6832629478462827, 'learning_rate': 6.235792383517156e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.9710669442241165}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:45:02,060][0m Trial 7 finished with value: 0.29284713437428345 and parameters: {'observation_period_num': 213, 'train_rates': 0.878624577769586, 'learning_rate': 6.638885487026617e-06, 'batch_size': 117, 'step_size': 9, 'gamma': 0.8519481256591545}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:46:31,657][0m Trial 8 finished with value: 0.2213621605539097 and parameters: {'observation_period_num': 118, 'train_rates': 0.9252717736838272, 'learning_rate': 2.324629225358736e-05, 'batch_size': 133, 'step_size': 4, 'gamma': 0.80768545290293}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:47:43,170][0m Trial 9 finished with value: 0.3247176399287836 and parameters: {'observation_period_num': 203, 'train_rates': 0.8171195432886509, 'learning_rate': 8.872959534347796e-06, 'batch_size': 133, 'step_size': 12, 'gamma': 0.8010365609712968}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:48:41,462][0m Trial 10 finished with value: 0.11729416996240616 and parameters: {'observation_period_num': 175, 'train_rates': 0.9827365351410011, 'learning_rate': 0.0005979663328106198, 'batch_size': 255, 'step_size': 10, 'gamma': 0.923689753858459}. Best is trial 2 with value: 0.10152459179653842.[0m
[32m[I 2025-01-06 01:50:34,472][0m Trial 11 finished with value: 0.08006775788581412 and parameters: {'observation_period_num': 64, 'train_rates': 0.7972581861592504, 'learning_rate': 0.0008320660953561872, 'batch_size': 65, 'step_size': 6, 'gamma': 0.9042879714164653}. Best is trial 11 with value: 0.08006775788581412.[0m
[32m[I 2025-01-06 01:52:13,983][0m Trial 12 finished with value: 0.11650996621359479 and parameters: {'observation_period_num': 58, 'train_rates': 0.8633743340598594, 'learning_rate': 0.0008900674111146659, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9177892438896748}. Best is trial 11 with value: 0.08006775788581412.[0m
[32m[I 2025-01-06 01:53:41,370][0m Trial 13 finished with value: 0.07649588584899902 and parameters: {'observation_period_num': 76, 'train_rates': 0.9764207700264177, 'learning_rate': 0.00033868772921632036, 'batch_size': 96, 'step_size': 7, 'gamma': 0.9378104759991253}. Best is trial 13 with value: 0.07649588584899902.[0m
[32m[I 2025-01-06 01:56:16,601][0m Trial 14 finished with value: 0.5273332730673722 and parameters: {'observation_period_num': 76, 'train_rates': 0.7401810494188387, 'learning_rate': 1.1005435288376311e-06, 'batch_size': 38, 'step_size': 7, 'gamma': 0.9278976606627789}. Best is trial 13 with value: 0.07649588584899902.[0m
[32m[I 2025-01-06 01:57:43,578][0m Trial 15 finished with value: 0.05481772869825363 and parameters: {'observation_period_num': 48, 'train_rates': 0.9848169833877873, 'learning_rate': 0.0002805967762039822, 'batch_size': 95, 'step_size': 7, 'gamma': 0.7513633290291137}. Best is trial 15 with value: 0.05481772869825363.[0m
[32m[I 2025-01-06 01:59:05,839][0m Trial 16 finished with value: 0.09738793224096298 and parameters: {'observation_period_num': 252, 'train_rates': 0.9844946906302741, 'learning_rate': 0.00024905985599150304, 'batch_size': 175, 'step_size': 8, 'gamma': 0.7640507038267559}. Best is trial 15 with value: 0.05481772869825363.[0m
[32m[I 2025-01-06 02:00:24,843][0m Trial 17 finished with value: 0.08587587883938914 and parameters: {'observation_period_num': 36, 'train_rates': 0.9212735686739771, 'learning_rate': 0.0002757668603111042, 'batch_size': 95, 'step_size': 11, 'gamma': 0.9549020078505541}. Best is trial 15 with value: 0.05481772869825363.[0m
[32m[I 2025-01-06 02:01:11,644][0m Trial 18 finished with value: 0.16261973932426046 and parameters: {'observation_period_num': 92, 'train_rates': 0.8719133695687988, 'learning_rate': 2.6080028561941262e-05, 'batch_size': 161, 'step_size': 8, 'gamma': 0.7517719287649876}. Best is trial 15 with value: 0.05481772869825363.[0m
[32m[I 2025-01-06 02:02:16,193][0m Trial 19 finished with value: 0.037963245525560546 and parameters: {'observation_period_num': 5, 'train_rates': 0.9618412792746778, 'learning_rate': 0.00015447313602095535, 'batch_size': 107, 'step_size': 12, 'gamma': 0.946818568263664}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:03:09,154][0m Trial 20 finished with value: 0.05288521047024166 and parameters: {'observation_period_num': 40, 'train_rates': 0.9419243680865037, 'learning_rate': 0.00014234066402133032, 'batch_size': 146, 'step_size': 13, 'gamma': 0.7743857517181107}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:03:53,828][0m Trial 21 finished with value: 0.03914688132814507 and parameters: {'observation_period_num': 6, 'train_rates': 0.9318760728112567, 'learning_rate': 0.00011560606508468381, 'batch_size': 153, 'step_size': 13, 'gamma': 0.7764738568436123}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:04:40,355][0m Trial 22 finished with value: 0.04047089465496483 and parameters: {'observation_period_num': 7, 'train_rates': 0.9005033396429639, 'learning_rate': 0.0001368713660118135, 'batch_size': 155, 'step_size': 13, 'gamma': 0.7789413787364072}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:05:21,765][0m Trial 23 finished with value: 0.059251331051604614 and parameters: {'observation_period_num': 5, 'train_rates': 0.9024543724393647, 'learning_rate': 5.4864681380309084e-05, 'batch_size': 217, 'step_size': 12, 'gamma': 0.7873795100365418}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:06:03,569][0m Trial 24 finished with value: 0.05759982640544573 and parameters: {'observation_period_num': 11, 'train_rates': 0.8389667690954462, 'learning_rate': 4.232649473031538e-05, 'batch_size': 158, 'step_size': 13, 'gamma': 0.8309974885782107}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:06:45,396][0m Trial 25 finished with value: 0.09183968526269326 and parameters: {'observation_period_num': 29, 'train_rates': 0.9012329976100706, 'learning_rate': 1.614323276639167e-05, 'batch_size': 195, 'step_size': 11, 'gamma': 0.8757961829120667}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:07:42,830][0m Trial 26 finished with value: 0.03858815289877512 and parameters: {'observation_period_num': 7, 'train_rates': 0.9515440867235601, 'learning_rate': 0.00013060638366718254, 'batch_size': 114, 'step_size': 14, 'gamma': 0.827352462308683}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:08:34,462][0m Trial 27 finished with value: 0.08611257471468138 and parameters: {'observation_period_num': 95, 'train_rates': 0.9520002219031366, 'learning_rate': 9.629483467248282e-05, 'batch_size': 124, 'step_size': 15, 'gamma': 0.8275237387243745}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:09:38,483][0m Trial 28 finished with value: 0.0551494714605996 and parameters: {'observation_period_num': 46, 'train_rates': 0.9563958368290119, 'learning_rate': 0.0001580481312423366, 'batch_size': 115, 'step_size': 10, 'gamma': 0.8909652308803405}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:11:33,822][0m Trial 29 finished with value: 0.05179063991514805 and parameters: {'observation_period_num': 27, 'train_rates': 0.8540006362470982, 'learning_rate': 1.4175481467824752e-05, 'batch_size': 52, 'step_size': 14, 'gamma': 0.8596286686227097}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:13:04,227][0m Trial 30 finished with value: 0.3685604765454763 and parameters: {'observation_period_num': 151, 'train_rates': 0.9140500919241477, 'learning_rate': 2.335321895099412e-06, 'batch_size': 79, 'step_size': 14, 'gamma': 0.8139237553691867}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:14:11,909][0m Trial 31 finished with value: 0.03979173799355825 and parameters: {'observation_period_num': 8, 'train_rates': 0.8961236361611342, 'learning_rate': 0.00012469767195885905, 'batch_size': 150, 'step_size': 13, 'gamma': 0.7917361619875137}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:15:11,572][0m Trial 32 finished with value: 0.05969376489520073 and parameters: {'observation_period_num': 24, 'train_rates': 0.9588967204276643, 'learning_rate': 3.9589870258265614e-05, 'batch_size': 141, 'step_size': 12, 'gamma': 0.788678134985288}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:16:17,254][0m Trial 33 finished with value: 0.04173955626007336 and parameters: {'observation_period_num': 5, 'train_rates': 0.8864534318139273, 'learning_rate': 8.751493292850412e-05, 'batch_size': 109, 'step_size': 15, 'gamma': 0.8443464987776694}. Best is trial 19 with value: 0.037963245525560546.[0m
[32m[I 2025-01-06 02:17:18,958][0m Trial 34 finished with value: 0.03561878987196563 and parameters: {'observation_period_num': 23, 'train_rates': 0.9296033310502639, 'learning_rate': 0.00046679806566110376, 'batch_size': 173, 'step_size': 11, 'gamma': 0.796698539951921}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:18:03,652][0m Trial 35 finished with value: 0.03574174915750821 and parameters: {'observation_period_num': 25, 'train_rates': 0.93341357481924, 'learning_rate': 0.0005289835653780507, 'batch_size': 172, 'step_size': 11, 'gamma': 0.8150677735360563}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:18:43,731][0m Trial 36 finished with value: 0.054919809103012085 and parameters: {'observation_period_num': 52, 'train_rates': 0.9520070213995481, 'learning_rate': 0.0005110156762055463, 'batch_size': 175, 'step_size': 10, 'gamma': 0.8348378424503939}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:19:15,077][0m Trial 37 finished with value: 0.16392218726205415 and parameters: {'observation_period_num': 30, 'train_rates': 0.722038982649702, 'learning_rate': 0.00041670090068343847, 'batch_size': 222, 'step_size': 11, 'gamma': 0.8140317172776007}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:19:51,963][0m Trial 38 finished with value: 0.03592837389534826 and parameters: {'observation_period_num': 23, 'train_rates': 0.8391916156086909, 'learning_rate': 0.0006471054543605922, 'batch_size': 203, 'step_size': 9, 'gamma': 0.8648515672527227}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:20:26,743][0m Trial 39 finished with value: 0.07344653402775708 and parameters: {'observation_period_num': 99, 'train_rates': 0.8278805481727085, 'learning_rate': 0.0007219413625111319, 'batch_size': 210, 'step_size': 9, 'gamma': 0.8635343512750392}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:21:01,290][0m Trial 40 finished with value: 0.06664589050010002 and parameters: {'observation_period_num': 63, 'train_rates': 0.80012313767602, 'learning_rate': 0.0009823917709808649, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8845926630150597}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:21:33,958][0m Trial 41 finished with value: 0.17877751420582494 and parameters: {'observation_period_num': 21, 'train_rates': 0.7672788080227559, 'learning_rate': 0.000209167758532334, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8526429249921263}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:22:14,535][0m Trial 42 finished with value: 0.05394770950078964 and parameters: {'observation_period_num': 38, 'train_rates': 0.9687405406374853, 'learning_rate': 0.0004216775658518173, 'batch_size': 183, 'step_size': 12, 'gamma': 0.8420477089285147}. Best is trial 34 with value: 0.03561878987196563.[0m
[32m[I 2025-01-06 02:22:56,307][0m Trial 43 finished with value: 0.03367915915118323 and parameters: {'observation_period_num': 21, 'train_rates': 0.9356428297728092, 'learning_rate': 0.0005821071483316139, 'batch_size': 168, 'step_size': 10, 'gamma': 0.812829420808725}. Best is trial 43 with value: 0.03367915915118323.[0m
[32m[I 2025-01-06 02:23:36,357][0m Trial 44 finished with value: 0.03607094569786175 and parameters: {'observation_period_num': 19, 'train_rates': 0.9243065759516079, 'learning_rate': 0.0005424246020869323, 'batch_size': 171, 'step_size': 10, 'gamma': 0.9757022417417109}. Best is trial 43 with value: 0.03367915915118323.[0m
[32m[I 2025-01-06 02:24:15,359][0m Trial 45 finished with value: 0.04317301407331702 and parameters: {'observation_period_num': 20, 'train_rates': 0.918498107263348, 'learning_rate': 0.0006410808200819412, 'batch_size': 187, 'step_size': 10, 'gamma': 0.9770994859551505}. Best is trial 43 with value: 0.03367915915118323.[0m
[32m[I 2025-01-06 02:24:53,262][0m Trial 46 finished with value: 0.0714236757882127 and parameters: {'observation_period_num': 56, 'train_rates': 0.8490971134570106, 'learning_rate': 0.0003659845628177146, 'batch_size': 168, 'step_size': 9, 'gamma': 0.9005865001907681}. Best is trial 43 with value: 0.03367915915118323.[0m
[32m[I 2025-01-06 02:25:22,446][0m Trial 47 finished with value: 0.18072982987290934 and parameters: {'observation_period_num': 69, 'train_rates': 0.6047704711252546, 'learning_rate': 0.0005725360868433663, 'batch_size': 198, 'step_size': 9, 'gamma': 0.8006048956729168}. Best is trial 43 with value: 0.03367915915118323.[0m
[32m[I 2025-01-06 02:25:56,978][0m Trial 48 finished with value: 0.06278644874691963 and parameters: {'observation_period_num': 82, 'train_rates': 0.8782578154986567, 'learning_rate': 0.0009701479039579487, 'batch_size': 238, 'step_size': 10, 'gamma': 0.8144522252652747}. Best is trial 43 with value: 0.03367915915118323.[0m
[32m[I 2025-01-06 02:26:34,039][0m Trial 49 finished with value: 0.1493740520297095 and parameters: {'observation_period_num': 43, 'train_rates': 0.6303591904971031, 'learning_rate': 0.000497223474018765, 'batch_size': 168, 'step_size': 11, 'gamma': 0.8016902137578701}. Best is trial 43 with value: 0.03367915915118323.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.9049211237928657, 'learning_rate': 0.00022729458521905302, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8593371678519443}
Epoch 1/300, trend Loss: 0.1965 | 0.0894
Epoch 2/300, trend Loss: 0.1187 | 0.0749
Epoch 3/300, trend Loss: 0.1100 | 0.0669
Epoch 4/300, trend Loss: 0.1041 | 0.0657
Epoch 5/300, trend Loss: 0.0976 | 0.0553
Epoch 6/300, trend Loss: 0.0942 | 0.0515
Epoch 7/300, trend Loss: 0.0905 | 0.0506
Epoch 8/300, trend Loss: 0.0875 | 0.0491
Epoch 9/300, trend Loss: 0.0852 | 0.0458
Epoch 10/300, trend Loss: 0.0820 | 0.0429
Epoch 11/300, trend Loss: 0.0792 | 0.0401
Epoch 12/300, trend Loss: 0.0771 | 0.0387
Epoch 13/300, trend Loss: 0.0754 | 0.0368
Epoch 14/300, trend Loss: 0.0744 | 0.0358
Epoch 15/300, trend Loss: 0.0733 | 0.0350
Epoch 16/300, trend Loss: 0.0722 | 0.0326
Epoch 17/300, trend Loss: 0.0714 | 0.0316
Epoch 18/300, trend Loss: 0.0704 | 0.0306
Epoch 19/300, trend Loss: 0.0694 | 0.0286
Epoch 20/300, trend Loss: 0.0683 | 0.0278
Epoch 21/300, trend Loss: 0.0673 | 0.0272
Epoch 22/300, trend Loss: 0.0664 | 0.0267
Epoch 23/300, trend Loss: 0.0657 | 0.0265
Epoch 24/300, trend Loss: 0.0650 | 0.0262
Epoch 25/300, trend Loss: 0.0643 | 0.0258
Epoch 26/300, trend Loss: 0.0636 | 0.0257
Epoch 27/300, trend Loss: 0.0630 | 0.0255
Epoch 28/300, trend Loss: 0.0623 | 0.0249
Epoch 29/300, trend Loss: 0.0618 | 0.0249
Epoch 30/300, trend Loss: 0.0614 | 0.0249
Epoch 31/300, trend Loss: 0.0610 | 0.0245
Epoch 32/300, trend Loss: 0.0606 | 0.0244
Epoch 33/300, trend Loss: 0.0603 | 0.0244
Epoch 34/300, trend Loss: 0.0600 | 0.0243
Epoch 35/300, trend Loss: 0.0597 | 0.0243
Epoch 36/300, trend Loss: 0.0594 | 0.0243
Epoch 37/300, trend Loss: 0.0592 | 0.0243
Epoch 38/300, trend Loss: 0.0591 | 0.0244
Epoch 39/300, trend Loss: 0.0590 | 0.0244
Epoch 40/300, trend Loss: 0.0588 | 0.0246
Epoch 41/300, trend Loss: 0.0588 | 0.0250
Epoch 42/300, trend Loss: 0.0586 | 0.0255
Epoch 43/300, trend Loss: 0.0583 | 0.0254
Epoch 44/300, trend Loss: 0.0580 | 0.0255
Epoch 45/300, trend Loss: 0.0578 | 0.0256
Epoch 46/300, trend Loss: 0.0576 | 0.0246
Epoch 47/300, trend Loss: 0.0574 | 0.0246
Epoch 48/300, trend Loss: 0.0573 | 0.0246
Epoch 49/300, trend Loss: 0.0571 | 0.0238
Epoch 50/300, trend Loss: 0.0570 | 0.0238
Epoch 51/300, trend Loss: 0.0569 | 0.0238
Epoch 52/300, trend Loss: 0.0567 | 0.0235
Epoch 53/300, trend Loss: 0.0566 | 0.0234
Epoch 54/300, trend Loss: 0.0565 | 0.0234
Epoch 55/300, trend Loss: 0.0564 | 0.0234
Epoch 56/300, trend Loss: 0.0563 | 0.0234
Epoch 57/300, trend Loss: 0.0563 | 0.0233
Epoch 58/300, trend Loss: 0.0561 | 0.0236
Epoch 59/300, trend Loss: 0.0561 | 0.0235
Epoch 60/300, trend Loss: 0.0560 | 0.0235
Epoch 61/300, trend Loss: 0.0559 | 0.0238
Epoch 62/300, trend Loss: 0.0558 | 0.0238
Epoch 63/300, trend Loss: 0.0558 | 0.0237
Epoch 64/300, trend Loss: 0.0557 | 0.0241
Epoch 65/300, trend Loss: 0.0556 | 0.0241
Epoch 66/300, trend Loss: 0.0555 | 0.0241
Epoch 67/300, trend Loss: 0.0555 | 0.0245
Epoch 68/300, trend Loss: 0.0554 | 0.0245
Epoch 69/300, trend Loss: 0.0553 | 0.0244
Epoch 70/300, trend Loss: 0.0552 | 0.0248
Epoch 71/300, trend Loss: 0.0552 | 0.0248
Epoch 72/300, trend Loss: 0.0551 | 0.0248
Epoch 73/300, trend Loss: 0.0550 | 0.0250
Epoch 74/300, trend Loss: 0.0550 | 0.0250
Epoch 75/300, trend Loss: 0.0549 | 0.0250
Epoch 76/300, trend Loss: 0.0549 | 0.0250
Epoch 77/300, trend Loss: 0.0548 | 0.0251
Epoch 78/300, trend Loss: 0.0548 | 0.0250
Epoch 79/300, trend Loss: 0.0547 | 0.0248
Epoch 80/300, trend Loss: 0.0547 | 0.0249
Epoch 81/300, trend Loss: 0.0546 | 0.0248
Epoch 82/300, trend Loss: 0.0546 | 0.0246
Epoch 83/300, trend Loss: 0.0546 | 0.0246
Epoch 84/300, trend Loss: 0.0545 | 0.0245
Epoch 85/300, trend Loss: 0.0545 | 0.0243
Epoch 86/300, trend Loss: 0.0545 | 0.0243
Epoch 87/300, trend Loss: 0.0544 | 0.0243
Epoch 88/300, trend Loss: 0.0544 | 0.0241
Epoch 89/300, trend Loss: 0.0544 | 0.0241
Epoch 90/300, trend Loss: 0.0543 | 0.0241
Epoch 91/300, trend Loss: 0.0543 | 0.0240
Epoch 92/300, trend Loss: 0.0543 | 0.0239
Epoch 93/300, trend Loss: 0.0543 | 0.0239
Epoch 94/300, trend Loss: 0.0542 | 0.0238
Epoch 95/300, trend Loss: 0.0542 | 0.0238
Epoch 96/300, trend Loss: 0.0542 | 0.0238
Epoch 97/300, trend Loss: 0.0542 | 0.0237
Epoch 98/300, trend Loss: 0.0542 | 0.0237
Epoch 99/300, trend Loss: 0.0541 | 0.0237
Epoch 100/300, trend Loss: 0.0541 | 0.0236
Epoch 101/300, trend Loss: 0.0541 | 0.0236
Epoch 102/300, trend Loss: 0.0541 | 0.0236
Epoch 103/300, trend Loss: 0.0541 | 0.0236
Epoch 104/300, trend Loss: 0.0541 | 0.0236
Epoch 105/300, trend Loss: 0.0541 | 0.0236
Epoch 106/300, trend Loss: 0.0540 | 0.0236
Epoch 107/300, trend Loss: 0.0540 | 0.0236
Epoch 108/300, trend Loss: 0.0540 | 0.0236
Epoch 109/300, trend Loss: 0.0540 | 0.0236
Epoch 110/300, trend Loss: 0.0540 | 0.0236
Epoch 111/300, trend Loss: 0.0540 | 0.0236
Epoch 112/300, trend Loss: 0.0540 | 0.0237
Epoch 113/300, trend Loss: 0.0540 | 0.0237
Epoch 114/300, trend Loss: 0.0540 | 0.0237
Epoch 115/300, trend Loss: 0.0540 | 0.0237
Epoch 116/300, trend Loss: 0.0540 | 0.0237
Epoch 117/300, trend Loss: 0.0539 | 0.0237
Epoch 118/300, trend Loss: 0.0539 | 0.0237
Epoch 119/300, trend Loss: 0.0539 | 0.0237
Epoch 120/300, trend Loss: 0.0539 | 0.0237
Epoch 121/300, trend Loss: 0.0539 | 0.0238
Epoch 122/300, trend Loss: 0.0539 | 0.0237
Epoch 123/300, trend Loss: 0.0539 | 0.0237
Epoch 124/300, trend Loss: 0.0539 | 0.0238
Epoch 125/300, trend Loss: 0.0539 | 0.0238
Epoch 126/300, trend Loss: 0.0539 | 0.0237
Epoch 127/300, trend Loss: 0.0539 | 0.0238
Epoch 128/300, trend Loss: 0.0539 | 0.0238
Epoch 129/300, trend Loss: 0.0539 | 0.0237
Epoch 130/300, trend Loss: 0.0538 | 0.0237
Epoch 131/300, trend Loss: 0.0538 | 0.0237
Epoch 132/300, trend Loss: 0.0538 | 0.0237
Epoch 133/300, trend Loss: 0.0538 | 0.0237
Epoch 134/300, trend Loss: 0.0538 | 0.0237
Epoch 135/300, trend Loss: 0.0538 | 0.0237
Epoch 136/300, trend Loss: 0.0538 | 0.0237
Epoch 137/300, trend Loss: 0.0538 | 0.0237
Epoch 138/300, trend Loss: 0.0538 | 0.0237
Epoch 139/300, trend Loss: 0.0538 | 0.0237
Epoch 140/300, trend Loss: 0.0538 | 0.0237
Epoch 141/300, trend Loss: 0.0538 | 0.0237
Epoch 142/300, trend Loss: 0.0538 | 0.0237
Epoch 143/300, trend Loss: 0.0538 | 0.0237
Epoch 144/300, trend Loss: 0.0538 | 0.0237
Epoch 145/300, trend Loss: 0.0538 | 0.0237
Epoch 146/300, trend Loss: 0.0538 | 0.0237
Epoch 147/300, trend Loss: 0.0538 | 0.0237
Epoch 148/300, trend Loss: 0.0538 | 0.0237
Epoch 149/300, trend Loss: 0.0538 | 0.0237
Epoch 150/300, trend Loss: 0.0538 | 0.0237
Epoch 151/300, trend Loss: 0.0538 | 0.0237
Epoch 152/300, trend Loss: 0.0538 | 0.0237
Epoch 153/300, trend Loss: 0.0538 | 0.0237
Epoch 154/300, trend Loss: 0.0538 | 0.0237
Epoch 155/300, trend Loss: 0.0538 | 0.0237
Epoch 156/300, trend Loss: 0.0538 | 0.0237
Epoch 157/300, trend Loss: 0.0538 | 0.0237
Epoch 158/300, trend Loss: 0.0538 | 0.0237
Epoch 159/300, trend Loss: 0.0538 | 0.0237
Epoch 160/300, trend Loss: 0.0538 | 0.0237
Epoch 161/300, trend Loss: 0.0538 | 0.0237
Epoch 162/300, trend Loss: 0.0538 | 0.0237
Epoch 163/300, trend Loss: 0.0538 | 0.0237
Epoch 164/300, trend Loss: 0.0538 | 0.0237
Epoch 165/300, trend Loss: 0.0538 | 0.0237
Epoch 166/300, trend Loss: 0.0538 | 0.0237
Epoch 167/300, trend Loss: 0.0538 | 0.0237
Epoch 168/300, trend Loss: 0.0538 | 0.0237
Epoch 169/300, trend Loss: 0.0538 | 0.0237
Epoch 170/300, trend Loss: 0.0538 | 0.0237
Epoch 171/300, trend Loss: 0.0538 | 0.0237
Epoch 172/300, trend Loss: 0.0538 | 0.0237
Epoch 173/300, trend Loss: 0.0538 | 0.0237
Epoch 174/300, trend Loss: 0.0538 | 0.0237
Epoch 175/300, trend Loss: 0.0538 | 0.0237
Epoch 176/300, trend Loss: 0.0538 | 0.0237
Epoch 177/300, trend Loss: 0.0538 | 0.0237
Epoch 178/300, trend Loss: 0.0538 | 0.0237
Epoch 179/300, trend Loss: 0.0538 | 0.0237
Epoch 180/300, trend Loss: 0.0538 | 0.0237
Epoch 181/300, trend Loss: 0.0538 | 0.0237
Epoch 182/300, trend Loss: 0.0538 | 0.0237
Epoch 183/300, trend Loss: 0.0538 | 0.0237
Epoch 184/300, trend Loss: 0.0538 | 0.0237
Epoch 185/300, trend Loss: 0.0538 | 0.0237
Epoch 186/300, trend Loss: 0.0538 | 0.0237
Epoch 187/300, trend Loss: 0.0538 | 0.0237
Epoch 188/300, trend Loss: 0.0538 | 0.0237
Epoch 189/300, trend Loss: 0.0538 | 0.0237
Epoch 190/300, trend Loss: 0.0538 | 0.0237
Epoch 191/300, trend Loss: 0.0538 | 0.0237
Epoch 192/300, trend Loss: 0.0538 | 0.0237
Epoch 193/300, trend Loss: 0.0538 | 0.0237
Epoch 194/300, trend Loss: 0.0538 | 0.0237
Epoch 195/300, trend Loss: 0.0538 | 0.0237
Epoch 196/300, trend Loss: 0.0538 | 0.0237
Epoch 197/300, trend Loss: 0.0538 | 0.0237
Epoch 198/300, trend Loss: 0.0538 | 0.0237
Epoch 199/300, trend Loss: 0.0538 | 0.0237
Epoch 200/300, trend Loss: 0.0538 | 0.0237
Epoch 201/300, trend Loss: 0.0538 | 0.0237
Epoch 202/300, trend Loss: 0.0538 | 0.0237
Epoch 203/300, trend Loss: 0.0538 | 0.0237
Epoch 204/300, trend Loss: 0.0538 | 0.0237
Epoch 205/300, trend Loss: 0.0538 | 0.0237
Epoch 206/300, trend Loss: 0.0538 | 0.0237
Epoch 207/300, trend Loss: 0.0538 | 0.0237
Epoch 208/300, trend Loss: 0.0538 | 0.0237
Epoch 209/300, trend Loss: 0.0538 | 0.0237
Epoch 210/300, trend Loss: 0.0538 | 0.0237
Epoch 211/300, trend Loss: 0.0538 | 0.0237
Epoch 212/300, trend Loss: 0.0538 | 0.0237
Epoch 213/300, trend Loss: 0.0538 | 0.0237
Epoch 214/300, trend Loss: 0.0538 | 0.0237
Epoch 215/300, trend Loss: 0.0538 | 0.0237
Epoch 216/300, trend Loss: 0.0538 | 0.0237
Epoch 217/300, trend Loss: 0.0538 | 0.0237
Epoch 218/300, trend Loss: 0.0538 | 0.0237
Epoch 219/300, trend Loss: 0.0538 | 0.0237
Epoch 220/300, trend Loss: 0.0538 | 0.0237
Epoch 221/300, trend Loss: 0.0538 | 0.0237
Epoch 222/300, trend Loss: 0.0538 | 0.0237
Epoch 223/300, trend Loss: 0.0538 | 0.0237
Epoch 224/300, trend Loss: 0.0538 | 0.0237
Epoch 225/300, trend Loss: 0.0538 | 0.0237
Epoch 226/300, trend Loss: 0.0538 | 0.0237
Epoch 227/300, trend Loss: 0.0538 | 0.0237
Epoch 228/300, trend Loss: 0.0538 | 0.0237
Epoch 229/300, trend Loss: 0.0538 | 0.0237
Epoch 230/300, trend Loss: 0.0538 | 0.0237
Epoch 231/300, trend Loss: 0.0538 | 0.0237
Epoch 232/300, trend Loss: 0.0538 | 0.0237
Epoch 233/300, trend Loss: 0.0538 | 0.0237
Epoch 234/300, trend Loss: 0.0538 | 0.0237
Epoch 235/300, trend Loss: 0.0538 | 0.0237
Epoch 236/300, trend Loss: 0.0538 | 0.0237
Epoch 237/300, trend Loss: 0.0538 | 0.0237
Epoch 238/300, trend Loss: 0.0538 | 0.0237
Epoch 239/300, trend Loss: 0.0538 | 0.0237
Epoch 240/300, trend Loss: 0.0538 | 0.0237
Epoch 241/300, trend Loss: 0.0538 | 0.0237
Epoch 242/300, trend Loss: 0.0538 | 0.0237
Epoch 243/300, trend Loss: 0.0538 | 0.0237
Epoch 244/300, trend Loss: 0.0538 | 0.0237
Epoch 245/300, trend Loss: 0.0538 | 0.0237
Epoch 246/300, trend Loss: 0.0538 | 0.0237
Epoch 247/300, trend Loss: 0.0538 | 0.0237
Epoch 248/300, trend Loss: 0.0538 | 0.0237
Epoch 249/300, trend Loss: 0.0538 | 0.0237
Epoch 250/300, trend Loss: 0.0538 | 0.0237
Epoch 251/300, trend Loss: 0.0538 | 0.0237
Epoch 252/300, trend Loss: 0.0538 | 0.0237
Epoch 253/300, trend Loss: 0.0538 | 0.0237
Epoch 254/300, trend Loss: 0.0538 | 0.0237
Epoch 255/300, trend Loss: 0.0538 | 0.0237
Epoch 256/300, trend Loss: 0.0538 | 0.0237
Epoch 257/300, trend Loss: 0.0538 | 0.0237
Epoch 258/300, trend Loss: 0.0538 | 0.0237
Epoch 259/300, trend Loss: 0.0538 | 0.0237
Epoch 260/300, trend Loss: 0.0538 | 0.0237
Epoch 261/300, trend Loss: 0.0538 | 0.0237
Epoch 262/300, trend Loss: 0.0538 | 0.0237
Epoch 263/300, trend Loss: 0.0538 | 0.0237
Epoch 264/300, trend Loss: 0.0538 | 0.0237
Epoch 265/300, trend Loss: 0.0538 | 0.0237
Epoch 266/300, trend Loss: 0.0538 | 0.0237
Epoch 267/300, trend Loss: 0.0538 | 0.0237
Epoch 268/300, trend Loss: 0.0538 | 0.0237
Epoch 269/300, trend Loss: 0.0538 | 0.0237
Epoch 270/300, trend Loss: 0.0538 | 0.0237
Epoch 271/300, trend Loss: 0.0538 | 0.0237
Epoch 272/300, trend Loss: 0.0538 | 0.0237
Epoch 273/300, trend Loss: 0.0538 | 0.0237
Epoch 274/300, trend Loss: 0.0538 | 0.0237
Epoch 275/300, trend Loss: 0.0538 | 0.0237
Epoch 276/300, trend Loss: 0.0538 | 0.0237
Epoch 277/300, trend Loss: 0.0538 | 0.0237
Epoch 278/300, trend Loss: 0.0538 | 0.0237
Epoch 279/300, trend Loss: 0.0538 | 0.0237
Epoch 280/300, trend Loss: 0.0538 | 0.0237
Epoch 281/300, trend Loss: 0.0538 | 0.0237
Epoch 282/300, trend Loss: 0.0538 | 0.0237
Epoch 283/300, trend Loss: 0.0538 | 0.0237
Epoch 284/300, trend Loss: 0.0538 | 0.0237
Epoch 285/300, trend Loss: 0.0538 | 0.0237
Epoch 286/300, trend Loss: 0.0538 | 0.0237
Epoch 287/300, trend Loss: 0.0538 | 0.0237
Epoch 288/300, trend Loss: 0.0538 | 0.0237
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 15, 'train_rates': 0.9670657911982976, 'learning_rate': 0.000641763897711025, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9090464857788871}
Epoch 1/300, seasonal_0 Loss: 0.3797 | 0.1637
Epoch 2/300, seasonal_0 Loss: 0.1786 | 0.1355
Epoch 3/300, seasonal_0 Loss: 0.1446 | 0.1171
Epoch 4/300, seasonal_0 Loss: 0.1355 | 0.1082
Epoch 5/300, seasonal_0 Loss: 0.1260 | 0.1004
Epoch 6/300, seasonal_0 Loss: 0.1207 | 0.0931
Epoch 7/300, seasonal_0 Loss: 0.1174 | 0.0927
Epoch 8/300, seasonal_0 Loss: 0.1130 | 0.0871
Epoch 9/300, seasonal_0 Loss: 0.1107 | 0.0853
Epoch 10/300, seasonal_0 Loss: 0.1069 | 0.0811
Epoch 11/300, seasonal_0 Loss: 0.1042 | 0.0809
Epoch 12/300, seasonal_0 Loss: 0.1074 | 0.0909
Epoch 13/300, seasonal_0 Loss: 0.1175 | 0.0978
Epoch 14/300, seasonal_0 Loss: 0.1061 | 0.0930
Epoch 15/300, seasonal_0 Loss: 0.1185 | 0.0845
Epoch 16/300, seasonal_0 Loss: 0.1027 | 0.0813
Epoch 17/300, seasonal_0 Loss: 0.0985 | 0.0781
Epoch 18/300, seasonal_0 Loss: 0.0927 | 0.0705
Epoch 19/300, seasonal_0 Loss: 0.0903 | 0.0672
Epoch 20/300, seasonal_0 Loss: 0.0884 | 0.0640
Epoch 21/300, seasonal_0 Loss: 0.0895 | 0.0635
Epoch 22/300, seasonal_0 Loss: 0.0894 | 0.0689
Epoch 23/300, seasonal_0 Loss: 0.0902 | 0.0699
Epoch 24/300, seasonal_0 Loss: 0.0859 | 0.0600
Epoch 25/300, seasonal_0 Loss: 0.0857 | 0.0605
Epoch 26/300, seasonal_0 Loss: 0.0839 | 0.0630
Epoch 27/300, seasonal_0 Loss: 0.0838 | 0.0579
Epoch 28/300, seasonal_0 Loss: 0.0814 | 0.0646
Epoch 29/300, seasonal_0 Loss: 0.0821 | 0.0599
Epoch 30/300, seasonal_0 Loss: 0.0817 | 0.0643
Epoch 31/300, seasonal_0 Loss: 0.0815 | 0.0626
Epoch 32/300, seasonal_0 Loss: 0.0843 | 0.0684
Epoch 33/300, seasonal_0 Loss: 0.0796 | 0.0610
Epoch 34/300, seasonal_0 Loss: 0.0772 | 0.0549
Epoch 35/300, seasonal_0 Loss: 0.0802 | 0.0575
Epoch 36/300, seasonal_0 Loss: 0.0836 | 0.0559
Epoch 37/300, seasonal_0 Loss: 0.0770 | 0.0543
Epoch 38/300, seasonal_0 Loss: 0.0751 | 0.0547
Epoch 39/300, seasonal_0 Loss: 0.0754 | 0.0507
Epoch 40/300, seasonal_0 Loss: 0.0747 | 0.0522
Epoch 41/300, seasonal_0 Loss: 0.0744 | 0.0513
Epoch 42/300, seasonal_0 Loss: 0.0757 | 0.0526
Epoch 43/300, seasonal_0 Loss: 0.0738 | 0.0628
Epoch 44/300, seasonal_0 Loss: 0.0741 | 0.0585
Epoch 45/300, seasonal_0 Loss: 0.0771 | 0.0576
Epoch 46/300, seasonal_0 Loss: 0.0745 | 0.0523
Epoch 47/300, seasonal_0 Loss: 0.0732 | 0.0493
Epoch 48/300, seasonal_0 Loss: 0.0719 | 0.0503
Epoch 49/300, seasonal_0 Loss: 0.0686 | 0.0485
Epoch 50/300, seasonal_0 Loss: 0.0691 | 0.0503
Epoch 51/300, seasonal_0 Loss: 0.0716 | 0.0531
Epoch 52/300, seasonal_0 Loss: 0.0717 | 0.0529
Epoch 53/300, seasonal_0 Loss: 0.0795 | 0.0547
Epoch 54/300, seasonal_0 Loss: 0.0732 | 0.0493
Epoch 55/300, seasonal_0 Loss: 0.0831 | 0.0543
Epoch 56/300, seasonal_0 Loss: 0.0963 | 0.0577
Epoch 57/300, seasonal_0 Loss: 0.0883 | 0.0586
Epoch 58/300, seasonal_0 Loss: 0.1064 | 0.1333
Epoch 59/300, seasonal_0 Loss: 0.0887 | 0.0854
Epoch 60/300, seasonal_0 Loss: 0.0800 | 0.0665
Epoch 61/300, seasonal_0 Loss: 0.0750 | 0.0491
Epoch 62/300, seasonal_0 Loss: 0.0752 | 0.0471
Epoch 63/300, seasonal_0 Loss: 0.0701 | 0.0464
Epoch 64/300, seasonal_0 Loss: 0.0684 | 0.0484
Epoch 65/300, seasonal_0 Loss: 0.0677 | 0.0528
Epoch 66/300, seasonal_0 Loss: 0.0658 | 0.0507
Epoch 67/300, seasonal_0 Loss: 0.0638 | 0.0433
Epoch 68/300, seasonal_0 Loss: 0.0631 | 0.0415
Epoch 69/300, seasonal_0 Loss: 0.0627 | 0.0407
Epoch 70/300, seasonal_0 Loss: 0.0609 | 0.0409
Epoch 71/300, seasonal_0 Loss: 0.0596 | 0.0402
Epoch 72/300, seasonal_0 Loss: 0.0587 | 0.0401
Epoch 73/300, seasonal_0 Loss: 0.0584 | 0.0403
Epoch 74/300, seasonal_0 Loss: 0.0586 | 0.0415
Epoch 75/300, seasonal_0 Loss: 0.0606 | 0.0432
Epoch 76/300, seasonal_0 Loss: 0.0585 | 0.0413
Epoch 77/300, seasonal_0 Loss: 0.0571 | 0.0395
Epoch 78/300, seasonal_0 Loss: 0.0572 | 0.0395
Epoch 79/300, seasonal_0 Loss: 0.0558 | 0.0392
Epoch 80/300, seasonal_0 Loss: 0.0543 | 0.0395
Epoch 81/300, seasonal_0 Loss: 0.0545 | 0.0410
Epoch 82/300, seasonal_0 Loss: 0.0567 | 0.0411
Epoch 83/300, seasonal_0 Loss: 0.0573 | 0.0408
Epoch 84/300, seasonal_0 Loss: 0.0561 | 0.0416
Epoch 85/300, seasonal_0 Loss: 0.0567 | 0.0404
Epoch 86/300, seasonal_0 Loss: 0.0567 | 0.0421
Epoch 87/300, seasonal_0 Loss: 0.0557 | 0.0412
Epoch 88/300, seasonal_0 Loss: 0.0536 | 0.0406
Epoch 89/300, seasonal_0 Loss: 0.0540 | 0.0397
Epoch 90/300, seasonal_0 Loss: 0.0564 | 0.0395
Epoch 91/300, seasonal_0 Loss: 0.0584 | 0.0393
Epoch 92/300, seasonal_0 Loss: 0.0590 | 0.0407
Epoch 93/300, seasonal_0 Loss: 0.0564 | 0.0401
Epoch 94/300, seasonal_0 Loss: 0.0537 | 0.0407
Epoch 95/300, seasonal_0 Loss: 0.0547 | 0.0434
Epoch 96/300, seasonal_0 Loss: 0.0544 | 0.0414
Epoch 97/300, seasonal_0 Loss: 0.0529 | 0.0411
Epoch 98/300, seasonal_0 Loss: 0.0512 | 0.0401
Epoch 99/300, seasonal_0 Loss: 0.0496 | 0.0384
Epoch 100/300, seasonal_0 Loss: 0.0495 | 0.0378
Epoch 101/300, seasonal_0 Loss: 0.0491 | 0.0378
Epoch 102/300, seasonal_0 Loss: 0.0486 | 0.0380
Epoch 103/300, seasonal_0 Loss: 0.0483 | 0.0384
Epoch 104/300, seasonal_0 Loss: 0.0482 | 0.0384
Epoch 105/300, seasonal_0 Loss: 0.0480 | 0.0383
Epoch 106/300, seasonal_0 Loss: 0.0478 | 0.0383
Epoch 107/300, seasonal_0 Loss: 0.0476 | 0.0381
Epoch 108/300, seasonal_0 Loss: 0.0476 | 0.0382
Epoch 109/300, seasonal_0 Loss: 0.0475 | 0.0382
Epoch 110/300, seasonal_0 Loss: 0.0474 | 0.0385
Epoch 111/300, seasonal_0 Loss: 0.0473 | 0.0384
Epoch 112/300, seasonal_0 Loss: 0.0472 | 0.0385
Epoch 113/300, seasonal_0 Loss: 0.0470 | 0.0383
Epoch 114/300, seasonal_0 Loss: 0.0469 | 0.0383
Epoch 115/300, seasonal_0 Loss: 0.0468 | 0.0381
Epoch 116/300, seasonal_0 Loss: 0.0467 | 0.0382
Epoch 117/300, seasonal_0 Loss: 0.0466 | 0.0382
Epoch 118/300, seasonal_0 Loss: 0.0466 | 0.0382
Epoch 119/300, seasonal_0 Loss: 0.0465 | 0.0383
Epoch 120/300, seasonal_0 Loss: 0.0464 | 0.0383
Epoch 121/300, seasonal_0 Loss: 0.0463 | 0.0383
Epoch 122/300, seasonal_0 Loss: 0.0461 | 0.0383
Epoch 123/300, seasonal_0 Loss: 0.0460 | 0.0383
Epoch 124/300, seasonal_0 Loss: 0.0459 | 0.0383
Epoch 125/300, seasonal_0 Loss: 0.0458 | 0.0383
Epoch 126/300, seasonal_0 Loss: 0.0458 | 0.0384
Epoch 127/300, seasonal_0 Loss: 0.0458 | 0.0386
Epoch 128/300, seasonal_0 Loss: 0.0458 | 0.0387
Epoch 129/300, seasonal_0 Loss: 0.0457 | 0.0387
Epoch 130/300, seasonal_0 Loss: 0.0456 | 0.0386
Epoch 131/300, seasonal_0 Loss: 0.0454 | 0.0385
Epoch 132/300, seasonal_0 Loss: 0.0453 | 0.0384
Epoch 133/300, seasonal_0 Loss: 0.0453 | 0.0383
Epoch 134/300, seasonal_0 Loss: 0.0452 | 0.0383
Epoch 135/300, seasonal_0 Loss: 0.0452 | 0.0383
Epoch 136/300, seasonal_0 Loss: 0.0451 | 0.0383
Epoch 137/300, seasonal_0 Loss: 0.0450 | 0.0383
Epoch 138/300, seasonal_0 Loss: 0.0449 | 0.0384
Epoch 139/300, seasonal_0 Loss: 0.0448 | 0.0384
Epoch 140/300, seasonal_0 Loss: 0.0448 | 0.0384
Epoch 141/300, seasonal_0 Loss: 0.0447 | 0.0385
Epoch 142/300, seasonal_0 Loss: 0.0447 | 0.0385
Epoch 143/300, seasonal_0 Loss: 0.0446 | 0.0385
Epoch 144/300, seasonal_0 Loss: 0.0446 | 0.0385
Epoch 145/300, seasonal_0 Loss: 0.0445 | 0.0385
Epoch 146/300, seasonal_0 Loss: 0.0445 | 0.0385
Epoch 147/300, seasonal_0 Loss: 0.0444 | 0.0385
Epoch 148/300, seasonal_0 Loss: 0.0444 | 0.0385
Epoch 149/300, seasonal_0 Loss: 0.0443 | 0.0385
Epoch 150/300, seasonal_0 Loss: 0.0443 | 0.0385
Epoch 151/300, seasonal_0 Loss: 0.0442 | 0.0385
Epoch 152/300, seasonal_0 Loss: 0.0442 | 0.0385
Epoch 153/300, seasonal_0 Loss: 0.0441 | 0.0386
Epoch 154/300, seasonal_0 Loss: 0.0441 | 0.0386
Epoch 155/300, seasonal_0 Loss: 0.0440 | 0.0386
Epoch 156/300, seasonal_0 Loss: 0.0440 | 0.0386
Epoch 157/300, seasonal_0 Loss: 0.0440 | 0.0386
Epoch 158/300, seasonal_0 Loss: 0.0439 | 0.0386
Epoch 159/300, seasonal_0 Loss: 0.0439 | 0.0386
Epoch 160/300, seasonal_0 Loss: 0.0438 | 0.0386
Epoch 161/300, seasonal_0 Loss: 0.0438 | 0.0386
Epoch 162/300, seasonal_0 Loss: 0.0438 | 0.0386
Epoch 163/300, seasonal_0 Loss: 0.0437 | 0.0387
Epoch 164/300, seasonal_0 Loss: 0.0437 | 0.0387
Epoch 165/300, seasonal_0 Loss: 0.0437 | 0.0387
Epoch 166/300, seasonal_0 Loss: 0.0436 | 0.0387
Epoch 167/300, seasonal_0 Loss: 0.0436 | 0.0387
Epoch 168/300, seasonal_0 Loss: 0.0436 | 0.0387
Epoch 169/300, seasonal_0 Loss: 0.0435 | 0.0387
Epoch 170/300, seasonal_0 Loss: 0.0435 | 0.0387
Epoch 171/300, seasonal_0 Loss: 0.0435 | 0.0387
Epoch 172/300, seasonal_0 Loss: 0.0434 | 0.0387
Epoch 173/300, seasonal_0 Loss: 0.0434 | 0.0387
Epoch 174/300, seasonal_0 Loss: 0.0434 | 0.0387
Epoch 175/300, seasonal_0 Loss: 0.0434 | 0.0387
Epoch 176/300, seasonal_0 Loss: 0.0433 | 0.0387
Epoch 177/300, seasonal_0 Loss: 0.0433 | 0.0388
Epoch 178/300, seasonal_0 Loss: 0.0433 | 0.0388
Epoch 179/300, seasonal_0 Loss: 0.0433 | 0.0388
Epoch 180/300, seasonal_0 Loss: 0.0432 | 0.0388
Epoch 181/300, seasonal_0 Loss: 0.0432 | 0.0388
Epoch 182/300, seasonal_0 Loss: 0.0432 | 0.0388
Epoch 183/300, seasonal_0 Loss: 0.0432 | 0.0388
Epoch 184/300, seasonal_0 Loss: 0.0431 | 0.0388
Epoch 185/300, seasonal_0 Loss: 0.0431 | 0.0388
Epoch 186/300, seasonal_0 Loss: 0.0431 | 0.0388
Epoch 187/300, seasonal_0 Loss: 0.0431 | 0.0388
Epoch 188/300, seasonal_0 Loss: 0.0431 | 0.0388
Epoch 189/300, seasonal_0 Loss: 0.0430 | 0.0388
Epoch 190/300, seasonal_0 Loss: 0.0430 | 0.0388
Epoch 191/300, seasonal_0 Loss: 0.0430 | 0.0388
Epoch 192/300, seasonal_0 Loss: 0.0430 | 0.0388
Epoch 193/300, seasonal_0 Loss: 0.0430 | 0.0388
Epoch 194/300, seasonal_0 Loss: 0.0429 | 0.0388
Epoch 195/300, seasonal_0 Loss: 0.0429 | 0.0388
Epoch 196/300, seasonal_0 Loss: 0.0429 | 0.0389
Epoch 197/300, seasonal_0 Loss: 0.0429 | 0.0389
Epoch 198/300, seasonal_0 Loss: 0.0429 | 0.0389
Epoch 199/300, seasonal_0 Loss: 0.0429 | 0.0389
Epoch 200/300, seasonal_0 Loss: 0.0429 | 0.0389
Epoch 201/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 202/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 203/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 204/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 205/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 206/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 207/300, seasonal_0 Loss: 0.0428 | 0.0389
Epoch 208/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 209/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 210/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 211/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 212/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 213/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 214/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 215/300, seasonal_0 Loss: 0.0427 | 0.0389
Epoch 216/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 217/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 218/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 219/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 220/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 221/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 222/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 223/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 224/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 225/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 226/300, seasonal_0 Loss: 0.0426 | 0.0389
Epoch 227/300, seasonal_0 Loss: 0.0425 | 0.0389
Epoch 228/300, seasonal_0 Loss: 0.0425 | 0.0389
Epoch 229/300, seasonal_0 Loss: 0.0425 | 0.0389
Epoch 230/300, seasonal_0 Loss: 0.0425 | 0.0389
Epoch 231/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 232/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 233/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 234/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 235/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 236/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 237/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 238/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 239/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 240/300, seasonal_0 Loss: 0.0425 | 0.0390
Epoch 241/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 242/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 243/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 244/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 245/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 246/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 247/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 248/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 249/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 250/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 251/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 252/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 253/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 254/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 255/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 256/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 257/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 258/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 259/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 260/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 261/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 262/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 263/300, seasonal_0 Loss: 0.0424 | 0.0390
Epoch 264/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 265/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 266/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 267/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 268/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 269/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 270/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 271/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 272/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 273/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 274/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 275/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 276/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 277/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 278/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 279/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 280/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 281/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 282/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 283/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 284/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 285/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 286/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 287/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 288/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 289/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 290/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 291/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 292/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 293/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 294/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 295/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 296/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 297/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 298/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 299/300, seasonal_0 Loss: 0.0423 | 0.0390
Epoch 300/300, seasonal_0 Loss: 0.0423 | 0.0390
Training seasonal_1 component with params: {'observation_period_num': 20, 'train_rates': 0.8317706625659463, 'learning_rate': 0.0006149944215370888, 'batch_size': 120, 'step_size': 9, 'gamma': 0.778281763023794}
Epoch 1/300, seasonal_1 Loss: 0.4806 | 0.3212
Epoch 2/300, seasonal_1 Loss: 0.2003 | 0.1937
Epoch 3/300, seasonal_1 Loss: 0.1881 | 0.1409
Epoch 4/300, seasonal_1 Loss: 0.1821 | 0.1219
Epoch 5/300, seasonal_1 Loss: 0.1544 | 0.0943
Epoch 6/300, seasonal_1 Loss: 0.1406 | 0.0992
Epoch 7/300, seasonal_1 Loss: 0.1337 | 0.0936
Epoch 8/300, seasonal_1 Loss: 0.1351 | 0.1234
Epoch 9/300, seasonal_1 Loss: 0.1404 | 0.2478
Epoch 10/300, seasonal_1 Loss: 0.1454 | 0.1323
Epoch 11/300, seasonal_1 Loss: 0.1735 | 0.1431
Epoch 12/300, seasonal_1 Loss: 0.1489 | 0.0858
Epoch 13/300, seasonal_1 Loss: 0.1233 | 0.0897
Epoch 14/300, seasonal_1 Loss: 0.1274 | 0.0958
Epoch 15/300, seasonal_1 Loss: 0.1246 | 0.0772
Epoch 16/300, seasonal_1 Loss: 0.1175 | 0.0766
Epoch 17/300, seasonal_1 Loss: 0.1135 | 0.0733
Epoch 18/300, seasonal_1 Loss: 0.1131 | 0.0689
Epoch 19/300, seasonal_1 Loss: 0.1201 | 0.0669
Epoch 20/300, seasonal_1 Loss: 0.1267 | 0.0995
Epoch 21/300, seasonal_1 Loss: 0.1175 | 0.0834
Epoch 22/300, seasonal_1 Loss: 0.1104 | 0.0644
Epoch 23/300, seasonal_1 Loss: 0.1115 | 0.0806
Epoch 24/300, seasonal_1 Loss: 0.1079 | 0.0662
Epoch 25/300, seasonal_1 Loss: 0.1052 | 0.0631
Epoch 26/300, seasonal_1 Loss: 0.0990 | 0.0584
Epoch 27/300, seasonal_1 Loss: 0.0948 | 0.0591
Epoch 28/300, seasonal_1 Loss: 0.0968 | 0.0595
Epoch 29/300, seasonal_1 Loss: 0.0935 | 0.0588
Epoch 30/300, seasonal_1 Loss: 0.0927 | 0.0603
Epoch 31/300, seasonal_1 Loss: 0.0925 | 0.0561
Epoch 32/300, seasonal_1 Loss: 0.0897 | 0.0536
Epoch 33/300, seasonal_1 Loss: 0.0877 | 0.0518
Epoch 34/300, seasonal_1 Loss: 0.0869 | 0.0512
Epoch 35/300, seasonal_1 Loss: 0.0861 | 0.0505
Epoch 36/300, seasonal_1 Loss: 0.0856 | 0.0499
Epoch 37/300, seasonal_1 Loss: 0.0847 | 0.0497
Epoch 38/300, seasonal_1 Loss: 0.0841 | 0.0488
Epoch 39/300, seasonal_1 Loss: 0.0836 | 0.0489
Epoch 40/300, seasonal_1 Loss: 0.0831 | 0.0482
Epoch 41/300, seasonal_1 Loss: 0.0826 | 0.0479
Epoch 42/300, seasonal_1 Loss: 0.0822 | 0.0474
Epoch 43/300, seasonal_1 Loss: 0.0818 | 0.0472
Epoch 44/300, seasonal_1 Loss: 0.0815 | 0.0469
Epoch 45/300, seasonal_1 Loss: 0.0812 | 0.0468
Epoch 46/300, seasonal_1 Loss: 0.0809 | 0.0464
Epoch 47/300, seasonal_1 Loss: 0.0806 | 0.0463
Epoch 48/300, seasonal_1 Loss: 0.0804 | 0.0461
Epoch 49/300, seasonal_1 Loss: 0.0801 | 0.0460
Epoch 50/300, seasonal_1 Loss: 0.0799 | 0.0458
Epoch 51/300, seasonal_1 Loss: 0.0797 | 0.0456
Epoch 52/300, seasonal_1 Loss: 0.0795 | 0.0455
Epoch 53/300, seasonal_1 Loss: 0.0793 | 0.0454
Epoch 54/300, seasonal_1 Loss: 0.0791 | 0.0452
Epoch 55/300, seasonal_1 Loss: 0.0789 | 0.0452
Epoch 56/300, seasonal_1 Loss: 0.0788 | 0.0451
Epoch 57/300, seasonal_1 Loss: 0.0786 | 0.0450
Epoch 58/300, seasonal_1 Loss: 0.0785 | 0.0448
Epoch 59/300, seasonal_1 Loss: 0.0783 | 0.0448
Epoch 60/300, seasonal_1 Loss: 0.0782 | 0.0447
Epoch 61/300, seasonal_1 Loss: 0.0781 | 0.0447
Epoch 62/300, seasonal_1 Loss: 0.0779 | 0.0446
Epoch 63/300, seasonal_1 Loss: 0.0778 | 0.0446
Epoch 64/300, seasonal_1 Loss: 0.0777 | 0.0445
Epoch 65/300, seasonal_1 Loss: 0.0777 | 0.0444
Epoch 66/300, seasonal_1 Loss: 0.0776 | 0.0443
Epoch 67/300, seasonal_1 Loss: 0.0775 | 0.0443
Epoch 68/300, seasonal_1 Loss: 0.0773 | 0.0442
Epoch 69/300, seasonal_1 Loss: 0.0772 | 0.0441
Epoch 70/300, seasonal_1 Loss: 0.0771 | 0.0440
Epoch 71/300, seasonal_1 Loss: 0.0771 | 0.0440
Epoch 72/300, seasonal_1 Loss: 0.0770 | 0.0440
Epoch 73/300, seasonal_1 Loss: 0.0769 | 0.0439
Epoch 74/300, seasonal_1 Loss: 0.0768 | 0.0438
Epoch 75/300, seasonal_1 Loss: 0.0768 | 0.0438
Epoch 76/300, seasonal_1 Loss: 0.0767 | 0.0438
Epoch 77/300, seasonal_1 Loss: 0.0767 | 0.0437
Epoch 78/300, seasonal_1 Loss: 0.0766 | 0.0437
Epoch 79/300, seasonal_1 Loss: 0.0765 | 0.0437
Epoch 80/300, seasonal_1 Loss: 0.0765 | 0.0436
Epoch 81/300, seasonal_1 Loss: 0.0765 | 0.0436
Epoch 82/300, seasonal_1 Loss: 0.0764 | 0.0436
Epoch 83/300, seasonal_1 Loss: 0.0764 | 0.0436
Epoch 84/300, seasonal_1 Loss: 0.0763 | 0.0435
Epoch 85/300, seasonal_1 Loss: 0.0763 | 0.0435
Epoch 86/300, seasonal_1 Loss: 0.0763 | 0.0435
Epoch 87/300, seasonal_1 Loss: 0.0762 | 0.0435
Epoch 88/300, seasonal_1 Loss: 0.0762 | 0.0435
Epoch 89/300, seasonal_1 Loss: 0.0762 | 0.0434
Epoch 90/300, seasonal_1 Loss: 0.0761 | 0.0434
Epoch 91/300, seasonal_1 Loss: 0.0761 | 0.0434
Epoch 92/300, seasonal_1 Loss: 0.0761 | 0.0434
Epoch 93/300, seasonal_1 Loss: 0.0761 | 0.0434
Epoch 94/300, seasonal_1 Loss: 0.0760 | 0.0434
Epoch 95/300, seasonal_1 Loss: 0.0760 | 0.0433
Epoch 96/300, seasonal_1 Loss: 0.0760 | 0.0433
Epoch 97/300, seasonal_1 Loss: 0.0760 | 0.0433
Epoch 98/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 99/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 100/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 101/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 102/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 103/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 104/300, seasonal_1 Loss: 0.0759 | 0.0433
Epoch 105/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 106/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 107/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 108/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 109/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 110/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 111/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 112/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 113/300, seasonal_1 Loss: 0.0758 | 0.0432
Epoch 114/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 115/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 116/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 117/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 118/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 119/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 120/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 121/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 122/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 123/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 124/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 125/300, seasonal_1 Loss: 0.0757 | 0.0432
Epoch 126/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 127/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 128/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 129/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 130/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 131/300, seasonal_1 Loss: 0.0757 | 0.0431
Epoch 132/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 133/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 134/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 135/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 136/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 137/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 138/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 139/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 140/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 141/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 142/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 143/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 144/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 145/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 146/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 147/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 148/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 149/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 150/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 151/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 152/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 153/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 154/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 155/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 156/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 157/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 158/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 159/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 160/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 161/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 162/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 163/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 164/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 165/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 166/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 167/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 168/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 169/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 170/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 171/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 172/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 173/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 174/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 175/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 176/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 177/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 178/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 179/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 180/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 181/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 182/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 183/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 184/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 185/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 186/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 187/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 188/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 189/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 190/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 191/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 192/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 193/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 194/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 195/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 196/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 197/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 198/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 199/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 200/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 201/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 202/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 203/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 204/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 205/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 206/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 207/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 208/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 209/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 210/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 211/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 212/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 213/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 214/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 215/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 216/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 217/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 218/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 219/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 220/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 221/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 222/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 223/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 224/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 225/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 226/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 227/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 228/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 229/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 230/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 231/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 232/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 233/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 234/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 235/300, seasonal_1 Loss: 0.0756 | 0.0431
Epoch 236/300, seasonal_1 Loss: 0.0756 | 0.0431
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 18, 'train_rates': 0.8176524260923114, 'learning_rate': 0.00032732087233840765, 'batch_size': 63, 'step_size': 6, 'gamma': 0.7746076969173901}
Epoch 1/300, seasonal_2 Loss: 0.4099 | 0.1746
Epoch 2/300, seasonal_2 Loss: 0.1734 | 0.1032
Epoch 3/300, seasonal_2 Loss: 0.1436 | 0.0930
Epoch 4/300, seasonal_2 Loss: 0.1328 | 0.0779
Epoch 5/300, seasonal_2 Loss: 0.1248 | 0.0733
Epoch 6/300, seasonal_2 Loss: 0.1206 | 0.0724
Epoch 7/300, seasonal_2 Loss: 0.1190 | 0.0947
Epoch 8/300, seasonal_2 Loss: 0.1266 | 0.0720
Epoch 9/300, seasonal_2 Loss: 0.1176 | 0.0758
Epoch 10/300, seasonal_2 Loss: 0.1140 | 0.0714
Epoch 11/300, seasonal_2 Loss: 0.1088 | 0.0641
Epoch 12/300, seasonal_2 Loss: 0.1052 | 0.0593
Epoch 13/300, seasonal_2 Loss: 0.1043 | 0.0566
Epoch 14/300, seasonal_2 Loss: 0.1049 | 0.0571
Epoch 15/300, seasonal_2 Loss: 0.1024 | 0.0587
Epoch 16/300, seasonal_2 Loss: 0.0992 | 0.0641
Epoch 17/300, seasonal_2 Loss: 0.0982 | 0.0665
Epoch 18/300, seasonal_2 Loss: 0.0981 | 0.0739
Epoch 19/300, seasonal_2 Loss: 0.0981 | 0.0742
Epoch 20/300, seasonal_2 Loss: 0.0999 | 0.0731
Epoch 21/300, seasonal_2 Loss: 0.1055 | 0.0580
Epoch 22/300, seasonal_2 Loss: 0.1122 | 0.0574
Epoch 23/300, seasonal_2 Loss: 0.1043 | 0.0530
Epoch 24/300, seasonal_2 Loss: 0.0982 | 0.0528
Epoch 25/300, seasonal_2 Loss: 0.0953 | 0.0551
Epoch 26/300, seasonal_2 Loss: 0.0933 | 0.0539
Epoch 27/300, seasonal_2 Loss: 0.0907 | 0.0540
Epoch 28/300, seasonal_2 Loss: 0.0898 | 0.0563
Epoch 29/300, seasonal_2 Loss: 0.0890 | 0.0562
Epoch 30/300, seasonal_2 Loss: 0.0883 | 0.0550
Epoch 31/300, seasonal_2 Loss: 0.0879 | 0.0542
Epoch 32/300, seasonal_2 Loss: 0.0875 | 0.0535
Epoch 33/300, seasonal_2 Loss: 0.0872 | 0.0530
Epoch 34/300, seasonal_2 Loss: 0.0869 | 0.0525
Epoch 35/300, seasonal_2 Loss: 0.0867 | 0.0520
Epoch 36/300, seasonal_2 Loss: 0.0865 | 0.0517
Epoch 37/300, seasonal_2 Loss: 0.0864 | 0.0514
Epoch 38/300, seasonal_2 Loss: 0.0863 | 0.0511
Epoch 39/300, seasonal_2 Loss: 0.0860 | 0.0509
Epoch 40/300, seasonal_2 Loss: 0.0858 | 0.0508
Epoch 41/300, seasonal_2 Loss: 0.0855 | 0.0506
Epoch 42/300, seasonal_2 Loss: 0.0852 | 0.0504
Epoch 43/300, seasonal_2 Loss: 0.0850 | 0.0504
Epoch 44/300, seasonal_2 Loss: 0.0848 | 0.0502
Epoch 45/300, seasonal_2 Loss: 0.0847 | 0.0500
Epoch 46/300, seasonal_2 Loss: 0.0845 | 0.0499
Epoch 47/300, seasonal_2 Loss: 0.0844 | 0.0498
Epoch 48/300, seasonal_2 Loss: 0.0843 | 0.0497
Epoch 49/300, seasonal_2 Loss: 0.0842 | 0.0496
Epoch 50/300, seasonal_2 Loss: 0.0841 | 0.0495
Epoch 51/300, seasonal_2 Loss: 0.0840 | 0.0494
Epoch 52/300, seasonal_2 Loss: 0.0840 | 0.0493
Epoch 53/300, seasonal_2 Loss: 0.0839 | 0.0493
Epoch 54/300, seasonal_2 Loss: 0.0838 | 0.0492
Epoch 55/300, seasonal_2 Loss: 0.0838 | 0.0492
Epoch 56/300, seasonal_2 Loss: 0.0837 | 0.0491
Epoch 57/300, seasonal_2 Loss: 0.0837 | 0.0490
Epoch 58/300, seasonal_2 Loss: 0.0836 | 0.0490
Epoch 59/300, seasonal_2 Loss: 0.0836 | 0.0489
Epoch 60/300, seasonal_2 Loss: 0.0835 | 0.0489
Epoch 61/300, seasonal_2 Loss: 0.0835 | 0.0489
Epoch 62/300, seasonal_2 Loss: 0.0835 | 0.0488
Epoch 63/300, seasonal_2 Loss: 0.0834 | 0.0488
Epoch 64/300, seasonal_2 Loss: 0.0834 | 0.0488
Epoch 65/300, seasonal_2 Loss: 0.0834 | 0.0487
Epoch 66/300, seasonal_2 Loss: 0.0834 | 0.0487
Epoch 67/300, seasonal_2 Loss: 0.0833 | 0.0487
Epoch 68/300, seasonal_2 Loss: 0.0833 | 0.0487
Epoch 69/300, seasonal_2 Loss: 0.0833 | 0.0487
Epoch 70/300, seasonal_2 Loss: 0.0833 | 0.0486
Epoch 71/300, seasonal_2 Loss: 0.0833 | 0.0486
Epoch 72/300, seasonal_2 Loss: 0.0832 | 0.0486
Epoch 73/300, seasonal_2 Loss: 0.0832 | 0.0486
Epoch 74/300, seasonal_2 Loss: 0.0832 | 0.0486
Epoch 75/300, seasonal_2 Loss: 0.0832 | 0.0486
Epoch 76/300, seasonal_2 Loss: 0.0832 | 0.0486
Epoch 77/300, seasonal_2 Loss: 0.0832 | 0.0485
Epoch 78/300, seasonal_2 Loss: 0.0832 | 0.0485
Epoch 79/300, seasonal_2 Loss: 0.0832 | 0.0485
Epoch 80/300, seasonal_2 Loss: 0.0832 | 0.0485
Epoch 81/300, seasonal_2 Loss: 0.0832 | 0.0485
Epoch 82/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 83/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 84/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 85/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 86/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 87/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 88/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 89/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 90/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 91/300, seasonal_2 Loss: 0.0831 | 0.0485
Epoch 92/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 93/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 94/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 95/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 96/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 97/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 98/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 99/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 100/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 101/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 102/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 103/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 104/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 105/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 106/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 107/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 108/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 109/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 110/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 111/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 112/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 113/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 114/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 115/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 116/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 117/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 118/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 119/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 120/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 121/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 122/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 123/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 124/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 125/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 126/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 127/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 128/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 129/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 130/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 131/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 132/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 133/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 134/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 135/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 136/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 137/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 138/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 139/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 140/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 141/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 142/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 143/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 144/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 145/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 146/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 147/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 148/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 149/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 150/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 151/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 152/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 153/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 154/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 155/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 156/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 157/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 158/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 159/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 160/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 161/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 162/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 163/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 164/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 165/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 166/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 167/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 168/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 169/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 170/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 171/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 172/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 173/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 174/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 175/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 176/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 177/300, seasonal_2 Loss: 0.0831 | 0.0484
Epoch 178/300, seasonal_2 Loss: 0.0831 | 0.0484
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.8929205758252304, 'learning_rate': 0.0003945828479062341, 'batch_size': 103, 'step_size': 10, 'gamma': 0.7776411222693225}
Epoch 1/300, seasonal_3 Loss: 0.4588 | 0.2439
Epoch 2/300, seasonal_3 Loss: 0.1616 | 0.1226
Epoch 3/300, seasonal_3 Loss: 0.1510 | 0.0975
Epoch 4/300, seasonal_3 Loss: 0.1303 | 0.0690
Epoch 5/300, seasonal_3 Loss: 0.1208 | 0.0698
Epoch 6/300, seasonal_3 Loss: 0.1163 | 0.0648
Epoch 7/300, seasonal_3 Loss: 0.1149 | 0.0647
Epoch 8/300, seasonal_3 Loss: 0.1121 | 0.0650
Epoch 9/300, seasonal_3 Loss: 0.1105 | 0.0658
Epoch 10/300, seasonal_3 Loss: 0.1096 | 0.0663
Epoch 11/300, seasonal_3 Loss: 0.1089 | 0.0583
Epoch 12/300, seasonal_3 Loss: 0.1108 | 0.0582
Epoch 13/300, seasonal_3 Loss: 0.1086 | 0.0581
Epoch 14/300, seasonal_3 Loss: 0.1067 | 0.0580
Epoch 15/300, seasonal_3 Loss: 0.1054 | 0.0577
Epoch 16/300, seasonal_3 Loss: 0.1062 | 0.0547
Epoch 17/300, seasonal_3 Loss: 0.1071 | 0.0538
Epoch 18/300, seasonal_3 Loss: 0.1062 | 0.0533
Epoch 19/300, seasonal_3 Loss: 0.1069 | 0.0551
Epoch 20/300, seasonal_3 Loss: 0.1060 | 0.0583
Epoch 21/300, seasonal_3 Loss: 0.1054 | 0.0771
Epoch 22/300, seasonal_3 Loss: 0.1041 | 0.0624
Epoch 23/300, seasonal_3 Loss: 0.0964 | 0.0542
Epoch 24/300, seasonal_3 Loss: 0.0944 | 0.0514
Epoch 25/300, seasonal_3 Loss: 0.0982 | 0.0495
Epoch 26/300, seasonal_3 Loss: 0.0996 | 0.0616
Epoch 27/300, seasonal_3 Loss: 0.0975 | 0.0510
Epoch 28/300, seasonal_3 Loss: 0.0935 | 0.0446
Epoch 29/300, seasonal_3 Loss: 0.0899 | 0.0430
Epoch 30/300, seasonal_3 Loss: 0.0871 | 0.0427
Epoch 31/300, seasonal_3 Loss: 0.0857 | 0.0426
Epoch 32/300, seasonal_3 Loss: 0.0852 | 0.0422
Epoch 33/300, seasonal_3 Loss: 0.0850 | 0.0417
Epoch 34/300, seasonal_3 Loss: 0.0848 | 0.0414
Epoch 35/300, seasonal_3 Loss: 0.0845 | 0.0411
Epoch 36/300, seasonal_3 Loss: 0.0841 | 0.0409
Epoch 37/300, seasonal_3 Loss: 0.0838 | 0.0408
Epoch 38/300, seasonal_3 Loss: 0.0835 | 0.0405
Epoch 39/300, seasonal_3 Loss: 0.0833 | 0.0404
Epoch 40/300, seasonal_3 Loss: 0.0831 | 0.0402
Epoch 41/300, seasonal_3 Loss: 0.0829 | 0.0401
Epoch 42/300, seasonal_3 Loss: 0.0828 | 0.0401
Epoch 43/300, seasonal_3 Loss: 0.0827 | 0.0400
Epoch 44/300, seasonal_3 Loss: 0.0827 | 0.0401
Epoch 45/300, seasonal_3 Loss: 0.0826 | 0.0401
Epoch 46/300, seasonal_3 Loss: 0.0826 | 0.0404
Epoch 47/300, seasonal_3 Loss: 0.0825 | 0.0402
Epoch 48/300, seasonal_3 Loss: 0.0820 | 0.0399
Epoch 49/300, seasonal_3 Loss: 0.0815 | 0.0397
Epoch 50/300, seasonal_3 Loss: 0.0812 | 0.0395
Epoch 51/300, seasonal_3 Loss: 0.0809 | 0.0395
Epoch 52/300, seasonal_3 Loss: 0.0808 | 0.0394
Epoch 53/300, seasonal_3 Loss: 0.0807 | 0.0393
Epoch 54/300, seasonal_3 Loss: 0.0806 | 0.0392
Epoch 55/300, seasonal_3 Loss: 0.0805 | 0.0391
Epoch 56/300, seasonal_3 Loss: 0.0804 | 0.0391
Epoch 57/300, seasonal_3 Loss: 0.0804 | 0.0391
Epoch 58/300, seasonal_3 Loss: 0.0804 | 0.0390
Epoch 59/300, seasonal_3 Loss: 0.0804 | 0.0390
Epoch 60/300, seasonal_3 Loss: 0.0804 | 0.0389
Epoch 61/300, seasonal_3 Loss: 0.0804 | 0.0390
Epoch 62/300, seasonal_3 Loss: 0.0805 | 0.0389
Epoch 63/300, seasonal_3 Loss: 0.0804 | 0.0389
Epoch 64/300, seasonal_3 Loss: 0.0802 | 0.0388
Epoch 65/300, seasonal_3 Loss: 0.0800 | 0.0387
Epoch 66/300, seasonal_3 Loss: 0.0799 | 0.0387
Epoch 67/300, seasonal_3 Loss: 0.0797 | 0.0387
Epoch 68/300, seasonal_3 Loss: 0.0796 | 0.0387
Epoch 69/300, seasonal_3 Loss: 0.0796 | 0.0387
Epoch 70/300, seasonal_3 Loss: 0.0795 | 0.0387
Epoch 71/300, seasonal_3 Loss: 0.0795 | 0.0386
Epoch 72/300, seasonal_3 Loss: 0.0794 | 0.0386
Epoch 73/300, seasonal_3 Loss: 0.0793 | 0.0385
Epoch 74/300, seasonal_3 Loss: 0.0792 | 0.0385
Epoch 75/300, seasonal_3 Loss: 0.0792 | 0.0384
Epoch 76/300, seasonal_3 Loss: 0.0791 | 0.0384
Epoch 77/300, seasonal_3 Loss: 0.0791 | 0.0384
Epoch 78/300, seasonal_3 Loss: 0.0791 | 0.0384
Epoch 79/300, seasonal_3 Loss: 0.0790 | 0.0383
Epoch 80/300, seasonal_3 Loss: 0.0790 | 0.0383
Epoch 81/300, seasonal_3 Loss: 0.0790 | 0.0383
Epoch 82/300, seasonal_3 Loss: 0.0789 | 0.0383
Epoch 83/300, seasonal_3 Loss: 0.0789 | 0.0383
Epoch 84/300, seasonal_3 Loss: 0.0789 | 0.0382
Epoch 85/300, seasonal_3 Loss: 0.0789 | 0.0382
Epoch 86/300, seasonal_3 Loss: 0.0788 | 0.0382
Epoch 87/300, seasonal_3 Loss: 0.0788 | 0.0382
Epoch 88/300, seasonal_3 Loss: 0.0788 | 0.0382
Epoch 89/300, seasonal_3 Loss: 0.0788 | 0.0382
Epoch 90/300, seasonal_3 Loss: 0.0787 | 0.0381
Epoch 91/300, seasonal_3 Loss: 0.0787 | 0.0381
Epoch 92/300, seasonal_3 Loss: 0.0787 | 0.0381
Epoch 93/300, seasonal_3 Loss: 0.0787 | 0.0381
Epoch 94/300, seasonal_3 Loss: 0.0787 | 0.0381
Epoch 95/300, seasonal_3 Loss: 0.0786 | 0.0381
Epoch 96/300, seasonal_3 Loss: 0.0786 | 0.0381
Epoch 97/300, seasonal_3 Loss: 0.0786 | 0.0381
Epoch 98/300, seasonal_3 Loss: 0.0786 | 0.0380
Epoch 99/300, seasonal_3 Loss: 0.0786 | 0.0380
Epoch 100/300, seasonal_3 Loss: 0.0786 | 0.0380
Epoch 101/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 102/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 103/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 104/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 105/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 106/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 107/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 108/300, seasonal_3 Loss: 0.0785 | 0.0380
Epoch 109/300, seasonal_3 Loss: 0.0785 | 0.0379
Epoch 110/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 111/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 112/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 113/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 114/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 115/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 116/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 117/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 118/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 119/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 120/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 121/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 122/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 123/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 124/300, seasonal_3 Loss: 0.0784 | 0.0379
Epoch 125/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 126/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 127/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 128/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 129/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 130/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 131/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 132/300, seasonal_3 Loss: 0.0783 | 0.0379
Epoch 133/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 134/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 135/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 136/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 137/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 138/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 139/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 140/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 141/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 142/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 143/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 144/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 145/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 146/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 147/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 148/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 149/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 150/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 151/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 152/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 153/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 154/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 155/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 156/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 157/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 158/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 159/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 160/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 161/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 162/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 163/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 164/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 165/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 166/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 167/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 168/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 169/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 170/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 171/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 172/300, seasonal_3 Loss: 0.0783 | 0.0378
Epoch 173/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 174/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 175/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 176/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 177/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 178/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 179/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 180/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 181/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 182/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 183/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 184/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 185/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 186/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 187/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 188/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 189/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 190/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 191/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 192/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 193/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 194/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 195/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 196/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 197/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 198/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 199/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 200/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 201/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 202/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 203/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 204/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 205/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 206/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 207/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 208/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 209/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 210/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 211/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 212/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 213/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 214/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 215/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 216/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 217/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 218/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 219/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 220/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 221/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 222/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 223/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 224/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 225/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 226/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 227/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 228/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 229/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 230/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 231/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 232/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 233/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 234/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 235/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 236/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 237/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 238/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 239/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 240/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 241/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 242/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 243/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 244/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 245/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 246/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 247/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 248/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 249/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 250/300, seasonal_3 Loss: 0.0782 | 0.0378
Epoch 251/300, seasonal_3 Loss: 0.0782 | 0.0378
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 21, 'train_rates': 0.9356428297728092, 'learning_rate': 0.0005821071483316139, 'batch_size': 168, 'step_size': 10, 'gamma': 0.812829420808725}
Epoch 1/300, resid Loss: 0.4470 | 0.2997
Epoch 2/300, resid Loss: 0.2184 | 0.1642
Epoch 3/300, resid Loss: 0.1746 | 0.1120
Epoch 4/300, resid Loss: 0.1622 | 0.0984
Epoch 5/300, resid Loss: 0.1922 | 0.1084
Epoch 6/300, resid Loss: 0.1920 | 0.1316
Epoch 7/300, resid Loss: 0.1609 | 0.1041
Epoch 8/300, resid Loss: 0.2269 | 0.1314
Epoch 9/300, resid Loss: 0.1816 | 0.0857
Epoch 10/300, resid Loss: 0.1891 | 0.2209
Epoch 11/300, resid Loss: 0.1625 | 0.2339
Epoch 12/300, resid Loss: 0.1570 | 0.0931
Epoch 13/300, resid Loss: 0.1401 | 0.0859
Epoch 14/300, resid Loss: 0.1332 | 0.0756
Epoch 15/300, resid Loss: 0.1085 | 0.0873
Epoch 16/300, resid Loss: 0.1163 | 0.0658
Epoch 17/300, resid Loss: 0.1080 | 0.0678
Epoch 18/300, resid Loss: 0.1121 | 0.0627
Epoch 19/300, resid Loss: 0.1010 | 0.0614
Epoch 20/300, resid Loss: 0.1066 | 0.0617
Epoch 21/300, resid Loss: 0.0977 | 0.0564
Epoch 22/300, resid Loss: 0.1002 | 0.0561
Epoch 23/300, resid Loss: 0.0962 | 0.0588
Epoch 24/300, resid Loss: 0.0961 | 0.0553
Epoch 25/300, resid Loss: 0.0913 | 0.0535
Epoch 26/300, resid Loss: 0.0915 | 0.0542
Epoch 27/300, resid Loss: 0.0897 | 0.0540
Epoch 28/300, resid Loss: 0.0888 | 0.0529
Epoch 29/300, resid Loss: 0.0875 | 0.0519
Epoch 30/300, resid Loss: 0.0866 | 0.0525
Epoch 31/300, resid Loss: 0.0859 | 0.0522
Epoch 32/300, resid Loss: 0.0847 | 0.0513
Epoch 33/300, resid Loss: 0.0842 | 0.0505
Epoch 34/300, resid Loss: 0.0832 | 0.0505
Epoch 35/300, resid Loss: 0.0826 | 0.0496
Epoch 36/300, resid Loss: 0.0817 | 0.0491
Epoch 37/300, resid Loss: 0.0812 | 0.0489
Epoch 38/300, resid Loss: 0.0805 | 0.0488
Epoch 39/300, resid Loss: 0.0798 | 0.0480
Epoch 40/300, resid Loss: 0.0792 | 0.0475
Epoch 41/300, resid Loss: 0.0785 | 0.0472
Epoch 42/300, resid Loss: 0.0779 | 0.0466
Epoch 43/300, resid Loss: 0.0774 | 0.0458
Epoch 44/300, resid Loss: 0.0769 | 0.0456
Epoch 45/300, resid Loss: 0.0765 | 0.0456
Epoch 46/300, resid Loss: 0.0761 | 0.0456
Epoch 47/300, resid Loss: 0.0758 | 0.0448
Epoch 48/300, resid Loss: 0.0755 | 0.0444
Epoch 49/300, resid Loss: 0.0753 | 0.0444
Epoch 50/300, resid Loss: 0.0752 | 0.0446
Epoch 51/300, resid Loss: 0.0753 | 0.0449
Epoch 52/300, resid Loss: 0.0756 | 0.0440
Epoch 53/300, resid Loss: 0.0752 | 0.0432
Epoch 54/300, resid Loss: 0.0743 | 0.0432
Epoch 55/300, resid Loss: 0.0737 | 0.0432
Epoch 56/300, resid Loss: 0.0734 | 0.0428
Epoch 57/300, resid Loss: 0.0733 | 0.0425
Epoch 58/300, resid Loss: 0.0730 | 0.0424
Epoch 59/300, resid Loss: 0.0729 | 0.0423
Epoch 60/300, resid Loss: 0.0727 | 0.0421
Epoch 61/300, resid Loss: 0.0725 | 0.0419
Epoch 62/300, resid Loss: 0.0723 | 0.0418
Epoch 63/300, resid Loss: 0.0722 | 0.0417
Epoch 64/300, resid Loss: 0.0720 | 0.0415
Epoch 65/300, resid Loss: 0.0719 | 0.0414
Epoch 66/300, resid Loss: 0.0717 | 0.0413
Epoch 67/300, resid Loss: 0.0716 | 0.0412
Epoch 68/300, resid Loss: 0.0715 | 0.0411
Epoch 69/300, resid Loss: 0.0714 | 0.0409
Epoch 70/300, resid Loss: 0.0713 | 0.0409
Epoch 71/300, resid Loss: 0.0711 | 0.0407
Epoch 72/300, resid Loss: 0.0710 | 0.0407
Epoch 73/300, resid Loss: 0.0709 | 0.0406
Epoch 74/300, resid Loss: 0.0708 | 0.0405
Epoch 75/300, resid Loss: 0.0707 | 0.0404
Epoch 76/300, resid Loss: 0.0706 | 0.0403
Epoch 77/300, resid Loss: 0.0705 | 0.0402
Epoch 78/300, resid Loss: 0.0704 | 0.0401
Epoch 79/300, resid Loss: 0.0703 | 0.0401
Epoch 80/300, resid Loss: 0.0703 | 0.0400
Epoch 81/300, resid Loss: 0.0702 | 0.0399
Epoch 82/300, resid Loss: 0.0701 | 0.0399
Epoch 83/300, resid Loss: 0.0700 | 0.0398
Epoch 84/300, resid Loss: 0.0699 | 0.0397
Epoch 85/300, resid Loss: 0.0699 | 0.0397
Epoch 86/300, resid Loss: 0.0698 | 0.0396
Epoch 87/300, resid Loss: 0.0697 | 0.0396
Epoch 88/300, resid Loss: 0.0697 | 0.0395
Epoch 89/300, resid Loss: 0.0696 | 0.0394
Epoch 90/300, resid Loss: 0.0695 | 0.0394
Epoch 91/300, resid Loss: 0.0695 | 0.0393
Epoch 92/300, resid Loss: 0.0694 | 0.0393
Epoch 93/300, resid Loss: 0.0694 | 0.0393
Epoch 94/300, resid Loss: 0.0693 | 0.0392
Epoch 95/300, resid Loss: 0.0693 | 0.0392
Epoch 96/300, resid Loss: 0.0692 | 0.0391
Epoch 97/300, resid Loss: 0.0692 | 0.0391
Epoch 98/300, resid Loss: 0.0691 | 0.0390
Epoch 99/300, resid Loss: 0.0691 | 0.0390
Epoch 100/300, resid Loss: 0.0691 | 0.0390
Epoch 101/300, resid Loss: 0.0690 | 0.0389
Epoch 102/300, resid Loss: 0.0690 | 0.0389
Epoch 103/300, resid Loss: 0.0689 | 0.0389
Epoch 104/300, resid Loss: 0.0689 | 0.0388
Epoch 105/300, resid Loss: 0.0689 | 0.0388
Epoch 106/300, resid Loss: 0.0688 | 0.0388
Epoch 107/300, resid Loss: 0.0688 | 0.0387
Epoch 108/300, resid Loss: 0.0688 | 0.0387
Epoch 109/300, resid Loss: 0.0687 | 0.0387
Epoch 110/300, resid Loss: 0.0687 | 0.0387
Epoch 111/300, resid Loss: 0.0687 | 0.0386
Epoch 112/300, resid Loss: 0.0686 | 0.0386
Epoch 113/300, resid Loss: 0.0686 | 0.0386
Epoch 114/300, resid Loss: 0.0686 | 0.0386
Epoch 115/300, resid Loss: 0.0686 | 0.0386
Epoch 116/300, resid Loss: 0.0685 | 0.0385
Epoch 117/300, resid Loss: 0.0685 | 0.0385
Epoch 118/300, resid Loss: 0.0685 | 0.0385
Epoch 119/300, resid Loss: 0.0685 | 0.0385
Epoch 120/300, resid Loss: 0.0685 | 0.0385
Epoch 121/300, resid Loss: 0.0684 | 0.0384
Epoch 122/300, resid Loss: 0.0684 | 0.0384
Epoch 123/300, resid Loss: 0.0684 | 0.0384
Epoch 124/300, resid Loss: 0.0684 | 0.0384
Epoch 125/300, resid Loss: 0.0684 | 0.0384
Epoch 126/300, resid Loss: 0.0684 | 0.0384
Epoch 127/300, resid Loss: 0.0683 | 0.0384
Epoch 128/300, resid Loss: 0.0683 | 0.0383
Epoch 129/300, resid Loss: 0.0683 | 0.0383
Epoch 130/300, resid Loss: 0.0683 | 0.0383
Epoch 131/300, resid Loss: 0.0683 | 0.0383
Epoch 132/300, resid Loss: 0.0683 | 0.0383
Epoch 133/300, resid Loss: 0.0683 | 0.0383
Epoch 134/300, resid Loss: 0.0683 | 0.0383
Epoch 135/300, resid Loss: 0.0682 | 0.0383
Epoch 136/300, resid Loss: 0.0682 | 0.0383
Epoch 137/300, resid Loss: 0.0682 | 0.0382
Epoch 138/300, resid Loss: 0.0682 | 0.0382
Epoch 139/300, resid Loss: 0.0682 | 0.0382
Epoch 140/300, resid Loss: 0.0682 | 0.0382
Epoch 141/300, resid Loss: 0.0682 | 0.0382
Epoch 142/300, resid Loss: 0.0682 | 0.0382
Epoch 143/300, resid Loss: 0.0682 | 0.0382
Epoch 144/300, resid Loss: 0.0682 | 0.0382
Epoch 145/300, resid Loss: 0.0681 | 0.0382
Epoch 146/300, resid Loss: 0.0681 | 0.0382
Epoch 147/300, resid Loss: 0.0681 | 0.0382
Epoch 148/300, resid Loss: 0.0681 | 0.0382
Epoch 149/300, resid Loss: 0.0681 | 0.0382
Epoch 150/300, resid Loss: 0.0681 | 0.0381
Epoch 151/300, resid Loss: 0.0681 | 0.0381
Epoch 152/300, resid Loss: 0.0681 | 0.0381
Epoch 153/300, resid Loss: 0.0681 | 0.0381
Epoch 154/300, resid Loss: 0.0681 | 0.0381
Epoch 155/300, resid Loss: 0.0681 | 0.0381
Epoch 156/300, resid Loss: 0.0681 | 0.0381
Epoch 157/300, resid Loss: 0.0681 | 0.0381
Epoch 158/300, resid Loss: 0.0681 | 0.0381
Epoch 159/300, resid Loss: 0.0681 | 0.0381
Epoch 160/300, resid Loss: 0.0681 | 0.0381
Epoch 161/300, resid Loss: 0.0681 | 0.0381
Epoch 162/300, resid Loss: 0.0681 | 0.0381
Epoch 163/300, resid Loss: 0.0680 | 0.0381
Epoch 164/300, resid Loss: 0.0680 | 0.0381
Epoch 165/300, resid Loss: 0.0680 | 0.0381
Epoch 166/300, resid Loss: 0.0680 | 0.0381
Epoch 167/300, resid Loss: 0.0680 | 0.0381
Epoch 168/300, resid Loss: 0.0680 | 0.0381
Epoch 169/300, resid Loss: 0.0680 | 0.0381
Epoch 170/300, resid Loss: 0.0680 | 0.0381
Epoch 171/300, resid Loss: 0.0680 | 0.0381
Epoch 172/300, resid Loss: 0.0680 | 0.0381
Epoch 173/300, resid Loss: 0.0680 | 0.0381
Epoch 174/300, resid Loss: 0.0680 | 0.0381
Epoch 175/300, resid Loss: 0.0680 | 0.0381
Epoch 176/300, resid Loss: 0.0680 | 0.0381
Epoch 177/300, resid Loss: 0.0680 | 0.0381
Epoch 178/300, resid Loss: 0.0680 | 0.0381
Epoch 179/300, resid Loss: 0.0680 | 0.0381
Epoch 180/300, resid Loss: 0.0680 | 0.0380
Epoch 181/300, resid Loss: 0.0680 | 0.0380
Epoch 182/300, resid Loss: 0.0680 | 0.0380
Epoch 183/300, resid Loss: 0.0680 | 0.0380
Epoch 184/300, resid Loss: 0.0680 | 0.0380
Epoch 185/300, resid Loss: 0.0680 | 0.0380
Epoch 186/300, resid Loss: 0.0680 | 0.0380
Epoch 187/300, resid Loss: 0.0680 | 0.0380
Epoch 188/300, resid Loss: 0.0680 | 0.0380
Epoch 189/300, resid Loss: 0.0680 | 0.0380
Epoch 190/300, resid Loss: 0.0680 | 0.0380
Epoch 191/300, resid Loss: 0.0680 | 0.0380
Epoch 192/300, resid Loss: 0.0680 | 0.0380
Epoch 193/300, resid Loss: 0.0680 | 0.0380
Epoch 194/300, resid Loss: 0.0680 | 0.0380
Epoch 195/300, resid Loss: 0.0680 | 0.0380
Epoch 196/300, resid Loss: 0.0680 | 0.0380
Epoch 197/300, resid Loss: 0.0680 | 0.0380
Epoch 198/300, resid Loss: 0.0680 | 0.0380
Epoch 199/300, resid Loss: 0.0680 | 0.0380
Epoch 200/300, resid Loss: 0.0680 | 0.0380
Epoch 201/300, resid Loss: 0.0680 | 0.0380
Epoch 202/300, resid Loss: 0.0680 | 0.0380
Epoch 203/300, resid Loss: 0.0680 | 0.0380
Epoch 204/300, resid Loss: 0.0680 | 0.0380
Epoch 205/300, resid Loss: 0.0680 | 0.0380
Epoch 206/300, resid Loss: 0.0680 | 0.0380
Epoch 207/300, resid Loss: 0.0680 | 0.0380
Epoch 208/300, resid Loss: 0.0680 | 0.0380
Epoch 209/300, resid Loss: 0.0680 | 0.0380
Epoch 210/300, resid Loss: 0.0680 | 0.0380
Epoch 211/300, resid Loss: 0.0680 | 0.0380
Epoch 212/300, resid Loss: 0.0680 | 0.0380
Epoch 213/300, resid Loss: 0.0680 | 0.0380
Epoch 214/300, resid Loss: 0.0680 | 0.0380
Epoch 215/300, resid Loss: 0.0680 | 0.0380
Epoch 216/300, resid Loss: 0.0680 | 0.0380
Epoch 217/300, resid Loss: 0.0680 | 0.0380
Epoch 218/300, resid Loss: 0.0680 | 0.0380
Epoch 219/300, resid Loss: 0.0680 | 0.0380
Epoch 220/300, resid Loss: 0.0680 | 0.0380
Epoch 221/300, resid Loss: 0.0680 | 0.0380
Epoch 222/300, resid Loss: 0.0680 | 0.0380
Epoch 223/300, resid Loss: 0.0680 | 0.0380
Epoch 224/300, resid Loss: 0.0680 | 0.0380
Epoch 225/300, resid Loss: 0.0680 | 0.0380
Epoch 226/300, resid Loss: 0.0680 | 0.0380
Epoch 227/300, resid Loss: 0.0680 | 0.0380
Epoch 228/300, resid Loss: 0.0680 | 0.0380
Epoch 229/300, resid Loss: 0.0680 | 0.0380
Epoch 230/300, resid Loss: 0.0680 | 0.0380
Epoch 231/300, resid Loss: 0.0680 | 0.0380
Epoch 232/300, resid Loss: 0.0680 | 0.0380
Epoch 233/300, resid Loss: 0.0680 | 0.0380
Epoch 234/300, resid Loss: 0.0680 | 0.0380
Epoch 235/300, resid Loss: 0.0680 | 0.0380
Epoch 236/300, resid Loss: 0.0680 | 0.0380
Epoch 237/300, resid Loss: 0.0680 | 0.0380
Epoch 238/300, resid Loss: 0.0680 | 0.0380
Epoch 239/300, resid Loss: 0.0680 | 0.0380
Epoch 240/300, resid Loss: 0.0680 | 0.0380
Epoch 241/300, resid Loss: 0.0679 | 0.0380
Epoch 242/300, resid Loss: 0.0679 | 0.0380
Epoch 243/300, resid Loss: 0.0679 | 0.0380
Epoch 244/300, resid Loss: 0.0679 | 0.0380
Epoch 245/300, resid Loss: 0.0679 | 0.0380
Epoch 246/300, resid Loss: 0.0679 | 0.0380
Epoch 247/300, resid Loss: 0.0679 | 0.0380
Epoch 248/300, resid Loss: 0.0679 | 0.0380
Epoch 249/300, resid Loss: 0.0679 | 0.0380
Epoch 250/300, resid Loss: 0.0679 | 0.0380
Epoch 251/300, resid Loss: 0.0679 | 0.0380
Epoch 252/300, resid Loss: 0.0679 | 0.0380
Epoch 253/300, resid Loss: 0.0679 | 0.0380
Epoch 254/300, resid Loss: 0.0679 | 0.0380
Epoch 255/300, resid Loss: 0.0679 | 0.0380
Epoch 256/300, resid Loss: 0.0679 | 0.0380
Epoch 257/300, resid Loss: 0.0679 | 0.0380
Epoch 258/300, resid Loss: 0.0679 | 0.0380
Epoch 259/300, resid Loss: 0.0679 | 0.0380
Epoch 260/300, resid Loss: 0.0679 | 0.0380
Epoch 261/300, resid Loss: 0.0679 | 0.0380
Epoch 262/300, resid Loss: 0.0679 | 0.0380
Epoch 263/300, resid Loss: 0.0679 | 0.0380
Epoch 264/300, resid Loss: 0.0679 | 0.0380
Epoch 265/300, resid Loss: 0.0679 | 0.0380
Epoch 266/300, resid Loss: 0.0679 | 0.0380
Epoch 267/300, resid Loss: 0.0679 | 0.0380
Epoch 268/300, resid Loss: 0.0679 | 0.0380
Epoch 269/300, resid Loss: 0.0679 | 0.0380
Epoch 270/300, resid Loss: 0.0679 | 0.0380
Epoch 271/300, resid Loss: 0.0679 | 0.0380
Epoch 272/300, resid Loss: 0.0679 | 0.0380
Epoch 273/300, resid Loss: 0.0679 | 0.0380
Epoch 274/300, resid Loss: 0.0679 | 0.0380
Epoch 275/300, resid Loss: 0.0679 | 0.0380
Epoch 276/300, resid Loss: 0.0679 | 0.0380
Epoch 277/300, resid Loss: 0.0679 | 0.0380
Epoch 278/300, resid Loss: 0.0679 | 0.0380
Epoch 279/300, resid Loss: 0.0679 | 0.0380
Epoch 280/300, resid Loss: 0.0679 | 0.0380
Epoch 281/300, resid Loss: 0.0679 | 0.0380
Epoch 282/300, resid Loss: 0.0679 | 0.0380
Epoch 283/300, resid Loss: 0.0679 | 0.0380
Epoch 284/300, resid Loss: 0.0679 | 0.0380
Epoch 285/300, resid Loss: 0.0679 | 0.0380
Epoch 286/300, resid Loss: 0.0679 | 0.0380
Epoch 287/300, resid Loss: 0.0679 | 0.0380
Epoch 288/300, resid Loss: 0.0679 | 0.0380
Epoch 289/300, resid Loss: 0.0679 | 0.0380
Epoch 290/300, resid Loss: 0.0679 | 0.0380
Epoch 291/300, resid Loss: 0.0679 | 0.0380
Epoch 292/300, resid Loss: 0.0679 | 0.0380
Epoch 293/300, resid Loss: 0.0679 | 0.0380
Epoch 294/300, resid Loss: 0.0679 | 0.0380
Epoch 295/300, resid Loss: 0.0679 | 0.0380
Epoch 296/300, resid Loss: 0.0679 | 0.0380
Epoch 297/300, resid Loss: 0.0679 | 0.0380
Epoch 298/300, resid Loss: 0.0679 | 0.0380
Epoch 299/300, resid Loss: 0.0679 | 0.0380
Epoch 300/300, resid Loss: 0.0679 | 0.0380
Early stopping for resid
Runtime (seconds): 1978.2294824123383
0.00022729458521905302
[186.65503]
[-0.7915868]
[-1.1409757]
[0.852703]
[-0.28577226]
[10.859334]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 28.103296861285344
RMSE: 5.3012542724609375
MAE: 5.3012542724609375
R-squared: nan
[196.14874]
