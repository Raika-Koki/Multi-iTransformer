[32m[I 2025-02-09 01:28:53,978][0m A new study created in memory with name: no-name-010595b1-350c-432d-9211-e37d6aff1f76[0m
[32m[I 2025-02-09 01:29:20,418][0m Trial 0 finished with value: 0.3173293471336365 and parameters: {'observation_period_num': 207, 'train_rates': 0.9774662436492607, 'learning_rate': 0.0006906704454394686, 'batch_size': 253, 'step_size': 1, 'gamma': 0.9894116214242159}. Best is trial 0 with value: 0.3173293471336365.[0m
[32m[I 2025-02-09 01:33:15,288][0m Trial 1 finished with value: 0.17406750715057603 and parameters: {'observation_period_num': 9, 'train_rates': 0.6854563422180973, 'learning_rate': 4.386108132714537e-06, 'batch_size': 20, 'step_size': 7, 'gamma': 0.8765980079719614}. Best is trial 1 with value: 0.17406750715057603.[0m
[32m[I 2025-02-09 01:34:01,811][0m Trial 2 finished with value: 0.18620386381390858 and parameters: {'observation_period_num': 71, 'train_rates': 0.7029095029107835, 'learning_rate': 9.252956413746788e-05, 'batch_size': 109, 'step_size': 7, 'gamma': 0.9452003145306235}. Best is trial 1 with value: 0.17406750715057603.[0m
[32m[I 2025-02-09 01:34:57,819][0m Trial 3 finished with value: 0.3458239711247958 and parameters: {'observation_period_num': 47, 'train_rates': 0.6304441447713293, 'learning_rate': 3.321753002887859e-05, 'batch_size': 84, 'step_size': 6, 'gamma': 0.8685934405650473}. Best is trial 1 with value: 0.17406750715057603.[0m
[32m[I 2025-02-09 01:35:19,225][0m Trial 4 finished with value: 0.9838133041203083 and parameters: {'observation_period_num': 243, 'train_rates': 0.6478394168552789, 'learning_rate': 2.667134226774636e-06, 'batch_size': 254, 'step_size': 9, 'gamma': 0.8153367497276662}. Best is trial 1 with value: 0.17406750715057603.[0m
[32m[I 2025-02-09 01:35:52,135][0m Trial 5 finished with value: 0.04092466259836384 and parameters: {'observation_period_num': 17, 'train_rates': 0.791760989189966, 'learning_rate': 0.0005053624307935258, 'batch_size': 170, 'step_size': 8, 'gamma': 0.858365491718898}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:36:19,720][0m Trial 6 finished with value: 0.32792207335448037 and parameters: {'observation_period_num': 124, 'train_rates': 0.7798768724672991, 'learning_rate': 1.9406108447035125e-05, 'batch_size': 222, 'step_size': 14, 'gamma': 0.837510770446964}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:37:06,134][0m Trial 7 finished with value: 0.06178575183296016 and parameters: {'observation_period_num': 63, 'train_rates': 0.8270190705133814, 'learning_rate': 0.00048805082619599906, 'batch_size': 123, 'step_size': 4, 'gamma': 0.8859545718022847}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:37:42,066][0m Trial 8 finished with value: 0.18647889609434712 and parameters: {'observation_period_num': 9, 'train_rates': 0.9106441756423338, 'learning_rate': 4.8491811093958186e-06, 'batch_size': 177, 'step_size': 14, 'gamma': 0.9864241664638328}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:38:14,190][0m Trial 9 finished with value: 0.23052435516711453 and parameters: {'observation_period_num': 144, 'train_rates': 0.6520890502111628, 'learning_rate': 0.00033064177271238204, 'batch_size': 155, 'step_size': 15, 'gamma': 0.7883245924358766}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:38:44,568][0m Trial 10 finished with value: 0.152010307676536 and parameters: {'observation_period_num': 127, 'train_rates': 0.7894225026731199, 'learning_rate': 0.00011329659612995893, 'batch_size': 184, 'step_size': 11, 'gamma': 0.7515531502975263}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:39:39,266][0m Trial 11 finished with value: 0.09956136820673642 and parameters: {'observation_period_num': 70, 'train_rates': 0.8645230188326446, 'learning_rate': 0.000969245999408162, 'batch_size': 106, 'step_size': 3, 'gamma': 0.9023058248410376}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:40:58,321][0m Trial 12 finished with value: 0.055345647037029266 and parameters: {'observation_period_num': 45, 'train_rates': 0.8560965935708693, 'learning_rate': 0.00022378631027329592, 'batch_size': 71, 'step_size': 4, 'gamma': 0.9249472759558509}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:42:41,996][0m Trial 13 finished with value: 0.1797134001692634 and parameters: {'observation_period_num': 35, 'train_rates': 0.7445103547215637, 'learning_rate': 0.00020464735208368777, 'batch_size': 49, 'step_size': 10, 'gamma': 0.9410987988140598}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:44:03,471][0m Trial 14 finished with value: 0.09706615147635687 and parameters: {'observation_period_num': 95, 'train_rates': 0.8818751180950679, 'learning_rate': 5.150395743617566e-05, 'batch_size': 70, 'step_size': 4, 'gamma': 0.9237810330410021}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:44:43,701][0m Trial 15 finished with value: 0.17310775233351666 and parameters: {'observation_period_num': 161, 'train_rates': 0.935505681170726, 'learning_rate': 0.0002065211059999262, 'batch_size': 151, 'step_size': 12, 'gamma': 0.8536157852856049}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:45:11,779][0m Trial 16 finished with value: 0.2893027007170618 and parameters: {'observation_period_num': 100, 'train_rates': 0.8494800845414417, 'learning_rate': 1.8058346446754665e-05, 'batch_size': 206, 'step_size': 5, 'gamma': 0.9097202841328136}. Best is trial 5 with value: 0.04092466259836384.[0m
Early stopping at epoch 87
[32m[I 2025-02-09 01:48:52,006][0m Trial 17 finished with value: 0.7883721676965555 and parameters: {'observation_period_num': 29, 'train_rates': 0.7417298698750319, 'learning_rate': 1.15135161465733e-06, 'batch_size': 20, 'step_size': 1, 'gamma': 0.8268562699861836}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:50:22,269][0m Trial 18 finished with value: 0.13199488850890162 and parameters: {'observation_period_num': 6, 'train_rates': 0.8188166144424242, 'learning_rate': 0.0002681871229666061, 'batch_size': 61, 'step_size': 8, 'gamma': 0.9573262182400981}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:51:25,477][0m Trial 19 finished with value: 0.21619167502927958 and parameters: {'observation_period_num': 92, 'train_rates': 0.9075376649210328, 'learning_rate': 9.79955687776756e-05, 'batch_size': 94, 'step_size': 3, 'gamma': 0.7811011413477398}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:52:03,362][0m Trial 20 finished with value: 0.3615459332363211 and parameters: {'observation_period_num': 171, 'train_rates': 0.7543796947736501, 'learning_rate': 0.00044419178599218716, 'batch_size': 138, 'step_size': 9, 'gamma': 0.897628279452775}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:52:48,810][0m Trial 21 finished with value: 0.061327656268957975 and parameters: {'observation_period_num': 60, 'train_rates': 0.8246477884542327, 'learning_rate': 0.0005051522516184798, 'batch_size': 122, 'step_size': 5, 'gamma': 0.8728261645827983}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:53:23,870][0m Trial 22 finished with value: 0.051352302781280945 and parameters: {'observation_period_num': 48, 'train_rates': 0.812995368397713, 'learning_rate': 0.0007981599972972672, 'batch_size': 167, 'step_size': 6, 'gamma': 0.8543665005349967}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:53:57,090][0m Trial 23 finished with value: 0.18591878425738798 and parameters: {'observation_period_num': 34, 'train_rates': 0.7788748087644548, 'learning_rate': 0.0009687690075249996, 'batch_size': 174, 'step_size': 7, 'gamma': 0.8502329459476053}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:54:24,267][0m Trial 24 finished with value: 0.1649485233249387 and parameters: {'observation_period_num': 27, 'train_rates': 0.7050491639832224, 'learning_rate': 0.00022367391046476114, 'batch_size': 203, 'step_size': 6, 'gamma': 0.8053131303541499}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:55:03,327][0m Trial 25 finished with value: 0.09076457910689732 and parameters: {'observation_period_num': 84, 'train_rates': 0.8470250754862619, 'learning_rate': 0.00012845282625968367, 'batch_size': 154, 'step_size': 3, 'gamma': 0.9229944205650787}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:55:31,598][0m Trial 26 finished with value: 0.06876423690588243 and parameters: {'observation_period_num': 48, 'train_rates': 0.8111789853936433, 'learning_rate': 0.0006417094084317358, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8543740993295581}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:56:03,751][0m Trial 27 finished with value: 0.23971712951081248 and parameters: {'observation_period_num': 114, 'train_rates': 0.8916474871884901, 'learning_rate': 5.592817321069188e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8401792461039321}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:56:50,277][0m Trial 28 finished with value: 0.060855386554297196 and parameters: {'observation_period_num': 50, 'train_rates': 0.9440287564743212, 'learning_rate': 0.00035193122725200355, 'batch_size': 135, 'step_size': 2, 'gamma': 0.9709633639026547}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:57:20,915][0m Trial 29 finished with value: 0.1441887617111206 and parameters: {'observation_period_num': 25, 'train_rates': 0.9773198115570072, 'learning_rate': 0.0007779313415392402, 'batch_size': 235, 'step_size': 1, 'gamma': 0.8851994573028256}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:57:51,975][0m Trial 30 finished with value: 0.29460025377549476 and parameters: {'observation_period_num': 214, 'train_rates': 0.758429049218745, 'learning_rate': 0.00014921163653801179, 'batch_size': 165, 'step_size': 6, 'gamma': 0.915983496062868}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 01:58:41,053][0m Trial 31 finished with value: 0.07451178133487701 and parameters: {'observation_period_num': 53, 'train_rates': 0.9598762266134998, 'learning_rate': 0.000445321765611604, 'batch_size': 134, 'step_size': 2, 'gamma': 0.980271101365915}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 02:00:57,344][0m Trial 32 finished with value: 0.16034179064695553 and parameters: {'observation_period_num': 80, 'train_rates': 0.9363354320854862, 'learning_rate': 0.0003107103106010707, 'batch_size': 43, 'step_size': 2, 'gamma': 0.9652666688330338}. Best is trial 5 with value: 0.04092466259836384.[0m
[32m[I 2025-02-09 02:01:39,895][0m Trial 33 finished with value: 0.03898455981333651 and parameters: {'observation_period_num': 17, 'train_rates': 0.8668798021929134, 'learning_rate': 0.0006221988666165145, 'batch_size': 142, 'step_size': 4, 'gamma': 0.9407710930934408}. Best is trial 33 with value: 0.03898455981333651.[0m
[32m[I 2025-02-09 02:02:40,142][0m Trial 34 finished with value: 0.11305622342042625 and parameters: {'observation_period_num': 17, 'train_rates': 0.8661024842208364, 'learning_rate': 0.0005635716347336149, 'batch_size': 97, 'step_size': 7, 'gamma': 0.9379929864527797}. Best is trial 33 with value: 0.03898455981333651.[0m
[32m[I 2025-02-09 02:03:19,792][0m Trial 35 finished with value: 0.05103135235455052 and parameters: {'observation_period_num': 41, 'train_rates': 0.802165660981597, 'learning_rate': 0.0007359295726951192, 'batch_size': 145, 'step_size': 4, 'gamma': 0.8673875643581861}. Best is trial 33 with value: 0.03898455981333651.[0m
[32m[I 2025-02-09 02:03:55,780][0m Trial 36 finished with value: 0.036461824303081425 and parameters: {'observation_period_num': 6, 'train_rates': 0.8021350674126783, 'learning_rate': 0.0007394705311144837, 'batch_size': 159, 'step_size': 6, 'gamma': 0.8647890706016277}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:04:38,186][0m Trial 37 finished with value: 0.15623652707102031 and parameters: {'observation_period_num': 16, 'train_rates': 0.7168268192090032, 'learning_rate': 6.395138689110536e-05, 'batch_size': 119, 'step_size': 9, 'gamma': 0.8710125579995383}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:05:17,656][0m Trial 38 finished with value: 0.21394033779779564 and parameters: {'observation_period_num': 18, 'train_rates': 0.7892590901338306, 'learning_rate': 1.0025262570454276e-05, 'batch_size': 142, 'step_size': 5, 'gamma': 0.8875563390396958}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:05:45,298][0m Trial 39 finished with value: 0.1311784060729533 and parameters: {'observation_period_num': 5, 'train_rates': 0.6761911806510205, 'learning_rate': 0.0007089883274846337, 'batch_size': 189, 'step_size': 7, 'gamma': 0.819505896085184}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:06:21,688][0m Trial 40 finished with value: 0.061604386673431204 and parameters: {'observation_period_num': 38, 'train_rates': 0.8331911688589756, 'learning_rate': 0.00038681535325916627, 'batch_size': 164, 'step_size': 4, 'gamma': 0.8396819735344935}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:06:54,966][0m Trial 41 finished with value: 0.1943779624233847 and parameters: {'observation_period_num': 62, 'train_rates': 0.7681422941527677, 'learning_rate': 0.0007391791422884415, 'batch_size': 167, 'step_size': 6, 'gamma': 0.8669701572684397}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:07:32,032][0m Trial 42 finished with value: 0.06054740284787865 and parameters: {'observation_period_num': 39, 'train_rates': 0.7998118371024967, 'learning_rate': 0.0009933601487133472, 'batch_size': 148, 'step_size': 6, 'gamma': 0.8596630622550474}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:08:04,244][0m Trial 43 finished with value: 0.04104403229382394 and parameters: {'observation_period_num': 20, 'train_rates': 0.8045554376609426, 'learning_rate': 0.0005721146692747698, 'batch_size': 179, 'step_size': 8, 'gamma': 0.8870186877575634}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:08:35,535][0m Trial 44 finished with value: 0.16648950988683478 and parameters: {'observation_period_num': 19, 'train_rates': 0.7349485156030435, 'learning_rate': 0.0005524782337504291, 'batch_size': 179, 'step_size': 9, 'gamma': 0.8926636199736444}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:09:01,451][0m Trial 45 finished with value: 0.04283364799743641 and parameters: {'observation_period_num': 5, 'train_rates': 0.8327697224018255, 'learning_rate': 0.00017822139274060425, 'batch_size': 229, 'step_size': 10, 'gamma': 0.878353698020276}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:09:29,749][0m Trial 46 finished with value: 0.0449226267331717 and parameters: {'observation_period_num': 15, 'train_rates': 0.8358709505836267, 'learning_rate': 0.00018657098828020748, 'batch_size': 243, 'step_size': 11, 'gamma': 0.8844026106289143}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:09:59,459][0m Trial 47 finished with value: 0.03716945118243926 and parameters: {'observation_period_num': 8, 'train_rates': 0.8728918171635076, 'learning_rate': 0.00031291519399606903, 'batch_size': 214, 'step_size': 10, 'gamma': 0.908450332242218}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:10:30,287][0m Trial 48 finished with value: 0.048756956768456414 and parameters: {'observation_period_num': 25, 'train_rates': 0.8904240429798532, 'learning_rate': 0.00029429775875487003, 'batch_size': 215, 'step_size': 12, 'gamma': 0.911457767044476}. Best is trial 36 with value: 0.036461824303081425.[0m
[32m[I 2025-02-09 02:11:02,223][0m Trial 49 finished with value: 0.06954971039742631 and parameters: {'observation_period_num': 68, 'train_rates': 0.8730944715782061, 'learning_rate': 0.000426129630114725, 'batch_size': 194, 'step_size': 8, 'gamma': 0.9344174150212778}. Best is trial 36 with value: 0.036461824303081425.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.5100 | 0.2035
Epoch 2/300, Loss: 0.1909 | 0.1556
Epoch 3/300, Loss: 0.2160 | 0.1546
Epoch 4/300, Loss: 0.1689 | 0.2810
Epoch 5/300, Loss: 0.1935 | 0.1850
Epoch 6/300, Loss: 0.1574 | 0.1215
Epoch 7/300, Loss: 0.1489 | 0.1215
Epoch 8/300, Loss: 0.1417 | 0.1040
Epoch 9/300, Loss: 0.1324 | 0.1247
Epoch 10/300, Loss: 0.1467 | 0.1498
Epoch 11/300, Loss: 0.1311 | 0.0925
Epoch 12/300, Loss: 0.1356 | 0.0875
Epoch 13/300, Loss: 0.1163 | 0.0769
Epoch 14/300, Loss: 0.1146 | 0.0724
Epoch 15/300, Loss: 0.1081 | 0.0673
Epoch 16/300, Loss: 0.1078 | 0.0710
Epoch 17/300, Loss: 0.1049 | 0.0651
Epoch 18/300, Loss: 0.1025 | 0.0652
Epoch 19/300, Loss: 0.1026 | 0.0661
Epoch 20/300, Loss: 0.1011 | 0.0638
Epoch 21/300, Loss: 0.0973 | 0.0626
Epoch 22/300, Loss: 0.0960 | 0.0606
Epoch 23/300, Loss: 0.0917 | 0.0580
Epoch 24/300, Loss: 0.0916 | 0.0565
Epoch 25/300, Loss: 0.0897 | 0.0551
Epoch 26/300, Loss: 0.0888 | 0.0535
Epoch 27/300, Loss: 0.0879 | 0.0535
Epoch 28/300, Loss: 0.0872 | 0.0528
Epoch 29/300, Loss: 0.0867 | 0.0529
Epoch 30/300, Loss: 0.0864 | 0.0517
Epoch 31/300, Loss: 0.0856 | 0.0510
Epoch 32/300, Loss: 0.0851 | 0.0508
Epoch 33/300, Loss: 0.0847 | 0.0505
Epoch 34/300, Loss: 0.0844 | 0.0502
Epoch 35/300, Loss: 0.0841 | 0.0496
Epoch 36/300, Loss: 0.0837 | 0.0495
Epoch 37/300, Loss: 0.0834 | 0.0492
Epoch 38/300, Loss: 0.0832 | 0.0493
Epoch 39/300, Loss: 0.0829 | 0.0488
Epoch 40/300, Loss: 0.0827 | 0.0490
Epoch 41/300, Loss: 0.0824 | 0.0484
Epoch 42/300, Loss: 0.0822 | 0.0484
Epoch 43/300, Loss: 0.0820 | 0.0481
Epoch 44/300, Loss: 0.0818 | 0.0482
Epoch 45/300, Loss: 0.0816 | 0.0477
Epoch 46/300, Loss: 0.0815 | 0.0478
Epoch 47/300, Loss: 0.0813 | 0.0475
Epoch 48/300, Loss: 0.0812 | 0.0474
Epoch 49/300, Loss: 0.0810 | 0.0470
Epoch 50/300, Loss: 0.0810 | 0.0470
Epoch 51/300, Loss: 0.0809 | 0.0467
Epoch 52/300, Loss: 0.0809 | 0.0464
Epoch 53/300, Loss: 0.0813 | 0.0464
Epoch 54/300, Loss: 0.0819 | 0.0466
Epoch 55/300, Loss: 0.0830 | 0.0461
Epoch 56/300, Loss: 0.0829 | 0.0466
Epoch 57/300, Loss: 0.0811 | 0.0463
Epoch 58/300, Loss: 0.0803 | 0.0460
Epoch 59/300, Loss: 0.0802 | 0.0455
Epoch 60/300, Loss: 0.0799 | 0.0455
Epoch 61/300, Loss: 0.0798 | 0.0455
Epoch 62/300, Loss: 0.0797 | 0.0455
Epoch 63/300, Loss: 0.0796 | 0.0453
Epoch 64/300, Loss: 0.0795 | 0.0452
Epoch 65/300, Loss: 0.0795 | 0.0452
Epoch 66/300, Loss: 0.0794 | 0.0451
Epoch 67/300, Loss: 0.0794 | 0.0451
Epoch 68/300, Loss: 0.0793 | 0.0450
Epoch 69/300, Loss: 0.0792 | 0.0449
Epoch 70/300, Loss: 0.0792 | 0.0449
Epoch 71/300, Loss: 0.0791 | 0.0448
Epoch 72/300, Loss: 0.0791 | 0.0448
Epoch 73/300, Loss: 0.0790 | 0.0447
Epoch 74/300, Loss: 0.0790 | 0.0447
Epoch 75/300, Loss: 0.0790 | 0.0446
Epoch 76/300, Loss: 0.0789 | 0.0446
Epoch 77/300, Loss: 0.0789 | 0.0445
Epoch 78/300, Loss: 0.0788 | 0.0445
Epoch 79/300, Loss: 0.0788 | 0.0445
Epoch 80/300, Loss: 0.0788 | 0.0444
Epoch 81/300, Loss: 0.0787 | 0.0444
Epoch 82/300, Loss: 0.0787 | 0.0443
Epoch 83/300, Loss: 0.0787 | 0.0443
Epoch 84/300, Loss: 0.0786 | 0.0443
Epoch 85/300, Loss: 0.0786 | 0.0442
Epoch 86/300, Loss: 0.0786 | 0.0442
Epoch 87/300, Loss: 0.0786 | 0.0442
Epoch 88/300, Loss: 0.0785 | 0.0442
Epoch 89/300, Loss: 0.0785 | 0.0441
Epoch 90/300, Loss: 0.0785 | 0.0441
Epoch 91/300, Loss: 0.0785 | 0.0441
Epoch 92/300, Loss: 0.0784 | 0.0441
Epoch 93/300, Loss: 0.0784 | 0.0440
Epoch 94/300, Loss: 0.0784 | 0.0440
Epoch 95/300, Loss: 0.0784 | 0.0440
Epoch 96/300, Loss: 0.0784 | 0.0440
Epoch 97/300, Loss: 0.0784 | 0.0440
Epoch 98/300, Loss: 0.0783 | 0.0439
Epoch 99/300, Loss: 0.0783 | 0.0439
Epoch 100/300, Loss: 0.0783 | 0.0439
Epoch 101/300, Loss: 0.0783 | 0.0439
Epoch 102/300, Loss: 0.0783 | 0.0439
Epoch 103/300, Loss: 0.0783 | 0.0439
Epoch 104/300, Loss: 0.0783 | 0.0439
Epoch 105/300, Loss: 0.0782 | 0.0438
Epoch 106/300, Loss: 0.0782 | 0.0438
Epoch 107/300, Loss: 0.0782 | 0.0438
Epoch 108/300, Loss: 0.0782 | 0.0438
Epoch 109/300, Loss: 0.0782 | 0.0438
Epoch 110/300, Loss: 0.0782 | 0.0438
Epoch 111/300, Loss: 0.0782 | 0.0438
Epoch 112/300, Loss: 0.0782 | 0.0438
Epoch 113/300, Loss: 0.0782 | 0.0438
Epoch 114/300, Loss: 0.0782 | 0.0438
Epoch 115/300, Loss: 0.0781 | 0.0437
Epoch 116/300, Loss: 0.0781 | 0.0437
Epoch 117/300, Loss: 0.0781 | 0.0437
Epoch 118/300, Loss: 0.0781 | 0.0437
Epoch 119/300, Loss: 0.0781 | 0.0437
Epoch 120/300, Loss: 0.0781 | 0.0437
Epoch 121/300, Loss: 0.0781 | 0.0437
Epoch 122/300, Loss: 0.0781 | 0.0437
Epoch 123/300, Loss: 0.0781 | 0.0437
Epoch 124/300, Loss: 0.0781 | 0.0437
Epoch 125/300, Loss: 0.0781 | 0.0437
Epoch 126/300, Loss: 0.0781 | 0.0437
Epoch 127/300, Loss: 0.0781 | 0.0437
Epoch 128/300, Loss: 0.0781 | 0.0437
Epoch 129/300, Loss: 0.0781 | 0.0437
Epoch 130/300, Loss: 0.0781 | 0.0437
Epoch 131/300, Loss: 0.0781 | 0.0436
Epoch 132/300, Loss: 0.0780 | 0.0436
Epoch 133/300, Loss: 0.0780 | 0.0436
Epoch 134/300, Loss: 0.0780 | 0.0436
Epoch 135/300, Loss: 0.0780 | 0.0436
Epoch 136/300, Loss: 0.0780 | 0.0436
Epoch 137/300, Loss: 0.0780 | 0.0436
Epoch 138/300, Loss: 0.0780 | 0.0436
Epoch 139/300, Loss: 0.0780 | 0.0436
Epoch 140/300, Loss: 0.0780 | 0.0436
Epoch 141/300, Loss: 0.0780 | 0.0436
Epoch 142/300, Loss: 0.0780 | 0.0436
Epoch 143/300, Loss: 0.0780 | 0.0436
Epoch 144/300, Loss: 0.0780 | 0.0436
Epoch 145/300, Loss: 0.0780 | 0.0436
Epoch 146/300, Loss: 0.0780 | 0.0436
Epoch 147/300, Loss: 0.0780 | 0.0436
Epoch 148/300, Loss: 0.0780 | 0.0436
Epoch 149/300, Loss: 0.0780 | 0.0436
Epoch 150/300, Loss: 0.0780 | 0.0436
Epoch 151/300, Loss: 0.0780 | 0.0436
Epoch 152/300, Loss: 0.0780 | 0.0436
Epoch 153/300, Loss: 0.0780 | 0.0436
Epoch 154/300, Loss: 0.0780 | 0.0436
Epoch 155/300, Loss: 0.0780 | 0.0436
Epoch 156/300, Loss: 0.0780 | 0.0436
Epoch 157/300, Loss: 0.0780 | 0.0436
Epoch 158/300, Loss: 0.0780 | 0.0436
Epoch 159/300, Loss: 0.0780 | 0.0436
Epoch 160/300, Loss: 0.0780 | 0.0436
Epoch 161/300, Loss: 0.0780 | 0.0436
Epoch 162/300, Loss: 0.0780 | 0.0436
Epoch 163/300, Loss: 0.0780 | 0.0436
Epoch 164/300, Loss: 0.0780 | 0.0436
Epoch 165/300, Loss: 0.0780 | 0.0436
Epoch 166/300, Loss: 0.0780 | 0.0436
Epoch 167/300, Loss: 0.0780 | 0.0436
Epoch 168/300, Loss: 0.0780 | 0.0436
Epoch 169/300, Loss: 0.0780 | 0.0436
Epoch 170/300, Loss: 0.0780 | 0.0436
Epoch 171/300, Loss: 0.0780 | 0.0436
Epoch 172/300, Loss: 0.0780 | 0.0436
Epoch 173/300, Loss: 0.0780 | 0.0436
Epoch 174/300, Loss: 0.0780 | 0.0436
Epoch 175/300, Loss: 0.0780 | 0.0436
Epoch 176/300, Loss: 0.0780 | 0.0436
Epoch 177/300, Loss: 0.0780 | 0.0436
Epoch 178/300, Loss: 0.0780 | 0.0436
Epoch 179/300, Loss: 0.0780 | 0.0436
Epoch 180/300, Loss: 0.0780 | 0.0436
Epoch 181/300, Loss: 0.0780 | 0.0436
Epoch 182/300, Loss: 0.0780 | 0.0436
Epoch 183/300, Loss: 0.0780 | 0.0436
Epoch 184/300, Loss: 0.0780 | 0.0436
Epoch 185/300, Loss: 0.0780 | 0.0436
Epoch 186/300, Loss: 0.0780 | 0.0436
Epoch 187/300, Loss: 0.0780 | 0.0436
Epoch 188/300, Loss: 0.0780 | 0.0436
Epoch 189/300, Loss: 0.0780 | 0.0436
Epoch 190/300, Loss: 0.0780 | 0.0436
Epoch 191/300, Loss: 0.0780 | 0.0436
Epoch 192/300, Loss: 0.0780 | 0.0436
Epoch 193/300, Loss: 0.0780 | 0.0436
Epoch 194/300, Loss: 0.0780 | 0.0436
Epoch 195/300, Loss: 0.0780 | 0.0436
Epoch 196/300, Loss: 0.0780 | 0.0436
Epoch 197/300, Loss: 0.0780 | 0.0436
Epoch 198/300, Loss: 0.0780 | 0.0436
Epoch 199/300, Loss: 0.0780 | 0.0436
Epoch 200/300, Loss: 0.0780 | 0.0436
Epoch 201/300, Loss: 0.0780 | 0.0436
Epoch 202/300, Loss: 0.0780 | 0.0436
Epoch 203/300, Loss: 0.0780 | 0.0436
Epoch 204/300, Loss: 0.0780 | 0.0436
Epoch 205/300, Loss: 0.0780 | 0.0436
Epoch 206/300, Loss: 0.0780 | 0.0436
Epoch 207/300, Loss: 0.0780 | 0.0436
Epoch 208/300, Loss: 0.0780 | 0.0436
Epoch 209/300, Loss: 0.0780 | 0.0436
Epoch 210/300, Loss: 0.0780 | 0.0436
Epoch 211/300, Loss: 0.0780 | 0.0436
Epoch 212/300, Loss: 0.0780 | 0.0436
Epoch 213/300, Loss: 0.0780 | 0.0436
Epoch 214/300, Loss: 0.0780 | 0.0436
Epoch 215/300, Loss: 0.0780 | 0.0436
Epoch 216/300, Loss: 0.0780 | 0.0436
Epoch 217/300, Loss: 0.0780 | 0.0436
Epoch 218/300, Loss: 0.0780 | 0.0436
Epoch 219/300, Loss: 0.0780 | 0.0436
Epoch 220/300, Loss: 0.0780 | 0.0436
Epoch 221/300, Loss: 0.0780 | 0.0436
Epoch 222/300, Loss: 0.0780 | 0.0436
Epoch 223/300, Loss: 0.0780 | 0.0436
Epoch 224/300, Loss: 0.0780 | 0.0436
Epoch 225/300, Loss: 0.0780 | 0.0436
Epoch 226/300, Loss: 0.0780 | 0.0436
Epoch 227/300, Loss: 0.0780 | 0.0436
Epoch 228/300, Loss: 0.0780 | 0.0436
Epoch 229/300, Loss: 0.0780 | 0.0436
Epoch 230/300, Loss: 0.0780 | 0.0436
Epoch 231/300, Loss: 0.0780 | 0.0436
Epoch 232/300, Loss: 0.0780 | 0.0436
Epoch 233/300, Loss: 0.0780 | 0.0436
Epoch 234/300, Loss: 0.0780 | 0.0436
Epoch 235/300, Loss: 0.0780 | 0.0436
Epoch 236/300, Loss: 0.0780 | 0.0436
Epoch 237/300, Loss: 0.0780 | 0.0436
Epoch 238/300, Loss: 0.0780 | 0.0436
Epoch 239/300, Loss: 0.0780 | 0.0436
Epoch 240/300, Loss: 0.0780 | 0.0436
Epoch 241/300, Loss: 0.0780 | 0.0436
Epoch 242/300, Loss: 0.0780 | 0.0436
Epoch 243/300, Loss: 0.0780 | 0.0436
Epoch 244/300, Loss: 0.0780 | 0.0436
Epoch 245/300, Loss: 0.0780 | 0.0436
Epoch 246/300, Loss: 0.0780 | 0.0436
Epoch 247/300, Loss: 0.0780 | 0.0436
Epoch 248/300, Loss: 0.0780 | 0.0436
Epoch 249/300, Loss: 0.0780 | 0.0436
Epoch 250/300, Loss: 0.0780 | 0.0436
Epoch 251/300, Loss: 0.0780 | 0.0436
Epoch 252/300, Loss: 0.0780 | 0.0436
Epoch 253/300, Loss: 0.0780 | 0.0436
Epoch 254/300, Loss: 0.0780 | 0.0436
Epoch 255/300, Loss: 0.0780 | 0.0436
Epoch 256/300, Loss: 0.0780 | 0.0436
Epoch 257/300, Loss: 0.0780 | 0.0436
Epoch 258/300, Loss: 0.0780 | 0.0436
Epoch 259/300, Loss: 0.0780 | 0.0436
Epoch 260/300, Loss: 0.0780 | 0.0436
Epoch 261/300, Loss: 0.0780 | 0.0436
Epoch 262/300, Loss: 0.0780 | 0.0436
Epoch 263/300, Loss: 0.0780 | 0.0436
Epoch 264/300, Loss: 0.0780 | 0.0436
Epoch 265/300, Loss: 0.0780 | 0.0436
Epoch 266/300, Loss: 0.0780 | 0.0436
Epoch 267/300, Loss: 0.0780 | 0.0436
Epoch 268/300, Loss: 0.0780 | 0.0436
Epoch 269/300, Loss: 0.0780 | 0.0436
Epoch 270/300, Loss: 0.0780 | 0.0436
Epoch 271/300, Loss: 0.0780 | 0.0436
Epoch 272/300, Loss: 0.0780 | 0.0436
Epoch 273/300, Loss: 0.0780 | 0.0436
Epoch 274/300, Loss: 0.0780 | 0.0436
Epoch 275/300, Loss: 0.0780 | 0.0436
Epoch 276/300, Loss: 0.0780 | 0.0436
Epoch 277/300, Loss: 0.0780 | 0.0436
Epoch 278/300, Loss: 0.0780 | 0.0436
Early stopping
Runtime (seconds): 101.36804986000061
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 105.53568020113744
RMSE: 10.273056030273438
MAE: 10.273056030273438
R-squared: nan
[197.41306]
