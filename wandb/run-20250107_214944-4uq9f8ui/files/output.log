[32m[I 2025-01-07 21:49:45,120][0m A new study created in memory with name: no-name-9ddc3359-8ac6-45a2-8291-a7e2085908fe[0m
[32m[I 2025-01-07 21:54:16,446][0m Trial 0 finished with value: 1.4229536677517087 and parameters: {'observation_period_num': 223, 'train_rates': 0.613198915140916, 'learning_rate': 0.00022103627632537244, 'batch_size': 27, 'step_size': 12, 'gamma': 0.8304137438902404}. Best is trial 0 with value: 1.4229536677517087.[0m
[32m[I 2025-01-07 21:58:18,054][0m Trial 1 finished with value: 1.1897214629986885 and parameters: {'observation_period_num': 202, 'train_rates': 0.6304940605119344, 'learning_rate': 1.1475696488247946e-05, 'batch_size': 83, 'step_size': 2, 'gamma': 0.9769781510246146}. Best is trial 1 with value: 1.1897214629986885.[0m
[32m[I 2025-01-07 22:00:40,504][0m Trial 2 finished with value: 1.0117929420973126 and parameters: {'observation_period_num': 93, 'train_rates': 0.9341273640466259, 'learning_rate': 1.0660038223595417e-06, 'batch_size': 40, 'step_size': 10, 'gamma': 0.7536229466989421}. Best is trial 2 with value: 1.0117929420973126.[0m
[32m[I 2025-01-07 22:02:25,855][0m Trial 3 finished with value: 0.9611562925671774 and parameters: {'observation_period_num': 79, 'train_rates': 0.9130723893708637, 'learning_rate': 2.113047949867834e-06, 'batch_size': 212, 'step_size': 7, 'gamma': 0.9634127916105385}. Best is trial 3 with value: 0.9611562925671774.[0m
[32m[I 2025-01-07 22:06:03,345][0m Trial 4 finished with value: 0.21347171068191528 and parameters: {'observation_period_num': 148, 'train_rates': 0.9792301580867883, 'learning_rate': 0.0006716362760400464, 'batch_size': 193, 'step_size': 5, 'gamma': 0.9738202881073412}. Best is trial 4 with value: 0.21347171068191528.[0m
[32m[I 2025-01-07 22:09:55,721][0m Trial 5 finished with value: 0.5448339847808189 and parameters: {'observation_period_num': 170, 'train_rates': 0.8489968444558308, 'learning_rate': 8.877116590447654e-06, 'batch_size': 216, 'step_size': 9, 'gamma': 0.9862738538554822}. Best is trial 4 with value: 0.21347171068191528.[0m
[32m[I 2025-01-07 22:13:56,294][0m Trial 6 finished with value: 2.4185944878448873 and parameters: {'observation_period_num': 161, 'train_rates': 0.7900159414273257, 'learning_rate': 0.000774078137693439, 'batch_size': 19, 'step_size': 10, 'gamma': 0.84311547900871}. Best is trial 4 with value: 0.21347171068191528.[0m
Early stopping at epoch 67
[32m[I 2025-01-07 22:15:45,235][0m Trial 7 finished with value: 1.973192061330292 and parameters: {'observation_period_num': 138, 'train_rates': 0.6883429374132896, 'learning_rate': 1.5288464062095492e-06, 'batch_size': 256, 'step_size': 2, 'gamma': 0.7569879927961632}. Best is trial 4 with value: 0.21347171068191528.[0m
[32m[I 2025-01-07 22:18:09,969][0m Trial 8 finished with value: 0.16225931150455997 and parameters: {'observation_period_num': 45, 'train_rates': 0.9501522455360871, 'learning_rate': 8.503456211717061e-05, 'batch_size': 28, 'step_size': 6, 'gamma': 0.8188434653119926}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:20:40,341][0m Trial 9 finished with value: 1.283624299258166 and parameters: {'observation_period_num': 127, 'train_rates': 0.7567471881564319, 'learning_rate': 0.0005312662544333319, 'batch_size': 89, 'step_size': 7, 'gamma': 0.9859675231769561}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:21:07,778][0m Trial 10 finished with value: 0.20563231426549244 and parameters: {'observation_period_num': 6, 'train_rates': 0.8601120537007153, 'learning_rate': 8.38559805970409e-05, 'batch_size': 145, 'step_size': 15, 'gamma': 0.900614445684576}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:21:35,772][0m Trial 11 finished with value: 0.2031908424406112 and parameters: {'observation_period_num': 10, 'train_rates': 0.8667170707427337, 'learning_rate': 8.353886483203235e-05, 'batch_size': 143, 'step_size': 15, 'gamma': 0.9048204847881749}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:22:04,885][0m Trial 12 finished with value: 0.1858783554052934 and parameters: {'observation_period_num': 7, 'train_rates': 0.8706218539769264, 'learning_rate': 6.995209178193115e-05, 'batch_size': 138, 'step_size': 15, 'gamma': 0.8895432623497979}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:23:08,909][0m Trial 13 finished with value: 0.37774866819381714 and parameters: {'observation_period_num': 41, 'train_rates': 0.9849459333066171, 'learning_rate': 2.5687999308751315e-05, 'batch_size': 102, 'step_size': 5, 'gamma': 0.8037960476986373}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:24:18,900][0m Trial 14 finished with value: 0.17512729263052026 and parameters: {'observation_period_num': 53, 'train_rates': 0.9195157014885338, 'learning_rate': 9.677611917387889e-05, 'batch_size': 167, 'step_size': 12, 'gamma': 0.87160077081058}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:25:48,083][0m Trial 15 finished with value: 0.17030805561214826 and parameters: {'observation_period_num': 65, 'train_rates': 0.9273496928447127, 'learning_rate': 0.00020885103365032864, 'batch_size': 187, 'step_size': 12, 'gamma': 0.801020549791931}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:28:02,436][0m Trial 16 finished with value: 0.17221416418369 and parameters: {'observation_period_num': 94, 'train_rates': 0.9457570107581629, 'learning_rate': 0.00023354151194234266, 'batch_size': 60, 'step_size': 12, 'gamma': 0.8016192895102809}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:29:01,439][0m Trial 17 finished with value: 0.5861897388166336 and parameters: {'observation_period_num': 50, 'train_rates': 0.7358738782737794, 'learning_rate': 2.922185699049259e-05, 'batch_size': 112, 'step_size': 5, 'gamma': 0.7876655772165638}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:31:15,208][0m Trial 18 finished with value: 0.3650082515225919 and parameters: {'observation_period_num': 112, 'train_rates': 0.8064686430946386, 'learning_rate': 0.0002994164251059795, 'batch_size': 178, 'step_size': 4, 'gamma': 0.8366816670309309}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:32:43,853][0m Trial 19 finished with value: 0.20369881724977826 and parameters: {'observation_period_num': 67, 'train_rates': 0.9016016680767545, 'learning_rate': 0.00019156962563758064, 'batch_size': 256, 'step_size': 8, 'gamma': 0.7818294178304701}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:33:54,267][0m Trial 20 finished with value: 0.23216814325965998 and parameters: {'observation_period_num': 33, 'train_rates': 0.9557188802714373, 'learning_rate': 1.3973654840556786e-05, 'batch_size': 58, 'step_size': 13, 'gamma': 0.8213959859880868}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:36:17,129][0m Trial 21 finished with value: 0.18094872558203928 and parameters: {'observation_period_num': 99, 'train_rates': 0.9523732278151283, 'learning_rate': 0.0001509457521912316, 'batch_size': 63, 'step_size': 13, 'gamma': 0.7952105381772292}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:38:03,868][0m Trial 22 finished with value: 0.22466113057024903 and parameters: {'observation_period_num': 73, 'train_rates': 0.8969542137012416, 'learning_rate': 0.0003830733638863131, 'batch_size': 53, 'step_size': 11, 'gamma': 0.8141719266487895}. Best is trial 8 with value: 0.16225931150455997.[0m
[32m[I 2025-01-07 22:40:40,276][0m Trial 23 finished with value: 0.14615054428577423 and parameters: {'observation_period_num': 112, 'train_rates': 0.9879852130005119, 'learning_rate': 5.1445407403163e-05, 'batch_size': 115, 'step_size': 8, 'gamma': 0.8572137212864387}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:41:28,837][0m Trial 24 finished with value: 0.16299757361412048 and parameters: {'observation_period_num': 31, 'train_rates': 0.9849485478266663, 'learning_rate': 4.972238033787977e-05, 'batch_size': 123, 'step_size': 7, 'gamma': 0.8629545087830465}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:42:13,331][0m Trial 25 finished with value: 0.15789614617824554 and parameters: {'observation_period_num': 26, 'train_rates': 0.9846096640643692, 'learning_rate': 4.611072686114325e-05, 'batch_size': 114, 'step_size': 7, 'gamma': 0.8663738818741119}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:48:55,171][0m Trial 26 finished with value: 0.41616764664649963 and parameters: {'observation_period_num': 247, 'train_rates': 0.9872168392872991, 'learning_rate': 5.251617946790044e-06, 'batch_size': 82, 'step_size': 6, 'gamma': 0.935645250927424}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:49:36,748][0m Trial 27 finished with value: 0.2770325243473053 and parameters: {'observation_period_num': 25, 'train_rates': 0.9625002454705233, 'learning_rate': 4.249160904755402e-05, 'batch_size': 123, 'step_size': 3, 'gamma': 0.8616834722925865}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:53:52,339][0m Trial 28 finished with value: 0.5621475038103393 and parameters: {'observation_period_num': 188, 'train_rates': 0.8247830443489494, 'learning_rate': 1.9014583564760017e-05, 'batch_size': 157, 'step_size': 8, 'gamma': 0.8482285237630605}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:56:36,940][0m Trial 29 finished with value: 0.15200076319344408 and parameters: {'observation_period_num': 112, 'train_rates': 0.8834550166101081, 'learning_rate': 0.00012233592367677588, 'batch_size': 33, 'step_size': 9, 'gamma': 0.889699128322978}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 22:59:06,489][0m Trial 30 finished with value: 0.2127755145490752 and parameters: {'observation_period_num': 113, 'train_rates': 0.8927063516379596, 'learning_rate': 4.658034031258363e-05, 'batch_size': 100, 'step_size': 9, 'gamma': 0.9247280905505932}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 23:02:16,157][0m Trial 31 finished with value: 0.14959006118774415 and parameters: {'observation_period_num': 127, 'train_rates': 0.9562605727066091, 'learning_rate': 0.00011639177977863319, 'batch_size': 38, 'step_size': 6, 'gamma': 0.8779157650203446}. Best is trial 23 with value: 0.14615054428577423.[0m
[32m[I 2025-01-07 23:05:25,671][0m Trial 32 finished with value: 0.1349204182624817 and parameters: {'observation_period_num': 125, 'train_rates': 0.9630151124928436, 'learning_rate': 0.00011532922069671194, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8812201756828941}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:08:17,704][0m Trial 33 finished with value: 0.20639285663279092 and parameters: {'observation_period_num': 122, 'train_rates': 0.8849510118966514, 'learning_rate': 0.0001302923689001365, 'batch_size': 40, 'step_size': 9, 'gamma': 0.8844795910487213}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:11:55,457][0m Trial 34 finished with value: 0.16007145525852465 and parameters: {'observation_period_num': 143, 'train_rates': 0.9251776000912308, 'learning_rate': 0.00012918284748562268, 'batch_size': 39, 'step_size': 10, 'gamma': 0.920682055796929}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:16:01,776][0m Trial 35 finished with value: 0.39678490309432 and parameters: {'observation_period_num': 164, 'train_rates': 0.9641472110825114, 'learning_rate': 0.0004343020461468757, 'batch_size': 69, 'step_size': 10, 'gamma': 0.8835272604448497}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:18:39,649][0m Trial 36 finished with value: 0.16657381885064146 and parameters: {'observation_period_num': 104, 'train_rates': 0.933300200077759, 'learning_rate': 6.239816443822792e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.9458312300655357}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:21:30,843][0m Trial 37 finished with value: 1.2728825320368227 and parameters: {'observation_period_num': 81, 'train_rates': 0.6019154723366873, 'learning_rate': 0.00012894942939860137, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9041285676237003}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:25:27,836][0m Trial 38 finished with value: 0.8701407293281933 and parameters: {'observation_period_num': 199, 'train_rates': 0.6355468176981613, 'learning_rate': 0.0002891925866632664, 'batch_size': 72, 'step_size': 6, 'gamma': 0.8513523423670099}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:28:42,598][0m Trial 39 finished with value: 0.29426328531269835 and parameters: {'observation_period_num': 140, 'train_rates': 0.8345370047383919, 'learning_rate': 2.078864780716295e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.874575330155793}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:32:26,396][0m Trial 40 finished with value: 0.27896907336358334 and parameters: {'observation_period_num': 152, 'train_rates': 0.9067692375213744, 'learning_rate': 3.519685717541789e-05, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9138687409464535}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:35:25,943][0m Trial 41 finished with value: 0.15128516041955284 and parameters: {'observation_period_num': 127, 'train_rates': 0.9700583307591633, 'learning_rate': 5.870552992857571e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.8592105545722613}. Best is trial 32 with value: 0.1349204182624817.[0m
[32m[I 2025-01-07 23:38:45,780][0m Trial 42 finished with value: 0.09968065694872155 and parameters: {'observation_period_num': 124, 'train_rates': 0.9711505737981135, 'learning_rate': 0.00010974338893786923, 'batch_size': 25, 'step_size': 7, 'gamma': 0.8535203655400592}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-07 23:41:44,958][0m Trial 43 finished with value: 0.18042792559980037 and parameters: {'observation_period_num': 127, 'train_rates': 0.968264441920125, 'learning_rate': 6.51575702790256e-05, 'batch_size': 82, 'step_size': 6, 'gamma': 0.8314021751727191}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-07 23:45:46,592][0m Trial 44 finished with value: 0.2567121386528015 and parameters: {'observation_period_num': 84, 'train_rates': 0.9392096795904455, 'learning_rate': 4.230773658037932e-06, 'batch_size': 16, 'step_size': 7, 'gamma': 0.8556010257144299}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-07 23:49:35,151][0m Trial 45 finished with value: 0.1745588481426239 and parameters: {'observation_period_num': 155, 'train_rates': 0.9704557305577849, 'learning_rate': 9.708887892234921e-05, 'batch_size': 92, 'step_size': 4, 'gamma': 0.8425989799214894}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-07 23:54:18,719][0m Trial 46 finished with value: 0.17300766865787265 and parameters: {'observation_period_num': 178, 'train_rates': 0.9430663814190826, 'learning_rate': 0.0001707059825524568, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8789071713388628}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-07 23:57:31,197][0m Trial 47 finished with value: 0.17354421931154587 and parameters: {'observation_period_num': 133, 'train_rates': 0.9163963619398219, 'learning_rate': 7.383308523554595e-05, 'batch_size': 70, 'step_size': 8, 'gamma': 0.9003513462416918}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-08 00:00:40,040][0m Trial 48 finished with value: 2.6051285398115804 and parameters: {'observation_period_num': 121, 'train_rates': 0.9617533557086504, 'learning_rate': 0.0009260653665977693, 'batch_size': 29, 'step_size': 5, 'gamma': 0.825535279959902}. Best is trial 42 with value: 0.09968065694872155.[0m
[32m[I 2025-01-08 00:02:35,954][0m Trial 49 finished with value: 0.38906781153475983 and parameters: {'observation_period_num': 89, 'train_rates': 0.7554445007304604, 'learning_rate': 0.0001045194620016494, 'batch_size': 50, 'step_size': 8, 'gamma': 0.8544808156931112}. Best is trial 42 with value: 0.09968065694872155.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_Transformer(nomstl).json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.4807 | 0.5475
Epoch 2/300, Loss: 0.3874 | 0.4119
Epoch 3/300, Loss: 0.3131 | 0.4099
Epoch 4/300, Loss: 0.2550 | 0.4015
Epoch 5/300, Loss: 0.2726 | 0.3175
Epoch 6/300, Loss: 0.2777 | 0.4693
Epoch 7/300, Loss: 0.2452 | 0.3517
Epoch 8/300, Loss: 0.2339 | 0.3371
Epoch 9/300, Loss: 0.2098 | 0.2660
Epoch 10/300, Loss: 0.1970 | 0.2541
Epoch 11/300, Loss: 0.1897 | 0.2381
Epoch 12/300, Loss: 0.1867 | 0.2320
Epoch 13/300, Loss: 0.1789 | 0.2345
Epoch 14/300, Loss: 0.1749 | 0.2073
Epoch 15/300, Loss: 0.1788 | 0.2162
Epoch 16/300, Loss: 0.1739 | 0.2288
Epoch 17/300, Loss: 0.1662 | 0.2287
Epoch 18/300, Loss: 0.1545 | 0.1956
Epoch 19/300, Loss: 0.1552 | 0.1894
Epoch 20/300, Loss: 0.1526 | 0.2034
Epoch 21/300, Loss: 0.1463 | 0.1834
Epoch 22/300, Loss: 0.1420 | 0.1777
Epoch 23/300, Loss: 0.1399 | 0.1773
Epoch 24/300, Loss: 0.1398 | 0.1723
Epoch 25/300, Loss: 0.1382 | 0.1640
Epoch 26/300, Loss: 0.1363 | 0.1575
Epoch 27/300, Loss: 0.1327 | 0.1570
Epoch 28/300, Loss: 0.1330 | 0.1628
Epoch 29/300, Loss: 0.1338 | 0.1476
Epoch 30/300, Loss: 0.1307 | 0.1439
Epoch 31/300, Loss: 0.1297 | 0.1476
Epoch 32/300, Loss: 0.1286 | 0.1514
Epoch 33/300, Loss: 0.1270 | 0.1394
Epoch 34/300, Loss: 0.1248 | 0.1408
Epoch 35/300, Loss: 0.1232 | 0.1444
Epoch 36/300, Loss: 0.1220 | 0.1406
Epoch 37/300, Loss: 0.1196 | 0.1356
Epoch 38/300, Loss: 0.1181 | 0.1358
Epoch 39/300, Loss: 0.1167 | 0.1368
Epoch 40/300, Loss: 0.1157 | 0.1336
Epoch 41/300, Loss: 0.1148 | 0.1298
Epoch 42/300, Loss: 0.1141 | 0.1302
Epoch 43/300, Loss: 0.1134 | 0.1298
Epoch 44/300, Loss: 0.1116 | 0.1283
Epoch 45/300, Loss: 0.1105 | 0.1253
Epoch 46/300, Loss: 0.1094 | 0.1228
Epoch 47/300, Loss: 0.1085 | 0.1237
Epoch 48/300, Loss: 0.1086 | 0.1226
Epoch 49/300, Loss: 0.1072 | 0.1214
Epoch 50/300, Loss: 0.1068 | 0.1211
Epoch 51/300, Loss: 0.1075 | 0.1203
Epoch 52/300, Loss: 0.1064 | 0.1175
Epoch 53/300, Loss: 0.1059 | 0.1180
Epoch 54/300, Loss: 0.1050 | 0.1177
Epoch 55/300, Loss: 0.1052 | 0.1175
Epoch 56/300, Loss: 0.1047 | 0.1172
Epoch 57/300, Loss: 0.1040 | 0.1153
Epoch 58/300, Loss: 0.1036 | 0.1149
Epoch 59/300, Loss: 0.1029 | 0.1146
Epoch 60/300, Loss: 0.1030 | 0.1153
Epoch 61/300, Loss: 0.1028 | 0.1153
Epoch 62/300, Loss: 0.1020 | 0.1158
Epoch 63/300, Loss: 0.1022 | 0.1137
Epoch 64/300, Loss: 0.1019 | 0.1129
Epoch 65/300, Loss: 0.1020 | 0.1139
Epoch 66/300, Loss: 0.1006 | 0.1132
Epoch 67/300, Loss: 0.1007 | 0.1130
Epoch 68/300, Loss: 0.1003 | 0.1139
Epoch 69/300, Loss: 0.1001 | 0.1125
Epoch 70/300, Loss: 0.1000 | 0.1129
Epoch 71/300, Loss: 0.1005 | 0.1125
Epoch 72/300, Loss: 0.0996 | 0.1122
Epoch 73/300, Loss: 0.0990 | 0.1121
Epoch 74/300, Loss: 0.0985 | 0.1120
Epoch 75/300, Loss: 0.0987 | 0.1116
Epoch 76/300, Loss: 0.0990 | 0.1120
Epoch 77/300, Loss: 0.0987 | 0.1107
Epoch 78/300, Loss: 0.0982 | 0.1112
Epoch 79/300, Loss: 0.0981 | 0.1117
Epoch 80/300, Loss: 0.0981 | 0.1114
Epoch 81/300, Loss: 0.0978 | 0.1107
Epoch 82/300, Loss: 0.0976 | 0.1106
Epoch 83/300, Loss: 0.0975 | 0.1112
Epoch 84/300, Loss: 0.0976 | 0.1108
Epoch 85/300, Loss: 0.0977 | 0.1106
Epoch 86/300, Loss: 0.0974 | 0.1098
Epoch 87/300, Loss: 0.0972 | 0.1101
Epoch 88/300, Loss: 0.0976 | 0.1104
Epoch 89/300, Loss: 0.0963 | 0.1097
Epoch 90/300, Loss: 0.0972 | 0.1100
Epoch 91/300, Loss: 0.0964 | 0.1103
Epoch 92/300, Loss: 0.0965 | 0.1101
Epoch 93/300, Loss: 0.0966 | 0.1102
Epoch 94/300, Loss: 0.0974 | 0.1097
Epoch 95/300, Loss: 0.0963 | 0.1097
Epoch 96/300, Loss: 0.0964 | 0.1097
Epoch 97/300, Loss: 0.0965 | 0.1099
Epoch 98/300, Loss: 0.0964 | 0.1096
Epoch 99/300, Loss: 0.0963 | 0.1095
Epoch 100/300, Loss: 0.0964 | 0.1091
Epoch 101/300, Loss: 0.0963 | 0.1095
Epoch 102/300, Loss: 0.0962 | 0.1095
Epoch 103/300, Loss: 0.0959 | 0.1093
Epoch 104/300, Loss: 0.0964 | 0.1095
Epoch 105/300, Loss: 0.0966 | 0.1094
Epoch 106/300, Loss: 0.0955 | 0.1092
Epoch 107/300, Loss: 0.0955 | 0.1092
Epoch 108/300, Loss: 0.0952 | 0.1093
Epoch 109/300, Loss: 0.0959 | 0.1092
Epoch 110/300, Loss: 0.0962 | 0.1092
Epoch 111/300, Loss: 0.0954 | 0.1090
Epoch 112/300, Loss: 0.0960 | 0.1088
Epoch 113/300, Loss: 0.0953 | 0.1087
Epoch 114/300, Loss: 0.0953 | 0.1088
Epoch 115/300, Loss: 0.0956 | 0.1090
Epoch 116/300, Loss: 0.0958 | 0.1090
Epoch 117/300, Loss: 0.0960 | 0.1090
Epoch 118/300, Loss: 0.0956 | 0.1087
Epoch 119/300, Loss: 0.0957 | 0.1089
Epoch 120/300, Loss: 0.0951 | 0.1089
Epoch 121/300, Loss: 0.0953 | 0.1088
Epoch 122/300, Loss: 0.0949 | 0.1088
Epoch 123/300, Loss: 0.0952 | 0.1089
Epoch 124/300, Loss: 0.0958 | 0.1089
Epoch 125/300, Loss: 0.0957 | 0.1088
Epoch 126/300, Loss: 0.0953 | 0.1089
Epoch 127/300, Loss: 0.0952 | 0.1088
Epoch 128/300, Loss: 0.0954 | 0.1086
Epoch 129/300, Loss: 0.0956 | 0.1088
Epoch 130/300, Loss: 0.0956 | 0.1087
Epoch 131/300, Loss: 0.0950 | 0.1088
Epoch 132/300, Loss: 0.0957 | 0.1089
Epoch 133/300, Loss: 0.0959 | 0.1088
Epoch 134/300, Loss: 0.0952 | 0.1087
Epoch 135/300, Loss: 0.0950 | 0.1087
Epoch 136/300, Loss: 0.0953 | 0.1088
Epoch 137/300, Loss: 0.0951 | 0.1087
Epoch 138/300, Loss: 0.0948 | 0.1088
Epoch 139/300, Loss: 0.0951 | 0.1088
Epoch 140/300, Loss: 0.0952 | 0.1088
Epoch 141/300, Loss: 0.0955 | 0.1088
Epoch 142/300, Loss: 0.0946 | 0.1088
Epoch 143/300, Loss: 0.0950 | 0.1088
Epoch 144/300, Loss: 0.0951 | 0.1088
Epoch 145/300, Loss: 0.0950 | 0.1087
Epoch 146/300, Loss: 0.0946 | 0.1086
Epoch 147/300, Loss: 0.0947 | 0.1086
Epoch 148/300, Loss: 0.0951 | 0.1086
Epoch 149/300, Loss: 0.0947 | 0.1086
Epoch 150/300, Loss: 0.0958 | 0.1087
Epoch 151/300, Loss: 0.0955 | 0.1087
Epoch 152/300, Loss: 0.0952 | 0.1087
Epoch 153/300, Loss: 0.0952 | 0.1086
Epoch 154/300, Loss: 0.0952 | 0.1087
Epoch 155/300, Loss: 0.0952 | 0.1087
Epoch 156/300, Loss: 0.0955 | 0.1087
Epoch 157/300, Loss: 0.0952 | 0.1087
Epoch 158/300, Loss: 0.0947 | 0.1087
Epoch 159/300, Loss: 0.0950 | 0.1087
Epoch 160/300, Loss: 0.0946 | 0.1087
Epoch 161/300, Loss: 0.0953 | 0.1087
Epoch 162/300, Loss: 0.0947 | 0.1087
Epoch 163/300, Loss: 0.0948 | 0.1087
Epoch 164/300, Loss: 0.0952 | 0.1086
Epoch 165/300, Loss: 0.0949 | 0.1086
Epoch 166/300, Loss: 0.0957 | 0.1086
Epoch 167/300, Loss: 0.0963 | 0.1086
Epoch 168/300, Loss: 0.0952 | 0.1087
Epoch 169/300, Loss: 0.0951 | 0.1087
Epoch 170/300, Loss: 0.0946 | 0.1087
Epoch 171/300, Loss: 0.0954 | 0.1087
Epoch 172/300, Loss: 0.0953 | 0.1087
Epoch 173/300, Loss: 0.0952 | 0.1087
Epoch 174/300, Loss: 0.0955 | 0.1086
Epoch 175/300, Loss: 0.0958 | 0.1086
Epoch 176/300, Loss: 0.0951 | 0.1086
Epoch 177/300, Loss: 0.0954 | 0.1086
Epoch 178/300, Loss: 0.0953 | 0.1087
Epoch 179/300, Loss: 0.0951 | 0.1087
Epoch 180/300, Loss: 0.0952 | 0.1087
Epoch 181/300, Loss: 0.0954 | 0.1087
Epoch 182/300, Loss: 0.0951 | 0.1087
Epoch 183/300, Loss: 0.0949 | 0.1087
Epoch 184/300, Loss: 0.0956 | 0.1087
Epoch 185/300, Loss: 0.0951 | 0.1087
Epoch 186/300, Loss: 0.0954 | 0.1087
Epoch 187/300, Loss: 0.0944 | 0.1087
Epoch 188/300, Loss: 0.0946 | 0.1087
Epoch 189/300, Loss: 0.0949 | 0.1087
Epoch 190/300, Loss: 0.0954 | 0.1087
Epoch 191/300, Loss: 0.0950 | 0.1087
Epoch 192/300, Loss: 0.0950 | 0.1087
Epoch 193/300, Loss: 0.0948 | 0.1087
Epoch 194/300, Loss: 0.0952 | 0.1087
Epoch 195/300, Loss: 0.0954 | 0.1087
Epoch 196/300, Loss: 0.0952 | 0.1087
Epoch 197/300, Loss: 0.0946 | 0.1087
Epoch 198/300, Loss: 0.0950 | 0.1087
Epoch 199/300, Loss: 0.0951 | 0.1087
Epoch 200/300, Loss: 0.0953 | 0.1087
Epoch 201/300, Loss: 0.0951 | 0.1087
Epoch 202/300, Loss: 0.0955 | 0.1087
Epoch 203/300, Loss: 0.0949 | 0.1087
Epoch 204/300, Loss: 0.0946 | 0.1087
Epoch 205/300, Loss: 0.0950 | 0.1087
Epoch 206/300, Loss: 0.0952 | 0.1087
Epoch 207/300, Loss: 0.0953 | 0.1087
Epoch 208/300, Loss: 0.0951 | 0.1087
Epoch 209/300, Loss: 0.0956 | 0.1087
Epoch 210/300, Loss: 0.0950 | 0.1087
Epoch 211/300, Loss: 0.0940 | 0.1087
Epoch 212/300, Loss: 0.0947 | 0.1087
Epoch 213/300, Loss: 0.0948 | 0.1087
Epoch 214/300, Loss: 0.0944 | 0.1087
Epoch 215/300, Loss: 0.0949 | 0.1087
Epoch 216/300, Loss: 0.0953 | 0.1087
Epoch 217/300, Loss: 0.0951 | 0.1087
Epoch 218/300, Loss: 0.0939 | 0.1087
Epoch 219/300, Loss: 0.0950 | 0.1087
Epoch 220/300, Loss: 0.0951 | 0.1087
Epoch 221/300, Loss: 0.0950 | 0.1087
Epoch 222/300, Loss: 0.0950 | 0.1087
Epoch 223/300, Loss: 0.0951 | 0.1087
Epoch 224/300, Loss: 0.0948 | 0.1087
Epoch 225/300, Loss: 0.0947 | 0.1087
Epoch 226/300, Loss: 0.0960 | 0.1087
Epoch 227/300, Loss: 0.0952 | 0.1087
Epoch 228/300, Loss: 0.0948 | 0.1087
Epoch 229/300, Loss: 0.0953 | 0.1087
Epoch 230/300, Loss: 0.0952 | 0.1087
Epoch 231/300, Loss: 0.0949 | 0.1087
Epoch 232/300, Loss: 0.0956 | 0.1087
Epoch 233/300, Loss: 0.0949 | 0.1087
Epoch 234/300, Loss: 0.0946 | 0.1087
Epoch 235/300, Loss: 0.0951 | 0.1087
Epoch 236/300, Loss: 0.0953 | 0.1087
Epoch 237/300, Loss: 0.0946 | 0.1087
Epoch 238/300, Loss: 0.0954 | 0.1087
Epoch 239/300, Loss: 0.0952 | 0.1087
Epoch 240/300, Loss: 0.0945 | 0.1086
Epoch 241/300, Loss: 0.0944 | 0.1086
Epoch 242/300, Loss: 0.0946 | 0.1086
Epoch 243/300, Loss: 0.0948 | 0.1086
Epoch 244/300, Loss: 0.0948 | 0.1086
Epoch 245/300, Loss: 0.0949 | 0.1086
Epoch 246/300, Loss: 0.0952 | 0.1086
Epoch 247/300, Loss: 0.0953 | 0.1086
Epoch 248/300, Loss: 0.0957 | 0.1086
Epoch 249/300, Loss: 0.0953 | 0.1086
Epoch 250/300, Loss: 0.0951 | 0.1086
Epoch 251/300, Loss: 0.0950 | 0.1086
Epoch 252/300, Loss: 0.0949 | 0.1086
Epoch 253/300, Loss: 0.0949 | 0.1086
Epoch 254/300, Loss: 0.0943 | 0.1086
Epoch 255/300, Loss: 0.0948 | 0.1086
Epoch 256/300, Loss: 0.0957 | 0.1086
Epoch 257/300, Loss: 0.0946 | 0.1086
Epoch 258/300, Loss: 0.0950 | 0.1086
Epoch 259/300, Loss: 0.0951 | 0.1086
Epoch 260/300, Loss: 0.0946 | 0.1086
Epoch 261/300, Loss: 0.0949 | 0.1086
Epoch 262/300, Loss: 0.0947 | 0.1086
Epoch 263/300, Loss: 0.0953 | 0.1086
Epoch 264/300, Loss: 0.0954 | 0.1086
Epoch 265/300, Loss: 0.0950 | 0.1086
Epoch 266/300, Loss: 0.0948 | 0.1086
Epoch 267/300, Loss: 0.0951 | 0.1086
Epoch 268/300, Loss: 0.0946 | 0.1086
Epoch 269/300, Loss: 0.0948 | 0.1086
Epoch 270/300, Loss: 0.0944 | 0.1086
Epoch 271/300, Loss: 0.0954 | 0.1086
Epoch 272/300, Loss: 0.0955 | 0.1086
Epoch 273/300, Loss: 0.0951 | 0.1086
Epoch 274/300, Loss: 0.0946 | 0.1086
Epoch 275/300, Loss: 0.0949 | 0.1086
Epoch 276/300, Loss: 0.0951 | 0.1086
Epoch 277/300, Loss: 0.0950 | 0.1086
Epoch 278/300, Loss: 0.0949 | 0.1086
Epoch 279/300, Loss: 0.0947 | 0.1086
Epoch 280/300, Loss: 0.0953 | 0.1086
Epoch 281/300, Loss: 0.0951 | 0.1086
Early stopping
Runtime (seconds): 553.1289491653442
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 530.0695403015707
RMSE: 23.023239135742188
MAE: 23.023239135742188
R-squared: nan
[231.46677]
/data/student/k2110261/Multi-iTransformer/Transformer_nomstl.py:486: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 6))
