ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 09:57:49,027][0m A new study created in memory with name: no-name-5b53c524-4d85-418b-8a88-9c8781d5ab29[0m
[32m[I 2025-01-04 10:03:05,146][0m Trial 0 finished with value: 1.014225853526074 and parameters: {'observation_period_num': 252, 'train_rates': 0.6936404017888077, 'learning_rate': 1.714480961000042e-05, 'batch_size': 84, 'step_size': 4, 'gamma': 0.8016905066975852}. Best is trial 0 with value: 1.014225853526074.[0m
[32m[I 2025-01-04 10:04:10,637][0m Trial 1 finished with value: 0.39243793011626105 and parameters: {'observation_period_num': 20, 'train_rates': 0.755511389455219, 'learning_rate': 6.559138650017239e-05, 'batch_size': 62, 'step_size': 14, 'gamma': 0.8663782630398522}. Best is trial 1 with value: 0.39243793011626105.[0m
Early stopping at epoch 78
[32m[I 2025-01-04 10:08:08,180][0m Trial 2 finished with value: 0.44095104280179437 and parameters: {'observation_period_num': 208, 'train_rates': 0.900295497433651, 'learning_rate': 0.00010983196568576166, 'batch_size': 146, 'step_size': 1, 'gamma': 0.8505181714340214}. Best is trial 1 with value: 0.39243793011626105.[0m
[32m[I 2025-01-04 10:10:04,966][0m Trial 3 finished with value: 0.9771822207215903 and parameters: {'observation_period_num': 113, 'train_rates': 0.6356686050014231, 'learning_rate': 3.9773216503464896e-05, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8745061383739311}. Best is trial 1 with value: 0.39243793011626105.[0m
[32m[I 2025-01-04 10:11:56,138][0m Trial 4 finished with value: 0.904912118842493 and parameters: {'observation_period_num': 103, 'train_rates': 0.6617861456508072, 'learning_rate': 1.0426837935981064e-05, 'batch_size': 154, 'step_size': 12, 'gamma': 0.8872437413807702}. Best is trial 1 with value: 0.39243793011626105.[0m
[32m[I 2025-01-04 10:15:16,097][0m Trial 5 finished with value: 0.21028934245885805 and parameters: {'observation_period_num': 133, 'train_rates': 0.9543302381414441, 'learning_rate': 2.819980861493022e-05, 'batch_size': 59, 'step_size': 7, 'gamma': 0.9163174768236888}. Best is trial 5 with value: 0.21028934245885805.[0m
[32m[I 2025-01-04 10:17:39,458][0m Trial 6 finished with value: 0.2722257093625998 and parameters: {'observation_period_num': 115, 'train_rates': 0.8319196260354869, 'learning_rate': 0.00032272738480371166, 'batch_size': 161, 'step_size': 15, 'gamma': 0.8526889315012367}. Best is trial 5 with value: 0.21028934245885805.[0m
[32m[I 2025-01-04 10:22:44,997][0m Trial 7 finished with value: 0.18116220872883404 and parameters: {'observation_period_num': 183, 'train_rates': 0.9606719184310732, 'learning_rate': 1.895827635623119e-05, 'batch_size': 19, 'step_size': 3, 'gamma': 0.9124901557433559}. Best is trial 7 with value: 0.18116220872883404.[0m
[32m[I 2025-01-04 10:23:58,857][0m Trial 8 finished with value: 0.814325039602212 and parameters: {'observation_period_num': 65, 'train_rates': 0.6926682763183585, 'learning_rate': 0.00031424744110029304, 'batch_size': 175, 'step_size': 11, 'gamma': 0.8771172121497653}. Best is trial 7 with value: 0.18116220872883404.[0m
[32m[I 2025-01-04 10:27:27,970][0m Trial 9 finished with value: 0.14706987142562866 and parameters: {'observation_period_num': 144, 'train_rates': 0.9860157419371538, 'learning_rate': 0.00018809248176353887, 'batch_size': 232, 'step_size': 2, 'gamma': 0.9461869382084924}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:31:21,637][0m Trial 10 finished with value: 1.2347222419129205 and parameters: {'observation_period_num': 177, 'train_rates': 0.8252685425698351, 'learning_rate': 1.5224123632138704e-06, 'batch_size': 251, 'step_size': 8, 'gamma': 0.9701726584034382}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:35:42,031][0m Trial 11 finished with value: 0.17967993021011353 and parameters: {'observation_period_num': 176, 'train_rates': 0.9876487466618874, 'learning_rate': 0.0008619372616922831, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9767817753483032}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:39:42,464][0m Trial 12 finished with value: 0.20596656203269958 and parameters: {'observation_period_num': 163, 'train_rates': 0.9834852595934438, 'learning_rate': 0.0009453433570277849, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9847117459437817}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:45:10,485][0m Trial 13 finished with value: 0.1955600410186011 and parameters: {'observation_period_num': 223, 'train_rates': 0.894024458954992, 'learning_rate': 0.0007746593691240929, 'batch_size': 209, 'step_size': 3, 'gamma': 0.9453624078972371}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:48:30,471][0m Trial 14 finished with value: 0.16818300217062562 and parameters: {'observation_period_num': 144, 'train_rates': 0.9028296660624378, 'learning_rate': 0.00021218278727828368, 'batch_size': 205, 'step_size': 6, 'gamma': 0.9479132483513991}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:50:01,961][0m Trial 15 finished with value: 0.16683497800621935 and parameters: {'observation_period_num': 70, 'train_rates': 0.9033928718759112, 'learning_rate': 0.00018859155743976869, 'batch_size': 205, 'step_size': 6, 'gamma': 0.9403463593427707}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:51:33,641][0m Trial 16 finished with value: 0.17915781888555973 and parameters: {'observation_period_num': 65, 'train_rates': 0.9188369859241288, 'learning_rate': 9.845115305385236e-05, 'batch_size': 120, 'step_size': 10, 'gamma': 0.7658331624305553}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:53:11,137][0m Trial 17 finished with value: 0.8276325469780411 and parameters: {'observation_period_num': 77, 'train_rates': 0.85037138946646, 'learning_rate': 3.807985836186163e-06, 'batch_size': 202, 'step_size': 9, 'gamma': 0.9193932606563958}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:53:44,494][0m Trial 18 finished with value: 0.3548075271471766 and parameters: {'observation_period_num': 6, 'train_rates': 0.7779921459840435, 'learning_rate': 0.00017808238096392873, 'batch_size': 118, 'step_size': 6, 'gamma': 0.9363224007382093}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:54:36,231][0m Trial 19 finished with value: 0.2436189502477646 and parameters: {'observation_period_num': 39, 'train_rates': 0.9372977753510591, 'learning_rate': 0.0004664105275422784, 'batch_size': 228, 'step_size': 3, 'gamma': 0.834311552108906}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:56:30,458][0m Trial 20 finished with value: 0.2679294378140084 and parameters: {'observation_period_num': 89, 'train_rates': 0.8735571962960639, 'learning_rate': 6.205758821511313e-05, 'batch_size': 188, 'step_size': 5, 'gamma': 0.8962254555834137}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 10:59:47,874][0m Trial 21 finished with value: 0.18439216486045293 and parameters: {'observation_period_num': 140, 'train_rates': 0.9180227995369412, 'learning_rate': 0.00019423668077944112, 'batch_size': 222, 'step_size': 7, 'gamma': 0.9534410594494869}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 11:03:02,006][0m Trial 22 finished with value: 0.40498888838504044 and parameters: {'observation_period_num': 150, 'train_rates': 0.8002088159001144, 'learning_rate': 0.00016112739407114155, 'batch_size': 197, 'step_size': 6, 'gamma': 0.9611071283052146}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 11:03:58,508][0m Trial 23 finished with value: 0.23938522192881193 and parameters: {'observation_period_num': 46, 'train_rates': 0.8646814438160401, 'learning_rate': 0.00029149542580379133, 'batch_size': 237, 'step_size': 8, 'gamma': 0.9308455316353296}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 11:07:00,361][0m Trial 24 finished with value: 0.21151387691497803 and parameters: {'observation_period_num': 129, 'train_rates': 0.944889822660296, 'learning_rate': 0.0005099532054653698, 'batch_size': 185, 'step_size': 4, 'gamma': 0.9036857727075461}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 11:09:03,784][0m Trial 25 finished with value: 0.19151871597353215 and parameters: {'observation_period_num': 97, 'train_rates': 0.8892659643301054, 'learning_rate': 0.0001026643282122038, 'batch_size': 218, 'step_size': 2, 'gamma': 0.9884524226530891}. Best is trial 9 with value: 0.14706987142562866.[0m
[32m[I 2025-01-04 11:13:50,796][0m Trial 26 finished with value: 0.14588141441345215 and parameters: {'observation_period_num': 191, 'train_rates': 0.969610938001189, 'learning_rate': 6.164001958065379e-05, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9542638987644589}. Best is trial 26 with value: 0.14588141441345215.[0m
[32m[I 2025-01-04 11:19:18,668][0m Trial 27 finished with value: 0.14292553067207336 and parameters: {'observation_period_num': 211, 'train_rates': 0.9863592287003538, 'learning_rate': 4.123201531032342e-05, 'batch_size': 122, 'step_size': 4, 'gamma': 0.9281565613565567}. Best is trial 27 with value: 0.14292553067207336.[0m
[32m[I 2025-01-04 11:24:35,085][0m Trial 28 finished with value: 0.3917181193828583 and parameters: {'observation_period_num': 206, 'train_rates': 0.9724256063637567, 'learning_rate': 7.462012323957615e-06, 'batch_size': 125, 'step_size': 4, 'gamma': 0.9631843629176157}. Best is trial 27 with value: 0.14292553067207336.[0m
[32m[I 2025-01-04 11:31:01,224][0m Trial 29 finished with value: 0.28960331035464 and parameters: {'observation_period_num': 249, 'train_rates': 0.9342272639206735, 'learning_rate': 4.041093721749339e-05, 'batch_size': 100, 'step_size': 2, 'gamma': 0.9294354988231409}. Best is trial 27 with value: 0.14292553067207336.[0m
[32m[I 2025-01-04 11:36:10,554][0m Trial 30 finished with value: 0.8057638644154685 and parameters: {'observation_period_num': 233, 'train_rates': 0.741544355420109, 'learning_rate': 2.5594431950647677e-05, 'batch_size': 85, 'step_size': 4, 'gamma': 0.8012730786384674}. Best is trial 27 with value: 0.14292553067207336.[0m
[32m[I 2025-01-04 11:41:15,409][0m Trial 31 finished with value: 0.17124709486961365 and parameters: {'observation_period_num': 199, 'train_rates': 0.9650528279898897, 'learning_rate': 6.615235738897649e-05, 'batch_size': 139, 'step_size': 5, 'gamma': 0.9364253982629953}. Best is trial 27 with value: 0.14292553067207336.[0m
[32m[I 2025-01-04 11:45:17,533][0m Trial 32 finished with value: 0.1301698386669159 and parameters: {'observation_period_num': 161, 'train_rates': 0.9868772955677754, 'learning_rate': 5.6628550433929306e-05, 'batch_size': 97, 'step_size': 2, 'gamma': 0.9524806036041166}. Best is trial 32 with value: 0.1301698386669159.[0m
[32m[I 2025-01-04 11:49:19,668][0m Trial 33 finished with value: 0.11254458129405975 and parameters: {'observation_period_num': 161, 'train_rates': 0.9886648934416493, 'learning_rate': 6.395459761698838e-05, 'batch_size': 98, 'step_size': 2, 'gamma': 0.9689094345515421}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 11:54:08,190][0m Trial 34 finished with value: 0.11875147372484207 and parameters: {'observation_period_num': 188, 'train_rates': 0.9896979698551768, 'learning_rate': 5.360611491670639e-05, 'batch_size': 100, 'step_size': 3, 'gamma': 0.9710514505929151}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 11:57:58,228][0m Trial 35 finished with value: 0.257813658962395 and parameters: {'observation_period_num': 160, 'train_rates': 0.9297484165420995, 'learning_rate': 1.3325088484536743e-05, 'batch_size': 66, 'step_size': 2, 'gamma': 0.9742161708025808}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:02:13,170][0m Trial 36 finished with value: 1.1008478122988197 and parameters: {'observation_period_num': 221, 'train_rates': 0.6044181665203223, 'learning_rate': 4.0889581450570825e-05, 'batch_size': 104, 'step_size': 1, 'gamma': 0.9768942912397826}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:06:19,798][0m Trial 37 finished with value: 0.2375182231434253 and parameters: {'observation_period_num': 167, 'train_rates': 0.9573623735937825, 'learning_rate': 2.1177122936283292e-05, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9634788439982286}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:11:27,291][0m Trial 38 finished with value: 0.30761921405792236 and parameters: {'observation_period_num': 192, 'train_rates': 0.9892856779327659, 'learning_rate': 6.6612209526582814e-06, 'batch_size': 40, 'step_size': 3, 'gamma': 0.9232692170281748}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:16:52,792][0m Trial 39 finished with value: 0.22070317031169423 and parameters: {'observation_period_num': 214, 'train_rates': 0.9464022066673109, 'learning_rate': 3.324589218787634e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.9896895135387898}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:19:07,450][0m Trial 40 finished with value: 0.6899032980283527 and parameters: {'observation_period_num': 115, 'train_rates': 0.7372049316155131, 'learning_rate': 8.652056795973007e-05, 'batch_size': 149, 'step_size': 2, 'gamma': 0.9053967700391091}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:23:50,119][0m Trial 41 finished with value: 0.18221941590309143 and parameters: {'observation_period_num': 192, 'train_rates': 0.9666375966700381, 'learning_rate': 6.075811679368662e-05, 'batch_size': 131, 'step_size': 5, 'gamma': 0.957986954822888}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:28:29,901][0m Trial 42 finished with value: 0.17516586184501648 and parameters: {'observation_period_num': 187, 'train_rates': 0.9679196802093901, 'learning_rate': 5.121475444783862e-05, 'batch_size': 90, 'step_size': 4, 'gamma': 0.9563835139539415}. Best is trial 33 with value: 0.11254458129405975.[0m
[32m[I 2025-01-04 12:32:49,453][0m Trial 43 finished with value: 0.10792016983032227 and parameters: {'observation_period_num': 173, 'train_rates': 0.9890222150952219, 'learning_rate': 0.0001329510035503019, 'batch_size': 106, 'step_size': 3, 'gamma': 0.9770287638224356}. Best is trial 43 with value: 0.10792016983032227.[0m
[32m[I 2025-01-04 12:37:02,071][0m Trial 44 finished with value: 0.11358116567134857 and parameters: {'observation_period_num': 169, 'train_rates': 0.9845768430310962, 'learning_rate': 0.00013914721120485574, 'batch_size': 107, 'step_size': 1, 'gamma': 0.9717086130528234}. Best is trial 43 with value: 0.10792016983032227.[0m
[32m[I 2025-01-04 12:41:01,238][0m Trial 45 finished with value: 0.16481302060345385 and parameters: {'observation_period_num': 158, 'train_rates': 0.9495890362351587, 'learning_rate': 0.00013007599877750304, 'batch_size': 42, 'step_size': 1, 'gamma': 0.9750195334408659}. Best is trial 43 with value: 0.10792016983032227.[0m
[32m[I 2025-01-04 12:45:18,537][0m Trial 46 finished with value: 0.1683505891937099 and parameters: {'observation_period_num': 179, 'train_rates': 0.9211633185475894, 'learning_rate': 0.00013473021168865977, 'batch_size': 111, 'step_size': 2, 'gamma': 0.9685382630129551}. Best is trial 43 with value: 0.10792016983032227.[0m
[32m[I 2025-01-04 12:49:38,576][0m Trial 47 finished with value: 0.10318811982870102 and parameters: {'observation_period_num': 172, 'train_rates': 0.9886219911817518, 'learning_rate': 8.863344909546302e-05, 'batch_size': 72, 'step_size': 1, 'gamma': 0.9821454764162649}. Best is trial 47 with value: 0.10318811982870102.[0m
[32m[I 2025-01-04 12:53:06,047][0m Trial 48 finished with value: 0.8742996106129228 and parameters: {'observation_period_num': 173, 'train_rates': 0.6992740298580151, 'learning_rate': 0.0002600158959873708, 'batch_size': 74, 'step_size': 1, 'gamma': 0.989917133067327}. Best is trial 47 with value: 0.10318811982870102.[0m
[32m[I 2025-01-04 12:56:52,231][0m Trial 49 finished with value: 0.17039291003476018 and parameters: {'observation_period_num': 149, 'train_rates': 0.9591090152093882, 'learning_rate': 8.470626667900844e-05, 'batch_size': 51, 'step_size': 1, 'gamma': 0.9786756201403137}. Best is trial 47 with value: 0.10318811982870102.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 12:56:52,239][0m A new study created in memory with name: no-name-fc43bc4b-696b-4a64-b2ce-31e44dbd97a5[0m
[32m[I 2025-01-04 12:59:39,174][0m Trial 0 finished with value: 0.7335342043898109 and parameters: {'observation_period_num': 135, 'train_rates': 0.7476954321834871, 'learning_rate': 1.8231047123005285e-05, 'batch_size': 217, 'step_size': 14, 'gamma': 0.9081625141804511}. Best is trial 0 with value: 0.7335342043898109.[0m
[32m[I 2025-01-04 13:03:22,775][0m Trial 1 finished with value: 0.9206798657485582 and parameters: {'observation_period_num': 185, 'train_rates': 0.7024187408468467, 'learning_rate': 1.741522441761571e-05, 'batch_size': 68, 'step_size': 4, 'gamma': 0.81774027069993}. Best is trial 0 with value: 0.7335342043898109.[0m
[32m[I 2025-01-04 13:08:15,523][0m Trial 2 finished with value: 0.4206267166244291 and parameters: {'observation_period_num': 222, 'train_rates': 0.754371514806493, 'learning_rate': 0.00017032604548566758, 'batch_size': 64, 'step_size': 13, 'gamma': 0.758502637935509}. Best is trial 2 with value: 0.4206267166244291.[0m
[32m[I 2025-01-04 13:08:40,246][0m Trial 3 finished with value: 0.32737296403453336 and parameters: {'observation_period_num': 10, 'train_rates': 0.8783889502405708, 'learning_rate': 0.0007724204205001188, 'batch_size': 183, 'step_size': 8, 'gamma': 0.8504984028697863}. Best is trial 3 with value: 0.32737296403453336.[0m
[32m[I 2025-01-04 13:10:48,835][0m Trial 4 finished with value: 0.5809203374245114 and parameters: {'observation_period_num': 106, 'train_rates': 0.7918667963697644, 'learning_rate': 1.3948913454127474e-05, 'batch_size': 135, 'step_size': 6, 'gamma': 0.9054789149014131}. Best is trial 3 with value: 0.32737296403453336.[0m
[32m[I 2025-01-04 13:13:00,221][0m Trial 5 finished with value: 0.16495198941206543 and parameters: {'observation_period_num': 89, 'train_rates': 0.9142399668150826, 'learning_rate': 4.741809979608514e-05, 'batch_size': 43, 'step_size': 4, 'gamma': 0.9346087459318262}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:13:56,559][0m Trial 6 finished with value: 1.028183921518033 and parameters: {'observation_period_num': 52, 'train_rates': 0.6857327264319035, 'learning_rate': 6.893576694539226e-06, 'batch_size': 166, 'step_size': 14, 'gamma': 0.931437840909436}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:19:12,913][0m Trial 7 finished with value: 0.18876464397977152 and parameters: {'observation_period_num': 214, 'train_rates': 0.8879493732439931, 'learning_rate': 0.00015525193750192214, 'batch_size': 92, 'step_size': 4, 'gamma': 0.8788123081112195}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:22:49,146][0m Trial 8 finished with value: 0.622720260050655 and parameters: {'observation_period_num': 183, 'train_rates': 0.6925986444701328, 'learning_rate': 8.076064006604359e-05, 'batch_size': 98, 'step_size': 11, 'gamma': 0.9348897297541735}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:25:36,325][0m Trial 9 finished with value: 0.39171168469546136 and parameters: {'observation_period_num': 122, 'train_rates': 0.7988995519769223, 'learning_rate': 1.36438109863234e-05, 'batch_size': 28, 'step_size': 9, 'gamma': 0.9122392953946191}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:29:17,610][0m Trial 10 finished with value: 0.5110480710864067 and parameters: {'observation_period_num': 84, 'train_rates': 0.9694834276168984, 'learning_rate': 1.7793309546844595e-06, 'batch_size': 19, 'step_size': 1, 'gamma': 0.9754600338616957}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:35:28,442][0m Trial 11 finished with value: 0.20811007401595513 and parameters: {'observation_period_num': 245, 'train_rates': 0.9114432777374627, 'learning_rate': 0.00014341013937635582, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8610186523406536}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:39:20,517][0m Trial 12 finished with value: 2.262716226021198 and parameters: {'observation_period_num': 164, 'train_rates': 0.8866258037128176, 'learning_rate': 0.0007965226660119377, 'batch_size': 65, 'step_size': 5, 'gamma': 0.9866964275365608}. Best is trial 5 with value: 0.16495198941206543.[0m
Early stopping at epoch 73
[32m[I 2025-01-04 13:40:03,894][0m Trial 13 finished with value: 1.4010591627120124 and parameters: {'observation_period_num': 57, 'train_rates': 0.6113607569601327, 'learning_rate': 7.663276357649055e-05, 'batch_size': 113, 'step_size': 1, 'gamma': 0.8182740627222608}. Best is trial 5 with value: 0.16495198941206543.[0m
[32m[I 2025-01-04 13:43:58,394][0m Trial 14 finished with value: 0.13969679176807404 and parameters: {'observation_period_num': 154, 'train_rates': 0.9827330823730571, 'learning_rate': 0.0002978289422305207, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9554840937625041}. Best is trial 14 with value: 0.13969679176807404.[0m
[32m[I 2025-01-04 13:47:57,358][0m Trial 15 finished with value: 0.3371705638951269 and parameters: {'observation_period_num': 153, 'train_rates': 0.9795261968260017, 'learning_rate': 0.0004181100570757955, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9600953391337}. Best is trial 14 with value: 0.13969679176807404.[0m
[32m[I 2025-01-04 13:50:11,903][0m Trial 16 finished with value: 0.3771440105214303 and parameters: {'observation_period_num': 89, 'train_rates': 0.9369536630519991, 'learning_rate': 3.414984243927544e-06, 'batch_size': 43, 'step_size': 10, 'gamma': 0.9503778339327255}. Best is trial 14 with value: 0.13969679176807404.[0m
[32m[I 2025-01-04 13:51:08,298][0m Trial 17 finished with value: 0.36727485541374455 and parameters: {'observation_period_num': 47, 'train_rates': 0.8402052916906934, 'learning_rate': 4.9272247375492566e-05, 'batch_size': 252, 'step_size': 7, 'gamma': 0.8890560422301809}. Best is trial 14 with value: 0.13969679176807404.[0m
[32m[I 2025-01-04 13:51:43,311][0m Trial 18 finished with value: 0.14389684556544513 and parameters: {'observation_period_num': 6, 'train_rates': 0.9409786600728077, 'learning_rate': 0.00035457257478781423, 'batch_size': 136, 'step_size': 3, 'gamma': 0.9564662270203138}. Best is trial 14 with value: 0.13969679176807404.[0m
[32m[I 2025-01-04 13:55:17,263][0m Trial 19 finished with value: 0.09939107298851013 and parameters: {'observation_period_num': 145, 'train_rates': 0.9893318856019488, 'learning_rate': 0.0003604553074306955, 'batch_size': 138, 'step_size': 2, 'gamma': 0.9657427558615735}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 13:58:53,774][0m Trial 20 finished with value: 0.11236009746789932 and parameters: {'observation_period_num': 148, 'train_rates': 0.9858291037569384, 'learning_rate': 0.00033194425688820367, 'batch_size': 168, 'step_size': 12, 'gamma': 0.9884360518340238}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:02:47,410][0m Trial 21 finished with value: 0.1111685261130333 and parameters: {'observation_period_num': 158, 'train_rates': 0.9893720500863391, 'learning_rate': 0.00035444718654431, 'batch_size': 172, 'step_size': 12, 'gamma': 0.9890142198904748}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:05:59,397][0m Trial 22 finished with value: 2.412226676940918 and parameters: {'observation_period_num': 133, 'train_rates': 0.94952399707553, 'learning_rate': 0.0009337659933371758, 'batch_size': 176, 'step_size': 11, 'gamma': 0.9852651162282091}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:10:38,252][0m Trial 23 finished with value: 0.09957366436719894 and parameters: {'observation_period_num': 185, 'train_rates': 0.9887058394179712, 'learning_rate': 0.0002624363779564397, 'batch_size': 200, 'step_size': 12, 'gamma': 0.988850305760775}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:14:44,129][0m Trial 24 finished with value: 0.38476644046862735 and parameters: {'observation_period_num': 184, 'train_rates': 0.8436184624505687, 'learning_rate': 0.0005026468953839107, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9695405714065158}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:19:57,462][0m Trial 25 finished with value: 0.19480769336223602 and parameters: {'observation_period_num': 208, 'train_rates': 0.9531336224459321, 'learning_rate': 0.00021833775786290514, 'batch_size': 204, 'step_size': 12, 'gamma': 0.941814209106055}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:24:07,535][0m Trial 26 finished with value: 0.20735349684162477 and parameters: {'observation_period_num': 175, 'train_rates': 0.9187922504150415, 'learning_rate': 8.231976330521074e-05, 'batch_size': 152, 'step_size': 9, 'gamma': 0.9701186868543836}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:26:42,932][0m Trial 27 finished with value: 0.19142472743988037 and parameters: {'observation_period_num': 115, 'train_rates': 0.9601333813713284, 'learning_rate': 0.0005325119234464141, 'batch_size': 233, 'step_size': 15, 'gamma': 0.9240808201894936}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:31:34,923][0m Trial 28 finished with value: 0.2441761356929274 and parameters: {'observation_period_num': 205, 'train_rates': 0.8584282252095423, 'learning_rate': 0.0001149994468147854, 'batch_size': 195, 'step_size': 10, 'gamma': 0.9893254994327074}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:38:21,679][0m Trial 29 finished with value: 0.18017923831939697 and parameters: {'observation_period_num': 252, 'train_rates': 0.9888809930504616, 'learning_rate': 3.4201204760280156e-05, 'batch_size': 151, 'step_size': 13, 'gamma': 0.7702443414923604}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:41:35,984][0m Trial 30 finished with value: 0.16897624731063843 and parameters: {'observation_period_num': 138, 'train_rates': 0.9250987992630936, 'learning_rate': 0.00023910318919481566, 'batch_size': 226, 'step_size': 13, 'gamma': 0.9139205721309257}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:45:12,429][0m Trial 31 finished with value: 0.13303829729557037 and parameters: {'observation_period_num': 147, 'train_rates': 0.9846911113584206, 'learning_rate': 0.00027917372893813264, 'batch_size': 165, 'step_size': 12, 'gamma': 0.9677177565277678}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:49:16,560][0m Trial 32 finished with value: 0.33337175846099854 and parameters: {'observation_period_num': 167, 'train_rates': 0.9674828257523524, 'learning_rate': 0.0004856905390843174, 'batch_size': 123, 'step_size': 11, 'gamma': 0.9898490306171219}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:52:44,915][0m Trial 33 finished with value: 0.25607720017433167 and parameters: {'observation_period_num': 140, 'train_rates': 0.989655169028963, 'learning_rate': 0.0006351711832579078, 'batch_size': 189, 'step_size': 12, 'gamma': 0.9752145081826699}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 14:57:42,651][0m Trial 34 finished with value: 0.19182145595550537 and parameters: {'observation_period_num': 197, 'train_rates': 0.951279877640496, 'learning_rate': 0.0001834581176583613, 'batch_size': 152, 'step_size': 14, 'gamma': 0.9447082792920456}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:01:02,125][0m Trial 35 finished with value: 0.5071204513792069 and parameters: {'observation_period_num': 163, 'train_rates': 0.7333969277721493, 'learning_rate': 0.0003418244179938163, 'batch_size': 174, 'step_size': 9, 'gamma': 0.8384304592969495}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:05:45,568][0m Trial 36 finished with value: 0.20213467802917748 and parameters: {'observation_period_num': 196, 'train_rates': 0.8966880265881526, 'learning_rate': 0.00011023156670372322, 'batch_size': 214, 'step_size': 10, 'gamma': 0.8955263783830234}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:07:49,808][0m Trial 37 finished with value: 1.0507034275201288 and parameters: {'observation_period_num': 117, 'train_rates': 0.6438083196221744, 'learning_rate': 0.0006448961100215321, 'batch_size': 126, 'step_size': 13, 'gamma': 0.9761534868604348}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:13:19,675][0m Trial 38 finished with value: 2.0737444166944945 and parameters: {'observation_period_num': 238, 'train_rates': 0.808187957940604, 'learning_rate': 0.0009152303187029271, 'batch_size': 161, 'step_size': 14, 'gamma': 0.9641446301610533}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:15:38,271][0m Trial 39 finished with value: 0.29790703435018273 and parameters: {'observation_period_num': 105, 'train_rates': 0.9323733726571226, 'learning_rate': 2.4936818153848896e-05, 'batch_size': 192, 'step_size': 8, 'gamma': 0.9252704293882285}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:20:39,747][0m Trial 40 finished with value: 0.6661314271869989 and parameters: {'observation_period_num': 228, 'train_rates': 0.7662946576804207, 'learning_rate': 0.00021095537699478798, 'batch_size': 145, 'step_size': 2, 'gamma': 0.7722322843083285}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:24:04,425][0m Trial 41 finished with value: 0.17271064221858978 and parameters: {'observation_period_num': 141, 'train_rates': 0.9686695196478371, 'learning_rate': 0.00028835831610828436, 'batch_size': 169, 'step_size': 12, 'gamma': 0.9759104996273694}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:27:40,233][0m Trial 42 finished with value: 0.2016030251979828 and parameters: {'observation_period_num': 149, 'train_rates': 0.9677424412229652, 'learning_rate': 0.00037126298081425856, 'batch_size': 164, 'step_size': 12, 'gamma': 0.9630031303799805}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:31:47,657][0m Trial 43 finished with value: 0.17123173981342676 and parameters: {'observation_period_num': 176, 'train_rates': 0.9047542547324967, 'learning_rate': 0.00013520841176160102, 'batch_size': 183, 'step_size': 11, 'gamma': 0.9483123463676929}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:34:42,586][0m Trial 44 finished with value: 0.11817175149917603 and parameters: {'observation_period_num': 128, 'train_rates': 0.9896742816200427, 'learning_rate': 0.00016321665670323345, 'batch_size': 142, 'step_size': 12, 'gamma': 0.9786599638084224}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:37:37,501][0m Trial 45 finished with value: 0.2254582941532135 and parameters: {'observation_period_num': 128, 'train_rates': 0.9496354968246873, 'learning_rate': 5.441918402064679e-05, 'batch_size': 91, 'step_size': 14, 'gamma': 0.9809262352702611}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:39:50,387][0m Trial 46 finished with value: 0.17525482535362244 and parameters: {'observation_period_num': 100, 'train_rates': 0.9301751498638459, 'learning_rate': 0.0001745922250437388, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9896696432445212}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:43:31,098][0m Trial 47 finished with value: 0.23232737302780151 and parameters: {'observation_period_num': 162, 'train_rates': 0.8746078775806164, 'learning_rate': 0.0001029507193837764, 'batch_size': 136, 'step_size': 10, 'gamma': 0.9369536875362616}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:48:15,264][0m Trial 48 finished with value: 0.3337424397468567 and parameters: {'observation_period_num': 190, 'train_rates': 0.9702785200902574, 'learning_rate': 1.0566667176718808e-05, 'batch_size': 181, 'step_size': 13, 'gamma': 0.9555464175596774}. Best is trial 19 with value: 0.09939107298851013.[0m
[32m[I 2025-01-04 15:50:06,159][0m Trial 49 finished with value: 0.7571533918380737 and parameters: {'observation_period_num': 76, 'train_rates': 0.9879249647303107, 'learning_rate': 0.000663232100398136, 'batch_size': 83, 'step_size': 11, 'gamma': 0.9768359767747916}. Best is trial 19 with value: 0.09939107298851013.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 15:50:06,167][0m A new study created in memory with name: no-name-44e23937-bf4e-4fed-8ce4-72b5ed64ef83[0m
[32m[I 2025-01-04 15:52:54,763][0m Trial 0 finished with value: 0.3644374907016754 and parameters: {'observation_period_num': 119, 'train_rates': 0.9787843933534358, 'learning_rate': 1.0361106380769953e-05, 'batch_size': 102, 'step_size': 7, 'gamma': 0.8391606357815121}. Best is trial 0 with value: 0.3644374907016754.[0m
[32m[I 2025-01-04 15:54:32,176][0m Trial 1 finished with value: 1.6360510071118672 and parameters: {'observation_period_num': 92, 'train_rates': 0.6646497646839425, 'learning_rate': 1.0174722056882031e-06, 'batch_size': 212, 'step_size': 11, 'gamma': 0.7696446458267254}. Best is trial 0 with value: 0.3644374907016754.[0m
[32m[I 2025-01-04 15:55:48,141][0m Trial 2 finished with value: 0.20532885561077183 and parameters: {'observation_period_num': 55, 'train_rates': 0.878323046353146, 'learning_rate': 7.416890016893859e-05, 'batch_size': 69, 'step_size': 4, 'gamma': 0.9820494509131852}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 15:56:14,324][0m Trial 3 finished with value: 0.9811469467079386 and parameters: {'observation_period_num': 20, 'train_rates': 0.743711134500434, 'learning_rate': 2.875093984084675e-05, 'batch_size': 226, 'step_size': 3, 'gamma': 0.7578656814634921}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 15:58:49,426][0m Trial 4 finished with value: 0.863687215992676 and parameters: {'observation_period_num': 143, 'train_rates': 0.6073987749265418, 'learning_rate': 0.00012512072880137463, 'batch_size': 84, 'step_size': 14, 'gamma': 0.8798262672755773}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:04:41,283][0m Trial 5 finished with value: 0.7131275506998529 and parameters: {'observation_period_num': 243, 'train_rates': 0.8328397027552333, 'learning_rate': 1.9600812567709726e-06, 'batch_size': 60, 'step_size': 11, 'gamma': 0.9177152100166523}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:06:49,444][0m Trial 6 finished with value: 0.8701662712967286 and parameters: {'observation_period_num': 113, 'train_rates': 0.7329635074214953, 'learning_rate': 2.6027363444845637e-05, 'batch_size': 175, 'step_size': 6, 'gamma': 0.7539000276567845}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:09:34,005][0m Trial 7 finished with value: 0.5704772053729921 and parameters: {'observation_period_num': 125, 'train_rates': 0.911179541914962, 'learning_rate': 0.0008221813093321513, 'batch_size': 181, 'step_size': 8, 'gamma': 0.9743521294691888}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:13:30,753][0m Trial 8 finished with value: 1.0168822838643052 and parameters: {'observation_period_num': 187, 'train_rates': 0.7025693938694981, 'learning_rate': 0.00022711048304117294, 'batch_size': 32, 'step_size': 4, 'gamma': 0.9569956548435111}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:16:07,520][0m Trial 9 finished with value: 0.2142785522502615 and parameters: {'observation_period_num': 124, 'train_rates': 0.8627720483107127, 'learning_rate': 0.0002635992740971241, 'batch_size': 180, 'step_size': 9, 'gamma': 0.8297698784243347}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:16:42,917][0m Trial 10 finished with value: 0.5707094669342041 and parameters: {'observation_period_num': 10, 'train_rates': 0.9827397052417531, 'learning_rate': 5.098161160357375e-06, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9258226589830539}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:17:52,611][0m Trial 11 finished with value: 0.23436184112321246 and parameters: {'observation_period_num': 57, 'train_rates': 0.8482233512110213, 'learning_rate': 0.0001653627148874507, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8308307961927265}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:19:18,725][0m Trial 12 finished with value: 0.20573634354775133 and parameters: {'observation_period_num': 66, 'train_rates': 0.8960420360932855, 'learning_rate': 0.0005850706898309052, 'batch_size': 249, 'step_size': 5, 'gamma': 0.8071400478254874}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:20:33,072][0m Trial 13 finished with value: 0.6254002451896667 and parameters: {'observation_period_num': 59, 'train_rates': 0.9160304285340273, 'learning_rate': 0.000889131897034189, 'batch_size': 249, 'step_size': 5, 'gamma': 0.8016539877445577}. Best is trial 2 with value: 0.20532885561077183.[0m
[32m[I 2025-01-04 16:23:13,918][0m Trial 14 finished with value: 0.18701401872754986 and parameters: {'observation_period_num': 68, 'train_rates': 0.9072592358042282, 'learning_rate': 6.008393906832751e-05, 'batch_size': 25, 'step_size': 2, 'gamma': 0.872945097771231}. Best is trial 14 with value: 0.18701401872754986.[0m
[32m[I 2025-01-04 16:26:34,236][0m Trial 15 finished with value: 0.3930830372349201 and parameters: {'observation_period_num': 42, 'train_rates': 0.8014248467816617, 'learning_rate': 6.556588126707242e-05, 'batch_size': 18, 'step_size': 1, 'gamma': 0.8880507405555776}. Best is trial 14 with value: 0.18701401872754986.[0m
[32m[I 2025-01-04 16:28:42,127][0m Trial 16 finished with value: 0.18491003349607374 and parameters: {'observation_period_num': 86, 'train_rates': 0.9436257308175193, 'learning_rate': 4.909986415558312e-05, 'batch_size': 53, 'step_size': 1, 'gamma': 0.9856151826771816}. Best is trial 16 with value: 0.18491003349607374.[0m
[32m[I 2025-01-04 16:32:47,552][0m Trial 17 finished with value: 0.5607345341107784 and parameters: {'observation_period_num': 163, 'train_rates': 0.944380521791937, 'learning_rate': 1.3165926393436268e-05, 'batch_size': 44, 'step_size': 1, 'gamma': 0.9143320515207728}. Best is trial 16 with value: 0.18491003349607374.[0m
[32m[I 2025-01-04 16:34:35,089][0m Trial 18 finished with value: 0.39192399650709026 and parameters: {'observation_period_num': 88, 'train_rates': 0.7984857348249904, 'learning_rate': 6.189588837089154e-05, 'batch_size': 101, 'step_size': 2, 'gamma': 0.9476537056712635}. Best is trial 16 with value: 0.18491003349607374.[0m
[32m[I 2025-01-04 16:38:49,340][0m Trial 19 finished with value: 0.17056288099151917 and parameters: {'observation_period_num': 87, 'train_rates': 0.939330738422982, 'learning_rate': 1.1596494649855077e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8590629976163}. Best is trial 19 with value: 0.17056288099151917.[0m
[32m[I 2025-01-04 16:43:38,073][0m Trial 20 finished with value: 0.33081582510793534 and parameters: {'observation_period_num': 190, 'train_rates': 0.9467627964836951, 'learning_rate': 6.078849173530377e-06, 'batch_size': 55, 'step_size': 15, 'gamma': 0.846575124525749}. Best is trial 19 with value: 0.17056288099151917.[0m
[32m[I 2025-01-04 16:47:17,236][0m Trial 21 finished with value: 0.16078448585101537 and parameters: {'observation_period_num': 92, 'train_rates': 0.9389896816085013, 'learning_rate': 1.9389195350685054e-05, 'batch_size': 18, 'step_size': 13, 'gamma': 0.8611422154687689}. Best is trial 21 with value: 0.16078448585101537.[0m
[32m[I 2025-01-04 16:49:40,054][0m Trial 22 finished with value: 0.18726873082774026 and parameters: {'observation_period_num': 95, 'train_rates': 0.9390916293866357, 'learning_rate': 1.615228368749465e-05, 'batch_size': 37, 'step_size': 13, 'gamma': 0.8968318664929528}. Best is trial 21 with value: 0.16078448585101537.[0m
[32m[I 2025-01-04 16:53:48,590][0m Trial 23 finished with value: 0.1266453007857005 and parameters: {'observation_period_num': 32, 'train_rates': 0.9897625403900909, 'learning_rate': 5.790036687555532e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.851679547585994}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 16:57:57,627][0m Trial 24 finished with value: 0.14818040281534195 and parameters: {'observation_period_num': 40, 'train_rates': 0.9858665049817645, 'learning_rate': 3.864851441648822e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8604030138121921}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 16:58:55,056][0m Trial 25 finished with value: 0.4942924976348877 and parameters: {'observation_period_num': 33, 'train_rates': 0.9865468403605113, 'learning_rate': 3.2898186227906576e-06, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8073019880131285}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 16:59:43,527][0m Trial 26 finished with value: 0.5831460952758789 and parameters: {'observation_period_num': 28, 'train_rates': 0.9762761672094129, 'learning_rate': 2.221741022789346e-06, 'batch_size': 91, 'step_size': 12, 'gamma': 0.858570370113493}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:00:42,302][0m Trial 27 finished with value: 0.5068259835243225 and parameters: {'observation_period_num': 42, 'train_rates': 0.9613247504128258, 'learning_rate': 5.742817487202289e-06, 'batch_size': 123, 'step_size': 9, 'gamma': 0.9019282066397055}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:02:23,612][0m Trial 28 finished with value: 0.8298345313352697 and parameters: {'observation_period_num': 7, 'train_rates': 0.826934196415694, 'learning_rate': 1.0174503271510911e-06, 'batch_size': 36, 'step_size': 13, 'gamma': 0.7869148077794662}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:05:05,406][0m Trial 29 finished with value: 0.1740974336862564 and parameters: {'observation_period_num': 108, 'train_rates': 0.9846490049729484, 'learning_rate': 8.23556567057057e-06, 'batch_size': 45, 'step_size': 10, 'gamma': 0.851914991579617}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:09:04,900][0m Trial 30 finished with value: 0.3001853325290651 and parameters: {'observation_period_num': 142, 'train_rates': 0.8835784857320171, 'learning_rate': 3.867225398345558e-06, 'batch_size': 17, 'step_size': 14, 'gamma': 0.8249373889469029}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:13:00,118][0m Trial 31 finished with value: 0.15243686752718524 and parameters: {'observation_period_num': 78, 'train_rates': 0.924360309570506, 'learning_rate': 1.7912361348330463e-05, 'batch_size': 17, 'step_size': 15, 'gamma': 0.8622770820874961}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:14:44,259][0m Trial 32 finished with value: 0.21055440435994346 and parameters: {'observation_period_num': 70, 'train_rates': 0.963276187490435, 'learning_rate': 2.0086850037425235e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.8701721559594112}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:17:11,369][0m Trial 33 finished with value: 0.1924680547567191 and parameters: {'observation_period_num': 45, 'train_rates': 0.9222379967816791, 'learning_rate': 9.001908826745156e-06, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8455270401204548}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:19:10,632][0m Trial 34 finished with value: 0.6208602551257971 and parameters: {'observation_period_num': 77, 'train_rates': 0.9656669595162587, 'learning_rate': 1.6880683516571456e-06, 'batch_size': 48, 'step_size': 15, 'gamma': 0.8201911466605588}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:20:02,799][0m Trial 35 finished with value: 0.19859089610796402 and parameters: {'observation_period_num': 22, 'train_rates': 0.8780545501728688, 'learning_rate': 3.8201796510433546e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8814324678487965}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:22:03,289][0m Trial 36 finished with value: 0.1580556734250142 and parameters: {'observation_period_num': 49, 'train_rates': 0.9286577863694921, 'learning_rate': 1.68355820108816e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8640545946180662}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:23:24,002][0m Trial 37 finished with value: 1.0699204668916504 and parameters: {'observation_period_num': 46, 'train_rates': 0.630897321120893, 'learning_rate': 2.987357659190841e-06, 'batch_size': 37, 'step_size': 14, 'gamma': 0.8372916628066772}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:24:20,828][0m Trial 38 finished with value: 0.6236678019027101 and parameters: {'observation_period_num': 5, 'train_rates': 0.745190270008522, 'learning_rate': 7.443824305780533e-06, 'batch_size': 60, 'step_size': 11, 'gamma': 0.8984856962693387}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:25:12,439][0m Trial 39 finished with value: 0.17036450822507182 and parameters: {'observation_period_num': 32, 'train_rates': 0.9257145678501547, 'learning_rate': 3.325533755327406e-05, 'batch_size': 87, 'step_size': 15, 'gamma': 0.8695836862793681}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:31:13,851][0m Trial 40 finished with value: 1.4001065100599455 and parameters: {'observation_period_num': 241, 'train_rates': 0.8902345107287666, 'learning_rate': 1.462496629090849e-06, 'batch_size': 197, 'step_size': 10, 'gamma': 0.7737930949621695}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:33:25,441][0m Trial 41 finished with value: 0.17369264207388224 and parameters: {'observation_period_num': 54, 'train_rates': 0.9608596601401854, 'learning_rate': 1.8393311842327018e-05, 'batch_size': 30, 'step_size': 13, 'gamma': 0.8594450065889784}. Best is trial 23 with value: 0.1266453007857005.[0m
[32m[I 2025-01-04 17:37:34,970][0m Trial 42 finished with value: 0.09560423993295239 and parameters: {'observation_period_num': 96, 'train_rates': 0.9893372903441753, 'learning_rate': 2.7029906398666445e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8848518972631241}. Best is trial 42 with value: 0.09560423993295239.[0m
[32m[I 2025-01-04 17:40:27,911][0m Trial 43 finished with value: 0.09848250150680542 and parameters: {'observation_period_num': 107, 'train_rates': 0.986211541263278, 'learning_rate': 0.00010552770412437698, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8857913288639917}. Best is trial 42 with value: 0.09560423993295239.[0m
[32m[I 2025-01-04 17:43:03,759][0m Trial 44 finished with value: 0.11715295410986189 and parameters: {'observation_period_num': 106, 'train_rates': 0.9723718945346372, 'learning_rate': 0.00010827715346305514, 'batch_size': 49, 'step_size': 12, 'gamma': 0.8844700400477359}. Best is trial 42 with value: 0.09560423993295239.[0m
[32m[I 2025-01-04 17:45:50,908][0m Trial 45 finished with value: 0.08896014839410782 and parameters: {'observation_period_num': 111, 'train_rates': 0.9865457944039786, 'learning_rate': 0.00010361117141358826, 'batch_size': 44, 'step_size': 12, 'gamma': 0.933505858689719}. Best is trial 45 with value: 0.08896014839410782.[0m
[32m[I 2025-01-04 17:48:22,344][0m Trial 46 finished with value: 0.17580354462067285 and parameters: {'observation_period_num': 105, 'train_rates': 0.9686251417510334, 'learning_rate': 9.73327013687949e-05, 'batch_size': 69, 'step_size': 12, 'gamma': 0.9343812532311092}. Best is trial 45 with value: 0.08896014839410782.[0m
[32m[I 2025-01-04 17:51:56,802][0m Trial 47 finished with value: 0.11288198083639145 and parameters: {'observation_period_num': 146, 'train_rates': 0.9895936968538028, 'learning_rate': 0.00031356854164107055, 'batch_size': 140, 'step_size': 11, 'gamma': 0.9110290346921904}. Best is trial 45 with value: 0.08896014839410782.[0m
[32m[I 2025-01-04 17:55:15,380][0m Trial 48 finished with value: 0.20722514390945435 and parameters: {'observation_period_num': 137, 'train_rates': 0.9565328379441655, 'learning_rate': 0.00036997664345686163, 'batch_size': 151, 'step_size': 10, 'gamma': 0.9141648531017533}. Best is trial 45 with value: 0.08896014839410782.[0m
[32m[I 2025-01-04 17:58:53,934][0m Trial 49 finished with value: 0.1878302233731922 and parameters: {'observation_period_num': 155, 'train_rates': 0.9029835000563402, 'learning_rate': 0.00012697393514307683, 'batch_size': 121, 'step_size': 11, 'gamma': 0.936080952288651}. Best is trial 45 with value: 0.08896014839410782.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 17:58:53,941][0m A new study created in memory with name: no-name-3a03c099-4b94-4dc3-8d04-5b0cb3075afe[0m
[32m[I 2025-01-04 18:03:17,957][0m Trial 0 finished with value: 0.9529973011592339 and parameters: {'observation_period_num': 221, 'train_rates': 0.6603676500186635, 'learning_rate': 0.0002477883402674452, 'batch_size': 224, 'step_size': 5, 'gamma': 0.7538647119084794}. Best is trial 0 with value: 0.9529973011592339.[0m
[32m[I 2025-01-04 18:04:47,588][0m Trial 1 finished with value: 0.6601971070395952 and parameters: {'observation_period_num': 71, 'train_rates': 0.7074305279175386, 'learning_rate': 0.0003018154631402158, 'batch_size': 59, 'step_size': 8, 'gamma': 0.8468961290102286}. Best is trial 1 with value: 0.6601971070395952.[0m
[32m[I 2025-01-04 18:05:28,430][0m Trial 2 finished with value: 1.51570226708237 and parameters: {'observation_period_num': 38, 'train_rates': 0.6469023333939891, 'learning_rate': 3.7000441715826477e-06, 'batch_size': 239, 'step_size': 9, 'gamma': 0.9311555372046587}. Best is trial 1 with value: 0.6601971070395952.[0m
[32m[I 2025-01-04 18:06:28,372][0m Trial 3 finished with value: 1.3029243892986282 and parameters: {'observation_period_num': 43, 'train_rates': 0.9151043147197848, 'learning_rate': 1.2612800528442855e-06, 'batch_size': 119, 'step_size': 8, 'gamma': 0.8006858222773858}. Best is trial 1 with value: 0.6601971070395952.[0m
[32m[I 2025-01-04 18:12:02,318][0m Trial 4 finished with value: 0.35336390261848766 and parameters: {'observation_period_num': 220, 'train_rates': 0.9385602424363919, 'learning_rate': 1.5694606890128578e-05, 'batch_size': 127, 'step_size': 6, 'gamma': 0.8897724323936862}. Best is trial 4 with value: 0.35336390261848766.[0m
[32m[I 2025-01-04 18:12:54,148][0m Trial 5 finished with value: 0.13718352430770475 and parameters: {'observation_period_num': 40, 'train_rates': 0.9149161544177398, 'learning_rate': 0.0004708285274281506, 'batch_size': 237, 'step_size': 6, 'gamma': 0.9229596151556377}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:16:19,438][0m Trial 6 finished with value: 0.18495865485983898 and parameters: {'observation_period_num': 129, 'train_rates': 0.9224700174588827, 'learning_rate': 9.812396333538811e-05, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8350670422532224}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:19:13,795][0m Trial 7 finished with value: 0.36921950089304073 and parameters: {'observation_period_num': 133, 'train_rates': 0.831540066144754, 'learning_rate': 0.00010361663788630937, 'batch_size': 234, 'step_size': 15, 'gamma': 0.796709944996403}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:23:10,863][0m Trial 8 finished with value: 0.3232775330543518 and parameters: {'observation_period_num': 163, 'train_rates': 0.9573174426744799, 'learning_rate': 1.6066755118371916e-05, 'batch_size': 177, 'step_size': 9, 'gamma': 0.959179534970541}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:26:16,543][0m Trial 9 finished with value: 0.8652314172039767 and parameters: {'observation_period_num': 160, 'train_rates': 0.6844027404443863, 'learning_rate': 7.093821769345836e-05, 'batch_size': 80, 'step_size': 4, 'gamma': 0.755323028248125}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:26:40,418][0m Trial 10 finished with value: 1.1800912171882874 and parameters: {'observation_period_num': 11, 'train_rates': 0.8196965580800911, 'learning_rate': 0.0008743866630377856, 'batch_size': 164, 'step_size': 13, 'gamma': 0.9039027848172828}. Best is trial 5 with value: 0.13718352430770475.[0m
Early stopping at epoch 92
[32m[I 2025-01-04 18:28:53,922][0m Trial 11 finished with value: 1.0462415629001551 and parameters: {'observation_period_num': 93, 'train_rates': 0.8799064397002254, 'learning_rate': 0.0007420590062396258, 'batch_size': 29, 'step_size': 1, 'gamma': 0.8468470751843299}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:32:17,147][0m Trial 12 finished with value: 0.5385300415103766 and parameters: {'observation_period_num': 103, 'train_rates': 0.7620789038491511, 'learning_rate': 5.63425685537868e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9866914581667339}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:36:16,463][0m Trial 13 finished with value: 0.24216027212632366 and parameters: {'observation_period_num': 173, 'train_rates': 0.8775427248097845, 'learning_rate': 0.00027578360827856845, 'batch_size': 182, 'step_size': 3, 'gamma': 0.8659242260276174}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:39:08,506][0m Trial 14 finished with value: 0.1853102147579193 and parameters: {'observation_period_num': 122, 'train_rates': 0.9844683479272491, 'learning_rate': 2.7041468081369476e-05, 'batch_size': 90, 'step_size': 3, 'gamma': 0.9277045119938389}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:40:27,121][0m Trial 15 finished with value: 0.4260672468805938 and parameters: {'observation_period_num': 67, 'train_rates': 0.7620844344430969, 'learning_rate': 0.0001705955344966231, 'batch_size': 204, 'step_size': 6, 'gamma': 0.8173668682619005}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:44:55,860][0m Trial 16 finished with value: 0.3019647687673569 and parameters: {'observation_period_num': 192, 'train_rates': 0.8841607831485534, 'learning_rate': 0.0005945025405272443, 'batch_size': 152, 'step_size': 11, 'gamma': 0.9045896647482992}. Best is trial 5 with value: 0.13718352430770475.[0m
[32m[I 2025-01-04 18:45:34,160][0m Trial 17 finished with value: 0.08496861159801483 and parameters: {'observation_period_num': 8, 'train_rates': 0.9865472169001089, 'learning_rate': 0.00011843480588275453, 'batch_size': 110, 'step_size': 2, 'gamma': 0.9412735701245838}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:46:03,398][0m Trial 18 finished with value: 0.9955700258772398 and parameters: {'observation_period_num': 22, 'train_rates': 0.6027543420014263, 'learning_rate': 3.647398856866592e-05, 'batch_size': 111, 'step_size': 6, 'gamma': 0.9708407449481679}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:46:24,998][0m Trial 19 finished with value: 0.1418062150478363 and parameters: {'observation_period_num': 5, 'train_rates': 0.9674379636332485, 'learning_rate': 0.00047626375125623224, 'batch_size': 206, 'step_size': 4, 'gamma': 0.9426246346582712}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:47:31,187][0m Trial 20 finished with value: 0.43256378173828125 and parameters: {'observation_period_num': 50, 'train_rates': 0.9864095423781165, 'learning_rate': 8.165132379993622e-06, 'batch_size': 252, 'step_size': 10, 'gamma': 0.8840598349802957}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:47:52,011][0m Trial 21 finished with value: 0.13832375407218933 and parameters: {'observation_period_num': 7, 'train_rates': 0.9509219319584736, 'learning_rate': 0.00041285984934808255, 'batch_size': 206, 'step_size': 4, 'gamma': 0.9387505093683616}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:48:28,899][0m Trial 22 finished with value: 0.1635245835259013 and parameters: {'observation_period_num': 25, 'train_rates': 0.9064502574491107, 'learning_rate': 0.00020616693316851458, 'batch_size': 204, 'step_size': 5, 'gamma': 0.9510629769701007}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:49:55,609][0m Trial 23 finished with value: 0.28737603295907127 and parameters: {'observation_period_num': 67, 'train_rates': 0.8557263382059664, 'learning_rate': 0.00041720174406183175, 'batch_size': 145, 'step_size': 3, 'gamma': 0.9186467842843001}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:50:38,801][0m Trial 24 finished with value: 0.17615607380867004 and parameters: {'observation_period_num': 32, 'train_rates': 0.948570684473536, 'learning_rate': 0.0001360365364688513, 'batch_size': 219, 'step_size': 7, 'gamma': 0.9812766971044571}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:50:55,693][0m Trial 25 finished with value: 0.5315755605697632 and parameters: {'observation_period_num': 6, 'train_rates': 0.9224999366960664, 'learning_rate': 0.0009724306732612169, 'batch_size': 253, 'step_size': 2, 'gamma': 0.957740222981505}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:52:01,733][0m Trial 26 finished with value: 0.18554259836673737 and parameters: {'observation_period_num': 49, 'train_rates': 0.950240015719906, 'learning_rate': 0.0004297806385887553, 'batch_size': 183, 'step_size': 4, 'gamma': 0.915462928832507}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 18:57:52,805][0m Trial 27 finished with value: 0.4168697001656037 and parameters: {'observation_period_num': 252, 'train_rates': 0.7904120913976582, 'learning_rate': 5.539945271057616e-05, 'batch_size': 98, 'step_size': 5, 'gamma': 0.9374585071272677}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:00:06,024][0m Trial 28 finished with value: 0.1330961436033249 and parameters: {'observation_period_num': 91, 'train_rates': 0.9882392585575747, 'learning_rate': 0.00014594146979543703, 'batch_size': 65, 'step_size': 2, 'gamma': 0.895118053724548}. Best is trial 17 with value: 0.08496861159801483.[0m
Early stopping at epoch 94
[32m[I 2025-01-04 19:02:09,079][0m Trial 29 finished with value: 0.20255818963050842 and parameters: {'observation_period_num': 87, 'train_rates': 0.9881432427588187, 'learning_rate': 0.0001491952759406217, 'batch_size': 58, 'step_size': 1, 'gamma': 0.8823341629598374}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:04:39,197][0m Trial 30 finished with value: 0.2240565200683786 and parameters: {'observation_period_num': 109, 'train_rates': 0.9058731204918063, 'learning_rate': 0.00023863388644974895, 'batch_size': 63, 'step_size': 2, 'gamma': 0.8641457774694993}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:05:22,805][0m Trial 31 finished with value: 0.15583302080631256 and parameters: {'observation_period_num': 24, 'train_rates': 0.9658138520591494, 'learning_rate': 0.0003183109755390264, 'batch_size': 104, 'step_size': 4, 'gamma': 0.9031669200856153}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:07:17,505][0m Trial 32 finished with value: 0.17774520913161426 and parameters: {'observation_period_num': 79, 'train_rates': 0.9423936672877992, 'learning_rate': 9.670923321546926e-05, 'batch_size': 69, 'step_size': 3, 'gamma': 0.9204890110359348}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:08:42,314][0m Trial 33 finished with value: 0.17998552322387695 and parameters: {'observation_period_num': 63, 'train_rates': 0.9692392912053258, 'learning_rate': 0.0003491511740365985, 'batch_size': 134, 'step_size': 5, 'gamma': 0.9453818928058143}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:09:48,772][0m Trial 34 finished with value: 0.15183301270008087 and parameters: {'observation_period_num': 51, 'train_rates': 0.926334215623631, 'learning_rate': 0.0005650270179581257, 'batch_size': 220, 'step_size': 7, 'gamma': 0.967070646253219}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:10:37,628][0m Trial 35 finished with value: 0.19800159908257997 and parameters: {'observation_period_num': 37, 'train_rates': 0.8975450239407224, 'learning_rate': 0.0002144969503341558, 'batch_size': 236, 'step_size': 2, 'gamma': 0.9321811003127664}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:12:14,914][0m Trial 36 finished with value: 0.3551941514015198 and parameters: {'observation_period_num': 15, 'train_rates': 0.9891397075734953, 'learning_rate': 2.840197681327145e-06, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9012779409602635}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:13:15,854][0m Trial 37 finished with value: 0.32800871848633295 and parameters: {'observation_period_num': 43, 'train_rates': 0.8561658842808025, 'learning_rate': 0.00012310723534189152, 'batch_size': 85, 'step_size': 2, 'gamma': 0.8922780984092393}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:14:35,569][0m Trial 38 finished with value: 0.18516183144905987 and parameters: {'observation_period_num': 59, 'train_rates': 0.9412533627045082, 'learning_rate': 7.620965186520625e-05, 'batch_size': 117, 'step_size': 8, 'gamma': 0.8783668743096366}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:16:10,044][0m Trial 39 finished with value: 0.12827137034190328 and parameters: {'observation_period_num': 34, 'train_rates': 0.9286879599283782, 'learning_rate': 3.963175272239211e-05, 'batch_size': 44, 'step_size': 6, 'gamma': 0.9228145870854151}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:17:25,132][0m Trial 40 finished with value: 0.2646344665164925 and parameters: {'observation_period_num': 33, 'train_rates': 0.8551334144305438, 'learning_rate': 3.139222725147973e-05, 'batch_size': 49, 'step_size': 12, 'gamma': 0.9129234672076475}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:18:58,316][0m Trial 41 finished with value: 0.1655306006471316 and parameters: {'observation_period_num': 20, 'train_rates': 0.9283865493442947, 'learning_rate': 1.6362160742879633e-05, 'batch_size': 41, 'step_size': 6, 'gamma': 0.9289396141634744}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:19:55,644][0m Trial 42 finished with value: 0.16192327522569233 and parameters: {'observation_period_num': 5, 'train_rates': 0.96340918963792, 'learning_rate': 4.884362485901326e-05, 'batch_size': 71, 'step_size': 4, 'gamma': 0.9507522019832062}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:23:52,649][0m Trial 43 finished with value: 0.16536631811690586 and parameters: {'observation_period_num': 145, 'train_rates': 0.9671120733785428, 'learning_rate': 2.0756316668431802e-05, 'batch_size': 28, 'step_size': 5, 'gamma': 0.9388632242072634}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:24:41,644][0m Trial 44 finished with value: 0.6423239605180149 and parameters: {'observation_period_num': 35, 'train_rates': 0.9304873865660839, 'learning_rate': 9.441516558666785e-06, 'batch_size': 168, 'step_size': 3, 'gamma': 0.8974213172310563}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:26:30,927][0m Trial 45 finished with value: 0.17943519283189133 and parameters: {'observation_period_num': 84, 'train_rates': 0.9118885361976938, 'learning_rate': 7.452739225655573e-05, 'batch_size': 191, 'step_size': 9, 'gamma': 0.9700255443590854}. Best is trial 17 with value: 0.08496861159801483.[0m
Early stopping at epoch 98
[32m[I 2025-01-04 19:28:00,440][0m Trial 46 finished with value: 0.8111571363416853 and parameters: {'observation_period_num': 77, 'train_rates': 0.7326776461202993, 'learning_rate': 0.0006297632536108707, 'batch_size': 124, 'step_size': 1, 'gamma': 0.8581450160539676}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:28:25,166][0m Trial 47 finished with value: 0.2684279978275299 and parameters: {'observation_period_num': 15, 'train_rates': 0.9518510086125633, 'learning_rate': 9.604790600300967e-05, 'batch_size': 244, 'step_size': 6, 'gamma': 0.7516497553039387}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:29:41,452][0m Trial 48 finished with value: 0.1876971834002973 and parameters: {'observation_period_num': 56, 'train_rates': 0.8921165987950469, 'learning_rate': 0.00018742429167022516, 'batch_size': 77, 'step_size': 8, 'gamma': 0.9223297744034255}. Best is trial 17 with value: 0.08496861159801483.[0m
[32m[I 2025-01-04 19:32:00,887][0m Trial 49 finished with value: 0.372524360709152 and parameters: {'observation_period_num': 112, 'train_rates': 0.8687795834783003, 'learning_rate': 4.0493047509693194e-05, 'batch_size': 230, 'step_size': 5, 'gamma': 0.9086102657892383}. Best is trial 17 with value: 0.08496861159801483.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 19:32:00,894][0m A new study created in memory with name: no-name-2ded6247-6333-4646-b8b6-d51e40cc8fc1[0m
[32m[I 2025-01-04 19:33:52,491][0m Trial 0 finished with value: 0.5803698745772088 and parameters: {'observation_period_num': 94, 'train_rates': 0.7979128322467168, 'learning_rate': 2.7168904295108347e-05, 'batch_size': 210, 'step_size': 14, 'gamma': 0.8812719724906294}. Best is trial 0 with value: 0.5803698745772088.[0m
[32m[I 2025-01-04 19:35:22,031][0m Trial 1 finished with value: 0.343121915956006 and parameters: {'observation_period_num': 32, 'train_rates': 0.7953235639293107, 'learning_rate': 6.350445329753022e-05, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9591148329377892}. Best is trial 1 with value: 0.343121915956006.[0m
[32m[I 2025-01-04 19:41:03,813][0m Trial 2 finished with value: 0.9034036993980408 and parameters: {'observation_period_num': 225, 'train_rates': 0.9275128456389758, 'learning_rate': 5.5773204383766524e-06, 'batch_size': 256, 'step_size': 2, 'gamma': 0.9507093576956547}. Best is trial 1 with value: 0.343121915956006.[0m
[32m[I 2025-01-04 19:44:59,536][0m Trial 3 finished with value: 1.0539811258433296 and parameters: {'observation_period_num': 203, 'train_rates': 0.622934544712585, 'learning_rate': 7.108631425723348e-05, 'batch_size': 179, 'step_size': 4, 'gamma': 0.9012884155292373}. Best is trial 1 with value: 0.343121915956006.[0m
[32m[I 2025-01-04 19:50:07,952][0m Trial 4 finished with value: 1.4372460432621763 and parameters: {'observation_period_num': 236, 'train_rates': 0.7470737840264392, 'learning_rate': 2.6441438868948187e-06, 'batch_size': 139, 'step_size': 11, 'gamma': 0.8302202839773152}. Best is trial 1 with value: 0.343121915956006.[0m
[32m[I 2025-01-04 19:53:03,105][0m Trial 5 finished with value: 0.1166583001613617 and parameters: {'observation_period_num': 125, 'train_rates': 0.9812669289135333, 'learning_rate': 0.0005153336724268629, 'batch_size': 144, 'step_size': 3, 'gamma': 0.9388735186402237}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 19:58:23,339][0m Trial 6 finished with value: 0.4800013908257721 and parameters: {'observation_period_num': 209, 'train_rates': 0.7946662705739607, 'learning_rate': 0.0001011178661143342, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9830713565858832}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:01:12,426][0m Trial 7 finished with value: 0.4897005553874705 and parameters: {'observation_period_num': 135, 'train_rates': 0.7829298034394604, 'learning_rate': 3.6129615821185305e-05, 'batch_size': 151, 'step_size': 14, 'gamma': 0.918843999045251}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:04:08,341][0m Trial 8 finished with value: 1.0817125289695069 and parameters: {'observation_period_num': 88, 'train_rates': 0.9445593930257159, 'learning_rate': 0.0009496852358202554, 'batch_size': 22, 'step_size': 2, 'gamma': 0.8817832555460692}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:05:46,603][0m Trial 9 finished with value: 1.4641106124708358 and parameters: {'observation_period_num': 62, 'train_rates': 0.6117954121328576, 'learning_rate': 1.6186975643497172e-06, 'batch_size': 29, 'step_size': 4, 'gamma': 0.915110939347654}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:09:30,353][0m Trial 10 finished with value: 0.5440961718559265 and parameters: {'observation_period_num': 152, 'train_rates': 0.9845055357862081, 'learning_rate': 0.0009302074297934181, 'batch_size': 91, 'step_size': 6, 'gamma': 0.7744206103750482}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:10:12,711][0m Trial 11 finished with value: 0.20235476547113343 and parameters: {'observation_period_num': 22, 'train_rates': 0.8780958074398898, 'learning_rate': 0.00024802013444396453, 'batch_size': 92, 'step_size': 1, 'gamma': 0.9884747726184328}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:10:58,694][0m Trial 12 finished with value: 0.1836282955859438 and parameters: {'observation_period_num': 7, 'train_rates': 0.8825238209927665, 'learning_rate': 0.0003797218178651482, 'batch_size': 86, 'step_size': 1, 'gamma': 0.9839846288813442}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:14:56,755][0m Trial 13 finished with value: 0.2614383362233639 and parameters: {'observation_period_num': 172, 'train_rates': 0.86763266487484, 'learning_rate': 0.00031072026697289155, 'batch_size': 92, 'step_size': 6, 'gamma': 0.8368930796659957}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:15:51,975][0m Trial 14 finished with value: 0.22109772427012497 and parameters: {'observation_period_num': 6, 'train_rates': 0.872251274671285, 'learning_rate': 0.0003165518871938348, 'batch_size': 68, 'step_size': 5, 'gamma': 0.9432960186332326}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:18:08,502][0m Trial 15 finished with value: 0.16072794795036316 and parameters: {'observation_period_num': 100, 'train_rates': 0.9727888657683579, 'learning_rate': 0.00018008714654809885, 'batch_size': 122, 'step_size': 9, 'gamma': 0.9589204696646826}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:20:24,207][0m Trial 16 finished with value: 0.14580266177654266 and parameters: {'observation_period_num': 99, 'train_rates': 0.9721860488072679, 'learning_rate': 0.00015503760292814753, 'batch_size': 165, 'step_size': 9, 'gamma': 0.8459012544733198}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:23:02,769][0m Trial 17 finished with value: 0.3591974940731298 and parameters: {'observation_period_num': 118, 'train_rates': 0.9297851115469095, 'learning_rate': 1.219781557692682e-05, 'batch_size': 182, 'step_size': 11, 'gamma': 0.840256130979651}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:24:08,188][0m Trial 18 finished with value: 0.6155907573125885 and parameters: {'observation_period_num': 62, 'train_rates': 0.6812229453465946, 'learning_rate': 0.00013448435380916974, 'batch_size': 212, 'step_size': 8, 'gamma': 0.7855327812083848}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:28:07,917][0m Trial 19 finished with value: 0.18409798166248947 and parameters: {'observation_period_num': 171, 'train_rates': 0.9079742576540186, 'learning_rate': 0.0005574939061506134, 'batch_size': 167, 'step_size': 11, 'gamma': 0.8149762936219461}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:29:35,893][0m Trial 20 finished with value: 0.38571444352467854 and parameters: {'observation_period_num': 68, 'train_rates': 0.8336721034656708, 'learning_rate': 3.651867353749313e-05, 'batch_size': 121, 'step_size': 7, 'gamma': 0.86722825550168}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:32:06,490][0m Trial 21 finished with value: 0.1683877557516098 and parameters: {'observation_period_num': 109, 'train_rates': 0.9684056991075894, 'learning_rate': 0.0001704633088747109, 'batch_size': 125, 'step_size': 9, 'gamma': 0.9312235059758064}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:35:01,505][0m Trial 22 finished with value: 0.12001011520624161 and parameters: {'observation_period_num': 125, 'train_rates': 0.9848125316820653, 'learning_rate': 0.00017521869325930756, 'batch_size': 116, 'step_size': 10, 'gamma': 0.8596351565186101}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:38:24,627][0m Trial 23 finished with value: 0.18410642445087433 and parameters: {'observation_period_num': 138, 'train_rates': 0.9863577328589029, 'learning_rate': 0.0005628381126422937, 'batch_size': 156, 'step_size': 12, 'gamma': 0.860660394159934}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:42:18,565][0m Trial 24 finished with value: 0.23210342228412628 and parameters: {'observation_period_num': 163, 'train_rates': 0.9414795736405768, 'learning_rate': 8.59567336678925e-05, 'batch_size': 199, 'step_size': 12, 'gamma': 0.8025107233886489}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:44:00,942][0m Trial 25 finished with value: 0.36653631925582886 and parameters: {'observation_period_num': 79, 'train_rates': 0.9137963623966514, 'learning_rate': 1.6438692813119812e-05, 'batch_size': 250, 'step_size': 8, 'gamma': 0.852004580400843}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:46:52,325][0m Trial 26 finished with value: 0.3072764186597452 and parameters: {'observation_period_num': 126, 'train_rates': 0.9566341754566231, 'learning_rate': 0.0005143728291235181, 'batch_size': 112, 'step_size': 10, 'gamma': 0.7507585876641731}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:51:26,469][0m Trial 27 finished with value: 0.18211242109537124 and parameters: {'observation_period_num': 189, 'train_rates': 0.8990144733592165, 'learning_rate': 0.00019959654975691235, 'batch_size': 62, 'step_size': 15, 'gamma': 0.8943871066147414}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:54:39,183][0m Trial 28 finished with value: 0.36309262369930356 and parameters: {'observation_period_num': 147, 'train_rates': 0.8351409091671156, 'learning_rate': 5.430672811682768e-05, 'batch_size': 144, 'step_size': 7, 'gamma': 0.8147288924467309}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:56:39,451][0m Trial 29 finished with value: 0.5714062343116935 and parameters: {'observation_period_num': 105, 'train_rates': 0.7239812398020634, 'learning_rate': 0.00011694661457317268, 'batch_size': 213, 'step_size': 14, 'gamma': 0.8857630907863132}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:57:38,394][0m Trial 30 finished with value: 0.2500476539134979 and parameters: {'observation_period_num': 43, 'train_rates': 0.9882162053527648, 'learning_rate': 1.8521790674917972e-05, 'batch_size': 232, 'step_size': 13, 'gamma': 0.8547745801222193}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 20:59:51,655][0m Trial 31 finished with value: 0.2004262749888316 and parameters: {'observation_period_num': 97, 'train_rates': 0.9585472475505233, 'learning_rate': 0.00014629518329897372, 'batch_size': 114, 'step_size': 9, 'gamma': 0.9597345285103228}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:02:41,873][0m Trial 32 finished with value: 0.20831671357154846 and parameters: {'observation_period_num': 122, 'train_rates': 0.9628871894968513, 'learning_rate': 0.00020821296979707055, 'batch_size': 129, 'step_size': 10, 'gamma': 0.9675938512763592}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:04:37,434][0m Trial 33 finished with value: 0.15062158282210186 and parameters: {'observation_period_num': 87, 'train_rates': 0.9287401595380179, 'learning_rate': 0.0004533984978560731, 'batch_size': 167, 'step_size': 8, 'gamma': 0.9362761092773779}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:06:28,709][0m Trial 34 finished with value: 0.19647390840976847 and parameters: {'observation_period_num': 83, 'train_rates': 0.929229125694038, 'learning_rate': 0.0004523774824420301, 'batch_size': 169, 'step_size': 3, 'gamma': 0.9102788772057434}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:07:23,819][0m Trial 35 finished with value: 0.3839116169152357 and parameters: {'observation_period_num': 44, 'train_rates': 0.8477094779969432, 'learning_rate': 0.0006859282455557767, 'batch_size': 190, 'step_size': 7, 'gamma': 0.9263532608641573}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:09:55,998][0m Trial 36 finished with value: 0.1938878744840622 and parameters: {'observation_period_num': 113, 'train_rates': 0.9490797280042309, 'learning_rate': 0.00029675603994068587, 'batch_size': 159, 'step_size': 10, 'gamma': 0.899633730508397}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:13:04,638][0m Trial 37 finished with value: 0.1934458329878022 and parameters: {'observation_period_num': 136, 'train_rates': 0.919827514047396, 'learning_rate': 6.397956574876422e-05, 'batch_size': 140, 'step_size': 5, 'gamma': 0.9402203283398244}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:14:43,395][0m Trial 38 finished with value: 0.24484912764947145 and parameters: {'observation_period_num': 76, 'train_rates': 0.8951349520883719, 'learning_rate': 0.0006550591421988947, 'batch_size': 170, 'step_size': 12, 'gamma': 0.8758091295310941}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:15:49,637][0m Trial 39 finished with value: 0.8963311797585981 and parameters: {'observation_period_num': 48, 'train_rates': 0.940114670408238, 'learning_rate': 5.777260557462634e-06, 'batch_size': 105, 'step_size': 3, 'gamma': 0.8206050054723526}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:17:28,347][0m Trial 40 finished with value: 1.2471536494070483 and parameters: {'observation_period_num': 93, 'train_rates': 0.6422953207436393, 'learning_rate': 0.0008078326955698995, 'batch_size': 198, 'step_size': 6, 'gamma': 0.8461689766035198}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:19:47,622][0m Trial 41 finished with value: 0.16708482801914215 and parameters: {'observation_period_num': 102, 'train_rates': 0.9713880877391063, 'learning_rate': 0.000234734093075451, 'batch_size': 135, 'step_size': 9, 'gamma': 0.965313066498189}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:21:53,156][0m Trial 42 finished with value: 0.12771983444690704 and parameters: {'observation_period_num': 92, 'train_rates': 0.9745046905185673, 'learning_rate': 9.457666508815328e-05, 'batch_size': 148, 'step_size': 8, 'gamma': 0.9522665034249361}. Best is trial 5 with value: 0.1166583001613617.[0m
[32m[I 2025-01-04 21:25:27,587][0m Trial 43 finished with value: 0.10898599028587341 and parameters: {'observation_period_num': 146, 'train_rates': 0.9825693926500034, 'learning_rate': 9.475902227791612e-05, 'batch_size': 152, 'step_size': 8, 'gamma': 0.9725671933044144}. Best is trial 43 with value: 0.10898599028587341.[0m
[32m[I 2025-01-04 21:29:11,019][0m Trial 44 finished with value: 0.15536899864673615 and parameters: {'observation_period_num': 151, 'train_rates': 0.9716373448688439, 'learning_rate': 8.954987680656281e-05, 'batch_size': 148, 'step_size': 10, 'gamma': 0.9754987624934564}. Best is trial 43 with value: 0.10898599028587341.[0m
[32m[I 2025-01-04 21:35:51,751][0m Trial 45 finished with value: 0.16207343339920044 and parameters: {'observation_period_num': 249, 'train_rates': 0.9835221025387643, 'learning_rate': 3.420255944264298e-05, 'batch_size': 179, 'step_size': 8, 'gamma': 0.9471734037377999}. Best is trial 43 with value: 0.10898599028587341.[0m
[32m[I 2025-01-04 21:40:26,736][0m Trial 46 finished with value: 0.11906538158655167 and parameters: {'observation_period_num': 182, 'train_rates': 0.9887432705161459, 'learning_rate': 4.7575342637500286e-05, 'batch_size': 105, 'step_size': 11, 'gamma': 0.9711524249050447}. Best is trial 43 with value: 0.10898599028587341.[0m
[32m[I 2025-01-04 21:45:22,282][0m Trial 47 finished with value: 0.18377780708773384 and parameters: {'observation_period_num': 194, 'train_rates': 0.9474502469699073, 'learning_rate': 5.752236628499235e-05, 'batch_size': 80, 'step_size': 13, 'gamma': 0.9797159842958219}. Best is trial 43 with value: 0.10898599028587341.[0m
[32m[I 2025-01-04 21:50:59,937][0m Trial 48 finished with value: 0.11828487366437912 and parameters: {'observation_period_num': 214, 'train_rates': 0.9877387650962812, 'learning_rate': 5.159995525033969e-05, 'batch_size': 108, 'step_size': 11, 'gamma': 0.9538700518400842}. Best is trial 43 with value: 0.10898599028587341.[0m
[32m[I 2025-01-04 21:55:51,452][0m Trial 49 finished with value: 0.6398495215360801 and parameters: {'observation_period_num': 222, 'train_rates': 0.7498247132981161, 'learning_rate': 2.5864169338953445e-05, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9231174525633494}. Best is trial 43 with value: 0.10898599028587341.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 21:55:51,459][0m A new study created in memory with name: no-name-aad904d5-09de-486f-8abf-c058f16e3b72[0m
[32m[I 2025-01-04 22:00:36,939][0m Trial 0 finished with value: 0.33384560538240077 and parameters: {'observation_period_num': 204, 'train_rates': 0.846428113710355, 'learning_rate': 0.000136687043177441, 'batch_size': 236, 'step_size': 10, 'gamma': 0.7587104092773405}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:03:49,204][0m Trial 1 finished with value: 0.8690591155304739 and parameters: {'observation_period_num': 148, 'train_rates': 0.7295605510906205, 'learning_rate': 0.00020950035560619374, 'batch_size': 36, 'step_size': 8, 'gamma': 0.9878888305839575}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:06:54,821][0m Trial 2 finished with value: 0.6584165569298447 and parameters: {'observation_period_num': 161, 'train_rates': 0.681742955834218, 'learning_rate': 0.00012095715132614033, 'batch_size': 177, 'step_size': 12, 'gamma': 0.9241739845712819}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:08:49,419][0m Trial 3 finished with value: 0.35554557654335306 and parameters: {'observation_period_num': 88, 'train_rates': 0.9012027858475895, 'learning_rate': 8.810051586351911e-05, 'batch_size': 125, 'step_size': 2, 'gamma': 0.8314826930109445}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:13:21,005][0m Trial 4 finished with value: 0.7502079870129541 and parameters: {'observation_period_num': 209, 'train_rates': 0.6754968692432153, 'learning_rate': 9.59715734111745e-05, 'batch_size': 26, 'step_size': 14, 'gamma': 0.7918630409725353}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:16:24,725][0m Trial 5 finished with value: 1.0227390082061056 and parameters: {'observation_period_num': 143, 'train_rates': 0.7292164637539804, 'learning_rate': 2.4601284138793134e-06, 'batch_size': 42, 'step_size': 8, 'gamma': 0.9749229719579969}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:19:49,649][0m Trial 6 finished with value: 0.6861437934853456 and parameters: {'observation_period_num': 178, 'train_rates': 0.6724450124188135, 'learning_rate': 0.00011175361117987121, 'batch_size': 172, 'step_size': 10, 'gamma': 0.7849261064385743}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:22:43,287][0m Trial 7 finished with value: 0.6305076705021982 and parameters: {'observation_period_num': 148, 'train_rates': 0.6998088299769261, 'learning_rate': 7.153945184825322e-05, 'batch_size': 244, 'step_size': 8, 'gamma': 0.9096833464246343}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:23:41,203][0m Trial 8 finished with value: 1.0589245965441032 and parameters: {'observation_period_num': 54, 'train_rates': 0.6071090853274091, 'learning_rate': 5.5389016378908735e-05, 'batch_size': 86, 'step_size': 2, 'gamma': 0.9885819368017492}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:28:11,330][0m Trial 9 finished with value: 0.372854203818563 and parameters: {'observation_period_num': 198, 'train_rates': 0.8181661092026877, 'learning_rate': 0.00018979005910683565, 'batch_size': 253, 'step_size': 7, 'gamma': 0.7835822976754918}. Best is trial 0 with value: 0.33384560538240077.[0m
[32m[I 2025-01-04 22:34:52,652][0m Trial 10 finished with value: 0.1956525295972824 and parameters: {'observation_period_num': 251, 'train_rates': 0.972860242525726, 'learning_rate': 0.0007816939843901303, 'batch_size': 205, 'step_size': 5, 'gamma': 0.8473854332518268}. Best is trial 10 with value: 0.1956525295972824.[0m
[32m[I 2025-01-04 22:41:21,468][0m Trial 11 finished with value: 0.4447309970855713 and parameters: {'observation_period_num': 242, 'train_rates': 0.9898666996871551, 'learning_rate': 0.0009703094837553907, 'batch_size': 203, 'step_size': 6, 'gamma': 0.8475138291564663}. Best is trial 10 with value: 0.1956525295972824.[0m
[32m[I 2025-01-04 22:47:20,238][0m Trial 12 finished with value: 0.32979434225932663 and parameters: {'observation_period_num': 243, 'train_rates': 0.8777731015413861, 'learning_rate': 0.0006895233115475342, 'batch_size': 217, 'step_size': 4, 'gamma': 0.7508437667535246}. Best is trial 10 with value: 0.1956525295972824.[0m
[32m[I 2025-01-04 22:53:58,205][0m Trial 13 finished with value: 0.5417136549949646 and parameters: {'observation_period_num': 250, 'train_rates': 0.9728516697368251, 'learning_rate': 0.0009080417015150028, 'batch_size': 200, 'step_size': 4, 'gamma': 0.893648637394049}. Best is trial 10 with value: 0.1956525295972824.[0m
[32m[I 2025-01-04 22:55:54,688][0m Trial 14 finished with value: 0.4991589584387839 and parameters: {'observation_period_num': 89, 'train_rates': 0.9105975650484417, 'learning_rate': 1.1811315726228972e-05, 'batch_size': 140, 'step_size': 5, 'gamma': 0.83141299083541}. Best is trial 10 with value: 0.1956525295972824.[0m
[32m[I 2025-01-04 23:01:38,580][0m Trial 15 finished with value: 0.18273619343252742 and parameters: {'observation_period_num': 227, 'train_rates': 0.9189578237485498, 'learning_rate': 0.00042809171941558685, 'batch_size': 214, 'step_size': 4, 'gamma': 0.8756504799090189}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:02:09,057][0m Trial 16 finished with value: 0.36058717029881104 and parameters: {'observation_period_num': 14, 'train_rates': 0.935127018025611, 'learning_rate': 0.0003426206533762193, 'batch_size': 162, 'step_size': 1, 'gamma': 0.8716024905928249}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:07:48,609][0m Trial 17 finished with value: 0.4593364432150004 and parameters: {'observation_period_num': 219, 'train_rates': 0.9465096387963889, 'learning_rate': 1.728255848668379e-05, 'batch_size': 101, 'step_size': 4, 'gamma': 0.8738092571751302}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:12:45,825][0m Trial 18 finished with value: 0.4486510925625094 and parameters: {'observation_period_num': 223, 'train_rates': 0.7794054754111756, 'learning_rate': 0.0003063607330393057, 'batch_size': 222, 'step_size': 3, 'gamma': 0.9523275045866425}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:16:55,435][0m Trial 19 finished with value: 1.0518435914853121 and parameters: {'observation_period_num': 180, 'train_rates': 0.8751367339327527, 'learning_rate': 4.4058771322701875e-06, 'batch_size': 189, 'step_size': 6, 'gamma': 0.8440434904749566}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:19:14,048][0m Trial 20 finished with value: 0.23858363926410675 and parameters: {'observation_period_num': 104, 'train_rates': 0.9472769598712758, 'learning_rate': 0.00042928814235198757, 'batch_size': 152, 'step_size': 6, 'gamma': 0.8121641281426296}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:21:38,036][0m Trial 21 finished with value: 0.27821969985961914 and parameters: {'observation_period_num': 106, 'train_rates': 0.949578221964805, 'learning_rate': 0.00041897451326055696, 'batch_size': 149, 'step_size': 6, 'gamma': 0.8134008181582083}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:24:15,497][0m Trial 22 finished with value: 0.20715247700780126 and parameters: {'observation_period_num': 119, 'train_rates': 0.9167335920925399, 'learning_rate': 0.0004582465219581176, 'batch_size': 112, 'step_size': 5, 'gamma': 0.8112081969234383}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:25:28,232][0m Trial 23 finished with value: 0.2717628705417309 and parameters: {'observation_period_num': 55, 'train_rates': 0.9023908827175267, 'learning_rate': 0.0005335506877540564, 'batch_size': 108, 'step_size': 3, 'gamma': 0.8614318242360062}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:28:11,490][0m Trial 24 finished with value: 0.5344660719588168 and parameters: {'observation_period_num': 126, 'train_rates': 0.8407594551995239, 'learning_rate': 4.097869628039572e-05, 'batch_size': 78, 'step_size': 1, 'gamma': 0.8877884248558778}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:32:06,615][0m Trial 25 finished with value: 0.40515033324491834 and parameters: {'observation_period_num': 180, 'train_rates': 0.7884079422189302, 'learning_rate': 0.00025846744069400523, 'batch_size': 66, 'step_size': 5, 'gamma': 0.8140089824945858}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:38:13,256][0m Trial 26 finished with value: 1.0839546918869019 and parameters: {'observation_period_num': 230, 'train_rates': 0.9724253565520846, 'learning_rate': 1.1739085225117416e-06, 'batch_size': 125, 'step_size': 10, 'gamma': 0.8588327834766774}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:40:51,909][0m Trial 27 finished with value: 0.6147864383097374 and parameters: {'observation_period_num': 124, 'train_rates': 0.8660983484525215, 'learning_rate': 1.7576638702305656e-05, 'batch_size': 225, 'step_size': 3, 'gamma': 0.9269249139743286}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:42:09,378][0m Trial 28 finished with value: 0.20715022401129904 and parameters: {'observation_period_num': 61, 'train_rates': 0.9166160994198463, 'learning_rate': 0.0006494883769620342, 'batch_size': 185, 'step_size': 5, 'gamma': 0.892168317330447}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:42:30,283][0m Trial 29 finished with value: 0.25011710747018545 and parameters: {'observation_period_num': 10, 'train_rates': 0.8325934466087016, 'learning_rate': 0.0007176165346528375, 'batch_size': 190, 'step_size': 9, 'gamma': 0.8893416104150255}. Best is trial 15 with value: 0.18273619343252742.[0m
[32m[I 2025-01-04 23:43:30,902][0m Trial 30 finished with value: 0.16791383922100067 and parameters: {'observation_period_num': 47, 'train_rates': 0.9271348992145774, 'learning_rate': 0.00017292611120359015, 'batch_size': 233, 'step_size': 7, 'gamma': 0.905477881940147}. Best is trial 30 with value: 0.16791383922100067.[0m
[32m[I 2025-01-04 23:44:38,167][0m Trial 31 finished with value: 0.1732553243637085 and parameters: {'observation_period_num': 53, 'train_rates': 0.9240763899675921, 'learning_rate': 0.0001810772167294666, 'batch_size': 234, 'step_size': 7, 'gamma': 0.9099080975694483}. Best is trial 30 with value: 0.16791383922100067.[0m
[32m[I 2025-01-04 23:45:30,575][0m Trial 32 finished with value: 0.14899377524852753 and parameters: {'observation_period_num': 38, 'train_rates': 0.9723630995185737, 'learning_rate': 0.00017260835755023913, 'batch_size': 238, 'step_size': 7, 'gamma': 0.9424004631541362}. Best is trial 32 with value: 0.14899377524852753.[0m
[32m[I 2025-01-04 23:46:13,159][0m Trial 33 finished with value: 0.1652370407682171 and parameters: {'observation_period_num': 32, 'train_rates': 0.8866594697339057, 'learning_rate': 0.00017253023338565412, 'batch_size': 233, 'step_size': 12, 'gamma': 0.9424337953420046}. Best is trial 32 with value: 0.14899377524852753.[0m
[32m[I 2025-01-04 23:46:54,112][0m Trial 34 finished with value: 0.18247193718949953 and parameters: {'observation_period_num': 31, 'train_rates': 0.8891718802976054, 'learning_rate': 0.00016341519064944982, 'batch_size': 237, 'step_size': 12, 'gamma': 0.9413406869709806}. Best is trial 32 with value: 0.14899377524852753.[0m
[32m[I 2025-01-04 23:47:41,012][0m Trial 35 finished with value: 0.1045009046792984 and parameters: {'observation_period_num': 33, 'train_rates': 0.9893990007188281, 'learning_rate': 0.0001641381974189213, 'batch_size': 255, 'step_size': 12, 'gamma': 0.914857804500774}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:48:25,366][0m Trial 36 finished with value: 0.13833405077457428 and parameters: {'observation_period_num': 31, 'train_rates': 0.9877087340603431, 'learning_rate': 5.780392106719603e-05, 'batch_size': 252, 'step_size': 15, 'gamma': 0.9646489719415652}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:49:08,943][0m Trial 37 finished with value: 0.16161178052425385 and parameters: {'observation_period_num': 30, 'train_rates': 0.980005786105002, 'learning_rate': 3.109942960805694e-05, 'batch_size': 255, 'step_size': 15, 'gamma': 0.9652074852689247}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:50:47,328][0m Trial 38 finished with value: 0.16282060742378235 and parameters: {'observation_period_num': 72, 'train_rates': 0.979919205352695, 'learning_rate': 3.593384690449442e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9653828153927363}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:51:23,054][0m Trial 39 finished with value: 0.23289959132671356 and parameters: {'observation_period_num': 25, 'train_rates': 0.9565271370269889, 'learning_rate': 6.47802499496505e-05, 'batch_size': 256, 'step_size': 13, 'gamma': 0.971788122786192}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:53:09,937][0m Trial 40 finished with value: 0.29986244440078735 and parameters: {'observation_period_num': 78, 'train_rates': 0.9626050654556515, 'learning_rate': 2.586937814291126e-05, 'batch_size': 247, 'step_size': 15, 'gamma': 0.9598084666053261}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:54:01,878][0m Trial 41 finished with value: 0.13327695429325104 and parameters: {'observation_period_num': 38, 'train_rates': 0.9883145247050208, 'learning_rate': 4.0487043565237755e-05, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9743132742125142}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:54:55,167][0m Trial 42 finished with value: 0.1326594054698944 and parameters: {'observation_period_num': 39, 'train_rates': 0.9829769688706379, 'learning_rate': 0.0001031326683784053, 'batch_size': 241, 'step_size': 14, 'gamma': 0.9806618537896048}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:55:47,647][0m Trial 43 finished with value: 0.24204377830028534 and parameters: {'observation_period_num': 39, 'train_rates': 0.9584552370515967, 'learning_rate': 9.968849827506227e-05, 'batch_size': 243, 'step_size': 14, 'gamma': 0.9835730585955315}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:56:15,843][0m Trial 44 finished with value: 0.14563891291618347 and parameters: {'observation_period_num': 17, 'train_rates': 0.9852243658906068, 'learning_rate': 5.396932862331901e-05, 'batch_size': 225, 'step_size': 14, 'gamma': 0.9357641336351619}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:56:37,312][0m Trial 45 finished with value: 0.10675983130931854 and parameters: {'observation_period_num': 6, 'train_rates': 0.9862742325157828, 'learning_rate': 6.0603294097714907e-05, 'batch_size': 213, 'step_size': 14, 'gamma': 0.9264040206579071}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:56:54,116][0m Trial 46 finished with value: 0.43682134988511756 and parameters: {'observation_period_num': 7, 'train_rates': 0.7498142953515037, 'learning_rate': 8.76693205547095e-05, 'batch_size': 207, 'step_size': 13, 'gamma': 0.9810245985436381}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:58:26,658][0m Trial 47 finished with value: 0.13226103782653809 and parameters: {'observation_period_num': 66, 'train_rates': 0.989667445253746, 'learning_rate': 0.00012812957352615408, 'batch_size': 246, 'step_size': 11, 'gamma': 0.9536387466049482}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-04 23:59:33,451][0m Trial 48 finished with value: 1.0307145446669066 and parameters: {'observation_period_num': 65, 'train_rates': 0.6035241660060443, 'learning_rate': 0.000124840534157308, 'batch_size': 212, 'step_size': 11, 'gamma': 0.9203592329871103}. Best is trial 35 with value: 0.1045009046792984.[0m
[32m[I 2025-01-05 00:00:31,647][0m Trial 49 finished with value: 0.20737773180007935 and parameters: {'observation_period_num': 44, 'train_rates': 0.9365228063875894, 'learning_rate': 9.005554867269775e-05, 'batch_size': 225, 'step_size': 13, 'gamma': 0.955152079817996}. Best is trial 35 with value: 0.1045009046792984.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_Transformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 172, 'train_rates': 0.9886219911817518, 'learning_rate': 8.863344909546302e-05, 'batch_size': 72, 'step_size': 1, 'gamma': 0.9821454764162649}
Epoch 1/300, trend Loss: 0.9520 | 0.9748
Epoch 2/300, trend Loss: 0.6493 | 0.7541
Epoch 3/300, trend Loss: 0.4736 | 0.6016
Epoch 4/300, trend Loss: 0.3693 | 0.4873
Epoch 5/300, trend Loss: 0.3339 | 0.4324
Epoch 6/300, trend Loss: 0.3223 | 0.4084
Epoch 7/300, trend Loss: 0.2946 | 0.3515
Epoch 8/300, trend Loss: 0.3233 | 0.3647
Epoch 9/300, trend Loss: 0.2908 | 0.3338
Epoch 10/300, trend Loss: 0.2903 | 0.2952
Epoch 11/300, trend Loss: 0.2883 | 0.3027
Epoch 12/300, trend Loss: 0.2420 | 0.2536
Epoch 13/300, trend Loss: 0.2305 | 0.2579
Epoch 14/300, trend Loss: 0.2180 | 0.2388
Epoch 15/300, trend Loss: 0.1999 | 0.2331
Epoch 16/300, trend Loss: 0.1915 | 0.2195
Epoch 17/300, trend Loss: 0.1890 | 0.2154
Epoch 18/300, trend Loss: 0.1830 | 0.1988
Epoch 19/300, trend Loss: 0.1779 | 0.1975
Epoch 20/300, trend Loss: 0.1751 | 0.1871
Epoch 21/300, trend Loss: 0.1733 | 0.1876
Epoch 22/300, trend Loss: 0.1696 | 0.1781
Epoch 23/300, trend Loss: 0.1675 | 0.1760
Epoch 24/300, trend Loss: 0.1671 | 0.1710
Epoch 25/300, trend Loss: 0.1637 | 0.1683
Epoch 26/300, trend Loss: 0.1619 | 0.1641
Epoch 27/300, trend Loss: 0.1607 | 0.1617
Epoch 28/300, trend Loss: 0.1597 | 0.1591
Epoch 29/300, trend Loss: 0.1572 | 0.1570
Epoch 30/300, trend Loss: 0.1569 | 0.1553
Epoch 31/300, trend Loss: 0.1543 | 0.1522
Epoch 32/300, trend Loss: 0.1542 | 0.1505
Epoch 33/300, trend Loss: 0.1530 | 0.1495
Epoch 34/300, trend Loss: 0.1525 | 0.1467
Epoch 35/300, trend Loss: 0.1512 | 0.1464
Epoch 36/300, trend Loss: 0.1509 | 0.1433
Epoch 37/300, trend Loss: 0.1494 | 0.1426
Epoch 38/300, trend Loss: 0.1483 | 0.1401
Epoch 39/300, trend Loss: 0.1472 | 0.1407
Epoch 40/300, trend Loss: 0.1465 | 0.1379
Epoch 41/300, trend Loss: 0.1460 | 0.1383
Epoch 42/300, trend Loss: 0.1446 | 0.1363
Epoch 43/300, trend Loss: 0.1441 | 0.1362
Epoch 44/300, trend Loss: 0.1437 | 0.1346
Epoch 45/300, trend Loss: 0.1439 | 0.1341
Epoch 46/300, trend Loss: 0.1431 | 0.1335
Epoch 47/300, trend Loss: 0.1420 | 0.1330
Epoch 48/300, trend Loss: 0.1422 | 0.1314
Epoch 49/300, trend Loss: 0.1409 | 0.1310
Epoch 50/300, trend Loss: 0.1409 | 0.1301
Epoch 51/300, trend Loss: 0.1405 | 0.1296
Epoch 52/300, trend Loss: 0.1393 | 0.1289
Epoch 53/300, trend Loss: 0.1393 | 0.1284
Epoch 54/300, trend Loss: 0.1395 | 0.1275
Epoch 55/300, trend Loss: 0.1389 | 0.1275
Epoch 56/300, trend Loss: 0.1381 | 0.1266
Epoch 57/300, trend Loss: 0.1379 | 0.1259
Epoch 58/300, trend Loss: 0.1377 | 0.1255
Epoch 59/300, trend Loss: 0.1375 | 0.1247
Epoch 60/300, trend Loss: 0.1367 | 0.1247
Epoch 61/300, trend Loss: 0.1366 | 0.1243
Epoch 62/300, trend Loss: 0.1357 | 0.1233
Epoch 63/300, trend Loss: 0.1355 | 0.1235
Epoch 64/300, trend Loss: 0.1355 | 0.1230
Epoch 65/300, trend Loss: 0.1351 | 0.1228
Epoch 66/300, trend Loss: 0.1340 | 0.1225
Epoch 67/300, trend Loss: 0.1348 | 0.1220
Epoch 68/300, trend Loss: 0.1342 | 0.1216
Epoch 69/300, trend Loss: 0.1340 | 0.1216
Epoch 70/300, trend Loss: 0.1339 | 0.1216
Epoch 71/300, trend Loss: 0.1334 | 0.1209
Epoch 72/300, trend Loss: 0.1331 | 0.1208
Epoch 73/300, trend Loss: 0.1328 | 0.1204
Epoch 74/300, trend Loss: 0.1328 | 0.1200
Epoch 75/300, trend Loss: 0.1322 | 0.1196
Epoch 76/300, trend Loss: 0.1325 | 0.1192
Epoch 77/300, trend Loss: 0.1325 | 0.1189
Epoch 78/300, trend Loss: 0.1324 | 0.1191
Epoch 79/300, trend Loss: 0.1319 | 0.1190
Epoch 80/300, trend Loss: 0.1314 | 0.1182
Epoch 81/300, trend Loss: 0.1318 | 0.1179
Epoch 82/300, trend Loss: 0.1315 | 0.1175
Epoch 83/300, trend Loss: 0.1310 | 0.1175
Epoch 84/300, trend Loss: 0.1306 | 0.1172
Epoch 85/300, trend Loss: 0.1307 | 0.1176
Epoch 86/300, trend Loss: 0.1310 | 0.1166
Epoch 87/300, trend Loss: 0.1304 | 0.1163
Epoch 88/300, trend Loss: 0.1300 | 0.1166
Epoch 89/300, trend Loss: 0.1307 | 0.1164
Epoch 90/300, trend Loss: 0.1298 | 0.1161
Epoch 91/300, trend Loss: 0.1302 | 0.1163
Epoch 92/300, trend Loss: 0.1300 | 0.1162
Epoch 93/300, trend Loss: 0.1303 | 0.1157
Epoch 94/300, trend Loss: 0.1292 | 0.1155
Epoch 95/300, trend Loss: 0.1294 | 0.1151
Epoch 96/300, trend Loss: 0.1299 | 0.1152
Epoch 97/300, trend Loss: 0.1292 | 0.1154
Epoch 98/300, trend Loss: 0.1290 | 0.1150
Epoch 99/300, trend Loss: 0.1297 | 0.1152
Epoch 100/300, trend Loss: 0.1285 | 0.1153
Epoch 101/300, trend Loss: 0.1281 | 0.1148
Epoch 102/300, trend Loss: 0.1289 | 0.1146
Epoch 103/300, trend Loss: 0.1282 | 0.1147
Epoch 104/300, trend Loss: 0.1288 | 0.1144
Epoch 105/300, trend Loss: 0.1285 | 0.1144
Epoch 106/300, trend Loss: 0.1278 | 0.1144
Epoch 107/300, trend Loss: 0.1280 | 0.1143
Epoch 108/300, trend Loss: 0.1283 | 0.1141
Epoch 109/300, trend Loss: 0.1285 | 0.1140
Epoch 110/300, trend Loss: 0.1274 | 0.1139
Epoch 111/300, trend Loss: 0.1276 | 0.1139
Epoch 112/300, trend Loss: 0.1283 | 0.1141
Epoch 113/300, trend Loss: 0.1281 | 0.1139
Epoch 114/300, trend Loss: 0.1277 | 0.1135
Epoch 115/300, trend Loss: 0.1274 | 0.1137
Epoch 116/300, trend Loss: 0.1281 | 0.1136
Epoch 117/300, trend Loss: 0.1272 | 0.1131
Epoch 118/300, trend Loss: 0.1278 | 0.1131
Epoch 119/300, trend Loss: 0.1276 | 0.1133
Epoch 120/300, trend Loss: 0.1273 | 0.1133
Epoch 121/300, trend Loss: 0.1272 | 0.1133
Epoch 122/300, trend Loss: 0.1278 | 0.1131
Epoch 123/300, trend Loss: 0.1276 | 0.1130
Epoch 124/300, trend Loss: 0.1268 | 0.1129
Epoch 125/300, trend Loss: 0.1273 | 0.1130
Epoch 126/300, trend Loss: 0.1277 | 0.1129
Epoch 127/300, trend Loss: 0.1271 | 0.1128
Epoch 128/300, trend Loss: 0.1270 | 0.1130
Epoch 129/300, trend Loss: 0.1271 | 0.1128
Epoch 130/300, trend Loss: 0.1267 | 0.1128
Epoch 131/300, trend Loss: 0.1268 | 0.1127
Epoch 132/300, trend Loss: 0.1269 | 0.1126
Epoch 133/300, trend Loss: 0.1271 | 0.1126
Epoch 134/300, trend Loss: 0.1270 | 0.1126
Epoch 135/300, trend Loss: 0.1269 | 0.1124
Epoch 136/300, trend Loss: 0.1268 | 0.1123
Epoch 137/300, trend Loss: 0.1265 | 0.1122
Epoch 138/300, trend Loss: 0.1268 | 0.1121
Epoch 139/300, trend Loss: 0.1268 | 0.1121
Epoch 140/300, trend Loss: 0.1264 | 0.1121
Epoch 141/300, trend Loss: 0.1271 | 0.1121
Epoch 142/300, trend Loss: 0.1263 | 0.1121
Epoch 143/300, trend Loss: 0.1267 | 0.1121
Epoch 144/300, trend Loss: 0.1271 | 0.1121
Epoch 145/300, trend Loss: 0.1264 | 0.1121
Epoch 146/300, trend Loss: 0.1266 | 0.1121
Epoch 147/300, trend Loss: 0.1264 | 0.1121
Epoch 148/300, trend Loss: 0.1271 | 0.1120
Epoch 149/300, trend Loss: 0.1271 | 0.1121
Epoch 150/300, trend Loss: 0.1271 | 0.1121
Epoch 151/300, trend Loss: 0.1266 | 0.1120
Epoch 152/300, trend Loss: 0.1262 | 0.1120
Epoch 153/300, trend Loss: 0.1264 | 0.1120
Epoch 154/300, trend Loss: 0.1263 | 0.1119
Epoch 155/300, trend Loss: 0.1265 | 0.1119
Epoch 156/300, trend Loss: 0.1268 | 0.1119
Epoch 157/300, trend Loss: 0.1265 | 0.1118
Epoch 158/300, trend Loss: 0.1264 | 0.1118
Epoch 159/300, trend Loss: 0.1265 | 0.1118
Epoch 160/300, trend Loss: 0.1266 | 0.1118
Epoch 161/300, trend Loss: 0.1267 | 0.1118
Epoch 162/300, trend Loss: 0.1261 | 0.1118
Epoch 163/300, trend Loss: 0.1262 | 0.1118
Epoch 164/300, trend Loss: 0.1263 | 0.1118
Epoch 165/300, trend Loss: 0.1266 | 0.1117
Epoch 166/300, trend Loss: 0.1263 | 0.1117
Epoch 167/300, trend Loss: 0.1257 | 0.1117
Epoch 168/300, trend Loss: 0.1263 | 0.1116
Epoch 169/300, trend Loss: 0.1263 | 0.1116
Epoch 170/300, trend Loss: 0.1267 | 0.1117
Epoch 171/300, trend Loss: 0.1263 | 0.1116
Epoch 172/300, trend Loss: 0.1259 | 0.1117
Epoch 173/300, trend Loss: 0.1263 | 0.1117
Epoch 174/300, trend Loss: 0.1264 | 0.1117
Epoch 175/300, trend Loss: 0.1260 | 0.1117
Epoch 176/300, trend Loss: 0.1263 | 0.1117
Epoch 177/300, trend Loss: 0.1260 | 0.1116
Epoch 178/300, trend Loss: 0.1266 | 0.1116
Epoch 179/300, trend Loss: 0.1266 | 0.1116
Epoch 180/300, trend Loss: 0.1264 | 0.1116
Epoch 181/300, trend Loss: 0.1264 | 0.1116
Epoch 182/300, trend Loss: 0.1256 | 0.1115
Epoch 183/300, trend Loss: 0.1265 | 0.1115
Epoch 184/300, trend Loss: 0.1259 | 0.1115
Epoch 185/300, trend Loss: 0.1257 | 0.1115
Epoch 186/300, trend Loss: 0.1255 | 0.1115
Epoch 187/300, trend Loss: 0.1262 | 0.1115
Epoch 188/300, trend Loss: 0.1262 | 0.1115
Epoch 189/300, trend Loss: 0.1261 | 0.1115
Epoch 190/300, trend Loss: 0.1262 | 0.1115
Epoch 191/300, trend Loss: 0.1261 | 0.1115
Epoch 192/300, trend Loss: 0.1256 | 0.1115
Epoch 193/300, trend Loss: 0.1264 | 0.1115
Epoch 194/300, trend Loss: 0.1260 | 0.1115
Epoch 195/300, trend Loss: 0.1259 | 0.1115
Epoch 196/300, trend Loss: 0.1269 | 0.1115
Epoch 197/300, trend Loss: 0.1264 | 0.1115
Epoch 198/300, trend Loss: 0.1263 | 0.1115
Epoch 199/300, trend Loss: 0.1260 | 0.1115
Epoch 200/300, trend Loss: 0.1262 | 0.1115
Epoch 201/300, trend Loss: 0.1259 | 0.1115
Epoch 202/300, trend Loss: 0.1255 | 0.1115
Epoch 203/300, trend Loss: 0.1256 | 0.1115
Epoch 204/300, trend Loss: 0.1257 | 0.1115
Epoch 205/300, trend Loss: 0.1262 | 0.1115
Epoch 206/300, trend Loss: 0.1257 | 0.1115
Epoch 207/300, trend Loss: 0.1258 | 0.1115
Epoch 208/300, trend Loss: 0.1267 | 0.1115
Epoch 209/300, trend Loss: 0.1268 | 0.1115
Epoch 210/300, trend Loss: 0.1263 | 0.1115
Epoch 211/300, trend Loss: 0.1260 | 0.1115
Epoch 212/300, trend Loss: 0.1261 | 0.1115
Epoch 213/300, trend Loss: 0.1259 | 0.1115
Epoch 214/300, trend Loss: 0.1260 | 0.1114
Epoch 215/300, trend Loss: 0.1256 | 0.1114
Epoch 216/300, trend Loss: 0.1261 | 0.1114
Epoch 217/300, trend Loss: 0.1260 | 0.1115
Epoch 218/300, trend Loss: 0.1258 | 0.1115
Epoch 219/300, trend Loss: 0.1257 | 0.1114
Epoch 220/300, trend Loss: 0.1257 | 0.1114
Epoch 221/300, trend Loss: 0.1260 | 0.1114
Epoch 222/300, trend Loss: 0.1261 | 0.1114
Epoch 223/300, trend Loss: 0.1260 | 0.1115
Epoch 224/300, trend Loss: 0.1265 | 0.1114
Epoch 225/300, trend Loss: 0.1263 | 0.1114
Epoch 226/300, trend Loss: 0.1259 | 0.1114
Epoch 227/300, trend Loss: 0.1262 | 0.1114
Epoch 228/300, trend Loss: 0.1258 | 0.1114
Epoch 229/300, trend Loss: 0.1260 | 0.1114
Epoch 230/300, trend Loss: 0.1255 | 0.1114
Epoch 231/300, trend Loss: 0.1264 | 0.1114
Epoch 232/300, trend Loss: 0.1259 | 0.1114
Epoch 233/300, trend Loss: 0.1261 | 0.1114
Epoch 234/300, trend Loss: 0.1262 | 0.1115
Epoch 235/300, trend Loss: 0.1265 | 0.1115
Epoch 236/300, trend Loss: 0.1263 | 0.1115
Epoch 237/300, trend Loss: 0.1261 | 0.1115
Epoch 238/300, trend Loss: 0.1259 | 0.1115
Epoch 239/300, trend Loss: 0.1265 | 0.1115
Epoch 240/300, trend Loss: 0.1265 | 0.1114
Epoch 241/300, trend Loss: 0.1259 | 0.1114
Epoch 242/300, trend Loss: 0.1260 | 0.1114
Epoch 243/300, trend Loss: 0.1261 | 0.1114
Epoch 244/300, trend Loss: 0.1265 | 0.1114
Epoch 245/300, trend Loss: 0.1255 | 0.1114
Epoch 246/300, trend Loss: 0.1260 | 0.1114
Epoch 247/300, trend Loss: 0.1260 | 0.1114
Epoch 248/300, trend Loss: 0.1263 | 0.1114
Epoch 249/300, trend Loss: 0.1261 | 0.1114
Epoch 250/300, trend Loss: 0.1259 | 0.1114
Epoch 251/300, trend Loss: 0.1261 | 0.1114
Epoch 252/300, trend Loss: 0.1264 | 0.1114
Epoch 253/300, trend Loss: 0.1259 | 0.1114
Epoch 254/300, trend Loss: 0.1260 | 0.1114
Epoch 255/300, trend Loss: 0.1261 | 0.1114
Epoch 256/300, trend Loss: 0.1261 | 0.1114
Epoch 257/300, trend Loss: 0.1259 | 0.1114
Epoch 258/300, trend Loss: 0.1258 | 0.1114
Epoch 259/300, trend Loss: 0.1260 | 0.1114
Epoch 260/300, trend Loss: 0.1261 | 0.1114
Epoch 261/300, trend Loss: 0.1265 | 0.1114
Epoch 262/300, trend Loss: 0.1261 | 0.1114
Epoch 263/300, trend Loss: 0.1260 | 0.1114
Epoch 264/300, trend Loss: 0.1262 | 0.1114
Epoch 265/300, trend Loss: 0.1258 | 0.1114
Epoch 266/300, trend Loss: 0.1261 | 0.1114
Epoch 267/300, trend Loss: 0.1260 | 0.1114
Epoch 268/300, trend Loss: 0.1258 | 0.1114
Epoch 269/300, trend Loss: 0.1264 | 0.1114
Epoch 270/300, trend Loss: 0.1257 | 0.1114
Epoch 271/300, trend Loss: 0.1260 | 0.1114
Epoch 272/300, trend Loss: 0.1254 | 0.1114
Epoch 273/300, trend Loss: 0.1261 | 0.1114
Epoch 274/300, trend Loss: 0.1259 | 0.1114
Epoch 275/300, trend Loss: 0.1258 | 0.1114
Epoch 276/300, trend Loss: 0.1260 | 0.1114
Epoch 277/300, trend Loss: 0.1256 | 0.1114
Epoch 278/300, trend Loss: 0.1259 | 0.1114
Epoch 279/300, trend Loss: 0.1260 | 0.1114
Epoch 280/300, trend Loss: 0.1262 | 0.1114
Epoch 281/300, trend Loss: 0.1261 | 0.1114
Epoch 282/300, trend Loss: 0.1258 | 0.1114
Epoch 283/300, trend Loss: 0.1257 | 0.1114
Epoch 284/300, trend Loss: 0.1260 | 0.1114
Epoch 285/300, trend Loss: 0.1261 | 0.1114
Epoch 286/300, trend Loss: 0.1260 | 0.1114
Epoch 287/300, trend Loss: 0.1265 | 0.1114
Epoch 288/300, trend Loss: 0.1265 | 0.1114
Epoch 289/300, trend Loss: 0.1265 | 0.1114
Epoch 290/300, trend Loss: 0.1264 | 0.1114
Epoch 291/300, trend Loss: 0.1261 | 0.1114
Epoch 292/300, trend Loss: 0.1261 | 0.1114
Epoch 293/300, trend Loss: 0.1257 | 0.1114
Epoch 294/300, trend Loss: 0.1267 | 0.1114
Epoch 295/300, trend Loss: 0.1257 | 0.1114
Epoch 296/300, trend Loss: 0.1263 | 0.1114
Epoch 297/300, trend Loss: 0.1260 | 0.1114
Epoch 298/300, trend Loss: 0.1264 | 0.1114
Epoch 299/300, trend Loss: 0.1261 | 0.1114
Epoch 300/300, trend Loss: 0.1259 | 0.1114
Training seasonal_0 component with params: {'observation_period_num': 145, 'train_rates': 0.9893318856019488, 'learning_rate': 0.0003604553074306955, 'batch_size': 138, 'step_size': 2, 'gamma': 0.9657427558615735}
Epoch 1/300, seasonal_0 Loss: 0.9685 | 0.9144
Epoch 2/300, seasonal_0 Loss: 0.9304 | 0.7407
Epoch 3/300, seasonal_0 Loss: 0.7248 | 0.6546
Epoch 4/300, seasonal_0 Loss: 0.7401 | 0.5446
Epoch 5/300, seasonal_0 Loss: 0.6493 | 0.6107
Epoch 6/300, seasonal_0 Loss: 0.6096 | 0.5879
Epoch 7/300, seasonal_0 Loss: 0.5608 | 0.5084
Epoch 8/300, seasonal_0 Loss: 0.6709 | 0.5250
Epoch 9/300, seasonal_0 Loss: 0.5204 | 0.5746
Epoch 10/300, seasonal_0 Loss: 0.5792 | 0.5689
Epoch 11/300, seasonal_0 Loss: 0.4912 | 0.3942
Epoch 12/300, seasonal_0 Loss: 0.4240 | 0.3662
Epoch 13/300, seasonal_0 Loss: 0.4311 | 0.3720
Epoch 14/300, seasonal_0 Loss: 0.3726 | 0.3657
Epoch 15/300, seasonal_0 Loss: 0.3453 | 0.3208
Epoch 16/300, seasonal_0 Loss: 0.3029 | 0.2904
Epoch 17/300, seasonal_0 Loss: 0.2998 | 0.2818
Epoch 18/300, seasonal_0 Loss: 0.2839 | 0.2464
Epoch 19/300, seasonal_0 Loss: 0.3194 | 0.2314
Epoch 20/300, seasonal_0 Loss: 0.3539 | 0.2199
Epoch 21/300, seasonal_0 Loss: 0.3653 | 0.2867
Epoch 22/300, seasonal_0 Loss: 0.3061 | 0.2295
Epoch 23/300, seasonal_0 Loss: 0.2810 | 0.2157
Epoch 24/300, seasonal_0 Loss: 0.2483 | 0.2051
Epoch 25/300, seasonal_0 Loss: 0.2570 | 0.2300
Epoch 26/300, seasonal_0 Loss: 0.2922 | 0.2000
Epoch 27/300, seasonal_0 Loss: 0.2631 | 0.2146
Epoch 28/300, seasonal_0 Loss: 0.2631 | 0.2074
Epoch 29/300, seasonal_0 Loss: 0.2496 | 0.1865
Epoch 30/300, seasonal_0 Loss: 0.2201 | 0.2217
Epoch 31/300, seasonal_0 Loss: 0.2088 | 0.1827
Epoch 32/300, seasonal_0 Loss: 0.1961 | 0.1863
Epoch 33/300, seasonal_0 Loss: 0.1860 | 0.1689
Epoch 34/300, seasonal_0 Loss: 0.1806 | 0.1679
Epoch 35/300, seasonal_0 Loss: 0.1773 | 0.1657
Epoch 36/300, seasonal_0 Loss: 0.1752 | 0.1594
Epoch 37/300, seasonal_0 Loss: 0.1731 | 0.1620
Epoch 38/300, seasonal_0 Loss: 0.1711 | 0.1560
Epoch 39/300, seasonal_0 Loss: 0.1685 | 0.1559
Epoch 40/300, seasonal_0 Loss: 0.1671 | 0.1516
Epoch 41/300, seasonal_0 Loss: 0.1643 | 0.1516
Epoch 42/300, seasonal_0 Loss: 0.1623 | 0.1499
Epoch 43/300, seasonal_0 Loss: 0.1609 | 0.1481
Epoch 44/300, seasonal_0 Loss: 0.1593 | 0.1459
Epoch 45/300, seasonal_0 Loss: 0.1587 | 0.1466
Epoch 46/300, seasonal_0 Loss: 0.1571 | 0.1444
Epoch 47/300, seasonal_0 Loss: 0.1557 | 0.1426
Epoch 48/300, seasonal_0 Loss: 0.1553 | 0.1430
Epoch 49/300, seasonal_0 Loss: 0.1537 | 0.1418
Epoch 50/300, seasonal_0 Loss: 0.1524 | 0.1413
Epoch 51/300, seasonal_0 Loss: 0.1527 | 0.1400
Epoch 52/300, seasonal_0 Loss: 0.1519 | 0.1406
Epoch 53/300, seasonal_0 Loss: 0.1514 | 0.1397
Epoch 54/300, seasonal_0 Loss: 0.1492 | 0.1378
Epoch 55/300, seasonal_0 Loss: 0.1496 | 0.1368
Epoch 56/300, seasonal_0 Loss: 0.1488 | 0.1364
Epoch 57/300, seasonal_0 Loss: 0.1488 | 0.1360
Epoch 58/300, seasonal_0 Loss: 0.1479 | 0.1359
Epoch 59/300, seasonal_0 Loss: 0.1476 | 0.1357
Epoch 60/300, seasonal_0 Loss: 0.1472 | 0.1350
Epoch 61/300, seasonal_0 Loss: 0.1456 | 0.1339
Epoch 62/300, seasonal_0 Loss: 0.1456 | 0.1342
Epoch 63/300, seasonal_0 Loss: 0.1449 | 0.1351
Epoch 64/300, seasonal_0 Loss: 0.1450 | 0.1346
Epoch 65/300, seasonal_0 Loss: 0.1437 | 0.1325
Epoch 66/300, seasonal_0 Loss: 0.1443 | 0.1322
Epoch 67/300, seasonal_0 Loss: 0.1437 | 0.1322
Epoch 68/300, seasonal_0 Loss: 0.1434 | 0.1324
Epoch 69/300, seasonal_0 Loss: 0.1420 | 0.1311
Epoch 70/300, seasonal_0 Loss: 0.1422 | 0.1311
Epoch 71/300, seasonal_0 Loss: 0.1423 | 0.1318
Epoch 72/300, seasonal_0 Loss: 0.1417 | 0.1314
Epoch 73/300, seasonal_0 Loss: 0.1418 | 0.1313
Epoch 74/300, seasonal_0 Loss: 0.1406 | 0.1308
Epoch 75/300, seasonal_0 Loss: 0.1401 | 0.1304
Epoch 76/300, seasonal_0 Loss: 0.1403 | 0.1292
Epoch 77/300, seasonal_0 Loss: 0.1398 | 0.1287
Epoch 78/300, seasonal_0 Loss: 0.1394 | 0.1282
Epoch 79/300, seasonal_0 Loss: 0.1399 | 0.1283
Epoch 80/300, seasonal_0 Loss: 0.1392 | 0.1290
Epoch 81/300, seasonal_0 Loss: 0.1392 | 0.1285
Epoch 82/300, seasonal_0 Loss: 0.1390 | 0.1272
Epoch 83/300, seasonal_0 Loss: 0.1384 | 0.1272
Epoch 84/300, seasonal_0 Loss: 0.1379 | 0.1276
Epoch 85/300, seasonal_0 Loss: 0.1378 | 0.1272
Epoch 86/300, seasonal_0 Loss: 0.1370 | 0.1268
Epoch 87/300, seasonal_0 Loss: 0.1374 | 0.1265
Epoch 88/300, seasonal_0 Loss: 0.1373 | 0.1267
Epoch 89/300, seasonal_0 Loss: 0.1369 | 0.1265
Epoch 90/300, seasonal_0 Loss: 0.1367 | 0.1265
Epoch 91/300, seasonal_0 Loss: 0.1369 | 0.1260
Epoch 92/300, seasonal_0 Loss: 0.1363 | 0.1260
Epoch 93/300, seasonal_0 Loss: 0.1361 | 0.1262
Epoch 94/300, seasonal_0 Loss: 0.1357 | 0.1261
Epoch 95/300, seasonal_0 Loss: 0.1360 | 0.1257
Epoch 96/300, seasonal_0 Loss: 0.1353 | 0.1249
Epoch 97/300, seasonal_0 Loss: 0.1354 | 0.1251
Epoch 98/300, seasonal_0 Loss: 0.1349 | 0.1253
Epoch 99/300, seasonal_0 Loss: 0.1349 | 0.1252
Epoch 100/300, seasonal_0 Loss: 0.1354 | 0.1251
Epoch 101/300, seasonal_0 Loss: 0.1346 | 0.1243
Epoch 102/300, seasonal_0 Loss: 0.1349 | 0.1237
Epoch 103/300, seasonal_0 Loss: 0.1342 | 0.1240
Epoch 104/300, seasonal_0 Loss: 0.1343 | 0.1242
Epoch 105/300, seasonal_0 Loss: 0.1339 | 0.1240
Epoch 106/300, seasonal_0 Loss: 0.1351 | 0.1241
Epoch 107/300, seasonal_0 Loss: 0.1332 | 0.1240
Epoch 108/300, seasonal_0 Loss: 0.1342 | 0.1239
Epoch 109/300, seasonal_0 Loss: 0.1335 | 0.1235
Epoch 110/300, seasonal_0 Loss: 0.1332 | 0.1234
Epoch 111/300, seasonal_0 Loss: 0.1335 | 0.1230
Epoch 112/300, seasonal_0 Loss: 0.1334 | 0.1233
Epoch 113/300, seasonal_0 Loss: 0.1324 | 0.1232
Epoch 114/300, seasonal_0 Loss: 0.1331 | 0.1231
Epoch 115/300, seasonal_0 Loss: 0.1325 | 0.1226
Epoch 116/300, seasonal_0 Loss: 0.1322 | 0.1224
Epoch 117/300, seasonal_0 Loss: 0.1322 | 0.1224
Epoch 118/300, seasonal_0 Loss: 0.1326 | 0.1228
Epoch 119/300, seasonal_0 Loss: 0.1325 | 0.1228
Epoch 120/300, seasonal_0 Loss: 0.1329 | 0.1225
Epoch 121/300, seasonal_0 Loss: 0.1327 | 0.1223
Epoch 122/300, seasonal_0 Loss: 0.1323 | 0.1221
Epoch 123/300, seasonal_0 Loss: 0.1321 | 0.1218
Epoch 124/300, seasonal_0 Loss: 0.1318 | 0.1216
Epoch 125/300, seasonal_0 Loss: 0.1325 | 0.1219
Epoch 126/300, seasonal_0 Loss: 0.1324 | 0.1220
Epoch 127/300, seasonal_0 Loss: 0.1318 | 0.1218
Epoch 128/300, seasonal_0 Loss: 0.1319 | 0.1217
Epoch 129/300, seasonal_0 Loss: 0.1314 | 0.1215
Epoch 130/300, seasonal_0 Loss: 0.1308 | 0.1213
Epoch 131/300, seasonal_0 Loss: 0.1317 | 0.1211
Epoch 132/300, seasonal_0 Loss: 0.1313 | 0.1213
Epoch 133/300, seasonal_0 Loss: 0.1316 | 0.1213
Epoch 134/300, seasonal_0 Loss: 0.1311 | 0.1213
Epoch 135/300, seasonal_0 Loss: 0.1319 | 0.1214
Epoch 136/300, seasonal_0 Loss: 0.1312 | 0.1213
Epoch 137/300, seasonal_0 Loss: 0.1317 | 0.1212
Epoch 138/300, seasonal_0 Loss: 0.1310 | 0.1212
Epoch 139/300, seasonal_0 Loss: 0.1317 | 0.1212
Epoch 140/300, seasonal_0 Loss: 0.1308 | 0.1210
Epoch 141/300, seasonal_0 Loss: 0.1311 | 0.1211
Epoch 142/300, seasonal_0 Loss: 0.1314 | 0.1208
Epoch 143/300, seasonal_0 Loss: 0.1310 | 0.1209
Epoch 144/300, seasonal_0 Loss: 0.1307 | 0.1210
Epoch 145/300, seasonal_0 Loss: 0.1302 | 0.1212
Epoch 146/300, seasonal_0 Loss: 0.1315 | 0.1210
Epoch 147/300, seasonal_0 Loss: 0.1312 | 0.1209
Epoch 148/300, seasonal_0 Loss: 0.1297 | 0.1209
Epoch 149/300, seasonal_0 Loss: 0.1307 | 0.1210
Epoch 150/300, seasonal_0 Loss: 0.1310 | 0.1210
Epoch 151/300, seasonal_0 Loss: 0.1308 | 0.1209
Epoch 152/300, seasonal_0 Loss: 0.1305 | 0.1209
Epoch 153/300, seasonal_0 Loss: 0.1303 | 0.1208
Epoch 154/300, seasonal_0 Loss: 0.1305 | 0.1207
Epoch 155/300, seasonal_0 Loss: 0.1304 | 0.1206
Epoch 156/300, seasonal_0 Loss: 0.1306 | 0.1206
Epoch 157/300, seasonal_0 Loss: 0.1307 | 0.1205
Epoch 158/300, seasonal_0 Loss: 0.1302 | 0.1206
Epoch 159/300, seasonal_0 Loss: 0.1298 | 0.1206
Epoch 160/300, seasonal_0 Loss: 0.1302 | 0.1204
Epoch 161/300, seasonal_0 Loss: 0.1303 | 0.1204
Epoch 162/300, seasonal_0 Loss: 0.1305 | 0.1204
Epoch 163/300, seasonal_0 Loss: 0.1296 | 0.1205
Epoch 164/300, seasonal_0 Loss: 0.1300 | 0.1205
Epoch 165/300, seasonal_0 Loss: 0.1303 | 0.1205
Epoch 166/300, seasonal_0 Loss: 0.1301 | 0.1205
Epoch 167/300, seasonal_0 Loss: 0.1293 | 0.1204
Epoch 168/300, seasonal_0 Loss: 0.1303 | 0.1204
Epoch 169/300, seasonal_0 Loss: 0.1305 | 0.1203
Epoch 170/300, seasonal_0 Loss: 0.1304 | 0.1203
Epoch 171/300, seasonal_0 Loss: 0.1297 | 0.1202
Epoch 172/300, seasonal_0 Loss: 0.1302 | 0.1202
Epoch 173/300, seasonal_0 Loss: 0.1303 | 0.1201
Epoch 174/300, seasonal_0 Loss: 0.1299 | 0.1202
Epoch 175/300, seasonal_0 Loss: 0.1301 | 0.1202
Epoch 176/300, seasonal_0 Loss: 0.1313 | 0.1201
Epoch 177/300, seasonal_0 Loss: 0.1298 | 0.1200
Epoch 178/300, seasonal_0 Loss: 0.1300 | 0.1200
Epoch 179/300, seasonal_0 Loss: 0.1305 | 0.1200
Epoch 180/300, seasonal_0 Loss: 0.1297 | 0.1200
Epoch 181/300, seasonal_0 Loss: 0.1302 | 0.1200
Epoch 182/300, seasonal_0 Loss: 0.1298 | 0.1200
Epoch 183/300, seasonal_0 Loss: 0.1298 | 0.1200
Epoch 184/300, seasonal_0 Loss: 0.1301 | 0.1199
Epoch 185/300, seasonal_0 Loss: 0.1300 | 0.1199
Epoch 186/300, seasonal_0 Loss: 0.1292 | 0.1199
Epoch 187/300, seasonal_0 Loss: 0.1302 | 0.1199
Epoch 188/300, seasonal_0 Loss: 0.1296 | 0.1199
Epoch 189/300, seasonal_0 Loss: 0.1300 | 0.1198
Epoch 190/300, seasonal_0 Loss: 0.1300 | 0.1199
Epoch 191/300, seasonal_0 Loss: 0.1301 | 0.1199
Epoch 192/300, seasonal_0 Loss: 0.1298 | 0.1199
Epoch 193/300, seasonal_0 Loss: 0.1302 | 0.1199
Epoch 194/300, seasonal_0 Loss: 0.1301 | 0.1199
Epoch 195/300, seasonal_0 Loss: 0.1293 | 0.1199
Epoch 196/300, seasonal_0 Loss: 0.1294 | 0.1199
Epoch 197/300, seasonal_0 Loss: 0.1298 | 0.1199
Epoch 198/300, seasonal_0 Loss: 0.1297 | 0.1199
Epoch 199/300, seasonal_0 Loss: 0.1296 | 0.1199
Epoch 200/300, seasonal_0 Loss: 0.1302 | 0.1199
Epoch 201/300, seasonal_0 Loss: 0.1304 | 0.1199
Epoch 202/300, seasonal_0 Loss: 0.1301 | 0.1199
Epoch 203/300, seasonal_0 Loss: 0.1295 | 0.1200
Epoch 204/300, seasonal_0 Loss: 0.1303 | 0.1200
Epoch 205/300, seasonal_0 Loss: 0.1297 | 0.1200
Epoch 206/300, seasonal_0 Loss: 0.1294 | 0.1200
Epoch 207/300, seasonal_0 Loss: 0.1298 | 0.1200
Epoch 208/300, seasonal_0 Loss: 0.1295 | 0.1199
Epoch 209/300, seasonal_0 Loss: 0.1300 | 0.1199
Epoch 210/300, seasonal_0 Loss: 0.1298 | 0.1199
Epoch 211/300, seasonal_0 Loss: 0.1295 | 0.1198
Epoch 212/300, seasonal_0 Loss: 0.1297 | 0.1198
Epoch 213/300, seasonal_0 Loss: 0.1304 | 0.1198
Epoch 214/300, seasonal_0 Loss: 0.1299 | 0.1198
Epoch 215/300, seasonal_0 Loss: 0.1294 | 0.1198
Epoch 216/300, seasonal_0 Loss: 0.1291 | 0.1198
Epoch 217/300, seasonal_0 Loss: 0.1297 | 0.1198
Epoch 218/300, seasonal_0 Loss: 0.1298 | 0.1198
Epoch 219/300, seasonal_0 Loss: 0.1292 | 0.1198
Epoch 220/300, seasonal_0 Loss: 0.1303 | 0.1198
Epoch 221/300, seasonal_0 Loss: 0.1300 | 0.1198
Epoch 222/300, seasonal_0 Loss: 0.1297 | 0.1198
Epoch 223/300, seasonal_0 Loss: 0.1296 | 0.1198
Epoch 224/300, seasonal_0 Loss: 0.1289 | 0.1198
Epoch 225/300, seasonal_0 Loss: 0.1300 | 0.1198
Epoch 226/300, seasonal_0 Loss: 0.1300 | 0.1198
Epoch 227/300, seasonal_0 Loss: 0.1296 | 0.1198
Epoch 228/300, seasonal_0 Loss: 0.1301 | 0.1198
Epoch 229/300, seasonal_0 Loss: 0.1300 | 0.1198
Epoch 230/300, seasonal_0 Loss: 0.1298 | 0.1198
Epoch 231/300, seasonal_0 Loss: 0.1294 | 0.1198
Epoch 232/300, seasonal_0 Loss: 0.1295 | 0.1198
Epoch 233/300, seasonal_0 Loss: 0.1292 | 0.1197
Epoch 234/300, seasonal_0 Loss: 0.1292 | 0.1198
Epoch 235/300, seasonal_0 Loss: 0.1296 | 0.1198
Epoch 236/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 237/300, seasonal_0 Loss: 0.1296 | 0.1197
Epoch 238/300, seasonal_0 Loss: 0.1301 | 0.1198
Epoch 239/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 240/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 241/300, seasonal_0 Loss: 0.1295 | 0.1198
Epoch 242/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 243/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 244/300, seasonal_0 Loss: 0.1297 | 0.1198
Epoch 245/300, seasonal_0 Loss: 0.1300 | 0.1197
Epoch 246/300, seasonal_0 Loss: 0.1300 | 0.1197
Epoch 247/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 248/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 249/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 250/300, seasonal_0 Loss: 0.1291 | 0.1197
Epoch 251/300, seasonal_0 Loss: 0.1290 | 0.1197
Epoch 252/300, seasonal_0 Loss: 0.1298 | 0.1197
Epoch 253/300, seasonal_0 Loss: 0.1293 | 0.1197
Epoch 254/300, seasonal_0 Loss: 0.1303 | 0.1197
Epoch 255/300, seasonal_0 Loss: 0.1300 | 0.1197
Epoch 256/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 257/300, seasonal_0 Loss: 0.1296 | 0.1197
Epoch 258/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 259/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 260/300, seasonal_0 Loss: 0.1292 | 0.1197
Epoch 261/300, seasonal_0 Loss: 0.1296 | 0.1197
Epoch 262/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 263/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 264/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 265/300, seasonal_0 Loss: 0.1298 | 0.1197
Epoch 266/300, seasonal_0 Loss: 0.1296 | 0.1197
Epoch 267/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 268/300, seasonal_0 Loss: 0.1292 | 0.1197
Epoch 269/300, seasonal_0 Loss: 0.1292 | 0.1197
Epoch 270/300, seasonal_0 Loss: 0.1302 | 0.1197
Epoch 271/300, seasonal_0 Loss: 0.1293 | 0.1197
Epoch 272/300, seasonal_0 Loss: 0.1293 | 0.1197
Epoch 273/300, seasonal_0 Loss: 0.1301 | 0.1197
Epoch 274/300, seasonal_0 Loss: 0.1303 | 0.1197
Epoch 275/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 276/300, seasonal_0 Loss: 0.1292 | 0.1197
Epoch 277/300, seasonal_0 Loss: 0.1291 | 0.1197
Epoch 278/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 279/300, seasonal_0 Loss: 0.1296 | 0.1197
Epoch 280/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 281/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 282/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 283/300, seasonal_0 Loss: 0.1290 | 0.1197
Epoch 284/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 285/300, seasonal_0 Loss: 0.1298 | 0.1197
Epoch 286/300, seasonal_0 Loss: 0.1300 | 0.1197
Epoch 287/300, seasonal_0 Loss: 0.1291 | 0.1197
Epoch 288/300, seasonal_0 Loss: 0.1293 | 0.1197
Epoch 289/300, seasonal_0 Loss: 0.1300 | 0.1197
Epoch 290/300, seasonal_0 Loss: 0.1292 | 0.1197
Epoch 291/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 292/300, seasonal_0 Loss: 0.1294 | 0.1197
Epoch 293/300, seasonal_0 Loss: 0.1298 | 0.1197
Epoch 294/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 295/300, seasonal_0 Loss: 0.1295 | 0.1197
Epoch 296/300, seasonal_0 Loss: 0.1298 | 0.1197
Epoch 297/300, seasonal_0 Loss: 0.1286 | 0.1197
Epoch 298/300, seasonal_0 Loss: 0.1297 | 0.1197
Epoch 299/300, seasonal_0 Loss: 0.1291 | 0.1197
Epoch 300/300, seasonal_0 Loss: 0.1294 | 0.1197
Training seasonal_1 component with params: {'observation_period_num': 111, 'train_rates': 0.9865457944039786, 'learning_rate': 0.00010361117141358826, 'batch_size': 44, 'step_size': 12, 'gamma': 0.933505858689719}
Epoch 1/300, seasonal_1 Loss: 0.7511 | 0.7832
Epoch 2/300, seasonal_1 Loss: 0.5944 | 0.5808
Epoch 3/300, seasonal_1 Loss: 0.4165 | 0.4683
Epoch 4/300, seasonal_1 Loss: 0.3678 | 0.4530
Epoch 5/300, seasonal_1 Loss: 0.3115 | 0.3968
Epoch 6/300, seasonal_1 Loss: 0.3152 | 0.3653
Epoch 7/300, seasonal_1 Loss: 0.3333 | 0.3135
Epoch 8/300, seasonal_1 Loss: 0.2999 | 0.3374
Epoch 9/300, seasonal_1 Loss: 0.2582 | 0.3121
Epoch 10/300, seasonal_1 Loss: 0.2594 | 0.3026
Epoch 11/300, seasonal_1 Loss: 0.2258 | 0.2439
Epoch 12/300, seasonal_1 Loss: 0.2325 | 0.2510
Epoch 13/300, seasonal_1 Loss: 0.2244 | 0.2751
Epoch 14/300, seasonal_1 Loss: 0.2217 | 0.2223
Epoch 15/300, seasonal_1 Loss: 0.2022 | 0.2140
Epoch 16/300, seasonal_1 Loss: 0.2130 | 0.2327
Epoch 17/300, seasonal_1 Loss: 0.1970 | 0.2260
Epoch 18/300, seasonal_1 Loss: 0.1897 | 0.1907
Epoch 19/300, seasonal_1 Loss: 0.1876 | 0.1879
Epoch 20/300, seasonal_1 Loss: 0.1939 | 0.1969
Epoch 21/300, seasonal_1 Loss: 0.1915 | 0.1742
Epoch 22/300, seasonal_1 Loss: 0.1914 | 0.2031
Epoch 23/300, seasonal_1 Loss: 0.1847 | 0.1953
Epoch 24/300, seasonal_1 Loss: 0.1895 | 0.1617
Epoch 25/300, seasonal_1 Loss: 0.2007 | 0.2049
Epoch 26/300, seasonal_1 Loss: 0.1778 | 0.1978
Epoch 27/300, seasonal_1 Loss: 0.1965 | 0.2033
Epoch 28/300, seasonal_1 Loss: 0.1793 | 0.1599
Epoch 29/300, seasonal_1 Loss: 0.1716 | 0.1685
Epoch 30/300, seasonal_1 Loss: 0.1711 | 0.1815
Epoch 31/300, seasonal_1 Loss: 0.1573 | 0.1588
Epoch 32/300, seasonal_1 Loss: 0.1523 | 0.1587
Epoch 33/300, seasonal_1 Loss: 0.1535 | 0.1554
Epoch 34/300, seasonal_1 Loss: 0.1491 | 0.1447
Epoch 35/300, seasonal_1 Loss: 0.1454 | 0.1427
Epoch 36/300, seasonal_1 Loss: 0.1452 | 0.1366
Epoch 37/300, seasonal_1 Loss: 0.1457 | 0.1304
Epoch 38/300, seasonal_1 Loss: 0.1443 | 0.1371
Epoch 39/300, seasonal_1 Loss: 0.1441 | 0.1374
Epoch 40/300, seasonal_1 Loss: 0.1458 | 0.1357
Epoch 41/300, seasonal_1 Loss: 0.1440 | 0.1364
Epoch 42/300, seasonal_1 Loss: 0.1435 | 0.1212
Epoch 43/300, seasonal_1 Loss: 0.1468 | 0.1302
Epoch 44/300, seasonal_1 Loss: 0.1426 | 0.1333
Epoch 45/300, seasonal_1 Loss: 0.1459 | 0.1284
Epoch 46/300, seasonal_1 Loss: 0.1459 | 0.1419
Epoch 47/300, seasonal_1 Loss: 0.1445 | 0.1354
Epoch 48/300, seasonal_1 Loss: 0.1531 | 0.1435
Epoch 49/300, seasonal_1 Loss: 0.1452 | 0.1290
Epoch 50/300, seasonal_1 Loss: 0.1533 | 0.1266
Epoch 51/300, seasonal_1 Loss: 0.1544 | 0.1512
Epoch 52/300, seasonal_1 Loss: 0.1410 | 0.1195
Epoch 53/300, seasonal_1 Loss: 0.1410 | 0.1266
Epoch 54/300, seasonal_1 Loss: 0.1416 | 0.1359
Epoch 55/300, seasonal_1 Loss: 0.1361 | 0.1180
Epoch 56/300, seasonal_1 Loss: 0.1317 | 0.1175
Epoch 57/300, seasonal_1 Loss: 0.1288 | 0.1132
Epoch 58/300, seasonal_1 Loss: 0.1238 | 0.1078
Epoch 59/300, seasonal_1 Loss: 0.1206 | 0.1076
Epoch 60/300, seasonal_1 Loss: 0.1190 | 0.1068
Epoch 61/300, seasonal_1 Loss: 0.1170 | 0.1053
Epoch 62/300, seasonal_1 Loss: 0.1151 | 0.1093
Epoch 63/300, seasonal_1 Loss: 0.1178 | 0.1059
Epoch 64/300, seasonal_1 Loss: 0.1178 | 0.1045
Epoch 65/300, seasonal_1 Loss: 0.1142 | 0.1149
Epoch 66/300, seasonal_1 Loss: 0.1140 | 0.1034
Epoch 67/300, seasonal_1 Loss: 0.1182 | 0.1181
Epoch 68/300, seasonal_1 Loss: 0.1139 | 0.1113
Epoch 69/300, seasonal_1 Loss: 0.1119 | 0.0990
Epoch 70/300, seasonal_1 Loss: 0.1133 | 0.1135
Epoch 71/300, seasonal_1 Loss: 0.1149 | 0.1015
Epoch 72/300, seasonal_1 Loss: 0.1130 | 0.1083
Epoch 73/300, seasonal_1 Loss: 0.1092 | 0.0980
Epoch 74/300, seasonal_1 Loss: 0.1101 | 0.0984
Epoch 75/300, seasonal_1 Loss: 0.1058 | 0.1047
Epoch 76/300, seasonal_1 Loss: 0.1074 | 0.1017
Epoch 77/300, seasonal_1 Loss: 0.1091 | 0.1009
Epoch 78/300, seasonal_1 Loss: 0.1091 | 0.1139
Epoch 79/300, seasonal_1 Loss: 0.1080 | 0.0979
Epoch 80/300, seasonal_1 Loss: 0.1028 | 0.0947
Epoch 81/300, seasonal_1 Loss: 0.1038 | 0.1153
Epoch 82/300, seasonal_1 Loss: 0.1021 | 0.0960
Epoch 83/300, seasonal_1 Loss: 0.1020 | 0.0934
Epoch 84/300, seasonal_1 Loss: 0.0998 | 0.1013
Epoch 85/300, seasonal_1 Loss: 0.1052 | 0.1058
Epoch 86/300, seasonal_1 Loss: 0.1054 | 0.0971
Epoch 87/300, seasonal_1 Loss: 0.1017 | 0.0958
Epoch 88/300, seasonal_1 Loss: 0.1056 | 0.1041
Epoch 89/300, seasonal_1 Loss: 0.0984 | 0.0979
Epoch 90/300, seasonal_1 Loss: 0.0962 | 0.0928
Epoch 91/300, seasonal_1 Loss: 0.0983 | 0.0997
Epoch 92/300, seasonal_1 Loss: 0.0977 | 0.1010
Epoch 93/300, seasonal_1 Loss: 0.0981 | 0.0968
Epoch 94/300, seasonal_1 Loss: 0.0989 | 0.0961
Epoch 95/300, seasonal_1 Loss: 0.0994 | 0.0940
Epoch 96/300, seasonal_1 Loss: 0.0906 | 0.0939
Epoch 97/300, seasonal_1 Loss: 0.0879 | 0.0904
Epoch 98/300, seasonal_1 Loss: 0.0875 | 0.0894
Epoch 99/300, seasonal_1 Loss: 0.0858 | 0.0911
Epoch 100/300, seasonal_1 Loss: 0.0848 | 0.0886
Epoch 101/300, seasonal_1 Loss: 0.0838 | 0.0857
Epoch 102/300, seasonal_1 Loss: 0.0837 | 0.0882
Epoch 103/300, seasonal_1 Loss: 0.0827 | 0.0866
Epoch 104/300, seasonal_1 Loss: 0.0840 | 0.0905
Epoch 105/300, seasonal_1 Loss: 0.0845 | 0.0906
Epoch 106/300, seasonal_1 Loss: 0.0858 | 0.0883
Epoch 107/300, seasonal_1 Loss: 0.0837 | 0.0879
Epoch 108/300, seasonal_1 Loss: 0.0843 | 0.0891
Epoch 109/300, seasonal_1 Loss: 0.0838 | 0.0894
Epoch 110/300, seasonal_1 Loss: 0.0827 | 0.0855
Epoch 111/300, seasonal_1 Loss: 0.0829 | 0.0888
Epoch 112/300, seasonal_1 Loss: 0.0806 | 0.0915
Epoch 113/300, seasonal_1 Loss: 0.0798 | 0.0847
Epoch 114/300, seasonal_1 Loss: 0.0787 | 0.0820
Epoch 115/300, seasonal_1 Loss: 0.0790 | 0.0850
Epoch 116/300, seasonal_1 Loss: 0.0793 | 0.0869
Epoch 117/300, seasonal_1 Loss: 0.0788 | 0.0837
Epoch 118/300, seasonal_1 Loss: 0.0780 | 0.0829
Epoch 119/300, seasonal_1 Loss: 0.0781 | 0.0829
Epoch 120/300, seasonal_1 Loss: 0.0806 | 0.0856
Epoch 121/300, seasonal_1 Loss: 0.0777 | 0.0840
Epoch 122/300, seasonal_1 Loss: 0.0788 | 0.0822
Epoch 123/300, seasonal_1 Loss: 0.0794 | 0.0809
Epoch 124/300, seasonal_1 Loss: 0.0773 | 0.0853
Epoch 125/300, seasonal_1 Loss: 0.0796 | 0.0886
Epoch 126/300, seasonal_1 Loss: 0.0769 | 0.0783
Epoch 127/300, seasonal_1 Loss: 0.0785 | 0.0881
Epoch 128/300, seasonal_1 Loss: 0.0759 | 0.0819
Epoch 129/300, seasonal_1 Loss: 0.0742 | 0.0773
Epoch 130/300, seasonal_1 Loss: 0.0733 | 0.0863
Epoch 131/300, seasonal_1 Loss: 0.0735 | 0.0851
Epoch 132/300, seasonal_1 Loss: 0.0726 | 0.0775
Epoch 133/300, seasonal_1 Loss: 0.0725 | 0.0797
Epoch 134/300, seasonal_1 Loss: 0.0718 | 0.0812
Epoch 135/300, seasonal_1 Loss: 0.0713 | 0.0821
Epoch 136/300, seasonal_1 Loss: 0.0712 | 0.0780
Epoch 137/300, seasonal_1 Loss: 0.0698 | 0.0807
Epoch 138/300, seasonal_1 Loss: 0.0703 | 0.0838
Epoch 139/300, seasonal_1 Loss: 0.0702 | 0.0809
Epoch 140/300, seasonal_1 Loss: 0.0695 | 0.0782
Epoch 141/300, seasonal_1 Loss: 0.0697 | 0.0808
Epoch 142/300, seasonal_1 Loss: 0.0686 | 0.0815
Epoch 143/300, seasonal_1 Loss: 0.0689 | 0.0768
Epoch 144/300, seasonal_1 Loss: 0.0696 | 0.0780
Epoch 145/300, seasonal_1 Loss: 0.0689 | 0.0785
Epoch 146/300, seasonal_1 Loss: 0.0680 | 0.0749
Epoch 147/300, seasonal_1 Loss: 0.0680 | 0.0742
Epoch 148/300, seasonal_1 Loss: 0.0667 | 0.0780
Epoch 149/300, seasonal_1 Loss: 0.0668 | 0.0807
Epoch 150/300, seasonal_1 Loss: 0.0668 | 0.0759
Epoch 151/300, seasonal_1 Loss: 0.0660 | 0.0746
Epoch 152/300, seasonal_1 Loss: 0.0667 | 0.0831
Epoch 153/300, seasonal_1 Loss: 0.0655 | 0.0767
Epoch 154/300, seasonal_1 Loss: 0.0665 | 0.0745
Epoch 155/300, seasonal_1 Loss: 0.0651 | 0.0791
Epoch 156/300, seasonal_1 Loss: 0.0658 | 0.0773
Epoch 157/300, seasonal_1 Loss: 0.0655 | 0.0733
Epoch 158/300, seasonal_1 Loss: 0.0640 | 0.0763
Epoch 159/300, seasonal_1 Loss: 0.0643 | 0.0779
Epoch 160/300, seasonal_1 Loss: 0.0642 | 0.0764
Epoch 161/300, seasonal_1 Loss: 0.0654 | 0.0753
Epoch 162/300, seasonal_1 Loss: 0.0648 | 0.0784
Epoch 163/300, seasonal_1 Loss: 0.0634 | 0.0773
Epoch 164/300, seasonal_1 Loss: 0.0629 | 0.0747
Epoch 165/300, seasonal_1 Loss: 0.0632 | 0.0749
Epoch 166/300, seasonal_1 Loss: 0.0641 | 0.0789
Epoch 167/300, seasonal_1 Loss: 0.0644 | 0.0754
Epoch 168/300, seasonal_1 Loss: 0.0634 | 0.0753
Epoch 169/300, seasonal_1 Loss: 0.0639 | 0.0763
Epoch 170/300, seasonal_1 Loss: 0.0622 | 0.0737
Epoch 171/300, seasonal_1 Loss: 0.0624 | 0.0738
Epoch 172/300, seasonal_1 Loss: 0.0620 | 0.0778
Epoch 173/300, seasonal_1 Loss: 0.0617 | 0.0755
Epoch 174/300, seasonal_1 Loss: 0.0606 | 0.0757
Epoch 175/300, seasonal_1 Loss: 0.0608 | 0.0752
Epoch 176/300, seasonal_1 Loss: 0.0617 | 0.0757
Epoch 177/300, seasonal_1 Loss: 0.0607 | 0.0757
Epoch 178/300, seasonal_1 Loss: 0.0610 | 0.0717
Epoch 179/300, seasonal_1 Loss: 0.0607 | 0.0724
Epoch 180/300, seasonal_1 Loss: 0.0615 | 0.0755
Epoch 181/300, seasonal_1 Loss: 0.0610 | 0.0723
Epoch 182/300, seasonal_1 Loss: 0.0598 | 0.0733
Epoch 183/300, seasonal_1 Loss: 0.0599 | 0.0734
Epoch 184/300, seasonal_1 Loss: 0.0593 | 0.0746
Epoch 185/300, seasonal_1 Loss: 0.0586 | 0.0732
Epoch 186/300, seasonal_1 Loss: 0.0594 | 0.0760
Epoch 187/300, seasonal_1 Loss: 0.0591 | 0.0757
Epoch 188/300, seasonal_1 Loss: 0.0591 | 0.0737
Epoch 189/300, seasonal_1 Loss: 0.0584 | 0.0742
Epoch 190/300, seasonal_1 Loss: 0.0594 | 0.0747
Epoch 191/300, seasonal_1 Loss: 0.0582 | 0.0741
Epoch 192/300, seasonal_1 Loss: 0.0582 | 0.0718
Epoch 193/300, seasonal_1 Loss: 0.0582 | 0.0738
Epoch 194/300, seasonal_1 Loss: 0.0578 | 0.0713
Epoch 195/300, seasonal_1 Loss: 0.0582 | 0.0733
Epoch 196/300, seasonal_1 Loss: 0.0584 | 0.0731
Epoch 197/300, seasonal_1 Loss: 0.0574 | 0.0730
Epoch 198/300, seasonal_1 Loss: 0.0568 | 0.0734
Epoch 199/300, seasonal_1 Loss: 0.0575 | 0.0728
Epoch 200/300, seasonal_1 Loss: 0.0568 | 0.0718
Epoch 201/300, seasonal_1 Loss: 0.0565 | 0.0726
Epoch 202/300, seasonal_1 Loss: 0.0572 | 0.0721
Epoch 203/300, seasonal_1 Loss: 0.0571 | 0.0731
Epoch 204/300, seasonal_1 Loss: 0.0568 | 0.0725
Epoch 205/300, seasonal_1 Loss: 0.0573 | 0.0731
Epoch 206/300, seasonal_1 Loss: 0.0565 | 0.0739
Epoch 207/300, seasonal_1 Loss: 0.0563 | 0.0748
Epoch 208/300, seasonal_1 Loss: 0.0567 | 0.0719
Epoch 209/300, seasonal_1 Loss: 0.0553 | 0.0750
Epoch 210/300, seasonal_1 Loss: 0.0555 | 0.0729
Epoch 211/300, seasonal_1 Loss: 0.0558 | 0.0743
Epoch 212/300, seasonal_1 Loss: 0.0556 | 0.0738
Epoch 213/300, seasonal_1 Loss: 0.0553 | 0.0738
Epoch 214/300, seasonal_1 Loss: 0.0552 | 0.0725
Epoch 215/300, seasonal_1 Loss: 0.0549 | 0.0744
Epoch 216/300, seasonal_1 Loss: 0.0547 | 0.0719
Epoch 217/300, seasonal_1 Loss: 0.0549 | 0.0714
Epoch 218/300, seasonal_1 Loss: 0.0549 | 0.0724
Epoch 219/300, seasonal_1 Loss: 0.0555 | 0.0718
Epoch 220/300, seasonal_1 Loss: 0.0551 | 0.0718
Epoch 221/300, seasonal_1 Loss: 0.0546 | 0.0730
Epoch 222/300, seasonal_1 Loss: 0.0545 | 0.0716
Epoch 223/300, seasonal_1 Loss: 0.0539 | 0.0728
Epoch 224/300, seasonal_1 Loss: 0.0546 | 0.0720
Epoch 225/300, seasonal_1 Loss: 0.0541 | 0.0731
Epoch 226/300, seasonal_1 Loss: 0.0540 | 0.0719
Epoch 227/300, seasonal_1 Loss: 0.0535 | 0.0732
Epoch 228/300, seasonal_1 Loss: 0.0537 | 0.0732
Epoch 229/300, seasonal_1 Loss: 0.0529 | 0.0715
Epoch 230/300, seasonal_1 Loss: 0.0529 | 0.0728
Epoch 231/300, seasonal_1 Loss: 0.0528 | 0.0707
Epoch 232/300, seasonal_1 Loss: 0.0536 | 0.0732
Epoch 233/300, seasonal_1 Loss: 0.0537 | 0.0730
Epoch 234/300, seasonal_1 Loss: 0.0535 | 0.0724
Epoch 235/300, seasonal_1 Loss: 0.0532 | 0.0722
Epoch 236/300, seasonal_1 Loss: 0.0530 | 0.0726
Epoch 237/300, seasonal_1 Loss: 0.0526 | 0.0716
Epoch 238/300, seasonal_1 Loss: 0.0528 | 0.0722
Epoch 239/300, seasonal_1 Loss: 0.0525 | 0.0733
Epoch 240/300, seasonal_1 Loss: 0.0524 | 0.0720
Epoch 241/300, seasonal_1 Loss: 0.0520 | 0.0723
Epoch 242/300, seasonal_1 Loss: 0.0534 | 0.0733
Epoch 243/300, seasonal_1 Loss: 0.0529 | 0.0723
Epoch 244/300, seasonal_1 Loss: 0.0525 | 0.0720
Epoch 245/300, seasonal_1 Loss: 0.0521 | 0.0722
Epoch 246/300, seasonal_1 Loss: 0.0529 | 0.0717
Epoch 247/300, seasonal_1 Loss: 0.0522 | 0.0724
Epoch 248/300, seasonal_1 Loss: 0.0515 | 0.0718
Epoch 249/300, seasonal_1 Loss: 0.0520 | 0.0724
Epoch 250/300, seasonal_1 Loss: 0.0517 | 0.0734
Epoch 251/300, seasonal_1 Loss: 0.0513 | 0.0728
Epoch 252/300, seasonal_1 Loss: 0.0518 | 0.0726
Epoch 253/300, seasonal_1 Loss: 0.0515 | 0.0721
Epoch 254/300, seasonal_1 Loss: 0.0513 | 0.0719
Epoch 255/300, seasonal_1 Loss: 0.0514 | 0.0727
Epoch 256/300, seasonal_1 Loss: 0.0515 | 0.0720
Epoch 257/300, seasonal_1 Loss: 0.0511 | 0.0719
Epoch 258/300, seasonal_1 Loss: 0.0512 | 0.0724
Epoch 259/300, seasonal_1 Loss: 0.0513 | 0.0709
Epoch 260/300, seasonal_1 Loss: 0.0514 | 0.0714
Epoch 261/300, seasonal_1 Loss: 0.0510 | 0.0725
Epoch 262/300, seasonal_1 Loss: 0.0504 | 0.0721
Epoch 263/300, seasonal_1 Loss: 0.0512 | 0.0718
Epoch 264/300, seasonal_1 Loss: 0.0506 | 0.0716
Epoch 265/300, seasonal_1 Loss: 0.0505 | 0.0716
Epoch 266/300, seasonal_1 Loss: 0.0507 | 0.0720
Epoch 267/300, seasonal_1 Loss: 0.0506 | 0.0716
Epoch 268/300, seasonal_1 Loss: 0.0509 | 0.0722
Epoch 269/300, seasonal_1 Loss: 0.0507 | 0.0720
Epoch 270/300, seasonal_1 Loss: 0.0501 | 0.0722
Epoch 271/300, seasonal_1 Loss: 0.0507 | 0.0715
Epoch 272/300, seasonal_1 Loss: 0.0505 | 0.0718
Epoch 273/300, seasonal_1 Loss: 0.0504 | 0.0720
Epoch 274/300, seasonal_1 Loss: 0.0501 | 0.0715
Epoch 275/300, seasonal_1 Loss: 0.0503 | 0.0724
Epoch 276/300, seasonal_1 Loss: 0.0503 | 0.0718
Epoch 277/300, seasonal_1 Loss: 0.0507 | 0.0721
Epoch 278/300, seasonal_1 Loss: 0.0500 | 0.0715
Epoch 279/300, seasonal_1 Loss: 0.0497 | 0.0720
Epoch 280/300, seasonal_1 Loss: 0.0500 | 0.0713
Epoch 281/300, seasonal_1 Loss: 0.0497 | 0.0714
Epoch 282/300, seasonal_1 Loss: 0.0496 | 0.0709
Epoch 283/300, seasonal_1 Loss: 0.0497 | 0.0714
Epoch 284/300, seasonal_1 Loss: 0.0495 | 0.0719
Epoch 285/300, seasonal_1 Loss: 0.0496 | 0.0709
Epoch 286/300, seasonal_1 Loss: 0.0499 | 0.0717
Epoch 287/300, seasonal_1 Loss: 0.0492 | 0.0709
Epoch 288/300, seasonal_1 Loss: 0.0503 | 0.0705
Epoch 289/300, seasonal_1 Loss: 0.0494 | 0.0722
Epoch 290/300, seasonal_1 Loss: 0.0493 | 0.0705
Epoch 291/300, seasonal_1 Loss: 0.0498 | 0.0714
Epoch 292/300, seasonal_1 Loss: 0.0498 | 0.0711
Epoch 293/300, seasonal_1 Loss: 0.0495 | 0.0720
Epoch 294/300, seasonal_1 Loss: 0.0497 | 0.0714
Epoch 295/300, seasonal_1 Loss: 0.0488 | 0.0716
Epoch 296/300, seasonal_1 Loss: 0.0492 | 0.0712
Epoch 297/300, seasonal_1 Loss: 0.0491 | 0.0713
Epoch 298/300, seasonal_1 Loss: 0.0489 | 0.0717
Epoch 299/300, seasonal_1 Loss: 0.0492 | 0.0713
Epoch 300/300, seasonal_1 Loss: 0.0491 | 0.0722
Training seasonal_2 component with params: {'observation_period_num': 8, 'train_rates': 0.9865472169001089, 'learning_rate': 0.00011843480588275453, 'batch_size': 110, 'step_size': 2, 'gamma': 0.9412735701245838}
Epoch 1/300, seasonal_2 Loss: 1.0498 | 1.4720
Epoch 2/300, seasonal_2 Loss: 0.6201 | 0.7709
Epoch 3/300, seasonal_2 Loss: 0.4693 | 0.5780
Epoch 4/300, seasonal_2 Loss: 0.3999 | 0.4887
Epoch 5/300, seasonal_2 Loss: 0.3754 | 0.4625
Epoch 6/300, seasonal_2 Loss: 0.3454 | 0.3980
Epoch 7/300, seasonal_2 Loss: 0.3187 | 0.4010
Epoch 8/300, seasonal_2 Loss: 0.2969 | 0.3347
Epoch 9/300, seasonal_2 Loss: 0.2526 | 0.3102
Epoch 10/300, seasonal_2 Loss: 0.2607 | 0.3061
Epoch 11/300, seasonal_2 Loss: 0.2618 | 0.2841
Epoch 12/300, seasonal_2 Loss: 0.2626 | 0.2910
Epoch 13/300, seasonal_2 Loss: 0.2317 | 0.2746
Epoch 14/300, seasonal_2 Loss: 0.2199 | 0.2668
Epoch 15/300, seasonal_2 Loss: 0.2217 | 0.2527
Epoch 16/300, seasonal_2 Loss: 0.2063 | 0.2429
Epoch 17/300, seasonal_2 Loss: 0.2039 | 0.2371
Epoch 18/300, seasonal_2 Loss: 0.2077 | 0.2347
Epoch 19/300, seasonal_2 Loss: 0.1997 | 0.2314
Epoch 20/300, seasonal_2 Loss: 0.1894 | 0.2167
Epoch 21/300, seasonal_2 Loss: 0.1799 | 0.2180
Epoch 22/300, seasonal_2 Loss: 0.1717 | 0.2070
Epoch 23/300, seasonal_2 Loss: 0.1677 | 0.2062
Epoch 24/300, seasonal_2 Loss: 0.1640 | 0.2000
Epoch 25/300, seasonal_2 Loss: 0.1610 | 0.1972
Epoch 26/300, seasonal_2 Loss: 0.1591 | 0.1940
Epoch 27/300, seasonal_2 Loss: 0.1576 | 0.1911
Epoch 28/300, seasonal_2 Loss: 0.1552 | 0.1883
Epoch 29/300, seasonal_2 Loss: 0.1547 | 0.1860
Epoch 30/300, seasonal_2 Loss: 0.1529 | 0.1837
Epoch 31/300, seasonal_2 Loss: 0.1528 | 0.1816
Epoch 32/300, seasonal_2 Loss: 0.1513 | 0.1797
Epoch 33/300, seasonal_2 Loss: 0.1500 | 0.1782
Epoch 34/300, seasonal_2 Loss: 0.1493 | 0.1764
Epoch 35/300, seasonal_2 Loss: 0.1486 | 0.1749
Epoch 36/300, seasonal_2 Loss: 0.1480 | 0.1733
Epoch 37/300, seasonal_2 Loss: 0.1471 | 0.1722
Epoch 38/300, seasonal_2 Loss: 0.1469 | 0.1709
Epoch 39/300, seasonal_2 Loss: 0.1460 | 0.1699
Epoch 40/300, seasonal_2 Loss: 0.1452 | 0.1685
Epoch 41/300, seasonal_2 Loss: 0.1453 | 0.1674
Epoch 42/300, seasonal_2 Loss: 0.1447 | 0.1663
Epoch 43/300, seasonal_2 Loss: 0.1444 | 0.1655
Epoch 44/300, seasonal_2 Loss: 0.1439 | 0.1646
Epoch 45/300, seasonal_2 Loss: 0.1430 | 0.1637
Epoch 46/300, seasonal_2 Loss: 0.1431 | 0.1631
Epoch 47/300, seasonal_2 Loss: 0.1424 | 0.1623
Epoch 48/300, seasonal_2 Loss: 0.1420 | 0.1616
Epoch 49/300, seasonal_2 Loss: 0.1424 | 0.1610
Epoch 50/300, seasonal_2 Loss: 0.1415 | 0.1604
Epoch 51/300, seasonal_2 Loss: 0.1414 | 0.1596
Epoch 52/300, seasonal_2 Loss: 0.1410 | 0.1589
Epoch 53/300, seasonal_2 Loss: 0.1413 | 0.1585
Epoch 54/300, seasonal_2 Loss: 0.1404 | 0.1582
Epoch 55/300, seasonal_2 Loss: 0.1401 | 0.1577
Epoch 56/300, seasonal_2 Loss: 0.1396 | 0.1572
Epoch 57/300, seasonal_2 Loss: 0.1396 | 0.1568
Epoch 58/300, seasonal_2 Loss: 0.1397 | 0.1564
Epoch 59/300, seasonal_2 Loss: 0.1398 | 0.1561
Epoch 60/300, seasonal_2 Loss: 0.1397 | 0.1558
Epoch 61/300, seasonal_2 Loss: 0.1388 | 0.1555
Epoch 62/300, seasonal_2 Loss: 0.1385 | 0.1551
Epoch 63/300, seasonal_2 Loss: 0.1387 | 0.1548
Epoch 64/300, seasonal_2 Loss: 0.1387 | 0.1545
Epoch 65/300, seasonal_2 Loss: 0.1386 | 0.1543
Epoch 66/300, seasonal_2 Loss: 0.1387 | 0.1541
Epoch 67/300, seasonal_2 Loss: 0.1383 | 0.1539
Epoch 68/300, seasonal_2 Loss: 0.1380 | 0.1537
Epoch 69/300, seasonal_2 Loss: 0.1382 | 0.1534
Epoch 70/300, seasonal_2 Loss: 0.1381 | 0.1532
Epoch 71/300, seasonal_2 Loss: 0.1378 | 0.1530
Epoch 72/300, seasonal_2 Loss: 0.1375 | 0.1528
Epoch 73/300, seasonal_2 Loss: 0.1381 | 0.1526
Epoch 74/300, seasonal_2 Loss: 0.1375 | 0.1524
Epoch 75/300, seasonal_2 Loss: 0.1370 | 0.1522
Epoch 76/300, seasonal_2 Loss: 0.1377 | 0.1521
Epoch 77/300, seasonal_2 Loss: 0.1372 | 0.1519
Epoch 78/300, seasonal_2 Loss: 0.1373 | 0.1518
Epoch 79/300, seasonal_2 Loss: 0.1372 | 0.1517
Epoch 80/300, seasonal_2 Loss: 0.1373 | 0.1516
Epoch 81/300, seasonal_2 Loss: 0.1372 | 0.1515
Epoch 82/300, seasonal_2 Loss: 0.1376 | 0.1513
Epoch 83/300, seasonal_2 Loss: 0.1368 | 0.1512
Epoch 84/300, seasonal_2 Loss: 0.1374 | 0.1512
Epoch 85/300, seasonal_2 Loss: 0.1368 | 0.1511
Epoch 86/300, seasonal_2 Loss: 0.1365 | 0.1510
Epoch 87/300, seasonal_2 Loss: 0.1366 | 0.1509
Epoch 88/300, seasonal_2 Loss: 0.1371 | 0.1509
Epoch 89/300, seasonal_2 Loss: 0.1369 | 0.1508
Epoch 90/300, seasonal_2 Loss: 0.1370 | 0.1507
Epoch 91/300, seasonal_2 Loss: 0.1365 | 0.1507
Epoch 92/300, seasonal_2 Loss: 0.1365 | 0.1506
Epoch 93/300, seasonal_2 Loss: 0.1366 | 0.1505
Epoch 94/300, seasonal_2 Loss: 0.1363 | 0.1505
Epoch 95/300, seasonal_2 Loss: 0.1366 | 0.1504
Epoch 96/300, seasonal_2 Loss: 0.1360 | 0.1503
Epoch 97/300, seasonal_2 Loss: 0.1363 | 0.1503
Epoch 98/300, seasonal_2 Loss: 0.1364 | 0.1503
Epoch 99/300, seasonal_2 Loss: 0.1362 | 0.1502
Epoch 100/300, seasonal_2 Loss: 0.1367 | 0.1502
Epoch 101/300, seasonal_2 Loss: 0.1360 | 0.1502
Epoch 102/300, seasonal_2 Loss: 0.1367 | 0.1501
Epoch 103/300, seasonal_2 Loss: 0.1361 | 0.1501
Epoch 104/300, seasonal_2 Loss: 0.1366 | 0.1501
Epoch 105/300, seasonal_2 Loss: 0.1361 | 0.1500
Epoch 106/300, seasonal_2 Loss: 0.1362 | 0.1500
Epoch 107/300, seasonal_2 Loss: 0.1368 | 0.1500
Epoch 108/300, seasonal_2 Loss: 0.1366 | 0.1499
Epoch 109/300, seasonal_2 Loss: 0.1360 | 0.1499
Epoch 110/300, seasonal_2 Loss: 0.1365 | 0.1499
Epoch 111/300, seasonal_2 Loss: 0.1358 | 0.1499
Epoch 112/300, seasonal_2 Loss: 0.1364 | 0.1498
Epoch 113/300, seasonal_2 Loss: 0.1364 | 0.1498
Epoch 114/300, seasonal_2 Loss: 0.1363 | 0.1498
Epoch 115/300, seasonal_2 Loss: 0.1361 | 0.1498
Epoch 116/300, seasonal_2 Loss: 0.1367 | 0.1498
Epoch 117/300, seasonal_2 Loss: 0.1363 | 0.1498
Epoch 118/300, seasonal_2 Loss: 0.1369 | 0.1498
Epoch 119/300, seasonal_2 Loss: 0.1367 | 0.1497
Epoch 120/300, seasonal_2 Loss: 0.1364 | 0.1497
Epoch 121/300, seasonal_2 Loss: 0.1362 | 0.1497
Epoch 122/300, seasonal_2 Loss: 0.1354 | 0.1497
Epoch 123/300, seasonal_2 Loss: 0.1355 | 0.1497
Epoch 124/300, seasonal_2 Loss: 0.1360 | 0.1497
Epoch 125/300, seasonal_2 Loss: 0.1363 | 0.1497
Epoch 126/300, seasonal_2 Loss: 0.1363 | 0.1497
Epoch 127/300, seasonal_2 Loss: 0.1358 | 0.1497
Epoch 128/300, seasonal_2 Loss: 0.1361 | 0.1496
Epoch 129/300, seasonal_2 Loss: 0.1360 | 0.1496
Epoch 130/300, seasonal_2 Loss: 0.1364 | 0.1496
Epoch 131/300, seasonal_2 Loss: 0.1360 | 0.1496
Epoch 132/300, seasonal_2 Loss: 0.1364 | 0.1496
Epoch 133/300, seasonal_2 Loss: 0.1367 | 0.1496
Epoch 134/300, seasonal_2 Loss: 0.1360 | 0.1496
Epoch 135/300, seasonal_2 Loss: 0.1360 | 0.1496
Epoch 136/300, seasonal_2 Loss: 0.1361 | 0.1496
Epoch 137/300, seasonal_2 Loss: 0.1361 | 0.1496
Epoch 138/300, seasonal_2 Loss: 0.1367 | 0.1496
Epoch 139/300, seasonal_2 Loss: 0.1359 | 0.1496
Epoch 140/300, seasonal_2 Loss: 0.1361 | 0.1496
Epoch 141/300, seasonal_2 Loss: 0.1358 | 0.1496
Epoch 142/300, seasonal_2 Loss: 0.1359 | 0.1496
Epoch 143/300, seasonal_2 Loss: 0.1366 | 0.1496
Epoch 144/300, seasonal_2 Loss: 0.1354 | 0.1496
Epoch 145/300, seasonal_2 Loss: 0.1362 | 0.1496
Epoch 146/300, seasonal_2 Loss: 0.1360 | 0.1496
Epoch 147/300, seasonal_2 Loss: 0.1364 | 0.1496
Epoch 148/300, seasonal_2 Loss: 0.1370 | 0.1495
Epoch 149/300, seasonal_2 Loss: 0.1357 | 0.1495
Epoch 150/300, seasonal_2 Loss: 0.1365 | 0.1495
Epoch 151/300, seasonal_2 Loss: 0.1362 | 0.1495
Epoch 152/300, seasonal_2 Loss: 0.1366 | 0.1495
Epoch 153/300, seasonal_2 Loss: 0.1363 | 0.1495
Epoch 154/300, seasonal_2 Loss: 0.1362 | 0.1495
Epoch 155/300, seasonal_2 Loss: 0.1359 | 0.1495
Epoch 156/300, seasonal_2 Loss: 0.1355 | 0.1495
Epoch 157/300, seasonal_2 Loss: 0.1363 | 0.1495
Epoch 158/300, seasonal_2 Loss: 0.1364 | 0.1495
Epoch 159/300, seasonal_2 Loss: 0.1362 | 0.1495
Epoch 160/300, seasonal_2 Loss: 0.1360 | 0.1495
Epoch 161/300, seasonal_2 Loss: 0.1358 | 0.1495
Epoch 162/300, seasonal_2 Loss: 0.1367 | 0.1495
Epoch 163/300, seasonal_2 Loss: 0.1362 | 0.1495
Epoch 164/300, seasonal_2 Loss: 0.1359 | 0.1495
Epoch 165/300, seasonal_2 Loss: 0.1361 | 0.1495
Epoch 166/300, seasonal_2 Loss: 0.1361 | 0.1495
Epoch 167/300, seasonal_2 Loss: 0.1364 | 0.1495
Epoch 168/300, seasonal_2 Loss: 0.1362 | 0.1495
Epoch 169/300, seasonal_2 Loss: 0.1364 | 0.1495
Epoch 170/300, seasonal_2 Loss: 0.1354 | 0.1495
Epoch 171/300, seasonal_2 Loss: 0.1366 | 0.1495
Epoch 172/300, seasonal_2 Loss: 0.1359 | 0.1495
Epoch 173/300, seasonal_2 Loss: 0.1362 | 0.1495
Epoch 174/300, seasonal_2 Loss: 0.1356 | 0.1495
Epoch 175/300, seasonal_2 Loss: 0.1356 | 0.1495
Epoch 176/300, seasonal_2 Loss: 0.1360 | 0.1495
Epoch 177/300, seasonal_2 Loss: 0.1358 | 0.1495
Epoch 178/300, seasonal_2 Loss: 0.1364 | 0.1495
Epoch 179/300, seasonal_2 Loss: 0.1357 | 0.1495
Epoch 180/300, seasonal_2 Loss: 0.1360 | 0.1495
Epoch 181/300, seasonal_2 Loss: 0.1357 | 0.1495
Epoch 182/300, seasonal_2 Loss: 0.1367 | 0.1495
Epoch 183/300, seasonal_2 Loss: 0.1366 | 0.1495
Epoch 184/300, seasonal_2 Loss: 0.1361 | 0.1495
Epoch 185/300, seasonal_2 Loss: 0.1359 | 0.1495
Epoch 186/300, seasonal_2 Loss: 0.1360 | 0.1495
Epoch 187/300, seasonal_2 Loss: 0.1365 | 0.1495
Epoch 188/300, seasonal_2 Loss: 0.1357 | 0.1495
Epoch 189/300, seasonal_2 Loss: 0.1357 | 0.1495
Epoch 190/300, seasonal_2 Loss: 0.1361 | 0.1495
Epoch 191/300, seasonal_2 Loss: 0.1358 | 0.1495
Epoch 192/300, seasonal_2 Loss: 0.1356 | 0.1495
Epoch 193/300, seasonal_2 Loss: 0.1364 | 0.1495
Epoch 194/300, seasonal_2 Loss: 0.1366 | 0.1495
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 146, 'train_rates': 0.9825693926500034, 'learning_rate': 9.475902227791612e-05, 'batch_size': 152, 'step_size': 8, 'gamma': 0.9725671933044144}
Epoch 1/300, seasonal_3 Loss: 1.1682 | 1.6067
Epoch 2/300, seasonal_3 Loss: 0.6668 | 0.7921
Epoch 3/300, seasonal_3 Loss: 0.5136 | 0.6185
Epoch 4/300, seasonal_3 Loss: 0.4413 | 0.5320
Epoch 5/300, seasonal_3 Loss: 0.4099 | 0.4662
Epoch 6/300, seasonal_3 Loss: 0.3772 | 0.4319
Epoch 7/300, seasonal_3 Loss: 0.3489 | 0.3811
Epoch 8/300, seasonal_3 Loss: 0.3526 | 0.3566
Epoch 9/300, seasonal_3 Loss: 0.3484 | 0.3565
Epoch 10/300, seasonal_3 Loss: 0.3136 | 0.3271
Epoch 11/300, seasonal_3 Loss: 0.2896 | 0.2696
Epoch 12/300, seasonal_3 Loss: 0.2996 | 0.2949
Epoch 13/300, seasonal_3 Loss: 0.2941 | 0.2513
Epoch 14/300, seasonal_3 Loss: 0.2621 | 0.2448
Epoch 15/300, seasonal_3 Loss: 0.2875 | 0.2342
Epoch 16/300, seasonal_3 Loss: 0.2684 | 0.2386
Epoch 17/300, seasonal_3 Loss: 0.2763 | 0.2686
Epoch 18/300, seasonal_3 Loss: 0.2626 | 0.2340
Epoch 19/300, seasonal_3 Loss: 0.2364 | 0.2139
Epoch 20/300, seasonal_3 Loss: 0.2187 | 0.2041
Epoch 21/300, seasonal_3 Loss: 0.2127 | 0.1944
Epoch 22/300, seasonal_3 Loss: 0.2122 | 0.2003
Epoch 23/300, seasonal_3 Loss: 0.2013 | 0.1820
Epoch 24/300, seasonal_3 Loss: 0.1978 | 0.1811
Epoch 25/300, seasonal_3 Loss: 0.1934 | 0.1740
Epoch 26/300, seasonal_3 Loss: 0.1936 | 0.1726
Epoch 27/300, seasonal_3 Loss: 0.1899 | 0.1639
Epoch 28/300, seasonal_3 Loss: 0.1971 | 0.1683
Epoch 29/300, seasonal_3 Loss: 0.2219 | 0.1806
Epoch 30/300, seasonal_3 Loss: 0.2190 | 0.1815
Epoch 31/300, seasonal_3 Loss: 0.2033 | 0.1528
Epoch 32/300, seasonal_3 Loss: 0.1860 | 0.1606
Epoch 33/300, seasonal_3 Loss: 0.1784 | 0.1543
Epoch 34/300, seasonal_3 Loss: 0.1832 | 0.1560
Epoch 35/300, seasonal_3 Loss: 0.1833 | 0.1513
Epoch 36/300, seasonal_3 Loss: 0.1981 | 0.1506
Epoch 37/300, seasonal_3 Loss: 0.1858 | 0.1500
Epoch 38/300, seasonal_3 Loss: 0.1945 | 0.1519
Epoch 39/300, seasonal_3 Loss: 0.1842 | 0.1526
Epoch 40/300, seasonal_3 Loss: 0.1836 | 0.1481
Epoch 41/300, seasonal_3 Loss: 0.1971 | 0.1470
Epoch 42/300, seasonal_3 Loss: 0.1871 | 0.1483
Epoch 43/300, seasonal_3 Loss: 0.1845 | 0.1512
Epoch 44/300, seasonal_3 Loss: 0.1754 | 0.1488
Epoch 45/300, seasonal_3 Loss: 0.1668 | 0.1510
Epoch 46/300, seasonal_3 Loss: 0.1644 | 0.1410
Epoch 47/300, seasonal_3 Loss: 0.1627 | 0.1403
Epoch 48/300, seasonal_3 Loss: 0.1661 | 0.1430
Epoch 49/300, seasonal_3 Loss: 0.1651 | 0.1388
Epoch 50/300, seasonal_3 Loss: 0.1622 | 0.1413
Epoch 51/300, seasonal_3 Loss: 0.1578 | 0.1329
Epoch 52/300, seasonal_3 Loss: 0.1554 | 0.1358
Epoch 53/300, seasonal_3 Loss: 0.1563 | 0.1304
Epoch 54/300, seasonal_3 Loss: 0.1566 | 0.1366
Epoch 55/300, seasonal_3 Loss: 0.1584 | 0.1295
Epoch 56/300, seasonal_3 Loss: 0.1588 | 0.1361
Epoch 57/300, seasonal_3 Loss: 0.1632 | 0.1281
Epoch 58/300, seasonal_3 Loss: 0.1721 | 0.1429
Epoch 59/300, seasonal_3 Loss: 0.1793 | 0.1340
Epoch 60/300, seasonal_3 Loss: 0.2016 | 0.1466
Epoch 61/300, seasonal_3 Loss: 0.1867 | 0.1342
Epoch 62/300, seasonal_3 Loss: 0.1828 | 0.1372
Epoch 63/300, seasonal_3 Loss: 0.1635 | 0.1300
Epoch 64/300, seasonal_3 Loss: 0.1525 | 0.1282
Epoch 65/300, seasonal_3 Loss: 0.1481 | 0.1292
Epoch 66/300, seasonal_3 Loss: 0.1453 | 0.1230
Epoch 67/300, seasonal_3 Loss: 0.1452 | 0.1274
Epoch 68/300, seasonal_3 Loss: 0.1441 | 0.1194
Epoch 69/300, seasonal_3 Loss: 0.1445 | 0.1258
Epoch 70/300, seasonal_3 Loss: 0.1445 | 0.1192
Epoch 71/300, seasonal_3 Loss: 0.1462 | 0.1243
Epoch 72/300, seasonal_3 Loss: 0.1485 | 0.1183
Epoch 73/300, seasonal_3 Loss: 0.1520 | 0.1239
Epoch 74/300, seasonal_3 Loss: 0.1533 | 0.1199
Epoch 75/300, seasonal_3 Loss: 0.1565 | 0.1257
Epoch 76/300, seasonal_3 Loss: 0.1582 | 0.1202
Epoch 77/300, seasonal_3 Loss: 0.1592 | 0.1311
Epoch 78/300, seasonal_3 Loss: 0.1539 | 0.1226
Epoch 79/300, seasonal_3 Loss: 0.1501 | 0.1281
Epoch 80/300, seasonal_3 Loss: 0.1461 | 0.1207
Epoch 81/300, seasonal_3 Loss: 0.1441 | 0.1213
Epoch 82/300, seasonal_3 Loss: 0.1429 | 0.1186
Epoch 83/300, seasonal_3 Loss: 0.1438 | 0.1238
Epoch 84/300, seasonal_3 Loss: 0.1443 | 0.1188
Epoch 85/300, seasonal_3 Loss: 0.1439 | 0.1216
Epoch 86/300, seasonal_3 Loss: 0.1438 | 0.1175
Epoch 87/300, seasonal_3 Loss: 0.1408 | 0.1172
Epoch 88/300, seasonal_3 Loss: 0.1398 | 0.1159
Epoch 89/300, seasonal_3 Loss: 0.1380 | 0.1158
Epoch 90/300, seasonal_3 Loss: 0.1376 | 0.1156
Epoch 91/300, seasonal_3 Loss: 0.1365 | 0.1150
Epoch 92/300, seasonal_3 Loss: 0.1359 | 0.1145
Epoch 93/300, seasonal_3 Loss: 0.1360 | 0.1145
Epoch 94/300, seasonal_3 Loss: 0.1360 | 0.1139
Epoch 95/300, seasonal_3 Loss: 0.1368 | 0.1132
Epoch 96/300, seasonal_3 Loss: 0.1357 | 0.1123
Epoch 97/300, seasonal_3 Loss: 0.1348 | 0.1126
Epoch 98/300, seasonal_3 Loss: 0.1322 | 0.1111
Epoch 99/300, seasonal_3 Loss: 0.1309 | 0.1104
Epoch 100/300, seasonal_3 Loss: 0.1309 | 0.1096
Epoch 101/300, seasonal_3 Loss: 0.1312 | 0.1081
Epoch 102/300, seasonal_3 Loss: 0.1311 | 0.1104
Epoch 103/300, seasonal_3 Loss: 0.1310 | 0.1073
Epoch 104/300, seasonal_3 Loss: 0.1304 | 0.1092
Epoch 105/300, seasonal_3 Loss: 0.1290 | 0.1063
Epoch 106/300, seasonal_3 Loss: 0.1293 | 0.1063
Epoch 107/300, seasonal_3 Loss: 0.1314 | 0.1066
Epoch 108/300, seasonal_3 Loss: 0.1320 | 0.1049
Epoch 109/300, seasonal_3 Loss: 0.1337 | 0.1096
Epoch 110/300, seasonal_3 Loss: 0.1308 | 0.1027
Epoch 111/300, seasonal_3 Loss: 0.1329 | 0.1102
Epoch 112/300, seasonal_3 Loss: 0.1373 | 0.1055
Epoch 113/300, seasonal_3 Loss: 0.1432 | 0.1091
Epoch 114/300, seasonal_3 Loss: 0.1510 | 0.1076
Epoch 115/300, seasonal_3 Loss: 0.1424 | 0.1060
Epoch 116/300, seasonal_3 Loss: 0.1377 | 0.1030
Epoch 117/300, seasonal_3 Loss: 0.1317 | 0.1063
Epoch 118/300, seasonal_3 Loss: 0.1312 | 0.1002
Epoch 119/300, seasonal_3 Loss: 0.1362 | 0.1085
Epoch 120/300, seasonal_3 Loss: 0.1365 | 0.1014
Epoch 121/300, seasonal_3 Loss: 0.1413 | 0.1090
Epoch 122/300, seasonal_3 Loss: 0.1367 | 0.1035
Epoch 123/300, seasonal_3 Loss: 0.1355 | 0.1088
Epoch 124/300, seasonal_3 Loss: 0.1316 | 0.1022
Epoch 125/300, seasonal_3 Loss: 0.1286 | 0.1073
Epoch 126/300, seasonal_3 Loss: 0.1271 | 0.1000
Epoch 127/300, seasonal_3 Loss: 0.1253 | 0.1048
Epoch 128/300, seasonal_3 Loss: 0.1246 | 0.1007
Epoch 129/300, seasonal_3 Loss: 0.1237 | 0.1029
Epoch 130/300, seasonal_3 Loss: 0.1228 | 0.1003
Epoch 131/300, seasonal_3 Loss: 0.1218 | 0.1022
Epoch 132/300, seasonal_3 Loss: 0.1215 | 0.0990
Epoch 133/300, seasonal_3 Loss: 0.1220 | 0.1001
Epoch 134/300, seasonal_3 Loss: 0.1215 | 0.0996
Epoch 135/300, seasonal_3 Loss: 0.1216 | 0.1005
Epoch 136/300, seasonal_3 Loss: 0.1227 | 0.0998
Epoch 137/300, seasonal_3 Loss: 0.1224 | 0.0989
Epoch 138/300, seasonal_3 Loss: 0.1213 | 0.0987
Epoch 139/300, seasonal_3 Loss: 0.1200 | 0.0974
Epoch 140/300, seasonal_3 Loss: 0.1188 | 0.0975
Epoch 141/300, seasonal_3 Loss: 0.1178 | 0.0957
Epoch 142/300, seasonal_3 Loss: 0.1166 | 0.0971
Epoch 143/300, seasonal_3 Loss: 0.1160 | 0.0962
Epoch 144/300, seasonal_3 Loss: 0.1157 | 0.0962
Epoch 145/300, seasonal_3 Loss: 0.1151 | 0.0952
Epoch 146/300, seasonal_3 Loss: 0.1150 | 0.0964
Epoch 147/300, seasonal_3 Loss: 0.1156 | 0.0962
Epoch 148/300, seasonal_3 Loss: 0.1168 | 0.0965
Epoch 149/300, seasonal_3 Loss: 0.1191 | 0.0962
Epoch 150/300, seasonal_3 Loss: 0.1211 | 0.0987
Epoch 151/300, seasonal_3 Loss: 0.1230 | 0.0974
Epoch 152/300, seasonal_3 Loss: 0.1221 | 0.0991
Epoch 153/300, seasonal_3 Loss: 0.1202 | 0.0956
Epoch 154/300, seasonal_3 Loss: 0.1172 | 0.0958
Epoch 155/300, seasonal_3 Loss: 0.1149 | 0.0942
Epoch 156/300, seasonal_3 Loss: 0.1140 | 0.0932
Epoch 157/300, seasonal_3 Loss: 0.1135 | 0.0943
Epoch 158/300, seasonal_3 Loss: 0.1128 | 0.0930
Epoch 159/300, seasonal_3 Loss: 0.1135 | 0.0942
Epoch 160/300, seasonal_3 Loss: 0.1142 | 0.0922
Epoch 161/300, seasonal_3 Loss: 0.1141 | 0.0935
Epoch 162/300, seasonal_3 Loss: 0.1132 | 0.0921
Epoch 163/300, seasonal_3 Loss: 0.1127 | 0.0924
Epoch 164/300, seasonal_3 Loss: 0.1117 | 0.0916
Epoch 165/300, seasonal_3 Loss: 0.1125 | 0.0919
Epoch 166/300, seasonal_3 Loss: 0.1116 | 0.0917
Epoch 167/300, seasonal_3 Loss: 0.1112 | 0.0915
Epoch 168/300, seasonal_3 Loss: 0.1121 | 0.0921
Epoch 169/300, seasonal_3 Loss: 0.1121 | 0.0918
Epoch 170/300, seasonal_3 Loss: 0.1131 | 0.0919
Epoch 171/300, seasonal_3 Loss: 0.1132 | 0.0922
Epoch 172/300, seasonal_3 Loss: 0.1134 | 0.0912
Epoch 173/300, seasonal_3 Loss: 0.1129 | 0.0932
Epoch 174/300, seasonal_3 Loss: 0.1127 | 0.0905
Epoch 175/300, seasonal_3 Loss: 0.1118 | 0.0926
Epoch 176/300, seasonal_3 Loss: 0.1109 | 0.0906
Epoch 177/300, seasonal_3 Loss: 0.1102 | 0.0913
Epoch 178/300, seasonal_3 Loss: 0.1087 | 0.0902
Epoch 179/300, seasonal_3 Loss: 0.1086 | 0.0906
Epoch 180/300, seasonal_3 Loss: 0.1080 | 0.0901
Epoch 181/300, seasonal_3 Loss: 0.1082 | 0.0907
Epoch 182/300, seasonal_3 Loss: 0.1084 | 0.0902
Epoch 183/300, seasonal_3 Loss: 0.1084 | 0.0906
Epoch 184/300, seasonal_3 Loss: 0.1087 | 0.0905
Epoch 185/300, seasonal_3 Loss: 0.1088 | 0.0890
Epoch 186/300, seasonal_3 Loss: 0.1083 | 0.0901
Epoch 187/300, seasonal_3 Loss: 0.1088 | 0.0887
Epoch 188/300, seasonal_3 Loss: 0.1071 | 0.0891
Epoch 189/300, seasonal_3 Loss: 0.1073 | 0.0893
Epoch 190/300, seasonal_3 Loss: 0.1066 | 0.0890
Epoch 191/300, seasonal_3 Loss: 0.1065 | 0.0901
Epoch 192/300, seasonal_3 Loss: 0.1069 | 0.0884
Epoch 193/300, seasonal_3 Loss: 0.1062 | 0.0887
Epoch 194/300, seasonal_3 Loss: 0.1053 | 0.0884
Epoch 195/300, seasonal_3 Loss: 0.1051 | 0.0894
Epoch 196/300, seasonal_3 Loss: 0.1050 | 0.0891
Epoch 197/300, seasonal_3 Loss: 0.1052 | 0.0876
Epoch 198/300, seasonal_3 Loss: 0.1049 | 0.0888
Epoch 199/300, seasonal_3 Loss: 0.1056 | 0.0877
Epoch 200/300, seasonal_3 Loss: 0.1046 | 0.0894
Epoch 201/300, seasonal_3 Loss: 0.1050 | 0.0874
Epoch 202/300, seasonal_3 Loss: 0.1048 | 0.0893
Epoch 203/300, seasonal_3 Loss: 0.1049 | 0.0890
Epoch 204/300, seasonal_3 Loss: 0.1069 | 0.0908
Epoch 205/300, seasonal_3 Loss: 0.1090 | 0.0907
Epoch 206/300, seasonal_3 Loss: 0.1104 | 0.0914
Epoch 207/300, seasonal_3 Loss: 0.1098 | 0.0909
Epoch 208/300, seasonal_3 Loss: 0.1078 | 0.0894
Epoch 209/300, seasonal_3 Loss: 0.1078 | 0.0886
Epoch 210/300, seasonal_3 Loss: 0.1061 | 0.0885
Epoch 211/300, seasonal_3 Loss: 0.1056 | 0.0893
Epoch 212/300, seasonal_3 Loss: 0.1061 | 0.0876
Epoch 213/300, seasonal_3 Loss: 0.1043 | 0.0892
Epoch 214/300, seasonal_3 Loss: 0.1034 | 0.0874
Epoch 215/300, seasonal_3 Loss: 0.1026 | 0.0878
Epoch 216/300, seasonal_3 Loss: 0.1020 | 0.0874
Epoch 217/300, seasonal_3 Loss: 0.1015 | 0.0882
Epoch 218/300, seasonal_3 Loss: 0.1019 | 0.0874
Epoch 219/300, seasonal_3 Loss: 0.1012 | 0.0879
Epoch 220/300, seasonal_3 Loss: 0.1007 | 0.0866
Epoch 221/300, seasonal_3 Loss: 0.1007 | 0.0868
Epoch 222/300, seasonal_3 Loss: 0.1012 | 0.0869
Epoch 223/300, seasonal_3 Loss: 0.1009 | 0.0870
Epoch 224/300, seasonal_3 Loss: 0.1013 | 0.0874
Epoch 225/300, seasonal_3 Loss: 0.1006 | 0.0870
Epoch 226/300, seasonal_3 Loss: 0.1007 | 0.0864
Epoch 227/300, seasonal_3 Loss: 0.0999 | 0.0868
Epoch 228/300, seasonal_3 Loss: 0.0999 | 0.0869
Epoch 229/300, seasonal_3 Loss: 0.0993 | 0.0852
Epoch 230/300, seasonal_3 Loss: 0.0985 | 0.0856
Epoch 231/300, seasonal_3 Loss: 0.0987 | 0.0858
Epoch 232/300, seasonal_3 Loss: 0.0984 | 0.0853
Epoch 233/300, seasonal_3 Loss: 0.0993 | 0.0858
Epoch 234/300, seasonal_3 Loss: 0.0986 | 0.0861
Epoch 235/300, seasonal_3 Loss: 0.0988 | 0.0864
Epoch 236/300, seasonal_3 Loss: 0.0985 | 0.0847
Epoch 237/300, seasonal_3 Loss: 0.0987 | 0.0851
Epoch 238/300, seasonal_3 Loss: 0.0977 | 0.0849
Epoch 239/300, seasonal_3 Loss: 0.0987 | 0.0856
Epoch 240/300, seasonal_3 Loss: 0.1000 | 0.0853
Epoch 241/300, seasonal_3 Loss: 0.1017 | 0.0870
Epoch 242/300, seasonal_3 Loss: 0.1030 | 0.0856
Epoch 243/300, seasonal_3 Loss: 0.1047 | 0.0888
Epoch 244/300, seasonal_3 Loss: 0.1043 | 0.0865
Epoch 245/300, seasonal_3 Loss: 0.1038 | 0.0887
Epoch 246/300, seasonal_3 Loss: 0.1015 | 0.0859
Epoch 247/300, seasonal_3 Loss: 0.0989 | 0.0868
Epoch 248/300, seasonal_3 Loss: 0.0984 | 0.0856
Epoch 249/300, seasonal_3 Loss: 0.0963 | 0.0868
Epoch 250/300, seasonal_3 Loss: 0.0973 | 0.0840
Epoch 251/300, seasonal_3 Loss: 0.0960 | 0.0870
Epoch 252/300, seasonal_3 Loss: 0.0962 | 0.0849
Epoch 253/300, seasonal_3 Loss: 0.0954 | 0.0869
Epoch 254/300, seasonal_3 Loss: 0.0961 | 0.0845
Epoch 255/300, seasonal_3 Loss: 0.0952 | 0.0867
Epoch 256/300, seasonal_3 Loss: 0.0959 | 0.0839
Epoch 257/300, seasonal_3 Loss: 0.0951 | 0.0856
Epoch 258/300, seasonal_3 Loss: 0.0947 | 0.0838
Epoch 259/300, seasonal_3 Loss: 0.0956 | 0.0872
Epoch 260/300, seasonal_3 Loss: 0.0959 | 0.0837
Epoch 261/300, seasonal_3 Loss: 0.0953 | 0.0879
Epoch 262/300, seasonal_3 Loss: 0.0947 | 0.0842
Epoch 263/300, seasonal_3 Loss: 0.0947 | 0.0862
Epoch 264/300, seasonal_3 Loss: 0.0946 | 0.0845
Epoch 265/300, seasonal_3 Loss: 0.0937 | 0.0853
Epoch 266/300, seasonal_3 Loss: 0.0937 | 0.0846
Epoch 267/300, seasonal_3 Loss: 0.0933 | 0.0846
Epoch 268/300, seasonal_3 Loss: 0.0933 | 0.0846
Epoch 269/300, seasonal_3 Loss: 0.0928 | 0.0858
Epoch 270/300, seasonal_3 Loss: 0.0934 | 0.0849
Epoch 271/300, seasonal_3 Loss: 0.0924 | 0.0852
Epoch 272/300, seasonal_3 Loss: 0.0925 | 0.0845
Epoch 273/300, seasonal_3 Loss: 0.0922 | 0.0855
Epoch 274/300, seasonal_3 Loss: 0.0930 | 0.0834
Epoch 275/300, seasonal_3 Loss: 0.0926 | 0.0844
Epoch 276/300, seasonal_3 Loss: 0.0922 | 0.0834
Epoch 277/300, seasonal_3 Loss: 0.0918 | 0.0858
Epoch 278/300, seasonal_3 Loss: 0.0927 | 0.0835
Epoch 279/300, seasonal_3 Loss: 0.0917 | 0.0841
Epoch 280/300, seasonal_3 Loss: 0.0914 | 0.0833
Epoch 281/300, seasonal_3 Loss: 0.0911 | 0.0861
Epoch 282/300, seasonal_3 Loss: 0.0912 | 0.0843
Epoch 283/300, seasonal_3 Loss: 0.0918 | 0.0855
Epoch 284/300, seasonal_3 Loss: 0.0911 | 0.0832
Epoch 285/300, seasonal_3 Loss: 0.0913 | 0.0860
Epoch 286/300, seasonal_3 Loss: 0.0907 | 0.0836
Epoch 287/300, seasonal_3 Loss: 0.0922 | 0.0849
Epoch 288/300, seasonal_3 Loss: 0.0913 | 0.0824
Epoch 289/300, seasonal_3 Loss: 0.0913 | 0.0860
Epoch 290/300, seasonal_3 Loss: 0.0913 | 0.0831
Epoch 291/300, seasonal_3 Loss: 0.0920 | 0.0871
Epoch 292/300, seasonal_3 Loss: 0.0920 | 0.0825
Epoch 293/300, seasonal_3 Loss: 0.0929 | 0.0858
Epoch 294/300, seasonal_3 Loss: 0.0923 | 0.0823
Epoch 295/300, seasonal_3 Loss: 0.0926 | 0.0879
Epoch 296/300, seasonal_3 Loss: 0.0908 | 0.0833
Epoch 297/300, seasonal_3 Loss: 0.0902 | 0.0846
Epoch 298/300, seasonal_3 Loss: 0.0895 | 0.0851
Epoch 299/300, seasonal_3 Loss: 0.0898 | 0.0826
Epoch 300/300, seasonal_3 Loss: 0.0895 | 0.0843
Training resid component with params: {'observation_period_num': 33, 'train_rates': 0.9893990007188281, 'learning_rate': 0.0001641381974189213, 'batch_size': 255, 'step_size': 12, 'gamma': 0.914857804500774}
Epoch 1/300, resid Loss: 1.0911 | 1.3668
Epoch 2/300, resid Loss: 0.6173 | 0.7849
Epoch 3/300, resid Loss: 0.4954 | 0.6840
Epoch 4/300, resid Loss: 0.4148 | 0.5082
Epoch 5/300, resid Loss: 0.3922 | 0.4633
Epoch 6/300, resid Loss: 0.4088 | 0.4366
Epoch 7/300, resid Loss: 0.4324 | 0.4813
Epoch 8/300, resid Loss: 0.3392 | 0.4528
Epoch 9/300, resid Loss: 0.2912 | 0.3591
Epoch 10/300, resid Loss: 0.2692 | 0.3033
Epoch 11/300, resid Loss: 0.2723 | 0.3137
Epoch 12/300, resid Loss: 0.2929 | 0.2854
Epoch 13/300, resid Loss: 0.3294 | 0.3743
Epoch 14/300, resid Loss: 0.2825 | 0.3130
Epoch 15/300, resid Loss: 0.2370 | 0.3017
Epoch 16/300, resid Loss: 0.2259 | 0.2955
Epoch 17/300, resid Loss: 0.2235 | 0.2534
Epoch 18/300, resid Loss: 0.2240 | 0.2438
Epoch 19/300, resid Loss: 0.2163 | 0.2454
Epoch 20/300, resid Loss: 0.2109 | 0.2255
Epoch 21/300, resid Loss: 0.1990 | 0.2202
Epoch 22/300, resid Loss: 0.1938 | 0.2062
Epoch 23/300, resid Loss: 0.1886 | 0.2001
Epoch 24/300, resid Loss: 0.1883 | 0.1924
Epoch 25/300, resid Loss: 0.1837 | 0.1864
Epoch 26/300, resid Loss: 0.1779 | 0.1817
Epoch 27/300, resid Loss: 0.1728 | 0.1767
Epoch 28/300, resid Loss: 0.1719 | 0.1760
Epoch 29/300, resid Loss: 0.1763 | 0.1724
Epoch 30/300, resid Loss: 0.1900 | 0.1789
Epoch 31/300, resid Loss: 0.1951 | 0.1695
Epoch 32/300, resid Loss: 0.1839 | 0.1724
Epoch 33/300, resid Loss: 0.1716 | 0.1620
Epoch 34/300, resid Loss: 0.1715 | 0.1642
Epoch 35/300, resid Loss: 0.1796 | 0.1568
Epoch 36/300, resid Loss: 0.1826 | 0.1587
Epoch 37/300, resid Loss: 0.1853 | 0.1548
Epoch 38/300, resid Loss: 0.1721 | 0.1591
Epoch 39/300, resid Loss: 0.1662 | 0.1494
Epoch 40/300, resid Loss: 0.1618 | 0.1567
Epoch 41/300, resid Loss: 0.1593 | 0.1439
Epoch 42/300, resid Loss: 0.1587 | 0.1530
Epoch 43/300, resid Loss: 0.1578 | 0.1430
Epoch 44/300, resid Loss: 0.1573 | 0.1520
Epoch 45/300, resid Loss: 0.1537 | 0.1431
Epoch 46/300, resid Loss: 0.1526 | 0.1437
Epoch 47/300, resid Loss: 0.1492 | 0.1385
Epoch 48/300, resid Loss: 0.1494 | 0.1380
Epoch 49/300, resid Loss: 0.1479 | 0.1364
Epoch 50/300, resid Loss: 0.1479 | 0.1348
Epoch 51/300, resid Loss: 0.1468 | 0.1334
Epoch 52/300, resid Loss: 0.1467 | 0.1323
Epoch 53/300, resid Loss: 0.1463 | 0.1316
Epoch 54/300, resid Loss: 0.1461 | 0.1308
Epoch 55/300, resid Loss: 0.1454 | 0.1293
Epoch 56/300, resid Loss: 0.1438 | 0.1284
Epoch 57/300, resid Loss: 0.1419 | 0.1277
Epoch 58/300, resid Loss: 0.1415 | 0.1265
Epoch 59/300, resid Loss: 0.1415 | 0.1265
Epoch 60/300, resid Loss: 0.1435 | 0.1258
Epoch 61/300, resid Loss: 0.1483 | 0.1295
Epoch 62/300, resid Loss: 0.1527 | 0.1276
Epoch 63/300, resid Loss: 0.1521 | 0.1291
Epoch 64/300, resid Loss: 0.1442 | 0.1222
Epoch 65/300, resid Loss: 0.1363 | 0.1220
Epoch 66/300, resid Loss: 0.1343 | 0.1204
Epoch 67/300, resid Loss: 0.1340 | 0.1201
Epoch 68/300, resid Loss: 0.1334 | 0.1196
Epoch 69/300, resid Loss: 0.1326 | 0.1185
Epoch 70/300, resid Loss: 0.1316 | 0.1184
Epoch 71/300, resid Loss: 0.1309 | 0.1180
Epoch 72/300, resid Loss: 0.1303 | 0.1175
Epoch 73/300, resid Loss: 0.1305 | 0.1181
Epoch 74/300, resid Loss: 0.1294 | 0.1169
Epoch 75/300, resid Loss: 0.1293 | 0.1172
Epoch 76/300, resid Loss: 0.1292 | 0.1160
Epoch 77/300, resid Loss: 0.1284 | 0.1160
Epoch 78/300, resid Loss: 0.1281 | 0.1140
Epoch 79/300, resid Loss: 0.1275 | 0.1157
Epoch 80/300, resid Loss: 0.1266 | 0.1135
Epoch 81/300, resid Loss: 0.1253 | 0.1146
Epoch 82/300, resid Loss: 0.1244 | 0.1125
Epoch 83/300, resid Loss: 0.1244 | 0.1133
Epoch 84/300, resid Loss: 0.1241 | 0.1120
Epoch 85/300, resid Loss: 0.1244 | 0.1114
Epoch 86/300, resid Loss: 0.1240 | 0.1119
Epoch 87/300, resid Loss: 0.1237 | 0.1105
Epoch 88/300, resid Loss: 0.1249 | 0.1144
Epoch 89/300, resid Loss: 0.1267 | 0.1137
Epoch 90/300, resid Loss: 0.1307 | 0.1157
Epoch 91/300, resid Loss: 0.1312 | 0.1115
Epoch 92/300, resid Loss: 0.1244 | 0.1090
Epoch 93/300, resid Loss: 0.1209 | 0.1076
Epoch 94/300, resid Loss: 0.1194 | 0.1073
Epoch 95/300, resid Loss: 0.1188 | 0.1067
Epoch 96/300, resid Loss: 0.1184 | 0.1064
Epoch 97/300, resid Loss: 0.1179 | 0.1062
Epoch 98/300, resid Loss: 0.1176 | 0.1065
Epoch 99/300, resid Loss: 0.1165 | 0.1052
Epoch 100/300, resid Loss: 0.1160 | 0.1055
Epoch 101/300, resid Loss: 0.1158 | 0.1055
Epoch 102/300, resid Loss: 0.1154 | 0.1059
Epoch 103/300, resid Loss: 0.1155 | 0.1050
Epoch 104/300, resid Loss: 0.1149 | 0.1054
Epoch 105/300, resid Loss: 0.1144 | 0.1050
Epoch 106/300, resid Loss: 0.1142 | 0.1040
Epoch 107/300, resid Loss: 0.1135 | 0.1046
Epoch 108/300, resid Loss: 0.1127 | 0.1036
Epoch 109/300, resid Loss: 0.1128 | 0.1033
Epoch 110/300, resid Loss: 0.1119 | 0.1026
Epoch 111/300, resid Loss: 0.1116 | 0.1028
Epoch 112/300, resid Loss: 0.1121 | 0.1023
Epoch 113/300, resid Loss: 0.1118 | 0.1035
Epoch 114/300, resid Loss: 0.1116 | 0.1016
Epoch 115/300, resid Loss: 0.1121 | 0.1033
Epoch 116/300, resid Loss: 0.1119 | 0.1012
Epoch 117/300, resid Loss: 0.1123 | 0.1020
Epoch 118/300, resid Loss: 0.1115 | 0.1008
Epoch 119/300, resid Loss: 0.1111 | 0.1013
Epoch 120/300, resid Loss: 0.1107 | 0.1011
Epoch 121/300, resid Loss: 0.1102 | 0.1002
Epoch 122/300, resid Loss: 0.1093 | 0.1014
Epoch 123/300, resid Loss: 0.1091 | 0.0994
Epoch 124/300, resid Loss: 0.1093 | 0.1016
Epoch 125/300, resid Loss: 0.1100 | 0.0994
Epoch 126/300, resid Loss: 0.1096 | 0.1012
Epoch 127/300, resid Loss: 0.1099 | 0.0990
Epoch 128/300, resid Loss: 0.1084 | 0.0996
Epoch 129/300, resid Loss: 0.1077 | 0.0987
Epoch 130/300, resid Loss: 0.1072 | 0.0981
Epoch 131/300, resid Loss: 0.1069 | 0.0983
Epoch 132/300, resid Loss: 0.1059 | 0.0981
Epoch 133/300, resid Loss: 0.1064 | 0.0981
Epoch 134/300, resid Loss: 0.1068 | 0.0977
Epoch 135/300, resid Loss: 0.1064 | 0.0981
Epoch 136/300, resid Loss: 0.1058 | 0.0972
Epoch 137/300, resid Loss: 0.1055 | 0.0975
Epoch 138/300, resid Loss: 0.1056 | 0.0968
Epoch 139/300, resid Loss: 0.1056 | 0.0980
Epoch 140/300, resid Loss: 0.1052 | 0.0967
Epoch 141/300, resid Loss: 0.1048 | 0.0973
Epoch 142/300, resid Loss: 0.1045 | 0.0962
Epoch 143/300, resid Loss: 0.1048 | 0.0968
Epoch 144/300, resid Loss: 0.1050 | 0.0968
Epoch 145/300, resid Loss: 0.1039 | 0.0969
Epoch 146/300, resid Loss: 0.1038 | 0.0968
Epoch 147/300, resid Loss: 0.1033 | 0.0953
Epoch 148/300, resid Loss: 0.1034 | 0.0962
Epoch 149/300, resid Loss: 0.1032 | 0.0959
Epoch 150/300, resid Loss: 0.1031 | 0.0955
Epoch 151/300, resid Loss: 0.1030 | 0.0956
Epoch 152/300, resid Loss: 0.1026 | 0.0957
Epoch 153/300, resid Loss: 0.1029 | 0.0956
Epoch 154/300, resid Loss: 0.1021 | 0.0953
Epoch 155/300, resid Loss: 0.1022 | 0.0952
Epoch 156/300, resid Loss: 0.1023 | 0.0952
Epoch 157/300, resid Loss: 0.1018 | 0.0946
Epoch 158/300, resid Loss: 0.1025 | 0.0949
Epoch 159/300, resid Loss: 0.1019 | 0.0945
Epoch 160/300, resid Loss: 0.1014 | 0.0944
Epoch 161/300, resid Loss: 0.1020 | 0.0942
Epoch 162/300, resid Loss: 0.1013 | 0.0943
Epoch 163/300, resid Loss: 0.1008 | 0.0943
Epoch 164/300, resid Loss: 0.1011 | 0.0937
Epoch 165/300, resid Loss: 0.1011 | 0.0930
Epoch 166/300, resid Loss: 0.1007 | 0.0937
Epoch 167/300, resid Loss: 0.1004 | 0.0938
Epoch 168/300, resid Loss: 0.1006 | 0.0930
Epoch 169/300, resid Loss: 0.1005 | 0.0937
Epoch 170/300, resid Loss: 0.1007 | 0.0929
Epoch 171/300, resid Loss: 0.1006 | 0.0931
Epoch 172/300, resid Loss: 0.1001 | 0.0931
Epoch 173/300, resid Loss: 0.0997 | 0.0933
Epoch 174/300, resid Loss: 0.1002 | 0.0931
Epoch 175/300, resid Loss: 0.0992 | 0.0928
Epoch 176/300, resid Loss: 0.0997 | 0.0927
Epoch 177/300, resid Loss: 0.0994 | 0.0931
Epoch 178/300, resid Loss: 0.0994 | 0.0931
Epoch 179/300, resid Loss: 0.0993 | 0.0924
Epoch 180/300, resid Loss: 0.0988 | 0.0920
Epoch 181/300, resid Loss: 0.0993 | 0.0926
Epoch 182/300, resid Loss: 0.0990 | 0.0922
Epoch 183/300, resid Loss: 0.0992 | 0.0920
Epoch 184/300, resid Loss: 0.0984 | 0.0918
Epoch 185/300, resid Loss: 0.0984 | 0.0924
Epoch 186/300, resid Loss: 0.0984 | 0.0917
Epoch 187/300, resid Loss: 0.0981 | 0.0917
Epoch 188/300, resid Loss: 0.0989 | 0.0914
Epoch 189/300, resid Loss: 0.0980 | 0.0912
Epoch 190/300, resid Loss: 0.0985 | 0.0918
Epoch 191/300, resid Loss: 0.0987 | 0.0916
Epoch 192/300, resid Loss: 0.0982 | 0.0917
Epoch 193/300, resid Loss: 0.0979 | 0.0910
Epoch 194/300, resid Loss: 0.0977 | 0.0909
Epoch 195/300, resid Loss: 0.0976 | 0.0911
Epoch 196/300, resid Loss: 0.0976 | 0.0909
Epoch 197/300, resid Loss: 0.0976 | 0.0909
Epoch 198/300, resid Loss: 0.0977 | 0.0912
Epoch 199/300, resid Loss: 0.0977 | 0.0912
Epoch 200/300, resid Loss: 0.0969 | 0.0912
Epoch 201/300, resid Loss: 0.0976 | 0.0909
Epoch 202/300, resid Loss: 0.0971 | 0.0908
Epoch 203/300, resid Loss: 0.0972 | 0.0903
Epoch 204/300, resid Loss: 0.0967 | 0.0901
Epoch 205/300, resid Loss: 0.0967 | 0.0903
Epoch 206/300, resid Loss: 0.0965 | 0.0901
Epoch 207/300, resid Loss: 0.0972 | 0.0901
Epoch 208/300, resid Loss: 0.0972 | 0.0901
Epoch 209/300, resid Loss: 0.0970 | 0.0910
Epoch 210/300, resid Loss: 0.0965 | 0.0902
Epoch 211/300, resid Loss: 0.0968 | 0.0900
Epoch 212/300, resid Loss: 0.0970 | 0.0900
Epoch 213/300, resid Loss: 0.0965 | 0.0897
Epoch 214/300, resid Loss: 0.0965 | 0.0897
Epoch 215/300, resid Loss: 0.0965 | 0.0895
Epoch 216/300, resid Loss: 0.0958 | 0.0897
Epoch 217/300, resid Loss: 0.0964 | 0.0899
Epoch 218/300, resid Loss: 0.0959 | 0.0895
Epoch 219/300, resid Loss: 0.0953 | 0.0896
Epoch 220/300, resid Loss: 0.0960 | 0.0897
Epoch 221/300, resid Loss: 0.0959 | 0.0899
Epoch 222/300, resid Loss: 0.0962 | 0.0896
Epoch 223/300, resid Loss: 0.0959 | 0.0895
Epoch 224/300, resid Loss: 0.0962 | 0.0893
Epoch 225/300, resid Loss: 0.0960 | 0.0894
Epoch 226/300, resid Loss: 0.0949 | 0.0897
Epoch 227/300, resid Loss: 0.0955 | 0.0896
Epoch 228/300, resid Loss: 0.0956 | 0.0892
Epoch 229/300, resid Loss: 0.0957 | 0.0892
Epoch 230/300, resid Loss: 0.0953 | 0.0892
Epoch 231/300, resid Loss: 0.0955 | 0.0892
Epoch 232/300, resid Loss: 0.0953 | 0.0893
Epoch 233/300, resid Loss: 0.0955 | 0.0891
Epoch 234/300, resid Loss: 0.0952 | 0.0895
Epoch 235/300, resid Loss: 0.0954 | 0.0896
Epoch 236/300, resid Loss: 0.0944 | 0.0894
Epoch 237/300, resid Loss: 0.0954 | 0.0892
Epoch 238/300, resid Loss: 0.0955 | 0.0891
Epoch 239/300, resid Loss: 0.0953 | 0.0890
Epoch 240/300, resid Loss: 0.0947 | 0.0892
Epoch 241/300, resid Loss: 0.0946 | 0.0894
Epoch 242/300, resid Loss: 0.0947 | 0.0889
Epoch 243/300, resid Loss: 0.0944 | 0.0885
Epoch 244/300, resid Loss: 0.0954 | 0.0886
Epoch 245/300, resid Loss: 0.0942 | 0.0890
Epoch 246/300, resid Loss: 0.0948 | 0.0889
Epoch 247/300, resid Loss: 0.0949 | 0.0893
Epoch 248/300, resid Loss: 0.0946 | 0.0891
Epoch 249/300, resid Loss: 0.0944 | 0.0888
Epoch 250/300, resid Loss: 0.0947 | 0.0888
Epoch 251/300, resid Loss: 0.0945 | 0.0887
Epoch 252/300, resid Loss: 0.0943 | 0.0886
Epoch 253/300, resid Loss: 0.0944 | 0.0886
Epoch 254/300, resid Loss: 0.0940 | 0.0887
Epoch 255/300, resid Loss: 0.0939 | 0.0886
Epoch 256/300, resid Loss: 0.0942 | 0.0884
Epoch 257/300, resid Loss: 0.0942 | 0.0882
Epoch 258/300, resid Loss: 0.0936 | 0.0880
Epoch 259/300, resid Loss: 0.0936 | 0.0878
Epoch 260/300, resid Loss: 0.0939 | 0.0883
Epoch 261/300, resid Loss: 0.0940 | 0.0884
Epoch 262/300, resid Loss: 0.0942 | 0.0885
Epoch 263/300, resid Loss: 0.0938 | 0.0885
Epoch 264/300, resid Loss: 0.0938 | 0.0880
Epoch 265/300, resid Loss: 0.0940 | 0.0876
Epoch 266/300, resid Loss: 0.0937 | 0.0879
Epoch 267/300, resid Loss: 0.0942 | 0.0883
Epoch 268/300, resid Loss: 0.0941 | 0.0885
Epoch 269/300, resid Loss: 0.0938 | 0.0882
Epoch 270/300, resid Loss: 0.0939 | 0.0880
Epoch 271/300, resid Loss: 0.0938 | 0.0882
Epoch 272/300, resid Loss: 0.0941 | 0.0880
Epoch 273/300, resid Loss: 0.0937 | 0.0880
Epoch 274/300, resid Loss: 0.0930 | 0.0881
Epoch 275/300, resid Loss: 0.0938 | 0.0881
Epoch 276/300, resid Loss: 0.0932 | 0.0882
Epoch 277/300, resid Loss: 0.0938 | 0.0880
Epoch 278/300, resid Loss: 0.0935 | 0.0878
Epoch 279/300, resid Loss: 0.0937 | 0.0879
Epoch 280/300, resid Loss: 0.0933 | 0.0880
Epoch 281/300, resid Loss: 0.0928 | 0.0878
Epoch 282/300, resid Loss: 0.0937 | 0.0876
Epoch 283/300, resid Loss: 0.0934 | 0.0875
Epoch 284/300, resid Loss: 0.0936 | 0.0876
Epoch 285/300, resid Loss: 0.0931 | 0.0877
Epoch 286/300, resid Loss: 0.0930 | 0.0877
Epoch 287/300, resid Loss: 0.0934 | 0.0875
Epoch 288/300, resid Loss: 0.0936 | 0.0875
Epoch 289/300, resid Loss: 0.0931 | 0.0876
Epoch 290/300, resid Loss: 0.0931 | 0.0876
Epoch 291/300, resid Loss: 0.0930 | 0.0876
Epoch 292/300, resid Loss: 0.0931 | 0.0876
Epoch 293/300, resid Loss: 0.0930 | 0.0872
Epoch 294/300, resid Loss: 0.0929 | 0.0873
Epoch 295/300, resid Loss: 0.0930 | 0.0873
Epoch 296/300, resid Loss: 0.0933 | 0.0874
Epoch 297/300, resid Loss: 0.0930 | 0.0876
Epoch 298/300, resid Loss: 0.0930 | 0.0878
Epoch 299/300, resid Loss: 0.0928 | 0.0876
Epoch 300/300, resid Loss: 0.0930 | 0.0877
Runtime (seconds): 2778.430413007736
8.863344909546302e-05
[213.22498]
[1.5374203]
[-0.10807595]
[5.3464136]
[-2.1873481]
[7.166787]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 101.60274554067291
RMSE: 10.079818725585938
MAE: 10.079818725585938
R-squared: nan
[224.98018]
File aapl_stock_price_prediction_by_Transformer.png exists. Logging to WandB.
