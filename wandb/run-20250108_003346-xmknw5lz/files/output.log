ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-08 00:33:47,468][0m A new study created in memory with name: no-name-b843230a-5200-41c2-8919-9b52b6444150[0m
[32m[I 2025-01-08 00:34:42,514][0m Trial 0 finished with value: 0.31580628707394964 and parameters: {'observation_period_num': 73, 'train_rates': 0.7626466767745218, 'learning_rate': 5.264270167837389e-05, 'batch_size': 244, 'step_size': 3, 'gamma': 0.7956379382304317}. Best is trial 0 with value: 0.31580628707394964.[0m
[32m[I 2025-01-08 00:35:38,081][0m Trial 1 finished with value: 0.19857070731537757 and parameters: {'observation_period_num': 170, 'train_rates': 0.6748035313654869, 'learning_rate': 2.7235034553171457e-05, 'batch_size': 98, 'step_size': 9, 'gamma': 0.9197645495012864}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:37:22,269][0m Trial 2 finished with value: 0.23634849709507666 and parameters: {'observation_period_num': 155, 'train_rates': 0.660635995505508, 'learning_rate': 0.00039769382311992055, 'batch_size': 51, 'step_size': 13, 'gamma': 0.7846514159916254}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:38:28,319][0m Trial 3 finished with value: 0.6104198485127841 and parameters: {'observation_period_num': 193, 'train_rates': 0.7999559289804257, 'learning_rate': 9.355807617252877e-06, 'batch_size': 82, 'step_size': 1, 'gamma': 0.9184646169469252}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:39:04,722][0m Trial 4 finished with value: 0.2661031186580658 and parameters: {'observation_period_num': 23, 'train_rates': 0.956009954025288, 'learning_rate': 1.7399995708022046e-05, 'batch_size': 251, 'step_size': 6, 'gamma': 0.8428464834828043}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:39:33,175][0m Trial 5 finished with value: 0.2896778134161327 and parameters: {'observation_period_num': 112, 'train_rates': 0.6178374901757526, 'learning_rate': 6.859009091481648e-05, 'batch_size': 200, 'step_size': 8, 'gamma': 0.8080300991271089}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:40:09,207][0m Trial 6 finished with value: 1.1766151986084878 and parameters: {'observation_period_num': 131, 'train_rates': 0.6375223451212277, 'learning_rate': 1.4170713314219876e-06, 'batch_size': 138, 'step_size': 12, 'gamma': 0.8291057883467523}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:41:37,074][0m Trial 7 finished with value: 0.28788023554917536 and parameters: {'observation_period_num': 166, 'train_rates': 0.8225728110444966, 'learning_rate': 3.43521684042748e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.8164117113635161}. Best is trial 1 with value: 0.19857070731537757.[0m
[32m[I 2025-01-08 00:42:31,556][0m Trial 8 finished with value: 0.189488588635665 and parameters: {'observation_period_num': 227, 'train_rates': 0.7239696764022203, 'learning_rate': 0.000112893599155254, 'batch_size': 95, 'step_size': 6, 'gamma': 0.7524237157521232}. Best is trial 8 with value: 0.189488588635665.[0m
[32m[I 2025-01-08 00:43:29,656][0m Trial 9 finished with value: 0.907407669660424 and parameters: {'observation_period_num': 153, 'train_rates': 0.7879145039049067, 'learning_rate': 2.9558247679096794e-06, 'batch_size': 156, 'step_size': 2, 'gamma': 0.8390505014294438}. Best is trial 8 with value: 0.189488588635665.[0m
[32m[I 2025-01-08 00:46:18,631][0m Trial 10 finished with value: 0.10877237010460633 and parameters: {'observation_period_num': 248, 'train_rates': 0.9327455503810401, 'learning_rate': 0.0008518073942292812, 'batch_size': 32, 'step_size': 5, 'gamma': 0.9823838887256456}. Best is trial 10 with value: 0.10877237010460633.[0m
[32m[I 2025-01-08 00:50:09,710][0m Trial 11 finished with value: 0.17219397823774177 and parameters: {'observation_period_num': 249, 'train_rates': 0.9517412359539277, 'learning_rate': 0.0006134302698564697, 'batch_size': 24, 'step_size': 5, 'gamma': 0.9826605093366972}. Best is trial 10 with value: 0.10877237010460633.[0m
[32m[I 2025-01-08 00:54:41,754][0m Trial 12 finished with value: 0.09133850564377023 and parameters: {'observation_period_num': 246, 'train_rates': 0.9598491720842641, 'learning_rate': 0.0006053819289071593, 'batch_size': 20, 'step_size': 5, 'gamma': 0.981652555756302}. Best is trial 12 with value: 0.09133850564377023.[0m
[32m[I 2025-01-08 00:57:33,570][0m Trial 13 finished with value: 0.16408682156638846 and parameters: {'observation_period_num': 221, 'train_rates': 0.8899237450619446, 'learning_rate': 0.000244119601974418, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9834059775869295}. Best is trial 12 with value: 0.09133850564377023.[0m
[32m[I 2025-01-08 01:01:41,609][0m Trial 14 finished with value: 0.24533859896698182 and parameters: {'observation_period_num': 252, 'train_rates': 0.8849258700975647, 'learning_rate': 0.0009275211450453949, 'batch_size': 22, 'step_size': 4, 'gamma': 0.9421031587021527}. Best is trial 12 with value: 0.09133850564377023.[0m
[32m[I 2025-01-08 01:03:09,142][0m Trial 15 finished with value: 0.09398099780082703 and parameters: {'observation_period_num': 205, 'train_rates': 0.9847664892061441, 'learning_rate': 0.00018197679857358105, 'batch_size': 66, 'step_size': 7, 'gamma': 0.9455753605250582}. Best is trial 12 with value: 0.09133850564377023.[0m
[32m[I 2025-01-08 01:04:31,470][0m Trial 16 finished with value: 0.08477038890123367 and parameters: {'observation_period_num': 196, 'train_rates': 0.9763766152515572, 'learning_rate': 0.0001235111148393233, 'batch_size': 70, 'step_size': 11, 'gamma': 0.881564232973016}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:05:26,650][0m Trial 17 finished with value: 0.17854223607203387 and parameters: {'observation_period_num': 196, 'train_rates': 0.8656539695842046, 'learning_rate': 0.00026362159222244243, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8773673091910817}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:06:23,960][0m Trial 18 finished with value: 0.1357494810189324 and parameters: {'observation_period_num': 100, 'train_rates': 0.8478252113154356, 'learning_rate': 0.00011306165961864354, 'batch_size': 125, 'step_size': 14, 'gamma': 0.8755265010331087}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:07:12,548][0m Trial 19 finished with value: 0.12552193859044244 and parameters: {'observation_period_num': 218, 'train_rates': 0.9193377794122768, 'learning_rate': 0.00041961192102711077, 'batch_size': 174, 'step_size': 10, 'gamma': 0.9004598046053472}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:09:12,737][0m Trial 20 finished with value: 0.176961972117424 and parameters: {'observation_period_num': 185, 'train_rates': 0.973176030268376, 'learning_rate': 1.0270520187030393e-05, 'batch_size': 51, 'step_size': 15, 'gamma': 0.9588038352605981}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:10:42,721][0m Trial 21 finished with value: 0.17819789052009583 and parameters: {'observation_period_num': 210, 'train_rates': 0.9894599000161047, 'learning_rate': 0.0001617904203048641, 'batch_size': 72, 'step_size': 7, 'gamma': 0.9521387146187578}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:11:33,159][0m Trial 22 finished with value: 0.1104527349763937 and parameters: {'observation_period_num': 235, 'train_rates': 0.9108725070756032, 'learning_rate': 0.00020092040703513557, 'batch_size': 119, 'step_size': 7, 'gamma': 0.9315834773855486}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:13:30,940][0m Trial 23 finished with value: 0.1565025895833969 and parameters: {'observation_period_num': 206, 'train_rates': 0.9896893574244652, 'learning_rate': 0.0004501264530041438, 'batch_size': 49, 'step_size': 11, 'gamma': 0.8921287362070707}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:14:52,231][0m Trial 24 finished with value: 0.14262742012929006 and parameters: {'observation_period_num': 185, 'train_rates': 0.943658295846064, 'learning_rate': 9.572462533946262e-05, 'batch_size': 72, 'step_size': 8, 'gamma': 0.9540475698051316}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:19:28,567][0m Trial 25 finished with value: 0.17779676921113774 and parameters: {'observation_period_num': 228, 'train_rates': 0.9011841443330447, 'learning_rate': 0.00030601072429752924, 'batch_size': 19, 'step_size': 10, 'gamma': 0.9681518746972675}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:21:54,489][0m Trial 26 finished with value: 0.09630986413743237 and parameters: {'observation_period_num': 134, 'train_rates': 0.9645094878557869, 'learning_rate': 6.341423700466103e-05, 'batch_size': 42, 'step_size': 4, 'gamma': 0.857739504299817}. Best is trial 16 with value: 0.08477038890123367.[0m
[32m[I 2025-01-08 01:23:20,995][0m Trial 27 finished with value: 0.049261369802094446 and parameters: {'observation_period_num': 61, 'train_rates': 0.9300870093901547, 'learning_rate': 0.00018590706659444152, 'batch_size': 72, 'step_size': 7, 'gamma': 0.902879918189371}. Best is trial 27 with value: 0.049261369802094446.[0m
[32m[I 2025-01-08 01:24:25,235][0m Trial 28 finished with value: 0.04347706439422537 and parameters: {'observation_period_num': 41, 'train_rates': 0.8563960336172092, 'learning_rate': 0.0006154504392301379, 'batch_size': 86, 'step_size': 13, 'gamma': 0.8998161411884118}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:25:00,872][0m Trial 29 finished with value: 0.08735586418093301 and parameters: {'observation_period_num': 41, 'train_rates': 0.8444051159477288, 'learning_rate': 4.070651505637233e-05, 'batch_size': 223, 'step_size': 13, 'gamma': 0.8969303379114264}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:25:48,859][0m Trial 30 finished with value: 0.06972571187952653 and parameters: {'observation_period_num': 64, 'train_rates': 0.753173746327043, 'learning_rate': 7.17110016474903e-05, 'batch_size': 109, 'step_size': 12, 'gamma': 0.8604968352718196}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:26:40,687][0m Trial 31 finished with value: 0.061844870156749 and parameters: {'observation_period_num': 64, 'train_rates': 0.7525807878729462, 'learning_rate': 8.955361903723262e-05, 'batch_size': 112, 'step_size': 12, 'gamma': 0.8699676952747175}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:27:32,713][0m Trial 32 finished with value: 0.12469351378179365 and parameters: {'observation_period_num': 64, 'train_rates': 0.7319452625838639, 'learning_rate': 1.8106293566405708e-05, 'batch_size': 110, 'step_size': 15, 'gamma': 0.854682216609934}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:28:20,921][0m Trial 33 finished with value: 0.0739446036842441 and parameters: {'observation_period_num': 70, 'train_rates': 0.7458190198664616, 'learning_rate': 6.784864896022234e-05, 'batch_size': 138, 'step_size': 13, 'gamma': 0.9123679218235556}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:29:24,723][0m Trial 34 finished with value: 0.06480992232737114 and parameters: {'observation_period_num': 52, 'train_rates': 0.7738863766169872, 'learning_rate': 4.497487685852054e-05, 'batch_size': 89, 'step_size': 12, 'gamma': 0.861983310770026}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:30:18,445][0m Trial 35 finished with value: 0.05682692791406925 and parameters: {'observation_period_num': 10, 'train_rates': 0.6999732485434456, 'learning_rate': 2.1528307778381866e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.9101386777057187}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:31:16,433][0m Trial 36 finished with value: 0.0723838127706818 and parameters: {'observation_period_num': 5, 'train_rates': 0.6978063441736412, 'learning_rate': 6.6366172252769995e-06, 'batch_size': 85, 'step_size': 14, 'gamma': 0.9244200166221851}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:31:48,369][0m Trial 37 finished with value: 0.19781220662237403 and parameters: {'observation_period_num': 88, 'train_rates': 0.6850254589021818, 'learning_rate': 2.6428545976771018e-05, 'batch_size': 158, 'step_size': 14, 'gamma': 0.908110476302184}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:32:32,133][0m Trial 38 finished with value: 0.144527320955929 and parameters: {'observation_period_num': 31, 'train_rates': 0.8050955440919911, 'learning_rate': 1.705025103745141e-05, 'batch_size': 126, 'step_size': 15, 'gamma': 0.8863884244853962}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:33:31,742][0m Trial 39 finished with value: 0.0669553507063257 and parameters: {'observation_period_num': 9, 'train_rates': 0.6624207483859972, 'learning_rate': 2.7928466273246424e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.931923275137706}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:34:17,011][0m Trial 40 finished with value: 0.34266095200848146 and parameters: {'observation_period_num': 22, 'train_rates': 0.6054134952732602, 'learning_rate': 4.432673665908479e-06, 'batch_size': 99, 'step_size': 13, 'gamma': 0.9056359033409871}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:35:18,871][0m Trial 41 finished with value: 0.055266318596540305 and parameters: {'observation_period_num': 45, 'train_rates': 0.7791119370396298, 'learning_rate': 4.501841677499444e-05, 'batch_size': 86, 'step_size': 12, 'gamma': 0.8628677507229663}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:36:18,487][0m Trial 42 finished with value: 0.1035079765381545 and parameters: {'observation_period_num': 46, 'train_rates': 0.7094318599366558, 'learning_rate': 2.057476586792542e-05, 'batch_size': 83, 'step_size': 12, 'gamma': 0.8392752123173398}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:37:52,312][0m Trial 43 finished with value: 0.05196996648377235 and parameters: {'observation_period_num': 24, 'train_rates': 0.8141006936552369, 'learning_rate': 1.1364427842439677e-05, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9206956538560133}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:39:25,799][0m Trial 44 finished with value: 0.0572525535048988 and parameters: {'observation_period_num': 22, 'train_rates': 0.8214371355362029, 'learning_rate': 1.086100531524808e-05, 'batch_size': 59, 'step_size': 14, 'gamma': 0.9184176488474907}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:41:42,808][0m Trial 45 finished with value: 0.076682547222317 and parameters: {'observation_period_num': 36, 'train_rates': 0.7781942759891285, 'learning_rate': 7.32556271089966e-06, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8920260902124962}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:43:17,919][0m Trial 46 finished with value: 0.07963116192503979 and parameters: {'observation_period_num': 84, 'train_rates': 0.8148199870263918, 'learning_rate': 1.3989430504398326e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9317041998166813}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:44:34,313][0m Trial 47 finished with value: 0.25746568784117696 and parameters: {'observation_period_num': 18, 'train_rates': 0.8640124854188236, 'learning_rate': 2.004364983506224e-06, 'batch_size': 76, 'step_size': 15, 'gamma': 0.8139832398463737}. Best is trial 28 with value: 0.04347706439422537.[0m
[32m[I 2025-01-08 01:45:46,000][0m Trial 48 finished with value: 0.04250810848102456 and parameters: {'observation_period_num': 49, 'train_rates': 0.8408543181453205, 'learning_rate': 0.000643830278682234, 'batch_size': 96, 'step_size': 10, 'gamma': 0.791035450221983}. Best is trial 48 with value: 0.04250810848102456.[0m
[32m[I 2025-01-08 01:47:22,080][0m Trial 49 finished with value: 0.05305592377719126 and parameters: {'observation_period_num': 52, 'train_rates': 0.8366227187124906, 'learning_rate': 0.0005921927298238323, 'batch_size': 63, 'step_size': 8, 'gamma': 0.7808218036925874}. Best is trial 48 with value: 0.04250810848102456.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-08 01:47:22,090][0m A new study created in memory with name: no-name-592b13cc-5233-448d-864d-e511bf9fb19f[0m
[32m[I 2025-01-08 01:48:19,483][0m Trial 0 finished with value: 0.123488399027922 and parameters: {'observation_period_num': 79, 'train_rates': 0.8062712562384069, 'learning_rate': 0.00035820838353457573, 'batch_size': 126, 'step_size': 5, 'gamma': 0.9709820098300781}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:49:12,589][0m Trial 1 finished with value: 0.603402099129874 and parameters: {'observation_period_num': 75, 'train_rates': 0.813792085778104, 'learning_rate': 3.605034811778722e-06, 'batch_size': 173, 'step_size': 5, 'gamma': 0.8926430939118524}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:50:00,749][0m Trial 2 finished with value: 0.625265594461478 and parameters: {'observation_period_num': 131, 'train_rates': 0.653429938770817, 'learning_rate': 3.4311478051350617e-06, 'batch_size': 204, 'step_size': 11, 'gamma': 0.9565711180965084}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:50:45,733][0m Trial 3 finished with value: 0.8500948426865486 and parameters: {'observation_period_num': 174, 'train_rates': 0.8855346421020724, 'learning_rate': 1.4834526771656545e-06, 'batch_size': 179, 'step_size': 8, 'gamma': 0.8763192089365616}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:52:02,477][0m Trial 4 finished with value: 0.34700199166933693 and parameters: {'observation_period_num': 211, 'train_rates': 0.9565776642477597, 'learning_rate': 1.4769816487143101e-05, 'batch_size': 76, 'step_size': 1, 'gamma': 0.9617678055119245}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:52:33,584][0m Trial 5 finished with value: 0.18743911404599514 and parameters: {'observation_period_num': 201, 'train_rates': 0.6594114161906552, 'learning_rate': 0.0003645931902998678, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8891144969375198}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:53:51,806][0m Trial 6 finished with value: 0.1872053215559343 and parameters: {'observation_period_num': 74, 'train_rates': 0.7174415408510125, 'learning_rate': 8.457158625915257e-05, 'batch_size': 62, 'step_size': 1, 'gamma': 0.906682187465548}. Best is trial 0 with value: 0.123488399027922.[0m
[32m[I 2025-01-08 01:54:51,057][0m Trial 7 finished with value: 0.0851995050907135 and parameters: {'observation_period_num': 112, 'train_rates': 0.9641389485539262, 'learning_rate': 0.0001348589410745974, 'batch_size': 123, 'step_size': 9, 'gamma': 0.9554696793309965}. Best is trial 7 with value: 0.0851995050907135.[0m
[32m[I 2025-01-08 01:55:36,459][0m Trial 8 finished with value: 0.4575546210965001 and parameters: {'observation_period_num': 144, 'train_rates': 0.6759081940039402, 'learning_rate': 1.2873851684609997e-05, 'batch_size': 143, 'step_size': 5, 'gamma': 0.8575734775688891}. Best is trial 7 with value: 0.0851995050907135.[0m
[32m[I 2025-01-08 01:56:10,441][0m Trial 9 finished with value: 0.2790336974367902 and parameters: {'observation_period_num': 121, 'train_rates': 0.634361966773139, 'learning_rate': 5.188660821311513e-05, 'batch_size': 211, 'step_size': 12, 'gamma': 0.8002039389619948}. Best is trial 7 with value: 0.0851995050907135.[0m
[32m[I 2025-01-08 02:02:16,173][0m Trial 10 finished with value: 0.03829895968324151 and parameters: {'observation_period_num': 17, 'train_rates': 0.9705746267589476, 'learning_rate': 0.000856211985528896, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7540163233132311}. Best is trial 10 with value: 0.03829895968324151.[0m
[32m[I 2025-01-08 02:07:36,641][0m Trial 11 finished with value: 0.036709895186430336 and parameters: {'observation_period_num': 24, 'train_rates': 0.9666012892829964, 'learning_rate': 0.0009412480002577198, 'batch_size': 19, 'step_size': 9, 'gamma': 0.771601088286029}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:13:35,412][0m Trial 12 finished with value: 0.07068347438412198 and parameters: {'observation_period_num': 6, 'train_rates': 0.9200750307151027, 'learning_rate': 0.0007203698340397854, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7602521782220715}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:16:38,845][0m Trial 13 finished with value: 0.05579505460475808 and parameters: {'observation_period_num': 19, 'train_rates': 0.8633410293881328, 'learning_rate': 0.0007902746018579437, 'batch_size': 30, 'step_size': 11, 'gamma': 0.756065630240279}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:18:16,379][0m Trial 14 finished with value: 0.0396386943757534 and parameters: {'observation_period_num': 36, 'train_rates': 0.9881095742871937, 'learning_rate': 0.0002125055763957537, 'batch_size': 73, 'step_size': 14, 'gamma': 0.8022418674405548}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:20:33,536][0m Trial 15 finished with value: 0.07851889493261896 and parameters: {'observation_period_num': 46, 'train_rates': 0.9014554884042506, 'learning_rate': 0.0005837800429473934, 'batch_size': 42, 'step_size': 7, 'gamma': 0.800216386456638}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:21:00,985][0m Trial 16 finished with value: 0.1828343439431134 and parameters: {'observation_period_num': 49, 'train_rates': 0.7382461969892887, 'learning_rate': 2.9321542920691683e-05, 'batch_size': 254, 'step_size': 10, 'gamma': 0.8318703052339633}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:22:02,529][0m Trial 17 finished with value: 0.06888347810669111 and parameters: {'observation_period_num': 93, 'train_rates': 0.8589284915468877, 'learning_rate': 0.00018050075666381081, 'batch_size': 90, 'step_size': 13, 'gamma': 0.776604756390431}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:23:00,810][0m Trial 18 finished with value: 0.05332360035356353 and parameters: {'observation_period_num': 26, 'train_rates': 0.9421031907605523, 'learning_rate': 0.0008724618296847056, 'batch_size': 103, 'step_size': 6, 'gamma': 0.8271068816430078}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:24:43,813][0m Trial 19 finished with value: 0.0918130704334804 and parameters: {'observation_period_num': 60, 'train_rates': 0.7680899798765289, 'learning_rate': 0.0002880670038198462, 'batch_size': 48, 'step_size': 3, 'gamma': 0.782106998590141}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:29:13,486][0m Trial 20 finished with value: 0.1471706751189875 and parameters: {'observation_period_num': 248, 'train_rates': 0.8357329634313125, 'learning_rate': 6.831209165194075e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.7527690109257127}. Best is trial 11 with value: 0.036709895186430336.[0m
[32m[I 2025-01-08 02:30:52,577][0m Trial 21 finished with value: 0.03494158014655113 and parameters: {'observation_period_num': 31, 'train_rates': 0.984822505783979, 'learning_rate': 0.0001837130267663075, 'batch_size': 62, 'step_size': 15, 'gamma': 0.8021808611944206}. Best is trial 21 with value: 0.03494158014655113.[0m
[32m[I 2025-01-08 02:33:17,105][0m Trial 22 finished with value: 0.02171107940375805 and parameters: {'observation_period_num': 8, 'train_rates': 0.9899585195151253, 'learning_rate': 0.00044837801302468423, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8262708060787456}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:35:22,813][0m Trial 23 finished with value: 0.06508026094835939 and parameters: {'observation_period_num': 5, 'train_rates': 0.9304373395111485, 'learning_rate': 0.0003649820459788343, 'batch_size': 48, 'step_size': 15, 'gamma': 0.8347184833087724}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:36:42,426][0m Trial 24 finished with value: 0.06220245361328125 and parameters: {'observation_period_num': 53, 'train_rates': 0.9779400307903257, 'learning_rate': 0.00012030684784693067, 'batch_size': 99, 'step_size': 13, 'gamma': 0.8498707124937489}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:38:22,724][0m Trial 25 finished with value: 0.06876823306083679 and parameters: {'observation_period_num': 33, 'train_rates': 0.9886344360182022, 'learning_rate': 0.00046833089362822153, 'batch_size': 66, 'step_size': 14, 'gamma': 0.81344837158383}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:41:02,204][0m Trial 26 finished with value: 0.09366660186921368 and parameters: {'observation_period_num': 104, 'train_rates': 0.9081163909962652, 'learning_rate': 0.0001897794676137677, 'batch_size': 38, 'step_size': 12, 'gamma': 0.7749598220656886}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:42:53,716][0m Trial 27 finished with value: 0.08059509161366038 and parameters: {'observation_period_num': 64, 'train_rates': 0.942621770211544, 'learning_rate': 3.9482705145474684e-05, 'batch_size': 59, 'step_size': 15, 'gamma': 0.7863662423977464}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:44:09,321][0m Trial 28 finished with value: 0.05952939732621113 and parameters: {'observation_period_num': 31, 'train_rates': 0.8770566814022114, 'learning_rate': 0.00010055400336881696, 'batch_size': 86, 'step_size': 13, 'gamma': 0.813114100634751}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:44:48,874][0m Trial 29 finished with value: 0.16365947580884116 and parameters: {'observation_period_num': 91, 'train_rates': 0.6002457154357781, 'learning_rate': 0.0002516719672785462, 'batch_size': 115, 'step_size': 14, 'gamma': 0.9290762052024756}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:47:31,161][0m Trial 30 finished with value: 0.1305755281223441 and parameters: {'observation_period_num': 148, 'train_rates': 0.9436698689306908, 'learning_rate': 0.0004975901477283425, 'batch_size': 34, 'step_size': 3, 'gamma': 0.9872718444538855}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:51:48,094][0m Trial 31 finished with value: 0.035452390107892306 and parameters: {'observation_period_num': 13, 'train_rates': 0.9655736373625061, 'learning_rate': 0.0009523145257052367, 'batch_size': 23, 'step_size': 10, 'gamma': 0.7709003240267794}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:55:15,453][0m Trial 32 finished with value: 0.046080287349851506 and parameters: {'observation_period_num': 8, 'train_rates': 0.9613665251283245, 'learning_rate': 0.00047435892487587573, 'batch_size': 30, 'step_size': 11, 'gamma': 0.7889580264658093}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 02:57:25,510][0m Trial 33 finished with value: 0.07169283482317741 and parameters: {'observation_period_num': 39, 'train_rates': 0.911001779832634, 'learning_rate': 0.0009542736240841424, 'batch_size': 50, 'step_size': 10, 'gamma': 0.7675362903791715}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:01:00,124][0m Trial 34 finished with value: 0.07659958548909795 and parameters: {'observation_period_num': 22, 'train_rates': 0.8076369789582628, 'learning_rate': 0.00039668564783608494, 'batch_size': 27, 'step_size': 7, 'gamma': 0.8191787987772297}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:02:51,764][0m Trial 35 finished with value: 0.12622211873531342 and parameters: {'observation_period_num': 73, 'train_rates': 0.9883003990560678, 'learning_rate': 0.0006038546722842033, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8438329092893869}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:04:07,515][0m Trial 36 finished with value: 0.5498221261296302 and parameters: {'observation_period_num': 17, 'train_rates': 0.8933108418882006, 'learning_rate': 1.0968027049084052e-06, 'batch_size': 77, 'step_size': 10, 'gamma': 0.7935468967608471}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:06:33,285][0m Trial 37 finished with value: 0.0835956212160361 and parameters: {'observation_period_num': 59, 'train_rates': 0.9529647674875849, 'learning_rate': 0.0002922909557821744, 'batch_size': 40, 'step_size': 15, 'gamma': 0.7691150467339827}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:08:06,852][0m Trial 38 finished with value: 0.06322478502988815 and parameters: {'observation_period_num': 42, 'train_rates': 0.9288535055837683, 'learning_rate': 0.00015989931442116483, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8699118906878791}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:09:04,272][0m Trial 39 finished with value: 0.24396051497968507 and parameters: {'observation_period_num': 86, 'train_rates': 0.8353196963337277, 'learning_rate': 1.1984863099102899e-05, 'batch_size': 141, 'step_size': 14, 'gamma': 0.8070725246130681}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:09:59,123][0m Trial 40 finished with value: 0.7116256952285767 and parameters: {'observation_period_num': 176, 'train_rates': 0.971987377454106, 'learning_rate': 3.3090754562749817e-06, 'batch_size': 185, 'step_size': 5, 'gamma': 0.8620604896689047}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:15:54,373][0m Trial 41 finished with value: 0.03442251474894199 and parameters: {'observation_period_num': 15, 'train_rates': 0.9672671219830028, 'learning_rate': 0.0009916267831558596, 'batch_size': 17, 'step_size': 8, 'gamma': 0.7608338417465682}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:19:56,494][0m Trial 42 finished with value: 0.050748063024224306 and parameters: {'observation_period_num': 14, 'train_rates': 0.9584992256004923, 'learning_rate': 0.0009515050047781506, 'batch_size': 24, 'step_size': 8, 'gamma': 0.7653831662427475}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:22:42,071][0m Trial 43 finished with value: 0.052587881684303284 and parameters: {'observation_period_num': 27, 'train_rates': 0.9344990837981945, 'learning_rate': 0.0006344468162475953, 'batch_size': 35, 'step_size': 6, 'gamma': 0.7812427761026945}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:27:09,853][0m Trial 44 finished with value: 0.047785480137703555 and parameters: {'observation_period_num': 6, 'train_rates': 0.9643685673066695, 'learning_rate': 0.0003045159814461719, 'batch_size': 23, 'step_size': 8, 'gamma': 0.7951588029339209}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:29:19,686][0m Trial 45 finished with value: 0.0858686275938724 and parameters: {'observation_period_num': 71, 'train_rates': 0.9186361017257024, 'learning_rate': 0.0006662359630791225, 'batch_size': 44, 'step_size': 9, 'gamma': 0.7501273631525581}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:30:11,138][0m Trial 46 finished with value: 0.05924021080136299 and parameters: {'observation_period_num': 26, 'train_rates': 0.968120987493767, 'learning_rate': 0.00042297936182384246, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8844219848251436}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:35:50,086][0m Trial 47 finished with value: 0.06479491555893963 and parameters: {'observation_period_num': 42, 'train_rates': 0.8794253863836412, 'learning_rate': 1.8081508761928045e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.906303348540779}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:36:57,607][0m Trial 48 finished with value: 0.09360449190522516 and parameters: {'observation_period_num': 17, 'train_rates': 0.694832220614321, 'learning_rate': 0.0007078454545569721, 'batch_size': 76, 'step_size': 11, 'gamma': 0.7729507038323965}. Best is trial 22 with value: 0.02171107940375805.[0m
[32m[I 2025-01-08 03:38:24,874][0m Trial 49 finished with value: 0.09381038650846783 and parameters: {'observation_period_num': 51, 'train_rates': 0.782103130979876, 'learning_rate': 0.000978986121167083, 'batch_size': 61, 'step_size': 9, 'gamma': 0.8213927747715862}. Best is trial 22 with value: 0.02171107940375805.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-08 03:38:24,885][0m A new study created in memory with name: no-name-705c90e9-f9df-4e3e-b2e5-e6f52bbb2b2e[0m
[32m[I 2025-01-08 03:38:54,846][0m Trial 0 finished with value: 0.13814611035440194 and parameters: {'observation_period_num': 165, 'train_rates': 0.6912314609543502, 'learning_rate': 0.00024512808303588275, 'batch_size': 255, 'step_size': 13, 'gamma': 0.9612476148033453}. Best is trial 0 with value: 0.13814611035440194.[0m
[32m[I 2025-01-08 03:39:26,058][0m Trial 1 finished with value: 0.32856711658974797 and parameters: {'observation_period_num': 107, 'train_rates': 0.6848986562813892, 'learning_rate': 4.645277720421705e-05, 'batch_size': 197, 'step_size': 4, 'gamma': 0.8655402970226763}. Best is trial 0 with value: 0.13814611035440194.[0m
[32m[I 2025-01-08 03:39:56,097][0m Trial 2 finished with value: 0.0674230849318885 and parameters: {'observation_period_num': 71, 'train_rates': 0.6611870674907717, 'learning_rate': 0.0007019580206551185, 'batch_size': 240, 'step_size': 4, 'gamma': 0.9833068015188449}. Best is trial 2 with value: 0.0674230849318885.[0m
[32m[I 2025-01-08 03:41:14,677][0m Trial 3 finished with value: 0.05972787736353272 and parameters: {'observation_period_num': 69, 'train_rates': 0.8273024937284996, 'learning_rate': 5.9087330888280045e-05, 'batch_size': 70, 'step_size': 12, 'gamma': 0.9221464480292991}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:42:55,831][0m Trial 4 finished with value: 0.07037546633268302 and parameters: {'observation_period_num': 51, 'train_rates': 0.9637987100939545, 'learning_rate': 0.0005932675875583602, 'batch_size': 65, 'step_size': 10, 'gamma': 0.956532549524757}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:46:26,878][0m Trial 5 finished with value: 0.13273000978143876 and parameters: {'observation_period_num': 188, 'train_rates': 0.7442084998177216, 'learning_rate': 6.229125831319048e-05, 'batch_size': 22, 'step_size': 14, 'gamma': 0.8792527453081491}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:47:09,577][0m Trial 6 finished with value: 0.08265211754723599 and parameters: {'observation_period_num': 32, 'train_rates': 0.8049357545208964, 'learning_rate': 3.3379672632266664e-05, 'batch_size': 136, 'step_size': 7, 'gamma': 0.8530541406641672}. Best is trial 3 with value: 0.05972787736353272.[0m
Early stopping at epoch 73
[32m[I 2025-01-08 03:47:36,465][0m Trial 7 finished with value: 1.8657079139025108 and parameters: {'observation_period_num': 226, 'train_rates': 0.6692547635549589, 'learning_rate': 1.97622318470068e-06, 'batch_size': 134, 'step_size': 1, 'gamma': 0.858468979709816}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:48:15,006][0m Trial 8 finished with value: 1.1958931858896593 and parameters: {'observation_period_num': 50, 'train_rates': 0.8000215894942397, 'learning_rate': 1.0657266320141957e-06, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8620850907038455}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:52:02,562][0m Trial 9 finished with value: 0.5599788473386583 and parameters: {'observation_period_num': 72, 'train_rates': 0.6542997373494461, 'learning_rate': 2.3076162577968743e-06, 'batch_size': 20, 'step_size': 2, 'gamma': 0.7666587086926967}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:53:11,132][0m Trial 10 finished with value: 0.16920254361629486 and parameters: {'observation_period_num': 116, 'train_rates': 0.9119712322800994, 'learning_rate': 1.172320470323528e-05, 'batch_size': 84, 'step_size': 11, 'gamma': 0.9128556411280409}. Best is trial 3 with value: 0.05972787736353272.[0m
[32m[I 2025-01-08 03:53:59,635][0m Trial 11 finished with value: 0.03576085842214525 and parameters: {'observation_period_num': 6, 'train_rates': 0.8645523240795862, 'learning_rate': 0.0008569552333903562, 'batch_size': 246, 'step_size': 6, 'gamma': 0.9894935801469662}. Best is trial 11 with value: 0.03576085842214525.[0m
[32m[I 2025-01-08 03:55:18,724][0m Trial 12 finished with value: 0.028849968400139075 and parameters: {'observation_period_num': 11, 'train_rates': 0.8720495668156174, 'learning_rate': 0.0001792422879174686, 'batch_size': 87, 'step_size': 7, 'gamma': 0.9283168423691336}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 03:56:10,158][0m Trial 13 finished with value: 0.04046277179112358 and parameters: {'observation_period_num': 11, 'train_rates': 0.8949803029203547, 'learning_rate': 0.00020068406431478488, 'batch_size': 183, 'step_size': 7, 'gamma': 0.9229963664676656}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 03:57:19,032][0m Trial 14 finished with value: 0.02907043075462755 and parameters: {'observation_period_num': 13, 'train_rates': 0.8820742498762819, 'learning_rate': 0.00022854513775211917, 'batch_size': 108, 'step_size': 9, 'gamma': 0.9850720775813088}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 03:58:25,254][0m Trial 15 finished with value: 0.08492750183299735 and parameters: {'observation_period_num': 157, 'train_rates': 0.9520359760989638, 'learning_rate': 0.00018777225739125558, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8090452621975245}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 03:59:28,639][0m Trial 16 finished with value: 0.10189352028797471 and parameters: {'observation_period_num': 25, 'train_rates': 0.8563879682942966, 'learning_rate': 1.0585649920726236e-05, 'batch_size': 107, 'step_size': 9, 'gamma': 0.9449095605254447}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:00:31,722][0m Trial 17 finished with value: 0.06370023680872029 and parameters: {'observation_period_num': 97, 'train_rates': 0.753856806896772, 'learning_rate': 0.0001478996888469228, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9095966817424206}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:02:23,825][0m Trial 18 finished with value: 0.11551646888256073 and parameters: {'observation_period_num': 251, 'train_rates': 0.9899512330335605, 'learning_rate': 0.00033345811880305514, 'batch_size': 50, 'step_size': 15, 'gamma': 0.8901413204203511}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:03:11,642][0m Trial 19 finished with value: 0.16619942099994914 and parameters: {'observation_period_num': 142, 'train_rates': 0.6094779774735997, 'learning_rate': 0.00010395065762928055, 'batch_size': 133, 'step_size': 8, 'gamma': 0.9464681868181246}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:05:37,403][0m Trial 20 finished with value: 0.1258053319742445 and parameters: {'observation_period_num': 43, 'train_rates': 0.916318270198232, 'learning_rate': 1.3833194639578829e-05, 'batch_size': 42, 'step_size': 5, 'gamma': 0.8271857533080811}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:06:15,250][0m Trial 21 finished with value: 0.030024074472998846 and parameters: {'observation_period_num': 5, 'train_rates': 0.8668567511794456, 'learning_rate': 0.0009128763792164717, 'batch_size': 218, 'step_size': 6, 'gamma': 0.9869289473539729}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:06:47,243][0m Trial 22 finished with value: 0.029950276472399062 and parameters: {'observation_period_num': 11, 'train_rates': 0.863829539737415, 'learning_rate': 0.00040660571943010445, 'batch_size': 214, 'step_size': 8, 'gamma': 0.9704393359631994}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:07:22,386][0m Trial 23 finished with value: 0.03889706849303704 and parameters: {'observation_period_num': 28, 'train_rates': 0.8366844540066201, 'learning_rate': 0.0003771308472507071, 'batch_size': 170, 'step_size': 10, 'gamma': 0.9685560320167775}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:08:09,417][0m Trial 24 finished with value: 0.06127115803523154 and parameters: {'observation_period_num': 93, 'train_rates': 0.8896031609102889, 'learning_rate': 9.453744286374603e-05, 'batch_size': 123, 'step_size': 8, 'gamma': 0.9377745686690961}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:09:11,107][0m Trial 25 finished with value: 0.0642603786369829 and parameters: {'observation_period_num': 54, 'train_rates': 0.762944453595458, 'learning_rate': 0.0004586030967798659, 'batch_size': 85, 'step_size': 8, 'gamma': 0.9670658272062763}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:09:53,320][0m Trial 26 finished with value: 0.049568776994250545 and parameters: {'observation_period_num': 26, 'train_rates': 0.9413919405575943, 'learning_rate': 0.0001198003238759924, 'batch_size': 150, 'step_size': 11, 'gamma': 0.8921198500084513}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:10:33,224][0m Trial 27 finished with value: 0.06644608758561049 and parameters: {'observation_period_num': 72, 'train_rates': 0.8339340187985337, 'learning_rate': 0.0002850839484487029, 'batch_size': 196, 'step_size': 9, 'gamma': 0.9323296220197704}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:11:29,191][0m Trial 28 finished with value: 0.07987383008003235 and parameters: {'observation_period_num': 18, 'train_rates': 0.9288059468431819, 'learning_rate': 2.7877860005304e-05, 'batch_size': 224, 'step_size': 7, 'gamma': 0.9792423491858029}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:12:24,691][0m Trial 29 finished with value: 0.04921236875429319 and parameters: {'observation_period_num': 35, 'train_rates': 0.8817237877726485, 'learning_rate': 0.0002499152127668613, 'batch_size': 116, 'step_size': 13, 'gamma': 0.9577097266916695}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:13:36,945][0m Trial 30 finished with value: 0.09482534112555258 and parameters: {'observation_period_num': 134, 'train_rates': 0.7791880202211381, 'learning_rate': 0.0005399088128345328, 'batch_size': 84, 'step_size': 10, 'gamma': 0.9587902691260419}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:14:29,045][0m Trial 31 finished with value: 0.03793570597477147 and parameters: {'observation_period_num': 14, 'train_rates': 0.8620676534724995, 'learning_rate': 0.0007493065909017509, 'batch_size': 216, 'step_size': 6, 'gamma': 0.9894620164828888}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:15:19,356][0m Trial 32 finished with value: 0.034848628625983286 and parameters: {'observation_period_num': 6, 'train_rates': 0.8222032925558337, 'learning_rate': 0.000977845760461959, 'batch_size': 208, 'step_size': 5, 'gamma': 0.9771471645932918}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:16:10,156][0m Trial 33 finished with value: 0.06290208107396347 and parameters: {'observation_period_num': 54, 'train_rates': 0.8743775929741534, 'learning_rate': 0.00042769335612850693, 'batch_size': 231, 'step_size': 5, 'gamma': 0.971364831138252}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:16:58,418][0m Trial 34 finished with value: 0.03740906377614879 and parameters: {'observation_period_num': 6, 'train_rates': 0.909171957899669, 'learning_rate': 0.00022953717485820553, 'batch_size': 193, 'step_size': 4, 'gamma': 0.9511560859735538}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:17:35,511][0m Trial 35 finished with value: 0.09909086854972503 and parameters: {'observation_period_num': 36, 'train_rates': 0.8446117883606574, 'learning_rate': 0.0006611327571576429, 'batch_size': 256, 'step_size': 8, 'gamma': 0.989611011974654}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:18:22,167][0m Trial 36 finished with value: 0.07191283833813963 and parameters: {'observation_period_num': 60, 'train_rates': 0.7218515125504461, 'learning_rate': 6.739844617382347e-05, 'batch_size': 167, 'step_size': 7, 'gamma': 0.9285697877911948}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:19:39,709][0m Trial 37 finished with value: 0.10200422260039611 and parameters: {'observation_period_num': 188, 'train_rates': 0.8115946136510703, 'learning_rate': 0.00015725239015230855, 'batch_size': 74, 'step_size': 11, 'gamma': 0.9692651081056265}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:20:45,113][0m Trial 38 finished with value: 0.03145749121904373 and parameters: {'observation_period_num': 21, 'train_rates': 0.9745707528906195, 'learning_rate': 0.0005563503307475463, 'batch_size': 238, 'step_size': 4, 'gamma': 0.9398192294406433}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:21:37,101][0m Trial 39 finished with value: 0.12856034888434656 and parameters: {'observation_period_num': 65, 'train_rates': 0.8993821367700169, 'learning_rate': 4.09562032059201e-05, 'batch_size': 182, 'step_size': 9, 'gamma': 0.9124915296459644}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:23:30,590][0m Trial 40 finished with value: 0.08474172931164503 and parameters: {'observation_period_num': 40, 'train_rates': 0.8518576068692167, 'learning_rate': 0.0002897023762755309, 'batch_size': 54, 'step_size': 6, 'gamma': 0.9582169560386908}. Best is trial 12 with value: 0.028849968400139075.[0m
[32m[I 2025-01-08 04:24:28,562][0m Trial 41 finished with value: 0.026940705254673958 and parameters: {'observation_period_num': 19, 'train_rates': 0.9851873798329772, 'learning_rate': 0.0005373957503477581, 'batch_size': 232, 'step_size': 4, 'gamma': 0.9399604662852258}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:25:20,083][0m Trial 42 finished with value: 0.04415222257375717 and parameters: {'observation_period_num': 19, 'train_rates': 0.9263556171003773, 'learning_rate': 0.0003671866601575085, 'batch_size': 222, 'step_size': 3, 'gamma': 0.9755315100597151}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:26:12,630][0m Trial 43 finished with value: 0.07183168828487396 and parameters: {'observation_period_num': 86, 'train_rates': 0.9513397692930596, 'learning_rate': 0.0009329914053681633, 'batch_size': 207, 'step_size': 4, 'gamma': 0.9503832596991901}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:26:59,604][0m Trial 44 finished with value: 0.04660082062245182 and parameters: {'observation_period_num': 44, 'train_rates': 0.7863275031821603, 'learning_rate': 0.0005642395925927804, 'batch_size': 241, 'step_size': 2, 'gamma': 0.9632114133709611}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:28:17,409][0m Trial 45 finished with value: 0.044860306540099176 and parameters: {'observation_period_num': 5, 'train_rates': 0.8752406614944594, 'learning_rate': 7.649753570192375e-05, 'batch_size': 91, 'step_size': 5, 'gamma': 0.8461106565040968}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:29:14,903][0m Trial 46 finished with value: 0.03710916409233831 and parameters: {'observation_period_num': 32, 'train_rates': 0.8186697153622697, 'learning_rate': 0.0007190504784303053, 'batch_size': 142, 'step_size': 7, 'gamma': 0.9006735257678034}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:30:08,878][0m Trial 47 finished with value: 0.04765443876385689 and parameters: {'observation_period_num': 18, 'train_rates': 0.9710563460657425, 'learning_rate': 0.0001418083867550079, 'batch_size': 209, 'step_size': 3, 'gamma': 0.9785432482082979}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:31:05,462][0m Trial 48 finished with value: 0.06597918825637963 and parameters: {'observation_period_num': 76, 'train_rates': 0.7249455523282547, 'learning_rate': 0.00019659024846421861, 'batch_size': 121, 'step_size': 6, 'gamma': 0.9231722229906806}. Best is trial 41 with value: 0.026940705254673958.[0m
[32m[I 2025-01-08 04:31:53,240][0m Trial 49 finished with value: 0.14741306006908417 and parameters: {'observation_period_num': 186, 'train_rates': 0.9331350147024794, 'learning_rate': 0.000449575368382469, 'batch_size': 248, 'step_size': 10, 'gamma': 0.936246899767854}. Best is trial 41 with value: 0.026940705254673958.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-08 04:31:53,250][0m A new study created in memory with name: no-name-aa90d4a9-b018-4c33-99c1-660f2298d2d2[0m
[32m[I 2025-01-08 04:32:34,566][0m Trial 0 finished with value: 0.7161165320515755 and parameters: {'observation_period_num': 79, 'train_rates': 0.6639897577233889, 'learning_rate': 2.943350440098628e-06, 'batch_size': 231, 'step_size': 15, 'gamma': 0.8742246258054099}. Best is trial 0 with value: 0.7161165320515755.[0m
[32m[I 2025-01-08 04:33:41,520][0m Trial 1 finished with value: 0.7760747348561007 and parameters: {'observation_period_num': 189, 'train_rates': 0.6929105599865262, 'learning_rate': 2.291820354996733e-06, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8081074038390844}. Best is trial 0 with value: 0.7161165320515755.[0m
[32m[I 2025-01-08 04:34:32,172][0m Trial 2 finished with value: 0.17791492313873478 and parameters: {'observation_period_num': 249, 'train_rates': 0.9245704922403131, 'learning_rate': 3.818282151211288e-05, 'batch_size': 157, 'step_size': 13, 'gamma': 0.9825126214550768}. Best is trial 2 with value: 0.17791492313873478.[0m
[32m[I 2025-01-08 04:35:35,849][0m Trial 3 finished with value: 0.7082003426791035 and parameters: {'observation_period_num': 170, 'train_rates': 0.6779447638845554, 'learning_rate': 2.0897678118325234e-06, 'batch_size': 87, 'step_size': 6, 'gamma': 0.8039994380834845}. Best is trial 2 with value: 0.17791492313873478.[0m
[32m[I 2025-01-08 04:36:12,577][0m Trial 4 finished with value: 0.13909028850026187 and parameters: {'observation_period_num': 211, 'train_rates': 0.7860339764694466, 'learning_rate': 0.00013469882749408827, 'batch_size': 219, 'step_size': 9, 'gamma': 0.9277113717017214}. Best is trial 4 with value: 0.13909028850026187.[0m
[32m[I 2025-01-08 04:37:24,306][0m Trial 5 finished with value: 0.16342376229231773 and parameters: {'observation_period_num': 170, 'train_rates': 0.933908521663798, 'learning_rate': 1.9360433611013846e-05, 'batch_size': 93, 'step_size': 8, 'gamma': 0.9326681408583645}. Best is trial 4 with value: 0.13909028850026187.[0m
[32m[I 2025-01-08 04:38:00,141][0m Trial 6 finished with value: 0.3132914136348344 and parameters: {'observation_period_num': 113, 'train_rates': 0.710347343754528, 'learning_rate': 1.9126076634500398e-05, 'batch_size': 216, 'step_size': 7, 'gamma': 0.8783352682370132}. Best is trial 4 with value: 0.13909028850026187.[0m
Early stopping at epoch 78
[32m[I 2025-01-08 04:38:54,601][0m Trial 7 finished with value: 0.15256637334823608 and parameters: {'observation_period_num': 28, 'train_rates': 0.9753438498820122, 'learning_rate': 0.00024979487632881205, 'batch_size': 152, 'step_size': 1, 'gamma': 0.8510738370644741}. Best is trial 4 with value: 0.13909028850026187.[0m
[32m[I 2025-01-08 04:39:43,316][0m Trial 8 finished with value: 0.17408926039934158 and parameters: {'observation_period_num': 241, 'train_rates': 0.7082337397654658, 'learning_rate': 0.0007074553832027839, 'batch_size': 135, 'step_size': 9, 'gamma': 0.788041041727011}. Best is trial 4 with value: 0.13909028850026187.[0m
[32m[I 2025-01-08 04:41:01,697][0m Trial 9 finished with value: 0.18196089884678426 and parameters: {'observation_period_num': 220, 'train_rates': 0.6976842449220041, 'learning_rate': 2.8677104107397795e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.7856422229289304}. Best is trial 4 with value: 0.13909028850026187.[0m
[32m[I 2025-01-08 04:41:44,732][0m Trial 10 finished with value: 0.10272916999789095 and parameters: {'observation_period_num': 127, 'train_rates': 0.8212779278714918, 'learning_rate': 0.00015837305362241972, 'batch_size': 255, 'step_size': 3, 'gamma': 0.9329142802879987}. Best is trial 10 with value: 0.10272916999789095.[0m
[32m[I 2025-01-08 04:42:31,159][0m Trial 11 finished with value: 0.09021832671972503 and parameters: {'observation_period_num': 122, 'train_rates': 0.8404874245482333, 'learning_rate': 0.00014022056137539963, 'batch_size': 254, 'step_size': 3, 'gamma': 0.9345374076026127}. Best is trial 11 with value: 0.09021832671972503.[0m
[32m[I 2025-01-08 04:43:23,218][0m Trial 12 finished with value: 0.1408216718408427 and parameters: {'observation_period_num': 119, 'train_rates': 0.8358703332659542, 'learning_rate': 0.00010596280707554148, 'batch_size': 255, 'step_size': 2, 'gamma': 0.9407052020280081}. Best is trial 11 with value: 0.09021832671972503.[0m
[32m[I 2025-01-08 04:44:05,686][0m Trial 13 finished with value: 0.04750085870669348 and parameters: {'observation_period_num': 67, 'train_rates': 0.824294756001652, 'learning_rate': 0.0005986685796456747, 'batch_size': 191, 'step_size': 4, 'gamma': 0.9873073296031591}. Best is trial 13 with value: 0.04750085870669348.[0m
[32m[I 2025-01-08 04:44:57,476][0m Trial 14 finished with value: 0.0721584465354681 and parameters: {'observation_period_num': 58, 'train_rates': 0.8753328529457082, 'learning_rate': 0.0009292814998994024, 'batch_size': 181, 'step_size': 4, 'gamma': 0.9804356860585706}. Best is trial 13 with value: 0.04750085870669348.[0m
[32m[I 2025-01-08 04:45:56,346][0m Trial 15 finished with value: 0.0356611361856547 and parameters: {'observation_period_num': 28, 'train_rates': 0.8870208630071127, 'learning_rate': 0.0008939696173469893, 'batch_size': 188, 'step_size': 5, 'gamma': 0.9868790426973327}. Best is trial 15 with value: 0.0356611361856547.[0m
[32m[I 2025-01-08 04:48:59,457][0m Trial 16 finished with value: 0.030673947357795002 and parameters: {'observation_period_num': 18, 'train_rates': 0.7648287725630509, 'learning_rate': 0.0004015734111527302, 'batch_size': 31, 'step_size': 5, 'gamma': 0.7503689976151358}. Best is trial 16 with value: 0.030673947357795002.[0m
[32m[I 2025-01-08 04:52:33,077][0m Trial 17 finished with value: 0.07395648401993585 and parameters: {'observation_period_num': 25, 'train_rates': 0.6034585455482331, 'learning_rate': 0.0003581709425058492, 'batch_size': 21, 'step_size': 5, 'gamma': 0.7512973195991646}. Best is trial 16 with value: 0.030673947357795002.[0m
[32m[I 2025-01-08 04:53:31,828][0m Trial 18 finished with value: 0.048306358076183706 and parameters: {'observation_period_num': 11, 'train_rates': 0.7630260901367839, 'learning_rate': 5.739161384980347e-05, 'batch_size': 119, 'step_size': 10, 'gamma': 0.8351902320719419}. Best is trial 16 with value: 0.030673947357795002.[0m
[32m[I 2025-01-08 04:58:13,162][0m Trial 19 finished with value: 0.08350727694712694 and parameters: {'observation_period_num': 46, 'train_rates': 0.7529216619520483, 'learning_rate': 9.031859490025225e-06, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8927513121786672}. Best is trial 16 with value: 0.030673947357795002.[0m
[32m[I 2025-01-08 05:00:43,645][0m Trial 20 finished with value: 0.06382450910463724 and parameters: {'observation_period_num': 85, 'train_rates': 0.8984307960850437, 'learning_rate': 0.00039928220891216246, 'batch_size': 40, 'step_size': 1, 'gamma': 0.9074953502130068}. Best is trial 16 with value: 0.030673947357795002.[0m
[32m[I 2025-01-08 05:01:35,797][0m Trial 21 finished with value: 0.03390098742199361 and parameters: {'observation_period_num': 5, 'train_rates': 0.8739336742505026, 'learning_rate': 0.0005493931133545827, 'batch_size': 179, 'step_size': 5, 'gamma': 0.9881606924942956}. Best is trial 16 with value: 0.030673947357795002.[0m
[32m[I 2025-01-08 05:02:27,966][0m Trial 22 finished with value: 0.030627711604420956 and parameters: {'observation_period_num': 18, 'train_rates': 0.8718751899871693, 'learning_rate': 0.0009182973252354652, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9663862091436941}. Best is trial 22 with value: 0.030627711604420956.[0m
[32m[I 2025-01-08 05:03:14,516][0m Trial 23 finished with value: 0.029860558175005946 and parameters: {'observation_period_num': 11, 'train_rates': 0.8611427714359929, 'learning_rate': 0.00030968857092537557, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9633179998461963}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:04:13,364][0m Trial 24 finished with value: 0.048343723099077904 and parameters: {'observation_period_num': 46, 'train_rates': 0.7868237451310106, 'learning_rate': 0.0002652151772547774, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9559955531191024}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:05:07,231][0m Trial 25 finished with value: 0.0847117081284523 and parameters: {'observation_period_num': 97, 'train_rates': 0.9862251203122382, 'learning_rate': 0.00022824196994380557, 'batch_size': 160, 'step_size': 7, 'gamma': 0.9608664337832005}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:06:01,817][0m Trial 26 finished with value: 0.06294086496246622 and parameters: {'observation_period_num': 42, 'train_rates': 0.7420272422770875, 'learning_rate': 7.578143664442153e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.9097984341889034}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:06:51,296][0m Trial 27 finished with value: 0.042341525270922546 and parameters: {'observation_period_num': 16, 'train_rates': 0.8512366058362687, 'learning_rate': 0.0004541125271986351, 'batch_size': 208, 'step_size': 6, 'gamma': 0.7564396174618013}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:08:45,038][0m Trial 28 finished with value: 0.4810435918874519 and parameters: {'observation_period_num': 58, 'train_rates': 0.9257944848049016, 'learning_rate': 1.1267658016984884e-06, 'batch_size': 56, 'step_size': 8, 'gamma': 0.9579377347324087}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:09:45,991][0m Trial 29 finished with value: 0.13116155605978116 and parameters: {'observation_period_num': 83, 'train_rates': 0.6378284350751844, 'learning_rate': 0.0002124934496681401, 'batch_size': 165, 'step_size': 15, 'gamma': 0.8504319656620145}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:10:56,864][0m Trial 30 finished with value: 0.09087096621160921 and parameters: {'observation_period_num': 5, 'train_rates': 0.8052582994431076, 'learning_rate': 7.3792502914029175e-06, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9148624349216266}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:11:45,429][0m Trial 31 finished with value: 0.031668253900373686 and parameters: {'observation_period_num': 5, 'train_rates': 0.8617818829061396, 'learning_rate': 0.0004961160962183461, 'batch_size': 176, 'step_size': 4, 'gamma': 0.969350240948783}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:12:23,218][0m Trial 32 finished with value: 0.039377205507006754 and parameters: {'observation_period_num': 30, 'train_rates': 0.8679856712263813, 'learning_rate': 0.0003468997961147609, 'batch_size': 201, 'step_size': 3, 'gamma': 0.9677972928321616}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:13:12,266][0m Trial 33 finished with value: 0.03456583105631777 and parameters: {'observation_period_num': 40, 'train_rates': 0.9114385800856584, 'learning_rate': 0.0008949484813106247, 'batch_size': 168, 'step_size': 4, 'gamma': 0.953226366452832}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:13:59,297][0m Trial 34 finished with value: 0.18941693007946014 and parameters: {'observation_period_num': 149, 'train_rates': 0.9537290405844308, 'learning_rate': 0.0005742336217872341, 'batch_size': 147, 'step_size': 6, 'gamma': 0.9713740046829632}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:14:45,237][0m Trial 35 finished with value: 0.05208374045515826 and parameters: {'observation_period_num': 18, 'train_rates': 0.8516369462665905, 'learning_rate': 8.441368502409522e-05, 'batch_size': 230, 'step_size': 7, 'gamma': 0.9486385180554195}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:15:30,969][0m Trial 36 finished with value: 0.10360175929632746 and parameters: {'observation_period_num': 70, 'train_rates': 0.8050053822880188, 'learning_rate': 0.00029016487713863336, 'batch_size': 173, 'step_size': 2, 'gamma': 0.8285623055760333}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:16:22,584][0m Trial 37 finished with value: 0.0953253155755924 and parameters: {'observation_period_num': 101, 'train_rates': 0.7702076013140909, 'learning_rate': 4.995459028715039e-05, 'batch_size': 143, 'step_size': 6, 'gamma': 0.9190170253488994}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:17:05,542][0m Trial 38 finished with value: 0.051458595037924174 and parameters: {'observation_period_num': 38, 'train_rates': 0.7357940961565134, 'learning_rate': 0.00018748469033897424, 'batch_size': 192, 'step_size': 9, 'gamma': 0.9008371617577232}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:17:45,535][0m Trial 39 finished with value: 0.056143831461668015 and parameters: {'observation_period_num': 56, 'train_rates': 0.9448325910875421, 'learning_rate': 0.0006289344141671807, 'batch_size': 223, 'step_size': 7, 'gamma': 0.8739436400120895}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:18:34,298][0m Trial 40 finished with value: 0.08373731252460079 and parameters: {'observation_period_num': 147, 'train_rates': 0.906834833884499, 'learning_rate': 0.00042213854025103143, 'batch_size': 125, 'step_size': 4, 'gamma': 0.7693855757681871}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:19:14,649][0m Trial 41 finished with value: 0.031113332107422814 and parameters: {'observation_period_num': 15, 'train_rates': 0.8737084247022622, 'learning_rate': 0.0005847072708552867, 'batch_size': 178, 'step_size': 5, 'gamma': 0.9740716093496673}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:20:03,072][0m Trial 42 finished with value: 0.03401780524177522 and parameters: {'observation_period_num': 20, 'train_rates': 0.8604467377201502, 'learning_rate': 0.000988902056635805, 'batch_size': 204, 'step_size': 5, 'gamma': 0.9718081635721677}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:20:51,736][0m Trial 43 finished with value: 0.03153787701990378 and parameters: {'observation_period_num': 5, 'train_rates': 0.885898318024496, 'learning_rate': 0.0006637297929587842, 'batch_size': 158, 'step_size': 7, 'gamma': 0.9451951848952869}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:21:34,580][0m Trial 44 finished with value: 0.036122493999951245 and parameters: {'observation_period_num': 20, 'train_rates': 0.8923331797720899, 'learning_rate': 0.0007215493060276639, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9471367873064508}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:22:22,334][0m Trial 45 finished with value: 0.03531201402757765 and parameters: {'observation_period_num': 32, 'train_rates': 0.8267165337562631, 'learning_rate': 0.0003471495663263866, 'batch_size': 158, 'step_size': 8, 'gamma': 0.8883934556458849}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:23:39,980][0m Trial 46 finished with value: 0.05559418895622579 and parameters: {'observation_period_num': 51, 'train_rates': 0.9576590629405959, 'learning_rate': 0.00017361823123335725, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8078004871299962}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:24:43,307][0m Trial 47 finished with value: 0.03103590101250969 and parameters: {'observation_period_num': 18, 'train_rates': 0.9181901193586222, 'learning_rate': 0.0007090011272111296, 'batch_size': 154, 'step_size': 7, 'gamma': 0.9234953798354012}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:25:34,916][0m Trial 48 finished with value: 0.0817488357424736 and parameters: {'observation_period_num': 71, 'train_rates': 0.9204436642507328, 'learning_rate': 0.00029065071545690557, 'batch_size': 240, 'step_size': 6, 'gamma': 0.9270044283183109}. Best is trial 23 with value: 0.029860558175005946.[0m
[32m[I 2025-01-08 05:26:15,309][0m Trial 49 finished with value: 0.0674262572720907 and parameters: {'observation_period_num': 34, 'train_rates': 0.7253835015927723, 'learning_rate': 0.00012255206400806103, 'batch_size': 197, 'step_size': 8, 'gamma': 0.8616380858848822}. Best is trial 23 with value: 0.029860558175005946.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-08 05:26:15,321][0m A new study created in memory with name: no-name-e4e53fed-2192-4bd1-986c-2bb6acd2763c[0m
[32m[I 2025-01-08 05:26:57,872][0m Trial 0 finished with value: 0.10178081689735213 and parameters: {'observation_period_num': 16, 'train_rates': 0.6685153933395724, 'learning_rate': 6.65325898965595e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.8608453447545292}. Best is trial 0 with value: 0.10178081689735213.[0m
[32m[I 2025-01-08 05:27:36,961][0m Trial 1 finished with value: 0.5739591662836547 and parameters: {'observation_period_num': 50, 'train_rates': 0.8261750015174272, 'learning_rate': 1.6888386858713994e-06, 'batch_size': 159, 'step_size': 11, 'gamma': 0.8138331646615521}. Best is trial 0 with value: 0.10178081689735213.[0m
[32m[I 2025-01-08 05:28:07,455][0m Trial 2 finished with value: 0.172800332698239 and parameters: {'observation_period_num': 219, 'train_rates': 0.7490532912180131, 'learning_rate': 0.000405449702704014, 'batch_size': 248, 'step_size': 8, 'gamma': 0.9794889823262074}. Best is trial 0 with value: 0.10178081689735213.[0m
[32m[I 2025-01-08 05:28:55,079][0m Trial 3 finished with value: 0.25860172488233624 and parameters: {'observation_period_num': 179, 'train_rates': 0.6325874174451958, 'learning_rate': 0.00026730796891460403, 'batch_size': 95, 'step_size': 1, 'gamma': 0.9046417380051299}. Best is trial 0 with value: 0.10178081689735213.[0m
Early stopping at epoch 49
[32m[I 2025-01-08 05:29:24,226][0m Trial 4 finished with value: 1.0734088190099136 and parameters: {'observation_period_num': 156, 'train_rates': 0.666081625577854, 'learning_rate': 3.951437911526282e-06, 'batch_size': 92, 'step_size': 1, 'gamma': 0.7825746960746418}. Best is trial 0 with value: 0.10178081689735213.[0m
[32m[I 2025-01-08 05:30:19,698][0m Trial 5 finished with value: 0.1625611735620962 and parameters: {'observation_period_num': 124, 'train_rates': 0.8256470441639947, 'learning_rate': 2.464516816334926e-05, 'batch_size': 162, 'step_size': 15, 'gamma': 0.8332204463667121}. Best is trial 0 with value: 0.10178081689735213.[0m
[32m[I 2025-01-08 05:33:01,897][0m Trial 6 finished with value: 0.07401613681035679 and parameters: {'observation_period_num': 158, 'train_rates': 0.8770952133154954, 'learning_rate': 0.0002201181071254209, 'batch_size': 34, 'step_size': 4, 'gamma': 0.8016961465856437}. Best is trial 6 with value: 0.07401613681035679.[0m
[32m[I 2025-01-08 05:33:36,627][0m Trial 7 finished with value: 0.6738392819473349 and parameters: {'observation_period_num': 79, 'train_rates': 0.6599593356083828, 'learning_rate': 9.264950845586181e-06, 'batch_size': 141, 'step_size': 3, 'gamma': 0.7816124460086185}. Best is trial 6 with value: 0.07401613681035679.[0m
[32m[I 2025-01-08 05:34:59,734][0m Trial 8 finished with value: 0.26736704973074105 and parameters: {'observation_period_num': 152, 'train_rates': 0.8841170400271806, 'learning_rate': 2.0450728880992494e-05, 'batch_size': 64, 'step_size': 4, 'gamma': 0.7762942494835632}. Best is trial 6 with value: 0.07401613681035679.[0m
Early stopping at epoch 67
[32m[I 2025-01-08 05:35:27,523][0m Trial 9 finished with value: 1.7695760720057858 and parameters: {'observation_period_num': 247, 'train_rates': 0.6155049212013399, 'learning_rate': 5.944232947767668e-06, 'batch_size': 110, 'step_size': 1, 'gamma': 0.8340999747708703}. Best is trial 6 with value: 0.07401613681035679.[0m
[32m[I 2025-01-08 05:39:00,150][0m Trial 10 finished with value: 0.2672777185516973 and parameters: {'observation_period_num': 106, 'train_rates': 0.989348429877416, 'learning_rate': 0.0009724645067753308, 'batch_size': 27, 'step_size': 6, 'gamma': 0.9059773974577134}. Best is trial 6 with value: 0.07401613681035679.[0m
[32m[I 2025-01-08 05:44:15,805][0m Trial 11 finished with value: 0.026488004717975855 and parameters: {'observation_period_num': 9, 'train_rates': 0.9188073512758619, 'learning_rate': 9.953911577050109e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8833149884261864}. Best is trial 11 with value: 0.026488004717975855.[0m
[32m[I 2025-01-08 05:50:04,031][0m Trial 12 finished with value: 0.03844441523796868 and parameters: {'observation_period_num': 21, 'train_rates': 0.9409697726332781, 'learning_rate': 0.00011106625638214904, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9074284908194867}. Best is trial 11 with value: 0.026488004717975855.[0m
[32m[I 2025-01-08 05:54:41,871][0m Trial 13 finished with value: 0.018736638581833324 and parameters: {'observation_period_num': 6, 'train_rates': 0.9751541628080517, 'learning_rate': 8.676292231311497e-05, 'batch_size': 22, 'step_size': 7, 'gamma': 0.9305264156335673}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 05:56:41,133][0m Trial 14 finished with value: 0.05527924373745918 and parameters: {'observation_period_num': 54, 'train_rates': 0.9876590500715606, 'learning_rate': 6.009965761658517e-05, 'batch_size': 60, 'step_size': 11, 'gamma': 0.9622510951876487}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 05:57:29,028][0m Trial 15 finished with value: 0.04274639643208925 and parameters: {'observation_period_num': 8, 'train_rates': 0.9125834593744138, 'learning_rate': 0.00011289593817858341, 'batch_size': 204, 'step_size': 9, 'gamma': 0.9391524468354994}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 05:59:20,002][0m Trial 16 finished with value: 0.06015575769193032 and parameters: {'observation_period_num': 47, 'train_rates': 0.9418451373904215, 'learning_rate': 3.8853512333029885e-05, 'batch_size': 54, 'step_size': 6, 'gamma': 0.8726882271192107}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:00:53,984][0m Trial 17 finished with value: 0.09547542967928507 and parameters: {'observation_period_num': 82, 'train_rates': 0.7474510380068526, 'learning_rate': 0.0005405715479138357, 'batch_size': 61, 'step_size': 8, 'gamma': 0.9382292848106824}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:05:58,846][0m Trial 18 finished with value: 0.04012069130754646 and parameters: {'observation_period_num': 36, 'train_rates': 0.8605135763636579, 'learning_rate': 0.00014419230756971462, 'batch_size': 18, 'step_size': 10, 'gamma': 0.8702006374712126}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:07:31,202][0m Trial 19 finished with value: 0.08896311556364035 and parameters: {'observation_period_num': 76, 'train_rates': 0.9465689575457903, 'learning_rate': 2.272971606138213e-05, 'batch_size': 76, 'step_size': 14, 'gamma': 0.9246862195849977}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:09:42,963][0m Trial 20 finished with value: 0.07274734976192924 and parameters: {'observation_period_num': 112, 'train_rates': 0.7708315717334083, 'learning_rate': 5.5906599765648804e-05, 'batch_size': 42, 'step_size': 5, 'gamma': 0.886694963834622}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:15:36,128][0m Trial 21 finished with value: 0.03717073705047369 and parameters: {'observation_period_num': 23, 'train_rates': 0.9420319742931055, 'learning_rate': 0.00011409169913892015, 'batch_size': 17, 'step_size': 7, 'gamma': 0.9081917445982151}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:18:21,200][0m Trial 22 finished with value: 0.02495879903010769 and parameters: {'observation_period_num': 7, 'train_rates': 0.91096040501199, 'learning_rate': 0.0001498107849883427, 'batch_size': 36, 'step_size': 7, 'gamma': 0.956563518968286}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:20:49,016][0m Trial 23 finished with value: 0.02587110730432754 and parameters: {'observation_period_num': 6, 'train_rates': 0.9061660545362845, 'learning_rate': 0.0002207997634697184, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9613512367938609}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:22:04,922][0m Trial 24 finished with value: 0.06818826045687441 and parameters: {'observation_period_num': 67, 'train_rates': 0.8406227298018278, 'learning_rate': 0.00024061623215969056, 'batch_size': 87, 'step_size': 9, 'gamma': 0.9889288080609676}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:24:19,718][0m Trial 25 finished with value: 0.05546631997353152 and parameters: {'observation_period_num': 36, 'train_rates': 0.9026030321071198, 'learning_rate': 0.0006154968351987857, 'batch_size': 45, 'step_size': 7, 'gamma': 0.9600394275869326}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:26:52,794][0m Trial 26 finished with value: 0.04554995856027013 and parameters: {'observation_period_num': 32, 'train_rates': 0.9670104962338888, 'learning_rate': 0.0003707591653308343, 'batch_size': 44, 'step_size': 12, 'gamma': 0.9564345384609492}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:28:15,185][0m Trial 27 finished with value: 0.07535067711868425 and parameters: {'observation_period_num': 95, 'train_rates': 0.8085721030533914, 'learning_rate': 0.00017806180935525537, 'batch_size': 76, 'step_size': 7, 'gamma': 0.9420626066519285}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:29:23,422][0m Trial 28 finished with value: 0.048442485672769264 and parameters: {'observation_period_num': 5, 'train_rates': 0.8637598054952775, 'learning_rate': 4.229053283202821e-05, 'batch_size': 116, 'step_size': 10, 'gamma': 0.9730136057892316}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:30:15,767][0m Trial 29 finished with value: 0.1073536935684113 and parameters: {'observation_period_num': 60, 'train_rates': 0.707111306498125, 'learning_rate': 6.90060883955179e-05, 'batch_size': 193, 'step_size': 3, 'gamma': 0.9263569064536775}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:31:46,129][0m Trial 30 finished with value: 0.18737955513049145 and parameters: {'observation_period_num': 32, 'train_rates': 0.9664948288232513, 'learning_rate': 1.2068433744067475e-05, 'batch_size': 74, 'step_size': 9, 'gamma': 0.7519837900255152}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:34:46,491][0m Trial 31 finished with value: 0.02748920589237285 and parameters: {'observation_period_num': 6, 'train_rates': 0.9100808100119289, 'learning_rate': 8.755768866912344e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.8879643871383555}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:37:54,462][0m Trial 32 finished with value: 0.028686974739351594 and parameters: {'observation_period_num': 20, 'train_rates': 0.9146392182431018, 'learning_rate': 0.00017207093622651146, 'batch_size': 32, 'step_size': 5, 'gamma': 0.8464143775083373}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:39:52,257][0m Trial 33 finished with value: 0.2948609536773042 and parameters: {'observation_period_num': 19, 'train_rates': 0.8923389912077506, 'learning_rate': 1.0872700821285475e-06, 'batch_size': 50, 'step_size': 7, 'gamma': 0.922470258935177}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:40:33,641][0m Trial 34 finished with value: 0.08061036467552185 and parameters: {'observation_period_num': 50, 'train_rates': 0.9273505972605318, 'learning_rate': 0.00024576665327058896, 'batch_size': 255, 'step_size': 8, 'gamma': 0.9525856609689843}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:44:03,971][0m Trial 35 finished with value: 0.12961038917002052 and parameters: {'observation_period_num': 205, 'train_rates': 0.9640084193389808, 'learning_rate': 0.0003641207038987951, 'batch_size': 26, 'step_size': 4, 'gamma': 0.9729625407768278}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:45:02,245][0m Trial 36 finished with value: 0.04297969523598166 and parameters: {'observation_period_num': 44, 'train_rates': 0.8539684061316667, 'learning_rate': 8.754782038978147e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.8854621761631601}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:45:52,435][0m Trial 37 finished with value: 0.047701060444720694 and parameters: {'observation_period_num': 16, 'train_rates': 0.7987159802578708, 'learning_rate': 3.484805905767173e-05, 'batch_size': 130, 'step_size': 8, 'gamma': 0.9886897239888895}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:48:10,630][0m Trial 38 finished with value: 0.023212134348924714 and parameters: {'observation_period_num': 5, 'train_rates': 0.8323720421387484, 'learning_rate': 0.0003333973147708023, 'batch_size': 40, 'step_size': 3, 'gamma': 0.9474581523583232}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:48:48,719][0m Trial 39 finished with value: 0.047063255462806486 and parameters: {'observation_period_num': 30, 'train_rates': 0.7682509349517191, 'learning_rate': 0.0009066369076889331, 'batch_size': 227, 'step_size': 2, 'gamma': 0.942828101884826}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:49:42,838][0m Trial 40 finished with value: 0.08548786709183141 and parameters: {'observation_period_num': 143, 'train_rates': 0.831132110682167, 'learning_rate': 0.0005361308432658076, 'batch_size': 148, 'step_size': 3, 'gamma': 0.9679585994700569}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:52:12,292][0m Trial 41 finished with value: 0.02535691048881152 and parameters: {'observation_period_num': 5, 'train_rates': 0.8715971135362295, 'learning_rate': 0.00028377488613493814, 'batch_size': 38, 'step_size': 4, 'gamma': 0.9251892280665289}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:54:34,183][0m Trial 42 finished with value: 0.05339032204614745 and parameters: {'observation_period_num': 41, 'train_rates': 0.8796109649030318, 'learning_rate': 0.00032600943745038347, 'batch_size': 39, 'step_size': 2, 'gamma': 0.9508098946288777}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:56:02,118][0m Trial 43 finished with value: 0.03406529745574613 and parameters: {'observation_period_num': 24, 'train_rates': 0.8189289530075151, 'learning_rate': 0.0001654712176347461, 'batch_size': 65, 'step_size': 4, 'gamma': 0.9300934981613551}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 06:57:58,172][0m Trial 44 finished with value: 0.023897006650438566 and parameters: {'observation_period_num': 8, 'train_rates': 0.8867247947638762, 'learning_rate': 0.0002828934953182919, 'batch_size': 51, 'step_size': 3, 'gamma': 0.9167105226161991}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 07:00:51,369][0m Trial 45 finished with value: 0.05799115416222969 and parameters: {'observation_period_num': 65, 'train_rates': 0.8438150779275928, 'learning_rate': 0.0004713214994835281, 'batch_size': 32, 'step_size': 1, 'gamma': 0.9172645521042317}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 07:02:03,802][0m Trial 46 finished with value: 0.0891570284248826 and parameters: {'observation_period_num': 179, 'train_rates': 0.879836628567044, 'learning_rate': 0.0003378140336836467, 'batch_size': 86, 'step_size': 3, 'gamma': 0.898068531726659}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 07:03:57,561][0m Trial 47 finished with value: 0.024015148764608252 and parameters: {'observation_period_num': 16, 'train_rates': 0.8610233891142927, 'learning_rate': 0.000701350744613375, 'batch_size': 54, 'step_size': 2, 'gamma': 0.9162952229843375}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 07:05:42,923][0m Trial 48 finished with value: 0.026177270223566314 and parameters: {'observation_period_num': 16, 'train_rates': 0.8450179254635414, 'learning_rate': 0.00075646394857941, 'batch_size': 56, 'step_size': 2, 'gamma': 0.9150842169111547}. Best is trial 13 with value: 0.018736638581833324.[0m
[32m[I 2025-01-08 07:07:14,360][0m Trial 49 finished with value: 0.050412056664117987 and parameters: {'observation_period_num': 54, 'train_rates': 0.892116444800453, 'learning_rate': 0.0006145160158816211, 'batch_size': 67, 'step_size': 1, 'gamma': 0.9340195574982241}. Best is trial 13 with value: 0.018736638581833324.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-08 07:07:14,370][0m A new study created in memory with name: no-name-e34e00ab-0974-466a-a8ba-e2b970f42a0a[0m
[32m[I 2025-01-08 07:07:56,782][0m Trial 0 finished with value: 0.6365872025489807 and parameters: {'observation_period_num': 232, 'train_rates': 0.9469382872727037, 'learning_rate': 5.281091859682658e-06, 'batch_size': 199, 'step_size': 9, 'gamma': 0.8764226054612672}. Best is trial 0 with value: 0.6365872025489807.[0m
[32m[I 2025-01-08 07:11:03,065][0m Trial 1 finished with value: 0.21346539842045825 and parameters: {'observation_period_num': 249, 'train_rates': 0.7875340055626743, 'learning_rate': 0.00019393851791039106, 'batch_size': 26, 'step_size': 7, 'gamma': 0.9408291189291007}. Best is trial 1 with value: 0.21346539842045825.[0m
[32m[I 2025-01-08 07:11:41,443][0m Trial 2 finished with value: 0.24984792887685492 and parameters: {'observation_period_num': 142, 'train_rates': 0.690477199344556, 'learning_rate': 4.046690605203708e-05, 'batch_size': 198, 'step_size': 5, 'gamma': 0.9537731514137022}. Best is trial 1 with value: 0.21346539842045825.[0m
[32m[I 2025-01-08 07:12:57,118][0m Trial 3 finished with value: 0.2656525544333182 and parameters: {'observation_period_num': 203, 'train_rates': 0.7176744514169424, 'learning_rate': 1.6050413566284206e-05, 'batch_size': 67, 'step_size': 8, 'gamma': 0.9011589364406958}. Best is trial 1 with value: 0.21346539842045825.[0m
Early stopping at epoch 71
[32m[I 2025-01-08 07:14:18,455][0m Trial 4 finished with value: 1.2458811123285267 and parameters: {'observation_period_num': 46, 'train_rates': 0.6276725963960657, 'learning_rate': 1.9922089870428296e-06, 'batch_size': 40, 'step_size': 1, 'gamma': 0.832941067644183}. Best is trial 1 with value: 0.21346539842045825.[0m
[32m[I 2025-01-08 07:15:50,872][0m Trial 5 finished with value: 0.08928758399167532 and parameters: {'observation_period_num': 109, 'train_rates': 0.9252390988231471, 'learning_rate': 3.1426217847258414e-05, 'batch_size': 67, 'step_size': 8, 'gamma': 0.9304918828200646}. Best is trial 5 with value: 0.08928758399167532.[0m
[32m[I 2025-01-08 07:19:54,219][0m Trial 6 finished with value: 0.03534130529589849 and parameters: {'observation_period_num': 17, 'train_rates': 0.8759877422529521, 'learning_rate': 2.3394404203830285e-05, 'batch_size': 23, 'step_size': 3, 'gamma': 0.976349064058236}. Best is trial 6 with value: 0.03534130529589849.[0m
[32m[I 2025-01-08 07:20:49,501][0m Trial 7 finished with value: 0.20841226734412022 and parameters: {'observation_period_num': 86, 'train_rates': 0.8642096076635508, 'learning_rate': 7.070112253597606e-05, 'batch_size': 127, 'step_size': 2, 'gamma': 0.8385117370770417}. Best is trial 6 with value: 0.03534130529589849.[0m
[32m[I 2025-01-08 07:21:33,970][0m Trial 8 finished with value: 0.3295361728834403 and parameters: {'observation_period_num': 57, 'train_rates': 0.7426576238408188, 'learning_rate': 2.1700130409660158e-05, 'batch_size': 195, 'step_size': 2, 'gamma': 0.9481330345583311}. Best is trial 6 with value: 0.03534130529589849.[0m
[32m[I 2025-01-08 07:22:34,776][0m Trial 9 finished with value: 0.07298332291344801 and parameters: {'observation_period_num': 123, 'train_rates': 0.8942471997150996, 'learning_rate': 0.00014327749243794614, 'batch_size': 110, 'step_size': 8, 'gamma': 0.7804525667261901}. Best is trial 6 with value: 0.03534130529589849.[0m
[32m[I 2025-01-08 07:23:21,912][0m Trial 10 finished with value: 0.04734441265463829 and parameters: {'observation_period_num': 33, 'train_rates': 0.989343776371933, 'learning_rate': 0.0005705332003556189, 'batch_size': 247, 'step_size': 14, 'gamma': 0.9891097702583121}. Best is trial 6 with value: 0.03534130529589849.[0m
[32m[I 2025-01-08 07:24:19,289][0m Trial 11 finished with value: 0.03312167897820473 and parameters: {'observation_period_num': 11, 'train_rates': 0.9850770846801137, 'learning_rate': 0.0009912670957663143, 'batch_size': 250, 'step_size': 14, 'gamma': 0.9748828441701076}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:25:15,606][0m Trial 12 finished with value: 0.03910875136291519 and parameters: {'observation_period_num': 14, 'train_rates': 0.8508672205954129, 'learning_rate': 0.0007677366733219441, 'batch_size': 242, 'step_size': 15, 'gamma': 0.9891517412218869}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:26:12,410][0m Trial 13 finished with value: 0.15945610404014587 and parameters: {'observation_period_num': 6, 'train_rates': 0.9667385114143179, 'learning_rate': 6.450612908334235e-06, 'batch_size': 165, 'step_size': 12, 'gamma': 0.9059674572657122}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:27:15,210][0m Trial 14 finished with value: 0.06744351878938243 and parameters: {'observation_period_num': 76, 'train_rates': 0.8237331069230345, 'learning_rate': 0.000247529522697819, 'batch_size': 101, 'step_size': 11, 'gamma': 0.9697979555802435}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:28:05,023][0m Trial 15 finished with value: 0.40109489199727083 and parameters: {'observation_period_num': 187, 'train_rates': 0.9070063976009191, 'learning_rate': 9.1493595085801e-06, 'batch_size': 152, 'step_size': 5, 'gamma': 0.9180378962484911}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:28:47,947][0m Trial 16 finished with value: 1.9306871437298758 and parameters: {'observation_period_num': 159, 'train_rates': 0.8007220489455724, 'learning_rate': 1.0654741752790093e-06, 'batch_size': 222, 'step_size': 4, 'gamma': 0.7505248326682166}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:30:14,952][0m Trial 17 finished with value: 0.08963796056129715 and parameters: {'observation_period_num': 87, 'train_rates': 0.8850862042384907, 'learning_rate': 7.984426908590802e-05, 'batch_size': 70, 'step_size': 11, 'gamma': 0.8710736162356668}. Best is trial 11 with value: 0.03312167897820473.[0m
[32m[I 2025-01-08 07:31:07,496][0m Trial 18 finished with value: 0.02867264859378338 and parameters: {'observation_period_num': 22, 'train_rates': 0.987865703435391, 'learning_rate': 0.00037351758510321845, 'batch_size': 168, 'step_size': 13, 'gamma': 0.9704531706966553}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:31:57,316][0m Trial 19 finished with value: 0.06423995643854141 and parameters: {'observation_period_num': 52, 'train_rates': 0.9834913581822566, 'learning_rate': 0.000410174018784209, 'batch_size': 172, 'step_size': 13, 'gamma': 0.8396510451148947}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:32:44,724][0m Trial 20 finished with value: 0.047736361622810364 and parameters: {'observation_period_num': 35, 'train_rates': 0.9372221962811951, 'learning_rate': 0.0009777916529576675, 'batch_size': 226, 'step_size': 15, 'gamma': 0.965336253670398}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:33:40,533][0m Trial 21 finished with value: 0.03852046893702613 and parameters: {'observation_period_num': 5, 'train_rates': 0.9392477622567048, 'learning_rate': 0.00033558582146127737, 'batch_size': 133, 'step_size': 10, 'gamma': 0.9761252618738431}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:34:53,530][0m Trial 22 finished with value: 0.0422234331342307 and parameters: {'observation_period_num': 23, 'train_rates': 0.9625593571256956, 'learning_rate': 0.00012771692745535803, 'batch_size': 93, 'step_size': 12, 'gamma': 0.9353218177994611}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:35:33,152][0m Trial 23 finished with value: 0.06528821983887498 and parameters: {'observation_period_num': 59, 'train_rates': 0.843401576793146, 'learning_rate': 8.356620382927034e-05, 'batch_size': 255, 'step_size': 13, 'gamma': 0.9626029278231216}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:36:22,321][0m Trial 24 finished with value: 0.034313487942410764 and parameters: {'observation_period_num': 30, 'train_rates': 0.9089454174534503, 'learning_rate': 0.0004947102459177375, 'batch_size': 175, 'step_size': 6, 'gamma': 0.899119194305612}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:37:10,933][0m Trial 25 finished with value: 0.06218147839177145 and parameters: {'observation_period_num': 71, 'train_rates': 0.9243346312343834, 'learning_rate': 0.00043348914031628524, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9146606219256017}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:37:59,935][0m Trial 26 finished with value: 0.03526080399751663 and parameters: {'observation_period_num': 37, 'train_rates': 0.9863319135964729, 'learning_rate': 0.0008300601317058796, 'batch_size': 217, 'step_size': 6, 'gamma': 0.822576452528309}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:38:47,677][0m Trial 27 finished with value: 0.11191468153681074 and parameters: {'observation_period_num': 105, 'train_rates': 0.9092308760360805, 'learning_rate': 0.0002710464990552332, 'batch_size': 151, 'step_size': 14, 'gamma': 0.8800215261639381}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:39:34,741][0m Trial 28 finished with value: 0.04181293025612831 and parameters: {'observation_period_num': 26, 'train_rates': 0.9595979725278567, 'learning_rate': 0.0005826226970748913, 'batch_size': 177, 'step_size': 10, 'gamma': 0.8901761958809894}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:40:20,922][0m Trial 29 finished with value: 0.11621470749378204 and parameters: {'observation_period_num': 71, 'train_rates': 0.951303567193697, 'learning_rate': 0.0001651951155959167, 'batch_size': 208, 'step_size': 9, 'gamma': 0.8554714521703816}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:41:09,136][0m Trial 30 finished with value: 0.0685122987011817 and parameters: {'observation_period_num': 39, 'train_rates': 0.9187914434184674, 'learning_rate': 0.00044211060802217195, 'batch_size': 149, 'step_size': 14, 'gamma': 0.9256402018479105}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:41:53,390][0m Trial 31 finished with value: 0.04502999782562256 and parameters: {'observation_period_num': 28, 'train_rates': 0.9826152622668614, 'learning_rate': 0.0007834710451164278, 'batch_size': 222, 'step_size': 6, 'gamma': 0.8026641783851395}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:42:40,381][0m Trial 32 finished with value: 0.039724159985780716 and parameters: {'observation_period_num': 43, 'train_rates': 0.9870319514279877, 'learning_rate': 0.0008503812086011449, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8112931110380657}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:43:25,373][0m Trial 33 finished with value: 0.038458045572042465 and parameters: {'observation_period_num': 7, 'train_rates': 0.9540226195864694, 'learning_rate': 0.0009997430467560387, 'batch_size': 204, 'step_size': 6, 'gamma': 0.856457091884701}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:44:09,617][0m Trial 34 finished with value: 0.1559295654296875 and parameters: {'observation_period_num': 250, 'train_rates': 0.9661334449463229, 'learning_rate': 0.00022889475087827925, 'batch_size': 232, 'step_size': 4, 'gamma': 0.9454737793176351}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:44:44,724][0m Trial 35 finished with value: 0.10345872403349841 and parameters: {'observation_period_num': 55, 'train_rates': 0.6186378420064511, 'learning_rate': 0.0005553842116642254, 'batch_size': 213, 'step_size': 7, 'gamma': 0.8127590084052925}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:45:45,271][0m Trial 36 finished with value: 0.17619389295578003 and parameters: {'observation_period_num': 226, 'train_rates': 0.9352513781359844, 'learning_rate': 0.0002993693213879171, 'batch_size': 195, 'step_size': 5, 'gamma': 0.7872042829452518}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:46:25,368][0m Trial 37 finished with value: 0.11757353197616588 and parameters: {'observation_period_num': 163, 'train_rates': 0.7518156463044056, 'learning_rate': 0.0006410248813553772, 'batch_size': 235, 'step_size': 9, 'gamma': 0.856304487707412}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:47:05,812][0m Trial 38 finished with value: 0.07040204173599847 and parameters: {'observation_period_num': 20, 'train_rates': 0.6953785221421872, 'learning_rate': 0.00010942334091999076, 'batch_size': 187, 'step_size': 4, 'gamma': 0.8939467820797817}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:47:43,589][0m Trial 39 finished with value: 0.08908400586205384 and parameters: {'observation_period_num': 100, 'train_rates': 0.6525392534013492, 'learning_rate': 0.0003546423139239301, 'batch_size': 214, 'step_size': 7, 'gamma': 0.9434316971854636}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:48:26,477][0m Trial 40 finished with value: 0.14015110999433175 and parameters: {'observation_period_num': 131, 'train_rates': 0.8795382325754095, 'learning_rate': 4.935504932816769e-05, 'batch_size': 254, 'step_size': 13, 'gamma': 0.8260362578495622}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:52:03,332][0m Trial 41 finished with value: 0.04657819825639524 and parameters: {'observation_period_num': 20, 'train_rates': 0.8625832292398647, 'learning_rate': 1.708270526949444e-05, 'batch_size': 26, 'step_size': 3, 'gamma': 0.9586680448831383}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:57:42,663][0m Trial 42 finished with value: 0.046708424684434834 and parameters: {'observation_period_num': 45, 'train_rates': 0.8265028565130513, 'learning_rate': 2.65493222470186e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9768933535804415}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 07:58:17,658][0m Trial 43 finished with value: 0.05752401095845842 and parameters: {'observation_period_num': 33, 'train_rates': 0.8968769735022379, 'learning_rate': 4.261096265848088e-05, 'batch_size': 157, 'step_size': 3, 'gamma': 0.9814046831094114}. Best is trial 18 with value: 0.02867264859378338.[0m
[32m[I 2025-01-08 08:00:01,399][0m Trial 44 finished with value: 0.02730521226171837 and parameters: {'observation_period_num': 12, 'train_rates': 0.9710425047982052, 'learning_rate': 0.0001887555085996004, 'batch_size': 56, 'step_size': 2, 'gamma': 0.955878247149972}. Best is trial 44 with value: 0.02730521226171837.[0m
[32m[I 2025-01-08 08:00:45,574][0m Trial 45 finished with value: 0.050436340272426605 and parameters: {'observation_period_num': 15, 'train_rates': 0.9701330641163629, 'learning_rate': 0.00018907120598477022, 'batch_size': 138, 'step_size': 2, 'gamma': 0.9529857696852388}. Best is trial 44 with value: 0.02730521226171837.[0m
[32m[I 2025-01-08 08:01:35,514][0m Trial 46 finished with value: 0.07139988243579865 and parameters: {'observation_period_num': 64, 'train_rates': 0.9738861769858319, 'learning_rate': 0.0005222657275680037, 'batch_size': 121, 'step_size': 9, 'gamma': 0.9532872044388763}. Best is trial 44 with value: 0.02730521226171837.[0m
[32m[I 2025-01-08 08:02:49,497][0m Trial 47 finished with value: 0.07714758342312228 and parameters: {'observation_period_num': 46, 'train_rates': 0.9470195107200308, 'learning_rate': 0.0006305859295841576, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9256170043954869}. Best is trial 44 with value: 0.02730521226171837.[0m
[32m[I 2025-01-08 08:04:24,947][0m Trial 48 finished with value: 0.10620209381406037 and parameters: {'observation_period_num': 87, 'train_rates': 0.9263054591093854, 'learning_rate': 0.00021776792864452004, 'batch_size': 57, 'step_size': 15, 'gamma': 0.9116652799031999}. Best is trial 44 with value: 0.02730521226171837.[0m
[32m[I 2025-01-08 08:05:02,212][0m Trial 49 finished with value: 0.025055253878235817 and parameters: {'observation_period_num': 13, 'train_rates': 0.9894631993442563, 'learning_rate': 0.0007189150108673919, 'batch_size': 188, 'step_size': 12, 'gamma': 0.9359711667805197}. Best is trial 49 with value: 0.025055253878235817.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 49, 'train_rates': 0.8408543181453205, 'learning_rate': 0.000643830278682234, 'batch_size': 96, 'step_size': 10, 'gamma': 0.791035450221983}
Epoch 1/300, trend Loss: 0.3953 | 0.2175
Epoch 2/300, trend Loss: 0.1955 | 0.1856
Epoch 3/300, trend Loss: 0.1851 | 0.2361
Epoch 4/300, trend Loss: 0.1581 | 0.1371
Epoch 5/300, trend Loss: 0.1466 | 0.1053
Epoch 6/300, trend Loss: 0.1510 | 0.3159
Epoch 7/300, trend Loss: 0.1498 | 0.2897
Epoch 8/300, trend Loss: 0.1323 | 0.3657
Epoch 9/300, trend Loss: 0.1389 | 0.1879
Epoch 10/300, trend Loss: 0.1433 | 0.1204
Epoch 11/300, trend Loss: 0.1512 | 0.1734
Epoch 12/300, trend Loss: 0.1239 | 0.1795
Epoch 13/300, trend Loss: 0.1095 | 0.1553
Epoch 14/300, trend Loss: 0.1085 | 0.0898
Epoch 15/300, trend Loss: 0.1027 | 0.0761
Epoch 16/300, trend Loss: 0.0999 | 0.0739
Epoch 17/300, trend Loss: 0.0934 | 0.0847
Epoch 18/300, trend Loss: 0.0907 | 0.0973
Epoch 19/300, trend Loss: 0.0887 | 0.0905
Epoch 20/300, trend Loss: 0.0812 | 0.0783
Epoch 21/300, trend Loss: 0.0807 | 0.0478
Epoch 22/300, trend Loss: 0.0861 | 0.0516
Epoch 23/300, trend Loss: 0.0866 | 0.0529
Epoch 24/300, trend Loss: 0.0776 | 0.0542
Epoch 25/300, trend Loss: 0.0739 | 0.0518
Epoch 26/300, trend Loss: 0.0731 | 0.0463
Epoch 27/300, trend Loss: 0.0728 | 0.0434
Epoch 28/300, trend Loss: 0.0735 | 0.0475
Epoch 29/300, trend Loss: 0.0729 | 0.0439
Epoch 30/300, trend Loss: 0.0717 | 0.0429
Epoch 31/300, trend Loss: 0.0710 | 0.0489
Epoch 32/300, trend Loss: 0.0717 | 0.0585
Epoch 33/300, trend Loss: 0.0732 | 0.0692
Epoch 34/300, trend Loss: 0.0725 | 0.0674
Epoch 35/300, trend Loss: 0.0694 | 0.0502
Epoch 36/300, trend Loss: 0.0690 | 0.0415
Epoch 37/300, trend Loss: 0.0685 | 0.0403
Epoch 38/300, trend Loss: 0.0675 | 0.0403
Epoch 39/300, trend Loss: 0.0672 | 0.0418
Epoch 40/300, trend Loss: 0.0662 | 0.0419
Epoch 41/300, trend Loss: 0.0656 | 0.0415
Epoch 42/300, trend Loss: 0.0663 | 0.0420
Epoch 43/300, trend Loss: 0.0653 | 0.0423
Epoch 44/300, trend Loss: 0.0645 | 0.0429
Epoch 45/300, trend Loss: 0.0641 | 0.0426
Epoch 46/300, trend Loss: 0.0636 | 0.0419
Epoch 47/300, trend Loss: 0.0631 | 0.0407
Epoch 48/300, trend Loss: 0.0625 | 0.0404
Epoch 49/300, trend Loss: 0.0620 | 0.0410
Epoch 50/300, trend Loss: 0.0616 | 0.0404
Epoch 51/300, trend Loss: 0.0611 | 0.0387
Epoch 52/300, trend Loss: 0.0607 | 0.0372
Epoch 53/300, trend Loss: 0.0604 | 0.0369
Epoch 54/300, trend Loss: 0.0603 | 0.0368
Epoch 55/300, trend Loss: 0.0602 | 0.0368
Epoch 56/300, trend Loss: 0.0600 | 0.0370
Epoch 57/300, trend Loss: 0.0598 | 0.0376
Epoch 58/300, trend Loss: 0.0596 | 0.0380
Epoch 59/300, trend Loss: 0.0594 | 0.0379
Epoch 60/300, trend Loss: 0.0593 | 0.0375
Epoch 61/300, trend Loss: 0.0591 | 0.0372
Epoch 62/300, trend Loss: 0.0589 | 0.0368
Epoch 63/300, trend Loss: 0.0588 | 0.0368
Epoch 64/300, trend Loss: 0.0586 | 0.0367
Epoch 65/300, trend Loss: 0.0585 | 0.0366
Epoch 66/300, trend Loss: 0.0584 | 0.0366
Epoch 67/300, trend Loss: 0.0583 | 0.0365
Epoch 68/300, trend Loss: 0.0582 | 0.0365
Epoch 69/300, trend Loss: 0.0581 | 0.0364
Epoch 70/300, trend Loss: 0.0580 | 0.0364
Epoch 71/300, trend Loss: 0.0578 | 0.0364
Epoch 72/300, trend Loss: 0.0578 | 0.0363
Epoch 73/300, trend Loss: 0.0577 | 0.0363
Epoch 74/300, trend Loss: 0.0576 | 0.0362
Epoch 75/300, trend Loss: 0.0575 | 0.0362
Epoch 76/300, trend Loss: 0.0574 | 0.0362
Epoch 77/300, trend Loss: 0.0574 | 0.0361
Epoch 78/300, trend Loss: 0.0573 | 0.0361
Epoch 79/300, trend Loss: 0.0572 | 0.0361
Epoch 80/300, trend Loss: 0.0572 | 0.0360
Epoch 81/300, trend Loss: 0.0571 | 0.0360
Epoch 82/300, trend Loss: 0.0570 | 0.0360
Epoch 83/300, trend Loss: 0.0570 | 0.0360
Epoch 84/300, trend Loss: 0.0569 | 0.0359
Epoch 85/300, trend Loss: 0.0569 | 0.0359
Epoch 86/300, trend Loss: 0.0568 | 0.0359
Epoch 87/300, trend Loss: 0.0567 | 0.0359
Epoch 88/300, trend Loss: 0.0567 | 0.0359
Epoch 89/300, trend Loss: 0.0567 | 0.0358
Epoch 90/300, trend Loss: 0.0566 | 0.0358
Epoch 91/300, trend Loss: 0.0565 | 0.0358
Epoch 92/300, trend Loss: 0.0565 | 0.0358
Epoch 93/300, trend Loss: 0.0565 | 0.0358
Epoch 94/300, trend Loss: 0.0564 | 0.0358
Epoch 95/300, trend Loss: 0.0564 | 0.0357
Epoch 96/300, trend Loss: 0.0564 | 0.0357
Epoch 97/300, trend Loss: 0.0563 | 0.0357
Epoch 98/300, trend Loss: 0.0563 | 0.0357
Epoch 99/300, trend Loss: 0.0563 | 0.0357
Epoch 100/300, trend Loss: 0.0562 | 0.0357
Epoch 101/300, trend Loss: 0.0562 | 0.0357
Epoch 102/300, trend Loss: 0.0562 | 0.0357
Epoch 103/300, trend Loss: 0.0561 | 0.0356
Epoch 104/300, trend Loss: 0.0561 | 0.0356
Epoch 105/300, trend Loss: 0.0561 | 0.0356
Epoch 106/300, trend Loss: 0.0561 | 0.0356
Epoch 107/300, trend Loss: 0.0560 | 0.0356
Epoch 108/300, trend Loss: 0.0560 | 0.0356
Epoch 109/300, trend Loss: 0.0560 | 0.0356
Epoch 110/300, trend Loss: 0.0560 | 0.0356
Epoch 111/300, trend Loss: 0.0560 | 0.0356
Epoch 112/300, trend Loss: 0.0559 | 0.0356
Epoch 113/300, trend Loss: 0.0559 | 0.0356
Epoch 114/300, trend Loss: 0.0559 | 0.0355
Epoch 115/300, trend Loss: 0.0559 | 0.0355
Epoch 116/300, trend Loss: 0.0559 | 0.0355
Epoch 117/300, trend Loss: 0.0559 | 0.0355
Epoch 118/300, trend Loss: 0.0558 | 0.0355
Epoch 119/300, trend Loss: 0.0558 | 0.0355
Epoch 120/300, trend Loss: 0.0558 | 0.0355
Epoch 121/300, trend Loss: 0.0558 | 0.0355
Epoch 122/300, trend Loss: 0.0558 | 0.0355
Epoch 123/300, trend Loss: 0.0558 | 0.0355
Epoch 124/300, trend Loss: 0.0558 | 0.0355
Epoch 125/300, trend Loss: 0.0558 | 0.0355
Epoch 126/300, trend Loss: 0.0557 | 0.0355
Epoch 127/300, trend Loss: 0.0557 | 0.0355
Epoch 128/300, trend Loss: 0.0557 | 0.0355
Epoch 129/300, trend Loss: 0.0557 | 0.0355
Epoch 130/300, trend Loss: 0.0557 | 0.0355
Epoch 131/300, trend Loss: 0.0557 | 0.0355
Epoch 132/300, trend Loss: 0.0557 | 0.0355
Epoch 133/300, trend Loss: 0.0557 | 0.0355
Epoch 134/300, trend Loss: 0.0557 | 0.0355
Epoch 135/300, trend Loss: 0.0557 | 0.0355
Epoch 136/300, trend Loss: 0.0557 | 0.0355
Epoch 137/300, trend Loss: 0.0556 | 0.0354
Epoch 138/300, trend Loss: 0.0556 | 0.0354
Epoch 139/300, trend Loss: 0.0556 | 0.0354
Epoch 140/300, trend Loss: 0.0556 | 0.0354
Epoch 141/300, trend Loss: 0.0556 | 0.0354
Epoch 142/300, trend Loss: 0.0556 | 0.0354
Epoch 143/300, trend Loss: 0.0556 | 0.0354
Epoch 144/300, trend Loss: 0.0556 | 0.0354
Epoch 145/300, trend Loss: 0.0556 | 0.0354
Epoch 146/300, trend Loss: 0.0556 | 0.0354
Epoch 147/300, trend Loss: 0.0556 | 0.0354
Epoch 148/300, trend Loss: 0.0556 | 0.0354
Epoch 149/300, trend Loss: 0.0556 | 0.0354
Epoch 150/300, trend Loss: 0.0556 | 0.0354
Epoch 151/300, trend Loss: 0.0556 | 0.0354
Epoch 152/300, trend Loss: 0.0556 | 0.0354
Epoch 153/300, trend Loss: 0.0556 | 0.0354
Epoch 154/300, trend Loss: 0.0556 | 0.0354
Epoch 155/300, trend Loss: 0.0556 | 0.0354
Epoch 156/300, trend Loss: 0.0556 | 0.0354
Epoch 157/300, trend Loss: 0.0556 | 0.0354
Epoch 158/300, trend Loss: 0.0556 | 0.0354
Epoch 159/300, trend Loss: 0.0556 | 0.0354
Epoch 160/300, trend Loss: 0.0555 | 0.0354
Epoch 161/300, trend Loss: 0.0555 | 0.0354
Epoch 162/300, trend Loss: 0.0555 | 0.0354
Epoch 163/300, trend Loss: 0.0555 | 0.0354
Epoch 164/300, trend Loss: 0.0555 | 0.0354
Epoch 165/300, trend Loss: 0.0555 | 0.0354
Epoch 166/300, trend Loss: 0.0555 | 0.0354
Epoch 167/300, trend Loss: 0.0555 | 0.0354
Epoch 168/300, trend Loss: 0.0555 | 0.0354
Epoch 169/300, trend Loss: 0.0555 | 0.0354
Epoch 170/300, trend Loss: 0.0555 | 0.0354
Epoch 171/300, trend Loss: 0.0555 | 0.0354
Epoch 172/300, trend Loss: 0.0555 | 0.0354
Epoch 173/300, trend Loss: 0.0555 | 0.0354
Epoch 174/300, trend Loss: 0.0555 | 0.0354
Epoch 175/300, trend Loss: 0.0555 | 0.0354
Epoch 176/300, trend Loss: 0.0555 | 0.0354
Epoch 177/300, trend Loss: 0.0555 | 0.0354
Epoch 178/300, trend Loss: 0.0555 | 0.0354
Epoch 179/300, trend Loss: 0.0555 | 0.0354
Epoch 180/300, trend Loss: 0.0555 | 0.0354
Epoch 181/300, trend Loss: 0.0555 | 0.0354
Epoch 182/300, trend Loss: 0.0555 | 0.0354
Epoch 183/300, trend Loss: 0.0555 | 0.0354
Epoch 184/300, trend Loss: 0.0555 | 0.0354
Epoch 185/300, trend Loss: 0.0555 | 0.0354
Epoch 186/300, trend Loss: 0.0555 | 0.0354
Epoch 187/300, trend Loss: 0.0555 | 0.0354
Epoch 188/300, trend Loss: 0.0555 | 0.0354
Epoch 189/300, trend Loss: 0.0555 | 0.0354
Epoch 190/300, trend Loss: 0.0555 | 0.0354
Epoch 191/300, trend Loss: 0.0555 | 0.0354
Epoch 192/300, trend Loss: 0.0555 | 0.0354
Epoch 193/300, trend Loss: 0.0555 | 0.0354
Epoch 194/300, trend Loss: 0.0555 | 0.0354
Epoch 195/300, trend Loss: 0.0555 | 0.0354
Epoch 196/300, trend Loss: 0.0555 | 0.0354
Epoch 197/300, trend Loss: 0.0555 | 0.0354
Epoch 198/300, trend Loss: 0.0555 | 0.0354
Epoch 199/300, trend Loss: 0.0555 | 0.0354
Epoch 200/300, trend Loss: 0.0555 | 0.0354
Epoch 201/300, trend Loss: 0.0555 | 0.0354
Epoch 202/300, trend Loss: 0.0555 | 0.0354
Epoch 203/300, trend Loss: 0.0555 | 0.0354
Epoch 204/300, trend Loss: 0.0555 | 0.0354
Epoch 205/300, trend Loss: 0.0555 | 0.0354
Epoch 206/300, trend Loss: 0.0555 | 0.0354
Epoch 207/300, trend Loss: 0.0555 | 0.0354
Epoch 208/300, trend Loss: 0.0555 | 0.0354
Epoch 209/300, trend Loss: 0.0555 | 0.0354
Epoch 210/300, trend Loss: 0.0555 | 0.0354
Epoch 211/300, trend Loss: 0.0555 | 0.0354
Epoch 212/300, trend Loss: 0.0555 | 0.0354
Epoch 213/300, trend Loss: 0.0555 | 0.0354
Epoch 214/300, trend Loss: 0.0555 | 0.0354
Epoch 215/300, trend Loss: 0.0555 | 0.0354
Epoch 216/300, trend Loss: 0.0555 | 0.0354
Epoch 217/300, trend Loss: 0.0555 | 0.0354
Epoch 218/300, trend Loss: 0.0555 | 0.0354
Epoch 219/300, trend Loss: 0.0555 | 0.0354
Epoch 220/300, trend Loss: 0.0555 | 0.0354
Epoch 221/300, trend Loss: 0.0555 | 0.0354
Epoch 222/300, trend Loss: 0.0555 | 0.0354
Epoch 223/300, trend Loss: 0.0555 | 0.0354
Epoch 224/300, trend Loss: 0.0555 | 0.0354
Epoch 225/300, trend Loss: 0.0555 | 0.0354
Epoch 226/300, trend Loss: 0.0555 | 0.0354
Epoch 227/300, trend Loss: 0.0555 | 0.0354
Epoch 228/300, trend Loss: 0.0555 | 0.0354
Epoch 229/300, trend Loss: 0.0555 | 0.0354
Epoch 230/300, trend Loss: 0.0555 | 0.0354
Epoch 231/300, trend Loss: 0.0555 | 0.0354
Epoch 232/300, trend Loss: 0.0555 | 0.0354
Epoch 233/300, trend Loss: 0.0555 | 0.0354
Epoch 234/300, trend Loss: 0.0555 | 0.0354
Epoch 235/300, trend Loss: 0.0555 | 0.0354
Epoch 236/300, trend Loss: 0.0555 | 0.0354
Epoch 237/300, trend Loss: 0.0555 | 0.0354
Epoch 238/300, trend Loss: 0.0555 | 0.0354
Epoch 239/300, trend Loss: 0.0555 | 0.0354
Epoch 240/300, trend Loss: 0.0555 | 0.0354
Epoch 241/300, trend Loss: 0.0555 | 0.0354
Epoch 242/300, trend Loss: 0.0555 | 0.0354
Epoch 243/300, trend Loss: 0.0555 | 0.0354
Epoch 244/300, trend Loss: 0.0555 | 0.0354
Epoch 245/300, trend Loss: 0.0555 | 0.0354
Epoch 246/300, trend Loss: 0.0555 | 0.0354
Epoch 247/300, trend Loss: 0.0555 | 0.0354
Epoch 248/300, trend Loss: 0.0555 | 0.0354
Epoch 249/300, trend Loss: 0.0555 | 0.0354
Epoch 250/300, trend Loss: 0.0555 | 0.0354
Epoch 251/300, trend Loss: 0.0555 | 0.0354
Epoch 252/300, trend Loss: 0.0555 | 0.0354
Epoch 253/300, trend Loss: 0.0555 | 0.0354
Epoch 254/300, trend Loss: 0.0555 | 0.0354
Epoch 255/300, trend Loss: 0.0555 | 0.0354
Epoch 256/300, trend Loss: 0.0555 | 0.0354
Epoch 257/300, trend Loss: 0.0555 | 0.0354
Epoch 258/300, trend Loss: 0.0555 | 0.0354
Epoch 259/300, trend Loss: 0.0555 | 0.0354
Epoch 260/300, trend Loss: 0.0555 | 0.0354
Epoch 261/300, trend Loss: 0.0555 | 0.0354
Epoch 262/300, trend Loss: 0.0555 | 0.0354
Epoch 263/300, trend Loss: 0.0555 | 0.0354
Epoch 264/300, trend Loss: 0.0555 | 0.0354
Epoch 265/300, trend Loss: 0.0555 | 0.0354
Epoch 266/300, trend Loss: 0.0555 | 0.0354
Epoch 267/300, trend Loss: 0.0555 | 0.0354
Epoch 268/300, trend Loss: 0.0555 | 0.0354
Epoch 269/300, trend Loss: 0.0555 | 0.0354
Epoch 270/300, trend Loss: 0.0555 | 0.0354
Epoch 271/300, trend Loss: 0.0555 | 0.0354
Epoch 272/300, trend Loss: 0.0555 | 0.0354
Epoch 273/300, trend Loss: 0.0555 | 0.0354
Epoch 274/300, trend Loss: 0.0555 | 0.0354
Epoch 275/300, trend Loss: 0.0555 | 0.0354
Epoch 276/300, trend Loss: 0.0555 | 0.0354
Epoch 277/300, trend Loss: 0.0555 | 0.0354
Epoch 278/300, trend Loss: 0.0555 | 0.0354
Epoch 279/300, trend Loss: 0.0555 | 0.0354
Epoch 280/300, trend Loss: 0.0555 | 0.0354
Epoch 281/300, trend Loss: 0.0555 | 0.0354
Epoch 282/300, trend Loss: 0.0555 | 0.0354
Epoch 283/300, trend Loss: 0.0555 | 0.0354
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 8, 'train_rates': 0.9899585195151253, 'learning_rate': 0.00044837801302468423, 'batch_size': 43, 'step_size': 15, 'gamma': 0.8262708060787456}
Epoch 1/300, seasonal_0 Loss: 0.3078 | 0.1431
Epoch 2/300, seasonal_0 Loss: 0.1298 | 0.1107
Epoch 3/300, seasonal_0 Loss: 0.1110 | 0.0912
Epoch 4/300, seasonal_0 Loss: 0.1059 | 0.0856
Epoch 5/300, seasonal_0 Loss: 0.1022 | 0.0830
Epoch 6/300, seasonal_0 Loss: 0.1020 | 0.0813
Epoch 7/300, seasonal_0 Loss: 0.1055 | 0.0762
Epoch 8/300, seasonal_0 Loss: 0.0941 | 0.0713
Epoch 9/300, seasonal_0 Loss: 0.0949 | 0.0709
Epoch 10/300, seasonal_0 Loss: 0.0913 | 0.0681
Epoch 11/300, seasonal_0 Loss: 0.0878 | 0.0660
Epoch 12/300, seasonal_0 Loss: 0.0853 | 0.0647
Epoch 13/300, seasonal_0 Loss: 0.0834 | 0.0648
Epoch 14/300, seasonal_0 Loss: 0.0816 | 0.0628
Epoch 15/300, seasonal_0 Loss: 0.0799 | 0.0612
Epoch 16/300, seasonal_0 Loss: 0.0781 | 0.0624
Epoch 17/300, seasonal_0 Loss: 0.0764 | 0.0606
Epoch 18/300, seasonal_0 Loss: 0.0765 | 0.0603
Epoch 19/300, seasonal_0 Loss: 0.0758 | 0.0597
Epoch 20/300, seasonal_0 Loss: 0.0742 | 0.0573
Epoch 21/300, seasonal_0 Loss: 0.0736 | 0.0570
Epoch 22/300, seasonal_0 Loss: 0.0730 | 0.0559
Epoch 23/300, seasonal_0 Loss: 0.0729 | 0.0559
Epoch 24/300, seasonal_0 Loss: 0.0739 | 0.0532
Epoch 25/300, seasonal_0 Loss: 0.0740 | 0.0521
Epoch 26/300, seasonal_0 Loss: 0.0734 | 0.0491
Epoch 27/300, seasonal_0 Loss: 0.0729 | 0.0495
Epoch 28/300, seasonal_0 Loss: 0.0726 | 0.0516
Epoch 29/300, seasonal_0 Loss: 0.0711 | 0.0499
Epoch 30/300, seasonal_0 Loss: 0.0706 | 0.0489
Epoch 31/300, seasonal_0 Loss: 0.0692 | 0.0459
Epoch 32/300, seasonal_0 Loss: 0.0688 | 0.0465
Epoch 33/300, seasonal_0 Loss: 0.0689 | 0.0471
Epoch 34/300, seasonal_0 Loss: 0.0681 | 0.0443
Epoch 35/300, seasonal_0 Loss: 0.0662 | 0.0427
Epoch 36/300, seasonal_0 Loss: 0.0654 | 0.0413
Epoch 37/300, seasonal_0 Loss: 0.0648 | 0.0421
Epoch 38/300, seasonal_0 Loss: 0.0637 | 0.0376
Epoch 39/300, seasonal_0 Loss: 0.0619 | 0.0340
Epoch 40/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 41/300, seasonal_0 Loss: 0.0613 | 0.0314
Epoch 42/300, seasonal_0 Loss: 0.0598 | 0.0290
Epoch 43/300, seasonal_0 Loss: 0.0592 | 0.0289
Epoch 44/300, seasonal_0 Loss: 0.0596 | 0.0285
Epoch 45/300, seasonal_0 Loss: 0.0598 | 0.0262
Epoch 46/300, seasonal_0 Loss: 0.0580 | 0.0281
Epoch 47/300, seasonal_0 Loss: 0.0573 | 0.0282
Epoch 48/300, seasonal_0 Loss: 0.0567 | 0.0279
Epoch 49/300, seasonal_0 Loss: 0.0562 | 0.0277
Epoch 50/300, seasonal_0 Loss: 0.0559 | 0.0296
Epoch 51/300, seasonal_0 Loss: 0.0554 | 0.0261
Epoch 52/300, seasonal_0 Loss: 0.0551 | 0.0271
Epoch 53/300, seasonal_0 Loss: 0.0548 | 0.0270
Epoch 54/300, seasonal_0 Loss: 0.0547 | 0.0296
Epoch 55/300, seasonal_0 Loss: 0.0559 | 0.0233
Epoch 56/300, seasonal_0 Loss: 0.0548 | 0.0236
Epoch 57/300, seasonal_0 Loss: 0.0549 | 0.0252
Epoch 58/300, seasonal_0 Loss: 0.0567 | 0.0293
Epoch 59/300, seasonal_0 Loss: 0.0576 | 0.0230
Epoch 60/300, seasonal_0 Loss: 0.0576 | 0.0310
Epoch 61/300, seasonal_0 Loss: 0.0587 | 0.0419
Epoch 62/300, seasonal_0 Loss: 0.0565 | 0.0298
Epoch 63/300, seasonal_0 Loss: 0.0554 | 0.0283
Epoch 64/300, seasonal_0 Loss: 0.0545 | 0.0246
Epoch 65/300, seasonal_0 Loss: 0.0536 | 0.0241
Epoch 66/300, seasonal_0 Loss: 0.0525 | 0.0237
Epoch 67/300, seasonal_0 Loss: 0.0518 | 0.0237
Epoch 68/300, seasonal_0 Loss: 0.0515 | 0.0242
Epoch 69/300, seasonal_0 Loss: 0.0513 | 0.0241
Epoch 70/300, seasonal_0 Loss: 0.0513 | 0.0230
Epoch 71/300, seasonal_0 Loss: 0.0510 | 0.0222
Epoch 72/300, seasonal_0 Loss: 0.0508 | 0.0216
Epoch 73/300, seasonal_0 Loss: 0.0507 | 0.0215
Epoch 74/300, seasonal_0 Loss: 0.0505 | 0.0212
Epoch 75/300, seasonal_0 Loss: 0.0505 | 0.0211
Epoch 76/300, seasonal_0 Loss: 0.0505 | 0.0213
Epoch 77/300, seasonal_0 Loss: 0.0511 | 0.0223
Epoch 78/300, seasonal_0 Loss: 0.0506 | 0.0223
Epoch 79/300, seasonal_0 Loss: 0.0501 | 0.0226
Epoch 80/300, seasonal_0 Loss: 0.0495 | 0.0226
Epoch 81/300, seasonal_0 Loss: 0.0494 | 0.0226
Epoch 82/300, seasonal_0 Loss: 0.0491 | 0.0228
Epoch 83/300, seasonal_0 Loss: 0.0487 | 0.0228
Epoch 84/300, seasonal_0 Loss: 0.0486 | 0.0229
Epoch 85/300, seasonal_0 Loss: 0.0484 | 0.0229
Epoch 86/300, seasonal_0 Loss: 0.0482 | 0.0225
Epoch 87/300, seasonal_0 Loss: 0.0480 | 0.0223
Epoch 88/300, seasonal_0 Loss: 0.0478 | 0.0223
Epoch 89/300, seasonal_0 Loss: 0.0475 | 0.0224
Epoch 90/300, seasonal_0 Loss: 0.0473 | 0.0224
Epoch 91/300, seasonal_0 Loss: 0.0471 | 0.0211
Epoch 92/300, seasonal_0 Loss: 0.0474 | 0.0212
Epoch 93/300, seasonal_0 Loss: 0.0469 | 0.0210
Epoch 94/300, seasonal_0 Loss: 0.0473 | 0.0217
Epoch 95/300, seasonal_0 Loss: 0.0468 | 0.0215
Epoch 96/300, seasonal_0 Loss: 0.0471 | 0.0229
Epoch 97/300, seasonal_0 Loss: 0.0466 | 0.0230
Epoch 98/300, seasonal_0 Loss: 0.0464 | 0.0234
Epoch 99/300, seasonal_0 Loss: 0.0460 | 0.0227
Epoch 100/300, seasonal_0 Loss: 0.0461 | 0.0233
Epoch 101/300, seasonal_0 Loss: 0.0469 | 0.0237
Epoch 102/300, seasonal_0 Loss: 0.0467 | 0.0229
Epoch 103/300, seasonal_0 Loss: 0.0454 | 0.0219
Epoch 104/300, seasonal_0 Loss: 0.0445 | 0.0216
Epoch 105/300, seasonal_0 Loss: 0.0466 | 0.0217
Epoch 106/300, seasonal_0 Loss: 0.0466 | 0.0213
Epoch 107/300, seasonal_0 Loss: 0.0445 | 0.0218
Epoch 108/300, seasonal_0 Loss: 0.0437 | 0.0218
Epoch 109/300, seasonal_0 Loss: 0.0416 | 0.0219
Epoch 110/300, seasonal_0 Loss: 0.0412 | 0.0225
Epoch 111/300, seasonal_0 Loss: 0.0412 | 0.0222
Epoch 112/300, seasonal_0 Loss: 0.0408 | 0.0216
Epoch 113/300, seasonal_0 Loss: 0.0411 | 0.0218
Epoch 114/300, seasonal_0 Loss: 0.0402 | 0.0217
Epoch 115/300, seasonal_0 Loss: 0.0398 | 0.0219
Epoch 116/300, seasonal_0 Loss: 0.0401 | 0.0215
Epoch 117/300, seasonal_0 Loss: 0.0396 | 0.0210
Epoch 118/300, seasonal_0 Loss: 0.0388 | 0.0203
Epoch 119/300, seasonal_0 Loss: 0.0412 | 0.0210
Epoch 120/300, seasonal_0 Loss: 0.0440 | 0.0214
Epoch 121/300, seasonal_0 Loss: 0.0396 | 0.0216
Epoch 122/300, seasonal_0 Loss: 0.0390 | 0.0213
Epoch 123/300, seasonal_0 Loss: 0.0389 | 0.0212
Epoch 124/300, seasonal_0 Loss: 0.0388 | 0.0212
Epoch 125/300, seasonal_0 Loss: 0.0399 | 0.0210
Epoch 126/300, seasonal_0 Loss: 0.0395 | 0.0209
Epoch 127/300, seasonal_0 Loss: 0.0388 | 0.0211
Epoch 128/300, seasonal_0 Loss: 0.0387 | 0.0216
Epoch 129/300, seasonal_0 Loss: 0.0386 | 0.0211
Epoch 130/300, seasonal_0 Loss: 0.0393 | 0.0209
Epoch 131/300, seasonal_0 Loss: 0.0385 | 0.0210
Epoch 132/300, seasonal_0 Loss: 0.0406 | 0.0216
Epoch 133/300, seasonal_0 Loss: 0.0380 | 0.0214
Epoch 134/300, seasonal_0 Loss: 0.0372 | 0.0213
Epoch 135/300, seasonal_0 Loss: 0.0375 | 0.0213
Epoch 136/300, seasonal_0 Loss: 0.0374 | 0.0213
Epoch 137/300, seasonal_0 Loss: 0.0376 | 0.0215
Epoch 138/300, seasonal_0 Loss: 0.0383 | 0.0210
Epoch 139/300, seasonal_0 Loss: 0.0377 | 0.0211
Epoch 140/300, seasonal_0 Loss: 0.0378 | 0.0211
Epoch 141/300, seasonal_0 Loss: 0.0371 | 0.0211
Epoch 142/300, seasonal_0 Loss: 0.0363 | 0.0210
Epoch 143/300, seasonal_0 Loss: 0.0363 | 0.0212
Epoch 144/300, seasonal_0 Loss: 0.0365 | 0.0215
Epoch 145/300, seasonal_0 Loss: 0.0363 | 0.0213
Epoch 146/300, seasonal_0 Loss: 0.0364 | 0.0211
Epoch 147/300, seasonal_0 Loss: 0.0360 | 0.0214
Epoch 148/300, seasonal_0 Loss: 0.0360 | 0.0208
Epoch 149/300, seasonal_0 Loss: 0.0365 | 0.0207
Epoch 150/300, seasonal_0 Loss: 0.0366 | 0.0209
Epoch 151/300, seasonal_0 Loss: 0.0364 | 0.0206
Epoch 152/300, seasonal_0 Loss: 0.0368 | 0.0209
Epoch 153/300, seasonal_0 Loss: 0.0370 | 0.0207
Epoch 154/300, seasonal_0 Loss: 0.0357 | 0.0206
Epoch 155/300, seasonal_0 Loss: 0.0358 | 0.0207
Epoch 156/300, seasonal_0 Loss: 0.0370 | 0.0208
Epoch 157/300, seasonal_0 Loss: 0.0359 | 0.0208
Epoch 158/300, seasonal_0 Loss: 0.0357 | 0.0207
Epoch 159/300, seasonal_0 Loss: 0.0361 | 0.0208
Epoch 160/300, seasonal_0 Loss: 0.0357 | 0.0208
Epoch 161/300, seasonal_0 Loss: 0.0355 | 0.0207
Epoch 162/300, seasonal_0 Loss: 0.0359 | 0.0208
Epoch 163/300, seasonal_0 Loss: 0.0357 | 0.0210
Epoch 164/300, seasonal_0 Loss: 0.0363 | 0.0209
Epoch 165/300, seasonal_0 Loss: 0.0355 | 0.0208
Epoch 166/300, seasonal_0 Loss: 0.0353 | 0.0208
Epoch 167/300, seasonal_0 Loss: 0.0351 | 0.0209
Epoch 168/300, seasonal_0 Loss: 0.0353 | 0.0209
Epoch 169/300, seasonal_0 Loss: 0.0352 | 0.0208
Epoch 170/300, seasonal_0 Loss: 0.0356 | 0.0211
Epoch 171/300, seasonal_0 Loss: 0.0366 | 0.0211
Epoch 172/300, seasonal_0 Loss: 0.0352 | 0.0209
Epoch 173/300, seasonal_0 Loss: 0.0350 | 0.0209
Epoch 174/300, seasonal_0 Loss: 0.0349 | 0.0211
Epoch 175/300, seasonal_0 Loss: 0.0353 | 0.0209
Epoch 176/300, seasonal_0 Loss: 0.0349 | 0.0210
Epoch 177/300, seasonal_0 Loss: 0.0348 | 0.0211
Epoch 178/300, seasonal_0 Loss: 0.0352 | 0.0209
Epoch 179/300, seasonal_0 Loss: 0.0350 | 0.0209
Epoch 180/300, seasonal_0 Loss: 0.0351 | 0.0213
Epoch 181/300, seasonal_0 Loss: 0.0359 | 0.0211
Epoch 182/300, seasonal_0 Loss: 0.0349 | 0.0210
Epoch 183/300, seasonal_0 Loss: 0.0346 | 0.0210
Epoch 184/300, seasonal_0 Loss: 0.0345 | 0.0210
Epoch 185/300, seasonal_0 Loss: 0.0344 | 0.0210
Epoch 186/300, seasonal_0 Loss: 0.0344 | 0.0210
Epoch 187/300, seasonal_0 Loss: 0.0343 | 0.0211
Epoch 188/300, seasonal_0 Loss: 0.0343 | 0.0211
Epoch 189/300, seasonal_0 Loss: 0.0343 | 0.0210
Epoch 190/300, seasonal_0 Loss: 0.0344 | 0.0210
Epoch 191/300, seasonal_0 Loss: 0.0348 | 0.0215
Epoch 192/300, seasonal_0 Loss: 0.0359 | 0.0212
Epoch 193/300, seasonal_0 Loss: 0.0349 | 0.0210
Epoch 194/300, seasonal_0 Loss: 0.0344 | 0.0211
Epoch 195/300, seasonal_0 Loss: 0.0342 | 0.0210
Epoch 196/300, seasonal_0 Loss: 0.0341 | 0.0211
Epoch 197/300, seasonal_0 Loss: 0.0342 | 0.0210
Epoch 198/300, seasonal_0 Loss: 0.0342 | 0.0212
Epoch 199/300, seasonal_0 Loss: 0.0344 | 0.0209
Epoch 200/300, seasonal_0 Loss: 0.0343 | 0.0213
Epoch 201/300, seasonal_0 Loss: 0.0347 | 0.0209
Epoch 202/300, seasonal_0 Loss: 0.0344 | 0.0214
Epoch 203/300, seasonal_0 Loss: 0.0348 | 0.0210
Epoch 204/300, seasonal_0 Loss: 0.0341 | 0.0212
Epoch 205/300, seasonal_0 Loss: 0.0341 | 0.0210
Epoch 206/300, seasonal_0 Loss: 0.0341 | 0.0212
Epoch 207/300, seasonal_0 Loss: 0.0342 | 0.0210
Epoch 208/300, seasonal_0 Loss: 0.0341 | 0.0213
Epoch 209/300, seasonal_0 Loss: 0.0342 | 0.0210
Epoch 210/300, seasonal_0 Loss: 0.0341 | 0.0213
Epoch 211/300, seasonal_0 Loss: 0.0342 | 0.0210
Epoch 212/300, seasonal_0 Loss: 0.0340 | 0.0213
Epoch 213/300, seasonal_0 Loss: 0.0340 | 0.0210
Epoch 214/300, seasonal_0 Loss: 0.0339 | 0.0213
Epoch 215/300, seasonal_0 Loss: 0.0340 | 0.0210
Epoch 216/300, seasonal_0 Loss: 0.0339 | 0.0213
Epoch 217/300, seasonal_0 Loss: 0.0340 | 0.0210
Epoch 218/300, seasonal_0 Loss: 0.0339 | 0.0213
Epoch 219/300, seasonal_0 Loss: 0.0340 | 0.0211
Epoch 220/300, seasonal_0 Loss: 0.0338 | 0.0213
Epoch 221/300, seasonal_0 Loss: 0.0339 | 0.0211
Epoch 222/300, seasonal_0 Loss: 0.0338 | 0.0213
Epoch 223/300, seasonal_0 Loss: 0.0338 | 0.0211
Epoch 224/300, seasonal_0 Loss: 0.0338 | 0.0213
Epoch 225/300, seasonal_0 Loss: 0.0338 | 0.0211
Epoch 226/300, seasonal_0 Loss: 0.0338 | 0.0213
Epoch 227/300, seasonal_0 Loss: 0.0338 | 0.0211
Epoch 228/300, seasonal_0 Loss: 0.0337 | 0.0213
Epoch 229/300, seasonal_0 Loss: 0.0337 | 0.0211
Epoch 230/300, seasonal_0 Loss: 0.0337 | 0.0213
Epoch 231/300, seasonal_0 Loss: 0.0337 | 0.0212
Epoch 232/300, seasonal_0 Loss: 0.0337 | 0.0213
Epoch 233/300, seasonal_0 Loss: 0.0337 | 0.0212
Epoch 234/300, seasonal_0 Loss: 0.0337 | 0.0213
Epoch 235/300, seasonal_0 Loss: 0.0337 | 0.0212
Epoch 236/300, seasonal_0 Loss: 0.0337 | 0.0213
Epoch 237/300, seasonal_0 Loss: 0.0337 | 0.0212
Epoch 238/300, seasonal_0 Loss: 0.0336 | 0.0213
Epoch 239/300, seasonal_0 Loss: 0.0336 | 0.0212
Epoch 240/300, seasonal_0 Loss: 0.0336 | 0.0213
Epoch 241/300, seasonal_0 Loss: 0.0336 | 0.0212
Epoch 242/300, seasonal_0 Loss: 0.0336 | 0.0213
Epoch 243/300, seasonal_0 Loss: 0.0336 | 0.0212
Epoch 244/300, seasonal_0 Loss: 0.0336 | 0.0213
Epoch 245/300, seasonal_0 Loss: 0.0336 | 0.0212
Epoch 246/300, seasonal_0 Loss: 0.0336 | 0.0213
Epoch 247/300, seasonal_0 Loss: 0.0336 | 0.0212
Epoch 248/300, seasonal_0 Loss: 0.0336 | 0.0213
Epoch 249/300, seasonal_0 Loss: 0.0336 | 0.0212
Epoch 250/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 251/300, seasonal_0 Loss: 0.0335 | 0.0212
Epoch 252/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 253/300, seasonal_0 Loss: 0.0335 | 0.0212
Epoch 254/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 255/300, seasonal_0 Loss: 0.0335 | 0.0212
Epoch 256/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 257/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 258/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 259/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 260/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 261/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 262/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 263/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 264/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 265/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 266/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 267/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 268/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 269/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 270/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 271/300, seasonal_0 Loss: 0.0335 | 0.0213
Epoch 272/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 273/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 274/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 275/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 276/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 277/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 278/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 279/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 280/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 281/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 282/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 283/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 284/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 285/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 286/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 287/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 288/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 289/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 290/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 291/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 292/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 293/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 294/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 295/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 296/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 297/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 298/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 299/300, seasonal_0 Loss: 0.0334 | 0.0213
Epoch 300/300, seasonal_0 Loss: 0.0334 | 0.0213
Training seasonal_1 component with params: {'observation_period_num': 19, 'train_rates': 0.9851873798329772, 'learning_rate': 0.0005373957503477581, 'batch_size': 232, 'step_size': 4, 'gamma': 0.9399604662852258}
Epoch 1/300, seasonal_1 Loss: 0.3964 | 0.2783
Epoch 2/300, seasonal_1 Loss: 0.2195 | 0.1976
Epoch 3/300, seasonal_1 Loss: 0.3384 | 0.7622
Epoch 4/300, seasonal_1 Loss: 0.1741 | 0.1713
Epoch 5/300, seasonal_1 Loss: 0.1594 | 0.1827
Epoch 6/300, seasonal_1 Loss: 0.1435 | 0.1525
Epoch 7/300, seasonal_1 Loss: 0.1619 | 0.1353
Epoch 8/300, seasonal_1 Loss: 0.1179 | 0.1023
Epoch 9/300, seasonal_1 Loss: 0.1282 | 0.1043
Epoch 10/300, seasonal_1 Loss: 0.1107 | 0.0823
Epoch 11/300, seasonal_1 Loss: 0.1377 | 0.0890
Epoch 12/300, seasonal_1 Loss: 0.1113 | 0.0821
Epoch 13/300, seasonal_1 Loss: 0.1326 | 0.0885
Epoch 14/300, seasonal_1 Loss: 0.1166 | 0.1077
Epoch 15/300, seasonal_1 Loss: 0.1174 | 0.1044
Epoch 16/300, seasonal_1 Loss: 0.1151 | 0.1050
Epoch 17/300, seasonal_1 Loss: 0.1214 | 0.1848
Epoch 18/300, seasonal_1 Loss: 0.1126 | 0.0658
Epoch 19/300, seasonal_1 Loss: 0.1053 | 0.1321
Epoch 20/300, seasonal_1 Loss: 0.1151 | 0.0742
Epoch 21/300, seasonal_1 Loss: 0.1535 | 0.2392
Epoch 22/300, seasonal_1 Loss: 0.1756 | 0.2371
Epoch 23/300, seasonal_1 Loss: 0.1789 | 0.2055
Epoch 24/300, seasonal_1 Loss: 0.1348 | 0.0978
Epoch 25/300, seasonal_1 Loss: 0.1074 | 0.0918
Epoch 26/300, seasonal_1 Loss: 0.1001 | 0.0831
Epoch 27/300, seasonal_1 Loss: 0.0955 | 0.0721
Epoch 28/300, seasonal_1 Loss: 0.0948 | 0.0760
Epoch 29/300, seasonal_1 Loss: 0.0948 | 0.0621
Epoch 30/300, seasonal_1 Loss: 0.0942 | 0.0764
Epoch 31/300, seasonal_1 Loss: 0.0988 | 0.0594
Epoch 32/300, seasonal_1 Loss: 0.1147 | 0.0877
Epoch 33/300, seasonal_1 Loss: 0.1446 | 0.1140
Epoch 34/300, seasonal_1 Loss: 0.1720 | 0.0708
Epoch 35/300, seasonal_1 Loss: 0.1477 | 0.2218
Epoch 36/300, seasonal_1 Loss: 0.1199 | 0.0709
Epoch 37/300, seasonal_1 Loss: 0.0957 | 0.1037
Epoch 38/300, seasonal_1 Loss: 0.0899 | 0.0566
Epoch 39/300, seasonal_1 Loss: 0.0840 | 0.0571
Epoch 40/300, seasonal_1 Loss: 0.0817 | 0.0523
Epoch 41/300, seasonal_1 Loss: 0.0805 | 0.0515
Epoch 42/300, seasonal_1 Loss: 0.0797 | 0.0506
Epoch 43/300, seasonal_1 Loss: 0.0792 | 0.0496
Epoch 44/300, seasonal_1 Loss: 0.0788 | 0.0496
Epoch 45/300, seasonal_1 Loss: 0.0788 | 0.0486
Epoch 46/300, seasonal_1 Loss: 0.0796 | 0.0505
Epoch 47/300, seasonal_1 Loss: 0.0813 | 0.0478
Epoch 48/300, seasonal_1 Loss: 0.0868 | 0.0728
Epoch 49/300, seasonal_1 Loss: 0.0866 | 0.0493
Epoch 50/300, seasonal_1 Loss: 0.0890 | 0.0951
Epoch 51/300, seasonal_1 Loss: 0.0830 | 0.0497
Epoch 52/300, seasonal_1 Loss: 0.0789 | 0.0572
Epoch 53/300, seasonal_1 Loss: 0.0774 | 0.0463
Epoch 54/300, seasonal_1 Loss: 0.0761 | 0.0486
Epoch 55/300, seasonal_1 Loss: 0.0754 | 0.0456
Epoch 56/300, seasonal_1 Loss: 0.0749 | 0.0465
Epoch 57/300, seasonal_1 Loss: 0.0745 | 0.0453
Epoch 58/300, seasonal_1 Loss: 0.0742 | 0.0456
Epoch 59/300, seasonal_1 Loss: 0.0739 | 0.0449
Epoch 60/300, seasonal_1 Loss: 0.0737 | 0.0451
Epoch 61/300, seasonal_1 Loss: 0.0734 | 0.0445
Epoch 62/300, seasonal_1 Loss: 0.0732 | 0.0447
Epoch 63/300, seasonal_1 Loss: 0.0729 | 0.0440
Epoch 64/300, seasonal_1 Loss: 0.0726 | 0.0442
Epoch 65/300, seasonal_1 Loss: 0.0724 | 0.0436
Epoch 66/300, seasonal_1 Loss: 0.0721 | 0.0437
Epoch 67/300, seasonal_1 Loss: 0.0718 | 0.0430
Epoch 68/300, seasonal_1 Loss: 0.0715 | 0.0430
Epoch 69/300, seasonal_1 Loss: 0.0712 | 0.0425
Epoch 70/300, seasonal_1 Loss: 0.0709 | 0.0424
Epoch 71/300, seasonal_1 Loss: 0.0706 | 0.0419
Epoch 72/300, seasonal_1 Loss: 0.0704 | 0.0417
Epoch 73/300, seasonal_1 Loss: 0.0701 | 0.0414
Epoch 74/300, seasonal_1 Loss: 0.0699 | 0.0412
Epoch 75/300, seasonal_1 Loss: 0.0697 | 0.0409
Epoch 76/300, seasonal_1 Loss: 0.0695 | 0.0407
Epoch 77/300, seasonal_1 Loss: 0.0693 | 0.0404
Epoch 78/300, seasonal_1 Loss: 0.0691 | 0.0402
Epoch 79/300, seasonal_1 Loss: 0.0689 | 0.0399
Epoch 80/300, seasonal_1 Loss: 0.0687 | 0.0397
Epoch 81/300, seasonal_1 Loss: 0.0686 | 0.0395
Epoch 82/300, seasonal_1 Loss: 0.0684 | 0.0393
Epoch 83/300, seasonal_1 Loss: 0.0683 | 0.0391
Epoch 84/300, seasonal_1 Loss: 0.0681 | 0.0389
Epoch 85/300, seasonal_1 Loss: 0.0680 | 0.0387
Epoch 86/300, seasonal_1 Loss: 0.0679 | 0.0385
Epoch 87/300, seasonal_1 Loss: 0.0677 | 0.0383
Epoch 88/300, seasonal_1 Loss: 0.0676 | 0.0381
Epoch 89/300, seasonal_1 Loss: 0.0675 | 0.0379
Epoch 90/300, seasonal_1 Loss: 0.0674 | 0.0378
Epoch 91/300, seasonal_1 Loss: 0.0673 | 0.0376
Epoch 92/300, seasonal_1 Loss: 0.0672 | 0.0375
Epoch 93/300, seasonal_1 Loss: 0.0671 | 0.0373
Epoch 94/300, seasonal_1 Loss: 0.0670 | 0.0372
Epoch 95/300, seasonal_1 Loss: 0.0669 | 0.0371
Epoch 96/300, seasonal_1 Loss: 0.0669 | 0.0369
Epoch 97/300, seasonal_1 Loss: 0.0668 | 0.0368
Epoch 98/300, seasonal_1 Loss: 0.0667 | 0.0367
Epoch 99/300, seasonal_1 Loss: 0.0666 | 0.0366
Epoch 100/300, seasonal_1 Loss: 0.0665 | 0.0365
Epoch 101/300, seasonal_1 Loss: 0.0665 | 0.0364
Epoch 102/300, seasonal_1 Loss: 0.0664 | 0.0363
Epoch 103/300, seasonal_1 Loss: 0.0663 | 0.0362
Epoch 104/300, seasonal_1 Loss: 0.0663 | 0.0361
Epoch 105/300, seasonal_1 Loss: 0.0662 | 0.0360
Epoch 106/300, seasonal_1 Loss: 0.0662 | 0.0359
Epoch 107/300, seasonal_1 Loss: 0.0661 | 0.0358
Epoch 108/300, seasonal_1 Loss: 0.0660 | 0.0357
Epoch 109/300, seasonal_1 Loss: 0.0660 | 0.0356
Epoch 110/300, seasonal_1 Loss: 0.0659 | 0.0355
Epoch 111/300, seasonal_1 Loss: 0.0659 | 0.0355
Epoch 112/300, seasonal_1 Loss: 0.0658 | 0.0354
Epoch 113/300, seasonal_1 Loss: 0.0658 | 0.0353
Epoch 114/300, seasonal_1 Loss: 0.0657 | 0.0352
Epoch 115/300, seasonal_1 Loss: 0.0657 | 0.0352
Epoch 116/300, seasonal_1 Loss: 0.0656 | 0.0351
Epoch 117/300, seasonal_1 Loss: 0.0656 | 0.0350
Epoch 118/300, seasonal_1 Loss: 0.0655 | 0.0350
Epoch 119/300, seasonal_1 Loss: 0.0655 | 0.0349
Epoch 120/300, seasonal_1 Loss: 0.0655 | 0.0348
Epoch 121/300, seasonal_1 Loss: 0.0654 | 0.0348
Epoch 122/300, seasonal_1 Loss: 0.0654 | 0.0347
Epoch 123/300, seasonal_1 Loss: 0.0653 | 0.0346
Epoch 124/300, seasonal_1 Loss: 0.0653 | 0.0346
Epoch 125/300, seasonal_1 Loss: 0.0653 | 0.0345
Epoch 126/300, seasonal_1 Loss: 0.0652 | 0.0345
Epoch 127/300, seasonal_1 Loss: 0.0652 | 0.0344
Epoch 128/300, seasonal_1 Loss: 0.0652 | 0.0344
Epoch 129/300, seasonal_1 Loss: 0.0651 | 0.0343
Epoch 130/300, seasonal_1 Loss: 0.0651 | 0.0343
Epoch 131/300, seasonal_1 Loss: 0.0651 | 0.0342
Epoch 132/300, seasonal_1 Loss: 0.0650 | 0.0342
Epoch 133/300, seasonal_1 Loss: 0.0650 | 0.0341
Epoch 134/300, seasonal_1 Loss: 0.0650 | 0.0341
Epoch 135/300, seasonal_1 Loss: 0.0650 | 0.0340
Epoch 136/300, seasonal_1 Loss: 0.0649 | 0.0340
Epoch 137/300, seasonal_1 Loss: 0.0649 | 0.0340
Epoch 138/300, seasonal_1 Loss: 0.0649 | 0.0339
Epoch 139/300, seasonal_1 Loss: 0.0649 | 0.0339
Epoch 140/300, seasonal_1 Loss: 0.0648 | 0.0338
Epoch 141/300, seasonal_1 Loss: 0.0648 | 0.0338
Epoch 142/300, seasonal_1 Loss: 0.0648 | 0.0338
Epoch 143/300, seasonal_1 Loss: 0.0648 | 0.0337
Epoch 144/300, seasonal_1 Loss: 0.0647 | 0.0337
Epoch 145/300, seasonal_1 Loss: 0.0647 | 0.0337
Epoch 146/300, seasonal_1 Loss: 0.0647 | 0.0336
Epoch 147/300, seasonal_1 Loss: 0.0647 | 0.0336
Epoch 148/300, seasonal_1 Loss: 0.0647 | 0.0336
Epoch 149/300, seasonal_1 Loss: 0.0646 | 0.0335
Epoch 150/300, seasonal_1 Loss: 0.0646 | 0.0335
Epoch 151/300, seasonal_1 Loss: 0.0646 | 0.0335
Epoch 152/300, seasonal_1 Loss: 0.0646 | 0.0335
Epoch 153/300, seasonal_1 Loss: 0.0646 | 0.0334
Epoch 154/300, seasonal_1 Loss: 0.0646 | 0.0334
Epoch 155/300, seasonal_1 Loss: 0.0645 | 0.0334
Epoch 156/300, seasonal_1 Loss: 0.0645 | 0.0334
Epoch 157/300, seasonal_1 Loss: 0.0645 | 0.0333
Epoch 158/300, seasonal_1 Loss: 0.0645 | 0.0333
Epoch 159/300, seasonal_1 Loss: 0.0645 | 0.0333
Epoch 160/300, seasonal_1 Loss: 0.0645 | 0.0333
Epoch 161/300, seasonal_1 Loss: 0.0645 | 0.0332
Epoch 162/300, seasonal_1 Loss: 0.0644 | 0.0332
Epoch 163/300, seasonal_1 Loss: 0.0644 | 0.0332
Epoch 164/300, seasonal_1 Loss: 0.0644 | 0.0332
Epoch 165/300, seasonal_1 Loss: 0.0644 | 0.0332
Epoch 166/300, seasonal_1 Loss: 0.0644 | 0.0331
Epoch 167/300, seasonal_1 Loss: 0.0644 | 0.0331
Epoch 168/300, seasonal_1 Loss: 0.0644 | 0.0331
Epoch 169/300, seasonal_1 Loss: 0.0644 | 0.0331
Epoch 170/300, seasonal_1 Loss: 0.0644 | 0.0331
Epoch 171/300, seasonal_1 Loss: 0.0643 | 0.0331
Epoch 172/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 173/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 174/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 175/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 176/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 177/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 178/300, seasonal_1 Loss: 0.0643 | 0.0330
Epoch 179/300, seasonal_1 Loss: 0.0643 | 0.0329
Epoch 180/300, seasonal_1 Loss: 0.0643 | 0.0329
Epoch 181/300, seasonal_1 Loss: 0.0643 | 0.0329
Epoch 182/300, seasonal_1 Loss: 0.0643 | 0.0329
Epoch 183/300, seasonal_1 Loss: 0.0642 | 0.0329
Epoch 184/300, seasonal_1 Loss: 0.0642 | 0.0329
Epoch 185/300, seasonal_1 Loss: 0.0642 | 0.0329
Epoch 186/300, seasonal_1 Loss: 0.0642 | 0.0329
Epoch 187/300, seasonal_1 Loss: 0.0642 | 0.0329
Epoch 188/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 189/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 190/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 191/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 192/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 193/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 194/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 195/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 196/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 197/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 198/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 199/300, seasonal_1 Loss: 0.0642 | 0.0328
Epoch 200/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 201/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 202/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 203/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 204/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 205/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 206/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 207/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 208/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 209/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 210/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 211/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 212/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 213/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 214/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 215/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 216/300, seasonal_1 Loss: 0.0641 | 0.0327
Epoch 217/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 218/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 219/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 220/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 221/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 222/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 223/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 224/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 225/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 226/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 227/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 228/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 229/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 230/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 231/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 232/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 233/300, seasonal_1 Loss: 0.0641 | 0.0326
Epoch 234/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 235/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 236/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 237/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 238/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 239/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 240/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 241/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 242/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 243/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 244/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 245/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 246/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 247/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 248/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 249/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 250/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 251/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 252/300, seasonal_1 Loss: 0.0640 | 0.0326
Epoch 253/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 254/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 255/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 256/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 257/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 258/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 259/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 260/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 261/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 262/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 263/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 264/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 265/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 266/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 267/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 268/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 269/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 270/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 271/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 272/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 273/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 274/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 275/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 276/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 277/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 278/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 279/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 280/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 281/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 282/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 283/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 284/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 285/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 286/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 287/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 288/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 289/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 290/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 291/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 292/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 293/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 294/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 295/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 296/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 297/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 298/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 299/300, seasonal_1 Loss: 0.0640 | 0.0325
Epoch 300/300, seasonal_1 Loss: 0.0640 | 0.0325
Training seasonal_2 component with params: {'observation_period_num': 11, 'train_rates': 0.8611427714359929, 'learning_rate': 0.00030968857092537557, 'batch_size': 168, 'step_size': 7, 'gamma': 0.9633179998461963}
Epoch 1/300, seasonal_2 Loss: 0.5525 | 0.2445
Epoch 2/300, seasonal_2 Loss: 0.1953 | 0.2627
Epoch 3/300, seasonal_2 Loss: 0.1915 | 0.3020
Epoch 4/300, seasonal_2 Loss: 0.1671 | 0.1651
Epoch 5/300, seasonal_2 Loss: 0.1562 | 0.1252
Epoch 6/300, seasonal_2 Loss: 0.1642 | 0.1162
Epoch 7/300, seasonal_2 Loss: 0.1402 | 0.1254
Epoch 8/300, seasonal_2 Loss: 0.1441 | 0.1876
Epoch 9/300, seasonal_2 Loss: 0.1421 | 0.1268
Epoch 10/300, seasonal_2 Loss: 0.1428 | 0.0861
Epoch 11/300, seasonal_2 Loss: 0.1413 | 0.1160
Epoch 12/300, seasonal_2 Loss: 0.1448 | 0.0884
Epoch 13/300, seasonal_2 Loss: 0.1643 | 0.1147
Epoch 14/300, seasonal_2 Loss: 0.1574 | 0.0770
Epoch 15/300, seasonal_2 Loss: 0.1303 | 0.1126
Epoch 16/300, seasonal_2 Loss: 0.1383 | 0.0785
Epoch 17/300, seasonal_2 Loss: 0.1291 | 0.1587
Epoch 18/300, seasonal_2 Loss: 0.1285 | 0.0974
Epoch 19/300, seasonal_2 Loss: 0.1056 | 0.0684
Epoch 20/300, seasonal_2 Loss: 0.1026 | 0.0626
Epoch 21/300, seasonal_2 Loss: 0.1027 | 0.0580
Epoch 22/300, seasonal_2 Loss: 0.0937 | 0.0580
Epoch 23/300, seasonal_2 Loss: 0.0959 | 0.0557
Epoch 24/300, seasonal_2 Loss: 0.0921 | 0.0529
Epoch 25/300, seasonal_2 Loss: 0.0903 | 0.0539
Epoch 26/300, seasonal_2 Loss: 0.0901 | 0.0517
Epoch 27/300, seasonal_2 Loss: 0.0878 | 0.0497
Epoch 28/300, seasonal_2 Loss: 0.0876 | 0.0502
Epoch 29/300, seasonal_2 Loss: 0.0868 | 0.0486
Epoch 30/300, seasonal_2 Loss: 0.0860 | 0.0504
Epoch 31/300, seasonal_2 Loss: 0.0860 | 0.0564
Epoch 32/300, seasonal_2 Loss: 0.0855 | 0.0553
Epoch 33/300, seasonal_2 Loss: 0.0839 | 0.0481
Epoch 34/300, seasonal_2 Loss: 0.0823 | 0.0451
Epoch 35/300, seasonal_2 Loss: 0.0815 | 0.0445
Epoch 36/300, seasonal_2 Loss: 0.0807 | 0.0438
Epoch 37/300, seasonal_2 Loss: 0.0804 | 0.0465
Epoch 38/300, seasonal_2 Loss: 0.0803 | 0.0524
Epoch 39/300, seasonal_2 Loss: 0.0803 | 0.0516
Epoch 40/300, seasonal_2 Loss: 0.0796 | 0.0449
Epoch 41/300, seasonal_2 Loss: 0.0786 | 0.0413
Epoch 42/300, seasonal_2 Loss: 0.0789 | 0.0411
Epoch 43/300, seasonal_2 Loss: 0.0808 | 0.0511
Epoch 44/300, seasonal_2 Loss: 0.0869 | 0.0697
Epoch 45/300, seasonal_2 Loss: 0.0898 | 0.0551
Epoch 46/300, seasonal_2 Loss: 0.0903 | 0.0530
Epoch 47/300, seasonal_2 Loss: 0.0892 | 0.0767
Epoch 48/300, seasonal_2 Loss: 0.0887 | 0.0551
Epoch 49/300, seasonal_2 Loss: 0.0867 | 0.0535
Epoch 50/300, seasonal_2 Loss: 0.0915 | 0.0510
Epoch 51/300, seasonal_2 Loss: 0.0883 | 0.0483
Epoch 52/300, seasonal_2 Loss: 0.0856 | 0.0663
Epoch 53/300, seasonal_2 Loss: 0.0837 | 0.0598
Epoch 54/300, seasonal_2 Loss: 0.0803 | 0.0400
Epoch 55/300, seasonal_2 Loss: 0.0779 | 0.0410
Epoch 56/300, seasonal_2 Loss: 0.0801 | 0.0415
Epoch 57/300, seasonal_2 Loss: 0.0787 | 0.0442
Epoch 58/300, seasonal_2 Loss: 0.0764 | 0.0455
Epoch 59/300, seasonal_2 Loss: 0.0752 | 0.0391
Epoch 60/300, seasonal_2 Loss: 0.0751 | 0.0386
Epoch 61/300, seasonal_2 Loss: 0.0751 | 0.0408
Epoch 62/300, seasonal_2 Loss: 0.0753 | 0.0430
Epoch 63/300, seasonal_2 Loss: 0.0761 | 0.0415
Epoch 64/300, seasonal_2 Loss: 0.0741 | 0.0395
Epoch 65/300, seasonal_2 Loss: 0.0726 | 0.0372
Epoch 66/300, seasonal_2 Loss: 0.0743 | 0.0387
Epoch 67/300, seasonal_2 Loss: 0.0758 | 0.0397
Epoch 68/300, seasonal_2 Loss: 0.0753 | 0.0399
Epoch 69/300, seasonal_2 Loss: 0.0736 | 0.0385
Epoch 70/300, seasonal_2 Loss: 0.0728 | 0.0377
Epoch 71/300, seasonal_2 Loss: 0.0738 | 0.0383
Epoch 72/300, seasonal_2 Loss: 0.0731 | 0.0387
Epoch 73/300, seasonal_2 Loss: 0.0733 | 0.0399
Epoch 74/300, seasonal_2 Loss: 0.0729 | 0.0390
Epoch 75/300, seasonal_2 Loss: 0.0708 | 0.0361
Epoch 76/300, seasonal_2 Loss: 0.0706 | 0.0361
Epoch 77/300, seasonal_2 Loss: 0.0714 | 0.0366
Epoch 78/300, seasonal_2 Loss: 0.0711 | 0.0371
Epoch 79/300, seasonal_2 Loss: 0.0702 | 0.0395
Epoch 80/300, seasonal_2 Loss: 0.0697 | 0.0375
Epoch 81/300, seasonal_2 Loss: 0.0698 | 0.0359
Epoch 82/300, seasonal_2 Loss: 0.0697 | 0.0355
Epoch 83/300, seasonal_2 Loss: 0.0693 | 0.0349
Epoch 84/300, seasonal_2 Loss: 0.0696 | 0.0353
Epoch 85/300, seasonal_2 Loss: 0.0697 | 0.0358
Epoch 86/300, seasonal_2 Loss: 0.0691 | 0.0342
Epoch 87/300, seasonal_2 Loss: 0.0683 | 0.0334
Epoch 88/300, seasonal_2 Loss: 0.0681 | 0.0340
Epoch 89/300, seasonal_2 Loss: 0.0680 | 0.0356
Epoch 90/300, seasonal_2 Loss: 0.0678 | 0.0369
Epoch 91/300, seasonal_2 Loss: 0.0676 | 0.0365
Epoch 92/300, seasonal_2 Loss: 0.0673 | 0.0357
Epoch 93/300, seasonal_2 Loss: 0.0673 | 0.0347
Epoch 94/300, seasonal_2 Loss: 0.0670 | 0.0338
Epoch 95/300, seasonal_2 Loss: 0.0667 | 0.0330
Epoch 96/300, seasonal_2 Loss: 0.0669 | 0.0328
Epoch 97/300, seasonal_2 Loss: 0.0676 | 0.0341
Epoch 98/300, seasonal_2 Loss: 0.0683 | 0.0355
Epoch 99/300, seasonal_2 Loss: 0.0676 | 0.0350
Epoch 100/300, seasonal_2 Loss: 0.0669 | 0.0341
Epoch 101/300, seasonal_2 Loss: 0.0666 | 0.0338
Epoch 102/300, seasonal_2 Loss: 0.0665 | 0.0339
Epoch 103/300, seasonal_2 Loss: 0.0666 | 0.0339
Epoch 104/300, seasonal_2 Loss: 0.0664 | 0.0333
Epoch 105/300, seasonal_2 Loss: 0.0658 | 0.0342
Epoch 106/300, seasonal_2 Loss: 0.0654 | 0.0352
Epoch 107/300, seasonal_2 Loss: 0.0657 | 0.0346
Epoch 108/300, seasonal_2 Loss: 0.0662 | 0.0342
Epoch 109/300, seasonal_2 Loss: 0.0659 | 0.0336
Epoch 110/300, seasonal_2 Loss: 0.0652 | 0.0342
Epoch 111/300, seasonal_2 Loss: 0.0653 | 0.0348
Epoch 112/300, seasonal_2 Loss: 0.0654 | 0.0342
Epoch 113/300, seasonal_2 Loss: 0.0660 | 0.0332
Epoch 114/300, seasonal_2 Loss: 0.0658 | 0.0324
Epoch 115/300, seasonal_2 Loss: 0.0648 | 0.0321
Epoch 116/300, seasonal_2 Loss: 0.0669 | 0.0356
Epoch 117/300, seasonal_2 Loss: 0.0680 | 0.0399
Epoch 118/300, seasonal_2 Loss: 0.0673 | 0.0342
Epoch 119/300, seasonal_2 Loss: 0.0668 | 0.0391
Epoch 120/300, seasonal_2 Loss: 0.0670 | 0.0344
Epoch 121/300, seasonal_2 Loss: 0.0657 | 0.0389
Epoch 122/300, seasonal_2 Loss: 0.0685 | 0.0500
Epoch 123/300, seasonal_2 Loss: 0.0697 | 0.0344
Epoch 124/300, seasonal_2 Loss: 0.0646 | 0.0313
Epoch 125/300, seasonal_2 Loss: 0.0645 | 0.0335
Epoch 126/300, seasonal_2 Loss: 0.0651 | 0.0352
Epoch 127/300, seasonal_2 Loss: 0.0634 | 0.0332
Epoch 128/300, seasonal_2 Loss: 0.0631 | 0.0322
Epoch 129/300, seasonal_2 Loss: 0.0625 | 0.0323
Epoch 130/300, seasonal_2 Loss: 0.0625 | 0.0329
Epoch 131/300, seasonal_2 Loss: 0.0624 | 0.0317
Epoch 132/300, seasonal_2 Loss: 0.0623 | 0.0307
Epoch 133/300, seasonal_2 Loss: 0.0623 | 0.0309
Epoch 134/300, seasonal_2 Loss: 0.0635 | 0.0318
Epoch 135/300, seasonal_2 Loss: 0.0648 | 0.0295
Epoch 136/300, seasonal_2 Loss: 0.0630 | 0.0310
Epoch 137/300, seasonal_2 Loss: 0.0625 | 0.0330
Epoch 138/300, seasonal_2 Loss: 0.0624 | 0.0342
Epoch 139/300, seasonal_2 Loss: 0.0623 | 0.0327
Epoch 140/300, seasonal_2 Loss: 0.0618 | 0.0331
Epoch 141/300, seasonal_2 Loss: 0.0616 | 0.0321
Epoch 142/300, seasonal_2 Loss: 0.0611 | 0.0325
Epoch 143/300, seasonal_2 Loss: 0.0610 | 0.0316
Epoch 144/300, seasonal_2 Loss: 0.0613 | 0.0308
Epoch 145/300, seasonal_2 Loss: 0.0619 | 0.0297
Epoch 146/300, seasonal_2 Loss: 0.0635 | 0.0308
Epoch 147/300, seasonal_2 Loss: 0.0639 | 0.0290
Epoch 148/300, seasonal_2 Loss: 0.0630 | 0.0293
Epoch 149/300, seasonal_2 Loss: 0.0628 | 0.0282
Epoch 150/300, seasonal_2 Loss: 0.0622 | 0.0310
Epoch 151/300, seasonal_2 Loss: 0.0616 | 0.0325
Epoch 152/300, seasonal_2 Loss: 0.0609 | 0.0309
Epoch 153/300, seasonal_2 Loss: 0.0608 | 0.0298
Epoch 154/300, seasonal_2 Loss: 0.0605 | 0.0319
Epoch 155/300, seasonal_2 Loss: 0.0608 | 0.0333
Epoch 156/300, seasonal_2 Loss: 0.0607 | 0.0334
Epoch 157/300, seasonal_2 Loss: 0.0602 | 0.0313
Epoch 158/300, seasonal_2 Loss: 0.0598 | 0.0308
Epoch 159/300, seasonal_2 Loss: 0.0598 | 0.0302
Epoch 160/300, seasonal_2 Loss: 0.0601 | 0.0295
Epoch 161/300, seasonal_2 Loss: 0.0611 | 0.0292
Epoch 162/300, seasonal_2 Loss: 0.0624 | 0.0320
Epoch 163/300, seasonal_2 Loss: 0.0605 | 0.0294
Epoch 164/300, seasonal_2 Loss: 0.0615 | 0.0324
Epoch 165/300, seasonal_2 Loss: 0.0605 | 0.0283
Epoch 166/300, seasonal_2 Loss: 0.0610 | 0.0305
Epoch 167/300, seasonal_2 Loss: 0.0601 | 0.0299
Epoch 168/300, seasonal_2 Loss: 0.0603 | 0.0315
Epoch 169/300, seasonal_2 Loss: 0.0593 | 0.0304
Epoch 170/300, seasonal_2 Loss: 0.0592 | 0.0304
Epoch 171/300, seasonal_2 Loss: 0.0596 | 0.0327
Epoch 172/300, seasonal_2 Loss: 0.0598 | 0.0322
Epoch 173/300, seasonal_2 Loss: 0.0594 | 0.0310
Epoch 174/300, seasonal_2 Loss: 0.0587 | 0.0294
Epoch 175/300, seasonal_2 Loss: 0.0589 | 0.0291
Epoch 176/300, seasonal_2 Loss: 0.0596 | 0.0283
Epoch 177/300, seasonal_2 Loss: 0.0605 | 0.0281
Epoch 178/300, seasonal_2 Loss: 0.0603 | 0.0291
Epoch 179/300, seasonal_2 Loss: 0.0601 | 0.0298
Epoch 180/300, seasonal_2 Loss: 0.0591 | 0.0293
Epoch 181/300, seasonal_2 Loss: 0.0587 | 0.0299
Epoch 182/300, seasonal_2 Loss: 0.0586 | 0.0314
Epoch 183/300, seasonal_2 Loss: 0.0585 | 0.0297
Epoch 184/300, seasonal_2 Loss: 0.0581 | 0.0288
Epoch 185/300, seasonal_2 Loss: 0.0580 | 0.0281
Epoch 186/300, seasonal_2 Loss: 0.0581 | 0.0284
Epoch 187/300, seasonal_2 Loss: 0.0587 | 0.0271
Epoch 188/300, seasonal_2 Loss: 0.0591 | 0.0273
Epoch 189/300, seasonal_2 Loss: 0.0583 | 0.0302
Epoch 190/300, seasonal_2 Loss: 0.0600 | 0.0295
Epoch 191/300, seasonal_2 Loss: 0.0595 | 0.0295
Epoch 192/300, seasonal_2 Loss: 0.0580 | 0.0277
Epoch 193/300, seasonal_2 Loss: 0.0581 | 0.0277
Epoch 194/300, seasonal_2 Loss: 0.0583 | 0.0271
Epoch 195/300, seasonal_2 Loss: 0.0579 | 0.0284
Epoch 196/300, seasonal_2 Loss: 0.0579 | 0.0295
Epoch 197/300, seasonal_2 Loss: 0.0586 | 0.0298
Epoch 198/300, seasonal_2 Loss: 0.0577 | 0.0281
Epoch 199/300, seasonal_2 Loss: 0.0574 | 0.0269
Epoch 200/300, seasonal_2 Loss: 0.0576 | 0.0272
Epoch 201/300, seasonal_2 Loss: 0.0580 | 0.0267
Epoch 202/300, seasonal_2 Loss: 0.0574 | 0.0284
Epoch 203/300, seasonal_2 Loss: 0.0580 | 0.0290
Epoch 204/300, seasonal_2 Loss: 0.0577 | 0.0283
Epoch 205/300, seasonal_2 Loss: 0.0571 | 0.0269
Epoch 206/300, seasonal_2 Loss: 0.0573 | 0.0266
Epoch 207/300, seasonal_2 Loss: 0.0573 | 0.0268
Epoch 208/300, seasonal_2 Loss: 0.0571 | 0.0274
Epoch 209/300, seasonal_2 Loss: 0.0570 | 0.0279
Epoch 210/300, seasonal_2 Loss: 0.0573 | 0.0288
Epoch 211/300, seasonal_2 Loss: 0.0567 | 0.0270
Epoch 212/300, seasonal_2 Loss: 0.0566 | 0.0262
Epoch 213/300, seasonal_2 Loss: 0.0567 | 0.0266
Epoch 214/300, seasonal_2 Loss: 0.0568 | 0.0261
Epoch 215/300, seasonal_2 Loss: 0.0564 | 0.0270
Epoch 216/300, seasonal_2 Loss: 0.0568 | 0.0285
Epoch 217/300, seasonal_2 Loss: 0.0568 | 0.0276
Epoch 218/300, seasonal_2 Loss: 0.0563 | 0.0264
Epoch 219/300, seasonal_2 Loss: 0.0562 | 0.0264
Epoch 220/300, seasonal_2 Loss: 0.0564 | 0.0258
Epoch 221/300, seasonal_2 Loss: 0.0563 | 0.0260
Epoch 222/300, seasonal_2 Loss: 0.0561 | 0.0274
Epoch 223/300, seasonal_2 Loss: 0.0565 | 0.0276
Epoch 224/300, seasonal_2 Loss: 0.0562 | 0.0267
Epoch 225/300, seasonal_2 Loss: 0.0559 | 0.0262
Epoch 226/300, seasonal_2 Loss: 0.0560 | 0.0258
Epoch 227/300, seasonal_2 Loss: 0.0561 | 0.0254
Epoch 228/300, seasonal_2 Loss: 0.0559 | 0.0263
Epoch 229/300, seasonal_2 Loss: 0.0559 | 0.0271
Epoch 230/300, seasonal_2 Loss: 0.0561 | 0.0268
Epoch 231/300, seasonal_2 Loss: 0.0558 | 0.0263
Epoch 232/300, seasonal_2 Loss: 0.0556 | 0.0257
Epoch 233/300, seasonal_2 Loss: 0.0557 | 0.0253
Epoch 234/300, seasonal_2 Loss: 0.0557 | 0.0256
Epoch 235/300, seasonal_2 Loss: 0.0555 | 0.0263
Epoch 236/300, seasonal_2 Loss: 0.0557 | 0.0266
Epoch 237/300, seasonal_2 Loss: 0.0557 | 0.0264
Epoch 238/300, seasonal_2 Loss: 0.0554 | 0.0258
Epoch 239/300, seasonal_2 Loss: 0.0554 | 0.0253
Epoch 240/300, seasonal_2 Loss: 0.0555 | 0.0253
Epoch 241/300, seasonal_2 Loss: 0.0554 | 0.0256
Epoch 242/300, seasonal_2 Loss: 0.0553 | 0.0262
Epoch 243/300, seasonal_2 Loss: 0.0555 | 0.0264
Epoch 244/300, seasonal_2 Loss: 0.0554 | 0.0260
Epoch 245/300, seasonal_2 Loss: 0.0551 | 0.0254
Epoch 246/300, seasonal_2 Loss: 0.0552 | 0.0252
Epoch 247/300, seasonal_2 Loss: 0.0553 | 0.0252
Epoch 248/300, seasonal_2 Loss: 0.0551 | 0.0256
Epoch 249/300, seasonal_2 Loss: 0.0552 | 0.0261
Epoch 250/300, seasonal_2 Loss: 0.0553 | 0.0260
Epoch 251/300, seasonal_2 Loss: 0.0551 | 0.0255
Epoch 252/300, seasonal_2 Loss: 0.0550 | 0.0251
Epoch 253/300, seasonal_2 Loss: 0.0551 | 0.0250
Epoch 254/300, seasonal_2 Loss: 0.0550 | 0.0252
Epoch 255/300, seasonal_2 Loss: 0.0549 | 0.0257
Epoch 256/300, seasonal_2 Loss: 0.0551 | 0.0259
Epoch 257/300, seasonal_2 Loss: 0.0550 | 0.0255
Epoch 258/300, seasonal_2 Loss: 0.0548 | 0.0251
Epoch 259/300, seasonal_2 Loss: 0.0549 | 0.0248
Epoch 260/300, seasonal_2 Loss: 0.0549 | 0.0250
Epoch 261/300, seasonal_2 Loss: 0.0548 | 0.0255
Epoch 262/300, seasonal_2 Loss: 0.0549 | 0.0256
Epoch 263/300, seasonal_2 Loss: 0.0549 | 0.0254
Epoch 264/300, seasonal_2 Loss: 0.0547 | 0.0250
Epoch 265/300, seasonal_2 Loss: 0.0547 | 0.0248
Epoch 266/300, seasonal_2 Loss: 0.0548 | 0.0249
Epoch 267/300, seasonal_2 Loss: 0.0546 | 0.0253
Epoch 268/300, seasonal_2 Loss: 0.0547 | 0.0254
Epoch 269/300, seasonal_2 Loss: 0.0547 | 0.0252
Epoch 270/300, seasonal_2 Loss: 0.0545 | 0.0249
Epoch 271/300, seasonal_2 Loss: 0.0546 | 0.0247
Epoch 272/300, seasonal_2 Loss: 0.0546 | 0.0249
Epoch 273/300, seasonal_2 Loss: 0.0545 | 0.0252
Epoch 274/300, seasonal_2 Loss: 0.0546 | 0.0252
Epoch 275/300, seasonal_2 Loss: 0.0545 | 0.0250
Epoch 276/300, seasonal_2 Loss: 0.0544 | 0.0247
Epoch 277/300, seasonal_2 Loss: 0.0545 | 0.0246
Epoch 278/300, seasonal_2 Loss: 0.0544 | 0.0250
Epoch 279/300, seasonal_2 Loss: 0.0544 | 0.0251
Epoch 280/300, seasonal_2 Loss: 0.0545 | 0.0250
Epoch 281/300, seasonal_2 Loss: 0.0543 | 0.0248
Epoch 282/300, seasonal_2 Loss: 0.0543 | 0.0245
Epoch 283/300, seasonal_2 Loss: 0.0544 | 0.0247
Epoch 284/300, seasonal_2 Loss: 0.0543 | 0.0249
Epoch 285/300, seasonal_2 Loss: 0.0543 | 0.0249
Epoch 286/300, seasonal_2 Loss: 0.0543 | 0.0248
Epoch 287/300, seasonal_2 Loss: 0.0542 | 0.0245
Epoch 288/300, seasonal_2 Loss: 0.0543 | 0.0245
Epoch 289/300, seasonal_2 Loss: 0.0542 | 0.0248
Epoch 290/300, seasonal_2 Loss: 0.0542 | 0.0248
Epoch 291/300, seasonal_2 Loss: 0.0542 | 0.0247
Epoch 292/300, seasonal_2 Loss: 0.0541 | 0.0246
Epoch 293/300, seasonal_2 Loss: 0.0541 | 0.0244
Epoch 294/300, seasonal_2 Loss: 0.0541 | 0.0247
Epoch 295/300, seasonal_2 Loss: 0.0540 | 0.0247
Epoch 296/300, seasonal_2 Loss: 0.0541 | 0.0246
Epoch 297/300, seasonal_2 Loss: 0.0540 | 0.0245
Epoch 298/300, seasonal_2 Loss: 0.0540 | 0.0244
Epoch 299/300, seasonal_2 Loss: 0.0540 | 0.0246
Epoch 300/300, seasonal_2 Loss: 0.0540 | 0.0247
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.9751541628080517, 'learning_rate': 8.676292231311497e-05, 'batch_size': 22, 'step_size': 7, 'gamma': 0.9305264156335673}
Epoch 1/300, seasonal_3 Loss: 0.2124 | 0.1095
Epoch 2/300, seasonal_3 Loss: 0.1163 | 0.0818
Epoch 3/300, seasonal_3 Loss: 0.1019 | 0.0671
Epoch 4/300, seasonal_3 Loss: 0.0959 | 0.0643
Epoch 5/300, seasonal_3 Loss: 0.0920 | 0.0613
Epoch 6/300, seasonal_3 Loss: 0.0890 | 0.0584
Epoch 7/300, seasonal_3 Loss: 0.0861 | 0.0544
Epoch 8/300, seasonal_3 Loss: 0.0830 | 0.0508
Epoch 9/300, seasonal_3 Loss: 0.0807 | 0.0477
Epoch 10/300, seasonal_3 Loss: 0.0787 | 0.0450
Epoch 11/300, seasonal_3 Loss: 0.0766 | 0.0427
Epoch 12/300, seasonal_3 Loss: 0.0748 | 0.0413
Epoch 13/300, seasonal_3 Loss: 0.0735 | 0.0402
Epoch 14/300, seasonal_3 Loss: 0.0725 | 0.0394
Epoch 15/300, seasonal_3 Loss: 0.0715 | 0.0391
Epoch 16/300, seasonal_3 Loss: 0.0707 | 0.0386
Epoch 17/300, seasonal_3 Loss: 0.0699 | 0.0380
Epoch 18/300, seasonal_3 Loss: 0.0691 | 0.0374
Epoch 19/300, seasonal_3 Loss: 0.0681 | 0.0371
Epoch 20/300, seasonal_3 Loss: 0.0671 | 0.0364
Epoch 21/300, seasonal_3 Loss: 0.0660 | 0.0356
Epoch 22/300, seasonal_3 Loss: 0.0651 | 0.0359
Epoch 23/300, seasonal_3 Loss: 0.0644 | 0.0358
Epoch 24/300, seasonal_3 Loss: 0.0638 | 0.0359
Epoch 25/300, seasonal_3 Loss: 0.0634 | 0.0360
Epoch 26/300, seasonal_3 Loss: 0.0631 | 0.0366
Epoch 27/300, seasonal_3 Loss: 0.0627 | 0.0353
Epoch 28/300, seasonal_3 Loss: 0.0622 | 0.0345
Epoch 29/300, seasonal_3 Loss: 0.0617 | 0.0337
Epoch 30/300, seasonal_3 Loss: 0.0614 | 0.0330
Epoch 31/300, seasonal_3 Loss: 0.0611 | 0.0324
Epoch 32/300, seasonal_3 Loss: 0.0606 | 0.0319
Epoch 33/300, seasonal_3 Loss: 0.0602 | 0.0312
Epoch 34/300, seasonal_3 Loss: 0.0599 | 0.0309
Epoch 35/300, seasonal_3 Loss: 0.0596 | 0.0305
Epoch 36/300, seasonal_3 Loss: 0.0592 | 0.0307
Epoch 37/300, seasonal_3 Loss: 0.0589 | 0.0304
Epoch 38/300, seasonal_3 Loss: 0.0586 | 0.0301
Epoch 39/300, seasonal_3 Loss: 0.0583 | 0.0298
Epoch 40/300, seasonal_3 Loss: 0.0580 | 0.0298
Epoch 41/300, seasonal_3 Loss: 0.0577 | 0.0293
Epoch 42/300, seasonal_3 Loss: 0.0574 | 0.0289
Epoch 43/300, seasonal_3 Loss: 0.0572 | 0.0286
Epoch 44/300, seasonal_3 Loss: 0.0569 | 0.0282
Epoch 45/300, seasonal_3 Loss: 0.0567 | 0.0278
Epoch 46/300, seasonal_3 Loss: 0.0565 | 0.0275
Epoch 47/300, seasonal_3 Loss: 0.0563 | 0.0271
Epoch 48/300, seasonal_3 Loss: 0.0560 | 0.0268
Epoch 49/300, seasonal_3 Loss: 0.0559 | 0.0265
Epoch 50/300, seasonal_3 Loss: 0.0556 | 0.0260
Epoch 51/300, seasonal_3 Loss: 0.0554 | 0.0258
Epoch 52/300, seasonal_3 Loss: 0.0552 | 0.0255
Epoch 53/300, seasonal_3 Loss: 0.0550 | 0.0252
Epoch 54/300, seasonal_3 Loss: 0.0548 | 0.0247
Epoch 55/300, seasonal_3 Loss: 0.0545 | 0.0244
Epoch 56/300, seasonal_3 Loss: 0.0543 | 0.0241
Epoch 57/300, seasonal_3 Loss: 0.0541 | 0.0235
Epoch 58/300, seasonal_3 Loss: 0.0539 | 0.0233
Epoch 59/300, seasonal_3 Loss: 0.0537 | 0.0230
Epoch 60/300, seasonal_3 Loss: 0.0535 | 0.0227
Epoch 61/300, seasonal_3 Loss: 0.0532 | 0.0222
Epoch 62/300, seasonal_3 Loss: 0.0531 | 0.0220
Epoch 63/300, seasonal_3 Loss: 0.0529 | 0.0218
Epoch 64/300, seasonal_3 Loss: 0.0527 | 0.0214
Epoch 65/300, seasonal_3 Loss: 0.0526 | 0.0213
Epoch 66/300, seasonal_3 Loss: 0.0525 | 0.0212
Epoch 67/300, seasonal_3 Loss: 0.0523 | 0.0210
Epoch 68/300, seasonal_3 Loss: 0.0522 | 0.0209
Epoch 69/300, seasonal_3 Loss: 0.0521 | 0.0209
Epoch 70/300, seasonal_3 Loss: 0.0520 | 0.0208
Epoch 71/300, seasonal_3 Loss: 0.0519 | 0.0207
Epoch 72/300, seasonal_3 Loss: 0.0518 | 0.0207
Epoch 73/300, seasonal_3 Loss: 0.0517 | 0.0206
Epoch 74/300, seasonal_3 Loss: 0.0517 | 0.0206
Epoch 75/300, seasonal_3 Loss: 0.0516 | 0.0204
Epoch 76/300, seasonal_3 Loss: 0.0515 | 0.0204
Epoch 77/300, seasonal_3 Loss: 0.0514 | 0.0204
Epoch 78/300, seasonal_3 Loss: 0.0513 | 0.0203
Epoch 79/300, seasonal_3 Loss: 0.0513 | 0.0202
Epoch 80/300, seasonal_3 Loss: 0.0512 | 0.0202
Epoch 81/300, seasonal_3 Loss: 0.0511 | 0.0202
Epoch 82/300, seasonal_3 Loss: 0.0510 | 0.0201
Epoch 83/300, seasonal_3 Loss: 0.0510 | 0.0201
Epoch 84/300, seasonal_3 Loss: 0.0509 | 0.0201
Epoch 85/300, seasonal_3 Loss: 0.0508 | 0.0200
Epoch 86/300, seasonal_3 Loss: 0.0508 | 0.0200
Epoch 87/300, seasonal_3 Loss: 0.0508 | 0.0200
Epoch 88/300, seasonal_3 Loss: 0.0507 | 0.0200
Epoch 89/300, seasonal_3 Loss: 0.0506 | 0.0200
Epoch 90/300, seasonal_3 Loss: 0.0506 | 0.0200
Epoch 91/300, seasonal_3 Loss: 0.0505 | 0.0199
Epoch 92/300, seasonal_3 Loss: 0.0505 | 0.0199
Epoch 93/300, seasonal_3 Loss: 0.0504 | 0.0199
Epoch 94/300, seasonal_3 Loss: 0.0504 | 0.0199
Epoch 95/300, seasonal_3 Loss: 0.0503 | 0.0199
Epoch 96/300, seasonal_3 Loss: 0.0503 | 0.0199
Epoch 97/300, seasonal_3 Loss: 0.0502 | 0.0199
Epoch 98/300, seasonal_3 Loss: 0.0502 | 0.0199
Epoch 99/300, seasonal_3 Loss: 0.0501 | 0.0199
Epoch 100/300, seasonal_3 Loss: 0.0501 | 0.0199
Epoch 101/300, seasonal_3 Loss: 0.0501 | 0.0199
Epoch 102/300, seasonal_3 Loss: 0.0500 | 0.0199
Epoch 103/300, seasonal_3 Loss: 0.0500 | 0.0198
Epoch 104/300, seasonal_3 Loss: 0.0499 | 0.0198
Epoch 105/300, seasonal_3 Loss: 0.0499 | 0.0198
Epoch 106/300, seasonal_3 Loss: 0.0498 | 0.0198
Epoch 107/300, seasonal_3 Loss: 0.0498 | 0.0198
Epoch 108/300, seasonal_3 Loss: 0.0498 | 0.0198
Epoch 109/300, seasonal_3 Loss: 0.0497 | 0.0198
Epoch 110/300, seasonal_3 Loss: 0.0497 | 0.0198
Epoch 111/300, seasonal_3 Loss: 0.0497 | 0.0198
Epoch 112/300, seasonal_3 Loss: 0.0496 | 0.0197
Epoch 113/300, seasonal_3 Loss: 0.0496 | 0.0198
Epoch 114/300, seasonal_3 Loss: 0.0496 | 0.0198
Epoch 115/300, seasonal_3 Loss: 0.0495 | 0.0197
Epoch 116/300, seasonal_3 Loss: 0.0495 | 0.0197
Epoch 117/300, seasonal_3 Loss: 0.0495 | 0.0198
Epoch 118/300, seasonal_3 Loss: 0.0494 | 0.0198
Epoch 119/300, seasonal_3 Loss: 0.0494 | 0.0198
Epoch 120/300, seasonal_3 Loss: 0.0494 | 0.0199
Epoch 121/300, seasonal_3 Loss: 0.0494 | 0.0199
Epoch 122/300, seasonal_3 Loss: 0.0494 | 0.0199
Epoch 123/300, seasonal_3 Loss: 0.0493 | 0.0199
Epoch 124/300, seasonal_3 Loss: 0.0493 | 0.0201
Epoch 125/300, seasonal_3 Loss: 0.0493 | 0.0201
Epoch 126/300, seasonal_3 Loss: 0.0493 | 0.0201
Epoch 127/300, seasonal_3 Loss: 0.0493 | 0.0203
Epoch 128/300, seasonal_3 Loss: 0.0492 | 0.0203
Epoch 129/300, seasonal_3 Loss: 0.0492 | 0.0203
Epoch 130/300, seasonal_3 Loss: 0.0492 | 0.0204
Epoch 131/300, seasonal_3 Loss: 0.0492 | 0.0205
Epoch 132/300, seasonal_3 Loss: 0.0492 | 0.0206
Epoch 133/300, seasonal_3 Loss: 0.0492 | 0.0206
Epoch 134/300, seasonal_3 Loss: 0.0491 | 0.0207
Epoch 135/300, seasonal_3 Loss: 0.0491 | 0.0207
Epoch 136/300, seasonal_3 Loss: 0.0491 | 0.0207
Epoch 137/300, seasonal_3 Loss: 0.0491 | 0.0207
Epoch 138/300, seasonal_3 Loss: 0.0491 | 0.0208
Epoch 139/300, seasonal_3 Loss: 0.0490 | 0.0207
Epoch 140/300, seasonal_3 Loss: 0.0490 | 0.0207
Epoch 141/300, seasonal_3 Loss: 0.0490 | 0.0207
Epoch 142/300, seasonal_3 Loss: 0.0490 | 0.0206
Epoch 143/300, seasonal_3 Loss: 0.0490 | 0.0206
Epoch 144/300, seasonal_3 Loss: 0.0489 | 0.0206
Epoch 145/300, seasonal_3 Loss: 0.0489 | 0.0205
Epoch 146/300, seasonal_3 Loss: 0.0489 | 0.0204
Epoch 147/300, seasonal_3 Loss: 0.0489 | 0.0204
Epoch 148/300, seasonal_3 Loss: 0.0488 | 0.0203
Epoch 149/300, seasonal_3 Loss: 0.0488 | 0.0202
Epoch 150/300, seasonal_3 Loss: 0.0488 | 0.0202
Epoch 151/300, seasonal_3 Loss: 0.0488 | 0.0202
Epoch 152/300, seasonal_3 Loss: 0.0488 | 0.0200
Epoch 153/300, seasonal_3 Loss: 0.0488 | 0.0200
Epoch 154/300, seasonal_3 Loss: 0.0487 | 0.0200
Epoch 155/300, seasonal_3 Loss: 0.0487 | 0.0199
Epoch 156/300, seasonal_3 Loss: 0.0487 | 0.0199
Epoch 157/300, seasonal_3 Loss: 0.0487 | 0.0199
Epoch 158/300, seasonal_3 Loss: 0.0487 | 0.0199
Epoch 159/300, seasonal_3 Loss: 0.0486 | 0.0197
Epoch 160/300, seasonal_3 Loss: 0.0486 | 0.0197
Epoch 161/300, seasonal_3 Loss: 0.0486 | 0.0197
Epoch 162/300, seasonal_3 Loss: 0.0486 | 0.0196
Epoch 163/300, seasonal_3 Loss: 0.0486 | 0.0196
Epoch 164/300, seasonal_3 Loss: 0.0486 | 0.0196
Epoch 165/300, seasonal_3 Loss: 0.0485 | 0.0196
Epoch 166/300, seasonal_3 Loss: 0.0485 | 0.0196
Epoch 167/300, seasonal_3 Loss: 0.0485 | 0.0196
Epoch 168/300, seasonal_3 Loss: 0.0485 | 0.0196
Epoch 169/300, seasonal_3 Loss: 0.0485 | 0.0195
Epoch 170/300, seasonal_3 Loss: 0.0485 | 0.0195
Epoch 171/300, seasonal_3 Loss: 0.0485 | 0.0195
Epoch 172/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 173/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 174/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 175/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 176/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 177/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 178/300, seasonal_3 Loss: 0.0484 | 0.0195
Epoch 179/300, seasonal_3 Loss: 0.0483 | 0.0195
Epoch 180/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 181/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 182/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 183/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 184/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 185/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 186/300, seasonal_3 Loss: 0.0483 | 0.0194
Epoch 187/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 188/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 189/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 190/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 191/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 192/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 193/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 194/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 195/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 196/300, seasonal_3 Loss: 0.0482 | 0.0195
Epoch 197/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 198/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 199/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 200/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 201/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 202/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 203/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 204/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 205/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 206/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 207/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 208/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 209/300, seasonal_3 Loss: 0.0481 | 0.0195
Epoch 210/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 211/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 212/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 213/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 214/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 215/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 216/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 217/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 218/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 219/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 220/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 221/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 222/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 223/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 224/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 225/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 226/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 227/300, seasonal_3 Loss: 0.0480 | 0.0195
Epoch 228/300, seasonal_3 Loss: 0.0479 | 0.0195
Epoch 229/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 230/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 231/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 232/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 233/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 234/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 235/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 236/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 237/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 238/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 239/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 240/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 241/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 242/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 243/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 244/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 245/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 246/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 247/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 248/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 249/300, seasonal_3 Loss: 0.0479 | 0.0194
Epoch 250/300, seasonal_3 Loss: 0.0479 | 0.0195
Epoch 251/300, seasonal_3 Loss: 0.0479 | 0.0195
Epoch 252/300, seasonal_3 Loss: 0.0479 | 0.0195
Epoch 253/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 254/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 255/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 256/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 257/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 258/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 259/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 260/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 261/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 262/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 263/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 264/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 265/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 266/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 267/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 268/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 269/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 270/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 271/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 272/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 273/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 274/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 275/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 276/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 277/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 278/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 279/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 280/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 281/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 282/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 283/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 284/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 285/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 286/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 287/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 288/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 289/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 290/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 291/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 292/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 293/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 294/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 295/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 296/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 297/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 298/300, seasonal_3 Loss: 0.0478 | 0.0195
Epoch 299/300, seasonal_3 Loss: 0.0477 | 0.0195
Epoch 300/300, seasonal_3 Loss: 0.0477 | 0.0195
Training resid component with params: {'observation_period_num': 13, 'train_rates': 0.9894631993442563, 'learning_rate': 0.0007189150108673919, 'batch_size': 188, 'step_size': 12, 'gamma': 0.9359711667805197}
Epoch 1/300, resid Loss: 0.4836 | 0.2628
Epoch 2/300, resid Loss: 0.1976 | 0.1449
Epoch 3/300, resid Loss: 0.1577 | 0.1574
Epoch 4/300, resid Loss: 0.2024 | 0.1779
Epoch 5/300, resid Loss: 0.2768 | 0.5953
Epoch 6/300, resid Loss: 0.1919 | 0.1778
Epoch 7/300, resid Loss: 0.1600 | 0.2152
Epoch 8/300, resid Loss: 0.1716 | 0.1060
Epoch 9/300, resid Loss: 0.1283 | 0.1043
Epoch 10/300, resid Loss: 0.1190 | 0.1116
Epoch 11/300, resid Loss: 0.1119 | 0.0719
Epoch 12/300, resid Loss: 0.1161 | 0.0761
Epoch 13/300, resid Loss: 0.1679 | 0.0963
Epoch 14/300, resid Loss: 0.1715 | 0.2728
Epoch 15/300, resid Loss: 0.1506 | 0.0886
Epoch 16/300, resid Loss: 0.1050 | 0.0855
Epoch 17/300, resid Loss: 0.1051 | 0.0853
Epoch 18/300, resid Loss: 0.0961 | 0.0755
Epoch 19/300, resid Loss: 0.0909 | 0.0641
Epoch 20/300, resid Loss: 0.0888 | 0.0628
Epoch 21/300, resid Loss: 0.0884 | 0.0659
Epoch 22/300, resid Loss: 0.0856 | 0.0632
Epoch 23/300, resid Loss: 0.0835 | 0.0537
Epoch 24/300, resid Loss: 0.0802 | 0.0556
Epoch 25/300, resid Loss: 0.0797 | 0.0596
Epoch 26/300, resid Loss: 0.0781 | 0.0565
Epoch 27/300, resid Loss: 0.0765 | 0.0514
Epoch 28/300, resid Loss: 0.0751 | 0.0492
Epoch 29/300, resid Loss: 0.0771 | 0.0491
Epoch 30/300, resid Loss: 0.0868 | 0.0702
Epoch 31/300, resid Loss: 0.0939 | 0.0643
Epoch 32/300, resid Loss: 0.0835 | 0.0666
Epoch 33/300, resid Loss: 0.0796 | 0.0544
Epoch 34/300, resid Loss: 0.0746 | 0.0469
Epoch 35/300, resid Loss: 0.0733 | 0.0449
Epoch 36/300, resid Loss: 0.0710 | 0.0429
Epoch 37/300, resid Loss: 0.0699 | 0.0416
Epoch 38/300, resid Loss: 0.0689 | 0.0403
Epoch 39/300, resid Loss: 0.0688 | 0.0406
Epoch 40/300, resid Loss: 0.0700 | 0.0428
Epoch 41/300, resid Loss: 0.0721 | 0.0512
Epoch 42/300, resid Loss: 0.0735 | 0.0596
Epoch 43/300, resid Loss: 0.0733 | 0.0556
Epoch 44/300, resid Loss: 0.0722 | 0.0452
Epoch 45/300, resid Loss: 0.0731 | 0.0432
Epoch 46/300, resid Loss: 0.0798 | 0.0628
Epoch 47/300, resid Loss: 0.0815 | 0.0489
Epoch 48/300, resid Loss: 0.0810 | 0.0665
Epoch 49/300, resid Loss: 0.0957 | 0.0832
Epoch 50/300, resid Loss: 0.0873 | 0.0493
Epoch 51/300, resid Loss: 0.0885 | 0.0558
Epoch 52/300, resid Loss: 0.0746 | 0.0455
Epoch 53/300, resid Loss: 0.0849 | 0.0514
Epoch 54/300, resid Loss: 0.1031 | 0.0580
Epoch 55/300, resid Loss: 0.0927 | 0.0630
Epoch 56/300, resid Loss: 0.0840 | 0.0431
Epoch 57/300, resid Loss: 0.0838 | 0.0654
Epoch 58/300, resid Loss: 0.0821 | 0.0585
Epoch 59/300, resid Loss: 0.0735 | 0.0436
Epoch 60/300, resid Loss: 0.0669 | 0.0420
Epoch 61/300, resid Loss: 0.0666 | 0.0409
Epoch 62/300, resid Loss: 0.0648 | 0.0393
Epoch 63/300, resid Loss: 0.0647 | 0.0394
Epoch 64/300, resid Loss: 0.0647 | 0.0410
Epoch 65/300, resid Loss: 0.0639 | 0.0384
Epoch 66/300, resid Loss: 0.0642 | 0.0372
Epoch 67/300, resid Loss: 0.0657 | 0.0378
Epoch 68/300, resid Loss: 0.0685 | 0.0379
Epoch 69/300, resid Loss: 0.0687 | 0.0484
Epoch 70/300, resid Loss: 0.0663 | 0.0487
Epoch 71/300, resid Loss: 0.0649 | 0.0410
Epoch 72/300, resid Loss: 0.0657 | 0.0420
Epoch 73/300, resid Loss: 0.0678 | 0.0444
Epoch 74/300, resid Loss: 0.0670 | 0.0373
Epoch 75/300, resid Loss: 0.0667 | 0.0501
Epoch 76/300, resid Loss: 0.0669 | 0.0386
Epoch 77/300, resid Loss: 0.0667 | 0.0395
Epoch 78/300, resid Loss: 0.0697 | 0.0410
Epoch 79/300, resid Loss: 0.0689 | 0.0450
Epoch 80/300, resid Loss: 0.0640 | 0.0373
Epoch 81/300, resid Loss: 0.0628 | 0.0328
Epoch 82/300, resid Loss: 0.0608 | 0.0330
Epoch 83/300, resid Loss: 0.0603 | 0.0330
Epoch 84/300, resid Loss: 0.0605 | 0.0320
Epoch 85/300, resid Loss: 0.0602 | 0.0354
Epoch 86/300, resid Loss: 0.0589 | 0.0313
Epoch 87/300, resid Loss: 0.0582 | 0.0313
Epoch 88/300, resid Loss: 0.0586 | 0.0300
Epoch 89/300, resid Loss: 0.0579 | 0.0306
Epoch 90/300, resid Loss: 0.0573 | 0.0309
Epoch 91/300, resid Loss: 0.0578 | 0.0321
Epoch 92/300, resid Loss: 0.0579 | 0.0315
Epoch 93/300, resid Loss: 0.0601 | 0.0346
Epoch 94/300, resid Loss: 0.0638 | 0.0375
Epoch 95/300, resid Loss: 0.0625 | 0.0288
Epoch 96/300, resid Loss: 0.0596 | 0.0284
Epoch 97/300, resid Loss: 0.0606 | 0.0320
Epoch 98/300, resid Loss: 0.0634 | 0.0353
Epoch 99/300, resid Loss: 0.0631 | 0.0368
Epoch 100/300, resid Loss: 0.0590 | 0.0323
Epoch 101/300, resid Loss: 0.0586 | 0.0318
Epoch 102/300, resid Loss: 0.0591 | 0.0295
Epoch 103/300, resid Loss: 0.0567 | 0.0243
Epoch 104/300, resid Loss: 0.0567 | 0.0266
Epoch 105/300, resid Loss: 0.0558 | 0.0260
Epoch 106/300, resid Loss: 0.0554 | 0.0265
Epoch 107/300, resid Loss: 0.0563 | 0.0278
Epoch 108/300, resid Loss: 0.0557 | 0.0266
Epoch 109/300, resid Loss: 0.0580 | 0.0274
Epoch 110/300, resid Loss: 0.0586 | 0.0307
Epoch 111/300, resid Loss: 0.0567 | 0.0252
Epoch 112/300, resid Loss: 0.0575 | 0.0269
Epoch 113/300, resid Loss: 0.0556 | 0.0211
Epoch 114/300, resid Loss: 0.0569 | 0.0227
Epoch 115/300, resid Loss: 0.0569 | 0.0264
Epoch 116/300, resid Loss: 0.0571 | 0.0214
Epoch 117/300, resid Loss: 0.0562 | 0.0262
Epoch 118/300, resid Loss: 0.0540 | 0.0205
Epoch 119/300, resid Loss: 0.0536 | 0.0196
Epoch 120/300, resid Loss: 0.0529 | 0.0190
Epoch 121/300, resid Loss: 0.0528 | 0.0190
Epoch 122/300, resid Loss: 0.0526 | 0.0188
Epoch 123/300, resid Loss: 0.0525 | 0.0186
Epoch 124/300, resid Loss: 0.0526 | 0.0184
Epoch 125/300, resid Loss: 0.0530 | 0.0192
Epoch 126/300, resid Loss: 0.0546 | 0.0240
Epoch 127/300, resid Loss: 0.0556 | 0.0279
Epoch 128/300, resid Loss: 0.0554 | 0.0222
Epoch 129/300, resid Loss: 0.0548 | 0.0196
Epoch 130/300, resid Loss: 0.0570 | 0.0203
Epoch 131/300, resid Loss: 0.0550 | 0.0182
Epoch 132/300, resid Loss: 0.0547 | 0.0235
Epoch 133/300, resid Loss: 0.0547 | 0.0212
Epoch 134/300, resid Loss: 0.0541 | 0.0163
Epoch 135/300, resid Loss: 0.0555 | 0.0302
Epoch 136/300, resid Loss: 0.0602 | 0.0253
Epoch 137/300, resid Loss: 0.0580 | 0.0211
Epoch 138/300, resid Loss: 0.0565 | 0.0212
Epoch 139/300, resid Loss: 0.0545 | 0.0190
Epoch 140/300, resid Loss: 0.0533 | 0.0195
Epoch 141/300, resid Loss: 0.0528 | 0.0195
Epoch 142/300, resid Loss: 0.0519 | 0.0177
Epoch 143/300, resid Loss: 0.0511 | 0.0168
Epoch 144/300, resid Loss: 0.0517 | 0.0171
Epoch 145/300, resid Loss: 0.0537 | 0.0177
Epoch 146/300, resid Loss: 0.0523 | 0.0235
Epoch 147/300, resid Loss: 0.0536 | 0.0186
Epoch 148/300, resid Loss: 0.0511 | 0.0176
Epoch 149/300, resid Loss: 0.0507 | 0.0190
Epoch 150/300, resid Loss: 0.0505 | 0.0178
Epoch 151/300, resid Loss: 0.0500 | 0.0169
Epoch 152/300, resid Loss: 0.0497 | 0.0170
Epoch 153/300, resid Loss: 0.0495 | 0.0174
Epoch 154/300, resid Loss: 0.0499 | 0.0172
Epoch 155/300, resid Loss: 0.0505 | 0.0195
Epoch 156/300, resid Loss: 0.0549 | 0.0238
Epoch 157/300, resid Loss: 0.0561 | 0.0220
Epoch 158/300, resid Loss: 0.0553 | 0.0221
Epoch 159/300, resid Loss: 0.0548 | 0.0194
Epoch 160/300, resid Loss: 0.0521 | 0.0208
Epoch 161/300, resid Loss: 0.0535 | 0.0225
Epoch 162/300, resid Loss: 0.0509 | 0.0193
Epoch 163/300, resid Loss: 0.0503 | 0.0205
Epoch 164/300, resid Loss: 0.0502 | 0.0218
Epoch 165/300, resid Loss: 0.0501 | 0.0217
Epoch 166/300, resid Loss: 0.0504 | 0.0237
Epoch 167/300, resid Loss: 0.0510 | 0.0161
Epoch 168/300, resid Loss: 0.0499 | 0.0178
Epoch 169/300, resid Loss: 0.0489 | 0.0175
Epoch 170/300, resid Loss: 0.0490 | 0.0161
Epoch 171/300, resid Loss: 0.0496 | 0.0152
Epoch 172/300, resid Loss: 0.0503 | 0.0169
Epoch 173/300, resid Loss: 0.0508 | 0.0224
Epoch 174/300, resid Loss: 0.0488 | 0.0179
Epoch 175/300, resid Loss: 0.0488 | 0.0179
Epoch 176/300, resid Loss: 0.0484 | 0.0200
Epoch 177/300, resid Loss: 0.0482 | 0.0188
Epoch 178/300, resid Loss: 0.0480 | 0.0186
Epoch 179/300, resid Loss: 0.0478 | 0.0179
Epoch 180/300, resid Loss: 0.0476 | 0.0176
Epoch 181/300, resid Loss: 0.0475 | 0.0168
Epoch 182/300, resid Loss: 0.0474 | 0.0162
Epoch 183/300, resid Loss: 0.0475 | 0.0161
Epoch 184/300, resid Loss: 0.0476 | 0.0164
Epoch 185/300, resid Loss: 0.0474 | 0.0164
Epoch 186/300, resid Loss: 0.0473 | 0.0166
Epoch 187/300, resid Loss: 0.0471 | 0.0170
Epoch 188/300, resid Loss: 0.0470 | 0.0176
Epoch 189/300, resid Loss: 0.0471 | 0.0181
Epoch 190/300, resid Loss: 0.0472 | 0.0188
Epoch 191/300, resid Loss: 0.0474 | 0.0195
Epoch 192/300, resid Loss: 0.0478 | 0.0209
Epoch 193/300, resid Loss: 0.0488 | 0.0181
Epoch 194/300, resid Loss: 0.0487 | 0.0172
Epoch 195/300, resid Loss: 0.0478 | 0.0164
Epoch 196/300, resid Loss: 0.0475 | 0.0168
Epoch 197/300, resid Loss: 0.0480 | 0.0164
Epoch 198/300, resid Loss: 0.0482 | 0.0165
Epoch 199/300, resid Loss: 0.0475 | 0.0183
Epoch 200/300, resid Loss: 0.0469 | 0.0186
Epoch 201/300, resid Loss: 0.0468 | 0.0183
Epoch 202/300, resid Loss: 0.0466 | 0.0175
Epoch 203/300, resid Loss: 0.0464 | 0.0161
Epoch 204/300, resid Loss: 0.0465 | 0.0159
Epoch 205/300, resid Loss: 0.0464 | 0.0167
Epoch 206/300, resid Loss: 0.0462 | 0.0173
Epoch 207/300, resid Loss: 0.0461 | 0.0173
Epoch 208/300, resid Loss: 0.0460 | 0.0169
Epoch 209/300, resid Loss: 0.0460 | 0.0168
Epoch 210/300, resid Loss: 0.0459 | 0.0167
Epoch 211/300, resid Loss: 0.0458 | 0.0165
Epoch 212/300, resid Loss: 0.0458 | 0.0161
Epoch 213/300, resid Loss: 0.0458 | 0.0160
Epoch 214/300, resid Loss: 0.0457 | 0.0160
Epoch 215/300, resid Loss: 0.0457 | 0.0162
Epoch 216/300, resid Loss: 0.0456 | 0.0163
Epoch 217/300, resid Loss: 0.0455 | 0.0165
Epoch 218/300, resid Loss: 0.0455 | 0.0166
Epoch 219/300, resid Loss: 0.0455 | 0.0169
Epoch 220/300, resid Loss: 0.0455 | 0.0172
Epoch 221/300, resid Loss: 0.0455 | 0.0175
Epoch 222/300, resid Loss: 0.0456 | 0.0175
Epoch 223/300, resid Loss: 0.0455 | 0.0171
Epoch 224/300, resid Loss: 0.0454 | 0.0167
Epoch 225/300, resid Loss: 0.0453 | 0.0162
Epoch 226/300, resid Loss: 0.0453 | 0.0156
Epoch 227/300, resid Loss: 0.0456 | 0.0154
Epoch 228/300, resid Loss: 0.0458 | 0.0159
Epoch 229/300, resid Loss: 0.0457 | 0.0168
Epoch 230/300, resid Loss: 0.0453 | 0.0172
Epoch 231/300, resid Loss: 0.0453 | 0.0174
Epoch 232/300, resid Loss: 0.0455 | 0.0175
Epoch 233/300, resid Loss: 0.0454 | 0.0174
Epoch 234/300, resid Loss: 0.0451 | 0.0165
Epoch 235/300, resid Loss: 0.0450 | 0.0156
Epoch 236/300, resid Loss: 0.0451 | 0.0154
Epoch 237/300, resid Loss: 0.0452 | 0.0161
Epoch 238/300, resid Loss: 0.0450 | 0.0169
Epoch 239/300, resid Loss: 0.0448 | 0.0168
Epoch 240/300, resid Loss: 0.0448 | 0.0167
Epoch 241/300, resid Loss: 0.0448 | 0.0167
Epoch 242/300, resid Loss: 0.0448 | 0.0167
Epoch 243/300, resid Loss: 0.0446 | 0.0166
Epoch 244/300, resid Loss: 0.0446 | 0.0160
Epoch 245/300, resid Loss: 0.0446 | 0.0157
Epoch 246/300, resid Loss: 0.0446 | 0.0156
Epoch 247/300, resid Loss: 0.0446 | 0.0159
Epoch 248/300, resid Loss: 0.0445 | 0.0162
Epoch 249/300, resid Loss: 0.0444 | 0.0163
Epoch 250/300, resid Loss: 0.0444 | 0.0166
Epoch 251/300, resid Loss: 0.0445 | 0.0169
Epoch 252/300, resid Loss: 0.0445 | 0.0169
Epoch 253/300, resid Loss: 0.0444 | 0.0167
Epoch 254/300, resid Loss: 0.0443 | 0.0160
Epoch 255/300, resid Loss: 0.0443 | 0.0155
Epoch 256/300, resid Loss: 0.0444 | 0.0155
Epoch 257/300, resid Loss: 0.0444 | 0.0160
Epoch 258/300, resid Loss: 0.0442 | 0.0162
Epoch 259/300, resid Loss: 0.0441 | 0.0162
Epoch 260/300, resid Loss: 0.0442 | 0.0165
Epoch 261/300, resid Loss: 0.0442 | 0.0166
Epoch 262/300, resid Loss: 0.0441 | 0.0167
Epoch 263/300, resid Loss: 0.0440 | 0.0161
Epoch 264/300, resid Loss: 0.0440 | 0.0156
Epoch 265/300, resid Loss: 0.0440 | 0.0155
Epoch 266/300, resid Loss: 0.0440 | 0.0159
Epoch 267/300, resid Loss: 0.0439 | 0.0161
Epoch 268/300, resid Loss: 0.0439 | 0.0160
Epoch 269/300, resid Loss: 0.0439 | 0.0162
Epoch 270/300, resid Loss: 0.0439 | 0.0165
Epoch 271/300, resid Loss: 0.0438 | 0.0166
Epoch 272/300, resid Loss: 0.0438 | 0.0161
Epoch 273/300, resid Loss: 0.0437 | 0.0157
Epoch 274/300, resid Loss: 0.0437 | 0.0156
Epoch 275/300, resid Loss: 0.0438 | 0.0157
Epoch 276/300, resid Loss: 0.0438 | 0.0156
Epoch 277/300, resid Loss: 0.0437 | 0.0158
Epoch 278/300, resid Loss: 0.0436 | 0.0161
Epoch 279/300, resid Loss: 0.0436 | 0.0164
Epoch 280/300, resid Loss: 0.0436 | 0.0163
Epoch 281/300, resid Loss: 0.0436 | 0.0160
Epoch 282/300, resid Loss: 0.0435 | 0.0158
Epoch 283/300, resid Loss: 0.0435 | 0.0156
Epoch 284/300, resid Loss: 0.0435 | 0.0156
Epoch 285/300, resid Loss: 0.0435 | 0.0156
Epoch 286/300, resid Loss: 0.0434 | 0.0158
Epoch 287/300, resid Loss: 0.0434 | 0.0160
Epoch 288/300, resid Loss: 0.0434 | 0.0162
Epoch 289/300, resid Loss: 0.0434 | 0.0161
Epoch 290/300, resid Loss: 0.0433 | 0.0159
Epoch 291/300, resid Loss: 0.0433 | 0.0158
Epoch 292/300, resid Loss: 0.0433 | 0.0156
Epoch 293/300, resid Loss: 0.0433 | 0.0155
Epoch 294/300, resid Loss: 0.0433 | 0.0155
Epoch 295/300, resid Loss: 0.0433 | 0.0158
Epoch 296/300, resid Loss: 0.0432 | 0.0160
Epoch 297/300, resid Loss: 0.0432 | 0.0160
Epoch 298/300, resid Loss: 0.0432 | 0.0160
Epoch 299/300, resid Loss: 0.0432 | 0.0159
Epoch 300/300, resid Loss: 0.0431 | 0.0157
Runtime (seconds): 1648.5760436058044
0.000643830278682234
[214.01102]
[0.064358]
[-3.1693664]
[3.712058]
[0.7906859]
[12.240026]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 93.72648338321596
RMSE: 9.681243896484375
MAE: 9.681243896484375
R-squared: nan
[227.64876]
