ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-03 09:03:34,496][0m A new study created in memory with name: no-name-e8f54c3f-fc56-48ca-acef-82bc6848cb1c[0m
[32m[I 2025-02-03 09:04:13,500][0m Trial 0 finished with value: 0.0586339191265917 and parameters: {'observation_period_num': 183, 'train_rates': 0.8218754321087678, 'learning_rate': 0.0005703772985207766, 'batch_size': 139, 'step_size': 7, 'gamma': 0.9281975239014322}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:04:56,223][0m Trial 1 finished with value: 0.08352900296449661 and parameters: {'observation_period_num': 162, 'train_rates': 0.9821693587396663, 'learning_rate': 5.33382323498346e-05, 'batch_size': 145, 'step_size': 14, 'gamma': 0.9656569224532825}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:05:16,364][0m Trial 2 finished with value: 0.14328821192788815 and parameters: {'observation_period_num': 206, 'train_rates': 0.7362932574520197, 'learning_rate': 7.234226126687546e-05, 'batch_size': 254, 'step_size': 6, 'gamma': 0.7725462454261398}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:05:53,550][0m Trial 3 finished with value: 0.30114761386474553 and parameters: {'observation_period_num': 33, 'train_rates': 0.6716671110837866, 'learning_rate': 1.6902055476170784e-06, 'batch_size': 133, 'step_size': 15, 'gamma': 0.9449902616440269}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:06:21,737][0m Trial 4 finished with value: 0.32074285542208053 and parameters: {'observation_period_num': 159, 'train_rates': 0.6616315973435252, 'learning_rate': 3.1311452521619982e-06, 'batch_size': 183, 'step_size': 9, 'gamma': 0.9258315386752208}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:06:43,976][0m Trial 5 finished with value: 0.08635997723304588 and parameters: {'observation_period_num': 215, 'train_rates': 0.7606388461354897, 'learning_rate': 0.00017792909190681163, 'batch_size': 248, 'step_size': 10, 'gamma': 0.8188460006409295}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:08:34,697][0m Trial 6 finished with value: 0.09587612582417475 and parameters: {'observation_period_num': 20, 'train_rates': 0.6472742247194827, 'learning_rate': 3.771221505865275e-06, 'batch_size': 42, 'step_size': 11, 'gamma': 0.8892822356771062}. Best is trial 0 with value: 0.0586339191265917.[0m
[32m[I 2025-02-03 09:09:18,655][0m Trial 7 finished with value: 0.05329942626024162 and parameters: {'observation_period_num': 106, 'train_rates': 0.817141517669376, 'learning_rate': 0.0002882276952111821, 'batch_size': 126, 'step_size': 11, 'gamma': 0.930341989202981}. Best is trial 7 with value: 0.05329942626024162.[0m
[32m[I 2025-02-03 09:10:38,860][0m Trial 8 finished with value: 0.33796504992418563 and parameters: {'observation_period_num': 175, 'train_rates': 0.8502117287545152, 'learning_rate': 1.4501667180233087e-06, 'batch_size': 66, 'step_size': 15, 'gamma': 0.8837701180035502}. Best is trial 7 with value: 0.05329942626024162.[0m
[32m[I 2025-02-03 09:11:33,006][0m Trial 9 finished with value: 0.05150676313578413 and parameters: {'observation_period_num': 47, 'train_rates': 0.6323639965543476, 'learning_rate': 0.00010127121504016375, 'batch_size': 84, 'step_size': 11, 'gamma': 0.8360426867780075}. Best is trial 9 with value: 0.05150676313578413.[0m
Early stopping at epoch 67
[32m[I 2025-02-03 09:12:10,544][0m Trial 10 finished with value: 0.43879998855768365 and parameters: {'observation_period_num': 80, 'train_rates': 0.6071149477430042, 'learning_rate': 1.2337221079038322e-05, 'batch_size': 81, 'step_size': 1, 'gamma': 0.8229037445306453}. Best is trial 9 with value: 0.05150676313578413.[0m
[32m[I 2025-02-03 09:13:05,616][0m Trial 11 finished with value: 0.05577421740295496 and parameters: {'observation_period_num': 93, 'train_rates': 0.8955773646330846, 'learning_rate': 0.0006716450171170531, 'batch_size': 103, 'step_size': 11, 'gamma': 0.8414409711387498}. Best is trial 9 with value: 0.05150676313578413.[0m
[32m[I 2025-02-03 09:16:53,788][0m Trial 12 finished with value: 0.07182301963907647 and parameters: {'observation_period_num': 88, 'train_rates': 0.7392252268998266, 'learning_rate': 0.00025112305990490733, 'batch_size': 21, 'step_size': 12, 'gamma': 0.7623544348472551}. Best is trial 9 with value: 0.05150676313578413.[0m
[32m[I 2025-02-03 09:17:26,401][0m Trial 13 finished with value: 0.04822660924312268 and parameters: {'observation_period_num': 56, 'train_rates': 0.909778980180872, 'learning_rate': 0.00015620620845862844, 'batch_size': 189, 'step_size': 5, 'gamma': 0.8602135511675679}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:17:59,955][0m Trial 14 finished with value: 0.10677982866764069 and parameters: {'observation_period_num': 46, 'train_rates': 0.9449301501713587, 'learning_rate': 2.1681989787975906e-05, 'batch_size': 192, 'step_size': 4, 'gamma': 0.8477282828762114}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:18:29,636][0m Trial 15 finished with value: 0.06666245825566057 and parameters: {'observation_period_num': 54, 'train_rates': 0.8867429082866354, 'learning_rate': 8.599205535987845e-05, 'batch_size': 213, 'step_size': 5, 'gamma': 0.7926129770294055}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:19:07,447][0m Trial 16 finished with value: 0.07569894590674722 and parameters: {'observation_period_num': 9, 'train_rates': 0.9088909494366276, 'learning_rate': 2.336437345330826e-05, 'batch_size': 166, 'step_size': 3, 'gamma': 0.8658706127952003}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:19:32,414][0m Trial 17 finished with value: 0.08296793764610698 and parameters: {'observation_period_num': 130, 'train_rates': 0.7637888184511517, 'learning_rate': 0.00012233942498392352, 'batch_size': 220, 'step_size': 8, 'gamma': 0.8031972317175664}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:20:20,387][0m Trial 18 finished with value: 0.0878068288189492 and parameters: {'observation_period_num': 59, 'train_rates': 0.6904015854707629, 'learning_rate': 4.337020503238943e-05, 'batch_size': 102, 'step_size': 2, 'gamma': 0.9061257555738519}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:21:39,459][0m Trial 19 finished with value: 0.06730086008507935 and parameters: {'observation_period_num': 136, 'train_rates': 0.6027713067035358, 'learning_rate': 0.0009645458480449418, 'batch_size': 53, 'step_size': 13, 'gamma': 0.8540688609911287}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:22:37,148][0m Trial 20 finished with value: 0.2665923237800598 and parameters: {'observation_period_num': 247, 'train_rates': 0.9891252682538489, 'learning_rate': 1.022291935217571e-05, 'batch_size': 100, 'step_size': 7, 'gamma': 0.8811882052662154}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:23:21,348][0m Trial 21 finished with value: 0.048459615342635445 and parameters: {'observation_period_num': 107, 'train_rates': 0.8180320660261894, 'learning_rate': 0.00032356047361287133, 'batch_size': 124, 'step_size': 10, 'gamma': 0.9806209448477745}. Best is trial 13 with value: 0.04822660924312268.[0m
[32m[I 2025-02-03 09:23:58,259][0m Trial 22 finished with value: 0.03674767935854457 and parameters: {'observation_period_num': 67, 'train_rates': 0.8653125838539634, 'learning_rate': 0.0003896857198555795, 'batch_size': 165, 'step_size': 9, 'gamma': 0.9810614792756207}. Best is trial 22 with value: 0.03674767935854457.[0m
[32m[I 2025-02-03 09:24:33,366][0m Trial 23 finished with value: 0.05048580606578279 and parameters: {'observation_period_num': 111, 'train_rates': 0.8602624446022441, 'learning_rate': 0.00028498188438162687, 'batch_size': 166, 'step_size': 9, 'gamma': 0.9888881414077101}. Best is trial 22 with value: 0.03674767935854457.[0m
[32m[I 2025-02-03 09:25:13,813][0m Trial 24 finished with value: 0.04349421709775925 and parameters: {'observation_period_num': 64, 'train_rates': 0.9435527007434015, 'learning_rate': 0.0004524479221800052, 'batch_size': 160, 'step_size': 5, 'gamma': 0.9858332572602728}. Best is trial 22 with value: 0.03674767935854457.[0m
[32m[I 2025-02-03 09:25:52,081][0m Trial 25 finished with value: 0.04408457336704368 and parameters: {'observation_period_num': 68, 'train_rates': 0.9364048281211352, 'learning_rate': 0.0005367967838418284, 'batch_size': 162, 'step_size': 4, 'gamma': 0.9550633707176297}. Best is trial 22 with value: 0.03674767935854457.[0m
[32m[I 2025-02-03 09:26:32,192][0m Trial 26 finished with value: 0.04187955246989135 and parameters: {'observation_period_num': 68, 'train_rates': 0.9455745885612978, 'learning_rate': 0.000539824488042596, 'batch_size': 154, 'step_size': 3, 'gamma': 0.9564385110255278}. Best is trial 22 with value: 0.03674767935854457.[0m
[32m[I 2025-02-03 09:27:12,127][0m Trial 27 finished with value: 0.03522084653377533 and parameters: {'observation_period_num': 27, 'train_rates': 0.9535194279424639, 'learning_rate': 0.0008495570730561163, 'batch_size': 157, 'step_size': 1, 'gamma': 0.9699162906046136}. Best is trial 27 with value: 0.03522084653377533.[0m
[32m[I 2025-02-03 09:27:39,694][0m Trial 28 finished with value: 0.03698085051932209 and parameters: {'observation_period_num': 29, 'train_rates': 0.8573861289699212, 'learning_rate': 0.0008653924770329286, 'batch_size': 209, 'step_size': 1, 'gamma': 0.9615645895278926}. Best is trial 27 with value: 0.03522084653377533.[0m
[32m[I 2025-02-03 09:28:07,643][0m Trial 29 finished with value: 0.04161565780224767 and parameters: {'observation_period_num': 25, 'train_rates': 0.8529592425957456, 'learning_rate': 0.0009470548511566315, 'batch_size': 217, 'step_size': 1, 'gamma': 0.9120383378029324}. Best is trial 27 with value: 0.03522084653377533.[0m
[32m[I 2025-02-03 09:28:35,101][0m Trial 30 finished with value: 0.02882187705075323 and parameters: {'observation_period_num': 6, 'train_rates': 0.8741768762356196, 'learning_rate': 0.0009840876918124912, 'batch_size': 232, 'step_size': 2, 'gamma': 0.9717662628121764}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:29:02,923][0m Trial 31 finished with value: 0.030793168451837313 and parameters: {'observation_period_num': 7, 'train_rates': 0.8737811092935419, 'learning_rate': 0.0009666402779694803, 'batch_size': 232, 'step_size': 2, 'gamma': 0.9662992674336182}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:29:26,940][0m Trial 32 finished with value: 0.03566691553744885 and parameters: {'observation_period_num': 5, 'train_rates': 0.7940762257755714, 'learning_rate': 0.000459329026585581, 'batch_size': 245, 'step_size': 2, 'gamma': 0.9776434383721583}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:29:51,638][0m Trial 33 finished with value: 0.03602567757114908 and parameters: {'observation_period_num': 8, 'train_rates': 0.7835441301453233, 'learning_rate': 0.0006098853997211866, 'batch_size': 237, 'step_size': 2, 'gamma': 0.9458233102252289}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:30:16,049][0m Trial 34 finished with value: 0.04312909826944778 and parameters: {'observation_period_num': 5, 'train_rates': 0.7869748145962342, 'learning_rate': 0.00020308521176482511, 'batch_size': 235, 'step_size': 2, 'gamma': 0.9736125425747921}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:30:44,197][0m Trial 35 finished with value: 0.04185963422060013 and parameters: {'observation_period_num': 39, 'train_rates': 0.9664663358451777, 'learning_rate': 0.0007134852233468847, 'batch_size': 254, 'step_size': 3, 'gamma': 0.9403924990851198}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:31:11,025][0m Trial 36 finished with value: 0.05425386617904768 and parameters: {'observation_period_num': 20, 'train_rates': 0.8326285249107981, 'learning_rate': 0.0003717202470120555, 'batch_size': 233, 'step_size': 2, 'gamma': 0.9696694738351066}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:31:41,711][0m Trial 37 finished with value: 0.03082158961998565 and parameters: {'observation_period_num': 38, 'train_rates': 0.8849919859493909, 'learning_rate': 0.0009432681909912204, 'batch_size': 202, 'step_size': 4, 'gamma': 0.9227885460790146}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:32:12,261][0m Trial 38 finished with value: 0.03435458264059641 and parameters: {'observation_period_num': 35, 'train_rates': 0.8796073659756646, 'learning_rate': 0.0007442532973882396, 'batch_size': 200, 'step_size': 4, 'gamma': 0.9195129097030962}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:32:42,351][0m Trial 39 finished with value: 0.041467194719349634 and parameters: {'observation_period_num': 38, 'train_rates': 0.8836081595910816, 'learning_rate': 0.0001956669763071439, 'batch_size': 205, 'step_size': 6, 'gamma': 0.9162482712774708}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:33:14,746][0m Trial 40 finished with value: 0.02926577115033427 and parameters: {'observation_period_num': 18, 'train_rates': 0.9204811517845469, 'learning_rate': 0.0009881665572323201, 'batch_size': 200, 'step_size': 4, 'gamma': 0.925542490874185}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:33:49,069][0m Trial 41 finished with value: 0.034125233201551634 and parameters: {'observation_period_num': 19, 'train_rates': 0.9185964201852785, 'learning_rate': 0.0006632076053624464, 'batch_size': 181, 'step_size': 4, 'gamma': 0.9279631024044394}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:34:23,167][0m Trial 42 finished with value: 0.03128419112513274 and parameters: {'observation_period_num': 18, 'train_rates': 0.910462751562309, 'learning_rate': 0.0005324122837323161, 'batch_size': 179, 'step_size': 6, 'gamma': 0.9038248513464393}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:34:50,790][0m Trial 43 finished with value: 0.04256367627927597 and parameters: {'observation_period_num': 16, 'train_rates': 0.9201653473976726, 'learning_rate': 0.0009353882033269513, 'batch_size': 225, 'step_size': 6, 'gamma': 0.9372473137097541}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:35:23,395][0m Trial 44 finished with value: 0.037780921093019774 and parameters: {'observation_period_num': 40, 'train_rates': 0.8349757607664898, 'learning_rate': 0.0004813487186754907, 'batch_size': 178, 'step_size': 3, 'gamma': 0.8984726466698421}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:35:57,873][0m Trial 45 finished with value: 0.043073881417512894 and parameters: {'observation_period_num': 14, 'train_rates': 0.9684977475108735, 'learning_rate': 0.0003402896864966732, 'batch_size': 196, 'step_size': 6, 'gamma': 0.9038974001174904}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:36:25,849][0m Trial 46 finished with value: 0.44455709887298556 and parameters: {'observation_period_num': 47, 'train_rates': 0.901422814779801, 'learning_rate': 2.2501809540192728e-06, 'batch_size': 228, 'step_size': 7, 'gamma': 0.8909281656636169}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:37:10,250][0m Trial 47 finished with value: 0.15439004437731846 and parameters: {'observation_period_num': 27, 'train_rates': 0.9237236235231698, 'learning_rate': 5.245887533241539e-06, 'batch_size': 138, 'step_size': 5, 'gamma': 0.9468292529764071}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:37:33,876][0m Trial 48 finished with value: 0.06265099674111622 and parameters: {'observation_period_num': 201, 'train_rates': 0.8768935895044676, 'learning_rate': 0.0005791848253151112, 'batch_size': 244, 'step_size': 4, 'gamma': 0.8762815329716109}. Best is trial 30 with value: 0.02882187705075323.[0m
[32m[I 2025-02-03 09:38:09,266][0m Trial 49 finished with value: 0.046750543601050666 and parameters: {'observation_period_num': 16, 'train_rates': 0.9001655547659458, 'learning_rate': 0.00012971938899297182, 'batch_size': 176, 'step_size': 3, 'gamma': 0.9327912347281584}. Best is trial 30 with value: 0.02882187705075323.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-03 09:38:09,277][0m A new study created in memory with name: no-name-1685804d-3cae-4e8a-9a46-5ce6c701529a[0m
[32m[I 2025-02-03 09:39:05,900][0m Trial 0 finished with value: 0.4963932221608538 and parameters: {'observation_period_num': 146, 'train_rates': 0.7423482461051828, 'learning_rate': 3.063123971120171e-06, 'batch_size': 90, 'step_size': 7, 'gamma': 0.7580379607440643}. Best is trial 0 with value: 0.4963932221608538.[0m
[32m[I 2025-02-03 09:39:56,349][0m Trial 1 finished with value: 0.08402178378185768 and parameters: {'observation_period_num': 60, 'train_rates': 0.8058995377870948, 'learning_rate': 1.158561846669136e-05, 'batch_size': 110, 'step_size': 14, 'gamma': 0.9233941011640007}. Best is trial 1 with value: 0.08402178378185768.[0m
[32m[I 2025-02-03 09:40:22,790][0m Trial 2 finished with value: 0.053914811462163925 and parameters: {'observation_period_num': 53, 'train_rates': 0.9228335978789989, 'learning_rate': 0.00014314558092032995, 'batch_size': 248, 'step_size': 4, 'gamma': 0.9691044077932631}. Best is trial 2 with value: 0.053914811462163925.[0m
[32m[I 2025-02-03 09:40:54,824][0m Trial 3 finished with value: 0.0881724206944554 and parameters: {'observation_period_num': 158, 'train_rates': 0.7413112350759934, 'learning_rate': 4.776425353791e-05, 'batch_size': 170, 'step_size': 13, 'gamma': 0.9577758239082174}. Best is trial 2 with value: 0.053914811462163925.[0m
[32m[I 2025-02-03 09:41:20,838][0m Trial 4 finished with value: 0.5902790151793381 and parameters: {'observation_period_num': 102, 'train_rates': 0.8474208949098866, 'learning_rate': 1.024140248770308e-06, 'batch_size': 237, 'step_size': 13, 'gamma': 0.8355824480993138}. Best is trial 2 with value: 0.053914811462163925.[0m
[32m[I 2025-02-03 09:44:15,151][0m Trial 5 finished with value: 0.26131719462974096 and parameters: {'observation_period_num': 131, 'train_rates': 0.7692514605089733, 'learning_rate': 1.0313814769998207e-06, 'batch_size': 28, 'step_size': 12, 'gamma': 0.9003222282788728}. Best is trial 2 with value: 0.053914811462163925.[0m
Early stopping at epoch 80
[32m[I 2025-02-03 09:44:37,913][0m Trial 6 finished with value: 1.1640092134475708 and parameters: {'observation_period_num': 28, 'train_rates': 0.9657545009805668, 'learning_rate': 2.6198813711572305e-06, 'batch_size': 256, 'step_size': 2, 'gamma': 0.7696092949454388}. Best is trial 2 with value: 0.053914811462163925.[0m
[32m[I 2025-02-03 09:45:35,362][0m Trial 7 finished with value: 0.03744357989894019 and parameters: {'observation_period_num': 54, 'train_rates': 0.9224094957476914, 'learning_rate': 0.0008904475090062594, 'batch_size': 103, 'step_size': 3, 'gamma': 0.8995451662801717}. Best is trial 7 with value: 0.03744357989894019.[0m
[32m[I 2025-02-03 09:47:38,797][0m Trial 8 finished with value: 0.29272723530667466 and parameters: {'observation_period_num': 189, 'train_rates': 0.6240550954376729, 'learning_rate': 3.3508878141692063e-06, 'batch_size': 34, 'step_size': 14, 'gamma': 0.8819651621836925}. Best is trial 7 with value: 0.03744357989894019.[0m
[32m[I 2025-02-03 09:48:04,451][0m Trial 9 finished with value: 0.21156752109527588 and parameters: {'observation_period_num': 223, 'train_rates': 0.9226735679539735, 'learning_rate': 1.335084035912269e-05, 'batch_size': 246, 'step_size': 8, 'gamma': 0.8455937041435854}. Best is trial 7 with value: 0.03744357989894019.[0m
Early stopping at epoch 69
[32m[I 2025-02-03 09:48:34,441][0m Trial 10 finished with value: 0.06759611517190933 and parameters: {'observation_period_num': 14, 'train_rates': 0.9852991827640097, 'learning_rate': 0.0008609309736734498, 'batch_size': 158, 'step_size': 1, 'gamma': 0.8278343352364683}. Best is trial 7 with value: 0.03744357989894019.[0m
[32m[I 2025-02-03 09:49:06,705][0m Trial 11 finished with value: 0.03462315834502676 and parameters: {'observation_period_num': 75, 'train_rates': 0.8837651056168611, 'learning_rate': 0.0007780911817275074, 'batch_size': 190, 'step_size': 4, 'gamma': 0.9608720062097857}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:49:36,884][0m Trial 12 finished with value: 0.03848047420026714 and parameters: {'observation_period_num': 89, 'train_rates': 0.8759058240152321, 'learning_rate': 0.0008418413998968456, 'batch_size': 193, 'step_size': 5, 'gamma': 0.9389821273143728}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:50:45,932][0m Trial 13 finished with value: 0.0673825146869207 and parameters: {'observation_period_num': 86, 'train_rates': 0.8641035349964119, 'learning_rate': 0.00028478630234519964, 'batch_size': 82, 'step_size': 4, 'gamma': 0.9897884746141207}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:51:18,245][0m Trial 14 finished with value: 0.042118722137725245 and parameters: {'observation_period_num': 51, 'train_rates': 0.9264239466658091, 'learning_rate': 0.0002846735990073142, 'batch_size': 197, 'step_size': 6, 'gamma': 0.9125837377637409}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:52:03,242][0m Trial 15 finished with value: 0.04649488808490215 and parameters: {'observation_period_num': 5, 'train_rates': 0.8110283358482451, 'learning_rate': 5.9158128078567166e-05, 'batch_size': 131, 'step_size': 10, 'gamma': 0.8669649520262706}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:53:18,389][0m Trial 16 finished with value: 0.08827454572021075 and parameters: {'observation_period_num': 107, 'train_rates': 0.6453756381721422, 'learning_rate': 0.000358850128427046, 'batch_size': 61, 'step_size': 3, 'gamma': 0.9414393382600568}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:54:04,130][0m Trial 17 finished with value: 0.046027424309390164 and parameters: {'observation_period_num': 74, 'train_rates': 0.8969208776310984, 'learning_rate': 0.0001287736927871534, 'batch_size': 134, 'step_size': 1, 'gamma': 0.9829330145764411}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:54:32,701][0m Trial 18 finished with value: 0.037502849058851664 and parameters: {'observation_period_num': 37, 'train_rates': 0.8402751548664417, 'learning_rate': 0.00053278042379794, 'batch_size': 209, 'step_size': 9, 'gamma': 0.8019996635044595}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:55:10,655][0m Trial 19 finished with value: 0.079523004591465 and parameters: {'observation_period_num': 246, 'train_rates': 0.9541444342805516, 'learning_rate': 0.00013504694262433405, 'batch_size': 157, 'step_size': 6, 'gamma': 0.8886966651696447}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:56:03,957][0m Trial 20 finished with value: 0.1136995407949834 and parameters: {'observation_period_num': 111, 'train_rates': 0.9036779667355617, 'learning_rate': 2.103292020051991e-05, 'batch_size': 111, 'step_size': 3, 'gamma': 0.9321780866158302}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:56:32,427][0m Trial 21 finished with value: 0.035567109720901616 and parameters: {'observation_period_num': 36, 'train_rates': 0.8299916380287703, 'learning_rate': 0.0005287688819593242, 'batch_size': 213, 'step_size': 10, 'gamma': 0.7957135415194384}. Best is trial 11 with value: 0.03462315834502676.[0m
[32m[I 2025-02-03 09:57:05,408][0m Trial 22 finished with value: 0.031640572705783576 and parameters: {'observation_period_num': 33, 'train_rates': 0.824282767891123, 'learning_rate': 0.0009792570248455216, 'batch_size': 179, 'step_size': 11, 'gamma': 0.7948027125184078}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 09:57:30,434][0m Trial 23 finished with value: 0.03711489538381509 and parameters: {'observation_period_num': 27, 'train_rates': 0.7153742102079147, 'learning_rate': 0.0004753815475666139, 'batch_size': 223, 'step_size': 11, 'gamma': 0.7875850671851793}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 09:58:03,561][0m Trial 24 finished with value: 0.04160476028622974 and parameters: {'observation_period_num': 72, 'train_rates': 0.8253160230241595, 'learning_rate': 0.00021414968428231615, 'batch_size': 178, 'step_size': 10, 'gamma': 0.8131818755173019}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 09:58:30,075][0m Trial 25 finished with value: 0.043950068283509586 and parameters: {'observation_period_num': 37, 'train_rates': 0.7901154603512536, 'learning_rate': 0.0005695916065418978, 'batch_size': 218, 'step_size': 8, 'gamma': 0.855522408623377}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 09:59:05,527][0m Trial 26 finished with value: 0.05197719128235527 and parameters: {'observation_period_num': 5, 'train_rates': 0.8754132105428669, 'learning_rate': 7.83279820314252e-05, 'batch_size': 183, 'step_size': 11, 'gamma': 0.782974955328696}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 09:59:41,696][0m Trial 27 finished with value: 0.03987871463741026 and parameters: {'observation_period_num': 73, 'train_rates': 0.7848916076612159, 'learning_rate': 0.0009959951296926077, 'batch_size': 153, 'step_size': 15, 'gamma': 0.8178970798505738}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:00:07,216][0m Trial 28 finished with value: 0.04414563712108644 and parameters: {'observation_period_num': 25, 'train_rates': 0.671779665634806, 'learning_rate': 0.00020706093637782693, 'batch_size': 200, 'step_size': 9, 'gamma': 0.752158485417434}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:00:31,488][0m Trial 29 finished with value: 0.05546270291882939 and parameters: {'observation_period_num': 120, 'train_rates': 0.7508260752584424, 'learning_rate': 0.0004873829132039431, 'batch_size': 214, 'step_size': 7, 'gamma': 0.7963321398068911}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:00:56,486][0m Trial 30 finished with value: 0.10112633871594223 and parameters: {'observation_period_num': 133, 'train_rates': 0.7151867310143807, 'learning_rate': 9.900724484381594e-05, 'batch_size': 223, 'step_size': 11, 'gamma': 0.770723052362972}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:01:23,264][0m Trial 31 finished with value: 0.039136151661349754 and parameters: {'observation_period_num': 41, 'train_rates': 0.8372840984990194, 'learning_rate': 0.00046312428228223023, 'batch_size': 230, 'step_size': 11, 'gamma': 0.7882984956589613}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:01:53,077][0m Trial 32 finished with value: 0.03424810197844497 and parameters: {'observation_period_num': 21, 'train_rates': 0.7143073399737366, 'learning_rate': 0.0006783029872709615, 'batch_size': 187, 'step_size': 12, 'gamma': 0.7651473797299045}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:02:20,548][0m Trial 33 finished with value: 0.04496183781678566 and parameters: {'observation_period_num': 63, 'train_rates': 0.6869807790385525, 'learning_rate': 0.0006622782129460507, 'batch_size': 183, 'step_size': 12, 'gamma': 0.7664946670026151}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:02:55,858][0m Trial 34 finished with value: 0.04136276902110223 and parameters: {'observation_period_num': 42, 'train_rates': 0.8168518355449463, 'learning_rate': 0.00021806099956522896, 'batch_size': 166, 'step_size': 9, 'gamma': 0.75204021893214}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:03:33,987][0m Trial 35 finished with value: 0.058429665199946614 and parameters: {'observation_period_num': 163, 'train_rates': 0.7706568369689517, 'learning_rate': 0.0003523391692604874, 'batch_size': 143, 'step_size': 13, 'gamma': 0.8063489390101695}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:04:04,471][0m Trial 36 finished with value: 0.05548219941556454 and parameters: {'observation_period_num': 18, 'train_rates': 0.8595547707018943, 'learning_rate': 3.761256807204875e-05, 'batch_size': 206, 'step_size': 12, 'gamma': 0.8249814330190187}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:04:36,386][0m Trial 37 finished with value: 0.09037339566337982 and parameters: {'observation_period_num': 86, 'train_rates': 0.7400311193956917, 'learning_rate': 0.00035763565317414463, 'batch_size': 172, 'step_size': 10, 'gamma': 0.9612634051098798}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:05:08,069][0m Trial 38 finished with value: 0.037195610447108535 and parameters: {'observation_period_num': 59, 'train_rates': 0.8016205761636752, 'learning_rate': 0.0006875578313628804, 'batch_size': 191, 'step_size': 14, 'gamma': 0.7806576451696388}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:05:32,376][0m Trial 39 finished with value: 0.3700747608611014 and parameters: {'observation_period_num': 51, 'train_rates': 0.7663251495748261, 'learning_rate': 5.64174898396867e-06, 'batch_size': 243, 'step_size': 8, 'gamma': 0.8431299653625411}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:05:53,819][0m Trial 40 finished with value: 0.045505713324608475 and parameters: {'observation_period_num': 16, 'train_rates': 0.6048335509766141, 'learning_rate': 0.0009869472545741532, 'batch_size': 234, 'step_size': 13, 'gamma': 0.7664866915789461}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:06:19,111][0m Trial 41 finished with value: 0.03905898793034743 and parameters: {'observation_period_num': 29, 'train_rates': 0.7073401372497333, 'learning_rate': 0.000460281112142649, 'batch_size': 218, 'step_size': 11, 'gamma': 0.7964921633278066}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:06:44,339][0m Trial 42 finished with value: 0.03695914491292942 and parameters: {'observation_period_num': 26, 'train_rates': 0.7165027407994907, 'learning_rate': 0.0005398929611550799, 'batch_size': 229, 'step_size': 12, 'gamma': 0.7780144556939531}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:07:14,078][0m Trial 43 finished with value: 0.04020240836036511 and parameters: {'observation_period_num': 43, 'train_rates': 0.7319101221552888, 'learning_rate': 0.0007075198759711826, 'batch_size': 190, 'step_size': 12, 'gamma': 0.7753509774021573}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:07:39,938][0m Trial 44 finished with value: 0.048847407316990034 and parameters: {'observation_period_num': 19, 'train_rates': 0.6684276257351423, 'learning_rate': 0.00017932626781161964, 'batch_size': 202, 'step_size': 13, 'gamma': 0.7621065209132895}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:08:04,147][0m Trial 45 finished with value: 0.03892283910163888 and parameters: {'observation_period_num': 63, 'train_rates': 0.8463495087376222, 'learning_rate': 0.00028588936326370205, 'batch_size': 254, 'step_size': 14, 'gamma': 0.8078587339723452}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:08:40,248][0m Trial 46 finished with value: 0.0339299239218235 and parameters: {'observation_period_num': 31, 'train_rates': 0.8975828496135734, 'learning_rate': 0.0007197011883483586, 'batch_size': 168, 'step_size': 10, 'gamma': 0.7904532937247533}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:09:17,710][0m Trial 47 finished with value: 0.04125258137364625 and parameters: {'observation_period_num': 47, 'train_rates': 0.8896443678790575, 'learning_rate': 0.000793523019956075, 'batch_size': 166, 'step_size': 10, 'gamma': 0.8653681985110391}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:10:08,248][0m Trial 48 finished with value: 0.05136618891983857 and parameters: {'observation_period_num': 97, 'train_rates': 0.9433037743863019, 'learning_rate': 0.00035597029464621346, 'batch_size': 122, 'step_size': 7, 'gamma': 0.8281810706304782}. Best is trial 22 with value: 0.031640572705783576.[0m
[32m[I 2025-02-03 10:10:42,396][0m Trial 49 finished with value: 0.043122175631338154 and parameters: {'observation_period_num': 5, 'train_rates': 0.8821917346153112, 'learning_rate': 0.0007203678754696809, 'batch_size': 177, 'step_size': 9, 'gamma': 0.8816849381504176}. Best is trial 22 with value: 0.031640572705783576.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-03 10:10:42,407][0m A new study created in memory with name: no-name-8b120190-f0ce-4ff5-aecf-47367e696d04[0m
[32m[I 2025-02-03 10:14:04,964][0m Trial 0 finished with value: 0.07260399022866992 and parameters: {'observation_period_num': 108, 'train_rates': 0.8073015735777203, 'learning_rate': 0.0002601628928637455, 'batch_size': 25, 'step_size': 9, 'gamma': 0.9310683716377086}. Best is trial 0 with value: 0.07260399022866992.[0m
[32m[I 2025-02-03 10:18:41,514][0m Trial 1 finished with value: 0.1407112863708715 and parameters: {'observation_period_num': 223, 'train_rates': 0.8884610663633821, 'learning_rate': 4.6718935115131325e-06, 'batch_size': 19, 'step_size': 4, 'gamma': 0.9321297919149714}. Best is trial 0 with value: 0.07260399022866992.[0m
[32m[I 2025-02-03 10:20:34,129][0m Trial 2 finished with value: 0.1063523293401848 and parameters: {'observation_period_num': 90, 'train_rates': 0.6974736319518946, 'learning_rate': 1.7593796177090496e-05, 'batch_size': 42, 'step_size': 7, 'gamma': 0.8480521697746434}. Best is trial 0 with value: 0.07260399022866992.[0m
[32m[I 2025-02-03 10:21:09,470][0m Trial 3 finished with value: 0.41251084794248394 and parameters: {'observation_period_num': 107, 'train_rates': 0.6333690189149987, 'learning_rate': 1.5628210049906976e-06, 'batch_size': 132, 'step_size': 14, 'gamma': 0.9476514946549018}. Best is trial 0 with value: 0.07260399022866992.[0m
[32m[I 2025-02-03 10:23:11,498][0m Trial 4 finished with value: 0.0692195574203545 and parameters: {'observation_period_num': 177, 'train_rates': 0.8278155408766784, 'learning_rate': 0.0008658934881137087, 'batch_size': 42, 'step_size': 15, 'gamma': 0.9207775285768252}. Best is trial 4 with value: 0.0692195574203545.[0m
[32m[I 2025-02-03 10:23:54,629][0m Trial 5 finished with value: 0.23246858290091219 and parameters: {'observation_period_num': 248, 'train_rates': 0.7261098932573427, 'learning_rate': 2.9488224316992577e-06, 'batch_size': 115, 'step_size': 11, 'gamma': 0.9883136768113729}. Best is trial 4 with value: 0.0692195574203545.[0m
[32m[I 2025-02-03 10:26:41,804][0m Trial 6 finished with value: 0.07572083042587265 and parameters: {'observation_period_num': 169, 'train_rates': 0.9577176422992622, 'learning_rate': 2.298601312021259e-05, 'batch_size': 34, 'step_size': 8, 'gamma': 0.8396386123267214}. Best is trial 4 with value: 0.0692195574203545.[0m
[32m[I 2025-02-03 10:27:11,309][0m Trial 7 finished with value: 0.11476979836439476 and parameters: {'observation_period_num': 228, 'train_rates': 0.7139013000405162, 'learning_rate': 0.00012692310545018665, 'batch_size': 176, 'step_size': 11, 'gamma': 0.7881078163792542}. Best is trial 4 with value: 0.0692195574203545.[0m
[32m[I 2025-02-03 10:28:11,825][0m Trial 8 finished with value: 0.06944198403333392 and parameters: {'observation_period_num': 110, 'train_rates': 0.7743030837293325, 'learning_rate': 8.80748106639492e-05, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8758758950839255}. Best is trial 4 with value: 0.0692195574203545.[0m
[32m[I 2025-02-03 10:29:03,629][0m Trial 9 finished with value: 0.10515255154294764 and parameters: {'observation_period_num': 84, 'train_rates': 0.7393497401191242, 'learning_rate': 5.025215144953588e-05, 'batch_size': 100, 'step_size': 6, 'gamma': 0.7828476952781442}. Best is trial 4 with value: 0.0692195574203545.[0m
[32m[I 2025-02-03 10:29:28,612][0m Trial 10 finished with value: 0.044648259143594285 and parameters: {'observation_period_num': 14, 'train_rates': 0.8551607398343045, 'learning_rate': 0.0005591613832731621, 'batch_size': 255, 'step_size': 2, 'gamma': 0.8940723215177874}. Best is trial 10 with value: 0.044648259143594285.[0m
[32m[I 2025-02-03 10:29:54,845][0m Trial 11 finished with value: 0.05196826056688252 and parameters: {'observation_period_num': 15, 'train_rates': 0.868662255226829, 'learning_rate': 0.0009637104558474918, 'batch_size': 241, 'step_size': 1, 'gamma': 0.8861259221010226}. Best is trial 10 with value: 0.044648259143594285.[0m
[32m[I 2025-02-03 10:30:21,169][0m Trial 12 finished with value: 0.05579842034388672 and parameters: {'observation_period_num': 8, 'train_rates': 0.9103176263189793, 'learning_rate': 0.0008680439400568347, 'batch_size': 249, 'step_size': 1, 'gamma': 0.8868207316025718}. Best is trial 10 with value: 0.044648259143594285.[0m
Early stopping at epoch 63
[32m[I 2025-02-03 10:30:38,255][0m Trial 13 finished with value: 0.07491795597119634 and parameters: {'observation_period_num': 5, 'train_rates': 0.8878773879063978, 'learning_rate': 0.0002972974481506871, 'batch_size': 243, 'step_size': 1, 'gamma': 0.815376540793816}. Best is trial 10 with value: 0.044648259143594285.[0m
[32m[I 2025-02-03 10:31:11,786][0m Trial 14 finished with value: 0.04777343571186066 and parameters: {'observation_period_num': 53, 'train_rates': 0.978175976515019, 'learning_rate': 0.0003771501933662357, 'batch_size': 197, 'step_size': 3, 'gamma': 0.8971902999314246}. Best is trial 10 with value: 0.044648259143594285.[0m
[32m[I 2025-02-03 10:31:45,636][0m Trial 15 finished with value: 0.04487941786646843 and parameters: {'observation_period_num': 49, 'train_rates': 0.9756675414029963, 'learning_rate': 0.0002923511370437708, 'batch_size': 193, 'step_size': 4, 'gamma': 0.9029168466812004}. Best is trial 10 with value: 0.044648259143594285.[0m
[32m[I 2025-02-03 10:32:17,096][0m Trial 16 finished with value: 0.04153785854578018 and parameters: {'observation_period_num': 48, 'train_rates': 0.9366879176298828, 'learning_rate': 0.00014926542496348374, 'batch_size': 195, 'step_size': 4, 'gamma': 0.955929533393726}. Best is trial 16 with value: 0.04153785854578018.[0m
[32m[I 2025-02-03 10:32:46,899][0m Trial 17 finished with value: 0.04920152169630824 and parameters: {'observation_period_num': 47, 'train_rates': 0.9252886050256667, 'learning_rate': 8.395844087876658e-05, 'batch_size': 215, 'step_size': 5, 'gamma': 0.9658017152488625}. Best is trial 16 with value: 0.04153785854578018.[0m
[32m[I 2025-02-03 10:33:23,818][0m Trial 18 finished with value: 0.09167175423232697 and parameters: {'observation_period_num': 39, 'train_rates': 0.8417959213118805, 'learning_rate': 1.0621096384129163e-05, 'batch_size': 158, 'step_size': 3, 'gamma': 0.9846350321775915}. Best is trial 16 with value: 0.04153785854578018.[0m
[32m[I 2025-02-03 10:33:53,296][0m Trial 19 finished with value: 0.050415389239788055 and parameters: {'observation_period_num': 73, 'train_rates': 0.9346611852924848, 'learning_rate': 0.00014574801223470553, 'batch_size': 223, 'step_size': 2, 'gamma': 0.9567549565046493}. Best is trial 16 with value: 0.04153785854578018.[0m
[32m[I 2025-02-03 10:34:30,195][0m Trial 20 finished with value: 0.11578517899231275 and parameters: {'observation_period_num': 142, 'train_rates': 0.8481632926273103, 'learning_rate': 4.565515899587975e-05, 'batch_size': 158, 'step_size': 6, 'gamma': 0.7522942091832457}. Best is trial 16 with value: 0.04153785854578018.[0m
[32m[I 2025-02-03 10:35:03,635][0m Trial 21 finished with value: 0.04389744624495506 and parameters: {'observation_period_num': 30, 'train_rates': 0.9769297437307735, 'learning_rate': 0.0003875461627830038, 'batch_size': 192, 'step_size': 4, 'gamma': 0.9054600414786798}. Best is trial 16 with value: 0.04153785854578018.[0m
[32m[I 2025-02-03 10:35:35,785][0m Trial 22 finished with value: 0.03947560489177704 and parameters: {'observation_period_num': 33, 'train_rates': 0.9422341982493485, 'learning_rate': 0.0005162047323975956, 'batch_size': 211, 'step_size': 3, 'gamma': 0.9132535486672471}. Best is trial 22 with value: 0.03947560489177704.[0m
[32m[I 2025-02-03 10:36:07,203][0m Trial 23 finished with value: 0.043108973652124405 and parameters: {'observation_period_num': 36, 'train_rates': 0.941418006576428, 'learning_rate': 0.00017416626547198263, 'batch_size': 211, 'step_size': 5, 'gamma': 0.9168856652534132}. Best is trial 22 with value: 0.03947560489177704.[0m
[32m[I 2025-02-03 10:36:37,185][0m Trial 24 finished with value: 0.046678800135850906 and parameters: {'observation_period_num': 64, 'train_rates': 0.9409405039909585, 'learning_rate': 0.00017110641756173955, 'batch_size': 218, 'step_size': 6, 'gamma': 0.9433743342434053}. Best is trial 22 with value: 0.03947560489177704.[0m
[32m[I 2025-02-03 10:37:15,577][0m Trial 25 finished with value: 0.04476100375466313 and parameters: {'observation_period_num': 31, 'train_rates': 0.9036172239453978, 'learning_rate': 7.398071131451805e-05, 'batch_size': 164, 'step_size': 5, 'gamma': 0.9199709004159281}. Best is trial 22 with value: 0.03947560489177704.[0m
[32m[I 2025-02-03 10:37:45,760][0m Trial 26 finished with value: 0.06547470390796661 and parameters: {'observation_period_num': 139, 'train_rates': 0.9409826762306556, 'learning_rate': 0.00015035544629577407, 'batch_size': 204, 'step_size': 8, 'gamma': 0.8519298381353276}. Best is trial 22 with value: 0.03947560489177704.[0m
[32m[I 2025-02-03 10:38:15,545][0m Trial 27 finished with value: 0.07611266523599625 and parameters: {'observation_period_num': 62, 'train_rates': 0.9862285505236551, 'learning_rate': 0.0005443048371623488, 'batch_size': 226, 'step_size': 3, 'gamma': 0.9695980009258505}. Best is trial 22 with value: 0.03947560489177704.[0m
[32m[I 2025-02-03 10:38:49,430][0m Trial 28 finished with value: 0.034821223898290156 and parameters: {'observation_period_num': 30, 'train_rates': 0.8861476894848748, 'learning_rate': 0.00020523668337382996, 'batch_size': 181, 'step_size': 7, 'gamma': 0.9200230665299881}. Best is trial 28 with value: 0.034821223898290156.[0m
[32m[I 2025-02-03 10:39:20,671][0m Trial 29 finished with value: 0.062004750509831036 and parameters: {'observation_period_num': 99, 'train_rates': 0.7842417791732892, 'learning_rate': 0.00022884065762531828, 'batch_size': 177, 'step_size': 10, 'gamma': 0.934851880720613}. Best is trial 28 with value: 0.034821223898290156.[0m
[32m[I 2025-02-03 10:39:59,491][0m Trial 30 finished with value: 0.03512988835573196 and parameters: {'observation_period_num': 72, 'train_rates': 0.8718267198025437, 'learning_rate': 0.0005245020041415996, 'batch_size': 148, 'step_size': 9, 'gamma': 0.8613116881776386}. Best is trial 28 with value: 0.034821223898290156.[0m
[32m[I 2025-02-03 10:40:38,400][0m Trial 31 finished with value: 0.03642136485220116 and parameters: {'observation_period_num': 76, 'train_rates': 0.8141003982707742, 'learning_rate': 0.000468565331299292, 'batch_size': 140, 'step_size': 9, 'gamma': 0.8656524794977414}. Best is trial 28 with value: 0.034821223898290156.[0m
[32m[I 2025-02-03 10:41:19,476][0m Trial 32 finished with value: 0.0404687659345335 and parameters: {'observation_period_num': 84, 'train_rates': 0.8771876791105798, 'learning_rate': 0.0005212055050316638, 'batch_size': 145, 'step_size': 9, 'gamma': 0.8584179216506498}. Best is trial 28 with value: 0.034821223898290156.[0m
[32m[I 2025-02-03 10:42:03,826][0m Trial 33 finished with value: 0.03368617272385401 and parameters: {'observation_period_num': 71, 'train_rates': 0.8151422703046989, 'learning_rate': 0.0005912265785055762, 'batch_size': 125, 'step_size': 12, 'gamma': 0.8320960479151197}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:42:47,265][0m Trial 34 finished with value: 0.046557026816763554 and parameters: {'observation_period_num': 119, 'train_rates': 0.8099738979520605, 'learning_rate': 0.00026218630421193464, 'batch_size': 127, 'step_size': 12, 'gamma': 0.8243763634453544}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:43:56,256][0m Trial 35 finished with value: 0.044270234118247855 and parameters: {'observation_period_num': 78, 'train_rates': 0.7981592636242963, 'learning_rate': 0.00070049312409079, 'batch_size': 78, 'step_size': 13, 'gamma': 0.8681031908345641}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:44:41,898][0m Trial 36 finished with value: 0.0433026846699618 and parameters: {'observation_period_num': 100, 'train_rates': 0.7665358178008795, 'learning_rate': 0.0004012033012158121, 'batch_size': 119, 'step_size': 9, 'gamma': 0.8309186889617911}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:45:21,773][0m Trial 37 finished with value: 0.03901569986178132 and parameters: {'observation_period_num': 67, 'train_rates': 0.8189615955916291, 'learning_rate': 0.00022545394844399142, 'batch_size': 142, 'step_size': 8, 'gamma': 0.810632893095205}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:46:06,437][0m Trial 38 finished with value: 0.4732315058770933 and parameters: {'observation_period_num': 126, 'train_rates': 0.6640469087935014, 'learning_rate': 1.2862343208816563e-06, 'batch_size': 105, 'step_size': 10, 'gamma': 0.86701751899153}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:47:14,773][0m Trial 39 finished with value: 0.19364029956058668 and parameters: {'observation_period_num': 176, 'train_rates': 0.6029381337120977, 'learning_rate': 1.3729585527326413e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.8414044265526228}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:47:47,270][0m Trial 40 finished with value: 0.38689332207043964 and parameters: {'observation_period_num': 149, 'train_rates': 0.8235733415714365, 'learning_rate': 6.354939049826956e-06, 'batch_size': 176, 'step_size': 7, 'gamma': 0.7972914235814446}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:48:28,869][0m Trial 41 finished with value: 0.03655239981081751 and parameters: {'observation_period_num': 67, 'train_rates': 0.8315706738476598, 'learning_rate': 0.0006948327092519962, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8015290321542269}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:49:07,261][0m Trial 42 finished with value: 0.03926176594048948 and parameters: {'observation_period_num': 90, 'train_rates': 0.7515335823514321, 'learning_rate': 0.000674413829704107, 'batch_size': 137, 'step_size': 10, 'gamma': 0.8050106357741701}. Best is trial 33 with value: 0.03368617272385401.[0m
[32m[I 2025-02-03 10:49:46,789][0m Trial 43 finished with value: 0.03275193332199486 and parameters: {'observation_period_num': 59, 'train_rates': 0.8628980450178907, 'learning_rate': 0.000984270866628762, 'batch_size': 148, 'step_size': 7, 'gamma': 0.7612829937879019}. Best is trial 43 with value: 0.03275193332199486.[0m
[32m[I 2025-02-03 10:50:26,713][0m Trial 44 finished with value: 0.032674435389960094 and parameters: {'observation_period_num': 19, 'train_rates': 0.8695650187651343, 'learning_rate': 0.0009350993117057379, 'batch_size': 150, 'step_size': 7, 'gamma': 0.7658297790376001}. Best is trial 44 with value: 0.032674435389960094.[0m
[32m[I 2025-02-03 10:51:06,169][0m Trial 45 finished with value: 0.03390212577037715 and parameters: {'observation_period_num': 21, 'train_rates': 0.8738581258155732, 'learning_rate': 0.000993760921386579, 'batch_size': 151, 'step_size': 7, 'gamma': 0.7593245438949275}. Best is trial 44 with value: 0.032674435389960094.[0m
[32m[I 2025-02-03 10:51:55,511][0m Trial 46 finished with value: 0.031004444144818247 and parameters: {'observation_period_num': 21, 'train_rates': 0.8996967489278387, 'learning_rate': 0.0009565597745019144, 'batch_size': 120, 'step_size': 7, 'gamma': 0.7594604847547441}. Best is trial 46 with value: 0.031004444144818247.[0m
[32m[I 2025-02-03 10:52:53,912][0m Trial 47 finished with value: 0.03244099599851979 and parameters: {'observation_period_num': 20, 'train_rates': 0.9052197785964858, 'learning_rate': 0.0008671213636509858, 'batch_size': 106, 'step_size': 7, 'gamma': 0.757458986313145}. Best is trial 46 with value: 0.031004444144818247.[0m
[32m[I 2025-02-03 10:53:52,997][0m Trial 48 finished with value: 0.029969075928300115 and parameters: {'observation_period_num': 20, 'train_rates': 0.899623776255662, 'learning_rate': 0.0007786747285151263, 'batch_size': 103, 'step_size': 7, 'gamma': 0.7739025708598736}. Best is trial 48 with value: 0.029969075928300115.[0m
[32m[I 2025-02-03 10:54:51,081][0m Trial 49 finished with value: 0.025623054745105598 and parameters: {'observation_period_num': 17, 'train_rates': 0.9070947808069317, 'learning_rate': 0.0009655436260558819, 'batch_size': 105, 'step_size': 7, 'gamma': 0.7744254542292559}. Best is trial 49 with value: 0.025623054745105598.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-03 10:54:51,092][0m A new study created in memory with name: no-name-07029a24-4230-4c2b-9792-621197901a7f[0m
[32m[I 2025-02-03 10:55:31,342][0m Trial 0 finished with value: 0.05756875871370236 and parameters: {'observation_period_num': 81, 'train_rates': 0.6864266018959223, 'learning_rate': 0.00019137890994478494, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9688328858155097}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 10:58:14,641][0m Trial 1 finished with value: 0.09734557386995717 and parameters: {'observation_period_num': 55, 'train_rates': 0.9089542176350003, 'learning_rate': 3.1790664434005014e-06, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9153286741991293}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 10:59:07,298][0m Trial 2 finished with value: 0.10868491232395172 and parameters: {'observation_period_num': 232, 'train_rates': 0.960738839108396, 'learning_rate': 4.04078647098091e-05, 'batch_size': 112, 'step_size': 6, 'gamma': 0.8563806213311678}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 10:59:35,583][0m Trial 3 finished with value: 0.6825085909743058 and parameters: {'observation_period_num': 140, 'train_rates': 0.8852874967055351, 'learning_rate': 1.2833608684960223e-06, 'batch_size': 221, 'step_size': 10, 'gamma': 0.9712724758859064}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 11:02:37,405][0m Trial 4 finished with value: 0.35502503677637864 and parameters: {'observation_period_num': 182, 'train_rates': 0.7525783700657255, 'learning_rate': 2.33520053915021e-06, 'batch_size': 26, 'step_size': 4, 'gamma': 0.7940918138071537}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 11:03:13,956][0m Trial 5 finished with value: 0.09751643240451813 and parameters: {'observation_period_num': 66, 'train_rates': 0.9898876643655943, 'learning_rate': 4.9079184448526835e-05, 'batch_size': 180, 'step_size': 8, 'gamma': 0.8383200709778421}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 11:03:40,636][0m Trial 6 finished with value: 0.09607234568904441 and parameters: {'observation_period_num': 17, 'train_rates': 0.6142326078210437, 'learning_rate': 3.3013630453667e-05, 'batch_size': 199, 'step_size': 3, 'gamma': 0.9790686524290116}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 11:04:34,109][0m Trial 7 finished with value: 0.1139117692812131 and parameters: {'observation_period_num': 227, 'train_rates': 0.6756527254844158, 'learning_rate': 4.670264441666503e-05, 'batch_size': 87, 'step_size': 14, 'gamma': 0.9533020932675741}. Best is trial 0 with value: 0.05756875871370236.[0m
[32m[I 2025-02-03 11:07:45,014][0m Trial 8 finished with value: 0.05525128276430987 and parameters: {'observation_period_num': 159, 'train_rates': 0.7991630547545863, 'learning_rate': 0.00017734919544245436, 'batch_size': 26, 'step_size': 7, 'gamma': 0.7721440600237115}. Best is trial 8 with value: 0.05525128276430987.[0m
[32m[I 2025-02-03 11:08:12,301][0m Trial 9 finished with value: 0.16634953793861884 and parameters: {'observation_period_num': 113, 'train_rates': 0.9023156271261137, 'learning_rate': 9.533617866753717e-06, 'batch_size': 234, 'step_size': 12, 'gamma': 0.9458641471975865}. Best is trial 8 with value: 0.05525128276430987.[0m
Early stopping at epoch 64
[32m[I 2025-02-03 11:09:02,497][0m Trial 10 finished with value: 0.11420111396340094 and parameters: {'observation_period_num': 169, 'train_rates': 0.7966956734646424, 'learning_rate': 0.0005826506882069256, 'batch_size': 67, 'step_size': 1, 'gamma': 0.7571471151257287}. Best is trial 8 with value: 0.05525128276430987.[0m
[32m[I 2025-02-03 11:09:36,025][0m Trial 11 finished with value: 0.04683194098180052 and parameters: {'observation_period_num': 92, 'train_rates': 0.75276091007535, 'learning_rate': 0.00029333175473063685, 'batch_size': 155, 'step_size': 8, 'gamma': 0.8952909363696808}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:10:11,480][0m Trial 12 finished with value: 0.04871114090172856 and parameters: {'observation_period_num': 122, 'train_rates': 0.8204114299766759, 'learning_rate': 0.0002500608571027321, 'batch_size': 164, 'step_size': 6, 'gamma': 0.888313586128587}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:10:43,852][0m Trial 13 finished with value: 0.05326750145908486 and parameters: {'observation_period_num': 116, 'train_rates': 0.8402217517409103, 'learning_rate': 0.0007640793953969839, 'batch_size': 176, 'step_size': 5, 'gamma': 0.897990704362733}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:11:18,756][0m Trial 14 finished with value: 0.04938970187681481 and parameters: {'observation_period_num': 94, 'train_rates': 0.7443187251155895, 'learning_rate': 0.00016761998623104397, 'batch_size': 158, 'step_size': 10, 'gamma': 0.8930186132314749}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:11:55,983][0m Trial 15 finished with value: 0.05352452168862025 and parameters: {'observation_period_num': 31, 'train_rates': 0.7431981914724888, 'learning_rate': 0.0003649079578742956, 'batch_size': 148, 'step_size': 2, 'gamma': 0.8251431529561523}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:12:49,191][0m Trial 16 finished with value: 0.06450168834228606 and parameters: {'observation_period_num': 200, 'train_rates': 0.8462404413728806, 'learning_rate': 0.00012535722453992513, 'batch_size': 104, 'step_size': 6, 'gamma': 0.9198432054674974}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:13:16,086][0m Trial 17 finished with value: 0.07300082757746942 and parameters: {'observation_period_num': 134, 'train_rates': 0.6881647781197447, 'learning_rate': 0.0009945667882121693, 'batch_size': 206, 'step_size': 12, 'gamma': 0.871867030973302}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:13:40,592][0m Trial 18 finished with value: 0.11998235074610546 and parameters: {'observation_period_num': 51, 'train_rates': 0.8400648204229533, 'learning_rate': 1.483184349790844e-05, 'batch_size': 254, 'step_size': 8, 'gamma': 0.8727518653339996}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:14:15,389][0m Trial 19 finished with value: 0.1454741256202756 and parameters: {'observation_period_num': 97, 'train_rates': 0.6100508858358722, 'learning_rate': 8.864214428578079e-05, 'batch_size': 134, 'step_size': 4, 'gamma': 0.810371463696028}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:14:46,680][0m Trial 20 finished with value: 0.06167631190876628 and parameters: {'observation_period_num': 135, 'train_rates': 0.7711157793835259, 'learning_rate': 0.0003421504227112677, 'batch_size': 174, 'step_size': 11, 'gamma': 0.9262283597062789}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:15:18,678][0m Trial 21 finished with value: 0.0505004482078349 and parameters: {'observation_period_num': 90, 'train_rates': 0.7132405856725252, 'learning_rate': 0.0003122941203268151, 'batch_size': 164, 'step_size': 10, 'gamma': 0.8985358041037229}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:15:53,272][0m Trial 22 finished with value: 0.07551259577022418 and parameters: {'observation_period_num': 111, 'train_rates': 0.7295177649213129, 'learning_rate': 7.446788772841091e-05, 'batch_size': 153, 'step_size': 7, 'gamma': 0.899715440600788}. Best is trial 11 with value: 0.04683194098180052.[0m
[32m[I 2025-02-03 11:16:22,627][0m Trial 23 finished with value: 0.04264111365617638 and parameters: {'observation_period_num': 71, 'train_rates': 0.7801410462502233, 'learning_rate': 0.00019331770830033237, 'batch_size': 194, 'step_size': 9, 'gamma': 0.8576817713100715}. Best is trial 23 with value: 0.04264111365617638.[0m
[32m[I 2025-02-03 11:16:53,348][0m Trial 24 finished with value: 0.03686533092787718 and parameters: {'observation_period_num': 73, 'train_rates': 0.8130848781680784, 'learning_rate': 0.0004751837520715112, 'batch_size': 190, 'step_size': 8, 'gamma': 0.8507225624229329}. Best is trial 24 with value: 0.03686533092787718.[0m
[32m[I 2025-02-03 11:17:22,279][0m Trial 25 finished with value: 0.03457069507567212 and parameters: {'observation_period_num': 36, 'train_rates': 0.7805370737500014, 'learning_rate': 0.0005660651328611909, 'batch_size': 197, 'step_size': 8, 'gamma': 0.8369002231764032}. Best is trial 25 with value: 0.03457069507567212.[0m
[32m[I 2025-02-03 11:17:52,556][0m Trial 26 finished with value: 0.0311949925463483 and parameters: {'observation_period_num': 8, 'train_rates': 0.8688048525210881, 'learning_rate': 0.00048380912767068753, 'batch_size': 199, 'step_size': 9, 'gamma': 0.850015636043663}. Best is trial 26 with value: 0.0311949925463483.[0m
[32m[I 2025-02-03 11:18:20,988][0m Trial 27 finished with value: 0.032951416958483956 and parameters: {'observation_period_num': 9, 'train_rates': 0.8738630453900575, 'learning_rate': 0.0004889759993431792, 'batch_size': 226, 'step_size': 12, 'gamma': 0.8342582238344334}. Best is trial 26 with value: 0.0311949925463483.[0m
[32m[I 2025-02-03 11:18:47,408][0m Trial 28 finished with value: 0.03372941559679072 and parameters: {'observation_period_num': 8, 'train_rates': 0.8797005105556455, 'learning_rate': 0.0006213875306974352, 'batch_size': 256, 'step_size': 12, 'gamma': 0.8097854249657951}. Best is trial 26 with value: 0.0311949925463483.[0m
[32m[I 2025-02-03 11:19:14,535][0m Trial 29 finished with value: 0.07901946383714675 and parameters: {'observation_period_num': 7, 'train_rates': 0.8728114390045065, 'learning_rate': 1.6161891232799772e-05, 'batch_size': 248, 'step_size': 13, 'gamma': 0.7932017096690683}. Best is trial 26 with value: 0.0311949925463483.[0m
[32m[I 2025-02-03 11:19:45,414][0m Trial 30 finished with value: 0.034675495233386755 and parameters: {'observation_period_num': 6, 'train_rates': 0.9227791916787925, 'learning_rate': 0.0006719701307391375, 'batch_size': 219, 'step_size': 13, 'gamma': 0.8128451533926015}. Best is trial 26 with value: 0.0311949925463483.[0m
[32m[I 2025-02-03 11:20:12,791][0m Trial 31 finished with value: 0.03001195803304149 and parameters: {'observation_period_num': 33, 'train_rates': 0.8694012310674082, 'learning_rate': 0.0009722044273311763, 'batch_size': 231, 'step_size': 11, 'gamma': 0.8336264200564073}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:20:41,128][0m Trial 32 finished with value: 0.0327627956867218 and parameters: {'observation_period_num': 30, 'train_rates': 0.9399702368963684, 'learning_rate': 0.000985621145040116, 'batch_size': 237, 'step_size': 11, 'gamma': 0.7952147668060328}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:21:08,371][0m Trial 33 finished with value: 0.0350099541246891 and parameters: {'observation_period_num': 33, 'train_rates': 0.9405634504796478, 'learning_rate': 0.000975113965714848, 'batch_size': 238, 'step_size': 15, 'gamma': 0.7882642555609517}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:21:38,239][0m Trial 34 finished with value: 0.03933936357498169 and parameters: {'observation_period_num': 43, 'train_rates': 0.9544719830556655, 'learning_rate': 0.000424077216204634, 'batch_size': 219, 'step_size': 11, 'gamma': 0.833058521579904}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:22:06,800][0m Trial 35 finished with value: 0.03286283252591436 and parameters: {'observation_period_num': 23, 'train_rates': 0.9062803085495272, 'learning_rate': 0.0009040151007992625, 'batch_size': 233, 'step_size': 11, 'gamma': 0.854311732323457}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:22:35,681][0m Trial 36 finished with value: 0.03218488553953677 and parameters: {'observation_period_num': 24, 'train_rates': 0.9158203959349915, 'learning_rate': 0.0009781441772462338, 'batch_size': 236, 'step_size': 11, 'gamma': 0.8551624661651722}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:23:05,577][0m Trial 37 finished with value: 0.03235331550240517 and parameters: {'observation_period_num': 45, 'train_rates': 0.9299044812347962, 'learning_rate': 0.0009842152428262718, 'batch_size': 210, 'step_size': 9, 'gamma': 0.782832661601871}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:23:37,831][0m Trial 38 finished with value: 0.2976151406764984 and parameters: {'observation_period_num': 59, 'train_rates': 0.9896870065562419, 'learning_rate': 6.542236008923689e-06, 'batch_size': 210, 'step_size': 9, 'gamma': 0.7507032839159262}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:24:10,251][0m Trial 39 finished with value: 0.05296327546238899 and parameters: {'observation_period_num': 58, 'train_rates': 0.9718252237732968, 'learning_rate': 0.0002341197676213811, 'batch_size': 214, 'step_size': 9, 'gamma': 0.8610381773002218}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:24:44,246][0m Trial 40 finished with value: 0.3717118944941449 and parameters: {'observation_period_num': 44, 'train_rates': 0.9184326694069731, 'learning_rate': 2.417034438709431e-06, 'batch_size': 185, 'step_size': 10, 'gamma': 0.7672620384862303}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:25:12,822][0m Trial 41 finished with value: 0.03363631293177605 and parameters: {'observation_period_num': 24, 'train_rates': 0.9394845436694151, 'learning_rate': 0.0007519657220064239, 'batch_size': 238, 'step_size': 10, 'gamma': 0.7863082211906591}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:25:39,856][0m Trial 42 finished with value: 0.030718287885142345 and parameters: {'observation_period_num': 22, 'train_rates': 0.8912200983494499, 'learning_rate': 0.0009635464004324525, 'batch_size': 244, 'step_size': 14, 'gamma': 0.8219637635634303}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:26:10,497][0m Trial 43 finished with value: 0.031422738325072835 and parameters: {'observation_period_num': 20, 'train_rates': 0.8969826842977812, 'learning_rate': 0.0004483517639025469, 'batch_size': 205, 'step_size': 14, 'gamma': 0.8185081907713866}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:26:36,976][0m Trial 44 finished with value: 0.43269589355269683 and parameters: {'observation_period_num': 18, 'train_rates': 0.8597926110800516, 'learning_rate': 1.0693062720664908e-06, 'batch_size': 246, 'step_size': 15, 'gamma': 0.8225758158833638}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:27:05,175][0m Trial 45 finished with value: 0.032459616816292204 and parameters: {'observation_period_num': 18, 'train_rates': 0.8897686690910649, 'learning_rate': 0.00042053159803489247, 'batch_size': 225, 'step_size': 14, 'gamma': 0.849225793518515}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:28:47,984][0m Trial 46 finished with value: 0.04804553626957586 and parameters: {'observation_period_num': 60, 'train_rates': 0.8980548270625949, 'learning_rate': 0.0006504693763387316, 'batch_size': 56, 'step_size': 14, 'gamma': 0.8232411488298899}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:29:13,613][0m Trial 47 finished with value: 0.04094339648293237 and parameters: {'observation_period_num': 39, 'train_rates': 0.8582383292883806, 'learning_rate': 0.00014732402027422934, 'batch_size': 229, 'step_size': 13, 'gamma': 0.8796839798069948}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:30:00,891][0m Trial 48 finished with value: 0.0459256030876061 and parameters: {'observation_period_num': 21, 'train_rates': 0.8219219303259623, 'learning_rate': 0.0002287702861198504, 'batch_size': 121, 'step_size': 15, 'gamma': 0.8436668842126773}. Best is trial 31 with value: 0.03001195803304149.[0m
[32m[I 2025-02-03 11:30:33,490][0m Trial 49 finished with value: 0.050983514476056195 and parameters: {'observation_period_num': 52, 'train_rates': 0.9123573798434575, 'learning_rate': 0.00010788681026481454, 'batch_size': 203, 'step_size': 13, 'gamma': 0.8016084229782027}. Best is trial 31 with value: 0.03001195803304149.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-03 11:30:33,500][0m A new study created in memory with name: no-name-b1f815d1-f121-454f-95e8-7454f94905db[0m
[32m[I 2025-02-03 11:31:24,548][0m Trial 0 finished with value: 0.7251171171086506 and parameters: {'observation_period_num': 229, 'train_rates': 0.8110488151688653, 'learning_rate': 1.4166537790743749e-06, 'batch_size': 107, 'step_size': 6, 'gamma': 0.8000003354722465}. Best is trial 0 with value: 0.7251171171086506.[0m
[32m[I 2025-02-03 11:32:21,757][0m Trial 1 finished with value: 0.044758940837292 and parameters: {'observation_period_num': 55, 'train_rates': 0.851378326348341, 'learning_rate': 6.982342208189606e-05, 'batch_size': 101, 'step_size': 7, 'gamma': 0.8516235867931587}. Best is trial 1 with value: 0.044758940837292.[0m
[32m[I 2025-02-03 11:33:23,532][0m Trial 2 finished with value: 0.06958737403101012 and parameters: {'observation_period_num': 131, 'train_rates': 0.6042538696729439, 'learning_rate': 0.0003402885056791018, 'batch_size': 71, 'step_size': 8, 'gamma': 0.8148743207741236}. Best is trial 1 with value: 0.044758940837292.[0m
[32m[I 2025-02-03 11:33:46,354][0m Trial 3 finished with value: 0.2267252857486407 and parameters: {'observation_period_num': 200, 'train_rates': 0.6730327769390321, 'learning_rate': 1.817319056750749e-05, 'batch_size': 231, 'step_size': 8, 'gamma': 0.9413136582051127}. Best is trial 1 with value: 0.044758940837292.[0m
[32m[I 2025-02-03 11:34:12,181][0m Trial 4 finished with value: 0.04127884323093885 and parameters: {'observation_period_num': 56, 'train_rates': 0.8763111449295662, 'learning_rate': 0.00018672106457523388, 'batch_size': 236, 'step_size': 15, 'gamma': 0.8412668470985676}. Best is trial 4 with value: 0.04127884323093885.[0m
[32m[I 2025-02-03 11:34:47,009][0m Trial 5 finished with value: 0.1129883202938082 and parameters: {'observation_period_num': 208, 'train_rates': 0.8429798406664448, 'learning_rate': 8.045949068076294e-05, 'batch_size': 161, 'step_size': 4, 'gamma': 0.8488484893444463}. Best is trial 4 with value: 0.04127884323093885.[0m
[32m[I 2025-02-03 11:36:07,094][0m Trial 6 finished with value: 0.116664796441543 and parameters: {'observation_period_num': 210, 'train_rates': 0.8264090752217517, 'learning_rate': 8.114939418920082e-05, 'batch_size': 64, 'step_size': 3, 'gamma': 0.7892483697126568}. Best is trial 4 with value: 0.04127884323093885.[0m
[32m[I 2025-02-03 11:36:48,059][0m Trial 7 finished with value: 0.0435756825763418 and parameters: {'observation_period_num': 54, 'train_rates': 0.7708732224150314, 'learning_rate': 0.0003033549358385404, 'batch_size': 134, 'step_size': 7, 'gamma': 0.9367473943248297}. Best is trial 4 with value: 0.04127884323093885.[0m
[32m[I 2025-02-03 11:37:27,928][0m Trial 8 finished with value: 0.07229133712044403 and parameters: {'observation_period_num': 159, 'train_rates': 0.7123324524709072, 'learning_rate': 0.0001569761214090145, 'batch_size': 132, 'step_size': 15, 'gamma': 0.7533652891683}. Best is trial 4 with value: 0.04127884323093885.[0m
[32m[I 2025-02-03 11:38:19,520][0m Trial 9 finished with value: 0.07183936187883261 and parameters: {'observation_period_num': 116, 'train_rates': 0.6663620612792326, 'learning_rate': 4.24834648381716e-05, 'batch_size': 92, 'step_size': 10, 'gamma': 0.9294966455326621}. Best is trial 4 with value: 0.04127884323093885.[0m
[32m[I 2025-02-03 11:38:47,736][0m Trial 10 finished with value: 0.032495371997356415 and parameters: {'observation_period_num': 12, 'train_rates': 0.973076045840293, 'learning_rate': 0.0009694673277879557, 'batch_size': 247, 'step_size': 15, 'gamma': 0.8901568097727968}. Best is trial 10 with value: 0.032495371997356415.[0m
[32m[I 2025-02-03 11:39:16,186][0m Trial 11 finished with value: 0.03751998767256737 and parameters: {'observation_period_num': 10, 'train_rates': 0.9733040916269923, 'learning_rate': 0.0006317318836585226, 'batch_size': 253, 'step_size': 15, 'gamma': 0.8891795269007862}. Best is trial 10 with value: 0.032495371997356415.[0m
[32m[I 2025-02-03 11:39:51,512][0m Trial 12 finished with value: 0.038116008043289185 and parameters: {'observation_period_num': 5, 'train_rates': 0.9719561384853094, 'learning_rate': 0.0007009767724915065, 'batch_size': 195, 'step_size': 12, 'gamma': 0.8925413072560577}. Best is trial 10 with value: 0.032495371997356415.[0m
[32m[I 2025-02-03 11:40:19,406][0m Trial 13 finished with value: 0.04868151992559433 and parameters: {'observation_period_num': 8, 'train_rates': 0.9881605047850354, 'learning_rate': 0.0008692605464907901, 'batch_size': 249, 'step_size': 12, 'gamma': 0.9892551804728302}. Best is trial 10 with value: 0.032495371997356415.[0m
[32m[I 2025-02-03 11:40:51,242][0m Trial 14 finished with value: 0.16849961695147725 and parameters: {'observation_period_num': 90, 'train_rates': 0.9284585444708784, 'learning_rate': 8.866016959556516e-06, 'batch_size': 196, 'step_size': 13, 'gamma': 0.8982319238799517}. Best is trial 10 with value: 0.032495371997356415.[0m
[32m[I 2025-02-03 11:46:40,890][0m Trial 15 finished with value: 0.03165000917889723 and parameters: {'observation_period_num': 40, 'train_rates': 0.9156243961516304, 'learning_rate': 0.0009946933407637387, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8897673129994769}. Best is trial 15 with value: 0.03165000917889723.[0m
[32m[I 2025-02-03 11:49:54,534][0m Trial 16 finished with value: 0.08533167656556315 and parameters: {'observation_period_num': 37, 'train_rates': 0.9153374623302634, 'learning_rate': 2.275882834463669e-06, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9747567139360652}. Best is trial 15 with value: 0.03165000917889723.[0m
[32m[I 2025-02-03 11:54:12,476][0m Trial 17 finished with value: 0.03139156718933922 and parameters: {'observation_period_num': 88, 'train_rates': 0.9021607318718192, 'learning_rate': 0.0009954732602319445, 'batch_size': 21, 'step_size': 10, 'gamma': 0.8754601012778067}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 11:58:34,557][0m Trial 18 finished with value: 0.23036697951795385 and parameters: {'observation_period_num': 89, 'train_rates': 0.903065363402378, 'learning_rate': 5.293914943518141e-06, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9179796638718974}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:00:27,279][0m Trial 19 finished with value: 0.041447439573464855 and parameters: {'observation_period_num': 91, 'train_rates': 0.7834714337920536, 'learning_rate': 0.0002851694582593044, 'batch_size': 44, 'step_size': 10, 'gamma': 0.866368468928883}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:02:27,278][0m Trial 20 finished with value: 0.07622488997297583 and parameters: {'observation_period_num': 159, 'train_rates': 0.9309149034229054, 'learning_rate': 1.7764898800452465e-05, 'batch_size': 46, 'step_size': 11, 'gamma': 0.8240232212621384}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:03:00,703][0m Trial 21 finished with value: 0.03172970935702324 and parameters: {'observation_period_num': 36, 'train_rates': 0.9456844878229785, 'learning_rate': 0.0009024846141970392, 'batch_size': 185, 'step_size': 13, 'gamma': 0.8779820063050504}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:03:37,303][0m Trial 22 finished with value: 0.04159105191996068 and parameters: {'observation_period_num': 37, 'train_rates': 0.888996388853742, 'learning_rate': 0.00047301282059694074, 'batch_size': 172, 'step_size': 13, 'gamma': 0.8704596295966808}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:09:27,998][0m Trial 23 finished with value: 0.052160297952047206 and parameters: {'observation_period_num': 69, 'train_rates': 0.9432056910991824, 'learning_rate': 0.00018029608410056758, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9089856148357979}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:09:57,851][0m Trial 24 finished with value: 0.033329718239566826 and parameters: {'observation_period_num': 34, 'train_rates': 0.8678201100915579, 'learning_rate': 0.0004684045101133165, 'batch_size': 204, 'step_size': 13, 'gamma': 0.8747772907829339}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:10:35,336][0m Trial 25 finished with value: 0.05604231730103493 and parameters: {'observation_period_num': 115, 'train_rates': 0.9500215422829758, 'learning_rate': 0.0009332086721159376, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9538724119201869}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:11:55,088][0m Trial 26 finished with value: 0.06338720638679977 and parameters: {'observation_period_num': 68, 'train_rates': 0.8966615633756115, 'learning_rate': 0.0004289638917713628, 'batch_size': 70, 'step_size': 9, 'gamma': 0.8319099461306552}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:13:52,226][0m Trial 27 finished with value: 0.03353826922010368 and parameters: {'observation_period_num': 29, 'train_rates': 0.9517414071330283, 'learning_rate': 0.00023619398213948152, 'batch_size': 50, 'step_size': 12, 'gamma': 0.8672716947509712}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:14:29,839][0m Trial 28 finished with value: 0.056903349817730486 and parameters: {'observation_period_num': 80, 'train_rates': 0.8664005180312981, 'learning_rate': 0.00010711010851660698, 'batch_size': 150, 'step_size': 14, 'gamma': 0.9108355372242463}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:15:14,776][0m Trial 29 finished with value: 0.4530207866580908 and parameters: {'observation_period_num': 107, 'train_rates': 0.8054783127059276, 'learning_rate': 1.216958022622898e-06, 'batch_size': 115, 'step_size': 11, 'gamma': 0.7913706194027244}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:16:12,240][0m Trial 30 finished with value: 0.05638646912686995 and parameters: {'observation_period_num': 142, 'train_rates': 0.8301368206442392, 'learning_rate': 0.0005667667377119199, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8794452760023248}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:16:42,964][0m Trial 31 finished with value: 0.03408140689134598 and parameters: {'observation_period_num': 25, 'train_rates': 0.9596219939757835, 'learning_rate': 0.0008673611928621335, 'batch_size': 214, 'step_size': 14, 'gamma': 0.8564068546162755}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:17:17,705][0m Trial 32 finished with value: 0.0647941529750824 and parameters: {'observation_period_num': 53, 'train_rates': 0.9822733817394635, 'learning_rate': 0.0009707000431036262, 'batch_size': 185, 'step_size': 14, 'gamma': 0.8977377761747902}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:17:45,019][0m Trial 33 finished with value: 0.0681543346878659 and parameters: {'observation_period_num': 249, 'train_rates': 0.9175017126028312, 'learning_rate': 0.0003609795390997744, 'batch_size': 219, 'step_size': 9, 'gamma': 0.9161983151318732}. Best is trial 17 with value: 0.03139156718933922.[0m
[32m[I 2025-02-03 12:20:34,229][0m Trial 34 finished with value: 0.03027766860861291 and parameters: {'observation_period_num': 19, 'train_rates': 0.9367192301926945, 'learning_rate': 0.0005650658348889159, 'batch_size': 34, 'step_size': 6, 'gamma': 0.8848895750510363}. Best is trial 34 with value: 0.03027766860861291.[0m
[32m[I 2025-02-03 12:23:10,114][0m Trial 35 finished with value: 0.03191685818358144 and parameters: {'observation_period_num': 47, 'train_rates': 0.908007022113911, 'learning_rate': 0.0005909980049980408, 'batch_size': 36, 'step_size': 5, 'gamma': 0.81034640074959}. Best is trial 34 with value: 0.03027766860861291.[0m
[32m[I 2025-02-03 12:24:36,749][0m Trial 36 finished with value: 0.04184100654261197 and parameters: {'observation_period_num': 67, 'train_rates': 0.7461990246927832, 'learning_rate': 0.00011522030895740549, 'batch_size': 58, 'step_size': 7, 'gamma': 0.8524851148627598}. Best is trial 34 with value: 0.03027766860861291.[0m
[32m[I 2025-02-03 12:27:36,557][0m Trial 37 finished with value: 0.02812399019329053 and parameters: {'observation_period_num': 20, 'train_rates': 0.8555146271343284, 'learning_rate': 0.000231718799466333, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8375956949206944}. Best is trial 37 with value: 0.02812399019329053.[0m
[32m[I 2025-02-03 12:30:18,153][0m Trial 38 finished with value: 0.04080736286265692 and parameters: {'observation_period_num': 48, 'train_rates': 0.8552782018444791, 'learning_rate': 3.997850389476843e-05, 'batch_size': 33, 'step_size': 8, 'gamma': 0.8359621237686884}. Best is trial 37 with value: 0.02812399019329053.[0m
[32m[I 2025-02-03 12:31:28,223][0m Trial 39 finished with value: 0.036838670855323816 and parameters: {'observation_period_num': 22, 'train_rates': 0.8814880348174065, 'learning_rate': 0.00023412641154000333, 'batch_size': 81, 'step_size': 6, 'gamma': 0.7697850896968317}. Best is trial 37 with value: 0.02812399019329053.[0m
[32m[I 2025-02-03 12:37:03,119][0m Trial 40 finished with value: 0.026737316824893387 and parameters: {'observation_period_num': 18, 'train_rates': 0.853118950782639, 'learning_rate': 0.00034493773750003876, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8118386944677493}. Best is trial 40 with value: 0.026737316824893387.[0m
[32m[I 2025-02-03 12:42:32,131][0m Trial 41 finished with value: 0.030330287534704497 and parameters: {'observation_period_num': 20, 'train_rates': 0.8374149230505512, 'learning_rate': 0.0003426171440126586, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8441683973605166}. Best is trial 40 with value: 0.026737316824893387.[0m
[32m[I 2025-02-03 12:45:17,879][0m Trial 42 finished with value: 0.025221747272184205 and parameters: {'observation_period_num': 17, 'train_rates': 0.8373935328129007, 'learning_rate': 0.00012186980809171436, 'batch_size': 32, 'step_size': 8, 'gamma': 0.8123985649150254}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:46:49,493][0m Trial 43 finished with value: 0.03295933021833812 and parameters: {'observation_period_num': 17, 'train_rates': 0.8271026727034662, 'learning_rate': 0.0001401160701484809, 'batch_size': 58, 'step_size': 8, 'gamma': 0.8136295749767573}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:49:14,426][0m Trial 44 finished with value: 0.039876566857131784 and parameters: {'observation_period_num': 21, 'train_rates': 0.8426533028063198, 'learning_rate': 7.571366023349876e-05, 'batch_size': 37, 'step_size': 5, 'gamma': 0.7988049227461865}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:52:06,157][0m Trial 45 finished with value: 0.044150425025939265 and parameters: {'observation_period_num': 18, 'train_rates': 0.7599897816243154, 'learning_rate': 0.00023057616462161598, 'batch_size': 29, 'step_size': 7, 'gamma': 0.8239134829396064}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:53:44,273][0m Trial 46 finished with value: 0.05663232521430866 and parameters: {'observation_period_num': 59, 'train_rates': 0.8082250637778773, 'learning_rate': 5.372937115569436e-05, 'batch_size': 52, 'step_size': 5, 'gamma': 0.7685410729329187}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:54:55,538][0m Trial 47 finished with value: 0.03238121319644719 and parameters: {'observation_period_num': 5, 'train_rates': 0.7857205731207885, 'learning_rate': 0.0003359969848167439, 'batch_size': 73, 'step_size': 8, 'gamma': 0.8428276040715257}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:56:49,652][0m Trial 48 finished with value: 0.06877191533325826 and parameters: {'observation_period_num': 179, 'train_rates': 0.7307503948316687, 'learning_rate': 0.00010874386685249976, 'batch_size': 41, 'step_size': 7, 'gamma': 0.8040939513302776}. Best is trial 42 with value: 0.025221747272184205.[0m
[32m[I 2025-02-03 12:57:43,386][0m Trial 49 finished with value: 0.05167520689449722 and parameters: {'observation_period_num': 45, 'train_rates': 0.8565293002493873, 'learning_rate': 0.00016265772728270959, 'batch_size': 112, 'step_size': 3, 'gamma': 0.7797401957037899}. Best is trial 42 with value: 0.025221747272184205.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-03 12:57:43,396][0m A new study created in memory with name: no-name-022ea4ba-7bb1-428e-8b28-fc820794690a[0m
[32m[I 2025-02-03 12:59:06,712][0m Trial 0 finished with value: 0.03544290669794593 and parameters: {'observation_period_num': 66, 'train_rates': 0.7962185987775182, 'learning_rate': 0.000539847393832774, 'batch_size': 63, 'step_size': 6, 'gamma': 0.8813496711353537}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:03:23,018][0m Trial 1 finished with value: 0.12199857165400728 and parameters: {'observation_period_num': 115, 'train_rates': 0.7719464173048719, 'learning_rate': 1.529994397319402e-05, 'batch_size': 19, 'step_size': 2, 'gamma': 0.8573660289213234}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:03:43,147][0m Trial 2 finished with value: 0.15680615391050065 and parameters: {'observation_period_num': 101, 'train_rates': 0.7128657571201147, 'learning_rate': 5.70671652518364e-05, 'batch_size': 255, 'step_size': 5, 'gamma': 0.7710649324159611}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:06:39,971][0m Trial 3 finished with value: 0.054214628240652596 and parameters: {'observation_period_num': 85, 'train_rates': 0.6989404186702495, 'learning_rate': 8.608023032851965e-05, 'batch_size': 26, 'step_size': 15, 'gamma': 0.8012234750510482}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:07:23,139][0m Trial 4 finished with value: 0.12023226171731949 and parameters: {'observation_period_num': 140, 'train_rates': 0.9688654747113579, 'learning_rate': 1.5153944819967456e-05, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8416513280093396}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:08:33,548][0m Trial 5 finished with value: 0.061803378126164175 and parameters: {'observation_period_num': 177, 'train_rates': 0.9306213888135572, 'learning_rate': 0.0005855737984273457, 'batch_size': 80, 'step_size': 4, 'gamma': 0.8745571565593359}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:08:58,220][0m Trial 6 finished with value: 0.1354247129434704 and parameters: {'observation_period_num': 205, 'train_rates': 0.6594638977820321, 'learning_rate': 0.00015215019218931076, 'batch_size': 196, 'step_size': 11, 'gamma': 0.8630031576851677}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:09:27,703][0m Trial 7 finished with value: 0.06485795802554163 and parameters: {'observation_period_num': 91, 'train_rates': 0.9170196286128889, 'learning_rate': 9.653945641921372e-05, 'batch_size': 211, 'step_size': 9, 'gamma': 0.7816666008581209}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:10:07,074][0m Trial 8 finished with value: 0.07298960260557852 and parameters: {'observation_period_num': 144, 'train_rates': 0.9078516320272577, 'learning_rate': 0.00013121437244998936, 'batch_size': 142, 'step_size': 1, 'gamma': 0.9605313392608675}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:10:45,419][0m Trial 9 finished with value: 0.16005763335592427 and parameters: {'observation_period_num': 61, 'train_rates': 0.8914908183169383, 'learning_rate': 1.0793137241518007e-05, 'batch_size': 153, 'step_size': 9, 'gamma': 0.7585192358347949}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:11:54,761][0m Trial 10 finished with value: 0.14212533225088012 and parameters: {'observation_period_num': 7, 'train_rates': 0.8211274035940619, 'learning_rate': 1.5836671925047945e-06, 'batch_size': 79, 'step_size': 6, 'gamma': 0.9429763320218447}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:14:53,605][0m Trial 11 finished with value: 0.06875947716305326 and parameters: {'observation_period_num': 41, 'train_rates': 0.6034916614576339, 'learning_rate': 0.0007217960916105174, 'batch_size': 24, 'step_size': 15, 'gamma': 0.9128682088990852}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:16:04,544][0m Trial 12 finished with value: 0.03673951283602391 and parameters: {'observation_period_num': 57, 'train_rates': 0.755469488160061, 'learning_rate': 0.00028047894056491845, 'batch_size': 72, 'step_size': 12, 'gamma': 0.8095543582852351}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:17:07,495][0m Trial 13 finished with value: 0.035936439709205716 and parameters: {'observation_period_num': 31, 'train_rates': 0.8083096850409828, 'learning_rate': 0.0003474329044818543, 'batch_size': 86, 'step_size': 11, 'gamma': 0.8182826219596989}. Best is trial 0 with value: 0.03544290669794593.[0m
[32m[I 2025-02-03 13:18:00,585][0m Trial 14 finished with value: 0.030835622897705972 and parameters: {'observation_period_num': 8, 'train_rates': 0.841007608956549, 'learning_rate': 0.0008736861937257821, 'batch_size': 105, 'step_size': 7, 'gamma': 0.9056870843434213}. Best is trial 14 with value: 0.030835622897705972.[0m
[32m[I 2025-02-03 13:18:53,058][0m Trial 15 finished with value: 0.03419352023283372 and parameters: {'observation_period_num': 11, 'train_rates': 0.8491160482378456, 'learning_rate': 0.0009423765047901649, 'batch_size': 110, 'step_size': 7, 'gamma': 0.9029195894655204}. Best is trial 14 with value: 0.030835622897705972.[0m
[32m[I 2025-02-03 13:19:46,814][0m Trial 16 finished with value: 0.028719557357956147 and parameters: {'observation_period_num': 6, 'train_rates': 0.8475705754032464, 'learning_rate': 0.0009233538779287182, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9065561078001826}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:20:34,151][0m Trial 17 finished with value: 0.2988322397445348 and parameters: {'observation_period_num': 224, 'train_rates': 0.843469253930075, 'learning_rate': 2.7153604690425257e-06, 'batch_size': 114, 'step_size': 8, 'gamma': 0.9893497669797876}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:21:09,046][0m Trial 18 finished with value: 0.038475265288437914 and parameters: {'observation_period_num': 30, 'train_rates': 0.8560914338290301, 'learning_rate': 0.0002375690667355857, 'batch_size': 172, 'step_size': 3, 'gamma': 0.915984525855183}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:22:02,894][0m Trial 19 finished with value: 0.09391362220048904 and parameters: {'observation_period_num': 174, 'train_rates': 0.9721413671210998, 'learning_rate': 3.366424398717567e-05, 'batch_size': 111, 'step_size': 9, 'gamma': 0.9358306154487848}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:24:01,028][0m Trial 20 finished with value: 0.058358042319751766 and parameters: {'observation_period_num': 6, 'train_rates': 0.8763192766157512, 'learning_rate': 6.260388047243935e-06, 'batch_size': 47, 'step_size': 11, 'gamma': 0.8990747158628709}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:24:57,936][0m Trial 21 finished with value: 0.029213773905732005 and parameters: {'observation_period_num': 7, 'train_rates': 0.8479185382905458, 'learning_rate': 0.0008319902368236381, 'batch_size': 103, 'step_size': 7, 'gamma': 0.8951106726561185}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:25:46,575][0m Trial 22 finished with value: 0.03488311035902176 and parameters: {'observation_period_num': 36, 'train_rates': 0.7528099236395387, 'learning_rate': 0.0004239479525264252, 'batch_size': 105, 'step_size': 7, 'gamma': 0.8878185423674603}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:26:33,943][0m Trial 23 finished with value: 0.032262990122478664 and parameters: {'observation_period_num': 26, 'train_rates': 0.8220720342468862, 'learning_rate': 0.0008635401055433541, 'batch_size': 125, 'step_size': 5, 'gamma': 0.9317310925005788}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:27:11,158][0m Trial 24 finished with value: 0.05551994530217988 and parameters: {'observation_period_num': 51, 'train_rates': 0.9399631764401827, 'learning_rate': 0.00022703841629680756, 'batch_size': 164, 'step_size': 8, 'gamma': 0.9615845766321015}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:28:05,818][0m Trial 25 finished with value: 0.0689775653119231 and parameters: {'observation_period_num': 251, 'train_rates': 0.8712901996207076, 'learning_rate': 0.00042546651719416054, 'batch_size': 95, 'step_size': 7, 'gamma': 0.8425315853025785}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:29:51,975][0m Trial 26 finished with value: 0.17488477199449895 and parameters: {'observation_period_num': 77, 'train_rates': 0.7850775261339936, 'learning_rate': 0.0008857228551913278, 'batch_size': 47, 'step_size': 10, 'gamma': 0.9223250061587804}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:30:35,046][0m Trial 27 finished with value: 0.04044598137441369 and parameters: {'observation_period_num': 22, 'train_rates': 0.8346703586214976, 'learning_rate': 0.0001866453205888022, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8924358214871866}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:32:15,878][0m Trial 28 finished with value: 0.04794319619418709 and parameters: {'observation_period_num': 45, 'train_rates': 0.8921570549424457, 'learning_rate': 0.0004275073136224818, 'batch_size': 57, 'step_size': 6, 'gamma': 0.9568833687756734}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:32:48,928][0m Trial 29 finished with value: 0.030900011667639116 and parameters: {'observation_period_num': 5, 'train_rates': 0.7970315210598009, 'learning_rate': 0.0005867122660158471, 'batch_size': 178, 'step_size': 13, 'gamma': 0.8798340087797699}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:33:40,142][0m Trial 30 finished with value: 0.1092222541229849 and parameters: {'observation_period_num': 66, 'train_rates': 0.7274141592924253, 'learning_rate': 3.3806534129069744e-05, 'batch_size': 96, 'step_size': 4, 'gamma': 0.8408447080101885}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:34:10,631][0m Trial 31 finished with value: 0.03142462111076648 and parameters: {'observation_period_num': 5, 'train_rates': 0.7969861038479094, 'learning_rate': 0.0005572072947171758, 'batch_size': 184, 'step_size': 8, 'gamma': 0.8747516343000036}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:34:36,773][0m Trial 32 finished with value: 0.03000667224062492 and parameters: {'observation_period_num': 22, 'train_rates': 0.7891207751974085, 'learning_rate': 0.0009794512586192969, 'batch_size': 229, 'step_size': 10, 'gamma': 0.8829883538686829}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:35:02,685][0m Trial 33 finished with value: 0.03270533973150385 and parameters: {'observation_period_num': 22, 'train_rates': 0.8640052325243216, 'learning_rate': 0.0009791183614863493, 'batch_size': 244, 'step_size': 10, 'gamma': 0.8541657483578762}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:35:26,523][0m Trial 34 finished with value: 0.049569398334337475 and parameters: {'observation_period_num': 111, 'train_rates': 0.7682767017534367, 'learning_rate': 0.00031423646739606236, 'batch_size': 225, 'step_size': 7, 'gamma': 0.900215617805857}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:36:03,667][0m Trial 35 finished with value: 0.030560092264136602 and parameters: {'observation_period_num': 18, 'train_rates': 0.8243797961765411, 'learning_rate': 0.0005833578833071081, 'batch_size': 153, 'step_size': 10, 'gamma': 0.9188150496840171}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:36:39,072][0m Trial 36 finished with value: 0.05917119969331675 and parameters: {'observation_period_num': 71, 'train_rates': 0.7789785283427488, 'learning_rate': 6.109108240521686e-05, 'batch_size': 151, 'step_size': 10, 'gamma': 0.9280092109672674}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:37:03,789][0m Trial 37 finished with value: 0.0451617463178224 and parameters: {'observation_period_num': 50, 'train_rates': 0.7318331213185688, 'learning_rate': 0.000471556063608017, 'batch_size': 228, 'step_size': 12, 'gamma': 0.9450829897711814}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:37:30,623][0m Trial 38 finished with value: 0.033779729166181764 and parameters: {'observation_period_num': 22, 'train_rates': 0.7007370677177731, 'learning_rate': 0.000595282321224076, 'batch_size': 200, 'step_size': 9, 'gamma': 0.8639488850515261}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:38:15,783][0m Trial 39 finished with value: 0.04037750226014586 and parameters: {'observation_period_num': 38, 'train_rates': 0.8161700913936432, 'learning_rate': 0.00011343254012456386, 'batch_size': 124, 'step_size': 13, 'gamma': 0.8855708056213039}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:38:41,154][0m Trial 40 finished with value: 0.05038204358313411 and parameters: {'observation_period_num': 94, 'train_rates': 0.8905603212000721, 'learning_rate': 0.0001663882050969303, 'batch_size': 245, 'step_size': 10, 'gamma': 0.9808226166940697}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:39:39,693][0m Trial 41 finished with value: 0.03229820022336495 and parameters: {'observation_period_num': 19, 'train_rates': 0.8345087009668861, 'learning_rate': 0.0006856390985534511, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9118563908003908}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:40:24,795][0m Trial 42 finished with value: 0.028756027683517 and parameters: {'observation_period_num': 16, 'train_rates': 0.9487807351825519, 'learning_rate': 0.0009776422406754482, 'batch_size': 138, 'step_size': 6, 'gamma': 0.9075955314234109}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:41:04,143][0m Trial 43 finished with value: 0.050434221748751826 and parameters: {'observation_period_num': 135, 'train_rates': 0.9299244761236333, 'learning_rate': 0.0003605120418297075, 'batch_size': 148, 'step_size': 6, 'gamma': 0.8914044813866084}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:41:51,887][0m Trial 44 finished with value: 0.029325922857969998 and parameters: {'observation_period_num': 17, 'train_rates': 0.9456728282029143, 'learning_rate': 0.0006694154739587592, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9180127089322622}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:42:37,758][0m Trial 45 finished with value: 0.04555236175656319 and parameters: {'observation_period_num': 41, 'train_rates': 0.9563440989407684, 'learning_rate': 0.0006647205794651982, 'batch_size': 139, 'step_size': 3, 'gamma': 0.8693046271471275}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:44:07,976][0m Trial 46 finished with value: 0.07940594106912613 and parameters: {'observation_period_num': 56, 'train_rates': 0.9884417176739823, 'learning_rate': 7.456898819071824e-05, 'batch_size': 68, 'step_size': 4, 'gamma': 0.8488784174707571}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:44:59,049][0m Trial 47 finished with value: 0.03592152211496599 and parameters: {'observation_period_num': 33, 'train_rates': 0.9080293276639829, 'learning_rate': 0.0002856301399150374, 'batch_size': 119, 'step_size': 5, 'gamma': 0.9444279024946131}. Best is trial 16 with value: 0.028719557357956147.[0m
Early stopping at epoch 78
[32m[I 2025-02-03 13:45:22,851][0m Trial 48 finished with value: 0.18393507611255394 and parameters: {'observation_period_num': 166, 'train_rates': 0.6723577728471289, 'learning_rate': 0.0009967898145702052, 'batch_size': 163, 'step_size': 1, 'gamma': 0.8258051809959128}. Best is trial 16 with value: 0.028719557357956147.[0m
[32m[I 2025-02-03 13:46:09,857][0m Trial 49 finished with value: 0.05380629375576973 and parameters: {'observation_period_num': 81, 'train_rates': 0.9594038520860895, 'learning_rate': 0.0007318644379468147, 'batch_size': 134, 'step_size': 6, 'gamma': 0.9086921425510447}. Best is trial 16 with value: 0.028719557357956147.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_BA_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.8741768762356196, 'learning_rate': 0.0009840876918124912, 'batch_size': 232, 'step_size': 2, 'gamma': 0.9717662628121764}
Epoch 1/300, trend Loss: 0.8877 | 0.1756
Epoch 2/300, trend Loss: 0.2879 | 0.1830
Epoch 3/300, trend Loss: 0.1869 | 0.1121
Epoch 4/300, trend Loss: 0.2180 | 0.1615
Epoch 5/300, trend Loss: 0.1868 | 0.0878
Epoch 6/300, trend Loss: 0.1270 | 0.0969
Epoch 7/300, trend Loss: 0.1294 | 0.0677
Epoch 8/300, trend Loss: 0.1051 | 0.0644
Epoch 9/300, trend Loss: 0.1243 | 0.1184
Epoch 10/300, trend Loss: 0.1045 | 0.0642
Epoch 11/300, trend Loss: 0.0839 | 0.0575
Epoch 12/300, trend Loss: 0.0817 | 0.0688
Epoch 13/300, trend Loss: 0.0798 | 0.0773
Epoch 14/300, trend Loss: 0.0687 | 0.0643
Epoch 15/300, trend Loss: 0.0662 | 0.0630
Epoch 16/300, trend Loss: 0.0625 | 0.0590
Epoch 17/300, trend Loss: 0.0592 | 0.0537
Epoch 18/300, trend Loss: 0.0567 | 0.0554
Epoch 19/300, trend Loss: 0.0549 | 0.0557
Epoch 20/300, trend Loss: 0.0530 | 0.0519
Epoch 21/300, trend Loss: 0.0515 | 0.0496
Epoch 22/300, trend Loss: 0.0503 | 0.0488
Epoch 23/300, trend Loss: 0.0492 | 0.0474
Epoch 24/300, trend Loss: 0.0484 | 0.0466
Epoch 25/300, trend Loss: 0.0476 | 0.0463
Epoch 26/300, trend Loss: 0.0470 | 0.0450
Epoch 27/300, trend Loss: 0.0462 | 0.0452
Epoch 28/300, trend Loss: 0.0454 | 0.0436
Epoch 29/300, trend Loss: 0.0449 | 0.0436
Epoch 30/300, trend Loss: 0.0442 | 0.0435
Epoch 31/300, trend Loss: 0.0462 | 0.0465
Epoch 32/300, trend Loss: 0.0475 | 0.0433
Epoch 33/300, trend Loss: 0.0479 | 0.0484
Epoch 34/300, trend Loss: 0.0461 | 0.0433
Epoch 35/300, trend Loss: 0.0455 | 0.0420
Epoch 36/300, trend Loss: 0.0434 | 0.0419
Epoch 37/300, trend Loss: 0.0424 | 0.0417
Epoch 38/300, trend Loss: 0.0444 | 0.0478
Epoch 39/300, trend Loss: 0.0485 | 0.0472
Epoch 40/300, trend Loss: 0.0446 | 0.0417
Epoch 41/300, trend Loss: 0.0418 | 0.0408
Epoch 42/300, trend Loss: 0.0414 | 0.0398
Epoch 43/300, trend Loss: 0.0406 | 0.0392
Epoch 44/300, trend Loss: 0.0398 | 0.0392
Epoch 45/300, trend Loss: 0.0394 | 0.0395
Epoch 46/300, trend Loss: 0.0394 | 0.0393
Epoch 47/300, trend Loss: 0.0391 | 0.0389
Epoch 48/300, trend Loss: 0.0389 | 0.0385
Epoch 49/300, trend Loss: 0.0388 | 0.0385
Epoch 50/300, trend Loss: 0.0390 | 0.0387
Epoch 51/300, trend Loss: 0.0392 | 0.0395
Epoch 52/300, trend Loss: 0.0396 | 0.0393
Epoch 53/300, trend Loss: 0.0393 | 0.0395
Epoch 54/300, trend Loss: 0.0389 | 0.0390
Epoch 55/300, trend Loss: 0.0385 | 0.0389
Epoch 56/300, trend Loss: 0.0383 | 0.0384
Epoch 57/300, trend Loss: 0.0381 | 0.0386
Epoch 58/300, trend Loss: 0.0382 | 0.0385
Epoch 59/300, trend Loss: 0.0379 | 0.0386
Epoch 60/300, trend Loss: 0.0381 | 0.0385
Epoch 61/300, trend Loss: 0.0377 | 0.0386
Epoch 62/300, trend Loss: 0.0378 | 0.0383
Epoch 63/300, trend Loss: 0.0375 | 0.0384
Epoch 64/300, trend Loss: 0.0376 | 0.0380
Epoch 65/300, trend Loss: 0.0373 | 0.0381
Epoch 66/300, trend Loss: 0.0373 | 0.0378
Epoch 67/300, trend Loss: 0.0370 | 0.0379
Epoch 68/300, trend Loss: 0.0371 | 0.0377
Epoch 69/300, trend Loss: 0.0368 | 0.0378
Epoch 70/300, trend Loss: 0.0368 | 0.0375
Epoch 71/300, trend Loss: 0.0366 | 0.0376
Epoch 72/300, trend Loss: 0.0366 | 0.0374
Epoch 73/300, trend Loss: 0.0364 | 0.0374
Epoch 74/300, trend Loss: 0.0364 | 0.0373
Epoch 75/300, trend Loss: 0.0363 | 0.0373
Epoch 76/300, trend Loss: 0.0363 | 0.0372
Epoch 77/300, trend Loss: 0.0362 | 0.0372
Epoch 78/300, trend Loss: 0.0361 | 0.0371
Epoch 79/300, trend Loss: 0.0360 | 0.0371
Epoch 80/300, trend Loss: 0.0360 | 0.0370
Epoch 81/300, trend Loss: 0.0359 | 0.0370
Epoch 82/300, trend Loss: 0.0359 | 0.0370
Epoch 83/300, trend Loss: 0.0358 | 0.0370
Epoch 84/300, trend Loss: 0.0357 | 0.0369
Epoch 85/300, trend Loss: 0.0357 | 0.0369
Epoch 86/300, trend Loss: 0.0356 | 0.0368
Epoch 87/300, trend Loss: 0.0356 | 0.0368
Epoch 88/300, trend Loss: 0.0355 | 0.0368
Epoch 89/300, trend Loss: 0.0355 | 0.0367
Epoch 90/300, trend Loss: 0.0354 | 0.0367
Epoch 91/300, trend Loss: 0.0353 | 0.0367
Epoch 92/300, trend Loss: 0.0353 | 0.0366
Epoch 93/300, trend Loss: 0.0352 | 0.0366
Epoch 94/300, trend Loss: 0.0352 | 0.0366
Epoch 95/300, trend Loss: 0.0351 | 0.0365
Epoch 96/300, trend Loss: 0.0351 | 0.0365
Epoch 97/300, trend Loss: 0.0350 | 0.0365
Epoch 98/300, trend Loss: 0.0350 | 0.0364
Epoch 99/300, trend Loss: 0.0349 | 0.0364
Epoch 100/300, trend Loss: 0.0349 | 0.0364
Epoch 101/300, trend Loss: 0.0348 | 0.0363
Epoch 102/300, trend Loss: 0.0348 | 0.0363
Epoch 103/300, trend Loss: 0.0347 | 0.0362
Epoch 104/300, trend Loss: 0.0347 | 0.0362
Epoch 105/300, trend Loss: 0.0347 | 0.0362
Epoch 106/300, trend Loss: 0.0346 | 0.0361
Epoch 107/300, trend Loss: 0.0346 | 0.0361
Epoch 108/300, trend Loss: 0.0345 | 0.0361
Epoch 109/300, trend Loss: 0.0345 | 0.0361
Epoch 110/300, trend Loss: 0.0344 | 0.0360
Epoch 111/300, trend Loss: 0.0344 | 0.0360
Epoch 112/300, trend Loss: 0.0344 | 0.0360
Epoch 113/300, trend Loss: 0.0343 | 0.0359
Epoch 114/300, trend Loss: 0.0343 | 0.0359
Epoch 115/300, trend Loss: 0.0342 | 0.0359
Epoch 116/300, trend Loss: 0.0342 | 0.0358
Epoch 117/300, trend Loss: 0.0342 | 0.0358
Epoch 118/300, trend Loss: 0.0341 | 0.0358
Epoch 119/300, trend Loss: 0.0341 | 0.0358
Epoch 120/300, trend Loss: 0.0341 | 0.0357
Epoch 121/300, trend Loss: 0.0340 | 0.0357
Epoch 122/300, trend Loss: 0.0340 | 0.0357
Epoch 123/300, trend Loss: 0.0340 | 0.0356
Epoch 124/300, trend Loss: 0.0339 | 0.0356
Epoch 125/300, trend Loss: 0.0339 | 0.0356
Epoch 126/300, trend Loss: 0.0339 | 0.0356
Epoch 127/300, trend Loss: 0.0338 | 0.0355
Epoch 128/300, trend Loss: 0.0338 | 0.0355
Epoch 129/300, trend Loss: 0.0338 | 0.0355
Epoch 130/300, trend Loss: 0.0337 | 0.0355
Epoch 131/300, trend Loss: 0.0337 | 0.0354
Epoch 132/300, trend Loss: 0.0337 | 0.0354
Epoch 133/300, trend Loss: 0.0336 | 0.0354
Epoch 134/300, trend Loss: 0.0336 | 0.0354
Epoch 135/300, trend Loss: 0.0336 | 0.0354
Epoch 136/300, trend Loss: 0.0336 | 0.0353
Epoch 137/300, trend Loss: 0.0335 | 0.0353
Epoch 138/300, trend Loss: 0.0335 | 0.0353
Epoch 139/300, trend Loss: 0.0335 | 0.0353
Epoch 140/300, trend Loss: 0.0335 | 0.0353
Epoch 141/300, trend Loss: 0.0334 | 0.0352
Epoch 142/300, trend Loss: 0.0334 | 0.0352
Epoch 143/300, trend Loss: 0.0334 | 0.0352
Epoch 144/300, trend Loss: 0.0334 | 0.0352
Epoch 145/300, trend Loss: 0.0333 | 0.0352
Epoch 146/300, trend Loss: 0.0333 | 0.0351
Epoch 147/300, trend Loss: 0.0333 | 0.0351
Epoch 148/300, trend Loss: 0.0333 | 0.0351
Epoch 149/300, trend Loss: 0.0333 | 0.0351
Epoch 150/300, trend Loss: 0.0332 | 0.0351
Epoch 151/300, trend Loss: 0.0332 | 0.0351
Epoch 152/300, trend Loss: 0.0332 | 0.0350
Epoch 153/300, trend Loss: 0.0332 | 0.0350
Epoch 154/300, trend Loss: 0.0332 | 0.0350
Epoch 155/300, trend Loss: 0.0331 | 0.0350
Epoch 156/300, trend Loss: 0.0331 | 0.0350
Epoch 157/300, trend Loss: 0.0331 | 0.0350
Epoch 158/300, trend Loss: 0.0331 | 0.0349
Epoch 159/300, trend Loss: 0.0331 | 0.0349
Epoch 160/300, trend Loss: 0.0331 | 0.0349
Epoch 161/300, trend Loss: 0.0330 | 0.0349
Epoch 162/300, trend Loss: 0.0330 | 0.0349
Epoch 163/300, trend Loss: 0.0330 | 0.0349
Epoch 164/300, trend Loss: 0.0330 | 0.0349
Epoch 165/300, trend Loss: 0.0330 | 0.0349
Epoch 166/300, trend Loss: 0.0330 | 0.0348
Epoch 167/300, trend Loss: 0.0329 | 0.0348
Epoch 168/300, trend Loss: 0.0329 | 0.0348
Epoch 169/300, trend Loss: 0.0329 | 0.0348
Epoch 170/300, trend Loss: 0.0329 | 0.0348
Epoch 171/300, trend Loss: 0.0329 | 0.0348
Epoch 172/300, trend Loss: 0.0329 | 0.0348
Epoch 173/300, trend Loss: 0.0329 | 0.0348
Epoch 174/300, trend Loss: 0.0329 | 0.0348
Epoch 175/300, trend Loss: 0.0328 | 0.0347
Epoch 176/300, trend Loss: 0.0328 | 0.0347
Epoch 177/300, trend Loss: 0.0328 | 0.0347
Epoch 178/300, trend Loss: 0.0328 | 0.0347
Epoch 179/300, trend Loss: 0.0328 | 0.0347
Epoch 180/300, trend Loss: 0.0328 | 0.0347
Epoch 181/300, trend Loss: 0.0328 | 0.0347
Epoch 182/300, trend Loss: 0.0328 | 0.0347
Epoch 183/300, trend Loss: 0.0328 | 0.0347
Epoch 184/300, trend Loss: 0.0327 | 0.0347
Epoch 185/300, trend Loss: 0.0327 | 0.0347
Epoch 186/300, trend Loss: 0.0327 | 0.0346
Epoch 187/300, trend Loss: 0.0327 | 0.0346
Epoch 188/300, trend Loss: 0.0327 | 0.0346
Epoch 189/300, trend Loss: 0.0327 | 0.0346
Epoch 190/300, trend Loss: 0.0327 | 0.0346
Epoch 191/300, trend Loss: 0.0327 | 0.0346
Epoch 192/300, trend Loss: 0.0327 | 0.0346
Epoch 193/300, trend Loss: 0.0327 | 0.0346
Epoch 194/300, trend Loss: 0.0327 | 0.0346
Epoch 195/300, trend Loss: 0.0326 | 0.0346
Epoch 196/300, trend Loss: 0.0326 | 0.0346
Epoch 197/300, trend Loss: 0.0326 | 0.0346
Epoch 198/300, trend Loss: 0.0326 | 0.0346
Epoch 199/300, trend Loss: 0.0326 | 0.0346
Epoch 200/300, trend Loss: 0.0326 | 0.0346
Epoch 201/300, trend Loss: 0.0326 | 0.0345
Epoch 202/300, trend Loss: 0.0326 | 0.0345
Epoch 203/300, trend Loss: 0.0326 | 0.0345
Epoch 204/300, trend Loss: 0.0326 | 0.0345
Epoch 205/300, trend Loss: 0.0326 | 0.0345
Epoch 206/300, trend Loss: 0.0326 | 0.0345
Epoch 207/300, trend Loss: 0.0326 | 0.0345
Epoch 208/300, trend Loss: 0.0326 | 0.0345
Epoch 209/300, trend Loss: 0.0326 | 0.0345
Epoch 210/300, trend Loss: 0.0326 | 0.0345
Epoch 211/300, trend Loss: 0.0325 | 0.0345
Epoch 212/300, trend Loss: 0.0325 | 0.0345
Epoch 213/300, trend Loss: 0.0325 | 0.0345
Epoch 214/300, trend Loss: 0.0325 | 0.0345
Epoch 215/300, trend Loss: 0.0325 | 0.0345
Epoch 216/300, trend Loss: 0.0325 | 0.0345
Epoch 217/300, trend Loss: 0.0325 | 0.0345
Epoch 218/300, trend Loss: 0.0325 | 0.0345
Epoch 219/300, trend Loss: 0.0325 | 0.0345
Epoch 220/300, trend Loss: 0.0325 | 0.0345
Epoch 221/300, trend Loss: 0.0325 | 0.0345
Epoch 222/300, trend Loss: 0.0325 | 0.0345
Epoch 223/300, trend Loss: 0.0325 | 0.0345
Epoch 224/300, trend Loss: 0.0325 | 0.0344
Epoch 225/300, trend Loss: 0.0325 | 0.0344
Epoch 226/300, trend Loss: 0.0325 | 0.0344
Epoch 227/300, trend Loss: 0.0325 | 0.0344
Epoch 228/300, trend Loss: 0.0325 | 0.0344
Epoch 229/300, trend Loss: 0.0325 | 0.0344
Epoch 230/300, trend Loss: 0.0325 | 0.0344
Epoch 231/300, trend Loss: 0.0325 | 0.0344
Epoch 232/300, trend Loss: 0.0325 | 0.0344
Epoch 233/300, trend Loss: 0.0325 | 0.0344
Epoch 234/300, trend Loss: 0.0325 | 0.0344
Epoch 235/300, trend Loss: 0.0325 | 0.0344
Epoch 236/300, trend Loss: 0.0325 | 0.0344
Epoch 237/300, trend Loss: 0.0325 | 0.0344
Epoch 238/300, trend Loss: 0.0324 | 0.0344
Epoch 239/300, trend Loss: 0.0324 | 0.0344
Epoch 240/300, trend Loss: 0.0324 | 0.0344
Epoch 241/300, trend Loss: 0.0324 | 0.0344
Epoch 242/300, trend Loss: 0.0324 | 0.0344
Epoch 243/300, trend Loss: 0.0324 | 0.0344
Epoch 244/300, trend Loss: 0.0324 | 0.0344
Epoch 245/300, trend Loss: 0.0324 | 0.0344
Epoch 246/300, trend Loss: 0.0324 | 0.0344
Epoch 247/300, trend Loss: 0.0324 | 0.0344
Epoch 248/300, trend Loss: 0.0324 | 0.0344
Epoch 249/300, trend Loss: 0.0324 | 0.0344
Epoch 250/300, trend Loss: 0.0324 | 0.0344
Epoch 251/300, trend Loss: 0.0324 | 0.0344
Epoch 252/300, trend Loss: 0.0324 | 0.0344
Epoch 253/300, trend Loss: 0.0324 | 0.0344
Epoch 254/300, trend Loss: 0.0324 | 0.0344
Epoch 255/300, trend Loss: 0.0324 | 0.0344
Epoch 256/300, trend Loss: 0.0324 | 0.0344
Epoch 257/300, trend Loss: 0.0324 | 0.0344
Epoch 258/300, trend Loss: 0.0324 | 0.0344
Epoch 259/300, trend Loss: 0.0324 | 0.0344
Epoch 260/300, trend Loss: 0.0324 | 0.0344
Epoch 261/300, trend Loss: 0.0324 | 0.0344
Epoch 262/300, trend Loss: 0.0324 | 0.0344
Epoch 263/300, trend Loss: 0.0324 | 0.0344
Epoch 264/300, trend Loss: 0.0324 | 0.0344
Epoch 265/300, trend Loss: 0.0324 | 0.0344
Epoch 266/300, trend Loss: 0.0324 | 0.0344
Epoch 267/300, trend Loss: 0.0324 | 0.0344
Epoch 268/300, trend Loss: 0.0324 | 0.0344
Epoch 269/300, trend Loss: 0.0324 | 0.0344
Epoch 270/300, trend Loss: 0.0324 | 0.0344
Epoch 271/300, trend Loss: 0.0324 | 0.0344
Epoch 272/300, trend Loss: 0.0324 | 0.0344
Epoch 273/300, trend Loss: 0.0324 | 0.0344
Epoch 274/300, trend Loss: 0.0324 | 0.0344
Epoch 275/300, trend Loss: 0.0324 | 0.0344
Epoch 276/300, trend Loss: 0.0324 | 0.0344
Epoch 277/300, trend Loss: 0.0324 | 0.0344
Epoch 278/300, trend Loss: 0.0324 | 0.0344
Epoch 279/300, trend Loss: 0.0324 | 0.0344
Epoch 280/300, trend Loss: 0.0324 | 0.0344
Epoch 281/300, trend Loss: 0.0324 | 0.0344
Epoch 282/300, trend Loss: 0.0324 | 0.0344
Epoch 283/300, trend Loss: 0.0324 | 0.0344
Epoch 284/300, trend Loss: 0.0324 | 0.0344
Epoch 285/300, trend Loss: 0.0324 | 0.0343
Epoch 286/300, trend Loss: 0.0324 | 0.0343
Epoch 287/300, trend Loss: 0.0324 | 0.0343
Epoch 288/300, trend Loss: 0.0324 | 0.0343
Epoch 289/300, trend Loss: 0.0324 | 0.0343
Epoch 290/300, trend Loss: 0.0324 | 0.0343
Epoch 291/300, trend Loss: 0.0324 | 0.0343
Epoch 292/300, trend Loss: 0.0324 | 0.0343
Epoch 293/300, trend Loss: 0.0324 | 0.0343
Epoch 294/300, trend Loss: 0.0324 | 0.0343
Epoch 295/300, trend Loss: 0.0324 | 0.0343
Epoch 296/300, trend Loss: 0.0324 | 0.0343
Epoch 297/300, trend Loss: 0.0324 | 0.0343
Epoch 298/300, trend Loss: 0.0324 | 0.0343
Epoch 299/300, trend Loss: 0.0324 | 0.0343
Epoch 300/300, trend Loss: 0.0324 | 0.0343
Training seasonal_0 component with params: {'observation_period_num': 33, 'train_rates': 0.824282767891123, 'learning_rate': 0.0009792570248455216, 'batch_size': 179, 'step_size': 11, 'gamma': 0.7948027125184078}
Epoch 1/300, seasonal_0 Loss: 0.6738 | 0.5391
Epoch 2/300, seasonal_0 Loss: 0.2321 | 0.2387
Epoch 3/300, seasonal_0 Loss: 0.3132 | 0.2337
Epoch 4/300, seasonal_0 Loss: 0.2356 | 0.1344
Epoch 5/300, seasonal_0 Loss: 0.3079 | 0.1819
Epoch 6/300, seasonal_0 Loss: 0.2107 | 0.1141
Epoch 7/300, seasonal_0 Loss: 0.1796 | 0.1010
Epoch 8/300, seasonal_0 Loss: 0.1858 | 0.1002
Epoch 9/300, seasonal_0 Loss: 0.1634 | 0.0880
Epoch 10/300, seasonal_0 Loss: 0.1143 | 0.0873
Epoch 11/300, seasonal_0 Loss: 0.1019 | 0.0772
Epoch 12/300, seasonal_0 Loss: 0.0864 | 0.0613
Epoch 13/300, seasonal_0 Loss: 0.0860 | 0.0602
Epoch 14/300, seasonal_0 Loss: 0.0846 | 0.0624
Epoch 15/300, seasonal_0 Loss: 0.0825 | 0.0689
Epoch 16/300, seasonal_0 Loss: 0.0892 | 0.0718
Epoch 17/300, seasonal_0 Loss: 0.0780 | 0.0518
Epoch 18/300, seasonal_0 Loss: 0.0670 | 0.0494
Epoch 19/300, seasonal_0 Loss: 0.0605 | 0.0507
Epoch 20/300, seasonal_0 Loss: 0.0647 | 0.0506
Epoch 21/300, seasonal_0 Loss: 0.0614 | 0.0510
Epoch 22/300, seasonal_0 Loss: 0.0567 | 0.0564
Epoch 23/300, seasonal_0 Loss: 0.0551 | 0.0469
Epoch 24/300, seasonal_0 Loss: 0.0520 | 0.0455
Epoch 25/300, seasonal_0 Loss: 0.0541 | 0.0484
Epoch 26/300, seasonal_0 Loss: 0.0551 | 0.0574
Epoch 27/300, seasonal_0 Loss: 0.0524 | 0.0476
Epoch 28/300, seasonal_0 Loss: 0.0499 | 0.0460
Epoch 29/300, seasonal_0 Loss: 0.0484 | 0.0450
Epoch 30/300, seasonal_0 Loss: 0.0457 | 0.0433
Epoch 31/300, seasonal_0 Loss: 0.0469 | 0.0419
Epoch 32/300, seasonal_0 Loss: 0.0448 | 0.0418
Epoch 33/300, seasonal_0 Loss: 0.0473 | 0.0440
Epoch 34/300, seasonal_0 Loss: 0.0431 | 0.0424
Epoch 35/300, seasonal_0 Loss: 0.0424 | 0.0397
Epoch 36/300, seasonal_0 Loss: 0.0401 | 0.0391
Epoch 37/300, seasonal_0 Loss: 0.0396 | 0.0414
Epoch 38/300, seasonal_0 Loss: 0.0390 | 0.0399
Epoch 39/300, seasonal_0 Loss: 0.0386 | 0.0391
Epoch 40/300, seasonal_0 Loss: 0.0382 | 0.0392
Epoch 41/300, seasonal_0 Loss: 0.0378 | 0.0386
Epoch 42/300, seasonal_0 Loss: 0.0374 | 0.0383
Epoch 43/300, seasonal_0 Loss: 0.0371 | 0.0386
Epoch 44/300, seasonal_0 Loss: 0.0368 | 0.0381
Epoch 45/300, seasonal_0 Loss: 0.0366 | 0.0377
Epoch 46/300, seasonal_0 Loss: 0.0364 | 0.0376
Epoch 47/300, seasonal_0 Loss: 0.0361 | 0.0376
Epoch 48/300, seasonal_0 Loss: 0.0359 | 0.0374
Epoch 49/300, seasonal_0 Loss: 0.0356 | 0.0372
Epoch 50/300, seasonal_0 Loss: 0.0354 | 0.0370
Epoch 51/300, seasonal_0 Loss: 0.0353 | 0.0369
Epoch 52/300, seasonal_0 Loss: 0.0351 | 0.0368
Epoch 53/300, seasonal_0 Loss: 0.0348 | 0.0368
Epoch 54/300, seasonal_0 Loss: 0.0346 | 0.0366
Epoch 55/300, seasonal_0 Loss: 0.0344 | 0.0365
Epoch 56/300, seasonal_0 Loss: 0.0343 | 0.0363
Epoch 57/300, seasonal_0 Loss: 0.0342 | 0.0363
Epoch 58/300, seasonal_0 Loss: 0.0340 | 0.0362
Epoch 59/300, seasonal_0 Loss: 0.0338 | 0.0361
Epoch 60/300, seasonal_0 Loss: 0.0337 | 0.0360
Epoch 61/300, seasonal_0 Loss: 0.0335 | 0.0360
Epoch 62/300, seasonal_0 Loss: 0.0334 | 0.0359
Epoch 63/300, seasonal_0 Loss: 0.0332 | 0.0358
Epoch 64/300, seasonal_0 Loss: 0.0331 | 0.0357
Epoch 65/300, seasonal_0 Loss: 0.0330 | 0.0357
Epoch 66/300, seasonal_0 Loss: 0.0328 | 0.0356
Epoch 67/300, seasonal_0 Loss: 0.0327 | 0.0355
Epoch 68/300, seasonal_0 Loss: 0.0326 | 0.0355
Epoch 69/300, seasonal_0 Loss: 0.0325 | 0.0354
Epoch 70/300, seasonal_0 Loss: 0.0324 | 0.0354
Epoch 71/300, seasonal_0 Loss: 0.0323 | 0.0353
Epoch 72/300, seasonal_0 Loss: 0.0322 | 0.0353
Epoch 73/300, seasonal_0 Loss: 0.0321 | 0.0352
Epoch 74/300, seasonal_0 Loss: 0.0320 | 0.0352
Epoch 75/300, seasonal_0 Loss: 0.0319 | 0.0351
Epoch 76/300, seasonal_0 Loss: 0.0318 | 0.0351
Epoch 77/300, seasonal_0 Loss: 0.0317 | 0.0350
Epoch 78/300, seasonal_0 Loss: 0.0316 | 0.0350
Epoch 79/300, seasonal_0 Loss: 0.0315 | 0.0350
Epoch 80/300, seasonal_0 Loss: 0.0315 | 0.0350
Epoch 81/300, seasonal_0 Loss: 0.0314 | 0.0349
Epoch 82/300, seasonal_0 Loss: 0.0313 | 0.0349
Epoch 83/300, seasonal_0 Loss: 0.0313 | 0.0348
Epoch 84/300, seasonal_0 Loss: 0.0312 | 0.0348
Epoch 85/300, seasonal_0 Loss: 0.0311 | 0.0348
Epoch 86/300, seasonal_0 Loss: 0.0311 | 0.0348
Epoch 87/300, seasonal_0 Loss: 0.0310 | 0.0347
Epoch 88/300, seasonal_0 Loss: 0.0310 | 0.0347
Epoch 89/300, seasonal_0 Loss: 0.0309 | 0.0347
Epoch 90/300, seasonal_0 Loss: 0.0308 | 0.0347
Epoch 91/300, seasonal_0 Loss: 0.0308 | 0.0346
Epoch 92/300, seasonal_0 Loss: 0.0307 | 0.0346
Epoch 93/300, seasonal_0 Loss: 0.0307 | 0.0346
Epoch 94/300, seasonal_0 Loss: 0.0306 | 0.0346
Epoch 95/300, seasonal_0 Loss: 0.0306 | 0.0346
Epoch 96/300, seasonal_0 Loss: 0.0306 | 0.0345
Epoch 97/300, seasonal_0 Loss: 0.0305 | 0.0345
Epoch 98/300, seasonal_0 Loss: 0.0305 | 0.0345
Epoch 99/300, seasonal_0 Loss: 0.0304 | 0.0345
Epoch 100/300, seasonal_0 Loss: 0.0304 | 0.0345
Epoch 101/300, seasonal_0 Loss: 0.0304 | 0.0345
Epoch 102/300, seasonal_0 Loss: 0.0303 | 0.0344
Epoch 103/300, seasonal_0 Loss: 0.0303 | 0.0344
Epoch 104/300, seasonal_0 Loss: 0.0303 | 0.0344
Epoch 105/300, seasonal_0 Loss: 0.0302 | 0.0344
Epoch 106/300, seasonal_0 Loss: 0.0302 | 0.0344
Epoch 107/300, seasonal_0 Loss: 0.0302 | 0.0344
Epoch 108/300, seasonal_0 Loss: 0.0301 | 0.0344
Epoch 109/300, seasonal_0 Loss: 0.0301 | 0.0344
Epoch 110/300, seasonal_0 Loss: 0.0301 | 0.0343
Epoch 111/300, seasonal_0 Loss: 0.0301 | 0.0343
Epoch 112/300, seasonal_0 Loss: 0.0300 | 0.0343
Epoch 113/300, seasonal_0 Loss: 0.0300 | 0.0343
Epoch 114/300, seasonal_0 Loss: 0.0300 | 0.0343
Epoch 115/300, seasonal_0 Loss: 0.0300 | 0.0343
Epoch 116/300, seasonal_0 Loss: 0.0300 | 0.0343
Epoch 117/300, seasonal_0 Loss: 0.0299 | 0.0343
Epoch 118/300, seasonal_0 Loss: 0.0299 | 0.0343
Epoch 119/300, seasonal_0 Loss: 0.0299 | 0.0343
Epoch 120/300, seasonal_0 Loss: 0.0299 | 0.0343
Epoch 121/300, seasonal_0 Loss: 0.0299 | 0.0342
Epoch 122/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 123/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 124/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 125/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 126/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 127/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 128/300, seasonal_0 Loss: 0.0298 | 0.0342
Epoch 129/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 130/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 131/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 132/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 133/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 134/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 135/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 136/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 137/300, seasonal_0 Loss: 0.0297 | 0.0342
Epoch 138/300, seasonal_0 Loss: 0.0296 | 0.0342
Epoch 139/300, seasonal_0 Loss: 0.0296 | 0.0342
Epoch 140/300, seasonal_0 Loss: 0.0296 | 0.0342
Epoch 141/300, seasonal_0 Loss: 0.0296 | 0.0342
Epoch 142/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 143/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 144/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 145/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 146/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 147/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 148/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 149/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 150/300, seasonal_0 Loss: 0.0296 | 0.0341
Epoch 151/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 152/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 153/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 154/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 155/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 156/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 157/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 158/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 159/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 160/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 161/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 162/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 163/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 164/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 165/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 166/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 167/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 168/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 169/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 170/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 171/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 172/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 173/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 174/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 175/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 176/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 177/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 178/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 179/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 180/300, seasonal_0 Loss: 0.0295 | 0.0341
Epoch 181/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 182/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 183/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 184/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 185/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 186/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 187/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 188/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 189/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 190/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 191/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 192/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 193/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 194/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 195/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 196/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 197/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 198/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 199/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 200/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 201/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 202/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 203/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 204/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 205/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 206/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 207/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 208/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 209/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 210/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 211/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 212/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 213/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 214/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 215/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 216/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 217/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 218/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 219/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 220/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 221/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 222/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 223/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 224/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 225/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 226/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 227/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 228/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 229/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 230/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 231/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 232/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 233/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 234/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 235/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 236/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 237/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 238/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 239/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 240/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 241/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 242/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 243/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 244/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 245/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 246/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 247/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 248/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 249/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 250/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 251/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 252/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 253/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 254/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 255/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 256/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 257/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 258/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 259/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 260/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 261/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 262/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 263/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 264/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 265/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 266/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 267/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 268/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 269/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 270/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 271/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 272/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 273/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 274/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 275/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 276/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 277/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 278/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 279/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 280/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 281/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 282/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 283/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 284/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 285/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 286/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 287/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 288/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 289/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 290/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 291/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 292/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 293/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 294/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 295/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 296/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 297/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 298/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 299/300, seasonal_0 Loss: 0.0294 | 0.0341
Epoch 300/300, seasonal_0 Loss: 0.0294 | 0.0341
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 17, 'train_rates': 0.9070947808069317, 'learning_rate': 0.0009655436260558819, 'batch_size': 105, 'step_size': 7, 'gamma': 0.7744254542292559}
Epoch 1/300, seasonal_1 Loss: 0.3570 | 0.1496
Epoch 2/300, seasonal_1 Loss: 0.1690 | 0.1307
Epoch 3/300, seasonal_1 Loss: 0.1509 | 0.0755
Epoch 4/300, seasonal_1 Loss: 0.1328 | 0.1661
Epoch 5/300, seasonal_1 Loss: 0.1083 | 0.0684
Epoch 6/300, seasonal_1 Loss: 0.0880 | 0.1308
Epoch 7/300, seasonal_1 Loss: 0.0885 | 0.0912
Epoch 8/300, seasonal_1 Loss: 0.0775 | 0.0700
Epoch 9/300, seasonal_1 Loss: 0.0662 | 0.0596
Epoch 10/300, seasonal_1 Loss: 0.0591 | 0.0567
Epoch 11/300, seasonal_1 Loss: 0.0536 | 0.0530
Epoch 12/300, seasonal_1 Loss: 0.0497 | 0.0504
Epoch 13/300, seasonal_1 Loss: 0.0471 | 0.0473
Epoch 14/300, seasonal_1 Loss: 0.0448 | 0.0443
Epoch 15/300, seasonal_1 Loss: 0.0421 | 0.0430
Epoch 16/300, seasonal_1 Loss: 0.0409 | 0.0428
Epoch 17/300, seasonal_1 Loss: 0.0400 | 0.0410
Epoch 18/300, seasonal_1 Loss: 0.0389 | 0.0424
Epoch 19/300, seasonal_1 Loss: 0.0385 | 0.0407
Epoch 20/300, seasonal_1 Loss: 0.0384 | 0.0412
Epoch 21/300, seasonal_1 Loss: 0.0378 | 0.0395
Epoch 22/300, seasonal_1 Loss: 0.0368 | 0.0392
Epoch 23/300, seasonal_1 Loss: 0.0372 | 0.0387
Epoch 24/300, seasonal_1 Loss: 0.0367 | 0.0380
Epoch 25/300, seasonal_1 Loss: 0.0362 | 0.0376
Epoch 26/300, seasonal_1 Loss: 0.0362 | 0.0377
Epoch 27/300, seasonal_1 Loss: 0.0383 | 0.0397
Epoch 28/300, seasonal_1 Loss: 0.0393 | 0.0407
Epoch 29/300, seasonal_1 Loss: 0.0446 | 0.0386
Epoch 30/300, seasonal_1 Loss: 0.0471 | 0.0479
Epoch 31/300, seasonal_1 Loss: 0.0519 | 0.0413
Epoch 32/300, seasonal_1 Loss: 0.0479 | 0.0471
Epoch 33/300, seasonal_1 Loss: 0.0434 | 0.0435
Epoch 34/300, seasonal_1 Loss: 0.0380 | 0.0353
Epoch 35/300, seasonal_1 Loss: 0.0366 | 0.0349
Epoch 36/300, seasonal_1 Loss: 0.0354 | 0.0366
Epoch 37/300, seasonal_1 Loss: 0.0352 | 0.0355
Epoch 38/300, seasonal_1 Loss: 0.0344 | 0.0356
Epoch 39/300, seasonal_1 Loss: 0.0335 | 0.0350
Epoch 40/300, seasonal_1 Loss: 0.0329 | 0.0350
Epoch 41/300, seasonal_1 Loss: 0.0322 | 0.0343
Epoch 42/300, seasonal_1 Loss: 0.0315 | 0.0340
Epoch 43/300, seasonal_1 Loss: 0.0310 | 0.0334
Epoch 44/300, seasonal_1 Loss: 0.0308 | 0.0333
Epoch 45/300, seasonal_1 Loss: 0.0307 | 0.0331
Epoch 46/300, seasonal_1 Loss: 0.0305 | 0.0330
Epoch 47/300, seasonal_1 Loss: 0.0303 | 0.0329
Epoch 48/300, seasonal_1 Loss: 0.0301 | 0.0328
Epoch 49/300, seasonal_1 Loss: 0.0299 | 0.0327
Epoch 50/300, seasonal_1 Loss: 0.0297 | 0.0326
Epoch 51/300, seasonal_1 Loss: 0.0296 | 0.0325
Epoch 52/300, seasonal_1 Loss: 0.0295 | 0.0325
Epoch 53/300, seasonal_1 Loss: 0.0294 | 0.0324
Epoch 54/300, seasonal_1 Loss: 0.0292 | 0.0323
Epoch 55/300, seasonal_1 Loss: 0.0292 | 0.0323
Epoch 56/300, seasonal_1 Loss: 0.0291 | 0.0322
Epoch 57/300, seasonal_1 Loss: 0.0290 | 0.0322
Epoch 58/300, seasonal_1 Loss: 0.0289 | 0.0321
Epoch 59/300, seasonal_1 Loss: 0.0289 | 0.0321
Epoch 60/300, seasonal_1 Loss: 0.0288 | 0.0320
Epoch 61/300, seasonal_1 Loss: 0.0288 | 0.0320
Epoch 62/300, seasonal_1 Loss: 0.0287 | 0.0320
Epoch 63/300, seasonal_1 Loss: 0.0287 | 0.0319
Epoch 64/300, seasonal_1 Loss: 0.0286 | 0.0319
Epoch 65/300, seasonal_1 Loss: 0.0286 | 0.0319
Epoch 66/300, seasonal_1 Loss: 0.0286 | 0.0319
Epoch 67/300, seasonal_1 Loss: 0.0285 | 0.0318
Epoch 68/300, seasonal_1 Loss: 0.0285 | 0.0318
Epoch 69/300, seasonal_1 Loss: 0.0284 | 0.0318
Epoch 70/300, seasonal_1 Loss: 0.0284 | 0.0318
Epoch 71/300, seasonal_1 Loss: 0.0284 | 0.0317
Epoch 72/300, seasonal_1 Loss: 0.0284 | 0.0317
Epoch 73/300, seasonal_1 Loss: 0.0283 | 0.0317
Epoch 74/300, seasonal_1 Loss: 0.0283 | 0.0317
Epoch 75/300, seasonal_1 Loss: 0.0283 | 0.0317
Epoch 76/300, seasonal_1 Loss: 0.0283 | 0.0317
Epoch 77/300, seasonal_1 Loss: 0.0282 | 0.0317
Epoch 78/300, seasonal_1 Loss: 0.0282 | 0.0316
Epoch 79/300, seasonal_1 Loss: 0.0282 | 0.0316
Epoch 80/300, seasonal_1 Loss: 0.0282 | 0.0316
Epoch 81/300, seasonal_1 Loss: 0.0282 | 0.0316
Epoch 82/300, seasonal_1 Loss: 0.0281 | 0.0316
Epoch 83/300, seasonal_1 Loss: 0.0281 | 0.0316
Epoch 84/300, seasonal_1 Loss: 0.0281 | 0.0316
Epoch 85/300, seasonal_1 Loss: 0.0281 | 0.0316
Epoch 86/300, seasonal_1 Loss: 0.0281 | 0.0316
Epoch 87/300, seasonal_1 Loss: 0.0281 | 0.0316
Epoch 88/300, seasonal_1 Loss: 0.0281 | 0.0315
Epoch 89/300, seasonal_1 Loss: 0.0281 | 0.0315
Epoch 90/300, seasonal_1 Loss: 0.0281 | 0.0315
Epoch 91/300, seasonal_1 Loss: 0.0281 | 0.0315
Epoch 92/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 93/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 94/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 95/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 96/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 97/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 98/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 99/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 100/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 101/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 102/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 103/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 104/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 105/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 106/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 107/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 108/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 109/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 110/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 111/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 112/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 113/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 114/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 115/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 116/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 117/300, seasonal_1 Loss: 0.0280 | 0.0315
Epoch 118/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 119/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 120/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 121/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 122/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 123/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 124/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 125/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 126/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 127/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 128/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 129/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 130/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 131/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 132/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 133/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 134/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 135/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 136/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 137/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 138/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 139/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 140/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 141/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 142/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 143/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 144/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 145/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 146/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 147/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 148/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 149/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 150/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 151/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 152/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 153/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 154/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 155/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 156/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 157/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 158/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 159/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 160/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 161/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 162/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 163/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 164/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 165/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 166/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 167/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 168/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 169/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 170/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 171/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 172/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 173/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 174/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 175/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 176/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 177/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 178/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 179/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 180/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 181/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 182/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 183/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 184/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 185/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 186/300, seasonal_1 Loss: 0.0279 | 0.0315
Epoch 187/300, seasonal_1 Loss: 0.0279 | 0.0315
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 33, 'train_rates': 0.8694012310674082, 'learning_rate': 0.0009722044273311763, 'batch_size': 231, 'step_size': 11, 'gamma': 0.8336264200564073}
Epoch 1/300, seasonal_2 Loss: 0.8226 | 0.3450
Epoch 2/300, seasonal_2 Loss: 0.2777 | 0.2774
Epoch 3/300, seasonal_2 Loss: 0.4435 | 0.2290
Epoch 4/300, seasonal_2 Loss: 0.2610 | 0.1387
Epoch 5/300, seasonal_2 Loss: 0.1938 | 0.1547
Epoch 6/300, seasonal_2 Loss: 0.1765 | 0.1058
Epoch 7/300, seasonal_2 Loss: 0.1589 | 0.1311
Epoch 8/300, seasonal_2 Loss: 0.1061 | 0.0702
Epoch 9/300, seasonal_2 Loss: 0.0983 | 0.0630
Epoch 10/300, seasonal_2 Loss: 0.0923 | 0.0607
Epoch 11/300, seasonal_2 Loss: 0.0975 | 0.0619
Epoch 12/300, seasonal_2 Loss: 0.1047 | 0.0633
Epoch 13/300, seasonal_2 Loss: 0.0997 | 0.0853
Epoch 14/300, seasonal_2 Loss: 0.0789 | 0.0575
Epoch 15/300, seasonal_2 Loss: 0.0749 | 0.0539
Epoch 16/300, seasonal_2 Loss: 0.0712 | 0.0528
Epoch 17/300, seasonal_2 Loss: 0.0639 | 0.0480
Epoch 18/300, seasonal_2 Loss: 0.0603 | 0.0519
Epoch 19/300, seasonal_2 Loss: 0.0572 | 0.0544
Epoch 20/300, seasonal_2 Loss: 0.0554 | 0.0527
Epoch 21/300, seasonal_2 Loss: 0.0524 | 0.0535
Epoch 22/300, seasonal_2 Loss: 0.0572 | 0.0544
Epoch 23/300, seasonal_2 Loss: 0.0705 | 0.1031
Epoch 24/300, seasonal_2 Loss: 0.0755 | 0.0594
Epoch 25/300, seasonal_2 Loss: 0.0633 | 0.0511
Epoch 26/300, seasonal_2 Loss: 0.0518 | 0.0449
Epoch 27/300, seasonal_2 Loss: 0.0517 | 0.0487
Epoch 28/300, seasonal_2 Loss: 0.0484 | 0.0755
Epoch 29/300, seasonal_2 Loss: 0.0496 | 0.0553
Epoch 30/300, seasonal_2 Loss: 0.0502 | 0.0451
Epoch 31/300, seasonal_2 Loss: 0.0454 | 0.0437
Epoch 32/300, seasonal_2 Loss: 0.0477 | 0.0733
Epoch 33/300, seasonal_2 Loss: 0.0455 | 0.0434
Epoch 34/300, seasonal_2 Loss: 0.0448 | 0.0414
Epoch 35/300, seasonal_2 Loss: 0.0433 | 0.0584
Epoch 36/300, seasonal_2 Loss: 0.0433 | 0.0584
Epoch 37/300, seasonal_2 Loss: 0.0491 | 0.0515
Epoch 38/300, seasonal_2 Loss: 0.0429 | 0.0422
Epoch 39/300, seasonal_2 Loss: 0.0457 | 0.0844
Epoch 40/300, seasonal_2 Loss: 0.0481 | 0.0463
Epoch 41/300, seasonal_2 Loss: 0.0438 | 0.0542
Epoch 42/300, seasonal_2 Loss: 0.0419 | 0.0435
Epoch 43/300, seasonal_2 Loss: 0.0396 | 0.0433
Epoch 44/300, seasonal_2 Loss: 0.0397 | 0.0386
Epoch 45/300, seasonal_2 Loss: 0.0385 | 0.0537
Epoch 46/300, seasonal_2 Loss: 0.0376 | 0.0378
Epoch 47/300, seasonal_2 Loss: 0.0368 | 0.0446
Epoch 48/300, seasonal_2 Loss: 0.0363 | 0.0407
Epoch 49/300, seasonal_2 Loss: 0.0357 | 0.0381
Epoch 50/300, seasonal_2 Loss: 0.0355 | 0.0392
Epoch 51/300, seasonal_2 Loss: 0.0354 | 0.0417
Epoch 52/300, seasonal_2 Loss: 0.0350 | 0.0366
Epoch 53/300, seasonal_2 Loss: 0.0348 | 0.0432
Epoch 54/300, seasonal_2 Loss: 0.0347 | 0.0368
Epoch 55/300, seasonal_2 Loss: 0.0342 | 0.0388
Epoch 56/300, seasonal_2 Loss: 0.0340 | 0.0380
Epoch 57/300, seasonal_2 Loss: 0.0338 | 0.0379
Epoch 58/300, seasonal_2 Loss: 0.0337 | 0.0369
Epoch 59/300, seasonal_2 Loss: 0.0336 | 0.0392
Epoch 60/300, seasonal_2 Loss: 0.0334 | 0.0359
Epoch 61/300, seasonal_2 Loss: 0.0332 | 0.0395
Epoch 62/300, seasonal_2 Loss: 0.0331 | 0.0358
Epoch 63/300, seasonal_2 Loss: 0.0329 | 0.0386
Epoch 64/300, seasonal_2 Loss: 0.0328 | 0.0355
Epoch 65/300, seasonal_2 Loss: 0.0327 | 0.0393
Epoch 66/300, seasonal_2 Loss: 0.0327 | 0.0351
Epoch 67/300, seasonal_2 Loss: 0.0326 | 0.0397
Epoch 68/300, seasonal_2 Loss: 0.0326 | 0.0349
Epoch 69/300, seasonal_2 Loss: 0.0327 | 0.0424
Epoch 70/300, seasonal_2 Loss: 0.0333 | 0.0354
Epoch 71/300, seasonal_2 Loss: 0.0335 | 0.0491
Epoch 72/300, seasonal_2 Loss: 0.0354 | 0.0376
Epoch 73/300, seasonal_2 Loss: 0.0352 | 0.0523
Epoch 74/300, seasonal_2 Loss: 0.0390 | 0.0356
Epoch 75/300, seasonal_2 Loss: 0.0374 | 0.0510
Epoch 76/300, seasonal_2 Loss: 0.0437 | 0.0371
Epoch 77/300, seasonal_2 Loss: 0.0373 | 0.0401
Epoch 78/300, seasonal_2 Loss: 0.0379 | 0.0381
Epoch 79/300, seasonal_2 Loss: 0.0340 | 0.0367
Epoch 80/300, seasonal_2 Loss: 0.0333 | 0.0369
Epoch 81/300, seasonal_2 Loss: 0.0322 | 0.0371
Epoch 82/300, seasonal_2 Loss: 0.0320 | 0.0361
Epoch 83/300, seasonal_2 Loss: 0.0316 | 0.0371
Epoch 84/300, seasonal_2 Loss: 0.0315 | 0.0362
Epoch 85/300, seasonal_2 Loss: 0.0312 | 0.0360
Epoch 86/300, seasonal_2 Loss: 0.0310 | 0.0360
Epoch 87/300, seasonal_2 Loss: 0.0309 | 0.0358
Epoch 88/300, seasonal_2 Loss: 0.0308 | 0.0357
Epoch 89/300, seasonal_2 Loss: 0.0307 | 0.0356
Epoch 90/300, seasonal_2 Loss: 0.0306 | 0.0357
Epoch 91/300, seasonal_2 Loss: 0.0306 | 0.0354
Epoch 92/300, seasonal_2 Loss: 0.0305 | 0.0355
Epoch 93/300, seasonal_2 Loss: 0.0304 | 0.0354
Epoch 94/300, seasonal_2 Loss: 0.0304 | 0.0353
Epoch 95/300, seasonal_2 Loss: 0.0303 | 0.0352
Epoch 96/300, seasonal_2 Loss: 0.0303 | 0.0352
Epoch 97/300, seasonal_2 Loss: 0.0302 | 0.0352
Epoch 98/300, seasonal_2 Loss: 0.0302 | 0.0351
Epoch 99/300, seasonal_2 Loss: 0.0301 | 0.0351
Epoch 100/300, seasonal_2 Loss: 0.0301 | 0.0350
Epoch 101/300, seasonal_2 Loss: 0.0300 | 0.0350
Epoch 102/300, seasonal_2 Loss: 0.0300 | 0.0350
Epoch 103/300, seasonal_2 Loss: 0.0299 | 0.0349
Epoch 104/300, seasonal_2 Loss: 0.0299 | 0.0349
Epoch 105/300, seasonal_2 Loss: 0.0299 | 0.0349
Epoch 106/300, seasonal_2 Loss: 0.0298 | 0.0348
Epoch 107/300, seasonal_2 Loss: 0.0298 | 0.0348
Epoch 108/300, seasonal_2 Loss: 0.0297 | 0.0348
Epoch 109/300, seasonal_2 Loss: 0.0297 | 0.0347
Epoch 110/300, seasonal_2 Loss: 0.0297 | 0.0347
Epoch 111/300, seasonal_2 Loss: 0.0296 | 0.0347
Epoch 112/300, seasonal_2 Loss: 0.0296 | 0.0347
Epoch 113/300, seasonal_2 Loss: 0.0296 | 0.0346
Epoch 114/300, seasonal_2 Loss: 0.0295 | 0.0346
Epoch 115/300, seasonal_2 Loss: 0.0295 | 0.0346
Epoch 116/300, seasonal_2 Loss: 0.0295 | 0.0346
Epoch 117/300, seasonal_2 Loss: 0.0294 | 0.0346
Epoch 118/300, seasonal_2 Loss: 0.0294 | 0.0345
Epoch 119/300, seasonal_2 Loss: 0.0294 | 0.0345
Epoch 120/300, seasonal_2 Loss: 0.0294 | 0.0345
Epoch 121/300, seasonal_2 Loss: 0.0293 | 0.0345
Epoch 122/300, seasonal_2 Loss: 0.0293 | 0.0345
Epoch 123/300, seasonal_2 Loss: 0.0293 | 0.0344
Epoch 124/300, seasonal_2 Loss: 0.0293 | 0.0344
Epoch 125/300, seasonal_2 Loss: 0.0292 | 0.0344
Epoch 126/300, seasonal_2 Loss: 0.0292 | 0.0344
Epoch 127/300, seasonal_2 Loss: 0.0292 | 0.0344
Epoch 128/300, seasonal_2 Loss: 0.0292 | 0.0344
Epoch 129/300, seasonal_2 Loss: 0.0292 | 0.0344
Epoch 130/300, seasonal_2 Loss: 0.0291 | 0.0344
Epoch 131/300, seasonal_2 Loss: 0.0291 | 0.0343
Epoch 132/300, seasonal_2 Loss: 0.0291 | 0.0343
Epoch 133/300, seasonal_2 Loss: 0.0291 | 0.0343
Epoch 134/300, seasonal_2 Loss: 0.0291 | 0.0343
Epoch 135/300, seasonal_2 Loss: 0.0290 | 0.0343
Epoch 136/300, seasonal_2 Loss: 0.0290 | 0.0343
Epoch 137/300, seasonal_2 Loss: 0.0290 | 0.0343
Epoch 138/300, seasonal_2 Loss: 0.0290 | 0.0343
Epoch 139/300, seasonal_2 Loss: 0.0290 | 0.0343
Epoch 140/300, seasonal_2 Loss: 0.0290 | 0.0342
Epoch 141/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 142/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 143/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 144/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 145/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 146/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 147/300, seasonal_2 Loss: 0.0289 | 0.0342
Epoch 148/300, seasonal_2 Loss: 0.0288 | 0.0342
Epoch 149/300, seasonal_2 Loss: 0.0288 | 0.0342
Epoch 150/300, seasonal_2 Loss: 0.0288 | 0.0342
Epoch 151/300, seasonal_2 Loss: 0.0288 | 0.0342
Epoch 152/300, seasonal_2 Loss: 0.0288 | 0.0342
Epoch 153/300, seasonal_2 Loss: 0.0288 | 0.0341
Epoch 154/300, seasonal_2 Loss: 0.0288 | 0.0341
Epoch 155/300, seasonal_2 Loss: 0.0288 | 0.0341
Epoch 156/300, seasonal_2 Loss: 0.0288 | 0.0341
Epoch 157/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 158/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 159/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 160/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 161/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 162/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 163/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 164/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 165/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 166/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 167/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 168/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 169/300, seasonal_2 Loss: 0.0287 | 0.0341
Epoch 170/300, seasonal_2 Loss: 0.0286 | 0.0341
Epoch 171/300, seasonal_2 Loss: 0.0286 | 0.0341
Epoch 172/300, seasonal_2 Loss: 0.0286 | 0.0341
Epoch 173/300, seasonal_2 Loss: 0.0286 | 0.0341
Epoch 174/300, seasonal_2 Loss: 0.0286 | 0.0341
Epoch 175/300, seasonal_2 Loss: 0.0286 | 0.0341
Epoch 176/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 177/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 178/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 179/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 180/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 181/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 182/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 183/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 184/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 185/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 186/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 187/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 188/300, seasonal_2 Loss: 0.0286 | 0.0340
Epoch 189/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 190/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 191/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 192/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 193/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 194/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 195/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 196/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 197/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 198/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 199/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 200/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 201/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 202/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 203/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 204/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 205/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 206/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 207/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 208/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 209/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 210/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 211/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 212/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 213/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 214/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 215/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 216/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 217/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 218/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 219/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 220/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 221/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 222/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 223/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 224/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 225/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 226/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 227/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 228/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 229/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 230/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 231/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 232/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 233/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 234/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 235/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 236/300, seasonal_2 Loss: 0.0285 | 0.0340
Epoch 237/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 238/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 239/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 240/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 241/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 242/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 243/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 244/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 245/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 246/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 247/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 248/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 249/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 250/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 251/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 252/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 253/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 254/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 255/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 256/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 257/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 258/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 259/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 260/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 261/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 262/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 263/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 264/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 265/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 266/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 267/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 268/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 269/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 270/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 271/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 272/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 273/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 274/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 275/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 276/300, seasonal_2 Loss: 0.0284 | 0.0340
Epoch 277/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 278/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 279/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 280/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 281/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 282/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 283/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 284/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 285/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 286/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 287/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 288/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 289/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 290/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 291/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 292/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 293/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 294/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 295/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 296/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 297/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 298/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 299/300, seasonal_2 Loss: 0.0284 | 0.0339
Epoch 300/300, seasonal_2 Loss: 0.0284 | 0.0339
Training seasonal_3 component with params: {'observation_period_num': 17, 'train_rates': 0.8373935328129007, 'learning_rate': 0.00012186980809171436, 'batch_size': 32, 'step_size': 8, 'gamma': 0.8123985649150254}
Epoch 1/300, seasonal_3 Loss: 0.3181 | 0.1359
Epoch 2/300, seasonal_3 Loss: 0.1359 | 0.1145
Epoch 3/300, seasonal_3 Loss: 0.1065 | 0.0868
Epoch 4/300, seasonal_3 Loss: 0.0924 | 0.0790
Epoch 5/300, seasonal_3 Loss: 0.0815 | 0.0826
Epoch 6/300, seasonal_3 Loss: 0.0758 | 0.0761
Epoch 7/300, seasonal_3 Loss: 0.0709 | 0.0726
Epoch 8/300, seasonal_3 Loss: 0.0667 | 0.0706
Epoch 9/300, seasonal_3 Loss: 0.0626 | 0.0678
Epoch 10/300, seasonal_3 Loss: 0.0602 | 0.0630
Epoch 11/300, seasonal_3 Loss: 0.0573 | 0.0590
Epoch 12/300, seasonal_3 Loss: 0.0548 | 0.0560
Epoch 13/300, seasonal_3 Loss: 0.0530 | 0.0496
Epoch 14/300, seasonal_3 Loss: 0.0519 | 0.0485
Epoch 15/300, seasonal_3 Loss: 0.0508 | 0.0480
Epoch 16/300, seasonal_3 Loss: 0.0499 | 0.0480
Epoch 17/300, seasonal_3 Loss: 0.0492 | 0.0487
Epoch 18/300, seasonal_3 Loss: 0.0487 | 0.0491
Epoch 19/300, seasonal_3 Loss: 0.0478 | 0.0488
Epoch 20/300, seasonal_3 Loss: 0.0467 | 0.0478
Epoch 21/300, seasonal_3 Loss: 0.0456 | 0.0457
Epoch 22/300, seasonal_3 Loss: 0.0452 | 0.0447
Epoch 23/300, seasonal_3 Loss: 0.0444 | 0.0438
Epoch 24/300, seasonal_3 Loss: 0.0435 | 0.0430
Epoch 25/300, seasonal_3 Loss: 0.0424 | 0.0422
Epoch 26/300, seasonal_3 Loss: 0.0417 | 0.0417
Epoch 27/300, seasonal_3 Loss: 0.0405 | 0.0412
Epoch 28/300, seasonal_3 Loss: 0.0396 | 0.0408
Epoch 29/300, seasonal_3 Loss: 0.0388 | 0.0404
Epoch 30/300, seasonal_3 Loss: 0.0382 | 0.0401
Epoch 31/300, seasonal_3 Loss: 0.0377 | 0.0399
Epoch 32/300, seasonal_3 Loss: 0.0373 | 0.0397
Epoch 33/300, seasonal_3 Loss: 0.0368 | 0.0393
Epoch 34/300, seasonal_3 Loss: 0.0365 | 0.0392
Epoch 35/300, seasonal_3 Loss: 0.0362 | 0.0391
Epoch 36/300, seasonal_3 Loss: 0.0360 | 0.0390
Epoch 37/300, seasonal_3 Loss: 0.0358 | 0.0386
Epoch 38/300, seasonal_3 Loss: 0.0356 | 0.0387
Epoch 39/300, seasonal_3 Loss: 0.0354 | 0.0386
Epoch 40/300, seasonal_3 Loss: 0.0352 | 0.0385
Epoch 41/300, seasonal_3 Loss: 0.0350 | 0.0383
Epoch 42/300, seasonal_3 Loss: 0.0349 | 0.0384
Epoch 43/300, seasonal_3 Loss: 0.0347 | 0.0383
Epoch 44/300, seasonal_3 Loss: 0.0346 | 0.0383
Epoch 45/300, seasonal_3 Loss: 0.0345 | 0.0381
Epoch 46/300, seasonal_3 Loss: 0.0344 | 0.0381
Epoch 47/300, seasonal_3 Loss: 0.0342 | 0.0380
Epoch 48/300, seasonal_3 Loss: 0.0341 | 0.0380
Epoch 49/300, seasonal_3 Loss: 0.0340 | 0.0379
Epoch 50/300, seasonal_3 Loss: 0.0339 | 0.0378
Epoch 51/300, seasonal_3 Loss: 0.0337 | 0.0378
Epoch 52/300, seasonal_3 Loss: 0.0336 | 0.0378
Epoch 53/300, seasonal_3 Loss: 0.0335 | 0.0378
Epoch 54/300, seasonal_3 Loss: 0.0334 | 0.0378
Epoch 55/300, seasonal_3 Loss: 0.0332 | 0.0378
Epoch 56/300, seasonal_3 Loss: 0.0331 | 0.0378
Epoch 57/300, seasonal_3 Loss: 0.0330 | 0.0377
Epoch 58/300, seasonal_3 Loss: 0.0330 | 0.0377
Epoch 59/300, seasonal_3 Loss: 0.0329 | 0.0377
Epoch 60/300, seasonal_3 Loss: 0.0328 | 0.0377
Epoch 61/300, seasonal_3 Loss: 0.0327 | 0.0374
Epoch 62/300, seasonal_3 Loss: 0.0327 | 0.0375
Epoch 63/300, seasonal_3 Loss: 0.0326 | 0.0375
Epoch 64/300, seasonal_3 Loss: 0.0325 | 0.0375
Epoch 65/300, seasonal_3 Loss: 0.0325 | 0.0372
Epoch 66/300, seasonal_3 Loss: 0.0325 | 0.0372
Epoch 67/300, seasonal_3 Loss: 0.0324 | 0.0372
Epoch 68/300, seasonal_3 Loss: 0.0324 | 0.0372
Epoch 69/300, seasonal_3 Loss: 0.0324 | 0.0368
Epoch 70/300, seasonal_3 Loss: 0.0324 | 0.0368
Epoch 71/300, seasonal_3 Loss: 0.0323 | 0.0368
Epoch 72/300, seasonal_3 Loss: 0.0323 | 0.0367
Epoch 73/300, seasonal_3 Loss: 0.0323 | 0.0364
Epoch 74/300, seasonal_3 Loss: 0.0322 | 0.0364
Epoch 75/300, seasonal_3 Loss: 0.0322 | 0.0364
Epoch 76/300, seasonal_3 Loss: 0.0321 | 0.0364
Epoch 77/300, seasonal_3 Loss: 0.0321 | 0.0363
Epoch 78/300, seasonal_3 Loss: 0.0320 | 0.0362
Epoch 79/300, seasonal_3 Loss: 0.0320 | 0.0362
Epoch 80/300, seasonal_3 Loss: 0.0319 | 0.0362
Epoch 81/300, seasonal_3 Loss: 0.0319 | 0.0363
Epoch 82/300, seasonal_3 Loss: 0.0318 | 0.0362
Epoch 83/300, seasonal_3 Loss: 0.0318 | 0.0362
Epoch 84/300, seasonal_3 Loss: 0.0317 | 0.0362
Epoch 85/300, seasonal_3 Loss: 0.0317 | 0.0362
Epoch 86/300, seasonal_3 Loss: 0.0316 | 0.0362
Epoch 87/300, seasonal_3 Loss: 0.0316 | 0.0362
Epoch 88/300, seasonal_3 Loss: 0.0316 | 0.0362
Epoch 89/300, seasonal_3 Loss: 0.0315 | 0.0362
Epoch 90/300, seasonal_3 Loss: 0.0315 | 0.0361
Epoch 91/300, seasonal_3 Loss: 0.0314 | 0.0361
Epoch 92/300, seasonal_3 Loss: 0.0314 | 0.0361
Epoch 93/300, seasonal_3 Loss: 0.0314 | 0.0361
Epoch 94/300, seasonal_3 Loss: 0.0313 | 0.0361
Epoch 95/300, seasonal_3 Loss: 0.0313 | 0.0361
Epoch 96/300, seasonal_3 Loss: 0.0313 | 0.0361
Epoch 97/300, seasonal_3 Loss: 0.0313 | 0.0361
Epoch 98/300, seasonal_3 Loss: 0.0313 | 0.0361
Epoch 99/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 100/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 101/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 102/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 103/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 104/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 105/300, seasonal_3 Loss: 0.0312 | 0.0361
Epoch 106/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 107/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 108/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 109/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 110/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 111/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 112/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 113/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 114/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 115/300, seasonal_3 Loss: 0.0311 | 0.0361
Epoch 116/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 117/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 118/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 119/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 120/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 121/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 122/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 123/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 124/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 125/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 126/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 127/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 128/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 129/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 130/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 131/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 132/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 133/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 134/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 135/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 136/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 137/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 138/300, seasonal_3 Loss: 0.0310 | 0.0361
Epoch 139/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 140/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 141/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 142/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 143/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 144/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 145/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 146/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 147/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 148/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 149/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 150/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 151/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 152/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 153/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 154/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 155/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 156/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 157/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 158/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 159/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 160/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 161/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 162/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 163/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 164/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 165/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 166/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 167/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 168/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 169/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 170/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 171/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 172/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 173/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 174/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 175/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 176/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 177/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 178/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 179/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 180/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 181/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 182/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 183/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 184/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 185/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 186/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 187/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 188/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 189/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 190/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 191/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 192/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 193/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 194/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 195/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 196/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 197/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 198/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 199/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 200/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 201/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 202/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 203/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 204/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 205/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 206/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 207/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 208/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 209/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 210/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 211/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 212/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 213/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 214/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 215/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 216/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 217/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 218/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 219/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 220/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 221/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 222/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 223/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 224/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 225/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 226/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 227/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 228/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 229/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 230/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 231/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 232/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 233/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 234/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 235/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 236/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 237/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 238/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 239/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 240/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 241/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 242/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 243/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 244/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 245/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 246/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 247/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 248/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 249/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 250/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 251/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 252/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 253/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 254/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 255/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 256/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 257/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 258/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 259/300, seasonal_3 Loss: 0.0309 | 0.0361
Epoch 260/300, seasonal_3 Loss: 0.0309 | 0.0361
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.8475705754032464, 'learning_rate': 0.0009233538779287182, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9065561078001826}
Epoch 1/300, resid Loss: 0.5219 | 0.1734
Epoch 2/300, resid Loss: 0.1360 | 0.1203
Epoch 3/300, resid Loss: 0.1517 | 0.1532
Epoch 4/300, resid Loss: 0.1109 | 0.0746
Epoch 5/300, resid Loss: 0.0882 | 0.0892
Epoch 6/300, resid Loss: 0.1129 | 0.1395
Epoch 7/300, resid Loss: 0.1385 | 0.1175
Epoch 8/300, resid Loss: 0.1270 | 0.1105
Epoch 9/300, resid Loss: 0.0920 | 0.0729
Epoch 10/300, resid Loss: 0.1386 | 0.0698
Epoch 11/300, resid Loss: 0.1488 | 0.0867
Epoch 12/300, resid Loss: 0.1536 | 0.1000
Epoch 13/300, resid Loss: 0.1056 | 0.0706
Epoch 14/300, resid Loss: 0.1010 | 0.0616
Epoch 15/300, resid Loss: 0.0742 | 0.0649
Epoch 16/300, resid Loss: 0.0701 | 0.0660
Epoch 17/300, resid Loss: 0.0669 | 0.0970
Epoch 18/300, resid Loss: 0.0624 | 0.0649
Epoch 19/300, resid Loss: 0.0556 | 0.0612
Epoch 20/300, resid Loss: 0.0516 | 0.0466
Epoch 21/300, resid Loss: 0.0563 | 0.0644
Epoch 22/300, resid Loss: 0.0528 | 0.0515
Epoch 23/300, resid Loss: 0.0633 | 0.0548
Epoch 24/300, resid Loss: 0.0505 | 0.0482
Epoch 25/300, resid Loss: 0.0479 | 0.0459
Epoch 26/300, resid Loss: 0.0468 | 0.0510
Epoch 27/300, resid Loss: 0.0517 | 0.0508
Epoch 28/300, resid Loss: 0.0474 | 0.0431
Epoch 29/300, resid Loss: 0.0460 | 0.0457
Epoch 30/300, resid Loss: 0.0449 | 0.0439
Epoch 31/300, resid Loss: 0.0436 | 0.0476
Epoch 32/300, resid Loss: 0.0432 | 0.0446
Epoch 33/300, resid Loss: 0.0439 | 0.0490
Epoch 34/300, resid Loss: 0.0433 | 0.0434
Epoch 35/300, resid Loss: 0.0419 | 0.0458
Epoch 36/300, resid Loss: 0.0429 | 0.0429
Epoch 37/300, resid Loss: 0.0420 | 0.0470
Epoch 38/300, resid Loss: 0.0428 | 0.0404
Epoch 39/300, resid Loss: 0.0415 | 0.0449
Epoch 40/300, resid Loss: 0.0428 | 0.0396
Epoch 41/300, resid Loss: 0.0417 | 0.0430
Epoch 42/300, resid Loss: 0.0421 | 0.0386
Epoch 43/300, resid Loss: 0.0410 | 0.0434
Epoch 44/300, resid Loss: 0.0397 | 0.0398
Epoch 45/300, resid Loss: 0.0407 | 0.0440
Epoch 46/300, resid Loss: 0.0381 | 0.0419
Epoch 47/300, resid Loss: 0.0379 | 0.0401
Epoch 48/300, resid Loss: 0.0369 | 0.0398
Epoch 49/300, resid Loss: 0.0381 | 0.0416
Epoch 50/300, resid Loss: 0.0369 | 0.0389
Epoch 51/300, resid Loss: 0.0371 | 0.0403
Epoch 52/300, resid Loss: 0.0364 | 0.0391
Epoch 53/300, resid Loss: 0.0367 | 0.0390
Epoch 54/300, resid Loss: 0.0360 | 0.0392
Epoch 55/300, resid Loss: 0.0360 | 0.0394
Epoch 56/300, resid Loss: 0.0352 | 0.0388
Epoch 57/300, resid Loss: 0.0358 | 0.0386
Epoch 58/300, resid Loss: 0.0342 | 0.0363
Epoch 59/300, resid Loss: 0.0349 | 0.0381
Epoch 60/300, resid Loss: 0.0334 | 0.0354
Epoch 61/300, resid Loss: 0.0352 | 0.0364
Epoch 62/300, resid Loss: 0.0339 | 0.0353
Epoch 63/300, resid Loss: 0.0360 | 0.0358
Epoch 64/300, resid Loss: 0.0349 | 0.0358
Epoch 65/300, resid Loss: 0.0379 | 0.0361
Epoch 66/300, resid Loss: 0.0373 | 0.0343
Epoch 67/300, resid Loss: 0.0394 | 0.0349
Epoch 68/300, resid Loss: 0.0398 | 0.0338
Epoch 69/300, resid Loss: 0.0448 | 0.0373
Epoch 70/300, resid Loss: 0.0469 | 0.0397
Epoch 71/300, resid Loss: 0.0490 | 0.0448
Epoch 72/300, resid Loss: 0.0439 | 0.0430
Epoch 73/300, resid Loss: 0.0402 | 0.0455
Epoch 74/300, resid Loss: 0.0370 | 0.0399
Epoch 75/300, resid Loss: 0.0325 | 0.0355
Epoch 76/300, resid Loss: 0.0328 | 0.0321
Epoch 77/300, resid Loss: 0.0337 | 0.0319
Epoch 78/300, resid Loss: 0.0341 | 0.0333
Epoch 79/300, resid Loss: 0.0320 | 0.0316
Epoch 80/300, resid Loss: 0.0314 | 0.0329
Epoch 81/300, resid Loss: 0.0301 | 0.0312
Epoch 82/300, resid Loss: 0.0298 | 0.0324
Epoch 83/300, resid Loss: 0.0293 | 0.0318
Epoch 84/300, resid Loss: 0.0291 | 0.0319
Epoch 85/300, resid Loss: 0.0290 | 0.0314
Epoch 86/300, resid Loss: 0.0283 | 0.0305
Epoch 87/300, resid Loss: 0.0282 | 0.0301
Epoch 88/300, resid Loss: 0.0278 | 0.0298
Epoch 89/300, resid Loss: 0.0280 | 0.0300
Epoch 90/300, resid Loss: 0.0278 | 0.0295
Epoch 91/300, resid Loss: 0.0272 | 0.0291
Epoch 92/300, resid Loss: 0.0265 | 0.0285
Epoch 93/300, resid Loss: 0.0262 | 0.0283
Epoch 94/300, resid Loss: 0.0260 | 0.0281
Epoch 95/300, resid Loss: 0.0258 | 0.0281
Epoch 96/300, resid Loss: 0.0256 | 0.0279
Epoch 97/300, resid Loss: 0.0255 | 0.0281
Epoch 98/300, resid Loss: 0.0253 | 0.0279
Epoch 99/300, resid Loss: 0.0251 | 0.0279
Epoch 100/300, resid Loss: 0.0250 | 0.0278
Epoch 101/300, resid Loss: 0.0249 | 0.0279
Epoch 102/300, resid Loss: 0.0247 | 0.0278
Epoch 103/300, resid Loss: 0.0246 | 0.0278
Epoch 104/300, resid Loss: 0.0246 | 0.0278
Epoch 105/300, resid Loss: 0.0246 | 0.0277
Epoch 106/300, resid Loss: 0.0245 | 0.0276
Epoch 107/300, resid Loss: 0.0246 | 0.0275
Epoch 108/300, resid Loss: 0.0247 | 0.0273
Epoch 109/300, resid Loss: 0.0251 | 0.0272
Epoch 110/300, resid Loss: 0.0260 | 0.0284
Epoch 111/300, resid Loss: 0.0272 | 0.0283
Epoch 112/300, resid Loss: 0.0259 | 0.0276
Epoch 113/300, resid Loss: 0.0249 | 0.0272
Epoch 114/300, resid Loss: 0.0251 | 0.0272
Epoch 115/300, resid Loss: 0.0244 | 0.0270
Epoch 116/300, resid Loss: 0.0239 | 0.0269
Epoch 117/300, resid Loss: 0.0237 | 0.0267
Epoch 118/300, resid Loss: 0.0236 | 0.0266
Epoch 119/300, resid Loss: 0.0236 | 0.0265
Epoch 120/300, resid Loss: 0.0235 | 0.0265
Epoch 121/300, resid Loss: 0.0234 | 0.0265
Epoch 122/300, resid Loss: 0.0233 | 0.0265
Epoch 123/300, resid Loss: 0.0232 | 0.0264
Epoch 124/300, resid Loss: 0.0232 | 0.0264
Epoch 125/300, resid Loss: 0.0231 | 0.0264
Epoch 126/300, resid Loss: 0.0230 | 0.0263
Epoch 127/300, resid Loss: 0.0230 | 0.0263
Epoch 128/300, resid Loss: 0.0230 | 0.0264
Epoch 129/300, resid Loss: 0.0231 | 0.0263
Epoch 130/300, resid Loss: 0.0233 | 0.0263
Epoch 131/300, resid Loss: 0.0239 | 0.0265
Epoch 132/300, resid Loss: 0.0251 | 0.0299
Epoch 133/300, resid Loss: 0.0247 | 0.0262
Epoch 134/300, resid Loss: 0.0243 | 0.0287
Epoch 135/300, resid Loss: 0.0237 | 0.0269
Epoch 136/300, resid Loss: 0.0231 | 0.0268
Epoch 137/300, resid Loss: 0.0229 | 0.0267
Epoch 138/300, resid Loss: 0.0227 | 0.0266
Epoch 139/300, resid Loss: 0.0227 | 0.0265
Epoch 140/300, resid Loss: 0.0226 | 0.0265
Epoch 141/300, resid Loss: 0.0226 | 0.0266
Epoch 142/300, resid Loss: 0.0225 | 0.0267
Epoch 143/300, resid Loss: 0.0225 | 0.0267
Epoch 144/300, resid Loss: 0.0224 | 0.0266
Epoch 145/300, resid Loss: 0.0223 | 0.0267
Epoch 146/300, resid Loss: 0.0223 | 0.0266
Epoch 147/300, resid Loss: 0.0222 | 0.0266
Epoch 148/300, resid Loss: 0.0221 | 0.0265
Epoch 149/300, resid Loss: 0.0221 | 0.0265
Epoch 150/300, resid Loss: 0.0220 | 0.0265
Epoch 151/300, resid Loss: 0.0220 | 0.0264
Epoch 152/300, resid Loss: 0.0219 | 0.0264
Epoch 153/300, resid Loss: 0.0219 | 0.0263
Epoch 154/300, resid Loss: 0.0219 | 0.0262
Epoch 155/300, resid Loss: 0.0218 | 0.0262
Epoch 156/300, resid Loss: 0.0218 | 0.0262
Epoch 157/300, resid Loss: 0.0218 | 0.0261
Epoch 158/300, resid Loss: 0.0218 | 0.0260
Epoch 159/300, resid Loss: 0.0217 | 0.0260
Epoch 160/300, resid Loss: 0.0217 | 0.0260
Epoch 161/300, resid Loss: 0.0217 | 0.0260
Epoch 162/300, resid Loss: 0.0216 | 0.0259
Epoch 163/300, resid Loss: 0.0216 | 0.0260
Epoch 164/300, resid Loss: 0.0216 | 0.0260
Epoch 165/300, resid Loss: 0.0215 | 0.0260
Epoch 166/300, resid Loss: 0.0215 | 0.0260
Epoch 167/300, resid Loss: 0.0215 | 0.0260
Epoch 168/300, resid Loss: 0.0215 | 0.0260
Epoch 169/300, resid Loss: 0.0214 | 0.0260
Epoch 170/300, resid Loss: 0.0214 | 0.0261
Epoch 171/300, resid Loss: 0.0214 | 0.0261
Epoch 172/300, resid Loss: 0.0214 | 0.0261
Epoch 173/300, resid Loss: 0.0214 | 0.0261
Epoch 174/300, resid Loss: 0.0213 | 0.0261
Epoch 175/300, resid Loss: 0.0213 | 0.0261
Epoch 176/300, resid Loss: 0.0213 | 0.0261
Epoch 177/300, resid Loss: 0.0213 | 0.0261
Epoch 178/300, resid Loss: 0.0212 | 0.0260
Epoch 179/300, resid Loss: 0.0212 | 0.0260
Epoch 180/300, resid Loss: 0.0212 | 0.0260
Epoch 181/300, resid Loss: 0.0212 | 0.0259
Epoch 182/300, resid Loss: 0.0211 | 0.0259
Epoch 183/300, resid Loss: 0.0211 | 0.0259
Epoch 184/300, resid Loss: 0.0211 | 0.0259
Epoch 185/300, resid Loss: 0.0211 | 0.0259
Epoch 186/300, resid Loss: 0.0211 | 0.0258
Epoch 187/300, resid Loss: 0.0211 | 0.0258
Epoch 188/300, resid Loss: 0.0210 | 0.0258
Epoch 189/300, resid Loss: 0.0210 | 0.0258
Epoch 190/300, resid Loss: 0.0210 | 0.0258
Epoch 191/300, resid Loss: 0.0210 | 0.0258
Epoch 192/300, resid Loss: 0.0210 | 0.0258
Epoch 193/300, resid Loss: 0.0210 | 0.0258
Epoch 194/300, resid Loss: 0.0209 | 0.0258
Epoch 195/300, resid Loss: 0.0209 | 0.0258
Epoch 196/300, resid Loss: 0.0209 | 0.0258
Epoch 197/300, resid Loss: 0.0209 | 0.0258
Epoch 198/300, resid Loss: 0.0209 | 0.0258
Epoch 199/300, resid Loss: 0.0209 | 0.0258
Epoch 200/300, resid Loss: 0.0209 | 0.0258
Epoch 201/300, resid Loss: 0.0208 | 0.0258
Epoch 202/300, resid Loss: 0.0208 | 0.0257
Epoch 203/300, resid Loss: 0.0208 | 0.0257
Epoch 204/300, resid Loss: 0.0208 | 0.0257
Epoch 205/300, resid Loss: 0.0208 | 0.0257
Epoch 206/300, resid Loss: 0.0208 | 0.0257
Epoch 207/300, resid Loss: 0.0208 | 0.0257
Epoch 208/300, resid Loss: 0.0208 | 0.0257
Epoch 209/300, resid Loss: 0.0208 | 0.0257
Epoch 210/300, resid Loss: 0.0207 | 0.0257
Epoch 211/300, resid Loss: 0.0207 | 0.0257
Epoch 212/300, resid Loss: 0.0207 | 0.0257
Epoch 213/300, resid Loss: 0.0207 | 0.0257
Epoch 214/300, resid Loss: 0.0207 | 0.0257
Epoch 215/300, resid Loss: 0.0207 | 0.0257
Epoch 216/300, resid Loss: 0.0207 | 0.0257
Epoch 217/300, resid Loss: 0.0207 | 0.0257
Epoch 218/300, resid Loss: 0.0207 | 0.0257
Epoch 219/300, resid Loss: 0.0207 | 0.0257
Epoch 220/300, resid Loss: 0.0207 | 0.0257
Epoch 221/300, resid Loss: 0.0206 | 0.0256
Epoch 222/300, resid Loss: 0.0206 | 0.0256
Epoch 223/300, resid Loss: 0.0206 | 0.0256
Epoch 224/300, resid Loss: 0.0206 | 0.0256
Epoch 225/300, resid Loss: 0.0206 | 0.0256
Epoch 226/300, resid Loss: 0.0206 | 0.0256
Epoch 227/300, resid Loss: 0.0206 | 0.0256
Epoch 228/300, resid Loss: 0.0206 | 0.0256
Epoch 229/300, resid Loss: 0.0206 | 0.0256
Epoch 230/300, resid Loss: 0.0206 | 0.0256
Epoch 231/300, resid Loss: 0.0206 | 0.0256
Epoch 232/300, resid Loss: 0.0206 | 0.0256
Epoch 233/300, resid Loss: 0.0206 | 0.0256
Epoch 234/300, resid Loss: 0.0205 | 0.0256
Epoch 235/300, resid Loss: 0.0205 | 0.0256
Epoch 236/300, resid Loss: 0.0205 | 0.0256
Epoch 237/300, resid Loss: 0.0205 | 0.0256
Epoch 238/300, resid Loss: 0.0205 | 0.0256
Epoch 239/300, resid Loss: 0.0205 | 0.0256
Epoch 240/300, resid Loss: 0.0205 | 0.0256
Epoch 241/300, resid Loss: 0.0205 | 0.0256
Epoch 242/300, resid Loss: 0.0205 | 0.0256
Epoch 243/300, resid Loss: 0.0205 | 0.0256
Epoch 244/300, resid Loss: 0.0205 | 0.0256
Epoch 245/300, resid Loss: 0.0205 | 0.0256
Epoch 246/300, resid Loss: 0.0205 | 0.0256
Epoch 247/300, resid Loss: 0.0205 | 0.0256
Epoch 248/300, resid Loss: 0.0205 | 0.0256
Epoch 249/300, resid Loss: 0.0205 | 0.0255
Epoch 250/300, resid Loss: 0.0205 | 0.0255
Epoch 251/300, resid Loss: 0.0205 | 0.0255
Epoch 252/300, resid Loss: 0.0205 | 0.0255
Epoch 253/300, resid Loss: 0.0205 | 0.0255
Epoch 254/300, resid Loss: 0.0204 | 0.0255
Epoch 255/300, resid Loss: 0.0204 | 0.0255
Epoch 256/300, resid Loss: 0.0204 | 0.0255
Epoch 257/300, resid Loss: 0.0204 | 0.0255
Epoch 258/300, resid Loss: 0.0204 | 0.0255
Epoch 259/300, resid Loss: 0.0204 | 0.0255
Epoch 260/300, resid Loss: 0.0204 | 0.0255
Epoch 261/300, resid Loss: 0.0204 | 0.0255
Epoch 262/300, resid Loss: 0.0204 | 0.0255
Epoch 263/300, resid Loss: 0.0204 | 0.0255
Epoch 264/300, resid Loss: 0.0204 | 0.0255
Epoch 265/300, resid Loss: 0.0204 | 0.0255
Epoch 266/300, resid Loss: 0.0204 | 0.0255
Epoch 267/300, resid Loss: 0.0204 | 0.0255
Epoch 268/300, resid Loss: 0.0204 | 0.0255
Epoch 269/300, resid Loss: 0.0204 | 0.0255
Epoch 270/300, resid Loss: 0.0204 | 0.0255
Epoch 271/300, resid Loss: 0.0204 | 0.0255
Epoch 272/300, resid Loss: 0.0204 | 0.0255
Epoch 273/300, resid Loss: 0.0204 | 0.0255
Epoch 274/300, resid Loss: 0.0204 | 0.0255
Epoch 275/300, resid Loss: 0.0204 | 0.0255
Epoch 276/300, resid Loss: 0.0204 | 0.0255
Epoch 277/300, resid Loss: 0.0204 | 0.0255
Epoch 278/300, resid Loss: 0.0204 | 0.0255
Epoch 279/300, resid Loss: 0.0204 | 0.0255
Epoch 280/300, resid Loss: 0.0204 | 0.0255
Epoch 281/300, resid Loss: 0.0204 | 0.0255
Epoch 282/300, resid Loss: 0.0204 | 0.0255
Epoch 283/300, resid Loss: 0.0204 | 0.0255
Epoch 284/300, resid Loss: 0.0204 | 0.0255
Epoch 285/300, resid Loss: 0.0204 | 0.0255
Epoch 286/300, resid Loss: 0.0204 | 0.0255
Epoch 287/300, resid Loss: 0.0204 | 0.0255
Epoch 288/300, resid Loss: 0.0204 | 0.0255
Epoch 289/300, resid Loss: 0.0203 | 0.0255
Epoch 290/300, resid Loss: 0.0203 | 0.0255
Epoch 291/300, resid Loss: 0.0203 | 0.0255
Epoch 292/300, resid Loss: 0.0203 | 0.0255
Epoch 293/300, resid Loss: 0.0203 | 0.0255
Epoch 294/300, resid Loss: 0.0203 | 0.0255
Epoch 295/300, resid Loss: 0.0203 | 0.0255
Epoch 296/300, resid Loss: 0.0203 | 0.0255
Epoch 297/300, resid Loss: 0.0203 | 0.0255
Epoch 298/300, resid Loss: 0.0203 | 0.0255
Epoch 299/300, resid Loss: 0.0203 | 0.0255
Epoch 300/300, resid Loss: 0.0203 | 0.0255
Runtime (seconds): 927.2220151424408
0.0009840876918124912
[111.71867]
[13.12718]
[-19.683918]
[13.605743]
[38.58819]
[-14.34374]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 101.76522940420546
RMSE: 10.087875366210938
MAE: 10.087875366210938
R-squared: nan
[143.01213]
