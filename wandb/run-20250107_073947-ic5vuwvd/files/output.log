[32m[I 2025-01-07 07:39:52,869][0m A new study created in memory with name: no-name-abacb45b-9db5-4c35-8651-dd0f852c899e[0m
[32m[I 2025-01-07 07:43:14,382][0m Trial 0 finished with value: 1.3742347674660225 and parameters: {'observation_period_num': 87, 'train_rates': 0.6463656522514983, 'learning_rate': 0.000366481919377756, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7611328018198404}. Best is trial 0 with value: 1.3742347674660225.[0m
[32m[I 2025-01-07 07:48:17,590][0m Trial 1 finished with value: 1.3105715452800697 and parameters: {'observation_period_num': 237, 'train_rates': 0.6019961679994933, 'learning_rate': 0.00020014107510621772, 'batch_size': 16, 'step_size': 8, 'gamma': 0.7759637129676543}. Best is trial 1 with value: 1.3105715452800697.[0m
[32m[I 2025-01-07 07:49:35,664][0m Trial 2 finished with value: 0.17508095502853394 and parameters: {'observation_period_num': 58, 'train_rates': 0.9773530873443022, 'learning_rate': 4.063133262022353e-05, 'batch_size': 164, 'step_size': 12, 'gamma': 0.8142605163867188}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 07:54:05,388][0m Trial 3 finished with value: 0.7220595325946289 and parameters: {'observation_period_num': 221, 'train_rates': 0.6650925939875938, 'learning_rate': 2.5555751606765143e-05, 'batch_size': 187, 'step_size': 4, 'gamma': 0.957863322988716}. Best is trial 2 with value: 0.17508095502853394.[0m
Early stopping at epoch 88
[32m[I 2025-01-07 07:59:34,010][0m Trial 4 finished with value: 0.7244103229235089 and parameters: {'observation_period_num': 244, 'train_rates': 0.9076141509434946, 'learning_rate': 0.0009646604353698154, 'batch_size': 116, 'step_size': 1, 'gamma': 0.8659443601430317}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:02:55,627][0m Trial 5 finished with value: 0.38131458642362037 and parameters: {'observation_period_num': 153, 'train_rates': 0.8314645104746871, 'learning_rate': 0.0003183380009493933, 'batch_size': 181, 'step_size': 4, 'gamma': 0.760352589593972}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:06:46,176][0m Trial 6 finished with value: 0.41376917495319016 and parameters: {'observation_period_num': 174, 'train_rates': 0.7449589721692411, 'learning_rate': 4.728241592706662e-05, 'batch_size': 46, 'step_size': 8, 'gamma': 0.8351996585211436}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:09:11,722][0m Trial 7 finished with value: 0.29313802731585803 and parameters: {'observation_period_num': 100, 'train_rates': 0.9166381502143373, 'learning_rate': 1.7014244877272824e-05, 'batch_size': 45, 'step_size': 5, 'gamma': 0.7863812752008154}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:12:54,438][0m Trial 8 finished with value: 0.47633705652037334 and parameters: {'observation_period_num': 173, 'train_rates': 0.7844431248294561, 'learning_rate': 7.166424034732444e-05, 'batch_size': 98, 'step_size': 15, 'gamma': 0.8089259669858276}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:15:06,796][0m Trial 9 finished with value: 0.5052601616746633 and parameters: {'observation_period_num': 114, 'train_rates': 0.7324800999555839, 'learning_rate': 0.00010854343075922196, 'batch_size': 123, 'step_size': 6, 'gamma': 0.8583334600799584}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:15:39,991][0m Trial 10 finished with value: 0.8660517334938049 and parameters: {'observation_period_num': 22, 'train_rates': 0.9743306039165699, 'learning_rate': 2.012615819058252e-06, 'batch_size': 245, 'step_size': 13, 'gamma': 0.9522545507246103}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:16:50,474][0m Trial 11 finished with value: 0.5169073939323425 and parameters: {'observation_period_num': 52, 'train_rates': 0.9835243733872487, 'learning_rate': 8.572431014416581e-06, 'batch_size': 171, 'step_size': 11, 'gamma': 0.8080508976851022}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:18:32,211][0m Trial 12 finished with value: 0.2973523269850033 and parameters: {'observation_period_num': 74, 'train_rates': 0.8815785584368302, 'learning_rate': 1.1731640554006835e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.9054537240385253}. Best is trial 2 with value: 0.17508095502853394.[0m
Early stopping at epoch 52
[32m[I 2025-01-07 08:18:49,257][0m Trial 13 finished with value: 1.7790375145492852 and parameters: {'observation_period_num': 8, 'train_rates': 0.9133206128183843, 'learning_rate': 3.875774776552375e-06, 'batch_size': 149, 'step_size': 1, 'gamma': 0.8124987237817058}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:21:26,301][0m Trial 14 finished with value: 0.3880288302898407 and parameters: {'observation_period_num': 116, 'train_rates': 0.9430537454273472, 'learning_rate': 2.1669159589438275e-05, 'batch_size': 228, 'step_size': 5, 'gamma': 0.8972780792405006}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:22:26,553][0m Trial 15 finished with value: 1.415393706437796 and parameters: {'observation_period_num': 49, 'train_rates': 0.8563958378462372, 'learning_rate': 1.007527178216859e-06, 'batch_size': 216, 'step_size': 15, 'gamma': 0.7957942735714784}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:24:24,812][0m Trial 16 finished with value: 0.5334951820196929 and parameters: {'observation_period_num': 82, 'train_rates': 0.9531567759419605, 'learning_rate': 6.499650690138585e-06, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8463040214452283}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:27:13,553][0m Trial 17 finished with value: 0.507521844448884 and parameters: {'observation_period_num': 130, 'train_rates': 0.8204915728907557, 'learning_rate': 1.8848536871453852e-05, 'batch_size': 159, 'step_size': 10, 'gamma': 0.7873617356824174}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:28:16,540][0m Trial 18 finished with value: 0.1777452117946732 and parameters: {'observation_period_num': 44, 'train_rates': 0.9147318867693818, 'learning_rate': 5.83624452704796e-05, 'batch_size': 101, 'step_size': 13, 'gamma': 0.8300510580083391}. Best is trial 2 with value: 0.17508095502853394.[0m
[32m[I 2025-01-07 08:29:17,308][0m Trial 19 finished with value: 0.12994326651096344 and parameters: {'observation_period_num': 39, 'train_rates': 0.9891717898550614, 'learning_rate': 5.187265101966205e-05, 'batch_size': 96, 'step_size': 13, 'gamma': 0.8891320221839909}. Best is trial 19 with value: 0.12994326651096344.[0m
[32m[I 2025-01-07 08:29:58,846][0m Trial 20 finished with value: 0.12448740750551224 and parameters: {'observation_period_num': 27, 'train_rates': 0.9881344424969394, 'learning_rate': 0.00012092097529720949, 'batch_size': 202, 'step_size': 13, 'gamma': 0.8969957964422322}. Best is trial 20 with value: 0.12448740750551224.[0m
[32m[I 2025-01-07 08:30:40,831][0m Trial 21 finished with value: 0.10350212454795837 and parameters: {'observation_period_num': 28, 'train_rates': 0.9818010899678055, 'learning_rate': 0.00012384791839376824, 'batch_size': 202, 'step_size': 13, 'gamma': 0.901192879041161}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:31:06,072][0m Trial 22 finished with value: 0.10622266680002213 and parameters: {'observation_period_num': 5, 'train_rates': 0.9881153793610562, 'learning_rate': 0.00013919928006007968, 'batch_size': 203, 'step_size': 14, 'gamma': 0.9020542805928548}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:31:32,661][0m Trial 23 finished with value: 0.16230854392051697 and parameters: {'observation_period_num': 16, 'train_rates': 0.9472155690646901, 'learning_rate': 0.00014693847772607617, 'batch_size': 206, 'step_size': 14, 'gamma': 0.9286868609848309}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:32:12,136][0m Trial 24 finished with value: 0.1860782209464482 and parameters: {'observation_period_num': 29, 'train_rates': 0.8809068702087468, 'learning_rate': 0.0005935026105922883, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9898090614954002}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:32:37,679][0m Trial 25 finished with value: 0.1830029934644699 and parameters: {'observation_period_num': 5, 'train_rates': 0.9505525362043138, 'learning_rate': 9.965345783847699e-05, 'batch_size': 201, 'step_size': 12, 'gamma': 0.9246551844192239}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:34:06,587][0m Trial 26 finished with value: 0.16509861629456282 and parameters: {'observation_period_num': 67, 'train_rates': 0.8897382113297064, 'learning_rate': 0.0002367686306280562, 'batch_size': 229, 'step_size': 9, 'gamma': 0.8792124856400209}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:34:49,148][0m Trial 27 finished with value: 0.22644644975662231 and parameters: {'observation_period_num': 30, 'train_rates': 0.9588069193915869, 'learning_rate': 0.0005391826631002862, 'batch_size': 192, 'step_size': 12, 'gamma': 0.9125601231002368}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:35:24,297][0m Trial 28 finished with value: 0.1229406227399637 and parameters: {'observation_period_num': 6, 'train_rates': 0.9252087542196643, 'learning_rate': 0.00012823659292565673, 'batch_size': 142, 'step_size': 14, 'gamma': 0.9481385917787021}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:35:59,198][0m Trial 29 finished with value: 0.1853939567632343 and parameters: {'observation_period_num': 10, 'train_rates': 0.9319670647138805, 'learning_rate': 0.00037084130041136387, 'batch_size': 142, 'step_size': 14, 'gamma': 0.948158205562014}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:37:53,117][0m Trial 30 finished with value: 0.2997561567054506 and parameters: {'observation_period_num': 91, 'train_rates': 0.8415526551903428, 'learning_rate': 0.00019057382916531438, 'batch_size': 130, 'step_size': 10, 'gamma': 0.972731023618161}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:38:42,395][0m Trial 31 finished with value: 0.11948234587907791 and parameters: {'observation_period_num': 34, 'train_rates': 0.9847577567856914, 'learning_rate': 0.00010675284883679917, 'batch_size': 219, 'step_size': 14, 'gamma': 0.9303599023455084}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:39:35,204][0m Trial 32 finished with value: 0.16708317399024963 and parameters: {'observation_period_num': 38, 'train_rates': 0.9668978411878861, 'learning_rate': 8.81730037860775e-05, 'batch_size': 225, 'step_size': 14, 'gamma': 0.9333594200782703}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:40:56,419][0m Trial 33 finished with value: 0.18525243766846194 and parameters: {'observation_period_num': 64, 'train_rates': 0.9359005304619682, 'learning_rate': 0.0001697424717240117, 'batch_size': 174, 'step_size': 14, 'gamma': 0.9191840069248358}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:41:26,465][0m Trial 34 finished with value: 0.11003952473402023 and parameters: {'observation_period_num': 17, 'train_rates': 0.9895009159990339, 'learning_rate': 0.00027347745583038017, 'batch_size': 240, 'step_size': 11, 'gamma': 0.9379015255006368}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:45:44,060][0m Trial 35 finished with value: 0.944479920462931 and parameters: {'observation_period_num': 220, 'train_rates': 0.6200617779152074, 'learning_rate': 3.178061631785587e-05, 'batch_size': 249, 'step_size': 11, 'gamma': 0.8787703509250812}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:46:45,023][0m Trial 36 finished with value: 0.862895378499831 and parameters: {'observation_period_num': 57, 'train_rates': 0.6863855773293843, 'learning_rate': 0.00031605354005382165, 'batch_size': 235, 'step_size': 12, 'gamma': 0.9790707911430115}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:47:19,301][0m Trial 37 finished with value: 0.18111067915719653 and parameters: {'observation_period_num': 23, 'train_rates': 0.8930511146907493, 'learning_rate': 0.0005255988755294304, 'batch_size': 215, 'step_size': 9, 'gamma': 0.9366328449383762}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:48:10,296][0m Trial 38 finished with value: 0.7705153226852417 and parameters: {'observation_period_num': 37, 'train_rates': 0.9660378308934832, 'learning_rate': 0.0009807642697357791, 'batch_size': 191, 'step_size': 12, 'gamma': 0.9652626391201033}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:49:56,768][0m Trial 39 finished with value: 0.1222468912601471 and parameters: {'observation_period_num': 77, 'train_rates': 0.9895352204161411, 'learning_rate': 0.0002234916065812266, 'batch_size': 242, 'step_size': 15, 'gamma': 0.9128712410792161}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:50:22,548][0m Trial 40 finished with value: 0.45226674723658666 and parameters: {'observation_period_num': 18, 'train_rates': 0.7583400079689012, 'learning_rate': 3.197542166267384e-05, 'batch_size': 213, 'step_size': 11, 'gamma': 0.9400171565952913}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:52:11,335][0m Trial 41 finished with value: 0.10790063440799713 and parameters: {'observation_period_num': 79, 'train_rates': 0.9896690724725341, 'learning_rate': 0.0002682697392226225, 'batch_size': 242, 'step_size': 15, 'gamma': 0.9102244894541243}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:54:18,106][0m Trial 42 finished with value: 0.18482990562915802 and parameters: {'observation_period_num': 93, 'train_rates': 0.9654768583078317, 'learning_rate': 0.0003096392564863965, 'batch_size': 239, 'step_size': 15, 'gamma': 0.9028175419067375}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:55:31,900][0m Trial 43 finished with value: 0.18364743888378143 and parameters: {'observation_period_num': 55, 'train_rates': 0.9706239335715598, 'learning_rate': 7.179087922409978e-05, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8838249538868347}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:59:02,094][0m Trial 44 finished with value: 0.18927808105945587 and parameters: {'observation_period_num': 147, 'train_rates': 0.9376070496645105, 'learning_rate': 0.000453004679745633, 'batch_size': 255, 'step_size': 14, 'gamma': 0.8578712535189407}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 08:59:42,383][0m Trial 45 finished with value: 0.5919493951299174 and parameters: {'observation_period_num': 34, 'train_rates': 0.7046365814237385, 'learning_rate': 0.00025468103939703434, 'batch_size': 186, 'step_size': 13, 'gamma': 0.869521861519906}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 09:00:10,477][0m Trial 46 finished with value: 0.10713954269886017 and parameters: {'observation_period_num': 17, 'train_rates': 0.974255263660193, 'learning_rate': 0.000162497881099249, 'batch_size': 206, 'step_size': 15, 'gamma': 0.9163024387624393}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 09:02:28,002][0m Trial 47 finished with value: 0.20340620213789298 and parameters: {'observation_period_num': 105, 'train_rates': 0.9013611713416724, 'learning_rate': 0.0001801324299197517, 'batch_size': 161, 'step_size': 15, 'gamma': 0.9114853422762336}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 09:02:58,175][0m Trial 48 finished with value: 0.33008936047554016 and parameters: {'observation_period_num': 17, 'train_rates': 0.9573688173674509, 'learning_rate': 0.0006919744146298829, 'batch_size': 178, 'step_size': 2, 'gamma': 0.8923311621102357}. Best is trial 21 with value: 0.10350212454795837.[0m
[32m[I 2025-01-07 09:03:56,745][0m Trial 49 finished with value: 0.24390377525066465 and parameters: {'observation_period_num': 46, 'train_rates': 0.8679314923752239, 'learning_rate': 0.00038982003928361665, 'batch_size': 197, 'step_size': 15, 'gamma': 0.9194734941390506}. Best is trial 21 with value: 0.10350212454795837.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.9960 | 1.4594
Epoch 2/300, Loss: 0.5884 | 0.9291
Epoch 3/300, Loss: 0.4994 | 0.8120
Epoch 4/300, Loss: 0.4122 | 0.6388
Epoch 5/300, Loss: 0.3708 | 0.5947
Epoch 6/300, Loss: 0.3652 | 0.5287
Epoch 7/300, Loss: 0.4011 | 0.6014
Epoch 8/300, Loss: 0.3394 | 0.4788
Epoch 9/300, Loss: 0.3126 | 0.4409
Epoch 10/300, Loss: 0.3052 | 0.4209
Epoch 11/300, Loss: 0.2959 | 0.3969
Epoch 12/300, Loss: 0.2628 | 0.3767
Epoch 13/300, Loss: 0.2630 | 0.3390
Epoch 14/300, Loss: 0.2736 | 0.3438
Epoch 15/300, Loss: 0.2471 | 0.3076
Epoch 16/300, Loss: 0.2474 | 0.3064
Epoch 17/300, Loss: 0.2215 | 0.2916
Epoch 18/300, Loss: 0.2127 | 0.2826
Epoch 19/300, Loss: 0.2099 | 0.2679
Epoch 20/300, Loss: 0.2104 | 0.2948
Epoch 21/300, Loss: 0.2171 | 0.2549
Epoch 22/300, Loss: 0.2051 | 0.2696
Epoch 23/300, Loss: 0.1979 | 0.2453
Epoch 24/300, Loss: 0.1869 | 0.2289
Epoch 25/300, Loss: 0.1794 | 0.2217
Epoch 26/300, Loss: 0.1735 | 0.2124
Epoch 27/300, Loss: 0.1713 | 0.2064
Epoch 28/300, Loss: 0.1672 | 0.2020
Epoch 29/300, Loss: 0.1649 | 0.1961
Epoch 30/300, Loss: 0.1632 | 0.1928
Epoch 31/300, Loss: 0.1625 | 0.1882
Epoch 32/300, Loss: 0.1618 | 0.1863
Epoch 33/300, Loss: 0.1659 | 0.1827
Epoch 34/300, Loss: 0.1762 | 0.1825
Epoch 35/300, Loss: 0.1799 | 0.1794
Epoch 36/300, Loss: 0.1954 | 0.1760
Epoch 37/300, Loss: 0.1877 | 0.1746
Epoch 38/300, Loss: 0.1968 | 0.1791
Epoch 39/300, Loss: 0.1897 | 0.1754
Epoch 40/300, Loss: 0.1821 | 0.1832
Epoch 41/300, Loss: 0.1711 | 0.1729
Epoch 42/300, Loss: 0.1601 | 0.1743
Epoch 43/300, Loss: 0.1569 | 0.1662
Epoch 44/300, Loss: 0.1537 | 0.1667
Epoch 45/300, Loss: 0.1525 | 0.1604
Epoch 46/300, Loss: 0.1515 | 0.1580
Epoch 47/300, Loss: 0.1500 | 0.1567
Epoch 48/300, Loss: 0.1492 | 0.1509
Epoch 49/300, Loss: 0.1487 | 0.1504
Epoch 50/300, Loss: 0.1481 | 0.1465
Epoch 51/300, Loss: 0.1470 | 0.1455
Epoch 52/300, Loss: 0.1473 | 0.1436
Epoch 53/300, Loss: 0.1470 | 0.1444
Epoch 54/300, Loss: 0.1474 | 0.1457
Epoch 55/300, Loss: 0.1497 | 0.1478
Epoch 56/300, Loss: 0.1521 | 0.1519
Epoch 57/300, Loss: 0.1502 | 0.1424
Epoch 58/300, Loss: 0.1446 | 0.1384
Epoch 59/300, Loss: 0.1394 | 0.1349
Epoch 60/300, Loss: 0.1362 | 0.1339
Epoch 61/300, Loss: 0.1347 | 0.1316
Epoch 62/300, Loss: 0.1344 | 0.1309
Epoch 63/300, Loss: 0.1333 | 0.1300
Epoch 64/300, Loss: 0.1323 | 0.1288
Epoch 65/300, Loss: 0.1325 | 0.1277
Epoch 66/300, Loss: 0.1314 | 0.1266
Epoch 67/300, Loss: 0.1305 | 0.1253
Epoch 68/300, Loss: 0.1303 | 0.1244
Epoch 69/300, Loss: 0.1296 | 0.1232
Epoch 70/300, Loss: 0.1282 | 0.1225
Epoch 71/300, Loss: 0.1281 | 0.1215
Epoch 72/300, Loss: 0.1278 | 0.1209
Epoch 73/300, Loss: 0.1276 | 0.1200
Epoch 74/300, Loss: 0.1266 | 0.1194
Epoch 75/300, Loss: 0.1255 | 0.1184
Epoch 76/300, Loss: 0.1250 | 0.1178
Epoch 77/300, Loss: 0.1245 | 0.1173
Epoch 78/300, Loss: 0.1241 | 0.1165
Epoch 79/300, Loss: 0.1233 | 0.1151
Epoch 80/300, Loss: 0.1234 | 0.1155
Epoch 81/300, Loss: 0.1224 | 0.1143
Epoch 82/300, Loss: 0.1226 | 0.1149
Epoch 83/300, Loss: 0.1227 | 0.1133
Epoch 84/300, Loss: 0.1229 | 0.1146
Epoch 85/300, Loss: 0.1242 | 0.1131
Epoch 86/300, Loss: 0.1252 | 0.1148
Epoch 87/300, Loss: 0.1255 | 0.1117
Epoch 88/300, Loss: 0.1259 | 0.1121
Epoch 89/300, Loss: 0.1236 | 0.1101
Epoch 90/300, Loss: 0.1214 | 0.1099
Epoch 91/300, Loss: 0.1196 | 0.1093
Epoch 92/300, Loss: 0.1188 | 0.1090
Epoch 93/300, Loss: 0.1177 | 0.1081
Epoch 94/300, Loss: 0.1176 | 0.1077
Epoch 95/300, Loss: 0.1170 | 0.1073
Epoch 96/300, Loss: 0.1163 | 0.1068
Epoch 97/300, Loss: 0.1159 | 0.1066
Epoch 98/300, Loss: 0.1157 | 0.1055
Epoch 99/300, Loss: 0.1162 | 0.1059
Epoch 100/300, Loss: 0.1158 | 0.1051
Epoch 101/300, Loss: 0.1163 | 0.1064
Epoch 102/300, Loss: 0.1163 | 0.1041
Epoch 103/300, Loss: 0.1167 | 0.1049
Epoch 104/300, Loss: 0.1160 | 0.1037
Epoch 105/300, Loss: 0.1156 | 0.1041
Epoch 106/300, Loss: 0.1144 | 0.1025
Epoch 107/300, Loss: 0.1140 | 0.1026
Epoch 108/300, Loss: 0.1133 | 0.1017
Epoch 109/300, Loss: 0.1125 | 0.1017
Epoch 110/300, Loss: 0.1123 | 0.1010
Epoch 111/300, Loss: 0.1122 | 0.1012
Epoch 112/300, Loss: 0.1117 | 0.1007
Epoch 113/300, Loss: 0.1109 | 0.1008
Epoch 114/300, Loss: 0.1109 | 0.0999
Epoch 115/300, Loss: 0.1105 | 0.0999
Epoch 116/300, Loss: 0.1104 | 0.0994
Epoch 117/300, Loss: 0.1106 | 0.0999
Epoch 118/300, Loss: 0.1100 | 0.0990
Epoch 119/300, Loss: 0.1095 | 0.0990
Epoch 120/300, Loss: 0.1095 | 0.0984
Epoch 121/300, Loss: 0.1091 | 0.0982
Epoch 122/300, Loss: 0.1089 | 0.0978
Epoch 123/300, Loss: 0.1096 | 0.0985
Epoch 124/300, Loss: 0.1095 | 0.0975
Epoch 125/300, Loss: 0.1091 | 0.0987
Epoch 126/300, Loss: 0.1095 | 0.0969
Epoch 127/300, Loss: 0.1088 | 0.0989
Epoch 128/300, Loss: 0.1081 | 0.0965
Epoch 129/300, Loss: 0.1069 | 0.0970
Epoch 130/300, Loss: 0.1074 | 0.0962
Epoch 131/300, Loss: 0.1067 | 0.0968
Epoch 132/300, Loss: 0.1071 | 0.0960
Epoch 133/300, Loss: 0.1065 | 0.0957
Epoch 134/300, Loss: 0.1066 | 0.0963
Epoch 135/300, Loss: 0.1061 | 0.0952
Epoch 136/300, Loss: 0.1062 | 0.0950
Epoch 137/300, Loss: 0.1063 | 0.0946
Epoch 138/300, Loss: 0.1052 | 0.0950
Epoch 139/300, Loss: 0.1052 | 0.0942
Epoch 140/300, Loss: 0.1054 | 0.0945
Epoch 141/300, Loss: 0.1053 | 0.0939
Epoch 142/300, Loss: 0.1055 | 0.0951
Epoch 143/300, Loss: 0.1056 | 0.0937
Epoch 144/300, Loss: 0.1053 | 0.0951
Epoch 145/300, Loss: 0.1053 | 0.0934
Epoch 146/300, Loss: 0.1042 | 0.0941
Epoch 147/300, Loss: 0.1037 | 0.0933
Epoch 148/300, Loss: 0.1045 | 0.0934
Epoch 149/300, Loss: 0.1038 | 0.0927
Epoch 150/300, Loss: 0.1042 | 0.0932
Epoch 151/300, Loss: 0.1044 | 0.0926
Epoch 152/300, Loss: 0.1039 | 0.0932
Epoch 153/300, Loss: 0.1032 | 0.0921
Epoch 154/300, Loss: 0.1030 | 0.0925
Epoch 155/300, Loss: 0.1033 | 0.0922
Epoch 156/300, Loss: 0.1023 | 0.0922
Epoch 157/300, Loss: 0.1025 | 0.0916
Epoch 158/300, Loss: 0.1022 | 0.0918
Epoch 159/300, Loss: 0.1027 | 0.0916
Epoch 160/300, Loss: 0.1023 | 0.0917
Epoch 161/300, Loss: 0.1026 | 0.0920
Epoch 162/300, Loss: 0.1021 | 0.0913
Epoch 163/300, Loss: 0.1025 | 0.0912
Epoch 164/300, Loss: 0.1023 | 0.0910
Epoch 165/300, Loss: 0.1020 | 0.0910
Epoch 166/300, Loss: 0.1019 | 0.0909
Epoch 167/300, Loss: 0.1010 | 0.0912
Epoch 168/300, Loss: 0.1016 | 0.0908
Epoch 169/300, Loss: 0.1011 | 0.0912
Epoch 170/300, Loss: 0.1015 | 0.0911
Epoch 171/300, Loss: 0.1009 | 0.0908
Epoch 172/300, Loss: 0.1011 | 0.0908
Epoch 173/300, Loss: 0.1013 | 0.0907
Epoch 174/300, Loss: 0.1008 | 0.0908
Epoch 175/300, Loss: 0.1006 | 0.0911
Epoch 176/300, Loss: 0.1010 | 0.0906
Epoch 177/300, Loss: 0.1003 | 0.0906
Epoch 178/300, Loss: 0.1006 | 0.0904
Epoch 179/300, Loss: 0.0997 | 0.0907
Epoch 180/300, Loss: 0.1002 | 0.0903
Epoch 181/300, Loss: 0.1006 | 0.0902
Epoch 182/300, Loss: 0.0999 | 0.0898
Epoch 183/300, Loss: 0.1000 | 0.0899
Epoch 184/300, Loss: 0.1003 | 0.0903
Epoch 185/300, Loss: 0.0999 | 0.0905
Epoch 186/300, Loss: 0.1004 | 0.0901
Epoch 187/300, Loss: 0.0995 | 0.0897
Epoch 188/300, Loss: 0.0998 | 0.0894
Epoch 189/300, Loss: 0.0995 | 0.0893
Epoch 190/300, Loss: 0.0993 | 0.0894
Epoch 191/300, Loss: 0.0995 | 0.0892
Epoch 192/300, Loss: 0.0994 | 0.0890
Epoch 193/300, Loss: 0.0993 | 0.0890
Epoch 194/300, Loss: 0.0990 | 0.0886
Epoch 195/300, Loss: 0.0992 | 0.0890
Epoch 196/300, Loss: 0.0989 | 0.0888
Epoch 197/300, Loss: 0.0988 | 0.0889
Epoch 198/300, Loss: 0.0985 | 0.0889
Epoch 199/300, Loss: 0.0982 | 0.0892
Epoch 200/300, Loss: 0.0984 | 0.0890
Epoch 201/300, Loss: 0.0987 | 0.0886
Epoch 202/300, Loss: 0.0989 | 0.0888
Epoch 203/300, Loss: 0.0985 | 0.0888
Epoch 204/300, Loss: 0.0984 | 0.0883
Epoch 205/300, Loss: 0.0978 | 0.0887
Epoch 206/300, Loss: 0.0984 | 0.0887
Epoch 207/300, Loss: 0.0985 | 0.0885
Epoch 208/300, Loss: 0.0985 | 0.0883
Epoch 209/300, Loss: 0.0984 | 0.0888
Epoch 210/300, Loss: 0.0980 | 0.0882
Epoch 211/300, Loss: 0.0981 | 0.0879
Epoch 212/300, Loss: 0.0976 | 0.0882
Epoch 213/300, Loss: 0.0982 | 0.0882
Epoch 214/300, Loss: 0.0982 | 0.0883
Epoch 215/300, Loss: 0.0971 | 0.0883
Epoch 216/300, Loss: 0.0979 | 0.0879
Epoch 217/300, Loss: 0.0977 | 0.0878
Epoch 218/300, Loss: 0.0976 | 0.0879
Epoch 219/300, Loss: 0.0974 | 0.0876
Epoch 220/300, Loss: 0.0978 | 0.0876
Epoch 221/300, Loss: 0.0969 | 0.0881
Epoch 222/300, Loss: 0.0979 | 0.0878
Epoch 223/300, Loss: 0.0977 | 0.0876
Epoch 224/300, Loss: 0.0974 | 0.0877
Epoch 225/300, Loss: 0.0971 | 0.0876
Epoch 226/300, Loss: 0.0973 | 0.0876
Epoch 227/300, Loss: 0.0970 | 0.0875
Epoch 228/300, Loss: 0.0968 | 0.0875
Epoch 229/300, Loss: 0.0972 | 0.0876
Epoch 230/300, Loss: 0.0970 | 0.0873
Epoch 231/300, Loss: 0.0964 | 0.0874
Epoch 232/300, Loss: 0.0965 | 0.0874
Epoch 233/300, Loss: 0.0966 | 0.0873
Epoch 234/300, Loss: 0.0966 | 0.0874
Epoch 235/300, Loss: 0.0970 | 0.0876
Epoch 236/300, Loss: 0.0968 | 0.0878
Epoch 237/300, Loss: 0.0961 | 0.0876
Epoch 238/300, Loss: 0.0973 | 0.0873
Epoch 239/300, Loss: 0.0966 | 0.0873
Epoch 240/300, Loss: 0.0970 | 0.0871
Epoch 241/300, Loss: 0.0969 | 0.0870
Epoch 242/300, Loss: 0.0973 | 0.0870
Epoch 243/300, Loss: 0.0963 | 0.0870
Epoch 244/300, Loss: 0.0965 | 0.0871
Epoch 245/300, Loss: 0.0968 | 0.0872
Epoch 246/300, Loss: 0.0966 | 0.0873
Epoch 247/300, Loss: 0.0960 | 0.0872
Epoch 248/300, Loss: 0.0963 | 0.0871
Epoch 249/300, Loss: 0.0969 | 0.0870
Epoch 250/300, Loss: 0.0957 | 0.0869
Epoch 251/300, Loss: 0.0966 | 0.0871
Epoch 252/300, Loss: 0.0960 | 0.0875
Epoch 253/300, Loss: 0.0964 | 0.0868
Epoch 254/300, Loss: 0.0962 | 0.0869
Epoch 255/300, Loss: 0.0964 | 0.0869
Epoch 256/300, Loss: 0.0958 | 0.0868
Epoch 257/300, Loss: 0.0962 | 0.0872
Epoch 258/300, Loss: 0.0961 | 0.0869
Epoch 259/300, Loss: 0.0958 | 0.0869
Epoch 260/300, Loss: 0.0955 | 0.0868
Epoch 261/300, Loss: 0.0958 | 0.0867
Epoch 262/300, Loss: 0.0957 | 0.0870
Epoch 263/300, Loss: 0.0959 | 0.0866
Epoch 264/300, Loss: 0.0962 | 0.0866
Epoch 265/300, Loss: 0.0956 | 0.0867
Epoch 266/300, Loss: 0.0956 | 0.0865
Epoch 267/300, Loss: 0.0953 | 0.0864
Epoch 268/300, Loss: 0.0955 | 0.0863
Epoch 269/300, Loss: 0.0961 | 0.0863
Epoch 270/300, Loss: 0.0949 | 0.0863
Epoch 271/300, Loss: 0.0949 | 0.0865
Epoch 272/300, Loss: 0.0955 | 0.0863
Epoch 273/300, Loss: 0.0953 | 0.0861
Epoch 274/300, Loss: 0.0954 | 0.0860
Epoch 275/300, Loss: 0.0948 | 0.0862
Epoch 276/300, Loss: 0.0950 | 0.0864
Epoch 277/300, Loss: 0.0953 | 0.0864
Epoch 278/300, Loss: 0.0954 | 0.0863
Epoch 279/300, Loss: 0.0959 | 0.0862
Epoch 280/300, Loss: 0.0948 | 0.0860
Epoch 281/300, Loss: 0.0955 | 0.0861
Epoch 282/300, Loss: 0.0952 | 0.0862
Epoch 283/300, Loss: 0.0951 | 0.0861
Epoch 284/300, Loss: 0.0951 | 0.0860
Epoch 285/300, Loss: 0.0950 | 0.0860
Epoch 286/300, Loss: 0.0952 | 0.0862
Epoch 287/300, Loss: 0.0949 | 0.0865
Epoch 288/300, Loss: 0.0952 | 0.0863
Epoch 289/300, Loss: 0.0948 | 0.0860
Epoch 290/300, Loss: 0.0951 | 0.0859
Epoch 291/300, Loss: 0.0951 | 0.0860
Epoch 292/300, Loss: 0.0952 | 0.0860
Epoch 293/300, Loss: 0.0949 | 0.0858
Epoch 294/300, Loss: 0.0952 | 0.0858
Epoch 295/300, Loss: 0.0947 | 0.0860
Epoch 296/300, Loss: 0.0955 | 0.0860
Epoch 297/300, Loss: 0.0953 | 0.0861
Epoch 298/300, Loss: 0.0955 | 0.0859
Epoch 299/300, Loss: 0.0949 | 0.0860
Epoch 300/300, Loss: 0.0948 | 0.0860
Runtime (seconds): 123.38382959365845
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1019.4893532395363
RMSE: 31.929443359375
MAE: 31.929443359375
R-squared: nan
[216.03056]
