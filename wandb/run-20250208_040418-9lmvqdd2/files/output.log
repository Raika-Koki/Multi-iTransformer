[32m[I 2025-02-08 04:04:23,079][0m A new study created in memory with name: no-name-9be62898-6422-4219-8994-46a2a2d591c2[0m
[32m[I 2025-02-08 04:04:50,624][0m Trial 0 finished with value: 0.24626024067401886 and parameters: {'observation_period_num': 201, 'train_rates': 0.9244284037055965, 'learning_rate': 0.00010944358395616468, 'batch_size': 232, 'step_size': 6, 'gamma': 0.8743569580166086}. Best is trial 0 with value: 0.24626024067401886.[0m
[32m[I 2025-02-08 04:07:13,848][0m Trial 1 finished with value: 0.6239348750282853 and parameters: {'observation_period_num': 105, 'train_rates': 0.7506790794542607, 'learning_rate': 2.7981084467989108e-06, 'batch_size': 34, 'step_size': 3, 'gamma': 0.8300861464644993}. Best is trial 0 with value: 0.24626024067401886.[0m
[32m[I 2025-02-08 04:07:44,675][0m Trial 2 finished with value: 0.8701351889334489 and parameters: {'observation_period_num': 252, 'train_rates': 0.7913628304696269, 'learning_rate': 1.106285145919625e-06, 'batch_size': 170, 'step_size': 15, 'gamma': 0.9409755467619305}. Best is trial 0 with value: 0.24626024067401886.[0m
[32m[I 2025-02-08 04:08:20,118][0m Trial 3 finished with value: 0.23319863506725858 and parameters: {'observation_period_num': 102, 'train_rates': 0.6861434253610632, 'learning_rate': 0.0001725180468889061, 'batch_size': 144, 'step_size': 12, 'gamma': 0.9328520452617529}. Best is trial 3 with value: 0.23319863506725858.[0m
[32m[I 2025-02-08 04:10:30,529][0m Trial 4 finished with value: 0.4346690611405806 and parameters: {'observation_period_num': 156, 'train_rates': 0.84551042877834, 'learning_rate': 3.3637223232254372e-06, 'batch_size': 41, 'step_size': 2, 'gamma': 0.946530721985708}. Best is trial 3 with value: 0.23319863506725858.[0m
[32m[I 2025-02-08 04:11:13,432][0m Trial 5 finished with value: 0.6190464691239961 and parameters: {'observation_period_num': 91, 'train_rates': 0.8484155277622518, 'learning_rate': 3.4077631240803083e-06, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8843226656194543}. Best is trial 3 with value: 0.23319863506725858.[0m
[32m[I 2025-02-08 04:11:40,632][0m Trial 6 finished with value: 0.2078663545470018 and parameters: {'observation_period_num': 80, 'train_rates': 0.6806818462667115, 'learning_rate': 7.190535012581328e-05, 'batch_size': 190, 'step_size': 14, 'gamma': 0.8821375930090063}. Best is trial 6 with value: 0.2078663545470018.[0m
[32m[I 2025-02-08 04:12:06,482][0m Trial 7 finished with value: 0.7145725606303466 and parameters: {'observation_period_num': 57, 'train_rates': 0.6129409265114376, 'learning_rate': 1.4173780593523285e-06, 'batch_size': 197, 'step_size': 10, 'gamma': 0.9599726029100635}. Best is trial 6 with value: 0.2078663545470018.[0m
Early stopping at epoch 50
[32m[I 2025-02-08 04:12:44,561][0m Trial 8 finished with value: 1.1356199960708617 and parameters: {'observation_period_num': 235, 'train_rates': 0.9098020376000051, 'learning_rate': 6.0425939188344136e-06, 'batch_size': 74, 'step_size': 1, 'gamma': 0.7870605490205611}. Best is trial 6 with value: 0.2078663545470018.[0m
[32m[I 2025-02-08 04:13:20,219][0m Trial 9 finished with value: 0.8730491594939976 and parameters: {'observation_period_num': 123, 'train_rates': 0.6063008593490519, 'learning_rate': 1.653797910653547e-06, 'batch_size': 129, 'step_size': 9, 'gamma': 0.9162919964844696}. Best is trial 6 with value: 0.2078663545470018.[0m
[32m[I 2025-02-08 04:13:43,076][0m Trial 10 finished with value: 0.1377396876282727 and parameters: {'observation_period_num': 6, 'train_rates': 0.7066839036936414, 'learning_rate': 0.0005896430817886766, 'batch_size': 247, 'step_size': 15, 'gamma': 0.820453576766717}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:14:05,941][0m Trial 11 finished with value: 0.14657994760538484 and parameters: {'observation_period_num': 16, 'train_rates': 0.6956666166021057, 'learning_rate': 0.0007459796868389526, 'batch_size': 255, 'step_size': 15, 'gamma': 0.8235790074899898}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:14:28,293][0m Trial 12 finished with value: 0.14069213710934503 and parameters: {'observation_period_num': 8, 'train_rates': 0.690678844483489, 'learning_rate': 0.0006126293355413261, 'batch_size': 254, 'step_size': 13, 'gamma': 0.7522088086954697}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:14:52,540][0m Trial 13 finished with value: 0.1529867425750775 and parameters: {'observation_period_num': 13, 'train_rates': 0.7373255761802598, 'learning_rate': 0.0009048030987998185, 'batch_size': 228, 'step_size': 12, 'gamma': 0.7512848600149034}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:15:13,182][0m Trial 14 finished with value: 0.17308649618496041 and parameters: {'observation_period_num': 48, 'train_rates': 0.6543866494244434, 'learning_rate': 0.000299355650898006, 'batch_size': 256, 'step_size': 13, 'gamma': 0.7638137412882084}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:15:39,226][0m Trial 15 finished with value: 0.3162151883236591 and parameters: {'observation_period_num': 42, 'train_rates': 0.7711930375536166, 'learning_rate': 2.3577317451918744e-05, 'batch_size': 219, 'step_size': 11, 'gamma': 0.8103045273132368}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:16:30,257][0m Trial 16 finished with value: 0.14590989968938223 and parameters: {'observation_period_num': 8, 'train_rates': 0.7215296217534131, 'learning_rate': 0.0003894697723877993, 'batch_size': 100, 'step_size': 7, 'gamma': 0.8486631385655847}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:16:58,535][0m Trial 17 finished with value: 0.3693556744782698 and parameters: {'observation_period_num': 140, 'train_rates': 0.8238532743906783, 'learning_rate': 2.989508403667146e-05, 'batch_size': 197, 'step_size': 5, 'gamma': 0.781045441765676}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:17:27,934][0m Trial 18 finished with value: 0.15408723056316376 and parameters: {'observation_period_num': 58, 'train_rates': 0.9851087916788559, 'learning_rate': 6.433166564102645e-05, 'batch_size': 232, 'step_size': 13, 'gamma': 0.7981236819681615}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:17:58,494][0m Trial 19 finished with value: 0.24815462378423606 and parameters: {'observation_period_num': 176, 'train_rates': 0.6443865073508858, 'learning_rate': 0.000422757982124464, 'batch_size': 158, 'step_size': 15, 'gamma': 0.8482814786359675}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:18:25,496][0m Trial 20 finished with value: 0.5615291782987049 and parameters: {'observation_period_num': 32, 'train_rates': 0.7121889119217661, 'learning_rate': 9.130566546467128e-06, 'batch_size': 208, 'step_size': 8, 'gamma': 0.7701269466815293}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:19:17,014][0m Trial 21 finished with value: 0.1460883957065322 and parameters: {'observation_period_num': 10, 'train_rates': 0.7277353420783441, 'learning_rate': 0.00038192435682998034, 'batch_size': 99, 'step_size': 7, 'gamma': 0.8524563462147965}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:20:05,646][0m Trial 22 finished with value: 0.19229333909299873 and parameters: {'observation_period_num': 72, 'train_rates': 0.6527523492612292, 'learning_rate': 0.0002564448709409935, 'batch_size': 98, 'step_size': 6, 'gamma': 0.9095523043539346}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:21:25,916][0m Trial 23 finished with value: 0.18741795110026363 and parameters: {'observation_period_num': 24, 'train_rates': 0.7674954793977093, 'learning_rate': 0.0006395299569275216, 'batch_size': 67, 'step_size': 8, 'gamma': 0.9819751759876851}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:22:10,001][0m Trial 24 finished with value: 0.17615539870804692 and parameters: {'observation_period_num': 35, 'train_rates': 0.707208235402641, 'learning_rate': 0.00016387764607520735, 'batch_size': 119, 'step_size': 4, 'gamma': 0.8168869306775612}. Best is trial 10 with value: 0.1377396876282727.[0m
[32m[I 2025-02-08 04:22:40,109][0m Trial 25 finished with value: 0.13151277365608954 and parameters: {'observation_period_num': 6, 'train_rates': 0.6682450552900457, 'learning_rate': 0.0009812449278499965, 'batch_size': 175, 'step_size': 13, 'gamma': 0.8512968997170782}. Best is trial 25 with value: 0.13151277365608954.[0m
[32m[I 2025-02-08 04:23:02,175][0m Trial 26 finished with value: 0.20075521999608567 and parameters: {'observation_period_num': 69, 'train_rates': 0.631142919525513, 'learning_rate': 0.0006797845270111176, 'batch_size': 242, 'step_size': 13, 'gamma': 0.8381675916022787}. Best is trial 25 with value: 0.13151277365608954.[0m
[32m[I 2025-02-08 04:23:32,038][0m Trial 27 finished with value: 0.14955104774139957 and parameters: {'observation_period_num': 32, 'train_rates': 0.6679049370043474, 'learning_rate': 0.00019983192327323036, 'batch_size': 176, 'step_size': 14, 'gamma': 0.8037198451155598}. Best is trial 25 with value: 0.13151277365608954.[0m
[32m[I 2025-02-08 04:23:55,480][0m Trial 28 finished with value: 0.11667977751280317 and parameters: {'observation_period_num': 5, 'train_rates': 0.6216316485338597, 'learning_rate': 0.00093990816688554, 'batch_size': 211, 'step_size': 11, 'gamma': 0.7505038077855037}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:24:18,524][0m Trial 29 finished with value: 0.25711506469683215 and parameters: {'observation_period_num': 200, 'train_rates': 0.6270603019277213, 'learning_rate': 0.0009369397645711906, 'batch_size': 210, 'step_size': 11, 'gamma': 0.8730877379588049}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:24:46,425][0m Trial 30 finished with value: 0.6461555007224812 and parameters: {'observation_period_num': 47, 'train_rates': 0.603537333281825, 'learning_rate': 7.040581194527195e-05, 'batch_size': 177, 'step_size': 11, 'gamma': 0.8973648744078184}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:25:09,346][0m Trial 31 finished with value: 0.13674929570615962 and parameters: {'observation_period_num': 7, 'train_rates': 0.673124615988596, 'learning_rate': 0.0004994257571979666, 'batch_size': 244, 'step_size': 14, 'gamma': 0.7578593105705786}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:25:32,502][0m Trial 32 finished with value: 0.14235319450209316 and parameters: {'observation_period_num': 23, 'train_rates': 0.6701438180297496, 'learning_rate': 0.0004765306284596233, 'batch_size': 233, 'step_size': 14, 'gamma': 0.8626228763569924}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:25:56,069][0m Trial 33 finished with value: 0.12115251141759108 and parameters: {'observation_period_num': 6, 'train_rates': 0.6376495766671324, 'learning_rate': 0.0009178150891509059, 'batch_size': 216, 'step_size': 12, 'gamma': 0.7823809690261091}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:26:28,900][0m Trial 34 finished with value: 0.13135446063627035 and parameters: {'observation_period_num': 21, 'train_rates': 0.6299172531318958, 'learning_rate': 0.0009986619667384105, 'batch_size': 154, 'step_size': 12, 'gamma': 0.7726435955814407}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:26:58,924][0m Trial 35 finished with value: 0.13858227235473455 and parameters: {'observation_period_num': 27, 'train_rates': 0.6318079053896865, 'learning_rate': 0.0009659669834475479, 'batch_size': 157, 'step_size': 12, 'gamma': 0.7764781823598688}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:27:27,897][0m Trial 36 finished with value: 1.0712473663182407 and parameters: {'observation_period_num': 110, 'train_rates': 0.600276871840986, 'learning_rate': 0.0001043247973659152, 'batch_size': 158, 'step_size': 10, 'gamma': 0.7962513900890319}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:27:55,019][0m Trial 37 finished with value: 0.16932448114506726 and parameters: {'observation_period_num': 54, 'train_rates': 0.6497493074932248, 'learning_rate': 0.0002747620409588108, 'batch_size': 186, 'step_size': 12, 'gamma': 0.7880730934476139}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:28:18,408][0m Trial 38 finished with value: 0.6247865420476675 and parameters: {'observation_period_num': 85, 'train_rates': 0.6236748929814901, 'learning_rate': 0.00013131933775758584, 'batch_size': 209, 'step_size': 9, 'gamma': 0.7658984760779898}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:28:56,066][0m Trial 39 finished with value: 0.16343240956512262 and parameters: {'observation_period_num': 24, 'train_rates': 0.7508313769754189, 'learning_rate': 0.00030186065657260633, 'batch_size': 145, 'step_size': 11, 'gamma': 0.777988989210253}. Best is trial 28 with value: 0.11667977751280317.[0m
[32m[I 2025-02-08 04:29:26,236][0m Trial 40 finished with value: 0.059536532578993 and parameters: {'observation_period_num': 42, 'train_rates': 0.7997616322824557, 'learning_rate': 0.0009326936095391206, 'batch_size': 193, 'step_size': 9, 'gamma': 0.8362598310184227}. Best is trial 40 with value: 0.059536532578993.[0m
[32m[I 2025-02-08 04:29:57,191][0m Trial 41 finished with value: 0.05501388778413557 and parameters: {'observation_period_num': 38, 'train_rates': 0.8113978506038312, 'learning_rate': 0.0009427843697036246, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8366066556542407}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:30:27,120][0m Trial 42 finished with value: 0.0876460087299347 and parameters: {'observation_period_num': 63, 'train_rates': 0.821446882232021, 'learning_rate': 0.0006961307459942606, 'batch_size': 189, 'step_size': 9, 'gamma': 0.8300546237365415}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:30:56,910][0m Trial 43 finished with value: 0.11034230466890513 and parameters: {'observation_period_num': 64, 'train_rates': 0.8169008853451092, 'learning_rate': 0.0006260187050920611, 'batch_size': 198, 'step_size': 9, 'gamma': 0.8330286598698137}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:31:26,550][0m Trial 44 finished with value: 0.1247886030384505 and parameters: {'observation_period_num': 104, 'train_rates': 0.819593041175593, 'learning_rate': 0.0005277039647391519, 'batch_size': 191, 'step_size': 9, 'gamma': 0.838528810372037}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:31:59,954][0m Trial 45 finished with value: 0.07441754429105749 and parameters: {'observation_period_num': 66, 'train_rates': 0.8720513202122924, 'learning_rate': 0.00021209824977760434, 'batch_size': 185, 'step_size': 10, 'gamma': 0.831989523168087}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:32:36,059][0m Trial 46 finished with value: 0.06696309970842826 and parameters: {'observation_period_num': 72, 'train_rates': 0.8736770661448572, 'learning_rate': 0.0003532301561606549, 'batch_size': 168, 'step_size': 7, 'gamma': 0.8325122566428854}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:33:12,371][0m Trial 47 finished with value: 0.08715329799124762 and parameters: {'observation_period_num': 94, 'train_rates': 0.8808781486925823, 'learning_rate': 0.00020818882271142976, 'batch_size': 169, 'step_size': 7, 'gamma': 0.8634800584597193}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:33:48,888][0m Trial 48 finished with value: 0.11487237620302018 and parameters: {'observation_period_num': 94, 'train_rates': 0.8810853425204731, 'learning_rate': 0.0001159596401012676, 'batch_size': 165, 'step_size': 7, 'gamma': 0.8636476422225488}. Best is trial 41 with value: 0.05501388778413557.[0m
[32m[I 2025-02-08 04:34:31,828][0m Trial 49 finished with value: 0.0718510278048688 and parameters: {'observation_period_num': 80, 'train_rates': 0.8724591571453044, 'learning_rate': 0.00021473210364709764, 'batch_size': 141, 'step_size': 6, 'gamma': 0.8833275714284952}. Best is trial 41 with value: 0.05501388778413557.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.6723 | 0.3568
Epoch 2/300, Loss: 0.2843 | 0.2788
Epoch 3/300, Loss: 0.2794 | 0.3958
Epoch 4/300, Loss: 0.2736 | 0.5677
Epoch 5/300, Loss: 0.2884 | 0.2885
Epoch 6/300, Loss: 0.3544 | 0.3295
Epoch 7/300, Loss: 0.1931 | 0.2678
Epoch 8/300, Loss: 0.1573 | 0.1794
Epoch 9/300, Loss: 0.1347 | 0.1798
Epoch 10/300, Loss: 0.1278 | 0.1127
Epoch 11/300, Loss: 0.1172 | 0.1247
Epoch 12/300, Loss: 0.1139 | 0.0953
Epoch 13/300, Loss: 0.1121 | 0.0959
Epoch 14/300, Loss: 0.1181 | 0.0959
Epoch 15/300, Loss: 0.1204 | 0.1810
Epoch 16/300, Loss: 0.1172 | 0.1074
Epoch 17/300, Loss: 0.1105 | 0.0804
Epoch 18/300, Loss: 0.1090 | 0.0801
Epoch 19/300, Loss: 0.1078 | 0.0794
Epoch 20/300, Loss: 0.0992 | 0.0902
Epoch 21/300, Loss: 0.0955 | 0.0932
Epoch 22/300, Loss: 0.1074 | 0.2181
Epoch 23/300, Loss: 0.1088 | 0.1036
Epoch 24/300, Loss: 0.1026 | 0.1095
Epoch 25/300, Loss: 0.1295 | 0.1771
Epoch 26/300, Loss: 0.1559 | 0.1329
Epoch 27/300, Loss: 0.1824 | 0.4704
Epoch 28/300, Loss: 0.1717 | 0.2358
Epoch 29/300, Loss: 0.1365 | 0.2109
Epoch 30/300, Loss: 0.1399 | 0.1725
Epoch 31/300, Loss: 0.1148 | 0.1605
Epoch 32/300, Loss: 0.1123 | 0.1097
Epoch 33/300, Loss: 0.1051 | 0.1540
Epoch 34/300, Loss: 0.1047 | 0.1075
Epoch 35/300, Loss: 0.1028 | 0.1310
Epoch 36/300, Loss: 0.1008 | 0.1027
Epoch 37/300, Loss: 0.0951 | 0.0961
Epoch 38/300, Loss: 0.0914 | 0.0821
Epoch 39/300, Loss: 0.0868 | 0.0726
Epoch 40/300, Loss: 0.0840 | 0.0671
Epoch 41/300, Loss: 0.0826 | 0.0650
Epoch 42/300, Loss: 0.0825 | 0.0623
Epoch 43/300, Loss: 0.0824 | 0.0658
Epoch 44/300, Loss: 0.0870 | 0.0762
Epoch 45/300, Loss: 0.0896 | 0.0742
Epoch 46/300, Loss: 0.0961 | 0.1452
Epoch 47/300, Loss: 0.0898 | 0.0778
Epoch 48/300, Loss: 0.0838 | 0.0728
Epoch 49/300, Loss: 0.0830 | 0.0706
Epoch 50/300, Loss: 0.0818 | 0.0586
Epoch 51/300, Loss: 0.0847 | 0.1052
Epoch 52/300, Loss: 0.0836 | 0.0615
Epoch 53/300, Loss: 0.0848 | 0.1183
Epoch 54/300, Loss: 0.0808 | 0.0634
Epoch 55/300, Loss: 0.0776 | 0.0704
Epoch 56/300, Loss: 0.0760 | 0.0601
Epoch 57/300, Loss: 0.0752 | 0.0581
Epoch 58/300, Loss: 0.0749 | 0.0588
Epoch 59/300, Loss: 0.0745 | 0.0559
Epoch 60/300, Loss: 0.0744 | 0.0590
Epoch 61/300, Loss: 0.0740 | 0.0550
Epoch 62/300, Loss: 0.0740 | 0.0588
Epoch 63/300, Loss: 0.0737 | 0.0544
Epoch 64/300, Loss: 0.0736 | 0.0591
Epoch 65/300, Loss: 0.0732 | 0.0539
Epoch 66/300, Loss: 0.0732 | 0.0583
Epoch 67/300, Loss: 0.0729 | 0.0535
Epoch 68/300, Loss: 0.0730 | 0.0583
Epoch 69/300, Loss: 0.0726 | 0.0532
Epoch 70/300, Loss: 0.0726 | 0.0578
Epoch 71/300, Loss: 0.0723 | 0.0529
Epoch 72/300, Loss: 0.0723 | 0.0570
Epoch 73/300, Loss: 0.0719 | 0.0527
Epoch 74/300, Loss: 0.0719 | 0.0558
Epoch 75/300, Loss: 0.0717 | 0.0527
Epoch 76/300, Loss: 0.0716 | 0.0547
Epoch 77/300, Loss: 0.0714 | 0.0526
Epoch 78/300, Loss: 0.0713 | 0.0540
Epoch 79/300, Loss: 0.0712 | 0.0527
Epoch 80/300, Loss: 0.0711 | 0.0533
Epoch 81/300, Loss: 0.0710 | 0.0527
Epoch 82/300, Loss: 0.0709 | 0.0530
Epoch 83/300, Loss: 0.0708 | 0.0527
Epoch 84/300, Loss: 0.0707 | 0.0527
Epoch 85/300, Loss: 0.0707 | 0.0526
Epoch 86/300, Loss: 0.0706 | 0.0525
Epoch 87/300, Loss: 0.0705 | 0.0525
Epoch 88/300, Loss: 0.0704 | 0.0524
Epoch 89/300, Loss: 0.0704 | 0.0523
Epoch 90/300, Loss: 0.0703 | 0.0523
Epoch 91/300, Loss: 0.0702 | 0.0522
Epoch 92/300, Loss: 0.0702 | 0.0522
Epoch 93/300, Loss: 0.0701 | 0.0521
Epoch 94/300, Loss: 0.0701 | 0.0521
Epoch 95/300, Loss: 0.0700 | 0.0520
Epoch 96/300, Loss: 0.0700 | 0.0520
Epoch 97/300, Loss: 0.0699 | 0.0519
Epoch 98/300, Loss: 0.0699 | 0.0519
Epoch 99/300, Loss: 0.0698 | 0.0518
Epoch 100/300, Loss: 0.0698 | 0.0518
Epoch 101/300, Loss: 0.0697 | 0.0518
Epoch 102/300, Loss: 0.0697 | 0.0517
Epoch 103/300, Loss: 0.0696 | 0.0517
Epoch 104/300, Loss: 0.0696 | 0.0517
Epoch 105/300, Loss: 0.0696 | 0.0516
Epoch 106/300, Loss: 0.0695 | 0.0516
Epoch 107/300, Loss: 0.0695 | 0.0516
Epoch 108/300, Loss: 0.0695 | 0.0515
Epoch 109/300, Loss: 0.0694 | 0.0515
Epoch 110/300, Loss: 0.0694 | 0.0515
Epoch 111/300, Loss: 0.0694 | 0.0515
Epoch 112/300, Loss: 0.0693 | 0.0514
Epoch 113/300, Loss: 0.0693 | 0.0514
Epoch 114/300, Loss: 0.0693 | 0.0514
Epoch 115/300, Loss: 0.0693 | 0.0514
Epoch 116/300, Loss: 0.0692 | 0.0513
Epoch 117/300, Loss: 0.0692 | 0.0513
Epoch 118/300, Loss: 0.0692 | 0.0513
Epoch 119/300, Loss: 0.0692 | 0.0513
Epoch 120/300, Loss: 0.0691 | 0.0513
Epoch 121/300, Loss: 0.0691 | 0.0512
Epoch 122/300, Loss: 0.0691 | 0.0512
Epoch 123/300, Loss: 0.0691 | 0.0512
Epoch 124/300, Loss: 0.0691 | 0.0512
Epoch 125/300, Loss: 0.0690 | 0.0512
Epoch 126/300, Loss: 0.0690 | 0.0512
Epoch 127/300, Loss: 0.0690 | 0.0512
Epoch 128/300, Loss: 0.0690 | 0.0511
Epoch 129/300, Loss: 0.0690 | 0.0511
Epoch 130/300, Loss: 0.0690 | 0.0511
Epoch 131/300, Loss: 0.0690 | 0.0511
Epoch 132/300, Loss: 0.0689 | 0.0511
Epoch 133/300, Loss: 0.0689 | 0.0511
Epoch 134/300, Loss: 0.0689 | 0.0511
Epoch 135/300, Loss: 0.0689 | 0.0511
Epoch 136/300, Loss: 0.0689 | 0.0511
Epoch 137/300, Loss: 0.0689 | 0.0510
Epoch 138/300, Loss: 0.0689 | 0.0510
Epoch 139/300, Loss: 0.0689 | 0.0510
Epoch 140/300, Loss: 0.0688 | 0.0510
Epoch 141/300, Loss: 0.0688 | 0.0510
Epoch 142/300, Loss: 0.0688 | 0.0510
Epoch 143/300, Loss: 0.0688 | 0.0510
Epoch 144/300, Loss: 0.0688 | 0.0510
Epoch 145/300, Loss: 0.0688 | 0.0510
Epoch 146/300, Loss: 0.0688 | 0.0510
Epoch 147/300, Loss: 0.0688 | 0.0510
Epoch 148/300, Loss: 0.0688 | 0.0510
Epoch 149/300, Loss: 0.0688 | 0.0510
Epoch 150/300, Loss: 0.0688 | 0.0509
Epoch 151/300, Loss: 0.0688 | 0.0509
Epoch 152/300, Loss: 0.0687 | 0.0509
Epoch 153/300, Loss: 0.0687 | 0.0509
Epoch 154/300, Loss: 0.0687 | 0.0509
Epoch 155/300, Loss: 0.0687 | 0.0509
Epoch 156/300, Loss: 0.0687 | 0.0509
Epoch 157/300, Loss: 0.0687 | 0.0509
Epoch 158/300, Loss: 0.0687 | 0.0509
Epoch 159/300, Loss: 0.0687 | 0.0509
Epoch 160/300, Loss: 0.0687 | 0.0509
Epoch 161/300, Loss: 0.0687 | 0.0509
Epoch 162/300, Loss: 0.0687 | 0.0509
Epoch 163/300, Loss: 0.0687 | 0.0509
Epoch 164/300, Loss: 0.0687 | 0.0509
Epoch 165/300, Loss: 0.0687 | 0.0509
Epoch 166/300, Loss: 0.0687 | 0.0509
Epoch 167/300, Loss: 0.0687 | 0.0509
Epoch 168/300, Loss: 0.0687 | 0.0509
Epoch 169/300, Loss: 0.0687 | 0.0509
Epoch 170/300, Loss: 0.0687 | 0.0509
Epoch 171/300, Loss: 0.0687 | 0.0509
Epoch 172/300, Loss: 0.0686 | 0.0509
Epoch 173/300, Loss: 0.0686 | 0.0509
Epoch 174/300, Loss: 0.0686 | 0.0508
Epoch 175/300, Loss: 0.0686 | 0.0508
Epoch 176/300, Loss: 0.0686 | 0.0508
Epoch 177/300, Loss: 0.0686 | 0.0508
Epoch 178/300, Loss: 0.0686 | 0.0508
Epoch 179/300, Loss: 0.0686 | 0.0508
Epoch 180/300, Loss: 0.0686 | 0.0508
Epoch 181/300, Loss: 0.0686 | 0.0508
Epoch 182/300, Loss: 0.0686 | 0.0508
Epoch 183/300, Loss: 0.0686 | 0.0508
Epoch 184/300, Loss: 0.0686 | 0.0508
Epoch 185/300, Loss: 0.0686 | 0.0508
Epoch 186/300, Loss: 0.0686 | 0.0508
Epoch 187/300, Loss: 0.0686 | 0.0508
Epoch 188/300, Loss: 0.0686 | 0.0508
Epoch 189/300, Loss: 0.0686 | 0.0508
Epoch 190/300, Loss: 0.0686 | 0.0508
Epoch 191/300, Loss: 0.0686 | 0.0508
Epoch 192/300, Loss: 0.0686 | 0.0508
Epoch 193/300, Loss: 0.0686 | 0.0508
Epoch 194/300, Loss: 0.0686 | 0.0508
Epoch 195/300, Loss: 0.0686 | 0.0508
Epoch 196/300, Loss: 0.0686 | 0.0508
Epoch 197/300, Loss: 0.0686 | 0.0508
Epoch 198/300, Loss: 0.0686 | 0.0508
Epoch 199/300, Loss: 0.0686 | 0.0508
Epoch 200/300, Loss: 0.0686 | 0.0508
Epoch 201/300, Loss: 0.0686 | 0.0508
Epoch 202/300, Loss: 0.0686 | 0.0508
Epoch 203/300, Loss: 0.0686 | 0.0508
Epoch 204/300, Loss: 0.0686 | 0.0508
Epoch 205/300, Loss: 0.0686 | 0.0508
Epoch 206/300, Loss: 0.0686 | 0.0508
Epoch 207/300, Loss: 0.0686 | 0.0508
Epoch 208/300, Loss: 0.0686 | 0.0508
Epoch 209/300, Loss: 0.0686 | 0.0508
Epoch 210/300, Loss: 0.0686 | 0.0508
Epoch 211/300, Loss: 0.0686 | 0.0508
Epoch 212/300, Loss: 0.0686 | 0.0508
Epoch 213/300, Loss: 0.0686 | 0.0508
Epoch 214/300, Loss: 0.0686 | 0.0508
Epoch 215/300, Loss: 0.0686 | 0.0508
Epoch 216/300, Loss: 0.0686 | 0.0508
Epoch 217/300, Loss: 0.0686 | 0.0508
Epoch 218/300, Loss: 0.0686 | 0.0508
Epoch 219/300, Loss: 0.0686 | 0.0508
Epoch 220/300, Loss: 0.0686 | 0.0508
Epoch 221/300, Loss: 0.0686 | 0.0508
Epoch 222/300, Loss: 0.0686 | 0.0508
Epoch 223/300, Loss: 0.0686 | 0.0508
Epoch 224/300, Loss: 0.0686 | 0.0508
Epoch 225/300, Loss: 0.0686 | 0.0508
Epoch 226/300, Loss: 0.0686 | 0.0508
Epoch 227/300, Loss: 0.0686 | 0.0508
Epoch 228/300, Loss: 0.0686 | 0.0508
Epoch 229/300, Loss: 0.0686 | 0.0508
Epoch 230/300, Loss: 0.0686 | 0.0508
Epoch 231/300, Loss: 0.0686 | 0.0508
Epoch 232/300, Loss: 0.0686 | 0.0508
Epoch 233/300, Loss: 0.0686 | 0.0508
Epoch 234/300, Loss: 0.0686 | 0.0508
Epoch 235/300, Loss: 0.0686 | 0.0508
Epoch 236/300, Loss: 0.0686 | 0.0508
Epoch 237/300, Loss: 0.0686 | 0.0508
Epoch 238/300, Loss: 0.0686 | 0.0508
Epoch 239/300, Loss: 0.0686 | 0.0508
Epoch 240/300, Loss: 0.0686 | 0.0508
Epoch 241/300, Loss: 0.0686 | 0.0508
Epoch 242/300, Loss: 0.0686 | 0.0508
Epoch 243/300, Loss: 0.0686 | 0.0508
Epoch 244/300, Loss: 0.0686 | 0.0508
Epoch 245/300, Loss: 0.0686 | 0.0508
Epoch 246/300, Loss: 0.0686 | 0.0508
Epoch 247/300, Loss: 0.0686 | 0.0508
Epoch 248/300, Loss: 0.0686 | 0.0508
Epoch 249/300, Loss: 0.0686 | 0.0508
Epoch 250/300, Loss: 0.0686 | 0.0508
Epoch 251/300, Loss: 0.0686 | 0.0508
Epoch 252/300, Loss: 0.0686 | 0.0508
Epoch 253/300, Loss: 0.0686 | 0.0508
Epoch 254/300, Loss: 0.0686 | 0.0508
Epoch 255/300, Loss: 0.0686 | 0.0508
Epoch 256/300, Loss: 0.0686 | 0.0508
Epoch 257/300, Loss: 0.0686 | 0.0508
Epoch 258/300, Loss: 0.0686 | 0.0508
Epoch 259/300, Loss: 0.0686 | 0.0508
Epoch 260/300, Loss: 0.0686 | 0.0508
Epoch 261/300, Loss: 0.0686 | 0.0508
Epoch 262/300, Loss: 0.0686 | 0.0508
Epoch 263/300, Loss: 0.0686 | 0.0508
Epoch 264/300, Loss: 0.0686 | 0.0508
Epoch 265/300, Loss: 0.0686 | 0.0508
Epoch 266/300, Loss: 0.0686 | 0.0508
Epoch 267/300, Loss: 0.0686 | 0.0508
Epoch 268/300, Loss: 0.0686 | 0.0508
Epoch 269/300, Loss: 0.0686 | 0.0508
Epoch 270/300, Loss: 0.0686 | 0.0508
Epoch 271/300, Loss: 0.0686 | 0.0508
Epoch 272/300, Loss: 0.0686 | 0.0508
Epoch 273/300, Loss: 0.0686 | 0.0508
Epoch 274/300, Loss: 0.0686 | 0.0508
Epoch 275/300, Loss: 0.0686 | 0.0508
Epoch 276/300, Loss: 0.0686 | 0.0508
Epoch 277/300, Loss: 0.0686 | 0.0508
Epoch 278/300, Loss: 0.0686 | 0.0508
Epoch 279/300, Loss: 0.0686 | 0.0508
Epoch 280/300, Loss: 0.0686 | 0.0508
Epoch 281/300, Loss: 0.0686 | 0.0508
Epoch 282/300, Loss: 0.0686 | 0.0508
Epoch 283/300, Loss: 0.0686 | 0.0508
Epoch 284/300, Loss: 0.0686 | 0.0508
Epoch 285/300, Loss: 0.0686 | 0.0508
Epoch 286/300, Loss: 0.0686 | 0.0508
Epoch 287/300, Loss: 0.0686 | 0.0508
Epoch 288/300, Loss: 0.0686 | 0.0508
Epoch 289/300, Loss: 0.0686 | 0.0508
Epoch 290/300, Loss: 0.0686 | 0.0508
Epoch 291/300, Loss: 0.0686 | 0.0508
Epoch 292/300, Loss: 0.0686 | 0.0508
Epoch 293/300, Loss: 0.0686 | 0.0508
Epoch 294/300, Loss: 0.0686 | 0.0508
Epoch 295/300, Loss: 0.0686 | 0.0508
Epoch 296/300, Loss: 0.0686 | 0.0508
Epoch 297/300, Loss: 0.0686 | 0.0508
Epoch 298/300, Loss: 0.0686 | 0.0508
Epoch 299/300, Loss: 0.0686 | 0.0508
Epoch 300/300, Loss: 0.0686 | 0.0508
Runtime (seconds): 90.13949608802795
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 522.8217341937125
RMSE: 22.86529541015625
MAE: 22.86529541015625
R-squared: nan
[210.3053]
