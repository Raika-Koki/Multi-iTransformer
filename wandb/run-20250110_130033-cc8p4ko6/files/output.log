ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-10 13:00:36,920][0m A new study created in memory with name: no-name-985d7a93-a72d-4022-bf24-e5e1d3330e7d[0m
Early stopping at epoch 39
[32m[I 2025-01-10 13:00:47,391][0m Trial 0 finished with value: 2.020583302202359 and parameters: {'observation_period_num': 220, 'train_rates': 0.792336124930658, 'learning_rate': 4.184761406016257e-06, 'batch_size': 216, 'step_size': 1, 'gamma': 0.7577496126123632}. Best is trial 0 with value: 2.020583302202359.[0m
[32m[I 2025-01-10 13:02:55,556][0m Trial 1 finished with value: 0.1107922470955937 and parameters: {'observation_period_num': 40, 'train_rates': 0.6756041265694445, 'learning_rate': 2.727907086534958e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.9846192435567643}. Best is trial 1 with value: 0.1107922470955937.[0m
[32m[I 2025-01-10 13:06:53,170][0m Trial 2 finished with value: 0.07366935574802859 and parameters: {'observation_period_num': 109, 'train_rates': 0.9695228702511828, 'learning_rate': 0.000843754063833378, 'batch_size': 22, 'step_size': 4, 'gamma': 0.8364430292979763}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:07:41,459][0m Trial 3 finished with value: 0.11224923196779305 and parameters: {'observation_period_num': 251, 'train_rates': 0.7213424336914344, 'learning_rate': 0.00023957709013721138, 'batch_size': 90, 'step_size': 5, 'gamma': 0.9267406964408278}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:08:56,541][0m Trial 4 finished with value: 0.08822150031725566 and parameters: {'observation_period_num': 103, 'train_rates': 0.9086759325937067, 'learning_rate': 0.000249937323984216, 'batch_size': 66, 'step_size': 4, 'gamma': 0.9199884918434467}. Best is trial 2 with value: 0.07366935574802859.[0m
Early stopping at epoch 57
[32m[I 2025-01-10 13:09:13,912][0m Trial 5 finished with value: 2.627059388364482 and parameters: {'observation_period_num': 113, 'train_rates': 0.9178450349526106, 'learning_rate': 1.2835104710036336e-06, 'batch_size': 188, 'step_size': 1, 'gamma': 0.8610231473602639}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:10:08,230][0m Trial 6 finished with value: 0.4850761061602265 and parameters: {'observation_period_num': 42, 'train_rates': 0.90597064387048, 'learning_rate': 2.337824062030869e-05, 'batch_size': 102, 'step_size': 2, 'gamma': 0.9402961111332765}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:11:08,314][0m Trial 7 finished with value: 0.34116547939646275 and parameters: {'observation_period_num': 29, 'train_rates': 0.7326822489443087, 'learning_rate': 8.279946927870574e-06, 'batch_size': 77, 'step_size': 12, 'gamma': 0.9620746601879849}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:12:10,174][0m Trial 8 finished with value: 1.3958453160745126 and parameters: {'observation_period_num': 162, 'train_rates': 0.7099517826226556, 'learning_rate': 2.41081765416649e-06, 'batch_size': 75, 'step_size': 14, 'gamma': 0.8576912626801606}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:12:47,024][0m Trial 9 finished with value: 0.18694351688583397 and parameters: {'observation_period_num': 235, 'train_rates': 0.6057806949407305, 'learning_rate': 0.0006163471351984672, 'batch_size': 115, 'step_size': 2, 'gamma': 0.8903408148419142}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:13:23,957][0m Trial 10 finished with value: 0.25576549768447876 and parameters: {'observation_period_num': 153, 'train_rates': 0.9737478187275252, 'learning_rate': 9.120588594278641e-05, 'batch_size': 162, 'step_size': 8, 'gamma': 0.8009251055921258}. Best is trial 2 with value: 0.07366935574802859.[0m
[32m[I 2025-01-10 13:17:22,590][0m Trial 11 finished with value: 0.06921541427864748 and parameters: {'observation_period_num': 100, 'train_rates': 0.988118279370538, 'learning_rate': 0.0009147862768441831, 'batch_size': 24, 'step_size': 6, 'gamma': 0.8239682528070136}. Best is trial 11 with value: 0.06921541427864748.[0m
[32m[I 2025-01-10 13:22:21,243][0m Trial 12 finished with value: 0.11997476539441518 and parameters: {'observation_period_num': 81, 'train_rates': 0.983099173067901, 'learning_rate': 0.0009839442612931833, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8122824515507802}. Best is trial 11 with value: 0.06921541427864748.[0m
[32m[I 2025-01-10 13:24:30,664][0m Trial 13 finished with value: 0.04603449605255319 and parameters: {'observation_period_num': 70, 'train_rates': 0.8450056837988201, 'learning_rate': 0.0002889617206100615, 'batch_size': 40, 'step_size': 10, 'gamma': 0.8139128732424966}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:26:15,351][0m Trial 14 finished with value: 0.06417042453587055 and parameters: {'observation_period_num': 72, 'train_rates': 0.826429854051371, 'learning_rate': 0.00012271621965428473, 'batch_size': 49, 'step_size': 10, 'gamma': 0.7565265231990346}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:26:37,729][0m Trial 15 finished with value: 0.22526365116671407 and parameters: {'observation_period_num': 67, 'train_rates': 0.8330825061805314, 'learning_rate': 8.624739408000191e-05, 'batch_size': 250, 'step_size': 10, 'gamma': 0.770220116272073}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:27:19,569][0m Trial 16 finished with value: 0.0649853203178458 and parameters: {'observation_period_num': 13, 'train_rates': 0.8389171389943182, 'learning_rate': 9.755390028528119e-05, 'batch_size': 134, 'step_size': 10, 'gamma': 0.7819750454327069}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:28:31,977][0m Trial 17 finished with value: 0.07797039390251731 and parameters: {'observation_period_num': 151, 'train_rates': 0.7844898257024763, 'learning_rate': 0.00024470520843748576, 'batch_size': 67, 'step_size': 10, 'gamma': 0.7519794314000744}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:30:20,471][0m Trial 18 finished with value: 0.07152423850624016 and parameters: {'observation_period_num': 65, 'train_rates': 0.8458309348630971, 'learning_rate': 5.581162035772058e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.7968230976769578}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:31:03,741][0m Trial 19 finished with value: 0.5302949105871135 and parameters: {'observation_period_num': 137, 'train_rates': 0.8766237028959453, 'learning_rate': 1.4177260593098642e-05, 'batch_size': 124, 'step_size': 11, 'gamma': 0.8353128255319483}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:31:36,033][0m Trial 20 finished with value: 0.13477991884421162 and parameters: {'observation_period_num': 189, 'train_rates': 0.7581871743676347, 'learning_rate': 0.0003698609059858606, 'batch_size': 154, 'step_size': 8, 'gamma': 0.7823608829907246}. Best is trial 13 with value: 0.04603449605255319.[0m
[32m[I 2025-01-10 13:33:11,770][0m Trial 21 finished with value: 0.04366881212523542 and parameters: {'observation_period_num': 16, 'train_rates': 0.837392135949436, 'learning_rate': 0.00011830018323295973, 'batch_size': 55, 'step_size': 10, 'gamma': 0.7848488635378357}. Best is trial 21 with value: 0.04366881212523542.[0m
[32m[I 2025-01-10 13:34:52,437][0m Trial 22 finished with value: 0.03961825129780884 and parameters: {'observation_period_num': 5, 'train_rates': 0.8165279882187642, 'learning_rate': 0.00015154796633615357, 'batch_size': 52, 'step_size': 9, 'gamma': 0.7830922258216807}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:36:43,764][0m Trial 23 finished with value: 0.05919135430420952 and parameters: {'observation_period_num': 8, 'train_rates': 0.8818473082277751, 'learning_rate': 5.079109980417746e-05, 'batch_size': 49, 'step_size': 9, 'gamma': 0.7885030122589514}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:37:40,725][0m Trial 24 finished with value: 0.045953390921684024 and parameters: {'observation_period_num': 28, 'train_rates': 0.8074309181261037, 'learning_rate': 0.00017341697617662873, 'batch_size': 93, 'step_size': 12, 'gamma': 0.8827855452196419}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:38:31,930][0m Trial 25 finished with value: 0.05443497343472972 and parameters: {'observation_period_num': 29, 'train_rates': 0.7670631147421121, 'learning_rate': 0.00015591015895238057, 'batch_size': 98, 'step_size': 12, 'gamma': 0.8915865014872512}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:39:30,344][0m Trial 26 finished with value: 0.11638501483767105 and parameters: {'observation_period_num': 49, 'train_rates': 0.8076205309421689, 'learning_rate': 5.365010772026852e-05, 'batch_size': 90, 'step_size': 12, 'gamma': 0.8739067575467555}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:40:48,950][0m Trial 27 finished with value: 0.03966638621383538 and parameters: {'observation_period_num': 10, 'train_rates': 0.6639863511534956, 'learning_rate': 0.00016743047820898713, 'batch_size': 59, 'step_size': 13, 'gamma': 0.9034639744686621}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:42:01,577][0m Trial 28 finished with value: 0.05369389659258164 and parameters: {'observation_period_num': 5, 'train_rates': 0.6367585102974817, 'learning_rate': 0.0004594719442357289, 'batch_size': 60, 'step_size': 15, 'gamma': 0.9042808017955858}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:42:25,098][0m Trial 29 finished with value: 0.1712846656356096 and parameters: {'observation_period_num': 19, 'train_rates': 0.6725852128722872, 'learning_rate': 4.2746323095063535e-05, 'batch_size': 207, 'step_size': 14, 'gamma': 0.7704299750644196}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:43:09,115][0m Trial 30 finished with value: 0.5765272753009237 and parameters: {'observation_period_num': 52, 'train_rates': 0.7531011676668541, 'learning_rate': 1.622743913643094e-05, 'batch_size': 111, 'step_size': 9, 'gamma': 0.8482465939608881}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:44:10,148][0m Trial 31 finished with value: 0.05478915516287088 and parameters: {'observation_period_num': 32, 'train_rates': 0.794711517115699, 'learning_rate': 0.00014704081772383602, 'batch_size': 85, 'step_size': 13, 'gamma': 0.8765757404234042}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:45:42,294][0m Trial 32 finished with value: 0.04093811346893139 and parameters: {'observation_period_num': 22, 'train_rates': 0.8674113977403447, 'learning_rate': 0.0001650153266405718, 'batch_size': 58, 'step_size': 13, 'gamma': 0.9076543387202738}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:48:22,603][0m Trial 33 finished with value: 0.05236462405795386 and parameters: {'observation_period_num': 50, 'train_rates': 0.8717009935030692, 'learning_rate': 7.455196549123068e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9051940876203909}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:50:01,757][0m Trial 34 finished with value: 0.041437511891126634 and parameters: {'observation_period_num': 19, 'train_rates': 0.9386880652229868, 'learning_rate': 0.00047991937841519716, 'batch_size': 58, 'step_size': 11, 'gamma': 0.9414800814828035}. Best is trial 22 with value: 0.03961825129780884.[0m
[32m[I 2025-01-10 13:52:36,807][0m Trial 35 finished with value: 0.03162699734309205 and parameters: {'observation_period_num': 7, 'train_rates': 0.9451746782416619, 'learning_rate': 0.0005788101212791833, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9502112065533127}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 13:55:38,116][0m Trial 36 finished with value: 0.08947657681698722 and parameters: {'observation_period_num': 89, 'train_rates': 0.9571920667733564, 'learning_rate': 0.0006129241164879588, 'batch_size': 31, 'step_size': 15, 'gamma': 0.9764857758583204}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 13:58:00,747][0m Trial 37 finished with value: 0.057386721096566465 and parameters: {'observation_period_num': 39, 'train_rates': 0.8954525901091639, 'learning_rate': 0.0003338643343540297, 'batch_size': 38, 'step_size': 13, 'gamma': 0.9232563734502652}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 13:59:25,572][0m Trial 38 finished with value: 0.03229247465093232 and parameters: {'observation_period_num': 6, 'train_rates': 0.9326769697493821, 'learning_rate': 0.00019181088044991208, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9500932413711137}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:00:36,722][0m Trial 39 finished with value: 0.12337293810135609 and parameters: {'observation_period_num': 204, 'train_rates': 0.9329144476310921, 'learning_rate': 0.00021340377589708684, 'batch_size': 76, 'step_size': 15, 'gamma': 0.950706611080864}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:04:27,159][0m Trial 40 finished with value: 0.16587346374681572 and parameters: {'observation_period_num': 55, 'train_rates': 0.6900347630778596, 'learning_rate': 0.000679665102001426, 'batch_size': 19, 'step_size': 14, 'gamma': 0.9845383073409824}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:05:51,455][0m Trial 41 finished with value: 0.04427430185958417 and parameters: {'observation_period_num': 6, 'train_rates': 0.9534971865252154, 'learning_rate': 0.00023295457324682098, 'batch_size': 69, 'step_size': 13, 'gamma': 0.9121686066908881}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:07:02,179][0m Trial 42 finished with value: 0.04797983777366186 and parameters: {'observation_period_num': 26, 'train_rates': 0.9223374927511006, 'learning_rate': 0.000400495674798731, 'batch_size': 80, 'step_size': 15, 'gamma': 0.960863188660925}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:09:12,809][0m Trial 43 finished with value: 0.04211111410066139 and parameters: {'observation_period_num': 40, 'train_rates': 0.8967674363374607, 'learning_rate': 0.0001932678778623292, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9343308869257523}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:10:40,895][0m Trial 44 finished with value: 0.06400671483557901 and parameters: {'observation_period_num': 18, 'train_rates': 0.8622663411836458, 'learning_rate': 3.182631963239683e-05, 'batch_size': 61, 'step_size': 4, 'gamma': 0.9629147155823257}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:13:59,525][0m Trial 45 finished with value: 0.03652923181653023 and parameters: {'observation_period_num': 5, 'train_rates': 0.960748394343682, 'learning_rate': 7.467837051616138e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9322926657674472}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:17:47,799][0m Trial 46 finished with value: 0.03598718421862406 and parameters: {'observation_period_num': 5, 'train_rates': 0.9540573982418252, 'learning_rate': 7.367271885358277e-05, 'batch_size': 25, 'step_size': 11, 'gamma': 0.9490934436255866}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:21:16,662][0m Trial 47 finished with value: 0.057516631516425505 and parameters: {'observation_period_num': 39, 'train_rates': 0.9608370939542124, 'learning_rate': 6.995717520883198e-05, 'batch_size': 27, 'step_size': 11, 'gamma': 0.9526840375976949}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:26:23,100][0m Trial 48 finished with value: 0.04257487617616583 and parameters: {'observation_period_num': 5, 'train_rates': 0.9314382474283092, 'learning_rate': 3.558177912224438e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9353330247287488}. Best is trial 35 with value: 0.03162699734309205.[0m
[32m[I 2025-01-10 14:29:18,445][0m Trial 49 finished with value: 0.08525799528548592 and parameters: {'observation_period_num': 32, 'train_rates': 0.9482144103510199, 'learning_rate': 1.8985398814406323e-05, 'batch_size': 32, 'step_size': 9, 'gamma': 0.9743380124543993}. Best is trial 35 with value: 0.03162699734309205.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-10 14:29:18,456][0m A new study created in memory with name: no-name-075ff929-980c-48e8-86c4-5d742d7f1ef6[0m
[32m[I 2025-01-10 14:30:41,513][0m Trial 0 finished with value: 0.08705343191618911 and parameters: {'observation_period_num': 30, 'train_rates': 0.615816218118118, 'learning_rate': 2.0821051806202766e-05, 'batch_size': 83, 'step_size': 12, 'gamma': 0.9310268030883022}. Best is trial 0 with value: 0.08705343191618911.[0m
[32m[I 2025-01-10 14:31:43,046][0m Trial 1 finished with value: 0.15148679368672824 and parameters: {'observation_period_num': 52, 'train_rates': 0.6702160307474135, 'learning_rate': 1.3562422160812123e-05, 'batch_size': 128, 'step_size': 9, 'gamma': 0.7619183726275267}. Best is trial 0 with value: 0.08705343191618911.[0m
[32m[I 2025-01-10 14:32:32,621][0m Trial 2 finished with value: 0.21661453920797738 and parameters: {'observation_period_num': 156, 'train_rates': 0.639327855548586, 'learning_rate': 1.2364964090949925e-05, 'batch_size': 191, 'step_size': 14, 'gamma': 0.9463479699809223}. Best is trial 0 with value: 0.08705343191618911.[0m
[32m[I 2025-01-10 14:33:35,765][0m Trial 3 finished with value: 0.0886388726452774 and parameters: {'observation_period_num': 69, 'train_rates': 0.7407700306141679, 'learning_rate': 0.00012089073182897903, 'batch_size': 129, 'step_size': 2, 'gamma': 0.8500705928775932}. Best is trial 0 with value: 0.08705343191618911.[0m
[32m[I 2025-01-10 14:34:40,430][0m Trial 4 finished with value: 0.23774920403957367 and parameters: {'observation_period_num': 244, 'train_rates': 0.9823309610031077, 'learning_rate': 1.358484669261996e-05, 'batch_size': 177, 'step_size': 6, 'gamma': 0.8020758823668515}. Best is trial 0 with value: 0.08705343191618911.[0m
[32m[I 2025-01-10 14:35:48,324][0m Trial 5 finished with value: 0.07367390204664107 and parameters: {'observation_period_num': 98, 'train_rates': 0.8411661804621526, 'learning_rate': 0.00010962651213071038, 'batch_size': 129, 'step_size': 7, 'gamma': 0.9833985456756886}. Best is trial 5 with value: 0.07367390204664107.[0m
[32m[I 2025-01-10 14:36:53,646][0m Trial 6 finished with value: 0.06654882431030273 and parameters: {'observation_period_num': 7, 'train_rates': 0.981739987096885, 'learning_rate': 3.639562019051101e-05, 'batch_size': 230, 'step_size': 2, 'gamma': 0.9200848617468118}. Best is trial 6 with value: 0.06654882431030273.[0m
[32m[I 2025-01-10 14:39:33,810][0m Trial 7 finished with value: 0.16052476702938676 and parameters: {'observation_period_num': 190, 'train_rates': 0.8725163637349759, 'learning_rate': 0.0005908140459847859, 'batch_size': 50, 'step_size': 4, 'gamma': 0.9421161514751413}. Best is trial 6 with value: 0.06654882431030273.[0m
[32m[I 2025-01-10 14:45:52,439][0m Trial 8 finished with value: 0.05153072927078586 and parameters: {'observation_period_num': 43, 'train_rates': 0.8319302329963176, 'learning_rate': 3.78958558313525e-06, 'batch_size': 21, 'step_size': 14, 'gamma': 0.981596976104069}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:48:47,812][0m Trial 9 finished with value: 0.17555033193554642 and parameters: {'observation_period_num': 191, 'train_rates': 0.7508240523049733, 'learning_rate': 0.00048353320917399775, 'batch_size': 41, 'step_size': 10, 'gamma': 0.828157383451857}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:50:26,962][0m Trial 10 finished with value: 0.35072664871360315 and parameters: {'observation_period_num': 104, 'train_rates': 0.884372262135555, 'learning_rate': 1.2400451381535906e-06, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8817843484501676}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:51:31,228][0m Trial 11 finished with value: 0.21975307166576385 and parameters: {'observation_period_num': 6, 'train_rates': 0.9523744531029907, 'learning_rate': 2.111881330228293e-06, 'batch_size': 253, 'step_size': 1, 'gamma': 0.9857048694173661}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:52:33,393][0m Trial 12 finished with value: 0.17304028498549615 and parameters: {'observation_period_num': 6, 'train_rates': 0.9160687985525094, 'learning_rate': 3.8126934307458668e-06, 'batch_size': 234, 'step_size': 4, 'gamma': 0.9025748459035478}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:53:33,607][0m Trial 13 finished with value: 0.10224187914516168 and parameters: {'observation_period_num': 54, 'train_rates': 0.8079134255025882, 'learning_rate': 6.054424402584639e-05, 'batch_size': 177, 'step_size': 12, 'gamma': 0.9120986872946275}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:54:27,317][0m Trial 14 finished with value: 0.20657523224079932 and parameters: {'observation_period_num': 87, 'train_rates': 0.7417850907546565, 'learning_rate': 4.416679501237581e-06, 'batch_size': 209, 'step_size': 11, 'gamma': 0.9632758404053032}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 14:59:49,949][0m Trial 15 finished with value: 0.10850390297843811 and parameters: {'observation_period_num': 135, 'train_rates': 0.9336314706929206, 'learning_rate': 3.751258279617169e-05, 'batch_size': 26, 'step_size': 5, 'gamma': 0.8861529539440675}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 15:01:54,676][0m Trial 16 finished with value: 0.07305703312158585 and parameters: {'observation_period_num': 36, 'train_rates': 0.9894534161764561, 'learning_rate': 5.322824134778139e-06, 'batch_size': 75, 'step_size': 8, 'gamma': 0.9185992926742059}. Best is trial 8 with value: 0.05153072927078586.[0m
[32m[I 2025-01-10 15:02:55,677][0m Trial 17 finished with value: 0.04664157053150461 and parameters: {'observation_period_num': 27, 'train_rates': 0.7957011118753117, 'learning_rate': 0.00022847451377131555, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9516310913752432}. Best is trial 17 with value: 0.04664157053150461.[0m
[32m[I 2025-01-10 15:04:14,669][0m Trial 18 finished with value: 0.16832620630359085 and parameters: {'observation_period_num': 80, 'train_rates': 0.794345190750283, 'learning_rate': 0.00019942141047262846, 'batch_size': 103, 'step_size': 14, 'gamma': 0.9681542737566611}. Best is trial 17 with value: 0.04664157053150461.[0m
[32m[I 2025-01-10 15:05:11,775][0m Trial 19 finished with value: 0.2674075612229501 and parameters: {'observation_period_num': 123, 'train_rates': 0.7048762189172157, 'learning_rate': 0.0003293595914283908, 'batch_size': 153, 'step_size': 13, 'gamma': 0.9537544003411131}. Best is trial 17 with value: 0.04664157053150461.[0m
[32m[I 2025-01-10 15:06:14,362][0m Trial 20 finished with value: 0.047314443041517704 and parameters: {'observation_period_num': 30, 'train_rates': 0.8128225947007282, 'learning_rate': 0.0009898355869675738, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9895930747954539}. Best is trial 17 with value: 0.04664157053150461.[0m
[32m[I 2025-01-10 15:07:16,356][0m Trial 21 finished with value: 0.059765896876342595 and parameters: {'observation_period_num': 34, 'train_rates': 0.8026420437596179, 'learning_rate': 0.0007918566353768689, 'batch_size': 153, 'step_size': 15, 'gamma': 0.9767175731714074}. Best is trial 17 with value: 0.04664157053150461.[0m
[32m[I 2025-01-10 15:08:33,377][0m Trial 22 finished with value: 0.05828256734371454 and parameters: {'observation_period_num': 53, 'train_rates': 0.8475174856527992, 'learning_rate': 0.0008859497753624915, 'batch_size': 107, 'step_size': 13, 'gamma': 0.9886272157268255}. Best is trial 17 with value: 0.04664157053150461.[0m
[32m[I 2025-01-10 15:09:35,016][0m Trial 23 finished with value: 0.041288865341262505 and parameters: {'observation_period_num': 30, 'train_rates': 0.782063605996068, 'learning_rate': 0.00029047696735329145, 'batch_size': 149, 'step_size': 15, 'gamma': 0.9612416496345418}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:10:34,683][0m Trial 24 finished with value: 0.04227551183839112 and parameters: {'observation_period_num': 27, 'train_rates': 0.780875999069738, 'learning_rate': 0.0002946171145338732, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9554103704726805}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:11:30,990][0m Trial 25 finished with value: 0.07244478406165611 and parameters: {'observation_period_num': 69, 'train_rates': 0.7611246857756542, 'learning_rate': 0.00021967150876194496, 'batch_size': 172, 'step_size': 11, 'gamma': 0.9357490413079409}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:12:23,734][0m Trial 26 finished with value: 0.06036501557811766 and parameters: {'observation_period_num': 18, 'train_rates': 0.70656509290545, 'learning_rate': 0.00010123891395416175, 'batch_size': 200, 'step_size': 13, 'gamma': 0.8587336066659931}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:13:33,387][0m Trial 27 finished with value: 0.0691181850419314 and parameters: {'observation_period_num': 60, 'train_rates': 0.7800390363244561, 'learning_rate': 0.0003497776264915886, 'batch_size': 115, 'step_size': 15, 'gamma': 0.8975940648660233}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:14:29,144][0m Trial 28 finished with value: 0.2127203656590148 and parameters: {'observation_period_num': 108, 'train_rates': 0.7178908524754025, 'learning_rate': 0.00020754905522391782, 'batch_size': 147, 'step_size': 12, 'gamma': 0.9500017826827666}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:15:18,305][0m Trial 29 finished with value: 0.12436395186467232 and parameters: {'observation_period_num': 24, 'train_rates': 0.6022999776291167, 'learning_rate': 0.00033073231840218583, 'batch_size': 167, 'step_size': 11, 'gamma': 0.9286679678998887}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:16:53,430][0m Trial 30 finished with value: 0.1621226838151496 and parameters: {'observation_period_num': 251, 'train_rates': 0.875317749954488, 'learning_rate': 7.099088072906095e-05, 'batch_size': 84, 'step_size': 12, 'gamma': 0.9580338412921943}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:17:54,770][0m Trial 31 finished with value: 0.06511774079667197 and parameters: {'observation_period_num': 25, 'train_rates': 0.7755410278859199, 'learning_rate': 0.00046230581920617803, 'batch_size': 146, 'step_size': 15, 'gamma': 0.9696010839037452}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:18:55,036][0m Trial 32 finished with value: 0.04443156991247267 and parameters: {'observation_period_num': 46, 'train_rates': 0.8214924281059758, 'learning_rate': 0.000935572150879846, 'batch_size': 161, 'step_size': 14, 'gamma': 0.9324058334446896}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:19:47,697][0m Trial 33 finished with value: 0.06334012891987667 and parameters: {'observation_period_num': 44, 'train_rates': 0.672922077866242, 'learning_rate': 0.00016871357160767162, 'batch_size': 189, 'step_size': 14, 'gamma': 0.9348782088963546}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:20:52,930][0m Trial 34 finished with value: 0.07338941203907991 and parameters: {'observation_period_num': 70, 'train_rates': 0.8256032751855714, 'learning_rate': 0.000550358383293561, 'batch_size': 136, 'step_size': 13, 'gamma': 0.9472799721177659}. Best is trial 23 with value: 0.041288865341262505.[0m
[32m[I 2025-01-10 15:22:02,337][0m Trial 35 finished with value: 0.03376182145937503 and parameters: {'observation_period_num': 16, 'train_rates': 0.7774100097418958, 'learning_rate': 0.0003405954258538107, 'batch_size': 118, 'step_size': 9, 'gamma': 0.9284443772576697}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:23:07,449][0m Trial 36 finished with value: 0.08197057449674486 and parameters: {'observation_period_num': 46, 'train_rates': 0.7279922263038627, 'learning_rate': 0.000649845006404256, 'batch_size': 118, 'step_size': 9, 'gamma': 0.9251016500209492}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:24:23,404][0m Trial 37 finished with value: 0.03951957056550348 and parameters: {'observation_period_num': 16, 'train_rates': 0.677181612450155, 'learning_rate': 0.00014457454027037804, 'batch_size': 97, 'step_size': 9, 'gamma': 0.8959723563774344}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:26:03,235][0m Trial 38 finished with value: 0.03686488002472355 and parameters: {'observation_period_num': 15, 'train_rates': 0.6559969941667749, 'learning_rate': 0.00011975194152099437, 'batch_size': 69, 'step_size': 8, 'gamma': 0.7552791460714918}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:27:33,288][0m Trial 39 finished with value: 0.04066808269492218 and parameters: {'observation_period_num': 13, 'train_rates': 0.6429332301705489, 'learning_rate': 0.0001401796815762942, 'batch_size': 78, 'step_size': 7, 'gamma': 0.7599978298084027}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:29:22,383][0m Trial 40 finished with value: 0.09644280878566791 and parameters: {'observation_period_num': 12, 'train_rates': 0.6325503866697509, 'learning_rate': 1.9325737721243528e-05, 'batch_size': 62, 'step_size': 7, 'gamma': 0.7588754074485619}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:30:40,859][0m Trial 41 finished with value: 0.03715101811103523 and parameters: {'observation_period_num': 15, 'train_rates': 0.6733791803601608, 'learning_rate': 0.00012914339306725373, 'batch_size': 93, 'step_size': 9, 'gamma': 0.7777668583001992}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:31:57,351][0m Trial 42 finished with value: 0.038244276746245455 and parameters: {'observation_period_num': 15, 'train_rates': 0.6707210742348122, 'learning_rate': 0.00013391537847114808, 'batch_size': 96, 'step_size': 9, 'gamma': 0.7762603065824969}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:33:09,354][0m Trial 43 finished with value: 0.16455696015195412 and parameters: {'observation_period_num': 209, 'train_rates': 0.6794490070022747, 'learning_rate': 8.157662789173693e-05, 'batch_size': 95, 'step_size': 9, 'gamma': 0.7820997072831293}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:34:51,733][0m Trial 44 finished with value: 0.04268808627943555 and parameters: {'observation_period_num': 5, 'train_rates': 0.6601018929997531, 'learning_rate': 4.374808675260933e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.7740550647201261}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:37:08,014][0m Trial 45 finished with value: 0.042255445040370286 and parameters: {'observation_period_num': 18, 'train_rates': 0.6893685044797971, 'learning_rate': 5.061638950009976e-05, 'batch_size': 52, 'step_size': 8, 'gamma': 0.8137233911863239}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:38:21,077][0m Trial 46 finished with value: 0.09892717946232074 and parameters: {'observation_period_num': 61, 'train_rates': 0.6514684487609829, 'learning_rate': 0.0001239890688039952, 'batch_size': 95, 'step_size': 10, 'gamma': 0.7848151308306919}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:39:20,820][0m Trial 47 finished with value: 0.07257862033666353 and parameters: {'observation_period_num': 39, 'train_rates': 0.6338201649966195, 'learning_rate': 8.917052150156799e-05, 'batch_size': 124, 'step_size': 8, 'gamma': 0.8013439233888675}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:40:35,016][0m Trial 48 finished with value: 0.11517880679155305 and parameters: {'observation_period_num': 19, 'train_rates': 0.6160739869047304, 'learning_rate': 2.28214158584991e-05, 'batch_size': 94, 'step_size': 10, 'gamma': 0.8379565898553551}. Best is trial 35 with value: 0.03376182145937503.[0m
[32m[I 2025-01-10 15:43:39,037][0m Trial 49 finished with value: 0.0329561286484957 and parameters: {'observation_period_num': 5, 'train_rates': 0.6943987969847646, 'learning_rate': 0.00015777252739766184, 'batch_size': 39, 'step_size': 6, 'gamma': 0.750001559259669}. Best is trial 49 with value: 0.0329561286484957.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-10 15:43:39,047][0m A new study created in memory with name: no-name-b9adee60-a8d2-460b-a801-4f4c0b1d0ebf[0m
[32m[I 2025-01-10 15:45:49,322][0m Trial 0 finished with value: 0.1633582492455861 and parameters: {'observation_period_num': 166, 'train_rates': 0.7415638483165133, 'learning_rate': 0.00019074034974099705, 'batch_size': 55, 'step_size': 7, 'gamma': 0.936212261418192}. Best is trial 0 with value: 0.1633582492455861.[0m
[32m[I 2025-01-10 15:46:38,157][0m Trial 1 finished with value: 0.2107969034049246 and parameters: {'observation_period_num': 178, 'train_rates': 0.6886912439481008, 'learning_rate': 5.257252468423599e-05, 'batch_size': 195, 'step_size': 6, 'gamma': 0.8439167798422177}. Best is trial 0 with value: 0.1633582492455861.[0m
[32m[I 2025-01-10 15:47:56,053][0m Trial 2 finished with value: 0.1600286118735216 and parameters: {'observation_period_num': 163, 'train_rates': 0.8910670007731902, 'learning_rate': 5.3656556869097715e-06, 'batch_size': 105, 'step_size': 5, 'gamma': 0.9888971943449889}. Best is trial 2 with value: 0.1600286118735216.[0m
[32m[I 2025-01-10 15:48:44,549][0m Trial 3 finished with value: 0.06154884263459179 and parameters: {'observation_period_num': 21, 'train_rates': 0.6315827023095911, 'learning_rate': 0.0009404135536231895, 'batch_size': 215, 'step_size': 4, 'gamma': 0.8534503873689351}. Best is trial 3 with value: 0.06154884263459179.[0m
[32m[I 2025-01-10 15:49:43,950][0m Trial 4 finished with value: 0.23198663905994138 and parameters: {'observation_period_num': 248, 'train_rates': 0.7817337300936432, 'learning_rate': 4.053253092302218e-05, 'batch_size': 140, 'step_size': 6, 'gamma': 0.9239822625430532}. Best is trial 3 with value: 0.06154884263459179.[0m
[32m[I 2025-01-10 15:50:40,527][0m Trial 5 finished with value: 0.05243825391509539 and parameters: {'observation_period_num': 41, 'train_rates': 0.7329083352452254, 'learning_rate': 0.0009763498311293718, 'batch_size': 156, 'step_size': 9, 'gamma': 0.888907873591935}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 15:52:03,468][0m Trial 6 finished with value: 0.39409326308618775 and parameters: {'observation_period_num': 169, 'train_rates': 0.6604935408443114, 'learning_rate': 1.4694494426588588e-05, 'batch_size': 80, 'step_size': 1, 'gamma': 0.9022089607807466}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 15:53:04,913][0m Trial 7 finished with value: 0.07267200416744891 and parameters: {'observation_period_num': 12, 'train_rates': 0.6075896231088155, 'learning_rate': 0.00020048988605060166, 'batch_size': 114, 'step_size': 9, 'gamma': 0.7842446730812875}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 15:53:52,414][0m Trial 8 finished with value: 0.6284203312201807 and parameters: {'observation_period_num': 252, 'train_rates': 0.6834720204509028, 'learning_rate': 3.6315943212147814e-06, 'batch_size': 223, 'step_size': 3, 'gamma': 0.9071133667385692}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 15:54:45,626][0m Trial 9 finished with value: 0.1764090788319595 and parameters: {'observation_period_num': 219, 'train_rates': 0.8054589667585228, 'learning_rate': 2.4156338117575673e-05, 'batch_size': 213, 'step_size': 10, 'gamma': 0.9754039172459013}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 16:01:41,959][0m Trial 10 finished with value: 0.2027030023543731 and parameters: {'observation_period_num': 80, 'train_rates': 0.983988282113696, 'learning_rate': 1.1898857083564131e-06, 'batch_size': 21, 'step_size': 13, 'gamma': 0.7934778089032638}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 16:02:32,174][0m Trial 11 finished with value: 0.08318665132923356 and parameters: {'observation_period_num': 6, 'train_rates': 0.6044514857291573, 'learning_rate': 0.00089468444016825, 'batch_size': 168, 'step_size': 12, 'gamma': 0.8381838657125467}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 16:03:27,637][0m Trial 12 finished with value: 0.08555396339971344 and parameters: {'observation_period_num': 79, 'train_rates': 0.7553449733205626, 'learning_rate': 0.0009954174643796822, 'batch_size': 239, 'step_size': 15, 'gamma': 0.8595841543145782}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 16:04:21,335][0m Trial 13 finished with value: 0.07915517218591432 and parameters: {'observation_period_num': 54, 'train_rates': 0.7010964308073283, 'learning_rate': 0.0002971655362752409, 'batch_size': 171, 'step_size': 3, 'gamma': 0.8245436694497221}. Best is trial 5 with value: 0.05243825391509539.[0m
[32m[I 2025-01-10 16:05:20,391][0m Trial 14 finished with value: 0.03944696859314532 and parameters: {'observation_period_num': 46, 'train_rates': 0.83790704621587, 'learning_rate': 0.0004712163690808283, 'batch_size': 175, 'step_size': 9, 'gamma': 0.7512072998782681}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:06:21,845][0m Trial 15 finished with value: 0.0748637807019408 and parameters: {'observation_period_num': 121, 'train_rates': 0.8438241721587366, 'learning_rate': 0.00035568560826102234, 'batch_size': 152, 'step_size': 11, 'gamma': 0.7546689049125456}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:07:24,418][0m Trial 16 finished with value: 0.05275568048711176 and parameters: {'observation_period_num': 56, 'train_rates': 0.8788517434686101, 'learning_rate': 7.999174129103923e-05, 'batch_size': 175, 'step_size': 8, 'gamma': 0.8846304544349329}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:08:24,252][0m Trial 17 finished with value: 0.09081346541643143 and parameters: {'observation_period_num': 113, 'train_rates': 0.9333547795650023, 'learning_rate': 0.00011005191716305741, 'batch_size': 252, 'step_size': 9, 'gamma': 0.8070202599901575}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:09:39,439][0m Trial 18 finished with value: 0.058116693247823 and parameters: {'observation_period_num': 45, 'train_rates': 0.8061508720250397, 'learning_rate': 0.0004426400242225968, 'batch_size': 109, 'step_size': 14, 'gamma': 0.758727747341456}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:10:32,508][0m Trial 19 finished with value: 0.15891123320407793 and parameters: {'observation_period_num': 92, 'train_rates': 0.7265212809837022, 'learning_rate': 0.00011341933318298292, 'batch_size': 192, 'step_size': 11, 'gamma': 0.9482762904361357}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:11:39,866][0m Trial 20 finished with value: 0.04873696966763432 and parameters: {'observation_period_num': 30, 'train_rates': 0.8422451942060826, 'learning_rate': 1.5565830255261058e-05, 'batch_size': 131, 'step_size': 8, 'gamma': 0.8800268725599412}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:12:50,787][0m Trial 21 finished with value: 0.05509300534979681 and parameters: {'observation_period_num': 35, 'train_rates': 0.847779994366487, 'learning_rate': 1.5064501620765797e-05, 'batch_size': 128, 'step_size': 8, 'gamma': 0.8794800678827335}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:13:52,420][0m Trial 22 finished with value: 0.15246101048229613 and parameters: {'observation_period_num': 70, 'train_rates': 0.835955301911489, 'learning_rate': 6.741911626232034e-06, 'batch_size': 152, 'step_size': 10, 'gamma': 0.8989827526048719}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:15:39,727][0m Trial 23 finished with value: 0.054885582682797286 and parameters: {'observation_period_num': 35, 'train_rates': 0.8971531498688851, 'learning_rate': 0.0005629815163155879, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8775655689088291}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:16:52,562][0m Trial 24 finished with value: 0.13409596979618071 and parameters: {'observation_period_num': 99, 'train_rates': 0.9265627971857507, 'learning_rate': 1.558391942710156e-05, 'batch_size': 132, 'step_size': 9, 'gamma': 0.8238663065860636}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:17:50,597][0m Trial 25 finished with value: 0.3615206226524783 and parameters: {'observation_period_num': 30, 'train_rates': 0.7756893571782452, 'learning_rate': 1.7808237057406391e-06, 'batch_size': 186, 'step_size': 11, 'gamma': 0.9192288991074562}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:18:52,934][0m Trial 26 finished with value: 0.06566439127899437 and parameters: {'observation_period_num': 62, 'train_rates': 0.8179523788375618, 'learning_rate': 0.0005920728741395384, 'batch_size': 154, 'step_size': 7, 'gamma': 0.951490209513836}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:20:32,332][0m Trial 27 finished with value: 0.1668957753938699 and parameters: {'observation_period_num': 133, 'train_rates': 0.7744632206351639, 'learning_rate': 8.776858872680794e-06, 'batch_size': 76, 'step_size': 10, 'gamma': 0.7750332952374949}. Best is trial 14 with value: 0.03944696859314532.[0m
[32m[I 2025-01-10 16:21:46,589][0m Trial 28 finished with value: 0.024742075403969294 and parameters: {'observation_period_num': 6, 'train_rates': 0.8655329045981331, 'learning_rate': 0.0002131369163969969, 'batch_size': 122, 'step_size': 12, 'gamma': 0.8661180532610977}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:26:30,895][0m Trial 29 finished with value: 0.0958696094053197 and parameters: {'observation_period_num': 15, 'train_rates': 0.8658634334755021, 'learning_rate': 0.0001756116923317083, 'batch_size': 29, 'step_size': 12, 'gamma': 0.8037883772263741}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:29:04,516][0m Trial 30 finished with value: 0.046061276797220795 and parameters: {'observation_period_num': 24, 'train_rates': 0.9186764163696044, 'learning_rate': 2.786103719597266e-05, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8643726558644973}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:31:49,175][0m Trial 31 finished with value: 0.059086117998395166 and parameters: {'observation_period_num': 24, 'train_rates': 0.9275474982779252, 'learning_rate': 2.5160344333823217e-05, 'batch_size': 53, 'step_size': 14, 'gamma': 0.8284102950088089}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:34:43,738][0m Trial 32 finished with value: 0.05540161472997924 and parameters: {'observation_period_num': 45, 'train_rates': 0.9558567132749615, 'learning_rate': 6.137480886707038e-05, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8658320061670158}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:36:12,570][0m Trial 33 finished with value: 0.03443592094949314 and parameters: {'observation_period_num': 13, 'train_rates': 0.8666612844595186, 'learning_rate': 3.4639541336679135e-05, 'batch_size': 96, 'step_size': 12, 'gamma': 0.8490658357564678}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:37:47,325][0m Trial 34 finished with value: 0.027328470024220952 and parameters: {'observation_period_num': 11, 'train_rates': 0.9095398584623546, 'learning_rate': 0.00023062739703204704, 'batch_size': 95, 'step_size': 15, 'gamma': 0.8475586882511466}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:39:19,133][0m Trial 35 finished with value: 0.02524223538526034 and parameters: {'observation_period_num': 6, 'train_rates': 0.9001810092011626, 'learning_rate': 0.00022662930187025964, 'batch_size': 95, 'step_size': 15, 'gamma': 0.8412387568326053}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:40:54,181][0m Trial 36 finished with value: 0.027496928367398532 and parameters: {'observation_period_num': 6, 'train_rates': 0.8962758430540463, 'learning_rate': 0.00021461021111489006, 'batch_size': 93, 'step_size': 15, 'gamma': 0.8456272104480498}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:42:37,678][0m Trial 37 finished with value: 0.03170236267826774 and parameters: {'observation_period_num': 5, 'train_rates': 0.9627863905263261, 'learning_rate': 0.00022226593101953762, 'batch_size': 90, 'step_size': 15, 'gamma': 0.837171044650939}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:43:51,184][0m Trial 38 finished with value: 0.09245455551605958 and parameters: {'observation_period_num': 147, 'train_rates': 0.9030185508142416, 'learning_rate': 0.00013915873490382357, 'batch_size': 118, 'step_size': 15, 'gamma': 0.8471636533164039}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:45:51,032][0m Trial 39 finished with value: 0.1309812649719941 and parameters: {'observation_period_num': 205, 'train_rates': 0.8811461320599044, 'learning_rate': 0.00025198587611074464, 'batch_size': 67, 'step_size': 14, 'gamma': 0.8146414406404651}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:47:25,116][0m Trial 40 finished with value: 0.04387973630685865 and parameters: {'observation_period_num': 18, 'train_rates': 0.9450813870476524, 'learning_rate': 7.830481565528558e-05, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8501769444581335}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:49:07,517][0m Trial 41 finished with value: 0.05409625545144081 and parameters: {'observation_period_num': 6, 'train_rates': 0.9725611736970268, 'learning_rate': 0.00021982525695682107, 'batch_size': 91, 'step_size': 15, 'gamma': 0.834832335243368}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:50:26,066][0m Trial 42 finished with value: 0.0307289008051157 and parameters: {'observation_period_num': 5, 'train_rates': 0.9612396120444289, 'learning_rate': 0.00017121155103864364, 'batch_size': 118, 'step_size': 15, 'gamma': 0.8425302783916}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:51:42,200][0m Trial 43 finished with value: 0.03360272856785896 and parameters: {'observation_period_num': 21, 'train_rates': 0.9040634129621687, 'learning_rate': 0.00015524156637821844, 'batch_size': 116, 'step_size': 14, 'gamma': 0.8552063418628525}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:52:49,444][0m Trial 44 finished with value: 0.033627143164457224 and parameters: {'observation_period_num': 18, 'train_rates': 0.9087516828144584, 'learning_rate': 0.0003404238771532136, 'batch_size': 141, 'step_size': 15, 'gamma': 0.8694425284506124}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:54:10,864][0m Trial 45 finished with value: 0.04309077933430672 and parameters: {'observation_period_num': 5, 'train_rates': 0.9879577907946763, 'learning_rate': 4.720112223873861e-05, 'batch_size': 121, 'step_size': 13, 'gamma': 0.894819301878676}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:55:36,804][0m Trial 46 finished with value: 0.04973653012088367 and parameters: {'observation_period_num': 37, 'train_rates': 0.9473500942742307, 'learning_rate': 0.0001034877537975998, 'batch_size': 104, 'step_size': 15, 'gamma': 0.8196701253442155}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:57:15,197][0m Trial 47 finished with value: 0.07912999727219965 and parameters: {'observation_period_num': 62, 'train_rates': 0.8672284835683819, 'learning_rate': 0.0006953651469220687, 'batch_size': 85, 'step_size': 13, 'gamma': 0.8416891459268963}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 16:59:20,829][0m Trial 48 finished with value: 0.05360708480245037 and parameters: {'observation_period_num': 28, 'train_rates': 0.8869449258207038, 'learning_rate': 0.00029757834152439525, 'batch_size': 70, 'step_size': 14, 'gamma': 0.7973660010897768}. Best is trial 28 with value: 0.024742075403969294.[0m
[32m[I 2025-01-10 17:00:46,665][0m Trial 49 finished with value: 0.16241434591109527 and parameters: {'observation_period_num': 184, 'train_rates': 0.9355652891976206, 'learning_rate': 0.00042542928990243833, 'batch_size': 103, 'step_size': 12, 'gamma': 0.8584076853585577}. Best is trial 28 with value: 0.024742075403969294.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-10 17:00:46,675][0m A new study created in memory with name: no-name-9bcf4305-e010-4316-bf18-78e49fad4dbf[0m
[32m[I 2025-01-10 17:01:43,463][0m Trial 0 finished with value: 0.44501341664217375 and parameters: {'observation_period_num': 153, 'train_rates': 0.8667188752254866, 'learning_rate': 2.142673685490973e-06, 'batch_size': 212, 'step_size': 6, 'gamma': 0.9018621867776345}. Best is trial 0 with value: 0.44501341664217375.[0m
[32m[I 2025-01-10 17:02:42,219][0m Trial 1 finished with value: 0.8730146309425091 and parameters: {'observation_period_num': 207, 'train_rates': 0.852179731000983, 'learning_rate': 1.888819562919775e-06, 'batch_size': 161, 'step_size': 1, 'gamma': 0.9401143120002047}. Best is trial 0 with value: 0.44501341664217375.[0m
[32m[I 2025-01-10 17:03:41,214][0m Trial 2 finished with value: 0.19831360760049074 and parameters: {'observation_period_num': 163, 'train_rates': 0.825960525629688, 'learning_rate': 8.17744646005355e-06, 'batch_size': 176, 'step_size': 12, 'gamma': 0.9005173657157204}. Best is trial 2 with value: 0.19831360760049074.[0m
[32m[I 2025-01-10 17:04:27,929][0m Trial 3 finished with value: 0.2920196470304376 and parameters: {'observation_period_num': 192, 'train_rates': 0.6528578485890729, 'learning_rate': 0.00014826460259803758, 'batch_size': 206, 'step_size': 3, 'gamma': 0.8507398279344999}. Best is trial 2 with value: 0.19831360760049074.[0m
[32m[I 2025-01-10 17:05:24,215][0m Trial 4 finished with value: 0.2571315208446517 and parameters: {'observation_period_num': 185, 'train_rates': 0.8048409119069965, 'learning_rate': 7.627340841016879e-06, 'batch_size': 182, 'step_size': 9, 'gamma': 0.7784344402755051}. Best is trial 2 with value: 0.19831360760049074.[0m
[32m[I 2025-01-10 17:06:37,828][0m Trial 5 finished with value: 0.0650812101561621 and parameters: {'observation_period_num': 7, 'train_rates': 0.7182698973206041, 'learning_rate': 3.667458330706997e-05, 'batch_size': 102, 'step_size': 2, 'gamma': 0.8383021178340551}. Best is trial 5 with value: 0.0650812101561621.[0m
[32m[I 2025-01-10 17:07:31,309][0m Trial 6 finished with value: 0.2767076553933531 and parameters: {'observation_period_num': 192, 'train_rates': 0.6900568860044448, 'learning_rate': 2.863570197283982e-05, 'batch_size': 158, 'step_size': 7, 'gamma': 0.8340161194833767}. Best is trial 5 with value: 0.0650812101561621.[0m
[32m[I 2025-01-10 17:08:36,454][0m Trial 7 finished with value: 0.3122113049030304 and parameters: {'observation_period_num': 146, 'train_rates': 0.8149221156068043, 'learning_rate': 2.9521439091779324e-06, 'batch_size': 128, 'step_size': 6, 'gamma': 0.9445246617405582}. Best is trial 5 with value: 0.0650812101561621.[0m
[32m[I 2025-01-10 17:12:46,804][0m Trial 8 finished with value: 0.15889541998917706 and parameters: {'observation_period_num': 135, 'train_rates': 0.7562747242122669, 'learning_rate': 6.777339967953123e-05, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9114781733045907}. Best is trial 5 with value: 0.0650812101561621.[0m
[32m[I 2025-01-10 17:14:33,937][0m Trial 9 finished with value: 0.5874017828219645 and parameters: {'observation_period_num': 240, 'train_rates': 0.9320444825672662, 'learning_rate': 1.5492350893977582e-06, 'batch_size': 78, 'step_size': 4, 'gamma': 0.7831506231041205}. Best is trial 5 with value: 0.0650812101561621.[0m
[32m[I 2025-01-10 17:15:23,141][0m Trial 10 finished with value: 0.06280207940178492 and parameters: {'observation_period_num': 6, 'train_rates': 0.6171071972473873, 'learning_rate': 0.0006665997173159414, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9866512258931489}. Best is trial 10 with value: 0.06280207940178492.[0m
[32m[I 2025-01-10 17:16:12,260][0m Trial 11 finished with value: 0.05833805858538943 and parameters: {'observation_period_num': 16, 'train_rates': 0.6126658637251374, 'learning_rate': 0.0007885320761400486, 'batch_size': 254, 'step_size': 1, 'gamma': 0.980354418832968}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:17:01,103][0m Trial 12 finished with value: 0.07331813689463072 and parameters: {'observation_period_num': 21, 'train_rates': 0.6056255562545326, 'learning_rate': 0.0009729307315171612, 'batch_size': 247, 'step_size': 1, 'gamma': 0.982148957136894}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:17:46,872][0m Trial 13 finished with value: 0.16119325886534147 and parameters: {'observation_period_num': 57, 'train_rates': 0.6042112433734672, 'learning_rate': 0.0009604134105274641, 'batch_size': 255, 'step_size': 4, 'gamma': 0.9837123818030805}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:18:38,164][0m Trial 14 finished with value: 0.11711838562734322 and parameters: {'observation_period_num': 87, 'train_rates': 0.6716168889668931, 'learning_rate': 0.0002581622421240766, 'batch_size': 233, 'step_size': 9, 'gamma': 0.9533323947403894}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:19:29,316][0m Trial 15 finished with value: 0.10296970794323405 and parameters: {'observation_period_num': 90, 'train_rates': 0.7350661047378296, 'learning_rate': 0.00041800476738299746, 'batch_size': 216, 'step_size': 4, 'gamma': 0.9856311941151158}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:20:19,455][0m Trial 16 finished with value: 0.07354593036659098 and parameters: {'observation_period_num': 46, 'train_rates': 0.6386181761323615, 'learning_rate': 0.00033193246323512583, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9279399343603757}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:21:25,039][0m Trial 17 finished with value: 0.12226568907499313 and parameters: {'observation_period_num': 96, 'train_rates': 0.97237464117891, 'learning_rate': 8.499887748562775e-05, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9623094045791091}. Best is trial 11 with value: 0.05833805858538943.[0m
[32m[I 2025-01-10 17:22:18,061][0m Trial 18 finished with value: 0.04763818118551167 and parameters: {'observation_period_num': 45, 'train_rates': 0.7666536272838449, 'learning_rate': 0.0006051097227814918, 'batch_size': 254, 'step_size': 3, 'gamma': 0.8860127435701511}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:26:15,716][0m Trial 19 finished with value: 0.05354629219781123 and parameters: {'observation_period_num': 46, 'train_rates': 0.769926885946883, 'learning_rate': 0.00015690299124518353, 'batch_size': 32, 'step_size': 5, 'gamma': 0.8724287742401527}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:31:53,044][0m Trial 20 finished with value: 0.10393379372162898 and parameters: {'observation_period_num': 55, 'train_rates': 0.7685391695590783, 'learning_rate': 0.00020767943890324724, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8789652724113663}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:34:04,781][0m Trial 21 finished with value: 0.07284188825892972 and parameters: {'observation_period_num': 34, 'train_rates': 0.7056350445452545, 'learning_rate': 0.00048223910199761687, 'batch_size': 55, 'step_size': 3, 'gamma': 0.8732408136718498}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:35:20,352][0m Trial 22 finished with value: 0.049841946226134644 and parameters: {'observation_period_num': 70, 'train_rates': 0.8984669469903397, 'learning_rate': 0.00013765120083857648, 'batch_size': 114, 'step_size': 5, 'gamma': 0.8149253319583503}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:36:37,251][0m Trial 23 finished with value: 0.06934881806373597 and parameters: {'observation_period_num': 112, 'train_rates': 0.8963194552294546, 'learning_rate': 0.00010569430055700106, 'batch_size': 114, 'step_size': 5, 'gamma': 0.8070609614505011}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:38:21,909][0m Trial 24 finished with value: 0.07072684589134302 and parameters: {'observation_period_num': 69, 'train_rates': 0.7756045552516877, 'learning_rate': 3.400723794828873e-05, 'batch_size': 74, 'step_size': 8, 'gamma': 0.7556014118136647}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:41:15,811][0m Trial 25 finished with value: 0.07631521672010422 and parameters: {'observation_period_num': 72, 'train_rates': 0.9888298284131324, 'learning_rate': 0.00017181395254933118, 'batch_size': 51, 'step_size': 5, 'gamma': 0.8149126443164205}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:42:43,533][0m Trial 26 finished with value: 0.09634786627946361 and parameters: {'observation_period_num': 116, 'train_rates': 0.9126746327490179, 'learning_rate': 5.5919539926642964e-05, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8557002158435315}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:43:46,937][0m Trial 27 finished with value: 0.05628674332466391 and parameters: {'observation_period_num': 32, 'train_rates': 0.852180253630617, 'learning_rate': 1.6318817853503998e-05, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8796160143713089}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:44:49,525][0m Trial 28 finished with value: 0.06034081353413269 and parameters: {'observation_period_num': 73, 'train_rates': 0.7852738738292968, 'learning_rate': 0.00012548098236767405, 'batch_size': 133, 'step_size': 5, 'gamma': 0.8086400710582544}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:48:07,261][0m Trial 29 finished with value: 0.06943621094627626 and parameters: {'observation_period_num': 47, 'train_rates': 0.8804599206141818, 'learning_rate': 0.0002901068528509646, 'batch_size': 42, 'step_size': 6, 'gamma': 0.8966107398508505}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:49:46,262][0m Trial 30 finished with value: 0.12379198338912457 and parameters: {'observation_period_num': 106, 'train_rates': 0.741846219515201, 'learning_rate': 0.0004452923473743546, 'batch_size': 75, 'step_size': 8, 'gamma': 0.8568546555522311}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:50:50,984][0m Trial 31 finished with value: 0.06353136221413602 and parameters: {'observation_period_num': 31, 'train_rates': 0.8456623155341483, 'learning_rate': 1.598440721100396e-05, 'batch_size': 150, 'step_size': 7, 'gamma': 0.8831051084569985}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:51:51,209][0m Trial 32 finished with value: 0.0570217628835463 and parameters: {'observation_period_num': 33, 'train_rates': 0.8605480612454779, 'learning_rate': 1.9530968401137798e-05, 'batch_size': 182, 'step_size': 5, 'gamma': 0.9185398846247912}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:52:55,263][0m Trial 33 finished with value: 0.08515885576975248 and parameters: {'observation_period_num': 59, 'train_rates': 0.8329202452815949, 'learning_rate': 1.1274909989024567e-05, 'batch_size': 148, 'step_size': 7, 'gamma': 0.8890242756799149}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:54:18,616][0m Trial 34 finished with value: 0.1429173800755631 and parameters: {'observation_period_num': 37, 'train_rates': 0.939960754551215, 'learning_rate': 5.898988632407166e-06, 'batch_size': 114, 'step_size': 10, 'gamma': 0.8672960338887645}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:55:15,498][0m Trial 35 finished with value: 0.4154408029296942 and parameters: {'observation_period_num': 83, 'train_rates': 0.8016492103064299, 'learning_rate': 3.975450931910021e-06, 'batch_size': 167, 'step_size': 3, 'gamma': 0.8290751897115799}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:56:16,758][0m Trial 36 finished with value: 0.07112856680056551 and parameters: {'observation_period_num': 25, 'train_rates': 0.8759377997621398, 'learning_rate': 5.543260078890681e-05, 'batch_size': 193, 'step_size': 2, 'gamma': 0.9038507387887081}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:57:43,217][0m Trial 37 finished with value: 0.11309262563256507 and parameters: {'observation_period_num': 165, 'train_rates': 0.8314114255117899, 'learning_rate': 2.2617419592488542e-05, 'batch_size': 91, 'step_size': 8, 'gamma': 0.846214293867256}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:59:00,373][0m Trial 38 finished with value: 0.2679348093912056 and parameters: {'observation_period_num': 46, 'train_rates': 0.7985231566326116, 'learning_rate': 1.083079107478503e-06, 'batch_size': 109, 'step_size': 13, 'gamma': 0.8691907090255461}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 17:59:58,608][0m Trial 39 finished with value: 0.08927150368690491 and parameters: {'observation_period_num': 64, 'train_rates': 0.849550005579617, 'learning_rate': 4.2096557913905374e-05, 'batch_size': 216, 'step_size': 6, 'gamma': 0.8244265190712495}. Best is trial 18 with value: 0.04763818118551167.[0m
[32m[I 2025-01-10 18:02:14,932][0m Trial 40 finished with value: 0.03459307869533764 and parameters: {'observation_period_num': 14, 'train_rates': 0.8987893048534606, 'learning_rate': 9.178845027005901e-05, 'batch_size': 63, 'step_size': 4, 'gamma': 0.7864581788557706}. Best is trial 40 with value: 0.03459307869533764.[0m
[32m[I 2025-01-10 18:06:10,337][0m Trial 41 finished with value: 0.023474511659393706 and parameters: {'observation_period_num': 14, 'train_rates': 0.9083644054545442, 'learning_rate': 9.030285193469562e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.7904174369401347}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:10:28,181][0m Trial 42 finished with value: 0.03549808149059482 and parameters: {'observation_period_num': 15, 'train_rates': 0.948797630384209, 'learning_rate': 9.968061571322596e-05, 'batch_size': 34, 'step_size': 2, 'gamma': 0.7863128274228384}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:12:58,815][0m Trial 43 finished with value: 0.06165132072422054 and parameters: {'observation_period_num': 11, 'train_rates': 0.9513936283475068, 'learning_rate': 9.600330042905079e-05, 'batch_size': 60, 'step_size': 2, 'gamma': 0.7846634472760199}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:21:15,881][0m Trial 44 finished with value: 0.027535991588600067 and parameters: {'observation_period_num': 7, 'train_rates': 0.9144168320249924, 'learning_rate': 7.636730849024332e-05, 'batch_size': 17, 'step_size': 2, 'gamma': 0.793559778300634}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:30:05,168][0m Trial 45 finished with value: 0.03226484018472993 and parameters: {'observation_period_num': 5, 'train_rates': 0.9168970147506345, 'learning_rate': 7.314331726713207e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7677542470478027}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:39:00,114][0m Trial 46 finished with value: 0.03226495318506893 and parameters: {'observation_period_num': 9, 'train_rates': 0.9225407462670381, 'learning_rate': 6.83551530581085e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.7603950382958982}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:46:32,549][0m Trial 47 finished with value: 0.03572986021828957 and parameters: {'observation_period_num': 5, 'train_rates': 0.9206111407431452, 'learning_rate': 6.924758255821064e-05, 'batch_size': 19, 'step_size': 2, 'gamma': 0.7511418079025529}. Best is trial 41 with value: 0.023474511659393706.[0m
[32m[I 2025-01-10 18:50:07,095][0m Trial 48 finished with value: 0.042085197244612675 and parameters: {'observation_period_num': 21, 'train_rates': 0.9640640918590383, 'learning_rate': 4.845824089458546e-05, 'batch_size': 42, 'step_size': 4, 'gamma': 0.7675896446370967}. Best is trial 41 with value: 0.023474511659393706.[0m
Early stopping at epoch 85
[32m[I 2025-01-10 18:56:38,611][0m Trial 49 finished with value: 0.19508891242245832 and parameters: {'observation_period_num': 233, 'train_rates': 0.8987624498644289, 'learning_rate': 2.86298465275959e-05, 'batch_size': 17, 'step_size': 1, 'gamma': 0.7957148954011748}. Best is trial 41 with value: 0.023474511659393706.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-10 18:56:38,621][0m A new study created in memory with name: no-name-1a9d5e35-4f17-4401-aa09-28eeec81a4ce[0m
[32m[I 2025-01-10 18:57:56,274][0m Trial 0 finished with value: 0.13269223845480033 and parameters: {'observation_period_num': 125, 'train_rates': 0.7248722155313736, 'learning_rate': 1.9642997991080573e-05, 'batch_size': 94, 'step_size': 7, 'gamma': 0.8415226442834174}. Best is trial 0 with value: 0.13269223845480033.[0m
[32m[I 2025-01-10 19:00:06,945][0m Trial 1 finished with value: 0.08275631337822555 and parameters: {'observation_period_num': 28, 'train_rates': 0.7146324448386965, 'learning_rate': 6.274694506963607e-06, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7912640255063754}. Best is trial 1 with value: 0.08275631337822555.[0m
[32m[I 2025-01-10 19:01:52,867][0m Trial 2 finished with value: 0.1215922507227108 and parameters: {'observation_period_num': 128, 'train_rates': 0.8686249332928568, 'learning_rate': 6.260549909997878e-06, 'batch_size': 77, 'step_size': 10, 'gamma': 0.90836359658904}. Best is trial 1 with value: 0.08275631337822555.[0m
[32m[I 2025-01-10 19:02:55,002][0m Trial 3 finished with value: 0.09426632969503335 and parameters: {'observation_period_num': 81, 'train_rates': 0.9029097169479301, 'learning_rate': 0.00011790868223534837, 'batch_size': 233, 'step_size': 15, 'gamma': 0.9599750515796173}. Best is trial 1 with value: 0.08275631337822555.[0m
[32m[I 2025-01-10 19:05:43,733][0m Trial 4 finished with value: 0.03683935332709991 and parameters: {'observation_period_num': 35, 'train_rates': 0.7819423865839499, 'learning_rate': 0.0001107021851199697, 'batch_size': 46, 'step_size': 4, 'gamma': 0.8436209370665505}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:07:30,180][0m Trial 5 finished with value: 0.1046676494573292 and parameters: {'observation_period_num': 182, 'train_rates': 0.965853183015237, 'learning_rate': 0.00010992572246678521, 'batch_size': 82, 'step_size': 11, 'gamma': 0.8390340642932455}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:08:46,991][0m Trial 6 finished with value: 0.04792019404629443 and parameters: {'observation_period_num': 15, 'train_rates': 0.7602732771099289, 'learning_rate': 1.7480667540630068e-05, 'batch_size': 102, 'step_size': 13, 'gamma': 0.8553119323807069}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:10:54,605][0m Trial 7 finished with value: 0.17126844611352052 and parameters: {'observation_period_num': 199, 'train_rates': 0.732119111700207, 'learning_rate': 0.0009250349268068927, 'batch_size': 55, 'step_size': 9, 'gamma': 0.7558632643115045}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:11:40,542][0m Trial 8 finished with value: 0.671619458407301 and parameters: {'observation_period_num': 217, 'train_rates': 0.6035018268493011, 'learning_rate': 2.8932811928342223e-06, 'batch_size': 225, 'step_size': 3, 'gamma': 0.9447278707117759}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:12:44,298][0m Trial 9 finished with value: 0.04908985855297808 and parameters: {'observation_period_num': 28, 'train_rates': 0.7215129985914929, 'learning_rate': 0.0005502393210824554, 'batch_size': 127, 'step_size': 14, 'gamma': 0.8195662751267652}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:18:23,954][0m Trial 10 finished with value: 0.06803798588821061 and parameters: {'observation_period_num': 82, 'train_rates': 0.8333937921881225, 'learning_rate': 0.00011900694725194189, 'batch_size': 23, 'step_size': 1, 'gamma': 0.890208798316942}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:19:23,521][0m Trial 11 finished with value: 0.056169790717271656 and parameters: {'observation_period_num': 10, 'train_rates': 0.792321825658094, 'learning_rate': 2.265317929372598e-05, 'batch_size': 164, 'step_size': 4, 'gamma': 0.8734614532565456}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:20:18,562][0m Trial 12 finished with value: 0.09401722839594237 and parameters: {'observation_period_num': 67, 'train_rates': 0.6459042943527351, 'learning_rate': 4.505024035291508e-05, 'batch_size': 137, 'step_size': 12, 'gamma': 0.9137008106798246}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:24:52,623][0m Trial 13 finished with value: 0.2125482079796136 and parameters: {'observation_period_num': 52, 'train_rates': 0.7844099905837622, 'learning_rate': 1.2461602881123373e-06, 'batch_size': 28, 'step_size': 5, 'gamma': 0.8003961289797664}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:26:04,129][0m Trial 14 finished with value: 0.06416365671758503 and parameters: {'observation_period_num': 107, 'train_rates': 0.8187220045903332, 'learning_rate': 4.766298825768953e-05, 'batch_size': 116, 'step_size': 13, 'gamma': 0.8568130940366794}. Best is trial 4 with value: 0.03683935332709991.[0m
Early stopping at epoch 50
[32m[I 2025-01-10 19:26:31,614][0m Trial 15 finished with value: 0.09589723954425254 and parameters: {'observation_period_num': 8, 'train_rates': 0.6681968106296281, 'learning_rate': 0.00025656611730179096, 'batch_size': 189, 'step_size': 1, 'gamma': 0.7609876157414026}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:29:05,269][0m Trial 16 finished with value: 0.1356575617765787 and parameters: {'observation_period_num': 156, 'train_rates': 0.755979801049801, 'learning_rate': 1.3124764201618095e-05, 'batch_size': 46, 'step_size': 6, 'gamma': 0.875579280081776}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:30:29,425][0m Trial 17 finished with value: 0.03940508318643121 and parameters: {'observation_period_num': 50, 'train_rates': 0.9014348698480049, 'learning_rate': 0.0002797485406805004, 'batch_size': 104, 'step_size': 9, 'gamma': 0.821003729659735}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:31:35,699][0m Trial 18 finished with value: 0.04522464501522702 and parameters: {'observation_period_num': 51, 'train_rates': 0.9420872663180596, 'learning_rate': 0.0002752396506967132, 'batch_size': 163, 'step_size': 8, 'gamma': 0.7922402379011305}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:33:29,491][0m Trial 19 finished with value: 0.0774551382197476 and parameters: {'observation_period_num': 94, 'train_rates': 0.8852699191815298, 'learning_rate': 0.00033707333206227055, 'batch_size': 73, 'step_size': 3, 'gamma': 0.8165707052711596}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:34:32,429][0m Trial 20 finished with value: 0.1479774681376476 and parameters: {'observation_period_num': 249, 'train_rates': 0.9247404791548943, 'learning_rate': 7.536178994897886e-05, 'batch_size': 152, 'step_size': 9, 'gamma': 0.821476221342633}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:35:38,854][0m Trial 21 finished with value: 0.04815943166613579 and parameters: {'observation_period_num': 46, 'train_rates': 0.9882712167004817, 'learning_rate': 0.00021542152326263289, 'batch_size': 187, 'step_size': 8, 'gamma': 0.7795971192796303}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:36:44,725][0m Trial 22 finished with value: 0.05202380195260048 and parameters: {'observation_period_num': 41, 'train_rates': 0.9416258048659863, 'learning_rate': 0.000451380722694901, 'batch_size': 182, 'step_size': 6, 'gamma': 0.7801089827314936}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:37:48,577][0m Trial 23 finished with value: 0.05439633638783417 and parameters: {'observation_period_num': 63, 'train_rates': 0.852406587308638, 'learning_rate': 0.0009341035837844918, 'batch_size': 150, 'step_size': 10, 'gamma': 0.8033002511655176}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:38:48,556][0m Trial 24 finished with value: 0.07645442956635932 and parameters: {'observation_period_num': 108, 'train_rates': 0.9164320494004754, 'learning_rate': 0.00016566812741546465, 'batch_size': 205, 'step_size': 4, 'gamma': 0.8371260417104055}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:40:08,564][0m Trial 25 finished with value: 0.06378145939146979 and parameters: {'observation_period_num': 66, 'train_rates': 0.9416285073479068, 'learning_rate': 6.432838470413258e-05, 'batch_size': 114, 'step_size': 7, 'gamma': 0.7782906715596112}. Best is trial 4 with value: 0.03683935332709991.[0m
[32m[I 2025-01-10 19:41:13,671][0m Trial 26 finished with value: 0.03182520940899849 and parameters: {'observation_period_num': 33, 'train_rates': 0.8204098951072781, 'learning_rate': 0.0005443150200413882, 'batch_size': 135, 'step_size': 9, 'gamma': 0.8114101190688204}. Best is trial 26 with value: 0.03182520940899849.[0m
[32m[I 2025-01-10 19:44:33,757][0m Trial 27 finished with value: 0.03750183095895705 and parameters: {'observation_period_num': 28, 'train_rates': 0.8261741753406726, 'learning_rate': 0.0004907739604394158, 'batch_size': 40, 'step_size': 11, 'gamma': 0.8540323589223248}. Best is trial 26 with value: 0.03182520940899849.[0m
[32m[I 2025-01-10 19:47:56,792][0m Trial 28 finished with value: 0.04214924010561734 and parameters: {'observation_period_num': 30, 'train_rates': 0.8179084362214829, 'learning_rate': 0.0005165737975446979, 'batch_size': 39, 'step_size': 12, 'gamma': 0.8977209902012261}. Best is trial 26 with value: 0.03182520940899849.[0m
[32m[I 2025-01-10 19:48:51,829][0m Trial 29 finished with value: 0.15331138422091803 and parameters: {'observation_period_num': 153, 'train_rates': 0.8426292064786071, 'learning_rate': 0.0007023956436848726, 'batch_size': 249, 'step_size': 11, 'gamma': 0.8558698216756307}. Best is trial 26 with value: 0.03182520940899849.[0m
[32m[I 2025-01-10 19:56:12,084][0m Trial 30 finished with value: 0.045313669066948135 and parameters: {'observation_period_num': 28, 'train_rates': 0.7698917117982761, 'learning_rate': 0.00040878043521749456, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8381363723576338}. Best is trial 26 with value: 0.03182520940899849.[0m
[32m[I 2025-01-10 19:57:45,574][0m Trial 31 finished with value: 0.022797531995271878 and parameters: {'observation_period_num': 6, 'train_rates': 0.8720053831880314, 'learning_rate': 0.00017614605967754274, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8224035702976122}. Best is trial 31 with value: 0.022797531995271878.[0m
[32m[I 2025-01-10 19:59:59,568][0m Trial 32 finished with value: 0.025677755769146115 and parameters: {'observation_period_num': 18, 'train_rates': 0.805953011450168, 'learning_rate': 0.00015523027869830062, 'batch_size': 60, 'step_size': 7, 'gamma': 0.8557327446894264}. Best is trial 31 with value: 0.022797531995271878.[0m
[32m[I 2025-01-10 20:02:12,473][0m Trial 33 finished with value: 0.021543488284882582 and parameters: {'observation_period_num': 11, 'train_rates': 0.8714275801437029, 'learning_rate': 0.00016375788074059092, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8079903297393677}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:04:13,677][0m Trial 34 finished with value: 0.021755006183222465 and parameters: {'observation_period_num': 6, 'train_rates': 0.8664968448063303, 'learning_rate': 0.0001586333022910551, 'batch_size': 70, 'step_size': 7, 'gamma': 0.8068297721306587}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:06:13,670][0m Trial 35 finished with value: 0.02411737910491259 and parameters: {'observation_period_num': 5, 'train_rates': 0.8697885848078647, 'learning_rate': 0.0001775113857311031, 'batch_size': 73, 'step_size': 7, 'gamma': 0.768792146948267}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:07:45,977][0m Trial 36 finished with value: 0.03517154326433675 and parameters: {'observation_period_num': 6, 'train_rates': 0.8646745515590544, 'learning_rate': 6.789918778608821e-05, 'batch_size': 91, 'step_size': 6, 'gamma': 0.7673021754603854}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:09:47,986][0m Trial 37 finished with value: 0.0381305566471484 and parameters: {'observation_period_num': 15, 'train_rates': 0.8775990438640684, 'learning_rate': 3.1536068703999925e-05, 'batch_size': 70, 'step_size': 7, 'gamma': 0.7908205040989098}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:11:28,869][0m Trial 38 finished with value: 0.03434883007271723 and parameters: {'observation_period_num': 6, 'train_rates': 0.8918122534943274, 'learning_rate': 0.0001662868505961666, 'batch_size': 87, 'step_size': 5, 'gamma': 0.7502129991545894}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:13:36,477][0m Trial 39 finished with value: 0.03676361332097686 and parameters: {'observation_period_num': 20, 'train_rates': 0.8586376151420715, 'learning_rate': 9.208140434996736e-05, 'batch_size': 66, 'step_size': 8, 'gamma': 0.9799542654797698}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:15:00,836][0m Trial 40 finished with value: 0.18534244133229077 and parameters: {'observation_period_num': 140, 'train_rates': 0.9145505411459032, 'learning_rate': 7.545579230851856e-06, 'batch_size': 99, 'step_size': 8, 'gamma': 0.8274075417295538}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:17:32,940][0m Trial 41 finished with value: 0.025202502722794052 and parameters: {'observation_period_num': 19, 'train_rates': 0.8794489265066463, 'learning_rate': 0.00016918936572028924, 'batch_size': 56, 'step_size': 7, 'gamma': 0.7968000172883661}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:19:21,358][0m Trial 42 finished with value: 0.02958034723997116 and parameters: {'observation_period_num': 21, 'train_rates': 0.8785106116661376, 'learning_rate': 0.00019986433213855636, 'batch_size': 80, 'step_size': 6, 'gamma': 0.7712723180698654}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:21:58,517][0m Trial 43 finished with value: 0.022561410229120935 and parameters: {'observation_period_num': 6, 'train_rates': 0.8457096572059252, 'learning_rate': 0.00011144032557704231, 'batch_size': 52, 'step_size': 5, 'gamma': 0.8044511404937553}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:24:36,788][0m Trial 44 finished with value: 0.026938913979536884 and parameters: {'observation_period_num': 5, 'train_rates': 0.8456074954395176, 'learning_rate': 0.00010886336182149139, 'batch_size': 53, 'step_size': 5, 'gamma': 0.80461821327233}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:26:29,173][0m Trial 45 finished with value: 0.037742714035173354 and parameters: {'observation_period_num': 39, 'train_rates': 0.8608544956337064, 'learning_rate': 0.00011840812055110636, 'batch_size': 75, 'step_size': 4, 'gamma': 0.786261476065041}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:30:55,296][0m Trial 46 finished with value: 0.03360765607079916 and parameters: {'observation_period_num': 36, 'train_rates': 0.9025249555371958, 'learning_rate': 0.0003378569665139487, 'batch_size': 32, 'step_size': 3, 'gamma': 0.8305191286594085}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:32:10,532][0m Trial 47 finished with value: 0.07390515646869333 and parameters: {'observation_period_num': 77, 'train_rates': 0.8031161359626138, 'learning_rate': 5.0852184146115356e-05, 'batch_size': 107, 'step_size': 5, 'gamma': 0.761818667740283}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:33:46,435][0m Trial 48 finished with value: 0.04842676284017077 and parameters: {'observation_period_num': 58, 'train_rates': 0.8373157220516301, 'learning_rate': 3.695831915229576e-05, 'batch_size': 84, 'step_size': 10, 'gamma': 0.8085663085717671}. Best is trial 33 with value: 0.021543488284882582.[0m
[32m[I 2025-01-10 20:36:06,073][0m Trial 49 finished with value: 0.032977412321737835 and parameters: {'observation_period_num': 22, 'train_rates': 0.9593894194592105, 'learning_rate': 0.0001432734900989764, 'batch_size': 64, 'step_size': 6, 'gamma': 0.7743632102996763}. Best is trial 33 with value: 0.021543488284882582.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-10 20:36:06,084][0m A new study created in memory with name: no-name-2a5a0fb1-00a1-4fb1-b585-a5a73fbb55eb[0m
[32m[I 2025-01-10 20:37:13,087][0m Trial 0 finished with value: 0.04029117461021354 and parameters: {'observation_period_num': 53, 'train_rates': 0.8445143261917561, 'learning_rate': 0.00013658716762101388, 'batch_size': 143, 'step_size': 15, 'gamma': 0.7600417990240077}. Best is trial 0 with value: 0.04029117461021354.[0m
[32m[I 2025-01-10 20:39:05,159][0m Trial 1 finished with value: 0.03596574440598488 and parameters: {'observation_period_num': 6, 'train_rates': 0.9796777681051554, 'learning_rate': 9.236131460214257e-05, 'batch_size': 83, 'step_size': 14, 'gamma': 0.7565319629093867}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:40:07,210][0m Trial 2 finished with value: 0.19403523733389788 and parameters: {'observation_period_num': 191, 'train_rates': 0.9163288423611639, 'learning_rate': 0.000314194043437391, 'batch_size': 165, 'step_size': 11, 'gamma': 0.9733059398630763}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:41:55,043][0m Trial 3 finished with value: 0.29493800925286434 and parameters: {'observation_period_num': 227, 'train_rates': 0.7346436392366417, 'learning_rate': 2.1167822883878234e-05, 'batch_size': 66, 'step_size': 3, 'gamma': 0.790397450966276}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:43:07,295][0m Trial 4 finished with value: 0.12279559940446255 and parameters: {'observation_period_num': 164, 'train_rates': 0.7532891779946557, 'learning_rate': 8.449244840010237e-05, 'batch_size': 100, 'step_size': 15, 'gamma': 0.8018561909977999}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:44:23,891][0m Trial 5 finished with value: 0.24732273689123116 and parameters: {'observation_period_num': 173, 'train_rates': 0.715029743986955, 'learning_rate': 4.477624849740847e-06, 'batch_size': 94, 'step_size': 5, 'gamma': 0.952027383665809}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:45:25,015][0m Trial 6 finished with value: 0.09780601403686438 and parameters: {'observation_period_num': 46, 'train_rates': 0.8163843615891639, 'learning_rate': 4.948888916877599e-06, 'batch_size': 150, 'step_size': 15, 'gamma': 0.8630462192941795}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:46:52,547][0m Trial 7 finished with value: 0.0521968202912702 and parameters: {'observation_period_num': 45, 'train_rates': 0.9222602926541728, 'learning_rate': 0.0009197698363387413, 'batch_size': 101, 'step_size': 2, 'gamma': 0.8059960886845019}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:48:03,073][0m Trial 8 finished with value: 0.43771544138708585 and parameters: {'observation_period_num': 166, 'train_rates': 0.7587015489472256, 'learning_rate': 2.4439844684235108e-06, 'batch_size': 106, 'step_size': 12, 'gamma': 0.8318410616578362}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:48:49,590][0m Trial 9 finished with value: 0.8128849996587802 and parameters: {'observation_period_num': 166, 'train_rates': 0.6256374070269677, 'learning_rate': 2.286975920616131e-06, 'batch_size': 234, 'step_size': 2, 'gamma': 0.928564609801171}. Best is trial 1 with value: 0.03596574440598488.[0m
[32m[I 2025-01-10 20:52:08,295][0m Trial 10 finished with value: 0.032217252999544144 and parameters: {'observation_period_num': 5, 'train_rates': 0.9897785238033389, 'learning_rate': 1.95398966736619e-05, 'batch_size': 46, 'step_size': 7, 'gamma': 0.901821768239087}. Best is trial 10 with value: 0.032217252999544144.[0m
[32m[I 2025-01-10 20:57:02,917][0m Trial 11 finished with value: 0.03053390124782187 and parameters: {'observation_period_num': 5, 'train_rates': 0.9794783886525228, 'learning_rate': 2.0255750069853177e-05, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8979124737857879}. Best is trial 11 with value: 0.03053390124782187.[0m
[32m[I 2025-01-10 21:03:08,252][0m Trial 12 finished with value: 0.09817426870850955 and parameters: {'observation_period_num': 97, 'train_rates': 0.9883442946574159, 'learning_rate': 1.6293605420900497e-05, 'batch_size': 24, 'step_size': 7, 'gamma': 0.9090504716042598}. Best is trial 11 with value: 0.03053390124782187.[0m
[32m[I 2025-01-10 21:09:12,745][0m Trial 13 finished with value: 0.029662364069692457 and parameters: {'observation_period_num': 18, 'train_rates': 0.9010171948818209, 'learning_rate': 1.2606759273933998e-05, 'batch_size': 23, 'step_size': 8, 'gamma': 0.8895711427859228}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:15:07,745][0m Trial 14 finished with value: 0.06218807956269699 and parameters: {'observation_period_num': 94, 'train_rates': 0.8911831563660304, 'learning_rate': 8.34051488816506e-06, 'batch_size': 23, 'step_size': 9, 'gamma': 0.8795049272977475}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:16:07,647][0m Trial 15 finished with value: 0.08078845217823982 and parameters: {'observation_period_num': 83, 'train_rates': 0.8685162629625772, 'learning_rate': 5.3374871078083824e-05, 'batch_size': 182, 'step_size': 9, 'gamma': 0.8574933867589226}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:18:55,155][0m Trial 16 finished with value: 0.18448344113365298 and parameters: {'observation_period_num': 28, 'train_rates': 0.9366078499449215, 'learning_rate': 1.1535201456619907e-06, 'batch_size': 53, 'step_size': 5, 'gamma': 0.9405932961757504}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:25:34,402][0m Trial 17 finished with value: 0.08552732595997135 and parameters: {'observation_period_num': 121, 'train_rates': 0.9516414005203169, 'learning_rate': 9.617692618844002e-06, 'batch_size': 21, 'step_size': 5, 'gamma': 0.8914697031315144}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:27:45,069][0m Trial 18 finished with value: 0.04774379293517311 and parameters: {'observation_period_num': 67, 'train_rates': 0.8251348002983273, 'learning_rate': 3.4595258243295844e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.8428591518667305}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:28:45,563][0m Trial 19 finished with value: 0.051065359952862314 and parameters: {'observation_period_num': 21, 'train_rates': 0.8838793583928126, 'learning_rate': 0.0001990664814216493, 'batch_size': 211, 'step_size': 8, 'gamma': 0.9799653234779715}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:29:42,963][0m Trial 20 finished with value: 0.11869279406564211 and parameters: {'observation_period_num': 127, 'train_rates': 0.6711442385675246, 'learning_rate': 4.6938546129087994e-05, 'batch_size': 127, 'step_size': 6, 'gamma': 0.9190896279465208}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:32:58,327][0m Trial 21 finished with value: 0.03861193956082104 and parameters: {'observation_period_num': 10, 'train_rates': 0.9556523718539058, 'learning_rate': 1.6330961487947356e-05, 'batch_size': 46, 'step_size': 8, 'gamma': 0.8986912968699894}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:36:41,034][0m Trial 22 finished with value: 0.04952051762049481 and parameters: {'observation_period_num': 6, 'train_rates': 0.9600632806398448, 'learning_rate': 8.227768766969162e-06, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8862401823016595}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:38:37,861][0m Trial 23 finished with value: 0.045847781990287884 and parameters: {'observation_period_num': 29, 'train_rates': 0.9100139832905825, 'learning_rate': 2.3196092491410784e-05, 'batch_size': 74, 'step_size': 10, 'gamma': 0.9501632781897237}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:42:47,544][0m Trial 24 finished with value: 0.09226752817630768 and parameters: {'observation_period_num': 66, 'train_rates': 0.9875785530028923, 'learning_rate': 1.1965441493244613e-05, 'batch_size': 36, 'step_size': 4, 'gamma': 0.908135312300552}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:50:43,949][0m Trial 25 finished with value: 0.04398156135258349 and parameters: {'observation_period_num': 33, 'train_rates': 0.8720204260976658, 'learning_rate': 5.221079007699264e-06, 'batch_size': 17, 'step_size': 7, 'gamma': 0.8704604349343079}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:53:25,799][0m Trial 26 finished with value: 0.23951096673431638 and parameters: {'observation_period_num': 252, 'train_rates': 0.9411930097384891, 'learning_rate': 3.4237276579571716e-05, 'batch_size': 50, 'step_size': 9, 'gamma': 0.9260702924156129}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:54:45,572][0m Trial 27 finished with value: 0.08216094225645065 and parameters: {'observation_period_num': 70, 'train_rates': 0.9648915948129679, 'learning_rate': 5.50211172573769e-05, 'batch_size': 118, 'step_size': 6, 'gamma': 0.8416350432452242}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 21:56:41,638][0m Trial 28 finished with value: 0.05864822730907927 and parameters: {'observation_period_num': 48, 'train_rates': 0.9050077999458426, 'learning_rate': 2.7659350551543408e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.9623190988085036}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 22:00:10,727][0m Trial 29 finished with value: 0.08480259589457109 and parameters: {'observation_period_num': 26, 'train_rates': 0.8384753065219501, 'learning_rate': 1.3664366370929022e-05, 'batch_size': 39, 'step_size': 1, 'gamma': 0.8804769174909545}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 22:02:34,758][0m Trial 30 finished with value: 0.0852816797354642 and parameters: {'observation_period_num': 107, 'train_rates': 0.8508495531971919, 'learning_rate': 0.00010326065983333444, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9056678037322718}. Best is trial 13 with value: 0.029662364069692457.[0m
[32m[I 2025-01-10 22:04:29,077][0m Trial 31 finished with value: 0.015636004507541656 and parameters: {'observation_period_num': 8, 'train_rates': 0.9880235239419404, 'learning_rate': 0.00029076026456633156, 'batch_size': 83, 'step_size': 13, 'gamma': 0.784870682275844}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:06:26,796][0m Trial 32 finished with value: 0.017035827040672302 and parameters: {'observation_period_num': 15, 'train_rates': 0.9863472333550499, 'learning_rate': 0.0008973065367317849, 'batch_size': 79, 'step_size': 10, 'gamma': 0.7752277952085943}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:07:38,639][0m Trial 33 finished with value: 0.03878933876209582 and parameters: {'observation_period_num': 19, 'train_rates': 0.9348612485741064, 'learning_rate': 0.0007464243264821509, 'batch_size': 143, 'step_size': 13, 'gamma': 0.7690848829355265}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:09:28,444][0m Trial 34 finished with value: 0.08000767224451952 and parameters: {'observation_period_num': 40, 'train_rates': 0.9609633387021138, 'learning_rate': 0.0005363254608411273, 'batch_size': 83, 'step_size': 12, 'gamma': 0.7722203164396766}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:10:59,848][0m Trial 35 finished with value: 0.06496816648557635 and parameters: {'observation_period_num': 60, 'train_rates': 0.7905868275424118, 'learning_rate': 0.00029777922442785897, 'batch_size': 85, 'step_size': 10, 'gamma': 0.8184976836059468}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:13:04,082][0m Trial 36 finished with value: 0.12523158699616618 and parameters: {'observation_period_num': 143, 'train_rates': 0.9693562902696662, 'learning_rate': 0.0005299708147081985, 'batch_size': 70, 'step_size': 11, 'gamma': 0.7539631502415575}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:14:18,575][0m Trial 37 finished with value: 0.13626721695713376 and parameters: {'observation_period_num': 201, 'train_rates': 0.9247567646717216, 'learning_rate': 0.00017794305325045682, 'batch_size': 118, 'step_size': 14, 'gamma': 0.7884987154021688}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:15:23,432][0m Trial 38 finished with value: 0.031186154052983747 and parameters: {'observation_period_num': 18, 'train_rates': 0.903645435739581, 'learning_rate': 0.0003189409828260143, 'batch_size': 164, 'step_size': 10, 'gamma': 0.7736829267578869}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:17:04,417][0m Trial 39 finished with value: 0.0680280551314354 and parameters: {'observation_period_num': 56, 'train_rates': 0.9784294439728628, 'learning_rate': 0.0004757237150881841, 'batch_size': 90, 'step_size': 12, 'gamma': 0.793139166481794}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:22:15,814][0m Trial 40 finished with value: 0.04291979174899018 and parameters: {'observation_period_num': 42, 'train_rates': 0.9447476730016611, 'learning_rate': 0.0009393376227006615, 'batch_size': 28, 'step_size': 14, 'gamma': 0.8093531548813925}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:23:15,369][0m Trial 41 finished with value: 0.033059552857211155 and parameters: {'observation_period_num': 14, 'train_rates': 0.793063666314802, 'learning_rate': 0.0002679689073454531, 'batch_size': 166, 'step_size': 10, 'gamma': 0.7776978093452752}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:24:19,963][0m Trial 42 finished with value: 0.04347642516972972 and parameters: {'observation_period_num': 19, 'train_rates': 0.9132212988146251, 'learning_rate': 0.00012792955458825798, 'batch_size': 173, 'step_size': 9, 'gamma': 0.7563441024966625}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:25:26,174][0m Trial 43 finished with value: 0.033644251793233396 and parameters: {'observation_period_num': 5, 'train_rates': 0.8961859125011588, 'learning_rate': 0.0003571581774626602, 'batch_size': 148, 'step_size': 8, 'gamma': 0.7852524385292756}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:26:54,114][0m Trial 44 finished with value: 0.03643124923110008 and parameters: {'observation_period_num': 32, 'train_rates': 0.9768126952259892, 'learning_rate': 0.00019951925651025423, 'batch_size': 107, 'step_size': 11, 'gamma': 0.7637961087140306}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:27:54,566][0m Trial 45 finished with value: 0.038827870376234024 and parameters: {'observation_period_num': 38, 'train_rates': 0.8532240133005232, 'learning_rate': 0.0007092045253532215, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8276539322497183}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:29:03,941][0m Trial 46 finished with value: 0.031745141851980095 and parameters: {'observation_period_num': 18, 'train_rates': 0.9219880145834007, 'learning_rate': 0.000678427419614186, 'batch_size': 136, 'step_size': 8, 'gamma': 0.7979615856689583}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:30:01,341][0m Trial 47 finished with value: 0.06787270489619689 and parameters: {'observation_period_num': 79, 'train_rates': 0.8762961052927338, 'learning_rate': 0.0004106616740086429, 'batch_size': 253, 'step_size': 11, 'gamma': 0.8588054068920152}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:30:53,962][0m Trial 48 finished with value: 0.17918806974316986 and parameters: {'observation_period_num': 54, 'train_rates': 0.7061283288178463, 'learning_rate': 5.893324665467693e-06, 'batch_size': 193, 'step_size': 15, 'gamma': 0.8164476972947333}. Best is trial 31 with value: 0.015636004507541656.[0m
[32m[I 2025-01-10 22:35:36,294][0m Trial 49 finished with value: 0.0644159664772451 and parameters: {'observation_period_num': 16, 'train_rates': 0.9319408858868704, 'learning_rate': 3.277738766054962e-06, 'batch_size': 31, 'step_size': 13, 'gamma': 0.7799501833736405}. Best is trial 31 with value: 0.015636004507541656.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 7, 'train_rates': 0.9451746782416619, 'learning_rate': 0.0005788101212791833, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9502112065533127}
Epoch 1/300, trend Loss: 0.2587 | 0.1716
Epoch 2/300, trend Loss: 0.1172 | 0.1205
Epoch 3/300, trend Loss: 0.1082 | 0.0969
Epoch 4/300, trend Loss: 0.1027 | 0.0874
Epoch 5/300, trend Loss: 0.0982 | 0.0826
Epoch 6/300, trend Loss: 0.0951 | 0.0806
Epoch 7/300, trend Loss: 0.0934 | 0.0895
Epoch 8/300, trend Loss: 0.0907 | 0.0937
Epoch 9/300, trend Loss: 0.0873 | 0.0892
Epoch 10/300, trend Loss: 0.0855 | 0.0830
Epoch 11/300, trend Loss: 0.0852 | 0.0714
Epoch 12/300, trend Loss: 0.0859 | 0.0570
Epoch 13/300, trend Loss: 0.0879 | 0.0573
Epoch 14/300, trend Loss: 0.0864 | 0.0543
Epoch 15/300, trend Loss: 0.0796 | 0.0501
Epoch 16/300, trend Loss: 0.0738 | 0.0454
Epoch 17/300, trend Loss: 0.0710 | 0.0428
Epoch 18/300, trend Loss: 0.0698 | 0.0421
Epoch 19/300, trend Loss: 0.0686 | 0.0420
Epoch 20/300, trend Loss: 0.0677 | 0.0420
Epoch 21/300, trend Loss: 0.0669 | 0.0402
Epoch 22/300, trend Loss: 0.0663 | 0.0401
Epoch 23/300, trend Loss: 0.0657 | 0.0394
Epoch 24/300, trend Loss: 0.0651 | 0.0385
Epoch 25/300, trend Loss: 0.0646 | 0.0379
Epoch 26/300, trend Loss: 0.0642 | 0.0377
Epoch 27/300, trend Loss: 0.0638 | 0.0371
Epoch 28/300, trend Loss: 0.0635 | 0.0368
Epoch 29/300, trend Loss: 0.0631 | 0.0365
Epoch 30/300, trend Loss: 0.0628 | 0.0361
Epoch 31/300, trend Loss: 0.0625 | 0.0358
Epoch 32/300, trend Loss: 0.0622 | 0.0355
Epoch 33/300, trend Loss: 0.0619 | 0.0353
Epoch 34/300, trend Loss: 0.0616 | 0.0359
Epoch 35/300, trend Loss: 0.0613 | 0.0359
Epoch 36/300, trend Loss: 0.0610 | 0.0349
Epoch 37/300, trend Loss: 0.0607 | 0.0348
Epoch 38/300, trend Loss: 0.0604 | 0.0348
Epoch 39/300, trend Loss: 0.0599 | 0.0348
Epoch 40/300, trend Loss: 0.0594 | 0.0347
Epoch 41/300, trend Loss: 0.0590 | 0.0347
Epoch 42/300, trend Loss: 0.0584 | 0.0342
Epoch 43/300, trend Loss: 0.0580 | 0.0338
Epoch 44/300, trend Loss: 0.0577 | 0.0338
Epoch 45/300, trend Loss: 0.0575 | 0.0337
Epoch 46/300, trend Loss: 0.0571 | 0.0334
Epoch 47/300, trend Loss: 0.0565 | 0.0328
Epoch 48/300, trend Loss: 0.0562 | 0.0326
Epoch 49/300, trend Loss: 0.0558 | 0.0326
Epoch 50/300, trend Loss: 0.0556 | 0.0324
Epoch 51/300, trend Loss: 0.0554 | 0.0323
Epoch 52/300, trend Loss: 0.0553 | 0.0323
Epoch 53/300, trend Loss: 0.0550 | 0.0318
Epoch 54/300, trend Loss: 0.0550 | 0.0317
Epoch 55/300, trend Loss: 0.0543 | 0.0324
Epoch 56/300, trend Loss: 0.0539 | 0.0323
Epoch 57/300, trend Loss: 0.0535 | 0.0315
Epoch 58/300, trend Loss: 0.0531 | 0.0310
Epoch 59/300, trend Loss: 0.0528 | 0.0307
Epoch 60/300, trend Loss: 0.0523 | 0.0301
Epoch 61/300, trend Loss: 0.0519 | 0.0302
Epoch 62/300, trend Loss: 0.0516 | 0.0304
Epoch 63/300, trend Loss: 0.0514 | 0.0306
Epoch 64/300, trend Loss: 0.0515 | 0.0325
Epoch 65/300, trend Loss: 0.0519 | 0.0369
Epoch 66/300, trend Loss: 0.0518 | 0.0355
Epoch 67/300, trend Loss: 0.0517 | 0.0337
Epoch 68/300, trend Loss: 0.0517 | 0.0353
Epoch 69/300, trend Loss: 0.0515 | 0.0339
Epoch 70/300, trend Loss: 0.0519 | 0.0360
Epoch 71/300, trend Loss: 0.0517 | 0.0307
Epoch 72/300, trend Loss: 0.0525 | 0.0292
Epoch 73/300, trend Loss: 0.0521 | 0.0324
Epoch 74/300, trend Loss: 0.0514 | 0.0331
Epoch 75/300, trend Loss: 0.0505 | 0.0332
Epoch 76/300, trend Loss: 0.0502 | 0.0324
Epoch 77/300, trend Loss: 0.0498 | 0.0321
Epoch 78/300, trend Loss: 0.0496 | 0.0320
Epoch 79/300, trend Loss: 0.0494 | 0.0317
Epoch 80/300, trend Loss: 0.0492 | 0.0314
Epoch 81/300, trend Loss: 0.0491 | 0.0320
Epoch 82/300, trend Loss: 0.0491 | 0.0320
Epoch 83/300, trend Loss: 0.0489 | 0.0317
Epoch 84/300, trend Loss: 0.0487 | 0.0324
Epoch 85/300, trend Loss: 0.0487 | 0.0321
Epoch 86/300, trend Loss: 0.0486 | 0.0319
Epoch 87/300, trend Loss: 0.0485 | 0.0317
Epoch 88/300, trend Loss: 0.0484 | 0.0313
Epoch 89/300, trend Loss: 0.0482 | 0.0319
Epoch 90/300, trend Loss: 0.0482 | 0.0321
Epoch 91/300, trend Loss: 0.0481 | 0.0323
Epoch 92/300, trend Loss: 0.0485 | 0.0307
Epoch 93/300, trend Loss: 0.0483 | 0.0294
Epoch 94/300, trend Loss: 0.0483 | 0.0300
Epoch 95/300, trend Loss: 0.0484 | 0.0272
Epoch 96/300, trend Loss: 0.0485 | 0.0289
Epoch 97/300, trend Loss: 0.0483 | 0.0289
Epoch 98/300, trend Loss: 0.0480 | 0.0298
Epoch 99/300, trend Loss: 0.0479 | 0.0302
Epoch 100/300, trend Loss: 0.0477 | 0.0299
Epoch 101/300, trend Loss: 0.0474 | 0.0307
Epoch 102/300, trend Loss: 0.0474 | 0.0324
Epoch 103/300, trend Loss: 0.0473 | 0.0309
Epoch 104/300, trend Loss: 0.0471 | 0.0322
Epoch 105/300, trend Loss: 0.0473 | 0.0370
Epoch 106/300, trend Loss: 0.0476 | 0.0358
Epoch 107/300, trend Loss: 0.0479 | 0.0327
Epoch 108/300, trend Loss: 0.0473 | 0.0306
Epoch 109/300, trend Loss: 0.0472 | 0.0297
Epoch 110/300, trend Loss: 0.0470 | 0.0281
Epoch 111/300, trend Loss: 0.0470 | 0.0279
Epoch 112/300, trend Loss: 0.0468 | 0.0289
Epoch 113/300, trend Loss: 0.0465 | 0.0283
Epoch 114/300, trend Loss: 0.0461 | 0.0277
Epoch 115/300, trend Loss: 0.0457 | 0.0280
Epoch 116/300, trend Loss: 0.0455 | 0.0290
Epoch 117/300, trend Loss: 0.0453 | 0.0289
Epoch 118/300, trend Loss: 0.0451 | 0.0291
Epoch 119/300, trend Loss: 0.0439 | 0.0292
Epoch 120/300, trend Loss: 0.0432 | 0.0302
Epoch 121/300, trend Loss: 0.0453 | 0.0306
Epoch 122/300, trend Loss: 0.0422 | 0.0297
Epoch 123/300, trend Loss: 0.0428 | 0.0302
Epoch 124/300, trend Loss: 0.0451 | 0.0306
Epoch 125/300, trend Loss: 0.0421 | 0.0299
Epoch 126/300, trend Loss: 0.0450 | 0.0349
Epoch 127/300, trend Loss: 0.0450 | 0.0306
Epoch 128/300, trend Loss: 0.0432 | 0.0312
Epoch 129/300, trend Loss: 0.0416 | 0.0340
Epoch 130/300, trend Loss: 0.0457 | 0.0316
Epoch 131/300, trend Loss: 0.0405 | 0.0312
Epoch 132/300, trend Loss: 0.0424 | 0.0306
Epoch 133/300, trend Loss: 0.0418 | 0.0329
Epoch 134/300, trend Loss: 0.0421 | 0.0318
Epoch 135/300, trend Loss: 0.0451 | 0.0321
Epoch 136/300, trend Loss: 0.0417 | 0.0316
Epoch 137/300, trend Loss: 0.0406 | 0.0311
Epoch 138/300, trend Loss: 0.0471 | 0.0375
Epoch 139/300, trend Loss: 0.0418 | 0.0345
Epoch 140/300, trend Loss: 0.0409 | 0.0328
Epoch 141/300, trend Loss: 0.0401 | 0.0337
Epoch 142/300, trend Loss: 0.0401 | 0.0324
Epoch 143/300, trend Loss: 0.0396 | 0.0325
Epoch 144/300, trend Loss: 0.0395 | 0.0323
Epoch 145/300, trend Loss: 0.0393 | 0.0322
Epoch 146/300, trend Loss: 0.0394 | 0.0322
Epoch 147/300, trend Loss: 0.0387 | 0.0323
Epoch 148/300, trend Loss: 0.0416 | 0.0346
Epoch 149/300, trend Loss: 0.0423 | 0.0339
Epoch 150/300, trend Loss: 0.0527 | 0.0465
Epoch 151/300, trend Loss: 0.0444 | 0.0493
Epoch 152/300, trend Loss: 0.0433 | 0.0574
Epoch 153/300, trend Loss: 0.0416 | 0.0403
Epoch 154/300, trend Loss: 0.0412 | 0.0349
Epoch 155/300, trend Loss: 0.0404 | 0.0327
Epoch 156/300, trend Loss: 0.0399 | 0.0330
Epoch 157/300, trend Loss: 0.0396 | 0.0324
Epoch 158/300, trend Loss: 0.0405 | 0.0350
Epoch 159/300, trend Loss: 0.0394 | 0.0340
Epoch 160/300, trend Loss: 0.0385 | 0.0315
Epoch 161/300, trend Loss: 0.0381 | 0.0344
Epoch 162/300, trend Loss: 0.0383 | 0.0326
Epoch 163/300, trend Loss: 0.0450 | 0.0327
Epoch 164/300, trend Loss: 0.0418 | 0.0317
Epoch 165/300, trend Loss: 0.0406 | 0.0315
Epoch 166/300, trend Loss: 0.0399 | 0.0322
Epoch 167/300, trend Loss: 0.0386 | 0.0327
Epoch 168/300, trend Loss: 0.0380 | 0.0330
Epoch 169/300, trend Loss: 0.0374 | 0.0323
Epoch 170/300, trend Loss: 0.0374 | 0.0345
Epoch 171/300, trend Loss: 0.0373 | 0.0325
Epoch 172/300, trend Loss: 0.0382 | 0.0349
Epoch 173/300, trend Loss: 0.0384 | 0.0356
Epoch 174/300, trend Loss: 0.0380 | 0.0351
Epoch 175/300, trend Loss: 0.0376 | 0.0323
Epoch 176/300, trend Loss: 0.0384 | 0.0342
Epoch 177/300, trend Loss: 0.0390 | 0.0381
Epoch 178/300, trend Loss: 0.0376 | 0.0310
Epoch 179/300, trend Loss: 0.0366 | 0.0318
Epoch 180/300, trend Loss: 0.0380 | 0.0349
Epoch 181/300, trend Loss: 0.0366 | 0.0322
Epoch 182/300, trend Loss: 0.0374 | 0.0320
Epoch 183/300, trend Loss: 0.0371 | 0.0343
Epoch 184/300, trend Loss: 0.0369 | 0.0324
Epoch 185/300, trend Loss: 0.0368 | 0.0331
Epoch 186/300, trend Loss: 0.0378 | 0.0345
Epoch 187/300, trend Loss: 0.0364 | 0.0388
Epoch 188/300, trend Loss: 0.0382 | 0.0374
Epoch 189/300, trend Loss: 0.0365 | 0.0345
Epoch 190/300, trend Loss: 0.0362 | 0.0337
Epoch 191/300, trend Loss: 0.0375 | 0.0339
Epoch 192/300, trend Loss: 0.0371 | 0.0392
Epoch 193/300, trend Loss: 0.0382 | 0.0353
Epoch 194/300, trend Loss: 0.0360 | 0.0358
Epoch 195/300, trend Loss: 0.0365 | 0.0393
Epoch 196/300, trend Loss: 0.0371 | 0.0351
Epoch 197/300, trend Loss: 0.0362 | 0.0365
Epoch 198/300, trend Loss: 0.0360 | 0.0366
Epoch 199/300, trend Loss: 0.0353 | 0.0351
Epoch 200/300, trend Loss: 0.0354 | 0.0353
Epoch 201/300, trend Loss: 0.0367 | 0.0445
Epoch 202/300, trend Loss: 0.0389 | 0.0319
Epoch 203/300, trend Loss: 0.0371 | 0.0382
Epoch 204/300, trend Loss: 0.0368 | 0.0358
Epoch 205/300, trend Loss: 0.0353 | 0.0355
Epoch 206/300, trend Loss: 0.0351 | 0.0390
Epoch 207/300, trend Loss: 0.0358 | 0.0362
Epoch 208/300, trend Loss: 0.0355 | 0.0371
Epoch 209/300, trend Loss: 0.0350 | 0.0400
Epoch 210/300, trend Loss: 0.0359 | 0.0362
Epoch 211/300, trend Loss: 0.0360 | 0.0546
Epoch 212/300, trend Loss: 0.0376 | 0.0420
Epoch 213/300, trend Loss: 0.0355 | 0.0408
Epoch 214/300, trend Loss: 0.0349 | 0.0390
Epoch 215/300, trend Loss: 0.0345 | 0.0378
Epoch 216/300, trend Loss: 0.0344 | 0.0372
Epoch 217/300, trend Loss: 0.0342 | 0.0365
Epoch 218/300, trend Loss: 0.0341 | 0.0359
Epoch 219/300, trend Loss: 0.0340 | 0.0359
Epoch 220/300, trend Loss: 0.0343 | 0.0406
Epoch 221/300, trend Loss: 0.0367 | 0.0341
Epoch 222/300, trend Loss: 0.0437 | 0.0380
Epoch 223/300, trend Loss: 0.0372 | 0.0340
Epoch 224/300, trend Loss: 0.0352 | 0.0346
Epoch 225/300, trend Loss: 0.0342 | 0.0352
Epoch 226/300, trend Loss: 0.0334 | 0.0351
Epoch 227/300, trend Loss: 0.0332 | 0.0371
Epoch 228/300, trend Loss: 0.0349 | 0.0348
Epoch 229/300, trend Loss: 0.0335 | 0.0352
Epoch 230/300, trend Loss: 0.0328 | 0.0347
Epoch 231/300, trend Loss: 0.0331 | 0.0344
Epoch 232/300, trend Loss: 0.0338 | 0.0383
Epoch 233/300, trend Loss: 0.0362 | 0.0365
Epoch 234/300, trend Loss: 0.0342 | 0.0349
Epoch 235/300, trend Loss: 0.0331 | 0.0351
Epoch 236/300, trend Loss: 0.0328 | 0.0349
Epoch 237/300, trend Loss: 0.0325 | 0.0348
Epoch 238/300, trend Loss: 0.0323 | 0.0345
Epoch 239/300, trend Loss: 0.0320 | 0.0345
Epoch 240/300, trend Loss: 0.0322 | 0.0338
Epoch 241/300, trend Loss: 0.0344 | 0.0359
Epoch 242/300, trend Loss: 0.0331 | 0.0341
Epoch 243/300, trend Loss: 0.0322 | 0.0334
Epoch 244/300, trend Loss: 0.0333 | 0.0412
Epoch 245/300, trend Loss: 0.0359 | 0.0332
Epoch 246/300, trend Loss: 0.0343 | 0.0347
Epoch 247/300, trend Loss: 0.0331 | 0.0343
Epoch 248/300, trend Loss: 0.0326 | 0.0348
Epoch 249/300, trend Loss: 0.0326 | 0.0350
Epoch 250/300, trend Loss: 0.0346 | 0.0370
Epoch 251/300, trend Loss: 0.0363 | 0.0360
Epoch 252/300, trend Loss: 0.0341 | 0.0367
Epoch 253/300, trend Loss: 0.0331 | 0.0356
Epoch 254/300, trend Loss: 0.0328 | 0.0358
Epoch 255/300, trend Loss: 0.0322 | 0.0357
Epoch 256/300, trend Loss: 0.0321 | 0.0355
Epoch 257/300, trend Loss: 0.0321 | 0.0354
Epoch 258/300, trend Loss: 0.0320 | 0.0353
Epoch 259/300, trend Loss: 0.0319 | 0.0350
Epoch 260/300, trend Loss: 0.0315 | 0.0347
Epoch 261/300, trend Loss: 0.0316 | 0.0371
Epoch 262/300, trend Loss: 0.0327 | 0.0355
Epoch 263/300, trend Loss: 0.0320 | 0.0350
Epoch 264/300, trend Loss: 0.0318 | 0.0346
Epoch 265/300, trend Loss: 0.0316 | 0.0345
Epoch 266/300, trend Loss: 0.0314 | 0.0344
Epoch 267/300, trend Loss: 0.0313 | 0.0341
Epoch 268/300, trend Loss: 0.0325 | 0.0371
Epoch 269/300, trend Loss: 0.0338 | 0.0357
Epoch 270/300, trend Loss: 0.0320 | 0.0344
Epoch 271/300, trend Loss: 0.0317 | 0.0345
Epoch 272/300, trend Loss: 0.0315 | 0.0344
Epoch 273/300, trend Loss: 0.0312 | 0.0343
Epoch 274/300, trend Loss: 0.0309 | 0.0345
Epoch 275/300, trend Loss: 0.0307 | 0.0343
Epoch 276/300, trend Loss: 0.0307 | 0.0345
Epoch 277/300, trend Loss: 0.0309 | 0.0339
Epoch 278/300, trend Loss: 0.0342 | 0.0378
Epoch 279/300, trend Loss: 0.0331 | 0.0354
Epoch 280/300, trend Loss: 0.0322 | 0.0348
Epoch 281/300, trend Loss: 0.0315 | 0.0349
Epoch 282/300, trend Loss: 0.0310 | 0.0351
Epoch 283/300, trend Loss: 0.0310 | 0.0348
Epoch 284/300, trend Loss: 0.0309 | 0.0356
Epoch 285/300, trend Loss: 0.0314 | 0.0349
Epoch 286/300, trend Loss: 0.0312 | 0.0348
Epoch 287/300, trend Loss: 0.0308 | 0.0345
Epoch 288/300, trend Loss: 0.0309 | 0.0361
Epoch 289/300, trend Loss: 0.0313 | 0.0350
Epoch 290/300, trend Loss: 0.0312 | 0.0348
Epoch 291/300, trend Loss: 0.0311 | 0.0348
Epoch 292/300, trend Loss: 0.0310 | 0.0347
Epoch 293/300, trend Loss: 0.0309 | 0.0346
Epoch 294/300, trend Loss: 0.0305 | 0.0345
Epoch 295/300, trend Loss: 0.0306 | 0.0353
Epoch 296/300, trend Loss: 0.0312 | 0.0341
Epoch 297/300, trend Loss: 0.0302 | 0.0344
Epoch 298/300, trend Loss: 0.0303 | 0.0337
Epoch 299/300, trend Loss: 0.0334 | 0.0348
Epoch 300/300, trend Loss: 0.0333 | 0.0367
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.6943987969847646, 'learning_rate': 0.00015777252739766184, 'batch_size': 39, 'step_size': 6, 'gamma': 0.750001559259669}
Epoch 1/300, seasonal_0 Loss: 0.4553 | 0.2219
Epoch 2/300, seasonal_0 Loss: 0.1305 | 0.1608
Epoch 3/300, seasonal_0 Loss: 0.1126 | 0.1948
Epoch 4/300, seasonal_0 Loss: 0.1069 | 0.2066
Epoch 5/300, seasonal_0 Loss: 0.1074 | 0.1458
Epoch 6/300, seasonal_0 Loss: 0.1165 | 0.0744
Epoch 7/300, seasonal_0 Loss: 0.1109 | 0.0554
Epoch 8/300, seasonal_0 Loss: 0.1031 | 0.0521
Epoch 9/300, seasonal_0 Loss: 0.0984 | 0.0519
Epoch 10/300, seasonal_0 Loss: 0.0940 | 0.0482
Epoch 11/300, seasonal_0 Loss: 0.0924 | 0.0463
Epoch 12/300, seasonal_0 Loss: 0.0890 | 0.0449
Epoch 13/300, seasonal_0 Loss: 0.0869 | 0.0464
Epoch 14/300, seasonal_0 Loss: 0.0859 | 0.0447
Epoch 15/300, seasonal_0 Loss: 0.0847 | 0.0449
Epoch 16/300, seasonal_0 Loss: 0.0836 | 0.0457
Epoch 17/300, seasonal_0 Loss: 0.0829 | 0.0462
Epoch 18/300, seasonal_0 Loss: 0.0822 | 0.0460
Epoch 19/300, seasonal_0 Loss: 0.0814 | 0.0444
Epoch 20/300, seasonal_0 Loss: 0.0810 | 0.0430
Epoch 21/300, seasonal_0 Loss: 0.0806 | 0.0450
Epoch 22/300, seasonal_0 Loss: 0.0799 | 0.0434
Epoch 23/300, seasonal_0 Loss: 0.0794 | 0.0452
Epoch 24/300, seasonal_0 Loss: 0.0791 | 0.0439
Epoch 25/300, seasonal_0 Loss: 0.0780 | 0.0435
Epoch 26/300, seasonal_0 Loss: 0.0779 | 0.0424
Epoch 27/300, seasonal_0 Loss: 0.0774 | 0.0419
Epoch 28/300, seasonal_0 Loss: 0.0770 | 0.0428
Epoch 29/300, seasonal_0 Loss: 0.0767 | 0.0410
Epoch 30/300, seasonal_0 Loss: 0.0764 | 0.0416
Epoch 31/300, seasonal_0 Loss: 0.0758 | 0.0403
Epoch 32/300, seasonal_0 Loss: 0.0759 | 0.0405
Epoch 33/300, seasonal_0 Loss: 0.0756 | 0.0397
Epoch 34/300, seasonal_0 Loss: 0.0757 | 0.0415
Epoch 35/300, seasonal_0 Loss: 0.0757 | 0.0412
Epoch 36/300, seasonal_0 Loss: 0.0752 | 0.0413
Epoch 37/300, seasonal_0 Loss: 0.0748 | 0.0407
Epoch 38/300, seasonal_0 Loss: 0.0745 | 0.0413
Epoch 39/300, seasonal_0 Loss: 0.0742 | 0.0414
Epoch 40/300, seasonal_0 Loss: 0.0740 | 0.0410
Epoch 41/300, seasonal_0 Loss: 0.0739 | 0.0412
Epoch 42/300, seasonal_0 Loss: 0.0737 | 0.0415
Epoch 43/300, seasonal_0 Loss: 0.0735 | 0.0412
Epoch 44/300, seasonal_0 Loss: 0.0734 | 0.0414
Epoch 45/300, seasonal_0 Loss: 0.0732 | 0.0415
Epoch 46/300, seasonal_0 Loss: 0.0731 | 0.0413
Epoch 47/300, seasonal_0 Loss: 0.0729 | 0.0412
Epoch 48/300, seasonal_0 Loss: 0.0727 | 0.0412
Epoch 49/300, seasonal_0 Loss: 0.0726 | 0.0412
Epoch 50/300, seasonal_0 Loss: 0.0725 | 0.0410
Epoch 51/300, seasonal_0 Loss: 0.0724 | 0.0409
Epoch 52/300, seasonal_0 Loss: 0.0723 | 0.0410
Epoch 53/300, seasonal_0 Loss: 0.0722 | 0.0408
Epoch 54/300, seasonal_0 Loss: 0.0722 | 0.0406
Epoch 55/300, seasonal_0 Loss: 0.0722 | 0.0408
Epoch 56/300, seasonal_0 Loss: 0.0721 | 0.0406
Epoch 57/300, seasonal_0 Loss: 0.0721 | 0.0404
Epoch 58/300, seasonal_0 Loss: 0.0721 | 0.0406
Epoch 59/300, seasonal_0 Loss: 0.0721 | 0.0404
Epoch 60/300, seasonal_0 Loss: 0.0720 | 0.0403
Epoch 61/300, seasonal_0 Loss: 0.0719 | 0.0405
Epoch 62/300, seasonal_0 Loss: 0.0719 | 0.0404
Epoch 63/300, seasonal_0 Loss: 0.0718 | 0.0403
Epoch 64/300, seasonal_0 Loss: 0.0717 | 0.0403
Epoch 65/300, seasonal_0 Loss: 0.0717 | 0.0403
Epoch 66/300, seasonal_0 Loss: 0.0716 | 0.0402
Epoch 67/300, seasonal_0 Loss: 0.0716 | 0.0403
Epoch 68/300, seasonal_0 Loss: 0.0715 | 0.0402
Epoch 69/300, seasonal_0 Loss: 0.0715 | 0.0402
Epoch 70/300, seasonal_0 Loss: 0.0715 | 0.0402
Epoch 71/300, seasonal_0 Loss: 0.0715 | 0.0402
Epoch 72/300, seasonal_0 Loss: 0.0715 | 0.0402
Epoch 73/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 74/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 75/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 76/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 77/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 78/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 79/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 80/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 81/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 82/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 83/300, seasonal_0 Loss: 0.0714 | 0.0402
Epoch 84/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 85/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 86/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 87/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 88/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 89/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 90/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 91/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 92/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 93/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 94/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 95/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 96/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 97/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 98/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 99/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 100/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 101/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 102/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 103/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 104/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 105/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 106/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 107/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 108/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 109/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 110/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 111/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 112/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 113/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 114/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 115/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 116/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 117/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 118/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 119/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 120/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 121/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 122/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 123/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 124/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 125/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 126/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 127/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 128/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 129/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 130/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 131/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 132/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 133/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 134/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 135/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 136/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 137/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 138/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 139/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 140/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 141/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 142/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 143/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 144/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 145/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 146/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 147/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 148/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 149/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 150/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 151/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 152/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 153/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 154/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 155/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 156/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 157/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 158/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 159/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 160/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 161/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 162/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 163/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 164/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 165/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 166/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 167/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 168/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 169/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 170/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 171/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 172/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 173/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 174/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 175/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 176/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 177/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 178/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 179/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 180/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 181/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 182/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 183/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 184/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 185/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 186/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 187/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 188/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 189/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 190/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 191/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 192/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 193/300, seasonal_0 Loss: 0.0713 | 0.0402
Epoch 194/300, seasonal_0 Loss: 0.0713 | 0.0402
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.8655329045981331, 'learning_rate': 0.0002131369163969969, 'batch_size': 122, 'step_size': 12, 'gamma': 0.8661180532610977}
Epoch 1/300, seasonal_1 Loss: 1.4505 | 0.3649
Epoch 2/300, seasonal_1 Loss: 0.2214 | 0.1802
Epoch 3/300, seasonal_1 Loss: 0.1707 | 0.1174
Epoch 4/300, seasonal_1 Loss: 0.1574 | 0.1058
Epoch 5/300, seasonal_1 Loss: 0.1295 | 0.0856
Epoch 6/300, seasonal_1 Loss: 0.1212 | 0.0694
Epoch 7/300, seasonal_1 Loss: 0.1268 | 0.0923
Epoch 8/300, seasonal_1 Loss: 0.1303 | 0.0752
Epoch 9/300, seasonal_1 Loss: 0.1191 | 0.0843
Epoch 10/300, seasonal_1 Loss: 0.1404 | 0.1024
Epoch 11/300, seasonal_1 Loss: 0.1534 | 0.0691
Epoch 12/300, seasonal_1 Loss: 0.1265 | 0.0869
Epoch 13/300, seasonal_1 Loss: 0.1366 | 0.1731
Epoch 14/300, seasonal_1 Loss: 0.1244 | 0.1203
Epoch 15/300, seasonal_1 Loss: 0.1100 | 0.1242
Epoch 16/300, seasonal_1 Loss: 0.1107 | 0.1251
Epoch 17/300, seasonal_1 Loss: 0.1040 | 0.0588
Epoch 18/300, seasonal_1 Loss: 0.1279 | 0.0958
Epoch 19/300, seasonal_1 Loss: 0.1304 | 0.0883
Epoch 20/300, seasonal_1 Loss: 0.1687 | 0.0732
Epoch 21/300, seasonal_1 Loss: 0.2065 | 0.0973
Epoch 22/300, seasonal_1 Loss: 0.1743 | 0.0688
Epoch 23/300, seasonal_1 Loss: 0.1509 | 0.0757
Epoch 24/300, seasonal_1 Loss: 0.1918 | 0.0709
Epoch 25/300, seasonal_1 Loss: 0.1237 | 0.0939
Epoch 26/300, seasonal_1 Loss: 0.1059 | 0.0643
Epoch 27/300, seasonal_1 Loss: 0.1103 | 0.0713
Epoch 28/300, seasonal_1 Loss: 0.1099 | 0.0603
Epoch 29/300, seasonal_1 Loss: 0.1062 | 0.0576
Epoch 30/300, seasonal_1 Loss: 0.1011 | 0.0471
Epoch 31/300, seasonal_1 Loss: 0.1005 | 0.0581
Epoch 32/300, seasonal_1 Loss: 0.1006 | 0.0563
Epoch 33/300, seasonal_1 Loss: 0.1008 | 0.0751
Epoch 34/300, seasonal_1 Loss: 0.1161 | 0.0878
Epoch 35/300, seasonal_1 Loss: 0.1070 | 0.1906
Epoch 36/300, seasonal_1 Loss: 0.1069 | 0.0541
Epoch 37/300, seasonal_1 Loss: 0.1299 | 0.0630
Epoch 38/300, seasonal_1 Loss: 0.1546 | 0.0923
Epoch 39/300, seasonal_1 Loss: 0.1710 | 0.0737
Epoch 40/300, seasonal_1 Loss: 0.1649 | 0.0509
Epoch 41/300, seasonal_1 Loss: 0.1180 | 0.0720
Epoch 42/300, seasonal_1 Loss: 0.1341 | 0.0867
Epoch 43/300, seasonal_1 Loss: 0.1456 | 0.0649
Epoch 44/300, seasonal_1 Loss: 0.1534 | 0.0745
Epoch 45/300, seasonal_1 Loss: 0.1362 | 0.0588
Epoch 46/300, seasonal_1 Loss: 0.1197 | 0.0903
Epoch 47/300, seasonal_1 Loss: 0.1264 | 0.0874
Epoch 48/300, seasonal_1 Loss: 0.1525 | 0.0683
Epoch 49/300, seasonal_1 Loss: 0.1203 | 0.0863
Epoch 50/300, seasonal_1 Loss: 0.1090 | 0.0601
Epoch 51/300, seasonal_1 Loss: 0.1182 | 0.0545
Epoch 52/300, seasonal_1 Loss: 0.0941 | 0.0651
Epoch 53/300, seasonal_1 Loss: 0.0880 | 0.0872
Epoch 54/300, seasonal_1 Loss: 0.0992 | 0.0597
Epoch 55/300, seasonal_1 Loss: 0.1004 | 0.0664
Epoch 56/300, seasonal_1 Loss: 0.0983 | 0.0564
Epoch 57/300, seasonal_1 Loss: 0.0824 | 0.0577
Epoch 58/300, seasonal_1 Loss: 0.0786 | 0.0389
Epoch 59/300, seasonal_1 Loss: 0.0743 | 0.0483
Epoch 60/300, seasonal_1 Loss: 0.0724 | 0.0449
Epoch 61/300, seasonal_1 Loss: 0.0705 | 0.0382
Epoch 62/300, seasonal_1 Loss: 0.0696 | 0.0456
Epoch 63/300, seasonal_1 Loss: 0.0709 | 0.0338
Epoch 64/300, seasonal_1 Loss: 0.0702 | 0.0425
Epoch 65/300, seasonal_1 Loss: 0.0689 | 0.0366
Epoch 66/300, seasonal_1 Loss: 0.0684 | 0.0364
Epoch 67/300, seasonal_1 Loss: 0.0682 | 0.0406
Epoch 68/300, seasonal_1 Loss: 0.0689 | 0.0333
Epoch 69/300, seasonal_1 Loss: 0.0680 | 0.0407
Epoch 70/300, seasonal_1 Loss: 0.0682 | 0.0321
Epoch 71/300, seasonal_1 Loss: 0.0680 | 0.0367
Epoch 72/300, seasonal_1 Loss: 0.0667 | 0.0369
Epoch 73/300, seasonal_1 Loss: 0.0668 | 0.0329
Epoch 74/300, seasonal_1 Loss: 0.0657 | 0.0370
Epoch 75/300, seasonal_1 Loss: 0.0657 | 0.0300
Epoch 76/300, seasonal_1 Loss: 0.0653 | 0.0362
Epoch 77/300, seasonal_1 Loss: 0.0666 | 0.0322
Epoch 78/300, seasonal_1 Loss: 0.0650 | 0.0343
Epoch 79/300, seasonal_1 Loss: 0.0642 | 0.0331
Epoch 80/300, seasonal_1 Loss: 0.0639 | 0.0311
Epoch 81/300, seasonal_1 Loss: 0.0635 | 0.0331
Epoch 82/300, seasonal_1 Loss: 0.0636 | 0.0300
Epoch 83/300, seasonal_1 Loss: 0.0634 | 0.0335
Epoch 84/300, seasonal_1 Loss: 0.0636 | 0.0298
Epoch 85/300, seasonal_1 Loss: 0.0635 | 0.0321
Epoch 86/300, seasonal_1 Loss: 0.0633 | 0.0304
Epoch 87/300, seasonal_1 Loss: 0.0645 | 0.0348
Epoch 88/300, seasonal_1 Loss: 0.0660 | 0.0349
Epoch 89/300, seasonal_1 Loss: 0.0659 | 0.0326
Epoch 90/300, seasonal_1 Loss: 0.0646 | 0.0338
Epoch 91/300, seasonal_1 Loss: 0.0632 | 0.0311
Epoch 92/300, seasonal_1 Loss: 0.0628 | 0.0314
Epoch 93/300, seasonal_1 Loss: 0.0623 | 0.0299
Epoch 94/300, seasonal_1 Loss: 0.0620 | 0.0304
Epoch 95/300, seasonal_1 Loss: 0.0619 | 0.0304
Epoch 96/300, seasonal_1 Loss: 0.0638 | 0.0329
Epoch 97/300, seasonal_1 Loss: 0.0623 | 0.0313
Epoch 98/300, seasonal_1 Loss: 0.0645 | 0.0334
Epoch 99/300, seasonal_1 Loss: 0.0634 | 0.0310
Epoch 100/300, seasonal_1 Loss: 0.0628 | 0.0308
Epoch 101/300, seasonal_1 Loss: 0.0617 | 0.0298
Epoch 102/300, seasonal_1 Loss: 0.0611 | 0.0300
Epoch 103/300, seasonal_1 Loss: 0.0609 | 0.0295
Epoch 104/300, seasonal_1 Loss: 0.0608 | 0.0302
Epoch 105/300, seasonal_1 Loss: 0.0607 | 0.0291
Epoch 106/300, seasonal_1 Loss: 0.0605 | 0.0299
Epoch 107/300, seasonal_1 Loss: 0.0605 | 0.0286
Epoch 108/300, seasonal_1 Loss: 0.0606 | 0.0297
Epoch 109/300, seasonal_1 Loss: 0.0608 | 0.0293
Epoch 110/300, seasonal_1 Loss: 0.0619 | 0.0322
Epoch 111/300, seasonal_1 Loss: 0.0634 | 0.0319
Epoch 112/300, seasonal_1 Loss: 0.0625 | 0.0303
Epoch 113/300, seasonal_1 Loss: 0.0608 | 0.0301
Epoch 114/300, seasonal_1 Loss: 0.0615 | 0.0302
Epoch 115/300, seasonal_1 Loss: 0.0612 | 0.0300
Epoch 116/300, seasonal_1 Loss: 0.0633 | 0.0332
Epoch 117/300, seasonal_1 Loss: 0.0628 | 0.0319
Epoch 118/300, seasonal_1 Loss: 0.0606 | 0.0295
Epoch 119/300, seasonal_1 Loss: 0.0600 | 0.0294
Epoch 120/300, seasonal_1 Loss: 0.0599 | 0.0291
Epoch 121/300, seasonal_1 Loss: 0.0597 | 0.0290
Epoch 122/300, seasonal_1 Loss: 0.0593 | 0.0290
Epoch 123/300, seasonal_1 Loss: 0.0592 | 0.0290
Epoch 124/300, seasonal_1 Loss: 0.0593 | 0.0290
Epoch 125/300, seasonal_1 Loss: 0.0595 | 0.0293
Epoch 126/300, seasonal_1 Loss: 0.0596 | 0.0290
Epoch 127/300, seasonal_1 Loss: 0.0592 | 0.0291
Epoch 128/300, seasonal_1 Loss: 0.0596 | 0.0288
Epoch 129/300, seasonal_1 Loss: 0.0599 | 0.0286
Epoch 130/300, seasonal_1 Loss: 0.0589 | 0.0284
Epoch 131/300, seasonal_1 Loss: 0.0588 | 0.0287
Epoch 132/300, seasonal_1 Loss: 0.0589 | 0.0288
Epoch 133/300, seasonal_1 Loss: 0.0587 | 0.0286
Epoch 134/300, seasonal_1 Loss: 0.0585 | 0.0285
Epoch 135/300, seasonal_1 Loss: 0.0588 | 0.0285
Epoch 136/300, seasonal_1 Loss: 0.0584 | 0.0284
Epoch 137/300, seasonal_1 Loss: 0.0584 | 0.0287
Epoch 138/300, seasonal_1 Loss: 0.0585 | 0.0286
Epoch 139/300, seasonal_1 Loss: 0.0584 | 0.0285
Epoch 140/300, seasonal_1 Loss: 0.0583 | 0.0285
Epoch 141/300, seasonal_1 Loss: 0.0586 | 0.0286
Epoch 142/300, seasonal_1 Loss: 0.0581 | 0.0283
Epoch 143/300, seasonal_1 Loss: 0.0582 | 0.0286
Epoch 144/300, seasonal_1 Loss: 0.0582 | 0.0282
Epoch 145/300, seasonal_1 Loss: 0.0581 | 0.0289
Epoch 146/300, seasonal_1 Loss: 0.0580 | 0.0284
Epoch 147/300, seasonal_1 Loss: 0.0579 | 0.0286
Epoch 148/300, seasonal_1 Loss: 0.0578 | 0.0281
Epoch 149/300, seasonal_1 Loss: 0.0578 | 0.0285
Epoch 150/300, seasonal_1 Loss: 0.0578 | 0.0282
Epoch 151/300, seasonal_1 Loss: 0.0577 | 0.0289
Epoch 152/300, seasonal_1 Loss: 0.0577 | 0.0283
Epoch 153/300, seasonal_1 Loss: 0.0576 | 0.0286
Epoch 154/300, seasonal_1 Loss: 0.0575 | 0.0282
Epoch 155/300, seasonal_1 Loss: 0.0576 | 0.0286
Epoch 156/300, seasonal_1 Loss: 0.0576 | 0.0281
Epoch 157/300, seasonal_1 Loss: 0.0575 | 0.0287
Epoch 158/300, seasonal_1 Loss: 0.0575 | 0.0284
Epoch 159/300, seasonal_1 Loss: 0.0576 | 0.0287
Epoch 160/300, seasonal_1 Loss: 0.0573 | 0.0281
Epoch 161/300, seasonal_1 Loss: 0.0574 | 0.0286
Epoch 162/300, seasonal_1 Loss: 0.0574 | 0.0281
Epoch 163/300, seasonal_1 Loss: 0.0574 | 0.0287
Epoch 164/300, seasonal_1 Loss: 0.0573 | 0.0283
Epoch 165/300, seasonal_1 Loss: 0.0573 | 0.0285
Epoch 166/300, seasonal_1 Loss: 0.0571 | 0.0282
Epoch 167/300, seasonal_1 Loss: 0.0572 | 0.0284
Epoch 168/300, seasonal_1 Loss: 0.0572 | 0.0282
Epoch 169/300, seasonal_1 Loss: 0.0571 | 0.0286
Epoch 170/300, seasonal_1 Loss: 0.0571 | 0.0284
Epoch 171/300, seasonal_1 Loss: 0.0570 | 0.0284
Epoch 172/300, seasonal_1 Loss: 0.0570 | 0.0282
Epoch 173/300, seasonal_1 Loss: 0.0570 | 0.0283
Epoch 174/300, seasonal_1 Loss: 0.0570 | 0.0282
Epoch 175/300, seasonal_1 Loss: 0.0569 | 0.0285
Epoch 176/300, seasonal_1 Loss: 0.0569 | 0.0284
Epoch 177/300, seasonal_1 Loss: 0.0569 | 0.0283
Epoch 178/300, seasonal_1 Loss: 0.0568 | 0.0282
Epoch 179/300, seasonal_1 Loss: 0.0568 | 0.0283
Epoch 180/300, seasonal_1 Loss: 0.0568 | 0.0283
Epoch 181/300, seasonal_1 Loss: 0.0567 | 0.0284
Epoch 182/300, seasonal_1 Loss: 0.0567 | 0.0284
Epoch 183/300, seasonal_1 Loss: 0.0567 | 0.0283
Epoch 184/300, seasonal_1 Loss: 0.0567 | 0.0282
Epoch 185/300, seasonal_1 Loss: 0.0567 | 0.0283
Epoch 186/300, seasonal_1 Loss: 0.0566 | 0.0283
Epoch 187/300, seasonal_1 Loss: 0.0566 | 0.0284
Epoch 188/300, seasonal_1 Loss: 0.0566 | 0.0283
Epoch 189/300, seasonal_1 Loss: 0.0565 | 0.0283
Epoch 190/300, seasonal_1 Loss: 0.0565 | 0.0282
Epoch 191/300, seasonal_1 Loss: 0.0565 | 0.0282
Epoch 192/300, seasonal_1 Loss: 0.0565 | 0.0283
Epoch 193/300, seasonal_1 Loss: 0.0564 | 0.0283
Epoch 194/300, seasonal_1 Loss: 0.0564 | 0.0283
Epoch 195/300, seasonal_1 Loss: 0.0564 | 0.0282
Epoch 196/300, seasonal_1 Loss: 0.0564 | 0.0282
Epoch 197/300, seasonal_1 Loss: 0.0564 | 0.0282
Epoch 198/300, seasonal_1 Loss: 0.0563 | 0.0283
Epoch 199/300, seasonal_1 Loss: 0.0563 | 0.0283
Epoch 200/300, seasonal_1 Loss: 0.0563 | 0.0282
Epoch 201/300, seasonal_1 Loss: 0.0563 | 0.0282
Epoch 202/300, seasonal_1 Loss: 0.0563 | 0.0282
Epoch 203/300, seasonal_1 Loss: 0.0563 | 0.0282
Epoch 204/300, seasonal_1 Loss: 0.0562 | 0.0282
Epoch 205/300, seasonal_1 Loss: 0.0562 | 0.0282
Epoch 206/300, seasonal_1 Loss: 0.0562 | 0.0282
Epoch 207/300, seasonal_1 Loss: 0.0562 | 0.0282
Epoch 208/300, seasonal_1 Loss: 0.0562 | 0.0282
Epoch 209/300, seasonal_1 Loss: 0.0561 | 0.0282
Epoch 210/300, seasonal_1 Loss: 0.0561 | 0.0282
Epoch 211/300, seasonal_1 Loss: 0.0561 | 0.0282
Epoch 212/300, seasonal_1 Loss: 0.0561 | 0.0282
Epoch 213/300, seasonal_1 Loss: 0.0561 | 0.0282
Epoch 214/300, seasonal_1 Loss: 0.0561 | 0.0282
Epoch 215/300, seasonal_1 Loss: 0.0560 | 0.0281
Epoch 216/300, seasonal_1 Loss: 0.0560 | 0.0281
Epoch 217/300, seasonal_1 Loss: 0.0560 | 0.0281
Epoch 218/300, seasonal_1 Loss: 0.0560 | 0.0281
Epoch 219/300, seasonal_1 Loss: 0.0560 | 0.0281
Epoch 220/300, seasonal_1 Loss: 0.0560 | 0.0281
Epoch 221/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 222/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 223/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 224/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 225/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 226/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 227/300, seasonal_1 Loss: 0.0559 | 0.0281
Epoch 228/300, seasonal_1 Loss: 0.0558 | 0.0281
Epoch 229/300, seasonal_1 Loss: 0.0558 | 0.0280
Epoch 230/300, seasonal_1 Loss: 0.0558 | 0.0280
Epoch 231/300, seasonal_1 Loss: 0.0558 | 0.0280
Epoch 232/300, seasonal_1 Loss: 0.0558 | 0.0280
Epoch 233/300, seasonal_1 Loss: 0.0558 | 0.0280
Epoch 234/300, seasonal_1 Loss: 0.0558 | 0.0280
Epoch 235/300, seasonal_1 Loss: 0.0557 | 0.0280
Epoch 236/300, seasonal_1 Loss: 0.0557 | 0.0280
Epoch 237/300, seasonal_1 Loss: 0.0557 | 0.0280
Epoch 238/300, seasonal_1 Loss: 0.0557 | 0.0280
Epoch 239/300, seasonal_1 Loss: 0.0557 | 0.0280
Epoch 240/300, seasonal_1 Loss: 0.0557 | 0.0280
Epoch 241/300, seasonal_1 Loss: 0.0557 | 0.0279
Epoch 242/300, seasonal_1 Loss: 0.0557 | 0.0279
Epoch 243/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 244/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 245/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 246/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 247/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 248/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 249/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 250/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 251/300, seasonal_1 Loss: 0.0556 | 0.0279
Epoch 252/300, seasonal_1 Loss: 0.0555 | 0.0279
Epoch 253/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 254/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 255/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 256/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 257/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 258/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 259/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 260/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 261/300, seasonal_1 Loss: 0.0555 | 0.0278
Epoch 262/300, seasonal_1 Loss: 0.0554 | 0.0278
Epoch 263/300, seasonal_1 Loss: 0.0554 | 0.0278
Epoch 264/300, seasonal_1 Loss: 0.0554 | 0.0278
Epoch 265/300, seasonal_1 Loss: 0.0554 | 0.0278
Epoch 266/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 267/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 268/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 269/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 270/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 271/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 272/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 273/300, seasonal_1 Loss: 0.0554 | 0.0277
Epoch 274/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 275/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 276/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 277/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 278/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 279/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 280/300, seasonal_1 Loss: 0.0553 | 0.0277
Epoch 281/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 282/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 283/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 284/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 285/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 286/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 287/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 288/300, seasonal_1 Loss: 0.0553 | 0.0276
Epoch 289/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 290/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 291/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 292/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 293/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 294/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 295/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 296/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 297/300, seasonal_1 Loss: 0.0552 | 0.0276
Epoch 298/300, seasonal_1 Loss: 0.0552 | 0.0275
Epoch 299/300, seasonal_1 Loss: 0.0552 | 0.0275
Epoch 300/300, seasonal_1 Loss: 0.0552 | 0.0275
Training seasonal_2 component with params: {'observation_period_num': 14, 'train_rates': 0.9083644054545442, 'learning_rate': 9.030285193469562e-05, 'batch_size': 37, 'step_size': 4, 'gamma': 0.7904174369401347}
Epoch 1/300, seasonal_2 Loss: 0.2426 | 0.1386
Epoch 2/300, seasonal_2 Loss: 0.1085 | 0.1239
Epoch 3/300, seasonal_2 Loss: 0.0976 | 0.0945
Epoch 4/300, seasonal_2 Loss: 0.0969 | 0.0757
Epoch 5/300, seasonal_2 Loss: 0.1088 | 0.0661
Epoch 6/300, seasonal_2 Loss: 0.0956 | 0.0986
Epoch 7/300, seasonal_2 Loss: 0.0902 | 0.0520
Epoch 8/300, seasonal_2 Loss: 0.1047 | 0.0493
Epoch 9/300, seasonal_2 Loss: 0.1038 | 0.0437
Epoch 10/300, seasonal_2 Loss: 0.0964 | 0.0443
Epoch 11/300, seasonal_2 Loss: 0.0877 | 0.0538
Epoch 12/300, seasonal_2 Loss: 0.0770 | 0.0442
Epoch 13/300, seasonal_2 Loss: 0.0736 | 0.0545
Epoch 14/300, seasonal_2 Loss: 0.0722 | 0.0529
Epoch 15/300, seasonal_2 Loss: 0.0703 | 0.0520
Epoch 16/300, seasonal_2 Loss: 0.0682 | 0.0463
Epoch 17/300, seasonal_2 Loss: 0.0660 | 0.0429
Epoch 18/300, seasonal_2 Loss: 0.0647 | 0.0408
Epoch 19/300, seasonal_2 Loss: 0.0634 | 0.0387
Epoch 20/300, seasonal_2 Loss: 0.0626 | 0.0383
Epoch 21/300, seasonal_2 Loss: 0.0620 | 0.0382
Epoch 22/300, seasonal_2 Loss: 0.0615 | 0.0387
Epoch 23/300, seasonal_2 Loss: 0.0614 | 0.0366
Epoch 24/300, seasonal_2 Loss: 0.0612 | 0.0364
Epoch 25/300, seasonal_2 Loss: 0.0613 | 0.0297
Epoch 26/300, seasonal_2 Loss: 0.0613 | 0.0292
Epoch 27/300, seasonal_2 Loss: 0.0612 | 0.0303
Epoch 28/300, seasonal_2 Loss: 0.0610 | 0.0296
Epoch 29/300, seasonal_2 Loss: 0.0602 | 0.0291
Epoch 30/300, seasonal_2 Loss: 0.0596 | 0.0294
Epoch 31/300, seasonal_2 Loss: 0.0593 | 0.0323
Epoch 32/300, seasonal_2 Loss: 0.0590 | 0.0330
Epoch 33/300, seasonal_2 Loss: 0.0586 | 0.0359
Epoch 34/300, seasonal_2 Loss: 0.0583 | 0.0349
Epoch 35/300, seasonal_2 Loss: 0.0580 | 0.0345
Epoch 36/300, seasonal_2 Loss: 0.0578 | 0.0332
Epoch 37/300, seasonal_2 Loss: 0.0577 | 0.0318
Epoch 38/300, seasonal_2 Loss: 0.0575 | 0.0312
Epoch 39/300, seasonal_2 Loss: 0.0573 | 0.0307
Epoch 40/300, seasonal_2 Loss: 0.0571 | 0.0306
Epoch 41/300, seasonal_2 Loss: 0.0568 | 0.0305
Epoch 42/300, seasonal_2 Loss: 0.0566 | 0.0304
Epoch 43/300, seasonal_2 Loss: 0.0564 | 0.0303
Epoch 44/300, seasonal_2 Loss: 0.0563 | 0.0302
Epoch 45/300, seasonal_2 Loss: 0.0562 | 0.0301
Epoch 46/300, seasonal_2 Loss: 0.0561 | 0.0300
Epoch 47/300, seasonal_2 Loss: 0.0560 | 0.0300
Epoch 48/300, seasonal_2 Loss: 0.0560 | 0.0299
Epoch 49/300, seasonal_2 Loss: 0.0559 | 0.0299
Epoch 50/300, seasonal_2 Loss: 0.0559 | 0.0298
Epoch 51/300, seasonal_2 Loss: 0.0558 | 0.0298
Epoch 52/300, seasonal_2 Loss: 0.0558 | 0.0297
Epoch 53/300, seasonal_2 Loss: 0.0558 | 0.0297
Epoch 54/300, seasonal_2 Loss: 0.0558 | 0.0296
Epoch 55/300, seasonal_2 Loss: 0.0557 | 0.0296
Epoch 56/300, seasonal_2 Loss: 0.0557 | 0.0295
Epoch 57/300, seasonal_2 Loss: 0.0557 | 0.0295
Epoch 58/300, seasonal_2 Loss: 0.0557 | 0.0295
Epoch 59/300, seasonal_2 Loss: 0.0557 | 0.0295
Epoch 60/300, seasonal_2 Loss: 0.0556 | 0.0295
Epoch 61/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 62/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 63/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 64/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 65/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 66/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 67/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 68/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 69/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 70/300, seasonal_2 Loss: 0.0556 | 0.0294
Epoch 71/300, seasonal_2 Loss: 0.0555 | 0.0294
Epoch 72/300, seasonal_2 Loss: 0.0555 | 0.0294
Epoch 73/300, seasonal_2 Loss: 0.0555 | 0.0294
Epoch 74/300, seasonal_2 Loss: 0.0555 | 0.0294
Epoch 75/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 76/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 77/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 78/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 79/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 80/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 81/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 82/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 83/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 84/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 85/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 86/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 87/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 88/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 89/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 90/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 91/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 92/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 93/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 94/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 95/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 96/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 97/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 98/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 99/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 100/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 101/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 102/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 103/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 104/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 105/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 106/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 107/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 108/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 109/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 110/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 111/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 112/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 113/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 114/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 115/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 116/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 117/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 118/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 119/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 120/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 121/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 122/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 123/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 124/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 125/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 126/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 127/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 128/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 129/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 130/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 131/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 132/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 133/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 134/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 135/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 136/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 137/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 138/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 139/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 140/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 141/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 142/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 143/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 144/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 145/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 146/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 147/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 148/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 149/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 150/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 151/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 152/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 153/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 154/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 155/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 156/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 157/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 158/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 159/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 160/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 161/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 162/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 163/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 164/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 165/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 166/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 167/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 168/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 169/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 170/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 171/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 172/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 173/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 174/300, seasonal_2 Loss: 0.0555 | 0.0293
Epoch 175/300, seasonal_2 Loss: 0.0555 | 0.0293
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 11, 'train_rates': 0.8714275801437029, 'learning_rate': 0.00016375788074059092, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8079903297393677}
Epoch 1/300, seasonal_3 Loss: 0.5473 | 0.3198
Epoch 2/300, seasonal_3 Loss: 0.1533 | 0.0861
Epoch 3/300, seasonal_3 Loss: 0.1188 | 0.0776
Epoch 4/300, seasonal_3 Loss: 0.1087 | 0.0724
Epoch 5/300, seasonal_3 Loss: 0.1131 | 0.0564
Epoch 6/300, seasonal_3 Loss: 0.1186 | 0.2126
Epoch 7/300, seasonal_3 Loss: 0.1125 | 0.0585
Epoch 8/300, seasonal_3 Loss: 0.1434 | 0.0704
Epoch 9/300, seasonal_3 Loss: 0.1274 | 0.0632
Epoch 10/300, seasonal_3 Loss: 0.1126 | 0.0560
Epoch 11/300, seasonal_3 Loss: 0.0988 | 0.0497
Epoch 12/300, seasonal_3 Loss: 0.0944 | 0.0455
Epoch 13/300, seasonal_3 Loss: 0.0903 | 0.0415
Epoch 14/300, seasonal_3 Loss: 0.0872 | 0.0437
Epoch 15/300, seasonal_3 Loss: 0.0896 | 0.0472
Epoch 16/300, seasonal_3 Loss: 0.0969 | 0.0478
Epoch 17/300, seasonal_3 Loss: 0.1188 | 0.0939
Epoch 18/300, seasonal_3 Loss: 0.1095 | 0.0509
Epoch 19/300, seasonal_3 Loss: 0.1045 | 0.0525
Epoch 20/300, seasonal_3 Loss: 0.1245 | 0.0815
Epoch 21/300, seasonal_3 Loss: 0.1202 | 0.0611
Epoch 22/300, seasonal_3 Loss: 0.0998 | 0.0432
Epoch 23/300, seasonal_3 Loss: 0.0972 | 0.0434
Epoch 24/300, seasonal_3 Loss: 0.0957 | 0.0472
Epoch 25/300, seasonal_3 Loss: 0.0831 | 0.0465
Epoch 26/300, seasonal_3 Loss: 0.1051 | 0.0888
Epoch 27/300, seasonal_3 Loss: 0.0857 | 0.0531
Epoch 28/300, seasonal_3 Loss: 0.0761 | 0.0406
Epoch 29/300, seasonal_3 Loss: 0.0745 | 0.0401
Epoch 30/300, seasonal_3 Loss: 0.0756 | 0.0375
Epoch 31/300, seasonal_3 Loss: 0.0801 | 0.0440
Epoch 32/300, seasonal_3 Loss: 0.0862 | 0.0551
Epoch 33/300, seasonal_3 Loss: 0.0890 | 0.0409
Epoch 34/300, seasonal_3 Loss: 0.0854 | 0.0372
Epoch 35/300, seasonal_3 Loss: 0.0849 | 0.0367
Epoch 36/300, seasonal_3 Loss: 0.0781 | 0.0492
Epoch 37/300, seasonal_3 Loss: 0.0778 | 0.0492
Epoch 38/300, seasonal_3 Loss: 0.0748 | 0.0556
Epoch 39/300, seasonal_3 Loss: 0.0718 | 0.0634
Epoch 40/300, seasonal_3 Loss: 0.0696 | 0.0408
Epoch 41/300, seasonal_3 Loss: 0.0681 | 0.0322
Epoch 42/300, seasonal_3 Loss: 0.0718 | 0.0324
Epoch 43/300, seasonal_3 Loss: 0.0702 | 0.0328
Epoch 44/300, seasonal_3 Loss: 0.0661 | 0.0330
Epoch 45/300, seasonal_3 Loss: 0.0650 | 0.0327
Epoch 46/300, seasonal_3 Loss: 0.0635 | 0.0326
Epoch 47/300, seasonal_3 Loss: 0.0624 | 0.0331
Epoch 48/300, seasonal_3 Loss: 0.0616 | 0.0328
Epoch 49/300, seasonal_3 Loss: 0.0612 | 0.0320
Epoch 50/300, seasonal_3 Loss: 0.0615 | 0.0315
Epoch 51/300, seasonal_3 Loss: 0.0615 | 0.0307
Epoch 52/300, seasonal_3 Loss: 0.0616 | 0.0308
Epoch 53/300, seasonal_3 Loss: 0.0602 | 0.0312
Epoch 54/300, seasonal_3 Loss: 0.0595 | 0.0308
Epoch 55/300, seasonal_3 Loss: 0.0591 | 0.0302
Epoch 56/300, seasonal_3 Loss: 0.0588 | 0.0298
Epoch 57/300, seasonal_3 Loss: 0.0585 | 0.0294
Epoch 58/300, seasonal_3 Loss: 0.0582 | 0.0292
Epoch 59/300, seasonal_3 Loss: 0.0579 | 0.0289
Epoch 60/300, seasonal_3 Loss: 0.0577 | 0.0286
Epoch 61/300, seasonal_3 Loss: 0.0575 | 0.0284
Epoch 62/300, seasonal_3 Loss: 0.0574 | 0.0283
Epoch 63/300, seasonal_3 Loss: 0.0573 | 0.0281
Epoch 64/300, seasonal_3 Loss: 0.0572 | 0.0281
Epoch 65/300, seasonal_3 Loss: 0.0571 | 0.0280
Epoch 66/300, seasonal_3 Loss: 0.0570 | 0.0279
Epoch 67/300, seasonal_3 Loss: 0.0569 | 0.0278
Epoch 68/300, seasonal_3 Loss: 0.0568 | 0.0277
Epoch 69/300, seasonal_3 Loss: 0.0568 | 0.0276
Epoch 70/300, seasonal_3 Loss: 0.0567 | 0.0275
Epoch 71/300, seasonal_3 Loss: 0.0567 | 0.0274
Epoch 72/300, seasonal_3 Loss: 0.0566 | 0.0273
Epoch 73/300, seasonal_3 Loss: 0.0566 | 0.0272
Epoch 74/300, seasonal_3 Loss: 0.0565 | 0.0272
Epoch 75/300, seasonal_3 Loss: 0.0564 | 0.0272
Epoch 76/300, seasonal_3 Loss: 0.0563 | 0.0272
Epoch 77/300, seasonal_3 Loss: 0.0563 | 0.0271
Epoch 78/300, seasonal_3 Loss: 0.0562 | 0.0271
Epoch 79/300, seasonal_3 Loss: 0.0562 | 0.0271
Epoch 80/300, seasonal_3 Loss: 0.0561 | 0.0271
Epoch 81/300, seasonal_3 Loss: 0.0561 | 0.0270
Epoch 82/300, seasonal_3 Loss: 0.0560 | 0.0270
Epoch 83/300, seasonal_3 Loss: 0.0560 | 0.0269
Epoch 84/300, seasonal_3 Loss: 0.0559 | 0.0269
Epoch 85/300, seasonal_3 Loss: 0.0559 | 0.0269
Epoch 86/300, seasonal_3 Loss: 0.0559 | 0.0269
Epoch 87/300, seasonal_3 Loss: 0.0559 | 0.0268
Epoch 88/300, seasonal_3 Loss: 0.0558 | 0.0268
Epoch 89/300, seasonal_3 Loss: 0.0558 | 0.0268
Epoch 90/300, seasonal_3 Loss: 0.0558 | 0.0268
Epoch 91/300, seasonal_3 Loss: 0.0558 | 0.0268
Epoch 92/300, seasonal_3 Loss: 0.0557 | 0.0267
Epoch 93/300, seasonal_3 Loss: 0.0557 | 0.0267
Epoch 94/300, seasonal_3 Loss: 0.0557 | 0.0267
Epoch 95/300, seasonal_3 Loss: 0.0557 | 0.0267
Epoch 96/300, seasonal_3 Loss: 0.0557 | 0.0267
Epoch 97/300, seasonal_3 Loss: 0.0556 | 0.0267
Epoch 98/300, seasonal_3 Loss: 0.0556 | 0.0267
Epoch 99/300, seasonal_3 Loss: 0.0556 | 0.0267
Epoch 100/300, seasonal_3 Loss: 0.0556 | 0.0266
Epoch 101/300, seasonal_3 Loss: 0.0556 | 0.0266
Epoch 102/300, seasonal_3 Loss: 0.0556 | 0.0266
Epoch 103/300, seasonal_3 Loss: 0.0556 | 0.0266
Epoch 104/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 105/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 106/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 107/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 108/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 109/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 110/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 111/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 112/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 113/300, seasonal_3 Loss: 0.0555 | 0.0266
Epoch 114/300, seasonal_3 Loss: 0.0555 | 0.0265
Epoch 115/300, seasonal_3 Loss: 0.0555 | 0.0265
Epoch 116/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 117/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 118/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 119/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 120/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 121/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 122/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 123/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 124/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 125/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 126/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 127/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 128/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 129/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 130/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 131/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 132/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 133/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 134/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 135/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 136/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 137/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 138/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 139/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 140/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 141/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 142/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 143/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 144/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 145/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 146/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 147/300, seasonal_3 Loss: 0.0554 | 0.0265
Epoch 148/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 149/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 150/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 151/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 152/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 153/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 154/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 155/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 156/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 157/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 158/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 159/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 160/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 161/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 162/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 163/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 164/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 165/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 166/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 167/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 168/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 169/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 170/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 171/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 172/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 173/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 174/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 175/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 176/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 177/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 178/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 179/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 180/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 181/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 182/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 183/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 184/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 185/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 186/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 187/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 188/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 189/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 190/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 191/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 192/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 193/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 194/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 195/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 196/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 197/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 198/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 199/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 200/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 201/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 202/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 203/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 204/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 205/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 206/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 207/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 208/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 209/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 210/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 211/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 212/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 213/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 214/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 215/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 216/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 217/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 218/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 219/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 220/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 221/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 222/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 223/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 224/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 225/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 226/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 227/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 228/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 229/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 230/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 231/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 232/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 233/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 234/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 235/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 236/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 237/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 238/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 239/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 240/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 241/300, seasonal_3 Loss: 0.0553 | 0.0265
Epoch 242/300, seasonal_3 Loss: 0.0553 | 0.0265
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 8, 'train_rates': 0.9880235239419404, 'learning_rate': 0.00029076026456633156, 'batch_size': 83, 'step_size': 13, 'gamma': 0.784870682275844}
Epoch 1/300, resid Loss: 0.8032 | 0.2904
Epoch 2/300, resid Loss: 0.1686 | 0.1416
Epoch 3/300, resid Loss: 0.1245 | 0.0863
Epoch 4/300, resid Loss: 0.1267 | 0.0663
Epoch 5/300, resid Loss: 0.1221 | 0.1052
Epoch 6/300, resid Loss: 0.1151 | 0.0640
Epoch 7/300, resid Loss: 0.1117 | 0.0632
Epoch 8/300, resid Loss: 0.1149 | 0.1018
Epoch 9/300, resid Loss: 0.1200 | 0.0697
Epoch 10/300, resid Loss: 0.1153 | 0.0574
Epoch 11/300, resid Loss: 0.0982 | 0.0760
Epoch 12/300, resid Loss: 0.1211 | 0.0663
Epoch 13/300, resid Loss: 0.1067 | 0.0546
Epoch 14/300, resid Loss: 0.0994 | 0.0543
Epoch 15/300, resid Loss: 0.0981 | 0.0505
Epoch 16/300, resid Loss: 0.1004 | 0.0579
Epoch 17/300, resid Loss: 0.1153 | 0.0900
Epoch 18/300, resid Loss: 0.1189 | 0.0526
Epoch 19/300, resid Loss: 0.1100 | 0.0626
Epoch 20/300, resid Loss: 0.1435 | 0.0617
Epoch 21/300, resid Loss: 0.1268 | 0.0792
Epoch 22/300, resid Loss: 0.1385 | 0.0621
Epoch 23/300, resid Loss: 0.1269 | 0.0527
Epoch 24/300, resid Loss: 0.1196 | 0.0676
Epoch 25/300, resid Loss: 0.1046 | 0.0553
Epoch 26/300, resid Loss: 0.1213 | 0.0530
Epoch 27/300, resid Loss: 0.1043 | 0.0406
Epoch 28/300, resid Loss: 0.1243 | 0.0457
Epoch 29/300, resid Loss: 0.1354 | 0.0616
Epoch 30/300, resid Loss: 0.1059 | 0.0512
Epoch 31/300, resid Loss: 0.1101 | 0.0446
Epoch 32/300, resid Loss: 0.1017 | 0.0463
Epoch 33/300, resid Loss: 0.0998 | 0.0460
Epoch 34/300, resid Loss: 0.0943 | 0.0543
Epoch 35/300, resid Loss: 0.0982 | 0.0573
Epoch 36/300, resid Loss: 0.0949 | 0.0517
Epoch 37/300, resid Loss: 0.0977 | 0.0469
Epoch 38/300, resid Loss: 0.1086 | 0.0573
Epoch 39/300, resid Loss: 0.1180 | 0.0688
Epoch 40/300, resid Loss: 0.1260 | 0.1055
Epoch 41/300, resid Loss: 0.1142 | 0.0830
Epoch 42/300, resid Loss: 0.0864 | 0.0472
Epoch 43/300, resid Loss: 0.0859 | 0.0549
Epoch 44/300, resid Loss: 0.0838 | 0.0744
Epoch 45/300, resid Loss: 0.0849 | 0.0783
Epoch 46/300, resid Loss: 0.0774 | 0.0359
Epoch 47/300, resid Loss: 0.0744 | 0.0316
Epoch 48/300, resid Loss: 0.0695 | 0.0333
Epoch 49/300, resid Loss: 0.0714 | 0.0358
Epoch 50/300, resid Loss: 0.0721 | 0.0342
Epoch 51/300, resid Loss: 0.0729 | 0.0343
Epoch 52/300, resid Loss: 0.0789 | 0.0387
Epoch 53/300, resid Loss: 0.0848 | 0.0721
Epoch 54/300, resid Loss: 0.0808 | 0.0864
Epoch 55/300, resid Loss: 0.0900 | 0.0383
Epoch 56/300, resid Loss: 0.1063 | 0.0525
Epoch 57/300, resid Loss: 0.1109 | 0.0475
Epoch 58/300, resid Loss: 0.1001 | 0.0428
Epoch 59/300, resid Loss: 0.0786 | 0.0390
Epoch 60/300, resid Loss: 0.0765 | 0.0305
Epoch 61/300, resid Loss: 0.0821 | 0.0269
Epoch 62/300, resid Loss: 0.0647 | 0.0246
Epoch 63/300, resid Loss: 0.0670 | 0.0248
Epoch 64/300, resid Loss: 0.0677 | 0.0224
Epoch 65/300, resid Loss: 0.0650 | 0.0228
Epoch 66/300, resid Loss: 0.0635 | 0.0194
Epoch 67/300, resid Loss: 0.0632 | 0.0224
Epoch 68/300, resid Loss: 0.0690 | 0.0273
Epoch 69/300, resid Loss: 0.0703 | 0.0254
Epoch 70/300, resid Loss: 0.0625 | 0.0269
Epoch 71/300, resid Loss: 0.0709 | 0.0300
Epoch 72/300, resid Loss: 0.0801 | 0.0277
Epoch 73/300, resid Loss: 0.0777 | 0.0276
Epoch 74/300, resid Loss: 0.0655 | 0.0331
Epoch 75/300, resid Loss: 0.0703 | 0.0231
Epoch 76/300, resid Loss: 0.0686 | 0.0202
Epoch 77/300, resid Loss: 0.0564 | 0.0166
Epoch 78/300, resid Loss: 0.0567 | 0.0164
Epoch 79/300, resid Loss: 0.0555 | 0.0170
Epoch 80/300, resid Loss: 0.0535 | 0.0173
Epoch 81/300, resid Loss: 0.0531 | 0.0172
Epoch 82/300, resid Loss: 0.0527 | 0.0165
Epoch 83/300, resid Loss: 0.0524 | 0.0160
Epoch 84/300, resid Loss: 0.0522 | 0.0159
Epoch 85/300, resid Loss: 0.0520 | 0.0162
Epoch 86/300, resid Loss: 0.0519 | 0.0190
Epoch 87/300, resid Loss: 0.0520 | 0.0203
Epoch 88/300, resid Loss: 0.0522 | 0.0201
Epoch 89/300, resid Loss: 0.0521 | 0.0194
Epoch 90/300, resid Loss: 0.0516 | 0.0177
Epoch 91/300, resid Loss: 0.0518 | 0.0166
Epoch 92/300, resid Loss: 0.0538 | 0.0193
Epoch 93/300, resid Loss: 0.0542 | 0.0161
Epoch 94/300, resid Loss: 0.0526 | 0.0157
Epoch 95/300, resid Loss: 0.0532 | 0.0172
Epoch 96/300, resid Loss: 0.0515 | 0.0147
Epoch 97/300, resid Loss: 0.0520 | 0.0153
Epoch 98/300, resid Loss: 0.0508 | 0.0164
Epoch 99/300, resid Loss: 0.0504 | 0.0171
Epoch 100/300, resid Loss: 0.0502 | 0.0165
Epoch 101/300, resid Loss: 0.0501 | 0.0161
Epoch 102/300, resid Loss: 0.0501 | 0.0160
Epoch 103/300, resid Loss: 0.0500 | 0.0160
Epoch 104/300, resid Loss: 0.0499 | 0.0162
Epoch 105/300, resid Loss: 0.0497 | 0.0169
Epoch 106/300, resid Loss: 0.0497 | 0.0175
Epoch 107/300, resid Loss: 0.0496 | 0.0173
Epoch 108/300, resid Loss: 0.0495 | 0.0169
Epoch 109/300, resid Loss: 0.0495 | 0.0164
Epoch 110/300, resid Loss: 0.0494 | 0.0160
Epoch 111/300, resid Loss: 0.0495 | 0.0158
Epoch 112/300, resid Loss: 0.0495 | 0.0156
Epoch 113/300, resid Loss: 0.0494 | 0.0161
Epoch 114/300, resid Loss: 0.0493 | 0.0174
Epoch 115/300, resid Loss: 0.0494 | 0.0179
Epoch 116/300, resid Loss: 0.0493 | 0.0173
Epoch 117/300, resid Loss: 0.0491 | 0.0166
Epoch 118/300, resid Loss: 0.0491 | 0.0159
Epoch 119/300, resid Loss: 0.0491 | 0.0156
Epoch 120/300, resid Loss: 0.0491 | 0.0158
Epoch 121/300, resid Loss: 0.0490 | 0.0163
Epoch 122/300, resid Loss: 0.0489 | 0.0170
Epoch 123/300, resid Loss: 0.0489 | 0.0171
Epoch 124/300, resid Loss: 0.0489 | 0.0167
Epoch 125/300, resid Loss: 0.0488 | 0.0162
Epoch 126/300, resid Loss: 0.0488 | 0.0160
Epoch 127/300, resid Loss: 0.0488 | 0.0160
Epoch 128/300, resid Loss: 0.0487 | 0.0163
Epoch 129/300, resid Loss: 0.0487 | 0.0165
Epoch 130/300, resid Loss: 0.0487 | 0.0165
Epoch 131/300, resid Loss: 0.0486 | 0.0164
Epoch 132/300, resid Loss: 0.0486 | 0.0163
Epoch 133/300, resid Loss: 0.0486 | 0.0162
Epoch 134/300, resid Loss: 0.0485 | 0.0163
Epoch 135/300, resid Loss: 0.0485 | 0.0163
Epoch 136/300, resid Loss: 0.0485 | 0.0163
Epoch 137/300, resid Loss: 0.0485 | 0.0163
Epoch 138/300, resid Loss: 0.0484 | 0.0163
Epoch 139/300, resid Loss: 0.0484 | 0.0163
Epoch 140/300, resid Loss: 0.0484 | 0.0163
Epoch 141/300, resid Loss: 0.0484 | 0.0163
Epoch 142/300, resid Loss: 0.0484 | 0.0163
Epoch 143/300, resid Loss: 0.0483 | 0.0163
Epoch 144/300, resid Loss: 0.0483 | 0.0163
Epoch 145/300, resid Loss: 0.0483 | 0.0163
Epoch 146/300, resid Loss: 0.0483 | 0.0163
Epoch 147/300, resid Loss: 0.0483 | 0.0163
Epoch 148/300, resid Loss: 0.0483 | 0.0163
Epoch 149/300, resid Loss: 0.0482 | 0.0163
Epoch 150/300, resid Loss: 0.0482 | 0.0162
Epoch 151/300, resid Loss: 0.0482 | 0.0163
Epoch 152/300, resid Loss: 0.0482 | 0.0163
Epoch 153/300, resid Loss: 0.0482 | 0.0163
Epoch 154/300, resid Loss: 0.0482 | 0.0163
Epoch 155/300, resid Loss: 0.0481 | 0.0162
Epoch 156/300, resid Loss: 0.0481 | 0.0162
Epoch 157/300, resid Loss: 0.0481 | 0.0163
Epoch 158/300, resid Loss: 0.0481 | 0.0163
Epoch 159/300, resid Loss: 0.0481 | 0.0163
Epoch 160/300, resid Loss: 0.0481 | 0.0163
Epoch 161/300, resid Loss: 0.0481 | 0.0162
Epoch 162/300, resid Loss: 0.0480 | 0.0162
Epoch 163/300, resid Loss: 0.0480 | 0.0162
Epoch 164/300, resid Loss: 0.0480 | 0.0163
Epoch 165/300, resid Loss: 0.0480 | 0.0163
Epoch 166/300, resid Loss: 0.0480 | 0.0163
Epoch 167/300, resid Loss: 0.0480 | 0.0162
Epoch 168/300, resid Loss: 0.0480 | 0.0162
Epoch 169/300, resid Loss: 0.0480 | 0.0162
Epoch 170/300, resid Loss: 0.0480 | 0.0163
Epoch 171/300, resid Loss: 0.0479 | 0.0163
Epoch 172/300, resid Loss: 0.0479 | 0.0163
Epoch 173/300, resid Loss: 0.0479 | 0.0162
Epoch 174/300, resid Loss: 0.0479 | 0.0162
Epoch 175/300, resid Loss: 0.0479 | 0.0162
Epoch 176/300, resid Loss: 0.0479 | 0.0162
Epoch 177/300, resid Loss: 0.0479 | 0.0163
Epoch 178/300, resid Loss: 0.0479 | 0.0163
Epoch 179/300, resid Loss: 0.0479 | 0.0162
Epoch 180/300, resid Loss: 0.0479 | 0.0162
Epoch 181/300, resid Loss: 0.0479 | 0.0162
Epoch 182/300, resid Loss: 0.0479 | 0.0162
Epoch 183/300, resid Loss: 0.0478 | 0.0163
Epoch 184/300, resid Loss: 0.0478 | 0.0163
Epoch 185/300, resid Loss: 0.0478 | 0.0162
Epoch 186/300, resid Loss: 0.0478 | 0.0162
Epoch 187/300, resid Loss: 0.0478 | 0.0162
Epoch 188/300, resid Loss: 0.0478 | 0.0162
Epoch 189/300, resid Loss: 0.0478 | 0.0162
Epoch 190/300, resid Loss: 0.0478 | 0.0163
Epoch 191/300, resid Loss: 0.0478 | 0.0163
Epoch 192/300, resid Loss: 0.0478 | 0.0162
Epoch 193/300, resid Loss: 0.0478 | 0.0162
Epoch 194/300, resid Loss: 0.0478 | 0.0162
Epoch 195/300, resid Loss: 0.0478 | 0.0162
Epoch 196/300, resid Loss: 0.0478 | 0.0163
Epoch 197/300, resid Loss: 0.0478 | 0.0163
Epoch 198/300, resid Loss: 0.0478 | 0.0162
Epoch 199/300, resid Loss: 0.0478 | 0.0162
Epoch 200/300, resid Loss: 0.0478 | 0.0162
Epoch 201/300, resid Loss: 0.0478 | 0.0162
Epoch 202/300, resid Loss: 0.0478 | 0.0162
Epoch 203/300, resid Loss: 0.0477 | 0.0162
Epoch 204/300, resid Loss: 0.0477 | 0.0163
Epoch 205/300, resid Loss: 0.0477 | 0.0162
Epoch 206/300, resid Loss: 0.0477 | 0.0162
Epoch 207/300, resid Loss: 0.0477 | 0.0162
Epoch 208/300, resid Loss: 0.0477 | 0.0162
Epoch 209/300, resid Loss: 0.0477 | 0.0162
Epoch 210/300, resid Loss: 0.0477 | 0.0163
Epoch 211/300, resid Loss: 0.0477 | 0.0162
Epoch 212/300, resid Loss: 0.0477 | 0.0162
Epoch 213/300, resid Loss: 0.0477 | 0.0162
Epoch 214/300, resid Loss: 0.0477 | 0.0162
Epoch 215/300, resid Loss: 0.0477 | 0.0162
Epoch 216/300, resid Loss: 0.0477 | 0.0162
Epoch 217/300, resid Loss: 0.0477 | 0.0162
Epoch 218/300, resid Loss: 0.0477 | 0.0162
Epoch 219/300, resid Loss: 0.0477 | 0.0162
Epoch 220/300, resid Loss: 0.0477 | 0.0162
Epoch 221/300, resid Loss: 0.0477 | 0.0162
Epoch 222/300, resid Loss: 0.0477 | 0.0162
Epoch 223/300, resid Loss: 0.0477 | 0.0162
Epoch 224/300, resid Loss: 0.0477 | 0.0162
Epoch 225/300, resid Loss: 0.0477 | 0.0162
Epoch 226/300, resid Loss: 0.0477 | 0.0162
Epoch 227/300, resid Loss: 0.0477 | 0.0162
Epoch 228/300, resid Loss: 0.0477 | 0.0162
Epoch 229/300, resid Loss: 0.0477 | 0.0162
Epoch 230/300, resid Loss: 0.0477 | 0.0162
Epoch 231/300, resid Loss: 0.0477 | 0.0162
Epoch 232/300, resid Loss: 0.0477 | 0.0162
Epoch 233/300, resid Loss: 0.0477 | 0.0162
Epoch 234/300, resid Loss: 0.0477 | 0.0162
Epoch 235/300, resid Loss: 0.0477 | 0.0162
Epoch 236/300, resid Loss: 0.0477 | 0.0162
Epoch 237/300, resid Loss: 0.0477 | 0.0162
Epoch 238/300, resid Loss: 0.0477 | 0.0162
Epoch 239/300, resid Loss: 0.0477 | 0.0162
Epoch 240/300, resid Loss: 0.0477 | 0.0162
Epoch 241/300, resid Loss: 0.0477 | 0.0162
Epoch 242/300, resid Loss: 0.0477 | 0.0162
Epoch 243/300, resid Loss: 0.0477 | 0.0162
Epoch 244/300, resid Loss: 0.0477 | 0.0162
Epoch 245/300, resid Loss: 0.0477 | 0.0162
Epoch 246/300, resid Loss: 0.0477 | 0.0162
Epoch 247/300, resid Loss: 0.0477 | 0.0162
Epoch 248/300, resid Loss: 0.0477 | 0.0162
Epoch 249/300, resid Loss: 0.0477 | 0.0162
Epoch 250/300, resid Loss: 0.0477 | 0.0162
Epoch 251/300, resid Loss: 0.0477 | 0.0162
Epoch 252/300, resid Loss: 0.0476 | 0.0162
Epoch 253/300, resid Loss: 0.0476 | 0.0162
Epoch 254/300, resid Loss: 0.0476 | 0.0162
Epoch 255/300, resid Loss: 0.0476 | 0.0162
Epoch 256/300, resid Loss: 0.0476 | 0.0162
Epoch 257/300, resid Loss: 0.0476 | 0.0162
Epoch 258/300, resid Loss: 0.0476 | 0.0162
Epoch 259/300, resid Loss: 0.0476 | 0.0162
Epoch 260/300, resid Loss: 0.0476 | 0.0162
Epoch 261/300, resid Loss: 0.0476 | 0.0162
Early stopping for resid
Runtime (seconds): 5693.166605234146
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[212.06015]
[2.4408817]
[0.40588465]
[4.8441014]
[-1.2868078]
[5.662696]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 119.5324615391437
RMSE: 10.933090209960938
MAE: 10.933090209960938
R-squared: nan
[224.1269]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
