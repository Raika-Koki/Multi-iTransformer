[32m[I 2025-01-07 06:54:53,399][0m A new study created in memory with name: no-name-ce221ac3-da35-4a36-b32e-bdb3adaf4eb3[0m
[32m[I 2025-01-07 06:58:10,522][0m Trial 0 finished with value: 0.30455763750644615 and parameters: {'observation_period_num': 146, 'train_rates': 0.8778750483908835, 'learning_rate': 0.00029215558696904514, 'batch_size': 194, 'step_size': 7, 'gamma': 0.785865987951037}. Best is trial 0 with value: 0.30455763750644615.[0m
[32m[I 2025-01-07 07:00:24,729][0m Trial 1 finished with value: 0.7248376272618771 and parameters: {'observation_period_num': 112, 'train_rates': 0.8176523479638184, 'learning_rate': 6.053886999001249e-06, 'batch_size': 221, 'step_size': 4, 'gamma': 0.9639450131955495}. Best is trial 0 with value: 0.30455763750644615.[0m
[32m[I 2025-01-07 07:01:53,562][0m Trial 2 finished with value: 0.3789829880531345 and parameters: {'observation_period_num': 71, 'train_rates': 0.84522367615308, 'learning_rate': 0.0004034434377071212, 'batch_size': 219, 'step_size': 15, 'gamma': 0.8169829811071672}. Best is trial 0 with value: 0.30455763750644615.[0m
[32m[I 2025-01-07 07:06:35,629][0m Trial 3 finished with value: 0.9834441936260198 and parameters: {'observation_period_num': 231, 'train_rates': 0.6781580231815555, 'learning_rate': 0.0009660116276105921, 'batch_size': 193, 'step_size': 8, 'gamma': 0.9136682953927229}. Best is trial 0 with value: 0.30455763750644615.[0m
[32m[I 2025-01-07 07:07:27,221][0m Trial 4 finished with value: 0.3347871088590778 and parameters: {'observation_period_num': 40, 'train_rates': 0.874906667992319, 'learning_rate': 4.066396450213317e-05, 'batch_size': 192, 'step_size': 4, 'gamma': 0.8434786624201727}. Best is trial 0 with value: 0.30455763750644615.[0m
[32m[I 2025-01-07 07:08:55,927][0m Trial 5 finished with value: 0.14395980536937714 and parameters: {'observation_period_num': 63, 'train_rates': 0.9786450940469922, 'learning_rate': 0.00020030789113530288, 'batch_size': 80, 'step_size': 5, 'gamma': 0.9401596164243262}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:10:09,618][0m Trial 6 finished with value: 0.6952617986701352 and parameters: {'observation_period_num': 23, 'train_rates': 0.7051236638471591, 'learning_rate': 6.666070362247032e-05, 'batch_size': 45, 'step_size': 14, 'gamma': 0.8679493903442695}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:15:44,098][0m Trial 7 finished with value: 0.5501127243041992 and parameters: {'observation_period_num': 217, 'train_rates': 0.964074862159992, 'learning_rate': 8.480386823790767e-06, 'batch_size': 248, 'step_size': 11, 'gamma': 0.7626181031494884}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:16:40,387][0m Trial 8 finished with value: 1.5105718896420568 and parameters: {'observation_period_num': 13, 'train_rates': 0.7182802223607859, 'learning_rate': 3.1136347987489846e-06, 'batch_size': 58, 'step_size': 3, 'gamma': 0.7796988979464309}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:19:04,095][0m Trial 9 finished with value: 0.1532484158220519 and parameters: {'observation_period_num': 111, 'train_rates': 0.9119700022676925, 'learning_rate': 0.00020385510200849022, 'batch_size': 179, 'step_size': 11, 'gamma': 0.9347143740716306}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:23:10,468][0m Trial 10 finished with value: 0.15393918752670288 and parameters: {'observation_period_num': 167, 'train_rates': 0.9893414157897209, 'learning_rate': 8.401315867399838e-05, 'batch_size': 114, 'step_size': 1, 'gamma': 0.9876489481528915}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:25:12,152][0m Trial 11 finished with value: 0.151026198013344 and parameters: {'observation_period_num': 90, 'train_rates': 0.9308259220951656, 'learning_rate': 0.0001948304943230739, 'batch_size': 119, 'step_size': 11, 'gamma': 0.9156119357053003}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:26:57,988][0m Trial 12 finished with value: 0.1697038682443755 and parameters: {'observation_period_num': 75, 'train_rates': 0.9515642602489335, 'learning_rate': 2.3882015633651786e-05, 'batch_size': 103, 'step_size': 10, 'gamma': 0.9098018175128253}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:28:24,380][0m Trial 13 finished with value: 0.6891249233808764 and parameters: {'observation_period_num': 72, 'train_rates': 0.758767540611664, 'learning_rate': 0.00013038552616750237, 'batch_size': 88, 'step_size': 6, 'gamma': 0.8941859057701528}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:31:32,448][0m Trial 14 finished with value: 1.2296080133591332 and parameters: {'observation_period_num': 176, 'train_rates': 0.6112780172777987, 'learning_rate': 0.0009002728129723173, 'batch_size': 142, 'step_size': 13, 'gamma': 0.9368744044141508}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:33:40,387][0m Trial 15 finished with value: 0.20298196509012728 and parameters: {'observation_period_num': 97, 'train_rates': 0.9224854992608247, 'learning_rate': 2.1619791532040183e-05, 'batch_size': 141, 'step_size': 9, 'gamma': 0.9555968520912583}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:36:36,576][0m Trial 16 finished with value: 0.4415836225075977 and parameters: {'observation_period_num': 57, 'train_rates': 0.9166328139439205, 'learning_rate': 0.0004445304263657707, 'batch_size': 22, 'step_size': 6, 'gamma': 0.8862423683853768}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:39:36,688][0m Trial 17 finished with value: 0.9398130205090486 and parameters: {'observation_period_num': 138, 'train_rates': 0.8099189230629797, 'learning_rate': 1.4961553402659954e-06, 'batch_size': 74, 'step_size': 12, 'gamma': 0.8655500083772858}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:41:49,819][0m Trial 18 finished with value: 0.14562447369098663 and parameters: {'observation_period_num': 95, 'train_rates': 0.9897680755547773, 'learning_rate': 0.00015661695092573244, 'batch_size': 128, 'step_size': 1, 'gamma': 0.9849899239215704}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:42:45,511][0m Trial 19 finished with value: 0.1789538562297821 and parameters: {'observation_period_num': 39, 'train_rates': 0.9892319207249888, 'learning_rate': 9.330103587710433e-05, 'batch_size': 163, 'step_size': 1, 'gamma': 0.9826701773531791}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:47:21,394][0m Trial 20 finished with value: 0.3025728121163353 and parameters: {'observation_period_num': 193, 'train_rates': 0.8637592137118715, 'learning_rate': 4.3051374080559916e-05, 'batch_size': 78, 'step_size': 3, 'gamma': 0.9563130493515679}. Best is trial 5 with value: 0.14395980536937714.[0m
[32m[I 2025-01-07 07:49:34,840][0m Trial 21 finished with value: 0.12844869278479312 and parameters: {'observation_period_num': 97, 'train_rates': 0.9520639652290522, 'learning_rate': 0.00016729174204341128, 'batch_size': 121, 'step_size': 5, 'gamma': 0.9318866859666068}. Best is trial 21 with value: 0.12844869278479312.[0m
[32m[I 2025-01-07 07:52:21,689][0m Trial 22 finished with value: 0.13483087718486786 and parameters: {'observation_period_num': 122, 'train_rates': 0.9600553131899053, 'learning_rate': 0.00016950593922766453, 'batch_size': 131, 'step_size': 5, 'gamma': 0.9426975282988808}. Best is trial 21 with value: 0.12844869278479312.[0m
[32m[I 2025-01-07 07:55:14,970][0m Trial 23 finished with value: 0.26742364168167115 and parameters: {'observation_period_num': 126, 'train_rates': 0.9507658768846486, 'learning_rate': 0.00047820402474399676, 'batch_size': 99, 'step_size': 5, 'gamma': 0.9355206139801395}. Best is trial 21 with value: 0.12844869278479312.[0m
[32m[I 2025-01-07 07:58:45,931][0m Trial 24 finished with value: 0.3634794927850554 and parameters: {'observation_period_num': 154, 'train_rates': 0.8876904225924004, 'learning_rate': 1.4805793700027329e-05, 'batch_size': 156, 'step_size': 7, 'gamma': 0.9466858763870831}. Best is trial 21 with value: 0.12844869278479312.[0m
[32m[I 2025-01-07 08:01:35,585][0m Trial 25 finished with value: 0.11929588290778073 and parameters: {'observation_period_num': 118, 'train_rates': 0.9577309938126407, 'learning_rate': 6.188115248822746e-05, 'batch_size': 54, 'step_size': 5, 'gamma': 0.8960556992092563}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:05:43,649][0m Trial 26 finished with value: 0.17398627225216146 and parameters: {'observation_period_num': 119, 'train_rates': 0.9020147263100622, 'learning_rate': 6.087735472427061e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.8477294735289792}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:10:14,859][0m Trial 27 finished with value: 0.43350033558291157 and parameters: {'observation_period_num': 192, 'train_rates': 0.8381803847383295, 'learning_rate': 0.00012056517765539527, 'batch_size': 42, 'step_size': 8, 'gamma': 0.9010182226619385}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:12:42,319][0m Trial 28 finished with value: 0.5033516291582074 and parameters: {'observation_period_num': 128, 'train_rates': 0.7774113329671452, 'learning_rate': 5.303120185025602e-05, 'batch_size': 160, 'step_size': 5, 'gamma': 0.8875574762830246}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:16:26,320][0m Trial 29 finished with value: 0.1582698736530848 and parameters: {'observation_period_num': 153, 'train_rates': 0.9445805761506711, 'learning_rate': 0.0002813146983060125, 'batch_size': 67, 'step_size': 7, 'gamma': 0.9219150781090105}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:19:43,755][0m Trial 30 finished with value: 0.4217461829676348 and parameters: {'observation_period_num': 142, 'train_rates': 0.8917646073260324, 'learning_rate': 3.246755699730913e-05, 'batch_size': 99, 'step_size': 2, 'gamma': 0.817386926748687}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:21:53,948][0m Trial 31 finished with value: 0.3252380493375444 and parameters: {'observation_period_num': 87, 'train_rates': 0.9664027466597664, 'learning_rate': 0.00026465411456060337, 'batch_size': 49, 'step_size': 5, 'gamma': 0.9726838049518101}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:24:23,715][0m Trial 32 finished with value: 0.14054838628793248 and parameters: {'observation_period_num': 107, 'train_rates': 0.9659629610066666, 'learning_rate': 0.00012085792192375907, 'batch_size': 87, 'step_size': 6, 'gamma': 0.9333815702833201}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:26:51,709][0m Trial 33 finished with value: 0.1575148261255688 and parameters: {'observation_period_num': 112, 'train_rates': 0.936879252264438, 'learning_rate': 0.00010578936688312372, 'batch_size': 118, 'step_size': 6, 'gamma': 0.9652232076488511}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:29:08,548][0m Trial 34 finished with value: 0.48628267223636307 and parameters: {'observation_period_num': 105, 'train_rates': 0.8532589815920559, 'learning_rate': 0.0005470529966097069, 'batch_size': 91, 'step_size': 4, 'gamma': 0.9262905345895235}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:32:35,165][0m Trial 35 finished with value: 0.13164598796818708 and parameters: {'observation_period_num': 133, 'train_rates': 0.947756054726831, 'learning_rate': 7.538155618579779e-05, 'batch_size': 36, 'step_size': 8, 'gamma': 0.9019863136751213}. Best is trial 25 with value: 0.11929588290778073.[0m
[32m[I 2025-01-07 08:36:00,535][0m Trial 36 finished with value: 0.11848177764699867 and parameters: {'observation_period_num': 133, 'train_rates': 0.9320800239501746, 'learning_rate': 6.820540474321688e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.902742831486717}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:40:09,571][0m Trial 37 finished with value: 0.22190677186429705 and parameters: {'observation_period_num': 169, 'train_rates': 0.8954073718516999, 'learning_rate': 1.0903508174779021e-05, 'batch_size': 35, 'step_size': 9, 'gamma': 0.8538018065938371}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:43:31,342][0m Trial 38 finished with value: 0.3568262112498795 and parameters: {'observation_period_num': 140, 'train_rates': 0.835094293888086, 'learning_rate': 2.6596518706383216e-05, 'batch_size': 32, 'step_size': 8, 'gamma': 0.8792986385044809}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:47:55,003][0m Trial 39 finished with value: 0.24321954128112686 and parameters: {'observation_period_num': 186, 'train_rates': 0.8720402815830102, 'learning_rate': 7.278533835310344e-05, 'batch_size': 52, 'step_size': 9, 'gamma': 0.8238758756287095}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:49:00,622][0m Trial 40 finished with value: 0.19438324868679047 and parameters: {'observation_period_num': 51, 'train_rates': 0.9336135485178855, 'learning_rate': 4.217268092605646e-05, 'batch_size': 215, 'step_size': 10, 'gamma': 0.9035388745043831}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:52:02,457][0m Trial 41 finished with value: 0.15480794116854668 and parameters: {'observation_period_num': 128, 'train_rates': 0.9647605346365149, 'learning_rate': 5.3850761272821834e-05, 'batch_size': 59, 'step_size': 4, 'gamma': 0.872303997007344}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:54:30,788][0m Trial 42 finished with value: 0.13467045966535807 and parameters: {'observation_period_num': 82, 'train_rates': 0.9224148717080985, 'learning_rate': 8.367250287883988e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.9222957880079542}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 08:56:48,005][0m Trial 43 finished with value: 0.14657474578695096 and parameters: {'observation_period_num': 80, 'train_rates': 0.9187475266324965, 'learning_rate': 3.74622057207496e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.9155836280794921}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 09:00:30,378][0m Trial 44 finished with value: 0.18855066299438478 and parameters: {'observation_period_num': 153, 'train_rates': 0.93978372147757, 'learning_rate': 1.529990232515573e-05, 'batch_size': 64, 'step_size': 7, 'gamma': 0.8986764844741584}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 09:03:13,405][0m Trial 45 finished with value: 0.1460898893830531 and parameters: {'observation_period_num': 99, 'train_rates': 0.9051280138912244, 'learning_rate': 9.043694944648847e-05, 'batch_size': 26, 'step_size': 8, 'gamma': 0.9231653386434755}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 09:05:12,994][0m Trial 46 finished with value: 0.12277419089454494 and parameters: {'observation_period_num': 66, 'train_rates': 0.9748889866404784, 'learning_rate': 7.181179476947667e-05, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9100755514194925}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 09:10:12,711][0m Trial 47 finished with value: 1.0223420955399896 and parameters: {'observation_period_num': 244, 'train_rates': 0.6384365970445767, 'learning_rate': 0.0002500231365340178, 'batch_size': 39, 'step_size': 10, 'gamma': 0.9090809211595747}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 09:11:49,712][0m Trial 48 finished with value: 0.12671605745951334 and parameters: {'observation_period_num': 63, 'train_rates': 0.9753420350919302, 'learning_rate': 6.39579892807087e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.8927193074394587}. Best is trial 36 with value: 0.11848177764699867.[0m
[32m[I 2025-01-07 09:13:14,292][0m Trial 49 finished with value: 0.13817520732762384 and parameters: {'observation_period_num': 42, 'train_rates': 0.979334535347458, 'learning_rate': 1.9529662542697692e-05, 'batch_size': 52, 'step_size': 15, 'gamma': 0.8637205766834313}. Best is trial 36 with value: 0.11848177764699867.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AMZN_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.6961 | 0.7998
Epoch 2/300, Loss: 0.5364 | 0.7558
Epoch 3/300, Loss: 0.4061 | 0.7059
Epoch 4/300, Loss: 0.3971 | 0.4520
Epoch 5/300, Loss: 0.3418 | 0.3981
Epoch 6/300, Loss: 0.3256 | 0.3789
Epoch 7/300, Loss: 0.3088 | 0.3313
Epoch 8/300, Loss: 0.2754 | 0.3546
Epoch 9/300, Loss: 0.2837 | 0.3460
Epoch 10/300, Loss: 0.2689 | 0.2935
Epoch 11/300, Loss: 0.2371 | 0.2776
Epoch 12/300, Loss: 0.2412 | 0.2994
Epoch 13/300, Loss: 0.2123 | 0.2805
Epoch 14/300, Loss: 0.2057 | 0.2638
Epoch 15/300, Loss: 0.2083 | 0.2823
Epoch 16/300, Loss: 0.2005 | 0.2542
Epoch 17/300, Loss: 0.2018 | 0.3020
Epoch 18/300, Loss: 0.2018 | 0.2798
Epoch 19/300, Loss: 0.2098 | 0.2311
Epoch 20/300, Loss: 0.2103 | 0.2840
Epoch 21/300, Loss: 0.2017 | 0.2367
Epoch 22/300, Loss: 0.2064 | 0.2254
Epoch 23/300, Loss: 0.2102 | 0.2257
Epoch 24/300, Loss: 0.1938 | 0.2059
Epoch 25/300, Loss: 0.1831 | 0.2270
Epoch 26/300, Loss: 0.1804 | 0.2046
Epoch 27/300, Loss: 0.1705 | 0.1908
Epoch 28/300, Loss: 0.1669 | 0.2055
Epoch 29/300, Loss: 0.1643 | 0.1914
Epoch 30/300, Loss: 0.1602 | 0.1870
Epoch 31/300, Loss: 0.1574 | 0.1875
Epoch 32/300, Loss: 0.1592 | 0.1885
Epoch 33/300, Loss: 0.1555 | 0.1903
Epoch 34/300, Loss: 0.1545 | 0.1784
Epoch 35/300, Loss: 0.1546 | 0.1755
Epoch 36/300, Loss: 0.1503 | 0.1829
Epoch 37/300, Loss: 0.1511 | 0.1750
Epoch 38/300, Loss: 0.1482 | 0.1663
Epoch 39/300, Loss: 0.1460 | 0.1802
Epoch 40/300, Loss: 0.1465 | 0.1734
Epoch 41/300, Loss: 0.1467 | 0.1630
Epoch 42/300, Loss: 0.1477 | 0.1805
Epoch 43/300, Loss: 0.1486 | 0.1638
Epoch 44/300, Loss: 0.1471 | 0.1666
Epoch 45/300, Loss: 0.1477 | 0.1798
Epoch 46/300, Loss: 0.1465 | 0.1570
Epoch 47/300, Loss: 0.1445 | 0.1751
Epoch 48/300, Loss: 0.1435 | 0.1629
Epoch 49/300, Loss: 0.1440 | 0.1625
Epoch 50/300, Loss: 0.1402 | 0.1645
Epoch 51/300, Loss: 0.1428 | 0.1567
Epoch 52/300, Loss: 0.1422 | 0.1622
Epoch 53/300, Loss: 0.1355 | 0.1498
Epoch 54/300, Loss: 0.1355 | 0.1507
Epoch 55/300, Loss: 0.1358 | 0.1670
Epoch 56/300, Loss: 0.1319 | 0.1405
Epoch 57/300, Loss: 0.1340 | 0.1441
Epoch 58/300, Loss: 0.1320 | 0.1582
Epoch 59/300, Loss: 0.1304 | 0.1384
Epoch 60/300, Loss: 0.1304 | 0.1396
Epoch 61/300, Loss: 0.1286 | 0.1507
Epoch 62/300, Loss: 0.1270 | 0.1400
Epoch 63/300, Loss: 0.1268 | 0.1364
Epoch 64/300, Loss: 0.1271 | 0.1391
Epoch 65/300, Loss: 0.1254 | 0.1384
Epoch 66/300, Loss: 0.1247 | 0.1373
Epoch 67/300, Loss: 0.1246 | 0.1378
Epoch 68/300, Loss: 0.1242 | 0.1328
Epoch 69/300, Loss: 0.1236 | 0.1408
Epoch 70/300, Loss: 0.1241 | 0.1365
Epoch 71/300, Loss: 0.1236 | 0.1277
Epoch 72/300, Loss: 0.1231 | 0.1373
Epoch 73/300, Loss: 0.1225 | 0.1375
Epoch 74/300, Loss: 0.1219 | 0.1271
Epoch 75/300, Loss: 0.1212 | 0.1290
Epoch 76/300, Loss: 0.1196 | 0.1301
Epoch 77/300, Loss: 0.1196 | 0.1284
Epoch 78/300, Loss: 0.1196 | 0.1280
Epoch 79/300, Loss: 0.1185 | 0.1273
Epoch 80/300, Loss: 0.1179 | 0.1281
Epoch 81/300, Loss: 0.1169 | 0.1273
Epoch 82/300, Loss: 0.1175 | 0.1263
Epoch 83/300, Loss: 0.1173 | 0.1244
Epoch 84/300, Loss: 0.1171 | 0.1243
Epoch 85/300, Loss: 0.1161 | 0.1276
Epoch 86/300, Loss: 0.1155 | 0.1214
Epoch 87/300, Loss: 0.1148 | 0.1228
Epoch 88/300, Loss: 0.1156 | 0.1275
Epoch 89/300, Loss: 0.1138 | 0.1211
Epoch 90/300, Loss: 0.1136 | 0.1203
Epoch 91/300, Loss: 0.1128 | 0.1239
Epoch 92/300, Loss: 0.1131 | 0.1209
Epoch 93/300, Loss: 0.1128 | 0.1177
Epoch 94/300, Loss: 0.1118 | 0.1206
Epoch 95/300, Loss: 0.1119 | 0.1211
Epoch 96/300, Loss: 0.1122 | 0.1186
Epoch 97/300, Loss: 0.1108 | 0.1173
Epoch 98/300, Loss: 0.1108 | 0.1182
Epoch 99/300, Loss: 0.1111 | 0.1188
Epoch 100/300, Loss: 0.1115 | 0.1177
Epoch 101/300, Loss: 0.1099 | 0.1161
Epoch 102/300, Loss: 0.1102 | 0.1190
Epoch 103/300, Loss: 0.1096 | 0.1160
Epoch 104/300, Loss: 0.1102 | 0.1146
Epoch 105/300, Loss: 0.1093 | 0.1175
Epoch 106/300, Loss: 0.1086 | 0.1159
Epoch 107/300, Loss: 0.1089 | 0.1148
Epoch 108/300, Loss: 0.1082 | 0.1156
Epoch 109/300, Loss: 0.1079 | 0.1156
Epoch 110/300, Loss: 0.1082 | 0.1132
Epoch 111/300, Loss: 0.1078 | 0.1143
Epoch 112/300, Loss: 0.1075 | 0.1149
Epoch 113/300, Loss: 0.1072 | 0.1142
Epoch 114/300, Loss: 0.1073 | 0.1116
Epoch 115/300, Loss: 0.1075 | 0.1129
Epoch 116/300, Loss: 0.1067 | 0.1143
Epoch 117/300, Loss: 0.1066 | 0.1131
Epoch 118/300, Loss: 0.1064 | 0.1120
Epoch 119/300, Loss: 0.1060 | 0.1132
Epoch 120/300, Loss: 0.1062 | 0.1130
Epoch 121/300, Loss: 0.1053 | 0.1114
Epoch 122/300, Loss: 0.1055 | 0.1113
Epoch 123/300, Loss: 0.1050 | 0.1120
Epoch 124/300, Loss: 0.1055 | 0.1113
Epoch 125/300, Loss: 0.1052 | 0.1116
Epoch 126/300, Loss: 0.1051 | 0.1100
Epoch 127/300, Loss: 0.1046 | 0.1100
Epoch 128/300, Loss: 0.1041 | 0.1106
Epoch 129/300, Loss: 0.1042 | 0.1105
Epoch 130/300, Loss: 0.1038 | 0.1098
Epoch 131/300, Loss: 0.1037 | 0.1101
Epoch 132/300, Loss: 0.1041 | 0.1094
Epoch 133/300, Loss: 0.1046 | 0.1098
Epoch 134/300, Loss: 0.1036 | 0.1095
Epoch 135/300, Loss: 0.1042 | 0.1094
Epoch 136/300, Loss: 0.1040 | 0.1089
Epoch 137/300, Loss: 0.1040 | 0.1090
Epoch 138/300, Loss: 0.1034 | 0.1090
Epoch 139/300, Loss: 0.1035 | 0.1095
Epoch 140/300, Loss: 0.1022 | 0.1088
Epoch 141/300, Loss: 0.1030 | 0.1079
Epoch 142/300, Loss: 0.1027 | 0.1097
Epoch 143/300, Loss: 0.1026 | 0.1084
Epoch 144/300, Loss: 0.1022 | 0.1082
Epoch 145/300, Loss: 0.1029 | 0.1080
Epoch 146/300, Loss: 0.1026 | 0.1085
Epoch 147/300, Loss: 0.1024 | 0.1082
Epoch 148/300, Loss: 0.1021 | 0.1080
Epoch 149/300, Loss: 0.1023 | 0.1079
Epoch 150/300, Loss: 0.1011 | 0.1076
Epoch 151/300, Loss: 0.1015 | 0.1076
Epoch 152/300, Loss: 0.1024 | 0.1076
Epoch 153/300, Loss: 0.1013 | 0.1074
Epoch 154/300, Loss: 0.1021 | 0.1073
Epoch 155/300, Loss: 0.1013 | 0.1075
Epoch 156/300, Loss: 0.1019 | 0.1072
Epoch 157/300, Loss: 0.1010 | 0.1065
Epoch 158/300, Loss: 0.1011 | 0.1074
Epoch 159/300, Loss: 0.1008 | 0.1069
Epoch 160/300, Loss: 0.1014 | 0.1063
Epoch 161/300, Loss: 0.1015 | 0.1066
Epoch 162/300, Loss: 0.1017 | 0.1063
Epoch 163/300, Loss: 0.1011 | 0.1059
Epoch 164/300, Loss: 0.1011 | 0.1062
Epoch 165/300, Loss: 0.1020 | 0.1063
Epoch 166/300, Loss: 0.1010 | 0.1065
Epoch 167/300, Loss: 0.1012 | 0.1063
Epoch 168/300, Loss: 0.1011 | 0.1064
Epoch 169/300, Loss: 0.1009 | 0.1061
Epoch 170/300, Loss: 0.1009 | 0.1059
Epoch 171/300, Loss: 0.1008 | 0.1061
Epoch 172/300, Loss: 0.1005 | 0.1063
Epoch 173/300, Loss: 0.1006 | 0.1063
Epoch 174/300, Loss: 0.1007 | 0.1059
Epoch 175/300, Loss: 0.1008 | 0.1063
Epoch 176/300, Loss: 0.0999 | 0.1056
Epoch 177/300, Loss: 0.1006 | 0.1059
Epoch 178/300, Loss: 0.1004 | 0.1063
Epoch 179/300, Loss: 0.1002 | 0.1058
Epoch 180/300, Loss: 0.1008 | 0.1058
Epoch 181/300, Loss: 0.1001 | 0.1059
Epoch 182/300, Loss: 0.1005 | 0.1056
Epoch 183/300, Loss: 0.0999 | 0.1056
Epoch 184/300, Loss: 0.1000 | 0.1055
Epoch 185/300, Loss: 0.1003 | 0.1055
Epoch 186/300, Loss: 0.0996 | 0.1054
Epoch 187/300, Loss: 0.0996 | 0.1049
Epoch 188/300, Loss: 0.1000 | 0.1051
Epoch 189/300, Loss: 0.1001 | 0.1051
Epoch 190/300, Loss: 0.0996 | 0.1048
Epoch 191/300, Loss: 0.1001 | 0.1051
Epoch 192/300, Loss: 0.1004 | 0.1050
Epoch 193/300, Loss: 0.0999 | 0.1051
Epoch 194/300, Loss: 0.1006 | 0.1052
Epoch 195/300, Loss: 0.1000 | 0.1054
Epoch 196/300, Loss: 0.0995 | 0.1053
Epoch 197/300, Loss: 0.0997 | 0.1051
Epoch 198/300, Loss: 0.0998 | 0.1052
Epoch 199/300, Loss: 0.0993 | 0.1053
Epoch 200/300, Loss: 0.0995 | 0.1051
Epoch 201/300, Loss: 0.0997 | 0.1051
Epoch 202/300, Loss: 0.0995 | 0.1052
Epoch 203/300, Loss: 0.1003 | 0.1048
Epoch 204/300, Loss: 0.0991 | 0.1048
Epoch 205/300, Loss: 0.1001 | 0.1051
Epoch 206/300, Loss: 0.0996 | 0.1051
Epoch 207/300, Loss: 0.0993 | 0.1049
Epoch 208/300, Loss: 0.0996 | 0.1049
Epoch 209/300, Loss: 0.0989 | 0.1048
Epoch 210/300, Loss: 0.0993 | 0.1048
Epoch 211/300, Loss: 0.0997 | 0.1048
Epoch 212/300, Loss: 0.0994 | 0.1047
Epoch 213/300, Loss: 0.0996 | 0.1046
Epoch 214/300, Loss: 0.0997 | 0.1046
Epoch 215/300, Loss: 0.0991 | 0.1044
Epoch 216/300, Loss: 0.0993 | 0.1044
Epoch 217/300, Loss: 0.0993 | 0.1043
Epoch 218/300, Loss: 0.0990 | 0.1043
Epoch 219/300, Loss: 0.0993 | 0.1043
Epoch 220/300, Loss: 0.0987 | 0.1044
Epoch 221/300, Loss: 0.1000 | 0.1045
Epoch 222/300, Loss: 0.0991 | 0.1045
Epoch 223/300, Loss: 0.0993 | 0.1044
Epoch 224/300, Loss: 0.0987 | 0.1043
Epoch 225/300, Loss: 0.0993 | 0.1042
Epoch 226/300, Loss: 0.0991 | 0.1043
Epoch 227/300, Loss: 0.0991 | 0.1042
Epoch 228/300, Loss: 0.0990 | 0.1043
Epoch 229/300, Loss: 0.0996 | 0.1044
Epoch 230/300, Loss: 0.0996 | 0.1044
Epoch 231/300, Loss: 0.0984 | 0.1044
Epoch 232/300, Loss: 0.0991 | 0.1044
Epoch 233/300, Loss: 0.0990 | 0.1044
Epoch 234/300, Loss: 0.0990 | 0.1044
Epoch 235/300, Loss: 0.0987 | 0.1044
Epoch 236/300, Loss: 0.0989 | 0.1044
Epoch 237/300, Loss: 0.0992 | 0.1044
Epoch 238/300, Loss: 0.0995 | 0.1044
Epoch 239/300, Loss: 0.0990 | 0.1043
Epoch 240/300, Loss: 0.0985 | 0.1043
Epoch 241/300, Loss: 0.0993 | 0.1042
Epoch 242/300, Loss: 0.0991 | 0.1042
Epoch 243/300, Loss: 0.0987 | 0.1043
Epoch 244/300, Loss: 0.0986 | 0.1043
Epoch 245/300, Loss: 0.0993 | 0.1043
Epoch 246/300, Loss: 0.0990 | 0.1043
Epoch 247/300, Loss: 0.0989 | 0.1044
Epoch 248/300, Loss: 0.0991 | 0.1043
Epoch 249/300, Loss: 0.0988 | 0.1044
Epoch 250/300, Loss: 0.0987 | 0.1043
Epoch 251/300, Loss: 0.0991 | 0.1043
Epoch 252/300, Loss: 0.0987 | 0.1043
Epoch 253/300, Loss: 0.0998 | 0.1044
Epoch 254/300, Loss: 0.0990 | 0.1043
Epoch 255/300, Loss: 0.0990 | 0.1043
Epoch 256/300, Loss: 0.0988 | 0.1042
Epoch 257/300, Loss: 0.0985 | 0.1042
Epoch 258/300, Loss: 0.0992 | 0.1042
Epoch 259/300, Loss: 0.0991 | 0.1042
Epoch 260/300, Loss: 0.0985 | 0.1041
Epoch 261/300, Loss: 0.0982 | 0.1042
Epoch 262/300, Loss: 0.0987 | 0.1041
Epoch 263/300, Loss: 0.0988 | 0.1041
Epoch 264/300, Loss: 0.0985 | 0.1041
Epoch 265/300, Loss: 0.0987 | 0.1042
Epoch 266/300, Loss: 0.0990 | 0.1042
Epoch 267/300, Loss: 0.0980 | 0.1041
Epoch 268/300, Loss: 0.0991 | 0.1040
Epoch 269/300, Loss: 0.0988 | 0.1040
Epoch 270/300, Loss: 0.0978 | 0.1040
Epoch 271/300, Loss: 0.0995 | 0.1040
Epoch 272/300, Loss: 0.0985 | 0.1040
Epoch 273/300, Loss: 0.0991 | 0.1039
Epoch 274/300, Loss: 0.0987 | 0.1039
Epoch 275/300, Loss: 0.0982 | 0.1039
Epoch 276/300, Loss: 0.0981 | 0.1040
Epoch 277/300, Loss: 0.0984 | 0.1040
Epoch 278/300, Loss: 0.0988 | 0.1040
Epoch 279/300, Loss: 0.0989 | 0.1040
Epoch 280/300, Loss: 0.0991 | 0.1040
Epoch 281/300, Loss: 0.0988 | 0.1040
Epoch 282/300, Loss: 0.0991 | 0.1039
Epoch 283/300, Loss: 0.0988 | 0.1040
Epoch 284/300, Loss: 0.0986 | 0.1040
Epoch 285/300, Loss: 0.0993 | 0.1040
Epoch 286/300, Loss: 0.0986 | 0.1040
Epoch 287/300, Loss: 0.0990 | 0.1040
Epoch 288/300, Loss: 0.0982 | 0.1040
Epoch 289/300, Loss: 0.0990 | 0.1040
Epoch 290/300, Loss: 0.0987 | 0.1040
Epoch 291/300, Loss: 0.0979 | 0.1040
Epoch 292/300, Loss: 0.0988 | 0.1040
Epoch 293/300, Loss: 0.0986 | 0.1040
Epoch 294/300, Loss: 0.0986 | 0.1040
Epoch 295/300, Loss: 0.0988 | 0.1040
Epoch 296/300, Loss: 0.0991 | 0.1040
Epoch 297/300, Loss: 0.0987 | 0.1040
Epoch 298/300, Loss: 0.0985 | 0.1040
Epoch 299/300, Loss: 0.0986 | 0.1040
Epoch 300/300, Loss: 0.0985 | 0.1040
Runtime (seconds): 606.8999090194702
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 992.6807107925415
RMSE: 31.5068359375
MAE: 31.5068359375
R-squared: nan
[198.75316]
