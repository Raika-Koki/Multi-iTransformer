ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-04 16:48:23,920][0m A new study created in memory with name: no-name-910b266e-589f-4bfb-94f6-fd5cdc19e268[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-02-04 16:48:48,249][0m Trial 0 finished with value: 0.8163903812544608 and parameters: {'observation_period_num': 214, 'train_rates': 0.6813228152417702, 'learning_rate': 2.5357087981426125e-05, 'batch_size': 249, 'step_size': 2, 'gamma': 0.8390894458363675}. Best is trial 0 with value: 0.8163903812544608.[0m
[32m[I 2025-02-04 16:49:48,270][0m Trial 1 finished with value: 0.47772858452193345 and parameters: {'observation_period_num': 234, 'train_rates': 0.7965662662629787, 'learning_rate': 4.504413635162051e-06, 'batch_size': 81, 'step_size': 12, 'gamma': 0.7882883608302327}. Best is trial 1 with value: 0.47772858452193345.[0m
[32m[I 2025-02-04 16:50:18,284][0m Trial 2 finished with value: 0.36423615775209794 and parameters: {'observation_period_num': 202, 'train_rates': 0.8291963839598807, 'learning_rate': 1.2028706824356866e-05, 'batch_size': 184, 'step_size': 10, 'gamma': 0.8885251342336542}. Best is trial 2 with value: 0.36423615775209794.[0m
[32m[I 2025-02-04 16:50:46,251][0m Trial 3 finished with value: 0.30450587469049817 and parameters: {'observation_period_num': 24, 'train_rates': 0.6274692395938782, 'learning_rate': 1.4945628210077257e-05, 'batch_size': 183, 'step_size': 14, 'gamma': 0.8013962637448845}. Best is trial 3 with value: 0.30450587469049817.[0m
[32m[I 2025-02-04 16:51:37,531][0m Trial 4 finished with value: 0.6925587926026198 and parameters: {'observation_period_num': 134, 'train_rates': 0.6475328855799068, 'learning_rate': 4.365052210896265e-06, 'batch_size': 87, 'step_size': 10, 'gamma': 0.7781924069009662}. Best is trial 3 with value: 0.30450587469049817.[0m
[32m[I 2025-02-04 16:53:30,424][0m Trial 5 finished with value: 0.1758043484141429 and parameters: {'observation_period_num': 223, 'train_rates': 0.8240053267038965, 'learning_rate': 6.788982984672819e-05, 'batch_size': 44, 'step_size': 7, 'gamma': 0.9544850720099791}. Best is trial 5 with value: 0.1758043484141429.[0m
[32m[I 2025-02-04 16:54:09,436][0m Trial 6 finished with value: 0.4836434095592822 and parameters: {'observation_period_num': 229, 'train_rates': 0.8266701826470746, 'learning_rate': 2.1066705217596005e-05, 'batch_size': 137, 'step_size': 4, 'gamma': 0.7884171305942935}. Best is trial 5 with value: 0.1758043484141429.[0m
[32m[I 2025-02-04 16:54:37,087][0m Trial 7 finished with value: 0.20585222200033101 and parameters: {'observation_period_num': 84, 'train_rates': 0.7039876905043011, 'learning_rate': 0.0005974767116783807, 'batch_size': 195, 'step_size': 15, 'gamma': 0.9235552476967706}. Best is trial 5 with value: 0.1758043484141429.[0m
[32m[I 2025-02-04 16:56:06,850][0m Trial 8 finished with value: 0.6694728041950025 and parameters: {'observation_period_num': 190, 'train_rates': 0.9521042318085182, 'learning_rate': 3.345913137693289e-06, 'batch_size': 63, 'step_size': 2, 'gamma': 0.9311331580615783}. Best is trial 5 with value: 0.1758043484141429.[0m
[32m[I 2025-02-04 16:56:42,563][0m Trial 9 finished with value: 0.20420386017368625 and parameters: {'observation_period_num': 37, 'train_rates': 0.7851194185207412, 'learning_rate': 5.5445204897589376e-05, 'batch_size': 157, 'step_size': 10, 'gamma': 0.8945394091221193}. Best is trial 5 with value: 0.1758043484141429.[0m
[32m[I 2025-02-04 17:00:31,272][0m Trial 10 finished with value: 0.14333585699399312 and parameters: {'observation_period_num': 156, 'train_rates': 0.9358444083291697, 'learning_rate': 0.00016810428356648196, 'batch_size': 24, 'step_size': 6, 'gamma': 0.9891241953990155}. Best is trial 10 with value: 0.14333585699399312.[0m
[32m[I 2025-02-04 17:05:43,709][0m Trial 11 finished with value: 0.09435269642959941 and parameters: {'observation_period_num': 150, 'train_rates': 0.9727394493298572, 'learning_rate': 0.00022161630906212976, 'batch_size': 18, 'step_size': 6, 'gamma': 0.9872862414293133}. Best is trial 11 with value: 0.09435269642959941.[0m
[32m[I 2025-02-04 17:09:49,062][0m Trial 12 finished with value: 0.10993826265136401 and parameters: {'observation_period_num': 144, 'train_rates': 0.9725417129173637, 'learning_rate': 0.0003163376760498984, 'batch_size': 23, 'step_size': 6, 'gamma': 0.9856389369604067}. Best is trial 11 with value: 0.09435269642959941.[0m
[32m[I 2025-02-04 17:14:52,335][0m Trial 13 finished with value: 0.119888299154891 and parameters: {'observation_period_num': 91, 'train_rates': 0.9032252766620672, 'learning_rate': 0.0008359306804395858, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9788789762980925}. Best is trial 11 with value: 0.09435269642959941.[0m
[32m[I 2025-02-04 17:15:54,505][0m Trial 14 finished with value: 0.15883398056030273 and parameters: {'observation_period_num': 167, 'train_rates': 0.97556875926274, 'learning_rate': 0.00021810294619102938, 'batch_size': 96, 'step_size': 8, 'gamma': 0.9487087813993578}. Best is trial 11 with value: 0.09435269642959941.[0m
[32m[I 2025-02-04 17:16:48,297][0m Trial 15 finished with value: 0.07669580298266848 and parameters: {'observation_period_num': 97, 'train_rates': 0.8971331269531986, 'learning_rate': 0.0002532559446852357, 'batch_size': 111, 'step_size': 4, 'gamma': 0.8494587556719864}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:17:35,854][0m Trial 16 finished with value: 0.100659400956031 and parameters: {'observation_period_num': 89, 'train_rates': 0.8861305264377481, 'learning_rate': 0.00010677216492746897, 'batch_size': 124, 'step_size': 4, 'gamma': 0.8411157063904371}. Best is trial 15 with value: 0.07669580298266848.[0m
Early stopping at epoch 72
[32m[I 2025-02-04 17:17:55,280][0m Trial 17 finished with value: 0.33331088479175125 and parameters: {'observation_period_num': 113, 'train_rates': 0.8789293718490238, 'learning_rate': 0.0003480328614107395, 'batch_size': 231, 'step_size': 1, 'gamma': 0.8379951103876299}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:18:47,811][0m Trial 18 finished with value: 0.13821163384074514 and parameters: {'observation_period_num': 57, 'train_rates': 0.9240934743592234, 'learning_rate': 5.950570347466066e-05, 'batch_size': 115, 'step_size': 3, 'gamma': 0.8705836626085032}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:20:17,596][0m Trial 19 finished with value: 0.14488738099324336 and parameters: {'observation_period_num': 118, 'train_rates': 0.8631815762742954, 'learning_rate': 0.0005209985961494067, 'batch_size': 61, 'step_size': 8, 'gamma': 0.8117872017453516}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:21:54,054][0m Trial 20 finished with value: 0.15994028162167237 and parameters: {'observation_period_num': 6, 'train_rates': 0.7410645819771515, 'learning_rate': 0.00013103552632366143, 'batch_size': 52, 'step_size': 5, 'gamma': 0.7551226230969352}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:22:40,743][0m Trial 21 finished with value: 0.12038970294526811 and parameters: {'observation_period_num': 89, 'train_rates': 0.893015685254355, 'learning_rate': 0.00010970511305967713, 'batch_size': 122, 'step_size': 4, 'gamma': 0.8372129771636554}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:23:19,574][0m Trial 22 finished with value: 1.2291571264025531 and parameters: {'observation_period_num': 106, 'train_rates': 0.861358940227681, 'learning_rate': 1.1555154185670375e-06, 'batch_size': 152, 'step_size': 4, 'gamma': 0.8640128239751478}. Best is trial 15 with value: 0.07669580298266848.[0m
[32m[I 2025-02-04 17:24:15,903][0m Trial 23 finished with value: 0.05645347386598587 and parameters: {'observation_period_num': 59, 'train_rates': 0.9891214047425024, 'learning_rate': 9.779145479994725e-05, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8607130363403074}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:25:20,474][0m Trial 24 finished with value: 0.10633868724107742 and parameters: {'observation_period_num': 65, 'train_rates': 0.9818645730208192, 'learning_rate': 0.0002649575027494591, 'batch_size': 97, 'step_size': 7, 'gamma': 0.8910112832393554}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:25:58,474][0m Trial 25 finished with value: 0.17480319738388062 and parameters: {'observation_period_num': 57, 'train_rates': 0.9457303021375082, 'learning_rate': 4.250450528297364e-05, 'batch_size': 166, 'step_size': 6, 'gamma': 0.8635419783142257}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:27:19,412][0m Trial 26 finished with value: 0.1605178564786911 and parameters: {'observation_period_num': 178, 'train_rates': 0.9878929446641409, 'learning_rate': 8.315478275533186e-05, 'batch_size': 73, 'step_size': 9, 'gamma': 0.9163303650467989}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:28:16,404][0m Trial 27 finished with value: 0.12018563974220141 and parameters: {'observation_period_num': 139, 'train_rates': 0.9198842911461595, 'learning_rate': 0.00038658411374147765, 'batch_size': 103, 'step_size': 7, 'gamma': 0.8113248789131468}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:29:00,306][0m Trial 28 finished with value: 0.11747398972511292 and parameters: {'observation_period_num': 70, 'train_rates': 0.9560086126012138, 'learning_rate': 0.00017999954231352893, 'batch_size': 140, 'step_size': 2, 'gamma': 0.9070612911833359}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:29:25,999][0m Trial 29 finished with value: 0.3414742052555084 and parameters: {'observation_period_num': 109, 'train_rates': 0.9204014989560666, 'learning_rate': 3.0491805599209693e-05, 'batch_size': 241, 'step_size': 5, 'gamma': 0.8501003297576877}. Best is trial 23 with value: 0.05645347386598587.[0m
Early stopping at epoch 71
[32m[I 2025-02-04 17:29:49,384][0m Trial 30 finished with value: 0.1572476327419281 and parameters: {'observation_period_num': 29, 'train_rates': 0.9588907222740123, 'learning_rate': 0.0008134116413265524, 'batch_size': 213, 'step_size': 1, 'gamma': 0.8268517748331794}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:30:38,332][0m Trial 31 finished with value: 0.12124180671286909 and parameters: {'observation_period_num': 83, 'train_rates': 0.898213848205128, 'learning_rate': 0.00010583338271854914, 'batch_size': 121, 'step_size': 3, 'gamma': 0.8527676550170086}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:31:33,071][0m Trial 32 finished with value: 0.14009273052215576 and parameters: {'observation_period_num': 125, 'train_rates': 0.9890298463107268, 'learning_rate': 0.00015323609413968775, 'batch_size': 113, 'step_size': 4, 'gamma': 0.826231245629219}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:32:41,900][0m Trial 33 finished with value: 0.06464931144433864 and parameters: {'observation_period_num': 45, 'train_rates': 0.853987365448366, 'learning_rate': 9.124094092197754e-05, 'batch_size': 82, 'step_size': 3, 'gamma': 0.875988561941449}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:34:46,597][0m Trial 34 finished with value: 0.1953726635031078 and parameters: {'observation_period_num': 247, 'train_rates': 0.7875313275326408, 'learning_rate': 4.360991725708417e-05, 'batch_size': 38, 'step_size': 3, 'gamma': 0.8766274983309381}. Best is trial 23 with value: 0.05645347386598587.[0m
[32m[I 2025-02-04 17:35:55,395][0m Trial 35 finished with value: 0.05109193339012563 and parameters: {'observation_period_num': 37, 'train_rates': 0.851940118341153, 'learning_rate': 0.00021925908923878715, 'batch_size': 81, 'step_size': 6, 'gamma': 0.9440061571342352}. Best is trial 35 with value: 0.05109193339012563.[0m
[32m[I 2025-02-04 17:37:03,712][0m Trial 36 finished with value: 0.11364392503724347 and parameters: {'observation_period_num': 43, 'train_rates': 0.8484029042564265, 'learning_rate': 1.0245768251891395e-05, 'batch_size': 82, 'step_size': 11, 'gamma': 0.8835223512696319}. Best is trial 35 with value: 0.05109193339012563.[0m
[32m[I 2025-02-04 17:38:17,023][0m Trial 37 finished with value: 0.029972630719605244 and parameters: {'observation_period_num': 5, 'train_rates': 0.8066113564404266, 'learning_rate': 0.000462769100818871, 'batch_size': 74, 'step_size': 5, 'gamma': 0.9624949595038581}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:39:31,382][0m Trial 38 finished with value: 0.03295264797512516 and parameters: {'observation_period_num': 8, 'train_rates': 0.8135173994613971, 'learning_rate': 0.0004814592878112197, 'batch_size': 74, 'step_size': 13, 'gamma': 0.9594587812001112}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:40:49,181][0m Trial 39 finished with value: 0.16768009041674886 and parameters: {'observation_period_num': 7, 'train_rates': 0.7642834872527857, 'learning_rate': 0.0005077884292712064, 'batch_size': 68, 'step_size': 13, 'gamma': 0.9644965135460163}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:42:48,721][0m Trial 40 finished with value: 0.1590562499480544 and parameters: {'observation_period_num': 16, 'train_rates': 0.7370419630729482, 'learning_rate': 0.000901836300369997, 'batch_size': 42, 'step_size': 15, 'gamma': 0.9403399761046021}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:43:52,577][0m Trial 41 finished with value: 0.0712544885142283 and parameters: {'observation_period_num': 40, 'train_rates': 0.8110985756054436, 'learning_rate': 0.0004198746722490646, 'batch_size': 86, 'step_size': 8, 'gamma': 0.9675568403166209}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:45:06,761][0m Trial 42 finished with value: 0.11303093813751873 and parameters: {'observation_period_num': 22, 'train_rates': 0.8378298341334662, 'learning_rate': 0.0006036962753185435, 'batch_size': 75, 'step_size': 13, 'gamma': 0.9595351776842219}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:46:36,924][0m Trial 43 finished with value: 0.06958584666142165 and parameters: {'observation_period_num': 48, 'train_rates': 0.8134460229575179, 'learning_rate': 1.9167836056667027e-05, 'batch_size': 59, 'step_size': 7, 'gamma': 0.9374935124821618}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:47:36,059][0m Trial 44 finished with value: 0.17216152287697586 and parameters: {'observation_period_num': 27, 'train_rates': 0.7626248899992764, 'learning_rate': 8.044920940067482e-05, 'batch_size': 88, 'step_size': 9, 'gamma': 0.9712038261349685}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:49:21,612][0m Trial 45 finished with value: 0.03388666049303377 and parameters: {'observation_period_num': 15, 'train_rates': 0.8033618718944786, 'learning_rate': 0.00018378875477239356, 'batch_size': 51, 'step_size': 5, 'gamma': 0.9200426732030961}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:51:36,057][0m Trial 46 finished with value: 0.1370773373380927 and parameters: {'observation_period_num': 16, 'train_rates': 0.6907471839481856, 'learning_rate': 0.0001994620213506522, 'batch_size': 35, 'step_size': 5, 'gamma': 0.9049483139523106}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:53:04,278][0m Trial 47 finished with value: 0.1840031874350837 and parameters: {'observation_period_num': 32, 'train_rates': 0.6448464493927453, 'learning_rate': 0.0003289807529993936, 'batch_size': 51, 'step_size': 6, 'gamma': 0.9258268881770437}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:54:00,908][0m Trial 48 finished with value: 0.05794871998582157 and parameters: {'observation_period_num': 17, 'train_rates': 0.8020629247923061, 'learning_rate': 0.0006909859408442112, 'batch_size': 97, 'step_size': 9, 'gamma': 0.9512137285666787}. Best is trial 37 with value: 0.029972630719605244.[0m
[32m[I 2025-02-04 17:56:44,750][0m Trial 49 finished with value: 0.1577351830004162 and parameters: {'observation_period_num': 5, 'train_rates': 0.761580545162028, 'learning_rate': 0.00027270350203989534, 'batch_size': 31, 'step_size': 5, 'gamma': 0.943209173757073}. Best is trial 37 with value: 0.029972630719605244.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-04 17:56:44,761][0m A new study created in memory with name: no-name-24e9ef95-d8b6-4467-bfd3-c8aa15f670dc[0m
[32m[I 2025-02-04 17:58:28,765][0m Trial 0 finished with value: 0.7228800612132639 and parameters: {'observation_period_num': 133, 'train_rates': 0.7447704569362434, 'learning_rate': 2.034529536943029e-06, 'batch_size': 46, 'step_size': 4, 'gamma': 0.9154498754295296}. Best is trial 0 with value: 0.7228800612132639.[0m
[32m[I 2025-02-04 17:59:49,118][0m Trial 1 finished with value: 0.28985030813650653 and parameters: {'observation_period_num': 112, 'train_rates': 0.9537468794290864, 'learning_rate': 1.7488870086265812e-05, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8622149425730686}. Best is trial 1 with value: 0.28985030813650653.[0m
[32m[I 2025-02-04 18:00:24,423][0m Trial 2 finished with value: 0.2799083888530731 and parameters: {'observation_period_num': 98, 'train_rates': 0.948286327713001, 'learning_rate': 7.574092857117646e-05, 'batch_size': 176, 'step_size': 8, 'gamma': 0.7681366721461779}. Best is trial 2 with value: 0.2799083888530731.[0m
[32m[I 2025-02-04 18:03:30,800][0m Trial 3 finished with value: 0.5526930742313207 and parameters: {'observation_period_num': 154, 'train_rates': 0.862048217055474, 'learning_rate': 1.9400052267204885e-06, 'batch_size': 28, 'step_size': 7, 'gamma': 0.8340493993788454}. Best is trial 2 with value: 0.2799083888530731.[0m
[32m[I 2025-02-04 18:04:22,511][0m Trial 4 finished with value: 0.5879811644554138 and parameters: {'observation_period_num': 75, 'train_rates': 0.9708531221004655, 'learning_rate': 5.4953087194135474e-06, 'batch_size': 117, 'step_size': 8, 'gamma': 0.881015242386092}. Best is trial 2 with value: 0.2799083888530731.[0m
[32m[I 2025-02-04 18:06:54,784][0m Trial 5 finished with value: 0.1708540687065446 and parameters: {'observation_period_num': 232, 'train_rates': 0.8692011408832362, 'learning_rate': 1.2444531461179698e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.9411639641037637}. Best is trial 5 with value: 0.1708540687065446.[0m
[32m[I 2025-02-04 18:07:28,450][0m Trial 6 finished with value: 0.19413325209406357 and parameters: {'observation_period_num': 68, 'train_rates': 0.6986743817102361, 'learning_rate': 0.0004940948336452404, 'batch_size': 154, 'step_size': 7, 'gamma': 0.9558723405651357}. Best is trial 5 with value: 0.1708540687065446.[0m
[32m[I 2025-02-04 18:08:15,277][0m Trial 7 finished with value: 0.9343718555238512 and parameters: {'observation_period_num': 196, 'train_rates': 0.6505876770755987, 'learning_rate': 1.5456833043254788e-06, 'batch_size': 98, 'step_size': 11, 'gamma': 0.7533368711641304}. Best is trial 5 with value: 0.1708540687065446.[0m
[32m[I 2025-02-04 18:08:43,987][0m Trial 8 finished with value: 0.8784744712315737 and parameters: {'observation_period_num': 151, 'train_rates': 0.7653242492593604, 'learning_rate': 1.8753524223153593e-06, 'batch_size': 179, 'step_size': 10, 'gamma': 0.9287371921252332}. Best is trial 5 with value: 0.1708540687065446.[0m
[32m[I 2025-02-04 18:09:48,266][0m Trial 9 finished with value: 0.22896630506394272 and parameters: {'observation_period_num': 188, 'train_rates': 0.9574169356352784, 'learning_rate': 0.00014075429454103285, 'batch_size': 90, 'step_size': 9, 'gamma': 0.8767682647181311}. Best is trial 5 with value: 0.1708540687065446.[0m
[32m[I 2025-02-04 18:10:14,520][0m Trial 10 finished with value: 0.3720085345781766 and parameters: {'observation_period_num': 247, 'train_rates': 0.8608810870822928, 'learning_rate': 1.5842970310871634e-05, 'batch_size': 232, 'step_size': 15, 'gamma': 0.9892440968777528}. Best is trial 5 with value: 0.1708540687065446.[0m
[32m[I 2025-02-04 18:10:45,638][0m Trial 11 finished with value: 0.13064154735658057 and parameters: {'observation_period_num': 11, 'train_rates': 0.6198193474237397, 'learning_rate': 0.0005893972021293694, 'batch_size': 157, 'step_size': 1, 'gamma': 0.9892712507976238}. Best is trial 11 with value: 0.13064154735658057.[0m
[32m[I 2025-02-04 18:11:08,824][0m Trial 12 finished with value: 0.12737355141840204 and parameters: {'observation_period_num': 12, 'train_rates': 0.6267623192922835, 'learning_rate': 0.0008904132211759636, 'batch_size': 223, 'step_size': 1, 'gamma': 0.9893219846898791}. Best is trial 12 with value: 0.12737355141840204.[0m
[32m[I 2025-02-04 18:11:30,322][0m Trial 13 finished with value: 0.1329078513015177 and parameters: {'observation_period_num': 13, 'train_rates': 0.6037887263674222, 'learning_rate': 0.000893913674283084, 'batch_size': 248, 'step_size': 1, 'gamma': 0.9885214083565644}. Best is trial 12 with value: 0.12737355141840204.[0m
[32m[I 2025-02-04 18:11:54,479][0m Trial 14 finished with value: 0.14842646871241733 and parameters: {'observation_period_num': 25, 'train_rates': 0.6125114518068879, 'learning_rate': 0.0003163050650588051, 'batch_size': 208, 'step_size': 1, 'gamma': 0.9671233667381326}. Best is trial 12 with value: 0.12737355141840204.[0m
[32m[I 2025-02-04 18:12:19,531][0m Trial 15 finished with value: 0.24496453381549924 and parameters: {'observation_period_num': 44, 'train_rates': 0.6729218331538945, 'learning_rate': 0.00015467833833192563, 'batch_size': 214, 'step_size': 3, 'gamma': 0.807817809814794}. Best is trial 12 with value: 0.12737355141840204.[0m
[32m[I 2025-02-04 18:12:55,495][0m Trial 16 finished with value: 0.14037246391428826 and parameters: {'observation_period_num': 8, 'train_rates': 0.7183063145089064, 'learning_rate': 0.0009592056090021513, 'batch_size': 142, 'step_size': 4, 'gamma': 0.9080011262954956}. Best is trial 12 with value: 0.12737355141840204.[0m
[32m[I 2025-02-04 18:13:26,228][0m Trial 17 finished with value: 0.09881882350690098 and parameters: {'observation_period_num': 49, 'train_rates': 0.8091393300983213, 'learning_rate': 5.0857203922957206e-05, 'batch_size': 182, 'step_size': 2, 'gamma': 0.967174602176259}. Best is trial 17 with value: 0.09881882350690098.[0m
[32m[I 2025-02-04 18:13:54,621][0m Trial 18 finished with value: 0.09996441794478375 and parameters: {'observation_period_num': 53, 'train_rates': 0.8016879004003261, 'learning_rate': 5.385146955148814e-05, 'batch_size': 196, 'step_size': 3, 'gamma': 0.9578530033007457}. Best is trial 17 with value: 0.09881882350690098.[0m
[32m[I 2025-02-04 18:14:25,458][0m Trial 19 finished with value: 0.1951368467153114 and parameters: {'observation_period_num': 59, 'train_rates': 0.8137693719089505, 'learning_rate': 4.29891853185181e-05, 'batch_size': 190, 'step_size': 3, 'gamma': 0.9001082487916037}. Best is trial 17 with value: 0.09881882350690098.[0m
[32m[I 2025-02-04 18:15:12,210][0m Trial 20 finished with value: 0.11178841748742308 and parameters: {'observation_period_num': 93, 'train_rates': 0.8172946296915453, 'learning_rate': 3.979706966266284e-05, 'batch_size': 121, 'step_size': 5, 'gamma': 0.950679677842795}. Best is trial 17 with value: 0.09881882350690098.[0m
[32m[I 2025-02-04 18:15:56,258][0m Trial 21 finished with value: 0.11497293947905356 and parameters: {'observation_period_num': 92, 'train_rates': 0.8038403479041222, 'learning_rate': 4.0456893176325415e-05, 'batch_size': 127, 'step_size': 5, 'gamma': 0.9581805095435529}. Best is trial 17 with value: 0.09881882350690098.[0m
[32m[I 2025-02-04 18:16:28,304][0m Trial 22 finished with value: 0.09866157390854575 and parameters: {'observation_period_num': 42, 'train_rates': 0.9056311350281225, 'learning_rate': 7.156445829606404e-05, 'batch_size': 190, 'step_size': 3, 'gamma': 0.9401989127693441}. Best is trial 22 with value: 0.09866157390854575.[0m
[32m[I 2025-02-04 18:17:01,263][0m Trial 23 finished with value: 0.08401545530469001 and parameters: {'observation_period_num': 38, 'train_rates': 0.9077744995139287, 'learning_rate': 9.71876261306732e-05, 'batch_size': 191, 'step_size': 3, 'gamma': 0.9335706279835937}. Best is trial 23 with value: 0.08401545530469001.[0m
[32m[I 2025-02-04 18:17:37,085][0m Trial 24 finished with value: 0.08013110706473098 and parameters: {'observation_period_num': 34, 'train_rates': 0.9069802060944328, 'learning_rate': 0.00010778006271376429, 'batch_size': 167, 'step_size': 2, 'gamma': 0.9291298126380754}. Best is trial 24 with value: 0.08013110706473098.[0m
[32m[I 2025-02-04 18:18:16,105][0m Trial 25 finished with value: 0.06263261040958175 and parameters: {'observation_period_num': 34, 'train_rates': 0.9061543459048725, 'learning_rate': 0.00014312805026905304, 'batch_size': 161, 'step_size': 13, 'gamma': 0.925748096320376}. Best is trial 25 with value: 0.06263261040958175.[0m
[32m[I 2025-02-04 18:18:54,502][0m Trial 26 finished with value: 0.04744065544418706 and parameters: {'observation_period_num': 33, 'train_rates': 0.9102342478004158, 'learning_rate': 0.00016518031327251274, 'batch_size': 162, 'step_size': 13, 'gamma': 0.9233643177205464}. Best is trial 26 with value: 0.04744065544418706.[0m
[32m[I 2025-02-04 18:19:32,614][0m Trial 27 finished with value: 0.12979407320099492 and parameters: {'observation_period_num': 80, 'train_rates': 0.9138714892112585, 'learning_rate': 0.00023205777516489853, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8929692139788594}. Best is trial 26 with value: 0.04744065544418706.[0m
[32m[I 2025-02-04 18:20:15,333][0m Trial 28 finished with value: 0.04266528661052386 and parameters: {'observation_period_num': 28, 'train_rates': 0.8853148237138361, 'learning_rate': 0.00024008077702103117, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8450913418238379}. Best is trial 28 with value: 0.04266528661052386.[0m
[32m[I 2025-02-04 18:20:58,887][0m Trial 29 finished with value: 0.12904423475265503 and parameters: {'observation_period_num': 127, 'train_rates': 0.989733272146982, 'learning_rate': 0.000292866067621213, 'batch_size': 140, 'step_size': 13, 'gamma': 0.8509361438482318}. Best is trial 28 with value: 0.04266528661052386.[0m
[32m[I 2025-02-04 18:22:29,025][0m Trial 30 finished with value: 0.04197536866579737 and parameters: {'observation_period_num': 27, 'train_rates': 0.8446586939932683, 'learning_rate': 0.0001990155665629602, 'batch_size': 61, 'step_size': 15, 'gamma': 0.8183002028573532}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:24:00,422][0m Trial 31 finished with value: 0.05785182074300552 and parameters: {'observation_period_num': 27, 'train_rates': 0.8443247679570576, 'learning_rate': 0.00019480839905256962, 'batch_size': 60, 'step_size': 15, 'gamma': 0.808827611484651}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:25:33,020][0m Trial 32 finished with value: 0.04569986494281233 and parameters: {'observation_period_num': 22, 'train_rates': 0.8355071288208508, 'learning_rate': 0.00020794892924652379, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8078432783596581}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:26:52,488][0m Trial 33 finished with value: 0.2287455558742521 and parameters: {'observation_period_num': 63, 'train_rates': 0.7745135749469124, 'learning_rate': 0.0004862100463665292, 'batch_size': 65, 'step_size': 14, 'gamma': 0.8066398535282874}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:28:46,728][0m Trial 34 finished with value: 0.05871513632615817 and parameters: {'observation_period_num': 22, 'train_rates': 0.8345799448544869, 'learning_rate': 2.3622904846263047e-05, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8286181096789141}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:33:41,748][0m Trial 35 finished with value: 0.18085874783757486 and parameters: {'observation_period_num': 106, 'train_rates': 0.8800627174167517, 'learning_rate': 0.00044382837672917857, 'batch_size': 18, 'step_size': 14, 'gamma': 0.7858228303273194}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:34:50,333][0m Trial 36 finished with value: 0.1307469731098727 and parameters: {'observation_period_num': 76, 'train_rates': 0.9341092852426869, 'learning_rate': 0.00031537240133832283, 'batch_size': 86, 'step_size': 14, 'gamma': 0.8567761172003477}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:35:43,835][0m Trial 37 finished with value: 0.47187497971662834 and parameters: {'observation_period_num': 117, 'train_rates': 0.8818260367702394, 'learning_rate': 5.1004945664870496e-06, 'batch_size': 107, 'step_size': 12, 'gamma': 0.834460390400527}. Best is trial 30 with value: 0.04197536866579737.[0m
[32m[I 2025-02-04 18:37:35,789][0m Trial 38 finished with value: 0.038302565188105424 and parameters: {'observation_period_num': 20, 'train_rates': 0.8402558749585078, 'learning_rate': 8.202696525359993e-05, 'batch_size': 49, 'step_size': 15, 'gamma': 0.7866423484976568}. Best is trial 38 with value: 0.038302565188105424.[0m
[32m[I 2025-02-04 18:39:24,002][0m Trial 39 finished with value: 0.16632119638975276 and parameters: {'observation_period_num': 6, 'train_rates': 0.7809702836626307, 'learning_rate': 8.121629386024156e-05, 'batch_size': 48, 'step_size': 15, 'gamma': 0.7848444430452012}. Best is trial 38 with value: 0.038302565188105424.[0m
[32m[I 2025-02-04 18:41:55,054][0m Trial 40 finished with value: 0.19384197906568765 and parameters: {'observation_period_num': 55, 'train_rates': 0.7396673801905579, 'learning_rate': 0.00024429442737645344, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7880268844562832}. Best is trial 38 with value: 0.038302565188105424.[0m
[32m[I 2025-02-04 18:43:14,799][0m Trial 41 finished with value: 0.03957110243382785 and parameters: {'observation_period_num': 21, 'train_rates': 0.842850848844869, 'learning_rate': 0.00015861748943782502, 'batch_size': 69, 'step_size': 14, 'gamma': 0.8208195909781337}. Best is trial 38 with value: 0.038302565188105424.[0m
[32m[I 2025-02-04 18:44:28,721][0m Trial 42 finished with value: 0.040246086327886385 and parameters: {'observation_period_num': 20, 'train_rates': 0.832709243689433, 'learning_rate': 0.0001140404288306001, 'batch_size': 76, 'step_size': 15, 'gamma': 0.8232845596354702}. Best is trial 38 with value: 0.038302565188105424.[0m
[32m[I 2025-02-04 18:45:42,266][0m Trial 43 finished with value: 0.03752102601257237 and parameters: {'observation_period_num': 17, 'train_rates': 0.853881813561671, 'learning_rate': 0.00011394985862486846, 'batch_size': 78, 'step_size': 14, 'gamma': 0.8260710605797589}. Best is trial 43 with value: 0.03752102601257237.[0m
[32m[I 2025-02-04 18:46:53,293][0m Trial 44 finished with value: 0.03585876647153454 and parameters: {'observation_period_num': 18, 'train_rates': 0.8562445268001535, 'learning_rate': 0.00010988897907479964, 'batch_size': 79, 'step_size': 14, 'gamma': 0.8218530921316765}. Best is trial 44 with value: 0.03585876647153454.[0m
[32m[I 2025-02-04 18:47:57,079][0m Trial 45 finished with value: 0.17750178860166135 and parameters: {'observation_period_num': 151, 'train_rates': 0.8609152038363574, 'learning_rate': 2.6169848814792047e-05, 'batch_size': 85, 'step_size': 14, 'gamma': 0.7682579894830512}. Best is trial 44 with value: 0.03585876647153454.[0m
[32m[I 2025-02-04 18:49:10,210][0m Trial 46 finished with value: 0.03364436742963224 and parameters: {'observation_period_num': 5, 'train_rates': 0.833003555552845, 'learning_rate': 0.00011521760220635876, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8226047512653859}. Best is trial 46 with value: 0.03364436742963224.[0m
[32m[I 2025-02-04 18:50:00,285][0m Trial 47 finished with value: 0.1857265712072452 and parameters: {'observation_period_num': 15, 'train_rates': 0.7855590318543153, 'learning_rate': 6.807189266814386e-05, 'batch_size': 105, 'step_size': 12, 'gamma': 0.7954623295925276}. Best is trial 46 with value: 0.03364436742963224.[0m
[32m[I 2025-02-04 18:51:06,864][0m Trial 48 finished with value: 0.2639641865315261 and parameters: {'observation_period_num': 169, 'train_rates': 0.7573763177998002, 'learning_rate': 0.00011945168995484229, 'batch_size': 73, 'step_size': 10, 'gamma': 0.866101942832834}. Best is trial 46 with value: 0.03364436742963224.[0m
[32m[I 2025-02-04 18:52:06,247][0m Trial 49 finished with value: 0.10552623194806716 and parameters: {'observation_period_num': 6, 'train_rates': 0.8558211368808937, 'learning_rate': 7.492679813245238e-06, 'batch_size': 94, 'step_size': 11, 'gamma': 0.8411956159485668}. Best is trial 46 with value: 0.03364436742963224.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-04 18:52:06,257][0m A new study created in memory with name: no-name-11385a72-ca03-45d4-b594-fa779931e928[0m
[32m[I 2025-02-04 18:55:15,824][0m Trial 0 finished with value: 0.19403145185715603 and parameters: {'observation_period_num': 196, 'train_rates': 0.8691288290160245, 'learning_rate': 0.00026458257063452186, 'batch_size': 27, 'step_size': 7, 'gamma': 0.9216435486917609}. Best is trial 0 with value: 0.19403145185715603.[0m
[32m[I 2025-02-04 18:57:25,423][0m Trial 1 finished with value: 0.2146863965943122 and parameters: {'observation_period_num': 97, 'train_rates': 0.6244001176311895, 'learning_rate': 0.00016671480130891388, 'batch_size': 33, 'step_size': 4, 'gamma': 0.8949319525177406}. Best is trial 0 with value: 0.19403145185715603.[0m
[32m[I 2025-02-04 18:58:04,116][0m Trial 2 finished with value: 0.07073684336873592 and parameters: {'observation_period_num': 82, 'train_rates': 0.8529480455844443, 'learning_rate': 0.00011107154041690542, 'batch_size': 150, 'step_size': 6, 'gamma': 0.8990552158779817}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 18:58:34,093][0m Trial 3 finished with value: 0.395912348125234 and parameters: {'observation_period_num': 78, 'train_rates': 0.6885203495347388, 'learning_rate': 1.8878439943990646e-05, 'batch_size': 174, 'step_size': 9, 'gamma': 0.7956953737623536}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 18:59:24,116][0m Trial 4 finished with value: 0.1862625797387912 and parameters: {'observation_period_num': 196, 'train_rates': 0.928681301141934, 'learning_rate': 3.17329476430601e-05, 'batch_size': 112, 'step_size': 13, 'gamma': 0.8722181862630596}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 18:59:46,732][0m Trial 5 finished with value: 0.1947335984899802 and parameters: {'observation_period_num': 55, 'train_rates': 0.7140284839657778, 'learning_rate': 0.00019125204654615993, 'batch_size': 238, 'step_size': 12, 'gamma': 0.7823286132138404}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:00:30,602][0m Trial 6 finished with value: 0.25650956456003515 and parameters: {'observation_period_num': 109, 'train_rates': 0.6940794080658674, 'learning_rate': 8.74256092399114e-05, 'batch_size': 110, 'step_size': 4, 'gamma': 0.8770067691036719}. Best is trial 2 with value: 0.07073684336873592.[0m
Early stopping at epoch 48
[32m[I 2025-02-04 19:00:46,659][0m Trial 7 finished with value: 1.3349831656066984 and parameters: {'observation_period_num': 93, 'train_rates': 0.6371290302400583, 'learning_rate': 2.3597708108751483e-05, 'batch_size': 154, 'step_size': 1, 'gamma': 0.7766777947889734}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:02:21,351][0m Trial 8 finished with value: 0.33244244167707626 and parameters: {'observation_period_num': 217, 'train_rates': 0.7577150828755436, 'learning_rate': 5.570313869218132e-05, 'batch_size': 50, 'step_size': 5, 'gamma': 0.9530653030879681}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:02:50,985][0m Trial 9 finished with value: 0.6504652500152588 and parameters: {'observation_period_num': 175, 'train_rates': 0.9358944486374747, 'learning_rate': 1.7698737123119076e-05, 'batch_size': 208, 'step_size': 4, 'gamma': 0.8539506226307834}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:03:59,901][0m Trial 10 finished with value: 0.36611683152366203 and parameters: {'observation_period_num': 38, 'train_rates': 0.8259388807117, 'learning_rate': 2.138045913265475e-06, 'batch_size': 80, 'step_size': 10, 'gamma': 0.966278121065059}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:04:53,782][0m Trial 11 finished with value: 0.579167366027832 and parameters: {'observation_period_num': 147, 'train_rates': 0.9817815784367911, 'learning_rate': 5.239069034124122e-06, 'batch_size': 111, 'step_size': 15, 'gamma': 0.8444503067436603}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:05:36,494][0m Trial 12 finished with value: 0.4982435782344974 and parameters: {'observation_period_num': 244, 'train_rates': 0.8914770260917263, 'learning_rate': 6.560753117430823e-06, 'batch_size': 131, 'step_size': 15, 'gamma': 0.828468369907314}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:06:06,733][0m Trial 13 finished with value: 0.15762943728912346 and parameters: {'observation_period_num': 157, 'train_rates': 0.8360349701261174, 'learning_rate': 0.0006482985167905012, 'batch_size': 185, 'step_size': 12, 'gamma': 0.9122440732854542}. Best is trial 2 with value: 0.07073684336873592.[0m
[32m[I 2025-02-04 19:06:36,785][0m Trial 14 finished with value: 0.03248429191117784 and parameters: {'observation_period_num': 8, 'train_rates': 0.8077176856847992, 'learning_rate': 0.0009873076861967231, 'batch_size': 197, 'step_size': 7, 'gamma': 0.9260768786395144}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:07:02,783][0m Trial 15 finished with value: 0.18313778537285602 and parameters: {'observation_period_num': 7, 'train_rates': 0.7942223724214887, 'learning_rate': 0.0005745291864797876, 'batch_size': 229, 'step_size': 7, 'gamma': 0.9845883976894739}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:07:32,209][0m Trial 16 finished with value: 0.16608719683688297 and parameters: {'observation_period_num': 7, 'train_rates': 0.7598590106388392, 'learning_rate': 0.0008860917304730538, 'batch_size': 198, 'step_size': 2, 'gamma': 0.936519181021026}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:07:56,670][0m Trial 17 finished with value: 0.0739010757742784 and parameters: {'observation_period_num': 46, 'train_rates': 0.79870299781984, 'learning_rate': 0.0003458400852255053, 'batch_size': 255, 'step_size': 6, 'gamma': 0.8987056679938231}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:08:33,164][0m Trial 18 finished with value: 0.09791988225086876 and parameters: {'observation_period_num': 127, 'train_rates': 0.8780379571147797, 'learning_rate': 0.00016571500104641895, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8154208721952352}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:09:09,087][0m Trial 19 finished with value: 0.2219790301293215 and parameters: {'observation_period_num': 68, 'train_rates': 0.7428386358197587, 'learning_rate': 8.706264116461316e-05, 'batch_size': 143, 'step_size': 8, 'gamma': 0.9408975530802148}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:09:36,993][0m Trial 20 finished with value: 0.06534820999248755 and parameters: {'observation_period_num': 34, 'train_rates': 0.8363531168001497, 'learning_rate': 0.00041002604182439667, 'batch_size': 213, 'step_size': 2, 'gamma': 0.8900189327006014}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:10:05,843][0m Trial 21 finished with value: 0.06390718669483536 and parameters: {'observation_period_num': 31, 'train_rates': 0.8375578262753486, 'learning_rate': 0.0004236250499457813, 'batch_size': 213, 'step_size': 2, 'gamma': 0.8926506640163826}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:10:32,748][0m Trial 22 finished with value: 0.20364560027917225 and parameters: {'observation_period_num': 21, 'train_rates': 0.7954614133848961, 'learning_rate': 0.00044833408333823084, 'batch_size': 221, 'step_size': 2, 'gamma': 0.8803604285328627}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:11:05,277][0m Trial 23 finished with value: 0.045748029738664625 and parameters: {'observation_period_num': 32, 'train_rates': 0.9145210809940137, 'learning_rate': 0.000943270986567187, 'batch_size': 198, 'step_size': 2, 'gamma': 0.9191479007339818}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:11:37,334][0m Trial 24 finished with value: 0.053511213777320724 and parameters: {'observation_period_num': 26, 'train_rates': 0.9282170026246366, 'learning_rate': 0.0009695575707939813, 'batch_size': 195, 'step_size': 3, 'gamma': 0.9230208926352113}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:12:10,608][0m Trial 25 finished with value: 0.06548945346461518 and parameters: {'observation_period_num': 60, 'train_rates': 0.921635688449609, 'learning_rate': 0.0009640159114530513, 'batch_size': 185, 'step_size': 3, 'gamma': 0.9263671131981979}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:12:38,251][0m Trial 26 finished with value: 0.043128423392772675 and parameters: {'observation_period_num': 5, 'train_rates': 0.9837752869145626, 'learning_rate': 0.000932446826020704, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9627805362114548}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:13:05,880][0m Trial 27 finished with value: 0.04341572895646095 and parameters: {'observation_period_num': 7, 'train_rates': 0.9847648246987891, 'learning_rate': 0.0003105374664302277, 'batch_size': 254, 'step_size': 1, 'gamma': 0.9858647743512492}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:13:33,093][0m Trial 28 finished with value: 0.04158753156661987 and parameters: {'observation_period_num': 5, 'train_rates': 0.9819801223770122, 'learning_rate': 0.0005621703578402389, 'batch_size': 255, 'step_size': 1, 'gamma': 0.9876028573007307}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:14:00,461][0m Trial 29 finished with value: 0.040108826011419296 and parameters: {'observation_period_num': 19, 'train_rates': 0.9578269523708047, 'learning_rate': 0.0002563348542472242, 'batch_size': 240, 'step_size': 5, 'gamma': 0.9665685780314566}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:14:27,410][0m Trial 30 finished with value: 0.1197786033153534 and parameters: {'observation_period_num': 49, 'train_rates': 0.949368557280406, 'learning_rate': 0.0002436603152939489, 'batch_size': 238, 'step_size': 6, 'gamma': 0.9705403400984468}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:14:54,599][0m Trial 31 finished with value: 0.061919957399368286 and parameters: {'observation_period_num': 18, 'train_rates': 0.9645159338332955, 'learning_rate': 0.0006299647848761731, 'batch_size': 242, 'step_size': 7, 'gamma': 0.9597163858981547}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:15:24,181][0m Trial 32 finished with value: 0.04282359778881073 and parameters: {'observation_period_num': 6, 'train_rates': 0.962945481030009, 'learning_rate': 0.00025398985043888927, 'batch_size': 228, 'step_size': 5, 'gamma': 0.9481977287682224}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:15:53,248][0m Trial 33 finished with value: 0.0682145431637764 and parameters: {'observation_period_num': 20, 'train_rates': 0.9622627994617299, 'learning_rate': 0.00015339685130277296, 'batch_size': 228, 'step_size': 5, 'gamma': 0.9447419850180505}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:16:19,436][0m Trial 34 finished with value: 0.23761460572573423 and parameters: {'observation_period_num': 75, 'train_rates': 0.8878981477084623, 'learning_rate': 0.00024362236358709715, 'batch_size': 242, 'step_size': 5, 'gamma': 0.978539853094222}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:16:56,981][0m Trial 35 finished with value: 0.07691998779773712 and parameters: {'observation_period_num': 44, 'train_rates': 0.9525804095449901, 'learning_rate': 4.889009226546017e-05, 'batch_size': 171, 'step_size': 7, 'gamma': 0.9446345531410574}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:17:24,307][0m Trial 36 finished with value: 0.053517666655291014 and parameters: {'observation_period_num': 20, 'train_rates': 0.9043721593106323, 'learning_rate': 0.00011968153400705774, 'batch_size': 224, 'step_size': 8, 'gamma': 0.9728923857841227}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:17:45,836][0m Trial 37 finished with value: 0.17955163426564208 and parameters: {'observation_period_num': 57, 'train_rates': 0.6129112189928312, 'learning_rate': 0.00025808728373870717, 'batch_size': 239, 'step_size': 9, 'gamma': 0.7503795220495946}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:18:09,152][0m Trial 38 finished with value: 0.21645719471513925 and parameters: {'observation_period_num': 101, 'train_rates': 0.6510238088308582, 'learning_rate': 0.0005945659562857723, 'batch_size': 213, 'step_size': 6, 'gamma': 0.9109120210672188}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:18:41,351][0m Trial 39 finished with value: 0.14157325656790481 and parameters: {'observation_period_num': 90, 'train_rates': 0.8605094857211321, 'learning_rate': 0.00010960149141731152, 'batch_size': 181, 'step_size': 4, 'gamma': 0.9894386022818521}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:19:12,350][0m Trial 40 finished with value: 0.1612459421157837 and parameters: {'observation_period_num': 115, 'train_rates': 0.9457264288611417, 'learning_rate': 6.334173446391516e-05, 'batch_size': 201, 'step_size': 3, 'gamma': 0.9556198775479139}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:19:39,741][0m Trial 41 finished with value: 0.04275950416922569 and parameters: {'observation_period_num': 8, 'train_rates': 0.9855948851082978, 'learning_rate': 0.0005283671519076669, 'batch_size': 253, 'step_size': 1, 'gamma': 0.9634390710491271}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:20:07,258][0m Trial 42 finished with value: 0.04503626003861427 and parameters: {'observation_period_num': 15, 'train_rates': 0.9662685293506297, 'learning_rate': 0.0005043169394824283, 'batch_size': 248, 'step_size': 4, 'gamma': 0.9337376616570485}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:20:36,969][0m Trial 43 finished with value: 0.0787614956498146 and parameters: {'observation_period_num': 38, 'train_rates': 0.9710977261282809, 'learning_rate': 0.0002039815033371384, 'batch_size': 231, 'step_size': 5, 'gamma': 0.9514893891292301}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:21:06,229][0m Trial 44 finished with value: 0.07164444029331207 and parameters: {'observation_period_num': 22, 'train_rates': 0.9872688696664617, 'learning_rate': 0.0006559443563697346, 'batch_size': 224, 'step_size': 10, 'gamma': 0.9746122735445587}. Best is trial 14 with value: 0.03248429191117784.[0m
[32m[I 2025-02-04 19:22:14,313][0m Trial 45 finished with value: 0.030903863773511894 and parameters: {'observation_period_num': 12, 'train_rates': 0.9402978350963426, 'learning_rate': 0.0003371910051475724, 'batch_size': 89, 'step_size': 3, 'gamma': 0.9641768718854303}. Best is trial 45 with value: 0.030903863773511894.[0m
[32m[I 2025-02-04 19:23:25,674][0m Trial 46 finished with value: 0.062270197798224056 and parameters: {'observation_period_num': 66, 'train_rates': 0.9353998584229133, 'learning_rate': 0.00034759288994587115, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9673472806609992}. Best is trial 45 with value: 0.030903863773511894.[0m
[32m[I 2025-02-04 19:25:52,063][0m Trial 47 finished with value: 0.0762019068029028 and parameters: {'observation_period_num': 42, 'train_rates': 0.9027388296717216, 'learning_rate': 1.1015415742724057e-05, 'batch_size': 39, 'step_size': 3, 'gamma': 0.9777641061823213}. Best is trial 45 with value: 0.030903863773511894.[0m
[32m[I 2025-02-04 19:30:09,871][0m Trial 48 finished with value: 0.36463765381854407 and parameters: {'observation_period_num': 200, 'train_rates': 0.697338814199064, 'learning_rate': 0.000756336237131471, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9595996614565208}. Best is trial 45 with value: 0.030903863773511894.[0m
[32m[I 2025-02-04 19:31:51,890][0m Trial 49 finished with value: 0.026321722153615165 and parameters: {'observation_period_num': 15, 'train_rates': 0.9383123444066815, 'learning_rate': 0.0004694591913094869, 'batch_size': 58, 'step_size': 4, 'gamma': 0.8603074228501688}. Best is trial 49 with value: 0.026321722153615165.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-04 19:31:51,911][0m A new study created in memory with name: no-name-df57e352-1618-4fbc-84f5-05a8be424586[0m
[32m[I 2025-02-04 19:32:55,253][0m Trial 0 finished with value: 0.32870053462999477 and parameters: {'observation_period_num': 191, 'train_rates': 0.7003056519759805, 'learning_rate': 0.00011944565137282507, 'batch_size': 72, 'step_size': 9, 'gamma': 0.8162724397653834}. Best is trial 0 with value: 0.32870053462999477.[0m
[32m[I 2025-02-04 19:34:08,962][0m Trial 1 finished with value: 0.36303873774342643 and parameters: {'observation_period_num': 208, 'train_rates': 0.657785752690869, 'learning_rate': 0.00022353394674698408, 'batch_size': 60, 'step_size': 13, 'gamma': 0.9687501400510344}. Best is trial 0 with value: 0.32870053462999477.[0m
[32m[I 2025-02-04 19:34:42,752][0m Trial 2 finished with value: 0.4487217857246905 and parameters: {'observation_period_num': 9, 'train_rates': 0.731313976851469, 'learning_rate': 4.2935046686388924e-06, 'batch_size': 154, 'step_size': 9, 'gamma': 0.8871975058283674}. Best is trial 0 with value: 0.32870053462999477.[0m
[32m[I 2025-02-04 19:35:49,767][0m Trial 3 finished with value: 0.7465552045303641 and parameters: {'observation_period_num': 111, 'train_rates': 0.9638093419001288, 'learning_rate': 2.670842293441548e-06, 'batch_size': 90, 'step_size': 3, 'gamma': 0.940553545458551}. Best is trial 0 with value: 0.32870053462999477.[0m
[32m[I 2025-02-04 19:39:07,459][0m Trial 4 finished with value: 0.14244329595245053 and parameters: {'observation_period_num': 22, 'train_rates': 0.9239879236445888, 'learning_rate': 2.341651805880623e-06, 'batch_size': 29, 'step_size': 8, 'gamma': 0.9402516290509375}. Best is trial 4 with value: 0.14244329595245053.[0m
[32m[I 2025-02-04 19:39:37,161][0m Trial 5 finished with value: 0.153008124873608 and parameters: {'observation_period_num': 169, 'train_rates': 0.8013724451341359, 'learning_rate': 0.0005054253080242847, 'batch_size': 185, 'step_size': 9, 'gamma': 0.766781475380383}. Best is trial 4 with value: 0.14244329595245053.[0m
[32m[I 2025-02-04 19:41:23,890][0m Trial 6 finished with value: 0.13668233991877451 and parameters: {'observation_period_num': 186, 'train_rates': 0.8664072171184423, 'learning_rate': 8.838699604324303e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9633882882325233}. Best is trial 6 with value: 0.13668233991877451.[0m
[32m[I 2025-02-04 19:42:44,994][0m Trial 7 finished with value: 0.201040286777748 and parameters: {'observation_period_num': 6, 'train_rates': 0.7985723371065986, 'learning_rate': 3.301345196649914e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.7893676290821837}. Best is trial 6 with value: 0.13668233991877451.[0m
[32m[I 2025-02-04 19:44:28,090][0m Trial 8 finished with value: 0.20295457723664073 and parameters: {'observation_period_num': 245, 'train_rates': 0.9698525944452181, 'learning_rate': 0.00015747292773148804, 'batch_size': 54, 'step_size': 10, 'gamma': 0.8319786204562873}. Best is trial 6 with value: 0.13668233991877451.[0m
[32m[I 2025-02-04 19:44:59,701][0m Trial 9 finished with value: 1.9259860515594482 and parameters: {'observation_period_num': 46, 'train_rates': 0.9565151256303597, 'learning_rate': 2.383171934757499e-06, 'batch_size': 211, 'step_size': 2, 'gamma': 0.8565369512653667}. Best is trial 6 with value: 0.13668233991877451.[0m
[32m[I 2025-02-04 19:45:46,988][0m Trial 10 finished with value: 0.2567054544176374 and parameters: {'observation_period_num': 126, 'train_rates': 0.8513802879107791, 'learning_rate': 1.8394128825662578e-05, 'batch_size': 120, 'step_size': 5, 'gamma': 0.8989588375223676}. Best is trial 6 with value: 0.13668233991877451.[0m
[32m[I 2025-02-04 19:50:03,907][0m Trial 11 finished with value: 0.05848828057008507 and parameters: {'observation_period_num': 71, 'train_rates': 0.8824870116941497, 'learning_rate': 1.7500897511638e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.9820722196555568}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 19:54:23,422][0m Trial 12 finished with value: 0.09884389057274788 and parameters: {'observation_period_num': 79, 'train_rates': 0.881257880047618, 'learning_rate': 2.5264105510688115e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.9894263808900796}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 19:59:39,980][0m Trial 13 finished with value: 0.06430008481594099 and parameters: {'observation_period_num': 77, 'train_rates': 0.879986164107915, 'learning_rate': 1.1063081592803942e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9832184075831565}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:00:33,807][0m Trial 14 finished with value: 0.34118212900663675 and parameters: {'observation_period_num': 82, 'train_rates': 0.9006302340188951, 'learning_rate': 1.0610890323345015e-05, 'batch_size': 111, 'step_size': 5, 'gamma': 0.9150738275181914}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:05:40,751][0m Trial 15 finished with value: 0.08696703405229975 and parameters: {'observation_period_num': 65, 'train_rates': 0.8364169260860537, 'learning_rate': 8.775129733230192e-06, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9865853480740623}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:06:02,775][0m Trial 16 finished with value: 0.36969090383369607 and parameters: {'observation_period_num': 100, 'train_rates': 0.7491925179676678, 'learning_rate': 5.044976053841575e-05, 'batch_size': 245, 'step_size': 4, 'gamma': 0.9260741770082196}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:06:42,739][0m Trial 17 finished with value: 0.45665348753207874 and parameters: {'observation_period_num': 151, 'train_rates': 0.9151116832892988, 'learning_rate': 9.3905902029226e-06, 'batch_size': 150, 'step_size': 6, 'gamma': 0.9560204502715924}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:07:34,176][0m Trial 18 finished with value: 0.35300349802351916 and parameters: {'observation_period_num': 41, 'train_rates': 0.8214718465889075, 'learning_rate': 5.446939179676611e-06, 'batch_size': 105, 'step_size': 11, 'gamma': 0.8626432772949778}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:09:41,569][0m Trial 19 finished with value: 0.5471119769265719 and parameters: {'observation_period_num': 54, 'train_rates': 0.778473689851572, 'learning_rate': 1.407305240180966e-06, 'batch_size': 40, 'step_size': 7, 'gamma': 0.9074116057884076}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:10:32,300][0m Trial 20 finished with value: 0.6359616378769004 and parameters: {'observation_period_num': 148, 'train_rates': 0.61335878221155, 'learning_rate': 6.37486869148103e-05, 'batch_size': 89, 'step_size': 3, 'gamma': 0.9405012883587482}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:15:40,089][0m Trial 21 finished with value: 0.07250979617732552 and parameters: {'observation_period_num': 74, 'train_rates': 0.8429073940369683, 'learning_rate': 1.2774821100966582e-05, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9885804703768514}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:21:14,226][0m Trial 22 finished with value: 0.08722759471774774 and parameters: {'observation_period_num': 96, 'train_rates': 0.8761420451023398, 'learning_rate': 1.4795735252869828e-05, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9744084292099242}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:23:23,440][0m Trial 23 finished with value: 0.07724549473844809 and parameters: {'observation_period_num': 73, 'train_rates': 0.9316936795543945, 'learning_rate': 3.496238425170323e-05, 'batch_size': 44, 'step_size': 4, 'gamma': 0.9592829028338816}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:24:29,617][0m Trial 24 finished with value: 0.3321300995993917 and parameters: {'observation_period_num': 126, 'train_rates': 0.8331516350930736, 'learning_rate': 6.0716687751455725e-06, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9859296694719007}. Best is trial 11 with value: 0.05848828057008507.[0m
[32m[I 2025-02-04 20:27:06,561][0m Trial 25 finished with value: 0.058217133521228225 and parameters: {'observation_period_num': 33, 'train_rates': 0.898628926810793, 'learning_rate': 1.813886606801899e-05, 'batch_size': 36, 'step_size': 6, 'gamma': 0.9534435091949608}. Best is trial 25 with value: 0.058217133521228225.[0m
[32m[I 2025-02-04 20:29:33,804][0m Trial 26 finished with value: 0.062478179037570955 and parameters: {'observation_period_num': 34, 'train_rates': 0.8972267980119956, 'learning_rate': 2.114644986379001e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.932488195334569}. Best is trial 25 with value: 0.058217133521228225.[0m
[32m[I 2025-02-04 20:32:26,530][0m Trial 27 finished with value: 0.05126049887874852 and parameters: {'observation_period_num': 31, 'train_rates': 0.9844395150885114, 'learning_rate': 2.5215120214256103e-05, 'batch_size': 35, 'step_size': 7, 'gamma': 0.8789383169061269}. Best is trial 27 with value: 0.05126049887874852.[0m
[32m[I 2025-02-04 20:34:03,644][0m Trial 28 finished with value: 0.052980661924396245 and parameters: {'observation_period_num': 26, 'train_rates': 0.9426503464219739, 'learning_rate': 4.282285469021521e-05, 'batch_size': 60, 'step_size': 7, 'gamma': 0.841298362569107}. Best is trial 27 with value: 0.05126049887874852.[0m
[32m[I 2025-02-04 20:35:30,026][0m Trial 29 finished with value: 0.0612640306353569 and parameters: {'observation_period_num': 21, 'train_rates': 0.9895179852523381, 'learning_rate': 5.238748949298062e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.8358283945193422}. Best is trial 27 with value: 0.05126049887874852.[0m
[32m[I 2025-02-04 20:36:16,149][0m Trial 30 finished with value: 0.06459903080637257 and parameters: {'observation_period_num': 34, 'train_rates': 0.942602673487388, 'learning_rate': 9.84980765167714e-05, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8076276343822418}. Best is trial 27 with value: 0.05126049887874852.[0m
[32m[I 2025-02-04 20:39:02,348][0m Trial 31 finished with value: 0.05865168571472168 and parameters: {'observation_period_num': 52, 'train_rates': 0.9861297356482706, 'learning_rate': 3.59599174585727e-05, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8829774156829645}. Best is trial 27 with value: 0.05126049887874852.[0m
[32m[I 2025-02-04 20:40:44,235][0m Trial 32 finished with value: 0.03943811087582096 and parameters: {'observation_period_num': 25, 'train_rates': 0.9458615739110152, 'learning_rate': 0.00029022850000520923, 'batch_size': 58, 'step_size': 7, 'gamma': 0.8429050240428096}. Best is trial 32 with value: 0.03943811087582096.[0m
[32m[I 2025-02-04 20:42:25,357][0m Trial 33 finished with value: 0.04285131756625424 and parameters: {'observation_period_num': 20, 'train_rates': 0.9410273901397735, 'learning_rate': 0.0006948448138544607, 'batch_size': 59, 'step_size': 12, 'gamma': 0.8433922832744418}. Best is trial 32 with value: 0.03943811087582096.[0m
[32m[I 2025-02-04 20:44:05,326][0m Trial 34 finished with value: 0.03918180740382298 and parameters: {'observation_period_num': 18, 'train_rates': 0.9435052693628873, 'learning_rate': 0.000826745747444965, 'batch_size': 59, 'step_size': 15, 'gamma': 0.8395392200157887}. Best is trial 34 with value: 0.03918180740382298.[0m
[32m[I 2025-02-04 20:45:15,695][0m Trial 35 finished with value: 0.02906982584413394 and parameters: {'observation_period_num': 8, 'train_rates': 0.9651148190175596, 'learning_rate': 0.0009536241953108215, 'batch_size': 87, 'step_size': 15, 'gamma': 0.8155769297215802}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:46:10,894][0m Trial 36 finished with value: 0.147682355057985 and parameters: {'observation_period_num': 11, 'train_rates': 0.6884636753825562, 'learning_rate': 0.0009329779921284245, 'batch_size': 90, 'step_size': 15, 'gamma': 0.8130182981684375}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:47:25,428][0m Trial 37 finished with value: 0.031423887714397074 and parameters: {'observation_period_num': 14, 'train_rates': 0.9561031593926306, 'learning_rate': 0.0003579141793388922, 'batch_size': 81, 'step_size': 12, 'gamma': 0.8487974911118505}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:48:25,181][0m Trial 38 finished with value: 0.03445485660008022 and parameters: {'observation_period_num': 12, 'train_rates': 0.9620771147115847, 'learning_rate': 0.00032957936174663946, 'batch_size': 102, 'step_size': 15, 'gamma': 0.7967874503239813}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:49:25,657][0m Trial 39 finished with value: 0.039183847104700714 and parameters: {'observation_period_num': 6, 'train_rates': 0.9647474512372126, 'learning_rate': 0.00036352691190038753, 'batch_size': 103, 'step_size': 15, 'gamma': 0.791256044256728}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:50:09,724][0m Trial 40 finished with value: 0.1701951950368747 and parameters: {'observation_period_num': 226, 'train_rates': 0.9220332485338745, 'learning_rate': 0.0001790065351770525, 'batch_size': 131, 'step_size': 13, 'gamma': 0.7515409704212426}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:51:14,964][0m Trial 41 finished with value: 0.03753919351626845 and parameters: {'observation_period_num': 9, 'train_rates': 0.965551136544762, 'learning_rate': 0.0003349329104622695, 'batch_size': 96, 'step_size': 15, 'gamma': 0.7976622845655538}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:52:30,833][0m Trial 42 finished with value: 0.03232535722715254 and parameters: {'observation_period_num': 13, 'train_rates': 0.9555206354702701, 'learning_rate': 0.0005372049769308152, 'batch_size': 79, 'step_size': 14, 'gamma': 0.7990117115866935}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:53:43,114][0m Trial 43 finished with value: 0.030442306771874428 and parameters: {'observation_period_num': 6, 'train_rates': 0.9708177377713503, 'learning_rate': 0.000448169919131562, 'batch_size': 86, 'step_size': 14, 'gamma': 0.7861368765389253}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:54:59,592][0m Trial 44 finished with value: 0.07175465082613433 and parameters: {'observation_period_num': 58, 'train_rates': 0.9719694209063845, 'learning_rate': 0.00047406280206369565, 'batch_size': 79, 'step_size': 13, 'gamma': 0.7738519308387412}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:55:50,190][0m Trial 45 finished with value: 0.08739909969804659 and parameters: {'observation_period_num': 44, 'train_rates': 0.918572199493499, 'learning_rate': 0.0005610066096098144, 'batch_size': 118, 'step_size': 14, 'gamma': 0.8225722365579552}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:57:11,155][0m Trial 46 finished with value: 0.03594536113284402 and parameters: {'observation_period_num': 5, 'train_rates': 0.9600404271651053, 'learning_rate': 0.00020344665603264144, 'batch_size': 75, 'step_size': 14, 'gamma': 0.7692065339722429}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:57:51,608][0m Trial 47 finished with value: 0.03931359794568008 and parameters: {'observation_period_num': 17, 'train_rates': 0.9034642454541943, 'learning_rate': 0.0002695343094471578, 'batch_size': 148, 'step_size': 12, 'gamma': 0.780727457315556}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:58:25,788][0m Trial 48 finished with value: 0.05359760654548269 and parameters: {'observation_period_num': 36, 'train_rates': 0.8599239795496371, 'learning_rate': 0.00013676270451569463, 'batch_size': 171, 'step_size': 14, 'gamma': 0.8003255815543562}. Best is trial 35 with value: 0.02906982584413394.[0m
[32m[I 2025-02-04 20:59:36,831][0m Trial 49 finished with value: 0.17200222120565525 and parameters: {'observation_period_num': 190, 'train_rates': 0.9694582010943883, 'learning_rate': 0.000560724401678973, 'batch_size': 83, 'step_size': 12, 'gamma': 0.8235235257698673}. Best is trial 35 with value: 0.02906982584413394.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-04 20:59:36,842][0m A new study created in memory with name: no-name-8b3fd396-33b5-40f7-9852-51d64eca981d[0m
[32m[I 2025-02-04 21:00:49,344][0m Trial 0 finished with value: 0.17959986726731755 and parameters: {'observation_period_num': 250, 'train_rates': 0.8728965697428079, 'learning_rate': 0.0001416745535714662, 'batch_size': 73, 'step_size': 9, 'gamma': 0.8617772869591909}. Best is trial 0 with value: 0.17959986726731755.[0m
[32m[I 2025-02-04 21:01:59,845][0m Trial 1 finished with value: 0.18967251063849921 and parameters: {'observation_period_num': 50, 'train_rates': 0.8309232879014702, 'learning_rate': 1.1669379937844948e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8464326819521294}. Best is trial 0 with value: 0.17959986726731755.[0m
[32m[I 2025-02-04 21:02:27,228][0m Trial 2 finished with value: 0.4128676950931549 and parameters: {'observation_period_num': 9, 'train_rates': 0.9309206995903117, 'learning_rate': 9.625552504711267e-06, 'batch_size': 246, 'step_size': 1, 'gamma': 0.9430974785653502}. Best is trial 0 with value: 0.17959986726731755.[0m
[32m[I 2025-02-04 21:03:01,789][0m Trial 3 finished with value: 0.114135988884502 and parameters: {'observation_period_num': 48, 'train_rates': 0.8388641653157995, 'learning_rate': 5.857396613662555e-05, 'batch_size': 169, 'step_size': 11, 'gamma': 0.7503727048075122}. Best is trial 3 with value: 0.114135988884502.[0m
[32m[I 2025-02-04 21:04:22,608][0m Trial 4 finished with value: 0.13553912330295237 and parameters: {'observation_period_num': 185, 'train_rates': 0.8474005586595936, 'learning_rate': 0.00011577083183871921, 'batch_size': 64, 'step_size': 11, 'gamma': 0.7601506376052082}. Best is trial 3 with value: 0.114135988884502.[0m
[32m[I 2025-02-04 21:04:52,750][0m Trial 5 finished with value: 0.495895038764398 and parameters: {'observation_period_num': 224, 'train_rates': 0.6336382114927958, 'learning_rate': 2.351180110578924e-05, 'batch_size': 155, 'step_size': 13, 'gamma': 0.9710474364093897}. Best is trial 3 with value: 0.114135988884502.[0m
[32m[I 2025-02-04 21:05:17,137][0m Trial 6 finished with value: 0.2848771941567224 and parameters: {'observation_period_num': 109, 'train_rates': 0.7782770963965596, 'learning_rate': 0.00012672841918476808, 'batch_size': 236, 'step_size': 2, 'gamma': 0.9631530210828775}. Best is trial 3 with value: 0.114135988884502.[0m
[32m[I 2025-02-04 21:05:41,823][0m Trial 7 finished with value: 0.6682317244946657 and parameters: {'observation_period_num': 62, 'train_rates': 0.7244776464048259, 'learning_rate': 4.047974513914744e-06, 'batch_size': 228, 'step_size': 15, 'gamma': 0.9212809610831676}. Best is trial 3 with value: 0.114135988884502.[0m
[32m[I 2025-02-04 21:06:21,628][0m Trial 8 finished with value: 0.0955452766284903 and parameters: {'observation_period_num': 46, 'train_rates': 0.9171862136725264, 'learning_rate': 6.212852447661982e-05, 'batch_size': 150, 'step_size': 11, 'gamma': 0.7969835007430849}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:06:57,021][0m Trial 9 finished with value: 0.6725796312093735 and parameters: {'observation_period_num': 201, 'train_rates': 0.9288133220844115, 'learning_rate': 8.409672789643496e-06, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9159507873505645}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:07:50,861][0m Trial 10 finished with value: 0.1254420280456543 and parameters: {'observation_period_num': 138, 'train_rates': 0.9690843576367933, 'learning_rate': 0.0007977801951391709, 'batch_size': 110, 'step_size': 6, 'gamma': 0.8200751726766523}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:08:21,690][0m Trial 11 finished with value: 0.22849568416332378 and parameters: {'observation_period_num': 5, 'train_rates': 0.7737162678416739, 'learning_rate': 4.721013277155177e-05, 'batch_size': 184, 'step_size': 12, 'gamma': 0.7513416596661902}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:12:17,965][0m Trial 12 finished with value: 0.15028315066145018 and parameters: {'observation_period_num': 79, 'train_rates': 0.896055900129453, 'learning_rate': 0.0005710803481379012, 'batch_size': 23, 'step_size': 7, 'gamma': 0.7960016268809154}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:12:51,066][0m Trial 13 finished with value: 2.3137927055358887 and parameters: {'observation_period_num': 40, 'train_rates': 0.9876923917466398, 'learning_rate': 1.0239299614114992e-06, 'batch_size': 197, 'step_size': 15, 'gamma': 0.7886594818783661}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:13:31,938][0m Trial 14 finished with value: 0.34138451920354307 and parameters: {'observation_period_num': 106, 'train_rates': 0.7146639364889784, 'learning_rate': 4.8218071652072925e-05, 'batch_size': 124, 'step_size': 10, 'gamma': 0.7898490738533788}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:14:00,385][0m Trial 15 finished with value: 0.1693564988383951 and parameters: {'observation_period_num': 146, 'train_rates': 0.8161312660846822, 'learning_rate': 0.00035636793718322204, 'batch_size': 196, 'step_size': 13, 'gamma': 0.826595779849662}. Best is trial 8 with value: 0.0955452766284903.[0m
[32m[I 2025-02-04 21:14:40,650][0m Trial 16 finished with value: 0.07967145313493541 and parameters: {'observation_period_num': 78, 'train_rates': 0.8939474573967303, 'learning_rate': 0.00020990274559446777, 'batch_size': 144, 'step_size': 6, 'gamma': 0.8895605650093233}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:15:36,189][0m Trial 17 finished with value: 0.0924954567485907 and parameters: {'observation_period_num': 92, 'train_rates': 0.9113399962800748, 'learning_rate': 0.0002760166436003267, 'batch_size': 106, 'step_size': 5, 'gamma': 0.8937348843207222}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:16:36,081][0m Trial 18 finished with value: 0.09469547511926338 and parameters: {'observation_period_num': 90, 'train_rates': 0.9531606646538134, 'learning_rate': 0.0003187364310476575, 'batch_size': 100, 'step_size': 5, 'gamma': 0.8859572162281973}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:20:46,045][0m Trial 19 finished with value: 0.1869568077479294 and parameters: {'observation_period_num': 166, 'train_rates': 0.8800333948652449, 'learning_rate': 0.00024170913592244984, 'batch_size': 21, 'step_size': 3, 'gamma': 0.8889298578548137}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:21:23,440][0m Trial 20 finished with value: 0.27772381626282 and parameters: {'observation_period_num': 119, 'train_rates': 0.6407861117013536, 'learning_rate': 0.0008756274801713287, 'batch_size': 128, 'step_size': 7, 'gamma': 0.8993948903741013}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:22:24,294][0m Trial 21 finished with value: 0.08678291743899894 and parameters: {'observation_period_num': 87, 'train_rates': 0.9559743102753641, 'learning_rate': 0.0002711026391037611, 'batch_size': 99, 'step_size': 5, 'gamma': 0.8757305899425817}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:23:26,766][0m Trial 22 finished with value: 0.09474482277119664 and parameters: {'observation_period_num': 86, 'train_rates': 0.9506950926309932, 'learning_rate': 0.00020124369790347517, 'batch_size': 95, 'step_size': 4, 'gamma': 0.8530997052718979}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:25:05,950][0m Trial 23 finished with value: 0.128905409862372 and parameters: {'observation_period_num': 72, 'train_rates': 0.9082849446722948, 'learning_rate': 0.00047204643315138553, 'batch_size': 57, 'step_size': 6, 'gamma': 0.9243381762988127}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:25:49,610][0m Trial 24 finished with value: 0.10123736788700154 and parameters: {'observation_period_num': 98, 'train_rates': 0.8650964258095118, 'learning_rate': 0.00011122622881920152, 'batch_size': 134, 'step_size': 5, 'gamma': 0.877061626253457}. Best is trial 16 with value: 0.07967145313493541.[0m
[32m[I 2025-02-04 21:27:51,578][0m Trial 25 finished with value: 0.0398316004464769 and parameters: {'observation_period_num': 27, 'train_rates': 0.9601245462141896, 'learning_rate': 0.00021217624863981887, 'batch_size': 49, 'step_size': 8, 'gamma': 0.91150802376088}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:30:26,516][0m Trial 26 finished with value: 0.06305644099007954 and parameters: {'observation_period_num': 28, 'train_rates': 0.9812651205126016, 'learning_rate': 8.19884178974185e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.9895130517160782}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:32:39,481][0m Trial 27 finished with value: 0.05005643425403901 and parameters: {'observation_period_num': 29, 'train_rates': 0.9820886416598587, 'learning_rate': 1.996742061984798e-05, 'batch_size': 45, 'step_size': 8, 'gamma': 0.9878680750210858}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:34:51,812][0m Trial 28 finished with value: 0.05156107541794578 and parameters: {'observation_period_num': 24, 'train_rates': 0.9838698290253082, 'learning_rate': 2.445584781770555e-05, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9846650154621406}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:36:59,361][0m Trial 29 finished with value: 0.0540634282153739 and parameters: {'observation_period_num': 23, 'train_rates': 0.9425448835714949, 'learning_rate': 2.8045105400525286e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.9504207602547816}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:38:19,975][0m Trial 30 finished with value: 0.14279787242412567 and parameters: {'observation_period_num': 21, 'train_rates': 0.9880243855957344, 'learning_rate': 1.6598656644111816e-05, 'batch_size': 78, 'step_size': 8, 'gamma': 0.9850246276905416}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:40:37,240][0m Trial 31 finished with value: 0.05003771642450153 and parameters: {'observation_period_num': 30, 'train_rates': 0.9441438721372415, 'learning_rate': 3.461060128495013e-05, 'batch_size': 43, 'step_size': 9, 'gamma': 0.94945807886423}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:43:04,896][0m Trial 32 finished with value: 0.19755776537643685 and parameters: {'observation_period_num': 64, 'train_rates': 0.9687753228046869, 'learning_rate': 6.598012742738843e-06, 'batch_size': 40, 'step_size': 9, 'gamma': 0.941478250846811}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:44:42,790][0m Trial 33 finished with value: 0.22579327406305255 and parameters: {'observation_period_num': 32, 'train_rates': 0.9321655684287001, 'learning_rate': 3.5873851841130994e-06, 'batch_size': 60, 'step_size': 10, 'gamma': 0.9695777636894612}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:45:59,722][0m Trial 34 finished with value: 0.05448844763221311 and parameters: {'observation_period_num': 14, 'train_rates': 0.9586380496126411, 'learning_rate': 3.285320250743083e-05, 'batch_size': 79, 'step_size': 7, 'gamma': 0.9377074980115869}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:49:09,550][0m Trial 35 finished with value: 0.06794119325394814 and parameters: {'observation_period_num': 57, 'train_rates': 0.865404101918398, 'learning_rate': 1.708504378284787e-05, 'batch_size': 29, 'step_size': 10, 'gamma': 0.9545406350676479}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:50:37,445][0m Trial 36 finished with value: 0.20383006945626236 and parameters: {'observation_period_num': 38, 'train_rates': 0.6032513671022615, 'learning_rate': 1.6969404962989784e-05, 'batch_size': 49, 'step_size': 8, 'gamma': 0.9788409864025321}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:56:28,385][0m Trial 37 finished with value: 0.06800442637533557 and parameters: {'observation_period_num': 51, 'train_rates': 0.9325450749246592, 'learning_rate': 3.616518648290601e-05, 'batch_size': 16, 'step_size': 8, 'gamma': 0.962314508470015}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 21:57:59,592][0m Trial 38 finished with value: 0.10120217502117157 and parameters: {'observation_period_num': 5, 'train_rates': 0.9876346080560837, 'learning_rate': 1.1508536278312389e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.9101920729022466}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:00:45,269][0m Trial 39 finished with value: 0.120730732930334 and parameters: {'observation_period_num': 22, 'train_rates': 0.9675796813189765, 'learning_rate': 5.618164546368671e-06, 'batch_size': 36, 'step_size': 10, 'gamma': 0.9317598093187403}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:02:25,956][0m Trial 40 finished with value: 0.26958058848158345 and parameters: {'observation_period_num': 38, 'train_rates': 0.8126149434490849, 'learning_rate': 2.4063362819084004e-06, 'batch_size': 53, 'step_size': 12, 'gamma': 0.9744869248462544}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:04:36,061][0m Trial 41 finished with value: 0.05408598542479532 and parameters: {'observation_period_num': 24, 'train_rates': 0.9425958911942046, 'learning_rate': 1.9717305472321752e-05, 'batch_size': 45, 'step_size': 9, 'gamma': 0.9551103603972431}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:05:56,173][0m Trial 42 finished with value: 0.24083162907740516 and parameters: {'observation_period_num': 244, 'train_rates': 0.9220629402455753, 'learning_rate': 2.266123223152112e-05, 'batch_size': 68, 'step_size': 7, 'gamma': 0.9519667062366736}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:08:57,788][0m Trial 43 finished with value: 0.044914013845703656 and parameters: {'observation_period_num': 16, 'train_rates': 0.8886276488064029, 'learning_rate': 8.005981084108141e-05, 'batch_size': 31, 'step_size': 9, 'gamma': 0.9492737501647301}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:10:02,466][0m Trial 44 finished with value: 0.06844835723349951 and parameters: {'observation_period_num': 49, 'train_rates': 0.8476928239951067, 'learning_rate': 9.07770633053369e-05, 'batch_size': 87, 'step_size': 11, 'gamma': 0.9897676546906136}. Best is trial 25 with value: 0.0398316004464769.[0m
[32m[I 2025-02-04 22:12:56,450][0m Trial 45 finished with value: 0.03950855543184127 and parameters: {'observation_period_num': 15, 'train_rates': 0.8942770281399419, 'learning_rate': 7.956973552490178e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.9674011027032974}. Best is trial 45 with value: 0.03950855543184127.[0m
[32m[I 2025-02-04 22:16:08,922][0m Trial 46 finished with value: 0.07422708788561443 and parameters: {'observation_period_num': 14, 'train_rates': 0.8927409664669742, 'learning_rate': 7.28575572287493e-05, 'batch_size': 29, 'step_size': 7, 'gamma': 0.9654477208932553}. Best is trial 45 with value: 0.03950855543184127.[0m
[32m[I 2025-02-04 22:18:45,864][0m Trial 47 finished with value: 0.2250646657901681 and parameters: {'observation_period_num': 67, 'train_rates': 0.7494923857362156, 'learning_rate': 0.00017907369507334724, 'batch_size': 31, 'step_size': 6, 'gamma': 0.9325613726096654}. Best is trial 45 with value: 0.03950855543184127.[0m
[32m[I 2025-02-04 22:20:11,132][0m Trial 48 finished with value: 0.05386676523372832 and parameters: {'observation_period_num': 41, 'train_rates': 0.8809911144607918, 'learning_rate': 0.00015181595157609594, 'batch_size': 66, 'step_size': 10, 'gamma': 0.9108502184931494}. Best is trial 45 with value: 0.03950855543184127.[0m
[32m[I 2025-02-04 22:20:37,540][0m Trial 49 finished with value: 0.0784858471565087 and parameters: {'observation_period_num': 10, 'train_rates': 0.9086809746588155, 'learning_rate': 4.539563322958286e-05, 'batch_size': 254, 'step_size': 11, 'gamma': 0.9443772766524566}. Best is trial 45 with value: 0.03950855543184127.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-04 22:20:37,551][0m A new study created in memory with name: no-name-135183e6-0957-4225-b87e-d3b5ccc60268[0m
[32m[I 2025-02-04 22:22:05,478][0m Trial 0 finished with value: 0.7188127561619407 and parameters: {'observation_period_num': 113, 'train_rates': 0.7656977897485205, 'learning_rate': 1.8255554024605123e-06, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9591187726337129}. Best is trial 0 with value: 0.7188127561619407.[0m
[32m[I 2025-02-04 22:22:36,729][0m Trial 1 finished with value: 0.2483344853638417 and parameters: {'observation_period_num': 36, 'train_rates': 0.6205265770642653, 'learning_rate': 0.00012887878711048177, 'batch_size': 155, 'step_size': 4, 'gamma': 0.9059281685409721}. Best is trial 1 with value: 0.2483344853638417.[0m
[32m[I 2025-02-04 22:23:44,070][0m Trial 2 finished with value: 0.10562063320579691 and parameters: {'observation_period_num': 59, 'train_rates': 0.9191976049945609, 'learning_rate': 5.414101550440406e-05, 'batch_size': 88, 'step_size': 8, 'gamma': 0.9596884737033541}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:24:27,891][0m Trial 3 finished with value: 2.8767203226859714 and parameters: {'observation_period_num': 176, 'train_rates': 0.660778848656508, 'learning_rate': 1.4814287007002924e-06, 'batch_size': 105, 'step_size': 6, 'gamma': 0.8076204175090979}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:24:53,871][0m Trial 4 finished with value: 0.388060986995697 and parameters: {'observation_period_num': 235, 'train_rates': 0.985344784484082, 'learning_rate': 5.470664338950422e-05, 'batch_size': 256, 'step_size': 8, 'gamma': 0.9073314909957222}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:25:25,943][0m Trial 5 finished with value: 1.4788018355277672 and parameters: {'observation_period_num': 232, 'train_rates': 0.8283897736552952, 'learning_rate': 3.832081184332445e-06, 'batch_size': 168, 'step_size': 1, 'gamma': 0.9066785922063367}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:26:15,655][0m Trial 6 finished with value: 0.5987429403195715 and parameters: {'observation_period_num': 168, 'train_rates': 0.7179371055598862, 'learning_rate': 1.4876447960317423e-05, 'batch_size': 96, 'step_size': 9, 'gamma': 0.8293553552318069}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:26:53,275][0m Trial 7 finished with value: 0.87833910226995 and parameters: {'observation_period_num': 190, 'train_rates': 0.6259544199378506, 'learning_rate': 0.0001000298905706285, 'batch_size': 119, 'step_size': 9, 'gamma': 0.9466753020010569}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:27:15,701][0m Trial 8 finished with value: 0.9090174015641533 and parameters: {'observation_period_num': 80, 'train_rates': 0.741767489089696, 'learning_rate': 5.876174933260523e-06, 'batch_size': 244, 'step_size': 4, 'gamma': 0.9082319382032076}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:27:41,667][0m Trial 9 finished with value: 0.1710921399114866 and parameters: {'observation_period_num': 81, 'train_rates': 0.899483230277399, 'learning_rate': 8.72138213974585e-05, 'batch_size': 254, 'step_size': 5, 'gamma': 0.9404868325114099}. Best is trial 2 with value: 0.10562063320579691.[0m
[32m[I 2025-02-04 22:31:45,475][0m Trial 10 finished with value: 0.054692025024410895 and parameters: {'observation_period_num': 15, 'train_rates': 0.8847654106745937, 'learning_rate': 0.0008731902804636543, 'batch_size': 23, 'step_size': 14, 'gamma': 0.7532356892229483}. Best is trial 10 with value: 0.054692025024410895.[0m
[32m[I 2025-02-04 22:37:33,228][0m Trial 11 finished with value: 0.03777481933467163 and parameters: {'observation_period_num': 10, 'train_rates': 0.8958186370390298, 'learning_rate': 0.0008213213002676373, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7676623930044139}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:42:44,569][0m Trial 12 finished with value: 0.04131772748059443 and parameters: {'observation_period_num': 9, 'train_rates': 0.8314523255212896, 'learning_rate': 0.000713072694098477, 'batch_size': 17, 'step_size': 15, 'gamma': 0.7532875759896306}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:46:20,550][0m Trial 13 finished with value: 0.05746236346937992 and parameters: {'observation_period_num': 5, 'train_rates': 0.8350924568508817, 'learning_rate': 0.0009908928075720307, 'batch_size': 25, 'step_size': 15, 'gamma': 0.7591805747883092}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:48:03,769][0m Trial 14 finished with value: 0.057655084656901875 and parameters: {'observation_period_num': 44, 'train_rates': 0.9653348992496891, 'learning_rate': 0.00029224106398568735, 'batch_size': 58, 'step_size': 12, 'gamma': 0.7942362125961131}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:49:47,741][0m Trial 15 finished with value: 0.16429519630936593 and parameters: {'observation_period_num': 120, 'train_rates': 0.8353762798628168, 'learning_rate': 0.0003410703178773762, 'batch_size': 51, 'step_size': 14, 'gamma': 0.8468581878441528}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:50:16,687][0m Trial 16 finished with value: 0.19727144006915429 and parameters: {'observation_period_num': 84, 'train_rates': 0.7973287918472455, 'learning_rate': 0.00033013131146875265, 'batch_size': 191, 'step_size': 12, 'gamma': 0.7861940825677621}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:54:15,975][0m Trial 17 finished with value: 0.0760167078115046 and parameters: {'observation_period_num': 27, 'train_rates': 0.8742678523091258, 'learning_rate': 2.1449536733499436e-05, 'batch_size': 23, 'step_size': 13, 'gamma': 0.8641640263876377}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:55:33,294][0m Trial 18 finished with value: 0.19470106995610334 and parameters: {'observation_period_num': 145, 'train_rates': 0.9328970945878896, 'learning_rate': 0.0004996623315755161, 'batch_size': 73, 'step_size': 11, 'gamma': 0.7720606190658085}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:57:45,502][0m Trial 19 finished with value: 0.22122495724166746 and parameters: {'observation_period_num': 51, 'train_rates': 0.7926686064780454, 'learning_rate': 0.00017474737224306867, 'batch_size': 39, 'step_size': 15, 'gamma': 0.8185494024758224}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:58:13,138][0m Trial 20 finished with value: 0.04582661328979148 and parameters: {'observation_period_num': 7, 'train_rates': 0.861838658084781, 'learning_rate': 0.0006172807765892298, 'batch_size': 215, 'step_size': 13, 'gamma': 0.7840033112019258}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:58:42,848][0m Trial 21 finished with value: 0.040934862825891066 and parameters: {'observation_period_num': 11, 'train_rates': 0.8619279722892301, 'learning_rate': 0.0006294510145805171, 'batch_size': 207, 'step_size': 13, 'gamma': 0.7711675861901184}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:59:29,024][0m Trial 22 finished with value: 0.10102206226941701 and parameters: {'observation_period_num': 64, 'train_rates': 0.9359069215742007, 'learning_rate': 0.00022543148415976098, 'batch_size': 135, 'step_size': 15, 'gamma': 0.7503270369376284}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 22:59:56,356][0m Trial 23 finished with value: 0.06062829633296098 and parameters: {'observation_period_num': 35, 'train_rates': 0.8548530459365039, 'learning_rate': 0.000499371881615772, 'batch_size': 214, 'step_size': 13, 'gamma': 0.7756874804007695}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:00:29,368][0m Trial 24 finished with value: 0.049926760389106234 and parameters: {'observation_period_num': 23, 'train_rates': 0.7994808202504794, 'learning_rate': 0.0009200724407397804, 'batch_size': 178, 'step_size': 11, 'gamma': 0.8003089947762976}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:01:03,579][0m Trial 25 finished with value: 0.24509326195301012 and parameters: {'observation_period_num': 108, 'train_rates': 0.6977380425863818, 'learning_rate': 0.00046035614195196975, 'batch_size': 145, 'step_size': 14, 'gamma': 0.8296800516708134}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:01:33,449][0m Trial 26 finished with value: 0.1273774656047859 and parameters: {'observation_period_num': 67, 'train_rates': 0.9131064001865079, 'learning_rate': 0.0001732073600371513, 'batch_size': 205, 'step_size': 12, 'gamma': 0.9892971082158347}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:02:49,577][0m Trial 27 finished with value: 0.05773462545202703 and parameters: {'observation_period_num': 24, 'train_rates': 0.8160811839209197, 'learning_rate': 0.0006737931398766163, 'batch_size': 72, 'step_size': 15, 'gamma': 0.7687795002104696}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:03:42,767][0m Trial 28 finished with value: 0.07327257174878352 and parameters: {'observation_period_num': 5, 'train_rates': 0.9581255819932064, 'learning_rate': 5.1034499656549926e-05, 'batch_size': 118, 'step_size': 11, 'gamma': 0.847688538118014}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:05:35,541][0m Trial 29 finished with value: 0.49334866403603517 and parameters: {'observation_period_num': 102, 'train_rates': 0.7751731280800078, 'learning_rate': 8.416559355002506e-06, 'batch_size': 45, 'step_size': 10, 'gamma': 0.8095561304136529}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:05:59,821][0m Trial 30 finished with value: 0.33308002388838565 and parameters: {'observation_period_num': 137, 'train_rates': 0.7657275247892897, 'learning_rate': 0.0003040275581388445, 'batch_size': 232, 'step_size': 14, 'gamma': 0.7664525939773605}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:06:29,636][0m Trial 31 finished with value: 0.040147414138340894 and parameters: {'observation_period_num': 6, 'train_rates': 0.8638938705267111, 'learning_rate': 0.0007099067094552472, 'batch_size': 206, 'step_size': 13, 'gamma': 0.7870985441407558}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:07:01,527][0m Trial 32 finished with value: 0.06178713158057753 and parameters: {'observation_period_num': 45, 'train_rates': 0.8920967119008508, 'learning_rate': 0.000540433801898188, 'batch_size': 192, 'step_size': 13, 'gamma': 0.7859184866387124}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:07:36,950][0m Trial 33 finished with value: 0.06370416882893314 and parameters: {'observation_period_num': 29, 'train_rates': 0.8507157672437466, 'learning_rate': 0.00019455954987376647, 'batch_size': 171, 'step_size': 12, 'gamma': 0.7652655896137112}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:08:04,702][0m Trial 34 finished with value: 0.04823826405812393 and parameters: {'observation_period_num': 19, 'train_rates': 0.8724434580740275, 'learning_rate': 0.0006954054899367745, 'batch_size': 229, 'step_size': 14, 'gamma': 0.750241579450741}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:13:33,582][0m Trial 35 finished with value: 0.09290450489590604 and parameters: {'observation_period_num': 45, 'train_rates': 0.9116380425923923, 'learning_rate': 0.00042289785394970087, 'batch_size': 17, 'step_size': 10, 'gamma': 0.7947918730554295}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:14:07,944][0m Trial 36 finished with value: 0.09966458065794148 and parameters: {'observation_period_num': 36, 'train_rates': 0.8132663834132873, 'learning_rate': 0.0009427811592603782, 'batch_size': 159, 'step_size': 1, 'gamma': 0.8808024725764756}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:16:36,812][0m Trial 37 finished with value: 0.7063274938390002 and parameters: {'observation_period_num': 58, 'train_rates': 0.8500246864445621, 'learning_rate': 1.0238362219055977e-06, 'batch_size': 36, 'step_size': 7, 'gamma': 0.8204389741005906}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:18:10,147][0m Trial 38 finished with value: 0.06339273264533594 and parameters: {'observation_period_num': 15, 'train_rates': 0.9355976080787058, 'learning_rate': 0.0001096115777782164, 'batch_size': 64, 'step_size': 15, 'gamma': 0.7779425250499061}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:18:38,122][0m Trial 39 finished with value: 1.1059213723042751 and parameters: {'observation_period_num': 218, 'train_rates': 0.7433395398560683, 'learning_rate': 2.5061348242173103e-06, 'batch_size': 191, 'step_size': 9, 'gamma': 0.8034785343847716}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:19:46,582][0m Trial 40 finished with value: 0.11254079114307057 and parameters: {'observation_period_num': 97, 'train_rates': 0.9038016465744704, 'learning_rate': 0.00024278361956695034, 'batch_size': 84, 'step_size': 11, 'gamma': 0.7624010057952748}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:20:14,564][0m Trial 41 finished with value: 0.042011571854730194 and parameters: {'observation_period_num': 6, 'train_rates': 0.8658409975173669, 'learning_rate': 0.000671935882863036, 'batch_size': 217, 'step_size': 13, 'gamma': 0.7876914291872257}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:20:42,579][0m Trial 42 finished with value: 0.04130630580515697 and parameters: {'observation_period_num': 13, 'train_rates': 0.8817669503693338, 'learning_rate': 0.000745620889405247, 'batch_size': 228, 'step_size': 13, 'gamma': 0.7892132936181948}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:21:08,290][0m Trial 43 finished with value: 0.07005557380285037 and parameters: {'observation_period_num': 37, 'train_rates': 0.8843957898103348, 'learning_rate': 0.0004110865653092585, 'batch_size': 242, 'step_size': 14, 'gamma': 0.8090969532228106}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:21:34,468][0m Trial 44 finished with value: 0.0488982735186573 and parameters: {'observation_period_num': 18, 'train_rates': 0.8325227793947021, 'learning_rate': 0.0007484041503026838, 'batch_size': 230, 'step_size': 12, 'gamma': 0.7775157741573524}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:22:06,535][0m Trial 45 finished with value: 0.13499455153942108 and parameters: {'observation_period_num': 73, 'train_rates': 0.9573561925309427, 'learning_rate': 0.0001386999888321851, 'batch_size': 199, 'step_size': 13, 'gamma': 0.7602398580080092}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:22:30,959][0m Trial 46 finished with value: 0.04755864771652041 and parameters: {'observation_period_num': 14, 'train_rates': 0.8206258505805005, 'learning_rate': 0.0009831707448426966, 'batch_size': 250, 'step_size': 14, 'gamma': 0.8308605152587981}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:23:25,228][0m Trial 47 finished with value: 0.08007829754454333 and parameters: {'observation_period_num': 53, 'train_rates': 0.8870993216433029, 'learning_rate': 0.00035238374483551407, 'batch_size': 108, 'step_size': 2, 'gamma': 0.9271213467698537}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:23:57,747][0m Trial 48 finished with value: 0.11897008613342756 and parameters: {'observation_period_num': 33, 'train_rates': 0.8444644215116921, 'learning_rate': 3.7363223812167795e-05, 'batch_size': 180, 'step_size': 15, 'gamma': 0.7983729295012528}. Best is trial 11 with value: 0.03777481933467163.[0m
[32m[I 2025-02-04 23:26:54,213][0m Trial 49 finished with value: 0.24097707853630676 and parameters: {'observation_period_num': 178, 'train_rates': 0.9234567063819767, 'learning_rate': 0.00024312346821335872, 'batch_size': 31, 'step_size': 7, 'gamma': 0.756954462413243}. Best is trial 11 with value: 0.03777481933467163.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_VMD.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 5, 'train_rates': 0.8066113564404266, 'learning_rate': 0.000462769100818871, 'batch_size': 74, 'step_size': 5, 'gamma': 0.9624949595038581}
Epoch 1/300, trend Loss: 0.2819 | 0.1262
Epoch 2/300, trend Loss: 0.1332 | 0.1006
Epoch 3/300, trend Loss: 0.1292 | 0.1446
Epoch 4/300, trend Loss: 0.1272 | 0.0818
Epoch 5/300, trend Loss: 0.1255 | 0.0773
Epoch 6/300, trend Loss: 0.1244 | 0.0766
Epoch 7/300, trend Loss: 0.1276 | 0.0786
Epoch 8/300, trend Loss: 0.1246 | 0.0826
Epoch 9/300, trend Loss: 0.1345 | 0.1139
Epoch 10/300, trend Loss: 0.1368 | 0.0978
Epoch 11/300, trend Loss: 0.1066 | 0.0744
Epoch 12/300, trend Loss: 0.1061 | 0.1058
Epoch 13/300, trend Loss: 0.1167 | 0.0672
Epoch 14/300, trend Loss: 0.0985 | 0.0541
Epoch 15/300, trend Loss: 0.1155 | 0.0667
Epoch 16/300, trend Loss: 0.0998 | 0.0502
Epoch 17/300, trend Loss: 0.0959 | 0.0525
Epoch 18/300, trend Loss: 0.1033 | 0.0566
Epoch 19/300, trend Loss: 0.0924 | 0.0483
Epoch 20/300, trend Loss: 0.0972 | 0.0460
Epoch 21/300, trend Loss: 0.0895 | 0.0503
Epoch 22/300, trend Loss: 0.0890 | 0.0474
Epoch 23/300, trend Loss: 0.0902 | 0.0474
Epoch 24/300, trend Loss: 0.0918 | 0.0486
Epoch 25/300, trend Loss: 0.0888 | 0.0441
Epoch 26/300, trend Loss: 0.0834 | 0.0422
Epoch 27/300, trend Loss: 0.0807 | 0.0414
Epoch 28/300, trend Loss: 0.0804 | 0.0411
Epoch 29/300, trend Loss: 0.0799 | 0.0409
Epoch 30/300, trend Loss: 0.0787 | 0.0393
Epoch 31/300, trend Loss: 0.0775 | 0.0386
Epoch 32/300, trend Loss: 0.0768 | 0.0381
Epoch 33/300, trend Loss: 0.0764 | 0.0369
Epoch 34/300, trend Loss: 0.0759 | 0.0363
Epoch 35/300, trend Loss: 0.0754 | 0.0371
Epoch 36/300, trend Loss: 0.0751 | 0.0364
Epoch 37/300, trend Loss: 0.0756 | 0.0355
Epoch 38/300, trend Loss: 0.0755 | 0.0362
Epoch 39/300, trend Loss: 0.0756 | 0.0364
Epoch 40/300, trend Loss: 0.0780 | 0.0408
Epoch 41/300, trend Loss: 0.0780 | 0.0441
Epoch 42/300, trend Loss: 0.0784 | 0.0430
Epoch 43/300, trend Loss: 0.0751 | 0.0379
Epoch 44/300, trend Loss: 0.0748 | 0.0363
Epoch 45/300, trend Loss: 0.0758 | 0.0378
Epoch 46/300, trend Loss: 0.0818 | 0.0404
Epoch 47/300, trend Loss: 0.0872 | 0.0415
Epoch 48/300, trend Loss: 0.0770 | 0.0435
Epoch 49/300, trend Loss: 0.0770 | 0.0375
Epoch 50/300, trend Loss: 0.0731 | 0.0404
Epoch 51/300, trend Loss: 0.0716 | 0.0376
Epoch 52/300, trend Loss: 0.0716 | 0.0364
Epoch 53/300, trend Loss: 0.0693 | 0.0356
Epoch 54/300, trend Loss: 0.0684 | 0.0337
Epoch 55/300, trend Loss: 0.0676 | 0.0338
Epoch 56/300, trend Loss: 0.0670 | 0.0336
Epoch 57/300, trend Loss: 0.0666 | 0.0325
Epoch 58/300, trend Loss: 0.0663 | 0.0327
Epoch 59/300, trend Loss: 0.0661 | 0.0322
Epoch 60/300, trend Loss: 0.0661 | 0.0316
Epoch 61/300, trend Loss: 0.0662 | 0.0321
Epoch 62/300, trend Loss: 0.0663 | 0.0327
Epoch 63/300, trend Loss: 0.0669 | 0.0341
Epoch 64/300, trend Loss: 0.0677 | 0.0331
Epoch 65/300, trend Loss: 0.0671 | 0.0326
Epoch 66/300, trend Loss: 0.0673 | 0.0329
Epoch 67/300, trend Loss: 0.0670 | 0.0324
Epoch 68/300, trend Loss: 0.0656 | 0.0320
Epoch 69/300, trend Loss: 0.0650 | 0.0324
Epoch 70/300, trend Loss: 0.0649 | 0.0336
Epoch 71/300, trend Loss: 0.0662 | 0.0333
Epoch 72/300, trend Loss: 0.0664 | 0.0320
Epoch 73/300, trend Loss: 0.0696 | 0.0348
Epoch 74/300, trend Loss: 0.0680 | 0.0370
Epoch 75/300, trend Loss: 0.0683 | 0.0341
Epoch 76/300, trend Loss: 0.0680 | 0.0331
Epoch 77/300, trend Loss: 0.0667 | 0.0350
Epoch 78/300, trend Loss: 0.0655 | 0.0322
Epoch 79/300, trend Loss: 0.0673 | 0.0334
Epoch 80/300, trend Loss: 0.0662 | 0.0327
Epoch 81/300, trend Loss: 0.0646 | 0.0313
Epoch 82/300, trend Loss: 0.0642 | 0.0323
Epoch 83/300, trend Loss: 0.0648 | 0.0339
Epoch 84/300, trend Loss: 0.0646 | 0.0352
Epoch 85/300, trend Loss: 0.0652 | 0.0364
Epoch 86/300, trend Loss: 0.0648 | 0.0352
Epoch 87/300, trend Loss: 0.0639 | 0.0350
Epoch 88/300, trend Loss: 0.0637 | 0.0334
Epoch 89/300, trend Loss: 0.0634 | 0.0328
Epoch 90/300, trend Loss: 0.0627 | 0.0334
Epoch 91/300, trend Loss: 0.0626 | 0.0344
Epoch 92/300, trend Loss: 0.0629 | 0.0346
Epoch 93/300, trend Loss: 0.0629 | 0.0335
Epoch 94/300, trend Loss: 0.0629 | 0.0315
Epoch 95/300, trend Loss: 0.0628 | 0.0305
Epoch 96/300, trend Loss: 0.0626 | 0.0301
Epoch 97/300, trend Loss: 0.0625 | 0.0317
Epoch 98/300, trend Loss: 0.0624 | 0.0332
Epoch 99/300, trend Loss: 0.0623 | 0.0304
Epoch 100/300, trend Loss: 0.0625 | 0.0298
Epoch 101/300, trend Loss: 0.0622 | 0.0294
Epoch 102/300, trend Loss: 0.0621 | 0.0288
Epoch 103/300, trend Loss: 0.0619 | 0.0289
Epoch 104/300, trend Loss: 0.0618 | 0.0297
Epoch 105/300, trend Loss: 0.0617 | 0.0308
Epoch 106/300, trend Loss: 0.0618 | 0.0301
Epoch 107/300, trend Loss: 0.0613 | 0.0292
Epoch 108/300, trend Loss: 0.0618 | 0.0303
Epoch 109/300, trend Loss: 0.0622 | 0.0314
Epoch 110/300, trend Loss: 0.0623 | 0.0328
Epoch 111/300, trend Loss: 0.0646 | 0.0297
Epoch 112/300, trend Loss: 0.0617 | 0.0327
Epoch 113/300, trend Loss: 0.0617 | 0.0300
Epoch 114/300, trend Loss: 0.0607 | 0.0289
Epoch 115/300, trend Loss: 0.0609 | 0.0301
Epoch 116/300, trend Loss: 0.0610 | 0.0290
Epoch 117/300, trend Loss: 0.0650 | 0.0323
Epoch 118/300, trend Loss: 0.0639 | 0.0311
Epoch 119/300, trend Loss: 0.0621 | 0.0304
Epoch 120/300, trend Loss: 0.0605 | 0.0301
Epoch 121/300, trend Loss: 0.0607 | 0.0296
Epoch 122/300, trend Loss: 0.0605 | 0.0282
Epoch 123/300, trend Loss: 0.0599 | 0.0281
Epoch 124/300, trend Loss: 0.0596 | 0.0282
Epoch 125/300, trend Loss: 0.0594 | 0.0286
Epoch 126/300, trend Loss: 0.0595 | 0.0289
Epoch 127/300, trend Loss: 0.0597 | 0.0290
Epoch 128/300, trend Loss: 0.0601 | 0.0291
Epoch 129/300, trend Loss: 0.0607 | 0.0300
Epoch 130/300, trend Loss: 0.0618 | 0.0317
Epoch 131/300, trend Loss: 0.0632 | 0.0320
Epoch 132/300, trend Loss: 0.0661 | 0.0346
Epoch 133/300, trend Loss: 0.0643 | 0.0306
Epoch 134/300, trend Loss: 0.0661 | 0.0352
Epoch 135/300, trend Loss: 0.0657 | 0.0312
Epoch 136/300, trend Loss: 0.0618 | 0.0298
Epoch 137/300, trend Loss: 0.0600 | 0.0289
Epoch 138/300, trend Loss: 0.0593 | 0.0290
Epoch 139/300, trend Loss: 0.0593 | 0.0293
Epoch 140/300, trend Loss: 0.0590 | 0.0294
Epoch 141/300, trend Loss: 0.0584 | 0.0290
Epoch 142/300, trend Loss: 0.0579 | 0.0295
Epoch 143/300, trend Loss: 0.0578 | 0.0293
Epoch 144/300, trend Loss: 0.0576 | 0.0291
Epoch 145/300, trend Loss: 0.0576 | 0.0286
Epoch 146/300, trend Loss: 0.0574 | 0.0284
Epoch 147/300, trend Loss: 0.0572 | 0.0281
Epoch 148/300, trend Loss: 0.0571 | 0.0281
Epoch 149/300, trend Loss: 0.0571 | 0.0288
Epoch 150/300, trend Loss: 0.0572 | 0.0293
Epoch 151/300, trend Loss: 0.0571 | 0.0293
Epoch 152/300, trend Loss: 0.0570 | 0.0288
Epoch 153/300, trend Loss: 0.0569 | 0.0284
Epoch 154/300, trend Loss: 0.0568 | 0.0284
Epoch 155/300, trend Loss: 0.0567 | 0.0287
Epoch 156/300, trend Loss: 0.0567 | 0.0287
Epoch 157/300, trend Loss: 0.0566 | 0.0284
Epoch 158/300, trend Loss: 0.0564 | 0.0283
Epoch 159/300, trend Loss: 0.0563 | 0.0282
Epoch 160/300, trend Loss: 0.0563 | 0.0283
Epoch 161/300, trend Loss: 0.0563 | 0.0284
Epoch 162/300, trend Loss: 0.0563 | 0.0284
Epoch 163/300, trend Loss: 0.0564 | 0.0284
Epoch 164/300, trend Loss: 0.0565 | 0.0287
Epoch 165/300, trend Loss: 0.0564 | 0.0289
Epoch 166/300, trend Loss: 0.0564 | 0.0296
Epoch 167/300, trend Loss: 0.0564 | 0.0292
Epoch 168/300, trend Loss: 0.0564 | 0.0289
Epoch 169/300, trend Loss: 0.0561 | 0.0291
Epoch 170/300, trend Loss: 0.0558 | 0.0288
Epoch 171/300, trend Loss: 0.0557 | 0.0289
Epoch 172/300, trend Loss: 0.0556 | 0.0289
Epoch 173/300, trend Loss: 0.0554 | 0.0287
Epoch 174/300, trend Loss: 0.0553 | 0.0286
Epoch 175/300, trend Loss: 0.0551 | 0.0286
Epoch 176/300, trend Loss: 0.0550 | 0.0285
Epoch 177/300, trend Loss: 0.0550 | 0.0284
Epoch 178/300, trend Loss: 0.0549 | 0.0283
Epoch 179/300, trend Loss: 0.0548 | 0.0282
Epoch 180/300, trend Loss: 0.0547 | 0.0283
Epoch 181/300, trend Loss: 0.0547 | 0.0282
Epoch 182/300, trend Loss: 0.0546 | 0.0283
Epoch 183/300, trend Loss: 0.0546 | 0.0284
Epoch 184/300, trend Loss: 0.0545 | 0.0285
Epoch 185/300, trend Loss: 0.0545 | 0.0285
Epoch 186/300, trend Loss: 0.0544 | 0.0286
Epoch 187/300, trend Loss: 0.0544 | 0.0286
Epoch 188/300, trend Loss: 0.0543 | 0.0287
Epoch 189/300, trend Loss: 0.0543 | 0.0287
Epoch 190/300, trend Loss: 0.0542 | 0.0287
Epoch 191/300, trend Loss: 0.0542 | 0.0288
Epoch 192/300, trend Loss: 0.0541 | 0.0288
Epoch 193/300, trend Loss: 0.0541 | 0.0288
Epoch 194/300, trend Loss: 0.0540 | 0.0288
Epoch 195/300, trend Loss: 0.0540 | 0.0289
Epoch 196/300, trend Loss: 0.0540 | 0.0289
Epoch 197/300, trend Loss: 0.0539 | 0.0289
Epoch 198/300, trend Loss: 0.0539 | 0.0289
Epoch 199/300, trend Loss: 0.0539 | 0.0290
Epoch 200/300, trend Loss: 0.0539 | 0.0290
Epoch 201/300, trend Loss: 0.0539 | 0.0292
Epoch 202/300, trend Loss: 0.0540 | 0.0292
Epoch 203/300, trend Loss: 0.0539 | 0.0294
Epoch 204/300, trend Loss: 0.0538 | 0.0296
Epoch 205/300, trend Loss: 0.0538 | 0.0297
Epoch 206/300, trend Loss: 0.0537 | 0.0296
Epoch 207/300, trend Loss: 0.0536 | 0.0294
Epoch 208/300, trend Loss: 0.0535 | 0.0294
Epoch 209/300, trend Loss: 0.0534 | 0.0296
Epoch 210/300, trend Loss: 0.0534 | 0.0297
Epoch 211/300, trend Loss: 0.0533 | 0.0299
Epoch 212/300, trend Loss: 0.0533 | 0.0300
Epoch 213/300, trend Loss: 0.0532 | 0.0302
Epoch 214/300, trend Loss: 0.0531 | 0.0304
Epoch 215/300, trend Loss: 0.0531 | 0.0306
Epoch 216/300, trend Loss: 0.0530 | 0.0308
Epoch 217/300, trend Loss: 0.0530 | 0.0309
Epoch 218/300, trend Loss: 0.0529 | 0.0311
Epoch 219/300, trend Loss: 0.0529 | 0.0312
Epoch 220/300, trend Loss: 0.0528 | 0.0313
Epoch 221/300, trend Loss: 0.0528 | 0.0315
Epoch 222/300, trend Loss: 0.0527 | 0.0316
Epoch 223/300, trend Loss: 0.0526 | 0.0317
Epoch 224/300, trend Loss: 0.0525 | 0.0318
Epoch 225/300, trend Loss: 0.0525 | 0.0321
Epoch 226/300, trend Loss: 0.0524 | 0.0326
Epoch 227/300, trend Loss: 0.0523 | 0.0330
Epoch 228/300, trend Loss: 0.0520 | 0.0334
Epoch 229/300, trend Loss: 0.0516 | 0.0338
Epoch 230/300, trend Loss: 0.0512 | 0.0340
Epoch 231/300, trend Loss: 0.0509 | 0.0340
Epoch 232/300, trend Loss: 0.0507 | 0.0342
Epoch 233/300, trend Loss: 0.0505 | 0.0343
Epoch 234/300, trend Loss: 0.0503 | 0.0345
Epoch 235/300, trend Loss: 0.0501 | 0.0345
Epoch 236/300, trend Loss: 0.0499 | 0.0346
Epoch 237/300, trend Loss: 0.0497 | 0.0346
Epoch 238/300, trend Loss: 0.0496 | 0.0348
Epoch 239/300, trend Loss: 0.0494 | 0.0348
Epoch 240/300, trend Loss: 0.0493 | 0.0348
Epoch 241/300, trend Loss: 0.0491 | 0.0348
Epoch 242/300, trend Loss: 0.0489 | 0.0349
Epoch 243/300, trend Loss: 0.0487 | 0.0350
Epoch 244/300, trend Loss: 0.0486 | 0.0350
Epoch 245/300, trend Loss: 0.0483 | 0.0352
Epoch 246/300, trend Loss: 0.0482 | 0.0350
Epoch 247/300, trend Loss: 0.0479 | 0.0355
Epoch 248/300, trend Loss: 0.0481 | 0.0347
Epoch 249/300, trend Loss: 0.0477 | 0.0367
Epoch 250/300, trend Loss: 0.0503 | 0.0311
Epoch 251/300, trend Loss: 0.0476 | 0.0372
Epoch 252/300, trend Loss: 0.0491 | 0.0324
Epoch 253/300, trend Loss: 0.0475 | 0.0377
Epoch 254/300, trend Loss: 0.0488 | 0.0322
Epoch 255/300, trend Loss: 0.0472 | 0.0370
Epoch 256/300, trend Loss: 0.0478 | 0.0328
Epoch 257/300, trend Loss: 0.0469 | 0.0363
Epoch 258/300, trend Loss: 0.0474 | 0.0327
Epoch 259/300, trend Loss: 0.0468 | 0.0360
Epoch 260/300, trend Loss: 0.0471 | 0.0326
Epoch 261/300, trend Loss: 0.0465 | 0.0357
Epoch 262/300, trend Loss: 0.0467 | 0.0326
Epoch 263/300, trend Loss: 0.0463 | 0.0352
Epoch 264/300, trend Loss: 0.0465 | 0.0326
Epoch 265/300, trend Loss: 0.0461 | 0.0348
Epoch 266/300, trend Loss: 0.0462 | 0.0326
Epoch 267/300, trend Loss: 0.0459 | 0.0345
Epoch 268/300, trend Loss: 0.0460 | 0.0325
Epoch 269/300, trend Loss: 0.0458 | 0.0343
Epoch 270/300, trend Loss: 0.0458 | 0.0324
Epoch 271/300, trend Loss: 0.0457 | 0.0341
Epoch 272/300, trend Loss: 0.0457 | 0.0324
Epoch 273/300, trend Loss: 0.0455 | 0.0340
Epoch 274/300, trend Loss: 0.0455 | 0.0323
Epoch 275/300, trend Loss: 0.0454 | 0.0338
Epoch 276/300, trend Loss: 0.0454 | 0.0322
Epoch 277/300, trend Loss: 0.0453 | 0.0336
Epoch 278/300, trend Loss: 0.0453 | 0.0321
Epoch 279/300, trend Loss: 0.0452 | 0.0335
Epoch 280/300, trend Loss: 0.0452 | 0.0321
Epoch 281/300, trend Loss: 0.0451 | 0.0333
Epoch 282/300, trend Loss: 0.0450 | 0.0320
Epoch 283/300, trend Loss: 0.0450 | 0.0332
Epoch 284/300, trend Loss: 0.0449 | 0.0320
Epoch 285/300, trend Loss: 0.0449 | 0.0331
Epoch 286/300, trend Loss: 0.0448 | 0.0320
Epoch 287/300, trend Loss: 0.0448 | 0.0330
Epoch 288/300, trend Loss: 0.0447 | 0.0319
Epoch 289/300, trend Loss: 0.0447 | 0.0329
Epoch 290/300, trend Loss: 0.0447 | 0.0319
Epoch 291/300, trend Loss: 0.0446 | 0.0328
Epoch 292/300, trend Loss: 0.0446 | 0.0318
Epoch 293/300, trend Loss: 0.0445 | 0.0327
Epoch 294/300, trend Loss: 0.0445 | 0.0318
Epoch 295/300, trend Loss: 0.0445 | 0.0326
Epoch 296/300, trend Loss: 0.0444 | 0.0318
Epoch 297/300, trend Loss: 0.0444 | 0.0325
Epoch 298/300, trend Loss: 0.0444 | 0.0317
Epoch 299/300, trend Loss: 0.0443 | 0.0325
Epoch 300/300, trend Loss: 0.0443 | 0.0317
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.833003555552845, 'learning_rate': 0.00011521760220635876, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8226047512653859}
Epoch 1/300, seasonal_0 Loss: 0.2571 | 0.2681
Epoch 2/300, seasonal_0 Loss: 0.1569 | 0.1922
Epoch 3/300, seasonal_0 Loss: 0.1387 | 0.1354
Epoch 4/300, seasonal_0 Loss: 0.1291 | 0.1060
Epoch 5/300, seasonal_0 Loss: 0.1257 | 0.0948
Epoch 6/300, seasonal_0 Loss: 0.1270 | 0.1114
Epoch 7/300, seasonal_0 Loss: 0.1261 | 0.1410
Epoch 8/300, seasonal_0 Loss: 0.1224 | 0.0906
Epoch 9/300, seasonal_0 Loss: 0.1152 | 0.0872
Epoch 10/300, seasonal_0 Loss: 0.1232 | 0.1040
Epoch 11/300, seasonal_0 Loss: 0.1209 | 0.0850
Epoch 12/300, seasonal_0 Loss: 0.1090 | 0.0745
Epoch 13/300, seasonal_0 Loss: 0.1042 | 0.0740
Epoch 14/300, seasonal_0 Loss: 0.1027 | 0.0713
Epoch 15/300, seasonal_0 Loss: 0.1007 | 0.0686
Epoch 16/300, seasonal_0 Loss: 0.0992 | 0.0664
Epoch 17/300, seasonal_0 Loss: 0.0982 | 0.0645
Epoch 18/300, seasonal_0 Loss: 0.0971 | 0.0627
Epoch 19/300, seasonal_0 Loss: 0.0959 | 0.0612
Epoch 20/300, seasonal_0 Loss: 0.0951 | 0.0599
Epoch 21/300, seasonal_0 Loss: 0.0945 | 0.0589
Epoch 22/300, seasonal_0 Loss: 0.0937 | 0.0579
Epoch 23/300, seasonal_0 Loss: 0.0928 | 0.0568
Epoch 24/300, seasonal_0 Loss: 0.0919 | 0.0556
Epoch 25/300, seasonal_0 Loss: 0.0911 | 0.0548
Epoch 26/300, seasonal_0 Loss: 0.0905 | 0.0540
Epoch 27/300, seasonal_0 Loss: 0.0899 | 0.0534
Epoch 28/300, seasonal_0 Loss: 0.0894 | 0.0527
Epoch 29/300, seasonal_0 Loss: 0.0889 | 0.0520
Epoch 30/300, seasonal_0 Loss: 0.0884 | 0.0513
Epoch 31/300, seasonal_0 Loss: 0.0880 | 0.0506
Epoch 32/300, seasonal_0 Loss: 0.0875 | 0.0500
Epoch 33/300, seasonal_0 Loss: 0.0872 | 0.0494
Epoch 34/300, seasonal_0 Loss: 0.0868 | 0.0488
Epoch 35/300, seasonal_0 Loss: 0.0864 | 0.0482
Epoch 36/300, seasonal_0 Loss: 0.0861 | 0.0476
Epoch 37/300, seasonal_0 Loss: 0.0857 | 0.0473
Epoch 38/300, seasonal_0 Loss: 0.0852 | 0.0468
Epoch 39/300, seasonal_0 Loss: 0.0850 | 0.0464
Epoch 40/300, seasonal_0 Loss: 0.0846 | 0.0459
Epoch 41/300, seasonal_0 Loss: 0.0843 | 0.0455
Epoch 42/300, seasonal_0 Loss: 0.0840 | 0.0451
Epoch 43/300, seasonal_0 Loss: 0.0836 | 0.0449
Epoch 44/300, seasonal_0 Loss: 0.0834 | 0.0445
Epoch 45/300, seasonal_0 Loss: 0.0833 | 0.0442
Epoch 46/300, seasonal_0 Loss: 0.0832 | 0.0439
Epoch 47/300, seasonal_0 Loss: 0.0830 | 0.0437
Epoch 48/300, seasonal_0 Loss: 0.0827 | 0.0434
Epoch 49/300, seasonal_0 Loss: 0.0824 | 0.0432
Epoch 50/300, seasonal_0 Loss: 0.0824 | 0.0430
Epoch 51/300, seasonal_0 Loss: 0.0823 | 0.0428
Epoch 52/300, seasonal_0 Loss: 0.0820 | 0.0426
Epoch 53/300, seasonal_0 Loss: 0.0818 | 0.0425
Epoch 54/300, seasonal_0 Loss: 0.0815 | 0.0423
Epoch 55/300, seasonal_0 Loss: 0.0812 | 0.0421
Epoch 56/300, seasonal_0 Loss: 0.0810 | 0.0420
Epoch 57/300, seasonal_0 Loss: 0.0808 | 0.0419
Epoch 58/300, seasonal_0 Loss: 0.0806 | 0.0418
Epoch 59/300, seasonal_0 Loss: 0.0804 | 0.0417
Epoch 60/300, seasonal_0 Loss: 0.0803 | 0.0416
Epoch 61/300, seasonal_0 Loss: 0.0802 | 0.0417
Epoch 62/300, seasonal_0 Loss: 0.0801 | 0.0415
Epoch 63/300, seasonal_0 Loss: 0.0799 | 0.0414
Epoch 64/300, seasonal_0 Loss: 0.0798 | 0.0413
Epoch 65/300, seasonal_0 Loss: 0.0797 | 0.0412
Epoch 66/300, seasonal_0 Loss: 0.0795 | 0.0411
Epoch 67/300, seasonal_0 Loss: 0.0794 | 0.0411
Epoch 68/300, seasonal_0 Loss: 0.0793 | 0.0410
Epoch 69/300, seasonal_0 Loss: 0.0793 | 0.0409
Epoch 70/300, seasonal_0 Loss: 0.0792 | 0.0408
Epoch 71/300, seasonal_0 Loss: 0.0791 | 0.0407
Epoch 72/300, seasonal_0 Loss: 0.0790 | 0.0407
Epoch 73/300, seasonal_0 Loss: 0.0790 | 0.0406
Epoch 74/300, seasonal_0 Loss: 0.0789 | 0.0405
Epoch 75/300, seasonal_0 Loss: 0.0788 | 0.0405
Epoch 76/300, seasonal_0 Loss: 0.0787 | 0.0404
Epoch 77/300, seasonal_0 Loss: 0.0786 | 0.0403
Epoch 78/300, seasonal_0 Loss: 0.0785 | 0.0402
Epoch 79/300, seasonal_0 Loss: 0.0784 | 0.0401
Epoch 80/300, seasonal_0 Loss: 0.0783 | 0.0401
Epoch 81/300, seasonal_0 Loss: 0.0782 | 0.0400
Epoch 82/300, seasonal_0 Loss: 0.0782 | 0.0400
Epoch 83/300, seasonal_0 Loss: 0.0781 | 0.0399
Epoch 84/300, seasonal_0 Loss: 0.0780 | 0.0399
Epoch 85/300, seasonal_0 Loss: 0.0780 | 0.0398
Epoch 86/300, seasonal_0 Loss: 0.0779 | 0.0397
Epoch 87/300, seasonal_0 Loss: 0.0778 | 0.0397
Epoch 88/300, seasonal_0 Loss: 0.0778 | 0.0397
Epoch 89/300, seasonal_0 Loss: 0.0777 | 0.0396
Epoch 90/300, seasonal_0 Loss: 0.0777 | 0.0396
Epoch 91/300, seasonal_0 Loss: 0.0776 | 0.0395
Epoch 92/300, seasonal_0 Loss: 0.0776 | 0.0395
Epoch 93/300, seasonal_0 Loss: 0.0775 | 0.0394
Epoch 94/300, seasonal_0 Loss: 0.0775 | 0.0394
Epoch 95/300, seasonal_0 Loss: 0.0775 | 0.0394
Epoch 96/300, seasonal_0 Loss: 0.0774 | 0.0393
Epoch 97/300, seasonal_0 Loss: 0.0774 | 0.0393
Epoch 98/300, seasonal_0 Loss: 0.0774 | 0.0393
Epoch 99/300, seasonal_0 Loss: 0.0774 | 0.0393
Epoch 100/300, seasonal_0 Loss: 0.0774 | 0.0392
Epoch 101/300, seasonal_0 Loss: 0.0774 | 0.0392
Epoch 102/300, seasonal_0 Loss: 0.0774 | 0.0392
Epoch 103/300, seasonal_0 Loss: 0.0774 | 0.0393
Epoch 104/300, seasonal_0 Loss: 0.0775 | 0.0394
Epoch 105/300, seasonal_0 Loss: 0.0776 | 0.0394
Epoch 106/300, seasonal_0 Loss: 0.0776 | 0.0394
Epoch 107/300, seasonal_0 Loss: 0.0776 | 0.0394
Epoch 108/300, seasonal_0 Loss: 0.0775 | 0.0393
Epoch 109/300, seasonal_0 Loss: 0.0774 | 0.0393
Epoch 110/300, seasonal_0 Loss: 0.0772 | 0.0392
Epoch 111/300, seasonal_0 Loss: 0.0770 | 0.0391
Epoch 112/300, seasonal_0 Loss: 0.0768 | 0.0390
Epoch 113/300, seasonal_0 Loss: 0.0767 | 0.0390
Epoch 114/300, seasonal_0 Loss: 0.0766 | 0.0389
Epoch 115/300, seasonal_0 Loss: 0.0765 | 0.0389
Epoch 116/300, seasonal_0 Loss: 0.0765 | 0.0388
Epoch 117/300, seasonal_0 Loss: 0.0765 | 0.0388
Epoch 118/300, seasonal_0 Loss: 0.0764 | 0.0387
Epoch 119/300, seasonal_0 Loss: 0.0764 | 0.0387
Epoch 120/300, seasonal_0 Loss: 0.0764 | 0.0387
Epoch 121/300, seasonal_0 Loss: 0.0763 | 0.0387
Epoch 122/300, seasonal_0 Loss: 0.0763 | 0.0386
Epoch 123/300, seasonal_0 Loss: 0.0763 | 0.0386
Epoch 124/300, seasonal_0 Loss: 0.0763 | 0.0386
Epoch 125/300, seasonal_0 Loss: 0.0762 | 0.0386
Epoch 126/300, seasonal_0 Loss: 0.0762 | 0.0386
Epoch 127/300, seasonal_0 Loss: 0.0762 | 0.0386
Epoch 128/300, seasonal_0 Loss: 0.0762 | 0.0385
Epoch 129/300, seasonal_0 Loss: 0.0761 | 0.0385
Epoch 130/300, seasonal_0 Loss: 0.0761 | 0.0385
Epoch 131/300, seasonal_0 Loss: 0.0761 | 0.0385
Epoch 132/300, seasonal_0 Loss: 0.0761 | 0.0385
Epoch 133/300, seasonal_0 Loss: 0.0761 | 0.0385
Epoch 134/300, seasonal_0 Loss: 0.0760 | 0.0384
Epoch 135/300, seasonal_0 Loss: 0.0760 | 0.0384
Epoch 136/300, seasonal_0 Loss: 0.0760 | 0.0384
Epoch 137/300, seasonal_0 Loss: 0.0760 | 0.0384
Epoch 138/300, seasonal_0 Loss: 0.0760 | 0.0384
Epoch 139/300, seasonal_0 Loss: 0.0759 | 0.0384
Epoch 140/300, seasonal_0 Loss: 0.0759 | 0.0384
Epoch 141/300, seasonal_0 Loss: 0.0759 | 0.0383
Epoch 142/300, seasonal_0 Loss: 0.0759 | 0.0383
Epoch 143/300, seasonal_0 Loss: 0.0759 | 0.0383
Epoch 144/300, seasonal_0 Loss: 0.0759 | 0.0383
Epoch 145/300, seasonal_0 Loss: 0.0758 | 0.0383
Epoch 146/300, seasonal_0 Loss: 0.0758 | 0.0383
Epoch 147/300, seasonal_0 Loss: 0.0758 | 0.0383
Epoch 148/300, seasonal_0 Loss: 0.0758 | 0.0383
Epoch 149/300, seasonal_0 Loss: 0.0758 | 0.0383
Epoch 150/300, seasonal_0 Loss: 0.0758 | 0.0382
Epoch 151/300, seasonal_0 Loss: 0.0758 | 0.0382
Epoch 152/300, seasonal_0 Loss: 0.0758 | 0.0382
Epoch 153/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 154/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 155/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 156/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 157/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 158/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 159/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 160/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 161/300, seasonal_0 Loss: 0.0757 | 0.0382
Epoch 162/300, seasonal_0 Loss: 0.0756 | 0.0382
Epoch 163/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 164/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 165/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 166/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 167/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 168/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 169/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 170/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 171/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 172/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 173/300, seasonal_0 Loss: 0.0756 | 0.0381
Epoch 174/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 175/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 176/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 177/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 178/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 179/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 180/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 181/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 182/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 183/300, seasonal_0 Loss: 0.0755 | 0.0381
Epoch 184/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 185/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 186/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 187/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 188/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 189/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 190/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 191/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 192/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 193/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 194/300, seasonal_0 Loss: 0.0755 | 0.0380
Epoch 195/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 196/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 197/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 198/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 199/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 200/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 201/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 202/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 203/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 204/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 205/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 206/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 207/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 208/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 209/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 210/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 211/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 212/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 213/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 214/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 215/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 216/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 217/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 218/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 219/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 220/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 221/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 222/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 223/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 224/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 225/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 226/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 227/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 228/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 229/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 230/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 231/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 232/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 233/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 234/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 235/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 236/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 237/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 238/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 239/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 240/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 241/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 242/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 243/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 244/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 245/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 246/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 247/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 248/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 249/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 250/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 251/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 252/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 253/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 254/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 255/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 256/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 257/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 258/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 259/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 260/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 261/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 262/300, seasonal_0 Loss: 0.0754 | 0.0380
Epoch 263/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 264/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 265/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 266/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 267/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 268/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 269/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 270/300, seasonal_0 Loss: 0.0754 | 0.0379
Epoch 271/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 272/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 273/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 274/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 275/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 276/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 277/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 278/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 279/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 280/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 281/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 282/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 283/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 284/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 285/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 286/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 287/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 288/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 289/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 290/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 291/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 292/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 293/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 294/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 295/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 296/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 297/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 298/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 299/300, seasonal_0 Loss: 0.0753 | 0.0379
Epoch 300/300, seasonal_0 Loss: 0.0753 | 0.0379
Training seasonal_1 component with params: {'observation_period_num': 15, 'train_rates': 0.9383123444066815, 'learning_rate': 0.0004694591913094869, 'batch_size': 58, 'step_size': 4, 'gamma': 0.8603074228501688}
Epoch 1/300, seasonal_1 Loss: 0.2297 | 0.1518
Epoch 2/300, seasonal_1 Loss: 0.1412 | 0.1105
Epoch 3/300, seasonal_1 Loss: 0.1167 | 0.0921
Epoch 4/300, seasonal_1 Loss: 0.1169 | 0.0924
Epoch 5/300, seasonal_1 Loss: 0.1088 | 0.1148
Epoch 6/300, seasonal_1 Loss: 0.1041 | 0.1062
Epoch 7/300, seasonal_1 Loss: 0.1020 | 0.0957
Epoch 8/300, seasonal_1 Loss: 0.1027 | 0.0904
Epoch 9/300, seasonal_1 Loss: 0.1028 | 0.0651
Epoch 10/300, seasonal_1 Loss: 0.1030 | 0.0685
Epoch 11/300, seasonal_1 Loss: 0.0895 | 0.0584
Epoch 12/300, seasonal_1 Loss: 0.0849 | 0.0523
Epoch 13/300, seasonal_1 Loss: 0.0827 | 0.0499
Epoch 14/300, seasonal_1 Loss: 0.0822 | 0.0483
Epoch 15/300, seasonal_1 Loss: 0.0806 | 0.0489
Epoch 16/300, seasonal_1 Loss: 0.0814 | 0.0471
Epoch 17/300, seasonal_1 Loss: 0.0775 | 0.0460
Epoch 18/300, seasonal_1 Loss: 0.0763 | 0.0451
Epoch 19/300, seasonal_1 Loss: 0.0777 | 0.0651
Epoch 20/300, seasonal_1 Loss: 0.0768 | 0.0483
Epoch 21/300, seasonal_1 Loss: 0.0735 | 0.0469
Epoch 22/300, seasonal_1 Loss: 0.0722 | 0.0438
Epoch 23/300, seasonal_1 Loss: 0.0709 | 0.0426
Epoch 24/300, seasonal_1 Loss: 0.0703 | 0.0419
Epoch 25/300, seasonal_1 Loss: 0.0695 | 0.0413
Epoch 26/300, seasonal_1 Loss: 0.0689 | 0.0408
Epoch 27/300, seasonal_1 Loss: 0.0683 | 0.0403
Epoch 28/300, seasonal_1 Loss: 0.0678 | 0.0400
Epoch 29/300, seasonal_1 Loss: 0.0674 | 0.0394
Epoch 30/300, seasonal_1 Loss: 0.0671 | 0.0393
Epoch 31/300, seasonal_1 Loss: 0.0669 | 0.0382
Epoch 32/300, seasonal_1 Loss: 0.0667 | 0.0382
Epoch 33/300, seasonal_1 Loss: 0.0668 | 0.0367
Epoch 34/300, seasonal_1 Loss: 0.0671 | 0.0365
Epoch 35/300, seasonal_1 Loss: 0.0669 | 0.0367
Epoch 36/300, seasonal_1 Loss: 0.0668 | 0.0366
Epoch 37/300, seasonal_1 Loss: 0.0660 | 0.0364
Epoch 38/300, seasonal_1 Loss: 0.0659 | 0.0361
Epoch 39/300, seasonal_1 Loss: 0.0666 | 0.0361
Epoch 40/300, seasonal_1 Loss: 0.0674 | 0.0364
Epoch 41/300, seasonal_1 Loss: 0.0678 | 0.0380
Epoch 42/300, seasonal_1 Loss: 0.0667 | 0.0370
Epoch 43/300, seasonal_1 Loss: 0.0652 | 0.0362
Epoch 44/300, seasonal_1 Loss: 0.0647 | 0.0357
Epoch 45/300, seasonal_1 Loss: 0.0651 | 0.0356
Epoch 46/300, seasonal_1 Loss: 0.0654 | 0.0355
Epoch 47/300, seasonal_1 Loss: 0.0654 | 0.0353
Epoch 48/300, seasonal_1 Loss: 0.0650 | 0.0350
Epoch 49/300, seasonal_1 Loss: 0.0649 | 0.0348
Epoch 50/300, seasonal_1 Loss: 0.0646 | 0.0349
Epoch 51/300, seasonal_1 Loss: 0.0644 | 0.0351
Epoch 52/300, seasonal_1 Loss: 0.0641 | 0.0353
Epoch 53/300, seasonal_1 Loss: 0.0639 | 0.0356
Epoch 54/300, seasonal_1 Loss: 0.0636 | 0.0357
Epoch 55/300, seasonal_1 Loss: 0.0634 | 0.0357
Epoch 56/300, seasonal_1 Loss: 0.0632 | 0.0356
Epoch 57/300, seasonal_1 Loss: 0.0631 | 0.0355
Epoch 58/300, seasonal_1 Loss: 0.0629 | 0.0353
Epoch 59/300, seasonal_1 Loss: 0.0628 | 0.0352
Epoch 60/300, seasonal_1 Loss: 0.0628 | 0.0351
Epoch 61/300, seasonal_1 Loss: 0.0627 | 0.0350
Epoch 62/300, seasonal_1 Loss: 0.0627 | 0.0349
Epoch 63/300, seasonal_1 Loss: 0.0626 | 0.0349
Epoch 64/300, seasonal_1 Loss: 0.0626 | 0.0348
Epoch 65/300, seasonal_1 Loss: 0.0626 | 0.0348
Epoch 66/300, seasonal_1 Loss: 0.0626 | 0.0347
Epoch 67/300, seasonal_1 Loss: 0.0625 | 0.0347
Epoch 68/300, seasonal_1 Loss: 0.0625 | 0.0346
Epoch 69/300, seasonal_1 Loss: 0.0625 | 0.0346
Epoch 70/300, seasonal_1 Loss: 0.0624 | 0.0346
Epoch 71/300, seasonal_1 Loss: 0.0624 | 0.0345
Epoch 72/300, seasonal_1 Loss: 0.0624 | 0.0345
Epoch 73/300, seasonal_1 Loss: 0.0624 | 0.0345
Epoch 74/300, seasonal_1 Loss: 0.0624 | 0.0345
Epoch 75/300, seasonal_1 Loss: 0.0624 | 0.0345
Epoch 76/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 77/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 78/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 79/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 80/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 81/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 82/300, seasonal_1 Loss: 0.0623 | 0.0344
Epoch 83/300, seasonal_1 Loss: 0.0623 | 0.0343
Epoch 84/300, seasonal_1 Loss: 0.0623 | 0.0343
Epoch 85/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 86/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 87/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 88/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 89/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 90/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 91/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 92/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 93/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 94/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 95/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 96/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 97/300, seasonal_1 Loss: 0.0622 | 0.0343
Epoch 98/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 99/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 100/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 101/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 102/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 103/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 104/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 105/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 106/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 107/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 108/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 109/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 110/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 111/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 112/300, seasonal_1 Loss: 0.0622 | 0.0342
Epoch 113/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 114/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 115/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 116/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 117/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 118/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 119/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 120/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 121/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 122/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 123/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 124/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 125/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 126/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 127/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 128/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 129/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 130/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 131/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 132/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 133/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 134/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 135/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 136/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 137/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 138/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 139/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 140/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 141/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 142/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 143/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 144/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 145/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 146/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 147/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 148/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 149/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 150/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 151/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 152/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 153/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 154/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 155/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 156/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 157/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 158/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 159/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 160/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 161/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 162/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 163/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 164/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 165/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 166/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 167/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 168/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 169/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 170/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 171/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 172/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 173/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 174/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 175/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 176/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 177/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 178/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 179/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 180/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 181/300, seasonal_1 Loss: 0.0621 | 0.0342
Epoch 182/300, seasonal_1 Loss: 0.0621 | 0.0342
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 8, 'train_rates': 0.9651148190175596, 'learning_rate': 0.0009536241953108215, 'batch_size': 87, 'step_size': 15, 'gamma': 0.8155769297215802}
Epoch 1/300, seasonal_2 Loss: 0.4069 | 0.1882
Epoch 2/300, seasonal_2 Loss: 0.1436 | 0.1211
Epoch 3/300, seasonal_2 Loss: 0.1326 | 0.1257
Epoch 4/300, seasonal_2 Loss: 0.1158 | 0.1075
Epoch 5/300, seasonal_2 Loss: 0.1086 | 0.0982
Epoch 6/300, seasonal_2 Loss: 0.1063 | 0.0897
Epoch 7/300, seasonal_2 Loss: 0.1067 | 0.0807
Epoch 8/300, seasonal_2 Loss: 0.1080 | 0.0714
Epoch 9/300, seasonal_2 Loss: 0.1355 | 0.1222
Epoch 10/300, seasonal_2 Loss: 0.1189 | 0.0867
Epoch 11/300, seasonal_2 Loss: 0.0947 | 0.0686
Epoch 12/300, seasonal_2 Loss: 0.0879 | 0.0636
Epoch 13/300, seasonal_2 Loss: 0.0886 | 0.0668
Epoch 14/300, seasonal_2 Loss: 0.0937 | 0.0788
Epoch 15/300, seasonal_2 Loss: 0.0975 | 0.0637
Epoch 16/300, seasonal_2 Loss: 0.0896 | 0.0606
Epoch 17/300, seasonal_2 Loss: 0.0870 | 0.0537
Epoch 18/300, seasonal_2 Loss: 0.0792 | 0.0482
Epoch 19/300, seasonal_2 Loss: 0.0766 | 0.0468
Epoch 20/300, seasonal_2 Loss: 0.0761 | 0.0470
Epoch 21/300, seasonal_2 Loss: 0.0761 | 0.0483
Epoch 22/300, seasonal_2 Loss: 0.0758 | 0.0488
Epoch 23/300, seasonal_2 Loss: 0.0769 | 0.0499
Epoch 24/300, seasonal_2 Loss: 0.0781 | 0.0656
Epoch 25/300, seasonal_2 Loss: 0.0787 | 0.0565
Epoch 26/300, seasonal_2 Loss: 0.0795 | 0.0687
Epoch 27/300, seasonal_2 Loss: 0.0823 | 0.0510
Epoch 28/300, seasonal_2 Loss: 0.0763 | 0.0462
Epoch 29/300, seasonal_2 Loss: 0.0809 | 0.0479
Epoch 30/300, seasonal_2 Loss: 0.0761 | 0.0517
Epoch 31/300, seasonal_2 Loss: 0.0725 | 0.0477
Epoch 32/300, seasonal_2 Loss: 0.0700 | 0.0437
Epoch 33/300, seasonal_2 Loss: 0.0674 | 0.0448
Epoch 34/300, seasonal_2 Loss: 0.0668 | 0.0450
Epoch 35/300, seasonal_2 Loss: 0.0657 | 0.0439
Epoch 36/300, seasonal_2 Loss: 0.0650 | 0.0424
Epoch 37/300, seasonal_2 Loss: 0.0646 | 0.0416
Epoch 38/300, seasonal_2 Loss: 0.0643 | 0.0413
Epoch 39/300, seasonal_2 Loss: 0.0642 | 0.0402
Epoch 40/300, seasonal_2 Loss: 0.0652 | 0.0408
Epoch 41/300, seasonal_2 Loss: 0.0656 | 0.0420
Epoch 42/300, seasonal_2 Loss: 0.0657 | 0.0432
Epoch 43/300, seasonal_2 Loss: 0.0653 | 0.0424
Epoch 44/300, seasonal_2 Loss: 0.0643 | 0.0393
Epoch 45/300, seasonal_2 Loss: 0.0642 | 0.0401
Epoch 46/300, seasonal_2 Loss: 0.0668 | 0.0407
Epoch 47/300, seasonal_2 Loss: 0.0655 | 0.0407
Epoch 48/300, seasonal_2 Loss: 0.0637 | 0.0421
Epoch 49/300, seasonal_2 Loss: 0.0671 | 0.0405
Epoch 50/300, seasonal_2 Loss: 0.0682 | 0.0484
Epoch 51/300, seasonal_2 Loss: 0.0674 | 0.0473
Epoch 52/300, seasonal_2 Loss: 0.0676 | 0.0446
Epoch 53/300, seasonal_2 Loss: 0.0623 | 0.0391
Epoch 54/300, seasonal_2 Loss: 0.0609 | 0.0393
Epoch 55/300, seasonal_2 Loss: 0.0604 | 0.0359
Epoch 56/300, seasonal_2 Loss: 0.0606 | 0.0413
Epoch 57/300, seasonal_2 Loss: 0.0608 | 0.0391
Epoch 58/300, seasonal_2 Loss: 0.0604 | 0.0390
Epoch 59/300, seasonal_2 Loss: 0.0598 | 0.0387
Epoch 60/300, seasonal_2 Loss: 0.0599 | 0.0385
Epoch 61/300, seasonal_2 Loss: 0.0598 | 0.0423
Epoch 62/300, seasonal_2 Loss: 0.0597 | 0.0409
Epoch 63/300, seasonal_2 Loss: 0.0590 | 0.0397
Epoch 64/300, seasonal_2 Loss: 0.0584 | 0.0377
Epoch 65/300, seasonal_2 Loss: 0.0583 | 0.0351
Epoch 66/300, seasonal_2 Loss: 0.0591 | 0.0343
Epoch 67/300, seasonal_2 Loss: 0.0606 | 0.0347
Epoch 68/300, seasonal_2 Loss: 0.0609 | 0.0356
Epoch 69/300, seasonal_2 Loss: 0.0610 | 0.0365
Epoch 70/300, seasonal_2 Loss: 0.0585 | 0.0322
Epoch 71/300, seasonal_2 Loss: 0.0566 | 0.0312
Epoch 72/300, seasonal_2 Loss: 0.0561 | 0.0319
Epoch 73/300, seasonal_2 Loss: 0.0559 | 0.0319
Epoch 74/300, seasonal_2 Loss: 0.0557 | 0.0319
Epoch 75/300, seasonal_2 Loss: 0.0551 | 0.0313
Epoch 76/300, seasonal_2 Loss: 0.0547 | 0.0306
Epoch 77/300, seasonal_2 Loss: 0.0543 | 0.0301
Epoch 78/300, seasonal_2 Loss: 0.0543 | 0.0296
Epoch 79/300, seasonal_2 Loss: 0.0542 | 0.0293
Epoch 80/300, seasonal_2 Loss: 0.0542 | 0.0293
Epoch 81/300, seasonal_2 Loss: 0.0543 | 0.0294
Epoch 82/300, seasonal_2 Loss: 0.0544 | 0.0295
Epoch 83/300, seasonal_2 Loss: 0.0547 | 0.0296
Epoch 84/300, seasonal_2 Loss: 0.0552 | 0.0291
Epoch 85/300, seasonal_2 Loss: 0.0558 | 0.0295
Epoch 86/300, seasonal_2 Loss: 0.0565 | 0.0302
Epoch 87/300, seasonal_2 Loss: 0.0569 | 0.0329
Epoch 88/300, seasonal_2 Loss: 0.0565 | 0.0317
Epoch 89/300, seasonal_2 Loss: 0.0547 | 0.0301
Epoch 90/300, seasonal_2 Loss: 0.0545 | 0.0286
Epoch 91/300, seasonal_2 Loss: 0.0551 | 0.0290
Epoch 92/300, seasonal_2 Loss: 0.0556 | 0.0298
Epoch 93/300, seasonal_2 Loss: 0.0559 | 0.0304
Epoch 94/300, seasonal_2 Loss: 0.0560 | 0.0301
Epoch 95/300, seasonal_2 Loss: 0.0561 | 0.0298
Epoch 96/300, seasonal_2 Loss: 0.0555 | 0.0290
Epoch 97/300, seasonal_2 Loss: 0.0544 | 0.0286
Epoch 98/300, seasonal_2 Loss: 0.0534 | 0.0283
Epoch 99/300, seasonal_2 Loss: 0.0531 | 0.0287
Epoch 100/300, seasonal_2 Loss: 0.0529 | 0.0291
Epoch 101/300, seasonal_2 Loss: 0.0528 | 0.0294
Epoch 102/300, seasonal_2 Loss: 0.0527 | 0.0288
Epoch 103/300, seasonal_2 Loss: 0.0532 | 0.0283
Epoch 104/300, seasonal_2 Loss: 0.0551 | 0.0339
Epoch 105/300, seasonal_2 Loss: 0.0560 | 0.0305
Epoch 106/300, seasonal_2 Loss: 0.0547 | 0.0327
Epoch 107/300, seasonal_2 Loss: 0.0548 | 0.0284
Epoch 108/300, seasonal_2 Loss: 0.0532 | 0.0309
Epoch 109/300, seasonal_2 Loss: 0.0536 | 0.0280
Epoch 110/300, seasonal_2 Loss: 0.0520 | 0.0295
Epoch 111/300, seasonal_2 Loss: 0.0516 | 0.0291
Epoch 112/300, seasonal_2 Loss: 0.0512 | 0.0283
Epoch 113/300, seasonal_2 Loss: 0.0511 | 0.0277
Epoch 114/300, seasonal_2 Loss: 0.0513 | 0.0275
Epoch 115/300, seasonal_2 Loss: 0.0515 | 0.0276
Epoch 116/300, seasonal_2 Loss: 0.0516 | 0.0278
Epoch 117/300, seasonal_2 Loss: 0.0515 | 0.0278
Epoch 118/300, seasonal_2 Loss: 0.0513 | 0.0277
Epoch 119/300, seasonal_2 Loss: 0.0510 | 0.0275
Epoch 120/300, seasonal_2 Loss: 0.0506 | 0.0274
Epoch 121/300, seasonal_2 Loss: 0.0505 | 0.0280
Epoch 122/300, seasonal_2 Loss: 0.0507 | 0.0289
Epoch 123/300, seasonal_2 Loss: 0.0513 | 0.0304
Epoch 124/300, seasonal_2 Loss: 0.0515 | 0.0292
Epoch 125/300, seasonal_2 Loss: 0.0507 | 0.0281
Epoch 126/300, seasonal_2 Loss: 0.0502 | 0.0277
Epoch 127/300, seasonal_2 Loss: 0.0500 | 0.0277
Epoch 128/300, seasonal_2 Loss: 0.0498 | 0.0276
Epoch 129/300, seasonal_2 Loss: 0.0496 | 0.0274
Epoch 130/300, seasonal_2 Loss: 0.0494 | 0.0273
Epoch 131/300, seasonal_2 Loss: 0.0494 | 0.0272
Epoch 132/300, seasonal_2 Loss: 0.0495 | 0.0271
Epoch 133/300, seasonal_2 Loss: 0.0495 | 0.0271
Epoch 134/300, seasonal_2 Loss: 0.0494 | 0.0271
Epoch 135/300, seasonal_2 Loss: 0.0493 | 0.0271
Epoch 136/300, seasonal_2 Loss: 0.0492 | 0.0272
Epoch 137/300, seasonal_2 Loss: 0.0490 | 0.0274
Epoch 138/300, seasonal_2 Loss: 0.0490 | 0.0277
Epoch 139/300, seasonal_2 Loss: 0.0490 | 0.0280
Epoch 140/300, seasonal_2 Loss: 0.0491 | 0.0283
Epoch 141/300, seasonal_2 Loss: 0.0491 | 0.0285
Epoch 142/300, seasonal_2 Loss: 0.0491 | 0.0285
Epoch 143/300, seasonal_2 Loss: 0.0490 | 0.0282
Epoch 144/300, seasonal_2 Loss: 0.0489 | 0.0278
Epoch 145/300, seasonal_2 Loss: 0.0487 | 0.0274
Epoch 146/300, seasonal_2 Loss: 0.0487 | 0.0273
Epoch 147/300, seasonal_2 Loss: 0.0487 | 0.0273
Epoch 148/300, seasonal_2 Loss: 0.0487 | 0.0274
Epoch 149/300, seasonal_2 Loss: 0.0486 | 0.0274
Epoch 150/300, seasonal_2 Loss: 0.0486 | 0.0276
Epoch 151/300, seasonal_2 Loss: 0.0485 | 0.0277
Epoch 152/300, seasonal_2 Loss: 0.0484 | 0.0278
Epoch 153/300, seasonal_2 Loss: 0.0484 | 0.0279
Epoch 154/300, seasonal_2 Loss: 0.0484 | 0.0280
Epoch 155/300, seasonal_2 Loss: 0.0484 | 0.0280
Epoch 156/300, seasonal_2 Loss: 0.0483 | 0.0280
Epoch 157/300, seasonal_2 Loss: 0.0483 | 0.0280
Epoch 158/300, seasonal_2 Loss: 0.0483 | 0.0280
Epoch 159/300, seasonal_2 Loss: 0.0482 | 0.0279
Epoch 160/300, seasonal_2 Loss: 0.0482 | 0.0279
Epoch 161/300, seasonal_2 Loss: 0.0482 | 0.0279
Epoch 162/300, seasonal_2 Loss: 0.0482 | 0.0279
Epoch 163/300, seasonal_2 Loss: 0.0481 | 0.0280
Epoch 164/300, seasonal_2 Loss: 0.0481 | 0.0280
Epoch 165/300, seasonal_2 Loss: 0.0481 | 0.0280
Epoch 166/300, seasonal_2 Loss: 0.0481 | 0.0280
Epoch 167/300, seasonal_2 Loss: 0.0480 | 0.0281
Epoch 168/300, seasonal_2 Loss: 0.0480 | 0.0281
Epoch 169/300, seasonal_2 Loss: 0.0480 | 0.0281
Epoch 170/300, seasonal_2 Loss: 0.0480 | 0.0281
Epoch 171/300, seasonal_2 Loss: 0.0480 | 0.0281
Epoch 172/300, seasonal_2 Loss: 0.0479 | 0.0282
Epoch 173/300, seasonal_2 Loss: 0.0479 | 0.0282
Epoch 174/300, seasonal_2 Loss: 0.0479 | 0.0282
Epoch 175/300, seasonal_2 Loss: 0.0479 | 0.0282
Epoch 176/300, seasonal_2 Loss: 0.0479 | 0.0282
Epoch 177/300, seasonal_2 Loss: 0.0479 | 0.0282
Epoch 178/300, seasonal_2 Loss: 0.0478 | 0.0282
Epoch 179/300, seasonal_2 Loss: 0.0478 | 0.0282
Epoch 180/300, seasonal_2 Loss: 0.0478 | 0.0282
Epoch 181/300, seasonal_2 Loss: 0.0478 | 0.0282
Epoch 182/300, seasonal_2 Loss: 0.0478 | 0.0283
Epoch 183/300, seasonal_2 Loss: 0.0478 | 0.0283
Epoch 184/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 185/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 186/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 187/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 188/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 189/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 190/300, seasonal_2 Loss: 0.0477 | 0.0283
Epoch 191/300, seasonal_2 Loss: 0.0476 | 0.0283
Epoch 192/300, seasonal_2 Loss: 0.0476 | 0.0283
Epoch 193/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 194/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 195/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 196/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 197/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 198/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 199/300, seasonal_2 Loss: 0.0476 | 0.0284
Epoch 200/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 201/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 202/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 203/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 204/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 205/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 206/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 207/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 208/300, seasonal_2 Loss: 0.0475 | 0.0284
Epoch 209/300, seasonal_2 Loss: 0.0475 | 0.0285
Epoch 210/300, seasonal_2 Loss: 0.0475 | 0.0285
Epoch 211/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 212/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 213/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 214/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 215/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 216/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 217/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 218/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 219/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 220/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 221/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 222/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 223/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 224/300, seasonal_2 Loss: 0.0474 | 0.0285
Epoch 225/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 226/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 227/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 228/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 229/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 230/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 231/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 232/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 233/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 234/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 235/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 236/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 237/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 238/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 239/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 240/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 241/300, seasonal_2 Loss: 0.0473 | 0.0285
Epoch 242/300, seasonal_2 Loss: 0.0473 | 0.0286
Epoch 243/300, seasonal_2 Loss: 0.0473 | 0.0286
Epoch 244/300, seasonal_2 Loss: 0.0473 | 0.0286
Epoch 245/300, seasonal_2 Loss: 0.0473 | 0.0286
Epoch 246/300, seasonal_2 Loss: 0.0473 | 0.0286
Epoch 247/300, seasonal_2 Loss: 0.0473 | 0.0286
Epoch 248/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 249/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 250/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 251/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 252/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 253/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 254/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 255/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 256/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 257/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 258/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 259/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 260/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 261/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 262/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 263/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 264/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 265/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 266/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 267/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 268/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 269/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 270/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 271/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 272/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 273/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 274/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 275/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 276/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 277/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 278/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 279/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 280/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 281/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 282/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 283/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 284/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 285/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 286/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 287/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 288/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 289/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 290/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 291/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 292/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 293/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 294/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 295/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 296/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 297/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 298/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 299/300, seasonal_2 Loss: 0.0472 | 0.0286
Epoch 300/300, seasonal_2 Loss: 0.0472 | 0.0286
Training seasonal_3 component with params: {'observation_period_num': 15, 'train_rates': 0.8942770281399419, 'learning_rate': 7.956973552490178e-05, 'batch_size': 32, 'step_size': 7, 'gamma': 0.9674011027032974}
Epoch 1/300, seasonal_3 Loss: 0.3976 | 0.3063
Epoch 2/300, seasonal_3 Loss: 0.1807 | 0.1961
Epoch 3/300, seasonal_3 Loss: 0.1438 | 0.1415
Epoch 4/300, seasonal_3 Loss: 0.1277 | 0.1178
Epoch 5/300, seasonal_3 Loss: 0.1202 | 0.1052
Epoch 6/300, seasonal_3 Loss: 0.1154 | 0.0965
Epoch 7/300, seasonal_3 Loss: 0.1111 | 0.0905
Epoch 8/300, seasonal_3 Loss: 0.1072 | 0.0858
Epoch 9/300, seasonal_3 Loss: 0.1038 | 0.0825
Epoch 10/300, seasonal_3 Loss: 0.1008 | 0.0798
Epoch 11/300, seasonal_3 Loss: 0.0981 | 0.0774
Epoch 12/300, seasonal_3 Loss: 0.0958 | 0.0744
Epoch 13/300, seasonal_3 Loss: 0.0937 | 0.0719
Epoch 14/300, seasonal_3 Loss: 0.0918 | 0.0696
Epoch 15/300, seasonal_3 Loss: 0.0901 | 0.0669
Epoch 16/300, seasonal_3 Loss: 0.0885 | 0.0647
Epoch 17/300, seasonal_3 Loss: 0.0873 | 0.0631
Epoch 18/300, seasonal_3 Loss: 0.0862 | 0.0615
Epoch 19/300, seasonal_3 Loss: 0.0855 | 0.0596
Epoch 20/300, seasonal_3 Loss: 0.0843 | 0.0579
Epoch 21/300, seasonal_3 Loss: 0.0831 | 0.0569
Epoch 22/300, seasonal_3 Loss: 0.0825 | 0.0561
Epoch 23/300, seasonal_3 Loss: 0.0821 | 0.0555
Epoch 24/300, seasonal_3 Loss: 0.0819 | 0.0552
Epoch 25/300, seasonal_3 Loss: 0.0817 | 0.0550
Epoch 26/300, seasonal_3 Loss: 0.0816 | 0.0541
Epoch 27/300, seasonal_3 Loss: 0.0812 | 0.0537
Epoch 28/300, seasonal_3 Loss: 0.0813 | 0.0539
Epoch 29/300, seasonal_3 Loss: 0.0821 | 0.0525
Epoch 30/300, seasonal_3 Loss: 0.0826 | 0.0511
Epoch 31/300, seasonal_3 Loss: 0.0824 | 0.0502
Epoch 32/300, seasonal_3 Loss: 0.0808 | 0.0499
Epoch 33/300, seasonal_3 Loss: 0.0785 | 0.0485
Epoch 34/300, seasonal_3 Loss: 0.0774 | 0.0536
Epoch 35/300, seasonal_3 Loss: 0.0771 | 0.0574
Epoch 36/300, seasonal_3 Loss: 0.0744 | 0.0481
Epoch 37/300, seasonal_3 Loss: 0.0736 | 0.0488
Epoch 38/300, seasonal_3 Loss: 0.0725 | 0.0457
Epoch 39/300, seasonal_3 Loss: 0.0717 | 0.0451
Epoch 40/300, seasonal_3 Loss: 0.0708 | 0.0440
Epoch 41/300, seasonal_3 Loss: 0.0702 | 0.0434
Epoch 42/300, seasonal_3 Loss: 0.0694 | 0.0428
Epoch 43/300, seasonal_3 Loss: 0.0688 | 0.0423
Epoch 44/300, seasonal_3 Loss: 0.0682 | 0.0419
Epoch 45/300, seasonal_3 Loss: 0.0677 | 0.0415
Epoch 46/300, seasonal_3 Loss: 0.0672 | 0.0413
Epoch 47/300, seasonal_3 Loss: 0.0668 | 0.0409
Epoch 48/300, seasonal_3 Loss: 0.0664 | 0.0407
Epoch 49/300, seasonal_3 Loss: 0.0660 | 0.0406
Epoch 50/300, seasonal_3 Loss: 0.0656 | 0.0402
Epoch 51/300, seasonal_3 Loss: 0.0653 | 0.0401
Epoch 52/300, seasonal_3 Loss: 0.0650 | 0.0399
Epoch 53/300, seasonal_3 Loss: 0.0647 | 0.0398
Epoch 54/300, seasonal_3 Loss: 0.0644 | 0.0394
Epoch 55/300, seasonal_3 Loss: 0.0641 | 0.0393
Epoch 56/300, seasonal_3 Loss: 0.0639 | 0.0391
Epoch 57/300, seasonal_3 Loss: 0.0637 | 0.0387
Epoch 58/300, seasonal_3 Loss: 0.0635 | 0.0387
Epoch 59/300, seasonal_3 Loss: 0.0634 | 0.0387
Epoch 60/300, seasonal_3 Loss: 0.0633 | 0.0389
Epoch 61/300, seasonal_3 Loss: 0.0633 | 0.0399
Epoch 62/300, seasonal_3 Loss: 0.0633 | 0.0408
Epoch 63/300, seasonal_3 Loss: 0.0632 | 0.0409
Epoch 64/300, seasonal_3 Loss: 0.0628 | 0.0398
Epoch 65/300, seasonal_3 Loss: 0.0622 | 0.0392
Epoch 66/300, seasonal_3 Loss: 0.0618 | 0.0389
Epoch 67/300, seasonal_3 Loss: 0.0614 | 0.0386
Epoch 68/300, seasonal_3 Loss: 0.0611 | 0.0380
Epoch 69/300, seasonal_3 Loss: 0.0608 | 0.0379
Epoch 70/300, seasonal_3 Loss: 0.0606 | 0.0379
Epoch 71/300, seasonal_3 Loss: 0.0603 | 0.0374
Epoch 72/300, seasonal_3 Loss: 0.0601 | 0.0375
Epoch 73/300, seasonal_3 Loss: 0.0599 | 0.0375
Epoch 74/300, seasonal_3 Loss: 0.0596 | 0.0376
Epoch 75/300, seasonal_3 Loss: 0.0594 | 0.0374
Epoch 76/300, seasonal_3 Loss: 0.0592 | 0.0376
Epoch 77/300, seasonal_3 Loss: 0.0590 | 0.0378
Epoch 78/300, seasonal_3 Loss: 0.0587 | 0.0376
Epoch 79/300, seasonal_3 Loss: 0.0586 | 0.0379
Epoch 80/300, seasonal_3 Loss: 0.0584 | 0.0381
Epoch 81/300, seasonal_3 Loss: 0.0582 | 0.0384
Epoch 82/300, seasonal_3 Loss: 0.0580 | 0.0385
Epoch 83/300, seasonal_3 Loss: 0.0578 | 0.0389
Epoch 84/300, seasonal_3 Loss: 0.0576 | 0.0394
Epoch 85/300, seasonal_3 Loss: 0.0575 | 0.0395
Epoch 86/300, seasonal_3 Loss: 0.0573 | 0.0403
Epoch 87/300, seasonal_3 Loss: 0.0572 | 0.0412
Epoch 88/300, seasonal_3 Loss: 0.0570 | 0.0425
Epoch 89/300, seasonal_3 Loss: 0.0569 | 0.0444
Epoch 90/300, seasonal_3 Loss: 0.0568 | 0.0472
Epoch 91/300, seasonal_3 Loss: 0.0564 | 0.0498
Epoch 92/300, seasonal_3 Loss: 0.0558 | 0.0534
Epoch 93/300, seasonal_3 Loss: 0.0552 | 0.0552
Epoch 94/300, seasonal_3 Loss: 0.0544 | 0.0554
Epoch 95/300, seasonal_3 Loss: 0.0536 | 0.0538
Epoch 96/300, seasonal_3 Loss: 0.0527 | 0.0517
Epoch 97/300, seasonal_3 Loss: 0.0520 | 0.0493
Epoch 98/300, seasonal_3 Loss: 0.0512 | 0.0474
Epoch 99/300, seasonal_3 Loss: 0.0506 | 0.0459
Epoch 100/300, seasonal_3 Loss: 0.0501 | 0.0452
Epoch 101/300, seasonal_3 Loss: 0.0497 | 0.0444
Epoch 102/300, seasonal_3 Loss: 0.0494 | 0.0446
Epoch 103/300, seasonal_3 Loss: 0.0491 | 0.0432
Epoch 104/300, seasonal_3 Loss: 0.0490 | 0.0489
Epoch 105/300, seasonal_3 Loss: 0.0486 | 0.0414
Epoch 106/300, seasonal_3 Loss: 0.0538 | 0.1328
Epoch 107/300, seasonal_3 Loss: 0.0618 | 0.0449
Epoch 108/300, seasonal_3 Loss: 0.0503 | 0.0423
Epoch 109/300, seasonal_3 Loss: 0.0486 | 0.0420
Epoch 110/300, seasonal_3 Loss: 0.0480 | 0.0411
Epoch 111/300, seasonal_3 Loss: 0.0477 | 0.0411
Epoch 112/300, seasonal_3 Loss: 0.0475 | 0.0414
Epoch 113/300, seasonal_3 Loss: 0.0473 | 0.0411
Epoch 114/300, seasonal_3 Loss: 0.0471 | 0.0414
Epoch 115/300, seasonal_3 Loss: 0.0469 | 0.0414
Epoch 116/300, seasonal_3 Loss: 0.0468 | 0.0419
Epoch 117/300, seasonal_3 Loss: 0.0466 | 0.0414
Epoch 118/300, seasonal_3 Loss: 0.0465 | 0.0421
Epoch 119/300, seasonal_3 Loss: 0.0464 | 0.0412
Epoch 120/300, seasonal_3 Loss: 0.0463 | 0.0432
Epoch 121/300, seasonal_3 Loss: 0.0463 | 0.0407
Epoch 122/300, seasonal_3 Loss: 0.0462 | 0.0463
Epoch 123/300, seasonal_3 Loss: 0.0464 | 0.0421
Epoch 124/300, seasonal_3 Loss: 0.0458 | 0.0431
Epoch 125/300, seasonal_3 Loss: 0.0457 | 0.0411
Epoch 126/300, seasonal_3 Loss: 0.0456 | 0.0442
Epoch 127/300, seasonal_3 Loss: 0.0456 | 0.0408
Epoch 128/300, seasonal_3 Loss: 0.0454 | 0.0451
Epoch 129/300, seasonal_3 Loss: 0.0455 | 0.0411
Epoch 130/300, seasonal_3 Loss: 0.0452 | 0.0447
Epoch 131/300, seasonal_3 Loss: 0.0452 | 0.0409
Epoch 132/300, seasonal_3 Loss: 0.0450 | 0.0448
Epoch 133/300, seasonal_3 Loss: 0.0450 | 0.0409
Epoch 134/300, seasonal_3 Loss: 0.0448 | 0.0448
Epoch 135/300, seasonal_3 Loss: 0.0449 | 0.0409
Epoch 136/300, seasonal_3 Loss: 0.0447 | 0.0449
Epoch 137/300, seasonal_3 Loss: 0.0447 | 0.0409
Epoch 138/300, seasonal_3 Loss: 0.0445 | 0.0448
Epoch 139/300, seasonal_3 Loss: 0.0446 | 0.0408
Epoch 140/300, seasonal_3 Loss: 0.0443 | 0.0447
Epoch 141/300, seasonal_3 Loss: 0.0444 | 0.0407
Epoch 142/300, seasonal_3 Loss: 0.0442 | 0.0441
Epoch 143/300, seasonal_3 Loss: 0.0443 | 0.0407
Epoch 144/300, seasonal_3 Loss: 0.0441 | 0.0444
Epoch 145/300, seasonal_3 Loss: 0.0442 | 0.0405
Epoch 146/300, seasonal_3 Loss: 0.0440 | 0.0435
Epoch 147/300, seasonal_3 Loss: 0.0441 | 0.0404
Epoch 148/300, seasonal_3 Loss: 0.0439 | 0.0431
Epoch 149/300, seasonal_3 Loss: 0.0440 | 0.0400
Epoch 150/300, seasonal_3 Loss: 0.0438 | 0.0429
Epoch 151/300, seasonal_3 Loss: 0.0439 | 0.0399
Epoch 152/300, seasonal_3 Loss: 0.0437 | 0.0422
Epoch 153/300, seasonal_3 Loss: 0.0438 | 0.0393
Epoch 154/300, seasonal_3 Loss: 0.0436 | 0.0420
Epoch 155/300, seasonal_3 Loss: 0.0437 | 0.0387
Epoch 156/300, seasonal_3 Loss: 0.0434 | 0.0412
Epoch 157/300, seasonal_3 Loss: 0.0435 | 0.0385
Epoch 158/300, seasonal_3 Loss: 0.0432 | 0.0411
Epoch 159/300, seasonal_3 Loss: 0.0432 | 0.0380
Epoch 160/300, seasonal_3 Loss: 0.0430 | 0.0402
Epoch 161/300, seasonal_3 Loss: 0.0430 | 0.0377
Epoch 162/300, seasonal_3 Loss: 0.0428 | 0.0397
Epoch 163/300, seasonal_3 Loss: 0.0428 | 0.0373
Epoch 164/300, seasonal_3 Loss: 0.0425 | 0.0395
Epoch 165/300, seasonal_3 Loss: 0.0425 | 0.0371
Epoch 166/300, seasonal_3 Loss: 0.0423 | 0.0392
Epoch 167/300, seasonal_3 Loss: 0.0424 | 0.0369
Epoch 168/300, seasonal_3 Loss: 0.0422 | 0.0392
Epoch 169/300, seasonal_3 Loss: 0.0422 | 0.0367
Epoch 170/300, seasonal_3 Loss: 0.0420 | 0.0388
Epoch 171/300, seasonal_3 Loss: 0.0420 | 0.0367
Epoch 172/300, seasonal_3 Loss: 0.0418 | 0.0390
Epoch 173/300, seasonal_3 Loss: 0.0418 | 0.0367
Epoch 174/300, seasonal_3 Loss: 0.0417 | 0.0388
Epoch 175/300, seasonal_3 Loss: 0.0416 | 0.0366
Epoch 176/300, seasonal_3 Loss: 0.0415 | 0.0386
Epoch 177/300, seasonal_3 Loss: 0.0415 | 0.0366
Epoch 178/300, seasonal_3 Loss: 0.0414 | 0.0386
Epoch 179/300, seasonal_3 Loss: 0.0413 | 0.0365
Epoch 180/300, seasonal_3 Loss: 0.0412 | 0.0384
Epoch 181/300, seasonal_3 Loss: 0.0412 | 0.0364
Epoch 182/300, seasonal_3 Loss: 0.0411 | 0.0383
Epoch 183/300, seasonal_3 Loss: 0.0410 | 0.0363
Epoch 184/300, seasonal_3 Loss: 0.0410 | 0.0381
Epoch 185/300, seasonal_3 Loss: 0.0409 | 0.0362
Epoch 186/300, seasonal_3 Loss: 0.0409 | 0.0380
Epoch 187/300, seasonal_3 Loss: 0.0408 | 0.0361
Epoch 188/300, seasonal_3 Loss: 0.0408 | 0.0378
Epoch 189/300, seasonal_3 Loss: 0.0407 | 0.0361
Epoch 190/300, seasonal_3 Loss: 0.0407 | 0.0378
Epoch 191/300, seasonal_3 Loss: 0.0407 | 0.0361
Epoch 192/300, seasonal_3 Loss: 0.0407 | 0.0379
Epoch 193/300, seasonal_3 Loss: 0.0407 | 0.0362
Epoch 194/300, seasonal_3 Loss: 0.0407 | 0.0381
Epoch 195/300, seasonal_3 Loss: 0.0407 | 0.0365
Epoch 196/300, seasonal_3 Loss: 0.0408 | 0.0385
Epoch 197/300, seasonal_3 Loss: 0.0409 | 0.0371
Epoch 198/300, seasonal_3 Loss: 0.0410 | 0.0392
Epoch 199/300, seasonal_3 Loss: 0.0410 | 0.0377
Epoch 200/300, seasonal_3 Loss: 0.0410 | 0.0398
Epoch 201/300, seasonal_3 Loss: 0.0410 | 0.0383
Epoch 202/300, seasonal_3 Loss: 0.0410 | 0.0400
Epoch 203/300, seasonal_3 Loss: 0.0409 | 0.0384
Epoch 204/300, seasonal_3 Loss: 0.0408 | 0.0399
Epoch 205/300, seasonal_3 Loss: 0.0407 | 0.0384
Epoch 206/300, seasonal_3 Loss: 0.0406 | 0.0397
Epoch 207/300, seasonal_3 Loss: 0.0404 | 0.0383
Epoch 208/300, seasonal_3 Loss: 0.0404 | 0.0396
Epoch 209/300, seasonal_3 Loss: 0.0403 | 0.0383
Epoch 210/300, seasonal_3 Loss: 0.0402 | 0.0394
Epoch 211/300, seasonal_3 Loss: 0.0401 | 0.0383
Epoch 212/300, seasonal_3 Loss: 0.0400 | 0.0393
Epoch 213/300, seasonal_3 Loss: 0.0399 | 0.0381
Epoch 214/300, seasonal_3 Loss: 0.0398 | 0.0390
Epoch 215/300, seasonal_3 Loss: 0.0398 | 0.0379
Epoch 216/300, seasonal_3 Loss: 0.0397 | 0.0388
Epoch 217/300, seasonal_3 Loss: 0.0396 | 0.0376
Epoch 218/300, seasonal_3 Loss: 0.0395 | 0.0386
Epoch 219/300, seasonal_3 Loss: 0.0394 | 0.0374
Epoch 220/300, seasonal_3 Loss: 0.0394 | 0.0383
Epoch 221/300, seasonal_3 Loss: 0.0393 | 0.0371
Epoch 222/300, seasonal_3 Loss: 0.0392 | 0.0381
Epoch 223/300, seasonal_3 Loss: 0.0392 | 0.0370
Epoch 224/300, seasonal_3 Loss: 0.0391 | 0.0378
Epoch 225/300, seasonal_3 Loss: 0.0390 | 0.0369
Epoch 226/300, seasonal_3 Loss: 0.0390 | 0.0376
Epoch 227/300, seasonal_3 Loss: 0.0389 | 0.0367
Epoch 228/300, seasonal_3 Loss: 0.0389 | 0.0375
Epoch 229/300, seasonal_3 Loss: 0.0388 | 0.0366
Epoch 230/300, seasonal_3 Loss: 0.0388 | 0.0374
Epoch 231/300, seasonal_3 Loss: 0.0387 | 0.0366
Epoch 232/300, seasonal_3 Loss: 0.0387 | 0.0373
Epoch 233/300, seasonal_3 Loss: 0.0387 | 0.0365
Epoch 234/300, seasonal_3 Loss: 0.0386 | 0.0372
Epoch 235/300, seasonal_3 Loss: 0.0386 | 0.0365
Epoch 236/300, seasonal_3 Loss: 0.0385 | 0.0371
Epoch 237/300, seasonal_3 Loss: 0.0385 | 0.0364
Epoch 238/300, seasonal_3 Loss: 0.0384 | 0.0370
Epoch 239/300, seasonal_3 Loss: 0.0384 | 0.0364
Epoch 240/300, seasonal_3 Loss: 0.0384 | 0.0369
Epoch 241/300, seasonal_3 Loss: 0.0384 | 0.0363
Epoch 242/300, seasonal_3 Loss: 0.0383 | 0.0367
Epoch 243/300, seasonal_3 Loss: 0.0383 | 0.0362
Epoch 244/300, seasonal_3 Loss: 0.0382 | 0.0366
Epoch 245/300, seasonal_3 Loss: 0.0382 | 0.0361
Epoch 246/300, seasonal_3 Loss: 0.0382 | 0.0364
Epoch 247/300, seasonal_3 Loss: 0.0381 | 0.0360
Epoch 248/300, seasonal_3 Loss: 0.0381 | 0.0363
Epoch 249/300, seasonal_3 Loss: 0.0380 | 0.0359
Epoch 250/300, seasonal_3 Loss: 0.0380 | 0.0362
Epoch 251/300, seasonal_3 Loss: 0.0379 | 0.0358
Epoch 252/300, seasonal_3 Loss: 0.0379 | 0.0361
Epoch 253/300, seasonal_3 Loss: 0.0378 | 0.0358
Epoch 254/300, seasonal_3 Loss: 0.0378 | 0.0361
Epoch 255/300, seasonal_3 Loss: 0.0378 | 0.0358
Epoch 256/300, seasonal_3 Loss: 0.0377 | 0.0361
Epoch 257/300, seasonal_3 Loss: 0.0377 | 0.0358
Epoch 258/300, seasonal_3 Loss: 0.0376 | 0.0361
Epoch 259/300, seasonal_3 Loss: 0.0376 | 0.0358
Epoch 260/300, seasonal_3 Loss: 0.0375 | 0.0362
Epoch 261/300, seasonal_3 Loss: 0.0375 | 0.0359
Epoch 262/300, seasonal_3 Loss: 0.0375 | 0.0362
Epoch 263/300, seasonal_3 Loss: 0.0374 | 0.0358
Epoch 264/300, seasonal_3 Loss: 0.0374 | 0.0363
Epoch 265/300, seasonal_3 Loss: 0.0374 | 0.0359
Epoch 266/300, seasonal_3 Loss: 0.0373 | 0.0363
Epoch 267/300, seasonal_3 Loss: 0.0373 | 0.0360
Epoch 268/300, seasonal_3 Loss: 0.0373 | 0.0364
Epoch 269/300, seasonal_3 Loss: 0.0372 | 0.0360
Epoch 270/300, seasonal_3 Loss: 0.0372 | 0.0364
Epoch 271/300, seasonal_3 Loss: 0.0372 | 0.0361
Epoch 272/300, seasonal_3 Loss: 0.0371 | 0.0365
Epoch 273/300, seasonal_3 Loss: 0.0371 | 0.0361
Epoch 274/300, seasonal_3 Loss: 0.0371 | 0.0366
Epoch 275/300, seasonal_3 Loss: 0.0370 | 0.0361
Epoch 276/300, seasonal_3 Loss: 0.0370 | 0.0366
Epoch 277/300, seasonal_3 Loss: 0.0370 | 0.0361
Epoch 278/300, seasonal_3 Loss: 0.0369 | 0.0366
Epoch 279/300, seasonal_3 Loss: 0.0369 | 0.0362
Epoch 280/300, seasonal_3 Loss: 0.0369 | 0.0366
Epoch 281/300, seasonal_3 Loss: 0.0368 | 0.0362
Epoch 282/300, seasonal_3 Loss: 0.0368 | 0.0366
Epoch 283/300, seasonal_3 Loss: 0.0367 | 0.0362
Epoch 284/300, seasonal_3 Loss: 0.0367 | 0.0366
Epoch 285/300, seasonal_3 Loss: 0.0367 | 0.0362
Epoch 286/300, seasonal_3 Loss: 0.0366 | 0.0365
Epoch 287/300, seasonal_3 Loss: 0.0366 | 0.0361
Epoch 288/300, seasonal_3 Loss: 0.0365 | 0.0365
Epoch 289/300, seasonal_3 Loss: 0.0365 | 0.0361
Epoch 290/300, seasonal_3 Loss: 0.0365 | 0.0364
Epoch 291/300, seasonal_3 Loss: 0.0364 | 0.0360
Epoch 292/300, seasonal_3 Loss: 0.0364 | 0.0364
Epoch 293/300, seasonal_3 Loss: 0.0364 | 0.0360
Epoch 294/300, seasonal_3 Loss: 0.0363 | 0.0363
Epoch 295/300, seasonal_3 Loss: 0.0363 | 0.0360
Epoch 296/300, seasonal_3 Loss: 0.0363 | 0.0363
Epoch 297/300, seasonal_3 Loss: 0.0363 | 0.0359
Epoch 298/300, seasonal_3 Loss: 0.0362 | 0.0362
Epoch 299/300, seasonal_3 Loss: 0.0362 | 0.0359
Epoch 300/300, seasonal_3 Loss: 0.0362 | 0.0362
Training resid component with params: {'observation_period_num': 10, 'train_rates': 0.8958186370390298, 'learning_rate': 0.0008213213002676373, 'batch_size': 16, 'step_size': 14, 'gamma': 0.7676623930044139}
Epoch 1/300, resid Loss: 0.1640 | 0.1133
Epoch 2/300, resid Loss: 0.1132 | 0.1450
Epoch 3/300, resid Loss: 0.1050 | 0.1163
Epoch 4/300, resid Loss: 0.0981 | 0.0885
Epoch 5/300, resid Loss: 0.0922 | 0.1047
Epoch 6/300, resid Loss: 0.0888 | 0.0800
Epoch 7/300, resid Loss: 0.0840 | 0.0690
Epoch 8/300, resid Loss: 0.0825 | 0.0699
Epoch 9/300, resid Loss: 0.0802 | 0.0663
Epoch 10/300, resid Loss: 0.0757 | 0.0576
Epoch 11/300, resid Loss: 0.0751 | 0.0612
Epoch 12/300, resid Loss: 0.0734 | 0.0671
Epoch 13/300, resid Loss: 0.0726 | 0.0577
Epoch 14/300, resid Loss: 0.0681 | 0.0595
Epoch 15/300, resid Loss: 0.0652 | 0.0575
Epoch 16/300, resid Loss: 0.0644 | 0.0554
Epoch 17/300, resid Loss: 0.0625 | 0.0477
Epoch 18/300, resid Loss: 0.0613 | 0.0501
Epoch 19/300, resid Loss: 0.0595 | 0.0513
Epoch 20/300, resid Loss: 0.0589 | 0.0557
Epoch 21/300, resid Loss: 0.0582 | 0.0509
Epoch 22/300, resid Loss: 0.0587 | 0.0499
Epoch 23/300, resid Loss: 0.0573 | 0.0530
Epoch 24/300, resid Loss: 0.0558 | 0.0477
Epoch 25/300, resid Loss: 0.0568 | 0.0470
Epoch 26/300, resid Loss: 0.0547 | 0.0452
Epoch 27/300, resid Loss: 0.0531 | 0.0476
Epoch 28/300, resid Loss: 0.0523 | 0.0515
Epoch 29/300, resid Loss: 0.0509 | 0.0550
Epoch 30/300, resid Loss: 0.0480 | 0.0454
Epoch 31/300, resid Loss: 0.0511 | 0.0489
Epoch 32/300, resid Loss: 0.0505 | 0.0476
Epoch 33/300, resid Loss: 0.0515 | 0.0710
Epoch 34/300, resid Loss: 0.0576 | 0.0573
Epoch 35/300, resid Loss: 0.0520 | 0.0533
Epoch 36/300, resid Loss: 0.0497 | 0.0460
Epoch 37/300, resid Loss: 0.0479 | 0.0483
Epoch 38/300, resid Loss: 0.0463 | 0.0530
Epoch 39/300, resid Loss: 0.0458 | 0.0505
Epoch 40/300, resid Loss: 0.0440 | 0.0466
Epoch 41/300, resid Loss: 0.0432 | 0.0571
Epoch 42/300, resid Loss: 0.0422 | 0.0474
Epoch 43/300, resid Loss: 0.0393 | 0.0466
Epoch 44/300, resid Loss: 0.0452 | 0.0477
Epoch 45/300, resid Loss: 0.0438 | 0.0524
Epoch 46/300, resid Loss: 0.0447 | 0.0742
Epoch 47/300, resid Loss: 0.0510 | 0.0925
Epoch 48/300, resid Loss: 0.0527 | 0.0577
Epoch 49/300, resid Loss: 0.0403 | 0.0484
Epoch 50/300, resid Loss: 0.0444 | 0.0490
Epoch 51/300, resid Loss: 0.0431 | 0.0506
Epoch 52/300, resid Loss: 0.0412 | 0.0564
Epoch 53/300, resid Loss: 0.0386 | 0.0551
Epoch 54/300, resid Loss: 0.0372 | 0.1195
Epoch 55/300, resid Loss: 0.0365 | 0.0496
Epoch 56/300, resid Loss: 0.0440 | 0.0487
Epoch 57/300, resid Loss: 0.0424 | 0.0486
Epoch 58/300, resid Loss: 0.0415 | 0.0495
Epoch 59/300, resid Loss: 0.0405 | 0.0518
Epoch 60/300, resid Loss: 0.0351 | 0.0534
Epoch 61/300, resid Loss: 0.0326 | 0.0497
Epoch 62/300, resid Loss: 0.0385 | 0.0657
Epoch 63/300, resid Loss: 0.0378 | 0.0702
Epoch 64/300, resid Loss: 0.0343 | 0.0466
Epoch 65/300, resid Loss: 0.0322 | 0.0466
Epoch 66/300, resid Loss: 0.0315 | 0.0474
Epoch 67/300, resid Loss: 0.0307 | 0.0471
Epoch 68/300, resid Loss: 0.0302 | 0.0472
Epoch 69/300, resid Loss: 0.0298 | 0.0473
Epoch 70/300, resid Loss: 0.0296 | 0.0474
Epoch 71/300, resid Loss: 0.0293 | 0.0472
Epoch 72/300, resid Loss: 0.0291 | 0.0473
Epoch 73/300, resid Loss: 0.0289 | 0.0474
Epoch 74/300, resid Loss: 0.0287 | 0.0476
Epoch 75/300, resid Loss: 0.0285 | 0.0478
Epoch 76/300, resid Loss: 0.0283 | 0.0479
Epoch 77/300, resid Loss: 0.0281 | 0.0481
Epoch 78/300, resid Loss: 0.0279 | 0.0479
Epoch 79/300, resid Loss: 0.0277 | 0.0480
Epoch 80/300, resid Loss: 0.0276 | 0.0480
Epoch 81/300, resid Loss: 0.0274 | 0.0482
Epoch 82/300, resid Loss: 0.0273 | 0.0483
Epoch 83/300, resid Loss: 0.0271 | 0.0482
Epoch 84/300, resid Loss: 0.0270 | 0.0488
Epoch 85/300, resid Loss: 0.0269 | 0.0480
Epoch 86/300, resid Loss: 0.0268 | 0.0484
Epoch 87/300, resid Loss: 0.0266 | 0.0486
Epoch 88/300, resid Loss: 0.0265 | 0.0480
Epoch 89/300, resid Loss: 0.0264 | 0.0486
Epoch 90/300, resid Loss: 0.0262 | 0.0487
Epoch 91/300, resid Loss: 0.0262 | 0.0481
Epoch 92/300, resid Loss: 0.0261 | 0.0491
Epoch 93/300, resid Loss: 0.0259 | 0.0489
Epoch 94/300, resid Loss: 0.0257 | 0.0489
Epoch 95/300, resid Loss: 0.0256 | 0.0491
Epoch 96/300, resid Loss: 0.0255 | 0.0491
Epoch 97/300, resid Loss: 0.0254 | 0.0493
Epoch 98/300, resid Loss: 0.0253 | 0.0492
Epoch 99/300, resid Loss: 0.0252 | 0.0494
Epoch 100/300, resid Loss: 0.0251 | 0.0495
Epoch 101/300, resid Loss: 0.0250 | 0.0495
Epoch 102/300, resid Loss: 0.0249 | 0.0495
Epoch 103/300, resid Loss: 0.0248 | 0.0497
Epoch 104/300, resid Loss: 0.0247 | 0.0497
Epoch 105/300, resid Loss: 0.0246 | 0.0499
Epoch 106/300, resid Loss: 0.0245 | 0.0500
Epoch 107/300, resid Loss: 0.0244 | 0.0501
Epoch 108/300, resid Loss: 0.0244 | 0.0501
Epoch 109/300, resid Loss: 0.0243 | 0.0502
Epoch 110/300, resid Loss: 0.0242 | 0.0503
Epoch 111/300, resid Loss: 0.0241 | 0.0504
Epoch 112/300, resid Loss: 0.0241 | 0.0504
Epoch 113/300, resid Loss: 0.0240 | 0.0506
Epoch 114/300, resid Loss: 0.0239 | 0.0506
Epoch 115/300, resid Loss: 0.0239 | 0.0507
Epoch 116/300, resid Loss: 0.0238 | 0.0507
Epoch 117/300, resid Loss: 0.0238 | 0.0508
Epoch 118/300, resid Loss: 0.0237 | 0.0508
Epoch 119/300, resid Loss: 0.0237 | 0.0509
Epoch 120/300, resid Loss: 0.0236 | 0.0511
Epoch 121/300, resid Loss: 0.0236 | 0.0511
Epoch 122/300, resid Loss: 0.0236 | 0.0511
Epoch 123/300, resid Loss: 0.0235 | 0.0512
Epoch 124/300, resid Loss: 0.0235 | 0.0512
Epoch 125/300, resid Loss: 0.0235 | 0.0512
Epoch 126/300, resid Loss: 0.0234 | 0.0513
Epoch 127/300, resid Loss: 0.0234 | 0.0514
Epoch 128/300, resid Loss: 0.0234 | 0.0514
Epoch 129/300, resid Loss: 0.0233 | 0.0514
Epoch 130/300, resid Loss: 0.0233 | 0.0514
Epoch 131/300, resid Loss: 0.0233 | 0.0514
Epoch 132/300, resid Loss: 0.0232 | 0.0515
Epoch 133/300, resid Loss: 0.0232 | 0.0515
Epoch 134/300, resid Loss: 0.0232 | 0.0515
Epoch 135/300, resid Loss: 0.0231 | 0.0516
Epoch 136/300, resid Loss: 0.0231 | 0.0516
Epoch 137/300, resid Loss: 0.0231 | 0.0516
Epoch 138/300, resid Loss: 0.0230 | 0.0516
Epoch 139/300, resid Loss: 0.0230 | 0.0516
Epoch 140/300, resid Loss: 0.0230 | 0.0517
Epoch 141/300, resid Loss: 0.0230 | 0.0517
Epoch 142/300, resid Loss: 0.0229 | 0.0518
Epoch 143/300, resid Loss: 0.0229 | 0.0518
Epoch 144/300, resid Loss: 0.0229 | 0.0518
Epoch 145/300, resid Loss: 0.0229 | 0.0518
Epoch 146/300, resid Loss: 0.0229 | 0.0519
Epoch 147/300, resid Loss: 0.0228 | 0.0519
Epoch 148/300, resid Loss: 0.0228 | 0.0520
Epoch 149/300, resid Loss: 0.0228 | 0.0520
Epoch 150/300, resid Loss: 0.0228 | 0.0520
Epoch 151/300, resid Loss: 0.0228 | 0.0520
Epoch 152/300, resid Loss: 0.0228 | 0.0520
Epoch 153/300, resid Loss: 0.0227 | 0.0521
Epoch 154/300, resid Loss: 0.0227 | 0.0521
Epoch 155/300, resid Loss: 0.0227 | 0.0521
Epoch 156/300, resid Loss: 0.0227 | 0.0522
Epoch 157/300, resid Loss: 0.0227 | 0.0522
Epoch 158/300, resid Loss: 0.0227 | 0.0522
Epoch 159/300, resid Loss: 0.0227 | 0.0522
Epoch 160/300, resid Loss: 0.0227 | 0.0522
Epoch 161/300, resid Loss: 0.0226 | 0.0522
Epoch 162/300, resid Loss: 0.0226 | 0.0523
Epoch 163/300, resid Loss: 0.0226 | 0.0523
Epoch 164/300, resid Loss: 0.0226 | 0.0523
Epoch 165/300, resid Loss: 0.0226 | 0.0523
Epoch 166/300, resid Loss: 0.0226 | 0.0523
Epoch 167/300, resid Loss: 0.0226 | 0.0523
Epoch 168/300, resid Loss: 0.0226 | 0.0524
Epoch 169/300, resid Loss: 0.0226 | 0.0524
Epoch 170/300, resid Loss: 0.0226 | 0.0524
Epoch 171/300, resid Loss: 0.0226 | 0.0524
Epoch 172/300, resid Loss: 0.0225 | 0.0524
Epoch 173/300, resid Loss: 0.0225 | 0.0524
Epoch 174/300, resid Loss: 0.0225 | 0.0524
Epoch 175/300, resid Loss: 0.0225 | 0.0524
Epoch 176/300, resid Loss: 0.0225 | 0.0525
Epoch 177/300, resid Loss: 0.0225 | 0.0525
Epoch 178/300, resid Loss: 0.0225 | 0.0525
Epoch 179/300, resid Loss: 0.0225 | 0.0525
Epoch 180/300, resid Loss: 0.0225 | 0.0525
Epoch 181/300, resid Loss: 0.0225 | 0.0525
Epoch 182/300, resid Loss: 0.0225 | 0.0525
Epoch 183/300, resid Loss: 0.0225 | 0.0525
Epoch 184/300, resid Loss: 0.0225 | 0.0525
Epoch 185/300, resid Loss: 0.0225 | 0.0525
Epoch 186/300, resid Loss: 0.0225 | 0.0525
Epoch 187/300, resid Loss: 0.0225 | 0.0525
Epoch 188/300, resid Loss: 0.0225 | 0.0525
Epoch 189/300, resid Loss: 0.0225 | 0.0526
Epoch 190/300, resid Loss: 0.0225 | 0.0526
Epoch 191/300, resid Loss: 0.0225 | 0.0526
Epoch 192/300, resid Loss: 0.0224 | 0.0526
Epoch 193/300, resid Loss: 0.0224 | 0.0526
Epoch 194/300, resid Loss: 0.0224 | 0.0526
Epoch 195/300, resid Loss: 0.0224 | 0.0526
Epoch 196/300, resid Loss: 0.0224 | 0.0526
Epoch 197/300, resid Loss: 0.0224 | 0.0526
Epoch 198/300, resid Loss: 0.0224 | 0.0526
Epoch 199/300, resid Loss: 0.0224 | 0.0526
Epoch 200/300, resid Loss: 0.0224 | 0.0526
Epoch 201/300, resid Loss: 0.0224 | 0.0526
Epoch 202/300, resid Loss: 0.0224 | 0.0526
Epoch 203/300, resid Loss: 0.0224 | 0.0526
Epoch 204/300, resid Loss: 0.0224 | 0.0526
Epoch 205/300, resid Loss: 0.0224 | 0.0526
Epoch 206/300, resid Loss: 0.0224 | 0.0526
Epoch 207/300, resid Loss: 0.0224 | 0.0526
Epoch 208/300, resid Loss: 0.0224 | 0.0526
Epoch 209/300, resid Loss: 0.0224 | 0.0526
Epoch 210/300, resid Loss: 0.0224 | 0.0527
Epoch 211/300, resid Loss: 0.0224 | 0.0526
Epoch 212/300, resid Loss: 0.0224 | 0.0527
Epoch 213/300, resid Loss: 0.0224 | 0.0527
Epoch 214/300, resid Loss: 0.0224 | 0.0527
Epoch 215/300, resid Loss: 0.0224 | 0.0527
Epoch 216/300, resid Loss: 0.0224 | 0.0527
Epoch 217/300, resid Loss: 0.0224 | 0.0527
Epoch 218/300, resid Loss: 0.0224 | 0.0527
Epoch 219/300, resid Loss: 0.0224 | 0.0527
Epoch 220/300, resid Loss: 0.0224 | 0.0527
Epoch 221/300, resid Loss: 0.0224 | 0.0527
Epoch 222/300, resid Loss: 0.0224 | 0.0527
Epoch 223/300, resid Loss: 0.0224 | 0.0527
Epoch 224/300, resid Loss: 0.0224 | 0.0527
Epoch 225/300, resid Loss: 0.0224 | 0.0527
Epoch 226/300, resid Loss: 0.0224 | 0.0527
Epoch 227/300, resid Loss: 0.0224 | 0.0527
Epoch 228/300, resid Loss: 0.0224 | 0.0527
Epoch 229/300, resid Loss: 0.0224 | 0.0527
Epoch 230/300, resid Loss: 0.0224 | 0.0527
Epoch 231/300, resid Loss: 0.0224 | 0.0527
Epoch 232/300, resid Loss: 0.0224 | 0.0527
Epoch 233/300, resid Loss: 0.0224 | 0.0527
Epoch 234/300, resid Loss: 0.0224 | 0.0527
Epoch 235/300, resid Loss: 0.0224 | 0.0527
Epoch 236/300, resid Loss: 0.0224 | 0.0527
Epoch 237/300, resid Loss: 0.0224 | 0.0527
Epoch 238/300, resid Loss: 0.0224 | 0.0527
Epoch 239/300, resid Loss: 0.0224 | 0.0527
Epoch 240/300, resid Loss: 0.0224 | 0.0527
Epoch 241/300, resid Loss: 0.0224 | 0.0527
Epoch 242/300, resid Loss: 0.0224 | 0.0527
Epoch 243/300, resid Loss: 0.0224 | 0.0527
Epoch 244/300, resid Loss: 0.0224 | 0.0527
Epoch 245/300, resid Loss: 0.0224 | 0.0527
Epoch 246/300, resid Loss: 0.0224 | 0.0527
Epoch 247/300, resid Loss: 0.0224 | 0.0527
Epoch 248/300, resid Loss: 0.0224 | 0.0527
Epoch 249/300, resid Loss: 0.0224 | 0.0527
Epoch 250/300, resid Loss: 0.0224 | 0.0527
Epoch 251/300, resid Loss: 0.0224 | 0.0527
Epoch 252/300, resid Loss: 0.0224 | 0.0527
Epoch 253/300, resid Loss: 0.0224 | 0.0527
Epoch 254/300, resid Loss: 0.0224 | 0.0527
Epoch 255/300, resid Loss: 0.0224 | 0.0527
Epoch 256/300, resid Loss: 0.0224 | 0.0527
Epoch 257/300, resid Loss: 0.0224 | 0.0527
Epoch 258/300, resid Loss: 0.0224 | 0.0527
Epoch 259/300, resid Loss: 0.0224 | 0.0527
Epoch 260/300, resid Loss: 0.0224 | 0.0527
Epoch 261/300, resid Loss: 0.0224 | 0.0527
Epoch 262/300, resid Loss: 0.0224 | 0.0527
Epoch 263/300, resid Loss: 0.0224 | 0.0527
Epoch 264/300, resid Loss: 0.0224 | 0.0527
Epoch 265/300, resid Loss: 0.0224 | 0.0527
Epoch 266/300, resid Loss: 0.0224 | 0.0527
Epoch 267/300, resid Loss: 0.0224 | 0.0527
Epoch 268/300, resid Loss: 0.0224 | 0.0527
Epoch 269/300, resid Loss: 0.0224 | 0.0527
Epoch 270/300, resid Loss: 0.0224 | 0.0527
Epoch 271/300, resid Loss: 0.0224 | 0.0527
Epoch 272/300, resid Loss: 0.0224 | 0.0527
Epoch 273/300, resid Loss: 0.0224 | 0.0527
Epoch 274/300, resid Loss: 0.0224 | 0.0527
Epoch 275/300, resid Loss: 0.0224 | 0.0527
Epoch 276/300, resid Loss: 0.0224 | 0.0527
Epoch 277/300, resid Loss: 0.0224 | 0.0527
Epoch 278/300, resid Loss: 0.0224 | 0.0527
Epoch 279/300, resid Loss: 0.0224 | 0.0527
Epoch 280/300, resid Loss: 0.0224 | 0.0527
Epoch 281/300, resid Loss: 0.0224 | 0.0527
Epoch 282/300, resid Loss: 0.0224 | 0.0527
Epoch 283/300, resid Loss: 0.0224 | 0.0527
Epoch 284/300, resid Loss: 0.0224 | 0.0527
Epoch 285/300, resid Loss: 0.0224 | 0.0527
Epoch 286/300, resid Loss: 0.0224 | 0.0527
Epoch 287/300, resid Loss: 0.0224 | 0.0527
Epoch 288/300, resid Loss: 0.0224 | 0.0527
Epoch 289/300, resid Loss: 0.0224 | 0.0527
Epoch 290/300, resid Loss: 0.0224 | 0.0527
Epoch 291/300, resid Loss: 0.0224 | 0.0527
Epoch 292/300, resid Loss: 0.0224 | 0.0527
Epoch 293/300, resid Loss: 0.0224 | 0.0527
Epoch 294/300, resid Loss: 0.0224 | 0.0527
Epoch 295/300, resid Loss: 0.0224 | 0.0527
Epoch 296/300, resid Loss: 0.0224 | 0.0527
Epoch 297/300, resid Loss: 0.0224 | 0.0527
Epoch 298/300, resid Loss: 0.0224 | 0.0527
Epoch 299/300, resid Loss: 0.0224 | 0.0527
Epoch 300/300, resid Loss: 0.0224 | 0.0527
Runtime (seconds): 2404.2228581905365
0.000462769100818871
[165.45703]
[6.3206997]
[-0.24853571]
[-2.1048791]
[0.8510934]
[0.24934632]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 17.16853964701295
RMSE: 4.14349365234375
MAE: 4.14349365234375
R-squared: nan
[170.52475]
