[32m[I 2025-02-03 08:29:48,465][0m A new study created in memory with name: no-name-69490109-3e55-4b88-a225-a94693c02cce[0m
[32m[I 2025-02-03 08:31:53,849][0m Trial 0 finished with value: 0.07380219230061855 and parameters: {'observation_period_num': 233, 'train_rates': 0.8189446654873247, 'learning_rate': 0.0002502823647236725, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8094677336353743}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:33:13,794][0m Trial 1 finished with value: 0.1564828978819402 and parameters: {'observation_period_num': 217, 'train_rates': 0.8515949232184215, 'learning_rate': 7.732453068201877e-06, 'batch_size': 67, 'step_size': 13, 'gamma': 0.935710371532905}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:33:52,854][0m Trial 2 finished with value: 0.09953787922859192 and parameters: {'observation_period_num': 93, 'train_rates': 0.9528580937773741, 'learning_rate': 1.3215158185310357e-05, 'batch_size': 163, 'step_size': 3, 'gamma': 0.9836092086541594}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:34:19,980][0m Trial 3 finished with value: 0.8604500652328739 and parameters: {'observation_period_num': 114, 'train_rates': 0.7403431135222445, 'learning_rate': 1.3308926261212578e-06, 'batch_size': 200, 'step_size': 3, 'gamma': 0.7925301265860621}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:34:48,461][0m Trial 4 finished with value: 0.26866610271079666 and parameters: {'observation_period_num': 238, 'train_rates': 0.8651284753613093, 'learning_rate': 5.426013813163221e-06, 'batch_size': 196, 'step_size': 14, 'gamma': 0.9307134643403534}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:36:23,927][0m Trial 5 finished with value: 0.17289537696966104 and parameters: {'observation_period_num': 148, 'train_rates': 0.8304825774513106, 'learning_rate': 5.442439867035232e-06, 'batch_size': 55, 'step_size': 15, 'gamma': 0.7763611798659064}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:36:48,445][0m Trial 6 finished with value: 0.1659386571496725 and parameters: {'observation_period_num': 75, 'train_rates': 0.7778744654299994, 'learning_rate': 1.1732474319112977e-05, 'batch_size': 234, 'step_size': 12, 'gamma': 0.7793143792014922}. Best is trial 0 with value: 0.07380219230061855.[0m
[32m[I 2025-02-03 08:37:32,115][0m Trial 7 finished with value: 0.037380318592445255 and parameters: {'observation_period_num': 73, 'train_rates': 0.8097655065514433, 'learning_rate': 0.0008726248406698208, 'batch_size': 131, 'step_size': 1, 'gamma': 0.9842816588576534}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:39:26,949][0m Trial 8 finished with value: 0.0990102679379994 and parameters: {'observation_period_num': 103, 'train_rates': 0.7643112443164465, 'learning_rate': 0.00011962931165529047, 'batch_size': 44, 'step_size': 2, 'gamma': 0.7584340378721157}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:41:05,933][0m Trial 9 finished with value: 0.1133153688542697 and parameters: {'observation_period_num': 192, 'train_rates': 0.8228886169814201, 'learning_rate': 1.1588146798409044e-05, 'batch_size': 52, 'step_size': 12, 'gamma': 0.8797757899271941}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:41:49,040][0m Trial 10 finished with value: 0.04660290222557795 and parameters: {'observation_period_num': 7, 'train_rates': 0.6242712642257486, 'learning_rate': 0.000999144234333306, 'batch_size': 113, 'step_size': 7, 'gamma': 0.9862102127710255}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:42:29,813][0m Trial 11 finished with value: 0.10370800446356292 and parameters: {'observation_period_num': 13, 'train_rates': 0.61595071588456, 'learning_rate': 0.000902584908948848, 'batch_size': 114, 'step_size': 7, 'gamma': 0.9852780166384613}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:43:12,827][0m Trial 12 finished with value: 0.04404963631813805 and parameters: {'observation_period_num': 7, 'train_rates': 0.6634143480196733, 'learning_rate': 0.0004565082890488929, 'batch_size': 118, 'step_size': 9, 'gamma': 0.9432572725010567}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:43:50,315][0m Trial 13 finished with value: 0.04544952639264126 and parameters: {'observation_period_num': 57, 'train_rates': 0.6884619180794744, 'learning_rate': 0.00017666069289456882, 'batch_size': 141, 'step_size': 10, 'gamma': 0.9314304151386193}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:44:44,195][0m Trial 14 finished with value: 0.05929104999874832 and parameters: {'observation_period_num': 43, 'train_rates': 0.6973419072495167, 'learning_rate': 5.7977616283234396e-05, 'batch_size': 93, 'step_size': 5, 'gamma': 0.8890829186696271}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:45:23,096][0m Trial 15 finished with value: 0.05402929750936372 and parameters: {'observation_period_num': 150, 'train_rates': 0.9127770559296172, 'learning_rate': 0.00039526231181558904, 'batch_size': 159, 'step_size': 10, 'gamma': 0.8349013795914139}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:46:19,715][0m Trial 16 finished with value: 0.07009304348680533 and parameters: {'observation_period_num': 37, 'train_rates': 0.6904796141986377, 'learning_rate': 5.679981344656558e-05, 'batch_size': 89, 'step_size': 1, 'gamma': 0.9576100635602882}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:46:58,933][0m Trial 17 finished with value: 0.05223059455174526 and parameters: {'observation_period_num': 74, 'train_rates': 0.7270738004878157, 'learning_rate': 0.00041111682282684203, 'batch_size': 133, 'step_size': 9, 'gamma': 0.9022722286130441}. Best is trial 7 with value: 0.037380318592445255.[0m
[32m[I 2025-02-03 08:52:12,332][0m Trial 18 finished with value: 0.026788822512197143 and parameters: {'observation_period_num': 30, 'train_rates': 0.8955142449727602, 'learning_rate': 0.0005128442234191685, 'batch_size': 18, 'step_size': 5, 'gamma': 0.8523206639290389}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 08:57:33,052][0m Trial 19 finished with value: 0.043580328631732196 and parameters: {'observation_period_num': 139, 'train_rates': 0.8977425027479932, 'learning_rate': 8.721726629586863e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.850014440619698}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:03:39,403][0m Trial 20 finished with value: 0.04457685050945128 and parameters: {'observation_period_num': 33, 'train_rates': 0.947105297377474, 'learning_rate': 3.1508665638682205e-05, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8475551091605613}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:09:14,100][0m Trial 21 finished with value: 0.048791012659515305 and parameters: {'observation_period_num': 162, 'train_rates': 0.8957839597752377, 'learning_rate': 0.00010912715257516493, 'batch_size': 16, 'step_size': 5, 'gamma': 0.8552414657998639}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:10:28,290][0m Trial 22 finished with value: 0.04293479323387146 and parameters: {'observation_period_num': 118, 'train_rates': 0.9012872579300845, 'learning_rate': 0.0006233109787685002, 'batch_size': 76, 'step_size': 4, 'gamma': 0.8215500116475563}. Best is trial 18 with value: 0.026788822512197143.[0m
Early stopping at epoch 66
[32m[I 2025-02-03 09:11:23,304][0m Trial 23 finished with value: 0.0807863101363182 and parameters: {'observation_period_num': 79, 'train_rates': 0.983277272836825, 'learning_rate': 0.0005889650780748872, 'batch_size': 76, 'step_size': 1, 'gamma': 0.8101473008812398}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:12:25,979][0m Trial 24 finished with value: 0.06243296284279316 and parameters: {'observation_period_num': 124, 'train_rates': 0.8706173748332985, 'learning_rate': 0.00026060148265288804, 'batch_size': 90, 'step_size': 3, 'gamma': 0.8255636675066539}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:13:02,161][0m Trial 25 finished with value: 0.034937767189014986 and parameters: {'observation_period_num': 59, 'train_rates': 0.9320379979465407, 'learning_rate': 0.0006751310439989762, 'batch_size': 175, 'step_size': 4, 'gamma': 0.9066173249941852}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:13:36,081][0m Trial 26 finished with value: 0.046278536319732666 and parameters: {'observation_period_num': 59, 'train_rates': 0.9427260656119495, 'learning_rate': 0.00021384059085803887, 'batch_size': 190, 'step_size': 2, 'gamma': 0.9623227305690409}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:14:00,704][0m Trial 27 finished with value: 0.036995019317678415 and parameters: {'observation_period_num': 30, 'train_rates': 0.7974411404586108, 'learning_rate': 0.0007940726362963923, 'batch_size': 248, 'step_size': 6, 'gamma': 0.9165866130685295}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:14:33,912][0m Trial 28 finished with value: 0.042941439896821976 and parameters: {'observation_period_num': 26, 'train_rates': 0.9742919157743988, 'learning_rate': 0.00032346721505417355, 'batch_size': 216, 'step_size': 6, 'gamma': 0.9080812202033539}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:14:58,478][0m Trial 29 finished with value: 0.042752111020187535 and parameters: {'observation_period_num': 51, 'train_rates': 0.8430037814814636, 'learning_rate': 0.00017295856279175202, 'batch_size': 254, 'step_size': 8, 'gamma': 0.8678192279059986}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:15:24,612][0m Trial 30 finished with value: 0.031073475256562233 and parameters: {'observation_period_num': 26, 'train_rates': 0.9279671618991348, 'learning_rate': 0.0006186469163501722, 'batch_size': 254, 'step_size': 6, 'gamma': 0.9104208653663289}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:15:53,372][0m Trial 31 finished with value: 0.03175561875104904 and parameters: {'observation_period_num': 22, 'train_rates': 0.9262416852264626, 'learning_rate': 0.0005751806832508266, 'batch_size': 241, 'step_size': 6, 'gamma': 0.9139666955836021}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:16:21,766][0m Trial 32 finished with value: 0.03740943315587625 and parameters: {'observation_period_num': 17, 'train_rates': 0.919039248037821, 'learning_rate': 0.0005366760211954638, 'batch_size': 228, 'step_size': 6, 'gamma': 0.8899198644056215}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:16:51,772][0m Trial 33 finished with value: 0.04238078370690346 and parameters: {'observation_period_num': 48, 'train_rates': 0.9301525898976261, 'learning_rate': 0.0002453952032173668, 'batch_size': 212, 'step_size': 4, 'gamma': 0.9187175672198938}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:17:27,638][0m Trial 34 finished with value: 0.056113239377737045 and parameters: {'observation_period_num': 91, 'train_rates': 0.9685604001493672, 'learning_rate': 0.0003227205066080908, 'batch_size': 177, 'step_size': 4, 'gamma': 0.8669177478054215}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:17:54,509][0m Trial 35 finished with value: 0.4350425249161425 and parameters: {'observation_period_num': 23, 'train_rates': 0.8734364741098866, 'learning_rate': 2.449919541580351e-06, 'batch_size': 237, 'step_size': 8, 'gamma': 0.8956076271865603}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:18:24,969][0m Trial 36 finished with value: 0.07399149239063263 and parameters: {'observation_period_num': 59, 'train_rates': 0.9565024364192545, 'learning_rate': 2.7635347390171322e-05, 'batch_size': 214, 'step_size': 6, 'gamma': 0.9457840262120636}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:18:59,925][0m Trial 37 finished with value: 0.06916980060966725 and parameters: {'observation_period_num': 223, 'train_rates': 0.9311389369439904, 'learning_rate': 0.0006192245459675399, 'batch_size': 171, 'step_size': 3, 'gamma': 0.9160987450243135}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:19:24,357][0m Trial 38 finished with value: 0.07066563509759449 and parameters: {'observation_period_num': 250, 'train_rates': 0.883718306060842, 'learning_rate': 0.00037483475970025246, 'batch_size': 248, 'step_size': 7, 'gamma': 0.8799176651166843}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:19:55,561][0m Trial 39 finished with value: 0.047593151238818916 and parameters: {'observation_period_num': 93, 'train_rates': 0.8509115744967048, 'learning_rate': 0.00013837418333546286, 'batch_size': 202, 'step_size': 4, 'gamma': 0.9657764897616078}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:20:23,764][0m Trial 40 finished with value: 0.05544202249401655 and parameters: {'observation_period_num': 41, 'train_rates': 0.9197583834673839, 'learning_rate': 7.079387880481592e-05, 'batch_size': 226, 'step_size': 7, 'gamma': 0.927478740508653}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:20:48,375][0m Trial 41 finished with value: 0.034150752877721034 and parameters: {'observation_period_num': 28, 'train_rates': 0.7867098420546735, 'learning_rate': 0.0006934385310949683, 'batch_size': 241, 'step_size': 6, 'gamma': 0.9153060471959481}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:21:12,631][0m Trial 42 finished with value: 0.033282109842815016 and parameters: {'observation_period_num': 21, 'train_rates': 0.7806924562598618, 'learning_rate': 0.000674031044843425, 'batch_size': 239, 'step_size': 5, 'gamma': 0.9054971005973907}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:21:37,143][0m Trial 43 finished with value: 0.03238500753548323 and parameters: {'observation_period_num': 20, 'train_rates': 0.7670209007969654, 'learning_rate': 0.0009260181738053926, 'batch_size': 239, 'step_size': 6, 'gamma': 0.8851263370824335}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:21:59,883][0m Trial 44 finished with value: 0.03774798476308055 and parameters: {'observation_period_num': 5, 'train_rates': 0.7708500192537011, 'learning_rate': 0.0009937329638415125, 'batch_size': 256, 'step_size': 5, 'gamma': 0.8589907784038072}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:22:25,314][0m Trial 45 finished with value: 0.03391702577724625 and parameters: {'observation_period_num': 20, 'train_rates': 0.7389595469819009, 'learning_rate': 0.0004832197593798833, 'batch_size': 226, 'step_size': 7, 'gamma': 0.8813901233371467}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:22:49,971][0m Trial 46 finished with value: 0.048872338600025864 and parameters: {'observation_period_num': 16, 'train_rates': 0.7588048683011811, 'learning_rate': 0.00031520723624564796, 'batch_size': 240, 'step_size': 3, 'gamma': 0.8725332603715401}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:23:21,594][0m Trial 47 finished with value: 0.03093074015983423 and parameters: {'observation_period_num': 40, 'train_rates': 0.8059000051074643, 'learning_rate': 0.0009809556841139898, 'batch_size': 188, 'step_size': 8, 'gamma': 0.8971702292689903}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:23:52,490][0m Trial 48 finished with value: 0.03305351573729167 and parameters: {'observation_period_num': 67, 'train_rates': 0.8102271094424282, 'learning_rate': 0.0009779267083683512, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8375488746377173}. Best is trial 18 with value: 0.026788822512197143.[0m
[32m[I 2025-02-03 09:24:29,776][0m Trial 49 finished with value: 0.1376817112395808 and parameters: {'observation_period_num': 187, 'train_rates': 0.8269834242876563, 'learning_rate': 1.6655619990515184e-05, 'batch_size': 149, 'step_size': 8, 'gamma': 0.8925638100127006}. Best is trial 18 with value: 0.026788822512197143.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_BA_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1742 | 0.0791
Epoch 2/300, Loss: 0.0997 | 0.0859
Epoch 3/300, Loss: 0.0838 | 0.0755
Epoch 4/300, Loss: 0.0714 | 0.0548
Epoch 5/300, Loss: 0.0670 | 0.0580
Epoch 6/300, Loss: 0.0692 | 0.1089
Epoch 7/300, Loss: 0.0742 | 0.0531
Epoch 8/300, Loss: 0.0574 | 0.0459
Epoch 9/300, Loss: 0.0444 | 0.0392
Epoch 10/300, Loss: 0.0387 | 0.0412
Epoch 11/300, Loss: 0.0392 | 0.0400
Epoch 12/300, Loss: 0.0368 | 0.0386
Epoch 13/300, Loss: 0.0345 | 0.0350
Epoch 14/300, Loss: 0.0318 | 0.0383
Epoch 15/300, Loss: 0.0302 | 0.0339
Epoch 16/300, Loss: 0.0275 | 0.0348
Epoch 17/300, Loss: 0.0256 | 0.0330
Epoch 18/300, Loss: 0.0251 | 0.0330
Epoch 19/300, Loss: 0.0238 | 0.0328
Epoch 20/300, Loss: 0.0229 | 0.0321
Epoch 21/300, Loss: 0.0222 | 0.0318
Epoch 22/300, Loss: 0.0215 | 0.0313
Epoch 23/300, Loss: 0.0209 | 0.0313
Epoch 24/300, Loss: 0.0204 | 0.0305
Epoch 25/300, Loss: 0.0198 | 0.0306
Epoch 26/300, Loss: 0.0194 | 0.0304
Epoch 27/300, Loss: 0.0189 | 0.0305
Epoch 28/300, Loss: 0.0187 | 0.0306
Epoch 29/300, Loss: 0.0184 | 0.0305
Epoch 30/300, Loss: 0.0183 | 0.0301
Epoch 31/300, Loss: 0.0180 | 0.0308
Epoch 32/300, Loss: 0.0175 | 0.0303
Epoch 33/300, Loss: 0.0173 | 0.0302
Epoch 34/300, Loss: 0.0169 | 0.0299
Epoch 35/300, Loss: 0.0165 | 0.0304
Epoch 36/300, Loss: 0.0164 | 0.0297
Epoch 37/300, Loss: 0.0162 | 0.0305
Epoch 38/300, Loss: 0.0162 | 0.0297
Epoch 39/300, Loss: 0.0158 | 0.0301
Epoch 40/300, Loss: 0.0156 | 0.0297
Epoch 41/300, Loss: 0.0153 | 0.0296
Epoch 42/300, Loss: 0.0151 | 0.0294
Epoch 43/300, Loss: 0.0149 | 0.0294
Epoch 44/300, Loss: 0.0148 | 0.0289
Epoch 45/300, Loss: 0.0146 | 0.0290
Epoch 46/300, Loss: 0.0145 | 0.0285
Epoch 47/300, Loss: 0.0144 | 0.0286
Epoch 48/300, Loss: 0.0142 | 0.0286
Epoch 49/300, Loss: 0.0141 | 0.0284
Epoch 50/300, Loss: 0.0140 | 0.0285
Epoch 51/300, Loss: 0.0139 | 0.0284
Epoch 52/300, Loss: 0.0138 | 0.0284
Epoch 53/300, Loss: 0.0137 | 0.0285
Epoch 54/300, Loss: 0.0137 | 0.0284
Epoch 55/300, Loss: 0.0136 | 0.0285
Epoch 56/300, Loss: 0.0135 | 0.0284
Epoch 57/300, Loss: 0.0134 | 0.0285
Epoch 58/300, Loss: 0.0133 | 0.0285
Epoch 59/300, Loss: 0.0132 | 0.0285
Epoch 60/300, Loss: 0.0131 | 0.0285
Epoch 61/300, Loss: 0.0131 | 0.0285
Epoch 62/300, Loss: 0.0130 | 0.0285
Epoch 63/300, Loss: 0.0129 | 0.0285
Epoch 64/300, Loss: 0.0128 | 0.0285
Epoch 65/300, Loss: 0.0128 | 0.0285
Epoch 66/300, Loss: 0.0127 | 0.0284
Epoch 67/300, Loss: 0.0126 | 0.0284
Epoch 68/300, Loss: 0.0126 | 0.0284
Epoch 69/300, Loss: 0.0125 | 0.0283
Epoch 70/300, Loss: 0.0124 | 0.0283
Epoch 71/300, Loss: 0.0123 | 0.0282
Epoch 72/300, Loss: 0.0123 | 0.0282
Epoch 73/300, Loss: 0.0122 | 0.0282
Epoch 74/300, Loss: 0.0122 | 0.0282
Epoch 75/300, Loss: 0.0121 | 0.0281
Epoch 76/300, Loss: 0.0120 | 0.0281
Epoch 77/300, Loss: 0.0120 | 0.0281
Epoch 78/300, Loss: 0.0119 | 0.0281
Epoch 79/300, Loss: 0.0119 | 0.0280
Epoch 80/300, Loss: 0.0118 | 0.0280
Epoch 81/300, Loss: 0.0118 | 0.0280
Epoch 82/300, Loss: 0.0117 | 0.0280
Epoch 83/300, Loss: 0.0117 | 0.0280
Epoch 84/300, Loss: 0.0117 | 0.0279
Epoch 85/300, Loss: 0.0116 | 0.0279
Epoch 86/300, Loss: 0.0116 | 0.0279
Epoch 87/300, Loss: 0.0116 | 0.0279
Epoch 88/300, Loss: 0.0115 | 0.0279
Epoch 89/300, Loss: 0.0115 | 0.0279
Epoch 90/300, Loss: 0.0115 | 0.0279
Epoch 91/300, Loss: 0.0114 | 0.0279
Epoch 92/300, Loss: 0.0114 | 0.0278
Epoch 93/300, Loss: 0.0114 | 0.0278
Epoch 94/300, Loss: 0.0114 | 0.0278
Epoch 95/300, Loss: 0.0114 | 0.0278
Epoch 96/300, Loss: 0.0113 | 0.0278
Epoch 97/300, Loss: 0.0113 | 0.0278
Epoch 98/300, Loss: 0.0113 | 0.0278
Epoch 99/300, Loss: 0.0113 | 0.0278
Epoch 100/300, Loss: 0.0113 | 0.0278
Epoch 101/300, Loss: 0.0113 | 0.0278
Epoch 102/300, Loss: 0.0113 | 0.0278
Epoch 103/300, Loss: 0.0113 | 0.0278
Epoch 104/300, Loss: 0.0112 | 0.0278
Epoch 105/300, Loss: 0.0112 | 0.0278
Epoch 106/300, Loss: 0.0112 | 0.0278
Epoch 107/300, Loss: 0.0112 | 0.0278
Epoch 108/300, Loss: 0.0112 | 0.0278
Epoch 109/300, Loss: 0.0112 | 0.0278
Epoch 110/300, Loss: 0.0112 | 0.0278
Epoch 111/300, Loss: 0.0112 | 0.0278
Epoch 112/300, Loss: 0.0112 | 0.0278
Epoch 113/300, Loss: 0.0112 | 0.0278
Epoch 114/300, Loss: 0.0112 | 0.0278
Epoch 115/300, Loss: 0.0112 | 0.0278
Epoch 116/300, Loss: 0.0112 | 0.0278
Epoch 117/300, Loss: 0.0112 | 0.0278
Epoch 118/300, Loss: 0.0111 | 0.0278
Epoch 119/300, Loss: 0.0111 | 0.0278
Epoch 120/300, Loss: 0.0111 | 0.0278
Epoch 121/300, Loss: 0.0111 | 0.0278
Epoch 122/300, Loss: 0.0111 | 0.0278
Epoch 123/300, Loss: 0.0111 | 0.0278
Epoch 124/300, Loss: 0.0111 | 0.0278
Epoch 125/300, Loss: 0.0111 | 0.0278
Epoch 126/300, Loss: 0.0111 | 0.0278
Epoch 127/300, Loss: 0.0111 | 0.0278
Epoch 128/300, Loss: 0.0111 | 0.0278
Epoch 129/300, Loss: 0.0111 | 0.0278
Epoch 130/300, Loss: 0.0111 | 0.0278
Epoch 131/300, Loss: 0.0111 | 0.0278
Epoch 132/300, Loss: 0.0111 | 0.0278
Epoch 133/300, Loss: 0.0111 | 0.0277
Epoch 134/300, Loss: 0.0111 | 0.0277
Epoch 135/300, Loss: 0.0111 | 0.0277
Epoch 136/300, Loss: 0.0111 | 0.0277
Epoch 137/300, Loss: 0.0111 | 0.0277
Epoch 138/300, Loss: 0.0111 | 0.0277
Epoch 139/300, Loss: 0.0111 | 0.0277
Epoch 140/300, Loss: 0.0111 | 0.0277
Epoch 141/300, Loss: 0.0111 | 0.0277
Epoch 142/300, Loss: 0.0111 | 0.0277
Epoch 143/300, Loss: 0.0111 | 0.0277
Epoch 144/300, Loss: 0.0111 | 0.0277
Epoch 145/300, Loss: 0.0111 | 0.0277
Epoch 146/300, Loss: 0.0111 | 0.0277
Epoch 147/300, Loss: 0.0111 | 0.0277
Epoch 148/300, Loss: 0.0111 | 0.0277
Epoch 149/300, Loss: 0.0111 | 0.0277
Epoch 150/300, Loss: 0.0111 | 0.0277
Epoch 151/300, Loss: 0.0111 | 0.0277
Epoch 152/300, Loss: 0.0111 | 0.0277
Epoch 153/300, Loss: 0.0111 | 0.0277
Epoch 154/300, Loss: 0.0111 | 0.0277
Epoch 155/300, Loss: 0.0111 | 0.0277
Epoch 156/300, Loss: 0.0111 | 0.0277
Epoch 157/300, Loss: 0.0111 | 0.0277
Epoch 158/300, Loss: 0.0111 | 0.0277
Epoch 159/300, Loss: 0.0111 | 0.0277
Epoch 160/300, Loss: 0.0111 | 0.0277
Epoch 161/300, Loss: 0.0111 | 0.0277
Epoch 162/300, Loss: 0.0111 | 0.0277
Epoch 163/300, Loss: 0.0111 | 0.0277
Epoch 164/300, Loss: 0.0111 | 0.0277
Epoch 165/300, Loss: 0.0111 | 0.0277
Epoch 166/300, Loss: 0.0111 | 0.0277
Epoch 167/300, Loss: 0.0111 | 0.0277
Epoch 168/300, Loss: 0.0111 | 0.0277
Epoch 169/300, Loss: 0.0111 | 0.0277
Epoch 170/300, Loss: 0.0111 | 0.0277
Epoch 171/300, Loss: 0.0111 | 0.0277
Epoch 172/300, Loss: 0.0111 | 0.0277
Epoch 173/300, Loss: 0.0111 | 0.0277
Epoch 174/300, Loss: 0.0111 | 0.0277
Epoch 175/300, Loss: 0.0111 | 0.0277
Epoch 176/300, Loss: 0.0111 | 0.0277
Epoch 177/300, Loss: 0.0111 | 0.0277
Epoch 178/300, Loss: 0.0111 | 0.0277
Epoch 179/300, Loss: 0.0111 | 0.0277
Epoch 180/300, Loss: 0.0111 | 0.0277
Epoch 181/300, Loss: 0.0111 | 0.0277
Epoch 182/300, Loss: 0.0111 | 0.0277
Epoch 183/300, Loss: 0.0111 | 0.0277
Epoch 184/300, Loss: 0.0111 | 0.0277
Epoch 185/300, Loss: 0.0111 | 0.0277
Epoch 186/300, Loss: 0.0111 | 0.0277
Epoch 187/300, Loss: 0.0111 | 0.0277
Epoch 188/300, Loss: 0.0111 | 0.0277
Epoch 189/300, Loss: 0.0111 | 0.0277
Epoch 190/300, Loss: 0.0111 | 0.0277
Epoch 191/300, Loss: 0.0111 | 0.0277
Epoch 192/300, Loss: 0.0111 | 0.0277
Epoch 193/300, Loss: 0.0111 | 0.0277
Epoch 194/300, Loss: 0.0111 | 0.0277
Epoch 195/300, Loss: 0.0111 | 0.0277
Epoch 196/300, Loss: 0.0111 | 0.0277
Epoch 197/300, Loss: 0.0111 | 0.0277
Epoch 198/300, Loss: 0.0111 | 0.0277
Epoch 199/300, Loss: 0.0111 | 0.0277
Epoch 200/300, Loss: 0.0111 | 0.0277
Epoch 201/300, Loss: 0.0111 | 0.0277
Epoch 202/300, Loss: 0.0111 | 0.0277
Epoch 203/300, Loss: 0.0111 | 0.0277
Epoch 204/300, Loss: 0.0111 | 0.0277
Epoch 205/300, Loss: 0.0111 | 0.0277
Epoch 206/300, Loss: 0.0111 | 0.0277
Epoch 207/300, Loss: 0.0111 | 0.0277
Epoch 208/300, Loss: 0.0111 | 0.0277
Epoch 209/300, Loss: 0.0111 | 0.0277
Epoch 210/300, Loss: 0.0111 | 0.0277
Epoch 211/300, Loss: 0.0111 | 0.0277
Epoch 212/300, Loss: 0.0111 | 0.0277
Epoch 213/300, Loss: 0.0111 | 0.0277
Epoch 214/300, Loss: 0.0111 | 0.0277
Epoch 215/300, Loss: 0.0111 | 0.0277
Epoch 216/300, Loss: 0.0111 | 0.0277
Epoch 217/300, Loss: 0.0111 | 0.0277
Epoch 218/300, Loss: 0.0111 | 0.0277
Epoch 219/300, Loss: 0.0111 | 0.0277
Epoch 220/300, Loss: 0.0111 | 0.0277
Epoch 221/300, Loss: 0.0111 | 0.0277
Epoch 222/300, Loss: 0.0111 | 0.0277
Epoch 223/300, Loss: 0.0111 | 0.0277
Epoch 224/300, Loss: 0.0111 | 0.0277
Epoch 225/300, Loss: 0.0111 | 0.0277
Epoch 226/300, Loss: 0.0111 | 0.0277
Epoch 227/300, Loss: 0.0111 | 0.0277
Epoch 228/300, Loss: 0.0111 | 0.0277
Epoch 229/300, Loss: 0.0111 | 0.0277
Epoch 230/300, Loss: 0.0111 | 0.0277
Epoch 231/300, Loss: 0.0111 | 0.0277
Epoch 232/300, Loss: 0.0111 | 0.0277
Epoch 233/300, Loss: 0.0111 | 0.0277
Epoch 234/300, Loss: 0.0111 | 0.0277
Epoch 235/300, Loss: 0.0111 | 0.0277
Epoch 236/300, Loss: 0.0111 | 0.0277
Epoch 237/300, Loss: 0.0111 | 0.0277
Epoch 238/300, Loss: 0.0111 | 0.0277
Epoch 239/300, Loss: 0.0111 | 0.0277
Epoch 240/300, Loss: 0.0111 | 0.0277
Epoch 241/300, Loss: 0.0111 | 0.0277
Epoch 242/300, Loss: 0.0111 | 0.0277
Epoch 243/300, Loss: 0.0111 | 0.0277
Epoch 244/300, Loss: 0.0111 | 0.0277
Epoch 245/300, Loss: 0.0111 | 0.0277
Epoch 246/300, Loss: 0.0111 | 0.0277
Epoch 247/300, Loss: 0.0111 | 0.0277
Epoch 248/300, Loss: 0.0111 | 0.0277
Epoch 249/300, Loss: 0.0111 | 0.0277
Epoch 250/300, Loss: 0.0111 | 0.0277
Epoch 251/300, Loss: 0.0111 | 0.0277
Early stopping
Runtime (seconds): 782.7570197582245
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 63.23302461975254
RMSE: 7.9519195556640625
MAE: 7.9519195556640625
R-squared: nan
[160.35191]
