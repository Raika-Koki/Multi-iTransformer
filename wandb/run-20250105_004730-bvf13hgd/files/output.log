ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-05 00:47:31,552][0m A new study created in memory with name: no-name-96a0ed43-1bf3-498a-97fb-d8fbc6b6be1c[0m
[32m[I 2025-01-05 00:47:51,703][0m Trial 0 finished with value: 0.30748427888708874 and parameters: {'observation_period_num': 142, 'train_rates': 0.6731994149525227, 'learning_rate': 3.622757438004278e-05, 'batch_size': 238, 'step_size': 2, 'gamma': 0.9752062132722185}. Best is trial 0 with value: 0.30748427888708874.[0m
[32m[I 2025-01-05 00:49:48,632][0m Trial 1 finished with value: 0.30686033904219473 and parameters: {'observation_period_num': 91, 'train_rates': 0.7501575857245752, 'learning_rate': 5.7351930803869704e-05, 'batch_size': 41, 'step_size': 1, 'gamma': 0.895503258174833}. Best is trial 1 with value: 0.30686033904219473.[0m
[32m[I 2025-01-05 00:50:11,528][0m Trial 2 finished with value: 0.04315081561302266 and parameters: {'observation_period_num': 30, 'train_rates': 0.8317762559786007, 'learning_rate': 0.0005905590461871001, 'batch_size': 249, 'step_size': 10, 'gamma': 0.8224349990822205}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:50:40,679][0m Trial 3 finished with value: 0.05622566484224301 and parameters: {'observation_period_num': 39, 'train_rates': 0.8929187701671318, 'learning_rate': 0.0005329187709551599, 'batch_size': 211, 'step_size': 9, 'gamma': 0.7860949957515377}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:51:05,807][0m Trial 4 finished with value: 0.46832986294132906 and parameters: {'observation_period_num': 124, 'train_rates': 0.65243870509631, 'learning_rate': 2.2987838289422692e-05, 'batch_size': 187, 'step_size': 8, 'gamma': 0.822726743825048}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:51:28,659][0m Trial 5 finished with value: 1.2991875294420019 and parameters: {'observation_period_num': 127, 'train_rates': 0.6555847184029806, 'learning_rate': 1.6828460441862883e-06, 'batch_size': 209, 'step_size': 2, 'gamma': 0.850662928445197}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:52:44,735][0m Trial 6 finished with value: 0.08257155824196703 and parameters: {'observation_period_num': 125, 'train_rates': 0.8150547245165761, 'learning_rate': 4.2180963800938756e-05, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9290177764868143}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:53:27,760][0m Trial 7 finished with value: 0.15110271370179648 and parameters: {'observation_period_num': 146, 'train_rates': 0.7983420768238603, 'learning_rate': 8.060339625233544e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9042390461448192}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:53:56,491][0m Trial 8 finished with value: 0.20058174472609439 and parameters: {'observation_period_num': 85, 'train_rates': 0.7298565380801139, 'learning_rate': 0.0009585181093406943, 'batch_size': 179, 'step_size': 8, 'gamma': 0.7652474731134727}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:54:32,344][0m Trial 9 finished with value: 0.30177768890125434 and parameters: {'observation_period_num': 216, 'train_rates': 0.6981229111544488, 'learning_rate': 0.00010816370247923269, 'batch_size': 134, 'step_size': 11, 'gamma': 0.9133810134287719}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:55:39,632][0m Trial 10 finished with value: 0.15432700514793396 and parameters: {'observation_period_num': 12, 'train_rates': 0.9884029545811839, 'learning_rate': 3.7266617231269452e-06, 'batch_size': 92, 'step_size': 15, 'gamma': 0.8167309972736261}. Best is trial 2 with value: 0.04315081561302266.[0m
[32m[I 2025-01-05 00:56:04,695][0m Trial 11 finished with value: 0.03380466294207602 and parameters: {'observation_period_num': 6, 'train_rates': 0.8877838981367104, 'learning_rate': 0.0007040721984630328, 'batch_size': 253, 'step_size': 10, 'gamma': 0.7732479448945758}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:56:29,736][0m Trial 12 finished with value: 0.05839404070958026 and parameters: {'observation_period_num': 5, 'train_rates': 0.8817771403414342, 'learning_rate': 0.0002808504159309491, 'batch_size': 255, 'step_size': 5, 'gamma': 0.7598395018931035}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:57:05,419][0m Trial 13 finished with value: 0.04743414409513909 and parameters: {'observation_period_num': 57, 'train_rates': 0.8837939525910322, 'learning_rate': 0.0002933963297717173, 'batch_size': 168, 'step_size': 11, 'gamma': 0.8094006188517205}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:57:32,320][0m Trial 14 finished with value: 0.3015354573726654 and parameters: {'observation_period_num': 46, 'train_rates': 0.9579687550783156, 'learning_rate': 1.0813868642399248e-05, 'batch_size': 234, 'step_size': 6, 'gamma': 0.8520947550572117}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:57:55,634][0m Trial 15 finished with value: 0.11495176789372466 and parameters: {'observation_period_num': 242, 'train_rates': 0.8417078864464779, 'learning_rate': 0.00019132136344206818, 'batch_size': 250, 'step_size': 11, 'gamma': 0.8008947400818968}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:58:24,649][0m Trial 16 finished with value: 0.05612989887595177 and parameters: {'observation_period_num': 78, 'train_rates': 0.93171850551159, 'learning_rate': 0.0008942139104554881, 'batch_size': 214, 'step_size': 13, 'gamma': 0.8409283919498762}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:58:57,414][0m Trial 17 finished with value: 0.26296787123674753 and parameters: {'observation_period_num': 179, 'train_rates': 0.7598079344138007, 'learning_rate': 0.00045144365440477944, 'batch_size': 153, 'step_size': 6, 'gamma': 0.7807683408788203}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 00:59:47,785][0m Trial 18 finished with value: 0.044266116202012144 and parameters: {'observation_period_num': 34, 'train_rates': 0.8484040658288641, 'learning_rate': 0.00016407642476720509, 'batch_size': 112, 'step_size': 10, 'gamma': 0.7860689791519808}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:00:12,999][0m Trial 19 finished with value: 0.15259233820204443 and parameters: {'observation_period_num': 18, 'train_rates': 0.6010139451817804, 'learning_rate': 0.00043542865323090716, 'batch_size': 194, 'step_size': 13, 'gamma': 0.7513029852846715}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:00:38,939][0m Trial 20 finished with value: 0.2115660294343277 and parameters: {'observation_period_num': 67, 'train_rates': 0.8397145958695488, 'learning_rate': 9.623102635579864e-06, 'batch_size': 229, 'step_size': 15, 'gamma': 0.8646244146353953}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:01:31,920][0m Trial 21 finished with value: 0.04270643214126248 and parameters: {'observation_period_num': 34, 'train_rates': 0.8488715794172409, 'learning_rate': 0.00014054801866361142, 'batch_size': 105, 'step_size': 10, 'gamma': 0.789460727070205}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:02:38,768][0m Trial 22 finished with value: 0.0412709128227413 and parameters: {'observation_period_num': 29, 'train_rates': 0.9340634093675406, 'learning_rate': 0.00013203599603740763, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8325955660817622}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:03:47,208][0m Trial 23 finished with value: 0.05713127180059471 and parameters: {'observation_period_num': 61, 'train_rates': 0.9264677334705063, 'learning_rate': 0.00012724374727101262, 'batch_size': 85, 'step_size': 8, 'gamma': 0.7901874089337625}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:06:55,076][0m Trial 24 finished with value: 0.04212404102810964 and parameters: {'observation_period_num': 7, 'train_rates': 0.9162437620279567, 'learning_rate': 2.099794881834152e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.8318198149266348}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:12:16,088][0m Trial 25 finished with value: 0.06890590287810933 and parameters: {'observation_period_num': 104, 'train_rates': 0.9208444051046039, 'learning_rate': 1.6844643791586007e-05, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8851332945032845}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:14:01,649][0m Trial 26 finished with value: 0.08804531395435333 and parameters: {'observation_period_num': 7, 'train_rates': 0.9819079769026287, 'learning_rate': 6.308987081542384e-06, 'batch_size': 57, 'step_size': 6, 'gamma': 0.8388300459810955}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:18:04,564][0m Trial 27 finished with value: 0.03380919180133126 and parameters: {'observation_period_num': 23, 'train_rates': 0.9551996244863454, 'learning_rate': 6.683735764533976e-05, 'batch_size': 24, 'step_size': 7, 'gamma': 0.8747092252821279}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:19:20,577][0m Trial 28 finished with value: 0.0852421610192819 and parameters: {'observation_period_num': 102, 'train_rates': 0.9578200765786512, 'learning_rate': 7.644482599142809e-05, 'batch_size': 78, 'step_size': 4, 'gamma': 0.9407444562487123}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:21:23,642][0m Trial 29 finished with value: 0.07177251081811178 and parameters: {'observation_period_num': 53, 'train_rates': 0.9627641534956752, 'learning_rate': 3.2333771227434085e-05, 'batch_size': 48, 'step_size': 9, 'gamma': 0.9747752153006018}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:22:03,443][0m Trial 30 finished with value: 0.07968552718945854 and parameters: {'observation_period_num': 155, 'train_rates': 0.8945089791585971, 'learning_rate': 0.00029458816288179603, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8694948198418225}. Best is trial 11 with value: 0.03380466294207602.[0m
[32m[I 2025-01-05 01:26:59,163][0m Trial 31 finished with value: 0.032206725785471346 and parameters: {'observation_period_num': 23, 'train_rates': 0.9150905790929971, 'learning_rate': 4.735616625640761e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8816352899735924}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:32:11,907][0m Trial 32 finished with value: 0.034838573176127215 and parameters: {'observation_period_num': 25, 'train_rates': 0.9335499641109352, 'learning_rate': 5.550150566054192e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.8897534050355496}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:35:40,899][0m Trial 33 finished with value: 0.037410776726290826 and parameters: {'observation_period_num': 25, 'train_rates': 0.8750791103794516, 'learning_rate': 4.7366581908682346e-05, 'batch_size': 26, 'step_size': 7, 'gamma': 0.8893891743044434}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:37:57,522][0m Trial 34 finished with value: 0.06029716922113529 and parameters: {'observation_period_num': 43, 'train_rates': 0.9465231719366773, 'learning_rate': 5.724360023047035e-05, 'batch_size': 42, 'step_size': 4, 'gamma': 0.8760382943203647}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:43:00,199][0m Trial 35 finished with value: 0.04450381004499446 and parameters: {'observation_period_num': 68, 'train_rates': 0.9038447287176223, 'learning_rate': 2.958322607121496e-05, 'batch_size': 18, 'step_size': 7, 'gamma': 0.9179652184296715}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:45:39,149][0m Trial 36 finished with value: 0.043715562324229656 and parameters: {'observation_period_num': 21, 'train_rates': 0.8649808456340305, 'learning_rate': 1.5941077691900902e-05, 'batch_size': 34, 'step_size': 9, 'gamma': 0.9546049870101575}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:47:10,322][0m Trial 37 finished with value: 0.062267358118498865 and parameters: {'observation_period_num': 43, 'train_rates': 0.9700005355972536, 'learning_rate': 7.455405337664424e-05, 'batch_size': 65, 'step_size': 8, 'gamma': 0.8581141838488987}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:49:06,269][0m Trial 38 finished with value: 0.1954885806589231 and parameters: {'observation_period_num': 21, 'train_rates': 0.9067436588846383, 'learning_rate': 2.9365349478641684e-06, 'batch_size': 49, 'step_size': 3, 'gamma': 0.8989231863831704}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:50:18,798][0m Trial 39 finished with value: 0.07772178071386673 and parameters: {'observation_period_num': 76, 'train_rates': 0.8110824999720858, 'learning_rate': 3.821129905150535e-05, 'batch_size': 72, 'step_size': 5, 'gamma': 0.8811961989239723}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:53:01,873][0m Trial 40 finished with value: 0.09843083369789772 and parameters: {'observation_period_num': 111, 'train_rates': 0.7827357238014232, 'learning_rate': 8.283742776760578e-05, 'batch_size': 30, 'step_size': 7, 'gamma': 0.9081443373890321}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 01:57:43,210][0m Trial 41 finished with value: 0.0466750067175995 and parameters: {'observation_period_num': 31, 'train_rates': 0.8695697825065138, 'learning_rate': 4.7079802009271116e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8910490350834126}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:00:55,526][0m Trial 42 finished with value: 0.03661985209743891 and parameters: {'observation_period_num': 22, 'train_rates': 0.9428959684499123, 'learning_rate': 4.979030995249812e-05, 'batch_size': 30, 'step_size': 8, 'gamma': 0.9270954030294144}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:03:14,839][0m Trial 43 finished with value: 0.05755387149618928 and parameters: {'observation_period_num': 47, 'train_rates': 0.9420622595168162, 'learning_rate': 2.350828575057114e-05, 'batch_size': 41, 'step_size': 8, 'gamma': 0.9472319822183549}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:05:02,054][0m Trial 44 finished with value: 0.03255035827557246 and parameters: {'observation_period_num': 18, 'train_rates': 0.9747445447312876, 'learning_rate': 5.902231201734605e-05, 'batch_size': 56, 'step_size': 10, 'gamma': 0.9216960388635113}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:06:43,584][0m Trial 45 finished with value: 0.043741438537836075 and parameters: {'observation_period_num': 10, 'train_rates': 0.9898224566484223, 'learning_rate': 0.00022774976783857215, 'batch_size': 60, 'step_size': 10, 'gamma': 0.9249383936098547}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:08:37,144][0m Trial 46 finished with value: 0.06581743930776913 and parameters: {'observation_period_num': 52, 'train_rates': 0.9754247783338464, 'learning_rate': 9.634786385862709e-05, 'batch_size': 52, 'step_size': 9, 'gamma': 0.9004685626774181}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:11:02,252][0m Trial 47 finished with value: 0.10452135617379099 and parameters: {'observation_period_num': 180, 'train_rates': 0.9541278564080288, 'learning_rate': 0.0006863218028262885, 'batch_size': 38, 'step_size': 12, 'gamma': 0.8738585327396341}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:11:58,108][0m Trial 48 finished with value: 0.048580089138577816 and parameters: {'observation_period_num': 15, 'train_rates': 0.9063623929622604, 'learning_rate': 6.728846889442925e-05, 'batch_size': 105, 'step_size': 6, 'gamma': 0.8579085541385931}. Best is trial 31 with value: 0.032206725785471346.[0m
[32m[I 2025-01-05 02:16:58,740][0m Trial 49 finished with value: 0.11056799197865423 and parameters: {'observation_period_num': 41, 'train_rates': 0.8900748437748436, 'learning_rate': 1.3663917943861046e-06, 'batch_size': 18, 'step_size': 10, 'gamma': 0.9638765228654348}. Best is trial 31 with value: 0.032206725785471346.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-05 02:16:58,751][0m A new study created in memory with name: no-name-eba01dd9-da57-44fc-8607-f9ccc1d43da0[0m
[32m[I 2025-01-05 02:17:52,367][0m Trial 0 finished with value: 0.19355197984149403 and parameters: {'observation_period_num': 42, 'train_rates': 0.7825964457474148, 'learning_rate': 0.000246400981997753, 'batch_size': 96, 'step_size': 15, 'gamma': 0.9775994772971662}. Best is trial 0 with value: 0.19355197984149403.[0m
[32m[I 2025-01-05 02:19:53,792][0m Trial 1 finished with value: 0.054226452425906534 and parameters: {'observation_period_num': 34, 'train_rates': 0.9674978984637542, 'learning_rate': 6.991891888693969e-05, 'batch_size': 48, 'step_size': 14, 'gamma': 0.9697384663804071}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:20:20,541][0m Trial 2 finished with value: 0.7746499046035434 and parameters: {'observation_period_num': 161, 'train_rates': 0.7120234776510193, 'learning_rate': 6.551038620841062e-06, 'batch_size': 189, 'step_size': 1, 'gamma': 0.9603946976507387}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:22:34,052][0m Trial 3 finished with value: 0.5982506679557956 and parameters: {'observation_period_num': 123, 'train_rates': 0.6201050025351498, 'learning_rate': 1.3093482859197577e-06, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9647026410311721}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:22:57,220][0m Trial 4 finished with value: 0.6063910082883613 and parameters: {'observation_period_num': 53, 'train_rates': 0.6297785749657367, 'learning_rate': 1.5320584642123968e-05, 'batch_size': 212, 'step_size': 2, 'gamma': 0.8851510474224776}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:23:19,431][0m Trial 5 finished with value: 0.6976267856088235 and parameters: {'observation_period_num': 15, 'train_rates': 0.7086760331979225, 'learning_rate': 3.951032047145793e-06, 'batch_size': 255, 'step_size': 6, 'gamma': 0.8194940760596842}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:27:03,129][0m Trial 6 finished with value: 0.11470479450442574 and parameters: {'observation_period_num': 159, 'train_rates': 0.8782140711091331, 'learning_rate': 0.00010621074501235968, 'batch_size': 23, 'step_size': 15, 'gamma': 0.7888943142438979}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:28:45,516][0m Trial 7 finished with value: 0.2731179786631997 and parameters: {'observation_period_num': 43, 'train_rates': 0.859691720177534, 'learning_rate': 1.3832193921368113e-06, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9244452673774934}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:29:51,079][0m Trial 8 finished with value: 0.2572362948622968 and parameters: {'observation_period_num': 214, 'train_rates': 0.9345684090671199, 'learning_rate': 3.063753378519013e-05, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9155198702538221}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:30:27,858][0m Trial 9 finished with value: 0.5843609041637845 and parameters: {'observation_period_num': 153, 'train_rates': 0.9198124869117634, 'learning_rate': 2.206323243439729e-06, 'batch_size': 152, 'step_size': 7, 'gamma': 0.8821805142999573}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:31:23,101][0m Trial 10 finished with value: 0.0892273560166359 and parameters: {'observation_period_num': 96, 'train_rates': 0.9796317997165693, 'learning_rate': 0.0008246929201307197, 'batch_size': 111, 'step_size': 11, 'gamma': 0.8381409865516246}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:32:20,501][0m Trial 11 finished with value: 0.10514719784259796 and parameters: {'observation_period_num': 94, 'train_rates': 0.9890816196169476, 'learning_rate': 0.0006480198295277466, 'batch_size': 108, 'step_size': 11, 'gamma': 0.8331357181252262}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:33:05,831][0m Trial 12 finished with value: 0.07635625451803207 and parameters: {'observation_period_num': 84, 'train_rates': 0.9841665770286128, 'learning_rate': 0.0007656753316313976, 'batch_size': 138, 'step_size': 12, 'gamma': 0.750349839934363}. Best is trial 1 with value: 0.054226452425906534.[0m
[32m[I 2025-01-05 02:33:41,725][0m Trial 13 finished with value: 0.048146828024038975 and parameters: {'observation_period_num': 8, 'train_rates': 0.8293101164424544, 'learning_rate': 9.105257920884294e-05, 'batch_size': 159, 'step_size': 12, 'gamma': 0.7512023659472312}. Best is trial 13 with value: 0.048146828024038975.[0m
[32m[I 2025-01-05 02:34:13,537][0m Trial 14 finished with value: 0.05454470132242754 and parameters: {'observation_period_num': 9, 'train_rates': 0.8021749270448264, 'learning_rate': 9.12524910533703e-05, 'batch_size': 177, 'step_size': 13, 'gamma': 0.7535098079165872}. Best is trial 13 with value: 0.048146828024038975.[0m
[32m[I 2025-01-05 02:35:32,040][0m Trial 15 finished with value: 0.03731443375649223 and parameters: {'observation_period_num': 5, 'train_rates': 0.8358926658140059, 'learning_rate': 7.160554245201239e-05, 'batch_size': 69, 'step_size': 9, 'gamma': 0.9333221556737193}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:36:37,464][0m Trial 16 finished with value: 0.15127802289279044 and parameters: {'observation_period_num': 205, 'train_rates': 0.8435214392297655, 'learning_rate': 0.0002235004364415665, 'batch_size': 79, 'step_size': 9, 'gamma': 0.917708261322667}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:37:12,529][0m Trial 17 finished with value: 0.3590425668449059 and parameters: {'observation_period_num': 68, 'train_rates': 0.7689575342751316, 'learning_rate': 1.4302752827885198e-05, 'batch_size': 152, 'step_size': 9, 'gamma': 0.8546811673065737}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:37:38,366][0m Trial 18 finished with value: 0.1931974436204458 and parameters: {'observation_period_num': 5, 'train_rates': 0.7370952100130354, 'learning_rate': 3.650781181867947e-05, 'batch_size': 216, 'step_size': 10, 'gamma': 0.7939060501623211}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:38:20,634][0m Trial 19 finished with value: 0.06674218980515804 and parameters: {'observation_period_num': 116, 'train_rates': 0.8271111140639774, 'learning_rate': 0.00019055322335232274, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9001000410340394}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:39:38,759][0m Trial 20 finished with value: 0.06971568650497287 and parameters: {'observation_period_num': 62, 'train_rates': 0.8969938294968656, 'learning_rate': 3.342215418490329e-05, 'batch_size': 73, 'step_size': 13, 'gamma': 0.9329785269259578}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:41:28,968][0m Trial 21 finished with value: 0.048021756137084017 and parameters: {'observation_period_num': 28, 'train_rates': 0.9481473268139, 'learning_rate': 7.174862448978525e-05, 'batch_size': 53, 'step_size': 13, 'gamma': 0.9505408447197726}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:42:59,993][0m Trial 22 finished with value: 0.04889448938008986 and parameters: {'observation_period_num': 26, 'train_rates': 0.9221779825021615, 'learning_rate': 4.9693164709224484e-05, 'batch_size': 63, 'step_size': 12, 'gamma': 0.9475738670834635}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:45:29,816][0m Trial 23 finished with value: 0.05204794160270876 and parameters: {'observation_period_num': 26, 'train_rates': 0.8240929850919241, 'learning_rate': 0.00013403220763749476, 'batch_size': 35, 'step_size': 10, 'gamma': 0.9432094302343628}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:46:03,560][0m Trial 24 finished with value: 0.15128426594793984 and parameters: {'observation_period_num': 248, 'train_rates': 0.8822846132236634, 'learning_rate': 1.7217717708636155e-05, 'batch_size': 170, 'step_size': 13, 'gamma': 0.9845554116057399}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:46:45,742][0m Trial 25 finished with value: 0.16082450010302202 and parameters: {'observation_period_num': 5, 'train_rates': 0.7593131307231389, 'learning_rate': 0.00040327172329615975, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8641185608480865}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:47:35,491][0m Trial 26 finished with value: 0.22072334908473212 and parameters: {'observation_period_num': 70, 'train_rates': 0.6749074275283257, 'learning_rate': 6.400591620740632e-05, 'batch_size': 96, 'step_size': 14, 'gamma': 0.9028005656331183}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:52:33,975][0m Trial 27 finished with value: 0.05844741212586834 and parameters: {'observation_period_num': 31, 'train_rates': 0.8004294959118599, 'learning_rate': 0.00040962826611789794, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8024640263473098}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:54:45,099][0m Trial 28 finished with value: 0.07275940677229788 and parameters: {'observation_period_num': 54, 'train_rates': 0.9512805229440541, 'learning_rate': 0.00014192059221514189, 'batch_size': 44, 'step_size': 12, 'gamma': 0.7691398676147942}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:55:45,468][0m Trial 29 finished with value: 0.06343342788569037 and parameters: {'observation_period_num': 43, 'train_rates': 0.9008143372727772, 'learning_rate': 2.2498426108154695e-05, 'batch_size': 96, 'step_size': 15, 'gamma': 0.947207522986303}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:57:21,888][0m Trial 30 finished with value: 0.06673291076323111 and parameters: {'observation_period_num': 22, 'train_rates': 0.8580449171324853, 'learning_rate': 8.517514112642337e-06, 'batch_size': 57, 'step_size': 7, 'gamma': 0.9854744281855212}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 02:58:50,819][0m Trial 31 finished with value: 0.041541727259755135 and parameters: {'observation_period_num': 25, 'train_rates': 0.9326395396518649, 'learning_rate': 5.344769515818484e-05, 'batch_size': 66, 'step_size': 12, 'gamma': 0.9510031321008783}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:00:18,982][0m Trial 32 finished with value: 0.055419696445919965 and parameters: {'observation_period_num': 38, 'train_rates': 0.9481986175665031, 'learning_rate': 5.55304111903916e-05, 'batch_size': 67, 'step_size': 14, 'gamma': 0.9596109075177213}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:01:28,647][0m Trial 33 finished with value: 0.04761889350635034 and parameters: {'observation_period_num': 20, 'train_rates': 0.9542185037952647, 'learning_rate': 8.392589268331482e-05, 'batch_size': 86, 'step_size': 10, 'gamma': 0.970305755657761}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:02:36,687][0m Trial 34 finished with value: 0.08045316423688616 and parameters: {'observation_period_num': 76, 'train_rates': 0.9515241121147976, 'learning_rate': 4.2348615611115546e-05, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9731775124790112}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:04:57,270][0m Trial 35 finished with value: 0.06632881141810053 and parameters: {'observation_period_num': 50, 'train_rates': 0.9144330920589135, 'learning_rate': 0.0003183326802065649, 'batch_size': 40, 'step_size': 10, 'gamma': 0.9618680089332116}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:06:45,241][0m Trial 36 finished with value: 0.047171745824964105 and parameters: {'observation_period_num': 26, 'train_rates': 0.9594415649054137, 'learning_rate': 8.618333028628527e-05, 'batch_size': 55, 'step_size': 8, 'gamma': 0.9358667531535351}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:07:39,395][0m Trial 37 finished with value: 0.0768325924873352 and parameters: {'observation_period_num': 108, 'train_rates': 0.8853019147454775, 'learning_rate': 0.00015816848228709108, 'batch_size': 102, 'step_size': 5, 'gamma': 0.9296150723747723}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:10:41,363][0m Trial 38 finished with value: 0.09863920077681541 and parameters: {'observation_period_num': 142, 'train_rates': 0.9645165037424405, 'learning_rate': 0.00010138107227288827, 'batch_size': 31, 'step_size': 8, 'gamma': 0.9048205012719145}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:11:48,534][0m Trial 39 finished with value: 0.1494826876922794 and parameters: {'observation_period_num': 182, 'train_rates': 0.9671119500772055, 'learning_rate': 2.1593067698933287e-05, 'batch_size': 85, 'step_size': 3, 'gamma': 0.9337534209537017}. Best is trial 15 with value: 0.03731443375649223.[0m
[32m[I 2025-01-05 03:13:10,230][0m Trial 40 finished with value: 0.03644310820561189 and parameters: {'observation_period_num': 18, 'train_rates': 0.933751784905584, 'learning_rate': 7.49389817115483e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.9713783202843411}. Best is trial 40 with value: 0.03644310820561189.[0m
[32m[I 2025-01-05 03:14:32,334][0m Trial 41 finished with value: 0.04256318050816103 and parameters: {'observation_period_num': 19, 'train_rates': 0.9264328543694849, 'learning_rate': 7.397198410057974e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.9729406134511345}. Best is trial 40 with value: 0.03644310820561189.[0m
[32m[I 2025-01-05 03:16:01,060][0m Trial 42 finished with value: 0.04855261189002676 and parameters: {'observation_period_num': 41, 'train_rates': 0.9325076342540352, 'learning_rate': 4.9364598796780786e-05, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9881705022781845}. Best is trial 40 with value: 0.03644310820561189.[0m
[32m[I 2025-01-05 03:17:58,001][0m Trial 43 finished with value: 0.06284377757045957 and parameters: {'observation_period_num': 55, 'train_rates': 0.9072038102580703, 'learning_rate': 2.690911351047052e-05, 'batch_size': 48, 'step_size': 7, 'gamma': 0.9588160968778768}. Best is trial 40 with value: 0.03644310820561189.[0m
[32m[I 2025-01-05 03:18:46,172][0m Trial 44 finished with value: 0.04406687475029548 and parameters: {'observation_period_num': 18, 'train_rates': 0.8645079824811286, 'learning_rate': 0.00013931768846019168, 'batch_size': 119, 'step_size': 5, 'gamma': 0.940594722261568}. Best is trial 40 with value: 0.03644310820561189.[0m
[32m[I 2025-01-05 03:19:34,869][0m Trial 45 finished with value: 0.03861139890025644 and parameters: {'observation_period_num': 15, 'train_rates': 0.8496768899302475, 'learning_rate': 0.00028524342309757544, 'batch_size': 117, 'step_size': 4, 'gamma': 0.9724845982938501}. Best is trial 40 with value: 0.03644310820561189.[0m
[32m[I 2025-01-05 03:20:49,870][0m Trial 46 finished with value: 0.03265373161102229 and parameters: {'observation_period_num': 16, 'train_rates': 0.8723360800596455, 'learning_rate': 0.00029467494425764156, 'batch_size': 75, 'step_size': 2, 'gamma': 0.9733126224241611}. Best is trial 46 with value: 0.03265373161102229.[0m
[32m[I 2025-01-05 03:21:40,766][0m Trial 47 finished with value: 0.08182432129979134 and parameters: {'observation_period_num': 39, 'train_rates': 0.8477821951913838, 'learning_rate': 0.0005580948128138659, 'batch_size': 110, 'step_size': 3, 'gamma': 0.9770047689194142}. Best is trial 46 with value: 0.03265373161102229.[0m
[32m[I 2025-01-05 03:22:22,686][0m Trial 48 finished with value: 0.04508839568562722 and parameters: {'observation_period_num': 13, 'train_rates': 0.8790289439318358, 'learning_rate': 0.00026447137250613695, 'batch_size': 142, 'step_size': 1, 'gamma': 0.9566914354613092}. Best is trial 46 with value: 0.03265373161102229.[0m
[32m[I 2025-01-05 03:23:18,445][0m Trial 49 finished with value: 0.05036946205626533 and parameters: {'observation_period_num': 49, 'train_rates': 0.7898525968527585, 'learning_rate': 0.0009462752624478745, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9236093001671843}. Best is trial 46 with value: 0.03265373161102229.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-05 03:23:18,456][0m A new study created in memory with name: no-name-89ab03f4-fb60-4620-8b42-e24fd6f7021e[0m
[32m[I 2025-01-05 03:23:43,491][0m Trial 0 finished with value: 0.25850232433250203 and parameters: {'observation_period_num': 142, 'train_rates': 0.8428006535848314, 'learning_rate': 1.0561108271150354e-05, 'batch_size': 229, 'step_size': 10, 'gamma': 0.8351799229491466}. Best is trial 0 with value: 0.25850232433250203.[0m
[32m[I 2025-01-05 03:24:51,775][0m Trial 1 finished with value: 0.0761619677085702 and parameters: {'observation_period_num': 102, 'train_rates': 0.9283264557072695, 'learning_rate': 2.454207951464114e-05, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8866516214306911}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:25:18,419][0m Trial 2 finished with value: 0.41648493786804547 and parameters: {'observation_period_num': 162, 'train_rates': 0.9061632253492227, 'learning_rate': 2.7642138628269454e-06, 'batch_size': 213, 'step_size': 15, 'gamma': 0.9852244848705719}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:26:24,225][0m Trial 3 finished with value: 0.142384434185655 and parameters: {'observation_period_num': 92, 'train_rates': 0.8247688457094123, 'learning_rate': 7.86192714891764e-06, 'batch_size': 80, 'step_size': 15, 'gamma': 0.8895531365072148}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:26:45,887][0m Trial 4 finished with value: 0.3980151701432008 and parameters: {'observation_period_num': 191, 'train_rates': 0.750978610750753, 'learning_rate': 4.341791591489252e-05, 'batch_size': 233, 'step_size': 8, 'gamma': 0.7588919579375115}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:27:25,574][0m Trial 5 finished with value: 0.288117974829555 and parameters: {'observation_period_num': 242, 'train_rates': 0.7040459356011581, 'learning_rate': 0.00012239692704356072, 'batch_size': 119, 'step_size': 8, 'gamma': 0.9487636685902276}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:28:40,001][0m Trial 6 finished with value: 0.14000904462556904 and parameters: {'observation_period_num': 8, 'train_rates': 0.6554117708091912, 'learning_rate': 2.829053559432961e-05, 'batch_size': 61, 'step_size': 14, 'gamma': 0.9635285243576346}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:30:04,868][0m Trial 7 finished with value: 0.2027990077499444 and parameters: {'observation_period_num': 31, 'train_rates': 0.8314170743778698, 'learning_rate': 5.131868741614843e-06, 'batch_size': 63, 'step_size': 5, 'gamma': 0.8111020071636033}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:30:31,785][0m Trial 8 finished with value: 0.2772061663309923 and parameters: {'observation_period_num': 103, 'train_rates': 0.7069273574943375, 'learning_rate': 5.218250572154131e-05, 'batch_size': 191, 'step_size': 8, 'gamma': 0.7882223456970983}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:31:50,729][0m Trial 9 finished with value: 0.20989102135985385 and parameters: {'observation_period_num': 153, 'train_rates': 0.606924769146858, 'learning_rate': 0.000211659414459749, 'batch_size': 53, 'step_size': 15, 'gamma': 0.9451393363645832}. Best is trial 1 with value: 0.0761619677085702.[0m
[32m[I 2025-01-05 03:37:35,300][0m Trial 10 finished with value: 0.04111691809406406 and parameters: {'observation_period_num': 55, 'train_rates': 0.9870372548778132, 'learning_rate': 0.0007641407187903165, 'batch_size': 17, 'step_size': 2, 'gamma': 0.8804671166425804}. Best is trial 10 with value: 0.04111691809406406.[0m
[32m[I 2025-01-05 03:40:45,966][0m Trial 11 finished with value: 0.060998806611020515 and parameters: {'observation_period_num': 71, 'train_rates': 0.983885451959712, 'learning_rate': 0.0004220241880767837, 'batch_size': 31, 'step_size': 1, 'gamma': 0.893212245687053}. Best is trial 10 with value: 0.04111691809406406.[0m
[32m[I 2025-01-05 03:44:26,043][0m Trial 12 finished with value: 0.04773884266614914 and parameters: {'observation_period_num': 50, 'train_rates': 0.987666054116235, 'learning_rate': 0.0009915294264197757, 'batch_size': 27, 'step_size': 1, 'gamma': 0.8565725483980995}. Best is trial 10 with value: 0.04111691809406406.[0m
[32m[I 2025-01-05 03:50:25,721][0m Trial 13 finished with value: 0.0650138276918181 and parameters: {'observation_period_num': 52, 'train_rates': 0.9700434812313848, 'learning_rate': 0.0009756597933493103, 'batch_size': 16, 'step_size': 1, 'gamma': 0.8487466374968866}. Best is trial 10 with value: 0.04111691809406406.[0m
[32m[I 2025-01-05 03:51:03,693][0m Trial 14 finished with value: 0.0279088369631148 and parameters: {'observation_period_num': 6, 'train_rates': 0.8957944714350006, 'learning_rate': 0.0009972504212748658, 'batch_size': 160, 'step_size': 4, 'gamma': 0.9179374712356478}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:51:43,373][0m Trial 15 finished with value: 0.5852530914194444 and parameters: {'observation_period_num': 14, 'train_rates': 0.9077220150367133, 'learning_rate': 1.1493000763869143e-06, 'batch_size': 152, 'step_size': 4, 'gamma': 0.918209213651721}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:52:21,740][0m Trial 16 finished with value: 0.043630058380083804 and parameters: {'observation_period_num': 42, 'train_rates': 0.8883480743925766, 'learning_rate': 0.00031254950176449635, 'batch_size': 157, 'step_size': 4, 'gamma': 0.9203837177936632}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:53:08,785][0m Trial 17 finished with value: 0.05249943429984713 and parameters: {'observation_period_num': 73, 'train_rates': 0.8708924290701203, 'learning_rate': 0.00011945971338637136, 'batch_size': 121, 'step_size': 6, 'gamma': 0.9217837948204055}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:53:44,209][0m Trial 18 finished with value: 0.04608440026640892 and parameters: {'observation_period_num': 8, 'train_rates': 0.941736703338347, 'learning_rate': 0.0005198128113400496, 'batch_size': 174, 'step_size': 3, 'gamma': 0.8248687105810183}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:54:06,106][0m Trial 19 finished with value: 0.25143670093900755 and parameters: {'observation_period_num': 79, 'train_rates': 0.7791158495215895, 'learning_rate': 0.00015982827923780775, 'batch_size': 254, 'step_size': 3, 'gamma': 0.8720430714285436}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:55:02,187][0m Trial 20 finished with value: 0.0743038355401068 and parameters: {'observation_period_num': 122, 'train_rates': 0.9420060377111084, 'learning_rate': 0.0005255808521883131, 'batch_size': 104, 'step_size': 6, 'gamma': 0.9078660697695099}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:55:42,676][0m Trial 21 finished with value: 0.04506657498976144 and parameters: {'observation_period_num': 39, 'train_rates': 0.8758288327451123, 'learning_rate': 0.00036867531237094037, 'batch_size': 150, 'step_size': 3, 'gamma': 0.9328017149917096}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:56:16,377][0m Trial 22 finished with value: 0.04000244581000099 and parameters: {'observation_period_num': 29, 'train_rates': 0.8777803669867542, 'learning_rate': 0.0008253342858555955, 'batch_size': 173, 'step_size': 5, 'gamma': 0.9056643200694351}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:56:46,571][0m Trial 23 finished with value: 0.040241266390187054 and parameters: {'observation_period_num': 26, 'train_rates': 0.7963346521707982, 'learning_rate': 0.0008595356822918467, 'batch_size': 183, 'step_size': 6, 'gamma': 0.8662110872932931}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:57:17,023][0m Trial 24 finished with value: 0.060655788070185644 and parameters: {'observation_period_num': 27, 'train_rates': 0.7892621849924214, 'learning_rate': 7.772985186164328e-05, 'batch_size': 187, 'step_size': 10, 'gamma': 0.861081019436653}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:57:47,815][0m Trial 25 finished with value: 0.043846643999765775 and parameters: {'observation_period_num': 22, 'train_rates': 0.8513743055577311, 'learning_rate': 0.00025554304711420084, 'batch_size': 202, 'step_size': 6, 'gamma': 0.9042589702849851}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:58:21,013][0m Trial 26 finished with value: 0.16050222750057916 and parameters: {'observation_period_num': 5, 'train_rates': 0.7566971276938166, 'learning_rate': 0.0006402708307023404, 'batch_size': 171, 'step_size': 10, 'gamma': 0.9565739692040792}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:58:59,010][0m Trial 27 finished with value: 0.05678428241648389 and parameters: {'observation_period_num': 63, 'train_rates': 0.8095566239291446, 'learning_rate': 0.00023900523245422235, 'batch_size': 139, 'step_size': 7, 'gamma': 0.8453396638651597}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:59:28,439][0m Trial 28 finished with value: 0.14345007182144728 and parameters: {'observation_period_num': 208, 'train_rates': 0.8669917971079979, 'learning_rate': 0.0006154271145452646, 'batch_size': 184, 'step_size': 5, 'gamma': 0.9734991080895132}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 03:59:52,225][0m Trial 29 finished with value: 0.4135830305468652 and parameters: {'observation_period_num': 126, 'train_rates': 0.7260325627736726, 'learning_rate': 1.601272915699964e-05, 'batch_size': 228, 'step_size': 9, 'gamma': 0.8347042934678913}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:00:26,248][0m Trial 30 finished with value: 0.04820387037470937 and parameters: {'observation_period_num': 30, 'train_rates': 0.8361813509542029, 'learning_rate': 8.848851124233916e-05, 'batch_size': 167, 'step_size': 12, 'gamma': 0.9375303118634808}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:01:13,333][0m Trial 31 finished with value: 0.06475543975830078 and parameters: {'observation_period_num': 50, 'train_rates': 0.9642062372935679, 'learning_rate': 0.0009483319216427222, 'batch_size': 132, 'step_size': 2, 'gamma': 0.8754044341028006}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:01:44,439][0m Trial 32 finished with value: 0.03922552777226694 and parameters: {'observation_period_num': 23, 'train_rates': 0.9247512491540151, 'learning_rate': 0.0006552895124630866, 'batch_size': 201, 'step_size': 4, 'gamma': 0.8807995514110867}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:02:13,919][0m Trial 33 finished with value: 0.038839527546907913 and parameters: {'observation_period_num': 21, 'train_rates': 0.9101269332570822, 'learning_rate': 0.0003722822699493115, 'batch_size': 214, 'step_size': 5, 'gamma': 0.8982214509175802}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:02:42,658][0m Trial 34 finished with value: 0.05730332927816689 and parameters: {'observation_period_num': 87, 'train_rates': 0.9153522450871867, 'learning_rate': 0.00039338181725522947, 'batch_size': 214, 'step_size': 4, 'gamma': 0.9021142587282884}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:03:13,471][0m Trial 35 finished with value: 0.05128542333841324 and parameters: {'observation_period_num': 42, 'train_rates': 0.9304747787320538, 'learning_rate': 0.0001683534638916267, 'batch_size': 207, 'step_size': 7, 'gamma': 0.891428903464209}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:03:40,699][0m Trial 36 finished with value: 0.031071473688620035 and parameters: {'observation_period_num': 18, 'train_rates': 0.8962754624992946, 'learning_rate': 0.0005047451268933736, 'batch_size': 231, 'step_size': 5, 'gamma': 0.9281663401806558}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:04:05,688][0m Trial 37 finished with value: 0.09711119298734804 and parameters: {'observation_period_num': 103, 'train_rates': 0.9041891952337818, 'learning_rate': 0.00037204059201110173, 'batch_size': 246, 'step_size': 4, 'gamma': 0.9877640432616707}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:04:34,606][0m Trial 38 finished with value: 0.03217605873942375 and parameters: {'observation_period_num': 18, 'train_rates': 0.9579265747300695, 'learning_rate': 0.000492554521140544, 'batch_size': 226, 'step_size': 7, 'gamma': 0.9297665057777525}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:05:03,702][0m Trial 39 finished with value: 0.06523045897483826 and parameters: {'observation_period_num': 16, 'train_rates': 0.9561784226850537, 'learning_rate': 4.914232168563758e-05, 'batch_size': 229, 'step_size': 7, 'gamma': 0.9331057631039911}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:05:32,892][0m Trial 40 finished with value: 0.11862924695014954 and parameters: {'observation_period_num': 188, 'train_rates': 0.9513189825117683, 'learning_rate': 0.00027052331980931916, 'batch_size': 218, 'step_size': 9, 'gamma': 0.962255240910394}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:06:03,698][0m Trial 41 finished with value: 0.0325269682048741 and parameters: {'observation_period_num': 6, 'train_rates': 0.9199557439713433, 'learning_rate': 0.0005397277695090571, 'batch_size': 200, 'step_size': 5, 'gamma': 0.9277445024982864}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:06:28,877][0m Trial 42 finished with value: 0.03658557476268874 and parameters: {'observation_period_num': 7, 'train_rates': 0.8932885582512096, 'learning_rate': 0.0004871998346632576, 'batch_size': 245, 'step_size': 5, 'gamma': 0.9456787727711196}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:06:54,178][0m Trial 43 finished with value: 0.040486101782932576 and parameters: {'observation_period_num': 5, 'train_rates': 0.8478952950757901, 'learning_rate': 0.0001938020369529379, 'batch_size': 246, 'step_size': 7, 'gamma': 0.972172925553014}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:07:19,193][0m Trial 44 finished with value: 0.050878768087561574 and parameters: {'observation_period_num': 37, 'train_rates': 0.8915028731044836, 'learning_rate': 0.00046935272556596135, 'batch_size': 238, 'step_size': 5, 'gamma': 0.9481494539493032}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:07:45,887][0m Trial 45 finished with value: 0.7555093169212341 and parameters: {'observation_period_num': 63, 'train_rates': 0.9319757930409289, 'learning_rate': 6.07016076348401e-06, 'batch_size': 226, 'step_size': 2, 'gamma': 0.9250987306116368}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:08:16,151][0m Trial 46 finished with value: 0.04323187707580467 and parameters: {'observation_period_num': 12, 'train_rates': 0.8569959790383356, 'learning_rate': 0.00013435637334599827, 'batch_size': 197, 'step_size': 6, 'gamma': 0.9454545697643931}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:08:41,591][0m Trial 47 finished with value: 0.07504591068371813 and parameters: {'observation_period_num': 141, 'train_rates': 0.9000106053081072, 'learning_rate': 0.0006429327089840723, 'batch_size': 241, 'step_size': 8, 'gamma': 0.9138487796727333}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:09:02,788][0m Trial 48 finished with value: 0.14992939655123086 and parameters: {'observation_period_num': 251, 'train_rates': 0.8177511871990673, 'learning_rate': 0.00029584980878645376, 'batch_size': 255, 'step_size': 3, 'gamma': 0.9732228614054069}. Best is trial 14 with value: 0.0279088369631148.[0m
[32m[I 2025-01-05 04:09:32,171][0m Trial 49 finished with value: 0.2743722200393677 and parameters: {'observation_period_num': 5, 'train_rates': 0.9742166454115905, 'learning_rate': 2.6461404764313183e-06, 'batch_size': 221, 'step_size': 5, 'gamma': 0.9370685748343944}. Best is trial 14 with value: 0.0279088369631148.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-05 04:09:32,181][0m A new study created in memory with name: no-name-2df8fd43-e9fb-4cfb-a633-7a9250826eab[0m
[32m[I 2025-01-05 04:10:04,317][0m Trial 0 finished with value: 0.200412978148349 and parameters: {'observation_period_num': 66, 'train_rates': 0.777807960249758, 'learning_rate': 0.00013478684586368332, 'batch_size': 162, 'step_size': 10, 'gamma': 0.9024106043969953}. Best is trial 0 with value: 0.200412978148349.[0m
[32m[I 2025-01-05 04:10:22,305][0m Trial 1 finished with value: 2.0371995494455986 and parameters: {'observation_period_num': 218, 'train_rates': 0.6430096706769901, 'learning_rate': 1.3678472447865094e-06, 'batch_size': 256, 'step_size': 2, 'gamma': 0.8664707792369502}. Best is trial 0 with value: 0.200412978148349.[0m
[32m[I 2025-01-05 04:11:54,440][0m Trial 2 finished with value: 0.24880118154039318 and parameters: {'observation_period_num': 150, 'train_rates': 0.734344936447327, 'learning_rate': 3.1904288130894e-05, 'batch_size': 51, 'step_size': 11, 'gamma': 0.9673954383561119}. Best is trial 0 with value: 0.200412978148349.[0m
[32m[I 2025-01-05 04:12:35,166][0m Trial 3 finished with value: 0.3896790811538121 and parameters: {'observation_period_num': 227, 'train_rates': 0.6963569082966753, 'learning_rate': 1.9663586944648986e-05, 'batch_size': 116, 'step_size': 15, 'gamma': 0.7681380561049118}. Best is trial 0 with value: 0.200412978148349.[0m
Early stopping at epoch 53
[32m[I 2025-01-05 04:12:45,832][0m Trial 4 finished with value: 0.5584698279663337 and parameters: {'observation_period_num': 138, 'train_rates': 0.6010538334404198, 'learning_rate': 0.00010196094111194294, 'batch_size': 254, 'step_size': 1, 'gamma': 0.7871379553906972}. Best is trial 0 with value: 0.200412978148349.[0m
[32m[I 2025-01-05 04:14:27,131][0m Trial 5 finished with value: 0.394430562099324 and parameters: {'observation_period_num': 95, 'train_rates': 0.6686817744531474, 'learning_rate': 3.3296088195771765e-06, 'batch_size': 44, 'step_size': 12, 'gamma': 0.8718999370203623}. Best is trial 0 with value: 0.200412978148349.[0m
[32m[I 2025-01-05 04:14:53,065][0m Trial 6 finished with value: 0.19415573379393153 and parameters: {'observation_period_num': 23, 'train_rates': 0.7603184354320567, 'learning_rate': 4.161704121657516e-05, 'batch_size': 211, 'step_size': 3, 'gamma': 0.984133292295922}. Best is trial 6 with value: 0.19415573379393153.[0m
[32m[I 2025-01-05 04:15:16,990][0m Trial 7 finished with value: 0.8944468704429833 and parameters: {'observation_period_num': 94, 'train_rates': 0.8062703638787074, 'learning_rate': 1.3345724822510066e-06, 'batch_size': 249, 'step_size': 5, 'gamma': 0.8670863517641876}. Best is trial 6 with value: 0.19415573379393153.[0m
[32m[I 2025-01-05 04:15:44,660][0m Trial 8 finished with value: 0.18972761499131838 and parameters: {'observation_period_num': 96, 'train_rates': 0.838792555705952, 'learning_rate': 7.47507391072508e-06, 'batch_size': 206, 'step_size': 13, 'gamma': 0.9690269792723356}. Best is trial 8 with value: 0.18972761499131838.[0m
[32m[I 2025-01-05 04:16:20,744][0m Trial 9 finished with value: 0.29447178988112616 and parameters: {'observation_period_num': 41, 'train_rates': 0.8335590283835123, 'learning_rate': 1.623350725880484e-05, 'batch_size': 153, 'step_size': 2, 'gamma': 0.8155498570419951}. Best is trial 8 with value: 0.18972761499131838.[0m
[32m[I 2025-01-05 04:16:51,706][0m Trial 10 finished with value: 0.11646318435668945 and parameters: {'observation_period_num': 178, 'train_rates': 0.9469384306877588, 'learning_rate': 0.0007298087624928502, 'batch_size': 193, 'step_size': 14, 'gamma': 0.9396841048286946}. Best is trial 10 with value: 0.11646318435668945.[0m
[32m[I 2025-01-05 04:17:22,228][0m Trial 11 finished with value: 0.10236969590187073 and parameters: {'observation_period_num': 173, 'train_rates': 0.9518323553843745, 'learning_rate': 0.0008934963667012158, 'batch_size': 201, 'step_size': 14, 'gamma': 0.9388486593682434}. Best is trial 11 with value: 0.10236969590187073.[0m
[32m[I 2025-01-05 04:17:53,078][0m Trial 12 finished with value: 0.1068512573838234 and parameters: {'observation_period_num': 176, 'train_rates': 0.9731893509178007, 'learning_rate': 0.0009126603913693798, 'batch_size': 194, 'step_size': 15, 'gamma': 0.9225379766540974}. Best is trial 11 with value: 0.10236969590187073.[0m
[32m[I 2025-01-05 04:18:46,338][0m Trial 13 finished with value: 0.09984510391950607 and parameters: {'observation_period_num': 193, 'train_rates': 0.9810083397810194, 'learning_rate': 0.0009230792484657431, 'batch_size': 111, 'step_size': 8, 'gamma': 0.9232521503208362}. Best is trial 13 with value: 0.09984510391950607.[0m
[32m[I 2025-01-05 04:19:40,676][0m Trial 14 finished with value: 0.11550574377179146 and parameters: {'observation_period_num': 252, 'train_rates': 0.9069515159120491, 'learning_rate': 0.0003365998932263117, 'batch_size': 98, 'step_size': 8, 'gamma': 0.9068273014129871}. Best is trial 13 with value: 0.09984510391950607.[0m
[32m[I 2025-01-05 04:20:40,935][0m Trial 15 finished with value: 0.13022613270851216 and parameters: {'observation_period_num': 189, 'train_rates': 0.8986728065382327, 'learning_rate': 0.0003016072647387046, 'batch_size': 89, 'step_size': 8, 'gamma': 0.9424820999013328}. Best is trial 13 with value: 0.09984510391950607.[0m
[32m[I 2025-01-05 04:21:25,917][0m Trial 16 finished with value: 0.10017509758472443 and parameters: {'observation_period_num': 209, 'train_rates': 0.9874442139256501, 'learning_rate': 0.0003347379684478277, 'batch_size': 135, 'step_size': 6, 'gamma': 0.8397743209863621}. Best is trial 13 with value: 0.09984510391950607.[0m
[32m[I 2025-01-05 04:22:37,523][0m Trial 17 finished with value: 0.0840226124867901 and parameters: {'observation_period_num': 209, 'train_rates': 0.8961089002160181, 'learning_rate': 0.00029447151882803656, 'batch_size': 73, 'step_size': 6, 'gamma': 0.8307713216517093}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:27:13,044][0m Trial 18 finished with value: 0.10203148580931283 and parameters: {'observation_period_num': 152, 'train_rates': 0.8979665764315474, 'learning_rate': 0.00015910903772728917, 'batch_size': 19, 'step_size': 6, 'gamma': 0.8224339141362705}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:28:18,856][0m Trial 19 finished with value: 0.09557193362973426 and parameters: {'observation_period_num': 251, 'train_rates': 0.8648691013499431, 'learning_rate': 7.212545699645057e-05, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8883033252092465}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:29:32,720][0m Trial 20 finished with value: 0.11557397756674519 and parameters: {'observation_period_num': 252, 'train_rates': 0.8652296801083396, 'learning_rate': 6.809791470109946e-05, 'batch_size': 69, 'step_size': 4, 'gamma': 0.8870003406208509}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:30:36,404][0m Trial 21 finished with value: 0.11710752889434284 and parameters: {'observation_period_num': 207, 'train_rates': 0.9321500310474639, 'learning_rate': 0.0004360590872440736, 'batch_size': 87, 'step_size': 9, 'gamma': 0.8339468743064218}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:31:21,313][0m Trial 22 finished with value: 0.08461204016901026 and parameters: {'observation_period_num': 233, 'train_rates': 0.8546382905215693, 'learning_rate': 0.0002051867372696215, 'batch_size': 119, 'step_size': 7, 'gamma': 0.8036840532506534}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:32:38,278][0m Trial 23 finished with value: 0.0880163863183326 and parameters: {'observation_period_num': 230, 'train_rates': 0.8611030720281417, 'learning_rate': 0.0002024035823625439, 'batch_size': 66, 'step_size': 6, 'gamma': 0.8003742747403151}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:34:00,988][0m Trial 24 finished with value: 0.10590523236966275 and parameters: {'observation_period_num': 230, 'train_rates': 0.8155451781893147, 'learning_rate': 0.0001862530495964968, 'batch_size': 59, 'step_size': 6, 'gamma': 0.7975927648864424}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:37:13,126][0m Trial 25 finished with value: 0.09970483711610238 and parameters: {'observation_period_num': 234, 'train_rates': 0.8679620120373335, 'learning_rate': 0.0001969097127109301, 'batch_size': 26, 'step_size': 5, 'gamma': 0.7565443842561879}. Best is trial 17 with value: 0.0840226124867901.[0m
[32m[I 2025-01-05 04:37:58,527][0m Trial 26 finished with value: 0.07557200073547986 and parameters: {'observation_period_num': 195, 'train_rates': 0.9166894618259751, 'learning_rate': 0.0004739105224359897, 'batch_size': 125, 'step_size': 7, 'gamma': 0.7955594982104265}. Best is trial 26 with value: 0.07557200073547986.[0m
[32m[I 2025-01-05 04:38:44,509][0m Trial 27 finished with value: 0.06163559533786355 and parameters: {'observation_period_num': 122, 'train_rates': 0.9197419198548069, 'learning_rate': 0.0004939008782593094, 'batch_size': 130, 'step_size': 7, 'gamma': 0.7769293242074816}. Best is trial 27 with value: 0.06163559533786355.[0m
[32m[I 2025-01-05 04:39:21,924][0m Trial 28 finished with value: 0.06759259307006905 and parameters: {'observation_period_num': 116, 'train_rates': 0.9222521677620417, 'learning_rate': 0.0005402355167246081, 'batch_size': 154, 'step_size': 4, 'gamma': 0.7745388903494284}. Best is trial 27 with value: 0.06163559533786355.[0m
[32m[I 2025-01-05 04:39:57,387][0m Trial 29 finished with value: 0.06320687980134024 and parameters: {'observation_period_num': 120, 'train_rates': 0.9221611779836363, 'learning_rate': 0.0005475897835971221, 'batch_size': 168, 'step_size': 4, 'gamma': 0.7747672493285223}. Best is trial 27 with value: 0.06163559533786355.[0m
[32m[I 2025-01-05 04:40:29,577][0m Trial 30 finished with value: 0.2273813462622452 and parameters: {'observation_period_num': 112, 'train_rates': 0.7717129569991528, 'learning_rate': 0.0005636034556672096, 'batch_size': 163, 'step_size': 4, 'gamma': 0.773311839612841}. Best is trial 27 with value: 0.06163559533786355.[0m
[32m[I 2025-01-05 04:41:04,789][0m Trial 31 finished with value: 0.060183522084030494 and parameters: {'observation_period_num': 67, 'train_rates': 0.9269841411324082, 'learning_rate': 0.0004686564333078544, 'batch_size': 171, 'step_size': 4, 'gamma': 0.7520274509194586}. Best is trial 31 with value: 0.060183522084030494.[0m
[32m[I 2025-01-05 04:41:42,252][0m Trial 32 finished with value: 0.07803983241319656 and parameters: {'observation_period_num': 72, 'train_rates': 0.9509519890372612, 'learning_rate': 0.0005572172720258562, 'batch_size': 164, 'step_size': 3, 'gamma': 0.7503994704735938}. Best is trial 31 with value: 0.060183522084030494.[0m
[32m[I 2025-01-05 04:42:23,627][0m Trial 33 finished with value: 0.05800242961388855 and parameters: {'observation_period_num': 66, 'train_rates': 0.9286471383020892, 'learning_rate': 0.000491985299826929, 'batch_size': 145, 'step_size': 4, 'gamma': 0.778244805645019}. Best is trial 33 with value: 0.05800242961388855.[0m
Early stopping at epoch 52
[32m[I 2025-01-05 04:42:41,868][0m Trial 34 finished with value: 0.2391869371044047 and parameters: {'observation_period_num': 60, 'train_rates': 0.88820797403129, 'learning_rate': 9.934305537602426e-05, 'batch_size': 179, 'step_size': 1, 'gamma': 0.7822209007925083}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:43:25,273][0m Trial 35 finished with value: 0.12555736303329468 and parameters: {'observation_period_num': 73, 'train_rates': 0.9612302866685105, 'learning_rate': 0.00012041342198582261, 'batch_size': 143, 'step_size': 3, 'gamma': 0.7643416972194137}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:44:00,635][0m Trial 36 finished with value: 0.07322301189689075 and parameters: {'observation_period_num': 50, 'train_rates': 0.9356104405937484, 'learning_rate': 0.0002857335784331192, 'batch_size': 176, 'step_size': 2, 'gamma': 0.8494153818497719}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:44:27,390][0m Trial 37 finished with value: 0.9152194261550903 and parameters: {'observation_period_num': 6, 'train_rates': 0.9273543211636028, 'learning_rate': 3.428463975114795e-06, 'batch_size': 228, 'step_size': 5, 'gamma': 0.783220059821674}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:44:56,300][0m Trial 38 finished with value: 0.24704481177031995 and parameters: {'observation_period_num': 137, 'train_rates': 0.7303317631740468, 'learning_rate': 0.0006409187197404196, 'batch_size': 179, 'step_size': 3, 'gamma': 0.7613048654710342}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:45:37,896][0m Trial 39 finished with value: 0.08317907071539334 and parameters: {'observation_period_num': 85, 'train_rates': 0.8781400117974898, 'learning_rate': 3.590724965828915e-05, 'batch_size': 139, 'step_size': 10, 'gamma': 0.807374499518957}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:46:03,586][0m Trial 40 finished with value: 0.1330236981312434 and parameters: {'observation_period_num': 114, 'train_rates': 0.8365520891795352, 'learning_rate': 0.0004148745152752857, 'batch_size': 217, 'step_size': 2, 'gamma': 0.7894434641283269}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:46:41,894][0m Trial 41 finished with value: 0.062328903440465315 and parameters: {'observation_period_num': 114, 'train_rates': 0.9174013971462827, 'learning_rate': 0.0006584322333332457, 'batch_size': 160, 'step_size': 4, 'gamma': 0.7739440479455348}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:47:21,264][0m Trial 42 finished with value: 0.0694678544364077 and parameters: {'observation_period_num': 151, 'train_rates': 0.9162277466004917, 'learning_rate': 0.0006463865594328274, 'batch_size': 151, 'step_size': 4, 'gamma': 0.7718706863349233}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:47:58,159][0m Trial 43 finished with value: 0.0840132012963295 and parameters: {'observation_period_num': 124, 'train_rates': 0.9652269742842335, 'learning_rate': 0.0009937730186079317, 'batch_size': 169, 'step_size': 5, 'gamma': 0.8125505777313345}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:48:32,922][0m Trial 44 finished with value: 0.45816014862613524 and parameters: {'observation_period_num': 100, 'train_rates': 0.6075177147649777, 'learning_rate': 1.8229832387654637e-05, 'batch_size': 132, 'step_size': 3, 'gamma': 0.7507576860950542}. Best is trial 33 with value: 0.05800242961388855.[0m
Early stopping at epoch 60
[32m[I 2025-01-05 04:49:07,624][0m Trial 45 finished with value: 0.09008312666381912 and parameters: {'observation_period_num': 37, 'train_rates': 0.9404129247897552, 'learning_rate': 0.00038778220845712063, 'batch_size': 107, 'step_size': 1, 'gamma': 0.7773125792936091}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:49:41,467][0m Trial 46 finished with value: 0.0917842909693718 and parameters: {'observation_period_num': 137, 'train_rates': 0.962105318233733, 'learning_rate': 0.00023840423231590743, 'batch_size': 187, 'step_size': 7, 'gamma': 0.7669776779682413}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:50:22,199][0m Trial 47 finished with value: 0.06149565358292368 and parameters: {'observation_period_num': 80, 'train_rates': 0.8858477427560795, 'learning_rate': 0.0007420719828534067, 'batch_size': 146, 'step_size': 5, 'gamma': 0.785099772497757}. Best is trial 33 with value: 0.05800242961388855.[0m
[32m[I 2025-01-05 04:51:00,450][0m Trial 48 finished with value: 0.05177759399105396 and parameters: {'observation_period_num': 84, 'train_rates': 0.8830629970076967, 'learning_rate': 0.0007120377614832365, 'batch_size': 150, 'step_size': 5, 'gamma': 0.8498296119359727}. Best is trial 48 with value: 0.05177759399105396.[0m
[32m[I 2025-01-05 04:51:36,804][0m Trial 49 finished with value: 0.056221026152799634 and parameters: {'observation_period_num': 81, 'train_rates': 0.8057986532036928, 'learning_rate': 0.0007317778699531584, 'batch_size': 146, 'step_size': 5, 'gamma': 0.8208637681262689}. Best is trial 48 with value: 0.05177759399105396.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 04:51:36,814][0m A new study created in memory with name: no-name-9d0b086e-4529-4599-b611-cd439e621fa5[0m
[32m[I 2025-01-05 04:52:36,459][0m Trial 0 finished with value: 0.4763820469379425 and parameters: {'observation_period_num': 238, 'train_rates': 0.972334276153682, 'learning_rate': 1.6398630135683019e-06, 'batch_size': 95, 'step_size': 5, 'gamma': 0.9563056282103269}. Best is trial 0 with value: 0.4763820469379425.[0m
[32m[I 2025-01-05 04:53:20,594][0m Trial 1 finished with value: 0.4207932056453817 and parameters: {'observation_period_num': 116, 'train_rates': 0.7118309672926639, 'learning_rate': 6.992374926938758e-06, 'batch_size': 108, 'step_size': 15, 'gamma': 0.8534740569284179}. Best is trial 1 with value: 0.4207932056453817.[0m
[32m[I 2025-01-05 04:53:45,523][0m Trial 2 finished with value: 0.23540551208803806 and parameters: {'observation_period_num': 155, 'train_rates': 0.6376436565796605, 'learning_rate': 4.1295026828723423e-05, 'batch_size': 188, 'step_size': 13, 'gamma': 0.9266887923239963}. Best is trial 2 with value: 0.23540551208803806.[0m
[32m[I 2025-01-05 04:54:38,457][0m Trial 3 finished with value: 0.20054291443126956 and parameters: {'observation_period_num': 108, 'train_rates': 0.6399554329091091, 'learning_rate': 6.575931798087707e-05, 'batch_size': 83, 'step_size': 2, 'gamma': 0.9761029713718502}. Best is trial 3 with value: 0.20054291443126956.[0m
[32m[I 2025-01-05 04:56:33,506][0m Trial 4 finished with value: 0.12804508358240127 and parameters: {'observation_period_num': 154, 'train_rates': 0.9428631458533573, 'learning_rate': 0.00016454097609268893, 'batch_size': 48, 'step_size': 3, 'gamma': 0.9868951906563448}. Best is trial 4 with value: 0.12804508358240127.[0m
[32m[I 2025-01-05 04:57:01,302][0m Trial 5 finished with value: 0.7000026974025406 and parameters: {'observation_period_num': 172, 'train_rates': 0.7729870016120972, 'learning_rate': 2.074653303718267e-06, 'batch_size': 195, 'step_size': 14, 'gamma': 0.9548751459057943}. Best is trial 4 with value: 0.12804508358240127.[0m
[32m[I 2025-01-05 04:57:47,437][0m Trial 6 finished with value: 0.41442559188166367 and parameters: {'observation_period_num': 246, 'train_rates': 0.8847096501783069, 'learning_rate': 2.2070027242183045e-06, 'batch_size': 115, 'step_size': 8, 'gamma': 0.7842783681930686}. Best is trial 4 with value: 0.12804508358240127.[0m
[32m[I 2025-01-05 04:58:59,606][0m Trial 7 finished with value: 0.35364559309630744 and parameters: {'observation_period_num': 135, 'train_rates': 0.9374841259061509, 'learning_rate': 2.831794146737807e-06, 'batch_size': 79, 'step_size': 15, 'gamma': 0.7650868442228773}. Best is trial 4 with value: 0.12804508358240127.[0m
[32m[I 2025-01-05 05:02:08,748][0m Trial 8 finished with value: 0.1547885595863727 and parameters: {'observation_period_num': 15, 'train_rates': 0.7121250315813106, 'learning_rate': 0.00024063125255627862, 'batch_size': 25, 'step_size': 15, 'gamma': 0.8362320063004448}. Best is trial 4 with value: 0.12804508358240127.[0m
[32m[I 2025-01-05 05:02:57,271][0m Trial 9 finished with value: 0.5226820720608827 and parameters: {'observation_period_num': 95, 'train_rates': 0.7256265463218318, 'learning_rate': 3.097297641820607e-06, 'batch_size': 102, 'step_size': 13, 'gamma': 0.8254779176302145}. Best is trial 4 with value: 0.12804508358240127.[0m
[32m[I 2025-01-05 05:03:20,365][0m Trial 10 finished with value: 0.07689595098920803 and parameters: {'observation_period_num': 54, 'train_rates': 0.8629247134080709, 'learning_rate': 0.000518409702356553, 'batch_size': 253, 'step_size': 1, 'gamma': 0.9037642544862773}. Best is trial 10 with value: 0.07689595098920803.[0m
[32m[I 2025-01-05 05:03:45,220][0m Trial 11 finished with value: 0.05585311280381411 and parameters: {'observation_period_num': 38, 'train_rates': 0.86319742352468, 'learning_rate': 0.0009977178233264042, 'batch_size': 246, 'step_size': 1, 'gamma': 0.9015168919923351}. Best is trial 11 with value: 0.05585311280381411.[0m
[32m[I 2025-01-05 05:04:10,450][0m Trial 12 finished with value: 0.05553269105570576 and parameters: {'observation_period_num': 26, 'train_rates': 0.8561355189166595, 'learning_rate': 0.0009264409867203186, 'batch_size': 249, 'step_size': 1, 'gamma': 0.8997482825751428}. Best is trial 12 with value: 0.05553269105570576.[0m
[32m[I 2025-01-05 05:04:33,399][0m Trial 13 finished with value: 0.03470301377646467 and parameters: {'observation_period_num': 5, 'train_rates': 0.8409559296065656, 'learning_rate': 0.0009305457562729524, 'batch_size': 252, 'step_size': 5, 'gamma': 0.8916585425536706}. Best is trial 13 with value: 0.03470301377646467.[0m
[32m[I 2025-01-05 05:05:01,486][0m Trial 14 finished with value: 0.03173146870181854 and parameters: {'observation_period_num': 8, 'train_rates': 0.8182102179343367, 'learning_rate': 0.0009693574430504561, 'batch_size': 206, 'step_size': 5, 'gamma': 0.878927972248682}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:05:30,388][0m Trial 15 finished with value: 0.06922013247385621 and parameters: {'observation_period_num': 69, 'train_rates': 0.7921222408117717, 'learning_rate': 0.00021691680223552437, 'batch_size': 195, 'step_size': 6, 'gamma': 0.8722029540200334}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:06:06,910][0m Trial 16 finished with value: 0.15237142234638706 and parameters: {'observation_period_num': 74, 'train_rates': 0.819826867931854, 'learning_rate': 1.4706892480174995e-05, 'batch_size': 151, 'step_size': 9, 'gamma': 0.8083775449197421}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:06:30,761][0m Trial 17 finished with value: 0.19444460990563245 and parameters: {'observation_period_num': 8, 'train_rates': 0.7590986381607652, 'learning_rate': 8.377710271456368e-05, 'batch_size': 228, 'step_size': 5, 'gamma': 0.8720649480240952}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:07:04,789][0m Trial 18 finished with value: 0.11277215200990377 and parameters: {'observation_period_num': 203, 'train_rates': 0.821592203340405, 'learning_rate': 0.00038582450645516513, 'batch_size': 152, 'step_size': 9, 'gamma': 0.9350902861307113}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:07:32,625][0m Trial 19 finished with value: 0.14380633051497232 and parameters: {'observation_period_num': 49, 'train_rates': 0.903358577514889, 'learning_rate': 1.606210694662873e-05, 'batch_size': 220, 'step_size': 6, 'gamma': 0.875326030851006}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:08:04,840][0m Trial 20 finished with value: 0.05189380566756155 and parameters: {'observation_period_num': 76, 'train_rates': 0.8157251659007208, 'learning_rate': 0.0005078176961521728, 'batch_size': 176, 'step_size': 4, 'gamma': 0.9240360282725916}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:08:36,549][0m Trial 21 finished with value: 0.03484744454423586 and parameters: {'observation_period_num': 27, 'train_rates': 0.8229576000074198, 'learning_rate': 0.0004871054054080403, 'batch_size': 173, 'step_size': 4, 'gamma': 0.9222290248262542}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:09:02,980][0m Trial 22 finished with value: 0.15149005323096557 and parameters: {'observation_period_num': 5, 'train_rates': 0.7438744274982939, 'learning_rate': 0.0009907218129578802, 'batch_size': 218, 'step_size': 7, 'gamma': 0.8879275893304039}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:09:37,025][0m Trial 23 finished with value: 0.060906821519262445 and parameters: {'observation_period_num': 35, 'train_rates': 0.8381778387619163, 'learning_rate': 0.0001191144729243808, 'batch_size': 167, 'step_size': 3, 'gamma': 0.8444836855078849}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:10:07,288][0m Trial 24 finished with value: 0.03706603656922068 and parameters: {'observation_period_num': 27, 'train_rates': 0.8997848330276742, 'learning_rate': 0.00035356614614027407, 'batch_size': 210, 'step_size': 4, 'gamma': 0.9161599243693239}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:10:45,274][0m Trial 25 finished with value: 0.069951707102868 and parameters: {'observation_period_num': 52, 'train_rates': 0.7884254501411913, 'learning_rate': 0.0006564477630284617, 'batch_size': 146, 'step_size': 11, 'gamma': 0.9420481430957606}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:11:12,765][0m Trial 26 finished with value: 0.0372086837887764 and parameters: {'observation_period_num': 6, 'train_rates': 0.9291206998170742, 'learning_rate': 0.0002726846574189102, 'batch_size': 237, 'step_size': 6, 'gamma': 0.8880268200495076}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:11:37,812][0m Trial 27 finished with value: 0.17825379826222795 and parameters: {'observation_period_num': 94, 'train_rates': 0.6686845387953883, 'learning_rate': 0.000546940248108485, 'batch_size': 204, 'step_size': 4, 'gamma': 0.8543256104760878}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:12:20,248][0m Trial 28 finished with value: 0.04324384818708948 and parameters: {'observation_period_num': 24, 'train_rates': 0.8411116028392753, 'learning_rate': 0.00011414770364018745, 'batch_size': 130, 'step_size': 8, 'gamma': 0.804724917976921}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:12:50,956][0m Trial 29 finished with value: 0.1507016913758384 and parameters: {'observation_period_num': 214, 'train_rates': 0.8031862025653599, 'learning_rate': 4.2948735307065124e-05, 'batch_size': 176, 'step_size': 5, 'gamma': 0.945503985745981}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:13:13,883][0m Trial 30 finished with value: 0.16602182410432118 and parameters: {'observation_period_num': 62, 'train_rates': 0.6800474164410174, 'learning_rate': 0.000655824836292888, 'batch_size': 223, 'step_size': 3, 'gamma': 0.9177762389536657}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:13:45,641][0m Trial 31 finished with value: 0.045220158994197845 and parameters: {'observation_period_num': 31, 'train_rates': 0.9850294490402094, 'learning_rate': 0.0003482284661604413, 'batch_size': 209, 'step_size': 4, 'gamma': 0.9120786546132222}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:14:13,006][0m Trial 32 finished with value: 0.05001179836246939 and parameters: {'observation_period_num': 41, 'train_rates': 0.8848460602705467, 'learning_rate': 0.00035359186750457544, 'batch_size': 233, 'step_size': 5, 'gamma': 0.8891261890915313}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:14:46,527][0m Trial 33 finished with value: 0.04109074624506294 and parameters: {'observation_period_num': 21, 'train_rates': 0.9039594017399823, 'learning_rate': 0.00015158614244584372, 'batch_size': 185, 'step_size': 7, 'gamma': 0.9659719064120711}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:15:17,972][0m Trial 34 finished with value: 0.08640018850564957 and parameters: {'observation_period_num': 86, 'train_rates': 0.9650545340278913, 'learning_rate': 0.0007298565031781806, 'batch_size': 202, 'step_size': 2, 'gamma': 0.852987503311599}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:15:56,974][0m Trial 35 finished with value: 0.03200757059645146 and parameters: {'observation_period_num': 20, 'train_rates': 0.8879736443971878, 'learning_rate': 0.0003826879841785144, 'batch_size': 162, 'step_size': 4, 'gamma': 0.9258421767251033}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:16:39,778][0m Trial 36 finished with value: 0.06275542179303886 and parameters: {'observation_period_num': 115, 'train_rates': 0.8407987372960416, 'learning_rate': 0.0004212835725429465, 'batch_size': 132, 'step_size': 2, 'gamma': 0.9337528869849785}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:17:13,851][0m Trial 37 finished with value: 0.058467654034613306 and parameters: {'observation_period_num': 45, 'train_rates': 0.8699289490948503, 'learning_rate': 0.0002254951308645386, 'batch_size': 170, 'step_size': 3, 'gamma': 0.8612510808913942}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:17:48,942][0m Trial 38 finished with value: 0.19499269915493975 and parameters: {'observation_period_num': 5, 'train_rates': 0.7657549529247805, 'learning_rate': 5.3602184609943603e-05, 'batch_size': 161, 'step_size': 7, 'gamma': 0.9622090606535214}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:18:39,331][0m Trial 39 finished with value: 0.09430944846301782 and parameters: {'observation_period_num': 62, 'train_rates': 0.9157997840872191, 'learning_rate': 0.0007244674697097316, 'batch_size': 120, 'step_size': 5, 'gamma': 0.9813668937244631}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:19:38,666][0m Trial 40 finished with value: 0.12991851349444802 and parameters: {'observation_period_num': 144, 'train_rates': 0.7820340127832983, 'learning_rate': 2.4681477219315903e-05, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9524118115381857}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:20:07,816][0m Trial 41 finished with value: 1.8511826622965903 and parameters: {'observation_period_num': 20, 'train_rates': 0.8899895661602933, 'learning_rate': 1.0450276824758356e-06, 'batch_size': 213, 'step_size': 4, 'gamma': 0.9128380160187616}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:20:39,889][0m Trial 42 finished with value: 0.04209947217536229 and parameters: {'observation_period_num': 32, 'train_rates': 0.8410060168079498, 'learning_rate': 0.00027277639094725863, 'batch_size': 187, 'step_size': 3, 'gamma': 0.9287416568703603}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:21:05,133][0m Trial 43 finished with value: 0.053923058038027716 and parameters: {'observation_period_num': 19, 'train_rates': 0.8812292588195564, 'learning_rate': 0.00016921656898073614, 'batch_size': 237, 'step_size': 4, 'gamma': 0.8938627824888529}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:21:38,776][0m Trial 44 finished with value: 0.046150557696819305 and parameters: {'observation_period_num': 15, 'train_rates': 0.9604608361834093, 'learning_rate': 0.000460539989900851, 'batch_size': 191, 'step_size': 2, 'gamma': 0.9086931997877554}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:22:09,686][0m Trial 45 finished with value: 0.18858125236299303 and parameters: {'observation_period_num': 34, 'train_rates': 0.6147456100397464, 'learning_rate': 0.00030983329277888856, 'batch_size': 160, 'step_size': 5, 'gamma': 0.8808793776168636}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:22:31,846][0m Trial 46 finished with value: 0.09846385137030952 and parameters: {'observation_period_num': 178, 'train_rates': 0.8084903150168955, 'learning_rate': 0.0007448365339028974, 'batch_size': 256, 'step_size': 2, 'gamma': 0.9218470392382438}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:24:19,128][0m Trial 47 finished with value: 0.0986030352769664 and parameters: {'observation_period_num': 58, 'train_rates': 0.9238544221246742, 'learning_rate': 0.0009918458953087652, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9075358325542416}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:24:50,598][0m Trial 48 finished with value: 0.48042966614306815 and parameters: {'observation_period_num': 16, 'train_rates': 0.8550442147982482, 'learning_rate': 5.4280135519887976e-06, 'batch_size': 180, 'step_size': 4, 'gamma': 0.8654233314915615}. Best is trial 14 with value: 0.03173146870181854.[0m
[32m[I 2025-01-05 05:25:20,228][0m Trial 49 finished with value: 0.04189386454696407 and parameters: {'observation_period_num': 27, 'train_rates': 0.829656366888998, 'learning_rate': 0.0001825259622276391, 'batch_size': 199, 'step_size': 9, 'gamma': 0.8972734477490132}. Best is trial 14 with value: 0.03173146870181854.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 05:25:20,239][0m A new study created in memory with name: no-name-00e09e15-7734-43dc-99ea-5342827a8cba[0m
[32m[I 2025-01-05 05:25:42,398][0m Trial 0 finished with value: 0.20970904432028048 and parameters: {'observation_period_num': 73, 'train_rates': 0.7653574737694285, 'learning_rate': 0.00044580629428337033, 'batch_size': 247, 'step_size': 10, 'gamma': 0.8120656718679364}. Best is trial 0 with value: 0.20970904432028048.[0m
[32m[I 2025-01-05 05:26:33,242][0m Trial 1 finished with value: 0.4108004112559629 and parameters: {'observation_period_num': 192, 'train_rates': 0.69960010503784, 'learning_rate': 1.9833545151387013e-05, 'batch_size': 88, 'step_size': 5, 'gamma': 0.7908013112246998}. Best is trial 0 with value: 0.20970904432028048.[0m
[32m[I 2025-01-05 05:27:04,158][0m Trial 2 finished with value: 0.13135717809200287 and parameters: {'observation_period_num': 5, 'train_rates': 0.9470488685796964, 'learning_rate': 9.25730274538478e-06, 'batch_size': 205, 'step_size': 6, 'gamma': 0.8893477527391807}. Best is trial 2 with value: 0.13135717809200287.[0m
[32m[I 2025-01-05 05:28:16,204][0m Trial 3 finished with value: 0.3071448504924774 and parameters: {'observation_period_num': 192, 'train_rates': 0.9853245933614712, 'learning_rate': 5.5462473286686225e-06, 'batch_size': 81, 'step_size': 4, 'gamma': 0.8999376451435219}. Best is trial 2 with value: 0.13135717809200287.[0m
Early stopping at epoch 56
[32m[I 2025-01-05 05:28:29,595][0m Trial 4 finished with value: 0.4628419786549524 and parameters: {'observation_period_num': 219, 'train_rates': 0.7702459492682365, 'learning_rate': 0.0003892900060548923, 'batch_size': 236, 'step_size': 1, 'gamma': 0.7660062978783784}. Best is trial 2 with value: 0.13135717809200287.[0m
[32m[I 2025-01-05 05:29:03,499][0m Trial 5 finished with value: 0.5648040360054084 and parameters: {'observation_period_num': 227, 'train_rates': 0.8759351267904125, 'learning_rate': 2.233650031704756e-06, 'batch_size': 165, 'step_size': 13, 'gamma': 0.8085654086111036}. Best is trial 2 with value: 0.13135717809200287.[0m
[32m[I 2025-01-05 05:29:28,298][0m Trial 6 finished with value: 0.07676108181476593 and parameters: {'observation_period_num': 116, 'train_rates': 0.9204085771973569, 'learning_rate': 0.0006475694364701662, 'batch_size': 249, 'step_size': 9, 'gamma': 0.791020375435749}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:29:57,996][0m Trial 7 finished with value: 0.11087867801951379 and parameters: {'observation_period_num': 213, 'train_rates': 0.8586741370384208, 'learning_rate': 8.479253057643031e-05, 'batch_size': 184, 'step_size': 11, 'gamma': 0.902892126186163}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:30:21,135][0m Trial 8 finished with value: 0.25037258863449097 and parameters: {'observation_period_num': 234, 'train_rates': 0.9096723946339834, 'learning_rate': 0.00014590215845993034, 'batch_size': 251, 'step_size': 2, 'gamma': 0.8095894561655728}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:30:59,272][0m Trial 9 finished with value: 0.31083714362130177 and parameters: {'observation_period_num': 14, 'train_rates': 0.8414788037994911, 'learning_rate': 1.309070065946488e-06, 'batch_size': 148, 'step_size': 15, 'gamma': 0.8758068797838027}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:33:38,773][0m Trial 10 finished with value: 0.21517340726421352 and parameters: {'observation_period_num': 122, 'train_rates': 0.6212472281028739, 'learning_rate': 0.0007732718203630094, 'batch_size': 26, 'step_size': 9, 'gamma': 0.9678950745755457}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:34:08,418][0m Trial 11 finished with value: 0.08367219523280142 and parameters: {'observation_period_num': 146, 'train_rates': 0.8372585562172743, 'learning_rate': 9.075759102597452e-05, 'batch_size': 193, 'step_size': 11, 'gamma': 0.9508410033801244}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:34:33,975][0m Trial 12 finished with value: 0.09859569453225364 and parameters: {'observation_period_num': 138, 'train_rates': 0.8077920804987971, 'learning_rate': 8.405761454246803e-05, 'batch_size': 210, 'step_size': 7, 'gamma': 0.9818330779223746}. Best is trial 6 with value: 0.07676108181476593.[0m
[32m[I 2025-01-05 05:35:20,949][0m Trial 13 finished with value: 0.0748998187482357 and parameters: {'observation_period_num': 135, 'train_rates': 0.929144189140498, 'learning_rate': 0.00020155671082450476, 'batch_size': 120, 'step_size': 12, 'gamma': 0.9400743950020365}. Best is trial 13 with value: 0.0748998187482357.[0m
[32m[I 2025-01-05 05:36:12,342][0m Trial 14 finished with value: 0.07411917423786119 and parameters: {'observation_period_num': 89, 'train_rates': 0.9252614214148893, 'learning_rate': 0.00023983536958188946, 'batch_size': 112, 'step_size': 13, 'gamma': 0.933753005858344}. Best is trial 14 with value: 0.07411917423786119.[0m
[32m[I 2025-01-05 05:37:09,195][0m Trial 15 finished with value: 0.08789196610450745 and parameters: {'observation_period_num': 68, 'train_rates': 0.979876670557386, 'learning_rate': 0.00020784136275784167, 'batch_size': 106, 'step_size': 15, 'gamma': 0.9374240957034884}. Best is trial 14 with value: 0.07411917423786119.[0m
[32m[I 2025-01-05 05:37:57,285][0m Trial 16 finished with value: 0.0701014281773608 and parameters: {'observation_period_num': 87, 'train_rates': 0.8984266552858459, 'learning_rate': 3.1254653814649484e-05, 'batch_size': 119, 'step_size': 13, 'gamma': 0.9274460970490929}. Best is trial 16 with value: 0.0701014281773608.[0m
[32m[I 2025-01-05 05:39:57,048][0m Trial 17 finished with value: 0.05910529442088202 and parameters: {'observation_period_num': 76, 'train_rates': 0.8806264483184362, 'learning_rate': 3.0275530640815136e-05, 'batch_size': 45, 'step_size': 13, 'gamma': 0.8515324481241893}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:42:29,813][0m Trial 18 finished with value: 0.17874607432182807 and parameters: {'observation_period_num': 50, 'train_rates': 0.7298779703614187, 'learning_rate': 2.9731021738164725e-05, 'batch_size': 31, 'step_size': 14, 'gamma': 0.8544964923785692}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:44:13,892][0m Trial 19 finished with value: 0.06188682290105472 and parameters: {'observation_period_num': 30, 'train_rates': 0.8876980109774067, 'learning_rate': 1.2173597786025728e-05, 'batch_size': 54, 'step_size': 13, 'gamma': 0.8494372765771444}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:45:47,290][0m Trial 20 finished with value: 0.091275890606034 and parameters: {'observation_period_num': 35, 'train_rates': 0.808546394728352, 'learning_rate': 9.48317599035498e-06, 'batch_size': 55, 'step_size': 8, 'gamma': 0.8519887439113721}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:47:20,456][0m Trial 21 finished with value: 0.06584260875687879 and parameters: {'observation_period_num': 91, 'train_rates': 0.8873444967101287, 'learning_rate': 4.092567200481024e-05, 'batch_size': 59, 'step_size': 13, 'gamma': 0.8410881800611075}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:49:18,112][0m Trial 22 finished with value: 0.059255588789367014 and parameters: {'observation_period_num': 37, 'train_rates': 0.8769104916097239, 'learning_rate': 1.594681504314453e-05, 'batch_size': 47, 'step_size': 11, 'gamma': 0.8378943385098475}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:50:56,114][0m Trial 23 finished with value: 0.05977143659677657 and parameters: {'observation_period_num': 37, 'train_rates': 0.8598685040960075, 'learning_rate': 1.5444518973155364e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8355651514024866}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:56:00,608][0m Trial 24 finished with value: 0.08072069169742975 and parameters: {'observation_period_num': 53, 'train_rates': 0.838531556942447, 'learning_rate': 4.325380123746336e-06, 'batch_size': 17, 'step_size': 11, 'gamma': 0.8282389393279511}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:57:22,693][0m Trial 25 finished with value: 0.06367066225835255 and parameters: {'observation_period_num': 31, 'train_rates': 0.9521798453502668, 'learning_rate': 1.9282806910580555e-05, 'batch_size': 72, 'step_size': 10, 'gamma': 0.8724057770264539}. Best is trial 17 with value: 0.05910529442088202.[0m
[32m[I 2025-01-05 05:59:30,611][0m Trial 26 finished with value: 0.05622794219185152 and parameters: {'observation_period_num': 57, 'train_rates': 0.8595276064384558, 'learning_rate': 4.673791713049218e-05, 'batch_size': 42, 'step_size': 10, 'gamma': 0.8273689138510154}. Best is trial 26 with value: 0.05622794219185152.[0m
[32m[I 2025-01-05 06:01:40,947][0m Trial 27 finished with value: 0.07187558092910391 and parameters: {'observation_period_num': 109, 'train_rates': 0.7965449858468553, 'learning_rate': 5.199190302666068e-05, 'batch_size': 38, 'step_size': 8, 'gamma': 0.7508939154778522}. Best is trial 26 with value: 0.05622794219185152.[0m
[32m[I 2025-01-05 06:02:37,992][0m Trial 28 finished with value: 0.23928759533847094 and parameters: {'observation_period_num': 62, 'train_rates': 0.8214647620560995, 'learning_rate': 5.622074057449677e-06, 'batch_size': 93, 'step_size': 9, 'gamma': 0.7922058286001288}. Best is trial 26 with value: 0.05622794219185152.[0m
[32m[I 2025-01-05 06:04:26,025][0m Trial 29 finished with value: 0.2453633314472134 and parameters: {'observation_period_num': 157, 'train_rates': 0.7679733567527698, 'learning_rate': 6.287382143721707e-05, 'batch_size': 44, 'step_size': 10, 'gamma': 0.8283326817965828}. Best is trial 26 with value: 0.05622794219185152.[0m
[32m[I 2025-01-05 06:09:19,047][0m Trial 30 finished with value: 0.19795219581954332 and parameters: {'observation_period_num': 78, 'train_rates': 0.7446541391340015, 'learning_rate': 2.4798328782079817e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.8659235509258744}. Best is trial 26 with value: 0.05622794219185152.[0m
[32m[I 2025-01-05 06:10:39,190][0m Trial 31 finished with value: 0.07286857913214516 and parameters: {'observation_period_num': 45, 'train_rates': 0.8768503095337183, 'learning_rate': 1.489632613664542e-05, 'batch_size': 69, 'step_size': 11, 'gamma': 0.8270402404745505}. Best is trial 26 with value: 0.05622794219185152.[0m
[32m[I 2025-01-05 06:12:37,073][0m Trial 32 finished with value: 0.05236990715139637 and parameters: {'observation_period_num': 24, 'train_rates': 0.860964276317071, 'learning_rate': 1.8087076672057593e-05, 'batch_size': 46, 'step_size': 12, 'gamma': 0.8362371754529788}. Best is trial 32 with value: 0.05236990715139637.[0m
[32m[I 2025-01-05 06:13:38,052][0m Trial 33 finished with value: 0.05173176890745512 and parameters: {'observation_period_num': 22, 'train_rates': 0.8557576308745077, 'learning_rate': 4.040821584455498e-05, 'batch_size': 92, 'step_size': 12, 'gamma': 0.8065328857582889}. Best is trial 33 with value: 0.05173176890745512.[0m
[32m[I 2025-01-05 06:14:39,033][0m Trial 34 finished with value: 0.05088824441107057 and parameters: {'observation_period_num': 19, 'train_rates': 0.8581354954791283, 'learning_rate': 5.436050618885675e-05, 'batch_size': 92, 'step_size': 14, 'gamma': 0.7733490829949637}. Best is trial 34 with value: 0.05088824441107057.[0m
[32m[I 2025-01-05 06:15:26,962][0m Trial 35 finished with value: 0.1364726613347347 and parameters: {'observation_period_num': 13, 'train_rates': 0.659676452150568, 'learning_rate': 0.000125840916211754, 'batch_size': 98, 'step_size': 15, 'gamma': 0.7689639176522061}. Best is trial 34 with value: 0.05088824441107057.[0m
[32m[I 2025-01-05 06:16:32,478][0m Trial 36 finished with value: 0.18191383322715207 and parameters: {'observation_period_num': 15, 'train_rates': 0.7792220851463749, 'learning_rate': 5.960406830290181e-05, 'batch_size': 80, 'step_size': 14, 'gamma': 0.7773413805519126}. Best is trial 34 with value: 0.05088824441107057.[0m
[32m[I 2025-01-05 06:17:12,708][0m Trial 37 finished with value: 0.052433968679396016 and parameters: {'observation_period_num': 6, 'train_rates': 0.8540629889818709, 'learning_rate': 4.9612842858149396e-05, 'batch_size': 141, 'step_size': 12, 'gamma': 0.807826657876541}. Best is trial 34 with value: 0.05088824441107057.[0m
[32m[I 2025-01-05 06:17:58,034][0m Trial 38 finished with value: 0.11283130943775177 and parameters: {'observation_period_num': 7, 'train_rates': 0.9531556669216422, 'learning_rate': 6.513393794603345e-06, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8011717016471963}. Best is trial 34 with value: 0.05088824441107057.[0m
[32m[I 2025-01-05 06:18:32,036][0m Trial 39 finished with value: 0.32001007080078125 and parameters: {'observation_period_num': 252, 'train_rates': 0.8245484425449838, 'learning_rate': 2.2741217700212263e-05, 'batch_size': 158, 'step_size': 4, 'gamma': 0.7828374189831859}. Best is trial 34 with value: 0.05088824441107057.[0m
[32m[I 2025-01-05 06:19:18,317][0m Trial 40 finished with value: 0.033994626670785774 and parameters: {'observation_period_num': 22, 'train_rates': 0.8599989704105016, 'learning_rate': 0.0003896980509204037, 'batch_size': 125, 'step_size': 12, 'gamma': 0.8147961239219652}. Best is trial 40 with value: 0.033994626670785774.[0m
[32m[I 2025-01-05 06:20:04,408][0m Trial 41 finished with value: 0.0301905471130329 and parameters: {'observation_period_num': 20, 'train_rates': 0.8554529334025961, 'learning_rate': 0.00044151889912445415, 'batch_size': 123, 'step_size': 12, 'gamma': 0.8123566030209062}. Best is trial 41 with value: 0.0301905471130329.[0m
[32m[I 2025-01-05 06:20:52,678][0m Trial 42 finished with value: 0.03370681658387184 and parameters: {'observation_period_num': 23, 'train_rates': 0.9062951489104406, 'learning_rate': 0.0004899784487779101, 'batch_size': 122, 'step_size': 12, 'gamma': 0.8164778914869549}. Best is trial 41 with value: 0.0301905471130329.[0m
[32m[I 2025-01-05 06:21:38,941][0m Trial 43 finished with value: 0.0298407632682728 and parameters: {'observation_period_num': 22, 'train_rates': 0.9056317878985378, 'learning_rate': 0.0004881474046408, 'batch_size': 128, 'step_size': 14, 'gamma': 0.7552665136674585}. Best is trial 43 with value: 0.0298407632682728.[0m
[32m[I 2025-01-05 06:22:25,101][0m Trial 44 finished with value: 0.05005058820695239 and parameters: {'observation_period_num': 45, 'train_rates': 0.9076464595450453, 'learning_rate': 0.00047714962430722393, 'batch_size': 131, 'step_size': 14, 'gamma': 0.7500585157056763}. Best is trial 43 with value: 0.0298407632682728.[0m
[32m[I 2025-01-05 06:23:09,137][0m Trial 45 finished with value: 0.08154799567446822 and parameters: {'observation_period_num': 173, 'train_rates': 0.9094281354469821, 'learning_rate': 0.00046339375327880255, 'batch_size': 127, 'step_size': 15, 'gamma': 0.7644550261531637}. Best is trial 43 with value: 0.0298407632682728.[0m
[32m[I 2025-01-05 06:23:44,484][0m Trial 46 finished with value: 0.0421113814861282 and parameters: {'observation_period_num': 42, 'train_rates': 0.927021056991739, 'learning_rate': 0.0009813693398150075, 'batch_size': 169, 'step_size': 14, 'gamma': 0.7549124300068155}. Best is trial 43 with value: 0.0298407632682728.[0m
[32m[I 2025-01-05 06:24:23,525][0m Trial 47 finished with value: 0.030719374744473275 and parameters: {'observation_period_num': 5, 'train_rates': 0.9374596444534026, 'learning_rate': 0.0009287215444655258, 'batch_size': 162, 'step_size': 14, 'gamma': 0.7603441876718282}. Best is trial 43 with value: 0.0298407632682728.[0m
[32m[I 2025-01-05 06:25:04,780][0m Trial 48 finished with value: 0.03923308849334717 and parameters: {'observation_period_num': 24, 'train_rates': 0.9663624676115464, 'learning_rate': 0.00031556765234416485, 'batch_size': 155, 'step_size': 15, 'gamma': 0.7867298484001979}. Best is trial 43 with value: 0.0298407632682728.[0m
[32m[I 2025-01-05 06:25:39,860][0m Trial 49 finished with value: 0.03635647892951965 and parameters: {'observation_period_num': 6, 'train_rates': 0.9385462971331844, 'learning_rate': 0.0006264534032563242, 'batch_size': 184, 'step_size': 6, 'gamma': 0.8166356155851255}. Best is trial 43 with value: 0.0298407632682728.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 23, 'train_rates': 0.9150905790929971, 'learning_rate': 4.735616625640761e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8816352899735924}
Epoch 1/300, trend Loss: 0.2733 | 0.1345
Epoch 2/300, trend Loss: 0.1590 | 0.0932
Epoch 3/300, trend Loss: 0.1382 | 0.0775
Epoch 4/300, trend Loss: 0.1273 | 0.0690
Epoch 5/300, trend Loss: 0.1203 | 0.0707
Epoch 6/300, trend Loss: 0.1141 | 0.0679
Epoch 7/300, trend Loss: 0.1097 | 0.0652
Epoch 8/300, trend Loss: 0.1059 | 0.0602
Epoch 9/300, trend Loss: 0.1026 | 0.0588
Epoch 10/300, trend Loss: 0.0998 | 0.0573
Epoch 11/300, trend Loss: 0.0973 | 0.0559
Epoch 12/300, trend Loss: 0.0949 | 0.0538
Epoch 13/300, trend Loss: 0.0931 | 0.0531
Epoch 14/300, trend Loss: 0.0915 | 0.0523
Epoch 15/300, trend Loss: 0.0900 | 0.0514
Epoch 16/300, trend Loss: 0.0888 | 0.0509
Epoch 17/300, trend Loss: 0.0877 | 0.0503
Epoch 18/300, trend Loss: 0.0867 | 0.0498
Epoch 19/300, trend Loss: 0.0857 | 0.0490
Epoch 20/300, trend Loss: 0.0849 | 0.0486
Epoch 21/300, trend Loss: 0.0841 | 0.0482
Epoch 22/300, trend Loss: 0.0833 | 0.0475
Epoch 23/300, trend Loss: 0.0826 | 0.0472
Epoch 24/300, trend Loss: 0.0820 | 0.0469
Epoch 25/300, trend Loss: 0.0814 | 0.0466
Epoch 26/300, trend Loss: 0.0807 | 0.0462
Epoch 27/300, trend Loss: 0.0801 | 0.0459
Epoch 28/300, trend Loss: 0.0796 | 0.0457
Epoch 29/300, trend Loss: 0.0791 | 0.0455
Epoch 30/300, trend Loss: 0.0786 | 0.0452
Epoch 31/300, trend Loss: 0.0782 | 0.0450
Epoch 32/300, trend Loss: 0.0777 | 0.0449
Epoch 33/300, trend Loss: 0.0772 | 0.0448
Epoch 34/300, trend Loss: 0.0768 | 0.0446
Epoch 35/300, trend Loss: 0.0765 | 0.0445
Epoch 36/300, trend Loss: 0.0761 | 0.0445
Epoch 37/300, trend Loss: 0.0757 | 0.0444
Epoch 38/300, trend Loss: 0.0754 | 0.0442
Epoch 39/300, trend Loss: 0.0750 | 0.0441
Epoch 40/300, trend Loss: 0.0747 | 0.0442
Epoch 41/300, trend Loss: 0.0744 | 0.0440
Epoch 42/300, trend Loss: 0.0741 | 0.0438
Epoch 43/300, trend Loss: 0.0738 | 0.0440
Epoch 44/300, trend Loss: 0.0735 | 0.0438
Epoch 45/300, trend Loss: 0.0733 | 0.0437
Epoch 46/300, trend Loss: 0.0730 | 0.0435
Epoch 47/300, trend Loss: 0.0727 | 0.0437
Epoch 48/300, trend Loss: 0.0725 | 0.0435
Epoch 49/300, trend Loss: 0.0723 | 0.0434
Epoch 50/300, trend Loss: 0.0721 | 0.0437
Epoch 51/300, trend Loss: 0.0719 | 0.0435
Epoch 52/300, trend Loss: 0.0717 | 0.0434
Epoch 53/300, trend Loss: 0.0716 | 0.0432
Epoch 54/300, trend Loss: 0.0714 | 0.0436
Epoch 55/300, trend Loss: 0.0712 | 0.0434
Epoch 56/300, trend Loss: 0.0711 | 0.0433
Epoch 57/300, trend Loss: 0.0709 | 0.0434
Epoch 58/300, trend Loss: 0.0708 | 0.0433
Epoch 59/300, trend Loss: 0.0707 | 0.0432
Epoch 60/300, trend Loss: 0.0705 | 0.0430
Epoch 61/300, trend Loss: 0.0704 | 0.0429
Epoch 62/300, trend Loss: 0.0703 | 0.0427
Epoch 63/300, trend Loss: 0.0702 | 0.0426
Epoch 64/300, trend Loss: 0.0701 | 0.0421
Epoch 65/300, trend Loss: 0.0700 | 0.0420
Epoch 66/300, trend Loss: 0.0699 | 0.0419
Epoch 67/300, trend Loss: 0.0698 | 0.0417
Epoch 68/300, trend Loss: 0.0697 | 0.0413
Epoch 69/300, trend Loss: 0.0696 | 0.0412
Epoch 70/300, trend Loss: 0.0695 | 0.0411
Epoch 71/300, trend Loss: 0.0694 | 0.0408
Epoch 72/300, trend Loss: 0.0694 | 0.0408
Epoch 73/300, trend Loss: 0.0693 | 0.0407
Epoch 74/300, trend Loss: 0.0692 | 0.0406
Epoch 75/300, trend Loss: 0.0691 | 0.0405
Epoch 76/300, trend Loss: 0.0690 | 0.0405
Epoch 77/300, trend Loss: 0.0690 | 0.0404
Epoch 78/300, trend Loss: 0.0689 | 0.0404
Epoch 79/300, trend Loss: 0.0688 | 0.0403
Epoch 80/300, trend Loss: 0.0688 | 0.0403
Epoch 81/300, trend Loss: 0.0687 | 0.0402
Epoch 82/300, trend Loss: 0.0686 | 0.0403
Epoch 83/300, trend Loss: 0.0686 | 0.0402
Epoch 84/300, trend Loss: 0.0685 | 0.0401
Epoch 85/300, trend Loss: 0.0685 | 0.0403
Epoch 86/300, trend Loss: 0.0684 | 0.0402
Epoch 87/300, trend Loss: 0.0684 | 0.0402
Epoch 88/300, trend Loss: 0.0684 | 0.0401
Epoch 89/300, trend Loss: 0.0683 | 0.0404
Epoch 90/300, trend Loss: 0.0683 | 0.0403
Epoch 91/300, trend Loss: 0.0682 | 0.0403
Epoch 92/300, trend Loss: 0.0682 | 0.0406
Epoch 93/300, trend Loss: 0.0682 | 0.0405
Epoch 94/300, trend Loss: 0.0681 | 0.0405
Epoch 95/300, trend Loss: 0.0681 | 0.0405
Epoch 96/300, trend Loss: 0.0680 | 0.0408
Epoch 97/300, trend Loss: 0.0680 | 0.0408
Epoch 98/300, trend Loss: 0.0680 | 0.0407
Epoch 99/300, trend Loss: 0.0679 | 0.0410
Epoch 100/300, trend Loss: 0.0679 | 0.0410
Epoch 101/300, trend Loss: 0.0679 | 0.0410
Epoch 102/300, trend Loss: 0.0679 | 0.0409
Epoch 103/300, trend Loss: 0.0678 | 0.0411
Epoch 104/300, trend Loss: 0.0678 | 0.0411
Epoch 105/300, trend Loss: 0.0678 | 0.0411
Epoch 106/300, trend Loss: 0.0677 | 0.0412
Epoch 107/300, trend Loss: 0.0677 | 0.0411
Epoch 108/300, trend Loss: 0.0677 | 0.0411
Epoch 109/300, trend Loss: 0.0677 | 0.0411
Epoch 110/300, trend Loss: 0.0676 | 0.0411
Epoch 111/300, trend Loss: 0.0676 | 0.0411
Epoch 112/300, trend Loss: 0.0676 | 0.0410
Epoch 113/300, trend Loss: 0.0676 | 0.0410
Epoch 114/300, trend Loss: 0.0676 | 0.0410
Epoch 115/300, trend Loss: 0.0675 | 0.0410
Epoch 116/300, trend Loss: 0.0675 | 0.0410
Epoch 117/300, trend Loss: 0.0675 | 0.0409
Epoch 118/300, trend Loss: 0.0675 | 0.0409
Epoch 119/300, trend Loss: 0.0675 | 0.0409
Epoch 120/300, trend Loss: 0.0674 | 0.0408
Epoch 121/300, trend Loss: 0.0674 | 0.0408
Epoch 122/300, trend Loss: 0.0674 | 0.0408
Epoch 123/300, trend Loss: 0.0674 | 0.0408
Epoch 124/300, trend Loss: 0.0674 | 0.0407
Epoch 125/300, trend Loss: 0.0674 | 0.0407
Epoch 126/300, trend Loss: 0.0674 | 0.0407
Epoch 127/300, trend Loss: 0.0674 | 0.0406
Epoch 128/300, trend Loss: 0.0673 | 0.0406
Epoch 129/300, trend Loss: 0.0673 | 0.0406
Epoch 130/300, trend Loss: 0.0673 | 0.0406
Epoch 131/300, trend Loss: 0.0673 | 0.0406
Epoch 132/300, trend Loss: 0.0673 | 0.0406
Epoch 133/300, trend Loss: 0.0673 | 0.0406
Epoch 134/300, trend Loss: 0.0673 | 0.0405
Epoch 135/300, trend Loss: 0.0673 | 0.0405
Epoch 136/300, trend Loss: 0.0673 | 0.0405
Epoch 137/300, trend Loss: 0.0673 | 0.0405
Epoch 138/300, trend Loss: 0.0672 | 0.0404
Epoch 139/300, trend Loss: 0.0672 | 0.0404
Epoch 140/300, trend Loss: 0.0672 | 0.0404
Epoch 141/300, trend Loss: 0.0672 | 0.0404
Epoch 142/300, trend Loss: 0.0672 | 0.0404
Epoch 143/300, trend Loss: 0.0672 | 0.0404
Epoch 144/300, trend Loss: 0.0672 | 0.0404
Epoch 145/300, trend Loss: 0.0672 | 0.0404
Epoch 146/300, trend Loss: 0.0672 | 0.0404
Epoch 147/300, trend Loss: 0.0672 | 0.0404
Epoch 148/300, trend Loss: 0.0672 | 0.0403
Epoch 149/300, trend Loss: 0.0672 | 0.0403
Epoch 150/300, trend Loss: 0.0672 | 0.0403
Epoch 151/300, trend Loss: 0.0672 | 0.0403
Epoch 152/300, trend Loss: 0.0672 | 0.0403
Epoch 153/300, trend Loss: 0.0672 | 0.0403
Epoch 154/300, trend Loss: 0.0671 | 0.0403
Epoch 155/300, trend Loss: 0.0671 | 0.0403
Epoch 156/300, trend Loss: 0.0671 | 0.0403
Epoch 157/300, trend Loss: 0.0671 | 0.0403
Epoch 158/300, trend Loss: 0.0671 | 0.0403
Epoch 159/300, trend Loss: 0.0671 | 0.0402
Epoch 160/300, trend Loss: 0.0671 | 0.0402
Epoch 161/300, trend Loss: 0.0671 | 0.0402
Epoch 162/300, trend Loss: 0.0671 | 0.0402
Epoch 163/300, trend Loss: 0.0671 | 0.0402
Epoch 164/300, trend Loss: 0.0671 | 0.0402
Epoch 165/300, trend Loss: 0.0671 | 0.0402
Epoch 166/300, trend Loss: 0.0671 | 0.0402
Epoch 167/300, trend Loss: 0.0671 | 0.0402
Epoch 168/300, trend Loss: 0.0671 | 0.0402
Epoch 169/300, trend Loss: 0.0671 | 0.0402
Epoch 170/300, trend Loss: 0.0671 | 0.0402
Epoch 171/300, trend Loss: 0.0671 | 0.0402
Epoch 172/300, trend Loss: 0.0671 | 0.0402
Epoch 173/300, trend Loss: 0.0671 | 0.0402
Epoch 174/300, trend Loss: 0.0671 | 0.0402
Epoch 175/300, trend Loss: 0.0671 | 0.0402
Epoch 176/300, trend Loss: 0.0671 | 0.0402
Epoch 177/300, trend Loss: 0.0671 | 0.0402
Epoch 178/300, trend Loss: 0.0671 | 0.0402
Epoch 179/300, trend Loss: 0.0671 | 0.0402
Epoch 180/300, trend Loss: 0.0671 | 0.0402
Epoch 181/300, trend Loss: 0.0671 | 0.0401
Epoch 182/300, trend Loss: 0.0671 | 0.0401
Epoch 183/300, trend Loss: 0.0671 | 0.0401
Epoch 184/300, trend Loss: 0.0671 | 0.0401
Epoch 185/300, trend Loss: 0.0671 | 0.0401
Epoch 186/300, trend Loss: 0.0671 | 0.0401
Epoch 187/300, trend Loss: 0.0671 | 0.0401
Epoch 188/300, trend Loss: 0.0671 | 0.0401
Epoch 189/300, trend Loss: 0.0671 | 0.0401
Epoch 190/300, trend Loss: 0.0671 | 0.0401
Epoch 191/300, trend Loss: 0.0671 | 0.0401
Epoch 192/300, trend Loss: 0.0671 | 0.0401
Epoch 193/300, trend Loss: 0.0671 | 0.0401
Epoch 194/300, trend Loss: 0.0671 | 0.0401
Epoch 195/300, trend Loss: 0.0671 | 0.0401
Epoch 196/300, trend Loss: 0.0671 | 0.0401
Epoch 197/300, trend Loss: 0.0670 | 0.0401
Epoch 198/300, trend Loss: 0.0670 | 0.0401
Epoch 199/300, trend Loss: 0.0670 | 0.0401
Epoch 200/300, trend Loss: 0.0670 | 0.0401
Epoch 201/300, trend Loss: 0.0670 | 0.0401
Epoch 202/300, trend Loss: 0.0670 | 0.0401
Epoch 203/300, trend Loss: 0.0670 | 0.0401
Epoch 204/300, trend Loss: 0.0670 | 0.0401
Epoch 205/300, trend Loss: 0.0670 | 0.0401
Epoch 206/300, trend Loss: 0.0670 | 0.0401
Epoch 207/300, trend Loss: 0.0670 | 0.0401
Epoch 208/300, trend Loss: 0.0670 | 0.0401
Epoch 209/300, trend Loss: 0.0670 | 0.0401
Epoch 210/300, trend Loss: 0.0670 | 0.0401
Epoch 211/300, trend Loss: 0.0670 | 0.0401
Epoch 212/300, trend Loss: 0.0670 | 0.0401
Epoch 213/300, trend Loss: 0.0670 | 0.0401
Epoch 214/300, trend Loss: 0.0670 | 0.0401
Epoch 215/300, trend Loss: 0.0670 | 0.0401
Epoch 216/300, trend Loss: 0.0670 | 0.0401
Epoch 217/300, trend Loss: 0.0670 | 0.0401
Epoch 218/300, trend Loss: 0.0670 | 0.0401
Epoch 219/300, trend Loss: 0.0670 | 0.0401
Epoch 220/300, trend Loss: 0.0670 | 0.0401
Epoch 221/300, trend Loss: 0.0670 | 0.0401
Epoch 222/300, trend Loss: 0.0670 | 0.0401
Epoch 223/300, trend Loss: 0.0670 | 0.0401
Epoch 224/300, trend Loss: 0.0670 | 0.0401
Epoch 225/300, trend Loss: 0.0670 | 0.0401
Epoch 226/300, trend Loss: 0.0670 | 0.0401
Epoch 227/300, trend Loss: 0.0670 | 0.0401
Epoch 228/300, trend Loss: 0.0670 | 0.0401
Epoch 229/300, trend Loss: 0.0670 | 0.0401
Epoch 230/300, trend Loss: 0.0670 | 0.0401
Epoch 231/300, trend Loss: 0.0670 | 0.0401
Epoch 232/300, trend Loss: 0.0670 | 0.0401
Epoch 233/300, trend Loss: 0.0670 | 0.0401
Epoch 234/300, trend Loss: 0.0670 | 0.0401
Epoch 235/300, trend Loss: 0.0670 | 0.0401
Epoch 236/300, trend Loss: 0.0670 | 0.0401
Epoch 237/300, trend Loss: 0.0670 | 0.0401
Epoch 238/300, trend Loss: 0.0670 | 0.0401
Epoch 239/300, trend Loss: 0.0670 | 0.0401
Epoch 240/300, trend Loss: 0.0670 | 0.0401
Epoch 241/300, trend Loss: 0.0670 | 0.0401
Epoch 242/300, trend Loss: 0.0670 | 0.0401
Epoch 243/300, trend Loss: 0.0670 | 0.0401
Epoch 244/300, trend Loss: 0.0670 | 0.0401
Epoch 245/300, trend Loss: 0.0670 | 0.0401
Epoch 246/300, trend Loss: 0.0670 | 0.0401
Epoch 247/300, trend Loss: 0.0670 | 0.0401
Epoch 248/300, trend Loss: 0.0670 | 0.0401
Epoch 249/300, trend Loss: 0.0670 | 0.0401
Epoch 250/300, trend Loss: 0.0670 | 0.0401
Epoch 251/300, trend Loss: 0.0670 | 0.0401
Epoch 252/300, trend Loss: 0.0670 | 0.0401
Epoch 253/300, trend Loss: 0.0670 | 0.0401
Epoch 254/300, trend Loss: 0.0670 | 0.0401
Epoch 255/300, trend Loss: 0.0670 | 0.0401
Epoch 256/300, trend Loss: 0.0670 | 0.0401
Epoch 257/300, trend Loss: 0.0670 | 0.0401
Epoch 258/300, trend Loss: 0.0670 | 0.0401
Epoch 259/300, trend Loss: 0.0670 | 0.0401
Epoch 260/300, trend Loss: 0.0670 | 0.0401
Epoch 261/300, trend Loss: 0.0670 | 0.0401
Epoch 262/300, trend Loss: 0.0670 | 0.0401
Epoch 263/300, trend Loss: 0.0670 | 0.0401
Epoch 264/300, trend Loss: 0.0670 | 0.0401
Epoch 265/300, trend Loss: 0.0670 | 0.0401
Epoch 266/300, trend Loss: 0.0670 | 0.0401
Epoch 267/300, trend Loss: 0.0670 | 0.0401
Epoch 268/300, trend Loss: 0.0670 | 0.0401
Epoch 269/300, trend Loss: 0.0670 | 0.0401
Epoch 270/300, trend Loss: 0.0670 | 0.0401
Epoch 271/300, trend Loss: 0.0670 | 0.0401
Epoch 272/300, trend Loss: 0.0670 | 0.0401
Epoch 273/300, trend Loss: 0.0670 | 0.0401
Epoch 274/300, trend Loss: 0.0670 | 0.0401
Epoch 275/300, trend Loss: 0.0670 | 0.0401
Epoch 276/300, trend Loss: 0.0670 | 0.0401
Epoch 277/300, trend Loss: 0.0670 | 0.0401
Epoch 278/300, trend Loss: 0.0670 | 0.0401
Epoch 279/300, trend Loss: 0.0670 | 0.0401
Epoch 280/300, trend Loss: 0.0670 | 0.0401
Epoch 281/300, trend Loss: 0.0670 | 0.0401
Epoch 282/300, trend Loss: 0.0670 | 0.0401
Epoch 283/300, trend Loss: 0.0670 | 0.0401
Epoch 284/300, trend Loss: 0.0670 | 0.0401
Epoch 285/300, trend Loss: 0.0670 | 0.0401
Epoch 286/300, trend Loss: 0.0670 | 0.0401
Epoch 287/300, trend Loss: 0.0670 | 0.0401
Epoch 288/300, trend Loss: 0.0670 | 0.0401
Epoch 289/300, trend Loss: 0.0670 | 0.0401
Epoch 290/300, trend Loss: 0.0670 | 0.0401
Epoch 291/300, trend Loss: 0.0670 | 0.0401
Epoch 292/300, trend Loss: 0.0670 | 0.0401
Epoch 293/300, trend Loss: 0.0670 | 0.0401
Epoch 294/300, trend Loss: 0.0670 | 0.0401
Epoch 295/300, trend Loss: 0.0670 | 0.0401
Epoch 296/300, trend Loss: 0.0670 | 0.0401
Epoch 297/300, trend Loss: 0.0670 | 0.0401
Epoch 298/300, trend Loss: 0.0670 | 0.0401
Epoch 299/300, trend Loss: 0.0670 | 0.0401
Epoch 300/300, trend Loss: 0.0670 | 0.0401
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.8723360800596455, 'learning_rate': 0.00029467494425764156, 'batch_size': 75, 'step_size': 2, 'gamma': 0.9733126224241611}
Epoch 1/300, seasonal_0 Loss: 0.3787 | 0.1514
Epoch 2/300, seasonal_0 Loss: 0.1737 | 0.1111
Epoch 3/300, seasonal_0 Loss: 0.1562 | 0.1003
Epoch 4/300, seasonal_0 Loss: 0.1340 | 0.0911
Epoch 5/300, seasonal_0 Loss: 0.1225 | 0.0861
Epoch 6/300, seasonal_0 Loss: 0.1157 | 0.0838
Epoch 7/300, seasonal_0 Loss: 0.1126 | 0.0837
Epoch 8/300, seasonal_0 Loss: 0.1120 | 0.0865
Epoch 9/300, seasonal_0 Loss: 0.1138 | 0.1050
Epoch 10/300, seasonal_0 Loss: 0.1153 | 0.1738
Epoch 11/300, seasonal_0 Loss: 0.1139 | 0.1239
Epoch 12/300, seasonal_0 Loss: 0.1058 | 0.0899
Epoch 13/300, seasonal_0 Loss: 0.1013 | 0.0802
Epoch 14/300, seasonal_0 Loss: 0.1002 | 0.0665
Epoch 15/300, seasonal_0 Loss: 0.1012 | 0.0653
Epoch 16/300, seasonal_0 Loss: 0.1019 | 0.0601
Epoch 17/300, seasonal_0 Loss: 0.1049 | 0.0562
Epoch 18/300, seasonal_0 Loss: 0.1083 | 0.0613
Epoch 19/300, seasonal_0 Loss: 0.1125 | 0.0898
Epoch 20/300, seasonal_0 Loss: 0.1122 | 0.1301
Epoch 21/300, seasonal_0 Loss: 0.1096 | 0.1179
Epoch 22/300, seasonal_0 Loss: 0.1021 | 0.1039
Epoch 23/300, seasonal_0 Loss: 0.1036 | 0.1161
Epoch 24/300, seasonal_0 Loss: 0.0977 | 0.0552
Epoch 25/300, seasonal_0 Loss: 0.1161 | 0.0623
Epoch 26/300, seasonal_0 Loss: 0.0990 | 0.0568
Epoch 27/300, seasonal_0 Loss: 0.0923 | 0.0568
Epoch 28/300, seasonal_0 Loss: 0.0949 | 0.0510
Epoch 29/300, seasonal_0 Loss: 0.0962 | 0.0459
Epoch 30/300, seasonal_0 Loss: 0.0962 | 0.0475
Epoch 31/300, seasonal_0 Loss: 0.0935 | 0.0472
Epoch 32/300, seasonal_0 Loss: 0.0938 | 0.0439
Epoch 33/300, seasonal_0 Loss: 0.0948 | 0.0445
Epoch 34/300, seasonal_0 Loss: 0.0910 | 0.0442
Epoch 35/300, seasonal_0 Loss: 0.0880 | 0.0432
Epoch 36/300, seasonal_0 Loss: 0.0857 | 0.0429
Epoch 37/300, seasonal_0 Loss: 0.0841 | 0.0437
Epoch 38/300, seasonal_0 Loss: 0.0821 | 0.0446
Epoch 39/300, seasonal_0 Loss: 0.0801 | 0.0443
Epoch 40/300, seasonal_0 Loss: 0.0783 | 0.0426
Epoch 41/300, seasonal_0 Loss: 0.0771 | 0.0421
Epoch 42/300, seasonal_0 Loss: 0.0764 | 0.0421
Epoch 43/300, seasonal_0 Loss: 0.0758 | 0.0419
Epoch 44/300, seasonal_0 Loss: 0.0753 | 0.0416
Epoch 45/300, seasonal_0 Loss: 0.0749 | 0.0415
Epoch 46/300, seasonal_0 Loss: 0.0746 | 0.0413
Epoch 47/300, seasonal_0 Loss: 0.0741 | 0.0409
Epoch 48/300, seasonal_0 Loss: 0.0736 | 0.0406
Epoch 49/300, seasonal_0 Loss: 0.0732 | 0.0402
Epoch 50/300, seasonal_0 Loss: 0.0728 | 0.0398
Epoch 51/300, seasonal_0 Loss: 0.0724 | 0.0394
Epoch 52/300, seasonal_0 Loss: 0.0721 | 0.0391
Epoch 53/300, seasonal_0 Loss: 0.0719 | 0.0387
Epoch 54/300, seasonal_0 Loss: 0.0716 | 0.0384
Epoch 55/300, seasonal_0 Loss: 0.0714 | 0.0381
Epoch 56/300, seasonal_0 Loss: 0.0711 | 0.0379
Epoch 57/300, seasonal_0 Loss: 0.0708 | 0.0377
Epoch 58/300, seasonal_0 Loss: 0.0705 | 0.0375
Epoch 59/300, seasonal_0 Loss: 0.0702 | 0.0373
Epoch 60/300, seasonal_0 Loss: 0.0699 | 0.0372
Epoch 61/300, seasonal_0 Loss: 0.0696 | 0.0371
Epoch 62/300, seasonal_0 Loss: 0.0693 | 0.0370
Epoch 63/300, seasonal_0 Loss: 0.0690 | 0.0369
Epoch 64/300, seasonal_0 Loss: 0.0687 | 0.0367
Epoch 65/300, seasonal_0 Loss: 0.0685 | 0.0366
Epoch 66/300, seasonal_0 Loss: 0.0682 | 0.0364
Epoch 67/300, seasonal_0 Loss: 0.0680 | 0.0362
Epoch 68/300, seasonal_0 Loss: 0.0678 | 0.0360
Epoch 69/300, seasonal_0 Loss: 0.0676 | 0.0358
Epoch 70/300, seasonal_0 Loss: 0.0674 | 0.0356
Epoch 71/300, seasonal_0 Loss: 0.0672 | 0.0355
Epoch 72/300, seasonal_0 Loss: 0.0671 | 0.0353
Epoch 73/300, seasonal_0 Loss: 0.0669 | 0.0351
Epoch 74/300, seasonal_0 Loss: 0.0668 | 0.0349
Epoch 75/300, seasonal_0 Loss: 0.0668 | 0.0348
Epoch 76/300, seasonal_0 Loss: 0.0667 | 0.0347
Epoch 77/300, seasonal_0 Loss: 0.0667 | 0.0347
Epoch 78/300, seasonal_0 Loss: 0.0668 | 0.0348
Epoch 79/300, seasonal_0 Loss: 0.0669 | 0.0350
Epoch 80/300, seasonal_0 Loss: 0.0671 | 0.0352
Epoch 81/300, seasonal_0 Loss: 0.0672 | 0.0354
Epoch 82/300, seasonal_0 Loss: 0.0673 | 0.0355
Epoch 83/300, seasonal_0 Loss: 0.0671 | 0.0354
Epoch 84/300, seasonal_0 Loss: 0.0669 | 0.0353
Epoch 85/300, seasonal_0 Loss: 0.0665 | 0.0350
Epoch 86/300, seasonal_0 Loss: 0.0660 | 0.0348
Epoch 87/300, seasonal_0 Loss: 0.0655 | 0.0347
Epoch 88/300, seasonal_0 Loss: 0.0652 | 0.0345
Epoch 89/300, seasonal_0 Loss: 0.0648 | 0.0343
Epoch 90/300, seasonal_0 Loss: 0.0646 | 0.0342
Epoch 91/300, seasonal_0 Loss: 0.0645 | 0.0340
Epoch 92/300, seasonal_0 Loss: 0.0644 | 0.0339
Epoch 93/300, seasonal_0 Loss: 0.0643 | 0.0338
Epoch 94/300, seasonal_0 Loss: 0.0643 | 0.0337
Epoch 95/300, seasonal_0 Loss: 0.0642 | 0.0336
Epoch 96/300, seasonal_0 Loss: 0.0642 | 0.0336
Epoch 97/300, seasonal_0 Loss: 0.0641 | 0.0336
Epoch 98/300, seasonal_0 Loss: 0.0640 | 0.0336
Epoch 99/300, seasonal_0 Loss: 0.0639 | 0.0335
Epoch 100/300, seasonal_0 Loss: 0.0637 | 0.0335
Epoch 101/300, seasonal_0 Loss: 0.0636 | 0.0335
Epoch 102/300, seasonal_0 Loss: 0.0634 | 0.0334
Epoch 103/300, seasonal_0 Loss: 0.0632 | 0.0333
Epoch 104/300, seasonal_0 Loss: 0.0631 | 0.0332
Epoch 105/300, seasonal_0 Loss: 0.0630 | 0.0331
Epoch 106/300, seasonal_0 Loss: 0.0629 | 0.0329
Epoch 107/300, seasonal_0 Loss: 0.0628 | 0.0328
Epoch 108/300, seasonal_0 Loss: 0.0627 | 0.0326
Epoch 109/300, seasonal_0 Loss: 0.0627 | 0.0325
Epoch 110/300, seasonal_0 Loss: 0.0626 | 0.0323
Epoch 111/300, seasonal_0 Loss: 0.0626 | 0.0321
Epoch 112/300, seasonal_0 Loss: 0.0626 | 0.0320
Epoch 113/300, seasonal_0 Loss: 0.0626 | 0.0318
Epoch 114/300, seasonal_0 Loss: 0.0626 | 0.0317
Epoch 115/300, seasonal_0 Loss: 0.0626 | 0.0317
Epoch 116/300, seasonal_0 Loss: 0.0626 | 0.0317
Epoch 117/300, seasonal_0 Loss: 0.0625 | 0.0316
Epoch 118/300, seasonal_0 Loss: 0.0623 | 0.0316
Epoch 119/300, seasonal_0 Loss: 0.0621 | 0.0315
Epoch 120/300, seasonal_0 Loss: 0.0619 | 0.0315
Epoch 121/300, seasonal_0 Loss: 0.0617 | 0.0314
Epoch 122/300, seasonal_0 Loss: 0.0616 | 0.0314
Epoch 123/300, seasonal_0 Loss: 0.0615 | 0.0314
Epoch 124/300, seasonal_0 Loss: 0.0614 | 0.0314
Epoch 125/300, seasonal_0 Loss: 0.0614 | 0.0315
Epoch 126/300, seasonal_0 Loss: 0.0614 | 0.0315
Epoch 127/300, seasonal_0 Loss: 0.0614 | 0.0315
Epoch 128/300, seasonal_0 Loss: 0.0614 | 0.0316
Epoch 129/300, seasonal_0 Loss: 0.0613 | 0.0315
Epoch 130/300, seasonal_0 Loss: 0.0613 | 0.0315
Epoch 131/300, seasonal_0 Loss: 0.0612 | 0.0315
Epoch 132/300, seasonal_0 Loss: 0.0611 | 0.0314
Epoch 133/300, seasonal_0 Loss: 0.0610 | 0.0313
Epoch 134/300, seasonal_0 Loss: 0.0609 | 0.0313
Epoch 135/300, seasonal_0 Loss: 0.0608 | 0.0312
Epoch 136/300, seasonal_0 Loss: 0.0608 | 0.0312
Epoch 137/300, seasonal_0 Loss: 0.0607 | 0.0312
Epoch 138/300, seasonal_0 Loss: 0.0607 | 0.0311
Epoch 139/300, seasonal_0 Loss: 0.0607 | 0.0311
Epoch 140/300, seasonal_0 Loss: 0.0606 | 0.0311
Epoch 141/300, seasonal_0 Loss: 0.0606 | 0.0311
Epoch 142/300, seasonal_0 Loss: 0.0606 | 0.0311
Epoch 143/300, seasonal_0 Loss: 0.0606 | 0.0311
Epoch 144/300, seasonal_0 Loss: 0.0605 | 0.0311
Epoch 145/300, seasonal_0 Loss: 0.0605 | 0.0310
Epoch 146/300, seasonal_0 Loss: 0.0605 | 0.0310
Epoch 147/300, seasonal_0 Loss: 0.0604 | 0.0310
Epoch 148/300, seasonal_0 Loss: 0.0604 | 0.0310
Epoch 149/300, seasonal_0 Loss: 0.0603 | 0.0310
Epoch 150/300, seasonal_0 Loss: 0.0603 | 0.0310
Epoch 151/300, seasonal_0 Loss: 0.0602 | 0.0310
Epoch 152/300, seasonal_0 Loss: 0.0602 | 0.0310
Epoch 153/300, seasonal_0 Loss: 0.0602 | 0.0309
Epoch 154/300, seasonal_0 Loss: 0.0601 | 0.0309
Epoch 155/300, seasonal_0 Loss: 0.0601 | 0.0309
Epoch 156/300, seasonal_0 Loss: 0.0601 | 0.0309
Epoch 157/300, seasonal_0 Loss: 0.0601 | 0.0309
Epoch 158/300, seasonal_0 Loss: 0.0600 | 0.0309
Epoch 159/300, seasonal_0 Loss: 0.0600 | 0.0309
Epoch 160/300, seasonal_0 Loss: 0.0600 | 0.0309
Epoch 161/300, seasonal_0 Loss: 0.0600 | 0.0309
Epoch 162/300, seasonal_0 Loss: 0.0599 | 0.0308
Epoch 163/300, seasonal_0 Loss: 0.0599 | 0.0308
Epoch 164/300, seasonal_0 Loss: 0.0599 | 0.0308
Epoch 165/300, seasonal_0 Loss: 0.0599 | 0.0308
Epoch 166/300, seasonal_0 Loss: 0.0599 | 0.0308
Epoch 167/300, seasonal_0 Loss: 0.0598 | 0.0308
Epoch 168/300, seasonal_0 Loss: 0.0598 | 0.0308
Epoch 169/300, seasonal_0 Loss: 0.0598 | 0.0308
Epoch 170/300, seasonal_0 Loss: 0.0598 | 0.0308
Epoch 171/300, seasonal_0 Loss: 0.0598 | 0.0308
Epoch 172/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 173/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 174/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 175/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 176/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 177/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 178/300, seasonal_0 Loss: 0.0597 | 0.0307
Epoch 179/300, seasonal_0 Loss: 0.0596 | 0.0307
Epoch 180/300, seasonal_0 Loss: 0.0596 | 0.0307
Epoch 181/300, seasonal_0 Loss: 0.0596 | 0.0307
Epoch 182/300, seasonal_0 Loss: 0.0596 | 0.0307
Epoch 183/300, seasonal_0 Loss: 0.0596 | 0.0307
Epoch 184/300, seasonal_0 Loss: 0.0596 | 0.0307
Epoch 185/300, seasonal_0 Loss: 0.0596 | 0.0306
Epoch 186/300, seasonal_0 Loss: 0.0596 | 0.0306
Epoch 187/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 188/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 189/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 190/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 191/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 192/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 193/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 194/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 195/300, seasonal_0 Loss: 0.0595 | 0.0306
Epoch 196/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 197/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 198/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 199/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 200/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 201/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 202/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 203/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 204/300, seasonal_0 Loss: 0.0594 | 0.0306
Epoch 205/300, seasonal_0 Loss: 0.0594 | 0.0305
Epoch 206/300, seasonal_0 Loss: 0.0594 | 0.0305
Epoch 207/300, seasonal_0 Loss: 0.0594 | 0.0305
Epoch 208/300, seasonal_0 Loss: 0.0594 | 0.0305
Epoch 209/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 210/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 211/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 212/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 213/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 214/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 215/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 216/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 217/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 218/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 219/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 220/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 221/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 222/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 223/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 224/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 225/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 226/300, seasonal_0 Loss: 0.0593 | 0.0305
Epoch 227/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 228/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 229/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 230/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 231/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 232/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 233/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 234/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 235/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 236/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 237/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 238/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 239/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 240/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 241/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 242/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 243/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 244/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 245/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 246/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 247/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 248/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 249/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 250/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 251/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 252/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 253/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 254/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 255/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 256/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 257/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 258/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 259/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 260/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 261/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 262/300, seasonal_0 Loss: 0.0592 | 0.0305
Epoch 263/300, seasonal_0 Loss: 0.0592 | 0.0304
Epoch 264/300, seasonal_0 Loss: 0.0592 | 0.0304
Epoch 265/300, seasonal_0 Loss: 0.0592 | 0.0304
Epoch 266/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 267/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 268/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 269/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 270/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 271/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 272/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 273/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 274/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 275/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 276/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 277/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 278/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 279/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 280/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 281/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 282/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 283/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 284/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 285/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 286/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 287/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 288/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 289/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 290/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 291/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 292/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 293/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 294/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 295/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 296/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 297/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 298/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 299/300, seasonal_0 Loss: 0.0591 | 0.0304
Epoch 300/300, seasonal_0 Loss: 0.0591 | 0.0304
Training seasonal_1 component with params: {'observation_period_num': 6, 'train_rates': 0.8957944714350006, 'learning_rate': 0.0009972504212748658, 'batch_size': 160, 'step_size': 4, 'gamma': 0.9179374712356478}
Epoch 1/300, seasonal_1 Loss: 0.6302 | 0.3072
Epoch 2/300, seasonal_1 Loss: 0.1745 | 0.1624
Epoch 3/300, seasonal_1 Loss: 0.1821 | 0.0891
Epoch 4/300, seasonal_1 Loss: 0.1548 | 0.0940
Epoch 5/300, seasonal_1 Loss: 0.1365 | 0.0746
Epoch 6/300, seasonal_1 Loss: 0.1216 | 0.0694
Epoch 7/300, seasonal_1 Loss: 0.1188 | 0.0664
Epoch 8/300, seasonal_1 Loss: 0.1211 | 0.0677
Epoch 9/300, seasonal_1 Loss: 0.1211 | 0.0803
Epoch 10/300, seasonal_1 Loss: 0.1145 | 0.0666
Epoch 11/300, seasonal_1 Loss: 0.1077 | 0.0643
Epoch 12/300, seasonal_1 Loss: 0.1055 | 0.0610
Epoch 13/300, seasonal_1 Loss: 0.1162 | 0.0856
Epoch 14/300, seasonal_1 Loss: 0.1283 | 0.1112
Epoch 15/300, seasonal_1 Loss: 0.1242 | 0.0893
Epoch 16/300, seasonal_1 Loss: 0.1072 | 0.0644
Epoch 17/300, seasonal_1 Loss: 0.0991 | 0.0584
Epoch 18/300, seasonal_1 Loss: 0.0964 | 0.0594
Epoch 19/300, seasonal_1 Loss: 0.0908 | 0.0491
Epoch 20/300, seasonal_1 Loss: 0.0889 | 0.0474
Epoch 21/300, seasonal_1 Loss: 0.0875 | 0.0480
Epoch 22/300, seasonal_1 Loss: 0.0872 | 0.0486
Epoch 23/300, seasonal_1 Loss: 0.0865 | 0.0460
Epoch 24/300, seasonal_1 Loss: 0.0861 | 0.0448
Epoch 25/300, seasonal_1 Loss: 0.0860 | 0.0458
Epoch 26/300, seasonal_1 Loss: 0.0866 | 0.0457
Epoch 27/300, seasonal_1 Loss: 0.0872 | 0.0463
Epoch 28/300, seasonal_1 Loss: 0.0886 | 0.0474
Epoch 29/300, seasonal_1 Loss: 0.0897 | 0.0503
Epoch 30/300, seasonal_1 Loss: 0.0889 | 0.0449
Epoch 31/300, seasonal_1 Loss: 0.0875 | 0.0475
Epoch 32/300, seasonal_1 Loss: 0.0862 | 0.0418
Epoch 33/300, seasonal_1 Loss: 0.0896 | 0.0450
Epoch 34/300, seasonal_1 Loss: 0.0933 | 0.0458
Epoch 35/300, seasonal_1 Loss: 0.0979 | 0.0469
Epoch 36/300, seasonal_1 Loss: 0.0893 | 0.0410
Epoch 37/300, seasonal_1 Loss: 0.0830 | 0.0420
Epoch 38/300, seasonal_1 Loss: 0.0820 | 0.0437
Epoch 39/300, seasonal_1 Loss: 0.0815 | 0.0403
Epoch 40/300, seasonal_1 Loss: 0.0802 | 0.0405
Epoch 41/300, seasonal_1 Loss: 0.0791 | 0.0395
Epoch 42/300, seasonal_1 Loss: 0.0789 | 0.0392
Epoch 43/300, seasonal_1 Loss: 0.0785 | 0.0398
Epoch 44/300, seasonal_1 Loss: 0.0782 | 0.0387
Epoch 45/300, seasonal_1 Loss: 0.0780 | 0.0386
Epoch 46/300, seasonal_1 Loss: 0.0779 | 0.0388
Epoch 47/300, seasonal_1 Loss: 0.0776 | 0.0379
Epoch 48/300, seasonal_1 Loss: 0.0777 | 0.0381
Epoch 49/300, seasonal_1 Loss: 0.0777 | 0.0385
Epoch 50/300, seasonal_1 Loss: 0.0779 | 0.0381
Epoch 51/300, seasonal_1 Loss: 0.0782 | 0.0389
Epoch 52/300, seasonal_1 Loss: 0.0785 | 0.0382
Epoch 53/300, seasonal_1 Loss: 0.0779 | 0.0374
Epoch 54/300, seasonal_1 Loss: 0.0772 | 0.0380
Epoch 55/300, seasonal_1 Loss: 0.0765 | 0.0369
Epoch 56/300, seasonal_1 Loss: 0.0761 | 0.0371
Epoch 57/300, seasonal_1 Loss: 0.0759 | 0.0371
Epoch 58/300, seasonal_1 Loss: 0.0758 | 0.0364
Epoch 59/300, seasonal_1 Loss: 0.0756 | 0.0368
Epoch 60/300, seasonal_1 Loss: 0.0754 | 0.0363
Epoch 61/300, seasonal_1 Loss: 0.0752 | 0.0362
Epoch 62/300, seasonal_1 Loss: 0.0751 | 0.0361
Epoch 63/300, seasonal_1 Loss: 0.0749 | 0.0358
Epoch 64/300, seasonal_1 Loss: 0.0748 | 0.0359
Epoch 65/300, seasonal_1 Loss: 0.0747 | 0.0356
Epoch 66/300, seasonal_1 Loss: 0.0745 | 0.0356
Epoch 67/300, seasonal_1 Loss: 0.0744 | 0.0354
Epoch 68/300, seasonal_1 Loss: 0.0743 | 0.0353
Epoch 69/300, seasonal_1 Loss: 0.0741 | 0.0352
Epoch 70/300, seasonal_1 Loss: 0.0740 | 0.0351
Epoch 71/300, seasonal_1 Loss: 0.0739 | 0.0350
Epoch 72/300, seasonal_1 Loss: 0.0738 | 0.0349
Epoch 73/300, seasonal_1 Loss: 0.0737 | 0.0348
Epoch 74/300, seasonal_1 Loss: 0.0736 | 0.0347
Epoch 75/300, seasonal_1 Loss: 0.0735 | 0.0346
Epoch 76/300, seasonal_1 Loss: 0.0734 | 0.0345
Epoch 77/300, seasonal_1 Loss: 0.0733 | 0.0345
Epoch 78/300, seasonal_1 Loss: 0.0732 | 0.0344
Epoch 79/300, seasonal_1 Loss: 0.0731 | 0.0343
Epoch 80/300, seasonal_1 Loss: 0.0730 | 0.0343
Epoch 81/300, seasonal_1 Loss: 0.0729 | 0.0342
Epoch 82/300, seasonal_1 Loss: 0.0728 | 0.0341
Epoch 83/300, seasonal_1 Loss: 0.0727 | 0.0341
Epoch 84/300, seasonal_1 Loss: 0.0727 | 0.0340
Epoch 85/300, seasonal_1 Loss: 0.0726 | 0.0340
Epoch 86/300, seasonal_1 Loss: 0.0725 | 0.0339
Epoch 87/300, seasonal_1 Loss: 0.0724 | 0.0339
Epoch 88/300, seasonal_1 Loss: 0.0724 | 0.0338
Epoch 89/300, seasonal_1 Loss: 0.0723 | 0.0338
Epoch 90/300, seasonal_1 Loss: 0.0722 | 0.0337
Epoch 91/300, seasonal_1 Loss: 0.0722 | 0.0337
Epoch 92/300, seasonal_1 Loss: 0.0721 | 0.0336
Epoch 93/300, seasonal_1 Loss: 0.0721 | 0.0336
Epoch 94/300, seasonal_1 Loss: 0.0720 | 0.0335
Epoch 95/300, seasonal_1 Loss: 0.0719 | 0.0335
Epoch 96/300, seasonal_1 Loss: 0.0719 | 0.0334
Epoch 97/300, seasonal_1 Loss: 0.0718 | 0.0334
Epoch 98/300, seasonal_1 Loss: 0.0718 | 0.0333
Epoch 99/300, seasonal_1 Loss: 0.0717 | 0.0332
Epoch 100/300, seasonal_1 Loss: 0.0717 | 0.0332
Epoch 101/300, seasonal_1 Loss: 0.0716 | 0.0331
Epoch 102/300, seasonal_1 Loss: 0.0716 | 0.0331
Epoch 103/300, seasonal_1 Loss: 0.0715 | 0.0330
Epoch 104/300, seasonal_1 Loss: 0.0715 | 0.0330
Epoch 105/300, seasonal_1 Loss: 0.0714 | 0.0329
Epoch 106/300, seasonal_1 Loss: 0.0714 | 0.0329
Epoch 107/300, seasonal_1 Loss: 0.0713 | 0.0328
Epoch 108/300, seasonal_1 Loss: 0.0713 | 0.0328
Epoch 109/300, seasonal_1 Loss: 0.0713 | 0.0327
Epoch 110/300, seasonal_1 Loss: 0.0712 | 0.0327
Epoch 111/300, seasonal_1 Loss: 0.0712 | 0.0327
Epoch 112/300, seasonal_1 Loss: 0.0712 | 0.0326
Epoch 113/300, seasonal_1 Loss: 0.0712 | 0.0326
Epoch 114/300, seasonal_1 Loss: 0.0711 | 0.0326
Epoch 115/300, seasonal_1 Loss: 0.0711 | 0.0326
Epoch 116/300, seasonal_1 Loss: 0.0711 | 0.0325
Epoch 117/300, seasonal_1 Loss: 0.0711 | 0.0325
Epoch 118/300, seasonal_1 Loss: 0.0710 | 0.0325
Epoch 119/300, seasonal_1 Loss: 0.0710 | 0.0325
Epoch 120/300, seasonal_1 Loss: 0.0710 | 0.0324
Epoch 121/300, seasonal_1 Loss: 0.0710 | 0.0324
Epoch 122/300, seasonal_1 Loss: 0.0710 | 0.0324
Epoch 123/300, seasonal_1 Loss: 0.0709 | 0.0324
Epoch 124/300, seasonal_1 Loss: 0.0709 | 0.0324
Epoch 125/300, seasonal_1 Loss: 0.0709 | 0.0323
Epoch 126/300, seasonal_1 Loss: 0.0709 | 0.0323
Epoch 127/300, seasonal_1 Loss: 0.0709 | 0.0323
Epoch 128/300, seasonal_1 Loss: 0.0709 | 0.0323
Epoch 129/300, seasonal_1 Loss: 0.0708 | 0.0323
Epoch 130/300, seasonal_1 Loss: 0.0708 | 0.0323
Epoch 131/300, seasonal_1 Loss: 0.0708 | 0.0323
Epoch 132/300, seasonal_1 Loss: 0.0708 | 0.0322
Epoch 133/300, seasonal_1 Loss: 0.0708 | 0.0322
Epoch 134/300, seasonal_1 Loss: 0.0708 | 0.0322
Epoch 135/300, seasonal_1 Loss: 0.0708 | 0.0322
Epoch 136/300, seasonal_1 Loss: 0.0708 | 0.0322
Epoch 137/300, seasonal_1 Loss: 0.0708 | 0.0322
Epoch 138/300, seasonal_1 Loss: 0.0707 | 0.0322
Epoch 139/300, seasonal_1 Loss: 0.0707 | 0.0322
Epoch 140/300, seasonal_1 Loss: 0.0707 | 0.0322
Epoch 141/300, seasonal_1 Loss: 0.0707 | 0.0322
Epoch 142/300, seasonal_1 Loss: 0.0707 | 0.0322
Epoch 143/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 144/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 145/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 146/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 147/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 148/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 149/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 150/300, seasonal_1 Loss: 0.0707 | 0.0321
Epoch 151/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 152/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 153/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 154/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 155/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 156/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 157/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 158/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 159/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 160/300, seasonal_1 Loss: 0.0706 | 0.0321
Epoch 161/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 162/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 163/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 164/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 165/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 166/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 167/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 168/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 169/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 170/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 171/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 172/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 173/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 174/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 175/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 176/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 177/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 178/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 179/300, seasonal_1 Loss: 0.0706 | 0.0320
Epoch 180/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 181/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 182/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 183/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 184/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 185/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 186/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 187/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 188/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 189/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 190/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 191/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 192/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 193/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 194/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 195/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 196/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 197/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 198/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 199/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 200/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 201/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 202/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 203/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 204/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 205/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 206/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 207/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 208/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 209/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 210/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 211/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 212/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 213/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 214/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 215/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 216/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 217/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 218/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 219/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 220/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 221/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 222/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 223/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 224/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 225/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 226/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 227/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 228/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 229/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 230/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 231/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 232/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 233/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 234/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 235/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 236/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 237/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 238/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 239/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 240/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 241/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 242/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 243/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 244/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 245/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 246/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 247/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 248/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 249/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 250/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 251/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 252/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 253/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 254/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 255/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 256/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 257/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 258/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 259/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 260/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 261/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 262/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 263/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 264/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 265/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 266/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 267/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 268/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 269/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 270/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 271/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 272/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 273/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 274/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 275/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 276/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 277/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 278/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 279/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 280/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 281/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 282/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 283/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 284/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 285/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 286/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 287/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 288/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 289/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 290/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 291/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 292/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 293/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 294/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 295/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 296/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 297/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 298/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 299/300, seasonal_1 Loss: 0.0705 | 0.0320
Epoch 300/300, seasonal_1 Loss: 0.0705 | 0.0320
Training seasonal_2 component with params: {'observation_period_num': 84, 'train_rates': 0.8830629970076967, 'learning_rate': 0.0007120377614832365, 'batch_size': 150, 'step_size': 5, 'gamma': 0.8498296119359727}
Epoch 1/300, seasonal_2 Loss: 0.8509 | 0.3152
Epoch 2/300, seasonal_2 Loss: 0.3213 | 0.2242
Epoch 3/300, seasonal_2 Loss: 0.2941 | 0.2390
Epoch 4/300, seasonal_2 Loss: 0.4005 | 0.6731
Epoch 5/300, seasonal_2 Loss: 0.2748 | 0.1681
Epoch 6/300, seasonal_2 Loss: 0.2707 | 0.1716
Epoch 7/300, seasonal_2 Loss: 0.1823 | 0.1897
Epoch 8/300, seasonal_2 Loss: 0.1631 | 0.2294
Epoch 9/300, seasonal_2 Loss: 0.1467 | 0.1315
Epoch 10/300, seasonal_2 Loss: 0.1302 | 0.1098
Epoch 11/300, seasonal_2 Loss: 0.1219 | 0.0990
Epoch 12/300, seasonal_2 Loss: 0.1157 | 0.0945
Epoch 13/300, seasonal_2 Loss: 0.1104 | 0.0945
Epoch 14/300, seasonal_2 Loss: 0.1081 | 0.0987
Epoch 15/300, seasonal_2 Loss: 0.1077 | 0.0946
Epoch 16/300, seasonal_2 Loss: 0.1063 | 0.1306
Epoch 17/300, seasonal_2 Loss: 0.1069 | 0.1825
Epoch 18/300, seasonal_2 Loss: 0.1064 | 0.0907
Epoch 19/300, seasonal_2 Loss: 0.1046 | 0.0818
Epoch 20/300, seasonal_2 Loss: 0.1061 | 0.0787
Epoch 21/300, seasonal_2 Loss: 0.0978 | 0.0864
Epoch 22/300, seasonal_2 Loss: 0.0909 | 0.0818
Epoch 23/300, seasonal_2 Loss: 0.0902 | 0.0697
Epoch 24/300, seasonal_2 Loss: 0.0866 | 0.0653
Epoch 25/300, seasonal_2 Loss: 0.0855 | 0.0645
Epoch 26/300, seasonal_2 Loss: 0.0837 | 0.0659
Epoch 27/300, seasonal_2 Loss: 0.0830 | 0.0641
Epoch 28/300, seasonal_2 Loss: 0.0822 | 0.0614
Epoch 29/300, seasonal_2 Loss: 0.0814 | 0.0608
Epoch 30/300, seasonal_2 Loss: 0.0807 | 0.0606
Epoch 31/300, seasonal_2 Loss: 0.0799 | 0.0616
Epoch 32/300, seasonal_2 Loss: 0.0797 | 0.0642
Epoch 33/300, seasonal_2 Loss: 0.0815 | 0.0759
Epoch 34/300, seasonal_2 Loss: 0.0818 | 0.0936
Epoch 35/300, seasonal_2 Loss: 0.0789 | 0.0664
Epoch 36/300, seasonal_2 Loss: 0.0776 | 0.0598
Epoch 37/300, seasonal_2 Loss: 0.0769 | 0.0622
Epoch 38/300, seasonal_2 Loss: 0.0765 | 0.0623
Epoch 39/300, seasonal_2 Loss: 0.0766 | 0.0664
Epoch 40/300, seasonal_2 Loss: 0.0758 | 0.0609
Epoch 41/300, seasonal_2 Loss: 0.0754 | 0.0592
Epoch 42/300, seasonal_2 Loss: 0.0752 | 0.0610
Epoch 43/300, seasonal_2 Loss: 0.0750 | 0.0611
Epoch 44/300, seasonal_2 Loss: 0.0747 | 0.0601
Epoch 45/300, seasonal_2 Loss: 0.0744 | 0.0588
Epoch 46/300, seasonal_2 Loss: 0.0742 | 0.0598
Epoch 47/300, seasonal_2 Loss: 0.0740 | 0.0592
Epoch 48/300, seasonal_2 Loss: 0.0738 | 0.0589
Epoch 49/300, seasonal_2 Loss: 0.0736 | 0.0587
Epoch 50/300, seasonal_2 Loss: 0.0735 | 0.0587
Epoch 51/300, seasonal_2 Loss: 0.0733 | 0.0584
Epoch 52/300, seasonal_2 Loss: 0.0732 | 0.0584
Epoch 53/300, seasonal_2 Loss: 0.0731 | 0.0581
Epoch 54/300, seasonal_2 Loss: 0.0729 | 0.0581
Epoch 55/300, seasonal_2 Loss: 0.0728 | 0.0579
Epoch 56/300, seasonal_2 Loss: 0.0727 | 0.0579
Epoch 57/300, seasonal_2 Loss: 0.0726 | 0.0577
Epoch 58/300, seasonal_2 Loss: 0.0725 | 0.0577
Epoch 59/300, seasonal_2 Loss: 0.0724 | 0.0576
Epoch 60/300, seasonal_2 Loss: 0.0724 | 0.0575
Epoch 61/300, seasonal_2 Loss: 0.0723 | 0.0574
Epoch 62/300, seasonal_2 Loss: 0.0722 | 0.0574
Epoch 63/300, seasonal_2 Loss: 0.0721 | 0.0572
Epoch 64/300, seasonal_2 Loss: 0.0721 | 0.0573
Epoch 65/300, seasonal_2 Loss: 0.0720 | 0.0571
Epoch 66/300, seasonal_2 Loss: 0.0719 | 0.0572
Epoch 67/300, seasonal_2 Loss: 0.0719 | 0.0570
Epoch 68/300, seasonal_2 Loss: 0.0718 | 0.0571
Epoch 69/300, seasonal_2 Loss: 0.0718 | 0.0569
Epoch 70/300, seasonal_2 Loss: 0.0717 | 0.0570
Epoch 71/300, seasonal_2 Loss: 0.0717 | 0.0568
Epoch 72/300, seasonal_2 Loss: 0.0717 | 0.0569
Epoch 73/300, seasonal_2 Loss: 0.0716 | 0.0568
Epoch 74/300, seasonal_2 Loss: 0.0716 | 0.0568
Epoch 75/300, seasonal_2 Loss: 0.0716 | 0.0568
Epoch 76/300, seasonal_2 Loss: 0.0715 | 0.0567
Epoch 77/300, seasonal_2 Loss: 0.0715 | 0.0567
Epoch 78/300, seasonal_2 Loss: 0.0715 | 0.0567
Epoch 79/300, seasonal_2 Loss: 0.0714 | 0.0567
Epoch 80/300, seasonal_2 Loss: 0.0714 | 0.0566
Epoch 81/300, seasonal_2 Loss: 0.0714 | 0.0566
Epoch 82/300, seasonal_2 Loss: 0.0714 | 0.0566
Epoch 83/300, seasonal_2 Loss: 0.0713 | 0.0566
Epoch 84/300, seasonal_2 Loss: 0.0713 | 0.0566
Epoch 85/300, seasonal_2 Loss: 0.0713 | 0.0565
Epoch 86/300, seasonal_2 Loss: 0.0713 | 0.0565
Epoch 87/300, seasonal_2 Loss: 0.0713 | 0.0565
Epoch 88/300, seasonal_2 Loss: 0.0713 | 0.0565
Epoch 89/300, seasonal_2 Loss: 0.0712 | 0.0565
Epoch 90/300, seasonal_2 Loss: 0.0712 | 0.0565
Epoch 91/300, seasonal_2 Loss: 0.0712 | 0.0565
Epoch 92/300, seasonal_2 Loss: 0.0712 | 0.0564
Epoch 93/300, seasonal_2 Loss: 0.0712 | 0.0564
Epoch 94/300, seasonal_2 Loss: 0.0712 | 0.0564
Epoch 95/300, seasonal_2 Loss: 0.0712 | 0.0564
Epoch 96/300, seasonal_2 Loss: 0.0712 | 0.0564
Epoch 97/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 98/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 99/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 100/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 101/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 102/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 103/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 104/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 105/300, seasonal_2 Loss: 0.0711 | 0.0564
Epoch 106/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 107/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 108/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 109/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 110/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 111/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 112/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 113/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 114/300, seasonal_2 Loss: 0.0711 | 0.0563
Epoch 115/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 116/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 117/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 118/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 119/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 120/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 121/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 122/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 123/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 124/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 125/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 126/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 127/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 128/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 129/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 130/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 131/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 132/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 133/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 134/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 135/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 136/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 137/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 138/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 139/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 140/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 141/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 142/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 143/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 144/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 145/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 146/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 147/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 148/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 149/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 150/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 151/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 152/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 153/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 154/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 155/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 156/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 157/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 158/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 159/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 160/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 161/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 162/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 163/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 164/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 165/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 166/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 167/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 168/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 169/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 170/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 171/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 172/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 173/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 174/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 175/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 176/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 177/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 178/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 179/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 180/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 181/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 182/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 183/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 184/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 185/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 186/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 187/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 188/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 189/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 190/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 191/300, seasonal_2 Loss: 0.0710 | 0.0563
Epoch 192/300, seasonal_2 Loss: 0.0710 | 0.0563
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 8, 'train_rates': 0.8182102179343367, 'learning_rate': 0.0009693574430504561, 'batch_size': 206, 'step_size': 5, 'gamma': 0.878927972248682}
Epoch 1/300, seasonal_3 Loss: 0.7282 | 0.2068
Epoch 2/300, seasonal_3 Loss: 0.2219 | 0.1363
Epoch 3/300, seasonal_3 Loss: 0.1752 | 0.1062
Epoch 4/300, seasonal_3 Loss: 0.1588 | 0.1038
Epoch 5/300, seasonal_3 Loss: 0.1518 | 0.1006
Epoch 6/300, seasonal_3 Loss: 0.1357 | 0.0855
Epoch 7/300, seasonal_3 Loss: 0.1517 | 0.1443
Epoch 8/300, seasonal_3 Loss: 0.1579 | 0.0925
Epoch 9/300, seasonal_3 Loss: 0.1249 | 0.0642
Epoch 10/300, seasonal_3 Loss: 0.1230 | 0.1045
Epoch 11/300, seasonal_3 Loss: 0.1259 | 0.0713
Epoch 12/300, seasonal_3 Loss: 0.1178 | 0.0667
Epoch 13/300, seasonal_3 Loss: 0.1108 | 0.0788
Epoch 14/300, seasonal_3 Loss: 0.1219 | 0.0684
Epoch 15/300, seasonal_3 Loss: 0.1136 | 0.0589
Epoch 16/300, seasonal_3 Loss: 0.1087 | 0.0623
Epoch 17/300, seasonal_3 Loss: 0.1124 | 0.0654
Epoch 18/300, seasonal_3 Loss: 0.1058 | 0.0564
Epoch 19/300, seasonal_3 Loss: 0.1050 | 0.0660
Epoch 20/300, seasonal_3 Loss: 0.1043 | 0.0589
Epoch 21/300, seasonal_3 Loss: 0.1008 | 0.0544
Epoch 22/300, seasonal_3 Loss: 0.0977 | 0.0576
Epoch 23/300, seasonal_3 Loss: 0.0977 | 0.0504
Epoch 24/300, seasonal_3 Loss: 0.0947 | 0.0499
Epoch 25/300, seasonal_3 Loss: 0.0941 | 0.0492
Epoch 26/300, seasonal_3 Loss: 0.0930 | 0.0490
Epoch 27/300, seasonal_3 Loss: 0.0923 | 0.0482
Epoch 28/300, seasonal_3 Loss: 0.0916 | 0.0478
Epoch 29/300, seasonal_3 Loss: 0.0908 | 0.0466
Epoch 30/300, seasonal_3 Loss: 0.0903 | 0.0466
Epoch 31/300, seasonal_3 Loss: 0.0896 | 0.0456
Epoch 32/300, seasonal_3 Loss: 0.0890 | 0.0448
Epoch 33/300, seasonal_3 Loss: 0.0884 | 0.0444
Epoch 34/300, seasonal_3 Loss: 0.0878 | 0.0439
Epoch 35/300, seasonal_3 Loss: 0.0874 | 0.0432
Epoch 36/300, seasonal_3 Loss: 0.0870 | 0.0432
Epoch 37/300, seasonal_3 Loss: 0.0867 | 0.0428
Epoch 38/300, seasonal_3 Loss: 0.0865 | 0.0428
Epoch 39/300, seasonal_3 Loss: 0.0865 | 0.0431
Epoch 40/300, seasonal_3 Loss: 0.0869 | 0.0428
Epoch 41/300, seasonal_3 Loss: 0.0869 | 0.0423
Epoch 42/300, seasonal_3 Loss: 0.0860 | 0.0424
Epoch 43/300, seasonal_3 Loss: 0.0853 | 0.0419
Epoch 44/300, seasonal_3 Loss: 0.0850 | 0.0414
Epoch 45/300, seasonal_3 Loss: 0.0845 | 0.0410
Epoch 46/300, seasonal_3 Loss: 0.0842 | 0.0410
Epoch 47/300, seasonal_3 Loss: 0.0841 | 0.0408
Epoch 48/300, seasonal_3 Loss: 0.0839 | 0.0408
Epoch 49/300, seasonal_3 Loss: 0.0837 | 0.0404
Epoch 50/300, seasonal_3 Loss: 0.0835 | 0.0405
Epoch 51/300, seasonal_3 Loss: 0.0833 | 0.0401
Epoch 52/300, seasonal_3 Loss: 0.0832 | 0.0402
Epoch 53/300, seasonal_3 Loss: 0.0830 | 0.0398
Epoch 54/300, seasonal_3 Loss: 0.0829 | 0.0399
Epoch 55/300, seasonal_3 Loss: 0.0828 | 0.0397
Epoch 56/300, seasonal_3 Loss: 0.0827 | 0.0396
Epoch 57/300, seasonal_3 Loss: 0.0827 | 0.0397
Epoch 58/300, seasonal_3 Loss: 0.0827 | 0.0394
Epoch 59/300, seasonal_3 Loss: 0.0827 | 0.0399
Epoch 60/300, seasonal_3 Loss: 0.0828 | 0.0389
Epoch 61/300, seasonal_3 Loss: 0.0827 | 0.0405
Epoch 62/300, seasonal_3 Loss: 0.0826 | 0.0387
Epoch 63/300, seasonal_3 Loss: 0.0824 | 0.0405
Epoch 64/300, seasonal_3 Loss: 0.0822 | 0.0385
Epoch 65/300, seasonal_3 Loss: 0.0819 | 0.0401
Epoch 66/300, seasonal_3 Loss: 0.0817 | 0.0385
Epoch 67/300, seasonal_3 Loss: 0.0815 | 0.0394
Epoch 68/300, seasonal_3 Loss: 0.0814 | 0.0386
Epoch 69/300, seasonal_3 Loss: 0.0814 | 0.0389
Epoch 70/300, seasonal_3 Loss: 0.0813 | 0.0387
Epoch 71/300, seasonal_3 Loss: 0.0812 | 0.0387
Epoch 72/300, seasonal_3 Loss: 0.0812 | 0.0387
Epoch 73/300, seasonal_3 Loss: 0.0811 | 0.0386
Epoch 74/300, seasonal_3 Loss: 0.0811 | 0.0386
Epoch 75/300, seasonal_3 Loss: 0.0810 | 0.0386
Epoch 76/300, seasonal_3 Loss: 0.0810 | 0.0385
Epoch 77/300, seasonal_3 Loss: 0.0809 | 0.0385
Epoch 78/300, seasonal_3 Loss: 0.0809 | 0.0385
Epoch 79/300, seasonal_3 Loss: 0.0809 | 0.0384
Epoch 80/300, seasonal_3 Loss: 0.0808 | 0.0384
Epoch 81/300, seasonal_3 Loss: 0.0808 | 0.0384
Epoch 82/300, seasonal_3 Loss: 0.0807 | 0.0384
Epoch 83/300, seasonal_3 Loss: 0.0807 | 0.0383
Epoch 84/300, seasonal_3 Loss: 0.0807 | 0.0383
Epoch 85/300, seasonal_3 Loss: 0.0807 | 0.0383
Epoch 86/300, seasonal_3 Loss: 0.0806 | 0.0383
Epoch 87/300, seasonal_3 Loss: 0.0806 | 0.0383
Epoch 88/300, seasonal_3 Loss: 0.0806 | 0.0383
Epoch 89/300, seasonal_3 Loss: 0.0805 | 0.0382
Epoch 90/300, seasonal_3 Loss: 0.0805 | 0.0382
Epoch 91/300, seasonal_3 Loss: 0.0805 | 0.0382
Epoch 92/300, seasonal_3 Loss: 0.0805 | 0.0382
Epoch 93/300, seasonal_3 Loss: 0.0805 | 0.0382
Epoch 94/300, seasonal_3 Loss: 0.0804 | 0.0382
Epoch 95/300, seasonal_3 Loss: 0.0804 | 0.0382
Epoch 96/300, seasonal_3 Loss: 0.0804 | 0.0382
Epoch 97/300, seasonal_3 Loss: 0.0804 | 0.0381
Epoch 98/300, seasonal_3 Loss: 0.0804 | 0.0381
Epoch 99/300, seasonal_3 Loss: 0.0804 | 0.0381
Epoch 100/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 101/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 102/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 103/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 104/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 105/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 106/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 107/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 108/300, seasonal_3 Loss: 0.0803 | 0.0381
Epoch 109/300, seasonal_3 Loss: 0.0802 | 0.0381
Epoch 110/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 111/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 112/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 113/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 114/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 115/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 116/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 117/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 118/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 119/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 120/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 121/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 122/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 123/300, seasonal_3 Loss: 0.0802 | 0.0380
Epoch 124/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 125/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 126/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 127/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 128/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 129/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 130/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 131/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 132/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 133/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 134/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 135/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 136/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 137/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 138/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 139/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 140/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 141/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 142/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 143/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 144/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 145/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 146/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 147/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 148/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 149/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 150/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 151/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 152/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 153/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 154/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 155/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 156/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 157/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 158/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 159/300, seasonal_3 Loss: 0.0801 | 0.0380
Epoch 160/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 161/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 162/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 163/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 164/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 165/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 166/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 167/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 168/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 169/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 170/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 171/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 172/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 173/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 174/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 175/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 176/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 177/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 178/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 179/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 180/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 181/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 182/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 183/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 184/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 185/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 186/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 187/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 188/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 189/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 190/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 191/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 192/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 193/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 194/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 195/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 196/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 197/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 198/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 199/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 200/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 201/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 202/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 203/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 204/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 205/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 206/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 207/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 208/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 209/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 210/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 211/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 212/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 213/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 214/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 215/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 216/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 217/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 218/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 219/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 220/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 221/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 222/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 223/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 224/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 225/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 226/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 227/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 228/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 229/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 230/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 231/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 232/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 233/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 234/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 235/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 236/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 237/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 238/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 239/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 240/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 241/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 242/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 243/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 244/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 245/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 246/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 247/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 248/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 249/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 250/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 251/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 252/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 253/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 254/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 255/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 256/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 257/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 258/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 259/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 260/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 261/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 262/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 263/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 264/300, seasonal_3 Loss: 0.0801 | 0.0379
Epoch 265/300, seasonal_3 Loss: 0.0801 | 0.0379
Early stopping for seasonal_3
Training resid component with params: {'observation_period_num': 22, 'train_rates': 0.9056317878985378, 'learning_rate': 0.0004881474046408, 'batch_size': 128, 'step_size': 14, 'gamma': 0.7552665136674585}
Epoch 1/300, resid Loss: 0.3652 | 0.2038
Epoch 2/300, resid Loss: 0.1941 | 0.1703
Epoch 3/300, resid Loss: 0.1655 | 0.1232
Epoch 4/300, resid Loss: 0.1553 | 0.1487
Epoch 5/300, resid Loss: 0.1590 | 0.1074
Epoch 6/300, resid Loss: 0.1963 | 0.2434
Epoch 7/300, resid Loss: 0.1792 | 0.2400
Epoch 8/300, resid Loss: 0.1642 | 0.1202
Epoch 9/300, resid Loss: 0.1596 | 0.0956
Epoch 10/300, resid Loss: 0.1340 | 0.0966
Epoch 11/300, resid Loss: 0.1224 | 0.0831
Epoch 12/300, resid Loss: 0.1199 | 0.0719
Epoch 13/300, resid Loss: 0.1298 | 0.0696
Epoch 14/300, resid Loss: 0.1379 | 0.0668
Epoch 15/300, resid Loss: 0.1380 | 0.0657
Epoch 16/300, resid Loss: 0.1165 | 0.0698
Epoch 17/300, resid Loss: 0.1082 | 0.0590
Epoch 18/300, resid Loss: 0.1082 | 0.0617
Epoch 19/300, resid Loss: 0.1429 | 0.0866
Epoch 20/300, resid Loss: 0.1302 | 0.0657
Epoch 21/300, resid Loss: 0.1049 | 0.0640
Epoch 22/300, resid Loss: 0.1040 | 0.1256
Epoch 23/300, resid Loss: 0.1008 | 0.0601
Epoch 24/300, resid Loss: 0.0948 | 0.0511
Epoch 25/300, resid Loss: 0.0984 | 0.0527
Epoch 26/300, resid Loss: 0.0991 | 0.0509
Epoch 27/300, resid Loss: 0.0944 | 0.0493
Epoch 28/300, resid Loss: 0.0887 | 0.0558
Epoch 29/300, resid Loss: 0.0941 | 0.0958
Epoch 30/300, resid Loss: 0.0961 | 0.0695
Epoch 31/300, resid Loss: 0.0869 | 0.0496
Epoch 32/300, resid Loss: 0.0840 | 0.0457
Epoch 33/300, resid Loss: 0.0842 | 0.0453
Epoch 34/300, resid Loss: 0.0815 | 0.0459
Epoch 35/300, resid Loss: 0.0812 | 0.0544
Epoch 36/300, resid Loss: 0.0817 | 0.0636
Epoch 37/300, resid Loss: 0.0803 | 0.0469
Epoch 38/300, resid Loss: 0.0784 | 0.0439
Epoch 39/300, resid Loss: 0.0783 | 0.0442
Epoch 40/300, resid Loss: 0.0774 | 0.0466
Epoch 41/300, resid Loss: 0.0770 | 0.0494
Epoch 42/300, resid Loss: 0.0766 | 0.0482
Epoch 43/300, resid Loss: 0.0762 | 0.0441
Epoch 44/300, resid Loss: 0.0761 | 0.0420
Epoch 45/300, resid Loss: 0.0754 | 0.0424
Epoch 46/300, resid Loss: 0.0745 | 0.0434
Epoch 47/300, resid Loss: 0.0740 | 0.0434
Epoch 48/300, resid Loss: 0.0736 | 0.0427
Epoch 49/300, resid Loss: 0.0734 | 0.0421
Epoch 50/300, resid Loss: 0.0731 | 0.0415
Epoch 51/300, resid Loss: 0.0728 | 0.0413
Epoch 52/300, resid Loss: 0.0724 | 0.0415
Epoch 53/300, resid Loss: 0.0720 | 0.0410
Epoch 54/300, resid Loss: 0.0718 | 0.0406
Epoch 55/300, resid Loss: 0.0715 | 0.0404
Epoch 56/300, resid Loss: 0.0713 | 0.0404
Epoch 57/300, resid Loss: 0.0711 | 0.0399
Epoch 58/300, resid Loss: 0.0709 | 0.0396
Epoch 59/300, resid Loss: 0.0707 | 0.0396
Epoch 60/300, resid Loss: 0.0705 | 0.0394
Epoch 61/300, resid Loss: 0.0704 | 0.0393
Epoch 62/300, resid Loss: 0.0702 | 0.0391
Epoch 63/300, resid Loss: 0.0701 | 0.0390
Epoch 64/300, resid Loss: 0.0699 | 0.0386
Epoch 65/300, resid Loss: 0.0697 | 0.0386
Epoch 66/300, resid Loss: 0.0696 | 0.0385
Epoch 67/300, resid Loss: 0.0695 | 0.0383
Epoch 68/300, resid Loss: 0.0694 | 0.0382
Epoch 69/300, resid Loss: 0.0693 | 0.0381
Epoch 70/300, resid Loss: 0.0691 | 0.0380
Epoch 71/300, resid Loss: 0.0690 | 0.0378
Epoch 72/300, resid Loss: 0.0689 | 0.0377
Epoch 73/300, resid Loss: 0.0688 | 0.0376
Epoch 74/300, resid Loss: 0.0687 | 0.0375
Epoch 75/300, resid Loss: 0.0686 | 0.0374
Epoch 76/300, resid Loss: 0.0685 | 0.0374
Epoch 77/300, resid Loss: 0.0684 | 0.0373
Epoch 78/300, resid Loss: 0.0683 | 0.0371
Epoch 79/300, resid Loss: 0.0682 | 0.0371
Epoch 80/300, resid Loss: 0.0682 | 0.0370
Epoch 81/300, resid Loss: 0.0681 | 0.0369
Epoch 82/300, resid Loss: 0.0680 | 0.0369
Epoch 83/300, resid Loss: 0.0679 | 0.0368
Epoch 84/300, resid Loss: 0.0679 | 0.0367
Epoch 85/300, resid Loss: 0.0678 | 0.0366
Epoch 86/300, resid Loss: 0.0677 | 0.0366
Epoch 87/300, resid Loss: 0.0677 | 0.0365
Epoch 88/300, resid Loss: 0.0676 | 0.0365
Epoch 89/300, resid Loss: 0.0676 | 0.0364
Epoch 90/300, resid Loss: 0.0675 | 0.0364
Epoch 91/300, resid Loss: 0.0674 | 0.0363
Epoch 92/300, resid Loss: 0.0674 | 0.0363
Epoch 93/300, resid Loss: 0.0673 | 0.0362
Epoch 94/300, resid Loss: 0.0673 | 0.0362
Epoch 95/300, resid Loss: 0.0672 | 0.0361
Epoch 96/300, resid Loss: 0.0672 | 0.0361
Epoch 97/300, resid Loss: 0.0671 | 0.0361
Epoch 98/300, resid Loss: 0.0671 | 0.0360
Epoch 99/300, resid Loss: 0.0670 | 0.0360
Epoch 100/300, resid Loss: 0.0670 | 0.0359
Epoch 101/300, resid Loss: 0.0670 | 0.0359
Epoch 102/300, resid Loss: 0.0669 | 0.0359
Epoch 103/300, resid Loss: 0.0669 | 0.0358
Epoch 104/300, resid Loss: 0.0669 | 0.0358
Epoch 105/300, resid Loss: 0.0668 | 0.0358
Epoch 106/300, resid Loss: 0.0668 | 0.0357
Epoch 107/300, resid Loss: 0.0668 | 0.0357
Epoch 108/300, resid Loss: 0.0667 | 0.0357
Epoch 109/300, resid Loss: 0.0667 | 0.0356
Epoch 110/300, resid Loss: 0.0667 | 0.0356
Epoch 111/300, resid Loss: 0.0667 | 0.0356
Epoch 112/300, resid Loss: 0.0666 | 0.0356
Epoch 113/300, resid Loss: 0.0666 | 0.0355
Epoch 114/300, resid Loss: 0.0666 | 0.0355
Epoch 115/300, resid Loss: 0.0665 | 0.0355
Epoch 116/300, resid Loss: 0.0665 | 0.0355
Epoch 117/300, resid Loss: 0.0665 | 0.0355
Epoch 118/300, resid Loss: 0.0665 | 0.0354
Epoch 119/300, resid Loss: 0.0665 | 0.0354
Epoch 120/300, resid Loss: 0.0664 | 0.0354
Epoch 121/300, resid Loss: 0.0664 | 0.0354
Epoch 122/300, resid Loss: 0.0664 | 0.0354
Epoch 123/300, resid Loss: 0.0664 | 0.0354
Epoch 124/300, resid Loss: 0.0664 | 0.0353
Epoch 125/300, resid Loss: 0.0664 | 0.0353
Epoch 126/300, resid Loss: 0.0663 | 0.0353
Epoch 127/300, resid Loss: 0.0663 | 0.0353
Epoch 128/300, resid Loss: 0.0663 | 0.0353
Epoch 129/300, resid Loss: 0.0663 | 0.0353
Epoch 130/300, resid Loss: 0.0663 | 0.0353
Epoch 131/300, resid Loss: 0.0663 | 0.0353
Epoch 132/300, resid Loss: 0.0663 | 0.0352
Epoch 133/300, resid Loss: 0.0662 | 0.0352
Epoch 134/300, resid Loss: 0.0662 | 0.0352
Epoch 135/300, resid Loss: 0.0662 | 0.0352
Epoch 136/300, resid Loss: 0.0662 | 0.0352
Epoch 137/300, resid Loss: 0.0662 | 0.0352
Epoch 138/300, resid Loss: 0.0662 | 0.0352
Epoch 139/300, resid Loss: 0.0662 | 0.0352
Epoch 140/300, resid Loss: 0.0662 | 0.0352
Epoch 141/300, resid Loss: 0.0662 | 0.0352
Epoch 142/300, resid Loss: 0.0661 | 0.0351
Epoch 143/300, resid Loss: 0.0661 | 0.0351
Epoch 144/300, resid Loss: 0.0661 | 0.0351
Epoch 145/300, resid Loss: 0.0661 | 0.0351
Epoch 146/300, resid Loss: 0.0661 | 0.0351
Epoch 147/300, resid Loss: 0.0661 | 0.0351
Epoch 148/300, resid Loss: 0.0661 | 0.0351
Epoch 149/300, resid Loss: 0.0661 | 0.0351
Epoch 150/300, resid Loss: 0.0661 | 0.0351
Epoch 151/300, resid Loss: 0.0661 | 0.0351
Epoch 152/300, resid Loss: 0.0661 | 0.0351
Epoch 153/300, resid Loss: 0.0661 | 0.0351
Epoch 154/300, resid Loss: 0.0661 | 0.0351
Epoch 155/300, resid Loss: 0.0661 | 0.0351
Epoch 156/300, resid Loss: 0.0661 | 0.0351
Epoch 157/300, resid Loss: 0.0660 | 0.0350
Epoch 158/300, resid Loss: 0.0660 | 0.0350
Epoch 159/300, resid Loss: 0.0660 | 0.0350
Epoch 160/300, resid Loss: 0.0660 | 0.0350
Epoch 161/300, resid Loss: 0.0660 | 0.0350
Epoch 162/300, resid Loss: 0.0660 | 0.0350
Epoch 163/300, resid Loss: 0.0660 | 0.0350
Epoch 164/300, resid Loss: 0.0660 | 0.0350
Epoch 165/300, resid Loss: 0.0660 | 0.0350
Epoch 166/300, resid Loss: 0.0660 | 0.0350
Epoch 167/300, resid Loss: 0.0660 | 0.0350
Epoch 168/300, resid Loss: 0.0660 | 0.0350
Epoch 169/300, resid Loss: 0.0660 | 0.0350
Epoch 170/300, resid Loss: 0.0660 | 0.0350
Epoch 171/300, resid Loss: 0.0660 | 0.0350
Epoch 172/300, resid Loss: 0.0660 | 0.0350
Epoch 173/300, resid Loss: 0.0660 | 0.0350
Epoch 174/300, resid Loss: 0.0660 | 0.0350
Epoch 175/300, resid Loss: 0.0660 | 0.0350
Epoch 176/300, resid Loss: 0.0660 | 0.0350
Epoch 177/300, resid Loss: 0.0660 | 0.0350
Epoch 178/300, resid Loss: 0.0660 | 0.0350
Epoch 179/300, resid Loss: 0.0660 | 0.0350
Epoch 180/300, resid Loss: 0.0660 | 0.0350
Epoch 181/300, resid Loss: 0.0660 | 0.0350
Epoch 182/300, resid Loss: 0.0660 | 0.0350
Epoch 183/300, resid Loss: 0.0660 | 0.0350
Epoch 184/300, resid Loss: 0.0660 | 0.0350
Epoch 185/300, resid Loss: 0.0660 | 0.0350
Epoch 186/300, resid Loss: 0.0660 | 0.0350
Epoch 187/300, resid Loss: 0.0660 | 0.0350
Epoch 188/300, resid Loss: 0.0660 | 0.0350
Epoch 189/300, resid Loss: 0.0660 | 0.0350
Epoch 190/300, resid Loss: 0.0659 | 0.0350
Epoch 191/300, resid Loss: 0.0659 | 0.0350
Epoch 192/300, resid Loss: 0.0659 | 0.0350
Epoch 193/300, resid Loss: 0.0659 | 0.0350
Epoch 194/300, resid Loss: 0.0659 | 0.0349
Epoch 195/300, resid Loss: 0.0659 | 0.0349
Epoch 196/300, resid Loss: 0.0659 | 0.0349
Epoch 197/300, resid Loss: 0.0659 | 0.0349
Epoch 198/300, resid Loss: 0.0659 | 0.0349
Epoch 199/300, resid Loss: 0.0659 | 0.0349
Epoch 200/300, resid Loss: 0.0659 | 0.0349
Epoch 201/300, resid Loss: 0.0659 | 0.0349
Epoch 202/300, resid Loss: 0.0659 | 0.0349
Epoch 203/300, resid Loss: 0.0659 | 0.0349
Epoch 204/300, resid Loss: 0.0659 | 0.0349
Epoch 205/300, resid Loss: 0.0659 | 0.0349
Epoch 206/300, resid Loss: 0.0659 | 0.0349
Epoch 207/300, resid Loss: 0.0659 | 0.0349
Epoch 208/300, resid Loss: 0.0659 | 0.0349
Epoch 209/300, resid Loss: 0.0659 | 0.0349
Epoch 210/300, resid Loss: 0.0659 | 0.0349
Epoch 211/300, resid Loss: 0.0659 | 0.0349
Epoch 212/300, resid Loss: 0.0659 | 0.0349
Epoch 213/300, resid Loss: 0.0659 | 0.0349
Epoch 214/300, resid Loss: 0.0659 | 0.0349
Epoch 215/300, resid Loss: 0.0659 | 0.0349
Epoch 216/300, resid Loss: 0.0659 | 0.0349
Epoch 217/300, resid Loss: 0.0659 | 0.0349
Epoch 218/300, resid Loss: 0.0659 | 0.0349
Epoch 219/300, resid Loss: 0.0659 | 0.0349
Epoch 220/300, resid Loss: 0.0659 | 0.0349
Epoch 221/300, resid Loss: 0.0659 | 0.0349
Epoch 222/300, resid Loss: 0.0659 | 0.0349
Epoch 223/300, resid Loss: 0.0659 | 0.0349
Epoch 224/300, resid Loss: 0.0659 | 0.0349
Epoch 225/300, resid Loss: 0.0659 | 0.0349
Epoch 226/300, resid Loss: 0.0659 | 0.0349
Epoch 227/300, resid Loss: 0.0659 | 0.0349
Epoch 228/300, resid Loss: 0.0659 | 0.0349
Epoch 229/300, resid Loss: 0.0659 | 0.0349
Epoch 230/300, resid Loss: 0.0659 | 0.0349
Epoch 231/300, resid Loss: 0.0659 | 0.0349
Epoch 232/300, resid Loss: 0.0659 | 0.0349
Epoch 233/300, resid Loss: 0.0659 | 0.0349
Epoch 234/300, resid Loss: 0.0659 | 0.0349
Epoch 235/300, resid Loss: 0.0659 | 0.0349
Epoch 236/300, resid Loss: 0.0659 | 0.0349
Epoch 237/300, resid Loss: 0.0659 | 0.0349
Epoch 238/300, resid Loss: 0.0659 | 0.0349
Epoch 239/300, resid Loss: 0.0659 | 0.0349
Epoch 240/300, resid Loss: 0.0659 | 0.0349
Epoch 241/300, resid Loss: 0.0659 | 0.0349
Epoch 242/300, resid Loss: 0.0659 | 0.0349
Epoch 243/300, resid Loss: 0.0659 | 0.0349
Epoch 244/300, resid Loss: 0.0659 | 0.0349
Epoch 245/300, resid Loss: 0.0659 | 0.0349
Epoch 246/300, resid Loss: 0.0659 | 0.0349
Epoch 247/300, resid Loss: 0.0659 | 0.0349
Epoch 248/300, resid Loss: 0.0659 | 0.0349
Epoch 249/300, resid Loss: 0.0659 | 0.0349
Epoch 250/300, resid Loss: 0.0659 | 0.0349
Epoch 251/300, resid Loss: 0.0659 | 0.0349
Epoch 252/300, resid Loss: 0.0659 | 0.0349
Epoch 253/300, resid Loss: 0.0659 | 0.0349
Epoch 254/300, resid Loss: 0.0659 | 0.0349
Epoch 255/300, resid Loss: 0.0659 | 0.0349
Epoch 256/300, resid Loss: 0.0659 | 0.0349
Epoch 257/300, resid Loss: 0.0659 | 0.0349
Epoch 258/300, resid Loss: 0.0659 | 0.0349
Epoch 259/300, resid Loss: 0.0659 | 0.0349
Epoch 260/300, resid Loss: 0.0659 | 0.0349
Epoch 261/300, resid Loss: 0.0659 | 0.0349
Epoch 262/300, resid Loss: 0.0659 | 0.0349
Epoch 263/300, resid Loss: 0.0659 | 0.0349
Epoch 264/300, resid Loss: 0.0659 | 0.0349
Epoch 265/300, resid Loss: 0.0659 | 0.0349
Epoch 266/300, resid Loss: 0.0659 | 0.0349
Epoch 267/300, resid Loss: 0.0659 | 0.0349
Epoch 268/300, resid Loss: 0.0659 | 0.0349
Epoch 269/300, resid Loss: 0.0659 | 0.0349
Epoch 270/300, resid Loss: 0.0659 | 0.0349
Epoch 271/300, resid Loss: 0.0659 | 0.0349
Epoch 272/300, resid Loss: 0.0659 | 0.0349
Epoch 273/300, resid Loss: 0.0659 | 0.0349
Epoch 274/300, resid Loss: 0.0659 | 0.0349
Epoch 275/300, resid Loss: 0.0659 | 0.0349
Epoch 276/300, resid Loss: 0.0659 | 0.0349
Epoch 277/300, resid Loss: 0.0659 | 0.0349
Epoch 278/300, resid Loss: 0.0659 | 0.0349
Epoch 279/300, resid Loss: 0.0659 | 0.0349
Epoch 280/300, resid Loss: 0.0659 | 0.0349
Epoch 281/300, resid Loss: 0.0659 | 0.0349
Epoch 282/300, resid Loss: 0.0659 | 0.0349
Epoch 283/300, resid Loss: 0.0659 | 0.0349
Epoch 284/300, resid Loss: 0.0659 | 0.0349
Epoch 285/300, resid Loss: 0.0659 | 0.0349
Epoch 286/300, resid Loss: 0.0659 | 0.0349
Epoch 287/300, resid Loss: 0.0659 | 0.0349
Epoch 288/300, resid Loss: 0.0659 | 0.0349
Epoch 289/300, resid Loss: 0.0659 | 0.0349
Epoch 290/300, resid Loss: 0.0659 | 0.0349
Epoch 291/300, resid Loss: 0.0659 | 0.0349
Epoch 292/300, resid Loss: 0.0659 | 0.0349
Epoch 293/300, resid Loss: 0.0659 | 0.0349
Epoch 294/300, resid Loss: 0.0659 | 0.0349
Epoch 295/300, resid Loss: 0.0659 | 0.0349
Epoch 296/300, resid Loss: 0.0659 | 0.0349
Epoch 297/300, resid Loss: 0.0659 | 0.0349
Epoch 298/300, resid Loss: 0.0659 | 0.0349
Epoch 299/300, resid Loss: 0.0659 | 0.0349
Epoch 300/300, resid Loss: 0.0659 | 0.0349
Runtime (seconds): 1491.4723107814789
4.735616625640761e-05
[158.04277]
[-5.4886036]
[2.801457]
[14.012123]
[2.6435258]
[20.717907]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 229.85069463029504
RMSE: 15.16082763671875
MAE: 15.16082763671875
R-squared: nan
[192.72917]
