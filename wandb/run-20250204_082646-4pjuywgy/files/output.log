ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-04 08:26:49,445][0m A new study created in memory with name: no-name-8f2a59b8-9d41-459e-bfa2-c1ed1316afce[0m
[32m[I 2025-02-04 08:28:02,124][0m Trial 0 finished with value: 0.5355824925965327 and parameters: {'observation_period_num': 232, 'train_rates': 0.7604271362358151, 'learning_rate': 6.2124684562741026e-06, 'batch_size': 66, 'step_size': 6, 'gamma': 0.875554286757917}. Best is trial 0 with value: 0.5355824925965327.[0m
[32m[I 2025-02-04 08:29:05,417][0m Trial 1 finished with value: 0.0773827363810842 and parameters: {'observation_period_num': 7, 'train_rates': 0.8290408165624743, 'learning_rate': 1.2183667661813414e-05, 'batch_size': 88, 'step_size': 14, 'gamma': 0.9270954538881896}. Best is trial 1 with value: 0.0773827363810842.[0m
[32m[I 2025-02-04 08:29:29,775][0m Trial 2 finished with value: 0.836906373025255 and parameters: {'observation_period_num': 251, 'train_rates': 0.7219159150308742, 'learning_rate': 2.568514495861553e-06, 'batch_size': 210, 'step_size': 14, 'gamma': 0.8339965792785184}. Best is trial 1 with value: 0.0773827363810842.[0m
[32m[I 2025-02-04 08:30:39,129][0m Trial 3 finished with value: 0.28189722384358273 and parameters: {'observation_period_num': 240, 'train_rates': 0.871794137983898, 'learning_rate': 1.9851718404581917e-05, 'batch_size': 75, 'step_size': 6, 'gamma': 0.8042340613157066}. Best is trial 1 with value: 0.0773827363810842.[0m
[32m[I 2025-02-04 08:31:35,986][0m Trial 4 finished with value: 1.3988280378256466 and parameters: {'observation_period_num': 78, 'train_rates': 0.7977923439848843, 'learning_rate': 2.847383199634053e-06, 'batch_size': 94, 'step_size': 1, 'gamma': 0.9260741409261988}. Best is trial 1 with value: 0.0773827363810842.[0m
[32m[I 2025-02-04 08:32:34,830][0m Trial 5 finished with value: 0.14098492321694733 and parameters: {'observation_period_num': 68, 'train_rates': 0.8912622677144192, 'learning_rate': 9.213565956228024e-06, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9373245140829068}. Best is trial 1 with value: 0.0773827363810842.[0m
Early stopping at epoch 80
[32m[I 2025-02-04 08:33:52,617][0m Trial 6 finished with value: 0.30632062172307106 and parameters: {'observation_period_num': 111, 'train_rates': 0.8128853267636853, 'learning_rate': 5.231980072896242e-05, 'batch_size': 54, 'step_size': 1, 'gamma': 0.8418720086925138}. Best is trial 1 with value: 0.0773827363810842.[0m
[32m[I 2025-02-04 08:34:24,878][0m Trial 7 finished with value: 0.3089595450935039 and parameters: {'observation_period_num': 243, 'train_rates': 0.6754786577415687, 'learning_rate': 0.0001835948791266395, 'batch_size': 146, 'step_size': 6, 'gamma': 0.840138360743955}. Best is trial 1 with value: 0.0773827363810842.[0m
[32m[I 2025-02-04 08:36:30,724][0m Trial 8 finished with value: 0.05969994510363402 and parameters: {'observation_period_num': 60, 'train_rates': 0.8398393731722473, 'learning_rate': 2.4899097523100157e-05, 'batch_size': 42, 'step_size': 6, 'gamma': 0.9813799966081018}. Best is trial 8 with value: 0.05969994510363402.[0m
[32m[I 2025-02-04 08:37:48,741][0m Trial 9 finished with value: 0.2627642734710914 and parameters: {'observation_period_num': 250, 'train_rates': 0.6286448856981375, 'learning_rate': 0.00010253703985525406, 'batch_size': 53, 'step_size': 12, 'gamma': 0.8860052365687843}. Best is trial 8 with value: 0.05969994510363402.[0m
[32m[I 2025-02-04 08:42:59,014][0m Trial 10 finished with value: 0.23637558023134866 and parameters: {'observation_period_num': 173, 'train_rates': 0.9806227081011908, 'learning_rate': 0.0004909061472400515, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9881083806008909}. Best is trial 8 with value: 0.05969994510363402.[0m
[32m[I 2025-02-04 08:43:43,035][0m Trial 11 finished with value: 0.061717078514248555 and parameters: {'observation_period_num': 12, 'train_rates': 0.8806144511288796, 'learning_rate': 2.2624790572695916e-05, 'batch_size': 142, 'step_size': 12, 'gamma': 0.9892424612053224}. Best is trial 8 with value: 0.05969994510363402.[0m
[32m[I 2025-02-04 08:44:22,991][0m Trial 12 finished with value: 0.05166195610376156 and parameters: {'observation_period_num': 22, 'train_rates': 0.9372593336830383, 'learning_rate': 4.2427488636092274e-05, 'batch_size': 155, 'step_size': 11, 'gamma': 0.989581529825888}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:44:55,974][0m Trial 13 finished with value: 0.10904517769813538 and parameters: {'observation_period_num': 48, 'train_rates': 0.98918990489309, 'learning_rate': 7.021901133582936e-05, 'batch_size': 193, 'step_size': 4, 'gamma': 0.9597888716221068}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:45:21,302][0m Trial 14 finished with value: 0.10042773932218552 and parameters: {'observation_period_num': 124, 'train_rates': 0.9251474573123109, 'learning_rate': 0.00029698387821500014, 'batch_size': 250, 'step_size': 11, 'gamma': 0.7720992685630228}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:45:56,437][0m Trial 15 finished with value: 0.5311343008990115 and parameters: {'observation_period_num': 39, 'train_rates': 0.933768038066996, 'learning_rate': 1.0829901190475083e-06, 'batch_size': 174, 'step_size': 8, 'gamma': 0.959125089076765}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:46:43,010][0m Trial 16 finished with value: 0.08916340580453043 and parameters: {'observation_period_num': 95, 'train_rates': 0.8392977342637629, 'learning_rate': 0.0009577361663778815, 'batch_size': 117, 'step_size': 4, 'gamma': 0.8947113110890791}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:49:40,517][0m Trial 17 finished with value: 0.1073743184407552 and parameters: {'observation_period_num': 154, 'train_rates': 0.9358434887689627, 'learning_rate': 4.006237692020249e-05, 'batch_size': 31, 'step_size': 10, 'gamma': 0.9658176819405094}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:50:13,364][0m Trial 18 finished with value: 0.09933390292607237 and parameters: {'observation_period_num': 38, 'train_rates': 0.7629990826606292, 'learning_rate': 2.8405692058014956e-05, 'batch_size': 169, 'step_size': 15, 'gamma': 0.9041616619430463}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:51:02,356][0m Trial 19 finished with value: 0.057972045651135534 and parameters: {'observation_period_num': 55, 'train_rates': 0.8564160479350731, 'learning_rate': 0.00011352937331420147, 'batch_size': 116, 'step_size': 4, 'gamma': 0.9504389254105089}. Best is trial 12 with value: 0.05166195610376156.[0m
[32m[I 2025-02-04 08:51:52,896][0m Trial 20 finished with value: 0.046890974597219276 and parameters: {'observation_period_num': 20, 'train_rates': 0.9106280937467218, 'learning_rate': 0.00013222300542333577, 'batch_size': 118, 'step_size': 3, 'gamma': 0.9420459222190152}. Best is trial 20 with value: 0.046890974597219276.[0m
[32m[I 2025-02-04 08:52:44,344][0m Trial 21 finished with value: 0.04742956206326168 and parameters: {'observation_period_num': 26, 'train_rates': 0.9178797563007624, 'learning_rate': 0.00012119601070242688, 'batch_size': 117, 'step_size': 3, 'gamma': 0.9380404068104974}. Best is trial 20 with value: 0.046890974597219276.[0m
[32m[I 2025-02-04 08:53:32,616][0m Trial 22 finished with value: 0.04467462748289108 and parameters: {'observation_period_num': 20, 'train_rates': 0.9148441438605313, 'learning_rate': 0.00021060751249785814, 'batch_size': 125, 'step_size': 3, 'gamma': 0.9116675451441694}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:54:23,142][0m Trial 23 finished with value: 0.04848804738633747 and parameters: {'observation_period_num': 27, 'train_rates': 0.9073886904241515, 'learning_rate': 0.00025369881329854923, 'batch_size': 119, 'step_size': 2, 'gamma': 0.9112621897817119}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:55:12,825][0m Trial 24 finished with value: 0.07570074719190598 and parameters: {'observation_period_num': 90, 'train_rates': 0.9565061889462698, 'learning_rate': 0.0006501420383022318, 'batch_size': 123, 'step_size': 3, 'gamma': 0.8605052484763012}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:56:04,800][0m Trial 25 finished with value: 0.09573573010568401 and parameters: {'observation_period_num': 204, 'train_rates': 0.9048168819752047, 'learning_rate': 0.00014862606923562113, 'batch_size': 104, 'step_size': 3, 'gamma': 0.9110124190657739}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:56:50,872][0m Trial 26 finished with value: 0.048758089542388916 and parameters: {'observation_period_num': 5, 'train_rates': 0.9593044343389613, 'learning_rate': 0.0003628448868755054, 'batch_size': 137, 'step_size': 2, 'gamma': 0.9382443769736605}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:57:27,681][0m Trial 27 finished with value: 0.06519835484550171 and parameters: {'observation_period_num': 28, 'train_rates': 0.8675075673336743, 'learning_rate': 7.928461959907698e-05, 'batch_size': 162, 'step_size': 5, 'gamma': 0.9209225643179653}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:58:33,398][0m Trial 28 finished with value: 0.07946994475840262 and parameters: {'observation_period_num': 81, 'train_rates': 0.7697319583945283, 'learning_rate': 0.00019496050680583828, 'batch_size': 77, 'step_size': 2, 'gamma': 0.9404705062895135}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:59:06,217][0m Trial 29 finished with value: 0.11613746732473373 and parameters: {'observation_period_num': 155, 'train_rates': 0.9606409876698041, 'learning_rate': 0.0005250267641907888, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8682761470094549}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 08:59:47,512][0m Trial 30 finished with value: 0.09934840812055384 and parameters: {'observation_period_num': 42, 'train_rates': 0.7156421167531326, 'learning_rate': 0.00012029875779049479, 'batch_size': 130, 'step_size': 7, 'gamma': 0.8832489941004362}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 09:00:42,041][0m Trial 31 finished with value: 0.050714778519277255 and parameters: {'observation_period_num': 25, 'train_rates': 0.9087324865650972, 'learning_rate': 0.00022720798680310876, 'batch_size': 109, 'step_size': 2, 'gamma': 0.8975218384627635}. Best is trial 22 with value: 0.04467462748289108.[0m
[32m[I 2025-02-04 09:01:28,377][0m Trial 32 finished with value: 0.03953869238998113 and parameters: {'observation_period_num': 20, 'train_rates': 0.9089883636970826, 'learning_rate': 0.00032814208771230434, 'batch_size': 129, 'step_size': 3, 'gamma': 0.9130894987734629}. Best is trial 32 with value: 0.03953869238998113.[0m
[32m[I 2025-02-04 09:02:35,156][0m Trial 33 finished with value: 0.030040605347661585 and parameters: {'observation_period_num': 7, 'train_rates': 0.8910502637348999, 'learning_rate': 0.00037720837755511713, 'batch_size': 86, 'step_size': 3, 'gamma': 0.9217696725671704}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:03:50,399][0m Trial 34 finished with value: 0.03969378339556547 and parameters: {'observation_period_num': 10, 'train_rates': 0.8895340618364939, 'learning_rate': 0.00081369241328801, 'batch_size': 75, 'step_size': 5, 'gamma': 0.9184711034674845}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:04:59,804][0m Trial 35 finished with value: 0.03534308599364841 and parameters: {'observation_period_num': 5, 'train_rates': 0.8552213336727196, 'learning_rate': 0.0008936969028744912, 'batch_size': 82, 'step_size': 5, 'gamma': 0.9184180010277783}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:06:09,860][0m Trial 36 finished with value: 0.03532452793913212 and parameters: {'observation_period_num': 8, 'train_rates': 0.8228909215806182, 'learning_rate': 0.0008319100068468039, 'batch_size': 77, 'step_size': 5, 'gamma': 0.9210776685454176}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:07:11,349][0m Trial 37 finished with value: 0.07319275741694403 and parameters: {'observation_period_num': 66, 'train_rates': 0.8100268452469732, 'learning_rate': 0.000406664325927512, 'batch_size': 88, 'step_size': 7, 'gamma': 0.8548675997305251}. Best is trial 33 with value: 0.030040605347661585.[0m
Early stopping at epoch 73
[32m[I 2025-02-04 09:08:17,312][0m Trial 38 finished with value: 0.05839637685271483 and parameters: {'observation_period_num': 8, 'train_rates': 0.7861975457303898, 'learning_rate': 0.0007770421922757004, 'batch_size': 59, 'step_size': 1, 'gamma': 0.8189426662052067}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:09:20,917][0m Trial 39 finished with value: 0.18206122947497005 and parameters: {'observation_period_num': 212, 'train_rates': 0.8508436020093793, 'learning_rate': 0.0005623143921160027, 'batch_size': 83, 'step_size': 5, 'gamma': 0.9297680981904762}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:10:41,713][0m Trial 40 finished with value: 0.050544021308308984 and parameters: {'observation_period_num': 38, 'train_rates': 0.8192000918411352, 'learning_rate': 0.000369740713145043, 'batch_size': 66, 'step_size': 4, 'gamma': 0.8775847314177873}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:11:43,144][0m Trial 41 finished with value: 0.03250784426373208 and parameters: {'observation_period_num': 8, 'train_rates': 0.8860046503736633, 'learning_rate': 0.0009684324141031986, 'batch_size': 95, 'step_size': 7, 'gamma': 0.9220302635027199}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:12:42,543][0m Trial 42 finished with value: 0.03017572338446247 and parameters: {'observation_period_num': 8, 'train_rates': 0.868012457047542, 'learning_rate': 0.0008632827976007432, 'batch_size': 97, 'step_size': 7, 'gamma': 0.8911425041405344}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:13:41,545][0m Trial 43 finished with value: 0.061199161945654945 and parameters: {'observation_period_num': 50, 'train_rates': 0.8717700982557202, 'learning_rate': 0.0009720826595635828, 'batch_size': 97, 'step_size': 7, 'gamma': 0.9011477155992396}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:15:43,732][0m Trial 44 finished with value: 0.03880702248548132 and parameters: {'observation_period_num': 8, 'train_rates': 0.8296529008847248, 'learning_rate': 0.0006738878244519659, 'batch_size': 44, 'step_size': 8, 'gamma': 0.8879991339214006}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:17:03,171][0m Trial 45 finished with value: 0.05957815141237083 and parameters: {'observation_period_num': 33, 'train_rates': 0.7961921150484668, 'learning_rate': 0.0004915135790581458, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9277570998855631}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:17:53,009][0m Trial 46 finished with value: 0.31109463583643193 and parameters: {'observation_period_num': 72, 'train_rates': 0.7401702991266045, 'learning_rate': 8.543923613368728e-06, 'batch_size': 97, 'step_size': 8, 'gamma': 0.871816546958959}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:19:00,249][0m Trial 47 finished with value: 0.036463015537769504 and parameters: {'observation_period_num': 5, 'train_rates': 0.8540615342806352, 'learning_rate': 0.0009984078651298295, 'batch_size': 85, 'step_size': 6, 'gamma': 0.9510500232368357}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:21:01,870][0m Trial 48 finished with value: 0.08721116360869598 and parameters: {'observation_period_num': 59, 'train_rates': 0.8785522296752587, 'learning_rate': 0.0006753745425419197, 'batch_size': 45, 'step_size': 9, 'gamma': 0.9690309944549904}. Best is trial 33 with value: 0.030040605347661585.[0m
[32m[I 2025-02-04 09:22:17,504][0m Trial 49 finished with value: 0.13957965172872952 and parameters: {'observation_period_num': 16, 'train_rates': 0.833682053589552, 'learning_rate': 4.493354567436749e-06, 'batch_size': 74, 'step_size': 7, 'gamma': 0.9277342600056386}. Best is trial 33 with value: 0.030040605347661585.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-04 09:22:17,515][0m A new study created in memory with name: no-name-116635e4-7013-4e5a-9ea1-dea5a28c54a9[0m
[32m[I 2025-02-04 09:22:49,102][0m Trial 0 finished with value: 0.16037492454051971 and parameters: {'observation_period_num': 203, 'train_rates': 0.9422651308188561, 'learning_rate': 0.00015298746703126207, 'batch_size': 196, 'step_size': 5, 'gamma': 0.7718242877183763}. Best is trial 0 with value: 0.16037492454051971.[0m
Early stopping at epoch 65
[32m[I 2025-02-04 09:23:29,822][0m Trial 1 finished with value: 0.7409850541692581 and parameters: {'observation_period_num': 167, 'train_rates': 0.9019218214366032, 'learning_rate': 4.672347010032718e-06, 'batch_size': 92, 'step_size': 1, 'gamma': 0.8471968901360121}. Best is trial 0 with value: 0.16037492454051971.[0m
[32m[I 2025-02-04 09:23:52,585][0m Trial 2 finished with value: 0.1723420767832315 and parameters: {'observation_period_num': 171, 'train_rates': 0.6487997531945373, 'learning_rate': 0.0007306202575399132, 'batch_size': 221, 'step_size': 10, 'gamma': 0.8630881240375852}. Best is trial 0 with value: 0.16037492454051971.[0m
[32m[I 2025-02-04 09:26:30,070][0m Trial 3 finished with value: 0.10795879211961007 and parameters: {'observation_period_num': 237, 'train_rates': 0.8556984234339124, 'learning_rate': 3.7220154113662634e-05, 'batch_size': 32, 'step_size': 14, 'gamma': 0.8005305386924098}. Best is trial 3 with value: 0.10795879211961007.[0m
[32m[I 2025-02-04 09:27:59,423][0m Trial 4 finished with value: 0.29268823222941664 and parameters: {'observation_period_num': 241, 'train_rates': 0.7552219546047293, 'learning_rate': 6.253526524139224e-06, 'batch_size': 52, 'step_size': 8, 'gamma': 0.9812997040783583}. Best is trial 3 with value: 0.10795879211961007.[0m
[32m[I 2025-02-04 09:28:27,973][0m Trial 5 finished with value: 0.06166348163026887 and parameters: {'observation_period_num': 41, 'train_rates': 0.7795428275868888, 'learning_rate': 0.000726887258203287, 'batch_size': 194, 'step_size': 4, 'gamma': 0.9856018088788262}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:29:01,458][0m Trial 6 finished with value: 0.14480894804000854 and parameters: {'observation_period_num': 87, 'train_rates': 0.9854260169566289, 'learning_rate': 3.6314462523662834e-05, 'batch_size': 199, 'step_size': 6, 'gamma': 0.9025064475277234}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:29:32,128][0m Trial 7 finished with value: 0.1144455075263977 and parameters: {'observation_period_num': 249, 'train_rates': 0.9376482476964891, 'learning_rate': 0.0003843797137545102, 'batch_size': 186, 'step_size': 7, 'gamma': 0.8245032723526725}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:30:31,590][0m Trial 8 finished with value: 0.0686984499565727 and parameters: {'observation_period_num': 123, 'train_rates': 0.9156810709079144, 'learning_rate': 0.00023609028923146508, 'batch_size': 96, 'step_size': 11, 'gamma': 0.7589503391745632}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:30:54,457][0m Trial 9 finished with value: 0.4135897912562788 and parameters: {'observation_period_num': 242, 'train_rates': 0.7296318902650838, 'learning_rate': 7.394306872253171e-05, 'batch_size': 220, 'step_size': 3, 'gamma': 0.8366310485345889}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:31:12,954][0m Trial 10 finished with value: 0.721866025794154 and parameters: {'observation_period_num': 8, 'train_rates': 0.6030450913412947, 'learning_rate': 1.7696109546734256e-06, 'batch_size': 256, 'step_size': 1, 'gamma': 0.9844395104450017}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:31:51,685][0m Trial 11 finished with value: 0.09953001287067309 and parameters: {'observation_period_num': 85, 'train_rates': 0.8216946388505337, 'learning_rate': 0.000977404195708868, 'batch_size': 140, 'step_size': 12, 'gamma': 0.916100197640225}. Best is trial 5 with value: 0.06166348163026887.[0m
[32m[I 2025-02-04 09:32:32,009][0m Trial 12 finished with value: 0.04826603198455552 and parameters: {'observation_period_num': 7, 'train_rates': 0.7599072322287548, 'learning_rate': 0.00025740581211596063, 'batch_size': 133, 'step_size': 10, 'gamma': 0.9305800081364823}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:33:08,212][0m Trial 13 finished with value: 0.05975562767803389 and parameters: {'observation_period_num': 7, 'train_rates': 0.7107034275107555, 'learning_rate': 0.0001261481051424795, 'batch_size': 140, 'step_size': 4, 'gamma': 0.9456547921849148}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:33:42,272][0m Trial 14 finished with value: 0.06482991807919083 and parameters: {'observation_period_num': 10, 'train_rates': 0.6853546602599283, 'learning_rate': 9.597552930482695e-05, 'batch_size': 144, 'step_size': 9, 'gamma': 0.9169229743646331}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:34:27,523][0m Trial 15 finished with value: 0.11298534400620551 and parameters: {'observation_period_num': 47, 'train_rates': 0.7081613588575805, 'learning_rate': 1.9577165875126967e-05, 'batch_size': 112, 'step_size': 15, 'gamma': 0.9472216389219053}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:35:04,402][0m Trial 16 finished with value: 0.05650417347147007 and parameters: {'observation_period_num': 48, 'train_rates': 0.8296531532883856, 'learning_rate': 0.00030272126634933174, 'batch_size': 158, 'step_size': 3, 'gamma': 0.9475054633387353}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:35:39,393][0m Trial 17 finished with value: 0.0592739722430706 and parameters: {'observation_period_num': 55, 'train_rates': 0.8276339484037059, 'learning_rate': 0.0003297275292899288, 'batch_size': 166, 'step_size': 13, 'gamma': 0.882996419249362}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:36:56,316][0m Trial 18 finished with value: 0.10010244223930881 and parameters: {'observation_period_num': 90, 'train_rates': 0.8698191724248076, 'learning_rate': 1.4209343006634396e-05, 'batch_size': 73, 'step_size': 8, 'gamma': 0.9528419854357906}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:37:32,630][0m Trial 19 finished with value: 0.07836480908940488 and parameters: {'observation_period_num': 36, 'train_rates': 0.7974029342653898, 'learning_rate': 7.22244990310381e-05, 'batch_size': 164, 'step_size': 10, 'gamma': 0.8912608575502932}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:38:17,596][0m Trial 20 finished with value: 0.10290254564846263 and parameters: {'observation_period_num': 126, 'train_rates': 0.7656136813422477, 'learning_rate': 0.0004304254089663978, 'batch_size': 117, 'step_size': 2, 'gamma': 0.9225758429409479}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:38:52,560][0m Trial 21 finished with value: 0.057982789419591424 and parameters: {'observation_period_num': 63, 'train_rates': 0.8270773099957909, 'learning_rate': 0.00024033625144932857, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8866477873208913}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:39:29,096][0m Trial 22 finished with value: 0.1050278134201598 and parameters: {'observation_period_num': 70, 'train_rates': 0.8539746963540805, 'learning_rate': 0.00017692326629862893, 'batch_size': 164, 'step_size': 12, 'gamma': 0.9314939020115368}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:40:12,544][0m Trial 23 finished with value: 0.0600400244767687 and parameters: {'observation_period_num': 28, 'train_rates': 0.8068292593499917, 'learning_rate': 0.0004934451271842796, 'batch_size': 126, 'step_size': 15, 'gamma': 0.9612012278696938}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:40:45,870][0m Trial 24 finished with value: 0.11467720198631287 and parameters: {'observation_period_num': 67, 'train_rates': 0.7402629488853942, 'learning_rate': 5.814250700955721e-05, 'batch_size': 156, 'step_size': 13, 'gamma': 0.8635833284747848}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:41:41,299][0m Trial 25 finished with value: 0.07921922821224782 and parameters: {'observation_period_num': 111, 'train_rates': 0.8797576949625597, 'learning_rate': 0.0002284617122399794, 'batch_size': 102, 'step_size': 11, 'gamma': 0.8991501055059846}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:42:12,961][0m Trial 26 finished with value: 0.049462983414294226 and parameters: {'observation_period_num': 21, 'train_rates': 0.8282866727915494, 'learning_rate': 0.00028622428857033285, 'batch_size': 178, 'step_size': 9, 'gamma': 0.9676363523936576}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:42:39,137][0m Trial 27 finished with value: 0.05881707101494451 and parameters: {'observation_period_num': 26, 'train_rates': 0.7841168341128677, 'learning_rate': 0.0005585292795569455, 'batch_size': 220, 'step_size': 7, 'gamma': 0.9679224324560562}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:43:06,608][0m Trial 28 finished with value: 0.07429678485847296 and parameters: {'observation_period_num': 22, 'train_rates': 0.6757289833984558, 'learning_rate': 0.00012593049669040694, 'batch_size': 181, 'step_size': 6, 'gamma': 0.9354453660758749}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:43:30,577][0m Trial 29 finished with value: 0.12949780474898484 and parameters: {'observation_period_num': 172, 'train_rates': 0.8423434647307407, 'learning_rate': 0.00030337009849017025, 'batch_size': 252, 'step_size': 9, 'gamma': 0.9679545769955863}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:44:39,181][0m Trial 30 finished with value: 0.08613915384655267 and parameters: {'observation_period_num': 152, 'train_rates': 0.8988132200892658, 'learning_rate': 0.0001675935679925506, 'batch_size': 80, 'step_size': 5, 'gamma': 0.9372152343340138}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:45:12,200][0m Trial 31 finished with value: 0.061939397205909096 and parameters: {'observation_period_num': 56, 'train_rates': 0.8138535079082561, 'learning_rate': 0.00022911514502459688, 'batch_size': 175, 'step_size': 9, 'gamma': 0.8801692216850061}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:45:40,236][0m Trial 32 finished with value: 0.06429435966460943 and parameters: {'observation_period_num': 68, 'train_rates': 0.8374889183041443, 'learning_rate': 0.00013069828766912203, 'batch_size': 205, 'step_size': 11, 'gamma': 0.9099572879548239}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:46:16,849][0m Trial 33 finished with value: 0.06342665347106316 and parameters: {'observation_period_num': 27, 'train_rates': 0.790851762869252, 'learning_rate': 0.0008268349520540375, 'batch_size': 149, 'step_size': 10, 'gamma': 0.9630324207526564}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:47:01,924][0m Trial 34 finished with value: 0.08453963955626644 and parameters: {'observation_period_num': 100, 'train_rates': 0.8819199107695398, 'learning_rate': 5.1660429880487554e-05, 'batch_size': 132, 'step_size': 13, 'gamma': 0.8539396156983834}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:47:32,784][0m Trial 35 finished with value: 0.050117229111492634 and parameters: {'observation_period_num': 18, 'train_rates': 0.760189529964462, 'learning_rate': 0.0006080887726555397, 'batch_size': 176, 'step_size': 8, 'gamma': 0.9273284849936309}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:47:59,182][0m Trial 36 finished with value: 0.05206014834649381 and parameters: {'observation_period_num': 17, 'train_rates': 0.7670708870219342, 'learning_rate': 0.0005887102023816925, 'batch_size': 208, 'step_size': 8, 'gamma': 0.9302095199892308}. Best is trial 12 with value: 0.04826603198455552.[0m
[32m[I 2025-02-04 09:48:23,111][0m Trial 37 finished with value: 0.0482113137946757 and parameters: {'observation_period_num': 17, 'train_rates': 0.7597900888842398, 'learning_rate': 0.0006392742162919346, 'batch_size': 235, 'step_size': 8, 'gamma': 0.9244197894570869}. Best is trial 37 with value: 0.0482113137946757.[0m
[32m[I 2025-02-04 09:48:45,710][0m Trial 38 finished with value: 0.06825589347476492 and parameters: {'observation_period_num': 37, 'train_rates': 0.7267587895009271, 'learning_rate': 0.0009875581315335312, 'batch_size': 246, 'step_size': 7, 'gamma': 0.7962346772507811}. Best is trial 37 with value: 0.0482113137946757.[0m
[32m[I 2025-02-04 09:49:07,494][0m Trial 39 finished with value: 0.1789482926090385 and parameters: {'observation_period_num': 196, 'train_rates': 0.7509510820521516, 'learning_rate': 0.0006321958948782616, 'batch_size': 242, 'step_size': 10, 'gamma': 0.9760164164348489}. Best is trial 37 with value: 0.0482113137946757.[0m
[32m[I 2025-02-04 09:49:32,278][0m Trial 40 finished with value: 0.17485523202037595 and parameters: {'observation_period_num': 222, 'train_rates': 0.6753597634154149, 'learning_rate': 0.00042198179418668954, 'batch_size': 191, 'step_size': 6, 'gamma': 0.9024820927291024}. Best is trial 37 with value: 0.0482113137946757.[0m
[32m[I 2025-02-04 09:49:56,476][0m Trial 41 finished with value: 0.050165282595053054 and parameters: {'observation_period_num': 18, 'train_rates': 0.767177870866796, 'learning_rate': 0.0005351039105082959, 'batch_size': 232, 'step_size': 8, 'gamma': 0.9233869889596702}. Best is trial 37 with value: 0.0482113137946757.[0m
[32m[I 2025-02-04 09:50:20,047][0m Trial 42 finished with value: 0.057376529191023797 and parameters: {'observation_period_num': 16, 'train_rates': 0.7802392129767438, 'learning_rate': 0.0006567397236072716, 'batch_size': 234, 'step_size': 8, 'gamma': 0.9251906092795479}. Best is trial 37 with value: 0.0482113137946757.[0m
[32m[I 2025-02-04 09:50:44,033][0m Trial 43 finished with value: 0.04743277191325894 and parameters: {'observation_period_num': 5, 'train_rates': 0.7631668147621694, 'learning_rate': 0.0004030907218056451, 'batch_size': 237, 'step_size': 8, 'gamma': 0.9562592561903684}. Best is trial 43 with value: 0.04743277191325894.[0m
[32m[I 2025-02-04 09:51:08,388][0m Trial 44 finished with value: 0.050977506562195456 and parameters: {'observation_period_num': 6, 'train_rates': 0.7390737468976805, 'learning_rate': 0.00035118534297262774, 'batch_size': 225, 'step_size': 9, 'gamma': 0.98827547586504}. Best is trial 43 with value: 0.04743277191325894.[0m
[32m[I 2025-02-04 09:51:33,297][0m Trial 45 finished with value: 0.16186297358128063 and parameters: {'observation_period_num': 5, 'train_rates': 0.7099177772018703, 'learning_rate': 3.506677127601362e-06, 'batch_size': 210, 'step_size': 7, 'gamma': 0.9560051685991403}. Best is trial 43 with value: 0.04743277191325894.[0m
[32m[I 2025-02-04 09:52:01,395][0m Trial 46 finished with value: 0.06284978333860636 and parameters: {'observation_period_num': 37, 'train_rates': 0.802562247682343, 'learning_rate': 0.00019945569978839725, 'batch_size': 198, 'step_size': 9, 'gamma': 0.9784253205414788}. Best is trial 43 with value: 0.04743277191325894.[0m
[32m[I 2025-02-04 09:56:52,563][0m Trial 47 finished with value: 0.06030394068102907 and parameters: {'observation_period_num': 32, 'train_rates': 0.7562902139569444, 'learning_rate': 0.0007883992086380507, 'batch_size': 17, 'step_size': 5, 'gamma': 0.9409296326114042}. Best is trial 43 with value: 0.04743277191325894.[0m
[32m[I 2025-02-04 09:57:21,603][0m Trial 48 finished with value: 0.08223743353149679 and parameters: {'observation_period_num': 47, 'train_rates': 0.7221894277460001, 'learning_rate': 9.776236499776228e-05, 'batch_size': 184, 'step_size': 10, 'gamma': 0.9107214783737287}. Best is trial 43 with value: 0.04743277191325894.[0m
[32m[I 2025-02-04 09:57:46,084][0m Trial 49 finished with value: 0.058789491183098466 and parameters: {'observation_period_num': 19, 'train_rates': 0.6950220596233674, 'learning_rate': 0.0002726424133011798, 'batch_size': 217, 'step_size': 6, 'gamma': 0.972973330705214}. Best is trial 43 with value: 0.04743277191325894.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-04 09:57:46,095][0m A new study created in memory with name: no-name-578d12a4-3e12-4bbf-9f45-bcdbde379988[0m
[32m[I 2025-02-04 09:58:51,421][0m Trial 0 finished with value: 0.3241526782512665 and parameters: {'observation_period_num': 231, 'train_rates': 0.9743697739246879, 'learning_rate': 1.6109912702588652e-05, 'batch_size': 88, 'step_size': 4, 'gamma': 0.8085935054065952}. Best is trial 0 with value: 0.3241526782512665.[0m
[32m[I 2025-02-04 09:59:13,417][0m Trial 1 finished with value: 0.16572731216134073 and parameters: {'observation_period_num': 150, 'train_rates': 0.6885418978820378, 'learning_rate': 6.781602701566141e-05, 'batch_size': 240, 'step_size': 11, 'gamma': 0.8484613054537293}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 09:59:44,331][0m Trial 2 finished with value: 0.21163707971572876 and parameters: {'observation_period_num': 214, 'train_rates': 0.9337062619049574, 'learning_rate': 7.106270395875604e-05, 'batch_size': 195, 'step_size': 3, 'gamma': 0.888236351818677}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 10:00:56,007][0m Trial 3 finished with value: 0.48119202504555386 and parameters: {'observation_period_num': 226, 'train_rates': 0.6831663877837076, 'learning_rate': 2.477890624995833e-06, 'batch_size': 63, 'step_size': 9, 'gamma': 0.9725743760020263}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 10:04:19,984][0m Trial 4 finished with value: 0.47654086249331906 and parameters: {'observation_period_num': 143, 'train_rates': 0.7921066031426298, 'learning_rate': 1.0235599099246413e-06, 'batch_size': 24, 'step_size': 10, 'gamma': 0.9081496518526697}. Best is trial 1 with value: 0.16572731216134073.[0m
Early stopping at epoch 80
[32m[I 2025-02-04 10:05:05,961][0m Trial 5 finished with value: 0.9567025372575769 and parameters: {'observation_period_num': 252, 'train_rates': 0.785040351530067, 'learning_rate': 5.0943174716920116e-06, 'batch_size': 86, 'step_size': 2, 'gamma': 0.7661851746886141}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 10:05:46,836][0m Trial 6 finished with value: 0.46266954531103877 and parameters: {'observation_period_num': 231, 'train_rates': 0.8918329951406718, 'learning_rate': 5.871122131678501e-06, 'batch_size': 135, 'step_size': 3, 'gamma': 0.924064900430315}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 10:06:21,036][0m Trial 7 finished with value: 0.7103593199039405 and parameters: {'observation_period_num': 242, 'train_rates': 0.6871265127873936, 'learning_rate': 1.1899249746610283e-06, 'batch_size': 138, 'step_size': 10, 'gamma': 0.9497270468330061}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 10:06:50,948][0m Trial 8 finished with value: 0.6339987516403198 and parameters: {'observation_period_num': 131, 'train_rates': 0.9860832317435048, 'learning_rate': 4.077036179384325e-06, 'batch_size': 230, 'step_size': 4, 'gamma': 0.8533049941463701}. Best is trial 1 with value: 0.16572731216134073.[0m
[32m[I 2025-02-04 10:07:13,982][0m Trial 9 finished with value: 0.08709796692247418 and parameters: {'observation_period_num': 90, 'train_rates': 0.6398369411452303, 'learning_rate': 0.0009520929848865398, 'batch_size': 212, 'step_size': 12, 'gamma': 0.7656510902854159}. Best is trial 9 with value: 0.08709796692247418.[0m
[32m[I 2025-02-04 10:07:39,432][0m Trial 10 finished with value: 0.06676282808762188 and parameters: {'observation_period_num': 27, 'train_rates': 0.6031517576770576, 'learning_rate': 0.0008927730028193881, 'batch_size': 180, 'step_size': 15, 'gamma': 0.7504820203126066}. Best is trial 10 with value: 0.06676282808762188.[0m
[32m[I 2025-02-04 10:08:07,144][0m Trial 11 finished with value: 0.05094848677330876 and parameters: {'observation_period_num': 14, 'train_rates': 0.616466423582384, 'learning_rate': 0.0008776417097458799, 'batch_size': 177, 'step_size': 15, 'gamma': 0.7524451099082826}. Best is trial 11 with value: 0.05094848677330876.[0m
[32m[I 2025-02-04 10:08:34,921][0m Trial 12 finished with value: 0.04780494526174075 and parameters: {'observation_period_num': 7, 'train_rates': 0.617420842990493, 'learning_rate': 0.0009538762341389121, 'batch_size': 172, 'step_size': 15, 'gamma': 0.8039507585335923}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:09:08,262][0m Trial 13 finished with value: 0.05411373275604422 and parameters: {'observation_period_num': 14, 'train_rates': 0.7288832716394111, 'learning_rate': 0.00027716989465535345, 'batch_size': 159, 'step_size': 15, 'gamma': 0.8102357497141562}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:09:36,380][0m Trial 14 finished with value: 0.1022436424161736 and parameters: {'observation_period_num': 54, 'train_rates': 0.6014034607652807, 'learning_rate': 0.00028916214248790736, 'batch_size': 167, 'step_size': 13, 'gamma': 0.8080428679338223}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:10:21,034][0m Trial 15 finished with value: 0.0531811888435943 and parameters: {'observation_period_num': 67, 'train_rates': 0.8450845799723735, 'learning_rate': 0.00032900297014459086, 'batch_size': 123, 'step_size': 7, 'gamma': 0.789847740518727}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:10:47,426][0m Trial 16 finished with value: 0.06646800580985691 and parameters: {'observation_period_num': 5, 'train_rates': 0.7509099053795959, 'learning_rate': 0.00012698789902273295, 'batch_size': 205, 'step_size': 7, 'gamma': 0.8339131334640445}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:11:29,686][0m Trial 17 finished with value: 0.205229781104201 and parameters: {'observation_period_num': 97, 'train_rates': 0.6400325741944544, 'learning_rate': 2.3051517286633826e-05, 'batch_size': 113, 'step_size': 14, 'gamma': 0.7832100423812841}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:11:50,172][0m Trial 18 finished with value: 0.07519447858525898 and parameters: {'observation_period_num': 34, 'train_rates': 0.647394186500215, 'learning_rate': 0.0004574272836638688, 'batch_size': 256, 'step_size': 13, 'gamma': 0.832078062290247}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:12:24,166][0m Trial 19 finished with value: 0.08126457198816223 and parameters: {'observation_period_num': 57, 'train_rates': 0.726021275728113, 'learning_rate': 0.0001302883759413936, 'batch_size': 154, 'step_size': 15, 'gamma': 0.7854440355355231}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:12:54,039][0m Trial 20 finished with value: 0.17499222553845856 and parameters: {'observation_period_num': 171, 'train_rates': 0.8219830587187368, 'learning_rate': 0.000550060467077834, 'batch_size': 181, 'step_size': 12, 'gamma': 0.8775428560494094}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:13:42,585][0m Trial 21 finished with value: 0.05674425247081808 and parameters: {'observation_period_num': 65, 'train_rates': 0.8519075730664415, 'learning_rate': 0.00030589527378819484, 'batch_size': 117, 'step_size': 7, 'gamma': 0.7891538738404981}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:14:25,786][0m Trial 22 finished with value: 0.06413029346595774 and parameters: {'observation_period_num': 91, 'train_rates': 0.8653363346347355, 'learning_rate': 0.00056020125869363, 'batch_size': 136, 'step_size': 7, 'gamma': 0.750032983897225}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:15:28,990][0m Trial 23 finished with value: 0.053651274556493406 and parameters: {'observation_period_num': 41, 'train_rates': 0.9071321522185345, 'learning_rate': 0.00015908857742307316, 'batch_size': 94, 'step_size': 6, 'gamma': 0.7964526932320097}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:16:01,056][0m Trial 24 finished with value: 0.060879644960528466 and parameters: {'observation_period_num': 75, 'train_rates': 0.8290832214876767, 'learning_rate': 0.0008532175276392336, 'batch_size': 182, 'step_size': 9, 'gamma': 0.8288760309941461}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:17:31,430][0m Trial 25 finished with value: 0.08290890059750608 and parameters: {'observation_period_num': 22, 'train_rates': 0.7644715989576858, 'learning_rate': 5.171095380500816e-05, 'batch_size': 56, 'step_size': 5, 'gamma': 0.7700784378771899}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:18:02,927][0m Trial 26 finished with value: 0.05639765537825309 and parameters: {'observation_period_num': 5, 'train_rates': 0.6684839839017732, 'learning_rate': 0.00020193747972286713, 'batch_size': 152, 'step_size': 13, 'gamma': 0.8165905950186018}. Best is trial 12 with value: 0.04780494526174075.[0m
Early stopping at epoch 57
[32m[I 2025-02-04 10:18:17,814][0m Trial 27 finished with value: 0.17234221578596265 and parameters: {'observation_period_num': 45, 'train_rates': 0.7202782532790142, 'learning_rate': 0.00045551463864164447, 'batch_size': 217, 'step_size': 1, 'gamma': 0.7735592076617536}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:18:57,883][0m Trial 28 finished with value: 0.16830272778434116 and parameters: {'observation_period_num': 110, 'train_rates': 0.6260355065650766, 'learning_rate': 0.0006376445057612431, 'batch_size': 118, 'step_size': 14, 'gamma': 0.8573068605680874}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:19:58,485][0m Trial 29 finished with value: 0.18802100846815348 and parameters: {'observation_period_num': 75, 'train_rates': 0.9309532784288507, 'learning_rate': 1.342395032253606e-05, 'batch_size': 96, 'step_size': 8, 'gamma': 0.8034323561575681}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:20:28,193][0m Trial 30 finished with value: 0.06668898604802627 and parameters: {'observation_period_num': 37, 'train_rates': 0.6644686820689059, 'learning_rate': 0.0003153534114904278, 'batch_size': 170, 'step_size': 11, 'gamma': 0.750166509856106}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:21:35,821][0m Trial 31 finished with value: 0.05334521546789902 and parameters: {'observation_period_num': 43, 'train_rates': 0.9098833899999172, 'learning_rate': 0.00015616911537905966, 'batch_size': 86, 'step_size': 5, 'gamma': 0.7965186478662929}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:23:11,795][0m Trial 32 finished with value: 0.04789837568496484 and parameters: {'observation_period_num': 20, 'train_rates': 0.8683527497730089, 'learning_rate': 0.000111491291017138, 'batch_size': 58, 'step_size': 5, 'gamma': 0.8210027213755555}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:25:53,810][0m Trial 33 finished with value: 0.04826283890331326 and parameters: {'observation_period_num': 19, 'train_rates': 0.8710896387860947, 'learning_rate': 7.727686065258004e-05, 'batch_size': 34, 'step_size': 5, 'gamma': 0.8152358366497279}. Best is trial 12 with value: 0.04780494526174075.[0m
[32m[I 2025-02-04 10:30:30,051][0m Trial 34 finished with value: 0.03836428881856982 and parameters: {'observation_period_num': 17, 'train_rates': 0.8781402956295896, 'learning_rate': 7.328833260400108e-05, 'batch_size': 20, 'step_size': 5, 'gamma': 0.8252258466622167}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:34:08,477][0m Trial 35 finished with value: 0.08746880469237558 and parameters: {'observation_period_num': 183, 'train_rates': 0.8780555379938857, 'learning_rate': 7.889625251049478e-05, 'batch_size': 24, 'step_size': 5, 'gamma': 0.8219309865677428}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:36:29,374][0m Trial 36 finished with value: 0.07049620691829055 and parameters: {'observation_period_num': 24, 'train_rates': 0.9555621833364009, 'learning_rate': 3.973100074726102e-05, 'batch_size': 42, 'step_size': 3, 'gamma': 0.8400601486537815}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:38:57,550][0m Trial 37 finished with value: 0.05115219662658965 and parameters: {'observation_period_num': 19, 'train_rates': 0.8123510361912117, 'learning_rate': 9.067050971588033e-05, 'batch_size': 36, 'step_size': 4, 'gamma': 0.8482458108722553}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:40:25,289][0m Trial 38 finished with value: 0.11270562375293058 and parameters: {'observation_period_num': 55, 'train_rates': 0.9413976829008353, 'learning_rate': 1.598424804534696e-05, 'batch_size': 67, 'step_size': 6, 'gamma': 0.8705663433987587}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:45:23,085][0m Trial 39 finished with value: 0.05021569624987264 and parameters: {'observation_period_num': 7, 'train_rates': 0.8975810919627216, 'learning_rate': 3.309433609859442e-05, 'batch_size': 19, 'step_size': 2, 'gamma': 0.9072505987771599}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:47:16,973][0m Trial 40 finished with value: 0.11541130257707634 and parameters: {'observation_period_num': 30, 'train_rates': 0.8782211852059081, 'learning_rate': 1.0271091000156484e-05, 'batch_size': 49, 'step_size': 4, 'gamma': 0.8636426807644331}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:52:45,997][0m Trial 41 finished with value: 0.04594918930830678 and parameters: {'observation_period_num': 8, 'train_rates': 0.9039197687770529, 'learning_rate': 2.9510433704662845e-05, 'batch_size': 17, 'step_size': 2, 'gamma': 0.9030750457601179}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:54:11,980][0m Trial 42 finished with value: 0.10740756165382644 and parameters: {'observation_period_num': 23, 'train_rates': 0.922135305504715, 'learning_rate': 4.9479901063557744e-05, 'batch_size': 69, 'step_size': 1, 'gamma': 0.894630637341839}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 10:56:36,668][0m Trial 43 finished with value: 0.09788703289411473 and parameters: {'observation_period_num': 47, 'train_rates': 0.8012897519269913, 'learning_rate': 2.1674503020926486e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9321879443124544}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 11:02:42,657][0m Trial 44 finished with value: 0.04009254085628883 and parameters: {'observation_period_num': 13, 'train_rates': 0.9610568060621398, 'learning_rate': 9.973229015789379e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9578875209837276}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 11:08:02,473][0m Trial 45 finished with value: 0.045620347324170564 and parameters: {'observation_period_num': 32, 'train_rates': 0.9416695433719356, 'learning_rate': 5.8250559962176914e-05, 'batch_size': 18, 'step_size': 3, 'gamma': 0.9131370203410778}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 11:12:17,572][0m Trial 46 finished with value: 0.07705754912066683 and parameters: {'observation_period_num': 34, 'train_rates': 0.9634946937981717, 'learning_rate': 7.336011767948059e-06, 'batch_size': 23, 'step_size': 3, 'gamma': 0.9580712508071609}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 11:18:08,223][0m Trial 47 finished with value: 0.3813782164028713 and parameters: {'observation_period_num': 207, 'train_rates': 0.9898614653360888, 'learning_rate': 2.475787785391205e-05, 'batch_size': 16, 'step_size': 2, 'gamma': 0.9886476766492575}. Best is trial 34 with value: 0.03836428881856982.[0m
[32m[I 2025-02-04 11:21:32,034][0m Trial 48 finished with value: 0.037682291564365096 and parameters: {'observation_period_num': 6, 'train_rates': 0.9489351382157483, 'learning_rate': 5.7346492947444206e-05, 'batch_size': 29, 'step_size': 3, 'gamma': 0.9310518392639932}. Best is trial 48 with value: 0.037682291564365096.[0m
[32m[I 2025-02-04 11:25:10,391][0m Trial 49 finished with value: 0.03330161260331378 and parameters: {'observation_period_num': 10, 'train_rates': 0.9711858807223085, 'learning_rate': 6.0109671044572594e-05, 'batch_size': 27, 'step_size': 3, 'gamma': 0.9350676439172101}. Best is trial 49 with value: 0.03330161260331378.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-04 11:25:10,402][0m A new study created in memory with name: no-name-6f73e925-1801-4a32-98d1-8df9f870749c[0m
[32m[I 2025-02-04 11:25:31,868][0m Trial 0 finished with value: 0.0575855622328523 and parameters: {'observation_period_num': 21, 'train_rates': 0.612387132253388, 'learning_rate': 0.0006092848258480396, 'batch_size': 243, 'step_size': 15, 'gamma': 0.9176815516786232}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:26:17,694][0m Trial 1 finished with value: 0.08171963349469309 and parameters: {'observation_period_num': 78, 'train_rates': 0.7005671617500606, 'learning_rate': 0.00036109040032593446, 'batch_size': 107, 'step_size': 3, 'gamma': 0.8556818020497676}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:26:41,525][0m Trial 2 finished with value: 0.19583927318184816 and parameters: {'observation_period_num': 24, 'train_rates': 0.8109022853738002, 'learning_rate': 7.540926906449932e-06, 'batch_size': 239, 'step_size': 6, 'gamma': 0.9807866294575807}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:28:54,965][0m Trial 3 finished with value: 0.1700884637359394 and parameters: {'observation_period_num': 236, 'train_rates': 0.7678338215872554, 'learning_rate': 0.00010877985020756888, 'batch_size': 35, 'step_size': 1, 'gamma': 0.95475267475209}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:30:05,705][0m Trial 4 finished with value: 0.07248404692743242 and parameters: {'observation_period_num': 45, 'train_rates': 0.833283456496235, 'learning_rate': 3.1269725227874995e-05, 'batch_size': 78, 'step_size': 8, 'gamma': 0.8730322837826936}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:30:39,975][0m Trial 5 finished with value: 0.4386300659456918 and parameters: {'observation_period_num': 129, 'train_rates': 0.6804381503174471, 'learning_rate': 3.813420600345866e-05, 'batch_size': 147, 'step_size': 1, 'gamma': 0.8928096999841777}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:31:12,974][0m Trial 6 finished with value: 0.24864103410210103 and parameters: {'observation_period_num': 33, 'train_rates': 0.7349458067265346, 'learning_rate': 8.35716805136868e-06, 'batch_size': 155, 'step_size': 6, 'gamma': 0.9525065121308135}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:32:12,417][0m Trial 7 finished with value: 0.6151394724845887 and parameters: {'observation_period_num': 111, 'train_rates': 0.9422350085155853, 'learning_rate': 1.5347051540702433e-06, 'batch_size': 99, 'step_size': 5, 'gamma': 0.819419531572001}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:32:51,080][0m Trial 8 finished with value: 0.05763669961324617 and parameters: {'observation_period_num': 11, 'train_rates': 0.8174954885233173, 'learning_rate': 4.533180385187824e-05, 'batch_size': 144, 'step_size': 4, 'gamma': 0.9777435569661105}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:33:18,584][0m Trial 9 finished with value: 0.1551047935422543 and parameters: {'observation_period_num': 164, 'train_rates': 0.6759474755626625, 'learning_rate': 6.389317128631072e-05, 'batch_size': 180, 'step_size': 7, 'gamma': 0.9400312569228655}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:33:37,577][0m Trial 10 finished with value: 0.17712250987578626 and parameters: {'observation_period_num': 204, 'train_rates': 0.6059220599268214, 'learning_rate': 0.0009359828489126816, 'batch_size': 238, 'step_size': 15, 'gamma': 0.7610385969659739}. Best is trial 0 with value: 0.0575855622328523.[0m
[32m[I 2025-02-04 11:34:06,768][0m Trial 11 finished with value: 0.04570989668167926 and parameters: {'observation_period_num': 8, 'train_rates': 0.8995770111909159, 'learning_rate': 0.00024004195417231473, 'batch_size': 212, 'step_size': 12, 'gamma': 0.9160235540608757}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:34:38,083][0m Trial 12 finished with value: 0.07304296730005223 and parameters: {'observation_period_num': 76, 'train_rates': 0.9202849953267201, 'learning_rate': 0.0003262466253084699, 'batch_size': 197, 'step_size': 14, 'gamma': 0.9054402418613947}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:35:01,421][0m Trial 13 finished with value: 0.1292856458822886 and parameters: {'observation_period_num': 72, 'train_rates': 0.8700943737526936, 'learning_rate': 0.0009830201277273027, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9138154064468024}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:35:34,743][0m Trial 14 finished with value: 0.04687267541885376 and parameters: {'observation_period_num': 5, 'train_rates': 0.982327639624526, 'learning_rate': 0.00025156063377588476, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8357852100227179}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:36:07,487][0m Trial 15 finished with value: 0.0746658593416214 and parameters: {'observation_period_num': 59, 'train_rates': 0.9766400773684862, 'learning_rate': 0.00015574517892505432, 'batch_size': 201, 'step_size': 11, 'gamma': 0.8236443603353837}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:36:36,977][0m Trial 16 finished with value: 0.09536156666121985 and parameters: {'observation_period_num': 119, 'train_rates': 0.8929840171963471, 'learning_rate': 0.00018516285874994645, 'batch_size': 204, 'step_size': 11, 'gamma': 0.825658296676664}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:37:11,711][0m Trial 17 finished with value: 0.1038934588432312 and parameters: {'observation_period_num': 169, 'train_rates': 0.9854859732512317, 'learning_rate': 0.00030846254523009476, 'batch_size': 175, 'step_size': 13, 'gamma': 0.7800391143792711}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:37:38,607][0m Trial 18 finished with value: 0.08806484536049648 and parameters: {'observation_period_num': 11, 'train_rates': 0.8651025398575565, 'learning_rate': 1.8561315012117175e-05, 'batch_size': 219, 'step_size': 9, 'gamma': 0.8516772908359296}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:38:13,985][0m Trial 19 finished with value: 0.10115053543465675 and parameters: {'observation_period_num': 103, 'train_rates': 0.9393698674926564, 'learning_rate': 9.795179897374537e-05, 'batch_size': 172, 'step_size': 12, 'gamma': 0.7953312650528969}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:39:06,035][0m Trial 20 finished with value: 0.6546688990390047 and parameters: {'observation_period_num': 51, 'train_rates': 0.9190638044379313, 'learning_rate': 1.1150194966038925e-06, 'batch_size': 121, 'step_size': 9, 'gamma': 0.8794066607712556}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:39:29,365][0m Trial 21 finished with value: 0.052053614786347825 and parameters: {'observation_period_num': 6, 'train_rates': 0.623228935561839, 'learning_rate': 0.0005039149238379113, 'batch_size': 227, 'step_size': 15, 'gamma': 0.9306333026324458}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:39:58,420][0m Trial 22 finished with value: 0.052139922976493835 and parameters: {'observation_period_num': 37, 'train_rates': 0.9634811363013523, 'learning_rate': 0.0005167859420416396, 'batch_size': 217, 'step_size': 13, 'gamma': 0.9250996333512088}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:40:25,319][0m Trial 23 finished with value: 0.05350531399000986 and parameters: {'observation_period_num': 7, 'train_rates': 0.7564471476926707, 'learning_rate': 0.00018606747107828846, 'batch_size': 218, 'step_size': 13, 'gamma': 0.8439032290009252}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:40:50,788][0m Trial 24 finished with value: 0.09095655039829366 and parameters: {'observation_period_num': 88, 'train_rates': 0.6441789986756583, 'learning_rate': 0.0002547285486455404, 'batch_size': 188, 'step_size': 10, 'gamma': 0.8901256142152749}. Best is trial 11 with value: 0.04570989668167926.[0m
[32m[I 2025-02-04 11:41:17,858][0m Trial 25 finished with value: 0.045100940834498573 and parameters: {'observation_period_num': 6, 'train_rates': 0.8533579590423007, 'learning_rate': 0.0005805828134514103, 'batch_size': 224, 'step_size': 14, 'gamma': 0.9358301462330197}. Best is trial 25 with value: 0.045100940834498573.[0m
[32m[I 2025-02-04 11:41:41,526][0m Trial 26 finished with value: 0.07223370301360547 and parameters: {'observation_period_num': 54, 'train_rates': 0.8527555438138342, 'learning_rate': 0.00010327958784462683, 'batch_size': 253, 'step_size': 12, 'gamma': 0.9586551203342327}. Best is trial 25 with value: 0.045100940834498573.[0m
[32m[I 2025-02-04 11:42:18,095][0m Trial 27 finished with value: 0.14731373004742448 and parameters: {'observation_period_num': 145, 'train_rates': 0.8958399120815912, 'learning_rate': 0.0006584238857579698, 'batch_size': 163, 'step_size': 10, 'gamma': 0.8966885876667076}. Best is trial 25 with value: 0.045100940834498573.[0m
[32m[I 2025-02-04 11:42:47,046][0m Trial 28 finished with value: 0.18212555786245105 and parameters: {'observation_period_num': 32, 'train_rates': 0.8998359454717488, 'learning_rate': 1.3107208954434049e-05, 'batch_size': 206, 'step_size': 14, 'gamma': 0.8032045770919611}. Best is trial 25 with value: 0.045100940834498573.[0m
[32m[I 2025-02-04 11:43:14,368][0m Trial 29 finished with value: 0.4344101846218109 and parameters: {'observation_period_num': 28, 'train_rates': 0.9508416473571694, 'learning_rate': 3.953582847317587e-06, 'batch_size': 234, 'step_size': 12, 'gamma': 0.9186057692090241}. Best is trial 25 with value: 0.045100940834498573.[0m
[32m[I 2025-02-04 11:44:50,065][0m Trial 30 finished with value: 0.08669754240346553 and parameters: {'observation_period_num': 61, 'train_rates': 0.7795855696327345, 'learning_rate': 0.0006002509461306961, 'batch_size': 53, 'step_size': 15, 'gamma': 0.8660799706587963}. Best is trial 25 with value: 0.045100940834498573.[0m
[32m[I 2025-02-04 11:45:17,294][0m Trial 31 finished with value: 0.039040490365309884 and parameters: {'observation_period_num': 8, 'train_rates': 0.834717332841017, 'learning_rate': 0.0004702380054729491, 'batch_size': 225, 'step_size': 14, 'gamma': 0.9374122065761546}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:45:45,800][0m Trial 32 finished with value: 0.04360180948818222 and parameters: {'observation_period_num': 23, 'train_rates': 0.8512318503699454, 'learning_rate': 0.0003610505744946854, 'batch_size': 213, 'step_size': 14, 'gamma': 0.9435064730555909}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:46:10,495][0m Trial 33 finished with value: 0.05389333125638861 and parameters: {'observation_period_num': 24, 'train_rates': 0.839169520664954, 'learning_rate': 0.000423651935504889, 'batch_size': 244, 'step_size': 14, 'gamma': 0.9419494184592688}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:46:41,015][0m Trial 34 finished with value: 0.05426544433130937 and parameters: {'observation_period_num': 25, 'train_rates': 0.8026502118874109, 'learning_rate': 0.0008042322501642677, 'batch_size': 187, 'step_size': 14, 'gamma': 0.9690374369423554}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:47:08,675][0m Trial 35 finished with value: 0.06044493286319859 and parameters: {'observation_period_num': 40, 'train_rates': 0.8759504149477696, 'learning_rate': 0.00042116319879805566, 'batch_size': 228, 'step_size': 13, 'gamma': 0.9363908659976903}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:47:49,094][0m Trial 36 finished with value: 0.17811286280248884 and parameters: {'observation_period_num': 247, 'train_rates': 0.8309649200260134, 'learning_rate': 0.00013484313756026398, 'batch_size': 130, 'step_size': 15, 'gamma': 0.9842812123785442}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:48:12,890][0m Trial 37 finished with value: 0.07316565771874314 and parameters: {'observation_period_num': 18, 'train_rates': 0.7824996856699955, 'learning_rate': 6.899017466793742e-05, 'batch_size': 245, 'step_size': 14, 'gamma': 0.9553347460475213}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:48:37,044][0m Trial 38 finished with value: 0.09424019466652889 and parameters: {'observation_period_num': 93, 'train_rates': 0.7382134873670281, 'learning_rate': 0.00021661809156410376, 'batch_size': 215, 'step_size': 12, 'gamma': 0.907315108142539}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:49:07,258][0m Trial 39 finished with value: 0.056548384914943274 and parameters: {'observation_period_num': 47, 'train_rates': 0.8148308178412385, 'learning_rate': 0.0003519718531031444, 'batch_size': 194, 'step_size': 13, 'gamma': 0.9714235767595274}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:50:07,154][0m Trial 40 finished with value: 0.046026724241987 and parameters: {'observation_period_num': 19, 'train_rates': 0.8380276536094546, 'learning_rate': 0.0006408800106060152, 'batch_size': 94, 'step_size': 15, 'gamma': 0.9413328504475882}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:51:24,080][0m Trial 41 finished with value: 0.04453928990671966 and parameters: {'observation_period_num': 19, 'train_rates': 0.8456325615773428, 'learning_rate': 0.0006531920845770573, 'batch_size': 72, 'step_size': 15, 'gamma': 0.9520147326029625}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:56:37,649][0m Trial 42 finished with value: 0.04907907645694667 and parameters: {'observation_period_num': 40, 'train_rates': 0.8540237390870554, 'learning_rate': 0.0008057745160590765, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9617561742352755}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:57:35,877][0m Trial 43 finished with value: 0.04554301049364241 and parameters: {'observation_period_num': 17, 'train_rates': 0.7930791245379389, 'learning_rate': 0.00040236389118234643, 'batch_size': 92, 'step_size': 15, 'gamma': 0.9463496076609299}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 11:58:44,082][0m Trial 44 finished with value: 0.052131790295243265 and parameters: {'observation_period_num': 20, 'train_rates': 0.7956565570073346, 'learning_rate': 0.0004136319129262234, 'batch_size': 79, 'step_size': 15, 'gamma': 0.9492657470351259}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 12:00:18,398][0m Trial 45 finished with value: 0.06329799667000771 and parameters: {'observation_period_num': 73, 'train_rates': 0.817663544906621, 'learning_rate': 0.0006841057048819953, 'batch_size': 55, 'step_size': 14, 'gamma': 0.9886784401518042}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 12:01:28,022][0m Trial 46 finished with value: 0.068198063295253 and parameters: {'observation_period_num': 62, 'train_rates': 0.7632957108507297, 'learning_rate': 0.0002736269332514295, 'batch_size': 72, 'step_size': 2, 'gamma': 0.924509528755654}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 12:02:16,681][0m Trial 47 finished with value: 0.2995197333126183 and parameters: {'observation_period_num': 214, 'train_rates': 0.7272361505122533, 'learning_rate': 0.0009624318042569878, 'batch_size': 99, 'step_size': 15, 'gamma': 0.9495125653272999}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 12:03:05,350][0m Trial 48 finished with value: 0.05573667851941926 and parameters: {'observation_period_num': 39, 'train_rates': 0.8511424860418735, 'learning_rate': 0.0005134005745063398, 'batch_size': 115, 'step_size': 6, 'gamma': 0.9678930648587818}. Best is trial 31 with value: 0.039040490365309884.[0m
[32m[I 2025-02-04 12:04:39,523][0m Trial 49 finished with value: 0.035435929366697864 and parameters: {'observation_period_num': 16, 'train_rates': 0.8774208371661724, 'learning_rate': 0.0001334945404965518, 'batch_size': 59, 'step_size': 13, 'gamma': 0.9761780892187532}. Best is trial 49 with value: 0.035435929366697864.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-04 12:04:39,533][0m A new study created in memory with name: no-name-526475e4-37e0-4d5a-abfa-a299e39c8bba[0m
[32m[I 2025-02-04 12:05:45,692][0m Trial 0 finished with value: 0.25702400909724576 and parameters: {'observation_period_num': 149, 'train_rates': 0.617969739825571, 'learning_rate': 0.0009626913001390614, 'batch_size': 65, 'step_size': 12, 'gamma': 0.9865942101883542}. Best is trial 0 with value: 0.25702400909724576.[0m
[32m[I 2025-02-04 12:06:25,993][0m Trial 1 finished with value: 0.05771207133164773 and parameters: {'observation_period_num': 12, 'train_rates': 0.8897678697470613, 'learning_rate': 6.0914162433191226e-05, 'batch_size': 153, 'step_size': 8, 'gamma': 0.8528462375649528}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:08:06,661][0m Trial 2 finished with value: 0.0759571275450814 and parameters: {'observation_period_num': 105, 'train_rates': 0.9501711704460678, 'learning_rate': 7.606994941717626e-05, 'batch_size': 57, 'step_size': 11, 'gamma': 0.7702528974843998}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:09:07,447][0m Trial 3 finished with value: 1.0591007088670636 and parameters: {'observation_period_num': 100, 'train_rates': 0.823028676218401, 'learning_rate': 1.7283635602340248e-06, 'batch_size': 89, 'step_size': 5, 'gamma': 0.7668701969298201}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:10:30,177][0m Trial 4 finished with value: 0.15663698367188672 and parameters: {'observation_period_num': 145, 'train_rates': 0.6784605080592798, 'learning_rate': 5.025881923924922e-05, 'batch_size': 55, 'step_size': 10, 'gamma': 0.844598792208867}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:10:55,971][0m Trial 5 finished with value: 0.1333158877509046 and parameters: {'observation_period_num': 127, 'train_rates': 0.8151709242255438, 'learning_rate': 6.904742507142014e-05, 'batch_size': 212, 'step_size': 12, 'gamma': 0.7880768211251811}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:13:05,735][0m Trial 6 finished with value: 0.1946548826713613 and parameters: {'observation_period_num': 132, 'train_rates': 0.7350270456564248, 'learning_rate': 0.0003244308095125899, 'batch_size': 36, 'step_size': 13, 'gamma': 0.7539290557902915}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:13:28,995][0m Trial 7 finished with value: 0.8890955654593805 and parameters: {'observation_period_num': 221, 'train_rates': 0.859405069050384, 'learning_rate': 1.7494168132327795e-06, 'batch_size': 241, 'step_size': 1, 'gamma': 0.9641050951103598}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:14:05,909][0m Trial 8 finished with value: 0.5355879607209963 and parameters: {'observation_period_num': 69, 'train_rates': 0.6458860077484615, 'learning_rate': 1.396951560537959e-06, 'batch_size': 128, 'step_size': 14, 'gamma': 0.9495985561915011}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:14:48,553][0m Trial 9 finished with value: 0.5099552886039724 and parameters: {'observation_period_num': 174, 'train_rates': 0.8480979436787518, 'learning_rate': 3.887887180344923e-06, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8400930921060825}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:15:23,319][0m Trial 10 finished with value: 0.18351300060749054 and parameters: {'observation_period_num': 17, 'train_rates': 0.9679713173167428, 'learning_rate': 1.0616225830429524e-05, 'batch_size': 186, 'step_size': 5, 'gamma': 0.9064815687012366}. Best is trial 1 with value: 0.05771207133164773.[0m
[32m[I 2025-02-04 12:16:24,890][0m Trial 11 finished with value: 0.04350125417113304 and parameters: {'observation_period_num': 27, 'train_rates': 0.966893777002767, 'learning_rate': 0.0001637518335034871, 'batch_size': 98, 'step_size': 8, 'gamma': 0.811352587192451}. Best is trial 11 with value: 0.04350125417113304.[0m
[32m[I 2025-02-04 12:17:01,154][0m Trial 12 finished with value: 0.04056824318564402 and parameters: {'observation_period_num': 7, 'train_rates': 0.9039648771829234, 'learning_rate': 0.00021843566331411924, 'batch_size': 167, 'step_size': 7, 'gamma': 0.8181519193874119}. Best is trial 12 with value: 0.04056824318564402.[0m
[32m[I 2025-02-04 12:18:04,101][0m Trial 13 finished with value: 0.05407871248162523 and parameters: {'observation_period_num': 49, 'train_rates': 0.915808007497468, 'learning_rate': 0.0002192427013455126, 'batch_size': 93, 'step_size': 5, 'gamma': 0.8055502968184693}. Best is trial 12 with value: 0.04056824318564402.[0m
[32m[I 2025-02-04 12:18:41,258][0m Trial 14 finished with value: 0.07318975031375885 and parameters: {'observation_period_num': 56, 'train_rates': 0.9705440366256614, 'learning_rate': 0.00023114784454141194, 'batch_size': 172, 'step_size': 7, 'gamma': 0.8102284925463538}. Best is trial 12 with value: 0.04056824318564402.[0m
[32m[I 2025-02-04 12:19:27,822][0m Trial 15 finished with value: 0.05772212345121253 and parameters: {'observation_period_num': 33, 'train_rates': 0.7597781461248262, 'learning_rate': 0.0007990323319070767, 'batch_size': 112, 'step_size': 2, 'gamma': 0.8816631020006402}. Best is trial 12 with value: 0.04056824318564402.[0m
[32m[I 2025-02-04 12:20:00,104][0m Trial 16 finished with value: 0.23613070143208958 and parameters: {'observation_period_num': 80, 'train_rates': 0.9163642021039917, 'learning_rate': 1.5612050024413914e-05, 'batch_size': 201, 'step_size': 10, 'gamma': 0.8185902779983627}. Best is trial 12 with value: 0.04056824318564402.[0m
[32m[I 2025-02-04 12:25:51,414][0m Trial 17 finished with value: 0.03482095224828255 and parameters: {'observation_period_num': 8, 'train_rates': 0.9167484077949692, 'learning_rate': 0.0003750177472101249, 'batch_size': 16, 'step_size': 3, 'gamma': 0.8897810919336879}. Best is trial 17 with value: 0.03482095224828255.[0m
[32m[I 2025-02-04 12:26:15,068][0m Trial 18 finished with value: 0.052464254545925945 and parameters: {'observation_period_num': 6, 'train_rates': 0.7805584428027041, 'learning_rate': 0.00044767542428822703, 'batch_size': 253, 'step_size': 3, 'gamma': 0.9074951628583493}. Best is trial 17 with value: 0.03482095224828255.[0m
[32m[I 2025-02-04 12:26:52,487][0m Trial 19 finished with value: 0.2875679345589685 and parameters: {'observation_period_num': 45, 'train_rates': 0.8823520535181534, 'learning_rate': 1.823094801078007e-05, 'batch_size': 152, 'step_size': 3, 'gamma': 0.8853162555537}. Best is trial 17 with value: 0.03482095224828255.[0m
[32m[I 2025-02-04 12:27:19,775][0m Trial 20 finished with value: 0.176044762134552 and parameters: {'observation_period_num': 246, 'train_rates': 0.9187880172687168, 'learning_rate': 0.00012063519336646536, 'batch_size': 222, 'step_size': 6, 'gamma': 0.9236218522371944}. Best is trial 17 with value: 0.03482095224828255.[0m
[32m[I 2025-02-04 12:30:55,569][0m Trial 21 finished with value: 0.028354151546955107 and parameters: {'observation_period_num': 23, 'train_rates': 0.9883705645687898, 'learning_rate': 0.0001398215778848956, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8633846199287517}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:36:33,004][0m Trial 22 finished with value: 0.03759181189040343 and parameters: {'observation_period_num': 33, 'train_rates': 0.9383435894678935, 'learning_rate': 0.00041713245108560546, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8605218714777362}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:42:37,222][0m Trial 23 finished with value: 0.05636461633582448 and parameters: {'observation_period_num': 78, 'train_rates': 0.9852524084808136, 'learning_rate': 0.0004966985116620272, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8625199532044475}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:48:32,410][0m Trial 24 finished with value: 0.042069783241584384 and parameters: {'observation_period_num': 33, 'train_rates': 0.9406221190608148, 'learning_rate': 0.0004936715827735574, 'batch_size': 16, 'step_size': 10, 'gamma': 0.8771410446209947}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:51:16,093][0m Trial 25 finished with value: 0.06924286564140755 and parameters: {'observation_period_num': 59, 'train_rates': 0.9322032045755161, 'learning_rate': 0.00011342678969635514, 'batch_size': 35, 'step_size': 9, 'gamma': 0.902437707767604}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:53:31,545][0m Trial 26 finished with value: 0.0776494176060669 and parameters: {'observation_period_num': 90, 'train_rates': 0.8676713952895412, 'learning_rate': 2.738328564000861e-05, 'batch_size': 41, 'step_size': 4, 'gamma': 0.9331869059366038}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:54:49,027][0m Trial 27 finished with value: 0.03214193508028984 and parameters: {'observation_period_num': 41, 'train_rates': 0.9894779395310679, 'learning_rate': 0.0006532310088502953, 'batch_size': 79, 'step_size': 7, 'gamma': 0.837471220897874}. Best is trial 21 with value: 0.028354151546955107.[0m
Early stopping at epoch 75
[32m[I 2025-02-04 12:55:43,368][0m Trial 28 finished with value: 0.19752396643161774 and parameters: {'observation_period_num': 177, 'train_rates': 0.9834097124351895, 'learning_rate': 0.0007082308310520474, 'batch_size': 81, 'step_size': 1, 'gamma': 0.8390905084787643}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:57:15,263][0m Trial 29 finished with value: 0.05125008895993233 and parameters: {'observation_period_num': 50, 'train_rates': 0.9882177928489794, 'learning_rate': 0.000884641535071106, 'batch_size': 66, 'step_size': 6, 'gamma': 0.9869135446181522}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 12:58:40,366][0m Trial 30 finished with value: 0.06196107997976501 and parameters: {'observation_period_num': 25, 'train_rates': 0.9507284708460846, 'learning_rate': 0.00011831127306106147, 'batch_size': 70, 'step_size': 3, 'gamma': 0.8305065941279424}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:02:08,735][0m Trial 31 finished with value: 0.04803164407610893 and parameters: {'observation_period_num': 32, 'train_rates': 0.9402938283889064, 'learning_rate': 0.000359024839673027, 'batch_size': 28, 'step_size': 12, 'gamma': 0.8624154365110541}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:04:03,835][0m Trial 32 finished with value: 0.035349190660556896 and parameters: {'observation_period_num': 5, 'train_rates': 0.8903427929605405, 'learning_rate': 0.000977089926761446, 'batch_size': 49, 'step_size': 9, 'gamma': 0.8947178444746038}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:06:10,902][0m Trial 33 finished with value: 0.03839134328640424 and parameters: {'observation_period_num': 6, 'train_rates': 0.8899923710517148, 'learning_rate': 0.0009736467109073022, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8947708538738648}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:07:58,174][0m Trial 34 finished with value: 0.04381203651428223 and parameters: {'observation_period_num': 18, 'train_rates': 0.9595986511437179, 'learning_rate': 0.0006199868594541777, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9269536764561596}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:09:12,995][0m Trial 35 finished with value: 0.05356782575019235 and parameters: {'observation_period_num': 43, 'train_rates': 0.8411287191092994, 'learning_rate': 0.00028225698580745605, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8745022155520671}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:10:58,868][0m Trial 36 finished with value: 0.07188933777312438 and parameters: {'observation_period_num': 106, 'train_rates': 0.8906477329433138, 'learning_rate': 5.023504019904405e-05, 'batch_size': 52, 'step_size': 11, 'gamma': 0.8497570865563111}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:14:01,319][0m Trial 37 finished with value: 0.06920501070409796 and parameters: {'observation_period_num': 65, 'train_rates': 0.8103680705405768, 'learning_rate': 0.00018182900470262482, 'batch_size': 28, 'step_size': 6, 'gamma': 0.7909656893872641}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:15:42,293][0m Trial 38 finished with value: 0.05913646564209886 and parameters: {'observation_period_num': 20, 'train_rates': 0.7321377358320906, 'learning_rate': 0.0006077599138089272, 'batch_size': 49, 'step_size': 9, 'gamma': 0.8901605170368815}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:16:31,437][0m Trial 39 finished with value: 0.07179700864336422 and parameters: {'observation_period_num': 40, 'train_rates': 0.8700620246596631, 'learning_rate': 0.00035230132864684196, 'batch_size': 116, 'step_size': 11, 'gamma': 0.9180573372366594}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:19:35,250][0m Trial 40 finished with value: 0.03887486726873451 and parameters: {'observation_period_num': 5, 'train_rates': 0.9056199214113044, 'learning_rate': 0.000989733757143149, 'batch_size': 31, 'step_size': 4, 'gamma': 0.9387080200295739}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:23:55,012][0m Trial 41 finished with value: 0.0463225774889447 and parameters: {'observation_period_num': 21, 'train_rates': 0.9339750103288932, 'learning_rate': 0.00040644952720568213, 'batch_size': 22, 'step_size': 9, 'gamma': 0.8669096572733673}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:26:17,056][0m Trial 42 finished with value: 0.059363911996650344 and parameters: {'observation_period_num': 37, 'train_rates': 0.953599954806969, 'learning_rate': 0.0006084360069956933, 'batch_size': 41, 'step_size': 8, 'gamma': 0.8502344583263817}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:28:03,746][0m Trial 43 finished with value: 0.029925599250765073 and parameters: {'observation_period_num': 16, 'train_rates': 0.971664852895025, 'learning_rate': 0.00032026408487563165, 'batch_size': 58, 'step_size': 10, 'gamma': 0.8306785707915603}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:29:20,054][0m Trial 44 finished with value: 0.03509157160182058 and parameters: {'observation_period_num': 17, 'train_rates': 0.9725057555390807, 'learning_rate': 0.0002962282486833548, 'batch_size': 80, 'step_size': 13, 'gamma': 0.8310401464973161}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:30:29,846][0m Trial 45 finished with value: 0.15016818046569824 and parameters: {'observation_period_num': 176, 'train_rates': 0.973504178580666, 'learning_rate': 8.591330004705993e-05, 'batch_size': 83, 'step_size': 13, 'gamma': 0.8321387233337467}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:32:08,695][0m Trial 46 finished with value: 0.14311230182647705 and parameters: {'observation_period_num': 122, 'train_rates': 0.9889240955700238, 'learning_rate': 0.00015347674885317028, 'batch_size': 60, 'step_size': 13, 'gamma': 0.791514231709773}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:32:54,285][0m Trial 47 finished with value: 0.06655289534332413 and parameters: {'observation_period_num': 18, 'train_rates': 0.6025973556169829, 'learning_rate': 0.00026965489927536265, 'batch_size': 102, 'step_size': 14, 'gamma': 0.8273371164107336}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:34:08,691][0m Trial 48 finished with value: 0.06943488866090775 and parameters: {'observation_period_num': 63, 'train_rates': 0.961821482977248, 'learning_rate': 8.918059141830266e-05, 'batch_size': 80, 'step_size': 12, 'gamma': 0.8009766780419183}. Best is trial 21 with value: 0.028354151546955107.[0m
[32m[I 2025-02-04 13:34:52,039][0m Trial 49 finished with value: 0.5592157261810083 and parameters: {'observation_period_num': 74, 'train_rates': 0.6676777621500657, 'learning_rate': 5.28971846858546e-05, 'batch_size': 109, 'step_size': 10, 'gamma': 0.7763101950763835}. Best is trial 21 with value: 0.028354151546955107.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-04 13:34:52,054][0m A new study created in memory with name: no-name-36e0e4ca-ef2b-4688-8691-5dcca124b941[0m
[32m[I 2025-02-04 13:35:33,276][0m Trial 0 finished with value: 0.21558014913759332 and parameters: {'observation_period_num': 170, 'train_rates': 0.6621777362767922, 'learning_rate': 0.0001412614482639728, 'batch_size': 110, 'step_size': 4, 'gamma': 0.7626876311499584}. Best is trial 0 with value: 0.21558014913759332.[0m
[32m[I 2025-02-04 13:38:32,052][0m Trial 1 finished with value: 0.11992547269116081 and parameters: {'observation_period_num': 226, 'train_rates': 0.845281843823819, 'learning_rate': 6.281038863110979e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.8123722035499606}. Best is trial 1 with value: 0.11992547269116081.[0m
[32m[I 2025-02-04 13:39:09,326][0m Trial 2 finished with value: 0.05113991725946141 and parameters: {'observation_period_num': 50, 'train_rates': 0.926331059854879, 'learning_rate': 0.00014754232266450364, 'batch_size': 164, 'step_size': 12, 'gamma': 0.9844871144236573}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:42:24,427][0m Trial 3 finished with value: 0.060317011671684864 and parameters: {'observation_period_num': 33, 'train_rates': 0.701129796314909, 'learning_rate': 0.00011589014632901182, 'batch_size': 24, 'step_size': 7, 'gamma': 0.7515137479063303}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:43:06,111][0m Trial 4 finished with value: 0.285465270280838 and parameters: {'observation_period_num': 66, 'train_rates': 0.9893842690411493, 'learning_rate': 7.857405135013398e-06, 'batch_size': 156, 'step_size': 7, 'gamma': 0.9131625142258779}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:43:33,335][0m Trial 5 finished with value: 0.07653851298746574 and parameters: {'observation_period_num': 18, 'train_rates': 0.724537597514906, 'learning_rate': 0.00016250218166091217, 'batch_size': 195, 'step_size': 5, 'gamma': 0.8598345321874403}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:44:08,637][0m Trial 6 finished with value: 0.05636066857438821 and parameters: {'observation_period_num': 45, 'train_rates': 0.9287050236161029, 'learning_rate': 9.159384179990458e-05, 'batch_size': 176, 'step_size': 7, 'gamma': 0.9110757500264547}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:47:04,114][0m Trial 7 finished with value: 0.32236176085182955 and parameters: {'observation_period_num': 227, 'train_rates': 0.6673334770447172, 'learning_rate': 2.655421022507504e-06, 'batch_size': 24, 'step_size': 15, 'gamma': 0.816206291978487}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:47:44,141][0m Trial 8 finished with value: 0.12560538473951205 and parameters: {'observation_period_num': 194, 'train_rates': 0.8519147867626483, 'learning_rate': 0.0003406912762508528, 'batch_size': 141, 'step_size': 10, 'gamma': 0.8619649089024444}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:51:01,557][0m Trial 9 finished with value: 0.05658820295731907 and parameters: {'observation_period_num': 32, 'train_rates': 0.8001481745467006, 'learning_rate': 0.0007358405005929847, 'batch_size': 26, 'step_size': 10, 'gamma': 0.8239156391480746}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:51:26,461][0m Trial 10 finished with value: 0.18834514915943146 and parameters: {'observation_period_num': 111, 'train_rates': 0.9265197153716963, 'learning_rate': 1.506799340806969e-05, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9825109549074204}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:51:58,570][0m Trial 11 finished with value: 0.11335258898990495 and parameters: {'observation_period_num': 88, 'train_rates': 0.9267293826612932, 'learning_rate': 3.8743984543990095e-05, 'batch_size': 193, 'step_size': 1, 'gamma': 0.9888698108031904}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:52:58,421][0m Trial 12 finished with value: 0.08507511019706726 and parameters: {'observation_period_num': 145, 'train_rates': 0.9818979097067796, 'learning_rate': 0.000980739471941446, 'batch_size': 100, 'step_size': 12, 'gamma': 0.9304738530157401}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:53:29,723][0m Trial 13 finished with value: 0.1185329478096079 and parameters: {'observation_period_num': 63, 'train_rates': 0.9067759145724057, 'learning_rate': 1.8389812681666103e-05, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9371624824214396}. Best is trial 2 with value: 0.05113991725946141.[0m
[32m[I 2025-02-04 13:53:54,819][0m Trial 14 finished with value: 0.03504942632479692 and parameters: {'observation_period_num': 8, 'train_rates': 0.8651597126211461, 'learning_rate': 0.0003161221816922007, 'batch_size': 237, 'step_size': 13, 'gamma': 0.8972752475748963}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:54:13,595][0m Trial 15 finished with value: 0.1967077847725539 and parameters: {'observation_period_num': 106, 'train_rates': 0.604248029465417, 'learning_rate': 0.00028872066639909017, 'batch_size': 251, 'step_size': 13, 'gamma': 0.9661329335090508}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:54:41,896][0m Trial 16 finished with value: 0.03924626753811376 and parameters: {'observation_period_num': 7, 'train_rates': 0.8591941653847581, 'learning_rate': 0.00037488583649867375, 'batch_size': 226, 'step_size': 13, 'gamma': 0.8876053933429646}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:55:08,331][0m Trial 17 finished with value: 0.047348999847022316 and parameters: {'observation_period_num': 12, 'train_rates': 0.7858414854724918, 'learning_rate': 0.0003049122057467473, 'batch_size': 230, 'step_size': 14, 'gamma': 0.888674784144041}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:55:37,334][0m Trial 18 finished with value: 0.03732579547951939 and parameters: {'observation_period_num': 7, 'train_rates': 0.842335862663849, 'learning_rate': 0.0005375212102068414, 'batch_size': 223, 'step_size': 9, 'gamma': 0.8847236279824379}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:56:03,770][0m Trial 19 finished with value: 0.6750635917148282 and parameters: {'observation_period_num': 81, 'train_rates': 0.7843381826177495, 'learning_rate': 1.4881308073243103e-06, 'batch_size': 221, 'step_size': 9, 'gamma': 0.8453485734701603}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:57:12,588][0m Trial 20 finished with value: 0.1289233586661411 and parameters: {'observation_period_num': 143, 'train_rates': 0.8260283927814049, 'learning_rate': 0.0005825664319959726, 'batch_size': 76, 'step_size': 5, 'gamma': 0.8876776164234588}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:57:39,784][0m Trial 21 finished with value: 0.03934817778823436 and parameters: {'observation_period_num': 7, 'train_rates': 0.8781710908262153, 'learning_rate': 0.0003648854022887735, 'batch_size': 224, 'step_size': 13, 'gamma': 0.8918170493630911}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:58:07,334][0m Trial 22 finished with value: 0.03616568958126633 and parameters: {'observation_period_num': 10, 'train_rates': 0.8785171654626373, 'learning_rate': 0.0005029379550894883, 'batch_size': 230, 'step_size': 11, 'gamma': 0.9393302694203215}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:58:33,745][0m Trial 23 finished with value: 0.07596317832549188 and parameters: {'observation_period_num': 34, 'train_rates': 0.7579872923117549, 'learning_rate': 0.0005978102800241204, 'batch_size': 209, 'step_size': 11, 'gamma': 0.9475775376186254}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:59:00,733][0m Trial 24 finished with value: 0.06440101251486809 and parameters: {'observation_period_num': 61, 'train_rates': 0.8930671150025782, 'learning_rate': 0.00023038383705483248, 'batch_size': 238, 'step_size': 9, 'gamma': 0.914607411359544}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 13:59:32,399][0m Trial 25 finished with value: 0.05920367216349541 and parameters: {'observation_period_num': 26, 'train_rates': 0.817639846180229, 'learning_rate': 5.8536405445127535e-05, 'batch_size': 179, 'step_size': 11, 'gamma': 0.9405029198404877}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:00:03,377][0m Trial 26 finished with value: 0.08005332201719284 and parameters: {'observation_period_num': 80, 'train_rates': 0.9583462581185971, 'learning_rate': 0.0009107890198755012, 'batch_size': 210, 'step_size': 9, 'gamma': 0.8419617961790468}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:00:28,584][0m Trial 27 finished with value: 0.08173650798684391 and parameters: {'observation_period_num': 44, 'train_rates': 0.8770252031239973, 'learning_rate': 0.0005055733245460892, 'batch_size': 245, 'step_size': 12, 'gamma': 0.9029855778634961}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:01:51,873][0m Trial 28 finished with value: 0.6153126537889737 and parameters: {'observation_period_num': 250, 'train_rates': 0.754961408714176, 'learning_rate': 0.000199831439353373, 'batch_size': 56, 'step_size': 14, 'gamma': 0.9668717447825625}. Best is trial 14 with value: 0.03504942632479692.[0m
Early stopping at epoch 53
[32m[I 2025-02-04 14:02:05,184][0m Trial 29 finished with value: 0.37227578115463256 and parameters: {'observation_period_num': 103, 'train_rates': 0.8245870956415481, 'learning_rate': 9.11724500033447e-05, 'batch_size': 256, 'step_size': 1, 'gamma': 0.7882820097619117}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:02:51,941][0m Trial 30 finished with value: 0.0949946573977782 and parameters: {'observation_period_num': 176, 'train_rates': 0.87334633922574, 'learning_rate': 0.0004970904494270515, 'batch_size': 117, 'step_size': 3, 'gamma': 0.8712139203410696}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:03:19,639][0m Trial 31 finished with value: 0.03922676801550807 and parameters: {'observation_period_num': 5, 'train_rates': 0.8452898608122233, 'learning_rate': 0.0004117500814203391, 'batch_size': 212, 'step_size': 13, 'gamma': 0.8743746510749225}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:03:47,666][0m Trial 32 finished with value: 0.0466406056694538 and parameters: {'observation_period_num': 5, 'train_rates': 0.8296992618181163, 'learning_rate': 0.0002094361175938104, 'batch_size': 210, 'step_size': 14, 'gamma': 0.9237815397289385}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:04:14,952][0m Trial 33 finished with value: 0.03654949755713828 and parameters: {'observation_period_num': 21, 'train_rates': 0.9056930664008608, 'learning_rate': 0.0009560152560502839, 'batch_size': 237, 'step_size': 10, 'gamma': 0.8807271483185193}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:04:41,649][0m Trial 34 finished with value: 0.0425841249525547 and parameters: {'observation_period_num': 25, 'train_rates': 0.9540424443622634, 'learning_rate': 0.000977407246523429, 'batch_size': 237, 'step_size': 8, 'gamma': 0.8482783162239133}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:05:14,632][0m Trial 35 finished with value: 0.057764409498001136 and parameters: {'observation_period_num': 51, 'train_rates': 0.9008697323682651, 'learning_rate': 0.0006193314714970325, 'batch_size': 179, 'step_size': 10, 'gamma': 0.901833958486895}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:05:52,372][0m Trial 36 finished with value: 0.0521939251559092 and parameters: {'observation_period_num': 23, 'train_rates': 0.9007927952124929, 'learning_rate': 0.00012095649202945459, 'batch_size': 158, 'step_size': 8, 'gamma': 0.9576914994363329}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:06:25,265][0m Trial 37 finished with value: 0.06696606427431107 and parameters: {'observation_period_num': 43, 'train_rates': 0.9496491838033373, 'learning_rate': 6.06274250176409e-05, 'batch_size': 193, 'step_size': 11, 'gamma': 0.9213585596508839}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:06:50,870][0m Trial 38 finished with value: 0.058696628422350496 and parameters: {'observation_period_num': 19, 'train_rates': 0.8614987598819468, 'learning_rate': 0.00015380529295368477, 'batch_size': 242, 'step_size': 6, 'gamma': 0.876816176450847}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:07:16,557][0m Trial 39 finished with value: 0.07010543432476718 and parameters: {'observation_period_num': 55, 'train_rates': 0.8088304374125935, 'learning_rate': 0.0007437009656724949, 'batch_size': 232, 'step_size': 12, 'gamma': 0.9045376630341029}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:07:49,527][0m Trial 40 finished with value: 0.29887434208031854 and parameters: {'observation_period_num': 33, 'train_rates': 0.7626062306758062, 'learning_rate': 8.058637805546438e-06, 'batch_size': 168, 'step_size': 10, 'gamma': 0.8623152881922108}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:08:17,939][0m Trial 41 finished with value: 0.036652079771284936 and parameters: {'observation_period_num': 16, 'train_rates': 0.8429366289539855, 'learning_rate': 0.0004359514159844015, 'batch_size': 212, 'step_size': 13, 'gamma': 0.8752409960743497}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:08:47,874][0m Trial 42 finished with value: 0.059565050506984796 and parameters: {'observation_period_num': 39, 'train_rates': 0.8441451650329586, 'learning_rate': 0.0002470831135454473, 'batch_size': 220, 'step_size': 9, 'gamma': 0.8369311069171796}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:09:17,230][0m Trial 43 finished with value: 0.03823747522355187 and parameters: {'observation_period_num': 22, 'train_rates': 0.885279634364531, 'learning_rate': 0.0004812833331319531, 'batch_size': 204, 'step_size': 11, 'gamma': 0.8545360197602746}. Best is trial 14 with value: 0.03504942632479692.[0m
[32m[I 2025-02-04 14:09:44,430][0m Trial 44 finished with value: 0.034092017503795924 and parameters: {'observation_period_num': 18, 'train_rates': 0.9125719003888111, 'learning_rate': 0.0007502021792523045, 'batch_size': 243, 'step_size': 12, 'gamma': 0.7955825522976268}. Best is trial 44 with value: 0.034092017503795924.[0m
[32m[I 2025-02-04 14:10:09,255][0m Trial 45 finished with value: 0.07797908782958984 and parameters: {'observation_period_num': 72, 'train_rates': 0.9194659300902922, 'learning_rate': 0.0007972119551957845, 'batch_size': 245, 'step_size': 15, 'gamma': 0.775139185887421}. Best is trial 44 with value: 0.034092017503795924.[0m
[32m[I 2025-02-04 14:10:34,257][0m Trial 46 finished with value: 0.05490085110068321 and parameters: {'observation_period_num': 18, 'train_rates': 0.9399795324667284, 'learning_rate': 0.0001628556354844994, 'batch_size': 256, 'step_size': 12, 'gamma': 0.8050130812652513}. Best is trial 44 with value: 0.034092017503795924.[0m
[32m[I 2025-02-04 14:11:17,322][0m Trial 47 finished with value: 0.05531436204910278 and parameters: {'observation_period_num': 33, 'train_rates': 0.9764136134223891, 'learning_rate': 9.019159883329855e-05, 'batch_size': 148, 'step_size': 14, 'gamma': 0.8339393111566958}. Best is trial 44 with value: 0.034092017503795924.[0m
[32m[I 2025-02-04 14:11:50,940][0m Trial 48 finished with value: 0.04595710960054687 and parameters: {'observation_period_num': 47, 'train_rates': 0.9150452263570223, 'learning_rate': 0.00032574935248488494, 'batch_size': 187, 'step_size': 12, 'gamma': 0.7579551771563574}. Best is trial 44 with value: 0.034092017503795924.[0m
[32m[I 2025-02-04 14:12:34,222][0m Trial 49 finished with value: 0.15694035627419436 and parameters: {'observation_period_num': 203, 'train_rates': 0.8565878881840844, 'learning_rate': 0.0006862595011766237, 'batch_size': 129, 'step_size': 13, 'gamma': 0.8244458242264642}. Best is trial 44 with value: 0.034092017503795924.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_XOM_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 7, 'train_rates': 0.8910502637348999, 'learning_rate': 0.00037720837755511713, 'batch_size': 86, 'step_size': 3, 'gamma': 0.9217696725671704}
Epoch 1/300, trend Loss: 0.1650 | 0.1138
Epoch 2/300, trend Loss: 0.1043 | 0.0872
Epoch 3/300, trend Loss: 0.0971 | 0.0876
Epoch 4/300, trend Loss: 0.0829 | 0.0828
Epoch 5/300, trend Loss: 0.0788 | 0.0729
Epoch 6/300, trend Loss: 0.0789 | 0.1005
Epoch 7/300, trend Loss: 0.0832 | 0.1907
Epoch 8/300, trend Loss: 0.0821 | 0.1806
Epoch 9/300, trend Loss: 0.0845 | 0.0683
Epoch 10/300, trend Loss: 0.0727 | 0.0768
Epoch 11/300, trend Loss: 0.0696 | 0.0619
Epoch 12/300, trend Loss: 0.0574 | 0.0608
Epoch 13/300, trend Loss: 0.0559 | 0.0667
Epoch 14/300, trend Loss: 0.0549 | 0.0675
Epoch 15/300, trend Loss: 0.0530 | 0.0665
Epoch 16/300, trend Loss: 0.0503 | 0.0582
Epoch 17/300, trend Loss: 0.0482 | 0.0510
Epoch 18/300, trend Loss: 0.0466 | 0.0505
Epoch 19/300, trend Loss: 0.0464 | 0.0501
Epoch 20/300, trend Loss: 0.0470 | 0.0500
Epoch 21/300, trend Loss: 0.0474 | 0.0498
Epoch 22/300, trend Loss: 0.0458 | 0.0493
Epoch 23/300, trend Loss: 0.0448 | 0.0480
Epoch 24/300, trend Loss: 0.0443 | 0.0478
Epoch 25/300, trend Loss: 0.0435 | 0.0473
Epoch 26/300, trend Loss: 0.0421 | 0.0466
Epoch 27/300, trend Loss: 0.0415 | 0.0458
Epoch 28/300, trend Loss: 0.0410 | 0.0455
Epoch 29/300, trend Loss: 0.0405 | 0.0450
Epoch 30/300, trend Loss: 0.0397 | 0.0442
Epoch 31/300, trend Loss: 0.0392 | 0.0438
Epoch 32/300, trend Loss: 0.0390 | 0.0438
Epoch 33/300, trend Loss: 0.0385 | 0.0432
Epoch 34/300, trend Loss: 0.0380 | 0.0427
Epoch 35/300, trend Loss: 0.0376 | 0.0427
Epoch 36/300, trend Loss: 0.0372 | 0.0424
Epoch 37/300, trend Loss: 0.0367 | 0.0417
Epoch 38/300, trend Loss: 0.0363 | 0.0415
Epoch 39/300, trend Loss: 0.0360 | 0.0413
Epoch 40/300, trend Loss: 0.0356 | 0.0408
Epoch 41/300, trend Loss: 0.0352 | 0.0406
Epoch 42/300, trend Loss: 0.0349 | 0.0403
Epoch 43/300, trend Loss: 0.0347 | 0.0400
Epoch 44/300, trend Loss: 0.0344 | 0.0398
Epoch 45/300, trend Loss: 0.0341 | 0.0396
Epoch 46/300, trend Loss: 0.0339 | 0.0394
Epoch 47/300, trend Loss: 0.0338 | 0.0393
Epoch 48/300, trend Loss: 0.0336 | 0.0391
Epoch 49/300, trend Loss: 0.0335 | 0.0391
Epoch 50/300, trend Loss: 0.0334 | 0.0390
Epoch 51/300, trend Loss: 0.0333 | 0.0389
Epoch 52/300, trend Loss: 0.0333 | 0.0390
Epoch 53/300, trend Loss: 0.0333 | 0.0389
Epoch 54/300, trend Loss: 0.0333 | 0.0393
Epoch 55/300, trend Loss: 0.0333 | 0.0394
Epoch 56/300, trend Loss: 0.0333 | 0.0390
Epoch 57/300, trend Loss: 0.0330 | 0.0386
Epoch 58/300, trend Loss: 0.0327 | 0.0384
Epoch 59/300, trend Loss: 0.0325 | 0.0383
Epoch 60/300, trend Loss: 0.0323 | 0.0383
Epoch 61/300, trend Loss: 0.0322 | 0.0382
Epoch 62/300, trend Loss: 0.0322 | 0.0381
Epoch 63/300, trend Loss: 0.0321 | 0.0380
Epoch 64/300, trend Loss: 0.0320 | 0.0380
Epoch 65/300, trend Loss: 0.0320 | 0.0379
Epoch 66/300, trend Loss: 0.0319 | 0.0379
Epoch 67/300, trend Loss: 0.0319 | 0.0378
Epoch 68/300, trend Loss: 0.0318 | 0.0378
Epoch 69/300, trend Loss: 0.0318 | 0.0378
Epoch 70/300, trend Loss: 0.0318 | 0.0377
Epoch 71/300, trend Loss: 0.0317 | 0.0377
Epoch 72/300, trend Loss: 0.0317 | 0.0377
Epoch 73/300, trend Loss: 0.0317 | 0.0377
Epoch 74/300, trend Loss: 0.0317 | 0.0376
Epoch 75/300, trend Loss: 0.0317 | 0.0376
Epoch 76/300, trend Loss: 0.0317 | 0.0376
Epoch 77/300, trend Loss: 0.0317 | 0.0376
Epoch 78/300, trend Loss: 0.0316 | 0.0376
Epoch 79/300, trend Loss: 0.0317 | 0.0375
Epoch 80/300, trend Loss: 0.0317 | 0.0375
Epoch 81/300, trend Loss: 0.0316 | 0.0374
Epoch 82/300, trend Loss: 0.0316 | 0.0373
Epoch 83/300, trend Loss: 0.0316 | 0.0373
Epoch 84/300, trend Loss: 0.0316 | 0.0373
Epoch 85/300, trend Loss: 0.0316 | 0.0372
Epoch 86/300, trend Loss: 0.0316 | 0.0372
Epoch 87/300, trend Loss: 0.0316 | 0.0372
Epoch 88/300, trend Loss: 0.0315 | 0.0371
Epoch 89/300, trend Loss: 0.0315 | 0.0371
Epoch 90/300, trend Loss: 0.0314 | 0.0371
Epoch 91/300, trend Loss: 0.0314 | 0.0371
Epoch 92/300, trend Loss: 0.0314 | 0.0371
Epoch 93/300, trend Loss: 0.0313 | 0.0371
Epoch 94/300, trend Loss: 0.0313 | 0.0370
Epoch 95/300, trend Loss: 0.0313 | 0.0370
Epoch 96/300, trend Loss: 0.0313 | 0.0370
Epoch 97/300, trend Loss: 0.0312 | 0.0370
Epoch 98/300, trend Loss: 0.0312 | 0.0370
Epoch 99/300, trend Loss: 0.0312 | 0.0370
Epoch 100/300, trend Loss: 0.0312 | 0.0370
Epoch 101/300, trend Loss: 0.0312 | 0.0370
Epoch 102/300, trend Loss: 0.0312 | 0.0370
Epoch 103/300, trend Loss: 0.0312 | 0.0370
Epoch 104/300, trend Loss: 0.0311 | 0.0369
Epoch 105/300, trend Loss: 0.0311 | 0.0369
Epoch 106/300, trend Loss: 0.0311 | 0.0369
Epoch 107/300, trend Loss: 0.0311 | 0.0369
Epoch 108/300, trend Loss: 0.0311 | 0.0369
Epoch 109/300, trend Loss: 0.0311 | 0.0369
Epoch 110/300, trend Loss: 0.0311 | 0.0369
Epoch 111/300, trend Loss: 0.0311 | 0.0369
Epoch 112/300, trend Loss: 0.0311 | 0.0369
Epoch 113/300, trend Loss: 0.0311 | 0.0369
Epoch 114/300, trend Loss: 0.0311 | 0.0369
Epoch 115/300, trend Loss: 0.0311 | 0.0369
Epoch 116/300, trend Loss: 0.0311 | 0.0369
Epoch 117/300, trend Loss: 0.0310 | 0.0369
Epoch 118/300, trend Loss: 0.0310 | 0.0369
Epoch 119/300, trend Loss: 0.0310 | 0.0369
Epoch 120/300, trend Loss: 0.0310 | 0.0369
Epoch 121/300, trend Loss: 0.0310 | 0.0369
Epoch 122/300, trend Loss: 0.0310 | 0.0369
Epoch 123/300, trend Loss: 0.0310 | 0.0369
Epoch 124/300, trend Loss: 0.0310 | 0.0369
Epoch 125/300, trend Loss: 0.0310 | 0.0369
Epoch 126/300, trend Loss: 0.0310 | 0.0369
Epoch 127/300, trend Loss: 0.0310 | 0.0369
Epoch 128/300, trend Loss: 0.0310 | 0.0369
Epoch 129/300, trend Loss: 0.0310 | 0.0369
Epoch 130/300, trend Loss: 0.0310 | 0.0369
Epoch 131/300, trend Loss: 0.0310 | 0.0369
Epoch 132/300, trend Loss: 0.0310 | 0.0369
Epoch 133/300, trend Loss: 0.0310 | 0.0369
Epoch 134/300, trend Loss: 0.0310 | 0.0369
Epoch 135/300, trend Loss: 0.0310 | 0.0369
Epoch 136/300, trend Loss: 0.0310 | 0.0369
Epoch 137/300, trend Loss: 0.0310 | 0.0368
Epoch 138/300, trend Loss: 0.0310 | 0.0368
Epoch 139/300, trend Loss: 0.0310 | 0.0368
Epoch 140/300, trend Loss: 0.0310 | 0.0368
Epoch 141/300, trend Loss: 0.0310 | 0.0368
Epoch 142/300, trend Loss: 0.0310 | 0.0368
Epoch 143/300, trend Loss: 0.0310 | 0.0368
Epoch 144/300, trend Loss: 0.0310 | 0.0368
Epoch 145/300, trend Loss: 0.0310 | 0.0368
Epoch 146/300, trend Loss: 0.0310 | 0.0368
Epoch 147/300, trend Loss: 0.0310 | 0.0368
Epoch 148/300, trend Loss: 0.0310 | 0.0368
Epoch 149/300, trend Loss: 0.0310 | 0.0368
Epoch 150/300, trend Loss: 0.0310 | 0.0368
Epoch 151/300, trend Loss: 0.0310 | 0.0368
Epoch 152/300, trend Loss: 0.0310 | 0.0368
Epoch 153/300, trend Loss: 0.0310 | 0.0368
Epoch 154/300, trend Loss: 0.0310 | 0.0368
Epoch 155/300, trend Loss: 0.0310 | 0.0368
Epoch 156/300, trend Loss: 0.0310 | 0.0368
Epoch 157/300, trend Loss: 0.0310 | 0.0368
Epoch 158/300, trend Loss: 0.0310 | 0.0368
Epoch 159/300, trend Loss: 0.0310 | 0.0368
Epoch 160/300, trend Loss: 0.0310 | 0.0368
Epoch 161/300, trend Loss: 0.0310 | 0.0368
Epoch 162/300, trend Loss: 0.0310 | 0.0368
Epoch 163/300, trend Loss: 0.0310 | 0.0368
Epoch 164/300, trend Loss: 0.0310 | 0.0368
Epoch 165/300, trend Loss: 0.0310 | 0.0368
Epoch 166/300, trend Loss: 0.0310 | 0.0368
Epoch 167/300, trend Loss: 0.0310 | 0.0368
Epoch 168/300, trend Loss: 0.0310 | 0.0368
Epoch 169/300, trend Loss: 0.0310 | 0.0368
Epoch 170/300, trend Loss: 0.0310 | 0.0368
Epoch 171/300, trend Loss: 0.0310 | 0.0368
Epoch 172/300, trend Loss: 0.0310 | 0.0368
Epoch 173/300, trend Loss: 0.0310 | 0.0368
Epoch 174/300, trend Loss: 0.0310 | 0.0368
Epoch 175/300, trend Loss: 0.0310 | 0.0368
Epoch 176/300, trend Loss: 0.0310 | 0.0368
Epoch 177/300, trend Loss: 0.0310 | 0.0368
Epoch 178/300, trend Loss: 0.0310 | 0.0368
Epoch 179/300, trend Loss: 0.0310 | 0.0368
Epoch 180/300, trend Loss: 0.0310 | 0.0368
Epoch 181/300, trend Loss: 0.0310 | 0.0368
Epoch 182/300, trend Loss: 0.0310 | 0.0368
Epoch 183/300, trend Loss: 0.0310 | 0.0368
Epoch 184/300, trend Loss: 0.0310 | 0.0368
Epoch 185/300, trend Loss: 0.0310 | 0.0368
Epoch 186/300, trend Loss: 0.0310 | 0.0368
Epoch 187/300, trend Loss: 0.0310 | 0.0368
Epoch 188/300, trend Loss: 0.0310 | 0.0368
Epoch 189/300, trend Loss: 0.0310 | 0.0368
Epoch 190/300, trend Loss: 0.0310 | 0.0368
Epoch 191/300, trend Loss: 0.0310 | 0.0368
Epoch 192/300, trend Loss: 0.0310 | 0.0368
Epoch 193/300, trend Loss: 0.0310 | 0.0368
Epoch 194/300, trend Loss: 0.0310 | 0.0368
Epoch 195/300, trend Loss: 0.0310 | 0.0368
Epoch 196/300, trend Loss: 0.0310 | 0.0368
Epoch 197/300, trend Loss: 0.0310 | 0.0368
Epoch 198/300, trend Loss: 0.0310 | 0.0368
Epoch 199/300, trend Loss: 0.0310 | 0.0368
Epoch 200/300, trend Loss: 0.0310 | 0.0368
Epoch 201/300, trend Loss: 0.0310 | 0.0368
Epoch 202/300, trend Loss: 0.0310 | 0.0368
Epoch 203/300, trend Loss: 0.0310 | 0.0368
Epoch 204/300, trend Loss: 0.0310 | 0.0368
Epoch 205/300, trend Loss: 0.0310 | 0.0368
Epoch 206/300, trend Loss: 0.0310 | 0.0368
Epoch 207/300, trend Loss: 0.0310 | 0.0368
Epoch 208/300, trend Loss: 0.0310 | 0.0368
Epoch 209/300, trend Loss: 0.0310 | 0.0368
Epoch 210/300, trend Loss: 0.0310 | 0.0368
Epoch 211/300, trend Loss: 0.0310 | 0.0368
Epoch 212/300, trend Loss: 0.0310 | 0.0368
Epoch 213/300, trend Loss: 0.0310 | 0.0368
Epoch 214/300, trend Loss: 0.0310 | 0.0368
Epoch 215/300, trend Loss: 0.0310 | 0.0368
Epoch 216/300, trend Loss: 0.0310 | 0.0368
Epoch 217/300, trend Loss: 0.0310 | 0.0368
Epoch 218/300, trend Loss: 0.0310 | 0.0368
Epoch 219/300, trend Loss: 0.0310 | 0.0368
Epoch 220/300, trend Loss: 0.0310 | 0.0368
Epoch 221/300, trend Loss: 0.0310 | 0.0368
Epoch 222/300, trend Loss: 0.0310 | 0.0368
Epoch 223/300, trend Loss: 0.0310 | 0.0368
Epoch 224/300, trend Loss: 0.0310 | 0.0368
Epoch 225/300, trend Loss: 0.0310 | 0.0368
Epoch 226/300, trend Loss: 0.0310 | 0.0368
Epoch 227/300, trend Loss: 0.0310 | 0.0368
Epoch 228/300, trend Loss: 0.0310 | 0.0368
Epoch 229/300, trend Loss: 0.0310 | 0.0368
Epoch 230/300, trend Loss: 0.0310 | 0.0368
Epoch 231/300, trend Loss: 0.0310 | 0.0368
Epoch 232/300, trend Loss: 0.0310 | 0.0368
Epoch 233/300, trend Loss: 0.0310 | 0.0368
Epoch 234/300, trend Loss: 0.0310 | 0.0368
Epoch 235/300, trend Loss: 0.0310 | 0.0368
Epoch 236/300, trend Loss: 0.0310 | 0.0368
Epoch 237/300, trend Loss: 0.0310 | 0.0368
Epoch 238/300, trend Loss: 0.0310 | 0.0368
Epoch 239/300, trend Loss: 0.0310 | 0.0368
Epoch 240/300, trend Loss: 0.0310 | 0.0368
Early stopping for trend
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.7631668147621694, 'learning_rate': 0.0004030907218056451, 'batch_size': 237, 'step_size': 8, 'gamma': 0.9562592561903684}
Epoch 1/300, seasonal_0 Loss: 0.7511 | 0.7465
Epoch 2/300, seasonal_0 Loss: 0.2114 | 0.2524
Epoch 3/300, seasonal_0 Loss: 0.1913 | 0.1979
Epoch 4/300, seasonal_0 Loss: 0.1370 | 0.1530
Epoch 5/300, seasonal_0 Loss: 0.1165 | 0.1424
Epoch 6/300, seasonal_0 Loss: 0.1109 | 0.1264
Epoch 7/300, seasonal_0 Loss: 0.1010 | 0.1250
Epoch 8/300, seasonal_0 Loss: 0.0987 | 0.1129
Epoch 9/300, seasonal_0 Loss: 0.0906 | 0.1203
Epoch 10/300, seasonal_0 Loss: 0.0873 | 0.1225
Epoch 11/300, seasonal_0 Loss: 0.0830 | 0.1234
Epoch 12/300, seasonal_0 Loss: 0.0790 | 0.1433
Epoch 13/300, seasonal_0 Loss: 0.0771 | 0.1357
Epoch 14/300, seasonal_0 Loss: 0.0739 | 0.1384
Epoch 15/300, seasonal_0 Loss: 0.0724 | 0.1333
Epoch 16/300, seasonal_0 Loss: 0.0708 | 0.1272
Epoch 17/300, seasonal_0 Loss: 0.0691 | 0.1132
Epoch 18/300, seasonal_0 Loss: 0.0675 | 0.1032
Epoch 19/300, seasonal_0 Loss: 0.0665 | 0.1012
Epoch 20/300, seasonal_0 Loss: 0.0652 | 0.0905
Epoch 21/300, seasonal_0 Loss: 0.0642 | 0.0878
Epoch 22/300, seasonal_0 Loss: 0.0632 | 0.0813
Epoch 23/300, seasonal_0 Loss: 0.0623 | 0.0788
Epoch 24/300, seasonal_0 Loss: 0.0616 | 0.0771
Epoch 25/300, seasonal_0 Loss: 0.0611 | 0.0759
Epoch 26/300, seasonal_0 Loss: 0.0611 | 0.0747
Epoch 27/300, seasonal_0 Loss: 0.0617 | 0.0740
Epoch 28/300, seasonal_0 Loss: 0.0627 | 0.0739
Epoch 29/300, seasonal_0 Loss: 0.0645 | 0.0744
Epoch 30/300, seasonal_0 Loss: 0.0665 | 0.0756
Epoch 31/300, seasonal_0 Loss: 0.0665 | 0.0759
Epoch 32/300, seasonal_0 Loss: 0.0632 | 0.0716
Epoch 33/300, seasonal_0 Loss: 0.0598 | 0.0739
Epoch 34/300, seasonal_0 Loss: 0.0630 | 0.0763
Epoch 35/300, seasonal_0 Loss: 0.0719 | 0.0797
Epoch 36/300, seasonal_0 Loss: 0.0687 | 0.0769
Epoch 37/300, seasonal_0 Loss: 0.0581 | 0.0813
Epoch 38/300, seasonal_0 Loss: 0.0609 | 0.0873
Epoch 39/300, seasonal_0 Loss: 0.0608 | 0.0778
Epoch 40/300, seasonal_0 Loss: 0.0633 | 0.0745
Epoch 41/300, seasonal_0 Loss: 0.0555 | 0.0672
Epoch 42/300, seasonal_0 Loss: 0.0580 | 0.0724
Epoch 43/300, seasonal_0 Loss: 0.0552 | 0.0695
Epoch 44/300, seasonal_0 Loss: 0.0585 | 0.0680
Epoch 45/300, seasonal_0 Loss: 0.0525 | 0.0703
Epoch 46/300, seasonal_0 Loss: 0.0537 | 0.0682
Epoch 47/300, seasonal_0 Loss: 0.0526 | 0.0658
Epoch 48/300, seasonal_0 Loss: 0.0536 | 0.0657
Epoch 49/300, seasonal_0 Loss: 0.0514 | 0.0636
Epoch 50/300, seasonal_0 Loss: 0.0531 | 0.0647
Epoch 51/300, seasonal_0 Loss: 0.0512 | 0.0630
Epoch 52/300, seasonal_0 Loss: 0.0528 | 0.0636
Epoch 53/300, seasonal_0 Loss: 0.0499 | 0.0629
Epoch 54/300, seasonal_0 Loss: 0.0505 | 0.0625
Epoch 55/300, seasonal_0 Loss: 0.0491 | 0.0615
Epoch 56/300, seasonal_0 Loss: 0.0496 | 0.0618
Epoch 57/300, seasonal_0 Loss: 0.0483 | 0.0609
Epoch 58/300, seasonal_0 Loss: 0.0488 | 0.0611
Epoch 59/300, seasonal_0 Loss: 0.0478 | 0.0603
Epoch 60/300, seasonal_0 Loss: 0.0482 | 0.0606
Epoch 61/300, seasonal_0 Loss: 0.0470 | 0.0601
Epoch 62/300, seasonal_0 Loss: 0.0473 | 0.0599
Epoch 63/300, seasonal_0 Loss: 0.0464 | 0.0597
Epoch 64/300, seasonal_0 Loss: 0.0466 | 0.0595
Epoch 65/300, seasonal_0 Loss: 0.0459 | 0.0594
Epoch 66/300, seasonal_0 Loss: 0.0461 | 0.0591
Epoch 67/300, seasonal_0 Loss: 0.0456 | 0.0590
Epoch 68/300, seasonal_0 Loss: 0.0459 | 0.0589
Epoch 69/300, seasonal_0 Loss: 0.0455 | 0.0587
Epoch 70/300, seasonal_0 Loss: 0.0458 | 0.0587
Epoch 71/300, seasonal_0 Loss: 0.0454 | 0.0583
Epoch 72/300, seasonal_0 Loss: 0.0452 | 0.0587
Epoch 73/300, seasonal_0 Loss: 0.0445 | 0.0580
Epoch 74/300, seasonal_0 Loss: 0.0441 | 0.0586
Epoch 75/300, seasonal_0 Loss: 0.0440 | 0.0580
Epoch 76/300, seasonal_0 Loss: 0.0442 | 0.0588
Epoch 77/300, seasonal_0 Loss: 0.0446 | 0.0582
Epoch 78/300, seasonal_0 Loss: 0.0448 | 0.0586
Epoch 79/300, seasonal_0 Loss: 0.0447 | 0.0580
Epoch 80/300, seasonal_0 Loss: 0.0438 | 0.0577
Epoch 81/300, seasonal_0 Loss: 0.0427 | 0.0571
Epoch 82/300, seasonal_0 Loss: 0.0423 | 0.0568
Epoch 83/300, seasonal_0 Loss: 0.0425 | 0.0566
Epoch 84/300, seasonal_0 Loss: 0.0427 | 0.0566
Epoch 85/300, seasonal_0 Loss: 0.0427 | 0.0562
Epoch 86/300, seasonal_0 Loss: 0.0423 | 0.0563
Epoch 87/300, seasonal_0 Loss: 0.0416 | 0.0557
Epoch 88/300, seasonal_0 Loss: 0.0411 | 0.0558
Epoch 89/300, seasonal_0 Loss: 0.0408 | 0.0553
Epoch 90/300, seasonal_0 Loss: 0.0406 | 0.0555
Epoch 91/300, seasonal_0 Loss: 0.0405 | 0.0550
Epoch 92/300, seasonal_0 Loss: 0.0403 | 0.0551
Epoch 93/300, seasonal_0 Loss: 0.0401 | 0.0546
Epoch 94/300, seasonal_0 Loss: 0.0399 | 0.0546
Epoch 95/300, seasonal_0 Loss: 0.0397 | 0.0541
Epoch 96/300, seasonal_0 Loss: 0.0395 | 0.0541
Epoch 97/300, seasonal_0 Loss: 0.0393 | 0.0536
Epoch 98/300, seasonal_0 Loss: 0.0392 | 0.0536
Epoch 99/300, seasonal_0 Loss: 0.0392 | 0.0532
Epoch 100/300, seasonal_0 Loss: 0.0392 | 0.0533
Epoch 101/300, seasonal_0 Loss: 0.0393 | 0.0529
Epoch 102/300, seasonal_0 Loss: 0.0395 | 0.0529
Epoch 103/300, seasonal_0 Loss: 0.0396 | 0.0526
Epoch 104/300, seasonal_0 Loss: 0.0397 | 0.0530
Epoch 105/300, seasonal_0 Loss: 0.0392 | 0.0535
Epoch 106/300, seasonal_0 Loss: 0.0388 | 0.0541
Epoch 107/300, seasonal_0 Loss: 0.0387 | 0.0530
Epoch 108/300, seasonal_0 Loss: 0.0385 | 0.0525
Epoch 109/300, seasonal_0 Loss: 0.0381 | 0.0519
Epoch 110/300, seasonal_0 Loss: 0.0378 | 0.0519
Epoch 111/300, seasonal_0 Loss: 0.0377 | 0.0518
Epoch 112/300, seasonal_0 Loss: 0.0380 | 0.0520
Epoch 113/300, seasonal_0 Loss: 0.0383 | 0.0522
Epoch 114/300, seasonal_0 Loss: 0.0389 | 0.0523
Epoch 115/300, seasonal_0 Loss: 0.0391 | 0.0521
Epoch 116/300, seasonal_0 Loss: 0.0386 | 0.0520
Epoch 117/300, seasonal_0 Loss: 0.0377 | 0.0517
Epoch 118/300, seasonal_0 Loss: 0.0372 | 0.0520
Epoch 119/300, seasonal_0 Loss: 0.0375 | 0.0519
Epoch 120/300, seasonal_0 Loss: 0.0380 | 0.0517
Epoch 121/300, seasonal_0 Loss: 0.0380 | 0.0511
Epoch 122/300, seasonal_0 Loss: 0.0373 | 0.0509
Epoch 123/300, seasonal_0 Loss: 0.0364 | 0.0508
Epoch 124/300, seasonal_0 Loss: 0.0364 | 0.0508
Epoch 125/300, seasonal_0 Loss: 0.0366 | 0.0506
Epoch 126/300, seasonal_0 Loss: 0.0365 | 0.0505
Epoch 127/300, seasonal_0 Loss: 0.0360 | 0.0504
Epoch 128/300, seasonal_0 Loss: 0.0356 | 0.0504
Epoch 129/300, seasonal_0 Loss: 0.0355 | 0.0504
Epoch 130/300, seasonal_0 Loss: 0.0355 | 0.0503
Epoch 131/300, seasonal_0 Loss: 0.0354 | 0.0502
Epoch 132/300, seasonal_0 Loss: 0.0353 | 0.0501
Epoch 133/300, seasonal_0 Loss: 0.0351 | 0.0500
Epoch 134/300, seasonal_0 Loss: 0.0350 | 0.0499
Epoch 135/300, seasonal_0 Loss: 0.0349 | 0.0498
Epoch 136/300, seasonal_0 Loss: 0.0349 | 0.0498
Epoch 137/300, seasonal_0 Loss: 0.0348 | 0.0497
Epoch 138/300, seasonal_0 Loss: 0.0348 | 0.0497
Epoch 139/300, seasonal_0 Loss: 0.0347 | 0.0497
Epoch 140/300, seasonal_0 Loss: 0.0346 | 0.0496
Epoch 141/300, seasonal_0 Loss: 0.0345 | 0.0496
Epoch 142/300, seasonal_0 Loss: 0.0344 | 0.0496
Epoch 143/300, seasonal_0 Loss: 0.0343 | 0.0495
Epoch 144/300, seasonal_0 Loss: 0.0342 | 0.0495
Epoch 145/300, seasonal_0 Loss: 0.0341 | 0.0495
Epoch 146/300, seasonal_0 Loss: 0.0341 | 0.0494
Epoch 147/300, seasonal_0 Loss: 0.0340 | 0.0494
Epoch 148/300, seasonal_0 Loss: 0.0340 | 0.0493
Epoch 149/300, seasonal_0 Loss: 0.0340 | 0.0493
Epoch 150/300, seasonal_0 Loss: 0.0340 | 0.0493
Epoch 151/300, seasonal_0 Loss: 0.0340 | 0.0492
Epoch 152/300, seasonal_0 Loss: 0.0340 | 0.0492
Epoch 153/300, seasonal_0 Loss: 0.0340 | 0.0492
Epoch 154/300, seasonal_0 Loss: 0.0341 | 0.0490
Epoch 155/300, seasonal_0 Loss: 0.0341 | 0.0487
Epoch 156/300, seasonal_0 Loss: 0.0340 | 0.0486
Epoch 157/300, seasonal_0 Loss: 0.0339 | 0.0488
Epoch 158/300, seasonal_0 Loss: 0.0345 | 0.0487
Epoch 159/300, seasonal_0 Loss: 0.0348 | 0.0488
Epoch 160/300, seasonal_0 Loss: 0.0348 | 0.0487
Epoch 161/300, seasonal_0 Loss: 0.0338 | 0.0490
Epoch 162/300, seasonal_0 Loss: 0.0332 | 0.0486
Epoch 163/300, seasonal_0 Loss: 0.0330 | 0.0489
Epoch 164/300, seasonal_0 Loss: 0.0329 | 0.0488
Epoch 165/300, seasonal_0 Loss: 0.0327 | 0.0486
Epoch 166/300, seasonal_0 Loss: 0.0326 | 0.0483
Epoch 167/300, seasonal_0 Loss: 0.0325 | 0.0481
Epoch 168/300, seasonal_0 Loss: 0.0327 | 0.0481
Epoch 169/300, seasonal_0 Loss: 0.0329 | 0.0481
Epoch 170/300, seasonal_0 Loss: 0.0328 | 0.0482
Epoch 171/300, seasonal_0 Loss: 0.0327 | 0.0480
Epoch 172/300, seasonal_0 Loss: 0.0324 | 0.0482
Epoch 173/300, seasonal_0 Loss: 0.0321 | 0.0482
Epoch 174/300, seasonal_0 Loss: 0.0319 | 0.0482
Epoch 175/300, seasonal_0 Loss: 0.0318 | 0.0482
Epoch 176/300, seasonal_0 Loss: 0.0317 | 0.0482
Epoch 177/300, seasonal_0 Loss: 0.0316 | 0.0482
Epoch 178/300, seasonal_0 Loss: 0.0315 | 0.0481
Epoch 179/300, seasonal_0 Loss: 0.0314 | 0.0480
Epoch 180/300, seasonal_0 Loss: 0.0314 | 0.0479
Epoch 181/300, seasonal_0 Loss: 0.0314 | 0.0478
Epoch 182/300, seasonal_0 Loss: 0.0315 | 0.0476
Epoch 183/300, seasonal_0 Loss: 0.0319 | 0.0476
Epoch 184/300, seasonal_0 Loss: 0.0320 | 0.0478
Epoch 185/300, seasonal_0 Loss: 0.0319 | 0.0478
Epoch 186/300, seasonal_0 Loss: 0.0318 | 0.0478
Epoch 187/300, seasonal_0 Loss: 0.0316 | 0.0479
Epoch 188/300, seasonal_0 Loss: 0.0314 | 0.0480
Epoch 189/300, seasonal_0 Loss: 0.0312 | 0.0481
Epoch 190/300, seasonal_0 Loss: 0.0311 | 0.0481
Epoch 191/300, seasonal_0 Loss: 0.0311 | 0.0481
Epoch 192/300, seasonal_0 Loss: 0.0310 | 0.0480
Epoch 193/300, seasonal_0 Loss: 0.0309 | 0.0479
Epoch 194/300, seasonal_0 Loss: 0.0309 | 0.0478
Epoch 195/300, seasonal_0 Loss: 0.0308 | 0.0477
Epoch 196/300, seasonal_0 Loss: 0.0307 | 0.0475
Epoch 197/300, seasonal_0 Loss: 0.0307 | 0.0474
Epoch 198/300, seasonal_0 Loss: 0.0308 | 0.0473
Epoch 199/300, seasonal_0 Loss: 0.0311 | 0.0473
Epoch 200/300, seasonal_0 Loss: 0.0313 | 0.0474
Epoch 201/300, seasonal_0 Loss: 0.0311 | 0.0476
Epoch 202/300, seasonal_0 Loss: 0.0309 | 0.0476
Epoch 203/300, seasonal_0 Loss: 0.0307 | 0.0476
Epoch 204/300, seasonal_0 Loss: 0.0306 | 0.0476
Epoch 205/300, seasonal_0 Loss: 0.0305 | 0.0476
Epoch 206/300, seasonal_0 Loss: 0.0305 | 0.0476
Epoch 207/300, seasonal_0 Loss: 0.0304 | 0.0475
Epoch 208/300, seasonal_0 Loss: 0.0304 | 0.0474
Epoch 209/300, seasonal_0 Loss: 0.0304 | 0.0473
Epoch 210/300, seasonal_0 Loss: 0.0304 | 0.0472
Epoch 211/300, seasonal_0 Loss: 0.0304 | 0.0472
Epoch 212/300, seasonal_0 Loss: 0.0305 | 0.0472
Epoch 213/300, seasonal_0 Loss: 0.0305 | 0.0472
Epoch 214/300, seasonal_0 Loss: 0.0305 | 0.0473
Epoch 215/300, seasonal_0 Loss: 0.0304 | 0.0474
Epoch 216/300, seasonal_0 Loss: 0.0303 | 0.0474
Epoch 217/300, seasonal_0 Loss: 0.0302 | 0.0474
Epoch 218/300, seasonal_0 Loss: 0.0302 | 0.0474
Epoch 219/300, seasonal_0 Loss: 0.0301 | 0.0473
Epoch 220/300, seasonal_0 Loss: 0.0301 | 0.0473
Epoch 221/300, seasonal_0 Loss: 0.0301 | 0.0472
Epoch 222/300, seasonal_0 Loss: 0.0301 | 0.0472
Epoch 223/300, seasonal_0 Loss: 0.0301 | 0.0471
Epoch 224/300, seasonal_0 Loss: 0.0301 | 0.0471
Epoch 225/300, seasonal_0 Loss: 0.0301 | 0.0471
Epoch 226/300, seasonal_0 Loss: 0.0301 | 0.0472
Epoch 227/300, seasonal_0 Loss: 0.0301 | 0.0472
Epoch 228/300, seasonal_0 Loss: 0.0300 | 0.0472
Epoch 229/300, seasonal_0 Loss: 0.0300 | 0.0472
Epoch 230/300, seasonal_0 Loss: 0.0299 | 0.0472
Epoch 231/300, seasonal_0 Loss: 0.0299 | 0.0472
Epoch 232/300, seasonal_0 Loss: 0.0299 | 0.0472
Epoch 233/300, seasonal_0 Loss: 0.0299 | 0.0472
Epoch 234/300, seasonal_0 Loss: 0.0299 | 0.0471
Epoch 235/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 236/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 237/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 238/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 239/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 240/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 241/300, seasonal_0 Loss: 0.0298 | 0.0471
Epoch 242/300, seasonal_0 Loss: 0.0297 | 0.0471
Epoch 243/300, seasonal_0 Loss: 0.0297 | 0.0471
Epoch 244/300, seasonal_0 Loss: 0.0297 | 0.0471
Epoch 245/300, seasonal_0 Loss: 0.0297 | 0.0471
Epoch 246/300, seasonal_0 Loss: 0.0297 | 0.0471
Epoch 247/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 248/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 249/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 250/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 251/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 252/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 253/300, seasonal_0 Loss: 0.0296 | 0.0471
Epoch 254/300, seasonal_0 Loss: 0.0295 | 0.0471
Epoch 255/300, seasonal_0 Loss: 0.0295 | 0.0471
Epoch 256/300, seasonal_0 Loss: 0.0295 | 0.0471
Epoch 257/300, seasonal_0 Loss: 0.0295 | 0.0471
Epoch 258/300, seasonal_0 Loss: 0.0295 | 0.0471
Epoch 259/300, seasonal_0 Loss: 0.0295 | 0.0470
Epoch 260/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 261/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 262/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 263/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 264/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 265/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 266/300, seasonal_0 Loss: 0.0294 | 0.0470
Epoch 267/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 268/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 269/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 270/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 271/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 272/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 273/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 274/300, seasonal_0 Loss: 0.0293 | 0.0470
Epoch 275/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 276/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 277/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 278/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 279/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 280/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 281/300, seasonal_0 Loss: 0.0292 | 0.0470
Epoch 282/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 283/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 284/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 285/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 286/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 287/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 288/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 289/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 290/300, seasonal_0 Loss: 0.0291 | 0.0470
Epoch 291/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 292/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 293/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 294/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 295/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 296/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 297/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 298/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 299/300, seasonal_0 Loss: 0.0290 | 0.0470
Epoch 300/300, seasonal_0 Loss: 0.0289 | 0.0470
Training seasonal_1 component with params: {'observation_period_num': 10, 'train_rates': 0.9711858807223085, 'learning_rate': 6.0109671044572594e-05, 'batch_size': 27, 'step_size': 3, 'gamma': 0.9350676439172101}
Epoch 1/300, seasonal_1 Loss: 0.3809 | 0.1810
Epoch 2/300, seasonal_1 Loss: 0.1289 | 0.1211
Epoch 3/300, seasonal_1 Loss: 0.1044 | 0.0973
Epoch 4/300, seasonal_1 Loss: 0.0909 | 0.0822
Epoch 5/300, seasonal_1 Loss: 0.0835 | 0.0730
Epoch 6/300, seasonal_1 Loss: 0.0791 | 0.0698
Epoch 7/300, seasonal_1 Loss: 0.0763 | 0.0688
Epoch 8/300, seasonal_1 Loss: 0.0741 | 0.0677
Epoch 9/300, seasonal_1 Loss: 0.0721 | 0.0668
Epoch 10/300, seasonal_1 Loss: 0.0707 | 0.0645
Epoch 11/300, seasonal_1 Loss: 0.0697 | 0.0632
Epoch 12/300, seasonal_1 Loss: 0.0686 | 0.0632
Epoch 13/300, seasonal_1 Loss: 0.0675 | 0.0646
Epoch 14/300, seasonal_1 Loss: 0.0657 | 0.0630
Epoch 15/300, seasonal_1 Loss: 0.0637 | 0.0621
Epoch 16/300, seasonal_1 Loss: 0.0622 | 0.0608
Epoch 17/300, seasonal_1 Loss: 0.0607 | 0.0597
Epoch 18/300, seasonal_1 Loss: 0.0593 | 0.0589
Epoch 19/300, seasonal_1 Loss: 0.0582 | 0.0581
Epoch 20/300, seasonal_1 Loss: 0.0572 | 0.0574
Epoch 21/300, seasonal_1 Loss: 0.0562 | 0.0568
Epoch 22/300, seasonal_1 Loss: 0.0553 | 0.0563
Epoch 23/300, seasonal_1 Loss: 0.0544 | 0.0557
Epoch 24/300, seasonal_1 Loss: 0.0536 | 0.0553
Epoch 25/300, seasonal_1 Loss: 0.0529 | 0.0548
Epoch 26/300, seasonal_1 Loss: 0.0522 | 0.0544
Epoch 27/300, seasonal_1 Loss: 0.0515 | 0.0540
Epoch 28/300, seasonal_1 Loss: 0.0509 | 0.0536
Epoch 29/300, seasonal_1 Loss: 0.0504 | 0.0532
Epoch 30/300, seasonal_1 Loss: 0.0498 | 0.0528
Epoch 31/300, seasonal_1 Loss: 0.0492 | 0.0525
Epoch 32/300, seasonal_1 Loss: 0.0487 | 0.0521
Epoch 33/300, seasonal_1 Loss: 0.0482 | 0.0518
Epoch 34/300, seasonal_1 Loss: 0.0477 | 0.0515
Epoch 35/300, seasonal_1 Loss: 0.0473 | 0.0511
Epoch 36/300, seasonal_1 Loss: 0.0468 | 0.0508
Epoch 37/300, seasonal_1 Loss: 0.0464 | 0.0506
Epoch 38/300, seasonal_1 Loss: 0.0460 | 0.0503
Epoch 39/300, seasonal_1 Loss: 0.0457 | 0.0500
Epoch 40/300, seasonal_1 Loss: 0.0453 | 0.0499
Epoch 41/300, seasonal_1 Loss: 0.0450 | 0.0496
Epoch 42/300, seasonal_1 Loss: 0.0447 | 0.0494
Epoch 43/300, seasonal_1 Loss: 0.0444 | 0.0492
Epoch 44/300, seasonal_1 Loss: 0.0441 | 0.0490
Epoch 45/300, seasonal_1 Loss: 0.0439 | 0.0489
Epoch 46/300, seasonal_1 Loss: 0.0437 | 0.0487
Epoch 47/300, seasonal_1 Loss: 0.0434 | 0.0485
Epoch 48/300, seasonal_1 Loss: 0.0432 | 0.0484
Epoch 49/300, seasonal_1 Loss: 0.0430 | 0.0482
Epoch 50/300, seasonal_1 Loss: 0.0428 | 0.0480
Epoch 51/300, seasonal_1 Loss: 0.0427 | 0.0479
Epoch 52/300, seasonal_1 Loss: 0.0425 | 0.0477
Epoch 53/300, seasonal_1 Loss: 0.0423 | 0.0476
Epoch 54/300, seasonal_1 Loss: 0.0422 | 0.0474
Epoch 55/300, seasonal_1 Loss: 0.0421 | 0.0473
Epoch 56/300, seasonal_1 Loss: 0.0419 | 0.0472
Epoch 57/300, seasonal_1 Loss: 0.0418 | 0.0471
Epoch 58/300, seasonal_1 Loss: 0.0417 | 0.0469
Epoch 59/300, seasonal_1 Loss: 0.0416 | 0.0468
Epoch 60/300, seasonal_1 Loss: 0.0415 | 0.0467
Epoch 61/300, seasonal_1 Loss: 0.0414 | 0.0466
Epoch 62/300, seasonal_1 Loss: 0.0413 | 0.0465
Epoch 63/300, seasonal_1 Loss: 0.0412 | 0.0464
Epoch 64/300, seasonal_1 Loss: 0.0411 | 0.0463
Epoch 65/300, seasonal_1 Loss: 0.0410 | 0.0463
Epoch 66/300, seasonal_1 Loss: 0.0410 | 0.0462
Epoch 67/300, seasonal_1 Loss: 0.0409 | 0.0462
Epoch 68/300, seasonal_1 Loss: 0.0408 | 0.0461
Epoch 69/300, seasonal_1 Loss: 0.0408 | 0.0462
Epoch 70/300, seasonal_1 Loss: 0.0407 | 0.0462
Epoch 71/300, seasonal_1 Loss: 0.0406 | 0.0462
Epoch 72/300, seasonal_1 Loss: 0.0406 | 0.0463
Epoch 73/300, seasonal_1 Loss: 0.0405 | 0.0464
Epoch 74/300, seasonal_1 Loss: 0.0404 | 0.0464
Epoch 75/300, seasonal_1 Loss: 0.0404 | 0.0465
Epoch 76/300, seasonal_1 Loss: 0.0403 | 0.0465
Epoch 77/300, seasonal_1 Loss: 0.0403 | 0.0465
Epoch 78/300, seasonal_1 Loss: 0.0402 | 0.0465
Epoch 79/300, seasonal_1 Loss: 0.0402 | 0.0465
Epoch 80/300, seasonal_1 Loss: 0.0401 | 0.0465
Epoch 81/300, seasonal_1 Loss: 0.0401 | 0.0465
Epoch 82/300, seasonal_1 Loss: 0.0400 | 0.0464
Epoch 83/300, seasonal_1 Loss: 0.0400 | 0.0464
Epoch 84/300, seasonal_1 Loss: 0.0399 | 0.0463
Epoch 85/300, seasonal_1 Loss: 0.0399 | 0.0463
Epoch 86/300, seasonal_1 Loss: 0.0399 | 0.0463
Epoch 87/300, seasonal_1 Loss: 0.0398 | 0.0462
Epoch 88/300, seasonal_1 Loss: 0.0398 | 0.0462
Epoch 89/300, seasonal_1 Loss: 0.0398 | 0.0462
Epoch 90/300, seasonal_1 Loss: 0.0398 | 0.0461
Epoch 91/300, seasonal_1 Loss: 0.0397 | 0.0461
Epoch 92/300, seasonal_1 Loss: 0.0397 | 0.0461
Epoch 93/300, seasonal_1 Loss: 0.0397 | 0.0460
Epoch 94/300, seasonal_1 Loss: 0.0396 | 0.0460
Epoch 95/300, seasonal_1 Loss: 0.0396 | 0.0460
Epoch 96/300, seasonal_1 Loss: 0.0396 | 0.0460
Epoch 97/300, seasonal_1 Loss: 0.0396 | 0.0459
Epoch 98/300, seasonal_1 Loss: 0.0396 | 0.0459
Epoch 99/300, seasonal_1 Loss: 0.0395 | 0.0459
Epoch 100/300, seasonal_1 Loss: 0.0395 | 0.0459
Epoch 101/300, seasonal_1 Loss: 0.0395 | 0.0459
Epoch 102/300, seasonal_1 Loss: 0.0395 | 0.0459
Epoch 103/300, seasonal_1 Loss: 0.0395 | 0.0458
Epoch 104/300, seasonal_1 Loss: 0.0395 | 0.0458
Epoch 105/300, seasonal_1 Loss: 0.0394 | 0.0458
Epoch 106/300, seasonal_1 Loss: 0.0394 | 0.0458
Epoch 107/300, seasonal_1 Loss: 0.0394 | 0.0458
Epoch 108/300, seasonal_1 Loss: 0.0394 | 0.0457
Epoch 109/300, seasonal_1 Loss: 0.0394 | 0.0457
Epoch 110/300, seasonal_1 Loss: 0.0394 | 0.0457
Epoch 111/300, seasonal_1 Loss: 0.0394 | 0.0457
Epoch 112/300, seasonal_1 Loss: 0.0394 | 0.0457
Epoch 113/300, seasonal_1 Loss: 0.0394 | 0.0457
Epoch 114/300, seasonal_1 Loss: 0.0393 | 0.0456
Epoch 115/300, seasonal_1 Loss: 0.0393 | 0.0456
Epoch 116/300, seasonal_1 Loss: 0.0393 | 0.0456
Epoch 117/300, seasonal_1 Loss: 0.0393 | 0.0456
Epoch 118/300, seasonal_1 Loss: 0.0393 | 0.0456
Epoch 119/300, seasonal_1 Loss: 0.0393 | 0.0456
Epoch 120/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 121/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 122/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 123/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 124/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 125/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 126/300, seasonal_1 Loss: 0.0393 | 0.0455
Epoch 127/300, seasonal_1 Loss: 0.0392 | 0.0455
Epoch 128/300, seasonal_1 Loss: 0.0392 | 0.0455
Epoch 129/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 130/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 131/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 132/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 133/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 134/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 135/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 136/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 137/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 138/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 139/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 140/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 141/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 142/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 143/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 144/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 145/300, seasonal_1 Loss: 0.0392 | 0.0454
Epoch 146/300, seasonal_1 Loss: 0.0391 | 0.0454
Epoch 147/300, seasonal_1 Loss: 0.0391 | 0.0454
Epoch 148/300, seasonal_1 Loss: 0.0391 | 0.0454
Epoch 149/300, seasonal_1 Loss: 0.0391 | 0.0454
Epoch 150/300, seasonal_1 Loss: 0.0391 | 0.0454
Epoch 151/300, seasonal_1 Loss: 0.0391 | 0.0454
Epoch 152/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 153/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 154/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 155/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 156/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 157/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 158/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 159/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 160/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 161/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 162/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 163/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 164/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 165/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 166/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 167/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 168/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 169/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 170/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 171/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 172/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 173/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 174/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 175/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 176/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 177/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 178/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 179/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 180/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 181/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 182/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 183/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 184/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 185/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 186/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 187/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 188/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 189/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 190/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 191/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 192/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 193/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 194/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 195/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 196/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 197/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 198/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 199/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 200/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 201/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 202/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 203/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 204/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 205/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 206/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 207/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 208/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 209/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 210/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 211/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 212/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 213/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 214/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 215/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 216/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 217/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 218/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 219/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 220/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 221/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 222/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 223/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 224/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 225/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 226/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 227/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 228/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 229/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 230/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 231/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 232/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 233/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 234/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 235/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 236/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 237/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 238/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 239/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 240/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 241/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 242/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 243/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 244/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 245/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 246/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 247/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 248/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 249/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 250/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 251/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 252/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 253/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 254/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 255/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 256/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 257/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 258/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 259/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 260/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 261/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 262/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 263/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 264/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 265/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 266/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 267/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 268/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 269/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 270/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 271/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 272/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 273/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 274/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 275/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 276/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 277/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 278/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 279/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 280/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 281/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 282/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 283/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 284/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 285/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 286/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 287/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 288/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 289/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 290/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 291/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 292/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 293/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 294/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 295/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 296/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 297/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 298/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 299/300, seasonal_1 Loss: 0.0391 | 0.0453
Epoch 300/300, seasonal_1 Loss: 0.0391 | 0.0453
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 16, 'train_rates': 0.8774208371661724, 'learning_rate': 0.0001334945404965518, 'batch_size': 59, 'step_size': 13, 'gamma': 0.9761780892187532}
Epoch 1/300, seasonal_2 Loss: 0.2131 | 0.1519
Epoch 2/300, seasonal_2 Loss: 0.1274 | 0.1095
Epoch 3/300, seasonal_2 Loss: 0.1233 | 0.1877
Epoch 4/300, seasonal_2 Loss: 0.1229 | 0.3157
Epoch 5/300, seasonal_2 Loss: 0.1041 | 0.1242
Epoch 6/300, seasonal_2 Loss: 0.0829 | 0.0911
Epoch 7/300, seasonal_2 Loss: 0.0816 | 0.0831
Epoch 8/300, seasonal_2 Loss: 0.0852 | 0.0783
Epoch 9/300, seasonal_2 Loss: 0.0912 | 0.0797
Epoch 10/300, seasonal_2 Loss: 0.0916 | 0.0738
Epoch 11/300, seasonal_2 Loss: 0.0806 | 0.0673
Epoch 12/300, seasonal_2 Loss: 0.0707 | 0.0665
Epoch 13/300, seasonal_2 Loss: 0.0715 | 0.0675
Epoch 14/300, seasonal_2 Loss: 0.0747 | 0.0714
Epoch 15/300, seasonal_2 Loss: 0.0727 | 0.0664
Epoch 16/300, seasonal_2 Loss: 0.0669 | 0.0622
Epoch 17/300, seasonal_2 Loss: 0.0641 | 0.0610
Epoch 18/300, seasonal_2 Loss: 0.0636 | 0.0609
Epoch 19/300, seasonal_2 Loss: 0.0629 | 0.0601
Epoch 20/300, seasonal_2 Loss: 0.0613 | 0.0586
Epoch 21/300, seasonal_2 Loss: 0.0589 | 0.0569
Epoch 22/300, seasonal_2 Loss: 0.0567 | 0.0554
Epoch 23/300, seasonal_2 Loss: 0.0549 | 0.0544
Epoch 24/300, seasonal_2 Loss: 0.0534 | 0.0536
Epoch 25/300, seasonal_2 Loss: 0.0521 | 0.0525
Epoch 26/300, seasonal_2 Loss: 0.0507 | 0.0514
Epoch 27/300, seasonal_2 Loss: 0.0492 | 0.0503
Epoch 28/300, seasonal_2 Loss: 0.0482 | 0.0493
Epoch 29/300, seasonal_2 Loss: 0.0472 | 0.0483
Epoch 30/300, seasonal_2 Loss: 0.0462 | 0.0474
Epoch 31/300, seasonal_2 Loss: 0.0453 | 0.0464
Epoch 32/300, seasonal_2 Loss: 0.0443 | 0.0455
Epoch 33/300, seasonal_2 Loss: 0.0432 | 0.0446
Epoch 34/300, seasonal_2 Loss: 0.0419 | 0.0438
Epoch 35/300, seasonal_2 Loss: 0.0406 | 0.0434
Epoch 36/300, seasonal_2 Loss: 0.0404 | 0.0472
Epoch 37/300, seasonal_2 Loss: 0.0399 | 0.0488
Epoch 38/300, seasonal_2 Loss: 0.0386 | 0.0463
Epoch 39/300, seasonal_2 Loss: 0.0372 | 0.0439
Epoch 40/300, seasonal_2 Loss: 0.0366 | 0.0423
Epoch 41/300, seasonal_2 Loss: 0.0364 | 0.0411
Epoch 42/300, seasonal_2 Loss: 0.0363 | 0.0408
Epoch 43/300, seasonal_2 Loss: 0.0363 | 0.0420
Epoch 44/300, seasonal_2 Loss: 0.0362 | 0.0434
Epoch 45/300, seasonal_2 Loss: 0.0361 | 0.0451
Epoch 46/300, seasonal_2 Loss: 0.0356 | 0.0423
Epoch 47/300, seasonal_2 Loss: 0.0342 | 0.0396
Epoch 48/300, seasonal_2 Loss: 0.0332 | 0.0389
Epoch 49/300, seasonal_2 Loss: 0.0331 | 0.0395
Epoch 50/300, seasonal_2 Loss: 0.0331 | 0.0408
Epoch 51/300, seasonal_2 Loss: 0.0332 | 0.0426
Epoch 52/300, seasonal_2 Loss: 0.0332 | 0.0447
Epoch 53/300, seasonal_2 Loss: 0.0330 | 0.0468
Epoch 54/300, seasonal_2 Loss: 0.0323 | 0.0464
Epoch 55/300, seasonal_2 Loss: 0.0315 | 0.0449
Epoch 56/300, seasonal_2 Loss: 0.0308 | 0.0433
Epoch 57/300, seasonal_2 Loss: 0.0305 | 0.0418
Epoch 58/300, seasonal_2 Loss: 0.0306 | 0.0405
Epoch 59/300, seasonal_2 Loss: 0.0312 | 0.0392
Epoch 60/300, seasonal_2 Loss: 0.0322 | 0.0386
Epoch 61/300, seasonal_2 Loss: 0.0334 | 0.0406
Epoch 62/300, seasonal_2 Loss: 0.0393 | 0.0540
Epoch 63/300, seasonal_2 Loss: 0.0427 | 0.0429
Epoch 64/300, seasonal_2 Loss: 0.0425 | 0.0470
Epoch 65/300, seasonal_2 Loss: 0.0392 | 0.0466
Epoch 66/300, seasonal_2 Loss: 0.0349 | 0.0461
Epoch 67/300, seasonal_2 Loss: 0.0314 | 0.0426
Epoch 68/300, seasonal_2 Loss: 0.0287 | 0.0380
Epoch 69/300, seasonal_2 Loss: 0.0276 | 0.0357
Epoch 70/300, seasonal_2 Loss: 0.0274 | 0.0355
Epoch 71/300, seasonal_2 Loss: 0.0273 | 0.0346
Epoch 72/300, seasonal_2 Loss: 0.0274 | 0.0356
Epoch 73/300, seasonal_2 Loss: 0.0274 | 0.0353
Epoch 74/300, seasonal_2 Loss: 0.0273 | 0.0355
Epoch 75/300, seasonal_2 Loss: 0.0272 | 0.0352
Epoch 76/300, seasonal_2 Loss: 0.0270 | 0.0350
Epoch 77/300, seasonal_2 Loss: 0.0265 | 0.0346
Epoch 78/300, seasonal_2 Loss: 0.0259 | 0.0342
Epoch 79/300, seasonal_2 Loss: 0.0255 | 0.0339
Epoch 80/300, seasonal_2 Loss: 0.0251 | 0.0336
Epoch 81/300, seasonal_2 Loss: 0.0248 | 0.0334
Epoch 82/300, seasonal_2 Loss: 0.0246 | 0.0334
Epoch 83/300, seasonal_2 Loss: 0.0249 | 0.0343
Epoch 84/300, seasonal_2 Loss: 0.0252 | 0.0350
Epoch 85/300, seasonal_2 Loss: 0.0257 | 0.0366
Epoch 86/300, seasonal_2 Loss: 0.0256 | 0.0385
Epoch 87/300, seasonal_2 Loss: 0.0251 | 0.0389
Epoch 88/300, seasonal_2 Loss: 0.0242 | 0.0380
Epoch 89/300, seasonal_2 Loss: 0.0233 | 0.0369
Epoch 90/300, seasonal_2 Loss: 0.0232 | 0.0354
Epoch 91/300, seasonal_2 Loss: 0.0229 | 0.0344
Epoch 92/300, seasonal_2 Loss: 0.0231 | 0.0336
Epoch 93/300, seasonal_2 Loss: 0.0235 | 0.0338
Epoch 94/300, seasonal_2 Loss: 0.0243 | 0.0355
Epoch 95/300, seasonal_2 Loss: 0.0251 | 0.0378
Epoch 96/300, seasonal_2 Loss: 0.0275 | 0.0391
Epoch 97/300, seasonal_2 Loss: 0.0307 | 0.0376
Epoch 98/300, seasonal_2 Loss: 0.0292 | 0.0407
Epoch 99/300, seasonal_2 Loss: 0.0259 | 0.0404
Epoch 100/300, seasonal_2 Loss: 0.0251 | 0.0403
Epoch 101/300, seasonal_2 Loss: 0.0239 | 0.0384
Epoch 102/300, seasonal_2 Loss: 0.0243 | 0.0379
Epoch 103/300, seasonal_2 Loss: 0.0227 | 0.0346
Epoch 104/300, seasonal_2 Loss: 0.0219 | 0.0343
Epoch 105/300, seasonal_2 Loss: 0.0219 | 0.0343
Epoch 106/300, seasonal_2 Loss: 0.0222 | 0.0341
Epoch 107/300, seasonal_2 Loss: 0.0222 | 0.0348
Epoch 108/300, seasonal_2 Loss: 0.0218 | 0.0338
Epoch 109/300, seasonal_2 Loss: 0.0216 | 0.0345
Epoch 110/300, seasonal_2 Loss: 0.0213 | 0.0343
Epoch 111/300, seasonal_2 Loss: 0.0215 | 0.0351
Epoch 112/300, seasonal_2 Loss: 0.0216 | 0.0360
Epoch 113/300, seasonal_2 Loss: 0.0217 | 0.0364
Epoch 114/300, seasonal_2 Loss: 0.0215 | 0.0366
Epoch 115/300, seasonal_2 Loss: 0.0212 | 0.0374
Epoch 116/300, seasonal_2 Loss: 0.0211 | 0.0378
Epoch 117/300, seasonal_2 Loss: 0.0208 | 0.0373
Epoch 118/300, seasonal_2 Loss: 0.0204 | 0.0365
Epoch 119/300, seasonal_2 Loss: 0.0200 | 0.0355
Epoch 120/300, seasonal_2 Loss: 0.0197 | 0.0349
Epoch 121/300, seasonal_2 Loss: 0.0196 | 0.0345
Epoch 122/300, seasonal_2 Loss: 0.0199 | 0.0349
Epoch 123/300, seasonal_2 Loss: 0.0201 | 0.0361
Epoch 124/300, seasonal_2 Loss: 0.0201 | 0.0369
Epoch 125/300, seasonal_2 Loss: 0.0196 | 0.0379
Epoch 126/300, seasonal_2 Loss: 0.0190 | 0.0377
Epoch 127/300, seasonal_2 Loss: 0.0185 | 0.0364
Epoch 128/300, seasonal_2 Loss: 0.0183 | 0.0351
Epoch 129/300, seasonal_2 Loss: 0.0185 | 0.0344
Epoch 130/300, seasonal_2 Loss: 0.0188 | 0.0343
Epoch 131/300, seasonal_2 Loss: 0.0196 | 0.0343
Epoch 132/300, seasonal_2 Loss: 0.0199 | 0.0356
Epoch 133/300, seasonal_2 Loss: 0.0206 | 0.0362
Epoch 134/300, seasonal_2 Loss: 0.0221 | 0.0358
Epoch 135/300, seasonal_2 Loss: 0.0210 | 0.0385
Epoch 136/300, seasonal_2 Loss: 0.0206 | 0.0377
Epoch 137/300, seasonal_2 Loss: 0.0193 | 0.0361
Epoch 138/300, seasonal_2 Loss: 0.0185 | 0.0367
Epoch 139/300, seasonal_2 Loss: 0.0185 | 0.0350
Epoch 140/300, seasonal_2 Loss: 0.0174 | 0.0354
Epoch 141/300, seasonal_2 Loss: 0.0169 | 0.0348
Epoch 142/300, seasonal_2 Loss: 0.0166 | 0.0356
Epoch 143/300, seasonal_2 Loss: 0.0165 | 0.0353
Epoch 144/300, seasonal_2 Loss: 0.0163 | 0.0357
Epoch 145/300, seasonal_2 Loss: 0.0163 | 0.0357
Epoch 146/300, seasonal_2 Loss: 0.0163 | 0.0358
Epoch 147/300, seasonal_2 Loss: 0.0164 | 0.0358
Epoch 148/300, seasonal_2 Loss: 0.0166 | 0.0366
Epoch 149/300, seasonal_2 Loss: 0.0167 | 0.0360
Epoch 150/300, seasonal_2 Loss: 0.0167 | 0.0366
Epoch 151/300, seasonal_2 Loss: 0.0163 | 0.0371
Epoch 152/300, seasonal_2 Loss: 0.0160 | 0.0370
Epoch 153/300, seasonal_2 Loss: 0.0156 | 0.0365
Epoch 154/300, seasonal_2 Loss: 0.0153 | 0.0362
Epoch 155/300, seasonal_2 Loss: 0.0151 | 0.0361
Epoch 156/300, seasonal_2 Loss: 0.0152 | 0.0368
Epoch 157/300, seasonal_2 Loss: 0.0154 | 0.0381
Epoch 158/300, seasonal_2 Loss: 0.0157 | 0.0403
Epoch 159/300, seasonal_2 Loss: 0.0156 | 0.0418
Epoch 160/300, seasonal_2 Loss: 0.0153 | 0.0417
Epoch 161/300, seasonal_2 Loss: 0.0151 | 0.0379
Epoch 162/300, seasonal_2 Loss: 0.0148 | 0.0366
Epoch 163/300, seasonal_2 Loss: 0.0154 | 0.0367
Epoch 164/300, seasonal_2 Loss: 0.0157 | 0.0386
Epoch 165/300, seasonal_2 Loss: 0.0163 | 0.0376
Epoch 166/300, seasonal_2 Loss: 0.0155 | 0.0378
Epoch 167/300, seasonal_2 Loss: 0.0147 | 0.0362
Epoch 168/300, seasonal_2 Loss: 0.0142 | 0.0384
Epoch 169/300, seasonal_2 Loss: 0.0145 | 0.0379
Epoch 170/300, seasonal_2 Loss: 0.0143 | 0.0409
Epoch 171/300, seasonal_2 Loss: 0.0146 | 0.0390
Epoch 172/300, seasonal_2 Loss: 0.0140 | 0.0396
Epoch 173/300, seasonal_2 Loss: 0.0145 | 0.0373
Epoch 174/300, seasonal_2 Loss: 0.0139 | 0.0380
Epoch 175/300, seasonal_2 Loss: 0.0138 | 0.0368
Epoch 176/300, seasonal_2 Loss: 0.0132 | 0.0377
Epoch 177/300, seasonal_2 Loss: 0.0130 | 0.0370
Epoch 178/300, seasonal_2 Loss: 0.0126 | 0.0380
Epoch 179/300, seasonal_2 Loss: 0.0124 | 0.0374
Epoch 180/300, seasonal_2 Loss: 0.0123 | 0.0383
Epoch 181/300, seasonal_2 Loss: 0.0121 | 0.0377
Epoch 182/300, seasonal_2 Loss: 0.0121 | 0.0386
Epoch 183/300, seasonal_2 Loss: 0.0120 | 0.0380
Epoch 184/300, seasonal_2 Loss: 0.0121 | 0.0388
Epoch 185/300, seasonal_2 Loss: 0.0120 | 0.0383
Epoch 186/300, seasonal_2 Loss: 0.0122 | 0.0391
Epoch 187/300, seasonal_2 Loss: 0.0120 | 0.0386
Epoch 188/300, seasonal_2 Loss: 0.0121 | 0.0392
Epoch 189/300, seasonal_2 Loss: 0.0119 | 0.0387
Epoch 190/300, seasonal_2 Loss: 0.0119 | 0.0394
Epoch 191/300, seasonal_2 Loss: 0.0117 | 0.0388
Epoch 192/300, seasonal_2 Loss: 0.0116 | 0.0397
Epoch 193/300, seasonal_2 Loss: 0.0113 | 0.0391
Epoch 194/300, seasonal_2 Loss: 0.0112 | 0.0403
Epoch 195/300, seasonal_2 Loss: 0.0112 | 0.0396
Epoch 196/300, seasonal_2 Loss: 0.0111 | 0.0412
Epoch 197/300, seasonal_2 Loss: 0.0111 | 0.0400
Epoch 198/300, seasonal_2 Loss: 0.0110 | 0.0418
Epoch 199/300, seasonal_2 Loss: 0.0109 | 0.0401
Epoch 200/300, seasonal_2 Loss: 0.0108 | 0.0418
Epoch 201/300, seasonal_2 Loss: 0.0108 | 0.0403
Epoch 202/300, seasonal_2 Loss: 0.0107 | 0.0418
Epoch 203/300, seasonal_2 Loss: 0.0107 | 0.0406
Epoch 204/300, seasonal_2 Loss: 0.0107 | 0.0419
Epoch 205/300, seasonal_2 Loss: 0.0107 | 0.0408
Epoch 206/300, seasonal_2 Loss: 0.0106 | 0.0422
Epoch 207/300, seasonal_2 Loss: 0.0107 | 0.0419
Epoch 208/300, seasonal_2 Loss: 0.0110 | 0.0428
Epoch 209/300, seasonal_2 Loss: 0.0114 | 0.0429
Epoch 210/300, seasonal_2 Loss: 0.0127 | 0.0496
Epoch 211/300, seasonal_2 Loss: 0.0132 | 0.0471
Epoch 212/300, seasonal_2 Loss: 0.0129 | 0.0422
Epoch 213/300, seasonal_2 Loss: 0.0127 | 0.0474
Epoch 214/300, seasonal_2 Loss: 0.0130 | 0.0420
Epoch 215/300, seasonal_2 Loss: 0.0111 | 0.0434
Epoch 216/300, seasonal_2 Loss: 0.0101 | 0.0414
Epoch 217/300, seasonal_2 Loss: 0.0097 | 0.0409
Epoch 218/300, seasonal_2 Loss: 0.0096 | 0.0414
Epoch 219/300, seasonal_2 Loss: 0.0094 | 0.0417
Epoch 220/300, seasonal_2 Loss: 0.0092 | 0.0413
Epoch 221/300, seasonal_2 Loss: 0.0092 | 0.0417
Epoch 222/300, seasonal_2 Loss: 0.0094 | 0.0419
Epoch 223/300, seasonal_2 Loss: 0.0092 | 0.0419
Epoch 224/300, seasonal_2 Loss: 0.0090 | 0.0416
Epoch 225/300, seasonal_2 Loss: 0.0089 | 0.0412
Epoch 226/300, seasonal_2 Loss: 0.0089 | 0.0414
Epoch 227/300, seasonal_2 Loss: 0.0090 | 0.0419
Epoch 228/300, seasonal_2 Loss: 0.0091 | 0.0425
Epoch 229/300, seasonal_2 Loss: 0.0091 | 0.0434
Epoch 230/300, seasonal_2 Loss: 0.0088 | 0.0433
Epoch 231/300, seasonal_2 Loss: 0.0088 | 0.0439
Epoch 232/300, seasonal_2 Loss: 0.0088 | 0.0429
Epoch 233/300, seasonal_2 Loss: 0.0089 | 0.0435
Epoch 234/300, seasonal_2 Loss: 0.0090 | 0.0422
Epoch 235/300, seasonal_2 Loss: 0.0090 | 0.0425
Epoch 236/300, seasonal_2 Loss: 0.0089 | 0.0420
Epoch 237/300, seasonal_2 Loss: 0.0085 | 0.0419
Epoch 238/300, seasonal_2 Loss: 0.0082 | 0.0419
Epoch 239/300, seasonal_2 Loss: 0.0081 | 0.0424
Epoch 240/300, seasonal_2 Loss: 0.0080 | 0.0431
Epoch 241/300, seasonal_2 Loss: 0.0080 | 0.0439
Epoch 242/300, seasonal_2 Loss: 0.0080 | 0.0437
Epoch 243/300, seasonal_2 Loss: 0.0078 | 0.0434
Epoch 244/300, seasonal_2 Loss: 0.0078 | 0.0425
Epoch 245/300, seasonal_2 Loss: 0.0078 | 0.0429
Epoch 246/300, seasonal_2 Loss: 0.0083 | 0.0422
Epoch 247/300, seasonal_2 Loss: 0.0084 | 0.0433
Epoch 248/300, seasonal_2 Loss: 0.0092 | 0.0426
Epoch 249/300, seasonal_2 Loss: 0.0090 | 0.0443
Epoch 250/300, seasonal_2 Loss: 0.0092 | 0.0442
Epoch 251/300, seasonal_2 Loss: 0.0094 | 0.0465
Epoch 252/300, seasonal_2 Loss: 0.0095 | 0.0442
Epoch 253/300, seasonal_2 Loss: 0.0088 | 0.0451
Epoch 254/300, seasonal_2 Loss: 0.0088 | 0.0431
Epoch 255/300, seasonal_2 Loss: 0.0080 | 0.0446
Epoch 256/300, seasonal_2 Loss: 0.0081 | 0.0442
Epoch 257/300, seasonal_2 Loss: 0.0078 | 0.0455
Epoch 258/300, seasonal_2 Loss: 0.0077 | 0.0447
Epoch 259/300, seasonal_2 Loss: 0.0076 | 0.0440
Epoch 260/300, seasonal_2 Loss: 0.0077 | 0.0436
Epoch 261/300, seasonal_2 Loss: 0.0082 | 0.0438
Epoch 262/300, seasonal_2 Loss: 0.0084 | 0.0441
Epoch 263/300, seasonal_2 Loss: 0.0083 | 0.0446
Epoch 264/300, seasonal_2 Loss: 0.0081 | 0.0444
Epoch 265/300, seasonal_2 Loss: 0.0078 | 0.0442
Epoch 266/300, seasonal_2 Loss: 0.0075 | 0.0442
Epoch 267/300, seasonal_2 Loss: 0.0073 | 0.0436
Epoch 268/300, seasonal_2 Loss: 0.0070 | 0.0437
Epoch 269/300, seasonal_2 Loss: 0.0069 | 0.0445
Epoch 270/300, seasonal_2 Loss: 0.0070 | 0.0451
Epoch 271/300, seasonal_2 Loss: 0.0071 | 0.0450
Epoch 272/300, seasonal_2 Loss: 0.0069 | 0.0443
Epoch 273/300, seasonal_2 Loss: 0.0067 | 0.0436
Epoch 274/300, seasonal_2 Loss: 0.0067 | 0.0438
Epoch 275/300, seasonal_2 Loss: 0.0069 | 0.0447
Epoch 276/300, seasonal_2 Loss: 0.0070 | 0.0450
Epoch 277/300, seasonal_2 Loss: 0.0069 | 0.0447
Epoch 278/300, seasonal_2 Loss: 0.0066 | 0.0443
Epoch 279/300, seasonal_2 Loss: 0.0063 | 0.0444
Epoch 280/300, seasonal_2 Loss: 0.0062 | 0.0445
Epoch 281/300, seasonal_2 Loss: 0.0062 | 0.0450
Epoch 282/300, seasonal_2 Loss: 0.0064 | 0.0451
Epoch 283/300, seasonal_2 Loss: 0.0063 | 0.0445
Epoch 284/300, seasonal_2 Loss: 0.0062 | 0.0441
Epoch 285/300, seasonal_2 Loss: 0.0062 | 0.0443
Epoch 286/300, seasonal_2 Loss: 0.0063 | 0.0449
Epoch 287/300, seasonal_2 Loss: 0.0065 | 0.0455
Epoch 288/300, seasonal_2 Loss: 0.0063 | 0.0450
Epoch 289/300, seasonal_2 Loss: 0.0060 | 0.0449
Epoch 290/300, seasonal_2 Loss: 0.0057 | 0.0448
Epoch 291/300, seasonal_2 Loss: 0.0057 | 0.0453
Epoch 292/300, seasonal_2 Loss: 0.0058 | 0.0450
Epoch 293/300, seasonal_2 Loss: 0.0059 | 0.0454
Epoch 294/300, seasonal_2 Loss: 0.0061 | 0.0451
Epoch 295/300, seasonal_2 Loss: 0.0063 | 0.0445
Epoch 296/300, seasonal_2 Loss: 0.0059 | 0.0445
Epoch 297/300, seasonal_2 Loss: 0.0058 | 0.0453
Epoch 298/300, seasonal_2 Loss: 0.0058 | 0.0455
Epoch 299/300, seasonal_2 Loss: 0.0058 | 0.0459
Epoch 300/300, seasonal_2 Loss: 0.0056 | 0.0448
Training seasonal_3 component with params: {'observation_period_num': 23, 'train_rates': 0.9883705645687898, 'learning_rate': 0.0001398215778848956, 'batch_size': 28, 'step_size': 9, 'gamma': 0.8633846199287517}
Epoch 1/300, seasonal_3 Loss: 0.1848 | 0.1525
Epoch 2/300, seasonal_3 Loss: 0.1074 | 0.1024
Epoch 3/300, seasonal_3 Loss: 0.0854 | 0.0855
Epoch 4/300, seasonal_3 Loss: 0.0750 | 0.0789
Epoch 5/300, seasonal_3 Loss: 0.0673 | 0.0737
Epoch 6/300, seasonal_3 Loss: 0.0606 | 0.0716
Epoch 7/300, seasonal_3 Loss: 0.0565 | 0.0697
Epoch 8/300, seasonal_3 Loss: 0.0531 | 0.0665
Epoch 9/300, seasonal_3 Loss: 0.0504 | 0.0636
Epoch 10/300, seasonal_3 Loss: 0.0480 | 0.0586
Epoch 11/300, seasonal_3 Loss: 0.0463 | 0.0585
Epoch 12/300, seasonal_3 Loss: 0.0447 | 0.0574
Epoch 13/300, seasonal_3 Loss: 0.0432 | 0.0560
Epoch 14/300, seasonal_3 Loss: 0.0419 | 0.0557
Epoch 15/300, seasonal_3 Loss: 0.0406 | 0.0549
Epoch 16/300, seasonal_3 Loss: 0.0396 | 0.0548
Epoch 17/300, seasonal_3 Loss: 0.0386 | 0.0539
Epoch 18/300, seasonal_3 Loss: 0.0377 | 0.0533
Epoch 19/300, seasonal_3 Loss: 0.0368 | 0.0517
Epoch 20/300, seasonal_3 Loss: 0.0365 | 0.0522
Epoch 21/300, seasonal_3 Loss: 0.0357 | 0.0515
Epoch 22/300, seasonal_3 Loss: 0.0354 | 0.0513
Epoch 23/300, seasonal_3 Loss: 0.0347 | 0.0529
Epoch 24/300, seasonal_3 Loss: 0.0346 | 0.0508
Epoch 25/300, seasonal_3 Loss: 0.0345 | 0.0501
Epoch 26/300, seasonal_3 Loss: 0.0333 | 0.0474
Epoch 27/300, seasonal_3 Loss: 0.0331 | 0.0473
Epoch 28/300, seasonal_3 Loss: 0.0327 | 0.0472
Epoch 29/300, seasonal_3 Loss: 0.0322 | 0.0465
Epoch 30/300, seasonal_3 Loss: 0.0318 | 0.0461
Epoch 31/300, seasonal_3 Loss: 0.0314 | 0.0454
Epoch 32/300, seasonal_3 Loss: 0.0311 | 0.0448
Epoch 33/300, seasonal_3 Loss: 0.0310 | 0.0451
Epoch 34/300, seasonal_3 Loss: 0.0305 | 0.0452
Epoch 35/300, seasonal_3 Loss: 0.0300 | 0.0445
Epoch 36/300, seasonal_3 Loss: 0.0296 | 0.0441
Epoch 37/300, seasonal_3 Loss: 0.0293 | 0.0424
Epoch 38/300, seasonal_3 Loss: 0.0289 | 0.0424
Epoch 39/300, seasonal_3 Loss: 0.0284 | 0.0419
Epoch 40/300, seasonal_3 Loss: 0.0280 | 0.0415
Epoch 41/300, seasonal_3 Loss: 0.0276 | 0.0411
Epoch 42/300, seasonal_3 Loss: 0.0273 | 0.0387
Epoch 43/300, seasonal_3 Loss: 0.0269 | 0.0381
Epoch 44/300, seasonal_3 Loss: 0.0265 | 0.0373
Epoch 45/300, seasonal_3 Loss: 0.0262 | 0.0365
Epoch 46/300, seasonal_3 Loss: 0.0259 | 0.0354
Epoch 47/300, seasonal_3 Loss: 0.0256 | 0.0349
Epoch 48/300, seasonal_3 Loss: 0.0253 | 0.0339
Epoch 49/300, seasonal_3 Loss: 0.0250 | 0.0330
Epoch 50/300, seasonal_3 Loss: 0.0247 | 0.0321
Epoch 51/300, seasonal_3 Loss: 0.0244 | 0.0313
Epoch 52/300, seasonal_3 Loss: 0.0241 | 0.0309
Epoch 53/300, seasonal_3 Loss: 0.0239 | 0.0303
Epoch 54/300, seasonal_3 Loss: 0.0236 | 0.0298
Epoch 55/300, seasonal_3 Loss: 0.0234 | 0.0294
Epoch 56/300, seasonal_3 Loss: 0.0232 | 0.0292
Epoch 57/300, seasonal_3 Loss: 0.0230 | 0.0289
Epoch 58/300, seasonal_3 Loss: 0.0228 | 0.0288
Epoch 59/300, seasonal_3 Loss: 0.0226 | 0.0286
Epoch 60/300, seasonal_3 Loss: 0.0224 | 0.0290
Epoch 61/300, seasonal_3 Loss: 0.0222 | 0.0289
Epoch 62/300, seasonal_3 Loss: 0.0221 | 0.0289
Epoch 63/300, seasonal_3 Loss: 0.0219 | 0.0289
Epoch 64/300, seasonal_3 Loss: 0.0218 | 0.0295
Epoch 65/300, seasonal_3 Loss: 0.0217 | 0.0296
Epoch 66/300, seasonal_3 Loss: 0.0215 | 0.0296
Epoch 67/300, seasonal_3 Loss: 0.0214 | 0.0297
Epoch 68/300, seasonal_3 Loss: 0.0213 | 0.0297
Epoch 69/300, seasonal_3 Loss: 0.0212 | 0.0303
Epoch 70/300, seasonal_3 Loss: 0.0211 | 0.0305
Epoch 71/300, seasonal_3 Loss: 0.0210 | 0.0306
Epoch 72/300, seasonal_3 Loss: 0.0208 | 0.0307
Epoch 73/300, seasonal_3 Loss: 0.0208 | 0.0312
Epoch 74/300, seasonal_3 Loss: 0.0207 | 0.0314
Epoch 75/300, seasonal_3 Loss: 0.0206 | 0.0315
Epoch 76/300, seasonal_3 Loss: 0.0205 | 0.0316
Epoch 77/300, seasonal_3 Loss: 0.0204 | 0.0317
Epoch 78/300, seasonal_3 Loss: 0.0204 | 0.0319
Epoch 79/300, seasonal_3 Loss: 0.0203 | 0.0321
Epoch 80/300, seasonal_3 Loss: 0.0202 | 0.0321
Epoch 81/300, seasonal_3 Loss: 0.0201 | 0.0321
Epoch 82/300, seasonal_3 Loss: 0.0202 | 0.0324
Epoch 83/300, seasonal_3 Loss: 0.0202 | 0.0332
Epoch 84/300, seasonal_3 Loss: 0.0201 | 0.0339
Epoch 85/300, seasonal_3 Loss: 0.0200 | 0.0339
Epoch 86/300, seasonal_3 Loss: 0.0199 | 0.0335
Epoch 87/300, seasonal_3 Loss: 0.0198 | 0.0324
Epoch 88/300, seasonal_3 Loss: 0.0197 | 0.0319
Epoch 89/300, seasonal_3 Loss: 0.0195 | 0.0315
Epoch 90/300, seasonal_3 Loss: 0.0194 | 0.0313
Epoch 91/300, seasonal_3 Loss: 0.0194 | 0.0308
Epoch 92/300, seasonal_3 Loss: 0.0193 | 0.0307
Epoch 93/300, seasonal_3 Loss: 0.0192 | 0.0307
Epoch 94/300, seasonal_3 Loss: 0.0191 | 0.0307
Epoch 95/300, seasonal_3 Loss: 0.0190 | 0.0307
Epoch 96/300, seasonal_3 Loss: 0.0190 | 0.0305
Epoch 97/300, seasonal_3 Loss: 0.0189 | 0.0305
Epoch 98/300, seasonal_3 Loss: 0.0188 | 0.0305
Epoch 99/300, seasonal_3 Loss: 0.0188 | 0.0305
Epoch 100/300, seasonal_3 Loss: 0.0187 | 0.0304
Epoch 101/300, seasonal_3 Loss: 0.0187 | 0.0304
Epoch 102/300, seasonal_3 Loss: 0.0186 | 0.0304
Epoch 103/300, seasonal_3 Loss: 0.0186 | 0.0304
Epoch 104/300, seasonal_3 Loss: 0.0185 | 0.0305
Epoch 105/300, seasonal_3 Loss: 0.0185 | 0.0304
Epoch 106/300, seasonal_3 Loss: 0.0184 | 0.0304
Epoch 107/300, seasonal_3 Loss: 0.0184 | 0.0304
Epoch 108/300, seasonal_3 Loss: 0.0184 | 0.0304
Epoch 109/300, seasonal_3 Loss: 0.0183 | 0.0304
Epoch 110/300, seasonal_3 Loss: 0.0183 | 0.0304
Epoch 111/300, seasonal_3 Loss: 0.0183 | 0.0304
Epoch 112/300, seasonal_3 Loss: 0.0182 | 0.0304
Epoch 113/300, seasonal_3 Loss: 0.0182 | 0.0305
Epoch 114/300, seasonal_3 Loss: 0.0182 | 0.0304
Epoch 115/300, seasonal_3 Loss: 0.0181 | 0.0305
Epoch 116/300, seasonal_3 Loss: 0.0181 | 0.0305
Epoch 117/300, seasonal_3 Loss: 0.0181 | 0.0305
Epoch 118/300, seasonal_3 Loss: 0.0181 | 0.0305
Epoch 119/300, seasonal_3 Loss: 0.0180 | 0.0305
Epoch 120/300, seasonal_3 Loss: 0.0180 | 0.0305
Epoch 121/300, seasonal_3 Loss: 0.0180 | 0.0305
Epoch 122/300, seasonal_3 Loss: 0.0180 | 0.0305
Epoch 123/300, seasonal_3 Loss: 0.0179 | 0.0305
Epoch 124/300, seasonal_3 Loss: 0.0179 | 0.0305
Epoch 125/300, seasonal_3 Loss: 0.0179 | 0.0305
Epoch 126/300, seasonal_3 Loss: 0.0179 | 0.0305
Epoch 127/300, seasonal_3 Loss: 0.0179 | 0.0306
Epoch 128/300, seasonal_3 Loss: 0.0179 | 0.0306
Epoch 129/300, seasonal_3 Loss: 0.0178 | 0.0306
Epoch 130/300, seasonal_3 Loss: 0.0178 | 0.0306
Epoch 131/300, seasonal_3 Loss: 0.0178 | 0.0306
Epoch 132/300, seasonal_3 Loss: 0.0178 | 0.0305
Epoch 133/300, seasonal_3 Loss: 0.0178 | 0.0305
Epoch 134/300, seasonal_3 Loss: 0.0178 | 0.0305
Epoch 135/300, seasonal_3 Loss: 0.0178 | 0.0305
Epoch 136/300, seasonal_3 Loss: 0.0178 | 0.0304
Epoch 137/300, seasonal_3 Loss: 0.0178 | 0.0304
Epoch 138/300, seasonal_3 Loss: 0.0178 | 0.0305
Epoch 139/300, seasonal_3 Loss: 0.0177 | 0.0305
Epoch 140/300, seasonal_3 Loss: 0.0177 | 0.0305
Epoch 141/300, seasonal_3 Loss: 0.0177 | 0.0306
Epoch 142/300, seasonal_3 Loss: 0.0177 | 0.0306
Epoch 143/300, seasonal_3 Loss: 0.0176 | 0.0306
Epoch 144/300, seasonal_3 Loss: 0.0176 | 0.0306
Epoch 145/300, seasonal_3 Loss: 0.0176 | 0.0307
Epoch 146/300, seasonal_3 Loss: 0.0176 | 0.0307
Epoch 147/300, seasonal_3 Loss: 0.0175 | 0.0307
Epoch 148/300, seasonal_3 Loss: 0.0175 | 0.0307
Epoch 149/300, seasonal_3 Loss: 0.0175 | 0.0307
Epoch 150/300, seasonal_3 Loss: 0.0175 | 0.0307
Epoch 151/300, seasonal_3 Loss: 0.0175 | 0.0307
Epoch 152/300, seasonal_3 Loss: 0.0175 | 0.0307
Epoch 153/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 154/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 155/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 156/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 157/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 158/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 159/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 160/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 161/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 162/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 163/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 164/300, seasonal_3 Loss: 0.0174 | 0.0307
Epoch 165/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 166/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 167/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 168/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 169/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 170/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 171/300, seasonal_3 Loss: 0.0173 | 0.0307
Epoch 172/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 173/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 174/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 175/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 176/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 177/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 178/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 179/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 180/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 181/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 182/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 183/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 184/300, seasonal_3 Loss: 0.0173 | 0.0308
Epoch 185/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 186/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 187/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 188/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 189/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 190/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 191/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 192/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 193/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 194/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 195/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 196/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 197/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 198/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 199/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 200/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 201/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 202/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 203/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 204/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 205/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 206/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 207/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 208/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 209/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 210/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 211/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 212/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 213/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 214/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 215/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 216/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 217/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 218/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 219/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 220/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 221/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 222/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 223/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 224/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 225/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 226/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 227/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 228/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 229/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 230/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 231/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 232/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 233/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 234/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 235/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 236/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 237/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 238/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 239/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 240/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 241/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 242/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 243/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 244/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 245/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 246/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 247/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 248/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 249/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 250/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 251/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 252/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 253/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 254/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 255/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 256/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 257/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 258/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 259/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 260/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 261/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 262/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 263/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 264/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 265/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 266/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 267/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 268/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 269/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 270/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 271/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 272/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 273/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 274/300, seasonal_3 Loss: 0.0172 | 0.0308
Epoch 275/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 276/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 277/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 278/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 279/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 280/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 281/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 282/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 283/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 284/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 285/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 286/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 287/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 288/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 289/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 290/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 291/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 292/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 293/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 294/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 295/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 296/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 297/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 298/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 299/300, seasonal_3 Loss: 0.0171 | 0.0308
Epoch 300/300, seasonal_3 Loss: 0.0171 | 0.0308
Training resid component with params: {'observation_period_num': 18, 'train_rates': 0.9125719003888111, 'learning_rate': 0.0007502021792523045, 'batch_size': 243, 'step_size': 12, 'gamma': 0.7955825522976268}
Epoch 1/300, resid Loss: 0.6515 | 0.2489
Epoch 2/300, resid Loss: 0.2950 | 0.4227
Epoch 3/300, resid Loss: 0.4088 | 0.2484
Epoch 4/300, resid Loss: 0.2070 | 0.2389
Epoch 5/300, resid Loss: 0.2160 | 0.2516
Epoch 6/300, resid Loss: 0.1396 | 0.1519
Epoch 7/300, resid Loss: 0.1205 | 0.1388
Epoch 8/300, resid Loss: 0.1000 | 0.1111
Epoch 9/300, resid Loss: 0.0894 | 0.1016
Epoch 10/300, resid Loss: 0.0809 | 0.1025
Epoch 11/300, resid Loss: 0.0816 | 0.0778
Epoch 12/300, resid Loss: 0.0906 | 0.0968
Epoch 13/300, resid Loss: 0.0925 | 0.0956
Epoch 14/300, resid Loss: 0.0755 | 0.1025
Epoch 15/300, resid Loss: 0.0867 | 0.0746
Epoch 16/300, resid Loss: 0.0831 | 0.1262
Epoch 17/300, resid Loss: 0.0815 | 0.0725
Epoch 18/300, resid Loss: 0.0719 | 0.1146
Epoch 19/300, resid Loss: 0.0734 | 0.0676
Epoch 20/300, resid Loss: 0.0718 | 0.1167
Epoch 21/300, resid Loss: 0.0894 | 0.0948
Epoch 22/300, resid Loss: 0.1067 | 0.1348
Epoch 23/300, resid Loss: 0.1459 | 0.2958
Epoch 24/300, resid Loss: 0.1440 | 0.1309
Epoch 25/300, resid Loss: 0.1003 | 0.1559
Epoch 26/300, resid Loss: 0.0797 | 0.0965
Epoch 27/300, resid Loss: 0.0728 | 0.1028
Epoch 28/300, resid Loss: 0.0665 | 0.0835
Epoch 29/300, resid Loss: 0.0653 | 0.0863
Epoch 30/300, resid Loss: 0.0628 | 0.0745
Epoch 31/300, resid Loss: 0.0647 | 0.0911
Epoch 32/300, resid Loss: 0.0633 | 0.0670
Epoch 33/300, resid Loss: 0.0681 | 0.1066
Epoch 34/300, resid Loss: 0.0654 | 0.0660
Epoch 35/300, resid Loss: 0.0681 | 0.1197
Epoch 36/300, resid Loss: 0.0625 | 0.0652
Epoch 37/300, resid Loss: 0.0598 | 0.1063
Epoch 38/300, resid Loss: 0.0540 | 0.0631
Epoch 39/300, resid Loss: 0.0525 | 0.0748
Epoch 40/300, resid Loss: 0.0502 | 0.0617
Epoch 41/300, resid Loss: 0.0494 | 0.0653
Epoch 42/300, resid Loss: 0.0480 | 0.0586
Epoch 43/300, resid Loss: 0.0476 | 0.0640
Epoch 44/300, resid Loss: 0.0464 | 0.0560
Epoch 45/300, resid Loss: 0.0462 | 0.0617
Epoch 46/300, resid Loss: 0.0452 | 0.0545
Epoch 47/300, resid Loss: 0.0452 | 0.0601
Epoch 48/300, resid Loss: 0.0444 | 0.0531
Epoch 49/300, resid Loss: 0.0444 | 0.0610
Epoch 50/300, resid Loss: 0.0434 | 0.0515
Epoch 51/300, resid Loss: 0.0433 | 0.0584
Epoch 52/300, resid Loss: 0.0425 | 0.0513
Epoch 53/300, resid Loss: 0.0424 | 0.0562
Epoch 54/300, resid Loss: 0.0419 | 0.0511
Epoch 55/300, resid Loss: 0.0416 | 0.0553
Epoch 56/300, resid Loss: 0.0411 | 0.0509
Epoch 57/300, resid Loss: 0.0409 | 0.0531
Epoch 58/300, resid Loss: 0.0406 | 0.0512
Epoch 59/300, resid Loss: 0.0405 | 0.0519
Epoch 60/300, resid Loss: 0.0402 | 0.0511
Epoch 61/300, resid Loss: 0.0400 | 0.0513
Epoch 62/300, resid Loss: 0.0397 | 0.0509
Epoch 63/300, resid Loss: 0.0396 | 0.0508
Epoch 64/300, resid Loss: 0.0394 | 0.0506
Epoch 65/300, resid Loss: 0.0393 | 0.0504
Epoch 66/300, resid Loss: 0.0391 | 0.0502
Epoch 67/300, resid Loss: 0.0389 | 0.0501
Epoch 68/300, resid Loss: 0.0387 | 0.0499
Epoch 69/300, resid Loss: 0.0386 | 0.0498
Epoch 70/300, resid Loss: 0.0384 | 0.0496
Epoch 71/300, resid Loss: 0.0383 | 0.0495
Epoch 72/300, resid Loss: 0.0382 | 0.0494
Epoch 73/300, resid Loss: 0.0380 | 0.0492
Epoch 74/300, resid Loss: 0.0379 | 0.0492
Epoch 75/300, resid Loss: 0.0378 | 0.0490
Epoch 76/300, resid Loss: 0.0377 | 0.0489
Epoch 77/300, resid Loss: 0.0376 | 0.0488
Epoch 78/300, resid Loss: 0.0375 | 0.0487
Epoch 79/300, resid Loss: 0.0374 | 0.0486
Epoch 80/300, resid Loss: 0.0373 | 0.0485
Epoch 81/300, resid Loss: 0.0372 | 0.0484
Epoch 82/300, resid Loss: 0.0371 | 0.0483
Epoch 83/300, resid Loss: 0.0370 | 0.0482
Epoch 84/300, resid Loss: 0.0369 | 0.0482
Epoch 85/300, resid Loss: 0.0369 | 0.0481
Epoch 86/300, resid Loss: 0.0368 | 0.0480
Epoch 87/300, resid Loss: 0.0367 | 0.0479
Epoch 88/300, resid Loss: 0.0367 | 0.0479
Epoch 89/300, resid Loss: 0.0366 | 0.0478
Epoch 90/300, resid Loss: 0.0365 | 0.0477
Epoch 91/300, resid Loss: 0.0364 | 0.0477
Epoch 92/300, resid Loss: 0.0364 | 0.0476
Epoch 93/300, resid Loss: 0.0363 | 0.0476
Epoch 94/300, resid Loss: 0.0363 | 0.0475
Epoch 95/300, resid Loss: 0.0362 | 0.0474
Epoch 96/300, resid Loss: 0.0362 | 0.0474
Epoch 97/300, resid Loss: 0.0361 | 0.0474
Epoch 98/300, resid Loss: 0.0361 | 0.0473
Epoch 99/300, resid Loss: 0.0360 | 0.0473
Epoch 100/300, resid Loss: 0.0360 | 0.0472
Epoch 101/300, resid Loss: 0.0359 | 0.0472
Epoch 102/300, resid Loss: 0.0359 | 0.0471
Epoch 103/300, resid Loss: 0.0359 | 0.0471
Epoch 104/300, resid Loss: 0.0358 | 0.0470
Epoch 105/300, resid Loss: 0.0358 | 0.0470
Epoch 106/300, resid Loss: 0.0357 | 0.0470
Epoch 107/300, resid Loss: 0.0357 | 0.0469
Epoch 108/300, resid Loss: 0.0357 | 0.0469
Epoch 109/300, resid Loss: 0.0356 | 0.0469
Epoch 110/300, resid Loss: 0.0356 | 0.0468
Epoch 111/300, resid Loss: 0.0356 | 0.0468
Epoch 112/300, resid Loss: 0.0356 | 0.0468
Epoch 113/300, resid Loss: 0.0355 | 0.0467
Epoch 114/300, resid Loss: 0.0355 | 0.0467
Epoch 115/300, resid Loss: 0.0355 | 0.0467
Epoch 116/300, resid Loss: 0.0354 | 0.0466
Epoch 117/300, resid Loss: 0.0354 | 0.0466
Epoch 118/300, resid Loss: 0.0354 | 0.0466
Epoch 119/300, resid Loss: 0.0354 | 0.0466
Epoch 120/300, resid Loss: 0.0354 | 0.0465
Epoch 121/300, resid Loss: 0.0353 | 0.0465
Epoch 122/300, resid Loss: 0.0353 | 0.0465
Epoch 123/300, resid Loss: 0.0353 | 0.0465
Epoch 124/300, resid Loss: 0.0353 | 0.0464
Epoch 125/300, resid Loss: 0.0353 | 0.0464
Epoch 126/300, resid Loss: 0.0352 | 0.0464
Epoch 127/300, resid Loss: 0.0352 | 0.0464
Epoch 128/300, resid Loss: 0.0352 | 0.0464
Epoch 129/300, resid Loss: 0.0352 | 0.0464
Epoch 130/300, resid Loss: 0.0352 | 0.0463
Epoch 131/300, resid Loss: 0.0352 | 0.0463
Epoch 132/300, resid Loss: 0.0351 | 0.0463
Epoch 133/300, resid Loss: 0.0351 | 0.0463
Epoch 134/300, resid Loss: 0.0351 | 0.0463
Epoch 135/300, resid Loss: 0.0351 | 0.0463
Epoch 136/300, resid Loss: 0.0351 | 0.0462
Epoch 137/300, resid Loss: 0.0351 | 0.0462
Epoch 138/300, resid Loss: 0.0351 | 0.0462
Epoch 139/300, resid Loss: 0.0350 | 0.0462
Epoch 140/300, resid Loss: 0.0350 | 0.0462
Epoch 141/300, resid Loss: 0.0350 | 0.0462
Epoch 142/300, resid Loss: 0.0350 | 0.0462
Epoch 143/300, resid Loss: 0.0350 | 0.0462
Epoch 144/300, resid Loss: 0.0350 | 0.0461
Epoch 145/300, resid Loss: 0.0350 | 0.0461
Epoch 146/300, resid Loss: 0.0350 | 0.0461
Epoch 147/300, resid Loss: 0.0350 | 0.0461
Epoch 148/300, resid Loss: 0.0350 | 0.0461
Epoch 149/300, resid Loss: 0.0349 | 0.0461
Epoch 150/300, resid Loss: 0.0349 | 0.0461
Epoch 151/300, resid Loss: 0.0349 | 0.0461
Epoch 152/300, resid Loss: 0.0349 | 0.0461
Epoch 153/300, resid Loss: 0.0349 | 0.0461
Epoch 154/300, resid Loss: 0.0349 | 0.0461
Epoch 155/300, resid Loss: 0.0349 | 0.0460
Epoch 156/300, resid Loss: 0.0349 | 0.0460
Epoch 157/300, resid Loss: 0.0349 | 0.0460
Epoch 158/300, resid Loss: 0.0349 | 0.0460
Epoch 159/300, resid Loss: 0.0349 | 0.0460
Epoch 160/300, resid Loss: 0.0349 | 0.0460
Epoch 161/300, resid Loss: 0.0349 | 0.0460
Epoch 162/300, resid Loss: 0.0349 | 0.0460
Epoch 163/300, resid Loss: 0.0349 | 0.0460
Epoch 164/300, resid Loss: 0.0349 | 0.0460
Epoch 165/300, resid Loss: 0.0349 | 0.0460
Epoch 166/300, resid Loss: 0.0348 | 0.0460
Epoch 167/300, resid Loss: 0.0348 | 0.0460
Epoch 168/300, resid Loss: 0.0348 | 0.0460
Epoch 169/300, resid Loss: 0.0348 | 0.0460
Epoch 170/300, resid Loss: 0.0348 | 0.0460
Epoch 171/300, resid Loss: 0.0348 | 0.0460
Epoch 172/300, resid Loss: 0.0348 | 0.0459
Epoch 173/300, resid Loss: 0.0348 | 0.0459
Epoch 174/300, resid Loss: 0.0348 | 0.0459
Epoch 175/300, resid Loss: 0.0348 | 0.0459
Epoch 176/300, resid Loss: 0.0348 | 0.0459
Epoch 177/300, resid Loss: 0.0348 | 0.0459
Epoch 178/300, resid Loss: 0.0348 | 0.0459
Epoch 179/300, resid Loss: 0.0348 | 0.0459
Epoch 180/300, resid Loss: 0.0348 | 0.0459
Epoch 181/300, resid Loss: 0.0348 | 0.0459
Epoch 182/300, resid Loss: 0.0348 | 0.0459
Epoch 183/300, resid Loss: 0.0348 | 0.0459
Epoch 184/300, resid Loss: 0.0348 | 0.0459
Epoch 185/300, resid Loss: 0.0348 | 0.0459
Epoch 186/300, resid Loss: 0.0348 | 0.0459
Epoch 187/300, resid Loss: 0.0348 | 0.0459
Epoch 188/300, resid Loss: 0.0348 | 0.0459
Epoch 189/300, resid Loss: 0.0348 | 0.0459
Epoch 190/300, resid Loss: 0.0348 | 0.0459
Epoch 191/300, resid Loss: 0.0348 | 0.0459
Epoch 192/300, resid Loss: 0.0348 | 0.0459
Epoch 193/300, resid Loss: 0.0348 | 0.0459
Epoch 194/300, resid Loss: 0.0348 | 0.0459
Epoch 195/300, resid Loss: 0.0348 | 0.0459
Epoch 196/300, resid Loss: 0.0348 | 0.0459
Epoch 197/300, resid Loss: 0.0348 | 0.0459
Epoch 198/300, resid Loss: 0.0348 | 0.0459
Epoch 199/300, resid Loss: 0.0348 | 0.0459
Epoch 200/300, resid Loss: 0.0348 | 0.0459
Epoch 201/300, resid Loss: 0.0348 | 0.0459
Epoch 202/300, resid Loss: 0.0348 | 0.0459
Epoch 203/300, resid Loss: 0.0348 | 0.0459
Epoch 204/300, resid Loss: 0.0348 | 0.0459
Epoch 205/300, resid Loss: 0.0347 | 0.0459
Epoch 206/300, resid Loss: 0.0347 | 0.0459
Epoch 207/300, resid Loss: 0.0347 | 0.0459
Epoch 208/300, resid Loss: 0.0347 | 0.0459
Epoch 209/300, resid Loss: 0.0347 | 0.0459
Epoch 210/300, resid Loss: 0.0347 | 0.0459
Epoch 211/300, resid Loss: 0.0347 | 0.0459
Epoch 212/300, resid Loss: 0.0347 | 0.0459
Epoch 213/300, resid Loss: 0.0347 | 0.0459
Epoch 214/300, resid Loss: 0.0347 | 0.0459
Epoch 215/300, resid Loss: 0.0347 | 0.0459
Epoch 216/300, resid Loss: 0.0347 | 0.0459
Epoch 217/300, resid Loss: 0.0347 | 0.0459
Epoch 218/300, resid Loss: 0.0347 | 0.0459
Epoch 219/300, resid Loss: 0.0347 | 0.0459
Epoch 220/300, resid Loss: 0.0347 | 0.0458
Epoch 221/300, resid Loss: 0.0347 | 0.0458
Epoch 222/300, resid Loss: 0.0347 | 0.0458
Epoch 223/300, resid Loss: 0.0347 | 0.0458
Epoch 224/300, resid Loss: 0.0347 | 0.0458
Epoch 225/300, resid Loss: 0.0347 | 0.0458
Epoch 226/300, resid Loss: 0.0347 | 0.0458
Epoch 227/300, resid Loss: 0.0347 | 0.0458
Epoch 228/300, resid Loss: 0.0347 | 0.0458
Epoch 229/300, resid Loss: 0.0347 | 0.0458
Epoch 230/300, resid Loss: 0.0347 | 0.0458
Epoch 231/300, resid Loss: 0.0347 | 0.0458
Epoch 232/300, resid Loss: 0.0347 | 0.0458
Epoch 233/300, resid Loss: 0.0347 | 0.0458
Epoch 234/300, resid Loss: 0.0347 | 0.0458
Epoch 235/300, resid Loss: 0.0347 | 0.0458
Epoch 236/300, resid Loss: 0.0347 | 0.0458
Epoch 237/300, resid Loss: 0.0347 | 0.0458
Epoch 238/300, resid Loss: 0.0347 | 0.0458
Epoch 239/300, resid Loss: 0.0347 | 0.0458
Epoch 240/300, resid Loss: 0.0347 | 0.0458
Epoch 241/300, resid Loss: 0.0347 | 0.0458
Epoch 242/300, resid Loss: 0.0347 | 0.0458
Epoch 243/300, resid Loss: 0.0347 | 0.0458
Epoch 244/300, resid Loss: 0.0347 | 0.0458
Epoch 245/300, resid Loss: 0.0347 | 0.0458
Epoch 246/300, resid Loss: 0.0347 | 0.0458
Epoch 247/300, resid Loss: 0.0347 | 0.0458
Epoch 248/300, resid Loss: 0.0347 | 0.0458
Epoch 249/300, resid Loss: 0.0347 | 0.0458
Epoch 250/300, resid Loss: 0.0347 | 0.0458
Epoch 251/300, resid Loss: 0.0347 | 0.0458
Epoch 252/300, resid Loss: 0.0347 | 0.0458
Epoch 253/300, resid Loss: 0.0347 | 0.0458
Epoch 254/300, resid Loss: 0.0347 | 0.0458
Epoch 255/300, resid Loss: 0.0347 | 0.0458
Epoch 256/300, resid Loss: 0.0347 | 0.0458
Epoch 257/300, resid Loss: 0.0347 | 0.0458
Epoch 258/300, resid Loss: 0.0347 | 0.0458
Epoch 259/300, resid Loss: 0.0347 | 0.0458
Epoch 260/300, resid Loss: 0.0347 | 0.0458
Epoch 261/300, resid Loss: 0.0347 | 0.0458
Epoch 262/300, resid Loss: 0.0347 | 0.0458
Epoch 263/300, resid Loss: 0.0347 | 0.0458
Epoch 264/300, resid Loss: 0.0347 | 0.0458
Epoch 265/300, resid Loss: 0.0347 | 0.0458
Epoch 266/300, resid Loss: 0.0347 | 0.0458
Epoch 267/300, resid Loss: 0.0347 | 0.0458
Epoch 268/300, resid Loss: 0.0347 | 0.0458
Epoch 269/300, resid Loss: 0.0347 | 0.0458
Epoch 270/300, resid Loss: 0.0347 | 0.0458
Epoch 271/300, resid Loss: 0.0347 | 0.0458
Epoch 272/300, resid Loss: 0.0347 | 0.0458
Epoch 273/300, resid Loss: 0.0347 | 0.0458
Epoch 274/300, resid Loss: 0.0347 | 0.0458
Epoch 275/300, resid Loss: 0.0347 | 0.0458
Epoch 276/300, resid Loss: 0.0347 | 0.0458
Epoch 277/300, resid Loss: 0.0347 | 0.0458
Epoch 278/300, resid Loss: 0.0347 | 0.0458
Epoch 279/300, resid Loss: 0.0347 | 0.0458
Epoch 280/300, resid Loss: 0.0347 | 0.0458
Epoch 281/300, resid Loss: 0.0347 | 0.0458
Epoch 282/300, resid Loss: 0.0347 | 0.0458
Epoch 283/300, resid Loss: 0.0347 | 0.0458
Epoch 284/300, resid Loss: 0.0347 | 0.0458
Epoch 285/300, resid Loss: 0.0347 | 0.0458
Epoch 286/300, resid Loss: 0.0347 | 0.0458
Epoch 287/300, resid Loss: 0.0347 | 0.0458
Epoch 288/300, resid Loss: 0.0347 | 0.0458
Epoch 289/300, resid Loss: 0.0347 | 0.0458
Epoch 290/300, resid Loss: 0.0347 | 0.0458
Epoch 291/300, resid Loss: 0.0347 | 0.0458
Epoch 292/300, resid Loss: 0.0347 | 0.0458
Epoch 293/300, resid Loss: 0.0347 | 0.0458
Epoch 294/300, resid Loss: 0.0347 | 0.0458
Epoch 295/300, resid Loss: 0.0347 | 0.0458
Epoch 296/300, resid Loss: 0.0347 | 0.0458
Epoch 297/300, resid Loss: 0.0347 | 0.0458
Epoch 298/300, resid Loss: 0.0347 | 0.0458
Epoch 299/300, resid Loss: 0.0347 | 0.0458
Epoch 300/300, resid Loss: 0.0347 | 0.0458
Runtime (seconds): 1889.2179334163666
0.00037720837755511713
[107.83229]
[0.3666687]
[1.7419068]
[0.9887707]
[6.9696016]
[2.3055787]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 4.99440928013064
RMSE: 2.2348175048828125
MAE: 2.2348175048828125
R-squared: nan
[120.20482]
