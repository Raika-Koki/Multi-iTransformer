ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 12:55:56,942][0m A new study created in memory with name: no-name-58252b97-e6bc-4b7f-bb73-b0d6e0fd07ad[0m
[32m[I 2025-01-04 12:57:32,646][0m Trial 0 finished with value: 0.44752212523299956 and parameters: {'observation_period_num': 166, 'train_rates': 0.6931584814451051, 'learning_rate': 2.9379054937099667e-06, 'batch_size': 46, 'step_size': 12, 'gamma': 0.9686200272358257}. Best is trial 0 with value: 0.44752212523299956.[0m
[32m[I 2025-01-04 12:57:56,559][0m Trial 1 finished with value: 0.8853343539565575 and parameters: {'observation_period_num': 111, 'train_rates': 0.7696632409542166, 'learning_rate': 1.4562083750036155e-06, 'batch_size': 226, 'step_size': 3, 'gamma': 0.9623916820431759}. Best is trial 0 with value: 0.44752212523299956.[0m
[32m[I 2025-01-04 12:58:41,590][0m Trial 2 finished with value: 0.04945860803127289 and parameters: {'observation_period_num': 14, 'train_rates': 0.9649426611756626, 'learning_rate': 8.124027945798299e-05, 'batch_size': 139, 'step_size': 5, 'gamma': 0.9438545312721016}. Best is trial 2 with value: 0.04945860803127289.[0m
Early stopping at epoch 95
[32m[I 2025-01-04 12:59:40,318][0m Trial 3 finished with value: 0.543447460498723 and parameters: {'observation_period_num': 199, 'train_rates': 0.7609534020128037, 'learning_rate': 2.4412023992895333e-05, 'batch_size': 79, 'step_size': 1, 'gamma': 0.8747058432044437}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:00:06,139][0m Trial 4 finished with value: 0.09081018966647376 and parameters: {'observation_period_num': 56, 'train_rates': 0.8050153776570547, 'learning_rate': 2.6610931038044825e-05, 'batch_size': 220, 'step_size': 7, 'gamma': 0.9786329993728662}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:00:49,533][0m Trial 5 finished with value: 0.4186657195856945 and parameters: {'observation_period_num': 20, 'train_rates': 0.7762904910723274, 'learning_rate': 3.466523515838825e-06, 'batch_size': 122, 'step_size': 5, 'gamma': 0.878580782222249}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:01:10,397][0m Trial 6 finished with value: 0.26983007527677355 and parameters: {'observation_period_num': 91, 'train_rates': 0.6291768666854889, 'learning_rate': 3.193727230715152e-05, 'batch_size': 226, 'step_size': 12, 'gamma': 0.9769053774251423}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:02:47,434][0m Trial 7 finished with value: 0.32954213454186737 and parameters: {'observation_period_num': 240, 'train_rates': 0.6266660904184002, 'learning_rate': 4.437625793380457e-05, 'batch_size': 42, 'step_size': 7, 'gamma': 0.8576778805998484}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:03:19,257][0m Trial 8 finished with value: 0.08848563988557022 and parameters: {'observation_period_num': 121, 'train_rates': 0.8431406795879666, 'learning_rate': 9.785858590817324e-05, 'batch_size': 182, 'step_size': 5, 'gamma': 0.9679050649356071}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:04:11,946][0m Trial 9 finished with value: 0.6516772592609579 and parameters: {'observation_period_num': 34, 'train_rates': 0.6231831405661361, 'learning_rate': 1.866308721864899e-06, 'batch_size': 86, 'step_size': 10, 'gamma': 0.9450571299253416}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:04:51,237][0m Trial 10 finished with value: 0.07346200197935104 and parameters: {'observation_period_num': 70, 'train_rates': 0.9766708860103074, 'learning_rate': 0.0005085309543642987, 'batch_size': 159, 'step_size': 15, 'gamma': 0.7968235987565977}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:05:30,296][0m Trial 11 finished with value: 0.08701220154762268 and parameters: {'observation_period_num': 65, 'train_rates': 0.9899886634670527, 'learning_rate': 0.0007535003530714448, 'batch_size': 160, 'step_size': 15, 'gamma': 0.7570645707268588}. Best is trial 2 with value: 0.04945860803127289.[0m
[32m[I 2025-01-04 13:06:20,253][0m Trial 12 finished with value: 0.035599615424871445 and parameters: {'observation_period_num': 7, 'train_rates': 0.9860117120028047, 'learning_rate': 0.0006354117149405845, 'batch_size': 130, 'step_size': 15, 'gamma': 0.774304254691671}. Best is trial 12 with value: 0.035599615424871445.[0m
[32m[I 2025-01-04 13:07:19,047][0m Trial 13 finished with value: 0.029492745430616196 and parameters: {'observation_period_num': 18, 'train_rates': 0.9128601566306631, 'learning_rate': 0.00023191580624332898, 'batch_size': 101, 'step_size': 9, 'gamma': 0.9194800486076312}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:08:09,817][0m Trial 14 finished with value: 0.03128635212779045 and parameters: {'observation_period_num': 5, 'train_rates': 0.9018025846104848, 'learning_rate': 0.00021967274230901313, 'batch_size': 116, 'step_size': 10, 'gamma': 0.906679773395604}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:09:05,857][0m Trial 15 finished with value: 0.10469033309307538 and parameters: {'observation_period_num': 160, 'train_rates': 0.8914938583337986, 'learning_rate': 0.00022457997222372287, 'batch_size': 98, 'step_size': 10, 'gamma': 0.9110593669863182}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:10:33,644][0m Trial 16 finished with value: 0.0827890999531478 and parameters: {'observation_period_num': 40, 'train_rates': 0.9084903722515585, 'learning_rate': 0.00026525444757749805, 'batch_size': 66, 'step_size': 9, 'gamma': 0.9104450298922714}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:14:49,918][0m Trial 17 finished with value: 0.07150407131523528 and parameters: {'observation_period_num': 93, 'train_rates': 0.901528498334877, 'learning_rate': 0.00020190555789387647, 'batch_size': 21, 'step_size': 12, 'gamma': 0.839825315670641}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:15:42,208][0m Trial 18 finished with value: 0.14272482409450873 and parameters: {'observation_period_num': 48, 'train_rates': 0.8599792599693025, 'learning_rate': 9.328773183366149e-06, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9141809521987796}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:16:13,788][0m Trial 19 finished with value: 0.0905799408360283 and parameters: {'observation_period_num': 156, 'train_rates': 0.9280924346352696, 'learning_rate': 0.00011352658746021811, 'batch_size': 195, 'step_size': 11, 'gamma': 0.8220369618101901}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:16:49,918][0m Trial 20 finished with value: 0.15091637052304935 and parameters: {'observation_period_num': 85, 'train_rates': 0.8303649593371115, 'learning_rate': 1.16124575754572e-05, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9010148351528422}. Best is trial 13 with value: 0.029492745430616196.[0m
[32m[I 2025-01-04 13:17:41,809][0m Trial 21 finished with value: 0.024773181602358817 and parameters: {'observation_period_num': 11, 'train_rates': 0.9424276143814838, 'learning_rate': 0.0009309185747357336, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7624354557953175}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:18:33,998][0m Trial 22 finished with value: 0.044027208190435055 and parameters: {'observation_period_num': 25, 'train_rates': 0.9467120398178079, 'learning_rate': 0.00037601592671144033, 'batch_size': 117, 'step_size': 13, 'gamma': 0.9348461368290641}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:19:59,714][0m Trial 23 finished with value: 0.027650910004052103 and parameters: {'observation_period_num': 5, 'train_rates': 0.8739929844988742, 'learning_rate': 0.0008726062308000934, 'batch_size': 65, 'step_size': 9, 'gamma': 0.8875614971348905}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:21:34,257][0m Trial 24 finished with value: 0.041110247805637795 and parameters: {'observation_period_num': 32, 'train_rates': 0.9338455688067414, 'learning_rate': 0.000867241527302551, 'batch_size': 61, 'step_size': 8, 'gamma': 0.8116824381958103}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:22:35,901][0m Trial 25 finished with value: 0.04818684197962284 and parameters: {'observation_period_num': 49, 'train_rates': 0.8624341807478568, 'learning_rate': 0.000983176182601565, 'batch_size': 90, 'step_size': 6, 'gamma': 0.8866426707202876}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:26:38,777][0m Trial 26 finished with value: 0.060707865962577066 and parameters: {'observation_period_num': 72, 'train_rates': 0.8755361626579932, 'learning_rate': 0.00042931252712275847, 'batch_size': 22, 'step_size': 3, 'gamma': 0.8532313285705893}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:27:05,641][0m Trial 27 finished with value: 0.04733213409781456 and parameters: {'observation_period_num': 26, 'train_rates': 0.9466140195963549, 'learning_rate': 0.0003835852067729153, 'batch_size': 250, 'step_size': 14, 'gamma': 0.8366036073487029}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:28:29,139][0m Trial 28 finished with value: 0.10601729239318886 and parameters: {'observation_period_num': 143, 'train_rates': 0.8188209305875598, 'learning_rate': 0.00012314036823906775, 'batch_size': 62, 'step_size': 9, 'gamma': 0.9306802135683668}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:30:34,102][0m Trial 29 finished with value: 0.11713740803793934 and parameters: {'observation_period_num': 190, 'train_rates': 0.9124881864094126, 'learning_rate': 0.000560443158619213, 'batch_size': 43, 'step_size': 11, 'gamma': 0.7928511371140256}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:31:39,460][0m Trial 30 finished with value: 0.1497900837172508 and parameters: {'observation_period_num': 5, 'train_rates': 0.7216319328042009, 'learning_rate': 0.0009833322825624656, 'batch_size': 75, 'step_size': 13, 'gamma': 0.8612376341925027}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:32:35,445][0m Trial 31 finished with value: 0.03210436809294952 and parameters: {'observation_period_num': 6, 'train_rates': 0.8867351918986622, 'learning_rate': 0.00017706818666211738, 'batch_size': 103, 'step_size': 10, 'gamma': 0.8921389175011923}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:33:29,704][0m Trial 32 finished with value: 0.04252560780817316 and parameters: {'observation_period_num': 24, 'train_rates': 0.9553526305164814, 'learning_rate': 0.00024318478796268484, 'batch_size': 113, 'step_size': 9, 'gamma': 0.9263163833909025}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:34:13,253][0m Trial 33 finished with value: 0.05493316956256565 and parameters: {'observation_period_num': 40, 'train_rates': 0.9220196579426039, 'learning_rate': 5.505410271239248e-05, 'batch_size': 138, 'step_size': 11, 'gamma': 0.9548543929333033}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:35:13,208][0m Trial 34 finished with value: 0.03187231695418861 and parameters: {'observation_period_num': 19, 'train_rates': 0.8499028508106283, 'learning_rate': 0.00032912270696387123, 'batch_size': 93, 'step_size': 7, 'gamma': 0.8976036383434006}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:35:59,149][0m Trial 35 finished with value: 0.05238650570954046 and parameters: {'observation_period_num': 54, 'train_rates': 0.8717227023566103, 'learning_rate': 0.00015291496478331924, 'batch_size': 126, 'step_size': 8, 'gamma': 0.9198246457342146}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:36:36,238][0m Trial 36 finished with value: 0.03646821645650887 and parameters: {'observation_period_num': 15, 'train_rates': 0.7887394987607901, 'learning_rate': 0.0005661449642263443, 'batch_size': 150, 'step_size': 6, 'gamma': 0.8779799166963809}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:37:48,731][0m Trial 37 finished with value: 0.14911429389205502 and parameters: {'observation_period_num': 242, 'train_rates': 0.8958251879362643, 'learning_rate': 7.296015810413267e-05, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9495424222783422}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:38:23,745][0m Trial 38 finished with value: 0.29527923464775085 and parameters: {'observation_period_num': 109, 'train_rates': 0.9645464005574707, 'learning_rate': 1.8423866847532394e-05, 'batch_size': 179, 'step_size': 3, 'gamma': 0.8679316382823493}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:40:40,708][0m Trial 39 finished with value: 0.16659127581635666 and parameters: {'observation_period_num': 5, 'train_rates': 0.7573351300039028, 'learning_rate': 0.0003170597680843247, 'batch_size': 36, 'step_size': 12, 'gamma': 0.9880306844499906}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:42:21,169][0m Trial 40 finished with value: 0.12105764376540337 and parameters: {'observation_period_num': 39, 'train_rates': 0.9363496271605334, 'learning_rate': 4.2430210551315736e-06, 'batch_size': 57, 'step_size': 14, 'gamma': 0.8454001839211791}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:43:22,390][0m Trial 41 finished with value: 0.030989818673251 and parameters: {'observation_period_num': 18, 'train_rates': 0.8423384537755786, 'learning_rate': 0.00031886987537012795, 'batch_size': 89, 'step_size': 7, 'gamma': 0.896983578802878}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:44:28,564][0m Trial 42 finished with value: 0.03091560070973265 and parameters: {'observation_period_num': 16, 'train_rates': 0.8243456453739231, 'learning_rate': 0.0006330316081697544, 'batch_size': 82, 'step_size': 6, 'gamma': 0.885639792338672}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:45:35,999][0m Trial 43 finished with value: 0.0604349741280828 and parameters: {'observation_period_num': 60, 'train_rates': 0.8075895294467653, 'learning_rate': 0.0006881829478319611, 'batch_size': 78, 'step_size': 4, 'gamma': 0.8842079835557284}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:46:39,419][0m Trial 44 finished with value: 0.03160884812922522 and parameters: {'observation_period_num': 20, 'train_rates': 0.835979678730254, 'learning_rate': 0.0004941988526373001, 'batch_size': 85, 'step_size': 6, 'gamma': 0.8713016091228599}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:48:20,198][0m Trial 45 finished with value: 0.18780540460544937 and parameters: {'observation_period_num': 32, 'train_rates': 0.7462664912493324, 'learning_rate': 0.0007044737100421965, 'batch_size': 49, 'step_size': 7, 'gamma': 0.8994248047832527}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:49:07,478][0m Trial 46 finished with value: 0.197524547993773 and parameters: {'observation_period_num': 78, 'train_rates': 0.6582090076006943, 'learning_rate': 0.0004477566177135962, 'batch_size': 99, 'step_size': 1, 'gamma': 0.8879992618983432}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:50:11,598][0m Trial 47 finished with value: 0.06171723400278507 and parameters: {'observation_period_num': 46, 'train_rates': 0.7947379080834551, 'learning_rate': 0.00029565280242821446, 'batch_size': 82, 'step_size': 4, 'gamma': 0.7578780081544938}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:50:53,159][0m Trial 48 finished with value: 0.10909068462608745 and parameters: {'observation_period_num': 216, 'train_rates': 0.825972975284251, 'learning_rate': 0.0007392001431944701, 'batch_size': 131, 'step_size': 5, 'gamma': 0.8659640684652085}. Best is trial 21 with value: 0.024773181602358817.[0m
[32m[I 2025-01-04 13:51:44,854][0m Trial 49 finished with value: 0.04519073282722878 and parameters: {'observation_period_num': 15, 'train_rates': 0.8464331220286698, 'learning_rate': 0.00014970927827176505, 'batch_size': 112, 'step_size': 2, 'gamma': 0.9386271971099844}. Best is trial 21 with value: 0.024773181602358817.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 13:51:44,864][0m A new study created in memory with name: no-name-fc41dc5a-e03e-4d85-9a88-4ffb1d9df517[0m
[32m[I 2025-01-04 13:53:19,690][0m Trial 0 finished with value: 0.4723221065913453 and parameters: {'observation_period_num': 178, 'train_rates': 0.7667211296456309, 'learning_rate': 3.2141380879641023e-06, 'batch_size': 50, 'step_size': 14, 'gamma': 0.8158289992961881}. Best is trial 0 with value: 0.4723221065913453.[0m
[32m[I 2025-01-04 13:53:58,287][0m Trial 1 finished with value: 0.25093263748034933 and parameters: {'observation_period_num': 184, 'train_rates': 0.8345569539171871, 'learning_rate': 5.572003857254989e-06, 'batch_size': 141, 'step_size': 11, 'gamma': 0.9753090559533937}. Best is trial 1 with value: 0.25093263748034933.[0m
[32m[I 2025-01-04 13:54:31,518][0m Trial 2 finished with value: 0.18820184502979698 and parameters: {'observation_period_num': 98, 'train_rates': 0.8921022468102427, 'learning_rate': 1.1185699914093277e-05, 'batch_size': 173, 'step_size': 11, 'gamma': 0.8807563197368059}. Best is trial 2 with value: 0.18820184502979698.[0m
[32m[I 2025-01-04 13:55:24,446][0m Trial 3 finished with value: 0.12369203978415691 and parameters: {'observation_period_num': 146, 'train_rates': 0.8825161556034575, 'learning_rate': 0.0002351863735645815, 'batch_size': 106, 'step_size': 7, 'gamma': 0.9414934751183143}. Best is trial 3 with value: 0.12369203978415691.[0m
Early stopping at epoch 75
[32m[I 2025-01-04 13:56:35,477][0m Trial 4 finished with value: 0.1924655918031931 and parameters: {'observation_period_num': 238, 'train_rates': 0.9708230627274161, 'learning_rate': 0.00016149006665506813, 'batch_size': 61, 'step_size': 1, 'gamma': 0.8493421698102664}. Best is trial 3 with value: 0.12369203978415691.[0m
[32m[I 2025-01-04 13:57:23,777][0m Trial 5 finished with value: 0.4717079700168335 and parameters: {'observation_period_num': 193, 'train_rates': 0.7131216453051532, 'learning_rate': 5.88754223203744e-06, 'batch_size': 98, 'step_size': 11, 'gamma': 0.8779911584307656}. Best is trial 3 with value: 0.12369203978415691.[0m
[32m[I 2025-01-04 13:57:46,617][0m Trial 6 finished with value: 0.5560930244459713 and parameters: {'observation_period_num': 252, 'train_rates': 0.6320944754545997, 'learning_rate': 5.098898836893893e-06, 'batch_size': 193, 'step_size': 9, 'gamma': 0.9879514572667794}. Best is trial 3 with value: 0.12369203978415691.[0m
[32m[I 2025-01-04 13:58:16,728][0m Trial 7 finished with value: 0.24931119036024105 and parameters: {'observation_period_num': 88, 'train_rates': 0.7667281352295525, 'learning_rate': 0.00014230745474027128, 'batch_size': 181, 'step_size': 15, 'gamma': 0.8972300320205109}. Best is trial 3 with value: 0.12369203978415691.[0m
[32m[I 2025-01-04 13:58:42,402][0m Trial 8 finished with value: 0.04455207024287704 and parameters: {'observation_period_num': 15, 'train_rates': 0.7947140805267577, 'learning_rate': 0.0003923611813271408, 'batch_size': 218, 'step_size': 12, 'gamma': 0.7502065495591375}. Best is trial 8 with value: 0.04455207024287704.[0m
Early stopping at epoch 92
[32m[I 2025-01-04 13:59:12,163][0m Trial 9 finished with value: 0.41268857914224155 and parameters: {'observation_period_num': 18, 'train_rates': 0.8629124285619894, 'learning_rate': 6.565332905810813e-06, 'batch_size': 193, 'step_size': 2, 'gamma': 0.7846044764072242}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 13:59:32,319][0m Trial 10 finished with value: 0.1507523655482046 and parameters: {'observation_period_num': 7, 'train_rates': 0.660496629571651, 'learning_rate': 0.0008935562746246935, 'batch_size': 253, 'step_size': 5, 'gamma': 0.759038427197835}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 13:59:57,261][0m Trial 11 finished with value: 0.0749661847949028 and parameters: {'observation_period_num': 62, 'train_rates': 0.9352623290919003, 'learning_rate': 0.0008310200576792348, 'batch_size': 255, 'step_size': 5, 'gamma': 0.9425729229802818}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:00:23,950][0m Trial 12 finished with value: 0.04779818654060364 and parameters: {'observation_period_num': 55, 'train_rates': 0.9839013976531779, 'learning_rate': 0.0009696629883324624, 'batch_size': 250, 'step_size': 5, 'gamma': 0.9299838943250666}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:00:50,119][0m Trial 13 finished with value: 0.0902866992119813 and parameters: {'observation_period_num': 54, 'train_rates': 0.8073530732126815, 'learning_rate': 4.345072616820437e-05, 'batch_size': 222, 'step_size': 5, 'gamma': 0.9215709218291255}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:01:14,393][0m Trial 14 finished with value: 0.18098453007738036 and parameters: {'observation_period_num': 46, 'train_rates': 0.7171542267843393, 'learning_rate': 0.0003769840759871366, 'batch_size': 228, 'step_size': 8, 'gamma': 0.834993837532243}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:01:43,029][0m Trial 15 finished with value: 0.13299331068992615 and parameters: {'observation_period_num': 107, 'train_rates': 0.9847384099674144, 'learning_rate': 5.719199361670577e-05, 'batch_size': 219, 'step_size': 13, 'gamma': 0.7513110231442969}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:02:16,649][0m Trial 16 finished with value: 0.1894724889502038 and parameters: {'observation_period_num': 39, 'train_rates': 0.7180709783126207, 'learning_rate': 0.0004629919547626622, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8033087690495184}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:02:45,589][0m Trial 17 finished with value: 0.08906087915510892 and parameters: {'observation_period_num': 81, 'train_rates': 0.9238127800363024, 'learning_rate': 8.361414200182191e-05, 'batch_size': 218, 'step_size': 9, 'gamma': 0.9082440054030732}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:03:32,371][0m Trial 18 finished with value: 0.5026420028121383 and parameters: {'observation_period_num': 125, 'train_rates': 0.8091945316021532, 'learning_rate': 1.3113267301919862e-06, 'batch_size': 112, 'step_size': 7, 'gamma': 0.9519874379440947}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:06:43,142][0m Trial 19 finished with value: 0.18419654105069502 and parameters: {'observation_period_num': 26, 'train_rates': 0.7649411748339726, 'learning_rate': 1.8720922226948667e-05, 'batch_size': 26, 'step_size': 13, 'gamma': 0.8502536399755439}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:07:08,434][0m Trial 20 finished with value: 0.07882400104268031 and parameters: {'observation_period_num': 70, 'train_rates': 0.8474006825357053, 'learning_rate': 0.00044239659708397066, 'batch_size': 238, 'step_size': 3, 'gamma': 0.7783229408243231}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:07:33,338][0m Trial 21 finished with value: 0.08538660407066345 and parameters: {'observation_period_num': 70, 'train_rates': 0.9370848199873493, 'learning_rate': 0.0006820253805486035, 'batch_size': 256, 'step_size': 5, 'gamma': 0.941888180874895}. Best is trial 8 with value: 0.04455207024287704.[0m
[32m[I 2025-01-04 14:08:04,464][0m Trial 22 finished with value: 0.03616577386856079 and parameters: {'observation_period_num': 5, 'train_rates': 0.9375133894210093, 'learning_rate': 0.0009923880276556686, 'batch_size': 202, 'step_size': 6, 'gamma': 0.9602566256848728}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:08:37,567][0m Trial 23 finished with value: 0.04034154489636421 and parameters: {'observation_period_num': 5, 'train_rates': 0.9601305804304288, 'learning_rate': 0.0002540328295379555, 'batch_size': 197, 'step_size': 7, 'gamma': 0.9702406197369657}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:09:08,593][0m Trial 24 finished with value: 0.03707696869969368 and parameters: {'observation_period_num': 5, 'train_rates': 0.9247028835788069, 'learning_rate': 0.00025684017275670455, 'batch_size': 201, 'step_size': 9, 'gamma': 0.9653051533044484}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:09:39,875][0m Trial 25 finished with value: 0.04140564054250717 and parameters: {'observation_period_num': 5, 'train_rates': 0.9105039834948806, 'learning_rate': 0.00021911174291290532, 'batch_size': 198, 'step_size': 7, 'gamma': 0.9683292733496874}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:10:18,837][0m Trial 26 finished with value: 0.06188654527068138 and parameters: {'observation_period_num': 33, 'train_rates': 0.9487019674142114, 'learning_rate': 8.929493191313783e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.9665425339523716}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:11:03,243][0m Trial 27 finished with value: 0.06307774867793048 and parameters: {'observation_period_num': 31, 'train_rates': 0.8899369032061691, 'learning_rate': 0.0002680164078856181, 'batch_size': 135, 'step_size': 8, 'gamma': 0.9574056002829883}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:11:33,737][0m Trial 28 finished with value: 0.08015131950378418 and parameters: {'observation_period_num': 24, 'train_rates': 0.9530315700460832, 'learning_rate': 0.00011417026402851348, 'batch_size': 201, 'step_size': 6, 'gamma': 0.9785501418601046}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:12:11,731][0m Trial 29 finished with value: 0.1200951563852622 and parameters: {'observation_period_num': 161, 'train_rates': 0.9047672584503489, 'learning_rate': 2.428142967352763e-05, 'batch_size': 141, 'step_size': 10, 'gamma': 0.9139396949081304}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:12:45,669][0m Trial 30 finished with value: 0.04126166179776192 and parameters: {'observation_period_num': 6, 'train_rates': 0.9615734982849512, 'learning_rate': 0.0005413612488768841, 'batch_size': 174, 'step_size': 4, 'gamma': 0.9858892472022397}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:13:18,590][0m Trial 31 finished with value: 0.037944577634334564 and parameters: {'observation_period_num': 5, 'train_rates': 0.9499394610578276, 'learning_rate': 0.000646715873210623, 'batch_size': 178, 'step_size': 3, 'gamma': 0.9787240993218361}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:13:48,354][0m Trial 32 finished with value: 0.06319411261758952 and parameters: {'observation_period_num': 43, 'train_rates': 0.866486558171982, 'learning_rate': 0.0002942552400620668, 'batch_size': 204, 'step_size': 6, 'gamma': 0.9649089247695996}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:14:28,029][0m Trial 33 finished with value: 0.05176871993298669 and parameters: {'observation_period_num': 32, 'train_rates': 0.9295356093655618, 'learning_rate': 0.00019338158889242474, 'batch_size': 156, 'step_size': 3, 'gamma': 0.9892608391935408}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:15:05,225][0m Trial 34 finished with value: 0.04139375314116478 and parameters: {'observation_period_num': 18, 'train_rates': 0.9872305127408456, 'learning_rate': 0.0007094951454523895, 'batch_size': 180, 'step_size': 8, 'gamma': 0.9306045308957736}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:15:54,325][0m Trial 35 finished with value: 0.05097583945383105 and parameters: {'observation_period_num': 44, 'train_rates': 0.9023609506765374, 'learning_rate': 0.0005717836811799453, 'batch_size': 121, 'step_size': 1, 'gamma': 0.9540927584737124}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:17:03,709][0m Trial 36 finished with value: 0.08898136284306785 and parameters: {'observation_period_num': 117, 'train_rates': 0.9626220390284779, 'learning_rate': 0.0003681243087522403, 'batch_size': 84, 'step_size': 10, 'gamma': 0.8906406410369436}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:17:31,823][0m Trial 37 finished with value: 0.11503673488051114 and parameters: {'observation_period_num': 209, 'train_rates': 0.8694756550154604, 'learning_rate': 0.0001554732682205229, 'batch_size': 206, 'step_size': 6, 'gamma': 0.9735762486531044}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:18:03,448][0m Trial 38 finished with value: 0.10917339470694985 and parameters: {'observation_period_num': 139, 'train_rates': 0.9193084703945323, 'learning_rate': 5.6790398288883696e-05, 'batch_size': 185, 'step_size': 4, 'gamma': 0.953516669569274}. Best is trial 22 with value: 0.03616577386856079.[0m
[32m[I 2025-01-04 14:18:37,574][0m Trial 39 finished with value: 0.035834990722953146 and parameters: {'observation_period_num': 17, 'train_rates': 0.8378161157051117, 'learning_rate': 0.0002507689338846089, 'batch_size': 166, 'step_size': 10, 'gamma': 0.9750045259143277}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:19:16,679][0m Trial 40 finished with value: 0.08506860185390824 and parameters: {'observation_period_num': 18, 'train_rates': 0.8396152990184685, 'learning_rate': 1.1236984597785956e-05, 'batch_size': 147, 'step_size': 11, 'gamma': 0.9324272658766586}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:19:52,510][0m Trial 41 finished with value: 0.05539009920169408 and parameters: {'observation_period_num': 9, 'train_rates': 0.8855782859141126, 'learning_rate': 0.0002527724631147542, 'batch_size': 172, 'step_size': 8, 'gamma': 0.9780113228988596}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:20:26,243][0m Trial 42 finished with value: 0.04433427378535271 and parameters: {'observation_period_num': 23, 'train_rates': 0.944975759219737, 'learning_rate': 0.0003205551659681867, 'batch_size': 189, 'step_size': 10, 'gamma': 0.9665946149136683}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:21:04,038][0m Trial 43 finished with value: 0.04481836356222629 and parameters: {'observation_period_num': 14, 'train_rates': 0.9152766333872094, 'learning_rate': 0.000123783095068386, 'batch_size': 165, 'step_size': 9, 'gamma': 0.9476280780004867}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:21:33,065][0m Trial 44 finished with value: 0.04491797462105751 and parameters: {'observation_period_num': 6, 'train_rates': 0.9703372207652076, 'learning_rate': 0.0006053463691065602, 'batch_size': 236, 'step_size': 11, 'gamma': 0.9892129719337122}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:21:59,678][0m Trial 45 finished with value: 0.2118294927144917 and parameters: {'observation_period_num': 53, 'train_rates': 0.744017965229472, 'learning_rate': 0.0009464729662610417, 'batch_size': 207, 'step_size': 7, 'gamma': 0.961199410581793}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:22:26,953][0m Trial 46 finished with value: 0.04718954272804752 and parameters: {'observation_period_num': 33, 'train_rates': 0.8275063867623347, 'learning_rate': 0.0004775577946732908, 'batch_size': 212, 'step_size': 2, 'gamma': 0.9358013011982217}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:22:58,584][0m Trial 47 finished with value: 0.0762683305110635 and parameters: {'observation_period_num': 94, 'train_rates': 0.8763787406211945, 'learning_rate': 0.00018952753060725295, 'batch_size': 190, 'step_size': 10, 'gamma': 0.9779622441163278}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:23:33,740][0m Trial 48 finished with value: 0.17741474355497058 and parameters: {'observation_period_num': 63, 'train_rates': 0.6731172612027131, 'learning_rate': 0.0009800854879950804, 'batch_size': 130, 'step_size': 12, 'gamma': 0.902480716060162}. Best is trial 39 with value: 0.035834990722953146.[0m
[32m[I 2025-01-04 14:24:08,591][0m Trial 49 finished with value: 0.10190678387880325 and parameters: {'observation_period_num': 221, 'train_rates': 0.9756314926688694, 'learning_rate': 8.911388261048654e-05, 'batch_size': 175, 'step_size': 12, 'gamma': 0.8721659745913333}. Best is trial 39 with value: 0.035834990722953146.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 14:24:08,600][0m A new study created in memory with name: no-name-2a6d0385-9325-448a-ae7a-6a2856d5c81b[0m
[32m[I 2025-01-04 14:24:39,863][0m Trial 0 finished with value: 0.5376232691447843 and parameters: {'observation_period_num': 173, 'train_rates': 0.7468264113812694, 'learning_rate': 4.599393347147097e-06, 'batch_size': 164, 'step_size': 10, 'gamma': 0.9362234016983609}. Best is trial 0 with value: 0.5376232691447843.[0m
[32m[I 2025-01-04 14:28:55,976][0m Trial 1 finished with value: 0.09355569128446652 and parameters: {'observation_period_num': 218, 'train_rates': 0.904792772917148, 'learning_rate': 2.2554450030757215e-05, 'batch_size': 20, 'step_size': 6, 'gamma': 0.8084254507756391}. Best is trial 1 with value: 0.09355569128446652.[0m
[32m[I 2025-01-04 14:30:03,406][0m Trial 2 finished with value: 0.7368598386556089 and parameters: {'observation_period_num': 186, 'train_rates': 0.698390254761506, 'learning_rate': 1.6220576191091726e-06, 'batch_size': 67, 'step_size': 10, 'gamma': 0.9818609193127502}. Best is trial 1 with value: 0.09355569128446652.[0m
[32m[I 2025-01-04 14:30:33,942][0m Trial 3 finished with value: 0.7648829341097164 and parameters: {'observation_period_num': 204, 'train_rates': 0.7332373713958216, 'learning_rate': 2.2665750753177597e-06, 'batch_size': 173, 'step_size': 10, 'gamma': 0.9144627138885988}. Best is trial 1 with value: 0.09355569128446652.[0m
[32m[I 2025-01-04 14:31:06,944][0m Trial 4 finished with value: 0.1444763536482344 and parameters: {'observation_period_num': 41, 'train_rates': 0.6518117222559447, 'learning_rate': 0.0007819389261610081, 'batch_size': 155, 'step_size': 13, 'gamma': 0.8627917340750318}. Best is trial 1 with value: 0.09355569128446652.[0m
[32m[I 2025-01-04 14:31:38,031][0m Trial 5 finished with value: 0.4858406272450915 and parameters: {'observation_period_num': 74, 'train_rates': 0.7532688302371259, 'learning_rate': 4.171946783564543e-06, 'batch_size': 172, 'step_size': 10, 'gamma': 0.9173815136213006}. Best is trial 1 with value: 0.09355569128446652.[0m
[32m[I 2025-01-04 14:32:10,731][0m Trial 6 finished with value: 0.06553796071188037 and parameters: {'observation_period_num': 45, 'train_rates': 0.8137278052713582, 'learning_rate': 7.703932718603653e-05, 'batch_size': 172, 'step_size': 10, 'gamma': 0.7637919955267907}. Best is trial 6 with value: 0.06553796071188037.[0m
[32m[I 2025-01-04 14:32:35,345][0m Trial 7 finished with value: 0.12296733260154724 and parameters: {'observation_period_num': 85, 'train_rates': 0.9533706951949747, 'learning_rate': 2.2622279849968456e-05, 'batch_size': 249, 'step_size': 15, 'gamma': 0.9039558729444355}. Best is trial 6 with value: 0.06553796071188037.[0m
[32m[I 2025-01-04 14:33:47,295][0m Trial 8 finished with value: 0.17421612184063917 and parameters: {'observation_period_num': 165, 'train_rates': 0.947488893690852, 'learning_rate': 9.989881474258335e-06, 'batch_size': 79, 'step_size': 11, 'gamma': 0.8074812800724438}. Best is trial 6 with value: 0.06553796071188037.[0m
[32m[I 2025-01-04 14:36:59,541][0m Trial 9 finished with value: 0.3895386214960705 and parameters: {'observation_period_num': 175, 'train_rates': 0.841943552132454, 'learning_rate': 3.4476414305723376e-06, 'batch_size': 26, 'step_size': 2, 'gamma': 0.8439285559595846}. Best is trial 6 with value: 0.06553796071188037.[0m
[32m[I 2025-01-04 14:37:24,834][0m Trial 10 finished with value: 0.057318736245666725 and parameters: {'observation_period_num': 9, 'train_rates': 0.8359805115513798, 'learning_rate': 0.00021155225855690256, 'batch_size': 233, 'step_size': 6, 'gamma': 0.753530219953758}. Best is trial 10 with value: 0.057318736245666725.[0m
[32m[I 2025-01-04 14:37:50,144][0m Trial 11 finished with value: 0.05580275871068818 and parameters: {'observation_period_num': 10, 'train_rates': 0.8428398812139937, 'learning_rate': 0.00018465773938980165, 'batch_size': 237, 'step_size': 6, 'gamma': 0.762931127233178}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:38:15,852][0m Trial 12 finished with value: 0.058779271481691375 and parameters: {'observation_period_num': 18, 'train_rates': 0.8709611181070944, 'learning_rate': 0.00023593239925158053, 'batch_size': 253, 'step_size': 5, 'gamma': 0.7537034510985031}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:38:41,710][0m Trial 13 finished with value: 0.06108465467569131 and parameters: {'observation_period_num': 8, 'train_rates': 0.7898874298023694, 'learning_rate': 0.00013442999786642494, 'batch_size': 217, 'step_size': 6, 'gamma': 0.7913706091790852}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:39:08,853][0m Trial 14 finished with value: 0.09512299822415678 and parameters: {'observation_period_num': 115, 'train_rates': 0.8878823761834308, 'learning_rate': 0.0005507263099425862, 'batch_size': 216, 'step_size': 3, 'gamma': 0.7804876757827235}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:39:34,197][0m Trial 15 finished with value: 0.1782585866673723 and parameters: {'observation_period_num': 246, 'train_rates': 0.8180744935839178, 'learning_rate': 6.239107247464728e-05, 'batch_size': 211, 'step_size': 7, 'gamma': 0.8416366967228507}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:40:24,278][0m Trial 16 finished with value: 0.07061806736775667 and parameters: {'observation_period_num': 72, 'train_rates': 0.9161438976140788, 'learning_rate': 0.00023726598902072286, 'batch_size': 118, 'step_size': 4, 'gamma': 0.7508771929784316}. Best is trial 11 with value: 0.05580275871068818.[0m
Early stopping at epoch 72
[32m[I 2025-01-04 14:40:42,324][0m Trial 17 finished with value: 0.1840853951600793 and parameters: {'observation_period_num': 135, 'train_rates': 0.8535618686911293, 'learning_rate': 0.00033924000275147015, 'batch_size': 229, 'step_size': 1, 'gamma': 0.8290742809949001}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:41:14,423][0m Trial 18 finished with value: 0.0804937332868576 and parameters: {'observation_period_num': 36, 'train_rates': 0.9857980458391834, 'learning_rate': 5.4665687821638674e-05, 'batch_size': 197, 'step_size': 8, 'gamma': 0.7848840223006622}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:41:51,921][0m Trial 19 finished with value: 0.18881125314669175 and parameters: {'observation_period_num': 103, 'train_rates': 0.61420427927992, 'learning_rate': 0.0001085725137774043, 'batch_size': 120, 'step_size': 8, 'gamma': 0.8722705897898411}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:42:20,172][0m Trial 20 finished with value: 0.07358181306932308 and parameters: {'observation_period_num': 59, 'train_rates': 0.7869301355164482, 'learning_rate': 0.0004418565976264933, 'batch_size': 195, 'step_size': 4, 'gamma': 0.8083578207407697}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:42:43,423][0m Trial 21 finished with value: 0.0607971899400029 and parameters: {'observation_period_num': 10, 'train_rates': 0.8659049055007018, 'learning_rate': 0.00019690695999780464, 'batch_size': 255, 'step_size': 5, 'gamma': 0.7533145528265516}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:43:08,143][0m Trial 22 finished with value: 0.05722459404661892 and parameters: {'observation_period_num': 24, 'train_rates': 0.8726846127648566, 'learning_rate': 0.00017316025811841892, 'batch_size': 237, 'step_size': 6, 'gamma': 0.7703780314526212}. Best is trial 11 with value: 0.05580275871068818.[0m
[32m[I 2025-01-04 14:43:34,160][0m Trial 23 finished with value: 0.043721249230904057 and parameters: {'observation_period_num': 26, 'train_rates': 0.8214692471173969, 'learning_rate': 0.0009229510635811001, 'batch_size': 234, 'step_size': 7, 'gamma': 0.7751804374231372}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:44:02,082][0m Trial 24 finished with value: 0.18053824861836656 and parameters: {'observation_period_num': 30, 'train_rates': 0.7801285075040523, 'learning_rate': 0.0009953733578707065, 'batch_size': 190, 'step_size': 7, 'gamma': 0.7856100368360902}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:44:29,271][0m Trial 25 finished with value: 0.119128281083792 and parameters: {'observation_period_num': 52, 'train_rates': 0.9077331249042585, 'learning_rate': 3.6497440802572984e-05, 'batch_size': 235, 'step_size': 8, 'gamma': 0.7759455233671717}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:45:07,702][0m Trial 26 finished with value: 0.07372435805393242 and parameters: {'observation_period_num': 138, 'train_rates': 0.8178657648283911, 'learning_rate': 0.000560657897457551, 'batch_size': 136, 'step_size': 4, 'gamma': 0.8220361682115578}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:45:29,743][0m Trial 27 finished with value: 0.25654562372610673 and parameters: {'observation_period_num': 94, 'train_rates': 0.7108792431893467, 'learning_rate': 0.00010409245888773534, 'batch_size': 233, 'step_size': 7, 'gamma': 0.7977495530254145}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:46:01,004][0m Trial 28 finished with value: 0.0732264295220375 and parameters: {'observation_period_num': 60, 'train_rates': 0.9399655755929085, 'learning_rate': 0.00034274750372070297, 'batch_size': 207, 'step_size': 5, 'gamma': 0.7723423775654273}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:46:38,327][0m Trial 29 finished with value: 0.21816111636221924 and parameters: {'observation_period_num': 26, 'train_rates': 0.7631196226991523, 'learning_rate': 1.4373794627687326e-05, 'batch_size': 146, 'step_size': 9, 'gamma': 0.9683449361711608}. Best is trial 23 with value: 0.043721249230904057.[0m
[32m[I 2025-01-04 14:47:09,391][0m Trial 30 finished with value: 0.04092356807419232 and parameters: {'observation_period_num': 27, 'train_rates': 0.892400964142387, 'learning_rate': 0.0001453023084325043, 'batch_size': 192, 'step_size': 12, 'gamma': 0.882545766187391}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:47:40,812][0m Trial 31 finished with value: 0.04379781111177192 and parameters: {'observation_period_num': 30, 'train_rates': 0.8864190489644821, 'learning_rate': 0.00013557280324516803, 'batch_size': 188, 'step_size': 12, 'gamma': 0.8852687723062342}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:48:13,700][0m Trial 32 finished with value: 0.054210611665466694 and parameters: {'observation_period_num': 5, 'train_rates': 0.8980131853705642, 'learning_rate': 4.403829789119688e-05, 'batch_size': 185, 'step_size': 12, 'gamma': 0.8830971640272346}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:48:47,450][0m Trial 33 finished with value: 0.055493399831596936 and parameters: {'observation_period_num': 42, 'train_rates': 0.9267532185402748, 'learning_rate': 4.3640339704197113e-05, 'batch_size': 185, 'step_size': 12, 'gamma': 0.8864544930638181}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:49:24,911][0m Trial 34 finished with value: 0.14327178264278115 and parameters: {'observation_period_num': 68, 'train_rates': 0.8959000493016231, 'learning_rate': 7.761889483486363e-06, 'batch_size': 159, 'step_size': 14, 'gamma': 0.9421583671576996}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:49:59,963][0m Trial 35 finished with value: 0.0702982023358345 and parameters: {'observation_period_num': 28, 'train_rates': 0.975704306614795, 'learning_rate': 2.493166076286444e-05, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8878790241400477}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:50:29,518][0m Trial 36 finished with value: 0.061082189977169035 and parameters: {'observation_period_num': 49, 'train_rates': 0.8884260665877657, 'learning_rate': 7.13047535644044e-05, 'batch_size': 204, 'step_size': 13, 'gamma': 0.8628864223207788}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:51:13,118][0m Trial 37 finished with value: 0.1068600946861618 and parameters: {'observation_period_num': 78, 'train_rates': 0.9263619512937417, 'learning_rate': 1.552373559920819e-05, 'batch_size': 138, 'step_size': 11, 'gamma': 0.9431680890522287}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:51:50,543][0m Trial 38 finished with value: 0.6556581258773804 and parameters: {'observation_period_num': 37, 'train_rates': 0.9682573341855515, 'learning_rate': 1.1285758473633335e-06, 'batch_size': 167, 'step_size': 15, 'gamma': 0.9218611654038454}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:52:37,626][0m Trial 39 finished with value: 0.05738520970748317 and parameters: {'observation_period_num': 21, 'train_rates': 0.8098143068640469, 'learning_rate': 3.103506943066798e-05, 'batch_size': 118, 'step_size': 13, 'gamma': 0.8850570381992398}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:53:07,376][0m Trial 40 finished with value: 0.20405154074118895 and parameters: {'observation_period_num': 55, 'train_rates': 0.7268310872078882, 'learning_rate': 9.457171036678064e-05, 'batch_size': 177, 'step_size': 11, 'gamma': 0.9023744401022737}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:53:40,750][0m Trial 41 finished with value: 0.060824886342595676 and parameters: {'observation_period_num': 40, 'train_rates': 0.932463694433199, 'learning_rate': 4.3600100714406964e-05, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8779309025853045}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:54:18,459][0m Trial 42 finished with value: 0.06169750848748805 and parameters: {'observation_period_num': 5, 'train_rates': 0.9129826564565674, 'learning_rate': 1.6900079911457203e-05, 'batch_size': 162, 'step_size': 12, 'gamma': 0.8493980940843899}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:54:45,697][0m Trial 43 finished with value: 0.0696413020292918 and parameters: {'observation_period_num': 45, 'train_rates': 0.8795044898498506, 'learning_rate': 4.936991678193567e-05, 'batch_size': 222, 'step_size': 9, 'gamma': 0.8966518579380135}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:55:18,512][0m Trial 44 finished with value: 0.04814392700791359 and parameters: {'observation_period_num': 21, 'train_rates': 0.9644283589642577, 'learning_rate': 0.00012888694376015148, 'batch_size': 189, 'step_size': 14, 'gamma': 0.8581111715609046}. Best is trial 30 with value: 0.04092356807419232.[0m
[32m[I 2025-01-04 14:55:59,334][0m Trial 45 finished with value: 0.037376631051301956 and parameters: {'observation_period_num': 21, 'train_rates': 0.9558327049062911, 'learning_rate': 0.00030894460937628334, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8567657462066496}. Best is trial 45 with value: 0.037376631051301956.[0m
[32m[I 2025-01-04 14:56:40,582][0m Trial 46 finished with value: 0.03657718375325203 and parameters: {'observation_period_num': 16, 'train_rates': 0.9546411665643705, 'learning_rate': 0.0007033946560515519, 'batch_size': 149, 'step_size': 14, 'gamma': 0.8530913197236093}. Best is trial 46 with value: 0.03657718375325203.[0m
[32m[I 2025-01-04 14:57:36,074][0m Trial 47 finished with value: 0.12255510553179016 and parameters: {'observation_period_num': 153, 'train_rates': 0.9408522001365794, 'learning_rate': 0.0007643130070758743, 'batch_size': 99, 'step_size': 14, 'gamma': 0.8283811700908235}. Best is trial 46 with value: 0.03657718375325203.[0m
[32m[I 2025-01-04 14:58:17,143][0m Trial 48 finished with value: 0.08801349252462387 and parameters: {'observation_period_num': 63, 'train_rates': 0.9849543237168973, 'learning_rate': 0.0003364958787560107, 'batch_size': 155, 'step_size': 15, 'gamma': 0.9293952615242956}. Best is trial 46 with value: 0.03657718375325203.[0m
[32m[I 2025-01-04 14:58:52,932][0m Trial 49 finished with value: 0.26604841919782174 and parameters: {'observation_period_num': 201, 'train_rates': 0.6745262386664622, 'learning_rate': 0.000668533363897436, 'batch_size': 129, 'step_size': 13, 'gamma': 0.9092594323992044}. Best is trial 46 with value: 0.03657718375325203.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 14:58:52,943][0m A new study created in memory with name: no-name-bfa7d320-2441-472d-86fc-f8a7f11e1262[0m
[32m[I 2025-01-04 14:59:27,276][0m Trial 0 finished with value: 0.18960014283657073 and parameters: {'observation_period_num': 162, 'train_rates': 0.8569582922090784, 'learning_rate': 8.068429855866794e-06, 'batch_size': 164, 'step_size': 15, 'gamma': 0.9722470194204712}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:00:45,941][0m Trial 1 finished with value: 0.2949961136053702 and parameters: {'observation_period_num': 243, 'train_rates': 0.7683738578332582, 'learning_rate': 0.00028258554819925776, 'batch_size': 61, 'step_size': 13, 'gamma': 0.8671968680655192}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:01:10,040][0m Trial 2 finished with value: 0.2257274308036811 and parameters: {'observation_period_num': 232, 'train_rates': 0.7964772955421948, 'learning_rate': 9.464088467986119e-05, 'batch_size': 207, 'step_size': 3, 'gamma': 0.842135249052757}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:01:45,347][0m Trial 3 finished with value: 0.22277418976952124 and parameters: {'observation_period_num': 139, 'train_rates': 0.6675981015028555, 'learning_rate': 6.68389615497997e-05, 'batch_size': 131, 'step_size': 14, 'gamma': 0.8572225172317878}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:02:34,788][0m Trial 4 finished with value: 0.2811245629770521 and parameters: {'observation_period_num': 111, 'train_rates': 0.6037568842440697, 'learning_rate': 0.00011869106677008982, 'batch_size': 89, 'step_size': 9, 'gamma': 0.8526377796046328}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:02:58,186][0m Trial 5 finished with value: 0.3300668184899471 and parameters: {'observation_period_num': 219, 'train_rates': 0.8713709880006589, 'learning_rate': 6.768546992467821e-06, 'batch_size': 247, 'step_size': 15, 'gamma': 0.8961922729178955}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:04:03,200][0m Trial 6 finished with value: 0.341652949943262 and parameters: {'observation_period_num': 60, 'train_rates': 0.776749311435868, 'learning_rate': 6.449631499464002e-06, 'batch_size': 76, 'step_size': 12, 'gamma': 0.7573975846418147}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:07:41,147][0m Trial 7 finished with value: 0.24974717357484613 and parameters: {'observation_period_num': 193, 'train_rates': 0.6483141418156428, 'learning_rate': 0.00010000049653143908, 'batch_size': 19, 'step_size': 11, 'gamma': 0.7784324494544089}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:08:04,428][0m Trial 8 finished with value: 0.339183703932504 and parameters: {'observation_period_num': 61, 'train_rates': 0.6617729495348883, 'learning_rate': 4.77235567759476e-06, 'batch_size': 227, 'step_size': 14, 'gamma': 0.9856387939245571}. Best is trial 0 with value: 0.18960014283657073.[0m
[32m[I 2025-01-04 15:08:59,522][0m Trial 9 finished with value: 0.09410297713781658 and parameters: {'observation_period_num': 125, 'train_rates': 0.8860388187715484, 'learning_rate': 0.0008792101038235498, 'batch_size': 102, 'step_size': 8, 'gamma': 0.8357286780104786}. Best is trial 9 with value: 0.09410297713781658.[0m
[32m[I 2025-01-04 15:09:51,330][0m Trial 10 finished with value: 0.03862304240465164 and parameters: {'observation_period_num': 9, 'train_rates': 0.9866978018695005, 'learning_rate': 0.0008976326501217513, 'batch_size': 125, 'step_size': 4, 'gamma': 0.8086795047782934}. Best is trial 10 with value: 0.03862304240465164.[0m
[32m[I 2025-01-04 15:10:36,331][0m Trial 11 finished with value: 0.06834493577480316 and parameters: {'observation_period_num': 94, 'train_rates': 0.9809173298876511, 'learning_rate': 0.0009437505557798316, 'batch_size': 136, 'step_size': 4, 'gamma': 0.8059894915181788}. Best is trial 10 with value: 0.03862304240465164.[0m
[32m[I 2025-01-04 15:11:18,071][0m Trial 12 finished with value: 0.04045237973332405 and parameters: {'observation_period_num': 13, 'train_rates': 0.9805181037251762, 'learning_rate': 0.0009473715841095493, 'batch_size': 153, 'step_size': 3, 'gamma': 0.7996299482445534}. Best is trial 10 with value: 0.03862304240465164.[0m
Early stopping at epoch 59
[32m[I 2025-01-04 15:11:38,733][0m Trial 13 finished with value: 0.0769248753786087 and parameters: {'observation_period_num': 8, 'train_rates': 0.9762538136512723, 'learning_rate': 0.00034399967445858333, 'batch_size': 193, 'step_size': 1, 'gamma': 0.8044790989669961}. Best is trial 10 with value: 0.03862304240465164.[0m
[32m[I 2025-01-04 15:12:18,577][0m Trial 14 finished with value: 0.030180199248226066 and parameters: {'observation_period_num': 9, 'train_rates': 0.9356594648373686, 'learning_rate': 0.00036986417088933135, 'batch_size': 168, 'step_size': 5, 'gamma': 0.9019083387436764}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:12:51,463][0m Trial 15 finished with value: 0.09258781971551912 and parameters: {'observation_period_num': 50, 'train_rates': 0.923505903055683, 'learning_rate': 2.0039126982762988e-05, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9197576115163841}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:13:45,853][0m Trial 16 finished with value: 0.04019886388310364 and parameters: {'observation_period_num': 39, 'train_rates': 0.9328546535051897, 'learning_rate': 0.0003117128195606257, 'batch_size': 110, 'step_size': 6, 'gamma': 0.9302237838959757}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:14:18,723][0m Trial 17 finished with value: 0.8296322566457093 and parameters: {'observation_period_num': 80, 'train_rates': 0.8218101027443392, 'learning_rate': 1.4868915137959212e-06, 'batch_size': 167, 'step_size': 6, 'gamma': 0.900751692271277}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:16:17,398][0m Trial 18 finished with value: 0.061399441421701666 and parameters: {'observation_period_num': 29, 'train_rates': 0.939328293765483, 'learning_rate': 3.725825643879011e-05, 'batch_size': 48, 'step_size': 1, 'gamma': 0.9513647012621513}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:16:59,402][0m Trial 19 finished with value: 0.06274384120990942 and parameters: {'observation_period_num': 82, 'train_rates': 0.9049717517409391, 'learning_rate': 0.00040900882365380904, 'batch_size': 138, 'step_size': 4, 'gamma': 0.8898956228747293}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:17:40,953][0m Trial 20 finished with value: 0.15197305473154538 and parameters: {'observation_period_num': 5, 'train_rates': 0.7277302084822405, 'learning_rate': 0.00016044880701510312, 'batch_size': 121, 'step_size': 8, 'gamma': 0.879479298743047}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:18:36,562][0m Trial 21 finished with value: 0.040186687035763516 and parameters: {'observation_period_num': 31, 'train_rates': 0.9356648201830918, 'learning_rate': 0.0004654269879859905, 'batch_size': 109, 'step_size': 6, 'gamma': 0.9178125664094675}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:19:18,008][0m Trial 22 finished with value: 0.04273771867156029 and parameters: {'observation_period_num': 29, 'train_rates': 0.9496493734032563, 'learning_rate': 0.0005102342275131269, 'batch_size': 152, 'step_size': 5, 'gamma': 0.9280298694565994}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:20:12,494][0m Trial 23 finished with value: 0.06723589478715905 and parameters: {'observation_period_num': 28, 'train_rates': 0.8397514177305944, 'learning_rate': 0.0005979077051480724, 'batch_size': 102, 'step_size': 8, 'gamma': 0.9459242997687163}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:20:43,461][0m Trial 24 finished with value: 0.06261165765329049 and parameters: {'observation_period_num': 69, 'train_rates': 0.9101351569337717, 'learning_rate': 0.00020453763389850297, 'batch_size': 199, 'step_size': 2, 'gamma': 0.912529354837007}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:21:18,036][0m Trial 25 finished with value: 0.09798654168844223 and parameters: {'observation_period_num': 41, 'train_rates': 0.9537811870591674, 'learning_rate': 4.417540913523791e-05, 'batch_size': 177, 'step_size': 5, 'gamma': 0.8226112263023106}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:22:26,722][0m Trial 26 finished with value: 0.05175232610434567 and parameters: {'observation_period_num': 22, 'train_rates': 0.8887369388961571, 'learning_rate': 1.8164654231279636e-05, 'batch_size': 83, 'step_size': 7, 'gamma': 0.947293262755382}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:23:20,444][0m Trial 27 finished with value: 0.04809866100549698 and parameters: {'observation_period_num': 47, 'train_rates': 0.986316850012943, 'learning_rate': 0.00020308872115555227, 'batch_size': 117, 'step_size': 10, 'gamma': 0.8777879856831637}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:24:03,010][0m Trial 28 finished with value: 0.0429889063829976 and parameters: {'observation_period_num': 6, 'train_rates': 0.9413239183903456, 'learning_rate': 0.0005701194183342393, 'batch_size': 151, 'step_size': 4, 'gamma': 0.7577017537396928}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:26:01,689][0m Trial 29 finished with value: 0.19490247961233423 and parameters: {'observation_period_num': 155, 'train_rates': 0.8531148577521621, 'learning_rate': 0.0005947691426933418, 'batch_size': 43, 'step_size': 7, 'gamma': 0.909770709217}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:26:38,744][0m Trial 30 finished with value: 0.0746762827038765 and parameters: {'observation_period_num': 91, 'train_rates': 0.9595201565627864, 'learning_rate': 0.0002391861933183118, 'batch_size': 167, 'step_size': 3, 'gamma': 0.9671105680420642}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:27:35,412][0m Trial 31 finished with value: 0.05314964776777703 and parameters: {'observation_period_num': 40, 'train_rates': 0.9212207351161475, 'learning_rate': 0.00035511684839407623, 'batch_size': 105, 'step_size': 6, 'gamma': 0.9331705348680706}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:28:23,946][0m Trial 32 finished with value: 0.03556834233717786 and parameters: {'observation_period_num': 24, 'train_rates': 0.8958174904477663, 'learning_rate': 0.00028678302149589023, 'batch_size': 119, 'step_size': 5, 'gamma': 0.9662846610375369}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:29:10,293][0m Trial 33 finished with value: 0.03499892771863088 and parameters: {'observation_period_num': 21, 'train_rates': 0.8851094135023372, 'learning_rate': 0.00015372026857802227, 'batch_size': 124, 'step_size': 5, 'gamma': 0.9642847255338868}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:29:56,502][0m Trial 34 finished with value: 0.04608994800668613 and parameters: {'observation_period_num': 20, 'train_rates': 0.8932216039293545, 'learning_rate': 6.423212037534528e-05, 'batch_size': 126, 'step_size': 5, 'gamma': 0.9666113148067201}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:30:35,148][0m Trial 35 finished with value: 0.05781289583669519 and parameters: {'observation_period_num': 56, 'train_rates': 0.817191592936871, 'learning_rate': 0.00014056299467808377, 'batch_size': 144, 'step_size': 2, 'gamma': 0.9887250307322781}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:32:00,815][0m Trial 36 finished with value: 0.08591731469285616 and parameters: {'observation_period_num': 107, 'train_rates': 0.8679242830284873, 'learning_rate': 7.190232679466857e-05, 'batch_size': 62, 'step_size': 4, 'gamma': 0.9554698058239185}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:32:24,688][0m Trial 37 finished with value: 0.33423570234339245 and parameters: {'observation_period_num': 194, 'train_rates': 0.7486591767753797, 'learning_rate': 0.00024678663173191513, 'batch_size': 215, 'step_size': 2, 'gamma': 0.8579071790950143}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:33:25,235][0m Trial 38 finished with value: 0.04087227086226145 and parameters: {'observation_period_num': 19, 'train_rates': 0.8438466757219606, 'learning_rate': 0.00015559264904197923, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9779128326121052}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:34:12,321][0m Trial 39 finished with value: 0.06996721029281616 and parameters: {'observation_period_num': 75, 'train_rates': 0.961155948700037, 'learning_rate': 0.0007038624383212998, 'batch_size': 130, 'step_size': 9, 'gamma': 0.8350230364741461}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:34:47,694][0m Trial 40 finished with value: 0.05563037254308399 and parameters: {'observation_period_num': 60, 'train_rates': 0.8688085716785184, 'learning_rate': 0.00011050076978913347, 'batch_size': 160, 'step_size': 3, 'gamma': 0.9730561853112351}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:35:37,963][0m Trial 41 finished with value: 0.03999109910731029 and parameters: {'observation_period_num': 34, 'train_rates': 0.9089402169983288, 'learning_rate': 0.0004518605527561822, 'batch_size': 116, 'step_size': 5, 'gamma': 0.9397694620395678}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:36:33,134][0m Trial 42 finished with value: 0.1122986858853927 and parameters: {'observation_period_num': 250, 'train_rates': 0.9039972663799468, 'learning_rate': 0.00030963611601328976, 'batch_size': 94, 'step_size': 5, 'gamma': 0.9395137287813522}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:37:48,556][0m Trial 43 finished with value: 0.037832906005375706 and parameters: {'observation_period_num': 17, 'train_rates': 0.8813944122386386, 'learning_rate': 0.0007997146248298994, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9569129164014636}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:39:01,479][0m Trial 44 finished with value: 0.033077195159752264 and parameters: {'observation_period_num': 15, 'train_rates': 0.8802312102758453, 'learning_rate': 0.000755820867959649, 'batch_size': 76, 'step_size': 4, 'gamma': 0.9576156305631034}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:40:13,722][0m Trial 45 finished with value: 0.03991397618392686 and parameters: {'observation_period_num': 19, 'train_rates': 0.8219338631367509, 'learning_rate': 0.0007082418481438187, 'batch_size': 71, 'step_size': 3, 'gamma': 0.9607291237495454}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:42:07,127][0m Trial 46 finished with value: 0.21817933015868848 and parameters: {'observation_period_num': 53, 'train_rates': 0.7831558479793059, 'learning_rate': 0.00019305492679877532, 'batch_size': 44, 'step_size': 4, 'gamma': 0.977275718130353}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:45:16,211][0m Trial 47 finished with value: 0.030230281284443276 and parameters: {'observation_period_num': 13, 'train_rates': 0.8768415279057771, 'learning_rate': 0.0007942734947414118, 'batch_size': 28, 'step_size': 7, 'gamma': 0.9565928715206633}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:49:21,221][0m Trial 48 finished with value: 0.07651959202377652 and parameters: {'observation_period_num': 67, 'train_rates': 0.8688686790311995, 'learning_rate': 8.148626110291151e-05, 'batch_size': 21, 'step_size': 9, 'gamma': 0.9887959229218207}. Best is trial 14 with value: 0.030180199248226066.[0m
[32m[I 2025-01-04 15:51:44,575][0m Trial 49 finished with value: 0.15414802520048052 and parameters: {'observation_period_num': 184, 'train_rates': 0.8106563525294082, 'learning_rate': 0.00037497114860161147, 'batch_size': 34, 'step_size': 7, 'gamma': 0.9791480606212932}. Best is trial 14 with value: 0.030180199248226066.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-04 15:51:44,585][0m A new study created in memory with name: no-name-3a37514a-f6d8-47df-8f1b-bdc48ba19aab[0m
[32m[I 2025-01-04 15:52:24,312][0m Trial 0 finished with value: 0.39901633090620875 and parameters: {'observation_period_num': 228, 'train_rates': 0.9454062377560148, 'learning_rate': 6.310663287437556e-06, 'batch_size': 145, 'step_size': 8, 'gamma': 0.7600962852323163}. Best is trial 0 with value: 0.39901633090620875.[0m
[32m[I 2025-01-04 15:52:53,950][0m Trial 1 finished with value: 0.10800992336140171 and parameters: {'observation_period_num': 145, 'train_rates': 0.917035738035983, 'learning_rate': 5.4117109414052055e-05, 'batch_size': 200, 'step_size': 7, 'gamma': 0.845310327968366}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:53:19,937][0m Trial 2 finished with value: 0.16640442609786987 and parameters: {'observation_period_num': 224, 'train_rates': 0.9523635677539175, 'learning_rate': 0.00032561335175430764, 'batch_size': 234, 'step_size': 3, 'gamma': 0.7692699712164841}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:53:45,128][0m Trial 3 finished with value: 0.6889410614967346 and parameters: {'observation_period_num': 131, 'train_rates': 0.9731923210792923, 'learning_rate': 2.1752581420279743e-06, 'batch_size': 239, 'step_size': 5, 'gamma': 0.8709656360488067}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:55:06,464][0m Trial 4 finished with value: 0.41575592202449974 and parameters: {'observation_period_num': 237, 'train_rates': 0.6413879988406929, 'learning_rate': 2.3764599262072577e-05, 'batch_size': 51, 'step_size': 1, 'gamma': 0.9359850964830434}. Best is trial 1 with value: 0.10800992336140171.[0m
Early stopping at epoch 62
[32m[I 2025-01-04 15:55:30,233][0m Trial 5 finished with value: 1.0302927494049072 and parameters: {'observation_period_num': 188, 'train_rates': 0.969639468478523, 'learning_rate': 1.0331270345780544e-06, 'batch_size': 159, 'step_size': 1, 'gamma': 0.8710404696031555}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:56:49,020][0m Trial 6 finished with value: 0.2533736377954483 and parameters: {'observation_period_num': 118, 'train_rates': 0.6626141658610759, 'learning_rate': 7.061237843852926e-05, 'batch_size': 58, 'step_size': 2, 'gamma': 0.8573102841793018}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:57:18,322][0m Trial 7 finished with value: 0.9367284062461931 and parameters: {'observation_period_num': 142, 'train_rates': 0.6537032206702098, 'learning_rate': 3.2134341164851586e-06, 'batch_size': 164, 'step_size': 2, 'gamma': 0.9191843510739284}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:57:45,716][0m Trial 8 finished with value: 0.21420889920382588 and parameters: {'observation_period_num': 66, 'train_rates': 0.7570697070192277, 'learning_rate': 0.00020504179497861678, 'batch_size': 211, 'step_size': 7, 'gamma': 0.7797863411317173}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:58:48,492][0m Trial 9 finished with value: 0.6304776124690535 and parameters: {'observation_period_num': 60, 'train_rates': 0.7000121240321364, 'learning_rate': 1.5857432752559262e-06, 'batch_size': 80, 'step_size': 7, 'gamma': 0.8891483936905122}. Best is trial 1 with value: 0.10800992336140171.[0m
[32m[I 2025-01-04 15:59:42,178][0m Trial 10 finished with value: 0.031125001295983234 and parameters: {'observation_period_num': 9, 'train_rates': 0.854657590834862, 'learning_rate': 0.000985849791705844, 'batch_size': 105, 'step_size': 13, 'gamma': 0.8161971893753791}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:00:33,818][0m Trial 11 finished with value: 0.03450209882410931 and parameters: {'observation_period_num': 18, 'train_rates': 0.8606821270924344, 'learning_rate': 0.0009298938260853309, 'batch_size': 110, 'step_size': 13, 'gamma': 0.819486932422761}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:01:30,174][0m Trial 12 finished with value: 0.03247339097852601 and parameters: {'observation_period_num': 12, 'train_rates': 0.8544888366740883, 'learning_rate': 0.000918231285622053, 'batch_size': 101, 'step_size': 14, 'gamma': 0.8189724109343499}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:06:03,016][0m Trial 13 finished with value: 0.04512698058650352 and parameters: {'observation_period_num': 9, 'train_rates': 0.8221878700740674, 'learning_rate': 0.0008746191278100612, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8142677079596604}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:06:50,745][0m Trial 14 finished with value: 0.05337706082804718 and parameters: {'observation_period_num': 56, 'train_rates': 0.8626378486685212, 'learning_rate': 0.00029434653247318676, 'batch_size': 117, 'step_size': 11, 'gamma': 0.8070035335670747}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:07:48,867][0m Trial 15 finished with value: 0.17849986310871435 and parameters: {'observation_period_num': 36, 'train_rates': 0.7836204708419855, 'learning_rate': 0.0001531009764881676, 'batch_size': 89, 'step_size': 11, 'gamma': 0.8251474864200508}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:08:37,048][0m Trial 16 finished with value: 0.08434042325329855 and parameters: {'observation_period_num': 93, 'train_rates': 0.8870969295754648, 'learning_rate': 1.5024677688854902e-05, 'batch_size': 118, 'step_size': 15, 'gamma': 0.9790183628718644}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:09:04,741][0m Trial 17 finished with value: 0.2024447689202509 and parameters: {'observation_period_num': 91, 'train_rates': 0.730740273170209, 'learning_rate': 0.0005131439316484149, 'batch_size': 176, 'step_size': 12, 'gamma': 0.786509977602643}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:10:08,296][0m Trial 18 finished with value: 0.04514992673222611 and parameters: {'observation_period_num': 34, 'train_rates': 0.8317008278588292, 'learning_rate': 9.802875570814278e-05, 'batch_size': 84, 'step_size': 13, 'gamma': 0.7955952013717383}. Best is trial 10 with value: 0.031125001295983234.[0m
[32m[I 2025-01-04 16:12:39,627][0m Trial 19 finished with value: 0.030027834888623685 and parameters: {'observation_period_num': 6, 'train_rates': 0.800697424381475, 'learning_rate': 0.0005189059376210182, 'batch_size': 34, 'step_size': 10, 'gamma': 0.8366595287915574}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:17:31,794][0m Trial 20 finished with value: 0.11522815261994825 and parameters: {'observation_period_num': 183, 'train_rates': 0.7910847929492895, 'learning_rate': 0.000504579854417192, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9126172721395919}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:19:19,972][0m Trial 21 finished with value: 0.04207067044606753 and parameters: {'observation_period_num': 7, 'train_rates': 0.8246149275896532, 'learning_rate': 0.0009965157053865616, 'batch_size': 48, 'step_size': 14, 'gamma': 0.8382310337026702}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:20:03,651][0m Trial 22 finished with value: 0.045188361653081856 and parameters: {'observation_period_num': 35, 'train_rates': 0.8936867694385284, 'learning_rate': 0.00042724157327586306, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8375818260723936}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:20:53,244][0m Trial 23 finished with value: 0.170463220086591 and parameters: {'observation_period_num': 28, 'train_rates': 0.7524501639542076, 'learning_rate': 0.00015197964371248946, 'batch_size': 102, 'step_size': 13, 'gamma': 0.7987338403961393}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:22:15,072][0m Trial 24 finished with value: 0.07841836467937187 and parameters: {'observation_period_num': 77, 'train_rates': 0.8592258535995785, 'learning_rate': 0.0005818518018757069, 'batch_size': 66, 'step_size': 9, 'gamma': 0.8502003694269076}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:24:44,709][0m Trial 25 finished with value: 0.17356567889245378 and parameters: {'observation_period_num': 46, 'train_rates': 0.6031329591254649, 'learning_rate': 0.0002184451373338652, 'batch_size': 28, 'step_size': 12, 'gamma': 0.7507742635077592}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:27:04,202][0m Trial 26 finished with value: 0.03086494243890047 and parameters: {'observation_period_num': 5, 'train_rates': 0.9153624280514553, 'learning_rate': 0.0005377378821170513, 'batch_size': 41, 'step_size': 14, 'gamma': 0.8297978926071194}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:29:35,910][0m Trial 27 finished with value: 0.053834738346134746 and parameters: {'observation_period_num': 82, 'train_rates': 0.9145283185496559, 'learning_rate': 4.1538394702971554e-05, 'batch_size': 36, 'step_size': 11, 'gamma': 0.896472396291016}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:31:58,488][0m Trial 28 finished with value: 0.03854835667508714 and parameters: {'observation_period_num': 24, 'train_rates': 0.9199675155354428, 'learning_rate': 0.00011179614071110038, 'batch_size': 40, 'step_size': 14, 'gamma': 0.8718714761097135}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:33:17,374][0m Trial 29 finished with value: 0.2247127619023635 and parameters: {'observation_period_num': 111, 'train_rates': 0.8873710293640384, 'learning_rate': 6.7590707166380245e-06, 'batch_size': 69, 'step_size': 9, 'gamma': 0.7707133919729354}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:33:58,313][0m Trial 30 finished with value: 0.043866786350410567 and parameters: {'observation_period_num': 42, 'train_rates': 0.8019198927668971, 'learning_rate': 0.0003594013283219807, 'batch_size': 131, 'step_size': 12, 'gamma': 0.833482154871203}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:34:55,483][0m Trial 31 finished with value: 0.03011795915931249 and parameters: {'observation_period_num': 7, 'train_rates': 0.8587112714698154, 'learning_rate': 0.0005695022254552413, 'batch_size': 98, 'step_size': 14, 'gamma': 0.8178106161463311}. Best is trial 19 with value: 0.030027834888623685.[0m
[32m[I 2025-01-04 16:36:17,643][0m Trial 32 finished with value: 0.02898004492468173 and parameters: {'observation_period_num': 6, 'train_rates': 0.9315214144498237, 'learning_rate': 0.0006633091176644882, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8570004682855097}. Best is trial 32 with value: 0.02898004492468173.[0m
[32m[I 2025-01-04 16:37:44,304][0m Trial 33 finished with value: 0.02501820548608595 and parameters: {'observation_period_num': 5, 'train_rates': 0.9337847183065806, 'learning_rate': 0.0002621386931709666, 'batch_size': 68, 'step_size': 15, 'gamma': 0.8569374159978577}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:39:05,462][0m Trial 34 finished with value: 0.07132850838659784 and parameters: {'observation_period_num': 53, 'train_rates': 0.9418647880919169, 'learning_rate': 0.00027244273694969676, 'batch_size': 72, 'step_size': 15, 'gamma': 0.8541027837685367}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:40:14,232][0m Trial 35 finished with value: 0.045572586357593536 and parameters: {'observation_period_num': 25, 'train_rates': 0.98337706698574, 'learning_rate': 0.0006404132271714619, 'batch_size': 89, 'step_size': 15, 'gamma': 0.8896524848025865}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:41:49,818][0m Trial 36 finished with value: 0.09870442574074169 and parameters: {'observation_period_num': 183, 'train_rates': 0.939944591508192, 'learning_rate': 0.00034936164499986755, 'batch_size': 57, 'step_size': 4, 'gamma': 0.8598414272696795}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:43:17,226][0m Trial 37 finished with value: 0.0634475367743585 and parameters: {'observation_period_num': 23, 'train_rates': 0.9600437506359081, 'learning_rate': 2.1290585342189315e-05, 'batch_size': 67, 'step_size': 6, 'gamma': 0.8805072340029247}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:45:04,925][0m Trial 38 finished with value: 0.05639393864623686 and parameters: {'observation_period_num': 71, 'train_rates': 0.9301190348522953, 'learning_rate': 6.566535448413908e-05, 'batch_size': 53, 'step_size': 8, 'gamma': 0.8460485102078562}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:48:08,202][0m Trial 39 finished with value: 0.10862086927923172 and parameters: {'observation_period_num': 166, 'train_rates': 0.9010565905590994, 'learning_rate': 0.00018671924213419297, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8617775194419943}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:48:47,079][0m Trial 40 finished with value: 0.1299203336238861 and parameters: {'observation_period_num': 219, 'train_rates': 0.9593047835152886, 'learning_rate': 0.00010343854375751564, 'batch_size': 147, 'step_size': 15, 'gamma': 0.9490365761769151}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:51:07,623][0m Trial 41 finished with value: 0.033349779888045834 and parameters: {'observation_period_num': 17, 'train_rates': 0.908471771740352, 'learning_rate': 0.0006000149743473828, 'batch_size': 40, 'step_size': 14, 'gamma': 0.8296735121096354}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:53:09,439][0m Trial 42 finished with value: 0.032096742177513286 and parameters: {'observation_period_num': 8, 'train_rates': 0.8797450468319891, 'learning_rate': 0.0002712177423573074, 'batch_size': 46, 'step_size': 13, 'gamma': 0.8434352296394151}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:54:32,698][0m Trial 43 finished with value: 0.03000561147928238 and parameters: {'observation_period_num': 5, 'train_rates': 0.9813357877087708, 'learning_rate': 0.00042586945145836765, 'batch_size': 74, 'step_size': 14, 'gamma': 0.8030828794337056}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:56:09,716][0m Trial 44 finished with value: 0.046009231358766556 and parameters: {'observation_period_num': 52, 'train_rates': 0.9863931878118055, 'learning_rate': 0.00040120181311113464, 'batch_size': 62, 'step_size': 12, 'gamma': 0.8040619061156818}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:57:27,926][0m Trial 45 finished with value: 0.03612664279838403 and parameters: {'observation_period_num': 22, 'train_rates': 0.965502050988305, 'learning_rate': 0.0007276163947025385, 'batch_size': 77, 'step_size': 15, 'gamma': 0.7878509858167211}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:58:30,842][0m Trial 46 finished with value: 0.05556652424010364 and parameters: {'observation_period_num': 43, 'train_rates': 0.9472009023469508, 'learning_rate': 0.0007272168081511986, 'batch_size': 92, 'step_size': 13, 'gamma': 0.8689935257170184}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:59:28,493][0m Trial 47 finished with value: 0.03188587997725675 and parameters: {'observation_period_num': 17, 'train_rates': 0.8376873308097057, 'learning_rate': 0.00025099401374246093, 'batch_size': 95, 'step_size': 14, 'gamma': 0.7723386158755258}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 16:59:50,378][0m Trial 48 finished with value: 0.13044221685180124 and parameters: {'observation_period_num': 247, 'train_rates': 0.804140208712859, 'learning_rate': 0.00040583571291154724, 'batch_size': 255, 'step_size': 5, 'gamma': 0.814100802686841}. Best is trial 33 with value: 0.02501820548608595.[0m
[32m[I 2025-01-04 17:00:52,044][0m Trial 49 finished with value: 0.17359474050985596 and parameters: {'observation_period_num': 34, 'train_rates': 0.7660663653623492, 'learning_rate': 0.00018145617293812672, 'batch_size': 83, 'step_size': 15, 'gamma': 0.881420232665784}. Best is trial 33 with value: 0.02501820548608595.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 17:00:52,054][0m A new study created in memory with name: no-name-0109501e-9d87-478a-b274-2c47fb5b7aed[0m
[32m[I 2025-01-04 17:01:19,170][0m Trial 0 finished with value: 0.04534217009607416 and parameters: {'observation_period_num': 35, 'train_rates': 0.8577706510709948, 'learning_rate': 0.0007761822956660781, 'batch_size': 218, 'step_size': 8, 'gamma': 0.7936997877358124}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:01:52,874][0m Trial 1 finished with value: 0.12342046861137662 and parameters: {'observation_period_num': 242, 'train_rates': 0.8452224565049011, 'learning_rate': 0.00048271483297508976, 'batch_size': 156, 'step_size': 11, 'gamma': 0.9478549542242821}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:04:21,159][0m Trial 2 finished with value: 0.1359575731679797 and parameters: {'observation_period_num': 184, 'train_rates': 0.9569107039328544, 'learning_rate': 8.343732135294745e-06, 'batch_size': 37, 'step_size': 6, 'gamma': 0.9454695737459643}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:05:17,504][0m Trial 3 finished with value: 0.09305013523947808 and parameters: {'observation_period_num': 102, 'train_rates': 0.902423611717426, 'learning_rate': 0.0007534241667153753, 'batch_size': 100, 'step_size': 8, 'gamma': 0.8801439944942175}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:06:02,927][0m Trial 4 finished with value: 0.08111593325932821 and parameters: {'observation_period_num': 120, 'train_rates': 0.9314391074461739, 'learning_rate': 0.0005909312117536498, 'batch_size': 127, 'step_size': 2, 'gamma': 0.8941722730806956}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:07:00,329][0m Trial 5 finished with value: 0.06907803697324304 and parameters: {'observation_period_num': 113, 'train_rates': 0.9335050842827424, 'learning_rate': 0.00011487072663475732, 'batch_size': 101, 'step_size': 7, 'gamma': 0.8644647012114338}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:07:29,972][0m Trial 6 finished with value: 0.38331877669346065 and parameters: {'observation_period_num': 168, 'train_rates': 0.6474795313646103, 'learning_rate': 2.0980647644834025e-05, 'batch_size': 162, 'step_size': 7, 'gamma': 0.8499377415810276}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:07:55,994][0m Trial 7 finished with value: 0.3615393531348171 and parameters: {'observation_period_num': 77, 'train_rates': 0.838900769709013, 'learning_rate': 1.2222137329517857e-05, 'batch_size': 221, 'step_size': 3, 'gamma': 0.8288741865377425}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:08:33,269][0m Trial 8 finished with value: 0.4586080312728882 and parameters: {'observation_period_num': 53, 'train_rates': 0.9577306507343206, 'learning_rate': 3.860875517914654e-06, 'batch_size': 170, 'step_size': 8, 'gamma': 0.8623934370263353}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:09:03,257][0m Trial 9 finished with value: 0.28498145214658466 and parameters: {'observation_period_num': 185, 'train_rates': 0.7283957045434969, 'learning_rate': 0.00014171244625276087, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9393701050553176}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:09:25,818][0m Trial 10 finished with value: 0.7715598042402789 and parameters: {'observation_period_num': 18, 'train_rates': 0.7450477275439413, 'learning_rate': 1.1217730014899747e-06, 'batch_size': 249, 'step_size': 15, 'gamma': 0.7752031555991269}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:10:42,068][0m Trial 11 finished with value: 0.04842958200514461 and parameters: {'observation_period_num': 5, 'train_rates': 0.8607339939574072, 'learning_rate': 0.00010097224698935068, 'batch_size': 73, 'step_size': 5, 'gamma': 0.7632652027830971}. Best is trial 0 with value: 0.04534217009607416.[0m
[32m[I 2025-01-04 17:12:58,927][0m Trial 12 finished with value: 0.03872329750201319 and parameters: {'observation_period_num': 10, 'train_rates': 0.8434370907361946, 'learning_rate': 0.00010886376400770347, 'batch_size': 39, 'step_size': 4, 'gamma': 0.7501391562059744}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:15:56,631][0m Trial 13 finished with value: 0.20050658729213935 and parameters: {'observation_period_num': 42, 'train_rates': 0.7768593730063034, 'learning_rate': 0.00023645088224608812, 'batch_size': 28, 'step_size': 11, 'gamma': 0.8028030723196123}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:16:21,589][0m Trial 14 finished with value: 0.31322360206476024 and parameters: {'observation_period_num': 55, 'train_rates': 0.6814558240408837, 'learning_rate': 5.1894225669661446e-05, 'batch_size': 206, 'step_size': 4, 'gamma': 0.7954598873183454}. Best is trial 12 with value: 0.03872329750201319.[0m
Early stopping at epoch 48
[32m[I 2025-01-04 17:16:36,261][0m Trial 15 finished with value: 0.1357396178360702 and parameters: {'observation_period_num': 25, 'train_rates': 0.8094212821726636, 'learning_rate': 0.00030256011267031096, 'batch_size': 207, 'step_size': 1, 'gamma': 0.7529504929422682}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:17:57,492][0m Trial 16 finished with value: 0.05860076673807354 and parameters: {'observation_period_num': 74, 'train_rates': 0.8865121646680967, 'learning_rate': 5.46868448437655e-05, 'batch_size': 68, 'step_size': 10, 'gamma': 0.8080311526110218}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:18:24,271][0m Trial 17 finished with value: 0.11151985079050064 and parameters: {'observation_period_num': 152, 'train_rates': 0.9880202792600458, 'learning_rate': 0.000864168019495412, 'batch_size': 249, 'step_size': 4, 'gamma': 0.7898973365464462}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:19:05,440][0m Trial 18 finished with value: 0.057484847803910576 and parameters: {'observation_period_num': 82, 'train_rates': 0.7996581719943446, 'learning_rate': 0.00022741440204201424, 'batch_size': 128, 'step_size': 9, 'gamma': 0.8321534023665299}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:20:19,430][0m Trial 19 finished with value: 0.2072416551931857 and parameters: {'observation_period_num': 39, 'train_rates': 0.6024693894412925, 'learning_rate': 3.8846503717186655e-05, 'batch_size': 58, 'step_size': 5, 'gamma': 0.9067987055735418}. Best is trial 12 with value: 0.03872329750201319.[0m
Early stopping at epoch 72
[32m[I 2025-01-04 17:23:39,153][0m Trial 20 finished with value: 0.17685111280044782 and parameters: {'observation_period_num': 6, 'train_rates': 0.7659241659946384, 'learning_rate': 0.000411532406458582, 'batch_size': 18, 'step_size': 1, 'gamma': 0.7514847547352239}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:24:52,903][0m Trial 21 finished with value: 0.049434740006780394 and parameters: {'observation_period_num': 12, 'train_rates': 0.8586591146149929, 'learning_rate': 8.221873891254663e-05, 'batch_size': 75, 'step_size': 5, 'gamma': 0.7696232995699117}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:26:48,282][0m Trial 22 finished with value: 0.086525400816384 and parameters: {'observation_period_num': 31, 'train_rates': 0.8789132308753904, 'learning_rate': 0.00011485397692703979, 'batch_size': 48, 'step_size': 6, 'gamma': 0.9895002221311117}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:27:52,297][0m Trial 23 finished with value: 0.11187833125107133 and parameters: {'observation_period_num': 6, 'train_rates': 0.8231679380208262, 'learning_rate': 2.2240757448704995e-05, 'batch_size': 86, 'step_size': 3, 'gamma': 0.7715813650347386}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:28:47,017][0m Trial 24 finished with value: 0.052129872514370304 and parameters: {'observation_period_num': 56, 'train_rates': 0.9101177080580385, 'learning_rate': 0.0001824722740309217, 'batch_size': 107, 'step_size': 6, 'gamma': 0.8190954664901902}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:30:45,863][0m Trial 25 finished with value: 0.07494131621586125 and parameters: {'observation_period_num': 94, 'train_rates': 0.8661629222154302, 'learning_rate': 7.212480862871382e-05, 'batch_size': 45, 'step_size': 4, 'gamma': 0.782721718285412}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:31:45,160][0m Trial 26 finished with value: 0.12200402047323144 and parameters: {'observation_period_num': 240, 'train_rates': 0.8220912610681307, 'learning_rate': 0.0009744979123855078, 'batch_size': 84, 'step_size': 9, 'gamma': 0.7529812158514676}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:35:28,874][0m Trial 27 finished with value: 0.1680561835181062 and parameters: {'observation_period_num': 30, 'train_rates': 0.7150225533091812, 'learning_rate': 3.166090662289974e-05, 'batch_size': 21, 'step_size': 7, 'gamma': 0.7670638551080551}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:35:57,230][0m Trial 28 finished with value: 0.13032385330708302 and parameters: {'observation_period_num': 144, 'train_rates': 0.783144734211096, 'learning_rate': 0.0003610960603837755, 'batch_size': 188, 'step_size': 3, 'gamma': 0.809853509050497}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:36:35,610][0m Trial 29 finished with value: 0.3232909980519065 and parameters: {'observation_period_num': 66, 'train_rates': 0.839594834349883, 'learning_rate': 3.40502229909336e-06, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8423802157043286}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:37:55,383][0m Trial 30 finished with value: 0.0944008620228256 and parameters: {'observation_period_num': 226, 'train_rates': 0.8532465532855611, 'learning_rate': 0.0004956464906976865, 'batch_size': 64, 'step_size': 5, 'gamma': 0.789365216369939}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:39:07,098][0m Trial 31 finished with value: 0.051371626993641256 and parameters: {'observation_period_num': 15, 'train_rates': 0.8641718699978185, 'learning_rate': 8.148496317886717e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.7684760654678146}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:41:22,767][0m Trial 32 finished with value: 0.03966447624356248 and parameters: {'observation_period_num': 5, 'train_rates': 0.898048383288625, 'learning_rate': 0.0001051987428821601, 'batch_size': 42, 'step_size': 5, 'gamma': 0.7639053840635427}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:43:36,391][0m Trial 33 finished with value: 0.055761642390634955 and parameters: {'observation_period_num': 38, 'train_rates': 0.9016292378859523, 'learning_rate': 0.00013846752038989356, 'batch_size': 42, 'step_size': 2, 'gamma': 0.7523531617159573}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:44:29,068][0m Trial 34 finished with value: 0.03925183789759147 and parameters: {'observation_period_num': 5, 'train_rates': 0.9305249170516197, 'learning_rate': 0.00019217410509143586, 'batch_size': 116, 'step_size': 5, 'gamma': 0.7826214577921554}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:45:22,778][0m Trial 35 finished with value: 0.03877728943450329 and parameters: {'observation_period_num': 27, 'train_rates': 0.9333919595641371, 'learning_rate': 0.0006206882353805006, 'batch_size': 113, 'step_size': 9, 'gamma': 0.7844174022735506}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:46:13,957][0m Trial 36 finished with value: 0.03979468876213738 and parameters: {'observation_period_num': 22, 'train_rates': 0.9326503959718182, 'learning_rate': 0.00019203032775929262, 'batch_size': 114, 'step_size': 9, 'gamma': 0.7812998986813304}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:46:58,981][0m Trial 37 finished with value: 0.057777289301157 and parameters: {'observation_period_num': 47, 'train_rates': 0.9642984832127441, 'learning_rate': 0.0006561937111098808, 'batch_size': 139, 'step_size': 7, 'gamma': 0.797637757836081}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:47:49,351][0m Trial 38 finished with value: 0.05802727665707289 and parameters: {'observation_period_num': 26, 'train_rates': 0.915043524645674, 'learning_rate': 0.000283678413676475, 'batch_size': 119, 'step_size': 2, 'gamma': 0.8126884974641675}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:48:52,251][0m Trial 39 finished with value: 0.0800824910402298 and parameters: {'observation_period_num': 64, 'train_rates': 0.981952090646224, 'learning_rate': 0.00046530383352459014, 'batch_size': 96, 'step_size': 10, 'gamma': 0.8831706732020383}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:51:47,879][0m Trial 40 finished with value: 0.07429789634127366 and parameters: {'observation_period_num': 93, 'train_rates': 0.9469669376217842, 'learning_rate': 0.00015991739744828872, 'batch_size': 32, 'step_size': 7, 'gamma': 0.7798236788598047}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:52:42,105][0m Trial 41 finished with value: 0.04057153695205736 and parameters: {'observation_period_num': 21, 'train_rates': 0.9392722754544345, 'learning_rate': 0.00019392927530477157, 'batch_size': 114, 'step_size': 9, 'gamma': 0.78420501248647}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:53:45,772][0m Trial 42 finished with value: 0.0481939754739606 and parameters: {'observation_period_num': 17, 'train_rates': 0.9250342895903646, 'learning_rate': 5.6425024896881915e-05, 'batch_size': 93, 'step_size': 10, 'gamma': 0.8227500479978102}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:54:25,719][0m Trial 43 finished with value: 0.0413657415416034 and parameters: {'observation_period_num': 36, 'train_rates': 0.8894558315566821, 'learning_rate': 0.0003152466061575941, 'batch_size': 149, 'step_size': 8, 'gamma': 0.7622378081719154}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:56:16,558][0m Trial 44 finished with value: 0.04160039262927097 and parameters: {'observation_period_num': 23, 'train_rates': 0.9608057832627178, 'learning_rate': 0.00011631763178058125, 'batch_size': 54, 'step_size': 11, 'gamma': 0.7815314951629778}. Best is trial 12 with value: 0.03872329750201319.[0m
[32m[I 2025-01-04 17:57:02,091][0m Trial 45 finished with value: 0.03229001318415006 and parameters: {'observation_period_num': 5, 'train_rates': 0.9238577739971966, 'learning_rate': 0.0006257673578872927, 'batch_size': 132, 'step_size': 8, 'gamma': 0.7979799875237218}. Best is trial 45 with value: 0.03229001318415006.[0m
[32m[I 2025-01-04 17:57:48,274][0m Trial 46 finished with value: 0.029588880650455656 and parameters: {'observation_period_num': 6, 'train_rates': 0.8964358836548523, 'learning_rate': 0.0008067522483319194, 'batch_size': 130, 'step_size': 4, 'gamma': 0.8491674302715776}. Best is trial 46 with value: 0.029588880650455656.[0m
[32m[I 2025-01-04 17:58:33,852][0m Trial 47 finished with value: 0.06274734828467211 and parameters: {'observation_period_num': 48, 'train_rates': 0.9168557317145998, 'learning_rate': 0.0006572979488367545, 'batch_size': 130, 'step_size': 13, 'gamma': 0.8721659244558657}. Best is trial 46 with value: 0.029588880650455656.[0m
[32m[I 2025-01-04 17:59:11,457][0m Trial 48 finished with value: 0.09342551231384277 and parameters: {'observation_period_num': 119, 'train_rates': 0.9712159788635694, 'learning_rate': 0.0005502019002025704, 'batch_size': 163, 'step_size': 4, 'gamma': 0.8514939500673846}. Best is trial 46 with value: 0.029588880650455656.[0m
[32m[I 2025-01-04 17:59:42,734][0m Trial 49 finished with value: 0.03995176542388356 and parameters: {'observation_period_num': 14, 'train_rates': 0.8750284258890453, 'learning_rate': 0.0007811875012593545, 'batch_size': 185, 'step_size': 3, 'gamma': 0.8311636369662528}. Best is trial 46 with value: 0.029588880650455656.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 11, 'train_rates': 0.9424276143814838, 'learning_rate': 0.0009309185747357336, 'batch_size': 119, 'step_size': 14, 'gamma': 0.7624354557953175}
Epoch 1/300, trend Loss: 0.4161 | 0.1296
Epoch 2/300, trend Loss: 0.1777 | 0.1485
Epoch 3/300, trend Loss: 0.1287 | 0.0893
Epoch 4/300, trend Loss: 0.1287 | 0.0922
Epoch 5/300, trend Loss: 0.1166 | 0.0683
Epoch 6/300, trend Loss: 0.1075 | 0.0638
Epoch 7/300, trend Loss: 0.1093 | 0.0607
Epoch 8/300, trend Loss: 0.1095 | 0.0691
Epoch 9/300, trend Loss: 0.1085 | 0.0719
Epoch 10/300, trend Loss: 0.1076 | 0.0690
Epoch 11/300, trend Loss: 0.1022 | 0.0651
Epoch 12/300, trend Loss: 0.0974 | 0.0696
Epoch 13/300, trend Loss: 0.0927 | 0.0487
Epoch 14/300, trend Loss: 0.0839 | 0.0444
Epoch 15/300, trend Loss: 0.0830 | 0.0466
Epoch 16/300, trend Loss: 0.0823 | 0.0511
Epoch 17/300, trend Loss: 0.0839 | 0.0466
Epoch 18/300, trend Loss: 0.0830 | 0.0452
Epoch 19/300, trend Loss: 0.0878 | 0.0460
Epoch 20/300, trend Loss: 0.0840 | 0.0486
Epoch 21/300, trend Loss: 0.0822 | 0.0418
Epoch 22/300, trend Loss: 0.0840 | 0.0431
Epoch 23/300, trend Loss: 0.0857 | 0.0450
Epoch 24/300, trend Loss: 0.0822 | 0.0434
Epoch 25/300, trend Loss: 0.0755 | 0.0396
Epoch 26/300, trend Loss: 0.0750 | 0.0401
Epoch 27/300, trend Loss: 0.0759 | 0.0389
Epoch 28/300, trend Loss: 0.0751 | 0.0397
Epoch 29/300, trend Loss: 0.0772 | 0.0435
Epoch 30/300, trend Loss: 0.0833 | 0.0495
Epoch 31/300, trend Loss: 0.0917 | 0.0526
Epoch 32/300, trend Loss: 0.0857 | 0.0428
Epoch 33/300, trend Loss: 0.1001 | 0.0569
Epoch 34/300, trend Loss: 0.1048 | 0.0553
Epoch 35/300, trend Loss: 0.1010 | 0.0884
Epoch 36/300, trend Loss: 0.1020 | 0.1150
Epoch 37/300, trend Loss: 0.1137 | 0.0546
Epoch 38/300, trend Loss: 0.1060 | 0.0515
Epoch 39/300, trend Loss: 0.0991 | 0.0529
Epoch 40/300, trend Loss: 0.0987 | 0.0481
Epoch 41/300, trend Loss: 0.0862 | 0.0391
Epoch 42/300, trend Loss: 0.0767 | 0.0361
Epoch 43/300, trend Loss: 0.0714 | 0.0434
Epoch 44/300, trend Loss: 0.0722 | 0.0460
Epoch 45/300, trend Loss: 0.0765 | 0.0456
Epoch 46/300, trend Loss: 0.0728 | 0.0351
Epoch 47/300, trend Loss: 0.0711 | 0.0347
Epoch 48/300, trend Loss: 0.0678 | 0.0353
Epoch 49/300, trend Loss: 0.0676 | 0.0346
Epoch 50/300, trend Loss: 0.0671 | 0.0339
Epoch 51/300, trend Loss: 0.0664 | 0.0339
Epoch 52/300, trend Loss: 0.0659 | 0.0343
Epoch 53/300, trend Loss: 0.0649 | 0.0324
Epoch 54/300, trend Loss: 0.0646 | 0.0323
Epoch 55/300, trend Loss: 0.0644 | 0.0328
Epoch 56/300, trend Loss: 0.0642 | 0.0328
Epoch 57/300, trend Loss: 0.0642 | 0.0323
Epoch 58/300, trend Loss: 0.0640 | 0.0322
Epoch 59/300, trend Loss: 0.0637 | 0.0321
Epoch 60/300, trend Loss: 0.0634 | 0.0320
Epoch 61/300, trend Loss: 0.0631 | 0.0319
Epoch 62/300, trend Loss: 0.0629 | 0.0318
Epoch 63/300, trend Loss: 0.0627 | 0.0315
Epoch 64/300, trend Loss: 0.0624 | 0.0311
Epoch 65/300, trend Loss: 0.0622 | 0.0310
Epoch 66/300, trend Loss: 0.0621 | 0.0310
Epoch 67/300, trend Loss: 0.0619 | 0.0308
Epoch 68/300, trend Loss: 0.0618 | 0.0306
Epoch 69/300, trend Loss: 0.0616 | 0.0304
Epoch 70/300, trend Loss: 0.0614 | 0.0303
Epoch 71/300, trend Loss: 0.0613 | 0.0301
Epoch 72/300, trend Loss: 0.0612 | 0.0300
Epoch 73/300, trend Loss: 0.0611 | 0.0299
Epoch 74/300, trend Loss: 0.0609 | 0.0297
Epoch 75/300, trend Loss: 0.0608 | 0.0295
Epoch 76/300, trend Loss: 0.0606 | 0.0294
Epoch 77/300, trend Loss: 0.0604 | 0.0292
Epoch 78/300, trend Loss: 0.0603 | 0.0291
Epoch 79/300, trend Loss: 0.0602 | 0.0290
Epoch 80/300, trend Loss: 0.0601 | 0.0290
Epoch 81/300, trend Loss: 0.0602 | 0.0292
Epoch 82/300, trend Loss: 0.0606 | 0.0298
Epoch 83/300, trend Loss: 0.0616 | 0.0283
Epoch 84/300, trend Loss: 0.0609 | 0.0294
Epoch 85/300, trend Loss: 0.0603 | 0.0287
Epoch 86/300, trend Loss: 0.0597 | 0.0289
Epoch 87/300, trend Loss: 0.0594 | 0.0288
Epoch 88/300, trend Loss: 0.0593 | 0.0286
Epoch 89/300, trend Loss: 0.0592 | 0.0285
Epoch 90/300, trend Loss: 0.0591 | 0.0285
Epoch 91/300, trend Loss: 0.0590 | 0.0285
Epoch 92/300, trend Loss: 0.0590 | 0.0285
Epoch 93/300, trend Loss: 0.0589 | 0.0285
Epoch 94/300, trend Loss: 0.0589 | 0.0285
Epoch 95/300, trend Loss: 0.0588 | 0.0284
Epoch 96/300, trend Loss: 0.0587 | 0.0283
Epoch 97/300, trend Loss: 0.0586 | 0.0283
Epoch 98/300, trend Loss: 0.0585 | 0.0282
Epoch 99/300, trend Loss: 0.0584 | 0.0281
Epoch 100/300, trend Loss: 0.0584 | 0.0281
Epoch 101/300, trend Loss: 0.0583 | 0.0280
Epoch 102/300, trend Loss: 0.0583 | 0.0279
Epoch 103/300, trend Loss: 0.0582 | 0.0279
Epoch 104/300, trend Loss: 0.0581 | 0.0279
Epoch 105/300, trend Loss: 0.0581 | 0.0278
Epoch 106/300, trend Loss: 0.0580 | 0.0277
Epoch 107/300, trend Loss: 0.0580 | 0.0277
Epoch 108/300, trend Loss: 0.0580 | 0.0277
Epoch 109/300, trend Loss: 0.0579 | 0.0276
Epoch 110/300, trend Loss: 0.0579 | 0.0276
Epoch 111/300, trend Loss: 0.0578 | 0.0275
Epoch 112/300, trend Loss: 0.0578 | 0.0275
Epoch 113/300, trend Loss: 0.0577 | 0.0274
Epoch 114/300, trend Loss: 0.0577 | 0.0274
Epoch 115/300, trend Loss: 0.0577 | 0.0274
Epoch 116/300, trend Loss: 0.0576 | 0.0274
Epoch 117/300, trend Loss: 0.0576 | 0.0273
Epoch 118/300, trend Loss: 0.0576 | 0.0273
Epoch 119/300, trend Loss: 0.0575 | 0.0273
Epoch 120/300, trend Loss: 0.0575 | 0.0272
Epoch 121/300, trend Loss: 0.0575 | 0.0272
Epoch 122/300, trend Loss: 0.0575 | 0.0272
Epoch 123/300, trend Loss: 0.0574 | 0.0272
Epoch 124/300, trend Loss: 0.0574 | 0.0271
Epoch 125/300, trend Loss: 0.0574 | 0.0271
Epoch 126/300, trend Loss: 0.0574 | 0.0271
Epoch 127/300, trend Loss: 0.0573 | 0.0270
Epoch 128/300, trend Loss: 0.0573 | 0.0270
Epoch 129/300, trend Loss: 0.0573 | 0.0270
Epoch 130/300, trend Loss: 0.0573 | 0.0270
Epoch 131/300, trend Loss: 0.0573 | 0.0270
Epoch 132/300, trend Loss: 0.0572 | 0.0270
Epoch 133/300, trend Loss: 0.0572 | 0.0269
Epoch 134/300, trend Loss: 0.0572 | 0.0269
Epoch 135/300, trend Loss: 0.0572 | 0.0269
Epoch 136/300, trend Loss: 0.0572 | 0.0269
Epoch 137/300, trend Loss: 0.0571 | 0.0269
Epoch 138/300, trend Loss: 0.0571 | 0.0269
Epoch 139/300, trend Loss: 0.0571 | 0.0268
Epoch 140/300, trend Loss: 0.0571 | 0.0268
Epoch 141/300, trend Loss: 0.0571 | 0.0268
Epoch 142/300, trend Loss: 0.0571 | 0.0268
Epoch 143/300, trend Loss: 0.0571 | 0.0268
Epoch 144/300, trend Loss: 0.0570 | 0.0268
Epoch 145/300, trend Loss: 0.0570 | 0.0268
Epoch 146/300, trend Loss: 0.0570 | 0.0268
Epoch 147/300, trend Loss: 0.0570 | 0.0268
Epoch 148/300, trend Loss: 0.0570 | 0.0267
Epoch 149/300, trend Loss: 0.0570 | 0.0267
Epoch 150/300, trend Loss: 0.0570 | 0.0267
Epoch 151/300, trend Loss: 0.0570 | 0.0267
Epoch 152/300, trend Loss: 0.0570 | 0.0267
Epoch 153/300, trend Loss: 0.0570 | 0.0267
Epoch 154/300, trend Loss: 0.0569 | 0.0267
Epoch 155/300, trend Loss: 0.0569 | 0.0267
Epoch 156/300, trend Loss: 0.0569 | 0.0267
Epoch 157/300, trend Loss: 0.0569 | 0.0267
Epoch 158/300, trend Loss: 0.0569 | 0.0267
Epoch 159/300, trend Loss: 0.0569 | 0.0266
Epoch 160/300, trend Loss: 0.0569 | 0.0266
Epoch 161/300, trend Loss: 0.0569 | 0.0266
Epoch 162/300, trend Loss: 0.0569 | 0.0266
Epoch 163/300, trend Loss: 0.0569 | 0.0266
Epoch 164/300, trend Loss: 0.0569 | 0.0266
Epoch 165/300, trend Loss: 0.0569 | 0.0266
Epoch 166/300, trend Loss: 0.0569 | 0.0266
Epoch 167/300, trend Loss: 0.0569 | 0.0266
Epoch 168/300, trend Loss: 0.0568 | 0.0266
Epoch 169/300, trend Loss: 0.0568 | 0.0266
Epoch 170/300, trend Loss: 0.0568 | 0.0266
Epoch 171/300, trend Loss: 0.0568 | 0.0266
Epoch 172/300, trend Loss: 0.0568 | 0.0266
Epoch 173/300, trend Loss: 0.0568 | 0.0266
Epoch 174/300, trend Loss: 0.0568 | 0.0266
Epoch 175/300, trend Loss: 0.0568 | 0.0266
Epoch 176/300, trend Loss: 0.0568 | 0.0266
Epoch 177/300, trend Loss: 0.0568 | 0.0266
Epoch 178/300, trend Loss: 0.0568 | 0.0266
Epoch 179/300, trend Loss: 0.0568 | 0.0265
Epoch 180/300, trend Loss: 0.0568 | 0.0265
Epoch 181/300, trend Loss: 0.0568 | 0.0265
Epoch 182/300, trend Loss: 0.0568 | 0.0265
Epoch 183/300, trend Loss: 0.0568 | 0.0265
Epoch 184/300, trend Loss: 0.0568 | 0.0265
Epoch 185/300, trend Loss: 0.0568 | 0.0265
Epoch 186/300, trend Loss: 0.0568 | 0.0265
Epoch 187/300, trend Loss: 0.0568 | 0.0265
Epoch 188/300, trend Loss: 0.0568 | 0.0265
Epoch 189/300, trend Loss: 0.0568 | 0.0265
Epoch 190/300, trend Loss: 0.0568 | 0.0265
Epoch 191/300, trend Loss: 0.0568 | 0.0265
Epoch 192/300, trend Loss: 0.0568 | 0.0265
Epoch 193/300, trend Loss: 0.0568 | 0.0265
Epoch 194/300, trend Loss: 0.0568 | 0.0265
Epoch 195/300, trend Loss: 0.0568 | 0.0265
Epoch 196/300, trend Loss: 0.0568 | 0.0265
Epoch 197/300, trend Loss: 0.0567 | 0.0265
Epoch 198/300, trend Loss: 0.0567 | 0.0265
Epoch 199/300, trend Loss: 0.0567 | 0.0265
Epoch 200/300, trend Loss: 0.0567 | 0.0265
Epoch 201/300, trend Loss: 0.0567 | 0.0265
Epoch 202/300, trend Loss: 0.0567 | 0.0265
Epoch 203/300, trend Loss: 0.0567 | 0.0265
Epoch 204/300, trend Loss: 0.0567 | 0.0265
Epoch 205/300, trend Loss: 0.0567 | 0.0265
Epoch 206/300, trend Loss: 0.0567 | 0.0265
Epoch 207/300, trend Loss: 0.0567 | 0.0265
Epoch 208/300, trend Loss: 0.0567 | 0.0265
Epoch 209/300, trend Loss: 0.0567 | 0.0265
Epoch 210/300, trend Loss: 0.0567 | 0.0265
Epoch 211/300, trend Loss: 0.0567 | 0.0265
Epoch 212/300, trend Loss: 0.0567 | 0.0265
Epoch 213/300, trend Loss: 0.0567 | 0.0265
Epoch 214/300, trend Loss: 0.0567 | 0.0265
Epoch 215/300, trend Loss: 0.0567 | 0.0265
Epoch 216/300, trend Loss: 0.0567 | 0.0265
Epoch 217/300, trend Loss: 0.0567 | 0.0265
Epoch 218/300, trend Loss: 0.0567 | 0.0265
Epoch 219/300, trend Loss: 0.0567 | 0.0265
Epoch 220/300, trend Loss: 0.0567 | 0.0265
Epoch 221/300, trend Loss: 0.0567 | 0.0265
Epoch 222/300, trend Loss: 0.0567 | 0.0265
Epoch 223/300, trend Loss: 0.0567 | 0.0265
Epoch 224/300, trend Loss: 0.0567 | 0.0265
Epoch 225/300, trend Loss: 0.0567 | 0.0265
Epoch 226/300, trend Loss: 0.0567 | 0.0265
Epoch 227/300, trend Loss: 0.0567 | 0.0265
Epoch 228/300, trend Loss: 0.0567 | 0.0265
Epoch 229/300, trend Loss: 0.0567 | 0.0265
Epoch 230/300, trend Loss: 0.0567 | 0.0265
Epoch 231/300, trend Loss: 0.0567 | 0.0265
Epoch 232/300, trend Loss: 0.0567 | 0.0265
Epoch 233/300, trend Loss: 0.0567 | 0.0265
Epoch 234/300, trend Loss: 0.0567 | 0.0265
Epoch 235/300, trend Loss: 0.0567 | 0.0265
Epoch 236/300, trend Loss: 0.0567 | 0.0265
Epoch 237/300, trend Loss: 0.0567 | 0.0265
Epoch 238/300, trend Loss: 0.0567 | 0.0265
Epoch 239/300, trend Loss: 0.0567 | 0.0265
Epoch 240/300, trend Loss: 0.0567 | 0.0265
Epoch 241/300, trend Loss: 0.0567 | 0.0265
Epoch 242/300, trend Loss: 0.0567 | 0.0265
Epoch 243/300, trend Loss: 0.0567 | 0.0265
Epoch 244/300, trend Loss: 0.0567 | 0.0265
Epoch 245/300, trend Loss: 0.0567 | 0.0265
Epoch 246/300, trend Loss: 0.0567 | 0.0265
Epoch 247/300, trend Loss: 0.0567 | 0.0265
Epoch 248/300, trend Loss: 0.0567 | 0.0265
Epoch 249/300, trend Loss: 0.0567 | 0.0265
Epoch 250/300, trend Loss: 0.0567 | 0.0265
Epoch 251/300, trend Loss: 0.0567 | 0.0265
Epoch 252/300, trend Loss: 0.0567 | 0.0265
Epoch 253/300, trend Loss: 0.0567 | 0.0265
Epoch 254/300, trend Loss: 0.0567 | 0.0265
Epoch 255/300, trend Loss: 0.0567 | 0.0265
Epoch 256/300, trend Loss: 0.0567 | 0.0265
Epoch 257/300, trend Loss: 0.0567 | 0.0265
Epoch 258/300, trend Loss: 0.0567 | 0.0265
Epoch 259/300, trend Loss: 0.0567 | 0.0265
Epoch 260/300, trend Loss: 0.0567 | 0.0265
Epoch 261/300, trend Loss: 0.0567 | 0.0265
Epoch 262/300, trend Loss: 0.0567 | 0.0265
Epoch 263/300, trend Loss: 0.0567 | 0.0265
Epoch 264/300, trend Loss: 0.0567 | 0.0265
Epoch 265/300, trend Loss: 0.0567 | 0.0265
Epoch 266/300, trend Loss: 0.0567 | 0.0265
Epoch 267/300, trend Loss: 0.0567 | 0.0265
Epoch 268/300, trend Loss: 0.0567 | 0.0265
Epoch 269/300, trend Loss: 0.0567 | 0.0265
Epoch 270/300, trend Loss: 0.0567 | 0.0265
Epoch 271/300, trend Loss: 0.0567 | 0.0265
Epoch 272/300, trend Loss: 0.0567 | 0.0265
Epoch 273/300, trend Loss: 0.0567 | 0.0265
Epoch 274/300, trend Loss: 0.0567 | 0.0265
Epoch 275/300, trend Loss: 0.0567 | 0.0265
Epoch 276/300, trend Loss: 0.0567 | 0.0265
Epoch 277/300, trend Loss: 0.0567 | 0.0265
Epoch 278/300, trend Loss: 0.0567 | 0.0265
Epoch 279/300, trend Loss: 0.0567 | 0.0265
Epoch 280/300, trend Loss: 0.0567 | 0.0265
Epoch 281/300, trend Loss: 0.0567 | 0.0265
Epoch 282/300, trend Loss: 0.0567 | 0.0265
Epoch 283/300, trend Loss: 0.0567 | 0.0265
Epoch 284/300, trend Loss: 0.0567 | 0.0265
Epoch 285/300, trend Loss: 0.0567 | 0.0265
Epoch 286/300, trend Loss: 0.0567 | 0.0265
Epoch 287/300, trend Loss: 0.0567 | 0.0265
Epoch 288/300, trend Loss: 0.0567 | 0.0265
Epoch 289/300, trend Loss: 0.0567 | 0.0265
Epoch 290/300, trend Loss: 0.0567 | 0.0265
Epoch 291/300, trend Loss: 0.0567 | 0.0265
Epoch 292/300, trend Loss: 0.0567 | 0.0265
Epoch 293/300, trend Loss: 0.0567 | 0.0265
Epoch 294/300, trend Loss: 0.0567 | 0.0265
Epoch 295/300, trend Loss: 0.0567 | 0.0265
Epoch 296/300, trend Loss: 0.0567 | 0.0264
Epoch 297/300, trend Loss: 0.0567 | 0.0264
Epoch 298/300, trend Loss: 0.0567 | 0.0264
Epoch 299/300, trend Loss: 0.0567 | 0.0264
Epoch 300/300, trend Loss: 0.0567 | 0.0264
Training seasonal_0 component with params: {'observation_period_num': 17, 'train_rates': 0.8378161157051117, 'learning_rate': 0.0002507689338846089, 'batch_size': 166, 'step_size': 10, 'gamma': 0.9750045259143277}
Epoch 1/300, seasonal_0 Loss: 0.3958 | 0.1782
Epoch 2/300, seasonal_0 Loss: 0.2157 | 0.1462
Epoch 3/300, seasonal_0 Loss: 0.1804 | 0.1264
Epoch 4/300, seasonal_0 Loss: 0.1900 | 0.1528
Epoch 5/300, seasonal_0 Loss: 0.1630 | 0.1522
Epoch 6/300, seasonal_0 Loss: 0.1552 | 0.0960
Epoch 7/300, seasonal_0 Loss: 0.1554 | 0.0993
Epoch 8/300, seasonal_0 Loss: 0.1455 | 0.0785
Epoch 9/300, seasonal_0 Loss: 0.1456 | 0.0941
Epoch 10/300, seasonal_0 Loss: 0.1576 | 0.0718
Epoch 11/300, seasonal_0 Loss: 0.1765 | 0.0856
Epoch 12/300, seasonal_0 Loss: 0.1799 | 0.1029
Epoch 13/300, seasonal_0 Loss: 0.1296 | 0.0797
Epoch 14/300, seasonal_0 Loss: 0.1530 | 0.1219
Epoch 15/300, seasonal_0 Loss: 0.1310 | 0.0718
Epoch 16/300, seasonal_0 Loss: 0.1321 | 0.0697
Epoch 17/300, seasonal_0 Loss: 0.1207 | 0.0708
Epoch 18/300, seasonal_0 Loss: 0.1222 | 0.0654
Epoch 19/300, seasonal_0 Loss: 0.1116 | 0.0656
Epoch 20/300, seasonal_0 Loss: 0.1114 | 0.0627
Epoch 21/300, seasonal_0 Loss: 0.1092 | 0.0698
Epoch 22/300, seasonal_0 Loss: 0.1107 | 0.0711
Epoch 23/300, seasonal_0 Loss: 0.1046 | 0.0618
Epoch 24/300, seasonal_0 Loss: 0.1040 | 0.0626
Epoch 25/300, seasonal_0 Loss: 0.1038 | 0.0801
Epoch 26/300, seasonal_0 Loss: 0.1050 | 0.0795
Epoch 27/300, seasonal_0 Loss: 0.1037 | 0.0570
Epoch 28/300, seasonal_0 Loss: 0.1000 | 0.0608
Epoch 29/300, seasonal_0 Loss: 0.1067 | 0.0782
Epoch 30/300, seasonal_0 Loss: 0.1014 | 0.0550
Epoch 31/300, seasonal_0 Loss: 0.0971 | 0.0695
Epoch 32/300, seasonal_0 Loss: 0.0996 | 0.0815
Epoch 33/300, seasonal_0 Loss: 0.0995 | 0.0527
Epoch 34/300, seasonal_0 Loss: 0.0937 | 0.0528
Epoch 35/300, seasonal_0 Loss: 0.0953 | 0.0539
Epoch 36/300, seasonal_0 Loss: 0.0939 | 0.0497
Epoch 37/300, seasonal_0 Loss: 0.0924 | 0.0590
Epoch 38/300, seasonal_0 Loss: 0.0915 | 0.0704
Epoch 39/300, seasonal_0 Loss: 0.0914 | 0.0509
Epoch 40/300, seasonal_0 Loss: 0.0888 | 0.0468
Epoch 41/300, seasonal_0 Loss: 0.0896 | 0.0486
Epoch 42/300, seasonal_0 Loss: 0.0883 | 0.0460
Epoch 43/300, seasonal_0 Loss: 0.0870 | 0.0548
Epoch 44/300, seasonal_0 Loss: 0.0878 | 0.0647
Epoch 45/300, seasonal_0 Loss: 0.0905 | 0.0480
Epoch 46/300, seasonal_0 Loss: 0.0863 | 0.0445
Epoch 47/300, seasonal_0 Loss: 0.0854 | 0.0458
Epoch 48/300, seasonal_0 Loss: 0.0862 | 0.0479
Epoch 49/300, seasonal_0 Loss: 0.0855 | 0.0536
Epoch 50/300, seasonal_0 Loss: 0.0895 | 0.0454
Epoch 51/300, seasonal_0 Loss: 0.0869 | 0.0456
Epoch 52/300, seasonal_0 Loss: 0.0840 | 0.0450
Epoch 53/300, seasonal_0 Loss: 0.0859 | 0.0472
Epoch 54/300, seasonal_0 Loss: 0.0887 | 0.0497
Epoch 55/300, seasonal_0 Loss: 0.0880 | 0.0440
Epoch 56/300, seasonal_0 Loss: 0.0857 | 0.0436
Epoch 57/300, seasonal_0 Loss: 0.0878 | 0.0443
Epoch 58/300, seasonal_0 Loss: 0.0875 | 0.0493
Epoch 59/300, seasonal_0 Loss: 0.0838 | 0.0506
Epoch 60/300, seasonal_0 Loss: 0.0915 | 0.0559
Epoch 61/300, seasonal_0 Loss: 0.0879 | 0.0437
Epoch 62/300, seasonal_0 Loss: 0.0865 | 0.0467
Epoch 63/300, seasonal_0 Loss: 0.0815 | 0.0462
Epoch 64/300, seasonal_0 Loss: 0.0801 | 0.0410
Epoch 65/300, seasonal_0 Loss: 0.0797 | 0.0419
Epoch 66/300, seasonal_0 Loss: 0.0788 | 0.0409
Epoch 67/300, seasonal_0 Loss: 0.0791 | 0.0435
Epoch 68/300, seasonal_0 Loss: 0.0816 | 0.0461
Epoch 69/300, seasonal_0 Loss: 0.0804 | 0.0412
Epoch 70/300, seasonal_0 Loss: 0.0786 | 0.0423
Epoch 71/300, seasonal_0 Loss: 0.0772 | 0.0439
Epoch 72/300, seasonal_0 Loss: 0.0785 | 0.0470
Epoch 73/300, seasonal_0 Loss: 0.0786 | 0.0486
Epoch 74/300, seasonal_0 Loss: 0.0790 | 0.0435
Epoch 75/300, seasonal_0 Loss: 0.0774 | 0.0427
Epoch 76/300, seasonal_0 Loss: 0.0790 | 0.0407
Epoch 77/300, seasonal_0 Loss: 0.0828 | 0.0439
Epoch 78/300, seasonal_0 Loss: 0.0903 | 0.0447
Epoch 79/300, seasonal_0 Loss: 0.0866 | 0.0492
Epoch 80/300, seasonal_0 Loss: 0.0817 | 0.0467
Epoch 81/300, seasonal_0 Loss: 0.0787 | 0.0430
Epoch 82/300, seasonal_0 Loss: 0.0780 | 0.0413
Epoch 83/300, seasonal_0 Loss: 0.0760 | 0.0403
Epoch 84/300, seasonal_0 Loss: 0.0743 | 0.0403
Epoch 85/300, seasonal_0 Loss: 0.0753 | 0.0425
Epoch 86/300, seasonal_0 Loss: 0.0744 | 0.0460
Epoch 87/300, seasonal_0 Loss: 0.0751 | 0.0390
Epoch 88/300, seasonal_0 Loss: 0.0733 | 0.0378
Epoch 89/300, seasonal_0 Loss: 0.0730 | 0.0387
Epoch 90/300, seasonal_0 Loss: 0.0736 | 0.0369
Epoch 91/300, seasonal_0 Loss: 0.0733 | 0.0360
Epoch 92/300, seasonal_0 Loss: 0.0721 | 0.0386
Epoch 93/300, seasonal_0 Loss: 0.0741 | 0.0386
Epoch 94/300, seasonal_0 Loss: 0.0732 | 0.0380
Epoch 95/300, seasonal_0 Loss: 0.0725 | 0.0426
Epoch 96/300, seasonal_0 Loss: 0.0748 | 0.0457
Epoch 97/300, seasonal_0 Loss: 0.0768 | 0.0436
Epoch 98/300, seasonal_0 Loss: 0.0746 | 0.0374
Epoch 99/300, seasonal_0 Loss: 0.0804 | 0.0370
Epoch 100/300, seasonal_0 Loss: 0.0733 | 0.0386
Epoch 101/300, seasonal_0 Loss: 0.0750 | 0.0365
Epoch 102/300, seasonal_0 Loss: 0.0728 | 0.0370
Epoch 103/300, seasonal_0 Loss: 0.0715 | 0.0369
Epoch 104/300, seasonal_0 Loss: 0.0724 | 0.0441
Epoch 105/300, seasonal_0 Loss: 0.0725 | 0.0376
Epoch 106/300, seasonal_0 Loss: 0.0716 | 0.0344
Epoch 107/300, seasonal_0 Loss: 0.0720 | 0.0353
Epoch 108/300, seasonal_0 Loss: 0.0708 | 0.0358
Epoch 109/300, seasonal_0 Loss: 0.0701 | 0.0350
Epoch 110/300, seasonal_0 Loss: 0.0687 | 0.0351
Epoch 111/300, seasonal_0 Loss: 0.0686 | 0.0345
Epoch 112/300, seasonal_0 Loss: 0.0681 | 0.0351
Epoch 113/300, seasonal_0 Loss: 0.0699 | 0.0342
Epoch 114/300, seasonal_0 Loss: 0.0679 | 0.0355
Epoch 115/300, seasonal_0 Loss: 0.0681 | 0.0332
Epoch 116/300, seasonal_0 Loss: 0.0668 | 0.0336
Epoch 117/300, seasonal_0 Loss: 0.0667 | 0.0334
Epoch 118/300, seasonal_0 Loss: 0.0664 | 0.0358
Epoch 119/300, seasonal_0 Loss: 0.0665 | 0.0350
Epoch 120/300, seasonal_0 Loss: 0.0672 | 0.0348
Epoch 121/300, seasonal_0 Loss: 0.0675 | 0.0354
Epoch 122/300, seasonal_0 Loss: 0.0685 | 0.0358
Epoch 123/300, seasonal_0 Loss: 0.0680 | 0.0344
Epoch 124/300, seasonal_0 Loss: 0.0661 | 0.0336
Epoch 125/300, seasonal_0 Loss: 0.0664 | 0.0337
Epoch 126/300, seasonal_0 Loss: 0.0656 | 0.0351
Epoch 127/300, seasonal_0 Loss: 0.0653 | 0.0341
Epoch 128/300, seasonal_0 Loss: 0.0648 | 0.0338
Epoch 129/300, seasonal_0 Loss: 0.0649 | 0.0340
Epoch 130/300, seasonal_0 Loss: 0.0647 | 0.0345
Epoch 131/300, seasonal_0 Loss: 0.0644 | 0.0338
Epoch 132/300, seasonal_0 Loss: 0.0639 | 0.0349
Epoch 133/300, seasonal_0 Loss: 0.0638 | 0.0352
Epoch 134/300, seasonal_0 Loss: 0.0653 | 0.0375
Epoch 135/300, seasonal_0 Loss: 0.0663 | 0.0379
Epoch 136/300, seasonal_0 Loss: 0.0678 | 0.0375
Epoch 137/300, seasonal_0 Loss: 0.0694 | 0.0354
Epoch 138/300, seasonal_0 Loss: 0.0663 | 0.0338
Epoch 139/300, seasonal_0 Loss: 0.0671 | 0.0337
Epoch 140/300, seasonal_0 Loss: 0.0652 | 0.0337
Epoch 141/300, seasonal_0 Loss: 0.0656 | 0.0340
Epoch 142/300, seasonal_0 Loss: 0.0654 | 0.0342
Epoch 143/300, seasonal_0 Loss: 0.0643 | 0.0333
Epoch 144/300, seasonal_0 Loss: 0.0633 | 0.0341
Epoch 145/300, seasonal_0 Loss: 0.0635 | 0.0349
Epoch 146/300, seasonal_0 Loss: 0.0647 | 0.0385
Epoch 147/300, seasonal_0 Loss: 0.0661 | 0.0396
Epoch 148/300, seasonal_0 Loss: 0.0670 | 0.0398
Epoch 149/300, seasonal_0 Loss: 0.0636 | 0.0362
Epoch 150/300, seasonal_0 Loss: 0.0636 | 0.0338
Epoch 151/300, seasonal_0 Loss: 0.0643 | 0.0335
Epoch 152/300, seasonal_0 Loss: 0.0659 | 0.0371
Epoch 153/300, seasonal_0 Loss: 0.0676 | 0.0385
Epoch 154/300, seasonal_0 Loss: 0.0653 | 0.0343
Epoch 155/300, seasonal_0 Loss: 0.0638 | 0.0411
Epoch 156/300, seasonal_0 Loss: 0.0673 | 0.0433
Epoch 157/300, seasonal_0 Loss: 0.0690 | 0.0361
Epoch 158/300, seasonal_0 Loss: 0.0635 | 0.0345
Epoch 159/300, seasonal_0 Loss: 0.0639 | 0.0338
Epoch 160/300, seasonal_0 Loss: 0.0669 | 0.0340
Epoch 161/300, seasonal_0 Loss: 0.0630 | 0.0357
Epoch 162/300, seasonal_0 Loss: 0.0641 | 0.0355
Epoch 163/300, seasonal_0 Loss: 0.0636 | 0.0338
Epoch 164/300, seasonal_0 Loss: 0.0634 | 0.0345
Epoch 165/300, seasonal_0 Loss: 0.0615 | 0.0361
Epoch 166/300, seasonal_0 Loss: 0.0615 | 0.0328
Epoch 167/300, seasonal_0 Loss: 0.0608 | 0.0330
Epoch 168/300, seasonal_0 Loss: 0.0613 | 0.0335
Epoch 169/300, seasonal_0 Loss: 0.0609 | 0.0365
Epoch 170/300, seasonal_0 Loss: 0.0605 | 0.0339
Epoch 171/300, seasonal_0 Loss: 0.0602 | 0.0335
Epoch 172/300, seasonal_0 Loss: 0.0598 | 0.0340
Epoch 173/300, seasonal_0 Loss: 0.0596 | 0.0338
Epoch 174/300, seasonal_0 Loss: 0.0599 | 0.0337
Epoch 175/300, seasonal_0 Loss: 0.0596 | 0.0340
Epoch 176/300, seasonal_0 Loss: 0.0595 | 0.0331
Epoch 177/300, seasonal_0 Loss: 0.0593 | 0.0334
Epoch 178/300, seasonal_0 Loss: 0.0595 | 0.0336
Epoch 179/300, seasonal_0 Loss: 0.0587 | 0.0347
Epoch 180/300, seasonal_0 Loss: 0.0585 | 0.0334
Epoch 181/300, seasonal_0 Loss: 0.0582 | 0.0333
Epoch 182/300, seasonal_0 Loss: 0.0580 | 0.0340
Epoch 183/300, seasonal_0 Loss: 0.0583 | 0.0355
Epoch 184/300, seasonal_0 Loss: 0.0587 | 0.0344
Epoch 185/300, seasonal_0 Loss: 0.0588 | 0.0329
Epoch 186/300, seasonal_0 Loss: 0.0589 | 0.0336
Epoch 187/300, seasonal_0 Loss: 0.0588 | 0.0347
Epoch 188/300, seasonal_0 Loss: 0.0585 | 0.0335
Epoch 189/300, seasonal_0 Loss: 0.0576 | 0.0338
Epoch 190/300, seasonal_0 Loss: 0.0574 | 0.0338
Epoch 191/300, seasonal_0 Loss: 0.0570 | 0.0338
Epoch 192/300, seasonal_0 Loss: 0.0570 | 0.0346
Epoch 193/300, seasonal_0 Loss: 0.0577 | 0.0359
Epoch 194/300, seasonal_0 Loss: 0.0579 | 0.0336
Epoch 195/300, seasonal_0 Loss: 0.0576 | 0.0332
Epoch 196/300, seasonal_0 Loss: 0.0590 | 0.0336
Epoch 197/300, seasonal_0 Loss: 0.0584 | 0.0357
Epoch 198/300, seasonal_0 Loss: 0.0576 | 0.0339
Epoch 199/300, seasonal_0 Loss: 0.0570 | 0.0342
Epoch 200/300, seasonal_0 Loss: 0.0570 | 0.0362
Epoch 201/300, seasonal_0 Loss: 0.0572 | 0.0357
Epoch 202/300, seasonal_0 Loss: 0.0581 | 0.0344
Epoch 203/300, seasonal_0 Loss: 0.0573 | 0.0337
Epoch 204/300, seasonal_0 Loss: 0.0584 | 0.0344
Epoch 205/300, seasonal_0 Loss: 0.0570 | 0.0389
Epoch 206/300, seasonal_0 Loss: 0.0568 | 0.0339
Epoch 207/300, seasonal_0 Loss: 0.0568 | 0.0349
Epoch 208/300, seasonal_0 Loss: 0.0563 | 0.0338
Epoch 209/300, seasonal_0 Loss: 0.0564 | 0.0358
Epoch 210/300, seasonal_0 Loss: 0.0566 | 0.0336
Epoch 211/300, seasonal_0 Loss: 0.0559 | 0.0332
Epoch 212/300, seasonal_0 Loss: 0.0553 | 0.0337
Epoch 213/300, seasonal_0 Loss: 0.0553 | 0.0345
Epoch 214/300, seasonal_0 Loss: 0.0553 | 0.0334
Epoch 215/300, seasonal_0 Loss: 0.0548 | 0.0336
Epoch 216/300, seasonal_0 Loss: 0.0548 | 0.0335
Epoch 217/300, seasonal_0 Loss: 0.0546 | 0.0337
Epoch 218/300, seasonal_0 Loss: 0.0555 | 0.0334
Epoch 219/300, seasonal_0 Loss: 0.0555 | 0.0352
Epoch 220/300, seasonal_0 Loss: 0.0546 | 0.0339
Epoch 221/300, seasonal_0 Loss: 0.0544 | 0.0335
Epoch 222/300, seasonal_0 Loss: 0.0542 | 0.0342
Epoch 223/300, seasonal_0 Loss: 0.0542 | 0.0345
Epoch 224/300, seasonal_0 Loss: 0.0541 | 0.0337
Epoch 225/300, seasonal_0 Loss: 0.0541 | 0.0336
Epoch 226/300, seasonal_0 Loss: 0.0541 | 0.0331
Epoch 227/300, seasonal_0 Loss: 0.0546 | 0.0345
Epoch 228/300, seasonal_0 Loss: 0.0546 | 0.0336
Epoch 229/300, seasonal_0 Loss: 0.0539 | 0.0340
Epoch 230/300, seasonal_0 Loss: 0.0536 | 0.0351
Epoch 231/300, seasonal_0 Loss: 0.0541 | 0.0347
Epoch 232/300, seasonal_0 Loss: 0.0541 | 0.0353
Epoch 233/300, seasonal_0 Loss: 0.0542 | 0.0347
Epoch 234/300, seasonal_0 Loss: 0.0545 | 0.0342
Epoch 235/300, seasonal_0 Loss: 0.0554 | 0.0352
Epoch 236/300, seasonal_0 Loss: 0.0561 | 0.0325
Epoch 237/300, seasonal_0 Loss: 0.0537 | 0.0352
Epoch 238/300, seasonal_0 Loss: 0.0536 | 0.0340
Epoch 239/300, seasonal_0 Loss: 0.0531 | 0.0345
Epoch 240/300, seasonal_0 Loss: 0.0526 | 0.0342
Epoch 241/300, seasonal_0 Loss: 0.0531 | 0.0355
Epoch 242/300, seasonal_0 Loss: 0.0531 | 0.0344
Epoch 243/300, seasonal_0 Loss: 0.0535 | 0.0339
Epoch 244/300, seasonal_0 Loss: 0.0526 | 0.0361
Epoch 245/300, seasonal_0 Loss: 0.0520 | 0.0346
Epoch 246/300, seasonal_0 Loss: 0.0525 | 0.0352
Epoch 247/300, seasonal_0 Loss: 0.0523 | 0.0356
Epoch 248/300, seasonal_0 Loss: 0.0531 | 0.0344
Epoch 249/300, seasonal_0 Loss: 0.0527 | 0.0338
Epoch 250/300, seasonal_0 Loss: 0.0524 | 0.0339
Epoch 251/300, seasonal_0 Loss: 0.0523 | 0.0362
Epoch 252/300, seasonal_0 Loss: 0.0522 | 0.0356
Epoch 253/300, seasonal_0 Loss: 0.0526 | 0.0363
Epoch 254/300, seasonal_0 Loss: 0.0523 | 0.0358
Epoch 255/300, seasonal_0 Loss: 0.0522 | 0.0350
Epoch 256/300, seasonal_0 Loss: 0.0525 | 0.0340
Epoch 257/300, seasonal_0 Loss: 0.0527 | 0.0352
Epoch 258/300, seasonal_0 Loss: 0.0522 | 0.0340
Epoch 259/300, seasonal_0 Loss: 0.0515 | 0.0356
Epoch 260/300, seasonal_0 Loss: 0.0511 | 0.0358
Epoch 261/300, seasonal_0 Loss: 0.0508 | 0.0341
Epoch 262/300, seasonal_0 Loss: 0.0505 | 0.0344
Epoch 263/300, seasonal_0 Loss: 0.0506 | 0.0354
Epoch 264/300, seasonal_0 Loss: 0.0507 | 0.0353
Epoch 265/300, seasonal_0 Loss: 0.0502 | 0.0346
Epoch 266/300, seasonal_0 Loss: 0.0502 | 0.0345
Epoch 267/300, seasonal_0 Loss: 0.0499 | 0.0346
Epoch 268/300, seasonal_0 Loss: 0.0500 | 0.0345
Epoch 269/300, seasonal_0 Loss: 0.0505 | 0.0353
Epoch 270/300, seasonal_0 Loss: 0.0514 | 0.0351
Epoch 271/300, seasonal_0 Loss: 0.0506 | 0.0358
Epoch 272/300, seasonal_0 Loss: 0.0503 | 0.0364
Epoch 273/300, seasonal_0 Loss: 0.0506 | 0.0371
Epoch 274/300, seasonal_0 Loss: 0.0511 | 0.0359
Epoch 275/300, seasonal_0 Loss: 0.0516 | 0.0358
Epoch 276/300, seasonal_0 Loss: 0.0503 | 0.0348
Epoch 277/300, seasonal_0 Loss: 0.0496 | 0.0349
Epoch 278/300, seasonal_0 Loss: 0.0494 | 0.0349
Epoch 279/300, seasonal_0 Loss: 0.0493 | 0.0355
Epoch 280/300, seasonal_0 Loss: 0.0492 | 0.0359
Epoch 281/300, seasonal_0 Loss: 0.0490 | 0.0349
Epoch 282/300, seasonal_0 Loss: 0.0486 | 0.0353
Epoch 283/300, seasonal_0 Loss: 0.0487 | 0.0356
Epoch 284/300, seasonal_0 Loss: 0.0488 | 0.0355
Epoch 285/300, seasonal_0 Loss: 0.0485 | 0.0356
Epoch 286/300, seasonal_0 Loss: 0.0484 | 0.0352
Epoch 287/300, seasonal_0 Loss: 0.0483 | 0.0351
Epoch 288/300, seasonal_0 Loss: 0.0486 | 0.0355
Epoch 289/300, seasonal_0 Loss: 0.0496 | 0.0356
Epoch 290/300, seasonal_0 Loss: 0.0494 | 0.0354
Epoch 291/300, seasonal_0 Loss: 0.0484 | 0.0363
Epoch 292/300, seasonal_0 Loss: 0.0491 | 0.0372
Epoch 293/300, seasonal_0 Loss: 0.0494 | 0.0366
Epoch 294/300, seasonal_0 Loss: 0.0496 | 0.0353
Epoch 295/300, seasonal_0 Loss: 0.0490 | 0.0358
Epoch 296/300, seasonal_0 Loss: 0.0486 | 0.0354
Epoch 297/300, seasonal_0 Loss: 0.0484 | 0.0357
Epoch 298/300, seasonal_0 Loss: 0.0477 | 0.0365
Epoch 299/300, seasonal_0 Loss: 0.0479 | 0.0361
Epoch 300/300, seasonal_0 Loss: 0.0476 | 0.0359
Training seasonal_1 component with params: {'observation_period_num': 16, 'train_rates': 0.9546411665643705, 'learning_rate': 0.0007033946560515519, 'batch_size': 149, 'step_size': 14, 'gamma': 0.8530913197236093}
Epoch 1/300, seasonal_1 Loss: 0.6142 | 0.2713
Epoch 2/300, seasonal_1 Loss: 0.1869 | 0.1485
Epoch 3/300, seasonal_1 Loss: 0.1487 | 0.1009
Epoch 4/300, seasonal_1 Loss: 0.1317 | 0.0858
Epoch 5/300, seasonal_1 Loss: 0.1254 | 0.0797
Epoch 6/300, seasonal_1 Loss: 0.1146 | 0.0751
Epoch 7/300, seasonal_1 Loss: 0.1091 | 0.0745
Epoch 8/300, seasonal_1 Loss: 0.1063 | 0.0765
Epoch 9/300, seasonal_1 Loss: 0.1066 | 0.0872
Epoch 10/300, seasonal_1 Loss: 0.1084 | 0.1058
Epoch 11/300, seasonal_1 Loss: 0.1161 | 0.0862
Epoch 12/300, seasonal_1 Loss: 0.1210 | 0.0674
Epoch 13/300, seasonal_1 Loss: 0.1115 | 0.0700
Epoch 14/300, seasonal_1 Loss: 0.1087 | 0.0644
Epoch 15/300, seasonal_1 Loss: 0.1145 | 0.1198
Epoch 16/300, seasonal_1 Loss: 0.1174 | 0.0792
Epoch 17/300, seasonal_1 Loss: 0.1080 | 0.0591
Epoch 18/300, seasonal_1 Loss: 0.0977 | 0.0734
Epoch 19/300, seasonal_1 Loss: 0.1020 | 0.0683
Epoch 20/300, seasonal_1 Loss: 0.1048 | 0.0677
Epoch 21/300, seasonal_1 Loss: 0.1082 | 0.0681
Epoch 22/300, seasonal_1 Loss: 0.1209 | 0.0824
Epoch 23/300, seasonal_1 Loss: 0.1187 | 0.0772
Epoch 24/300, seasonal_1 Loss: 0.1250 | 0.0678
Epoch 25/300, seasonal_1 Loss: 0.1521 | 0.0929
Epoch 26/300, seasonal_1 Loss: 0.1707 | 0.1951
Epoch 27/300, seasonal_1 Loss: 0.1405 | 0.2489
Epoch 28/300, seasonal_1 Loss: 0.1206 | 0.0862
Epoch 29/300, seasonal_1 Loss: 0.1067 | 0.0672
Epoch 30/300, seasonal_1 Loss: 0.1037 | 0.0587
Epoch 31/300, seasonal_1 Loss: 0.0932 | 0.0534
Epoch 32/300, seasonal_1 Loss: 0.0933 | 0.0482
Epoch 33/300, seasonal_1 Loss: 0.0890 | 0.0623
Epoch 34/300, seasonal_1 Loss: 0.0879 | 0.0563
Epoch 35/300, seasonal_1 Loss: 0.0886 | 0.0491
Epoch 36/300, seasonal_1 Loss: 0.0865 | 0.0454
Epoch 37/300, seasonal_1 Loss: 0.0883 | 0.0559
Epoch 38/300, seasonal_1 Loss: 0.0935 | 0.0665
Epoch 39/300, seasonal_1 Loss: 0.0948 | 0.0530
Epoch 40/300, seasonal_1 Loss: 0.0898 | 0.0544
Epoch 41/300, seasonal_1 Loss: 0.0815 | 0.0571
Epoch 42/300, seasonal_1 Loss: 0.0802 | 0.0434
Epoch 43/300, seasonal_1 Loss: 0.0836 | 0.0454
Epoch 44/300, seasonal_1 Loss: 0.0812 | 0.0432
Epoch 45/300, seasonal_1 Loss: 0.0789 | 0.0466
Epoch 46/300, seasonal_1 Loss: 0.0754 | 0.0454
Epoch 47/300, seasonal_1 Loss: 0.0754 | 0.0413
Epoch 48/300, seasonal_1 Loss: 0.0741 | 0.0405
Epoch 49/300, seasonal_1 Loss: 0.0735 | 0.0405
Epoch 50/300, seasonal_1 Loss: 0.0739 | 0.0401
Epoch 51/300, seasonal_1 Loss: 0.0758 | 0.0426
Epoch 52/300, seasonal_1 Loss: 0.0779 | 0.0418
Epoch 53/300, seasonal_1 Loss: 0.0763 | 0.0399
Epoch 54/300, seasonal_1 Loss: 0.0717 | 0.0379
Epoch 55/300, seasonal_1 Loss: 0.0704 | 0.0360
Epoch 56/300, seasonal_1 Loss: 0.0701 | 0.0350
Epoch 57/300, seasonal_1 Loss: 0.0699 | 0.0347
Epoch 58/300, seasonal_1 Loss: 0.0696 | 0.0354
Epoch 59/300, seasonal_1 Loss: 0.0698 | 0.0370
Epoch 60/300, seasonal_1 Loss: 0.0702 | 0.0376
Epoch 61/300, seasonal_1 Loss: 0.0704 | 0.0368
Epoch 62/300, seasonal_1 Loss: 0.0709 | 0.0364
Epoch 63/300, seasonal_1 Loss: 0.0722 | 0.0368
Epoch 64/300, seasonal_1 Loss: 0.0741 | 0.0359
Epoch 65/300, seasonal_1 Loss: 0.0723 | 0.0348
Epoch 66/300, seasonal_1 Loss: 0.0709 | 0.0369
Epoch 67/300, seasonal_1 Loss: 0.0733 | 0.0421
Epoch 68/300, seasonal_1 Loss: 0.0728 | 0.0394
Epoch 69/300, seasonal_1 Loss: 0.0713 | 0.0375
Epoch 70/300, seasonal_1 Loss: 0.0685 | 0.0339
Epoch 71/300, seasonal_1 Loss: 0.0667 | 0.0324
Epoch 72/300, seasonal_1 Loss: 0.0660 | 0.0322
Epoch 73/300, seasonal_1 Loss: 0.0653 | 0.0328
Epoch 74/300, seasonal_1 Loss: 0.0649 | 0.0325
Epoch 75/300, seasonal_1 Loss: 0.0646 | 0.0317
Epoch 76/300, seasonal_1 Loss: 0.0644 | 0.0315
Epoch 77/300, seasonal_1 Loss: 0.0642 | 0.0315
Epoch 78/300, seasonal_1 Loss: 0.0640 | 0.0316
Epoch 79/300, seasonal_1 Loss: 0.0641 | 0.0319
Epoch 80/300, seasonal_1 Loss: 0.0641 | 0.0319
Epoch 81/300, seasonal_1 Loss: 0.0638 | 0.0316
Epoch 82/300, seasonal_1 Loss: 0.0635 | 0.0312
Epoch 83/300, seasonal_1 Loss: 0.0631 | 0.0307
Epoch 84/300, seasonal_1 Loss: 0.0627 | 0.0304
Epoch 85/300, seasonal_1 Loss: 0.0626 | 0.0305
Epoch 86/300, seasonal_1 Loss: 0.0629 | 0.0311
Epoch 87/300, seasonal_1 Loss: 0.0635 | 0.0314
Epoch 88/300, seasonal_1 Loss: 0.0637 | 0.0311
Epoch 89/300, seasonal_1 Loss: 0.0628 | 0.0308
Epoch 90/300, seasonal_1 Loss: 0.0621 | 0.0306
Epoch 91/300, seasonal_1 Loss: 0.0621 | 0.0304
Epoch 92/300, seasonal_1 Loss: 0.0621 | 0.0304
Epoch 93/300, seasonal_1 Loss: 0.0622 | 0.0303
Epoch 94/300, seasonal_1 Loss: 0.0620 | 0.0300
Epoch 95/300, seasonal_1 Loss: 0.0615 | 0.0297
Epoch 96/300, seasonal_1 Loss: 0.0612 | 0.0297
Epoch 97/300, seasonal_1 Loss: 0.0611 | 0.0298
Epoch 98/300, seasonal_1 Loss: 0.0612 | 0.0299
Epoch 99/300, seasonal_1 Loss: 0.0611 | 0.0297
Epoch 100/300, seasonal_1 Loss: 0.0609 | 0.0295
Epoch 101/300, seasonal_1 Loss: 0.0606 | 0.0293
Epoch 102/300, seasonal_1 Loss: 0.0605 | 0.0291
Epoch 103/300, seasonal_1 Loss: 0.0605 | 0.0291
Epoch 104/300, seasonal_1 Loss: 0.0606 | 0.0293
Epoch 105/300, seasonal_1 Loss: 0.0607 | 0.0296
Epoch 106/300, seasonal_1 Loss: 0.0607 | 0.0292
Epoch 107/300, seasonal_1 Loss: 0.0602 | 0.0290
Epoch 108/300, seasonal_1 Loss: 0.0601 | 0.0295
Epoch 109/300, seasonal_1 Loss: 0.0605 | 0.0296
Epoch 110/300, seasonal_1 Loss: 0.0603 | 0.0292
Epoch 111/300, seasonal_1 Loss: 0.0597 | 0.0288
Epoch 112/300, seasonal_1 Loss: 0.0595 | 0.0286
Epoch 113/300, seasonal_1 Loss: 0.0595 | 0.0284
Epoch 114/300, seasonal_1 Loss: 0.0595 | 0.0284
Epoch 115/300, seasonal_1 Loss: 0.0594 | 0.0284
Epoch 116/300, seasonal_1 Loss: 0.0590 | 0.0284
Epoch 117/300, seasonal_1 Loss: 0.0589 | 0.0285
Epoch 118/300, seasonal_1 Loss: 0.0589 | 0.0287
Epoch 119/300, seasonal_1 Loss: 0.0589 | 0.0288
Epoch 120/300, seasonal_1 Loss: 0.0588 | 0.0285
Epoch 121/300, seasonal_1 Loss: 0.0586 | 0.0281
Epoch 122/300, seasonal_1 Loss: 0.0586 | 0.0279
Epoch 123/300, seasonal_1 Loss: 0.0588 | 0.0280
Epoch 124/300, seasonal_1 Loss: 0.0589 | 0.0285
Epoch 125/300, seasonal_1 Loss: 0.0590 | 0.0282
Epoch 126/300, seasonal_1 Loss: 0.0583 | 0.0284
Epoch 127/300, seasonal_1 Loss: 0.0583 | 0.0288
Epoch 128/300, seasonal_1 Loss: 0.0584 | 0.0285
Epoch 129/300, seasonal_1 Loss: 0.0580 | 0.0280
Epoch 130/300, seasonal_1 Loss: 0.0578 | 0.0276
Epoch 131/300, seasonal_1 Loss: 0.0578 | 0.0275
Epoch 132/300, seasonal_1 Loss: 0.0578 | 0.0276
Epoch 133/300, seasonal_1 Loss: 0.0577 | 0.0277
Epoch 134/300, seasonal_1 Loss: 0.0575 | 0.0277
Epoch 135/300, seasonal_1 Loss: 0.0574 | 0.0281
Epoch 136/300, seasonal_1 Loss: 0.0575 | 0.0284
Epoch 137/300, seasonal_1 Loss: 0.0575 | 0.0283
Epoch 138/300, seasonal_1 Loss: 0.0573 | 0.0279
Epoch 139/300, seasonal_1 Loss: 0.0571 | 0.0276
Epoch 140/300, seasonal_1 Loss: 0.0571 | 0.0274
Epoch 141/300, seasonal_1 Loss: 0.0571 | 0.0273
Epoch 142/300, seasonal_1 Loss: 0.0572 | 0.0274
Epoch 143/300, seasonal_1 Loss: 0.0569 | 0.0276
Epoch 144/300, seasonal_1 Loss: 0.0568 | 0.0280
Epoch 145/300, seasonal_1 Loss: 0.0569 | 0.0281
Epoch 146/300, seasonal_1 Loss: 0.0568 | 0.0278
Epoch 147/300, seasonal_1 Loss: 0.0566 | 0.0275
Epoch 148/300, seasonal_1 Loss: 0.0565 | 0.0272
Epoch 149/300, seasonal_1 Loss: 0.0566 | 0.0271
Epoch 150/300, seasonal_1 Loss: 0.0566 | 0.0272
Epoch 151/300, seasonal_1 Loss: 0.0565 | 0.0273
Epoch 152/300, seasonal_1 Loss: 0.0563 | 0.0277
Epoch 153/300, seasonal_1 Loss: 0.0563 | 0.0279
Epoch 154/300, seasonal_1 Loss: 0.0563 | 0.0277
Epoch 155/300, seasonal_1 Loss: 0.0562 | 0.0273
Epoch 156/300, seasonal_1 Loss: 0.0561 | 0.0271
Epoch 157/300, seasonal_1 Loss: 0.0561 | 0.0271
Epoch 158/300, seasonal_1 Loss: 0.0562 | 0.0272
Epoch 159/300, seasonal_1 Loss: 0.0560 | 0.0273
Epoch 160/300, seasonal_1 Loss: 0.0559 | 0.0276
Epoch 161/300, seasonal_1 Loss: 0.0560 | 0.0277
Epoch 162/300, seasonal_1 Loss: 0.0559 | 0.0274
Epoch 163/300, seasonal_1 Loss: 0.0558 | 0.0271
Epoch 164/300, seasonal_1 Loss: 0.0558 | 0.0271
Epoch 165/300, seasonal_1 Loss: 0.0558 | 0.0271
Epoch 166/300, seasonal_1 Loss: 0.0557 | 0.0273
Epoch 167/300, seasonal_1 Loss: 0.0557 | 0.0275
Epoch 168/300, seasonal_1 Loss: 0.0557 | 0.0275
Epoch 169/300, seasonal_1 Loss: 0.0556 | 0.0272
Epoch 170/300, seasonal_1 Loss: 0.0555 | 0.0271
Epoch 171/300, seasonal_1 Loss: 0.0555 | 0.0271
Epoch 172/300, seasonal_1 Loss: 0.0555 | 0.0272
Epoch 173/300, seasonal_1 Loss: 0.0554 | 0.0273
Epoch 174/300, seasonal_1 Loss: 0.0554 | 0.0274
Epoch 175/300, seasonal_1 Loss: 0.0554 | 0.0273
Epoch 176/300, seasonal_1 Loss: 0.0553 | 0.0272
Epoch 177/300, seasonal_1 Loss: 0.0553 | 0.0271
Epoch 178/300, seasonal_1 Loss: 0.0553 | 0.0272
Epoch 179/300, seasonal_1 Loss: 0.0553 | 0.0272
Epoch 180/300, seasonal_1 Loss: 0.0552 | 0.0273
Epoch 181/300, seasonal_1 Loss: 0.0552 | 0.0272
Epoch 182/300, seasonal_1 Loss: 0.0552 | 0.0272
Epoch 183/300, seasonal_1 Loss: 0.0551 | 0.0271
Epoch 184/300, seasonal_1 Loss: 0.0551 | 0.0272
Epoch 185/300, seasonal_1 Loss: 0.0551 | 0.0272
Epoch 186/300, seasonal_1 Loss: 0.0551 | 0.0272
Epoch 187/300, seasonal_1 Loss: 0.0550 | 0.0272
Epoch 188/300, seasonal_1 Loss: 0.0550 | 0.0272
Epoch 189/300, seasonal_1 Loss: 0.0550 | 0.0272
Epoch 190/300, seasonal_1 Loss: 0.0550 | 0.0272
Epoch 191/300, seasonal_1 Loss: 0.0550 | 0.0272
Epoch 192/300, seasonal_1 Loss: 0.0549 | 0.0272
Epoch 193/300, seasonal_1 Loss: 0.0549 | 0.0272
Epoch 194/300, seasonal_1 Loss: 0.0549 | 0.0272
Epoch 195/300, seasonal_1 Loss: 0.0549 | 0.0272
Epoch 196/300, seasonal_1 Loss: 0.0549 | 0.0272
Epoch 197/300, seasonal_1 Loss: 0.0548 | 0.0272
Epoch 198/300, seasonal_1 Loss: 0.0548 | 0.0272
Epoch 199/300, seasonal_1 Loss: 0.0548 | 0.0272
Epoch 200/300, seasonal_1 Loss: 0.0548 | 0.0272
Epoch 201/300, seasonal_1 Loss: 0.0548 | 0.0272
Epoch 202/300, seasonal_1 Loss: 0.0548 | 0.0272
Epoch 203/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 204/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 205/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 206/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 207/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 208/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 209/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 210/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 211/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 212/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 213/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 214/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 215/300, seasonal_1 Loss: 0.0546 | 0.0272
Epoch 216/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 217/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 218/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 219/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 220/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 221/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 222/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 223/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 224/300, seasonal_1 Loss: 0.0545 | 0.0272
Epoch 225/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 226/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 227/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 228/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 229/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 230/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 231/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 232/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 233/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 234/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 235/300, seasonal_1 Loss: 0.0544 | 0.0272
Epoch 236/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 237/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 238/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 239/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 240/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 241/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 242/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 243/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 244/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 245/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 246/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 247/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 248/300, seasonal_1 Loss: 0.0543 | 0.0272
Epoch 249/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 250/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 251/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 252/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 253/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 254/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 255/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 256/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 257/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 258/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 259/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 260/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 261/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 262/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 263/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 264/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 265/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 266/300, seasonal_1 Loss: 0.0542 | 0.0272
Epoch 267/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 268/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 269/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 270/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 271/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 272/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 273/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 274/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 275/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 276/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 277/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 278/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 279/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 280/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 281/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 282/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 283/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 284/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 285/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 286/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 287/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 288/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 289/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 290/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 291/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 292/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 293/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 294/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 295/300, seasonal_1 Loss: 0.0541 | 0.0272
Epoch 296/300, seasonal_1 Loss: 0.0540 | 0.0272
Epoch 297/300, seasonal_1 Loss: 0.0540 | 0.0272
Epoch 298/300, seasonal_1 Loss: 0.0540 | 0.0272
Epoch 299/300, seasonal_1 Loss: 0.0540 | 0.0272
Epoch 300/300, seasonal_1 Loss: 0.0540 | 0.0272
Training seasonal_2 component with params: {'observation_period_num': 9, 'train_rates': 0.9356594648373686, 'learning_rate': 0.00036986417088933135, 'batch_size': 168, 'step_size': 5, 'gamma': 0.9019083387436764}
Epoch 1/300, seasonal_2 Loss: 0.8513 | 0.2460
Epoch 2/300, seasonal_2 Loss: 0.2452 | 0.2445
Epoch 3/300, seasonal_2 Loss: 0.1861 | 0.2047
Epoch 4/300, seasonal_2 Loss: 0.1673 | 0.1247
Epoch 5/300, seasonal_2 Loss: 0.1468 | 0.1032
Epoch 6/300, seasonal_2 Loss: 0.1428 | 0.0942
Epoch 7/300, seasonal_2 Loss: 0.1361 | 0.0847
Epoch 8/300, seasonal_2 Loss: 0.1266 | 0.0779
Epoch 9/300, seasonal_2 Loss: 0.1203 | 0.0732
Epoch 10/300, seasonal_2 Loss: 0.1169 | 0.0696
Epoch 11/300, seasonal_2 Loss: 0.1165 | 0.0682
Epoch 12/300, seasonal_2 Loss: 0.1198 | 0.0696
Epoch 13/300, seasonal_2 Loss: 0.1248 | 0.0790
Epoch 14/300, seasonal_2 Loss: 0.1253 | 0.0899
Epoch 15/300, seasonal_2 Loss: 0.1179 | 0.0699
Epoch 16/300, seasonal_2 Loss: 0.1151 | 0.0720
Epoch 17/300, seasonal_2 Loss: 0.1238 | 0.1214
Epoch 18/300, seasonal_2 Loss: 0.1190 | 0.0988
Epoch 19/300, seasonal_2 Loss: 0.1113 | 0.0625
Epoch 20/300, seasonal_2 Loss: 0.1047 | 0.0637
Epoch 21/300, seasonal_2 Loss: 0.1104 | 0.0687
Epoch 22/300, seasonal_2 Loss: 0.1167 | 0.0649
Epoch 23/300, seasonal_2 Loss: 0.1064 | 0.0588
Epoch 24/300, seasonal_2 Loss: 0.1002 | 0.0591
Epoch 25/300, seasonal_2 Loss: 0.1026 | 0.0589
Epoch 26/300, seasonal_2 Loss: 0.0991 | 0.0591
Epoch 27/300, seasonal_2 Loss: 0.0954 | 0.0565
Epoch 28/300, seasonal_2 Loss: 0.0951 | 0.0559
Epoch 29/300, seasonal_2 Loss: 0.0937 | 0.0558
Epoch 30/300, seasonal_2 Loss: 0.0930 | 0.0547
Epoch 31/300, seasonal_2 Loss: 0.0923 | 0.0542
Epoch 32/300, seasonal_2 Loss: 0.0914 | 0.0536
Epoch 33/300, seasonal_2 Loss: 0.0908 | 0.0531
Epoch 34/300, seasonal_2 Loss: 0.0902 | 0.0526
Epoch 35/300, seasonal_2 Loss: 0.0896 | 0.0522
Epoch 36/300, seasonal_2 Loss: 0.0891 | 0.0518
Epoch 37/300, seasonal_2 Loss: 0.0886 | 0.0514
Epoch 38/300, seasonal_2 Loss: 0.0882 | 0.0510
Epoch 39/300, seasonal_2 Loss: 0.0878 | 0.0506
Epoch 40/300, seasonal_2 Loss: 0.0874 | 0.0503
Epoch 41/300, seasonal_2 Loss: 0.0870 | 0.0500
Epoch 42/300, seasonal_2 Loss: 0.0866 | 0.0497
Epoch 43/300, seasonal_2 Loss: 0.0862 | 0.0494
Epoch 44/300, seasonal_2 Loss: 0.0859 | 0.0491
Epoch 45/300, seasonal_2 Loss: 0.0855 | 0.0488
Epoch 46/300, seasonal_2 Loss: 0.0852 | 0.0485
Epoch 47/300, seasonal_2 Loss: 0.0849 | 0.0482
Epoch 48/300, seasonal_2 Loss: 0.0846 | 0.0479
Epoch 49/300, seasonal_2 Loss: 0.0842 | 0.0476
Epoch 50/300, seasonal_2 Loss: 0.0840 | 0.0473
Epoch 51/300, seasonal_2 Loss: 0.0837 | 0.0471
Epoch 52/300, seasonal_2 Loss: 0.0834 | 0.0468
Epoch 53/300, seasonal_2 Loss: 0.0831 | 0.0465
Epoch 54/300, seasonal_2 Loss: 0.0829 | 0.0463
Epoch 55/300, seasonal_2 Loss: 0.0826 | 0.0460
Epoch 56/300, seasonal_2 Loss: 0.0824 | 0.0458
Epoch 57/300, seasonal_2 Loss: 0.0822 | 0.0456
Epoch 58/300, seasonal_2 Loss: 0.0820 | 0.0454
Epoch 59/300, seasonal_2 Loss: 0.0818 | 0.0452
Epoch 60/300, seasonal_2 Loss: 0.0816 | 0.0450
Epoch 61/300, seasonal_2 Loss: 0.0814 | 0.0449
Epoch 62/300, seasonal_2 Loss: 0.0813 | 0.0447
Epoch 63/300, seasonal_2 Loss: 0.0812 | 0.0446
Epoch 64/300, seasonal_2 Loss: 0.0810 | 0.0445
Epoch 65/300, seasonal_2 Loss: 0.0810 | 0.0445
Epoch 66/300, seasonal_2 Loss: 0.0808 | 0.0444
Epoch 67/300, seasonal_2 Loss: 0.0807 | 0.0442
Epoch 68/300, seasonal_2 Loss: 0.0806 | 0.0440
Epoch 69/300, seasonal_2 Loss: 0.0804 | 0.0437
Epoch 70/300, seasonal_2 Loss: 0.0803 | 0.0435
Epoch 71/300, seasonal_2 Loss: 0.0802 | 0.0434
Epoch 72/300, seasonal_2 Loss: 0.0801 | 0.0433
Epoch 73/300, seasonal_2 Loss: 0.0800 | 0.0432
Epoch 74/300, seasonal_2 Loss: 0.0799 | 0.0431
Epoch 75/300, seasonal_2 Loss: 0.0798 | 0.0430
Epoch 76/300, seasonal_2 Loss: 0.0797 | 0.0429
Epoch 77/300, seasonal_2 Loss: 0.0797 | 0.0428
Epoch 78/300, seasonal_2 Loss: 0.0796 | 0.0428
Epoch 79/300, seasonal_2 Loss: 0.0795 | 0.0427
Epoch 80/300, seasonal_2 Loss: 0.0794 | 0.0426
Epoch 81/300, seasonal_2 Loss: 0.0794 | 0.0426
Epoch 82/300, seasonal_2 Loss: 0.0793 | 0.0425
Epoch 83/300, seasonal_2 Loss: 0.0793 | 0.0424
Epoch 84/300, seasonal_2 Loss: 0.0792 | 0.0424
Epoch 85/300, seasonal_2 Loss: 0.0791 | 0.0423
Epoch 86/300, seasonal_2 Loss: 0.0791 | 0.0423
Epoch 87/300, seasonal_2 Loss: 0.0790 | 0.0422
Epoch 88/300, seasonal_2 Loss: 0.0790 | 0.0422
Epoch 89/300, seasonal_2 Loss: 0.0790 | 0.0421
Epoch 90/300, seasonal_2 Loss: 0.0789 | 0.0421
Epoch 91/300, seasonal_2 Loss: 0.0789 | 0.0420
Epoch 92/300, seasonal_2 Loss: 0.0788 | 0.0420
Epoch 93/300, seasonal_2 Loss: 0.0788 | 0.0420
Epoch 94/300, seasonal_2 Loss: 0.0787 | 0.0419
Epoch 95/300, seasonal_2 Loss: 0.0787 | 0.0419
Epoch 96/300, seasonal_2 Loss: 0.0787 | 0.0419
Epoch 97/300, seasonal_2 Loss: 0.0786 | 0.0418
Epoch 98/300, seasonal_2 Loss: 0.0786 | 0.0418
Epoch 99/300, seasonal_2 Loss: 0.0786 | 0.0418
Epoch 100/300, seasonal_2 Loss: 0.0786 | 0.0417
Epoch 101/300, seasonal_2 Loss: 0.0785 | 0.0417
Epoch 102/300, seasonal_2 Loss: 0.0785 | 0.0417
Epoch 103/300, seasonal_2 Loss: 0.0785 | 0.0416
Epoch 104/300, seasonal_2 Loss: 0.0784 | 0.0416
Epoch 105/300, seasonal_2 Loss: 0.0784 | 0.0416
Epoch 106/300, seasonal_2 Loss: 0.0784 | 0.0416
Epoch 107/300, seasonal_2 Loss: 0.0784 | 0.0415
Epoch 108/300, seasonal_2 Loss: 0.0783 | 0.0415
Epoch 109/300, seasonal_2 Loss: 0.0783 | 0.0415
Epoch 110/300, seasonal_2 Loss: 0.0783 | 0.0415
Epoch 111/300, seasonal_2 Loss: 0.0783 | 0.0415
Epoch 112/300, seasonal_2 Loss: 0.0783 | 0.0414
Epoch 113/300, seasonal_2 Loss: 0.0782 | 0.0414
Epoch 114/300, seasonal_2 Loss: 0.0782 | 0.0414
Epoch 115/300, seasonal_2 Loss: 0.0782 | 0.0414
Epoch 116/300, seasonal_2 Loss: 0.0782 | 0.0414
Epoch 117/300, seasonal_2 Loss: 0.0782 | 0.0414
Epoch 118/300, seasonal_2 Loss: 0.0782 | 0.0414
Epoch 119/300, seasonal_2 Loss: 0.0782 | 0.0413
Epoch 120/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 121/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 122/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 123/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 124/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 125/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 126/300, seasonal_2 Loss: 0.0781 | 0.0413
Epoch 127/300, seasonal_2 Loss: 0.0781 | 0.0412
Epoch 128/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 129/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 130/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 131/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 132/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 133/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 134/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 135/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 136/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 137/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 138/300, seasonal_2 Loss: 0.0780 | 0.0412
Epoch 139/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 140/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 141/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 142/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 143/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 144/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 145/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 146/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 147/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 148/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 149/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 150/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 151/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 152/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 153/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 154/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 155/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 156/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 157/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 158/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 159/300, seasonal_2 Loss: 0.0779 | 0.0411
Epoch 160/300, seasonal_2 Loss: 0.0778 | 0.0411
Epoch 161/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 162/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 163/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 164/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 165/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 166/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 167/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 168/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 169/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 170/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 171/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 172/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 173/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 174/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 175/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 176/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 177/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 178/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 179/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 180/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 181/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 182/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 183/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 184/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 185/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 186/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 187/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 188/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 189/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 190/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 191/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 192/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 193/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 194/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 195/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 196/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 197/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 198/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 199/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 200/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 201/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 202/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 203/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 204/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 205/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 206/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 207/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 208/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 209/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 210/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 211/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 212/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 213/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 214/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 215/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 216/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 217/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 218/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 219/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 220/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 221/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 222/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 223/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 224/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 225/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 226/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 227/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 228/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 229/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 230/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 231/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 232/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 233/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 234/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 235/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 236/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 237/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 238/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 239/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 240/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 241/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 242/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 243/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 244/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 245/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 246/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 247/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 248/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 249/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 250/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 251/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 252/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 253/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 254/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 255/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 256/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 257/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 258/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 259/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 260/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 261/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 262/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 263/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 264/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 265/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 266/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 267/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 268/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 269/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 270/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 271/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 272/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 273/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 274/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 275/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 276/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 277/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 278/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 279/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 280/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 281/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 282/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 283/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 284/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 285/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 286/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 287/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 288/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 289/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 290/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 291/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 292/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 293/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 294/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 295/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 296/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 297/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 298/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 299/300, seasonal_2 Loss: 0.0778 | 0.0410
Epoch 300/300, seasonal_2 Loss: 0.0778 | 0.0410
Training seasonal_3 component with params: {'observation_period_num': 5, 'train_rates': 0.9337847183065806, 'learning_rate': 0.0002621386931709666, 'batch_size': 68, 'step_size': 15, 'gamma': 0.8569374159978577}
Epoch 1/300, seasonal_3 Loss: 0.2304 | 0.1032
Epoch 2/300, seasonal_3 Loss: 0.1369 | 0.0793
Epoch 3/300, seasonal_3 Loss: 0.1318 | 0.0715
Epoch 4/300, seasonal_3 Loss: 0.1205 | 0.0706
Epoch 5/300, seasonal_3 Loss: 0.1106 | 0.0676
Epoch 6/300, seasonal_3 Loss: 0.1060 | 0.0626
Epoch 7/300, seasonal_3 Loss: 0.1028 | 0.0599
Epoch 8/300, seasonal_3 Loss: 0.1003 | 0.0572
Epoch 9/300, seasonal_3 Loss: 0.0988 | 0.0556
Epoch 10/300, seasonal_3 Loss: 0.0941 | 0.0521
Epoch 11/300, seasonal_3 Loss: 0.0943 | 0.0522
Epoch 12/300, seasonal_3 Loss: 0.0919 | 0.0500
Epoch 13/300, seasonal_3 Loss: 0.0916 | 0.0508
Epoch 14/300, seasonal_3 Loss: 0.0914 | 0.0516
Epoch 15/300, seasonal_3 Loss: 0.0921 | 0.0542
Epoch 16/300, seasonal_3 Loss: 0.0918 | 0.0536
Epoch 17/300, seasonal_3 Loss: 0.0920 | 0.0508
Epoch 18/300, seasonal_3 Loss: 0.0851 | 0.0448
Epoch 19/300, seasonal_3 Loss: 0.0829 | 0.0418
Epoch 20/300, seasonal_3 Loss: 0.0807 | 0.0400
Epoch 21/300, seasonal_3 Loss: 0.0802 | 0.0391
Epoch 22/300, seasonal_3 Loss: 0.0785 | 0.0393
Epoch 23/300, seasonal_3 Loss: 0.0775 | 0.0396
Epoch 24/300, seasonal_3 Loss: 0.0769 | 0.0420
Epoch 25/300, seasonal_3 Loss: 0.0779 | 0.0511
Epoch 26/300, seasonal_3 Loss: 0.0789 | 0.0512
Epoch 27/300, seasonal_3 Loss: 0.0799 | 0.0456
Epoch 28/300, seasonal_3 Loss: 0.0769 | 0.0387
Epoch 29/300, seasonal_3 Loss: 0.0755 | 0.0369
Epoch 30/300, seasonal_3 Loss: 0.0750 | 0.0358
Epoch 31/300, seasonal_3 Loss: 0.0742 | 0.0353
Epoch 32/300, seasonal_3 Loss: 0.0738 | 0.0350
Epoch 33/300, seasonal_3 Loss: 0.0729 | 0.0345
Epoch 34/300, seasonal_3 Loss: 0.0721 | 0.0340
Epoch 35/300, seasonal_3 Loss: 0.0713 | 0.0334
Epoch 36/300, seasonal_3 Loss: 0.0705 | 0.0328
Epoch 37/300, seasonal_3 Loss: 0.0699 | 0.0322
Epoch 38/300, seasonal_3 Loss: 0.0695 | 0.0316
Epoch 39/300, seasonal_3 Loss: 0.0691 | 0.0316
Epoch 40/300, seasonal_3 Loss: 0.0689 | 0.0314
Epoch 41/300, seasonal_3 Loss: 0.0685 | 0.0311
Epoch 42/300, seasonal_3 Loss: 0.0683 | 0.0307
Epoch 43/300, seasonal_3 Loss: 0.0680 | 0.0304
Epoch 44/300, seasonal_3 Loss: 0.0677 | 0.0301
Epoch 45/300, seasonal_3 Loss: 0.0673 | 0.0299
Epoch 46/300, seasonal_3 Loss: 0.0669 | 0.0298
Epoch 47/300, seasonal_3 Loss: 0.0668 | 0.0296
Epoch 48/300, seasonal_3 Loss: 0.0663 | 0.0294
Epoch 49/300, seasonal_3 Loss: 0.0660 | 0.0292
Epoch 50/300, seasonal_3 Loss: 0.0656 | 0.0289
Epoch 51/300, seasonal_3 Loss: 0.0652 | 0.0287
Epoch 52/300, seasonal_3 Loss: 0.0649 | 0.0283
Epoch 53/300, seasonal_3 Loss: 0.0645 | 0.0282
Epoch 54/300, seasonal_3 Loss: 0.0644 | 0.0282
Epoch 55/300, seasonal_3 Loss: 0.0643 | 0.0283
Epoch 56/300, seasonal_3 Loss: 0.0643 | 0.0286
Epoch 57/300, seasonal_3 Loss: 0.0643 | 0.0304
Epoch 58/300, seasonal_3 Loss: 0.0661 | 0.0308
Epoch 59/300, seasonal_3 Loss: 0.0644 | 0.0302
Epoch 60/300, seasonal_3 Loss: 0.0637 | 0.0293
Epoch 61/300, seasonal_3 Loss: 0.0636 | 0.0301
Epoch 62/300, seasonal_3 Loss: 0.0634 | 0.0293
Epoch 63/300, seasonal_3 Loss: 0.0626 | 0.0287
Epoch 64/300, seasonal_3 Loss: 0.0622 | 0.0281
Epoch 65/300, seasonal_3 Loss: 0.0618 | 0.0275
Epoch 66/300, seasonal_3 Loss: 0.0615 | 0.0271
Epoch 67/300, seasonal_3 Loss: 0.0612 | 0.0266
Epoch 68/300, seasonal_3 Loss: 0.0609 | 0.0268
Epoch 69/300, seasonal_3 Loss: 0.0605 | 0.0264
Epoch 70/300, seasonal_3 Loss: 0.0605 | 0.0268
Epoch 71/300, seasonal_3 Loss: 0.0601 | 0.0264
Epoch 72/300, seasonal_3 Loss: 0.0599 | 0.0269
Epoch 73/300, seasonal_3 Loss: 0.0595 | 0.0264
Epoch 74/300, seasonal_3 Loss: 0.0594 | 0.0269
Epoch 75/300, seasonal_3 Loss: 0.0591 | 0.0263
Epoch 76/300, seasonal_3 Loss: 0.0590 | 0.0263
Epoch 77/300, seasonal_3 Loss: 0.0587 | 0.0259
Epoch 78/300, seasonal_3 Loss: 0.0586 | 0.0260
Epoch 79/300, seasonal_3 Loss: 0.0584 | 0.0259
Epoch 80/300, seasonal_3 Loss: 0.0583 | 0.0260
Epoch 81/300, seasonal_3 Loss: 0.0581 | 0.0258
Epoch 82/300, seasonal_3 Loss: 0.0580 | 0.0259
Epoch 83/300, seasonal_3 Loss: 0.0579 | 0.0258
Epoch 84/300, seasonal_3 Loss: 0.0578 | 0.0256
Epoch 85/300, seasonal_3 Loss: 0.0577 | 0.0256
Epoch 86/300, seasonal_3 Loss: 0.0575 | 0.0255
Epoch 87/300, seasonal_3 Loss: 0.0574 | 0.0256
Epoch 88/300, seasonal_3 Loss: 0.0573 | 0.0255
Epoch 89/300, seasonal_3 Loss: 0.0572 | 0.0256
Epoch 90/300, seasonal_3 Loss: 0.0571 | 0.0255
Epoch 91/300, seasonal_3 Loss: 0.0570 | 0.0257
Epoch 92/300, seasonal_3 Loss: 0.0569 | 0.0257
Epoch 93/300, seasonal_3 Loss: 0.0568 | 0.0257
Epoch 94/300, seasonal_3 Loss: 0.0568 | 0.0257
Epoch 95/300, seasonal_3 Loss: 0.0568 | 0.0258
Epoch 96/300, seasonal_3 Loss: 0.0568 | 0.0260
Epoch 97/300, seasonal_3 Loss: 0.0569 | 0.0261
Epoch 98/300, seasonal_3 Loss: 0.0568 | 0.0263
Epoch 99/300, seasonal_3 Loss: 0.0568 | 0.0262
Epoch 100/300, seasonal_3 Loss: 0.0569 | 0.0262
Epoch 101/300, seasonal_3 Loss: 0.0569 | 0.0261
Epoch 102/300, seasonal_3 Loss: 0.0570 | 0.0261
Epoch 103/300, seasonal_3 Loss: 0.0570 | 0.0260
Epoch 104/300, seasonal_3 Loss: 0.0571 | 0.0258
Epoch 105/300, seasonal_3 Loss: 0.0572 | 0.0257
Epoch 106/300, seasonal_3 Loss: 0.0574 | 0.0267
Epoch 107/300, seasonal_3 Loss: 0.0579 | 0.0268
Epoch 108/300, seasonal_3 Loss: 0.0575 | 0.0267
Epoch 109/300, seasonal_3 Loss: 0.0571 | 0.0265
Epoch 110/300, seasonal_3 Loss: 0.0568 | 0.0264
Epoch 111/300, seasonal_3 Loss: 0.0566 | 0.0264
Epoch 112/300, seasonal_3 Loss: 0.0564 | 0.0264
Epoch 113/300, seasonal_3 Loss: 0.0563 | 0.0263
Epoch 114/300, seasonal_3 Loss: 0.0561 | 0.0258
Epoch 115/300, seasonal_3 Loss: 0.0560 | 0.0258
Epoch 116/300, seasonal_3 Loss: 0.0559 | 0.0258
Epoch 117/300, seasonal_3 Loss: 0.0558 | 0.0258
Epoch 118/300, seasonal_3 Loss: 0.0557 | 0.0258
Epoch 119/300, seasonal_3 Loss: 0.0556 | 0.0258
Epoch 120/300, seasonal_3 Loss: 0.0556 | 0.0257
Epoch 121/300, seasonal_3 Loss: 0.0555 | 0.0254
Epoch 122/300, seasonal_3 Loss: 0.0555 | 0.0254
Epoch 123/300, seasonal_3 Loss: 0.0554 | 0.0254
Epoch 124/300, seasonal_3 Loss: 0.0554 | 0.0254
Epoch 125/300, seasonal_3 Loss: 0.0553 | 0.0254
Epoch 126/300, seasonal_3 Loss: 0.0553 | 0.0254
Epoch 127/300, seasonal_3 Loss: 0.0552 | 0.0254
Epoch 128/300, seasonal_3 Loss: 0.0552 | 0.0254
Epoch 129/300, seasonal_3 Loss: 0.0551 | 0.0253
Epoch 130/300, seasonal_3 Loss: 0.0551 | 0.0253
Epoch 131/300, seasonal_3 Loss: 0.0550 | 0.0254
Epoch 132/300, seasonal_3 Loss: 0.0550 | 0.0254
Epoch 133/300, seasonal_3 Loss: 0.0549 | 0.0254
Epoch 134/300, seasonal_3 Loss: 0.0548 | 0.0253
Epoch 135/300, seasonal_3 Loss: 0.0548 | 0.0253
Epoch 136/300, seasonal_3 Loss: 0.0548 | 0.0253
Epoch 137/300, seasonal_3 Loss: 0.0547 | 0.0253
Epoch 138/300, seasonal_3 Loss: 0.0547 | 0.0253
Epoch 139/300, seasonal_3 Loss: 0.0547 | 0.0252
Epoch 140/300, seasonal_3 Loss: 0.0547 | 0.0252
Epoch 141/300, seasonal_3 Loss: 0.0547 | 0.0252
Epoch 142/300, seasonal_3 Loss: 0.0546 | 0.0252
Epoch 143/300, seasonal_3 Loss: 0.0546 | 0.0252
Epoch 144/300, seasonal_3 Loss: 0.0546 | 0.0251
Epoch 145/300, seasonal_3 Loss: 0.0546 | 0.0251
Epoch 146/300, seasonal_3 Loss: 0.0546 | 0.0251
Epoch 147/300, seasonal_3 Loss: 0.0546 | 0.0251
Epoch 148/300, seasonal_3 Loss: 0.0545 | 0.0251
Epoch 149/300, seasonal_3 Loss: 0.0544 | 0.0251
Epoch 150/300, seasonal_3 Loss: 0.0544 | 0.0251
Epoch 151/300, seasonal_3 Loss: 0.0543 | 0.0251
Epoch 152/300, seasonal_3 Loss: 0.0543 | 0.0251
Epoch 153/300, seasonal_3 Loss: 0.0542 | 0.0251
Epoch 154/300, seasonal_3 Loss: 0.0541 | 0.0251
Epoch 155/300, seasonal_3 Loss: 0.0540 | 0.0250
Epoch 156/300, seasonal_3 Loss: 0.0539 | 0.0250
Epoch 157/300, seasonal_3 Loss: 0.0539 | 0.0250
Epoch 158/300, seasonal_3 Loss: 0.0538 | 0.0250
Epoch 159/300, seasonal_3 Loss: 0.0538 | 0.0249
Epoch 160/300, seasonal_3 Loss: 0.0538 | 0.0249
Epoch 161/300, seasonal_3 Loss: 0.0537 | 0.0248
Epoch 162/300, seasonal_3 Loss: 0.0537 | 0.0248
Epoch 163/300, seasonal_3 Loss: 0.0537 | 0.0248
Epoch 164/300, seasonal_3 Loss: 0.0537 | 0.0248
Epoch 165/300, seasonal_3 Loss: 0.0536 | 0.0248
Epoch 166/300, seasonal_3 Loss: 0.0536 | 0.0248
Epoch 167/300, seasonal_3 Loss: 0.0536 | 0.0248
Epoch 168/300, seasonal_3 Loss: 0.0536 | 0.0248
Epoch 169/300, seasonal_3 Loss: 0.0536 | 0.0248
Epoch 170/300, seasonal_3 Loss: 0.0536 | 0.0248
Epoch 171/300, seasonal_3 Loss: 0.0536 | 0.0249
Epoch 172/300, seasonal_3 Loss: 0.0536 | 0.0249
Epoch 173/300, seasonal_3 Loss: 0.0536 | 0.0249
Epoch 174/300, seasonal_3 Loss: 0.0536 | 0.0252
Epoch 175/300, seasonal_3 Loss: 0.0536 | 0.0253
Epoch 176/300, seasonal_3 Loss: 0.0536 | 0.0254
Epoch 177/300, seasonal_3 Loss: 0.0536 | 0.0255
Epoch 178/300, seasonal_3 Loss: 0.0536 | 0.0257
Epoch 179/300, seasonal_3 Loss: 0.0536 | 0.0258
Epoch 180/300, seasonal_3 Loss: 0.0536 | 0.0259
Epoch 181/300, seasonal_3 Loss: 0.0537 | 0.0258
Epoch 182/300, seasonal_3 Loss: 0.0538 | 0.0256
Epoch 183/300, seasonal_3 Loss: 0.0540 | 0.0254
Epoch 184/300, seasonal_3 Loss: 0.0542 | 0.0252
Epoch 185/300, seasonal_3 Loss: 0.0543 | 0.0253
Epoch 186/300, seasonal_3 Loss: 0.0543 | 0.0255
Epoch 187/300, seasonal_3 Loss: 0.0542 | 0.0257
Epoch 188/300, seasonal_3 Loss: 0.0541 | 0.0258
Epoch 189/300, seasonal_3 Loss: 0.0540 | 0.0260
Epoch 190/300, seasonal_3 Loss: 0.0538 | 0.0259
Epoch 191/300, seasonal_3 Loss: 0.0537 | 0.0258
Epoch 192/300, seasonal_3 Loss: 0.0536 | 0.0257
Epoch 193/300, seasonal_3 Loss: 0.0535 | 0.0256
Epoch 194/300, seasonal_3 Loss: 0.0535 | 0.0256
Epoch 195/300, seasonal_3 Loss: 0.0535 | 0.0255
Epoch 196/300, seasonal_3 Loss: 0.0536 | 0.0254
Epoch 197/300, seasonal_3 Loss: 0.0537 | 0.0254
Epoch 198/300, seasonal_3 Loss: 0.0537 | 0.0254
Epoch 199/300, seasonal_3 Loss: 0.0537 | 0.0254
Epoch 200/300, seasonal_3 Loss: 0.0536 | 0.0253
Epoch 201/300, seasonal_3 Loss: 0.0535 | 0.0252
Epoch 202/300, seasonal_3 Loss: 0.0534 | 0.0252
Epoch 203/300, seasonal_3 Loss: 0.0533 | 0.0251
Epoch 204/300, seasonal_3 Loss: 0.0533 | 0.0250
Epoch 205/300, seasonal_3 Loss: 0.0532 | 0.0249
Epoch 206/300, seasonal_3 Loss: 0.0531 | 0.0249
Epoch 207/300, seasonal_3 Loss: 0.0530 | 0.0248
Epoch 208/300, seasonal_3 Loss: 0.0529 | 0.0248
Epoch 209/300, seasonal_3 Loss: 0.0529 | 0.0248
Epoch 210/300, seasonal_3 Loss: 0.0528 | 0.0248
Epoch 211/300, seasonal_3 Loss: 0.0528 | 0.0248
Epoch 212/300, seasonal_3 Loss: 0.0528 | 0.0248
Epoch 213/300, seasonal_3 Loss: 0.0528 | 0.0248
Epoch 214/300, seasonal_3 Loss: 0.0527 | 0.0248
Epoch 215/300, seasonal_3 Loss: 0.0527 | 0.0248
Epoch 216/300, seasonal_3 Loss: 0.0527 | 0.0248
Epoch 217/300, seasonal_3 Loss: 0.0527 | 0.0248
Epoch 218/300, seasonal_3 Loss: 0.0527 | 0.0248
Epoch 219/300, seasonal_3 Loss: 0.0526 | 0.0248
Epoch 220/300, seasonal_3 Loss: 0.0526 | 0.0248
Epoch 221/300, seasonal_3 Loss: 0.0526 | 0.0248
Epoch 222/300, seasonal_3 Loss: 0.0526 | 0.0248
Epoch 223/300, seasonal_3 Loss: 0.0526 | 0.0248
Epoch 224/300, seasonal_3 Loss: 0.0526 | 0.0248
Epoch 225/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 226/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 227/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 228/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 229/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 230/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 231/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 232/300, seasonal_3 Loss: 0.0525 | 0.0248
Epoch 233/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 234/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 235/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 236/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 237/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 238/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 239/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 240/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 241/300, seasonal_3 Loss: 0.0524 | 0.0248
Epoch 242/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 243/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 244/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 245/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 246/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 247/300, seasonal_3 Loss: 0.0523 | 0.0248
Epoch 248/300, seasonal_3 Loss: 0.0523 | 0.0249
Epoch 249/300, seasonal_3 Loss: 0.0523 | 0.0249
Epoch 250/300, seasonal_3 Loss: 0.0523 | 0.0249
Epoch 251/300, seasonal_3 Loss: 0.0523 | 0.0249
Epoch 252/300, seasonal_3 Loss: 0.0523 | 0.0249
Epoch 253/300, seasonal_3 Loss: 0.0523 | 0.0249
Epoch 254/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 255/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 256/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 257/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 258/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 259/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 260/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 261/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 262/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 263/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 264/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 265/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 266/300, seasonal_3 Loss: 0.0522 | 0.0249
Epoch 267/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 268/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 269/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 270/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 271/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 272/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 273/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 274/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 275/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 276/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 277/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 278/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 279/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 280/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 281/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 282/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 283/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 284/300, seasonal_3 Loss: 0.0521 | 0.0249
Epoch 285/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 286/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 287/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 288/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 289/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 290/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 291/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 292/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 293/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 294/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 295/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 296/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 297/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 298/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 299/300, seasonal_3 Loss: 0.0520 | 0.0249
Epoch 300/300, seasonal_3 Loss: 0.0520 | 0.0249
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.8964358836548523, 'learning_rate': 0.0008067522483319194, 'batch_size': 130, 'step_size': 4, 'gamma': 0.8491674302715776}
Epoch 1/300, resid Loss: 0.4808 | 0.1810
Epoch 2/300, resid Loss: 0.1558 | 0.0886
Epoch 3/300, resid Loss: 0.1505 | 0.0860
Epoch 4/300, resid Loss: 0.1429 | 0.0926
Epoch 5/300, resid Loss: 0.1379 | 0.0703
Epoch 6/300, resid Loss: 0.1348 | 0.0743
Epoch 7/300, resid Loss: 0.1384 | 0.0814
Epoch 8/300, resid Loss: 0.1527 | 0.0970
Epoch 9/300, resid Loss: 0.1541 | 0.1423
Epoch 10/300, resid Loss: 0.1419 | 0.0963
Epoch 11/300, resid Loss: 0.1396 | 0.0861
Epoch 12/300, resid Loss: 0.1648 | 0.0995
Epoch 13/300, resid Loss: 0.1245 | 0.0750
Epoch 14/300, resid Loss: 0.1148 | 0.0583
Epoch 15/300, resid Loss: 0.1138 | 0.0624
Epoch 16/300, resid Loss: 0.1233 | 0.0822
Epoch 17/300, resid Loss: 0.1457 | 0.0759
Epoch 18/300, resid Loss: 0.1275 | 0.0555
Epoch 19/300, resid Loss: 0.1038 | 0.0558
Epoch 20/300, resid Loss: 0.1038 | 0.0532
Epoch 21/300, resid Loss: 0.0959 | 0.0510
Epoch 22/300, resid Loss: 0.0928 | 0.0493
Epoch 23/300, resid Loss: 0.0913 | 0.0487
Epoch 24/300, resid Loss: 0.0900 | 0.0475
Epoch 25/300, resid Loss: 0.0889 | 0.0466
Epoch 26/300, resid Loss: 0.0881 | 0.0459
Epoch 27/300, resid Loss: 0.0874 | 0.0453
Epoch 28/300, resid Loss: 0.0869 | 0.0447
Epoch 29/300, resid Loss: 0.0864 | 0.0443
Epoch 30/300, resid Loss: 0.0860 | 0.0439
Epoch 31/300, resid Loss: 0.0857 | 0.0436
Epoch 32/300, resid Loss: 0.0854 | 0.0433
Epoch 33/300, resid Loss: 0.0852 | 0.0431
Epoch 34/300, resid Loss: 0.0850 | 0.0430
Epoch 35/300, resid Loss: 0.0849 | 0.0432
Epoch 36/300, resid Loss: 0.0851 | 0.0434
Epoch 37/300, resid Loss: 0.0854 | 0.0428
Epoch 38/300, resid Loss: 0.0852 | 0.0426
Epoch 39/300, resid Loss: 0.0844 | 0.0426
Epoch 40/300, resid Loss: 0.0840 | 0.0423
Epoch 41/300, resid Loss: 0.0838 | 0.0420
Epoch 42/300, resid Loss: 0.0837 | 0.0419
Epoch 43/300, resid Loss: 0.0835 | 0.0418
Epoch 44/300, resid Loss: 0.0834 | 0.0417
Epoch 45/300, resid Loss: 0.0833 | 0.0415
Epoch 46/300, resid Loss: 0.0832 | 0.0415
Epoch 47/300, resid Loss: 0.0831 | 0.0414
Epoch 48/300, resid Loss: 0.0830 | 0.0413
Epoch 49/300, resid Loss: 0.0830 | 0.0412
Epoch 50/300, resid Loss: 0.0829 | 0.0412
Epoch 51/300, resid Loss: 0.0828 | 0.0411
Epoch 52/300, resid Loss: 0.0828 | 0.0411
Epoch 53/300, resid Loss: 0.0827 | 0.0410
Epoch 54/300, resid Loss: 0.0826 | 0.0410
Epoch 55/300, resid Loss: 0.0826 | 0.0409
Epoch 56/300, resid Loss: 0.0826 | 0.0409
Epoch 57/300, resid Loss: 0.0825 | 0.0409
Epoch 58/300, resid Loss: 0.0825 | 0.0408
Epoch 59/300, resid Loss: 0.0824 | 0.0408
Epoch 60/300, resid Loss: 0.0824 | 0.0408
Epoch 61/300, resid Loss: 0.0824 | 0.0407
Epoch 62/300, resid Loss: 0.0823 | 0.0407
Epoch 63/300, resid Loss: 0.0823 | 0.0407
Epoch 64/300, resid Loss: 0.0823 | 0.0407
Epoch 65/300, resid Loss: 0.0823 | 0.0406
Epoch 66/300, resid Loss: 0.0822 | 0.0406
Epoch 67/300, resid Loss: 0.0822 | 0.0406
Epoch 68/300, resid Loss: 0.0822 | 0.0406
Epoch 69/300, resid Loss: 0.0822 | 0.0406
Epoch 70/300, resid Loss: 0.0822 | 0.0406
Epoch 71/300, resid Loss: 0.0822 | 0.0406
Epoch 72/300, resid Loss: 0.0821 | 0.0405
Epoch 73/300, resid Loss: 0.0821 | 0.0405
Epoch 74/300, resid Loss: 0.0821 | 0.0405
Epoch 75/300, resid Loss: 0.0821 | 0.0405
Epoch 76/300, resid Loss: 0.0821 | 0.0405
Epoch 77/300, resid Loss: 0.0821 | 0.0405
Epoch 78/300, resid Loss: 0.0821 | 0.0405
Epoch 79/300, resid Loss: 0.0821 | 0.0405
Epoch 80/300, resid Loss: 0.0821 | 0.0405
Epoch 81/300, resid Loss: 0.0821 | 0.0405
Epoch 82/300, resid Loss: 0.0821 | 0.0405
Epoch 83/300, resid Loss: 0.0820 | 0.0405
Epoch 84/300, resid Loss: 0.0820 | 0.0405
Epoch 85/300, resid Loss: 0.0820 | 0.0404
Epoch 86/300, resid Loss: 0.0820 | 0.0404
Epoch 87/300, resid Loss: 0.0820 | 0.0404
Epoch 88/300, resid Loss: 0.0820 | 0.0404
Epoch 89/300, resid Loss: 0.0820 | 0.0404
Epoch 90/300, resid Loss: 0.0820 | 0.0404
Epoch 91/300, resid Loss: 0.0820 | 0.0404
Epoch 92/300, resid Loss: 0.0820 | 0.0404
Epoch 93/300, resid Loss: 0.0820 | 0.0404
Epoch 94/300, resid Loss: 0.0820 | 0.0404
Epoch 95/300, resid Loss: 0.0820 | 0.0404
Epoch 96/300, resid Loss: 0.0820 | 0.0404
Epoch 97/300, resid Loss: 0.0820 | 0.0404
Epoch 98/300, resid Loss: 0.0820 | 0.0404
Epoch 99/300, resid Loss: 0.0820 | 0.0404
Epoch 100/300, resid Loss: 0.0820 | 0.0404
Epoch 101/300, resid Loss: 0.0820 | 0.0404
Epoch 102/300, resid Loss: 0.0820 | 0.0404
Epoch 103/300, resid Loss: 0.0820 | 0.0404
Epoch 104/300, resid Loss: 0.0820 | 0.0404
Epoch 105/300, resid Loss: 0.0820 | 0.0404
Epoch 106/300, resid Loss: 0.0820 | 0.0404
Epoch 107/300, resid Loss: 0.0820 | 0.0404
Epoch 108/300, resid Loss: 0.0820 | 0.0404
Epoch 109/300, resid Loss: 0.0820 | 0.0404
Epoch 110/300, resid Loss: 0.0820 | 0.0404
Epoch 111/300, resid Loss: 0.0820 | 0.0404
Epoch 112/300, resid Loss: 0.0820 | 0.0404
Epoch 113/300, resid Loss: 0.0820 | 0.0404
Epoch 114/300, resid Loss: 0.0820 | 0.0404
Epoch 115/300, resid Loss: 0.0820 | 0.0404
Epoch 116/300, resid Loss: 0.0820 | 0.0404
Epoch 117/300, resid Loss: 0.0820 | 0.0404
Epoch 118/300, resid Loss: 0.0820 | 0.0404
Epoch 119/300, resid Loss: 0.0820 | 0.0404
Epoch 120/300, resid Loss: 0.0820 | 0.0404
Epoch 121/300, resid Loss: 0.0820 | 0.0404
Epoch 122/300, resid Loss: 0.0820 | 0.0404
Epoch 123/300, resid Loss: 0.0820 | 0.0404
Epoch 124/300, resid Loss: 0.0820 | 0.0404
Epoch 125/300, resid Loss: 0.0820 | 0.0404
Epoch 126/300, resid Loss: 0.0820 | 0.0404
Epoch 127/300, resid Loss: 0.0820 | 0.0404
Epoch 128/300, resid Loss: 0.0820 | 0.0404
Epoch 129/300, resid Loss: 0.0820 | 0.0404
Epoch 130/300, resid Loss: 0.0820 | 0.0404
Epoch 131/300, resid Loss: 0.0820 | 0.0404
Epoch 132/300, resid Loss: 0.0820 | 0.0404
Epoch 133/300, resid Loss: 0.0820 | 0.0404
Epoch 134/300, resid Loss: 0.0820 | 0.0404
Epoch 135/300, resid Loss: 0.0820 | 0.0404
Epoch 136/300, resid Loss: 0.0820 | 0.0404
Epoch 137/300, resid Loss: 0.0820 | 0.0404
Epoch 138/300, resid Loss: 0.0820 | 0.0404
Epoch 139/300, resid Loss: 0.0820 | 0.0404
Epoch 140/300, resid Loss: 0.0820 | 0.0404
Epoch 141/300, resid Loss: 0.0820 | 0.0404
Epoch 142/300, resid Loss: 0.0820 | 0.0404
Epoch 143/300, resid Loss: 0.0820 | 0.0404
Epoch 144/300, resid Loss: 0.0820 | 0.0404
Epoch 145/300, resid Loss: 0.0820 | 0.0404
Epoch 146/300, resid Loss: 0.0820 | 0.0404
Epoch 147/300, resid Loss: 0.0820 | 0.0404
Epoch 148/300, resid Loss: 0.0820 | 0.0404
Epoch 149/300, resid Loss: 0.0820 | 0.0404
Epoch 150/300, resid Loss: 0.0820 | 0.0404
Epoch 151/300, resid Loss: 0.0820 | 0.0404
Epoch 152/300, resid Loss: 0.0820 | 0.0404
Epoch 153/300, resid Loss: 0.0820 | 0.0404
Epoch 154/300, resid Loss: 0.0820 | 0.0404
Epoch 155/300, resid Loss: 0.0820 | 0.0404
Epoch 156/300, resid Loss: 0.0820 | 0.0404
Epoch 157/300, resid Loss: 0.0820 | 0.0404
Epoch 158/300, resid Loss: 0.0820 | 0.0404
Epoch 159/300, resid Loss: 0.0820 | 0.0404
Epoch 160/300, resid Loss: 0.0820 | 0.0404
Epoch 161/300, resid Loss: 0.0820 | 0.0404
Epoch 162/300, resid Loss: 0.0820 | 0.0404
Epoch 163/300, resid Loss: 0.0820 | 0.0404
Early stopping for resid
Runtime (seconds): 815.3705871105194
0.0009309185747357336
[161.12755]
[-4.7216315]
[2.1126182]
[15.310471]
[1.3712258]
[20.315552]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 104.53452424053103
RMSE: 10.224212646484375
MAE: 10.224212646484375
R-squared: nan
[195.5158]
