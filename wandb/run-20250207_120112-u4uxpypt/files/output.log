[32m[I 2025-02-07 12:01:18,685][0m A new study created in memory with name: no-name-1e9b27fb-672e-4cc6-bf1a-fa7287cd1221[0m
[32m[I 2025-02-07 12:01:59,230][0m Trial 0 finished with value: 0.07046724543066102 and parameters: {'observation_period_num': 9, 'train_rates': 0.8353078706617261, 'learning_rate': 2.4830964712338278e-05, 'batch_size': 142, 'step_size': 11, 'gamma': 0.898292270851659}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:02:23,949][0m Trial 1 finished with value: 0.9527561433853642 and parameters: {'observation_period_num': 54, 'train_rates': 0.7158936507137698, 'learning_rate': 8.220322389680221e-06, 'batch_size': 218, 'step_size': 5, 'gamma': 0.8066826164793756}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:03:18,134][0m Trial 2 finished with value: 0.7624688599589948 and parameters: {'observation_period_num': 150, 'train_rates': 0.6133098334171974, 'learning_rate': 1.0459006367990138e-06, 'batch_size': 83, 'step_size': 15, 'gamma': 0.7507621324996162}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:03:43,290][0m Trial 3 finished with value: 0.8668909686900623 and parameters: {'observation_period_num': 161, 'train_rates': 0.8482510644124721, 'learning_rate': 5.126442489225705e-06, 'batch_size': 222, 'step_size': 5, 'gamma': 0.7727702152533154}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:04:07,136][0m Trial 4 finished with value: 0.20707522337860346 and parameters: {'observation_period_num': 118, 'train_rates': 0.800794494896282, 'learning_rate': 0.00033428179094691795, 'batch_size': 247, 'step_size': 12, 'gamma': 0.9644624268403437}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:04:57,459][0m Trial 5 finished with value: 0.14807866122096014 and parameters: {'observation_period_num': 55, 'train_rates': 0.9473007276434384, 'learning_rate': 3.432433657717509e-05, 'batch_size': 125, 'step_size': 12, 'gamma': 0.7775392041190039}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:05:28,072][0m Trial 6 finished with value: 0.2515734335355146 and parameters: {'observation_period_num': 11, 'train_rates': 0.6200776534700507, 'learning_rate': 8.507764115893553e-06, 'batch_size': 158, 'step_size': 13, 'gamma': 0.8011843382887637}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:05:50,951][0m Trial 7 finished with value: 0.4737428717636035 and parameters: {'observation_period_num': 163, 'train_rates': 0.7052461174563386, 'learning_rate': 3.192898597207544e-05, 'batch_size': 224, 'step_size': 12, 'gamma': 0.7675814692878639}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:06:17,476][0m Trial 8 finished with value: 0.22917214825685625 and parameters: {'observation_period_num': 98, 'train_rates': 0.7584915654934049, 'learning_rate': 0.00038825677648839834, 'batch_size': 209, 'step_size': 8, 'gamma': 0.8542467707415874}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:11:20,237][0m Trial 9 finished with value: 0.30852905302557326 and parameters: {'observation_period_num': 247, 'train_rates': 0.8098948315668704, 'learning_rate': 0.00013800093172698858, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8937408305459177}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:12:00,145][0m Trial 10 finished with value: 0.3731266722596925 and parameters: {'observation_period_num': 237, 'train_rates': 0.9373356428421047, 'learning_rate': 9.680878445571138e-05, 'batch_size': 150, 'step_size': 1, 'gamma': 0.9410921687275867}. Best is trial 0 with value: 0.07046724543066102.[0m
[32m[I 2025-02-07 12:13:07,802][0m Trial 11 finished with value: 0.04466591402888298 and parameters: {'observation_period_num': 5, 'train_rates': 0.9883726169409469, 'learning_rate': 3.583109526981373e-05, 'batch_size': 95, 'step_size': 9, 'gamma': 0.8858031456948561}. Best is trial 11 with value: 0.04466591402888298.[0m
[32m[I 2025-02-07 12:14:13,056][0m Trial 12 finished with value: 0.03973069909396906 and parameters: {'observation_period_num': 5, 'train_rates': 0.8853946681483608, 'learning_rate': 7.425655838678245e-05, 'batch_size': 91, 'step_size': 9, 'gamma': 0.8963308066519914}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:15:38,730][0m Trial 13 finished with value: 0.07238570600748062 and parameters: {'observation_period_num': 53, 'train_rates': 0.9848796187273092, 'learning_rate': 0.00010606209979758548, 'batch_size': 72, 'step_size': 8, 'gamma': 0.8534708812327004}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:16:47,777][0m Trial 14 finished with value: 0.06950173289938406 and parameters: {'observation_period_num': 28, 'train_rates': 0.8964633613589369, 'learning_rate': 0.0009870297548740376, 'batch_size': 84, 'step_size': 6, 'gamma': 0.9159898433964972}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:19:15,116][0m Trial 15 finished with value: 0.12677376449414313 and parameters: {'observation_period_num': 95, 'train_rates': 0.8995952407389689, 'learning_rate': 5.541881397459109e-05, 'batch_size': 38, 'step_size': 9, 'gamma': 0.8767150703245365}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:20:06,818][0m Trial 16 finished with value: 0.40792742371559143 and parameters: {'observation_period_num': 203, 'train_rates': 0.9751046284628712, 'learning_rate': 1.551007713012094e-05, 'batch_size': 115, 'step_size': 2, 'gamma': 0.9852253146647398}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:20:40,135][0m Trial 17 finished with value: 0.6692662586027117 and parameters: {'observation_period_num': 39, 'train_rates': 0.8870549579540525, 'learning_rate': 2.9270753922845984e-06, 'batch_size': 178, 'step_size': 7, 'gamma': 0.9270868294299914}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:21:37,234][0m Trial 18 finished with value: 0.0804623627761529 and parameters: {'observation_period_num': 78, 'train_rates': 0.9278346559327274, 'learning_rate': 0.0001942957960859855, 'batch_size': 106, 'step_size': 9, 'gamma': 0.8309721417514164}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:23:09,939][0m Trial 19 finished with value: 0.23631552330190592 and parameters: {'observation_period_num': 77, 'train_rates': 0.861871326638669, 'learning_rate': 1.611541202192757e-05, 'batch_size': 59, 'step_size': 3, 'gamma': 0.8745676126313783}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:24:14,239][0m Trial 20 finished with value: 0.0533086359500885 and parameters: {'observation_period_num': 6, 'train_rates': 0.9877727180887913, 'learning_rate': 6.807697775269813e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9447479294301654}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:25:16,150][0m Trial 21 finished with value: 0.04520373046398163 and parameters: {'observation_period_num': 5, 'train_rates': 0.9779516973569385, 'learning_rate': 5.9838451868287605e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9416820158499557}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:27:16,667][0m Trial 22 finished with value: 0.058309814892709255 and parameters: {'observation_period_num': 35, 'train_rates': 0.9527770709201555, 'learning_rate': 4.809922371244792e-05, 'batch_size': 49, 'step_size': 14, 'gamma': 0.913540202740559}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:28:19,354][0m Trial 23 finished with value: 0.05329614948099992 and parameters: {'observation_period_num': 28, 'train_rates': 0.9146478837519751, 'learning_rate': 0.00018265137130502404, 'batch_size': 94, 'step_size': 10, 'gamma': 0.960023536419749}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:29:10,230][0m Trial 24 finished with value: 0.08310414031147957 and parameters: {'observation_period_num': 7, 'train_rates': 0.9518673847533098, 'learning_rate': 1.8314783220188444e-05, 'batch_size': 128, 'step_size': 7, 'gamma': 0.8959263975402902}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:29:44,647][0m Trial 25 finished with value: 0.10541003502228043 and parameters: {'observation_period_num': 69, 'train_rates': 0.8798798768173455, 'learning_rate': 6.603530001262802e-05, 'batch_size': 172, 'step_size': 9, 'gamma': 0.8546903999612996}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:31:10,472][0m Trial 26 finished with value: 0.053059637905997145 and parameters: {'observation_period_num': 31, 'train_rates': 0.9618214147818074, 'learning_rate': 0.0002795079918476891, 'batch_size': 71, 'step_size': 10, 'gamma': 0.9312409506158348}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:34:50,442][0m Trial 27 finished with value: 0.047132655933677436 and parameters: {'observation_period_num': 22, 'train_rates': 0.9159456589289235, 'learning_rate': 0.000748654598187825, 'batch_size': 26, 'step_size': 7, 'gamma': 0.9742000152963287}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:35:46,703][0m Trial 28 finished with value: 0.15067167580127716 and parameters: {'observation_period_num': 53, 'train_rates': 0.9886188683423293, 'learning_rate': 4.100790709885251e-05, 'batch_size': 113, 'step_size': 4, 'gamma': 0.9077733959496574}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:36:29,421][0m Trial 29 finished with value: 0.06684261352076369 and parameters: {'observation_period_num': 5, 'train_rates': 0.8423540563084356, 'learning_rate': 2.4551150756983207e-05, 'batch_size': 140, 'step_size': 11, 'gamma': 0.8815356151782499}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:37:59,314][0m Trial 30 finished with value: 0.23848065072598384 and parameters: {'observation_period_num': 122, 'train_rates': 0.8207566562128561, 'learning_rate': 1.071297296756814e-05, 'batch_size': 59, 'step_size': 9, 'gamma': 0.8308458635514141}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:41:58,335][0m Trial 31 finished with value: 0.055560854579112966 and parameters: {'observation_period_num': 22, 'train_rates': 0.9179503305104155, 'learning_rate': 0.0006288504567603033, 'batch_size': 24, 'step_size': 7, 'gamma': 0.9871999252684648}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:44:45,508][0m Trial 32 finished with value: 0.04711574626465639 and parameters: {'observation_period_num': 21, 'train_rates': 0.9297918218350145, 'learning_rate': 0.0005522894975298723, 'batch_size': 35, 'step_size': 8, 'gamma': 0.9730998223242728}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:45:49,272][0m Trial 33 finished with value: 0.05104548823802582 and parameters: {'observation_period_num': 39, 'train_rates': 0.8696155766896985, 'learning_rate': 0.00011459043221394561, 'batch_size': 91, 'step_size': 6, 'gamma': 0.955924436277983}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:47:01,026][0m Trial 34 finished with value: 0.27563911725190426 and parameters: {'observation_period_num': 15, 'train_rates': 0.7749400464057564, 'learning_rate': 2.899918897478802e-06, 'batch_size': 73, 'step_size': 11, 'gamma': 0.9440434923773633}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:49:05,946][0m Trial 35 finished with value: 0.06298683442567524 and parameters: {'observation_period_num': 48, 'train_rates': 0.968042338137262, 'learning_rate': 7.590235922906973e-05, 'batch_size': 48, 'step_size': 8, 'gamma': 0.9260372677313439}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:49:55,436][0m Trial 36 finished with value: 0.07040081860610793 and parameters: {'observation_period_num': 16, 'train_rates': 0.9396272589811607, 'learning_rate': 2.4590362181357144e-05, 'batch_size': 125, 'step_size': 10, 'gamma': 0.9745684112107352}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:51:07,356][0m Trial 37 finished with value: 0.1270783632993698 and parameters: {'observation_period_num': 69, 'train_rates': 0.964417154214937, 'learning_rate': 0.0004967827752092298, 'batch_size': 84, 'step_size': 5, 'gamma': 0.88971618300316}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:53:08,204][0m Trial 38 finished with value: 0.23185841333778465 and parameters: {'observation_period_num': 145, 'train_rates': 0.66654616843337, 'learning_rate': 0.00022996542426092104, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9079215024212218}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:54:41,082][0m Trial 39 finished with value: 0.059232457741120154 and parameters: {'observation_period_num': 43, 'train_rates': 0.8553272363636245, 'learning_rate': 0.00014954520737469565, 'batch_size': 60, 'step_size': 8, 'gamma': 0.8622236745062721}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:55:31,931][0m Trial 40 finished with value: 0.651988914193109 and parameters: {'observation_period_num': 192, 'train_rates': 0.908139895329334, 'learning_rate': 4.017323412280977e-06, 'batch_size': 113, 'step_size': 6, 'gamma': 0.8360052999438927}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 12:59:34,155][0m Trial 41 finished with value: 0.05065946472753392 and parameters: {'observation_period_num': 23, 'train_rates': 0.9322512368142919, 'learning_rate': 0.0009515156188489836, 'batch_size': 24, 'step_size': 7, 'gamma': 0.9764817942964803}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 13:02:19,793][0m Trial 42 finished with value: 0.04552710184005931 and parameters: {'observation_period_num': 22, 'train_rates': 0.9230877953141049, 'learning_rate': 0.0004109050979293504, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9674393607613299}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 13:04:42,350][0m Trial 43 finished with value: 0.05842435765080154 and parameters: {'observation_period_num': 18, 'train_rates': 0.9466454943476296, 'learning_rate': 0.00037581300811447645, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9565307253431042}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 13:05:44,707][0m Trial 44 finished with value: 0.09707316756248474 and parameters: {'observation_period_num': 62, 'train_rates': 0.9679641299136652, 'learning_rate': 0.0004970592299519567, 'batch_size': 101, 'step_size': 9, 'gamma': 0.9376485960256483}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 13:06:53,759][0m Trial 45 finished with value: 0.21390564747577928 and parameters: {'observation_period_num': 93, 'train_rates': 0.7396890249661836, 'learning_rate': 3.493642977909207e-05, 'batch_size': 72, 'step_size': 12, 'gamma': 0.9642334553291441}. Best is trial 12 with value: 0.03973069909396906.[0m
[32m[I 2025-02-07 13:07:34,396][0m Trial 46 finished with value: 0.03876468588688732 and parameters: {'observation_period_num': 5, 'train_rates': 0.8870943508344702, 'learning_rate': 0.00029706256269902475, 'batch_size': 151, 'step_size': 8, 'gamma': 0.9512068929660524}. Best is trial 46 with value: 0.03876468588688732.[0m
[32m[I 2025-02-07 13:08:10,904][0m Trial 47 finished with value: 0.046599741863777855 and parameters: {'observation_period_num': 10, 'train_rates': 0.8223937130919775, 'learning_rate': 8.364665031668904e-05, 'batch_size': 158, 'step_size': 10, 'gamma': 0.9497634956825693}. Best is trial 46 with value: 0.03876468588688732.[0m
[32m[I 2025-02-07 13:08:40,701][0m Trial 48 finished with value: 0.060848984143263854 and parameters: {'observation_period_num': 40, 'train_rates': 0.8801960745649208, 'learning_rate': 0.00032302842519334326, 'batch_size': 202, 'step_size': 9, 'gamma': 0.8873222871849542}. Best is trial 46 with value: 0.03876468588688732.[0m
[32m[I 2025-02-07 13:09:21,060][0m Trial 49 finished with value: 0.04062071546621911 and parameters: {'observation_period_num': 6, 'train_rates': 0.8973931307653743, 'learning_rate': 0.000120638355281396, 'batch_size': 150, 'step_size': 13, 'gamma': 0.9153249350840701}. Best is trial 46 with value: 0.03876468588688732.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.3052 | 0.2072
Epoch 2/300, Loss: 0.1609 | 0.1670
Epoch 3/300, Loss: 0.1439 | 0.1524
Epoch 4/300, Loss: 0.1321 | 0.1261
Epoch 5/300, Loss: 0.1388 | 0.1923
Epoch 6/300, Loss: 0.1657 | 0.3034
Epoch 7/300, Loss: 0.1435 | 0.1070
Epoch 8/300, Loss: 0.1269 | 0.0907
Epoch 9/300, Loss: 0.1263 | 0.0943
Epoch 10/300, Loss: 0.1133 | 0.0742
Epoch 11/300, Loss: 0.1096 | 0.0726
Epoch 12/300, Loss: 0.1105 | 0.0716
Epoch 13/300, Loss: 0.1091 | 0.0755
Epoch 14/300, Loss: 0.1171 | 0.0788
Epoch 15/300, Loss: 0.1268 | 0.0973
Epoch 16/300, Loss: 0.1284 | 0.0746
Epoch 17/300, Loss: 0.1205 | 0.0966
Epoch 18/300, Loss: 0.1107 | 0.0929
Epoch 19/300, Loss: 0.1257 | 0.0925
Epoch 20/300, Loss: 0.1196 | 0.0775
Epoch 21/300, Loss: 0.0993 | 0.0790
Epoch 22/300, Loss: 0.0957 | 0.0623
Epoch 23/300, Loss: 0.0949 | 0.0702
Epoch 24/300, Loss: 0.0980 | 0.0703
Epoch 25/300, Loss: 0.0921 | 0.0560
Epoch 26/300, Loss: 0.0900 | 0.0706
Epoch 27/300, Loss: 0.0894 | 0.0632
Epoch 28/300, Loss: 0.0871 | 0.0527
Epoch 29/300, Loss: 0.0851 | 0.0526
Epoch 30/300, Loss: 0.0838 | 0.0498
Epoch 31/300, Loss: 0.0828 | 0.0478
Epoch 32/300, Loss: 0.0820 | 0.0475
Epoch 33/300, Loss: 0.0813 | 0.0464
Epoch 34/300, Loss: 0.0807 | 0.0456
Epoch 35/300, Loss: 0.0802 | 0.0447
Epoch 36/300, Loss: 0.0798 | 0.0437
Epoch 37/300, Loss: 0.0795 | 0.0430
Epoch 38/300, Loss: 0.0797 | 0.0428
Epoch 39/300, Loss: 0.0802 | 0.0429
Epoch 40/300, Loss: 0.0809 | 0.0428
Epoch 41/300, Loss: 0.0811 | 0.0440
Epoch 42/300, Loss: 0.0807 | 0.0436
Epoch 43/300, Loss: 0.0793 | 0.0435
Epoch 44/300, Loss: 0.0795 | 0.0495
Epoch 45/300, Loss: 0.0837 | 0.0736
Epoch 46/300, Loss: 0.0851 | 0.0674
Epoch 47/300, Loss: 0.0801 | 0.0421
Epoch 48/300, Loss: 0.0798 | 0.0437
Epoch 49/300, Loss: 0.0804 | 0.0485
Epoch 50/300, Loss: 0.0785 | 0.0409
Epoch 51/300, Loss: 0.0768 | 0.0500
Epoch 52/300, Loss: 0.0767 | 0.0478
Epoch 53/300, Loss: 0.0766 | 0.0405
Epoch 54/300, Loss: 0.0754 | 0.0436
Epoch 55/300, Loss: 0.0743 | 0.0409
Epoch 56/300, Loss: 0.0736 | 0.0382
Epoch 57/300, Loss: 0.0733 | 0.0377
Epoch 58/300, Loss: 0.0730 | 0.0392
Epoch 59/300, Loss: 0.0727 | 0.0375
Epoch 60/300, Loss: 0.0725 | 0.0369
Epoch 61/300, Loss: 0.0724 | 0.0375
Epoch 62/300, Loss: 0.0723 | 0.0389
Epoch 63/300, Loss: 0.0725 | 0.0394
Epoch 64/300, Loss: 0.0729 | 0.0392
Epoch 65/300, Loss: 0.0730 | 0.0385
Epoch 66/300, Loss: 0.0724 | 0.0363
Epoch 67/300, Loss: 0.0730 | 0.0379
Epoch 68/300, Loss: 0.0762 | 0.0419
Epoch 69/300, Loss: 0.0767 | 0.0389
Epoch 70/300, Loss: 0.0750 | 0.0528
Epoch 71/300, Loss: 0.0747 | 0.0410
Epoch 72/300, Loss: 0.0737 | 0.0365
Epoch 73/300, Loss: 0.0724 | 0.0485
Epoch 74/300, Loss: 0.0727 | 0.0360
Epoch 75/300, Loss: 0.0720 | 0.0359
Epoch 76/300, Loss: 0.0711 | 0.0403
Epoch 77/300, Loss: 0.0715 | 0.0344
Epoch 78/300, Loss: 0.0718 | 0.0363
Epoch 79/300, Loss: 0.0712 | 0.0444
Epoch 80/300, Loss: 0.0703 | 0.0345
Epoch 81/300, Loss: 0.0704 | 0.0346
Epoch 82/300, Loss: 0.0718 | 0.0381
Epoch 83/300, Loss: 0.0745 | 0.0370
Epoch 84/300, Loss: 0.0778 | 0.0400
Epoch 85/300, Loss: 0.0772 | 0.0392
Epoch 86/300, Loss: 0.0753 | 0.0367
Epoch 87/300, Loss: 0.0753 | 0.0376
Epoch 88/300, Loss: 0.0749 | 0.0416
Epoch 89/300, Loss: 0.0755 | 0.0429
Epoch 90/300, Loss: 0.0735 | 0.0387
Epoch 91/300, Loss: 0.0693 | 0.0348
Epoch 92/300, Loss: 0.0694 | 0.0344
Epoch 93/300, Loss: 0.0689 | 0.0348
Epoch 94/300, Loss: 0.0681 | 0.0373
Epoch 95/300, Loss: 0.0677 | 0.0341
Epoch 96/300, Loss: 0.0673 | 0.0340
Epoch 97/300, Loss: 0.0672 | 0.0350
Epoch 98/300, Loss: 0.0671 | 0.0339
Epoch 99/300, Loss: 0.0668 | 0.0336
Epoch 100/300, Loss: 0.0666 | 0.0337
Epoch 101/300, Loss: 0.0664 | 0.0331
Epoch 102/300, Loss: 0.0663 | 0.0329
Epoch 103/300, Loss: 0.0661 | 0.0330
Epoch 104/300, Loss: 0.0659 | 0.0326
Epoch 105/300, Loss: 0.0658 | 0.0322
Epoch 106/300, Loss: 0.0656 | 0.0322
Epoch 107/300, Loss: 0.0655 | 0.0322
Epoch 108/300, Loss: 0.0654 | 0.0319
Epoch 109/300, Loss: 0.0654 | 0.0319
Epoch 110/300, Loss: 0.0653 | 0.0319
Epoch 111/300, Loss: 0.0652 | 0.0317
Epoch 112/300, Loss: 0.0651 | 0.0316
Epoch 113/300, Loss: 0.0651 | 0.0316
Epoch 114/300, Loss: 0.0651 | 0.0316
Epoch 115/300, Loss: 0.0653 | 0.0315
Epoch 116/300, Loss: 0.0661 | 0.0336
Epoch 117/300, Loss: 0.0687 | 0.0360
Epoch 118/300, Loss: 0.0696 | 0.0352
Epoch 119/300, Loss: 0.0680 | 0.0323
Epoch 120/300, Loss: 0.0690 | 0.0349
Epoch 121/300, Loss: 0.0682 | 0.0361
Epoch 122/300, Loss: 0.0690 | 0.0348
Epoch 123/300, Loss: 0.0682 | 0.0341
Epoch 124/300, Loss: 0.0666 | 0.0322
Epoch 125/300, Loss: 0.0659 | 0.0324
Epoch 126/300, Loss: 0.0658 | 0.0342
Epoch 127/300, Loss: 0.0648 | 0.0332
Epoch 128/300, Loss: 0.0642 | 0.0314
Epoch 129/300, Loss: 0.0641 | 0.0325
Epoch 130/300, Loss: 0.0639 | 0.0327
Epoch 131/300, Loss: 0.0637 | 0.0318
Epoch 132/300, Loss: 0.0637 | 0.0321
Epoch 133/300, Loss: 0.0637 | 0.0325
Epoch 134/300, Loss: 0.0639 | 0.0319
Epoch 135/300, Loss: 0.0642 | 0.0316
Epoch 136/300, Loss: 0.0648 | 0.0320
Epoch 137/300, Loss: 0.0664 | 0.0335
Epoch 138/300, Loss: 0.0673 | 0.0332
Epoch 139/300, Loss: 0.0655 | 0.0320
Epoch 140/300, Loss: 0.0655 | 0.0321
Epoch 141/300, Loss: 0.0651 | 0.0319
Epoch 142/300, Loss: 0.0641 | 0.0311
Epoch 143/300, Loss: 0.0634 | 0.0323
Epoch 144/300, Loss: 0.0635 | 0.0321
Epoch 145/300, Loss: 0.0635 | 0.0330
Epoch 146/300, Loss: 0.0637 | 0.0324
Epoch 147/300, Loss: 0.0635 | 0.0322
Epoch 148/300, Loss: 0.0630 | 0.0319
Epoch 149/300, Loss: 0.0627 | 0.0318
Epoch 150/300, Loss: 0.0627 | 0.0315
Epoch 151/300, Loss: 0.0630 | 0.0310
Epoch 152/300, Loss: 0.0636 | 0.0316
Epoch 153/300, Loss: 0.0652 | 0.0319
Epoch 154/300, Loss: 0.0650 | 0.0326
Epoch 155/300, Loss: 0.0632 | 0.0304
Epoch 156/300, Loss: 0.0633 | 0.0300
Epoch 157/300, Loss: 0.0625 | 0.0310
Epoch 158/300, Loss: 0.0619 | 0.0309
Epoch 159/300, Loss: 0.0618 | 0.0305
Epoch 160/300, Loss: 0.0617 | 0.0303
Epoch 161/300, Loss: 0.0616 | 0.0307
Epoch 162/300, Loss: 0.0616 | 0.0304
Epoch 163/300, Loss: 0.0617 | 0.0304
Epoch 164/300, Loss: 0.0619 | 0.0303
Epoch 165/300, Loss: 0.0624 | 0.0311
Epoch 166/300, Loss: 0.0631 | 0.0306
Epoch 167/300, Loss: 0.0623 | 0.0306
Epoch 168/300, Loss: 0.0620 | 0.0305
Epoch 169/300, Loss: 0.0628 | 0.0317
Epoch 170/300, Loss: 0.0629 | 0.0316
Epoch 171/300, Loss: 0.0619 | 0.0307
Epoch 172/300, Loss: 0.0612 | 0.0308
Epoch 173/300, Loss: 0.0613 | 0.0307
Epoch 174/300, Loss: 0.0614 | 0.0300
Epoch 175/300, Loss: 0.0615 | 0.0303
Epoch 176/300, Loss: 0.0614 | 0.0301
Epoch 177/300, Loss: 0.0609 | 0.0298
Epoch 178/300, Loss: 0.0607 | 0.0301
Epoch 179/300, Loss: 0.0607 | 0.0303
Epoch 180/300, Loss: 0.0607 | 0.0306
Epoch 181/300, Loss: 0.0606 | 0.0307
Epoch 182/300, Loss: 0.0605 | 0.0305
Epoch 183/300, Loss: 0.0603 | 0.0302
Epoch 184/300, Loss: 0.0602 | 0.0299
Epoch 185/300, Loss: 0.0602 | 0.0295
Epoch 186/300, Loss: 0.0603 | 0.0293
Epoch 187/300, Loss: 0.0605 | 0.0295
Epoch 188/300, Loss: 0.0607 | 0.0298
Epoch 189/300, Loss: 0.0605 | 0.0295
Epoch 190/300, Loss: 0.0600 | 0.0298
Epoch 191/300, Loss: 0.0601 | 0.0302
Epoch 192/300, Loss: 0.0602 | 0.0303
Epoch 193/300, Loss: 0.0600 | 0.0299
Epoch 194/300, Loss: 0.0597 | 0.0296
Epoch 195/300, Loss: 0.0595 | 0.0294
Epoch 196/300, Loss: 0.0595 | 0.0289
Epoch 197/300, Loss: 0.0597 | 0.0288
Epoch 198/300, Loss: 0.0599 | 0.0290
Epoch 199/300, Loss: 0.0598 | 0.0289
Epoch 200/300, Loss: 0.0594 | 0.0294
Epoch 201/300, Loss: 0.0595 | 0.0296
Epoch 202/300, Loss: 0.0595 | 0.0295
Epoch 203/300, Loss: 0.0592 | 0.0292
Epoch 204/300, Loss: 0.0590 | 0.0289
Epoch 205/300, Loss: 0.0590 | 0.0286
Epoch 206/300, Loss: 0.0590 | 0.0283
Epoch 207/300, Loss: 0.0590 | 0.0283
Epoch 208/300, Loss: 0.0589 | 0.0283
Epoch 209/300, Loss: 0.0588 | 0.0285
Epoch 210/300, Loss: 0.0587 | 0.0287
Epoch 211/300, Loss: 0.0587 | 0.0290
Epoch 212/300, Loss: 0.0587 | 0.0291
Epoch 213/300, Loss: 0.0587 | 0.0290
Epoch 214/300, Loss: 0.0585 | 0.0288
Epoch 215/300, Loss: 0.0584 | 0.0285
Epoch 216/300, Loss: 0.0584 | 0.0282
Epoch 217/300, Loss: 0.0584 | 0.0279
Epoch 218/300, Loss: 0.0584 | 0.0278
Epoch 219/300, Loss: 0.0584 | 0.0279
Epoch 220/300, Loss: 0.0583 | 0.0280
Epoch 221/300, Loss: 0.0582 | 0.0282
Epoch 222/300, Loss: 0.0581 | 0.0285
Epoch 223/300, Loss: 0.0581 | 0.0286
Epoch 224/300, Loss: 0.0581 | 0.0286
Epoch 225/300, Loss: 0.0580 | 0.0284
Epoch 226/300, Loss: 0.0579 | 0.0282
Epoch 227/300, Loss: 0.0579 | 0.0279
Epoch 228/300, Loss: 0.0579 | 0.0276
Epoch 229/300, Loss: 0.0579 | 0.0275
Epoch 230/300, Loss: 0.0579 | 0.0275
Epoch 231/300, Loss: 0.0579 | 0.0276
Epoch 232/300, Loss: 0.0577 | 0.0279
Epoch 233/300, Loss: 0.0577 | 0.0281
Epoch 234/300, Loss: 0.0577 | 0.0283
Epoch 235/300, Loss: 0.0576 | 0.0282
Epoch 236/300, Loss: 0.0576 | 0.0281
Epoch 237/300, Loss: 0.0575 | 0.0279
Epoch 238/300, Loss: 0.0575 | 0.0276
Epoch 239/300, Loss: 0.0574 | 0.0275
Epoch 240/300, Loss: 0.0575 | 0.0274
Epoch 241/300, Loss: 0.0575 | 0.0274
Epoch 242/300, Loss: 0.0574 | 0.0274
Epoch 243/300, Loss: 0.0574 | 0.0276
Epoch 244/300, Loss: 0.0573 | 0.0279
Epoch 245/300, Loss: 0.0573 | 0.0280
Epoch 246/300, Loss: 0.0573 | 0.0280
Epoch 247/300, Loss: 0.0572 | 0.0279
Epoch 248/300, Loss: 0.0572 | 0.0277
Epoch 249/300, Loss: 0.0571 | 0.0275
Epoch 250/300, Loss: 0.0571 | 0.0273
Epoch 251/300, Loss: 0.0571 | 0.0273
Epoch 252/300, Loss: 0.0571 | 0.0273
Epoch 253/300, Loss: 0.0571 | 0.0273
Epoch 254/300, Loss: 0.0570 | 0.0275
Epoch 255/300, Loss: 0.0570 | 0.0276
Epoch 256/300, Loss: 0.0570 | 0.0278
Epoch 257/300, Loss: 0.0570 | 0.0278
Epoch 258/300, Loss: 0.0569 | 0.0277
Epoch 259/300, Loss: 0.0569 | 0.0276
Epoch 260/300, Loss: 0.0569 | 0.0275
Epoch 261/300, Loss: 0.0568 | 0.0273
Epoch 262/300, Loss: 0.0568 | 0.0273
Epoch 263/300, Loss: 0.0568 | 0.0273
Epoch 264/300, Loss: 0.0568 | 0.0273
Epoch 265/300, Loss: 0.0568 | 0.0274
Epoch 266/300, Loss: 0.0567 | 0.0275
Epoch 267/300, Loss: 0.0567 | 0.0275
Epoch 268/300, Loss: 0.0567 | 0.0276
Epoch 269/300, Loss: 0.0567 | 0.0276
Epoch 270/300, Loss: 0.0567 | 0.0275
Epoch 271/300, Loss: 0.0566 | 0.0274
Epoch 272/300, Loss: 0.0566 | 0.0274
Epoch 273/300, Loss: 0.0566 | 0.0273
Epoch 274/300, Loss: 0.0566 | 0.0273
Epoch 275/300, Loss: 0.0566 | 0.0273
Epoch 276/300, Loss: 0.0565 | 0.0273
Epoch 277/300, Loss: 0.0565 | 0.0274
Epoch 278/300, Loss: 0.0565 | 0.0274
Epoch 279/300, Loss: 0.0565 | 0.0274
Epoch 280/300, Loss: 0.0565 | 0.0274
Epoch 281/300, Loss: 0.0565 | 0.0274
Epoch 282/300, Loss: 0.0564 | 0.0274
Epoch 283/300, Loss: 0.0564 | 0.0273
Epoch 284/300, Loss: 0.0564 | 0.0273
Epoch 285/300, Loss: 0.0564 | 0.0273
Epoch 286/300, Loss: 0.0564 | 0.0273
Epoch 287/300, Loss: 0.0564 | 0.0273
Epoch 288/300, Loss: 0.0563 | 0.0273
Epoch 289/300, Loss: 0.0563 | 0.0274
Epoch 290/300, Loss: 0.0563 | 0.0274
Epoch 291/300, Loss: 0.0563 | 0.0274
Epoch 292/300, Loss: 0.0563 | 0.0273
Epoch 293/300, Loss: 0.0563 | 0.0273
Epoch 294/300, Loss: 0.0562 | 0.0273
Epoch 295/300, Loss: 0.0562 | 0.0273
Epoch 296/300, Loss: 0.0562 | 0.0273
Epoch 297/300, Loss: 0.0562 | 0.0273
Epoch 298/300, Loss: 0.0562 | 0.0273
Epoch 299/300, Loss: 0.0562 | 0.0273
Epoch 300/300, Loss: 0.0562 | 0.0273
Runtime (seconds): 120.68257141113281
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 5.11992818210274
RMSE: 2.262725830078125
MAE: 2.262725830078125
R-squared: nan
[195.57272]
