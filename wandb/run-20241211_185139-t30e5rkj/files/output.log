[*********************100%***********************]  1 of 1 completed
[*********************100%***********************]  1 of 1 completed
Date
2012-05-18     10.692500
2012-05-21     10.905500
2012-05-22     10.766500
2012-05-23     10.864000
2012-05-24     10.762000
                 ...
2023-05-24    116.750000
2023-05-25    115.000000
2023-05-26    120.110001
2023-05-30    121.660004
2023-05-31    120.580002
Name: AMZN, Length: 2776, dtype: float64
Price         Volume    BB_Upper    BB_Lower   BB_Middle      MACD MACD_Signal MACD_Diff        RSI    SMA_50    SMA_200 SMA_200-50
Ticker          AMZN
Date
2023-05-24  63487900  120.005693  100.053308  110.029501  3.584615    3.109054  0.475561  66.184276  104.7736  105.39470    0.62110
2023-05-25  66496700  120.495777  100.081225  110.288501  3.450388    3.177321  0.273067  61.346390  105.1496  105.28055    0.13095
2023-05-26  96779900  121.821939  100.221062  111.021501  3.713539    3.284565  0.428975  68.570791  105.5510  105.16765   -0.38335
2023-05-30  64314800  122.926375  101.077627  112.002001  4.001039    3.427859  0.573180  70.379232  106.0052  105.07275   -0.93245
2023-05-31  72800800  123.673883  102.025119  112.849501  4.094538    3.561195  0.533343  67.466273  106.4626  104.95790   -1.50470
{'observation_period_num': 5, 'train_rates': 0.9674192065835944, 'learning_rate': 0.0006703481785402301, 'batch_size': 197, 'step_size': 5, 'gamma': 0.9119241664414688, 'depth': 4, 'dim': 123}
{0: {'observation_period_num': 14, 'train_rates': 0.9534530255286052, 'learning_rate': 2.383027942061777e-05, 'batch_size': 24, 'step_size': 6, 'gamma': 0.9343469177823375, 'depth': 5, 'dim': 162}, 1: {'observation_period_num': 10, 'train_rates': 0.7859430649262442, 'learning_rate': 0.00011626904101397289, 'batch_size': 75, 'step_size': 7, 'gamma': 0.9591926500011835, 'depth': 5, 'dim': 221}, 2: {'observation_period_num': 5, 'train_rates': 0.9881973682668941, 'learning_rate': 0.0008599253544665889, 'batch_size': 46, 'step_size': 2, 'gamma': 0.8332821730513479, 'depth': 5, 'dim': 127}, 3: {'observation_period_num': 11, 'train_rates': 0.9524554300989908, 'learning_rate': 0.00011007906140572895, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9320524916302059, 'depth': 6, 'dim': 145}}
{'observation_period_num': 13, 'train_rates': 0.8047176482675613, 'learning_rate': 0.0007766732022188664, 'batch_size': 128, 'step_size': 2, 'gamma': 0.925182630519391, 'depth': 4, 'dim': 228}
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Epoch 1/500, trend Loss: 0.4300 | 0.2884
Epoch 2/500, trend Loss: 0.1724 | 0.1741
Epoch 3/500, trend Loss: 0.1608 | 0.1367
Epoch 4/500, trend Loss: 0.1594 | 0.1899
Epoch 5/500, trend Loss: 0.1883 | 0.2966
Epoch 6/500, trend Loss: 0.2048 | 0.1985
Epoch 7/500, trend Loss: 0.1502 | 0.1348
Epoch 8/500, trend Loss: 0.1373 | 0.0785
Epoch 9/500, trend Loss: 0.1195 | 0.0868
Epoch 10/500, trend Loss: 0.1501 | 0.1689
Epoch 11/500, trend Loss: 0.1805 | 0.0841
Epoch 12/500, trend Loss: 0.1495 | 0.1020
Epoch 13/500, trend Loss: 0.1189 | 0.0661
Epoch 14/500, trend Loss: 0.1123 | 0.1063
Epoch 15/500, trend Loss: 0.1216 | 0.0839
Epoch 16/500, trend Loss: 0.1158 | 0.1132
Epoch 17/500, trend Loss: 0.1001 | 0.0655
Epoch 18/500, trend Loss: 0.0823 | 0.0601
Epoch 19/500, trend Loss: 0.0756 | 0.0536
Epoch 20/500, trend Loss: 0.0765 | 0.0591
Epoch 21/500, trend Loss: 0.0747 | 0.0559
Epoch 22/500, trend Loss: 0.0752 | 0.0549
Epoch 23/500, trend Loss: 0.0742 | 0.0554
Epoch 24/500, trend Loss: 0.0755 | 0.0563
Epoch 25/500, trend Loss: 0.0719 | 0.0529
Epoch 26/500, trend Loss: 0.0736 | 0.0608
Epoch 27/500, trend Loss: 0.0698 | 0.0542
Epoch 28/500, trend Loss: 0.0697 | 0.0510
Epoch 29/500, trend Loss: 0.0661 | 0.0511
Epoch 30/500, trend Loss: 0.0648 | 0.0467
Epoch 31/500, trend Loss: 0.0618 | 0.0470
Epoch 32/500, trend Loss: 0.0608 | 0.0448
Epoch 33/500, trend Loss: 0.0591 | 0.0443
Epoch 34/500, trend Loss: 0.0585 | 0.0432
Epoch 35/500, trend Loss: 0.0572 | 0.0428
Epoch 36/500, trend Loss: 0.0564 | 0.0412
Epoch 37/500, trend Loss: 0.0553 | 0.0411
Epoch 38/500, trend Loss: 0.0546 | 0.0400
Epoch 39/500, trend Loss: 0.0539 | 0.0399
Epoch 40/500, trend Loss: 0.0534 | 0.0395
Epoch 41/500, trend Loss: 0.0529 | 0.0395
Epoch 42/500, trend Loss: 0.0527 | 0.0396
Epoch 43/500, trend Loss: 0.0526 | 0.0390
Epoch 44/500, trend Loss: 0.0521 | 0.0386
Epoch 45/500, trend Loss: 0.0515 | 0.0385
Epoch 46/500, trend Loss: 0.0509 | 0.0384
Epoch 47/500, trend Loss: 0.0505 | 0.0382
Epoch 48/500, trend Loss: 0.0502 | 0.0381
Epoch 49/500, trend Loss: 0.0499 | 0.0380
Epoch 50/500, trend Loss: 0.0497 | 0.0380
Epoch 51/500, trend Loss: 0.0494 | 0.0379
Epoch 52/500, trend Loss: 0.0492 | 0.0379
Epoch 53/500, trend Loss: 0.0490 | 0.0379
Epoch 54/500, trend Loss: 0.0488 | 0.0379
Epoch 55/500, trend Loss: 0.0486 | 0.0379
Epoch 56/500, trend Loss: 0.0484 | 0.0378
Epoch 57/500, trend Loss: 0.0482 | 0.0378
Epoch 58/500, trend Loss: 0.0480 | 0.0378
Epoch 59/500, trend Loss: 0.0478 | 0.0377
Epoch 60/500, trend Loss: 0.0477 | 0.0377
Epoch 61/500, trend Loss: 0.0475 | 0.0377
Epoch 62/500, trend Loss: 0.0473 | 0.0376
Epoch 63/500, trend Loss: 0.0471 | 0.0376
Epoch 64/500, trend Loss: 0.0470 | 0.0375
Epoch 65/500, trend Loss: 0.0468 | 0.0375
Epoch 66/500, trend Loss: 0.0467 | 0.0375
Epoch 67/500, trend Loss: 0.0465 | 0.0374
Epoch 68/500, trend Loss: 0.0464 | 0.0374
Epoch 69/500, trend Loss: 0.0462 | 0.0373
Epoch 70/500, trend Loss: 0.0461 | 0.0373
Epoch 71/500, trend Loss: 0.0459 | 0.0372
Epoch 72/500, trend Loss: 0.0458 | 0.0372
Epoch 73/500, trend Loss: 0.0457 | 0.0371
Epoch 74/500, trend Loss: 0.0456 | 0.0371
Epoch 75/500, trend Loss: 0.0455 | 0.0370
Epoch 76/500, trend Loss: 0.0453 | 0.0370
Epoch 77/500, trend Loss: 0.0452 | 0.0369
Epoch 78/500, trend Loss: 0.0451 | 0.0369
Epoch 79/500, trend Loss: 0.0450 | 0.0368
Epoch 80/500, trend Loss: 0.0450 | 0.0368
Epoch 81/500, trend Loss: 0.0449 | 0.0367
Epoch 82/500, trend Loss: 0.0448 | 0.0367
Epoch 83/500, trend Loss: 0.0447 | 0.0367
Epoch 84/500, trend Loss: 0.0446 | 0.0366
Epoch 85/500, trend Loss: 0.0445 | 0.0366
Epoch 86/500, trend Loss: 0.0445 | 0.0365
Epoch 87/500, trend Loss: 0.0444 | 0.0365
Epoch 88/500, trend Loss: 0.0443 | 0.0365
Epoch 89/500, trend Loss: 0.0443 | 0.0364
Epoch 90/500, trend Loss: 0.0442 | 0.0364
Epoch 91/500, trend Loss: 0.0441 | 0.0364
Epoch 92/500, trend Loss: 0.0441 | 0.0363
Epoch 93/500, trend Loss: 0.0440 | 0.0363
Epoch 94/500, trend Loss: 0.0440 | 0.0363
Epoch 95/500, trend Loss: 0.0439 | 0.0362
Epoch 96/500, trend Loss: 0.0439 | 0.0362
Epoch 97/500, trend Loss: 0.0438 | 0.0362
Epoch 98/500, trend Loss: 0.0438 | 0.0361
Epoch 99/500, trend Loss: 0.0437 | 0.0361
Epoch 100/500, trend Loss: 0.0437 | 0.0361
Epoch 101/500, trend Loss: 0.0436 | 0.0361
Epoch 102/500, trend Loss: 0.0436 | 0.0360
Epoch 103/500, trend Loss: 0.0436 | 0.0360
Epoch 104/500, trend Loss: 0.0435 | 0.0360
Epoch 105/500, trend Loss: 0.0435 | 0.0360
Epoch 106/500, trend Loss: 0.0434 | 0.0359
Epoch 107/500, trend Loss: 0.0434 | 0.0359
Epoch 108/500, trend Loss: 0.0434 | 0.0359
Epoch 109/500, trend Loss: 0.0433 | 0.0359
Epoch 110/500, trend Loss: 0.0433 | 0.0359
Epoch 111/500, trend Loss: 0.0433 | 0.0358
Epoch 112/500, trend Loss: 0.0433 | 0.0358
Epoch 113/500, trend Loss: 0.0432 | 0.0358
Epoch 114/500, trend Loss: 0.0432 | 0.0358
Epoch 115/500, trend Loss: 0.0432 | 0.0358
Epoch 116/500, trend Loss: 0.0431 | 0.0358
Epoch 117/500, trend Loss: 0.0431 | 0.0357
Epoch 118/500, trend Loss: 0.0431 | 0.0357
Epoch 119/500, trend Loss: 0.0431 | 0.0357
Epoch 120/500, trend Loss: 0.0431 | 0.0357
Epoch 121/500, trend Loss: 0.0430 | 0.0357
Epoch 122/500, trend Loss: 0.0430 | 0.0357
Epoch 123/500, trend Loss: 0.0430 | 0.0357
Epoch 124/500, trend Loss: 0.0430 | 0.0356
Epoch 125/500, trend Loss: 0.0430 | 0.0356
Epoch 126/500, trend Loss: 0.0429 | 0.0356
Epoch 127/500, trend Loss: 0.0429 | 0.0356
Epoch 128/500, trend Loss: 0.0429 | 0.0356
Epoch 129/500, trend Loss: 0.0429 | 0.0356
Epoch 130/500, trend Loss: 0.0429 | 0.0356
Epoch 131/500, trend Loss: 0.0428 | 0.0356
Epoch 132/500, trend Loss: 0.0428 | 0.0356
Epoch 133/500, trend Loss: 0.0428 | 0.0355
Epoch 134/500, trend Loss: 0.0428 | 0.0355
Epoch 135/500, trend Loss: 0.0428 | 0.0355
Epoch 136/500, trend Loss: 0.0428 | 0.0355
Epoch 137/500, trend Loss: 0.0428 | 0.0355
Epoch 138/500, trend Loss: 0.0428 | 0.0355
Epoch 139/500, trend Loss: 0.0427 | 0.0355
Epoch 140/500, trend Loss: 0.0427 | 0.0355
Epoch 141/500, trend Loss: 0.0427 | 0.0355
Epoch 142/500, trend Loss: 0.0427 | 0.0355
Epoch 143/500, trend Loss: 0.0427 | 0.0355
Epoch 144/500, trend Loss: 0.0427 | 0.0355
Epoch 145/500, trend Loss: 0.0427 | 0.0354
Epoch 146/500, trend Loss: 0.0427 | 0.0354
Epoch 147/500, trend Loss: 0.0427 | 0.0354
Epoch 148/500, trend Loss: 0.0427 | 0.0354
Epoch 149/500, trend Loss: 0.0426 | 0.0354
Epoch 150/500, trend Loss: 0.0426 | 0.0354
Epoch 151/500, trend Loss: 0.0426 | 0.0354
Epoch 152/500, trend Loss: 0.0426 | 0.0354
Epoch 153/500, trend Loss: 0.0426 | 0.0354
Epoch 154/500, trend Loss: 0.0426 | 0.0354
Epoch 155/500, trend Loss: 0.0426 | 0.0354
Epoch 156/500, trend Loss: 0.0426 | 0.0354
Epoch 157/500, trend Loss: 0.0426 | 0.0354
Epoch 158/500, trend Loss: 0.0426 | 0.0354
Epoch 159/500, trend Loss: 0.0426 | 0.0354
Epoch 160/500, trend Loss: 0.0426 | 0.0354
Epoch 161/500, trend Loss: 0.0426 | 0.0354
Epoch 162/500, trend Loss: 0.0426 | 0.0354
Epoch 163/500, trend Loss: 0.0425 | 0.0354
Epoch 164/500, trend Loss: 0.0425 | 0.0354
Epoch 165/500, trend Loss: 0.0425 | 0.0354
Epoch 166/500, trend Loss: 0.0425 | 0.0354
Epoch 167/500, trend Loss: 0.0425 | 0.0353
Epoch 168/500, trend Loss: 0.0425 | 0.0353
Epoch 169/500, trend Loss: 0.0425 | 0.0353
Epoch 170/500, trend Loss: 0.0425 | 0.0353
Epoch 171/500, trend Loss: 0.0425 | 0.0353
Epoch 172/500, trend Loss: 0.0425 | 0.0353
Epoch 173/500, trend Loss: 0.0425 | 0.0353
Epoch 174/500, trend Loss: 0.0425 | 0.0353
Epoch 175/500, trend Loss: 0.0425 | 0.0353
Epoch 176/500, trend Loss: 0.0425 | 0.0353
Epoch 177/500, trend Loss: 0.0425 | 0.0353
Epoch 178/500, trend Loss: 0.0425 | 0.0353
Epoch 179/500, trend Loss: 0.0425 | 0.0353
Epoch 180/500, trend Loss: 0.0425 | 0.0353
Epoch 181/500, trend Loss: 0.0425 | 0.0353
Epoch 182/500, trend Loss: 0.0425 | 0.0353
Epoch 183/500, trend Loss: 0.0425 | 0.0353
Epoch 184/500, trend Loss: 0.0425 | 0.0353
Epoch 185/500, trend Loss: 0.0425 | 0.0353
Epoch 186/500, trend Loss: 0.0425 | 0.0353
Epoch 187/500, trend Loss: 0.0425 | 0.0353
Epoch 188/500, trend Loss: 0.0425 | 0.0353
Epoch 189/500, trend Loss: 0.0425 | 0.0353
Epoch 190/500, trend Loss: 0.0425 | 0.0353
Epoch 191/500, trend Loss: 0.0425 | 0.0353
Epoch 192/500, trend Loss: 0.0424 | 0.0353
Epoch 193/500, trend Loss: 0.0424 | 0.0353
Epoch 194/500, trend Loss: 0.0424 | 0.0353
Epoch 195/500, trend Loss: 0.0424 | 0.0353
Epoch 196/500, trend Loss: 0.0424 | 0.0353
Epoch 197/500, trend Loss: 0.0424 | 0.0353
Epoch 198/500, trend Loss: 0.0424 | 0.0353
Epoch 199/500, trend Loss: 0.0424 | 0.0353
Epoch 200/500, trend Loss: 0.0424 | 0.0353
Epoch 201/500, trend Loss: 0.0424 | 0.0353
Epoch 202/500, trend Loss: 0.0424 | 0.0353
Epoch 203/500, trend Loss: 0.0424 | 0.0353
Epoch 204/500, trend Loss: 0.0424 | 0.0353
Epoch 205/500, trend Loss: 0.0424 | 0.0353
Epoch 206/500, trend Loss: 0.0424 | 0.0353
Epoch 207/500, trend Loss: 0.0424 | 0.0353
Epoch 208/500, trend Loss: 0.0424 | 0.0353
Epoch 209/500, trend Loss: 0.0424 | 0.0353
Epoch 210/500, trend Loss: 0.0424 | 0.0353
Epoch 211/500, trend Loss: 0.0424 | 0.0353
Epoch 212/500, trend Loss: 0.0424 | 0.0353
Epoch 213/500, trend Loss: 0.0424 | 0.0353
Epoch 214/500, trend Loss: 0.0424 | 0.0353
Epoch 215/500, trend Loss: 0.0424 | 0.0353
Epoch 216/500, trend Loss: 0.0424 | 0.0353
Epoch 217/500, trend Loss: 0.0424 | 0.0353
Epoch 218/500, trend Loss: 0.0424 | 0.0353
Epoch 219/500, trend Loss: 0.0424 | 0.0353
Epoch 220/500, trend Loss: 0.0424 | 0.0353
Epoch 221/500, trend Loss: 0.0424 | 0.0353
Epoch 222/500, trend Loss: 0.0424 | 0.0353
Epoch 223/500, trend Loss: 0.0424 | 0.0353
Epoch 224/500, trend Loss: 0.0424 | 0.0353
Epoch 225/500, trend Loss: 0.0424 | 0.0353
Epoch 226/500, trend Loss: 0.0424 | 0.0353
Epoch 227/500, trend Loss: 0.0424 | 0.0353
Epoch 228/500, trend Loss: 0.0424 | 0.0353
Epoch 229/500, trend Loss: 0.0424 | 0.0353
Epoch 230/500, trend Loss: 0.0424 | 0.0353
Epoch 231/500, trend Loss: 0.0424 | 0.0353
Epoch 232/500, trend Loss: 0.0424 | 0.0353
Epoch 233/500, trend Loss: 0.0424 | 0.0353
Epoch 234/500, trend Loss: 0.0424 | 0.0353
Epoch 235/500, trend Loss: 0.0424 | 0.0353
Epoch 236/500, trend Loss: 0.0424 | 0.0353
Epoch 237/500, trend Loss: 0.0424 | 0.0353
Epoch 238/500, trend Loss: 0.0424 | 0.0353
Epoch 239/500, trend Loss: 0.0424 | 0.0353
Epoch 240/500, trend Loss: 0.0424 | 0.0353
Epoch 241/500, trend Loss: 0.0424 | 0.0353
Epoch 242/500, trend Loss: 0.0424 | 0.0353
Epoch 243/500, trend Loss: 0.0424 | 0.0353
Epoch 244/500, trend Loss: 0.0424 | 0.0353
Epoch 245/500, trend Loss: 0.0424 | 0.0353
Epoch 246/500, trend Loss: 0.0424 | 0.0353
Epoch 247/500, trend Loss: 0.0424 | 0.0353
Epoch 248/500, trend Loss: 0.0424 | 0.0353
Epoch 249/500, trend Loss: 0.0424 | 0.0353
Epoch 250/500, trend Loss: 0.0424 | 0.0353
Epoch 251/500, trend Loss: 0.0424 | 0.0353
Epoch 252/500, trend Loss: 0.0424 | 0.0353
Epoch 253/500, trend Loss: 0.0424 | 0.0353
Epoch 254/500, trend Loss: 0.0424 | 0.0353
Epoch 255/500, trend Loss: 0.0424 | 0.0353
Epoch 256/500, trend Loss: 0.0424 | 0.0353
Epoch 257/500, trend Loss: 0.0424 | 0.0353
Epoch 258/500, trend Loss: 0.0424 | 0.0353
Epoch 259/500, trend Loss: 0.0424 | 0.0353
Epoch 260/500, trend Loss: 0.0424 | 0.0353
Epoch 261/500, trend Loss: 0.0424 | 0.0353
Epoch 262/500, trend Loss: 0.0424 | 0.0353
Epoch 263/500, trend Loss: 0.0424 | 0.0353
Epoch 264/500, trend Loss: 0.0424 | 0.0353
Epoch 265/500, trend Loss: 0.0424 | 0.0353
Epoch 266/500, trend Loss: 0.0424 | 0.0353
Epoch 267/500, trend Loss: 0.0424 | 0.0353
Epoch 268/500, trend Loss: 0.0424 | 0.0353
Epoch 269/500, trend Loss: 0.0424 | 0.0353
Epoch 270/500, trend Loss: 0.0424 | 0.0353
Epoch 271/500, trend Loss: 0.0424 | 0.0353
Epoch 272/500, trend Loss: 0.0424 | 0.0353
Epoch 273/500, trend Loss: 0.0424 | 0.0353
Epoch 274/500, trend Loss: 0.0424 | 0.0353
Epoch 275/500, trend Loss: 0.0424 | 0.0353
Epoch 276/500, trend Loss: 0.0424 | 0.0353
Epoch 277/500, trend Loss: 0.0424 | 0.0353
Epoch 278/500, trend Loss: 0.0424 | 0.0353
Epoch 279/500, trend Loss: 0.0424 | 0.0353
Epoch 280/500, trend Loss: 0.0424 | 0.0353
Epoch 281/500, trend Loss: 0.0424 | 0.0353
Epoch 282/500, trend Loss: 0.0424 | 0.0353
Epoch 283/500, trend Loss: 0.0424 | 0.0353
Epoch 284/500, trend Loss: 0.0424 | 0.0353
Epoch 285/500, trend Loss: 0.0424 | 0.0353
Epoch 286/500, trend Loss: 0.0424 | 0.0353
Epoch 287/500, trend Loss: 0.0424 | 0.0353
Epoch 288/500, trend Loss: 0.0424 | 0.0353
Epoch 289/500, trend Loss: 0.0424 | 0.0353
Epoch 290/500, trend Loss: 0.0424 | 0.0353
Epoch 291/500, trend Loss: 0.0424 | 0.0353
Epoch 292/500, trend Loss: 0.0424 | 0.0353
Epoch 293/500, trend Loss: 0.0424 | 0.0353
Epoch 294/500, trend Loss: 0.0424 | 0.0353
Epoch 295/500, trend Loss: 0.0424 | 0.0353
Epoch 296/500, trend Loss: 0.0424 | 0.0353
Epoch 297/500, trend Loss: 0.0424 | 0.0353
Epoch 298/500, trend Loss: 0.0424 | 0.0353
Epoch 299/500, trend Loss: 0.0424 | 0.0353
Epoch 300/500, trend Loss: 0.0424 | 0.0353
Epoch 301/500, trend Loss: 0.0424 | 0.0353
Epoch 302/500, trend Loss: 0.0424 | 0.0353
Epoch 303/500, trend Loss: 0.0424 | 0.0353
Epoch 304/500, trend Loss: 0.0424 | 0.0353
Epoch 305/500, trend Loss: 0.0424 | 0.0353
Epoch 306/500, trend Loss: 0.0424 | 0.0353
Epoch 307/500, trend Loss: 0.0424 | 0.0353
Epoch 308/500, trend Loss: 0.0424 | 0.0353
Epoch 309/500, trend Loss: 0.0424 | 0.0353
Epoch 310/500, trend Loss: 0.0424 | 0.0353
Epoch 311/500, trend Loss: 0.0424 | 0.0353
Epoch 312/500, trend Loss: 0.0424 | 0.0353
Epoch 313/500, trend Loss: 0.0424 | 0.0353
Epoch 314/500, trend Loss: 0.0424 | 0.0353
Epoch 315/500, trend Loss: 0.0424 | 0.0353
Epoch 316/500, trend Loss: 0.0424 | 0.0353
Epoch 317/500, trend Loss: 0.0424 | 0.0353
Epoch 318/500, trend Loss: 0.0424 | 0.0353
Epoch 319/500, trend Loss: 0.0424 | 0.0353
Epoch 320/500, trend Loss: 0.0424 | 0.0353
Epoch 321/500, trend Loss: 0.0424 | 0.0353
Epoch 322/500, trend Loss: 0.0424 | 0.0353
Early stopping for trend
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.
  self.gen = func(*args, **kwds)
Epoch 1/500, seasonal_0 Loss: 0.3120 | 0.1972
Epoch 2/500, seasonal_0 Loss: 0.1464 | 0.1405
Epoch 3/500, seasonal_0 Loss: 0.1188 | 0.1169
Epoch 4/500, seasonal_0 Loss: 0.1043 | 0.0987
Epoch 5/500, seasonal_0 Loss: 0.0963 | 0.0860
Epoch 6/500, seasonal_0 Loss: 0.0920 | 0.0789
Epoch 7/500, seasonal_0 Loss: 0.0890 | 0.0752
Epoch 8/500, seasonal_0 Loss: 0.0863 | 0.0716
Epoch 9/500, seasonal_0 Loss: 0.0838 | 0.0687
Epoch 10/500, seasonal_0 Loss: 0.0815 | 0.0670
Epoch 11/500, seasonal_0 Loss: 0.0794 | 0.0647
Epoch 12/500, seasonal_0 Loss: 0.0776 | 0.0630
Epoch 13/500, seasonal_0 Loss: 0.0759 | 0.0619
Epoch 14/500, seasonal_0 Loss: 0.0744 | 0.0606
Epoch 15/500, seasonal_0 Loss: 0.0730 | 0.0598
Epoch 16/500, seasonal_0 Loss: 0.0718 | 0.0590
Epoch 17/500, seasonal_0 Loss: 0.0707 | 0.0584
Epoch 18/500, seasonal_0 Loss: 0.0698 | 0.0581
Epoch 19/500, seasonal_0 Loss: 0.0689 | 0.0573
Epoch 20/500, seasonal_0 Loss: 0.0683 | 0.0572
Epoch 21/500, seasonal_0 Loss: 0.0678 | 0.0571
Epoch 22/500, seasonal_0 Loss: 0.0675 | 0.0569
Epoch 23/500, seasonal_0 Loss: 0.0674 | 0.0581
Epoch 24/500, seasonal_0 Loss: 0.0672 | 0.0598
Epoch 25/500, seasonal_0 Loss: 0.0669 | 0.0643
Epoch 26/500, seasonal_0 Loss: 0.0664 | 0.0647
Epoch 27/500, seasonal_0 Loss: 0.0654 | 0.0636
Epoch 28/500, seasonal_0 Loss: 0.0642 | 0.0638
Epoch 29/500, seasonal_0 Loss: 0.0632 | 0.0612
Epoch 30/500, seasonal_0 Loss: 0.0621 | 0.0594
Epoch 31/500, seasonal_0 Loss: 0.0612 | 0.0589
Epoch 32/500, seasonal_0 Loss: 0.0604 | 0.0570
Epoch 33/500, seasonal_0 Loss: 0.0595 | 0.0558
Epoch 34/500, seasonal_0 Loss: 0.0587 | 0.0553
Epoch 35/500, seasonal_0 Loss: 0.0581 | 0.0540
Epoch 36/500, seasonal_0 Loss: 0.0574 | 0.0530
Epoch 37/500, seasonal_0 Loss: 0.0567 | 0.0526
Epoch 38/500, seasonal_0 Loss: 0.0561 | 0.0516
Epoch 39/500, seasonal_0 Loss: 0.0555 | 0.0508
Epoch 40/500, seasonal_0 Loss: 0.0549 | 0.0505
Epoch 41/500, seasonal_0 Loss: 0.0544 | 0.0496
Epoch 42/500, seasonal_0 Loss: 0.0539 | 0.0490
Epoch 43/500, seasonal_0 Loss: 0.0533 | 0.0488
Epoch 44/500, seasonal_0 Loss: 0.0529 | 0.0480
Epoch 45/500, seasonal_0 Loss: 0.0524 | 0.0475
Epoch 46/500, seasonal_0 Loss: 0.0520 | 0.0474
Epoch 47/500, seasonal_0 Loss: 0.0516 | 0.0467
Epoch 48/500, seasonal_0 Loss: 0.0512 | 0.0462
Epoch 49/500, seasonal_0 Loss: 0.0508 | 0.0462
Epoch 50/500, seasonal_0 Loss: 0.0504 | 0.0456
Epoch 51/500, seasonal_0 Loss: 0.0501 | 0.0452
Epoch 52/500, seasonal_0 Loss: 0.0497 | 0.0452
Epoch 53/500, seasonal_0 Loss: 0.0494 | 0.0447
Epoch 54/500, seasonal_0 Loss: 0.0491 | 0.0443
Epoch 55/500, seasonal_0 Loss: 0.0488 | 0.0443
Epoch 56/500, seasonal_0 Loss: 0.0485 | 0.0438
Epoch 57/500, seasonal_0 Loss: 0.0483 | 0.0435
Epoch 58/500, seasonal_0 Loss: 0.0480 | 0.0435
Epoch 59/500, seasonal_0 Loss: 0.0477 | 0.0431
Epoch 60/500, seasonal_0 Loss: 0.0475 | 0.0428
Epoch 61/500, seasonal_0 Loss: 0.0473 | 0.0427
Epoch 62/500, seasonal_0 Loss: 0.0471 | 0.0424
Epoch 63/500, seasonal_0 Loss: 0.0469 | 0.0422
Epoch 64/500, seasonal_0 Loss: 0.0467 | 0.0421
Epoch 65/500, seasonal_0 Loss: 0.0465 | 0.0418
Epoch 66/500, seasonal_0 Loss: 0.0463 | 0.0416
Epoch 67/500, seasonal_0 Loss: 0.0461 | 0.0414
Epoch 68/500, seasonal_0 Loss: 0.0459 | 0.0412
Epoch 69/500, seasonal_0 Loss: 0.0458 | 0.0410
Epoch 70/500, seasonal_0 Loss: 0.0456 | 0.0408
Epoch 71/500, seasonal_0 Loss: 0.0454 | 0.0406
Epoch 72/500, seasonal_0 Loss: 0.0453 | 0.0405
Epoch 73/500, seasonal_0 Loss: 0.0451 | 0.0403
Epoch 74/500, seasonal_0 Loss: 0.0450 | 0.0401
Epoch 75/500, seasonal_0 Loss: 0.0448 | 0.0400
Epoch 76/500, seasonal_0 Loss: 0.0447 | 0.0398
Epoch 77/500, seasonal_0 Loss: 0.0446 | 0.0397
Epoch 78/500, seasonal_0 Loss: 0.0444 | 0.0396
Epoch 79/500, seasonal_0 Loss: 0.0443 | 0.0394
Epoch 80/500, seasonal_0 Loss: 0.0442 | 0.0393
Epoch 81/500, seasonal_0 Loss: 0.0440 | 0.0392
Epoch 82/500, seasonal_0 Loss: 0.0439 | 0.0391
Epoch 83/500, seasonal_0 Loss: 0.0438 | 0.0390
Epoch 84/500, seasonal_0 Loss: 0.0437 | 0.0390
Epoch 85/500, seasonal_0 Loss: 0.0436 | 0.0389
Epoch 86/500, seasonal_0 Loss: 0.0435 | 0.0388
Epoch 87/500, seasonal_0 Loss: 0.0434 | 0.0387
Epoch 88/500, seasonal_0 Loss: 0.0433 | 0.0387
Epoch 89/500, seasonal_0 Loss: 0.0432 | 0.0386
Epoch 90/500, seasonal_0 Loss: 0.0431 | 0.0385
Epoch 91/500, seasonal_0 Loss: 0.0430 | 0.0385
Epoch 92/500, seasonal_0 Loss: 0.0429 | 0.0385
Epoch 93/500, seasonal_0 Loss: 0.0428 | 0.0384
Epoch 94/500, seasonal_0 Loss: 0.0427 | 0.0384
Epoch 95/500, seasonal_0 Loss: 0.0427 | 0.0383
Epoch 96/500, seasonal_0 Loss: 0.0426 | 0.0383
Epoch 97/500, seasonal_0 Loss: 0.0425 | 0.0383
Epoch 98/500, seasonal_0 Loss: 0.0424 | 0.0383
Epoch 99/500, seasonal_0 Loss: 0.0424 | 0.0382
Epoch 100/500, seasonal_0 Loss: 0.0423 | 0.0382
Epoch 101/500, seasonal_0 Loss: 0.0422 | 0.0382
Epoch 102/500, seasonal_0 Loss: 0.0421 | 0.0382
Epoch 103/500, seasonal_0 Loss: 0.0420 | 0.0382
Epoch 104/500, seasonal_0 Loss: 0.0420 | 0.0381
Epoch 105/500, seasonal_0 Loss: 0.0418 | 0.0380
Epoch 106/500, seasonal_0 Loss: 0.0418 | 0.0380
Epoch 107/500, seasonal_0 Loss: 0.0417 | 0.0379
Epoch 108/500, seasonal_0 Loss: 0.0416 | 0.0379
Epoch 109/500, seasonal_0 Loss: 0.0415 | 0.0378
Epoch 110/500, seasonal_0 Loss: 0.0414 | 0.0377
Epoch 111/500, seasonal_0 Loss: 0.0413 | 0.0377
Epoch 112/500, seasonal_0 Loss: 0.0412 | 0.0376
Epoch 113/500, seasonal_0 Loss: 0.0411 | 0.0375
Epoch 114/500, seasonal_0 Loss: 0.0411 | 0.0375
Epoch 115/500, seasonal_0 Loss: 0.0410 | 0.0374
Epoch 116/500, seasonal_0 Loss: 0.0409 | 0.0374
Epoch 117/500, seasonal_0 Loss: 0.0408 | 0.0373
Epoch 118/500, seasonal_0 Loss: 0.0408 | 0.0373
Epoch 119/500, seasonal_0 Loss: 0.0407 | 0.0372
Epoch 120/500, seasonal_0 Loss: 0.0406 | 0.0372
Epoch 121/500, seasonal_0 Loss: 0.0406 | 0.0372
Epoch 122/500, seasonal_0 Loss: 0.0405 | 0.0372
Epoch 123/500, seasonal_0 Loss: 0.0404 | 0.0371
Epoch 124/500, seasonal_0 Loss: 0.0404 | 0.0371
Epoch 125/500, seasonal_0 Loss: 0.0403 | 0.0371
Epoch 126/500, seasonal_0 Loss: 0.0402 | 0.0371
Epoch 127/500, seasonal_0 Loss: 0.0402 | 0.0371
Epoch 128/500, seasonal_0 Loss: 0.0401 | 0.0370
Epoch 129/500, seasonal_0 Loss: 0.0401 | 0.0370
Epoch 130/500, seasonal_0 Loss: 0.0400 | 0.0370
Epoch 131/500, seasonal_0 Loss: 0.0400 | 0.0370
Epoch 132/500, seasonal_0 Loss: 0.0399 | 0.0370
Epoch 133/500, seasonal_0 Loss: 0.0399 | 0.0370
Epoch 134/500, seasonal_0 Loss: 0.0398 | 0.0369
Epoch 135/500, seasonal_0 Loss: 0.0398 | 0.0369
Epoch 136/500, seasonal_0 Loss: 0.0397 | 0.0369
Epoch 137/500, seasonal_0 Loss: 0.0397 | 0.0369
Epoch 138/500, seasonal_0 Loss: 0.0396 | 0.0368
Traceback (most recent call last):
  File "/data/student/k2110261/Multi-iTransformer/main.py", line 1037, in <module>
    models[comp], train_loss, valid_loss = train(model, train_data, valid_data, optimizer, criterion, scheduler, params['batch_size'], params['observation_period_num'])
                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/train.py", line 32, in train
    output = model(data)
             ^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<@beartype(src.model.iTransformer.forward) at 0x7f1aecad79c0>", line 66, in forward
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 188, in forward
    x = attn(x) + x
        ^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/student/k2110261/Multi-iTransformer/src/model.py", line 69, in forward
    out = out * self.to_v_gates(x)
                ^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/einops/layers/torch.py", line 15, in forward
    return apply_for_scriptable_torch(recipe, input, reduction_type="rearrange", axes_dims=self._axes_lengths)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/einops/_torch_specific.py", line 98, in apply_for_scriptable_torch
    tensor = backend.reshape(tensor, final_shapes)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/einops/_torch_specific.py", line 73, in reshape
    return x.reshape(shape)
           ^^^^^^^^^^^^^^^^
KeyboardInterrupt
