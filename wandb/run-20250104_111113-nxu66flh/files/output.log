ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-04 11:11:14,574][0m A new study created in memory with name: no-name-d525940b-1b64-4c46-8925-0903dacc1667[0m
[32m[I 2025-01-04 11:13:08,601][0m Trial 0 finished with value: 0.07471486600771757 and parameters: {'observation_period_num': 58, 'train_rates': 0.9266343072114498, 'learning_rate': 5.315295051838874e-06, 'batch_size': 50, 'step_size': 11, 'gamma': 0.9737468624934725}. Best is trial 0 with value: 0.07471486600771757.[0m
[32m[I 2025-01-04 11:13:56,912][0m Trial 1 finished with value: 0.16893932939241718 and parameters: {'observation_period_num': 7, 'train_rates': 0.8787014899700415, 'learning_rate': 2.601228084544738e-06, 'batch_size': 120, 'step_size': 4, 'gamma': 0.8445854933266819}. Best is trial 0 with value: 0.07471486600771757.[0m
[32m[I 2025-01-04 11:14:35,341][0m Trial 2 finished with value: 0.2948367591227991 and parameters: {'observation_period_num': 174, 'train_rates': 0.7708079392217526, 'learning_rate': 2.7374094548116406e-05, 'batch_size': 142, 'step_size': 10, 'gamma': 0.795729325887482}. Best is trial 0 with value: 0.07471486600771757.[0m
[32m[I 2025-01-04 11:15:09,792][0m Trial 3 finished with value: 0.1372663974761963 and parameters: {'observation_period_num': 212, 'train_rates': 0.9465035465212508, 'learning_rate': 3.665962181849801e-05, 'batch_size': 189, 'step_size': 11, 'gamma': 0.8706662337254153}. Best is trial 0 with value: 0.07471486600771757.[0m
[32m[I 2025-01-04 11:15:38,368][0m Trial 4 finished with value: 0.48314298680946655 and parameters: {'observation_period_num': 24, 'train_rates': 0.6976538425330363, 'learning_rate': 2.0193772461631757e-06, 'batch_size': 206, 'step_size': 14, 'gamma': 0.7998485374430926}. Best is trial 0 with value: 0.07471486600771757.[0m
[32m[I 2025-01-04 11:16:42,795][0m Trial 5 finished with value: 0.21151617336106468 and parameters: {'observation_period_num': 94, 'train_rates': 0.6998133173950378, 'learning_rate': 0.0005793721721566871, 'batch_size': 72, 'step_size': 13, 'gamma': 0.7768725046869508}. Best is trial 0 with value: 0.07471486600771757.[0m
[32m[I 2025-01-04 11:18:08,146][0m Trial 6 finished with value: 0.060311145682067156 and parameters: {'observation_period_num': 88, 'train_rates': 0.8043824754641462, 'learning_rate': 0.00031664659807984023, 'batch_size': 60, 'step_size': 4, 'gamma': 0.8340273976861862}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:18:34,402][0m Trial 7 finished with value: 0.9146149311651731 and parameters: {'observation_period_num': 208, 'train_rates': 0.608635858755264, 'learning_rate': 1.2956099980251274e-06, 'batch_size': 200, 'step_size': 12, 'gamma': 0.7975802708275719}. Best is trial 6 with value: 0.060311145682067156.[0m
Early stopping at epoch 51
[32m[I 2025-01-04 11:18:57,975][0m Trial 8 finished with value: 1.7270531938651066 and parameters: {'observation_period_num': 207, 'train_rates': 0.8658374841129214, 'learning_rate': 1.2413503548440582e-06, 'batch_size': 119, 'step_size': 1, 'gamma': 0.8006055794988661}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:23:17,953][0m Trial 9 finished with value: 0.34333191611143515 and parameters: {'observation_period_num': 242, 'train_rates': 0.6852460976178357, 'learning_rate': 7.135622951037244e-06, 'batch_size': 16, 'step_size': 5, 'gamma': 0.9571732476502387}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:24:28,062][0m Trial 10 finished with value: 0.11720149557698857 and parameters: {'observation_period_num': 131, 'train_rates': 0.805081220299226, 'learning_rate': 0.0008022186486842329, 'batch_size': 73, 'step_size': 7, 'gamma': 0.9037479900120957}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:28:03,722][0m Trial 11 finished with value: 0.09156669117510319 and parameters: {'observation_period_num': 78, 'train_rates': 0.9863176705343825, 'learning_rate': 0.0001750054781913918, 'batch_size': 27, 'step_size': 9, 'gamma': 0.988536267842687}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:29:25,726][0m Trial 12 finished with value: 0.1475332392186954 and parameters: {'observation_period_num': 59, 'train_rates': 0.9000451241966121, 'learning_rate': 1.4907682294443617e-05, 'batch_size': 70, 'step_size': 1, 'gamma': 0.9147053250228978}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:31:10,943][0m Trial 13 finished with value: 0.08513075200950398 and parameters: {'observation_period_num': 123, 'train_rates': 0.8079001275206042, 'learning_rate': 0.00017642585619369867, 'batch_size': 48, 'step_size': 7, 'gamma': 0.8443969177411274}. Best is trial 6 with value: 0.060311145682067156.[0m
[32m[I 2025-01-04 11:32:11,275][0m Trial 14 finished with value: 0.043957726906914754 and parameters: {'observation_period_num': 47, 'train_rates': 0.927284387648941, 'learning_rate': 0.00010733243346411026, 'batch_size': 97, 'step_size': 4, 'gamma': 0.9238694632236193}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:32:48,956][0m Trial 15 finished with value: 0.06077391270841106 and parameters: {'observation_period_num': 114, 'train_rates': 0.8431603443474315, 'learning_rate': 0.00010684820710195623, 'batch_size': 156, 'step_size': 4, 'gamma': 0.9241685837854616}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:33:17,714][0m Trial 16 finished with value: 0.17342570728787018 and parameters: {'observation_period_num': 40, 'train_rates': 0.7399379477003607, 'learning_rate': 0.00035170225515308753, 'batch_size': 256, 'step_size': 5, 'gamma': 0.8749674120620796}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:34:15,302][0m Trial 17 finished with value: 0.19234365224838257 and parameters: {'observation_period_num': 155, 'train_rates': 0.9709608699583043, 'learning_rate': 5.395396675590384e-05, 'batch_size': 102, 'step_size': 3, 'gamma': 0.7536387817191899}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:35:12,955][0m Trial 18 finished with value: 0.0585256863736251 and parameters: {'observation_period_num': 89, 'train_rates': 0.844364249947814, 'learning_rate': 8.402019280755987e-05, 'batch_size': 95, 'step_size': 7, 'gamma': 0.8317152456024131}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:36:19,010][0m Trial 19 finished with value: 0.0504028708395046 and parameters: {'observation_period_num': 51, 'train_rates': 0.9252760051872384, 'learning_rate': 7.806765388816108e-05, 'batch_size': 88, 'step_size': 7, 'gamma': 0.9435493308749676}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:36:56,682][0m Trial 20 finished with value: 0.05918616024188665 and parameters: {'observation_period_num': 37, 'train_rates': 0.9209507646316856, 'learning_rate': 1.943349668341829e-05, 'batch_size': 170, 'step_size': 8, 'gamma': 0.9381508631613075}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:37:54,434][0m Trial 21 finished with value: 0.05027437632410263 and parameters: {'observation_period_num': 65, 'train_rates': 0.8653878487700154, 'learning_rate': 6.375141104799154e-05, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8888778181329247}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:38:55,459][0m Trial 22 finished with value: 0.04893669873476028 and parameters: {'observation_period_num': 46, 'train_rates': 0.8969689424734375, 'learning_rate': 5.921460633540178e-05, 'batch_size': 96, 'step_size': 6, 'gamma': 0.8916696893764365}. Best is trial 14 with value: 0.043957726906914754.[0m
[32m[I 2025-01-04 11:39:44,365][0m Trial 23 finished with value: 0.029384422251396435 and parameters: {'observation_period_num': 9, 'train_rates': 0.8855816978801135, 'learning_rate': 0.00016551987324123525, 'batch_size': 120, 'step_size': 6, 'gamma': 0.8935663624658816}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:40:34,680][0m Trial 24 finished with value: 0.04810692869402744 and parameters: {'observation_period_num': 22, 'train_rates': 0.9539850501353302, 'learning_rate': 0.00017362272174740358, 'batch_size': 124, 'step_size': 2, 'gamma': 0.8916378042592906}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:41:24,168][0m Trial 25 finished with value: 0.041115564000729014 and parameters: {'observation_period_num': 11, 'train_rates': 0.9505692007832978, 'learning_rate': 0.00015492903662489636, 'batch_size': 125, 'step_size': 2, 'gamma': 0.9216652491125923}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:42:09,592][0m Trial 26 finished with value: 0.040237098932266235 and parameters: {'observation_period_num': 13, 'train_rates': 0.9850175564114874, 'learning_rate': 0.0001274026663614012, 'batch_size': 144, 'step_size': 2, 'gamma': 0.9274745050574046}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:42:54,272][0m Trial 27 finished with value: 0.0398421511054039 and parameters: {'observation_period_num': 5, 'train_rates': 0.9865347278841917, 'learning_rate': 0.00032384083031096456, 'batch_size': 147, 'step_size': 2, 'gamma': 0.9579238939799886}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:43:34,413][0m Trial 28 finished with value: 0.03243015706539154 and parameters: {'observation_period_num': 25, 'train_rates': 0.9842147875805229, 'learning_rate': 0.0003288395222724416, 'batch_size': 166, 'step_size': 2, 'gamma': 0.9589853477918778}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:44:14,304][0m Trial 29 finished with value: 0.04642763361334801 and parameters: {'observation_period_num': 26, 'train_rates': 0.9617195871137683, 'learning_rate': 0.000351268989199901, 'batch_size': 171, 'step_size': 3, 'gamma': 0.9625212554227291}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:44:48,271][0m Trial 30 finished with value: 0.03445909477854043 and parameters: {'observation_period_num': 5, 'train_rates': 0.9093801391763616, 'learning_rate': 0.000606565275828397, 'batch_size': 230, 'step_size': 1, 'gamma': 0.9871838649850211}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:45:22,852][0m Trial 31 finished with value: 0.04699492081999779 and parameters: {'observation_period_num': 30, 'train_rates': 0.9897286120618715, 'learning_rate': 0.0005726398400441186, 'batch_size': 243, 'step_size': 1, 'gamma': 0.9880163251909404}. Best is trial 23 with value: 0.029384422251396435.[0m
[32m[I 2025-01-04 11:45:56,350][0m Trial 32 finished with value: 0.02769856168100467 and parameters: {'observation_period_num': 6, 'train_rates': 0.8897984827361468, 'learning_rate': 0.0009598979144146301, 'batch_size': 229, 'step_size': 3, 'gamma': 0.9687724715675567}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:46:29,997][0m Trial 33 finished with value: 0.02811841620132327 and parameters: {'observation_period_num': 5, 'train_rates': 0.8982260182891817, 'learning_rate': 0.0009892934307219425, 'batch_size': 225, 'step_size': 3, 'gamma': 0.9717198405773323}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:47:02,333][0m Trial 34 finished with value: 0.12434055059516834 and parameters: {'observation_period_num': 68, 'train_rates': 0.8362940889066226, 'learning_rate': 0.00083630100947089, 'batch_size': 214, 'step_size': 3, 'gamma': 0.9733760406883676}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:47:34,868][0m Trial 35 finished with value: 0.03965886285332194 and parameters: {'observation_period_num': 21, 'train_rates': 0.8805434672620644, 'learning_rate': 0.0009149040716389075, 'batch_size': 223, 'step_size': 5, 'gamma': 0.9474116044253705}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:48:08,069][0m Trial 36 finished with value: 0.18946831324461624 and parameters: {'observation_period_num': 34, 'train_rates': 0.778498025972411, 'learning_rate': 0.0004363126817880908, 'batch_size': 186, 'step_size': 3, 'gamma': 0.9685708032040817}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:48:43,025][0m Trial 37 finished with value: 0.030445533650724783 and parameters: {'observation_period_num': 16, 'train_rates': 0.8826808837490473, 'learning_rate': 0.0002444856713929559, 'batch_size': 193, 'step_size': 10, 'gamma': 0.9755361356921474}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:49:15,406][0m Trial 38 finished with value: 0.05833469019578316 and parameters: {'observation_period_num': 16, 'train_rates': 0.8811174161735569, 'learning_rate': 0.00022496951396844668, 'batch_size': 236, 'step_size': 9, 'gamma': 0.9778938122367525}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:49:47,332][0m Trial 39 finished with value: 0.10666860613716367 and parameters: {'observation_period_num': 180, 'train_rates': 0.8264579367956764, 'learning_rate': 0.00022984903914684087, 'batch_size': 207, 'step_size': 11, 'gamma': 0.861231760661971}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:50:24,031][0m Trial 40 finished with value: 0.08407243341207504 and parameters: {'observation_period_num': 77, 'train_rates': 0.9369673256592103, 'learning_rate': 3.605082903073703e-05, 'batch_size': 192, 'step_size': 10, 'gamma': 0.9404937669067406}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:51:00,339][0m Trial 41 finished with value: 0.043742256807556223 and parameters: {'observation_period_num': 30, 'train_rates': 0.8622879609680258, 'learning_rate': 0.000493110980595041, 'batch_size': 176, 'step_size': 12, 'gamma': 0.9546657849717777}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:51:34,827][0m Trial 42 finished with value: 0.03228714690787559 and parameters: {'observation_period_num': 5, 'train_rates': 0.8927275168611414, 'learning_rate': 0.0006860112584654301, 'batch_size': 218, 'step_size': 10, 'gamma': 0.9747269572553591}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:52:07,623][0m Trial 43 finished with value: 0.035536251593226445 and parameters: {'observation_period_num': 5, 'train_rates': 0.8939954066021729, 'learning_rate': 0.0007938881887813602, 'batch_size': 248, 'step_size': 9, 'gamma': 0.9760110444263826}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:52:39,992][0m Trial 44 finished with value: 0.03402356308852804 and parameters: {'observation_period_num': 17, 'train_rates': 0.8618367303719459, 'learning_rate': 0.000646801284086552, 'batch_size': 219, 'step_size': 14, 'gamma': 0.9705452006345238}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:53:14,008][0m Trial 45 finished with value: 0.054410285711514225 and parameters: {'observation_period_num': 54, 'train_rates': 0.9090739571685766, 'learning_rate': 0.0009839122970349444, 'batch_size': 206, 'step_size': 10, 'gamma': 0.9066823950252603}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:53:47,755][0m Trial 46 finished with value: 0.05531638946670752 and parameters: {'observation_period_num': 42, 'train_rates': 0.8838481201250739, 'learning_rate': 0.0002431751247452293, 'batch_size': 231, 'step_size': 12, 'gamma': 0.9494228990330066}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:54:20,680][0m Trial 47 finished with value: 0.03740132096059182 and parameters: {'observation_period_num': 16, 'train_rates': 0.8262856005347391, 'learning_rate': 0.0004751382549857343, 'batch_size': 196, 'step_size': 8, 'gamma': 0.9341332890554861}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:54:44,773][0m Trial 48 finished with value: 0.7258710496889681 and parameters: {'observation_period_num': 244, 'train_rates': 0.6283474713943689, 'learning_rate': 3.644583420635409e-06, 'batch_size': 255, 'step_size': 11, 'gamma': 0.8103663445816782}. Best is trial 32 with value: 0.02769856168100467.[0m
[32m[I 2025-01-04 11:55:16,941][0m Trial 49 finished with value: 0.06634229227865278 and parameters: {'observation_period_num': 34, 'train_rates': 0.8551582681881561, 'learning_rate': 0.0006567947875327721, 'batch_size': 218, 'step_size': 15, 'gamma': 0.9817321382748716}. Best is trial 32 with value: 0.02769856168100467.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-04 11:55:16,951][0m A new study created in memory with name: no-name-d3b5d348-4420-4b4d-acd3-c64e16ac8522[0m
[32m[I 2025-01-04 12:00:14,450][0m Trial 0 finished with value: 2.062585230160477 and parameters: {'observation_period_num': 71, 'train_rates': 0.9540994110964742, 'learning_rate': 0.0009301167534156319, 'batch_size': 110, 'step_size': 2, 'gamma': 0.8949830973808154}. Best is trial 0 with value: 2.062585230160477.[0m
Early stopping at epoch 90
[32m[I 2025-01-04 12:03:57,687][0m Trial 1 finished with value: 0.6619656953837846 and parameters: {'observation_period_num': 190, 'train_rates': 0.7693653610363227, 'learning_rate': 2.7945621131473685e-06, 'batch_size': 119, 'step_size': 1, 'gamma': 0.8843550880995561}. Best is trial 1 with value: 0.6619656953837846.[0m
[32m[I 2025-01-04 12:07:17,541][0m Trial 2 finished with value: 0.15796536044882878 and parameters: {'observation_period_num': 214, 'train_rates': 0.795488817064634, 'learning_rate': 0.0002748366554752146, 'batch_size': 219, 'step_size': 3, 'gamma': 0.8050839877271162}. Best is trial 2 with value: 0.15796536044882878.[0m
[32m[I 2025-01-04 12:10:43,817][0m Trial 3 finished with value: 0.34724027100994653 and parameters: {'observation_period_num': 102, 'train_rates': 0.6450090652696642, 'learning_rate': 3.384843021687079e-06, 'batch_size': 145, 'step_size': 5, 'gamma': 0.7995419842078363}. Best is trial 2 with value: 0.15796536044882878.[0m
[32m[I 2025-01-04 12:15:33,653][0m Trial 4 finished with value: 0.08320010706074635 and parameters: {'observation_period_num': 109, 'train_rates': 0.8667393591438916, 'learning_rate': 0.0008208944978006433, 'batch_size': 105, 'step_size': 15, 'gamma': 0.7500422721392042}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:29:23,131][0m Trial 5 finished with value: 0.16606817708203667 and parameters: {'observation_period_num': 173, 'train_rates': 0.7812952366296302, 'learning_rate': 0.0001035264364083157, 'batch_size': 32, 'step_size': 7, 'gamma': 0.7514237325328212}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:33:04,704][0m Trial 6 finished with value: 0.14048430580856666 and parameters: {'observation_period_num': 89, 'train_rates': 0.8151159390772071, 'learning_rate': 0.00018758637921116063, 'batch_size': 207, 'step_size': 15, 'gamma': 0.8938988734918691}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:37:02,892][0m Trial 7 finished with value: 0.1307581939407297 and parameters: {'observation_period_num': 168, 'train_rates': 0.8672066187782026, 'learning_rate': 0.00011962914340762253, 'batch_size': 168, 'step_size': 5, 'gamma': 0.8504061764147275}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:44:53,929][0m Trial 8 finished with value: 0.18813328949304728 and parameters: {'observation_period_num': 197, 'train_rates': 0.8585176175658799, 'learning_rate': 2.2419797326357834e-06, 'batch_size': 62, 'step_size': 6, 'gamma': 0.7931287564974111}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:48:52,279][0m Trial 9 finished with value: 0.08900674432516098 and parameters: {'observation_period_num': 108, 'train_rates': 0.9309303620808768, 'learning_rate': 8.682618817836984e-05, 'batch_size': 218, 'step_size': 12, 'gamma': 0.7896794231081005}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:54:04,532][0m Trial 10 finished with value: 0.1646816083981145 and parameters: {'observation_period_num': 22, 'train_rates': 0.6932072179014508, 'learning_rate': 1.3146080716529368e-05, 'batch_size': 87, 'step_size': 10, 'gamma': 0.9774301216928971}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 12:58:15,994][0m Trial 11 finished with value: 0.1010211929678917 and parameters: {'observation_period_num': 136, 'train_rates': 0.9814121841074585, 'learning_rate': 0.0008616957208646564, 'batch_size': 250, 'step_size': 15, 'gamma': 0.7509960060148588}. Best is trial 4 with value: 0.08320010706074635.[0m
[32m[I 2025-01-04 13:02:21,677][0m Trial 12 finished with value: 0.057903027863103626 and parameters: {'observation_period_num': 41, 'train_rates': 0.9098326514460272, 'learning_rate': 2.9384133733765014e-05, 'batch_size': 182, 'step_size': 11, 'gamma': 0.8318106178517791}. Best is trial 12 with value: 0.057903027863103626.[0m
[32m[I 2025-01-04 13:06:32,655][0m Trial 13 finished with value: 0.05965706198798411 and parameters: {'observation_period_num': 29, 'train_rates': 0.8912602908985604, 'learning_rate': 1.4965761066289454e-05, 'batch_size': 169, 'step_size': 12, 'gamma': 0.8511751802243027}. Best is trial 12 with value: 0.057903027863103626.[0m
[32m[I 2025-01-04 13:10:47,562][0m Trial 14 finished with value: 0.04400850360871254 and parameters: {'observation_period_num': 5, 'train_rates': 0.9095023615815573, 'learning_rate': 1.7081848186456174e-05, 'batch_size': 164, 'step_size': 11, 'gamma': 0.8493398082087142}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:15:12,248][0m Trial 15 finished with value: 0.051509457715054885 and parameters: {'observation_period_num': 5, 'train_rates': 0.9365641465151172, 'learning_rate': 3.0609100066881476e-05, 'batch_size': 168, 'step_size': 10, 'gamma': 0.9430033723887107}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:19:39,199][0m Trial 16 finished with value: 0.1586754322052002 and parameters: {'observation_period_num': 251, 'train_rates': 0.9840856774004793, 'learning_rate': 7.673105232564386e-06, 'batch_size': 145, 'step_size': 9, 'gamma': 0.9365782421566458}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:23:05,306][0m Trial 17 finished with value: 0.15235980699632778 and parameters: {'observation_period_num': 8, 'train_rates': 0.7075286330878716, 'learning_rate': 3.2687197200733133e-05, 'batch_size': 193, 'step_size': 13, 'gamma': 0.9856511076468906}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:26:47,854][0m Trial 18 finished with value: 0.09346396371037825 and parameters: {'observation_period_num': 55, 'train_rates': 0.8346938386399426, 'learning_rate': 3.8342235032606536e-05, 'batch_size': 243, 'step_size': 8, 'gamma': 0.9283249838952756}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:31:22,263][0m Trial 19 finished with value: 0.05118544621096141 and parameters: {'observation_period_num': 5, 'train_rates': 0.9297937862020752, 'learning_rate': 6.45956027578799e-06, 'batch_size': 150, 'step_size': 9, 'gamma': 0.9380860353461382}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:37:31,488][0m Trial 20 finished with value: 0.2909218604993267 and parameters: {'observation_period_num': 62, 'train_rates': 0.7316578756679241, 'learning_rate': 1.1244686607301215e-06, 'batch_size': 74, 'step_size': 8, 'gamma': 0.9116403979563666}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:42:05,742][0m Trial 21 finished with value: 0.04714359622448683 and parameters: {'observation_period_num': 7, 'train_rates': 0.945730610612398, 'learning_rate': 9.391039944435738e-06, 'batch_size': 150, 'step_size': 10, 'gamma': 0.9569687454933987}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:46:38,298][0m Trial 22 finished with value: 0.08383152719403876 and parameters: {'observation_period_num': 41, 'train_rates': 0.8953596027941697, 'learning_rate': 5.981488394657871e-06, 'batch_size': 135, 'step_size': 13, 'gamma': 0.964908521408595}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:51:18,099][0m Trial 23 finished with value: 0.04568684288511978 and parameters: {'observation_period_num': 5, 'train_rates': 0.9447275556556121, 'learning_rate': 1.268660734027113e-05, 'batch_size': 140, 'step_size': 10, 'gamma': 0.9653428527278294}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 13:56:07,325][0m Trial 24 finished with value: 0.06672632694244385 and parameters: {'observation_period_num': 26, 'train_rates': 0.9644190304449779, 'learning_rate': 1.5982143362841686e-05, 'batch_size': 129, 'step_size': 11, 'gamma': 0.9586163442198048}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:00:20,541][0m Trial 25 finished with value: 0.09616710436075254 and parameters: {'observation_period_num': 74, 'train_rates': 0.9075996835226026, 'learning_rate': 5.640356044537423e-05, 'batch_size': 158, 'step_size': 13, 'gamma': 0.8685453911791806}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:04:44,658][0m Trial 26 finished with value: 0.08277275413274765 and parameters: {'observation_period_num': 45, 'train_rates': 0.9877049487700728, 'learning_rate': 1.0925026692976786e-05, 'batch_size': 189, 'step_size': 10, 'gamma': 0.9117614102045047}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:10:01,536][0m Trial 27 finished with value: 0.04759386454413577 and parameters: {'observation_period_num': 23, 'train_rates': 0.8322832181574072, 'learning_rate': 2.0295663224429404e-05, 'batch_size': 96, 'step_size': 9, 'gamma': 0.8229015707311834}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:14:54,833][0m Trial 28 finished with value: 0.11025599302916692 and parameters: {'observation_period_num': 148, 'train_rates': 0.9484308319433911, 'learning_rate': 5.033637320071151e-06, 'batch_size': 122, 'step_size': 7, 'gamma': 0.9588875842766762}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:19:00,500][0m Trial 29 finished with value: 0.14538972079753876 and parameters: {'observation_period_num': 70, 'train_rates': 0.9592697763007804, 'learning_rate': 1.685454640209192e-06, 'batch_size': 202, 'step_size': 11, 'gamma': 0.8677030115692703}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:22:44,332][0m Trial 30 finished with value: 0.42895847631727185 and parameters: {'observation_period_num': 83, 'train_rates': 0.6000983244195772, 'learning_rate': 9.641625625273483e-06, 'batch_size': 116, 'step_size': 12, 'gamma': 0.9047216190671304}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:27:54,247][0m Trial 31 finished with value: 0.044839231376635265 and parameters: {'observation_period_num': 22, 'train_rates': 0.8396556146427971, 'learning_rate': 2.5312375392842903e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8214799979373012}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:37:38,572][0m Trial 32 finished with value: 0.04701538181546106 and parameters: {'observation_period_num': 17, 'train_rates': 0.8823953351062976, 'learning_rate': 2.2301679684805256e-05, 'batch_size': 55, 'step_size': 9, 'gamma': 0.8259128337209629}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:50:17,758][0m Trial 33 finished with value: 0.09923131417396457 and parameters: {'observation_period_num': 51, 'train_rates': 0.8815281715117537, 'learning_rate': 5.535469622601543e-05, 'batch_size': 40, 'step_size': 7, 'gamma': 0.8197358045498436}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 14:59:31,839][0m Trial 34 finished with value: 0.06003802111412323 and parameters: {'observation_period_num': 34, 'train_rates': 0.840272198974166, 'learning_rate': 2.2567286892009755e-05, 'batch_size': 55, 'step_size': 9, 'gamma': 0.7745779077166057}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 15:05:28,931][0m Trial 35 finished with value: 0.044240309710962225 and parameters: {'observation_period_num': 17, 'train_rates': 0.9151488603283239, 'learning_rate': 6.0546836217225624e-05, 'batch_size': 90, 'step_size': 3, 'gamma': 0.8432182790767822}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 15:10:09,180][0m Trial 36 finished with value: 0.04865473698277929 and parameters: {'observation_period_num': 20, 'train_rates': 0.800380558218734, 'learning_rate': 0.00043351283020083076, 'batch_size': 107, 'step_size': 2, 'gamma': 0.8459535348756374}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 15:15:43,497][0m Trial 37 finished with value: 0.2281191172459495 and parameters: {'observation_period_num': 58, 'train_rates': 0.7638825888828797, 'learning_rate': 5.167426906855399e-05, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8840337110922204}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 15:21:06,733][0m Trial 38 finished with value: 0.08894618228077888 and parameters: {'observation_period_num': 35, 'train_rates': 0.9095440072548528, 'learning_rate': 3.757165794067283e-06, 'batch_size': 99, 'step_size': 2, 'gamma': 0.8804651749903912}. Best is trial 14 with value: 0.04400850360871254.[0m
Early stopping at epoch 65
[32m[I 2025-01-04 15:25:25,394][0m Trial 39 finished with value: 0.11743529030927723 and parameters: {'observation_period_num': 123, 'train_rates': 0.8635479208600584, 'learning_rate': 0.00020245749000699374, 'batch_size': 77, 'step_size': 1, 'gamma': 0.809397925376298}. Best is trial 14 with value: 0.04400850360871254.[0m
[32m[I 2025-01-04 15:50:51,970][0m Trial 40 finished with value: 0.034723491675179935 and parameters: {'observation_period_num': 17, 'train_rates': 0.921777805638303, 'learning_rate': 8.59012060458893e-05, 'batch_size': 21, 'step_size': 6, 'gamma': 0.8423840610507695}. Best is trial 40 with value: 0.034723491675179935.[0m
[32m[I 2025-01-04 16:11:12,169][0m Trial 41 finished with value: 0.032445431454107165 and parameters: {'observation_period_num': 16, 'train_rates': 0.9266419745075964, 'learning_rate': 7.88099611941642e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8533799031056413}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 16:29:11,801][0m Trial 42 finished with value: 0.037108533694285205 and parameters: {'observation_period_num': 17, 'train_rates': 0.9232032130232259, 'learning_rate': 8.092445769571774e-05, 'batch_size': 30, 'step_size': 4, 'gamma': 0.8382827919198574}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 17:00:53,301][0m Trial 43 finished with value: 0.050262150603802604 and parameters: {'observation_period_num': 36, 'train_rates': 0.9169217889620677, 'learning_rate': 8.17436640089718e-05, 'batch_size': 16, 'step_size': 4, 'gamma': 0.8375233746763893}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 17:30:31,385][0m Trial 44 finished with value: 0.08877534689786641 and parameters: {'observation_period_num': 95, 'train_rates': 0.9197899339792459, 'learning_rate': 0.00015224909414910876, 'batch_size': 17, 'step_size': 5, 'gamma': 0.8583055239694848}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 17:47:06,278][0m Trial 45 finished with value: 0.039804699751449395 and parameters: {'observation_period_num': 16, 'train_rates': 0.9681621580631649, 'learning_rate': 0.00030590354480366583, 'batch_size': 33, 'step_size': 3, 'gamma': 0.8411453727567352}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 18:05:34,394][0m Trial 46 finished with value: 0.08893336299806834 and parameters: {'observation_period_num': 67, 'train_rates': 0.9723147365705286, 'learning_rate': 0.00039733029064507457, 'batch_size': 29, 'step_size': 4, 'gamma': 0.860028311662217}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 18:18:08,712][0m Trial 47 finished with value: 0.07393540415206766 and parameters: {'observation_period_num': 48, 'train_rates': 0.8946507393313952, 'learning_rate': 0.00011318990253855115, 'batch_size': 41, 'step_size': 6, 'gamma': 0.8081323734914926}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 18:36:58,454][0m Trial 48 finished with value: 0.03735267833989357 and parameters: {'observation_period_num': 16, 'train_rates': 0.963757512671412, 'learning_rate': 0.0002811602457313272, 'batch_size': 29, 'step_size': 3, 'gamma': 0.7813167247966719}. Best is trial 41 with value: 0.032445431454107165.[0m
[32m[I 2025-01-04 18:50:45,347][0m Trial 49 finished with value: 0.14974715312321982 and parameters: {'observation_period_num': 211, 'train_rates': 0.9695069914558763, 'learning_rate': 0.00032710503876270844, 'batch_size': 37, 'step_size': 3, 'gamma': 0.7753499501423814}. Best is trial 41 with value: 0.032445431454107165.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-04 18:50:45,356][0m A new study created in memory with name: no-name-53789db3-2e3f-41ad-81aa-a8c2532feca7[0m
[32m[I 2025-01-04 18:54:50,792][0m Trial 0 finished with value: 0.11343260984355157 and parameters: {'observation_period_num': 123, 'train_rates': 0.8074256300975219, 'learning_rate': 0.0007168249346776347, 'batch_size': 140, 'step_size': 3, 'gamma': 0.8854461511982208}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:02:24,042][0m Trial 1 finished with value: 0.16510049624590095 and parameters: {'observation_period_num': 166, 'train_rates': 0.8370719883144521, 'learning_rate': 2.004833083302931e-06, 'batch_size': 64, 'step_size': 8, 'gamma': 0.8124611678778146}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:05:30,147][0m Trial 2 finished with value: 0.2970312559497907 and parameters: {'observation_period_num': 183, 'train_rates': 0.6168855134809832, 'learning_rate': 0.0008700922209977391, 'batch_size': 173, 'step_size': 2, 'gamma': 0.7641271894886363}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:15:22,478][0m Trial 3 finished with value: 0.12376015335321426 and parameters: {'observation_period_num': 109, 'train_rates': 0.9457127456946381, 'learning_rate': 0.00018830283034241158, 'batch_size': 53, 'step_size': 10, 'gamma': 0.7912898563917828}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:23:07,473][0m Trial 4 finished with value: 0.17657431960105896 and parameters: {'observation_period_num': 240, 'train_rates': 0.980403303745744, 'learning_rate': 4.083102939799848e-06, 'batch_size': 68, 'step_size': 7, 'gamma': 0.8441979869763147}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:26:20,884][0m Trial 5 finished with value: 0.267218405852502 and parameters: {'observation_period_num': 75, 'train_rates': 0.6760514821468172, 'learning_rate': 3.80252100799151e-06, 'batch_size': 198, 'step_size': 12, 'gamma': 0.9544781472720624}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:29:23,784][0m Trial 6 finished with value: 0.4333472196851765 and parameters: {'observation_period_num': 152, 'train_rates': 0.6431103113891705, 'learning_rate': 6.271321696679818e-06, 'batch_size': 213, 'step_size': 10, 'gamma': 0.8180678965605909}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:44:14,052][0m Trial 7 finished with value: 0.1929354622097719 and parameters: {'observation_period_num': 44, 'train_rates': 0.6701132606461321, 'learning_rate': 7.257292736095353e-05, 'batch_size': 29, 'step_size': 8, 'gamma': 0.7880900936050058}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:48:16,222][0m Trial 8 finished with value: 0.11385674067217895 and parameters: {'observation_period_num': 114, 'train_rates': 0.7907599435248027, 'learning_rate': 0.0005462124546589424, 'batch_size': 136, 'step_size': 12, 'gamma': 0.7782355512923549}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:52:15,756][0m Trial 9 finished with value: 0.21647552410695728 and parameters: {'observation_period_num': 159, 'train_rates': 0.7838585278811773, 'learning_rate': 7.2363308220173505e-06, 'batch_size': 130, 'step_size': 3, 'gamma': 0.881517112496685}. Best is trial 0 with value: 0.11343260984355157.[0m
[32m[I 2025-01-04 19:56:14,049][0m Trial 10 finished with value: 0.043993504693755854 and parameters: {'observation_period_num': 9, 'train_rates': 0.8956445319207507, 'learning_rate': 2.831644411387198e-05, 'batch_size': 251, 'step_size': 5, 'gamma': 0.9076526429757524}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:00:14,304][0m Trial 11 finished with value: 0.051926296381723316 and parameters: {'observation_period_num': 16, 'train_rates': 0.8857871145633693, 'learning_rate': 2.9103620481826705e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9107677328890319}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:04:11,243][0m Trial 12 finished with value: 0.044516875264456586 and parameters: {'observation_period_num': 9, 'train_rates': 0.8859058666545939, 'learning_rate': 2.559242804318037e-05, 'batch_size': 251, 'step_size': 5, 'gamma': 0.9283484092577565}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:08:09,394][0m Trial 13 finished with value: 0.07063412395390598 and parameters: {'observation_period_num': 18, 'train_rates': 0.8952064552842679, 'learning_rate': 2.073971452372719e-05, 'batch_size': 252, 'step_size': 5, 'gamma': 0.9848984182101719}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:11:57,844][0m Trial 14 finished with value: 0.12093317334772284 and parameters: {'observation_period_num': 58, 'train_rates': 0.8907046397203805, 'learning_rate': 9.049986861590014e-05, 'batch_size': 221, 'step_size': 15, 'gamma': 0.9344223095283262}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:15:33,105][0m Trial 15 finished with value: 0.1794389619828712 and parameters: {'observation_period_num': 10, 'train_rates': 0.7188323937085519, 'learning_rate': 1.7259055952227262e-05, 'batch_size': 233, 'step_size': 5, 'gamma': 0.918095653335786}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:19:36,845][0m Trial 16 finished with value: 0.0959770530462265 and parameters: {'observation_period_num': 92, 'train_rates': 0.9408303439287986, 'learning_rate': 8.377884296356386e-05, 'batch_size': 180, 'step_size': 1, 'gamma': 0.9740371855919177}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:23:37,060][0m Trial 17 finished with value: 0.06887034851942819 and parameters: {'observation_period_num': 44, 'train_rates': 0.8438463495024308, 'learning_rate': 1.5071632582664895e-05, 'batch_size': 170, 'step_size': 6, 'gamma': 0.8633230322978295}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:28:17,207][0m Trial 18 finished with value: 0.3454814337951652 and parameters: {'observation_period_num': 208, 'train_rates': 0.7419290730692751, 'learning_rate': 5.085460907277423e-05, 'batch_size': 93, 'step_size': 4, 'gamma': 0.9067718031223561}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:32:19,129][0m Trial 19 finished with value: 0.0756758376955986 and parameters: {'observation_period_num': 38, 'train_rates': 0.9282416963663661, 'learning_rate': 0.0002955525564099903, 'batch_size': 232, 'step_size': 7, 'gamma': 0.950980417845001}. Best is trial 10 with value: 0.043993504693755854.[0m
Early stopping at epoch 64
[32m[I 2025-01-04 20:34:49,520][0m Trial 20 finished with value: 0.4395714621794851 and parameters: {'observation_period_num': 76, 'train_rates': 0.8614494871588934, 'learning_rate': 1.2933874187844667e-06, 'batch_size': 201, 'step_size': 1, 'gamma': 0.8521548958169721}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:38:49,704][0m Trial 21 finished with value: 0.04588811157025708 and parameters: {'observation_period_num': 5, 'train_rates': 0.8946623919651319, 'learning_rate': 2.5962651409540546e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9085354883144151}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:43:04,750][0m Trial 22 finished with value: 0.06179353967308998 and parameters: {'observation_period_num': 6, 'train_rates': 0.98451369356648, 'learning_rate': 1.0963244952425002e-05, 'batch_size': 243, 'step_size': 4, 'gamma': 0.8930596020413438}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:47:02,738][0m Trial 23 finished with value: 0.060380782932043076 and parameters: {'observation_period_num': 28, 'train_rates': 0.9173114768000986, 'learning_rate': 0.00013454524054404778, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9355397417508675}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:50:48,064][0m Trial 24 finished with value: 0.07798526302126828 and parameters: {'observation_period_num': 61, 'train_rates': 0.8598155353836404, 'learning_rate': 4.24461809621534e-05, 'batch_size': 218, 'step_size': 3, 'gamma': 0.9291106855515746}. Best is trial 10 with value: 0.043993504693755854.[0m
[32m[I 2025-01-04 20:54:35,153][0m Trial 25 finished with value: 0.042578560086631134 and parameters: {'observation_period_num': 5, 'train_rates': 0.8238761097810764, 'learning_rate': 3.023212087541594e-05, 'batch_size': 193, 'step_size': 6, 'gamma': 0.9555136294121471}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 20:58:23,784][0m Trial 26 finished with value: 0.10420778734822658 and parameters: {'observation_period_num': 55, 'train_rates': 0.8162598830296661, 'learning_rate': 1.0942575094348431e-05, 'batch_size': 196, 'step_size': 10, 'gamma': 0.9630596772677071}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:02:08,747][0m Trial 27 finished with value: 0.19766096600064162 and parameters: {'observation_period_num': 30, 'train_rates': 0.7545039845737432, 'learning_rate': 4.610003156184906e-05, 'batch_size': 159, 'step_size': 7, 'gamma': 0.9462540145676052}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:05:58,817][0m Trial 28 finished with value: 0.13312713376113347 and parameters: {'observation_period_num': 93, 'train_rates': 0.8313281734826099, 'learning_rate': 3.3069213522945206e-05, 'batch_size': 234, 'step_size': 9, 'gamma': 0.9886679115139362}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:09:43,472][0m Trial 29 finished with value: 0.12396635710091335 and parameters: {'observation_period_num': 138, 'train_rates': 0.8078650604958385, 'learning_rate': 0.00017620599173561816, 'batch_size': 189, 'step_size': 4, 'gamma': 0.889536706099827}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:13:55,807][0m Trial 30 finished with value: 0.05245435396327248 and parameters: {'observation_period_num': 27, 'train_rates': 0.8651658909022597, 'learning_rate': 1.1306268806415557e-05, 'batch_size': 153, 'step_size': 2, 'gamma': 0.9641355574873344}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:17:55,638][0m Trial 31 finished with value: 0.04543578136673372 and parameters: {'observation_period_num': 7, 'train_rates': 0.902812422634133, 'learning_rate': 2.292839019784754e-05, 'batch_size': 215, 'step_size': 6, 'gamma': 0.9033794590768603}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:22:05,798][0m Trial 32 finished with value: 0.045231893658638 and parameters: {'observation_period_num': 23, 'train_rates': 0.9597154831597711, 'learning_rate': 6.110130962089454e-05, 'batch_size': 211, 'step_size': 6, 'gamma': 0.8767125850071892}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:26:13,158][0m Trial 33 finished with value: 0.053092487156391144 and parameters: {'observation_period_num': 25, 'train_rates': 0.9651019752632048, 'learning_rate': 6.086272035313797e-05, 'batch_size': 237, 'step_size': 7, 'gamma': 0.8770246390126206}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:30:21,239][0m Trial 34 finished with value: 0.06241317838430405 and parameters: {'observation_period_num': 43, 'train_rates': 0.9661776478240782, 'learning_rate': 0.00013071181307701087, 'batch_size': 207, 'step_size': 8, 'gamma': 0.8328656217687408}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:34:34,065][0m Trial 35 finished with value: 0.09939722716808319 and parameters: {'observation_period_num': 64, 'train_rates': 0.9449920060386193, 'learning_rate': 3.612085183745603e-05, 'batch_size': 224, 'step_size': 6, 'gamma': 0.9248832644354794}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:38:24,509][0m Trial 36 finished with value: 0.1330811712661935 and parameters: {'observation_period_num': 198, 'train_rates': 0.8702998001330537, 'learning_rate': 0.00011240306845410793, 'batch_size': 183, 'step_size': 4, 'gamma': 0.8951741967800827}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:42:18,874][0m Trial 37 finished with value: 0.07342750657833022 and parameters: {'observation_period_num': 35, 'train_rates': 0.8362343404476469, 'learning_rate': 0.0003219048188293946, 'batch_size': 243, 'step_size': 9, 'gamma': 0.8601312689149678}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:46:58,259][0m Trial 38 finished with value: 0.19869712944784312 and parameters: {'observation_period_num': 252, 'train_rates': 0.9286003153405086, 'learning_rate': 6.264327076578579e-06, 'batch_size': 116, 'step_size': 2, 'gamma': 0.940186295603162}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:51:02,623][0m Trial 39 finished with value: 0.1356278359889984 and parameters: {'observation_period_num': 73, 'train_rates': 0.9623499485491015, 'learning_rate': 3.2738234053094224e-06, 'batch_size': 202, 'step_size': 9, 'gamma': 0.871648553249532}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:55:18,358][0m Trial 40 finished with value: 0.06305198523239429 and parameters: {'observation_period_num': 47, 'train_rates': 0.9183392309962047, 'learning_rate': 6.20457824184502e-05, 'batch_size': 159, 'step_size': 8, 'gamma': 0.8398088126348987}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 21:59:19,482][0m Trial 41 finished with value: 0.05653048720624712 and parameters: {'observation_period_num': 19, 'train_rates': 0.9050520075936955, 'learning_rate': 2.2307186441456845e-05, 'batch_size': 217, 'step_size': 6, 'gamma': 0.8948758240298393}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:03:18,475][0m Trial 42 finished with value: 0.047694244656784054 and parameters: {'observation_period_num': 6, 'train_rates': 0.8777020886958422, 'learning_rate': 1.4377639189482127e-05, 'batch_size': 226, 'step_size': 7, 'gamma': 0.9194872739203245}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:07:17,362][0m Trial 43 finished with value: 0.05324274295841465 and parameters: {'observation_period_num': 18, 'train_rates': 0.910777794673109, 'learning_rate': 2.917310665955292e-05, 'batch_size': 210, 'step_size': 5, 'gamma': 0.9057945241229353}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:10:55,766][0m Trial 44 finished with value: 0.2194420394256599 and parameters: {'observation_period_num': 19, 'train_rates': 0.7733047905209522, 'learning_rate': 8.35351290315825e-06, 'batch_size': 243, 'step_size': 6, 'gamma': 0.8169149090845728}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:14:59,521][0m Trial 45 finished with value: 0.04558517173674312 and parameters: {'observation_period_num': 5, 'train_rates': 0.8502135608595237, 'learning_rate': 1.846603506552703e-05, 'batch_size': 192, 'step_size': 4, 'gamma': 0.9618585629933105}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:18:53,485][0m Trial 46 finished with value: 0.05874106246091071 and parameters: {'observation_period_num': 37, 'train_rates': 0.8198656018961965, 'learning_rate': 4.0561218056204685e-05, 'batch_size': 173, 'step_size': 5, 'gamma': 0.879483345066748}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:33:55,310][0m Trial 47 finished with value: 0.07541001019932383 and parameters: {'observation_period_num': 51, 'train_rates': 0.9522454579088717, 'learning_rate': 2.2869117004021137e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.7531183620937046}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:38:03,914][0m Trial 48 finished with value: 0.04392361268401146 and parameters: {'observation_period_num': 19, 'train_rates': 0.9359149488979245, 'learning_rate': 7.994998405269503e-05, 'batch_size': 245, 'step_size': 3, 'gamma': 0.9161553626123723}. Best is trial 25 with value: 0.042578560086631134.[0m
[32m[I 2025-01-04 22:41:53,738][0m Trial 49 finished with value: 0.1352273089739777 and parameters: {'observation_period_num': 95, 'train_rates': 0.8816030455167942, 'learning_rate': 9.017917464206233e-05, 'batch_size': 229, 'step_size': 3, 'gamma': 0.9768241786671138}. Best is trial 25 with value: 0.042578560086631134.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-04 22:41:53,746][0m A new study created in memory with name: no-name-b81f02ac-f0d3-486f-b8c9-09ce7d1b9c51[0m
[32m[I 2025-01-04 22:46:10,398][0m Trial 0 finished with value: 0.03178039565682411 and parameters: {'observation_period_num': 17, 'train_rates': 0.9890247803890839, 'learning_rate': 0.00024360545052564452, 'batch_size': 199, 'step_size': 15, 'gamma': 0.9045494940495339}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 22:50:29,298][0m Trial 1 finished with value: 0.27376770289840213 and parameters: {'observation_period_num': 120, 'train_rates': 0.7785224207910554, 'learning_rate': 9.936787894526417e-06, 'batch_size': 120, 'step_size': 15, 'gamma': 0.7565851559429967}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:01:04,021][0m Trial 2 finished with value: 0.3677000705797302 and parameters: {'observation_period_num': 248, 'train_rates': 0.747926734434245, 'learning_rate': 1.3013138142952261e-05, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9270877657676078}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:05:09,504][0m Trial 3 finished with value: 0.3260839937056176 and parameters: {'observation_period_num': 112, 'train_rates': 0.8778376900893898, 'learning_rate': 2.1844689399839784e-06, 'batch_size': 162, 'step_size': 3, 'gamma': 0.7671781750897613}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:09:44,901][0m Trial 4 finished with value: 0.28878241536997945 and parameters: {'observation_period_num': 252, 'train_rates': 0.779698799269269, 'learning_rate': 4.4919787608612084e-06, 'batch_size': 99, 'step_size': 5, 'gamma': 0.7780327525625037}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:13:17,636][0m Trial 5 finished with value: 0.19632557525233993 and parameters: {'observation_period_num': 206, 'train_rates': 0.7877711107933293, 'learning_rate': 1.868701756051134e-05, 'batch_size': 189, 'step_size': 5, 'gamma': 0.9583348478100608}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:22:03,208][0m Trial 6 finished with value: 0.24356733901798724 and parameters: {'observation_period_num': 64, 'train_rates': 0.7403282627059895, 'learning_rate': 0.0004357579055796586, 'batch_size': 51, 'step_size': 15, 'gamma': 0.7972328451071194}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:26:21,638][0m Trial 7 finished with value: 0.25172927645753274 and parameters: {'observation_period_num': 84, 'train_rates': 0.719619557262031, 'learning_rate': 2.1726614182985305e-05, 'batch_size': 107, 'step_size': 10, 'gamma': 0.989116639961399}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:29:45,986][0m Trial 8 finished with value: 1.806308583597983 and parameters: {'observation_period_num': 52, 'train_rates': 0.7328361607146013, 'learning_rate': 0.0007182713345298297, 'batch_size': 241, 'step_size': 14, 'gamma': 0.9432777333161437}. Best is trial 0 with value: 0.03178039565682411.[0m
Early stopping at epoch 50
[32m[I 2025-01-04 23:31:53,812][0m Trial 9 finished with value: 0.4339842676049909 and parameters: {'observation_period_num': 30, 'train_rates': 0.8279334263101714, 'learning_rate': 1.7825271561321364e-06, 'batch_size': 143, 'step_size': 1, 'gamma': 0.7585726429088758}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:36:06,850][0m Trial 10 finished with value: 0.03933367133140564 and parameters: {'observation_period_num': 14, 'train_rates': 0.9645528294999256, 'learning_rate': 0.0001781432910257502, 'batch_size': 254, 'step_size': 9, 'gamma': 0.8646603612116306}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:40:20,821][0m Trial 11 finished with value: 0.03687597066164017 and parameters: {'observation_period_num': 9, 'train_rates': 0.9847950684507555, 'learning_rate': 0.00012774738714126888, 'batch_size': 244, 'step_size': 11, 'gamma': 0.8639890677215831}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:44:27,007][0m Trial 12 finished with value: 0.1319867968559265 and parameters: {'observation_period_num': 164, 'train_rates': 0.9897434259591448, 'learning_rate': 9.776007899594488e-05, 'batch_size': 206, 'step_size': 12, 'gamma': 0.8730640283625831}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:48:28,385][0m Trial 13 finished with value: 0.03353676059215366 and parameters: {'observation_period_num': 7, 'train_rates': 0.9211889570595818, 'learning_rate': 8.24113495448597e-05, 'batch_size': 214, 'step_size': 12, 'gamma': 0.8815943901902777}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:51:30,979][0m Trial 14 finished with value: 0.19106292819892112 and parameters: {'observation_period_num': 50, 'train_rates': 0.6119996702746695, 'learning_rate': 5.7448018730926176e-05, 'batch_size': 200, 'step_size': 13, 'gamma': 0.8995142105994629}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:55:34,216][0m Trial 15 finished with value: 0.10547078217639298 and parameters: {'observation_period_num': 89, 'train_rates': 0.915079156164333, 'learning_rate': 0.0003085819643899424, 'batch_size': 176, 'step_size': 7, 'gamma': 0.9046613070254987}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-04 23:59:31,802][0m Trial 16 finished with value: 0.10377109232156173 and parameters: {'observation_period_num': 158, 'train_rates': 0.9177179261058664, 'learning_rate': 4.273265274859039e-05, 'batch_size': 212, 'step_size': 12, 'gamma': 0.8287069297784143}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-05 00:03:16,273][0m Trial 17 finished with value: 1.674412522270781 and parameters: {'observation_period_num': 30, 'train_rates': 0.8557286267761088, 'learning_rate': 0.0008038204266848334, 'batch_size': 218, 'step_size': 8, 'gamma': 0.8356326092305102}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-05 00:07:40,486][0m Trial 18 finished with value: 0.09792197232737261 and parameters: {'observation_period_num': 82, 'train_rates': 0.9410609647275959, 'learning_rate': 0.0002582843461264077, 'batch_size': 155, 'step_size': 14, 'gamma': 0.9054054913917275}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-05 00:14:26,354][0m Trial 19 finished with value: 0.04319530725479126 and parameters: {'observation_period_num': 5, 'train_rates': 0.8988554207727261, 'learning_rate': 6.895831731708632e-05, 'batch_size': 80, 'step_size': 11, 'gamma': 0.8394909566123641}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-05 00:18:09,355][0m Trial 20 finished with value: 0.14551975967687206 and parameters: {'observation_period_num': 153, 'train_rates': 0.8441879054863608, 'learning_rate': 3.363728128350756e-05, 'batch_size': 229, 'step_size': 13, 'gamma': 0.8837617749582153}. Best is trial 0 with value: 0.03178039565682411.[0m
[32m[I 2025-01-05 00:22:25,327][0m Trial 21 finished with value: 0.02835613302886486 and parameters: {'observation_period_num': 29, 'train_rates': 0.984767153773091, 'learning_rate': 0.00014048446779042743, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8588557637610812}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 00:26:24,405][0m Trial 22 finished with value: 0.05937793105840683 and parameters: {'observation_period_num': 33, 'train_rates': 0.9408714112451885, 'learning_rate': 0.0001819277179988384, 'batch_size': 255, 'step_size': 10, 'gamma': 0.9244291667181466}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 00:30:31,606][0m Trial 23 finished with value: 0.08672773838043213 and parameters: {'observation_period_num': 60, 'train_rates': 0.9526123480183826, 'learning_rate': 8.974092285664735e-05, 'batch_size': 184, 'step_size': 13, 'gamma': 0.8460510879981281}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 00:34:47,936][0m Trial 24 finished with value: 0.05373835563659668 and parameters: {'observation_period_num': 34, 'train_rates': 0.9876080192674341, 'learning_rate': 0.0004607244948963833, 'batch_size': 225, 'step_size': 15, 'gamma': 0.8084711530983564}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:05:51,095][0m Trial 25 finished with value: 0.07135446528536442 and parameters: {'observation_period_num': 22, 'train_rates': 0.8816449032156037, 'learning_rate': 0.00015688897009120628, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8825764789964893}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:09:52,101][0m Trial 26 finished with value: 0.06850862503051758 and parameters: {'observation_period_num': 46, 'train_rates': 0.9268340942363107, 'learning_rate': 0.0002898982869272745, 'batch_size': 231, 'step_size': 9, 'gamma': 0.8565306935086512}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:13:04,644][0m Trial 27 finished with value: 0.20102442567882362 and parameters: {'observation_period_num': 71, 'train_rates': 0.6605792382193827, 'learning_rate': 5.0843726619991316e-05, 'batch_size': 197, 'step_size': 12, 'gamma': 0.8170384515478073}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:17:16,380][0m Trial 28 finished with value: 0.09259826689958572 and parameters: {'observation_period_num': 105, 'train_rates': 0.9554155391091842, 'learning_rate': 0.0005064069552315743, 'batch_size': 164, 'step_size': 14, 'gamma': 0.8858791641408351}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:21:37,391][0m Trial 29 finished with value: 0.15581398281392117 and parameters: {'observation_period_num': 138, 'train_rates': 0.8149073344912855, 'learning_rate': 5.335686152138525e-06, 'batch_size': 126, 'step_size': 15, 'gamma': 0.9215424511653738}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:25:37,858][0m Trial 30 finished with value: 0.11748819470405579 and parameters: {'observation_period_num': 192, 'train_rates': 0.9004952336908625, 'learning_rate': 0.00010185279731925558, 'batch_size': 176, 'step_size': 10, 'gamma': 0.9559721897171689}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:29:49,724][0m Trial 31 finished with value: 0.04925001040101051 and parameters: {'observation_period_num': 6, 'train_rates': 0.9726405828659628, 'learning_rate': 0.0001349018805727642, 'batch_size': 246, 'step_size': 11, 'gamma': 0.853829755732172}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:34:03,398][0m Trial 32 finished with value: 0.03191959857940674 and parameters: {'observation_period_num': 17, 'train_rates': 0.9860478241580437, 'learning_rate': 0.00024318818088053907, 'batch_size': 243, 'step_size': 7, 'gamma': 0.8693674000626118}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:38:20,273][0m Trial 33 finished with value: 0.0655989721417427 and parameters: {'observation_period_num': 46, 'train_rates': 0.9598918144889712, 'learning_rate': 0.0002483384572768914, 'batch_size': 232, 'step_size': 6, 'gamma': 0.8920973996417025}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:42:19,159][0m Trial 34 finished with value: 0.1106669083237648 and parameters: {'observation_period_num': 21, 'train_rates': 0.9315187721168808, 'learning_rate': 1.0441879520649624e-06, 'batch_size': 215, 'step_size': 7, 'gamma': 0.9139602487165497}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:46:10,869][0m Trial 35 finished with value: 0.06386574755391378 and parameters: {'observation_period_num': 39, 'train_rates': 0.8779038277124248, 'learning_rate': 7.202514231197137e-05, 'batch_size': 241, 'step_size': 3, 'gamma': 0.873993207001484}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:50:25,410][0m Trial 36 finished with value: 0.05781719461083412 and parameters: {'observation_period_num': 19, 'train_rates': 0.9712101200715271, 'learning_rate': 2.1383354744240502e-05, 'batch_size': 191, 'step_size': 8, 'gamma': 0.939560562013579}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:54:20,755][0m Trial 37 finished with value: 0.10758905756776616 and parameters: {'observation_period_num': 69, 'train_rates': 0.8977658052885749, 'learning_rate': 1.0986744978872377e-05, 'batch_size': 209, 'step_size': 6, 'gamma': 0.8236970358334714}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 01:58:32,623][0m Trial 38 finished with value: 0.07196835428476334 and parameters: {'observation_period_num': 96, 'train_rates': 0.9475613218509443, 'learning_rate': 0.0005976020808218269, 'batch_size': 224, 'step_size': 4, 'gamma': 0.7824036614148455}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:02:45,314][0m Trial 39 finished with value: 0.07207747548818588 and parameters: {'observation_period_num': 57, 'train_rates': 0.9884615583172757, 'learning_rate': 0.00022450753434115822, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8948638269421673}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:06:20,618][0m Trial 40 finished with value: 0.19843364141691572 and parameters: {'observation_period_num': 23, 'train_rates': 0.7725218417084775, 'learning_rate': 0.00039815075135843375, 'batch_size': 240, 'step_size': 8, 'gamma': 0.9371265845826965}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:10:31,718][0m Trial 41 finished with value: 0.03871507570147514 and parameters: {'observation_period_num': 6, 'train_rates': 0.9724368967494621, 'learning_rate': 0.00012536016718130082, 'batch_size': 239, 'step_size': 11, 'gamma': 0.8599065213064851}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:14:47,188][0m Trial 42 finished with value: 0.039691679179668427 and parameters: {'observation_period_num': 16, 'train_rates': 0.9874135415059769, 'learning_rate': 0.0003471591710358673, 'batch_size': 256, 'step_size': 13, 'gamma': 0.8709984691306467}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:18:50,758][0m Trial 43 finished with value: 0.03621149808168411 and parameters: {'observation_period_num': 7, 'train_rates': 0.9628457417567634, 'learning_rate': 0.0002020971095473702, 'batch_size': 221, 'step_size': 10, 'gamma': 0.8491721570409498}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:22:51,654][0m Trial 44 finished with value: 0.05995964631438255 and parameters: {'observation_period_num': 40, 'train_rates': 0.9566970939823866, 'learning_rate': 0.0001902899087190186, 'batch_size': 221, 'step_size': 10, 'gamma': 0.8435230093106317}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:26:42,833][0m Trial 45 finished with value: 0.12342805483124474 and parameters: {'observation_period_num': 230, 'train_rates': 0.9233332215367165, 'learning_rate': 3.1836609132514765e-05, 'batch_size': 204, 'step_size': 9, 'gamma': 0.8506562164769245}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:30:58,746][0m Trial 46 finished with value: 1.976085901260376 and parameters: {'observation_period_num': 30, 'train_rates': 0.9702990105843808, 'learning_rate': 0.00097292540838101, 'batch_size': 182, 'step_size': 12, 'gamma': 0.9090063823462331}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:38:11,346][0m Trial 47 finished with value: 0.04091513386139503 and parameters: {'observation_period_num': 16, 'train_rates': 0.9382493728647783, 'learning_rate': 8.838561529272312e-05, 'batch_size': 76, 'step_size': 14, 'gamma': 0.988061432439475}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:41:51,155][0m Trial 48 finished with value: 0.17577778603049488 and parameters: {'observation_period_num': 27, 'train_rates': 0.7016623262792364, 'learning_rate': 0.00021001715101011975, 'batch_size': 147, 'step_size': 7, 'gamma': 0.8763053446030564}. Best is trial 21 with value: 0.02835613302886486.[0m
[32m[I 2025-01-05 02:46:00,402][0m Trial 49 finished with value: 0.09521851751185556 and parameters: {'observation_period_num': 127, 'train_rates': 0.9074084264721928, 'learning_rate': 0.0001218992890623855, 'batch_size': 168, 'step_size': 10, 'gamma': 0.7963988516762692}. Best is trial 21 with value: 0.02835613302886486.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-05 02:46:00,410][0m A new study created in memory with name: no-name-b5dfb973-2550-4bee-9dc7-62fc2a779691[0m
[32m[I 2025-01-05 02:49:16,812][0m Trial 0 finished with value: 0.3979653356316822 and parameters: {'observation_period_num': 240, 'train_rates': 0.7179055418404356, 'learning_rate': 5.270065473762783e-06, 'batch_size': 179, 'step_size': 8, 'gamma': 0.932184669119587}. Best is trial 0 with value: 0.3979653356316822.[0m
Early stopping at epoch 53
[32m[I 2025-01-05 02:51:58,153][0m Trial 1 finished with value: 0.2757899906253659 and parameters: {'observation_period_num': 70, 'train_rates': 0.6277060411683356, 'learning_rate': 4.3776612471005094e-05, 'batch_size': 83, 'step_size': 1, 'gamma': 0.7793221679718385}. Best is trial 1 with value: 0.2757899906253659.[0m
[32m[I 2025-01-05 02:56:45,067][0m Trial 2 finished with value: 1.8487655067443847 and parameters: {'observation_period_num': 65, 'train_rates': 0.9135855787577785, 'learning_rate': 0.0008695334378712021, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8669550758551414}. Best is trial 1 with value: 0.2757899906253659.[0m
[32m[I 2025-01-05 03:00:22,676][0m Trial 3 finished with value: 0.09753615929455053 and parameters: {'observation_period_num': 71, 'train_rates': 0.7884471019334105, 'learning_rate': 2.285259708447223e-05, 'batch_size': 197, 'step_size': 5, 'gamma': 0.8944480689751741}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:04:25,734][0m Trial 4 finished with value: 0.10349632242423856 and parameters: {'observation_period_num': 77, 'train_rates': 0.8373885282189799, 'learning_rate': 0.00045615086240617095, 'batch_size': 154, 'step_size': 12, 'gamma': 0.9837974918056447}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:09:22,603][0m Trial 5 finished with value: 1.6756903373977636 and parameters: {'observation_period_num': 21, 'train_rates': 0.7944330804432589, 'learning_rate': 0.0008831640334120321, 'batch_size': 97, 'step_size': 15, 'gamma': 0.9326506970717593}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:28:53,506][0m Trial 6 finished with value: 0.3967304527759552 and parameters: {'observation_period_num': 235, 'train_rates': 0.6569352742438144, 'learning_rate': 4.788347940977759e-06, 'batch_size': 20, 'step_size': 13, 'gamma': 0.9254458261041938}. Best is trial 3 with value: 0.09753615929455053.[0m
Early stopping at epoch 63
[32m[I 2025-01-05 03:32:56,381][0m Trial 7 finished with value: 0.19785816929934005 and parameters: {'observation_period_num': 16, 'train_rates': 0.74670211445825, 'learning_rate': 4.5005953654225786e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.7748636083966811}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:37:09,415][0m Trial 8 finished with value: 0.13754297088790726 and parameters: {'observation_period_num': 123, 'train_rates': 0.8069776701540314, 'learning_rate': 4.213913488460067e-06, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9842151146318334}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:40:29,877][0m Trial 9 finished with value: 0.2128745235380579 and parameters: {'observation_period_num': 241, 'train_rates': 0.7749381313635995, 'learning_rate': 0.00031706775021338624, 'batch_size': 248, 'step_size': 6, 'gamma': 0.8927752076248212}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:44:28,481][0m Trial 10 finished with value: 0.20972709357738495 and parameters: {'observation_period_num': 176, 'train_rates': 0.9642900914895296, 'learning_rate': 1.458096977931012e-05, 'batch_size': 227, 'step_size': 4, 'gamma': 0.82783990498986}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:48:25,383][0m Trial 11 finished with value: 0.12979675514964006 and parameters: {'observation_period_num': 101, 'train_rates': 0.8695339058456766, 'learning_rate': 0.00014451789716427368, 'batch_size': 185, 'step_size': 10, 'gamma': 0.9749282073454938}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:52:19,535][0m Trial 12 finished with value: 0.1443361014768279 and parameters: {'observation_period_num': 69, 'train_rates': 0.8469574773026107, 'learning_rate': 1.4722611468911517e-06, 'batch_size': 183, 'step_size': 9, 'gamma': 0.8310328746853641}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:55:36,977][0m Trial 13 finished with value: 0.30199405768935345 and parameters: {'observation_period_num': 164, 'train_rates': 0.7010158091103168, 'learning_rate': 0.00011791483256482666, 'batch_size': 215, 'step_size': 6, 'gamma': 0.9547426962793091}. Best is trial 3 with value: 0.09753615929455053.[0m
[32m[I 2025-01-05 03:59:42,208][0m Trial 14 finished with value: 0.07169862363268348 and parameters: {'observation_period_num': 43, 'train_rates': 0.8541627357685393, 'learning_rate': 1.801392090782202e-05, 'batch_size': 160, 'step_size': 11, 'gamma': 0.8856620331300116}. Best is trial 14 with value: 0.07169862363268348.[0m
[32m[I 2025-01-05 04:04:07,987][0m Trial 15 finished with value: 0.06108461323517636 and parameters: {'observation_period_num': 42, 'train_rates': 0.9180280612597109, 'learning_rate': 1.4820419802246582e-05, 'batch_size': 156, 'step_size': 6, 'gamma': 0.878224232163268}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 04:08:29,648][0m Trial 16 finished with value: 0.06166056873677652 and parameters: {'observation_period_num': 37, 'train_rates': 0.9188589044768619, 'learning_rate': 1.1859530053171645e-05, 'batch_size': 150, 'step_size': 8, 'gamma': 0.8520604274240358}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 04:13:49,350][0m Trial 17 finished with value: 0.06986652213979412 and parameters: {'observation_period_num': 5, 'train_rates': 0.9625855285753773, 'learning_rate': 1.0218498770447573e-06, 'batch_size': 107, 'step_size': 8, 'gamma': 0.8465117119669614}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 04:23:53,771][0m Trial 18 finished with value: 0.065733509754492 and parameters: {'observation_period_num': 39, 'train_rates': 0.9077548709549284, 'learning_rate': 9.466373865229889e-06, 'batch_size': 51, 'step_size': 7, 'gamma': 0.807658732991017}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 04:28:03,722][0m Trial 19 finished with value: 0.30130742839277896 and parameters: {'observation_period_num': 105, 'train_rates': 0.9057798048618747, 'learning_rate': 2.226717130645397e-06, 'batch_size': 153, 'step_size': 3, 'gamma': 0.7518993429809486}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 04:32:41,768][0m Trial 20 finished with value: 0.12452027201652527 and parameters: {'observation_period_num': 151, 'train_rates': 0.9790943924090225, 'learning_rate': 8.31565129591707e-05, 'batch_size': 131, 'step_size': 9, 'gamma': 0.8615641403129352}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 04:43:33,447][0m Trial 21 finished with value: 0.06482826368730576 and parameters: {'observation_period_num': 41, 'train_rates': 0.9162611488995267, 'learning_rate': 9.806614449794478e-06, 'batch_size': 48, 'step_size': 7, 'gamma': 0.8073254758695046}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 05:15:46,235][0m Trial 22 finished with value: 0.08401033064803562 and parameters: {'observation_period_num': 43, 'train_rates': 0.9365678270093865, 'learning_rate': 9.764364026195948e-06, 'batch_size': 16, 'step_size': 7, 'gamma': 0.806962212632396}. Best is trial 15 with value: 0.06108461323517636.[0m
[32m[I 2025-01-05 05:26:38,878][0m Trial 23 finished with value: 0.050669751666567245 and parameters: {'observation_period_num': 26, 'train_rates': 0.874383481351507, 'learning_rate': 1.0406369306772751e-05, 'batch_size': 47, 'step_size': 6, 'gamma': 0.8185122758418646}. Best is trial 23 with value: 0.050669751666567245.[0m
[32m[I 2025-01-05 05:30:55,049][0m Trial 24 finished with value: 0.0483328787615462 and parameters: {'observation_period_num': 26, 'train_rates': 0.8849925660780744, 'learning_rate': 3.1657731054517885e-05, 'batch_size': 156, 'step_size': 3, 'gamma': 0.8464115039975366}. Best is trial 24 with value: 0.0483328787615462.[0m
[32m[I 2025-01-05 05:35:49,485][0m Trial 25 finished with value: 0.04052205891187849 and parameters: {'observation_period_num': 5, 'train_rates': 0.8818798843307006, 'learning_rate': 2.696731977841316e-05, 'batch_size': 114, 'step_size': 3, 'gamma': 0.9076679498288581}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 05:45:33,297][0m Trial 26 finished with value: 0.055354726479150525 and parameters: {'observation_period_num': 18, 'train_rates': 0.882764461122245, 'learning_rate': 3.780749755238327e-05, 'batch_size': 52, 'step_size': 3, 'gamma': 0.9100232859807096}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 05:52:07,788][0m Trial 27 finished with value: 0.14406065530260326 and parameters: {'observation_period_num': 204, 'train_rates': 0.817143117298013, 'learning_rate': 6.579206067636139e-05, 'batch_size': 71, 'step_size': 2, 'gamma': 0.8297469399869805}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 05:57:01,230][0m Trial 28 finished with value: 0.046637396242542185 and parameters: {'observation_period_num': 6, 'train_rates': 0.8784369476483173, 'learning_rate': 2.858547299693297e-05, 'batch_size': 109, 'step_size': 3, 'gamma': 0.7945089290128106}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:02:07,383][0m Trial 29 finished with value: 0.05784185884857741 and parameters: {'observation_period_num': 7, 'train_rates': 0.8275527010536632, 'learning_rate': 2.7084287911045198e-05, 'batch_size': 100, 'step_size': 2, 'gamma': 0.7859462567787704}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:06:35,217][0m Trial 30 finished with value: 0.21321489702493893 and parameters: {'observation_period_num': 56, 'train_rates': 0.7591141254400046, 'learning_rate': 0.0002014122082565328, 'batch_size': 109, 'step_size': 3, 'gamma': 0.7528373070755253}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:12:33,850][0m Trial 31 finished with value: 0.04979182756637449 and parameters: {'observation_period_num': 22, 'train_rates': 0.8793200317953859, 'learning_rate': 3.15119528877579e-05, 'batch_size': 88, 'step_size': 4, 'gamma': 0.7917386061542219}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:18:14,406][0m Trial 32 finished with value: 0.08187952240923241 and parameters: {'observation_period_num': 89, 'train_rates': 0.8874405486095186, 'learning_rate': 5.864368908079088e-05, 'batch_size': 88, 'step_size': 4, 'gamma': 0.7898160438369297}. Best is trial 25 with value: 0.04052205891187849.[0m
Early stopping at epoch 53
[32m[I 2025-01-05 06:21:04,424][0m Trial 33 finished with value: 0.07204402072562112 and parameters: {'observation_period_num': 7, 'train_rates': 0.9420774798559366, 'learning_rate': 3.0189069868693054e-05, 'batch_size': 115, 'step_size': 1, 'gamma': 0.7674780742685258}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:25:46,650][0m Trial 34 finished with value: 0.06816867780146288 and parameters: {'observation_period_num': 57, 'train_rates': 0.8624845412383564, 'learning_rate': 2.1256500459110728e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9062777664848766}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:29:59,747][0m Trial 35 finished with value: 0.04722603250827108 and parameters: {'observation_period_num': 24, 'train_rates': 0.8925129282875004, 'learning_rate': 4.884552989236469e-05, 'batch_size': 171, 'step_size': 5, 'gamma': 0.7936115791896735}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:34:01,891][0m Trial 36 finished with value: 0.04902874017134309 and parameters: {'observation_period_num': 25, 'train_rates': 0.8360220702793566, 'learning_rate': 4.724167035099992e-05, 'batch_size': 171, 'step_size': 5, 'gamma': 0.8437783000151735}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:38:03,293][0m Trial 37 finished with value: 0.06756504625082016 and parameters: {'observation_period_num': 57, 'train_rates': 0.9412709365178783, 'learning_rate': 7.88598324639151e-05, 'batch_size': 200, 'step_size': 5, 'gamma': 0.8681331572946569}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:42:22,936][0m Trial 38 finished with value: 0.08521763503269442 and parameters: {'observation_period_num': 81, 'train_rates': 0.8895571649140566, 'learning_rate': 0.00012725729086807795, 'batch_size': 143, 'step_size': 2, 'gamma': 0.9410574388149338}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:46:18,650][0m Trial 39 finished with value: 0.09247035827325738 and parameters: {'observation_period_num': 26, 'train_rates': 0.8036150261802857, 'learning_rate': 6.538754313703351e-06, 'batch_size': 167, 'step_size': 3, 'gamma': 0.7692398889220734}. Best is trial 25 with value: 0.04052205891187849.[0m
Early stopping at epoch 63
[32m[I 2025-01-05 06:48:37,955][0m Trial 40 finished with value: 0.3183797474723854 and parameters: {'observation_period_num': 57, 'train_rates': 0.6197409252176991, 'learning_rate': 2.281829911171755e-05, 'batch_size': 136, 'step_size': 1, 'gamma': 0.8155095698056939}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:52:39,541][0m Trial 41 finished with value: 0.04604510650038719 and parameters: {'observation_period_num': 30, 'train_rates': 0.837513832014829, 'learning_rate': 4.8158419351913135e-05, 'batch_size': 171, 'step_size': 5, 'gamma': 0.9127566310184946}. Best is trial 25 with value: 0.04052205891187849.[0m
[32m[I 2025-01-05 06:56:31,440][0m Trial 42 finished with value: 0.03902269097474905 and parameters: {'observation_period_num': 13, 'train_rates': 0.8540909196148662, 'learning_rate': 4.5637397619110445e-05, 'batch_size': 198, 'step_size': 5, 'gamma': 0.9131569464916931}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:00:26,219][0m Trial 43 finished with value: 0.04158086099875531 and parameters: {'observation_period_num': 13, 'train_rates': 0.84732104783144, 'learning_rate': 5.05530669248082e-05, 'batch_size': 203, 'step_size': 5, 'gamma': 0.9168207323884521}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:04:00,074][0m Trial 44 finished with value: 0.1899049210496856 and parameters: {'observation_period_num': 6, 'train_rates': 0.7842847088327831, 'learning_rate': 9.304873410547935e-05, 'batch_size': 217, 'step_size': 5, 'gamma': 0.9156145977669982}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:07:53,829][0m Trial 45 finished with value: 0.04541040437862925 and parameters: {'observation_period_num': 15, 'train_rates': 0.8436009693937415, 'learning_rate': 0.00030362854288620947, 'batch_size': 201, 'step_size': 4, 'gamma': 0.9393675534117422}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:11:43,957][0m Trial 46 finished with value: 0.0718972713282678 and parameters: {'observation_period_num': 32, 'train_rates': 0.8452616644653546, 'learning_rate': 0.000400134771705053, 'batch_size': 197, 'step_size': 15, 'gamma': 0.9517372243639218}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:15:34,694][0m Trial 47 finished with value: 0.05233194300082172 and parameters: {'observation_period_num': 14, 'train_rates': 0.8219527864931807, 'learning_rate': 0.00066521958836366, 'batch_size': 236, 'step_size': 5, 'gamma': 0.9240263819477669}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:18:56,062][0m Trial 48 finished with value: 0.2091670559162152 and parameters: {'observation_period_num': 50, 'train_rates': 0.7052046755357718, 'learning_rate': 0.00021024771932915098, 'batch_size': 209, 'step_size': 4, 'gamma': 0.9702821689395901}. Best is trial 42 with value: 0.03902269097474905.[0m
[32m[I 2025-01-05 07:22:30,600][0m Trial 49 finished with value: 0.24037403295450865 and parameters: {'observation_period_num': 72, 'train_rates': 0.7420541843478102, 'learning_rate': 0.00019125234748400708, 'batch_size': 190, 'step_size': 6, 'gamma': 0.9021797883699659}. Best is trial 42 with value: 0.03902269097474905.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-05 07:22:30,608][0m A new study created in memory with name: no-name-6269cd8a-28d5-4477-b964-50a1ec4140d7[0m
[32m[I 2025-01-05 07:26:25,346][0m Trial 0 finished with value: 0.2750996644358321 and parameters: {'observation_period_num': 81, 'train_rates': 0.7409852673139413, 'learning_rate': 1.0718822293184971e-05, 'batch_size': 137, 'step_size': 11, 'gamma': 0.9590377374776724}. Best is trial 0 with value: 0.2750996644358321.[0m
[32m[I 2025-01-05 07:30:24,141][0m Trial 1 finished with value: 0.2562140504001406 and parameters: {'observation_period_num': 251, 'train_rates': 0.8864329029636359, 'learning_rate': 1.434542676222791e-06, 'batch_size': 150, 'step_size': 11, 'gamma': 0.8533176783420786}. Best is trial 1 with value: 0.2562140504001406.[0m
[32m[I 2025-01-05 07:34:31,270][0m Trial 2 finished with value: 1.6157094272912718 and parameters: {'observation_period_num': 246, 'train_rates': 0.8118093945528264, 'learning_rate': 0.0008744428367631741, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9575890810777464}. Best is trial 1 with value: 0.2562140504001406.[0m
[32m[I 2025-01-05 07:38:38,012][0m Trial 3 finished with value: 0.13322633339838408 and parameters: {'observation_period_num': 155, 'train_rates': 0.890419002403059, 'learning_rate': 2.4575908284722835e-05, 'batch_size': 163, 'step_size': 15, 'gamma': 0.7853480613504069}. Best is trial 3 with value: 0.13322633339838408.[0m
[32m[I 2025-01-05 07:41:47,187][0m Trial 4 finished with value: 0.3649168995798976 and parameters: {'observation_period_num': 225, 'train_rates': 0.6580039114023721, 'learning_rate': 8.921775630487885e-06, 'batch_size': 196, 'step_size': 7, 'gamma': 0.9466817875606082}. Best is trial 3 with value: 0.13322633339838408.[0m
[32m[I 2025-01-05 07:45:26,492][0m Trial 5 finished with value: 0.12555652895898542 and parameters: {'observation_period_num': 11, 'train_rates': 0.6239971621400867, 'learning_rate': 5.7432959888342844e-05, 'batch_size': 136, 'step_size': 8, 'gamma': 0.9409802671958085}. Best is trial 5 with value: 0.12555652895898542.[0m
[32m[I 2025-01-05 07:49:00,613][0m Trial 6 finished with value: 0.33551336684234023 and parameters: {'observation_period_num': 40, 'train_rates': 0.7672626829431557, 'learning_rate': 1.0869029896590004e-06, 'batch_size': 215, 'step_size': 4, 'gamma': 0.9446198242279692}. Best is trial 5 with value: 0.12555652895898542.[0m
[32m[I 2025-01-05 07:54:23,371][0m Trial 7 finished with value: 0.37840563727884874 and parameters: {'observation_period_num': 251, 'train_rates': 0.7278062079747702, 'learning_rate': 2.5650567454384974e-05, 'batch_size': 79, 'step_size': 10, 'gamma': 0.9598569695664693}. Best is trial 5 with value: 0.12555652895898542.[0m
[32m[I 2025-01-05 08:10:44,427][0m Trial 8 finished with value: 0.633575451977749 and parameters: {'observation_period_num': 195, 'train_rates': 0.6044032160178645, 'learning_rate': 1.8797728890140015e-06, 'batch_size': 23, 'step_size': 3, 'gamma': 0.8159928236661083}. Best is trial 5 with value: 0.12555652895898542.[0m
[32m[I 2025-01-05 08:14:20,322][0m Trial 9 finished with value: 0.20577499647579953 and parameters: {'observation_period_num': 14, 'train_rates': 0.7526752053932909, 'learning_rate': 5.612843013729184e-06, 'batch_size': 194, 'step_size': 5, 'gamma': 0.8651184017201927}. Best is trial 5 with value: 0.12555652895898542.[0m
[32m[I 2025-01-05 08:18:31,431][0m Trial 10 finished with value: 0.11250225454568863 and parameters: {'observation_period_num': 93, 'train_rates': 0.9780326776007197, 'learning_rate': 0.00019297223796873353, 'batch_size': 250, 'step_size': 1, 'gamma': 0.9038223371254663}. Best is trial 10 with value: 0.11250225454568863.[0m
[32m[I 2025-01-05 08:22:12,379][0m Trial 11 finished with value: 0.09299962040725478 and parameters: {'observation_period_num': 92, 'train_rates': 0.8526257762448832, 'learning_rate': 0.00021619991350336744, 'batch_size': 249, 'step_size': 1, 'gamma': 0.9105951899601081}. Best is trial 11 with value: 0.09299962040725478.[0m
[32m[I 2025-01-05 08:26:21,525][0m Trial 12 finished with value: 0.11815839260816574 and parameters: {'observation_period_num': 99, 'train_rates': 0.9870068831119605, 'learning_rate': 0.00023627303149382437, 'batch_size': 247, 'step_size': 1, 'gamma': 0.9050593084788255}. Best is trial 11 with value: 0.09299962040725478.[0m
[32m[I 2025-01-05 08:30:29,854][0m Trial 13 finished with value: 0.13864372670650482 and parameters: {'observation_period_num': 128, 'train_rates': 0.9785121663542652, 'learning_rate': 0.00016989974285334318, 'batch_size': 246, 'step_size': 1, 'gamma': 0.895898747688237}. Best is trial 11 with value: 0.09299962040725478.[0m
[32m[I 2025-01-05 08:34:29,544][0m Trial 14 finished with value: 0.07510415936699351 and parameters: {'observation_period_num': 69, 'train_rates': 0.9093320898321485, 'learning_rate': 0.0003191492324796814, 'batch_size': 256, 'step_size': 2, 'gamma': 0.9048748804345921}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 08:38:15,524][0m Trial 15 finished with value: 0.0861537499839197 and parameters: {'observation_period_num': 55, 'train_rates': 0.8732401381007949, 'learning_rate': 0.0007487225893812287, 'batch_size': 218, 'step_size': 3, 'gamma': 0.8288124304363279}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 08:42:14,168][0m Trial 16 finished with value: 1.9317235193754498 and parameters: {'observation_period_num': 45, 'train_rates': 0.9217245478795258, 'learning_rate': 0.0009024526633011244, 'batch_size': 212, 'step_size': 5, 'gamma': 0.8312562850782733}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 08:47:18,836][0m Trial 17 finished with value: 0.07605402885923687 and parameters: {'observation_period_num': 56, 'train_rates': 0.8203630606743251, 'learning_rate': 0.0004429629515213049, 'batch_size': 97, 'step_size': 3, 'gamma': 0.7566894785472704}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 08:53:22,651][0m Trial 18 finished with value: 0.09184325504742685 and parameters: {'observation_period_num': 132, 'train_rates': 0.9352931501896787, 'learning_rate': 0.00010060601654733602, 'batch_size': 87, 'step_size': 6, 'gamma': 0.7502128355819813}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 08:58:37,179][0m Trial 19 finished with value: 0.1179038579908976 and parameters: {'observation_period_num': 66, 'train_rates': 0.8210417903698773, 'learning_rate': 0.0003313259848957367, 'batch_size': 92, 'step_size': 3, 'gamma': 0.9874005961598015}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 09:06:50,539][0m Trial 20 finished with value: 0.22549220354952476 and parameters: {'observation_period_num': 117, 'train_rates': 0.7086014152442557, 'learning_rate': 0.000417493505636828, 'batch_size': 52, 'step_size': 3, 'gamma': 0.7552999033578099}. Best is trial 14 with value: 0.07510415936699351.[0m
[32m[I 2025-01-05 09:11:31,668][0m Trial 21 finished with value: 0.07323743156366597 and parameters: {'observation_period_num': 61, 'train_rates': 0.8536662266641263, 'learning_rate': 0.0005546647217351485, 'batch_size': 110, 'step_size': 3, 'gamma': 0.7947360335121847}. Best is trial 21 with value: 0.07323743156366597.[0m
[32m[I 2025-01-05 09:16:17,144][0m Trial 22 finished with value: 0.05830945305171467 and parameters: {'observation_period_num': 32, 'train_rates': 0.8418118040438546, 'learning_rate': 0.0004944970805307357, 'batch_size': 110, 'step_size': 5, 'gamma': 0.7840356998834925}. Best is trial 22 with value: 0.05830945305171467.[0m
[32m[I 2025-01-05 09:21:27,767][0m Trial 23 finished with value: 0.052209214186640804 and parameters: {'observation_period_num': 28, 'train_rates': 0.9262226902221411, 'learning_rate': 8.214497020440184e-05, 'batch_size': 107, 'step_size': 5, 'gamma': 0.7905666745532622}. Best is trial 23 with value: 0.052209214186640804.[0m
[32m[I 2025-01-05 09:26:10,454][0m Trial 24 finished with value: 0.058598196607091604 and parameters: {'observation_period_num': 30, 'train_rates': 0.8504557280619557, 'learning_rate': 8.670915254238184e-05, 'batch_size': 113, 'step_size': 6, 'gamma': 0.782223458740509}. Best is trial 23 with value: 0.052209214186640804.[0m
[32m[I 2025-01-05 09:34:57,259][0m Trial 25 finished with value: 0.06843010558859333 and parameters: {'observation_period_num': 28, 'train_rates': 0.8446191811816287, 'learning_rate': 6.867484432428739e-05, 'batch_size': 57, 'step_size': 6, 'gamma': 0.7831442450811567}. Best is trial 23 with value: 0.052209214186640804.[0m
[32m[I 2025-01-05 09:40:10,007][0m Trial 26 finished with value: 0.05011754908732006 and parameters: {'observation_period_num': 29, 'train_rates': 0.9449394362997274, 'learning_rate': 0.00011070864243142297, 'batch_size': 115, 'step_size': 9, 'gamma': 0.8001641243897112}. Best is trial 26 with value: 0.05011754908732006.[0m
[32m[I 2025-01-05 09:48:08,200][0m Trial 27 finished with value: 0.0301037139739051 and parameters: {'observation_period_num': 12, 'train_rates': 0.9439941211340596, 'learning_rate': 4.1708753753200956e-05, 'batch_size': 68, 'step_size': 9, 'gamma': 0.8078156284335485}. Best is trial 27 with value: 0.0301037139739051.[0m
[32m[I 2025-01-05 09:56:29,950][0m Trial 28 finished with value: 0.029985775958888138 and parameters: {'observation_period_num': 7, 'train_rates': 0.9463629617802828, 'learning_rate': 4.1216716776338004e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.8058714037983135}. Best is trial 28 with value: 0.029985775958888138.[0m
[32m[I 2025-01-05 10:05:13,833][0m Trial 29 finished with value: 0.02927794759827001 and parameters: {'observation_period_num': 5, 'train_rates': 0.9501651222123992, 'learning_rate': 4.2309139631617654e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8138158844905959}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 10:25:13,655][0m Trial 30 finished with value: 0.03980087647930934 and parameters: {'observation_period_num': 7, 'train_rates': 0.9560588042219393, 'learning_rate': 4.354512308200282e-05, 'batch_size': 27, 'step_size': 14, 'gamma': 0.8497824903030398}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 10:49:34,248][0m Trial 31 finished with value: 0.03658142457752265 and parameters: {'observation_period_num': 10, 'train_rates': 0.9562800147554247, 'learning_rate': 3.688385432660203e-05, 'batch_size': 22, 'step_size': 14, 'gamma': 0.8439367744234931}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 11:00:27,322][0m Trial 32 finished with value: 0.029962240933225706 and parameters: {'observation_period_num': 6, 'train_rates': 0.956058193599089, 'learning_rate': 1.7903246995428834e-05, 'batch_size': 50, 'step_size': 13, 'gamma': 0.8183842079746307}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 11:08:32,140][0m Trial 33 finished with value: 0.047267512418329716 and parameters: {'observation_period_num': 20, 'train_rates': 0.8978398308059587, 'learning_rate': 1.6758694664389296e-05, 'batch_size': 65, 'step_size': 12, 'gamma': 0.8064754911437222}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 11:21:09,446][0m Trial 34 finished with value: 0.03126242899452336 and parameters: {'observation_period_num': 5, 'train_rates': 0.9567771184386225, 'learning_rate': 1.1063227623793028e-05, 'batch_size': 43, 'step_size': 12, 'gamma': 0.8193473308629551}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 11:28:20,223][0m Trial 35 finished with value: 0.06562939577776453 and parameters: {'observation_period_num': 46, 'train_rates': 0.881452626037196, 'learning_rate': 1.7100406306569e-05, 'batch_size': 73, 'step_size': 11, 'gamma': 0.7695266990934558}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 11:41:41,693][0m Trial 36 finished with value: 0.11092626903539501 and parameters: {'observation_period_num': 159, 'train_rates': 0.9080163776014212, 'learning_rate': 4.046900143888977e-06, 'batch_size': 37, 'step_size': 13, 'gamma': 0.837854791746946}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 11:53:33,212][0m Trial 37 finished with value: 0.14940740358322224 and parameters: {'observation_period_num': 78, 'train_rates': 0.967557500302793, 'learning_rate': 1.7947991881878362e-05, 'batch_size': 45, 'step_size': 10, 'gamma': 0.8632055729552331}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 12:01:42,673][0m Trial 38 finished with value: 0.05866677137581926 and parameters: {'observation_period_num': 42, 'train_rates': 0.9349550691166114, 'learning_rate': 5.5395468297680345e-05, 'batch_size': 66, 'step_size': 9, 'gamma': 0.8140577432253331}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 12:05:38,235][0m Trial 39 finished with value: 0.061747107916452416 and parameters: {'observation_period_num': 18, 'train_rates': 0.7963042628990527, 'learning_rate': 9.85100996151862e-06, 'batch_size': 166, 'step_size': 12, 'gamma': 0.8249451154899075}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 12:19:43,606][0m Trial 40 finished with value: 0.18387751776555722 and parameters: {'observation_period_num': 170, 'train_rates': 0.8735932085661464, 'learning_rate': 2.8542727750764133e-05, 'batch_size': 34, 'step_size': 15, 'gamma': 0.8826887785624651}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 12:31:10,415][0m Trial 41 finished with value: 0.033818477957888886 and parameters: {'observation_period_num': 5, 'train_rates': 0.9542788727095142, 'learning_rate': 7.264228777746773e-06, 'batch_size': 47, 'step_size': 12, 'gamma': 0.8147497769592975}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 12:37:52,401][0m Trial 42 finished with value: 0.04488874342421006 and parameters: {'observation_period_num': 19, 'train_rates': 0.9211776575764694, 'learning_rate': 1.982832888788236e-05, 'batch_size': 80, 'step_size': 13, 'gamma': 0.7732496521337828}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 12:47:21,327][0m Trial 43 finished with value: 0.040607381612062454 and parameters: {'observation_period_num': 7, 'train_rates': 0.9891279795591125, 'learning_rate': 1.2668830594590248e-05, 'batch_size': 60, 'step_size': 11, 'gamma': 0.8107315263923913}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 13:17:57,866][0m Trial 44 finished with value: 0.16245298137825526 and parameters: {'observation_period_num': 225, 'train_rates': 0.9676563272858119, 'learning_rate': 3.5911643651871513e-06, 'batch_size': 16, 'step_size': 9, 'gamma': 0.8219797063137204}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 13:31:07,978][0m Trial 45 finished with value: 0.0711768701798835 and parameters: {'observation_period_num': 38, 'train_rates': 0.8989528055881424, 'learning_rate': 4.548592041770772e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.8571839611791524}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 13:37:17,140][0m Trial 46 finished with value: 0.15924646143791496 and parameters: {'observation_period_num': 18, 'train_rates': 0.6830090295802962, 'learning_rate': 3.0477141482152047e-05, 'batch_size': 73, 'step_size': 10, 'gamma': 0.7977151788791883}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 13:47:23,968][0m Trial 47 finished with value: 0.11467092808555154 and parameters: {'observation_period_num': 51, 'train_rates': 0.9413811801986885, 'learning_rate': 1.1982257997659859e-05, 'batch_size': 52, 'step_size': 13, 'gamma': 0.8376767381093924}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 13:51:56,811][0m Trial 48 finished with value: 0.036523908376693726 and parameters: {'observation_period_num': 18, 'train_rates': 0.9673051443879775, 'learning_rate': 0.00013652125299594374, 'batch_size': 150, 'step_size': 14, 'gamma': 0.8748107411273037}. Best is trial 29 with value: 0.02927794759827001.[0m
[32m[I 2025-01-05 13:56:44,547][0m Trial 49 finished with value: 0.08526903604630565 and parameters: {'observation_period_num': 77, 'train_rates': 0.9154381905818118, 'learning_rate': 6.087653058874868e-06, 'batch_size': 125, 'step_size': 15, 'gamma': 0.8039804935523894}. Best is trial 29 with value: 0.02927794759827001.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 6, 'train_rates': 0.8897984827361468, 'learning_rate': 0.0009598979144146301, 'batch_size': 229, 'step_size': 3, 'gamma': 0.9687724715675567}
Epoch 1/300, trend Loss: 0.5555 | 0.2690
Epoch 2/300, trend Loss: 0.2493 | 0.2026
Epoch 3/300, trend Loss: 0.4494 | 0.1380
Epoch 4/300, trend Loss: 0.4090 | 0.3530
Epoch 5/300, trend Loss: 0.2809 | 0.1500
Epoch 6/300, trend Loss: 0.1940 | 0.1499
Epoch 7/300, trend Loss: 0.1580 | 0.1022
Epoch 8/300, trend Loss: 0.1473 | 0.1171
Epoch 9/300, trend Loss: 0.1393 | 0.0904
Epoch 10/300, trend Loss: 0.1371 | 0.1103
Epoch 11/300, trend Loss: 0.1326 | 0.0823
Epoch 12/300, trend Loss: 0.1291 | 0.0904
Epoch 13/300, trend Loss: 0.1258 | 0.0801
Epoch 14/300, trend Loss: 0.1254 | 0.0798
Epoch 15/300, trend Loss: 0.1235 | 0.0755
Epoch 16/300, trend Loss: 0.1272 | 0.0821
Epoch 17/300, trend Loss: 0.1279 | 0.0725
Epoch 18/300, trend Loss: 0.1374 | 0.1059
Epoch 19/300, trend Loss: 0.1333 | 0.0725
Epoch 20/300, trend Loss: 0.1353 | 0.1063
Epoch 21/300, trend Loss: 0.1268 | 0.0701
Epoch 22/300, trend Loss: 0.1237 | 0.0881
Epoch 23/300, trend Loss: 0.1183 | 0.0660
Epoch 24/300, trend Loss: 0.1185 | 0.0771
Epoch 25/300, trend Loss: 0.1152 | 0.0632
Epoch 26/300, trend Loss: 0.1167 | 0.0762
Epoch 27/300, trend Loss: 0.1146 | 0.0615
Epoch 28/300, trend Loss: 0.1168 | 0.0793
Epoch 29/300, trend Loss: 0.1159 | 0.0603
Epoch 30/300, trend Loss: 0.1166 | 0.0793
Epoch 31/300, trend Loss: 0.1171 | 0.0597
Epoch 32/300, trend Loss: 0.1145 | 0.0758
Epoch 33/300, trend Loss: 0.1181 | 0.0616
Epoch 34/300, trend Loss: 0.1190 | 0.0684
Epoch 35/300, trend Loss: 0.1133 | 0.0577
Epoch 36/300, trend Loss: 0.1129 | 0.0756
Epoch 37/300, trend Loss: 0.1083 | 0.0567
Epoch 38/300, trend Loss: 0.1074 | 0.0669
Epoch 39/300, trend Loss: 0.1052 | 0.0585
Epoch 40/300, trend Loss: 0.1045 | 0.0598
Epoch 41/300, trend Loss: 0.1034 | 0.0568
Epoch 42/300, trend Loss: 0.1027 | 0.0599
Epoch 43/300, trend Loss: 0.1020 | 0.0550
Epoch 44/300, trend Loss: 0.1016 | 0.0595
Epoch 45/300, trend Loss: 0.1009 | 0.0545
Epoch 46/300, trend Loss: 0.1004 | 0.0575
Epoch 47/300, trend Loss: 0.0999 | 0.0543
Epoch 48/300, trend Loss: 0.0994 | 0.0562
Epoch 49/300, trend Loss: 0.0989 | 0.0534
Epoch 50/300, trend Loss: 0.0984 | 0.0553
Epoch 51/300, trend Loss: 0.0980 | 0.0529
Epoch 52/300, trend Loss: 0.0975 | 0.0541
Epoch 53/300, trend Loss: 0.0971 | 0.0525
Epoch 54/300, trend Loss: 0.0967 | 0.0531
Epoch 55/300, trend Loss: 0.0963 | 0.0518
Epoch 56/300, trend Loss: 0.0960 | 0.0524
Epoch 57/300, trend Loss: 0.0956 | 0.0513
Epoch 58/300, trend Loss: 0.0953 | 0.0516
Epoch 59/300, trend Loss: 0.0950 | 0.0508
Epoch 60/300, trend Loss: 0.0947 | 0.0509
Epoch 61/300, trend Loss: 0.0944 | 0.0503
Epoch 62/300, trend Loss: 0.0942 | 0.0503
Epoch 63/300, trend Loss: 0.0939 | 0.0498
Epoch 64/300, trend Loss: 0.0937 | 0.0497
Epoch 65/300, trend Loss: 0.0935 | 0.0494
Epoch 66/300, trend Loss: 0.0933 | 0.0492
Epoch 67/300, trend Loss: 0.0931 | 0.0489
Epoch 68/300, trend Loss: 0.0928 | 0.0488
Epoch 69/300, trend Loss: 0.0927 | 0.0485
Epoch 70/300, trend Loss: 0.0925 | 0.0483
Epoch 71/300, trend Loss: 0.0923 | 0.0481
Epoch 72/300, trend Loss: 0.0921 | 0.0480
Epoch 73/300, trend Loss: 0.0919 | 0.0478
Epoch 74/300, trend Loss: 0.0917 | 0.0476
Epoch 75/300, trend Loss: 0.0916 | 0.0474
Epoch 76/300, trend Loss: 0.0914 | 0.0472
Epoch 77/300, trend Loss: 0.0913 | 0.0471
Epoch 78/300, trend Loss: 0.0911 | 0.0469
Epoch 79/300, trend Loss: 0.0909 | 0.0467
Epoch 80/300, trend Loss: 0.0908 | 0.0466
Epoch 81/300, trend Loss: 0.0906 | 0.0464
Epoch 82/300, trend Loss: 0.0905 | 0.0463
Epoch 83/300, trend Loss: 0.0903 | 0.0461
Epoch 84/300, trend Loss: 0.0902 | 0.0460
Epoch 85/300, trend Loss: 0.0901 | 0.0458
Epoch 86/300, trend Loss: 0.0899 | 0.0456
Epoch 87/300, trend Loss: 0.0898 | 0.0455
Epoch 88/300, trend Loss: 0.0896 | 0.0454
Epoch 89/300, trend Loss: 0.0895 | 0.0452
Epoch 90/300, trend Loss: 0.0894 | 0.0451
Epoch 91/300, trend Loss: 0.0892 | 0.0449
Epoch 92/300, trend Loss: 0.0891 | 0.0448
Epoch 93/300, trend Loss: 0.0890 | 0.0447
Epoch 94/300, trend Loss: 0.0889 | 0.0445
Epoch 95/300, trend Loss: 0.0887 | 0.0444
Epoch 96/300, trend Loss: 0.0886 | 0.0443
Epoch 97/300, trend Loss: 0.0885 | 0.0441
Epoch 98/300, trend Loss: 0.0884 | 0.0440
Epoch 99/300, trend Loss: 0.0882 | 0.0439
Epoch 100/300, trend Loss: 0.0881 | 0.0437
Epoch 101/300, trend Loss: 0.0880 | 0.0436
Epoch 102/300, trend Loss: 0.0879 | 0.0435
Epoch 103/300, trend Loss: 0.0878 | 0.0434
Epoch 104/300, trend Loss: 0.0876 | 0.0433
Epoch 105/300, trend Loss: 0.0875 | 0.0431
Epoch 106/300, trend Loss: 0.0874 | 0.0430
Epoch 107/300, trend Loss: 0.0873 | 0.0429
Epoch 108/300, trend Loss: 0.0872 | 0.0428
Epoch 109/300, trend Loss: 0.0871 | 0.0427
Epoch 110/300, trend Loss: 0.0870 | 0.0426
Epoch 111/300, trend Loss: 0.0869 | 0.0425
Epoch 112/300, trend Loss: 0.0868 | 0.0424
Epoch 113/300, trend Loss: 0.0867 | 0.0423
Epoch 114/300, trend Loss: 0.0865 | 0.0422
Epoch 115/300, trend Loss: 0.0864 | 0.0421
Epoch 116/300, trend Loss: 0.0863 | 0.0420
Epoch 117/300, trend Loss: 0.0862 | 0.0419
Epoch 118/300, trend Loss: 0.0861 | 0.0418
Epoch 119/300, trend Loss: 0.0860 | 0.0417
Epoch 120/300, trend Loss: 0.0859 | 0.0416
Epoch 121/300, trend Loss: 0.0858 | 0.0415
Epoch 122/300, trend Loss: 0.0857 | 0.0414
Epoch 123/300, trend Loss: 0.0856 | 0.0413
Epoch 124/300, trend Loss: 0.0855 | 0.0413
Epoch 125/300, trend Loss: 0.0855 | 0.0412
Epoch 126/300, trend Loss: 0.0854 | 0.0411
Epoch 127/300, trend Loss: 0.0853 | 0.0410
Epoch 128/300, trend Loss: 0.0852 | 0.0409
Epoch 129/300, trend Loss: 0.0851 | 0.0409
Epoch 130/300, trend Loss: 0.0850 | 0.0408
Epoch 131/300, trend Loss: 0.0849 | 0.0407
Epoch 132/300, trend Loss: 0.0848 | 0.0406
Epoch 133/300, trend Loss: 0.0848 | 0.0406
Epoch 134/300, trend Loss: 0.0847 | 0.0405
Epoch 135/300, trend Loss: 0.0846 | 0.0404
Epoch 136/300, trend Loss: 0.0845 | 0.0404
Epoch 137/300, trend Loss: 0.0844 | 0.0403
Epoch 138/300, trend Loss: 0.0844 | 0.0402
Epoch 139/300, trend Loss: 0.0843 | 0.0402
Epoch 140/300, trend Loss: 0.0842 | 0.0401
Epoch 141/300, trend Loss: 0.0841 | 0.0401
Epoch 142/300, trend Loss: 0.0841 | 0.0400
Epoch 143/300, trend Loss: 0.0840 | 0.0400
Epoch 144/300, trend Loss: 0.0839 | 0.0399
Epoch 145/300, trend Loss: 0.0839 | 0.0398
Epoch 146/300, trend Loss: 0.0838 | 0.0398
Epoch 147/300, trend Loss: 0.0837 | 0.0397
Epoch 148/300, trend Loss: 0.0837 | 0.0397
Epoch 149/300, trend Loss: 0.0836 | 0.0396
Epoch 150/300, trend Loss: 0.0836 | 0.0396
Epoch 151/300, trend Loss: 0.0835 | 0.0395
Epoch 152/300, trend Loss: 0.0834 | 0.0395
Epoch 153/300, trend Loss: 0.0834 | 0.0395
Epoch 154/300, trend Loss: 0.0833 | 0.0394
Epoch 155/300, trend Loss: 0.0833 | 0.0394
Epoch 156/300, trend Loss: 0.0832 | 0.0393
Epoch 157/300, trend Loss: 0.0832 | 0.0393
Epoch 158/300, trend Loss: 0.0831 | 0.0392
Epoch 159/300, trend Loss: 0.0831 | 0.0392
Epoch 160/300, trend Loss: 0.0830 | 0.0392
Epoch 161/300, trend Loss: 0.0830 | 0.0391
Epoch 162/300, trend Loss: 0.0829 | 0.0391
Epoch 163/300, trend Loss: 0.0829 | 0.0391
Epoch 164/300, trend Loss: 0.0828 | 0.0390
Epoch 165/300, trend Loss: 0.0828 | 0.0390
Epoch 166/300, trend Loss: 0.0828 | 0.0390
Epoch 167/300, trend Loss: 0.0827 | 0.0389
Epoch 168/300, trend Loss: 0.0827 | 0.0389
Epoch 169/300, trend Loss: 0.0826 | 0.0389
Epoch 170/300, trend Loss: 0.0826 | 0.0388
Epoch 171/300, trend Loss: 0.0825 | 0.0388
Epoch 172/300, trend Loss: 0.0825 | 0.0388
Epoch 173/300, trend Loss: 0.0825 | 0.0387
Epoch 174/300, trend Loss: 0.0824 | 0.0387
Epoch 175/300, trend Loss: 0.0824 | 0.0387
Epoch 176/300, trend Loss: 0.0824 | 0.0386
Epoch 177/300, trend Loss: 0.0823 | 0.0386
Epoch 178/300, trend Loss: 0.0823 | 0.0386
Epoch 179/300, trend Loss: 0.0823 | 0.0386
Epoch 180/300, trend Loss: 0.0822 | 0.0385
Epoch 181/300, trend Loss: 0.0822 | 0.0385
Epoch 182/300, trend Loss: 0.0822 | 0.0385
Epoch 183/300, trend Loss: 0.0821 | 0.0385
Epoch 184/300, trend Loss: 0.0821 | 0.0384
Epoch 185/300, trend Loss: 0.0821 | 0.0384
Epoch 186/300, trend Loss: 0.0820 | 0.0384
Epoch 187/300, trend Loss: 0.0820 | 0.0384
Epoch 188/300, trend Loss: 0.0820 | 0.0383
Epoch 189/300, trend Loss: 0.0820 | 0.0383
Epoch 190/300, trend Loss: 0.0819 | 0.0383
Epoch 191/300, trend Loss: 0.0819 | 0.0383
Epoch 192/300, trend Loss: 0.0819 | 0.0383
Epoch 193/300, trend Loss: 0.0819 | 0.0382
Epoch 194/300, trend Loss: 0.0818 | 0.0382
Epoch 195/300, trend Loss: 0.0818 | 0.0382
Epoch 196/300, trend Loss: 0.0818 | 0.0382
Epoch 197/300, trend Loss: 0.0818 | 0.0382
Epoch 198/300, trend Loss: 0.0817 | 0.0382
Epoch 199/300, trend Loss: 0.0817 | 0.0381
Epoch 200/300, trend Loss: 0.0817 | 0.0381
Epoch 201/300, trend Loss: 0.0817 | 0.0381
Epoch 202/300, trend Loss: 0.0817 | 0.0381
Epoch 203/300, trend Loss: 0.0816 | 0.0381
Epoch 204/300, trend Loss: 0.0816 | 0.0381
Epoch 205/300, trend Loss: 0.0816 | 0.0380
Epoch 206/300, trend Loss: 0.0816 | 0.0380
Epoch 207/300, trend Loss: 0.0816 | 0.0380
Epoch 208/300, trend Loss: 0.0815 | 0.0380
Epoch 209/300, trend Loss: 0.0815 | 0.0380
Epoch 210/300, trend Loss: 0.0815 | 0.0380
Epoch 211/300, trend Loss: 0.0815 | 0.0379
Epoch 212/300, trend Loss: 0.0815 | 0.0379
Epoch 213/300, trend Loss: 0.0815 | 0.0379
Epoch 214/300, trend Loss: 0.0814 | 0.0379
Epoch 215/300, trend Loss: 0.0814 | 0.0379
Epoch 216/300, trend Loss: 0.0814 | 0.0379
Epoch 217/300, trend Loss: 0.0814 | 0.0379
Epoch 218/300, trend Loss: 0.0814 | 0.0379
Epoch 219/300, trend Loss: 0.0814 | 0.0378
Epoch 220/300, trend Loss: 0.0813 | 0.0378
Epoch 221/300, trend Loss: 0.0813 | 0.0378
Epoch 222/300, trend Loss: 0.0813 | 0.0378
Epoch 223/300, trend Loss: 0.0813 | 0.0378
Epoch 224/300, trend Loss: 0.0813 | 0.0378
Epoch 225/300, trend Loss: 0.0813 | 0.0378
Epoch 226/300, trend Loss: 0.0813 | 0.0378
Epoch 227/300, trend Loss: 0.0813 | 0.0378
Epoch 228/300, trend Loss: 0.0812 | 0.0377
Epoch 229/300, trend Loss: 0.0812 | 0.0377
Epoch 230/300, trend Loss: 0.0812 | 0.0377
Epoch 231/300, trend Loss: 0.0812 | 0.0377
Epoch 232/300, trend Loss: 0.0812 | 0.0377
Epoch 233/300, trend Loss: 0.0812 | 0.0377
Epoch 234/300, trend Loss: 0.0812 | 0.0377
Epoch 235/300, trend Loss: 0.0812 | 0.0377
Epoch 236/300, trend Loss: 0.0812 | 0.0377
Epoch 237/300, trend Loss: 0.0811 | 0.0377
Epoch 238/300, trend Loss: 0.0811 | 0.0377
Epoch 239/300, trend Loss: 0.0811 | 0.0376
Epoch 240/300, trend Loss: 0.0811 | 0.0376
Epoch 241/300, trend Loss: 0.0811 | 0.0376
Epoch 242/300, trend Loss: 0.0811 | 0.0376
Epoch 243/300, trend Loss: 0.0811 | 0.0376
Epoch 244/300, trend Loss: 0.0811 | 0.0376
Epoch 245/300, trend Loss: 0.0811 | 0.0376
Epoch 246/300, trend Loss: 0.0811 | 0.0376
Epoch 247/300, trend Loss: 0.0810 | 0.0376
Epoch 248/300, trend Loss: 0.0810 | 0.0376
Epoch 249/300, trend Loss: 0.0810 | 0.0376
Epoch 250/300, trend Loss: 0.0810 | 0.0376
Epoch 251/300, trend Loss: 0.0810 | 0.0376
Epoch 252/300, trend Loss: 0.0810 | 0.0376
Epoch 253/300, trend Loss: 0.0810 | 0.0375
Epoch 254/300, trend Loss: 0.0810 | 0.0375
Epoch 255/300, trend Loss: 0.0810 | 0.0375
Epoch 256/300, trend Loss: 0.0810 | 0.0375
Epoch 257/300, trend Loss: 0.0810 | 0.0375
Epoch 258/300, trend Loss: 0.0810 | 0.0375
Epoch 259/300, trend Loss: 0.0810 | 0.0375
Epoch 260/300, trend Loss: 0.0809 | 0.0375
Epoch 261/300, trend Loss: 0.0809 | 0.0375
Epoch 262/300, trend Loss: 0.0809 | 0.0375
Epoch 263/300, trend Loss: 0.0809 | 0.0375
Epoch 264/300, trend Loss: 0.0809 | 0.0375
Epoch 265/300, trend Loss: 0.0809 | 0.0375
Epoch 266/300, trend Loss: 0.0809 | 0.0375
Epoch 267/300, trend Loss: 0.0809 | 0.0375
Epoch 268/300, trend Loss: 0.0809 | 0.0375
Epoch 269/300, trend Loss: 0.0809 | 0.0375
Epoch 270/300, trend Loss: 0.0809 | 0.0375
Epoch 271/300, trend Loss: 0.0809 | 0.0375
Epoch 272/300, trend Loss: 0.0809 | 0.0375
Epoch 273/300, trend Loss: 0.0809 | 0.0374
Epoch 274/300, trend Loss: 0.0809 | 0.0374
Epoch 275/300, trend Loss: 0.0809 | 0.0374
Epoch 276/300, trend Loss: 0.0809 | 0.0374
Epoch 277/300, trend Loss: 0.0808 | 0.0374
Epoch 278/300, trend Loss: 0.0808 | 0.0374
Epoch 279/300, trend Loss: 0.0808 | 0.0374
Epoch 280/300, trend Loss: 0.0808 | 0.0374
Epoch 281/300, trend Loss: 0.0808 | 0.0374
Epoch 282/300, trend Loss: 0.0808 | 0.0374
Epoch 283/300, trend Loss: 0.0808 | 0.0374
Epoch 284/300, trend Loss: 0.0808 | 0.0374
Epoch 285/300, trend Loss: 0.0808 | 0.0374
Epoch 286/300, trend Loss: 0.0808 | 0.0374
Epoch 287/300, trend Loss: 0.0808 | 0.0374
Epoch 288/300, trend Loss: 0.0808 | 0.0374
Epoch 289/300, trend Loss: 0.0808 | 0.0374
Epoch 290/300, trend Loss: 0.0808 | 0.0374
Epoch 291/300, trend Loss: 0.0808 | 0.0374
Epoch 292/300, trend Loss: 0.0808 | 0.0374
Epoch 293/300, trend Loss: 0.0808 | 0.0374
Epoch 294/300, trend Loss: 0.0808 | 0.0374
Epoch 295/300, trend Loss: 0.0808 | 0.0374
Epoch 296/300, trend Loss: 0.0808 | 0.0374
Epoch 297/300, trend Loss: 0.0808 | 0.0374
Epoch 298/300, trend Loss: 0.0808 | 0.0374
Epoch 299/300, trend Loss: 0.0808 | 0.0374
Epoch 300/300, trend Loss: 0.0808 | 0.0374
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.9266419745075964, 'learning_rate': 7.88099611941642e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8533799031056413}
Epoch 1/300, seasonal_0 Loss: 0.2625 | 0.2358
Epoch 2/300, seasonal_0 Loss: 0.1422 | 0.1465
Epoch 3/300, seasonal_0 Loss: 0.1408 | 0.0791
Epoch 4/300, seasonal_0 Loss: 0.1324 | 0.0848
Epoch 5/300, seasonal_0 Loss: 0.1088 | 0.0752
Epoch 6/300, seasonal_0 Loss: 0.0979 | 0.0714
Epoch 7/300, seasonal_0 Loss: 0.0928 | 0.0712
Epoch 8/300, seasonal_0 Loss: 0.0931 | 0.0745
Epoch 9/300, seasonal_0 Loss: 0.0900 | 0.0636
Epoch 10/300, seasonal_0 Loss: 0.0866 | 0.0578
Epoch 11/300, seasonal_0 Loss: 0.0834 | 0.0549
Epoch 12/300, seasonal_0 Loss: 0.0810 | 0.0533
Epoch 13/300, seasonal_0 Loss: 0.0795 | 0.0488
Epoch 14/300, seasonal_0 Loss: 0.0759 | 0.0465
Epoch 15/300, seasonal_0 Loss: 0.0724 | 0.0409
Epoch 16/300, seasonal_0 Loss: 0.0709 | 0.0382
Epoch 17/300, seasonal_0 Loss: 0.0680 | 0.0366
Epoch 18/300, seasonal_0 Loss: 0.0660 | 0.0358
Epoch 19/300, seasonal_0 Loss: 0.0643 | 0.0348
Epoch 20/300, seasonal_0 Loss: 0.0629 | 0.0342
Epoch 21/300, seasonal_0 Loss: 0.0620 | 0.0358
Epoch 22/300, seasonal_0 Loss: 0.0619 | 0.0372
Epoch 23/300, seasonal_0 Loss: 0.0616 | 0.0344
Epoch 24/300, seasonal_0 Loss: 0.0609 | 0.0335
Epoch 25/300, seasonal_0 Loss: 0.0599 | 0.0320
Epoch 26/300, seasonal_0 Loss: 0.0587 | 0.0315
Epoch 27/300, seasonal_0 Loss: 0.0577 | 0.0313
Epoch 28/300, seasonal_0 Loss: 0.0573 | 0.0313
Epoch 29/300, seasonal_0 Loss: 0.0570 | 0.0318
Epoch 30/300, seasonal_0 Loss: 0.0565 | 0.0316
Epoch 31/300, seasonal_0 Loss: 0.0562 | 0.0325
Epoch 32/300, seasonal_0 Loss: 0.0557 | 0.0318
Epoch 33/300, seasonal_0 Loss: 0.0555 | 0.0323
Epoch 34/300, seasonal_0 Loss: 0.0550 | 0.0318
Epoch 35/300, seasonal_0 Loss: 0.0551 | 0.0327
Epoch 36/300, seasonal_0 Loss: 0.0546 | 0.0323
Epoch 37/300, seasonal_0 Loss: 0.0547 | 0.0338
Epoch 38/300, seasonal_0 Loss: 0.0543 | 0.0336
Epoch 39/300, seasonal_0 Loss: 0.0543 | 0.0350
Epoch 40/300, seasonal_0 Loss: 0.0540 | 0.0345
Epoch 41/300, seasonal_0 Loss: 0.0539 | 0.0338
Epoch 42/300, seasonal_0 Loss: 0.0538 | 0.0329
Epoch 43/300, seasonal_0 Loss: 0.0536 | 0.0317
Epoch 44/300, seasonal_0 Loss: 0.0534 | 0.0315
Epoch 45/300, seasonal_0 Loss: 0.0530 | 0.0312
Epoch 46/300, seasonal_0 Loss: 0.0526 | 0.0312
Epoch 47/300, seasonal_0 Loss: 0.0521 | 0.0312
Epoch 48/300, seasonal_0 Loss: 0.0518 | 0.0311
Epoch 49/300, seasonal_0 Loss: 0.0515 | 0.0312
Epoch 50/300, seasonal_0 Loss: 0.0513 | 0.0310
Epoch 51/300, seasonal_0 Loss: 0.0511 | 0.0312
Epoch 52/300, seasonal_0 Loss: 0.0509 | 0.0310
Epoch 53/300, seasonal_0 Loss: 0.0508 | 0.0311
Epoch 54/300, seasonal_0 Loss: 0.0506 | 0.0310
Epoch 55/300, seasonal_0 Loss: 0.0506 | 0.0311
Epoch 56/300, seasonal_0 Loss: 0.0505 | 0.0309
Epoch 57/300, seasonal_0 Loss: 0.0504 | 0.0310
Epoch 58/300, seasonal_0 Loss: 0.0503 | 0.0309
Epoch 59/300, seasonal_0 Loss: 0.0503 | 0.0310
Epoch 60/300, seasonal_0 Loss: 0.0502 | 0.0309
Epoch 61/300, seasonal_0 Loss: 0.0501 | 0.0310
Epoch 62/300, seasonal_0 Loss: 0.0501 | 0.0310
Epoch 63/300, seasonal_0 Loss: 0.0500 | 0.0310
Epoch 64/300, seasonal_0 Loss: 0.0499 | 0.0310
Epoch 65/300, seasonal_0 Loss: 0.0499 | 0.0310
Epoch 66/300, seasonal_0 Loss: 0.0498 | 0.0310
Epoch 67/300, seasonal_0 Loss: 0.0497 | 0.0308
Epoch 68/300, seasonal_0 Loss: 0.0496 | 0.0309
Epoch 69/300, seasonal_0 Loss: 0.0496 | 0.0307
Epoch 70/300, seasonal_0 Loss: 0.0495 | 0.0308
Epoch 71/300, seasonal_0 Loss: 0.0494 | 0.0307
Epoch 72/300, seasonal_0 Loss: 0.0494 | 0.0307
Epoch 73/300, seasonal_0 Loss: 0.0493 | 0.0307
Epoch 74/300, seasonal_0 Loss: 0.0493 | 0.0307
Epoch 75/300, seasonal_0 Loss: 0.0492 | 0.0307
Epoch 76/300, seasonal_0 Loss: 0.0492 | 0.0307
Epoch 77/300, seasonal_0 Loss: 0.0492 | 0.0307
Epoch 78/300, seasonal_0 Loss: 0.0491 | 0.0307
Epoch 79/300, seasonal_0 Loss: 0.0491 | 0.0307
Epoch 80/300, seasonal_0 Loss: 0.0491 | 0.0307
Epoch 81/300, seasonal_0 Loss: 0.0491 | 0.0307
Epoch 82/300, seasonal_0 Loss: 0.0490 | 0.0307
Epoch 83/300, seasonal_0 Loss: 0.0490 | 0.0307
Epoch 84/300, seasonal_0 Loss: 0.0490 | 0.0307
Epoch 85/300, seasonal_0 Loss: 0.0490 | 0.0307
Epoch 86/300, seasonal_0 Loss: 0.0490 | 0.0307
Epoch 87/300, seasonal_0 Loss: 0.0490 | 0.0306
Epoch 88/300, seasonal_0 Loss: 0.0490 | 0.0306
Epoch 89/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 90/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 91/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 92/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 93/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 94/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 95/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 96/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 97/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 98/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 99/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 100/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 101/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 102/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 103/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 104/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 105/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 106/300, seasonal_0 Loss: 0.0489 | 0.0306
Epoch 107/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 108/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 109/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 110/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 111/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 112/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 113/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 114/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 115/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 116/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 117/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 118/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 119/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 120/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 121/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 122/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 123/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 124/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 125/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 126/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 127/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 128/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 129/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 130/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 131/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 132/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 133/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 134/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 135/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 136/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 137/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 138/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 139/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 140/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 141/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 142/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 143/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 144/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 145/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 146/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 147/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 148/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 149/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 150/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 151/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 152/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 153/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 154/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 155/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 156/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 157/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 158/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 159/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 160/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 161/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 162/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 163/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 164/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 165/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 166/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 167/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 168/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 169/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 170/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 171/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 172/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 173/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 174/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 175/300, seasonal_0 Loss: 0.0488 | 0.0306
Epoch 176/300, seasonal_0 Loss: 0.0488 | 0.0306
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 5, 'train_rates': 0.8238761097810764, 'learning_rate': 3.023212087541594e-05, 'batch_size': 193, 'step_size': 6, 'gamma': 0.9555136294121471}
Epoch 1/300, seasonal_1 Loss: 0.6442 | 0.2162
Epoch 2/300, seasonal_1 Loss: 0.2429 | 0.3419
Epoch 3/300, seasonal_1 Loss: 0.3208 | 0.1986
Epoch 4/300, seasonal_1 Loss: 0.4007 | 0.1366
Epoch 5/300, seasonal_1 Loss: 0.1843 | 0.1409
Epoch 6/300, seasonal_1 Loss: 0.3253 | 0.1159
Epoch 7/300, seasonal_1 Loss: 0.2045 | 0.0946
Epoch 8/300, seasonal_1 Loss: 0.2258 | 0.1136
Epoch 9/300, seasonal_1 Loss: 0.1798 | 0.0898
Epoch 10/300, seasonal_1 Loss: 0.2199 | 0.1285
Epoch 11/300, seasonal_1 Loss: 0.1791 | 0.1040
Epoch 12/300, seasonal_1 Loss: 0.1524 | 0.0821
Epoch 13/300, seasonal_1 Loss: 0.1434 | 0.0828
Epoch 14/300, seasonal_1 Loss: 0.1507 | 0.1021
Epoch 15/300, seasonal_1 Loss: 0.1391 | 0.0812
Epoch 16/300, seasonal_1 Loss: 0.1400 | 0.0824
Epoch 17/300, seasonal_1 Loss: 0.1318 | 0.0781
Epoch 18/300, seasonal_1 Loss: 0.1343 | 0.0804
Epoch 19/300, seasonal_1 Loss: 0.1268 | 0.0725
Epoch 20/300, seasonal_1 Loss: 0.1309 | 0.0794
Epoch 21/300, seasonal_1 Loss: 0.1247 | 0.0712
Epoch 22/300, seasonal_1 Loss: 0.1261 | 0.0756
Epoch 23/300, seasonal_1 Loss: 0.1202 | 0.0680
Epoch 24/300, seasonal_1 Loss: 0.1226 | 0.0727
Epoch 25/300, seasonal_1 Loss: 0.1175 | 0.0665
Epoch 26/300, seasonal_1 Loss: 0.1185 | 0.0704
Epoch 27/300, seasonal_1 Loss: 0.1145 | 0.0643
Epoch 28/300, seasonal_1 Loss: 0.1152 | 0.0685
Epoch 29/300, seasonal_1 Loss: 0.1119 | 0.0629
Epoch 30/300, seasonal_1 Loss: 0.1122 | 0.0659
Epoch 31/300, seasonal_1 Loss: 0.1093 | 0.0615
Epoch 32/300, seasonal_1 Loss: 0.1092 | 0.0643
Epoch 33/300, seasonal_1 Loss: 0.1072 | 0.0604
Epoch 34/300, seasonal_1 Loss: 0.1068 | 0.0629
Epoch 35/300, seasonal_1 Loss: 0.1050 | 0.0593
Epoch 36/300, seasonal_1 Loss: 0.1047 | 0.0613
Epoch 37/300, seasonal_1 Loss: 0.1032 | 0.0585
Epoch 38/300, seasonal_1 Loss: 0.1026 | 0.0601
Epoch 39/300, seasonal_1 Loss: 0.1016 | 0.0577
Epoch 40/300, seasonal_1 Loss: 0.1010 | 0.0590
Epoch 41/300, seasonal_1 Loss: 0.1000 | 0.0568
Epoch 42/300, seasonal_1 Loss: 0.0995 | 0.0578
Epoch 43/300, seasonal_1 Loss: 0.0987 | 0.0559
Epoch 44/300, seasonal_1 Loss: 0.0981 | 0.0568
Epoch 45/300, seasonal_1 Loss: 0.0974 | 0.0551
Epoch 46/300, seasonal_1 Loss: 0.0968 | 0.0558
Epoch 47/300, seasonal_1 Loss: 0.0963 | 0.0542
Epoch 48/300, seasonal_1 Loss: 0.0957 | 0.0547
Epoch 49/300, seasonal_1 Loss: 0.0952 | 0.0534
Epoch 50/300, seasonal_1 Loss: 0.0946 | 0.0538
Epoch 51/300, seasonal_1 Loss: 0.0942 | 0.0526
Epoch 52/300, seasonal_1 Loss: 0.0936 | 0.0529
Epoch 53/300, seasonal_1 Loss: 0.0932 | 0.0518
Epoch 54/300, seasonal_1 Loss: 0.0927 | 0.0519
Epoch 55/300, seasonal_1 Loss: 0.0924 | 0.0510
Epoch 56/300, seasonal_1 Loss: 0.0919 | 0.0511
Epoch 57/300, seasonal_1 Loss: 0.0917 | 0.0504
Epoch 58/300, seasonal_1 Loss: 0.0913 | 0.0505
Epoch 59/300, seasonal_1 Loss: 0.0910 | 0.0497
Epoch 60/300, seasonal_1 Loss: 0.0907 | 0.0497
Epoch 61/300, seasonal_1 Loss: 0.0905 | 0.0491
Epoch 62/300, seasonal_1 Loss: 0.0902 | 0.0492
Epoch 63/300, seasonal_1 Loss: 0.0900 | 0.0487
Epoch 64/300, seasonal_1 Loss: 0.0897 | 0.0486
Epoch 65/300, seasonal_1 Loss: 0.0896 | 0.0481
Epoch 66/300, seasonal_1 Loss: 0.0893 | 0.0481
Epoch 67/300, seasonal_1 Loss: 0.0891 | 0.0478
Epoch 68/300, seasonal_1 Loss: 0.0889 | 0.0477
Epoch 69/300, seasonal_1 Loss: 0.0888 | 0.0473
Epoch 70/300, seasonal_1 Loss: 0.0886 | 0.0472
Epoch 71/300, seasonal_1 Loss: 0.0884 | 0.0469
Epoch 72/300, seasonal_1 Loss: 0.0882 | 0.0468
Epoch 73/300, seasonal_1 Loss: 0.0881 | 0.0466
Epoch 74/300, seasonal_1 Loss: 0.0879 | 0.0464
Epoch 75/300, seasonal_1 Loss: 0.0877 | 0.0461
Epoch 76/300, seasonal_1 Loss: 0.0876 | 0.0460
Epoch 77/300, seasonal_1 Loss: 0.0874 | 0.0458
Epoch 78/300, seasonal_1 Loss: 0.0873 | 0.0457
Epoch 79/300, seasonal_1 Loss: 0.0871 | 0.0455
Epoch 80/300, seasonal_1 Loss: 0.0869 | 0.0453
Epoch 81/300, seasonal_1 Loss: 0.0868 | 0.0451
Epoch 82/300, seasonal_1 Loss: 0.0867 | 0.0450
Epoch 83/300, seasonal_1 Loss: 0.0865 | 0.0448
Epoch 84/300, seasonal_1 Loss: 0.0864 | 0.0447
Epoch 85/300, seasonal_1 Loss: 0.0862 | 0.0446
Epoch 86/300, seasonal_1 Loss: 0.0861 | 0.0445
Epoch 87/300, seasonal_1 Loss: 0.0860 | 0.0443
Epoch 88/300, seasonal_1 Loss: 0.0859 | 0.0442
Epoch 89/300, seasonal_1 Loss: 0.0858 | 0.0440
Epoch 90/300, seasonal_1 Loss: 0.0856 | 0.0439
Epoch 91/300, seasonal_1 Loss: 0.0855 | 0.0439
Epoch 92/300, seasonal_1 Loss: 0.0854 | 0.0438
Epoch 93/300, seasonal_1 Loss: 0.0853 | 0.0436
Epoch 94/300, seasonal_1 Loss: 0.0852 | 0.0435
Epoch 95/300, seasonal_1 Loss: 0.0852 | 0.0434
Epoch 96/300, seasonal_1 Loss: 0.0850 | 0.0433
Epoch 97/300, seasonal_1 Loss: 0.0849 | 0.0434
Epoch 98/300, seasonal_1 Loss: 0.0850 | 0.0432
Epoch 99/300, seasonal_1 Loss: 0.0846 | 0.0430
Epoch 100/300, seasonal_1 Loss: 0.0847 | 0.0431
Epoch 101/300, seasonal_1 Loss: 0.0849 | 0.0428
Epoch 102/300, seasonal_1 Loss: 0.0844 | 0.0428
Epoch 103/300, seasonal_1 Loss: 0.0849 | 0.0430
Epoch 104/300, seasonal_1 Loss: 0.0847 | 0.0427
Epoch 105/300, seasonal_1 Loss: 0.0845 | 0.0431
Epoch 106/300, seasonal_1 Loss: 0.0859 | 0.0424
Epoch 107/300, seasonal_1 Loss: 0.0843 | 0.0425
Epoch 108/300, seasonal_1 Loss: 0.0863 | 0.0432
Epoch 109/300, seasonal_1 Loss: 0.0857 | 0.0429
Epoch 110/300, seasonal_1 Loss: 0.0863 | 0.0431
Epoch 111/300, seasonal_1 Loss: 0.0875 | 0.0416
Epoch 112/300, seasonal_1 Loss: 0.0855 | 0.0428
Epoch 113/300, seasonal_1 Loss: 0.0883 | 0.0428
Epoch 114/300, seasonal_1 Loss: 0.0861 | 0.0429
Epoch 115/300, seasonal_1 Loss: 0.0885 | 0.0418
Epoch 116/300, seasonal_1 Loss: 0.0856 | 0.0421
Epoch 117/300, seasonal_1 Loss: 0.0870 | 0.0430
Epoch 118/300, seasonal_1 Loss: 0.0860 | 0.0424
Epoch 119/300, seasonal_1 Loss: 0.0852 | 0.0418
Epoch 120/300, seasonal_1 Loss: 0.0848 | 0.0418
Epoch 121/300, seasonal_1 Loss: 0.0837 | 0.0423
Epoch 122/300, seasonal_1 Loss: 0.0842 | 0.0418
Epoch 123/300, seasonal_1 Loss: 0.0830 | 0.0413
Epoch 124/300, seasonal_1 Loss: 0.0832 | 0.0417
Epoch 125/300, seasonal_1 Loss: 0.0827 | 0.0417
Epoch 126/300, seasonal_1 Loss: 0.0827 | 0.0413
Epoch 127/300, seasonal_1 Loss: 0.0824 | 0.0413
Epoch 128/300, seasonal_1 Loss: 0.0823 | 0.0414
Epoch 129/300, seasonal_1 Loss: 0.0823 | 0.0412
Epoch 130/300, seasonal_1 Loss: 0.0821 | 0.0411
Epoch 131/300, seasonal_1 Loss: 0.0821 | 0.0412
Epoch 132/300, seasonal_1 Loss: 0.0820 | 0.0411
Epoch 133/300, seasonal_1 Loss: 0.0819 | 0.0410
Epoch 134/300, seasonal_1 Loss: 0.0818 | 0.0409
Epoch 135/300, seasonal_1 Loss: 0.0818 | 0.0410
Epoch 136/300, seasonal_1 Loss: 0.0817 | 0.0409
Epoch 137/300, seasonal_1 Loss: 0.0817 | 0.0408
Epoch 138/300, seasonal_1 Loss: 0.0816 | 0.0408
Epoch 139/300, seasonal_1 Loss: 0.0815 | 0.0408
Epoch 140/300, seasonal_1 Loss: 0.0815 | 0.0407
Epoch 141/300, seasonal_1 Loss: 0.0814 | 0.0407
Epoch 142/300, seasonal_1 Loss: 0.0814 | 0.0407
Epoch 143/300, seasonal_1 Loss: 0.0813 | 0.0407
Epoch 144/300, seasonal_1 Loss: 0.0813 | 0.0406
Epoch 145/300, seasonal_1 Loss: 0.0812 | 0.0406
Epoch 146/300, seasonal_1 Loss: 0.0812 | 0.0406
Epoch 147/300, seasonal_1 Loss: 0.0811 | 0.0405
Epoch 148/300, seasonal_1 Loss: 0.0810 | 0.0405
Epoch 149/300, seasonal_1 Loss: 0.0810 | 0.0405
Epoch 150/300, seasonal_1 Loss: 0.0809 | 0.0404
Epoch 151/300, seasonal_1 Loss: 0.0809 | 0.0404
Epoch 152/300, seasonal_1 Loss: 0.0809 | 0.0404
Epoch 153/300, seasonal_1 Loss: 0.0808 | 0.0404
Epoch 154/300, seasonal_1 Loss: 0.0808 | 0.0403
Epoch 155/300, seasonal_1 Loss: 0.0807 | 0.0403
Epoch 156/300, seasonal_1 Loss: 0.0807 | 0.0403
Epoch 157/300, seasonal_1 Loss: 0.0806 | 0.0403
Epoch 158/300, seasonal_1 Loss: 0.0806 | 0.0402
Epoch 159/300, seasonal_1 Loss: 0.0805 | 0.0402
Epoch 160/300, seasonal_1 Loss: 0.0805 | 0.0402
Epoch 161/300, seasonal_1 Loss: 0.0804 | 0.0402
Epoch 162/300, seasonal_1 Loss: 0.0804 | 0.0401
Epoch 163/300, seasonal_1 Loss: 0.0803 | 0.0401
Epoch 164/300, seasonal_1 Loss: 0.0803 | 0.0401
Epoch 165/300, seasonal_1 Loss: 0.0803 | 0.0401
Epoch 166/300, seasonal_1 Loss: 0.0802 | 0.0400
Epoch 167/300, seasonal_1 Loss: 0.0802 | 0.0400
Epoch 168/300, seasonal_1 Loss: 0.0801 | 0.0400
Epoch 169/300, seasonal_1 Loss: 0.0801 | 0.0400
Epoch 170/300, seasonal_1 Loss: 0.0800 | 0.0399
Epoch 171/300, seasonal_1 Loss: 0.0800 | 0.0399
Epoch 172/300, seasonal_1 Loss: 0.0800 | 0.0399
Epoch 173/300, seasonal_1 Loss: 0.0799 | 0.0399
Epoch 174/300, seasonal_1 Loss: 0.0799 | 0.0399
Epoch 175/300, seasonal_1 Loss: 0.0798 | 0.0398
Epoch 176/300, seasonal_1 Loss: 0.0798 | 0.0398
Epoch 177/300, seasonal_1 Loss: 0.0798 | 0.0398
Epoch 178/300, seasonal_1 Loss: 0.0797 | 0.0398
Epoch 179/300, seasonal_1 Loss: 0.0797 | 0.0397
Epoch 180/300, seasonal_1 Loss: 0.0797 | 0.0397
Epoch 181/300, seasonal_1 Loss: 0.0796 | 0.0397
Epoch 182/300, seasonal_1 Loss: 0.0796 | 0.0397
Epoch 183/300, seasonal_1 Loss: 0.0795 | 0.0397
Epoch 184/300, seasonal_1 Loss: 0.0795 | 0.0396
Epoch 185/300, seasonal_1 Loss: 0.0795 | 0.0396
Epoch 186/300, seasonal_1 Loss: 0.0794 | 0.0396
Epoch 187/300, seasonal_1 Loss: 0.0794 | 0.0396
Epoch 188/300, seasonal_1 Loss: 0.0794 | 0.0395
Epoch 189/300, seasonal_1 Loss: 0.0793 | 0.0395
Epoch 190/300, seasonal_1 Loss: 0.0793 | 0.0395
Epoch 191/300, seasonal_1 Loss: 0.0793 | 0.0395
Epoch 192/300, seasonal_1 Loss: 0.0792 | 0.0395
Epoch 193/300, seasonal_1 Loss: 0.0792 | 0.0394
Epoch 194/300, seasonal_1 Loss: 0.0792 | 0.0394
Epoch 195/300, seasonal_1 Loss: 0.0791 | 0.0394
Epoch 196/300, seasonal_1 Loss: 0.0791 | 0.0394
Epoch 197/300, seasonal_1 Loss: 0.0791 | 0.0394
Epoch 198/300, seasonal_1 Loss: 0.0790 | 0.0393
Epoch 199/300, seasonal_1 Loss: 0.0790 | 0.0393
Epoch 200/300, seasonal_1 Loss: 0.0790 | 0.0393
Epoch 201/300, seasonal_1 Loss: 0.0789 | 0.0393
Epoch 202/300, seasonal_1 Loss: 0.0789 | 0.0393
Epoch 203/300, seasonal_1 Loss: 0.0789 | 0.0392
Epoch 204/300, seasonal_1 Loss: 0.0788 | 0.0392
Epoch 205/300, seasonal_1 Loss: 0.0788 | 0.0392
Epoch 206/300, seasonal_1 Loss: 0.0788 | 0.0392
Epoch 207/300, seasonal_1 Loss: 0.0787 | 0.0392
Epoch 208/300, seasonal_1 Loss: 0.0787 | 0.0391
Epoch 209/300, seasonal_1 Loss: 0.0787 | 0.0391
Epoch 210/300, seasonal_1 Loss: 0.0786 | 0.0391
Epoch 211/300, seasonal_1 Loss: 0.0786 | 0.0391
Epoch 212/300, seasonal_1 Loss: 0.0786 | 0.0391
Epoch 213/300, seasonal_1 Loss: 0.0786 | 0.0391
Epoch 214/300, seasonal_1 Loss: 0.0785 | 0.0390
Epoch 215/300, seasonal_1 Loss: 0.0785 | 0.0390
Epoch 216/300, seasonal_1 Loss: 0.0785 | 0.0390
Epoch 217/300, seasonal_1 Loss: 0.0784 | 0.0390
Epoch 218/300, seasonal_1 Loss: 0.0784 | 0.0390
Epoch 219/300, seasonal_1 Loss: 0.0784 | 0.0389
Epoch 220/300, seasonal_1 Loss: 0.0784 | 0.0389
Epoch 221/300, seasonal_1 Loss: 0.0783 | 0.0389
Epoch 222/300, seasonal_1 Loss: 0.0783 | 0.0389
Epoch 223/300, seasonal_1 Loss: 0.0783 | 0.0389
Epoch 224/300, seasonal_1 Loss: 0.0783 | 0.0389
Epoch 225/300, seasonal_1 Loss: 0.0782 | 0.0388
Epoch 226/300, seasonal_1 Loss: 0.0782 | 0.0388
Epoch 227/300, seasonal_1 Loss: 0.0782 | 0.0388
Epoch 228/300, seasonal_1 Loss: 0.0782 | 0.0388
Epoch 229/300, seasonal_1 Loss: 0.0781 | 0.0388
Epoch 230/300, seasonal_1 Loss: 0.0781 | 0.0387
Epoch 231/300, seasonal_1 Loss: 0.0781 | 0.0387
Epoch 232/300, seasonal_1 Loss: 0.0781 | 0.0387
Epoch 233/300, seasonal_1 Loss: 0.0780 | 0.0387
Epoch 234/300, seasonal_1 Loss: 0.0780 | 0.0387
Epoch 235/300, seasonal_1 Loss: 0.0780 | 0.0387
Epoch 236/300, seasonal_1 Loss: 0.0780 | 0.0386
Epoch 237/300, seasonal_1 Loss: 0.0779 | 0.0386
Epoch 238/300, seasonal_1 Loss: 0.0779 | 0.0386
Epoch 239/300, seasonal_1 Loss: 0.0779 | 0.0386
Epoch 240/300, seasonal_1 Loss: 0.0779 | 0.0386
Epoch 241/300, seasonal_1 Loss: 0.0778 | 0.0386
Epoch 242/300, seasonal_1 Loss: 0.0778 | 0.0386
Epoch 243/300, seasonal_1 Loss: 0.0778 | 0.0385
Epoch 244/300, seasonal_1 Loss: 0.0778 | 0.0385
Epoch 245/300, seasonal_1 Loss: 0.0777 | 0.0385
Epoch 246/300, seasonal_1 Loss: 0.0777 | 0.0385
Epoch 247/300, seasonal_1 Loss: 0.0777 | 0.0385
Epoch 248/300, seasonal_1 Loss: 0.0777 | 0.0385
Epoch 249/300, seasonal_1 Loss: 0.0777 | 0.0384
Epoch 250/300, seasonal_1 Loss: 0.0776 | 0.0384
Epoch 251/300, seasonal_1 Loss: 0.0776 | 0.0384
Epoch 252/300, seasonal_1 Loss: 0.0776 | 0.0384
Epoch 253/300, seasonal_1 Loss: 0.0776 | 0.0384
Epoch 254/300, seasonal_1 Loss: 0.0776 | 0.0384
Epoch 255/300, seasonal_1 Loss: 0.0775 | 0.0384
Epoch 256/300, seasonal_1 Loss: 0.0775 | 0.0383
Epoch 257/300, seasonal_1 Loss: 0.0775 | 0.0383
Epoch 258/300, seasonal_1 Loss: 0.0775 | 0.0383
Epoch 259/300, seasonal_1 Loss: 0.0775 | 0.0383
Epoch 260/300, seasonal_1 Loss: 0.0774 | 0.0383
Epoch 261/300, seasonal_1 Loss: 0.0774 | 0.0383
Epoch 262/300, seasonal_1 Loss: 0.0774 | 0.0383
Epoch 263/300, seasonal_1 Loss: 0.0774 | 0.0382
Epoch 264/300, seasonal_1 Loss: 0.0774 | 0.0382
Epoch 265/300, seasonal_1 Loss: 0.0773 | 0.0382
Epoch 266/300, seasonal_1 Loss: 0.0773 | 0.0382
Epoch 267/300, seasonal_1 Loss: 0.0773 | 0.0382
Epoch 268/300, seasonal_1 Loss: 0.0773 | 0.0382
Epoch 269/300, seasonal_1 Loss: 0.0773 | 0.0382
Epoch 270/300, seasonal_1 Loss: 0.0773 | 0.0382
Epoch 271/300, seasonal_1 Loss: 0.0772 | 0.0381
Epoch 272/300, seasonal_1 Loss: 0.0772 | 0.0381
Epoch 273/300, seasonal_1 Loss: 0.0772 | 0.0381
Epoch 274/300, seasonal_1 Loss: 0.0772 | 0.0381
Epoch 275/300, seasonal_1 Loss: 0.0772 | 0.0381
Epoch 276/300, seasonal_1 Loss: 0.0771 | 0.0381
Epoch 277/300, seasonal_1 Loss: 0.0771 | 0.0381
Epoch 278/300, seasonal_1 Loss: 0.0771 | 0.0381
Epoch 279/300, seasonal_1 Loss: 0.0771 | 0.0381
Epoch 280/300, seasonal_1 Loss: 0.0771 | 0.0380
Epoch 281/300, seasonal_1 Loss: 0.0771 | 0.0380
Epoch 282/300, seasonal_1 Loss: 0.0771 | 0.0380
Epoch 283/300, seasonal_1 Loss: 0.0770 | 0.0380
Epoch 284/300, seasonal_1 Loss: 0.0770 | 0.0380
Epoch 285/300, seasonal_1 Loss: 0.0770 | 0.0380
Epoch 286/300, seasonal_1 Loss: 0.0770 | 0.0380
Epoch 287/300, seasonal_1 Loss: 0.0770 | 0.0380
Epoch 288/300, seasonal_1 Loss: 0.0770 | 0.0380
Epoch 289/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 290/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 291/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 292/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 293/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 294/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 295/300, seasonal_1 Loss: 0.0769 | 0.0379
Epoch 296/300, seasonal_1 Loss: 0.0768 | 0.0379
Epoch 297/300, seasonal_1 Loss: 0.0768 | 0.0379
Epoch 298/300, seasonal_1 Loss: 0.0768 | 0.0379
Epoch 299/300, seasonal_1 Loss: 0.0768 | 0.0378
Epoch 300/300, seasonal_1 Loss: 0.0768 | 0.0378
Training seasonal_2 component with params: {'observation_period_num': 29, 'train_rates': 0.984767153773091, 'learning_rate': 0.00014048446779042743, 'batch_size': 256, 'step_size': 11, 'gamma': 0.8588557637610812}
Epoch 1/300, seasonal_2 Loss: 1.7217 | 0.2626
Epoch 2/300, seasonal_2 Loss: 0.3474 | 0.3267
Epoch 3/300, seasonal_2 Loss: 0.5862 | 1.1483
Epoch 4/300, seasonal_2 Loss: 0.4002 | 0.3511
Epoch 5/300, seasonal_2 Loss: 0.3880 | 0.3349
Epoch 6/300, seasonal_2 Loss: 0.3890 | 0.4555
Epoch 7/300, seasonal_2 Loss: 0.2995 | 0.1981
Epoch 8/300, seasonal_2 Loss: 0.4661 | 0.1929
Epoch 9/300, seasonal_2 Loss: 0.2249 | 0.1994
Epoch 10/300, seasonal_2 Loss: 0.2026 | 0.1280
Epoch 11/300, seasonal_2 Loss: 0.2306 | 0.1186
Epoch 12/300, seasonal_2 Loss: 0.2243 | 0.1246
Epoch 13/300, seasonal_2 Loss: 0.1762 | 0.2016
Epoch 14/300, seasonal_2 Loss: 0.1937 | 0.1329
Epoch 15/300, seasonal_2 Loss: 0.1848 | 0.1731
Epoch 16/300, seasonal_2 Loss: 0.1696 | 0.1180
Epoch 17/300, seasonal_2 Loss: 0.1387 | 0.1231
Epoch 18/300, seasonal_2 Loss: 0.1290 | 0.0834
Epoch 19/300, seasonal_2 Loss: 0.1226 | 0.1101
Epoch 20/300, seasonal_2 Loss: 0.1206 | 0.0699
Epoch 21/300, seasonal_2 Loss: 0.1193 | 0.1101
Epoch 22/300, seasonal_2 Loss: 0.1227 | 0.0743
Epoch 23/300, seasonal_2 Loss: 0.1314 | 0.1063
Epoch 24/300, seasonal_2 Loss: 0.1215 | 0.0713
Epoch 25/300, seasonal_2 Loss: 0.1522 | 0.1338
Epoch 26/300, seasonal_2 Loss: 0.1679 | 0.0932
Epoch 27/300, seasonal_2 Loss: 0.1724 | 0.0975
Epoch 28/300, seasonal_2 Loss: 0.1526 | 0.0838
Epoch 29/300, seasonal_2 Loss: 0.1467 | 0.1121
Epoch 30/300, seasonal_2 Loss: 0.1398 | 0.1309
Epoch 31/300, seasonal_2 Loss: 0.1366 | 0.0903
Epoch 32/300, seasonal_2 Loss: 0.1102 | 0.0717
Epoch 33/300, seasonal_2 Loss: 0.1047 | 0.0775
Epoch 34/300, seasonal_2 Loss: 0.0998 | 0.0586
Epoch 35/300, seasonal_2 Loss: 0.1060 | 0.0754
Epoch 36/300, seasonal_2 Loss: 0.1102 | 0.0691
Epoch 37/300, seasonal_2 Loss: 0.1174 | 0.0779
Epoch 38/300, seasonal_2 Loss: 0.1184 | 0.0953
Epoch 39/300, seasonal_2 Loss: 0.1190 | 0.0789
Epoch 40/300, seasonal_2 Loss: 0.1057 | 0.0798
Epoch 41/300, seasonal_2 Loss: 0.0933 | 0.0625
Epoch 42/300, seasonal_2 Loss: 0.0879 | 0.0551
Epoch 43/300, seasonal_2 Loss: 0.0856 | 0.0577
Epoch 44/300, seasonal_2 Loss: 0.0848 | 0.0504
Epoch 45/300, seasonal_2 Loss: 0.0861 | 0.0561
Epoch 46/300, seasonal_2 Loss: 0.0884 | 0.0555
Epoch 47/300, seasonal_2 Loss: 0.0918 | 0.0582
Epoch 48/300, seasonal_2 Loss: 0.0977 | 0.0720
Epoch 49/300, seasonal_2 Loss: 0.1014 | 0.0588
Epoch 50/300, seasonal_2 Loss: 0.0993 | 0.0768
Epoch 51/300, seasonal_2 Loss: 0.0916 | 0.0546
Epoch 52/300, seasonal_2 Loss: 0.0856 | 0.0604
Epoch 53/300, seasonal_2 Loss: 0.0814 | 0.0522
Epoch 54/300, seasonal_2 Loss: 0.0796 | 0.0524
Epoch 55/300, seasonal_2 Loss: 0.0793 | 0.0524
Epoch 56/300, seasonal_2 Loss: 0.0794 | 0.0566
Epoch 57/300, seasonal_2 Loss: 0.0805 | 0.0495
Epoch 58/300, seasonal_2 Loss: 0.0816 | 0.0617
Epoch 59/300, seasonal_2 Loss: 0.0807 | 0.0495
Epoch 60/300, seasonal_2 Loss: 0.0794 | 0.0578
Epoch 61/300, seasonal_2 Loss: 0.0785 | 0.0503
Epoch 62/300, seasonal_2 Loss: 0.0774 | 0.0592
Epoch 63/300, seasonal_2 Loss: 0.0765 | 0.0483
Epoch 64/300, seasonal_2 Loss: 0.0758 | 0.0568
Epoch 65/300, seasonal_2 Loss: 0.0748 | 0.0486
Epoch 66/300, seasonal_2 Loss: 0.0738 | 0.0533
Epoch 67/300, seasonal_2 Loss: 0.0728 | 0.0480
Epoch 68/300, seasonal_2 Loss: 0.0721 | 0.0531
Epoch 69/300, seasonal_2 Loss: 0.0718 | 0.0477
Epoch 70/300, seasonal_2 Loss: 0.0714 | 0.0519
Epoch 71/300, seasonal_2 Loss: 0.0711 | 0.0477
Epoch 72/300, seasonal_2 Loss: 0.0708 | 0.0509
Epoch 73/300, seasonal_2 Loss: 0.0703 | 0.0473
Epoch 74/300, seasonal_2 Loss: 0.0699 | 0.0508
Epoch 75/300, seasonal_2 Loss: 0.0696 | 0.0471
Epoch 76/300, seasonal_2 Loss: 0.0693 | 0.0498
Epoch 77/300, seasonal_2 Loss: 0.0690 | 0.0472
Epoch 78/300, seasonal_2 Loss: 0.0687 | 0.0493
Epoch 79/300, seasonal_2 Loss: 0.0684 | 0.0469
Epoch 80/300, seasonal_2 Loss: 0.0681 | 0.0484
Epoch 81/300, seasonal_2 Loss: 0.0679 | 0.0469
Epoch 82/300, seasonal_2 Loss: 0.0677 | 0.0478
Epoch 83/300, seasonal_2 Loss: 0.0675 | 0.0469
Epoch 84/300, seasonal_2 Loss: 0.0673 | 0.0474
Epoch 85/300, seasonal_2 Loss: 0.0671 | 0.0467
Epoch 86/300, seasonal_2 Loss: 0.0670 | 0.0470
Epoch 87/300, seasonal_2 Loss: 0.0668 | 0.0466
Epoch 88/300, seasonal_2 Loss: 0.0667 | 0.0467
Epoch 89/300, seasonal_2 Loss: 0.0665 | 0.0465
Epoch 90/300, seasonal_2 Loss: 0.0664 | 0.0464
Epoch 91/300, seasonal_2 Loss: 0.0662 | 0.0464
Epoch 92/300, seasonal_2 Loss: 0.0661 | 0.0462
Epoch 93/300, seasonal_2 Loss: 0.0660 | 0.0462
Epoch 94/300, seasonal_2 Loss: 0.0659 | 0.0461
Epoch 95/300, seasonal_2 Loss: 0.0657 | 0.0460
Epoch 96/300, seasonal_2 Loss: 0.0656 | 0.0459
Epoch 97/300, seasonal_2 Loss: 0.0655 | 0.0458
Epoch 98/300, seasonal_2 Loss: 0.0654 | 0.0457
Epoch 99/300, seasonal_2 Loss: 0.0653 | 0.0456
Epoch 100/300, seasonal_2 Loss: 0.0652 | 0.0456
Epoch 101/300, seasonal_2 Loss: 0.0651 | 0.0455
Epoch 102/300, seasonal_2 Loss: 0.0650 | 0.0454
Epoch 103/300, seasonal_2 Loss: 0.0649 | 0.0454
Epoch 104/300, seasonal_2 Loss: 0.0648 | 0.0453
Epoch 105/300, seasonal_2 Loss: 0.0647 | 0.0452
Epoch 106/300, seasonal_2 Loss: 0.0647 | 0.0451
Epoch 107/300, seasonal_2 Loss: 0.0646 | 0.0451
Epoch 108/300, seasonal_2 Loss: 0.0645 | 0.0450
Epoch 109/300, seasonal_2 Loss: 0.0644 | 0.0449
Epoch 110/300, seasonal_2 Loss: 0.0643 | 0.0449
Epoch 111/300, seasonal_2 Loss: 0.0643 | 0.0448
Epoch 112/300, seasonal_2 Loss: 0.0642 | 0.0448
Epoch 113/300, seasonal_2 Loss: 0.0641 | 0.0447
Epoch 114/300, seasonal_2 Loss: 0.0641 | 0.0447
Epoch 115/300, seasonal_2 Loss: 0.0640 | 0.0446
Epoch 116/300, seasonal_2 Loss: 0.0639 | 0.0446
Epoch 117/300, seasonal_2 Loss: 0.0639 | 0.0445
Epoch 118/300, seasonal_2 Loss: 0.0638 | 0.0444
Epoch 119/300, seasonal_2 Loss: 0.0637 | 0.0444
Epoch 120/300, seasonal_2 Loss: 0.0637 | 0.0444
Epoch 121/300, seasonal_2 Loss: 0.0636 | 0.0443
Epoch 122/300, seasonal_2 Loss: 0.0636 | 0.0443
Epoch 123/300, seasonal_2 Loss: 0.0635 | 0.0442
Epoch 124/300, seasonal_2 Loss: 0.0635 | 0.0442
Epoch 125/300, seasonal_2 Loss: 0.0634 | 0.0441
Epoch 126/300, seasonal_2 Loss: 0.0634 | 0.0441
Epoch 127/300, seasonal_2 Loss: 0.0633 | 0.0441
Epoch 128/300, seasonal_2 Loss: 0.0633 | 0.0440
Epoch 129/300, seasonal_2 Loss: 0.0632 | 0.0440
Epoch 130/300, seasonal_2 Loss: 0.0632 | 0.0439
Epoch 131/300, seasonal_2 Loss: 0.0631 | 0.0439
Epoch 132/300, seasonal_2 Loss: 0.0631 | 0.0439
Epoch 133/300, seasonal_2 Loss: 0.0630 | 0.0438
Epoch 134/300, seasonal_2 Loss: 0.0630 | 0.0438
Epoch 135/300, seasonal_2 Loss: 0.0630 | 0.0438
Epoch 136/300, seasonal_2 Loss: 0.0629 | 0.0437
Epoch 137/300, seasonal_2 Loss: 0.0629 | 0.0437
Epoch 138/300, seasonal_2 Loss: 0.0628 | 0.0437
Epoch 139/300, seasonal_2 Loss: 0.0628 | 0.0437
Epoch 140/300, seasonal_2 Loss: 0.0628 | 0.0436
Epoch 141/300, seasonal_2 Loss: 0.0627 | 0.0436
Epoch 142/300, seasonal_2 Loss: 0.0627 | 0.0436
Epoch 143/300, seasonal_2 Loss: 0.0627 | 0.0435
Epoch 144/300, seasonal_2 Loss: 0.0626 | 0.0435
Epoch 145/300, seasonal_2 Loss: 0.0626 | 0.0435
Epoch 146/300, seasonal_2 Loss: 0.0626 | 0.0435
Epoch 147/300, seasonal_2 Loss: 0.0625 | 0.0435
Epoch 148/300, seasonal_2 Loss: 0.0625 | 0.0434
Epoch 149/300, seasonal_2 Loss: 0.0625 | 0.0434
Epoch 150/300, seasonal_2 Loss: 0.0624 | 0.0434
Epoch 151/300, seasonal_2 Loss: 0.0624 | 0.0434
Epoch 152/300, seasonal_2 Loss: 0.0624 | 0.0433
Epoch 153/300, seasonal_2 Loss: 0.0624 | 0.0433
Epoch 154/300, seasonal_2 Loss: 0.0623 | 0.0433
Epoch 155/300, seasonal_2 Loss: 0.0623 | 0.0433
Epoch 156/300, seasonal_2 Loss: 0.0623 | 0.0433
Epoch 157/300, seasonal_2 Loss: 0.0623 | 0.0432
Epoch 158/300, seasonal_2 Loss: 0.0622 | 0.0432
Epoch 159/300, seasonal_2 Loss: 0.0622 | 0.0432
Epoch 160/300, seasonal_2 Loss: 0.0622 | 0.0432
Epoch 161/300, seasonal_2 Loss: 0.0622 | 0.0432
Epoch 162/300, seasonal_2 Loss: 0.0621 | 0.0432
Epoch 163/300, seasonal_2 Loss: 0.0621 | 0.0431
Epoch 164/300, seasonal_2 Loss: 0.0621 | 0.0431
Epoch 165/300, seasonal_2 Loss: 0.0621 | 0.0431
Epoch 166/300, seasonal_2 Loss: 0.0621 | 0.0431
Epoch 167/300, seasonal_2 Loss: 0.0620 | 0.0431
Epoch 168/300, seasonal_2 Loss: 0.0620 | 0.0431
Epoch 169/300, seasonal_2 Loss: 0.0620 | 0.0431
Epoch 170/300, seasonal_2 Loss: 0.0620 | 0.0430
Epoch 171/300, seasonal_2 Loss: 0.0620 | 0.0430
Epoch 172/300, seasonal_2 Loss: 0.0619 | 0.0430
Epoch 173/300, seasonal_2 Loss: 0.0619 | 0.0430
Epoch 174/300, seasonal_2 Loss: 0.0619 | 0.0430
Epoch 175/300, seasonal_2 Loss: 0.0619 | 0.0430
Epoch 176/300, seasonal_2 Loss: 0.0619 | 0.0430
Epoch 177/300, seasonal_2 Loss: 0.0619 | 0.0430
Epoch 178/300, seasonal_2 Loss: 0.0619 | 0.0429
Epoch 179/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 180/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 181/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 182/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 183/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 184/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 185/300, seasonal_2 Loss: 0.0618 | 0.0429
Epoch 186/300, seasonal_2 Loss: 0.0617 | 0.0429
Epoch 187/300, seasonal_2 Loss: 0.0617 | 0.0429
Epoch 188/300, seasonal_2 Loss: 0.0617 | 0.0428
Epoch 189/300, seasonal_2 Loss: 0.0617 | 0.0428
Epoch 190/300, seasonal_2 Loss: 0.0617 | 0.0428
Epoch 191/300, seasonal_2 Loss: 0.0617 | 0.0428
Epoch 192/300, seasonal_2 Loss: 0.0617 | 0.0428
Epoch 193/300, seasonal_2 Loss: 0.0617 | 0.0428
Epoch 194/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 195/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 196/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 197/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 198/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 199/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 200/300, seasonal_2 Loss: 0.0616 | 0.0428
Epoch 201/300, seasonal_2 Loss: 0.0616 | 0.0427
Epoch 202/300, seasonal_2 Loss: 0.0616 | 0.0427
Epoch 203/300, seasonal_2 Loss: 0.0616 | 0.0427
Epoch 204/300, seasonal_2 Loss: 0.0616 | 0.0427
Epoch 205/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 206/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 207/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 208/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 209/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 210/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 211/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 212/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 213/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 214/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 215/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 216/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 217/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 218/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 219/300, seasonal_2 Loss: 0.0615 | 0.0427
Epoch 220/300, seasonal_2 Loss: 0.0614 | 0.0427
Epoch 221/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 222/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 223/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 224/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 225/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 226/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 227/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 228/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 229/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 230/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 231/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 232/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 233/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 234/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 235/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 236/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 237/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 238/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 239/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 240/300, seasonal_2 Loss: 0.0614 | 0.0426
Epoch 241/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 242/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 243/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 244/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 245/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 246/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 247/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 248/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 249/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 250/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 251/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 252/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 253/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 254/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 255/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 256/300, seasonal_2 Loss: 0.0613 | 0.0426
Epoch 257/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 258/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 259/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 260/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 261/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 262/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 263/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 264/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 265/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 266/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 267/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 268/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 269/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 270/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 271/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 272/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 273/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 274/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 275/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 276/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 277/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 278/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 279/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 280/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 281/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 282/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 283/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 284/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 285/300, seasonal_2 Loss: 0.0613 | 0.0425
Epoch 286/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 287/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 288/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 289/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 290/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 291/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 292/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 293/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 294/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 295/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 296/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 297/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 298/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 299/300, seasonal_2 Loss: 0.0612 | 0.0425
Epoch 300/300, seasonal_2 Loss: 0.0612 | 0.0425
Training seasonal_3 component with params: {'observation_period_num': 13, 'train_rates': 0.8540909196148662, 'learning_rate': 4.5637397619110445e-05, 'batch_size': 198, 'step_size': 5, 'gamma': 0.9131569464916931}
Epoch 1/300, seasonal_3 Loss: 0.7368 | 0.2232
Epoch 2/300, seasonal_3 Loss: 0.2745 | 0.1794
Epoch 3/300, seasonal_3 Loss: 0.2854 | 0.1786
Epoch 4/300, seasonal_3 Loss: 0.2564 | 0.1298
Epoch 5/300, seasonal_3 Loss: 0.2785 | 0.3766
Epoch 6/300, seasonal_3 Loss: 0.3334 | 0.1269
Epoch 7/300, seasonal_3 Loss: 0.2281 | 0.1033
Epoch 8/300, seasonal_3 Loss: 0.1917 | 0.1074
Epoch 9/300, seasonal_3 Loss: 0.1671 | 0.0829
Epoch 10/300, seasonal_3 Loss: 0.1455 | 0.0808
Epoch 11/300, seasonal_3 Loss: 0.1620 | 0.0995
Epoch 12/300, seasonal_3 Loss: 0.1581 | 0.1020
Epoch 13/300, seasonal_3 Loss: 0.1767 | 0.1428
Epoch 14/300, seasonal_3 Loss: 0.1512 | 0.0955
Epoch 15/300, seasonal_3 Loss: 0.1360 | 0.0804
Epoch 16/300, seasonal_3 Loss: 0.1253 | 0.0759
Epoch 17/300, seasonal_3 Loss: 0.1281 | 0.0781
Epoch 18/300, seasonal_3 Loss: 0.1242 | 0.0732
Epoch 19/300, seasonal_3 Loss: 0.1320 | 0.0948
Epoch 20/300, seasonal_3 Loss: 0.1241 | 0.0719
Epoch 21/300, seasonal_3 Loss: 0.1270 | 0.0894
Epoch 22/300, seasonal_3 Loss: 0.1183 | 0.0682
Epoch 23/300, seasonal_3 Loss: 0.1203 | 0.0783
Epoch 24/300, seasonal_3 Loss: 0.1136 | 0.0645
Epoch 25/300, seasonal_3 Loss: 0.1156 | 0.0771
Epoch 26/300, seasonal_3 Loss: 0.1102 | 0.0629
Epoch 27/300, seasonal_3 Loss: 0.1117 | 0.0740
Epoch 28/300, seasonal_3 Loss: 0.1076 | 0.0612
Epoch 29/300, seasonal_3 Loss: 0.1084 | 0.0717
Epoch 30/300, seasonal_3 Loss: 0.1048 | 0.0595
Epoch 31/300, seasonal_3 Loss: 0.1054 | 0.0690
Epoch 32/300, seasonal_3 Loss: 0.1024 | 0.0585
Epoch 33/300, seasonal_3 Loss: 0.1030 | 0.0652
Epoch 34/300, seasonal_3 Loss: 0.1004 | 0.0572
Epoch 35/300, seasonal_3 Loss: 0.1006 | 0.0638
Epoch 36/300, seasonal_3 Loss: 0.0986 | 0.0563
Epoch 37/300, seasonal_3 Loss: 0.0986 | 0.0618
Epoch 38/300, seasonal_3 Loss: 0.0972 | 0.0558
Epoch 39/300, seasonal_3 Loss: 0.0970 | 0.0604
Epoch 40/300, seasonal_3 Loss: 0.0959 | 0.0550
Epoch 41/300, seasonal_3 Loss: 0.0956 | 0.0591
Epoch 42/300, seasonal_3 Loss: 0.0947 | 0.0546
Epoch 43/300, seasonal_3 Loss: 0.0945 | 0.0574
Epoch 44/300, seasonal_3 Loss: 0.0938 | 0.0543
Epoch 45/300, seasonal_3 Loss: 0.0935 | 0.0565
Epoch 46/300, seasonal_3 Loss: 0.0930 | 0.0540
Epoch 47/300, seasonal_3 Loss: 0.0927 | 0.0556
Epoch 48/300, seasonal_3 Loss: 0.0923 | 0.0539
Epoch 49/300, seasonal_3 Loss: 0.0920 | 0.0548
Epoch 50/300, seasonal_3 Loss: 0.0917 | 0.0537
Epoch 51/300, seasonal_3 Loss: 0.0915 | 0.0542
Epoch 52/300, seasonal_3 Loss: 0.0912 | 0.0536
Epoch 53/300, seasonal_3 Loss: 0.0910 | 0.0537
Epoch 54/300, seasonal_3 Loss: 0.0907 | 0.0534
Epoch 55/300, seasonal_3 Loss: 0.0905 | 0.0533
Epoch 56/300, seasonal_3 Loss: 0.0903 | 0.0531
Epoch 57/300, seasonal_3 Loss: 0.0900 | 0.0530
Epoch 58/300, seasonal_3 Loss: 0.0898 | 0.0528
Epoch 59/300, seasonal_3 Loss: 0.0896 | 0.0527
Epoch 60/300, seasonal_3 Loss: 0.0894 | 0.0525
Epoch 61/300, seasonal_3 Loss: 0.0893 | 0.0524
Epoch 62/300, seasonal_3 Loss: 0.0891 | 0.0523
Epoch 63/300, seasonal_3 Loss: 0.0889 | 0.0521
Epoch 64/300, seasonal_3 Loss: 0.0887 | 0.0520
Epoch 65/300, seasonal_3 Loss: 0.0886 | 0.0519
Epoch 66/300, seasonal_3 Loss: 0.0884 | 0.0518
Epoch 67/300, seasonal_3 Loss: 0.0883 | 0.0516
Epoch 68/300, seasonal_3 Loss: 0.0881 | 0.0515
Epoch 69/300, seasonal_3 Loss: 0.0880 | 0.0514
Epoch 70/300, seasonal_3 Loss: 0.0878 | 0.0513
Epoch 71/300, seasonal_3 Loss: 0.0877 | 0.0512
Epoch 72/300, seasonal_3 Loss: 0.0875 | 0.0511
Epoch 73/300, seasonal_3 Loss: 0.0874 | 0.0510
Epoch 74/300, seasonal_3 Loss: 0.0873 | 0.0509
Epoch 75/300, seasonal_3 Loss: 0.0872 | 0.0508
Epoch 76/300, seasonal_3 Loss: 0.0870 | 0.0507
Epoch 77/300, seasonal_3 Loss: 0.0869 | 0.0506
Epoch 78/300, seasonal_3 Loss: 0.0868 | 0.0505
Epoch 79/300, seasonal_3 Loss: 0.0867 | 0.0504
Epoch 80/300, seasonal_3 Loss: 0.0866 | 0.0503
Epoch 81/300, seasonal_3 Loss: 0.0865 | 0.0503
Epoch 82/300, seasonal_3 Loss: 0.0864 | 0.0502
Epoch 83/300, seasonal_3 Loss: 0.0863 | 0.0501
Epoch 84/300, seasonal_3 Loss: 0.0862 | 0.0500
Epoch 85/300, seasonal_3 Loss: 0.0861 | 0.0500
Epoch 86/300, seasonal_3 Loss: 0.0860 | 0.0499
Epoch 87/300, seasonal_3 Loss: 0.0860 | 0.0498
Epoch 88/300, seasonal_3 Loss: 0.0859 | 0.0497
Epoch 89/300, seasonal_3 Loss: 0.0858 | 0.0497
Epoch 90/300, seasonal_3 Loss: 0.0857 | 0.0496
Epoch 91/300, seasonal_3 Loss: 0.0856 | 0.0495
Epoch 92/300, seasonal_3 Loss: 0.0856 | 0.0495
Epoch 93/300, seasonal_3 Loss: 0.0855 | 0.0494
Epoch 94/300, seasonal_3 Loss: 0.0854 | 0.0494
Epoch 95/300, seasonal_3 Loss: 0.0853 | 0.0493
Epoch 96/300, seasonal_3 Loss: 0.0853 | 0.0493
Epoch 97/300, seasonal_3 Loss: 0.0852 | 0.0492
Epoch 98/300, seasonal_3 Loss: 0.0852 | 0.0492
Epoch 99/300, seasonal_3 Loss: 0.0851 | 0.0491
Epoch 100/300, seasonal_3 Loss: 0.0850 | 0.0491
Epoch 101/300, seasonal_3 Loss: 0.0850 | 0.0490
Epoch 102/300, seasonal_3 Loss: 0.0849 | 0.0490
Epoch 103/300, seasonal_3 Loss: 0.0849 | 0.0489
Epoch 104/300, seasonal_3 Loss: 0.0848 | 0.0489
Epoch 105/300, seasonal_3 Loss: 0.0848 | 0.0488
Epoch 106/300, seasonal_3 Loss: 0.0847 | 0.0488
Epoch 107/300, seasonal_3 Loss: 0.0847 | 0.0487
Epoch 108/300, seasonal_3 Loss: 0.0846 | 0.0487
Epoch 109/300, seasonal_3 Loss: 0.0846 | 0.0487
Epoch 110/300, seasonal_3 Loss: 0.0845 | 0.0486
Epoch 111/300, seasonal_3 Loss: 0.0845 | 0.0486
Epoch 112/300, seasonal_3 Loss: 0.0845 | 0.0486
Epoch 113/300, seasonal_3 Loss: 0.0844 | 0.0485
Epoch 114/300, seasonal_3 Loss: 0.0844 | 0.0485
Epoch 115/300, seasonal_3 Loss: 0.0843 | 0.0485
Epoch 116/300, seasonal_3 Loss: 0.0843 | 0.0484
Epoch 117/300, seasonal_3 Loss: 0.0843 | 0.0484
Epoch 118/300, seasonal_3 Loss: 0.0842 | 0.0484
Epoch 119/300, seasonal_3 Loss: 0.0842 | 0.0484
Epoch 120/300, seasonal_3 Loss: 0.0842 | 0.0483
Epoch 121/300, seasonal_3 Loss: 0.0842 | 0.0483
Epoch 122/300, seasonal_3 Loss: 0.0841 | 0.0483
Epoch 123/300, seasonal_3 Loss: 0.0841 | 0.0483
Epoch 124/300, seasonal_3 Loss: 0.0841 | 0.0482
Epoch 125/300, seasonal_3 Loss: 0.0840 | 0.0482
Epoch 126/300, seasonal_3 Loss: 0.0840 | 0.0482
Epoch 127/300, seasonal_3 Loss: 0.0840 | 0.0482
Epoch 128/300, seasonal_3 Loss: 0.0840 | 0.0481
Epoch 129/300, seasonal_3 Loss: 0.0839 | 0.0481
Epoch 130/300, seasonal_3 Loss: 0.0839 | 0.0481
Epoch 131/300, seasonal_3 Loss: 0.0839 | 0.0481
Epoch 132/300, seasonal_3 Loss: 0.0839 | 0.0481
Epoch 133/300, seasonal_3 Loss: 0.0839 | 0.0480
Epoch 134/300, seasonal_3 Loss: 0.0838 | 0.0480
Epoch 135/300, seasonal_3 Loss: 0.0838 | 0.0480
Epoch 136/300, seasonal_3 Loss: 0.0838 | 0.0480
Epoch 137/300, seasonal_3 Loss: 0.0838 | 0.0480
Epoch 138/300, seasonal_3 Loss: 0.0838 | 0.0480
Epoch 139/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 140/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 141/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 142/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 143/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 144/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 145/300, seasonal_3 Loss: 0.0837 | 0.0479
Epoch 146/300, seasonal_3 Loss: 0.0836 | 0.0479
Epoch 147/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 148/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 149/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 150/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 151/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 152/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 153/300, seasonal_3 Loss: 0.0836 | 0.0478
Epoch 154/300, seasonal_3 Loss: 0.0835 | 0.0478
Epoch 155/300, seasonal_3 Loss: 0.0835 | 0.0478
Epoch 156/300, seasonal_3 Loss: 0.0835 | 0.0478
Epoch 157/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 158/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 159/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 160/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 161/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 162/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 163/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 164/300, seasonal_3 Loss: 0.0835 | 0.0477
Epoch 165/300, seasonal_3 Loss: 0.0834 | 0.0477
Epoch 166/300, seasonal_3 Loss: 0.0834 | 0.0477
Epoch 167/300, seasonal_3 Loss: 0.0834 | 0.0477
Epoch 168/300, seasonal_3 Loss: 0.0834 | 0.0477
Epoch 169/300, seasonal_3 Loss: 0.0834 | 0.0477
Epoch 170/300, seasonal_3 Loss: 0.0834 | 0.0477
Epoch 171/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 172/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 173/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 174/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 175/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 176/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 177/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 178/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 179/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 180/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 181/300, seasonal_3 Loss: 0.0834 | 0.0476
Epoch 182/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 183/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 184/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 185/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 186/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 187/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 188/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 189/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 190/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 191/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 192/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 193/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 194/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 195/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 196/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 197/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 198/300, seasonal_3 Loss: 0.0833 | 0.0476
Epoch 199/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 200/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 201/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 202/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 203/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 204/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 205/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 206/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 207/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 208/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 209/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 210/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 211/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 212/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 213/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 214/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 215/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 216/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 217/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 218/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 219/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 220/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 221/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 222/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 223/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 224/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 225/300, seasonal_3 Loss: 0.0833 | 0.0475
Epoch 226/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 227/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 228/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 229/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 230/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 231/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 232/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 233/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 234/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 235/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 236/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 237/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 238/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 239/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 240/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 241/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 242/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 243/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 244/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 245/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 246/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 247/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 248/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 249/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 250/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 251/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 252/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 253/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 254/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 255/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 256/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 257/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 258/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 259/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 260/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 261/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 262/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 263/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 264/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 265/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 266/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 267/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 268/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 269/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 270/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 271/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 272/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 273/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 274/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 275/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 276/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 277/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 278/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 279/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 280/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 281/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 282/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 283/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 284/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 285/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 286/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 287/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 288/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 289/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 290/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 291/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 292/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 293/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 294/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 295/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 296/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 297/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 298/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 299/300, seasonal_3 Loss: 0.0832 | 0.0475
Epoch 300/300, seasonal_3 Loss: 0.0832 | 0.0475
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9501651222123992, 'learning_rate': 4.2309139631617654e-05, 'batch_size': 63, 'step_size': 13, 'gamma': 0.8138158844905959}
Epoch 1/300, resid Loss: 0.3194 | 0.1276
Epoch 2/300, resid Loss: 0.1270 | 0.0871
Epoch 3/300, resid Loss: 0.1195 | 0.1119
Epoch 4/300, resid Loss: 0.1136 | 0.0862
Epoch 5/300, resid Loss: 0.1075 | 0.0786
Epoch 6/300, resid Loss: 0.1044 | 0.0761
Epoch 7/300, resid Loss: 0.1017 | 0.0718
Epoch 8/300, resid Loss: 0.0986 | 0.0610
Epoch 9/300, resid Loss: 0.0980 | 0.0603
Epoch 10/300, resid Loss: 0.0957 | 0.0575
Epoch 11/300, resid Loss: 0.0944 | 0.0548
Epoch 12/300, resid Loss: 0.0933 | 0.0532
Epoch 13/300, resid Loss: 0.0922 | 0.0516
Epoch 14/300, resid Loss: 0.0919 | 0.0489
Epoch 15/300, resid Loss: 0.0886 | 0.0505
Epoch 16/300, resid Loss: 0.0894 | 0.0479
Epoch 17/300, resid Loss: 0.0829 | 0.0473
Epoch 18/300, resid Loss: 0.0823 | 0.0520
Epoch 19/300, resid Loss: 0.0841 | 0.0670
Epoch 20/300, resid Loss: 0.0858 | 0.0618
Epoch 21/300, resid Loss: 0.0849 | 0.0567
Epoch 22/300, resid Loss: 0.0909 | 0.0607
Epoch 23/300, resid Loss: 0.0958 | 0.0500
Epoch 24/300, resid Loss: 0.0960 | 0.0574
Epoch 25/300, resid Loss: 0.0940 | 0.0446
Epoch 26/300, resid Loss: 0.0952 | 0.0520
Epoch 27/300, resid Loss: 0.0875 | 0.0437
Epoch 28/300, resid Loss: 0.0818 | 0.0434
Epoch 29/300, resid Loss: 0.0792 | 0.0422
Epoch 30/300, resid Loss: 0.0792 | 0.0425
Epoch 31/300, resid Loss: 0.0775 | 0.0409
Epoch 32/300, resid Loss: 0.0773 | 0.0402
Epoch 33/300, resid Loss: 0.0758 | 0.0386
Epoch 34/300, resid Loss: 0.0750 | 0.0371
Epoch 35/300, resid Loss: 0.0740 | 0.0369
Epoch 36/300, resid Loss: 0.0726 | 0.0365
Epoch 37/300, resid Loss: 0.0721 | 0.0365
Epoch 38/300, resid Loss: 0.0726 | 0.0367
Epoch 39/300, resid Loss: 0.0729 | 0.0369
Epoch 40/300, resid Loss: 0.0726 | 0.0378
Epoch 41/300, resid Loss: 0.0711 | 0.0358
Epoch 42/300, resid Loss: 0.0697 | 0.0352
Epoch 43/300, resid Loss: 0.0696 | 0.0350
Epoch 44/300, resid Loss: 0.0697 | 0.0349
Epoch 45/300, resid Loss: 0.0699 | 0.0348
Epoch 46/300, resid Loss: 0.0698 | 0.0346
Epoch 47/300, resid Loss: 0.0697 | 0.0340
Epoch 48/300, resid Loss: 0.0689 | 0.0337
Epoch 49/300, resid Loss: 0.0680 | 0.0336
Epoch 50/300, resid Loss: 0.0673 | 0.0333
Epoch 51/300, resid Loss: 0.0668 | 0.0331
Epoch 52/300, resid Loss: 0.0665 | 0.0330
Epoch 53/300, resid Loss: 0.0667 | 0.0332
Epoch 54/300, resid Loss: 0.0672 | 0.0328
Epoch 55/300, resid Loss: 0.0671 | 0.0325
Epoch 56/300, resid Loss: 0.0669 | 0.0325
Epoch 57/300, resid Loss: 0.0666 | 0.0319
Epoch 58/300, resid Loss: 0.0661 | 0.0316
Epoch 59/300, resid Loss: 0.0655 | 0.0314
Epoch 60/300, resid Loss: 0.0653 | 0.0312
Epoch 61/300, resid Loss: 0.0652 | 0.0312
Epoch 62/300, resid Loss: 0.0656 | 0.0313
Epoch 63/300, resid Loss: 0.0659 | 0.0314
Epoch 64/300, resid Loss: 0.0658 | 0.0314
Epoch 65/300, resid Loss: 0.0654 | 0.0314
Epoch 66/300, resid Loss: 0.0652 | 0.0316
Epoch 67/300, resid Loss: 0.0645 | 0.0317
Epoch 68/300, resid Loss: 0.0639 | 0.0317
Epoch 69/300, resid Loss: 0.0638 | 0.0315
Epoch 70/300, resid Loss: 0.0640 | 0.0313
Epoch 71/300, resid Loss: 0.0643 | 0.0310
Epoch 72/300, resid Loss: 0.0642 | 0.0307
Epoch 73/300, resid Loss: 0.0640 | 0.0299
Epoch 74/300, resid Loss: 0.0635 | 0.0296
Epoch 75/300, resid Loss: 0.0629 | 0.0297
Epoch 76/300, resid Loss: 0.0629 | 0.0303
Epoch 77/300, resid Loss: 0.0631 | 0.0308
Epoch 78/300, resid Loss: 0.0631 | 0.0310
Epoch 79/300, resid Loss: 0.0629 | 0.0307
Epoch 80/300, resid Loss: 0.0631 | 0.0307
Epoch 81/300, resid Loss: 0.0634 | 0.0304
Epoch 82/300, resid Loss: 0.0632 | 0.0301
Epoch 83/300, resid Loss: 0.0628 | 0.0300
Epoch 84/300, resid Loss: 0.0622 | 0.0300
Epoch 85/300, resid Loss: 0.0617 | 0.0300
Epoch 86/300, resid Loss: 0.0616 | 0.0300
Epoch 87/300, resid Loss: 0.0617 | 0.0300
Epoch 88/300, resid Loss: 0.0622 | 0.0298
Epoch 89/300, resid Loss: 0.0625 | 0.0297
Epoch 90/300, resid Loss: 0.0623 | 0.0295
Epoch 91/300, resid Loss: 0.0617 | 0.0293
Epoch 92/300, resid Loss: 0.0616 | 0.0296
Epoch 93/300, resid Loss: 0.0616 | 0.0298
Epoch 94/300, resid Loss: 0.0616 | 0.0299
Epoch 95/300, resid Loss: 0.0617 | 0.0301
Epoch 96/300, resid Loss: 0.0617 | 0.0302
Epoch 97/300, resid Loss: 0.0617 | 0.0301
Epoch 98/300, resid Loss: 0.0616 | 0.0299
Epoch 99/300, resid Loss: 0.0616 | 0.0294
Epoch 100/300, resid Loss: 0.0619 | 0.0294
Epoch 101/300, resid Loss: 0.0625 | 0.0294
Epoch 102/300, resid Loss: 0.0627 | 0.0294
Epoch 103/300, resid Loss: 0.0623 | 0.0294
Epoch 104/300, resid Loss: 0.0614 | 0.0295
Epoch 105/300, resid Loss: 0.0608 | 0.0298
Epoch 106/300, resid Loss: 0.0606 | 0.0301
Epoch 107/300, resid Loss: 0.0613 | 0.0303
Epoch 108/300, resid Loss: 0.0621 | 0.0306
Epoch 109/300, resid Loss: 0.0623 | 0.0303
Epoch 110/300, resid Loss: 0.0617 | 0.0297
Epoch 111/300, resid Loss: 0.0609 | 0.0293
Epoch 112/300, resid Loss: 0.0606 | 0.0290
Epoch 113/300, resid Loss: 0.0609 | 0.0292
Epoch 114/300, resid Loss: 0.0615 | 0.0293
Epoch 115/300, resid Loss: 0.0614 | 0.0293
Epoch 116/300, resid Loss: 0.0609 | 0.0293
Epoch 117/300, resid Loss: 0.0607 | 0.0294
Epoch 118/300, resid Loss: 0.0607 | 0.0303
Epoch 119/300, resid Loss: 0.0606 | 0.0301
Epoch 120/300, resid Loss: 0.0606 | 0.0293
Epoch 121/300, resid Loss: 0.0606 | 0.0288
Epoch 122/300, resid Loss: 0.0602 | 0.0286
Epoch 123/300, resid Loss: 0.0598 | 0.0286
Epoch 124/300, resid Loss: 0.0596 | 0.0287
Epoch 125/300, resid Loss: 0.0597 | 0.0288
Epoch 126/300, resid Loss: 0.0597 | 0.0290
Epoch 127/300, resid Loss: 0.0596 | 0.0291
Epoch 128/300, resid Loss: 0.0593 | 0.0291
Epoch 129/300, resid Loss: 0.0592 | 0.0289
Epoch 130/300, resid Loss: 0.0593 | 0.0287
Epoch 131/300, resid Loss: 0.0593 | 0.0285
Epoch 132/300, resid Loss: 0.0591 | 0.0285
Epoch 133/300, resid Loss: 0.0590 | 0.0285
Epoch 134/300, resid Loss: 0.0590 | 0.0286
Epoch 135/300, resid Loss: 0.0590 | 0.0287
Epoch 136/300, resid Loss: 0.0589 | 0.0287
Epoch 137/300, resid Loss: 0.0589 | 0.0287
Epoch 138/300, resid Loss: 0.0589 | 0.0286
Epoch 139/300, resid Loss: 0.0589 | 0.0285
Epoch 140/300, resid Loss: 0.0588 | 0.0285
Epoch 141/300, resid Loss: 0.0588 | 0.0285
Epoch 142/300, resid Loss: 0.0588 | 0.0285
Epoch 143/300, resid Loss: 0.0587 | 0.0285
Epoch 144/300, resid Loss: 0.0587 | 0.0286
Epoch 145/300, resid Loss: 0.0587 | 0.0286
Epoch 146/300, resid Loss: 0.0587 | 0.0285
Epoch 147/300, resid Loss: 0.0587 | 0.0285
Epoch 148/300, resid Loss: 0.0586 | 0.0285
Epoch 149/300, resid Loss: 0.0586 | 0.0285
Epoch 150/300, resid Loss: 0.0586 | 0.0285
Epoch 151/300, resid Loss: 0.0586 | 0.0285
Epoch 152/300, resid Loss: 0.0586 | 0.0285
Epoch 153/300, resid Loss: 0.0586 | 0.0285
Epoch 154/300, resid Loss: 0.0585 | 0.0285
Epoch 155/300, resid Loss: 0.0585 | 0.0284
Epoch 156/300, resid Loss: 0.0585 | 0.0284
Epoch 157/300, resid Loss: 0.0585 | 0.0284
Epoch 158/300, resid Loss: 0.0585 | 0.0284
Epoch 159/300, resid Loss: 0.0585 | 0.0284
Epoch 160/300, resid Loss: 0.0585 | 0.0284
Epoch 161/300, resid Loss: 0.0584 | 0.0284
Epoch 162/300, resid Loss: 0.0584 | 0.0284
Epoch 163/300, resid Loss: 0.0584 | 0.0284
Epoch 164/300, resid Loss: 0.0584 | 0.0284
Epoch 165/300, resid Loss: 0.0584 | 0.0284
Epoch 166/300, resid Loss: 0.0584 | 0.0284
Epoch 167/300, resid Loss: 0.0584 | 0.0284
Epoch 168/300, resid Loss: 0.0584 | 0.0284
Epoch 169/300, resid Loss: 0.0584 | 0.0284
Epoch 170/300, resid Loss: 0.0583 | 0.0284
Epoch 171/300, resid Loss: 0.0583 | 0.0283
Epoch 172/300, resid Loss: 0.0583 | 0.0283
Epoch 173/300, resid Loss: 0.0583 | 0.0283
Epoch 174/300, resid Loss: 0.0583 | 0.0283
Epoch 175/300, resid Loss: 0.0583 | 0.0283
Epoch 176/300, resid Loss: 0.0583 | 0.0283
Epoch 177/300, resid Loss: 0.0583 | 0.0283
Epoch 178/300, resid Loss: 0.0583 | 0.0283
Epoch 179/300, resid Loss: 0.0583 | 0.0283
Epoch 180/300, resid Loss: 0.0583 | 0.0283
Epoch 181/300, resid Loss: 0.0583 | 0.0283
Epoch 182/300, resid Loss: 0.0582 | 0.0283
Epoch 183/300, resid Loss: 0.0582 | 0.0283
Epoch 184/300, resid Loss: 0.0582 | 0.0283
Epoch 185/300, resid Loss: 0.0582 | 0.0283
Epoch 186/300, resid Loss: 0.0582 | 0.0283
Epoch 187/300, resid Loss: 0.0582 | 0.0283
Epoch 188/300, resid Loss: 0.0582 | 0.0283
Epoch 189/300, resid Loss: 0.0582 | 0.0283
Epoch 190/300, resid Loss: 0.0582 | 0.0283
Epoch 191/300, resid Loss: 0.0582 | 0.0283
Epoch 192/300, resid Loss: 0.0582 | 0.0283
Epoch 193/300, resid Loss: 0.0582 | 0.0283
Epoch 194/300, resid Loss: 0.0582 | 0.0283
Epoch 195/300, resid Loss: 0.0582 | 0.0283
Epoch 196/300, resid Loss: 0.0582 | 0.0283
Epoch 197/300, resid Loss: 0.0582 | 0.0283
Epoch 198/300, resid Loss: 0.0582 | 0.0283
Epoch 199/300, resid Loss: 0.0582 | 0.0283
Epoch 200/300, resid Loss: 0.0581 | 0.0283
Epoch 201/300, resid Loss: 0.0581 | 0.0282
Epoch 202/300, resid Loss: 0.0581 | 0.0282
Epoch 203/300, resid Loss: 0.0581 | 0.0282
Epoch 204/300, resid Loss: 0.0581 | 0.0282
Epoch 205/300, resid Loss: 0.0581 | 0.0282
Epoch 206/300, resid Loss: 0.0581 | 0.0282
Epoch 207/300, resid Loss: 0.0581 | 0.0282
Epoch 208/300, resid Loss: 0.0581 | 0.0282
Epoch 209/300, resid Loss: 0.0581 | 0.0282
Epoch 210/300, resid Loss: 0.0581 | 0.0282
Epoch 211/300, resid Loss: 0.0581 | 0.0282
Epoch 212/300, resid Loss: 0.0581 | 0.0282
Epoch 213/300, resid Loss: 0.0581 | 0.0282
Epoch 214/300, resid Loss: 0.0581 | 0.0282
Epoch 215/300, resid Loss: 0.0581 | 0.0282
Epoch 216/300, resid Loss: 0.0581 | 0.0282
Epoch 217/300, resid Loss: 0.0581 | 0.0282
Epoch 218/300, resid Loss: 0.0581 | 0.0282
Epoch 219/300, resid Loss: 0.0581 | 0.0282
Epoch 220/300, resid Loss: 0.0581 | 0.0282
Epoch 221/300, resid Loss: 0.0581 | 0.0282
Epoch 222/300, resid Loss: 0.0581 | 0.0282
Epoch 223/300, resid Loss: 0.0581 | 0.0282
Epoch 224/300, resid Loss: 0.0581 | 0.0282
Epoch 225/300, resid Loss: 0.0581 | 0.0282
Epoch 226/300, resid Loss: 0.0581 | 0.0282
Epoch 227/300, resid Loss: 0.0581 | 0.0282
Epoch 228/300, resid Loss: 0.0581 | 0.0282
Epoch 229/300, resid Loss: 0.0581 | 0.0282
Epoch 230/300, resid Loss: 0.0581 | 0.0282
Epoch 231/300, resid Loss: 0.0581 | 0.0282
Epoch 232/300, resid Loss: 0.0581 | 0.0282
Epoch 233/300, resid Loss: 0.0581 | 0.0282
Epoch 234/300, resid Loss: 0.0581 | 0.0282
Epoch 235/300, resid Loss: 0.0581 | 0.0282
Epoch 236/300, resid Loss: 0.0581 | 0.0282
Epoch 237/300, resid Loss: 0.0581 | 0.0282
Epoch 238/300, resid Loss: 0.0580 | 0.0282
Epoch 239/300, resid Loss: 0.0580 | 0.0282
Epoch 240/300, resid Loss: 0.0580 | 0.0282
Epoch 241/300, resid Loss: 0.0580 | 0.0282
Epoch 242/300, resid Loss: 0.0580 | 0.0282
Epoch 243/300, resid Loss: 0.0580 | 0.0282
Epoch 244/300, resid Loss: 0.0580 | 0.0282
Epoch 245/300, resid Loss: 0.0580 | 0.0282
Epoch 246/300, resid Loss: 0.0580 | 0.0282
Epoch 247/300, resid Loss: 0.0580 | 0.0282
Epoch 248/300, resid Loss: 0.0580 | 0.0282
Epoch 249/300, resid Loss: 0.0580 | 0.0282
Epoch 250/300, resid Loss: 0.0580 | 0.0282
Epoch 251/300, resid Loss: 0.0580 | 0.0282
Epoch 252/300, resid Loss: 0.0580 | 0.0282
Epoch 253/300, resid Loss: 0.0580 | 0.0282
Epoch 254/300, resid Loss: 0.0580 | 0.0282
Epoch 255/300, resid Loss: 0.0580 | 0.0282
Epoch 256/300, resid Loss: 0.0580 | 0.0282
Epoch 257/300, resid Loss: 0.0580 | 0.0282
Epoch 258/300, resid Loss: 0.0580 | 0.0282
Epoch 259/300, resid Loss: 0.0580 | 0.0282
Epoch 260/300, resid Loss: 0.0580 | 0.0282
Epoch 261/300, resid Loss: 0.0580 | 0.0282
Epoch 262/300, resid Loss: 0.0580 | 0.0282
Epoch 263/300, resid Loss: 0.0580 | 0.0282
Epoch 264/300, resid Loss: 0.0580 | 0.0282
Epoch 265/300, resid Loss: 0.0580 | 0.0282
Epoch 266/300, resid Loss: 0.0580 | 0.0282
Epoch 267/300, resid Loss: 0.0580 | 0.0282
Epoch 268/300, resid Loss: 0.0580 | 0.0282
Epoch 269/300, resid Loss: 0.0580 | 0.0282
Epoch 270/300, resid Loss: 0.0580 | 0.0282
Epoch 271/300, resid Loss: 0.0580 | 0.0282
Epoch 272/300, resid Loss: 0.0580 | 0.0282
Epoch 273/300, resid Loss: 0.0580 | 0.0282
Epoch 274/300, resid Loss: 0.0580 | 0.0282
Epoch 275/300, resid Loss: 0.0580 | 0.0282
Epoch 276/300, resid Loss: 0.0580 | 0.0282
Epoch 277/300, resid Loss: 0.0580 | 0.0282
Epoch 278/300, resid Loss: 0.0580 | 0.0282
Epoch 279/300, resid Loss: 0.0580 | 0.0282
Epoch 280/300, resid Loss: 0.0580 | 0.0282
Epoch 281/300, resid Loss: 0.0580 | 0.0282
Epoch 282/300, resid Loss: 0.0580 | 0.0282
Epoch 283/300, resid Loss: 0.0580 | 0.0282
Epoch 284/300, resid Loss: 0.0580 | 0.0282
Epoch 285/300, resid Loss: 0.0580 | 0.0282
Epoch 286/300, resid Loss: 0.0580 | 0.0282
Epoch 287/300, resid Loss: 0.0580 | 0.0282
Epoch 288/300, resid Loss: 0.0580 | 0.0282
Epoch 289/300, resid Loss: 0.0580 | 0.0282
Epoch 290/300, resid Loss: 0.0580 | 0.0282
Epoch 291/300, resid Loss: 0.0580 | 0.0282
Epoch 292/300, resid Loss: 0.0580 | 0.0282
Epoch 293/300, resid Loss: 0.0580 | 0.0282
Epoch 294/300, resid Loss: 0.0580 | 0.0282
Epoch 295/300, resid Loss: 0.0580 | 0.0282
Epoch 296/300, resid Loss: 0.0580 | 0.0282
Epoch 297/300, resid Loss: 0.0580 | 0.0282
Epoch 298/300, resid Loss: 0.0580 | 0.0282
Epoch 299/300, resid Loss: 0.0580 | 0.0282
Epoch 300/300, resid Loss: 0.0580 | 0.0282
Runtime (seconds): 5815.194744825363
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[156.77606]
[-2.7355807]
[3.0636954]
[14.944647]
[1.4373555]
[23.227924]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 22.42875713482499
RMSE: 4.73590087890625
MAE: 4.73590087890625
R-squared: nan
[196.7141]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
