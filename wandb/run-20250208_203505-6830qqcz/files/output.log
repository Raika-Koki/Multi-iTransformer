ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-08 20:35:11,139][0m A new study created in memory with name: no-name-c9044ce4-08a9-46dc-b84f-6e7af48a98d9[0m
[32m[I 2025-02-08 20:36:56,006][0m Trial 0 finished with value: 0.20372251894038457 and parameters: {'observation_period_num': 79, 'train_rates': 0.6754624626991544, 'learning_rate': 0.0006588452177412805, 'batch_size': 44, 'step_size': 7, 'gamma': 0.869230803201341}. Best is trial 0 with value: 0.20372251894038457.[0m
[32m[I 2025-02-08 20:37:23,343][0m Trial 1 finished with value: 0.988472044467926 and parameters: {'observation_period_num': 68, 'train_rates': 0.9715605352933824, 'learning_rate': 2.0341336066946634e-06, 'batch_size': 255, 'step_size': 6, 'gamma': 0.7778562638271325}. Best is trial 0 with value: 0.20372251894038457.[0m
[32m[I 2025-02-08 20:37:49,484][0m Trial 2 finished with value: 0.28023568693699175 and parameters: {'observation_period_num': 167, 'train_rates': 0.7525118289794042, 'learning_rate': 0.0007186002432931742, 'batch_size': 194, 'step_size': 11, 'gamma': 0.812310485968538}. Best is trial 0 with value: 0.20372251894038457.[0m
[32m[I 2025-02-08 20:38:35,686][0m Trial 3 finished with value: 0.4736252642464484 and parameters: {'observation_period_num': 160, 'train_rates': 0.6675412684539934, 'learning_rate': 2.107550015380703e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.8529727054540459}. Best is trial 0 with value: 0.20372251894038457.[0m
[32m[I 2025-02-08 20:39:13,193][0m Trial 4 finished with value: 0.12629934629568687 and parameters: {'observation_period_num': 102, 'train_rates': 0.9321842618178378, 'learning_rate': 0.00010283133679854734, 'batch_size': 165, 'step_size': 15, 'gamma': 0.7635988077771128}. Best is trial 4 with value: 0.12629934629568687.[0m
[32m[I 2025-02-08 20:39:48,272][0m Trial 5 finished with value: 0.14409047290281504 and parameters: {'observation_period_num': 167, 'train_rates': 0.8010401280997839, 'learning_rate': 0.00010233925603364734, 'batch_size': 150, 'step_size': 12, 'gamma': 0.8944120285867673}. Best is trial 4 with value: 0.12629934629568687.[0m
[32m[I 2025-02-08 20:41:00,374][0m Trial 6 finished with value: 0.06061023737214488 and parameters: {'observation_period_num': 36, 'train_rates': 0.8542714875699435, 'learning_rate': 0.0005994661381528819, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9077376894505899}. Best is trial 6 with value: 0.06061023737214488.[0m
[32m[I 2025-02-08 20:42:31,282][0m Trial 7 finished with value: 0.7113787967133461 and parameters: {'observation_period_num': 230, 'train_rates': 0.8563880038713758, 'learning_rate': 1.2050943979916029e-06, 'batch_size': 56, 'step_size': 7, 'gamma': 0.9032025656159929}. Best is trial 6 with value: 0.06061023737214488.[0m
Early stopping at epoch 48
[32m[I 2025-02-08 20:42:52,096][0m Trial 8 finished with value: 0.6958432886687624 and parameters: {'observation_period_num': 193, 'train_rates': 0.7826954586693526, 'learning_rate': 2.2803915285661412e-05, 'batch_size': 125, 'step_size': 1, 'gamma': 0.7748126853670634}. Best is trial 6 with value: 0.06061023737214488.[0m
[32m[I 2025-02-08 20:44:04,135][0m Trial 9 finished with value: 0.2069866914037869 and parameters: {'observation_period_num': 91, 'train_rates': 0.7499437831314907, 'learning_rate': 0.0007215106775928812, 'batch_size': 71, 'step_size': 5, 'gamma': 0.8607461166196363}. Best is trial 6 with value: 0.06061023737214488.[0m
[32m[I 2025-02-08 20:48:13,664][0m Trial 10 finished with value: 0.02672924031374863 and parameters: {'observation_period_num': 11, 'train_rates': 0.8776029462099143, 'learning_rate': 0.00015861638379094685, 'batch_size': 22, 'step_size': 1, 'gamma': 0.9745532342127611}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 20:53:56,995][0m Trial 11 finished with value: 0.029063650158377027 and parameters: {'observation_period_num': 5, 'train_rates': 0.8748921281327546, 'learning_rate': 0.0001543776927142742, 'batch_size': 16, 'step_size': 1, 'gamma': 0.9861139960469825}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 20:59:04,294][0m Trial 12 finished with value: 0.032839752526746854 and parameters: {'observation_period_num': 11, 'train_rates': 0.890573070607008, 'learning_rate': 0.00011947565851865777, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9897939854385377}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:04:58,934][0m Trial 13 finished with value: 0.02912187587845233 and parameters: {'observation_period_num': 5, 'train_rates': 0.9130647425765003, 'learning_rate': 0.00019033437950219896, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9867312718127109}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:07:34,092][0m Trial 14 finished with value: 0.12564631638138793 and parameters: {'observation_period_num': 45, 'train_rates': 0.9853603029725757, 'learning_rate': 6.418892826192145e-06, 'batch_size': 39, 'step_size': 3, 'gamma': 0.9586309867244321}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:08:25,428][0m Trial 15 finished with value: 0.10082879231384631 and parameters: {'observation_period_num': 120, 'train_rates': 0.8436567260844651, 'learning_rate': 0.0002544274766454908, 'batch_size': 105, 'step_size': 10, 'gamma': 0.9405584491235366}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:08:52,308][0m Trial 16 finished with value: 0.2633683681488037 and parameters: {'observation_period_num': 45, 'train_rates': 0.9469209157683762, 'learning_rate': 5.824210328260075e-05, 'batch_size': 236, 'step_size': 1, 'gamma': 0.9487309986906073}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:10:02,894][0m Trial 17 finished with value: 0.05353164193885667 and parameters: {'observation_period_num': 27, 'train_rates': 0.8930467573979182, 'learning_rate': 4.249216028352815e-05, 'batch_size': 81, 'step_size': 4, 'gamma': 0.9641418798392233}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:12:33,085][0m Trial 18 finished with value: 0.129554809553616 and parameters: {'observation_period_num': 65, 'train_rates': 0.8281755038510331, 'learning_rate': 0.00028484813931686087, 'batch_size': 35, 'step_size': 9, 'gamma': 0.9282240510569779}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:13:02,114][0m Trial 19 finished with value: 0.9180022770531933 and parameters: {'observation_period_num': 137, 'train_rates': 0.7175154531193852, 'learning_rate': 1.0682912345343261e-05, 'batch_size': 180, 'step_size': 2, 'gamma': 0.8230682023982261}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:13:37,299][0m Trial 20 finished with value: 0.4817857472305624 and parameters: {'observation_period_num': 244, 'train_rates': 0.6127633562622626, 'learning_rate': 0.00034556514251710813, 'batch_size': 120, 'step_size': 14, 'gamma': 0.9203435058938894}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:19:08,687][0m Trial 21 finished with value: 0.038654377563826496 and parameters: {'observation_period_num': 12, 'train_rates': 0.9057628299461009, 'learning_rate': 0.00018601419949542533, 'batch_size': 17, 'step_size': 3, 'gamma': 0.9827333621013568}. Best is trial 10 with value: 0.02672924031374863.[0m
[32m[I 2025-02-08 21:22:42,498][0m Trial 22 finished with value: 0.02634357857139055 and parameters: {'observation_period_num': 8, 'train_rates': 0.9201910377771746, 'learning_rate': 6.282516697543941e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.9726977588947996}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:24:15,693][0m Trial 23 finished with value: 0.05556594715504856 and parameters: {'observation_period_num': 53, 'train_rates': 0.8750548616025715, 'learning_rate': 6.55077630087953e-05, 'batch_size': 60, 'step_size': 5, 'gamma': 0.9650748218199683}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:26:57,713][0m Trial 24 finished with value: 0.07158948183059692 and parameters: {'observation_period_num': 27, 'train_rates': 0.9388333541774807, 'learning_rate': 1.2641796906197101e-05, 'batch_size': 36, 'step_size': 2, 'gamma': 0.9701646553654644}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:27:57,000][0m Trial 25 finished with value: 0.09259183777709289 and parameters: {'observation_period_num': 5, 'train_rates': 0.816785826120589, 'learning_rate': 3.464006753270621e-05, 'batch_size': 93, 'step_size': 1, 'gamma': 0.9335305394854325}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:29:48,348][0m Trial 26 finished with value: 0.04343421320269879 and parameters: {'observation_period_num': 23, 'train_rates': 0.9548818627784845, 'learning_rate': 8.307302547271848e-05, 'batch_size': 53, 'step_size': 4, 'gamma': 0.9506803368594542}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:33:19,030][0m Trial 27 finished with value: 0.23254247810295287 and parameters: {'observation_period_num': 64, 'train_rates': 0.8705701988383098, 'learning_rate': 0.0001397908724805547, 'batch_size': 26, 'step_size': 2, 'gamma': 0.973587188998022}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:34:47,516][0m Trial 28 finished with value: 0.05566965648904443 and parameters: {'observation_period_num': 34, 'train_rates': 0.918244883539025, 'learning_rate': 0.00036294509894499466, 'batch_size': 67, 'step_size': 4, 'gamma': 0.9178039853598581}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:36:37,990][0m Trial 29 finished with value: 0.21296052513427513 and parameters: {'observation_period_num': 97, 'train_rates': 0.775162619586217, 'learning_rate': 4.89533097013352e-05, 'batch_size': 45, 'step_size': 8, 'gamma': 0.8815374233184122}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:37:06,024][0m Trial 30 finished with value: 0.07498720932774748 and parameters: {'observation_period_num': 76, 'train_rates': 0.8306553924666679, 'learning_rate': 0.0004056057519652865, 'batch_size': 212, 'step_size': 2, 'gamma': 0.9437144205398335}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:42:20,538][0m Trial 31 finished with value: 0.03354771144598002 and parameters: {'observation_period_num': 15, 'train_rates': 0.9159283538922471, 'learning_rate': 0.0001894553023706062, 'batch_size': 18, 'step_size': 3, 'gamma': 0.9808016160797939}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:45:30,344][0m Trial 32 finished with value: 0.03270348495115404 and parameters: {'observation_period_num': 5, 'train_rates': 0.9612935300818051, 'learning_rate': 0.00017500229481448495, 'batch_size': 32, 'step_size': 5, 'gamma': 0.987283903773155}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:47:27,960][0m Trial 33 finished with value: 0.06174015510849088 and parameters: {'observation_period_num': 53, 'train_rates': 0.8825359822300995, 'learning_rate': 7.85136284941003e-05, 'batch_size': 47, 'step_size': 2, 'gamma': 0.9599449671606938}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:53:17,634][0m Trial 34 finished with value: 0.06110740204652151 and parameters: {'observation_period_num': 25, 'train_rates': 0.9827864585512329, 'learning_rate': 0.0009837894259838527, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8353136171275971}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:55:16,095][0m Trial 35 finished with value: 0.05240389969079725 and parameters: {'observation_period_num': 42, 'train_rates': 0.9259833215965714, 'learning_rate': 2.83608415491856e-05, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9736091837076911}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:58:23,674][0m Trial 36 finished with value: 0.04097154929206289 and parameters: {'observation_period_num': 19, 'train_rates': 0.9017964490996905, 'learning_rate': 0.0005116292364333338, 'batch_size': 30, 'step_size': 7, 'gamma': 0.799041947440344}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 21:59:48,645][0m Trial 37 finished with value: 0.04007417433951274 and parameters: {'observation_period_num': 5, 'train_rates': 0.860003863300631, 'learning_rate': 0.00023757506859520448, 'batch_size': 66, 'step_size': 1, 'gamma': 0.9569773164837783}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:00:49,325][0m Trial 38 finished with value: 0.06858839523449219 and parameters: {'observation_period_num': 81, 'train_rates': 0.8073078894608516, 'learning_rate': 0.0001260784608006024, 'batch_size': 88, 'step_size': 3, 'gamma': 0.9899063258259095}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:03:38,748][0m Trial 39 finished with value: 0.280639906724294 and parameters: {'observation_period_num': 195, 'train_rates': 0.9309035798887836, 'learning_rate': 4.000640754378371e-06, 'batch_size': 32, 'step_size': 6, 'gamma': 0.975767751357657}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:05:25,455][0m Trial 40 finished with value: 0.15898406024401388 and parameters: {'observation_period_num': 113, 'train_rates': 0.9664585718995508, 'learning_rate': 9.262492244813433e-05, 'batch_size': 55, 'step_size': 2, 'gamma': 0.8881768530057903}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:08:44,587][0m Trial 41 finished with value: 0.029291789333980817 and parameters: {'observation_period_num': 6, 'train_rates': 0.9650992898690104, 'learning_rate': 0.00014751462836058588, 'batch_size': 30, 'step_size': 5, 'gamma': 0.9857008150518783}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:12:37,429][0m Trial 42 finished with value: 0.05193490275870199 and parameters: {'observation_period_num': 33, 'train_rates': 0.9451769084029243, 'learning_rate': 0.00014875306081848494, 'batch_size': 25, 'step_size': 5, 'gamma': 0.9745363415532502}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:14:49,177][0m Trial 43 finished with value: 0.043886931882061685 and parameters: {'observation_period_num': 20, 'train_rates': 0.9059802001167171, 'learning_rate': 0.00022099229801480152, 'batch_size': 43, 'step_size': 4, 'gamma': 0.951878384344623}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:20:56,488][0m Trial 44 finished with value: 0.05410437442754444 and parameters: {'observation_period_num': 58, 'train_rates': 0.9741055407783602, 'learning_rate': 9.823174448647246e-05, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9367000036063505}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:22:08,695][0m Trial 45 finished with value: 0.059767800680500395 and parameters: {'observation_period_num': 39, 'train_rates': 0.8394828048121793, 'learning_rate': 7.009265955078949e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.965688534206403}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:24:24,591][0m Trial 46 finished with value: 0.044472992696827 and parameters: {'observation_period_num': 16, 'train_rates': 0.8632405151481473, 'learning_rate': 1.753120734160355e-05, 'batch_size': 41, 'step_size': 7, 'gamma': 0.989557587692124}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:25:09,523][0m Trial 47 finished with value: 0.06446384768332204 and parameters: {'observation_period_num': 33, 'train_rates': 0.8944304252961347, 'learning_rate': 4.850611944704736e-05, 'batch_size': 134, 'step_size': 6, 'gamma': 0.9782026484409352}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:26:04,598][0m Trial 48 finished with value: 0.06449893346199623 and parameters: {'observation_period_num': 47, 'train_rates': 0.9333468042285525, 'learning_rate': 0.00011817136461110234, 'batch_size': 110, 'step_size': 12, 'gamma': 0.7560582349456753}. Best is trial 22 with value: 0.02634357857139055.[0m
[32m[I 2025-02-08 22:27:31,821][0m Trial 49 finished with value: 0.16233210065907025 and parameters: {'observation_period_num': 142, 'train_rates': 0.8826168872590527, 'learning_rate': 0.000309213602895514, 'batch_size': 63, 'step_size': 2, 'gamma': 0.9583158590483376}. Best is trial 22 with value: 0.02634357857139055.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-08 22:27:31,831][0m A new study created in memory with name: no-name-390cb813-368a-4fa9-a146-b405bc4800b6[0m
[32m[I 2025-02-08 22:28:32,616][0m Trial 0 finished with value: 0.2194846659576764 and parameters: {'observation_period_num': 104, 'train_rates': 0.708828151489654, 'learning_rate': 0.0002848285750862314, 'batch_size': 80, 'step_size': 12, 'gamma': 0.8742517184552888}. Best is trial 0 with value: 0.2194846659576764.[0m
[32m[I 2025-02-08 22:29:37,457][0m Trial 1 finished with value: 0.8337028279024012 and parameters: {'observation_period_num': 251, 'train_rates': 0.7182791854937303, 'learning_rate': 3.991528758951351e-06, 'batch_size': 72, 'step_size': 5, 'gamma': 0.8457150328729689}. Best is trial 0 with value: 0.2194846659576764.[0m
[32m[I 2025-02-08 22:30:08,371][0m Trial 2 finished with value: 0.16602434673022928 and parameters: {'observation_period_num': 50, 'train_rates': 0.6128112684460649, 'learning_rate': 0.0002791717433768308, 'batch_size': 154, 'step_size': 10, 'gamma': 0.8866888888419515}. Best is trial 2 with value: 0.16602434673022928.[0m
[32m[I 2025-02-08 22:30:53,092][0m Trial 3 finished with value: 0.10823556061448722 and parameters: {'observation_period_num': 78, 'train_rates': 0.899697349959775, 'learning_rate': 0.0003055176798722346, 'batch_size': 130, 'step_size': 12, 'gamma': 0.9098534293353325}. Best is trial 3 with value: 0.10823556061448722.[0m
[32m[I 2025-02-08 22:31:25,475][0m Trial 4 finished with value: 0.1517035165178274 and parameters: {'observation_period_num': 141, 'train_rates': 0.8233604868422333, 'learning_rate': 0.0005435742362849505, 'batch_size': 177, 'step_size': 8, 'gamma': 0.9220818077396558}. Best is trial 3 with value: 0.10823556061448722.[0m
[32m[I 2025-02-08 22:32:00,619][0m Trial 5 finished with value: 0.14861072614439763 and parameters: {'observation_period_num': 89, 'train_rates': 0.8219637686411962, 'learning_rate': 3.495017916880983e-05, 'batch_size': 166, 'step_size': 5, 'gamma': 0.9253030955756767}. Best is trial 3 with value: 0.10823556061448722.[0m
[32m[I 2025-02-08 22:32:31,402][0m Trial 6 finished with value: 0.5808011293411255 and parameters: {'observation_period_num': 124, 'train_rates': 0.9793297578441854, 'learning_rate': 6.348490712450615e-06, 'batch_size': 209, 'step_size': 13, 'gamma': 0.756827656369586}. Best is trial 3 with value: 0.10823556061448722.[0m
Early stopping at epoch 62
[32m[I 2025-02-08 22:33:01,315][0m Trial 7 finished with value: 1.3134335033164537 and parameters: {'observation_period_num': 206, 'train_rates': 0.7367485605706936, 'learning_rate': 4.193664320864183e-06, 'batch_size': 104, 'step_size': 1, 'gamma': 0.7828382522646001}. Best is trial 3 with value: 0.10823556061448722.[0m
[32m[I 2025-02-08 22:33:55,067][0m Trial 8 finished with value: 1.5039518419787181 and parameters: {'observation_period_num': 181, 'train_rates': 0.9503864458135314, 'learning_rate': 1.6394378173911503e-06, 'batch_size': 108, 'step_size': 3, 'gamma': 0.8417888720263005}. Best is trial 3 with value: 0.10823556061448722.[0m
[32m[I 2025-02-08 22:34:32,129][0m Trial 9 finished with value: 0.2116884018936223 and parameters: {'observation_period_num': 49, 'train_rates': 0.6778459872987113, 'learning_rate': 4.608283575980828e-05, 'batch_size': 133, 'step_size': 7, 'gamma': 0.8498126123896992}. Best is trial 3 with value: 0.10823556061448722.[0m
[32m[I 2025-02-08 22:37:19,547][0m Trial 10 finished with value: 0.03512739601825911 and parameters: {'observation_period_num': 11, 'train_rates': 0.8933064980422442, 'learning_rate': 0.0001038739535399735, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9897970535894725}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:43:05,997][0m Trial 11 finished with value: 0.040519610703366356 and parameters: {'observation_period_num': 14, 'train_rates': 0.8924437145913621, 'learning_rate': 9.370096600270285e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9749602775563369}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:48:23,019][0m Trial 12 finished with value: 0.058328271118414855 and parameters: {'observation_period_num': 6, 'train_rates': 0.9023471257437757, 'learning_rate': 0.00010099581167595269, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9890942906873151}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:50:41,829][0m Trial 13 finished with value: 0.04015278671230067 and parameters: {'observation_period_num': 9, 'train_rates': 0.8840824746103682, 'learning_rate': 8.335339313958832e-05, 'batch_size': 41, 'step_size': 15, 'gamma': 0.9874684519371184}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:52:26,912][0m Trial 14 finished with value: 0.07037414611283554 and parameters: {'observation_period_num': 41, 'train_rates': 0.858202631479907, 'learning_rate': 1.5092954903125168e-05, 'batch_size': 53, 'step_size': 10, 'gamma': 0.9520296135030959}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:54:25,049][0m Trial 15 finished with value: 0.19368569878352784 and parameters: {'observation_period_num': 27, 'train_rates': 0.781914213026103, 'learning_rate': 0.00010785408472734198, 'batch_size': 44, 'step_size': 14, 'gamma': 0.9533804008845872}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:54:52,194][0m Trial 16 finished with value: 0.27516835927963257 and parameters: {'observation_period_num': 69, 'train_rates': 0.940812250021367, 'learning_rate': 1.4300662144140835e-05, 'batch_size': 247, 'step_size': 10, 'gamma': 0.9563547440255233}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:56:26,642][0m Trial 17 finished with value: 0.04164524262442308 and parameters: {'observation_period_num': 5, 'train_rates': 0.8563278358680557, 'learning_rate': 5.90160970918729e-05, 'batch_size': 59, 'step_size': 12, 'gamma': 0.9855690958531993}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:58:53,113][0m Trial 18 finished with value: 0.3224845610558987 and parameters: {'observation_period_num': 160, 'train_rates': 0.775571953069456, 'learning_rate': 0.0008794985924706472, 'batch_size': 33, 'step_size': 15, 'gamma': 0.7966808147942506}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 22:59:50,875][0m Trial 19 finished with value: 0.0906804864489755 and parameters: {'observation_period_num': 66, 'train_rates': 0.8516972252924946, 'learning_rate': 1.6151572222618185e-05, 'batch_size': 97, 'step_size': 13, 'gamma': 0.943412385324295}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:01:14,853][0m Trial 20 finished with value: 0.11494248360395432 and parameters: {'observation_period_num': 99, 'train_rates': 0.9844552301874059, 'learning_rate': 0.00011476739633280972, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9010634168449649}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:07:03,587][0m Trial 21 finished with value: 0.07043204988752093 and parameters: {'observation_period_num': 23, 'train_rates': 0.9003158959019826, 'learning_rate': 0.00015346584173276425, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9728181010643759}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:09:19,383][0m Trial 22 finished with value: 0.04762447398333322 and parameters: {'observation_period_num': 25, 'train_rates': 0.928641782283667, 'learning_rate': 7.138095197031882e-05, 'batch_size': 43, 'step_size': 14, 'gamma': 0.9712553088890302}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:12:31,263][0m Trial 23 finished with value: 0.042561196118551595 and parameters: {'observation_period_num': 8, 'train_rates': 0.8791478740973557, 'learning_rate': 2.5459752580022204e-05, 'batch_size': 29, 'step_size': 13, 'gamma': 0.936833159188598}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:13:57,303][0m Trial 24 finished with value: 0.07013660349414846 and parameters: {'observation_period_num': 41, 'train_rates': 0.8183621866358581, 'learning_rate': 0.0002027025011353017, 'batch_size': 62, 'step_size': 15, 'gamma': 0.9700773005020137}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:15:09,110][0m Trial 25 finished with value: 0.06176081878276399 and parameters: {'observation_period_num': 30, 'train_rates': 0.9292123705722912, 'learning_rate': 2.7213442599049857e-05, 'batch_size': 85, 'step_size': 11, 'gamma': 0.987181394019961}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:20:49,751][0m Trial 26 finished with value: 0.2696349189601131 and parameters: {'observation_period_num': 65, 'train_rates': 0.8765736132762919, 'learning_rate': 7.483601105621012e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9646353168999875}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:23:23,452][0m Trial 27 finished with value: 0.20223934992925444 and parameters: {'observation_period_num': 121, 'train_rates': 0.9529243074412239, 'learning_rate': 0.00015363627955536125, 'batch_size': 37, 'step_size': 14, 'gamma': 0.9300165348476691}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:24:58,497][0m Trial 28 finished with value: 0.07516331971324158 and parameters: {'observation_period_num': 54, 'train_rates': 0.7978657456001866, 'learning_rate': 0.0005457090568463405, 'batch_size': 55, 'step_size': 11, 'gamma': 0.988660309575121}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:26:03,556][0m Trial 29 finished with value: 0.05754817936282892 and parameters: {'observation_period_num': 21, 'train_rates': 0.9118815578102483, 'learning_rate': 0.0003447883401872609, 'batch_size': 90, 'step_size': 13, 'gamma': 0.8650539975710246}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:28:54,807][0m Trial 30 finished with value: 0.13389574482160457 and parameters: {'observation_period_num': 106, 'train_rates': 0.8811382418748517, 'learning_rate': 4.679553029245739e-05, 'batch_size': 32, 'step_size': 12, 'gamma': 0.8176720586840974}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:30:32,886][0m Trial 31 finished with value: 0.04382053691676647 and parameters: {'observation_period_num': 5, 'train_rates': 0.8493884251700663, 'learning_rate': 6.0140382338376525e-05, 'batch_size': 57, 'step_size': 15, 'gamma': 0.9773501505467035}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:31:54,986][0m Trial 32 finished with value: 0.04277377482093378 and parameters: {'observation_period_num': 16, 'train_rates': 0.8461487922031407, 'learning_rate': 0.00018870418488140108, 'batch_size': 70, 'step_size': 12, 'gamma': 0.960029615991367}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:32:44,018][0m Trial 33 finished with value: 0.0688982565881889 and parameters: {'observation_period_num': 36, 'train_rates': 0.8818847087181096, 'learning_rate': 4.2114468189778996e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.9886242229053336}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:33:58,733][0m Trial 34 finished with value: 0.1468573913626049 and parameters: {'observation_period_num': 216, 'train_rates': 0.9165730374790587, 'learning_rate': 8.735524027445706e-05, 'batch_size': 77, 'step_size': 11, 'gamma': 0.9438363634919631}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:35:33,028][0m Trial 35 finished with value: 0.348129924329033 and parameters: {'observation_period_num': 56, 'train_rates': 0.6005898736002961, 'learning_rate': 2.2446135831733975e-05, 'batch_size': 45, 'step_size': 12, 'gamma': 0.905576096893725}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:38:29,725][0m Trial 36 finished with value: 0.23006194855855858 and parameters: {'observation_period_num': 252, 'train_rates': 0.8304354343172602, 'learning_rate': 0.00025807762206698554, 'batch_size': 28, 'step_size': 8, 'gamma': 0.8858828583400439}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:38:58,459][0m Trial 37 finished with value: 0.26104496623470574 and parameters: {'observation_period_num': 80, 'train_rates': 0.7493641831174391, 'learning_rate': 5.494313586815158e-05, 'batch_size': 196, 'step_size': 6, 'gamma': 0.9770046536427752}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:40:20,202][0m Trial 38 finished with value: 0.060874850385718875 and parameters: {'observation_period_num': 14, 'train_rates': 0.801880250055439, 'learning_rate': 0.00037058120250850804, 'batch_size': 65, 'step_size': 13, 'gamma': 0.9172876573072946}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:41:04,717][0m Trial 39 finished with value: 0.16968126595020294 and parameters: {'observation_period_num': 40, 'train_rates': 0.9663941971981989, 'learning_rate': 9.967619033717766e-06, 'batch_size': 144, 'step_size': 15, 'gamma': 0.9414881805793638}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:41:54,832][0m Trial 40 finished with value: 0.1325241407389159 and parameters: {'observation_period_num': 144, 'train_rates': 0.8633645569887235, 'learning_rate': 0.00013927131050050636, 'batch_size': 113, 'step_size': 14, 'gamma': 0.9635247065667881}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:45:15,030][0m Trial 41 finished with value: 0.03682435108998487 and parameters: {'observation_period_num': 7, 'train_rates': 0.8839357881569553, 'learning_rate': 2.9889329941616466e-05, 'batch_size': 28, 'step_size': 14, 'gamma': 0.978703075040755}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:49:07,901][0m Trial 42 finished with value: 0.039041457288875814 and parameters: {'observation_period_num': 16, 'train_rates': 0.8888860855130611, 'learning_rate': 3.029669667949411e-05, 'batch_size': 24, 'step_size': 14, 'gamma': 0.9776711253997054}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:52:54,491][0m Trial 43 finished with value: 0.04351748634651778 and parameters: {'observation_period_num': 21, 'train_rates': 0.8934945138836856, 'learning_rate': 3.837963397246707e-05, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9780899185757217}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:55:06,551][0m Trial 44 finished with value: 0.28329134794301863 and parameters: {'observation_period_num': 46, 'train_rates': 0.9216059011025959, 'learning_rate': 1.6679367407240463e-06, 'batch_size': 44, 'step_size': 14, 'gamma': 0.9557921596649873}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-08 23:58:30,309][0m Trial 45 finished with value: 0.16785672622854295 and parameters: {'observation_period_num': 33, 'train_rates': 0.6547521041376462, 'learning_rate': 1.9701563920577474e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.9492081999108405}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-09 00:00:19,765][0m Trial 46 finished with value: 0.07266699593253587 and parameters: {'observation_period_num': 16, 'train_rates': 0.835393050843062, 'learning_rate': 9.179092946895875e-06, 'batch_size': 50, 'step_size': 13, 'gamma': 0.966044076722289}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-09 00:02:57,400][0m Trial 47 finished with value: 0.06096489841410936 and parameters: {'observation_period_num': 32, 'train_rates': 0.9589974536338713, 'learning_rate': 3.80568315503849e-05, 'batch_size': 37, 'step_size': 15, 'gamma': 0.9791008563542772}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-09 00:06:49,144][0m Trial 48 finished with value: 0.13447185371132844 and parameters: {'observation_period_num': 53, 'train_rates': 0.9378376468464301, 'learning_rate': 9.294920615268129e-05, 'batch_size': 25, 'step_size': 13, 'gamma': 0.928131934872244}. Best is trial 10 with value: 0.03512739601825911.[0m
[32m[I 2025-02-09 00:09:11,145][0m Trial 49 finished with value: 0.1502492115145824 and parameters: {'observation_period_num': 229, 'train_rates': 0.8664935851866991, 'learning_rate': 2.9271387231493806e-05, 'batch_size': 36, 'step_size': 14, 'gamma': 0.7678934295998125}. Best is trial 10 with value: 0.03512739601825911.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-09 00:09:11,156][0m A new study created in memory with name: no-name-d16a200c-02bd-491f-9112-c743659aa439[0m
[32m[I 2025-02-09 00:09:44,660][0m Trial 0 finished with value: 0.23405896758669645 and parameters: {'observation_period_num': 102, 'train_rates': 0.7824212930076546, 'learning_rate': 8.382098981691145e-05, 'batch_size': 164, 'step_size': 9, 'gamma': 0.8554194342390895}. Best is trial 0 with value: 0.23405896758669645.[0m
[32m[I 2025-02-09 00:10:21,975][0m Trial 1 finished with value: 0.6900737925196987 and parameters: {'observation_period_num': 119, 'train_rates': 0.7665496764899006, 'learning_rate': 2.2547047534084016e-06, 'batch_size': 137, 'step_size': 10, 'gamma': 0.797366967789057}. Best is trial 0 with value: 0.23405896758669645.[0m
[32m[I 2025-02-09 00:11:20,696][0m Trial 2 finished with value: 0.42176159441126565 and parameters: {'observation_period_num': 160, 'train_rates': 0.9383560981299651, 'learning_rate': 9.88971832071135e-06, 'batch_size': 98, 'step_size': 3, 'gamma': 0.9216710911687668}. Best is trial 0 with value: 0.23405896758669645.[0m
[32m[I 2025-02-09 00:12:00,816][0m Trial 3 finished with value: 0.6908515984729185 and parameters: {'observation_period_num': 36, 'train_rates': 0.9194651329687054, 'learning_rate': 1.0137658200871055e-06, 'batch_size': 153, 'step_size': 5, 'gamma': 0.9889561514844484}. Best is trial 0 with value: 0.23405896758669645.[0m
[32m[I 2025-02-09 00:12:25,518][0m Trial 4 finished with value: 0.19338451325893402 and parameters: {'observation_period_num': 149, 'train_rates': 0.9283028074280202, 'learning_rate': 3.098141295611475e-05, 'batch_size': 238, 'step_size': 5, 'gamma': 0.9887186465294132}. Best is trial 4 with value: 0.19338451325893402.[0m
[32m[I 2025-02-09 00:13:06,045][0m Trial 5 finished with value: 1.1178160658315144 and parameters: {'observation_period_num': 122, 'train_rates': 0.7053552702602327, 'learning_rate': 1.7849933593595214e-06, 'batch_size': 121, 'step_size': 5, 'gamma': 0.757081570830155}. Best is trial 4 with value: 0.19338451325893402.[0m
[32m[I 2025-02-09 00:13:29,826][0m Trial 6 finished with value: 0.3726773828268051 and parameters: {'observation_period_num': 74, 'train_rates': 0.6091259600026404, 'learning_rate': 3.881000683358303e-05, 'batch_size': 218, 'step_size': 5, 'gamma': 0.8342129677442334}. Best is trial 4 with value: 0.19338451325893402.[0m
[32m[I 2025-02-09 00:14:21,528][0m Trial 7 finished with value: 0.7816527027037877 and parameters: {'observation_period_num': 77, 'train_rates': 0.7458691183799113, 'learning_rate': 2.0448885520907203e-06, 'batch_size': 101, 'step_size': 7, 'gamma': 0.816378086069024}. Best is trial 4 with value: 0.19338451325893402.[0m
[32m[I 2025-02-09 00:14:48,037][0m Trial 8 finished with value: 0.4591184238518924 and parameters: {'observation_period_num': 178, 'train_rates': 0.903661724165914, 'learning_rate': 1.1719888275585723e-05, 'batch_size': 241, 'step_size': 4, 'gamma': 0.9142434701902437}. Best is trial 4 with value: 0.19338451325893402.[0m
[32m[I 2025-02-09 00:15:16,421][0m Trial 9 finished with value: 0.6493380285263062 and parameters: {'observation_period_num': 123, 'train_rates': 0.7800602780245661, 'learning_rate': 1.7747229547518852e-05, 'batch_size': 186, 'step_size': 2, 'gamma': 0.8595796588429668}. Best is trial 4 with value: 0.19338451325893402.[0m
[32m[I 2025-02-09 00:18:59,903][0m Trial 10 finished with value: 0.1870426253692524 and parameters: {'observation_period_num': 245, 'train_rates': 0.9864467351462876, 'learning_rate': 0.0007631042132274971, 'batch_size': 25, 'step_size': 15, 'gamma': 0.9853425179770664}. Best is trial 10 with value: 0.1870426253692524.[0m
[32m[I 2025-02-09 00:21:52,557][0m Trial 11 finished with value: 0.1258040761947632 and parameters: {'observation_period_num': 238, 'train_rates': 0.9817737080697041, 'learning_rate': 0.000581908695857585, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9822202423193769}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:26:57,161][0m Trial 12 finished with value: 0.12762425385289272 and parameters: {'observation_period_num': 245, 'train_rates': 0.9783164066877647, 'learning_rate': 0.0008979944914080299, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9430440124846681}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:30:19,322][0m Trial 13 finished with value: 0.1485176451300105 and parameters: {'observation_period_num': 252, 'train_rates': 0.8657320131585735, 'learning_rate': 0.0007737544678849915, 'batch_size': 25, 'step_size': 15, 'gamma': 0.9368841837473744}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:31:55,314][0m Trial 14 finished with value: 0.2388414740562439 and parameters: {'observation_period_num': 204, 'train_rates': 0.986210533200665, 'learning_rate': 0.00024701364925857407, 'batch_size': 60, 'step_size': 12, 'gamma': 0.948016391928978}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:33:26,982][0m Trial 15 finished with value: 0.1599461697504183 and parameters: {'observation_period_num': 205, 'train_rates': 0.8541095620843745, 'learning_rate': 0.0002493089251584566, 'batch_size': 57, 'step_size': 13, 'gamma': 0.8934808589195373}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:34:56,353][0m Trial 16 finished with value: 0.2574487109566085 and parameters: {'observation_period_num': 219, 'train_rates': 0.8511290373074842, 'learning_rate': 0.0002723923787292983, 'batch_size': 57, 'step_size': 12, 'gamma': 0.9584634471244762}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:40:22,263][0m Trial 17 finished with value: 0.12658001778989422 and parameters: {'observation_period_num': 220, 'train_rates': 0.9822340943381201, 'learning_rate': 0.0009580041556288748, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8985940884924555}. Best is trial 11 with value: 0.1258040761947632.[0m
[32m[I 2025-02-09 00:41:26,870][0m Trial 18 finished with value: 0.09973110399329442 and parameters: {'observation_period_num': 185, 'train_rates': 0.8249925306702679, 'learning_rate': 0.0001198726215538979, 'batch_size': 80, 'step_size': 13, 'gamma': 0.8893167026734351}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:42:25,581][0m Trial 19 finished with value: 0.2496225870696895 and parameters: {'observation_period_num': 182, 'train_rates': 0.6719702906726945, 'learning_rate': 0.0001302486489120026, 'batch_size': 77, 'step_size': 11, 'gamma': 0.767254862297595}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:43:21,797][0m Trial 20 finished with value: 0.10293115717340186 and parameters: {'observation_period_num': 178, 'train_rates': 0.8214359720018095, 'learning_rate': 9.386545788923077e-05, 'batch_size': 92, 'step_size': 8, 'gamma': 0.8711279707045256}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:44:19,759][0m Trial 21 finished with value: 0.10630068810362565 and parameters: {'observation_period_num': 181, 'train_rates': 0.822497174871007, 'learning_rate': 8.586953959215037e-05, 'batch_size': 91, 'step_size': 7, 'gamma': 0.8783206347338658}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:45:20,319][0m Trial 22 finished with value: 0.10177703395517382 and parameters: {'observation_period_num': 163, 'train_rates': 0.8314048186318996, 'learning_rate': 8.465209381473001e-05, 'batch_size': 89, 'step_size': 7, 'gamma': 0.8802971574408811}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:46:04,450][0m Trial 23 finished with value: 0.12892000272160484 and parameters: {'observation_period_num': 142, 'train_rates': 0.8140287360080205, 'learning_rate': 5.0025709086523313e-05, 'batch_size': 121, 'step_size': 8, 'gamma': 0.8785244857043525}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:47:18,411][0m Trial 24 finished with value: 0.104964423226896 and parameters: {'observation_period_num': 163, 'train_rates': 0.8818494501112165, 'learning_rate': 0.0001494965906374938, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8566517298887704}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:48:05,921][0m Trial 25 finished with value: 0.1652296306933286 and parameters: {'observation_period_num': 193, 'train_rates': 0.8265703720991376, 'learning_rate': 5.926244393423029e-05, 'batch_size': 114, 'step_size': 9, 'gamma': 0.8408557365440789}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:49:07,808][0m Trial 26 finished with value: 0.2768040190317801 and parameters: {'observation_period_num': 166, 'train_rates': 0.7300096461103718, 'learning_rate': 2.604713488914605e-05, 'batch_size': 81, 'step_size': 9, 'gamma': 0.9039001115638454}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:51:06,232][0m Trial 27 finished with value: 0.14458265378792187 and parameters: {'observation_period_num': 141, 'train_rates': 0.8108706192717887, 'learning_rate': 0.00014779792801456422, 'batch_size': 43, 'step_size': 1, 'gamma': 0.8864034709839107}. Best is trial 18 with value: 0.09973110399329442.[0m
[32m[I 2025-02-09 00:51:48,631][0m Trial 28 finished with value: 0.07587756958069666 and parameters: {'observation_period_num': 100, 'train_rates': 0.8890272046345256, 'learning_rate': 0.0003595026888394628, 'batch_size': 135, 'step_size': 6, 'gamma': 0.8330297010294752}. Best is trial 28 with value: 0.07587756958069666.[0m
[32m[I 2025-02-09 00:52:20,835][0m Trial 29 finished with value: 0.0736935812253332 and parameters: {'observation_period_num': 97, 'train_rates': 0.8874508990338951, 'learning_rate': 0.0004744914731897306, 'batch_size': 184, 'step_size': 6, 'gamma': 0.7977934873160333}. Best is trial 29 with value: 0.0736935812253332.[0m
[32m[I 2025-02-09 00:52:54,284][0m Trial 30 finished with value: 0.0709644817555977 and parameters: {'observation_period_num': 91, 'train_rates': 0.8949490292309294, 'learning_rate': 0.00042716305109933726, 'batch_size': 176, 'step_size': 6, 'gamma': 0.7970164098299986}. Best is trial 30 with value: 0.0709644817555977.[0m
[32m[I 2025-02-09 00:53:29,708][0m Trial 31 finished with value: 0.0786198092820613 and parameters: {'observation_period_num': 81, 'train_rates': 0.8945743979416527, 'learning_rate': 0.0004075891663698987, 'batch_size': 182, 'step_size': 6, 'gamma': 0.7899502231276357}. Best is trial 30 with value: 0.0709644817555977.[0m
[32m[I 2025-02-09 00:54:03,735][0m Trial 32 finished with value: 0.0802080798894167 and parameters: {'observation_period_num': 95, 'train_rates': 0.8906718704153629, 'learning_rate': 0.0004202284217083902, 'batch_size': 179, 'step_size': 6, 'gamma': 0.7917758728018673}. Best is trial 30 with value: 0.0709644817555977.[0m
[32m[I 2025-02-09 00:54:35,148][0m Trial 33 finished with value: 0.10545014590024948 and parameters: {'observation_period_num': 45, 'train_rates': 0.9541082626998448, 'learning_rate': 0.00040866357535547835, 'batch_size': 201, 'step_size': 3, 'gamma': 0.790120793730061}. Best is trial 30 with value: 0.0709644817555977.[0m
[32m[I 2025-02-09 00:55:17,454][0m Trial 34 finished with value: 0.06377553721924997 and parameters: {'observation_period_num': 60, 'train_rates': 0.9453744962187696, 'learning_rate': 0.00038368781127094876, 'batch_size': 151, 'step_size': 6, 'gamma': 0.8079358319458971}. Best is trial 34 with value: 0.06377553721924997.[0m
[32m[I 2025-02-09 00:55:58,928][0m Trial 35 finished with value: 0.08695366233587265 and parameters: {'observation_period_num': 56, 'train_rates': 0.9489777075553734, 'learning_rate': 0.0001903202397453029, 'batch_size': 153, 'step_size': 4, 'gamma': 0.8109063679669771}. Best is trial 34 with value: 0.06377553721924997.[0m
[32m[I 2025-02-09 00:56:42,414][0m Trial 36 finished with value: 0.3856608415224466 and parameters: {'observation_period_num': 20, 'train_rates': 0.9156193319753727, 'learning_rate': 4.483563099192622e-06, 'batch_size': 141, 'step_size': 6, 'gamma': 0.773958556020503}. Best is trial 34 with value: 0.06377553721924997.[0m
[32m[I 2025-02-09 00:57:20,128][0m Trial 37 finished with value: 0.0927659434825182 and parameters: {'observation_period_num': 103, 'train_rates': 0.930252798666523, 'learning_rate': 0.0004923208249033716, 'batch_size': 163, 'step_size': 4, 'gamma': 0.8241643749572735}. Best is trial 34 with value: 0.06377553721924997.[0m
[32m[I 2025-02-09 00:57:48,401][0m Trial 38 finished with value: 0.0961417522651987 and parameters: {'observation_period_num': 105, 'train_rates': 0.8663802021663342, 'learning_rate': 0.00031794389734457864, 'batch_size': 222, 'step_size': 6, 'gamma': 0.8418906838084105}. Best is trial 34 with value: 0.06377553721924997.[0m
[32m[I 2025-02-09 00:58:32,171][0m Trial 39 finished with value: 0.06246467419197223 and parameters: {'observation_period_num': 61, 'train_rates': 0.9441246695944007, 'learning_rate': 0.000571336134671057, 'batch_size': 141, 'step_size': 8, 'gamma': 0.8049769302453113}. Best is trial 39 with value: 0.06246467419197223.[0m
[32m[I 2025-02-09 00:59:03,458][0m Trial 40 finished with value: 0.06932401657104492 and parameters: {'observation_period_num': 60, 'train_rates': 0.9560398179593749, 'learning_rate': 0.0005655046050078534, 'batch_size': 199, 'step_size': 10, 'gamma': 0.8076495198765604}. Best is trial 39 with value: 0.06246467419197223.[0m
[32m[I 2025-02-09 00:59:37,366][0m Trial 41 finished with value: 0.07720129936933517 and parameters: {'observation_period_num': 63, 'train_rates': 0.9619741339440172, 'learning_rate': 0.0006048138772020777, 'batch_size': 199, 'step_size': 10, 'gamma': 0.8062676133634551}. Best is trial 39 with value: 0.06246467419197223.[0m
[32m[I 2025-02-09 01:00:15,021][0m Trial 42 finished with value: 0.048948421608656645 and parameters: {'observation_period_num': 6, 'train_rates': 0.9352703222443298, 'learning_rate': 0.00020820634100893475, 'batch_size': 168, 'step_size': 10, 'gamma': 0.757028801868425}. Best is trial 42 with value: 0.048948421608656645.[0m
[32m[I 2025-02-09 01:00:53,261][0m Trial 43 finished with value: 0.04558125310447587 and parameters: {'observation_period_num': 7, 'train_rates': 0.9153542407669039, 'learning_rate': 0.00020900484018739009, 'batch_size': 166, 'step_size': 9, 'gamma': 0.7512167654408639}. Best is trial 43 with value: 0.04558125310447587.[0m
[32m[I 2025-02-09 01:01:33,002][0m Trial 44 finished with value: 0.04800799645108884 and parameters: {'observation_period_num': 8, 'train_rates': 0.939576879136752, 'learning_rate': 0.00020333503210860827, 'batch_size': 163, 'step_size': 10, 'gamma': 0.7597440839257469}. Best is trial 43 with value: 0.04558125310447587.[0m
[32m[I 2025-02-09 01:02:12,463][0m Trial 45 finished with value: 0.04543165337708261 and parameters: {'observation_period_num': 5, 'train_rates': 0.9239648067795966, 'learning_rate': 0.0001814043013161988, 'batch_size': 158, 'step_size': 10, 'gamma': 0.7502607407238537}. Best is trial 45 with value: 0.04543165337708261.[0m
[32m[I 2025-02-09 01:02:59,573][0m Trial 46 finished with value: 0.06691690659024326 and parameters: {'observation_period_num': 9, 'train_rates': 0.9151436797357365, 'learning_rate': 6.238315026598686e-05, 'batch_size': 133, 'step_size': 9, 'gamma': 0.7515063155285323}. Best is trial 45 with value: 0.04543165337708261.[0m
[32m[I 2025-02-09 01:03:35,219][0m Trial 47 finished with value: 0.04760652516658107 and parameters: {'observation_period_num': 29, 'train_rates': 0.9267197398725036, 'learning_rate': 0.0001903848968063755, 'batch_size': 171, 'step_size': 11, 'gamma': 0.7651621152541974}. Best is trial 45 with value: 0.04543165337708261.[0m
[32m[I 2025-02-09 01:04:13,852][0m Trial 48 finished with value: 0.04599155019968748 and parameters: {'observation_period_num': 29, 'train_rates': 0.9251426205435842, 'learning_rate': 0.00020188481472949106, 'batch_size': 165, 'step_size': 11, 'gamma': 0.7682783266527315}. Best is trial 45 with value: 0.04543165337708261.[0m
[32m[I 2025-02-09 01:04:51,361][0m Trial 49 finished with value: 0.04506912463626196 and parameters: {'observation_period_num': 29, 'train_rates': 0.9123757823494736, 'learning_rate': 0.00020205995157404424, 'batch_size': 158, 'step_size': 11, 'gamma': 0.7753561186314534}. Best is trial 49 with value: 0.04506912463626196.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-09 01:04:51,371][0m A new study created in memory with name: no-name-98d2031c-d38d-4a35-b932-898b966963b2[0m
[32m[I 2025-02-09 01:05:20,584][0m Trial 0 finished with value: 0.41134414310075657 and parameters: {'observation_period_num': 198, 'train_rates': 0.7321099538800409, 'learning_rate': 2.9051961393609447e-05, 'batch_size': 178, 'step_size': 3, 'gamma': 0.9564237226169481}. Best is trial 0 with value: 0.41134414310075657.[0m
[32m[I 2025-02-09 01:06:15,560][0m Trial 1 finished with value: 0.7133431423373388 and parameters: {'observation_period_num': 107, 'train_rates': 0.6663427007289391, 'learning_rate': 1.6087747371240582e-06, 'batch_size': 84, 'step_size': 3, 'gamma': 0.9654257279199135}. Best is trial 0 with value: 0.41134414310075657.[0m
[32m[I 2025-02-09 01:06:46,982][0m Trial 2 finished with value: 0.375268873182081 and parameters: {'observation_period_num': 118, 'train_rates': 0.7640147880264051, 'learning_rate': 1.1275920088022305e-05, 'batch_size': 172, 'step_size': 7, 'gamma': 0.9704622895106713}. Best is trial 2 with value: 0.375268873182081.[0m
[32m[I 2025-02-09 01:07:17,385][0m Trial 3 finished with value: 0.1300896566723828 and parameters: {'observation_period_num': 73, 'train_rates': 0.843625226901412, 'learning_rate': 0.0005872258937342427, 'batch_size': 202, 'step_size': 2, 'gamma': 0.7814774058528514}. Best is trial 3 with value: 0.1300896566723828.[0m
[32m[I 2025-02-09 01:08:08,190][0m Trial 4 finished with value: 0.14421196807833278 and parameters: {'observation_period_num': 228, 'train_rates': 0.9440824398184734, 'learning_rate': 0.00032417244505861525, 'batch_size': 114, 'step_size': 12, 'gamma': 0.7860635423653398}. Best is trial 3 with value: 0.1300896566723828.[0m
[32m[I 2025-02-09 01:08:37,947][0m Trial 5 finished with value: 0.6223554068430889 and parameters: {'observation_period_num': 201, 'train_rates': 0.6357103633096952, 'learning_rate': 3.6157312104341657e-06, 'batch_size': 154, 'step_size': 3, 'gamma': 0.9743018287514156}. Best is trial 3 with value: 0.1300896566723828.[0m
[32m[I 2025-02-09 01:10:46,934][0m Trial 6 finished with value: 0.04175524647106175 and parameters: {'observation_period_num': 9, 'train_rates': 0.8172839964362489, 'learning_rate': 2.1168762074250487e-05, 'batch_size': 41, 'step_size': 11, 'gamma': 0.9395575529283238}. Best is trial 6 with value: 0.04175524647106175.[0m
[32m[I 2025-02-09 01:11:46,738][0m Trial 7 finished with value: 0.3003374081954621 and parameters: {'observation_period_num': 235, 'train_rates': 0.6868370537056875, 'learning_rate': 0.00021655059336080836, 'batch_size': 75, 'step_size': 10, 'gamma': 0.7655266313266218}. Best is trial 6 with value: 0.04175524647106175.[0m
[32m[I 2025-02-09 01:12:15,677][0m Trial 8 finished with value: 0.23944941111501938 and parameters: {'observation_period_num': 236, 'train_rates': 0.788125086390025, 'learning_rate': 0.00043852576358843905, 'batch_size': 191, 'step_size': 12, 'gamma': 0.9743015516077717}. Best is trial 6 with value: 0.04175524647106175.[0m
[32m[I 2025-02-09 01:12:48,229][0m Trial 9 finished with value: 0.22093162126326976 and parameters: {'observation_period_num': 101, 'train_rates': 0.6188774453015555, 'learning_rate': 0.00012485274990291516, 'batch_size': 137, 'step_size': 6, 'gamma': 0.8137215894304395}. Best is trial 6 with value: 0.04175524647106175.[0m
[32m[I 2025-02-09 01:17:26,228][0m Trial 10 finished with value: 0.027174489322288702 and parameters: {'observation_period_num': 5, 'train_rates': 0.874533665912694, 'learning_rate': 5.192011337583312e-05, 'batch_size': 20, 'step_size': 14, 'gamma': 0.9002943954255419}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:21:37,794][0m Trial 11 finished with value: 0.02767732147994923 and parameters: {'observation_period_num': 7, 'train_rates': 0.8768075610442502, 'learning_rate': 5.653649423759654e-05, 'batch_size': 22, 'step_size': 15, 'gamma': 0.8987570996623839}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:26:53,007][0m Trial 12 finished with value: 0.03924290119170748 and parameters: {'observation_period_num': 13, 'train_rates': 0.8979222485304641, 'learning_rate': 9.12136475014855e-05, 'batch_size': 18, 'step_size': 15, 'gamma': 0.893254284704242}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:27:21,327][0m Trial 13 finished with value: 0.10856583714485168 and parameters: {'observation_period_num': 53, 'train_rates': 0.987080597835447, 'learning_rate': 6.787628679113052e-05, 'batch_size': 245, 'step_size': 15, 'gamma': 0.8849473297740805}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:29:03,550][0m Trial 14 finished with value: 0.11282418787722355 and parameters: {'observation_period_num': 58, 'train_rates': 0.8735740550380303, 'learning_rate': 9.276620095544444e-06, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8508478371236914}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:33:37,508][0m Trial 15 finished with value: 0.12762179834714074 and parameters: {'observation_period_num': 158, 'train_rates': 0.9142947682536046, 'learning_rate': 5.66754997666462e-05, 'batch_size': 20, 'step_size': 13, 'gamma': 0.9171684856771963}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:34:35,458][0m Trial 16 finished with value: 0.09861658079779312 and parameters: {'observation_period_num': 20, 'train_rates': 0.8513897945515626, 'learning_rate': 1.2480124813599398e-05, 'batch_size': 99, 'step_size': 9, 'gamma': 0.8500688232600349}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:36:30,577][0m Trial 17 finished with value: 0.0678192675113678 and parameters: {'observation_period_num': 43, 'train_rates': 0.9477892506267318, 'learning_rate': 3.767917523176139e-05, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9149883686714156}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:37:49,739][0m Trial 18 finished with value: 0.11147272585424653 and parameters: {'observation_period_num': 147, 'train_rates': 0.8108313121260498, 'learning_rate': 0.00017197276103016955, 'batch_size': 67, 'step_size': 6, 'gamma': 0.8568879237297174}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:40:08,674][0m Trial 19 finished with value: 0.23552007956478077 and parameters: {'observation_period_num': 73, 'train_rates': 0.7302312549642194, 'learning_rate': 0.0008366795961062164, 'batch_size': 35, 'step_size': 15, 'gamma': 0.8267201774527313}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:41:04,094][0m Trial 20 finished with value: 0.29019354043468354 and parameters: {'observation_period_num': 39, 'train_rates': 0.8942236525865833, 'learning_rate': 4.201957289491916e-06, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9223214866900616}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:44:33,412][0m Trial 21 finished with value: 0.027486294366676232 and parameters: {'observation_period_num': 12, 'train_rates': 0.9018370053494892, 'learning_rate': 8.904554283904274e-05, 'batch_size': 27, 'step_size': 15, 'gamma': 0.8860826563520198}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:49:36,858][0m Trial 22 finished with value: 0.04542357840338828 and parameters: {'observation_period_num': 29, 'train_rates': 0.9344758864897756, 'learning_rate': 5.507840031731099e-05, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8880190881367181}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:52:17,202][0m Trial 23 finished with value: 0.06406223773956299 and parameters: {'observation_period_num': 10, 'train_rates': 0.98893164232069, 'learning_rate': 0.00011850776902257084, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8720200914586121}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:53:44,000][0m Trial 24 finished with value: 0.08040700524690605 and parameters: {'observation_period_num': 70, 'train_rates': 0.8551643589166379, 'learning_rate': 1.9955824089591707e-05, 'batch_size': 65, 'step_size': 11, 'gamma': 0.9009400285714838}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:54:48,364][0m Trial 25 finished with value: 0.09315586981610355 and parameters: {'observation_period_num': 89, 'train_rates': 0.8780621073209777, 'learning_rate': 0.0002095347633831859, 'batch_size': 88, 'step_size': 14, 'gamma': 0.9430420076636664}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:57:24,256][0m Trial 26 finished with value: 0.048380837454172425 and parameters: {'observation_period_num': 39, 'train_rates': 0.8287422787198045, 'learning_rate': 3.782686200016784e-05, 'batch_size': 35, 'step_size': 15, 'gamma': 0.8274179247703023}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 01:59:11,880][0m Trial 27 finished with value: 0.04491857348786674 and parameters: {'observation_period_num': 30, 'train_rates': 0.9158006643380252, 'learning_rate': 7.75519322901394e-05, 'batch_size': 54, 'step_size': 12, 'gamma': 0.8717750051214571}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:00:02,697][0m Trial 28 finished with value: 0.07209927945033363 and parameters: {'observation_period_num': 5, 'train_rates': 0.9534109855097304, 'learning_rate': 2.0342344928691698e-05, 'batch_size': 123, 'step_size': 14, 'gamma': 0.9355756033837989}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:02:57,693][0m Trial 29 finished with value: 0.17959486606469138 and parameters: {'observation_period_num': 29, 'train_rates': 0.7788620946388205, 'learning_rate': 3.3138330377910596e-05, 'batch_size': 29, 'step_size': 11, 'gamma': 0.9089627329160714}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:04:35,401][0m Trial 30 finished with value: 0.3939250388285594 and parameters: {'observation_period_num': 178, 'train_rates': 0.7466292361628846, 'learning_rate': 6.705788588375183e-06, 'batch_size': 49, 'step_size': 13, 'gamma': 0.8761123891685024}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:09:00,928][0m Trial 31 finished with value: 0.05088253477864926 and parameters: {'observation_period_num': 18, 'train_rates': 0.8972166768052712, 'learning_rate': 9.28221581181751e-05, 'batch_size': 21, 'step_size': 15, 'gamma': 0.8953368750667742}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:13:37,643][0m Trial 32 finished with value: 0.02722987289618289 and parameters: {'observation_period_num': 5, 'train_rates': 0.8746557136341525, 'learning_rate': 4.73664500015676e-05, 'batch_size': 20, 'step_size': 15, 'gamma': 0.9280086655907068}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:15:06,081][0m Trial 33 finished with value: 0.059579399741447006 and parameters: {'observation_period_num': 53, 'train_rates': 0.8719046759804643, 'learning_rate': 4.80361764132212e-05, 'batch_size': 65, 'step_size': 14, 'gamma': 0.9579238190487529}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:16:12,261][0m Trial 34 finished with value: 0.29307021457787114 and parameters: {'observation_period_num': 5, 'train_rates': 0.8375891628725093, 'learning_rate': 1.1833559266682626e-06, 'batch_size': 85, 'step_size': 15, 'gamma': 0.9280007813168698}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:16:41,177][0m Trial 35 finished with value: 0.11005721883419667 and parameters: {'observation_period_num': 28, 'train_rates': 0.9188249428906938, 'learning_rate': 2.703405604377699e-05, 'batch_size': 225, 'step_size': 13, 'gamma': 0.9015107916178559}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:19:24,494][0m Trial 36 finished with value: 0.11072946866433038 and parameters: {'observation_period_num': 129, 'train_rates': 0.80228192194946, 'learning_rate': 0.00015097425689724188, 'batch_size': 31, 'step_size': 1, 'gamma': 0.9526806436381339}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:21:26,320][0m Trial 37 finished with value: 0.1117940794909373 and parameters: {'observation_period_num': 62, 'train_rates': 0.8621298366646934, 'learning_rate': 0.00030040507464296364, 'batch_size': 45, 'step_size': 5, 'gamma': 0.8587107735753883}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:26:45,226][0m Trial 38 finished with value: 0.07417860011199325 and parameters: {'observation_period_num': 91, 'train_rates': 0.8856658615394821, 'learning_rate': 1.3714185648579183e-05, 'batch_size': 17, 'step_size': 12, 'gamma': 0.9489461930528781}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:27:22,268][0m Trial 39 finished with value: 0.06356776846391306 and parameters: {'observation_period_num': 44, 'train_rates': 0.8278347373913802, 'learning_rate': 4.6673184481610915e-05, 'batch_size': 152, 'step_size': 14, 'gamma': 0.932939268994413}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:28:41,355][0m Trial 40 finished with value: 0.32839952685214857 and parameters: {'observation_period_num': 214, 'train_rates': 0.9609710221484474, 'learning_rate': 2.578660334852252e-05, 'batch_size': 74, 'step_size': 4, 'gamma': 0.8842469619177331}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:32:02,676][0m Trial 41 finished with value: 0.03523169319233755 and parameters: {'observation_period_num': 17, 'train_rates': 0.9071085008226564, 'learning_rate': 9.028260443859682e-05, 'batch_size': 28, 'step_size': 15, 'gamma': 0.9062711493269392}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:35:21,732][0m Trial 42 finished with value: 0.0672512270617657 and parameters: {'observation_period_num': 20, 'train_rates': 0.9293525179323273, 'learning_rate': 0.00011372915692784123, 'batch_size': 29, 'step_size': 15, 'gamma': 0.9059573464570086}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:37:42,887][0m Trial 43 finished with value: 0.06444686436161552 and parameters: {'observation_period_num': 20, 'train_rates': 0.9672465327226326, 'learning_rate': 0.0003344415332540051, 'batch_size': 42, 'step_size': 14, 'gamma': 0.9169965901222871}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:39:26,485][0m Trial 44 finished with value: 0.04111917644326662 and parameters: {'observation_period_num': 6, 'train_rates': 0.9024528463837478, 'learning_rate': 7.012308157829487e-05, 'batch_size': 56, 'step_size': 15, 'gamma': 0.8844161885682676}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:44:57,915][0m Trial 45 finished with value: 0.062005899345393675 and parameters: {'observation_period_num': 37, 'train_rates': 0.8478332916225604, 'learning_rate': 9.641916303846473e-05, 'batch_size': 16, 'step_size': 12, 'gamma': 0.9264693521468724}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:48:22,868][0m Trial 46 finished with value: 0.0448737056572226 and parameters: {'observation_period_num': 19, 'train_rates': 0.8659532643574416, 'learning_rate': 0.00015445742586879082, 'batch_size': 27, 'step_size': 13, 'gamma': 0.9605126417139838}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:50:38,942][0m Trial 47 finished with value: 0.07132756176658651 and parameters: {'observation_period_num': 46, 'train_rates': 0.9098236202763196, 'learning_rate': 6.355995520052777e-05, 'batch_size': 42, 'step_size': 11, 'gamma': 0.9873324606127495}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:52:10,878][0m Trial 48 finished with value: 0.058710215208323105 and parameters: {'observation_period_num': 31, 'train_rates': 0.9295496323006013, 'learning_rate': 3.708625176658273e-05, 'batch_size': 63, 'step_size': 8, 'gamma': 0.8621027031057672}. Best is trial 10 with value: 0.027174489322288702.[0m
[32m[I 2025-02-09 02:52:35,813][0m Trial 49 finished with value: 0.3060803656362825 and parameters: {'observation_period_num': 246, 'train_rates': 0.6689337667986344, 'learning_rate': 0.00023328634466389598, 'batch_size': 191, 'step_size': 14, 'gamma': 0.910323871020421}. Best is trial 10 with value: 0.027174489322288702.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-09 02:52:35,824][0m A new study created in memory with name: no-name-89cc278a-150f-4c4a-b42a-a54602b692b9[0m
[32m[I 2025-02-09 02:55:30,901][0m Trial 0 finished with value: 0.3314296062146581 and parameters: {'observation_period_num': 114, 'train_rates': 0.7134976029242669, 'learning_rate': 8.336930116978249e-06, 'batch_size': 27, 'step_size': 4, 'gamma': 0.8994968540297121}. Best is trial 0 with value: 0.3314296062146581.[0m
[32m[I 2025-02-09 02:56:35,294][0m Trial 1 finished with value: 0.08068061915375072 and parameters: {'observation_period_num': 37, 'train_rates': 0.9425216643235546, 'learning_rate': 3.355649136644619e-05, 'batch_size': 94, 'step_size': 11, 'gamma': 0.7843936818491777}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 02:57:11,558][0m Trial 2 finished with value: 0.1159396418698237 and parameters: {'observation_period_num': 110, 'train_rates': 0.8012221424338618, 'learning_rate': 0.00014667758715927802, 'batch_size': 148, 'step_size': 7, 'gamma': 0.7668427713836988}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 02:57:33,504][0m Trial 3 finished with value: 0.6580841493379193 and parameters: {'observation_period_num': 107, 'train_rates': 0.6147731427929393, 'learning_rate': 4.50859740338305e-06, 'batch_size': 237, 'step_size': 9, 'gamma': 0.8471474233936611}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 02:59:35,524][0m Trial 4 finished with value: 0.10081746520623108 and parameters: {'observation_period_num': 134, 'train_rates': 0.8849560985219237, 'learning_rate': 9.74780554742927e-05, 'batch_size': 44, 'step_size': 8, 'gamma': 0.7737578282314662}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:00:48,809][0m Trial 5 finished with value: 0.4236806233723958 and parameters: {'observation_period_num': 89, 'train_rates': 0.8186389432569029, 'learning_rate': 1.6349132722707643e-06, 'batch_size': 73, 'step_size': 10, 'gamma': 0.8560450064907342}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:01:21,174][0m Trial 6 finished with value: 0.2473940140297336 and parameters: {'observation_period_num': 11, 'train_rates': 0.7796463048796638, 'learning_rate': 1.7021636397577643e-05, 'batch_size': 175, 'step_size': 7, 'gamma': 0.7941169182905319}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:01:48,675][0m Trial 7 finished with value: 0.0986228361099708 and parameters: {'observation_period_num': 143, 'train_rates': 0.8737420572657262, 'learning_rate': 0.00031189677306316425, 'batch_size': 222, 'step_size': 7, 'gamma': 0.8116916751177129}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:02:16,449][0m Trial 8 finished with value: 0.47832651533170817 and parameters: {'observation_period_num': 128, 'train_rates': 0.6637499817092546, 'learning_rate': 8.521954860453988e-06, 'batch_size': 181, 'step_size': 14, 'gamma': 0.7875428574455152}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:02:39,882][0m Trial 9 finished with value: 0.6060860863108518 and parameters: {'observation_period_num': 26, 'train_rates': 0.6812650275266456, 'learning_rate': 4.005135817904027e-06, 'batch_size': 232, 'step_size': 4, 'gamma': 0.9121162205998794}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:03:39,927][0m Trial 10 finished with value: 0.13621336221694946 and parameters: {'observation_period_num': 181, 'train_rates': 0.9802138238243303, 'learning_rate': 0.0009065681165076535, 'batch_size': 99, 'step_size': 15, 'gamma': 0.9721275472783449}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:04:31,703][0m Trial 11 finished with value: 0.13575085924125055 and parameters: {'observation_period_num': 252, 'train_rates': 0.947696183806374, 'learning_rate': 0.0008851808398352027, 'batch_size': 111, 'step_size': 11, 'gamma': 0.8350657794870462}. Best is trial 1 with value: 0.08068061915375072.[0m
Early stopping at epoch 62
[32m[I 2025-02-09 03:04:50,545][0m Trial 12 finished with value: 0.4619710250994638 and parameters: {'observation_period_num': 57, 'train_rates': 0.9040297575361295, 'learning_rate': 7.596060435764109e-05, 'batch_size': 203, 'step_size': 1, 'gamma': 0.814177673530293}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:05:31,779][0m Trial 13 finished with value: 0.0989646201500785 and parameters: {'observation_period_num': 172, 'train_rates': 0.8734043354387633, 'learning_rate': 0.000251242590252183, 'batch_size': 139, 'step_size': 12, 'gamma': 0.7504726671181701}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:06:45,719][0m Trial 14 finished with value: 0.15568247692925588 and parameters: {'observation_period_num': 177, 'train_rates': 0.9248088798566767, 'learning_rate': 3.648131030866476e-05, 'batch_size': 76, 'step_size': 13, 'gamma': 0.8128149306106206}. Best is trial 1 with value: 0.08068061915375072.[0m
[32m[I 2025-02-09 03:07:38,580][0m Trial 15 finished with value: 0.052421787700247816 and parameters: {'observation_period_num': 60, 'train_rates': 0.8441767316200214, 'learning_rate': 0.0003551650394151578, 'batch_size': 110, 'step_size': 5, 'gamma': 0.8879419875573075}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:08:27,792][0m Trial 16 finished with value: 0.125699740005598 and parameters: {'observation_period_num': 55, 'train_rates': 0.8376413215873064, 'learning_rate': 3.991244154397891e-05, 'batch_size': 115, 'step_size': 3, 'gamma': 0.8968888289370075}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:10:02,136][0m Trial 17 finished with value: 0.08215600252151489 and parameters: {'observation_period_num': 53, 'train_rates': 0.988469535274425, 'learning_rate': 0.00037458361877967843, 'batch_size': 65, 'step_size': 5, 'gamma': 0.9472262202235664}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:10:35,379][0m Trial 18 finished with value: 1.222566093315417 and parameters: {'observation_period_num': 81, 'train_rates': 0.7650844151414679, 'learning_rate': 1.0345042271485115e-06, 'batch_size': 160, 'step_size': 10, 'gamma': 0.878816989726766}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:11:32,850][0m Trial 19 finished with value: 0.07738627034884232 and parameters: {'observation_period_num': 32, 'train_rates': 0.8449530595243884, 'learning_rate': 2.072190336424916e-05, 'batch_size': 98, 'step_size': 6, 'gamma': 0.9315794873022181}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:12:16,521][0m Trial 20 finished with value: 0.2177708397110303 and parameters: {'observation_period_num': 5, 'train_rates': 0.7465792188810393, 'learning_rate': 1.6149160989062844e-05, 'batch_size': 122, 'step_size': 2, 'gamma': 0.938255694244456}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:13:18,445][0m Trial 21 finished with value: 0.05249382251429349 and parameters: {'observation_period_num': 35, 'train_rates': 0.8439456260604381, 'learning_rate': 5.76196383817143e-05, 'batch_size': 89, 'step_size': 5, 'gamma': 0.9276071449625592}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:15:05,208][0m Trial 22 finished with value: 0.07163178503112408 and parameters: {'observation_period_num': 77, 'train_rates': 0.8457462609561708, 'learning_rate': 8.052755555614132e-05, 'batch_size': 50, 'step_size': 5, 'gamma': 0.930197293204938}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:19:55,436][0m Trial 23 finished with value: 0.15382517626945008 and parameters: {'observation_period_num': 77, 'train_rates': 0.8372725960860578, 'learning_rate': 7.284627324839907e-05, 'batch_size': 18, 'step_size': 5, 'gamma': 0.9765045931746598}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:22:26,538][0m Trial 24 finished with value: 0.05849273608703362 and parameters: {'observation_period_num': 66, 'train_rates': 0.8035796700626124, 'learning_rate': 0.0001660767557970634, 'batch_size': 34, 'step_size': 3, 'gamma': 0.8809027664813031}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:23:56,447][0m Trial 25 finished with value: 0.20469299315027223 and parameters: {'observation_period_num': 55, 'train_rates': 0.7327765285356681, 'learning_rate': 0.00017686844843448237, 'batch_size': 55, 'step_size': 1, 'gamma': 0.874742685877535}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:24:59,931][0m Trial 26 finished with value: 0.1783471331121279 and parameters: {'observation_period_num': 28, 'train_rates': 0.7902203613279366, 'learning_rate': 0.0004732968244410683, 'batch_size': 83, 'step_size': 3, 'gamma': 0.9066182404703209}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:25:41,449][0m Trial 27 finished with value: 0.05928819946186785 and parameters: {'observation_period_num': 68, 'train_rates': 0.8203571226102853, 'learning_rate': 0.0001504644361874535, 'batch_size': 133, 'step_size': 3, 'gamma': 0.9563038927232776}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:29:02,297][0m Trial 28 finished with value: 0.11551351971066123 and parameters: {'observation_period_num': 94, 'train_rates': 0.8668608528275611, 'learning_rate': 0.0005490681081582307, 'batch_size': 27, 'step_size': 6, 'gamma': 0.8819675095004812}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:30:23,261][0m Trial 29 finished with value: 0.16348749504660268 and parameters: {'observation_period_num': 40, 'train_rates': 0.7161831537339228, 'learning_rate': 0.0002331485841534195, 'batch_size': 61, 'step_size': 4, 'gamma': 0.9088686359063615}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:31:19,979][0m Trial 30 finished with value: 0.3829867606784435 and parameters: {'observation_period_num': 239, 'train_rates': 0.7587999633965258, 'learning_rate': 5.794836559913894e-05, 'batch_size': 83, 'step_size': 2, 'gamma': 0.8933776998788409}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:32:07,452][0m Trial 31 finished with value: 0.06412031061334775 and parameters: {'observation_period_num': 66, 'train_rates': 0.8194722260598173, 'learning_rate': 0.00014554059720001925, 'batch_size': 118, 'step_size': 3, 'gamma': 0.9594077647213428}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:32:49,283][0m Trial 32 finished with value: 0.054361055312131534 and parameters: {'observation_period_num': 41, 'train_rates': 0.8055244462688734, 'learning_rate': 0.0001241060123644718, 'batch_size': 132, 'step_size': 4, 'gamma': 0.9221844653621994}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:33:24,248][0m Trial 33 finished with value: 0.05606582488289615 and parameters: {'observation_period_num': 21, 'train_rates': 0.797476370264815, 'learning_rate': 0.00011769425596473247, 'batch_size': 158, 'step_size': 6, 'gamma': 0.9216414761183565}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:34:03,586][0m Trial 34 finished with value: 0.06339275784842618 and parameters: {'observation_period_num': 16, 'train_rates': 0.9030115692991812, 'learning_rate': 5.297749133778399e-05, 'batch_size': 153, 'step_size': 6, 'gamma': 0.9163924247895046}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:34:37,427][0m Trial 35 finished with value: 0.18796178732874208 and parameters: {'observation_period_num': 42, 'train_rates': 0.7772279837566588, 'learning_rate': 0.00011539911578271974, 'batch_size': 173, 'step_size': 8, 'gamma': 0.9256434560689226}. Best is trial 15 with value: 0.052421787700247816.[0m
[32m[I 2025-02-09 03:35:21,105][0m Trial 36 finished with value: 0.03337479654003824 and parameters: {'observation_period_num': 16, 'train_rates': 0.8566057987187734, 'learning_rate': 0.0005703802171064247, 'batch_size': 133, 'step_size': 5, 'gamma': 0.8554670901385715}. Best is trial 36 with value: 0.03337479654003824.[0m
[32m[I 2025-02-09 03:36:07,479][0m Trial 37 finished with value: 0.049593838428457576 and parameters: {'observation_period_num': 42, 'train_rates': 0.8975600898034569, 'learning_rate': 0.0005679685122900744, 'batch_size': 130, 'step_size': 5, 'gamma': 0.8618496297483826}. Best is trial 36 with value: 0.03337479654003824.[0m
[32m[I 2025-02-09 03:36:33,433][0m Trial 38 finished with value: 0.08288505607017196 and parameters: {'observation_period_num': 99, 'train_rates': 0.8942172207370946, 'learning_rate': 0.000638614497990225, 'batch_size': 255, 'step_size': 8, 'gamma': 0.8565505218858562}. Best is trial 36 with value: 0.03337479654003824.[0m
[32m[I 2025-02-09 03:37:30,923][0m Trial 39 finished with value: 0.036148928759390846 and parameters: {'observation_period_num': 5, 'train_rates': 0.9232305343227044, 'learning_rate': 0.0003677726544929421, 'batch_size': 106, 'step_size': 5, 'gamma': 0.8439938076626696}. Best is trial 36 with value: 0.03337479654003824.[0m
[32m[I 2025-02-09 03:38:31,149][0m Trial 40 finished with value: 0.041858499798455524 and parameters: {'observation_period_num': 5, 'train_rates': 0.93834102386898, 'learning_rate': 0.0006722974972815825, 'batch_size': 101, 'step_size': 9, 'gamma': 0.8345120761590383}. Best is trial 36 with value: 0.03337479654003824.[0m
[32m[I 2025-02-09 03:39:31,991][0m Trial 41 finished with value: 0.03138951020656337 and parameters: {'observation_period_num': 6, 'train_rates': 0.9520679629661928, 'learning_rate': 0.00039467821505350374, 'batch_size': 103, 'step_size': 9, 'gamma': 0.8430033932187998}. Best is trial 41 with value: 0.03138951020656337.[0m
[32m[I 2025-02-09 03:40:32,813][0m Trial 42 finished with value: 0.029544208547194226 and parameters: {'observation_period_num': 9, 'train_rates': 0.9530006785023715, 'learning_rate': 0.0007880854141748699, 'batch_size': 104, 'step_size': 9, 'gamma': 0.8480540634271523}. Best is trial 42 with value: 0.029544208547194226.[0m
[32m[I 2025-02-09 03:41:35,013][0m Trial 43 finished with value: 0.02745757328103418 and parameters: {'observation_period_num': 6, 'train_rates': 0.9612813971997037, 'learning_rate': 0.000960176488606088, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8327231057248112}. Best is trial 43 with value: 0.02745757328103418.[0m
[32m[I 2025-02-09 03:42:19,137][0m Trial 44 finished with value: 0.036201536655426025 and parameters: {'observation_period_num': 19, 'train_rates': 0.9588688910584571, 'learning_rate': 0.0009467661271548581, 'batch_size': 145, 'step_size': 9, 'gamma': 0.8423710165743369}. Best is trial 43 with value: 0.02745757328103418.[0m
[32m[I 2025-02-09 03:43:17,768][0m Trial 45 finished with value: 0.03303932026028633 and parameters: {'observation_period_num': 12, 'train_rates': 0.9659248432142216, 'learning_rate': 0.0003994130116728034, 'batch_size': 106, 'step_size': 10, 'gamma': 0.8240211379275075}. Best is trial 43 with value: 0.02745757328103418.[0m
[32m[I 2025-02-09 03:44:07,045][0m Trial 46 finished with value: 0.03516538068652153 and parameters: {'observation_period_num': 18, 'train_rates': 0.9686436219584904, 'learning_rate': 0.0002463257292883599, 'batch_size': 124, 'step_size': 10, 'gamma': 0.8285141714846762}. Best is trial 43 with value: 0.02745757328103418.[0m
[32m[I 2025-02-09 03:45:12,045][0m Trial 47 finished with value: 0.04183299533308369 and parameters: {'observation_period_num': 26, 'train_rates': 0.9255185038579089, 'learning_rate': 0.0007975630844643277, 'batch_size': 93, 'step_size': 11, 'gamma': 0.7996036753850442}. Best is trial 43 with value: 0.02745757328103418.[0m
[32m[I 2025-02-09 03:45:58,615][0m Trial 48 finished with value: 0.03220861777663231 and parameters: {'observation_period_num': 14, 'train_rates': 0.9678114272524984, 'learning_rate': 0.0004386228640396624, 'batch_size': 141, 'step_size': 11, 'gamma': 0.8197081448010533}. Best is trial 43 with value: 0.02745757328103418.[0m
[32m[I 2025-02-09 03:46:32,726][0m Trial 49 finished with value: 0.11174993216991425 and parameters: {'observation_period_num': 114, 'train_rates': 0.9700913954239323, 'learning_rate': 0.00044374771914306983, 'batch_size': 192, 'step_size': 12, 'gamma': 0.8203222836802644}. Best is trial 43 with value: 0.02745757328103418.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-09 03:46:32,736][0m A new study created in memory with name: no-name-f5781de2-4df3-4dea-be6d-d97544055d83[0m
[32m[I 2025-02-09 03:47:05,947][0m Trial 0 finished with value: 1.2286469311010642 and parameters: {'observation_period_num': 107, 'train_rates': 0.7440387349058696, 'learning_rate': 1.719954981205915e-06, 'batch_size': 162, 'step_size': 9, 'gamma': 0.8140910214287066}. Best is trial 0 with value: 1.2286469311010642.[0m
[32m[I 2025-02-09 03:47:59,675][0m Trial 1 finished with value: 0.5951316952705383 and parameters: {'observation_period_num': 69, 'train_rates': 0.9886903095133716, 'learning_rate': 6.365813406478177e-06, 'batch_size': 115, 'step_size': 5, 'gamma': 0.7662427174445134}. Best is trial 1 with value: 0.5951316952705383.[0m
[32m[I 2025-02-09 03:48:52,972][0m Trial 2 finished with value: 0.04751047119498253 and parameters: {'observation_period_num': 12, 'train_rates': 0.8700744090019411, 'learning_rate': 0.000532630257357226, 'batch_size': 108, 'step_size': 2, 'gamma': 0.8581382418416537}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:50:01,331][0m Trial 3 finished with value: 0.898154509360673 and parameters: {'observation_period_num': 251, 'train_rates': 0.9553035198829065, 'learning_rate': 1.3793728287983012e-06, 'batch_size': 83, 'step_size': 5, 'gamma': 0.8691376910897869}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:50:39,809][0m Trial 4 finished with value: 0.18999819457530975 and parameters: {'observation_period_num': 158, 'train_rates': 0.9782226853998578, 'learning_rate': 0.0002585259423472748, 'batch_size': 167, 'step_size': 10, 'gamma': 0.8126110048657038}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:51:15,449][0m Trial 5 finished with value: 0.05137047747319395 and parameters: {'observation_period_num': 34, 'train_rates': 0.8948705776492738, 'learning_rate': 0.00017341290805123884, 'batch_size': 164, 'step_size': 15, 'gamma': 0.9724496928363067}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:52:59,002][0m Trial 6 finished with value: 0.14431539555022746 and parameters: {'observation_period_num': 52, 'train_rates': 0.9166611449303506, 'learning_rate': 1.005310722103355e-05, 'batch_size': 57, 'step_size': 1, 'gamma': 0.9867050113690621}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:53:23,994][0m Trial 7 finished with value: 0.22155916040135062 and parameters: {'observation_period_num': 45, 'train_rates': 0.767581297486943, 'learning_rate': 0.0001334817873517063, 'batch_size': 237, 'step_size': 3, 'gamma': 0.8829844715531664}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:53:49,806][0m Trial 8 finished with value: 0.2956833273310034 and parameters: {'observation_period_num': 241, 'train_rates': 0.681909422365452, 'learning_rate': 0.00048050956322038017, 'batch_size': 199, 'step_size': 9, 'gamma': 0.8168893485436164}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:57:24,290][0m Trial 9 finished with value: 0.5549592506013067 and parameters: {'observation_period_num': 219, 'train_rates': 0.7339132742304944, 'learning_rate': 1.9049320644056456e-06, 'batch_size': 21, 'step_size': 13, 'gamma': 0.759774514637838}. Best is trial 2 with value: 0.04751047119498253.[0m
[32m[I 2025-02-09 03:58:15,814][0m Trial 10 finished with value: 0.04225647359066237 and parameters: {'observation_period_num': 6, 'train_rates': 0.8372564793316162, 'learning_rate': 0.0008829065447024578, 'batch_size': 111, 'step_size': 1, 'gamma': 0.9261632630178157}. Best is trial 10 with value: 0.04225647359066237.[0m
[32m[I 2025-02-09 03:59:07,284][0m Trial 11 finished with value: 0.041632200280825295 and parameters: {'observation_period_num': 7, 'train_rates': 0.8458129754278718, 'learning_rate': 0.0009979606629368092, 'batch_size': 112, 'step_size': 1, 'gamma': 0.9297053137511377}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:00:26,945][0m Trial 12 finished with value: 0.045460873078308706 and parameters: {'observation_period_num': 5, 'train_rates': 0.8468040114065419, 'learning_rate': 0.0009697137447283284, 'batch_size': 71, 'step_size': 5, 'gamma': 0.9345752294709533}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:01:11,215][0m Trial 13 finished with value: 0.3145443334661681 and parameters: {'observation_period_num': 106, 'train_rates': 0.8173802537640217, 'learning_rate': 3.9173447181088206e-05, 'batch_size': 126, 'step_size': 1, 'gamma': 0.9265068421853511}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:01:56,410][0m Trial 14 finished with value: 0.9392450330483967 and parameters: {'observation_period_num': 159, 'train_rates': 0.6060400677706901, 'learning_rate': 6.158364242419926e-05, 'batch_size': 99, 'step_size': 4, 'gamma': 0.9276648644895779}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:05:05,487][0m Trial 15 finished with value: 0.09503707454189078 and parameters: {'observation_period_num': 82, 'train_rates': 0.8131384658560649, 'learning_rate': 0.0009715723331433929, 'batch_size': 27, 'step_size': 7, 'gamma': 0.9008613615219138}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:05:47,652][0m Trial 16 finished with value: 0.050179048258567525 and parameters: {'observation_period_num': 21, 'train_rates': 0.9274518021252663, 'learning_rate': 8.375752366420414e-05, 'batch_size': 148, 'step_size': 7, 'gamma': 0.9590652587425192}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:06:17,167][0m Trial 17 finished with value: 0.4469354493277414 and parameters: {'observation_period_num': 160, 'train_rates': 0.8454220782632167, 'learning_rate': 1.7481588254588877e-05, 'batch_size': 198, 'step_size': 3, 'gamma': 0.9041703130492534}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:07:54,029][0m Trial 18 finished with value: 0.16796217087465218 and parameters: {'observation_period_num': 75, 'train_rates': 0.7032069391641492, 'learning_rate': 0.0003525371626190143, 'batch_size': 49, 'step_size': 1, 'gamma': 0.951145692727427}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:08:31,741][0m Trial 19 finished with value: 0.13707740068385413 and parameters: {'observation_period_num': 193, 'train_rates': 0.7862150006536515, 'learning_rate': 0.000952502583224665, 'batch_size': 137, 'step_size': 7, 'gamma': 0.8411161300415051}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:09:34,439][0m Trial 20 finished with value: 0.1215570375489789 and parameters: {'observation_period_num': 117, 'train_rates': 0.8745225154765112, 'learning_rate': 0.00019402082768992898, 'batch_size': 89, 'step_size': 12, 'gamma': 0.90412160803862}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:10:55,726][0m Trial 21 finished with value: 0.06412726308362118 and parameters: {'observation_period_num': 8, 'train_rates': 0.836977179103368, 'learning_rate': 0.0007852585918739904, 'batch_size': 68, 'step_size': 5, 'gamma': 0.9379826208754114}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:12:09,573][0m Trial 22 finished with value: 0.057108757157738395 and parameters: {'observation_period_num': 30, 'train_rates': 0.858246024709942, 'learning_rate': 0.0004290486088540315, 'batch_size': 76, 'step_size': 3, 'gamma': 0.919632175864353}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:14:15,472][0m Trial 23 finished with value: 0.17363364965963324 and parameters: {'observation_period_num': 5, 'train_rates': 0.7930193377541767, 'learning_rate': 0.0005940083010331869, 'batch_size': 42, 'step_size': 2, 'gamma': 0.9505459101387067}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:15:09,684][0m Trial 24 finished with value: 0.09201320498147597 and parameters: {'observation_period_num': 57, 'train_rates': 0.8910623293740356, 'learning_rate': 0.00028429053382710977, 'batch_size': 109, 'step_size': 4, 'gamma': 0.9895377458654256}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:15:52,687][0m Trial 25 finished with value: 0.05376261658966541 and parameters: {'observation_period_num': 36, 'train_rates': 0.8250810864150148, 'learning_rate': 0.00012988456131160205, 'batch_size': 128, 'step_size': 6, 'gamma': 0.8913863594371707}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:16:55,468][0m Trial 26 finished with value: 0.048481498363203014 and parameters: {'observation_period_num': 20, 'train_rates': 0.9212722702573867, 'learning_rate': 0.00094944020146267, 'batch_size': 94, 'step_size': 2, 'gamma': 0.9656718874049665}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:18:15,468][0m Trial 27 finished with value: 0.20496751490551615 and parameters: {'observation_period_num': 82, 'train_rates': 0.7679676517326792, 'learning_rate': 0.0003112712584102514, 'batch_size': 64, 'step_size': 4, 'gamma': 0.9376589598950797}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:18:47,242][0m Trial 28 finished with value: 0.9765973390119472 and parameters: {'observation_period_num': 57, 'train_rates': 0.8944735777008617, 'learning_rate': 4.111272279821468e-06, 'batch_size': 190, 'step_size': 1, 'gamma': 0.9189301782268688}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:19:09,709][0m Trial 29 finished with value: 0.2527152234774978 and parameters: {'observation_period_num': 91, 'train_rates': 0.7380029850068212, 'learning_rate': 8.722707012545603e-05, 'batch_size': 256, 'step_size': 10, 'gamma': 0.8750815018950645}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:19:46,784][0m Trial 30 finished with value: 0.09459609840508107 and parameters: {'observation_period_num': 27, 'train_rates': 0.8053307611876915, 'learning_rate': 2.845237395030008e-05, 'batch_size': 148, 'step_size': 3, 'gamma': 0.9421135675568688}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:20:40,418][0m Trial 31 finished with value: 0.05075810482868781 and parameters: {'observation_period_num': 6, 'train_rates': 0.8681201030753091, 'learning_rate': 0.000574741602772838, 'batch_size': 108, 'step_size': 2, 'gamma': 0.8544100521239069}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:21:35,895][0m Trial 32 finished with value: 0.05542016279953059 and parameters: {'observation_period_num': 20, 'train_rates': 0.8497331249825893, 'learning_rate': 0.0005833717250159552, 'batch_size': 104, 'step_size': 2, 'gamma': 0.8402167901194894}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:22:24,661][0m Trial 33 finished with value: 0.05108948035554572 and parameters: {'observation_period_num': 43, 'train_rates': 0.8755720322862979, 'learning_rate': 0.0006362484373242262, 'batch_size': 120, 'step_size': 5, 'gamma': 0.8615281868057041}. Best is trial 11 with value: 0.041632200280825295.[0m
Early stopping at epoch 53
[32m[I 2025-02-09 04:23:04,791][0m Trial 34 finished with value: 0.22873429501358467 and parameters: {'observation_period_num': 61, 'train_rates': 0.9456401265718177, 'learning_rate': 0.0003927446108976515, 'batch_size': 82, 'step_size': 1, 'gamma': 0.7765481461625737}. Best is trial 11 with value: 0.041632200280825295.[0m
[32m[I 2025-02-09 04:23:46,717][0m Trial 35 finished with value: 0.040448268911071644 and parameters: {'observation_period_num': 13, 'train_rates': 0.8405836224489843, 'learning_rate': 0.00025949742097792404, 'batch_size': 141, 'step_size': 6, 'gamma': 0.9104499970606522}. Best is trial 35 with value: 0.040448268911071644.[0m
[32m[I 2025-02-09 04:24:27,979][0m Trial 36 finished with value: 0.040888886107411794 and parameters: {'observation_period_num': 17, 'train_rates': 0.8307815030245433, 'learning_rate': 0.00019863728902617188, 'batch_size': 140, 'step_size': 8, 'gamma': 0.9160976433168497}. Best is trial 35 with value: 0.040448268911071644.[0m
[32m[I 2025-02-09 04:25:05,326][0m Trial 37 finished with value: 0.1828568080098725 and parameters: {'observation_period_num': 41, 'train_rates': 0.7705657783116449, 'learning_rate': 0.0002473810495570697, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9115595140656688}. Best is trial 35 with value: 0.040448268911071644.[0m
[32m[I 2025-02-09 04:25:38,185][0m Trial 38 finished with value: 0.16007770864776444 and parameters: {'observation_period_num': 138, 'train_rates': 0.8240926404723266, 'learning_rate': 0.00013363682076096466, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8917807224372047}. Best is trial 35 with value: 0.040448268911071644.[0m
[32m[I 2025-02-09 04:26:13,392][0m Trial 39 finished with value: 0.18004457767975082 and parameters: {'observation_period_num': 18, 'train_rates': 0.788753382866543, 'learning_rate': 0.00021316298999434188, 'batch_size': 158, 'step_size': 9, 'gamma': 0.9732175196254247}. Best is trial 35 with value: 0.040448268911071644.[0m
[32m[I 2025-02-09 04:26:58,073][0m Trial 40 finished with value: 0.08483716524114795 and parameters: {'observation_period_num': 68, 'train_rates': 0.8944225755190048, 'learning_rate': 5.098350271555559e-05, 'batch_size': 133, 'step_size': 12, 'gamma': 0.8873131218791006}. Best is trial 35 with value: 0.040448268911071644.[0m
[32m[I 2025-02-09 04:27:29,997][0m Trial 41 finished with value: 0.03407533211927665 and parameters: {'observation_period_num': 6, 'train_rates': 0.8395009498867446, 'learning_rate': 0.0007234273774435494, 'batch_size': 179, 'step_size': 6, 'gamma': 0.9283912681042312}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:28:02,330][0m Trial 42 finished with value: 0.046772814526849865 and parameters: {'observation_period_num': 31, 'train_rates': 0.832977804853211, 'learning_rate': 0.0004191554476852228, 'batch_size': 184, 'step_size': 6, 'gamma': 0.9120267771577771}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:28:28,454][0m Trial 43 finished with value: 0.08682077907314392 and parameters: {'observation_period_num': 46, 'train_rates': 0.805487949447868, 'learning_rate': 0.0007081626418619062, 'batch_size': 217, 'step_size': 8, 'gamma': 0.9495526236017361}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:29:06,503][0m Trial 44 finished with value: 0.042603362232078744 and parameters: {'observation_period_num': 17, 'train_rates': 0.8625793991102735, 'learning_rate': 0.00014629626953960594, 'batch_size': 158, 'step_size': 6, 'gamma': 0.927262706088448}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:29:36,358][0m Trial 45 finished with value: 0.0689909274435856 and parameters: {'observation_period_num': 30, 'train_rates': 0.910182378448087, 'learning_rate': 0.000472761331160516, 'batch_size': 215, 'step_size': 10, 'gamma': 0.8758366409605547}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:30:21,175][0m Trial 46 finished with value: 0.15482508063154377 and parameters: {'observation_period_num': 14, 'train_rates': 0.7506932858569919, 'learning_rate': 0.0002178947257183344, 'batch_size': 120, 'step_size': 9, 'gamma': 0.975497347271067}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:30:56,643][0m Trial 47 finished with value: 0.08014929710306982 and parameters: {'observation_period_num': 50, 'train_rates': 0.8342515974474715, 'learning_rate': 0.000334976901987653, 'batch_size': 173, 'step_size': 15, 'gamma': 0.9175577112874341}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:31:48,837][0m Trial 48 finished with value: 0.07061768862596482 and parameters: {'observation_period_num': 38, 'train_rates': 0.8818947634755395, 'learning_rate': 0.0007343176943490895, 'batch_size': 113, 'step_size': 7, 'gamma': 0.930716695111176}. Best is trial 41 with value: 0.03407533211927665.[0m
[32m[I 2025-02-09 04:32:27,294][0m Trial 49 finished with value: 0.1466107907438139 and parameters: {'observation_period_num': 5, 'train_rates': 0.7111172328708736, 'learning_rate': 0.0005093119263743558, 'batch_size': 140, 'step_size': 6, 'gamma': 0.9011439919373818}. Best is trial 41 with value: 0.03407533211927665.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 8, 'train_rates': 0.9201910377771746, 'learning_rate': 6.282516697543941e-05, 'batch_size': 27, 'step_size': 4, 'gamma': 0.9726977588947996}
Epoch 1/300, trend Loss: 0.4338 | 0.2147
Epoch 2/300, trend Loss: 0.1407 | 0.1438
Epoch 3/300, trend Loss: 0.1243 | 0.1148
Epoch 4/300, trend Loss: 0.1152 | 0.0966
Epoch 5/300, trend Loss: 0.1095 | 0.0844
Epoch 6/300, trend Loss: 0.1060 | 0.0771
Epoch 7/300, trend Loss: 0.1035 | 0.0727
Epoch 8/300, trend Loss: 0.1019 | 0.0706
Epoch 9/300, trend Loss: 0.1006 | 0.0691
Epoch 10/300, trend Loss: 0.0994 | 0.0677
Epoch 11/300, trend Loss: 0.0977 | 0.0657
Epoch 12/300, trend Loss: 0.0957 | 0.0640
Epoch 13/300, trend Loss: 0.0932 | 0.0619
Epoch 14/300, trend Loss: 0.0909 | 0.0602
Epoch 15/300, trend Loss: 0.0885 | 0.0582
Epoch 16/300, trend Loss: 0.0865 | 0.0564
Epoch 17/300, trend Loss: 0.0846 | 0.0545
Epoch 18/300, trend Loss: 0.0830 | 0.0527
Epoch 19/300, trend Loss: 0.0814 | 0.0507
Epoch 20/300, trend Loss: 0.0799 | 0.0488
Epoch 21/300, trend Loss: 0.0786 | 0.0472
Epoch 22/300, trend Loss: 0.0776 | 0.0460
Epoch 23/300, trend Loss: 0.0767 | 0.0450
Epoch 24/300, trend Loss: 0.0761 | 0.0441
Epoch 25/300, trend Loss: 0.0755 | 0.0434
Epoch 26/300, trend Loss: 0.0750 | 0.0428
Epoch 27/300, trend Loss: 0.0744 | 0.0422
Epoch 28/300, trend Loss: 0.0739 | 0.0417
Epoch 29/300, trend Loss: 0.0734 | 0.0412
Epoch 30/300, trend Loss: 0.0729 | 0.0408
Epoch 31/300, trend Loss: 0.0724 | 0.0403
Epoch 32/300, trend Loss: 0.0720 | 0.0400
Epoch 33/300, trend Loss: 0.0715 | 0.0396
Epoch 34/300, trend Loss: 0.0711 | 0.0392
Epoch 35/300, trend Loss: 0.0706 | 0.0388
Epoch 36/300, trend Loss: 0.0701 | 0.0383
Epoch 37/300, trend Loss: 0.0696 | 0.0379
Epoch 38/300, trend Loss: 0.0691 | 0.0374
Epoch 39/300, trend Loss: 0.0686 | 0.0369
Epoch 40/300, trend Loss: 0.0681 | 0.0364
Epoch 41/300, trend Loss: 0.0677 | 0.0359
Epoch 42/300, trend Loss: 0.0673 | 0.0354
Epoch 43/300, trend Loss: 0.0669 | 0.0349
Epoch 44/300, trend Loss: 0.0666 | 0.0345
Epoch 45/300, trend Loss: 0.0662 | 0.0341
Epoch 46/300, trend Loss: 0.0660 | 0.0337
Epoch 47/300, trend Loss: 0.0657 | 0.0333
Epoch 48/300, trend Loss: 0.0654 | 0.0330
Epoch 49/300, trend Loss: 0.0652 | 0.0327
Epoch 50/300, trend Loss: 0.0649 | 0.0324
Epoch 51/300, trend Loss: 0.0647 | 0.0321
Epoch 52/300, trend Loss: 0.0645 | 0.0319
Epoch 53/300, trend Loss: 0.0642 | 0.0316
Epoch 54/300, trend Loss: 0.0640 | 0.0314
Epoch 55/300, trend Loss: 0.0638 | 0.0312
Epoch 56/300, trend Loss: 0.0636 | 0.0310
Epoch 57/300, trend Loss: 0.0634 | 0.0308
Epoch 58/300, trend Loss: 0.0632 | 0.0307
Epoch 59/300, trend Loss: 0.0630 | 0.0305
Epoch 60/300, trend Loss: 0.0628 | 0.0303
Epoch 61/300, trend Loss: 0.0626 | 0.0301
Epoch 62/300, trend Loss: 0.0624 | 0.0300
Epoch 63/300, trend Loss: 0.0622 | 0.0298
Epoch 64/300, trend Loss: 0.0620 | 0.0297
Epoch 65/300, trend Loss: 0.0618 | 0.0295
Epoch 66/300, trend Loss: 0.0616 | 0.0294
Epoch 67/300, trend Loss: 0.0615 | 0.0293
Epoch 68/300, trend Loss: 0.0613 | 0.0291
Epoch 69/300, trend Loss: 0.0611 | 0.0290
Epoch 70/300, trend Loss: 0.0610 | 0.0289
Epoch 71/300, trend Loss: 0.0608 | 0.0287
Epoch 72/300, trend Loss: 0.0606 | 0.0286
Epoch 73/300, trend Loss: 0.0604 | 0.0285
Epoch 74/300, trend Loss: 0.0603 | 0.0284
Epoch 75/300, trend Loss: 0.0601 | 0.0282
Epoch 76/300, trend Loss: 0.0600 | 0.0281
Epoch 77/300, trend Loss: 0.0598 | 0.0280
Epoch 78/300, trend Loss: 0.0596 | 0.0279
Epoch 79/300, trend Loss: 0.0595 | 0.0278
Epoch 80/300, trend Loss: 0.0593 | 0.0277
Epoch 81/300, trend Loss: 0.0592 | 0.0275
Epoch 82/300, trend Loss: 0.0590 | 0.0274
Epoch 83/300, trend Loss: 0.0588 | 0.0273
Epoch 84/300, trend Loss: 0.0587 | 0.0272
Epoch 85/300, trend Loss: 0.0585 | 0.0271
Epoch 86/300, trend Loss: 0.0584 | 0.0270
Epoch 87/300, trend Loss: 0.0582 | 0.0268
Epoch 88/300, trend Loss: 0.0581 | 0.0267
Epoch 89/300, trend Loss: 0.0579 | 0.0266
Epoch 90/300, trend Loss: 0.0578 | 0.0265
Epoch 91/300, trend Loss: 0.0577 | 0.0264
Epoch 92/300, trend Loss: 0.0575 | 0.0263
Epoch 93/300, trend Loss: 0.0574 | 0.0262
Epoch 94/300, trend Loss: 0.0573 | 0.0261
Epoch 95/300, trend Loss: 0.0571 | 0.0260
Epoch 96/300, trend Loss: 0.0570 | 0.0259
Epoch 97/300, trend Loss: 0.0569 | 0.0258
Epoch 98/300, trend Loss: 0.0568 | 0.0257
Epoch 99/300, trend Loss: 0.0566 | 0.0256
Epoch 100/300, trend Loss: 0.0565 | 0.0256
Epoch 101/300, trend Loss: 0.0564 | 0.0255
Epoch 102/300, trend Loss: 0.0563 | 0.0254
Epoch 103/300, trend Loss: 0.0562 | 0.0253
Epoch 104/300, trend Loss: 0.0561 | 0.0253
Epoch 105/300, trend Loss: 0.0560 | 0.0252
Epoch 106/300, trend Loss: 0.0559 | 0.0251
Epoch 107/300, trend Loss: 0.0558 | 0.0251
Epoch 108/300, trend Loss: 0.0557 | 0.0250
Epoch 109/300, trend Loss: 0.0557 | 0.0250
Epoch 110/300, trend Loss: 0.0556 | 0.0249
Epoch 111/300, trend Loss: 0.0555 | 0.0248
Epoch 112/300, trend Loss: 0.0554 | 0.0248
Epoch 113/300, trend Loss: 0.0553 | 0.0247
Epoch 114/300, trend Loss: 0.0553 | 0.0247
Epoch 115/300, trend Loss: 0.0552 | 0.0246
Epoch 116/300, trend Loss: 0.0551 | 0.0246
Epoch 117/300, trend Loss: 0.0550 | 0.0245
Epoch 118/300, trend Loss: 0.0549 | 0.0244
Epoch 119/300, trend Loss: 0.0549 | 0.0244
Epoch 120/300, trend Loss: 0.0548 | 0.0243
Epoch 121/300, trend Loss: 0.0547 | 0.0243
Epoch 122/300, trend Loss: 0.0547 | 0.0243
Epoch 123/300, trend Loss: 0.0546 | 0.0242
Epoch 124/300, trend Loss: 0.0545 | 0.0242
Epoch 125/300, trend Loss: 0.0544 | 0.0241
Epoch 126/300, trend Loss: 0.0544 | 0.0241
Epoch 127/300, trend Loss: 0.0543 | 0.0241
Epoch 128/300, trend Loss: 0.0542 | 0.0241
Epoch 129/300, trend Loss: 0.0542 | 0.0240
Epoch 130/300, trend Loss: 0.0541 | 0.0240
Epoch 131/300, trend Loss: 0.0540 | 0.0240
Epoch 132/300, trend Loss: 0.0540 | 0.0240
Epoch 133/300, trend Loss: 0.0539 | 0.0239
Epoch 134/300, trend Loss: 0.0539 | 0.0239
Epoch 135/300, trend Loss: 0.0538 | 0.0239
Epoch 136/300, trend Loss: 0.0538 | 0.0239
Epoch 137/300, trend Loss: 0.0537 | 0.0239
Epoch 138/300, trend Loss: 0.0537 | 0.0238
Epoch 139/300, trend Loss: 0.0536 | 0.0238
Epoch 140/300, trend Loss: 0.0536 | 0.0238
Epoch 141/300, trend Loss: 0.0536 | 0.0238
Epoch 142/300, trend Loss: 0.0535 | 0.0238
Epoch 143/300, trend Loss: 0.0535 | 0.0237
Epoch 144/300, trend Loss: 0.0534 | 0.0237
Epoch 145/300, trend Loss: 0.0534 | 0.0237
Epoch 146/300, trend Loss: 0.0534 | 0.0237
Epoch 147/300, trend Loss: 0.0533 | 0.0237
Epoch 148/300, trend Loss: 0.0533 | 0.0237
Epoch 149/300, trend Loss: 0.0533 | 0.0237
Epoch 150/300, trend Loss: 0.0532 | 0.0237
Epoch 151/300, trend Loss: 0.0532 | 0.0237
Epoch 152/300, trend Loss: 0.0532 | 0.0237
Epoch 153/300, trend Loss: 0.0531 | 0.0237
Epoch 154/300, trend Loss: 0.0531 | 0.0237
Epoch 155/300, trend Loss: 0.0531 | 0.0237
Epoch 156/300, trend Loss: 0.0530 | 0.0237
Epoch 157/300, trend Loss: 0.0530 | 0.0237
Epoch 158/300, trend Loss: 0.0530 | 0.0237
Epoch 159/300, trend Loss: 0.0529 | 0.0237
Epoch 160/300, trend Loss: 0.0529 | 0.0237
Epoch 161/300, trend Loss: 0.0529 | 0.0237
Epoch 162/300, trend Loss: 0.0528 | 0.0237
Epoch 163/300, trend Loss: 0.0528 | 0.0237
Epoch 164/300, trend Loss: 0.0528 | 0.0237
Epoch 165/300, trend Loss: 0.0527 | 0.0237
Epoch 166/300, trend Loss: 0.0527 | 0.0237
Epoch 167/300, trend Loss: 0.0527 | 0.0237
Epoch 168/300, trend Loss: 0.0526 | 0.0237
Epoch 169/300, trend Loss: 0.0526 | 0.0237
Epoch 170/300, trend Loss: 0.0526 | 0.0237
Epoch 171/300, trend Loss: 0.0526 | 0.0237
Epoch 172/300, trend Loss: 0.0525 | 0.0237
Epoch 173/300, trend Loss: 0.0525 | 0.0237
Epoch 174/300, trend Loss: 0.0525 | 0.0237
Epoch 175/300, trend Loss: 0.0524 | 0.0237
Epoch 176/300, trend Loss: 0.0524 | 0.0237
Epoch 177/300, trend Loss: 0.0524 | 0.0237
Epoch 178/300, trend Loss: 0.0524 | 0.0237
Epoch 179/300, trend Loss: 0.0523 | 0.0237
Epoch 180/300, trend Loss: 0.0523 | 0.0237
Epoch 181/300, trend Loss: 0.0523 | 0.0237
Epoch 182/300, trend Loss: 0.0523 | 0.0237
Epoch 183/300, trend Loss: 0.0522 | 0.0238
Epoch 184/300, trend Loss: 0.0522 | 0.0237
Epoch 185/300, trend Loss: 0.0522 | 0.0238
Epoch 186/300, trend Loss: 0.0522 | 0.0238
Epoch 187/300, trend Loss: 0.0521 | 0.0238
Epoch 188/300, trend Loss: 0.0521 | 0.0238
Epoch 189/300, trend Loss: 0.0521 | 0.0238
Epoch 190/300, trend Loss: 0.0521 | 0.0238
Epoch 191/300, trend Loss: 0.0520 | 0.0238
Epoch 192/300, trend Loss: 0.0520 | 0.0238
Epoch 193/300, trend Loss: 0.0520 | 0.0238
Epoch 194/300, trend Loss: 0.0520 | 0.0238
Epoch 195/300, trend Loss: 0.0520 | 0.0239
Epoch 196/300, trend Loss: 0.0519 | 0.0238
Epoch 197/300, trend Loss: 0.0519 | 0.0239
Epoch 198/300, trend Loss: 0.0519 | 0.0239
Epoch 199/300, trend Loss: 0.0519 | 0.0239
Epoch 200/300, trend Loss: 0.0519 | 0.0239
Epoch 201/300, trend Loss: 0.0519 | 0.0239
Epoch 202/300, trend Loss: 0.0518 | 0.0239
Epoch 203/300, trend Loss: 0.0518 | 0.0239
Epoch 204/300, trend Loss: 0.0518 | 0.0239
Epoch 205/300, trend Loss: 0.0518 | 0.0240
Epoch 206/300, trend Loss: 0.0518 | 0.0240
Epoch 207/300, trend Loss: 0.0518 | 0.0240
Epoch 208/300, trend Loss: 0.0518 | 0.0240
Epoch 209/300, trend Loss: 0.0517 | 0.0240
Epoch 210/300, trend Loss: 0.0517 | 0.0240
Epoch 211/300, trend Loss: 0.0517 | 0.0240
Epoch 212/300, trend Loss: 0.0517 | 0.0240
Epoch 213/300, trend Loss: 0.0517 | 0.0240
Epoch 214/300, trend Loss: 0.0517 | 0.0240
Epoch 215/300, trend Loss: 0.0517 | 0.0240
Epoch 216/300, trend Loss: 0.0517 | 0.0240
Epoch 217/300, trend Loss: 0.0517 | 0.0240
Epoch 218/300, trend Loss: 0.0517 | 0.0240
Epoch 219/300, trend Loss: 0.0516 | 0.0239
Epoch 220/300, trend Loss: 0.0516 | 0.0239
Epoch 221/300, trend Loss: 0.0516 | 0.0239
Epoch 222/300, trend Loss: 0.0516 | 0.0239
Epoch 223/300, trend Loss: 0.0516 | 0.0238
Epoch 224/300, trend Loss: 0.0516 | 0.0238
Epoch 225/300, trend Loss: 0.0516 | 0.0238
Epoch 226/300, trend Loss: 0.0516 | 0.0238
Epoch 227/300, trend Loss: 0.0515 | 0.0238
Epoch 228/300, trend Loss: 0.0515 | 0.0238
Epoch 229/300, trend Loss: 0.0515 | 0.0237
Epoch 230/300, trend Loss: 0.0515 | 0.0237
Epoch 231/300, trend Loss: 0.0515 | 0.0237
Epoch 232/300, trend Loss: 0.0515 | 0.0237
Epoch 233/300, trend Loss: 0.0515 | 0.0237
Epoch 234/300, trend Loss: 0.0514 | 0.0237
Epoch 235/300, trend Loss: 0.0514 | 0.0236
Epoch 236/300, trend Loss: 0.0514 | 0.0236
Epoch 237/300, trend Loss: 0.0514 | 0.0236
Epoch 238/300, trend Loss: 0.0514 | 0.0236
Epoch 239/300, trend Loss: 0.0514 | 0.0236
Epoch 240/300, trend Loss: 0.0514 | 0.0236
Epoch 241/300, trend Loss: 0.0513 | 0.0236
Epoch 242/300, trend Loss: 0.0513 | 0.0236
Epoch 243/300, trend Loss: 0.0513 | 0.0236
Epoch 244/300, trend Loss: 0.0513 | 0.0236
Epoch 245/300, trend Loss: 0.0513 | 0.0235
Epoch 246/300, trend Loss: 0.0513 | 0.0235
Epoch 247/300, trend Loss: 0.0513 | 0.0235
Epoch 248/300, trend Loss: 0.0512 | 0.0235
Epoch 249/300, trend Loss: 0.0512 | 0.0235
Epoch 250/300, trend Loss: 0.0512 | 0.0235
Epoch 251/300, trend Loss: 0.0512 | 0.0235
Epoch 252/300, trend Loss: 0.0512 | 0.0235
Epoch 253/300, trend Loss: 0.0512 | 0.0235
Epoch 254/300, trend Loss: 0.0512 | 0.0235
Epoch 255/300, trend Loss: 0.0512 | 0.0235
Epoch 256/300, trend Loss: 0.0511 | 0.0235
Epoch 257/300, trend Loss: 0.0511 | 0.0235
Epoch 258/300, trend Loss: 0.0511 | 0.0235
Epoch 259/300, trend Loss: 0.0511 | 0.0235
Epoch 260/300, trend Loss: 0.0511 | 0.0235
Epoch 261/300, trend Loss: 0.0511 | 0.0235
Epoch 262/300, trend Loss: 0.0511 | 0.0235
Epoch 263/300, trend Loss: 0.0511 | 0.0235
Epoch 264/300, trend Loss: 0.0511 | 0.0235
Epoch 265/300, trend Loss: 0.0510 | 0.0235
Epoch 266/300, trend Loss: 0.0510 | 0.0235
Epoch 267/300, trend Loss: 0.0510 | 0.0235
Epoch 268/300, trend Loss: 0.0510 | 0.0235
Epoch 269/300, trend Loss: 0.0510 | 0.0235
Epoch 270/300, trend Loss: 0.0510 | 0.0235
Epoch 271/300, trend Loss: 0.0510 | 0.0235
Epoch 272/300, trend Loss: 0.0510 | 0.0235
Epoch 273/300, trend Loss: 0.0510 | 0.0235
Epoch 274/300, trend Loss: 0.0510 | 0.0235
Epoch 275/300, trend Loss: 0.0509 | 0.0235
Epoch 276/300, trend Loss: 0.0509 | 0.0235
Epoch 277/300, trend Loss: 0.0509 | 0.0235
Epoch 278/300, trend Loss: 0.0509 | 0.0235
Epoch 279/300, trend Loss: 0.0509 | 0.0235
Epoch 280/300, trend Loss: 0.0509 | 0.0235
Epoch 281/300, trend Loss: 0.0509 | 0.0235
Epoch 282/300, trend Loss: 0.0509 | 0.0235
Epoch 283/300, trend Loss: 0.0509 | 0.0235
Epoch 284/300, trend Loss: 0.0509 | 0.0235
Epoch 285/300, trend Loss: 0.0509 | 0.0235
Epoch 286/300, trend Loss: 0.0509 | 0.0235
Epoch 287/300, trend Loss: 0.0508 | 0.0235
Epoch 288/300, trend Loss: 0.0508 | 0.0235
Epoch 289/300, trend Loss: 0.0508 | 0.0235
Epoch 290/300, trend Loss: 0.0508 | 0.0235
Epoch 291/300, trend Loss: 0.0508 | 0.0235
Epoch 292/300, trend Loss: 0.0508 | 0.0235
Epoch 293/300, trend Loss: 0.0508 | 0.0235
Epoch 294/300, trend Loss: 0.0508 | 0.0235
Epoch 295/300, trend Loss: 0.0508 | 0.0235
Epoch 296/300, trend Loss: 0.0508 | 0.0235
Epoch 297/300, trend Loss: 0.0508 | 0.0235
Epoch 298/300, trend Loss: 0.0508 | 0.0235
Epoch 299/300, trend Loss: 0.0508 | 0.0235
Epoch 300/300, trend Loss: 0.0508 | 0.0235
Training seasonal_0 component with params: {'observation_period_num': 11, 'train_rates': 0.8933064980422442, 'learning_rate': 0.0001038739535399735, 'batch_size': 34, 'step_size': 15, 'gamma': 0.9897970535894725}
Epoch 1/300, seasonal_0 Loss: 0.2373 | 0.1683
Epoch 2/300, seasonal_0 Loss: 0.1365 | 0.1157
Epoch 3/300, seasonal_0 Loss: 0.1212 | 0.0961
Epoch 4/300, seasonal_0 Loss: 0.1137 | 0.0872
Epoch 5/300, seasonal_0 Loss: 0.1098 | 0.0821
Epoch 6/300, seasonal_0 Loss: 0.1069 | 0.0785
Epoch 7/300, seasonal_0 Loss: 0.1036 | 0.0766
Epoch 8/300, seasonal_0 Loss: 0.1005 | 0.0750
Epoch 9/300, seasonal_0 Loss: 0.0979 | 0.0736
Epoch 10/300, seasonal_0 Loss: 0.0956 | 0.0721
Epoch 11/300, seasonal_0 Loss: 0.0929 | 0.0707
Epoch 12/300, seasonal_0 Loss: 0.0900 | 0.0699
Epoch 13/300, seasonal_0 Loss: 0.0876 | 0.0692
Epoch 14/300, seasonal_0 Loss: 0.0859 | 0.0687
Epoch 15/300, seasonal_0 Loss: 0.0845 | 0.0684
Epoch 16/300, seasonal_0 Loss: 0.0834 | 0.0681
Epoch 17/300, seasonal_0 Loss: 0.0825 | 0.0679
Epoch 18/300, seasonal_0 Loss: 0.0817 | 0.0673
Epoch 19/300, seasonal_0 Loss: 0.0808 | 0.0656
Epoch 20/300, seasonal_0 Loss: 0.0796 | 0.0625
Epoch 21/300, seasonal_0 Loss: 0.0781 | 0.0579
Epoch 22/300, seasonal_0 Loss: 0.0765 | 0.0524
Epoch 23/300, seasonal_0 Loss: 0.0750 | 0.0469
Epoch 24/300, seasonal_0 Loss: 0.0738 | 0.0431
Epoch 25/300, seasonal_0 Loss: 0.0730 | 0.0422
Epoch 26/300, seasonal_0 Loss: 0.0722 | 0.0424
Epoch 27/300, seasonal_0 Loss: 0.0716 | 0.0423
Epoch 28/300, seasonal_0 Loss: 0.0711 | 0.0419
Epoch 29/300, seasonal_0 Loss: 0.0706 | 0.0414
Epoch 30/300, seasonal_0 Loss: 0.0701 | 0.0408
Epoch 31/300, seasonal_0 Loss: 0.0695 | 0.0402
Epoch 32/300, seasonal_0 Loss: 0.0690 | 0.0397
Epoch 33/300, seasonal_0 Loss: 0.0685 | 0.0391
Epoch 34/300, seasonal_0 Loss: 0.0681 | 0.0387
Epoch 35/300, seasonal_0 Loss: 0.0677 | 0.0382
Epoch 36/300, seasonal_0 Loss: 0.0674 | 0.0379
Epoch 37/300, seasonal_0 Loss: 0.0670 | 0.0376
Epoch 38/300, seasonal_0 Loss: 0.0667 | 0.0375
Epoch 39/300, seasonal_0 Loss: 0.0664 | 0.0380
Epoch 40/300, seasonal_0 Loss: 0.0659 | 0.0386
Epoch 41/300, seasonal_0 Loss: 0.0654 | 0.0391
Epoch 42/300, seasonal_0 Loss: 0.0652 | 0.0394
Epoch 43/300, seasonal_0 Loss: 0.0653 | 0.0396
Epoch 44/300, seasonal_0 Loss: 0.0656 | 0.0410
Epoch 45/300, seasonal_0 Loss: 0.0656 | 0.0410
Epoch 46/300, seasonal_0 Loss: 0.0659 | 0.0397
Epoch 47/300, seasonal_0 Loss: 0.0653 | 0.0395
Epoch 48/300, seasonal_0 Loss: 0.0640 | 0.0371
Epoch 49/300, seasonal_0 Loss: 0.0633 | 0.0364
Epoch 50/300, seasonal_0 Loss: 0.0632 | 0.0361
Epoch 51/300, seasonal_0 Loss: 0.0623 | 0.0359
Epoch 52/300, seasonal_0 Loss: 0.0620 | 0.0360
Epoch 53/300, seasonal_0 Loss: 0.0614 | 0.0359
Epoch 54/300, seasonal_0 Loss: 0.0610 | 0.0359
Epoch 55/300, seasonal_0 Loss: 0.0606 | 0.0359
Epoch 56/300, seasonal_0 Loss: 0.0602 | 0.0360
Epoch 57/300, seasonal_0 Loss: 0.0598 | 0.0360
Epoch 58/300, seasonal_0 Loss: 0.0595 | 0.0363
Epoch 59/300, seasonal_0 Loss: 0.0591 | 0.0366
Epoch 60/300, seasonal_0 Loss: 0.0587 | 0.0375
Epoch 61/300, seasonal_0 Loss: 0.0578 | 0.0392
Epoch 62/300, seasonal_0 Loss: 0.0559 | 0.0383
Epoch 63/300, seasonal_0 Loss: 0.0541 | 0.0363
Epoch 64/300, seasonal_0 Loss: 0.0597 | 0.0563
Epoch 65/300, seasonal_0 Loss: 0.0627 | 0.0398
Epoch 66/300, seasonal_0 Loss: 0.0542 | 0.0364
Epoch 67/300, seasonal_0 Loss: 0.0519 | 0.0374
Epoch 68/300, seasonal_0 Loss: 0.0513 | 0.0354
Epoch 69/300, seasonal_0 Loss: 0.0529 | 0.0414
Epoch 70/300, seasonal_0 Loss: 0.0524 | 0.0364
Epoch 71/300, seasonal_0 Loss: 0.0508 | 0.0368
Epoch 72/300, seasonal_0 Loss: 0.0502 | 0.0376
Epoch 73/300, seasonal_0 Loss: 0.0498 | 0.0376
Epoch 74/300, seasonal_0 Loss: 0.0543 | 0.0461
Epoch 75/300, seasonal_0 Loss: 0.0521 | 0.0407
Epoch 76/300, seasonal_0 Loss: 0.0497 | 0.0399
Epoch 77/300, seasonal_0 Loss: 0.0490 | 0.0405
Epoch 78/300, seasonal_0 Loss: 0.0485 | 0.0414
Epoch 79/300, seasonal_0 Loss: 0.0485 | 0.0410
Epoch 80/300, seasonal_0 Loss: 0.0484 | 0.0445
Epoch 81/300, seasonal_0 Loss: 0.0494 | 0.0405
Epoch 82/300, seasonal_0 Loss: 0.0475 | 0.0416
Epoch 83/300, seasonal_0 Loss: 0.0473 | 0.0414
Epoch 84/300, seasonal_0 Loss: 0.0469 | 0.0418
Epoch 85/300, seasonal_0 Loss: 0.0472 | 0.0425
Epoch 86/300, seasonal_0 Loss: 0.0470 | 0.0437
Epoch 87/300, seasonal_0 Loss: 0.0469 | 0.0452
Epoch 88/300, seasonal_0 Loss: 0.0478 | 0.0464
Epoch 89/300, seasonal_0 Loss: 0.0480 | 0.0460
Epoch 90/300, seasonal_0 Loss: 0.0475 | 0.0434
Epoch 91/300, seasonal_0 Loss: 0.0479 | 0.0437
Epoch 92/300, seasonal_0 Loss: 0.0475 | 0.0419
Epoch 93/300, seasonal_0 Loss: 0.0467 | 0.0425
Epoch 94/300, seasonal_0 Loss: 0.0462 | 0.0422
Epoch 95/300, seasonal_0 Loss: 0.0459 | 0.0426
Epoch 96/300, seasonal_0 Loss: 0.0457 | 0.0423
Epoch 97/300, seasonal_0 Loss: 0.0454 | 0.0431
Epoch 98/300, seasonal_0 Loss: 0.0454 | 0.0429
Epoch 99/300, seasonal_0 Loss: 0.0449 | 0.0433
Epoch 100/300, seasonal_0 Loss: 0.0448 | 0.0433
Epoch 101/300, seasonal_0 Loss: 0.0444 | 0.0433
Epoch 102/300, seasonal_0 Loss: 0.0444 | 0.0438
Epoch 103/300, seasonal_0 Loss: 0.0442 | 0.0441
Epoch 104/300, seasonal_0 Loss: 0.0441 | 0.0445
Epoch 105/300, seasonal_0 Loss: 0.0440 | 0.0449
Epoch 106/300, seasonal_0 Loss: 0.0437 | 0.0453
Epoch 107/300, seasonal_0 Loss: 0.0434 | 0.0461
Epoch 108/300, seasonal_0 Loss: 0.0433 | 0.0458
Epoch 109/300, seasonal_0 Loss: 0.0430 | 0.0474
Epoch 110/300, seasonal_0 Loss: 0.0428 | 0.0476
Epoch 111/300, seasonal_0 Loss: 0.0425 | 0.0461
Epoch 112/300, seasonal_0 Loss: 0.0422 | 0.0475
Epoch 113/300, seasonal_0 Loss: 0.0422 | 0.0498
Epoch 114/300, seasonal_0 Loss: 0.0420 | 0.0477
Epoch 115/300, seasonal_0 Loss: 0.0420 | 0.0510
Epoch 116/300, seasonal_0 Loss: 0.0417 | 0.0527
Epoch 117/300, seasonal_0 Loss: 0.0414 | 0.0567
Epoch 118/300, seasonal_0 Loss: 0.0413 | 0.0542
Epoch 119/300, seasonal_0 Loss: 0.0417 | 0.0528
Epoch 120/300, seasonal_0 Loss: 0.0427 | 0.0538
Epoch 121/300, seasonal_0 Loss: 0.0416 | 0.0507
Epoch 122/300, seasonal_0 Loss: 0.0410 | 0.0539
Epoch 123/300, seasonal_0 Loss: 0.0409 | 0.0540
Epoch 124/300, seasonal_0 Loss: 0.0404 | 0.0566
Epoch 125/300, seasonal_0 Loss: 0.0404 | 0.0537
Epoch 126/300, seasonal_0 Loss: 0.0402 | 0.0571
Epoch 127/300, seasonal_0 Loss: 0.0397 | 0.0561
Epoch 128/300, seasonal_0 Loss: 0.0398 | 0.0555
Epoch 129/300, seasonal_0 Loss: 0.0399 | 0.0530
Epoch 130/300, seasonal_0 Loss: 0.0398 | 0.0491
Epoch 131/300, seasonal_0 Loss: 0.0393 | 0.0454
Epoch 132/300, seasonal_0 Loss: 0.0391 | 0.0436
Epoch 133/300, seasonal_0 Loss: 0.0390 | 0.0420
Epoch 134/300, seasonal_0 Loss: 0.0388 | 0.0403
Epoch 135/300, seasonal_0 Loss: 0.0384 | 0.0391
Epoch 136/300, seasonal_0 Loss: 0.0380 | 0.0381
Epoch 137/300, seasonal_0 Loss: 0.0373 | 0.0381
Epoch 138/300, seasonal_0 Loss: 0.0369 | 0.0374
Epoch 139/300, seasonal_0 Loss: 0.0364 | 0.0375
Epoch 140/300, seasonal_0 Loss: 0.0361 | 0.0371
Epoch 141/300, seasonal_0 Loss: 0.0358 | 0.0374
Epoch 142/300, seasonal_0 Loss: 0.0357 | 0.0376
Epoch 143/300, seasonal_0 Loss: 0.0354 | 0.0384
Epoch 144/300, seasonal_0 Loss: 0.0351 | 0.0377
Epoch 145/300, seasonal_0 Loss: 0.0349 | 0.0377
Epoch 146/300, seasonal_0 Loss: 0.0348 | 0.0377
Epoch 147/300, seasonal_0 Loss: 0.0346 | 0.0389
Epoch 148/300, seasonal_0 Loss: 0.0345 | 0.0385
Epoch 149/300, seasonal_0 Loss: 0.0345 | 0.0386
Epoch 150/300, seasonal_0 Loss: 0.0349 | 0.0410
Epoch 151/300, seasonal_0 Loss: 0.0345 | 0.0394
Epoch 152/300, seasonal_0 Loss: 0.0349 | 0.0391
Epoch 153/300, seasonal_0 Loss: 0.0358 | 0.0448
Epoch 154/300, seasonal_0 Loss: 0.0355 | 0.0415
Epoch 155/300, seasonal_0 Loss: 0.0345 | 0.0453
Epoch 156/300, seasonal_0 Loss: 0.0344 | 0.0439
Epoch 157/300, seasonal_0 Loss: 0.0347 | 0.0440
Epoch 158/300, seasonal_0 Loss: 0.0339 | 0.0435
Epoch 159/300, seasonal_0 Loss: 0.0338 | 0.0457
Epoch 160/300, seasonal_0 Loss: 0.0346 | 0.0450
Epoch 161/300, seasonal_0 Loss: 0.0337 | 0.0441
Epoch 162/300, seasonal_0 Loss: 0.0335 | 0.0461
Epoch 163/300, seasonal_0 Loss: 0.0348 | 0.0439
Epoch 164/300, seasonal_0 Loss: 0.0337 | 0.0429
Epoch 165/300, seasonal_0 Loss: 0.0331 | 0.0439
Epoch 166/300, seasonal_0 Loss: 0.0331 | 0.0427
Epoch 167/300, seasonal_0 Loss: 0.0341 | 0.0397
Epoch 168/300, seasonal_0 Loss: 0.0328 | 0.0385
Epoch 169/300, seasonal_0 Loss: 0.0331 | 0.0396
Epoch 170/300, seasonal_0 Loss: 0.0323 | 0.0396
Epoch 171/300, seasonal_0 Loss: 0.0319 | 0.0391
Epoch 172/300, seasonal_0 Loss: 0.0312 | 0.0395
Epoch 173/300, seasonal_0 Loss: 0.0309 | 0.0399
Epoch 174/300, seasonal_0 Loss: 0.0306 | 0.0398
Epoch 175/300, seasonal_0 Loss: 0.0303 | 0.0407
Epoch 176/300, seasonal_0 Loss: 0.0303 | 0.0407
Epoch 177/300, seasonal_0 Loss: 0.0301 | 0.0413
Epoch 178/300, seasonal_0 Loss: 0.0301 | 0.0416
Epoch 179/300, seasonal_0 Loss: 0.0300 | 0.0421
Epoch 180/300, seasonal_0 Loss: 0.0298 | 0.0419
Epoch 181/300, seasonal_0 Loss: 0.0300 | 0.0426
Epoch 182/300, seasonal_0 Loss: 0.0307 | 0.0429
Epoch 183/300, seasonal_0 Loss: 0.0307 | 0.0441
Epoch 184/300, seasonal_0 Loss: 0.0315 | 0.0461
Epoch 185/300, seasonal_0 Loss: 0.0309 | 0.0493
Epoch 186/300, seasonal_0 Loss: 0.0324 | 0.0504
Epoch 187/300, seasonal_0 Loss: 0.0311 | 0.0591
Epoch 188/300, seasonal_0 Loss: 0.0324 | 0.0513
Epoch 189/300, seasonal_0 Loss: 0.0313 | 0.0486
Epoch 190/300, seasonal_0 Loss: 0.0302 | 0.0565
Epoch 191/300, seasonal_0 Loss: 0.0326 | 0.0573
Epoch 192/300, seasonal_0 Loss: 0.0450 | 0.0495
Epoch 193/300, seasonal_0 Loss: 0.0438 | 0.0554
Epoch 194/300, seasonal_0 Loss: 0.0443 | 0.0548
Epoch 195/300, seasonal_0 Loss: 0.0451 | 0.0530
Epoch 196/300, seasonal_0 Loss: 0.0468 | 0.0487
Epoch 197/300, seasonal_0 Loss: 0.0468 | 0.0585
Epoch 198/300, seasonal_0 Loss: 0.0462 | 0.0561
Epoch 199/300, seasonal_0 Loss: 0.0441 | 0.0467
Epoch 200/300, seasonal_0 Loss: 0.0428 | 0.0454
Epoch 201/300, seasonal_0 Loss: 0.0412 | 0.0470
Epoch 202/300, seasonal_0 Loss: 0.0404 | 0.0454
Epoch 203/300, seasonal_0 Loss: 0.0399 | 0.0464
Epoch 204/300, seasonal_0 Loss: 0.0394 | 0.0471
Epoch 205/300, seasonal_0 Loss: 0.0394 | 0.0472
Epoch 206/300, seasonal_0 Loss: 0.0393 | 0.0475
Epoch 207/300, seasonal_0 Loss: 0.0391 | 0.0485
Epoch 208/300, seasonal_0 Loss: 0.0390 | 0.0491
Epoch 209/300, seasonal_0 Loss: 0.0388 | 0.0495
Epoch 210/300, seasonal_0 Loss: 0.0387 | 0.0515
Epoch 211/300, seasonal_0 Loss: 0.0384 | 0.0535
Epoch 212/300, seasonal_0 Loss: 0.0375 | 0.0570
Epoch 213/300, seasonal_0 Loss: 0.0412 | 0.0645
Epoch 214/300, seasonal_0 Loss: 0.0445 | 0.0530
Epoch 215/300, seasonal_0 Loss: 0.0330 | 0.0560
Epoch 216/300, seasonal_0 Loss: 0.0309 | 0.0496
Epoch 217/300, seasonal_0 Loss: 0.0285 | 0.0473
Epoch 218/300, seasonal_0 Loss: 0.0270 | 0.0465
Epoch 219/300, seasonal_0 Loss: 0.0263 | 0.0465
Epoch 220/300, seasonal_0 Loss: 0.0259 | 0.0469
Epoch 221/300, seasonal_0 Loss: 0.0256 | 0.0467
Epoch 222/300, seasonal_0 Loss: 0.0255 | 0.0473
Epoch 223/300, seasonal_0 Loss: 0.0253 | 0.0475
Epoch 224/300, seasonal_0 Loss: 0.0251 | 0.0484
Epoch 225/300, seasonal_0 Loss: 0.0250 | 0.0495
Epoch 226/300, seasonal_0 Loss: 0.0248 | 0.0507
Epoch 227/300, seasonal_0 Loss: 0.0247 | 0.0527
Epoch 228/300, seasonal_0 Loss: 0.0246 | 0.0546
Epoch 229/300, seasonal_0 Loss: 0.0246 | 0.0570
Epoch 230/300, seasonal_0 Loss: 0.0246 | 0.0599
Epoch 231/300, seasonal_0 Loss: 0.0246 | 0.0623
Epoch 232/300, seasonal_0 Loss: 0.0247 | 0.0641
Epoch 233/300, seasonal_0 Loss: 0.0247 | 0.0642
Epoch 234/300, seasonal_0 Loss: 0.0248 | 0.0620
Epoch 235/300, seasonal_0 Loss: 0.0248 | 0.0577
Epoch 236/300, seasonal_0 Loss: 0.0247 | 0.0537
Epoch 237/300, seasonal_0 Loss: 0.0245 | 0.0501
Epoch 238/300, seasonal_0 Loss: 0.0243 | 0.0485
Epoch 239/300, seasonal_0 Loss: 0.0241 | 0.0478
Epoch 240/300, seasonal_0 Loss: 0.0241 | 0.0481
Epoch 241/300, seasonal_0 Loss: 0.0245 | 0.0498
Epoch 242/300, seasonal_0 Loss: 0.0251 | 0.0516
Epoch 243/300, seasonal_0 Loss: 0.0257 | 0.0526
Epoch 244/300, seasonal_0 Loss: 0.0264 | 0.0546
Epoch 245/300, seasonal_0 Loss: 0.0268 | 0.0535
Epoch 246/300, seasonal_0 Loss: 0.0275 | 0.0524
Epoch 247/300, seasonal_0 Loss: 0.0271 | 0.0562
Epoch 248/300, seasonal_0 Loss: 0.0283 | 0.0526
Epoch 249/300, seasonal_0 Loss: 0.0275 | 0.0551
Epoch 250/300, seasonal_0 Loss: 0.0298 | 0.0449
Epoch 251/300, seasonal_0 Loss: 0.0270 | 0.0484
Epoch 252/300, seasonal_0 Loss: 0.0260 | 0.0538
Epoch 253/300, seasonal_0 Loss: 0.0261 | 0.0499
Epoch 254/300, seasonal_0 Loss: 0.0256 | 0.0580
Epoch 255/300, seasonal_0 Loss: 0.0252 | 0.0574
Epoch 256/300, seasonal_0 Loss: 0.0249 | 0.0543
Epoch 257/300, seasonal_0 Loss: 0.0244 | 0.0521
Epoch 258/300, seasonal_0 Loss: 0.0238 | 0.0503
Epoch 259/300, seasonal_0 Loss: 0.0238 | 0.0526
Epoch 260/300, seasonal_0 Loss: 0.0243 | 0.0524
Epoch 261/300, seasonal_0 Loss: 0.0236 | 0.0537
Epoch 262/300, seasonal_0 Loss: 0.0249 | 0.0521
Epoch 263/300, seasonal_0 Loss: 0.0235 | 0.0530
Epoch 264/300, seasonal_0 Loss: 0.0231 | 0.0513
Epoch 265/300, seasonal_0 Loss: 0.0228 | 0.0526
Epoch 266/300, seasonal_0 Loss: 0.0222 | 0.0511
Epoch 267/300, seasonal_0 Loss: 0.0220 | 0.0518
Epoch 268/300, seasonal_0 Loss: 0.0217 | 0.0499
Epoch 269/300, seasonal_0 Loss: 0.0217 | 0.0497
Epoch 270/300, seasonal_0 Loss: 0.0218 | 0.0481
Epoch 271/300, seasonal_0 Loss: 0.0217 | 0.0514
Epoch 272/300, seasonal_0 Loss: 0.0218 | 0.0515
Epoch 273/300, seasonal_0 Loss: 0.0216 | 0.0550
Epoch 274/300, seasonal_0 Loss: 0.0220 | 0.0553
Epoch 275/300, seasonal_0 Loss: 0.0222 | 0.0579
Epoch 276/300, seasonal_0 Loss: 0.0223 | 0.0639
Epoch 277/300, seasonal_0 Loss: 0.0225 | 0.0611
Epoch 278/300, seasonal_0 Loss: 0.0223 | 0.0633
Epoch 279/300, seasonal_0 Loss: 0.0225 | 0.0580
Epoch 280/300, seasonal_0 Loss: 0.0223 | 0.0558
Epoch 281/300, seasonal_0 Loss: 0.0216 | 0.0537
Epoch 282/300, seasonal_0 Loss: 0.0212 | 0.0517
Epoch 283/300, seasonal_0 Loss: 0.0209 | 0.0508
Epoch 284/300, seasonal_0 Loss: 0.0212 | 0.0487
Epoch 285/300, seasonal_0 Loss: 0.0214 | 0.0531
Epoch 286/300, seasonal_0 Loss: 0.0230 | 0.0476
Epoch 287/300, seasonal_0 Loss: 0.0219 | 0.0512
Epoch 288/300, seasonal_0 Loss: 0.0227 | 0.0479
Epoch 289/300, seasonal_0 Loss: 0.0215 | 0.0528
Epoch 290/300, seasonal_0 Loss: 0.0218 | 0.0468
Epoch 291/300, seasonal_0 Loss: 0.0211 | 0.0511
Epoch 292/300, seasonal_0 Loss: 0.0211 | 0.0476
Epoch 293/300, seasonal_0 Loss: 0.0210 | 0.0521
Epoch 294/300, seasonal_0 Loss: 0.0209 | 0.0526
Epoch 295/300, seasonal_0 Loss: 0.0212 | 0.0553
Epoch 296/300, seasonal_0 Loss: 0.0209 | 0.0602
Epoch 297/300, seasonal_0 Loss: 0.0212 | 0.0586
Epoch 298/300, seasonal_0 Loss: 0.0209 | 0.0638
Epoch 299/300, seasonal_0 Loss: 0.0210 | 0.0592
Epoch 300/300, seasonal_0 Loss: 0.0213 | 0.0619
Training seasonal_1 component with params: {'observation_period_num': 29, 'train_rates': 0.9123757823494736, 'learning_rate': 0.00020205995157404424, 'batch_size': 158, 'step_size': 11, 'gamma': 0.7753561186314534}
Epoch 1/300, seasonal_1 Loss: 0.5760 | 0.4369
Epoch 2/300, seasonal_1 Loss: 0.3304 | 0.4918
Epoch 3/300, seasonal_1 Loss: 0.2288 | 0.3815
Epoch 4/300, seasonal_1 Loss: 0.1843 | 0.2628
Epoch 5/300, seasonal_1 Loss: 0.1692 | 0.2312
Epoch 6/300, seasonal_1 Loss: 0.1599 | 0.2336
Epoch 7/300, seasonal_1 Loss: 0.1545 | 0.1853
Epoch 8/300, seasonal_1 Loss: 0.1394 | 0.1588
Epoch 9/300, seasonal_1 Loss: 0.1332 | 0.1353
Epoch 10/300, seasonal_1 Loss: 0.1375 | 0.1266
Epoch 11/300, seasonal_1 Loss: 0.1326 | 0.1185
Epoch 12/300, seasonal_1 Loss: 0.1256 | 0.1329
Epoch 13/300, seasonal_1 Loss: 0.1372 | 0.2489
Epoch 14/300, seasonal_1 Loss: 0.1358 | 0.2440
Epoch 15/300, seasonal_1 Loss: 0.1281 | 0.1085
Epoch 16/300, seasonal_1 Loss: 0.1322 | 0.1059
Epoch 17/300, seasonal_1 Loss: 0.1291 | 0.1134
Epoch 18/300, seasonal_1 Loss: 0.1192 | 0.1090
Epoch 19/300, seasonal_1 Loss: 0.1094 | 0.1046
Epoch 20/300, seasonal_1 Loss: 0.1109 | 0.0925
Epoch 21/300, seasonal_1 Loss: 0.1053 | 0.0883
Epoch 22/300, seasonal_1 Loss: 0.1048 | 0.0865
Epoch 23/300, seasonal_1 Loss: 0.1019 | 0.0861
Epoch 24/300, seasonal_1 Loss: 0.1036 | 0.0896
Epoch 25/300, seasonal_1 Loss: 0.0998 | 0.0804
Epoch 26/300, seasonal_1 Loss: 0.0998 | 0.0788
Epoch 27/300, seasonal_1 Loss: 0.0983 | 0.0781
Epoch 28/300, seasonal_1 Loss: 0.0987 | 0.0829
Epoch 29/300, seasonal_1 Loss: 0.0964 | 0.0763
Epoch 30/300, seasonal_1 Loss: 0.0959 | 0.0744
Epoch 31/300, seasonal_1 Loss: 0.0951 | 0.0735
Epoch 32/300, seasonal_1 Loss: 0.0945 | 0.0761
Epoch 33/300, seasonal_1 Loss: 0.0932 | 0.0713
Epoch 34/300, seasonal_1 Loss: 0.0927 | 0.0705
Epoch 35/300, seasonal_1 Loss: 0.0921 | 0.0709
Epoch 36/300, seasonal_1 Loss: 0.0914 | 0.0697
Epoch 37/300, seasonal_1 Loss: 0.0906 | 0.0682
Epoch 38/300, seasonal_1 Loss: 0.0904 | 0.0677
Epoch 39/300, seasonal_1 Loss: 0.0898 | 0.0674
Epoch 40/300, seasonal_1 Loss: 0.0893 | 0.0669
Epoch 41/300, seasonal_1 Loss: 0.0888 | 0.0661
Epoch 42/300, seasonal_1 Loss: 0.0885 | 0.0658
Epoch 43/300, seasonal_1 Loss: 0.0881 | 0.0653
Epoch 44/300, seasonal_1 Loss: 0.0877 | 0.0648
Epoch 45/300, seasonal_1 Loss: 0.0874 | 0.0645
Epoch 46/300, seasonal_1 Loss: 0.0871 | 0.0642
Epoch 47/300, seasonal_1 Loss: 0.0868 | 0.0638
Epoch 48/300, seasonal_1 Loss: 0.0866 | 0.0635
Epoch 49/300, seasonal_1 Loss: 0.0863 | 0.0632
Epoch 50/300, seasonal_1 Loss: 0.0861 | 0.0629
Epoch 51/300, seasonal_1 Loss: 0.0858 | 0.0626
Epoch 52/300, seasonal_1 Loss: 0.0856 | 0.0624
Epoch 53/300, seasonal_1 Loss: 0.0854 | 0.0622
Epoch 54/300, seasonal_1 Loss: 0.0852 | 0.0619
Epoch 55/300, seasonal_1 Loss: 0.0850 | 0.0617
Epoch 56/300, seasonal_1 Loss: 0.0848 | 0.0615
Epoch 57/300, seasonal_1 Loss: 0.0847 | 0.0614
Epoch 58/300, seasonal_1 Loss: 0.0846 | 0.0612
Epoch 59/300, seasonal_1 Loss: 0.0844 | 0.0610
Epoch 60/300, seasonal_1 Loss: 0.0843 | 0.0608
Epoch 61/300, seasonal_1 Loss: 0.0841 | 0.0607
Epoch 62/300, seasonal_1 Loss: 0.0840 | 0.0605
Epoch 63/300, seasonal_1 Loss: 0.0839 | 0.0604
Epoch 64/300, seasonal_1 Loss: 0.0838 | 0.0603
Epoch 65/300, seasonal_1 Loss: 0.0838 | 0.0602
Epoch 66/300, seasonal_1 Loss: 0.0838 | 0.0600
Epoch 67/300, seasonal_1 Loss: 0.0838 | 0.0602
Epoch 68/300, seasonal_1 Loss: 0.0839 | 0.0602
Epoch 69/300, seasonal_1 Loss: 0.0842 | 0.0606
Epoch 70/300, seasonal_1 Loss: 0.0843 | 0.0602
Epoch 71/300, seasonal_1 Loss: 0.0849 | 0.0621
Epoch 72/300, seasonal_1 Loss: 0.0851 | 0.0606
Epoch 73/300, seasonal_1 Loss: 0.0850 | 0.0655
Epoch 74/300, seasonal_1 Loss: 0.0840 | 0.0615
Epoch 75/300, seasonal_1 Loss: 0.0837 | 0.0618
Epoch 76/300, seasonal_1 Loss: 0.0832 | 0.0602
Epoch 77/300, seasonal_1 Loss: 0.0830 | 0.0599
Epoch 78/300, seasonal_1 Loss: 0.0827 | 0.0595
Epoch 79/300, seasonal_1 Loss: 0.0826 | 0.0591
Epoch 80/300, seasonal_1 Loss: 0.0826 | 0.0590
Epoch 81/300, seasonal_1 Loss: 0.0825 | 0.0589
Epoch 82/300, seasonal_1 Loss: 0.0825 | 0.0589
Epoch 83/300, seasonal_1 Loss: 0.0824 | 0.0588
Epoch 84/300, seasonal_1 Loss: 0.0824 | 0.0588
Epoch 85/300, seasonal_1 Loss: 0.0823 | 0.0587
Epoch 86/300, seasonal_1 Loss: 0.0823 | 0.0587
Epoch 87/300, seasonal_1 Loss: 0.0823 | 0.0586
Epoch 88/300, seasonal_1 Loss: 0.0822 | 0.0586
Epoch 89/300, seasonal_1 Loss: 0.0822 | 0.0586
Epoch 90/300, seasonal_1 Loss: 0.0821 | 0.0585
Epoch 91/300, seasonal_1 Loss: 0.0821 | 0.0585
Epoch 92/300, seasonal_1 Loss: 0.0821 | 0.0585
Epoch 93/300, seasonal_1 Loss: 0.0821 | 0.0584
Epoch 94/300, seasonal_1 Loss: 0.0820 | 0.0584
Epoch 95/300, seasonal_1 Loss: 0.0820 | 0.0584
Epoch 96/300, seasonal_1 Loss: 0.0820 | 0.0583
Epoch 97/300, seasonal_1 Loss: 0.0820 | 0.0583
Epoch 98/300, seasonal_1 Loss: 0.0819 | 0.0583
Epoch 99/300, seasonal_1 Loss: 0.0819 | 0.0583
Epoch 100/300, seasonal_1 Loss: 0.0819 | 0.0582
Epoch 101/300, seasonal_1 Loss: 0.0819 | 0.0582
Epoch 102/300, seasonal_1 Loss: 0.0818 | 0.0582
Epoch 103/300, seasonal_1 Loss: 0.0818 | 0.0582
Epoch 104/300, seasonal_1 Loss: 0.0818 | 0.0581
Epoch 105/300, seasonal_1 Loss: 0.0818 | 0.0581
Epoch 106/300, seasonal_1 Loss: 0.0818 | 0.0581
Epoch 107/300, seasonal_1 Loss: 0.0818 | 0.0581
Epoch 108/300, seasonal_1 Loss: 0.0817 | 0.0581
Epoch 109/300, seasonal_1 Loss: 0.0817 | 0.0581
Epoch 110/300, seasonal_1 Loss: 0.0817 | 0.0580
Epoch 111/300, seasonal_1 Loss: 0.0817 | 0.0580
Epoch 112/300, seasonal_1 Loss: 0.0817 | 0.0580
Epoch 113/300, seasonal_1 Loss: 0.0817 | 0.0580
Epoch 114/300, seasonal_1 Loss: 0.0817 | 0.0580
Epoch 115/300, seasonal_1 Loss: 0.0817 | 0.0580
Epoch 116/300, seasonal_1 Loss: 0.0816 | 0.0580
Epoch 117/300, seasonal_1 Loss: 0.0816 | 0.0580
Epoch 118/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 119/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 120/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 121/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 122/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 123/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 124/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 125/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 126/300, seasonal_1 Loss: 0.0816 | 0.0579
Epoch 127/300, seasonal_1 Loss: 0.0815 | 0.0579
Epoch 128/300, seasonal_1 Loss: 0.0815 | 0.0579
Epoch 129/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 130/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 131/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 132/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 133/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 134/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 135/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 136/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 137/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 138/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 139/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 140/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 141/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 142/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 143/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 144/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 145/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 146/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 147/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 148/300, seasonal_1 Loss: 0.0815 | 0.0578
Epoch 149/300, seasonal_1 Loss: 0.0814 | 0.0578
Epoch 150/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 151/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 152/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 153/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 154/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 155/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 156/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 157/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 158/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 159/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 160/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 161/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 162/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 163/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 164/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 165/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 166/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 167/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 168/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 169/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 170/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 171/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 172/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 173/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 174/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 175/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 176/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 177/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 178/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 179/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 180/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 181/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 182/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 183/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 184/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 185/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 186/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 187/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 188/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 189/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 190/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 191/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 192/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 193/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 194/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 195/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 196/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 197/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 198/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 199/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 200/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 201/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 202/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 203/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 204/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 205/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 206/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 207/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 208/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 209/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 210/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 211/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 212/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 213/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 214/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 215/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 216/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 217/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 218/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 219/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 220/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 221/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 222/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 223/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 224/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 225/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 226/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 227/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 228/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 229/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 230/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 231/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 232/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 233/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 234/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 235/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 236/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 237/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 238/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 239/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 240/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 241/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 242/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 243/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 244/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 245/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 246/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 247/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 248/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 249/300, seasonal_1 Loss: 0.0814 | 0.0577
Epoch 250/300, seasonal_1 Loss: 0.0814 | 0.0577
Early stopping for seasonal_1
Training seasonal_2 component with params: {'observation_period_num': 5, 'train_rates': 0.874533665912694, 'learning_rate': 5.192011337583312e-05, 'batch_size': 20, 'step_size': 14, 'gamma': 0.9002943954255419}
Epoch 1/300, seasonal_2 Loss: 0.2238 | 0.1257
Epoch 2/300, seasonal_2 Loss: 0.1334 | 0.0948
Epoch 3/300, seasonal_2 Loss: 0.1197 | 0.0795
Epoch 4/300, seasonal_2 Loss: 0.1118 | 0.0742
Epoch 5/300, seasonal_2 Loss: 0.1071 | 0.0720
Epoch 6/300, seasonal_2 Loss: 0.1039 | 0.0694
Epoch 7/300, seasonal_2 Loss: 0.1013 | 0.0664
Epoch 8/300, seasonal_2 Loss: 0.0989 | 0.0632
Epoch 9/300, seasonal_2 Loss: 0.0969 | 0.0608
Epoch 10/300, seasonal_2 Loss: 0.0948 | 0.0587
Epoch 11/300, seasonal_2 Loss: 0.0927 | 0.0566
Epoch 12/300, seasonal_2 Loss: 0.0907 | 0.0546
Epoch 13/300, seasonal_2 Loss: 0.0888 | 0.0527
Epoch 14/300, seasonal_2 Loss: 0.0872 | 0.0510
Epoch 15/300, seasonal_2 Loss: 0.0855 | 0.0497
Epoch 16/300, seasonal_2 Loss: 0.0843 | 0.0485
Epoch 17/300, seasonal_2 Loss: 0.0833 | 0.0474
Epoch 18/300, seasonal_2 Loss: 0.0824 | 0.0466
Epoch 19/300, seasonal_2 Loss: 0.0816 | 0.0458
Epoch 20/300, seasonal_2 Loss: 0.0808 | 0.0452
Epoch 21/300, seasonal_2 Loss: 0.0801 | 0.0445
Epoch 22/300, seasonal_2 Loss: 0.0793 | 0.0439
Epoch 23/300, seasonal_2 Loss: 0.0788 | 0.0433
Epoch 24/300, seasonal_2 Loss: 0.0782 | 0.0427
Epoch 25/300, seasonal_2 Loss: 0.0777 | 0.0422
Epoch 26/300, seasonal_2 Loss: 0.0771 | 0.0416
Epoch 27/300, seasonal_2 Loss: 0.0766 | 0.0411
Epoch 28/300, seasonal_2 Loss: 0.0761 | 0.0406
Epoch 29/300, seasonal_2 Loss: 0.0756 | 0.0403
Epoch 30/300, seasonal_2 Loss: 0.0751 | 0.0400
Epoch 31/300, seasonal_2 Loss: 0.0747 | 0.0397
Epoch 32/300, seasonal_2 Loss: 0.0743 | 0.0394
Epoch 33/300, seasonal_2 Loss: 0.0739 | 0.0391
Epoch 34/300, seasonal_2 Loss: 0.0736 | 0.0389
Epoch 35/300, seasonal_2 Loss: 0.0732 | 0.0387
Epoch 36/300, seasonal_2 Loss: 0.0730 | 0.0389
Epoch 37/300, seasonal_2 Loss: 0.0725 | 0.0388
Epoch 38/300, seasonal_2 Loss: 0.0722 | 0.0385
Epoch 39/300, seasonal_2 Loss: 0.0718 | 0.0383
Epoch 40/300, seasonal_2 Loss: 0.0714 | 0.0381
Epoch 41/300, seasonal_2 Loss: 0.0711 | 0.0379
Epoch 42/300, seasonal_2 Loss: 0.0707 | 0.0378
Epoch 43/300, seasonal_2 Loss: 0.0703 | 0.0375
Epoch 44/300, seasonal_2 Loss: 0.0700 | 0.0374
Epoch 45/300, seasonal_2 Loss: 0.0697 | 0.0373
Epoch 46/300, seasonal_2 Loss: 0.0694 | 0.0372
Epoch 47/300, seasonal_2 Loss: 0.0692 | 0.0370
Epoch 48/300, seasonal_2 Loss: 0.0689 | 0.0369
Epoch 49/300, seasonal_2 Loss: 0.0686 | 0.0366
Epoch 50/300, seasonal_2 Loss: 0.0683 | 0.0366
Epoch 51/300, seasonal_2 Loss: 0.0680 | 0.0363
Epoch 52/300, seasonal_2 Loss: 0.0677 | 0.0360
Epoch 53/300, seasonal_2 Loss: 0.0675 | 0.0357
Epoch 54/300, seasonal_2 Loss: 0.0673 | 0.0354
Epoch 55/300, seasonal_2 Loss: 0.0670 | 0.0351
Epoch 56/300, seasonal_2 Loss: 0.0668 | 0.0349
Epoch 57/300, seasonal_2 Loss: 0.0665 | 0.0348
Epoch 58/300, seasonal_2 Loss: 0.0663 | 0.0346
Epoch 59/300, seasonal_2 Loss: 0.0661 | 0.0344
Epoch 60/300, seasonal_2 Loss: 0.0659 | 0.0342
Epoch 61/300, seasonal_2 Loss: 0.0657 | 0.0340
Epoch 62/300, seasonal_2 Loss: 0.0655 | 0.0338
Epoch 63/300, seasonal_2 Loss: 0.0654 | 0.0336
Epoch 64/300, seasonal_2 Loss: 0.0652 | 0.0336
Epoch 65/300, seasonal_2 Loss: 0.0651 | 0.0334
Epoch 66/300, seasonal_2 Loss: 0.0649 | 0.0333
Epoch 67/300, seasonal_2 Loss: 0.0647 | 0.0331
Epoch 68/300, seasonal_2 Loss: 0.0646 | 0.0329
Epoch 69/300, seasonal_2 Loss: 0.0644 | 0.0328
Epoch 70/300, seasonal_2 Loss: 0.0643 | 0.0326
Epoch 71/300, seasonal_2 Loss: 0.0641 | 0.0326
Epoch 72/300, seasonal_2 Loss: 0.0640 | 0.0324
Epoch 73/300, seasonal_2 Loss: 0.0638 | 0.0322
Epoch 74/300, seasonal_2 Loss: 0.0637 | 0.0321
Epoch 75/300, seasonal_2 Loss: 0.0635 | 0.0319
Epoch 76/300, seasonal_2 Loss: 0.0634 | 0.0318
Epoch 77/300, seasonal_2 Loss: 0.0633 | 0.0317
Epoch 78/300, seasonal_2 Loss: 0.0631 | 0.0317
Epoch 79/300, seasonal_2 Loss: 0.0630 | 0.0315
Epoch 80/300, seasonal_2 Loss: 0.0629 | 0.0314
Epoch 81/300, seasonal_2 Loss: 0.0628 | 0.0313
Epoch 82/300, seasonal_2 Loss: 0.0626 | 0.0312
Epoch 83/300, seasonal_2 Loss: 0.0625 | 0.0311
Epoch 84/300, seasonal_2 Loss: 0.0624 | 0.0310
Epoch 85/300, seasonal_2 Loss: 0.0623 | 0.0311
Epoch 86/300, seasonal_2 Loss: 0.0622 | 0.0310
Epoch 87/300, seasonal_2 Loss: 0.0621 | 0.0309
Epoch 88/300, seasonal_2 Loss: 0.0620 | 0.0308
Epoch 89/300, seasonal_2 Loss: 0.0619 | 0.0307
Epoch 90/300, seasonal_2 Loss: 0.0618 | 0.0307
Epoch 91/300, seasonal_2 Loss: 0.0617 | 0.0306
Epoch 92/300, seasonal_2 Loss: 0.0615 | 0.0307
Epoch 93/300, seasonal_2 Loss: 0.0615 | 0.0306
Epoch 94/300, seasonal_2 Loss: 0.0614 | 0.0306
Epoch 95/300, seasonal_2 Loss: 0.0613 | 0.0305
Epoch 96/300, seasonal_2 Loss: 0.0612 | 0.0305
Epoch 97/300, seasonal_2 Loss: 0.0611 | 0.0304
Epoch 98/300, seasonal_2 Loss: 0.0611 | 0.0304
Epoch 99/300, seasonal_2 Loss: 0.0609 | 0.0304
Epoch 100/300, seasonal_2 Loss: 0.0609 | 0.0304
Epoch 101/300, seasonal_2 Loss: 0.0608 | 0.0304
Epoch 102/300, seasonal_2 Loss: 0.0607 | 0.0303
Epoch 103/300, seasonal_2 Loss: 0.0607 | 0.0303
Epoch 104/300, seasonal_2 Loss: 0.0606 | 0.0303
Epoch 105/300, seasonal_2 Loss: 0.0605 | 0.0303
Epoch 106/300, seasonal_2 Loss: 0.0604 | 0.0302
Epoch 107/300, seasonal_2 Loss: 0.0604 | 0.0302
Epoch 108/300, seasonal_2 Loss: 0.0603 | 0.0302
Epoch 109/300, seasonal_2 Loss: 0.0602 | 0.0302
Epoch 110/300, seasonal_2 Loss: 0.0602 | 0.0302
Epoch 111/300, seasonal_2 Loss: 0.0601 | 0.0302
Epoch 112/300, seasonal_2 Loss: 0.0601 | 0.0302
Epoch 113/300, seasonal_2 Loss: 0.0600 | 0.0302
Epoch 114/300, seasonal_2 Loss: 0.0599 | 0.0302
Epoch 115/300, seasonal_2 Loss: 0.0599 | 0.0302
Epoch 116/300, seasonal_2 Loss: 0.0598 | 0.0302
Epoch 117/300, seasonal_2 Loss: 0.0598 | 0.0303
Epoch 118/300, seasonal_2 Loss: 0.0597 | 0.0303
Epoch 119/300, seasonal_2 Loss: 0.0597 | 0.0303
Epoch 120/300, seasonal_2 Loss: 0.0596 | 0.0303
Epoch 121/300, seasonal_2 Loss: 0.0596 | 0.0304
Epoch 122/300, seasonal_2 Loss: 0.0595 | 0.0304
Epoch 123/300, seasonal_2 Loss: 0.0595 | 0.0304
Epoch 124/300, seasonal_2 Loss: 0.0594 | 0.0305
Epoch 125/300, seasonal_2 Loss: 0.0594 | 0.0305
Epoch 126/300, seasonal_2 Loss: 0.0593 | 0.0305
Epoch 127/300, seasonal_2 Loss: 0.0593 | 0.0303
Epoch 128/300, seasonal_2 Loss: 0.0592 | 0.0304
Epoch 129/300, seasonal_2 Loss: 0.0592 | 0.0304
Epoch 130/300, seasonal_2 Loss: 0.0591 | 0.0304
Epoch 131/300, seasonal_2 Loss: 0.0591 | 0.0304
Epoch 132/300, seasonal_2 Loss: 0.0591 | 0.0304
Epoch 133/300, seasonal_2 Loss: 0.0590 | 0.0304
Epoch 134/300, seasonal_2 Loss: 0.0590 | 0.0303
Epoch 135/300, seasonal_2 Loss: 0.0589 | 0.0303
Epoch 136/300, seasonal_2 Loss: 0.0589 | 0.0304
Epoch 137/300, seasonal_2 Loss: 0.0589 | 0.0304
Epoch 138/300, seasonal_2 Loss: 0.0588 | 0.0304
Epoch 139/300, seasonal_2 Loss: 0.0588 | 0.0304
Epoch 140/300, seasonal_2 Loss: 0.0587 | 0.0304
Epoch 141/300, seasonal_2 Loss: 0.0587 | 0.0302
Epoch 142/300, seasonal_2 Loss: 0.0586 | 0.0302
Epoch 143/300, seasonal_2 Loss: 0.0586 | 0.0302
Epoch 144/300, seasonal_2 Loss: 0.0586 | 0.0302
Epoch 145/300, seasonal_2 Loss: 0.0585 | 0.0303
Epoch 146/300, seasonal_2 Loss: 0.0585 | 0.0303
Epoch 147/300, seasonal_2 Loss: 0.0585 | 0.0303
Epoch 148/300, seasonal_2 Loss: 0.0584 | 0.0301
Epoch 149/300, seasonal_2 Loss: 0.0584 | 0.0301
Epoch 150/300, seasonal_2 Loss: 0.0583 | 0.0301
Epoch 151/300, seasonal_2 Loss: 0.0583 | 0.0301
Epoch 152/300, seasonal_2 Loss: 0.0582 | 0.0301
Epoch 153/300, seasonal_2 Loss: 0.0582 | 0.0301
Epoch 154/300, seasonal_2 Loss: 0.0582 | 0.0301
Epoch 155/300, seasonal_2 Loss: 0.0581 | 0.0300
Epoch 156/300, seasonal_2 Loss: 0.0581 | 0.0300
Epoch 157/300, seasonal_2 Loss: 0.0581 | 0.0300
Epoch 158/300, seasonal_2 Loss: 0.0580 | 0.0300
Epoch 159/300, seasonal_2 Loss: 0.0580 | 0.0300
Epoch 160/300, seasonal_2 Loss: 0.0580 | 0.0301
Epoch 161/300, seasonal_2 Loss: 0.0579 | 0.0301
Epoch 162/300, seasonal_2 Loss: 0.0579 | 0.0299
Epoch 163/300, seasonal_2 Loss: 0.0578 | 0.0299
Epoch 164/300, seasonal_2 Loss: 0.0578 | 0.0299
Epoch 165/300, seasonal_2 Loss: 0.0578 | 0.0300
Epoch 166/300, seasonal_2 Loss: 0.0577 | 0.0300
Epoch 167/300, seasonal_2 Loss: 0.0577 | 0.0300
Epoch 168/300, seasonal_2 Loss: 0.0577 | 0.0300
Epoch 169/300, seasonal_2 Loss: 0.0577 | 0.0298
Epoch 170/300, seasonal_2 Loss: 0.0576 | 0.0298
Epoch 171/300, seasonal_2 Loss: 0.0576 | 0.0298
Epoch 172/300, seasonal_2 Loss: 0.0576 | 0.0299
Epoch 173/300, seasonal_2 Loss: 0.0575 | 0.0299
Epoch 174/300, seasonal_2 Loss: 0.0575 | 0.0299
Epoch 175/300, seasonal_2 Loss: 0.0575 | 0.0299
Epoch 176/300, seasonal_2 Loss: 0.0575 | 0.0297
Epoch 177/300, seasonal_2 Loss: 0.0574 | 0.0297
Epoch 178/300, seasonal_2 Loss: 0.0574 | 0.0297
Epoch 179/300, seasonal_2 Loss: 0.0574 | 0.0297
Epoch 180/300, seasonal_2 Loss: 0.0574 | 0.0297
Epoch 181/300, seasonal_2 Loss: 0.0574 | 0.0297
Epoch 182/300, seasonal_2 Loss: 0.0573 | 0.0296
Epoch 183/300, seasonal_2 Loss: 0.0573 | 0.0294
Epoch 184/300, seasonal_2 Loss: 0.0573 | 0.0294
Epoch 185/300, seasonal_2 Loss: 0.0573 | 0.0294
Epoch 186/300, seasonal_2 Loss: 0.0573 | 0.0294
Epoch 187/300, seasonal_2 Loss: 0.0572 | 0.0294
Epoch 188/300, seasonal_2 Loss: 0.0572 | 0.0293
Epoch 189/300, seasonal_2 Loss: 0.0572 | 0.0293
Epoch 190/300, seasonal_2 Loss: 0.0572 | 0.0291
Epoch 191/300, seasonal_2 Loss: 0.0572 | 0.0291
Epoch 192/300, seasonal_2 Loss: 0.0571 | 0.0291
Epoch 193/300, seasonal_2 Loss: 0.0571 | 0.0290
Epoch 194/300, seasonal_2 Loss: 0.0571 | 0.0290
Epoch 195/300, seasonal_2 Loss: 0.0571 | 0.0290
Epoch 196/300, seasonal_2 Loss: 0.0571 | 0.0290
Epoch 197/300, seasonal_2 Loss: 0.0571 | 0.0288
Epoch 198/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 199/300, seasonal_2 Loss: 0.0570 | 0.0288
Epoch 200/300, seasonal_2 Loss: 0.0570 | 0.0287
Epoch 201/300, seasonal_2 Loss: 0.0570 | 0.0287
Epoch 202/300, seasonal_2 Loss: 0.0570 | 0.0287
Epoch 203/300, seasonal_2 Loss: 0.0570 | 0.0287
Epoch 204/300, seasonal_2 Loss: 0.0569 | 0.0286
Epoch 205/300, seasonal_2 Loss: 0.0569 | 0.0285
Epoch 206/300, seasonal_2 Loss: 0.0569 | 0.0285
Epoch 207/300, seasonal_2 Loss: 0.0569 | 0.0285
Epoch 208/300, seasonal_2 Loss: 0.0569 | 0.0285
Epoch 209/300, seasonal_2 Loss: 0.0569 | 0.0285
Epoch 210/300, seasonal_2 Loss: 0.0568 | 0.0285
Epoch 211/300, seasonal_2 Loss: 0.0568 | 0.0284
Epoch 212/300, seasonal_2 Loss: 0.0568 | 0.0284
Epoch 213/300, seasonal_2 Loss: 0.0568 | 0.0284
Epoch 214/300, seasonal_2 Loss: 0.0568 | 0.0283
Epoch 215/300, seasonal_2 Loss: 0.0568 | 0.0283
Epoch 216/300, seasonal_2 Loss: 0.0568 | 0.0283
Epoch 217/300, seasonal_2 Loss: 0.0567 | 0.0283
Epoch 218/300, seasonal_2 Loss: 0.0567 | 0.0283
Epoch 219/300, seasonal_2 Loss: 0.0567 | 0.0283
Epoch 220/300, seasonal_2 Loss: 0.0567 | 0.0282
Epoch 221/300, seasonal_2 Loss: 0.0567 | 0.0282
Epoch 222/300, seasonal_2 Loss: 0.0567 | 0.0282
Epoch 223/300, seasonal_2 Loss: 0.0567 | 0.0282
Epoch 224/300, seasonal_2 Loss: 0.0567 | 0.0282
Epoch 225/300, seasonal_2 Loss: 0.0566 | 0.0282
Epoch 226/300, seasonal_2 Loss: 0.0566 | 0.0282
Epoch 227/300, seasonal_2 Loss: 0.0566 | 0.0282
Epoch 228/300, seasonal_2 Loss: 0.0566 | 0.0282
Epoch 229/300, seasonal_2 Loss: 0.0566 | 0.0282
Epoch 230/300, seasonal_2 Loss: 0.0566 | 0.0281
Epoch 231/300, seasonal_2 Loss: 0.0566 | 0.0281
Epoch 232/300, seasonal_2 Loss: 0.0565 | 0.0282
Epoch 233/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 234/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 235/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 236/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 237/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 238/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 239/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 240/300, seasonal_2 Loss: 0.0565 | 0.0281
Epoch 241/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 242/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 243/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 244/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 245/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 246/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 247/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 248/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 249/300, seasonal_2 Loss: 0.0564 | 0.0281
Epoch 250/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 251/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 252/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 253/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 254/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 255/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 256/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 257/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 258/300, seasonal_2 Loss: 0.0563 | 0.0281
Epoch 259/300, seasonal_2 Loss: 0.0563 | 0.0280
Epoch 260/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 261/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 262/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 263/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 264/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 265/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 266/300, seasonal_2 Loss: 0.0562 | 0.0280
Epoch 267/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 268/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 269/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 270/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 271/300, seasonal_2 Loss: 0.0562 | 0.0281
Epoch 272/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 273/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 274/300, seasonal_2 Loss: 0.0561 | 0.0281
Epoch 275/300, seasonal_2 Loss: 0.0561 | 0.0281
Epoch 276/300, seasonal_2 Loss: 0.0561 | 0.0281
Epoch 277/300, seasonal_2 Loss: 0.0561 | 0.0281
Epoch 278/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 279/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 280/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 281/300, seasonal_2 Loss: 0.0561 | 0.0281
Epoch 282/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 283/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 284/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 285/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 286/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 287/300, seasonal_2 Loss: 0.0561 | 0.0280
Epoch 288/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 289/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 290/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 291/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 292/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 293/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 294/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 295/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 296/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 297/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 298/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 299/300, seasonal_2 Loss: 0.0560 | 0.0280
Epoch 300/300, seasonal_2 Loss: 0.0560 | 0.0280
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.9612813971997037, 'learning_rate': 0.000960176488606088, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8327231057248112}
Epoch 1/300, seasonal_3 Loss: 0.3249 | 0.1685
Epoch 2/300, seasonal_3 Loss: 0.1419 | 0.1253
Epoch 3/300, seasonal_3 Loss: 0.1146 | 0.0999
Epoch 4/300, seasonal_3 Loss: 0.1044 | 0.0938
Epoch 5/300, seasonal_3 Loss: 0.1022 | 0.0943
Epoch 6/300, seasonal_3 Loss: 0.1019 | 0.0870
Epoch 7/300, seasonal_3 Loss: 0.1011 | 0.0734
Epoch 8/300, seasonal_3 Loss: 0.0946 | 0.0706
Epoch 9/300, seasonal_3 Loss: 0.0899 | 0.0695
Epoch 10/300, seasonal_3 Loss: 0.0927 | 0.0650
Epoch 11/300, seasonal_3 Loss: 0.0874 | 0.0673
Epoch 12/300, seasonal_3 Loss: 0.0869 | 0.0616
Epoch 13/300, seasonal_3 Loss: 0.0813 | 0.0563
Epoch 14/300, seasonal_3 Loss: 0.0803 | 0.0533
Epoch 15/300, seasonal_3 Loss: 0.0771 | 0.0524
Epoch 16/300, seasonal_3 Loss: 0.0782 | 0.0488
Epoch 17/300, seasonal_3 Loss: 0.0778 | 0.0487
Epoch 18/300, seasonal_3 Loss: 0.0758 | 0.0476
Epoch 19/300, seasonal_3 Loss: 0.0814 | 0.0482
Epoch 20/300, seasonal_3 Loss: 0.0780 | 0.0535
Epoch 21/300, seasonal_3 Loss: 0.0779 | 0.0470
Epoch 22/300, seasonal_3 Loss: 0.0738 | 0.0460
Epoch 23/300, seasonal_3 Loss: 0.0736 | 0.0456
Epoch 24/300, seasonal_3 Loss: 0.0741 | 0.0475
Epoch 25/300, seasonal_3 Loss: 0.0744 | 0.0450
Epoch 26/300, seasonal_3 Loss: 0.0736 | 0.0567
Epoch 27/300, seasonal_3 Loss: 0.0729 | 0.0483
Epoch 28/300, seasonal_3 Loss: 0.0697 | 0.0446
Epoch 29/300, seasonal_3 Loss: 0.0683 | 0.0435
Epoch 30/300, seasonal_3 Loss: 0.0674 | 0.0422
Epoch 31/300, seasonal_3 Loss: 0.0665 | 0.0426
Epoch 32/300, seasonal_3 Loss: 0.0663 | 0.0423
Epoch 33/300, seasonal_3 Loss: 0.0656 | 0.0415
Epoch 34/300, seasonal_3 Loss: 0.0654 | 0.0410
Epoch 35/300, seasonal_3 Loss: 0.0653 | 0.0407
Epoch 36/300, seasonal_3 Loss: 0.0650 | 0.0414
Epoch 37/300, seasonal_3 Loss: 0.0658 | 0.0422
Epoch 38/300, seasonal_3 Loss: 0.0661 | 0.0424
Epoch 39/300, seasonal_3 Loss: 0.0676 | 0.0440
Epoch 40/300, seasonal_3 Loss: 0.0695 | 0.0457
Epoch 41/300, seasonal_3 Loss: 0.0720 | 0.0433
Epoch 42/300, seasonal_3 Loss: 0.0800 | 0.0480
Epoch 43/300, seasonal_3 Loss: 0.0768 | 0.0480
Epoch 44/300, seasonal_3 Loss: 0.0825 | 0.0681
Epoch 45/300, seasonal_3 Loss: 0.0840 | 0.0982
Epoch 46/300, seasonal_3 Loss: 0.0805 | 0.1038
Epoch 47/300, seasonal_3 Loss: 0.0765 | 0.0473
Epoch 48/300, seasonal_3 Loss: 0.0712 | 0.0407
Epoch 49/300, seasonal_3 Loss: 0.0668 | 0.0396
Epoch 50/300, seasonal_3 Loss: 0.0641 | 0.0405
Epoch 51/300, seasonal_3 Loss: 0.0634 | 0.0414
Epoch 52/300, seasonal_3 Loss: 0.0633 | 0.0405
Epoch 53/300, seasonal_3 Loss: 0.0636 | 0.0415
Epoch 54/300, seasonal_3 Loss: 0.0637 | 0.0418
Epoch 55/300, seasonal_3 Loss: 0.0632 | 0.0405
Epoch 56/300, seasonal_3 Loss: 0.0624 | 0.0382
Epoch 57/300, seasonal_3 Loss: 0.0618 | 0.0367
Epoch 58/300, seasonal_3 Loss: 0.0619 | 0.0369
Epoch 59/300, seasonal_3 Loss: 0.0635 | 0.0394
Epoch 60/300, seasonal_3 Loss: 0.0653 | 0.0388
Epoch 61/300, seasonal_3 Loss: 0.0637 | 0.0413
Epoch 62/300, seasonal_3 Loss: 0.0616 | 0.0372
Epoch 63/300, seasonal_3 Loss: 0.0608 | 0.0360
Epoch 64/300, seasonal_3 Loss: 0.0604 | 0.0357
Epoch 65/300, seasonal_3 Loss: 0.0602 | 0.0356
Epoch 66/300, seasonal_3 Loss: 0.0601 | 0.0356
Epoch 67/300, seasonal_3 Loss: 0.0599 | 0.0357
Epoch 68/300, seasonal_3 Loss: 0.0596 | 0.0356
Epoch 69/300, seasonal_3 Loss: 0.0593 | 0.0355
Epoch 70/300, seasonal_3 Loss: 0.0591 | 0.0353
Epoch 71/300, seasonal_3 Loss: 0.0590 | 0.0352
Epoch 72/300, seasonal_3 Loss: 0.0589 | 0.0351
Epoch 73/300, seasonal_3 Loss: 0.0588 | 0.0350
Epoch 74/300, seasonal_3 Loss: 0.0587 | 0.0349
Epoch 75/300, seasonal_3 Loss: 0.0586 | 0.0347
Epoch 76/300, seasonal_3 Loss: 0.0584 | 0.0346
Epoch 77/300, seasonal_3 Loss: 0.0583 | 0.0343
Epoch 78/300, seasonal_3 Loss: 0.0582 | 0.0340
Epoch 79/300, seasonal_3 Loss: 0.0580 | 0.0338
Epoch 80/300, seasonal_3 Loss: 0.0579 | 0.0336
Epoch 81/300, seasonal_3 Loss: 0.0579 | 0.0335
Epoch 82/300, seasonal_3 Loss: 0.0579 | 0.0334
Epoch 83/300, seasonal_3 Loss: 0.0580 | 0.0333
Epoch 84/300, seasonal_3 Loss: 0.0580 | 0.0332
Epoch 85/300, seasonal_3 Loss: 0.0581 | 0.0332
Epoch 86/300, seasonal_3 Loss: 0.0582 | 0.0334
Epoch 87/300, seasonal_3 Loss: 0.0584 | 0.0333
Epoch 88/300, seasonal_3 Loss: 0.0590 | 0.0333
Epoch 89/300, seasonal_3 Loss: 0.0609 | 0.0357
Epoch 90/300, seasonal_3 Loss: 0.0604 | 0.0342
Epoch 91/300, seasonal_3 Loss: 0.0604 | 0.0345
Epoch 92/300, seasonal_3 Loss: 0.0602 | 0.0347
Epoch 93/300, seasonal_3 Loss: 0.0595 | 0.0343
Epoch 94/300, seasonal_3 Loss: 0.0589 | 0.0338
Epoch 95/300, seasonal_3 Loss: 0.0584 | 0.0333
Epoch 96/300, seasonal_3 Loss: 0.0582 | 0.0328
Epoch 97/300, seasonal_3 Loss: 0.0580 | 0.0326
Epoch 98/300, seasonal_3 Loss: 0.0580 | 0.0325
Epoch 99/300, seasonal_3 Loss: 0.0582 | 0.0326
Epoch 100/300, seasonal_3 Loss: 0.0582 | 0.0328
Epoch 101/300, seasonal_3 Loss: 0.0583 | 0.0333
Epoch 102/300, seasonal_3 Loss: 0.0581 | 0.0330
Epoch 103/300, seasonal_3 Loss: 0.0576 | 0.0325
Epoch 104/300, seasonal_3 Loss: 0.0571 | 0.0321
Epoch 105/300, seasonal_3 Loss: 0.0569 | 0.0319
Epoch 106/300, seasonal_3 Loss: 0.0567 | 0.0318
Epoch 107/300, seasonal_3 Loss: 0.0566 | 0.0317
Epoch 108/300, seasonal_3 Loss: 0.0565 | 0.0316
Epoch 109/300, seasonal_3 Loss: 0.0565 | 0.0316
Epoch 110/300, seasonal_3 Loss: 0.0564 | 0.0315
Epoch 111/300, seasonal_3 Loss: 0.0564 | 0.0315
Epoch 112/300, seasonal_3 Loss: 0.0564 | 0.0314
Epoch 113/300, seasonal_3 Loss: 0.0563 | 0.0314
Epoch 114/300, seasonal_3 Loss: 0.0563 | 0.0313
Epoch 115/300, seasonal_3 Loss: 0.0562 | 0.0313
Epoch 116/300, seasonal_3 Loss: 0.0562 | 0.0313
Epoch 117/300, seasonal_3 Loss: 0.0562 | 0.0312
Epoch 118/300, seasonal_3 Loss: 0.0561 | 0.0312
Epoch 119/300, seasonal_3 Loss: 0.0561 | 0.0311
Epoch 120/300, seasonal_3 Loss: 0.0561 | 0.0311
Epoch 121/300, seasonal_3 Loss: 0.0560 | 0.0311
Epoch 122/300, seasonal_3 Loss: 0.0560 | 0.0310
Epoch 123/300, seasonal_3 Loss: 0.0560 | 0.0310
Epoch 124/300, seasonal_3 Loss: 0.0560 | 0.0309
Epoch 125/300, seasonal_3 Loss: 0.0559 | 0.0309
Epoch 126/300, seasonal_3 Loss: 0.0559 | 0.0309
Epoch 127/300, seasonal_3 Loss: 0.0559 | 0.0308
Epoch 128/300, seasonal_3 Loss: 0.0558 | 0.0308
Epoch 129/300, seasonal_3 Loss: 0.0558 | 0.0308
Epoch 130/300, seasonal_3 Loss: 0.0558 | 0.0307
Epoch 131/300, seasonal_3 Loss: 0.0558 | 0.0307
Epoch 132/300, seasonal_3 Loss: 0.0558 | 0.0307
Epoch 133/300, seasonal_3 Loss: 0.0557 | 0.0306
Epoch 134/300, seasonal_3 Loss: 0.0557 | 0.0306
Epoch 135/300, seasonal_3 Loss: 0.0557 | 0.0306
Epoch 136/300, seasonal_3 Loss: 0.0557 | 0.0306
Epoch 137/300, seasonal_3 Loss: 0.0556 | 0.0305
Epoch 138/300, seasonal_3 Loss: 0.0556 | 0.0305
Epoch 139/300, seasonal_3 Loss: 0.0556 | 0.0305
Epoch 140/300, seasonal_3 Loss: 0.0556 | 0.0305
Epoch 141/300, seasonal_3 Loss: 0.0556 | 0.0304
Epoch 142/300, seasonal_3 Loss: 0.0556 | 0.0304
Epoch 143/300, seasonal_3 Loss: 0.0555 | 0.0304
Epoch 144/300, seasonal_3 Loss: 0.0555 | 0.0304
Epoch 145/300, seasonal_3 Loss: 0.0555 | 0.0303
Epoch 146/300, seasonal_3 Loss: 0.0555 | 0.0303
Epoch 147/300, seasonal_3 Loss: 0.0555 | 0.0303
Epoch 148/300, seasonal_3 Loss: 0.0555 | 0.0303
Epoch 149/300, seasonal_3 Loss: 0.0555 | 0.0303
Epoch 150/300, seasonal_3 Loss: 0.0554 | 0.0302
Epoch 151/300, seasonal_3 Loss: 0.0554 | 0.0302
Epoch 152/300, seasonal_3 Loss: 0.0554 | 0.0302
Epoch 153/300, seasonal_3 Loss: 0.0554 | 0.0302
Epoch 154/300, seasonal_3 Loss: 0.0554 | 0.0302
Epoch 155/300, seasonal_3 Loss: 0.0554 | 0.0301
Epoch 156/300, seasonal_3 Loss: 0.0554 | 0.0301
Epoch 157/300, seasonal_3 Loss: 0.0554 | 0.0301
Epoch 158/300, seasonal_3 Loss: 0.0554 | 0.0301
Epoch 159/300, seasonal_3 Loss: 0.0553 | 0.0301
Epoch 160/300, seasonal_3 Loss: 0.0553 | 0.0301
Epoch 161/300, seasonal_3 Loss: 0.0553 | 0.0301
Epoch 162/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 163/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 164/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 165/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 166/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 167/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 168/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 169/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 170/300, seasonal_3 Loss: 0.0553 | 0.0300
Epoch 171/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 172/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 173/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 174/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 175/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 176/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 177/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 178/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 179/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 180/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 181/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 182/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 183/300, seasonal_3 Loss: 0.0552 | 0.0299
Epoch 184/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 185/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 186/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 187/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 188/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 189/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 190/300, seasonal_3 Loss: 0.0552 | 0.0298
Epoch 191/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 192/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 193/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 194/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 195/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 196/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 197/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 198/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 199/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 200/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 201/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 202/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 203/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 204/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 205/300, seasonal_3 Loss: 0.0551 | 0.0298
Epoch 206/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 207/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 208/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 209/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 210/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 211/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 212/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 213/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 214/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 215/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 216/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 217/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 218/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 219/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 220/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 221/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 222/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 223/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 224/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 225/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 226/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 227/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 228/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 229/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 230/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 231/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 232/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 233/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 234/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 235/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 236/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 237/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 238/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 239/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 240/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 241/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 242/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 243/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 244/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 245/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 246/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 247/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 248/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 249/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 250/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 251/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 252/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 253/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 254/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 255/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 256/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 257/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 258/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 259/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 260/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 261/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 262/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 263/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 264/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 265/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 266/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 267/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 268/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 269/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 270/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 271/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 272/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 273/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 274/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 275/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 276/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 277/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 278/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 279/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 280/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 281/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 282/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 283/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 284/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 285/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 286/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 287/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 288/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 289/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 290/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 291/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 292/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 293/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 294/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 295/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 296/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 297/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 298/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 299/300, seasonal_3 Loss: 0.0551 | 0.0297
Epoch 300/300, seasonal_3 Loss: 0.0551 | 0.0297
Training resid component with params: {'observation_period_num': 6, 'train_rates': 0.8395009498867446, 'learning_rate': 0.0007234273774435494, 'batch_size': 179, 'step_size': 6, 'gamma': 0.9283912681042312}
Epoch 1/300, resid Loss: 0.5097 | 0.2241
Epoch 2/300, resid Loss: 0.1929 | 0.1573
Epoch 3/300, resid Loss: 0.1590 | 0.1406
Epoch 4/300, resid Loss: 0.1557 | 0.3102
Epoch 5/300, resid Loss: 0.1490 | 0.1260
Epoch 6/300, resid Loss: 0.1241 | 0.1011
Epoch 7/300, resid Loss: 0.1354 | 0.1181
Epoch 8/300, resid Loss: 0.1439 | 0.0815
Epoch 9/300, resid Loss: 0.1190 | 0.0773
Epoch 10/300, resid Loss: 0.1294 | 0.0764
Epoch 11/300, resid Loss: 0.1109 | 0.0759
Epoch 12/300, resid Loss: 0.1209 | 0.1163
Epoch 13/300, resid Loss: 0.1270 | 0.0801
Epoch 14/300, resid Loss: 0.1199 | 0.1003
Epoch 15/300, resid Loss: 0.1137 | 0.0674
Epoch 16/300, resid Loss: 0.1096 | 0.0763
Epoch 17/300, resid Loss: 0.1231 | 0.0905
Epoch 18/300, resid Loss: 0.1349 | 0.0841
Epoch 19/300, resid Loss: 0.1372 | 0.1465
Epoch 20/300, resid Loss: 0.1105 | 0.0615
Epoch 21/300, resid Loss: 0.1117 | 0.0730
Epoch 22/300, resid Loss: 0.1280 | 0.0679
Epoch 23/300, resid Loss: 0.1127 | 0.0991
Epoch 24/300, resid Loss: 0.1139 | 0.0573
Epoch 25/300, resid Loss: 0.0952 | 0.0625
Epoch 26/300, resid Loss: 0.0950 | 0.0530
Epoch 27/300, resid Loss: 0.0931 | 0.0611
Epoch 28/300, resid Loss: 0.0922 | 0.0536
Epoch 29/300, resid Loss: 0.0917 | 0.0627
Epoch 30/300, resid Loss: 0.0918 | 0.0526
Epoch 31/300, resid Loss: 0.0888 | 0.0556
Epoch 32/300, resid Loss: 0.0891 | 0.0516
Epoch 33/300, resid Loss: 0.0887 | 0.0556
Epoch 34/300, resid Loss: 0.0893 | 0.0518
Epoch 35/300, resid Loss: 0.0890 | 0.0551
Epoch 36/300, resid Loss: 0.0906 | 0.0543
Epoch 37/300, resid Loss: 0.0893 | 0.0539
Epoch 38/300, resid Loss: 0.0909 | 0.0569
Epoch 39/300, resid Loss: 0.0902 | 0.0549
Epoch 40/300, resid Loss: 0.0904 | 0.0577
Epoch 41/300, resid Loss: 0.0889 | 0.0529
Epoch 42/300, resid Loss: 0.0885 | 0.0552
Epoch 43/300, resid Loss: 0.0865 | 0.0503
Epoch 44/300, resid Loss: 0.0857 | 0.0527
Epoch 45/300, resid Loss: 0.0843 | 0.0483
Epoch 46/300, resid Loss: 0.0830 | 0.0497
Epoch 47/300, resid Loss: 0.0820 | 0.0468
Epoch 48/300, resid Loss: 0.0812 | 0.0477
Epoch 49/300, resid Loss: 0.0811 | 0.0470
Epoch 50/300, resid Loss: 0.0808 | 0.0472
Epoch 51/300, resid Loss: 0.0816 | 0.0462
Epoch 52/300, resid Loss: 0.0818 | 0.0461
Epoch 53/300, resid Loss: 0.0812 | 0.0468
Epoch 54/300, resid Loss: 0.0789 | 0.0452
Epoch 55/300, resid Loss: 0.0785 | 0.0449
Epoch 56/300, resid Loss: 0.0780 | 0.0452
Epoch 57/300, resid Loss: 0.0775 | 0.0441
Epoch 58/300, resid Loss: 0.0772 | 0.0439
Epoch 59/300, resid Loss: 0.0772 | 0.0430
Epoch 60/300, resid Loss: 0.0768 | 0.0432
Epoch 61/300, resid Loss: 0.0768 | 0.0424
Epoch 62/300, resid Loss: 0.0764 | 0.0425
Epoch 63/300, resid Loss: 0.0763 | 0.0420
Epoch 64/300, resid Loss: 0.0759 | 0.0419
Epoch 65/300, resid Loss: 0.0756 | 0.0415
Epoch 66/300, resid Loss: 0.0752 | 0.0414
Epoch 67/300, resid Loss: 0.0749 | 0.0411
Epoch 68/300, resid Loss: 0.0746 | 0.0410
Epoch 69/300, resid Loss: 0.0744 | 0.0408
Epoch 70/300, resid Loss: 0.0742 | 0.0408
Epoch 71/300, resid Loss: 0.0741 | 0.0406
Epoch 72/300, resid Loss: 0.0739 | 0.0405
Epoch 73/300, resid Loss: 0.0738 | 0.0399
Epoch 74/300, resid Loss: 0.0736 | 0.0395
Epoch 75/300, resid Loss: 0.0732 | 0.0392
Epoch 76/300, resid Loss: 0.0729 | 0.0391
Epoch 77/300, resid Loss: 0.0727 | 0.0389
Epoch 78/300, resid Loss: 0.0726 | 0.0388
Epoch 79/300, resid Loss: 0.0725 | 0.0388
Epoch 80/300, resid Loss: 0.0727 | 0.0391
Epoch 81/300, resid Loss: 0.0730 | 0.0393
Epoch 82/300, resid Loss: 0.0738 | 0.0386
Epoch 83/300, resid Loss: 0.0736 | 0.0389
Epoch 84/300, resid Loss: 0.0725 | 0.0388
Epoch 85/300, resid Loss: 0.0727 | 0.0382
Epoch 86/300, resid Loss: 0.0726 | 0.0381
Epoch 87/300, resid Loss: 0.0716 | 0.0377
Epoch 88/300, resid Loss: 0.0711 | 0.0377
Epoch 89/300, resid Loss: 0.0712 | 0.0376
Epoch 90/300, resid Loss: 0.0711 | 0.0375
Epoch 91/300, resid Loss: 0.0709 | 0.0372
Epoch 92/300, resid Loss: 0.0708 | 0.0373
Epoch 93/300, resid Loss: 0.0708 | 0.0371
Epoch 94/300, resid Loss: 0.0706 | 0.0370
Epoch 95/300, resid Loss: 0.0705 | 0.0370
Epoch 96/300, resid Loss: 0.0703 | 0.0369
Epoch 97/300, resid Loss: 0.0702 | 0.0368
Epoch 98/300, resid Loss: 0.0702 | 0.0368
Epoch 99/300, resid Loss: 0.0701 | 0.0366
Epoch 100/300, resid Loss: 0.0700 | 0.0366
Epoch 101/300, resid Loss: 0.0699 | 0.0365
Epoch 102/300, resid Loss: 0.0699 | 0.0365
Epoch 103/300, resid Loss: 0.0698 | 0.0364
Epoch 104/300, resid Loss: 0.0698 | 0.0363
Epoch 105/300, resid Loss: 0.0697 | 0.0362
Epoch 106/300, resid Loss: 0.0697 | 0.0362
Epoch 107/300, resid Loss: 0.0696 | 0.0361
Epoch 108/300, resid Loss: 0.0695 | 0.0360
Epoch 109/300, resid Loss: 0.0694 | 0.0359
Epoch 110/300, resid Loss: 0.0693 | 0.0358
Epoch 111/300, resid Loss: 0.0692 | 0.0358
Epoch 112/300, resid Loss: 0.0691 | 0.0357
Epoch 113/300, resid Loss: 0.0691 | 0.0357
Epoch 114/300, resid Loss: 0.0690 | 0.0356
Epoch 115/300, resid Loss: 0.0691 | 0.0356
Epoch 116/300, resid Loss: 0.0691 | 0.0356
Epoch 117/300, resid Loss: 0.0692 | 0.0355
Epoch 118/300, resid Loss: 0.0693 | 0.0355
Epoch 119/300, resid Loss: 0.0690 | 0.0357
Epoch 120/300, resid Loss: 0.0689 | 0.0355
Epoch 121/300, resid Loss: 0.0691 | 0.0352
Epoch 122/300, resid Loss: 0.0690 | 0.0351
Epoch 123/300, resid Loss: 0.0686 | 0.0350
Epoch 124/300, resid Loss: 0.0684 | 0.0349
Epoch 125/300, resid Loss: 0.0684 | 0.0349
Epoch 126/300, resid Loss: 0.0684 | 0.0349
Epoch 127/300, resid Loss: 0.0683 | 0.0350
Epoch 128/300, resid Loss: 0.0682 | 0.0350
Epoch 129/300, resid Loss: 0.0682 | 0.0348
Epoch 130/300, resid Loss: 0.0682 | 0.0346
Epoch 131/300, resid Loss: 0.0681 | 0.0345
Epoch 132/300, resid Loss: 0.0679 | 0.0345
Epoch 133/300, resid Loss: 0.0679 | 0.0345
Epoch 134/300, resid Loss: 0.0679 | 0.0344
Epoch 135/300, resid Loss: 0.0679 | 0.0345
Epoch 136/300, resid Loss: 0.0677 | 0.0345
Epoch 137/300, resid Loss: 0.0677 | 0.0344
Epoch 138/300, resid Loss: 0.0677 | 0.0343
Epoch 139/300, resid Loss: 0.0676 | 0.0342
Epoch 140/300, resid Loss: 0.0676 | 0.0341
Epoch 141/300, resid Loss: 0.0675 | 0.0341
Epoch 142/300, resid Loss: 0.0675 | 0.0341
Epoch 143/300, resid Loss: 0.0675 | 0.0341
Epoch 144/300, resid Loss: 0.0674 | 0.0341
Epoch 145/300, resid Loss: 0.0673 | 0.0341
Epoch 146/300, resid Loss: 0.0673 | 0.0339
Epoch 147/300, resid Loss: 0.0673 | 0.0338
Epoch 148/300, resid Loss: 0.0672 | 0.0338
Epoch 149/300, resid Loss: 0.0672 | 0.0338
Epoch 150/300, resid Loss: 0.0672 | 0.0338
Epoch 151/300, resid Loss: 0.0672 | 0.0340
Epoch 152/300, resid Loss: 0.0671 | 0.0339
Epoch 153/300, resid Loss: 0.0670 | 0.0337
Epoch 154/300, resid Loss: 0.0671 | 0.0336
Epoch 155/300, resid Loss: 0.0670 | 0.0336
Epoch 156/300, resid Loss: 0.0670 | 0.0336
Epoch 157/300, resid Loss: 0.0670 | 0.0337
Epoch 158/300, resid Loss: 0.0669 | 0.0337
Epoch 159/300, resid Loss: 0.0669 | 0.0336
Epoch 160/300, resid Loss: 0.0668 | 0.0335
Epoch 161/300, resid Loss: 0.0668 | 0.0335
Epoch 162/300, resid Loss: 0.0668 | 0.0335
Epoch 163/300, resid Loss: 0.0668 | 0.0335
Epoch 164/300, resid Loss: 0.0667 | 0.0335
Epoch 165/300, resid Loss: 0.0667 | 0.0335
Epoch 166/300, resid Loss: 0.0667 | 0.0334
Epoch 167/300, resid Loss: 0.0667 | 0.0334
Epoch 168/300, resid Loss: 0.0666 | 0.0334
Epoch 169/300, resid Loss: 0.0666 | 0.0334
Epoch 170/300, resid Loss: 0.0666 | 0.0334
Epoch 171/300, resid Loss: 0.0666 | 0.0333
Epoch 172/300, resid Loss: 0.0666 | 0.0333
Epoch 173/300, resid Loss: 0.0665 | 0.0333
Epoch 174/300, resid Loss: 0.0665 | 0.0333
Epoch 175/300, resid Loss: 0.0665 | 0.0333
Epoch 176/300, resid Loss: 0.0665 | 0.0332
Epoch 177/300, resid Loss: 0.0665 | 0.0332
Epoch 178/300, resid Loss: 0.0665 | 0.0332
Epoch 179/300, resid Loss: 0.0664 | 0.0332
Epoch 180/300, resid Loss: 0.0664 | 0.0332
Epoch 181/300, resid Loss: 0.0664 | 0.0332
Epoch 182/300, resid Loss: 0.0664 | 0.0331
Epoch 183/300, resid Loss: 0.0664 | 0.0331
Epoch 184/300, resid Loss: 0.0664 | 0.0331
Epoch 185/300, resid Loss: 0.0664 | 0.0331
Epoch 186/300, resid Loss: 0.0663 | 0.0331
Epoch 187/300, resid Loss: 0.0663 | 0.0331
Epoch 188/300, resid Loss: 0.0663 | 0.0331
Epoch 189/300, resid Loss: 0.0663 | 0.0331
Epoch 190/300, resid Loss: 0.0663 | 0.0331
Epoch 191/300, resid Loss: 0.0663 | 0.0330
Epoch 192/300, resid Loss: 0.0663 | 0.0330
Epoch 193/300, resid Loss: 0.0663 | 0.0330
Epoch 194/300, resid Loss: 0.0663 | 0.0330
Epoch 195/300, resid Loss: 0.0662 | 0.0330
Epoch 196/300, resid Loss: 0.0662 | 0.0330
Epoch 197/300, resid Loss: 0.0662 | 0.0330
Epoch 198/300, resid Loss: 0.0662 | 0.0330
Epoch 199/300, resid Loss: 0.0662 | 0.0330
Epoch 200/300, resid Loss: 0.0662 | 0.0330
Epoch 201/300, resid Loss: 0.0662 | 0.0329
Epoch 202/300, resid Loss: 0.0662 | 0.0329
Epoch 203/300, resid Loss: 0.0662 | 0.0329
Epoch 204/300, resid Loss: 0.0662 | 0.0329
Epoch 205/300, resid Loss: 0.0661 | 0.0329
Epoch 206/300, resid Loss: 0.0661 | 0.0329
Epoch 207/300, resid Loss: 0.0661 | 0.0329
Epoch 208/300, resid Loss: 0.0661 | 0.0329
Epoch 209/300, resid Loss: 0.0661 | 0.0329
Epoch 210/300, resid Loss: 0.0661 | 0.0329
Epoch 211/300, resid Loss: 0.0661 | 0.0329
Epoch 212/300, resid Loss: 0.0661 | 0.0329
Epoch 213/300, resid Loss: 0.0661 | 0.0329
Epoch 214/300, resid Loss: 0.0661 | 0.0328
Epoch 215/300, resid Loss: 0.0661 | 0.0328
Epoch 216/300, resid Loss: 0.0661 | 0.0328
Epoch 217/300, resid Loss: 0.0661 | 0.0328
Epoch 218/300, resid Loss: 0.0661 | 0.0328
Epoch 219/300, resid Loss: 0.0660 | 0.0328
Epoch 220/300, resid Loss: 0.0660 | 0.0328
Epoch 221/300, resid Loss: 0.0660 | 0.0328
Epoch 222/300, resid Loss: 0.0660 | 0.0328
Epoch 223/300, resid Loss: 0.0660 | 0.0328
Epoch 224/300, resid Loss: 0.0660 | 0.0328
Epoch 225/300, resid Loss: 0.0660 | 0.0328
Epoch 226/300, resid Loss: 0.0660 | 0.0328
Epoch 227/300, resid Loss: 0.0660 | 0.0328
Epoch 228/300, resid Loss: 0.0660 | 0.0328
Epoch 229/300, resid Loss: 0.0660 | 0.0328
Epoch 230/300, resid Loss: 0.0660 | 0.0328
Epoch 231/300, resid Loss: 0.0660 | 0.0328
Epoch 232/300, resid Loss: 0.0660 | 0.0328
Epoch 233/300, resid Loss: 0.0660 | 0.0328
Epoch 234/300, resid Loss: 0.0660 | 0.0327
Epoch 235/300, resid Loss: 0.0660 | 0.0327
Epoch 236/300, resid Loss: 0.0660 | 0.0327
Epoch 237/300, resid Loss: 0.0660 | 0.0327
Epoch 238/300, resid Loss: 0.0660 | 0.0327
Epoch 239/300, resid Loss: 0.0659 | 0.0327
Epoch 240/300, resid Loss: 0.0659 | 0.0327
Epoch 241/300, resid Loss: 0.0659 | 0.0327
Epoch 242/300, resid Loss: 0.0659 | 0.0327
Epoch 243/300, resid Loss: 0.0659 | 0.0327
Epoch 244/300, resid Loss: 0.0659 | 0.0327
Epoch 245/300, resid Loss: 0.0659 | 0.0327
Epoch 246/300, resid Loss: 0.0659 | 0.0327
Epoch 247/300, resid Loss: 0.0659 | 0.0327
Epoch 248/300, resid Loss: 0.0659 | 0.0327
Epoch 249/300, resid Loss: 0.0659 | 0.0327
Epoch 250/300, resid Loss: 0.0659 | 0.0327
Epoch 251/300, resid Loss: 0.0659 | 0.0327
Epoch 252/300, resid Loss: 0.0659 | 0.0327
Epoch 253/300, resid Loss: 0.0659 | 0.0327
Epoch 254/300, resid Loss: 0.0659 | 0.0327
Epoch 255/300, resid Loss: 0.0659 | 0.0327
Epoch 256/300, resid Loss: 0.0659 | 0.0327
Epoch 257/300, resid Loss: 0.0659 | 0.0327
Epoch 258/300, resid Loss: 0.0659 | 0.0327
Epoch 259/300, resid Loss: 0.0659 | 0.0327
Epoch 260/300, resid Loss: 0.0659 | 0.0327
Epoch 261/300, resid Loss: 0.0659 | 0.0327
Epoch 262/300, resid Loss: 0.0659 | 0.0327
Epoch 263/300, resid Loss: 0.0659 | 0.0327
Epoch 264/300, resid Loss: 0.0659 | 0.0327
Epoch 265/300, resid Loss: 0.0659 | 0.0327
Epoch 266/300, resid Loss: 0.0659 | 0.0327
Epoch 267/300, resid Loss: 0.0659 | 0.0327
Epoch 268/300, resid Loss: 0.0659 | 0.0327
Epoch 269/300, resid Loss: 0.0659 | 0.0327
Epoch 270/300, resid Loss: 0.0659 | 0.0326
Epoch 271/300, resid Loss: 0.0659 | 0.0326
Epoch 272/300, resid Loss: 0.0659 | 0.0326
Epoch 273/300, resid Loss: 0.0659 | 0.0326
Epoch 274/300, resid Loss: 0.0659 | 0.0326
Epoch 275/300, resid Loss: 0.0659 | 0.0326
Epoch 276/300, resid Loss: 0.0659 | 0.0326
Epoch 277/300, resid Loss: 0.0658 | 0.0326
Epoch 278/300, resid Loss: 0.0658 | 0.0326
Epoch 279/300, resid Loss: 0.0658 | 0.0326
Epoch 280/300, resid Loss: 0.0658 | 0.0326
Epoch 281/300, resid Loss: 0.0658 | 0.0326
Epoch 282/300, resid Loss: 0.0658 | 0.0326
Epoch 283/300, resid Loss: 0.0658 | 0.0326
Epoch 284/300, resid Loss: 0.0658 | 0.0326
Epoch 285/300, resid Loss: 0.0658 | 0.0326
Epoch 286/300, resid Loss: 0.0658 | 0.0326
Epoch 287/300, resid Loss: 0.0658 | 0.0326
Epoch 288/300, resid Loss: 0.0658 | 0.0326
Epoch 289/300, resid Loss: 0.0658 | 0.0326
Epoch 290/300, resid Loss: 0.0658 | 0.0326
Epoch 291/300, resid Loss: 0.0658 | 0.0326
Epoch 292/300, resid Loss: 0.0658 | 0.0326
Epoch 293/300, resid Loss: 0.0658 | 0.0326
Epoch 294/300, resid Loss: 0.0658 | 0.0326
Epoch 295/300, resid Loss: 0.0658 | 0.0326
Epoch 296/300, resid Loss: 0.0658 | 0.0326
Epoch 297/300, resid Loss: 0.0658 | 0.0326
Epoch 298/300, resid Loss: 0.0658 | 0.0326
Epoch 299/300, resid Loss: 0.0658 | 0.0326
Epoch 300/300, resid Loss: 0.0658 | 0.0326
Runtime (seconds): 2334.1930134296417
6.282516697543941e-05
[155.01883]
[-1.0042012]
[-3.6074855]
[11.044105]
[3.9788544]
[9.723932]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 341.3613716328982
RMSE: 18.475967407226562
MAE: 18.475967407226562
R-squared: nan
[175.15404]
