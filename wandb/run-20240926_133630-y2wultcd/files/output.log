[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
[*********************100%%**********************]  1 of 1 completed
/home/raikakoki/.local/lib/python3.10/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.
  warnings.warn(
Date
2019-12-31     71.250435
2020-01-02     72.876106
2020-01-03     72.167610
2020-01-06     72.742661
2020-01-07     72.400551
                 ...
2023-05-24    170.734589
2023-05-25    171.877197
2023-05-26    174.301498
2023-05-30    176.159470
2023-05-31    176.109802
Name: Adj Close, Length: 860, dtype: float64
Dataset created successfully.
Epoch 1/100, Training Loss: 0.7339, Validation Loss: 0.2720
Epoch 2/100, Training Loss: 0.2061, Validation Loss: 0.2092
Epoch 3/100, Training Loss: 0.1365, Validation Loss: 0.1519
Epoch 4/100, Training Loss: 0.1133, Validation Loss: 0.1528
Epoch 5/100, Training Loss: 0.0904, Validation Loss: 0.1158
Epoch 6/100, Training Loss: 0.0760, Validation Loss: 0.1118
Epoch 7/100, Training Loss: 0.0652, Validation Loss: 0.0704
Epoch 8/100, Training Loss: 0.0560, Validation Loss: 0.0831
Epoch 9/100, Training Loss: 0.0559, Validation Loss: 0.0543
Epoch 10/100, Training Loss: 0.0492, Validation Loss: 0.0588
Epoch 11/100, Training Loss: 0.0453, Validation Loss: 0.0552
Epoch 12/100, Training Loss: 0.0368, Validation Loss: 0.0418
Epoch 13/100, Training Loss: 0.0370, Validation Loss: 0.0600
Epoch 14/100, Training Loss: 0.0385, Validation Loss: 0.0428
Epoch 15/100, Training Loss: 0.0354, Validation Loss: 0.0528
Epoch 16/100, Training Loss: 0.0397, Validation Loss: 0.0432
Epoch 17/100, Training Loss: 0.0326, Validation Loss: 0.0383
Epoch 18/100, Training Loss: 0.0290, Validation Loss: 0.0490
Epoch 19/100, Training Loss: 0.0285, Validation Loss: 0.0316
Epoch 20/100, Training Loss: 0.0321, Validation Loss: 0.0664
Epoch 21/100, Training Loss: 0.0414, Validation Loss: 0.0296
Epoch 22/100, Training Loss: 0.0404, Validation Loss: 0.1021
Epoch 23/100, Training Loss: 0.0522, Validation Loss: 0.0457
Epoch 24/100, Training Loss: 0.0540, Validation Loss: 0.1378
Epoch 25/100, Training Loss: 0.0512, Validation Loss: 0.0471
Epoch 26/100, Training Loss: 0.0482, Validation Loss: 0.0946
Epoch 27/100, Training Loss: 0.0346, Validation Loss: 0.0367
Epoch 28/100, Training Loss: 0.0317, Validation Loss: 0.0593
Epoch 29/100, Training Loss: 0.0267, Validation Loss: 0.0384
Epoch 30/100, Training Loss: 0.0260, Validation Loss: 0.0459
Epoch 31/100, Training Loss: 0.0247, Validation Loss: 0.0380
Epoch 32/100, Training Loss: 0.0243, Validation Loss: 0.0410
Epoch 33/100, Training Loss: 0.0236, Validation Loss: 0.0375
Epoch 34/100, Training Loss: 0.0233, Validation Loss: 0.0378
Epoch 35/100, Training Loss: 0.0229, Validation Loss: 0.0362
Epoch 36/100, Training Loss: 0.0226, Validation Loss: 0.0363
Epoch 37/100, Training Loss: 0.0222, Validation Loss: 0.0353
Epoch 38/100, Training Loss: 0.0220, Validation Loss: 0.0348
Epoch 39/100, Training Loss: 0.0217, Validation Loss: 0.0344
Epoch 40/100, Training Loss: 0.0215, Validation Loss: 0.0340
Epoch 41/100, Training Loss: 0.0212, Validation Loss: 0.0335
Epoch 42/100, Training Loss: 0.0210, Validation Loss: 0.0332
Epoch 43/100, Training Loss: 0.0208, Validation Loss: 0.0328
Epoch 44/100, Training Loss: 0.0207, Validation Loss: 0.0325
Epoch 45/100, Training Loss: 0.0205, Validation Loss: 0.0322
Epoch 46/100, Training Loss: 0.0203, Validation Loss: 0.0319
Epoch 47/100, Training Loss: 0.0202, Validation Loss: 0.0317
Epoch 48/100, Training Loss: 0.0200, Validation Loss: 0.0314
Epoch 49/100, Training Loss: 0.0199, Validation Loss: 0.0312
Epoch 50/100, Training Loss: 0.0198, Validation Loss: 0.0310
Epoch 51/100, Training Loss: 0.0197, Validation Loss: 0.0307
Epoch 52/100, Training Loss: 0.0195, Validation Loss: 0.0306
Epoch 53/100, Training Loss: 0.0194, Validation Loss: 0.0304
Epoch 54/100, Training Loss: 0.0193, Validation Loss: 0.0302
Epoch 55/100, Training Loss: 0.0192, Validation Loss: 0.0300
Epoch 56/100, Training Loss: 0.0191, Validation Loss: 0.0299
Epoch 57/100, Training Loss: 0.0191, Validation Loss: 0.0297
Epoch 58/100, Training Loss: 0.0190, Validation Loss: 0.0296
Epoch 59/100, Training Loss: 0.0189, Validation Loss: 0.0294
Epoch 60/100, Training Loss: 0.0188, Validation Loss: 0.0293
Epoch 61/100, Training Loss: 0.0188, Validation Loss: 0.0292
Epoch 62/100, Training Loss: 0.0187, Validation Loss: 0.0291
Epoch 63/100, Training Loss: 0.0187, Validation Loss: 0.0290
Epoch 64/100, Training Loss: 0.0186, Validation Loss: 0.0289
Epoch 65/100, Training Loss: 0.0187, Validation Loss: 0.0288
Epoch 66/100, Training Loss: 0.0187, Validation Loss: 0.0288
Epoch 67/100, Training Loss: 0.0189, Validation Loss: 0.0286
Epoch 68/100, Training Loss: 0.0190, Validation Loss: 0.0287
Epoch 69/100, Training Loss: 0.0194, Validation Loss: 0.0285
Epoch 70/100, Training Loss: 0.0194, Validation Loss: 0.0285
Epoch 71/100, Training Loss: 0.0195, Validation Loss: 0.0283
Epoch 72/100, Training Loss: 0.0191, Validation Loss: 0.0283
Epoch 73/100, Training Loss: 0.0188, Validation Loss: 0.0282
Epoch 74/100, Training Loss: 0.0184, Validation Loss: 0.0281
Epoch 75/100, Training Loss: 0.0183, Validation Loss: 0.0281
Epoch 76/100, Training Loss: 0.0181, Validation Loss: 0.0280
Epoch 77/100, Training Loss: 0.0180, Validation Loss: 0.0280
Epoch 78/100, Training Loss: 0.0180, Validation Loss: 0.0279
Epoch 79/100, Training Loss: 0.0180, Validation Loss: 0.0279
Epoch 80/100, Training Loss: 0.0179, Validation Loss: 0.0278
Epoch 81/100, Training Loss: 0.0179, Validation Loss: 0.0278
Epoch 82/100, Training Loss: 0.0179, Validation Loss: 0.0277
Epoch 83/100, Training Loss: 0.0179, Validation Loss: 0.0277
Epoch 84/100, Training Loss: 0.0178, Validation Loss: 0.0276
Epoch 85/100, Training Loss: 0.0178, Validation Loss: 0.0276
Epoch 86/100, Training Loss: 0.0178, Validation Loss: 0.0275
Epoch 87/100, Training Loss: 0.0178, Validation Loss: 0.0275
Epoch 88/100, Training Loss: 0.0178, Validation Loss: 0.0275
Epoch 89/100, Training Loss: 0.0177, Validation Loss: 0.0274
Epoch 90/100, Training Loss: 0.0177, Validation Loss: 0.0274
Epoch 91/100, Training Loss: 0.0177, Validation Loss: 0.0274
Epoch 92/100, Training Loss: 0.0177, Validation Loss: 0.0274
Epoch 93/100, Training Loss: 0.0177, Validation Loss: 0.0273
Epoch 94/100, Training Loss: 0.0177, Validation Loss: 0.0273
Epoch 95/100, Training Loss: 0.0176, Validation Loss: 0.0273
Epoch 96/100, Training Loss: 0.0176, Validation Loss: 0.0273
Epoch 97/100, Training Loss: 0.0176, Validation Loss: 0.0272
Epoch 98/100, Training Loss: 0.0176, Validation Loss: 0.0272
/mnt/c/Users/RAIKA KOKI/B4研究/Multi_iTransformer/demo.py:117: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_selected_stock_price = predicted_selected_stock_price.cpu().numpy().flatten() * std_list[0] + mean_list[0]  # Using AAPL normalization factors
Epoch 99/100, Training Loss: 0.0176, Validation Loss: 0.0272
Epoch 100/100, Training Loss: 0.0176, Validation Loss: 0.0272
['2023-05-17', '2023-05-18', '2023-05-19', '2023-05-22', '2023-05-23', '2023-05-24', '2023-05-25', '2023-05-26', '2023-05-30', '2023-05-31']
[168.91464]
[171.57914734 173.9239502  174.0332489  173.079422   170.45639038
 170.73458862 171.87719727 174.30149841 176.1594696  168.91464233]
[171.57914734 173.9239502  174.0332489  173.079422   170.45639038
 170.73458862 171.87719727 174.30149841 176.1594696  176.10980225]
/mnt/c/Users/RAIKA KOKI/B4研究/Multi_iTransformer/demo.py:149: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()