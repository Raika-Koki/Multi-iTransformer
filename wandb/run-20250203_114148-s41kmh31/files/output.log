[32m[I 2025-02-03 11:41:54,976][0m A new study created in memory with name: no-name-80403bf4-26a9-414b-9981-504c6d642df6[0m
[32m[I 2025-02-03 11:42:24,694][0m Trial 0 finished with value: 0.6500369696056142 and parameters: {'observation_period_num': 180, 'train_rates': 0.8530735059946846, 'learning_rate': 2.4356511696112237e-06, 'batch_size': 192, 'step_size': 5, 'gamma': 0.7760275375698115}. Best is trial 0 with value: 0.6500369696056142.[0m
[32m[I 2025-02-03 11:42:48,085][0m Trial 1 finished with value: 0.19556572430712335 and parameters: {'observation_period_num': 50, 'train_rates': 0.6258905479439064, 'learning_rate': 2.891339543796937e-05, 'batch_size': 203, 'step_size': 4, 'gamma': 0.7958295754522614}. Best is trial 1 with value: 0.19556572430712335.[0m
[32m[I 2025-02-03 11:43:29,631][0m Trial 2 finished with value: 0.07080070120456926 and parameters: {'observation_period_num': 22, 'train_rates': 0.8819662026656653, 'learning_rate': 0.0001138511227908458, 'batch_size': 144, 'step_size': 1, 'gamma': 0.8882992838166675}. Best is trial 2 with value: 0.07080070120456926.[0m
[32m[I 2025-02-03 11:44:09,750][0m Trial 3 finished with value: 0.08364818386341397 and parameters: {'observation_period_num': 10, 'train_rates': 0.9034772275898424, 'learning_rate': 7.492808480160563e-06, 'batch_size': 153, 'step_size': 11, 'gamma': 0.9086167197445663}. Best is trial 2 with value: 0.07080070120456926.[0m
[32m[I 2025-02-03 11:44:30,459][0m Trial 4 finished with value: 0.2801472229520792 and parameters: {'observation_period_num': 227, 'train_rates': 0.6272198667994264, 'learning_rate': 9.352643739669791e-05, 'batch_size': 219, 'step_size': 4, 'gamma': 0.7507699507971286}. Best is trial 2 with value: 0.07080070120456926.[0m
[32m[I 2025-02-03 11:44:59,922][0m Trial 5 finished with value: 0.051804691553115845 and parameters: {'observation_period_num': 151, 'train_rates': 0.937000790189205, 'learning_rate': 0.0005009272225279347, 'batch_size': 228, 'step_size': 7, 'gamma': 0.7815688265249796}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:45:51,272][0m Trial 6 finished with value: 0.05551885680634846 and parameters: {'observation_period_num': 9, 'train_rates': 0.7811558025745545, 'learning_rate': 1.2706985502837104e-05, 'batch_size': 109, 'step_size': 5, 'gamma': 0.9374939489730155}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:48:09,138][0m Trial 7 finished with value: 0.11498725491141153 and parameters: {'observation_period_num': 95, 'train_rates': 0.6578560144391344, 'learning_rate': 1.352259589038161e-05, 'batch_size': 32, 'step_size': 13, 'gamma': 0.7935790071065308}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:48:54,945][0m Trial 8 finished with value: 0.18709148993738478 and parameters: {'observation_period_num': 93, 'train_rates': 0.8007842848680388, 'learning_rate': 5.8196780745616974e-06, 'batch_size': 117, 'step_size': 13, 'gamma': 0.8186525328080474}. Best is trial 5 with value: 0.051804691553115845.[0m
Early stopping at epoch 50
[32m[I 2025-02-03 11:49:09,921][0m Trial 9 finished with value: 0.5585222145401529 and parameters: {'observation_period_num': 164, 'train_rates': 0.7779229935828567, 'learning_rate': 1.5503353687752084e-05, 'batch_size': 191, 'step_size': 1, 'gamma': 0.7677252009227042}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:49:35,002][0m Trial 10 finished with value: 0.10872239619493484 and parameters: {'observation_period_num': 241, 'train_rates': 0.9858448291426154, 'learning_rate': 0.0004713786838276859, 'batch_size': 253, 'step_size': 9, 'gamma': 0.8412386514878749}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:50:30,005][0m Trial 11 finished with value: 0.1045124245547053 and parameters: {'observation_period_num': 138, 'train_rates': 0.7312609619045036, 'learning_rate': 0.0006021732290342211, 'batch_size': 89, 'step_size': 7, 'gamma': 0.9754997745604379}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:51:58,242][0m Trial 12 finished with value: 0.0722324401140213 and parameters: {'observation_period_num': 89, 'train_rates': 0.9818157252028357, 'learning_rate': 0.00012126118045161924, 'batch_size': 67, 'step_size': 7, 'gamma': 0.9433858723895989}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:52:43,111][0m Trial 13 finished with value: 0.45579909014671394 and parameters: {'observation_period_num': 191, 'train_rates': 0.7171281578071853, 'learning_rate': 1.2175155416008286e-06, 'batch_size': 106, 'step_size': 9, 'gamma': 0.8624510540587386}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:53:07,820][0m Trial 14 finished with value: 0.056644257158041 and parameters: {'observation_period_num': 130, 'train_rates': 0.9346935434418798, 'learning_rate': 0.0009418453072454649, 'batch_size': 255, 'step_size': 6, 'gamma': 0.9218421526759794}. Best is trial 5 with value: 0.051804691553115845.[0m
[32m[I 2025-02-03 11:53:43,262][0m Trial 15 finished with value: 0.037605643706636195 and parameters: {'observation_period_num': 54, 'train_rates': 0.8315265426594873, 'learning_rate': 0.0002946762849842374, 'batch_size': 169, 'step_size': 3, 'gamma': 0.9873098385211962}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:54:18,741][0m Trial 16 finished with value: 0.041211331420081224 and parameters: {'observation_period_num': 62, 'train_rates': 0.8403451580006113, 'learning_rate': 0.00023584715899846567, 'batch_size': 170, 'step_size': 3, 'gamma': 0.982159631423954}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:54:53,082][0m Trial 17 finished with value: 0.0473227733746171 and parameters: {'observation_period_num': 49, 'train_rates': 0.8349816935535389, 'learning_rate': 0.0002162162948086541, 'batch_size': 168, 'step_size': 3, 'gamma': 0.98854460888014}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:55:22,204][0m Trial 18 finished with value: 0.07994533856013777 and parameters: {'observation_period_num': 61, 'train_rates': 0.7288850961192256, 'learning_rate': 5.3192870798615644e-05, 'batch_size': 176, 'step_size': 2, 'gamma': 0.959442748136034}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:56:07,618][0m Trial 19 finished with value: 0.044402241633483566 and parameters: {'observation_period_num': 69, 'train_rates': 0.8417436903730989, 'learning_rate': 0.00023380986303954575, 'batch_size': 131, 'step_size': 15, 'gamma': 0.9620830905534586}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:56:43,016][0m Trial 20 finished with value: 0.05509530128772134 and parameters: {'observation_period_num': 113, 'train_rates': 0.8116668510153533, 'learning_rate': 0.00020949890141212912, 'batch_size': 159, 'step_size': 3, 'gamma': 0.8937704222947856}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:57:27,081][0m Trial 21 finished with value: 0.05032992238799731 and parameters: {'observation_period_num': 68, 'train_rates': 0.8639981554476808, 'learning_rate': 0.0002443482023018286, 'batch_size': 131, 'step_size': 15, 'gamma': 0.9626669167027462}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:58:07,586][0m Trial 22 finished with value: 0.03966973886724962 and parameters: {'observation_period_num': 35, 'train_rates': 0.8284223545840386, 'learning_rate': 5.7407790845517384e-05, 'batch_size': 136, 'step_size': 15, 'gamma': 0.9814444603246334}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:58:38,705][0m Trial 23 finished with value: 0.04942750813924286 and parameters: {'observation_period_num': 36, 'train_rates': 0.750276054083322, 'learning_rate': 4.267243602568649e-05, 'batch_size': 173, 'step_size': 11, 'gamma': 0.9886445656188396}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 11:59:57,115][0m Trial 24 finished with value: 0.040767320742209755 and parameters: {'observation_period_num': 32, 'train_rates': 0.901505723926198, 'learning_rate': 5.57146573241045e-05, 'batch_size': 75, 'step_size': 11, 'gamma': 0.9405627735475723}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 12:01:37,615][0m Trial 25 finished with value: 0.04155795055588609 and parameters: {'observation_period_num': 30, 'train_rates': 0.8963680687623552, 'learning_rate': 6.962071604836835e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.9383357833110655}. Best is trial 15 with value: 0.037605643706636195.[0m
[32m[I 2025-02-03 12:07:32,815][0m Trial 26 finished with value: 0.032837410855321256 and parameters: {'observation_period_num': 37, 'train_rates': 0.9273043589950117, 'learning_rate': 3.2505513379764245e-05, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9182270821106481}. Best is trial 26 with value: 0.032837410855321256.[0m
[32m[I 2025-02-03 12:09:39,630][0m Trial 27 finished with value: 0.053962498903274536 and parameters: {'observation_period_num': 82, 'train_rates': 0.9521635516737652, 'learning_rate': 2.4013522795789544e-05, 'batch_size': 46, 'step_size': 9, 'gamma': 0.9169608318420488}. Best is trial 26 with value: 0.032837410855321256.[0m
[32m[I 2025-02-03 12:14:06,027][0m Trial 28 finished with value: 0.04867819284963432 and parameters: {'observation_period_num': 45, 'train_rates': 0.6733809154159568, 'learning_rate': 3.4609403872776036e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.8619875370504315}. Best is trial 26 with value: 0.032837410855321256.[0m
[32m[I 2025-02-03 12:15:08,308][0m Trial 29 finished with value: 0.1676503113989465 and parameters: {'observation_period_num': 107, 'train_rates': 0.867286106503387, 'learning_rate': 4.24630451588269e-06, 'batch_size': 90, 'step_size': 10, 'gamma': 0.9632612808029818}. Best is trial 26 with value: 0.032837410855321256.[0m
[32m[I 2025-02-03 12:15:39,173][0m Trial 30 finished with value: 0.05912777580119468 and parameters: {'observation_period_num': 21, 'train_rates': 0.811012500994581, 'learning_rate': 2.064326890774387e-05, 'batch_size': 195, 'step_size': 12, 'gamma': 0.9501767140540587}. Best is trial 26 with value: 0.032837410855321256.[0m
[32m[I 2025-02-03 12:16:52,673][0m Trial 31 finished with value: 0.04029760777440361 and parameters: {'observation_period_num': 33, 'train_rates': 0.910288060690821, 'learning_rate': 7.64593598313073e-05, 'batch_size': 80, 'step_size': 11, 'gamma': 0.9265780283093198}. Best is trial 26 with value: 0.032837410855321256.[0m
[32m[I 2025-02-03 12:20:16,927][0m Trial 32 finished with value: 0.027683443389832973 and parameters: {'observation_period_num': 8, 'train_rates': 0.9145873943142342, 'learning_rate': 8.589780690975923e-05, 'batch_size': 28, 'step_size': 10, 'gamma': 0.8856955103403998}. Best is trial 32 with value: 0.027683443389832973.[0m
[32m[I 2025-02-03 12:26:20,055][0m Trial 33 finished with value: 0.024985027735274803 and parameters: {'observation_period_num': 5, 'train_rates': 0.9466960666321117, 'learning_rate': 0.00016883387664438806, 'batch_size': 16, 'step_size': 8, 'gamma': 0.8816513916236339}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:32:06,717][0m Trial 34 finished with value: 0.029560820159749087 and parameters: {'observation_period_num': 5, 'train_rates': 0.9603828629500907, 'learning_rate': 0.00012456831417121907, 'batch_size': 17, 'step_size': 8, 'gamma': 0.8798105311940971}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:37:14,978][0m Trial 35 finished with value: 0.03168156029035648 and parameters: {'observation_period_num': 5, 'train_rates': 0.9615646095645662, 'learning_rate': 0.00015328481441590289, 'batch_size': 19, 'step_size': 8, 'gamma': 0.8834577898326426}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:39:36,119][0m Trial 36 finished with value: 0.03394993102631053 and parameters: {'observation_period_num': 10, 'train_rates': 0.9624689368473484, 'learning_rate': 0.0001332548546954302, 'batch_size': 42, 'step_size': 8, 'gamma': 0.8816503489090816}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:42:53,890][0m Trial 37 finished with value: 0.03447585587424261 and parameters: {'observation_period_num': 5, 'train_rates': 0.9728499244509253, 'learning_rate': 0.00014895969984019073, 'batch_size': 30, 'step_size': 8, 'gamma': 0.8445310316832082}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:46:09,499][0m Trial 38 finished with value: 0.030329783551937696 and parameters: {'observation_period_num': 19, 'train_rates': 0.9487427611259273, 'learning_rate': 0.00036640221841751307, 'batch_size': 30, 'step_size': 6, 'gamma': 0.8995307254422072}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:47:57,467][0m Trial 39 finished with value: 0.030827738012632597 and parameters: {'observation_period_num': 20, 'train_rates': 0.9218489334001316, 'learning_rate': 0.00033500127019126603, 'batch_size': 54, 'step_size': 5, 'gamma': 0.9067150705709718}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:50:39,309][0m Trial 40 finished with value: 0.02791597110886138 and parameters: {'observation_period_num': 21, 'train_rates': 0.8872990394740468, 'learning_rate': 0.0006371359411167143, 'batch_size': 35, 'step_size': 6, 'gamma': 0.8989110288394642}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:53:37,385][0m Trial 41 finished with value: 0.02852092205353503 and parameters: {'observation_period_num': 20, 'train_rates': 0.9461000919155378, 'learning_rate': 0.0008749332141385886, 'batch_size': 33, 'step_size': 6, 'gamma': 0.9014877034031114}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:55:54,411][0m Trial 42 finished with value: 0.029023064189823346 and parameters: {'observation_period_num': 17, 'train_rates': 0.8912370297753713, 'learning_rate': 0.0009304514996543515, 'batch_size': 41, 'step_size': 6, 'gamma': 0.8719813659187299}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:58:28,351][0m Trial 43 finished with value: 0.02684225771447708 and parameters: {'observation_period_num': 19, 'train_rates': 0.8815350339585633, 'learning_rate': 0.0009516999535576408, 'batch_size': 36, 'step_size': 6, 'gamma': 0.8714857919028156}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 12:59:53,087][0m Trial 44 finished with value: 0.057069758639523856 and parameters: {'observation_period_num': 203, 'train_rates': 0.8758806831756796, 'learning_rate': 0.0006935446989153818, 'batch_size': 62, 'step_size': 5, 'gamma': 0.8437627523568185}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 13:02:41,907][0m Trial 45 finished with value: 0.053409363471620806 and parameters: {'observation_period_num': 78, 'train_rates': 0.9165298511738443, 'learning_rate': 0.0006420726088725218, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8562632796075276}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 13:04:38,572][0m Trial 46 finished with value: 0.037782891114599676 and parameters: {'observation_period_num': 47, 'train_rates': 0.9456325213865787, 'learning_rate': 0.00046962514707259866, 'batch_size': 50, 'step_size': 6, 'gamma': 0.8146784904708948}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 13:07:18,221][0m Trial 47 finished with value: 0.02846889873035252 and parameters: {'observation_period_num': 22, 'train_rates': 0.857058863978572, 'learning_rate': 0.0007950184046136566, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8984035698086363}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 13:08:47,556][0m Trial 48 finished with value: 0.030399481898971965 and parameters: {'observation_period_num': 26, 'train_rates': 0.8854069760623818, 'learning_rate': 0.00036121544876851667, 'batch_size': 64, 'step_size': 4, 'gamma': 0.8710071683053134}. Best is trial 33 with value: 0.024985027735274803.[0m
[32m[I 2025-02-03 13:11:01,755][0m Trial 49 finished with value: 0.04628150843922287 and parameters: {'observation_period_num': 164, 'train_rates': 0.8601625398222068, 'learning_rate': 0.0004800940924419032, 'batch_size': 39, 'step_size': 5, 'gamma': 0.8294439514240203}. Best is trial 33 with value: 0.024985027735274803.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_BA_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1220 | 0.0658
Epoch 2/300, Loss: 0.0820 | 0.0616
Epoch 3/300, Loss: 0.0715 | 0.0589
Epoch 4/300, Loss: 0.0635 | 0.0554
Epoch 5/300, Loss: 0.0550 | 0.0518
Epoch 6/300, Loss: 0.0476 | 0.0499
Epoch 7/300, Loss: 0.0441 | 0.0490
Epoch 8/300, Loss: 0.0418 | 0.0463
Epoch 9/300, Loss: 0.0410 | 0.0449
Epoch 10/300, Loss: 0.0397 | 0.0431
Epoch 11/300, Loss: 0.0379 | 0.0417
Epoch 12/300, Loss: 0.0377 | 0.0405
Epoch 13/300, Loss: 0.0383 | 0.0418
Epoch 14/300, Loss: 0.0369 | 0.0419
Epoch 15/300, Loss: 0.0339 | 0.0399
Epoch 16/300, Loss: 0.0328 | 0.0382
Epoch 17/300, Loss: 0.0318 | 0.0382
Epoch 18/300, Loss: 0.0309 | 0.0374
Epoch 19/300, Loss: 0.0301 | 0.0370
Epoch 20/300, Loss: 0.0295 | 0.0366
Epoch 21/300, Loss: 0.0288 | 0.0362
Epoch 22/300, Loss: 0.0282 | 0.0360
Epoch 23/300, Loss: 0.0277 | 0.0356
Epoch 24/300, Loss: 0.0273 | 0.0351
Epoch 25/300, Loss: 0.0269 | 0.0353
Epoch 26/300, Loss: 0.0270 | 0.0354
Epoch 27/300, Loss: 0.0281 | 0.0352
Epoch 28/300, Loss: 0.0271 | 0.0347
Epoch 29/300, Loss: 0.0282 | 0.0345
Epoch 30/300, Loss: 0.0266 | 0.0335
Epoch 31/300, Loss: 0.0258 | 0.0332
Epoch 32/300, Loss: 0.0248 | 0.0328
Epoch 33/300, Loss: 0.0244 | 0.0329
Epoch 34/300, Loss: 0.0245 | 0.0327
Epoch 35/300, Loss: 0.0245 | 0.0326
Epoch 36/300, Loss: 0.0238 | 0.0323
Epoch 37/300, Loss: 0.0235 | 0.0321
Epoch 38/300, Loss: 0.0239 | 0.0322
Epoch 39/300, Loss: 0.0236 | 0.0317
Epoch 40/300, Loss: 0.0226 | 0.0313
Epoch 41/300, Loss: 0.0223 | 0.0311
Epoch 42/300, Loss: 0.0225 | 0.0307
Epoch 43/300, Loss: 0.0221 | 0.0305
Epoch 44/300, Loss: 0.0216 | 0.0298
Epoch 45/300, Loss: 0.0214 | 0.0302
Epoch 46/300, Loss: 0.0222 | 0.0294
Epoch 47/300, Loss: 0.0219 | 0.0296
Epoch 48/300, Loss: 0.0217 | 0.0285
Epoch 49/300, Loss: 0.0212 | 0.0282
Epoch 50/300, Loss: 0.0209 | 0.0275
Epoch 51/300, Loss: 0.0204 | 0.0277
Epoch 52/300, Loss: 0.0202 | 0.0269
Epoch 53/300, Loss: 0.0199 | 0.0270
Epoch 54/300, Loss: 0.0194 | 0.0266
Epoch 55/300, Loss: 0.0192 | 0.0267
Epoch 56/300, Loss: 0.0190 | 0.0262
Epoch 57/300, Loss: 0.0188 | 0.0264
Epoch 58/300, Loss: 0.0185 | 0.0262
Epoch 59/300, Loss: 0.0184 | 0.0262
Epoch 60/300, Loss: 0.0182 | 0.0261
Epoch 61/300, Loss: 0.0181 | 0.0261
Epoch 62/300, Loss: 0.0179 | 0.0260
Epoch 63/300, Loss: 0.0178 | 0.0261
Epoch 64/300, Loss: 0.0177 | 0.0260
Epoch 65/300, Loss: 0.0176 | 0.0261
Epoch 66/300, Loss: 0.0174 | 0.0260
Epoch 67/300, Loss: 0.0174 | 0.0261
Epoch 68/300, Loss: 0.0173 | 0.0260
Epoch 69/300, Loss: 0.0172 | 0.0260
Epoch 70/300, Loss: 0.0171 | 0.0259
Epoch 71/300, Loss: 0.0170 | 0.0260
Epoch 72/300, Loss: 0.0169 | 0.0260
Epoch 73/300, Loss: 0.0169 | 0.0259
Epoch 74/300, Loss: 0.0168 | 0.0259
Epoch 75/300, Loss: 0.0167 | 0.0259
Epoch 76/300, Loss: 0.0166 | 0.0259
Epoch 77/300, Loss: 0.0165 | 0.0258
Epoch 78/300, Loss: 0.0165 | 0.0258
Epoch 79/300, Loss: 0.0164 | 0.0258
Epoch 80/300, Loss: 0.0163 | 0.0258
Epoch 81/300, Loss: 0.0163 | 0.0257
Epoch 82/300, Loss: 0.0162 | 0.0257
Epoch 83/300, Loss: 0.0162 | 0.0257
Epoch 84/300, Loss: 0.0161 | 0.0257
Epoch 85/300, Loss: 0.0160 | 0.0256
Epoch 86/300, Loss: 0.0160 | 0.0257
Epoch 87/300, Loss: 0.0159 | 0.0256
Epoch 88/300, Loss: 0.0159 | 0.0256
Epoch 89/300, Loss: 0.0158 | 0.0257
Epoch 90/300, Loss: 0.0158 | 0.0257
Epoch 91/300, Loss: 0.0158 | 0.0256
Epoch 92/300, Loss: 0.0157 | 0.0257
Epoch 93/300, Loss: 0.0157 | 0.0257
Epoch 94/300, Loss: 0.0156 | 0.0258
Epoch 95/300, Loss: 0.0156 | 0.0257
Epoch 96/300, Loss: 0.0155 | 0.0257
Epoch 97/300, Loss: 0.0155 | 0.0258
Epoch 98/300, Loss: 0.0155 | 0.0258
Epoch 99/300, Loss: 0.0154 | 0.0257
Epoch 100/300, Loss: 0.0154 | 0.0258
Epoch 101/300, Loss: 0.0154 | 0.0257
Epoch 102/300, Loss: 0.0154 | 0.0257
Epoch 103/300, Loss: 0.0154 | 0.0257
Epoch 104/300, Loss: 0.0154 | 0.0257
Epoch 105/300, Loss: 0.0155 | 0.0257
Epoch 106/300, Loss: 0.0156 | 0.0255
Epoch 107/300, Loss: 0.0158 | 0.0257
Epoch 108/300, Loss: 0.0160 | 0.0254
Epoch 109/300, Loss: 0.0156 | 0.0253
Epoch 110/300, Loss: 0.0154 | 0.0252
Epoch 111/300, Loss: 0.0153 | 0.0252
Epoch 112/300, Loss: 0.0152 | 0.0252
Epoch 113/300, Loss: 0.0151 | 0.0251
Epoch 114/300, Loss: 0.0151 | 0.0251
Epoch 115/300, Loss: 0.0151 | 0.0251
Epoch 116/300, Loss: 0.0150 | 0.0251
Epoch 117/300, Loss: 0.0150 | 0.0251
Epoch 118/300, Loss: 0.0150 | 0.0251
Epoch 119/300, Loss: 0.0150 | 0.0250
Epoch 120/300, Loss: 0.0149 | 0.0251
Epoch 121/300, Loss: 0.0149 | 0.0250
Epoch 122/300, Loss: 0.0149 | 0.0250
Epoch 123/300, Loss: 0.0149 | 0.0250
Epoch 124/300, Loss: 0.0149 | 0.0250
Epoch 125/300, Loss: 0.0149 | 0.0249
Epoch 126/300, Loss: 0.0148 | 0.0249
Epoch 127/300, Loss: 0.0148 | 0.0249
Epoch 128/300, Loss: 0.0148 | 0.0249
Epoch 129/300, Loss: 0.0148 | 0.0249
Epoch 130/300, Loss: 0.0148 | 0.0249
Epoch 131/300, Loss: 0.0148 | 0.0248
Epoch 132/300, Loss: 0.0147 | 0.0249
Epoch 133/300, Loss: 0.0147 | 0.0248
Epoch 134/300, Loss: 0.0147 | 0.0248
Epoch 135/300, Loss: 0.0147 | 0.0248
Epoch 136/300, Loss: 0.0147 | 0.0248
Epoch 137/300, Loss: 0.0147 | 0.0248
Epoch 138/300, Loss: 0.0147 | 0.0248
Epoch 139/300, Loss: 0.0146 | 0.0248
Epoch 140/300, Loss: 0.0146 | 0.0248
Epoch 141/300, Loss: 0.0146 | 0.0248
Epoch 142/300, Loss: 0.0146 | 0.0248
Epoch 143/300, Loss: 0.0146 | 0.0247
Epoch 144/300, Loss: 0.0146 | 0.0248
Epoch 145/300, Loss: 0.0146 | 0.0247
Epoch 146/300, Loss: 0.0146 | 0.0247
Epoch 147/300, Loss: 0.0146 | 0.0247
Epoch 148/300, Loss: 0.0146 | 0.0247
Epoch 149/300, Loss: 0.0145 | 0.0247
Epoch 150/300, Loss: 0.0145 | 0.0247
Epoch 151/300, Loss: 0.0145 | 0.0247
Epoch 152/300, Loss: 0.0145 | 0.0247
Epoch 153/300, Loss: 0.0145 | 0.0247
Epoch 154/300, Loss: 0.0145 | 0.0247
Epoch 155/300, Loss: 0.0145 | 0.0247
Epoch 156/300, Loss: 0.0145 | 0.0247
Epoch 157/300, Loss: 0.0145 | 0.0247
Epoch 158/300, Loss: 0.0145 | 0.0247
Epoch 159/300, Loss: 0.0145 | 0.0247
Epoch 160/300, Loss: 0.0145 | 0.0247
Epoch 161/300, Loss: 0.0145 | 0.0247
Epoch 162/300, Loss: 0.0145 | 0.0247
Epoch 163/300, Loss: 0.0145 | 0.0247
Epoch 164/300, Loss: 0.0144 | 0.0247
Epoch 165/300, Loss: 0.0144 | 0.0247
Epoch 166/300, Loss: 0.0144 | 0.0247
Epoch 167/300, Loss: 0.0144 | 0.0247
Epoch 168/300, Loss: 0.0144 | 0.0247
Epoch 169/300, Loss: 0.0144 | 0.0247
Epoch 170/300, Loss: 0.0144 | 0.0247
Epoch 171/300, Loss: 0.0144 | 0.0247
Epoch 172/300, Loss: 0.0144 | 0.0247
Epoch 173/300, Loss: 0.0144 | 0.0247
Epoch 174/300, Loss: 0.0144 | 0.0247
Epoch 175/300, Loss: 0.0144 | 0.0247
Epoch 176/300, Loss: 0.0144 | 0.0247
Epoch 177/300, Loss: 0.0144 | 0.0247
Epoch 178/300, Loss: 0.0144 | 0.0247
Epoch 179/300, Loss: 0.0144 | 0.0247
Epoch 180/300, Loss: 0.0144 | 0.0247
Epoch 181/300, Loss: 0.0144 | 0.0247
Epoch 182/300, Loss: 0.0144 | 0.0247
Epoch 183/300, Loss: 0.0144 | 0.0247
Epoch 184/300, Loss: 0.0144 | 0.0247
Epoch 185/300, Loss: 0.0144 | 0.0247
Epoch 186/300, Loss: 0.0144 | 0.0247
Epoch 187/300, Loss: 0.0144 | 0.0247
Epoch 188/300, Loss: 0.0144 | 0.0247
Epoch 189/300, Loss: 0.0143 | 0.0246
Epoch 190/300, Loss: 0.0143 | 0.0246
Epoch 191/300, Loss: 0.0143 | 0.0246
Epoch 192/300, Loss: 0.0143 | 0.0246
Epoch 193/300, Loss: 0.0143 | 0.0246
Epoch 194/300, Loss: 0.0143 | 0.0246
Epoch 195/300, Loss: 0.0143 | 0.0246
Epoch 196/300, Loss: 0.0143 | 0.0246
Epoch 197/300, Loss: 0.0143 | 0.0246
Epoch 198/300, Loss: 0.0143 | 0.0246
Epoch 199/300, Loss: 0.0143 | 0.0246
Epoch 200/300, Loss: 0.0143 | 0.0246
Epoch 201/300, Loss: 0.0143 | 0.0246
Epoch 202/300, Loss: 0.0143 | 0.0246
Epoch 203/300, Loss: 0.0143 | 0.0246
Epoch 204/300, Loss: 0.0143 | 0.0246
Epoch 205/300, Loss: 0.0143 | 0.0246
Epoch 206/300, Loss: 0.0143 | 0.0246
Epoch 207/300, Loss: 0.0143 | 0.0246
Epoch 208/300, Loss: 0.0143 | 0.0246
Epoch 209/300, Loss: 0.0143 | 0.0246
Epoch 210/300, Loss: 0.0143 | 0.0246
Epoch 211/300, Loss: 0.0143 | 0.0246
Epoch 212/300, Loss: 0.0143 | 0.0246
Epoch 213/300, Loss: 0.0143 | 0.0246
Epoch 214/300, Loss: 0.0143 | 0.0246
Epoch 215/300, Loss: 0.0143 | 0.0246
Epoch 216/300, Loss: 0.0143 | 0.0246
Epoch 217/300, Loss: 0.0143 | 0.0246
Epoch 218/300, Loss: 0.0143 | 0.0246
Epoch 219/300, Loss: 0.0143 | 0.0246
Epoch 220/300, Loss: 0.0143 | 0.0246
Epoch 221/300, Loss: 0.0143 | 0.0246
Epoch 222/300, Loss: 0.0143 | 0.0246
Epoch 223/300, Loss: 0.0143 | 0.0246
Epoch 224/300, Loss: 0.0143 | 0.0246
Epoch 225/300, Loss: 0.0143 | 0.0246
Epoch 226/300, Loss: 0.0143 | 0.0246
Epoch 227/300, Loss: 0.0143 | 0.0246
Epoch 228/300, Loss: 0.0143 | 0.0246
Epoch 229/300, Loss: 0.0143 | 0.0246
Epoch 230/300, Loss: 0.0143 | 0.0246
Epoch 231/300, Loss: 0.0143 | 0.0246
Epoch 232/300, Loss: 0.0143 | 0.0246
Epoch 233/300, Loss: 0.0143 | 0.0246
Epoch 234/300, Loss: 0.0143 | 0.0246
Epoch 235/300, Loss: 0.0143 | 0.0246
Epoch 236/300, Loss: 0.0143 | 0.0246
Epoch 237/300, Loss: 0.0143 | 0.0246
Epoch 238/300, Loss: 0.0143 | 0.0246
Epoch 239/300, Loss: 0.0143 | 0.0246
Epoch 240/300, Loss: 0.0143 | 0.0246
Epoch 241/300, Loss: 0.0143 | 0.0246
Epoch 242/300, Loss: 0.0143 | 0.0246
Epoch 243/300, Loss: 0.0143 | 0.0246
Epoch 244/300, Loss: 0.0143 | 0.0246
Epoch 245/300, Loss: 0.0143 | 0.0246
Epoch 246/300, Loss: 0.0143 | 0.0246
Epoch 247/300, Loss: 0.0143 | 0.0246
Epoch 248/300, Loss: 0.0143 | 0.0246
Epoch 249/300, Loss: 0.0143 | 0.0246
Epoch 250/300, Loss: 0.0143 | 0.0246
Epoch 251/300, Loss: 0.0143 | 0.0246
Epoch 252/300, Loss: 0.0143 | 0.0246
Epoch 253/300, Loss: 0.0143 | 0.0246
Epoch 254/300, Loss: 0.0143 | 0.0246
Epoch 255/300, Loss: 0.0143 | 0.0246
Epoch 256/300, Loss: 0.0143 | 0.0246
Epoch 257/300, Loss: 0.0143 | 0.0246
Epoch 258/300, Loss: 0.0143 | 0.0246
Epoch 259/300, Loss: 0.0143 | 0.0246
Epoch 260/300, Loss: 0.0143 | 0.0246
Epoch 261/300, Loss: 0.0143 | 0.0246
Epoch 262/300, Loss: 0.0143 | 0.0246
Epoch 263/300, Loss: 0.0143 | 0.0246
Epoch 264/300, Loss: 0.0143 | 0.0246
Epoch 265/300, Loss: 0.0143 | 0.0246
Epoch 266/300, Loss: 0.0143 | 0.0246
Epoch 267/300, Loss: 0.0143 | 0.0246
Epoch 268/300, Loss: 0.0143 | 0.0246
Epoch 269/300, Loss: 0.0143 | 0.0246
Epoch 270/300, Loss: 0.0143 | 0.0246
Epoch 271/300, Loss: 0.0143 | 0.0246
Epoch 272/300, Loss: 0.0143 | 0.0246
Epoch 273/300, Loss: 0.0143 | 0.0246
Epoch 274/300, Loss: 0.0143 | 0.0246
Epoch 275/300, Loss: 0.0143 | 0.0246
Epoch 276/300, Loss: 0.0143 | 0.0246
Epoch 277/300, Loss: 0.0143 | 0.0246
Epoch 278/300, Loss: 0.0143 | 0.0246
Epoch 279/300, Loss: 0.0143 | 0.0246
Epoch 280/300, Loss: 0.0143 | 0.0246
Epoch 281/300, Loss: 0.0143 | 0.0246
Epoch 282/300, Loss: 0.0143 | 0.0246
Epoch 283/300, Loss: 0.0143 | 0.0246
Epoch 284/300, Loss: 0.0143 | 0.0246
Epoch 285/300, Loss: 0.0143 | 0.0246
Epoch 286/300, Loss: 0.0143 | 0.0246
Epoch 287/300, Loss: 0.0143 | 0.0246
Epoch 288/300, Loss: 0.0143 | 0.0246
Epoch 289/300, Loss: 0.0143 | 0.0246
Epoch 290/300, Loss: 0.0143 | 0.0246
Epoch 291/300, Loss: 0.0143 | 0.0246
Epoch 292/300, Loss: 0.0143 | 0.0246
Epoch 293/300, Loss: 0.0143 | 0.0246
Epoch 294/300, Loss: 0.0143 | 0.0246
Epoch 295/300, Loss: 0.0143 | 0.0246
Epoch 296/300, Loss: 0.0143 | 0.0246
Epoch 297/300, Loss: 0.0143 | 0.0246
Epoch 298/300, Loss: 0.0143 | 0.0246
Epoch 299/300, Loss: 0.0143 | 0.0246
Epoch 300/300, Loss: 0.0143 | 0.0246
Runtime (seconds): 1098.6560702323914
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.448599066818133
RMSE: 1.8570404052734375
MAE: 1.8570404052734375
R-squared: nan
[153.14296]
