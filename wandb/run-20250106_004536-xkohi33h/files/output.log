[32m[I 2025-01-06 00:45:37,914][0m A new study created in memory with name: no-name-8e123c6e-4b4c-4a97-8cde-09c7d9677722[0m
[32m[I 2025-01-06 00:47:08,631][0m Trial 0 finished with value: 0.28592594496860463 and parameters: {'observation_period_num': 74, 'train_rates': 0.8288808166748455, 'learning_rate': 0.0006915069141638214, 'batch_size': 225, 'step_size': 11, 'gamma': 0.8455616404712318}. Best is trial 0 with value: 0.28592594496860463.[0m
[32m[I 2025-01-06 00:51:10,050][0m Trial 1 finished with value: 1.1847006683753007 and parameters: {'observation_period_num': 181, 'train_rates': 0.8339766383731317, 'learning_rate': 6.135497672959017e-06, 'batch_size': 203, 'step_size': 5, 'gamma': 0.7800092840114343}. Best is trial 0 with value: 0.28592594496860463.[0m
[32m[I 2025-01-06 00:53:00,417][0m Trial 2 finished with value: 0.33225235468462894 and parameters: {'observation_period_num': 90, 'train_rates': 0.8341438614625806, 'learning_rate': 0.00020062287983612083, 'batch_size': 251, 'step_size': 12, 'gamma': 0.7728601174976593}. Best is trial 0 with value: 0.28592594496860463.[0m
[32m[I 2025-01-06 00:55:25,453][0m Trial 3 finished with value: 0.1526052455107371 and parameters: {'observation_period_num': 33, 'train_rates': 0.9835690919388125, 'learning_rate': 6.465268390409248e-06, 'batch_size': 32, 'step_size': 8, 'gamma': 0.9590556053284676}. Best is trial 3 with value: 0.1526052455107371.[0m
[32m[I 2025-01-06 00:59:10,441][0m Trial 4 finished with value: 0.8698947415328943 and parameters: {'observation_period_num': 200, 'train_rates': 0.6220995055065096, 'learning_rate': 0.0003295562122128123, 'batch_size': 100, 'step_size': 3, 'gamma': 0.78630262519563}. Best is trial 3 with value: 0.1526052455107371.[0m
[32m[I 2025-01-06 01:00:09,443][0m Trial 5 finished with value: 1.4396647114931427 and parameters: {'observation_period_num': 57, 'train_rates': 0.6663642385115653, 'learning_rate': 1.4540634693088478e-06, 'batch_size': 198, 'step_size': 12, 'gamma': 0.9821808824496038}. Best is trial 3 with value: 0.1526052455107371.[0m
Early stopping at epoch 51
[32m[I 2025-01-06 01:02:34,685][0m Trial 6 finished with value: 1.34461483313012 and parameters: {'observation_period_num': 208, 'train_rates': 0.7892307527204208, 'learning_rate': 2.7866234966462195e-05, 'batch_size': 198, 'step_size': 1, 'gamma': 0.78716778378749}. Best is trial 3 with value: 0.1526052455107371.[0m
[32m[I 2025-01-06 01:06:47,150][0m Trial 7 finished with value: 0.3041057729395684 and parameters: {'observation_period_num': 186, 'train_rates': 0.8546301296985648, 'learning_rate': 8.641220531977519e-05, 'batch_size': 245, 'step_size': 10, 'gamma': 0.8386880715631367}. Best is trial 3 with value: 0.1526052455107371.[0m
[32m[I 2025-01-06 01:09:32,131][0m Trial 8 finished with value: 0.5959453266296014 and parameters: {'observation_period_num': 129, 'train_rates': 0.7828391067525895, 'learning_rate': 0.00026367290999119736, 'batch_size': 112, 'step_size': 7, 'gamma': 0.9719253123224956}. Best is trial 3 with value: 0.1526052455107371.[0m
[32m[I 2025-01-06 01:11:34,431][0m Trial 9 finished with value: 0.4094219218963891 and parameters: {'observation_period_num': 104, 'train_rates': 0.7853511290029008, 'learning_rate': 0.00026919810270208106, 'batch_size': 137, 'step_size': 4, 'gamma': 0.9557577134337237}. Best is trial 3 with value: 0.1526052455107371.[0m
[32m[I 2025-01-06 01:14:18,403][0m Trial 10 finished with value: 0.1381677763886971 and parameters: {'observation_period_num': 6, 'train_rates': 0.9658704267988064, 'learning_rate': 1.0787754560287205e-05, 'batch_size': 25, 'step_size': 15, 'gamma': 0.9030076335503762}. Best is trial 10 with value: 0.1381677763886971.[0m
[32m[I 2025-01-06 01:19:09,109][0m Trial 11 finished with value: 0.06623916590915006 and parameters: {'observation_period_num': 7, 'train_rates': 0.9829358199258026, 'learning_rate': 1.0112494880494356e-05, 'batch_size': 16, 'step_size': 14, 'gamma': 0.9228584456879079}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:21:59,879][0m Trial 12 finished with value: 0.0713527071411195 and parameters: {'observation_period_num': 11, 'train_rates': 0.9845321187060035, 'learning_rate': 1.4175738126106512e-05, 'batch_size': 27, 'step_size': 15, 'gamma': 0.9093833099453095}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:23:05,585][0m Trial 13 finished with value: 0.15734416029105583 and parameters: {'observation_period_num': 14, 'train_rates': 0.9348396969223789, 'learning_rate': 2.8673753215952766e-05, 'batch_size': 67, 'step_size': 15, 'gamma': 0.9086229468249033}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:29:17,362][0m Trial 14 finished with value: 0.5979837167587726 and parameters: {'observation_period_num': 245, 'train_rates': 0.9050986159052814, 'learning_rate': 1.5171990987565833e-06, 'batch_size': 58, 'step_size': 14, 'gamma': 0.9053187615210403}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:33:33,276][0m Trial 15 finished with value: 0.15064637076950843 and parameters: {'observation_period_num': 44, 'train_rates': 0.8937006907429067, 'learning_rate': 1.249419828703474e-05, 'batch_size': 17, 'step_size': 13, 'gamma': 0.9242112319945355}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:36:22,630][0m Trial 16 finished with value: 1.045320729500261 and parameters: {'observation_period_num': 139, 'train_rates': 0.7115560427041172, 'learning_rate': 3.421732925929228e-06, 'batch_size': 63, 'step_size': 10, 'gamma': 0.8667768354788519}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:37:06,708][0m Trial 17 finished with value: 0.18217994645237923 and parameters: {'observation_period_num': 31, 'train_rates': 0.9372512601025866, 'learning_rate': 7.220975413007632e-05, 'batch_size': 162, 'step_size': 15, 'gamma': 0.9362569045821558}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:38:34,403][0m Trial 18 finished with value: 0.20069709420204163 and parameters: {'observation_period_num': 62, 'train_rates': 0.9796173224833558, 'learning_rate': 1.5419035399047066e-05, 'batch_size': 84, 'step_size': 8, 'gamma': 0.8801952668743694}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:40:38,663][0m Trial 19 finished with value: 0.1590140629795533 and parameters: {'observation_period_num': 86, 'train_rates': 0.8897859236130652, 'learning_rate': 6.401389090215019e-05, 'batch_size': 48, 'step_size': 13, 'gamma': 0.8204577684904594}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:43:38,140][0m Trial 20 finished with value: 1.2295345510827023 and parameters: {'observation_period_num': 147, 'train_rates': 0.7464808021959852, 'learning_rate': 2.9165409562824362e-06, 'batch_size': 155, 'step_size': 10, 'gamma': 0.8820289400283614}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:47:55,102][0m Trial 21 finished with value: 0.12922795204555287 and parameters: {'observation_period_num': 7, 'train_rates': 0.9539791978374356, 'learning_rate': 1.0589431973837922e-05, 'batch_size': 16, 'step_size': 15, 'gamma': 0.9021440950653185}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:49:55,471][0m Trial 22 finished with value: 0.12857127176565036 and parameters: {'observation_period_num': 6, 'train_rates': 0.9421382910286292, 'learning_rate': 2.061519072810815e-05, 'batch_size': 33, 'step_size': 14, 'gamma': 0.9319221300623262}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:51:30,284][0m Trial 23 finished with value: 0.16409089856527068 and parameters: {'observation_period_num': 28, 'train_rates': 0.9175154284499024, 'learning_rate': 2.072383583784146e-05, 'batch_size': 41, 'step_size': 13, 'gamma': 0.9273249574636756}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:52:44,571][0m Trial 24 finished with value: 0.10872233659029007 and parameters: {'observation_period_num': 50, 'train_rates': 0.9888732626564484, 'learning_rate': 5.069057804286277e-05, 'batch_size': 89, 'step_size': 14, 'gamma': 0.9446395074998486}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:53:50,206][0m Trial 25 finished with value: 0.10443191230297089 and parameters: {'observation_period_num': 43, 'train_rates': 0.9870130289089298, 'learning_rate': 4.768009572867485e-05, 'batch_size': 83, 'step_size': 12, 'gamma': 0.9525196308824095}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:56:13,396][0m Trial 26 finished with value: 0.2786282795984872 and parameters: {'observation_period_num': 108, 'train_rates': 0.8747302280554061, 'learning_rate': 0.00013993271161820734, 'batch_size': 74, 'step_size': 12, 'gamma': 0.9633923266402189}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:56:54,312][0m Trial 27 finished with value: 0.1822796860273848 and parameters: {'observation_period_num': 27, 'train_rates': 0.9199576050885343, 'learning_rate': 4.467554091530269e-05, 'batch_size': 130, 'step_size': 11, 'gamma': 0.9147253949294772}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 01:58:34,808][0m Trial 28 finished with value: 0.27347274895372065 and parameters: {'observation_period_num': 64, 'train_rates': 0.9599705204503759, 'learning_rate': 6.1025695313260886e-06, 'batch_size': 47, 'step_size': 9, 'gamma': 0.9880735822466359}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:00:15,474][0m Trial 29 finished with value: 1.0151387930540514 and parameters: {'observation_period_num': 75, 'train_rates': 0.8672376702635762, 'learning_rate': 0.0009649028579142868, 'batch_size': 107, 'step_size': 11, 'gamma': 0.8533293918661682}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:01:34,175][0m Trial 30 finished with value: 0.15723148132524184 and parameters: {'observation_period_num': 39, 'train_rates': 0.9574335863960879, 'learning_rate': 0.00010788625074157169, 'batch_size': 56, 'step_size': 6, 'gamma': 0.888012741037019}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:02:47,408][0m Trial 31 finished with value: 0.10787436366081238 and parameters: {'observation_period_num': 50, 'train_rates': 0.9861656293216096, 'learning_rate': 4.6311405242383125e-05, 'batch_size': 88, 'step_size': 14, 'gamma': 0.9448001758173314}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:03:45,060][0m Trial 32 finished with value: 0.16478804687222282 and parameters: {'observation_period_num': 27, 'train_rates': 0.9689408592030356, 'learning_rate': 3.661788849567758e-05, 'batch_size': 82, 'step_size': 14, 'gamma': 0.9534897981303263}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:04:25,644][0m Trial 33 finished with value: 0.2769645154476166 and parameters: {'observation_period_num': 17, 'train_rates': 0.9886743250071874, 'learning_rate': 6.655506761342419e-06, 'batch_size': 120, 'step_size': 13, 'gamma': 0.9425022478577182}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:05:34,924][0m Trial 34 finished with value: 0.21478723670755115 and parameters: {'observation_period_num': 49, 'train_rates': 0.9279897410465504, 'learning_rate': 1.5724751287726174e-05, 'batch_size': 93, 'step_size': 12, 'gamma': 0.9745476440504192}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:07:40,136][0m Trial 35 finished with value: 0.45728790760040283 and parameters: {'observation_period_num': 79, 'train_rates': 0.8286610276339066, 'learning_rate': 3.7773970451060585e-06, 'batch_size': 34, 'step_size': 14, 'gamma': 0.9238789055917577}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:09:57,755][0m Trial 36 finished with value: 0.1455313265323639 and parameters: {'observation_period_num': 99, 'train_rates': 0.9883681665364552, 'learning_rate': 0.00043826101696997714, 'batch_size': 150, 'step_size': 12, 'gamma': 0.9478624238285343}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:11:41,169][0m Trial 37 finished with value: 0.16777375588814417 and parameters: {'observation_period_num': 66, 'train_rates': 0.952197911052289, 'learning_rate': 0.0001469210427449911, 'batch_size': 49, 'step_size': 15, 'gamma': 0.8926396833951329}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:12:14,817][0m Trial 38 finished with value: 0.38167900693812495 and parameters: {'observation_period_num': 22, 'train_rates': 0.9076397436786922, 'learning_rate': 7.588010809688484e-06, 'batch_size': 184, 'step_size': 13, 'gamma': 0.9683824957012154}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:14:37,127][0m Trial 39 finished with value: 0.15845234266349248 and parameters: {'observation_period_num': 42, 'train_rates': 0.9639493468579456, 'learning_rate': 2.306424695353949e-05, 'batch_size': 30, 'step_size': 11, 'gamma': 0.8671425819744216}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:15:38,676][0m Trial 40 finished with value: 0.9986629580909555 and parameters: {'observation_period_num': 55, 'train_rates': 0.6431413123373891, 'learning_rate': 3.7396167044562935e-05, 'batch_size': 75, 'step_size': 1, 'gamma': 0.9176197260549672}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:16:51,471][0m Trial 41 finished with value: 0.12728658318519592 and parameters: {'observation_period_num': 49, 'train_rates': 0.989965795250791, 'learning_rate': 4.788133019539095e-05, 'batch_size': 91, 'step_size': 14, 'gamma': 0.7549471029183792}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:17:47,762][0m Trial 42 finished with value: 0.11817935854196548 and parameters: {'observation_period_num': 37, 'train_rates': 0.9761599462875297, 'learning_rate': 6.089647335688216e-05, 'batch_size': 104, 'step_size': 14, 'gamma': 0.9448471175349724}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:21:46,397][0m Trial 43 finished with value: 0.18760256844245835 and parameters: {'observation_period_num': 162, 'train_rates': 0.9460204519855558, 'learning_rate': 5.395219897857525e-05, 'batch_size': 70, 'step_size': 15, 'gamma': 0.9378258993403942}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:22:23,635][0m Trial 44 finished with value: 0.23311327570091092 and parameters: {'observation_period_num': 19, 'train_rates': 0.8231521725222044, 'learning_rate': 8.677558748788781e-05, 'batch_size': 114, 'step_size': 13, 'gamma': 0.955029145992951}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:25:06,112][0m Trial 45 finished with value: 0.17885658144950867 and parameters: {'observation_period_num': 117, 'train_rates': 0.9733467869506984, 'learning_rate': 2.8357414307728966e-05, 'batch_size': 129, 'step_size': 12, 'gamma': 0.9788602319326768}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:25:51,581][0m Trial 46 finished with value: 0.2793883586802134 and parameters: {'observation_period_num': 15, 'train_rates': 0.9303140469322296, 'learning_rate': 9.287595703679521e-06, 'batch_size': 97, 'step_size': 14, 'gamma': 0.8971592743834637}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:28:20,052][0m Trial 47 finished with value: 0.5949287830464115 and parameters: {'observation_period_num': 85, 'train_rates': 0.720348688525978, 'learning_rate': 4.9300180578877574e-06, 'batch_size': 23, 'step_size': 15, 'gamma': 0.9150643454753106}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:30:10,785][0m Trial 48 finished with value: 0.15359891951084137 and parameters: {'observation_period_num': 71, 'train_rates': 0.9897672168456062, 'learning_rate': 1.6085536367256866e-05, 'batch_size': 59, 'step_size': 3, 'gamma': 0.9628003760715179}. Best is trial 11 with value: 0.06623916590915006.[0m
[32m[I 2025-01-06 02:31:27,604][0m Trial 49 finished with value: 0.18580489381690699 and parameters: {'observation_period_num': 54, 'train_rates': 0.9437878115512263, 'learning_rate': 0.0001706919235067614, 'batch_size': 85, 'step_size': 13, 'gamma': 0.9321730223005491}. Best is trial 11 with value: 0.06623916590915006.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.8196 | 0.8990
Epoch 2/300, Loss: 0.5240 | 0.6687
Epoch 3/300, Loss: 0.4176 | 0.5752
Epoch 4/300, Loss: 0.3548 | 0.5070
Epoch 5/300, Loss: 0.3099 | 0.4509
Epoch 6/300, Loss: 0.2773 | 0.4037
Epoch 7/300, Loss: 0.2529 | 0.3657
Epoch 8/300, Loss: 0.2359 | 0.3372
Epoch 9/300, Loss: 0.2215 | 0.3129
Epoch 10/300, Loss: 0.2097 | 0.2928
Epoch 11/300, Loss: 0.2014 | 0.2731
Epoch 12/300, Loss: 0.1937 | 0.2587
Epoch 13/300, Loss: 0.1861 | 0.2441
Epoch 14/300, Loss: 0.1806 | 0.2320
Epoch 15/300, Loss: 0.1749 | 0.2235
Epoch 16/300, Loss: 0.1707 | 0.2138
Epoch 17/300, Loss: 0.1667 | 0.2050
Epoch 18/300, Loss: 0.1622 | 0.1978
Epoch 19/300, Loss: 0.1595 | 0.1906
Epoch 20/300, Loss: 0.1558 | 0.1855
Epoch 21/300, Loss: 0.1525 | 0.1776
Epoch 22/300, Loss: 0.1500 | 0.1722
Epoch 23/300, Loss: 0.1472 | 0.1679
Epoch 24/300, Loss: 0.1453 | 0.1615
Epoch 25/300, Loss: 0.1429 | 0.1576
Epoch 26/300, Loss: 0.1393 | 0.1524
Epoch 27/300, Loss: 0.1380 | 0.1488
Epoch 28/300, Loss: 0.1363 | 0.1448
Epoch 29/300, Loss: 0.1346 | 0.1411
Epoch 30/300, Loss: 0.1340 | 0.1382
Epoch 31/300, Loss: 0.1317 | 0.1348
Epoch 32/300, Loss: 0.1305 | 0.1312
Epoch 33/300, Loss: 0.1285 | 0.1280
Epoch 34/300, Loss: 0.1276 | 0.1263
Epoch 35/300, Loss: 0.1254 | 0.1236
Epoch 36/300, Loss: 0.1243 | 0.1212
Epoch 37/300, Loss: 0.1226 | 0.1190
Epoch 38/300, Loss: 0.1213 | 0.1165
Epoch 39/300, Loss: 0.1211 | 0.1148
Epoch 40/300, Loss: 0.1199 | 0.1131
Epoch 41/300, Loss: 0.1187 | 0.1104
Epoch 42/300, Loss: 0.1176 | 0.1083
Epoch 43/300, Loss: 0.1160 | 0.1069
Epoch 44/300, Loss: 0.1153 | 0.1047
Epoch 45/300, Loss: 0.1139 | 0.1046
Epoch 46/300, Loss: 0.1134 | 0.1019
Epoch 47/300, Loss: 0.1126 | 0.1002
Epoch 48/300, Loss: 0.1115 | 0.0990
Epoch 49/300, Loss: 0.1116 | 0.0980
Epoch 50/300, Loss: 0.1101 | 0.0969
Epoch 51/300, Loss: 0.1093 | 0.0946
Epoch 52/300, Loss: 0.1083 | 0.0941
Epoch 53/300, Loss: 0.1072 | 0.0934
Epoch 54/300, Loss: 0.1076 | 0.0919
Epoch 55/300, Loss: 0.1059 | 0.0911
Epoch 56/300, Loss: 0.1052 | 0.0902
Epoch 57/300, Loss: 0.1049 | 0.0892
Epoch 58/300, Loss: 0.1042 | 0.0881
Epoch 59/300, Loss: 0.1037 | 0.0875
Epoch 60/300, Loss: 0.1033 | 0.0859
Epoch 61/300, Loss: 0.1021 | 0.0850
Epoch 62/300, Loss: 0.1020 | 0.0845
Epoch 63/300, Loss: 0.1012 | 0.0842
Epoch 64/300, Loss: 0.1007 | 0.0833
Epoch 65/300, Loss: 0.1001 | 0.0826
Epoch 66/300, Loss: 0.0997 | 0.0828
Epoch 67/300, Loss: 0.0987 | 0.0816
Epoch 68/300, Loss: 0.0987 | 0.0805
Epoch 69/300, Loss: 0.0984 | 0.0801
Epoch 70/300, Loss: 0.0966 | 0.0800
Epoch 71/300, Loss: 0.0971 | 0.0793
Epoch 72/300, Loss: 0.0965 | 0.0788
Epoch 73/300, Loss: 0.0964 | 0.0784
Epoch 74/300, Loss: 0.0964 | 0.0777
Epoch 75/300, Loss: 0.0953 | 0.0772
Epoch 76/300, Loss: 0.0953 | 0.0765
Epoch 77/300, Loss: 0.0943 | 0.0764
Epoch 78/300, Loss: 0.0942 | 0.0761
Epoch 79/300, Loss: 0.0940 | 0.0749
Epoch 80/300, Loss: 0.0930 | 0.0758
Epoch 81/300, Loss: 0.0931 | 0.0748
Epoch 82/300, Loss: 0.0925 | 0.0745
Epoch 83/300, Loss: 0.0924 | 0.0743
Epoch 84/300, Loss: 0.0913 | 0.0739
Epoch 85/300, Loss: 0.0911 | 0.0735
Epoch 86/300, Loss: 0.0911 | 0.0726
Epoch 87/300, Loss: 0.0909 | 0.0727
Epoch 88/300, Loss: 0.0908 | 0.0721
Epoch 89/300, Loss: 0.0895 | 0.0723
Epoch 90/300, Loss: 0.0896 | 0.0719
Epoch 91/300, Loss: 0.0898 | 0.0714
Epoch 92/300, Loss: 0.0891 | 0.0707
Epoch 93/300, Loss: 0.0888 | 0.0710
Epoch 94/300, Loss: 0.0889 | 0.0706
Epoch 95/300, Loss: 0.0884 | 0.0706
Epoch 96/300, Loss: 0.0881 | 0.0703
Epoch 97/300, Loss: 0.0879 | 0.0696
Epoch 98/300, Loss: 0.0880 | 0.0697
Epoch 99/300, Loss: 0.0875 | 0.0700
Epoch 100/300, Loss: 0.0876 | 0.0691
Epoch 101/300, Loss: 0.0869 | 0.0691
Epoch 102/300, Loss: 0.0859 | 0.0687
Epoch 103/300, Loss: 0.0862 | 0.0692
Epoch 104/300, Loss: 0.0862 | 0.0686
Epoch 105/300, Loss: 0.0862 | 0.0677
Epoch 106/300, Loss: 0.0854 | 0.0675
Epoch 107/300, Loss: 0.0855 | 0.0679
Epoch 108/300, Loss: 0.0856 | 0.0675
Epoch 109/300, Loss: 0.0847 | 0.0682
Epoch 110/300, Loss: 0.0849 | 0.0672
Epoch 111/300, Loss: 0.0847 | 0.0674
Epoch 112/300, Loss: 0.0841 | 0.0666
Epoch 113/300, Loss: 0.0844 | 0.0662
Epoch 114/300, Loss: 0.0839 | 0.0660
Epoch 115/300, Loss: 0.0837 | 0.0657
Epoch 116/300, Loss: 0.0836 | 0.0661
Epoch 117/300, Loss: 0.0831 | 0.0656
Epoch 118/300, Loss: 0.0833 | 0.0658
Epoch 119/300, Loss: 0.0828 | 0.0655
Epoch 120/300, Loss: 0.0831 | 0.0655
Epoch 121/300, Loss: 0.0826 | 0.0651
Epoch 122/300, Loss: 0.0822 | 0.0651
Epoch 123/300, Loss: 0.0827 | 0.0650
Epoch 124/300, Loss: 0.0819 | 0.0652
Epoch 125/300, Loss: 0.0820 | 0.0646
Epoch 126/300, Loss: 0.0822 | 0.0643
Epoch 127/300, Loss: 0.0818 | 0.0645
Epoch 128/300, Loss: 0.0817 | 0.0645
Epoch 129/300, Loss: 0.0815 | 0.0651
Epoch 130/300, Loss: 0.0816 | 0.0643
Epoch 131/300, Loss: 0.0808 | 0.0640
Epoch 132/300, Loss: 0.0812 | 0.0640
Epoch 133/300, Loss: 0.0804 | 0.0642
Epoch 134/300, Loss: 0.0807 | 0.0639
Epoch 135/300, Loss: 0.0807 | 0.0639
Epoch 136/300, Loss: 0.0807 | 0.0636
Epoch 137/300, Loss: 0.0808 | 0.0633
Epoch 138/300, Loss: 0.0802 | 0.0639
Epoch 139/300, Loss: 0.0795 | 0.0634
Epoch 140/300, Loss: 0.0804 | 0.0633
Epoch 141/300, Loss: 0.0794 | 0.0624
Epoch 142/300, Loss: 0.0802 | 0.0630
Epoch 143/300, Loss: 0.0793 | 0.0627
Epoch 144/300, Loss: 0.0790 | 0.0629
Epoch 145/300, Loss: 0.0790 | 0.0628
Epoch 146/300, Loss: 0.0795 | 0.0627
Epoch 147/300, Loss: 0.0795 | 0.0622
Epoch 148/300, Loss: 0.0784 | 0.0622
Epoch 149/300, Loss: 0.0785 | 0.0627
Epoch 150/300, Loss: 0.0793 | 0.0619
Epoch 151/300, Loss: 0.0786 | 0.0620
Epoch 152/300, Loss: 0.0786 | 0.0620
Epoch 153/300, Loss: 0.0785 | 0.0619
Epoch 154/300, Loss: 0.0783 | 0.0616
Epoch 155/300, Loss: 0.0779 | 0.0617
Epoch 156/300, Loss: 0.0786 | 0.0619
Epoch 157/300, Loss: 0.0782 | 0.0619
Epoch 158/300, Loss: 0.0783 | 0.0616
Epoch 159/300, Loss: 0.0775 | 0.0616
Epoch 160/300, Loss: 0.0771 | 0.0614
Epoch 161/300, Loss: 0.0779 | 0.0611
Epoch 162/300, Loss: 0.0769 | 0.0613
Epoch 163/300, Loss: 0.0777 | 0.0610
Epoch 164/300, Loss: 0.0769 | 0.0611
Epoch 165/300, Loss: 0.0781 | 0.0611
Epoch 166/300, Loss: 0.0773 | 0.0609
Epoch 167/300, Loss: 0.0770 | 0.0608
Epoch 168/300, Loss: 0.0768 | 0.0609
Epoch 169/300, Loss: 0.0771 | 0.0606
Epoch 170/300, Loss: 0.0768 | 0.0607
Epoch 171/300, Loss: 0.0769 | 0.0606
Epoch 172/300, Loss: 0.0766 | 0.0605
Epoch 173/300, Loss: 0.0763 | 0.0604
Epoch 174/300, Loss: 0.0768 | 0.0604
Epoch 175/300, Loss: 0.0770 | 0.0606
Epoch 176/300, Loss: 0.0767 | 0.0599
Epoch 177/300, Loss: 0.0770 | 0.0600
Epoch 178/300, Loss: 0.0765 | 0.0601
Epoch 179/300, Loss: 0.0763 | 0.0601
Epoch 180/300, Loss: 0.0758 | 0.0600
Epoch 181/300, Loss: 0.0761 | 0.0601
Epoch 182/300, Loss: 0.0762 | 0.0598
Epoch 183/300, Loss: 0.0763 | 0.0597
Epoch 184/300, Loss: 0.0760 | 0.0601
Epoch 185/300, Loss: 0.0760 | 0.0599
Epoch 186/300, Loss: 0.0762 | 0.0597
Epoch 187/300, Loss: 0.0758 | 0.0597
Epoch 188/300, Loss: 0.0756 | 0.0598
Epoch 189/300, Loss: 0.0756 | 0.0602
Epoch 190/300, Loss: 0.0758 | 0.0600
Epoch 191/300, Loss: 0.0756 | 0.0598
Epoch 192/300, Loss: 0.0755 | 0.0595
Epoch 193/300, Loss: 0.0754 | 0.0595
Epoch 194/300, Loss: 0.0750 | 0.0596
Epoch 195/300, Loss: 0.0752 | 0.0594
Epoch 196/300, Loss: 0.0754 | 0.0593
Epoch 197/300, Loss: 0.0745 | 0.0593
Epoch 198/300, Loss: 0.0745 | 0.0593
Epoch 199/300, Loss: 0.0750 | 0.0596
Epoch 200/300, Loss: 0.0749 | 0.0592
Epoch 201/300, Loss: 0.0751 | 0.0591
Epoch 202/300, Loss: 0.0746 | 0.0590
Epoch 203/300, Loss: 0.0748 | 0.0591
Epoch 204/300, Loss: 0.0747 | 0.0593
Epoch 205/300, Loss: 0.0747 | 0.0590
Epoch 206/300, Loss: 0.0744 | 0.0591
Epoch 207/300, Loss: 0.0745 | 0.0588
Epoch 208/300, Loss: 0.0749 | 0.0590
Epoch 209/300, Loss: 0.0744 | 0.0589
Epoch 210/300, Loss: 0.0747 | 0.0587
Epoch 211/300, Loss: 0.0743 | 0.0588
Epoch 212/300, Loss: 0.0733 | 0.0590
Epoch 213/300, Loss: 0.0739 | 0.0588
Epoch 214/300, Loss: 0.0746 | 0.0585
Epoch 215/300, Loss: 0.0747 | 0.0586
Epoch 216/300, Loss: 0.0742 | 0.0589
Epoch 217/300, Loss: 0.0738 | 0.0587
Epoch 218/300, Loss: 0.0740 | 0.0586
Epoch 219/300, Loss: 0.0741 | 0.0586
Epoch 220/300, Loss: 0.0740 | 0.0585
Epoch 221/300, Loss: 0.0737 | 0.0586
Epoch 222/300, Loss: 0.0739 | 0.0586
Epoch 223/300, Loss: 0.0738 | 0.0588
Epoch 224/300, Loss: 0.0737 | 0.0587
Epoch 225/300, Loss: 0.0738 | 0.0586
Epoch 226/300, Loss: 0.0736 | 0.0586
Epoch 227/300, Loss: 0.0736 | 0.0584
Epoch 228/300, Loss: 0.0734 | 0.0583
Epoch 229/300, Loss: 0.0736 | 0.0584
Epoch 230/300, Loss: 0.0734 | 0.0584
Epoch 231/300, Loss: 0.0737 | 0.0583
Epoch 232/300, Loss: 0.0740 | 0.0581
Epoch 233/300, Loss: 0.0733 | 0.0583
Epoch 234/300, Loss: 0.0730 | 0.0583
Epoch 235/300, Loss: 0.0735 | 0.0581
Epoch 236/300, Loss: 0.0732 | 0.0580
Epoch 237/300, Loss: 0.0732 | 0.0583
Epoch 238/300, Loss: 0.0734 | 0.0580
Epoch 239/300, Loss: 0.0732 | 0.0584
Epoch 240/300, Loss: 0.0730 | 0.0580
Epoch 241/300, Loss: 0.0727 | 0.0581
Epoch 242/300, Loss: 0.0735 | 0.0579
Epoch 243/300, Loss: 0.0730 | 0.0580
Epoch 244/300, Loss: 0.0733 | 0.0579
Epoch 245/300, Loss: 0.0730 | 0.0579
Epoch 246/300, Loss: 0.0735 | 0.0580
Epoch 247/300, Loss: 0.0732 | 0.0579
Epoch 248/300, Loss: 0.0729 | 0.0579
Epoch 249/300, Loss: 0.0727 | 0.0579
Epoch 250/300, Loss: 0.0733 | 0.0580
Epoch 251/300, Loss: 0.0727 | 0.0578
Epoch 252/300, Loss: 0.0729 | 0.0580
Epoch 253/300, Loss: 0.0730 | 0.0578
Epoch 254/300, Loss: 0.0727 | 0.0578
Epoch 255/300, Loss: 0.0730 | 0.0579
Epoch 256/300, Loss: 0.0726 | 0.0577
Epoch 257/300, Loss: 0.0732 | 0.0576
Epoch 258/300, Loss: 0.0724 | 0.0578
Epoch 259/300, Loss: 0.0723 | 0.0580
Epoch 260/300, Loss: 0.0730 | 0.0577
Epoch 261/300, Loss: 0.0725 | 0.0577
Epoch 262/300, Loss: 0.0732 | 0.0577
Epoch 263/300, Loss: 0.0729 | 0.0577
Epoch 264/300, Loss: 0.0724 | 0.0576
Epoch 265/300, Loss: 0.0732 | 0.0577
Epoch 266/300, Loss: 0.0726 | 0.0575
Epoch 267/300, Loss: 0.0725 | 0.0575
Epoch 268/300, Loss: 0.0726 | 0.0575
Epoch 269/300, Loss: 0.0724 | 0.0575
Epoch 270/300, Loss: 0.0725 | 0.0576
Epoch 271/300, Loss: 0.0722 | 0.0575
Epoch 272/300, Loss: 0.0728 | 0.0575
Epoch 273/300, Loss: 0.0727 | 0.0575
Epoch 274/300, Loss: 0.0724 | 0.0576
Epoch 275/300, Loss: 0.0727 | 0.0576
Epoch 276/300, Loss: 0.0723 | 0.0575
Epoch 277/300, Loss: 0.0721 | 0.0574
Epoch 278/300, Loss: 0.0734 | 0.0575
Epoch 279/300, Loss: 0.0725 | 0.0575
Epoch 280/300, Loss: 0.0729 | 0.0573
Epoch 281/300, Loss: 0.0721 | 0.0572
Epoch 282/300, Loss: 0.0726 | 0.0573
Epoch 283/300, Loss: 0.0728 | 0.0573
Epoch 284/300, Loss: 0.0719 | 0.0573
Epoch 285/300, Loss: 0.0718 | 0.0572
Epoch 286/300, Loss: 0.0718 | 0.0573
Epoch 287/300, Loss: 0.0719 | 0.0574
Epoch 288/300, Loss: 0.0721 | 0.0573
Epoch 289/300, Loss: 0.0723 | 0.0573
Epoch 290/300, Loss: 0.0719 | 0.0573
Epoch 291/300, Loss: 0.0716 | 0.0572
Epoch 292/300, Loss: 0.0717 | 0.0574
Epoch 293/300, Loss: 0.0722 | 0.0573
Epoch 294/300, Loss: 0.0716 | 0.0571
Epoch 295/300, Loss: 0.0717 | 0.0571
Epoch 296/300, Loss: 0.0721 | 0.0570
Epoch 297/300, Loss: 0.0718 | 0.0571
Epoch 298/300, Loss: 0.0722 | 0.0571
Epoch 299/300, Loss: 0.0717 | 0.0570
Epoch 300/300, Loss: 0.0716 | 0.0572
Runtime (seconds): 823.529887676239
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 30.721457795239985
RMSE: 5.542694091796875
MAE: 5.542694091796875
R-squared: nan
[229.5173]
