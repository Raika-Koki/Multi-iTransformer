ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-05 18:39:36,428][0m A new study created in memory with name: no-name-9b446d89-136b-4bd3-9145-9f5b27819a30[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-05 18:40:20,358][0m Trial 0 finished with value: 0.882591811122087 and parameters: {'observation_period_num': 147, 'train_rates': 0.7304030594100418, 'learning_rate': 3.6083812154815274e-06, 'batch_size': 134, 'step_size': 8, 'gamma': 0.8988808687507038}. Best is trial 0 with value: 0.882591811122087.[0m
[32m[I 2025-01-05 18:40:55,937][0m Trial 1 finished with value: 1.198414586994746 and parameters: {'observation_period_num': 249, 'train_rates': 0.8108514258922135, 'learning_rate': 2.2985185561264765e-06, 'batch_size': 233, 'step_size': 4, 'gamma': 0.8427291432084406}. Best is trial 0 with value: 0.882591811122087.[0m
[32m[I 2025-01-05 18:41:43,135][0m Trial 2 finished with value: 0.3633597659778771 and parameters: {'observation_period_num': 40, 'train_rates': 0.6276426100616118, 'learning_rate': 4.8615015465490245e-05, 'batch_size': 120, 'step_size': 14, 'gamma': 0.903021726251242}. Best is trial 2 with value: 0.3633597659778771.[0m
[32m[I 2025-01-05 18:42:23,144][0m Trial 3 finished with value: 0.3319189456316943 and parameters: {'observation_period_num': 247, 'train_rates': 0.6617363043098625, 'learning_rate': 0.00016752191262014976, 'batch_size': 161, 'step_size': 3, 'gamma': 0.9576192055189672}. Best is trial 3 with value: 0.3319189456316943.[0m
Early stopping at epoch 93
[32m[I 2025-01-05 18:43:00,979][0m Trial 4 finished with value: 0.35201847553253174 and parameters: {'observation_period_num': 246, 'train_rates': 0.9327442708227032, 'learning_rate': 0.00013045683670279853, 'batch_size': 192, 'step_size': 2, 'gamma': 0.7749967632214848}. Best is trial 3 with value: 0.3319189456316943.[0m
[32m[I 2025-01-05 18:45:34,690][0m Trial 5 finished with value: 0.07919734774432222 and parameters: {'observation_period_num': 133, 'train_rates': 0.9136870139914958, 'learning_rate': 0.0003793152668191775, 'batch_size': 38, 'step_size': 14, 'gamma': 0.8276659811978343}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:50:14,985][0m Trial 6 finished with value: 0.3079664750338819 and parameters: {'observation_period_num': 180, 'train_rates': 0.7884388670058177, 'learning_rate': 1.3354544716637791e-05, 'batch_size': 18, 'step_size': 4, 'gamma': 0.7892333674110624}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:50:48,546][0m Trial 7 finished with value: 0.12154976433456534 and parameters: {'observation_period_num': 139, 'train_rates': 0.8564446703331585, 'learning_rate': 0.0005640565991144156, 'batch_size': 217, 'step_size': 14, 'gamma': 0.7869311795001257}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:51:29,791][0m Trial 8 finished with value: 0.1807263876432958 and parameters: {'observation_period_num': 171, 'train_rates': 0.9175337737028259, 'learning_rate': 5.758437414250199e-05, 'batch_size': 179, 'step_size': 12, 'gamma': 0.9004189714273538}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:53:05,440][0m Trial 9 finished with value: 0.5250146389007568 and parameters: {'observation_period_num': 133, 'train_rates': 0.9894699657283594, 'learning_rate': 3.2999176114075922e-06, 'batch_size': 66, 'step_size': 13, 'gamma': 0.9207569235994324}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:54:17,107][0m Trial 10 finished with value: 0.10114231162620356 and parameters: {'observation_period_num': 63, 'train_rates': 0.8812645055223455, 'learning_rate': 0.0008881221203363744, 'batch_size': 80, 'step_size': 10, 'gamma': 0.835570863614192}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:55:41,022][0m Trial 11 finished with value: 0.12361308993374714 and parameters: {'observation_period_num': 64, 'train_rates': 0.8945569789222909, 'learning_rate': 0.0009408335935040146, 'batch_size': 70, 'step_size': 10, 'gamma': 0.8455602945408119}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 18:58:51,601][0m Trial 12 finished with value: 0.08959085310958578 and parameters: {'observation_period_num': 76, 'train_rates': 0.8458728092063758, 'learning_rate': 0.0003051183278272328, 'batch_size': 28, 'step_size': 8, 'gamma': 0.8219365413109877}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 19:01:48,482][0m Trial 13 finished with value: 0.07997531488032664 and parameters: {'observation_period_num': 95, 'train_rates': 0.9796682000364204, 'learning_rate': 0.00025662225153409875, 'batch_size': 33, 'step_size': 7, 'gamma': 0.8117957590956386}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 19:02:50,157][0m Trial 14 finished with value: 0.5147300362586975 and parameters: {'observation_period_num': 104, 'train_rates': 0.9862397158166436, 'learning_rate': 1.538910724965622e-05, 'batch_size': 102, 'step_size': 6, 'gamma': 0.7516987654728121}. Best is trial 5 with value: 0.07919734774432222.[0m
[32m[I 2025-01-05 19:05:08,500][0m Trial 15 finished with value: 0.03632437537978344 and parameters: {'observation_period_num': 6, 'train_rates': 0.9466513947578208, 'learning_rate': 0.00024753458841961755, 'batch_size': 43, 'step_size': 6, 'gamma': 0.8107790275837972}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:07:03,311][0m Trial 16 finished with value: 0.04912328084602075 and parameters: {'observation_period_num': 10, 'train_rates': 0.9366017105394769, 'learning_rate': 8.857278014935043e-05, 'batch_size': 55, 'step_size': 11, 'gamma': 0.8648128783462785}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:08:02,034][0m Trial 17 finished with value: 0.1939759166347883 and parameters: {'observation_period_num': 10, 'train_rates': 0.7505216994825957, 'learning_rate': 9.024194792347442e-05, 'batch_size': 92, 'step_size': 10, 'gamma': 0.8765742473108158}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:09:44,016][0m Trial 18 finished with value: 0.06010837690985721 and parameters: {'observation_period_num': 6, 'train_rates': 0.9376371464627254, 'learning_rate': 2.2462862491581816e-05, 'batch_size': 60, 'step_size': 6, 'gamma': 0.97650374445098}. Best is trial 15 with value: 0.03632437537978344.[0m
Early stopping at epoch 88
[32m[I 2025-01-05 19:11:28,026][0m Trial 19 finished with value: 0.2944643992743046 and parameters: {'observation_period_num': 36, 'train_rates': 0.9525461207860737, 'learning_rate': 5.26387733425823e-05, 'batch_size': 51, 'step_size': 1, 'gamma': 0.871946384023387}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:12:24,528][0m Trial 20 finished with value: 0.25954541013020427 and parameters: {'observation_period_num': 29, 'train_rates': 0.8371830069469687, 'learning_rate': 1.117553189868809e-05, 'batch_size': 104, 'step_size': 11, 'gamma': 0.8578192280756632}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:14:17,857][0m Trial 21 finished with value: 0.06024931932363329 and parameters: {'observation_period_num': 8, 'train_rates': 0.9465044019007504, 'learning_rate': 2.4647442372400263e-05, 'batch_size': 53, 'step_size': 6, 'gamma': 0.9861979233887719}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:15:57,938][0m Trial 22 finished with value: 0.19690474355459897 and parameters: {'observation_period_num': 7, 'train_rates': 0.8814464979687627, 'learning_rate': 7.6227269392135746e-06, 'batch_size': 57, 'step_size': 6, 'gamma': 0.9490468404531978}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:17:13,277][0m Trial 23 finished with value: 0.8575597490583148 and parameters: {'observation_period_num': 44, 'train_rates': 0.95182482721905, 'learning_rate': 1.0484220621184082e-06, 'batch_size': 80, 'step_size': 5, 'gamma': 0.9339545088736536}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:18:05,194][0m Trial 24 finished with value: 0.0754290835917458 and parameters: {'observation_period_num': 25, 'train_rates': 0.9118367741468713, 'learning_rate': 3.48673713435268e-05, 'batch_size': 118, 'step_size': 9, 'gamma': 0.9862395702660548}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:20:18,735][0m Trial 25 finished with value: 0.0631953570331962 and parameters: {'observation_period_num': 50, 'train_rates': 0.8736461379458691, 'learning_rate': 8.898587436467729e-05, 'batch_size': 41, 'step_size': 7, 'gamma': 0.807948880077868}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:21:01,500][0m Trial 26 finished with value: 0.26640212535858154 and parameters: {'observation_period_num': 87, 'train_rates': 0.9564764837078285, 'learning_rate': 2.4179099546637928e-05, 'batch_size': 145, 'step_size': 12, 'gamma': 0.8769025847486085}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:26:15,087][0m Trial 27 finished with value: 0.04729862155621512 and parameters: {'observation_period_num': 24, 'train_rates': 0.8054483840739385, 'learning_rate': 0.00018079260033337073, 'batch_size': 16, 'step_size': 9, 'gamma': 0.7518132371964173}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:30:43,326][0m Trial 28 finished with value: 0.20116283639098506 and parameters: {'observation_period_num': 113, 'train_rates': 0.7077236637713761, 'learning_rate': 0.0002037755809438381, 'batch_size': 17, 'step_size': 9, 'gamma': 0.7521994916761651}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:35:25,794][0m Trial 29 finished with value: 0.17436220799304458 and parameters: {'observation_period_num': 23, 'train_rates': 0.7333623767149391, 'learning_rate': 0.0005047098759377081, 'batch_size': 17, 'step_size': 8, 'gamma': 0.7724601349827043}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:37:31,519][0m Trial 30 finished with value: 0.2043507226545879 and parameters: {'observation_period_num': 57, 'train_rates': 0.7827974107214868, 'learning_rate': 0.00011708959638193562, 'batch_size': 41, 'step_size': 12, 'gamma': 0.8065266462332414}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:38:45,954][0m Trial 31 finished with value: 0.06792392954921457 and parameters: {'observation_period_num': 18, 'train_rates': 0.8164462540309119, 'learning_rate': 6.565837490927946e-05, 'batch_size': 73, 'step_size': 7, 'gamma': 0.8631378986115181}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:40:22,092][0m Trial 32 finished with value: 0.16492041347802971 and parameters: {'observation_period_num': 5, 'train_rates': 0.7640469290263819, 'learning_rate': 0.0001651360357425005, 'batch_size': 55, 'step_size': 5, 'gamma': 0.9202661360283115}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:41:21,534][0m Trial 33 finished with value: 0.25620696683613103 and parameters: {'observation_period_num': 32, 'train_rates': 0.6950414161986629, 'learning_rate': 3.260921476067582e-05, 'batch_size': 94, 'step_size': 9, 'gamma': 0.8904339329835818}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:44:08,546][0m Trial 34 finished with value: 0.5391454280239262 and parameters: {'observation_period_num': 44, 'train_rates': 0.824188915789448, 'learning_rate': 5.421839204644571e-06, 'batch_size': 32, 'step_size': 4, 'gamma': 0.8480193966427649}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:44:44,490][0m Trial 35 finished with value: 0.38947141696876403 and parameters: {'observation_period_num': 211, 'train_rates': 0.6457205607895533, 'learning_rate': 8.4846434324012e-05, 'batch_size': 129, 'step_size': 11, 'gamma': 0.7665404088076844}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:46:18,431][0m Trial 36 finished with value: 0.30246986229497497 and parameters: {'observation_period_num': 20, 'train_rates': 0.6054259127737485, 'learning_rate': 0.00014459290011984512, 'batch_size': 47, 'step_size': 8, 'gamma': 0.7931474959072835}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:46:55,145][0m Trial 37 finished with value: 0.09145625680685043 and parameters: {'observation_period_num': 75, 'train_rates': 0.9252015117717179, 'learning_rate': 0.00023442286938289215, 'batch_size': 239, 'step_size': 5, 'gamma': 0.963071323030711}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:50:30,978][0m Trial 38 finished with value: 0.056365826446892094 and parameters: {'observation_period_num': 38, 'train_rates': 0.9030830923672939, 'learning_rate': 0.00039189607873864877, 'batch_size': 26, 'step_size': 3, 'gamma': 0.8876449227651221}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:53:57,774][0m Trial 39 finished with value: 0.10811953164104904 and parameters: {'observation_period_num': 160, 'train_rates': 0.8996692240999228, 'learning_rate': 0.00042223303068373, 'batch_size': 26, 'step_size': 3, 'gamma': 0.8945650423651534}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:54:24,100][0m Trial 40 finished with value: 0.08278893863800729 and parameters: {'observation_period_num': 38, 'train_rates': 0.861529088003028, 'learning_rate': 0.0005950084187175756, 'batch_size': 256, 'step_size': 2, 'gamma': 0.9121067324916655}. Best is trial 15 with value: 0.03632437537978344.[0m
[32m[I 2025-01-05 19:55:57,481][0m Trial 41 finished with value: 0.03514239522160361 and parameters: {'observation_period_num': 20, 'train_rates': 0.9355532192796326, 'learning_rate': 0.0003222157165404009, 'batch_size': 65, 'step_size': 15, 'gamma': 0.8236038284581738}. Best is trial 41 with value: 0.03514239522160361.[0m
[32m[I 2025-01-05 19:58:27,689][0m Trial 42 finished with value: 0.03484669700264931 and parameters: {'observation_period_num': 19, 'train_rates': 0.9695685152408033, 'learning_rate': 0.0003209314956014357, 'batch_size': 40, 'step_size': 15, 'gamma': 0.8279589513674962}. Best is trial 42 with value: 0.03484669700264931.[0m
[32m[I 2025-01-05 20:00:51,100][0m Trial 43 finished with value: 0.03444216564297676 and parameters: {'observation_period_num': 21, 'train_rates': 0.9660894205097864, 'learning_rate': 0.0003010715173492714, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8282156122908964}. Best is trial 43 with value: 0.03444216564297676.[0m
[32m[I 2025-01-05 20:03:12,628][0m Trial 44 finished with value: 0.06661094301803545 and parameters: {'observation_period_num': 58, 'train_rates': 0.9696794288042161, 'learning_rate': 0.0006960902084603115, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8251088642028155}. Best is trial 43 with value: 0.03444216564297676.[0m
[32m[I 2025-01-05 20:04:39,896][0m Trial 45 finished with value: 0.04255574405357078 and parameters: {'observation_period_num': 20, 'train_rates': 0.9689563288428235, 'learning_rate': 0.00029468214850530934, 'batch_size': 71, 'step_size': 13, 'gamma': 0.7976087449518116}. Best is trial 43 with value: 0.03444216564297676.[0m
[32m[I 2025-01-05 20:05:55,412][0m Trial 46 finished with value: 0.06291857179043428 and parameters: {'observation_period_num': 51, 'train_rates': 0.9637733127269664, 'learning_rate': 0.0003149536430056051, 'batch_size': 82, 'step_size': 15, 'gamma': 0.8361476516368603}. Best is trial 43 with value: 0.03444216564297676.[0m
[32m[I 2025-01-05 20:07:31,484][0m Trial 47 finished with value: 0.07157283322885633 and parameters: {'observation_period_num': 73, 'train_rates': 0.9724898861991731, 'learning_rate': 0.0007350426815811777, 'batch_size': 63, 'step_size': 14, 'gamma': 0.7968094551659436}. Best is trial 43 with value: 0.03444216564297676.[0m
[32m[I 2025-01-05 20:08:50,454][0m Trial 48 finished with value: 0.07567380509566311 and parameters: {'observation_period_num': 223, 'train_rates': 0.9265298194918954, 'learning_rate': 0.00028221065359441174, 'batch_size': 71, 'step_size': 13, 'gamma': 0.8199319078923243}. Best is trial 43 with value: 0.03444216564297676.[0m
[32m[I 2025-01-05 20:09:53,685][0m Trial 49 finished with value: 0.039681654423475266 and parameters: {'observation_period_num': 21, 'train_rates': 0.9869425235991997, 'learning_rate': 0.0004889521287469034, 'batch_size': 110, 'step_size': 15, 'gamma': 0.7818112189688959}. Best is trial 43 with value: 0.03444216564297676.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-05 20:09:53,695][0m A new study created in memory with name: no-name-2f475226-2552-42cb-a548-f8854be7383a[0m
[32m[I 2025-01-05 20:14:08,946][0m Trial 0 finished with value: 0.26636403276191367 and parameters: {'observation_period_num': 177, 'train_rates': 0.7365317224047452, 'learning_rate': 3.916522641258279e-05, 'batch_size': 37, 'step_size': 13, 'gamma': 0.8139670210186256}. Best is trial 0 with value: 0.26636403276191367.[0m
[32m[I 2025-01-05 20:17:12,758][0m Trial 1 finished with value: 0.10602731504368543 and parameters: {'observation_period_num': 152, 'train_rates': 0.89348596818953, 'learning_rate': 2.1104112428287637e-05, 'batch_size': 238, 'step_size': 11, 'gamma': 0.7898344881086645}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:20:37,912][0m Trial 2 finished with value: 0.15508598501377918 and parameters: {'observation_period_num': 132, 'train_rates': 0.9169354416243305, 'learning_rate': 1.9295959203486656e-05, 'batch_size': 217, 'step_size': 3, 'gamma': 0.8699198420359925}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:23:21,143][0m Trial 3 finished with value: 0.11121692236226338 and parameters: {'observation_period_num': 144, 'train_rates': 0.833240435402382, 'learning_rate': 4.904797209552265e-05, 'batch_size': 189, 'step_size': 12, 'gamma': 0.8797417858556832}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:25:57,735][0m Trial 4 finished with value: 0.1981130901054578 and parameters: {'observation_period_num': 106, 'train_rates': 0.6949994263198539, 'learning_rate': 0.0003645075353569309, 'batch_size': 243, 'step_size': 5, 'gamma': 0.8787872264313199}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:28:40,932][0m Trial 5 finished with value: 0.35751995929216934 and parameters: {'observation_period_num': 195, 'train_rates': 0.786021736138427, 'learning_rate': 1.9482610354464175e-06, 'batch_size': 204, 'step_size': 9, 'gamma': 0.970981189093282}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:31:46,395][0m Trial 6 finished with value: 0.15606849279926016 and parameters: {'observation_period_num': 149, 'train_rates': 0.89896911338373, 'learning_rate': 4.66264012805706e-06, 'batch_size': 197, 'step_size': 11, 'gamma': 0.9783589730111932}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:34:32,867][0m Trial 7 finished with value: 0.2287282676419668 and parameters: {'observation_period_num': 176, 'train_rates': 0.8358585345040095, 'learning_rate': 6.8110227315730035e-06, 'batch_size': 214, 'step_size': 13, 'gamma': 0.7783170230522782}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:37:17,679][0m Trial 8 finished with value: 0.1103745311166274 and parameters: {'observation_period_num': 145, 'train_rates': 0.8085353481740447, 'learning_rate': 0.0006754299680718927, 'batch_size': 160, 'step_size': 1, 'gamma': 0.9008981939707119}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:41:37,588][0m Trial 9 finished with value: 0.280562979202817 and parameters: {'observation_period_num': 137, 'train_rates': 0.7738688852796404, 'learning_rate': 0.00011098591149028922, 'batch_size': 36, 'step_size': 7, 'gamma': 0.9616369629843295}. Best is trial 1 with value: 0.10602731504368543.[0m
[32m[I 2025-01-05 20:45:12,266][0m Trial 10 finished with value: 0.04335632175207138 and parameters: {'observation_period_num': 42, 'train_rates': 0.9867094299444372, 'learning_rate': 0.00015606686191935282, 'batch_size': 108, 'step_size': 15, 'gamma': 0.754063992510577}. Best is trial 10 with value: 0.04335632175207138.[0m
[32m[I 2025-01-05 20:48:49,551][0m Trial 11 finished with value: 0.03929751366376877 and parameters: {'observation_period_num': 18, 'train_rates': 0.9734003648014313, 'learning_rate': 0.00015836929678840017, 'batch_size': 105, 'step_size': 15, 'gamma': 0.7514319241366205}. Best is trial 11 with value: 0.03929751366376877.[0m
[32m[I 2025-01-05 20:52:38,062][0m Trial 12 finished with value: 0.035450708121061325 and parameters: {'observation_period_num': 17, 'train_rates': 0.9878596741232757, 'learning_rate': 0.00017128823365563736, 'batch_size': 96, 'step_size': 15, 'gamma': 0.7506082117832349}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 20:55:59,310][0m Trial 13 finished with value: 0.04585058614611626 and parameters: {'observation_period_num': 12, 'train_rates': 0.9898687986473821, 'learning_rate': 0.00021439470421444543, 'batch_size': 98, 'step_size': 15, 'gamma': 0.8230220663115705}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 20:58:32,621][0m Trial 14 finished with value: 0.2053192660252395 and parameters: {'observation_period_num': 73, 'train_rates': 0.6104649405432785, 'learning_rate': 0.0006372509126832413, 'batch_size': 80, 'step_size': 15, 'gamma': 0.7509582218084495}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:01:02,224][0m Trial 15 finished with value: 0.10841520503163338 and parameters: {'observation_period_num': 244, 'train_rates': 0.9394215902966643, 'learning_rate': 8.544296781361686e-05, 'batch_size': 135, 'step_size': 9, 'gamma': 0.8295449484416297}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:04:23,236][0m Trial 16 finished with value: 0.035492175380731454 and parameters: {'observation_period_num': 14, 'train_rates': 0.9508872322483831, 'learning_rate': 0.0002802787601095387, 'batch_size': 69, 'step_size': 13, 'gamma': 0.7859970591896219}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:07:56,601][0m Trial 17 finished with value: 0.1431481047213234 and parameters: {'observation_period_num': 62, 'train_rates': 0.8682146806703703, 'learning_rate': 0.0009701160759389491, 'batch_size': 65, 'step_size': 13, 'gamma': 0.791160068813244}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:16:12,679][0m Trial 18 finished with value: 0.08084663252035777 and parameters: {'observation_period_num': 88, 'train_rates': 0.9403521009154141, 'learning_rate': 0.0002988146251879378, 'batch_size': 20, 'step_size': 7, 'gamma': 0.8445748373669989}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:19:55,292][0m Trial 19 finished with value: 0.06337409096581119 and parameters: {'observation_period_num': 47, 'train_rates': 0.9508326250158504, 'learning_rate': 0.0004186433048386849, 'batch_size': 61, 'step_size': 10, 'gamma': 0.7814431166514024}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:22:14,824][0m Trial 20 finished with value: 0.04191076434312426 and parameters: {'observation_period_num': 29, 'train_rates': 0.8661006832465983, 'learning_rate': 6.645215574980939e-05, 'batch_size': 138, 'step_size': 13, 'gamma': 0.9430931393062625}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:25:45,380][0m Trial 21 finished with value: 0.03910274803638458 and parameters: {'observation_period_num': 9, 'train_rates': 0.9680014277327524, 'learning_rate': 0.00017916676917010003, 'batch_size': 111, 'step_size': 15, 'gamma': 0.7607856654843587}. Best is trial 12 with value: 0.035450708121061325.[0m
[32m[I 2025-01-05 21:29:25,379][0m Trial 22 finished with value: 0.032681295370210466 and parameters: {'observation_period_num': 9, 'train_rates': 0.9544825248886449, 'learning_rate': 0.00024556096939814945, 'batch_size': 131, 'step_size': 14, 'gamma': 0.7710157728119498}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:32:33,234][0m Trial 23 finished with value: 0.048278405430859754 and parameters: {'observation_period_num': 36, 'train_rates': 0.9232070907310388, 'learning_rate': 0.0004122455144539554, 'batch_size': 142, 'step_size': 14, 'gamma': 0.8033408221285947}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:36:02,881][0m Trial 24 finished with value: 0.0659811932593584 and parameters: {'observation_period_num': 61, 'train_rates': 0.8860628276526641, 'learning_rate': 0.00011322677525558319, 'batch_size': 81, 'step_size': 11, 'gamma': 0.771517740418229}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:38:55,978][0m Trial 25 finished with value: 0.08374512940645218 and parameters: {'observation_period_num': 98, 'train_rates': 0.9531914996994489, 'learning_rate': 2.4180305971059194e-05, 'batch_size': 166, 'step_size': 12, 'gamma': 0.8054283276141804}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:41:44,003][0m Trial 26 finished with value: 0.03877817364834651 and parameters: {'observation_period_num': 26, 'train_rates': 0.9177731716021412, 'learning_rate': 0.000251521606922569, 'batch_size': 123, 'step_size': 14, 'gamma': 0.8362221362800708}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:44:22,012][0m Trial 27 finished with value: 0.15410418995814015 and parameters: {'observation_period_num': 6, 'train_rates': 0.6515318037794189, 'learning_rate': 1.2301420714745339e-05, 'batch_size': 84, 'step_size': 14, 'gamma': 0.848952498509446}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:48:19,000][0m Trial 28 finished with value: 0.08393976273713077 and parameters: {'observation_period_num': 58, 'train_rates': 0.8552015348904003, 'learning_rate': 0.0009994513214455648, 'batch_size': 60, 'step_size': 10, 'gamma': 0.769728992990848}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:51:56,781][0m Trial 29 finished with value: 0.20546801517159724 and parameters: {'observation_period_num': 74, 'train_rates': 0.7347249588433641, 'learning_rate': 4.120887681782728e-05, 'batch_size': 44, 'step_size': 12, 'gamma': 0.8070988478456451}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:55:28,178][0m Trial 30 finished with value: 0.06237483397126198 and parameters: {'observation_period_num': 44, 'train_rates': 0.9599188403481588, 'learning_rate': 8.097466427081529e-05, 'batch_size': 161, 'step_size': 13, 'gamma': 0.9164238748852883}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 21:58:53,813][0m Trial 31 finished with value: 0.037445228872820736 and parameters: {'observation_period_num': 25, 'train_rates': 0.9180688590893678, 'learning_rate': 0.0002210226618607998, 'batch_size': 126, 'step_size': 14, 'gamma': 0.8363172051772553}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 22:02:10,000][0m Trial 32 finished with value: 0.041065871715545654 and parameters: {'observation_period_num': 26, 'train_rates': 0.8990570914414976, 'learning_rate': 0.0004861001010689013, 'batch_size': 126, 'step_size': 14, 'gamma': 0.8166779900401997}. Best is trial 22 with value: 0.032681295370210466.[0m
[32m[I 2025-01-05 22:05:21,583][0m Trial 33 finished with value: 0.030193102790169346 and parameters: {'observation_period_num': 7, 'train_rates': 0.9301261731146679, 'learning_rate': 0.00024568693784124206, 'batch_size': 90, 'step_size': 12, 'gamma': 0.7928565968756358}. Best is trial 33 with value: 0.030193102790169346.[0m
[32m[I 2025-01-05 22:08:32,433][0m Trial 34 finished with value: 0.03142109250147191 and parameters: {'observation_period_num': 6, 'train_rates': 0.9360602095735646, 'learning_rate': 0.00013148526615853928, 'batch_size': 93, 'step_size': 12, 'gamma': 0.7915591256158274}. Best is trial 33 with value: 0.030193102790169346.[0m
[32m[I 2025-01-05 22:11:54,093][0m Trial 35 finished with value: 0.028608886341550458 and parameters: {'observation_period_num': 5, 'train_rates': 0.8863300400348151, 'learning_rate': 0.0001340968792268166, 'batch_size': 94, 'step_size': 11, 'gamma': 0.7930123440833114}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:14:56,966][0m Trial 36 finished with value: 0.05022885082460986 and parameters: {'observation_period_num': 47, 'train_rates': 0.8880154915002426, 'learning_rate': 5.063895113204193e-05, 'batch_size': 91, 'step_size': 11, 'gamma': 0.793985995103438}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:18:03,510][0m Trial 37 finished with value: 0.03903373341475214 and parameters: {'observation_period_num': 5, 'train_rates': 0.9287217830159753, 'learning_rate': 0.00010755041873762883, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8544171032395103}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:21:08,578][0m Trial 38 finished with value: 0.13435352052287233 and parameters: {'observation_period_num': 230, 'train_rates': 0.846696097490812, 'learning_rate': 2.861060054759268e-05, 'batch_size': 116, 'step_size': 8, 'gamma': 0.7965724413283521}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:23:56,383][0m Trial 39 finished with value: 0.10096832012801658 and parameters: {'observation_period_num': 124, 'train_rates': 0.8203025307885364, 'learning_rate': 5.9537129985256705e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.7707193821977935}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:26:46,306][0m Trial 40 finished with value: 0.17478295969399246 and parameters: {'observation_period_num': 170, 'train_rates': 0.8936832565890477, 'learning_rate': 1.69622284579687e-05, 'batch_size': 153, 'step_size': 4, 'gamma': 0.8137058524559773}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:30:08,123][0m Trial 41 finished with value: 0.03879163786768913 and parameters: {'observation_period_num': 20, 'train_rates': 0.9705598999666772, 'learning_rate': 0.00016332291632010248, 'batch_size': 95, 'step_size': 12, 'gamma': 0.7696714818154476}. Best is trial 35 with value: 0.028608886341550458.[0m
[32m[I 2025-01-05 22:33:43,886][0m Trial 42 finished with value: 0.028069044744426553 and parameters: {'observation_period_num': 5, 'train_rates': 0.9069483227435597, 'learning_rate': 0.00013080095431673434, 'batch_size': 74, 'step_size': 11, 'gamma': 0.7618033529001876}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 22:37:24,206][0m Trial 43 finished with value: 0.14430374108599597 and parameters: {'observation_period_num': 32, 'train_rates': 0.9038142631404514, 'learning_rate': 1.2825616963029108e-06, 'batch_size': 54, 'step_size': 9, 'gamma': 0.7794894856441735}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 22:40:26,977][0m Trial 44 finished with value: 0.030131460437851566 and parameters: {'observation_period_num': 6, 'train_rates': 0.8736490607451071, 'learning_rate': 0.00011167151508752783, 'batch_size': 76, 'step_size': 11, 'gamma': 0.7622892861316224}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 22:44:03,435][0m Trial 45 finished with value: 0.04712087884545326 and parameters: {'observation_period_num': 38, 'train_rates': 0.8765374345633443, 'learning_rate': 0.00011588915960576426, 'batch_size': 72, 'step_size': 11, 'gamma': 0.7628343598965563}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 22:47:53,611][0m Trial 46 finished with value: 0.12473686161653075 and parameters: {'observation_period_num': 194, 'train_rates': 0.7943733557349874, 'learning_rate': 7.915862448006823e-05, 'batch_size': 47, 'step_size': 10, 'gamma': 0.8607554265291979}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 22:53:25,259][0m Trial 47 finished with value: 0.20731096299367874 and parameters: {'observation_period_num': 54, 'train_rates': 0.7630661169650809, 'learning_rate': 4.1603624087517865e-05, 'batch_size': 28, 'step_size': 8, 'gamma': 0.8824028766502159}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 22:56:35,668][0m Trial 48 finished with value: 0.0733532103313448 and parameters: {'observation_period_num': 116, 'train_rates': 0.8268226939140506, 'learning_rate': 0.00012431128280257833, 'batch_size': 74, 'step_size': 9, 'gamma': 0.7870675404126416}. Best is trial 42 with value: 0.028069044744426553.[0m
[32m[I 2025-01-05 23:00:03,835][0m Trial 49 finished with value: 0.046883932129943864 and parameters: {'observation_period_num': 19, 'train_rates': 0.9091173063126549, 'learning_rate': 0.0006051132403125398, 'batch_size': 101, 'step_size': 11, 'gamma': 0.79772184519603}. Best is trial 42 with value: 0.028069044744426553.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-05 23:00:03,845][0m A new study created in memory with name: no-name-a428bd97-5ae4-4f8a-94b3-b41ed19d4aa5[0m
[32m[I 2025-01-05 23:02:52,209][0m Trial 0 finished with value: 0.23476369005095055 and parameters: {'observation_period_num': 177, 'train_rates': 0.6527447523269726, 'learning_rate': 4.188011450123387e-05, 'batch_size': 67, 'step_size': 9, 'gamma': 0.8614344714645618}. Best is trial 0 with value: 0.23476369005095055.[0m
[32m[I 2025-01-05 23:04:59,222][0m Trial 1 finished with value: 0.19640586094559584 and parameters: {'observation_period_num': 87, 'train_rates': 0.6128199932219482, 'learning_rate': 0.00038088987773268144, 'batch_size': 212, 'step_size': 7, 'gamma': 0.8671103798116934}. Best is trial 1 with value: 0.19640586094559584.[0m
[32m[I 2025-01-05 23:07:34,729][0m Trial 2 finished with value: 0.20063765905797482 and parameters: {'observation_period_num': 226, 'train_rates': 0.9296956115130235, 'learning_rate': 4.021148993139167e-06, 'batch_size': 139, 'step_size': 11, 'gamma': 0.8715928765920441}. Best is trial 1 with value: 0.19640586094559584.[0m
[32m[I 2025-01-05 23:10:05,218][0m Trial 3 finished with value: 0.20989956255331524 and parameters: {'observation_period_num': 72, 'train_rates': 0.7706622154501921, 'learning_rate': 8.993124199352113e-05, 'batch_size': 121, 'step_size': 7, 'gamma': 0.8007054180400721}. Best is trial 1 with value: 0.19640586094559584.[0m
[32m[I 2025-01-05 23:13:20,714][0m Trial 4 finished with value: 0.08357468992471695 and parameters: {'observation_period_num': 123, 'train_rates': 0.9550968385190273, 'learning_rate': 4.2478324673853e-05, 'batch_size': 181, 'step_size': 8, 'gamma': 0.8857489733158432}. Best is trial 4 with value: 0.08357468992471695.[0m
Early stopping at epoch 45
[32m[I 2025-01-05 23:14:58,139][0m Trial 5 finished with value: 0.27577337622642517 and parameters: {'observation_period_num': 38, 'train_rates': 0.9709069914745503, 'learning_rate': 9.704018810245495e-06, 'batch_size': 132, 'step_size': 1, 'gamma': 0.770521633030636}. Best is trial 4 with value: 0.08357468992471695.[0m
[32m[I 2025-01-05 23:19:20,267][0m Trial 6 finished with value: 0.07814226950070373 and parameters: {'observation_period_num': 85, 'train_rates': 0.811670601902519, 'learning_rate': 0.0006496142633830215, 'batch_size': 37, 'step_size': 4, 'gamma': 0.8942735989984366}. Best is trial 6 with value: 0.07814226950070373.[0m
[32m[I 2025-01-05 23:23:55,668][0m Trial 7 finished with value: 0.09060848594471148 and parameters: {'observation_period_num': 65, 'train_rates': 0.8914592712395528, 'learning_rate': 4.175512469112213e-05, 'batch_size': 40, 'step_size': 9, 'gamma': 0.845225612297678}. Best is trial 6 with value: 0.07814226950070373.[0m
[32m[I 2025-01-05 23:26:52,365][0m Trial 8 finished with value: 0.6527093084025568 and parameters: {'observation_period_num': 145, 'train_rates': 0.6795395127440956, 'learning_rate': 1.0784989381966756e-06, 'batch_size': 100, 'step_size': 7, 'gamma': 0.7780110810468112}. Best is trial 6 with value: 0.07814226950070373.[0m
[32m[I 2025-01-05 23:30:16,515][0m Trial 9 finished with value: 0.05035058946455296 and parameters: {'observation_period_num': 20, 'train_rates': 0.9314931966218621, 'learning_rate': 4.4025119610806434e-05, 'batch_size': 181, 'step_size': 11, 'gamma': 0.8007903136514052}. Best is trial 9 with value: 0.05035058946455296.[0m
[32m[I 2025-01-05 23:32:54,432][0m Trial 10 finished with value: 0.04824037280981704 and parameters: {'observation_period_num': 13, 'train_rates': 0.8560042998513931, 'learning_rate': 0.00020675547654972281, 'batch_size': 245, 'step_size': 15, 'gamma': 0.9712993852066139}. Best is trial 10 with value: 0.04824037280981704.[0m
[32m[I 2025-01-05 23:35:47,727][0m Trial 11 finished with value: 0.042367996841531426 and parameters: {'observation_period_num': 6, 'train_rates': 0.8529863293249756, 'learning_rate': 0.00018466235411414823, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9836337656339019}. Best is trial 11 with value: 0.042367996841531426.[0m
[32m[I 2025-01-05 23:38:44,600][0m Trial 12 finished with value: 0.04298839355984372 and parameters: {'observation_period_num': 11, 'train_rates': 0.8296244342930319, 'learning_rate': 0.0001668014789208696, 'batch_size': 251, 'step_size': 15, 'gamma': 0.9883550576053745}. Best is trial 11 with value: 0.042367996841531426.[0m
[32m[I 2025-01-05 23:41:17,903][0m Trial 13 finished with value: 0.1822522333672931 and parameters: {'observation_period_num': 9, 'train_rates': 0.768334746975576, 'learning_rate': 0.00014985415348982796, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9876040626023427}. Best is trial 11 with value: 0.042367996841531426.[0m
[32m[I 2025-01-05 23:43:56,082][0m Trial 14 finished with value: 0.0658414471679582 and parameters: {'observation_period_num': 47, 'train_rates': 0.8211809983420039, 'learning_rate': 0.0008657068647448209, 'batch_size': 215, 'step_size': 13, 'gamma': 0.9325377814261167}. Best is trial 11 with value: 0.042367996841531426.[0m
[32m[I 2025-01-05 23:46:39,886][0m Trial 15 finished with value: 0.08838389006046608 and parameters: {'observation_period_num': 116, 'train_rates': 0.8452703650945858, 'learning_rate': 0.0002683308260331705, 'batch_size': 219, 'step_size': 13, 'gamma': 0.9482127512810449}. Best is trial 11 with value: 0.042367996841531426.[0m
[32m[I 2025-01-05 23:49:27,084][0m Trial 16 finished with value: 0.31392388977110386 and parameters: {'observation_period_num': 210, 'train_rates': 0.7228263416653384, 'learning_rate': 1.2591623170125246e-05, 'batch_size': 171, 'step_size': 13, 'gamma': 0.9269458527036558}. Best is trial 11 with value: 0.042367996841531426.[0m
[32m[I 2025-01-05 23:52:26,892][0m Trial 17 finished with value: 0.0401268722979646 and parameters: {'observation_period_num': 5, 'train_rates': 0.8839908506720112, 'learning_rate': 9.898304634855669e-05, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9602647511367413}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-05 23:55:37,781][0m Trial 18 finished with value: 0.059729246275244854 and parameters: {'observation_period_num': 44, 'train_rates': 0.9044121582756517, 'learning_rate': 9.934701082494449e-05, 'batch_size': 226, 'step_size': 11, 'gamma': 0.9527032272326204}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-05 23:58:39,696][0m Trial 19 finished with value: 0.1297497330368429 and parameters: {'observation_period_num': 159, 'train_rates': 0.8767053351263768, 'learning_rate': 1.6493619083001625e-05, 'batch_size': 194, 'step_size': 4, 'gamma': 0.909688540287531}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:01:42,553][0m Trial 20 finished with value: 0.2315130859428311 and parameters: {'observation_period_num': 96, 'train_rates': 0.7746914333054562, 'learning_rate': 0.00043862205264358475, 'batch_size': 152, 'step_size': 13, 'gamma': 0.9541680574165816}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:04:41,946][0m Trial 21 finished with value: 0.04209571539404544 and parameters: {'observation_period_num': 7, 'train_rates': 0.8406361448707557, 'learning_rate': 0.00010018500163227293, 'batch_size': 254, 'step_size': 15, 'gamma': 0.9819464454992152}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:07:52,483][0m Trial 22 finished with value: 0.055899685204733075 and parameters: {'observation_period_num': 42, 'train_rates': 0.8692325324187642, 'learning_rate': 7.71550445919241e-05, 'batch_size': 235, 'step_size': 14, 'gamma': 0.9696337993290357}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:11:10,082][0m Trial 23 finished with value: 0.05079801854762164 and parameters: {'observation_period_num': 26, 'train_rates': 0.9099970094540076, 'learning_rate': 2.1809297374667646e-05, 'batch_size': 204, 'step_size': 12, 'gamma': 0.9237564226242141}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:13:49,941][0m Trial 24 finished with value: 0.08986520744168881 and parameters: {'observation_period_num': 61, 'train_rates': 0.7986692368088539, 'learning_rate': 8.025652206076655e-05, 'batch_size': 234, 'step_size': 15, 'gamma': 0.9694611967972238}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:16:29,647][0m Trial 25 finished with value: 0.16375623084850796 and parameters: {'observation_period_num': 7, 'train_rates': 0.7428100853759081, 'learning_rate': 0.00032594792068097405, 'batch_size': 254, 'step_size': 14, 'gamma': 0.9883836683020443}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:19:36,466][0m Trial 26 finished with value: 0.05019398778676987 and parameters: {'observation_period_num': 30, 'train_rates': 0.988496448262347, 'learning_rate': 0.00012111706760293379, 'batch_size': 194, 'step_size': 12, 'gamma': 0.941765699266826}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:22:34,071][0m Trial 27 finished with value: 0.08167279311097585 and parameters: {'observation_period_num': 53, 'train_rates': 0.8520051251040692, 'learning_rate': 6.868888816273633e-05, 'batch_size': 231, 'step_size': 14, 'gamma': 0.9101350918357027}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:32:40,139][0m Trial 28 finished with value: 0.06389641347077657 and parameters: {'observation_period_num': 107, 'train_rates': 0.9255494099847518, 'learning_rate': 6.365307306726547e-06, 'batch_size': 16, 'step_size': 10, 'gamma': 0.9687982724989552}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:36:00,382][0m Trial 29 finished with value: 0.10238706866900125 and parameters: {'observation_period_num': 252, 'train_rates': 0.8889569846017609, 'learning_rate': 2.2725691061611984e-05, 'batch_size': 83, 'step_size': 14, 'gamma': 0.845985024761694}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:38:37,075][0m Trial 30 finished with value: 0.2782860614798748 and parameters: {'observation_period_num': 177, 'train_rates': 0.7127215301775944, 'learning_rate': 0.00022575982733982072, 'batch_size': 159, 'step_size': 12, 'gamma': 0.9612964421313219}. Best is trial 17 with value: 0.0401268722979646.[0m
[32m[I 2025-01-06 00:41:45,521][0m Trial 31 finished with value: 0.038048212555889596 and parameters: {'observation_period_num': 7, 'train_rates': 0.8298946919096865, 'learning_rate': 0.0001662341142994145, 'batch_size': 246, 'step_size': 15, 'gamma': 0.9882430225367003}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 00:44:41,972][0m Trial 32 finished with value: 0.059752277170806 and parameters: {'observation_period_num': 27, 'train_rates': 0.8280804223863146, 'learning_rate': 0.0004724378073592624, 'batch_size': 242, 'step_size': 15, 'gamma': 0.978683783297947}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 00:47:28,050][0m Trial 33 finished with value: 0.0401871994889083 and parameters: {'observation_period_num': 5, 'train_rates': 0.7982777461947503, 'learning_rate': 0.0001611963163208358, 'batch_size': 206, 'step_size': 14, 'gamma': 0.9404965842052702}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 00:50:19,260][0m Trial 34 finished with value: 0.05512179789809406 and parameters: {'observation_period_num': 29, 'train_rates': 0.7977243535469677, 'learning_rate': 5.123391306499542e-05, 'batch_size': 206, 'step_size': 14, 'gamma': 0.9412038530537774}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 00:53:12,100][0m Trial 35 finished with value: 0.23402949230946726 and parameters: {'observation_period_num': 74, 'train_rates': 0.7718623144003594, 'learning_rate': 0.00012298583197414847, 'batch_size': 218, 'step_size': 13, 'gamma': 0.9588710225074268}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 00:56:10,362][0m Trial 36 finished with value: 0.1775865457139779 and parameters: {'observation_period_num': 22, 'train_rates': 0.7463793593484699, 'learning_rate': 6.01580873138343e-05, 'batch_size': 238, 'step_size': 10, 'gamma': 0.9143570664493431}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 00:58:40,314][0m Trial 37 finished with value: 0.09971042665774407 and parameters: {'observation_period_num': 57, 'train_rates': 0.8094026121061281, 'learning_rate': 2.6976910690041526e-05, 'batch_size': 222, 'step_size': 3, 'gamma': 0.8899162423203597}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 01:01:44,161][0m Trial 38 finished with value: 0.07309544803979603 and parameters: {'observation_period_num': 80, 'train_rates': 0.8397354599237655, 'learning_rate': 0.0002988605255356736, 'batch_size': 197, 'step_size': 14, 'gamma': 0.9350582917282556}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 01:04:19,942][0m Trial 39 finished with value: 0.21367959193692307 and parameters: {'observation_period_num': 37, 'train_rates': 0.6027406236296252, 'learning_rate': 0.0005996001970941943, 'batch_size': 111, 'step_size': 6, 'gamma': 0.9729398855194452}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 01:06:57,997][0m Trial 40 finished with value: 0.13750341957846976 and parameters: {'observation_period_num': 5, 'train_rates': 0.6408127697399689, 'learning_rate': 3.498966911905275e-05, 'batch_size': 178, 'step_size': 12, 'gamma': 0.8742697509051085}. Best is trial 31 with value: 0.038048212555889596.[0m
[32m[I 2025-01-06 01:09:51,740][0m Trial 41 finished with value: 0.0379116118830793 and parameters: {'observation_period_num': 5, 'train_rates': 0.8674039690644638, 'learning_rate': 0.00018491066263992426, 'batch_size': 253, 'step_size': 15, 'gamma': 0.9837930549028001}. Best is trial 41 with value: 0.0379116118830793.[0m
[32m[I 2025-01-06 01:12:54,987][0m Trial 42 finished with value: 0.05350338002400739 and parameters: {'observation_period_num': 17, 'train_rates': 0.8665982778750219, 'learning_rate': 0.00011875893826752353, 'batch_size': 242, 'step_size': 15, 'gamma': 0.95982648434773}. Best is trial 41 with value: 0.0379116118830793.[0m
[32m[I 2025-01-06 01:16:02,772][0m Trial 43 finished with value: 0.30189910531044006 and parameters: {'observation_period_num': 38, 'train_rates': 0.9478737851236337, 'learning_rate': 1.0565556966781161e-06, 'batch_size': 256, 'step_size': 14, 'gamma': 0.9771232295585756}. Best is trial 41 with value: 0.0379116118830793.[0m
[32m[I 2025-01-06 01:19:02,211][0m Trial 44 finished with value: 0.037725950280825295 and parameters: {'observation_period_num': 20, 'train_rates': 0.8822915606825786, 'learning_rate': 0.00020505398437173418, 'batch_size': 227, 'step_size': 15, 'gamma': 0.8140714388178996}. Best is trial 44 with value: 0.037725950280825295.[0m
[32m[I 2025-01-06 01:22:09,292][0m Trial 45 finished with value: 0.03679758379405195 and parameters: {'observation_period_num': 19, 'train_rates': 0.9063661015755176, 'learning_rate': 0.00016203868434834786, 'batch_size': 208, 'step_size': 13, 'gamma': 0.8209973654452418}. Best is trial 45 with value: 0.03679758379405195.[0m
[32m[I 2025-01-06 01:25:23,550][0m Trial 46 finished with value: 0.03489110760745548 and parameters: {'observation_period_num': 19, 'train_rates': 0.8928163234319546, 'learning_rate': 0.0002561402649349968, 'batch_size': 228, 'step_size': 13, 'gamma': 0.8126581878471357}. Best is trial 46 with value: 0.03489110760745548.[0m
[32m[I 2025-01-06 01:28:27,786][0m Trial 47 finished with value: 0.07020587789000206 and parameters: {'observation_period_num': 67, 'train_rates': 0.9083034670228284, 'learning_rate': 0.000235562398310495, 'batch_size': 224, 'step_size': 10, 'gamma': 0.8050478761111398}. Best is trial 46 with value: 0.03489110760745548.[0m
[32m[I 2025-01-06 01:31:47,169][0m Trial 48 finished with value: 0.03825463307460388 and parameters: {'observation_period_num': 20, 'train_rates': 0.94335822730517, 'learning_rate': 0.0008874047536874092, 'batch_size': 133, 'step_size': 8, 'gamma': 0.8108391209758624}. Best is trial 46 with value: 0.03489110760745548.[0m
[32m[I 2025-01-06 01:34:46,539][0m Trial 49 finished with value: 0.054933543909679756 and parameters: {'observation_period_num': 34, 'train_rates': 0.9210163219536777, 'learning_rate': 0.0001895134353230075, 'batch_size': 210, 'step_size': 13, 'gamma': 0.8268519330389064}. Best is trial 46 with value: 0.03489110760745548.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-06 01:34:46,547][0m A new study created in memory with name: no-name-288334c5-63aa-4981-8b5c-394097d1d1ea[0m
[32m[I 2025-01-06 01:37:49,268][0m Trial 0 finished with value: 0.14970578278440075 and parameters: {'observation_period_num': 153, 'train_rates': 0.8944206022770504, 'learning_rate': 1.1115611398467659e-05, 'batch_size': 147, 'step_size': 7, 'gamma': 0.8012364310199572}. Best is trial 0 with value: 0.14970578278440075.[0m
[32m[I 2025-01-06 01:39:52,778][0m Trial 1 finished with value: 0.43230755713267544 and parameters: {'observation_period_num': 70, 'train_rates': 0.7238661353266764, 'learning_rate': 2.9076809089920517e-06, 'batch_size': 226, 'step_size': 12, 'gamma': 0.8371212217919798}. Best is trial 0 with value: 0.14970578278440075.[0m
[32m[I 2025-01-06 01:42:19,207][0m Trial 2 finished with value: 0.07759057659375874 and parameters: {'observation_period_num': 76, 'train_rates': 0.879680692269737, 'learning_rate': 0.00040769822879152914, 'batch_size': 150, 'step_size': 13, 'gamma': 0.8898824246226333}. Best is trial 2 with value: 0.07759057659375874.[0m
[32m[I 2025-01-06 01:44:42,352][0m Trial 3 finished with value: 0.17297357485762666 and parameters: {'observation_period_num': 63, 'train_rates': 0.6123067270826679, 'learning_rate': 6.853590766961093e-05, 'batch_size': 221, 'step_size': 13, 'gamma': 0.8520010105374128}. Best is trial 2 with value: 0.07759057659375874.[0m
[32m[I 2025-01-06 01:47:45,959][0m Trial 4 finished with value: 0.06814922642156568 and parameters: {'observation_period_num': 117, 'train_rates': 0.8781272485025922, 'learning_rate': 0.0003679892492436173, 'batch_size': 131, 'step_size': 4, 'gamma': 0.8207046459462711}. Best is trial 4 with value: 0.06814922642156568.[0m
Early stopping at epoch 82
[32m[I 2025-01-06 01:50:11,234][0m Trial 5 finished with value: 0.25243243720214326 and parameters: {'observation_period_num': 91, 'train_rates': 0.7056139293893052, 'learning_rate': 0.00019868575289656184, 'batch_size': 91, 'step_size': 1, 'gamma': 0.8150433346438557}. Best is trial 4 with value: 0.06814922642156568.[0m
[32m[I 2025-01-06 01:52:29,612][0m Trial 6 finished with value: 0.3930337704949776 and parameters: {'observation_period_num': 223, 'train_rates': 0.6344932429331479, 'learning_rate': 0.0005715830711631574, 'batch_size': 224, 'step_size': 14, 'gamma': 0.7706428950625566}. Best is trial 4 with value: 0.06814922642156568.[0m
[32m[I 2025-01-06 01:54:26,370][0m Trial 7 finished with value: 0.4295792869060715 and parameters: {'observation_period_num': 133, 'train_rates': 0.6317867585860926, 'learning_rate': 1.0617695178457294e-05, 'batch_size': 180, 'step_size': 4, 'gamma': 0.8480706561038481}. Best is trial 4 with value: 0.06814922642156568.[0m
[32m[I 2025-01-06 01:56:24,619][0m Trial 8 finished with value: 0.40108779197778044 and parameters: {'observation_period_num': 70, 'train_rates': 0.7437079968572663, 'learning_rate': 2.3108535010210597e-06, 'batch_size': 197, 'step_size': 15, 'gamma': 0.7771722816531677}. Best is trial 4 with value: 0.06814922642156568.[0m
[32m[I 2025-01-06 01:59:21,832][0m Trial 9 finished with value: 0.05008470380100711 and parameters: {'observation_period_num': 53, 'train_rates': 0.9000060406239896, 'learning_rate': 0.0001886983742484157, 'batch_size': 176, 'step_size': 12, 'gamma': 0.9262665775016687}. Best is trial 9 with value: 0.05008470380100711.[0m
[32m[I 2025-01-06 02:03:11,550][0m Trial 10 finished with value: 0.03147755443033847 and parameters: {'observation_period_num': 6, 'train_rates': 0.9627278417486529, 'learning_rate': 7.353708316883962e-05, 'batch_size': 47, 'step_size': 10, 'gamma': 0.9637099783817532}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:10:02,781][0m Trial 11 finished with value: 0.04356862285307476 and parameters: {'observation_period_num': 7, 'train_rates': 0.981282317840248, 'learning_rate': 7.95608184907774e-05, 'batch_size': 25, 'step_size': 10, 'gamma': 0.964204247387455}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:17:58,104][0m Trial 12 finished with value: 0.04438656494021416 and parameters: {'observation_period_num': 8, 'train_rates': 0.9815661449223831, 'learning_rate': 4.418778973126636e-05, 'batch_size': 22, 'step_size': 9, 'gamma': 0.9789213421523186}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:28:17,010][0m Trial 13 finished with value: 0.04646575994789601 and parameters: {'observation_period_num': 9, 'train_rates': 0.9660573748476187, 'learning_rate': 9.977355608689158e-05, 'batch_size': 16, 'step_size': 9, 'gamma': 0.9885378322531914}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:31:31,997][0m Trial 14 finished with value: 0.04509725907574529 and parameters: {'observation_period_num': 29, 'train_rates': 0.8188979724444106, 'learning_rate': 1.5375376382226036e-05, 'batch_size': 64, 'step_size': 10, 'gamma': 0.9419809244085492}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:34:27,915][0m Trial 15 finished with value: 0.10379808848794503 and parameters: {'observation_period_num': 188, 'train_rates': 0.9429029858890838, 'learning_rate': 2.85675757927168e-05, 'batch_size': 62, 'step_size': 11, 'gamma': 0.9435330898127464}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:37:36,787][0m Trial 16 finished with value: 0.07014038754115864 and parameters: {'observation_period_num': 38, 'train_rates': 0.8213870728451645, 'learning_rate': 0.00013132071107902954, 'batch_size': 47, 'step_size': 7, 'gamma': 0.8920782246265685}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:39:44,759][0m Trial 17 finished with value: 0.03650278937010491 and parameters: {'observation_period_num': 6, 'train_rates': 0.9379211836842563, 'learning_rate': 3.348173868276473e-05, 'batch_size': 99, 'step_size': 6, 'gamma': 0.9571282292862568}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:41:54,364][0m Trial 18 finished with value: 0.1401380826333526 and parameters: {'observation_period_num': 114, 'train_rates': 0.9448971444863066, 'learning_rate': 5.175368072806256e-06, 'batch_size': 108, 'step_size': 4, 'gamma': 0.9153505077447258}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:44:31,610][0m Trial 19 finished with value: 0.15738649231395083 and parameters: {'observation_period_num': 248, 'train_rates': 0.9228807341188087, 'learning_rate': 0.0009604441949796643, 'batch_size': 89, 'step_size': 6, 'gamma': 0.9544890175359872}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:47:46,193][0m Trial 20 finished with value: 0.7990780716422219 and parameters: {'observation_period_num': 35, 'train_rates': 0.8365551493237773, 'learning_rate': 1.1659037438214596e-06, 'batch_size': 124, 'step_size': 1, 'gamma': 0.9065306311654724}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:53:04,378][0m Trial 21 finished with value: 0.03659939756731928 and parameters: {'observation_period_num': 7, 'train_rates': 0.9726214692006085, 'learning_rate': 3.4463386085842074e-05, 'batch_size': 40, 'step_size': 8, 'gamma': 0.96735662156304}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 02:57:10,191][0m Trial 22 finished with value: 0.04160765453594402 and parameters: {'observation_period_num': 33, 'train_rates': 0.9294800025871521, 'learning_rate': 3.1236766136721844e-05, 'batch_size': 52, 'step_size': 6, 'gamma': 0.9684689717698768}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:01:28,619][0m Trial 23 finished with value: 0.04772781580686569 and parameters: {'observation_period_num': 19, 'train_rates': 0.9891385964923498, 'learning_rate': 1.670283451941457e-05, 'batch_size': 82, 'step_size': 7, 'gamma': 0.9372406611239711}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:05:57,450][0m Trial 24 finished with value: 0.08200286210015201 and parameters: {'observation_period_num': 42, 'train_rates': 0.8577105999083332, 'learning_rate': 4.605464067272038e-05, 'batch_size': 39, 'step_size': 8, 'gamma': 0.9885924928733978}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:08:53,004][0m Trial 25 finished with value: 0.2256143854797951 and parameters: {'observation_period_num': 88, 'train_rates': 0.7654118465595247, 'learning_rate': 2.5958613871106716e-05, 'batch_size': 76, 'step_size': 3, 'gamma': 0.9659440420619129}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:12:03,275][0m Trial 26 finished with value: 0.07894279363941639 and parameters: {'observation_period_num': 50, 'train_rates': 0.9516993902880727, 'learning_rate': 6.991913697085131e-06, 'batch_size': 114, 'step_size': 9, 'gamma': 0.8706708864475508}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:17:04,580][0m Trial 27 finished with value: 0.03512232490163997 and parameters: {'observation_period_num': 22, 'train_rates': 0.9094868894695347, 'learning_rate': 5.818047388485594e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9243572109337584}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:20:29,014][0m Trial 28 finished with value: 0.034850267729922835 and parameters: {'observation_period_num': 22, 'train_rates': 0.9129893838297365, 'learning_rate': 5.991739019523335e-05, 'batch_size': 106, 'step_size': 5, 'gamma': 0.9218420006330512}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:23:17,366][0m Trial 29 finished with value: 0.07892862300711324 and parameters: {'observation_period_num': 166, 'train_rates': 0.9101061631615692, 'learning_rate': 0.00016742238161137832, 'batch_size': 141, 'step_size': 5, 'gamma': 0.9002447230907702}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:26:15,382][0m Trial 30 finished with value: 0.07640080353421336 and parameters: {'observation_period_num': 98, 'train_rates': 0.7927518022808698, 'learning_rate': 6.24803039629793e-05, 'batch_size': 65, 'step_size': 3, 'gamma': 0.923773904548831}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:29:10,388][0m Trial 31 finished with value: 0.038814409886205624 and parameters: {'observation_period_num': 22, 'train_rates': 0.8768488342094088, 'learning_rate': 0.00010162738858472467, 'batch_size': 101, 'step_size': 6, 'gamma': 0.9486522974026923}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:31:49,479][0m Trial 32 finished with value: 0.05330438956088228 and parameters: {'observation_period_num': 23, 'train_rates': 0.913904142079264, 'learning_rate': 2.0536430985637103e-05, 'batch_size': 155, 'step_size': 5, 'gamma': 0.8766811699460862}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:34:22,973][0m Trial 33 finished with value: 0.062053855167578234 and parameters: {'observation_period_num': 57, 'train_rates': 0.8492195694157387, 'learning_rate': 5.24666331968202e-05, 'batch_size': 251, 'step_size': 5, 'gamma': 0.9342253260187786}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:37:27,098][0m Trial 34 finished with value: 0.048873746316440356 and parameters: {'observation_period_num': 45, 'train_rates': 0.8892737423260355, 'learning_rate': 0.0002482266766858948, 'batch_size': 100, 'step_size': 7, 'gamma': 0.9151290792552597}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:40:48,622][0m Trial 35 finished with value: 0.11527954484185865 and parameters: {'observation_period_num': 75, 'train_rates': 0.9462451260804776, 'learning_rate': 1.1871802024085352e-05, 'batch_size': 74, 'step_size': 2, 'gamma': 0.8849115277779435}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:43:29,859][0m Trial 36 finished with value: 0.03726482321855737 and parameters: {'observation_period_num': 22, 'train_rates': 0.8627900985671109, 'learning_rate': 7.740702276413335e-05, 'batch_size': 121, 'step_size': 7, 'gamma': 0.9561721860269833}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:48:20,125][0m Trial 37 finished with value: 0.05608659788318302 and parameters: {'observation_period_num': 62, 'train_rates': 0.9284367558857632, 'learning_rate': 0.00030322170979765615, 'batch_size': 36, 'step_size': 8, 'gamma': 0.8594783410773829}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:51:38,367][0m Trial 38 finished with value: 0.08112912814186983 and parameters: {'observation_period_num': 146, 'train_rates': 0.8836624303042022, 'learning_rate': 0.00012346887072197377, 'batch_size': 56, 'step_size': 3, 'gamma': 0.9255318038812279}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:54:22,531][0m Trial 39 finished with value: 0.043406661446254276 and parameters: {'observation_period_num': 19, 'train_rates': 0.9010260340414881, 'learning_rate': 4.696374709861678e-05, 'batch_size': 158, 'step_size': 11, 'gamma': 0.9067588607267264}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 03:57:12,033][0m Trial 40 finished with value: 0.2478932123169426 and parameters: {'observation_period_num': 83, 'train_rates': 0.7046946217684927, 'learning_rate': 2.169168675517359e-05, 'batch_size': 91, 'step_size': 5, 'gamma': 0.8316961219819324}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:03:02,894][0m Trial 41 finished with value: 0.03510068716891741 and parameters: {'observation_period_num': 6, 'train_rates': 0.9602007015496992, 'learning_rate': 3.8052715014252905e-05, 'batch_size': 34, 'step_size': 6, 'gamma': 0.7501028095330979}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:08:57,781][0m Trial 42 finished with value: 0.03210963726206852 and parameters: {'observation_period_num': 8, 'train_rates': 0.953740907032276, 'learning_rate': 3.925371761145154e-05, 'batch_size': 32, 'step_size': 6, 'gamma': 0.7912954538345894}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:15:10,342][0m Trial 43 finished with value: 0.04293396056164056 and parameters: {'observation_period_num': 29, 'train_rates': 0.9562069532121813, 'learning_rate': 6.61508656669678e-05, 'batch_size': 30, 'step_size': 4, 'gamma': 0.7909412388862183}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:25:36,867][0m Trial 44 finished with value: 0.03190588698018598 and parameters: {'observation_period_num': 15, 'train_rates': 0.9556313443548539, 'learning_rate': 8.473231790759454e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.7602047049593672}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:36:08,641][0m Trial 45 finished with value: 0.045039630410346115 and parameters: {'observation_period_num': 5, 'train_rates': 0.9890557084748371, 'learning_rate': 0.00014862559707145163, 'batch_size': 16, 'step_size': 10, 'gamma': 0.7557030889663}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:42:01,366][0m Trial 46 finished with value: 0.06021433822267378 and parameters: {'observation_period_num': 47, 'train_rates': 0.96704482541797, 'learning_rate': 0.0001004323283467564, 'batch_size': 31, 'step_size': 4, 'gamma': 0.7566080300902329}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:49:55,097][0m Trial 47 finished with value: 0.03351014086959559 and parameters: {'observation_period_num': 17, 'train_rates': 0.9631236547050063, 'learning_rate': 3.887185887746031e-05, 'batch_size': 22, 'step_size': 8, 'gamma': 0.7757914664654196}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 04:54:08,844][0m Trial 48 finished with value: 0.0862296543577138 and parameters: {'observation_period_num': 68, 'train_rates': 0.9708208757878458, 'learning_rate': 0.0004767504693858713, 'batch_size': 49, 'step_size': 13, 'gamma': 0.804626018806596}. Best is trial 10 with value: 0.03147755443033847.[0m
[32m[I 2025-01-06 05:01:19,376][0m Trial 49 finished with value: 0.11746099224516765 and parameters: {'observation_period_num': 188, 'train_rates': 0.9303531704512046, 'learning_rate': 8.329428858916447e-05, 'batch_size': 22, 'step_size': 9, 'gamma': 0.7778033445921064}. Best is trial 10 with value: 0.03147755443033847.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-06 05:01:19,387][0m A new study created in memory with name: no-name-f306fc62-0fa9-4b36-b71f-f77850af10ce[0m
[32m[I 2025-01-06 05:03:36,049][0m Trial 0 finished with value: 0.6730642386224128 and parameters: {'observation_period_num': 205, 'train_rates': 0.620646745220619, 'learning_rate': 1.2786984024251066e-06, 'batch_size': 184, 'step_size': 14, 'gamma': 0.9412323887661767}. Best is trial 0 with value: 0.6730642386224128.[0m
[32m[I 2025-01-06 05:06:46,445][0m Trial 1 finished with value: 0.3112131953239441 and parameters: {'observation_period_num': 116, 'train_rates': 0.9697503089037336, 'learning_rate': 1.9966670245772344e-06, 'batch_size': 231, 'step_size': 9, 'gamma': 0.9733924810390264}. Best is trial 1 with value: 0.3112131953239441.[0m
[32m[I 2025-01-06 05:10:54,015][0m Trial 2 finished with value: 0.2193809698814946 and parameters: {'observation_period_num': 99, 'train_rates': 0.6967349821993679, 'learning_rate': 0.00038157866510179676, 'batch_size': 39, 'step_size': 8, 'gamma': 0.7622520378421548}. Best is trial 2 with value: 0.2193809698814946.[0m
[32m[I 2025-01-06 05:13:17,428][0m Trial 3 finished with value: 0.12137118184905567 and parameters: {'observation_period_num': 176, 'train_rates': 0.8797126170419218, 'learning_rate': 8.211686092850035e-05, 'batch_size': 191, 'step_size': 11, 'gamma': 0.912733293184717}. Best is trial 3 with value: 0.12137118184905567.[0m
[32m[I 2025-01-06 05:15:50,566][0m Trial 4 finished with value: 0.08626811749467225 and parameters: {'observation_period_num': 134, 'train_rates': 0.8860283954773007, 'learning_rate': 8.764787079634176e-06, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9460483841301953}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:17:32,636][0m Trial 5 finished with value: 0.22181310342026392 and parameters: {'observation_period_num': 154, 'train_rates': 0.6250271240636943, 'learning_rate': 0.0005072031491529059, 'batch_size': 162, 'step_size': 13, 'gamma': 0.8551241058279944}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:19:48,138][0m Trial 6 finished with value: 0.3212914357634739 and parameters: {'observation_period_num': 193, 'train_rates': 0.9309795649807172, 'learning_rate': 2.3154445495033926e-06, 'batch_size': 154, 'step_size': 10, 'gamma': 0.7784059309782612}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:21:19,471][0m Trial 7 finished with value: 0.48352103999265844 and parameters: {'observation_period_num': 221, 'train_rates': 0.6259422211901186, 'learning_rate': 4.783946055302419e-06, 'batch_size': 240, 'step_size': 7, 'gamma': 0.9189944297491772}. Best is trial 4 with value: 0.08626811749467225.[0m
Early stopping at epoch 98
[32m[I 2025-01-06 05:26:30,061][0m Trial 8 finished with value: 0.30751841114333617 and parameters: {'observation_period_num': 168, 'train_rates': 0.6186299928897111, 'learning_rate': 0.0002976766983854256, 'batch_size': 22, 'step_size': 1, 'gamma': 0.7976996569233192}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:29:54,536][0m Trial 9 finished with value: 0.13254688345799548 and parameters: {'observation_period_num': 17, 'train_rates': 0.6206279571936175, 'learning_rate': 5.768096929975004e-05, 'batch_size': 39, 'step_size': 6, 'gamma': 0.9250602720822123}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:32:13,359][0m Trial 10 finished with value: 0.09863582883256575 and parameters: {'observation_period_num': 56, 'train_rates': 0.8124973120781889, 'learning_rate': 1.1922374272451158e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8605748145587289}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:34:52,435][0m Trial 11 finished with value: 0.09799943995252948 and parameters: {'observation_period_num': 58, 'train_rates': 0.8151908163760088, 'learning_rate': 1.4542295930976768e-05, 'batch_size': 96, 'step_size': 3, 'gamma': 0.8531408916837311}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:37:03,231][0m Trial 12 finished with value: 0.09104716177372371 and parameters: {'observation_period_num': 72, 'train_rates': 0.8229359317289991, 'learning_rate': 1.539233104910985e-05, 'batch_size': 97, 'step_size': 4, 'gamma': 0.8270430386823933}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:39:18,863][0m Trial 13 finished with value: 0.261022792494559 and parameters: {'observation_period_num': 84, 'train_rates': 0.7462065210480042, 'learning_rate': 1.8379121391560715e-05, 'batch_size': 100, 'step_size': 5, 'gamma': 0.8226670095762719}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:41:42,744][0m Trial 14 finished with value: 0.10809639671610462 and parameters: {'observation_period_num': 144, 'train_rates': 0.8718613942859027, 'learning_rate': 5.960393371454818e-06, 'batch_size': 122, 'step_size': 4, 'gamma': 0.9814955640717762}. Best is trial 4 with value: 0.08626811749467225.[0m
Early stopping at epoch 64
[32m[I 2025-01-06 05:43:22,197][0m Trial 15 finished with value: 0.16776253955036985 and parameters: {'observation_period_num': 244, 'train_rates': 0.8682247518664383, 'learning_rate': 8.300390681324733e-05, 'batch_size': 64, 'step_size': 1, 'gamma': 0.8194611832423527}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:45:48,235][0m Trial 16 finished with value: 0.19576433386610842 and parameters: {'observation_period_num': 16, 'train_rates': 0.7630834505234879, 'learning_rate': 5.75635888418734e-06, 'batch_size': 70, 'step_size': 6, 'gamma': 0.8887614209552616}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:48:15,981][0m Trial 17 finished with value: 0.08666387969671294 and parameters: {'observation_period_num': 123, 'train_rates': 0.9241832719599414, 'learning_rate': 3.1492759480810255e-05, 'batch_size': 117, 'step_size': 3, 'gamma': 0.8908932505484902}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:50:39,108][0m Trial 18 finished with value: 0.09199466556310654 and parameters: {'observation_period_num': 121, 'train_rates': 0.9889733814140585, 'learning_rate': 3.7664682473079186e-05, 'batch_size': 134, 'step_size': 3, 'gamma': 0.9490237359712621}. Best is trial 4 with value: 0.08626811749467225.[0m
[32m[I 2025-01-06 05:52:53,617][0m Trial 19 finished with value: 0.08273858740433769 and parameters: {'observation_period_num': 137, 'train_rates': 0.9246987011508631, 'learning_rate': 0.00013962089146608985, 'batch_size': 121, 'step_size': 7, 'gamma': 0.8913765556989349}. Best is trial 19 with value: 0.08273858740433769.[0m
[32m[I 2025-01-06 05:55:13,774][0m Trial 20 finished with value: 0.12091485561276766 and parameters: {'observation_period_num': 157, 'train_rates': 0.9204057356801061, 'learning_rate': 0.000188885628690908, 'batch_size': 166, 'step_size': 12, 'gamma': 0.9615038795209523}. Best is trial 19 with value: 0.08273858740433769.[0m
[32m[I 2025-01-06 05:57:50,786][0m Trial 21 finished with value: 0.10544798101382281 and parameters: {'observation_period_num': 133, 'train_rates': 0.9332445697287397, 'learning_rate': 0.0009787230582455936, 'batch_size': 122, 'step_size': 7, 'gamma': 0.8896434744545649}. Best is trial 19 with value: 0.08273858740433769.[0m
[32m[I 2025-01-06 06:00:42,877][0m Trial 22 finished with value: 0.07601615657956999 and parameters: {'observation_period_num': 104, 'train_rates': 0.901539008899599, 'learning_rate': 0.00016551407609428188, 'batch_size': 73, 'step_size': 5, 'gamma': 0.8926113370994899}. Best is trial 22 with value: 0.07601615657956999.[0m
[32m[I 2025-01-06 06:03:41,728][0m Trial 23 finished with value: 0.08989041846158892 and parameters: {'observation_period_num': 103, 'train_rates': 0.8491635314695687, 'learning_rate': 0.00016292599624115214, 'batch_size': 71, 'step_size': 8, 'gamma': 0.9013488050904849}. Best is trial 22 with value: 0.07601615657956999.[0m
[32m[I 2025-01-06 06:07:01,183][0m Trial 24 finished with value: 0.07980111572477552 and parameters: {'observation_period_num': 90, 'train_rates': 0.8994786168915593, 'learning_rate': 0.0001539486042611042, 'batch_size': 56, 'step_size': 6, 'gamma': 0.9320912715396104}. Best is trial 22 with value: 0.07601615657956999.[0m
[32m[I 2025-01-06 06:10:35,262][0m Trial 25 finished with value: 0.07877130179028762 and parameters: {'observation_period_num': 38, 'train_rates': 0.9610667260153397, 'learning_rate': 0.00014584888130237766, 'batch_size': 54, 'step_size': 9, 'gamma': 0.8768847222092824}. Best is trial 22 with value: 0.07601615657956999.[0m
[32m[I 2025-01-06 06:14:32,942][0m Trial 26 finished with value: 0.042261730943774355 and parameters: {'observation_period_num': 29, 'train_rates': 0.9604992462439632, 'learning_rate': 0.000736460027560885, 'batch_size': 46, 'step_size': 10, 'gamma': 0.8714911595910471}. Best is trial 26 with value: 0.042261730943774355.[0m
[32m[I 2025-01-06 06:22:05,873][0m Trial 27 finished with value: 0.05695119760930538 and parameters: {'observation_period_num': 32, 'train_rates': 0.9625887835020176, 'learning_rate': 0.0009308268615529294, 'batch_size': 22, 'step_size': 10, 'gamma': 0.8419434186528562}. Best is trial 26 with value: 0.042261730943774355.[0m
[32m[I 2025-01-06 06:31:36,325][0m Trial 28 finished with value: 0.03136474126577377 and parameters: {'observation_period_num': 7, 'train_rates': 0.9578159912077565, 'learning_rate': 0.0008465695872684265, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8374362589588797}. Best is trial 28 with value: 0.03136474126577377.[0m
[32m[I 2025-01-06 06:41:50,414][0m Trial 29 finished with value: 0.030528535308167997 and parameters: {'observation_period_num': 7, 'train_rates': 0.9616576871082235, 'learning_rate': 0.0009773697098823264, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8359037662687613}. Best is trial 29 with value: 0.030528535308167997.[0m
[32m[I 2025-01-06 06:51:26,596][0m Trial 30 finished with value: 0.03543321410535087 and parameters: {'observation_period_num': 5, 'train_rates': 0.9760538475211024, 'learning_rate': 0.000498166380625361, 'batch_size': 18, 'step_size': 15, 'gamma': 0.8055591436707734}. Best is trial 29 with value: 0.030528535308167997.[0m
[32m[I 2025-01-06 07:00:39,389][0m Trial 31 finished with value: 0.02396488312099661 and parameters: {'observation_period_num': 7, 'train_rates': 0.9762909679870211, 'learning_rate': 0.0006094601430873331, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8029019476141832}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:11:14,790][0m Trial 32 finished with value: 0.04425927975348064 and parameters: {'observation_period_num': 9, 'train_rates': 0.988194701195334, 'learning_rate': 0.0005727515168155571, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8009322840742763}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:16:49,546][0m Trial 33 finished with value: 0.04136350492636363 and parameters: {'observation_period_num': 5, 'train_rates': 0.9490672051381723, 'learning_rate': 0.00035633728091205374, 'batch_size': 32, 'step_size': 15, 'gamma': 0.8039928426260036}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:20:18,648][0m Trial 34 finished with value: 0.056534167379140854 and parameters: {'observation_period_num': 51, 'train_rates': 0.9896701530598209, 'learning_rate': 0.0002661353389802348, 'batch_size': 208, 'step_size': 14, 'gamma': 0.7764122116692437}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:25:51,043][0m Trial 35 finished with value: 0.052644952833652496 and parameters: {'observation_period_num': 40, 'train_rates': 0.9484794659136588, 'learning_rate': 0.00047885594371252266, 'batch_size': 36, 'step_size': 14, 'gamma': 0.7532216168948753}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:35:29,561][0m Trial 36 finished with value: 0.035085834983375766 and parameters: {'observation_period_num': 22, 'train_rates': 0.9050208301953585, 'learning_rate': 0.0006547140992333441, 'batch_size': 17, 'step_size': 13, 'gamma': 0.8327227643081045}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:41:19,695][0m Trial 37 finished with value: 0.03317106494473086 and parameters: {'observation_period_num': 26, 'train_rates': 0.9017195729289424, 'learning_rate': 0.0006742774115480585, 'batch_size': 32, 'step_size': 13, 'gamma': 0.8362930912068405}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:45:20,607][0m Trial 38 finished with value: 0.07322640078408378 and parameters: {'observation_period_num': 73, 'train_rates': 0.8420581698574073, 'learning_rate': 0.0002349165543289352, 'batch_size': 46, 'step_size': 12, 'gamma': 0.7843708054473303}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:50:54,634][0m Trial 39 finished with value: 0.05018512929479281 and parameters: {'observation_period_num': 44, 'train_rates': 0.9383403235462092, 'learning_rate': 0.00041079384414080387, 'batch_size': 33, 'step_size': 13, 'gamma': 0.8448052577420533}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:53:11,761][0m Trial 40 finished with value: 0.16046117603974908 and parameters: {'observation_period_num': 26, 'train_rates': 0.688126073951667, 'learning_rate': 0.0008459418920055187, 'batch_size': 84, 'step_size': 12, 'gamma': 0.8160582972358522}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 07:58:47,283][0m Trial 41 finished with value: 0.035310040919050094 and parameters: {'observation_period_num': 21, 'train_rates': 0.9048402250512081, 'learning_rate': 0.0006282572156727023, 'batch_size': 30, 'step_size': 13, 'gamma': 0.8394388511139338}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:08:44,336][0m Trial 42 finished with value: 0.03828351805965464 and parameters: {'observation_period_num': 20, 'train_rates': 0.8864503572343891, 'learning_rate': 0.0007358258289231604, 'batch_size': 16, 'step_size': 14, 'gamma': 0.8322920982032808}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:12:48,065][0m Trial 43 finished with value: 0.08064226066903846 and parameters: {'observation_period_num': 65, 'train_rates': 0.9110733472503757, 'learning_rate': 0.0003655647844827514, 'batch_size': 46, 'step_size': 14, 'gamma': 0.7903261980463095}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:19:15,060][0m Trial 44 finished with value: 0.05842393127152289 and parameters: {'observation_period_num': 49, 'train_rates': 0.9701420783851514, 'learning_rate': 0.000550082077763541, 'batch_size': 28, 'step_size': 11, 'gamma': 0.8630447166989522}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:23:18,862][0m Trial 45 finished with value: 0.04858926707242109 and parameters: {'observation_period_num': 33, 'train_rates': 0.9458704540220048, 'learning_rate': 9.961143522234179e-05, 'batch_size': 56, 'step_size': 13, 'gamma': 0.8501356608658552}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:26:08,376][0m Trial 46 finished with value: 0.03771152450907521 and parameters: {'observation_period_num': 14, 'train_rates': 0.8603674771519263, 'learning_rate': 0.00034569970598818353, 'batch_size': 253, 'step_size': 15, 'gamma': 0.8123046789227326}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:30:38,866][0m Trial 47 finished with value: 0.04304965645699444 and parameters: {'observation_period_num': 25, 'train_rates': 0.8860614488627785, 'learning_rate': 0.0006732414244332814, 'batch_size': 42, 'step_size': 14, 'gamma': 0.8334105637591942}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:36:38,705][0m Trial 48 finished with value: 0.17800346793897645 and parameters: {'observation_period_num': 5, 'train_rates': 0.7803722784287312, 'learning_rate': 0.0009855202761990635, 'batch_size': 27, 'step_size': 11, 'gamma': 0.8271902846301529}. Best is trial 31 with value: 0.02396488312099661.[0m
[32m[I 2025-01-06 08:40:45,843][0m Trial 49 finished with value: 0.18475253172552905 and parameters: {'observation_period_num': 63, 'train_rates': 0.6911203172078257, 'learning_rate': 0.0002197508576477627, 'batch_size': 38, 'step_size': 13, 'gamma': 0.8600005007220741}. Best is trial 31 with value: 0.02396488312099661.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-06 08:40:45,852][0m A new study created in memory with name: no-name-60ec7803-390a-4c29-b6a4-a72aaab00d6e[0m
[32m[I 2025-01-06 08:43:27,939][0m Trial 0 finished with value: 0.1260600434815907 and parameters: {'observation_period_num': 215, 'train_rates': 0.8127775881077244, 'learning_rate': 7.948742851500192e-05, 'batch_size': 227, 'step_size': 9, 'gamma': 0.7749109518735897}. Best is trial 0 with value: 0.1260600434815907.[0m
[32m[I 2025-01-06 08:45:52,593][0m Trial 1 finished with value: 0.2959834076223213 and parameters: {'observation_period_num': 181, 'train_rates': 0.7209011818187032, 'learning_rate': 0.0006163198763533101, 'batch_size': 215, 'step_size': 14, 'gamma': 0.8111270732152018}. Best is trial 0 with value: 0.1260600434815907.[0m
[32m[I 2025-01-06 08:48:34,513][0m Trial 2 finished with value: 0.4356093189200839 and parameters: {'observation_period_num': 155, 'train_rates': 0.6559985960510893, 'learning_rate': 2.796314307290021e-06, 'batch_size': 130, 'step_size': 13, 'gamma': 0.8792684770098824}. Best is trial 0 with value: 0.1260600434815907.[0m
[32m[I 2025-01-06 08:52:05,088][0m Trial 3 finished with value: 0.3436687737379854 and parameters: {'observation_period_num': 165, 'train_rates': 0.752012686198426, 'learning_rate': 0.000727755822194914, 'batch_size': 47, 'step_size': 6, 'gamma': 0.9034624618853775}. Best is trial 0 with value: 0.1260600434815907.[0m
[32m[I 2025-01-06 08:54:44,868][0m Trial 4 finished with value: 0.13405469714777585 and parameters: {'observation_period_num': 14, 'train_rates': 0.6400475092891836, 'learning_rate': 6.282283405016886e-05, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8783893494157603}. Best is trial 0 with value: 0.1260600434815907.[0m
[32m[I 2025-01-06 08:58:20,123][0m Trial 5 finished with value: 0.1245091386989459 and parameters: {'observation_period_num': 95, 'train_rates': 0.8436006632960271, 'learning_rate': 2.1436376409635295e-06, 'batch_size': 57, 'step_size': 15, 'gamma': 0.822669763419088}. Best is trial 5 with value: 0.1245091386989459.[0m
[32m[I 2025-01-06 09:01:14,318][0m Trial 6 finished with value: 0.2574446201324463 and parameters: {'observation_period_num': 78, 'train_rates': 0.939562152737441, 'learning_rate': 1.6129417957977671e-06, 'batch_size': 222, 'step_size': 11, 'gamma': 0.9728638260857699}. Best is trial 5 with value: 0.1245091386989459.[0m
[32m[I 2025-01-06 09:03:50,635][0m Trial 7 finished with value: 0.7277275046476951 and parameters: {'observation_period_num': 106, 'train_rates': 0.6802385446733301, 'learning_rate': 1.292435717475512e-06, 'batch_size': 147, 'step_size': 6, 'gamma': 0.8161473682997253}. Best is trial 5 with value: 0.1245091386989459.[0m
[32m[I 2025-01-06 09:06:40,364][0m Trial 8 finished with value: 0.4849233455517713 and parameters: {'observation_period_num': 43, 'train_rates': 0.8833592794377174, 'learning_rate': 1.6423019943313823e-06, 'batch_size': 196, 'step_size': 3, 'gamma': 0.7575062881740859}. Best is trial 5 with value: 0.1245091386989459.[0m
[32m[I 2025-01-06 09:09:35,731][0m Trial 9 finished with value: 0.31176644563674927 and parameters: {'observation_period_num': 83, 'train_rates': 0.9575797266453079, 'learning_rate': 1.480333659666134e-06, 'batch_size': 224, 'step_size': 8, 'gamma': 0.9307509718344809}. Best is trial 5 with value: 0.1245091386989459.[0m
Early stopping at epoch 76
[32m[I 2025-01-06 09:13:02,527][0m Trial 10 finished with value: 0.26464581852924635 and parameters: {'observation_period_num': 251, 'train_rates': 0.8345480738036406, 'learning_rate': 9.316607393577438e-06, 'batch_size': 35, 'step_size': 1, 'gamma': 0.8324664822060158}. Best is trial 5 with value: 0.1245091386989459.[0m
[32m[I 2025-01-06 09:15:53,765][0m Trial 11 finished with value: 0.10869880404716693 and parameters: {'observation_period_num': 247, 'train_rates': 0.8216311010338003, 'learning_rate': 7.127353477115221e-05, 'batch_size': 79, 'step_size': 10, 'gamma': 0.7530366061224496}. Best is trial 11 with value: 0.10869880404716693.[0m
[32m[I 2025-01-06 09:19:11,324][0m Trial 12 finished with value: 0.07256878650337045 and parameters: {'observation_period_num': 115, 'train_rates': 0.8729681012403508, 'learning_rate': 2.0556616891392452e-05, 'batch_size': 81, 'step_size': 15, 'gamma': 0.7819783669166404}. Best is trial 12 with value: 0.07256878650337045.[0m
[32m[I 2025-01-06 09:22:29,132][0m Trial 13 finished with value: 0.08592575749146267 and parameters: {'observation_period_num': 130, 'train_rates': 0.8993646285731273, 'learning_rate': 2.0051825935560445e-05, 'batch_size': 89, 'step_size': 10, 'gamma': 0.7824183045731302}. Best is trial 12 with value: 0.07256878650337045.[0m
[32m[I 2025-01-06 09:26:03,926][0m Trial 14 finished with value: 0.0844780788894444 and parameters: {'observation_period_num': 128, 'train_rates': 0.9161839493697091, 'learning_rate': 1.6751612954184194e-05, 'batch_size': 94, 'step_size': 12, 'gamma': 0.7912181358221172}. Best is trial 12 with value: 0.07256878650337045.[0m
[32m[I 2025-01-06 09:29:22,649][0m Trial 15 finished with value: 0.1123846173286438 and parameters: {'observation_period_num': 127, 'train_rates': 0.9811022244408512, 'learning_rate': 8.494441125035043e-06, 'batch_size': 98, 'step_size': 12, 'gamma': 0.8469814656586965}. Best is trial 12 with value: 0.07256878650337045.[0m
[32m[I 2025-01-06 09:39:01,503][0m Trial 16 finished with value: 0.0784626948631416 and parameters: {'observation_period_num': 58, 'train_rates': 0.8981612352253032, 'learning_rate': 0.00018205690155291912, 'batch_size': 16, 'step_size': 15, 'gamma': 0.7878966422080033}. Best is trial 12 with value: 0.07256878650337045.[0m
[32m[I 2025-01-06 09:47:28,235][0m Trial 17 finished with value: 0.21687825354761628 and parameters: {'observation_period_num': 50, 'train_rates': 0.773236164317731, 'learning_rate': 0.0002463962760141525, 'batch_size': 16, 'step_size': 15, 'gamma': 0.8588652268131004}. Best is trial 12 with value: 0.07256878650337045.[0m
[32m[I 2025-01-06 09:49:25,239][0m Trial 18 finished with value: 0.02990841556959646 and parameters: {'observation_period_num': 5, 'train_rates': 0.8754219466403381, 'learning_rate': 0.0002093614812893629, 'batch_size': 176, 'step_size': 15, 'gamma': 0.7963531487610382}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 09:51:11,909][0m Trial 19 finished with value: 0.052810130270311624 and parameters: {'observation_period_num': 24, 'train_rates': 0.8712766087571928, 'learning_rate': 3.551788734851321e-05, 'batch_size': 167, 'step_size': 7, 'gamma': 0.7996572904300897}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 09:53:16,759][0m Trial 20 finished with value: 0.03921836696473169 and parameters: {'observation_period_num': 18, 'train_rates': 0.861981133877362, 'learning_rate': 0.0002304483791356192, 'batch_size': 166, 'step_size': 6, 'gamma': 0.8026919090408248}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 09:56:20,014][0m Trial 21 finished with value: 0.03687485244385814 and parameters: {'observation_period_num': 7, 'train_rates': 0.849581027777143, 'learning_rate': 0.0002901149017504541, 'batch_size': 168, 'step_size': 6, 'gamma': 0.8053520164474922}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 09:59:04,707][0m Trial 22 finished with value: 0.17831151373684406 and parameters: {'observation_period_num': 9, 'train_rates': 0.7723966739795843, 'learning_rate': 0.0002959401096602121, 'batch_size': 255, 'step_size': 4, 'gamma': 0.8379807480337034}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:02:05,314][0m Trial 23 finished with value: 0.05028554714192146 and parameters: {'observation_period_num': 31, 'train_rates': 0.8557470440625138, 'learning_rate': 0.000141982561020053, 'batch_size': 179, 'step_size': 4, 'gamma': 0.8072311785105285}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:05:08,024][0m Trial 24 finished with value: 0.04091529164959568 and parameters: {'observation_period_num': 7, 'train_rates': 0.7985580129281414, 'learning_rate': 0.00038345577686455754, 'batch_size': 149, 'step_size': 5, 'gamma': 0.7670142854131808}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:08:31,309][0m Trial 25 finished with value: 0.060433413833379745 and parameters: {'observation_period_num': 64, 'train_rates': 0.939461697381152, 'learning_rate': 0.0009311284816791769, 'batch_size': 185, 'step_size': 8, 'gamma': 0.851607507450505}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:11:15,594][0m Trial 26 finished with value: 0.19469674843365067 and parameters: {'observation_period_num': 36, 'train_rates': 0.6012990276306467, 'learning_rate': 0.00012805241240277228, 'batch_size': 158, 'step_size': 2, 'gamma': 0.9046453298006895}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:14:01,660][0m Trial 27 finished with value: 0.16365294176152642 and parameters: {'observation_period_num': 27, 'train_rates': 0.7336214870577803, 'learning_rate': 0.0004721736546546844, 'batch_size': 196, 'step_size': 6, 'gamma': 0.8276378772807009}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:17:23,476][0m Trial 28 finished with value: 0.057901538780574635 and parameters: {'observation_period_num': 66, 'train_rates': 0.9192373030770569, 'learning_rate': 0.00011159213625857219, 'batch_size': 126, 'step_size': 7, 'gamma': 0.8023251449706704}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:20:23,218][0m Trial 29 finished with value: 0.05708878613892213 and parameters: {'observation_period_num': 9, 'train_rates': 0.8038668337858791, 'learning_rate': 4.4665911391712574e-05, 'batch_size': 244, 'step_size': 9, 'gamma': 0.7663667787880866}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:23:10,398][0m Trial 30 finished with value: 0.1012134775519371 and parameters: {'observation_period_num': 194, 'train_rates': 0.8530448345554156, 'learning_rate': 0.00020672051049772992, 'batch_size': 203, 'step_size': 4, 'gamma': 0.7753625311328672}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:25:17,600][0m Trial 31 finished with value: 0.040881964553348756 and parameters: {'observation_period_num': 5, 'train_rates': 0.792668172123397, 'learning_rate': 0.00047489532820050255, 'batch_size': 157, 'step_size': 5, 'gamma': 0.7509573163161137}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:26:52,356][0m Trial 32 finished with value: 0.044702449827703813 and parameters: {'observation_period_num': 24, 'train_rates': 0.8210186944556349, 'learning_rate': 0.0003471485505906878, 'batch_size': 180, 'step_size': 5, 'gamma': 0.7501802120723575}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:28:37,623][0m Trial 33 finished with value: 0.2005830410493725 and parameters: {'observation_period_num': 45, 'train_rates': 0.7704693277007221, 'learning_rate': 0.0005881472240087525, 'batch_size': 169, 'step_size': 7, 'gamma': 0.7945984635238841}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:30:35,535][0m Trial 34 finished with value: 0.06809551966020731 and parameters: {'observation_period_num': 23, 'train_rates': 0.7899930784472305, 'learning_rate': 0.0009315418040065527, 'batch_size': 143, 'step_size': 5, 'gamma': 0.8128468415788154}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:32:54,117][0m Trial 35 finished with value: 0.17394705745646782 and parameters: {'observation_period_num': 7, 'train_rates': 0.7212447288855037, 'learning_rate': 9.282687485353656e-05, 'batch_size': 113, 'step_size': 3, 'gamma': 0.7723316167051274}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:34:56,360][0m Trial 36 finished with value: 0.0449837993437867 and parameters: {'observation_period_num': 37, 'train_rates': 0.8471780007207123, 'learning_rate': 0.0005618076512011529, 'batch_size': 135, 'step_size': 9, 'gamma': 0.8672354048337275}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:36:38,560][0m Trial 37 finished with value: 0.18994665523118612 and parameters: {'observation_period_num': 77, 'train_rates': 0.6976502930244229, 'learning_rate': 0.00017364582240710807, 'batch_size': 206, 'step_size': 6, 'gamma': 0.8195644015703433}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:38:38,046][0m Trial 38 finished with value: 0.11279489349767652 and parameters: {'observation_period_num': 154, 'train_rates': 0.8147086004146, 'learning_rate': 0.0002821094873996907, 'batch_size': 160, 'step_size': 7, 'gamma': 0.8977886256986378}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:41:09,314][0m Trial 39 finished with value: 0.039229466980640365 and parameters: {'observation_period_num': 18, 'train_rates': 0.8901415937814823, 'learning_rate': 5.6886419346055585e-05, 'batch_size': 113, 'step_size': 3, 'gamma': 0.955039324903868}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:43:27,147][0m Trial 40 finished with value: 0.05910534303515188 and parameters: {'observation_period_num': 51, 'train_rates': 0.8931507350110036, 'learning_rate': 4.45891704973073e-05, 'batch_size': 110, 'step_size': 1, 'gamma': 0.9661593201401183}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:45:32,481][0m Trial 41 finished with value: 0.038550717279661535 and parameters: {'observation_period_num': 16, 'train_rates': 0.8683907291549361, 'learning_rate': 0.0004306489904794124, 'batch_size': 153, 'step_size': 3, 'gamma': 0.9885443269332238}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:47:43,136][0m Trial 42 finished with value: 0.04872001896698758 and parameters: {'observation_period_num': 18, 'train_rates': 0.867832463672075, 'learning_rate': 0.00022322331315068267, 'batch_size': 137, 'step_size': 3, 'gamma': 0.9860114948392603}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:50:01,411][0m Trial 43 finished with value: 0.048949256009387175 and parameters: {'observation_period_num': 38, 'train_rates': 0.9179521404877002, 'learning_rate': 7.919173757912505e-05, 'batch_size': 124, 'step_size': 2, 'gamma': 0.9510490727680475}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:52:08,720][0m Trial 44 finished with value: 0.039422351866960526 and parameters: {'observation_period_num': 19, 'train_rates': 0.9445154079862782, 'learning_rate': 0.000421269613602613, 'batch_size': 172, 'step_size': 2, 'gamma': 0.988571000933259}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:53:54,116][0m Trial 45 finished with value: 0.10452019362961602 and parameters: {'observation_period_num': 32, 'train_rates': 0.8351573559696674, 'learning_rate': 4.62871765417662e-06, 'batch_size': 191, 'step_size': 3, 'gamma': 0.9371308977790213}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:55:47,335][0m Trial 46 finished with value: 0.08328350194703753 and parameters: {'observation_period_num': 87, 'train_rates': 0.8880067768662042, 'learning_rate': 5.732264180382332e-05, 'batch_size': 148, 'step_size': 14, 'gamma': 0.9706379255581102}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:57:26,711][0m Trial 47 finished with value: 0.15432942814884648 and parameters: {'observation_period_num': 228, 'train_rates': 0.8634357302460868, 'learning_rate': 0.0007638185467541855, 'batch_size': 211, 'step_size': 6, 'gamma': 0.9566077092573193}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 10:59:17,269][0m Trial 48 finished with value: 0.03182494262154035 and parameters: {'observation_period_num': 16, 'train_rates': 0.8320968990589334, 'learning_rate': 0.00014520954480435786, 'batch_size': 107, 'step_size': 4, 'gamma': 0.9173932626276285}. Best is trial 18 with value: 0.02990841556959646.[0m
[32m[I 2025-01-06 11:01:05,864][0m Trial 49 finished with value: 0.04401196544271668 and parameters: {'observation_period_num': 44, 'train_rates': 0.8423511478771384, 'learning_rate': 0.00015157955856847781, 'batch_size': 174, 'step_size': 8, 'gamma': 0.8892858298971156}. Best is trial 18 with value: 0.02990841556959646.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AMZN_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 21, 'train_rates': 0.9660894205097864, 'learning_rate': 0.0003010715173492714, 'batch_size': 42, 'step_size': 15, 'gamma': 0.8282156122908964}
Epoch 1/300, trend Loss: 0.4786 | 0.2934
Epoch 2/300, trend Loss: 0.2148 | 0.1592
Epoch 3/300, trend Loss: 0.1608 | 0.1183
Epoch 4/300, trend Loss: 0.1404 | 0.0976
Epoch 5/300, trend Loss: 0.1275 | 0.0862
Epoch 6/300, trend Loss: 0.1203 | 0.0817
Epoch 7/300, trend Loss: 0.1171 | 0.0822
Epoch 8/300, trend Loss: 0.1187 | 0.0799
Epoch 9/300, trend Loss: 0.1192 | 0.0746
Epoch 10/300, trend Loss: 0.1163 | 0.0718
Epoch 11/300, trend Loss: 0.1077 | 0.0783
Epoch 12/300, trend Loss: 0.1051 | 0.0715
Epoch 13/300, trend Loss: 0.1028 | 0.0669
Epoch 14/300, trend Loss: 0.1006 | 0.0647
Epoch 15/300, trend Loss: 0.0990 | 0.0633
Epoch 16/300, trend Loss: 0.0981 | 0.0650
Epoch 17/300, trend Loss: 0.0966 | 0.0638
Epoch 18/300, trend Loss: 0.0944 | 0.0609
Epoch 19/300, trend Loss: 0.0932 | 0.0596
Epoch 20/300, trend Loss: 0.0922 | 0.0587
Epoch 21/300, trend Loss: 0.0911 | 0.0580
Epoch 22/300, trend Loss: 0.0902 | 0.0573
Epoch 23/300, trend Loss: 0.0894 | 0.0567
Epoch 24/300, trend Loss: 0.0889 | 0.0609
Epoch 25/300, trend Loss: 0.0882 | 0.0603
Epoch 26/300, trend Loss: 0.0866 | 0.0578
Epoch 27/300, trend Loss: 0.0857 | 0.0562
Epoch 28/300, trend Loss: 0.0850 | 0.0553
Epoch 29/300, trend Loss: 0.0844 | 0.0547
Epoch 30/300, trend Loss: 0.0837 | 0.0541
Epoch 31/300, trend Loss: 0.0829 | 0.0565
Epoch 32/300, trend Loss: 0.0822 | 0.0552
Epoch 33/300, trend Loss: 0.0817 | 0.0541
Epoch 34/300, trend Loss: 0.0811 | 0.0533
Epoch 35/300, trend Loss: 0.0806 | 0.0527
Epoch 36/300, trend Loss: 0.0801 | 0.0521
Epoch 37/300, trend Loss: 0.0795 | 0.0515
Epoch 38/300, trend Loss: 0.0790 | 0.0510
Epoch 39/300, trend Loss: 0.0785 | 0.0541
Epoch 40/300, trend Loss: 0.0779 | 0.0526
Epoch 41/300, trend Loss: 0.0774 | 0.0510
Epoch 42/300, trend Loss: 0.0771 | 0.0500
Epoch 43/300, trend Loss: 0.0768 | 0.0494
Epoch 44/300, trend Loss: 0.0765 | 0.0489
Epoch 45/300, trend Loss: 0.0761 | 0.0485
Epoch 46/300, trend Loss: 0.0756 | 0.0487
Epoch 47/300, trend Loss: 0.0752 | 0.0478
Epoch 48/300, trend Loss: 0.0750 | 0.0472
Epoch 49/300, trend Loss: 0.0747 | 0.0467
Epoch 50/300, trend Loss: 0.0744 | 0.0463
Epoch 51/300, trend Loss: 0.0741 | 0.0460
Epoch 52/300, trend Loss: 0.0738 | 0.0457
Epoch 53/300, trend Loss: 0.0735 | 0.0455
Epoch 54/300, trend Loss: 0.0731 | 0.0449
Epoch 55/300, trend Loss: 0.0728 | 0.0444
Epoch 56/300, trend Loss: 0.0726 | 0.0440
Epoch 57/300, trend Loss: 0.0723 | 0.0437
Epoch 58/300, trend Loss: 0.0721 | 0.0435
Epoch 59/300, trend Loss: 0.0718 | 0.0432
Epoch 60/300, trend Loss: 0.0716 | 0.0430
Epoch 61/300, trend Loss: 0.0714 | 0.0432
Epoch 62/300, trend Loss: 0.0713 | 0.0436
Epoch 63/300, trend Loss: 0.0711 | 0.0442
Epoch 64/300, trend Loss: 0.0711 | 0.0448
Epoch 65/300, trend Loss: 0.0711 | 0.0455
Epoch 66/300, trend Loss: 0.0712 | 0.0461
Epoch 67/300, trend Loss: 0.0716 | 0.0465
Epoch 68/300, trend Loss: 0.0725 | 0.0459
Epoch 69/300, trend Loss: 0.0742 | 0.0498
Epoch 70/300, trend Loss: 0.0753 | 0.0504
Epoch 71/300, trend Loss: 0.0716 | 0.0457
Epoch 72/300, trend Loss: 0.0710 | 0.0431
Epoch 73/300, trend Loss: 0.0712 | 0.0426
Epoch 74/300, trend Loss: 0.0709 | 0.0425
Epoch 75/300, trend Loss: 0.0707 | 0.0425
Epoch 76/300, trend Loss: 0.0705 | 0.0417
Epoch 77/300, trend Loss: 0.0702 | 0.0414
Epoch 78/300, trend Loss: 0.0700 | 0.0413
Epoch 79/300, trend Loss: 0.0697 | 0.0412
Epoch 80/300, trend Loss: 0.0694 | 0.0412
Epoch 81/300, trend Loss: 0.0692 | 0.0411
Epoch 82/300, trend Loss: 0.0690 | 0.0410
Epoch 83/300, trend Loss: 0.0687 | 0.0409
Epoch 84/300, trend Loss: 0.0685 | 0.0405
Epoch 85/300, trend Loss: 0.0683 | 0.0403
Epoch 86/300, trend Loss: 0.0681 | 0.0402
Epoch 87/300, trend Loss: 0.0679 | 0.0402
Epoch 88/300, trend Loss: 0.0677 | 0.0401
Epoch 89/300, trend Loss: 0.0676 | 0.0400
Epoch 90/300, trend Loss: 0.0675 | 0.0400
Epoch 91/300, trend Loss: 0.0673 | 0.0397
Epoch 92/300, trend Loss: 0.0672 | 0.0395
Epoch 93/300, trend Loss: 0.0671 | 0.0394
Epoch 94/300, trend Loss: 0.0670 | 0.0394
Epoch 95/300, trend Loss: 0.0669 | 0.0393
Epoch 96/300, trend Loss: 0.0668 | 0.0393
Epoch 97/300, trend Loss: 0.0667 | 0.0392
Epoch 98/300, trend Loss: 0.0667 | 0.0391
Epoch 99/300, trend Loss: 0.0666 | 0.0389
Epoch 100/300, trend Loss: 0.0665 | 0.0387
Epoch 101/300, trend Loss: 0.0664 | 0.0387
Epoch 102/300, trend Loss: 0.0664 | 0.0387
Epoch 103/300, trend Loss: 0.0663 | 0.0386
Epoch 104/300, trend Loss: 0.0663 | 0.0386
Epoch 105/300, trend Loss: 0.0662 | 0.0385
Epoch 106/300, trend Loss: 0.0662 | 0.0382
Epoch 107/300, trend Loss: 0.0661 | 0.0381
Epoch 108/300, trend Loss: 0.0660 | 0.0381
Epoch 109/300, trend Loss: 0.0660 | 0.0380
Epoch 110/300, trend Loss: 0.0660 | 0.0380
Epoch 111/300, trend Loss: 0.0659 | 0.0379
Epoch 112/300, trend Loss: 0.0659 | 0.0379
Epoch 113/300, trend Loss: 0.0658 | 0.0378
Epoch 114/300, trend Loss: 0.0658 | 0.0375
Epoch 115/300, trend Loss: 0.0657 | 0.0374
Epoch 116/300, trend Loss: 0.0657 | 0.0374
Epoch 117/300, trend Loss: 0.0657 | 0.0373
Epoch 118/300, trend Loss: 0.0656 | 0.0373
Epoch 119/300, trend Loss: 0.0656 | 0.0372
Epoch 120/300, trend Loss: 0.0655 | 0.0372
Epoch 121/300, trend Loss: 0.0655 | 0.0373
Epoch 122/300, trend Loss: 0.0655 | 0.0373
Epoch 123/300, trend Loss: 0.0654 | 0.0373
Epoch 124/300, trend Loss: 0.0654 | 0.0373
Epoch 125/300, trend Loss: 0.0653 | 0.0373
Epoch 126/300, trend Loss: 0.0653 | 0.0373
Epoch 127/300, trend Loss: 0.0653 | 0.0373
Epoch 128/300, trend Loss: 0.0652 | 0.0373
Epoch 129/300, trend Loss: 0.0651 | 0.0374
Epoch 130/300, trend Loss: 0.0652 | 0.0375
Epoch 131/300, trend Loss: 0.0651 | 0.0374
Epoch 132/300, trend Loss: 0.0651 | 0.0374
Epoch 133/300, trend Loss: 0.0651 | 0.0373
Epoch 134/300, trend Loss: 0.0650 | 0.0372
Epoch 135/300, trend Loss: 0.0649 | 0.0371
Epoch 136/300, trend Loss: 0.0649 | 0.0369
Epoch 137/300, trend Loss: 0.0648 | 0.0368
Epoch 138/300, trend Loss: 0.0648 | 0.0368
Epoch 139/300, trend Loss: 0.0647 | 0.0367
Epoch 140/300, trend Loss: 0.0647 | 0.0367
Epoch 141/300, trend Loss: 0.0646 | 0.0366
Epoch 142/300, trend Loss: 0.0646 | 0.0366
Epoch 143/300, trend Loss: 0.0646 | 0.0366
Epoch 144/300, trend Loss: 0.0645 | 0.0365
Epoch 145/300, trend Loss: 0.0645 | 0.0364
Epoch 146/300, trend Loss: 0.0644 | 0.0364
Epoch 147/300, trend Loss: 0.0644 | 0.0364
Epoch 148/300, trend Loss: 0.0644 | 0.0364
Epoch 149/300, trend Loss: 0.0644 | 0.0363
Epoch 150/300, trend Loss: 0.0643 | 0.0363
Epoch 151/300, trend Loss: 0.0643 | 0.0363
Epoch 152/300, trend Loss: 0.0642 | 0.0362
Epoch 153/300, trend Loss: 0.0642 | 0.0362
Epoch 154/300, trend Loss: 0.0642 | 0.0362
Epoch 155/300, trend Loss: 0.0642 | 0.0362
Epoch 156/300, trend Loss: 0.0642 | 0.0362
Epoch 157/300, trend Loss: 0.0641 | 0.0361
Epoch 158/300, trend Loss: 0.0641 | 0.0361
Epoch 159/300, trend Loss: 0.0641 | 0.0361
Epoch 160/300, trend Loss: 0.0640 | 0.0361
Epoch 161/300, trend Loss: 0.0640 | 0.0361
Epoch 162/300, trend Loss: 0.0640 | 0.0360
Epoch 163/300, trend Loss: 0.0640 | 0.0360
Epoch 164/300, trend Loss: 0.0640 | 0.0360
Epoch 165/300, trend Loss: 0.0640 | 0.0360
Epoch 166/300, trend Loss: 0.0639 | 0.0360
Epoch 167/300, trend Loss: 0.0639 | 0.0360
Epoch 168/300, trend Loss: 0.0639 | 0.0360
Epoch 169/300, trend Loss: 0.0639 | 0.0359
Epoch 170/300, trend Loss: 0.0639 | 0.0359
Epoch 171/300, trend Loss: 0.0639 | 0.0359
Epoch 172/300, trend Loss: 0.0638 | 0.0359
Epoch 173/300, trend Loss: 0.0638 | 0.0359
Epoch 174/300, trend Loss: 0.0638 | 0.0359
Epoch 175/300, trend Loss: 0.0638 | 0.0359
Epoch 176/300, trend Loss: 0.0638 | 0.0359
Epoch 177/300, trend Loss: 0.0638 | 0.0359
Epoch 178/300, trend Loss: 0.0638 | 0.0358
Epoch 179/300, trend Loss: 0.0638 | 0.0358
Epoch 180/300, trend Loss: 0.0637 | 0.0358
Epoch 181/300, trend Loss: 0.0637 | 0.0358
Epoch 182/300, trend Loss: 0.0637 | 0.0358
Epoch 183/300, trend Loss: 0.0637 | 0.0358
Epoch 184/300, trend Loss: 0.0637 | 0.0358
Epoch 185/300, trend Loss: 0.0637 | 0.0358
Epoch 186/300, trend Loss: 0.0637 | 0.0358
Epoch 187/300, trend Loss: 0.0637 | 0.0358
Epoch 188/300, trend Loss: 0.0637 | 0.0358
Epoch 189/300, trend Loss: 0.0636 | 0.0358
Epoch 190/300, trend Loss: 0.0636 | 0.0357
Epoch 191/300, trend Loss: 0.0636 | 0.0357
Epoch 192/300, trend Loss: 0.0636 | 0.0357
Epoch 193/300, trend Loss: 0.0636 | 0.0357
Epoch 194/300, trend Loss: 0.0636 | 0.0357
Epoch 195/300, trend Loss: 0.0636 | 0.0357
Epoch 196/300, trend Loss: 0.0636 | 0.0357
Epoch 197/300, trend Loss: 0.0636 | 0.0357
Epoch 198/300, trend Loss: 0.0636 | 0.0357
Epoch 199/300, trend Loss: 0.0636 | 0.0357
Epoch 200/300, trend Loss: 0.0635 | 0.0357
Epoch 201/300, trend Loss: 0.0635 | 0.0357
Epoch 202/300, trend Loss: 0.0635 | 0.0357
Epoch 203/300, trend Loss: 0.0635 | 0.0357
Epoch 204/300, trend Loss: 0.0635 | 0.0357
Epoch 205/300, trend Loss: 0.0635 | 0.0357
Epoch 206/300, trend Loss: 0.0635 | 0.0357
Epoch 207/300, trend Loss: 0.0635 | 0.0357
Epoch 208/300, trend Loss: 0.0635 | 0.0356
Epoch 209/300, trend Loss: 0.0635 | 0.0356
Epoch 210/300, trend Loss: 0.0635 | 0.0356
Epoch 211/300, trend Loss: 0.0635 | 0.0356
Epoch 212/300, trend Loss: 0.0635 | 0.0356
Epoch 213/300, trend Loss: 0.0635 | 0.0356
Epoch 214/300, trend Loss: 0.0635 | 0.0356
Epoch 215/300, trend Loss: 0.0635 | 0.0356
Epoch 216/300, trend Loss: 0.0635 | 0.0356
Epoch 217/300, trend Loss: 0.0634 | 0.0356
Epoch 218/300, trend Loss: 0.0634 | 0.0356
Epoch 219/300, trend Loss: 0.0634 | 0.0356
Epoch 220/300, trend Loss: 0.0634 | 0.0356
Epoch 221/300, trend Loss: 0.0634 | 0.0356
Epoch 222/300, trend Loss: 0.0634 | 0.0356
Epoch 223/300, trend Loss: 0.0634 | 0.0356
Epoch 224/300, trend Loss: 0.0634 | 0.0356
Epoch 225/300, trend Loss: 0.0634 | 0.0356
Epoch 226/300, trend Loss: 0.0634 | 0.0356
Epoch 227/300, trend Loss: 0.0634 | 0.0356
Epoch 228/300, trend Loss: 0.0634 | 0.0356
Epoch 229/300, trend Loss: 0.0634 | 0.0356
Epoch 230/300, trend Loss: 0.0634 | 0.0356
Epoch 231/300, trend Loss: 0.0634 | 0.0356
Epoch 232/300, trend Loss: 0.0634 | 0.0356
Epoch 233/300, trend Loss: 0.0634 | 0.0356
Epoch 234/300, trend Loss: 0.0634 | 0.0356
Epoch 235/300, trend Loss: 0.0634 | 0.0356
Epoch 236/300, trend Loss: 0.0634 | 0.0356
Epoch 237/300, trend Loss: 0.0634 | 0.0356
Epoch 238/300, trend Loss: 0.0634 | 0.0356
Epoch 239/300, trend Loss: 0.0634 | 0.0356
Epoch 240/300, trend Loss: 0.0634 | 0.0355
Epoch 241/300, trend Loss: 0.0634 | 0.0355
Epoch 242/300, trend Loss: 0.0634 | 0.0355
Epoch 243/300, trend Loss: 0.0634 | 0.0355
Epoch 244/300, trend Loss: 0.0634 | 0.0355
Epoch 245/300, trend Loss: 0.0633 | 0.0355
Epoch 246/300, trend Loss: 0.0633 | 0.0355
Epoch 247/300, trend Loss: 0.0633 | 0.0355
Epoch 248/300, trend Loss: 0.0633 | 0.0355
Epoch 249/300, trend Loss: 0.0633 | 0.0355
Epoch 250/300, trend Loss: 0.0633 | 0.0355
Epoch 251/300, trend Loss: 0.0633 | 0.0355
Epoch 252/300, trend Loss: 0.0633 | 0.0355
Epoch 253/300, trend Loss: 0.0633 | 0.0355
Epoch 254/300, trend Loss: 0.0633 | 0.0355
Epoch 255/300, trend Loss: 0.0633 | 0.0355
Epoch 256/300, trend Loss: 0.0633 | 0.0355
Epoch 257/300, trend Loss: 0.0633 | 0.0355
Epoch 258/300, trend Loss: 0.0633 | 0.0355
Epoch 259/300, trend Loss: 0.0633 | 0.0355
Epoch 260/300, trend Loss: 0.0633 | 0.0355
Epoch 261/300, trend Loss: 0.0633 | 0.0355
Epoch 262/300, trend Loss: 0.0633 | 0.0355
Epoch 263/300, trend Loss: 0.0633 | 0.0355
Epoch 264/300, trend Loss: 0.0633 | 0.0355
Epoch 265/300, trend Loss: 0.0633 | 0.0355
Epoch 266/300, trend Loss: 0.0633 | 0.0355
Epoch 267/300, trend Loss: 0.0633 | 0.0355
Epoch 268/300, trend Loss: 0.0633 | 0.0355
Epoch 269/300, trend Loss: 0.0633 | 0.0355
Epoch 270/300, trend Loss: 0.0633 | 0.0355
Epoch 271/300, trend Loss: 0.0633 | 0.0355
Epoch 272/300, trend Loss: 0.0633 | 0.0355
Epoch 273/300, trend Loss: 0.0633 | 0.0355
Epoch 274/300, trend Loss: 0.0633 | 0.0355
Epoch 275/300, trend Loss: 0.0633 | 0.0355
Epoch 276/300, trend Loss: 0.0633 | 0.0355
Epoch 277/300, trend Loss: 0.0633 | 0.0355
Epoch 278/300, trend Loss: 0.0633 | 0.0355
Epoch 279/300, trend Loss: 0.0633 | 0.0355
Epoch 280/300, trend Loss: 0.0633 | 0.0355
Epoch 281/300, trend Loss: 0.0633 | 0.0355
Epoch 282/300, trend Loss: 0.0633 | 0.0355
Epoch 283/300, trend Loss: 0.0633 | 0.0355
Epoch 284/300, trend Loss: 0.0633 | 0.0355
Epoch 285/300, trend Loss: 0.0633 | 0.0355
Epoch 286/300, trend Loss: 0.0633 | 0.0355
Epoch 287/300, trend Loss: 0.0633 | 0.0355
Epoch 288/300, trend Loss: 0.0633 | 0.0355
Epoch 289/300, trend Loss: 0.0633 | 0.0355
Epoch 290/300, trend Loss: 0.0633 | 0.0355
Epoch 291/300, trend Loss: 0.0633 | 0.0355
Epoch 292/300, trend Loss: 0.0633 | 0.0355
Epoch 293/300, trend Loss: 0.0633 | 0.0355
Epoch 294/300, trend Loss: 0.0633 | 0.0355
Epoch 295/300, trend Loss: 0.0633 | 0.0355
Epoch 296/300, trend Loss: 0.0633 | 0.0355
Epoch 297/300, trend Loss: 0.0633 | 0.0355
Epoch 298/300, trend Loss: 0.0633 | 0.0355
Epoch 299/300, trend Loss: 0.0633 | 0.0355
Epoch 300/300, trend Loss: 0.0633 | 0.0355
Training seasonal_0 component with params: {'observation_period_num': 5, 'train_rates': 0.9069483227435597, 'learning_rate': 0.00013080095431673434, 'batch_size': 74, 'step_size': 11, 'gamma': 0.7618033529001876}
Epoch 1/300, seasonal_0 Loss: 0.5226 | 0.1074
Epoch 2/300, seasonal_0 Loss: 0.1557 | 0.0967
Epoch 3/300, seasonal_0 Loss: 0.1282 | 0.0731
Epoch 4/300, seasonal_0 Loss: 0.1213 | 0.0903
Epoch 5/300, seasonal_0 Loss: 0.1129 | 0.0779
Epoch 6/300, seasonal_0 Loss: 0.1081 | 0.0731
Epoch 7/300, seasonal_0 Loss: 0.1051 | 0.0759
Epoch 8/300, seasonal_0 Loss: 0.1007 | 0.0575
Epoch 9/300, seasonal_0 Loss: 0.1003 | 0.0586
Epoch 10/300, seasonal_0 Loss: 0.0992 | 0.0565
Epoch 11/300, seasonal_0 Loss: 0.0989 | 0.0538
Epoch 12/300, seasonal_0 Loss: 0.1021 | 0.0473
Epoch 13/300, seasonal_0 Loss: 0.1032 | 0.0484
Epoch 14/300, seasonal_0 Loss: 0.1072 | 0.0542
Epoch 15/300, seasonal_0 Loss: 0.0980 | 0.0481
Epoch 16/300, seasonal_0 Loss: 0.0923 | 0.0514
Epoch 17/300, seasonal_0 Loss: 0.0887 | 0.0449
Epoch 18/300, seasonal_0 Loss: 0.0996 | 0.0700
Epoch 19/300, seasonal_0 Loss: 0.0879 | 0.0591
Epoch 20/300, seasonal_0 Loss: 0.0899 | 0.0588
Epoch 21/300, seasonal_0 Loss: 0.0950 | 0.0543
Epoch 22/300, seasonal_0 Loss: 0.0927 | 0.0500
Epoch 23/300, seasonal_0 Loss: 0.0954 | 0.0444
Epoch 24/300, seasonal_0 Loss: 0.1005 | 0.0500
Epoch 25/300, seasonal_0 Loss: 0.0997 | 0.0457
Epoch 26/300, seasonal_0 Loss: 0.0954 | 0.0532
Epoch 27/300, seasonal_0 Loss: 0.1031 | 0.0443
Epoch 28/300, seasonal_0 Loss: 0.0866 | 0.0407
Epoch 29/300, seasonal_0 Loss: 0.0896 | 0.0407
Epoch 30/300, seasonal_0 Loss: 0.0857 | 0.0484
Epoch 31/300, seasonal_0 Loss: 0.0797 | 0.0453
Epoch 32/300, seasonal_0 Loss: 0.0774 | 0.0395
Epoch 33/300, seasonal_0 Loss: 0.0737 | 0.0385
Epoch 34/300, seasonal_0 Loss: 0.0737 | 0.0389
Epoch 35/300, seasonal_0 Loss: 0.0730 | 0.0396
Epoch 36/300, seasonal_0 Loss: 0.0733 | 0.0385
Epoch 37/300, seasonal_0 Loss: 0.0724 | 0.0381
Epoch 38/300, seasonal_0 Loss: 0.0723 | 0.0379
Epoch 39/300, seasonal_0 Loss: 0.0724 | 0.0398
Epoch 40/300, seasonal_0 Loss: 0.0729 | 0.0357
Epoch 41/300, seasonal_0 Loss: 0.0737 | 0.0355
Epoch 42/300, seasonal_0 Loss: 0.0727 | 0.0354
Epoch 43/300, seasonal_0 Loss: 0.0707 | 0.0353
Epoch 44/300, seasonal_0 Loss: 0.0696 | 0.0358
Epoch 45/300, seasonal_0 Loss: 0.0691 | 0.0365
Epoch 46/300, seasonal_0 Loss: 0.0690 | 0.0365
Epoch 47/300, seasonal_0 Loss: 0.0688 | 0.0369
Epoch 48/300, seasonal_0 Loss: 0.0686 | 0.0372
Epoch 49/300, seasonal_0 Loss: 0.0689 | 0.0377
Epoch 50/300, seasonal_0 Loss: 0.0694 | 0.0395
Epoch 51/300, seasonal_0 Loss: 0.0706 | 0.0354
Epoch 52/300, seasonal_0 Loss: 0.0717 | 0.0355
Epoch 53/300, seasonal_0 Loss: 0.0732 | 0.0364
Epoch 54/300, seasonal_0 Loss: 0.0736 | 0.0368
Epoch 55/300, seasonal_0 Loss: 0.0735 | 0.0366
Epoch 56/300, seasonal_0 Loss: 0.0738 | 0.0349
Epoch 57/300, seasonal_0 Loss: 0.0726 | 0.0343
Epoch 58/300, seasonal_0 Loss: 0.0723 | 0.0370
Epoch 59/300, seasonal_0 Loss: 0.0726 | 0.0385
Epoch 60/300, seasonal_0 Loss: 0.0717 | 0.0351
Epoch 61/300, seasonal_0 Loss: 0.0721 | 0.0353
Epoch 62/300, seasonal_0 Loss: 0.0743 | 0.0395
Epoch 63/300, seasonal_0 Loss: 0.0725 | 0.0364
Epoch 64/300, seasonal_0 Loss: 0.0693 | 0.0354
Epoch 65/300, seasonal_0 Loss: 0.0678 | 0.0336
Epoch 66/300, seasonal_0 Loss: 0.0663 | 0.0343
Epoch 67/300, seasonal_0 Loss: 0.0656 | 0.0342
Epoch 68/300, seasonal_0 Loss: 0.0652 | 0.0337
Epoch 69/300, seasonal_0 Loss: 0.0648 | 0.0332
Epoch 70/300, seasonal_0 Loss: 0.0648 | 0.0329
Epoch 71/300, seasonal_0 Loss: 0.0649 | 0.0328
Epoch 72/300, seasonal_0 Loss: 0.0648 | 0.0327
Epoch 73/300, seasonal_0 Loss: 0.0648 | 0.0325
Epoch 74/300, seasonal_0 Loss: 0.0646 | 0.0329
Epoch 75/300, seasonal_0 Loss: 0.0649 | 0.0340
Epoch 76/300, seasonal_0 Loss: 0.0645 | 0.0334
Epoch 77/300, seasonal_0 Loss: 0.0639 | 0.0333
Epoch 78/300, seasonal_0 Loss: 0.0639 | 0.0330
Epoch 79/300, seasonal_0 Loss: 0.0640 | 0.0330
Epoch 80/300, seasonal_0 Loss: 0.0641 | 0.0328
Epoch 81/300, seasonal_0 Loss: 0.0640 | 0.0326
Epoch 82/300, seasonal_0 Loss: 0.0636 | 0.0326
Epoch 83/300, seasonal_0 Loss: 0.0633 | 0.0325
Epoch 84/300, seasonal_0 Loss: 0.0631 | 0.0326
Epoch 85/300, seasonal_0 Loss: 0.0631 | 0.0328
Epoch 86/300, seasonal_0 Loss: 0.0631 | 0.0329
Epoch 87/300, seasonal_0 Loss: 0.0630 | 0.0328
Epoch 88/300, seasonal_0 Loss: 0.0629 | 0.0327
Epoch 89/300, seasonal_0 Loss: 0.0628 | 0.0326
Epoch 90/300, seasonal_0 Loss: 0.0627 | 0.0325
Epoch 91/300, seasonal_0 Loss: 0.0627 | 0.0325
Epoch 92/300, seasonal_0 Loss: 0.0627 | 0.0325
Epoch 93/300, seasonal_0 Loss: 0.0626 | 0.0325
Epoch 94/300, seasonal_0 Loss: 0.0626 | 0.0325
Epoch 95/300, seasonal_0 Loss: 0.0625 | 0.0326
Epoch 96/300, seasonal_0 Loss: 0.0625 | 0.0326
Epoch 97/300, seasonal_0 Loss: 0.0624 | 0.0325
Epoch 98/300, seasonal_0 Loss: 0.0624 | 0.0325
Epoch 99/300, seasonal_0 Loss: 0.0624 | 0.0325
Epoch 100/300, seasonal_0 Loss: 0.0623 | 0.0325
Epoch 101/300, seasonal_0 Loss: 0.0623 | 0.0325
Epoch 102/300, seasonal_0 Loss: 0.0623 | 0.0325
Epoch 103/300, seasonal_0 Loss: 0.0623 | 0.0325
Epoch 104/300, seasonal_0 Loss: 0.0622 | 0.0325
Epoch 105/300, seasonal_0 Loss: 0.0622 | 0.0324
Epoch 106/300, seasonal_0 Loss: 0.0622 | 0.0324
Epoch 107/300, seasonal_0 Loss: 0.0622 | 0.0324
Epoch 108/300, seasonal_0 Loss: 0.0621 | 0.0324
Epoch 109/300, seasonal_0 Loss: 0.0621 | 0.0324
Epoch 110/300, seasonal_0 Loss: 0.0621 | 0.0324
Epoch 111/300, seasonal_0 Loss: 0.0621 | 0.0324
Epoch 112/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 113/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 114/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 115/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 116/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 117/300, seasonal_0 Loss: 0.0620 | 0.0324
Epoch 118/300, seasonal_0 Loss: 0.0619 | 0.0324
Epoch 119/300, seasonal_0 Loss: 0.0619 | 0.0324
Epoch 120/300, seasonal_0 Loss: 0.0619 | 0.0324
Epoch 121/300, seasonal_0 Loss: 0.0619 | 0.0324
Epoch 122/300, seasonal_0 Loss: 0.0619 | 0.0323
Epoch 123/300, seasonal_0 Loss: 0.0619 | 0.0323
Epoch 124/300, seasonal_0 Loss: 0.0619 | 0.0323
Epoch 125/300, seasonal_0 Loss: 0.0619 | 0.0323
Epoch 126/300, seasonal_0 Loss: 0.0619 | 0.0323
Epoch 127/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 128/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 129/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 130/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 131/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 132/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 133/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 134/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 135/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 136/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 137/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 138/300, seasonal_0 Loss: 0.0618 | 0.0323
Epoch 139/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 140/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 141/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 142/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 143/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 144/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 145/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 146/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 147/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 148/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 149/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 150/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 151/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 152/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 153/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 154/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 155/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 156/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 157/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 158/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 159/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 160/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 161/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 162/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 163/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 164/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 165/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 166/300, seasonal_0 Loss: 0.0617 | 0.0323
Epoch 167/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 168/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 169/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 170/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 171/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 172/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 173/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 174/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 175/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 176/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 177/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 178/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 179/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 180/300, seasonal_0 Loss: 0.0616 | 0.0323
Epoch 181/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 182/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 183/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 184/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 185/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 186/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 187/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 188/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 189/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 190/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 191/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 192/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 193/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 194/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 195/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 196/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 197/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 198/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 199/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 200/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 201/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 202/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 203/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 204/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 205/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 206/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 207/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 208/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 209/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 210/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 211/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 212/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 213/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 214/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 215/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 216/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 217/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 218/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 219/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 220/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 221/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 222/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 223/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 224/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 225/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 226/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 227/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 228/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 229/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 230/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 231/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 232/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 233/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 234/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 235/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 236/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 237/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 238/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 239/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 240/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 241/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 242/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 243/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 244/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 245/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 246/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 247/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 248/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 249/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 250/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 251/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 252/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 253/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 254/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 255/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 256/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 257/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 258/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 259/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 260/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 261/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 262/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 263/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 264/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 265/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 266/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 267/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 268/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 269/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 270/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 271/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 272/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 273/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 274/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 275/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 276/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 277/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 278/300, seasonal_0 Loss: 0.0616 | 0.0322
Epoch 279/300, seasonal_0 Loss: 0.0616 | 0.0322
Early stopping for seasonal_0
Training seasonal_1 component with params: {'observation_period_num': 19, 'train_rates': 0.8928163234319546, 'learning_rate': 0.0002561402649349968, 'batch_size': 228, 'step_size': 13, 'gamma': 0.8126581878471357}
Epoch 1/300, seasonal_1 Loss: 1.7148 | 0.4436
Epoch 2/300, seasonal_1 Loss: 0.4136 | 0.3228
Epoch 3/300, seasonal_1 Loss: 0.5004 | 0.7145
Epoch 4/300, seasonal_1 Loss: 0.4012 | 0.3772
Epoch 5/300, seasonal_1 Loss: 0.3030 | 0.2541
Epoch 6/300, seasonal_1 Loss: 0.2481 | 0.2192
Epoch 7/300, seasonal_1 Loss: 0.2580 | 0.1830
Epoch 8/300, seasonal_1 Loss: 0.4357 | 0.1961
Epoch 9/300, seasonal_1 Loss: 0.3617 | 0.3593
Epoch 10/300, seasonal_1 Loss: 0.2535 | 0.2302
Epoch 11/300, seasonal_1 Loss: 0.1897 | 0.1461
Epoch 12/300, seasonal_1 Loss: 0.1739 | 0.1441
Epoch 13/300, seasonal_1 Loss: 0.2447 | 0.2157
Epoch 14/300, seasonal_1 Loss: 0.2216 | 0.1228
Epoch 15/300, seasonal_1 Loss: 0.2292 | 0.0951
Epoch 16/300, seasonal_1 Loss: 0.2098 | 0.1721
Epoch 17/300, seasonal_1 Loss: 0.1618 | 0.1226
Epoch 18/300, seasonal_1 Loss: 0.1747 | 0.1341
Epoch 19/300, seasonal_1 Loss: 0.1918 | 0.1324
Epoch 20/300, seasonal_1 Loss: 0.2133 | 0.1436
Epoch 21/300, seasonal_1 Loss: 0.1314 | 0.0952
Epoch 22/300, seasonal_1 Loss: 0.1368 | 0.1816
Epoch 23/300, seasonal_1 Loss: 0.1414 | 0.1161
Epoch 24/300, seasonal_1 Loss: 0.1201 | 0.0730
Epoch 25/300, seasonal_1 Loss: 0.1166 | 0.0671
Epoch 26/300, seasonal_1 Loss: 0.1158 | 0.0661
Epoch 27/300, seasonal_1 Loss: 0.1165 | 0.0699
Epoch 28/300, seasonal_1 Loss: 0.1075 | 0.0630
Epoch 29/300, seasonal_1 Loss: 0.1131 | 0.0648
Epoch 30/300, seasonal_1 Loss: 0.1163 | 0.0725
Epoch 31/300, seasonal_1 Loss: 0.1113 | 0.0738
Epoch 32/300, seasonal_1 Loss: 0.1051 | 0.0664
Epoch 33/300, seasonal_1 Loss: 0.1079 | 0.0585
Epoch 34/300, seasonal_1 Loss: 0.1058 | 0.0668
Epoch 35/300, seasonal_1 Loss: 0.0973 | 0.0669
Epoch 36/300, seasonal_1 Loss: 0.0932 | 0.0559
Epoch 37/300, seasonal_1 Loss: 0.0916 | 0.0569
Epoch 38/300, seasonal_1 Loss: 0.0899 | 0.0700
Epoch 39/300, seasonal_1 Loss: 0.0902 | 0.0595
Epoch 40/300, seasonal_1 Loss: 0.0887 | 0.0540
Epoch 41/300, seasonal_1 Loss: 0.0880 | 0.0604
Epoch 42/300, seasonal_1 Loss: 0.0868 | 0.0557
Epoch 43/300, seasonal_1 Loss: 0.0845 | 0.0558
Epoch 44/300, seasonal_1 Loss: 0.0858 | 0.0506
Epoch 45/300, seasonal_1 Loss: 0.0829 | 0.0533
Epoch 46/300, seasonal_1 Loss: 0.0836 | 0.0565
Epoch 47/300, seasonal_1 Loss: 0.0839 | 0.0469
Epoch 48/300, seasonal_1 Loss: 0.0836 | 0.0573
Epoch 49/300, seasonal_1 Loss: 0.0825 | 0.0473
Epoch 50/300, seasonal_1 Loss: 0.0814 | 0.0493
Epoch 51/300, seasonal_1 Loss: 0.0805 | 0.0472
Epoch 52/300, seasonal_1 Loss: 0.0799 | 0.0536
Epoch 53/300, seasonal_1 Loss: 0.0805 | 0.0464
Epoch 54/300, seasonal_1 Loss: 0.0779 | 0.0445
Epoch 55/300, seasonal_1 Loss: 0.0793 | 0.0543
Epoch 56/300, seasonal_1 Loss: 0.0789 | 0.0417
Epoch 57/300, seasonal_1 Loss: 0.0805 | 0.0603
Epoch 58/300, seasonal_1 Loss: 0.0809 | 0.0424
Epoch 59/300, seasonal_1 Loss: 0.0787 | 0.0543
Epoch 60/300, seasonal_1 Loss: 0.0795 | 0.0431
Epoch 61/300, seasonal_1 Loss: 0.0798 | 0.0551
Epoch 62/300, seasonal_1 Loss: 0.0872 | 0.0501
Epoch 63/300, seasonal_1 Loss: 0.0900 | 0.0575
Epoch 64/300, seasonal_1 Loss: 0.1006 | 0.0671
Epoch 65/300, seasonal_1 Loss: 0.1006 | 0.0608
Epoch 66/300, seasonal_1 Loss: 0.1016 | 0.0843
Epoch 67/300, seasonal_1 Loss: 0.0893 | 0.0508
Epoch 68/300, seasonal_1 Loss: 0.0860 | 0.0664
Epoch 69/300, seasonal_1 Loss: 0.0795 | 0.0559
Epoch 70/300, seasonal_1 Loss: 0.0807 | 0.0600
Epoch 71/300, seasonal_1 Loss: 0.0766 | 0.0482
Epoch 72/300, seasonal_1 Loss: 0.0758 | 0.0560
Epoch 73/300, seasonal_1 Loss: 0.0732 | 0.0464
Epoch 74/300, seasonal_1 Loss: 0.0723 | 0.0557
Epoch 75/300, seasonal_1 Loss: 0.0712 | 0.0462
Epoch 76/300, seasonal_1 Loss: 0.0707 | 0.0511
Epoch 77/300, seasonal_1 Loss: 0.0705 | 0.0470
Epoch 78/300, seasonal_1 Loss: 0.0701 | 0.0490
Epoch 79/300, seasonal_1 Loss: 0.0698 | 0.0465
Epoch 80/300, seasonal_1 Loss: 0.0695 | 0.0480
Epoch 81/300, seasonal_1 Loss: 0.0694 | 0.0463
Epoch 82/300, seasonal_1 Loss: 0.0692 | 0.0467
Epoch 83/300, seasonal_1 Loss: 0.0690 | 0.0463
Epoch 84/300, seasonal_1 Loss: 0.0689 | 0.0461
Epoch 85/300, seasonal_1 Loss: 0.0687 | 0.0458
Epoch 86/300, seasonal_1 Loss: 0.0687 | 0.0455
Epoch 87/300, seasonal_1 Loss: 0.0683 | 0.0455
Epoch 88/300, seasonal_1 Loss: 0.0683 | 0.0453
Epoch 89/300, seasonal_1 Loss: 0.0681 | 0.0449
Epoch 90/300, seasonal_1 Loss: 0.0680 | 0.0448
Epoch 91/300, seasonal_1 Loss: 0.0678 | 0.0448
Epoch 92/300, seasonal_1 Loss: 0.0677 | 0.0446
Epoch 93/300, seasonal_1 Loss: 0.0675 | 0.0443
Epoch 94/300, seasonal_1 Loss: 0.0674 | 0.0443
Epoch 95/300, seasonal_1 Loss: 0.0673 | 0.0442
Epoch 96/300, seasonal_1 Loss: 0.0672 | 0.0440
Epoch 97/300, seasonal_1 Loss: 0.0671 | 0.0439
Epoch 98/300, seasonal_1 Loss: 0.0669 | 0.0438
Epoch 99/300, seasonal_1 Loss: 0.0668 | 0.0437
Epoch 100/300, seasonal_1 Loss: 0.0667 | 0.0436
Epoch 101/300, seasonal_1 Loss: 0.0667 | 0.0435
Epoch 102/300, seasonal_1 Loss: 0.0666 | 0.0434
Epoch 103/300, seasonal_1 Loss: 0.0665 | 0.0433
Epoch 104/300, seasonal_1 Loss: 0.0664 | 0.0432
Epoch 105/300, seasonal_1 Loss: 0.0663 | 0.0431
Epoch 106/300, seasonal_1 Loss: 0.0662 | 0.0430
Epoch 107/300, seasonal_1 Loss: 0.0661 | 0.0430
Epoch 108/300, seasonal_1 Loss: 0.0661 | 0.0429
Epoch 109/300, seasonal_1 Loss: 0.0660 | 0.0428
Epoch 110/300, seasonal_1 Loss: 0.0659 | 0.0427
Epoch 111/300, seasonal_1 Loss: 0.0659 | 0.0427
Epoch 112/300, seasonal_1 Loss: 0.0658 | 0.0426
Epoch 113/300, seasonal_1 Loss: 0.0657 | 0.0425
Epoch 114/300, seasonal_1 Loss: 0.0657 | 0.0425
Epoch 115/300, seasonal_1 Loss: 0.0656 | 0.0424
Epoch 116/300, seasonal_1 Loss: 0.0655 | 0.0423
Epoch 117/300, seasonal_1 Loss: 0.0655 | 0.0423
Epoch 118/300, seasonal_1 Loss: 0.0654 | 0.0422
Epoch 119/300, seasonal_1 Loss: 0.0654 | 0.0422
Epoch 120/300, seasonal_1 Loss: 0.0653 | 0.0421
Epoch 121/300, seasonal_1 Loss: 0.0653 | 0.0421
Epoch 122/300, seasonal_1 Loss: 0.0652 | 0.0420
Epoch 123/300, seasonal_1 Loss: 0.0652 | 0.0420
Epoch 124/300, seasonal_1 Loss: 0.0651 | 0.0419
Epoch 125/300, seasonal_1 Loss: 0.0650 | 0.0419
Epoch 126/300, seasonal_1 Loss: 0.0650 | 0.0418
Epoch 127/300, seasonal_1 Loss: 0.0650 | 0.0418
Epoch 128/300, seasonal_1 Loss: 0.0649 | 0.0418
Epoch 129/300, seasonal_1 Loss: 0.0649 | 0.0417
Epoch 130/300, seasonal_1 Loss: 0.0648 | 0.0417
Epoch 131/300, seasonal_1 Loss: 0.0648 | 0.0417
Epoch 132/300, seasonal_1 Loss: 0.0647 | 0.0416
Epoch 133/300, seasonal_1 Loss: 0.0647 | 0.0416
Epoch 134/300, seasonal_1 Loss: 0.0647 | 0.0415
Epoch 135/300, seasonal_1 Loss: 0.0646 | 0.0415
Epoch 136/300, seasonal_1 Loss: 0.0646 | 0.0415
Epoch 137/300, seasonal_1 Loss: 0.0646 | 0.0414
Epoch 138/300, seasonal_1 Loss: 0.0645 | 0.0414
Epoch 139/300, seasonal_1 Loss: 0.0645 | 0.0414
Epoch 140/300, seasonal_1 Loss: 0.0645 | 0.0414
Epoch 141/300, seasonal_1 Loss: 0.0644 | 0.0413
Epoch 142/300, seasonal_1 Loss: 0.0644 | 0.0413
Epoch 143/300, seasonal_1 Loss: 0.0644 | 0.0413
Epoch 144/300, seasonal_1 Loss: 0.0643 | 0.0412
Epoch 145/300, seasonal_1 Loss: 0.0643 | 0.0412
Epoch 146/300, seasonal_1 Loss: 0.0643 | 0.0412
Epoch 147/300, seasonal_1 Loss: 0.0643 | 0.0412
Epoch 148/300, seasonal_1 Loss: 0.0642 | 0.0411
Epoch 149/300, seasonal_1 Loss: 0.0642 | 0.0411
Epoch 150/300, seasonal_1 Loss: 0.0642 | 0.0411
Epoch 151/300, seasonal_1 Loss: 0.0641 | 0.0411
Epoch 152/300, seasonal_1 Loss: 0.0641 | 0.0411
Epoch 153/300, seasonal_1 Loss: 0.0641 | 0.0410
Epoch 154/300, seasonal_1 Loss: 0.0641 | 0.0410
Epoch 155/300, seasonal_1 Loss: 0.0640 | 0.0410
Epoch 156/300, seasonal_1 Loss: 0.0640 | 0.0410
Epoch 157/300, seasonal_1 Loss: 0.0640 | 0.0410
Epoch 158/300, seasonal_1 Loss: 0.0640 | 0.0409
Epoch 159/300, seasonal_1 Loss: 0.0640 | 0.0409
Epoch 160/300, seasonal_1 Loss: 0.0639 | 0.0409
Epoch 161/300, seasonal_1 Loss: 0.0639 | 0.0409
Epoch 162/300, seasonal_1 Loss: 0.0639 | 0.0409
Epoch 163/300, seasonal_1 Loss: 0.0639 | 0.0408
Epoch 164/300, seasonal_1 Loss: 0.0639 | 0.0408
Epoch 165/300, seasonal_1 Loss: 0.0638 | 0.0408
Epoch 166/300, seasonal_1 Loss: 0.0638 | 0.0408
Epoch 167/300, seasonal_1 Loss: 0.0638 | 0.0408
Epoch 168/300, seasonal_1 Loss: 0.0638 | 0.0408
Epoch 169/300, seasonal_1 Loss: 0.0638 | 0.0408
Epoch 170/300, seasonal_1 Loss: 0.0638 | 0.0407
Epoch 171/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 172/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 173/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 174/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 175/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 176/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 177/300, seasonal_1 Loss: 0.0637 | 0.0407
Epoch 178/300, seasonal_1 Loss: 0.0637 | 0.0406
Epoch 179/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 180/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 181/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 182/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 183/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 184/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 185/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 186/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 187/300, seasonal_1 Loss: 0.0636 | 0.0406
Epoch 188/300, seasonal_1 Loss: 0.0635 | 0.0406
Epoch 189/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 190/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 191/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 192/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 193/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 194/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 195/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 196/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 197/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 198/300, seasonal_1 Loss: 0.0635 | 0.0405
Epoch 199/300, seasonal_1 Loss: 0.0634 | 0.0405
Epoch 200/300, seasonal_1 Loss: 0.0634 | 0.0405
Epoch 201/300, seasonal_1 Loss: 0.0634 | 0.0405
Epoch 202/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 203/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 204/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 205/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 206/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 207/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 208/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 209/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 210/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 211/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 212/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 213/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 214/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 215/300, seasonal_1 Loss: 0.0634 | 0.0404
Epoch 216/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 217/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 218/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 219/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 220/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 221/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 222/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 223/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 224/300, seasonal_1 Loss: 0.0633 | 0.0404
Epoch 225/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 226/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 227/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 228/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 229/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 230/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 231/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 232/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 233/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 234/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 235/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 236/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 237/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 238/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 239/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 240/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 241/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 242/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 243/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 244/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 245/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 246/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 247/300, seasonal_1 Loss: 0.0633 | 0.0403
Epoch 248/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 249/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 250/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 251/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 252/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 253/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 254/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 255/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 256/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 257/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 258/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 259/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 260/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 261/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 262/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 263/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 264/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 265/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 266/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 267/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 268/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 269/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 270/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 271/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 272/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 273/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 274/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 275/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 276/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 277/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 278/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 279/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 280/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 281/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 282/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 283/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 284/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 285/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 286/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 287/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 288/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 289/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 290/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 291/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 292/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 293/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 294/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 295/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 296/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 297/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 298/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 299/300, seasonal_1 Loss: 0.0632 | 0.0403
Epoch 300/300, seasonal_1 Loss: 0.0632 | 0.0403
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.9627278417486529, 'learning_rate': 7.353708316883962e-05, 'batch_size': 47, 'step_size': 10, 'gamma': 0.9637099783817532}
Epoch 1/300, seasonal_2 Loss: 0.3508 | 0.1012
Epoch 2/300, seasonal_2 Loss: 0.1300 | 0.0761
Epoch 3/300, seasonal_2 Loss: 0.1095 | 0.0659
Epoch 4/300, seasonal_2 Loss: 0.1052 | 0.0613
Epoch 5/300, seasonal_2 Loss: 0.1087 | 0.0595
Epoch 6/300, seasonal_2 Loss: 0.1144 | 0.0642
Epoch 7/300, seasonal_2 Loss: 0.1280 | 0.0813
Epoch 8/300, seasonal_2 Loss: 0.1638 | 0.1142
Epoch 9/300, seasonal_2 Loss: 0.1548 | 0.0804
Epoch 10/300, seasonal_2 Loss: 0.1143 | 0.1377
Epoch 11/300, seasonal_2 Loss: 0.0951 | 0.0694
Epoch 12/300, seasonal_2 Loss: 0.0867 | 0.0557
Epoch 13/300, seasonal_2 Loss: 0.0840 | 0.0526
Epoch 14/300, seasonal_2 Loss: 0.0774 | 0.0467
Epoch 15/300, seasonal_2 Loss: 0.0759 | 0.0441
Epoch 16/300, seasonal_2 Loss: 0.0736 | 0.0434
Epoch 17/300, seasonal_2 Loss: 0.0717 | 0.0440
Epoch 18/300, seasonal_2 Loss: 0.0704 | 0.0409
Epoch 19/300, seasonal_2 Loss: 0.0735 | 0.0465
Epoch 20/300, seasonal_2 Loss: 0.0765 | 0.0422
Epoch 21/300, seasonal_2 Loss: 0.0777 | 0.0509
Epoch 22/300, seasonal_2 Loss: 0.0806 | 0.0414
Epoch 23/300, seasonal_2 Loss: 0.1027 | 0.1059
Epoch 24/300, seasonal_2 Loss: 0.1152 | 0.0724
Epoch 25/300, seasonal_2 Loss: 0.0866 | 0.0646
Epoch 26/300, seasonal_2 Loss: 0.0818 | 0.0567
Epoch 27/300, seasonal_2 Loss: 0.0822 | 0.0520
Epoch 28/300, seasonal_2 Loss: 0.0781 | 0.0554
Epoch 29/300, seasonal_2 Loss: 0.0814 | 0.0478
Epoch 30/300, seasonal_2 Loss: 0.0815 | 0.0535
Epoch 31/300, seasonal_2 Loss: 0.0774 | 0.0538
Epoch 32/300, seasonal_2 Loss: 0.0729 | 0.0445
Epoch 33/300, seasonal_2 Loss: 0.0717 | 0.0387
Epoch 34/300, seasonal_2 Loss: 0.0708 | 0.0405
Epoch 35/300, seasonal_2 Loss: 0.0734 | 0.0399
Epoch 36/300, seasonal_2 Loss: 0.0700 | 0.0381
Epoch 37/300, seasonal_2 Loss: 0.0655 | 0.0387
Epoch 38/300, seasonal_2 Loss: 0.0672 | 0.0401
Epoch 39/300, seasonal_2 Loss: 0.0677 | 0.0403
Epoch 40/300, seasonal_2 Loss: 0.0678 | 0.0714
Epoch 41/300, seasonal_2 Loss: 0.0659 | 0.0494
Epoch 42/300, seasonal_2 Loss: 0.0687 | 0.0671
Epoch 43/300, seasonal_2 Loss: 0.0691 | 0.0461
Epoch 44/300, seasonal_2 Loss: 0.0681 | 0.0441
Epoch 45/300, seasonal_2 Loss: 0.0703 | 0.0493
Epoch 46/300, seasonal_2 Loss: 0.0720 | 0.0517
Epoch 47/300, seasonal_2 Loss: 0.0722 | 0.0494
Epoch 48/300, seasonal_2 Loss: 0.0724 | 0.0527
Epoch 49/300, seasonal_2 Loss: 0.0846 | 0.0566
Epoch 50/300, seasonal_2 Loss: 0.0808 | 0.0438
Epoch 51/300, seasonal_2 Loss: 0.0766 | 0.0520
Epoch 52/300, seasonal_2 Loss: 0.0665 | 0.0369
Epoch 53/300, seasonal_2 Loss: 0.0647 | 0.0350
Epoch 54/300, seasonal_2 Loss: 0.0641 | 0.0371
Epoch 55/300, seasonal_2 Loss: 0.0594 | 0.0381
Epoch 56/300, seasonal_2 Loss: 0.0587 | 0.0370
Epoch 57/300, seasonal_2 Loss: 0.0563 | 0.0344
Epoch 58/300, seasonal_2 Loss: 0.0557 | 0.0340
Epoch 59/300, seasonal_2 Loss: 0.0556 | 0.0349
Epoch 60/300, seasonal_2 Loss: 0.0558 | 0.0338
Epoch 61/300, seasonal_2 Loss: 0.0564 | 0.0340
Epoch 62/300, seasonal_2 Loss: 0.0538 | 0.0339
Epoch 63/300, seasonal_2 Loss: 0.0560 | 0.0420
Epoch 64/300, seasonal_2 Loss: 0.0552 | 0.0341
Epoch 65/300, seasonal_2 Loss: 0.0576 | 0.0350
Epoch 66/300, seasonal_2 Loss: 0.0588 | 0.0394
Epoch 67/300, seasonal_2 Loss: 0.0563 | 0.0378
Epoch 68/300, seasonal_2 Loss: 0.0545 | 0.0457
Epoch 69/300, seasonal_2 Loss: 0.0536 | 0.0794
Epoch 70/300, seasonal_2 Loss: 0.0600 | 0.0465
Epoch 71/300, seasonal_2 Loss: 0.0574 | 0.0544
Epoch 72/300, seasonal_2 Loss: 0.0641 | 0.0379
Epoch 73/300, seasonal_2 Loss: 0.0620 | 0.0560
Epoch 74/300, seasonal_2 Loss: 0.0598 | 0.0470
Epoch 75/300, seasonal_2 Loss: 0.0543 | 0.0401
Epoch 76/300, seasonal_2 Loss: 0.0585 | 0.0512
Epoch 77/300, seasonal_2 Loss: 0.0632 | 0.0425
Epoch 78/300, seasonal_2 Loss: 0.0576 | 0.0420
Epoch 79/300, seasonal_2 Loss: 0.0521 | 0.0373
Epoch 80/300, seasonal_2 Loss: 0.0489 | 0.0414
Epoch 81/300, seasonal_2 Loss: 0.0534 | 0.0334
Epoch 82/300, seasonal_2 Loss: 0.0534 | 0.0462
Epoch 83/300, seasonal_2 Loss: 0.0512 | 0.0592
Epoch 84/300, seasonal_2 Loss: 0.0513 | 0.0495
Epoch 85/300, seasonal_2 Loss: 0.0445 | 0.0408
Epoch 86/300, seasonal_2 Loss: 0.0435 | 0.0421
Epoch 87/300, seasonal_2 Loss: 0.0434 | 0.0370
Epoch 88/300, seasonal_2 Loss: 0.0409 | 0.0336
Epoch 89/300, seasonal_2 Loss: 0.0395 | 0.0352
Epoch 90/300, seasonal_2 Loss: 0.0385 | 0.0358
Epoch 91/300, seasonal_2 Loss: 0.0376 | 0.0344
Epoch 92/300, seasonal_2 Loss: 0.0376 | 0.0351
Epoch 93/300, seasonal_2 Loss: 0.0375 | 0.0357
Epoch 94/300, seasonal_2 Loss: 0.0364 | 0.0358
Epoch 95/300, seasonal_2 Loss: 0.0374 | 0.0383
Epoch 96/300, seasonal_2 Loss: 0.0374 | 0.0403
Epoch 97/300, seasonal_2 Loss: 0.0375 | 0.0411
Epoch 98/300, seasonal_2 Loss: 0.0365 | 0.0373
Epoch 99/300, seasonal_2 Loss: 0.0364 | 0.0386
Epoch 100/300, seasonal_2 Loss: 0.0363 | 0.0395
Epoch 101/300, seasonal_2 Loss: 0.0372 | 0.0414
Epoch 102/300, seasonal_2 Loss: 0.0372 | 0.0379
Epoch 103/300, seasonal_2 Loss: 0.0421 | 0.0491
Epoch 104/300, seasonal_2 Loss: 0.0369 | 0.0394
Epoch 105/300, seasonal_2 Loss: 0.0378 | 0.0480
Epoch 106/300, seasonal_2 Loss: 0.0366 | 0.0753
Epoch 107/300, seasonal_2 Loss: 0.0484 | 0.0525
Epoch 108/300, seasonal_2 Loss: 0.0634 | 0.0434
Epoch 109/300, seasonal_2 Loss: 0.0522 | 0.0430
Epoch 110/300, seasonal_2 Loss: 0.0454 | 0.0456
Epoch 111/300, seasonal_2 Loss: 0.0452 | 0.0416
Epoch 112/300, seasonal_2 Loss: 0.0447 | 0.0374
Epoch 113/300, seasonal_2 Loss: 0.0419 | 0.0373
Epoch 114/300, seasonal_2 Loss: 0.0403 | 0.0416
Epoch 115/300, seasonal_2 Loss: 0.0381 | 0.0389
Epoch 116/300, seasonal_2 Loss: 0.0413 | 0.0395
Epoch 117/300, seasonal_2 Loss: 0.0380 | 0.0420
Epoch 118/300, seasonal_2 Loss: 0.0428 | 0.0384
Epoch 119/300, seasonal_2 Loss: 0.0413 | 0.0385
Epoch 120/300, seasonal_2 Loss: 0.0339 | 0.0384
Epoch 121/300, seasonal_2 Loss: 0.0338 | 0.0400
Epoch 122/300, seasonal_2 Loss: 0.0341 | 0.0412
Epoch 123/300, seasonal_2 Loss: 0.0328 | 0.0397
Epoch 124/300, seasonal_2 Loss: 0.0407 | 0.0380
Epoch 125/300, seasonal_2 Loss: 0.0326 | 0.0441
Epoch 126/300, seasonal_2 Loss: 0.0354 | 0.0413
Epoch 127/300, seasonal_2 Loss: 0.0405 | 0.0431
Epoch 128/300, seasonal_2 Loss: 0.0315 | 0.0404
Epoch 129/300, seasonal_2 Loss: 0.0312 | 0.0387
Epoch 130/300, seasonal_2 Loss: 0.0306 | 0.0395
Epoch 131/300, seasonal_2 Loss: 0.0306 | 0.0387
Epoch 132/300, seasonal_2 Loss: 0.0301 | 0.0389
Epoch 133/300, seasonal_2 Loss: 0.0297 | 0.0430
Epoch 134/300, seasonal_2 Loss: 0.0295 | 0.0426
Epoch 135/300, seasonal_2 Loss: 0.0295 | 0.0392
Epoch 136/300, seasonal_2 Loss: 0.0295 | 0.0386
Epoch 137/300, seasonal_2 Loss: 0.0291 | 0.0384
Epoch 138/300, seasonal_2 Loss: 0.0288 | 0.0384
Epoch 139/300, seasonal_2 Loss: 0.0287 | 0.0386
Epoch 140/300, seasonal_2 Loss: 0.0291 | 0.0428
Epoch 141/300, seasonal_2 Loss: 0.0292 | 0.0443
Epoch 142/300, seasonal_2 Loss: 0.0288 | 0.0398
Epoch 143/300, seasonal_2 Loss: 0.0282 | 0.0395
Epoch 144/300, seasonal_2 Loss: 0.0278 | 0.0389
Epoch 145/300, seasonal_2 Loss: 0.0277 | 0.0398
Epoch 146/300, seasonal_2 Loss: 0.0275 | 0.0387
Epoch 147/300, seasonal_2 Loss: 0.0279 | 0.0407
Epoch 148/300, seasonal_2 Loss: 0.0274 | 0.0405
Epoch 149/300, seasonal_2 Loss: 0.0271 | 0.0424
Epoch 150/300, seasonal_2 Loss: 0.0272 | 0.0414
Epoch 151/300, seasonal_2 Loss: 0.0273 | 0.0408
Epoch 152/300, seasonal_2 Loss: 0.0268 | 0.0415
Epoch 153/300, seasonal_2 Loss: 0.0272 | 0.0439
Epoch 154/300, seasonal_2 Loss: 0.0270 | 0.0413
Epoch 155/300, seasonal_2 Loss: 0.0277 | 0.0418
Epoch 156/300, seasonal_2 Loss: 0.0276 | 0.0405
Epoch 157/300, seasonal_2 Loss: 0.0282 | 0.0440
Epoch 158/300, seasonal_2 Loss: 0.0284 | 0.0386
Epoch 159/300, seasonal_2 Loss: 0.0276 | 0.0406
Epoch 160/300, seasonal_2 Loss: 0.0276 | 0.0476
Epoch 161/300, seasonal_2 Loss: 0.0270 | 0.0414
Epoch 162/300, seasonal_2 Loss: 0.0275 | 0.0425
Epoch 163/300, seasonal_2 Loss: 0.0274 | 0.0398
Epoch 164/300, seasonal_2 Loss: 0.0276 | 0.0389
Epoch 165/300, seasonal_2 Loss: 0.0281 | 0.0427
Epoch 166/300, seasonal_2 Loss: 0.0273 | 0.0410
Epoch 167/300, seasonal_2 Loss: 0.0268 | 0.0422
Epoch 168/300, seasonal_2 Loss: 0.0261 | 0.0411
Epoch 169/300, seasonal_2 Loss: 0.0268 | 0.0402
Epoch 170/300, seasonal_2 Loss: 0.0260 | 0.0387
Epoch 171/300, seasonal_2 Loss: 0.0262 | 0.0422
Epoch 172/300, seasonal_2 Loss: 0.0258 | 0.0428
Epoch 173/300, seasonal_2 Loss: 0.0260 | 0.0418
Epoch 174/300, seasonal_2 Loss: 0.0260 | 0.0442
Epoch 175/300, seasonal_2 Loss: 0.0260 | 0.0418
Epoch 176/300, seasonal_2 Loss: 0.0265 | 0.0407
Epoch 177/300, seasonal_2 Loss: 0.0258 | 0.0394
Epoch 178/300, seasonal_2 Loss: 0.0256 | 0.0391
Epoch 179/300, seasonal_2 Loss: 0.0261 | 0.0438
Epoch 180/300, seasonal_2 Loss: 0.0257 | 0.0393
Epoch 181/300, seasonal_2 Loss: 0.0258 | 0.0432
Epoch 182/300, seasonal_2 Loss: 0.0251 | 0.0400
Epoch 183/300, seasonal_2 Loss: 0.0254 | 0.0413
Epoch 184/300, seasonal_2 Loss: 0.0254 | 0.0416
Epoch 185/300, seasonal_2 Loss: 0.0260 | 0.0391
Epoch 186/300, seasonal_2 Loss: 0.0267 | 0.0416
Epoch 187/300, seasonal_2 Loss: 0.0271 | 0.0414
Epoch 188/300, seasonal_2 Loss: 0.0256 | 0.0443
Epoch 189/300, seasonal_2 Loss: 0.0248 | 0.0397
Epoch 190/300, seasonal_2 Loss: 0.0247 | 0.0409
Epoch 191/300, seasonal_2 Loss: 0.0242 | 0.0399
Epoch 192/300, seasonal_2 Loss: 0.0241 | 0.0437
Epoch 193/300, seasonal_2 Loss: 0.0241 | 0.0447
Epoch 194/300, seasonal_2 Loss: 0.0239 | 0.0420
Epoch 195/300, seasonal_2 Loss: 0.0242 | 0.0410
Epoch 196/300, seasonal_2 Loss: 0.0241 | 0.0406
Epoch 197/300, seasonal_2 Loss: 0.0240 | 0.0387
Epoch 198/300, seasonal_2 Loss: 0.0242 | 0.0415
Epoch 199/300, seasonal_2 Loss: 0.0245 | 0.0410
Epoch 200/300, seasonal_2 Loss: 0.0244 | 0.0424
Epoch 201/300, seasonal_2 Loss: 0.0240 | 0.0419
Epoch 202/300, seasonal_2 Loss: 0.0243 | 0.0424
Epoch 203/300, seasonal_2 Loss: 0.0241 | 0.0474
Epoch 204/300, seasonal_2 Loss: 0.0240 | 0.0493
Epoch 205/300, seasonal_2 Loss: 0.0239 | 0.0438
Epoch 206/300, seasonal_2 Loss: 0.0243 | 0.0399
Epoch 207/300, seasonal_2 Loss: 0.0247 | 0.0408
Epoch 208/300, seasonal_2 Loss: 0.0252 | 0.0395
Epoch 209/300, seasonal_2 Loss: 0.0255 | 0.0423
Epoch 210/300, seasonal_2 Loss: 0.0241 | 0.0393
Epoch 211/300, seasonal_2 Loss: 0.0242 | 0.0397
Epoch 212/300, seasonal_2 Loss: 0.0238 | 0.0427
Epoch 213/300, seasonal_2 Loss: 0.0236 | 0.0395
Epoch 214/300, seasonal_2 Loss: 0.0238 | 0.0413
Epoch 215/300, seasonal_2 Loss: 0.0236 | 0.0406
Epoch 216/300, seasonal_2 Loss: 0.0229 | 0.0421
Epoch 217/300, seasonal_2 Loss: 0.0230 | 0.0393
Epoch 218/300, seasonal_2 Loss: 0.0229 | 0.0397
Epoch 219/300, seasonal_2 Loss: 0.0227 | 0.0380
Epoch 220/300, seasonal_2 Loss: 0.0233 | 0.0403
Epoch 221/300, seasonal_2 Loss: 0.0236 | 0.0413
Epoch 222/300, seasonal_2 Loss: 0.0237 | 0.0411
Epoch 223/300, seasonal_2 Loss: 0.0232 | 0.0408
Epoch 224/300, seasonal_2 Loss: 0.0234 | 0.0403
Epoch 225/300, seasonal_2 Loss: 0.0233 | 0.0413
Epoch 226/300, seasonal_2 Loss: 0.0230 | 0.0409
Epoch 227/300, seasonal_2 Loss: 0.0230 | 0.0404
Epoch 228/300, seasonal_2 Loss: 0.0231 | 0.0417
Epoch 229/300, seasonal_2 Loss: 0.0226 | 0.0407
Epoch 230/300, seasonal_2 Loss: 0.0226 | 0.0423
Epoch 231/300, seasonal_2 Loss: 0.0228 | 0.0422
Epoch 232/300, seasonal_2 Loss: 0.0228 | 0.0423
Epoch 233/300, seasonal_2 Loss: 0.0230 | 0.0421
Epoch 234/300, seasonal_2 Loss: 0.0230 | 0.0425
Epoch 235/300, seasonal_2 Loss: 0.0230 | 0.0414
Epoch 236/300, seasonal_2 Loss: 0.0229 | 0.0410
Epoch 237/300, seasonal_2 Loss: 0.0231 | 0.0399
Epoch 238/300, seasonal_2 Loss: 0.0233 | 0.0403
Epoch 239/300, seasonal_2 Loss: 0.0237 | 0.0409
Epoch 240/300, seasonal_2 Loss: 0.0230 | 0.0429
Epoch 241/300, seasonal_2 Loss: 0.0229 | 0.0417
Epoch 242/300, seasonal_2 Loss: 0.0226 | 0.0426
Epoch 243/300, seasonal_2 Loss: 0.0225 | 0.0402
Epoch 244/300, seasonal_2 Loss: 0.0229 | 0.0418
Epoch 245/300, seasonal_2 Loss: 0.0229 | 0.0400
Epoch 246/300, seasonal_2 Loss: 0.0231 | 0.0408
Epoch 247/300, seasonal_2 Loss: 0.0221 | 0.0383
Epoch 248/300, seasonal_2 Loss: 0.0221 | 0.0403
Epoch 249/300, seasonal_2 Loss: 0.0220 | 0.0412
Epoch 250/300, seasonal_2 Loss: 0.0220 | 0.0413
Epoch 251/300, seasonal_2 Loss: 0.0221 | 0.0435
Epoch 252/300, seasonal_2 Loss: 0.0220 | 0.0415
Epoch 253/300, seasonal_2 Loss: 0.0221 | 0.0426
Epoch 254/300, seasonal_2 Loss: 0.0218 | 0.0390
Epoch 255/300, seasonal_2 Loss: 0.0218 | 0.0411
Epoch 256/300, seasonal_2 Loss: 0.0220 | 0.0396
Epoch 257/300, seasonal_2 Loss: 0.0221 | 0.0411
Epoch 258/300, seasonal_2 Loss: 0.0223 | 0.0406
Epoch 259/300, seasonal_2 Loss: 0.0220 | 0.0417
Epoch 260/300, seasonal_2 Loss: 0.0220 | 0.0423
Epoch 261/300, seasonal_2 Loss: 0.0218 | 0.0434
Epoch 262/300, seasonal_2 Loss: 0.0219 | 0.0426
Epoch 263/300, seasonal_2 Loss: 0.0220 | 0.0406
Epoch 264/300, seasonal_2 Loss: 0.0221 | 0.0391
Epoch 265/300, seasonal_2 Loss: 0.0224 | 0.0416
Epoch 266/300, seasonal_2 Loss: 0.0217 | 0.0396
Epoch 267/300, seasonal_2 Loss: 0.0214 | 0.0411
Epoch 268/300, seasonal_2 Loss: 0.0211 | 0.0404
Epoch 269/300, seasonal_2 Loss: 0.0211 | 0.0414
Epoch 270/300, seasonal_2 Loss: 0.0211 | 0.0403
Epoch 271/300, seasonal_2 Loss: 0.0213 | 0.0406
Epoch 272/300, seasonal_2 Loss: 0.0212 | 0.0402
Epoch 273/300, seasonal_2 Loss: 0.0211 | 0.0411
Epoch 274/300, seasonal_2 Loss: 0.0211 | 0.0409
Epoch 275/300, seasonal_2 Loss: 0.0211 | 0.0407
Epoch 276/300, seasonal_2 Loss: 0.0212 | 0.0407
Epoch 277/300, seasonal_2 Loss: 0.0211 | 0.0412
Epoch 278/300, seasonal_2 Loss: 0.0211 | 0.0414
Epoch 279/300, seasonal_2 Loss: 0.0213 | 0.0406
Epoch 280/300, seasonal_2 Loss: 0.0214 | 0.0402
Epoch 281/300, seasonal_2 Loss: 0.0215 | 0.0404
Epoch 282/300, seasonal_2 Loss: 0.0215 | 0.0419
Epoch 283/300, seasonal_2 Loss: 0.0214 | 0.0423
Epoch 284/300, seasonal_2 Loss: 0.0215 | 0.0422
Epoch 285/300, seasonal_2 Loss: 0.0215 | 0.0428
Epoch 286/300, seasonal_2 Loss: 0.0214 | 0.0441
Epoch 287/300, seasonal_2 Loss: 0.0214 | 0.0437
Epoch 288/300, seasonal_2 Loss: 0.0217 | 0.0444
Epoch 289/300, seasonal_2 Loss: 0.0222 | 0.0446
Epoch 290/300, seasonal_2 Loss: 0.0223 | 0.0450
Epoch 291/300, seasonal_2 Loss: 0.0224 | 0.0477
Epoch 292/300, seasonal_2 Loss: 0.0223 | 0.0440
Epoch 293/300, seasonal_2 Loss: 0.0222 | 0.0419
Epoch 294/300, seasonal_2 Loss: 0.0220 | 0.0440
Epoch 295/300, seasonal_2 Loss: 0.0223 | 0.0437
Epoch 296/300, seasonal_2 Loss: 0.0229 | 0.0431
Epoch 297/300, seasonal_2 Loss: 0.0232 | 0.0421
Epoch 298/300, seasonal_2 Loss: 0.0226 | 0.0439
Epoch 299/300, seasonal_2 Loss: 0.0220 | 0.0418
Epoch 300/300, seasonal_2 Loss: 0.0214 | 0.0430
Training seasonal_3 component with params: {'observation_period_num': 7, 'train_rates': 0.9762909679870211, 'learning_rate': 0.0006094601430873331, 'batch_size': 19, 'step_size': 15, 'gamma': 0.8029019476141832}
Epoch 1/300, seasonal_3 Loss: 1.0379 | 1.4773
Epoch 2/300, seasonal_3 Loss: 0.8937 | 1.3091
Epoch 3/300, seasonal_3 Loss: 0.9092 | 1.3051
Epoch 4/300, seasonal_3 Loss: 0.9105 | 1.3008
Epoch 5/300, seasonal_3 Loss: 0.9151 | 1.3008
Epoch 6/300, seasonal_3 Loss: 0.9056 | 1.3013
Epoch 7/300, seasonal_3 Loss: 0.9164 | 1.3008
Epoch 8/300, seasonal_3 Loss: 0.9143 | 1.3009
Epoch 9/300, seasonal_3 Loss: 0.9175 | 1.2997
Epoch 10/300, seasonal_3 Loss: 0.9014 | 1.3080
Epoch 11/300, seasonal_3 Loss: 0.9181 | 1.3486
Epoch 12/300, seasonal_3 Loss: 0.9280 | 1.3518
Epoch 13/300, seasonal_3 Loss: 0.9297 | 1.3499
Epoch 14/300, seasonal_3 Loss: 0.9318 | 1.3476
Epoch 15/300, seasonal_3 Loss: 0.9316 | 1.3440
Epoch 16/300, seasonal_3 Loss: 0.9467 | 1.3132
Epoch 17/300, seasonal_3 Loss: 0.9571 | 1.3343
Epoch 18/300, seasonal_3 Loss: 0.9694 | 1.3653
Epoch 19/300, seasonal_3 Loss: 0.9854 | 1.3939
Epoch 20/300, seasonal_3 Loss: 0.9947 | 1.4321
Epoch 21/300, seasonal_3 Loss: 0.9999 | 1.4912
Epoch 22/300, seasonal_3 Loss: 1.0021 | 1.5676
Epoch 23/300, seasonal_3 Loss: 1.0010 | 1.6467
Epoch 24/300, seasonal_3 Loss: 1.0018 | 1.7628
Epoch 25/300, seasonal_3 Loss: 0.9903 | 1.7881
Epoch 26/300, seasonal_3 Loss: 0.9882 | 1.8043
Epoch 27/300, seasonal_3 Loss: 0.9869 | 1.8190
Epoch 28/300, seasonal_3 Loss: 0.9856 | 1.8327
Epoch 29/300, seasonal_3 Loss: 0.9844 | 1.8454
Epoch 30/300, seasonal_3 Loss: 0.9802 | 1.7706
Epoch 31/300, seasonal_3 Loss: 0.9892 | 1.8524
Epoch 32/300, seasonal_3 Loss: 0.9822 | 1.8656
Epoch 33/300, seasonal_3 Loss: 0.9809 | 1.8727
Epoch 34/300, seasonal_3 Loss: 0.9803 | 1.8756
Epoch 35/300, seasonal_3 Loss: 0.9769 | 1.8058
Epoch 36/300, seasonal_3 Loss: 0.9833 | 1.8406
Epoch 37/300, seasonal_3 Loss: 0.9807 | 1.8463
Epoch 38/300, seasonal_3 Loss: 0.9793 | 1.8429
Epoch 39/300, seasonal_3 Loss: 0.9818 | 1.8694
Epoch 40/300, seasonal_3 Loss: 0.9795 | 1.8764
Epoch 41/300, seasonal_3 Loss: 0.9790 | 1.8807
Epoch 42/300, seasonal_3 Loss: 0.9791 | 1.8780
Epoch 43/300, seasonal_3 Loss: 0.9768 | 1.8513
Epoch 44/300, seasonal_3 Loss: 0.9711 | 1.6173
Epoch 45/300, seasonal_3 Loss: 0.9737 | 1.5224
Epoch 46/300, seasonal_3 Loss: 1.0057 | 1.8586
Epoch 47/300, seasonal_3 Loss: 0.9803 | 1.8771
Epoch 48/300, seasonal_3 Loss: 0.9787 | 1.8804
Epoch 49/300, seasonal_3 Loss: 0.9784 | 1.8823
Epoch 50/300, seasonal_3 Loss: 0.9783 | 1.8839
Epoch 51/300, seasonal_3 Loss: 0.9781 | 1.8853
Epoch 52/300, seasonal_3 Loss: 0.9780 | 1.8849
Epoch 53/300, seasonal_3 Loss: 0.9789 | 1.8830
Epoch 54/300, seasonal_3 Loss: 0.9787 | 1.8860
Epoch 55/300, seasonal_3 Loss: 0.9780 | 1.8874
Epoch 56/300, seasonal_3 Loss: 0.9779 | 1.8880
Epoch 57/300, seasonal_3 Loss: 0.9782 | 1.8883
Epoch 58/300, seasonal_3 Loss: 0.9785 | 1.8856
Epoch 59/300, seasonal_3 Loss: 0.9785 | 1.8816
Epoch 60/300, seasonal_3 Loss: 0.9789 | 1.8835
Epoch 61/300, seasonal_3 Loss: 0.9780 | 1.8875
Epoch 62/300, seasonal_3 Loss: 0.9776 | 1.8884
Epoch 63/300, seasonal_3 Loss: 0.9775 | 1.8891
Epoch 64/300, seasonal_3 Loss: 0.9775 | 1.8897
Epoch 65/300, seasonal_3 Loss: 0.9776 | 1.8903
Epoch 66/300, seasonal_3 Loss: 0.9782 | 1.8894
Epoch 67/300, seasonal_3 Loss: 0.9779 | 1.8805
Epoch 68/300, seasonal_3 Loss: 0.9788 | 1.8822
Epoch 69/300, seasonal_3 Loss: 0.9782 | 1.8888
Epoch 70/300, seasonal_3 Loss: 0.9776 | 1.8896
Epoch 71/300, seasonal_3 Loss: 0.9775 | 1.8902
Epoch 72/300, seasonal_3 Loss: 0.9775 | 1.8907
Epoch 73/300, seasonal_3 Loss: 0.9775 | 1.8907
Epoch 74/300, seasonal_3 Loss: 0.9780 | 1.8910
Epoch 75/300, seasonal_3 Loss: 0.9783 | 1.8850
Epoch 76/300, seasonal_3 Loss: 0.9780 | 1.8911
Epoch 77/300, seasonal_3 Loss: 0.9774 | 1.8918
Epoch 78/300, seasonal_3 Loss: 0.9773 | 1.8922
Epoch 79/300, seasonal_3 Loss: 0.9773 | 1.8926
Epoch 80/300, seasonal_3 Loss: 0.9773 | 1.8929
Epoch 81/300, seasonal_3 Loss: 0.9776 | 1.8931
Epoch 82/300, seasonal_3 Loss: 0.9780 | 1.8921
Epoch 83/300, seasonal_3 Loss: 0.9782 | 1.8913
Epoch 84/300, seasonal_3 Loss: 0.9774 | 1.8935
Epoch 85/300, seasonal_3 Loss: 0.9772 | 1.8939
Epoch 86/300, seasonal_3 Loss: 0.9771 | 1.8942
Epoch 87/300, seasonal_3 Loss: 0.9771 | 1.8945
Epoch 88/300, seasonal_3 Loss: 0.9771 | 1.8948
Epoch 89/300, seasonal_3 Loss: 0.9776 | 1.8938
Epoch 90/300, seasonal_3 Loss: 0.9780 | 1.8945
Epoch 91/300, seasonal_3 Loss: 0.9772 | 1.8952
Epoch 92/300, seasonal_3 Loss: 0.9770 | 1.8955
Epoch 93/300, seasonal_3 Loss: 0.9770 | 1.8957
Epoch 94/300, seasonal_3 Loss: 0.9770 | 1.8960
Epoch 95/300, seasonal_3 Loss: 0.9770 | 1.8962
Epoch 96/300, seasonal_3 Loss: 0.9771 | 1.8963
Epoch 97/300, seasonal_3 Loss: 0.9775 | 1.8961
Epoch 98/300, seasonal_3 Loss: 0.9775 | 1.8951
Epoch 99/300, seasonal_3 Loss: 0.9771 | 1.8967
Epoch 100/300, seasonal_3 Loss: 0.9769 | 1.8970
Epoch 101/300, seasonal_3 Loss: 0.9769 | 1.8972
Epoch 102/300, seasonal_3 Loss: 0.9769 | 1.8974
Epoch 103/300, seasonal_3 Loss: 0.9769 | 1.8975
Epoch 104/300, seasonal_3 Loss: 0.9770 | 1.8977
Epoch 105/300, seasonal_3 Loss: 0.9776 | 1.8971
Epoch 106/300, seasonal_3 Loss: 0.9770 | 1.8970
Epoch 107/300, seasonal_3 Loss: 0.9769 | 1.8980
Epoch 108/300, seasonal_3 Loss: 0.9769 | 1.8982
Epoch 109/300, seasonal_3 Loss: 0.9769 | 1.8983
Epoch 110/300, seasonal_3 Loss: 0.9768 | 1.8985
Epoch 111/300, seasonal_3 Loss: 0.9769 | 1.8986
Epoch 112/300, seasonal_3 Loss: 0.9770 | 1.8987
Epoch 113/300, seasonal_3 Loss: 0.9772 | 1.8987
Epoch 114/300, seasonal_3 Loss: 0.9769 | 1.8989
Epoch 115/300, seasonal_3 Loss: 0.9768 | 1.8990
Epoch 116/300, seasonal_3 Loss: 0.9768 | 1.8991
Epoch 117/300, seasonal_3 Loss: 0.9768 | 1.8992
Epoch 118/300, seasonal_3 Loss: 0.9768 | 1.8993
Epoch 119/300, seasonal_3 Loss: 0.9768 | 1.8994
Epoch 120/300, seasonal_3 Loss: 0.9770 | 1.8995
Epoch 121/300, seasonal_3 Loss: 0.9768 | 1.8996
Epoch 122/300, seasonal_3 Loss: 0.9768 | 1.8997
Epoch 123/300, seasonal_3 Loss: 0.9768 | 1.8998
Epoch 124/300, seasonal_3 Loss: 0.9768 | 1.8999
Epoch 125/300, seasonal_3 Loss: 0.9768 | 1.9000
Epoch 126/300, seasonal_3 Loss: 0.9768 | 1.9000
Epoch 127/300, seasonal_3 Loss: 0.9769 | 1.9000
Epoch 128/300, seasonal_3 Loss: 0.9768 | 1.9002
Epoch 129/300, seasonal_3 Loss: 0.9767 | 1.9002
Epoch 130/300, seasonal_3 Loss: 0.9767 | 1.9003
Epoch 131/300, seasonal_3 Loss: 0.9767 | 1.9004
Epoch 132/300, seasonal_3 Loss: 0.9767 | 1.9004
Epoch 133/300, seasonal_3 Loss: 0.9767 | 1.9005
Epoch 134/300, seasonal_3 Loss: 0.9767 | 1.9006
Epoch 135/300, seasonal_3 Loss: 0.9768 | 1.9006
Epoch 136/300, seasonal_3 Loss: 0.9767 | 1.9007
Epoch 137/300, seasonal_3 Loss: 0.9767 | 1.9007
Epoch 138/300, seasonal_3 Loss: 0.9767 | 1.9008
Epoch 139/300, seasonal_3 Loss: 0.9767 | 1.9008
Epoch 140/300, seasonal_3 Loss: 0.9767 | 1.9009
Epoch 141/300, seasonal_3 Loss: 0.9767 | 1.9009
Epoch 142/300, seasonal_3 Loss: 0.9767 | 1.9010
Epoch 143/300, seasonal_3 Loss: 0.9767 | 1.9010
Epoch 144/300, seasonal_3 Loss: 0.9767 | 1.9011
Epoch 145/300, seasonal_3 Loss: 0.9767 | 1.9011
Epoch 146/300, seasonal_3 Loss: 0.9767 | 1.9012
Epoch 147/300, seasonal_3 Loss: 0.9767 | 1.9012
Epoch 148/300, seasonal_3 Loss: 0.9767 | 1.9012
Epoch 149/300, seasonal_3 Loss: 0.9767 | 1.9013
Epoch 150/300, seasonal_3 Loss: 0.9767 | 1.9013
Epoch 151/300, seasonal_3 Loss: 0.9767 | 1.9013
Epoch 152/300, seasonal_3 Loss: 0.9767 | 1.9014
Epoch 153/300, seasonal_3 Loss: 0.9767 | 1.9014
Epoch 154/300, seasonal_3 Loss: 0.9767 | 1.9014
Epoch 155/300, seasonal_3 Loss: 0.9767 | 1.9015
Epoch 156/300, seasonal_3 Loss: 0.9767 | 1.9015
Epoch 157/300, seasonal_3 Loss: 0.9767 | 1.9015
Epoch 158/300, seasonal_3 Loss: 0.9767 | 1.9016
Epoch 159/300, seasonal_3 Loss: 0.9767 | 1.9016
Epoch 160/300, seasonal_3 Loss: 0.9767 | 1.9016
Epoch 161/300, seasonal_3 Loss: 0.9767 | 1.9016
Epoch 162/300, seasonal_3 Loss: 0.9767 | 1.9017
Epoch 163/300, seasonal_3 Loss: 0.9767 | 1.9017
Epoch 164/300, seasonal_3 Loss: 0.9767 | 1.9017
Epoch 165/300, seasonal_3 Loss: 0.9767 | 1.9017
Epoch 166/300, seasonal_3 Loss: 0.9767 | 1.9018
Epoch 167/300, seasonal_3 Loss: 0.9767 | 1.9018
Epoch 168/300, seasonal_3 Loss: 0.9767 | 1.9018
Epoch 169/300, seasonal_3 Loss: 0.9767 | 1.9018
Epoch 170/300, seasonal_3 Loss: 0.9767 | 1.9018
Epoch 171/300, seasonal_3 Loss: 0.9767 | 1.9019
Epoch 172/300, seasonal_3 Loss: 0.9767 | 1.9019
Epoch 173/300, seasonal_3 Loss: 0.9767 | 1.9019
Epoch 174/300, seasonal_3 Loss: 0.9767 | 1.9019
Epoch 175/300, seasonal_3 Loss: 0.9767 | 1.9019
Epoch 176/300, seasonal_3 Loss: 0.9767 | 1.9019
Epoch 177/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 178/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 179/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 180/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 181/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 182/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 183/300, seasonal_3 Loss: 0.9767 | 1.9020
Epoch 184/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 185/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 186/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 187/300, seasonal_3 Loss: 0.9767 | 1.9021
Epoch 188/300, seasonal_3 Loss: 0.9767 | 1.9021
Epoch 189/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 190/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 191/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 192/300, seasonal_3 Loss: 0.9766 | 1.9021
Epoch 193/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 194/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 195/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 196/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 197/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 198/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 199/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 200/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 201/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 202/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 203/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 204/300, seasonal_3 Loss: 0.9766 | 1.9022
Epoch 205/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 206/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 207/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 208/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 209/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 210/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 211/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 212/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 213/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 214/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 215/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 216/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 217/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 218/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 219/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 220/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 221/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 222/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 223/300, seasonal_3 Loss: 0.9766 | 1.9023
Epoch 224/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 225/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 226/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 227/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 228/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 229/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 230/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 231/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 232/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 233/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 234/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 235/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 236/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 237/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 238/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 239/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 240/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 241/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 242/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 243/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 244/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 245/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 246/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 247/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 248/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 249/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 250/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 251/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 252/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 253/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 254/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 255/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 256/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 257/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 258/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 259/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 260/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 261/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 262/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 263/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 264/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 265/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 266/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 267/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 268/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 269/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 270/300, seasonal_3 Loss: 0.9766 | 1.9024
Epoch 271/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 272/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 273/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 274/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 275/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 276/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 277/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 278/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 279/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 280/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 281/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 282/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 283/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 284/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 285/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 286/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 287/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 288/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 289/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 290/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 291/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 292/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 293/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 294/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 295/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 296/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 297/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 298/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 299/300, seasonal_3 Loss: 0.9766 | 1.9025
Epoch 300/300, seasonal_3 Loss: 0.9766 | 1.9025
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.8754219466403381, 'learning_rate': 0.0002093614812893629, 'batch_size': 176, 'step_size': 15, 'gamma': 0.7963531487610382}
Epoch 1/300, resid Loss: 1.2953 | 0.3193
Epoch 2/300, resid Loss: 0.2666 | 0.1588
Epoch 3/300, resid Loss: 0.2641 | 0.1939
Epoch 4/300, resid Loss: 0.3384 | 0.1788
Epoch 5/300, resid Loss: 0.3248 | 0.1599
Epoch 6/300, resid Loss: 0.2973 | 0.1482
Epoch 7/300, resid Loss: 0.3218 | 0.3941
Epoch 8/300, resid Loss: 0.2045 | 0.1364
Epoch 9/300, resid Loss: 0.2536 | 0.1790
Epoch 10/300, resid Loss: 0.3478 | 0.1758
Epoch 11/300, resid Loss: 0.3648 | 0.0945
Epoch 12/300, resid Loss: 0.2444 | 0.1404
Epoch 13/300, resid Loss: 0.1950 | 0.1417
Epoch 14/300, resid Loss: 0.1706 | 0.2035
Epoch 15/300, resid Loss: 0.1552 | 0.1384
Epoch 16/300, resid Loss: 0.1574 | 0.1425
Epoch 17/300, resid Loss: 0.1643 | 0.0783
Epoch 18/300, resid Loss: 0.1446 | 0.0803
Epoch 19/300, resid Loss: 0.1593 | 0.0838
Epoch 20/300, resid Loss: 0.1848 | 0.1221
Epoch 21/300, resid Loss: 0.1813 | 0.2843
Epoch 22/300, resid Loss: 0.2156 | 0.1485
Epoch 23/300, resid Loss: 0.2841 | 0.0741
Epoch 24/300, resid Loss: 0.1880 | 0.1121
Epoch 25/300, resid Loss: 0.1462 | 0.0748
Epoch 26/300, resid Loss: 0.1553 | 0.0971
Epoch 27/300, resid Loss: 0.1296 | 0.0760
Epoch 28/300, resid Loss: 0.1384 | 0.0634
Epoch 29/300, resid Loss: 0.1066 | 0.0698
Epoch 30/300, resid Loss: 0.1178 | 0.0618
Epoch 31/300, resid Loss: 0.1130 | 0.0667
Epoch 32/300, resid Loss: 0.1194 | 0.0644
Epoch 33/300, resid Loss: 0.1127 | 0.0580
Epoch 34/300, resid Loss: 0.1017 | 0.0566
Epoch 35/300, resid Loss: 0.0944 | 0.0573
Epoch 36/300, resid Loss: 0.0914 | 0.0453
Epoch 37/300, resid Loss: 0.0894 | 0.0508
Epoch 38/300, resid Loss: 0.0908 | 0.0498
Epoch 39/300, resid Loss: 0.0917 | 0.0458
Epoch 40/300, resid Loss: 0.0893 | 0.0471
Epoch 41/300, resid Loss: 0.0845 | 0.0446
Epoch 42/300, resid Loss: 0.0833 | 0.0505
Epoch 43/300, resid Loss: 0.0835 | 0.0411
Epoch 44/300, resid Loss: 0.0829 | 0.0502
Epoch 45/300, resid Loss: 0.0840 | 0.0423
Epoch 46/300, resid Loss: 0.0832 | 0.0438
Epoch 47/300, resid Loss: 0.0828 | 0.0435
Epoch 48/300, resid Loss: 0.0815 | 0.0406
Epoch 49/300, resid Loss: 0.0810 | 0.0462
Epoch 50/300, resid Loss: 0.0808 | 0.0388
Epoch 51/300, resid Loss: 0.0821 | 0.0545
Epoch 52/300, resid Loss: 0.0836 | 0.0385
Epoch 53/300, resid Loss: 0.0790 | 0.0386
Epoch 54/300, resid Loss: 0.0802 | 0.0395
Epoch 55/300, resid Loss: 0.0796 | 0.0456
Epoch 56/300, resid Loss: 0.0838 | 0.0419
Epoch 57/300, resid Loss: 0.0808 | 0.0417
Epoch 58/300, resid Loss: 0.0805 | 0.0387
Epoch 59/300, resid Loss: 0.0802 | 0.0453
Epoch 60/300, resid Loss: 0.0824 | 0.0412
Epoch 61/300, resid Loss: 0.0789 | 0.0390
Epoch 62/300, resid Loss: 0.0787 | 0.0373
Epoch 63/300, resid Loss: 0.0795 | 0.0433
Epoch 64/300, resid Loss: 0.0813 | 0.0377
Epoch 65/300, resid Loss: 0.0801 | 0.0448
Epoch 66/300, resid Loss: 0.0817 | 0.0372
Epoch 67/300, resid Loss: 0.0814 | 0.0441
Epoch 68/300, resid Loss: 0.0834 | 0.0382
Epoch 69/300, resid Loss: 0.0797 | 0.0427
Epoch 70/300, resid Loss: 0.0803 | 0.0401
Epoch 71/300, resid Loss: 0.0797 | 0.0428
Epoch 72/300, resid Loss: 0.0801 | 0.0391
Epoch 73/300, resid Loss: 0.0774 | 0.0413
Epoch 74/300, resid Loss: 0.0773 | 0.0386
Epoch 75/300, resid Loss: 0.0771 | 0.0418
Epoch 76/300, resid Loss: 0.0765 | 0.0385
Epoch 77/300, resid Loss: 0.0745 | 0.0383
Epoch 78/300, resid Loss: 0.0739 | 0.0377
Epoch 79/300, resid Loss: 0.0736 | 0.0384
Epoch 80/300, resid Loss: 0.0730 | 0.0365
Epoch 81/300, resid Loss: 0.0726 | 0.0379
Epoch 82/300, resid Loss: 0.0724 | 0.0360
Epoch 83/300, resid Loss: 0.0722 | 0.0375
Epoch 84/300, resid Loss: 0.0722 | 0.0362
Epoch 85/300, resid Loss: 0.0717 | 0.0361
Epoch 86/300, resid Loss: 0.0719 | 0.0372
Epoch 87/300, resid Loss: 0.0721 | 0.0364
Epoch 88/300, resid Loss: 0.0716 | 0.0358
Epoch 89/300, resid Loss: 0.0712 | 0.0358
Epoch 90/300, resid Loss: 0.0714 | 0.0356
Epoch 91/300, resid Loss: 0.0710 | 0.0357
Epoch 92/300, resid Loss: 0.0708 | 0.0350
Epoch 93/300, resid Loss: 0.0706 | 0.0351
Epoch 94/300, resid Loss: 0.0707 | 0.0350
Epoch 95/300, resid Loss: 0.0704 | 0.0350
Epoch 96/300, resid Loss: 0.0704 | 0.0347
Epoch 97/300, resid Loss: 0.0702 | 0.0345
Epoch 98/300, resid Loss: 0.0702 | 0.0350
Epoch 99/300, resid Loss: 0.0702 | 0.0347
Epoch 100/300, resid Loss: 0.0700 | 0.0345
Epoch 101/300, resid Loss: 0.0699 | 0.0341
Epoch 102/300, resid Loss: 0.0698 | 0.0342
Epoch 103/300, resid Loss: 0.0698 | 0.0342
Epoch 104/300, resid Loss: 0.0697 | 0.0342
Epoch 105/300, resid Loss: 0.0697 | 0.0343
Epoch 106/300, resid Loss: 0.0696 | 0.0338
Epoch 107/300, resid Loss: 0.0696 | 0.0341
Epoch 108/300, resid Loss: 0.0696 | 0.0340
Epoch 109/300, resid Loss: 0.0694 | 0.0340
Epoch 110/300, resid Loss: 0.0694 | 0.0337
Epoch 111/300, resid Loss: 0.0693 | 0.0337
Epoch 112/300, resid Loss: 0.0693 | 0.0338
Epoch 113/300, resid Loss: 0.0692 | 0.0338
Epoch 114/300, resid Loss: 0.0692 | 0.0337
Epoch 115/300, resid Loss: 0.0691 | 0.0334
Epoch 116/300, resid Loss: 0.0690 | 0.0336
Epoch 117/300, resid Loss: 0.0691 | 0.0337
Epoch 118/300, resid Loss: 0.0689 | 0.0336
Epoch 119/300, resid Loss: 0.0689 | 0.0334
Epoch 120/300, resid Loss: 0.0688 | 0.0334
Epoch 121/300, resid Loss: 0.0688 | 0.0335
Epoch 122/300, resid Loss: 0.0687 | 0.0335
Epoch 123/300, resid Loss: 0.0687 | 0.0333
Epoch 124/300, resid Loss: 0.0686 | 0.0333
Epoch 125/300, resid Loss: 0.0686 | 0.0333
Epoch 126/300, resid Loss: 0.0685 | 0.0333
Epoch 127/300, resid Loss: 0.0685 | 0.0332
Epoch 128/300, resid Loss: 0.0684 | 0.0332
Epoch 129/300, resid Loss: 0.0684 | 0.0332
Epoch 130/300, resid Loss: 0.0684 | 0.0333
Epoch 131/300, resid Loss: 0.0683 | 0.0332
Epoch 132/300, resid Loss: 0.0683 | 0.0331
Epoch 133/300, resid Loss: 0.0683 | 0.0332
Epoch 134/300, resid Loss: 0.0682 | 0.0332
Epoch 135/300, resid Loss: 0.0682 | 0.0331
Epoch 136/300, resid Loss: 0.0682 | 0.0331
Epoch 137/300, resid Loss: 0.0681 | 0.0331
Epoch 138/300, resid Loss: 0.0681 | 0.0331
Epoch 139/300, resid Loss: 0.0681 | 0.0331
Epoch 140/300, resid Loss: 0.0681 | 0.0331
Epoch 141/300, resid Loss: 0.0680 | 0.0331
Epoch 142/300, resid Loss: 0.0680 | 0.0331
Epoch 143/300, resid Loss: 0.0680 | 0.0331
Epoch 144/300, resid Loss: 0.0679 | 0.0331
Epoch 145/300, resid Loss: 0.0679 | 0.0330
Epoch 146/300, resid Loss: 0.0679 | 0.0330
Epoch 147/300, resid Loss: 0.0679 | 0.0330
Epoch 148/300, resid Loss: 0.0679 | 0.0330
Epoch 149/300, resid Loss: 0.0678 | 0.0330
Epoch 150/300, resid Loss: 0.0678 | 0.0330
Epoch 151/300, resid Loss: 0.0678 | 0.0330
Epoch 152/300, resid Loss: 0.0678 | 0.0330
Epoch 153/300, resid Loss: 0.0678 | 0.0330
Epoch 154/300, resid Loss: 0.0677 | 0.0330
Epoch 155/300, resid Loss: 0.0677 | 0.0330
Epoch 156/300, resid Loss: 0.0677 | 0.0330
Epoch 157/300, resid Loss: 0.0677 | 0.0330
Epoch 158/300, resid Loss: 0.0677 | 0.0330
Epoch 159/300, resid Loss: 0.0677 | 0.0330
Epoch 160/300, resid Loss: 0.0676 | 0.0330
Epoch 161/300, resid Loss: 0.0676 | 0.0329
Epoch 162/300, resid Loss: 0.0676 | 0.0329
Epoch 163/300, resid Loss: 0.0676 | 0.0329
Epoch 164/300, resid Loss: 0.0676 | 0.0329
Epoch 165/300, resid Loss: 0.0676 | 0.0329
Epoch 166/300, resid Loss: 0.0676 | 0.0329
Epoch 167/300, resid Loss: 0.0676 | 0.0329
Epoch 168/300, resid Loss: 0.0675 | 0.0329
Epoch 169/300, resid Loss: 0.0675 | 0.0329
Epoch 170/300, resid Loss: 0.0675 | 0.0329
Epoch 171/300, resid Loss: 0.0675 | 0.0329
Epoch 172/300, resid Loss: 0.0675 | 0.0329
Epoch 173/300, resid Loss: 0.0675 | 0.0329
Epoch 174/300, resid Loss: 0.0675 | 0.0329
Epoch 175/300, resid Loss: 0.0675 | 0.0329
Epoch 176/300, resid Loss: 0.0675 | 0.0329
Epoch 177/300, resid Loss: 0.0674 | 0.0329
Epoch 178/300, resid Loss: 0.0674 | 0.0329
Epoch 179/300, resid Loss: 0.0674 | 0.0329
Epoch 180/300, resid Loss: 0.0674 | 0.0329
Epoch 181/300, resid Loss: 0.0674 | 0.0329
Epoch 182/300, resid Loss: 0.0674 | 0.0329
Epoch 183/300, resid Loss: 0.0674 | 0.0329
Epoch 184/300, resid Loss: 0.0674 | 0.0329
Epoch 185/300, resid Loss: 0.0674 | 0.0329
Epoch 186/300, resid Loss: 0.0674 | 0.0329
Epoch 187/300, resid Loss: 0.0674 | 0.0328
Epoch 188/300, resid Loss: 0.0673 | 0.0328
Epoch 189/300, resid Loss: 0.0673 | 0.0328
Epoch 190/300, resid Loss: 0.0673 | 0.0328
Epoch 191/300, resid Loss: 0.0673 | 0.0328
Epoch 192/300, resid Loss: 0.0673 | 0.0328
Epoch 193/300, resid Loss: 0.0673 | 0.0328
Epoch 194/300, resid Loss: 0.0673 | 0.0328
Epoch 195/300, resid Loss: 0.0673 | 0.0328
Epoch 196/300, resid Loss: 0.0673 | 0.0328
Epoch 197/300, resid Loss: 0.0673 | 0.0328
Epoch 198/300, resid Loss: 0.0673 | 0.0328
Epoch 199/300, resid Loss: 0.0673 | 0.0328
Epoch 200/300, resid Loss: 0.0673 | 0.0328
Epoch 201/300, resid Loss: 0.0673 | 0.0328
Epoch 202/300, resid Loss: 0.0673 | 0.0328
Epoch 203/300, resid Loss: 0.0673 | 0.0328
Epoch 204/300, resid Loss: 0.0673 | 0.0328
Epoch 205/300, resid Loss: 0.0672 | 0.0328
Epoch 206/300, resid Loss: 0.0672 | 0.0328
Epoch 207/300, resid Loss: 0.0672 | 0.0328
Epoch 208/300, resid Loss: 0.0672 | 0.0328
Epoch 209/300, resid Loss: 0.0672 | 0.0328
Epoch 210/300, resid Loss: 0.0672 | 0.0328
Epoch 211/300, resid Loss: 0.0672 | 0.0328
Epoch 212/300, resid Loss: 0.0672 | 0.0328
Epoch 213/300, resid Loss: 0.0672 | 0.0328
Epoch 214/300, resid Loss: 0.0672 | 0.0328
Epoch 215/300, resid Loss: 0.0672 | 0.0328
Epoch 216/300, resid Loss: 0.0672 | 0.0328
Epoch 217/300, resid Loss: 0.0672 | 0.0328
Epoch 218/300, resid Loss: 0.0672 | 0.0328
Epoch 219/300, resid Loss: 0.0672 | 0.0328
Epoch 220/300, resid Loss: 0.0672 | 0.0328
Epoch 221/300, resid Loss: 0.0672 | 0.0328
Epoch 222/300, resid Loss: 0.0672 | 0.0328
Epoch 223/300, resid Loss: 0.0672 | 0.0328
Epoch 224/300, resid Loss: 0.0672 | 0.0328
Epoch 225/300, resid Loss: 0.0672 | 0.0328
Epoch 226/300, resid Loss: 0.0672 | 0.0328
Epoch 227/300, resid Loss: 0.0672 | 0.0328
Epoch 228/300, resid Loss: 0.0672 | 0.0328
Epoch 229/300, resid Loss: 0.0672 | 0.0328
Epoch 230/300, resid Loss: 0.0672 | 0.0328
Epoch 231/300, resid Loss: 0.0672 | 0.0328
Epoch 232/300, resid Loss: 0.0672 | 0.0328
Epoch 233/300, resid Loss: 0.0672 | 0.0328
Epoch 234/300, resid Loss: 0.0671 | 0.0328
Epoch 235/300, resid Loss: 0.0671 | 0.0328
Epoch 236/300, resid Loss: 0.0671 | 0.0328
Epoch 237/300, resid Loss: 0.0671 | 0.0328
Epoch 238/300, resid Loss: 0.0671 | 0.0328
Epoch 239/300, resid Loss: 0.0671 | 0.0328
Epoch 240/300, resid Loss: 0.0671 | 0.0328
Epoch 241/300, resid Loss: 0.0671 | 0.0328
Epoch 242/300, resid Loss: 0.0671 | 0.0328
Epoch 243/300, resid Loss: 0.0671 | 0.0328
Epoch 244/300, resid Loss: 0.0671 | 0.0328
Epoch 245/300, resid Loss: 0.0671 | 0.0328
Epoch 246/300, resid Loss: 0.0671 | 0.0328
Epoch 247/300, resid Loss: 0.0671 | 0.0328
Epoch 248/300, resid Loss: 0.0671 | 0.0328
Epoch 249/300, resid Loss: 0.0671 | 0.0328
Epoch 250/300, resid Loss: 0.0671 | 0.0328
Epoch 251/300, resid Loss: 0.0671 | 0.0328
Epoch 252/300, resid Loss: 0.0671 | 0.0328
Epoch 253/300, resid Loss: 0.0671 | 0.0328
Epoch 254/300, resid Loss: 0.0671 | 0.0328
Epoch 255/300, resid Loss: 0.0671 | 0.0328
Epoch 256/300, resid Loss: 0.0671 | 0.0328
Epoch 257/300, resid Loss: 0.0671 | 0.0328
Epoch 258/300, resid Loss: 0.0671 | 0.0328
Epoch 259/300, resid Loss: 0.0671 | 0.0328
Epoch 260/300, resid Loss: 0.0671 | 0.0328
Epoch 261/300, resid Loss: 0.0671 | 0.0328
Epoch 262/300, resid Loss: 0.0671 | 0.0328
Epoch 263/300, resid Loss: 0.0671 | 0.0328
Epoch 264/300, resid Loss: 0.0671 | 0.0328
Epoch 265/300, resid Loss: 0.0671 | 0.0328
Epoch 266/300, resid Loss: 0.0671 | 0.0328
Epoch 267/300, resid Loss: 0.0671 | 0.0328
Epoch 268/300, resid Loss: 0.0671 | 0.0328
Epoch 269/300, resid Loss: 0.0671 | 0.0328
Epoch 270/300, resid Loss: 0.0671 | 0.0328
Epoch 271/300, resid Loss: 0.0671 | 0.0328
Epoch 272/300, resid Loss: 0.0671 | 0.0328
Epoch 273/300, resid Loss: 0.0671 | 0.0328
Epoch 274/300, resid Loss: 0.0671 | 0.0328
Epoch 275/300, resid Loss: 0.0671 | 0.0328
Epoch 276/300, resid Loss: 0.0671 | 0.0328
Epoch 277/300, resid Loss: 0.0671 | 0.0328
Epoch 278/300, resid Loss: 0.0671 | 0.0328
Epoch 279/300, resid Loss: 0.0671 | 0.0328
Epoch 280/300, resid Loss: 0.0671 | 0.0328
Epoch 281/300, resid Loss: 0.0671 | 0.0328
Epoch 282/300, resid Loss: 0.0671 | 0.0328
Epoch 283/300, resid Loss: 0.0671 | 0.0328
Epoch 284/300, resid Loss: 0.0671 | 0.0328
Epoch 285/300, resid Loss: 0.0671 | 0.0328
Epoch 286/300, resid Loss: 0.0671 | 0.0328
Epoch 287/300, resid Loss: 0.0671 | 0.0328
Epoch 288/300, resid Loss: 0.0671 | 0.0328
Epoch 289/300, resid Loss: 0.0671 | 0.0328
Epoch 290/300, resid Loss: 0.0671 | 0.0328
Epoch 291/300, resid Loss: 0.0671 | 0.0327
Epoch 292/300, resid Loss: 0.0671 | 0.0327
Epoch 293/300, resid Loss: 0.0671 | 0.0327
Epoch 294/300, resid Loss: 0.0671 | 0.0327
Epoch 295/300, resid Loss: 0.0671 | 0.0327
Epoch 296/300, resid Loss: 0.0671 | 0.0327
Epoch 297/300, resid Loss: 0.0671 | 0.0327
Epoch 298/300, resid Loss: 0.0671 | 0.0327
Epoch 299/300, resid Loss: 0.0671 | 0.0327
Epoch 300/300, resid Loss: 0.0671 | 0.0327
Runtime (seconds): 11393.831143140793
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[162.42113]
[-3.071847]
[3.1163049]
[15.390993]
[-0.5883045]
[21.260973]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 1.9859594143927097
RMSE: 1.40924072265625
MAE: 1.40924072265625
R-squared: nan
[198.52924]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
