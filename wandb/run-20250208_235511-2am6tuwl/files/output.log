[32m[I 2025-02-08 23:55:15,884][0m A new study created in memory with name: no-name-ea4fec94-b2f5-4d16-9041-1aff8092e6a6[0m
[32m[I 2025-02-08 23:56:05,569][0m Trial 0 finished with value: 0.8906553547080373 and parameters: {'observation_period_num': 55, 'train_rates': 0.6169321149795592, 'learning_rate': 1.992351974727775e-06, 'batch_size': 91, 'step_size': 3, 'gamma': 0.8326725860140464}. Best is trial 0 with value: 0.8906553547080373.[0m
[32m[I 2025-02-08 23:56:34,034][0m Trial 1 finished with value: 0.04951697932456129 and parameters: {'observation_period_num': 32, 'train_rates': 0.8434816902085384, 'learning_rate': 0.0005664328938780038, 'batch_size': 215, 'step_size': 8, 'gamma': 0.7529355529624835}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-08 23:57:48,046][0m Trial 2 finished with value: 0.43377531230595406 and parameters: {'observation_period_num': 150, 'train_rates': 0.7132351435560177, 'learning_rate': 1.3223367433121237e-05, 'batch_size': 64, 'step_size': 13, 'gamma': 0.7792899387264258}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-08 23:58:16,991][0m Trial 3 finished with value: 0.26379638164083963 and parameters: {'observation_period_num': 18, 'train_rates': 0.7830984261455626, 'learning_rate': 2.2754232814690316e-05, 'batch_size': 210, 'step_size': 8, 'gamma': 0.8491052084837158}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-08 23:58:40,473][0m Trial 4 finished with value: 1.04161669810613 and parameters: {'observation_period_num': 66, 'train_rates': 0.6779920851484429, 'learning_rate': 3.4278794941011702e-06, 'batch_size': 231, 'step_size': 1, 'gamma': 0.9108059770347082}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-08 23:59:20,338][0m Trial 5 finished with value: 0.2847549067389075 and parameters: {'observation_period_num': 132, 'train_rates': 0.6684562983649041, 'learning_rate': 6.395507868472308e-05, 'batch_size': 121, 'step_size': 7, 'gamma': 0.7881439702663395}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:01:09,056][0m Trial 6 finished with value: 0.15646961471944484 and parameters: {'observation_period_num': 213, 'train_rates': 0.8670891834670928, 'learning_rate': 3.215741699151357e-05, 'batch_size': 49, 'step_size': 3, 'gamma': 0.9504883415246508}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:01:35,842][0m Trial 7 finished with value: 0.5852072834968567 and parameters: {'observation_period_num': 204, 'train_rates': 0.9730986905407204, 'learning_rate': 3.830356364595689e-05, 'batch_size': 237, 'step_size': 4, 'gamma': 0.8069466531689353}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:02:09,118][0m Trial 8 finished with value: 1.2720890166985728 and parameters: {'observation_period_num': 17, 'train_rates': 0.8166176340435591, 'learning_rate': 1.033028941830632e-06, 'batch_size': 178, 'step_size': 10, 'gamma': 0.821351051747898}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:02:30,695][0m Trial 9 finished with value: 0.4284878071131451 and parameters: {'observation_period_num': 58, 'train_rates': 0.6196023996971837, 'learning_rate': 8.242037644659722e-06, 'batch_size': 229, 'step_size': 7, 'gamma': 0.9164104437188607}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:03:08,256][0m Trial 10 finished with value: 0.16044269350110268 and parameters: {'observation_period_num': 100, 'train_rates': 0.9326035803343804, 'learning_rate': 0.0008610271411631488, 'batch_size': 180, 'step_size': 15, 'gamma': 0.754587995346534}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:08:27,765][0m Trial 11 finished with value: 0.36408800813886855 and parameters: {'observation_period_num': 235, 'train_rates': 0.8699201701883783, 'learning_rate': 0.0002595582912771005, 'batch_size': 16, 'step_size': 4, 'gamma': 0.9837827874907423}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:11:46,508][0m Trial 12 finished with value: 0.3443525948006251 and parameters: {'observation_period_num': 178, 'train_rates': 0.880767523490476, 'learning_rate': 0.00014080797072008993, 'batch_size': 27, 'step_size': 11, 'gamma': 0.9843962427156336}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:12:22,914][0m Trial 13 finished with value: 0.259656848050259 and parameters: {'observation_period_num': 242, 'train_rates': 0.7950813812036167, 'learning_rate': 0.0009910535894822443, 'batch_size': 146, 'step_size': 1, 'gamma': 0.8957934529263278}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:13:39,082][0m Trial 14 finished with value: 0.15327223716049246 and parameters: {'observation_period_num': 94, 'train_rates': 0.8741033478171574, 'learning_rate': 0.00024017954160575916, 'batch_size': 73, 'step_size': 6, 'gamma': 0.9392682546979638}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:14:39,260][0m Trial 15 finished with value: 0.12121109186131278 and parameters: {'observation_period_num': 100, 'train_rates': 0.9164603112994167, 'learning_rate': 0.00031211800539159435, 'batch_size': 98, 'step_size': 6, 'gamma': 0.8695065100486876}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:15:32,552][0m Trial 16 finished with value: 0.15034091972105995 and parameters: {'observation_period_num': 101, 'train_rates': 0.9385405871420812, 'learning_rate': 0.00043826111268142784, 'batch_size': 116, 'step_size': 10, 'gamma': 0.863767175402847}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:16:06,137][0m Trial 17 finished with value: 0.17183154460155603 and parameters: {'observation_period_num': 6, 'train_rates': 0.7467546177298561, 'learning_rate': 0.00010475472619474178, 'batch_size': 161, 'step_size': 6, 'gamma': 0.8802145905159351}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:16:30,339][0m Trial 18 finished with value: 0.0550669452303466 and parameters: {'observation_period_num': 40, 'train_rates': 0.8229879391338872, 'learning_rate': 0.00042283385263994347, 'batch_size': 254, 'step_size': 9, 'gamma': 0.7706267708557898}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:16:54,587][0m Trial 19 finished with value: 0.058208715811013736 and parameters: {'observation_period_num': 36, 'train_rates': 0.817560350785261, 'learning_rate': 0.0006223448594740427, 'batch_size': 255, 'step_size': 12, 'gamma': 0.7596252926579657}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:17:21,385][0m Trial 20 finished with value: 0.22469659445155019 and parameters: {'observation_period_num': 78, 'train_rates': 0.7569092362804712, 'learning_rate': 0.00013625752893110563, 'batch_size': 201, 'step_size': 8, 'gamma': 0.7856600539271621}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:17:45,467][0m Trial 21 finished with value: 0.054320027909575144 and parameters: {'observation_period_num': 41, 'train_rates': 0.8275971201008545, 'learning_rate': 0.0005412635423950374, 'batch_size': 255, 'step_size': 13, 'gamma': 0.7545459646429307}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:18:08,842][0m Trial 22 finished with value: 0.06230928934720077 and parameters: {'observation_period_num': 41, 'train_rates': 0.8389311725470046, 'learning_rate': 0.00041415791736344493, 'batch_size': 256, 'step_size': 15, 'gamma': 0.7506246189138776}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:18:40,248][0m Trial 23 finished with value: 0.052788602537475526 and parameters: {'observation_period_num': 29, 'train_rates': 0.838573751013775, 'learning_rate': 0.0005307650010318164, 'batch_size': 205, 'step_size': 13, 'gamma': 0.7738661127095061}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:19:12,002][0m Trial 24 finished with value: 0.059336467457363026 and parameters: {'observation_period_num': 38, 'train_rates': 0.8967527678651326, 'learning_rate': 0.00016744542127306594, 'batch_size': 203, 'step_size': 13, 'gamma': 0.8033231159793206}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:19:40,613][0m Trial 25 finished with value: 0.12008805071856238 and parameters: {'observation_period_num': 81, 'train_rates': 0.8462465257649285, 'learning_rate': 8.312916630248323e-05, 'batch_size': 218, 'step_size': 13, 'gamma': 0.8003709009038658}. Best is trial 1 with value: 0.04951697932456129.[0m
[32m[I 2025-02-09 00:20:16,386][0m Trial 26 finished with value: 0.04662002623081207 and parameters: {'observation_period_num': 24, 'train_rates': 0.9696082982519357, 'learning_rate': 0.0006771745432058317, 'batch_size': 183, 'step_size': 11, 'gamma': 0.7730782245151512}. Best is trial 26 with value: 0.04662002623081207.[0m
[32m[I 2025-02-09 00:20:53,856][0m Trial 27 finished with value: 0.03971509262919426 and parameters: {'observation_period_num': 5, 'train_rates': 0.9791516656697851, 'learning_rate': 0.0009525159003130021, 'batch_size': 183, 'step_size': 11, 'gamma': 0.822727270212412}. Best is trial 27 with value: 0.03971509262919426.[0m
[32m[I 2025-02-09 00:21:31,255][0m Trial 28 finished with value: 0.023891394957900047 and parameters: {'observation_period_num': 11, 'train_rates': 0.9895761872055975, 'learning_rate': 0.0009397043000847746, 'batch_size': 181, 'step_size': 11, 'gamma': 0.832387037583148}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:22:08,595][0m Trial 29 finished with value: 0.04316508397459984 and parameters: {'observation_period_num': 5, 'train_rates': 0.9855726146409148, 'learning_rate': 0.0007731475423564735, 'batch_size': 178, 'step_size': 11, 'gamma': 0.82822244147228}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:22:49,746][0m Trial 30 finished with value: 0.05346144363284111 and parameters: {'observation_period_num': 5, 'train_rates': 0.9760784456115067, 'learning_rate': 0.00020406611205194417, 'batch_size': 158, 'step_size': 11, 'gamma': 0.8360356931748875}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:23:27,095][0m Trial 31 finished with value: 0.04486934095621109 and parameters: {'observation_period_num': 15, 'train_rates': 0.9847686687240519, 'learning_rate': 0.0009377994186929179, 'batch_size': 175, 'step_size': 11, 'gamma': 0.8286025411429572}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:24:06,336][0m Trial 32 finished with value: 0.03690173849463463 and parameters: {'observation_period_num': 5, 'train_rates': 0.9549269798936658, 'learning_rate': 0.0008540281032489898, 'batch_size': 166, 'step_size': 10, 'gamma': 0.825339583373926}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:24:52,337][0m Trial 33 finished with value: 0.07186954603953795 and parameters: {'observation_period_num': 59, 'train_rates': 0.9479584628151843, 'learning_rate': 0.0003406899557399321, 'batch_size': 134, 'step_size': 10, 'gamma': 0.8463639981291481}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:25:31,882][0m Trial 34 finished with value: 0.03451184183359146 and parameters: {'observation_period_num': 16, 'train_rates': 0.9550890066876204, 'learning_rate': 0.0007500922206788684, 'batch_size': 161, 'step_size': 9, 'gamma': 0.8169954598518604}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:26:10,290][0m Trial 35 finished with value: 0.40746245227118794 and parameters: {'observation_period_num': 54, 'train_rates': 0.9124366944232802, 'learning_rate': 5.029626317796154e-06, 'batch_size': 160, 'step_size': 12, 'gamma': 0.8165592080101272}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:26:44,046][0m Trial 36 finished with value: 0.1900094598531723 and parameters: {'observation_period_num': 22, 'train_rates': 0.9574262763391017, 'learning_rate': 1.5500397973927187e-05, 'batch_size': 192, 'step_size': 9, 'gamma': 0.8507790891101589}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:27:24,198][0m Trial 37 finished with value: 0.18448929767630415 and parameters: {'observation_period_num': 134, 'train_rates': 0.9237287907063129, 'learning_rate': 0.0009698136510736057, 'batch_size': 148, 'step_size': 9, 'gamma': 0.8419772588746747}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:28:13,529][0m Trial 38 finished with value: 0.09495666497991286 and parameters: {'observation_period_num': 75, 'train_rates': 0.955220760349811, 'learning_rate': 0.0005947274133369434, 'batch_size': 128, 'step_size': 8, 'gamma': 0.8557724494393039}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:29:12,862][0m Trial 39 finished with value: 0.053935594856739044 and parameters: {'observation_period_num': 19, 'train_rates': 0.9882532953256358, 'learning_rate': 4.8796492558601936e-05, 'batch_size': 108, 'step_size': 14, 'gamma': 0.8120803723289611}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:29:48,719][0m Trial 40 finished with value: 0.1295646878117222 and parameters: {'observation_period_num': 163, 'train_rates': 0.8960401189817806, 'learning_rate': 0.00031662734084378103, 'batch_size': 167, 'step_size': 12, 'gamma': 0.7956005798304151}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:30:24,773][0m Trial 41 finished with value: 0.03543860837817192 and parameters: {'observation_period_num': 6, 'train_rates': 0.9600441751073199, 'learning_rate': 0.0007250415915423158, 'batch_size': 189, 'step_size': 10, 'gamma': 0.8260744841007984}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:31:00,133][0m Trial 42 finished with value: 0.04055099934339523 and parameters: {'observation_period_num': 15, 'train_rates': 0.9590703695231623, 'learning_rate': 0.0007258002821145461, 'batch_size': 190, 'step_size': 10, 'gamma': 0.8203566239585516}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:31:41,520][0m Trial 43 finished with value: 0.05803248348682286 and parameters: {'observation_period_num': 51, 'train_rates': 0.939628325461586, 'learning_rate': 0.0003967999539890018, 'batch_size': 147, 'step_size': 9, 'gamma': 0.883894267403009}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:32:12,452][0m Trial 44 finished with value: 0.05385354906320572 and parameters: {'observation_period_num': 26, 'train_rates': 0.9615184254548534, 'learning_rate': 0.0005225036969197899, 'batch_size': 218, 'step_size': 10, 'gamma': 0.7942031574197721}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:32:44,337][0m Trial 45 finished with value: 0.03661992216580792 and parameters: {'observation_period_num': 7, 'train_rates': 0.9049610750075361, 'learning_rate': 0.0009896412079627777, 'batch_size': 194, 'step_size': 7, 'gamma': 0.8354072911594593}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:33:15,950][0m Trial 46 finished with value: 0.06653543445046502 and parameters: {'observation_period_num': 49, 'train_rates': 0.899549542945287, 'learning_rate': 0.00026565791202538895, 'batch_size': 195, 'step_size': 7, 'gamma': 0.8358429063619148}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:33:42,579][0m Trial 47 finished with value: 1.1715378761291504 and parameters: {'observation_period_num': 29, 'train_rates': 0.9281952248788647, 'learning_rate': 1.854174449752974e-06, 'batch_size': 234, 'step_size': 7, 'gamma': 0.8592969087173099}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:34:20,340][0m Trial 48 finished with value: 0.41421687602996826 and parameters: {'observation_period_num': 67, 'train_rates': 0.9439813607607489, 'learning_rate': 2.248879747191842e-05, 'batch_size': 170, 'step_size': 5, 'gamma': 0.8122090077319285}. Best is trial 28 with value: 0.023891394957900047.[0m
[32m[I 2025-02-09 00:34:50,224][0m Trial 49 finished with value: 0.04026900442207561 and parameters: {'observation_period_num': 14, 'train_rates': 0.914797509660855, 'learning_rate': 0.0007182423832047698, 'batch_size': 216, 'step_size': 8, 'gamma': 0.8423408708068184}. Best is trial 28 with value: 0.023891394957900047.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_GOOG_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.4029 | 0.1843
Epoch 2/300, Loss: 0.1818 | 0.1384
Epoch 3/300, Loss: 0.2473 | 0.2328
Epoch 4/300, Loss: 0.3296 | 0.5541
Epoch 5/300, Loss: 0.2903 | 0.4169
Epoch 6/300, Loss: 0.1894 | 0.3165
Epoch 7/300, Loss: 0.1511 | 0.0949
Epoch 8/300, Loss: 0.1522 | 0.0920
Epoch 9/300, Loss: 0.1472 | 0.0913
Epoch 10/300, Loss: 0.2436 | 0.1387
Epoch 11/300, Loss: 0.1430 | 0.1847
Epoch 12/300, Loss: 0.1201 | 0.1049
Epoch 13/300, Loss: 0.1080 | 0.0463
Epoch 14/300, Loss: 0.1082 | 0.0435
Epoch 15/300, Loss: 0.0986 | 0.0399
Epoch 16/300, Loss: 0.0959 | 0.0406
Epoch 17/300, Loss: 0.0956 | 0.0435
Epoch 18/300, Loss: 0.0919 | 0.0522
Epoch 19/300, Loss: 0.0895 | 0.0392
Epoch 20/300, Loss: 0.0872 | 0.0335
Epoch 21/300, Loss: 0.0856 | 0.0449
Epoch 22/300, Loss: 0.0832 | 0.0364
Epoch 23/300, Loss: 0.0817 | 0.0413
Epoch 24/300, Loss: 0.0804 | 0.0387
Epoch 25/300, Loss: 0.0797 | 0.0340
Epoch 26/300, Loss: 0.0795 | 0.0446
Epoch 27/300, Loss: 0.0786 | 0.0371
Epoch 28/300, Loss: 0.0817 | 0.0442
Epoch 29/300, Loss: 0.0900 | 0.0699
Epoch 30/300, Loss: 0.0862 | 0.0320
Epoch 31/300, Loss: 0.0807 | 0.0562
Epoch 32/300, Loss: 0.0817 | 0.0436
Epoch 33/300, Loss: 0.0776 | 0.0365
Epoch 34/300, Loss: 0.0770 | 0.0353
Epoch 35/300, Loss: 0.0752 | 0.0303
Epoch 36/300, Loss: 0.0755 | 0.0317
Epoch 37/300, Loss: 0.0746 | 0.0284
Epoch 38/300, Loss: 0.0747 | 0.0316
Epoch 39/300, Loss: 0.0757 | 0.0314
Epoch 40/300, Loss: 0.0766 | 0.0341
Epoch 41/300, Loss: 0.0778 | 0.0315
Epoch 42/300, Loss: 0.0748 | 0.0302
Epoch 43/300, Loss: 0.0738 | 0.0300
Epoch 44/300, Loss: 0.0718 | 0.0310
Epoch 45/300, Loss: 0.0718 | 0.0360
Epoch 46/300, Loss: 0.0718 | 0.0324
Epoch 47/300, Loss: 0.0709 | 0.0300
Epoch 48/300, Loss: 0.0698 | 0.0309
Epoch 49/300, Loss: 0.0691 | 0.0291
Epoch 50/300, Loss: 0.0686 | 0.0294
Epoch 51/300, Loss: 0.0684 | 0.0315
Epoch 52/300, Loss: 0.0679 | 0.0303
Epoch 53/300, Loss: 0.0675 | 0.0302
Epoch 54/300, Loss: 0.0672 | 0.0305
Epoch 55/300, Loss: 0.0669 | 0.0306
Epoch 56/300, Loss: 0.0666 | 0.0312
Epoch 57/300, Loss: 0.0664 | 0.0317
Epoch 58/300, Loss: 0.0662 | 0.0321
Epoch 59/300, Loss: 0.0659 | 0.0327
Epoch 60/300, Loss: 0.0657 | 0.0330
Epoch 61/300, Loss: 0.0655 | 0.0332
Epoch 62/300, Loss: 0.0653 | 0.0343
Epoch 63/300, Loss: 0.0651 | 0.0347
Epoch 64/300, Loss: 0.0649 | 0.0348
Epoch 65/300, Loss: 0.0648 | 0.0350
Epoch 66/300, Loss: 0.0647 | 0.0348
Epoch 67/300, Loss: 0.0647 | 0.0335
Epoch 68/300, Loss: 0.0648 | 0.0318
Epoch 69/300, Loss: 0.0647 | 0.0300
Epoch 70/300, Loss: 0.0647 | 0.0281
Epoch 71/300, Loss: 0.0646 | 0.0270
Epoch 72/300, Loss: 0.0649 | 0.0272
Epoch 73/300, Loss: 0.0656 | 0.0305
Epoch 74/300, Loss: 0.0649 | 0.0362
Epoch 75/300, Loss: 0.0638 | 0.0346
Epoch 76/300, Loss: 0.0633 | 0.0310
Epoch 77/300, Loss: 0.0631 | 0.0298
Epoch 78/300, Loss: 0.0629 | 0.0312
Epoch 79/300, Loss: 0.0628 | 0.0318
Epoch 80/300, Loss: 0.0627 | 0.0329
Epoch 81/300, Loss: 0.0625 | 0.0340
Epoch 82/300, Loss: 0.0623 | 0.0340
Epoch 83/300, Loss: 0.0621 | 0.0341
Epoch 84/300, Loss: 0.0620 | 0.0337
Epoch 85/300, Loss: 0.0619 | 0.0331
Epoch 86/300, Loss: 0.0618 | 0.0331
Epoch 87/300, Loss: 0.0617 | 0.0333
Epoch 88/300, Loss: 0.0616 | 0.0334
Epoch 89/300, Loss: 0.0615 | 0.0335
Epoch 90/300, Loss: 0.0614 | 0.0339
Epoch 91/300, Loss: 0.0612 | 0.0344
Epoch 92/300, Loss: 0.0611 | 0.0349
Epoch 93/300, Loss: 0.0610 | 0.0351
Epoch 94/300, Loss: 0.0610 | 0.0350
Epoch 95/300, Loss: 0.0609 | 0.0335
Epoch 96/300, Loss: 0.0608 | 0.0322
Epoch 97/300, Loss: 0.0607 | 0.0316
Epoch 98/300, Loss: 0.0606 | 0.0312
Epoch 99/300, Loss: 0.0606 | 0.0312
Epoch 100/300, Loss: 0.0605 | 0.0306
Epoch 101/300, Loss: 0.0606 | 0.0319
Epoch 102/300, Loss: 0.0605 | 0.0332
Epoch 103/300, Loss: 0.0606 | 0.0348
Epoch 104/300, Loss: 0.0609 | 0.0327
Epoch 105/300, Loss: 0.0618 | 0.0339
Epoch 106/300, Loss: 0.0624 | 0.0324
Epoch 107/300, Loss: 0.0629 | 0.0328
Epoch 108/300, Loss: 0.0628 | 0.0367
Epoch 109/300, Loss: 0.0623 | 0.0335
Epoch 110/300, Loss: 0.0615 | 0.0356
Epoch 111/300, Loss: 0.0605 | 0.0337
Epoch 112/300, Loss: 0.0598 | 0.0319
Epoch 113/300, Loss: 0.0596 | 0.0313
Epoch 114/300, Loss: 0.0595 | 0.0301
Epoch 115/300, Loss: 0.0594 | 0.0301
Epoch 116/300, Loss: 0.0593 | 0.0298
Epoch 117/300, Loss: 0.0593 | 0.0300
Epoch 118/300, Loss: 0.0592 | 0.0301
Epoch 119/300, Loss: 0.0592 | 0.0301
Epoch 120/300, Loss: 0.0591 | 0.0299
Epoch 121/300, Loss: 0.0591 | 0.0297
Epoch 122/300, Loss: 0.0591 | 0.0294
Epoch 123/300, Loss: 0.0590 | 0.0294
Epoch 124/300, Loss: 0.0590 | 0.0294
Epoch 125/300, Loss: 0.0589 | 0.0294
Epoch 126/300, Loss: 0.0589 | 0.0293
Epoch 127/300, Loss: 0.0589 | 0.0293
Epoch 128/300, Loss: 0.0588 | 0.0291
Epoch 129/300, Loss: 0.0588 | 0.0290
Epoch 130/300, Loss: 0.0588 | 0.0290
Epoch 131/300, Loss: 0.0587 | 0.0290
Epoch 132/300, Loss: 0.0587 | 0.0290
Epoch 133/300, Loss: 0.0587 | 0.0288
Epoch 134/300, Loss: 0.0587 | 0.0288
Epoch 135/300, Loss: 0.0586 | 0.0288
Epoch 136/300, Loss: 0.0586 | 0.0288
Epoch 137/300, Loss: 0.0586 | 0.0287
Epoch 138/300, Loss: 0.0586 | 0.0287
Epoch 139/300, Loss: 0.0585 | 0.0286
Epoch 140/300, Loss: 0.0585 | 0.0286
Epoch 141/300, Loss: 0.0585 | 0.0286
Epoch 142/300, Loss: 0.0585 | 0.0286
Epoch 143/300, Loss: 0.0585 | 0.0285
Epoch 144/300, Loss: 0.0584 | 0.0284
Epoch 145/300, Loss: 0.0584 | 0.0284
Epoch 146/300, Loss: 0.0584 | 0.0284
Epoch 147/300, Loss: 0.0584 | 0.0284
Epoch 148/300, Loss: 0.0584 | 0.0284
Epoch 149/300, Loss: 0.0583 | 0.0284
Epoch 150/300, Loss: 0.0583 | 0.0283
Epoch 151/300, Loss: 0.0583 | 0.0283
Epoch 152/300, Loss: 0.0583 | 0.0283
Epoch 153/300, Loss: 0.0583 | 0.0283
Epoch 154/300, Loss: 0.0583 | 0.0283
Epoch 155/300, Loss: 0.0583 | 0.0282
Epoch 156/300, Loss: 0.0582 | 0.0282
Epoch 157/300, Loss: 0.0582 | 0.0282
Epoch 158/300, Loss: 0.0582 | 0.0282
Epoch 159/300, Loss: 0.0582 | 0.0282
Epoch 160/300, Loss: 0.0582 | 0.0282
Epoch 161/300, Loss: 0.0582 | 0.0281
Epoch 162/300, Loss: 0.0582 | 0.0281
Epoch 163/300, Loss: 0.0582 | 0.0281
Epoch 164/300, Loss: 0.0582 | 0.0281
Epoch 165/300, Loss: 0.0581 | 0.0281
Epoch 166/300, Loss: 0.0581 | 0.0281
Epoch 167/300, Loss: 0.0581 | 0.0281
Epoch 168/300, Loss: 0.0581 | 0.0281
Epoch 169/300, Loss: 0.0581 | 0.0281
Epoch 170/300, Loss: 0.0581 | 0.0281
Epoch 171/300, Loss: 0.0581 | 0.0281
Epoch 172/300, Loss: 0.0581 | 0.0280
Epoch 173/300, Loss: 0.0581 | 0.0280
Epoch 174/300, Loss: 0.0581 | 0.0280
Epoch 175/300, Loss: 0.0581 | 0.0280
Epoch 176/300, Loss: 0.0580 | 0.0280
Epoch 177/300, Loss: 0.0580 | 0.0280
Epoch 178/300, Loss: 0.0580 | 0.0280
Epoch 179/300, Loss: 0.0580 | 0.0280
Epoch 180/300, Loss: 0.0580 | 0.0280
Epoch 181/300, Loss: 0.0580 | 0.0280
Epoch 182/300, Loss: 0.0580 | 0.0280
Epoch 183/300, Loss: 0.0580 | 0.0280
Epoch 184/300, Loss: 0.0580 | 0.0279
Epoch 185/300, Loss: 0.0580 | 0.0279
Epoch 186/300, Loss: 0.0580 | 0.0279
Epoch 187/300, Loss: 0.0580 | 0.0279
Epoch 188/300, Loss: 0.0580 | 0.0279
Epoch 189/300, Loss: 0.0580 | 0.0279
Epoch 190/300, Loss: 0.0580 | 0.0279
Epoch 191/300, Loss: 0.0580 | 0.0279
Epoch 192/300, Loss: 0.0580 | 0.0279
Epoch 193/300, Loss: 0.0580 | 0.0279
Epoch 194/300, Loss: 0.0579 | 0.0279
Epoch 195/300, Loss: 0.0579 | 0.0279
Epoch 196/300, Loss: 0.0579 | 0.0279
Epoch 197/300, Loss: 0.0579 | 0.0279
Epoch 198/300, Loss: 0.0579 | 0.0279
Epoch 199/300, Loss: 0.0579 | 0.0279
Epoch 200/300, Loss: 0.0579 | 0.0279
Epoch 201/300, Loss: 0.0579 | 0.0279
Epoch 202/300, Loss: 0.0579 | 0.0279
Epoch 203/300, Loss: 0.0579 | 0.0279
Epoch 204/300, Loss: 0.0579 | 0.0279
Epoch 205/300, Loss: 0.0579 | 0.0279
Epoch 206/300, Loss: 0.0579 | 0.0279
Epoch 207/300, Loss: 0.0579 | 0.0279
Epoch 208/300, Loss: 0.0579 | 0.0279
Epoch 209/300, Loss: 0.0579 | 0.0279
Epoch 210/300, Loss: 0.0579 | 0.0279
Epoch 211/300, Loss: 0.0579 | 0.0278
Epoch 212/300, Loss: 0.0579 | 0.0278
Epoch 213/300, Loss: 0.0579 | 0.0278
Epoch 214/300, Loss: 0.0579 | 0.0278
Epoch 215/300, Loss: 0.0579 | 0.0278
Epoch 216/300, Loss: 0.0579 | 0.0278
Epoch 217/300, Loss: 0.0579 | 0.0278
Epoch 218/300, Loss: 0.0579 | 0.0278
Epoch 219/300, Loss: 0.0579 | 0.0278
Epoch 220/300, Loss: 0.0579 | 0.0278
Epoch 221/300, Loss: 0.0579 | 0.0278
Epoch 222/300, Loss: 0.0579 | 0.0278
Epoch 223/300, Loss: 0.0579 | 0.0278
Epoch 224/300, Loss: 0.0579 | 0.0278
Epoch 225/300, Loss: 0.0579 | 0.0278
Epoch 226/300, Loss: 0.0579 | 0.0278
Epoch 227/300, Loss: 0.0579 | 0.0278
Epoch 228/300, Loss: 0.0579 | 0.0278
Epoch 229/300, Loss: 0.0579 | 0.0278
Epoch 230/300, Loss: 0.0579 | 0.0278
Epoch 231/300, Loss: 0.0579 | 0.0278
Epoch 232/300, Loss: 0.0579 | 0.0278
Epoch 233/300, Loss: 0.0579 | 0.0278
Epoch 234/300, Loss: 0.0579 | 0.0278
Epoch 235/300, Loss: 0.0579 | 0.0278
Epoch 236/300, Loss: 0.0579 | 0.0278
Epoch 237/300, Loss: 0.0579 | 0.0278
Epoch 238/300, Loss: 0.0578 | 0.0278
Epoch 239/300, Loss: 0.0578 | 0.0278
Epoch 240/300, Loss: 0.0578 | 0.0278
Epoch 241/300, Loss: 0.0578 | 0.0278
Epoch 242/300, Loss: 0.0578 | 0.0278
Epoch 243/300, Loss: 0.0578 | 0.0278
Epoch 244/300, Loss: 0.0578 | 0.0278
Epoch 245/300, Loss: 0.0578 | 0.0278
Epoch 246/300, Loss: 0.0578 | 0.0278
Epoch 247/300, Loss: 0.0578 | 0.0278
Epoch 248/300, Loss: 0.0578 | 0.0278
Epoch 249/300, Loss: 0.0578 | 0.0278
Epoch 250/300, Loss: 0.0578 | 0.0278
Epoch 251/300, Loss: 0.0578 | 0.0278
Epoch 252/300, Loss: 0.0578 | 0.0278
Epoch 253/300, Loss: 0.0578 | 0.0278
Epoch 254/300, Loss: 0.0578 | 0.0278
Epoch 255/300, Loss: 0.0578 | 0.0278
Epoch 256/300, Loss: 0.0578 | 0.0278
Epoch 257/300, Loss: 0.0578 | 0.0278
Epoch 258/300, Loss: 0.0578 | 0.0278
Epoch 259/300, Loss: 0.0578 | 0.0278
Epoch 260/300, Loss: 0.0578 | 0.0278
Epoch 261/300, Loss: 0.0578 | 0.0278
Epoch 262/300, Loss: 0.0578 | 0.0278
Epoch 263/300, Loss: 0.0578 | 0.0278
Epoch 264/300, Loss: 0.0578 | 0.0278
Epoch 265/300, Loss: 0.0578 | 0.0278
Epoch 266/300, Loss: 0.0578 | 0.0278
Epoch 267/300, Loss: 0.0578 | 0.0278
Epoch 268/300, Loss: 0.0578 | 0.0278
Epoch 269/300, Loss: 0.0578 | 0.0278
Epoch 270/300, Loss: 0.0578 | 0.0278
Epoch 271/300, Loss: 0.0578 | 0.0278
Epoch 272/300, Loss: 0.0578 | 0.0278
Epoch 273/300, Loss: 0.0578 | 0.0278
Epoch 274/300, Loss: 0.0578 | 0.0278
Epoch 275/300, Loss: 0.0578 | 0.0278
Epoch 276/300, Loss: 0.0578 | 0.0278
Epoch 277/300, Loss: 0.0578 | 0.0278
Epoch 278/300, Loss: 0.0578 | 0.0278
Epoch 279/300, Loss: 0.0578 | 0.0278
Epoch 280/300, Loss: 0.0578 | 0.0278
Epoch 281/300, Loss: 0.0578 | 0.0278
Epoch 282/300, Loss: 0.0578 | 0.0278
Epoch 283/300, Loss: 0.0578 | 0.0278
Epoch 284/300, Loss: 0.0578 | 0.0278
Epoch 285/300, Loss: 0.0578 | 0.0278
Epoch 286/300, Loss: 0.0578 | 0.0278
Epoch 287/300, Loss: 0.0578 | 0.0278
Epoch 288/300, Loss: 0.0578 | 0.0278
Epoch 289/300, Loss: 0.0578 | 0.0278
Epoch 290/300, Loss: 0.0578 | 0.0278
Epoch 291/300, Loss: 0.0578 | 0.0278
Epoch 292/300, Loss: 0.0578 | 0.0278
Epoch 293/300, Loss: 0.0578 | 0.0278
Epoch 294/300, Loss: 0.0578 | 0.0278
Epoch 295/300, Loss: 0.0578 | 0.0278
Epoch 296/300, Loss: 0.0578 | 0.0278
Epoch 297/300, Loss: 0.0578 | 0.0278
Epoch 298/300, Loss: 0.0578 | 0.0278
Epoch 299/300, Loss: 0.0578 | 0.0278
Epoch 300/300, Loss: 0.0578 | 0.0278
Runtime (seconds): 110.94831728935242
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 419.6477531513665
RMSE: 20.485305786132812
MAE: 20.485305786132812
R-squared: nan
[207.6253]
