ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-01-03 02:41:53,031][0m A new study created in memory with name: no-name-b979bbd4-4db3-46f1-a949-f6d1416f4c2b[0m
Non-A100 GPU detected, using math or mem efficient attention if input tensor is on cuda
[32m[I 2025-01-03 02:45:36,248][0m Trial 0 finished with value: 0.08252510818151328 and parameters: {'observation_period_num': 215, 'train_rates': 0.8337045226855049, 'learning_rate': 0.0009461425617671291, 'batch_size': 21, 'step_size': 11, 'gamma': 0.8779965630931883}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:46:09,990][0m Trial 1 finished with value: 0.24949029088020325 and parameters: {'observation_period_num': 237, 'train_rates': 0.7309452664200254, 'learning_rate': 5.202791093151466e-05, 'batch_size': 146, 'step_size': 7, 'gamma': 0.7925749719133809}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:46:45,664][0m Trial 2 finished with value: 0.22890048260145848 and parameters: {'observation_period_num': 39, 'train_rates': 0.6530511578736221, 'learning_rate': 3.382039104281606e-06, 'batch_size': 140, 'step_size': 11, 'gamma': 0.9396567300799856}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:47:18,603][0m Trial 3 finished with value: 0.527431309223175 and parameters: {'observation_period_num': 219, 'train_rates': 0.9251392923820374, 'learning_rate': 3.1114174166153767e-06, 'batch_size': 224, 'step_size': 14, 'gamma': 0.8127379966681278}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:48:16,408][0m Trial 4 finished with value: 0.5195156356867622 and parameters: {'observation_period_num': 61, 'train_rates': 0.8236520672804647, 'learning_rate': 2.7320684347944815e-06, 'batch_size': 93, 'step_size': 5, 'gamma': 0.7958101705284506}. Best is trial 0 with value: 0.08252510818151328.[0m
Early stopping at epoch 57
[32m[I 2025-01-03 02:48:34,415][0m Trial 5 finished with value: 0.40789385845786647 and parameters: {'observation_period_num': 234, 'train_rates': 0.6644317989905255, 'learning_rate': 0.00015353548575783614, 'batch_size': 180, 'step_size': 1, 'gamma': 0.797713316831133}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:49:30,745][0m Trial 6 finished with value: 0.5534193722101358 and parameters: {'observation_period_num': 69, 'train_rates': 0.9007631332910676, 'learning_rate': 1.1511167073004352e-06, 'batch_size': 102, 'step_size': 11, 'gamma': 0.7995705569033962}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:50:44,209][0m Trial 7 finished with value: 0.08972796140727117 and parameters: {'observation_period_num': 128, 'train_rates': 0.7543453194886465, 'learning_rate': 5.447158893611828e-05, 'batch_size': 68, 'step_size': 1, 'gamma': 0.9894369737645743}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:53:15,853][0m Trial 8 finished with value: 0.09286742400131569 and parameters: {'observation_period_num': 162, 'train_rates': 0.8156848562857245, 'learning_rate': 1.7264393191749263e-05, 'batch_size': 33, 'step_size': 5, 'gamma': 0.9078746121478688}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:54:07,922][0m Trial 9 finished with value: 0.0911980365392373 and parameters: {'observation_period_num': 96, 'train_rates': 0.9079272023617839, 'learning_rate': 0.0004502895299098726, 'batch_size': 108, 'step_size': 6, 'gamma': 0.9591880930002181}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:57:22,216][0m Trial 10 finished with value: 0.11684625625025992 and parameters: {'observation_period_num': 181, 'train_rates': 0.9819014218690159, 'learning_rate': 0.0005017280153003381, 'batch_size': 29, 'step_size': 14, 'gamma': 0.8609152092420785}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 02:58:59,980][0m Trial 11 finished with value: 0.15409408236434888 and parameters: {'observation_period_num': 153, 'train_rates': 0.7545638168619532, 'learning_rate': 4.429867991498818e-05, 'batch_size': 50, 'step_size': 2, 'gamma': 0.8731842817235504}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 03:00:20,525][0m Trial 12 finished with value: 0.14336269221726863 and parameters: {'observation_period_num': 117, 'train_rates': 0.7334240148189043, 'learning_rate': 0.00020931872277817027, 'batch_size': 61, 'step_size': 11, 'gamma': 0.9871741269512845}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 03:05:16,028][0m Trial 13 finished with value: 0.4432184195085879 and parameters: {'observation_period_num': 194, 'train_rates': 0.8540703574609105, 'learning_rate': 0.0008926280563420267, 'batch_size': 17, 'step_size': 9, 'gamma': 0.8588915495595331}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 03:06:26,027][0m Trial 14 finished with value: 0.10588626714446876 and parameters: {'observation_period_num': 129, 'train_rates': 0.7760034669551898, 'learning_rate': 1.5153019911172289e-05, 'batch_size': 72, 'step_size': 9, 'gamma': 0.9101610880342326}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 03:07:27,629][0m Trial 15 finished with value: 0.2716146217350908 and parameters: {'observation_period_num': 203, 'train_rates': 0.6965914353659306, 'learning_rate': 9.30920941494096e-05, 'batch_size': 75, 'step_size': 3, 'gamma': 0.7603054740988484}. Best is trial 0 with value: 0.08252510818151328.[0m
[32m[I 2025-01-03 03:07:59,883][0m Trial 16 finished with value: 0.05658995528966922 and parameters: {'observation_period_num': 5, 'train_rates': 0.8623614281538623, 'learning_rate': 1.128603980609227e-05, 'batch_size': 243, 'step_size': 15, 'gamma': 0.9761586981912874}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:08:31,736][0m Trial 17 finished with value: 0.06051674197955304 and parameters: {'observation_period_num': 13, 'train_rates': 0.8589314188836974, 'learning_rate': 1.4262719529729555e-05, 'batch_size': 240, 'step_size': 15, 'gamma': 0.9055841045971438}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:09:04,948][0m Trial 18 finished with value: 0.07699525691371746 and parameters: {'observation_period_num': 10, 'train_rates': 0.871600264032938, 'learning_rate': 8.28057864799316e-06, 'batch_size': 256, 'step_size': 15, 'gamma': 0.9426309900320766}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:09:42,077][0m Trial 19 finished with value: 0.10108237713575363 and parameters: {'observation_period_num': 5, 'train_rates': 0.9625958807758193, 'learning_rate': 8.880119239774965e-06, 'batch_size': 202, 'step_size': 13, 'gamma': 0.9046371265547479}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:10:14,684][0m Trial 20 finished with value: 0.09143167734146118 and parameters: {'observation_period_num': 30, 'train_rates': 0.9403962162447793, 'learning_rate': 2.1460945219686133e-05, 'batch_size': 255, 'step_size': 13, 'gamma': 0.9621847018686299}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:10:47,337][0m Trial 21 finished with value: 0.06734867308079077 and parameters: {'observation_period_num': 7, 'train_rates': 0.8734230374493462, 'learning_rate': 6.491398464726506e-06, 'batch_size': 253, 'step_size': 15, 'gamma': 0.925195211590005}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:11:20,944][0m Trial 22 finished with value: 0.18674795094939226 and parameters: {'observation_period_num': 35, 'train_rates': 0.8760886413292643, 'learning_rate': 5.543506260340194e-06, 'batch_size': 227, 'step_size': 15, 'gamma': 0.9352130479310502}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:11:52,191][0m Trial 23 finished with value: 0.142439805615732 and parameters: {'observation_period_num': 64, 'train_rates': 0.7999400377057178, 'learning_rate': 1.086492761690356e-05, 'batch_size': 226, 'step_size': 13, 'gamma': 0.9671478365952387}. Best is trial 16 with value: 0.05658995528966922.[0m
[32m[I 2025-01-03 03:12:29,304][0m Trial 24 finished with value: 0.04794276959970421 and parameters: {'observation_period_num': 21, 'train_rates': 0.8522895220246314, 'learning_rate': 2.8037144551792654e-05, 'batch_size': 175, 'step_size': 15, 'gamma': 0.9230328786186774}. Best is trial 24 with value: 0.04794276959970421.[0m
[32m[I 2025-01-03 03:13:04,004][0m Trial 25 finished with value: 0.08192735619377345 and parameters: {'observation_period_num': 82, 'train_rates': 0.8439623295230242, 'learning_rate': 2.7606110110402702e-05, 'batch_size': 173, 'step_size': 12, 'gamma': 0.89490863073321}. Best is trial 24 with value: 0.04794276959970421.[0m
[32m[I 2025-01-03 03:13:39,340][0m Trial 26 finished with value: 0.06844088214240991 and parameters: {'observation_period_num': 48, 'train_rates': 0.8909202414302351, 'learning_rate': 3.2377217678272656e-05, 'batch_size': 195, 'step_size': 14, 'gamma': 0.8328891106239057}. Best is trial 24 with value: 0.04794276959970421.[0m
[32m[I 2025-01-03 03:14:15,493][0m Trial 27 finished with value: 0.03606287566389864 and parameters: {'observation_period_num': 24, 'train_rates': 0.7938831718061212, 'learning_rate': 8.07364211034104e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.923444092511684}. Best is trial 27 with value: 0.03606287566389864.[0m
[32m[I 2025-01-03 03:14:51,855][0m Trial 28 finished with value: 0.042294596003270464 and parameters: {'observation_period_num': 30, 'train_rates': 0.7917982207472523, 'learning_rate': 8.413776328459884e-05, 'batch_size': 162, 'step_size': 9, 'gamma': 0.9734203363863257}. Best is trial 27 with value: 0.03606287566389864.[0m
[32m[I 2025-01-03 03:15:27,842][0m Trial 29 finished with value: 0.06773104403288134 and parameters: {'observation_period_num': 94, 'train_rates': 0.7829321138292186, 'learning_rate': 8.300481192780413e-05, 'batch_size': 156, 'step_size': 9, 'gamma': 0.8899067021873791}. Best is trial 27 with value: 0.03606287566389864.[0m
[32m[I 2025-01-03 03:16:05,895][0m Trial 30 finished with value: 0.12371725463408896 and parameters: {'observation_period_num': 28, 'train_rates': 0.6196596033523847, 'learning_rate': 0.00016835154565627972, 'batch_size': 127, 'step_size': 8, 'gamma': 0.9244940412190068}. Best is trial 27 with value: 0.03606287566389864.[0m
[32m[I 2025-01-03 03:16:41,700][0m Trial 31 finished with value: 0.04385386633197057 and parameters: {'observation_period_num': 47, 'train_rates': 0.8331429715431619, 'learning_rate': 8.856674470153357e-05, 'batch_size': 164, 'step_size': 12, 'gamma': 0.9739377179303871}. Best is trial 27 with value: 0.03606287566389864.[0m
[32m[I 2025-01-03 03:17:18,185][0m Trial 32 finished with value: 0.05854266149774967 and parameters: {'observation_period_num': 49, 'train_rates': 0.8009807627865799, 'learning_rate': 8.49081357419401e-05, 'batch_size': 164, 'step_size': 10, 'gamma': 0.952768149478865}. Best is trial 27 with value: 0.03606287566389864.[0m
[32m[I 2025-01-03 03:18:02,467][0m Trial 33 finished with value: 0.032521713309306435 and parameters: {'observation_period_num': 21, 'train_rates': 0.8232954011256048, 'learning_rate': 0.00010682109215432726, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9725806076941893}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:18:45,506][0m Trial 34 finished with value: 0.05172621604119767 and parameters: {'observation_period_num': 46, 'train_rates': 0.7617793748407454, 'learning_rate': 0.00024764789056809966, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9746733486160101}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:19:26,892][0m Trial 35 finished with value: 0.05300096358674077 and parameters: {'observation_period_num': 58, 'train_rates': 0.8202037144098718, 'learning_rate': 0.00012464031475804447, 'batch_size': 139, 'step_size': 7, 'gamma': 0.9495758443854116}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:20:01,322][0m Trial 36 finished with value: 0.13991784596000123 and parameters: {'observation_period_num': 81, 'train_rates': 0.7001947653221171, 'learning_rate': 0.00028564823818968634, 'batch_size': 152, 'step_size': 10, 'gamma': 0.9745074702127876}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:20:34,677][0m Trial 37 finished with value: 0.042042768874508654 and parameters: {'observation_period_num': 23, 'train_rates': 0.8290827971810291, 'learning_rate': 6.244701555205798e-05, 'batch_size': 196, 'step_size': 8, 'gamma': 0.9373188298096891}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:21:04,715][0m Trial 38 finished with value: 0.09393867153405237 and parameters: {'observation_period_num': 26, 'train_rates': 0.7304860611307739, 'learning_rate': 5.6179331880040335e-05, 'batch_size': 201, 'step_size': 8, 'gamma': 0.935502819803456}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:21:50,746][0m Trial 39 finished with value: 0.039576012457550415 and parameters: {'observation_period_num': 37, 'train_rates': 0.8051464367778983, 'learning_rate': 4.733474712473956e-05, 'batch_size': 120, 'step_size': 6, 'gamma': 0.9480550333594321}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:22:37,102][0m Trial 40 finished with value: 0.05943838740102679 and parameters: {'observation_period_num': 77, 'train_rates': 0.806740204667826, 'learning_rate': 4.499644945660384e-05, 'batch_size': 116, 'step_size': 5, 'gamma': 0.9477167852717224}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:23:19,108][0m Trial 41 finished with value: 0.041970073471065635 and parameters: {'observation_period_num': 22, 'train_rates': 0.7802817371179871, 'learning_rate': 0.0001238629369484687, 'batch_size': 129, 'step_size': 6, 'gamma': 0.9889429401276828}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:24:20,626][0m Trial 42 finished with value: 0.04805929493015111 and parameters: {'observation_period_num': 38, 'train_rates': 0.7780365335651652, 'learning_rate': 0.00034338463500509486, 'batch_size': 86, 'step_size': 6, 'gamma': 0.9592850339440704}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:25:06,139][0m Trial 43 finished with value: 0.0483295526355505 and parameters: {'observation_period_num': 24, 'train_rates': 0.8265721953043562, 'learning_rate': 0.00012716706937957107, 'batch_size': 127, 'step_size': 4, 'gamma': 0.989318518050603}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:25:44,950][0m Trial 44 finished with value: 0.04229930913639166 and parameters: {'observation_period_num': 16, 'train_rates': 0.7498961482075092, 'learning_rate': 6.019223008023681e-05, 'batch_size': 144, 'step_size': 6, 'gamma': 0.9291245913111063}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:26:16,931][0m Trial 45 finished with value: 0.0637714718712378 and parameters: {'observation_period_num': 54, 'train_rates': 0.7165430464849473, 'learning_rate': 0.00018033231695498925, 'batch_size': 186, 'step_size': 8, 'gamma': 0.9157854550890104}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:27:06,193][0m Trial 46 finished with value: 0.16650264228091521 and parameters: {'observation_period_num': 245, 'train_rates': 0.836916943672005, 'learning_rate': 3.740011083606192e-05, 'batch_size': 104, 'step_size': 7, 'gamma': 0.9562559558772659}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:27:52,423][0m Trial 47 finished with value: 0.05942030745668173 and parameters: {'observation_period_num': 69, 'train_rates': 0.7703386563567953, 'learning_rate': 0.00012865049064137072, 'batch_size': 113, 'step_size': 5, 'gamma': 0.9394745137929482}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:28:33,634][0m Trial 48 finished with value: 0.043758035206426946 and parameters: {'observation_period_num': 40, 'train_rates': 0.8108435301852568, 'learning_rate': 6.586502893469738e-05, 'batch_size': 134, 'step_size': 4, 'gamma': 0.9808539104411846}. Best is trial 33 with value: 0.032521713309306435.[0m
[32m[I 2025-01-03 03:29:28,075][0m Trial 49 finished with value: 0.04781030304127141 and parameters: {'observation_period_num': 16, 'train_rates': 0.7894779127179716, 'learning_rate': 2.131929565939117e-05, 'batch_size': 97, 'step_size': 6, 'gamma': 0.8851324846164264}. Best is trial 33 with value: 0.032521713309306435.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-01-03 03:29:28,254][0m A new study created in memory with name: no-name-0a7096b6-6a99-4a14-9434-dae64aa864b4[0m
[32m[I 2025-01-03 03:33:40,908][0m Trial 0 finished with value: 0.23250640843595777 and parameters: {'observation_period_num': 225, 'train_rates': 0.8974460127069146, 'learning_rate': 3.908163693543679e-06, 'batch_size': 131, 'step_size': 12, 'gamma': 0.7634806025281718}. Best is trial 0 with value: 0.23250640843595777.[0m
[32m[I 2025-01-03 03:37:29,195][0m Trial 1 finished with value: 0.09075547464946235 and parameters: {'observation_period_num': 79, 'train_rates': 0.8041200571263442, 'learning_rate': 6.359897872328878e-05, 'batch_size': 158, 'step_size': 6, 'gamma': 0.9231217148691381}. Best is trial 1 with value: 0.09075547464946235.[0m
[32m[I 2025-01-03 03:45:04,218][0m Trial 2 finished with value: 0.04766771362887488 and parameters: {'observation_period_num': 25, 'train_rates': 0.8956375300287402, 'learning_rate': 0.00015046514872369828, 'batch_size': 68, 'step_size': 8, 'gamma': 0.9534118935215583}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 03:48:33,733][0m Trial 3 finished with value: 0.25837181853475394 and parameters: {'observation_period_num': 46, 'train_rates': 0.7027627447591935, 'learning_rate': 3.0828129540256445e-06, 'batch_size': 164, 'step_size': 7, 'gamma': 0.8113117257192083}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 03:53:26,686][0m Trial 4 finished with value: 0.2289586363431208 and parameters: {'observation_period_num': 213, 'train_rates': 0.6357274107018974, 'learning_rate': 0.00035397930899620507, 'batch_size': 80, 'step_size': 7, 'gamma': 0.9696329903953953}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 03:57:11,406][0m Trial 5 finished with value: 0.08916713126183078 and parameters: {'observation_period_num': 12, 'train_rates': 0.6890717126643986, 'learning_rate': 0.00046927034275650117, 'batch_size': 140, 'step_size': 13, 'gamma': 0.9160037275759045}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:00:40,980][0m Trial 6 finished with value: 0.13348263596080337 and parameters: {'observation_period_num': 129, 'train_rates': 0.8005881574214129, 'learning_rate': 3.2978917723137414e-05, 'batch_size': 231, 'step_size': 7, 'gamma': 0.8203426367858523}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:04:13,544][0m Trial 7 finished with value: 0.2734468630753613 and parameters: {'observation_period_num': 250, 'train_rates': 0.8049226093856504, 'learning_rate': 0.0006878946872834221, 'batch_size': 174, 'step_size': 7, 'gamma': 0.9444713272922002}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:07:54,408][0m Trial 8 finished with value: 0.19331006688682262 and parameters: {'observation_period_num': 241, 'train_rates': 0.8712480664864706, 'learning_rate': 4.079891511898559e-05, 'batch_size': 235, 'step_size': 4, 'gamma': 0.9285388575973272}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:11:55,163][0m Trial 9 finished with value: 0.09980335790575959 and parameters: {'observation_period_num': 48, 'train_rates': 0.9124880754175444, 'learning_rate': 1.8374670520239044e-05, 'batch_size': 235, 'step_size': 12, 'gamma': 0.8883253980422244}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:36:07,525][0m Trial 10 finished with value: 0.1274538984631791 and parameters: {'observation_period_num': 131, 'train_rates': 0.9759886795727128, 'learning_rate': 0.0001694215950332427, 'batch_size': 21, 'step_size': 1, 'gamma': 0.9896583211532961}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:41:00,399][0m Trial 11 finished with value: 1.8760110229253768 and parameters: {'observation_period_num': 6, 'train_rates': 0.7012651110514793, 'learning_rate': 0.0009885076962735065, 'batch_size': 91, 'step_size': 10, 'gamma': 0.8792809565420726}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:54:45,430][0m Trial 12 finished with value: 0.11487471779465397 and parameters: {'observation_period_num': 5, 'train_rates': 0.7087045682676246, 'learning_rate': 0.0001497481572657511, 'batch_size': 32, 'step_size': 15, 'gamma': 0.9051712823155843}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 04:58:58,449][0m Trial 13 finished with value: 0.482256051115994 and parameters: {'observation_period_num': 90, 'train_rates': 0.6125684877723083, 'learning_rate': 0.00023017316374225694, 'batch_size': 96, 'step_size': 15, 'gamma': 0.8465739996269062}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:03:45,799][0m Trial 14 finished with value: 0.24966667592525482 and parameters: {'observation_period_num': 164, 'train_rates': 0.9805207520793348, 'learning_rate': 1.2532831714601434e-05, 'batch_size': 121, 'step_size': 11, 'gamma': 0.9540183257528806}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:12:09,428][0m Trial 15 finished with value: 0.085789952432183 and parameters: {'observation_period_num': 44, 'train_rates': 0.7453679291632631, 'learning_rate': 8.167939687615857e-05, 'batch_size': 53, 'step_size': 9, 'gamma': 0.9087367755686263}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:19:33,436][0m Trial 16 finished with value: 0.08257902750152755 and parameters: {'observation_period_num': 55, 'train_rates': 0.7569862627206793, 'learning_rate': 8.567916849070545e-05, 'batch_size': 61, 'step_size': 9, 'gamma': 0.8547165395213099}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:27:17,620][0m Trial 17 finished with value: 0.12536388744721474 and parameters: {'observation_period_num': 78, 'train_rates': 0.762497073220806, 'learning_rate': 8.386413895518408e-06, 'batch_size': 58, 'step_size': 3, 'gamma': 0.8453854297986867}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:35:41,780][0m Trial 18 finished with value: 0.28427254234690047 and parameters: {'observation_period_num': 101, 'train_rates': 0.857341555879196, 'learning_rate': 1.2211508272714932e-06, 'batch_size': 57, 'step_size': 4, 'gamma': 0.7890082900299299}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:40:32,523][0m Trial 19 finished with value: 0.14112762120716713 and parameters: {'observation_period_num': 161, 'train_rates': 0.9391436998033519, 'learning_rate': 9.180199659172741e-05, 'batch_size': 109, 'step_size': 9, 'gamma': 0.8516677235534367}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:44:18,305][0m Trial 20 finished with value: 0.05148844156838671 and parameters: {'observation_period_num': 38, 'train_rates': 0.8372994924548961, 'learning_rate': 0.0002383114323500999, 'batch_size': 192, 'step_size': 5, 'gamma': 0.8593245542042289}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:48:05,324][0m Trial 21 finished with value: 0.056441610768894875 and parameters: {'observation_period_num': 34, 'train_rates': 0.8436487470204445, 'learning_rate': 0.00029197762075278377, 'batch_size': 199, 'step_size': 5, 'gamma': 0.8597311716569154}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:51:52,921][0m Trial 22 finished with value: 0.04840414579700215 and parameters: {'observation_period_num': 28, 'train_rates': 0.851812727413962, 'learning_rate': 0.00036692246357125375, 'batch_size': 200, 'step_size': 5, 'gamma': 0.8158317363689036}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 05:55:35,265][0m Trial 23 finished with value: 0.04825082956913203 and parameters: {'observation_period_num': 25, 'train_rates': 0.8407515059676725, 'learning_rate': 0.0004585764959763313, 'batch_size': 206, 'step_size': 2, 'gamma': 0.8187787981087314}. Best is trial 2 with value: 0.04766771362887488.[0m
Early stopping at epoch 70
[32m[I 2025-01-03 05:58:21,748][0m Trial 24 finished with value: 0.11153425974307266 and parameters: {'observation_period_num': 25, 'train_rates': 0.8972215659339091, 'learning_rate': 0.0005088710716199943, 'batch_size': 254, 'step_size': 1, 'gamma': 0.8178426472878961}. Best is trial 2 with value: 0.04766771362887488.[0m
Early stopping at epoch 88
[32m[I 2025-01-03 06:01:56,391][0m Trial 25 finished with value: 2.4138293266296387 and parameters: {'observation_period_num': 63, 'train_rates': 0.9434753004144618, 'learning_rate': 0.0008943143498776033, 'batch_size': 205, 'step_size': 2, 'gamma': 0.7850570983380836}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 06:05:38,955][0m Trial 26 finished with value: 0.05904851473006467 and parameters: {'observation_period_num': 24, 'train_rates': 0.8315393471946031, 'learning_rate': 0.00015089056174205274, 'batch_size': 214, 'step_size': 3, 'gamma': 0.7517382499143989}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 06:09:28,560][0m Trial 27 finished with value: 0.10086532345639819 and parameters: {'observation_period_num': 104, 'train_rates': 0.8656688574420627, 'learning_rate': 0.0004887391812236757, 'batch_size': 181, 'step_size': 5, 'gamma': 0.8311727401521274}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 06:13:54,630][0m Trial 28 finished with value: 0.10755151274514063 and parameters: {'observation_period_num': 73, 'train_rates': 0.9385052701391468, 'learning_rate': 4.990711669056343e-05, 'batch_size': 149, 'step_size': 3, 'gamma': 0.7916448907153858}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 06:17:33,046][0m Trial 29 finished with value: 0.22922103453988898 and parameters: {'observation_period_num': 197, 'train_rates': 0.8940470416592912, 'learning_rate': 0.00012096655577648589, 'batch_size': 215, 'step_size': 2, 'gamma': 0.7980357357065955}. Best is trial 2 with value: 0.04766771362887488.[0m
[32m[I 2025-01-03 06:22:17,632][0m Trial 30 finished with value: 0.03941728158688253 and parameters: {'observation_period_num': 24, 'train_rates': 0.8887593777159435, 'learning_rate': 0.0003466874524755569, 'batch_size': 123, 'step_size': 6, 'gamma': 0.7600427275342316}. Best is trial 30 with value: 0.03941728158688253.[0m
[32m[I 2025-01-03 06:26:54,188][0m Trial 31 finished with value: 0.04498298679235394 and parameters: {'observation_period_num': 24, 'train_rates': 0.8835110648895718, 'learning_rate': 0.0003387099117426336, 'batch_size': 128, 'step_size': 8, 'gamma': 0.7704296280269686}. Best is trial 30 with value: 0.03941728158688253.[0m
[32m[I 2025-01-03 06:31:29,301][0m Trial 32 finished with value: 0.06948856381830833 and parameters: {'observation_period_num': 63, 'train_rates': 0.9168510991132307, 'learning_rate': 0.00022276260057784025, 'batch_size': 131, 'step_size': 8, 'gamma': 0.7720825014522322}. Best is trial 30 with value: 0.03941728158688253.[0m
[32m[I 2025-01-03 06:36:16,726][0m Trial 33 finished with value: 0.03410426368692526 and parameters: {'observation_period_num': 19, 'train_rates': 0.8832850179979435, 'learning_rate': 0.0006463927043303679, 'batch_size': 116, 'step_size': 8, 'gamma': 0.7663334741195418}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 06:41:06,449][0m Trial 34 finished with value: 0.04821785786485224 and parameters: {'observation_period_num': 16, 'train_rates': 0.8822253500319049, 'learning_rate': 0.0006254698977592627, 'batch_size': 115, 'step_size': 8, 'gamma': 0.7675117997377846}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 06:47:52,817][0m Trial 35 finished with value: 0.06938910060044792 and parameters: {'observation_period_num': 51, 'train_rates': 0.9195038532335549, 'learning_rate': 0.0006826602916677643, 'batch_size': 77, 'step_size': 6, 'gamma': 0.7553648193600914}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 06:52:13,931][0m Trial 36 finished with value: 0.03896739333868027 and parameters: {'observation_period_num': 16, 'train_rates': 0.9592445635106224, 'learning_rate': 0.00031668192275880917, 'batch_size': 158, 'step_size': 10, 'gamma': 0.7719689102460949}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 06:56:40,938][0m Trial 37 finished with value: 0.042673930525779724 and parameters: {'observation_period_num': 5, 'train_rates': 0.9620774959952743, 'learning_rate': 0.00031964926327940905, 'batch_size': 151, 'step_size': 10, 'gamma': 0.7741185158306001}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:01:02,127][0m Trial 38 finished with value: 0.047772787511348724 and parameters: {'observation_period_num': 6, 'train_rates': 0.9545340661913245, 'learning_rate': 2.3372798758784405e-05, 'batch_size': 158, 'step_size': 11, 'gamma': 0.781276566411693}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:05:37,228][0m Trial 39 finished with value: 0.04170263186097145 and parameters: {'observation_period_num': 37, 'train_rates': 0.9897084156322289, 'learning_rate': 6.24109882396765e-05, 'batch_size': 146, 'step_size': 13, 'gamma': 0.8000061417304292}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:10:22,115][0m Trial 40 finished with value: 0.06566409766674042 and parameters: {'observation_period_num': 39, 'train_rates': 0.9817052204114185, 'learning_rate': 6.61996394753312e-05, 'batch_size': 142, 'step_size': 13, 'gamma': 0.8050665144667732}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:14:41,674][0m Trial 41 finished with value: 0.03679737076163292 and parameters: {'observation_period_num': 16, 'train_rates': 0.963474091128622, 'learning_rate': 0.00029239629317356523, 'batch_size': 164, 'step_size': 10, 'gamma': 0.7621738660597933}. Best is trial 33 with value: 0.03410426368692526.[0m
Early stopping at epoch 51
[32m[I 2025-01-03 07:16:56,241][0m Trial 42 finished with value: 2.5285894870758057 and parameters: {'observation_period_num': 18, 'train_rates': 0.9660591053845357, 'learning_rate': 0.0007973679975403316, 'batch_size': 170, 'step_size': 14, 'gamma': 0.7547648366340759}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:22:09,474][0m Trial 43 finished with value: 0.06758645939272503 and parameters: {'observation_period_num': 64, 'train_rates': 0.9258469634303316, 'learning_rate': 0.00019559791094862287, 'batch_size': 100, 'step_size': 12, 'gamma': 0.7622458783378836}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:26:34,311][0m Trial 44 finished with value: 0.03501363843679428 and parameters: {'observation_period_num': 15, 'train_rates': 0.9849220805010245, 'learning_rate': 0.0001157866521293907, 'batch_size': 180, 'step_size': 10, 'gamma': 0.7972583751451587}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:30:26,000][0m Trial 45 finished with value: 0.038671490541853745 and parameters: {'observation_period_num': 14, 'train_rates': 0.8140406510439512, 'learning_rate': 0.00012015790304054979, 'batch_size': 180, 'step_size': 10, 'gamma': 0.7800645867256762}. Best is trial 33 with value: 0.03410426368692526.[0m
[32m[I 2025-01-03 07:34:14,998][0m Trial 46 finished with value: 0.03000579482015175 and parameters: {'observation_period_num': 16, 'train_rates': 0.8115718456697919, 'learning_rate': 0.00013578587770864508, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8301783681818624}. Best is trial 46 with value: 0.03000579482015175.[0m
[32m[I 2025-01-03 07:37:56,241][0m Trial 47 finished with value: 0.06980085237247041 and parameters: {'observation_period_num': 55, 'train_rates': 0.7800142117991655, 'learning_rate': 0.00011274954260319432, 'batch_size': 185, 'step_size': 11, 'gamma': 0.8259862322046716}. Best is trial 46 with value: 0.03000579482015175.[0m
[32m[I 2025-01-03 07:42:01,300][0m Trial 48 finished with value: 0.04029748681591544 and parameters: {'observation_period_num': 16, 'train_rates': 0.8140844815342932, 'learning_rate': 3.668983895777117e-05, 'batch_size': 170, 'step_size': 10, 'gamma': 0.8348278722827309}. Best is trial 46 with value: 0.03000579482015175.[0m
[32m[I 2025-01-03 07:45:44,477][0m Trial 49 finished with value: 0.06699315859180577 and parameters: {'observation_period_num': 47, 'train_rates': 0.8174449872089038, 'learning_rate': 0.00013973405359715067, 'batch_size': 185, 'step_size': 9, 'gamma': 0.808443536084637}. Best is trial 46 with value: 0.03000579482015175.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-01-03 07:45:44,485][0m A new study created in memory with name: no-name-888d6a2f-38b3-492d-bd13-908a34b0de66[0m
[32m[I 2025-01-03 07:50:04,723][0m Trial 0 finished with value: 0.09574841485521338 and parameters: {'observation_period_num': 88, 'train_rates': 0.7859282171369525, 'learning_rate': 0.0003658644358502103, 'batch_size': 113, 'step_size': 4, 'gamma': 0.8231540376278087}. Best is trial 0 with value: 0.09574841485521338.[0m
[32m[I 2025-01-03 07:53:56,606][0m Trial 1 finished with value: 0.11450656033842496 and parameters: {'observation_period_num': 20, 'train_rates': 0.612625402024987, 'learning_rate': 2.860489798020333e-05, 'batch_size': 107, 'step_size': 4, 'gamma': 0.9501148514011152}. Best is trial 0 with value: 0.09574841485521338.[0m
[32m[I 2025-01-03 07:58:00,675][0m Trial 2 finished with value: 0.06330835819244385 and parameters: {'observation_period_num': 47, 'train_rates': 0.9739789825382857, 'learning_rate': 0.00013883903355399333, 'batch_size': 236, 'step_size': 3, 'gamma': 0.8735455605558379}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:13:44,605][0m Trial 3 finished with value: 0.14501521587371827 and parameters: {'observation_period_num': 182, 'train_rates': 0.9784903582824455, 'learning_rate': 2.4585077752333468e-05, 'batch_size': 32, 'step_size': 11, 'gamma': 0.7855765979377239}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:17:16,693][0m Trial 4 finished with value: 0.09061906753621311 and parameters: {'observation_period_num': 21, 'train_rates': 0.7437693184360123, 'learning_rate': 2.5160599774295252e-06, 'batch_size': 184, 'step_size': 14, 'gamma': 0.8236712823319553}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:20:59,158][0m Trial 5 finished with value: 0.07686849387328318 and parameters: {'observation_period_num': 42, 'train_rates': 0.7200608598944155, 'learning_rate': 2.0310697192032785e-05, 'batch_size': 158, 'step_size': 12, 'gamma': 0.8000516680617493}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:24:56,877][0m Trial 6 finished with value: 0.20530711114406586 and parameters: {'observation_period_num': 151, 'train_rates': 0.9513751365887774, 'learning_rate': 3.1848612835390607e-06, 'batch_size': 201, 'step_size': 12, 'gamma': 0.973220123036733}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:28:04,480][0m Trial 7 finished with value: 0.294905234620807 and parameters: {'observation_period_num': 146, 'train_rates': 0.6360019516194029, 'learning_rate': 1.537951635661352e-05, 'batch_size': 166, 'step_size': 9, 'gamma': 0.8574678524750445}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:31:02,202][0m Trial 8 finished with value: 1.12567893774154 and parameters: {'observation_period_num': 229, 'train_rates': 0.6205081010193738, 'learning_rate': 5.230146378842854e-05, 'batch_size': 177, 'step_size': 14, 'gamma': 0.7651053041394007}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:35:31,054][0m Trial 9 finished with value: 0.2603400094168527 and parameters: {'observation_period_num': 124, 'train_rates': 0.9430860955985632, 'learning_rate': 4.821071763758825e-06, 'batch_size': 138, 'step_size': 3, 'gamma': 0.8392225253505465}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:39:11,658][0m Trial 10 finished with value: 0.07813795688359634 and parameters: {'observation_period_num': 84, 'train_rates': 0.8717408170047587, 'learning_rate': 0.0006261282978264441, 'batch_size': 256, 'step_size': 1, 'gamma': 0.909218229460266}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:42:35,390][0m Trial 11 finished with value: 0.16297815987203695 and parameters: {'observation_period_num': 59, 'train_rates': 0.7267766235463647, 'learning_rate': 0.00013882391237960283, 'batch_size': 253, 'step_size': 7, 'gamma': 0.8996864602139794}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:46:20,560][0m Trial 12 finished with value: 0.07244504849025492 and parameters: {'observation_period_num': 49, 'train_rates': 0.8487456821268958, 'learning_rate': 0.00017515643400771588, 'batch_size': 219, 'step_size': 7, 'gamma': 0.8849545300139827}. Best is trial 2 with value: 0.06330835819244385.[0m
[32m[I 2025-01-03 08:50:15,153][0m Trial 13 finished with value: 0.035508230672584046 and parameters: {'observation_period_num': 6, 'train_rates': 0.8713561276608753, 'learning_rate': 0.00015435466050243356, 'batch_size': 213, 'step_size': 7, 'gamma': 0.8865336094586127}. Best is trial 13 with value: 0.035508230672584046.[0m
[32m[I 2025-01-03 08:54:11,284][0m Trial 14 finished with value: 0.033601751601373825 and parameters: {'observation_period_num': 8, 'train_rates': 0.8995272668843128, 'learning_rate': 0.00010648651774361755, 'batch_size': 216, 'step_size': 6, 'gamma': 0.9346915041958734}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 09:02:40,616][0m Trial 15 finished with value: 0.04552627063006352 and parameters: {'observation_period_num': 8, 'train_rates': 0.89354280427251, 'learning_rate': 6.943540316544493e-05, 'batch_size': 61, 'step_size': 6, 'gamma': 0.9297786482465759}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 09:06:17,463][0m Trial 16 finished with value: 1.9770574829795144 and parameters: {'observation_period_num': 91, 'train_rates': 0.8270279035969161, 'learning_rate': 0.000748910813355396, 'batch_size': 207, 'step_size': 9, 'gamma': 0.9891720208703649}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 09:10:16,409][0m Trial 17 finished with value: 0.05672665711375479 and parameters: {'observation_period_num': 5, 'train_rates': 0.9026243253052859, 'learning_rate': 7.581866346944465e-06, 'batch_size': 226, 'step_size': 6, 'gamma': 0.9334695828978914}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 09:14:11,540][0m Trial 18 finished with value: 0.3273359342761662 and parameters: {'observation_period_num': 223, 'train_rates': 0.7977261084942014, 'learning_rate': 0.0002736413601382025, 'batch_size': 134, 'step_size': 9, 'gamma': 0.9585013023373534}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 09:20:06,271][0m Trial 19 finished with value: 0.0992109010911594 and parameters: {'observation_period_num': 70, 'train_rates': 0.918186132628388, 'learning_rate': 6.693450850380302e-05, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9150320037000983}. Best is trial 14 with value: 0.033601751601373825.[0m
Early stopping at epoch 68
[32m[I 2025-01-03 09:22:42,500][0m Trial 20 finished with value: 0.7785403111694583 and parameters: {'observation_period_num': 110, 'train_rates': 0.8484597295763603, 'learning_rate': 1.3504217642533306e-06, 'batch_size': 197, 'step_size': 1, 'gamma': 0.8542193407631202}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 09:53:34,296][0m Trial 21 finished with value: 0.035139276822759406 and parameters: {'observation_period_num': 13, 'train_rates': 0.8942696773381906, 'learning_rate': 6.823570259232358e-05, 'batch_size': 16, 'step_size': 7, 'gamma': 0.930938316639152}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 10:20:32,433][0m Trial 22 finished with value: 0.10663739305276138 and parameters: {'observation_period_num': 30, 'train_rates': 0.8799301544000344, 'learning_rate': 9.473422979837748e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9341271579754968}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 10:24:58,279][0m Trial 23 finished with value: 0.057424328100212196 and parameters: {'observation_period_num': 29, 'train_rates': 0.9262812395497971, 'learning_rate': 0.0003056942727161894, 'batch_size': 149, 'step_size': 10, 'gamma': 0.8988110028443909}. Best is trial 14 with value: 0.033601751601373825.[0m
[32m[I 2025-01-03 10:32:32,418][0m Trial 24 finished with value: 0.02834688079568137 and parameters: {'observation_period_num': 9, 'train_rates': 0.8081958742236945, 'learning_rate': 4.369295299451781e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9533968650645747}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 10:41:30,856][0m Trial 25 finished with value: 0.0983091319431217 and parameters: {'observation_period_num': 66, 'train_rates': 0.7968296585887944, 'learning_rate': 4.493543886757693e-05, 'batch_size': 52, 'step_size': 5, 'gamma': 0.9572185551250629}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 10:48:16,102][0m Trial 26 finished with value: 0.06451569665391166 and parameters: {'observation_period_num': 40, 'train_rates': 0.771303527529027, 'learning_rate': 9.634837803656467e-06, 'batch_size': 68, 'step_size': 8, 'gamma': 0.9890175160746013}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 10:59:03,902][0m Trial 27 finished with value: 0.08056728054465194 and parameters: {'observation_period_num': 26, 'train_rates': 0.6628880470988487, 'learning_rate': 3.834962556735026e-05, 'batch_size': 38, 'step_size': 6, 'gamma': 0.9432375460254535}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:03:42,538][0m Trial 28 finished with value: 0.24527001834967557 and parameters: {'observation_period_num': 193, 'train_rates': 0.6917430597367007, 'learning_rate': 1.3749007684712475e-05, 'batch_size': 87, 'step_size': 5, 'gamma': 0.9720619478154475}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:08:10,246][0m Trial 29 finished with value: 0.07740266172587872 and parameters: {'observation_period_num': 84, 'train_rates': 0.8255477679758102, 'learning_rate': 8.469832200276714e-05, 'batch_size': 109, 'step_size': 4, 'gamma': 0.9261150575734174}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:28:22,748][0m Trial 30 finished with value: 0.21986650821282083 and parameters: {'observation_period_num': 100, 'train_rates': 0.7761128810551136, 'learning_rate': 0.00022857039392521953, 'batch_size': 22, 'step_size': 10, 'gamma': 0.9692821675191515}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:39:55,932][0m Trial 31 finished with value: 0.05300305529790758 and parameters: {'observation_period_num': 5, 'train_rates': 0.8536205520010507, 'learning_rate': 0.0005410501473244013, 'batch_size': 43, 'step_size': 7, 'gamma': 0.917473959601195}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:45:54,929][0m Trial 32 finished with value: 0.03640988149966758 and parameters: {'observation_period_num': 17, 'train_rates': 0.8230009866842283, 'learning_rate': 0.00010760345840261206, 'batch_size': 81, 'step_size': 7, 'gamma': 0.8831935869446633}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:50:28,332][0m Trial 33 finished with value: 0.052739939229054886 and parameters: {'observation_period_num': 36, 'train_rates': 0.8755635353886484, 'learning_rate': 3.3656647273468627e-05, 'batch_size': 123, 'step_size': 8, 'gamma': 0.9411167330809738}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:54:15,635][0m Trial 34 finished with value: 0.06602838957111774 and parameters: {'observation_period_num': 53, 'train_rates': 0.9066536212150196, 'learning_rate': 0.000166840849071951, 'batch_size': 221, 'step_size': 3, 'gamma': 0.8974498740260729}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 11:58:18,922][0m Trial 35 finished with value: 0.05894329771399498 and parameters: {'observation_period_num': 19, 'train_rates': 0.9598721418360248, 'learning_rate': 0.0003582363647668215, 'batch_size': 239, 'step_size': 4, 'gamma': 0.9493011900016171}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:02:35,130][0m Trial 36 finished with value: 0.031157251447439194 and parameters: {'observation_period_num': 16, 'train_rates': 0.9874090511882255, 'learning_rate': 5.757454538500544e-05, 'batch_size': 186, 'step_size': 6, 'gamma': 0.8789986874070782}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:06:49,252][0m Trial 37 finished with value: 0.11888428032398224 and parameters: {'observation_period_num': 68, 'train_rates': 0.9888028933737918, 'learning_rate': 2.6482607567483782e-05, 'batch_size': 184, 'step_size': 6, 'gamma': 0.8671203984603438}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:11:08,420][0m Trial 38 finished with value: 0.0661061391234398 and parameters: {'observation_period_num': 41, 'train_rates': 0.9332951637872652, 'learning_rate': 4.680132917898829e-05, 'batch_size': 169, 'step_size': 2, 'gamma': 0.9644179925745096}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:15:16,907][0m Trial 39 finished with value: 0.08135892450809479 and parameters: {'observation_period_num': 19, 'train_rates': 0.9689189496234863, 'learning_rate': 1.9075836088143336e-05, 'batch_size': 190, 'step_size': 5, 'gamma': 0.8296718289689948}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:20:27,002][0m Trial 40 finished with value: 0.16958435852143725 and parameters: {'observation_period_num': 169, 'train_rates': 0.9578669546769921, 'learning_rate': 5.3336425983258974e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.8095147598140965}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:24:18,720][0m Trial 41 finished with value: 0.03660640561236785 and parameters: {'observation_period_num': 16, 'train_rates': 0.8894989902759987, 'learning_rate': 0.0001236219963205984, 'batch_size': 209, 'step_size': 8, 'gamma': 0.882859564194151}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:27:52,370][0m Trial 42 finished with value: 0.12958514550960928 and parameters: {'observation_period_num': 32, 'train_rates': 0.7547764335060889, 'learning_rate': 0.00020834399297285094, 'batch_size': 237, 'step_size': 15, 'gamma': 0.8656548207719398}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:31:17,646][0m Trial 43 finished with value: 0.19092457715942865 and parameters: {'observation_period_num': 251, 'train_rates': 0.8599416695437961, 'learning_rate': 7.69418567086999e-05, 'batch_size': 216, 'step_size': 7, 'gamma': 0.8474496060049452}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:35:04,850][0m Trial 44 finished with value: 0.036755864245250174 and parameters: {'observation_period_num': 12, 'train_rates': 0.81234922998915, 'learning_rate': 6.179890175522489e-05, 'batch_size': 195, 'step_size': 6, 'gamma': 0.9091730709308278}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:39:18,241][0m Trial 45 finished with value: 0.06681300235180719 and parameters: {'observation_period_num': 50, 'train_rates': 0.9151047992706524, 'learning_rate': 2.9722808185887487e-05, 'batch_size': 160, 'step_size': 5, 'gamma': 0.921892284781244}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 12:56:20,722][0m Trial 46 finished with value: 0.043793495420528496 and parameters: {'observation_period_num': 6, 'train_rates': 0.9455831074949004, 'learning_rate': 0.0001252107857703087, 'batch_size': 31, 'step_size': 7, 'gamma': 0.8903739690119999}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 13:00:19,655][0m Trial 47 finished with value: 2.004743215060286 and parameters: {'observation_period_num': 20, 'train_rates': 0.8429293183643676, 'learning_rate': 0.0004873553916006352, 'batch_size': 176, 'step_size': 9, 'gamma': 0.8771376523074262}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 13:04:24,777][0m Trial 48 finished with value: 0.0658707246184349 and parameters: {'observation_period_num': 43, 'train_rates': 0.9352582256923722, 'learning_rate': 0.00016183469750821955, 'batch_size': 247, 'step_size': 4, 'gamma': 0.9075467843574991}. Best is trial 24 with value: 0.02834688079568137.[0m
[32m[I 2025-01-03 13:08:21,369][0m Trial 49 finished with value: 0.07031162739280732 and parameters: {'observation_period_num': 29, 'train_rates': 0.8940073683351264, 'learning_rate': 2.1778632608247545e-05, 'batch_size': 227, 'step_size': 6, 'gamma': 0.9788712357323405}. Best is trial 24 with value: 0.02834688079568137.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-01-03 13:08:21,377][0m A new study created in memory with name: no-name-14580a88-ba2a-4092-8153-f4ee27e28300[0m
[32m[I 2025-01-03 13:11:20,547][0m Trial 0 finished with value: 0.29697520242834 and parameters: {'observation_period_num': 146, 'train_rates': 0.6172099100009351, 'learning_rate': 0.000393252583712916, 'batch_size': 235, 'step_size': 7, 'gamma': 0.7649887319886467}. Best is trial 0 with value: 0.29697520242834.[0m
[32m[I 2025-01-03 13:14:24,135][0m Trial 1 finished with value: 1.696430347289833 and parameters: {'observation_period_num': 17, 'train_rates': 0.621476277270648, 'learning_rate': 0.0008527159403803482, 'batch_size': 241, 'step_size': 14, 'gamma': 0.7881427881430861}. Best is trial 0 with value: 0.29697520242834.[0m
[32m[I 2025-01-03 13:17:41,414][0m Trial 2 finished with value: 0.2883654880953433 and parameters: {'observation_period_num': 246, 'train_rates': 0.7643031153775133, 'learning_rate': 4.351697508936364e-06, 'batch_size': 190, 'step_size': 10, 'gamma': 0.9245835633905819}. Best is trial 2 with value: 0.2883654880953433.[0m
[32m[I 2025-01-03 13:39:40,114][0m Trial 3 finished with value: 0.3139616669820225 and parameters: {'observation_period_num': 235, 'train_rates': 0.7043607510060542, 'learning_rate': 4.3256739685982475e-05, 'batch_size': 18, 'step_size': 9, 'gamma': 0.780442912779954}. Best is trial 2 with value: 0.2883654880953433.[0m
[32m[I 2025-01-03 13:43:12,308][0m Trial 4 finished with value: 0.24008118893708225 and parameters: {'observation_period_num': 207, 'train_rates': 0.7317758375756307, 'learning_rate': 3.124801839170724e-05, 'batch_size': 151, 'step_size': 5, 'gamma': 0.8695919708862918}. Best is trial 4 with value: 0.24008118893708225.[0m
[32m[I 2025-01-03 13:47:27,128][0m Trial 5 finished with value: 0.10647642711451004 and parameters: {'observation_period_num': 15, 'train_rates': 0.8538463837300121, 'learning_rate': 1.0588348875876324e-06, 'batch_size': 145, 'step_size': 8, 'gamma': 0.927424016461357}. Best is trial 5 with value: 0.10647642711451004.[0m
[32m[I 2025-01-03 13:51:59,408][0m Trial 6 finished with value: 0.1527051629096854 and parameters: {'observation_period_num': 173, 'train_rates': 0.9266760116507138, 'learning_rate': 0.0005705972545170742, 'batch_size': 127, 'step_size': 15, 'gamma': 0.893503790696863}. Best is trial 5 with value: 0.10647642711451004.[0m
[32m[I 2025-01-03 13:55:45,321][0m Trial 7 finished with value: 0.19632249846253344 and parameters: {'observation_period_num': 143, 'train_rates': 0.9007379234661533, 'learning_rate': 0.0001304719445578141, 'batch_size': 238, 'step_size': 10, 'gamma': 0.9751336435230861}. Best is trial 5 with value: 0.10647642711451004.[0m
[32m[I 2025-01-03 13:58:53,270][0m Trial 8 finished with value: 0.23915660604834557 and parameters: {'observation_period_num': 240, 'train_rates': 0.6682482769637237, 'learning_rate': 0.00018382870319613766, 'batch_size': 201, 'step_size': 5, 'gamma': 0.893150479042162}. Best is trial 5 with value: 0.10647642711451004.[0m
[32m[I 2025-01-03 14:02:53,807][0m Trial 9 finished with value: 0.4395863828145795 and parameters: {'observation_period_num': 200, 'train_rates': 0.7384121668915578, 'learning_rate': 2.0737041170291703e-06, 'batch_size': 107, 'step_size': 5, 'gamma': 0.7827240631416637}. Best is trial 5 with value: 0.10647642711451004.[0m
[32m[I 2025-01-03 14:10:24,286][0m Trial 10 finished with value: 0.03397731153371697 and parameters: {'observation_period_num': 9, 'train_rates': 0.8526491094401868, 'learning_rate': 8.055276222597548e-06, 'batch_size': 67, 'step_size': 2, 'gamma': 0.9797064508834091}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 14:18:19,958][0m Trial 11 finished with value: 0.03596205920573102 and parameters: {'observation_period_num': 10, 'train_rates': 0.8459980494232441, 'learning_rate': 8.392815861406901e-06, 'batch_size': 63, 'step_size': 1, 'gamma': 0.9775682103651813}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 14:26:39,299][0m Trial 12 finished with value: 0.06944000713586335 and parameters: {'observation_period_num': 64, 'train_rates': 0.825431342352947, 'learning_rate': 9.288054174539556e-06, 'batch_size': 58, 'step_size': 1, 'gamma': 0.9896822293456028}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 14:33:06,908][0m Trial 13 finished with value: 0.10033929236133361 and parameters: {'observation_period_num': 85, 'train_rates': 0.8701442620033717, 'learning_rate': 1.2285006657115731e-05, 'batch_size': 77, 'step_size': 1, 'gamma': 0.9525497783916369}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 14:46:46,972][0m Trial 14 finished with value: 0.07462620093727744 and parameters: {'observation_period_num': 56, 'train_rates': 0.9478294621927882, 'learning_rate': 1.608489989146663e-05, 'batch_size': 38, 'step_size': 3, 'gamma': 0.844239250460801}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 14:52:44,606][0m Trial 15 finished with value: 0.09960360976422104 and parameters: {'observation_period_num': 95, 'train_rates': 0.8002118643453785, 'learning_rate': 5.030533173651754e-06, 'batch_size': 79, 'step_size': 3, 'gamma': 0.9552499190604548}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 14:58:34,582][0m Trial 16 finished with value: 0.053480301052331924 and parameters: {'observation_period_num': 36, 'train_rates': 0.9816453924527324, 'learning_rate': 2.6691512456049504e-05, 'batch_size': 95, 'step_size': 3, 'gamma': 0.8362878231711274}. Best is trial 10 with value: 0.03397731153371697.[0m
[32m[I 2025-01-03 15:09:04,167][0m Trial 17 finished with value: 0.02707786309300463 and parameters: {'observation_period_num': 6, 'train_rates': 0.83287600345271, 'learning_rate': 7.039412496932759e-05, 'batch_size': 47, 'step_size': 2, 'gamma': 0.9402736999755821}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 15:35:30,847][0m Trial 18 finished with value: 0.1422284251657464 and parameters: {'observation_period_num': 98, 'train_rates': 0.7946626225570964, 'learning_rate': 6.696130372540099e-05, 'batch_size': 17, 'step_size': 6, 'gamma': 0.9349699969628916}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 15:46:37,398][0m Trial 19 finished with value: 0.1480413651306357 and parameters: {'observation_period_num': 47, 'train_rates': 0.8975940550916937, 'learning_rate': 8.918438158296257e-05, 'batch_size': 45, 'step_size': 13, 'gamma': 0.9079794542589837}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 15:50:58,891][0m Trial 20 finished with value: 0.12641552975980258 and parameters: {'observation_period_num': 111, 'train_rates': 0.8017535970586254, 'learning_rate': 0.0003125256809934709, 'batch_size': 110, 'step_size': 3, 'gamma': 0.9553993348090379}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 15:59:28,834][0m Trial 21 finished with value: 0.03418704785273246 and parameters: {'observation_period_num': 11, 'train_rates': 0.8481899254339651, 'learning_rate': 5.397824453729471e-06, 'batch_size': 59, 'step_size': 1, 'gamma': 0.9848584729107133}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:12:03,597][0m Trial 22 finished with value: 0.05407350407667524 and parameters: {'observation_period_num': 34, 'train_rates': 0.8833989440500166, 'learning_rate': 3.2014568908256084e-06, 'batch_size': 40, 'step_size': 2, 'gamma': 0.9890965394018038}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:18:19,349][0m Trial 23 finished with value: 0.03461667485907674 and parameters: {'observation_period_num': 6, 'train_rates': 0.8304189428203338, 'learning_rate': 1.8872656498307802e-05, 'batch_size': 79, 'step_size': 4, 'gamma': 0.9633218422070069}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:25:57,718][0m Trial 24 finished with value: 0.1837978694722595 and parameters: {'observation_period_num': 75, 'train_rates': 0.7693035918802629, 'learning_rate': 1.7207516225972365e-06, 'batch_size': 60, 'step_size': 2, 'gamma': 0.9402862833066914}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:31:22,305][0m Trial 25 finished with value: 0.05416961682734089 and parameters: {'observation_period_num': 29, 'train_rates': 0.9268080515071528, 'learning_rate': 7.148877992737769e-06, 'batch_size': 97, 'step_size': 2, 'gamma': 0.9725838162382437}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:45:46,669][0m Trial 26 finished with value: 0.056481880393434075 and parameters: {'observation_period_num': 46, 'train_rates': 0.8222049346973177, 'learning_rate': 3.1582405656214725e-06, 'batch_size': 33, 'step_size': 4, 'gamma': 0.92194024229054}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:49:45,862][0m Trial 27 finished with value: 0.07492786248391818 and parameters: {'observation_period_num': 68, 'train_rates': 0.8709167705076872, 'learning_rate': 5.091965256378401e-05, 'batch_size': 174, 'step_size': 12, 'gamma': 0.9453718777243477}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 16:56:43,663][0m Trial 28 finished with value: 0.07782269671916783 and parameters: {'observation_period_num': 34, 'train_rates': 0.7713179818089435, 'learning_rate': 2.4907580522000273e-05, 'batch_size': 67, 'step_size': 1, 'gamma': 0.8711246949446497}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 17:01:36,889][0m Trial 29 finished with value: 0.15082156658172607 and parameters: {'observation_period_num': 136, 'train_rates': 0.9881440531549783, 'learning_rate': 0.00021630627620454699, 'batch_size': 125, 'step_size': 4, 'gamma': 0.96808767072268}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 17:07:13,304][0m Trial 30 finished with value: 0.12392860980687943 and parameters: {'observation_period_num': 118, 'train_rates': 0.9205314786666337, 'learning_rate': 5.785971102394382e-06, 'batch_size': 90, 'step_size': 7, 'gamma': 0.9109585458735184}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 17:13:41,022][0m Trial 31 finished with value: 0.031392542345386375 and parameters: {'observation_period_num': 8, 'train_rates': 0.8293400088414833, 'learning_rate': 1.5688842375532094e-05, 'batch_size': 78, 'step_size': 4, 'gamma': 0.9635692736210247}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 17:23:27,462][0m Trial 32 finished with value: 0.03826164728189263 and parameters: {'observation_period_num': 21, 'train_rates': 0.8479204076386334, 'learning_rate': 1.2448679350189388e-05, 'batch_size': 50, 'step_size': 2, 'gamma': 0.9851803049637978}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 17:39:57,355][0m Trial 33 finished with value: 0.031226470442806475 and parameters: {'observation_period_num': 6, 'train_rates': 0.8110738182120416, 'learning_rate': 1.8617417371621545e-05, 'batch_size': 29, 'step_size': 6, 'gamma': 0.9612144986723604}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 17:57:16,973][0m Trial 34 finished with value: 0.049488668852236226 and parameters: {'observation_period_num': 23, 'train_rates': 0.7947629647215251, 'learning_rate': 4.1319052077358487e-05, 'batch_size': 27, 'step_size': 6, 'gamma': 0.961925965113267}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 18:13:17,981][0m Trial 35 finished with value: 0.10052344337911219 and parameters: {'observation_period_num': 49, 'train_rates': 0.7449335700575143, 'learning_rate': 1.9542201351326675e-05, 'batch_size': 28, 'step_size': 7, 'gamma': 0.9449903366609237}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 18:42:37,925][0m Trial 36 finished with value: 0.038561542071191605 and parameters: {'observation_period_num': 8, 'train_rates': 0.8198195546743121, 'learning_rate': 8.602176949753787e-05, 'batch_size': 16, 'step_size': 6, 'gamma': 0.9169199345604933}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 18:45:55,883][0m Trial 37 finished with value: 0.07188647287705588 and parameters: {'observation_period_num': 29, 'train_rates': 0.698119310427268, 'learning_rate': 4.0662255068754344e-05, 'batch_size': 221, 'step_size': 8, 'gamma': 0.8133479482142396}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 18:49:18,711][0m Trial 38 finished with value: 0.13417289622160264 and parameters: {'observation_period_num': 18, 'train_rates': 0.6060780425861019, 'learning_rate': 1.1526697056522684e-05, 'batch_size': 160, 'step_size': 5, 'gamma': 0.9368068228150644}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 18:59:00,273][0m Trial 39 finished with value: 0.08107965088115549 and parameters: {'observation_period_num': 53, 'train_rates': 0.7805361391649187, 'learning_rate': 6.123283742723621e-05, 'batch_size': 48, 'step_size': 4, 'gamma': 0.8760354629435347}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:03:30,224][0m Trial 40 finished with value: 2.110378796895345 and parameters: {'observation_period_num': 158, 'train_rates': 0.8659397465818861, 'learning_rate': 0.0008619440594290382, 'batch_size': 116, 'step_size': 9, 'gamma': 0.8975625509939518}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:10:43,650][0m Trial 41 finished with value: 0.03734789656802035 and parameters: {'observation_period_num': 6, 'train_rates': 0.8395324715924237, 'learning_rate': 4.022745671180924e-06, 'batch_size': 69, 'step_size': 2, 'gamma': 0.9773057221748036}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:20:14,954][0m Trial 42 finished with value: 0.06960882902626069 and parameters: {'observation_period_num': 19, 'train_rates': 0.8945517335387264, 'learning_rate': 6.6140228197764425e-06, 'batch_size': 54, 'step_size': 3, 'gamma': 0.7593554083732721}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:23:54,476][0m Trial 43 finished with value: 0.06615366865574879 and parameters: {'observation_period_num': 40, 'train_rates': 0.8118722083397845, 'learning_rate': 3.151551852472147e-05, 'batch_size': 254, 'step_size': 1, 'gamma': 0.9645661469418264}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:30:50,385][0m Trial 44 finished with value: 0.03221896499607738 and parameters: {'observation_period_num': 5, 'train_rates': 0.8572279868273432, 'learning_rate': 9.684032283980543e-06, 'batch_size': 73, 'step_size': 4, 'gamma': 0.9793579169267779}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:36:36,418][0m Trial 45 finished with value: 0.1689752326087547 and parameters: {'observation_period_num': 223, 'train_rates': 0.8617536004822625, 'learning_rate': 1.5096829678802558e-05, 'batch_size': 80, 'step_size': 5, 'gamma': 0.9285658991763097}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:42:13,232][0m Trial 46 finished with value: 0.04442048202922095 and parameters: {'observation_period_num': 22, 'train_rates': 0.8858577175521183, 'learning_rate': 1.0438197261420627e-05, 'batch_size': 89, 'step_size': 4, 'gamma': 0.9534781988173942}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 19:49:17,291][0m Trial 47 finished with value: 0.031429011553525926 and parameters: {'observation_period_num': 5, 'train_rates': 0.9153691670410837, 'learning_rate': 2.182977056154988e-05, 'batch_size': 71, 'step_size': 7, 'gamma': 0.977036826908264}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 20:03:24,366][0m Trial 48 finished with value: 0.04990668031328087 and parameters: {'observation_period_num': 5, 'train_rates': 0.9464631980074932, 'learning_rate': 2.2471472010413582e-05, 'batch_size': 34, 'step_size': 7, 'gamma': 0.9715572383492452}. Best is trial 17 with value: 0.02707786309300463.[0m
[32m[I 2025-01-03 20:07:02,038][0m Trial 49 finished with value: 0.38625157159088674 and parameters: {'observation_period_num': 188, 'train_rates': 0.7472919736650055, 'learning_rate': 0.00010664782425856083, 'batch_size': 131, 'step_size': 9, 'gamma': 0.9482762182930992}. Best is trial 17 with value: 0.02707786309300463.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-01-03 20:07:02,048][0m A new study created in memory with name: no-name-5aa39f4c-a645-4591-b7f9-973fb333187b[0m
[32m[I 2025-01-03 20:11:09,737][0m Trial 0 finished with value: 0.060690325892196514 and parameters: {'observation_period_num': 26, 'train_rates': 0.7283004982699612, 'learning_rate': 0.0004614198096348002, 'batch_size': 102, 'step_size': 12, 'gamma': 0.7995498178879665}. Best is trial 0 with value: 0.060690325892196514.[0m
[32m[I 2025-01-03 20:15:02,448][0m Trial 1 finished with value: 0.1112303234810053 and parameters: {'observation_period_num': 97, 'train_rates': 0.7136874037471217, 'learning_rate': 4.6898057204974756e-05, 'batch_size': 114, 'step_size': 6, 'gamma': 0.7678165016878168}. Best is trial 0 with value: 0.060690325892196514.[0m
[32m[I 2025-01-03 20:19:02,529][0m Trial 2 finished with value: 0.17495819926261902 and parameters: {'observation_period_num': 232, 'train_rates': 0.9880731733669286, 'learning_rate': 2.8054450660584946e-05, 'batch_size': 164, 'step_size': 14, 'gamma': 0.9271472804682824}. Best is trial 0 with value: 0.060690325892196514.[0m
[32m[I 2025-01-03 20:22:38,388][0m Trial 3 finished with value: 0.0442368817374349 and parameters: {'observation_period_num': 10, 'train_rates': 0.6392602294585208, 'learning_rate': 0.00015953403693655022, 'batch_size': 118, 'step_size': 6, 'gamma': 0.9024815578652854}. Best is trial 3 with value: 0.0442368817374349.[0m
[32m[I 2025-01-03 20:26:00,289][0m Trial 4 finished with value: 0.14121532602648382 and parameters: {'observation_period_num': 62, 'train_rates': 0.7046917411175422, 'learning_rate': 0.0001108413818508766, 'batch_size': 171, 'step_size': 14, 'gamma': 0.851438784074502}. Best is trial 3 with value: 0.0442368817374349.[0m
[32m[I 2025-01-03 20:54:07,082][0m Trial 5 finished with value: 0.03172760922461748 and parameters: {'observation_period_num': 40, 'train_rates': 0.9768700499861844, 'learning_rate': 0.00025621853292641795, 'batch_size': 17, 'step_size': 8, 'gamma': 0.9847837133371576}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 20:57:09,728][0m Trial 6 finished with value: 0.2945653401831589 and parameters: {'observation_period_num': 104, 'train_rates': 0.6453357509301486, 'learning_rate': 0.00023292057170645636, 'batch_size': 234, 'step_size': 11, 'gamma': 0.9897238927973383}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 21:01:30,420][0m Trial 7 finished with value: 0.28367656603699826 and parameters: {'observation_period_num': 216, 'train_rates': 0.8664079450546553, 'learning_rate': 3.3452804909078916e-06, 'batch_size': 105, 'step_size': 4, 'gamma': 0.8710517701660796}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 21:04:29,159][0m Trial 8 finished with value: 1.7141544595436242 and parameters: {'observation_period_num': 136, 'train_rates': 0.6494716165985169, 'learning_rate': 0.0008143366314699856, 'batch_size': 215, 'step_size': 13, 'gamma': 0.8804274823786531}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 21:14:15,500][0m Trial 9 finished with value: 0.14371324740132393 and parameters: {'observation_period_num': 108, 'train_rates': 0.9398892339940065, 'learning_rate': 7.987428363947757e-05, 'batch_size': 47, 'step_size': 10, 'gamma': 0.777156828973307}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 21:33:09,670][0m Trial 10 finished with value: 0.13596900457568611 and parameters: {'observation_period_num': 165, 'train_rates': 0.8339711693272435, 'learning_rate': 8.615052514827967e-06, 'batch_size': 22, 'step_size': 1, 'gamma': 0.975596586275443}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 21:40:13,868][0m Trial 11 finished with value: 0.04185605044193368 and parameters: {'observation_period_num': 5, 'train_rates': 0.7824553740513323, 'learning_rate': 0.0002178017121531294, 'batch_size': 61, 'step_size': 8, 'gamma': 0.935571083519513}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 21:47:32,110][0m Trial 12 finished with value: 0.06112837047588459 and parameters: {'observation_period_num': 49, 'train_rates': 0.7867530379251795, 'learning_rate': 1.7116586651195287e-05, 'batch_size': 58, 'step_size': 8, 'gamma': 0.9473986893557462}. Best is trial 5 with value: 0.03172760922461748.[0m
[32m[I 2025-01-03 22:12:39,210][0m Trial 13 finished with value: 0.02473344654859899 and parameters: {'observation_period_num': 6, 'train_rates': 0.8756141539070926, 'learning_rate': 0.00036938987044359006, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9473219628826309}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 22:37:47,821][0m Trial 14 finished with value: 2.0630885111658195 and parameters: {'observation_period_num': 58, 'train_rates': 0.9082348637106005, 'learning_rate': 0.0009172240187824559, 'batch_size': 18, 'step_size': 10, 'gamma': 0.961240573381253}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 22:44:23,908][0m Trial 15 finished with value: 0.11374979591988899 and parameters: {'observation_period_num': 46, 'train_rates': 0.9737101860161825, 'learning_rate': 1.0913589586549852e-06, 'batch_size': 75, 'step_size': 6, 'gamma': 0.9127910249081399}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:01:11,227][0m Trial 16 finished with value: 0.11476585102996023 and parameters: {'observation_period_num': 77, 'train_rates': 0.8947503066250808, 'learning_rate': 0.00037031857506659137, 'batch_size': 27, 'step_size': 3, 'gamma': 0.8288217672753316}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:06:46,580][0m Trial 17 finished with value: 0.21185842277698738 and parameters: {'observation_period_num': 158, 'train_rates': 0.9385698439487149, 'learning_rate': 7.000554594681662e-05, 'batch_size': 83, 'step_size': 9, 'gamma': 0.9584947881649735}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:17:07,535][0m Trial 18 finished with value: 0.08271858505989534 and parameters: {'observation_period_num': 35, 'train_rates': 0.8495157198581149, 'learning_rate': 0.0004301884869790069, 'batch_size': 43, 'step_size': 7, 'gamma': 0.9845262964833995}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:21:17,474][0m Trial 19 finished with value: 0.08836392117174048 and parameters: {'observation_period_num': 86, 'train_rates': 0.9404083342457829, 'learning_rate': 2.788012500588602e-05, 'batch_size': 153, 'step_size': 4, 'gamma': 0.9028444981152566}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:24:59,959][0m Trial 20 finished with value: 0.03725730122467241 and parameters: {'observation_period_num': 24, 'train_rates': 0.8300883326468232, 'learning_rate': 0.00013387169985412247, 'batch_size': 187, 'step_size': 11, 'gamma': 0.9614120499049985}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:28:34,511][0m Trial 21 finished with value: 0.037306051204388194 and parameters: {'observation_period_num': 22, 'train_rates': 0.801894167612456, 'learning_rate': 0.00015330686790997804, 'batch_size': 197, 'step_size': 11, 'gamma': 0.9624677385842904}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:32:28,531][0m Trial 22 finished with value: 0.02977031862010827 and parameters: {'observation_period_num': 6, 'train_rates': 0.886908002005209, 'learning_rate': 0.0003513831457191466, 'batch_size': 189, 'step_size': 9, 'gamma': 0.9319905060455771}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:36:20,648][0m Trial 23 finished with value: 0.03956049491196524 and parameters: {'observation_period_num': 5, 'train_rates': 0.8833504385578227, 'learning_rate': 0.00032561825259984226, 'batch_size': 253, 'step_size': 9, 'gamma': 0.9256533716665226}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:40:33,717][0m Trial 24 finished with value: 0.10651517262862574 and parameters: {'observation_period_num': 68, 'train_rates': 0.9141227579222985, 'learning_rate': 0.000614278111937814, 'batch_size': 140, 'step_size': 7, 'gamma': 0.9423784789610291}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:46:41,078][0m Trial 25 finished with value: 0.08298197462539042 and parameters: {'observation_period_num': 37, 'train_rates': 0.9587883936672491, 'learning_rate': 0.0002665421371055184, 'batch_size': 80, 'step_size': 9, 'gamma': 0.8904259267562141}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-03 23:57:06,015][0m Trial 26 finished with value: 2.08716282082443 and parameters: {'observation_period_num': 195, 'train_rates': 0.8763323044883978, 'learning_rate': 0.0009908329788695046, 'batch_size': 41, 'step_size': 8, 'gamma': 0.9202235579939853}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:01:21,686][0m Trial 27 finished with value: 0.13545859058698018 and parameters: {'observation_period_num': 124, 'train_rates': 0.9256962770502334, 'learning_rate': 5.1157589152193324e-05, 'batch_size': 131, 'step_size': 15, 'gamma': 0.9712231567856346}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:04:48,832][0m Trial 28 finished with value: 1.9154531160990398 and parameters: {'observation_period_num': 20, 'train_rates': 0.7637135637855752, 'learning_rate': 0.000622810489937089, 'batch_size': 210, 'step_size': 10, 'gamma': 0.8479289133770992}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:08:55,672][0m Trial 29 finished with value: 0.3135534293949604 and parameters: {'observation_period_num': 35, 'train_rates': 0.6021828960445772, 'learning_rate': 0.00045700138614708, 'batch_size': 91, 'step_size': 12, 'gamma': 0.9489678616989115}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:12:36,029][0m Trial 30 finished with value: 0.04930875405831182 and parameters: {'observation_period_num': 51, 'train_rates': 0.8099327349707206, 'learning_rate': 8.742483154229892e-05, 'batch_size': 183, 'step_size': 5, 'gamma': 0.8041876442278172}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:16:18,832][0m Trial 31 finished with value: 0.036904030614139806 and parameters: {'observation_period_num': 22, 'train_rates': 0.829926122375005, 'learning_rate': 0.0001456514347700541, 'batch_size': 189, 'step_size': 11, 'gamma': 0.9757720628163316}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:19:46,366][0m Trial 32 finished with value: 0.04375837686570497 and parameters: {'observation_period_num': 15, 'train_rates': 0.7562079596801731, 'learning_rate': 0.00019794272401076242, 'batch_size': 209, 'step_size': 12, 'gamma': 0.978162701208234}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:23:50,894][0m Trial 33 finished with value: 0.0397013397470339 and parameters: {'observation_period_num': 31, 'train_rates': 0.862428782901328, 'learning_rate': 0.00031054553923419686, 'batch_size': 157, 'step_size': 7, 'gamma': 0.9379441730685069}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:27:33,052][0m Trial 34 finished with value: 0.18256487462242119 and parameters: {'observation_period_num': 86, 'train_rates': 0.8297589173186098, 'learning_rate': 0.0005601088620671325, 'batch_size': 229, 'step_size': 9, 'gamma': 0.9888499758388802}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:31:49,128][0m Trial 35 finished with value: 0.03830062225461006 and parameters: {'observation_period_num': 5, 'train_rates': 0.9854606877035896, 'learning_rate': 5.462822553196411e-05, 'batch_size': 186, 'step_size': 10, 'gamma': 0.9685620944432839}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:36:22,471][0m Trial 36 finished with value: 0.12228898885835897 and parameters: {'observation_period_num': 68, 'train_rates': 0.8990933551621283, 'learning_rate': 0.00014097103886161774, 'batch_size': 121, 'step_size': 13, 'gamma': 0.9486644776862114}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:39:56,007][0m Trial 37 finished with value: 0.07163723269250334 and parameters: {'observation_period_num': 41, 'train_rates': 0.7181999605857073, 'learning_rate': 0.00010554171781592615, 'batch_size': 172, 'step_size': 8, 'gamma': 0.9014597501985945}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:43:37,467][0m Trial 38 finished with value: 0.05332994401734104 and parameters: {'observation_period_num': 24, 'train_rates': 0.8543660102999242, 'learning_rate': 0.00019926281820208198, 'batch_size': 250, 'step_size': 11, 'gamma': 0.9332893140698924}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 00:57:47,774][0m Trial 39 finished with value: 0.06804383199511867 and parameters: {'observation_period_num': 17, 'train_rates': 0.8139671985859731, 'learning_rate': 3.69738400622092e-05, 'batch_size': 32, 'step_size': 13, 'gamma': 0.9155926368857044}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:00:57,764][0m Trial 40 finished with value: 0.3929907060085177 and parameters: {'observation_period_num': 243, 'train_rates': 0.687522941664937, 'learning_rate': 0.0006228908901121908, 'batch_size': 227, 'step_size': 7, 'gamma': 0.9777806211252718}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:04:45,193][0m Trial 41 finished with value: 0.04400703459978104 and parameters: {'observation_period_num': 27, 'train_rates': 0.8358533502753919, 'learning_rate': 0.0001100937833043258, 'batch_size': 198, 'step_size': 11, 'gamma': 0.9559996791644682}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:08:21,767][0m Trial 42 finished with value: 0.035663147275023836 and parameters: {'observation_period_num': 18, 'train_rates': 0.7662229953878583, 'learning_rate': 0.00014241621433016787, 'batch_size': 176, 'step_size': 12, 'gamma': 0.9656152794232322}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:12:12,692][0m Trial 43 finished with value: 0.046229964764375714 and parameters: {'observation_period_num': 13, 'train_rates': 0.7620257126639648, 'learning_rate': 0.0002684183208192473, 'batch_size': 170, 'step_size': 14, 'gamma': 0.9705449777965968}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:16:00,787][0m Trial 44 finished with value: 0.11329393669221942 and parameters: {'observation_period_num': 56, 'train_rates': 0.7500888919093549, 'learning_rate': 0.0002035801102372746, 'batch_size': 146, 'step_size': 12, 'gamma': 0.9524596857820429}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:23:52,235][0m Trial 45 finished with value: 0.05596745310180834 and parameters: {'observation_period_num': 46, 'train_rates': 0.7855590265802729, 'learning_rate': 1.0052615012809952e-05, 'batch_size': 59, 'step_size': 10, 'gamma': 0.9874275369530913}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:27:31,697][0m Trial 46 finished with value: 0.0728351382830762 and parameters: {'observation_period_num': 13, 'train_rates': 0.7353536188886356, 'learning_rate': 0.00041272255249512796, 'batch_size': 164, 'step_size': 14, 'gamma': 0.9323710768490844}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:31:42,792][0m Trial 47 finished with value: 0.07623805105686188 and parameters: {'observation_period_num': 30, 'train_rates': 0.9677886088085952, 'learning_rate': 6.805181230321806e-05, 'batch_size': 199, 'step_size': 9, 'gamma': 0.9787586050704203}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 01:35:35,903][0m Trial 48 finished with value: 0.11987067759037018 and parameters: {'observation_period_num': 116, 'train_rates': 0.9529403209964824, 'learning_rate': 0.0001756770895211857, 'batch_size': 218, 'step_size': 8, 'gamma': 0.7538905066831241}. Best is trial 13 with value: 0.02473344654859899.[0m
[32m[I 2025-01-04 02:00:01,929][0m Trial 49 finished with value: 0.10082371136893264 and parameters: {'observation_period_num': 64, 'train_rates': 0.6951068172237252, 'learning_rate': 0.0002726366787550236, 'batch_size': 17, 'step_size': 6, 'gamma': 0.8616343499989475}. Best is trial 13 with value: 0.02473344654859899.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-01-04 02:00:01,938][0m A new study created in memory with name: no-name-d4e6d9fd-8cf7-4ce5-82ca-fbea42acf753[0m
[32m[I 2025-01-04 02:04:55,562][0m Trial 0 finished with value: 0.11528636730579009 and parameters: {'observation_period_num': 84, 'train_rates': 0.9132050905540146, 'learning_rate': 4.6588308158778055e-06, 'batch_size': 107, 'step_size': 11, 'gamma': 0.9392451646895194}. Best is trial 0 with value: 0.11528636730579009.[0m
Early stopping at epoch 64
[32m[I 2025-01-04 02:07:30,048][0m Trial 1 finished with value: 0.47506569248240144 and parameters: {'observation_period_num': 191, 'train_rates': 0.923783178102167, 'learning_rate': 6.396949067968929e-06, 'batch_size': 165, 'step_size': 1, 'gamma': 0.8471169303890753}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:17:33,611][0m Trial 2 finished with value: 0.17764119926620933 and parameters: {'observation_period_num': 114, 'train_rates': 0.8802521733957855, 'learning_rate': 1.1198047200537788e-06, 'batch_size': 48, 'step_size': 7, 'gamma': 0.8896059703114503}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:21:00,772][0m Trial 3 finished with value: 0.16732979775285783 and parameters: {'observation_period_num': 75, 'train_rates': 0.7317559888082242, 'learning_rate': 0.0003967349189637923, 'batch_size': 167, 'step_size': 15, 'gamma': 0.8604504693509266}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:25:03,206][0m Trial 4 finished with value: 0.16800316536663384 and parameters: {'observation_period_num': 137, 'train_rates': 0.7319575816503712, 'learning_rate': 0.00016799316451092465, 'batch_size': 115, 'step_size': 1, 'gamma': 0.9168083895389826}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:31:35,467][0m Trial 5 finished with value: 0.1919975346181451 and parameters: {'observation_period_num': 234, 'train_rates': 0.9700671820425983, 'learning_rate': 1.7694733620459585e-05, 'batch_size': 78, 'step_size': 1, 'gamma': 0.9795048521367578}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:35:25,512][0m Trial 6 finished with value: 0.30671686914118695 and parameters: {'observation_period_num': 227, 'train_rates': 0.8498906523507437, 'learning_rate': 4.469055154629002e-06, 'batch_size': 147, 'step_size': 7, 'gamma': 0.8376374919273357}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:38:46,156][0m Trial 7 finished with value: 0.3155260706462059 and parameters: {'observation_period_num': 192, 'train_rates': 0.6805815982765188, 'learning_rate': 3.185921789361604e-06, 'batch_size': 149, 'step_size': 15, 'gamma': 0.9284253201525644}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:42:21,915][0m Trial 8 finished with value: 1.9618279197636772 and parameters: {'observation_period_num': 85, 'train_rates': 0.8220939394037708, 'learning_rate': 0.0009479246692861728, 'batch_size': 205, 'step_size': 4, 'gamma': 0.7818776887659138}. Best is trial 0 with value: 0.11528636730579009.[0m
[32m[I 2025-01-04 02:46:51,105][0m Trial 9 finished with value: 0.06694832495735877 and parameters: {'observation_period_num': 47, 'train_rates': 0.9455573232495016, 'learning_rate': 0.00014349241690711967, 'batch_size': 151, 'step_size': 11, 'gamma': 0.9403138666243622}. Best is trial 9 with value: 0.06694832495735877.[0m
[32m[I 2025-01-04 02:51:01,545][0m Trial 10 finished with value: 0.054699596017599106 and parameters: {'observation_period_num': 7, 'train_rates': 0.9894813957711113, 'learning_rate': 8.54174414029526e-05, 'batch_size': 250, 'step_size': 10, 'gamma': 0.9885719087168929}. Best is trial 10 with value: 0.054699596017599106.[0m
[32m[I 2025-01-04 02:55:10,313][0m Trial 11 finished with value: 0.03992278128862381 and parameters: {'observation_period_num': 7, 'train_rates': 0.9814685084235889, 'learning_rate': 7.68848035207353e-05, 'batch_size': 253, 'step_size': 11, 'gamma': 0.9709435477144893}. Best is trial 11 with value: 0.03992278128862381.[0m
[32m[I 2025-01-04 02:59:19,345][0m Trial 12 finished with value: 0.0450347363948822 and parameters: {'observation_period_num': 5, 'train_rates': 0.9669909552603042, 'learning_rate': 5.06204383896846e-05, 'batch_size': 252, 'step_size': 11, 'gamma': 0.9858635904972759}. Best is trial 11 with value: 0.03992278128862381.[0m
[32m[I 2025-01-04 03:02:29,016][0m Trial 13 finished with value: 0.08501865385428008 and parameters: {'observation_period_num': 7, 'train_rates': 0.6044237657896369, 'learning_rate': 3.7925158943832705e-05, 'batch_size': 252, 'step_size': 13, 'gamma': 0.9695727560589563}. Best is trial 11 with value: 0.03992278128862381.[0m
[32m[I 2025-01-04 03:06:12,725][0m Trial 14 finished with value: 0.06534020082447035 and parameters: {'observation_period_num': 44, 'train_rates': 0.8804377595909668, 'learning_rate': 3.087751211415321e-05, 'batch_size': 215, 'step_size': 9, 'gamma': 0.8026015047675308}. Best is trial 11 with value: 0.03992278128862381.[0m
[32m[I 2025-01-04 03:09:46,890][0m Trial 15 finished with value: 0.10539763597991522 and parameters: {'observation_period_num': 39, 'train_rates': 0.7820256940345287, 'learning_rate': 5.5999172452732565e-05, 'batch_size': 197, 'step_size': 13, 'gamma': 0.9014341861575772}. Best is trial 11 with value: 0.03992278128862381.[0m
[32m[I 2025-01-04 03:13:40,793][0m Trial 16 finished with value: 0.17167820036411285 and parameters: {'observation_period_num': 127, 'train_rates': 0.939136680965117, 'learning_rate': 1.9413772363953005e-05, 'batch_size': 231, 'step_size': 5, 'gamma': 0.957526837897127}. Best is trial 11 with value: 0.03992278128862381.[0m
[32m[I 2025-01-04 03:17:57,967][0m Trial 17 finished with value: 0.03306964412331581 and parameters: {'observation_period_num': 5, 'train_rates': 0.9867333570276001, 'learning_rate': 0.00024448875517737854, 'batch_size': 187, 'step_size': 13, 'gamma': 0.7508120258537639}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 03:21:50,484][0m Trial 18 finished with value: 0.04874016314852929 and parameters: {'observation_period_num': 32, 'train_rates': 0.8820781608628823, 'learning_rate': 0.00032939819315791003, 'batch_size': 185, 'step_size': 13, 'gamma': 0.7500380399106622}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 03:43:04,115][0m Trial 19 finished with value: 0.08727457193423868 and parameters: {'observation_period_num': 65, 'train_rates': 0.7709270416573241, 'learning_rate': 0.00046864598039484507, 'batch_size': 21, 'step_size': 13, 'gamma': 0.8191859007603002}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 03:46:34,498][0m Trial 20 finished with value: 0.13589621670304836 and parameters: {'observation_period_num': 153, 'train_rates': 0.8331182685582319, 'learning_rate': 0.000138294202047315, 'batch_size': 221, 'step_size': 8, 'gamma': 0.7501637745847015}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 03:50:50,824][0m Trial 21 finished with value: 0.04332887381315231 and parameters: {'observation_period_num': 7, 'train_rates': 0.9875384894188579, 'learning_rate': 7.707021673083267e-05, 'batch_size': 237, 'step_size': 11, 'gamma': 0.9566214418027147}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 03:55:03,741][0m Trial 22 finished with value: 0.057025566697120667 and parameters: {'observation_period_num': 24, 'train_rates': 0.9842165824846965, 'learning_rate': 0.00021075646113222332, 'batch_size': 230, 'step_size': 12, 'gamma': 0.8782396015744134}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 03:59:05,319][0m Trial 23 finished with value: 0.13332332550517975 and parameters: {'observation_period_num': 60, 'train_rates': 0.9142613790055073, 'learning_rate': 9.032519292430265e-05, 'batch_size': 182, 'step_size': 9, 'gamma': 0.9570200288562029}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:03:06,621][0m Trial 24 finished with value: 2.5167031288146973 and parameters: {'observation_period_num': 25, 'train_rates': 0.9573188522951381, 'learning_rate': 0.0008751720002349466, 'batch_size': 234, 'step_size': 14, 'gamma': 0.9127005534083276}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:07:01,654][0m Trial 25 finished with value: 0.12155677269645564 and parameters: {'observation_period_num': 101, 'train_rates': 0.9110314395018859, 'learning_rate': 1.4443287816080209e-05, 'batch_size': 193, 'step_size': 10, 'gamma': 0.7797204326193672}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:11:10,623][0m Trial 26 finished with value: 0.036454346030950546 and parameters: {'observation_period_num': 19, 'train_rates': 0.9887169009913458, 'learning_rate': 7.776740207835382e-05, 'batch_size': 215, 'step_size': 12, 'gamma': 0.9597935837232892}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:15:05,164][0m Trial 27 finished with value: 0.10328187793493271 and parameters: {'observation_period_num': 60, 'train_rates': 0.9452328713139255, 'learning_rate': 0.00030132310659395625, 'batch_size': 211, 'step_size': 14, 'gamma': 0.8900762055770075}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:19:13,805][0m Trial 28 finished with value: 0.11184160244242053 and parameters: {'observation_period_num': 29, 'train_rates': 0.8901785411119182, 'learning_rate': 0.00010878591169256627, 'batch_size': 173, 'step_size': 12, 'gamma': 0.9399199821228351}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:23:52,641][0m Trial 29 finished with value: 0.2763866525703827 and parameters: {'observation_period_num': 96, 'train_rates': 0.9209439868996397, 'learning_rate': 0.00022754579825427367, 'batch_size': 121, 'step_size': 12, 'gamma': 0.9323980342453093}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:27:43,539][0m Trial 30 finished with value: 0.09212640315002087 and parameters: {'observation_period_num': 53, 'train_rates': 0.8594555868876057, 'learning_rate': 0.0005653957506243883, 'batch_size': 206, 'step_size': 10, 'gamma': 0.8645598762923117}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:32:00,914][0m Trial 31 finished with value: 0.0718798115849495 and parameters: {'observation_period_num': 19, 'train_rates': 0.9788987283569898, 'learning_rate': 6.313731821762787e-05, 'batch_size': 238, 'step_size': 11, 'gamma': 0.9553085560945452}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:36:04,701][0m Trial 32 finished with value: 0.07408476620912552 and parameters: {'observation_period_num': 17, 'train_rates': 0.9867786746773254, 'learning_rate': 3.725756868315251e-05, 'batch_size': 223, 'step_size': 14, 'gamma': 0.966537442940426}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:39:59,909][0m Trial 33 finished with value: 0.12188752740621567 and parameters: {'observation_period_num': 37, 'train_rates': 0.9377307396778877, 'learning_rate': 7.47235603422322e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9519632987233532}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:44:02,307][0m Trial 34 finished with value: 0.08203413337469101 and parameters: {'observation_period_num': 15, 'train_rates': 0.9537990964776226, 'learning_rate': 2.6717130277038366e-05, 'batch_size': 239, 'step_size': 9, 'gamma': 0.9712414855234419}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:49:24,626][0m Trial 35 finished with value: 0.09104740122954051 and parameters: {'observation_period_num': 72, 'train_rates': 0.9011078321672221, 'learning_rate': 0.00011572369801301065, 'batch_size': 95, 'step_size': 10, 'gamma': 0.8471762255848526}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:53:19,925][0m Trial 36 finished with value: 0.26515117287635803 and parameters: {'observation_period_num': 157, 'train_rates': 0.9292506531364543, 'learning_rate': 0.00019829844594282194, 'batch_size': 199, 'step_size': 15, 'gamma': 0.911774721509873}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 04:57:38,685][0m Trial 37 finished with value: 0.04770948365330696 and parameters: {'observation_period_num': 5, 'train_rates': 0.9681945762310059, 'learning_rate': 1.112697207106052e-05, 'batch_size': 170, 'step_size': 11, 'gamma': 0.9244233122253098}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:00:38,908][0m Trial 38 finished with value: 0.4971927212404688 and parameters: {'observation_period_num': 206, 'train_rates': 0.6805092740055382, 'learning_rate': 1.0558558950568968e-06, 'batch_size': 221, 'step_size': 8, 'gamma': 0.9477608718430819}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:04:39,618][0m Trial 39 finished with value: 0.18368330597877502 and parameters: {'observation_period_num': 81, 'train_rates': 0.9577576066718573, 'learning_rate': 8.285091110682014e-06, 'batch_size': 242, 'step_size': 7, 'gamma': 0.9768612320958504}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:09:06,518][0m Trial 40 finished with value: 0.060489031483902236 and parameters: {'observation_period_num': 42, 'train_rates': 0.8616926225979165, 'learning_rate': 4.2760598820093023e-05, 'batch_size': 127, 'step_size': 14, 'gamma': 0.8988725801124618}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:13:13,018][0m Trial 41 finished with value: 0.06915678083896637 and parameters: {'observation_period_num': 6, 'train_rates': 0.967364310824289, 'learning_rate': 5.7327913377007784e-05, 'batch_size': 246, 'step_size': 11, 'gamma': 0.9860960849253811}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:17:24,641][0m Trial 42 finished with value: 0.0410173162817955 and parameters: {'observation_period_num': 18, 'train_rates': 0.9891801432139795, 'learning_rate': 2.1194185130636973e-05, 'batch_size': 256, 'step_size': 12, 'gamma': 0.9657365350889251}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:21:18,158][0m Trial 43 finished with value: 0.2467932254076004 and parameters: {'observation_period_num': 250, 'train_rates': 0.9855152442691792, 'learning_rate': 2.310715516882329e-06, 'batch_size': 228, 'step_size': 13, 'gamma': 0.9691331888054913}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:25:49,817][0m Trial 44 finished with value: 0.053163010627031326 and parameters: {'observation_period_num': 28, 'train_rates': 0.9888991732507906, 'learning_rate': 2.4431720813455348e-05, 'batch_size': 156, 'step_size': 12, 'gamma': 0.9431982813627792}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:29:45,395][0m Trial 45 finished with value: 0.04989107698202133 and parameters: {'observation_period_num': 19, 'train_rates': 0.9318763033737211, 'learning_rate': 8.901198200712775e-05, 'batch_size': 256, 'step_size': 5, 'gamma': 0.9263948189888833}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:33:52,096][0m Trial 46 finished with value: 0.11432790011167526 and parameters: {'observation_period_num': 45, 'train_rates': 0.9654023517212544, 'learning_rate': 0.00015753162318968606, 'batch_size': 212, 'step_size': 2, 'gamma': 0.8311077622367835}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:37:55,525][0m Trial 47 finished with value: 0.12618733942508698 and parameters: {'observation_period_num': 53, 'train_rates': 0.9471260178457218, 'learning_rate': 1.749546083789634e-05, 'batch_size': 244, 'step_size': 15, 'gamma': 0.9607115468357085}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:42:17,728][0m Trial 48 finished with value: 0.046613582564612566 and parameters: {'observation_period_num': 15, 'train_rates': 0.8995843382479065, 'learning_rate': 5.023271458590035e-06, 'batch_size': 139, 'step_size': 11, 'gamma': 0.979001642246456}. Best is trial 17 with value: 0.03306964412331581.[0m
[32m[I 2025-01-04 05:45:47,390][0m Trial 49 finished with value: 0.0834249390167555 and parameters: {'observation_period_num': 35, 'train_rates': 0.7321966255669698, 'learning_rate': 0.0002761819487812369, 'batch_size': 184, 'step_size': 9, 'gamma': 0.8017297551451646}. Best is trial 17 with value: 0.03306964412331581.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_AAPL_change_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 21, 'train_rates': 0.8232954011256048, 'learning_rate': 0.00010682109215432726, 'batch_size': 127, 'step_size': 7, 'gamma': 0.9725806076941893}
Epoch 1/300, trend Loss: 0.5017 | 1.0388
Epoch 2/300, trend Loss: 0.4417 | 0.6570
Epoch 3/300, trend Loss: 0.3482 | 0.5721
Epoch 4/300, trend Loss: 0.3120 | 0.5042
Epoch 5/300, trend Loss: 0.2782 | 0.4386
Epoch 6/300, trend Loss: 0.2555 | 0.3925
Epoch 7/300, trend Loss: 0.2357 | 0.3475
Epoch 8/300, trend Loss: 0.2211 | 0.3146
Epoch 9/300, trend Loss: 0.2092 | 0.2874
Epoch 10/300, trend Loss: 0.1991 | 0.2663
Epoch 11/300, trend Loss: 0.1904 | 0.2486
Epoch 12/300, trend Loss: 0.1827 | 0.2342
Epoch 13/300, trend Loss: 0.1759 | 0.2213
Epoch 14/300, trend Loss: 0.1698 | 0.2098
Epoch 15/300, trend Loss: 0.1642 | 0.1994
Epoch 16/300, trend Loss: 0.1591 | 0.1900
Epoch 17/300, trend Loss: 0.1544 | 0.1810
Epoch 18/300, trend Loss: 0.1501 | 0.1729
Epoch 19/300, trend Loss: 0.1462 | 0.1656
Epoch 20/300, trend Loss: 0.1428 | 0.1588
Epoch 21/300, trend Loss: 0.1396 | 0.1523
Epoch 22/300, trend Loss: 0.1367 | 0.1466
Epoch 23/300, trend Loss: 0.1342 | 0.1410
Epoch 24/300, trend Loss: 0.1318 | 0.1358
Epoch 25/300, trend Loss: 0.1296 | 0.1308
Epoch 26/300, trend Loss: 0.1277 | 0.1263
Epoch 27/300, trend Loss: 0.1259 | 0.1219
Epoch 28/300, trend Loss: 0.1242 | 0.1177
Epoch 29/300, trend Loss: 0.1226 | 0.1139
Epoch 30/300, trend Loss: 0.1212 | 0.1102
Epoch 31/300, trend Loss: 0.1198 | 0.1067
Epoch 32/300, trend Loss: 0.1185 | 0.1034
Epoch 33/300, trend Loss: 0.1174 | 0.1004
Epoch 34/300, trend Loss: 0.1163 | 0.0975
Epoch 35/300, trend Loss: 0.1152 | 0.0949
Epoch 36/300, trend Loss: 0.1143 | 0.0925
Epoch 37/300, trend Loss: 0.1134 | 0.0902
Epoch 38/300, trend Loss: 0.1126 | 0.0881
Epoch 39/300, trend Loss: 0.1118 | 0.0861
Epoch 40/300, trend Loss: 0.1111 | 0.0843
Epoch 41/300, trend Loss: 0.1104 | 0.0827
Epoch 42/300, trend Loss: 0.1098 | 0.0811
Epoch 43/300, trend Loss: 0.1092 | 0.0797
Epoch 44/300, trend Loss: 0.1087 | 0.0785
Epoch 45/300, trend Loss: 0.1081 | 0.0772
Epoch 46/300, trend Loss: 0.1076 | 0.0761
Epoch 47/300, trend Loss: 0.1072 | 0.0751
Epoch 48/300, trend Loss: 0.1067 | 0.0741
Epoch 49/300, trend Loss: 0.1063 | 0.0732
Epoch 50/300, trend Loss: 0.1059 | 0.0723
Epoch 51/300, trend Loss: 0.1056 | 0.0715
Epoch 52/300, trend Loss: 0.1052 | 0.0707
Epoch 53/300, trend Loss: 0.1049 | 0.0700
Epoch 54/300, trend Loss: 0.1045 | 0.0693
Epoch 55/300, trend Loss: 0.1042 | 0.0686
Epoch 56/300, trend Loss: 0.1039 | 0.0680
Epoch 57/300, trend Loss: 0.1035 | 0.0674
Epoch 58/300, trend Loss: 0.1032 | 0.0668
Epoch 59/300, trend Loss: 0.1028 | 0.0664
Epoch 60/300, trend Loss: 0.1023 | 0.0659
Epoch 61/300, trend Loss: 0.1018 | 0.0655
Epoch 62/300, trend Loss: 0.1014 | 0.0651
Epoch 63/300, trend Loss: 0.1010 | 0.0648
Epoch 64/300, trend Loss: 0.1007 | 0.0645
Epoch 65/300, trend Loss: 0.1006 | 0.0642
Epoch 66/300, trend Loss: 0.1005 | 0.0639
Epoch 67/300, trend Loss: 0.1003 | 0.0636
Epoch 68/300, trend Loss: 0.1001 | 0.0632
Epoch 69/300, trend Loss: 0.0999 | 0.0629
Epoch 70/300, trend Loss: 0.0995 | 0.0625
Epoch 71/300, trend Loss: 0.0992 | 0.0621
Epoch 72/300, trend Loss: 0.0991 | 0.0617
Epoch 73/300, trend Loss: 0.0992 | 0.0613
Epoch 74/300, trend Loss: 0.0993 | 0.0610
Epoch 75/300, trend Loss: 0.0993 | 0.0608
Epoch 76/300, trend Loss: 0.0992 | 0.0605
Epoch 77/300, trend Loss: 0.0985 | 0.0604
Epoch 78/300, trend Loss: 0.0976 | 0.0604
Epoch 79/300, trend Loss: 0.0971 | 0.0605
Epoch 80/300, trend Loss: 0.0977 | 0.0605
Epoch 81/300, trend Loss: 0.0982 | 0.0601
Epoch 82/300, trend Loss: 0.0978 | 0.0597
Epoch 83/300, trend Loss: 0.0970 | 0.0591
Epoch 84/300, trend Loss: 0.0969 | 0.0586
Epoch 85/300, trend Loss: 0.0974 | 0.0585
Epoch 86/300, trend Loss: 0.0971 | 0.0584
Epoch 87/300, trend Loss: 0.0958 | 0.0584
Epoch 88/300, trend Loss: 0.0951 | 0.0589
Epoch 89/300, trend Loss: 0.0955 | 0.0588
Epoch 90/300, trend Loss: 0.0954 | 0.0579
Epoch 91/300, trend Loss: 0.0947 | 0.0572
Epoch 92/300, trend Loss: 0.0945 | 0.0569
Epoch 93/300, trend Loss: 0.0945 | 0.0568
Epoch 94/300, trend Loss: 0.0940 | 0.0568
Epoch 95/300, trend Loss: 0.0936 | 0.0570
Epoch 96/300, trend Loss: 0.0935 | 0.0571
Epoch 97/300, trend Loss: 0.0935 | 0.0568
Epoch 98/300, trend Loss: 0.0932 | 0.0562
Epoch 99/300, trend Loss: 0.0930 | 0.0559
Epoch 100/300, trend Loss: 0.0929 | 0.0558
Epoch 101/300, trend Loss: 0.0927 | 0.0557
Epoch 102/300, trend Loss: 0.0924 | 0.0557
Epoch 103/300, trend Loss: 0.0922 | 0.0557
Epoch 104/300, trend Loss: 0.0921 | 0.0557
Epoch 105/300, trend Loss: 0.0920 | 0.0554
Epoch 106/300, trend Loss: 0.0918 | 0.0551
Epoch 107/300, trend Loss: 0.0917 | 0.0549
Epoch 108/300, trend Loss: 0.0915 | 0.0548
Epoch 109/300, trend Loss: 0.0913 | 0.0547
Epoch 110/300, trend Loss: 0.0911 | 0.0547
Epoch 111/300, trend Loss: 0.0910 | 0.0547
Epoch 112/300, trend Loss: 0.0909 | 0.0546
Epoch 113/300, trend Loss: 0.0908 | 0.0544
Epoch 114/300, trend Loss: 0.0906 | 0.0542
Epoch 115/300, trend Loss: 0.0905 | 0.0540
Epoch 116/300, trend Loss: 0.0903 | 0.0539
Epoch 117/300, trend Loss: 0.0902 | 0.0538
Epoch 118/300, trend Loss: 0.0900 | 0.0537
Epoch 119/300, trend Loss: 0.0899 | 0.0537
Epoch 120/300, trend Loss: 0.0898 | 0.0536
Epoch 121/300, trend Loss: 0.0897 | 0.0535
Epoch 122/300, trend Loss: 0.0895 | 0.0533
Epoch 123/300, trend Loss: 0.0894 | 0.0532
Epoch 124/300, trend Loss: 0.0893 | 0.0530
Epoch 125/300, trend Loss: 0.0891 | 0.0529
Epoch 126/300, trend Loss: 0.0890 | 0.0528
Epoch 127/300, trend Loss: 0.0889 | 0.0528
Epoch 128/300, trend Loss: 0.0887 | 0.0527
Epoch 129/300, trend Loss: 0.0886 | 0.0526
Epoch 130/300, trend Loss: 0.0885 | 0.0525
Epoch 131/300, trend Loss: 0.0884 | 0.0524
Epoch 132/300, trend Loss: 0.0883 | 0.0522
Epoch 133/300, trend Loss: 0.0882 | 0.0521
Epoch 134/300, trend Loss: 0.0881 | 0.0520
Epoch 135/300, trend Loss: 0.0879 | 0.0519
Epoch 136/300, trend Loss: 0.0878 | 0.0518
Epoch 137/300, trend Loss: 0.0877 | 0.0518
Epoch 138/300, trend Loss: 0.0876 | 0.0517
Epoch 139/300, trend Loss: 0.0875 | 0.0516
Epoch 140/300, trend Loss: 0.0874 | 0.0515
Epoch 141/300, trend Loss: 0.0873 | 0.0514
Epoch 142/300, trend Loss: 0.0872 | 0.0512
Epoch 143/300, trend Loss: 0.0871 | 0.0511
Epoch 144/300, trend Loss: 0.0869 | 0.0510
Epoch 145/300, trend Loss: 0.0868 | 0.0510
Epoch 146/300, trend Loss: 0.0867 | 0.0509
Epoch 147/300, trend Loss: 0.0866 | 0.0509
Epoch 148/300, trend Loss: 0.0865 | 0.0508
Epoch 149/300, trend Loss: 0.0864 | 0.0507
Epoch 150/300, trend Loss: 0.0863 | 0.0506
Epoch 151/300, trend Loss: 0.0862 | 0.0505
Epoch 152/300, trend Loss: 0.0861 | 0.0503
Epoch 153/300, trend Loss: 0.0860 | 0.0502
Epoch 154/300, trend Loss: 0.0859 | 0.0501
Epoch 155/300, trend Loss: 0.0858 | 0.0501
Epoch 156/300, trend Loss: 0.0857 | 0.0500
Epoch 157/300, trend Loss: 0.0856 | 0.0500
Epoch 158/300, trend Loss: 0.0855 | 0.0499
Epoch 159/300, trend Loss: 0.0854 | 0.0499
Epoch 160/300, trend Loss: 0.0854 | 0.0498
Epoch 161/300, trend Loss: 0.0853 | 0.0496
Epoch 162/300, trend Loss: 0.0852 | 0.0495
Epoch 163/300, trend Loss: 0.0851 | 0.0494
Epoch 164/300, trend Loss: 0.0850 | 0.0493
Epoch 165/300, trend Loss: 0.0849 | 0.0493
Epoch 166/300, trend Loss: 0.0848 | 0.0492
Epoch 167/300, trend Loss: 0.0847 | 0.0492
Epoch 168/300, trend Loss: 0.0846 | 0.0491
Epoch 169/300, trend Loss: 0.0846 | 0.0491
Epoch 170/300, trend Loss: 0.0845 | 0.0490
Epoch 171/300, trend Loss: 0.0844 | 0.0488
Epoch 172/300, trend Loss: 0.0844 | 0.0487
Epoch 173/300, trend Loss: 0.0843 | 0.0486
Epoch 174/300, trend Loss: 0.0842 | 0.0485
Epoch 175/300, trend Loss: 0.0841 | 0.0485
Epoch 176/300, trend Loss: 0.0840 | 0.0485
Epoch 177/300, trend Loss: 0.0839 | 0.0484
Epoch 178/300, trend Loss: 0.0838 | 0.0484
Epoch 179/300, trend Loss: 0.0838 | 0.0483
Epoch 180/300, trend Loss: 0.0837 | 0.0482
Epoch 181/300, trend Loss: 0.0837 | 0.0481
Epoch 182/300, trend Loss: 0.0836 | 0.0480
Epoch 183/300, trend Loss: 0.0835 | 0.0479
Epoch 184/300, trend Loss: 0.0834 | 0.0478
Epoch 185/300, trend Loss: 0.0834 | 0.0478
Epoch 186/300, trend Loss: 0.0833 | 0.0478
Epoch 187/300, trend Loss: 0.0832 | 0.0478
Epoch 188/300, trend Loss: 0.0831 | 0.0477
Epoch 189/300, trend Loss: 0.0831 | 0.0477
Epoch 190/300, trend Loss: 0.0830 | 0.0476
Epoch 191/300, trend Loss: 0.0830 | 0.0475
Epoch 192/300, trend Loss: 0.0829 | 0.0474
Epoch 193/300, trend Loss: 0.0828 | 0.0473
Epoch 194/300, trend Loss: 0.0827 | 0.0472
Epoch 195/300, trend Loss: 0.0827 | 0.0472
Epoch 196/300, trend Loss: 0.0826 | 0.0472
Epoch 197/300, trend Loss: 0.0825 | 0.0471
Epoch 198/300, trend Loss: 0.0825 | 0.0471
Epoch 199/300, trend Loss: 0.0824 | 0.0470
Epoch 200/300, trend Loss: 0.0824 | 0.0470
Epoch 201/300, trend Loss: 0.0823 | 0.0469
Epoch 202/300, trend Loss: 0.0823 | 0.0468
Epoch 203/300, trend Loss: 0.0822 | 0.0467
Epoch 204/300, trend Loss: 0.0821 | 0.0467
Epoch 205/300, trend Loss: 0.0821 | 0.0466
Epoch 206/300, trend Loss: 0.0820 | 0.0466
Epoch 207/300, trend Loss: 0.0819 | 0.0466
Epoch 208/300, trend Loss: 0.0819 | 0.0465
Epoch 209/300, trend Loss: 0.0818 | 0.0465
Epoch 210/300, trend Loss: 0.0818 | 0.0464
Epoch 211/300, trend Loss: 0.0817 | 0.0463
Epoch 212/300, trend Loss: 0.0817 | 0.0462
Epoch 213/300, trend Loss: 0.0816 | 0.0462
Epoch 214/300, trend Loss: 0.0816 | 0.0462
Epoch 215/300, trend Loss: 0.0815 | 0.0461
Epoch 216/300, trend Loss: 0.0814 | 0.0461
Epoch 217/300, trend Loss: 0.0814 | 0.0461
Epoch 218/300, trend Loss: 0.0813 | 0.0460
Epoch 219/300, trend Loss: 0.0813 | 0.0459
Epoch 220/300, trend Loss: 0.0812 | 0.0459
Epoch 221/300, trend Loss: 0.0812 | 0.0458
Epoch 222/300, trend Loss: 0.0811 | 0.0458
Epoch 223/300, trend Loss: 0.0811 | 0.0457
Epoch 224/300, trend Loss: 0.0810 | 0.0457
Epoch 225/300, trend Loss: 0.0810 | 0.0457
Epoch 226/300, trend Loss: 0.0809 | 0.0456
Epoch 227/300, trend Loss: 0.0809 | 0.0456
Epoch 228/300, trend Loss: 0.0808 | 0.0455
Epoch 229/300, trend Loss: 0.0808 | 0.0455
Epoch 230/300, trend Loss: 0.0807 | 0.0454
Epoch 231/300, trend Loss: 0.0807 | 0.0454
Epoch 232/300, trend Loss: 0.0806 | 0.0453
Epoch 233/300, trend Loss: 0.0806 | 0.0453
Epoch 234/300, trend Loss: 0.0806 | 0.0453
Epoch 235/300, trend Loss: 0.0805 | 0.0452
Epoch 236/300, trend Loss: 0.0805 | 0.0452
Epoch 237/300, trend Loss: 0.0804 | 0.0451
Epoch 238/300, trend Loss: 0.0804 | 0.0451
Epoch 239/300, trend Loss: 0.0803 | 0.0451
Epoch 240/300, trend Loss: 0.0803 | 0.0450
Epoch 241/300, trend Loss: 0.0802 | 0.0450
Epoch 242/300, trend Loss: 0.0802 | 0.0450
Epoch 243/300, trend Loss: 0.0802 | 0.0449
Epoch 244/300, trend Loss: 0.0801 | 0.0449
Epoch 245/300, trend Loss: 0.0801 | 0.0448
Epoch 246/300, trend Loss: 0.0800 | 0.0448
Epoch 247/300, trend Loss: 0.0800 | 0.0448
Epoch 248/300, trend Loss: 0.0800 | 0.0447
Epoch 249/300, trend Loss: 0.0799 | 0.0447
Epoch 250/300, trend Loss: 0.0799 | 0.0447
Epoch 251/300, trend Loss: 0.0798 | 0.0446
Epoch 252/300, trend Loss: 0.0798 | 0.0446
Epoch 253/300, trend Loss: 0.0798 | 0.0446
Epoch 254/300, trend Loss: 0.0797 | 0.0445
Epoch 255/300, trend Loss: 0.0797 | 0.0445
Epoch 256/300, trend Loss: 0.0796 | 0.0445
Epoch 257/300, trend Loss: 0.0796 | 0.0444
Epoch 258/300, trend Loss: 0.0796 | 0.0444
Epoch 259/300, trend Loss: 0.0795 | 0.0444
Epoch 260/300, trend Loss: 0.0795 | 0.0443
Epoch 261/300, trend Loss: 0.0795 | 0.0443
Epoch 262/300, trend Loss: 0.0794 | 0.0443
Epoch 263/300, trend Loss: 0.0794 | 0.0442
Epoch 264/300, trend Loss: 0.0794 | 0.0442
Epoch 265/300, trend Loss: 0.0793 | 0.0442
Epoch 266/300, trend Loss: 0.0793 | 0.0441
Epoch 267/300, trend Loss: 0.0792 | 0.0441
Epoch 268/300, trend Loss: 0.0792 | 0.0441
Epoch 269/300, trend Loss: 0.0792 | 0.0441
Epoch 270/300, trend Loss: 0.0791 | 0.0440
Epoch 271/300, trend Loss: 0.0791 | 0.0440
Epoch 272/300, trend Loss: 0.0791 | 0.0440
Epoch 273/300, trend Loss: 0.0790 | 0.0439
Epoch 274/300, trend Loss: 0.0790 | 0.0439
Epoch 275/300, trend Loss: 0.0790 | 0.0439
Epoch 276/300, trend Loss: 0.0789 | 0.0439
Epoch 277/300, trend Loss: 0.0789 | 0.0438
Epoch 278/300, trend Loss: 0.0789 | 0.0438
Epoch 279/300, trend Loss: 0.0789 | 0.0438
Epoch 280/300, trend Loss: 0.0788 | 0.0437
Epoch 281/300, trend Loss: 0.0788 | 0.0437
Epoch 282/300, trend Loss: 0.0788 | 0.0437
Epoch 283/300, trend Loss: 0.0787 | 0.0437
Epoch 284/300, trend Loss: 0.0787 | 0.0436
Epoch 285/300, trend Loss: 0.0787 | 0.0436
Epoch 286/300, trend Loss: 0.0786 | 0.0436
Epoch 287/300, trend Loss: 0.0786 | 0.0436
Epoch 288/300, trend Loss: 0.0786 | 0.0436
Epoch 289/300, trend Loss: 0.0786 | 0.0435
Epoch 290/300, trend Loss: 0.0785 | 0.0435
Epoch 291/300, trend Loss: 0.0785 | 0.0435
Epoch 292/300, trend Loss: 0.0785 | 0.0435
Epoch 293/300, trend Loss: 0.0784 | 0.0434
Epoch 294/300, trend Loss: 0.0784 | 0.0434
Epoch 295/300, trend Loss: 0.0784 | 0.0434
Epoch 296/300, trend Loss: 0.0784 | 0.0434
Epoch 297/300, trend Loss: 0.0783 | 0.0433
Epoch 298/300, trend Loss: 0.0783 | 0.0433
Epoch 299/300, trend Loss: 0.0783 | 0.0433
Epoch 300/300, trend Loss: 0.0782 | 0.0433
Training seasonal_0 component with params: {'observation_period_num': 16, 'train_rates': 0.8115718456697919, 'learning_rate': 0.00013578587770864508, 'batch_size': 174, 'step_size': 10, 'gamma': 0.8301783681818624}
Epoch 1/300, seasonal_0 Loss: 1.4398 | 0.3839
Epoch 2/300, seasonal_0 Loss: 0.3720 | 0.2943
Epoch 3/300, seasonal_0 Loss: 0.4656 | 0.2241
Epoch 4/300, seasonal_0 Loss: 0.2815 | 0.2369
Epoch 5/300, seasonal_0 Loss: 0.1584 | 0.1338
Epoch 6/300, seasonal_0 Loss: 0.1650 | 0.1257
Epoch 7/300, seasonal_0 Loss: 0.1455 | 0.1041
Epoch 8/300, seasonal_0 Loss: 0.1353 | 0.1261
Epoch 9/300, seasonal_0 Loss: 0.1291 | 0.1099
Epoch 10/300, seasonal_0 Loss: 0.1254 | 0.1256
Epoch 11/300, seasonal_0 Loss: 0.1727 | 0.1333
Epoch 12/300, seasonal_0 Loss: 0.1607 | 0.1289
Epoch 13/300, seasonal_0 Loss: 0.1361 | 0.1579
Epoch 14/300, seasonal_0 Loss: 0.1312 | 0.0921
Epoch 15/300, seasonal_0 Loss: 0.1196 | 0.0799
Epoch 16/300, seasonal_0 Loss: 0.1127 | 0.0715
Epoch 17/300, seasonal_0 Loss: 0.1034 | 0.0609
Epoch 18/300, seasonal_0 Loss: 0.1086 | 0.0847
Epoch 19/300, seasonal_0 Loss: 0.1231 | 0.0669
Epoch 20/300, seasonal_0 Loss: 0.1132 | 0.0629
Epoch 21/300, seasonal_0 Loss: 0.1022 | 0.0782
Epoch 22/300, seasonal_0 Loss: 0.1053 | 0.0678
Epoch 23/300, seasonal_0 Loss: 0.1091 | 0.0619
Epoch 24/300, seasonal_0 Loss: 0.0957 | 0.0634
Epoch 25/300, seasonal_0 Loss: 0.0964 | 0.0558
Epoch 26/300, seasonal_0 Loss: 0.1006 | 0.0601
Epoch 27/300, seasonal_0 Loss: 0.0966 | 0.0694
Epoch 28/300, seasonal_0 Loss: 0.0942 | 0.0547
Epoch 29/300, seasonal_0 Loss: 0.0999 | 0.1607
Epoch 30/300, seasonal_0 Loss: 0.1098 | 0.0870
Epoch 31/300, seasonal_0 Loss: 0.0996 | 0.0606
Epoch 32/300, seasonal_0 Loss: 0.0965 | 0.0639
Epoch 33/300, seasonal_0 Loss: 0.0945 | 0.0526
Epoch 34/300, seasonal_0 Loss: 0.0868 | 0.0494
Epoch 35/300, seasonal_0 Loss: 0.0861 | 0.0483
Epoch 36/300, seasonal_0 Loss: 0.0857 | 0.0464
Epoch 37/300, seasonal_0 Loss: 0.0829 | 0.0478
Epoch 38/300, seasonal_0 Loss: 0.0830 | 0.0464
Epoch 39/300, seasonal_0 Loss: 0.0817 | 0.0455
Epoch 40/300, seasonal_0 Loss: 0.0815 | 0.0460
Epoch 41/300, seasonal_0 Loss: 0.0811 | 0.0456
Epoch 42/300, seasonal_0 Loss: 0.0801 | 0.0446
Epoch 43/300, seasonal_0 Loss: 0.0808 | 0.0451
Epoch 44/300, seasonal_0 Loss: 0.0805 | 0.0446
Epoch 45/300, seasonal_0 Loss: 0.0815 | 0.0451
Epoch 46/300, seasonal_0 Loss: 0.0793 | 0.0435
Epoch 47/300, seasonal_0 Loss: 0.0792 | 0.0424
Epoch 48/300, seasonal_0 Loss: 0.0784 | 0.0420
Epoch 49/300, seasonal_0 Loss: 0.0777 | 0.0427
Epoch 50/300, seasonal_0 Loss: 0.0777 | 0.0420
Epoch 51/300, seasonal_0 Loss: 0.0773 | 0.0416
Epoch 52/300, seasonal_0 Loss: 0.0770 | 0.0411
Epoch 53/300, seasonal_0 Loss: 0.0770 | 0.0416
Epoch 54/300, seasonal_0 Loss: 0.0770 | 0.0413
Epoch 55/300, seasonal_0 Loss: 0.0771 | 0.0416
Epoch 56/300, seasonal_0 Loss: 0.0770 | 0.0411
Epoch 57/300, seasonal_0 Loss: 0.0767 | 0.0404
Epoch 58/300, seasonal_0 Loss: 0.0777 | 0.0412
Epoch 59/300, seasonal_0 Loss: 0.0777 | 0.0412
Epoch 60/300, seasonal_0 Loss: 0.0789 | 0.0422
Epoch 61/300, seasonal_0 Loss: 0.0774 | 0.0404
Epoch 62/300, seasonal_0 Loss: 0.0791 | 0.0432
Epoch 63/300, seasonal_0 Loss: 0.0788 | 0.0409
Epoch 64/300, seasonal_0 Loss: 0.0803 | 0.0437
Epoch 65/300, seasonal_0 Loss: 0.0794 | 0.0411
Epoch 66/300, seasonal_0 Loss: 0.0801 | 0.0446
Epoch 67/300, seasonal_0 Loss: 0.0789 | 0.0413
Epoch 68/300, seasonal_0 Loss: 0.0804 | 0.0443
Epoch 69/300, seasonal_0 Loss: 0.0802 | 0.0413
Epoch 70/300, seasonal_0 Loss: 0.0819 | 0.0472
Epoch 71/300, seasonal_0 Loss: 0.0800 | 0.0428
Epoch 72/300, seasonal_0 Loss: 0.0803 | 0.0490
Epoch 73/300, seasonal_0 Loss: 0.0792 | 0.0426
Epoch 74/300, seasonal_0 Loss: 0.0794 | 0.0490
Epoch 75/300, seasonal_0 Loss: 0.0784 | 0.0422
Epoch 76/300, seasonal_0 Loss: 0.0773 | 0.0475
Epoch 77/300, seasonal_0 Loss: 0.0758 | 0.0413
Epoch 78/300, seasonal_0 Loss: 0.0750 | 0.0426
Epoch 79/300, seasonal_0 Loss: 0.0743 | 0.0398
Epoch 80/300, seasonal_0 Loss: 0.0739 | 0.0405
Epoch 81/300, seasonal_0 Loss: 0.0735 | 0.0390
Epoch 82/300, seasonal_0 Loss: 0.0731 | 0.0392
Epoch 83/300, seasonal_0 Loss: 0.0730 | 0.0385
Epoch 84/300, seasonal_0 Loss: 0.0728 | 0.0386
Epoch 85/300, seasonal_0 Loss: 0.0728 | 0.0384
Epoch 86/300, seasonal_0 Loss: 0.0727 | 0.0384
Epoch 87/300, seasonal_0 Loss: 0.0726 | 0.0383
Epoch 88/300, seasonal_0 Loss: 0.0726 | 0.0382
Epoch 89/300, seasonal_0 Loss: 0.0725 | 0.0382
Epoch 90/300, seasonal_0 Loss: 0.0724 | 0.0381
Epoch 91/300, seasonal_0 Loss: 0.0724 | 0.0381
Epoch 92/300, seasonal_0 Loss: 0.0723 | 0.0380
Epoch 93/300, seasonal_0 Loss: 0.0723 | 0.0380
Epoch 94/300, seasonal_0 Loss: 0.0722 | 0.0379
Epoch 95/300, seasonal_0 Loss: 0.0722 | 0.0379
Epoch 96/300, seasonal_0 Loss: 0.0721 | 0.0379
Epoch 97/300, seasonal_0 Loss: 0.0721 | 0.0378
Epoch 98/300, seasonal_0 Loss: 0.0721 | 0.0378
Epoch 99/300, seasonal_0 Loss: 0.0720 | 0.0378
Epoch 100/300, seasonal_0 Loss: 0.0720 | 0.0377
Epoch 101/300, seasonal_0 Loss: 0.0719 | 0.0377
Epoch 102/300, seasonal_0 Loss: 0.0719 | 0.0377
Epoch 103/300, seasonal_0 Loss: 0.0719 | 0.0376
Epoch 104/300, seasonal_0 Loss: 0.0718 | 0.0376
Epoch 105/300, seasonal_0 Loss: 0.0718 | 0.0376
Epoch 106/300, seasonal_0 Loss: 0.0718 | 0.0376
Epoch 107/300, seasonal_0 Loss: 0.0717 | 0.0375
Epoch 108/300, seasonal_0 Loss: 0.0717 | 0.0375
Epoch 109/300, seasonal_0 Loss: 0.0717 | 0.0375
Epoch 110/300, seasonal_0 Loss: 0.0716 | 0.0375
Epoch 111/300, seasonal_0 Loss: 0.0716 | 0.0375
Epoch 112/300, seasonal_0 Loss: 0.0716 | 0.0374
Epoch 113/300, seasonal_0 Loss: 0.0715 | 0.0374
Epoch 114/300, seasonal_0 Loss: 0.0715 | 0.0374
Epoch 115/300, seasonal_0 Loss: 0.0715 | 0.0374
Epoch 116/300, seasonal_0 Loss: 0.0715 | 0.0374
Epoch 117/300, seasonal_0 Loss: 0.0714 | 0.0373
Epoch 118/300, seasonal_0 Loss: 0.0714 | 0.0373
Epoch 119/300, seasonal_0 Loss: 0.0714 | 0.0373
Epoch 120/300, seasonal_0 Loss: 0.0714 | 0.0373
Epoch 121/300, seasonal_0 Loss: 0.0714 | 0.0373
Epoch 122/300, seasonal_0 Loss: 0.0713 | 0.0373
Epoch 123/300, seasonal_0 Loss: 0.0713 | 0.0372
Epoch 124/300, seasonal_0 Loss: 0.0713 | 0.0372
Epoch 125/300, seasonal_0 Loss: 0.0713 | 0.0372
Epoch 126/300, seasonal_0 Loss: 0.0713 | 0.0372
Epoch 127/300, seasonal_0 Loss: 0.0712 | 0.0372
Epoch 128/300, seasonal_0 Loss: 0.0712 | 0.0372
Epoch 129/300, seasonal_0 Loss: 0.0712 | 0.0372
Epoch 130/300, seasonal_0 Loss: 0.0712 | 0.0372
Epoch 131/300, seasonal_0 Loss: 0.0712 | 0.0372
Epoch 132/300, seasonal_0 Loss: 0.0712 | 0.0371
Epoch 133/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 134/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 135/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 136/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 137/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 138/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 139/300, seasonal_0 Loss: 0.0711 | 0.0371
Epoch 140/300, seasonal_0 Loss: 0.0710 | 0.0371
Epoch 141/300, seasonal_0 Loss: 0.0710 | 0.0371
Epoch 142/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 143/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 144/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 145/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 146/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 147/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 148/300, seasonal_0 Loss: 0.0710 | 0.0370
Epoch 149/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 150/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 151/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 152/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 153/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 154/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 155/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 156/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 157/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 158/300, seasonal_0 Loss: 0.0709 | 0.0370
Epoch 159/300, seasonal_0 Loss: 0.0709 | 0.0369
Epoch 160/300, seasonal_0 Loss: 0.0709 | 0.0369
Epoch 161/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 162/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 163/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 164/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 165/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 166/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 167/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 168/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 169/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 170/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 171/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 172/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 173/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 174/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 175/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 176/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 177/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 178/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 179/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 180/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 181/300, seasonal_0 Loss: 0.0708 | 0.0369
Epoch 182/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 183/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 184/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 185/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 186/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 187/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 188/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 189/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 190/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 191/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 192/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 193/300, seasonal_0 Loss: 0.0707 | 0.0369
Epoch 194/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 195/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 196/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 197/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 198/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 199/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 200/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 201/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 202/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 203/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 204/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 205/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 206/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 207/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 208/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 209/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 210/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 211/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 212/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 213/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 214/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 215/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 216/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 217/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 218/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 219/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 220/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 221/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 222/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 223/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 224/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 225/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 226/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 227/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 228/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 229/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 230/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 231/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 232/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 233/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 234/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 235/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 236/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 237/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 238/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 239/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 240/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 241/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 242/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 243/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 244/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 245/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 246/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 247/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 248/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 249/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 250/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 251/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 252/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 253/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 254/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 255/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 256/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 257/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 258/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 259/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 260/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 261/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 262/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 263/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 264/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 265/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 266/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 267/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 268/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 269/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 270/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 271/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 272/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 273/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 274/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 275/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 276/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 277/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 278/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 279/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 280/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 281/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 282/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 283/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 284/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 285/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 286/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 287/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 288/300, seasonal_0 Loss: 0.0707 | 0.0368
Epoch 289/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 290/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 291/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 292/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 293/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 294/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 295/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 296/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 297/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 298/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 299/300, seasonal_0 Loss: 0.0706 | 0.0368
Epoch 300/300, seasonal_0 Loss: 0.0706 | 0.0368
Training seasonal_1 component with params: {'observation_period_num': 9, 'train_rates': 0.8081958742236945, 'learning_rate': 4.369295299451781e-05, 'batch_size': 63, 'step_size': 7, 'gamma': 0.9533968650645747}
Epoch 1/300, seasonal_1 Loss: 0.2954 | 0.1196
Epoch 2/300, seasonal_1 Loss: 0.1277 | 0.0885
Epoch 3/300, seasonal_1 Loss: 0.1395 | 0.0951
Epoch 4/300, seasonal_1 Loss: 0.1187 | 0.0706
Epoch 5/300, seasonal_1 Loss: 0.1086 | 0.0607
Epoch 6/300, seasonal_1 Loss: 0.1061 | 0.0557
Epoch 7/300, seasonal_1 Loss: 0.1010 | 0.0516
Epoch 8/300, seasonal_1 Loss: 0.0973 | 0.0509
Epoch 9/300, seasonal_1 Loss: 0.0942 | 0.0519
Epoch 10/300, seasonal_1 Loss: 0.0900 | 0.0561
Epoch 11/300, seasonal_1 Loss: 0.0892 | 0.0720
Epoch 12/300, seasonal_1 Loss: 0.0896 | 0.0540
Epoch 13/300, seasonal_1 Loss: 0.0861 | 0.0439
Epoch 14/300, seasonal_1 Loss: 0.0831 | 0.0422
Epoch 15/300, seasonal_1 Loss: 0.0835 | 0.0408
Epoch 16/300, seasonal_1 Loss: 0.0873 | 0.0447
Epoch 17/300, seasonal_1 Loss: 0.1010 | 0.0493
Epoch 18/300, seasonal_1 Loss: 0.0892 | 0.0444
Epoch 19/300, seasonal_1 Loss: 0.0876 | 0.0572
Epoch 20/300, seasonal_1 Loss: 0.0856 | 0.0416
Epoch 21/300, seasonal_1 Loss: 0.0800 | 0.0390
Epoch 22/300, seasonal_1 Loss: 0.0756 | 0.0389
Epoch 23/300, seasonal_1 Loss: 0.0751 | 0.0387
Epoch 24/300, seasonal_1 Loss: 0.0761 | 0.0379
Epoch 25/300, seasonal_1 Loss: 0.0773 | 0.0377
Epoch 26/300, seasonal_1 Loss: 0.0753 | 0.0361
Epoch 27/300, seasonal_1 Loss: 0.0722 | 0.0366
Epoch 28/300, seasonal_1 Loss: 0.0728 | 0.0384
Epoch 29/300, seasonal_1 Loss: 0.0759 | 0.0417
Epoch 30/300, seasonal_1 Loss: 0.0791 | 0.0411
Epoch 31/300, seasonal_1 Loss: 0.0815 | 0.0424
Epoch 32/300, seasonal_1 Loss: 0.0832 | 0.0378
Epoch 33/300, seasonal_1 Loss: 0.0821 | 0.0472
Epoch 34/300, seasonal_1 Loss: 0.0813 | 0.0368
Epoch 35/300, seasonal_1 Loss: 0.0772 | 0.0375
Epoch 36/300, seasonal_1 Loss: 0.0841 | 0.0414
Epoch 37/300, seasonal_1 Loss: 0.0795 | 0.0413
Epoch 38/300, seasonal_1 Loss: 0.0800 | 0.0396
Epoch 39/300, seasonal_1 Loss: 0.0808 | 0.0415
Epoch 40/300, seasonal_1 Loss: 0.0786 | 0.0419
Epoch 41/300, seasonal_1 Loss: 0.0737 | 0.0377
Epoch 42/300, seasonal_1 Loss: 0.0746 | 0.0360
Epoch 43/300, seasonal_1 Loss: 0.0761 | 0.0346
Epoch 44/300, seasonal_1 Loss: 0.0727 | 0.0352
Epoch 45/300, seasonal_1 Loss: 0.0734 | 0.0347
Epoch 46/300, seasonal_1 Loss: 0.0723 | 0.0369
Epoch 47/300, seasonal_1 Loss: 0.0702 | 0.0330
Epoch 48/300, seasonal_1 Loss: 0.0668 | 0.0323
Epoch 49/300, seasonal_1 Loss: 0.0665 | 0.0321
Epoch 50/300, seasonal_1 Loss: 0.0660 | 0.0326
Epoch 51/300, seasonal_1 Loss: 0.0667 | 0.0427
Epoch 52/300, seasonal_1 Loss: 0.0699 | 0.0347
Epoch 53/300, seasonal_1 Loss: 0.0705 | 0.0349
Epoch 54/300, seasonal_1 Loss: 0.0690 | 0.0321
Epoch 55/300, seasonal_1 Loss: 0.0656 | 0.0312
Epoch 56/300, seasonal_1 Loss: 0.0637 | 0.0302
Epoch 57/300, seasonal_1 Loss: 0.0630 | 0.0319
Epoch 58/300, seasonal_1 Loss: 0.0630 | 0.0324
Epoch 59/300, seasonal_1 Loss: 0.0656 | 0.0332
Epoch 60/300, seasonal_1 Loss: 0.0647 | 0.0316
Epoch 61/300, seasonal_1 Loss: 0.0657 | 0.0321
Epoch 62/300, seasonal_1 Loss: 0.0645 | 0.0303
Epoch 63/300, seasonal_1 Loss: 0.0630 | 0.0287
Epoch 64/300, seasonal_1 Loss: 0.0624 | 0.0324
Epoch 65/300, seasonal_1 Loss: 0.0627 | 0.0376
Epoch 66/300, seasonal_1 Loss: 0.0643 | 0.0312
Epoch 67/300, seasonal_1 Loss: 0.0635 | 0.0305
Epoch 68/300, seasonal_1 Loss: 0.0621 | 0.0279
Epoch 69/300, seasonal_1 Loss: 0.0612 | 0.0290
Epoch 70/300, seasonal_1 Loss: 0.0610 | 0.0284
Epoch 71/300, seasonal_1 Loss: 0.0602 | 0.0287
Epoch 72/300, seasonal_1 Loss: 0.0604 | 0.0291
Epoch 73/300, seasonal_1 Loss: 0.0595 | 0.0280
Epoch 74/300, seasonal_1 Loss: 0.0596 | 0.0285
Epoch 75/300, seasonal_1 Loss: 0.0608 | 0.0291
Epoch 76/300, seasonal_1 Loss: 0.0624 | 0.0295
Epoch 77/300, seasonal_1 Loss: 0.0628 | 0.0325
Epoch 78/300, seasonal_1 Loss: 0.0631 | 0.0313
Epoch 79/300, seasonal_1 Loss: 0.0598 | 0.0320
Epoch 80/300, seasonal_1 Loss: 0.0606 | 0.0276
Epoch 81/300, seasonal_1 Loss: 0.0595 | 0.0287
Epoch 82/300, seasonal_1 Loss: 0.0597 | 0.0303
Epoch 83/300, seasonal_1 Loss: 0.0591 | 0.0304
Epoch 84/300, seasonal_1 Loss: 0.0594 | 0.0326
Epoch 85/300, seasonal_1 Loss: 0.0610 | 0.0347
Epoch 86/300, seasonal_1 Loss: 0.0639 | 0.0304
Epoch 87/300, seasonal_1 Loss: 0.0648 | 0.0274
Epoch 88/300, seasonal_1 Loss: 0.0708 | 0.0820
Epoch 89/300, seasonal_1 Loss: 0.0806 | 0.0328
Epoch 90/300, seasonal_1 Loss: 0.0681 | 0.0303
Epoch 91/300, seasonal_1 Loss: 0.0622 | 0.0258
Epoch 92/300, seasonal_1 Loss: 0.0587 | 0.0260
Epoch 93/300, seasonal_1 Loss: 0.0578 | 0.0276
Epoch 94/300, seasonal_1 Loss: 0.0591 | 0.0277
Epoch 95/300, seasonal_1 Loss: 0.0577 | 0.0273
Epoch 96/300, seasonal_1 Loss: 0.0567 | 0.0267
Epoch 97/300, seasonal_1 Loss: 0.0577 | 0.0264
Epoch 98/300, seasonal_1 Loss: 0.0581 | 0.0264
Epoch 99/300, seasonal_1 Loss: 0.0566 | 0.0271
Epoch 100/300, seasonal_1 Loss: 0.0564 | 0.0292
Epoch 101/300, seasonal_1 Loss: 0.0574 | 0.0279
Epoch 102/300, seasonal_1 Loss: 0.0569 | 0.0276
Epoch 103/300, seasonal_1 Loss: 0.0559 | 0.0273
Epoch 104/300, seasonal_1 Loss: 0.0554 | 0.0275
Epoch 105/300, seasonal_1 Loss: 0.0571 | 0.0287
Epoch 106/300, seasonal_1 Loss: 0.0584 | 0.0263
Epoch 107/300, seasonal_1 Loss: 0.0571 | 0.0304
Epoch 108/300, seasonal_1 Loss: 0.0587 | 0.0313
Epoch 109/300, seasonal_1 Loss: 0.0584 | 0.0278
Epoch 110/300, seasonal_1 Loss: 0.0562 | 0.0276
Epoch 111/300, seasonal_1 Loss: 0.0552 | 0.0261
Epoch 112/300, seasonal_1 Loss: 0.0548 | 0.0265
Epoch 113/300, seasonal_1 Loss: 0.0544 | 0.0257
Epoch 114/300, seasonal_1 Loss: 0.0540 | 0.0275
Epoch 115/300, seasonal_1 Loss: 0.0546 | 0.0283
Epoch 116/300, seasonal_1 Loss: 0.0547 | 0.0272
Epoch 117/300, seasonal_1 Loss: 0.0541 | 0.0261
Epoch 118/300, seasonal_1 Loss: 0.0538 | 0.0266
Epoch 119/300, seasonal_1 Loss: 0.0543 | 0.0266
Epoch 120/300, seasonal_1 Loss: 0.0538 | 0.0268
Epoch 121/300, seasonal_1 Loss: 0.0529 | 0.0270
Epoch 122/300, seasonal_1 Loss: 0.0523 | 0.0266
Epoch 123/300, seasonal_1 Loss: 0.0520 | 0.0255
Epoch 124/300, seasonal_1 Loss: 0.0518 | 0.0255
Epoch 125/300, seasonal_1 Loss: 0.0517 | 0.0253
Epoch 126/300, seasonal_1 Loss: 0.0516 | 0.0261
Epoch 127/300, seasonal_1 Loss: 0.0514 | 0.0261
Epoch 128/300, seasonal_1 Loss: 0.0517 | 0.0266
Epoch 129/300, seasonal_1 Loss: 0.0522 | 0.0263
Epoch 130/300, seasonal_1 Loss: 0.0524 | 0.0266
Epoch 131/300, seasonal_1 Loss: 0.0520 | 0.0261
Epoch 132/300, seasonal_1 Loss: 0.0518 | 0.0263
Epoch 133/300, seasonal_1 Loss: 0.0520 | 0.0264
Epoch 134/300, seasonal_1 Loss: 0.0514 | 0.0262
Epoch 135/300, seasonal_1 Loss: 0.0513 | 0.0260
Epoch 136/300, seasonal_1 Loss: 0.0518 | 0.0265
Epoch 137/300, seasonal_1 Loss: 0.0514 | 0.0260
Epoch 138/300, seasonal_1 Loss: 0.0510 | 0.0262
Epoch 139/300, seasonal_1 Loss: 0.0511 | 0.0261
Epoch 140/300, seasonal_1 Loss: 0.0515 | 0.0268
Epoch 141/300, seasonal_1 Loss: 0.0519 | 0.0269
Epoch 142/300, seasonal_1 Loss: 0.0523 | 0.0280
Epoch 143/300, seasonal_1 Loss: 0.0530 | 0.0268
Epoch 144/300, seasonal_1 Loss: 0.0524 | 0.0265
Epoch 145/300, seasonal_1 Loss: 0.0513 | 0.0260
Epoch 146/300, seasonal_1 Loss: 0.0514 | 0.0265
Epoch 147/300, seasonal_1 Loss: 0.0520 | 0.0255
Epoch 148/300, seasonal_1 Loss: 0.0511 | 0.0283
Epoch 149/300, seasonal_1 Loss: 0.0512 | 0.0265
Epoch 150/300, seasonal_1 Loss: 0.0507 | 0.0260
Epoch 151/300, seasonal_1 Loss: 0.0499 | 0.0251
Epoch 152/300, seasonal_1 Loss: 0.0498 | 0.0260
Epoch 153/300, seasonal_1 Loss: 0.0494 | 0.0258
Epoch 154/300, seasonal_1 Loss: 0.0494 | 0.0264
Epoch 155/300, seasonal_1 Loss: 0.0495 | 0.0259
Epoch 156/300, seasonal_1 Loss: 0.0495 | 0.0260
Epoch 157/300, seasonal_1 Loss: 0.0492 | 0.0257
Epoch 158/300, seasonal_1 Loss: 0.0490 | 0.0260
Epoch 159/300, seasonal_1 Loss: 0.0488 | 0.0259
Epoch 160/300, seasonal_1 Loss: 0.0488 | 0.0261
Epoch 161/300, seasonal_1 Loss: 0.0488 | 0.0261
Epoch 162/300, seasonal_1 Loss: 0.0487 | 0.0267
Epoch 163/300, seasonal_1 Loss: 0.0491 | 0.0268
Epoch 164/300, seasonal_1 Loss: 0.0495 | 0.0266
Epoch 165/300, seasonal_1 Loss: 0.0493 | 0.0262
Epoch 166/300, seasonal_1 Loss: 0.0490 | 0.0260
Epoch 167/300, seasonal_1 Loss: 0.0493 | 0.0258
Epoch 168/300, seasonal_1 Loss: 0.0493 | 0.0261
Epoch 169/300, seasonal_1 Loss: 0.0488 | 0.0272
Epoch 170/300, seasonal_1 Loss: 0.0488 | 0.0269
Epoch 171/300, seasonal_1 Loss: 0.0488 | 0.0260
Epoch 172/300, seasonal_1 Loss: 0.0483 | 0.0258
Epoch 173/300, seasonal_1 Loss: 0.0481 | 0.0261
Epoch 174/300, seasonal_1 Loss: 0.0483 | 0.0262
Epoch 175/300, seasonal_1 Loss: 0.0483 | 0.0268
Epoch 176/300, seasonal_1 Loss: 0.0481 | 0.0267
Epoch 177/300, seasonal_1 Loss: 0.0479 | 0.0269
Epoch 178/300, seasonal_1 Loss: 0.0481 | 0.0271
Epoch 179/300, seasonal_1 Loss: 0.0483 | 0.0272
Epoch 180/300, seasonal_1 Loss: 0.0482 | 0.0263
Epoch 181/300, seasonal_1 Loss: 0.0479 | 0.0265
Epoch 182/300, seasonal_1 Loss: 0.0478 | 0.0262
Epoch 183/300, seasonal_1 Loss: 0.0476 | 0.0267
Epoch 184/300, seasonal_1 Loss: 0.0474 | 0.0271
Epoch 185/300, seasonal_1 Loss: 0.0474 | 0.0273
Epoch 186/300, seasonal_1 Loss: 0.0474 | 0.0271
Epoch 187/300, seasonal_1 Loss: 0.0474 | 0.0268
Epoch 188/300, seasonal_1 Loss: 0.0474 | 0.0265
Epoch 189/300, seasonal_1 Loss: 0.0473 | 0.0268
Epoch 190/300, seasonal_1 Loss: 0.0472 | 0.0268
Epoch 191/300, seasonal_1 Loss: 0.0470 | 0.0272
Epoch 192/300, seasonal_1 Loss: 0.0469 | 0.0273
Epoch 193/300, seasonal_1 Loss: 0.0473 | 0.0273
Epoch 194/300, seasonal_1 Loss: 0.0478 | 0.0269
Epoch 195/300, seasonal_1 Loss: 0.0473 | 0.0269
Epoch 196/300, seasonal_1 Loss: 0.0469 | 0.0268
Epoch 197/300, seasonal_1 Loss: 0.0467 | 0.0273
Epoch 198/300, seasonal_1 Loss: 0.0469 | 0.0272
Epoch 199/300, seasonal_1 Loss: 0.0469 | 0.0272
Epoch 200/300, seasonal_1 Loss: 0.0468 | 0.0267
Epoch 201/300, seasonal_1 Loss: 0.0469 | 0.0265
Epoch 202/300, seasonal_1 Loss: 0.0468 | 0.0266
Epoch 203/300, seasonal_1 Loss: 0.0468 | 0.0272
Epoch 204/300, seasonal_1 Loss: 0.0467 | 0.0278
Epoch 205/300, seasonal_1 Loss: 0.0465 | 0.0288
Epoch 206/300, seasonal_1 Loss: 0.0467 | 0.0276
Epoch 207/300, seasonal_1 Loss: 0.0466 | 0.0273
Epoch 208/300, seasonal_1 Loss: 0.0462 | 0.0266
Epoch 209/300, seasonal_1 Loss: 0.0459 | 0.0272
Epoch 210/300, seasonal_1 Loss: 0.0460 | 0.0277
Epoch 211/300, seasonal_1 Loss: 0.0462 | 0.0274
Epoch 212/300, seasonal_1 Loss: 0.0457 | 0.0267
Epoch 213/300, seasonal_1 Loss: 0.0456 | 0.0268
Epoch 214/300, seasonal_1 Loss: 0.0460 | 0.0276
Epoch 215/300, seasonal_1 Loss: 0.0459 | 0.0283
Epoch 216/300, seasonal_1 Loss: 0.0455 | 0.0292
Epoch 217/300, seasonal_1 Loss: 0.0459 | 0.0270
Epoch 218/300, seasonal_1 Loss: 0.0446 | 0.0267
Epoch 219/300, seasonal_1 Loss: 0.0441 | 0.0275
Epoch 220/300, seasonal_1 Loss: 0.0441 | 0.0283
Epoch 221/300, seasonal_1 Loss: 0.0434 | 0.0290
Epoch 222/300, seasonal_1 Loss: 0.0429 | 0.0283
Epoch 223/300, seasonal_1 Loss: 0.0425 | 0.0280
Epoch 224/300, seasonal_1 Loss: 0.0420 | 0.0281
Epoch 225/300, seasonal_1 Loss: 0.0416 | 0.0286
Epoch 226/300, seasonal_1 Loss: 0.0412 | 0.0289
Epoch 227/300, seasonal_1 Loss: 0.0408 | 0.0290
Epoch 228/300, seasonal_1 Loss: 0.0405 | 0.0290
Epoch 229/300, seasonal_1 Loss: 0.0401 | 0.0291
Epoch 230/300, seasonal_1 Loss: 0.0398 | 0.0292
Epoch 231/300, seasonal_1 Loss: 0.0394 | 0.0291
Epoch 232/300, seasonal_1 Loss: 0.0391 | 0.0292
Epoch 233/300, seasonal_1 Loss: 0.0388 | 0.0296
Epoch 234/300, seasonal_1 Loss: 0.0386 | 0.0299
Epoch 235/300, seasonal_1 Loss: 0.0382 | 0.0298
Epoch 236/300, seasonal_1 Loss: 0.0379 | 0.0293
Epoch 237/300, seasonal_1 Loss: 0.0377 | 0.0291
Epoch 238/300, seasonal_1 Loss: 0.0374 | 0.0300
Epoch 239/300, seasonal_1 Loss: 0.0373 | 0.0307
Epoch 240/300, seasonal_1 Loss: 0.0370 | 0.0293
Epoch 241/300, seasonal_1 Loss: 0.0369 | 0.0291
Epoch 242/300, seasonal_1 Loss: 0.0369 | 0.0286
Epoch 243/300, seasonal_1 Loss: 0.0378 | 0.0279
Epoch 244/300, seasonal_1 Loss: 0.0365 | 0.0288
Epoch 245/300, seasonal_1 Loss: 0.0367 | 0.0322
Epoch 246/300, seasonal_1 Loss: 0.0364 | 0.0299
Epoch 247/300, seasonal_1 Loss: 0.0361 | 0.0292
Epoch 248/300, seasonal_1 Loss: 0.0362 | 0.0288
Epoch 249/300, seasonal_1 Loss: 0.0366 | 0.0300
Epoch 250/300, seasonal_1 Loss: 0.0359 | 0.0291
Epoch 251/300, seasonal_1 Loss: 0.0357 | 0.0294
Epoch 252/300, seasonal_1 Loss: 0.0354 | 0.0293
Epoch 253/300, seasonal_1 Loss: 0.0352 | 0.0290
Epoch 254/300, seasonal_1 Loss: 0.0351 | 0.0293
Epoch 255/300, seasonal_1 Loss: 0.0349 | 0.0288
Epoch 256/300, seasonal_1 Loss: 0.0348 | 0.0290
Epoch 257/300, seasonal_1 Loss: 0.0347 | 0.0288
Epoch 258/300, seasonal_1 Loss: 0.0346 | 0.0289
Epoch 259/300, seasonal_1 Loss: 0.0345 | 0.0288
Epoch 260/300, seasonal_1 Loss: 0.0344 | 0.0289
Epoch 261/300, seasonal_1 Loss: 0.0344 | 0.0287
Epoch 262/300, seasonal_1 Loss: 0.0343 | 0.0289
Epoch 263/300, seasonal_1 Loss: 0.0342 | 0.0287
Epoch 264/300, seasonal_1 Loss: 0.0342 | 0.0291
Epoch 265/300, seasonal_1 Loss: 0.0341 | 0.0288
Epoch 266/300, seasonal_1 Loss: 0.0341 | 0.0293
Epoch 267/300, seasonal_1 Loss: 0.0341 | 0.0289
Epoch 268/300, seasonal_1 Loss: 0.0341 | 0.0295
Epoch 269/300, seasonal_1 Loss: 0.0341 | 0.0289
Epoch 270/300, seasonal_1 Loss: 0.0340 | 0.0295
Epoch 271/300, seasonal_1 Loss: 0.0340 | 0.0290
Epoch 272/300, seasonal_1 Loss: 0.0338 | 0.0293
Epoch 273/300, seasonal_1 Loss: 0.0339 | 0.0290
Epoch 274/300, seasonal_1 Loss: 0.0337 | 0.0290
Epoch 275/300, seasonal_1 Loss: 0.0337 | 0.0290
Epoch 276/300, seasonal_1 Loss: 0.0335 | 0.0289
Epoch 277/300, seasonal_1 Loss: 0.0335 | 0.0289
Epoch 278/300, seasonal_1 Loss: 0.0334 | 0.0288
Epoch 279/300, seasonal_1 Loss: 0.0334 | 0.0289
Epoch 280/300, seasonal_1 Loss: 0.0333 | 0.0288
Epoch 281/300, seasonal_1 Loss: 0.0333 | 0.0288
Epoch 282/300, seasonal_1 Loss: 0.0332 | 0.0288
Epoch 283/300, seasonal_1 Loss: 0.0332 | 0.0288
Epoch 284/300, seasonal_1 Loss: 0.0332 | 0.0288
Epoch 285/300, seasonal_1 Loss: 0.0331 | 0.0288
Epoch 286/300, seasonal_1 Loss: 0.0331 | 0.0288
Epoch 287/300, seasonal_1 Loss: 0.0331 | 0.0288
Epoch 288/300, seasonal_1 Loss: 0.0330 | 0.0288
Epoch 289/300, seasonal_1 Loss: 0.0330 | 0.0288
Epoch 290/300, seasonal_1 Loss: 0.0330 | 0.0288
Epoch 291/300, seasonal_1 Loss: 0.0329 | 0.0288
Epoch 292/300, seasonal_1 Loss: 0.0329 | 0.0288
Epoch 293/300, seasonal_1 Loss: 0.0329 | 0.0287
Epoch 294/300, seasonal_1 Loss: 0.0328 | 0.0288
Epoch 295/300, seasonal_1 Loss: 0.0328 | 0.0288
Epoch 296/300, seasonal_1 Loss: 0.0328 | 0.0288
Epoch 297/300, seasonal_1 Loss: 0.0328 | 0.0288
Epoch 298/300, seasonal_1 Loss: 0.0327 | 0.0288
Epoch 299/300, seasonal_1 Loss: 0.0327 | 0.0288
Epoch 300/300, seasonal_1 Loss: 0.0327 | 0.0289
Training seasonal_2 component with params: {'observation_period_num': 6, 'train_rates': 0.83287600345271, 'learning_rate': 7.039412496932759e-05, 'batch_size': 47, 'step_size': 2, 'gamma': 0.9402736999755821}
Epoch 1/300, seasonal_2 Loss: 0.3668 | 0.1811
Epoch 2/300, seasonal_2 Loss: 0.1188 | 0.0962
Epoch 3/300, seasonal_2 Loss: 0.1127 | 0.0894
Epoch 4/300, seasonal_2 Loss: 0.1212 | 0.0677
Epoch 5/300, seasonal_2 Loss: 0.1225 | 0.0603
Epoch 6/300, seasonal_2 Loss: 0.1079 | 0.0608
Epoch 7/300, seasonal_2 Loss: 0.1211 | 0.0645
Epoch 8/300, seasonal_2 Loss: 0.1074 | 0.0567
Epoch 9/300, seasonal_2 Loss: 0.1111 | 0.0563
Epoch 10/300, seasonal_2 Loss: 0.1028 | 0.0537
Epoch 11/300, seasonal_2 Loss: 0.0955 | 0.0575
Epoch 12/300, seasonal_2 Loss: 0.0867 | 0.0504
Epoch 13/300, seasonal_2 Loss: 0.0825 | 0.0470
Epoch 14/300, seasonal_2 Loss: 0.0808 | 0.0444
Epoch 15/300, seasonal_2 Loss: 0.0787 | 0.0421
Epoch 16/300, seasonal_2 Loss: 0.0772 | 0.0417
Epoch 17/300, seasonal_2 Loss: 0.0773 | 0.0436
Epoch 18/300, seasonal_2 Loss: 0.0788 | 0.0462
Epoch 19/300, seasonal_2 Loss: 0.0802 | 0.0458
Epoch 20/300, seasonal_2 Loss: 0.0801 | 0.0439
Epoch 21/300, seasonal_2 Loss: 0.0787 | 0.0412
Epoch 22/300, seasonal_2 Loss: 0.0770 | 0.0390
Epoch 23/300, seasonal_2 Loss: 0.0751 | 0.0365
Epoch 24/300, seasonal_2 Loss: 0.0732 | 0.0348
Epoch 25/300, seasonal_2 Loss: 0.0718 | 0.0333
Epoch 26/300, seasonal_2 Loss: 0.0709 | 0.0327
Epoch 27/300, seasonal_2 Loss: 0.0705 | 0.0324
Epoch 28/300, seasonal_2 Loss: 0.0700 | 0.0323
Epoch 29/300, seasonal_2 Loss: 0.0695 | 0.0324
Epoch 30/300, seasonal_2 Loss: 0.0695 | 0.0331
Epoch 31/300, seasonal_2 Loss: 0.0695 | 0.0341
Epoch 32/300, seasonal_2 Loss: 0.0692 | 0.0351
Epoch 33/300, seasonal_2 Loss: 0.0686 | 0.0357
Epoch 34/300, seasonal_2 Loss: 0.0678 | 0.0356
Epoch 35/300, seasonal_2 Loss: 0.0671 | 0.0347
Epoch 36/300, seasonal_2 Loss: 0.0665 | 0.0335
Epoch 37/300, seasonal_2 Loss: 0.0660 | 0.0322
Epoch 38/300, seasonal_2 Loss: 0.0656 | 0.0309
Epoch 39/300, seasonal_2 Loss: 0.0652 | 0.0300
Epoch 40/300, seasonal_2 Loss: 0.0651 | 0.0293
Epoch 41/300, seasonal_2 Loss: 0.0650 | 0.0289
Epoch 42/300, seasonal_2 Loss: 0.0649 | 0.0285
Epoch 43/300, seasonal_2 Loss: 0.0648 | 0.0284
Epoch 44/300, seasonal_2 Loss: 0.0646 | 0.0284
Epoch 45/300, seasonal_2 Loss: 0.0643 | 0.0287
Epoch 46/300, seasonal_2 Loss: 0.0640 | 0.0290
Epoch 47/300, seasonal_2 Loss: 0.0637 | 0.0292
Epoch 48/300, seasonal_2 Loss: 0.0634 | 0.0293
Epoch 49/300, seasonal_2 Loss: 0.0631 | 0.0293
Epoch 50/300, seasonal_2 Loss: 0.0628 | 0.0292
Epoch 51/300, seasonal_2 Loss: 0.0626 | 0.0292
Epoch 52/300, seasonal_2 Loss: 0.0625 | 0.0291
Epoch 53/300, seasonal_2 Loss: 0.0625 | 0.0291
Epoch 54/300, seasonal_2 Loss: 0.0625 | 0.0290
Epoch 55/300, seasonal_2 Loss: 0.0624 | 0.0289
Epoch 56/300, seasonal_2 Loss: 0.0624 | 0.0288
Epoch 57/300, seasonal_2 Loss: 0.0624 | 0.0286
Epoch 58/300, seasonal_2 Loss: 0.0625 | 0.0287
Epoch 59/300, seasonal_2 Loss: 0.0624 | 0.0291
Epoch 60/300, seasonal_2 Loss: 0.0625 | 0.0300
Epoch 61/300, seasonal_2 Loss: 0.0631 | 0.0297
Epoch 62/300, seasonal_2 Loss: 0.0645 | 0.0301
Epoch 63/300, seasonal_2 Loss: 0.0638 | 0.0295
Epoch 64/300, seasonal_2 Loss: 0.0629 | 0.0290
Epoch 65/300, seasonal_2 Loss: 0.0624 | 0.0293
Epoch 66/300, seasonal_2 Loss: 0.0619 | 0.0291
Epoch 67/300, seasonal_2 Loss: 0.0617 | 0.0288
Epoch 68/300, seasonal_2 Loss: 0.0615 | 0.0286
Epoch 69/300, seasonal_2 Loss: 0.0614 | 0.0284
Epoch 70/300, seasonal_2 Loss: 0.0613 | 0.0283
Epoch 71/300, seasonal_2 Loss: 0.0612 | 0.0281
Epoch 72/300, seasonal_2 Loss: 0.0612 | 0.0281
Epoch 73/300, seasonal_2 Loss: 0.0611 | 0.0281
Epoch 74/300, seasonal_2 Loss: 0.0611 | 0.0280
Epoch 75/300, seasonal_2 Loss: 0.0610 | 0.0280
Epoch 76/300, seasonal_2 Loss: 0.0610 | 0.0280
Epoch 77/300, seasonal_2 Loss: 0.0610 | 0.0280
Epoch 78/300, seasonal_2 Loss: 0.0609 | 0.0280
Epoch 79/300, seasonal_2 Loss: 0.0609 | 0.0280
Epoch 80/300, seasonal_2 Loss: 0.0608 | 0.0280
Epoch 81/300, seasonal_2 Loss: 0.0608 | 0.0280
Epoch 82/300, seasonal_2 Loss: 0.0608 | 0.0280
Epoch 83/300, seasonal_2 Loss: 0.0608 | 0.0280
Epoch 84/300, seasonal_2 Loss: 0.0607 | 0.0280
Epoch 85/300, seasonal_2 Loss: 0.0607 | 0.0280
Epoch 86/300, seasonal_2 Loss: 0.0607 | 0.0280
Epoch 87/300, seasonal_2 Loss: 0.0607 | 0.0280
Epoch 88/300, seasonal_2 Loss: 0.0606 | 0.0280
Epoch 89/300, seasonal_2 Loss: 0.0606 | 0.0280
Epoch 90/300, seasonal_2 Loss: 0.0606 | 0.0280
Epoch 91/300, seasonal_2 Loss: 0.0606 | 0.0280
Epoch 92/300, seasonal_2 Loss: 0.0606 | 0.0280
Epoch 93/300, seasonal_2 Loss: 0.0606 | 0.0280
Epoch 94/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 95/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 96/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 97/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 98/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 99/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 100/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 101/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 102/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 103/300, seasonal_2 Loss: 0.0605 | 0.0280
Epoch 104/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 105/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 106/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 107/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 108/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 109/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 110/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 111/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 112/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 113/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 114/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 115/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 116/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 117/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 118/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 119/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 120/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 121/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 122/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 123/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 124/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 125/300, seasonal_2 Loss: 0.0604 | 0.0280
Epoch 126/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 127/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 128/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 129/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 130/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 131/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 132/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 133/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 134/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 135/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 136/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 137/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 138/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 139/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 140/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 141/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 142/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 143/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 144/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 145/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 146/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 147/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 148/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 149/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 150/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 151/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 152/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 153/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 154/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 155/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 156/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 157/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 158/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 159/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 160/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 161/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 162/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 163/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 164/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 165/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 166/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 167/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 168/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 169/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 170/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 171/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 172/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 173/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 174/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 175/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 176/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 177/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 178/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 179/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 180/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 181/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 182/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 183/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 184/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 185/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 186/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 187/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 188/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 189/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 190/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 191/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 192/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 193/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 194/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 195/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 196/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 197/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 198/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 199/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 200/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 201/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 202/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 203/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 204/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 205/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 206/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 207/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 208/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 209/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 210/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 211/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 212/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 213/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 214/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 215/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 216/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 217/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 218/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 219/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 220/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 221/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 222/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 223/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 224/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 225/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 226/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 227/300, seasonal_2 Loss: 0.0603 | 0.0280
Epoch 228/300, seasonal_2 Loss: 0.0603 | 0.0280
Early stopping for seasonal_2
Training seasonal_3 component with params: {'observation_period_num': 6, 'train_rates': 0.8756141539070926, 'learning_rate': 0.00036938987044359006, 'batch_size': 18, 'step_size': 9, 'gamma': 0.9473219628826309}
Epoch 1/300, seasonal_3 Loss: 0.3178 | 0.0916
Epoch 2/300, seasonal_3 Loss: 0.1416 | 0.1432
Epoch 3/300, seasonal_3 Loss: 0.1344 | 0.1833
Epoch 4/300, seasonal_3 Loss: 0.1261 | 0.1461
Epoch 5/300, seasonal_3 Loss: 0.1296 | 0.1493
Epoch 6/300, seasonal_3 Loss: 0.1273 | 0.1948
Epoch 7/300, seasonal_3 Loss: 0.1443 | 0.1430
Epoch 8/300, seasonal_3 Loss: 0.1277 | 0.1306
Epoch 9/300, seasonal_3 Loss: 0.1265 | 0.1103
Epoch 10/300, seasonal_3 Loss: 0.1186 | 0.0833
Epoch 11/300, seasonal_3 Loss: 0.1165 | 0.0660
Epoch 12/300, seasonal_3 Loss: 0.1063 | 0.0706
Epoch 13/300, seasonal_3 Loss: 0.1030 | 0.0778
Epoch 14/300, seasonal_3 Loss: 0.1030 | 0.0636
Epoch 15/300, seasonal_3 Loss: 0.0946 | 0.0558
Epoch 16/300, seasonal_3 Loss: 0.0923 | 0.0754
Epoch 17/300, seasonal_3 Loss: 0.0971 | 0.0783
Epoch 18/300, seasonal_3 Loss: 0.0990 | 0.0609
Epoch 19/300, seasonal_3 Loss: 0.0918 | 0.0552
Epoch 20/300, seasonal_3 Loss: 0.0930 | 0.0729
Epoch 21/300, seasonal_3 Loss: 0.0906 | 0.0524
Epoch 22/300, seasonal_3 Loss: 0.0811 | 0.0451
Epoch 23/300, seasonal_3 Loss: 0.0796 | 0.0559
Epoch 24/300, seasonal_3 Loss: 0.0782 | 0.0396
Epoch 25/300, seasonal_3 Loss: 0.0765 | 0.0426
Epoch 26/300, seasonal_3 Loss: 0.0779 | 0.0689
Epoch 27/300, seasonal_3 Loss: 0.0816 | 0.0535
Epoch 28/300, seasonal_3 Loss: 0.0829 | 0.0515
Epoch 29/300, seasonal_3 Loss: 0.0780 | 0.0715
Epoch 30/300, seasonal_3 Loss: 0.0830 | 0.0498
Epoch 31/300, seasonal_3 Loss: 0.0863 | 0.0508
Epoch 32/300, seasonal_3 Loss: 0.0743 | 0.0418
Epoch 33/300, seasonal_3 Loss: 0.0693 | 0.0375
Epoch 34/300, seasonal_3 Loss: 0.0687 | 0.0373
Epoch 35/300, seasonal_3 Loss: 0.0648 | 0.0366
Epoch 36/300, seasonal_3 Loss: 0.0664 | 0.0402
Epoch 37/300, seasonal_3 Loss: 0.0639 | 0.0371
Epoch 38/300, seasonal_3 Loss: 0.0623 | 0.0340
Epoch 39/300, seasonal_3 Loss: 0.0624 | 0.0468
Epoch 40/300, seasonal_3 Loss: 0.0672 | 0.0363
Epoch 41/300, seasonal_3 Loss: 0.0628 | 0.0352
Epoch 42/300, seasonal_3 Loss: 0.0593 | 0.0345
Epoch 43/300, seasonal_3 Loss: 0.0601 | 0.0388
Epoch 44/300, seasonal_3 Loss: 0.0596 | 0.0396
Epoch 45/300, seasonal_3 Loss: 0.0624 | 0.0508
Epoch 46/300, seasonal_3 Loss: 0.0648 | 0.0484
Epoch 47/300, seasonal_3 Loss: 0.0628 | 0.0434
Epoch 48/300, seasonal_3 Loss: 0.0608 | 0.0378
Epoch 49/300, seasonal_3 Loss: 0.0574 | 0.0374
Epoch 50/300, seasonal_3 Loss: 0.0596 | 0.0430
Epoch 51/300, seasonal_3 Loss: 0.0573 | 0.0441
Epoch 52/300, seasonal_3 Loss: 0.0551 | 0.0372
Epoch 53/300, seasonal_3 Loss: 0.0569 | 0.0615
Epoch 54/300, seasonal_3 Loss: 0.0665 | 0.0372
Epoch 55/300, seasonal_3 Loss: 0.0603 | 0.0380
Epoch 56/300, seasonal_3 Loss: 0.0552 | 0.0401
Epoch 57/300, seasonal_3 Loss: 0.0537 | 0.0383
Epoch 58/300, seasonal_3 Loss: 0.0523 | 0.0475
Epoch 59/300, seasonal_3 Loss: 0.0517 | 0.0352
Epoch 60/300, seasonal_3 Loss: 0.0508 | 0.0395
Epoch 61/300, seasonal_3 Loss: 0.0608 | 0.0424
Epoch 62/300, seasonal_3 Loss: 0.0582 | 0.0317
Epoch 63/300, seasonal_3 Loss: 0.0608 | 0.0409
Epoch 64/300, seasonal_3 Loss: 0.0548 | 0.0490
Epoch 65/300, seasonal_3 Loss: 0.0520 | 0.0616
Epoch 66/300, seasonal_3 Loss: 0.0507 | 0.0497
Epoch 67/300, seasonal_3 Loss: 0.0491 | 0.0531
Epoch 68/300, seasonal_3 Loss: 0.0477 | 0.0384
Epoch 69/300, seasonal_3 Loss: 0.0472 | 0.0443
Epoch 70/300, seasonal_3 Loss: 0.0469 | 0.0437
Epoch 71/300, seasonal_3 Loss: 0.0475 | 0.0378
Epoch 72/300, seasonal_3 Loss: 0.0480 | 0.0395
Epoch 73/300, seasonal_3 Loss: 0.0447 | 0.0375
Epoch 74/300, seasonal_3 Loss: 0.0540 | 0.0518
Epoch 75/300, seasonal_3 Loss: 0.0582 | 0.0495
Epoch 76/300, seasonal_3 Loss: 0.0495 | 0.0565
Epoch 77/300, seasonal_3 Loss: 0.0465 | 0.0589
Epoch 78/300, seasonal_3 Loss: 0.0446 | 0.0559
Epoch 79/300, seasonal_3 Loss: 0.0431 | 0.0485
Epoch 80/300, seasonal_3 Loss: 0.0462 | 0.0413
Epoch 81/300, seasonal_3 Loss: 0.0485 | 0.0432
Epoch 82/300, seasonal_3 Loss: 0.0454 | 0.0516
Epoch 83/300, seasonal_3 Loss: 0.0440 | 0.0456
Epoch 84/300, seasonal_3 Loss: 0.0421 | 0.0346
Epoch 85/300, seasonal_3 Loss: 0.0413 | 0.0426
Epoch 86/300, seasonal_3 Loss: 0.0439 | 0.0462
Epoch 87/300, seasonal_3 Loss: 0.0449 | 0.0504
Epoch 88/300, seasonal_3 Loss: 0.0464 | 0.0403
Epoch 89/300, seasonal_3 Loss: 0.0414 | 0.0454
Epoch 90/300, seasonal_3 Loss: 0.0439 | 0.0370
Epoch 91/300, seasonal_3 Loss: 0.0462 | 0.0777
Epoch 92/300, seasonal_3 Loss: 0.0447 | 0.0534
Epoch 93/300, seasonal_3 Loss: 0.0427 | 0.0368
Epoch 94/300, seasonal_3 Loss: 0.0387 | 0.0272
Epoch 95/300, seasonal_3 Loss: 0.0372 | 0.0306
Epoch 96/300, seasonal_3 Loss: 0.0428 | 0.0282
Epoch 97/300, seasonal_3 Loss: 0.0452 | 0.0364
Epoch 98/300, seasonal_3 Loss: 0.0394 | 0.0307
Epoch 99/300, seasonal_3 Loss: 0.0410 | 0.0384
Epoch 100/300, seasonal_3 Loss: 0.0399 | 0.0359
Epoch 101/300, seasonal_3 Loss: 0.0374 | 0.0298
Epoch 102/300, seasonal_3 Loss: 0.0359 | 0.0352
Epoch 103/300, seasonal_3 Loss: 0.0402 | 0.0523
Epoch 104/300, seasonal_3 Loss: 0.0387 | 0.0299
Epoch 105/300, seasonal_3 Loss: 0.0397 | 0.0304
Epoch 106/300, seasonal_3 Loss: 0.0401 | 0.0427
Epoch 107/300, seasonal_3 Loss: 0.0572 | 0.0421
Epoch 108/300, seasonal_3 Loss: 0.0488 | 0.0383
Epoch 109/300, seasonal_3 Loss: 0.0424 | 0.0310
Epoch 110/300, seasonal_3 Loss: 0.0377 | 0.0303
Epoch 111/300, seasonal_3 Loss: 0.0363 | 0.0324
Epoch 112/300, seasonal_3 Loss: 0.0344 | 0.0323
Epoch 113/300, seasonal_3 Loss: 0.0331 | 0.0314
Epoch 114/300, seasonal_3 Loss: 0.0322 | 0.0330
Epoch 115/300, seasonal_3 Loss: 0.0322 | 0.0298
Epoch 116/300, seasonal_3 Loss: 0.0318 | 0.0334
Epoch 117/300, seasonal_3 Loss: 0.0381 | 0.0294
Epoch 118/300, seasonal_3 Loss: 0.0369 | 0.0300
Epoch 119/300, seasonal_3 Loss: 0.0348 | 0.0287
Epoch 120/300, seasonal_3 Loss: 0.0355 | 0.0336
Epoch 121/300, seasonal_3 Loss: 0.0406 | 0.0393
Epoch 122/300, seasonal_3 Loss: 0.0393 | 0.0412
Epoch 123/300, seasonal_3 Loss: 0.0414 | 0.0283
Epoch 124/300, seasonal_3 Loss: 0.0407 | 0.0327
Epoch 125/300, seasonal_3 Loss: 0.0389 | 0.0300
Epoch 126/300, seasonal_3 Loss: 0.0374 | 0.0312
Epoch 127/300, seasonal_3 Loss: 0.0375 | 0.0312
Epoch 128/300, seasonal_3 Loss: 0.0377 | 0.0276
Epoch 129/300, seasonal_3 Loss: 0.0403 | 0.0328
Epoch 130/300, seasonal_3 Loss: 0.0359 | 0.0305
Epoch 131/300, seasonal_3 Loss: 0.0369 | 0.0312
Epoch 132/300, seasonal_3 Loss: 0.0338 | 0.0313
Epoch 133/300, seasonal_3 Loss: 0.0355 | 0.0368
Epoch 134/300, seasonal_3 Loss: 0.0342 | 0.0291
Epoch 135/300, seasonal_3 Loss: 0.0332 | 0.0373
Epoch 136/300, seasonal_3 Loss: 0.0381 | 0.0315
Epoch 137/300, seasonal_3 Loss: 0.0335 | 0.0291
Epoch 138/300, seasonal_3 Loss: 0.0306 | 0.0321
Epoch 139/300, seasonal_3 Loss: 0.0334 | 0.0310
Epoch 140/300, seasonal_3 Loss: 0.0368 | 0.0330
Epoch 141/300, seasonal_3 Loss: 0.0349 | 0.0344
Epoch 142/300, seasonal_3 Loss: 0.0394 | 0.0323
Epoch 143/300, seasonal_3 Loss: 0.0380 | 0.0353
Epoch 144/300, seasonal_3 Loss: 0.0361 | 0.0362
Epoch 145/300, seasonal_3 Loss: 0.0346 | 0.0390
Epoch 146/300, seasonal_3 Loss: 0.0309 | 0.0418
Epoch 147/300, seasonal_3 Loss: 0.0376 | 0.0387
Epoch 148/300, seasonal_3 Loss: 0.0380 | 0.0434
Epoch 149/300, seasonal_3 Loss: 0.0370 | 0.0423
Epoch 150/300, seasonal_3 Loss: 0.0362 | 0.0491
Epoch 151/300, seasonal_3 Loss: 0.0356 | 0.0402
Epoch 152/300, seasonal_3 Loss: 0.0346 | 0.0459
Epoch 153/300, seasonal_3 Loss: 0.0327 | 0.0458
Epoch 154/300, seasonal_3 Loss: 0.0311 | 0.0481
Epoch 155/300, seasonal_3 Loss: 0.0320 | 0.0473
Epoch 156/300, seasonal_3 Loss: 0.0308 | 0.0433
Epoch 157/300, seasonal_3 Loss: 0.0357 | 0.0469
Epoch 158/300, seasonal_3 Loss: 0.0343 | 0.0411
Epoch 159/300, seasonal_3 Loss: 0.0309 | 0.0435
Epoch 160/300, seasonal_3 Loss: 0.0307 | 0.0438
Epoch 161/300, seasonal_3 Loss: 0.0293 | 0.0412
Epoch 162/300, seasonal_3 Loss: 0.0287 | 0.0409
Epoch 163/300, seasonal_3 Loss: 0.0290 | 0.0332
Epoch 164/300, seasonal_3 Loss: 0.0277 | 0.0305
Epoch 165/300, seasonal_3 Loss: 0.0284 | 0.0345
Epoch 166/300, seasonal_3 Loss: 0.0254 | 0.0319
Epoch 167/300, seasonal_3 Loss: 0.0302 | 0.0319
Epoch 168/300, seasonal_3 Loss: 0.0264 | 0.0307
Epoch 169/300, seasonal_3 Loss: 0.0273 | 0.0295
Epoch 170/300, seasonal_3 Loss: 0.0267 | 0.0323
Epoch 171/300, seasonal_3 Loss: 0.0268 | 0.0310
Epoch 172/300, seasonal_3 Loss: 0.0274 | 0.0321
Epoch 173/300, seasonal_3 Loss: 0.0255 | 0.0308
Epoch 174/300, seasonal_3 Loss: 0.0250 | 0.0311
Epoch 175/300, seasonal_3 Loss: 0.0244 | 0.0296
Epoch 176/300, seasonal_3 Loss: 0.0248 | 0.0285
Epoch 177/300, seasonal_3 Loss: 0.0232 | 0.0275
Epoch 178/300, seasonal_3 Loss: 0.0232 | 0.0273
Epoch 179/300, seasonal_3 Loss: 0.0335 | 0.0270
Epoch 180/300, seasonal_3 Loss: 0.0378 | 0.0310
Epoch 181/300, seasonal_3 Loss: 0.0364 | 0.0280
Epoch 182/300, seasonal_3 Loss: 0.0307 | 0.0276
Epoch 183/300, seasonal_3 Loss: 0.0320 | 0.0302
Epoch 184/300, seasonal_3 Loss: 0.0323 | 0.0309
Epoch 185/300, seasonal_3 Loss: 0.0343 | 0.0268
Epoch 186/300, seasonal_3 Loss: 0.0309 | 0.0269
Epoch 187/300, seasonal_3 Loss: 0.0305 | 0.0259
Epoch 188/300, seasonal_3 Loss: 0.0292 | 0.0284
Epoch 189/300, seasonal_3 Loss: 0.0286 | 0.0276
Epoch 190/300, seasonal_3 Loss: 0.0285 | 0.0260
Epoch 191/300, seasonal_3 Loss: 0.0276 | 0.0261
Epoch 192/300, seasonal_3 Loss: 0.0302 | 0.0275
Epoch 193/300, seasonal_3 Loss: 0.0284 | 0.0265
Epoch 194/300, seasonal_3 Loss: 0.0436 | 0.0280
Epoch 195/300, seasonal_3 Loss: 0.0362 | 0.0291
Epoch 196/300, seasonal_3 Loss: 0.0348 | 0.0293
Epoch 197/300, seasonal_3 Loss: 0.0336 | 0.0296
Epoch 198/300, seasonal_3 Loss: 0.0330 | 0.0299
Epoch 199/300, seasonal_3 Loss: 0.0330 | 0.0300
Epoch 200/300, seasonal_3 Loss: 0.0315 | 0.0296
Epoch 201/300, seasonal_3 Loss: 0.0313 | 0.0294
Epoch 202/300, seasonal_3 Loss: 0.0316 | 0.0299
Epoch 203/300, seasonal_3 Loss: 0.0287 | 0.0308
Epoch 204/300, seasonal_3 Loss: 0.0284 | 0.0303
Epoch 205/300, seasonal_3 Loss: 0.0290 | 0.0305
Epoch 206/300, seasonal_3 Loss: 0.0260 | 0.0307
Epoch 207/300, seasonal_3 Loss: 0.0275 | 0.0316
Epoch 208/300, seasonal_3 Loss: 0.0244 | 0.0309
Epoch 209/300, seasonal_3 Loss: 0.0246 | 0.0317
Epoch 210/300, seasonal_3 Loss: 0.0259 | 0.0309
Epoch 211/300, seasonal_3 Loss: 0.0252 | 0.0323
Epoch 212/300, seasonal_3 Loss: 0.0233 | 0.0323
Epoch 213/300, seasonal_3 Loss: 0.0231 | 0.0337
Epoch 214/300, seasonal_3 Loss: 0.0228 | 0.0359
Epoch 215/300, seasonal_3 Loss: 0.0226 | 0.0410
Epoch 216/300, seasonal_3 Loss: 0.0225 | 0.0479
Epoch 217/300, seasonal_3 Loss: 0.0225 | 0.0639
Epoch 218/300, seasonal_3 Loss: 0.0227 | 0.0745
Epoch 219/300, seasonal_3 Loss: 0.0244 | 0.0805
Epoch 220/300, seasonal_3 Loss: 0.0232 | 0.0669
Epoch 221/300, seasonal_3 Loss: 0.0257 | 0.0605
Epoch 222/300, seasonal_3 Loss: 0.0260 | 0.0549
Epoch 223/300, seasonal_3 Loss: 0.0359 | 0.0509
Epoch 224/300, seasonal_3 Loss: 0.0331 | 0.0445
Epoch 225/300, seasonal_3 Loss: 0.0273 | 0.0403
Epoch 226/300, seasonal_3 Loss: 0.0234 | 0.0402
Epoch 227/300, seasonal_3 Loss: 0.0252 | 0.0403
Epoch 228/300, seasonal_3 Loss: 0.0240 | 0.0403
Epoch 229/300, seasonal_3 Loss: 0.0233 | 0.0428
Epoch 230/300, seasonal_3 Loss: 0.0240 | 0.0448
Epoch 231/300, seasonal_3 Loss: 0.0222 | 0.0413
Epoch 232/300, seasonal_3 Loss: 0.0208 | 0.0405
Epoch 233/300, seasonal_3 Loss: 0.0212 | 0.0395
Epoch 234/300, seasonal_3 Loss: 0.0345 | 0.0381
Epoch 235/300, seasonal_3 Loss: 0.0320 | 0.0373
Epoch 236/300, seasonal_3 Loss: 0.0259 | 0.0362
Epoch 237/300, seasonal_3 Loss: 0.0258 | 0.0356
Epoch 238/300, seasonal_3 Loss: 0.0285 | 0.0363
Epoch 239/300, seasonal_3 Loss: 0.0264 | 0.0392
Epoch 240/300, seasonal_3 Loss: 0.0293 | 0.0370
Epoch 241/300, seasonal_3 Loss: 0.0272 | 0.0388
Epoch 242/300, seasonal_3 Loss: 0.0275 | 0.0375
Epoch 243/300, seasonal_3 Loss: 0.0284 | 0.0378
Epoch 244/300, seasonal_3 Loss: 0.0233 | 0.0398
Epoch 245/300, seasonal_3 Loss: 0.0219 | 0.0423
Epoch 246/300, seasonal_3 Loss: 0.0206 | 0.0490
Epoch 247/300, seasonal_3 Loss: 0.0211 | 0.0405
Epoch 248/300, seasonal_3 Loss: 0.0206 | 0.0484
Epoch 249/300, seasonal_3 Loss: 0.0240 | 0.0445
Epoch 250/300, seasonal_3 Loss: 0.0211 | 0.0424
Epoch 251/300, seasonal_3 Loss: 0.0211 | 0.0439
Epoch 252/300, seasonal_3 Loss: 0.0211 | 0.0385
Epoch 253/300, seasonal_3 Loss: 0.0209 | 0.0408
Epoch 254/300, seasonal_3 Loss: 0.0208 | 0.0381
Epoch 255/300, seasonal_3 Loss: 0.0206 | 0.0375
Epoch 256/300, seasonal_3 Loss: 0.0211 | 0.0391
Epoch 257/300, seasonal_3 Loss: 0.0200 | 0.0358
Epoch 258/300, seasonal_3 Loss: 0.0258 | 0.0348
Epoch 259/300, seasonal_3 Loss: 0.0300 | 0.0380
Epoch 260/300, seasonal_3 Loss: 0.0247 | 0.0340
Epoch 261/300, seasonal_3 Loss: 0.0220 | 0.0338
Epoch 262/300, seasonal_3 Loss: 0.0197 | 0.0333
Epoch 263/300, seasonal_3 Loss: 0.0243 | 0.0338
Epoch 264/300, seasonal_3 Loss: 0.0198 | 0.0453
Epoch 265/300, seasonal_3 Loss: 0.0240 | 0.0356
Epoch 266/300, seasonal_3 Loss: 0.0227 | 0.0345
Epoch 267/300, seasonal_3 Loss: 0.0223 | 0.0345
Epoch 268/300, seasonal_3 Loss: 0.0222 | 0.0346
Epoch 269/300, seasonal_3 Loss: 0.0222 | 0.0347
Epoch 270/300, seasonal_3 Loss: 0.0219 | 0.0353
Epoch 271/300, seasonal_3 Loss: 0.0234 | 0.0412
Epoch 272/300, seasonal_3 Loss: 0.0221 | 0.0348
Epoch 273/300, seasonal_3 Loss: 0.0202 | 0.0365
Epoch 274/300, seasonal_3 Loss: 0.0201 | 0.0362
Epoch 275/300, seasonal_3 Loss: 0.0196 | 0.0368
Epoch 276/300, seasonal_3 Loss: 0.0193 | 0.0361
Epoch 277/300, seasonal_3 Loss: 0.0210 | 0.0360
Epoch 278/300, seasonal_3 Loss: 0.0208 | 0.0391
Epoch 279/300, seasonal_3 Loss: 0.0196 | 0.0364
Epoch 280/300, seasonal_3 Loss: 0.0204 | 0.0405
Epoch 281/300, seasonal_3 Loss: 0.0201 | 0.0384
Epoch 282/300, seasonal_3 Loss: 0.0194 | 0.0398
Epoch 283/300, seasonal_3 Loss: 0.0191 | 0.0366
Epoch 284/300, seasonal_3 Loss: 0.0193 | 0.0383
Epoch 285/300, seasonal_3 Loss: 0.0190 | 0.0455
Epoch 286/300, seasonal_3 Loss: 0.0201 | 0.0419
Epoch 287/300, seasonal_3 Loss: 0.0196 | 0.0367
Epoch 288/300, seasonal_3 Loss: 0.0194 | 0.0398
Epoch 289/300, seasonal_3 Loss: 0.0195 | 0.0355
Epoch 290/300, seasonal_3 Loss: 0.0223 | 0.0355
Epoch 291/300, seasonal_3 Loss: 0.0193 | 0.0351
Epoch 292/300, seasonal_3 Loss: 0.0190 | 0.0348
Epoch 293/300, seasonal_3 Loss: 0.0188 | 0.0354
Epoch 294/300, seasonal_3 Loss: 0.0182 | 0.0349
Epoch 295/300, seasonal_3 Loss: 0.0174 | 0.0346
Epoch 296/300, seasonal_3 Loss: 0.0166 | 0.0345
Epoch 297/300, seasonal_3 Loss: 0.0176 | 0.0346
Epoch 298/300, seasonal_3 Loss: 0.0200 | 0.0350
Epoch 299/300, seasonal_3 Loss: 0.0215 | 0.0348
Epoch 300/300, seasonal_3 Loss: 0.0197 | 0.0352
Training resid component with params: {'observation_period_num': 5, 'train_rates': 0.9867333570276001, 'learning_rate': 0.00024448875517737854, 'batch_size': 187, 'step_size': 13, 'gamma': 0.7508120258537639}
Epoch 1/300, resid Loss: 1.9174 | 0.5036
Epoch 2/300, resid Loss: 0.5011 | 0.4963
Epoch 3/300, resid Loss: 0.4740 | 0.5995
Epoch 4/300, resid Loss: 0.6058 | 0.4561
Epoch 5/300, resid Loss: 0.3638 | 0.3266
Epoch 6/300, resid Loss: 0.3995 | 0.2184
Epoch 7/300, resid Loss: 0.4290 | 0.3328
Epoch 8/300, resid Loss: 0.3017 | 0.1964
Epoch 9/300, resid Loss: 0.1672 | 0.1199
Epoch 10/300, resid Loss: 0.1466 | 0.1180
Epoch 11/300, resid Loss: 0.1902 | 0.2494
Epoch 12/300, resid Loss: 0.1960 | 0.3258
Epoch 13/300, resid Loss: 0.1788 | 0.1022
Epoch 14/300, resid Loss: 0.1839 | 0.1004
Epoch 15/300, resid Loss: 0.1773 | 0.1370
Epoch 16/300, resid Loss: 0.1912 | 0.0917
Epoch 17/300, resid Loss: 0.1627 | 0.1089
Epoch 18/300, resid Loss: 0.1602 | 0.1341
Epoch 19/300, resid Loss: 0.1455 | 0.0871
Epoch 20/300, resid Loss: 0.1153 | 0.0706
Epoch 21/300, resid Loss: 0.1068 | 0.0623
Epoch 22/300, resid Loss: 0.1050 | 0.0538
Epoch 23/300, resid Loss: 0.0922 | 0.0544
Epoch 24/300, resid Loss: 0.0963 | 0.0525
Epoch 25/300, resid Loss: 0.0891 | 0.0489
Epoch 26/300, resid Loss: 0.0998 | 0.0485
Epoch 27/300, resid Loss: 0.1018 | 0.0499
Epoch 28/300, resid Loss: 0.0851 | 0.0518
Epoch 29/300, resid Loss: 0.0893 | 0.0496
Epoch 30/300, resid Loss: 0.0844 | 0.0463
Epoch 31/300, resid Loss: 0.0900 | 0.0457
Epoch 32/300, resid Loss: 0.0841 | 0.0468
Epoch 33/300, resid Loss: 0.0820 | 0.0525
Epoch 34/300, resid Loss: 0.0825 | 0.0454
Epoch 35/300, resid Loss: 0.0828 | 0.0455
Epoch 36/300, resid Loss: 0.0827 | 0.0502
Epoch 37/300, resid Loss: 0.0778 | 0.0445
Epoch 38/300, resid Loss: 0.0769 | 0.0443
Epoch 39/300, resid Loss: 0.0764 | 0.0437
Epoch 40/300, resid Loss: 0.0759 | 0.0435
Epoch 41/300, resid Loss: 0.0746 | 0.0431
Epoch 42/300, resid Loss: 0.0743 | 0.0430
Epoch 43/300, resid Loss: 0.0741 | 0.0424
Epoch 44/300, resid Loss: 0.0736 | 0.0420
Epoch 45/300, resid Loss: 0.0730 | 0.0420
Epoch 46/300, resid Loss: 0.0727 | 0.0418
Epoch 47/300, resid Loss: 0.0725 | 0.0417
Epoch 48/300, resid Loss: 0.0722 | 0.0413
Epoch 49/300, resid Loss: 0.0718 | 0.0409
Epoch 50/300, resid Loss: 0.0717 | 0.0404
Epoch 51/300, resid Loss: 0.0725 | 0.0401
Epoch 52/300, resid Loss: 0.0738 | 0.0404
Epoch 53/300, resid Loss: 0.0720 | 0.0412
Epoch 54/300, resid Loss: 0.0719 | 0.0407
Epoch 55/300, resid Loss: 0.0711 | 0.0406
Epoch 56/300, resid Loss: 0.0706 | 0.0400
Epoch 57/300, resid Loss: 0.0703 | 0.0401
Epoch 58/300, resid Loss: 0.0701 | 0.0398
Epoch 59/300, resid Loss: 0.0699 | 0.0397
Epoch 60/300, resid Loss: 0.0698 | 0.0395
Epoch 61/300, resid Loss: 0.0696 | 0.0394
Epoch 62/300, resid Loss: 0.0695 | 0.0393
Epoch 63/300, resid Loss: 0.0694 | 0.0392
Epoch 64/300, resid Loss: 0.0692 | 0.0390
Epoch 65/300, resid Loss: 0.0691 | 0.0389
Epoch 66/300, resid Loss: 0.0690 | 0.0389
Epoch 67/300, resid Loss: 0.0689 | 0.0388
Epoch 68/300, resid Loss: 0.0688 | 0.0387
Epoch 69/300, resid Loss: 0.0687 | 0.0386
Epoch 70/300, resid Loss: 0.0686 | 0.0385
Epoch 71/300, resid Loss: 0.0685 | 0.0384
Epoch 72/300, resid Loss: 0.0684 | 0.0384
Epoch 73/300, resid Loss: 0.0682 | 0.0383
Epoch 74/300, resid Loss: 0.0682 | 0.0382
Epoch 75/300, resid Loss: 0.0681 | 0.0382
Epoch 76/300, resid Loss: 0.0680 | 0.0381
Epoch 77/300, resid Loss: 0.0679 | 0.0380
Epoch 78/300, resid Loss: 0.0678 | 0.0379
Epoch 79/300, resid Loss: 0.0677 | 0.0379
Epoch 80/300, resid Loss: 0.0677 | 0.0378
Epoch 81/300, resid Loss: 0.0676 | 0.0378
Epoch 82/300, resid Loss: 0.0675 | 0.0377
Epoch 83/300, resid Loss: 0.0674 | 0.0377
Epoch 84/300, resid Loss: 0.0674 | 0.0376
Epoch 85/300, resid Loss: 0.0673 | 0.0375
Epoch 86/300, resid Loss: 0.0672 | 0.0375
Epoch 87/300, resid Loss: 0.0671 | 0.0375
Epoch 88/300, resid Loss: 0.0671 | 0.0374
Epoch 89/300, resid Loss: 0.0670 | 0.0374
Epoch 90/300, resid Loss: 0.0669 | 0.0373
Epoch 91/300, resid Loss: 0.0669 | 0.0373
Epoch 92/300, resid Loss: 0.0668 | 0.0372
Epoch 93/300, resid Loss: 0.0667 | 0.0372
Epoch 94/300, resid Loss: 0.0667 | 0.0371
Epoch 95/300, resid Loss: 0.0666 | 0.0371
Epoch 96/300, resid Loss: 0.0666 | 0.0371
Epoch 97/300, resid Loss: 0.0665 | 0.0370
Epoch 98/300, resid Loss: 0.0665 | 0.0370
Epoch 99/300, resid Loss: 0.0664 | 0.0369
Epoch 100/300, resid Loss: 0.0663 | 0.0369
Epoch 101/300, resid Loss: 0.0663 | 0.0369
Epoch 102/300, resid Loss: 0.0662 | 0.0368
Epoch 103/300, resid Loss: 0.0662 | 0.0368
Epoch 104/300, resid Loss: 0.0661 | 0.0368
Epoch 105/300, resid Loss: 0.0661 | 0.0367
Epoch 106/300, resid Loss: 0.0660 | 0.0367
Epoch 107/300, resid Loss: 0.0660 | 0.0367
Epoch 108/300, resid Loss: 0.0660 | 0.0367
Epoch 109/300, resid Loss: 0.0659 | 0.0366
Epoch 110/300, resid Loss: 0.0659 | 0.0366
Epoch 111/300, resid Loss: 0.0658 | 0.0366
Epoch 112/300, resid Loss: 0.0658 | 0.0366
Epoch 113/300, resid Loss: 0.0657 | 0.0365
Epoch 114/300, resid Loss: 0.0657 | 0.0365
Epoch 115/300, resid Loss: 0.0657 | 0.0365
Epoch 116/300, resid Loss: 0.0656 | 0.0365
Epoch 117/300, resid Loss: 0.0656 | 0.0364
Epoch 118/300, resid Loss: 0.0656 | 0.0364
Epoch 119/300, resid Loss: 0.0655 | 0.0364
Epoch 120/300, resid Loss: 0.0655 | 0.0364
Epoch 121/300, resid Loss: 0.0655 | 0.0364
Epoch 122/300, resid Loss: 0.0654 | 0.0364
Epoch 123/300, resid Loss: 0.0654 | 0.0363
Epoch 124/300, resid Loss: 0.0654 | 0.0363
Epoch 125/300, resid Loss: 0.0653 | 0.0363
Epoch 126/300, resid Loss: 0.0653 | 0.0363
Epoch 127/300, resid Loss: 0.0653 | 0.0363
Epoch 128/300, resid Loss: 0.0653 | 0.0363
Epoch 129/300, resid Loss: 0.0652 | 0.0363
Epoch 130/300, resid Loss: 0.0652 | 0.0362
Epoch 131/300, resid Loss: 0.0652 | 0.0362
Epoch 132/300, resid Loss: 0.0652 | 0.0362
Epoch 133/300, resid Loss: 0.0651 | 0.0362
Epoch 134/300, resid Loss: 0.0651 | 0.0362
Epoch 135/300, resid Loss: 0.0651 | 0.0362
Epoch 136/300, resid Loss: 0.0651 | 0.0362
Epoch 137/300, resid Loss: 0.0651 | 0.0362
Epoch 138/300, resid Loss: 0.0650 | 0.0362
Epoch 139/300, resid Loss: 0.0650 | 0.0362
Epoch 140/300, resid Loss: 0.0650 | 0.0362
Epoch 141/300, resid Loss: 0.0650 | 0.0361
Epoch 142/300, resid Loss: 0.0650 | 0.0361
Epoch 143/300, resid Loss: 0.0650 | 0.0361
Epoch 144/300, resid Loss: 0.0649 | 0.0361
Epoch 145/300, resid Loss: 0.0649 | 0.0361
Epoch 146/300, resid Loss: 0.0649 | 0.0361
Epoch 147/300, resid Loss: 0.0649 | 0.0361
Epoch 148/300, resid Loss: 0.0649 | 0.0361
Epoch 149/300, resid Loss: 0.0649 | 0.0361
Epoch 150/300, resid Loss: 0.0649 | 0.0361
Epoch 151/300, resid Loss: 0.0648 | 0.0361
Epoch 152/300, resid Loss: 0.0648 | 0.0361
Epoch 153/300, resid Loss: 0.0648 | 0.0361
Epoch 154/300, resid Loss: 0.0648 | 0.0361
Epoch 155/300, resid Loss: 0.0648 | 0.0361
Epoch 156/300, resid Loss: 0.0648 | 0.0361
Epoch 157/300, resid Loss: 0.0648 | 0.0361
Epoch 158/300, resid Loss: 0.0648 | 0.0361
Epoch 159/300, resid Loss: 0.0648 | 0.0361
Epoch 160/300, resid Loss: 0.0648 | 0.0361
Epoch 161/300, resid Loss: 0.0648 | 0.0360
Epoch 162/300, resid Loss: 0.0647 | 0.0360
Epoch 163/300, resid Loss: 0.0647 | 0.0360
Epoch 164/300, resid Loss: 0.0647 | 0.0360
Epoch 165/300, resid Loss: 0.0647 | 0.0360
Epoch 166/300, resid Loss: 0.0647 | 0.0360
Epoch 167/300, resid Loss: 0.0647 | 0.0360
Epoch 168/300, resid Loss: 0.0647 | 0.0360
Epoch 169/300, resid Loss: 0.0647 | 0.0360
Epoch 170/300, resid Loss: 0.0647 | 0.0360
Epoch 171/300, resid Loss: 0.0647 | 0.0360
Epoch 172/300, resid Loss: 0.0647 | 0.0360
Epoch 173/300, resid Loss: 0.0647 | 0.0360
Epoch 174/300, resid Loss: 0.0647 | 0.0360
Epoch 175/300, resid Loss: 0.0647 | 0.0360
Epoch 176/300, resid Loss: 0.0647 | 0.0360
Epoch 177/300, resid Loss: 0.0647 | 0.0360
Epoch 178/300, resid Loss: 0.0647 | 0.0360
Epoch 179/300, resid Loss: 0.0647 | 0.0360
Epoch 180/300, resid Loss: 0.0647 | 0.0360
Epoch 181/300, resid Loss: 0.0646 | 0.0360
Epoch 182/300, resid Loss: 0.0646 | 0.0360
Epoch 183/300, resid Loss: 0.0646 | 0.0360
Epoch 184/300, resid Loss: 0.0646 | 0.0360
Epoch 185/300, resid Loss: 0.0646 | 0.0360
Epoch 186/300, resid Loss: 0.0646 | 0.0360
Epoch 187/300, resid Loss: 0.0646 | 0.0360
Epoch 188/300, resid Loss: 0.0646 | 0.0360
Epoch 189/300, resid Loss: 0.0646 | 0.0360
Epoch 190/300, resid Loss: 0.0646 | 0.0360
Epoch 191/300, resid Loss: 0.0646 | 0.0360
Epoch 192/300, resid Loss: 0.0646 | 0.0360
Epoch 193/300, resid Loss: 0.0646 | 0.0360
Epoch 194/300, resid Loss: 0.0646 | 0.0360
Epoch 195/300, resid Loss: 0.0646 | 0.0360
Epoch 196/300, resid Loss: 0.0646 | 0.0360
Epoch 197/300, resid Loss: 0.0646 | 0.0360
Epoch 198/300, resid Loss: 0.0646 | 0.0360
Epoch 199/300, resid Loss: 0.0646 | 0.0360
Epoch 200/300, resid Loss: 0.0646 | 0.0360
Epoch 201/300, resid Loss: 0.0646 | 0.0360
Epoch 202/300, resid Loss: 0.0646 | 0.0360
Epoch 203/300, resid Loss: 0.0646 | 0.0360
Epoch 204/300, resid Loss: 0.0646 | 0.0360
Epoch 205/300, resid Loss: 0.0646 | 0.0360
Epoch 206/300, resid Loss: 0.0646 | 0.0360
Epoch 207/300, resid Loss: 0.0646 | 0.0360
Epoch 208/300, resid Loss: 0.0646 | 0.0360
Epoch 209/300, resid Loss: 0.0646 | 0.0360
Epoch 210/300, resid Loss: 0.0646 | 0.0360
Epoch 211/300, resid Loss: 0.0646 | 0.0360
Epoch 212/300, resid Loss: 0.0646 | 0.0360
Epoch 213/300, resid Loss: 0.0646 | 0.0360
Epoch 214/300, resid Loss: 0.0646 | 0.0360
Epoch 215/300, resid Loss: 0.0646 | 0.0360
Epoch 216/300, resid Loss: 0.0646 | 0.0360
Epoch 217/300, resid Loss: 0.0646 | 0.0360
Epoch 218/300, resid Loss: 0.0646 | 0.0360
Epoch 219/300, resid Loss: 0.0646 | 0.0360
Epoch 220/300, resid Loss: 0.0646 | 0.0360
Epoch 221/300, resid Loss: 0.0646 | 0.0360
Epoch 222/300, resid Loss: 0.0646 | 0.0360
Epoch 223/300, resid Loss: 0.0646 | 0.0360
Epoch 224/300, resid Loss: 0.0646 | 0.0360
Epoch 225/300, resid Loss: 0.0646 | 0.0360
Epoch 226/300, resid Loss: 0.0646 | 0.0360
Epoch 227/300, resid Loss: 0.0646 | 0.0360
Epoch 228/300, resid Loss: 0.0646 | 0.0360
Epoch 229/300, resid Loss: 0.0646 | 0.0360
Epoch 230/300, resid Loss: 0.0646 | 0.0360
Epoch 231/300, resid Loss: 0.0646 | 0.0360
Epoch 232/300, resid Loss: 0.0646 | 0.0360
Epoch 233/300, resid Loss: 0.0646 | 0.0360
Epoch 234/300, resid Loss: 0.0646 | 0.0360
Epoch 235/300, resid Loss: 0.0646 | 0.0360
Epoch 236/300, resid Loss: 0.0646 | 0.0360
Epoch 237/300, resid Loss: 0.0646 | 0.0360
Epoch 238/300, resid Loss: 0.0646 | 0.0360
Epoch 239/300, resid Loss: 0.0646 | 0.0360
Epoch 240/300, resid Loss: 0.0646 | 0.0360
Epoch 241/300, resid Loss: 0.0646 | 0.0360
Epoch 242/300, resid Loss: 0.0646 | 0.0360
Epoch 243/300, resid Loss: 0.0646 | 0.0360
Epoch 244/300, resid Loss: 0.0646 | 0.0360
Epoch 245/300, resid Loss: 0.0646 | 0.0360
Epoch 246/300, resid Loss: 0.0646 | 0.0360
Epoch 247/300, resid Loss: 0.0646 | 0.0360
Epoch 248/300, resid Loss: 0.0646 | 0.0360
Epoch 249/300, resid Loss: 0.0646 | 0.0360
Epoch 250/300, resid Loss: 0.0646 | 0.0360
Epoch 251/300, resid Loss: 0.0646 | 0.0360
Epoch 252/300, resid Loss: 0.0646 | 0.0360
Epoch 253/300, resid Loss: 0.0646 | 0.0360
Epoch 254/300, resid Loss: 0.0646 | 0.0360
Epoch 255/300, resid Loss: 0.0646 | 0.0360
Epoch 256/300, resid Loss: 0.0646 | 0.0360
Epoch 257/300, resid Loss: 0.0646 | 0.0360
Epoch 258/300, resid Loss: 0.0646 | 0.0360
Epoch 259/300, resid Loss: 0.0646 | 0.0360
Epoch 260/300, resid Loss: 0.0646 | 0.0360
Epoch 261/300, resid Loss: 0.0646 | 0.0360
Epoch 262/300, resid Loss: 0.0646 | 0.0360
Epoch 263/300, resid Loss: 0.0646 | 0.0360
Epoch 264/300, resid Loss: 0.0646 | 0.0360
Epoch 265/300, resid Loss: 0.0646 | 0.0360
Epoch 266/300, resid Loss: 0.0646 | 0.0360
Epoch 267/300, resid Loss: 0.0646 | 0.0360
Epoch 268/300, resid Loss: 0.0646 | 0.0360
Epoch 269/300, resid Loss: 0.0646 | 0.0360
Epoch 270/300, resid Loss: 0.0646 | 0.0360
Epoch 271/300, resid Loss: 0.0646 | 0.0360
Epoch 272/300, resid Loss: 0.0646 | 0.0360
Epoch 273/300, resid Loss: 0.0646 | 0.0360
Epoch 274/300, resid Loss: 0.0646 | 0.0360
Early stopping for resid
Runtime (seconds): 9272.766054391861
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:690: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_trend_stock_price = predicted_trend[1][0, :, 0].cpu().numpy().flatten() * std_lists['trend'][0] + mean_lists['trend'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:691: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_0_stock_price = predicted_seasonal_0[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_0'][0] + mean_lists['seasonal_0'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:692: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_1_stock_price = predicted_seasonal_1[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_1'][0] + mean_lists['seasonal_1'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:693: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_2_stock_price = predicted_seasonal_2[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_2'][0] + mean_lists['seasonal_2'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:694: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_seasonal_3_stock_price = predicted_seasonal_3[1][0, :, 0].cpu().numpy().flatten() * std_lists['seasonal_3'][0] + mean_lists['seasonal_3'][0]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:695: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  predicted_resid_stock_price = predicted_resid[1][0, :, 0].cpu().numpy().flatten() * std_lists['resid'][0] + mean_lists['resid'][0]
[216.37787]
[1.0890107]
[1.396986]
[4.002683]
[0.2980079]
[7.188978]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 0.23380683758296072
RMSE: 0.4835357666015625
MAE: 0.4835357666015625
R-squared: nan
[230.35353]
/data/student/k2110261/Multi-iTransformer/roop_optuna_change.py:737: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`
  "real_stock_price": close_data[-1],
