[32m[I 2025-02-08 00:18:38,878][0m A new study created in memory with name: no-name-2eac1ed6-da74-48f9-aba4-49b284413326[0m
[32m[I 2025-02-08 00:19:09,181][0m Trial 0 finished with value: 0.3921491073911983 and parameters: {'observation_period_num': 154, 'train_rates': 0.7326251292848995, 'learning_rate': 3.1446436103845104e-05, 'batch_size': 181, 'step_size': 13, 'gamma': 0.7934756686986935}. Best is trial 0 with value: 0.3921491073911983.[0m
[32m[I 2025-02-08 00:19:37,110][0m Trial 1 finished with value: 0.3929588371414723 and parameters: {'observation_period_num': 103, 'train_rates': 0.6318377434700883, 'learning_rate': 1.890852950193962e-05, 'batch_size': 167, 'step_size': 1, 'gamma': 0.9709671744919868}. Best is trial 0 with value: 0.3921491073911983.[0m
[32m[I 2025-02-08 00:22:07,891][0m Trial 2 finished with value: 0.15289973865992135 and parameters: {'observation_period_num': 37, 'train_rates': 0.6209997641486282, 'learning_rate': 0.0002692846694656926, 'batch_size': 29, 'step_size': 15, 'gamma': 0.8253059945930418}. Best is trial 2 with value: 0.15289973865992135.[0m
[32m[I 2025-02-08 00:22:37,778][0m Trial 3 finished with value: 0.35885152220726013 and parameters: {'observation_period_num': 108, 'train_rates': 0.9781052987298324, 'learning_rate': 1.564403219478251e-05, 'batch_size': 218, 'step_size': 13, 'gamma': 0.9537222081401149}. Best is trial 2 with value: 0.15289973865992135.[0m
[32m[I 2025-02-08 00:23:02,408][0m Trial 4 finished with value: 0.24412669879668075 and parameters: {'observation_period_num': 159, 'train_rates': 0.803860362537328, 'learning_rate': 2.613035039279569e-05, 'batch_size': 235, 'step_size': 7, 'gamma': 0.9370873689283444}. Best is trial 2 with value: 0.15289973865992135.[0m
[32m[I 2025-02-08 00:23:59,113][0m Trial 5 finished with value: 0.1528828740119934 and parameters: {'observation_period_num': 79, 'train_rates': 0.9269208734054817, 'learning_rate': 0.0008068135949681471, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9451419180407112}. Best is trial 5 with value: 0.1528828740119934.[0m
[32m[I 2025-02-08 00:28:30,307][0m Trial 6 finished with value: 0.165377764031291 and parameters: {'observation_period_num': 57, 'train_rates': 0.8066310764398539, 'learning_rate': 6.358743884604383e-06, 'batch_size': 19, 'step_size': 5, 'gamma': 0.8364478865375882}. Best is trial 5 with value: 0.1528828740119934.[0m
[32m[I 2025-02-08 00:28:53,692][0m Trial 7 finished with value: 0.3272562451535763 and parameters: {'observation_period_num': 222, 'train_rates': 0.8713278425099495, 'learning_rate': 1.9908043519131383e-05, 'batch_size': 251, 'step_size': 8, 'gamma': 0.9548545149092542}. Best is trial 5 with value: 0.1528828740119934.[0m
Early stopping at epoch 54
[32m[I 2025-02-08 00:29:07,600][0m Trial 8 finished with value: 0.29596961090160956 and parameters: {'observation_period_num': 6, 'train_rates': 0.7932397152388646, 'learning_rate': 9.212119072844028e-05, 'batch_size': 251, 'step_size': 1, 'gamma': 0.786349493027427}. Best is trial 5 with value: 0.1528828740119934.[0m
[32m[I 2025-02-08 00:30:40,470][0m Trial 9 finished with value: 0.1412500962615013 and parameters: {'observation_period_num': 211, 'train_rates': 0.8754498306068992, 'learning_rate': 0.0002050175162342482, 'batch_size': 58, 'step_size': 3, 'gamma': 0.8059638806364859}. Best is trial 9 with value: 0.1412500962615013.[0m
[32m[I 2025-02-08 00:31:45,543][0m Trial 10 finished with value: 0.22192795165613585 and parameters: {'observation_period_num': 246, 'train_rates': 0.8889682125644351, 'learning_rate': 0.0001584176128624847, 'batch_size': 83, 'step_size': 4, 'gamma': 0.7547875402520114}. Best is trial 9 with value: 0.1412500962615013.[0m
[32m[I 2025-02-08 00:32:45,465][0m Trial 11 finished with value: 0.3257720470428467 and parameters: {'observation_period_num': 193, 'train_rates': 0.9773110940419598, 'learning_rate': 0.0008867165702023132, 'batch_size': 98, 'step_size': 10, 'gamma': 0.8968001840011723}. Best is trial 9 with value: 0.1412500962615013.[0m
[32m[I 2025-02-08 00:33:53,397][0m Trial 12 finished with value: 0.11189847855598896 and parameters: {'observation_period_num': 77, 'train_rates': 0.9211927760249192, 'learning_rate': 0.0008389784720054945, 'batch_size': 86, 'step_size': 4, 'gamma': 0.8875840365053502}. Best is trial 12 with value: 0.11189847855598896.[0m
[32m[I 2025-02-08 00:35:23,759][0m Trial 13 finished with value: 0.8310656618263762 and parameters: {'observation_period_num': 146, 'train_rates': 0.8762579165772565, 'learning_rate': 1.1797085638211181e-06, 'batch_size': 60, 'step_size': 4, 'gamma': 0.8863498609098538}. Best is trial 12 with value: 0.11189847855598896.[0m
[32m[I 2025-02-08 00:36:05,739][0m Trial 14 finished with value: 0.17747172752152318 and parameters: {'observation_period_num': 193, 'train_rates': 0.9183096398069654, 'learning_rate': 0.0003375762272279292, 'batch_size': 136, 'step_size': 3, 'gamma': 0.8542316636835879}. Best is trial 12 with value: 0.11189847855598896.[0m
[32m[I 2025-02-08 00:37:36,292][0m Trial 15 finished with value: 0.09250605528333546 and parameters: {'observation_period_num': 103, 'train_rates': 0.8440202868019536, 'learning_rate': 8.74802330845407e-05, 'batch_size': 61, 'step_size': 6, 'gamma': 0.9068280839013356}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:38:14,395][0m Trial 16 finished with value: 0.2168935230281204 and parameters: {'observation_period_num': 83, 'train_rates': 0.7259532108454745, 'learning_rate': 6.574561939024175e-05, 'batch_size': 133, 'step_size': 6, 'gamma': 0.9097393403627715}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:39:46,024][0m Trial 17 finished with value: 0.36777269661672846 and parameters: {'observation_period_num': 119, 'train_rates': 0.832325565418157, 'learning_rate': 3.7048969999827184e-06, 'batch_size': 58, 'step_size': 10, 'gamma': 0.9185303720493578}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:40:28,249][0m Trial 18 finished with value: 0.18310681768820314 and parameters: {'observation_period_num': 35, 'train_rates': 0.7510742804431065, 'learning_rate': 6.6641591531346e-05, 'batch_size': 124, 'step_size': 9, 'gamma': 0.8665051915405498}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:41:42,433][0m Trial 19 finished with value: 0.1445283873213662 and parameters: {'observation_period_num': 74, 'train_rates': 0.9386691264881817, 'learning_rate': 0.0005201868344608279, 'batch_size': 80, 'step_size': 6, 'gamma': 0.8799288003736129}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:42:13,838][0m Trial 20 finished with value: 0.3336488286742067 and parameters: {'observation_period_num': 133, 'train_rates': 0.684771497694232, 'learning_rate': 0.00011125937006358252, 'batch_size': 162, 'step_size': 2, 'gamma': 0.9866527905115229}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:44:03,549][0m Trial 21 finished with value: 0.1433290060904386 and parameters: {'observation_period_num': 180, 'train_rates': 0.8523795062796837, 'learning_rate': 0.00025747024000970453, 'batch_size': 48, 'step_size': 3, 'gamma': 0.8208653831127258}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:46:26,932][0m Trial 22 finished with value: 0.1625812171299136 and parameters: {'observation_period_num': 93, 'train_rates': 0.9082235399977941, 'learning_rate': 0.00045202725564114126, 'batch_size': 39, 'step_size': 5, 'gamma': 0.857197235856061}. Best is trial 15 with value: 0.09250605528333546.[0m
[32m[I 2025-02-08 00:47:36,167][0m Trial 23 finished with value: 0.05559042305441522 and parameters: {'observation_period_num': 57, 'train_rates': 0.8381413187944499, 'learning_rate': 0.0001725811062387693, 'batch_size': 80, 'step_size': 3, 'gamma': 0.9181934615983396}. Best is trial 23 with value: 0.05559042305441522.[0m
[32m[I 2025-02-08 00:48:45,425][0m Trial 24 finished with value: 0.19157857417819962 and parameters: {'observation_period_num': 48, 'train_rates': 0.7692366398136123, 'learning_rate': 5.2814131511079614e-05, 'batch_size': 77, 'step_size': 6, 'gamma': 0.9265370313211617}. Best is trial 23 with value: 0.05559042305441522.[0m
[32m[I 2025-02-08 00:49:39,165][0m Trial 25 finished with value: 0.049146460819672916 and parameters: {'observation_period_num': 25, 'train_rates': 0.846002673767747, 'learning_rate': 0.0001371602299689159, 'batch_size': 105, 'step_size': 4, 'gamma': 0.9012912731898357}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:50:29,441][0m Trial 26 finished with value: 0.05358170290788015 and parameters: {'observation_period_num': 11, 'train_rates': 0.8246417116443983, 'learning_rate': 0.00014324756671607285, 'batch_size': 113, 'step_size': 2, 'gamma': 0.9044569414496405}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:51:18,841][0m Trial 27 finished with value: 0.0674608941101416 and parameters: {'observation_period_num': 9, 'train_rates': 0.8242859869083343, 'learning_rate': 0.0001505171127675685, 'batch_size': 113, 'step_size': 1, 'gamma': 0.9255154720705622}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:51:54,292][0m Trial 28 finished with value: 0.21924557285522348 and parameters: {'observation_period_num': 22, 'train_rates': 0.7750266640030622, 'learning_rate': 0.00013808879669991185, 'batch_size': 154, 'step_size': 2, 'gamma': 0.8718565032257416}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:52:30,778][0m Trial 29 finished with value: 0.33938353985171643 and parameters: {'observation_period_num': 58, 'train_rates': 0.7040631491235292, 'learning_rate': 3.390220865332174e-05, 'batch_size': 145, 'step_size': 2, 'gamma': 0.9025764153018904}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:53:02,680][0m Trial 30 finished with value: 0.10572166780987129 and parameters: {'observation_period_num': 37, 'train_rates': 0.8496995026382902, 'learning_rate': 4.4050627737745214e-05, 'batch_size': 188, 'step_size': 3, 'gamma': 0.9653986859103428}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:53:52,129][0m Trial 31 finished with value: 0.0636098550340689 and parameters: {'observation_period_num': 7, 'train_rates': 0.8211012068117294, 'learning_rate': 0.00016405262373552824, 'batch_size': 113, 'step_size': 1, 'gamma': 0.9274145075902265}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:54:39,419][0m Trial 32 finished with value: 0.05824274827706085 and parameters: {'observation_period_num': 22, 'train_rates': 0.8135479037465141, 'learning_rate': 0.00045690043625906066, 'batch_size': 121, 'step_size': 1, 'gamma': 0.9314642674963232}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:55:09,723][0m Trial 33 finished with value: 0.18468907568463705 and parameters: {'observation_period_num': 26, 'train_rates': 0.7765096288067341, 'learning_rate': 0.0004789941392041707, 'batch_size': 192, 'step_size': 2, 'gamma': 0.9370877616491045}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:56:02,016][0m Trial 34 finished with value: 0.21789795958641892 and parameters: {'observation_period_num': 56, 'train_rates': 0.748491367767947, 'learning_rate': 0.00026978301695940843, 'batch_size': 97, 'step_size': 1, 'gamma': 0.91302813200847}. Best is trial 25 with value: 0.049146460819672916.[0m
[32m[I 2025-02-08 00:56:45,406][0m Trial 35 finished with value: 0.04082352744047857 and parameters: {'observation_period_num': 22, 'train_rates': 0.799446066564673, 'learning_rate': 0.00045196509222162186, 'batch_size': 128, 'step_size': 4, 'gamma': 0.895750574084817}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 00:57:14,635][0m Trial 36 finished with value: 0.18768073395785106 and parameters: {'observation_period_num': 47, 'train_rates': 0.6603123825935276, 'learning_rate': 0.0003082351392933113, 'batch_size': 171, 'step_size': 5, 'gamma': 0.8925967033294796}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 00:58:09,300][0m Trial 37 finished with value: 0.10606045017834582 and parameters: {'observation_period_num': 22, 'train_rates': 0.7947213655797372, 'learning_rate': 1.048641004692211e-05, 'batch_size': 100, 'step_size': 13, 'gamma': 0.8471330907715692}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 00:58:42,371][0m Trial 38 finished with value: 0.15942183452254677 and parameters: {'observation_period_num': 65, 'train_rates': 0.6008257259315103, 'learning_rate': 0.000220951912165002, 'batch_size': 136, 'step_size': 4, 'gamma': 0.9510467562912696}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:00:02,403][0m Trial 39 finished with value: 0.13179564745311279 and parameters: {'observation_period_num': 35, 'train_rates': 0.8602909303028586, 'learning_rate': 0.0006357171981147558, 'batch_size': 70, 'step_size': 7, 'gamma': 0.9012822075757756}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:00:32,301][0m Trial 40 finished with value: 0.053384143150104854 and parameters: {'observation_period_num': 14, 'train_rates': 0.8894330513954454, 'learning_rate': 0.0003606611716870953, 'batch_size': 208, 'step_size': 14, 'gamma': 0.8648869942018679}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:01:03,496][0m Trial 41 finished with value: 0.04114852100610733 and parameters: {'observation_period_num': 14, 'train_rates': 0.950693170847773, 'learning_rate': 0.00031004696710580735, 'batch_size': 210, 'step_size': 15, 'gamma': 0.8727245453412914}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:01:34,603][0m Trial 42 finished with value: 0.04085221141576767 and parameters: {'observation_period_num': 15, 'train_rates': 0.9639956632804649, 'learning_rate': 0.0006710732760578386, 'batch_size': 207, 'step_size': 14, 'gamma': 0.8752229417174237}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:02:05,916][0m Trial 43 finished with value: 0.0821707546710968 and parameters: {'observation_period_num': 41, 'train_rates': 0.9540645653722457, 'learning_rate': 0.00038561552431339155, 'batch_size': 216, 'step_size': 15, 'gamma': 0.8727125789239529}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:02:37,046][0m Trial 44 finished with value: 0.05000152811408043 and parameters: {'observation_period_num': 19, 'train_rates': 0.9568469563259746, 'learning_rate': 0.0006517990393009619, 'batch_size': 210, 'step_size': 14, 'gamma': 0.8418177306411401}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:03:06,625][0m Trial 45 finished with value: 0.06132334843277931 and parameters: {'observation_period_num': 28, 'train_rates': 0.9814720556780505, 'learning_rate': 0.0006048611843368799, 'batch_size': 229, 'step_size': 12, 'gamma': 0.8370669172839359}. Best is trial 35 with value: 0.04082352744047857.[0m
[32m[I 2025-02-08 01:03:39,762][0m Trial 46 finished with value: 0.038709841668605804 and parameters: {'observation_period_num': 17, 'train_rates': 0.9554526630074924, 'learning_rate': 0.0009867570731698454, 'batch_size': 203, 'step_size': 14, 'gamma': 0.8184711777892045}. Best is trial 46 with value: 0.038709841668605804.[0m
[32m[I 2025-02-08 01:04:06,905][0m Trial 47 finished with value: 0.08307645469903946 and parameters: {'observation_period_num': 48, 'train_rates': 0.9620806385477161, 'learning_rate': 0.0007766504100626604, 'batch_size': 237, 'step_size': 12, 'gamma': 0.7630350569341798}. Best is trial 46 with value: 0.038709841668605804.[0m
[32m[I 2025-02-08 01:04:40,551][0m Trial 48 finished with value: 0.04385859692598333 and parameters: {'observation_period_num': 31, 'train_rates': 0.898060535520053, 'learning_rate': 0.0008884198447336241, 'batch_size': 179, 'step_size': 14, 'gamma': 0.8117610844324037}. Best is trial 46 with value: 0.038709841668605804.[0m
[32m[I 2025-02-08 01:05:13,344][0m Trial 49 finished with value: 0.038140326738357544 and parameters: {'observation_period_num': 5, 'train_rates': 0.9383472611010683, 'learning_rate': 0.0008841488372323358, 'batch_size': 198, 'step_size': 14, 'gamma': 0.794857656444935}. Best is trial 49 with value: 0.038140326738357544.[0m
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Å™„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer_noMSTL.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Epoch 1/300, Loss: 0.5664 | 0.3221
Epoch 2/300, Loss: 0.2204 | 0.2075
Epoch 3/300, Loss: 0.2082 | 0.1451
Epoch 4/300, Loss: 0.2143 | 0.5526
Epoch 5/300, Loss: 0.1613 | 0.2590
Epoch 6/300, Loss: 0.1456 | 0.1663
Epoch 7/300, Loss: 0.1669 | 0.1556
Epoch 8/300, Loss: 0.1536 | 0.1729
Epoch 9/300, Loss: 0.1568 | 0.2268
Epoch 10/300, Loss: 0.1366 | 0.1070
Epoch 11/300, Loss: 0.1193 | 0.1524
Epoch 12/300, Loss: 0.1264 | 0.1072
Epoch 13/300, Loss: 0.1303 | 0.1525
Epoch 14/300, Loss: 0.1226 | 0.0985
Epoch 15/300, Loss: 0.1302 | 0.1285
Epoch 16/300, Loss: 0.1304 | 0.1127
Epoch 17/300, Loss: 0.1497 | 0.1760
Epoch 18/300, Loss: 0.1673 | 0.1177
Epoch 19/300, Loss: 0.1369 | 0.1371
Epoch 20/300, Loss: 0.1411 | 0.1684
Epoch 21/300, Loss: 0.1327 | 0.0954
Epoch 22/300, Loss: 0.1155 | 0.0773
Epoch 23/300, Loss: 0.1073 | 0.0766
Epoch 24/300, Loss: 0.1061 | 0.0718
Epoch 25/300, Loss: 0.1053 | 0.0760
Epoch 26/300, Loss: 0.1095 | 0.0891
Epoch 27/300, Loss: 0.1133 | 0.0731
Epoch 28/300, Loss: 0.1167 | 0.0927
Epoch 29/300, Loss: 0.1101 | 0.0681
Epoch 30/300, Loss: 0.1048 | 0.0872
Epoch 31/300, Loss: 0.1001 | 0.0618
Epoch 32/300, Loss: 0.0966 | 0.0794
Epoch 33/300, Loss: 0.0954 | 0.0597
Epoch 34/300, Loss: 0.0939 | 0.0702
Epoch 35/300, Loss: 0.0914 | 0.0580
Epoch 36/300, Loss: 0.0883 | 0.0625
Epoch 37/300, Loss: 0.0866 | 0.0537
Epoch 38/300, Loss: 0.0842 | 0.0575
Epoch 39/300, Loss: 0.0831 | 0.0517
Epoch 40/300, Loss: 0.0820 | 0.0532
Epoch 41/300, Loss: 0.0813 | 0.0503
Epoch 42/300, Loss: 0.0801 | 0.0514
Epoch 43/300, Loss: 0.0796 | 0.0487
Epoch 44/300, Loss: 0.0786 | 0.0498
Epoch 45/300, Loss: 0.0783 | 0.0478
Epoch 46/300, Loss: 0.0776 | 0.0482
Epoch 47/300, Loss: 0.0773 | 0.0469
Epoch 48/300, Loss: 0.0768 | 0.0471
Epoch 49/300, Loss: 0.0765 | 0.0461
Epoch 50/300, Loss: 0.0761 | 0.0463
Epoch 51/300, Loss: 0.0758 | 0.0453
Epoch 52/300, Loss: 0.0754 | 0.0455
Epoch 53/300, Loss: 0.0751 | 0.0447
Epoch 54/300, Loss: 0.0747 | 0.0447
Epoch 55/300, Loss: 0.0745 | 0.0440
Epoch 56/300, Loss: 0.0742 | 0.0441
Epoch 57/300, Loss: 0.0740 | 0.0432
Epoch 58/300, Loss: 0.0737 | 0.0431
Epoch 59/300, Loss: 0.0735 | 0.0426
Epoch 60/300, Loss: 0.0733 | 0.0426
Epoch 61/300, Loss: 0.0733 | 0.0424
Epoch 62/300, Loss: 0.0732 | 0.0423
Epoch 63/300, Loss: 0.0730 | 0.0420
Epoch 64/300, Loss: 0.0727 | 0.0420
Epoch 65/300, Loss: 0.0724 | 0.0419
Epoch 66/300, Loss: 0.0720 | 0.0417
Epoch 67/300, Loss: 0.0717 | 0.0415
Epoch 68/300, Loss: 0.0716 | 0.0413
Epoch 69/300, Loss: 0.0714 | 0.0412
Epoch 70/300, Loss: 0.0713 | 0.0411
Epoch 71/300, Loss: 0.0712 | 0.0412
Epoch 72/300, Loss: 0.0714 | 0.0412
Epoch 73/300, Loss: 0.0715 | 0.0410
Epoch 74/300, Loss: 0.0715 | 0.0407
Epoch 75/300, Loss: 0.0713 | 0.0406
Epoch 76/300, Loss: 0.0712 | 0.0408
Epoch 77/300, Loss: 0.0714 | 0.0414
Epoch 78/300, Loss: 0.0724 | 0.0407
Epoch 79/300, Loss: 0.0721 | 0.0413
Epoch 80/300, Loss: 0.0707 | 0.0414
Epoch 81/300, Loss: 0.0701 | 0.0407
Epoch 82/300, Loss: 0.0698 | 0.0403
Epoch 83/300, Loss: 0.0696 | 0.0402
Epoch 84/300, Loss: 0.0695 | 0.0401
Epoch 85/300, Loss: 0.0694 | 0.0400
Epoch 86/300, Loss: 0.0693 | 0.0400
Epoch 87/300, Loss: 0.0692 | 0.0399
Epoch 88/300, Loss: 0.0691 | 0.0398
Epoch 89/300, Loss: 0.0690 | 0.0397
Epoch 90/300, Loss: 0.0689 | 0.0397
Epoch 91/300, Loss: 0.0688 | 0.0396
Epoch 92/300, Loss: 0.0687 | 0.0395
Epoch 93/300, Loss: 0.0687 | 0.0395
Epoch 94/300, Loss: 0.0686 | 0.0394
Epoch 95/300, Loss: 0.0685 | 0.0394
Epoch 96/300, Loss: 0.0684 | 0.0393
Epoch 97/300, Loss: 0.0683 | 0.0393
Epoch 98/300, Loss: 0.0683 | 0.0392
Epoch 99/300, Loss: 0.0682 | 0.0391
Epoch 100/300, Loss: 0.0682 | 0.0391
Epoch 101/300, Loss: 0.0681 | 0.0391
Epoch 102/300, Loss: 0.0680 | 0.0390
Epoch 103/300, Loss: 0.0680 | 0.0390
Epoch 104/300, Loss: 0.0679 | 0.0389
Epoch 105/300, Loss: 0.0679 | 0.0389
Epoch 106/300, Loss: 0.0678 | 0.0388
Epoch 107/300, Loss: 0.0678 | 0.0388
Epoch 108/300, Loss: 0.0677 | 0.0388
Epoch 109/300, Loss: 0.0676 | 0.0388
Epoch 110/300, Loss: 0.0676 | 0.0387
Epoch 111/300, Loss: 0.0676 | 0.0387
Epoch 112/300, Loss: 0.0675 | 0.0386
Epoch 113/300, Loss: 0.0675 | 0.0386
Epoch 114/300, Loss: 0.0674 | 0.0386
Epoch 115/300, Loss: 0.0674 | 0.0386
Epoch 116/300, Loss: 0.0673 | 0.0385
Epoch 117/300, Loss: 0.0673 | 0.0385
Epoch 118/300, Loss: 0.0673 | 0.0385
Epoch 119/300, Loss: 0.0672 | 0.0384
Epoch 120/300, Loss: 0.0672 | 0.0384
Epoch 121/300, Loss: 0.0671 | 0.0384
Epoch 122/300, Loss: 0.0671 | 0.0384
Epoch 123/300, Loss: 0.0671 | 0.0383
Epoch 124/300, Loss: 0.0670 | 0.0383
Epoch 125/300, Loss: 0.0670 | 0.0383
Epoch 126/300, Loss: 0.0670 | 0.0383
Epoch 127/300, Loss: 0.0669 | 0.0383
Epoch 128/300, Loss: 0.0669 | 0.0382
Epoch 129/300, Loss: 0.0669 | 0.0382
Epoch 130/300, Loss: 0.0669 | 0.0382
Epoch 131/300, Loss: 0.0668 | 0.0382
Epoch 132/300, Loss: 0.0668 | 0.0381
Epoch 133/300, Loss: 0.0668 | 0.0381
Epoch 134/300, Loss: 0.0667 | 0.0381
Epoch 135/300, Loss: 0.0667 | 0.0381
Epoch 136/300, Loss: 0.0667 | 0.0381
Epoch 137/300, Loss: 0.0667 | 0.0381
Epoch 138/300, Loss: 0.0667 | 0.0380
Epoch 139/300, Loss: 0.0666 | 0.0380
Epoch 140/300, Loss: 0.0666 | 0.0380
Epoch 141/300, Loss: 0.0666 | 0.0380
Epoch 142/300, Loss: 0.0666 | 0.0380
Epoch 143/300, Loss: 0.0665 | 0.0380
Epoch 144/300, Loss: 0.0665 | 0.0379
Epoch 145/300, Loss: 0.0665 | 0.0379
Epoch 146/300, Loss: 0.0665 | 0.0379
Epoch 147/300, Loss: 0.0665 | 0.0379
Epoch 148/300, Loss: 0.0664 | 0.0379
Epoch 149/300, Loss: 0.0664 | 0.0379
Epoch 150/300, Loss: 0.0664 | 0.0379
Epoch 151/300, Loss: 0.0664 | 0.0378
Epoch 152/300, Loss: 0.0664 | 0.0378
Epoch 153/300, Loss: 0.0664 | 0.0378
Epoch 154/300, Loss: 0.0664 | 0.0378
Epoch 155/300, Loss: 0.0663 | 0.0378
Epoch 156/300, Loss: 0.0663 | 0.0378
Epoch 157/300, Loss: 0.0663 | 0.0378
Epoch 158/300, Loss: 0.0663 | 0.0378
Epoch 159/300, Loss: 0.0663 | 0.0378
Epoch 160/300, Loss: 0.0663 | 0.0377
Epoch 161/300, Loss: 0.0663 | 0.0377
Epoch 162/300, Loss: 0.0662 | 0.0377
Epoch 163/300, Loss: 0.0662 | 0.0377
Epoch 164/300, Loss: 0.0662 | 0.0377
Epoch 165/300, Loss: 0.0662 | 0.0377
Epoch 166/300, Loss: 0.0662 | 0.0377
Epoch 167/300, Loss: 0.0662 | 0.0377
Epoch 168/300, Loss: 0.0662 | 0.0377
Epoch 169/300, Loss: 0.0662 | 0.0377
Epoch 170/300, Loss: 0.0662 | 0.0376
Epoch 171/300, Loss: 0.0661 | 0.0376
Epoch 172/300, Loss: 0.0661 | 0.0376
Epoch 173/300, Loss: 0.0661 | 0.0376
Epoch 174/300, Loss: 0.0661 | 0.0376
Epoch 175/300, Loss: 0.0661 | 0.0376
Epoch 176/300, Loss: 0.0661 | 0.0376
Epoch 177/300, Loss: 0.0661 | 0.0376
Epoch 178/300, Loss: 0.0661 | 0.0376
Epoch 179/300, Loss: 0.0661 | 0.0376
Epoch 180/300, Loss: 0.0661 | 0.0376
Epoch 181/300, Loss: 0.0661 | 0.0376
Epoch 182/300, Loss: 0.0661 | 0.0376
Epoch 183/300, Loss: 0.0661 | 0.0376
Epoch 184/300, Loss: 0.0660 | 0.0375
Epoch 185/300, Loss: 0.0660 | 0.0375
Epoch 186/300, Loss: 0.0660 | 0.0375
Epoch 187/300, Loss: 0.0660 | 0.0375
Epoch 188/300, Loss: 0.0660 | 0.0375
Epoch 189/300, Loss: 0.0660 | 0.0375
Epoch 190/300, Loss: 0.0660 | 0.0375
Epoch 191/300, Loss: 0.0660 | 0.0375
Epoch 192/300, Loss: 0.0660 | 0.0375
Epoch 193/300, Loss: 0.0660 | 0.0375
Epoch 194/300, Loss: 0.0660 | 0.0375
Epoch 195/300, Loss: 0.0660 | 0.0375
Epoch 196/300, Loss: 0.0660 | 0.0375
Epoch 197/300, Loss: 0.0660 | 0.0375
Epoch 198/300, Loss: 0.0660 | 0.0375
Epoch 199/300, Loss: 0.0660 | 0.0375
Epoch 200/300, Loss: 0.0660 | 0.0375
Epoch 201/300, Loss: 0.0660 | 0.0375
Epoch 202/300, Loss: 0.0660 | 0.0375
Epoch 203/300, Loss: 0.0660 | 0.0375
Epoch 204/300, Loss: 0.0660 | 0.0375
Epoch 205/300, Loss: 0.0659 | 0.0374
Epoch 206/300, Loss: 0.0659 | 0.0374
Epoch 207/300, Loss: 0.0659 | 0.0374
Epoch 208/300, Loss: 0.0659 | 0.0374
Epoch 209/300, Loss: 0.0659 | 0.0374
Epoch 210/300, Loss: 0.0659 | 0.0374
Epoch 211/300, Loss: 0.0659 | 0.0374
Epoch 212/300, Loss: 0.0659 | 0.0374
Epoch 213/300, Loss: 0.0659 | 0.0374
Epoch 214/300, Loss: 0.0659 | 0.0374
Epoch 215/300, Loss: 0.0659 | 0.0374
Epoch 216/300, Loss: 0.0659 | 0.0374
Epoch 217/300, Loss: 0.0659 | 0.0374
Epoch 218/300, Loss: 0.0659 | 0.0374
Epoch 219/300, Loss: 0.0659 | 0.0374
Epoch 220/300, Loss: 0.0659 | 0.0374
Epoch 221/300, Loss: 0.0659 | 0.0374
Epoch 222/300, Loss: 0.0659 | 0.0374
Epoch 223/300, Loss: 0.0659 | 0.0374
Epoch 224/300, Loss: 0.0659 | 0.0374
Epoch 225/300, Loss: 0.0659 | 0.0374
Epoch 226/300, Loss: 0.0659 | 0.0374
Epoch 227/300, Loss: 0.0659 | 0.0374
Epoch 228/300, Loss: 0.0659 | 0.0374
Epoch 229/300, Loss: 0.0659 | 0.0374
Epoch 230/300, Loss: 0.0659 | 0.0374
Epoch 231/300, Loss: 0.0659 | 0.0374
Epoch 232/300, Loss: 0.0659 | 0.0374
Epoch 233/300, Loss: 0.0659 | 0.0374
Epoch 234/300, Loss: 0.0659 | 0.0374
Epoch 235/300, Loss: 0.0659 | 0.0374
Epoch 236/300, Loss: 0.0659 | 0.0374
Epoch 237/300, Loss: 0.0659 | 0.0374
Epoch 238/300, Loss: 0.0659 | 0.0374
Epoch 239/300, Loss: 0.0659 | 0.0374
Epoch 240/300, Loss: 0.0659 | 0.0374
Epoch 241/300, Loss: 0.0659 | 0.0374
Epoch 242/300, Loss: 0.0659 | 0.0374
Epoch 243/300, Loss: 0.0659 | 0.0374
Epoch 244/300, Loss: 0.0659 | 0.0374
Epoch 245/300, Loss: 0.0659 | 0.0374
Epoch 246/300, Loss: 0.0659 | 0.0374
Epoch 247/300, Loss: 0.0659 | 0.0374
Epoch 248/300, Loss: 0.0659 | 0.0374
Epoch 249/300, Loss: 0.0659 | 0.0374
Epoch 250/300, Loss: 0.0659 | 0.0374
Epoch 251/300, Loss: 0.0659 | 0.0374
Epoch 252/300, Loss: 0.0659 | 0.0374
Epoch 253/300, Loss: 0.0659 | 0.0374
Epoch 254/300, Loss: 0.0659 | 0.0374
Epoch 255/300, Loss: 0.0659 | 0.0374
Epoch 256/300, Loss: 0.0659 | 0.0374
Epoch 257/300, Loss: 0.0659 | 0.0374
Epoch 258/300, Loss: 0.0659 | 0.0374
Epoch 259/300, Loss: 0.0659 | 0.0374
Epoch 260/300, Loss: 0.0659 | 0.0374
Epoch 261/300, Loss: 0.0659 | 0.0374
Epoch 262/300, Loss: 0.0659 | 0.0374
Epoch 263/300, Loss: 0.0659 | 0.0374
Epoch 264/300, Loss: 0.0659 | 0.0374
Epoch 265/300, Loss: 0.0659 | 0.0374
Epoch 266/300, Loss: 0.0659 | 0.0374
Epoch 267/300, Loss: 0.0659 | 0.0374
Epoch 268/300, Loss: 0.0659 | 0.0374
Epoch 269/300, Loss: 0.0659 | 0.0374
Epoch 270/300, Loss: 0.0659 | 0.0374
Epoch 271/300, Loss: 0.0659 | 0.0374
Epoch 272/300, Loss: 0.0659 | 0.0373
Epoch 273/300, Loss: 0.0659 | 0.0373
Epoch 274/300, Loss: 0.0659 | 0.0373
Epoch 275/300, Loss: 0.0659 | 0.0373
Epoch 276/300, Loss: 0.0659 | 0.0373
Epoch 277/300, Loss: 0.0659 | 0.0373
Epoch 278/300, Loss: 0.0658 | 0.0373
Epoch 279/300, Loss: 0.0658 | 0.0373
Epoch 280/300, Loss: 0.0658 | 0.0373
Epoch 281/300, Loss: 0.0658 | 0.0373
Epoch 282/300, Loss: 0.0658 | 0.0373
Epoch 283/300, Loss: 0.0658 | 0.0373
Epoch 284/300, Loss: 0.0658 | 0.0373
Epoch 285/300, Loss: 0.0658 | 0.0373
Epoch 286/300, Loss: 0.0658 | 0.0373
Epoch 287/300, Loss: 0.0658 | 0.0373
Epoch 288/300, Loss: 0.0658 | 0.0373
Epoch 289/300, Loss: 0.0658 | 0.0373
Epoch 290/300, Loss: 0.0658 | 0.0373
Epoch 291/300, Loss: 0.0658 | 0.0373
Epoch 292/300, Loss: 0.0658 | 0.0373
Epoch 293/300, Loss: 0.0658 | 0.0373
Epoch 294/300, Loss: 0.0658 | 0.0373
Epoch 295/300, Loss: 0.0658 | 0.0373
Epoch 296/300, Loss: 0.0658 | 0.0373
Epoch 297/300, Loss: 0.0658 | 0.0373
Epoch 298/300, Loss: 0.0658 | 0.0373
Epoch 299/300, Loss: 0.0658 | 0.0373
Epoch 300/300, Loss: 0.0658 | 0.0373
Runtime (seconds): 96.58867526054382
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 214.25453010969795
RMSE: 14.637435913085938
MAE: 14.637435913085938
R-squared: nan
[203.24744]
