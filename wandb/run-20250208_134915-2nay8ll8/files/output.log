ÊúÄÈÅ©ÂåñÂØæË±°: trend
[32m[I 2025-02-08 13:49:20,887][0m A new study created in memory with name: no-name-91cb89ca-e256-4ddf-a06c-7ee6976a4040[0m
[32m[I 2025-02-08 13:50:04,058][0m Trial 0 finished with value: 0.47713957135246104 and parameters: {'observation_period_num': 103, 'train_rates': 0.8320791852347578, 'learning_rate': 3.2571616548766875e-06, 'batch_size': 129, 'step_size': 15, 'gamma': 0.8088217717464722}. Best is trial 0 with value: 0.47713957135246104.[0m
[32m[I 2025-02-08 13:51:03,396][0m Trial 1 finished with value: 0.673440412474357 and parameters: {'observation_period_num': 145, 'train_rates': 0.6507881175582427, 'learning_rate': 5.277864820731061e-05, 'batch_size': 76, 'step_size': 11, 'gamma': 0.8216989351872698}. Best is trial 0 with value: 0.47713957135246104.[0m
[32m[I 2025-02-08 13:51:30,348][0m Trial 2 finished with value: 0.36624444524447125 and parameters: {'observation_period_num': 18, 'train_rates': 0.615128463937033, 'learning_rate': 2.1476398300779036e-06, 'batch_size': 189, 'step_size': 15, 'gamma': 0.9787273785876387}. Best is trial 2 with value: 0.36624444524447125.[0m
[32m[I 2025-02-08 13:51:55,944][0m Trial 3 finished with value: 0.22775533979263488 and parameters: {'observation_period_num': 229, 'train_rates': 0.8846698519749938, 'learning_rate': 0.00012619856063780783, 'batch_size': 225, 'step_size': 15, 'gamma': 0.966407791214386}. Best is trial 3 with value: 0.22775533979263488.[0m
[32m[I 2025-02-08 13:53:09,877][0m Trial 4 finished with value: 0.30503448053379917 and parameters: {'observation_period_num': 204, 'train_rates': 0.7747052249360886, 'learning_rate': 0.00019457159263174244, 'batch_size': 66, 'step_size': 14, 'gamma': 0.8662801408907508}. Best is trial 3 with value: 0.22775533979263488.[0m
[32m[I 2025-02-08 13:54:05,831][0m Trial 5 finished with value: 0.12200126755568716 and parameters: {'observation_period_num': 6, 'train_rates': 0.6349576828132344, 'learning_rate': 0.0006753220139107656, 'batch_size': 82, 'step_size': 6, 'gamma': 0.9064246332308082}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 13:54:46,218][0m Trial 6 finished with value: 0.26857837367998927 and parameters: {'observation_period_num': 226, 'train_rates': 0.6806962662197463, 'learning_rate': 8.705052041950456e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.9685144933071221}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 13:58:21,312][0m Trial 7 finished with value: 0.15088065767117095 and parameters: {'observation_period_num': 226, 'train_rates': 0.9238213282423797, 'learning_rate': 2.081410149267992e-05, 'batch_size': 25, 'step_size': 11, 'gamma': 0.8224195820711775}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 13:59:29,014][0m Trial 8 finished with value: 0.45313114761596635 and parameters: {'observation_period_num': 16, 'train_rates': 0.9431312057443523, 'learning_rate': 1.2212562613000512e-06, 'batch_size': 89, 'step_size': 12, 'gamma': 0.831596370088162}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 14:00:18,214][0m Trial 9 finished with value: 0.13578440924811233 and parameters: {'observation_period_num': 180, 'train_rates': 0.8687399963059346, 'learning_rate': 0.00016855960772501458, 'batch_size': 112, 'step_size': 9, 'gamma': 0.8174764659364359}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 14:00:46,959][0m Trial 10 finished with value: 0.19474029734250037 and parameters: {'observation_period_num': 75, 'train_rates': 0.731563802164887, 'learning_rate': 0.0007045803507160116, 'batch_size': 181, 'step_size': 3, 'gamma': 0.9078174106538507}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 14:03:58,995][0m Trial 11 finished with value: 0.20137405541280043 and parameters: {'observation_period_num': 168, 'train_rates': 0.8520331961411477, 'learning_rate': 0.000943461136799804, 'batch_size': 27, 'step_size': 6, 'gamma': 0.7679158484604321}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 14:04:40,326][0m Trial 12 finished with value: 0.12875181436538696 and parameters: {'observation_period_num': 73, 'train_rates': 0.9895066983070019, 'learning_rate': 0.0003210255544735087, 'batch_size': 153, 'step_size': 7, 'gamma': 0.8968197651128382}. Best is trial 5 with value: 0.12200126755568716.[0m
[32m[I 2025-02-08 14:05:20,065][0m Trial 13 finished with value: 0.0856095403432846 and parameters: {'observation_period_num': 49, 'train_rates': 0.9711807472497695, 'learning_rate': 0.0004859048942600778, 'batch_size': 164, 'step_size': 5, 'gamma': 0.9165335805357772}. Best is trial 13 with value: 0.0856095403432846.[0m
[32m[I 2025-02-08 14:05:42,623][0m Trial 14 finished with value: 0.46796522816176855 and parameters: {'observation_period_num': 47, 'train_rates': 0.7357061696964596, 'learning_rate': 1.1271502825687782e-05, 'batch_size': 256, 'step_size': 4, 'gamma': 0.9281782985126762}. Best is trial 13 with value: 0.0856095403432846.[0m
[32m[I 2025-02-08 14:06:18,027][0m Trial 15 finished with value: 0.21396724529255776 and parameters: {'observation_period_num': 49, 'train_rates': 0.7680460173804906, 'learning_rate': 0.00042505146349711235, 'batch_size': 158, 'step_size': 1, 'gamma': 0.9284481812198161}. Best is trial 13 with value: 0.0856095403432846.[0m
[32m[I 2025-02-08 14:07:36,777][0m Trial 16 finished with value: 0.11552667015505408 and parameters: {'observation_period_num': 8, 'train_rates': 0.6045441050123241, 'learning_rate': 0.000340319375760159, 'batch_size': 56, 'step_size': 5, 'gamma': 0.8621631183696228}. Best is trial 13 with value: 0.0856095403432846.[0m
[32m[I 2025-02-08 14:08:04,038][0m Trial 17 finished with value: 0.42740621886672236 and parameters: {'observation_period_num': 111, 'train_rates': 0.7086419908525119, 'learning_rate': 3.3318234752185256e-05, 'batch_size': 200, 'step_size': 4, 'gamma': 0.864391173047241}. Best is trial 13 with value: 0.0856095403432846.[0m
[32m[I 2025-02-08 14:09:51,994][0m Trial 18 finished with value: 0.0665976496124203 and parameters: {'observation_period_num': 40, 'train_rates': 0.8105601541918094, 'learning_rate': 0.00032683849812889385, 'batch_size': 49, 'step_size': 1, 'gamma': 0.8824013059321932}. Best is trial 18 with value: 0.0665976496124203.[0m
[32m[I 2025-02-08 14:10:28,629][0m Trial 19 finished with value: 0.5504109037409928 and parameters: {'observation_period_num': 43, 'train_rates': 0.8157680109232325, 'learning_rate': 8.50278174232117e-06, 'batch_size': 157, 'step_size': 1, 'gamma': 0.9454901568207011}. Best is trial 18 with value: 0.0665976496124203.[0m
[32m[I 2025-02-08 14:11:19,273][0m Trial 20 finished with value: 0.21749907732009888 and parameters: {'observation_period_num': 85, 'train_rates': 0.9864490558305992, 'learning_rate': 9.399560559165528e-05, 'batch_size': 133, 'step_size': 2, 'gamma': 0.8854864006494518}. Best is trial 18 with value: 0.0665976496124203.[0m
[32m[I 2025-02-08 14:13:32,022][0m Trial 21 finished with value: 0.049741430580615996 and parameters: {'observation_period_num': 33, 'train_rates': 0.9233797078550601, 'learning_rate': 0.00031921029958176294, 'batch_size': 45, 'step_size': 5, 'gamma': 0.85047462267875}. Best is trial 21 with value: 0.049741430580615996.[0m
[32m[I 2025-02-08 14:15:24,621][0m Trial 22 finished with value: 0.03996287294162008 and parameters: {'observation_period_num': 38, 'train_rates': 0.9110844130075363, 'learning_rate': 0.00024845282250094444, 'batch_size': 51, 'step_size': 8, 'gamma': 0.844891064327435}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:17:34,125][0m Trial 23 finished with value: 0.04796531729733766 and parameters: {'observation_period_num': 35, 'train_rates': 0.9085190391777059, 'learning_rate': 0.00022945807470424896, 'batch_size': 44, 'step_size': 9, 'gamma': 0.8482829600341208}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:19:50,955][0m Trial 24 finished with value: 0.06445187343271462 and parameters: {'observation_period_num': 66, 'train_rates': 0.9012871094967804, 'learning_rate': 5.5398663226873696e-05, 'batch_size': 42, 'step_size': 9, 'gamma': 0.8482528390423567}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:24:41,813][0m Trial 25 finished with value: 0.04238148951946303 and parameters: {'observation_period_num': 27, 'train_rates': 0.9416291871008362, 'learning_rate': 0.00021207611589867833, 'batch_size': 20, 'step_size': 8, 'gamma': 0.7966729069590384}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:29:29,139][0m Trial 26 finished with value: 0.13918120408950208 and parameters: {'observation_period_num': 93, 'train_rates': 0.9489606825024424, 'learning_rate': 0.0001765407204802108, 'batch_size': 20, 'step_size': 8, 'gamma': 0.7889976484200225}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:30:27,739][0m Trial 27 finished with value: 0.1270664763356839 and parameters: {'observation_period_num': 120, 'train_rates': 0.8996329785426105, 'learning_rate': 6.204135150593644e-05, 'batch_size': 97, 'step_size': 9, 'gamma': 0.7501928956683513}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:35:39,742][0m Trial 28 finished with value: 0.11971441924791126 and parameters: {'observation_period_num': 135, 'train_rates': 0.9521407929665642, 'learning_rate': 2.962344706762962e-05, 'batch_size': 18, 'step_size': 8, 'gamma': 0.7943870984216994}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:37:07,096][0m Trial 29 finished with value: 0.04278450915732582 and parameters: {'observation_period_num': 25, 'train_rates': 0.8446982161026138, 'learning_rate': 0.0001759440672109956, 'batch_size': 62, 'step_size': 10, 'gamma': 0.7992255532324701}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:38:32,103][0m Trial 30 finished with value: 0.07657714879590702 and parameters: {'observation_period_num': 66, 'train_rates': 0.8418182212503417, 'learning_rate': 0.00010744409581430123, 'batch_size': 64, 'step_size': 13, 'gamma': 0.7902314021202852}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:41:04,252][0m Trial 31 finished with value: 0.17456615692766758 and parameters: {'observation_period_num': 26, 'train_rates': 0.8596436696349655, 'learning_rate': 0.00020682950679557405, 'batch_size': 36, 'step_size': 10, 'gamma': 0.8387337799783903}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:42:27,737][0m Trial 32 finished with value: 0.04460297345620468 and parameters: {'observation_period_num': 29, 'train_rates': 0.9180026987875887, 'learning_rate': 0.00023961288214656082, 'batch_size': 70, 'step_size': 7, 'gamma': 0.771396368501643}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:43:45,912][0m Trial 33 finished with value: 0.0949176951226863 and parameters: {'observation_period_num': 58, 'train_rates': 0.8789123451067086, 'learning_rate': 0.0005752348058393282, 'batch_size': 72, 'step_size': 7, 'gamma': 0.7721356957095084}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:44:38,370][0m Trial 34 finished with value: 0.051707180049182115 and parameters: {'observation_period_num': 26, 'train_rates': 0.9281949677716574, 'learning_rate': 0.00012800584922834096, 'batch_size': 116, 'step_size': 7, 'gamma': 0.8034625616884719}. Best is trial 22 with value: 0.03996287294162008.[0m
[32m[I 2025-02-08 14:45:34,257][0m Trial 35 finished with value: 0.03303589838586988 and parameters: {'observation_period_num': 17, 'train_rates': 0.8317133965799608, 'learning_rate': 0.000993386921539998, 'batch_size': 98, 'step_size': 10, 'gamma': 0.7743876685929992}. Best is trial 35 with value: 0.03303589838586988.[0m
[32m[I 2025-02-08 14:46:34,184][0m Trial 36 finished with value: 0.028968574671659322 and parameters: {'observation_period_num': 7, 'train_rates': 0.8265208184683778, 'learning_rate': 0.000760651056777363, 'batch_size': 92, 'step_size': 10, 'gamma': 0.753616608118676}. Best is trial 36 with value: 0.028968574671659322.[0m
[32m[I 2025-02-08 14:47:28,667][0m Trial 37 finished with value: 0.16346633077989664 and parameters: {'observation_period_num': 5, 'train_rates': 0.79036743073034, 'learning_rate': 0.000961448884696542, 'batch_size': 98, 'step_size': 12, 'gamma': 0.7521741744531771}. Best is trial 36 with value: 0.028968574671659322.[0m
[32m[I 2025-02-08 14:48:25,916][0m Trial 38 finished with value: 0.34577663112612606 and parameters: {'observation_period_num': 251, 'train_rates': 0.7721164737057011, 'learning_rate': 0.0007116339730237951, 'batch_size': 84, 'step_size': 10, 'gamma': 0.7819534551784176}. Best is trial 36 with value: 0.028968574671659322.[0m
[32m[I 2025-02-08 14:49:11,267][0m Trial 39 finished with value: 0.035370473802652 and parameters: {'observation_period_num': 17, 'train_rates': 0.820215847726404, 'learning_rate': 0.00083657128656166, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8162561955860428}. Best is trial 36 with value: 0.028968574671659322.[0m
[32m[I 2025-02-08 14:49:58,247][0m Trial 40 finished with value: 0.03637713576608803 and parameters: {'observation_period_num': 16, 'train_rates': 0.82639067050693, 'learning_rate': 0.000977275133084035, 'batch_size': 126, 'step_size': 12, 'gamma': 0.8130193057960364}. Best is trial 36 with value: 0.028968574671659322.[0m
[32m[I 2025-02-08 14:50:48,225][0m Trial 41 finished with value: 0.03266610373024025 and parameters: {'observation_period_num': 14, 'train_rates': 0.8216467894352535, 'learning_rate': 0.0008911148635531416, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8142565568139846}. Best is trial 36 with value: 0.028968574671659322.[0m
[32m[I 2025-02-08 14:51:33,656][0m Trial 42 finished with value: 0.028845892673948915 and parameters: {'observation_period_num': 10, 'train_rates': 0.8196662231749537, 'learning_rate': 0.0007884243027725701, 'batch_size': 122, 'step_size': 14, 'gamma': 0.8111189781780156}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:52:13,496][0m Trial 43 finished with value: 0.17335974551296154 and parameters: {'observation_period_num': 15, 'train_rates': 0.7937186240232488, 'learning_rate': 0.0005947582451710779, 'batch_size': 139, 'step_size': 14, 'gamma': 0.8309193989556277}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:53:04,435][0m Trial 44 finished with value: 0.14776311598068526 and parameters: {'observation_period_num': 9, 'train_rates': 0.7517108921818005, 'learning_rate': 0.0007937634008754057, 'batch_size': 101, 'step_size': 14, 'gamma': 0.7655961093956847}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:53:49,994][0m Trial 45 finished with value: 0.0350890092930109 and parameters: {'observation_period_num': 16, 'train_rates': 0.8064088472453379, 'learning_rate': 0.000518208198847862, 'batch_size': 120, 'step_size': 13, 'gamma': 0.8100832583762806}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:54:29,494][0m Trial 46 finished with value: 0.03419565570515555 and parameters: {'observation_period_num': 5, 'train_rates': 0.8000724514617098, 'learning_rate': 0.00044071086573226894, 'batch_size': 144, 'step_size': 13, 'gamma': 0.7812961244875573}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:55:09,997][0m Trial 47 finished with value: 0.05611454798337306 and parameters: {'observation_period_num': 59, 'train_rates': 0.8354518944693881, 'learning_rate': 0.0004732043252352053, 'batch_size': 142, 'step_size': 15, 'gamma': 0.782635090636093}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:56:00,538][0m Trial 48 finished with value: 0.13256205756899336 and parameters: {'observation_period_num': 153, 'train_rates': 0.8738798069178999, 'learning_rate': 0.00044893815998612007, 'batch_size': 108, 'step_size': 13, 'gamma': 0.7580674923353724}. Best is trial 42 with value: 0.028845892673948915.[0m
[32m[I 2025-02-08 14:56:33,789][0m Trial 49 finished with value: 0.16720110828616014 and parameters: {'observation_period_num': 5, 'train_rates': 0.7827349890281428, 'learning_rate': 0.0006372241614678919, 'batch_size': 172, 'step_size': 12, 'gamma': 0.7753254253068335}. Best is trial 42 with value: 0.028845892673948915.[0m
trend „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_0
[32m[I 2025-02-08 14:56:33,800][0m A new study created in memory with name: no-name-cac6756b-726b-4a2b-abe0-6207e32bddbe[0m
[32m[I 2025-02-08 15:00:51,883][0m Trial 0 finished with value: 0.12894530301749477 and parameters: {'observation_period_num': 194, 'train_rates': 0.8117181666276903, 'learning_rate': 1.9357748911184584e-05, 'batch_size': 19, 'step_size': 6, 'gamma': 0.9864005977993084}. Best is trial 0 with value: 0.12894530301749477.[0m
[32m[I 2025-02-08 15:01:39,879][0m Trial 1 finished with value: 0.3293091356754303 and parameters: {'observation_period_num': 95, 'train_rates': 0.9759130372662925, 'learning_rate': 1.4306963239174109e-05, 'batch_size': 133, 'step_size': 10, 'gamma': 0.7785768741593597}. Best is trial 0 with value: 0.12894530301749477.[0m
[32m[I 2025-02-08 15:02:44,643][0m Trial 2 finished with value: 0.12217168065108659 and parameters: {'observation_period_num': 200, 'train_rates': 0.9012394562765227, 'learning_rate': 0.0001236030815388638, 'batch_size': 83, 'step_size': 3, 'gamma': 0.959987654861919}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:03:22,014][0m Trial 3 finished with value: 0.4719261522175836 and parameters: {'observation_period_num': 249, 'train_rates': 0.9325568539571645, 'learning_rate': 7.386904088205321e-06, 'batch_size': 159, 'step_size': 15, 'gamma': 0.9497192934144892}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:04:40,464][0m Trial 4 finished with value: 0.14788769709445557 and parameters: {'observation_period_num': 250, 'train_rates': 0.8723546392060524, 'learning_rate': 6.078176931807449e-05, 'batch_size': 66, 'step_size': 10, 'gamma': 0.7921656528526714}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:05:11,575][0m Trial 5 finished with value: 0.2057826109972608 and parameters: {'observation_period_num': 119, 'train_rates': 0.9117215267408645, 'learning_rate': 6.716473441134344e-05, 'batch_size': 197, 'step_size': 12, 'gamma': 0.946998086524134}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:05:45,810][0m Trial 6 finished with value: 0.2197528861470839 and parameters: {'observation_period_num': 77, 'train_rates': 0.6676928048216253, 'learning_rate': 4.1919052292692505e-05, 'batch_size': 145, 'step_size': 15, 'gamma': 0.8884032588079147}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:06:22,381][0m Trial 7 finished with value: 0.12810334048487923 and parameters: {'observation_period_num': 188, 'train_rates': 0.8416856007806843, 'learning_rate': 0.0004081537277978171, 'batch_size': 152, 'step_size': 5, 'gamma': 0.9179882488314136}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:06:47,663][0m Trial 8 finished with value: 0.9660609285036723 and parameters: {'observation_period_num': 163, 'train_rates': 0.8875240442559699, 'learning_rate': 6.589677115388761e-06, 'batch_size': 231, 'step_size': 3, 'gamma': 0.8310047281509509}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:08:05,494][0m Trial 9 finished with value: 0.1981887064894811 and parameters: {'observation_period_num': 33, 'train_rates': 0.7537743896265725, 'learning_rate': 0.0009484030853174694, 'batch_size': 65, 'step_size': 13, 'gamma': 0.9697377095421024}. Best is trial 2 with value: 0.12217168065108659.[0m
Early stopping at epoch 70
[32m[I 2025-02-08 15:08:47,240][0m Trial 10 finished with value: 1.3583442046158556 and parameters: {'observation_period_num': 5, 'train_rates': 0.7201675400494152, 'learning_rate': 1.310940967797119e-06, 'batch_size': 86, 'step_size': 1, 'gamma': 0.8581203319352962}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:09:31,260][0m Trial 11 finished with value: 0.15020740635194352 and parameters: {'observation_period_num': 187, 'train_rates': 0.8224687127067337, 'learning_rate': 0.00035737823395388225, 'batch_size': 121, 'step_size': 5, 'gamma': 0.9035344647596673}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:10:26,518][0m Trial 12 finished with value: 0.12732866078110064 and parameters: {'observation_period_num': 206, 'train_rates': 0.8503149268761037, 'learning_rate': 0.0002172040067309012, 'batch_size': 96, 'step_size': 6, 'gamma': 0.9169423511461415}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:15:03,675][0m Trial 13 finished with value: 0.17869329705553236 and parameters: {'observation_period_num': 212, 'train_rates': 0.9810404921160958, 'learning_rate': 0.00015705331496319394, 'batch_size': 20, 'step_size': 7, 'gamma': 0.9410307548080662}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:15:49,953][0m Trial 14 finished with value: 0.3116508206776457 and parameters: {'observation_period_num': 143, 'train_rates': 0.6088201960700826, 'learning_rate': 0.00018132545074516472, 'batch_size': 100, 'step_size': 2, 'gamma': 0.8706312646559113}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:17:11,760][0m Trial 15 finished with value: 0.2731471603344876 and parameters: {'observation_period_num': 229, 'train_rates': 0.7545109438052549, 'learning_rate': 0.0001339858093064261, 'batch_size': 58, 'step_size': 4, 'gamma': 0.9228713015538279}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:18:08,428][0m Trial 16 finished with value: 0.15196808736297965 and parameters: {'observation_period_num': 159, 'train_rates': 0.9354996968896951, 'learning_rate': 0.0009950133388669118, 'batch_size': 104, 'step_size': 8, 'gamma': 0.9853850271395107}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:18:37,946][0m Trial 17 finished with value: 0.19241158853877674 and parameters: {'observation_period_num': 227, 'train_rates': 0.8796698500176827, 'learning_rate': 0.0003728270412999262, 'batch_size': 188, 'step_size': 3, 'gamma': 0.8346031821988059}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:20:27,901][0m Trial 18 finished with value: 0.325929244117039 and parameters: {'observation_period_num': 171, 'train_rates': 0.779768986790935, 'learning_rate': 9.072139665382991e-05, 'batch_size': 45, 'step_size': 1, 'gamma': 0.8913259302185952}. Best is trial 2 with value: 0.12217168065108659.[0m
[32m[I 2025-02-08 15:21:31,767][0m Trial 19 finished with value: 0.11343605478136647 and parameters: {'observation_period_num': 126, 'train_rates': 0.8354532170505917, 'learning_rate': 2.563788282349895e-05, 'batch_size': 86, 'step_size': 8, 'gamma': 0.9599757062201857}. Best is trial 19 with value: 0.11343605478136647.[0m
[32m[I 2025-02-08 15:21:55,581][0m Trial 20 finished with value: 0.7566950655256222 and parameters: {'observation_period_num': 115, 'train_rates': 0.7851085860811099, 'learning_rate': 2.42591481783249e-06, 'batch_size': 251, 'step_size': 10, 'gamma': 0.9601948719648905}. Best is trial 19 with value: 0.11343605478136647.[0m
[32m[I 2025-02-08 15:22:53,454][0m Trial 21 finished with value: 0.12721437353052592 and parameters: {'observation_period_num': 138, 'train_rates': 0.8521184361200007, 'learning_rate': 2.7253281699583885e-05, 'batch_size': 93, 'step_size': 8, 'gamma': 0.9305090579458383}. Best is trial 19 with value: 0.11343605478136647.[0m
[32m[I 2025-02-08 15:24:05,569][0m Trial 22 finished with value: 0.10050793074269712 and parameters: {'observation_period_num': 72, 'train_rates': 0.9092840016718413, 'learning_rate': 2.5911162546764243e-05, 'batch_size': 80, 'step_size': 8, 'gamma': 0.9337376397664915}. Best is trial 22 with value: 0.10050793074269712.[0m
[32m[I 2025-02-08 15:26:24,911][0m Trial 23 finished with value: 0.10865182109588495 and parameters: {'observation_period_num': 57, 'train_rates': 0.943919916007265, 'learning_rate': 8.674389938400888e-06, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9675576847893008}. Best is trial 22 with value: 0.10050793074269712.[0m
[32m[I 2025-02-08 15:28:47,087][0m Trial 24 finished with value: 0.11287337323638047 and parameters: {'observation_period_num': 63, 'train_rates': 0.949347293901733, 'learning_rate': 7.909276498487543e-06, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9759853845969778}. Best is trial 22 with value: 0.10050793074269712.[0m
[32m[I 2025-02-08 15:31:21,299][0m Trial 25 finished with value: 0.08778438887564964 and parameters: {'observation_period_num': 59, 'train_rates': 0.9474021056456737, 'learning_rate': 9.58143943699617e-06, 'batch_size': 38, 'step_size': 11, 'gamma': 0.9804842385242925}. Best is trial 25 with value: 0.08778438887564964.[0m
[32m[I 2025-02-08 15:34:20,305][0m Trial 26 finished with value: 0.2534905307683624 and parameters: {'observation_period_num': 39, 'train_rates': 0.9595653113353328, 'learning_rate': 3.5641164756547315e-06, 'batch_size': 33, 'step_size': 12, 'gamma': 0.7502512486139807}. Best is trial 25 with value: 0.08778438887564964.[0m
[32m[I 2025-02-08 15:36:06,309][0m Trial 27 finished with value: 0.09353334403463773 and parameters: {'observation_period_num': 56, 'train_rates': 0.9134051223998079, 'learning_rate': 1.360125268624297e-05, 'batch_size': 54, 'step_size': 11, 'gamma': 0.9887726943038284}. Best is trial 25 with value: 0.08778438887564964.[0m
[32m[I 2025-02-08 15:37:40,949][0m Trial 28 finished with value: 0.11272071792355066 and parameters: {'observation_period_num': 95, 'train_rates': 0.917393039213173, 'learning_rate': 1.623426558126799e-05, 'batch_size': 60, 'step_size': 12, 'gamma': 0.9890211070971284}. Best is trial 25 with value: 0.08778438887564964.[0m
[32m[I 2025-02-08 15:41:20,563][0m Trial 29 finished with value: 0.09443441994728581 and parameters: {'observation_period_num': 10, 'train_rates': 0.9898295641833076, 'learning_rate': 3.6609859961368926e-06, 'batch_size': 28, 'step_size': 13, 'gamma': 0.9392121169716522}. Best is trial 25 with value: 0.08778438887564964.[0m
[32m[I 2025-02-08 15:47:16,342][0m Trial 30 finished with value: 0.08563880406079753 and parameters: {'observation_period_num': 9, 'train_rates': 0.9898361382179747, 'learning_rate': 3.873811404130413e-06, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9836921734525076}. Best is trial 30 with value: 0.08563880406079753.[0m
[32m[I 2025-02-08 15:53:08,410][0m Trial 31 finished with value: 0.08511193817661654 and parameters: {'observation_period_num': 10, 'train_rates': 0.9896221959357421, 'learning_rate': 3.922877306977362e-06, 'batch_size': 17, 'step_size': 14, 'gamma': 0.987964334098528}. Best is trial 31 with value: 0.08511193817661654.[0m
[32m[I 2025-02-08 15:58:51,853][0m Trial 32 finished with value: 0.05918892620588249 and parameters: {'observation_period_num': 28, 'train_rates': 0.9639146165227073, 'learning_rate': 1.2257272337803675e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9864625184175219}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:03:33,274][0m Trial 33 finished with value: 0.160669686573167 and parameters: {'observation_period_num': 25, 'train_rates': 0.9683945303137733, 'learning_rate': 1.277522778146072e-06, 'batch_size': 21, 'step_size': 14, 'gamma': 0.975705972490081}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:09:27,995][0m Trial 34 finished with value: 0.07508103320231804 and parameters: {'observation_period_num': 19, 'train_rates': 0.9870568291359411, 'learning_rate': 4.491010893458178e-06, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9573480116601869}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:14:29,302][0m Trial 35 finished with value: 0.07738906145095825 and parameters: {'observation_period_num': 20, 'train_rates': 0.9865493990392644, 'learning_rate': 4.399386505473657e-06, 'batch_size': 20, 'step_size': 14, 'gamma': 0.9580009728838375}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:17:55,043][0m Trial 36 finished with value: 0.17452016584975746 and parameters: {'observation_period_num': 27, 'train_rates': 0.9685561048721959, 'learning_rate': 2.0172167217379947e-06, 'batch_size': 29, 'step_size': 14, 'gamma': 0.954254389346604}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:19:55,556][0m Trial 37 finished with value: 0.12898826599121094 and parameters: {'observation_period_num': 38, 'train_rates': 0.9305674279772985, 'learning_rate': 4.7797337482870105e-06, 'batch_size': 48, 'step_size': 15, 'gamma': 0.9547226986766533}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:21:17,659][0m Trial 38 finished with value: 0.24951189094119602 and parameters: {'observation_period_num': 20, 'train_rates': 0.9664231574779729, 'learning_rate': 1.890262469985654e-06, 'batch_size': 73, 'step_size': 13, 'gamma': 0.9673215095135973}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:26:24,813][0m Trial 39 finished with value: 0.08048429978745324 and parameters: {'observation_period_num': 48, 'train_rates': 0.8919211072726551, 'learning_rate': 5.554215568712507e-06, 'batch_size': 18, 'step_size': 15, 'gamma': 0.9094042495050716}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:29:26,541][0m Trial 40 finished with value: 0.06971589658458374 and parameters: {'observation_period_num': 47, 'train_rates': 0.8959739648036843, 'learning_rate': 1.27851742871161e-05, 'batch_size': 31, 'step_size': 15, 'gamma': 0.9081582427091475}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:32:30,072][0m Trial 41 finished with value: 0.06455604356239762 and parameters: {'observation_period_num': 45, 'train_rates': 0.8830397826419741, 'learning_rate': 1.2140283700904243e-05, 'batch_size': 30, 'step_size': 15, 'gamma': 0.913656872833972}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:35:26,122][0m Trial 42 finished with value: 0.09551854899116591 and parameters: {'observation_period_num': 87, 'train_rates': 0.9292294290119041, 'learning_rate': 1.5170567265362532e-05, 'batch_size': 32, 'step_size': 15, 'gamma': 0.8898552437276164}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:36:02,121][0m Trial 43 finished with value: 0.07138554744863061 and parameters: {'observation_period_num': 45, 'train_rates': 0.8727674080968412, 'learning_rate': 4.582082695154359e-05, 'batch_size': 169, 'step_size': 13, 'gamma': 0.87224461768554}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:36:34,575][0m Trial 44 finished with value: 0.08785907529495858 and parameters: {'observation_period_num': 42, 'train_rates': 0.8101498110089054, 'learning_rate': 3.7439068225790054e-05, 'batch_size': 171, 'step_size': 13, 'gamma': 0.8704676638415061}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:37:09,671][0m Trial 45 finished with value: 0.087333281677735 and parameters: {'observation_period_num': 72, 'train_rates': 0.8635188888539135, 'learning_rate': 5.203185653059837e-05, 'batch_size': 170, 'step_size': 15, 'gamma': 0.8334699233756605}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:37:59,201][0m Trial 46 finished with value: 0.1218405992297803 and parameters: {'observation_period_num': 31, 'train_rates': 0.8993534165106728, 'learning_rate': 1.1921841669210901e-05, 'batch_size': 122, 'step_size': 12, 'gamma': 0.9025800883391881}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:38:26,606][0m Trial 47 finished with value: 0.2489243797706754 and parameters: {'observation_period_num': 105, 'train_rates': 0.8754307968047437, 'learning_rate': 2.0273126135404395e-05, 'batch_size': 210, 'step_size': 11, 'gamma': 0.8546412776601375}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:39:01,127][0m Trial 48 finished with value: 0.06846359117178742 and parameters: {'observation_period_num': 45, 'train_rates': 0.8134708329259426, 'learning_rate': 7.492135144548325e-05, 'batch_size': 161, 'step_size': 13, 'gamma': 0.8825450965682248}. Best is trial 32 with value: 0.05918892620588249.[0m
[32m[I 2025-02-08 16:39:41,774][0m Trial 49 finished with value: 0.09451898272784492 and parameters: {'observation_period_num': 83, 'train_rates': 0.8121365788710968, 'learning_rate': 7.517239998109324e-05, 'batch_size': 137, 'step_size': 13, 'gamma': 0.8585328028656056}. Best is trial 32 with value: 0.05918892620588249.[0m
seasonal_0 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_1
[32m[I 2025-02-08 16:39:41,784][0m A new study created in memory with name: no-name-67f81529-f283-4b85-a7e6-503f918fab93[0m
[32m[I 2025-02-08 16:40:17,362][0m Trial 0 finished with value: 0.46118526362084056 and parameters: {'observation_period_num': 53, 'train_rates': 0.7584031290104303, 'learning_rate': 7.156378784983247e-06, 'batch_size': 152, 'step_size': 11, 'gamma': 0.8109164522369041}. Best is trial 0 with value: 0.46118526362084056.[0m
[32m[I 2025-02-08 16:40:57,170][0m Trial 1 finished with value: 0.09458577917335001 and parameters: {'observation_period_num': 133, 'train_rates': 0.7909974498452046, 'learning_rate': 0.0004895124388132997, 'batch_size': 135, 'step_size': 8, 'gamma': 0.8693822171646507}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:41:23,141][0m Trial 2 finished with value: 0.5228951546395647 and parameters: {'observation_period_num': 214, 'train_rates': 0.6625835192914624, 'learning_rate': 5.0765943103869555e-05, 'batch_size': 191, 'step_size': 3, 'gamma': 0.7651119221977347}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:41:53,015][0m Trial 3 finished with value: 0.7422102208705557 and parameters: {'observation_period_num': 55, 'train_rates': 0.79514415654096, 'learning_rate': 3.7805272296243717e-06, 'batch_size': 183, 'step_size': 3, 'gamma': 0.7588133571765572}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:42:16,162][0m Trial 4 finished with value: 0.8071399753642194 and parameters: {'observation_period_num': 83, 'train_rates': 0.6302163903014693, 'learning_rate': 5.318385326596917e-06, 'batch_size': 220, 'step_size': 4, 'gamma': 0.7514221277997655}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:42:42,458][0m Trial 5 finished with value: 1.2214714594356848 and parameters: {'observation_period_num': 10, 'train_rates': 0.7352858882116909, 'learning_rate': 1.605475298996363e-06, 'batch_size': 199, 'step_size': 1, 'gamma': 0.9420394982223245}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:43:03,370][0m Trial 6 finished with value: 0.18890515356045395 and parameters: {'observation_period_num': 83, 'train_rates': 0.640934913309226, 'learning_rate': 0.0004160634088786632, 'batch_size': 256, 'step_size': 9, 'gamma': 0.9094410397707042}. Best is trial 1 with value: 0.09458577917335001.[0m
Early stopping at epoch 73
[32m[I 2025-02-08 16:43:27,191][0m Trial 7 finished with value: 0.8777130790044109 and parameters: {'observation_period_num': 123, 'train_rates': 0.7548045685075402, 'learning_rate': 5.023566733620378e-06, 'batch_size': 173, 'step_size': 1, 'gamma': 0.8581649956829679}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:44:00,353][0m Trial 8 finished with value: 0.1443215709557568 and parameters: {'observation_period_num': 175, 'train_rates': 0.9011729083876001, 'learning_rate': 0.0004691897692898406, 'batch_size': 182, 'step_size': 12, 'gamma': 0.9818885792074123}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:45:01,594][0m Trial 9 finished with value: 0.1293994263572208 and parameters: {'observation_period_num': 41, 'train_rates': 0.8990337025584659, 'learning_rate': 0.0007650290779137882, 'batch_size': 96, 'step_size': 14, 'gamma': 0.9893460053479521}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:50:05,164][0m Trial 10 finished with value: 0.1246554481016623 and parameters: {'observation_period_num': 152, 'train_rates': 0.9870412358622531, 'learning_rate': 7.26320471630977e-05, 'batch_size': 19, 'step_size': 7, 'gamma': 0.8587222061375145}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:55:24,350][0m Trial 11 finished with value: 0.158676378428936 and parameters: {'observation_period_num': 153, 'train_rates': 0.988903691850809, 'learning_rate': 9.479302124696839e-05, 'batch_size': 18, 'step_size': 6, 'gamma': 0.859977043797018}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:56:27,304][0m Trial 12 finished with value: 0.12166333686319186 and parameters: {'observation_period_num': 244, 'train_rates': 0.8466951901222797, 'learning_rate': 0.00015176311633442396, 'batch_size': 87, 'step_size': 8, 'gamma': 0.823169693756227}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:57:20,663][0m Trial 13 finished with value: 0.12052835381194336 and parameters: {'observation_period_num': 246, 'train_rates': 0.8390251622390833, 'learning_rate': 0.00023083368091198118, 'batch_size': 102, 'step_size': 9, 'gamma': 0.823621544698378}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:58:07,615][0m Trial 14 finished with value: 0.11346706651369398 and parameters: {'observation_period_num': 206, 'train_rates': 0.8267105459237922, 'learning_rate': 0.00024234695165078275, 'batch_size': 114, 'step_size': 10, 'gamma': 0.807359842921439}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:58:46,322][0m Trial 15 finished with value: 0.3498485198240007 and parameters: {'observation_period_num': 203, 'train_rates': 0.6965759134308558, 'learning_rate': 2.2786271132731672e-05, 'batch_size': 126, 'step_size': 11, 'gamma': 0.9025767960941509}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 16:59:57,256][0m Trial 16 finished with value: 0.11362147179228821 and parameters: {'observation_period_num': 113, 'train_rates': 0.816844901548211, 'learning_rate': 0.0009825211680972324, 'batch_size': 75, 'step_size': 15, 'gamma': 0.7921373700998909}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:01:32,799][0m Trial 17 finished with value: 0.1895820259144812 and parameters: {'observation_period_num': 204, 'train_rates': 0.892424719802581, 'learning_rate': 2.140182947697602e-05, 'batch_size': 56, 'step_size': 5, 'gamma': 0.8961144893010509}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:02:10,163][0m Trial 18 finished with value: 0.22601329114126123 and parameters: {'observation_period_num': 168, 'train_rates': 0.7118848264724141, 'learning_rate': 0.0002287385100398507, 'batch_size': 133, 'step_size': 13, 'gamma': 0.7863796657647192}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:02:58,254][0m Trial 19 finished with value: 0.2276018379481545 and parameters: {'observation_period_num': 107, 'train_rates': 0.7801363351015572, 'learning_rate': 0.00039060196345573904, 'batch_size': 115, 'step_size': 10, 'gamma': 0.8398555621372258}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:03:35,632][0m Trial 20 finished with value: 0.13343173240342837 and parameters: {'observation_period_num': 187, 'train_rates': 0.8719305818350425, 'learning_rate': 0.00014089018802542847, 'batch_size': 154, 'step_size': 7, 'gamma': 0.884324713485985}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:04:51,402][0m Trial 21 finished with value: 0.11910781687287983 and parameters: {'observation_period_num': 126, 'train_rates': 0.8101526772226573, 'learning_rate': 0.0009720045524165159, 'batch_size': 68, 'step_size': 15, 'gamma': 0.7880064902885109}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:06:18,845][0m Trial 22 finished with value: 0.10387785277680722 and parameters: {'observation_period_num': 104, 'train_rates': 0.8247945059540696, 'learning_rate': 0.0009603724460003624, 'batch_size': 60, 'step_size': 15, 'gamma': 0.7870944283998406}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:08:14,633][0m Trial 23 finished with value: 0.1455687392089102 and parameters: {'observation_period_num': 92, 'train_rates': 0.8528762968363927, 'learning_rate': 0.0004749492324503285, 'batch_size': 47, 'step_size': 13, 'gamma': 0.8019985572619172}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:08:56,100][0m Trial 24 finished with value: 0.12672735750675201 and parameters: {'observation_period_num': 139, 'train_rates': 0.9541697396310017, 'learning_rate': 0.00021135303429168172, 'batch_size': 151, 'step_size': 10, 'gamma': 0.8377525087233288}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:09:45,280][0m Trial 25 finished with value: 0.22478413069473832 and parameters: {'observation_period_num': 100, 'train_rates': 0.7786810679716137, 'learning_rate': 0.0005736749058584157, 'batch_size': 108, 'step_size': 8, 'gamma': 0.9347021755651471}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:11:40,638][0m Trial 26 finished with value: 0.16525347841029264 and parameters: {'observation_period_num': 224, 'train_rates': 0.8213193666559889, 'learning_rate': 0.0003119291264948104, 'batch_size': 44, 'step_size': 12, 'gamma': 0.77770843850721}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:12:28,386][0m Trial 27 finished with value: 0.11413964055513552 and parameters: {'observation_period_num': 72, 'train_rates': 0.9264770158646862, 'learning_rate': 4.745077477799995e-05, 'batch_size': 124, 'step_size': 7, 'gamma': 0.8392684369040796}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:13:27,439][0m Trial 28 finished with value: 0.22912188940394893 and parameters: {'observation_period_num': 140, 'train_rates': 0.6030083171055393, 'learning_rate': 0.00013027568273682645, 'batch_size': 73, 'step_size': 10, 'gamma': 0.8794512312049677}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:14:05,444][0m Trial 29 finished with value: 0.16705299143675512 and parameters: {'observation_period_num': 28, 'train_rates': 0.7578508489243814, 'learning_rate': 0.0006629575630741306, 'batch_size': 147, 'step_size': 12, 'gamma': 0.8108795689061536}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:15:11,128][0m Trial 30 finished with value: 0.1552165576276627 and parameters: {'observation_period_num': 68, 'train_rates': 0.8700899551810375, 'learning_rate': 1.3205545452074785e-05, 'batch_size': 86, 'step_size': 14, 'gamma': 0.775564622822526}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:16:23,958][0m Trial 31 finished with value: 0.12997757200615473 and parameters: {'observation_period_num': 112, 'train_rates': 0.8255827581332162, 'learning_rate': 0.0009019161798584418, 'batch_size': 73, 'step_size': 15, 'gamma': 0.8031034215946905}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:18:42,095][0m Trial 32 finished with value: 0.11775650782510638 and parameters: {'observation_period_num': 118, 'train_rates': 0.7976794828327106, 'learning_rate': 0.00027988358414763264, 'batch_size': 36, 'step_size': 14, 'gamma': 0.7940728736627364}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:19:37,041][0m Trial 33 finished with value: 0.2605635521382938 and parameters: {'observation_period_num': 166, 'train_rates': 0.7353060514112769, 'learning_rate': 0.0009978103366242521, 'batch_size': 88, 'step_size': 15, 'gamma': 0.7680145914076565}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:20:56,194][0m Trial 34 finished with value: 0.15748262806580618 and parameters: {'observation_period_num': 228, 'train_rates': 0.7814995691648816, 'learning_rate': 0.0005816058179266247, 'batch_size': 62, 'step_size': 13, 'gamma': 0.8245638080868107}. Best is trial 1 with value: 0.09458577917335001.[0m
[32m[I 2025-02-08 17:21:44,659][0m Trial 35 finished with value: 0.05444116849953079 and parameters: {'observation_period_num': 58, 'train_rates': 0.8132381395432028, 'learning_rate': 0.00035838916275795406, 'batch_size': 115, 'step_size': 11, 'gamma': 0.7596629344850745}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:22:34,401][0m Trial 36 finished with value: 0.06784493260425153 and parameters: {'observation_period_num': 49, 'train_rates': 0.8720911883338555, 'learning_rate': 5.883684718255477e-05, 'batch_size': 116, 'step_size': 11, 'gamma': 0.7520487340650455}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:23:01,715][0m Trial 37 finished with value: 0.920445363998413 and parameters: {'observation_period_num': 52, 'train_rates': 0.8713435546471432, 'learning_rate': 1.498193639540854e-06, 'batch_size': 214, 'step_size': 11, 'gamma': 0.7542152740720076}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:23:37,082][0m Trial 38 finished with value: 0.33365450750030046 and parameters: {'observation_period_num': 25, 'train_rates': 0.9284048427924756, 'learning_rate': 9.222513407598936e-06, 'batch_size': 171, 'step_size': 9, 'gamma': 0.7606476805514185}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:24:15,021][0m Trial 39 finished with value: 0.36495346250558924 and parameters: {'observation_period_num': 64, 'train_rates': 0.7354151355759189, 'learning_rate': 4.274785609058374e-05, 'batch_size': 142, 'step_size': 3, 'gamma': 0.7514270793678791}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:24:58,208][0m Trial 40 finished with value: 0.34913798289554177 and parameters: {'observation_period_num': 10, 'train_rates': 0.7975729661647704, 'learning_rate': 2.9781203050103878e-06, 'batch_size': 126, 'step_size': 11, 'gamma': 0.770514529090216}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:25:48,328][0m Trial 41 finished with value: 0.06685760400651657 and parameters: {'observation_period_num': 42, 'train_rates': 0.8339179584490046, 'learning_rate': 7.438196328218433e-05, 'batch_size': 112, 'step_size': 8, 'gamma': 0.7807250639556123}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:26:22,673][0m Trial 42 finished with value: 0.06421818578038938 and parameters: {'observation_period_num': 46, 'train_rates': 0.8552117045003507, 'learning_rate': 8.586122362782575e-05, 'batch_size': 169, 'step_size': 6, 'gamma': 0.9665162412969948}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:27:00,626][0m Trial 43 finished with value: 0.06064834656475791 and parameters: {'observation_period_num': 43, 'train_rates': 0.8619882774376184, 'learning_rate': 7.564544034524868e-05, 'batch_size': 163, 'step_size': 6, 'gamma': 0.9685975345451542}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:27:34,816][0m Trial 44 finished with value: 0.06030673164587754 and parameters: {'observation_period_num': 42, 'train_rates': 0.8889575040133422, 'learning_rate': 8.188195111402146e-05, 'batch_size': 173, 'step_size': 5, 'gamma': 0.9641218905372195}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:28:10,450][0m Trial 45 finished with value: 0.056069607566753776 and parameters: {'observation_period_num': 33, 'train_rates': 0.8899733694910219, 'learning_rate': 8.771268976261862e-05, 'batch_size': 164, 'step_size': 5, 'gamma': 0.9645022903801762}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:28:47,812][0m Trial 46 finished with value: 0.06574416752055026 and parameters: {'observation_period_num': 27, 'train_rates': 0.9226419947344504, 'learning_rate': 3.3251605363062776e-05, 'batch_size': 163, 'step_size': 4, 'gamma': 0.9643876152157154}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:29:19,600][0m Trial 47 finished with value: 0.06216118811695218 and parameters: {'observation_period_num': 20, 'train_rates': 0.8890096199136847, 'learning_rate': 9.52827074230551e-05, 'batch_size': 197, 'step_size': 5, 'gamma': 0.9634688420775998}. Best is trial 35 with value: 0.05444116849953079.[0m
[32m[I 2025-02-08 17:29:51,422][0m Trial 48 finished with value: 0.04582266131682056 and parameters: {'observation_period_num': 7, 'train_rates': 0.9053233051349603, 'learning_rate': 0.00010594736550149728, 'batch_size': 204, 'step_size': 5, 'gamma': 0.9316872660467107}. Best is trial 48 with value: 0.04582266131682056.[0m
[32m[I 2025-02-08 17:30:18,616][0m Trial 49 finished with value: 0.11480417102575302 and parameters: {'observation_period_num': 34, 'train_rates': 0.9600458437814553, 'learning_rate': 0.0001151590823327123, 'batch_size': 242, 'step_size': 2, 'gamma': 0.9281525153097446}. Best is trial 48 with value: 0.04582266131682056.[0m
seasonal_1 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_2
[32m[I 2025-02-08 17:30:18,626][0m A new study created in memory with name: no-name-9c879fa1-9500-42b1-94fd-bab53a0bdaa3[0m
[32m[I 2025-02-08 17:30:44,540][0m Trial 0 finished with value: 0.07859251807158894 and parameters: {'observation_period_num': 77, 'train_rates': 0.8654380051859505, 'learning_rate': 0.000281476551339398, 'batch_size': 244, 'step_size': 6, 'gamma': 0.8294463791549781}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:31:10,478][0m Trial 1 finished with value: 0.3402140843279568 and parameters: {'observation_period_num': 191, 'train_rates': 0.6492455865573616, 'learning_rate': 0.00011691932108977036, 'batch_size': 186, 'step_size': 14, 'gamma': 0.9685981878819743}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:31:41,831][0m Trial 2 finished with value: 0.20457069062128816 and parameters: {'observation_period_num': 110, 'train_rates': 0.8973141406941179, 'learning_rate': 2.389142197907233e-05, 'batch_size': 188, 'step_size': 11, 'gamma': 0.8230790607223698}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:32:22,727][0m Trial 3 finished with value: 0.27422890894942814 and parameters: {'observation_period_num': 237, 'train_rates': 0.7325245882762806, 'learning_rate': 0.0001670130300340572, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8826599469087697}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:32:54,157][0m Trial 4 finished with value: 0.2033020406961441 and parameters: {'observation_period_num': 103, 'train_rates': 0.9405558428263482, 'learning_rate': 2.7161900491181286e-05, 'batch_size': 199, 'step_size': 12, 'gamma': 0.9197451624529625}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:33:48,738][0m Trial 5 finished with value: 0.1588732009105261 and parameters: {'observation_period_num': 110, 'train_rates': 0.9368469944244506, 'learning_rate': 2.9114846620476093e-05, 'batch_size': 111, 'step_size': 11, 'gamma': 0.8595000122924006}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:34:30,392][0m Trial 6 finished with value: 0.5739196929577235 and parameters: {'observation_period_num': 76, 'train_rates': 0.8207167820921072, 'learning_rate': 4.779023430883481e-06, 'batch_size': 139, 'step_size': 4, 'gamma': 0.8379729534921903}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:37:54,648][0m Trial 7 finished with value: 0.420966510154024 and parameters: {'observation_period_num': 182, 'train_rates': 0.943377051689453, 'learning_rate': 1.8681986869896331e-06, 'batch_size': 27, 'step_size': 12, 'gamma': 0.9434045245480651}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:40:18,409][0m Trial 8 finished with value: 0.1930676327911035 and parameters: {'observation_period_num': 244, 'train_rates': 0.8597824680882483, 'learning_rate': 0.00033121142955092205, 'batch_size': 35, 'step_size': 11, 'gamma': 0.7520951421808492}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:41:45,381][0m Trial 9 finished with value: 0.15395492549254516 and parameters: {'observation_period_num': 195, 'train_rates': 0.8068603348597437, 'learning_rate': 5.3046739551708915e-05, 'batch_size': 58, 'step_size': 8, 'gamma': 0.7853857036793673}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:42:08,866][0m Trial 10 finished with value: 0.1907477797799973 and parameters: {'observation_period_num': 35, 'train_rates': 0.7283006155373344, 'learning_rate': 0.0009320428965402377, 'batch_size': 256, 'step_size': 1, 'gamma': 0.8990425147515528}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:43:11,682][0m Trial 11 finished with value: 0.27547304898269415 and parameters: {'observation_period_num': 169, 'train_rates': 0.7692421423043951, 'learning_rate': 7.532248427656727e-05, 'batch_size': 80, 'step_size': 7, 'gamma': 0.7815048484890617}. Best is trial 0 with value: 0.07859251807158894.[0m
[32m[I 2025-02-08 17:43:35,118][0m Trial 12 finished with value: 0.04691100336387434 and parameters: {'observation_period_num': 6, 'train_rates': 0.8424220546915675, 'learning_rate': 0.0005756570692116397, 'batch_size': 255, 'step_size': 6, 'gamma': 0.7983493877174735}. Best is trial 12 with value: 0.04691100336387434.[0m
[32m[I 2025-02-08 17:44:00,284][0m Trial 13 finished with value: 0.045757252233799055 and parameters: {'observation_period_num': 15, 'train_rates': 0.8656634719782971, 'learning_rate': 0.0007959185178521744, 'batch_size': 254, 'step_size': 5, 'gamma': 0.8023670874695941}. Best is trial 13 with value: 0.045757252233799055.[0m
[32m[I 2025-02-08 17:44:28,989][0m Trial 14 finished with value: 0.06436961144208908 and parameters: {'observation_period_num': 18, 'train_rates': 0.9852418191374893, 'learning_rate': 0.0008016413312975297, 'batch_size': 226, 'step_size': 3, 'gamma': 0.786260453536025}. Best is trial 13 with value: 0.045757252233799055.[0m
[32m[I 2025-02-08 17:45:06,732][0m Trial 15 finished with value: 0.03978930556467761 and parameters: {'observation_period_num': 7, 'train_rates': 0.8552139933060878, 'learning_rate': 0.00046482504765023807, 'batch_size': 156, 'step_size': 5, 'gamma': 0.8045557075601124}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:45:34,290][0m Trial 16 finished with value: 1.3896623485966733 and parameters: {'observation_period_num': 55, 'train_rates': 0.6080968436256926, 'learning_rate': 9.436247866241615e-06, 'batch_size': 168, 'step_size': 4, 'gamma': 0.7512265140831268}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:46:12,530][0m Trial 17 finished with value: 0.25611283935709483 and parameters: {'observation_period_num': 43, 'train_rates': 0.7771073643454544, 'learning_rate': 0.0003350034988115679, 'batch_size': 151, 'step_size': 1, 'gamma': 0.8548356869747433}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:46:39,775][0m Trial 18 finished with value: 0.12087515637230019 and parameters: {'observation_period_num': 153, 'train_rates': 0.8907820092587824, 'learning_rate': 0.00017793084897513924, 'batch_size': 223, 'step_size': 9, 'gamma': 0.8099250977303792}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:47:34,992][0m Trial 19 finished with value: 0.21530983229226705 and parameters: {'observation_period_num': 72, 'train_rates': 0.6995603741481919, 'learning_rate': 0.00046510419301914357, 'batch_size': 88, 'step_size': 3, 'gamma': 0.8429883623670613}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:48:05,037][0m Trial 20 finished with value: 0.18701839610389698 and parameters: {'observation_period_num': 5, 'train_rates': 0.8983219033730703, 'learning_rate': 1.2823211021778838e-05, 'batch_size': 212, 'step_size': 5, 'gamma': 0.8811172917221011}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:48:30,640][0m Trial 21 finished with value: 0.04941071069102414 and parameters: {'observation_period_num': 20, 'train_rates': 0.8463474186688374, 'learning_rate': 0.0006011969101603012, 'batch_size': 235, 'step_size': 6, 'gamma': 0.8043874303134277}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:49:04,742][0m Trial 22 finished with value: 0.039859152559576366 and parameters: {'observation_period_num': 7, 'train_rates': 0.833404698345479, 'learning_rate': 0.0005513772949176246, 'batch_size': 170, 'step_size': 8, 'gamma': 0.7733046156560367}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:49:39,445][0m Trial 23 finished with value: 0.056600154366022275 and parameters: {'observation_period_num': 37, 'train_rates': 0.806431676611986, 'learning_rate': 0.00022302014703764455, 'batch_size': 164, 'step_size': 9, 'gamma': 0.7658846026963084}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:50:24,264][0m Trial 24 finished with value: 0.21090179158629274 and parameters: {'observation_period_num': 59, 'train_rates': 0.7608049310831644, 'learning_rate': 8.339858818949557e-05, 'batch_size': 118, 'step_size': 8, 'gamma': 0.770330710920365}. Best is trial 15 with value: 0.03978930556467761.[0m
[32m[I 2025-02-08 17:50:58,404][0m Trial 25 finished with value: 0.03340636936663207 and parameters: {'observation_period_num': 24, 'train_rates': 0.8819138234244378, 'learning_rate': 0.0008994704379514357, 'batch_size': 173, 'step_size': 7, 'gamma': 0.8217413490348723}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:51:34,533][0m Trial 26 finished with value: 0.04238005141495716 and parameters: {'observation_period_num': 34, 'train_rates': 0.9138558921349997, 'learning_rate': 0.00040291982266675526, 'batch_size': 171, 'step_size': 9, 'gamma': 0.8166995287253076}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:52:09,183][0m Trial 27 finished with value: 0.06862825267016888 and parameters: {'observation_period_num': 89, 'train_rates': 0.8263101174798821, 'learning_rate': 0.0009704637310460373, 'batch_size': 159, 'step_size': 7, 'gamma': 0.7717499469730219}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:52:51,678][0m Trial 28 finished with value: 0.09345918491652067 and parameters: {'observation_period_num': 139, 'train_rates': 0.8755555074155985, 'learning_rate': 0.00012978456673265796, 'batch_size': 135, 'step_size': 7, 'gamma': 0.8556505111148005}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:53:27,317][0m Trial 29 finished with value: 0.06809153407812119 and parameters: {'observation_period_num': 55, 'train_rates': 0.9882525012867673, 'learning_rate': 0.00021692411477418702, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8183183212006166}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:53:55,829][0m Trial 30 finished with value: 0.1834916609028975 and parameters: {'observation_period_num': 27, 'train_rates': 0.7897276421914952, 'learning_rate': 0.00029206693215242334, 'batch_size': 206, 'step_size': 15, 'gamma': 0.8357671532991617}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:54:31,847][0m Trial 31 finished with value: 0.04705637976114929 and parameters: {'observation_period_num': 45, 'train_rates': 0.9135757503450236, 'learning_rate': 0.0004566368954299844, 'batch_size': 175, 'step_size': 9, 'gamma': 0.8254389683526125}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:55:13,853][0m Trial 32 finished with value: 0.048846239036367846 and parameters: {'observation_period_num': 28, 'train_rates': 0.9123156699650836, 'learning_rate': 0.0004118337000070981, 'batch_size': 146, 'step_size': 7, 'gamma': 0.9868866414614702}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:55:48,002][0m Trial 33 finished with value: 0.03588365763425827 and parameters: {'observation_period_num': 6, 'train_rates': 0.9559348994855171, 'learning_rate': 0.0005807869240191595, 'batch_size': 192, 'step_size': 10, 'gamma': 0.7956092443723259}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:56:21,688][0m Trial 34 finished with value: 0.045424919575452805 and parameters: {'observation_period_num': 6, 'train_rates': 0.9618721317636642, 'learning_rate': 0.0006163193516874325, 'batch_size': 192, 'step_size': 5, 'gamma': 0.7938558247821271}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:57:04,456][0m Trial 35 finished with value: 0.12994805140054821 and parameters: {'observation_period_num': 217, 'train_rates': 0.8760472047538393, 'learning_rate': 0.00012233469343332604, 'batch_size': 129, 'step_size': 13, 'gamma': 0.7695017303830751}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:57:40,046][0m Trial 36 finished with value: 0.08014824241399765 and parameters: {'observation_period_num': 66, 'train_rates': 0.951400073435156, 'learning_rate': 0.0002579986459174053, 'batch_size': 183, 'step_size': 10, 'gamma': 0.8293532506762381}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:58:16,981][0m Trial 37 finished with value: 0.06835202629116013 and parameters: {'observation_period_num': 85, 'train_rates': 0.8354215656082883, 'learning_rate': 0.0006631876713355236, 'batch_size': 154, 'step_size': 10, 'gamma': 0.7869814623902013}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:58:48,255][0m Trial 38 finished with value: 0.2318459004163742 and parameters: {'observation_period_num': 125, 'train_rates': 0.9335862271428818, 'learning_rate': 4.343996895038908e-05, 'batch_size': 201, 'step_size': 8, 'gamma': 0.8091195579981034}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 17:59:15,973][0m Trial 39 finished with value: 0.7119504102277305 and parameters: {'observation_period_num': 19, 'train_rates': 0.8561051557868499, 'learning_rate': 2.65699075416625e-06, 'batch_size': 217, 'step_size': 6, 'gamma': 0.8717822541234799}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 18:00:22,458][0m Trial 40 finished with value: 0.09624503673542113 and parameters: {'observation_period_num': 49, 'train_rates': 0.9640430369903437, 'learning_rate': 0.00015194180330358424, 'batch_size': 92, 'step_size': 4, 'gamma': 0.7599526861982714}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 18:00:59,610][0m Trial 41 finished with value: 0.041162586868985704 and parameters: {'observation_period_num': 31, 'train_rates': 0.9224695739757829, 'learning_rate': 0.00040669674746767343, 'batch_size': 175, 'step_size': 9, 'gamma': 0.8170114777997224}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 18:01:31,651][0m Trial 42 finished with value: 0.045822736850393474 and parameters: {'observation_period_num': 25, 'train_rates': 0.9195463632772208, 'learning_rate': 0.00047659313338741964, 'batch_size': 194, 'step_size': 11, 'gamma': 0.8490133541504602}. Best is trial 25 with value: 0.03340636936663207.[0m
[32m[I 2025-02-08 18:02:05,448][0m Trial 43 finished with value: 0.030553265388503118 and parameters: {'observation_period_num': 12, 'train_rates': 0.8904159767658559, 'learning_rate': 0.000959208801412133, 'batch_size': 181, 'step_size': 8, 'gamma': 0.7798213023168111}. Best is trial 43 with value: 0.030553265388503118.[0m
[32m[I 2025-02-08 18:02:39,195][0m Trial 44 finished with value: 0.03731533446732689 and parameters: {'observation_period_num': 11, 'train_rates': 0.8964764800787188, 'learning_rate': 0.000945562878264695, 'batch_size': 183, 'step_size': 6, 'gamma': 0.7763147630436551}. Best is trial 43 with value: 0.030553265388503118.[0m
[32m[I 2025-02-08 18:03:21,556][0m Trial 45 finished with value: 0.029831871994443843 and parameters: {'observation_period_num': 16, 'train_rates': 0.8872855754316614, 'learning_rate': 0.0009823880597525038, 'batch_size': 144, 'step_size': 7, 'gamma': 0.7940777193407992}. Best is trial 45 with value: 0.029831871994443843.[0m
[32m[I 2025-02-08 18:04:05,130][0m Trial 46 finished with value: 0.05094388066654485 and parameters: {'observation_period_num': 40, 'train_rates': 0.8950886451056275, 'learning_rate': 0.000999342426996141, 'batch_size': 140, 'step_size': 6, 'gamma': 0.7900939078271254}. Best is trial 45 with value: 0.029831871994443843.[0m
[32m[I 2025-02-08 18:04:41,124][0m Trial 47 finished with value: 0.029161537066102028 and parameters: {'observation_period_num': 17, 'train_rates': 0.9664019259493056, 'learning_rate': 0.0007057165460247904, 'batch_size': 182, 'step_size': 8, 'gamma': 0.9159744553767339}. Best is trial 47 with value: 0.029161537066102028.[0m
[32m[I 2025-02-08 18:05:12,022][0m Trial 48 finished with value: 0.7356106042861938 and parameters: {'observation_period_num': 23, 'train_rates': 0.9673389501741729, 'learning_rate': 1.1456065131874555e-06, 'batch_size': 209, 'step_size': 12, 'gamma': 0.9217321509978068}. Best is trial 47 with value: 0.029161537066102028.[0m
[32m[I 2025-02-08 18:06:00,685][0m Trial 49 finished with value: 0.1407273685085703 and parameters: {'observation_period_num': 92, 'train_rates': 0.9411700451490428, 'learning_rate': 0.000746673503664897, 'batch_size': 125, 'step_size': 7, 'gamma': 0.9119958972937459}. Best is trial 47 with value: 0.029161537066102028.[0m
seasonal_2 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: seasonal_3
[32m[I 2025-02-08 18:06:00,696][0m A new study created in memory with name: no-name-f24654b0-94f6-4d16-86e2-fd7fcdb4a6f2[0m
[32m[I 2025-02-08 18:06:38,156][0m Trial 0 finished with value: 0.868004618209512 and parameters: {'observation_period_num': 91, 'train_rates': 0.788780853586735, 'learning_rate': 8.491539309108289e-06, 'batch_size': 143, 'step_size': 1, 'gamma': 0.9137439233097406}. Best is trial 0 with value: 0.868004618209512.[0m
[32m[I 2025-02-08 18:07:20,783][0m Trial 1 finished with value: 0.7013957476383464 and parameters: {'observation_period_num': 244, 'train_rates': 0.8680381927394742, 'learning_rate': 2.703818848761968e-06, 'batch_size': 125, 'step_size': 11, 'gamma': 0.8145882910320031}. Best is trial 1 with value: 0.7013957476383464.[0m
[32m[I 2025-02-08 18:07:43,590][0m Trial 2 finished with value: 0.8555145889556166 and parameters: {'observation_period_num': 154, 'train_rates': 0.7012971899932269, 'learning_rate': 3.712208554753935e-06, 'batch_size': 237, 'step_size': 4, 'gamma': 0.9697097796465842}. Best is trial 1 with value: 0.7013957476383464.[0m
[32m[I 2025-02-08 18:08:18,038][0m Trial 3 finished with value: 0.233689358359889 and parameters: {'observation_period_num': 49, 'train_rates': 0.934832231878981, 'learning_rate': 1.3286851949556635e-05, 'batch_size': 182, 'step_size': 12, 'gamma': 0.8721287032008288}. Best is trial 3 with value: 0.233689358359889.[0m
[32m[I 2025-02-08 18:11:11,368][0m Trial 4 finished with value: 0.18219997078340733 and parameters: {'observation_period_num': 74, 'train_rates': 0.7108530694926816, 'learning_rate': 5.44436684923843e-05, 'batch_size': 27, 'step_size': 11, 'gamma': 0.8531454247745445}. Best is trial 4 with value: 0.18219997078340733.[0m
[32m[I 2025-02-08 18:11:47,335][0m Trial 5 finished with value: 0.20600804861046765 and parameters: {'observation_period_num': 88, 'train_rates': 0.7358426480870928, 'learning_rate': 7.80621303141482e-05, 'batch_size': 142, 'step_size': 10, 'gamma': 0.9468812039038481}. Best is trial 4 with value: 0.18219997078340733.[0m
[32m[I 2025-02-08 18:12:13,141][0m Trial 6 finished with value: 0.672828197479248 and parameters: {'observation_period_num': 213, 'train_rates': 0.9220472476320242, 'learning_rate': 3.454440192518549e-05, 'batch_size': 242, 'step_size': 2, 'gamma': 0.8015857123026394}. Best is trial 4 with value: 0.18219997078340733.[0m
[32m[I 2025-02-08 18:12:49,263][0m Trial 7 finished with value: 0.34121141092079443 and parameters: {'observation_period_num': 231, 'train_rates': 0.9402064621457444, 'learning_rate': 2.3572945259466835e-05, 'batch_size': 162, 'step_size': 15, 'gamma': 0.7856967100515864}. Best is trial 4 with value: 0.18219997078340733.[0m
[32m[I 2025-02-08 18:13:27,174][0m Trial 8 finished with value: 0.681052701577653 and parameters: {'observation_period_num': 200, 'train_rates': 0.7521760602177829, 'learning_rate': 3.5574409020272105e-06, 'batch_size': 133, 'step_size': 15, 'gamma': 0.8818600702304058}. Best is trial 4 with value: 0.18219997078340733.[0m
[32m[I 2025-02-08 18:14:20,500][0m Trial 9 finished with value: 0.3548163486431571 and parameters: {'observation_period_num': 138, 'train_rates': 0.6875374201194593, 'learning_rate': 3.180995800041963e-05, 'batch_size': 92, 'step_size': 2, 'gamma': 0.9297095792306312}. Best is trial 4 with value: 0.18219997078340733.[0m
[32m[I 2025-02-08 18:17:31,438][0m Trial 10 finished with value: 0.13006319435144614 and parameters: {'observation_period_num': 20, 'train_rates': 0.6174937166064237, 'learning_rate': 0.0003823799512152429, 'batch_size': 23, 'step_size': 7, 'gamma': 0.760738891082499}. Best is trial 10 with value: 0.13006319435144614.[0m
[32m[I 2025-02-08 18:21:08,628][0m Trial 11 finished with value: 0.1257378457218025 and parameters: {'observation_period_num': 6, 'train_rates': 0.605971828838668, 'learning_rate': 0.0009462005125538403, 'batch_size': 20, 'step_size': 7, 'gamma': 0.7654753553081134}. Best is trial 11 with value: 0.1257378457218025.[0m
[32m[I 2025-02-08 18:25:08,364][0m Trial 12 finished with value: 0.1198287999238374 and parameters: {'observation_period_num': 6, 'train_rates': 0.6038405815387218, 'learning_rate': 0.0009884194426668803, 'batch_size': 18, 'step_size': 6, 'gamma': 0.758177954278815}. Best is trial 12 with value: 0.1198287999238374.[0m
[32m[I 2025-02-08 18:26:17,622][0m Trial 13 finished with value: 0.11361860047997263 and parameters: {'observation_period_num': 13, 'train_rates': 0.6024538678814323, 'learning_rate': 0.0009190987937743115, 'batch_size': 65, 'step_size': 6, 'gamma': 0.7605670837919566}. Best is trial 13 with value: 0.11361860047997263.[0m
[32m[I 2025-02-08 18:27:26,510][0m Trial 14 finished with value: 0.16214106497595984 and parameters: {'observation_period_num': 39, 'train_rates': 0.6566669641918145, 'learning_rate': 0.00021262325564204518, 'batch_size': 68, 'step_size': 5, 'gamma': 0.8245308138137453}. Best is trial 13 with value: 0.11361860047997263.[0m
[32m[I 2025-02-08 18:28:42,339][0m Trial 15 finished with value: 0.1568205953235078 and parameters: {'observation_period_num': 52, 'train_rates': 0.6534400428639466, 'learning_rate': 0.0007056901043593719, 'batch_size': 61, 'step_size': 5, 'gamma': 0.7582774896118726}. Best is trial 13 with value: 0.11361860047997263.[0m
[32m[I 2025-02-08 18:30:11,949][0m Trial 16 finished with value: 0.089438247650019 and parameters: {'observation_period_num': 115, 'train_rates': 0.8636525080487447, 'learning_rate': 0.0001721418050758192, 'batch_size': 60, 'step_size': 8, 'gamma': 0.8308282365938027}. Best is trial 16 with value: 0.089438247650019.[0m
[32m[I 2025-02-08 18:31:13,757][0m Trial 17 finished with value: 0.09748811029465251 and parameters: {'observation_period_num': 163, 'train_rates': 0.8545916725127394, 'learning_rate': 0.00015679168239297355, 'batch_size': 87, 'step_size': 9, 'gamma': 0.8489632364525272}. Best is trial 16 with value: 0.089438247650019.[0m
[32m[I 2025-02-08 18:32:08,091][0m Trial 18 finished with value: 0.10268230823664692 and parameters: {'observation_period_num': 173, 'train_rates': 0.8717365104380657, 'learning_rate': 0.0001376923974725597, 'batch_size': 105, 'step_size': 9, 'gamma': 0.8465105027501266}. Best is trial 16 with value: 0.089438247650019.[0m
[32m[I 2025-02-08 18:33:05,186][0m Trial 19 finished with value: 0.07943742021041758 and parameters: {'observation_period_num': 105, 'train_rates': 0.8513449735478855, 'learning_rate': 0.00020665100674002764, 'batch_size': 95, 'step_size': 13, 'gamma': 0.9004416670790152}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:33:35,745][0m Trial 20 finished with value: 1.1029165983200073 and parameters: {'observation_period_num': 115, 'train_rates': 0.9896988040327546, 'learning_rate': 1.0445471513056685e-06, 'batch_size': 206, 'step_size': 14, 'gamma': 0.8952102230346882}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:34:35,307][0m Trial 21 finished with value: 0.07990696574411084 and parameters: {'observation_period_num': 125, 'train_rates': 0.8363328338376056, 'learning_rate': 0.00019586183275929273, 'batch_size': 88, 'step_size': 9, 'gamma': 0.8467728176233378}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:36:25,086][0m Trial 22 finished with value: 0.1421575842803693 and parameters: {'observation_period_num': 115, 'train_rates': 0.8239454507466819, 'learning_rate': 0.00033935630304131385, 'batch_size': 47, 'step_size': 13, 'gamma': 0.9040376123661972}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:37:15,951][0m Trial 23 finished with value: 0.10672922829786936 and parameters: {'observation_period_num': 126, 'train_rates': 0.8045728900981399, 'learning_rate': 8.731363220229908e-05, 'batch_size': 106, 'step_size': 8, 'gamma': 0.8283973517075877}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:38:21,340][0m Trial 24 finished with value: 0.11222600748001689 and parameters: {'observation_period_num': 94, 'train_rates': 0.9010190003912126, 'learning_rate': 0.000339501630172928, 'batch_size': 88, 'step_size': 13, 'gamma': 0.874551945261079}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:40:11,498][0m Trial 25 finished with value: 0.10562083830958918 and parameters: {'observation_period_num': 143, 'train_rates': 0.8384390229053146, 'learning_rate': 0.00022774610329434419, 'batch_size': 48, 'step_size': 10, 'gamma': 0.8521871102890253}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:40:58,661][0m Trial 26 finished with value: 0.2836005394023532 and parameters: {'observation_period_num': 180, 'train_rates': 0.7777755754738132, 'learning_rate': 8.57586544024035e-05, 'batch_size': 112, 'step_size': 8, 'gamma': 0.798569566368835}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:42:13,264][0m Trial 27 finished with value: 0.13965309045876667 and parameters: {'observation_period_num': 106, 'train_rates': 0.8904003202880812, 'learning_rate': 0.0005008066178488143, 'batch_size': 75, 'step_size': 12, 'gamma': 0.9307481238614622}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:44:15,520][0m Trial 28 finished with value: 0.08747796848972561 and parameters: {'observation_period_num': 71, 'train_rates': 0.8241271791424197, 'learning_rate': 0.00013899286812157094, 'batch_size': 43, 'step_size': 9, 'gamma': 0.8886332138126636}. Best is trial 19 with value: 0.07943742021041758.[0m
[32m[I 2025-02-08 18:46:19,402][0m Trial 29 finished with value: 0.07835220283009862 and parameters: {'observation_period_num': 87, 'train_rates': 0.7929924393966723, 'learning_rate': 5.870478016002619e-05, 'batch_size': 42, 'step_size': 10, 'gamma': 0.8914690418431275}. Best is trial 29 with value: 0.07835220283009862.[0m
[32m[I 2025-02-08 18:46:53,897][0m Trial 30 finished with value: 0.18840430669308522 and parameters: {'observation_period_num': 89, 'train_rates': 0.7960610868393897, 'learning_rate': 1.4582427223860345e-05, 'batch_size': 159, 'step_size': 11, 'gamma': 0.9179786240898643}. Best is trial 29 with value: 0.07835220283009862.[0m
[32m[I 2025-02-08 18:48:59,307][0m Trial 31 finished with value: 0.07208985417477183 and parameters: {'observation_period_num': 76, 'train_rates': 0.8205289235648265, 'learning_rate': 4.847706834344424e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.8970898888147684}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 18:51:14,128][0m Trial 32 finished with value: 0.18982842979640457 and parameters: {'observation_period_num': 70, 'train_rates': 0.7609882490853124, 'learning_rate': 5.5049818871877086e-05, 'batch_size': 37, 'step_size': 10, 'gamma': 0.9118224248790697}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 18:52:20,571][0m Trial 33 finished with value: 0.09924776091281619 and parameters: {'observation_period_num': 130, 'train_rates': 0.8170567791900074, 'learning_rate': 5.0553679139650166e-05, 'batch_size': 79, 'step_size': 12, 'gamma': 0.9532126254478472}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 18:53:05,176][0m Trial 34 finished with value: 0.38127275083456247 and parameters: {'observation_period_num': 59, 'train_rates': 0.7774975140475402, 'learning_rate': 7.312385838745261e-06, 'batch_size': 122, 'step_size': 13, 'gamma': 0.8668625892475328}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 18:54:03,350][0m Trial 35 finished with value: 0.07228732936912113 and parameters: {'observation_period_num': 96, 'train_rates': 0.8398746275043374, 'learning_rate': 0.00010286121080108376, 'batch_size': 96, 'step_size': 11, 'gamma': 0.9033721868640763}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 18:56:30,663][0m Trial 36 finished with value: 0.0757474789654747 and parameters: {'observation_period_num': 95, 'train_rates': 0.8471216371371656, 'learning_rate': 2.3889158693439894e-05, 'batch_size': 36, 'step_size': 11, 'gamma': 0.9001728097803423}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 18:58:57,451][0m Trial 37 finished with value: 0.17499033315107226 and parameters: {'observation_period_num': 37, 'train_rates': 0.7292720259374624, 'learning_rate': 1.686944201805896e-05, 'batch_size': 33, 'step_size': 11, 'gamma': 0.924371298361028}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:00:43,138][0m Trial 38 finished with value: 0.1265529115410412 and parameters: {'observation_period_num': 81, 'train_rates': 0.8820928990130557, 'learning_rate': 7.605942968343694e-06, 'batch_size': 52, 'step_size': 10, 'gamma': 0.9469840635404932}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:03:31,728][0m Trial 39 finished with value: 0.08774382516741752 and parameters: {'observation_period_num': 98, 'train_rates': 0.904048461736362, 'learning_rate': 2.292411992749061e-05, 'batch_size': 33, 'step_size': 12, 'gamma': 0.8655057656598294}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:03:55,804][0m Trial 40 finished with value: 0.09765368394884233 and parameters: {'observation_period_num': 63, 'train_rates': 0.7980888709055963, 'learning_rate': 4.451245607643251e-05, 'batch_size': 256, 'step_size': 11, 'gamma': 0.9716816666921128}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:04:34,194][0m Trial 41 finished with value: 0.08180136018422213 and parameters: {'observation_period_num': 100, 'train_rates': 0.8445546384236305, 'learning_rate': 8.039321357433626e-05, 'batch_size': 151, 'step_size': 14, 'gamma': 0.9035140562902599}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:05:31,700][0m Trial 42 finished with value: 0.09125919904973771 and parameters: {'observation_period_num': 80, 'train_rates': 0.8595007969123128, 'learning_rate': 2.5517798853772574e-05, 'batch_size': 99, 'step_size': 12, 'gamma': 0.8911699143577201}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:06:15,163][0m Trial 43 finished with value: 0.11509311606835526 and parameters: {'observation_period_num': 106, 'train_rates': 0.8107496423121641, 'learning_rate': 0.00011011352372792196, 'batch_size': 128, 'step_size': 10, 'gamma': 0.9850249112553799}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:06:50,260][0m Trial 44 finished with value: 0.2181479895634275 and parameters: {'observation_period_num': 84, 'train_rates': 0.9164944497792213, 'learning_rate': 1.0317393034421247e-05, 'batch_size': 173, 'step_size': 14, 'gamma': 0.9355651434137415}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:11:52,900][0m Trial 45 finished with value: 0.29501109581062757 and parameters: {'observation_period_num': 147, 'train_rates': 0.7794434292282004, 'learning_rate': 5.889907324722758e-05, 'batch_size': 16, 'step_size': 11, 'gamma': 0.8788185195055385}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:13:01,901][0m Trial 46 finished with value: 0.18931488022208215 and parameters: {'observation_period_num': 45, 'train_rates': 0.7499723739718104, 'learning_rate': 3.672978986516731e-05, 'batch_size': 73, 'step_size': 13, 'gamma': 0.9051661539145316}. Best is trial 31 with value: 0.07208985417477183.[0m
[32m[I 2025-02-08 19:14:37,975][0m Trial 47 finished with value: 0.060539376329291954 and parameters: {'observation_period_num': 64, 'train_rates': 0.8405860253933487, 'learning_rate': 6.540152282500592e-05, 'batch_size': 56, 'step_size': 11, 'gamma': 0.916592267479566}. Best is trial 47 with value: 0.060539376329291954.[0m
[32m[I 2025-02-08 19:16:14,244][0m Trial 48 finished with value: 0.0575125927214503 and parameters: {'observation_period_num': 28, 'train_rates': 0.8777386523083193, 'learning_rate': 2.1486496107241683e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9174659883555601}. Best is trial 48 with value: 0.0575125927214503.[0m
[32m[I 2025-02-08 19:17:51,989][0m Trial 49 finished with value: 0.06729913462335482 and parameters: {'observation_period_num': 30, 'train_rates': 0.9469213752291004, 'learning_rate': 2.199913444630417e-05, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9167806299875675}. Best is trial 48 with value: 0.0575125927214503.[0m
seasonal_3 „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©ÂåñÂØæË±°: resid
[32m[I 2025-02-08 19:17:51,998][0m A new study created in memory with name: no-name-5394bf58-1318-4778-8759-2fb3c50b5521[0m
[32m[I 2025-02-08 19:18:19,158][0m Trial 0 finished with value: 0.9320868849754333 and parameters: {'observation_period_num': 150, 'train_rates': 0.9319560555667286, 'learning_rate': 3.861169825763762e-06, 'batch_size': 237, 'step_size': 2, 'gamma': 0.9227012230722823}. Best is trial 0 with value: 0.9320868849754333.[0m
[32m[I 2025-02-08 19:18:51,574][0m Trial 1 finished with value: 0.20382876347125686 and parameters: {'observation_period_num': 142, 'train_rates': 0.8861535440315491, 'learning_rate': 3.0352190439489075e-05, 'batch_size': 179, 'step_size': 12, 'gamma': 0.78419863076017}. Best is trial 1 with value: 0.20382876347125686.[0m
[32m[I 2025-02-08 19:20:32,763][0m Trial 2 finished with value: 0.25581902677708485 and parameters: {'observation_period_num': 170, 'train_rates': 0.8990369064786095, 'learning_rate': 7.43646397607283e-06, 'batch_size': 54, 'step_size': 14, 'gamma': 0.8379576475052792}. Best is trial 1 with value: 0.20382876347125686.[0m
[32m[I 2025-02-08 19:20:55,938][0m Trial 3 finished with value: 0.21354196550113694 and parameters: {'observation_period_num': 96, 'train_rates': 0.7041329419441774, 'learning_rate': 0.0004950321311920186, 'batch_size': 249, 'step_size': 5, 'gamma': 0.928470430637377}. Best is trial 1 with value: 0.20382876347125686.[0m
[32m[I 2025-02-08 19:21:20,720][0m Trial 4 finished with value: 0.31593847274780273 and parameters: {'observation_period_num': 252, 'train_rates': 0.961380547582576, 'learning_rate': 5.0526879472086476e-05, 'batch_size': 247, 'step_size': 11, 'gamma': 0.806267849094612}. Best is trial 1 with value: 0.20382876347125686.[0m
[32m[I 2025-02-08 19:25:19,980][0m Trial 5 finished with value: 0.17447982273869594 and parameters: {'observation_period_num': 246, 'train_rates': 0.9782936184614711, 'learning_rate': 0.00013097831789306096, 'batch_size': 23, 'step_size': 8, 'gamma': 0.9482604239871544}. Best is trial 5 with value: 0.17447982273869594.[0m
[32m[I 2025-02-08 19:27:21,617][0m Trial 6 finished with value: 0.1685522845623517 and parameters: {'observation_period_num': 155, 'train_rates': 0.856123142599679, 'learning_rate': 1.5222426916670166e-05, 'batch_size': 43, 'step_size': 10, 'gamma': 0.813038076609915}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:27:50,186][0m Trial 7 finished with value: 0.7676950688308111 and parameters: {'observation_period_num': 142, 'train_rates': 0.7810344685857054, 'learning_rate': 2.939839159025988e-06, 'batch_size': 185, 'step_size': 4, 'gamma': 0.88849896267272}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:28:20,578][0m Trial 8 finished with value: 0.6011653105988515 and parameters: {'observation_period_num': 185, 'train_rates': 0.8624059837852492, 'learning_rate': 1.0496102303163522e-05, 'batch_size': 191, 'step_size': 2, 'gamma': 0.8804901928801863}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:28:49,006][0m Trial 9 finished with value: 0.7971617848027585 and parameters: {'observation_period_num': 143, 'train_rates': 0.7560553326288425, 'learning_rate': 5.704146145783753e-06, 'batch_size': 181, 'step_size': 5, 'gamma': 0.8075299482351347}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:29:42,217][0m Trial 10 finished with value: 0.7717319718412209 and parameters: {'observation_period_num': 27, 'train_rates': 0.6549568058637922, 'learning_rate': 1.3761327901695635e-06, 'batch_size': 91, 'step_size': 9, 'gamma': 0.7679258855774288}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:33:34,441][0m Trial 11 finished with value: 0.1947514479432036 and parameters: {'observation_period_num': 250, 'train_rates': 0.8245308486865967, 'learning_rate': 0.00018210682387713467, 'batch_size': 21, 'step_size': 8, 'gamma': 0.9846830572253665}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:34:36,898][0m Trial 12 finished with value: 0.20237188041210175 and parameters: {'observation_period_num': 209, 'train_rates': 0.9839780355632372, 'learning_rate': 7.653590437635938e-05, 'batch_size': 94, 'step_size': 8, 'gamma': 0.9733614195138576}. Best is trial 6 with value: 0.1685522845623517.[0m
[32m[I 2025-02-08 19:39:42,658][0m Trial 13 finished with value: 0.07593160105769527 and parameters: {'observation_period_num': 89, 'train_rates': 0.8310693168975695, 'learning_rate': 0.0009052912354660772, 'batch_size': 17, 'step_size': 10, 'gamma': 0.8452338563918659}. Best is trial 13 with value: 0.07593160105769527.[0m
[32m[I 2025-02-08 19:40:57,247][0m Trial 14 finished with value: 0.09614713495397124 and parameters: {'observation_period_num': 88, 'train_rates': 0.8319062554711363, 'learning_rate': 2.0402252327587033e-05, 'batch_size': 71, 'step_size': 15, 'gamma': 0.8474644671364263}. Best is trial 13 with value: 0.07593160105769527.[0m
[32m[I 2025-02-08 19:41:56,996][0m Trial 15 finished with value: 0.21681978132985177 and parameters: {'observation_period_num': 89, 'train_rates': 0.743775165674893, 'learning_rate': 0.000861357856315619, 'batch_size': 85, 'step_size': 15, 'gamma': 0.8416080636365599}. Best is trial 13 with value: 0.07593160105769527.[0m
[32m[I 2025-02-08 19:42:42,518][0m Trial 16 finished with value: 0.08631291792299566 and parameters: {'observation_period_num': 41, 'train_rates': 0.8074790031333077, 'learning_rate': 0.0002550870335316148, 'batch_size': 124, 'step_size': 13, 'gamma': 0.8541770854560637}. Best is trial 13 with value: 0.07593160105769527.[0m
[32m[I 2025-02-08 19:43:19,751][0m Trial 17 finished with value: 0.13436324395956695 and parameters: {'observation_period_num': 13, 'train_rates': 0.690163787734786, 'learning_rate': 0.00031365222494920844, 'batch_size': 138, 'step_size': 13, 'gamma': 0.9018200265217082}. Best is trial 13 with value: 0.07593160105769527.[0m
[32m[I 2025-02-08 19:44:04,657][0m Trial 18 finished with value: 0.06620336558061185 and parameters: {'observation_period_num': 54, 'train_rates': 0.8084759725439082, 'learning_rate': 0.0008946834296087796, 'batch_size': 120, 'step_size': 12, 'gamma': 0.8533110196923109}. Best is trial 18 with value: 0.06620336558061185.[0m
[32m[I 2025-02-08 19:44:40,565][0m Trial 19 finished with value: 0.1689951588710149 and parameters: {'observation_period_num': 63, 'train_rates': 0.6120032022563703, 'learning_rate': 0.0009029573800471859, 'batch_size': 128, 'step_size': 11, 'gamma': 0.8667118761896667}. Best is trial 18 with value: 0.06620336558061185.[0m
[32m[I 2025-02-08 19:45:06,815][0m Trial 20 finished with value: 0.23643467036757881 and parameters: {'observation_period_num': 113, 'train_rates': 0.7773576571809102, 'learning_rate': 0.0004234092410604409, 'batch_size': 216, 'step_size': 6, 'gamma': 0.819455991213901}. Best is trial 18 with value: 0.06620336558061185.[0m
[32m[I 2025-02-08 19:45:48,541][0m Trial 21 finished with value: 0.09131769369417261 and parameters: {'observation_period_num': 50, 'train_rates': 0.8165120773152782, 'learning_rate': 0.0003015046824003903, 'batch_size': 134, 'step_size': 13, 'gamma': 0.8662851477657629}. Best is trial 18 with value: 0.06620336558061185.[0m
[32m[I 2025-02-08 19:46:32,286][0m Trial 22 finished with value: 0.20286966507274315 and parameters: {'observation_period_num': 55, 'train_rates': 0.7243477336611159, 'learning_rate': 0.0008974263358394343, 'batch_size': 116, 'step_size': 12, 'gamma': 0.85141588971964}. Best is trial 18 with value: 0.06620336558061185.[0m
[32m[I 2025-02-08 19:47:08,419][0m Trial 23 finished with value: 0.04864126016644822 and parameters: {'observation_period_num': 38, 'train_rates': 0.802293405005989, 'learning_rate': 0.00019133550275598875, 'batch_size': 155, 'step_size': 10, 'gamma': 0.9025579221597654}. Best is trial 23 with value: 0.04864126016644822.[0m
[32m[I 2025-02-08 19:47:42,793][0m Trial 24 finished with value: 0.10125614541700517 and parameters: {'observation_period_num': 70, 'train_rates': 0.8498810451465176, 'learning_rate': 0.0005582008529848198, 'batch_size': 164, 'step_size': 10, 'gamma': 0.896407535197963}. Best is trial 23 with value: 0.04864126016644822.[0m
[32m[I 2025-02-08 19:48:20,599][0m Trial 25 finished with value: 0.04188446108909214 and parameters: {'observation_period_num': 6, 'train_rates': 0.8968072644099089, 'learning_rate': 0.00012414960247751462, 'batch_size': 156, 'step_size': 10, 'gamma': 0.9085904160180581}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:49:02,787][0m Trial 26 finished with value: 0.054943054257070315 and parameters: {'observation_period_num': 6, 'train_rates': 0.919805980613058, 'learning_rate': 0.00010399493588887083, 'batch_size': 147, 'step_size': 7, 'gamma': 0.9175019322229766}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:49:42,319][0m Trial 27 finished with value: 0.0496133965478368 and parameters: {'observation_period_num': 7, 'train_rates': 0.9260332304590192, 'learning_rate': 9.947469558737942e-05, 'batch_size': 159, 'step_size': 7, 'gamma': 0.920879715612369}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:50:21,487][0m Trial 28 finished with value: 0.07280675044586492 and parameters: {'observation_period_num': 32, 'train_rates': 0.941599090949149, 'learning_rate': 6.195684699027844e-05, 'batch_size': 158, 'step_size': 7, 'gamma': 0.9544514318868912}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:50:50,822][0m Trial 29 finished with value: 0.12603545188903809 and parameters: {'observation_period_num': 18, 'train_rates': 0.9295768536572493, 'learning_rate': 3.426676896866926e-05, 'batch_size': 211, 'step_size': 3, 'gamma': 0.9197001250499026}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:51:20,719][0m Trial 30 finished with value: 0.05185635286411178 and parameters: {'observation_period_num': 30, 'train_rates': 0.8975271018582895, 'learning_rate': 0.00012642834433255101, 'batch_size': 209, 'step_size': 9, 'gamma': 0.93539237115628}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:51:48,137][0m Trial 31 finished with value: 0.10657088955243428 and parameters: {'observation_period_num': 31, 'train_rates': 0.8958581777640027, 'learning_rate': 0.0001481503712335346, 'batch_size': 220, 'step_size': 9, 'gamma': 0.9391204215075956}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:52:24,523][0m Trial 32 finished with value: 0.04454635039970861 and parameters: {'observation_period_num': 5, 'train_rates': 0.879130507495155, 'learning_rate': 0.0001053826023953172, 'batch_size': 165, 'step_size': 9, 'gamma': 0.9079173166468749}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:53:02,568][0m Trial 33 finished with value: 0.0616043883562088 and parameters: {'observation_period_num': 8, 'train_rates': 0.873073275250491, 'learning_rate': 4.0069322954184085e-05, 'batch_size': 160, 'step_size': 7, 'gamma': 0.9066015246425446}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:53:33,996][0m Trial 34 finished with value: 0.05987126231193542 and parameters: {'observation_period_num': 19, 'train_rates': 0.9153107584711637, 'learning_rate': 8.996312432181354e-05, 'batch_size': 196, 'step_size': 11, 'gamma': 0.9110300654299144}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:54:12,010][0m Trial 35 finished with value: 0.05995726212859154 and parameters: {'observation_period_num': 42, 'train_rates': 0.9433569383233147, 'learning_rate': 0.00020229923090036886, 'batch_size': 170, 'step_size': 6, 'gamma': 0.8773251504728208}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:54:51,556][0m Trial 36 finished with value: 0.10077340524011885 and parameters: {'observation_period_num': 75, 'train_rates': 0.8817634122539976, 'learning_rate': 5.018412610444497e-05, 'batch_size': 151, 'step_size': 9, 'gamma': 0.8916071974407478}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:55:27,783][0m Trial 37 finished with value: 0.2639700472354889 and parameters: {'observation_period_num': 121, 'train_rates': 0.9553917681028894, 'learning_rate': 2.7541718544484888e-05, 'batch_size': 175, 'step_size': 11, 'gamma': 0.9621965020412732}. Best is trial 25 with value: 0.04188446108909214.[0m
[32m[I 2025-02-08 19:56:23,768][0m Trial 38 finished with value: 0.041880931144285385 and parameters: {'observation_period_num': 7, 'train_rates': 0.9124328692025402, 'learning_rate': 7.465923128929692e-05, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9300217540955196}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 19:57:17,370][0m Trial 39 finished with value: 0.05368319652664844 and parameters: {'observation_period_num': 40, 'train_rates': 0.8437433004985188, 'learning_rate': 6.78322665967536e-05, 'batch_size': 105, 'step_size': 9, 'gamma': 0.9305658555236964}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 19:58:08,585][0m Trial 40 finished with value: 0.18246696182550529 and parameters: {'observation_period_num': 21, 'train_rates': 0.7879162648526566, 'learning_rate': 0.00019443623915707176, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9503267866607086}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 19:58:50,627][0m Trial 41 finished with value: 0.05809546347427745 and parameters: {'observation_period_num': 6, 'train_rates': 0.9145562311356898, 'learning_rate': 9.926663368004436e-05, 'batch_size': 144, 'step_size': 8, 'gamma': 0.9176313036291208}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 19:59:21,977][0m Trial 42 finished with value: 0.043135339537492164 and parameters: {'observation_period_num': 6, 'train_rates': 0.8725866500374897, 'learning_rate': 0.0001416216219716486, 'batch_size': 198, 'step_size': 6, 'gamma': 0.9432156685140949}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 19:59:51,186][0m Trial 43 finished with value: 0.05578278811174616 and parameters: {'observation_period_num': 21, 'train_rates': 0.8652513998915716, 'learning_rate': 0.00014492589879611619, 'batch_size': 199, 'step_size': 5, 'gamma': 0.9416118917605222}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 20:00:24,640][0m Trial 44 finished with value: 0.04778246966929271 and parameters: {'observation_period_num': 36, 'train_rates': 0.8811674628441339, 'learning_rate': 0.00045180228239864576, 'batch_size': 175, 'step_size': 1, 'gamma': 0.9654102296867769}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 20:00:52,124][0m Trial 45 finished with value: 0.05237936194952544 and parameters: {'observation_period_num': 22, 'train_rates': 0.8867922201622188, 'learning_rate': 0.0004858773621703577, 'batch_size': 233, 'step_size': 1, 'gamma': 0.9655468336700634}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 20:01:25,338][0m Trial 46 finished with value: 0.23759828507900238 and parameters: {'observation_period_num': 224, 'train_rates': 0.9639090339637161, 'learning_rate': 5.9394694210322075e-05, 'batch_size': 182, 'step_size': 3, 'gamma': 0.9896443252973336}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 20:01:53,133][0m Trial 47 finished with value: 0.18914900227234913 and parameters: {'observation_period_num': 49, 'train_rates': 0.9108318923010429, 'learning_rate': 2.3706223869091365e-05, 'batch_size': 237, 'step_size': 1, 'gamma': 0.9761444369033502}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 20:03:17,234][0m Trial 48 finished with value: 0.05164882793061195 and parameters: {'observation_period_num': 30, 'train_rates': 0.8733558639076835, 'learning_rate': 4.655530252239038e-05, 'batch_size': 67, 'step_size': 4, 'gamma': 0.9533917163231757}. Best is trial 38 with value: 0.041880931144285385.[0m
[32m[I 2025-02-08 20:03:42,083][0m Trial 49 finished with value: 0.1283963679384805 and parameters: {'observation_period_num': 178, 'train_rates': 0.8424651131190712, 'learning_rate': 0.00033845084759543797, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9272640478123934}. Best is trial 38 with value: 0.041880931144285385.[0m
resid „ÅÆÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åó„Åü
ÊúÄÈÅ©„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„Åå best_hyperparameters_GOOG_iTransformer.json „Å´‰øùÂ≠ò„Åï„Çå„Åæ„Åó„Åü
Training trend component with params: {'observation_period_num': 10, 'train_rates': 0.8196662231749537, 'learning_rate': 0.0007884243027725701, 'batch_size': 122, 'step_size': 14, 'gamma': 0.8111189781780156}
Epoch 1/300, trend Loss: 0.4546 | 0.2392
Epoch 2/300, trend Loss: 0.1709 | 0.1922
Epoch 3/300, trend Loss: 0.1313 | 0.1242
Epoch 4/300, trend Loss: 0.1306 | 0.1189
Epoch 5/300, trend Loss: 0.1233 | 0.0981
Epoch 6/300, trend Loss: 0.1121 | 0.0886
Epoch 7/300, trend Loss: 0.1094 | 0.0863
Epoch 8/300, trend Loss: 0.1049 | 0.0989
Epoch 9/300, trend Loss: 0.1033 | 0.1163
Epoch 10/300, trend Loss: 0.1066 | 0.0761
Epoch 11/300, trend Loss: 0.1181 | 0.0662
Epoch 12/300, trend Loss: 0.1317 | 0.1036
Epoch 13/300, trend Loss: 0.1261 | 0.0684
Epoch 14/300, trend Loss: 0.1092 | 0.1210
Epoch 15/300, trend Loss: 0.1110 | 0.0941
Epoch 16/300, trend Loss: 0.1032 | 0.0624
Epoch 17/300, trend Loss: 0.1233 | 0.1167
Epoch 18/300, trend Loss: 0.1153 | 0.0707
Epoch 19/300, trend Loss: 0.1057 | 0.0645
Epoch 20/300, trend Loss: 0.1001 | 0.0574
Epoch 21/300, trend Loss: 0.0914 | 0.0631
Epoch 22/300, trend Loss: 0.0898 | 0.0599
Epoch 23/300, trend Loss: 0.0851 | 0.0470
Epoch 24/300, trend Loss: 0.0830 | 0.0464
Epoch 25/300, trend Loss: 0.0815 | 0.0465
Epoch 26/300, trend Loss: 0.0808 | 0.0466
Epoch 27/300, trend Loss: 0.0831 | 0.0626
Epoch 28/300, trend Loss: 0.0890 | 0.0747
Epoch 29/300, trend Loss: 0.0851 | 0.0443
Epoch 30/300, trend Loss: 0.0797 | 0.0435
Epoch 31/300, trend Loss: 0.0790 | 0.0437
Epoch 32/300, trend Loss: 0.0776 | 0.0468
Epoch 33/300, trend Loss: 0.0784 | 0.0579
Epoch 34/300, trend Loss: 0.0805 | 0.0506
Epoch 35/300, trend Loss: 0.0793 | 0.0421
Epoch 36/300, trend Loss: 0.0764 | 0.0399
Epoch 37/300, trend Loss: 0.0761 | 0.0486
Epoch 38/300, trend Loss: 0.0750 | 0.0460
Epoch 39/300, trend Loss: 0.0745 | 0.0394
Epoch 40/300, trend Loss: 0.0738 | 0.0401
Epoch 41/300, trend Loss: 0.0735 | 0.0424
Epoch 42/300, trend Loss: 0.0726 | 0.0402
Epoch 43/300, trend Loss: 0.0719 | 0.0376
Epoch 44/300, trend Loss: 0.0716 | 0.0375
Epoch 45/300, trend Loss: 0.0714 | 0.0386
Epoch 46/300, trend Loss: 0.0712 | 0.0382
Epoch 47/300, trend Loss: 0.0704 | 0.0367
Epoch 48/300, trend Loss: 0.0695 | 0.0362
Epoch 49/300, trend Loss: 0.0692 | 0.0365
Epoch 50/300, trend Loss: 0.0691 | 0.0365
Epoch 51/300, trend Loss: 0.0687 | 0.0355
Epoch 52/300, trend Loss: 0.0683 | 0.0356
Epoch 53/300, trend Loss: 0.0687 | 0.0357
Epoch 54/300, trend Loss: 0.0690 | 0.0353
Epoch 55/300, trend Loss: 0.0696 | 0.0362
Epoch 56/300, trend Loss: 0.0705 | 0.0370
Epoch 57/300, trend Loss: 0.0696 | 0.0380
Epoch 58/300, trend Loss: 0.0705 | 0.0353
Epoch 59/300, trend Loss: 0.0719 | 0.0352
Epoch 60/300, trend Loss: 0.0706 | 0.0350
Epoch 61/300, trend Loss: 0.0705 | 0.0392
Epoch 62/300, trend Loss: 0.0725 | 0.0339
Epoch 63/300, trend Loss: 0.0713 | 0.0362
Epoch 64/300, trend Loss: 0.0693 | 0.0358
Epoch 65/300, trend Loss: 0.0681 | 0.0334
Epoch 66/300, trend Loss: 0.0677 | 0.0328
Epoch 67/300, trend Loss: 0.0677 | 0.0346
Epoch 68/300, trend Loss: 0.0669 | 0.0339
Epoch 69/300, trend Loss: 0.0677 | 0.0333
Epoch 70/300, trend Loss: 0.0686 | 0.0340
Epoch 71/300, trend Loss: 0.0700 | 0.0343
Epoch 72/300, trend Loss: 0.0719 | 0.0360
Epoch 73/300, trend Loss: 0.0703 | 0.0369
Epoch 74/300, trend Loss: 0.0675 | 0.0354
Epoch 75/300, trend Loss: 0.0660 | 0.0338
Epoch 76/300, trend Loss: 0.0658 | 0.0328
Epoch 77/300, trend Loss: 0.0659 | 0.0324
Epoch 78/300, trend Loss: 0.0660 | 0.0315
Epoch 79/300, trend Loss: 0.0661 | 0.0314
Epoch 80/300, trend Loss: 0.0658 | 0.0331
Epoch 81/300, trend Loss: 0.0647 | 0.0320
Epoch 82/300, trend Loss: 0.0648 | 0.0322
Epoch 83/300, trend Loss: 0.0651 | 0.0318
Epoch 84/300, trend Loss: 0.0645 | 0.0314
Epoch 85/300, trend Loss: 0.0649 | 0.0315
Epoch 86/300, trend Loss: 0.0653 | 0.0317
Epoch 87/300, trend Loss: 0.0655 | 0.0310
Epoch 88/300, trend Loss: 0.0650 | 0.0317
Epoch 89/300, trend Loss: 0.0647 | 0.0310
Epoch 90/300, trend Loss: 0.0640 | 0.0313
Epoch 91/300, trend Loss: 0.0634 | 0.0313
Epoch 92/300, trend Loss: 0.0633 | 0.0308
Epoch 93/300, trend Loss: 0.0634 | 0.0305
Epoch 94/300, trend Loss: 0.0635 | 0.0304
Epoch 95/300, trend Loss: 0.0638 | 0.0306
Epoch 96/300, trend Loss: 0.0635 | 0.0308
Epoch 97/300, trend Loss: 0.0643 | 0.0311
Epoch 98/300, trend Loss: 0.0646 | 0.0322
Epoch 99/300, trend Loss: 0.0634 | 0.0309
Epoch 100/300, trend Loss: 0.0630 | 0.0312
Epoch 101/300, trend Loss: 0.0628 | 0.0313
Epoch 102/300, trend Loss: 0.0625 | 0.0309
Epoch 103/300, trend Loss: 0.0622 | 0.0305
Epoch 104/300, trend Loss: 0.0620 | 0.0302
Epoch 105/300, trend Loss: 0.0619 | 0.0301
Epoch 106/300, trend Loss: 0.0618 | 0.0298
Epoch 107/300, trend Loss: 0.0618 | 0.0298
Epoch 108/300, trend Loss: 0.0618 | 0.0298
Epoch 109/300, trend Loss: 0.0618 | 0.0298
Epoch 110/300, trend Loss: 0.0618 | 0.0298
Epoch 111/300, trend Loss: 0.0619 | 0.0298
Epoch 112/300, trend Loss: 0.0618 | 0.0299
Epoch 113/300, trend Loss: 0.0617 | 0.0302
Epoch 114/300, trend Loss: 0.0616 | 0.0304
Epoch 115/300, trend Loss: 0.0616 | 0.0302
Epoch 116/300, trend Loss: 0.0615 | 0.0299
Epoch 117/300, trend Loss: 0.0614 | 0.0297
Epoch 118/300, trend Loss: 0.0613 | 0.0296
Epoch 119/300, trend Loss: 0.0612 | 0.0295
Epoch 120/300, trend Loss: 0.0612 | 0.0294
Epoch 121/300, trend Loss: 0.0611 | 0.0294
Epoch 122/300, trend Loss: 0.0611 | 0.0294
Epoch 123/300, trend Loss: 0.0611 | 0.0294
Epoch 124/300, trend Loss: 0.0610 | 0.0295
Epoch 125/300, trend Loss: 0.0610 | 0.0295
Epoch 126/300, trend Loss: 0.0609 | 0.0295
Epoch 127/300, trend Loss: 0.0609 | 0.0295
Epoch 128/300, trend Loss: 0.0608 | 0.0295
Epoch 129/300, trend Loss: 0.0608 | 0.0294
Epoch 130/300, trend Loss: 0.0608 | 0.0293
Epoch 131/300, trend Loss: 0.0607 | 0.0293
Epoch 132/300, trend Loss: 0.0607 | 0.0293
Epoch 133/300, trend Loss: 0.0607 | 0.0293
Epoch 134/300, trend Loss: 0.0607 | 0.0293
Epoch 135/300, trend Loss: 0.0606 | 0.0293
Epoch 136/300, trend Loss: 0.0606 | 0.0293
Epoch 137/300, trend Loss: 0.0606 | 0.0293
Epoch 138/300, trend Loss: 0.0605 | 0.0292
Epoch 139/300, trend Loss: 0.0605 | 0.0292
Epoch 140/300, trend Loss: 0.0605 | 0.0292
Epoch 141/300, trend Loss: 0.0605 | 0.0292
Epoch 142/300, trend Loss: 0.0604 | 0.0292
Epoch 143/300, trend Loss: 0.0604 | 0.0292
Epoch 144/300, trend Loss: 0.0604 | 0.0292
Epoch 145/300, trend Loss: 0.0604 | 0.0291
Epoch 146/300, trend Loss: 0.0604 | 0.0291
Epoch 147/300, trend Loss: 0.0603 | 0.0291
Epoch 148/300, trend Loss: 0.0603 | 0.0291
Epoch 149/300, trend Loss: 0.0603 | 0.0291
Epoch 150/300, trend Loss: 0.0603 | 0.0291
Epoch 151/300, trend Loss: 0.0603 | 0.0291
Epoch 152/300, trend Loss: 0.0602 | 0.0291
Epoch 153/300, trend Loss: 0.0602 | 0.0291
Epoch 154/300, trend Loss: 0.0602 | 0.0291
Epoch 155/300, trend Loss: 0.0602 | 0.0290
Epoch 156/300, trend Loss: 0.0602 | 0.0290
Epoch 157/300, trend Loss: 0.0602 | 0.0290
Epoch 158/300, trend Loss: 0.0601 | 0.0290
Epoch 159/300, trend Loss: 0.0601 | 0.0290
Epoch 160/300, trend Loss: 0.0601 | 0.0290
Epoch 161/300, trend Loss: 0.0601 | 0.0290
Epoch 162/300, trend Loss: 0.0601 | 0.0290
Epoch 163/300, trend Loss: 0.0601 | 0.0290
Epoch 164/300, trend Loss: 0.0600 | 0.0290
Epoch 165/300, trend Loss: 0.0600 | 0.0290
Epoch 166/300, trend Loss: 0.0600 | 0.0290
Epoch 167/300, trend Loss: 0.0600 | 0.0290
Epoch 168/300, trend Loss: 0.0600 | 0.0289
Epoch 169/300, trend Loss: 0.0600 | 0.0289
Epoch 170/300, trend Loss: 0.0600 | 0.0289
Epoch 171/300, trend Loss: 0.0600 | 0.0289
Epoch 172/300, trend Loss: 0.0600 | 0.0289
Epoch 173/300, trend Loss: 0.0599 | 0.0289
Epoch 174/300, trend Loss: 0.0599 | 0.0289
Epoch 175/300, trend Loss: 0.0599 | 0.0289
Epoch 176/300, trend Loss: 0.0599 | 0.0289
Epoch 177/300, trend Loss: 0.0599 | 0.0289
Epoch 178/300, trend Loss: 0.0599 | 0.0289
Epoch 179/300, trend Loss: 0.0599 | 0.0289
Epoch 180/300, trend Loss: 0.0599 | 0.0289
Epoch 181/300, trend Loss: 0.0599 | 0.0289
Epoch 182/300, trend Loss: 0.0599 | 0.0289
Epoch 183/300, trend Loss: 0.0598 | 0.0289
Epoch 184/300, trend Loss: 0.0598 | 0.0289
Epoch 185/300, trend Loss: 0.0598 | 0.0289
Epoch 186/300, trend Loss: 0.0598 | 0.0289
Epoch 187/300, trend Loss: 0.0598 | 0.0289
Epoch 188/300, trend Loss: 0.0598 | 0.0289
Epoch 189/300, trend Loss: 0.0598 | 0.0288
Epoch 190/300, trend Loss: 0.0598 | 0.0288
Epoch 191/300, trend Loss: 0.0598 | 0.0288
Epoch 192/300, trend Loss: 0.0598 | 0.0288
Epoch 193/300, trend Loss: 0.0598 | 0.0288
Epoch 194/300, trend Loss: 0.0598 | 0.0288
Epoch 195/300, trend Loss: 0.0598 | 0.0288
Epoch 196/300, trend Loss: 0.0598 | 0.0288
Epoch 197/300, trend Loss: 0.0597 | 0.0288
Epoch 198/300, trend Loss: 0.0597 | 0.0288
Epoch 199/300, trend Loss: 0.0597 | 0.0288
Epoch 200/300, trend Loss: 0.0597 | 0.0288
Epoch 201/300, trend Loss: 0.0597 | 0.0288
Epoch 202/300, trend Loss: 0.0597 | 0.0288
Epoch 203/300, trend Loss: 0.0597 | 0.0288
Epoch 204/300, trend Loss: 0.0597 | 0.0288
Epoch 205/300, trend Loss: 0.0597 | 0.0288
Epoch 206/300, trend Loss: 0.0597 | 0.0288
Epoch 207/300, trend Loss: 0.0597 | 0.0288
Epoch 208/300, trend Loss: 0.0597 | 0.0288
Epoch 209/300, trend Loss: 0.0597 | 0.0288
Epoch 210/300, trend Loss: 0.0597 | 0.0288
Epoch 211/300, trend Loss: 0.0597 | 0.0288
Epoch 212/300, trend Loss: 0.0597 | 0.0288
Epoch 213/300, trend Loss: 0.0597 | 0.0288
Epoch 214/300, trend Loss: 0.0597 | 0.0288
Epoch 215/300, trend Loss: 0.0597 | 0.0288
Epoch 216/300, trend Loss: 0.0597 | 0.0288
Epoch 217/300, trend Loss: 0.0597 | 0.0288
Epoch 218/300, trend Loss: 0.0597 | 0.0288
Epoch 219/300, trend Loss: 0.0596 | 0.0288
Epoch 220/300, trend Loss: 0.0596 | 0.0288
Epoch 221/300, trend Loss: 0.0596 | 0.0288
Epoch 222/300, trend Loss: 0.0596 | 0.0288
Epoch 223/300, trend Loss: 0.0596 | 0.0288
Epoch 224/300, trend Loss: 0.0596 | 0.0288
Epoch 225/300, trend Loss: 0.0596 | 0.0288
Epoch 226/300, trend Loss: 0.0596 | 0.0288
Epoch 227/300, trend Loss: 0.0596 | 0.0288
Epoch 228/300, trend Loss: 0.0596 | 0.0288
Epoch 229/300, trend Loss: 0.0596 | 0.0288
Epoch 230/300, trend Loss: 0.0596 | 0.0288
Epoch 231/300, trend Loss: 0.0596 | 0.0288
Epoch 232/300, trend Loss: 0.0596 | 0.0288
Epoch 233/300, trend Loss: 0.0596 | 0.0288
Epoch 234/300, trend Loss: 0.0596 | 0.0287
Epoch 235/300, trend Loss: 0.0596 | 0.0287
Epoch 236/300, trend Loss: 0.0596 | 0.0287
Epoch 237/300, trend Loss: 0.0596 | 0.0287
Epoch 238/300, trend Loss: 0.0596 | 0.0287
Epoch 239/300, trend Loss: 0.0596 | 0.0287
Epoch 240/300, trend Loss: 0.0596 | 0.0287
Epoch 241/300, trend Loss: 0.0596 | 0.0287
Epoch 242/300, trend Loss: 0.0596 | 0.0287
Epoch 243/300, trend Loss: 0.0596 | 0.0287
Epoch 244/300, trend Loss: 0.0596 | 0.0287
Epoch 245/300, trend Loss: 0.0596 | 0.0287
Epoch 246/300, trend Loss: 0.0596 | 0.0287
Epoch 247/300, trend Loss: 0.0596 | 0.0287
Epoch 248/300, trend Loss: 0.0596 | 0.0287
Epoch 249/300, trend Loss: 0.0596 | 0.0287
Epoch 250/300, trend Loss: 0.0596 | 0.0287
Epoch 251/300, trend Loss: 0.0596 | 0.0287
Epoch 252/300, trend Loss: 0.0596 | 0.0287
Epoch 253/300, trend Loss: 0.0596 | 0.0287
Epoch 254/300, trend Loss: 0.0596 | 0.0287
Epoch 255/300, trend Loss: 0.0596 | 0.0287
Epoch 256/300, trend Loss: 0.0596 | 0.0287
Epoch 257/300, trend Loss: 0.0596 | 0.0287
Epoch 258/300, trend Loss: 0.0596 | 0.0287
Epoch 259/300, trend Loss: 0.0596 | 0.0287
Epoch 260/300, trend Loss: 0.0596 | 0.0287
Epoch 261/300, trend Loss: 0.0596 | 0.0287
Epoch 262/300, trend Loss: 0.0596 | 0.0287
Epoch 263/300, trend Loss: 0.0596 | 0.0287
Epoch 264/300, trend Loss: 0.0596 | 0.0287
Epoch 265/300, trend Loss: 0.0596 | 0.0287
Epoch 266/300, trend Loss: 0.0596 | 0.0287
Epoch 267/300, trend Loss: 0.0596 | 0.0287
Epoch 268/300, trend Loss: 0.0596 | 0.0287
Epoch 269/300, trend Loss: 0.0596 | 0.0287
Epoch 270/300, trend Loss: 0.0596 | 0.0287
Epoch 271/300, trend Loss: 0.0596 | 0.0287
Epoch 272/300, trend Loss: 0.0596 | 0.0287
Epoch 273/300, trend Loss: 0.0596 | 0.0287
Epoch 274/300, trend Loss: 0.0596 | 0.0287
Epoch 275/300, trend Loss: 0.0596 | 0.0287
Epoch 276/300, trend Loss: 0.0596 | 0.0287
Epoch 277/300, trend Loss: 0.0596 | 0.0287
Epoch 278/300, trend Loss: 0.0595 | 0.0287
Epoch 279/300, trend Loss: 0.0595 | 0.0287
Epoch 280/300, trend Loss: 0.0595 | 0.0287
Epoch 281/300, trend Loss: 0.0595 | 0.0287
Epoch 282/300, trend Loss: 0.0595 | 0.0287
Epoch 283/300, trend Loss: 0.0595 | 0.0287
Epoch 284/300, trend Loss: 0.0595 | 0.0287
Epoch 285/300, trend Loss: 0.0595 | 0.0287
Epoch 286/300, trend Loss: 0.0595 | 0.0287
Epoch 287/300, trend Loss: 0.0595 | 0.0287
Epoch 288/300, trend Loss: 0.0595 | 0.0287
Epoch 289/300, trend Loss: 0.0595 | 0.0287
Epoch 290/300, trend Loss: 0.0595 | 0.0287
Epoch 291/300, trend Loss: 0.0595 | 0.0287
Epoch 292/300, trend Loss: 0.0595 | 0.0287
Epoch 293/300, trend Loss: 0.0595 | 0.0287
Epoch 294/300, trend Loss: 0.0595 | 0.0287
Epoch 295/300, trend Loss: 0.0595 | 0.0287
Epoch 296/300, trend Loss: 0.0595 | 0.0287
Epoch 297/300, trend Loss: 0.0595 | 0.0287
Epoch 298/300, trend Loss: 0.0595 | 0.0287
Epoch 299/300, trend Loss: 0.0595 | 0.0287
Epoch 300/300, trend Loss: 0.0595 | 0.0287
Training seasonal_0 component with params: {'observation_period_num': 28, 'train_rates': 0.9639146165227073, 'learning_rate': 1.2257272337803675e-05, 'batch_size': 17, 'step_size': 14, 'gamma': 0.9864625184175219}
Epoch 1/300, seasonal_0 Loss: 0.7079 | 0.6637
Epoch 2/300, seasonal_0 Loss: 0.3428 | 0.4652
Epoch 3/300, seasonal_0 Loss: 0.2513 | 0.3598
Epoch 4/300, seasonal_0 Loss: 0.2156 | 0.3049
Epoch 5/300, seasonal_0 Loss: 0.1950 | 0.2702
Epoch 6/300, seasonal_0 Loss: 0.1800 | 0.2445
Epoch 7/300, seasonal_0 Loss: 0.1684 | 0.2241
Epoch 8/300, seasonal_0 Loss: 0.1590 | 0.2078
Epoch 9/300, seasonal_0 Loss: 0.1513 | 0.1936
Epoch 10/300, seasonal_0 Loss: 0.1447 | 0.1814
Epoch 11/300, seasonal_0 Loss: 0.1392 | 0.1707
Epoch 12/300, seasonal_0 Loss: 0.1345 | 0.1616
Epoch 13/300, seasonal_0 Loss: 0.1306 | 0.1538
Epoch 14/300, seasonal_0 Loss: 0.1273 | 0.1472
Epoch 15/300, seasonal_0 Loss: 0.1244 | 0.1417
Epoch 16/300, seasonal_0 Loss: 0.1220 | 0.1369
Epoch 17/300, seasonal_0 Loss: 0.1197 | 0.1326
Epoch 18/300, seasonal_0 Loss: 0.1177 | 0.1288
Epoch 19/300, seasonal_0 Loss: 0.1158 | 0.1253
Epoch 20/300, seasonal_0 Loss: 0.1140 | 0.1222
Epoch 21/300, seasonal_0 Loss: 0.1124 | 0.1194
Epoch 22/300, seasonal_0 Loss: 0.1109 | 0.1168
Epoch 23/300, seasonal_0 Loss: 0.1094 | 0.1144
Epoch 24/300, seasonal_0 Loss: 0.1080 | 0.1123
Epoch 25/300, seasonal_0 Loss: 0.1067 | 0.1103
Epoch 26/300, seasonal_0 Loss: 0.1055 | 0.1085
Epoch 27/300, seasonal_0 Loss: 0.1042 | 0.1068
Epoch 28/300, seasonal_0 Loss: 0.1031 | 0.1052
Epoch 29/300, seasonal_0 Loss: 0.1020 | 0.1038
Epoch 30/300, seasonal_0 Loss: 0.1009 | 0.1025
Epoch 31/300, seasonal_0 Loss: 0.0999 | 0.1013
Epoch 32/300, seasonal_0 Loss: 0.0989 | 0.1002
Epoch 33/300, seasonal_0 Loss: 0.0980 | 0.0991
Epoch 34/300, seasonal_0 Loss: 0.0971 | 0.0981
Epoch 35/300, seasonal_0 Loss: 0.0963 | 0.0972
Epoch 36/300, seasonal_0 Loss: 0.0955 | 0.0964
Epoch 37/300, seasonal_0 Loss: 0.0947 | 0.0956
Epoch 38/300, seasonal_0 Loss: 0.0940 | 0.0949
Epoch 39/300, seasonal_0 Loss: 0.0933 | 0.0942
Epoch 40/300, seasonal_0 Loss: 0.0926 | 0.0935
Epoch 41/300, seasonal_0 Loss: 0.0919 | 0.0929
Epoch 42/300, seasonal_0 Loss: 0.0913 | 0.0924
Epoch 43/300, seasonal_0 Loss: 0.0908 | 0.0921
Epoch 44/300, seasonal_0 Loss: 0.0902 | 0.0915
Epoch 45/300, seasonal_0 Loss: 0.0896 | 0.0909
Epoch 46/300, seasonal_0 Loss: 0.0890 | 0.0902
Epoch 47/300, seasonal_0 Loss: 0.0885 | 0.0895
Epoch 48/300, seasonal_0 Loss: 0.0879 | 0.0888
Epoch 49/300, seasonal_0 Loss: 0.0874 | 0.0880
Epoch 50/300, seasonal_0 Loss: 0.0869 | 0.0875
Epoch 51/300, seasonal_0 Loss: 0.0864 | 0.0865
Epoch 52/300, seasonal_0 Loss: 0.0860 | 0.0856
Epoch 53/300, seasonal_0 Loss: 0.0855 | 0.0847
Epoch 54/300, seasonal_0 Loss: 0.0850 | 0.0839
Epoch 55/300, seasonal_0 Loss: 0.0846 | 0.0830
Epoch 56/300, seasonal_0 Loss: 0.0842 | 0.0821
Epoch 57/300, seasonal_0 Loss: 0.0838 | 0.0815
Epoch 58/300, seasonal_0 Loss: 0.0833 | 0.0805
Epoch 59/300, seasonal_0 Loss: 0.0830 | 0.0795
Epoch 60/300, seasonal_0 Loss: 0.0826 | 0.0786
Epoch 61/300, seasonal_0 Loss: 0.0822 | 0.0777
Epoch 62/300, seasonal_0 Loss: 0.0818 | 0.0767
Epoch 63/300, seasonal_0 Loss: 0.0815 | 0.0758
Epoch 64/300, seasonal_0 Loss: 0.0811 | 0.0752
Epoch 65/300, seasonal_0 Loss: 0.0808 | 0.0742
Epoch 66/300, seasonal_0 Loss: 0.0805 | 0.0733
Epoch 67/300, seasonal_0 Loss: 0.0801 | 0.0725
Epoch 68/300, seasonal_0 Loss: 0.0798 | 0.0717
Epoch 69/300, seasonal_0 Loss: 0.0795 | 0.0710
Epoch 70/300, seasonal_0 Loss: 0.0792 | 0.0703
Epoch 71/300, seasonal_0 Loss: 0.0789 | 0.0697
Epoch 72/300, seasonal_0 Loss: 0.0786 | 0.0691
Epoch 73/300, seasonal_0 Loss: 0.0783 | 0.0686
Epoch 74/300, seasonal_0 Loss: 0.0780 | 0.0680
Epoch 75/300, seasonal_0 Loss: 0.0777 | 0.0675
Epoch 76/300, seasonal_0 Loss: 0.0774 | 0.0671
Epoch 77/300, seasonal_0 Loss: 0.0772 | 0.0666
Epoch 78/300, seasonal_0 Loss: 0.0769 | 0.0662
Epoch 79/300, seasonal_0 Loss: 0.0766 | 0.0659
Epoch 80/300, seasonal_0 Loss: 0.0763 | 0.0655
Epoch 81/300, seasonal_0 Loss: 0.0761 | 0.0651
Epoch 82/300, seasonal_0 Loss: 0.0758 | 0.0648
Epoch 83/300, seasonal_0 Loss: 0.0755 | 0.0645
Epoch 84/300, seasonal_0 Loss: 0.0753 | 0.0641
Epoch 85/300, seasonal_0 Loss: 0.0750 | 0.0638
Epoch 86/300, seasonal_0 Loss: 0.0748 | 0.0636
Epoch 87/300, seasonal_0 Loss: 0.0745 | 0.0633
Epoch 88/300, seasonal_0 Loss: 0.0743 | 0.0630
Epoch 89/300, seasonal_0 Loss: 0.0741 | 0.0627
Epoch 90/300, seasonal_0 Loss: 0.0738 | 0.0624
Epoch 91/300, seasonal_0 Loss: 0.0736 | 0.0622
Epoch 92/300, seasonal_0 Loss: 0.0734 | 0.0619
Epoch 93/300, seasonal_0 Loss: 0.0732 | 0.0617
Epoch 94/300, seasonal_0 Loss: 0.0730 | 0.0615
Epoch 95/300, seasonal_0 Loss: 0.0727 | 0.0612
Epoch 96/300, seasonal_0 Loss: 0.0725 | 0.0610
Epoch 97/300, seasonal_0 Loss: 0.0723 | 0.0608
Epoch 98/300, seasonal_0 Loss: 0.0721 | 0.0605
Epoch 99/300, seasonal_0 Loss: 0.0719 | 0.0603
Epoch 100/300, seasonal_0 Loss: 0.0717 | 0.0601
Epoch 101/300, seasonal_0 Loss: 0.0715 | 0.0599
Epoch 102/300, seasonal_0 Loss: 0.0713 | 0.0597
Epoch 103/300, seasonal_0 Loss: 0.0711 | 0.0595
Epoch 104/300, seasonal_0 Loss: 0.0709 | 0.0593
Epoch 105/300, seasonal_0 Loss: 0.0707 | 0.0591
Epoch 106/300, seasonal_0 Loss: 0.0706 | 0.0589
Epoch 107/300, seasonal_0 Loss: 0.0704 | 0.0587
Epoch 108/300, seasonal_0 Loss: 0.0702 | 0.0585
Epoch 109/300, seasonal_0 Loss: 0.0700 | 0.0584
Epoch 110/300, seasonal_0 Loss: 0.0698 | 0.0582
Epoch 111/300, seasonal_0 Loss: 0.0697 | 0.0580
Epoch 112/300, seasonal_0 Loss: 0.0695 | 0.0579
Epoch 113/300, seasonal_0 Loss: 0.0693 | 0.0577
Epoch 114/300, seasonal_0 Loss: 0.0692 | 0.0575
Epoch 115/300, seasonal_0 Loss: 0.0690 | 0.0573
Epoch 116/300, seasonal_0 Loss: 0.0688 | 0.0572
Epoch 117/300, seasonal_0 Loss: 0.0687 | 0.0570
Epoch 118/300, seasonal_0 Loss: 0.0685 | 0.0569
Epoch 119/300, seasonal_0 Loss: 0.0683 | 0.0567
Epoch 120/300, seasonal_0 Loss: 0.0682 | 0.0566
Epoch 121/300, seasonal_0 Loss: 0.0680 | 0.0564
Epoch 122/300, seasonal_0 Loss: 0.0679 | 0.0563
Epoch 123/300, seasonal_0 Loss: 0.0677 | 0.0561
Epoch 124/300, seasonal_0 Loss: 0.0676 | 0.0560
Epoch 125/300, seasonal_0 Loss: 0.0674 | 0.0559
Epoch 126/300, seasonal_0 Loss: 0.0673 | 0.0557
Epoch 127/300, seasonal_0 Loss: 0.0671 | 0.0556
Epoch 128/300, seasonal_0 Loss: 0.0670 | 0.0554
Epoch 129/300, seasonal_0 Loss: 0.0668 | 0.0553
Epoch 130/300, seasonal_0 Loss: 0.0667 | 0.0552
Epoch 131/300, seasonal_0 Loss: 0.0665 | 0.0551
Epoch 132/300, seasonal_0 Loss: 0.0664 | 0.0549
Epoch 133/300, seasonal_0 Loss: 0.0662 | 0.0548
Epoch 134/300, seasonal_0 Loss: 0.0661 | 0.0547
Epoch 135/300, seasonal_0 Loss: 0.0659 | 0.0546
Epoch 136/300, seasonal_0 Loss: 0.0658 | 0.0544
Epoch 137/300, seasonal_0 Loss: 0.0657 | 0.0543
Epoch 138/300, seasonal_0 Loss: 0.0655 | 0.0542
Epoch 139/300, seasonal_0 Loss: 0.0654 | 0.0541
Epoch 140/300, seasonal_0 Loss: 0.0652 | 0.0540
Epoch 141/300, seasonal_0 Loss: 0.0651 | 0.0539
Epoch 142/300, seasonal_0 Loss: 0.0650 | 0.0538
Epoch 143/300, seasonal_0 Loss: 0.0648 | 0.0537
Epoch 144/300, seasonal_0 Loss: 0.0647 | 0.0536
Epoch 145/300, seasonal_0 Loss: 0.0646 | 0.0535
Epoch 146/300, seasonal_0 Loss: 0.0644 | 0.0534
Epoch 147/300, seasonal_0 Loss: 0.0643 | 0.0533
Epoch 148/300, seasonal_0 Loss: 0.0642 | 0.0531
Epoch 149/300, seasonal_0 Loss: 0.0640 | 0.0531
Epoch 150/300, seasonal_0 Loss: 0.0639 | 0.0530
Epoch 151/300, seasonal_0 Loss: 0.0638 | 0.0529
Epoch 152/300, seasonal_0 Loss: 0.0636 | 0.0528
Epoch 153/300, seasonal_0 Loss: 0.0635 | 0.0527
Epoch 154/300, seasonal_0 Loss: 0.0634 | 0.0526
Epoch 155/300, seasonal_0 Loss: 0.0632 | 0.0525
Epoch 156/300, seasonal_0 Loss: 0.0631 | 0.0524
Epoch 157/300, seasonal_0 Loss: 0.0630 | 0.0523
Epoch 158/300, seasonal_0 Loss: 0.0629 | 0.0522
Epoch 159/300, seasonal_0 Loss: 0.0627 | 0.0522
Epoch 160/300, seasonal_0 Loss: 0.0626 | 0.0521
Epoch 161/300, seasonal_0 Loss: 0.0625 | 0.0520
Epoch 162/300, seasonal_0 Loss: 0.0623 | 0.0519
Epoch 163/300, seasonal_0 Loss: 0.0622 | 0.0518
Epoch 164/300, seasonal_0 Loss: 0.0621 | 0.0518
Epoch 165/300, seasonal_0 Loss: 0.0620 | 0.0517
Epoch 166/300, seasonal_0 Loss: 0.0619 | 0.0516
Epoch 167/300, seasonal_0 Loss: 0.0617 | 0.0515
Epoch 168/300, seasonal_0 Loss: 0.0616 | 0.0515
Epoch 169/300, seasonal_0 Loss: 0.0615 | 0.0514
Epoch 170/300, seasonal_0 Loss: 0.0614 | 0.0513
Epoch 171/300, seasonal_0 Loss: 0.0612 | 0.0512
Epoch 172/300, seasonal_0 Loss: 0.0611 | 0.0512
Epoch 173/300, seasonal_0 Loss: 0.0610 | 0.0511
Epoch 174/300, seasonal_0 Loss: 0.0609 | 0.0510
Epoch 175/300, seasonal_0 Loss: 0.0608 | 0.0510
Epoch 176/300, seasonal_0 Loss: 0.0606 | 0.0509
Epoch 177/300, seasonal_0 Loss: 0.0605 | 0.0508
Epoch 178/300, seasonal_0 Loss: 0.0604 | 0.0508
Epoch 179/300, seasonal_0 Loss: 0.0603 | 0.0507
Epoch 180/300, seasonal_0 Loss: 0.0602 | 0.0506
Epoch 181/300, seasonal_0 Loss: 0.0601 | 0.0506
Epoch 182/300, seasonal_0 Loss: 0.0600 | 0.0505
Epoch 183/300, seasonal_0 Loss: 0.0598 | 0.0504
Epoch 184/300, seasonal_0 Loss: 0.0597 | 0.0504
Epoch 185/300, seasonal_0 Loss: 0.0596 | 0.0503
Epoch 186/300, seasonal_0 Loss: 0.0595 | 0.0503
Epoch 187/300, seasonal_0 Loss: 0.0594 | 0.0502
Epoch 188/300, seasonal_0 Loss: 0.0593 | 0.0502
Epoch 189/300, seasonal_0 Loss: 0.0592 | 0.0501
Epoch 190/300, seasonal_0 Loss: 0.0591 | 0.0501
Epoch 191/300, seasonal_0 Loss: 0.0590 | 0.0500
Epoch 192/300, seasonal_0 Loss: 0.0589 | 0.0500
Epoch 193/300, seasonal_0 Loss: 0.0587 | 0.0499
Epoch 194/300, seasonal_0 Loss: 0.0586 | 0.0499
Epoch 195/300, seasonal_0 Loss: 0.0585 | 0.0498
Epoch 196/300, seasonal_0 Loss: 0.0584 | 0.0498
Epoch 197/300, seasonal_0 Loss: 0.0583 | 0.0497
Epoch 198/300, seasonal_0 Loss: 0.0582 | 0.0497
Epoch 199/300, seasonal_0 Loss: 0.0581 | 0.0496
Epoch 200/300, seasonal_0 Loss: 0.0580 | 0.0496
Epoch 201/300, seasonal_0 Loss: 0.0579 | 0.0496
Epoch 202/300, seasonal_0 Loss: 0.0578 | 0.0495
Epoch 203/300, seasonal_0 Loss: 0.0577 | 0.0495
Epoch 204/300, seasonal_0 Loss: 0.0576 | 0.0494
Epoch 205/300, seasonal_0 Loss: 0.0575 | 0.0494
Epoch 206/300, seasonal_0 Loss: 0.0574 | 0.0494
Epoch 207/300, seasonal_0 Loss: 0.0573 | 0.0493
Epoch 208/300, seasonal_0 Loss: 0.0572 | 0.0493
Epoch 209/300, seasonal_0 Loss: 0.0571 | 0.0493
Epoch 210/300, seasonal_0 Loss: 0.0570 | 0.0493
Epoch 211/300, seasonal_0 Loss: 0.0569 | 0.0492
Epoch 212/300, seasonal_0 Loss: 0.0568 | 0.0492
Epoch 213/300, seasonal_0 Loss: 0.0567 | 0.0492
Epoch 214/300, seasonal_0 Loss: 0.0566 | 0.0492
Epoch 215/300, seasonal_0 Loss: 0.0566 | 0.0491
Epoch 216/300, seasonal_0 Loss: 0.0565 | 0.0491
Epoch 217/300, seasonal_0 Loss: 0.0564 | 0.0491
Epoch 218/300, seasonal_0 Loss: 0.0563 | 0.0491
Epoch 219/300, seasonal_0 Loss: 0.0562 | 0.0491
Epoch 220/300, seasonal_0 Loss: 0.0561 | 0.0491
Epoch 221/300, seasonal_0 Loss: 0.0560 | 0.0491
Epoch 222/300, seasonal_0 Loss: 0.0559 | 0.0491
Epoch 223/300, seasonal_0 Loss: 0.0558 | 0.0491
Epoch 224/300, seasonal_0 Loss: 0.0557 | 0.0491
Epoch 225/300, seasonal_0 Loss: 0.0557 | 0.0491
Epoch 226/300, seasonal_0 Loss: 0.0556 | 0.0491
Epoch 227/300, seasonal_0 Loss: 0.0555 | 0.0491
Epoch 228/300, seasonal_0 Loss: 0.0554 | 0.0491
Epoch 229/300, seasonal_0 Loss: 0.0553 | 0.0491
Epoch 230/300, seasonal_0 Loss: 0.0552 | 0.0492
Epoch 231/300, seasonal_0 Loss: 0.0551 | 0.0492
Epoch 232/300, seasonal_0 Loss: 0.0551 | 0.0492
Epoch 233/300, seasonal_0 Loss: 0.0550 | 0.0492
Epoch 234/300, seasonal_0 Loss: 0.0549 | 0.0492
Epoch 235/300, seasonal_0 Loss: 0.0548 | 0.0492
Epoch 236/300, seasonal_0 Loss: 0.0547 | 0.0493
Epoch 237/300, seasonal_0 Loss: 0.0547 | 0.0493
Epoch 238/300, seasonal_0 Loss: 0.0546 | 0.0492
Epoch 239/300, seasonal_0 Loss: 0.0545 | 0.0493
Epoch 240/300, seasonal_0 Loss: 0.0544 | 0.0493
Epoch 241/300, seasonal_0 Loss: 0.0543 | 0.0493
Epoch 242/300, seasonal_0 Loss: 0.0542 | 0.0493
Epoch 243/300, seasonal_0 Loss: 0.0542 | 0.0493
Epoch 244/300, seasonal_0 Loss: 0.0541 | 0.0493
Epoch 245/300, seasonal_0 Loss: 0.0540 | 0.0493
Epoch 246/300, seasonal_0 Loss: 0.0539 | 0.0493
Epoch 247/300, seasonal_0 Loss: 0.0539 | 0.0493
Epoch 248/300, seasonal_0 Loss: 0.0538 | 0.0493
Epoch 249/300, seasonal_0 Loss: 0.0537 | 0.0493
Epoch 250/300, seasonal_0 Loss: 0.0536 | 0.0492
Epoch 251/300, seasonal_0 Loss: 0.0535 | 0.0492
Epoch 252/300, seasonal_0 Loss: 0.0535 | 0.0492
Epoch 253/300, seasonal_0 Loss: 0.0534 | 0.0492
Epoch 254/300, seasonal_0 Loss: 0.0533 | 0.0492
Epoch 255/300, seasonal_0 Loss: 0.0532 | 0.0492
Epoch 256/300, seasonal_0 Loss: 0.0532 | 0.0492
Epoch 257/300, seasonal_0 Loss: 0.0531 | 0.0491
Epoch 258/300, seasonal_0 Loss: 0.0530 | 0.0491
Epoch 259/300, seasonal_0 Loss: 0.0529 | 0.0491
Epoch 260/300, seasonal_0 Loss: 0.0529 | 0.0491
Epoch 261/300, seasonal_0 Loss: 0.0528 | 0.0491
Epoch 262/300, seasonal_0 Loss: 0.0527 | 0.0491
Epoch 263/300, seasonal_0 Loss: 0.0526 | 0.0490
Epoch 264/300, seasonal_0 Loss: 0.0526 | 0.0490
Epoch 265/300, seasonal_0 Loss: 0.0525 | 0.0490
Epoch 266/300, seasonal_0 Loss: 0.0524 | 0.0489
Epoch 267/300, seasonal_0 Loss: 0.0523 | 0.0490
Epoch 268/300, seasonal_0 Loss: 0.0523 | 0.0490
Epoch 269/300, seasonal_0 Loss: 0.0522 | 0.0489
Epoch 270/300, seasonal_0 Loss: 0.0521 | 0.0489
Epoch 271/300, seasonal_0 Loss: 0.0520 | 0.0488
Epoch 272/300, seasonal_0 Loss: 0.0520 | 0.0488
Epoch 273/300, seasonal_0 Loss: 0.0519 | 0.0488
Epoch 274/300, seasonal_0 Loss: 0.0518 | 0.0488
Epoch 275/300, seasonal_0 Loss: 0.0517 | 0.0488
Epoch 276/300, seasonal_0 Loss: 0.0517 | 0.0487
Epoch 277/300, seasonal_0 Loss: 0.0516 | 0.0487
Epoch 278/300, seasonal_0 Loss: 0.0515 | 0.0486
Epoch 279/300, seasonal_0 Loss: 0.0515 | 0.0486
Epoch 280/300, seasonal_0 Loss: 0.0514 | 0.0486
Epoch 281/300, seasonal_0 Loss: 0.0513 | 0.0486
Epoch 282/300, seasonal_0 Loss: 0.0512 | 0.0486
Epoch 283/300, seasonal_0 Loss: 0.0512 | 0.0485
Epoch 284/300, seasonal_0 Loss: 0.0511 | 0.0485
Epoch 285/300, seasonal_0 Loss: 0.0510 | 0.0484
Epoch 286/300, seasonal_0 Loss: 0.0509 | 0.0484
Epoch 287/300, seasonal_0 Loss: 0.0509 | 0.0483
Epoch 288/300, seasonal_0 Loss: 0.0508 | 0.0484
Epoch 289/300, seasonal_0 Loss: 0.0507 | 0.0483
Epoch 290/300, seasonal_0 Loss: 0.0507 | 0.0483
Epoch 291/300, seasonal_0 Loss: 0.0506 | 0.0482
Epoch 292/300, seasonal_0 Loss: 0.0505 | 0.0482
Epoch 293/300, seasonal_0 Loss: 0.0504 | 0.0481
Epoch 294/300, seasonal_0 Loss: 0.0504 | 0.0481
Epoch 295/300, seasonal_0 Loss: 0.0503 | 0.0481
Epoch 296/300, seasonal_0 Loss: 0.0502 | 0.0481
Epoch 297/300, seasonal_0 Loss: 0.0502 | 0.0480
Epoch 298/300, seasonal_0 Loss: 0.0501 | 0.0480
Epoch 299/300, seasonal_0 Loss: 0.0500 | 0.0480
Epoch 300/300, seasonal_0 Loss: 0.0499 | 0.0479
Training seasonal_1 component with params: {'observation_period_num': 7, 'train_rates': 0.9053233051349603, 'learning_rate': 0.00010594736550149728, 'batch_size': 204, 'step_size': 5, 'gamma': 0.9316872660467107}
Epoch 1/300, seasonal_1 Loss: 0.6392 | 0.4831
Epoch 2/300, seasonal_1 Loss: 0.3713 | 0.3973
Epoch 3/300, seasonal_1 Loss: 0.2174 | 0.2524
Epoch 4/300, seasonal_1 Loss: 0.1704 | 0.1844
Epoch 5/300, seasonal_1 Loss: 0.1602 | 0.1669
Epoch 6/300, seasonal_1 Loss: 0.1617 | 0.1738
Epoch 7/300, seasonal_1 Loss: 0.1735 | 0.1716
Epoch 8/300, seasonal_1 Loss: 0.1701 | 0.1584
Epoch 9/300, seasonal_1 Loss: 0.1544 | 0.1598
Epoch 10/300, seasonal_1 Loss: 0.1512 | 0.1390
Epoch 11/300, seasonal_1 Loss: 0.1537 | 0.1590
Epoch 12/300, seasonal_1 Loss: 0.1547 | 0.1281
Epoch 13/300, seasonal_1 Loss: 0.1491 | 0.1537
Epoch 14/300, seasonal_1 Loss: 0.1375 | 0.1170
Epoch 15/300, seasonal_1 Loss: 0.1341 | 0.1372
Epoch 16/300, seasonal_1 Loss: 0.1323 | 0.1097
Epoch 17/300, seasonal_1 Loss: 0.1312 | 0.1270
Epoch 18/300, seasonal_1 Loss: 0.1249 | 0.1035
Epoch 19/300, seasonal_1 Loss: 0.1226 | 0.1121
Epoch 20/300, seasonal_1 Loss: 0.1208 | 0.0970
Epoch 21/300, seasonal_1 Loss: 0.1209 | 0.1056
Epoch 22/300, seasonal_1 Loss: 0.1182 | 0.0921
Epoch 23/300, seasonal_1 Loss: 0.1171 | 0.0965
Epoch 24/300, seasonal_1 Loss: 0.1152 | 0.0874
Epoch 25/300, seasonal_1 Loss: 0.1153 | 0.0918
Epoch 26/300, seasonal_1 Loss: 0.1139 | 0.0836
Epoch 27/300, seasonal_1 Loss: 0.1138 | 0.0881
Epoch 28/300, seasonal_1 Loss: 0.1120 | 0.0804
Epoch 29/300, seasonal_1 Loss: 0.1123 | 0.0848
Epoch 30/300, seasonal_1 Loss: 0.1113 | 0.0780
Epoch 31/300, seasonal_1 Loss: 0.1119 | 0.0837
Epoch 32/300, seasonal_1 Loss: 0.1103 | 0.0763
Epoch 33/300, seasonal_1 Loss: 0.1108 | 0.0809
Epoch 34/300, seasonal_1 Loss: 0.1092 | 0.0750
Epoch 35/300, seasonal_1 Loss: 0.1099 | 0.0798
Epoch 36/300, seasonal_1 Loss: 0.1081 | 0.0741
Epoch 37/300, seasonal_1 Loss: 0.1084 | 0.0778
Epoch 38/300, seasonal_1 Loss: 0.1067 | 0.0729
Epoch 39/300, seasonal_1 Loss: 0.1067 | 0.0754
Epoch 40/300, seasonal_1 Loss: 0.1053 | 0.0720
Epoch 41/300, seasonal_1 Loss: 0.1054 | 0.0736
Epoch 42/300, seasonal_1 Loss: 0.1041 | 0.0711
Epoch 43/300, seasonal_1 Loss: 0.1041 | 0.0716
Epoch 44/300, seasonal_1 Loss: 0.1031 | 0.0703
Epoch 45/300, seasonal_1 Loss: 0.1031 | 0.0704
Epoch 46/300, seasonal_1 Loss: 0.1024 | 0.0697
Epoch 47/300, seasonal_1 Loss: 0.1023 | 0.0695
Epoch 48/300, seasonal_1 Loss: 0.1018 | 0.0690
Epoch 49/300, seasonal_1 Loss: 0.1017 | 0.0688
Epoch 50/300, seasonal_1 Loss: 0.1013 | 0.0686
Epoch 51/300, seasonal_1 Loss: 0.1011 | 0.0683
Epoch 52/300, seasonal_1 Loss: 0.1009 | 0.0682
Epoch 53/300, seasonal_1 Loss: 0.1007 | 0.0679
Epoch 54/300, seasonal_1 Loss: 0.1004 | 0.0678
Epoch 55/300, seasonal_1 Loss: 0.1003 | 0.0676
Epoch 56/300, seasonal_1 Loss: 0.1000 | 0.0674
Epoch 57/300, seasonal_1 Loss: 0.0999 | 0.0673
Epoch 58/300, seasonal_1 Loss: 0.0997 | 0.0671
Epoch 59/300, seasonal_1 Loss: 0.0995 | 0.0670
Epoch 60/300, seasonal_1 Loss: 0.0993 | 0.0668
Epoch 61/300, seasonal_1 Loss: 0.0992 | 0.0667
Epoch 62/300, seasonal_1 Loss: 0.0990 | 0.0666
Epoch 63/300, seasonal_1 Loss: 0.0988 | 0.0664
Epoch 64/300, seasonal_1 Loss: 0.0987 | 0.0663
Epoch 65/300, seasonal_1 Loss: 0.0985 | 0.0662
Epoch 66/300, seasonal_1 Loss: 0.0984 | 0.0661
Epoch 67/300, seasonal_1 Loss: 0.0983 | 0.0659
Epoch 68/300, seasonal_1 Loss: 0.0981 | 0.0658
Epoch 69/300, seasonal_1 Loss: 0.0980 | 0.0657
Epoch 70/300, seasonal_1 Loss: 0.0979 | 0.0656
Epoch 71/300, seasonal_1 Loss: 0.0977 | 0.0655
Epoch 72/300, seasonal_1 Loss: 0.0976 | 0.0654
Epoch 73/300, seasonal_1 Loss: 0.0975 | 0.0653
Epoch 74/300, seasonal_1 Loss: 0.0974 | 0.0652
Epoch 75/300, seasonal_1 Loss: 0.0973 | 0.0651
Epoch 76/300, seasonal_1 Loss: 0.0972 | 0.0650
Epoch 77/300, seasonal_1 Loss: 0.0971 | 0.0649
Epoch 78/300, seasonal_1 Loss: 0.0970 | 0.0648
Epoch 79/300, seasonal_1 Loss: 0.0969 | 0.0647
Epoch 80/300, seasonal_1 Loss: 0.0968 | 0.0646
Epoch 81/300, seasonal_1 Loss: 0.0967 | 0.0646
Epoch 82/300, seasonal_1 Loss: 0.0966 | 0.0645
Epoch 83/300, seasonal_1 Loss: 0.0965 | 0.0644
Epoch 84/300, seasonal_1 Loss: 0.0964 | 0.0643
Epoch 85/300, seasonal_1 Loss: 0.0964 | 0.0642
Epoch 86/300, seasonal_1 Loss: 0.0963 | 0.0642
Epoch 87/300, seasonal_1 Loss: 0.0962 | 0.0641
Epoch 88/300, seasonal_1 Loss: 0.0961 | 0.0640
Epoch 89/300, seasonal_1 Loss: 0.0960 | 0.0639
Epoch 90/300, seasonal_1 Loss: 0.0960 | 0.0639
Epoch 91/300, seasonal_1 Loss: 0.0959 | 0.0638
Epoch 92/300, seasonal_1 Loss: 0.0958 | 0.0637
Epoch 93/300, seasonal_1 Loss: 0.0958 | 0.0637
Epoch 94/300, seasonal_1 Loss: 0.0957 | 0.0636
Epoch 95/300, seasonal_1 Loss: 0.0956 | 0.0635
Epoch 96/300, seasonal_1 Loss: 0.0956 | 0.0635
Epoch 97/300, seasonal_1 Loss: 0.0955 | 0.0634
Epoch 98/300, seasonal_1 Loss: 0.0955 | 0.0634
Epoch 99/300, seasonal_1 Loss: 0.0954 | 0.0633
Epoch 100/300, seasonal_1 Loss: 0.0954 | 0.0633
Epoch 101/300, seasonal_1 Loss: 0.0953 | 0.0632
Epoch 102/300, seasonal_1 Loss: 0.0953 | 0.0632
Epoch 103/300, seasonal_1 Loss: 0.0952 | 0.0631
Epoch 104/300, seasonal_1 Loss: 0.0952 | 0.0631
Epoch 105/300, seasonal_1 Loss: 0.0951 | 0.0630
Epoch 106/300, seasonal_1 Loss: 0.0951 | 0.0630
Epoch 107/300, seasonal_1 Loss: 0.0950 | 0.0629
Epoch 108/300, seasonal_1 Loss: 0.0950 | 0.0629
Epoch 109/300, seasonal_1 Loss: 0.0949 | 0.0628
Epoch 110/300, seasonal_1 Loss: 0.0949 | 0.0628
Epoch 111/300, seasonal_1 Loss: 0.0948 | 0.0627
Epoch 112/300, seasonal_1 Loss: 0.0948 | 0.0627
Epoch 113/300, seasonal_1 Loss: 0.0948 | 0.0627
Epoch 114/300, seasonal_1 Loss: 0.0947 | 0.0626
Epoch 115/300, seasonal_1 Loss: 0.0947 | 0.0626
Epoch 116/300, seasonal_1 Loss: 0.0947 | 0.0625
Epoch 117/300, seasonal_1 Loss: 0.0946 | 0.0625
Epoch 118/300, seasonal_1 Loss: 0.0946 | 0.0625
Epoch 119/300, seasonal_1 Loss: 0.0946 | 0.0624
Epoch 120/300, seasonal_1 Loss: 0.0945 | 0.0624
Epoch 121/300, seasonal_1 Loss: 0.0945 | 0.0624
Epoch 122/300, seasonal_1 Loss: 0.0945 | 0.0623
Epoch 123/300, seasonal_1 Loss: 0.0944 | 0.0623
Epoch 124/300, seasonal_1 Loss: 0.0944 | 0.0623
Epoch 125/300, seasonal_1 Loss: 0.0944 | 0.0622
Epoch 126/300, seasonal_1 Loss: 0.0943 | 0.0622
Epoch 127/300, seasonal_1 Loss: 0.0943 | 0.0622
Epoch 128/300, seasonal_1 Loss: 0.0943 | 0.0621
Epoch 129/300, seasonal_1 Loss: 0.0943 | 0.0621
Epoch 130/300, seasonal_1 Loss: 0.0942 | 0.0621
Epoch 131/300, seasonal_1 Loss: 0.0942 | 0.0621
Epoch 132/300, seasonal_1 Loss: 0.0942 | 0.0620
Epoch 133/300, seasonal_1 Loss: 0.0942 | 0.0620
Epoch 134/300, seasonal_1 Loss: 0.0941 | 0.0620
Epoch 135/300, seasonal_1 Loss: 0.0941 | 0.0620
Epoch 136/300, seasonal_1 Loss: 0.0941 | 0.0619
Epoch 137/300, seasonal_1 Loss: 0.0941 | 0.0619
Epoch 138/300, seasonal_1 Loss: 0.0941 | 0.0619
Epoch 139/300, seasonal_1 Loss: 0.0940 | 0.0619
Epoch 140/300, seasonal_1 Loss: 0.0940 | 0.0619
Epoch 141/300, seasonal_1 Loss: 0.0940 | 0.0618
Epoch 142/300, seasonal_1 Loss: 0.0940 | 0.0618
Epoch 143/300, seasonal_1 Loss: 0.0940 | 0.0618
Epoch 144/300, seasonal_1 Loss: 0.0939 | 0.0618
Epoch 145/300, seasonal_1 Loss: 0.0939 | 0.0618
Epoch 146/300, seasonal_1 Loss: 0.0939 | 0.0617
Epoch 147/300, seasonal_1 Loss: 0.0939 | 0.0617
Epoch 148/300, seasonal_1 Loss: 0.0939 | 0.0617
Epoch 149/300, seasonal_1 Loss: 0.0939 | 0.0617
Epoch 150/300, seasonal_1 Loss: 0.0938 | 0.0617
Epoch 151/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 152/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 153/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 154/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 155/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 156/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 157/300, seasonal_1 Loss: 0.0938 | 0.0616
Epoch 158/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 159/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 160/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 161/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 162/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 163/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 164/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 165/300, seasonal_1 Loss: 0.0937 | 0.0615
Epoch 166/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 167/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 168/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 169/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 170/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 171/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 172/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 173/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 174/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 175/300, seasonal_1 Loss: 0.0936 | 0.0614
Epoch 176/300, seasonal_1 Loss: 0.0936 | 0.0613
Epoch 177/300, seasonal_1 Loss: 0.0936 | 0.0613
Epoch 178/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 179/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 180/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 181/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 182/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 183/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 184/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 185/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 186/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 187/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 188/300, seasonal_1 Loss: 0.0935 | 0.0613
Epoch 189/300, seasonal_1 Loss: 0.0935 | 0.0612
Epoch 190/300, seasonal_1 Loss: 0.0935 | 0.0612
Epoch 191/300, seasonal_1 Loss: 0.0935 | 0.0612
Epoch 192/300, seasonal_1 Loss: 0.0935 | 0.0612
Epoch 193/300, seasonal_1 Loss: 0.0935 | 0.0612
Epoch 194/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 195/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 196/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 197/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 198/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 199/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 200/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 201/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 202/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 203/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 204/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 205/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 206/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 207/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 208/300, seasonal_1 Loss: 0.0934 | 0.0612
Epoch 209/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 210/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 211/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 212/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 213/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 214/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 215/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 216/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 217/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 218/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 219/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 220/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 221/300, seasonal_1 Loss: 0.0934 | 0.0611
Epoch 222/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 223/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 224/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 225/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 226/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 227/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 228/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 229/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 230/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 231/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 232/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 233/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 234/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 235/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 236/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 237/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 238/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 239/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 240/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 241/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 242/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 243/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 244/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 245/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 246/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 247/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 248/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 249/300, seasonal_1 Loss: 0.0933 | 0.0611
Epoch 250/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 251/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 252/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 253/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 254/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 255/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 256/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 257/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 258/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 259/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 260/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 261/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 262/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 263/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 264/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 265/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 266/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 267/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 268/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 269/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 270/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 271/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 272/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 273/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 274/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 275/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 276/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 277/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 278/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 279/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 280/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 281/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 282/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 283/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 284/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 285/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 286/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 287/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 288/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 289/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 290/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 291/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 292/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 293/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 294/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 295/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 296/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 297/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 298/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 299/300, seasonal_1 Loss: 0.0933 | 0.0610
Epoch 300/300, seasonal_1 Loss: 0.0933 | 0.0610
Training seasonal_2 component with params: {'observation_period_num': 17, 'train_rates': 0.9664019259493056, 'learning_rate': 0.0007057165460247904, 'batch_size': 182, 'step_size': 8, 'gamma': 0.9159744553767339}
Epoch 1/300, seasonal_2 Loss: 0.5230 | 0.3680
Epoch 2/300, seasonal_2 Loss: 0.2553 | 0.2723
Epoch 3/300, seasonal_2 Loss: 0.2099 | 0.2193
Epoch 4/300, seasonal_2 Loss: 0.2074 | 0.1872
Epoch 5/300, seasonal_2 Loss: 0.2700 | 0.3769
Epoch 6/300, seasonal_2 Loss: 0.2001 | 0.3776
Epoch 7/300, seasonal_2 Loss: 0.1727 | 0.1927
Epoch 8/300, seasonal_2 Loss: 0.1560 | 0.2435
Epoch 9/300, seasonal_2 Loss: 0.1347 | 0.1346
Epoch 10/300, seasonal_2 Loss: 0.1156 | 0.1326
Epoch 11/300, seasonal_2 Loss: 0.1092 | 0.1049
Epoch 12/300, seasonal_2 Loss: 0.1152 | 0.0938
Epoch 13/300, seasonal_2 Loss: 0.1082 | 0.0969
Epoch 14/300, seasonal_2 Loss: 0.1151 | 0.0914
Epoch 15/300, seasonal_2 Loss: 0.1155 | 0.0950
Epoch 16/300, seasonal_2 Loss: 0.1247 | 0.1192
Epoch 17/300, seasonal_2 Loss: 0.1200 | 0.0898
Epoch 18/300, seasonal_2 Loss: 0.1200 | 0.1307
Epoch 19/300, seasonal_2 Loss: 0.1730 | 0.2296
Epoch 20/300, seasonal_2 Loss: 0.1577 | 0.1028
Epoch 21/300, seasonal_2 Loss: 0.1180 | 0.1112
Epoch 22/300, seasonal_2 Loss: 0.1387 | 0.1149
Epoch 23/300, seasonal_2 Loss: 0.1268 | 0.1387
Epoch 24/300, seasonal_2 Loss: 0.1129 | 0.0848
Epoch 25/300, seasonal_2 Loss: 0.0984 | 0.0846
Epoch 26/300, seasonal_2 Loss: 0.0982 | 0.0970
Epoch 27/300, seasonal_2 Loss: 0.0944 | 0.0792
Epoch 28/300, seasonal_2 Loss: 0.0878 | 0.0771
Epoch 29/300, seasonal_2 Loss: 0.0846 | 0.0684
Epoch 30/300, seasonal_2 Loss: 0.0836 | 0.0637
Epoch 31/300, seasonal_2 Loss: 0.0839 | 0.0683
Epoch 32/300, seasonal_2 Loss: 0.0817 | 0.0584
Epoch 33/300, seasonal_2 Loss: 0.0802 | 0.0634
Epoch 34/300, seasonal_2 Loss: 0.0790 | 0.0582
Epoch 35/300, seasonal_2 Loss: 0.0775 | 0.0518
Epoch 36/300, seasonal_2 Loss: 0.0772 | 0.0550
Epoch 37/300, seasonal_2 Loss: 0.0765 | 0.0509
Epoch 38/300, seasonal_2 Loss: 0.0768 | 0.0541
Epoch 39/300, seasonal_2 Loss: 0.0761 | 0.0561
Epoch 40/300, seasonal_2 Loss: 0.0749 | 0.0468
Epoch 41/300, seasonal_2 Loss: 0.0745 | 0.0494
Epoch 42/300, seasonal_2 Loss: 0.0747 | 0.0479
Epoch 43/300, seasonal_2 Loss: 0.0750 | 0.0487
Epoch 44/300, seasonal_2 Loss: 0.0752 | 0.0523
Epoch 45/300, seasonal_2 Loss: 0.0762 | 0.0491
Epoch 46/300, seasonal_2 Loss: 0.0772 | 0.0504
Epoch 47/300, seasonal_2 Loss: 0.0757 | 0.0479
Epoch 48/300, seasonal_2 Loss: 0.0773 | 0.0542
Epoch 49/300, seasonal_2 Loss: 0.0770 | 0.0516
Epoch 50/300, seasonal_2 Loss: 0.0746 | 0.0435
Epoch 51/300, seasonal_2 Loss: 0.0719 | 0.0453
Epoch 52/300, seasonal_2 Loss: 0.0715 | 0.0426
Epoch 53/300, seasonal_2 Loss: 0.0715 | 0.0465
Epoch 54/300, seasonal_2 Loss: 0.0706 | 0.0423
Epoch 55/300, seasonal_2 Loss: 0.0705 | 0.0415
Epoch 56/300, seasonal_2 Loss: 0.0697 | 0.0411
Epoch 57/300, seasonal_2 Loss: 0.0692 | 0.0413
Epoch 58/300, seasonal_2 Loss: 0.0687 | 0.0407
Epoch 59/300, seasonal_2 Loss: 0.0685 | 0.0397
Epoch 60/300, seasonal_2 Loss: 0.0682 | 0.0397
Epoch 61/300, seasonal_2 Loss: 0.0679 | 0.0396
Epoch 62/300, seasonal_2 Loss: 0.0676 | 0.0395
Epoch 63/300, seasonal_2 Loss: 0.0673 | 0.0389
Epoch 64/300, seasonal_2 Loss: 0.0672 | 0.0388
Epoch 65/300, seasonal_2 Loss: 0.0672 | 0.0389
Epoch 66/300, seasonal_2 Loss: 0.0673 | 0.0391
Epoch 67/300, seasonal_2 Loss: 0.0676 | 0.0392
Epoch 68/300, seasonal_2 Loss: 0.0685 | 0.0407
Epoch 69/300, seasonal_2 Loss: 0.0701 | 0.0431
Epoch 70/300, seasonal_2 Loss: 0.0725 | 0.0424
Epoch 71/300, seasonal_2 Loss: 0.0717 | 0.0417
Epoch 72/300, seasonal_2 Loss: 0.0701 | 0.0413
Epoch 73/300, seasonal_2 Loss: 0.0703 | 0.0382
Epoch 74/300, seasonal_2 Loss: 0.0692 | 0.0379
Epoch 75/300, seasonal_2 Loss: 0.0673 | 0.0368
Epoch 76/300, seasonal_2 Loss: 0.0659 | 0.0361
Epoch 77/300, seasonal_2 Loss: 0.0661 | 0.0373
Epoch 78/300, seasonal_2 Loss: 0.0660 | 0.0370
Epoch 79/300, seasonal_2 Loss: 0.0656 | 0.0366
Epoch 80/300, seasonal_2 Loss: 0.0648 | 0.0356
Epoch 81/300, seasonal_2 Loss: 0.0641 | 0.0348
Epoch 82/300, seasonal_2 Loss: 0.0637 | 0.0344
Epoch 83/300, seasonal_2 Loss: 0.0634 | 0.0341
Epoch 84/300, seasonal_2 Loss: 0.0632 | 0.0339
Epoch 85/300, seasonal_2 Loss: 0.0630 | 0.0338
Epoch 86/300, seasonal_2 Loss: 0.0628 | 0.0337
Epoch 87/300, seasonal_2 Loss: 0.0626 | 0.0338
Epoch 88/300, seasonal_2 Loss: 0.0624 | 0.0338
Epoch 89/300, seasonal_2 Loss: 0.0622 | 0.0339
Epoch 90/300, seasonal_2 Loss: 0.0620 | 0.0339
Epoch 91/300, seasonal_2 Loss: 0.0619 | 0.0338
Epoch 92/300, seasonal_2 Loss: 0.0617 | 0.0337
Epoch 93/300, seasonal_2 Loss: 0.0616 | 0.0336
Epoch 94/300, seasonal_2 Loss: 0.0615 | 0.0334
Epoch 95/300, seasonal_2 Loss: 0.0613 | 0.0333
Epoch 96/300, seasonal_2 Loss: 0.0612 | 0.0331
Epoch 97/300, seasonal_2 Loss: 0.0611 | 0.0328
Epoch 98/300, seasonal_2 Loss: 0.0610 | 0.0327
Epoch 99/300, seasonal_2 Loss: 0.0609 | 0.0326
Epoch 100/300, seasonal_2 Loss: 0.0608 | 0.0325
Epoch 101/300, seasonal_2 Loss: 0.0607 | 0.0324
Epoch 102/300, seasonal_2 Loss: 0.0606 | 0.0324
Epoch 103/300, seasonal_2 Loss: 0.0605 | 0.0325
Epoch 104/300, seasonal_2 Loss: 0.0604 | 0.0326
Epoch 105/300, seasonal_2 Loss: 0.0602 | 0.0327
Epoch 106/300, seasonal_2 Loss: 0.0602 | 0.0329
Epoch 107/300, seasonal_2 Loss: 0.0601 | 0.0329
Epoch 108/300, seasonal_2 Loss: 0.0600 | 0.0329
Epoch 109/300, seasonal_2 Loss: 0.0600 | 0.0329
Epoch 110/300, seasonal_2 Loss: 0.0599 | 0.0328
Epoch 111/300, seasonal_2 Loss: 0.0598 | 0.0326
Epoch 112/300, seasonal_2 Loss: 0.0597 | 0.0324
Epoch 113/300, seasonal_2 Loss: 0.0597 | 0.0322
Epoch 114/300, seasonal_2 Loss: 0.0596 | 0.0321
Epoch 115/300, seasonal_2 Loss: 0.0596 | 0.0320
Epoch 116/300, seasonal_2 Loss: 0.0595 | 0.0319
Epoch 117/300, seasonal_2 Loss: 0.0594 | 0.0318
Epoch 118/300, seasonal_2 Loss: 0.0594 | 0.0318
Epoch 119/300, seasonal_2 Loss: 0.0593 | 0.0319
Epoch 120/300, seasonal_2 Loss: 0.0592 | 0.0319
Epoch 121/300, seasonal_2 Loss: 0.0591 | 0.0321
Epoch 122/300, seasonal_2 Loss: 0.0591 | 0.0322
Epoch 123/300, seasonal_2 Loss: 0.0590 | 0.0322
Epoch 124/300, seasonal_2 Loss: 0.0590 | 0.0322
Epoch 125/300, seasonal_2 Loss: 0.0589 | 0.0322
Epoch 126/300, seasonal_2 Loss: 0.0589 | 0.0321
Epoch 127/300, seasonal_2 Loss: 0.0588 | 0.0320
Epoch 128/300, seasonal_2 Loss: 0.0588 | 0.0319
Epoch 129/300, seasonal_2 Loss: 0.0587 | 0.0319
Epoch 130/300, seasonal_2 Loss: 0.0587 | 0.0318
Epoch 131/300, seasonal_2 Loss: 0.0586 | 0.0318
Epoch 132/300, seasonal_2 Loss: 0.0586 | 0.0318
Epoch 133/300, seasonal_2 Loss: 0.0586 | 0.0318
Epoch 134/300, seasonal_2 Loss: 0.0585 | 0.0318
Epoch 135/300, seasonal_2 Loss: 0.0585 | 0.0318
Epoch 136/300, seasonal_2 Loss: 0.0584 | 0.0318
Epoch 137/300, seasonal_2 Loss: 0.0584 | 0.0318
Epoch 138/300, seasonal_2 Loss: 0.0583 | 0.0318
Epoch 139/300, seasonal_2 Loss: 0.0583 | 0.0318
Epoch 140/300, seasonal_2 Loss: 0.0583 | 0.0318
Epoch 141/300, seasonal_2 Loss: 0.0582 | 0.0318
Epoch 142/300, seasonal_2 Loss: 0.0582 | 0.0318
Epoch 143/300, seasonal_2 Loss: 0.0582 | 0.0317
Epoch 144/300, seasonal_2 Loss: 0.0581 | 0.0317
Epoch 145/300, seasonal_2 Loss: 0.0581 | 0.0317
Epoch 146/300, seasonal_2 Loss: 0.0581 | 0.0317
Epoch 147/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 148/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 149/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 150/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 151/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 152/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 153/300, seasonal_2 Loss: 0.0580 | 0.0317
Epoch 154/300, seasonal_2 Loss: 0.0581 | 0.0317
Epoch 155/300, seasonal_2 Loss: 0.0583 | 0.0317
Epoch 156/300, seasonal_2 Loss: 0.0586 | 0.0318
Epoch 157/300, seasonal_2 Loss: 0.0591 | 0.0319
Epoch 158/300, seasonal_2 Loss: 0.0597 | 0.0322
Epoch 159/300, seasonal_2 Loss: 0.0609 | 0.0332
Epoch 160/300, seasonal_2 Loss: 0.0619 | 0.0330
Epoch 161/300, seasonal_2 Loss: 0.0627 | 0.0377
Epoch 162/300, seasonal_2 Loss: 0.0620 | 0.0347
Epoch 163/300, seasonal_2 Loss: 0.0612 | 0.0371
Epoch 164/300, seasonal_2 Loss: 0.0601 | 0.0340
Epoch 165/300, seasonal_2 Loss: 0.0590 | 0.0342
Epoch 166/300, seasonal_2 Loss: 0.0583 | 0.0328
Epoch 167/300, seasonal_2 Loss: 0.0579 | 0.0322
Epoch 168/300, seasonal_2 Loss: 0.0577 | 0.0321
Epoch 169/300, seasonal_2 Loss: 0.0576 | 0.0317
Epoch 170/300, seasonal_2 Loss: 0.0575 | 0.0318
Epoch 171/300, seasonal_2 Loss: 0.0575 | 0.0316
Epoch 172/300, seasonal_2 Loss: 0.0574 | 0.0317
Epoch 173/300, seasonal_2 Loss: 0.0574 | 0.0316
Epoch 174/300, seasonal_2 Loss: 0.0574 | 0.0317
Epoch 175/300, seasonal_2 Loss: 0.0574 | 0.0316
Epoch 176/300, seasonal_2 Loss: 0.0573 | 0.0316
Epoch 177/300, seasonal_2 Loss: 0.0573 | 0.0316
Epoch 178/300, seasonal_2 Loss: 0.0573 | 0.0316
Epoch 179/300, seasonal_2 Loss: 0.0573 | 0.0316
Epoch 180/300, seasonal_2 Loss: 0.0573 | 0.0316
Epoch 181/300, seasonal_2 Loss: 0.0572 | 0.0316
Epoch 182/300, seasonal_2 Loss: 0.0572 | 0.0316
Epoch 183/300, seasonal_2 Loss: 0.0572 | 0.0316
Epoch 184/300, seasonal_2 Loss: 0.0572 | 0.0316
Epoch 185/300, seasonal_2 Loss: 0.0572 | 0.0316
Epoch 186/300, seasonal_2 Loss: 0.0572 | 0.0316
Epoch 187/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 188/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 189/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 190/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 191/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 192/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 193/300, seasonal_2 Loss: 0.0571 | 0.0316
Epoch 194/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 195/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 196/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 197/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 198/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 199/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 200/300, seasonal_2 Loss: 0.0570 | 0.0316
Epoch 201/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 202/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 203/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 204/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 205/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 206/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 207/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 208/300, seasonal_2 Loss: 0.0569 | 0.0316
Epoch 209/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 210/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 211/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 212/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 213/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 214/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 215/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 216/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 217/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 218/300, seasonal_2 Loss: 0.0568 | 0.0316
Epoch 219/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 220/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 221/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 222/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 223/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 224/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 225/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 226/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 227/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 228/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 229/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 230/300, seasonal_2 Loss: 0.0567 | 0.0316
Epoch 231/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 232/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 233/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 234/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 235/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 236/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 237/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 238/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 239/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 240/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 241/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 242/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 243/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 244/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 245/300, seasonal_2 Loss: 0.0566 | 0.0316
Epoch 246/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 247/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 248/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 249/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 250/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 251/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 252/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 253/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 254/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 255/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 256/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 257/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 258/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 259/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 260/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 261/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 262/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 263/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 264/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 265/300, seasonal_2 Loss: 0.0565 | 0.0316
Epoch 266/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 267/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 268/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 269/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 270/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 271/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 272/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 273/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 274/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 275/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 276/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 277/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 278/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 279/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 280/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 281/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 282/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 283/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 284/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 285/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 286/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 287/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 288/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 289/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 290/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 291/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 292/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 293/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 294/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 295/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 296/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 297/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 298/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 299/300, seasonal_2 Loss: 0.0564 | 0.0316
Epoch 300/300, seasonal_2 Loss: 0.0563 | 0.0316
Training seasonal_3 component with params: {'observation_period_num': 28, 'train_rates': 0.8777386523083193, 'learning_rate': 2.1486496107241683e-05, 'batch_size': 58, 'step_size': 9, 'gamma': 0.9174659883555601}
Epoch 1/300, seasonal_3 Loss: 0.4097 | 0.5727
Epoch 2/300, seasonal_3 Loss: 0.3141 | 0.4669
Epoch 3/300, seasonal_3 Loss: 0.2602 | 0.3720
Epoch 4/300, seasonal_3 Loss: 0.2307 | 0.3273
Epoch 5/300, seasonal_3 Loss: 0.2103 | 0.2955
Epoch 6/300, seasonal_3 Loss: 0.1956 | 0.2709
Epoch 7/300, seasonal_3 Loss: 0.1850 | 0.2507
Epoch 8/300, seasonal_3 Loss: 0.1762 | 0.2340
Epoch 9/300, seasonal_3 Loss: 0.1685 | 0.2194
Epoch 10/300, seasonal_3 Loss: 0.1620 | 0.2041
Epoch 11/300, seasonal_3 Loss: 0.1571 | 0.1924
Epoch 12/300, seasonal_3 Loss: 0.1524 | 0.1816
Epoch 13/300, seasonal_3 Loss: 0.1482 | 0.1712
Epoch 14/300, seasonal_3 Loss: 0.1444 | 0.1614
Epoch 15/300, seasonal_3 Loss: 0.1413 | 0.1500
Epoch 16/300, seasonal_3 Loss: 0.1388 | 0.1423
Epoch 17/300, seasonal_3 Loss: 0.1362 | 0.1354
Epoch 18/300, seasonal_3 Loss: 0.1337 | 0.1293
Epoch 19/300, seasonal_3 Loss: 0.1314 | 0.1234
Epoch 20/300, seasonal_3 Loss: 0.1294 | 0.1192
Epoch 21/300, seasonal_3 Loss: 0.1272 | 0.1155
Epoch 22/300, seasonal_3 Loss: 0.1253 | 0.1122
Epoch 23/300, seasonal_3 Loss: 0.1236 | 0.1091
Epoch 24/300, seasonal_3 Loss: 0.1221 | 0.1063
Epoch 25/300, seasonal_3 Loss: 0.1209 | 0.1039
Epoch 26/300, seasonal_3 Loss: 0.1197 | 0.1017
Epoch 27/300, seasonal_3 Loss: 0.1186 | 0.0996
Epoch 28/300, seasonal_3 Loss: 0.1174 | 0.0976
Epoch 29/300, seasonal_3 Loss: 0.1166 | 0.0959
Epoch 30/300, seasonal_3 Loss: 0.1157 | 0.0942
Epoch 31/300, seasonal_3 Loss: 0.1147 | 0.0926
Epoch 32/300, seasonal_3 Loss: 0.1138 | 0.0911
Epoch 33/300, seasonal_3 Loss: 0.1129 | 0.0897
Epoch 34/300, seasonal_3 Loss: 0.1122 | 0.0884
Epoch 35/300, seasonal_3 Loss: 0.1114 | 0.0872
Epoch 36/300, seasonal_3 Loss: 0.1105 | 0.0860
Epoch 37/300, seasonal_3 Loss: 0.1097 | 0.0850
Epoch 38/300, seasonal_3 Loss: 0.1090 | 0.0839
Epoch 39/300, seasonal_3 Loss: 0.1082 | 0.0829
Epoch 40/300, seasonal_3 Loss: 0.1074 | 0.0820
Epoch 41/300, seasonal_3 Loss: 0.1066 | 0.0811
Epoch 42/300, seasonal_3 Loss: 0.1059 | 0.0803
Epoch 43/300, seasonal_3 Loss: 0.1053 | 0.0795
Epoch 44/300, seasonal_3 Loss: 0.1047 | 0.0788
Epoch 45/300, seasonal_3 Loss: 0.1040 | 0.0781
Epoch 46/300, seasonal_3 Loss: 0.1034 | 0.0774
Epoch 47/300, seasonal_3 Loss: 0.1029 | 0.0768
Epoch 48/300, seasonal_3 Loss: 0.1024 | 0.0762
Epoch 49/300, seasonal_3 Loss: 0.1018 | 0.0756
Epoch 50/300, seasonal_3 Loss: 0.1013 | 0.0751
Epoch 51/300, seasonal_3 Loss: 0.1008 | 0.0746
Epoch 52/300, seasonal_3 Loss: 0.1004 | 0.0741
Epoch 53/300, seasonal_3 Loss: 0.0999 | 0.0736
Epoch 54/300, seasonal_3 Loss: 0.0995 | 0.0731
Epoch 55/300, seasonal_3 Loss: 0.0991 | 0.0727
Epoch 56/300, seasonal_3 Loss: 0.0987 | 0.0722
Epoch 57/300, seasonal_3 Loss: 0.0983 | 0.0717
Epoch 58/300, seasonal_3 Loss: 0.0979 | 0.0713
Epoch 59/300, seasonal_3 Loss: 0.0976 | 0.0709
Epoch 60/300, seasonal_3 Loss: 0.0972 | 0.0705
Epoch 61/300, seasonal_3 Loss: 0.0969 | 0.0701
Epoch 62/300, seasonal_3 Loss: 0.0965 | 0.0698
Epoch 63/300, seasonal_3 Loss: 0.0962 | 0.0694
Epoch 64/300, seasonal_3 Loss: 0.0959 | 0.0691
Epoch 65/300, seasonal_3 Loss: 0.0956 | 0.0687
Epoch 66/300, seasonal_3 Loss: 0.0953 | 0.0684
Epoch 67/300, seasonal_3 Loss: 0.0950 | 0.0681
Epoch 68/300, seasonal_3 Loss: 0.0948 | 0.0678
Epoch 69/300, seasonal_3 Loss: 0.0945 | 0.0675
Epoch 70/300, seasonal_3 Loss: 0.0942 | 0.0672
Epoch 71/300, seasonal_3 Loss: 0.0940 | 0.0670
Epoch 72/300, seasonal_3 Loss: 0.0937 | 0.0667
Epoch 73/300, seasonal_3 Loss: 0.0935 | 0.0664
Epoch 74/300, seasonal_3 Loss: 0.0933 | 0.0662
Epoch 75/300, seasonal_3 Loss: 0.0931 | 0.0659
Epoch 76/300, seasonal_3 Loss: 0.0928 | 0.0657
Epoch 77/300, seasonal_3 Loss: 0.0926 | 0.0655
Epoch 78/300, seasonal_3 Loss: 0.0924 | 0.0652
Epoch 79/300, seasonal_3 Loss: 0.0922 | 0.0650
Epoch 80/300, seasonal_3 Loss: 0.0920 | 0.0648
Epoch 81/300, seasonal_3 Loss: 0.0918 | 0.0646
Epoch 82/300, seasonal_3 Loss: 0.0916 | 0.0644
Epoch 83/300, seasonal_3 Loss: 0.0915 | 0.0642
Epoch 84/300, seasonal_3 Loss: 0.0913 | 0.0640
Epoch 85/300, seasonal_3 Loss: 0.0911 | 0.0638
Epoch 86/300, seasonal_3 Loss: 0.0910 | 0.0637
Epoch 87/300, seasonal_3 Loss: 0.0908 | 0.0634
Epoch 88/300, seasonal_3 Loss: 0.0907 | 0.0633
Epoch 89/300, seasonal_3 Loss: 0.0905 | 0.0631
Epoch 90/300, seasonal_3 Loss: 0.0904 | 0.0630
Epoch 91/300, seasonal_3 Loss: 0.0902 | 0.0628
Epoch 92/300, seasonal_3 Loss: 0.0901 | 0.0626
Epoch 93/300, seasonal_3 Loss: 0.0899 | 0.0625
Epoch 94/300, seasonal_3 Loss: 0.0898 | 0.0623
Epoch 95/300, seasonal_3 Loss: 0.0897 | 0.0622
Epoch 96/300, seasonal_3 Loss: 0.0895 | 0.0620
Epoch 97/300, seasonal_3 Loss: 0.0894 | 0.0619
Epoch 98/300, seasonal_3 Loss: 0.0893 | 0.0617
Epoch 99/300, seasonal_3 Loss: 0.0892 | 0.0616
Epoch 100/300, seasonal_3 Loss: 0.0890 | 0.0614
Epoch 101/300, seasonal_3 Loss: 0.0889 | 0.0613
Epoch 102/300, seasonal_3 Loss: 0.0888 | 0.0612
Epoch 103/300, seasonal_3 Loss: 0.0887 | 0.0611
Epoch 104/300, seasonal_3 Loss: 0.0886 | 0.0610
Epoch 105/300, seasonal_3 Loss: 0.0885 | 0.0608
Epoch 106/300, seasonal_3 Loss: 0.0884 | 0.0607
Epoch 107/300, seasonal_3 Loss: 0.0883 | 0.0606
Epoch 108/300, seasonal_3 Loss: 0.0882 | 0.0605
Epoch 109/300, seasonal_3 Loss: 0.0881 | 0.0604
Epoch 110/300, seasonal_3 Loss: 0.0880 | 0.0603
Epoch 111/300, seasonal_3 Loss: 0.0879 | 0.0602
Epoch 112/300, seasonal_3 Loss: 0.0878 | 0.0601
Epoch 113/300, seasonal_3 Loss: 0.0877 | 0.0600
Epoch 114/300, seasonal_3 Loss: 0.0877 | 0.0599
Epoch 115/300, seasonal_3 Loss: 0.0876 | 0.0598
Epoch 116/300, seasonal_3 Loss: 0.0875 | 0.0597
Epoch 117/300, seasonal_3 Loss: 0.0874 | 0.0596
Epoch 118/300, seasonal_3 Loss: 0.0873 | 0.0595
Epoch 119/300, seasonal_3 Loss: 0.0873 | 0.0594
Epoch 120/300, seasonal_3 Loss: 0.0872 | 0.0594
Epoch 121/300, seasonal_3 Loss: 0.0871 | 0.0593
Epoch 122/300, seasonal_3 Loss: 0.0870 | 0.0592
Epoch 123/300, seasonal_3 Loss: 0.0870 | 0.0591
Epoch 124/300, seasonal_3 Loss: 0.0869 | 0.0590
Epoch 125/300, seasonal_3 Loss: 0.0868 | 0.0590
Epoch 126/300, seasonal_3 Loss: 0.0868 | 0.0589
Epoch 127/300, seasonal_3 Loss: 0.0867 | 0.0588
Epoch 128/300, seasonal_3 Loss: 0.0866 | 0.0587
Epoch 129/300, seasonal_3 Loss: 0.0866 | 0.0587
Epoch 130/300, seasonal_3 Loss: 0.0865 | 0.0586
Epoch 131/300, seasonal_3 Loss: 0.0865 | 0.0585
Epoch 132/300, seasonal_3 Loss: 0.0864 | 0.0585
Epoch 133/300, seasonal_3 Loss: 0.0864 | 0.0584
Epoch 134/300, seasonal_3 Loss: 0.0863 | 0.0583
Epoch 135/300, seasonal_3 Loss: 0.0862 | 0.0583
Epoch 136/300, seasonal_3 Loss: 0.0862 | 0.0582
Epoch 137/300, seasonal_3 Loss: 0.0861 | 0.0581
Epoch 138/300, seasonal_3 Loss: 0.0861 | 0.0581
Epoch 139/300, seasonal_3 Loss: 0.0860 | 0.0580
Epoch 140/300, seasonal_3 Loss: 0.0860 | 0.0580
Epoch 141/300, seasonal_3 Loss: 0.0859 | 0.0579
Epoch 142/300, seasonal_3 Loss: 0.0859 | 0.0579
Epoch 143/300, seasonal_3 Loss: 0.0859 | 0.0578
Epoch 144/300, seasonal_3 Loss: 0.0858 | 0.0578
Epoch 145/300, seasonal_3 Loss: 0.0858 | 0.0577
Epoch 146/300, seasonal_3 Loss: 0.0857 | 0.0577
Epoch 147/300, seasonal_3 Loss: 0.0857 | 0.0576
Epoch 148/300, seasonal_3 Loss: 0.0856 | 0.0576
Epoch 149/300, seasonal_3 Loss: 0.0856 | 0.0575
Epoch 150/300, seasonal_3 Loss: 0.0856 | 0.0575
Epoch 151/300, seasonal_3 Loss: 0.0855 | 0.0574
Epoch 152/300, seasonal_3 Loss: 0.0855 | 0.0574
Epoch 153/300, seasonal_3 Loss: 0.0854 | 0.0574
Epoch 154/300, seasonal_3 Loss: 0.0854 | 0.0573
Epoch 155/300, seasonal_3 Loss: 0.0854 | 0.0573
Epoch 156/300, seasonal_3 Loss: 0.0853 | 0.0572
Epoch 157/300, seasonal_3 Loss: 0.0853 | 0.0572
Epoch 158/300, seasonal_3 Loss: 0.0853 | 0.0572
Epoch 159/300, seasonal_3 Loss: 0.0852 | 0.0571
Epoch 160/300, seasonal_3 Loss: 0.0852 | 0.0571
Epoch 161/300, seasonal_3 Loss: 0.0852 | 0.0570
Epoch 162/300, seasonal_3 Loss: 0.0851 | 0.0570
Epoch 163/300, seasonal_3 Loss: 0.0851 | 0.0570
Epoch 164/300, seasonal_3 Loss: 0.0851 | 0.0569
Epoch 165/300, seasonal_3 Loss: 0.0851 | 0.0569
Epoch 166/300, seasonal_3 Loss: 0.0850 | 0.0569
Epoch 167/300, seasonal_3 Loss: 0.0850 | 0.0568
Epoch 168/300, seasonal_3 Loss: 0.0850 | 0.0568
Epoch 169/300, seasonal_3 Loss: 0.0849 | 0.0568
Epoch 170/300, seasonal_3 Loss: 0.0849 | 0.0567
Epoch 171/300, seasonal_3 Loss: 0.0849 | 0.0567
Epoch 172/300, seasonal_3 Loss: 0.0849 | 0.0567
Epoch 173/300, seasonal_3 Loss: 0.0848 | 0.0566
Epoch 174/300, seasonal_3 Loss: 0.0848 | 0.0566
Epoch 175/300, seasonal_3 Loss: 0.0848 | 0.0566
Epoch 176/300, seasonal_3 Loss: 0.0848 | 0.0566
Epoch 177/300, seasonal_3 Loss: 0.0847 | 0.0565
Epoch 178/300, seasonal_3 Loss: 0.0847 | 0.0565
Epoch 179/300, seasonal_3 Loss: 0.0847 | 0.0565
Epoch 180/300, seasonal_3 Loss: 0.0847 | 0.0565
Epoch 181/300, seasonal_3 Loss: 0.0847 | 0.0564
Epoch 182/300, seasonal_3 Loss: 0.0846 | 0.0564
Epoch 183/300, seasonal_3 Loss: 0.0846 | 0.0564
Epoch 184/300, seasonal_3 Loss: 0.0846 | 0.0564
Epoch 185/300, seasonal_3 Loss: 0.0846 | 0.0564
Epoch 186/300, seasonal_3 Loss: 0.0846 | 0.0563
Epoch 187/300, seasonal_3 Loss: 0.0845 | 0.0563
Epoch 188/300, seasonal_3 Loss: 0.0845 | 0.0563
Epoch 189/300, seasonal_3 Loss: 0.0845 | 0.0563
Epoch 190/300, seasonal_3 Loss: 0.0845 | 0.0562
Epoch 191/300, seasonal_3 Loss: 0.0845 | 0.0562
Epoch 192/300, seasonal_3 Loss: 0.0845 | 0.0562
Epoch 193/300, seasonal_3 Loss: 0.0844 | 0.0562
Epoch 194/300, seasonal_3 Loss: 0.0844 | 0.0562
Epoch 195/300, seasonal_3 Loss: 0.0844 | 0.0561
Epoch 196/300, seasonal_3 Loss: 0.0844 | 0.0561
Epoch 197/300, seasonal_3 Loss: 0.0844 | 0.0561
Epoch 198/300, seasonal_3 Loss: 0.0844 | 0.0561
Epoch 199/300, seasonal_3 Loss: 0.0843 | 0.0561
Epoch 200/300, seasonal_3 Loss: 0.0843 | 0.0561
Epoch 201/300, seasonal_3 Loss: 0.0843 | 0.0560
Epoch 202/300, seasonal_3 Loss: 0.0843 | 0.0560
Epoch 203/300, seasonal_3 Loss: 0.0843 | 0.0560
Epoch 204/300, seasonal_3 Loss: 0.0843 | 0.0560
Epoch 205/300, seasonal_3 Loss: 0.0843 | 0.0560
Epoch 206/300, seasonal_3 Loss: 0.0842 | 0.0560
Epoch 207/300, seasonal_3 Loss: 0.0842 | 0.0560
Epoch 208/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 209/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 210/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 211/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 212/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 213/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 214/300, seasonal_3 Loss: 0.0842 | 0.0559
Epoch 215/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 216/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 217/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 218/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 219/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 220/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 221/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 222/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 223/300, seasonal_3 Loss: 0.0841 | 0.0558
Epoch 224/300, seasonal_3 Loss: 0.0841 | 0.0557
Epoch 225/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 226/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 227/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 228/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 229/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 230/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 231/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 232/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 233/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 234/300, seasonal_3 Loss: 0.0840 | 0.0557
Epoch 235/300, seasonal_3 Loss: 0.0840 | 0.0556
Epoch 236/300, seasonal_3 Loss: 0.0840 | 0.0556
Epoch 237/300, seasonal_3 Loss: 0.0840 | 0.0556
Epoch 238/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 239/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 240/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 241/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 242/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 243/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 244/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 245/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 246/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 247/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 248/300, seasonal_3 Loss: 0.0839 | 0.0556
Epoch 249/300, seasonal_3 Loss: 0.0839 | 0.0555
Epoch 250/300, seasonal_3 Loss: 0.0839 | 0.0555
Epoch 251/300, seasonal_3 Loss: 0.0839 | 0.0555
Epoch 252/300, seasonal_3 Loss: 0.0839 | 0.0555
Epoch 253/300, seasonal_3 Loss: 0.0839 | 0.0555
Epoch 254/300, seasonal_3 Loss: 0.0839 | 0.0555
Epoch 255/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 256/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 257/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 258/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 259/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 260/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 261/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 262/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 263/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 264/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 265/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 266/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 267/300, seasonal_3 Loss: 0.0838 | 0.0555
Epoch 268/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 269/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 270/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 271/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 272/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 273/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 274/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 275/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 276/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 277/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 278/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 279/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 280/300, seasonal_3 Loss: 0.0838 | 0.0554
Epoch 281/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 282/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 283/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 284/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 285/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 286/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 287/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 288/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 289/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 290/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 291/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 292/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 293/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 294/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 295/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 296/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 297/300, seasonal_3 Loss: 0.0837 | 0.0554
Epoch 298/300, seasonal_3 Loss: 0.0837 | 0.0553
Epoch 299/300, seasonal_3 Loss: 0.0837 | 0.0553
Epoch 300/300, seasonal_3 Loss: 0.0837 | 0.0553
Training resid component with params: {'observation_period_num': 7, 'train_rates': 0.9124328692025402, 'learning_rate': 7.465923128929692e-05, 'batch_size': 107, 'step_size': 8, 'gamma': 0.9300217540955196}
Epoch 1/300, resid Loss: 0.2918 | 0.2825
Epoch 2/300, resid Loss: 0.1790 | 0.2935
Epoch 3/300, resid Loss: 0.1708 | 0.2769
Epoch 4/300, resid Loss: 0.1513 | 0.1759
Epoch 5/300, resid Loss: 0.1476 | 0.1531
Epoch 6/300, resid Loss: 0.1375 | 0.1430
Epoch 7/300, resid Loss: 0.1247 | 0.1388
Epoch 8/300, resid Loss: 0.1193 | 0.1302
Epoch 9/300, resid Loss: 0.1154 | 0.1160
Epoch 10/300, resid Loss: 0.1133 | 0.1034
Epoch 11/300, resid Loss: 0.1126 | 0.0942
Epoch 12/300, resid Loss: 0.1117 | 0.0885
Epoch 13/300, resid Loss: 0.1095 | 0.0846
Epoch 14/300, resid Loss: 0.1069 | 0.0812
Epoch 15/300, resid Loss: 0.1074 | 0.0811
Epoch 16/300, resid Loss: 0.1143 | 0.0974
Epoch 17/300, resid Loss: 0.1228 | 0.1475
Epoch 18/300, resid Loss: 0.1160 | 0.0993
Epoch 19/300, resid Loss: 0.1090 | 0.0858
Epoch 20/300, resid Loss: 0.1181 | 0.0883
Epoch 21/300, resid Loss: 0.1106 | 0.0764
Epoch 22/300, resid Loss: 0.1019 | 0.0756
Epoch 23/300, resid Loss: 0.1016 | 0.0726
Epoch 24/300, resid Loss: 0.0995 | 0.0714
Epoch 25/300, resid Loss: 0.0984 | 0.0707
Epoch 26/300, resid Loss: 0.0980 | 0.0701
Epoch 27/300, resid Loss: 0.0976 | 0.0693
Epoch 28/300, resid Loss: 0.0970 | 0.0686
Epoch 29/300, resid Loss: 0.0963 | 0.0680
Epoch 30/300, resid Loss: 0.0955 | 0.0673
Epoch 31/300, resid Loss: 0.0950 | 0.0666
Epoch 32/300, resid Loss: 0.0944 | 0.0660
Epoch 33/300, resid Loss: 0.0938 | 0.0654
Epoch 34/300, resid Loss: 0.0932 | 0.0646
Epoch 35/300, resid Loss: 0.0926 | 0.0639
Epoch 36/300, resid Loss: 0.0920 | 0.0631
Epoch 37/300, resid Loss: 0.0913 | 0.0624
Epoch 38/300, resid Loss: 0.0906 | 0.0617
Epoch 39/300, resid Loss: 0.0900 | 0.0610
Epoch 40/300, resid Loss: 0.0894 | 0.0602
Epoch 41/300, resid Loss: 0.0888 | 0.0596
Epoch 42/300, resid Loss: 0.0883 | 0.0589
Epoch 43/300, resid Loss: 0.0878 | 0.0583
Epoch 44/300, resid Loss: 0.0874 | 0.0577
Epoch 45/300, resid Loss: 0.0869 | 0.0571
Epoch 46/300, resid Loss: 0.0865 | 0.0565
Epoch 47/300, resid Loss: 0.0861 | 0.0560
Epoch 48/300, resid Loss: 0.0857 | 0.0554
Epoch 49/300, resid Loss: 0.0853 | 0.0548
Epoch 50/300, resid Loss: 0.0849 | 0.0543
Epoch 51/300, resid Loss: 0.0845 | 0.0538
Epoch 52/300, resid Loss: 0.0841 | 0.0532
Epoch 53/300, resid Loss: 0.0837 | 0.0527
Epoch 54/300, resid Loss: 0.0833 | 0.0522
Epoch 55/300, resid Loss: 0.0830 | 0.0518
Epoch 56/300, resid Loss: 0.0827 | 0.0514
Epoch 57/300, resid Loss: 0.0825 | 0.0510
Epoch 58/300, resid Loss: 0.0824 | 0.0507
Epoch 59/300, resid Loss: 0.0824 | 0.0505
Epoch 60/300, resid Loss: 0.0824 | 0.0503
Epoch 61/300, resid Loss: 0.0825 | 0.0499
Epoch 62/300, resid Loss: 0.0823 | 0.0493
Epoch 63/300, resid Loss: 0.0816 | 0.0487
Epoch 64/300, resid Loss: 0.0810 | 0.0484
Epoch 65/300, resid Loss: 0.0806 | 0.0483
Epoch 66/300, resid Loss: 0.0805 | 0.0480
Epoch 67/300, resid Loss: 0.0803 | 0.0477
Epoch 68/300, resid Loss: 0.0801 | 0.0472
Epoch 69/300, resid Loss: 0.0799 | 0.0467
Epoch 70/300, resid Loss: 0.0797 | 0.0465
Epoch 71/300, resid Loss: 0.0796 | 0.0464
Epoch 72/300, resid Loss: 0.0795 | 0.0463
Epoch 73/300, resid Loss: 0.0793 | 0.0461
Epoch 74/300, resid Loss: 0.0791 | 0.0458
Epoch 75/300, resid Loss: 0.0789 | 0.0458
Epoch 76/300, resid Loss: 0.0788 | 0.0458
Epoch 77/300, resid Loss: 0.0787 | 0.0457
Epoch 78/300, resid Loss: 0.0785 | 0.0453
Epoch 79/300, resid Loss: 0.0784 | 0.0449
Epoch 80/300, resid Loss: 0.0782 | 0.0446
Epoch 81/300, resid Loss: 0.0781 | 0.0445
Epoch 82/300, resid Loss: 0.0781 | 0.0444
Epoch 83/300, resid Loss: 0.0780 | 0.0443
Epoch 84/300, resid Loss: 0.0779 | 0.0442
Epoch 85/300, resid Loss: 0.0778 | 0.0441
Epoch 86/300, resid Loss: 0.0776 | 0.0440
Epoch 87/300, resid Loss: 0.0775 | 0.0440
Epoch 88/300, resid Loss: 0.0774 | 0.0439
Epoch 89/300, resid Loss: 0.0773 | 0.0437
Epoch 90/300, resid Loss: 0.0771 | 0.0435
Epoch 91/300, resid Loss: 0.0771 | 0.0433
Epoch 92/300, resid Loss: 0.0770 | 0.0432
Epoch 93/300, resid Loss: 0.0770 | 0.0431
Epoch 94/300, resid Loss: 0.0770 | 0.0431
Epoch 95/300, resid Loss: 0.0769 | 0.0430
Epoch 96/300, resid Loss: 0.0768 | 0.0430
Epoch 97/300, resid Loss: 0.0767 | 0.0429
Epoch 98/300, resid Loss: 0.0766 | 0.0429
Epoch 99/300, resid Loss: 0.0765 | 0.0428
Epoch 100/300, resid Loss: 0.0765 | 0.0427
Epoch 101/300, resid Loss: 0.0765 | 0.0426
Epoch 102/300, resid Loss: 0.0766 | 0.0426
Epoch 103/300, resid Loss: 0.0767 | 0.0425
Epoch 104/300, resid Loss: 0.0769 | 0.0425
Epoch 105/300, resid Loss: 0.0772 | 0.0426
Epoch 106/300, resid Loss: 0.0776 | 0.0427
Epoch 107/300, resid Loss: 0.0783 | 0.0429
Epoch 108/300, resid Loss: 0.0792 | 0.0431
Epoch 109/300, resid Loss: 0.0800 | 0.0430
Epoch 110/300, resid Loss: 0.0796 | 0.0428
Epoch 111/300, resid Loss: 0.0787 | 0.0440
Epoch 112/300, resid Loss: 0.0805 | 0.0470
Epoch 113/300, resid Loss: 0.0852 | 0.0485
Epoch 114/300, resid Loss: 0.0860 | 0.0448
Epoch 115/300, resid Loss: 0.0792 | 0.0432
Epoch 116/300, resid Loss: 0.0761 | 0.0428
Epoch 117/300, resid Loss: 0.0765 | 0.0423
Epoch 118/300, resid Loss: 0.0758 | 0.0421
Epoch 119/300, resid Loss: 0.0753 | 0.0421
Epoch 120/300, resid Loss: 0.0752 | 0.0421
Epoch 121/300, resid Loss: 0.0752 | 0.0420
Epoch 122/300, resid Loss: 0.0751 | 0.0419
Epoch 123/300, resid Loss: 0.0751 | 0.0418
Epoch 124/300, resid Loss: 0.0750 | 0.0418
Epoch 125/300, resid Loss: 0.0750 | 0.0417
Epoch 126/300, resid Loss: 0.0749 | 0.0417
Epoch 127/300, resid Loss: 0.0749 | 0.0416
Epoch 128/300, resid Loss: 0.0748 | 0.0416
Epoch 129/300, resid Loss: 0.0748 | 0.0415
Epoch 130/300, resid Loss: 0.0747 | 0.0415
Epoch 131/300, resid Loss: 0.0747 | 0.0414
Epoch 132/300, resid Loss: 0.0746 | 0.0414
Epoch 133/300, resid Loss: 0.0746 | 0.0413
Epoch 134/300, resid Loss: 0.0746 | 0.0413
Epoch 135/300, resid Loss: 0.0745 | 0.0412
Epoch 136/300, resid Loss: 0.0745 | 0.0412
Epoch 137/300, resid Loss: 0.0744 | 0.0411
Epoch 138/300, resid Loss: 0.0744 | 0.0411
Epoch 139/300, resid Loss: 0.0743 | 0.0411
Epoch 140/300, resid Loss: 0.0743 | 0.0410
Epoch 141/300, resid Loss: 0.0743 | 0.0410
Epoch 142/300, resid Loss: 0.0742 | 0.0410
Epoch 143/300, resid Loss: 0.0742 | 0.0409
Epoch 144/300, resid Loss: 0.0742 | 0.0409
Epoch 145/300, resid Loss: 0.0741 | 0.0408
Epoch 146/300, resid Loss: 0.0741 | 0.0408
Epoch 147/300, resid Loss: 0.0740 | 0.0408
Epoch 148/300, resid Loss: 0.0740 | 0.0407
Epoch 149/300, resid Loss: 0.0740 | 0.0407
Epoch 150/300, resid Loss: 0.0739 | 0.0407
Epoch 151/300, resid Loss: 0.0739 | 0.0406
Epoch 152/300, resid Loss: 0.0739 | 0.0406
Epoch 153/300, resid Loss: 0.0738 | 0.0406
Epoch 154/300, resid Loss: 0.0738 | 0.0405
Epoch 155/300, resid Loss: 0.0738 | 0.0405
Epoch 156/300, resid Loss: 0.0737 | 0.0405
Epoch 157/300, resid Loss: 0.0737 | 0.0405
Epoch 158/300, resid Loss: 0.0737 | 0.0404
Epoch 159/300, resid Loss: 0.0736 | 0.0404
Epoch 160/300, resid Loss: 0.0736 | 0.0404
Epoch 161/300, resid Loss: 0.0736 | 0.0403
Epoch 162/300, resid Loss: 0.0735 | 0.0403
Epoch 163/300, resid Loss: 0.0735 | 0.0403
Epoch 164/300, resid Loss: 0.0735 | 0.0403
Epoch 165/300, resid Loss: 0.0734 | 0.0402
Epoch 166/300, resid Loss: 0.0734 | 0.0402
Epoch 167/300, resid Loss: 0.0734 | 0.0402
Epoch 168/300, resid Loss: 0.0733 | 0.0401
Epoch 169/300, resid Loss: 0.0733 | 0.0401
Epoch 170/300, resid Loss: 0.0733 | 0.0401
Epoch 171/300, resid Loss: 0.0733 | 0.0401
Epoch 172/300, resid Loss: 0.0732 | 0.0400
Epoch 173/300, resid Loss: 0.0732 | 0.0400
Epoch 174/300, resid Loss: 0.0732 | 0.0400
Epoch 175/300, resid Loss: 0.0731 | 0.0400
Epoch 176/300, resid Loss: 0.0731 | 0.0400
Epoch 177/300, resid Loss: 0.0731 | 0.0399
Epoch 178/300, resid Loss: 0.0731 | 0.0399
Epoch 179/300, resid Loss: 0.0730 | 0.0399
Epoch 180/300, resid Loss: 0.0730 | 0.0399
Epoch 181/300, resid Loss: 0.0730 | 0.0398
Epoch 182/300, resid Loss: 0.0730 | 0.0398
Epoch 183/300, resid Loss: 0.0729 | 0.0398
Epoch 184/300, resid Loss: 0.0729 | 0.0398
Epoch 185/300, resid Loss: 0.0729 | 0.0398
Epoch 186/300, resid Loss: 0.0729 | 0.0397
Epoch 187/300, resid Loss: 0.0728 | 0.0397
Epoch 188/300, resid Loss: 0.0728 | 0.0397
Epoch 189/300, resid Loss: 0.0728 | 0.0397
Epoch 190/300, resid Loss: 0.0728 | 0.0396
Epoch 191/300, resid Loss: 0.0727 | 0.0396
Epoch 192/300, resid Loss: 0.0727 | 0.0396
Epoch 193/300, resid Loss: 0.0727 | 0.0396
Epoch 194/300, resid Loss: 0.0727 | 0.0396
Epoch 195/300, resid Loss: 0.0726 | 0.0396
Epoch 196/300, resid Loss: 0.0726 | 0.0395
Epoch 197/300, resid Loss: 0.0726 | 0.0395
Epoch 198/300, resid Loss: 0.0726 | 0.0395
Epoch 199/300, resid Loss: 0.0726 | 0.0395
Epoch 200/300, resid Loss: 0.0725 | 0.0395
Epoch 201/300, resid Loss: 0.0725 | 0.0394
Epoch 202/300, resid Loss: 0.0725 | 0.0394
Epoch 203/300, resid Loss: 0.0725 | 0.0394
Epoch 204/300, resid Loss: 0.0724 | 0.0394
Epoch 205/300, resid Loss: 0.0724 | 0.0394
Epoch 206/300, resid Loss: 0.0724 | 0.0394
Epoch 207/300, resid Loss: 0.0724 | 0.0393
Epoch 208/300, resid Loss: 0.0724 | 0.0393
Epoch 209/300, resid Loss: 0.0724 | 0.0393
Epoch 210/300, resid Loss: 0.0723 | 0.0393
Epoch 211/300, resid Loss: 0.0723 | 0.0393
Epoch 212/300, resid Loss: 0.0723 | 0.0393
Epoch 213/300, resid Loss: 0.0723 | 0.0393
Epoch 214/300, resid Loss: 0.0723 | 0.0392
Epoch 215/300, resid Loss: 0.0722 | 0.0392
Epoch 216/300, resid Loss: 0.0722 | 0.0392
Epoch 217/300, resid Loss: 0.0722 | 0.0392
Epoch 218/300, resid Loss: 0.0722 | 0.0392
Epoch 219/300, resid Loss: 0.0722 | 0.0392
Epoch 220/300, resid Loss: 0.0722 | 0.0392
Epoch 221/300, resid Loss: 0.0721 | 0.0391
Epoch 222/300, resid Loss: 0.0721 | 0.0391
Epoch 223/300, resid Loss: 0.0721 | 0.0391
Epoch 224/300, resid Loss: 0.0721 | 0.0391
Epoch 225/300, resid Loss: 0.0721 | 0.0391
Epoch 226/300, resid Loss: 0.0721 | 0.0391
Epoch 227/300, resid Loss: 0.0720 | 0.0391
Epoch 228/300, resid Loss: 0.0720 | 0.0391
Epoch 229/300, resid Loss: 0.0720 | 0.0390
Epoch 230/300, resid Loss: 0.0720 | 0.0390
Epoch 231/300, resid Loss: 0.0720 | 0.0390
Epoch 232/300, resid Loss: 0.0720 | 0.0390
Epoch 233/300, resid Loss: 0.0720 | 0.0390
Epoch 234/300, resid Loss: 0.0719 | 0.0390
Epoch 235/300, resid Loss: 0.0719 | 0.0390
Epoch 236/300, resid Loss: 0.0719 | 0.0390
Epoch 237/300, resid Loss: 0.0719 | 0.0390
Epoch 238/300, resid Loss: 0.0719 | 0.0389
Epoch 239/300, resid Loss: 0.0719 | 0.0389
Epoch 240/300, resid Loss: 0.0719 | 0.0389
Epoch 241/300, resid Loss: 0.0719 | 0.0389
Epoch 242/300, resid Loss: 0.0718 | 0.0389
Epoch 243/300, resid Loss: 0.0718 | 0.0389
Epoch 244/300, resid Loss: 0.0718 | 0.0389
Epoch 245/300, resid Loss: 0.0718 | 0.0389
Epoch 246/300, resid Loss: 0.0718 | 0.0389
Epoch 247/300, resid Loss: 0.0718 | 0.0389
Epoch 248/300, resid Loss: 0.0718 | 0.0389
Epoch 249/300, resid Loss: 0.0718 | 0.0388
Epoch 250/300, resid Loss: 0.0718 | 0.0388
Epoch 251/300, resid Loss: 0.0717 | 0.0388
Epoch 252/300, resid Loss: 0.0717 | 0.0388
Epoch 253/300, resid Loss: 0.0717 | 0.0388
Epoch 254/300, resid Loss: 0.0717 | 0.0388
Epoch 255/300, resid Loss: 0.0717 | 0.0388
Epoch 256/300, resid Loss: 0.0717 | 0.0388
Epoch 257/300, resid Loss: 0.0717 | 0.0388
Epoch 258/300, resid Loss: 0.0717 | 0.0388
Epoch 259/300, resid Loss: 0.0717 | 0.0388
Epoch 260/300, resid Loss: 0.0717 | 0.0388
Epoch 261/300, resid Loss: 0.0716 | 0.0388
Epoch 262/300, resid Loss: 0.0716 | 0.0387
Epoch 263/300, resid Loss: 0.0716 | 0.0387
Epoch 264/300, resid Loss: 0.0716 | 0.0387
Epoch 265/300, resid Loss: 0.0716 | 0.0387
Epoch 266/300, resid Loss: 0.0716 | 0.0387
Epoch 267/300, resid Loss: 0.0716 | 0.0387
Epoch 268/300, resid Loss: 0.0716 | 0.0387
Epoch 269/300, resid Loss: 0.0716 | 0.0387
Epoch 270/300, resid Loss: 0.0716 | 0.0387
Epoch 271/300, resid Loss: 0.0716 | 0.0387
Epoch 272/300, resid Loss: 0.0716 | 0.0387
Epoch 273/300, resid Loss: 0.0716 | 0.0387
Epoch 274/300, resid Loss: 0.0715 | 0.0387
Epoch 275/300, resid Loss: 0.0715 | 0.0387
Epoch 276/300, resid Loss: 0.0715 | 0.0387
Epoch 277/300, resid Loss: 0.0715 | 0.0387
Epoch 278/300, resid Loss: 0.0715 | 0.0387
Epoch 279/300, resid Loss: 0.0715 | 0.0386
Epoch 280/300, resid Loss: 0.0715 | 0.0386
Epoch 281/300, resid Loss: 0.0715 | 0.0386
Epoch 282/300, resid Loss: 0.0715 | 0.0386
Epoch 283/300, resid Loss: 0.0715 | 0.0386
Epoch 284/300, resid Loss: 0.0715 | 0.0386
Epoch 285/300, resid Loss: 0.0715 | 0.0386
Epoch 286/300, resid Loss: 0.0715 | 0.0386
Epoch 287/300, resid Loss: 0.0715 | 0.0386
Epoch 288/300, resid Loss: 0.0715 | 0.0386
Epoch 289/300, resid Loss: 0.0715 | 0.0386
Epoch 290/300, resid Loss: 0.0714 | 0.0386
Epoch 291/300, resid Loss: 0.0714 | 0.0386
Epoch 292/300, resid Loss: 0.0714 | 0.0386
Epoch 293/300, resid Loss: 0.0714 | 0.0386
Epoch 294/300, resid Loss: 0.0714 | 0.0386
Epoch 295/300, resid Loss: 0.0714 | 0.0386
Epoch 296/300, resid Loss: 0.0714 | 0.0386
Epoch 297/300, resid Loss: 0.0714 | 0.0386
Epoch 298/300, resid Loss: 0.0714 | 0.0386
Epoch 299/300, resid Loss: 0.0714 | 0.0386
Epoch 300/300, resid Loss: 0.0714 | 0.0386
Runtime (seconds): 1824.3974168300629
0.0007884243027725701
[155.78064]
[-1.3201123]
[-3.2845368]
[11.4046545]
[3.8160446]
[9.239314]
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 444.1136920908466
RMSE: 21.074005126953125
MAE: 21.074005126953125
R-squared: nan
[175.636]
