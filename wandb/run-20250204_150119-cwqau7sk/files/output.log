[32m[I 2025-02-04 15:01:24,217][0m A new study created in memory with name: no-name-d342d5bf-4266-40fd-a7df-af5fc5d7f2ad[0m
[32m[I 2025-02-04 15:02:21,205][0m Trial 0 finished with value: 0.1475795692107419 and parameters: {'observation_period_num': 32, 'train_rates': 0.7204704200827972, 'learning_rate': 1.7294822300768494e-05, 'batch_size': 85, 'step_size': 8, 'gamma': 0.826369226301221}. Best is trial 0 with value: 0.1475795692107419.[0m
[32m[I 2025-02-04 15:02:49,558][0m Trial 1 finished with value: 0.3632263243198395 and parameters: {'observation_period_num': 231, 'train_rates': 0.950430779686019, 'learning_rate': 1.000109511192775e-05, 'batch_size': 205, 'step_size': 11, 'gamma': 0.8287871132685449}. Best is trial 0 with value: 0.1475795692107419.[0m
[32m[I 2025-02-04 15:03:18,072][0m Trial 2 finished with value: 0.08265448562451638 and parameters: {'observation_period_num': 126, 'train_rates': 0.9102437761603621, 'learning_rate': 0.00034612817236527653, 'batch_size': 205, 'step_size': 10, 'gamma': 0.7555649530835583}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:03:40,582][0m Trial 3 finished with value: 0.1601237577101416 and parameters: {'observation_period_num': 113, 'train_rates': 0.6610700924891086, 'learning_rate': 0.0002270490984978202, 'batch_size': 232, 'step_size': 9, 'gamma': 0.9764435687402826}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:04:56,825][0m Trial 4 finished with value: 0.7645415487399521 and parameters: {'observation_period_num': 27, 'train_rates': 0.7645068541747326, 'learning_rate': 1.2143398284276305e-06, 'batch_size': 68, 'step_size': 2, 'gamma': 0.9642486712850691}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:05:27,630][0m Trial 5 finished with value: 0.6189168095588684 and parameters: {'observation_period_num': 90, 'train_rates': 0.9365740381107059, 'learning_rate': 1.4105392358088978e-06, 'batch_size': 196, 'step_size': 14, 'gamma': 0.9413243004699957}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:06:38,739][0m Trial 6 finished with value: 0.8454531093136288 and parameters: {'observation_period_num': 179, 'train_rates': 0.7172908471797412, 'learning_rate': 2.317063484242182e-06, 'batch_size': 65, 'step_size': 6, 'gamma': 0.8355745648385743}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:07:15,908][0m Trial 7 finished with value: 0.5093678642127474 and parameters: {'observation_period_num': 239, 'train_rates': 0.7840550695532462, 'learning_rate': 2.0039603635475768e-05, 'batch_size': 136, 'step_size': 2, 'gamma': 0.9122637685125006}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:07:44,521][0m Trial 8 finished with value: 0.11479409653989095 and parameters: {'observation_period_num': 155, 'train_rates': 0.8657953461895846, 'learning_rate': 0.000704706839806632, 'batch_size': 192, 'step_size': 8, 'gamma': 0.8262058641809401}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:08:11,976][0m Trial 9 finished with value: 0.2066528962961599 and parameters: {'observation_period_num': 171, 'train_rates': 0.7814554809492127, 'learning_rate': 0.0009091320272205918, 'batch_size': 195, 'step_size': 15, 'gamma': 0.8205590391342302}. Best is trial 2 with value: 0.08265448562451638.[0m
[32m[I 2025-02-04 15:13:05,955][0m Trial 10 finished with value: 0.07858888914297822 and parameters: {'observation_period_num': 79, 'train_rates': 0.8750983661191841, 'learning_rate': 0.00013487948792058834, 'batch_size': 18, 'step_size': 12, 'gamma': 0.7505637337239106}. Best is trial 10 with value: 0.07858888914297822.[0m
[32m[I 2025-02-04 15:17:33,333][0m Trial 11 finished with value: 0.08713182540443855 and parameters: {'observation_period_num': 76, 'train_rates': 0.8726414564890191, 'learning_rate': 0.00012236953927110875, 'batch_size': 20, 'step_size': 12, 'gamma': 0.7524565011859649}. Best is trial 10 with value: 0.07858888914297822.[0m
[32m[I 2025-02-04 15:18:13,973][0m Trial 12 finished with value: 0.06473210826516151 and parameters: {'observation_period_num': 65, 'train_rates': 0.8708568997843559, 'learning_rate': 9.667175256704855e-05, 'batch_size': 141, 'step_size': 12, 'gamma': 0.7502979183628976}. Best is trial 12 with value: 0.06473210826516151.[0m
[32m[I 2025-02-04 15:18:57,888][0m Trial 13 finished with value: 0.06958674251483882 and parameters: {'observation_period_num': 62, 'train_rates': 0.8507757048280596, 'learning_rate': 6.287525279443492e-05, 'batch_size': 128, 'step_size': 12, 'gamma': 0.7846481968732274}. Best is trial 12 with value: 0.06473210826516151.[0m
[32m[I 2025-02-04 15:19:37,517][0m Trial 14 finished with value: 0.06543840569185196 and parameters: {'observation_period_num': 9, 'train_rates': 0.8337028534805019, 'learning_rate': 6.357684988599231e-05, 'batch_size': 142, 'step_size': 13, 'gamma': 0.7870490477325068}. Best is trial 12 with value: 0.06473210826516151.[0m
[32m[I 2025-02-04 15:20:19,766][0m Trial 15 finished with value: 0.06035393103957176 and parameters: {'observation_period_num': 6, 'train_rates': 0.9875342630531181, 'learning_rate': 4.481818974774272e-05, 'batch_size': 153, 'step_size': 14, 'gamma': 0.8792247352717951}. Best is trial 15 with value: 0.06035393103957176.[0m
[32m[I 2025-02-04 15:21:00,574][0m Trial 16 finished with value: 0.3354181945323944 and parameters: {'observation_period_num': 43, 'train_rates': 0.9810028755428684, 'learning_rate': 5.168016615474267e-06, 'batch_size': 159, 'step_size': 15, 'gamma': 0.8806625624276236}. Best is trial 15 with value: 0.06035393103957176.[0m
[32m[I 2025-02-04 15:21:42,979][0m Trial 17 finished with value: 0.2197999283965998 and parameters: {'observation_period_num': 12, 'train_rates': 0.6154653987727854, 'learning_rate': 4.599611207450342e-05, 'batch_size': 107, 'step_size': 6, 'gamma': 0.8749442684330883}. Best is trial 15 with value: 0.06035393103957176.[0m
[32m[I 2025-02-04 15:22:22,204][0m Trial 18 finished with value: 0.11689140647649765 and parameters: {'observation_period_num': 51, 'train_rates': 0.9768222637816251, 'learning_rate': 3.247851402463078e-05, 'batch_size': 162, 'step_size': 14, 'gamma': 0.9070718169452402}. Best is trial 15 with value: 0.06035393103957176.[0m
[32m[I 2025-02-04 15:22:47,898][0m Trial 19 finished with value: 0.4840885465609208 and parameters: {'observation_period_num': 102, 'train_rates': 0.9088930672530453, 'learning_rate': 7.337867114813604e-06, 'batch_size': 249, 'step_size': 6, 'gamma': 0.8578324550805743}. Best is trial 15 with value: 0.06035393103957176.[0m
[32m[I 2025-02-04 15:23:41,198][0m Trial 20 finished with value: 0.057907657666107205 and parameters: {'observation_period_num': 59, 'train_rates': 0.818318785277379, 'learning_rate': 9.976483313097681e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.9011009320530172}. Best is trial 20 with value: 0.057907657666107205.[0m
[32m[I 2025-02-04 15:24:32,415][0m Trial 21 finished with value: 0.060292619277002676 and parameters: {'observation_period_num': 56, 'train_rates': 0.8181596862533053, 'learning_rate': 0.00010865789660382446, 'batch_size': 107, 'step_size': 10, 'gamma': 0.9067961082383543}. Best is trial 20 with value: 0.057907657666107205.[0m
[32m[I 2025-02-04 15:25:25,367][0m Trial 22 finished with value: 0.03359722668902754 and parameters: {'observation_period_num': 8, 'train_rates': 0.8191914796401703, 'learning_rate': 0.00028499471758244795, 'batch_size': 105, 'step_size': 10, 'gamma': 0.9087105635539456}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:26:16,459][0m Trial 23 finished with value: 0.056157761700849095 and parameters: {'observation_period_num': 33, 'train_rates': 0.8086866210892232, 'learning_rate': 0.00028712739470406997, 'batch_size': 107, 'step_size': 10, 'gamma': 0.9158292705664589}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:27:08,626][0m Trial 24 finished with value: 0.10881242156028748 and parameters: {'observation_period_num': 30, 'train_rates': 0.7386918640378941, 'learning_rate': 0.0003793414557397271, 'batch_size': 96, 'step_size': 9, 'gamma': 0.9337609262070392}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:28:36,348][0m Trial 25 finished with value: 0.05478656832903462 and parameters: {'observation_period_num': 25, 'train_rates': 0.8121888479363856, 'learning_rate': 0.0002361122861750215, 'batch_size': 61, 'step_size': 7, 'gamma': 0.9361583808986932}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:30:36,728][0m Trial 26 finished with value: 0.07683176015104566 and parameters: {'observation_period_num': 26, 'train_rates': 0.7504301232243958, 'learning_rate': 0.0005375341127910632, 'batch_size': 41, 'step_size': 5, 'gamma': 0.9409601746750049}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:31:50,062][0m Trial 27 finished with value: 0.056861581649574534 and parameters: {'observation_period_num': 37, 'train_rates': 0.7986557539994434, 'learning_rate': 0.00022806378294443506, 'batch_size': 72, 'step_size': 4, 'gamma': 0.9254230947036107}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:33:24,884][0m Trial 28 finished with value: 0.06198398931319326 and parameters: {'observation_period_num': 18, 'train_rates': 0.706520680656032, 'learning_rate': 0.00023934695690997184, 'batch_size': 50, 'step_size': 8, 'gamma': 0.9588574721073129}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:34:03,936][0m Trial 29 finished with value: 0.14658510391829443 and parameters: {'observation_period_num': 147, 'train_rates': 0.6758055123848989, 'learning_rate': 0.00019662795087143153, 'batch_size': 120, 'step_size': 7, 'gamma': 0.890753096119839}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:35:07,351][0m Trial 30 finished with value: 0.058654145224372276 and parameters: {'observation_period_num': 35, 'train_rates': 0.7982448722094609, 'learning_rate': 0.00047483586913493997, 'batch_size': 85, 'step_size': 4, 'gamma': 0.9831415771511913}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:36:20,904][0m Trial 31 finished with value: 0.05418430053948486 and parameters: {'observation_period_num': 40, 'train_rates': 0.8129949408874093, 'learning_rate': 0.00028708350821300644, 'batch_size': 74, 'step_size': 4, 'gamma': 0.9231106032694932}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:37:26,356][0m Trial 32 finished with value: 0.05835375484503523 and parameters: {'observation_period_num': 45, 'train_rates': 0.8380668881707749, 'learning_rate': 0.00032194122361467686, 'batch_size': 83, 'step_size': 1, 'gamma': 0.9251156219691955}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:39:18,915][0m Trial 33 finished with value: 0.05138707315886369 and parameters: {'observation_period_num': 17, 'train_rates': 0.8149313439519441, 'learning_rate': 0.0009629567254503686, 'batch_size': 47, 'step_size': 7, 'gamma': 0.9540511273544963}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:41:28,099][0m Trial 34 finished with value: 0.10686750767839945 and parameters: {'observation_period_num': 213, 'train_rates': 0.8997045125849602, 'learning_rate': 0.0009415117591352426, 'batch_size': 41, 'step_size': 4, 'gamma': 0.9566363493419732}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:43:01,555][0m Trial 35 finished with value: 0.06302212734251868 and parameters: {'observation_period_num': 20, 'train_rates': 0.7669253264975363, 'learning_rate': 0.0006087075617368744, 'batch_size': 54, 'step_size': 7, 'gamma': 0.94809990854575}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:45:32,002][0m Trial 36 finished with value: 0.04823502500800101 and parameters: {'observation_period_num': 5, 'train_rates': 0.8385006010794359, 'learning_rate': 0.00018408849641326962, 'batch_size': 36, 'step_size': 3, 'gamma': 0.9716822913494926}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:48:26,299][0m Trial 37 finished with value: 0.033600522959722746 and parameters: {'observation_period_num': 5, 'train_rates': 0.8934992245728399, 'learning_rate': 0.00015447717306572923, 'batch_size': 32, 'step_size': 2, 'gamma': 0.9694281039110392}. Best is trial 22 with value: 0.03359722668902754.[0m
[32m[I 2025-02-04 15:51:34,578][0m Trial 38 finished with value: 0.03140601597420157 and parameters: {'observation_period_num': 5, 'train_rates': 0.9510004669530714, 'learning_rate': 0.00014607025704344698, 'batch_size': 31, 'step_size': 2, 'gamma': 0.972871228278634}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 15:54:33,454][0m Trial 39 finished with value: 0.05071725355277599 and parameters: {'observation_period_num': 7, 'train_rates': 0.9521114160059243, 'learning_rate': 1.692397575220781e-05, 'batch_size': 33, 'step_size': 2, 'gamma': 0.9722222074987934}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 15:57:18,469][0m Trial 40 finished with value: 0.1292955413698516 and parameters: {'observation_period_num': 201, 'train_rates': 0.9322874808926639, 'learning_rate': 0.00016113054510188753, 'batch_size': 33, 'step_size': 1, 'gamma': 0.9885202710052116}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:00:41,811][0m Trial 41 finished with value: 0.04476862888138993 and parameters: {'observation_period_num': 9, 'train_rates': 0.9553818540095868, 'learning_rate': 2.401108436821001e-05, 'batch_size': 29, 'step_size': 2, 'gamma': 0.9694205317160202}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:04:18,461][0m Trial 42 finished with value: 0.04330471512350373 and parameters: {'observation_period_num': 7, 'train_rates': 0.955234815481297, 'learning_rate': 1.9645963116395253e-05, 'batch_size': 27, 'step_size': 3, 'gamma': 0.9719863964388736}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:08:02,972][0m Trial 43 finished with value: 0.0968013814626596 and parameters: {'observation_period_num': 129, 'train_rates': 0.9588653794192935, 'learning_rate': 1.8059013147519322e-05, 'batch_size': 25, 'step_size': 3, 'gamma': 0.9700302687776727}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:12:00,888][0m Trial 44 finished with value: 0.056295337411464676 and parameters: {'observation_period_num': 20, 'train_rates': 0.9286954971397858, 'learning_rate': 1.1303376558348136e-05, 'batch_size': 24, 'step_size': 2, 'gamma': 0.9817586390375235}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:13:35,293][0m Trial 45 finished with value: 0.07478225781865742 and parameters: {'observation_period_num': 80, 'train_rates': 0.920510832200381, 'learning_rate': 2.7890388078974073e-05, 'batch_size': 60, 'step_size': 3, 'gamma': 0.9641651498335795}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:14:08,150][0m Trial 46 finished with value: 0.10425600888136821 and parameters: {'observation_period_num': 27, 'train_rates': 0.8927560818679109, 'learning_rate': 6.430327723403339e-05, 'batch_size': 181, 'step_size': 1, 'gamma': 0.9471135586913729}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:14:37,279][0m Trial 47 finished with value: 0.12550997734069824 and parameters: {'observation_period_num': 46, 'train_rates': 0.9462919907221414, 'learning_rate': 2.773673666547898e-05, 'batch_size': 219, 'step_size': 2, 'gamma': 0.9781147377898173}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:20:14,817][0m Trial 48 finished with value: 0.2717166323329985 and parameters: {'observation_period_num': 73, 'train_rates': 0.9665639843967738, 'learning_rate': 4.11212824849174e-06, 'batch_size': 17, 'step_size': 3, 'gamma': 0.8608690600846725}. Best is trial 38 with value: 0.03140601597420157.[0m
[32m[I 2025-02-04 16:23:11,737][0m Trial 49 finished with value: 0.07349488303865775 and parameters: {'observation_period_num': 94, 'train_rates': 0.915984852314884, 'learning_rate': 1.3782667066141851e-05, 'batch_size': 31, 'step_size': 5, 'gamma': 0.9665985291175021}. Best is trial 38 with value: 0.03140601597420157.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_XOM_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.1366 | 0.0951
Epoch 2/300, Loss: 0.0896 | 0.0787
Epoch 3/300, Loss: 0.0755 | 0.0719
Epoch 4/300, Loss: 0.0678 | 0.0694
Epoch 5/300, Loss: 0.0638 | 0.0670
Epoch 6/300, Loss: 0.0592 | 0.0640
Epoch 7/300, Loss: 0.0549 | 0.0611
Epoch 8/300, Loss: 0.0517 | 0.0586
Epoch 9/300, Loss: 0.0491 | 0.0564
Epoch 10/300, Loss: 0.0470 | 0.0544
Epoch 11/300, Loss: 0.0453 | 0.0526
Epoch 12/300, Loss: 0.0439 | 0.0510
Epoch 13/300, Loss: 0.0429 | 0.0496
Epoch 14/300, Loss: 0.0420 | 0.0484
Epoch 15/300, Loss: 0.0410 | 0.0473
Epoch 16/300, Loss: 0.0398 | 0.0462
Epoch 17/300, Loss: 0.0387 | 0.0452
Epoch 18/300, Loss: 0.0377 | 0.0441
Epoch 19/300, Loss: 0.0368 | 0.0432
Epoch 20/300, Loss: 0.0360 | 0.0423
Epoch 21/300, Loss: 0.0353 | 0.0415
Epoch 22/300, Loss: 0.0347 | 0.0408
Epoch 23/300, Loss: 0.0343 | 0.0402
Epoch 24/300, Loss: 0.0339 | 0.0397
Epoch 25/300, Loss: 0.0336 | 0.0392
Epoch 26/300, Loss: 0.0332 | 0.0389
Epoch 27/300, Loss: 0.0329 | 0.0386
Epoch 28/300, Loss: 0.0325 | 0.0385
Epoch 29/300, Loss: 0.0322 | 0.0383
Epoch 30/300, Loss: 0.0319 | 0.0381
Epoch 31/300, Loss: 0.0316 | 0.0380
Epoch 32/300, Loss: 0.0314 | 0.0378
Epoch 33/300, Loss: 0.0312 | 0.0377
Epoch 34/300, Loss: 0.0309 | 0.0376
Epoch 35/300, Loss: 0.0307 | 0.0375
Epoch 36/300, Loss: 0.0305 | 0.0373
Epoch 37/300, Loss: 0.0303 | 0.0372
Epoch 38/300, Loss: 0.0301 | 0.0370
Epoch 39/300, Loss: 0.0299 | 0.0369
Epoch 40/300, Loss: 0.0297 | 0.0367
Epoch 41/300, Loss: 0.0295 | 0.0365
Epoch 42/300, Loss: 0.0293 | 0.0363
Epoch 43/300, Loss: 0.0291 | 0.0361
Epoch 44/300, Loss: 0.0289 | 0.0360
Epoch 45/300, Loss: 0.0287 | 0.0356
Epoch 46/300, Loss: 0.0284 | 0.0351
Epoch 47/300, Loss: 0.0282 | 0.0348
Epoch 48/300, Loss: 0.0279 | 0.0346
Epoch 49/300, Loss: 0.0277 | 0.0344
Epoch 50/300, Loss: 0.0275 | 0.0342
Epoch 51/300, Loss: 0.0272 | 0.0340
Epoch 52/300, Loss: 0.0270 | 0.0339
Epoch 53/300, Loss: 0.0267 | 0.0337
Epoch 54/300, Loss: 0.0265 | 0.0336
Epoch 55/300, Loss: 0.0263 | 0.0334
Epoch 56/300, Loss: 0.0262 | 0.0333
Epoch 57/300, Loss: 0.0260 | 0.0331
Epoch 58/300, Loss: 0.0259 | 0.0331
Epoch 59/300, Loss: 0.0259 | 0.0330
Epoch 60/300, Loss: 0.0260 | 0.0327
Epoch 61/300, Loss: 0.0262 | 0.0327
Epoch 62/300, Loss: 0.0263 | 0.0329
Epoch 63/300, Loss: 0.0263 | 0.0332
Epoch 64/300, Loss: 0.0263 | 0.0335
Epoch 65/300, Loss: 0.0261 | 0.0333
Epoch 66/300, Loss: 0.0257 | 0.0331
Epoch 67/300, Loss: 0.0254 | 0.0328
Epoch 68/300, Loss: 0.0252 | 0.0326
Epoch 69/300, Loss: 0.0250 | 0.0324
Epoch 70/300, Loss: 0.0249 | 0.0324
Epoch 71/300, Loss: 0.0248 | 0.0321
Epoch 72/300, Loss: 0.0247 | 0.0322
Epoch 73/300, Loss: 0.0246 | 0.0320
Epoch 74/300, Loss: 0.0247 | 0.0322
Epoch 75/300, Loss: 0.0245 | 0.0319
Epoch 76/300, Loss: 0.0246 | 0.0321
Epoch 77/300, Loss: 0.0243 | 0.0319
Epoch 78/300, Loss: 0.0243 | 0.0320
Epoch 79/300, Loss: 0.0241 | 0.0318
Epoch 80/300, Loss: 0.0240 | 0.0319
Epoch 81/300, Loss: 0.0239 | 0.0318
Epoch 82/300, Loss: 0.0238 | 0.0318
Epoch 83/300, Loss: 0.0238 | 0.0318
Epoch 84/300, Loss: 0.0237 | 0.0318
Epoch 85/300, Loss: 0.0236 | 0.0318
Epoch 86/300, Loss: 0.0236 | 0.0318
Epoch 87/300, Loss: 0.0235 | 0.0318
Epoch 88/300, Loss: 0.0235 | 0.0319
Epoch 89/300, Loss: 0.0234 | 0.0319
Epoch 90/300, Loss: 0.0234 | 0.0319
Epoch 91/300, Loss: 0.0233 | 0.0319
Epoch 92/300, Loss: 0.0233 | 0.0320
Epoch 93/300, Loss: 0.0232 | 0.0320
Epoch 94/300, Loss: 0.0232 | 0.0320
Epoch 95/300, Loss: 0.0232 | 0.0320
Epoch 96/300, Loss: 0.0231 | 0.0321
Epoch 97/300, Loss: 0.0231 | 0.0321
Epoch 98/300, Loss: 0.0231 | 0.0321
Epoch 99/300, Loss: 0.0230 | 0.0321
Epoch 100/300, Loss: 0.0230 | 0.0321
Epoch 101/300, Loss: 0.0230 | 0.0321
Epoch 102/300, Loss: 0.0230 | 0.0321
Epoch 103/300, Loss: 0.0229 | 0.0321
Epoch 104/300, Loss: 0.0229 | 0.0321
Epoch 105/300, Loss: 0.0229 | 0.0321
Epoch 106/300, Loss: 0.0229 | 0.0320
Epoch 107/300, Loss: 0.0228 | 0.0320
Epoch 108/300, Loss: 0.0228 | 0.0320
Epoch 109/300, Loss: 0.0228 | 0.0320
Epoch 110/300, Loss: 0.0227 | 0.0320
Epoch 111/300, Loss: 0.0227 | 0.0320
Epoch 112/300, Loss: 0.0227 | 0.0320
Epoch 113/300, Loss: 0.0227 | 0.0320
Epoch 114/300, Loss: 0.0226 | 0.0320
Epoch 115/300, Loss: 0.0226 | 0.0320
Epoch 116/300, Loss: 0.0226 | 0.0320
Epoch 117/300, Loss: 0.0226 | 0.0320
Epoch 118/300, Loss: 0.0225 | 0.0320
Epoch 119/300, Loss: 0.0225 | 0.0320
Epoch 120/300, Loss: 0.0225 | 0.0320
Epoch 121/300, Loss: 0.0225 | 0.0320
Epoch 122/300, Loss: 0.0225 | 0.0320
Epoch 123/300, Loss: 0.0224 | 0.0320
Epoch 124/300, Loss: 0.0224 | 0.0320
Epoch 125/300, Loss: 0.0224 | 0.0320
Epoch 126/300, Loss: 0.0224 | 0.0320
Epoch 127/300, Loss: 0.0224 | 0.0320
Epoch 128/300, Loss: 0.0223 | 0.0320
Epoch 129/300, Loss: 0.0223 | 0.0321
Epoch 130/300, Loss: 0.0223 | 0.0321
Epoch 131/300, Loss: 0.0223 | 0.0321
Epoch 132/300, Loss: 0.0223 | 0.0322
Epoch 133/300, Loss: 0.0223 | 0.0322
Epoch 134/300, Loss: 0.0223 | 0.0322
Epoch 135/300, Loss: 0.0223 | 0.0322
Epoch 136/300, Loss: 0.0222 | 0.0323
Epoch 137/300, Loss: 0.0222 | 0.0323
Epoch 138/300, Loss: 0.0222 | 0.0323
Epoch 139/300, Loss: 0.0222 | 0.0323
Epoch 140/300, Loss: 0.0222 | 0.0324
Epoch 141/300, Loss: 0.0222 | 0.0324
Epoch 142/300, Loss: 0.0222 | 0.0324
Epoch 143/300, Loss: 0.0222 | 0.0324
Epoch 144/300, Loss: 0.0222 | 0.0324
Epoch 145/300, Loss: 0.0222 | 0.0325
Epoch 146/300, Loss: 0.0222 | 0.0325
Epoch 147/300, Loss: 0.0222 | 0.0325
Epoch 148/300, Loss: 0.0222 | 0.0325
Epoch 149/300, Loss: 0.0222 | 0.0325
Epoch 150/300, Loss: 0.0221 | 0.0325
Epoch 151/300, Loss: 0.0221 | 0.0325
Epoch 152/300, Loss: 0.0221 | 0.0325
Epoch 153/300, Loss: 0.0221 | 0.0325
Epoch 154/300, Loss: 0.0221 | 0.0325
Epoch 155/300, Loss: 0.0221 | 0.0325
Epoch 156/300, Loss: 0.0221 | 0.0325
Epoch 157/300, Loss: 0.0220 | 0.0325
Epoch 158/300, Loss: 0.0220 | 0.0325
Epoch 159/300, Loss: 0.0220 | 0.0325
Epoch 160/300, Loss: 0.0220 | 0.0325
Epoch 161/300, Loss: 0.0220 | 0.0325
Epoch 162/300, Loss: 0.0220 | 0.0325
Epoch 163/300, Loss: 0.0220 | 0.0325
Epoch 164/300, Loss: 0.0220 | 0.0325
Epoch 165/300, Loss: 0.0220 | 0.0325
Epoch 166/300, Loss: 0.0219 | 0.0325
Epoch 167/300, Loss: 0.0219 | 0.0325
Epoch 168/300, Loss: 0.0219 | 0.0325
Epoch 169/300, Loss: 0.0219 | 0.0325
Epoch 170/300, Loss: 0.0219 | 0.0325
Epoch 171/300, Loss: 0.0219 | 0.0325
Epoch 172/300, Loss: 0.0219 | 0.0325
Epoch 173/300, Loss: 0.0219 | 0.0325
Epoch 174/300, Loss: 0.0219 | 0.0325
Epoch 175/300, Loss: 0.0219 | 0.0324
Epoch 176/300, Loss: 0.0218 | 0.0324
Epoch 177/300, Loss: 0.0218 | 0.0324
Epoch 178/300, Loss: 0.0218 | 0.0324
Epoch 179/300, Loss: 0.0218 | 0.0324
Epoch 180/300, Loss: 0.0218 | 0.0324
Epoch 181/300, Loss: 0.0218 | 0.0324
Epoch 182/300, Loss: 0.0218 | 0.0324
Epoch 183/300, Loss: 0.0218 | 0.0324
Epoch 184/300, Loss: 0.0218 | 0.0324
Epoch 185/300, Loss: 0.0218 | 0.0324
Epoch 186/300, Loss: 0.0218 | 0.0324
Epoch 187/300, Loss: 0.0218 | 0.0324
Epoch 188/300, Loss: 0.0218 | 0.0324
Epoch 189/300, Loss: 0.0218 | 0.0324
Epoch 190/300, Loss: 0.0218 | 0.0324
Epoch 191/300, Loss: 0.0218 | 0.0324
Epoch 192/300, Loss: 0.0218 | 0.0324
Epoch 193/300, Loss: 0.0218 | 0.0324
Epoch 194/300, Loss: 0.0217 | 0.0324
Epoch 195/300, Loss: 0.0217 | 0.0324
Epoch 196/300, Loss: 0.0217 | 0.0324
Epoch 197/300, Loss: 0.0217 | 0.0324
Epoch 198/300, Loss: 0.0217 | 0.0324
Epoch 199/300, Loss: 0.0217 | 0.0324
Epoch 200/300, Loss: 0.0217 | 0.0324
Epoch 201/300, Loss: 0.0217 | 0.0324
Epoch 202/300, Loss: 0.0217 | 0.0323
Epoch 203/300, Loss: 0.0217 | 0.0323
Epoch 204/300, Loss: 0.0217 | 0.0323
Epoch 205/300, Loss: 0.0217 | 0.0323
Epoch 206/300, Loss: 0.0217 | 0.0323
Epoch 207/300, Loss: 0.0217 | 0.0323
Epoch 208/300, Loss: 0.0217 | 0.0323
Epoch 209/300, Loss: 0.0217 | 0.0323
Epoch 210/300, Loss: 0.0217 | 0.0323
Epoch 211/300, Loss: 0.0217 | 0.0323
Epoch 212/300, Loss: 0.0217 | 0.0323
Epoch 213/300, Loss: 0.0217 | 0.0323
Epoch 214/300, Loss: 0.0217 | 0.0323
Epoch 215/300, Loss: 0.0217 | 0.0323
Epoch 216/300, Loss: 0.0217 | 0.0323
Epoch 217/300, Loss: 0.0217 | 0.0323
Epoch 218/300, Loss: 0.0217 | 0.0323
Epoch 219/300, Loss: 0.0217 | 0.0323
Epoch 220/300, Loss: 0.0217 | 0.0323
Epoch 221/300, Loss: 0.0217 | 0.0323
Epoch 222/300, Loss: 0.0217 | 0.0323
Epoch 223/300, Loss: 0.0217 | 0.0323
Epoch 224/300, Loss: 0.0217 | 0.0323
Epoch 225/300, Loss: 0.0217 | 0.0323
Epoch 226/300, Loss: 0.0217 | 0.0323
Epoch 227/300, Loss: 0.0217 | 0.0323
Epoch 228/300, Loss: 0.0217 | 0.0323
Epoch 229/300, Loss: 0.0217 | 0.0323
Epoch 230/300, Loss: 0.0217 | 0.0323
Epoch 231/300, Loss: 0.0217 | 0.0323
Epoch 232/300, Loss: 0.0217 | 0.0323
Epoch 233/300, Loss: 0.0217 | 0.0323
Epoch 234/300, Loss: 0.0216 | 0.0323
Epoch 235/300, Loss: 0.0216 | 0.0323
Epoch 236/300, Loss: 0.0216 | 0.0323
Epoch 237/300, Loss: 0.0216 | 0.0323
Epoch 238/300, Loss: 0.0216 | 0.0323
Epoch 239/300, Loss: 0.0216 | 0.0323
Epoch 240/300, Loss: 0.0216 | 0.0323
Epoch 241/300, Loss: 0.0216 | 0.0323
Epoch 242/300, Loss: 0.0216 | 0.0323
Epoch 243/300, Loss: 0.0216 | 0.0323
Epoch 244/300, Loss: 0.0216 | 0.0323
Epoch 245/300, Loss: 0.0216 | 0.0323
Epoch 246/300, Loss: 0.0216 | 0.0323
Epoch 247/300, Loss: 0.0216 | 0.0323
Epoch 248/300, Loss: 0.0216 | 0.0323
Epoch 249/300, Loss: 0.0216 | 0.0323
Epoch 250/300, Loss: 0.0216 | 0.0323
Epoch 251/300, Loss: 0.0216 | 0.0323
Epoch 252/300, Loss: 0.0216 | 0.0323
Epoch 253/300, Loss: 0.0216 | 0.0323
Epoch 254/300, Loss: 0.0216 | 0.0323
Epoch 255/300, Loss: 0.0216 | 0.0323
Epoch 256/300, Loss: 0.0216 | 0.0323
Epoch 257/300, Loss: 0.0216 | 0.0323
Epoch 258/300, Loss: 0.0216 | 0.0323
Epoch 259/300, Loss: 0.0216 | 0.0323
Epoch 260/300, Loss: 0.0216 | 0.0323
Epoch 261/300, Loss: 0.0216 | 0.0323
Epoch 262/300, Loss: 0.0216 | 0.0323
Epoch 263/300, Loss: 0.0216 | 0.0323
Epoch 264/300, Loss: 0.0216 | 0.0323
Epoch 265/300, Loss: 0.0216 | 0.0323
Epoch 266/300, Loss: 0.0216 | 0.0323
Epoch 267/300, Loss: 0.0216 | 0.0323
Epoch 268/300, Loss: 0.0216 | 0.0323
Epoch 269/300, Loss: 0.0216 | 0.0323
Epoch 270/300, Loss: 0.0216 | 0.0323
Epoch 271/300, Loss: 0.0216 | 0.0323
Epoch 272/300, Loss: 0.0216 | 0.0323
Epoch 273/300, Loss: 0.0216 | 0.0323
Epoch 274/300, Loss: 0.0216 | 0.0323
Epoch 275/300, Loss: 0.0216 | 0.0323
Epoch 276/300, Loss: 0.0216 | 0.0323
Epoch 277/300, Loss: 0.0216 | 0.0323
Epoch 278/300, Loss: 0.0216 | 0.0323
Epoch 279/300, Loss: 0.0216 | 0.0323
Epoch 280/300, Loss: 0.0216 | 0.0323
Epoch 281/300, Loss: 0.0216 | 0.0323
Epoch 282/300, Loss: 0.0216 | 0.0323
Epoch 283/300, Loss: 0.0216 | 0.0323
Epoch 284/300, Loss: 0.0216 | 0.0323
Epoch 285/300, Loss: 0.0216 | 0.0323
Epoch 286/300, Loss: 0.0216 | 0.0323
Epoch 287/300, Loss: 0.0216 | 0.0323
Epoch 288/300, Loss: 0.0216 | 0.0323
Epoch 289/300, Loss: 0.0216 | 0.0323
Epoch 290/300, Loss: 0.0216 | 0.0323
Epoch 291/300, Loss: 0.0216 | 0.0323
Epoch 292/300, Loss: 0.0216 | 0.0323
Epoch 293/300, Loss: 0.0216 | 0.0323
Epoch 294/300, Loss: 0.0216 | 0.0323
Epoch 295/300, Loss: 0.0216 | 0.0323
Epoch 296/300, Loss: 0.0216 | 0.0323
Epoch 297/300, Loss: 0.0216 | 0.0323
Epoch 298/300, Loss: 0.0216 | 0.0323
Epoch 299/300, Loss: 0.0216 | 0.0323
Epoch 300/300, Loss: 0.0216 | 0.0323
Runtime (seconds): 566.8746728897095
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 3.219909780251328
RMSE: 1.7944107055664062
MAE: 1.7944107055664062
R-squared: nan
[107.66441]
/data/student/k2110261/Multi-iTransformer/nomstl.py:515: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.
  plt.figure(figsize=(10, 6))
