[32m[I 2025-01-04 20:27:50,741][0m A new study created in memory with name: no-name-39d64de3-7e21-4c3d-a6d5-e41c00dc9012[0m
Early stopping at epoch 71
[32m[I 2025-01-04 20:28:19,528][0m Trial 0 finished with value: 3.1032958030700684 and parameters: {'observation_period_num': 243, 'train_rates': 0.9885419208718564, 'learning_rate': 3.0602139873345284e-06, 'batch_size': 231, 'step_size': 1, 'gamma': 0.8610010403093463}. Best is trial 0 with value: 3.1032958030700684.[0m
[32m[I 2025-01-04 20:28:57,810][0m Trial 1 finished with value: 0.226902454084074 and parameters: {'observation_period_num': 189, 'train_rates': 0.871714077038674, 'learning_rate': 4.903629769200769e-05, 'batch_size': 214, 'step_size': 2, 'gamma': 0.9514279901455864}. Best is trial 1 with value: 0.226902454084074.[0m
[32m[I 2025-01-04 20:29:39,946][0m Trial 2 finished with value: 0.6672451825360828 and parameters: {'observation_period_num': 239, 'train_rates': 0.7433100747731145, 'learning_rate': 1.3433838816725094e-06, 'batch_size': 100, 'step_size': 11, 'gamma': 0.8826598227838882}. Best is trial 1 with value: 0.226902454084074.[0m
[32m[I 2025-01-04 20:30:15,943][0m Trial 3 finished with value: 0.07243394615705327 and parameters: {'observation_period_num': 57, 'train_rates': 0.806215158987948, 'learning_rate': 7.442797301163923e-05, 'batch_size': 240, 'step_size': 15, 'gamma': 0.7759538386742756}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:31:04,034][0m Trial 4 finished with value: 0.3574569338198864 and parameters: {'observation_period_num': 236, 'train_rates': 0.9271497844462837, 'learning_rate': 1.0541582578343088e-05, 'batch_size': 101, 'step_size': 10, 'gamma': 0.7992773340422472}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:31:39,301][0m Trial 5 finished with value: 0.29644717110527885 and parameters: {'observation_period_num': 6, 'train_rates': 0.8778619792761915, 'learning_rate': 1.413500466304099e-06, 'batch_size': 238, 'step_size': 14, 'gamma': 0.9751883835355686}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:32:31,951][0m Trial 6 finished with value: 0.16908555396162628 and parameters: {'observation_period_num': 180, 'train_rates': 0.7535695973946578, 'learning_rate': 0.00041437786626591545, 'batch_size': 75, 'step_size': 13, 'gamma': 0.9714104055131265}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:33:10,241][0m Trial 7 finished with value: 0.2576459867925179 and parameters: {'observation_period_num': 155, 'train_rates': 0.7657892638301139, 'learning_rate': 2.2475771160646568e-05, 'batch_size': 200, 'step_size': 14, 'gamma': 0.807262321256582}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:34:01,185][0m Trial 8 finished with value: 0.18757924437522888 and parameters: {'observation_period_num': 252, 'train_rates': 0.9535929749611718, 'learning_rate': 0.0003622155475420094, 'batch_size': 174, 'step_size': 15, 'gamma': 0.8155656273988433}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:34:58,598][0m Trial 9 finished with value: 0.073851577937603 and parameters: {'observation_period_num': 19, 'train_rates': 0.9544939882620622, 'learning_rate': 9.739570222807086e-05, 'batch_size': 161, 'step_size': 4, 'gamma': 0.9086255638609655}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:36:48,781][0m Trial 10 finished with value: 0.10750739955605723 and parameters: {'observation_period_num': 76, 'train_rates': 0.6225736591863402, 'learning_rate': 0.0009778024634951722, 'batch_size': 33, 'step_size': 7, 'gamma': 0.7548590225794816}. Best is trial 3 with value: 0.07243394615705327.[0m
[32m[I 2025-01-04 20:37:25,189][0m Trial 11 finished with value: 0.06014535638446711 and parameters: {'observation_period_num': 33, 'train_rates': 0.6644865654565799, 'learning_rate': 9.155253365634994e-05, 'batch_size': 156, 'step_size': 5, 'gamma': 0.9110571125691882}. Best is trial 11 with value: 0.06014535638446711.[0m
[32m[I 2025-01-04 20:37:58,929][0m Trial 12 finished with value: 0.10186788603617501 and parameters: {'observation_period_num': 70, 'train_rates': 0.6627887521975871, 'learning_rate': 0.00010699785298659022, 'batch_size': 256, 'step_size': 6, 'gamma': 0.9201667611270139}. Best is trial 11 with value: 0.06014535638446711.[0m
[32m[I 2025-01-04 20:38:36,396][0m Trial 13 finished with value: 0.068903197816729 and parameters: {'observation_period_num': 64, 'train_rates': 0.6943268288200802, 'learning_rate': 0.00014006024586621414, 'batch_size': 150, 'step_size': 9, 'gamma': 0.8509789527799312}. Best is trial 11 with value: 0.06014535638446711.[0m
[32m[I 2025-01-04 20:39:20,272][0m Trial 14 finished with value: 0.09058920141628811 and parameters: {'observation_period_num': 112, 'train_rates': 0.692052582849108, 'learning_rate': 0.0002132955344884047, 'batch_size': 136, 'step_size': 9, 'gamma': 0.851570550087222}. Best is trial 11 with value: 0.06014535638446711.[0m
[32m[I 2025-01-04 20:40:10,415][0m Trial 15 finished with value: 0.29533581806259007 and parameters: {'observation_period_num': 44, 'train_rates': 0.6061383981738612, 'learning_rate': 1.8556662335355625e-05, 'batch_size': 143, 'step_size': 5, 'gamma': 0.8959667502072638}. Best is trial 11 with value: 0.06014535638446711.[0m
[32m[I 2025-01-04 20:40:47,082][0m Trial 16 finished with value: 0.5268426902865598 and parameters: {'observation_period_num': 103, 'train_rates': 0.6942688902024111, 'learning_rate': 8.663617890870568e-06, 'batch_size': 185, 'step_size': 8, 'gamma': 0.8396420970985594}. Best is trial 11 with value: 0.06014535638446711.[0m
[32m[I 2025-01-04 20:41:25,341][0m Trial 17 finished with value: 0.04498671812140953 and parameters: {'observation_period_num': 33, 'train_rates': 0.645114257119519, 'learning_rate': 0.00018983982492511194, 'batch_size': 127, 'step_size': 4, 'gamma': 0.9377209688018934}. Best is trial 17 with value: 0.04498671812140953.[0m
[32m[I 2025-01-04 20:42:04,037][0m Trial 18 finished with value: 0.07765286595732003 and parameters: {'observation_period_num': 101, 'train_rates': 0.6400773451508502, 'learning_rate': 0.0008986239340267524, 'batch_size': 114, 'step_size': 3, 'gamma': 0.9351157151339957}. Best is trial 17 with value: 0.04498671812140953.[0m
[32m[I 2025-01-04 20:43:10,742][0m Trial 19 finished with value: 0.04011022861426075 and parameters: {'observation_period_num': 32, 'train_rates': 0.815164481259804, 'learning_rate': 4.4996676522618736e-05, 'batch_size': 63, 'step_size': 5, 'gamma': 0.9461681202602746}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:46:24,538][0m Trial 20 finished with value: 0.1086246106040647 and parameters: {'observation_period_num': 139, 'train_rates': 0.8248073253319488, 'learning_rate': 3.603412880475081e-05, 'batch_size': 20, 'step_size': 3, 'gamma': 0.9888928781582302}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:47:27,267][0m Trial 21 finished with value: 0.06296925381853041 and parameters: {'observation_period_num': 25, 'train_rates': 0.7178007451403156, 'learning_rate': 0.0002255729775165828, 'batch_size': 60, 'step_size': 5, 'gamma': 0.9426837689436713}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:48:29,920][0m Trial 22 finished with value: 0.043889632228738776 and parameters: {'observation_period_num': 40, 'train_rates': 0.8342462243540054, 'learning_rate': 5.1038189816451634e-05, 'batch_size': 67, 'step_size': 6, 'gamma': 0.9201569019852218}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:49:39,874][0m Trial 23 finished with value: 0.0918550538791049 and parameters: {'observation_period_num': 85, 'train_rates': 0.8434946637339468, 'learning_rate': 5.0884965055408724e-05, 'batch_size': 58, 'step_size': 7, 'gamma': 0.958290035604527}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:50:34,745][0m Trial 24 finished with value: 0.11318693285269595 and parameters: {'observation_period_num': 45, 'train_rates': 0.793015531464996, 'learning_rate': 1.255906864316091e-05, 'batch_size': 79, 'step_size': 4, 'gamma': 0.9285778576441788}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:51:19,967][0m Trial 25 finished with value: 0.28858243627121827 and parameters: {'observation_period_num': 45, 'train_rates': 0.8636697695783718, 'learning_rate': 5.215874838793513e-06, 'batch_size': 119, 'step_size': 6, 'gamma': 0.8777249193707346}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:53:04,002][0m Trial 26 finished with value: 0.1244828427831332 and parameters: {'observation_period_num': 19, 'train_rates': 0.9081349244256793, 'learning_rate': 2.8845532536569695e-05, 'batch_size': 45, 'step_size': 1, 'gamma': 0.8962330309659414}. Best is trial 19 with value: 0.04011022861426075.[0m
[32m[I 2025-01-04 20:54:10,018][0m Trial 27 finished with value: 0.02791957850201746 and parameters: {'observation_period_num': 6, 'train_rates': 0.7887040067408599, 'learning_rate': 0.00018508127130834742, 'batch_size': 84, 'step_size': 7, 'gamma': 0.9615883794150175}. Best is trial 27 with value: 0.02791957850201746.[0m
[32m[I 2025-01-04 20:55:30,998][0m Trial 28 finished with value: 0.032549265851553585 and parameters: {'observation_period_num': 6, 'train_rates': 0.7853345115937777, 'learning_rate': 5.645964062404577e-05, 'batch_size': 78, 'step_size': 7, 'gamma': 0.9628759722492385}. Best is trial 27 with value: 0.02791957850201746.[0m
[32m[I 2025-01-04 20:56:31,290][0m Trial 29 finished with value: 0.09886318883337315 and parameters: {'observation_period_num': 9, 'train_rates': 0.7841108500716162, 'learning_rate': 3.396700958425394e-06, 'batch_size': 92, 'step_size': 11, 'gamma': 0.9623888039807257}. Best is trial 27 with value: 0.02791957850201746.[0m
[32m[I 2025-01-04 20:58:16,903][0m Trial 30 finished with value: 0.02460580010946727 and parameters: {'observation_period_num': 8, 'train_rates': 0.8102427969463627, 'learning_rate': 0.00036214570446825714, 'batch_size': 37, 'step_size': 8, 'gamma': 0.9894768878191984}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 20:59:55,474][0m Trial 31 finished with value: 0.029275260272565006 and parameters: {'observation_period_num': 7, 'train_rates': 0.814035724629008, 'learning_rate': 0.0005013669359563413, 'batch_size': 47, 'step_size': 8, 'gamma': 0.9815704159551305}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:03:30,707][0m Trial 32 finished with value: 0.03398104841892536 and parameters: {'observation_period_num': 10, 'train_rates': 0.7792620417936836, 'learning_rate': 0.00046511630038245606, 'batch_size': 18, 'step_size': 8, 'gamma': 0.9873066749993139}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:05:02,335][0m Trial 33 finished with value: 0.03928624924446922 and parameters: {'observation_period_num': 18, 'train_rates': 0.7385053615871794, 'learning_rate': 0.0006172394788636006, 'batch_size': 41, 'step_size': 9, 'gamma': 0.9723918047301456}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:06:02,089][0m Trial 34 finished with value: 0.02473226370715792 and parameters: {'observation_period_num': 6, 'train_rates': 0.8531592861248841, 'learning_rate': 0.00028448322242129523, 'batch_size': 88, 'step_size': 11, 'gamma': 0.9849913390592063}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:07:02,672][0m Trial 35 finished with value: 0.05655989554807818 and parameters: {'observation_period_num': 56, 'train_rates': 0.8877436448534078, 'learning_rate': 0.00029087291672754567, 'batch_size': 91, 'step_size': 11, 'gamma': 0.9883156082233305}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:08:36,488][0m Trial 36 finished with value: 0.24729860871313855 and parameters: {'observation_period_num': 222, 'train_rates': 0.8479781729397784, 'learning_rate': 0.000586939929535555, 'batch_size': 45, 'step_size': 12, 'gamma': 0.9747622211813148}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:09:26,800][0m Trial 37 finished with value: 0.04270633616039843 and parameters: {'observation_period_num': 54, 'train_rates': 0.7994192255897817, 'learning_rate': 0.00015161010073051341, 'batch_size': 106, 'step_size': 10, 'gamma': 0.9530154900103517}. Best is trial 30 with value: 0.02460580010946727.[0m
[32m[I 2025-01-04 21:11:31,915][0m Trial 38 finished with value: 0.02335463707116945 and parameters: {'observation_period_num': 5, 'train_rates': 0.8584706922303944, 'learning_rate': 0.0006651056875330585, 'batch_size': 34, 'step_size': 10, 'gamma': 0.9777750367963017}. Best is trial 38 with value: 0.02335463707116945.[0m
[32m[I 2025-01-04 21:13:54,748][0m Trial 39 finished with value: 0.13680753361039602 and parameters: {'observation_period_num': 212, 'train_rates': 0.8932936905546187, 'learning_rate': 0.0002722988609293579, 'batch_size': 28, 'step_size': 12, 'gamma': 0.9629243075028334}. Best is trial 38 with value: 0.02335463707116945.[0m
[32m[I 2025-01-04 21:14:49,124][0m Trial 40 finished with value: 0.10500101051680387 and parameters: {'observation_period_num': 87, 'train_rates': 0.861746239094853, 'learning_rate': 0.0007412154743722913, 'batch_size': 85, 'step_size': 10, 'gamma': 0.9733696516722664}. Best is trial 38 with value: 0.02335463707116945.[0m
[32m[I 2025-01-04 21:16:10,477][0m Trial 41 finished with value: 0.022279301923845196 and parameters: {'observation_period_num': 5, 'train_rates': 0.8265965239922661, 'learning_rate': 0.0004085539288143739, 'batch_size': 51, 'step_size': 9, 'gamma': 0.9810837136288791}. Best is trial 41 with value: 0.022279301923845196.[0m
[32m[I 2025-01-04 21:18:23,975][0m Trial 42 finished with value: 0.03871218977902813 and parameters: {'observation_period_num': 24, 'train_rates': 0.9109802475020982, 'learning_rate': 0.0003254948363734605, 'batch_size': 33, 'step_size': 12, 'gamma': 0.95430359153054}. Best is trial 41 with value: 0.022279301923845196.[0m
[32m[I 2025-01-04 21:19:37,501][0m Trial 43 finished with value: 0.03804966691679764 and parameters: {'observation_period_num': 20, 'train_rates': 0.7607896934439098, 'learning_rate': 0.000423243801942716, 'batch_size': 56, 'step_size': 11, 'gamma': 0.9778973249276088}. Best is trial 41 with value: 0.022279301923845196.[0m
[32m[I 2025-01-04 21:20:43,919][0m Trial 44 finished with value: 0.03765051974859925 and parameters: {'observation_period_num': 33, 'train_rates': 0.8553840028291566, 'learning_rate': 0.00016210283585560233, 'batch_size': 70, 'step_size': 9, 'gamma': 0.9652589426902708}. Best is trial 41 with value: 0.022279301923845196.[0m
[32m[I 2025-01-04 21:21:34,697][0m Trial 45 finished with value: 0.14934442178432986 and parameters: {'observation_period_num': 172, 'train_rates': 0.8264953400054563, 'learning_rate': 0.00067219249378535, 'batch_size': 102, 'step_size': 10, 'gamma': 0.9889151710417994}. Best is trial 41 with value: 0.022279301923845196.[0m
[32m[I 2025-01-04 21:23:43,236][0m Trial 46 finished with value: 0.02064801537408558 and parameters: {'observation_period_num': 6, 'train_rates': 0.8777995460303056, 'learning_rate': 0.00011560379182060972, 'batch_size': 33, 'step_size': 13, 'gamma': 0.9491492754205678}. Best is trial 46 with value: 0.02064801537408558.[0m
[32m[I 2025-01-04 21:26:32,291][0m Trial 47 finished with value: 0.03568787626116662 and parameters: {'observation_period_num': 20, 'train_rates': 0.9369653124239765, 'learning_rate': 0.00012514916063432576, 'batch_size': 27, 'step_size': 14, 'gamma': 0.9800817694227496}. Best is trial 46 with value: 0.02064801537408558.[0m
[32m[I 2025-01-04 21:28:36,094][0m Trial 48 finished with value: 0.08340021270051809 and parameters: {'observation_period_num': 56, 'train_rates': 0.8776204618596553, 'learning_rate': 0.00031715909879701556, 'batch_size': 37, 'step_size': 13, 'gamma': 0.9476292701188357}. Best is trial 46 with value: 0.02064801537408558.[0m
[32m[I 2025-01-04 21:30:28,943][0m Trial 49 finished with value: 0.06046355606579199 and parameters: {'observation_period_num': 29, 'train_rates': 0.9581757104455617, 'learning_rate': 0.000996871911075528, 'batch_size': 51, 'step_size': 13, 'gamma': 0.9276637954605877}. Best is trial 46 with value: 0.02064801537408558.[0m
最適ハイパーパラメータが見つかりました
最適なハイパーパラメータが best_hyperparameters_AAPL_iTransformer_noMSTL.json に保存されました
Epoch 1/300, Loss: 0.2405 | 0.1850
Epoch 2/300, Loss: 0.1307 | 0.1031
Epoch 3/300, Loss: 0.1151 | 0.0801
Epoch 4/300, Loss: 0.1062 | 0.0711
Epoch 5/300, Loss: 0.1000 | 0.0632
Epoch 6/300, Loss: 0.0958 | 0.0577
Epoch 7/300, Loss: 0.0930 | 0.0539
Epoch 8/300, Loss: 0.0906 | 0.0512
Epoch 9/300, Loss: 0.0885 | 0.0482
Epoch 10/300, Loss: 0.0862 | 0.0455
Epoch 11/300, Loss: 0.0839 | 0.0433
Epoch 12/300, Loss: 0.0818 | 0.0414
Epoch 13/300, Loss: 0.0797 | 0.0394
Epoch 14/300, Loss: 0.0779 | 0.0379
Epoch 15/300, Loss: 0.0765 | 0.0367
Epoch 16/300, Loss: 0.0751 | 0.0359
Epoch 17/300, Loss: 0.0739 | 0.0354
Epoch 18/300, Loss: 0.0729 | 0.0351
Epoch 19/300, Loss: 0.0721 | 0.0348
Epoch 20/300, Loss: 0.0713 | 0.0342
Epoch 21/300, Loss: 0.0703 | 0.0336
Epoch 22/300, Loss: 0.0695 | 0.0332
Epoch 23/300, Loss: 0.0688 | 0.0326
Epoch 24/300, Loss: 0.0681 | 0.0318
Epoch 25/300, Loss: 0.0674 | 0.0308
Epoch 26/300, Loss: 0.0667 | 0.0299
Epoch 27/300, Loss: 0.0660 | 0.0290
Epoch 28/300, Loss: 0.0654 | 0.0285
Epoch 29/300, Loss: 0.0648 | 0.0280
Epoch 30/300, Loss: 0.0643 | 0.0276
Epoch 31/300, Loss: 0.0639 | 0.0273
Epoch 32/300, Loss: 0.0634 | 0.0269
Epoch 33/300, Loss: 0.0629 | 0.0268
Epoch 34/300, Loss: 0.0623 | 0.0263
Epoch 35/300, Loss: 0.0619 | 0.0260
Epoch 36/300, Loss: 0.0617 | 0.0258
Epoch 37/300, Loss: 0.0614 | 0.0258
Epoch 38/300, Loss: 0.0613 | 0.0254
Epoch 39/300, Loss: 0.0608 | 0.0253
Epoch 40/300, Loss: 0.0605 | 0.0249
Epoch 41/300, Loss: 0.0600 | 0.0245
Epoch 42/300, Loss: 0.0596 | 0.0244
Epoch 43/300, Loss: 0.0593 | 0.0244
Epoch 44/300, Loss: 0.0590 | 0.0244
Epoch 45/300, Loss: 0.0587 | 0.0244
Epoch 46/300, Loss: 0.0584 | 0.0244
Epoch 47/300, Loss: 0.0580 | 0.0238
Epoch 48/300, Loss: 0.0576 | 0.0237
Epoch 49/300, Loss: 0.0574 | 0.0236
Epoch 50/300, Loss: 0.0571 | 0.0234
Epoch 51/300, Loss: 0.0569 | 0.0232
Epoch 52/300, Loss: 0.0566 | 0.0229
Epoch 53/300, Loss: 0.0565 | 0.0227
Epoch 54/300, Loss: 0.0561 | 0.0223
Epoch 55/300, Loss: 0.0559 | 0.0222
Epoch 56/300, Loss: 0.0557 | 0.0222
Epoch 57/300, Loss: 0.0554 | 0.0223
Epoch 58/300, Loss: 0.0552 | 0.0224
Epoch 59/300, Loss: 0.0550 | 0.0226
Epoch 60/300, Loss: 0.0548 | 0.0226
Epoch 61/300, Loss: 0.0549 | 0.0220
Epoch 62/300, Loss: 0.0551 | 0.0217
Epoch 63/300, Loss: 0.0554 | 0.0215
Epoch 64/300, Loss: 0.0551 | 0.0215
Epoch 65/300, Loss: 0.0560 | 0.0215
Epoch 66/300, Loss: 0.0562 | 0.0216
Epoch 67/300, Loss: 0.0552 | 0.0214
Epoch 68/300, Loss: 0.0546 | 0.0214
Epoch 69/300, Loss: 0.0543 | 0.0215
Epoch 70/300, Loss: 0.0542 | 0.0215
Epoch 71/300, Loss: 0.0539 | 0.0215
Epoch 72/300, Loss: 0.0535 | 0.0214
Epoch 73/300, Loss: 0.0532 | 0.0213
Epoch 74/300, Loss: 0.0530 | 0.0212
Epoch 75/300, Loss: 0.0526 | 0.0211
Epoch 76/300, Loss: 0.0524 | 0.0211
Epoch 77/300, Loss: 0.0521 | 0.0211
Epoch 78/300, Loss: 0.0519 | 0.0212
Epoch 79/300, Loss: 0.0517 | 0.0213
Epoch 80/300, Loss: 0.0516 | 0.0214
Epoch 81/300, Loss: 0.0515 | 0.0214
Epoch 82/300, Loss: 0.0512 | 0.0215
Epoch 83/300, Loss: 0.0510 | 0.0216
Epoch 84/300, Loss: 0.0508 | 0.0217
Epoch 85/300, Loss: 0.0508 | 0.0218
Epoch 86/300, Loss: 0.0506 | 0.0218
Epoch 87/300, Loss: 0.0504 | 0.0222
Epoch 88/300, Loss: 0.0503 | 0.0225
Epoch 89/300, Loss: 0.0503 | 0.0224
Epoch 90/300, Loss: 0.0501 | 0.0224
Epoch 91/300, Loss: 0.0500 | 0.0227
Epoch 92/300, Loss: 0.0498 | 0.0224
Epoch 93/300, Loss: 0.0496 | 0.0225
Epoch 94/300, Loss: 0.0495 | 0.0227
Epoch 95/300, Loss: 0.0494 | 0.0227
Epoch 96/300, Loss: 0.0493 | 0.0229
Epoch 97/300, Loss: 0.0491 | 0.0229
Epoch 98/300, Loss: 0.0490 | 0.0229
Epoch 99/300, Loss: 0.0489 | 0.0228
Epoch 100/300, Loss: 0.0488 | 0.0229
Epoch 101/300, Loss: 0.0489 | 0.0231
Epoch 102/300, Loss: 0.0489 | 0.0233
Epoch 103/300, Loss: 0.0488 | 0.0234
Epoch 104/300, Loss: 0.0488 | 0.0235
Epoch 105/300, Loss: 0.0487 | 0.0239
Epoch 106/300, Loss: 0.0488 | 0.0239
Epoch 107/300, Loss: 0.0488 | 0.0241
Epoch 108/300, Loss: 0.0489 | 0.0241
Epoch 109/300, Loss: 0.0487 | 0.0237
Epoch 110/300, Loss: 0.0486 | 0.0238
Epoch 111/300, Loss: 0.0485 | 0.0235
Epoch 112/300, Loss: 0.0488 | 0.0239
Epoch 113/300, Loss: 0.0489 | 0.0237
Epoch 114/300, Loss: 0.0492 | 0.0235
Epoch 115/300, Loss: 0.0489 | 0.0232
Epoch 116/300, Loss: 0.0497 | 0.0267
Epoch 117/300, Loss: 0.0520 | 0.0233
Epoch 118/300, Loss: 0.0488 | 0.0233
Epoch 119/300, Loss: 0.0484 | 0.0236
Epoch 120/300, Loss: 0.0481 | 0.0230
Epoch 121/300, Loss: 0.0478 | 0.0231
Epoch 122/300, Loss: 0.0477 | 0.0232
Epoch 123/300, Loss: 0.0476 | 0.0232
Epoch 124/300, Loss: 0.0475 | 0.0230
Epoch 125/300, Loss: 0.0475 | 0.0227
Epoch 126/300, Loss: 0.0475 | 0.0226
Epoch 127/300, Loss: 0.0475 | 0.0225
Epoch 128/300, Loss: 0.0475 | 0.0224
Epoch 129/300, Loss: 0.0475 | 0.0223
Epoch 130/300, Loss: 0.0475 | 0.0223
Epoch 131/300, Loss: 0.0476 | 0.0222
Epoch 132/300, Loss: 0.0476 | 0.0225
Epoch 133/300, Loss: 0.0476 | 0.0227
Epoch 134/300, Loss: 0.0476 | 0.0228
Epoch 135/300, Loss: 0.0476 | 0.0228
Epoch 136/300, Loss: 0.0476 | 0.0227
Epoch 137/300, Loss: 0.0474 | 0.0227
Epoch 138/300, Loss: 0.0472 | 0.0226
Epoch 139/300, Loss: 0.0470 | 0.0228
Epoch 140/300, Loss: 0.0468 | 0.0229
Epoch 141/300, Loss: 0.0467 | 0.0228
Epoch 142/300, Loss: 0.0464 | 0.0227
Epoch 143/300, Loss: 0.0463 | 0.0228
Epoch 144/300, Loss: 0.0459 | 0.0228
Epoch 145/300, Loss: 0.0456 | 0.0229
Epoch 146/300, Loss: 0.0451 | 0.0229
Epoch 147/300, Loss: 0.0444 | 0.0230
Epoch 148/300, Loss: 0.0450 | 0.0237
Epoch 149/300, Loss: 0.0459 | 0.0239
Epoch 150/300, Loss: 0.0443 | 0.0232
Epoch 151/300, Loss: 0.0489 | 0.0258
Epoch 152/300, Loss: 0.0496 | 0.0229
Epoch 153/300, Loss: 0.0468 | 0.0224
Epoch 154/300, Loss: 0.0438 | 0.0230
Epoch 155/300, Loss: 0.0430 | 0.0226
Epoch 156/300, Loss: 0.0423 | 0.0224
Epoch 157/300, Loss: 0.0419 | 0.0227
Epoch 158/300, Loss: 0.0416 | 0.0225
Epoch 159/300, Loss: 0.0412 | 0.0225
Epoch 160/300, Loss: 0.0408 | 0.0225
Epoch 161/300, Loss: 0.0404 | 0.0226
Epoch 162/300, Loss: 0.0413 | 0.0228
Epoch 163/300, Loss: 0.0429 | 0.0229
Epoch 164/300, Loss: 0.0445 | 0.0238
Epoch 165/300, Loss: 0.0428 | 0.0232
Epoch 166/300, Loss: 0.0410 | 0.0231
Epoch 167/300, Loss: 0.0409 | 0.0227
Epoch 168/300, Loss: 0.0404 | 0.0223
Epoch 169/300, Loss: 0.0399 | 0.0234
Epoch 170/300, Loss: 0.0392 | 0.0223
Epoch 171/300, Loss: 0.0452 | 0.0234
Epoch 172/300, Loss: 0.0462 | 0.0229
Epoch 173/300, Loss: 0.0451 | 0.0224
Epoch 174/300, Loss: 0.0413 | 0.0220
Epoch 175/300, Loss: 0.0406 | 0.0219
Epoch 176/300, Loss: 0.0401 | 0.0217
Epoch 177/300, Loss: 0.0398 | 0.0219
Epoch 178/300, Loss: 0.0396 | 0.0220
Epoch 179/300, Loss: 0.0394 | 0.0220
Epoch 180/300, Loss: 0.0392 | 0.0220
Epoch 181/300, Loss: 0.0389 | 0.0221
Epoch 182/300, Loss: 0.0387 | 0.0219
Epoch 183/300, Loss: 0.0384 | 0.0234
Epoch 184/300, Loss: 0.0387 | 0.0219
Epoch 185/300, Loss: 0.0400 | 0.0225
Epoch 186/300, Loss: 0.0408 | 0.0225
Epoch 187/300, Loss: 0.0392 | 0.0220
Epoch 188/300, Loss: 0.0386 | 0.0219
Epoch 189/300, Loss: 0.0380 | 0.0216
Epoch 190/300, Loss: 0.0375 | 0.0216
Epoch 191/300, Loss: 0.0369 | 0.0216
Epoch 192/300, Loss: 0.0367 | 0.0215
Epoch 193/300, Loss: 0.0367 | 0.0218
Epoch 194/300, Loss: 0.0383 | 0.0227
Epoch 195/300, Loss: 0.0404 | 0.0220
Epoch 196/300, Loss: 0.0380 | 0.0230
Epoch 197/300, Loss: 0.0377 | 0.0221
Epoch 198/300, Loss: 0.0373 | 0.0222
Epoch 199/300, Loss: 0.0371 | 0.0217
Epoch 200/300, Loss: 0.0389 | 0.0225
Epoch 201/300, Loss: 0.0388 | 0.0218
Epoch 202/300, Loss: 0.0370 | 0.0219
Epoch 203/300, Loss: 0.0363 | 0.0219
Epoch 204/300, Loss: 0.0360 | 0.0220
Epoch 205/300, Loss: 0.0359 | 0.0222
Epoch 206/300, Loss: 0.0358 | 0.0222
Epoch 207/300, Loss: 0.0357 | 0.0220
Epoch 208/300, Loss: 0.0362 | 0.0222
Epoch 209/300, Loss: 0.0381 | 0.0228
Epoch 210/300, Loss: 0.0376 | 0.0226
Epoch 211/300, Loss: 0.0371 | 0.0221
Epoch 212/300, Loss: 0.0362 | 0.0220
Epoch 213/300, Loss: 0.0355 | 0.0221
Epoch 214/300, Loss: 0.0367 | 0.0237
Epoch 215/300, Loss: 0.0378 | 0.0229
Epoch 216/300, Loss: 0.0358 | 0.0219
Epoch 217/300, Loss: 0.0352 | 0.0221
Epoch 218/300, Loss: 0.0355 | 0.0228
Epoch 219/300, Loss: 0.0397 | 0.0223
Epoch 220/300, Loss: 0.0362 | 0.0229
Epoch 221/300, Loss: 0.0358 | 0.0221
Epoch 222/300, Loss: 0.0358 | 0.0223
Epoch 223/300, Loss: 0.0351 | 0.0224
Epoch 224/300, Loss: 0.0349 | 0.0221
Epoch 225/300, Loss: 0.0347 | 0.0222
Epoch 226/300, Loss: 0.0345 | 0.0222
Epoch 227/300, Loss: 0.0346 | 0.0225
Epoch 228/300, Loss: 0.0373 | 0.0223
Epoch 229/300, Loss: 0.0351 | 0.0224
Epoch 230/300, Loss: 0.0346 | 0.0221
Epoch 231/300, Loss: 0.0345 | 0.0222
Epoch 232/300, Loss: 0.0343 | 0.0221
Epoch 233/300, Loss: 0.0343 | 0.0221
Epoch 234/300, Loss: 0.0341 | 0.0221
Epoch 235/300, Loss: 0.0371 | 0.0227
Epoch 236/300, Loss: 0.0369 | 0.0228
Epoch 237/300, Loss: 0.0354 | 0.0227
Epoch 238/300, Loss: 0.0370 | 0.0228
Epoch 239/300, Loss: 0.0350 | 0.0231
Epoch 240/300, Loss: 0.0347 | 0.0225
Epoch 241/300, Loss: 0.0342 | 0.0224
Epoch 242/300, Loss: 0.0340 | 0.0223
Epoch 243/300, Loss: 0.0339 | 0.0222
Epoch 244/300, Loss: 0.0336 | 0.0222
Epoch 245/300, Loss: 0.0335 | 0.0222
Epoch 246/300, Loss: 0.0334 | 0.0221
Epoch 247/300, Loss: 0.0334 | 0.0224
Epoch 248/300, Loss: 0.0338 | 0.0222
Epoch 249/300, Loss: 0.0335 | 0.0221
Epoch 250/300, Loss: 0.0355 | 0.0228
Epoch 251/300, Loss: 0.0348 | 0.0226
Epoch 252/300, Loss: 0.0367 | 0.0227
Epoch 253/300, Loss: 0.0343 | 0.0223
Epoch 254/300, Loss: 0.0340 | 0.0230
Epoch 255/300, Loss: 0.0366 | 0.0227
Epoch 256/300, Loss: 0.0342 | 0.0226
Epoch 257/300, Loss: 0.0337 | 0.0222
Epoch 258/300, Loss: 0.0335 | 0.0223
Epoch 259/300, Loss: 0.0334 | 0.0223
Epoch 260/300, Loss: 0.0333 | 0.0222
Epoch 261/300, Loss: 0.0333 | 0.0222
Epoch 262/300, Loss: 0.0331 | 0.0221
Epoch 263/300, Loss: 0.0335 | 0.0222
Epoch 264/300, Loss: 0.0328 | 0.0224
Epoch 265/300, Loss: 0.0330 | 0.0221
Epoch 266/300, Loss: 0.0349 | 0.0228
Epoch 267/300, Loss: 0.0373 | 0.0227
Epoch 268/300, Loss: 0.0351 | 0.0229
Epoch 269/300, Loss: 0.0345 | 0.0225
Epoch 270/300, Loss: 0.0332 | 0.0224
Epoch 271/300, Loss: 0.0331 | 0.0224
Epoch 272/300, Loss: 0.0330 | 0.0223
Epoch 273/300, Loss: 0.0329 | 0.0224
Epoch 274/300, Loss: 0.0328 | 0.0225
Epoch 275/300, Loss: 0.0330 | 0.0225
Epoch 276/300, Loss: 0.0329 | 0.0224
Epoch 277/300, Loss: 0.0328 | 0.0224
Epoch 278/300, Loss: 0.0325 | 0.0223
Epoch 279/300, Loss: 0.0324 | 0.0224
Epoch 280/300, Loss: 0.0325 | 0.0224
Epoch 281/300, Loss: 0.0327 | 0.0225
Epoch 282/300, Loss: 0.0356 | 0.0228
Epoch 283/300, Loss: 0.0329 | 0.0224
Epoch 284/300, Loss: 0.0335 | 0.0228
Epoch 285/300, Loss: 0.0354 | 0.0231
Epoch 286/300, Loss: 0.0334 | 0.0228
Epoch 287/300, Loss: 0.0328 | 0.0225
Epoch 288/300, Loss: 0.0326 | 0.0226
Epoch 289/300, Loss: 0.0325 | 0.0225
Epoch 290/300, Loss: 0.0325 | 0.0225
Epoch 291/300, Loss: 0.0323 | 0.0225
Epoch 292/300, Loss: 0.0320 | 0.0225
Epoch 293/300, Loss: 0.0319 | 0.0225
Epoch 294/300, Loss: 0.0318 | 0.0226
Epoch 295/300, Loss: 0.0321 | 0.0227
Epoch 296/300, Loss: 0.0328 | 0.0226
Epoch 297/300, Loss: 0.0324 | 0.0226
Epoch 298/300, Loss: 0.0318 | 0.0226
Epoch 299/300, Loss: 0.0317 | 0.0228
Epoch 300/300, Loss: 0.0320 | 0.0228
Runtime (seconds): 398.92144680023193
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 60.50653971824795
RMSE: 7.778594970703125
MAE: 7.778594970703125
R-squared: nan
[227.1514]
