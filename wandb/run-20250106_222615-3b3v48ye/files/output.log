[32m[I 2025-01-06 22:26:20,012][0m A new study created in memory with name: no-name-d2b2d103-d9d8-4d62-9deb-98e631703d24[0m
[32m[I 2025-01-06 22:29:53,656][0m Trial 0 finished with value: 0.25278233152560214 and parameters: {'observation_period_num': 149, 'train_rates': 0.9375397627909534, 'learning_rate': 2.0572817961069078e-05, 'batch_size': 81, 'step_size': 6, 'gamma': 0.9351207975375015}. Best is trial 0 with value: 0.25278233152560214.[0m
[32m[I 2025-01-06 22:31:02,714][0m Trial 1 finished with value: 0.5740705153605717 and parameters: {'observation_period_num': 45, 'train_rates': 0.7362594731769347, 'learning_rate': 5.907632903067214e-05, 'batch_size': 49, 'step_size': 2, 'gamma': 0.7762731383729231}. Best is trial 0 with value: 0.25278233152560214.[0m
[32m[I 2025-01-06 22:35:25,731][0m Trial 2 finished with value: 2.1639451169802464 and parameters: {'observation_period_num': 215, 'train_rates': 0.6846938964718394, 'learning_rate': 1.3984821847546064e-06, 'batch_size': 251, 'step_size': 1, 'gamma': 0.9600430229694353}. Best is trial 0 with value: 0.25278233152560214.[0m
[32m[I 2025-01-06 22:40:02,134][0m Trial 3 finished with value: 0.707176449744567 and parameters: {'observation_period_num': 213, 'train_rates': 0.7548180305087686, 'learning_rate': 1.6970924675609616e-05, 'batch_size': 128, 'step_size': 8, 'gamma': 0.9206139603778474}. Best is trial 0 with value: 0.25278233152560214.[0m
[32m[I 2025-01-06 22:40:53,095][0m Trial 4 finished with value: 0.1629013933918693 and parameters: {'observation_period_num': 13, 'train_rates': 0.9069942779774409, 'learning_rate': 2.891461769881091e-05, 'batch_size': 75, 'step_size': 12, 'gamma': 0.8520635081499944}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 22:41:44,682][0m Trial 5 finished with value: 0.4166731655330401 and parameters: {'observation_period_num': 41, 'train_rates': 0.8474125706658255, 'learning_rate': 0.00015209116408426834, 'batch_size': 192, 'step_size': 3, 'gamma': 0.7566269784287516}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 22:45:06,096][0m Trial 6 finished with value: 0.9982278423565802 and parameters: {'observation_period_num': 171, 'train_rates': 0.6734774275124913, 'learning_rate': 9.880727767010342e-06, 'batch_size': 75, 'step_size': 3, 'gamma': 0.9005583324448521}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 22:49:58,123][0m Trial 7 finished with value: 0.5013879330657975 and parameters: {'observation_period_num': 198, 'train_rates': 0.9173309396663168, 'learning_rate': 5.717880986079353e-06, 'batch_size': 173, 'step_size': 15, 'gamma': 0.9231514021496022}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 22:51:24,900][0m Trial 8 finished with value: 0.33712923370782943 and parameters: {'observation_period_num': 66, 'train_rates': 0.845157650681173, 'learning_rate': 0.0004986493171854548, 'batch_size': 113, 'step_size': 5, 'gamma': 0.8187875358915473}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 22:53:13,793][0m Trial 9 finished with value: 0.5773347661327133 and parameters: {'observation_period_num': 96, 'train_rates': 0.7326727741515693, 'learning_rate': 6.534106384233003e-05, 'batch_size': 153, 'step_size': 9, 'gamma': 0.9634047451150258}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 22:57:26,485][0m Trial 10 finished with value: 0.21753018763330248 and parameters: {'observation_period_num': 5, 'train_rates': 0.9878871962225834, 'learning_rate': 2.4179686127398693e-06, 'batch_size': 16, 'step_size': 13, 'gamma': 0.8534413006545958}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:00:59,645][0m Trial 11 finished with value: 0.3870461732149124 and parameters: {'observation_period_num': 6, 'train_rates': 0.9858526926424968, 'learning_rate': 1.0821211582293808e-06, 'batch_size': 19, 'step_size': 13, 'gamma': 0.8430638741771758}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:04:28,107][0m Trial 12 finished with value: 0.19001105800271034 and parameters: {'observation_period_num': 7, 'train_rates': 0.9893321199404207, 'learning_rate': 3.3150473156570666e-06, 'batch_size': 20, 'step_size': 12, 'gamma': 0.8742202866286445}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:06:53,647][0m Trial 13 finished with value: 0.3650967737114475 and parameters: {'observation_period_num': 106, 'train_rates': 0.9079494787293947, 'learning_rate': 4.503160415641625e-06, 'batch_size': 58, 'step_size': 10, 'gamma': 0.8819570979874632}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:12:57,255][0m Trial 14 finished with value: 0.27411271816061944 and parameters: {'observation_period_num': 250, 'train_rates': 0.8570343758360593, 'learning_rate': 0.0003292040764581614, 'batch_size': 103, 'step_size': 11, 'gamma': 0.8107223771702072}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:14:47,334][0m Trial 15 finished with value: 0.17449209541082383 and parameters: {'observation_period_num': 73, 'train_rates': 0.9446319388553313, 'learning_rate': 4.8654696838820234e-05, 'batch_size': 48, 'step_size': 12, 'gamma': 0.8684970618759882}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:16:06,467][0m Trial 16 finished with value: 0.80424370677831 and parameters: {'observation_period_num': 74, 'train_rates': 0.607679649596194, 'learning_rate': 5.5360358708274264e-05, 'batch_size': 89, 'step_size': 15, 'gamma': 0.8205673357830464}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:18:44,533][0m Trial 17 finished with value: 0.18849446721698926 and parameters: {'observation_period_num': 118, 'train_rates': 0.8869580435989689, 'learning_rate': 0.00013920165513301659, 'batch_size': 60, 'step_size': 7, 'gamma': 0.8447975729910812}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:20:04,915][0m Trial 18 finished with value: 0.36025281505584716 and parameters: {'observation_period_num': 40, 'train_rates': 0.7860024344992259, 'learning_rate': 3.239698060889543e-05, 'batch_size': 44, 'step_size': 13, 'gamma': 0.8941243355608419}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:21:52,047][0m Trial 19 finished with value: 2.606438636779785 and parameters: {'observation_period_num': 80, 'train_rates': 0.9446087548139506, 'learning_rate': 0.000988054982089226, 'batch_size': 214, 'step_size': 10, 'gamma': 0.7934630530754927}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:22:41,891][0m Trial 20 finished with value: 0.27633952748321144 and parameters: {'observation_period_num': 39, 'train_rates': 0.8115462789975934, 'learning_rate': 0.00015114776392393954, 'batch_size': 138, 'step_size': 11, 'gamma': 0.8630558256652173}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:25:32,299][0m Trial 21 finished with value: 0.16757308404091037 and parameters: {'observation_period_num': 123, 'train_rates': 0.8886841813496325, 'learning_rate': 0.00011847278616471513, 'batch_size': 62, 'step_size': 6, 'gamma': 0.8421185166552536}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:29:14,759][0m Trial 22 finished with value: 0.21539037135140648 and parameters: {'observation_period_num': 142, 'train_rates': 0.9488331442672254, 'learning_rate': 3.3001246771727094e-05, 'batch_size': 35, 'step_size': 5, 'gamma': 0.8489669142248071}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:32:03,489][0m Trial 23 finished with value: 0.19262365350951108 and parameters: {'observation_period_num': 125, 'train_rates': 0.8969668026573606, 'learning_rate': 7.935997106927054e-05, 'batch_size': 72, 'step_size': 8, 'gamma': 0.8300411276720855}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:34:06,468][0m Trial 24 finished with value: 0.4001258595168095 and parameters: {'observation_period_num': 94, 'train_rates': 0.8743557949473617, 'learning_rate': 1.1196511548586105e-05, 'batch_size': 101, 'step_size': 6, 'gamma': 0.8962423935433929}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:35:34,198][0m Trial 25 finished with value: 0.3303402058821357 and parameters: {'observation_period_num': 57, 'train_rates': 0.8130777089766995, 'learning_rate': 0.0002444529123580008, 'batch_size': 42, 'step_size': 14, 'gamma': 0.8047375727986393}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:39:41,264][0m Trial 26 finished with value: 0.18258652527157854 and parameters: {'observation_period_num': 164, 'train_rates': 0.9561312284714983, 'learning_rate': 8.655868483321169e-05, 'batch_size': 66, 'step_size': 11, 'gamma': 0.8683277128554024}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:40:27,145][0m Trial 27 finished with value: 0.1855081845402219 and parameters: {'observation_period_num': 26, 'train_rates': 0.918713291227298, 'learning_rate': 4.102625322270295e-05, 'batch_size': 92, 'step_size': 9, 'gamma': 0.8325047472463707}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:42:17,851][0m Trial 28 finished with value: 0.3817116511662801 and parameters: {'observation_period_num': 86, 'train_rates': 0.869662043276921, 'learning_rate': 1.3889460545774467e-05, 'batch_size': 124, 'step_size': 12, 'gamma': 0.7897878446107954}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:45:31,646][0m Trial 29 finished with value: 0.17035909561496793 and parameters: {'observation_period_num': 136, 'train_rates': 0.930064462220229, 'learning_rate': 0.00011321642885999814, 'batch_size': 86, 'step_size': 5, 'gamma': 0.909803495790893}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:48:38,372][0m Trial 30 finished with value: 0.3407370166544534 and parameters: {'observation_period_num': 141, 'train_rates': 0.8265751790981265, 'learning_rate': 2.1959299668823614e-05, 'batch_size': 84, 'step_size': 5, 'gamma': 0.9436543769514956}. Best is trial 4 with value: 0.1629013933918693.[0m
[32m[I 2025-01-06 23:52:47,139][0m Trial 31 finished with value: 0.1535933584463401 and parameters: {'observation_period_num': 164, 'train_rates': 0.9348252513253661, 'learning_rate': 0.00010713378124334499, 'batch_size': 36, 'step_size': 4, 'gamma': 0.9086670089690958}. Best is trial 31 with value: 0.1535933584463401.[0m
[32m[I 2025-01-06 23:56:55,384][0m Trial 32 finished with value: 0.14746019845837158 and parameters: {'observation_period_num': 163, 'train_rates': 0.9241022982002856, 'learning_rate': 0.00011240976338713237, 'batch_size': 34, 'step_size': 4, 'gamma': 0.907600313455432}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:00:58,944][0m Trial 33 finished with value: 0.34644724019602235 and parameters: {'observation_period_num': 164, 'train_rates': 0.8906216318930406, 'learning_rate': 0.0002422197154751487, 'batch_size': 33, 'step_size': 3, 'gamma': 0.9865199580505959}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:05:57,995][0m Trial 34 finished with value: 0.14766789775296865 and parameters: {'observation_period_num': 186, 'train_rates': 0.9701091802820713, 'learning_rate': 2.1284236670248623e-05, 'batch_size': 29, 'step_size': 4, 'gamma': 0.8861551508783413}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:10:53,215][0m Trial 35 finished with value: 0.14933027818645397 and parameters: {'observation_period_num': 185, 'train_rates': 0.9652347164567792, 'learning_rate': 2.37627088651426e-05, 'batch_size': 30, 'step_size': 4, 'gamma': 0.9293574424978948}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:15:54,079][0m Trial 36 finished with value: 0.3193495190805859 and parameters: {'observation_period_num': 191, 'train_rates': 0.9611403054875336, 'learning_rate': 1.937272088735037e-05, 'batch_size': 33, 'step_size': 1, 'gamma': 0.9388368004415778}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:20:24,077][0m Trial 37 finished with value: 0.6915518045425415 and parameters: {'observation_period_num': 182, 'train_rates': 0.9672146123301225, 'learning_rate': 6.897694510612365e-06, 'batch_size': 252, 'step_size': 4, 'gamma': 0.9205551342640882}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:26:06,956][0m Trial 38 finished with value: 0.1484679512910007 and parameters: {'observation_period_num': 215, 'train_rates': 0.9293301591663778, 'learning_rate': 0.00022260220809046957, 'batch_size': 30, 'step_size': 2, 'gamma': 0.9596977610479474}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:32:17,825][0m Trial 39 finished with value: 0.40011064135111296 and parameters: {'observation_period_num': 223, 'train_rates': 0.9717388765545096, 'learning_rate': 0.0005067832528913501, 'batch_size': 27, 'step_size': 2, 'gamma': 0.9580558044781116}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:38:11,856][0m Trial 40 finished with value: 0.1638506885184798 and parameters: {'observation_period_num': 229, 'train_rates': 0.9215021059477074, 'learning_rate': 2.349786748536997e-05, 'batch_size': 52, 'step_size': 2, 'gamma': 0.9813241215406538}. Best is trial 32 with value: 0.14746019845837158.[0m
[32m[I 2025-01-07 00:43:32,697][0m Trial 41 finished with value: 0.14468458215606975 and parameters: {'observation_period_num': 203, 'train_rates': 0.932066071801791, 'learning_rate': 0.00021787833298099409, 'batch_size': 40, 'step_size': 4, 'gamma': 0.9246946785573856}. Best is trial 41 with value: 0.14468458215606975.[0m
[32m[I 2025-01-07 00:49:00,953][0m Trial 42 finished with value: 0.09488399038712184 and parameters: {'observation_period_num': 206, 'train_rates': 0.9727921296908713, 'learning_rate': 0.0002130367041450415, 'batch_size': 49, 'step_size': 4, 'gamma': 0.9544545896216994}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 00:54:16,509][0m Trial 43 finished with value: 0.150461995817512 and parameters: {'observation_period_num': 207, 'train_rates': 0.9223583929075323, 'learning_rate': 0.00021489472594486429, 'batch_size': 49, 'step_size': 2, 'gamma': 0.9560006277345617}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 01:00:49,540][0m Trial 44 finished with value: 0.10597308725118637 and parameters: {'observation_period_num': 244, 'train_rates': 0.9769744921179071, 'learning_rate': 0.00041456304906422106, 'batch_size': 71, 'step_size': 1, 'gamma': 0.971568977741514}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 01:08:05,056][0m Trial 45 finished with value: 0.12935464621567336 and parameters: {'observation_period_num': 245, 'train_rates': 0.9777051937562086, 'learning_rate': 0.00047219990815253394, 'batch_size': 16, 'step_size': 3, 'gamma': 0.9710732853278475}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 01:15:26,351][0m Trial 46 finished with value: 0.12535219703401837 and parameters: {'observation_period_num': 251, 'train_rates': 0.9742556551930686, 'learning_rate': 0.0005278166139830882, 'batch_size': 18, 'step_size': 1, 'gamma': 0.9734913670553855}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 01:22:54,554][0m Trial 47 finished with value: 2.4201676547527313 and parameters: {'observation_period_num': 252, 'train_rates': 0.9824345857682701, 'learning_rate': 0.000995274864782622, 'batch_size': 17, 'step_size': 1, 'gamma': 0.9723445950058172}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 01:29:13,329][0m Trial 48 finished with value: 0.22336611151695251 and parameters: {'observation_period_num': 234, 'train_rates': 0.9877930911784253, 'learning_rate': 0.0005870163614889736, 'batch_size': 72, 'step_size': 3, 'gamma': 0.9478516309931028}. Best is trial 42 with value: 0.09488399038712184.[0m
[32m[I 2025-01-07 01:34:25,257][0m Trial 49 finished with value: 0.6923581832636367 and parameters: {'observation_period_num': 239, 'train_rates': 0.7004641436021872, 'learning_rate': 0.00037396282456400885, 'batch_size': 55, 'step_size': 1, 'gamma': 0.9709452091044181}. Best is trial 42 with value: 0.09488399038712184.[0m
最適ハイパーパラメータが見つかりました
最適ハイパーパラメータが best_hyperparameters_AAPL_Transformer(nomstl).json に保存されました
Epoch 1/300, Loss: 0.5558 | 0.5519
Epoch 2/300, Loss: 0.5962 | 0.5817
Epoch 3/300, Loss: 0.4744 | 0.5470
Epoch 4/300, Loss: 0.4066 | 0.4967
Epoch 5/300, Loss: 0.4169 | 0.4431
Epoch 6/300, Loss: 0.4365 | 0.5210
Epoch 7/300, Loss: 0.3947 | 0.5208
Epoch 8/300, Loss: 0.4565 | 0.5229
Epoch 9/300, Loss: 0.7343 | 0.5755
Epoch 10/300, Loss: 0.4415 | 0.5249
Epoch 11/300, Loss: 0.3749 | 0.4335
Epoch 12/300, Loss: 0.2960 | 0.3803
Epoch 13/300, Loss: 0.3072 | 0.3680
Epoch 14/300, Loss: 0.3438 | 0.3585
Epoch 15/300, Loss: 0.2814 | 0.3048
Epoch 16/300, Loss: 0.2599 | 0.3478
Epoch 17/300, Loss: 0.2326 | 0.2822
Epoch 18/300, Loss: 0.2193 | 0.2644
Epoch 19/300, Loss: 0.2153 | 0.2666
Epoch 20/300, Loss: 0.2030 | 0.2231
Epoch 21/300, Loss: 0.1869 | 0.2211
Epoch 22/300, Loss: 0.1824 | 0.2355
Epoch 23/300, Loss: 0.1729 | 0.2125
Epoch 24/300, Loss: 0.1719 | 0.2028
Epoch 25/300, Loss: 0.1699 | 0.2303
Epoch 26/300, Loss: 0.1689 | 0.1907
Epoch 27/300, Loss: 0.1670 | 0.1946
Epoch 28/300, Loss: 0.1706 | 0.2090
Epoch 29/300, Loss: 0.1677 | 0.1872
Epoch 30/300, Loss: 0.1706 | 0.1847
Epoch 31/300, Loss: 0.1804 | 0.2390
Epoch 32/300, Loss: 0.1707 | 0.1860
Epoch 33/300, Loss: 0.1682 | 0.1832
Epoch 34/300, Loss: 0.1830 | 0.1897
Epoch 35/300, Loss: 0.1630 | 0.1636
Epoch 36/300, Loss: 0.1637 | 0.1520
Epoch 37/300, Loss: 0.1670 | 0.2098
Epoch 38/300, Loss: 0.1497 | 0.1823
Epoch 39/300, Loss: 0.1491 | 0.1654
Epoch 40/300, Loss: 0.1510 | 0.1664
Epoch 41/300, Loss: 0.1398 | 0.1531
Epoch 42/300, Loss: 0.1365 | 0.1479
Epoch 43/300, Loss: 0.1418 | 0.1590
Epoch 44/300, Loss: 0.1344 | 0.1426
Epoch 45/300, Loss: 0.1315 | 0.1510
Epoch 46/300, Loss: 0.1340 | 0.1445
Epoch 47/300, Loss: 0.1307 | 0.1351
Epoch 48/300, Loss: 0.1286 | 0.1345
Epoch 49/300, Loss: 0.1271 | 0.1351
Epoch 50/300, Loss: 0.1242 | 0.1252
Epoch 51/300, Loss: 0.1247 | 0.1398
Epoch 52/300, Loss: 0.1234 | 0.1272
Epoch 53/300, Loss: 0.1248 | 0.1215
Epoch 54/300, Loss: 0.1210 | 0.1329
Epoch 55/300, Loss: 0.1211 | 0.1242
Epoch 56/300, Loss: 0.1222 | 0.1209
Epoch 57/300, Loss: 0.1229 | 0.1458
Epoch 58/300, Loss: 0.1234 | 0.1266
Epoch 59/300, Loss: 0.1261 | 0.1171
Epoch 60/300, Loss: 0.1228 | 0.1456
Epoch 61/300, Loss: 0.1168 | 0.1224
Epoch 62/300, Loss: 0.1181 | 0.1167
Epoch 63/300, Loss: 0.1186 | 0.1342
Epoch 64/300, Loss: 0.1133 | 0.1157
Epoch 65/300, Loss: 0.1132 | 0.1176
Epoch 66/300, Loss: 0.1149 | 0.1203
Epoch 67/300, Loss: 0.1106 | 0.1122
Epoch 68/300, Loss: 0.1076 | 0.1177
Epoch 69/300, Loss: 0.1071 | 0.1122
Epoch 70/300, Loss: 0.1061 | 0.1108
Epoch 71/300, Loss: 0.1048 | 0.1092
Epoch 72/300, Loss: 0.1039 | 0.1113
Epoch 73/300, Loss: 0.1037 | 0.1082
Epoch 74/300, Loss: 0.1037 | 0.1083
Epoch 75/300, Loss: 0.1034 | 0.1064
Epoch 76/300, Loss: 0.1029 | 0.1086
Epoch 77/300, Loss: 0.1023 | 0.1039
Epoch 78/300, Loss: 0.1015 | 0.1045
Epoch 79/300, Loss: 0.1010 | 0.1054
Epoch 80/300, Loss: 0.1004 | 0.1020
Epoch 81/300, Loss: 0.0999 | 0.1022
Epoch 82/300, Loss: 0.0999 | 0.1056
Epoch 83/300, Loss: 0.1001 | 0.1019
Epoch 84/300, Loss: 0.0994 | 0.1059
Epoch 85/300, Loss: 0.0989 | 0.1014
Epoch 86/300, Loss: 0.0995 | 0.1032
Epoch 87/300, Loss: 0.1008 | 0.1038
Epoch 88/300, Loss: 0.0988 | 0.1010
Epoch 89/300, Loss: 0.0975 | 0.1009
Epoch 90/300, Loss: 0.0966 | 0.1047
Epoch 91/300, Loss: 0.0967 | 0.1003
Epoch 92/300, Loss: 0.0954 | 0.1002
Epoch 93/300, Loss: 0.0946 | 0.1025
Epoch 94/300, Loss: 0.0935 | 0.0989
Epoch 95/300, Loss: 0.0932 | 0.0996
Epoch 96/300, Loss: 0.0927 | 0.1010
Epoch 97/300, Loss: 0.0918 | 0.0983
Epoch 98/300, Loss: 0.0913 | 0.1012
Epoch 99/300, Loss: 0.0912 | 0.0988
Epoch 100/300, Loss: 0.0909 | 0.0976
Epoch 101/300, Loss: 0.0898 | 0.0990
Epoch 102/300, Loss: 0.0903 | 0.0989
Epoch 103/300, Loss: 0.0897 | 0.0989
Epoch 104/300, Loss: 0.0890 | 0.0969
Epoch 105/300, Loss: 0.0884 | 0.0991
Epoch 106/300, Loss: 0.0887 | 0.0993
Epoch 107/300, Loss: 0.0884 | 0.1005
Epoch 108/300, Loss: 0.0881 | 0.0984
Epoch 109/300, Loss: 0.0874 | 0.0984
Epoch 110/300, Loss: 0.0876 | 0.0996
Epoch 111/300, Loss: 0.0867 | 0.1006
Epoch 112/300, Loss: 0.0871 | 0.0968
Epoch 113/300, Loss: 0.0867 | 0.0984
Epoch 114/300, Loss: 0.0861 | 0.0990
Epoch 115/300, Loss: 0.0865 | 0.0978
Epoch 116/300, Loss: 0.0862 | 0.1007
Epoch 117/300, Loss: 0.0862 | 0.0975
Epoch 118/300, Loss: 0.0853 | 0.0968
Epoch 119/300, Loss: 0.0854 | 0.0972
Epoch 120/300, Loss: 0.0849 | 0.0980
Epoch 121/300, Loss: 0.0845 | 0.0984
Epoch 122/300, Loss: 0.0841 | 0.0972
Epoch 123/300, Loss: 0.0836 | 0.0969
Epoch 124/300, Loss: 0.0835 | 0.1003
Epoch 125/300, Loss: 0.0838 | 0.0982
Epoch 126/300, Loss: 0.0834 | 0.0980
Epoch 127/300, Loss: 0.0836 | 0.0981
Epoch 128/300, Loss: 0.0829 | 0.0990
Epoch 129/300, Loss: 0.0825 | 0.0984
Epoch 130/300, Loss: 0.0832 | 0.0971
Epoch 131/300, Loss: 0.0820 | 0.0979
Epoch 132/300, Loss: 0.0823 | 0.0979
Epoch 133/300, Loss: 0.0820 | 0.0974
Epoch 134/300, Loss: 0.0824 | 0.0975
Epoch 135/300, Loss: 0.0817 | 0.0993
Epoch 136/300, Loss: 0.0814 | 0.0972
Epoch 137/300, Loss: 0.0822 | 0.0997
Epoch 138/300, Loss: 0.0814 | 0.0980
Epoch 139/300, Loss: 0.0810 | 0.0989
Epoch 140/300, Loss: 0.0806 | 0.0971
Epoch 141/300, Loss: 0.0801 | 0.0978
Epoch 142/300, Loss: 0.0802 | 0.0987
Epoch 143/300, Loss: 0.0807 | 0.0998
Epoch 144/300, Loss: 0.0807 | 0.0979
Epoch 145/300, Loss: 0.0793 | 0.0980
Epoch 146/300, Loss: 0.0805 | 0.0975
Epoch 147/300, Loss: 0.0799 | 0.0981
Epoch 148/300, Loss: 0.0792 | 0.0999
Epoch 149/300, Loss: 0.0795 | 0.0974
Epoch 150/300, Loss: 0.0791 | 0.0991
Epoch 151/300, Loss: 0.0790 | 0.0979
Epoch 152/300, Loss: 0.0801 | 0.0987
Epoch 153/300, Loss: 0.0793 | 0.0982
Epoch 154/300, Loss: 0.0794 | 0.0976
Epoch 155/300, Loss: 0.0794 | 0.0987
Epoch 156/300, Loss: 0.0784 | 0.0985
Epoch 157/300, Loss: 0.0784 | 0.0988
Epoch 158/300, Loss: 0.0784 | 0.0978
Epoch 159/300, Loss: 0.0790 | 0.0985
Epoch 160/300, Loss: 0.0789 | 0.0988
Epoch 161/300, Loss: 0.0782 | 0.0970
Epoch 162/300, Loss: 0.0790 | 0.0967
Epoch 163/300, Loss: 0.0779 | 0.0977
Epoch 164/300, Loss: 0.0782 | 0.0973
Epoch 165/300, Loss: 0.0784 | 0.0967
Epoch 166/300, Loss: 0.0781 | 0.0982
Epoch 167/300, Loss: 0.0781 | 0.0975
Epoch 168/300, Loss: 0.0775 | 0.0979
Epoch 169/300, Loss: 0.0779 | 0.0985
Epoch 170/300, Loss: 0.0772 | 0.0982
Epoch 171/300, Loss: 0.0776 | 0.0991
Epoch 172/300, Loss: 0.0772 | 0.0975
Epoch 173/300, Loss: 0.0770 | 0.0979
Epoch 174/300, Loss: 0.0777 | 0.0991
Epoch 175/300, Loss: 0.0763 | 0.0977
Epoch 176/300, Loss: 0.0774 | 0.0974
Epoch 177/300, Loss: 0.0772 | 0.0976
Epoch 178/300, Loss: 0.0775 | 0.0977
Epoch 179/300, Loss: 0.0772 | 0.0981
Epoch 180/300, Loss: 0.0773 | 0.0982
Epoch 181/300, Loss: 0.0763 | 0.0968
Epoch 182/300, Loss: 0.0766 | 0.0978
Epoch 183/300, Loss: 0.0766 | 0.0982
Epoch 184/300, Loss: 0.0765 | 0.0970
Epoch 185/300, Loss: 0.0766 | 0.0985
Epoch 186/300, Loss: 0.0768 | 0.0973
Epoch 187/300, Loss: 0.0770 | 0.0977
Epoch 188/300, Loss: 0.0760 | 0.0978
Epoch 189/300, Loss: 0.0762 | 0.0975
Epoch 190/300, Loss: 0.0761 | 0.0981
Epoch 191/300, Loss: 0.0764 | 0.0977
Epoch 192/300, Loss: 0.0762 | 0.0978
Epoch 193/300, Loss: 0.0750 | 0.0979
Epoch 194/300, Loss: 0.0759 | 0.0984
Epoch 195/300, Loss: 0.0758 | 0.0984
Epoch 196/300, Loss: 0.0758 | 0.0974
Epoch 197/300, Loss: 0.0769 | 0.0989
Epoch 198/300, Loss: 0.0766 | 0.0989
Epoch 199/300, Loss: 0.0761 | 0.0988
Epoch 200/300, Loss: 0.0759 | 0.0990
Epoch 201/300, Loss: 0.0754 | 0.0986
Epoch 202/300, Loss: 0.0757 | 0.0987
Epoch 203/300, Loss: 0.0752 | 0.0989
Epoch 204/300, Loss: 0.0756 | 0.0980
Epoch 205/300, Loss: 0.0760 | 0.0985
Epoch 206/300, Loss: 0.0754 | 0.0981
Epoch 207/300, Loss: 0.0758 | 0.0981
Epoch 208/300, Loss: 0.0754 | 0.0988
Epoch 209/300, Loss: 0.0755 | 0.0987
Epoch 210/300, Loss: 0.0762 | 0.0980
Epoch 211/300, Loss: 0.0759 | 0.0976
Epoch 212/300, Loss: 0.0755 | 0.0983
Epoch 213/300, Loss: 0.0763 | 0.0977
Epoch 214/300, Loss: 0.0750 | 0.0983
Epoch 215/300, Loss: 0.0756 | 0.0985
Epoch 216/300, Loss: 0.0756 | 0.0985
Epoch 217/300, Loss: 0.0748 | 0.0988
Epoch 218/300, Loss: 0.0752 | 0.0984
Epoch 219/300, Loss: 0.0758 | 0.0982
Epoch 220/300, Loss: 0.0754 | 0.0986
Epoch 221/300, Loss: 0.0749 | 0.0989
Epoch 222/300, Loss: 0.0752 | 0.0988
Epoch 223/300, Loss: 0.0755 | 0.0990
Epoch 224/300, Loss: 0.0753 | 0.0990
Epoch 225/300, Loss: 0.0746 | 0.0989
Epoch 226/300, Loss: 0.0754 | 0.0988
Epoch 227/300, Loss: 0.0752 | 0.0993
Epoch 228/300, Loss: 0.0749 | 0.0992
Epoch 229/300, Loss: 0.0753 | 0.0989
Epoch 230/300, Loss: 0.0752 | 0.0987
Epoch 231/300, Loss: 0.0753 | 0.0984
Epoch 232/300, Loss: 0.0745 | 0.0984
Epoch 233/300, Loss: 0.0750 | 0.0986
Epoch 234/300, Loss: 0.0748 | 0.0985
Epoch 235/300, Loss: 0.0749 | 0.0990
Epoch 236/300, Loss: 0.0755 | 0.0990
Epoch 237/300, Loss: 0.0749 | 0.0987
Epoch 238/300, Loss: 0.0750 | 0.0987
Epoch 239/300, Loss: 0.0750 | 0.0982
Epoch 240/300, Loss: 0.0754 | 0.0980
Epoch 241/300, Loss: 0.0758 | 0.0979
Epoch 242/300, Loss: 0.0752 | 0.0979
Epoch 243/300, Loss: 0.0746 | 0.0981
Epoch 244/300, Loss: 0.0760 | 0.0983
Epoch 245/300, Loss: 0.0747 | 0.0984
Epoch 246/300, Loss: 0.0751 | 0.0985
Epoch 247/300, Loss: 0.0760 | 0.0984
Epoch 248/300, Loss: 0.0743 | 0.0985
Epoch 249/300, Loss: 0.0750 | 0.0987
Epoch 250/300, Loss: 0.0746 | 0.0988
Epoch 251/300, Loss: 0.0748 | 0.0986
Epoch 252/300, Loss: 0.0749 | 0.0987
Epoch 253/300, Loss: 0.0744 | 0.0990
Epoch 254/300, Loss: 0.0749 | 0.0989
Epoch 255/300, Loss: 0.0746 | 0.0989
Epoch 256/300, Loss: 0.0748 | 0.0987
Epoch 257/300, Loss: 0.0751 | 0.0987
Epoch 258/300, Loss: 0.0757 | 0.0988
Epoch 259/300, Loss: 0.0742 | 0.0988
Epoch 260/300, Loss: 0.0749 | 0.0987
Epoch 261/300, Loss: 0.0752 | 0.0988
Epoch 262/300, Loss: 0.0749 | 0.0989
Epoch 263/300, Loss: 0.0742 | 0.0989
Epoch 264/300, Loss: 0.0750 | 0.0987
Epoch 265/300, Loss: 0.0747 | 0.0988
Epoch 266/300, Loss: 0.0737 | 0.0990
Epoch 267/300, Loss: 0.0751 | 0.0988
Epoch 268/300, Loss: 0.0745 | 0.0986
Epoch 269/300, Loss: 0.0752 | 0.0985
Epoch 270/300, Loss: 0.0746 | 0.0986
Epoch 271/300, Loss: 0.0742 | 0.0987
Epoch 272/300, Loss: 0.0753 | 0.0987
Epoch 273/300, Loss: 0.0745 | 0.0988
Epoch 274/300, Loss: 0.0744 | 0.0988
Epoch 275/300, Loss: 0.0752 | 0.0987
Epoch 276/300, Loss: 0.0745 | 0.0985
Epoch 277/300, Loss: 0.0741 | 0.0983
Epoch 278/300, Loss: 0.0746 | 0.0983
Epoch 279/300, Loss: 0.0750 | 0.0981
Epoch 280/300, Loss: 0.0746 | 0.0981
Epoch 281/300, Loss: 0.0751 | 0.0981
Epoch 282/300, Loss: 0.0748 | 0.0982
Epoch 283/300, Loss: 0.0746 | 0.0983
Epoch 284/300, Loss: 0.0748 | 0.0983
Epoch 285/300, Loss: 0.0751 | 0.0982
Epoch 286/300, Loss: 0.0747 | 0.0983
Epoch 287/300, Loss: 0.0749 | 0.0984
Epoch 288/300, Loss: 0.0744 | 0.0986
Epoch 289/300, Loss: 0.0747 | 0.0986
Epoch 290/300, Loss: 0.0747 | 0.0987
Epoch 291/300, Loss: 0.0746 | 0.0988
Epoch 292/300, Loss: 0.0747 | 0.0990
Epoch 293/300, Loss: 0.0744 | 0.0991
Epoch 294/300, Loss: 0.0749 | 0.0990
Epoch 295/300, Loss: 0.0750 | 0.0990
Epoch 296/300, Loss: 0.0743 | 0.0989
Epoch 297/300, Loss: 0.0741 | 0.0990
Epoch 298/300, Loss: 0.0745 | 0.0990
Epoch 299/300, Loss: 0.0744 | 0.0989
Epoch 300/300, Loss: 0.0745 | 0.0989
Runtime (seconds): 975.3276402950287
/home2/y2021/k2110261/.conda/envs/tensorflow/lib/python3.11/site-packages/sklearn/metrics/_regression.py:1266: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.
  warnings.warn(msg, UndefinedMetricWarning)
MSE: 307.339376472868
RMSE: 17.531097412109375
MAE: 17.531097412109375
R-squared: nan
[229.2189]
